{
  "date": "2026-01-18",
  "category": "cs.AI",
  "summary": "æ¬¢è¿æ¥åˆ° UTC æ—¶é—´ 2026-01-18 çš„ arXiv ä¸­æ–‡ TLDR å¿«æŠ¥ï¼\n\n**ä»Šæ—¥æ€»ç»“**ï¼š\nä»Šå¤©çš„ arXiv è®ºæ–‡çˆ†å‘å¼åœ°é›†ä¸­åœ¨ **Agentic AIï¼ˆä»£ç†æ™ºèƒ½ï¼‰** çš„æ¶æ„ä¸åº”ç”¨ä¸Šï¼Œå°¤å…¶æ˜¯å¤šæ™ºèƒ½ä½“ï¼ˆMulti-Agentï¼‰ç³»ç»Ÿåœ¨ç§‘å­¦å‘ç°ï¼ˆAI Scientistï¼‰å’Œå¤æ‚å·¥ä½œæµä¸­çš„è½åœ°ã€‚æ­¤å¤–ï¼Œå…³äº LLM **æ¨ç†èƒ½åŠ›è¾¹ç•Œ**çš„è®¨è®ºä¹Ÿéå¸¸ç²¾å½©â€”â€”ä»â€œLLM æ˜¯å¦æ¯”é»‘çŒ©çŒ©èªæ˜â€çš„å¿ƒæ™ºç†è®ºæ¢è®¨ï¼Œåˆ°å¤šè·³æ¨ç†ä¸­çš„â€œæœ€å¼±ä¸€ç¯å®šå¾‹â€ã€‚æœ€åï¼Œå¤šæ¨¡æ€ï¼ˆVLM/Audioï¼‰é¢†åŸŸä¹Ÿæœ‰å‡ ç¯‡é’ˆå¯¹æ€§å¾ˆå¼ºçš„å„ä¸ªå‡»ç ´ï¼ŒåŒ…æ‹¬é•¿å°¾éŸ³é¢‘é¢„è®­ç»ƒå’Œéè§†åŸŸæˆåƒã€‚\n\n---\n\n### ğŸš€ Agentic AIï¼šä»æ¦‚å¿µèµ°å‘ç§‘å­¦å®æˆ˜\n**Agentic AI æ­£åœ¨ä»å•çº¯çš„æ–‡æœ¬ç”Ÿæˆè¿ˆå‘è‡ªä¸»æ„ŸçŸ¥ã€è§„åˆ’å’Œè¡ŒåŠ¨çš„å®ä½“ã€‚ä»Šå¤©çš„å‡ ç¯‡è®ºæ–‡ä¸ä»…åšäº†ç»¼è¿°ï¼Œè¿˜å±•ç¤ºäº†å…¶åœ¨åŒ–å­¦ã€ææ–™ç§‘å­¦ç­‰ç¡¬æ ¸é¢†åŸŸçš„â€œå…¨è‡ªåŠ¨â€æ½œåŠ›ã€‚**\n\n**1. Agentic Artificial Intelligence (AI): Architectures, Taxonomies, and Evaluation of Large Language Model Agents**\n**Agentic AIï¼šå¤§å‹è¯­è¨€æ¨¡å‹ä»£ç†çš„æ¶æ„ã€åˆ†ç±»ä¸è¯„ä¼°**\nè¿™æ˜¯ä¸€ç¯‡å…¨é¢çš„ç»¼è¿°ã€‚ä½œè€…æå‡ºå°† Agent åˆ†ä¸ºæ„ŸçŸ¥ã€å¤§è„‘ã€è§„åˆ’ã€è¡ŒåŠ¨ã€å·¥å…·ä½¿ç”¨å’Œåä½œå…­ä¸ªæ¨¡å—ã€‚è®ºæ–‡è¯¦ç»†è®¨è®ºäº†ä»çº¿æ€§æ¨ç†åˆ°åŸç”Ÿæ¨ç†æ¨¡å‹çš„è½¬å˜ï¼Œä»¥åŠä»å›ºå®š API åˆ° MCPï¼ˆæ¨¡å‹ä¸Šä¸‹æ–‡åè®®ï¼‰å’ŒåŸç”Ÿè®¡ç®—æœºä½¿ç”¨çš„æ¼”è¿›ã€‚\n**æ ¸å¿ƒçœ‹ç‚¹**ï¼šæå‡ºäº†ä¸€ä¸ªç»Ÿä¸€çš„åˆ†ç±»æ³•ï¼Œæ˜¯äº†è§£å½“å‰ Agent é¢†åŸŸçš„ç»ä½³å…¥é—¨ææ–™ã€‚\n\n**2. Agentic Reasoning for Large Language Models**\n**å¤§å‹è¯­è¨€æ¨¡å‹çš„ä»£ç†æ¨ç†**\nå¦ä¸€ç¯‡é‡é‡çº§ç»¼è¿°ã€‚æ–‡ç« å°†â€œä»£ç†æ¨ç†â€åˆ†ä¸ºä¸‰ä¸ªå±‚æ¬¡ï¼šåŸºç¡€ä»£ç†æ¨ç†ï¼ˆå•ä½“è§„åˆ’/å·¥å…·ä½¿ç”¨ï¼‰ã€è‡ªè¿›åŒ–ä»£ç†æ¨ç†ï¼ˆé€šè¿‡åé¦ˆ/è®°å¿†è¿›åŒ–ï¼‰å’Œé›†ä½“å¤šæ™ºèƒ½ä½“æ¨ç†ï¼ˆåä½œ/å…±äº«ç›®æ ‡ï¼‰ã€‚\n**æ ¸å¿ƒçœ‹ç‚¹**ï¼šå¼ºè°ƒäº†â€œæ¨ç†â€æ˜¯è¿æ¥æ€ç»´ä¸è¡ŒåŠ¨çš„æ¡¥æ¢ï¼Œå¹¶å±•æœ›äº†ä¸–ç•Œæ¨¡å‹ï¼ˆWorld Modelingï¼‰åœ¨ Agent ä¸­çš„é‡è¦æ€§ã€‚\n\n**3. Rethinking the AI Scientist: Interactive Multi-Agent Workflows for Scientific Discovery**\n**åæ€ AI ç§‘å­¦å®¶ï¼šç”¨äºç§‘å­¦å‘ç°çš„äº¤äº’å¼å¤šæ™ºèƒ½ä½“å·¥ä½œæµ**\nç°æœ‰çš„ AI ç§‘å­¦å‘ç°ç³»ç»Ÿé€šå¸¸æ˜¯æ‰¹å¤„ç†çš„ï¼Œååº”æ…¢ã€‚æœ¬æ–‡æå‡ºäº† **Deep Research**ï¼Œä¸€ä¸ªå¤šæ™ºèƒ½ä½“ç³»ç»Ÿï¼ŒåŒ…å«è§„åˆ’ã€æ•°æ®åˆ†æã€æ–‡çŒ®æœç´¢å’Œæ–°é¢–æ€§æ£€æµ‹ç­‰ä¸“é—¨ Agentã€‚\n**æ ¸å¿ƒçœ‹ç‚¹**ï¼šåœ¨ç”Ÿç‰©å­¦åŸºå‡†æµ‹è¯• BixBench ä¸Šï¼Œè¯¥ç³»ç»Ÿæ¯”ç°æœ‰åŸºçº¿é«˜å‡º 14-26 ä¸ªç™¾åˆ†ç‚¹ï¼Œå®ç°äº†åˆ†é’Ÿçº§çš„ç§‘ç ”è¿­ä»£å¾ªç¯ã€‚\n\n**4. A Cloud-based Multi-Agentic Workflow for Science**\n**åŸºäºäº‘çš„ç§‘å­¦å¤šæ™ºèƒ½ä½“å·¥ä½œæµ**\né’ˆå¯¹åŒ–å­¦å’Œææ–™ç§‘å­¦ï¼Œä½œè€…è®¾è®¡äº†ä¸€ä¸ªå…¨äº‘ç«¯çš„ Supervisor Agent æ¶æ„ï¼ŒæŒ‡æŒ¥ä¸€ç³»åˆ—å­ Agent è¿›è¡Œæ–‡çŒ®ç»¼è¿°ã€æ•°æ®åˆ†æç”šè‡³è¿è¡Œæ¨¡æ‹Ÿã€‚\n**æ ¸å¿ƒçœ‹ç‚¹**ï¼šåœ¨å‚¬åŒ–å‰‚ç ”ç©¶çš„ PoC ä¸­ï¼Œç³»ç»Ÿèƒ½ä»¥ 90% çš„å‡†ç¡®ç‡å°†ä»»åŠ¡è·¯ç”±ç»™æ­£ç¡®çš„ Agentï¼Œå±•ç¤ºäº† Agent åœ¨å¤æ‚ç§‘å­¦è®¡ç®—ä¸­çš„è°ƒåº¦èƒ½åŠ›ã€‚\n\n**5. Zero-Permission Manipulation: Can We Trust Large Multimodal Model Powered GUI Agents?**\n**é›¶æƒé™æ“çºµï¼šæˆ‘ä»¬å¯ä»¥ä¿¡ä»»ç”±å¤§å‹å¤šæ¨¡æ€æ¨¡å‹é©±åŠ¨çš„ GUI ä»£ç†å—ï¼Ÿ**\n**å®‰å…¨è­¦é’Ÿ**ã€‚GUI Agent è¢«èµ‹äºˆäº†æ“ä½œæ‰‹æœºå±å¹•çš„é«˜æƒé™ã€‚æœ¬æ–‡æ­ç¤ºäº†ä¸€ç§â€œè§†è§‰åŸå­æ€§â€å‡è®¾çš„ç¼ºé™·ï¼Œæ”»å‡»è€…å¯ä»¥åˆ©ç”¨è§‚å¯Ÿ-è¡ŒåŠ¨çš„æ—¶é—´å·®ï¼Œé€šè¿‡ **Action Rebinding** æ”»å‡»ï¼Œåœ¨é›¶æƒé™çš„æƒ…å†µä¸‹åŠ«æŒ Agent çš„æ“ä½œã€‚\n**æ ¸å¿ƒçœ‹ç‚¹**ï¼šåˆ©ç”¨ UI çŠ¶æ€å˜åŒ–çš„æ—¶é—´å·®è¿›è¡Œæ”»å‡»ï¼Œåœ¨ Android GUI Agent ä¸Šå®ç°äº† 100% çš„æ”»å‡»æˆåŠŸç‡ã€‚\n\n---\n\n### ğŸ§  LLM æ¨ç†ä¸è®¤çŸ¥ï¼šé»‘çŒ©çŒ©ã€æ‰©æ•£æ¨¡å‹ä¸ RLHF\n**ä»Šå¤©çš„è®ºæ–‡å¯¹ LLM çš„â€œæ™ºåŠ›â€è¿›è¡Œäº†å»é­…å’Œæ·±åº¦å‰–æï¼ŒåŒæ—¶ä¹Ÿæå‡ºäº†æ–°çš„ä¼˜åŒ–æ–¹å‘ã€‚**\n\n**6. Are LLMs Smarter Than Chimpanzees? An Evaluation on Perspective Taking and Knowledge State Estimation**\n**LLM æ¯”é»‘çŒ©çŒ©èªæ˜å—ï¼Ÿå…³äºè§‚ç‚¹é‡‡æ‹©å’ŒçŸ¥è¯†çŠ¶æ€ä¼°è®¡çš„è¯„ä¼°**\n**è¶£å‘³ä¸æ·±åº¦å¹¶å­˜**ã€‚äººç±»æ™ºåŠ›åŒºåˆ«äºåŠ¨ç‰©çš„å…³é”®åœ¨äºâ€œå¿ƒæ™ºç†è®ºâ€ï¼ˆToMï¼‰ï¼Œå³ç†è§£ä»–äººçš„çŸ¥è¯†çŠ¶æ€ã€‚ç ”ç©¶å‘ç°ï¼Œè™½ç„¶é»‘çŒ©çŒ©åšä¸åˆ°è¿™ä¸€ç‚¹ï¼Œä½†ç›®å‰çš„ SOTA LLM åœ¨è¿™ä¸¤é¡¹ä»»åŠ¡ä¸Šçš„è¡¨ç°ä¹Ÿæ¥è¿‘éšæœºï¼Œç”šè‡³ä¸å¦‚äººç±»ã€‚\n**æ ¸å¿ƒçœ‹ç‚¹**ï¼šLLM ä»ç„¶ç¼ºä¹çœŸæ­£çš„â€œæ„å›¾ç†è§£â€èƒ½åŠ›ï¼Œè¿™å¯èƒ½æ˜¯æœªæ¥ç ”ç©¶çš„é‡ç‚¹ã€‚\n\n**7. Failure Modes in Multi-Hop QA: The Weakest Link Law and the Recognition Bottleneck**\n**å¤šè·³é—®ç­”ä¸­çš„æ•…éšœæ¨¡å¼ï¼šæœ€å¼±ä¸€ç¯å®šå¾‹ä¸è¯†åˆ«ç“¶é¢ˆ**\nLLM åœ¨å¤„ç†å¤šè·³æ¨ç†ï¼ˆMulti-hop Reasoningï¼‰æ—¶ä¸ºä½•å¤±è´¥ï¼Ÿæœ¬æ–‡æå‡ºäº†**â€œæœ€å¼±ä¸€ç¯å®šå¾‹â€**ï¼šå¤šè·³æ¨ç†çš„æ€§èƒ½ä¼šåç¼©åˆ°è¯æ®é“¾ä¸­æœ€ä¸æ˜æ˜¾çš„é‚£ä¸ªè¯æ®çš„æ°´å¹³ã€‚\n**æ ¸å¿ƒçœ‹ç‚¹**ï¼šè¿™æ˜¯ç”±ç»å¯¹ä½ç½®åå·®å¼•èµ·çš„ï¼Œè€Œéè¯æ®é—´çš„è·ç¦»ã€‚åªè¦æœ‰ä¸€ä¸ªè¯æ®éš¾ä»¥â€œè¯†åˆ«â€ï¼Œæ•´ä¸ªæ¨ç†é“¾å°±ä¼šæ–­è£‚ã€‚\n\n**8. Plan, Verify and Fill: A Structured Parallel Decoding Approach for Diffusion Language Models**\n**è§„åˆ’ã€éªŒè¯ä¸å¡«å……ï¼šæ‰©æ•£è¯­è¨€æ¨¡å‹çš„ç»“æ„åŒ–å¹¶è¡Œè§£ç æ–¹æ³•**\næ‰©æ•£è¯­è¨€æ¨¡å‹ï¼ˆDiffusion LMï¼‰æ˜¯éè‡ªå›å½’ç”Ÿæˆçš„å¸Œæœ›ã€‚æœ¬æ–‡æå‡º **PVF** èŒƒå¼ï¼Œå…ˆç”Ÿæˆéª¨æ¶ï¼ˆPlanï¼‰ï¼Œå†éªŒè¯ï¼ˆVerifyï¼‰ï¼Œæœ€åå¡«å……ç»†èŠ‚ï¼ˆFillï¼‰ã€‚\n**æ ¸å¿ƒçœ‹ç‚¹**ï¼šæ— éœ€è®­ç»ƒï¼Œåœ¨ LLaDA-8B ä¸Šå‡å°‘äº† 65% çš„è¯„ä¼°æ¬¡æ•°ï¼ˆNFEï¼‰ï¼Œæ•ˆç‡å¤§å¢ä¸”ä¸å¤±å‡†ç¡®æ€§ã€‚\n\n**9. Orthogonalized Policy Optimization: Decoupling Sampling Geometry from Optimization Geometry in RLHF**\n**æ­£äº¤ç­–ç•¥ä¼˜åŒ–ï¼šè§£è€¦ RLHF ä¸­çš„é‡‡æ ·å‡ ä½•ä¸ä¼˜åŒ–å‡ ä½•**\né’ˆå¯¹ PPO/DPO ç­‰ç®—æ³•ä¸­é‡‡æ ·ä¸ä¼˜åŒ–è€¦åˆå¯¼è‡´çš„ä¸ç¨³å®šæ€§ï¼Œä½œè€…æå‡ºäº† **OPO**ã€‚é€šè¿‡æ¬§å‡ é‡Œå¾—é•œåƒæ˜ å°„ï¼Œå°†é‡‡æ ·å‡ ä½•ä»…ä½œä¸ºçº¿æ€§é©±åŠ¨åŠ›ï¼Œè€Œç‹¬ç«‹ç¡®å®šä¼˜åŒ–å‡ ä½•ã€‚\n**æ ¸å¿ƒçœ‹ç‚¹**ï¼šæä¾›äº†ä¸€ä¸ªé—­å¼è§£ï¼ˆclosed-form solutionï¼‰ï¼Œæ¢¯åº¦åŠ¨æ€çº¿æ€§ä¸”ä¸é¥±å’Œï¼Œä»ç†è®ºä¸Šä¼˜åŒ–äº†å¯¹é½ï¼ˆAlignmentï¼‰è¿‡ç¨‹ã€‚\n\n---\n\n### ğŸ‘ï¸ ğŸ‘‚ å¤šæ¨¡æ€ï¼šéŸ³é¢‘ã€è§†è§‰ä¸ 3D\n**å¤šæ¨¡æ€é¢†åŸŸå¼€å§‹å…³æ³¨æ›´ç»†ç²’åº¦çš„æ§åˆ¶å’Œæ›´å¹¿æ³›çš„åœºæ™¯è¦†ç›–ã€‚**\n\n**10. SLAP: Scalable Language-Audio Pretraining with Variable-Duration Audio and Multi-Objective Training**\n**SLAPï¼šå…·æœ‰å¯å˜æ—¶é•¿éŸ³é¢‘å’Œå¤šç›®æ ‡è®­ç»ƒçš„å¯æ‰©å±•è¯­è¨€-éŸ³é¢‘é¢„è®­ç»ƒ**\nç°æœ‰çš„ CLAP æ¨¡å‹å—é™äºçŸ­ä¸”å›ºå®šæ—¶é•¿çš„éŸ³é¢‘ã€‚SLAPé€šè¿‡ 1.09 äº¿ä¸ªéŸ³é¢‘-æ–‡æœ¬å¯¹å’Œå¯å˜æ—¶é•¿è®­ç»ƒï¼Œè§£å†³äº†è¿™ä¸€é—®é¢˜ã€‚\n**æ ¸å¿ƒçœ‹ç‚¹**ï¼šç»Ÿä¸€äº†å¯¹æ¯”æŸå¤±ã€è‡ªç›‘ç£æŸå¤±å’Œç”ŸæˆæŸå¤±ï¼Œåœ¨éŸ³é¢‘æ£€ç´¢å’Œåˆ†ç±»ä¸Šè¾¾åˆ°äº†æ–° SOTAã€‚\n\n**11. Soft Shadow Diffusion (SSD): Physics-inspired Learning for 3D Computational Periscopy**\n**è½¯é˜´å½±æ‰©æ•£ (SSD)ï¼šç”¨äº 3D è®¡ç®—æ½œæœ›é•œçš„ç‰©ç†å¯å‘å­¦ä¹ **\n**é»‘ç§‘æŠ€**ã€‚å¦‚ä½•çœ‹è§å¢™åé¢çš„ç‰©ä½“ï¼Ÿéè§†åŸŸæˆåƒï¼ˆNLOSï¼‰ã€‚æœ¬æ–‡åˆ©ç”¨ç‰©ä½“æŠ•å°„åœ¨å¢™ä¸Šçš„å¾®å¼±â€œè½¯é˜´å½±â€ï¼Œç»“åˆç‰©ç†è§„å¾‹å’Œæ‰©æ•£æ¨¡å‹ï¼Œé‡å»ºéšè—çš„ 3D åœºæ™¯ã€‚\n**æ ¸å¿ƒçœ‹ç‚¹**ï¼šç‰©ç†å¯å‘çš„ç¥ç»ç½‘ç»œï¼Œèƒ½åœ¨ä»…æœ‰ä¸€å¼ æ™®é€šç…§ç‰‡çš„æƒ…å†µä¸‹â€œçœ‹è§â€éšè—ç‰©ä½“ã€‚\n\n**12. Do MLLMs See What We See? Analyzing Visualization Literacy Barriers in AI Systems**\n**MLLM çœ‹åˆ°æˆ‘ä»¬æ‰€çœ‹åˆ°çš„äº†å—ï¼Ÿåˆ†æ AI ç³»ç»Ÿä¸­çš„å¯è§†åŒ–ç´ å…»éšœç¢**\nMLLM åœ¨çœ‹å›¾è¡¨æ—¶ä¹Ÿä¼šâ€œæ–‡ç›²â€ã€‚ç ”ç©¶å‘ç°ï¼Œæ¨¡å‹åœ¨å¤„ç†é¢œè‰²å¯†é›†ã€åŸºäºç‰‡æ®µçš„å¯è§†åŒ–ï¼ˆå¦‚å †å æŸ±çŠ¶å›¾ï¼‰æ—¶è¡¨ç°ç³Ÿç³•ï¼Œç»å¸¸æ— æ³•å½¢æˆä¸€è‡´çš„æ¯”è¾ƒæ¨ç†ã€‚\n**æ ¸å¿ƒçœ‹ç‚¹**ï¼šå»ºç«‹äº†ä¸€ä¸ª MLLM çš„è§†è§‰ç´ å…»å¤±è´¥åˆ†ç±»å­¦ï¼ŒæŒ‡å‡ºäº†æœºå™¨ç‰¹æœ‰çš„è®¤çŸ¥éšœç¢ã€‚\n\n**13. MemeLens: Multilingual Multitask VLMs for Memes**\n**MemeLensï¼šç”¨äºæ¨¡å› çš„å¤šè¯­è¨€å¤šä»»åŠ¡ VLM**\nMemeï¼ˆæ¢—å›¾ï¼‰æ˜¯äº’è”ç½‘æ–‡åŒ–çš„ç»“æ™¶ï¼ŒåŒ…å«å¤æ‚çš„éšå–»å’Œè®½åˆºã€‚ä½œè€…æ•´åˆäº† 38 ä¸ªæ•°æ®é›†ï¼Œè®­ç»ƒäº†ä¸€ä¸ªä¸“é—¨ç†è§£ Meme çš„ VLMï¼Œæ¶µç›–ä»‡æ¨è¨€è®ºã€å¹½é»˜ã€è®½åˆºç­‰ 20 ä¸ªä»»åŠ¡ã€‚\n**æ ¸å¿ƒçœ‹ç‚¹**ï¼šç†è§£ Meme éœ€è¦æå¼ºçš„è·¨æ¨¡æ€å’Œæ–‡åŒ–èƒŒæ™¯çŸ¥è¯†ï¼Œå•ä¸€æ•°æ®é›†å¾®è°ƒå®¹æ˜“å¯¼è‡´è¿‡æ‹Ÿåˆï¼Œç»Ÿä¸€è®­ç»ƒæ‰æ˜¯å‡ºè·¯ã€‚\n\n---\n\n### âš—ï¸ AI for Scienceï¼šææ–™ä¸åˆ†å­\n**é™¤äº†å‰é¢çš„ Agent å·¥ä½œæµï¼Œè¿™äº›ä¸“ç”¨æ¨¡å‹ä¹Ÿåœ¨å‘åŠ›ã€‚**\n\n**14. CoLLaMo: Improving Large Molecular Language Model via Relation-aware Multimodal Collaboration**\n**CoLLaMoï¼šé€šè¿‡å…³ç³»æ„ŸçŸ¥å¤šæ¨¡æ€åä½œæ”¹è¿›å¤§å‹åˆ†å­è¯­è¨€æ¨¡å‹**\nåˆ†å­æœ‰ 1D å­—ç¬¦ä¸²ã€2D å›¾å’Œ 3D æ„è±¡ã€‚CoLLaMo é€šè¿‡å¤šçº§æŠ•å½±ä»ªå°†è¿™äº›æ¨¡æ€èåˆï¼Œå¹¶å¼•å…¥äº†åŸºäº GPT çš„å„ç§è¯„ä¼°æŒ‡æ ‡ã€‚\n**æ ¸å¿ƒçœ‹ç‚¹**ï¼šåœ¨åˆ†å­æè¿°å’Œå±æ€§é—®ç­”ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œè§£å†³äº†å•ä¸€æ¨¡æ€ä¿¡æ¯ä¸è¶³çš„é—®é¢˜ã€‚\n\n**15. Ontology-aligned structuring and reuse of multimodal materials data**\n**æœ¬ä½“å¯¹é½çš„å¤šæ¨¡æ€ææ–™æ•°æ®ç»“æ„åŒ–ä¸é‡ç”¨**\nä¸ºäº†è§£å†³ææ–™ç§‘å­¦ä¸­æ•°æ®â€œæ­»â€åœ¨ PDF é‡Œçš„é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºåˆ©ç”¨ LLM ç»“åˆæœ¬ä½“ï¼ˆOntologyï¼‰è‡ªåŠ¨æå–æ–‡çŒ®ä¸­çš„è®¡ç®—å·¥ä½œæµå’Œå‚æ•°ã€‚\n**æ ¸å¿ƒçœ‹ç‚¹**ï¼šæ„å»ºäº†é•åˆé‡‘å±‚é”™èƒ½çš„çŸ¥è¯†å›¾è°±ï¼Œè®©æ–‡çŒ®æ•°æ®å˜å¾—æœºå™¨å¯è¯»ã€å¯å¤ç”¨ã€‚\n\n---\n\n### ğŸ›¡ï¸ å…¶ä»–å€¼å¾—å…³æ³¨çš„è®ºæ–‡\n*   **[Medical] Knowing When to Abstain: Medical LLMs Under Clinical Uncertainty**: åŒ»ç–— LLM æœ€å¤§çš„ç¾å¾·æ˜¯â€œçŸ¥ä¹‹ä¸ºçŸ¥ä¹‹ï¼Œä¸çŸ¥ä¸ºä¸çŸ¥â€ã€‚æœ¬æ–‡æå‡ºäº† **MedAbstain** åŸºå‡†ï¼Œå‘ç°å³ä½¿æ˜¯ SOTA æ¨¡å‹ä¹Ÿå¾€å¾€ä¸çŸ¥é“ä½•æ—¶è¯¥æ‹’ç»å›ç­”ã€‚\n*   **[Security] Multimodal Generative Engine Optimization: Rank Manipulation for Vision-Language Model Rankers**: **MGEO** æ”»å‡»ã€‚é’ˆå¯¹åŸºäº VLM çš„æœç´¢æ’åºç³»ç»Ÿï¼Œé€šè¿‡ä¼˜åŒ–å›¾åƒæ‰°åŠ¨å’Œæ–‡æœ¬åç¼€ï¼Œæ¶æ„æå‡å•†å“æ’åã€‚è¿™å®é™…ä¸Šæ˜¯é’ˆå¯¹ AI æœç´¢å¼•æ“çš„ SEO æ”»å‡»ã€‚\n*   **[Audio] AQUA-Bench: Beyond Finding Answers to Knowing When There Are None in Audio Question Answering**: éŸ³é¢‘é—®ç­”ä¸ä»…è¦å›ç­”é—®é¢˜ï¼Œè¿˜è¦èƒ½æ£€æµ‹â€œæ— æ³•å›ç­”â€çš„é—®é¢˜ï¼ˆå¦‚é—®é¢˜ä¸éŸ³é¢‘æ— å…³ï¼‰ã€‚\n*   **[Accessibility] Creating Disability Story Videos with Generative AI**: æ¢è®¨æ®‹éšœäººå£«å¦‚ä½•åˆ©ç”¨ GenAI åˆ›ä½œæ•…äº‹ï¼ŒAI æ—¢èƒ½é™ä½é—¨æ§›ï¼Œä¹Ÿå¯èƒ½å¼•å…¥åè§ï¼Œéœ€è¦æ›´å¥½çš„è®¾è®¡æ¥æ”¯æŒâ€œéå…¸å‹â€çš„è¡¨è¾¾ã€‚\n\nå¸Œæœ›ä»Šå¤©çš„å¿«æŠ¥èƒ½ä¸ºä½ å¸¦æ¥æ–°çš„å¯å‘ï¼æˆ‘ä»¬æ˜å¤©è§ã€‚",
  "papers": [
    {
      "arxiv_id": "2601.12617v1",
      "title": "Creating Disability Story Videos with Generative AI: Motivation, Expression, and Sharing",
      "title_zh": "åˆ©ç”¨ç”Ÿæˆå¼ AI åˆ›ä½œæ®‹éšœæ•…äº‹è§†é¢‘ï¼šåŠ¨æœºã€è¡¨è¾¾ä¸åˆ†äº«",
      "authors": [
        "Shuo Niu",
        "Dylan Clements",
        "Hyungsin Kim"
      ],
      "abstract": "Generative AI (GenAI) is both promising and challenging in supporting people with disabilities (PwDs) in creating stories about disability. GenAI can reduce barriers to media production and inspire the creativity of PwDs, but it may also introduce biases and imperfections that hinder its adoption for personal expression. In this research, we examine how nine PwD from a disability advocacy group used GenAI to create videos sharing their disability experiences. Grounded in digital storytelling theory, we explore the motivations, expression, and sharing of PwD-created GenAI story videos. We conclude with a framework of momentous depiction, which highlights four core affordances of GenAI that either facilitate or require improvements to better support disability storytelling: non-capturable depiction, identity concealment and representation, contextual realism and consistency, and emotional articulation. Based on this framework, we further discuss design implications for GenAI in relation to story completion, media formats, and corrective mechanisms.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†æ®‹éšœäººå£« (PwD) å¦‚ä½•åˆ©ç”¨ç”Ÿæˆå¼äººå·¥æ™ºèƒ½ (GenAI) åˆ›ä½œåæ˜ å…¶æ®‹éšœç»å†çš„è§†é¢‘æ•…äº‹ï¼Œåˆ†æäº†å…¶ä¸­çš„åˆ›ä½œåŠ¨åŠ›ã€è¡¨è¾¾æ–¹å¼åŠåˆ†äº«è¡Œä¸ºã€‚ç ”ç©¶åŸºäºæ•°å­—å™äº‹ç†è®º (digital storytelling theory)ï¼Œé€šè¿‡å¯¹ä¹åæ®‹éšœäººå£«çš„å®è¯ç ”ç©¶ï¼Œæå‡ºäº†ä¸€ä¸ªâ€œé‡å¤§æç»˜â€ (momentous depiction) æ¡†æ¶ã€‚è¯¥æ¡†æ¶æ­ç¤ºäº† GenAI åœ¨æ”¯æŒæ®‹éšœå™äº‹ä¸­çš„å››é¡¹æ ¸å¿ƒåŠŸèƒ½ (affordances)ï¼šä¸å¯æ•æ‰çš„æç»˜ (non-capturable depiction)ã€èº«ä»½éšè—ä¸å‘ˆç° (identity concealment and representation)ã€æƒ…å¢ƒçœŸå®æ€§ä¸ä¸€è‡´æ€§ (contextual realism and consistency) ä»¥åŠæƒ…æ„Ÿè¡¨è¾¾ (emotional articulation)ã€‚æœ€åï¼Œæ–‡ç« ä¸º GenAI åœ¨æ•…äº‹å®Œæ•´æ€§ã€åª’ä½“æ ¼å¼åŠçº é”™æœºåˆ¶æ–¹é¢çš„è®¾è®¡æ”¹è¿›æä¾›äº†é‡è¦è§è§£ã€‚",
      "categories": [
        "cs.HC",
        "cs.AI"
      ],
      "primary_category": "cs.HC",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2601.12617v1",
      "published_date": "2026-01-18 23:18:34 UTC",
      "updated_date": "2026-01-18 23:18:34 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-22T23:28:44.488238+00:00"
    },
    {
      "arxiv_id": "2601.12607v1",
      "title": "A Cloud-based Multi-Agentic Workflow for Science",
      "title_zh": "é¢å‘ç§‘å­¦ç ”ç©¶çš„äº‘ç«¯å¤šæ™ºèƒ½ä½“å·¥ä½œæµ",
      "authors": [
        "Anurag Acharya",
        "Timothy Vega",
        "Rizwan A. Ashraf",
        "Anshu Sharma",
        "Derek Parker",
        "Robert Rallo"
      ],
      "abstract": "As Large Language Models (LLMs) become ubiquitous across various scientific domains, their lack of ability to perform complex tasks like running simulations or to make complex decisions limits their utility. LLM-based agents bridge this gap due to their ability to call external resources and tools and thus are now rapidly gaining popularity. However, coming up with a workflow that can balance the models, cloud providers, and external resources is very challenging, making implementing an agentic system more of a hindrance than a help. In this work, we present a domain-agnostic, model-independent workflow for an agentic framework that can act as a scientific assistant while being run entirely on cloud. Built with a supervisor agent marshaling an array of agents with individual capabilities, our framework brings together straightforward tasks like literature review and data analysis with more complex ones like simulation runs. We describe the framework here in full, including a proof-of-concept system we built to accelerate the study of Catalysts, which is highly important in the field of Chemistry and Material Science. We report the cost to operate and use this framework, including the breakdown of the cost by services use. We also evaluate our system on a custom-curated synthetic benchmark and a popular Chemistry benchmark, and also perform expert validation of the system. The results show that our system is able to route the task to the correct agent 90% of the time and successfully complete the assigned task 97.5% of the time for the synthetic tasks and 91% of the time for real-world tasks, while still achieving better or comparable accuracy to most frontier models, showing that this is a viable framework for other scientific domains to replicate.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§äº‘åŸç”Ÿã€è·¨é¢†åŸŸä¸”ç‹¬ç«‹äºæ¨¡å‹çš„ Multi-Agentic Workflow æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³å¤§å‹è¯­è¨€æ¨¡å‹ (LLMs) åœ¨æ‰§è¡Œå¤æ‚ç§‘å­¦ä»»åŠ¡ï¼ˆå¦‚æ¨¡æ‹Ÿå’Œå†³ç­–ï¼‰æ–¹é¢çš„å±€é™æ€§ã€‚è¯¥æ¡†æ¶é€šè¿‡ä¸€ä¸ª Supervisor Agent ç»Ÿç­¹è°ƒåº¦å…·å¤‡ä¸åŒä¸“ä¸šèƒ½åŠ›çš„æ™ºèƒ½ä½“é˜µåˆ—ï¼Œå®ç°äº†ä»åŸºç¡€æ–‡çŒ®ç»¼è¿°åˆ°å¤æ‚ä»¿çœŸæ¨¡æ‹Ÿ (Simulation Runs) çš„å…¨æµç¨‹è‡ªåŠ¨åŒ–ã€‚ä½œè€…é€šè¿‡å‚¬åŒ–å‰‚ (Catalysts) ç ”ç©¶çš„ç³»ç»ŸåŸå‹éªŒè¯äº†è¯¥æ–¹æ¡ˆï¼Œå¹¶è¯¦ç»†è¯„ä¼°äº†å…¶åœ¨äº‘ç«¯è¿è¡Œçš„å„é¡¹æœåŠ¡æˆæœ¬ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥ç³»ç»Ÿåœ¨ä»»åŠ¡è·¯ç”±å‡†ç¡®ç‡ä¸Šè¾¾åˆ° 90%ï¼Œåœ¨çœŸå®ç§‘å­¦ä»»åŠ¡ä¸­çš„æˆåŠŸç‡é«˜è¾¾ 91%ï¼Œæ€§èƒ½è¡¨ç°ä¼˜äºæˆ–åª²ç¾ç°æœ‰çš„ Frontier Modelsï¼Œä¸ºç§‘å­¦ç ”ç©¶çš„è‡ªåŠ¨åŒ–æä¾›äº†ä¸€ç§å¯å¤åˆ¶çš„èŒƒä¾‹ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2601.12607v1",
      "published_date": "2026-01-18 22:37:09 UTC",
      "updated_date": "2026-01-18 22:37:09 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-22T23:28:45.799404+00:00"
    },
    {
      "arxiv_id": "2601.12594v1",
      "title": "SLAP: Scalable Language-Audio Pretraining with Variable-Duration Audio and Multi-Objective Training",
      "title_zh": "SLAPï¼šæ”¯æŒå˜é•¿éŸ³é¢‘ä¸å¤šç›®æ ‡è®­ç»ƒçš„å¯æ‰©å±•è¯­è¨€-éŸ³é¢‘é¢„è®­ç»ƒ",
      "authors": [
        "Xinhao Mei",
        "Gael Le Lan",
        "Haohe Liu",
        "Zhaoheng Ni",
        "Varun Nagaraja",
        "Yang Liu",
        "Yangyang Shi",
        "Vikas Chandra"
      ],
      "abstract": "Contrastive language-audio pretraining (CLAP) has achieved notable success in learning semantically rich audio representations and is widely adopted for various audio-related tasks. However, current CLAP models face several key limitations. First, they are typically trained on relatively small datasets, often comprising a few million audio samples. Second, existing CLAP models are restricted to short and fixed duration, which constrains their usage in real-world scenarios with variable-duration audio. Third, the standard contrastive training objective operates on global representations, which may hinder the learning of dense, fine-grained audio features. To address these challenges, we introduce Scalable Language-Audio Pretraining (SLAP), which scales language-audio pretraining to 109 million audio-text pairs with variable audio durations and incorporates multiple training objectives. SLAP unifies contrastive loss with additional self-supervised and captioning losses in a single-stage training, facilitating the learning of richer dense audio representations. The proposed SLAP model achieves new state-of-the-art performance on audio-text retrieval and zero-shot audio classification tasks, demonstrating its effectiveness across diverse benchmarks.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† SLAP (Scalable Language-Audio Pretraining)ï¼Œæ—¨åœ¨è§£å†³ç°æœ‰ CLAP æ¨¡å‹åœ¨æ•°æ®é›†è§„æ¨¡ã€å›ºå®šéŸ³é¢‘é•¿åº¦ä»¥åŠç»†ç²’åº¦ç‰¹å¾æå–æ–¹é¢çš„å±€é™æ€§ã€‚SLAP å°†é¢„è®­ç»ƒè§„æ¨¡æ‰©å±•è‡³ 1.09 äº¿ä¸ªéŸ³é¢‘-æ–‡æœ¬å¯¹ï¼Œæ”¯æŒå¤„ç†å˜é•¿ (variable-duration) éŸ³é¢‘ï¼Œæ˜¾è‘—æå‡äº†æ¨¡å‹åœ¨ç°å®åœºæ™¯ä¸­çš„çµæ´»æ€§ã€‚é€šè¿‡åœ¨å•é˜¶æ®µè®­ç»ƒä¸­æ•´åˆå¯¹æ¯”æŸå¤± (contrastive loss)ã€è‡ªç›‘ç£å­¦ä¹ å’Œæ ‡é¢˜ç”Ÿæˆ (captioning) ç­‰å¤šç›®æ ‡ä»»åŠ¡ï¼ŒSLAP èƒ½å¤Ÿå­¦ä¹ åˆ°æ›´ä¸°å¯Œçš„å¯†é›†éŸ³é¢‘è¡¨å¾ã€‚å®éªŒè¯æ˜ï¼Œè¯¥æ¨¡å‹åœ¨éŸ³é¢‘-æ–‡æœ¬æ£€ç´¢å’Œé›¶æ ·æœ¬ (zero-shot) éŸ³é¢‘åˆ†ç±»ä»»åŠ¡ä¸Šå‡å–å¾—äº†æ–°çš„ SOTA æ€§èƒ½ã€‚",
      "categories": [
        "eess.AS",
        "cs.AI",
        "cs.SD"
      ],
      "primary_category": "eess.AS",
      "comment": "Accepted to ICASSP 2026",
      "pdf_url": "https://arxiv.org/pdf/2601.12594v1",
      "published_date": "2026-01-18 21:36:19 UTC",
      "updated_date": "2026-01-18 21:36:19 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-22T23:28:50.434055+00:00"
    },
    {
      "arxiv_id": "2601.12585v1",
      "title": "Do MLLMs See What We See? Analyzing Visualization Literacy Barriers in AI Systems",
      "title_zh": "MLLM æ˜¯å¦æ‰€è§å¦‚äººæ‰€è§ï¼Ÿåˆ†æ AI ç³»ç»Ÿä¸­çš„è§†è§‰ç´ å…»éšœç¢",
      "authors": [
        "Mengli",
        "Duan",
        "Yuhe",
        "Jiang",
        "Matthew Varona",
        "Carolina Nobre"
      ],
      "abstract": "Multimodal Large Language Models (MLLMs) are increasingly used to interpret visualizations, yet little is known about why they fail. We present the first systematic analysis of barriers to visualization literacy in MLLMs. Using the regenerated Visualization Literacy Assessment Test (reVLAT) benchmark with synthetic data, we open-coded 309 erroneous responses from four state-of-the-art models with a barrier-centric strategy adapted from human visualization literacy research. Our analysis yields a taxonomy of MLLM failures, revealing two machine-specific barriers that extend prior human-participation frameworks. Results show that models perform well on simple charts but struggle with color-intensive, segment-based visualizations, often failing to form consistent comparative reasoning. Our findings inform future evaluation and design of reliable AI-driven visualization assistants.",
      "tldr_zh": "è¯¥ç ”ç©¶å¯¹å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ (MLLMs) åœ¨ç†è§£å¯è§†åŒ–å›¾è¡¨æ—¶é‡åˆ°çš„éšœç¢è¿›è¡Œäº†é¦–æ¬¡ç³»ç»Ÿæ€§åˆ†æã€‚ç ”ç©¶äººå‘˜åˆ©ç”¨ reVLAT åŸºå‡†æµ‹è¯•ï¼Œå¯¹å››ç§æœ€å…ˆè¿›æ¨¡å‹äº§ç”Ÿçš„ 309 ä¸ªé”™è¯¯å“åº”è¿›è¡Œäº†åŸºäºéšœç¢ä¸­å¿ƒç­–ç•¥ (barrier-centric strategy) çš„å¼€æ”¾ç¼–ç åˆ†æã€‚è¯¥ç ”ç©¶æ„å»ºäº† MLLM å¤±è´¥ä»»åŠ¡çš„åˆ†ç±»æ³• (taxonomy)ï¼Œå¹¶å‘ç°äº†ä¸¤ç§è¶…è¶Šç°æœ‰æ¨¡å‹è®¤çŸ¥æ¡†æ¶çš„æœºå™¨ç‰¹æœ‰éšœç¢ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œæ¨¡å‹åœ¨ç®€å•å›¾è¡¨ä¸Šè¡¨ç°è‰¯å¥½ï¼Œä½†åœ¨å¤„ç†é«˜è‰²å½©å¯†åº¦å’Œåˆ†æ®µå¼å¯è§†åŒ–æ—¶è¡¨ç°ä¸ä½³ï¼Œä¸”å¾€å¾€éš¾ä»¥å½¢æˆè¿è´¯çš„æ¯”è¾ƒæ¨ç† (comparative reasoning)ã€‚è¯¥é¡¹å·¥ä½œä¸ºæœªæ¥å¼€å‘å’Œè¯„ä¼°æ›´å¯é çš„ AI å¯è§†åŒ–åŠ©æ‰‹æä¾›äº†é‡è¦ä¾æ®ã€‚",
      "categories": [
        "cs.HC",
        "cs.AI",
        "cs.ET"
      ],
      "primary_category": "cs.HC",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2601.12585v1",
      "published_date": "2026-01-18 21:08:23 UTC",
      "updated_date": "2026-01-18 21:08:23 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-22T23:28:51.291510+00:00"
    },
    {
      "arxiv_id": "2601.12582v1",
      "title": "Ontology-aligned structuring and reuse of multimodal materials data and workflows towards automatic reproduction",
      "title_zh": "é¢å‘è‡ªåŠ¨å¤ç°çš„æœ¬ä½“å¯¹é½å¤šæ¨¡æ€ææ–™æ•°æ®ä¸å·¥ä½œæµçš„ç»“æ„åŒ–ä¸å¤ç”¨",
      "authors": [
        "Sepideh Baghaee Ravari",
        "Abril Azocar Guzman",
        "Sarath Menon",
        "Stefan Sandfeld",
        "Tilmann Hickel",
        "Markus Stricker"
      ],
      "abstract": "Reproducibility of computational results remains a challenge in materials science, as simulation workflows and parameters are often reported only in unstructured text and tables. While literature data are valuable for validation and reuse, the lack of machine-readable workflow descriptions prevents large-scale curation and systematic comparison. Existing text-mining approaches are insufficient to extract complete computational workflows with their associated parameters. An ontology-driven, large language model (LLM)-assisted framework is introduced for the automated extraction and structuring of computational workflows from the literature. The approach focuses on density functional theory-based stacking fault energy (SFE) calculations in hexagonal close-packed magnesium and its binary alloys, and uses a multi-stage filtering strategy together with prompt-engineered LLM extraction applied to method sections and tables. Extracted information is unified into a canonical schema and aligned with established materials ontologies (CMSO, ASMO, and PLDO), enabling the construction of a knowledge graph using atomRDF. The resulting knowledge graph enables systematic comparison of reported SFE values and supports the structured reuse of computational protocols. While full computational reproducibility is still constrained by missing or implicit metadata, the framework provides a foundation for organizing and contextualizing published results in a semantically interoperable form, thereby improving transparency and reusability of computational materials data.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§åŸºäºæœ¬ä½“é©±åŠ¨(Ontology-driven)ä¸”ç”±å¤§è¯­è¨€æ¨¡å‹(LLM)è¾…åŠ©çš„æ¡†æ¶ï¼Œæ—¨åœ¨ä»å­¦æœ¯æ–‡çŒ®ä¸­è‡ªåŠ¨æå–å¹¶ç»“æ„åŒ–è®¡ç®—ææ–™å­¦çš„å·¥ä½œæµ(Workflows)ã€‚è¯¥æ–¹æ³•é‡‡ç”¨å¤šé˜¶æ®µè¿‡æ»¤ç­–ç•¥ä¸æç¤ºå·¥ç¨‹(Prompt engineering)ï¼Œä»è®ºæ–‡çš„æ–¹æ³•ç« èŠ‚å’Œè¡¨æ ¼ä¸­è·å–ä¿¡æ¯ï¼Œå¹¶å°†å…¶ç»Ÿä¸€åˆ°è§„èŒƒæ¨¡å¼ä¸­ï¼Œä¸ CMSOã€ASMO å’Œ PLDO ç­‰ç°æœ‰ææ–™æœ¬ä½“å¯¹é½ã€‚é€šè¿‡åˆ©ç”¨ atomRDF æ„å»ºçŸ¥è¯†å›¾è°±(Knowledge graph)ï¼Œç ”ç©¶å±•ç¤ºäº†å¯¹é•åˆé‡‘å¯†åº¦æ³›å‡½ç†è®º(DFT)è®¡ç®—ä¸­å †å›å±‚é”™èƒ½(SFE)æ•°æ®çš„ç³»ç»ŸåŒ–æ¯”è¾ƒä¸åè®®é‡ç”¨ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ¡†æ¶ä¸ºå®ç°è®¡ç®—ææ–™æ•°æ®çš„è¯­ä¹‰äº’æ“ä½œæ€§å¥ å®šäº†åŸºç¡€ï¼Œæ˜¾è‘—æå‡äº†æ¨¡æ‹Ÿç»“æœçš„é€æ˜åº¦ä¸å¯é‡ç”¨æ€§ã€‚",
      "categories": [
        "cond-mat.mtrl-sci",
        "cs.AI"
      ],
      "primary_category": "cond-mat.mtrl-sci",
      "comment": "39 pages, 7 figures",
      "pdf_url": "https://arxiv.org/pdf/2601.12582v1",
      "published_date": "2026-01-18 20:51:23 UTC",
      "updated_date": "2026-01-18 20:51:23 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-22T23:28:59.601660+00:00"
    },
    {
      "arxiv_id": "2601.12577v1",
      "title": "Primate-like perceptual decision making emerges through deep recurrent reinforcement learning",
      "title_zh": "ç±»çµé•¿ç±»çŸ¥è§‰å†³ç­–åœ¨æ·±åº¦å¾ªç¯å¼ºåŒ–å­¦ä¹ ä¸­è‡ªå‘æ¶Œç°",
      "authors": [
        "Nathan J. Wispinski",
        "Scott A. Stone",
        "Anthony Singhal",
        "Patrick M. Pilarski",
        "Craig S. Chapman"
      ],
      "abstract": "Progress has led to a detailed understanding of the neural mechanisms that underlie decision making in primates. However, less is known about why such mechanisms are present in the first place. Theory suggests that primate decision making mechanisms, and their resultant behavioral abilities, emerged to maximize reward in the face of noisy, temporally evolving information. To test this theory, we trained an end-to-end deep recurrent neural network using reinforcement learning on a noisy perceptual discrimination task. Networks learned several key abilities of primate-like decision making including trading off speed for accuracy, and flexibly changing their mind in the face of new information. Internal dynamics of these networks suggest that these abilities were supported by similar decision mechanisms as those observed in primate neurophysiological studies. These results provide experimental support for key pressures that gave rise to the primate ability to make flexible decisions.",
      "tldr_zh": "è¯¥ç ”ç©¶åˆ©ç”¨æ·±åº¦å¾ªç¯å¼ºåŒ–å­¦ä¹ (deep recurrent reinforcement learning)åœ¨å™ªå£°æ„ŸçŸ¥è¾¨åˆ«ä»»åŠ¡ä¸­è®­ç»ƒäº†ä¸€ä¸ªç«¯åˆ°ç«¯çš„å¾ªç¯ç¥ç»ç½‘ç»œï¼Œæ—¨åœ¨æ¢ç©¶çµé•¿ç±»æ„ŸçŸ¥å†³ç­–æœºåˆ¶çš„èµ·æºã€‚å®éªŒè¡¨æ˜ï¼Œç¥ç»ç½‘ç»œæˆåŠŸå­¦ä¹ åˆ°äº†çµé•¿ç±»å†³ç­–çš„æ ¸å¿ƒèƒ½åŠ›ï¼ŒåŒ…æ‹¬åœ¨é€Ÿåº¦ä¸å‡†ç¡®ç‡ä¹‹é—´è¿›è¡Œæƒè¡¡(speed-accuracy tradeoff)ï¼Œä»¥åŠæ ¹æ®æ–°ä¿¡æ¯çµæ´»æ”¹å˜ä¸»æ„(changing their mind)ã€‚ç½‘ç»œå†…éƒ¨åŠ¨åŠ›å­¦(internal dynamics)åˆ†ææ˜¾ç¤ºï¼Œå…¶å†³ç­–æœºåˆ¶ä¸çµé•¿ç±»ç¥ç»ç”Ÿç†å­¦ç ”ç©¶ä¸­è§‚å¯Ÿåˆ°çš„ç°è±¡é«˜åº¦ä¸€è‡´ã€‚è¿™äº›ç»“æœä¸ºçµé•¿ç±»åŠ¨ç‰©åœ¨é¢å¯¹å˜ˆæ‚ã€éšæ—¶é—´æ¼”å˜çš„ä¿¡æ¯æ—¶ï¼Œä¸ºäº†å®ç°å¥–åŠ±æœ€å¤§åŒ–è€Œè¿›åŒ–å‡ºçµæ´»å†³ç­–èƒ½åŠ›çš„ç†è®ºæä¾›äº†å®éªŒæ”¯æŒã€‚",
      "categories": [
        "q-bio.NC",
        "cs.AI"
      ],
      "primary_category": "q-bio.NC",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2601.12577v1",
      "published_date": "2026-01-18 20:43:53 UTC",
      "updated_date": "2026-01-18 20:43:53 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-22T23:29:00.009480+00:00"
    },
    {
      "arxiv_id": "2601.12560v1",
      "title": "Agentic Artificial Intelligence (AI): Architectures, Taxonomies, and Evaluation of Large Language Model Agents",
      "title_zh": "ä»£ç†å¼äººå·¥æ™ºèƒ½ (AI)ï¼šå¤§è¯­è¨€æ¨¡å‹æ™ºèƒ½ä½“çš„æ¶æ„ã€åˆ†ç±»ä½“ç³»ä¸è¯„ä¼°",
      "authors": [
        "Arunkumar V",
        "Gangadharan G. R.",
        "Rajkumar Buyya"
      ],
      "abstract": "Artificial Intelligence is moving from models that only generate text to Agentic AI, where systems behave as autonomous entities that can perceive, reason, plan, and act. Large Language Models (LLMs) are no longer used only as passive knowledge engines but as cognitive controllers that combine memory, tool use, and feedback from their environment to pursue extended goals. This shift already supports the automation of complex workflows in software engineering, scientific discovery, and web navigation, yet the variety of emerging designs, from simple single loop agents to hierarchical multi agent systems, makes the landscape hard to navigate. In this paper, we investigate architectures and propose a unified taxonomy that breaks agents into Perception, Brain, Planning, Action, Tool Use, and Collaboration. We use this lens to describe the move from linear reasoning procedures to native inference time reasoning models, and the transition from fixed API calls to open standards like the Model Context Protocol (MCP) and Native Computer Use. We also group the environments in which these agents operate, including digital operating systems, embodied robotics, and other specialized domains, and we review current evaluation practices. Finally, we highlight open challenges, such as hallucination in action, infinite loops, and prompt injection, and outline future research directions toward more robust and reliable autonomous systems.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†äººå·¥æ™ºèƒ½ä»ç”Ÿæˆå¼æ¨¡å‹å‘Agentic AIï¼ˆæ™ºèƒ½ä½“äººå·¥æ™ºèƒ½ï¼‰çš„è½¬å˜ï¼Œå¼ºè°ƒLarge Language Models (LLMs)ä½œä¸ºå…·å¤‡æ„ŸçŸ¥ã€æ¨ç†ã€è§„åˆ’å’Œè¡ŒåŠ¨èƒ½åŠ›çš„è®¤çŸ¥æ§åˆ¶å™¨çš„æ ¸å¿ƒä½œç”¨ã€‚æ–‡ç« æå‡ºäº†ä¸€ä¸ªç»Ÿä¸€çš„åˆ†ç±»ä½“ç³»(Taxonomy)ï¼Œå°†æ™ºèƒ½ä½“æ‹†è§£ä¸ºPerception, Brain, Planning, Action, Tool Useå’ŒCollaborationå…­å¤§æ ¸å¿ƒæ¨¡å—ã€‚è®ºæ–‡æ·±å…¥åˆ†æäº†æ¶æ„æ¼”è¿›ï¼ŒåŒ…æ‹¬ä»çº¿æ€§æ¨ç†å‘Native inference-time reasoningçš„è½¬å˜ï¼Œä»¥åŠModel Context Protocol (MCP)å’ŒNative Computer Useç­‰å¼€æ”¾æ ‡å‡†çš„åº”ç”¨ã€‚æ­¤å¤–ï¼Œç ”ç©¶è¿˜ç»¼è¿°äº†ä¸åŒé¢†åŸŸçš„è¿è¡Œç¯å¢ƒä¸è¯„ä¼°å®è·µï¼Œå¹¶æŒ‡å‡ºäº†å¹»è§‰ã€æ— é™å¾ªç¯å’ŒPrompt injectionç­‰æœªæ¥äºŸå¾…è§£å†³çš„å®‰å…¨æ€§ä¸é²æ£’æ€§æŒ‘æˆ˜ã€‚",
      "categories": [
        "cs.AI",
        "cs.MA"
      ],
      "primary_category": "cs.AI",
      "comment": "28 pages, 4 figures, 5 tables",
      "pdf_url": "https://arxiv.org/pdf/2601.12560v1",
      "published_date": "2026-01-18 19:51:16 UTC",
      "updated_date": "2026-01-18 19:51:16 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-22T23:29:01.808582+00:00"
    },
    {
      "arxiv_id": "2601.12557v1",
      "title": "Life, Machine Learning, and the Search for Habitability: Predicting Biosignature Fluxes for the Habitable Worlds Observatory",
      "title_zh": "ç”Ÿå‘½ã€æœºå™¨å­¦ä¹ ä¸å®œå±…æ€§æ¢ç´¢ï¼šé¢å‘ Habitable Worlds Observatory çš„ç”Ÿç‰©æ ‡å¿—ç‰©é€šé‡é¢„æµ‹",
      "authors": [
        "Mark Moussa",
        "Amber V. Young",
        "Brianna Isola",
        "Vasuda Trehan",
        "Michael D. Himes",
        "Nicholas Wogan",
        "Giada Arney"
      ],
      "abstract": "Future direct-imaging flagship missions, such as NASA's Habitable Worlds Observatory (HWO), face critical decisions in prioritizing observations due to extremely stringent time and resource constraints. In this paper, we introduce two advanced machine-learning architectures tailored for predicting biosignature species fluxes from exoplanetary reflected-light spectra: a Bayesian Convolutional Neural Network (BCNN) and our novel model architecture, the Spectral Query Adaptive Transformer (SQuAT). The BCNN robustly quantifies both epistemic and aleatoric uncertainties, offering reliable predictions under diverse observational conditions, whereas SQuAT employs query-driven attention mechanisms to enhance interpretability by explicitly associating spectral features with specific biosignature species. We demonstrate that both models achieve comparably high predictive accuracy on an augmented dataset spanning a wide range of exoplanetary conditions, while highlighting their distinct advantages in uncertainty quantification and spectral interpretability. These capabilities position our methods as promising tools for accelerating target triage, optimizing observation schedules, and maximizing scientific return for upcoming flagship missions such as HWO.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ NASA çš„ Habitable Worlds Observatory (HWO) ç­‰æœªæ¥ç›´æ¥æˆåƒæ——èˆ°ä»»åŠ¡é¢ä¸´çš„è§‚æµ‹ä¼˜å…ˆæ’åºéš¾é¢˜ï¼Œæå‡ºäº†ä¸¤ç§å…ˆè¿›çš„æœºå™¨å­¦ä¹ æ¶æ„ç”¨äºä»ç³»å¤–è¡Œæ˜Ÿåå°„å…‰è°±ä¸­é¢„æµ‹ç”Ÿç‰©ç‰¹å¾ç‰©ç§é€šé‡ (biosignature species fluxes)ã€‚ç ”ç©¶å¼•å…¥äº†èƒ½å¤Ÿç¨³å¥é‡åŒ–è®¤çŸ¥ä¸å¶ç„¶ä¸ç¡®å®šæ€§ (uncertainties) çš„è´å¶æ–¯å·ç§¯ç¥ç»ç½‘ç»œ (BCNN)ï¼Œä»¥åŠä¸€ç§æ–°å‹æ¶æ„â€”â€”å…‰è°±æŸ¥è¯¢è‡ªé€‚åº” Transformer (SQuAT)ï¼Œåè€…é€šè¿‡æŸ¥è¯¢é©±åŠ¨çš„æ³¨æ„åŠ›æœºåˆ¶æ˜¾è‘—æå‡äº†å…‰è°±ç‰¹å¾ä¸ç”Ÿç‰©ç‰¹å¾ç‰©ç§å…³è”çš„å¯è§£é‡Šæ€§ (interpretability)ã€‚å®éªŒè¯æ˜ï¼Œä¸¤ç§æ¨¡å‹åœ¨å¹¿æ³›çš„ç³»å¤–è¡Œæ˜Ÿæ¡ä»¶ä¸‹å‡å®ç°äº†æé«˜çš„é¢„æµ‹ç²¾åº¦ã€‚è¿™äº›æ–¹æ³•ä¸ºæœªæ¥æ——èˆ°ä»»åŠ¡çš„ç›®æ ‡ç­›é€‰ã€è§‚æµ‹è®¡åˆ’ä¼˜åŒ–å’Œç§‘å­¦äº§å‡ºæœ€å¤§åŒ–æä¾›äº†å¼ºæœ‰åŠ›çš„æ”¯æŒå·¥å…·ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.LG",
      "comment": "8 pages, 4 figures. Submitted and accepted in AAAI-26 (IAAI Emerging Applications track)",
      "pdf_url": "https://arxiv.org/pdf/2601.12557v1",
      "published_date": "2026-01-18 19:43:48 UTC",
      "updated_date": "2026-01-18 19:43:48 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-22T23:29:03.789977+00:00"
    },
    {
      "arxiv_id": "2601.12554v1",
      "title": "Artificial Intelligence in Materials Science and Engineering: Current Landscape, Key Challenges, and Future Trajectorie",
      "title_zh": "äººå·¥æ™ºèƒ½åœ¨ææ–™ç§‘å­¦ä¸å·¥ç¨‹ä¸­çš„åº”ç”¨ï¼šç°çŠ¶ã€æ ¸å¿ƒæŒ‘æˆ˜ä¸æœªæ¥è¶‹åŠ¿",
      "authors": [
        "Iman Peivaste",
        "Salim Belouettar",
        "Francesco Mercuri",
        "Nicholas Fantuzzi",
        "Hamidreza Dehghani",
        "Razieh Izadi",
        "Halliru Ibrahim",
        "Jakub Lengiewicz",
        "MaÃ«l Belouettar-Mathis",
        "Kouider Bendine",
        "Ahmed Makradi",
        "Martin HÃ¶rsch",
        "Peter Klein",
        "Mohamed El Hachemi",
        "Heinz A. Preisig",
        "Yacine Rezgui",
        "Natalia Konchakova",
        "Ali Daouadji"
      ],
      "abstract": "Artificial Intelligence is rapidly transforming materials science and engineering, offering powerful tools to navigate complexity, accelerate discovery, and optimize material design in ways previously unattainable. Driven by the accelerating pace of algorithmic advancements and increasing data availability, AI is becoming an essential competency for materials researchers. This review provides a comprehensive and structured overview of the current landscape, synthesizing recent advancements and methodologies for materials scientists seeking to effectively leverage these data-driven techniques. We survey the spectrum of machine learning approaches, from traditional algorithms to advanced deep learning architectures, including CNNs, GNNs, and Transformers, alongside emerging generative AI and probabilistic models such as Gaussian Processes for uncertainty quantification. The review also examines the pivotal role of data in this field, emphasizing how effective representation and featurization strategies, spanning compositional, structural, image-based, and language-inspired approaches, combined with appropriate preprocessing, fundamentally underpin the performance of machine learning models in materials research. Persistent challenges related to data quality, quantity, and standardization, which critically impact model development and application in materials science and engineering, are also addressed.",
      "tldr_zh": "è¯¥ç»¼è¿°å…¨é¢æ¢³ç†äº†äººå·¥æ™ºèƒ½(Artificial Intelligence)åœ¨ææ–™ç§‘å­¦ä¸å·¥ç¨‹é¢†åŸŸçš„ç°çŠ¶ã€æŒ‘æˆ˜åŠæœªæ¥è¶‹åŠ¿ï¼Œå¼ºè°ƒäº†å…¶åœ¨åŠ é€Ÿææ–™å‘ç°ä¸ä¼˜åŒ–è®¾è®¡ä¸­çš„æ ¸å¿ƒä½œç”¨ã€‚æ–‡ç« ç³»ç»Ÿè°ƒç ”äº†ä»ä¼ ç»Ÿæœºå™¨å­¦ä¹ (Machine Learning)åˆ°æ·±åº¦å­¦ä¹ (Deep Learning)æ¶æ„ï¼ˆå¦‚CNNsã€GNNså’ŒTransformersï¼‰çš„å¤šç§æ–¹æ³•ï¼Œå¹¶æ¢è®¨äº†ç”Ÿæˆå¼AI(Generative AI)åŠç”¨äºä¸ç¡®å®šæ€§é‡åŒ–çš„é«˜æ–¯è¿‡ç¨‹(Gaussian Processes)ç­‰æ–°å…´æŠ€æœ¯ã€‚ç ”ç©¶é‡ç‚¹åˆ†æäº†æ•°æ®è¡¨ç¤º(Data Representation)å’Œç‰¹å¾åŒ–(Featurization)ç­–ç•¥ï¼ˆæ¶µç›–æˆåˆ†ã€ç»“æ„åŠå›¾åƒç­‰ç»´åº¦ï¼‰å¯¹æ¨¡å‹æ€§èƒ½çš„æ”¯æ’‘ä½œç”¨ã€‚æœ€åï¼Œé’ˆå¯¹æ•°æ®è´¨é‡ã€æ•°é‡åŠæ ‡å‡†åŒ–(Standardization)ç­‰å½±å“æ¨¡å‹å¼€å‘çš„å…³é”®æŒ‘æˆ˜æå‡ºäº†è§è§£ï¼Œä¸ºææ–™ç§‘å­¦å®¶æœ‰æ•ˆåˆ©ç”¨æ•°æ®é©±åŠ¨æŠ€æœ¯æä¾›äº†ç»“æ„åŒ–æŒ‡å—ã€‚",
      "categories": [
        "cond-mat.mtrl-sci",
        "cs.AI",
        "physics.comp-ph",
        "quant-ph"
      ],
      "primary_category": "cond-mat.mtrl-sci",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2601.12554v1",
      "published_date": "2026-01-18 19:36:10 UTC",
      "updated_date": "2026-01-18 19:36:10 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-22T23:29:09.498442+00:00"
    },
    {
      "arxiv_id": "2601.12549v1",
      "title": "Benchmarking Concept-Spilling Across Languages in LLMs",
      "title_zh": "å¤§è¯­è¨€æ¨¡å‹è·¨è¯­è¨€æ¦‚å¿µæº¢å‡ºçš„åŸºå‡†æµ‹è¯•",
      "authors": [
        "Ilia Badanin",
        "Daniil Dzenhaliou",
        "Imanol Schlag"
      ],
      "abstract": "Multilingual Large Language Models (LLMs) exhibit remarkable cross-lingual abilities, yet often exhibit a systematic bias toward the representations from other languages, resulting in semantic interference when generating content in non-English languages$-$a phenomenon we define as language spilling. This paper presents a novel comparative framework for evaluating multilingual semantic robustness by systematically measuring how models handle polysemous words across languages. Our methodology provides a relative measure of model performance: when required to generate exactly five meanings, both strong and weak models may resort to meanings from dominant languages, but semantically stronger models do so later in the generation sequence, producing more true meanings from the target language before failing, while weaker models resort to dominant-language meanings earlier in the sequence. We evaluate a diverse set of open and closed multilingual LLMs using a structured meaning generation task across nine languages, employing a carefully curated benchmark of 100 high-polysemy English words. Our findings reveal significant variation in semantic robustness across both models and languages, providing a principled ranking system for model comparison without requiring definitive causal attribution of error sources. We contribute both a scalable comparative benchmark for multilingual semantic evaluation and a rigorous validation pipeline$-$critical tools for developing more linguistically balanced AI systems.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†å¤šè¯­è¨€å¤§è¯­è¨€æ¨¡å‹ (LLMs) ä¸­çš„ language spilling ç°è±¡ï¼Œå³æ¨¡å‹åœ¨ç”Ÿæˆéè‹±è¯­å†…å®¹æ—¶å—åˆ°ä¸»å¯¼è¯­è¨€è¯­ä¹‰å¹²æ‰°çš„é—®é¢˜ã€‚ä½œè€…æå‡ºäº†ä¸€ç§è¯„ä¼°å¤šè¯­è¨€è¯­ä¹‰é²æ£’æ€§çš„æ¯”è¾ƒæ¡†æ¶ï¼Œé€šè¿‡æµ‹é‡æ¨¡å‹åœ¨ä¸åŒè¯­è¨€ä¸­å¤„ç†å¤šä¹‰è¯ (polysemous words) çš„èƒ½åŠ›ï¼Œé‡åŒ–ä¸»å¯¼è¯­è¨€æ¦‚å¿µçš„æº¢å‡ºç¨‹åº¦ã€‚ç ”ç©¶åˆ©ç”¨åŒ…å« 100 ä¸ªé«˜å¤šä¹‰æ€§è¯é¡¹çš„åŸºå‡†æµ‹è¯•ï¼Œåœ¨ 9 ç§è¯­è¨€ä¸‹å¯¹å¤šç§å¼€æºå’Œé—­æºæ¨¡å‹è¿›è¡Œäº†è¯„ä¼°ã€‚å®éªŒå‘ç°ï¼Œè¯­ä¹‰é²æ£’æ€§å¼ºçš„æ¨¡å‹èƒ½äº§ç”Ÿæ›´å¤šç›®æ ‡è¯­è¨€çš„çœŸå®å«ä¹‰ï¼Œè€Œè¾ƒå¼±çš„æ¨¡å‹ä¼šæ›´æ—©åœ°è½¬å‘ä¸»å¯¼è¯­è¨€çš„è¯­ä¹‰ã€‚è¯¥å·¥ä½œè´¡çŒ®äº†ä¸€ä¸ªå¯æ‰©å±•çš„æ¯”è¾ƒåŸºå‡†å’ŒéªŒè¯æµæ°´çº¿ï¼Œä¸ºå¼€å‘è¯­è¨€æ›´å‡è¡¡çš„ AI ç³»ç»Ÿæä¾›äº†å…³é”®å·¥å…·ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2601.12549v1",
      "published_date": "2026-01-18 19:28:26 UTC",
      "updated_date": "2026-01-18 19:28:26 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-22T23:29:10.971922+00:00"
    },
    {
      "arxiv_id": "2601.12547v1",
      "title": "How Clinicians Think and What AI Can Learn From It",
      "title_zh": "ä¸´åºŠæ€ç»´æ¨¡å¼åŠå…¶å¯¹äººå·¥æ™ºèƒ½çš„å¯ç¤º",
      "authors": [
        "Dipayan Sengupta",
        "Saumya Panda"
      ],
      "abstract": "Most clinical AI systems operate as prediction engines -- producing labels or risk scores -- yet real clinical reasoning is a time-bounded, sequential control problem under uncertainty. Clinicians interleave information gathering with irreversible actions, guided by regret, constraints and patient values. We argue that the dominant computational substrate of clinician reasoning is not cardinal optimization but ordinal, non-compensatory decision-making: Clinicians frequently rely on fast-and-frugal, lexicographic heuristics (e.g., fast-and-frugal trees) that stop early after checking a small, fixed sequence of cues. We provide a normative rationale for why such algorithms are not merely bounded rationality shortcuts, but can be epistemically preferred in medicine. First, many clinical trade-offs are constructed through human judgment and are only weakly measurable on absolute scales; without strong measurement axioms, only orderings are invariant, motivating an ordinal-by-default stance. Second, preference and signal elicitation are structurally crude: The mapping from truth $\\to$ perception $\\to$ inference $\\to$ recorded variables introduces layered noise, leaving a persistent uncertainty floor. When this 'crudeness' overwhelms the decision margin, plug-in expected-utility optimization becomes brittle (high flip probability under small perturbations), whereas robust dominance/filtering rules ($Îµ$-dominance, maximin) stabilize decisions.Finally, we outline a clinician-aligned AI blueprint: Use rich models for beliefs and trajectories, but choose actions through robust ordinal rules; treat heuristics as the low-dimensional special case; and deploy AI as 'selective complexity' -- invoked mainly for tie-breaking when decisions are fragile and information has positive expected impact.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†ä¸´åºŠåŒ»ç”Ÿçš„æ€ç»´æ–¹å¼ä¸å½“å‰ä¸´åºŠ AI ç³»ç»Ÿï¼ˆå¤šä¸ºé¢„æµ‹å¼•æ“ï¼‰ä¹‹é—´çš„è„±èŠ‚ï¼ŒæŒ‡å‡ºçœŸå®çš„ä¸´åºŠæ¨ç†æ˜¯ä¸€ä¸ªåœ¨ä¸ç¡®å®šæ€§ä¸‹çš„å—æ—¶é™çº¦æŸçš„åºåˆ—æ§åˆ¶è¿‡ç¨‹ã€‚ç ”ç©¶å‘ç°ï¼Œä¸´åºŠåŒ»ç”Ÿä¸»è¦ä¾èµ–äºåºæ•°ï¼ˆordinalï¼‰ã€éè¡¥å¿æ€§ï¼ˆnon-compensatoryï¼‰çš„å†³ç­–æ¨¡å¼ï¼Œå¸¸ç”¨ç®€å•é«˜æ•ˆçš„å¯å‘å¼ç®—æ³•ï¼ˆfast-and-frugal heuristicsï¼‰é€šè¿‡å°‘é‡çš„å…³é”®çº¿ç´¢å¿«é€Ÿåšå‡ºåˆ¤æ–­ã€‚ç ”ç©¶è¿›ä¸€æ­¥è®ºè¯äº†åœ¨åŒ»å­¦æ•°æ®å…·æœ‰â€œç²—ç³™æ€§â€ï¼ˆcrudenessï¼‰ä¸”æµ‹é‡æ ‡å‡†ä¸ç¡®å®šçš„æƒ…å†µä¸‹ï¼Œè¿™ç§åºæ•°è§„åˆ™æ¯”ä¼ ç»Ÿçš„æœŸæœ›æ•ˆç”¨ä¼˜åŒ–ï¼ˆexpected-utility optimizationï¼‰æ›´å…·é²æ£’æ€§ã€‚åŸºäºæ­¤ï¼Œä½œè€…æå‡ºäº†ä¸€ä¸ªä¸åŒ»ç”Ÿæ€ç»´å¯¹é½çš„ AI è“å›¾ï¼šåˆ©ç”¨ä¸°å¯Œæ¨¡å‹å¤„ç†ä¿¡å¿µï¼Œä½†é‡‡ç”¨é²æ£’çš„åºæ•°è§„åˆ™é€‰æ‹©è¡ŒåŠ¨ã€‚æœ€ç»ˆï¼Œå»ºè®®å°† AI éƒ¨ç½²ä¸ºâ€œé€‰æ‹©æ€§å¤æ‚æ€§â€ï¼ˆselective complexityï¼‰ï¼Œä»…åœ¨å†³ç­–è„†å¼±ä¸”ä¿¡æ¯å…·æœ‰æ­£å‘é¢„æœŸå½±å“æ—¶ç”¨äºè¾…åŠ©å†³ç­–ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "34 pages",
      "pdf_url": "https://arxiv.org/pdf/2601.12547v1",
      "published_date": "2026-01-18 19:19:41 UTC",
      "updated_date": "2026-01-18 19:19:41 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-22T23:29:27.045206+00:00"
    },
    {
      "arxiv_id": "2601.12542v1",
      "title": "Rethinking the AI Scientist: Interactive Multi-Agent Workflows for Scientific Discovery",
      "title_zh": "é‡æ–°å®¡è§† AI ç§‘å­¦å®¶ï¼šé¢å‘ç§‘å­¦å‘ç°çš„äº¤äº’å¼å¤šæ™ºèƒ½ä½“å·¥ä½œæµ",
      "authors": [
        "Lukas Weidener",
        "Marko BrkiÄ‡",
        "Mihailo JovanoviÄ‡",
        "Ritvik Singh",
        "Chiara Baccin",
        "Emre Ulgac",
        "Alex Dobrin",
        "Aakaash Meduri"
      ],
      "abstract": "Artificial intelligence systems for scientific discovery have demonstrated remarkable potential, yet existing approaches remain largely proprietary and operate in batch-processing modes requiring hours per research cycle, precluding real-time researcher guidance. This paper introduces Deep Research, a multi-agent system enabling interactive scientific investigation with turnaround times measured in minutes. The architecture comprises specialized agents for planning, data analysis, literature search, and novelty detection, unified through a persistent world state that maintains context across iterative research cycles. Two operational modes support different workflows: semi-autonomous mode with selective human checkpoints, and fully autonomous mode for extended investigations. Evaluation on the BixBench computational biology benchmark demonstrated state-of-the-art performance, achieving 48.8% accuracy on open response and 64.5% on multiple-choice evaluation, exceeding existing baselines by 14 to 26 percentage points. Analysis of architectural constraints, including open access literature limitations and challenges inherent to automated novelty assessment, informs practical deployment considerations for AI-assisted scientific workflows.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† Deep Researchï¼Œè¿™æ˜¯ä¸€ä¸ªäº¤äº’å¼å¤šæ™ºèƒ½ä½“ç³»ç»Ÿï¼ˆmulti-agent systemï¼‰ï¼Œæ—¨åœ¨è§£å†³ç°æœ‰ AI ç§‘å­¦å‘ç°ç³»ç»Ÿå“åº”ç¼“æ…¢ä¸”ç¼ºä¹å®æ—¶ç ”ç©¶äººå‘˜æŒ‡å¯¼çš„é—®é¢˜ã€‚è¯¥ç³»ç»Ÿç”±è´Ÿè´£è§„åˆ’ã€æ•°æ®åˆ†æã€æ–‡çŒ®æ£€ç´¢å’Œæ–°é¢–æ€§æ£€æµ‹ï¼ˆnovelty detectionï¼‰çš„ä¸“é—¨åŒ–æ™ºèƒ½ä½“ç»„æˆï¼Œé€šè¿‡æŒä¹…åŒ–ä¸–ç•ŒçŠ¶æ€ï¼ˆpersistent world stateï¼‰åœ¨è¿­ä»£å‘¨æœŸä¸­ç»´æŒä¸Šä¸‹æ–‡ï¼Œå°†ç ”ç©¶å‘¨è½¬æ—¶é—´ä»å°æ—¶çº§ç¼©çŸ­è‡³åˆ†é’Ÿçº§ã€‚Deep Research æ”¯æŒåŒ…å«äººå·¥æ£€æŸ¥ç‚¹çš„åŠè‡ªä¸»æ¨¡å¼ä¸å®Œå…¨è‡ªä¸»æ¨¡å¼ï¼Œèƒ½å¤Ÿçµæ´»é€‚åº”ä¸åŒçš„ç§‘å­¦å‘ç°å·¥ä½œæµã€‚åœ¨è®¡ç®—ç”Ÿç‰©å­¦åŸºå‡† BixBench ä¸Šçš„è¯„ä¼°ç»“æœæ˜¾ç¤ºï¼Œè¯¥ç³»ç»Ÿåœ¨å¼€æ”¾å¼å›ç­”å’Œå¤šé¡¹é€‰æ‹©è¯„ä»·ä¸­åˆ†åˆ«å–å¾—äº† 48.8% å’Œ 64.5% çš„å‡†ç¡®ç‡ï¼Œè¶…å‡ºåŸºçº¿æ¨¡å‹ 14 è‡³ 26 ä¸ªç™¾åˆ†ç‚¹ã€‚æ­¤å¤–ï¼Œç ”ç©¶è¿˜åˆ†æäº†å¼€æ”¾è·å–æ–‡çŒ®é™åˆ¶åŠè‡ªåŠ¨åŒ–æ–°é¢–æ€§è¯„ä¼°å¸¦æ¥çš„æŒ‘æˆ˜ï¼Œä¸º AI è¾…åŠ©ç§‘å­¦å·¥ä½œæµçš„å®é™…éƒ¨ç½²æä¾›äº†é‡è¦è§è§£ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2601.12542v1",
      "published_date": "2026-01-18 19:12:41 UTC",
      "updated_date": "2026-01-18 19:12:41 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-22T23:29:24.660877+00:00"
    },
    {
      "arxiv_id": "2601.12539v1",
      "title": "MemeLens: Multilingual Multitask VLMs for Memes",
      "title_zh": "MemeLensï¼šé’ˆå¯¹æ¨¡å› çš„å¤šè¯­è¨€å¤šä»»åŠ¡è§†è§‰è¯­è¨€æ¨¡å‹",
      "authors": [
        "Ali Ezzat Shahroor",
        "Mohamed Bayan Kmainasi",
        "Abul Hasnat",
        "Dimitar Dimitrov",
        "Giovanni Da San Martino",
        "Preslav Nakov",
        "Firoj Alam"
      ],
      "abstract": "Memes are a dominant medium for online communication and manipulation because meaning emerges from interactions between embedded text, imagery, and cultural context. Existing meme research is distributed across tasks (hate, misogyny, propaganda, sentiment, humour) and languages, which limits cross-domain generalization. To address this gap we propose MemeLens, a unified multilingual and multitask explanation-enhanced Vision Language Model (VLM) for meme understanding. We consolidate 38 public meme datasets, filter and map dataset-specific labels into a shared taxonomy of $20$ tasks spanning harm, targets, figurative/pragmatic intent, and affect. We present a comprehensive empirical analysis across modeling paradigms, task categories, and datasets. Our findings suggest that robust meme understanding requires multimodal training, exhibits substantial variation across semantic categories, and remains sensitive to over-specialization when models are fine-tuned on individual datasets rather than trained in a unified setting. We will make the experimental resources and datasets publicly available for the community.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† MemeLensï¼Œè¿™æ˜¯ä¸€ä¸ªç»Ÿä¸€çš„å¤šè¯­è¨€ã€å¤šä»»åŠ¡è§£é‡Šå¢å¼ºå‹è§†è§‰è¯­è¨€æ¨¡å‹ (Vision Language Model, VLM)ï¼Œæ—¨åœ¨è§£å†³ç½‘ç»œæ¨¡å›  (Memes) ç†è§£ä¸­ä»»åŠ¡å’Œè¯­è¨€ç¢ç‰‡åŒ–çš„æŒ‘æˆ˜ã€‚ç ”ç©¶å›¢é˜Ÿæ•´åˆäº† 38 ä¸ªå…¬å¼€æ•°æ®é›†ï¼Œå¹¶å°†ç‰¹å®šæ ‡ç­¾æ˜ å°„ä¸ºåŒ…å«ä¼¤å®³æ€§ã€æ”»å‡»ç›®æ ‡ã€æ„å›¾å’Œæƒ…æ„Ÿç­‰ 20 ä¸ªä»»åŠ¡çš„ç»Ÿä¸€åˆ†ç±»ä½“ç³»ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œç¨³å¥çš„æ¨¡å› ç†è§£é«˜åº¦ä¾èµ–å¤šæ¨¡æ€ (Multimodal) è®­ç»ƒï¼Œä¸”æ¨¡å‹åœ¨å•ä¸€æ•°æ®é›†ä¸Šå¾®è°ƒæ—¶å®¹æ˜“å‡ºç°è¿‡åº¦ä¸“ä¸šåŒ– (Over-specialization) é—®é¢˜ï¼Œè€Œç»Ÿä¸€è®­ç»ƒèƒ½æ˜¾è‘—æå‡æ³›åŒ–èƒ½åŠ›ã€‚è¯¥é¡¹ç›®å°†å…¬å¼€ç›¸å…³å®éªŒèµ„æºå’Œæ•°æ®é›†ï¼Œä¸ºè·¨é¢†åŸŸã€è·¨è¯­è¨€çš„æ¨¡å› ç ”ç©¶æä¾›äº†é‡è¦åŸºç¡€ã€‚",
      "categories": [
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.AI",
      "comment": "disinformation, misinformation, factuality, harmfulness, fake news, propaganda, hateful meme, multimodality, text, images",
      "pdf_url": "https://arxiv.org/pdf/2601.12539v1",
      "published_date": "2026-01-18 19:01:03 UTC",
      "updated_date": "2026-01-18 19:01:03 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-22T23:29:27.453675+00:00"
    },
    {
      "arxiv_id": "2601.12538v1",
      "title": "Agentic Reasoning for Large Language Models",
      "title_zh": "å¤§è¯­è¨€æ¨¡å‹çš„æ™ºèƒ½ä½“æ¨ç†",
      "authors": [
        "Tianxin Wei",
        "Ting-Wei Li",
        "Zhining Liu",
        "Xuying Ning",
        "Ze Yang",
        "Jiaru Zou",
        "Zhichen Zeng",
        "Ruizhong Qiu",
        "Xiao Lin",
        "Dongqi Fu",
        "Zihao Li",
        "Mengting Ai",
        "Duo Zhou",
        "Wenxuan Bao",
        "Yunzhe Li",
        "Gaotang Li",
        "Cheng Qian",
        "Yu Wang",
        "Xiangru Tang",
        "Yin Xiao",
        "Liri Fang",
        "Hui Liu",
        "Xianfeng Tang",
        "Yuji Zhang",
        "Chi Wang",
        "Jiaxuan You",
        "Heng Ji",
        "Hanghang Tong",
        "Jingrui He"
      ],
      "abstract": "Reasoning is a fundamental cognitive process underlying inference, problem-solving, and decision-making. While large language models (LLMs) demonstrate strong reasoning capabilities in closed-world settings, they struggle in open-ended and dynamic environments. Agentic reasoning marks a paradigm shift by reframing LLMs as autonomous agents that plan, act, and learn through continual interaction. In this survey, we organize agentic reasoning along three complementary dimensions. First, we characterize environmental dynamics through three layers: foundational agentic reasoning, which establishes core single-agent capabilities including planning, tool use, and search in stable environments; self-evolving agentic reasoning, which studies how agents refine these capabilities through feedback, memory, and adaptation; and collective multi-agent reasoning, which extends intelligence to collaborative settings involving coordination, knowledge sharing, and shared goals. Across these layers, we distinguish in-context reasoning, which scales test-time interaction through structured orchestration, from post-training reasoning, which optimizes behaviors via reinforcement learning and supervised fine-tuning. We further review representative agentic reasoning frameworks across real-world applications and benchmarks, including science, robotics, healthcare, autonomous research, and mathematics. This survey synthesizes agentic reasoning methods into a unified roadmap bridging thought and action, and outlines open challenges and future directions, including personalization, long-horizon interaction, world modeling, scalable multi-agent training, and governance for real-world deployment.",
      "tldr_zh": "è¯¥ç»¼è¿°æ¢è®¨äº†å¤§å‹è¯­è¨€æ¨¡å‹(LLMs)ä¸­çš„Agentic Reasoningï¼Œå°†å…¶ä»å°é—­ç¯å¢ƒæ¨ç†è½¬å˜ä¸ºåœ¨åŠ¨æ€ç¯å¢ƒä¸­è¿›è¡Œè§„åˆ’ã€è¡ŒåŠ¨ä¸å­¦ä¹ çš„è‡ªä¸»æ™ºèƒ½ä½“èŒƒå¼ã€‚æ–‡ç« ä»ä¸‰ä¸ªç»´åº¦æ„å»ºäº†æ¨ç†æ¡†æ¶ï¼šæ¶µç›–è§„åˆ’ä¸å·¥å…·è°ƒç”¨çš„åŸºç¡€æ™ºèƒ½ä½“æ¨ç†(Foundational Agentic Reasoning)ã€é€šè¿‡åé¦ˆä¸è®°å¿†å®ç°çš„è‡ªæˆ‘è¿›åŒ–æ¨ç†(Self-evolving Agentic Reasoning)ï¼Œä»¥åŠæ¶‰åŠåä½œä¸çŸ¥è¯†å…±äº«çš„é›†ä½“å¤šæ™ºèƒ½ä½“æ¨ç†(Collective Multi-agent Reasoning)ã€‚ç ”ç©¶è¿›ä¸€æ­¥åŒºåˆ†äº†åŸºäºç»“æ„åŒ–ç¼–æ’çš„In-context Reasoningä¸åŸºäºå¼ºåŒ–å­¦ä¹ å’Œå¾®è°ƒçš„Post-training Reasoningã€‚è¯¥è°ƒæŸ¥åˆæˆäº†è¿æ¥æ€ç»´ä¸è¡ŒåŠ¨çš„ç»Ÿä¸€è·¯çº¿å›¾ï¼Œå¹¶æ¢è®¨äº†åœ¨æœºå™¨äººã€åŒ»ç–—åŠç§‘å­¦ç ”ç©¶ç­‰é¢†åŸŸçš„åº”ç”¨ï¼Œæœ€åæŒ‡å‡ºäº†é•¿æ—¶äº¤äº’å’Œæ¨¡å‹æ²»ç†ç­‰æœªæ¥æŒ‘æˆ˜ã€‚",
      "categories": [
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.AI",
      "comment": "Project: https://github.com/weitianxin/Awesome-Agentic-Reasoning",
      "pdf_url": "https://arxiv.org/pdf/2601.12538v1",
      "published_date": "2026-01-18 18:58:23 UTC",
      "updated_date": "2026-01-18 18:58:23 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-22T23:29:31.342686+00:00"
    },
    {
      "arxiv_id": "2601.12535v1",
      "title": "Improving Low-Resource Machine Translation via Round-Trip Reinforcement Learning",
      "title_zh": "åŸºäºå¾€è¿”å¼ºåŒ–å­¦ä¹ æå‡ä½èµ„æºæœºå™¨ç¿»è¯‘æ€§èƒ½",
      "authors": [
        "Ahmed Attia",
        "Alham Fikri"
      ],
      "abstract": "Low-resource machine translation (MT) has gained increasing attention as parallel data from low-resource language communities is collected, but many potential methods for improving low-resource MT remain unexplored. We investigate a self-supervised reinforcement-learning-based fine-tuning for translation in low-resource settings using round-trip bootstrapping with the No Language Left Behind (NLLB) family of models. Our approach translates English into a target low-resource language and then back into English, using a combination of chrF++ and BLEU as the reward function on the reconstructed English sentences. Using the NLLB-MD dataset, we evaluate both the 600M and 1.3B parameter NLLB models and observe consistent improvements for the following languages: Central Aymara, Friulian, Wolof and Russian. Qualitative inspection of translation outputs indicates increased fluency and semantic fidelity. We argue that our method can further benefit from scale, enabling models to increasingly leverage their pretrained knowledge and continue self-improving.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†åœ¨ä½èµ„æºï¼ˆlow-resourceï¼‰æœºå™¨ç¿»è¯‘ä¸­åº”ç”¨è‡ªæˆ‘ç›‘ç£å¼ºåŒ–å­¦ä¹ ï¼ˆself-supervised reinforcement-learningï¼‰çš„å¾®è°ƒæ–¹æ³•ã€‚é€šè¿‡åˆ©ç”¨ NLLB ç³»åˆ—æ¨¡å‹è¿›è¡Œå¾€è¿”å¼•å¯¼ï¼ˆround-trip bootstrappingï¼‰ï¼Œè¯¥æ–¹æ³•å°†è‹±è¯­ç¿»è¯‘ä¸ºç›®æ ‡ä½èµ„æºè¯­è¨€åå†ç¿»è¯‘å›è‹±è¯­ï¼Œå¹¶ä»¥ chrF++ å’Œ BLEU çš„ç»„åˆä½œä¸ºé‡æ„å¥å­çš„å¥–åŠ±å‡½æ•°ã€‚å®éªŒåœ¨ 600M å’Œ 1.3B å‚æ•°çš„ NLLB æ¨¡å‹ä¸Šè¿›è¡Œï¼Œç»“æœæ˜¾ç¤º Central Aymaraã€Friulianã€Wolof å’Œä¿„è¯­ï¼ˆRussianï¼‰ç­‰è¯­è¨€çš„ç¿»è¯‘æ€§èƒ½å¾—åˆ°äº†ä¸€è‡´æå‡ã€‚å®šæ€§åˆ†æè¡¨æ˜ï¼Œè¯¥æ–¹æ³•å¢å¼ºäº†ç¿»è¯‘çš„æµç•…æ€§å’Œè¯­ä¹‰ä¿çœŸåº¦ï¼Œè¯æ˜äº†æ¨¡å‹èƒ½å¤Ÿæœ‰æ•ˆåˆ©ç”¨é¢„è®­ç»ƒçŸ¥è¯†è¿›è¡ŒæŒç»­çš„è‡ªæˆ‘æ”¹è¿›ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2601.12535v1",
      "published_date": "2026-01-18 18:44:49 UTC",
      "updated_date": "2026-01-18 18:44:49 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-22T23:29:35.767878+00:00"
    },
    {
      "arxiv_id": "2601.12534v2",
      "title": "Encoding Emotion Through Self-Supervised Eye Movement Reconstruction",
      "title_zh": "åŸºäºè‡ªç›‘ç£çœ¼åŠ¨é‡å»ºçš„æƒ…æ„Ÿç¼–ç ",
      "authors": [
        "Marcus Ma",
        "Jordan Prescott",
        "Emily Zhou",
        "Tiantian Feng",
        "Kleanthis Avramidis",
        "Gabor Mihaly Toth",
        "Shrikanth Narayanan"
      ],
      "abstract": "The relationship between emotional expression and eye movement is well-documented, with literature establishing gaze patterns are reliable indicators of emotion. However, most studies utilize specialized, high-resolution eye-tracking equipment, limiting the potential reach of findings. We investigate how eye movement can be used to predict multimodal markers of emotional expression from naturalistic, low-resolution videos. We utilize a collection of video interviews from the USC Shoah Foundation's Visual History Archive with Holocaust survivors as they recount their experiences in the Auschwitz concentration camp. Inspired by pretraining methods on language models, we develop a novel gaze detection model that uses self-supervised eye movement reconstruction that can effectively leverage unlabeled video. We use this model's encoder embeddings to fine-tune models on two downstream tasks related to emotional expression. The first is aligning eye movement with directional emotion estimates from speech. The second task is using eye gaze as a predictor of three momentary manifestations of emotional behaviors: laughing, crying/sobbing, and sighing. We find our new model is predictive of emotion outcomes and observe a positive correlation between pretraining performance and emotion processing performance for both experiments. We conclude self-supervised eye movement reconstruction is an effective method for encoding the affective signal they carry.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†å¦‚ä½•åˆ©ç”¨ä½åˆ†è¾¨ç‡è§†é¢‘ä¸­çš„çœ¼åŠ¨ï¼ˆEye Movementï¼‰æ¥é¢„æµ‹å¤šæ¨¡æ€çš„æƒ…æ„Ÿè¡¨è¾¾ï¼Œæ—¨åœ¨è§£å†³ä¼ ç»Ÿæƒ…æ„Ÿç ”ç©¶ä¾èµ–é«˜åˆ†è¾¨ç‡çœ¼åŠ¨è¿½è¸ªè®¾å¤‡çš„é—®é¢˜ã€‚ä½œè€…å—è¯­è¨€æ¨¡å‹é¢„è®­ç»ƒå¯å‘ï¼Œæå‡ºäº†ä¸€ç§åŸºäºè‡ªç›‘ç£çœ¼åŠ¨é‡å»ºï¼ˆSelf-Supervised Eye Movement Reconstructionï¼‰çš„åˆ›æ–°æ¨¡å‹ï¼Œèƒ½å¤Ÿæœ‰æ•ˆåˆ©ç”¨å¤§å± æ€å¹¸å­˜è€…è®¿è°ˆç­‰æ— æ ‡ç­¾è§†é¢‘æ•°æ®ã€‚ç ”ç©¶é€šè¿‡ä¸¤ä¸ªä¸‹æ¸¸ä»»åŠ¡éªŒè¯äº†æ¨¡å‹æ€§èƒ½ï¼šä¸€æ˜¯å°†çœ¼åŠ¨ä¸è¯­éŸ³çš„æƒ…æ„Ÿä¼°è®¡å¯¹é½ï¼ŒäºŒæ˜¯åˆ©ç”¨æ³¨è§†ï¼ˆEye Gazeï¼‰é¢„æµ‹ç¬‘ã€å“­æ³£å’Œå¹æ¯ç­‰å…·ä½“æƒ…æ„Ÿè¡Œä¸ºã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ¨¡å‹èƒ½æœ‰æ•ˆç¼–ç æƒ…æ„Ÿä¿¡å·ï¼Œä¸”é¢„è®­ç»ƒè¡¨ç°ä¸æƒ…æ„Ÿå¤„ç†ä»»åŠ¡çš„æ€§èƒ½å‘ˆæ­£ç›¸å…³ï¼Œä¸ºåœ¨è‡ªç„¶åœºæ™¯ä¸‹è¿›è¡Œæƒ…æ„Ÿè®¡ç®—æä¾›äº†æ–°çš„æŠ€æœ¯é€”å¾„ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2601.12534v2",
      "published_date": "2026-01-18 18:37:41 UTC",
      "updated_date": "2026-01-21 03:08:38 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-22T23:29:36.292402+00:00"
    },
    {
      "arxiv_id": "2601.12522v1",
      "title": "Improved Bug Localization with AI Agents Leveraging Hypothesis and Dynamic Cognition",
      "title_zh": "åŸºäºå‡è®¾éªŒè¯ä¸åŠ¨æ€è®¤çŸ¥çš„ AI æ™ºèƒ½ä½“æ”¹è¿›ç¼ºé™·å®šä½",
      "authors": [
        "Asif Mohammed Samir",
        "Mohammad Masudur Rahman"
      ],
      "abstract": "Software bugs cost technology providers (e.g., AT&T) billions annually and cause developers to spend roughly 50% of their time on bug resolution. Traditional methods for bug localization often analyze the suspiciousness of code components (e.g., methods, documents) in isolation, overlooking their connections with other components in the codebase. Recent advances in Large Language Models (LLMs) and agentic AI techniques have shown strong potential for code understanding, but still lack causal reasoning during code exploration and struggle to manage growing context effectively, limiting their capability. In this paper, we present a novel agentic technique for bug localization -- CogniGent -- that overcomes the limitations above by leveraging multiple AI agents capable of causal reasoning, call-graph-based root cause analysis and context engineering. It emulates developers-inspired debugging practices (a.k.a., dynamic cognitive debugging) and conducts hypothesis testing to support bug localization. We evaluate CogniGent on a curated dataset of 591 bug reports using three widely adopted performance metrics and compare it against six established baselines from the literature. Experimental results show that our technique consistently outperformed existing traditional and LLM-based techniques, achieving MAP improvements of 23.33-38.57% at the document and method levels. Similar gains were observed in MRR, with increases of 25.14-53.74% at both granularity levels. Statistical significance tests also confirm the superiority of our technique. By addressing the reasoning, dependency, and context limitations, CogniGent advances the state of bug localization, bridging human-like cognition with agentic automation for improved performance.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† CogniGentï¼Œä¸€ç§æ—¨åœ¨æ”¹è¿›ç¼ºé™·å®šä½ï¼ˆBug Localizationï¼‰çš„æ–°é¢–å¤šæ™ºèƒ½ä½“æŠ€æœ¯ï¼Œä»¥è§£å†³ç°æœ‰æ–¹æ³•åœ¨å› æœæ¨ç†å’Œå¤§è§„æ¨¡ä¸Šä¸‹æ–‡ç®¡ç†æ–¹é¢çš„å±€é™ã€‚è¯¥æ¡†æ¶é€šè¿‡æ¨¡æ‹Ÿå¼€å‘è€…çš„åŠ¨æ€è®¤çŸ¥è°ƒè¯•ï¼ˆDynamic Cognitive Debuggingï¼‰è¿‡ç¨‹ï¼Œåˆ©ç”¨ AI Agents æ‰§è¡Œå› æœæ¨ç†ï¼ˆCausal Reasoningï¼‰ã€åŸºäºè°ƒç”¨å›¾ï¼ˆCall-graphï¼‰çš„æ ¹å› åˆ†æåŠä¸Šä¸‹æ–‡å·¥ç¨‹ï¼Œå¹¶ç»“åˆå‡è®¾æ£€éªŒï¼ˆHypothesis Testingï¼‰æ¥ç²¾ç¡®å®šä½ä»£ç ç¼ºé™·ã€‚åœ¨åŒ…å« 591 ä¸ªç¼ºé™·æŠ¥å‘Šçš„å®éªŒä¸­ï¼ŒCogniGent åœ¨æ–‡æ¡£å’Œæ–¹æ³•å±‚é¢çš„å¹³å‡ç²¾åº¦å‡å€¼ï¼ˆMAPï¼‰æå‡äº† 23.33-38.57%ï¼Œå¹³å‡é€†æ–‡æ¡£é¢‘ç‡ï¼ˆMRRï¼‰æå‡äº† 25.14-53.74%ã€‚ç»Ÿè®¡åˆ†æè¯å®è¯¥æŠ€æœ¯æ˜¾è‘—ä¼˜äºç°æœ‰çš„ä¼ ç»Ÿæ–¹æ³•å’ŒåŸºäº LLM çš„åŸºçº¿æ¨¡å‹ï¼ŒæˆåŠŸå°†ç±»äººè®¤çŸ¥ä¸æ™ºèƒ½ä½“è‡ªåŠ¨åŒ–ç›¸ç»“åˆï¼Œå¤§å¹…æå‡äº†ç¼ºé™·å®šä½çš„æ•ˆç‡ã€‚",
      "categories": [
        "cs.SE",
        "cs.AI",
        "cs.IR",
        "cs.LG",
        "cs.MA"
      ],
      "primary_category": "cs.SE",
      "comment": "13 pages, 7 tables, 5 figures",
      "pdf_url": "https://arxiv.org/pdf/2601.12522v1",
      "published_date": "2026-01-18 18:12:21 UTC",
      "updated_date": "2026-01-18 18:12:21 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-22T23:30:27.443017+00:00"
    },
    {
      "arxiv_id": "2601.12518v1",
      "title": "Cooperative Multi-agent RL with Communication Constraints",
      "title_zh": "é€šä¿¡å—é™ä¸‹çš„åä½œå¼å¤šæ™ºèƒ½ä½“å¼ºåŒ–å­¦ä¹ ",
      "authors": [
        "Nuoya Xiong",
        "Aarti Singh"
      ],
      "abstract": "Cooperative MARL often assumes frequent access to global information in a data buffer, such as team rewards or other agents' actions, which is typically unrealistic in decentralized MARL systems due to high communication costs. When communication is limited, agents must rely on outdated information to estimate gradients and update their policies. A common approach to handle missing data is called importance sampling, in which we reweigh old data from a base policy to estimate gradients for the current policy. However, it quickly becomes unstable when the communication is limited (i.e. missing data probability is high), so that the base policy in importance sampling is outdated. To address this issue, we propose a technique called base policy prediction, which utilizes old gradients to predict the policy update and collect samples for a sequence of base policies, which reduces the gap between the base policy and the current policy. This approach enables effective learning with significantly fewer communication rounds, since the samples of predicted base policies could be collected within one communication round. Theoretically, we show that our algorithm converges to an $\\varepsilon$-Nash equilibrium in potential games with only $O(\\varepsilon^{-3/4})$ communication rounds and $O(poly(\\max_i |A_i|)\\varepsilon^{-11/4})$ samples, improving existing state-of-the-art results in communication cost, as well as sample complexity without the exponential dependence on the joint action space size. We also extend these results to general Markov Cooperative Games to find an agent-wise local maximum. Empirically, we test the base policy prediction algorithm in both simulated games and MAPPO for complex environments.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹åä½œå¼å¤šæ™ºèƒ½ä½“å¼ºåŒ–å­¦ä¹  (Cooperative MARL) ä¸­ç”±äºé€šä¿¡å—é™å¯¼è‡´çš„æ•°æ®ç¼ºå¤±å’Œé‡è¦æ€§é‡‡æ · (Importance Sampling) ä¸ç¨³å®šé—®é¢˜ï¼Œæå‡ºäº†åŸºç¡€ç­–ç•¥é¢„æµ‹ (Base Policy Prediction) æŠ€æœ¯ã€‚è¯¥æ–¹æ³•åˆ©ç”¨å†å²æ¢¯åº¦é¢„æµ‹ç­–ç•¥æ›´æ–°ï¼Œå¹¶ä¸ºä¸€ç³»åˆ—åŸºç¡€ç­–ç•¥æ”¶é›†æ ·æœ¬ï¼Œä»è€Œæœ‰æ•ˆç¼©å°äº†åŸºç¡€ç­–ç•¥ä¸å½“å‰ç­–ç•¥ä¹‹é—´çš„å·®è·ã€‚ç†è®ºåˆ†æè¯æ˜ï¼Œè¯¥ç®—æ³•åœ¨åŠ¿èƒ½åšå¼ˆ (Potential Games) ä¸­èƒ½ä»¥ $O(\\varepsilon^{-3/4})$ çš„é€šä¿¡è½®æ•°æ”¶æ•›è‡³ $\\varepsilon$-Nash equilibriumï¼Œæ˜¾è‘—ä¼˜åŒ–äº†é€šä¿¡æˆæœ¬ä¸æ ·æœ¬å¤æ‚åº¦ï¼Œä¸”æ‘†è„±äº†å¯¹è”åˆåŠ¨ä½œç©ºé—´å¤§å°çš„æŒ‡æ•°ä¾èµ–ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥ç®—æ³•åœ¨æ¨¡æ‹Ÿåšå¼ˆåŠå¤æ‚çš„ MAPPO ç¯å¢ƒä¸­å‡è¡¨ç°å‡ºä¼˜å¼‚çš„æ€§èƒ½ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "stat.ML"
      ],
      "primary_category": "cs.LG",
      "comment": "33 pages",
      "pdf_url": "https://arxiv.org/pdf/2601.12518v1",
      "published_date": "2026-01-18 18:05:23 UTC",
      "updated_date": "2026-01-18 18:05:23 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-22T23:29:40.915325+00:00"
    },
    {
      "arxiv_id": "2601.12499v1",
      "title": "Failure Modes in Multi-Hop QA: The Weakest Link Law and the Recognition Bottleneck",
      "title_zh": "å¤šè·³é—®ç­”ä¸­çš„å¤±æ•ˆæ¨¡å¼ï¼šæœ€å¼±ç¯å®šå¾‹ä¸è¯†åˆ«ç“¶é¢ˆ",
      "authors": [
        "Meiru Zhang",
        "Zaiqiao Meng",
        "Nigel Collier"
      ],
      "abstract": "Despite scaling to massive context windows, Large Language Models (LLMs) struggle with multi-hop reasoning due to inherent position bias, which causes them to overlook information at certain positions. Whether these failures stem from an inability to locate evidence (recognition failure) or integrate it (synthesis failure) is unclear. We introduce Multi-Focus Attention Instruction (MFAI), a semantic probe to disentangle these mechanisms by explicitly steering attention towards selected positions. Across 5 LLMs on two multi-hop QA tasks (MuSiQue and NeoQA), we establish the \"Weakest Link Law\": multi-hop reasoning performance collapses to the performance level of the least visible evidence. Crucially, this failure is governed by absolute position rather than the linear distance between facts (performance variance $<3%$). We further identify a duality in attention steering: while matched MFAI resolves recognition bottlenecks, improving accuracy by up to 11.5% in low-visibility positions, misleading MFAI triggers confusion in real-world tasks but is successfully filtered in synthetic tasks. Finally, we demonstrate that \"thinking\" models that utilize System-2 reasoning, effectively locate and integrate the required information, matching gold-only baselines even in noisy, long-context settings.",
      "tldr_zh": "è¯¥ç ”ç©¶é€šè¿‡å¼•å…¥è¯­ä¹‰æ¢é’ˆ Multi-Focus Attention Instruction (MFAI)ï¼Œæ¢è®¨äº†å¤§è¯­è¨€æ¨¡å‹(LLMs)åœ¨å¤šè·³æ¨ç†(multi-hop reasoning)ä¸­çš„å¤±è´¥æ¨¡å¼ã€‚ç ”ç©¶æå‡ºäº†â€œæœ€å¼±ç¯å®šå¾‹(Weakest Link Law)â€ï¼Œå³å¤šè·³æ¨ç†çš„æ•´ä½“æ€§èƒ½å–å†³äºå¯è§åº¦æœ€ä½çš„é‚£æ¡è¯æ®ï¼Œä¸”è¯¥å¤±è´¥ä¸»è¦ç”±ç»å¯¹ä½ç½®è€Œéäº‹å®é—´çš„çº¿æ€§è·ç¦»å†³å®šã€‚å®éªŒå‘ç°ï¼ŒåŒ¹é…çš„ MFAI èƒ½å¤Ÿæœ‰æ•ˆè§£å†³è¯†åˆ«ç“¶é¢ˆ(recognition bottleneck)ï¼Œåœ¨ä½å¯è§åº¦ä½ç½®ä½¿å‡†ç¡®ç‡æå‡é«˜è¾¾11.5%ã€‚æœ€åï¼Œç ”ç©¶è¯æ˜é‡‡ç”¨ç³»ç»Ÿ2æ¨ç†(System-2 reasoning)çš„â€œæ€è€ƒâ€æ¨¡å‹èƒ½æ›´æœ‰æ•ˆåœ°å®šä½å’Œæ•´åˆä¿¡æ¯ï¼Œåœ¨é•¿æ–‡æœ¬ç¯å¢ƒä¸­å…‹æœä½ç½®åè§å¹¶è¾¾åˆ°åŸºçº¿æ°´å¹³ã€‚",
      "categories": [
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "preprint",
      "pdf_url": "https://arxiv.org/pdf/2601.12499v1",
      "published_date": "2026-01-18 17:16:04 UTC",
      "updated_date": "2026-01-18 17:16:04 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-22T23:29:47.032027+00:00"
    },
    {
      "arxiv_id": "2601.12494v1",
      "title": "Harmonizing the Arabic Audio Space with Data Scheduling",
      "title_zh": "åˆ©ç”¨æ•°æ®è°ƒåº¦ååŒé˜¿æ‹‰ä¼¯è¯­éŸ³é¢‘ç©ºé—´",
      "authors": [
        "Hunzalah Hassan Bhatti",
        "Firoj Alam",
        "Shammur Absar Chowdhury"
      ],
      "abstract": "Audio large language models (LLMs) enable unified speech understanding and generation, yet their adaptation to linguistically complex, dialect-rich settings remains underexplored. This paper presents the first systematic study of multi-task instruction tuning for an Arabic-centric audio LLM, covering a hierarchy of generative tasks (ASR, speech summarization) and discriminative tasks (dialect and emotion identification). To support this study, we introduce AraMega-SSum, a novel dataset for Arabic speech summarization. We fine-tune Qwen2.5-Omni (7B) and propose Task-Progressive Curriculum (TPC) along with Aligner-Based Diverse Sampling (ADS), a strategy that constructs information-dense batches by selecting task- and label-balanced examples. Our results reveal a critical efficiency, robustness trade-off: while ADS accelerates initial convergence and boosts paralinguistic F1-scores, its inherent gradient volatility can destabilize generative decoding under prolonged training. Furthermore, while the TPC stabilizes core acoustic mapping, it often induces negative transfer in downstream tasks. We demonstrate that a Hybrid TPC+ADS Strategy provides an optimal training ``recipe'', first establishing a robust representative foundation before employing diversity-aware refinement to capture fine-grained nuances. These findings offer practical guidance for the efficient adaptation of Omni-models in complex, low-resource multimodal environments.",
      "tldr_zh": "è¯¥ç ”ç©¶å¯¹ä»¥é˜¿æ‹‰ä¼¯è¯­ä¸ºæ ¸å¿ƒçš„éŸ³é¢‘å¤§è¯­è¨€æ¨¡å‹(Audio LLMs)è¿›è¡Œäº†é¦–æ¬¡ç³»ç»Ÿæ€§çš„å¤šä»»åŠ¡æŒ‡ä»¤å¾®è°ƒç ”ç©¶ï¼ŒåŸºäº Qwen2.5-Omni (7B) æ¶µç›–äº†è¯­éŸ³è¯†åˆ«(ASR)ã€è¯­éŸ³æ‘˜è¦(speech summarization)ä»¥åŠæ–¹è¨€å’Œæƒ…æ„Ÿè¯†åˆ«ç­‰ä»»åŠ¡ã€‚ç ”ç©¶è€…å¼•å…¥äº†æ–°å‹æ•°æ®é›† AraMega-SSumï¼Œå¹¶æå‡ºäº†ä»»åŠ¡é€’è¿›å¼è¯¾ç¨‹(Task-Progressive Curriculum, TPC)ä¸åŸºäºå¯¹é½å™¨çš„å¤šæ ·åŒ–é‡‡æ ·(Aligner-Based Diverse Sampling, ADS)ä¸¤ç§æ•°æ®è°ƒåº¦ç­–ç•¥ã€‚å®éªŒå‘ç°ï¼ŒADS è™½ç„¶èƒ½åŠ é€Ÿæ”¶æ•›å¹¶æå‡å‰¯è¯­è¨€ä»»åŠ¡è¡¨ç°ï¼Œä½†å¯èƒ½å¯¼è‡´è§£ç ä¸ç¨³å®šï¼Œè€Œ TPC åœ¨ç¨³å®šå£°å­¦æ˜ å°„çš„åŒæ—¶å¯èƒ½è¯±å‘è´Ÿè¿ç§»(negative transfer)ã€‚ç ”ç©¶æœ€ç»ˆè¯æ˜ï¼Œé‡‡ç”¨æ··åˆç­–ç•¥(Hybrid TPC+ADS Strategy)èƒ½æä¾›æœ€ä½³è®­ç»ƒæ•ˆæœï¼Œä¸ºåœ¨å¤æ‚ã€ä½èµ„æºçš„å¤šæ¨¡æ€ç¯å¢ƒä¸‹é€‚é…å…¨èƒ½æ¨¡å‹(Omni-models)æä¾›äº†é‡è¦çš„å®è·µæŒ‡å¯¼ã€‚",
      "categories": [
        "cs.SD",
        "cs.AI",
        "cs.CL",
        "eess.AS"
      ],
      "primary_category": "cs.SD",
      "comment": "Foundation Models, Large Language Models, Native, Speech Models, Arabic",
      "pdf_url": "https://arxiv.org/pdf/2601.12494v1",
      "published_date": "2026-01-18 17:08:31 UTC",
      "updated_date": "2026-01-18 17:08:31 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-22T23:29:49.073481+00:00"
    },
    {
      "arxiv_id": "2601.12471v1",
      "title": "Knowing When to Abstain: Medical LLMs Under Clinical Uncertainty",
      "title_zh": "æ˜ç¡®å¼ƒæƒæ—¶æœºï¼šä¸´åºŠä¸ç¡®å®šæ€§ä¸‹çš„åŒ»ç–—å¤§è¯­è¨€æ¨¡å‹",
      "authors": [
        "Sravanthi Machcha",
        "Sushrita Yerra",
        "Sahil Gupta",
        "Aishwarya Sahoo",
        "Sharmin Sultana",
        "Hong Yu",
        "Zonghai Yao"
      ],
      "abstract": "Current evaluation of large language models (LLMs) overwhelmingly prioritizes accuracy; however, in real-world and safety-critical applications, the ability to abstain when uncertain is equally vital for trustworthy deployment. We introduce MedAbstain, a unified benchmark and evaluation protocol for abstention in medical multiple-choice question answering (MCQA) -- a discrete-choice setting that generalizes to agentic action selection -- integrating conformal prediction, adversarial question perturbations, and explicit abstention options. Our systematic evaluation of both open- and closed-source LLMs reveals that even state-of-the-art, high-accuracy models often fail to abstain with uncertain. Notably, providing explicit abstention options consistently increases model uncertainty and safer abstention, far more than input perturbations, while scaling model size or advanced prompting brings little improvement. These findings highlight the central role of abstention mechanisms for trustworthy LLM deployment and offer practical guidance for improving safety in high-stakes applications.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† **MedAbstain**ï¼Œè¿™æ˜¯ä¸€ä¸ªé’ˆå¯¹åŒ»ç–—å¤šé€‰é¢˜å›ç­” (MCQA) åœºæ™¯ä¸‹å¼ƒæƒ (abstention) æœºåˆ¶çš„ç»Ÿä¸€åŸºå‡†å’Œè¯„ä¼°åè®®ï¼Œæ—¨åœ¨æå‡å¤§å‹è¯­è¨€æ¨¡å‹ (LLMs) åœ¨ä¸´åºŠä¸ç¡®å®šæ€§ä¸‹çš„å¯é æ€§ã€‚è¯¥åŸºå‡†ç»“åˆäº† **conformal prediction**ã€å¯¹æŠ—æ€§é—®é¢˜æ‰°åŠ¨ (**adversarial question perturbations**) ä»¥åŠæ˜¾å¼å¼ƒæƒé€‰é¡¹ï¼Œå¯¹å¤šç§å¼€æºå’Œé—­æº LLMs è¿›è¡Œäº†ç³»ç»Ÿè¯„ä¼°ã€‚å®éªŒå‘ç°ï¼Œå³ä¾¿é«˜å‡†ç¡®ç‡çš„æ¨¡å‹åœ¨ä¸ç¡®å®šæ—¶ä¹Ÿå¾€å¾€éš¾ä»¥ä¸»åŠ¨å¼ƒæƒï¼Œè€Œæä¾›æ˜¾å¼çš„å¼ƒæƒé€‰é¡¹æ¯”å¢åŠ æ¨¡å‹è§„æ¨¡æˆ–æ”¹è¿›æç¤ºè¯ (**prompting**) èƒ½æ›´æœ‰æ•ˆåœ°è¯±å¯¼å®‰å…¨çš„å¼ƒæƒè¡Œä¸ºã€‚è¿™äº›ç ”ç©¶ç»“æœå¼ºè°ƒäº†å¼ƒæƒæœºåˆ¶åœ¨åŒ»ç–—ç­‰é«˜é£é™©åº”ç”¨éƒ¨ç½²ä¸­çš„æ ¸å¿ƒä½œç”¨ï¼Œå¹¶ä¸ºæå‡ LLM çš„å®‰å…¨æ€§æä¾›äº†å®æ“å»ºè®®ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "Equal contribution for the first two authors; To appear in proceedings of the Main Conference of the European Chapter of the Association for Computational Linguistics (EACL) 2026",
      "pdf_url": "https://arxiv.org/pdf/2601.12471v1",
      "published_date": "2026-01-18 16:19:29 UTC",
      "updated_date": "2026-01-18 16:19:29 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-22T23:30:38.403195+00:00"
    },
    {
      "arxiv_id": "2601.12467v2",
      "title": "Patch-Level Tokenization with CNN Encoders and Attention for Improved Transformer Time-Series Forecasting",
      "title_zh": "ç»“åˆ CNN ç¼–ç å™¨ä¸æ³¨æ„åŠ›æœºåˆ¶çš„åˆ†å—çº§æ ‡è®°åŒ–ï¼šæå‡ Transformer æ—¶é—´åºåˆ—é¢„æµ‹æ€§èƒ½",
      "authors": [
        "Saurish Nagrath",
        "Saroj Kumar Panigrahy"
      ],
      "abstract": "Transformer-based models have shown strong performance in time-series forecasting by leveraging self-attention to model long-range temporal dependencies. However, their effectiveness depends critically on the quality and structure of input representations derived from raw multivariate time-series data, particularly as sequence length and data scale increase. This paper proposes a two-stage forecasting framework that explicitly separates local temporal representation learning from global dependency modelling. In the proposed approach, a convolutional neural network operates on fixed-length temporal patches to extract short-range temporal dynamics and non-linear feature interactions, producing compact patch-level token embeddings. Token-level self-attention is applied during representation learning to refine these embeddings, after which a Transformer encoder models inter-patch temporal dependencies to generate forecasts. The method is evaluated on a synthetic multivariate time-series dataset with controlled static and dynamic factors, using an extended sequence length and a larger number of samples. Experimental results demonstrate that the proposed framework consistently outperforms a convolutional baseline under increased temporal context and remains competitive with a strong patch-based Transformer model. These findings indicate that structured patch-level tokenization provides a scalable and effective representation for multivariate time-series forecasting, particularly when longer input sequences are considered.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§ä¸¤é˜¶æ®µçš„æ—¶é—´åºåˆ—é¢„æµ‹æ¡†æ¶ï¼Œæ ¸å¿ƒåœ¨äºå°†å±€éƒ¨æ—¶é—´è¡¨ç¤ºå­¦ä¹ ä¸å…¨å±€ä¾èµ–å»ºæ¨¡ç›¸åˆ†ç¦»ã€‚è¯¥æ–¹æ³•åˆ©ç”¨ CNN ç¼–ç å™¨å¯¹å›ºå®šé•¿åº¦çš„æ—¶é—´è¡¥ä¸(temporal patches)è¿›è¡Œå¤„ç†ï¼Œä»¥æå–çŸ­ç¨‹åŠ¨æ€å’Œéçº¿æ€§ç‰¹å¾äº¤äº’ï¼Œç”Ÿæˆç´§å‡‘çš„ patch-level token åµŒå…¥ï¼Œå¹¶ç»“åˆ token çº§åˆ«çš„ self-attention è¿›è¡Œç²¾ç‚¼ã€‚æœ€åï¼Œé€šè¿‡ Transformer encoder å»ºæ¨¡è¡¥ä¸é—´çš„é•¿æœŸä¾èµ–å…³ç³»ä»¥ç”Ÿæˆé¢„æµ‹ç»“æœã€‚å®éªŒè¯æ˜ï¼Œè¯¥æ¡†æ¶åœ¨å¤„ç†é•¿è¾“å…¥åºåˆ—æ—¶å…·æœ‰æ˜¾è‘—çš„æœ‰æ•ˆæ€§ä¸å¯æ‰©å±•æ€§ï¼Œå…¶è¡¨ç°ä¸ä»…ä¼˜äºå·ç§¯åŸºçº¿æ¨¡å‹ï¼Œä¸”åœ¨ä¸å¼ºåŠ› patch-based Transformer æ¨¡å‹çš„ç«äº‰ä¸­ä¿æŒäº†æå¼ºçš„ç«äº‰åŠ›ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "6 pages, 2 figures, 3 tables",
      "pdf_url": "https://arxiv.org/pdf/2601.12467v2",
      "published_date": "2026-01-18 16:16:01 UTC",
      "updated_date": "2026-01-21 14:41:56 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-22T23:30:45.783045+00:00"
    },
    {
      "arxiv_id": "2601.12465v1",
      "title": "Incentivizing In-depth Reasoning over Long Contexts with Process Advantage Shaping",
      "title_zh": "é€šè¿‡è¿‡ç¨‹ä¼˜åŠ¿å¡‘é€ æ¿€åŠ±é•¿ä¸Šä¸‹æ–‡æ·±åº¦æ¨ç†",
      "authors": [
        "Miao Peng",
        "Weizhou Shen",
        "Nuo Chen",
        "Chenliang Li",
        "Ming Yan",
        "Jia Li"
      ],
      "abstract": "Reinforcement Learning with Verifiable Rewards (RLVR) has proven effective in enhancing LLMs short-context reasoning, but its performance degrades in long-context scenarios that require both precise grounding and robust long-range reasoning. We identify the \"almost-there\" phenomenon in long-context reasoning, where trajectories are largely correct but fail at the final step, and attribute this failure to two factors: (1) the lack of high reasoning density in long-context QA data that push LLMs beyond mere grounding toward sophisticated multi-hop reasoning; and (2) the loss of valuable learning signals during long-context RL training due to the indiscriminate penalization of partially correct trajectories with incorrect outcomes. To overcome this bottleneck, we propose DeepReasonQA, a KG-driven synthesis framework that controllably constructs high-difficulty, multi-hop long-context QA pairs with inherent reasoning chains. Building on this, we introduce Long-context Process Advantage Shaping (LongPAS), a simple yet effective method that performs fine-grained credit assignment by evaluating reasoning steps along Validity and Relevance dimensions, which captures critical learning signals from \"almost-there\" trajectories. Experiments on three long-context reasoning benchmarks show that our approach substantially outperforms RLVR baselines and matches frontier LLMs while using far fewer parameters. Further analysis confirms the effectiveness of our methods in strengthening long-context reasoning while maintaining stable RL training.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¼ºåŒ–å­¦ä¹ ï¼ˆRLVRï¼‰åœ¨é•¿ä¸Šä¸‹æ–‡æ¨ç†ä¸­æ€§èƒ½ä¸‹é™çš„é—®é¢˜ï¼Œè¯†åˆ«å¹¶åˆ†æäº†æ¨ç†è½¨è¿¹â€œå‡ ä¹æˆåŠŸ(almost-there)â€ä½†åœ¨æœ€åä¸€æ­¥å¤±è´¥çš„ç°è±¡ã€‚ä¸ºäº†çªç ´è¿™ä¸€ç“¶é¢ˆï¼Œç ”ç©¶è€…æå‡ºäº† DeepReasonQAï¼Œè¿™æ˜¯ä¸€ä¸ªåŸºäºçŸ¥è¯†å›¾è°±(KG)é©±åŠ¨çš„åˆæˆæ¡†æ¶ï¼Œèƒ½å¤Ÿå—æ§åœ°æ„å»ºå…·æœ‰å¤æ‚æ¨ç†é“¾çš„é«˜éš¾åº¦ã€å¤šè·³é•¿ä¸Šä¸‹æ–‡é—®ç­”å¯¹ã€‚åŒæ—¶ï¼Œç ”ç©¶å¼•å…¥äº†é•¿ä¸Šä¸‹æ–‡è¿‡ç¨‹ä¼˜åŠ¿å¡‘é€ (Long-context Process Advantage Shaping, LongPAS)ï¼Œé€šè¿‡åœ¨æœ‰æ•ˆæ€§(Validity)å’Œç›¸å…³æ€§(Relevance)ç»´åº¦è¿›è¡Œç»†ç²’åº¦çš„ä¿¡ç”¨åˆ†é…ï¼Œä»è€Œæ•æ‰éƒ¨åˆ†æ­£ç¡®è½¨è¿¹ä¸­çš„å…³é”®å­¦ä¹ ä¿¡å·ã€‚å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨ä¸‰ä¸ªé•¿ä¸Šä¸‹æ–‡æ¨ç†åŸºå‡†æµ‹è¯•ä¸­æ˜¾è‘—ä¼˜äº RLVR åŸºå‡†ï¼Œå¹¶èƒ½ä»¥æ›´å°‘çš„å‚æ•°é‡è¾¾åˆ°å‰æ²¿å¤§å‹è¯­è¨€æ¨¡å‹(LLMs)çš„æ€§èƒ½æ°´å¹³ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2601.12465v1",
      "published_date": "2026-01-18 16:10:04 UTC",
      "updated_date": "2026-01-18 16:10:04 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-22T23:30:47.994958+00:00"
    },
    {
      "arxiv_id": "2601.12449v1",
      "title": "AgenTRIM: Tool Risk Mitigation for Agentic AI",
      "title_zh": "AgenTRIMï¼šé’ˆå¯¹æ™ºèƒ½ä½“ AI çš„å·¥å…·é£é™©ç¼“è§£",
      "authors": [
        "Roy Betser",
        "Shamik Bose",
        "Amit Giloni",
        "Chiara Picardi",
        "Sindhu Padakandla",
        "Roman Vainshtein"
      ],
      "abstract": "AI agents are autonomous systems that combine LLMs with external tools to solve complex tasks. While such tools extend capability, improper tool permissions introduce security risks such as indirect prompt injection and tool misuse. We characterize these failures as unbalanced tool-driven agency. Agents may retain unnecessary permissions (excessive agency) or fail to invoke required tools (insufficient agency), amplifying the attack surface and reducing performance. We introduce AgenTRIM, a framework for detecting and mitigating tool-driven agency risks without altering an agent's internal reasoning. AgenTRIM addresses these risks through complementary offline and online phases. Offline, AgenTRIM reconstructs and verifies the agent's tool interface from code and execution traces. At runtime, it enforces per-step least-privilege tool access through adaptive filtering and status-aware validation of tool calls. Evaluating on the AgentDojo benchmark, AgenTRIM substantially reduces attack success while maintaining high task performance. Additional experiments show robustness to description-based attacks and effective enforcement of explicit safety policies. Together, these results demonstrate that AgenTRIM provides a practical, capability-preserving approach to safer tool use in LLM-based agents.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† **AgenTRIM**ï¼Œä¸€ä¸ªæ—¨åœ¨ç¼“è§£ AI æ™ºèƒ½ä½“ (AI agents) å·¥å…·ä½¿ç”¨é£é™©çš„æ¡†æ¶ï¼Œé‡ç‚¹è§£å†³å› å·¥å…·æƒé™ä¸å½“å¼•å‘çš„é—´æ¥æç¤ºæ³¨å…¥ (indirect prompt injection) å’Œå·¥å…·æ»¥ç”¨ç­‰å®‰å…¨é—®é¢˜ã€‚ç ”ç©¶å°†æ­¤ç±»é£é™©å½’çº³ä¸ºâ€œä¸å¹³è¡¡çš„å·¥å…·é©±åŠ¨ä»£ç† (unbalanced tool-driven agency)â€ï¼Œå³æ™ºèƒ½ä½“å­˜åœ¨è¿‡åº¦æˆæƒ (excessive agency) æˆ–æˆæƒä¸è¶³ (insufficient agency) çš„æƒ…å†µã€‚AgenTRIM é€šè¿‡ç¦»çº¿éªŒè¯å·¥å…·æ¥å£ä¸åœ¨çº¿è¿è¡Œæ—¶å¼ºåˆ¶æ‰§è¡Œæ¯ä¸€æ­¥çš„æœ€å°æƒé™ (least-privilege) è®¿é—®ï¼Œåœ¨ä¸æ”¹å˜æ™ºèƒ½ä½“å†…éƒ¨æ¨ç†é€»è¾‘çš„å‰æä¸‹å®ç°äº†é£é™©ç®¡æ§ã€‚åœ¨ AgentDojo åŸºå‡†æµ‹è¯•ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼ŒAgenTRIM åœ¨ä¿æŒé«˜ä»»åŠ¡è¡¨ç°çš„åŒæ—¶æ˜¾è‘—é™ä½äº†æ”»å‡»æˆåŠŸç‡ï¼Œä¸ºæ„å»ºå®‰å…¨ä¸”é«˜æ•ˆçš„ LLM æ™ºèƒ½ä½“æä¾›äº†ä¸€ç§å®ç”¨çš„è§£å†³æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.CR",
        "cs.AI"
      ],
      "primary_category": "cs.CR",
      "comment": "Under review",
      "pdf_url": "https://arxiv.org/pdf/2601.12449v1",
      "published_date": "2026-01-18 15:10:18 UTC",
      "updated_date": "2026-01-18 15:10:18 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-22T23:30:47.726332+00:00"
    },
    {
      "arxiv_id": "2601.12444v1",
      "title": "Large Language Model for OWL Proofs",
      "title_zh": "é¢å‘ OWL è¯æ˜çš„å¤§è¯­è¨€æ¨¡å‹",
      "authors": [
        "Hui Yang",
        "Jiaoyan Chen",
        "Uli Sattler"
      ],
      "abstract": "The ability of Large Language Models (LLMs) to perform reasoning tasks such as deduction has been widely investigated in recent years. Yet, their capacity to generate proofs-faithful, human-readable explanations of why conclusions follow-remains largely under explored. In this work, we study proof generation in the context of OWL ontologies, which are widely adopted for representing and reasoning over complex knowledge, by developing an automated dataset construction and evaluation framework. Our evaluation encompassing three sequential tasks for complete proving: Extraction, Simplification, and Explanation, as well as an additional task of assessing Logic Completeness of the premise. Through extensive experiments on widely used reasoning LLMs, we achieve important findings including: (1) Some models achieve overall strong results but remain limited on complex cases; (2) Logical complexity, rather than representation format (formal logic language versus natural language), is the dominant factor shaping LLM performance; and (3) Noise and incompleteness in input data substantially diminish LLMs' performance. Together, these results underscore both the promise of LLMs for explanation with rigorous logics and the gap of supporting resilient reasoning under complex or imperfect conditions. Code and data are available at https://github.com/HuiYang1997/LLMOwlR.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†å¤§è¯­è¨€æ¨¡å‹(LLMs)åœ¨OWLæœ¬ä½“(OWL ontologies)èƒŒæ™¯ä¸‹ç”Ÿæˆå¿ å®ã€äººç±»å¯è¯»è¯æ˜çš„èƒ½åŠ›ï¼Œå¹¶å¼€å‘äº†ä¸€å¥—è‡ªåŠ¨åŒ–çš„æ•°æ®é›†æ„å»ºä¸è¯„ä¼°æ¡†æ¶ã€‚è¯„ä¼°è¿‡ç¨‹æ¶µç›–äº†æå–(Extraction)ã€ç®€åŒ–(Simplification)ã€è§£é‡Š(Explanation)ä»¥åŠå‰æé€»è¾‘å®Œå¤‡æ€§(Logic Completeness)è¯„ä¼°å››ä¸ªå…³é”®ä»»åŠ¡ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œé€»è¾‘å¤æ‚åº¦(Logical complexity)è€Œéè¡¨ç¤ºæ ¼å¼æ˜¯å½±å“æ¨¡å‹æ€§èƒ½çš„ä¸»å¯¼å› ç´ ï¼Œä¸”è¾“å…¥æ•°æ®çš„å™ªå£°å’Œä¸å®Œæ•´æ€§ä¼šæ˜¾è‘—å‰Šå¼±LLMsçš„è¡¨ç°ã€‚è¿™é¡¹å·¥ä½œçªæ˜¾äº†LLMsåœ¨ä¸¥å¯†é€»è¾‘è§£é‡Šæ–¹é¢çš„æ½œåŠ›ï¼ŒåŒæ—¶ä¹Ÿæ­ç¤ºäº†å…¶åœ¨å¤æ‚æˆ–ä¸å®Œç¾æ¡ä»¶ä¸‹å®ç°éŸ§æ€§æ¨ç†çš„å·®è·ã€‚",
      "categories": [
        "cs.AI",
        "cs.LO"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2601.12444v1",
      "published_date": "2026-01-18 14:57:57 UTC",
      "updated_date": "2026-01-18 14:57:57 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-22T23:30:52.701286+00:00"
    },
    {
      "arxiv_id": "2601.12443v1",
      "title": "Adversarial Defense in Vision-Language Models: An Overview",
      "title_zh": "è§†è§‰-è¯­è¨€æ¨¡å‹å¯¹æŠ—é˜²å¾¡ç»¼è¿°",
      "authors": [
        "Xiaowei Fu",
        "Lei Zhang"
      ],
      "abstract": "The widespread use of Vision Language Models (VLMs, e.g. CLIP) has raised concerns about their vulnerability to sophisticated and imperceptible adversarial attacks. These attacks could compromise model performance and system security in cross-modal tasks. To address this challenge, three main defense paradigms have been proposed: Training-time Defense, Test-time Adaptation Defense, and Training-free Defense. Training-time Defense involves modifying the training process, typically through adversarial fine-tuning to improve the robustness to adversarial examples. While effective, this approach requires substantial computational resources and may not generalize across all adversarial attacks. Test-time Adaptation Defense focuses on adapting the model at inference time by updating its parameters to handle unlabeled adversarial examples, offering flexibility but often at the cost of increased complexity and computational overhead. Training-free Defense avoids modifying the model itself, instead focusing on altering the adversarial inputs or their feature embeddings, which enforces input perturbations to mitigate the impact of attacks without additional training. This survey reviews the latest advancements in adversarial defense strategies for VLMs, highlighting the strengths and limitations of such approaches and discussing ongoing challenges in enhancing the robustness of VLMs.",
      "tldr_zh": "è¯¥ç»¼è¿°æ¢è®¨äº†è§†è§‰è¯­è¨€æ¨¡å‹ (Vision-Language Models, VLMs) åœ¨é¢å¯¹ç²¾å¯†å¯¹æŠ—æ€§æ”»å‡» (adversarial attacks) æ—¶çš„è„†å¼±æ€§åŠå…¶å¯¹ç³»ç»Ÿå®‰å…¨çš„å½±å“ã€‚æ–‡ç« ç³»ç»Ÿæ€»ç»“äº†ä¸‰ç§ä¸»è¦çš„é˜²å¾¡èŒƒå¼ï¼šé€šè¿‡å¯¹æŠ—æ€§å¾®è°ƒæå‡é²æ£’æ€§çš„è®­ç»ƒæ—¶é˜²å¾¡ (Training-time Defense)ã€åœ¨æ¨ç†é˜¶æ®µåŠ¨æ€æ›´æ–°å‚æ•°çš„æµ‹è¯•æ—¶è‡ªé€‚åº”é˜²å¾¡ (Test-time Adaptation Defense)ï¼Œä»¥åŠé€šè¿‡ä¿®æ”¹è¾“å…¥æˆ–ç‰¹å¾åµŒå…¥æ¥å‡è½»å¹²æ‰°çš„æ— éœ€è®­ç»ƒé˜²å¾¡ (Training-free Defense)ã€‚è¯¥ç ”ç©¶å›é¡¾äº†å„ç­–ç•¥çš„æœ€æ–°è¿›å±•ï¼Œåˆ†æäº†å…¶åœ¨è®¡ç®—æˆæœ¬ã€æ³›åŒ–èƒ½åŠ›å’Œå¤æ‚æ€§æ–¹é¢çš„ä¼˜ç¼ºç‚¹ï¼Œå¹¶æŒ‡å‡ºäº†å¢å¼º VLMs é²æ£’æ€§æ‰€é¢ä¸´çš„æŒç»­æŒ‘æˆ˜ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2601.12443v1",
      "published_date": "2026-01-18 14:57:51 UTC",
      "updated_date": "2026-01-18 14:57:51 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-22T23:30:55.382731+00:00"
    },
    {
      "arxiv_id": "2601.12442v1",
      "title": "Constraint-Aware Neurosymbolic Uncertainty Quantification with Bayesian Deep Learning for Scientific Discovery",
      "title_zh": "é¢å‘ç§‘å­¦å‘ç°çš„ç»“åˆè´å¶æ–¯æ·±åº¦å­¦ä¹ çš„çº¦æŸæ„ŸçŸ¥ç¥ç»ç¬¦å·ä¸ç¡®å®šæ€§é‡åŒ–",
      "authors": [
        "Shahnawaz Alam",
        "Mohammed Mudassir Uddin",
        "Mohammed Kaif Pasha"
      ],
      "abstract": "Scientific Artificial Intelligence (AI) applications require models that deliver trustworthy uncertainty estimates while respecting domain constraints. Existing uncertainty quantification methods lack mechanisms to incorporate symbolic scientific knowledge, while neurosymbolic approaches operate deterministically without principled uncertainty modeling. We introduce the Constraint-Aware Neurosymbolic Uncertainty Framework (CANUF), unifying Bayesian deep learning with differentiable symbolic reasoning. The architecture comprises three components: automated constraint extraction from scientific literature, probabilistic neural backbone with variational inference, and differentiable constraint satisfaction layer ensuring physical consistency. Experiments on Materials Project (140,000+ materials), QM9 molecular properties, and climate benchmarks show CANUF reduces Expected Calibration Error by 34.7% versus Bayesian neural networks while maintaining 99.2% constraint satisfaction. Ablations reveal constraint-guided recalibration contributes 18.3% performance gain, with constraint extraction achieving 91.4% precision. CANUF provides the first end-to-end differentiable pipeline simultaneously addressing uncertainty quantification, constraint satisfaction, and interpretable explanations for scientific predictions.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† CANUF (Constraint-Aware Neurosymbolic Uncertainty Framework)ï¼Œæ—¨åœ¨é€šè¿‡ç»Ÿä¸€ Bayesian deep learning å’Œ differentiable symbolic reasoningï¼Œè§£å†³ç§‘å­¦ AI é¢†åŸŸä¸­ä¸ç¡®å®šæ€§ä¼°è®¡ä¸é¢†åŸŸçº¦æŸä¸€è‡´æ€§éš¾ä»¥å…¼é¡¾çš„é—®é¢˜ã€‚è¯¥æ¡†æ¶ç”±è‡ªåŠ¨çº¦æŸæå–ã€åŸºäºå˜åˆ†æ¨ç†(variational inference)çš„æ¦‚ç‡ç¥ç»éª¨æ¶ä»¥åŠå¾®åˆ†çº¦æŸæ»¡è¶³å±‚ç»„æˆï¼Œç¡®ä¿é¢„æµ‹ç»“æœä¸¥æ ¼éµå¾ªç‰©ç†è§„å¾‹ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œåœ¨ Materials Projectã€QM9 å’Œæ°”å€™åŸºå‡†æµ‹è¯•ä¸­ï¼ŒCANUF ç›¸æ¯”ä¼ ç»Ÿ Bayesian neural networks å°† Expected Calibration Error é™ä½äº† 34.7%ï¼ŒåŒæ—¶ä¿æŒäº† 99.2% çš„çº¦æŸæ»¡è¶³ç‡ã€‚ä½œä¸ºé¦–ä¸ªç«¯åˆ°ç«¯å¾®åˆ†æµæ°´çº¿ï¼ŒCANUF åŒæ—¶å®ç°äº†å¯é çš„ä¸ç¡®å®šæ€§é‡åŒ–ã€ç§‘å­¦çº¦æŸæ»¡è¶³ä»¥åŠé¢„æµ‹ç»“æœçš„å¯è§£é‡Šæ€§ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2601.12442v1",
      "published_date": "2026-01-18 14:57:35 UTC",
      "updated_date": "2026-01-18 14:57:35 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-22T23:31:26.375207+00:00"
    },
    {
      "arxiv_id": "2601.12436v1",
      "title": "Purification Before Fusion: Toward Mask-Free Speech Enhancement for Robust Audio-Visual Speech Recognition",
      "title_zh": "èåˆå‰å‡€åŒ–ï¼šé¢å‘é²æ£’éŸ³è§†é¢‘è¯­éŸ³è¯†åˆ«çš„æ— æ©ç è¯­éŸ³å¢å¼º",
      "authors": [
        "Linzhi Wu",
        "Xingyu Zhang",
        "Hao Yuan",
        "Yakun Zhang",
        "Changyan Zheng",
        "Liang Xie",
        "Tiejun Liu",
        "Erwei Yin"
      ],
      "abstract": "Audio-visual speech recognition (AVSR) typically improves recognition accuracy in noisy environments by integrating noise-immune visual cues with audio signals. Nevertheless, high-noise audio inputs are prone to introducing adverse interference into the feature fusion process. To mitigate this, recent AVSR methods often adopt mask-based strategies to filter audio noise during feature interaction and fusion, yet such methods risk discarding semantically relevant information alongside noise. In this work, we propose an end-to-end noise-robust AVSR framework coupled with speech enhancement, eliminating the need for explicit noise mask generation. This framework leverages a Conformer-based bottleneck fusion module to implicitly refine noisy audio features with video assistance. By reducing modality redundancy and enhancing inter-modal interactions, our method preserves speech semantic integrity to achieve robust recognition performance. Experimental evaluations on the public LRS3 benchmark suggest that our method outperforms prior advanced mask-based baselines under noisy conditions.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§ç»“åˆè¯­éŸ³å¢å¼ºçš„ç«¯åˆ°ç«¯é²æ£’éŸ³è§†é¢‘è¯­éŸ³è¯†åˆ«(AVSR)æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³ä¼ ç»ŸåŸºäºæ©ç (mask-based)æ–¹æ³•åœ¨è¿‡æ»¤å™ªå£°æ—¶å¯èƒ½ä¸¢å¤±è¯­ä¹‰ä¿¡æ¯çš„é—®é¢˜ã€‚è¯¥æ¡†æ¶åˆ©ç”¨åŸºäºConformerçš„ç“¶é¢ˆèåˆ(bottleneck fusion)æ¨¡å—ï¼Œåœ¨è§†é¢‘è¾…åŠ©ä¸‹éšå¼åœ°ä¼˜åŒ–å˜ˆæ‚çš„éŸ³é¢‘ç‰¹å¾ï¼Œä»è€Œæ¶ˆé™¤äº†æ˜¾å¼ç”Ÿæˆå™ªå£°æ©ç çš„éœ€æ±‚ã€‚é€šè¿‡å‡å°‘æ¨¡æ€å†—ä½™å¹¶å¢å¼ºè·¨æ¨¡æ€äº¤äº’ï¼Œè¯¥æ–¹æ³•åœ¨ä¿ç•™è¯­éŸ³è¯­ä¹‰å®Œæ•´æ€§çš„åŒæ—¶å®ç°äº†é²æ£’çš„è¯†åˆ«æ€§èƒ½ã€‚åœ¨ LRS3 åŸºå‡†æ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å™ªå£°ç¯å¢ƒä¸‹çš„è¡¨ç°ä¼˜äºç°æœ‰çš„å…ˆè¿›æ©ç åŸºå‡†æ¨¡å‹ã€‚",
      "categories": [
        "eess.AS",
        "cs.AI",
        "cs.LG",
        "cs.MM",
        "cs.SD"
      ],
      "primary_category": "eess.AS",
      "comment": "Accepted by ICASSP2026",
      "pdf_url": "https://arxiv.org/pdf/2601.12436v1",
      "published_date": "2026-01-18 14:46:08 UTC",
      "updated_date": "2026-01-18 14:46:08 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-22T23:30:57.806567+00:00"
    },
    {
      "arxiv_id": "2601.12415v2",
      "title": "Orthogonalized Policy Optimization:Decoupling Sampling Geometry from Optimization Geometry in RLHF",
      "title_zh": "æ­£äº¤åŒ–ç­–ç•¥ä¼˜åŒ–ï¼šè§£è€¦ RLHF ä¸­çš„é‡‡æ ·å‡ ä½•ä¸ä¼˜åŒ–å‡ ä½•",
      "authors": [
        "Wang Zixian"
      ],
      "abstract": "Large language model alignment objectives are often presented as a collection of distinct algorithms, such as PPO, DPO, IPO, and their variants, each motivated by different derivations. In this work, we argue that this diversity obscures a simpler underlying structure. At a fundamental level, alignment objectives involve two independent design choices: (i) how training signals are sampled and weighted, and (ii) how deviations from a reference policy are geometrically penalized. Existing methods typically entangle these choices through a single divergence, most commonly the Kullback-Leibler divergence.\n  We show that this entanglement is not merely a modeling convenience but a source of systematic instability. When the same divergence simultaneously determines sample weighting and optimization curvature, adjusting one aspect, such as exploration strength, inevitably alters the other, such as gradient geometry. This coupling is particularly problematic in preference-based reinforcement learning, where advantage signals are unbounded and high-confidence regimes are common.\n  We propose a simple but structural remedy by formulating alignment as an orthogonal mirror descent problem, in which sampling geometry enters only as a linear driving force, while optimization geometry is determined independently by a mirror map. This perspective leads to a new alignment objective called Orthogonalized Policy Optimization (OPO), obtained by choosing a Euclidean mirror map in likelihood ratio space. The resulting objective admits a closed-form solution, linear and non-saturating gradient dynamics, and a well-conditioned trust region, while remaining fully compatible with standard large language model training pipelines.",
      "tldr_zh": "è¯¥é¡¹ç ”ç©¶æŒ‡å‡ºï¼Œç°æœ‰çš„è¯­è¨€æ¨¡å‹å¯¹é½æ–¹æ³•ï¼ˆå¦‚ PPOã€DPO ç­‰ï¼‰å°†é‡‡æ ·æƒé‡ä¸ä¼˜åŒ–å‡ ä½•è€¦åˆåœ¨å•ä¸€ç¦»æ•£åº¦ï¼ˆé€šå¸¸æ˜¯ KL divergenceï¼‰ä¸­ï¼Œå¯¼è‡´äº†ç³»ç»Ÿæ€§çš„è®­ç»ƒä¸ç¨³å®šæ€§ã€‚ä¸ºæ­¤ï¼Œç ”ç©¶è€…æå‡ºäº† Orthogonalized Policy Optimization (OPO)ï¼Œåˆ©ç”¨æ­£äº¤é•œåƒä¸‹é™ (orthogonal mirror descent) æ¡†æ¶å°†é‡‡æ ·å‡ ä½•ä¸ä¼˜åŒ–å‡ ä½•è§£è€¦ã€‚é€šè¿‡åœ¨ä¼¼ç„¶æ¯”ç©ºé—´é€‰æ‹©æ¬§å‡ é‡Œå¾—é•œåƒæ˜ å°„ï¼ŒOPO å®ç°äº†å…·æœ‰é—­å¼è§£ (closed-form solution) å’Œçº¿æ€§éé¥±å’Œæ¢¯åº¦åŠ¨åŠ›å­¦çš„ä¼˜åŒ–ç›®æ ‡ã€‚è¯¥æ–¹æ³•ä¸ä»…æä¾›äº†ç¨³å¥çš„ç½®ä¿¡åŒºåŸŸ (trust region)ï¼Œä¸”èƒ½å®Œå…¨å…¼å®¹ç°æœ‰çš„å¤§å‹è¯­è¨€æ¨¡å‹è®­ç»ƒæµç¨‹ï¼Œä¸ºæå‡ RLHF çš„ç¨³å®šæ€§æä¾›äº†æ–°çš„ç†è®ºæ¡†æ¶å’Œå®è·µæ–¹æ¡ˆã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2601.12415v2",
      "published_date": "2026-01-18 13:57:44 UTC",
      "updated_date": "2026-01-21 14:54:54 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-22T23:31:05.264394+00:00"
    },
    {
      "arxiv_id": "2601.12410v1",
      "title": "Are LLMs Smarter Than Chimpanzees? An Evaluation on Perspective Taking and Knowledge State Estimation",
      "title_zh": "LLMsæ¯”é»‘çŒ©çŒ©æ›´èªæ˜å—ï¼Ÿä¸€é¡¹å…³äºè§‚ç‚¹é‡‡æ‹©ä¸çŸ¥è¯†çŠ¶æ€ä¼°è®¡çš„è¯„ä¼°",
      "authors": [
        "Dingyi Yang",
        "Junqi Zhao",
        "Xue Li",
        "Ce Li",
        "Boyang Li"
      ],
      "abstract": "Cognitive anthropology suggests that the distinction of human intelligence lies in the ability to infer other individuals' knowledge states and understand their intentions. In comparison, our closest animal relative, chimpanzees, lack the capacity to do so. With this paper, we aim to evaluate LLM performance in the area of knowledge state tracking and estimation. We design two tasks to test (1) if LLMs can detect when story characters, through their actions, demonstrate knowledge they should not possess, and (2) if LLMs can predict story characters' next actions based on their own knowledge vs. objective truths they do not know. Results reveal that most current state-of-the-art LLMs achieve near-random performance on both tasks, and are substantially inferior to humans. We argue future LLM research should place more weight on the abilities of knowledge estimation and intention understanding.",
      "tldr_zh": "è¯¥ç ”ç©¶è¯„ä¼°äº†å¤§å‹è¯­è¨€æ¨¡å‹(LLMs)åœ¨è§‚ç‚¹é‡‡æ‹©(Perspective Taking)å’ŒçŸ¥è¯†çŠ¶æ€ä¼°è®¡(Knowledge State Estimation)æ–¹é¢çš„èƒ½åŠ›ï¼Œæ—¨åœ¨æ¢ç©¶å…¶æ˜¯å¦å…·å¤‡åŒºåˆ†è‡ªèº«è®¤çŸ¥ä¸ä»–äººçŸ¥è¯†çŠ¶æ€çš„ç±»äººæ™ºèƒ½ã€‚ç ”ç©¶è®¾è®¡äº†ä¸¤é¡¹ä»»åŠ¡ï¼Œåˆ†åˆ«æµ‹è¯• LLMs æ˜¯å¦èƒ½æ£€æµ‹å‡ºæ•…äº‹è§’è‰²è¡¨ç°å‡ºçš„å¼‚å¸¸çŸ¥è¯†ï¼Œä»¥åŠèƒ½å¦åŸºäºè§’è‰²çš„ä¸»è§‚è®¤çŸ¥è€Œéå®¢è§‚äº‹å®æ¥é¢„æµ‹å…¶åç»­è¡ŒåŠ¨ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œç›®å‰æœ€å…ˆè¿›çš„ LLMs åœ¨è¿™äº›ä»»åŠ¡ä¸Šçš„è¡¨ç°æ¥è¿‘éšæœºæ°´å¹³ï¼Œä¸”æ˜¾è‘—é€Šäºäººç±»ã€‚ç ”ç©¶æŒ‡å‡ºï¼ŒLLMs åœ¨è¿½è¸ªä»–äººçŸ¥è¯†çŠ¶æ€æ–¹é¢ä»é¢ä¸´å·¨å¤§æŒ‘æˆ˜ï¼Œæœªæ¥çš„ç ”ç©¶åº”æ›´åŠ é‡è§†çŸ¥è¯†ä¼°è®¡å’Œæ„å›¾ç†è§£èƒ½åŠ›çš„æå‡ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "23 pages, 11 figures",
      "pdf_url": "https://arxiv.org/pdf/2601.12410v1",
      "published_date": "2026-01-18 13:53:24 UTC",
      "updated_date": "2026-01-18 13:53:24 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-22T23:31:05.882847+00:00"
    },
    {
      "arxiv_id": "2601.12405v1",
      "title": "Explainable Machine Learning for Pediatric Dental Risk Stratification Using Socio-Demographic Determinants",
      "title_zh": "åŸºäºç¤¾ä¼šäººå£å­¦å†³å®šå› ç´ çš„å„¿ç«¥ç‰™ç§‘é£é™©åˆ†å±‚å¯è§£é‡Šæœºå™¨å­¦ä¹ ",
      "authors": [
        "Manasi Kanade",
        "Abhi Thakkar",
        "Gabriela Fernandes"
      ],
      "abstract": "Background: Pediatric dental disease remains one of the most prevalent and inequitable chronic health conditions worldwide. Although strong epidemiological evidence links oral health outcomes to socio-economic and demographic determinants, most artificial intelligence (AI) applications in dentistry rely on image-based diagnosis and black-box prediction models, limiting transparency and ethical applicability in pediatric populations.\n  Objective: This study aimed to develop and evaluate an explainable machine learning framework for pediatric dental risk stratification that prioritizes interpretability, calibration, and ethical deployment over maximal predictive accuracy.\n  Methods: A supervised machine learning model was trained using population-level pediatric data including age, income-to-poverty ratio, race/ethnicity, gender, and medical history. Model performance was assessed using receiver operating characteristic (ROC) analysis and calibration curves. Explainability was achieved using SHapley Additive exPlanations (SHAP) to provide global and individual-level interpretation of predictions.\n  Results: The model achieved modest discrimination (AUC = 0.61) with conservative calibration, underestimating risk at higher probability levels. SHAP analysis identified age and income-to-poverty ratio as the strongest contributors to predicted risk, followed by race/ethnicity and gender.\n  Conclusion: Explainable machine learning enables transparent, prevention-oriented pediatric dental risk stratification and supports population screening and equitable resource allocation rather than diagnostic decision-making.",
      "tldr_zh": "è¯¥ç ”ç©¶å¼€å‘å¹¶è¯„ä¼°äº†ä¸€ä¸ªå¯è§£é‡Šæœºå™¨å­¦ä¹ (Explainable Machine Learning)æ¡†æ¶ï¼Œç”¨äºå„¿ç§‘ç‰™ç§‘é£é™©åˆ†å±‚(Pediatric Dental Risk Stratification)ï¼Œé‡ç‚¹å…³æ³¨ç¤¾ä¼šäººå£ç»Ÿè®¡å­¦å› ç´ ã€‚è¯¥æ¡†æ¶åˆ©ç”¨å¹´é¾„ã€æ”¶å…¥è´«å›°æ¯”ã€ç§æ—ã€æ€§åˆ«åŠç—…å²ç­‰ç¾¤ä½“æ•°æ®è¿›è¡Œè®­ç»ƒï¼Œå¹¶é‡‡ç”¨ SHapley Additive exPlanations (SHAP) æŠ€æœ¯æä¾›å…¨å±€å’Œä¸ªä½“å±‚é¢çš„é¢„æµ‹è§£é‡Šã€‚å®éªŒç»“æœæ˜¾ç¤ºæ¨¡å‹å…·æœ‰ä¸­ç­‰çš„åŒºåˆ†åº¦(AUC = 0.61)ï¼Œå…¶ä¸­ SHAP åˆ†æç¡®å®šäº†å¹´é¾„å’Œæ”¶å…¥è´«å›°æ¯”æ˜¯é£é™©é¢„æµ‹çš„æœ€æ ¸å¿ƒè´¡çŒ®å› ç´ ã€‚ç ”ç©¶ç»“è®ºè¡¨æ˜ï¼Œè¯¥æ¡†æ¶æ”¯æŒé¢„é˜²å¯¼å‘çš„ç¾¤ä½“ç­›æŸ¥å’Œå…¬å¹³çš„èµ„æºåˆ†é…ï¼Œä¸ºå„¿ç§‘ç‰™ç§‘é¢†åŸŸçš„é€æ˜åŒ–åŠä¼¦ç†åŒ–å†³ç­–æä¾›äº†æŠ€æœ¯æ”¯æŒã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2601.12405v1",
      "published_date": "2026-01-18 13:40:41 UTC",
      "updated_date": "2026-01-18 13:40:41 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-22T23:31:37.838263+00:00"
    },
    {
      "arxiv_id": "2601.12402v1",
      "title": "Weaknesses of Facial Emotion Recognition Systems",
      "title_zh": "é¢éƒ¨è¡¨æƒ…è¯†åˆ«ç³»ç»Ÿçš„å±€é™æ€§",
      "authors": [
        "Aleksandra JamrÃ³z",
        "Patrycja Wysocka",
        "Piotr Garbat"
      ],
      "abstract": "Emotion detection from faces is one of the machine learning problems needed for human-computer interaction. The variety of methods used is enormous, which motivated an in-depth review of articles and scientific studies. Three of the most interesting and best solutions are selected, followed by the selection of three datasets that stood out for the diversity and number of images in them. The selected neural networks are trained, and then a series of experiments are performed to compare their performance, including testing on different datasets than a model was trained on. This reveals weaknesses in existing solutions, including differences between datasets, unequal levels of difficulty in recognizing certain emotions and the challenges in differentiating between closely related emotions.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹äººæœºäº¤äº’(Human-computer interaction)ä¸­é‡è¦çš„äººè„¸æƒ…æ„Ÿè¯†åˆ«é—®é¢˜ï¼Œé€šè¿‡æ·±å…¥ç»¼è¿°é€‰å–äº†ä¸‰ç§ä»£è¡¨æ€§æ–¹æ¡ˆåŠä¸‰ä¸ªå¤§å‹å¤šæ ·åŒ–æ•°æ®é›†è¿›è¡Œå¯¹æ¯”åˆ†æã€‚ç ”ç©¶äººå‘˜å¯¹é€‰å®šçš„ç¥ç»ç½‘ç»œè¿›è¡Œäº†è®­ç»ƒï¼Œå¹¶å¼€å±•äº†ä¸€ç³»åˆ—è·¨æ•°æ®é›†çš„æµ‹è¯•å®éªŒï¼Œä»¥è¯„ä¼°æ¨¡å‹çš„æ€§èƒ½ä¸æ³›åŒ–èƒ½åŠ›ã€‚å®éªŒç»“æœæ­ç¤ºäº†ç°æœ‰æ–¹æ¡ˆçš„å±€é™æ€§ï¼Œä¸»è¦åŒ…æ‹¬æ•°æ®é›†ä¹‹é—´çš„æ˜¾è‘—å·®å¼‚ã€ä¸åŒæƒ…æ„Ÿè¯†åˆ«éš¾åº¦çš„ä¸å‡è¡¡ï¼Œä»¥åŠç³»ç»Ÿåœ¨åŒºåˆ†ç›¸è¿‘æƒ…æ„Ÿæ—¶é¢ä¸´çš„å·¨å¤§æŒ‘æˆ˜ã€‚è¯¥å·¥ä½œä¸ºç†è§£å’Œæ”¹è¿›ç°æœ‰æƒ…æ„Ÿè¯†åˆ«ç³»ç»Ÿçš„å¼±ç‚¹æä¾›äº†é‡è¦çš„å®è¯ä¾æ®ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2601.12402v1",
      "published_date": "2026-01-18 13:27:01 UTC",
      "updated_date": "2026-01-18 13:27:01 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-22T23:31:43.982803+00:00"
    },
    {
      "arxiv_id": "2601.12401v1",
      "title": "Beyond the Dirac Delta: Mitigating Diversity Collapse in Reinforcement Fine-Tuning for Versatile Image Generation",
      "title_zh": "è¶…è¶Šç‹„æ‹‰å…‹ Deltaï¼šç¼“è§£é¢å‘å¤šæ ·åŒ–å›¾åƒç”Ÿæˆçš„å¼ºåŒ–å­¦ä¹ å¾®è°ƒä¸­çš„å¤šæ ·æ€§åç¼©",
      "authors": [
        "Jinmei Liu",
        "Haoru Li",
        "Zhenhong Sun",
        "Chaofeng Chen",
        "Yatao Bian",
        "Bo Wang",
        "Daoyi Dong",
        "Chunlin Chen",
        "Zhi Wang"
      ],
      "abstract": "Reinforcement learning (RL) has emerged as a powerful paradigm for fine-tuning large-scale generative models, such as diffusion and flow models, to align with complex human preferences and user-specified tasks. A fundamental limitation remains \\textit{the curse of diversity collapse}, where the objective formulation and optimization landscape inherently collapse the policy to a Dirac delta distribution. To address this challenge, we propose \\textbf{DRIFT} (\\textbf{D}ive\\textbf{R}sity-\\textbf{I}ncentivized Reinforcement \\textbf{F}ine-\\textbf{T}uning for Versatile Image Generation), an innovative framework that systematically incentivizes output diversity throughout the on-policy fine-tuning process, reconciling strong task alignment with high generation diversity to enhance versatility essential for applications that demand diverse candidate generations. We approach the problem across three representative perspectives: i) \\textbf{sampling} a reward-concentrated subset that filters out reward outliers to prevent premature collapse; ii) \\textbf{prompting} with stochastic variations to expand the conditioning space, and iii) \\textbf{optimization} of the intra-group diversity with a potential-based reward shaping mechanism. Experimental results show that DRIFT achieves superior Pareto dominance regarding task alignment and generation diversity, yielding a $ 9.08\\%\\!\\sim\\! 43.46\\%$ increase in diversity at equivalent alignment levels and a $ 59.65\\% \\!\\sim\\! 65.86\\%$ increase in alignment at equivalent levels of diversity.",
      "tldr_zh": "### è®ºæ–‡ TLDR æ‘˜è¦ ğŸ“„\n\n---\n\nè¯¥ç ”ç©¶æå‡ºäº† **DRIFT** (**D**ive**R**sity-**I**ncentivized Reinforcement **F**ine-**T**uning)ï¼Œè¿™æ˜¯ä¸€ä¸ªæ—¨åœ¨è§£å†³å¤§è§„æ¨¡ç”Ÿæˆæ¨¡å‹åœ¨å¼ºåŒ–å­¦ä¹ å¾®è°ƒ (Reinforcement Fine-Tuning) è¿‡ç¨‹ä¸­å¸¸è§çš„â€œå¤šæ ·æ€§å´©å¡Œâ€ (Diversity Collapse) é—®é¢˜çš„åˆ›æ–°æ¡†æ¶ã€‚è¯¥æ¡†æ¶ä»ä¸‰ä¸ªç»´åº¦ç³»ç»Ÿæ€§åœ°æ¿€åŠ±è¾“å‡ºå¤šæ ·æ€§ï¼šé€šè¿‡é‡‡æ · (**Sampling**) è¿‡æ»¤å¥–åŠ±å¼‚å¸¸å€¼ä»¥é˜²æ­¢è¿‡æ—©æ”¶æ•›ã€åˆ©ç”¨éšæœºå˜ä½“æç¤º (**Prompting**) æ‰©å±•æ¡ä»¶ç©ºé—´ï¼Œä»¥åŠåœ¨ä¼˜åŒ– (**Optimization**) é˜¶æ®µå¼•å…¥åŸºäºåŠ¿èƒ½çš„å¥–åŠ±å¡‘é€  (**Reward Shaping**) æœºåˆ¶ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒDRIFT åœ¨ä»»åŠ¡å¯¹é½ (Task Alignment) å’Œç”Ÿæˆå¤šæ ·æ€§æ–¹é¢å®ç°äº†æ˜¾è‘—çš„ **Pareto dominance**ï¼Œåœ¨åŒç­‰å¯¹é½æ°´å¹³ä¸‹å¤šæ ·æ€§æå‡äº† 9.08%~43.46%ã€‚è¯¥æ–¹æ³•æˆåŠŸè°ƒå’Œäº†å¼ºä»»åŠ¡å¯¹é½ä¸é«˜ç”Ÿæˆå¤šæ ·æ€§ä¹‹é—´çš„çŸ›ç›¾ï¼Œä¸ºéœ€è¦å¤šæ ·åŒ–å€™é€‰ç»“æœçš„åº”ç”¨åœºæ™¯æä¾›äº†æœ‰æ•ˆæ”¯æŒã€‚\n\n---\n\nå¸Œæœ›è¿™ä¸ªæ‘˜è¦èƒ½å‡†ç¡®ä¼ è¾¾æ‚¨è®ºæ–‡çš„æ ¸å¿ƒä»·å€¼ã€‚å¦‚æœæ‚¨è¿˜æœ‰å…¶ä»–è®ºæ–‡éœ€è¦å¤„ç†ï¼Œéšæ—¶å‘Šè¯‰æˆ‘ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2601.12401v1",
      "published_date": "2026-01-18 13:25:43 UTC",
      "updated_date": "2026-01-18 13:25:43 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-22T23:31:50.584220+00:00"
    },
    {
      "arxiv_id": "2601.12392v1",
      "title": "PsychÄ“Chat: An Empathic Framework Focused on Emotion Shift Tracking and Safety Risk Analysis in Psychological Counseling",
      "title_zh": "PsychÄ“Chatï¼šèšç„¦å¿ƒç†å’¨è¯¢ä¸­æƒ…ç»ªè½¬å˜è¿½è¸ªä¸å®‰å…¨é£é™©åˆ†æçš„å…±æƒ…æ¡†æ¶",
      "authors": [
        "Zhentao Xia",
        "Yongqi Fan",
        "Yuxiang Chu",
        "Yichao Yin",
        "Liangliang Chen",
        "Tong Ruan",
        "Weiyan Zhang"
      ],
      "abstract": "Large language models (LLMs) have demonstrated notable advancements in psychological counseling. However, existing models generally do not explicitly model seekers' emotion shifts across counseling sessions, a core focus in classical psychological schools. Moreover, how to align counselor models' responses with these emotion shifts while proactively mitigating safety risks remains underexplored. To bridge these gaps, we propose PsychÄ“Chat, which explicitly integrates emotion shift tracking and safety risk analysis for psychological counseling. Specifically, we employ interactive role-playing to synthesize counselor--seeker dialogues, incorporating two modules: Emotion Management Module, to capture seekers' current emotions and emotion shifts; and Risk Control Module, to anticipate seekers' subsequent reactions and identify potential risks. Furthermore, we introduce two modeling paradigms. The Agent Mode structures emotion management, risk control, and counselor responses into a collaborative multi-agent pipeline. The LLM Mode integrates these stages into a unified chain-of-thought for end-to-end inference, balancing efficiency and performance. Extensive experiments, including interactive scoring, dialogue-level evaluation, and human assessment, demonstrate that PsychÄ“Chat outperforms existing methods for emotional insight and safety control.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† PsychÄ“Chatï¼Œä¸€ä¸ªæ—¨åœ¨è§£å†³å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨å¿ƒç†å’¨è¯¢ä¸­ç¼ºä¹æƒ…æ„Ÿè½¬å˜ï¼ˆemotion shiftsï¼‰æ˜¾å¼å»ºæ¨¡åŠå®‰å…¨é£é™©é¢„æ§é—®é¢˜çš„å…±æƒ…æ¡†æ¶ã€‚è¯¥æ¡†æ¶æ ¸å¿ƒåŒ…å«æƒ…ç»ªç®¡ç†æ¨¡å—ï¼ˆEmotion Management Moduleï¼‰å’Œé£é™©æ§åˆ¶æ¨¡å—ï¼ˆRisk Control Moduleï¼‰ï¼Œåˆ†åˆ«ç”¨äºå®æ—¶è¿½è¸ªæ±‚åŠ©è€…çš„æƒ…æ„ŸåŠ¨æ€å¹¶å‰ç»æ€§åœ°è¯†åˆ«äº¤äº’è¿‡ç¨‹ä¸­çš„å®‰å…¨é£é™©ã€‚æ­¤å¤–ï¼Œç ”ç©¶å¼•å…¥äº†åŸºäºå¤šæ™ºèƒ½ä½“åä½œçš„æ™ºèƒ½ä½“æ¨¡å¼ï¼ˆAgent Modeï¼‰ä»¥åŠåŸºäºé“¾å¼æ€ç»´ï¼ˆChain-of-Thoughtï¼‰çš„ç»Ÿä¸€æ¨¡å‹æ¨¡å¼ï¼ˆLLM Modeï¼‰ï¼Œå®ç°äº†æ¨ç†æ•ˆç‡ä¸è¡¨ç°çš„å¹³è¡¡ã€‚å¤šé¡¹å®éªŒè¯„ä¼°è¯æ˜ï¼ŒPsychÄ“Chat åœ¨æƒ…æ„Ÿæ´å¯ŸåŠ›ã€å®‰å…¨æ§åˆ¶åŠå¯¹è¯è´¨é‡ä¸Šå‡æ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œä¸ºæ„å»ºæ›´å…·å…±æƒ…åŠ›ä¸”å®‰å…¨çš„æ™ºèƒ½å¿ƒç†å’¨è¯¢ç³»ç»Ÿæä¾›äº†æ–°è·¯å¾„ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2601.12392v1",
      "published_date": "2026-01-18 13:06:13 UTC",
      "updated_date": "2026-01-18 13:06:13 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-22T23:31:46.281653+00:00"
    },
    {
      "arxiv_id": "2601.12389v1",
      "title": "NADIR: Differential Attention Flow for Non-Autoregressive Transliteration in Indic Languages",
      "title_zh": "NADIRï¼šé¢å‘å°åº¦è¯­ç³»éè‡ªå›å½’éŸ³è¯‘çš„å·®åˆ†æ³¨æ„åŠ›æµ",
      "authors": [
        "Lakshya Tomar",
        "Vinayak Abrol",
        "Puneet Agarwal"
      ],
      "abstract": "In this work, we argue that not all sequence-to-sequence tasks require the strong inductive biases of autoregressive (AR) models. Tasks like multilingual transliteration, code refactoring, grammatical correction or text normalization often rely on local dependencies where the full modeling capacity of AR models can be overkill, creating a trade-off between their high accuracy and high inference latency. While non-autoregressive (NAR) models offer speed, they typically suffer from hallucinations and poor length control. To explore this trade-off, we focus on the multilingual transliteration task in Indic languages and introduce NADIR, a novel NAR architecture designed to strike a balance between speed and accuracy. NADIR integrates a Differential Transformer and a Mixture-of-Experts mechanism, enabling it to robustly model complex character mappings without sequential dependencies. NADIR achieves over a 13x speed-up compared to the state-of-the-art AR baseline. It maintains a competitive mean Character Error Rate of 15.78%, compared to 14.44% for the AR model and 21.88% for a standard NAR equivalent. Importantly, NADIR reduces Repetition errors by 49.53%, Substitution errors by 24.45%, Omission errors by 32.92%, and Insertion errors by 16.87%. This work provides a practical blueprint for building fast and reliable NAR systems, effectively bridging the gap between AR accuracy and the demands of real-time, large-scale deployment.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å°åœ°è¯­ç³»(Indic languages)çš„éŸ³è¯‘ä»»åŠ¡æå‡ºäº†NADIRï¼Œä¸€ç§æ—¨åœ¨å¹³è¡¡æ¨ç†é€Ÿåº¦ä¸å‡†ç¡®æ€§çš„éè‡ªå›å½’(Non-autoregressive, NAR)æ¶æ„ã€‚NADIRé›†æˆäº†Differential Transformerå’Œæ··åˆä¸“å®¶æœºåˆ¶(Mixture-of-Experts)ï¼Œèƒ½å¤Ÿåœ¨æ— éœ€åºåˆ—ä¾èµ–çš„æƒ…å†µä¸‹ç¨³å¥åœ°å»ºæ¨¡å¤æ‚çš„å­—ç¬¦æ˜ å°„ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒNADIRç›¸æ¯”æœ€å…ˆè¿›çš„è‡ªå›å½’(Autoregressive)åŸºçº¿å®ç°äº†è¶…è¿‡13å€çš„æ¨ç†åŠ é€Ÿï¼ŒåŒæ—¶ä¿æŒäº†15.78%çš„ç«äº‰åŠ›å¹³å‡å­—ç¬¦é”™è¯¯ç‡(Character Error Rate)ã€‚æ­¤å¤–ï¼Œè¯¥æ¨¡å‹æ˜¾è‘—é™ä½äº†é‡å¤(Repetition)å’Œé—æ¼(Omission)ç­‰å„ç±»é”™è¯¯ï¼Œæœ‰æ•ˆåœ°å¼¥è¡¥äº†éè‡ªå›å½’æ¨¡å‹ä¸è‡ªå›å½’æ¨¡å‹ä¹‹é—´çš„æ€§èƒ½å·®è·ï¼Œä¸ºå¤§è§„æ¨¡å®æ—¶éƒ¨ç½²æä¾›äº†é«˜æ•ˆå¯é çš„æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "Accepted at the AAAI Conference on Artificial Intelligence (AAAI 2026)",
      "pdf_url": "https://arxiv.org/pdf/2601.12389v1",
      "published_date": "2026-01-18 12:56:47 UTC",
      "updated_date": "2026-01-18 12:56:47 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-22T23:31:51.021478+00:00"
    },
    {
      "arxiv_id": "2601.12374v1",
      "title": "A Scalable Entity-Based Framework for Auditing Bias in LLMs",
      "title_zh": "ä¸€ç§åŸºäºå®ä½“çš„å¯æ‰©å±•å¤§è¯­è¨€æ¨¡å‹åè§å®¡è®¡æ¡†æ¶",
      "authors": [
        "Akram Elbouanani",
        "Aboubacar Tuo",
        "Adrian Popescu"
      ],
      "abstract": "Existing approaches to bias evaluation in large language models (LLMs) trade ecological validity for statistical control, relying on artificial prompts that poorly reflect real-world use, or on naturalistic tasks that lack scale and rigor. We introduce a scalable bias-auditing framework using named entities as probes to measure structural disparities in model behavior. We show that synthetic data reliably reproduces bias patterns observed in natural text, enabling large-scale analysis. Using this approach, we conduct the largest bias audit to date, comprising 1.9 billion data points across multiple entity types, tasks, languages, models, and prompting strategies. Our results reveal systematic biases: models penalize right-wing politicians, favor left-wing politicians, prefer Western and wealthy nations over the Global South, favor Western companies, and penalize firms in the defense and pharmaceutical sectors. While instruction tuning reduces bias, increasing model scale amplifies it, and prompting in Chinese or Russian does not attenuate Western-aligned preferences. These results indicate that LLMs should undergo rigorous auditing before deployment in high-stakes applications.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§å¯æ‰©å±•çš„åŸºäºå®ä½“çš„ LLMs åè§å®¡è®¡æ¡†æ¶ï¼Œåˆ©ç”¨å‘½åå®ä½“ (named entities) ä½œä¸ºæ¢æµ‹å™¨æ¥è¡¡é‡æ¨¡å‹è¡Œä¸ºä¸­çš„ç»“æ„æ€§å·®å¼‚ã€‚é€šè¿‡è¯æ˜åˆæˆæ•°æ®èƒ½å¯é åœ°å¤ç°è‡ªç„¶æ–‡æœ¬ä¸­çš„åè§æ¨¡å¼ï¼Œç ”ç©¶è€…è¿›è¡Œäº†è§„æ¨¡æœ€å¤§çš„åè§å®¡è®¡ï¼Œæ¶µç›– 19 äº¿ä¸ªæ•°æ®ç‚¹ï¼Œæ¨ªè·¨å¤šç§å®ä½“ç±»å‹ã€ä»»åŠ¡ã€è¯­è¨€å’Œæ¨¡å‹ã€‚å®éªŒæ­ç¤ºäº†ç³»ç»Ÿæ€§çš„åè§ï¼šæ¨¡å‹æ™®éåå¥½å·¦ç¿¼æ”¿æ²»äººç‰©ã€è¥¿æ–¹å‘è¾¾å›½å®¶åŠè¥¿æ–¹å…¬å¸ï¼Œè€Œå¯¹å³ç¿¼æ”¿æ²»äººç‰©ã€å…¨çƒå—æ–¹ (Global South) å›½å®¶ä»¥åŠå›½é˜²å’Œåˆ¶è¯è¡Œä¸šè¡¨ç°å‡ºè´Ÿé¢åå¥½ã€‚ç ”ç©¶è¿›ä¸€æ­¥å‘ç°ï¼Œè™½ç„¶æŒ‡ä»¤å¾®è°ƒ (instruction tuning) èƒ½å‡è½»åè§ï¼Œä½†æ¨¡å‹è§„æ¨¡ (scale) çš„å¢åŠ åè€Œä¼šæ”¾å¤§åè§ï¼Œä¸”ä½¿ç”¨ä¸­æ–‡æˆ–ä¿„æ–‡æç¤ºä¹Ÿæ— æ³•æ¶ˆé™¤å…¶è¥¿æ–¹å¯¼å‘çš„åå¥½ï¼Œå¼ºè°ƒäº†åœ¨å…³é”®åº”ç”¨éƒ¨ç½²å‰è¿›è¡Œä¸¥æ ¼å®¡è®¡çš„å¿…è¦æ€§ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2601.12374v1",
      "published_date": "2026-01-18 12:07:31 UTC",
      "updated_date": "2026-01-18 12:07:31 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-22T23:31:52.520840+00:00"
    },
    {
      "arxiv_id": "2601.12358v1",
      "title": "From Prompts to Pavement: LMMs-based Agentic Behavior-Tree Generation Framework for Autonomous Vehicles",
      "title_zh": "ä»æç¤ºåˆ°è·¯é¢ï¼šåŸºäº LMMs çš„è‡ªåŠ¨é©¾é©¶æ™ºèƒ½ä½“è¡Œä¸ºæ ‘ç”Ÿæˆæ¡†æ¶",
      "authors": [
        "Omar Y. Goba",
        "Ahmed Y. Gado",
        "Catherine M. Elias",
        "Ahmed Hussein"
      ],
      "abstract": "Autonomous vehicles (AVs) require adaptive behavior planners to navigate unpredictable, real-world environments safely. Traditional behavior trees (BTs) offer structured decision logic but are inherently static and demand labor-intensive manual tuning, limiting their applicability at SAE Level 5 autonomy. This paper presents an agentic framework that leverages large language models (LLMs) and multi-modal vision models (LVMs) to generate and adapt BTs on the fly. A specialized Descriptor agent applies chain-of-symbols prompting to assess scene criticality, a Planner agent constructs high-level sub-goals via in-context learning, and a Generator agent synthesizes executable BT sub-trees in XML format. Integrated into a CARLA+Nav2 simulation, our system triggers only upon baseline BT failure, demonstrating successful navigation around unexpected obstacles (e.g., street blockage) with no human intervention. Compared to a static BT baseline, this approach is a proof-of-concept that extends to diverse driving scenarios.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§åŸºäºå¤§å‹å¤šæ¨¡æ€æ¨¡å‹ï¼ˆLMMsï¼‰çš„æ™ºèƒ½ä½“è¡Œä¸ºæ ‘ç”Ÿæˆæ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³ä¼ ç»Ÿè¡Œä¸ºæ ‘ï¼ˆBehavior Treesï¼‰åœ¨è‡ªåŠ¨é©¾é©¶ä¸­å­˜åœ¨çš„é™æ€ã€éœ€æ‰‹åŠ¨è°ƒä¼˜ç­‰å±€é™æ€§ã€‚è¯¥æ¡†æ¶é€šè¿‡ Descriptor æ™ºèƒ½ä½“è¿›è¡Œåœºæ™¯è¯„ä¼°ï¼ŒPlanner æ™ºèƒ½ä½“æ„å»ºé«˜å±‚å­ç›®æ ‡ï¼Œå¹¶ç”± Generator æ™ºèƒ½ä½“è‡ªåŠ¨åˆæˆå¯æ‰§è¡Œçš„ XML æ ¼å¼è¡Œä¸ºæ ‘å­æ ‘ã€‚åœ¨ CARLA+Nav2 ä»¿çœŸç¯å¢ƒä¸‹çš„å®éªŒè¡¨æ˜ï¼Œè¯¥ç³»ç»Ÿåœ¨åŸºå‡†è¡Œä¸ºæ ‘å¤±æ•ˆçš„æƒ…å†µä¸‹ï¼Œèƒ½å¤Ÿæ— äººå·¥å¹²é¢„åœ°ç»•è¿‡çªå‘éšœç¢ç‰©ã€‚è¯¥æ–¹æ³•ä¸ºè‡ªåŠ¨é©¾é©¶è½¦è¾†åœ¨ä¸å¯é¢„æµ‹ç¯å¢ƒä¸­çš„è‡ªé€‚åº”è¡Œä¸ºè§„åˆ’æä¾›äº†ä¸€ç§é«˜æ•ˆçš„è§£å†³æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.RO"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2601.12358v1",
      "published_date": "2026-01-18 11:32:29 UTC",
      "updated_date": "2026-01-18 11:32:29 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-22T23:31:56.645377+00:00"
    },
    {
      "arxiv_id": "2601.12357v1",
      "title": "SimpleMatch: A Simple and Strong Baseline for Semantic Correspondence",
      "title_zh": "SimpleMatchï¼šä¸€ç§ç®€å•ä¸”å¼ºå¤§çš„è¯­ä¹‰å¯¹åº”åŸºå‡†",
      "authors": [
        "Hailing Jin",
        "Huiying Li"
      ],
      "abstract": "Recent advances in semantic correspondence have been largely driven by the use of pre-trained large-scale models. However, a limitation of these approaches is their dependence on high-resolution input images to achieve optimal performance, which results in considerable computational overhead. In this work, we address a fundamental limitation in current methods: the irreversible fusion of adjacent keypoint features caused by deep downsampling operations. This issue is triggered when semantically distinct keypoints fall within the same downsampled receptive field (e.g., 16x16 patches). To address this issue, we present SimpleMatch, a simple yet effective framework for semantic correspondence that delivers strong performance even at low resolutions. We propose a lightweight upsample decoder that progressively recovers spatial detail by upsampling deep features to 1/4 resolution, and a multi-scale supervised loss that ensures the upsampled features retain discriminative features across different spatial scales. In addition, we introduce sparse matching and window-based localization to optimize training memory usage and reduce it by 51%. At a resolution of 252x252 (3.3x smaller than current SOTA methods), SimpleMatch achieves superior performance with 84.1% PCK@0.1 on the SPair-71k benchmark. We believe this framework provides a practical and efficient baseline for future research in semantic correspondence. Code is available at: https://github.com/hailong23-jin/SimpleMatch.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† SimpleMatchï¼Œè¿™æ˜¯ä¸€ä¸ªé’ˆå¯¹è¯­ä¹‰å¯¹åº”ï¼ˆSemantic Correspondenceï¼‰ä»»åŠ¡çš„ç®€å•ä¸”å¼ºå¤§çš„åŸºçº¿æ¡†æ¶ã€‚é’ˆå¯¹ç°æœ‰æ–¹æ³•ä¾èµ–é«˜åˆ†è¾¨ç‡è¾“å…¥ä¸”åœ¨æ·±å±‚ä¸‹é‡‡æ ·è¿‡ç¨‹ä¸­å­˜åœ¨å…³é”®ç‚¹ç‰¹å¾â€œä¸å¯é€†èåˆâ€çš„é—®é¢˜ï¼ŒSimpleMatch å¼•å…¥äº†è½»é‡åŒ–ä¸Šé‡‡æ ·è§£ç å™¨ï¼ˆupsample decoderï¼‰å’Œå¤šå°ºåº¦ç›‘ç£æŸå¤±ï¼ˆmulti-scale supervised lossï¼‰ï¼Œæ—¨åœ¨ä½åˆ†è¾¨ç‡ä¸‹æœ‰æ•ˆæ¢å¤ç©ºé—´ç»†èŠ‚å¹¶ä¿æŒç‰¹å¾çš„åŒºåˆ†åº¦ã€‚æ­¤å¤–ï¼Œè¯¥æ¡†æ¶é€šè¿‡ç¨€ç–åŒ¹é…ï¼ˆsparse matchingï¼‰å’ŒåŸºäºçª—å£çš„å®šä½ï¼ˆwindow-based localizationï¼‰ä¼˜åŒ–äº†å†…å­˜ä½¿ç”¨ï¼Œä½¿è®­ç»ƒé˜¶æ®µçš„å†…å­˜å ç”¨é™ä½äº† 51%ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒSimpleMatch åœ¨ä»…ä¸º 252x252 çš„åˆ†è¾¨ç‡ä¸‹ï¼ˆæ¯”å½“å‰ SOTA æ–¹æ³•å° 3.3 å€ï¼‰ï¼Œäº SPair-71k åŸºå‡†æµ‹è¯•ä¸­è¾¾åˆ°äº† 84.1% çš„ PCK@0.1ï¼Œä¸ºè¯¥é¢†åŸŸæä¾›äº†ä¸€ä¸ªå…¼å…·æ€§èƒ½ä¸æ•ˆç‡çš„å®ç”¨åŸºçº¿ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2601.12357v1",
      "published_date": "2026-01-18 11:31:46 UTC",
      "updated_date": "2026-01-18 11:31:46 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-22T23:31:58.588819+00:00"
    },
    {
      "arxiv_id": "2601.12349v1",
      "title": "Zero-Permission Manipulation: Can We Trust Large Multimodal Model Powered GUI Agents?",
      "title_zh": "é›¶æƒé™æ“çºµï¼šå¤šæ¨¡æ€å¤§æ¨¡å‹é©±åŠ¨çš„ GUI æ™ºèƒ½ä½“æ˜¯å¦å€¼å¾—ä¿¡ä»»ï¼Ÿ",
      "authors": [
        "Yi Qian",
        "Kunwei Qian",
        "Xingbang He",
        "Ligeng Chen",
        "Jikang Zhang",
        "Tiantai Zhang",
        "Haiyang Wei",
        "Linzhang Wang",
        "Hao Wu",
        "Bing Mao"
      ],
      "abstract": "Large multimodal model powered GUI agents are emerging as high-privilege operators on mobile platforms, entrusted with perceiving screen content and injecting inputs. However, their design operates under the implicit assumption of Visual Atomicity: that the UI state remains invariant between observation and action. We demonstrate that this assumption is fundamentally invalid in Android, creating a critical attack surface.\n  We present Action Rebinding, a novel attack that allows a seemingly-benign app with zero dangerous permissions to rebind an agent's execution. By exploiting the inevitable observation-to-action gap inherent in the agent's reasoning pipeline, the attacker triggers foreground transitions to rebind the agent's planned action toward the target app. We weaponize the agent's task-recovery logic and Android's UI state preservation to orchestrate programmable, multi-step attack chains. Furthermore, we introduce an Intent Alignment Strategy (IAS) that manipulates the agent's reasoning process to rationalize UI states, enabling it to bypass verification gates (e.g., confirmation dialogs) that would otherwise be rejected.\n  We evaluate Action Rebinding Attacks on six widely-used Android GUI agents across 15 tasks. Our results demonstrate a 100% success rate for atomic action rebinding and the ability to reliably orchestrate multi-step attack chains. With IAS, the success rate in bypassing verification gates increases (from 0% to up to 100%). Notably, the attacker application requires no sensitive permissions and contains no privileged API calls, achieving a 0% detection rate across malware scanners (e.g., VirusTotal). Our findings reveal a fundamental architectural flaw in current agent-OS integration and provide critical insights for the secure design of future agent systems. To access experimental logs and demonstration videos, please contact yi_qian@smail.nju.edu.cn.",
      "tldr_zh": "---\n\n### è®ºæ–‡ TLDR æ‘˜è¦ ğŸ“„\n\nè¯¥ç ”ç©¶æ­ç¤ºäº†åŸºäºå¤§å‹å¤šæ¨¡æ€æ¨¡å‹(LMMs)çš„GUIæ™ºèƒ½ä½“åœ¨Androidå¹³å°ä¸Šå­˜åœ¨çš„ä¸¥é‡å®‰å…¨æ¼æ´ï¼ŒæŒ‘æˆ˜äº†â€œè§†è§‰åŸå­æ€§â€(Visual Atomicity)è¿™ä¸€æ ¸å¿ƒè®¾è®¡å‡è®¾ã€‚ç ”ç©¶è€…æå‡ºäº†ä¸€ç§åä¸ºAction Rebindingçš„æ–°å‹æ”»å‡»æ–¹æ³•ï¼Œåˆ©ç”¨æ™ºèƒ½ä½“ä»â€œè§‚å¯Ÿâ€åˆ°â€œæ‰§è¡Œâ€ä¹‹é—´çš„æ—¶é—´é—´éš™ï¼Œé€šè¿‡æ— éœ€ä»»ä½•æ•æ„Ÿæƒé™çš„åº”ç”¨æˆåŠŸé‡å®šå‘æ™ºèƒ½ä½“çš„æ“ä½œé€»è¾‘ã€‚æ­¤å¤–ï¼Œç ”ç©¶å¼•å…¥äº†æ„å›¾å¯¹é½ç­–ç•¥(Intent Alignment Strategy, IAS)ï¼Œé€šè¿‡æ“çºµæ™ºèƒ½ä½“çš„æ¨ç†è¿‡ç¨‹ä½¿å…¶è‡ªåŠ¨â€œåˆç†åŒ–â€å¼‚å¸¸çŠ¶æ€ï¼Œä»è€Œç»•è¿‡ç¡®è®¤å¯¹è¯æ¡†ç­‰å®‰å…¨éªŒè¯ç¯èŠ‚ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯¥æ”»å‡»åœ¨6ç§ä¸»æµAndroid GUIæ™ºèƒ½ä½“ä¸Šè¾¾åˆ°äº†100%çš„åŠ¨ä½œé‡å®šå‘æˆåŠŸç‡ï¼Œä¸”èƒ½å®Œå…¨è§„é¿VirusTotalç­‰æ¶æ„è½¯ä»¶æ‰«æå™¨çš„æ£€æµ‹ã€‚æ­¤é¡¹å·¥ä½œå‘ç°äº†å½“å‰æ™ºèƒ½ä½“ä¸æ“ä½œç³»ç»Ÿé›†æˆä¸­çš„æ ¹æœ¬æ€§æ¶æ„ç¼ºé™·ï¼Œä¸ºæœªæ¥å®‰å…¨æ™ºèƒ½ä½“ç³»ç»Ÿçš„è®¾è®¡æä¾›äº†å…³é”®æ´å¯Ÿã€‚\n\n---\n\nå…³äºè¿™é¡¹å…³äº GUI æ™ºèƒ½ä½“å®‰å…¨æ€§çš„ç ”ç©¶ï¼Œæ‚¨æ˜¯å¦éœ€è¦æˆ‘ä¸ºæ‚¨è¿›ä¸€æ­¥åˆ†æå…¶é˜²å¾¡å¯¹ç­–ï¼Œæˆ–è€…å¯¹æ¯”å…¶ä»–ç±»ä¼¼çš„å®‰å…¨æ¼æ´ç ”ç©¶ï¼Ÿ",
      "categories": [
        "cs.CR",
        "cs.AI",
        "cs.SE"
      ],
      "primary_category": "cs.CR",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2601.12349v1",
      "published_date": "2026-01-18 10:54:54 UTC",
      "updated_date": "2026-01-18 10:54:54 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-22T23:32:03.312620+00:00"
    },
    {
      "arxiv_id": "2601.12343v1",
      "title": "How Well Do LLMs Predict Human Behavior? A Measure of their Pretrained Knowledge",
      "title_zh": "å¤§è¯­è¨€æ¨¡å‹é¢„æµ‹äººç±»è¡Œä¸ºçš„èƒ½åŠ›è¯„ä¼°ï¼šå¯¹å…¶é¢„è®­ç»ƒçŸ¥è¯†çš„é‡åŒ–è¡¡é‡",
      "authors": [
        "Wayne Gao",
        "Sukjin Han",
        "Annie Liang"
      ],
      "abstract": "Large language models (LLMs) are increasingly used to predict human behavior. We propose a measure for evaluating how much knowledge a pretrained LLM brings to such a prediction: its equivalent sample size, defined as the amount of task-specific data needed to match the predictive accuracy of the LLM. We estimate this measure by comparing the prediction error of a fixed LLM in a given domain to that of flexible machine learning models trained on increasing samples of domain-specific data. We further provide a statistical inference procedure by developing a new asymptotic theory for cross-validated prediction error. Finally, we apply this method to the Panel Study of Income Dynamics. We find that LLMs encode considerable predictive information for some economic variables but much less for others, suggesting that their value as substitutes for domain-specific data differs markedly across settings.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§è¡¡é‡é¢„è®­ç»ƒ Large language models (LLMs) åœ¨é¢„æµ‹äººç±»è¡Œä¸ºæ–¹é¢æ‰€å«çŸ¥è¯†çš„æ–°æŒ‡æ ‡ï¼šç­‰æ•ˆæ ·æœ¬é‡ (equivalent sample size)ï¼Œå³è¾¾åˆ°ä¸ LLMs ç›¸åŒçš„é¢„æµ‹å‡†ç¡®ç‡æ‰€éœ€çš„ä»»åŠ¡ç‰¹å®šæ•°æ®é‡ã€‚ç ”ç©¶äººå‘˜é€šè¿‡æ¯”è¾ƒ LLMs ä¸åœ¨ä¸æ–­å¢åŠ çš„é¢†åŸŸæ•°æ®ä¸Šè®­ç»ƒçš„æœºå™¨å­¦ä¹ æ¨¡å‹çš„é¢„æµ‹è¯¯å·®æ¥ä¼°ç®—è¯¥æŒ‡æ ‡ï¼Œå¹¶ä¸ºäº¤å‰éªŒè¯é¢„æµ‹è¯¯å·®å¼€å‘äº†æ–°çš„æ¸è¿‘ç†è®º (asymptotic theory) ä»¥è¿›è¡Œç»Ÿè®¡æ¨æ–­ã€‚åœ¨å¯¹ Panel Study of Income Dynamics (PSID) çš„åº”ç”¨ä¸­å‘ç°ï¼ŒLLMs å¯¹æŸäº›ç»æµå˜é‡å…·æœ‰æ˜¾è‘—çš„é¢„æµ‹èƒ½åŠ›ï¼Œä½†åœ¨å…¶ä»–å˜é‡ä¸Šè¡¨ç°æœ‰é™ã€‚è¿™è¡¨æ˜ LLMs ä½œä¸ºé¢†åŸŸç‰¹å®šæ•°æ®æ›¿ä»£å“çš„ä»·å€¼åœ¨ä¸åŒåº”ç”¨åœºæ™¯ä¸‹å­˜åœ¨æ˜¾è‘—å·®å¼‚ã€‚",
      "categories": [
        "econ.EM",
        "cs.AI",
        "stat.ML"
      ],
      "primary_category": "econ.EM",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2601.12343v1",
      "published_date": "2026-01-18 10:28:54 UTC",
      "updated_date": "2026-01-18 10:28:54 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-22T23:32:12.025235+00:00"
    },
    {
      "arxiv_id": "2601.12341v1",
      "title": "Time-Continuous Modeling for Temporal Affective Pattern Recognition in LLMs",
      "title_zh": "LLMs ä¸­æ—¶åºæƒ…æ„Ÿæ¨¡å¼è¯†åˆ«çš„æ—¶é—´è¿ç»­å»ºæ¨¡",
      "authors": [
        "Rezky Kam",
        "Coddy N. Siswanto"
      ],
      "abstract": "This paper introduces a dataset and conceptual framework for LLMs to mimic real world emotional dynamics through time and in-context learning leveraging physics-informed neural network, opening a possibility for interpretable dialogue modeling.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ä¸ªæ•°æ®é›†å’Œæ¦‚å¿µæ¡†æ¶ï¼Œæ—¨åœ¨è®©å¤§è¯­è¨€æ¨¡å‹(LLMs)èƒ½å¤Ÿåˆ©ç”¨ä¸Šä¸‹æ–‡å­¦ä¹ (In-context learning)å’Œç‰©ç†ä¿¡æ¯ç¥ç»ç½‘ç»œ(Physics-informed neural network)æ¥æ¨¡æ‹ŸçœŸå®ä¸–ç•Œä¸­éšæ—¶é—´å˜åŒ–çš„æƒ…æ„ŸåŠ¨æ€ã€‚é€šè¿‡å¼•å…¥æ—¶é—´è¿ç»­å»ºæ¨¡(Time-continuous modeling)ï¼Œè¯¥æ¡†æ¶å®ç°äº†å¯¹æ—¶é—´æƒ…æ„Ÿæ¨¡å¼çš„æœ‰æ•ˆè¯†åˆ«ã€‚è¿™ä¸€è¿›å±•ä¸ºæ„å»ºå¯è§£é‡Šçš„å¯¹è¯å»ºæ¨¡(Interpretable dialogue modeling)å¼€è¾Ÿäº†æ–°é€”å¾„ï¼Œæ˜¾è‘—æå‡äº†å¯¹è¯ç³»ç»Ÿä¸­æƒ…æ„Ÿæ¼”å˜çš„å¯ç†è§£æ€§ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.ET",
        "cs.HC",
        "eess.SY"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2601.12341v1",
      "published_date": "2026-01-18 10:16:26 UTC",
      "updated_date": "2026-01-18 10:16:26 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-22T23:32:23.598570+00:00"
    },
    {
      "arxiv_id": "2601.12338v1",
      "title": "Actionable Advice from Reviews via Mixture of LoRA Experts: A Two-LLM Pipeline for Issue Extraction and Business Recommendations",
      "title_zh": "åŸºäº LoRA ä¸“å®¶æ··åˆæ¨¡å‹ä»è¯„è®ºä¸­æå–å¯æ“ä½œå»ºè®®ï¼šä¸€ç§ç”¨äºé—®é¢˜æå–ä¸å•†ä¸šå»ºè®®çš„åŒå¤§è¯­è¨€æ¨¡å‹æµæ°´çº¿",
      "authors": [
        "Kartikey Singh Bhandari",
        "Manav Ganesh",
        "Yashwant Viswanathan",
        "Archit Agrawal",
        "Dhruv Kumar",
        "Pratik Narang"
      ],
      "abstract": "Customer reviews contain detailed, domain specific signals about service failures and user expectations, but converting this unstructured feedback into actionable business decisions remains difficult. We study review-to-action generation: producing concrete, implementable recommendations grounded in review text. We propose a modular two-LLM framework in which an Issue model extracts salient issues and assigns coarse themes, and an Advice model generates targeted operational fixes conditioned on the extracted issue representation. To enable specialization without expensive full fine-tuning, we adapt the Advice model using a mixture of LoRA experts strategy: multiple low-rank adapters are trained and a lightweight gating mechanism performs token-level expert mixing at inference, combining complementary expertise across issue types. We construct synthetic review-issue-advice triples from Yelp reviews (airlines and restaurants) to supervise training, and evaluate recommendations using an eight dimension operational rubric spanning actionability, specificity, feasibility, expected impact, novelty, non-redundancy, bias, and clarity. Across both domains, our approach consistently outperforms prompting-only and single-adapter baselines, yielding higher actionability and specificity while retaining favorable efficiency-quality trade-offs.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ä»éç»“æ„åŒ–å®¢æˆ·è¯„è®ºä¸­æå–å¯æ‰§è¡Œä¸šåŠ¡å»ºè®®çš„æŒ‘æˆ˜ï¼Œæå‡ºäº†ä¸€ç§åŒå¤§è¯­è¨€æ¨¡å‹ï¼ˆTwo-LLMï¼‰æ¡†æ¶ã€‚è¯¥æ¡†æ¶ç”±è´Ÿè´£æå–æ˜¾è‘—é—®é¢˜å¹¶åˆ†é…ä¸»é¢˜çš„ Issue modelï¼Œä»¥åŠåŸºäºæå–å‡ºçš„é—®é¢˜ç”Ÿæˆé’ˆå¯¹æ€§è¿è¥æ”¹è¿›å»ºè®®çš„ Advice model ç»„æˆã€‚ä¸ºäº†åœ¨é¿å…æ˜‚è´µçš„å…¨é‡å¾®è°ƒçš„åŒæ—¶å®ç°æ¨¡å‹ä¸“ä¸šåŒ–ï¼ŒAdvice model é‡‡ç”¨äº† Mixture of LoRA Experts ç­–ç•¥ï¼Œé€šè¿‡è½»é‡çº§é—¨æ§æœºåˆ¶åœ¨æ¨ç†é˜¶æ®µå®ç°ä»¤ç‰Œçº§çš„ä¸“å®¶æ··åˆã€‚ç ”ç©¶å›¢é˜Ÿåˆ©ç”¨ Yelp è¯„è®ºæ•°æ®æ„å»ºäº†ç›‘ç£è®­ç»ƒé›†ï¼Œå¹¶ä»å¯è¡Œæ€§ã€å…·ä½“æ€§ç­‰å…«ä¸ªç»´åº¦å¯¹ç”Ÿæˆå»ºè®®è¿›è¡Œäº†è¯„ä¼°ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨ Airline å’Œ Restaurant é¢†åŸŸå‡æ˜¾è‘—ä¼˜äº Prompting-only å’Œ Single-adapter åŸºçº¿æ¨¡å‹ï¼Œåœ¨æå‡å»ºè®®å¯æ‰§è¡Œæ€§ä¸å…·ä½“æ€§çš„åŒæ—¶ï¼Œå®ç°äº†ä¼˜å¼‚çš„æ•ˆç‡ä¸è´¨é‡æƒè¡¡ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2601.12338v1",
      "published_date": "2026-01-18 10:11:29 UTC",
      "updated_date": "2026-01-18 10:11:29 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-22T23:32:26.329070+00:00"
    },
    {
      "arxiv_id": "2601.12331v1",
      "title": "Efficient Privacy-Preserving Retrieval Augmented Generation with Distance-Preserving Encryption",
      "title_zh": "åŸºäºè·ç¦»ä¿æŒåŠ å¯†çš„é«˜æ•ˆéšç§ä¿æŠ¤æ£€ç´¢å¢å¼ºç”Ÿæˆ",
      "authors": [
        "Huanyi Ye",
        "Jiale Guo",
        "Ziyao Liu",
        "Kwok-Yan Lam"
      ],
      "abstract": "RAG has emerged as a key technique for enhancing response quality of LLMs without high computational cost. In traditional architectures, RAG services are provided by a single entity that hosts the dataset within a trusted local environment. However, individuals or small organizations often lack the resources to maintain data storage servers, leading them to rely on outsourced cloud storage. This dependence on untrusted third-party services introduces privacy risks. Embedding-based retrieval mechanisms, commonly used in RAG systems, are vulnerable to privacy leakage such as vector-to-text reconstruction attacks and structural leakage via vector analysis. Several privacy-preserving RAG techniques have been proposed but most existing approaches rely on partially homomorphic encryption, which incurs substantial computational overhead. To address these challenges, we propose an efficient privacy-preserving RAG framework (ppRAG) tailored for untrusted cloud environments that defends against vector-to-text attack, vector analysis, and query analysis. We propose Conditional Approximate Distance-Comparison-Preserving Symmetric Encryption (CAPRISE) that encrypts embeddings while still allowing the cloud to compute similarity between an encrypted query and the encrypted database embeddings. CAPRISE preserves only the relative distance ordering between the encrypted query and each encrypted database embedding, without exposing inter-database distances, thereby enhancing both privacy and efficiency. To mitigate query analysis, we introduce DP by perturbing the query embedding prior to encryption, preventing the cloud from inferring sensitive patterns. Experimental results show that ppRAG achieves efficient processing throughput, high retrieval accuracy, strong privacy guarantees, making it a practical solution for resource-constrained users seeking secure cloud-augmented LLMs.",
      "tldr_zh": "---\n\n### è®ºæ–‡æ‘˜è¦ TLDR ğŸ“„\n\nè¯¥ç ”ç©¶æå‡ºäº† ppRAG æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³ Retrieval Augmented Generation (RAG) åœ¨å¤–åŒ…äº‘å­˜å‚¨ä¸­é¢ä¸´çš„éšç§é£é™©ï¼ˆå¦‚ vector-to-text é‡æ„æ”»å‡»ï¼‰åŠä¼ ç»ŸåŠ å¯†æ–¹æ³•è®¡ç®—å¼€é”€è¿‡å¤§çš„æŒ‘æˆ˜ã€‚å…¶æ ¸å¿ƒæ–¹æ¡ˆ CAPRISE é‡‡ç”¨æ¡ä»¶è¿‘ä¼¼è·ç¦»æ¯”è¾ƒä¿ç•™å¯¹ç§°åŠ å¯†æŠ€æœ¯ï¼Œåœ¨å…è®¸äº‘ç«¯æ‰§è¡Œç›¸ä¼¼åº¦æ£€ç´¢çš„åŒæ—¶ï¼Œä»…ä¿ç•™æŸ¥è¯¢ä¸æ•°æ®é—´çš„ç›¸å¯¹è·ç¦»é¡ºåºï¼Œä»è€Œæœ‰æ•ˆé˜²æ­¢ç»“æ„æ€§æ³„éœ²ã€‚æ­¤å¤–ï¼Œæ¡†æ¶ç»“åˆäº† Differential Privacy (DP) æŠ€æœ¯å¯¹æŸ¥è¯¢å‘é‡è¿›è¡Œæ‰°åŠ¨ï¼Œä»¥é˜²å¾¡æ•æ„Ÿçš„æŸ¥è¯¢æ¨¡å¼åˆ†æã€‚å®éªŒè¯æ˜ï¼ŒppRAG åœ¨æä¾›å¼ºéšç§ä¿éšœå’Œé«˜æ£€ç´¢å‡†ç¡®ç‡çš„åŒæ—¶ï¼Œå…·å¤‡æé«˜çš„å¤„ç†ååé‡ï¼Œä¸ºèµ„æºå—é™ç”¨æˆ·å®‰å…¨åˆ©ç”¨äº‘ç«¯ Large Language Models (LLMs) æä¾›äº†å®ç”¨çš„è§£å†³æ–¹æ¡ˆã€‚\n\n---\n\næ˜¯å¦éœ€è¦æˆ‘å¯¹è®ºæ–‡ä¸­çš„å…·ä½“å®éªŒæ•°æ®æˆ– CAPRISE åŠ å¯†ç®—æ³•çš„æ•°å­¦åŸç†è¿›è¡Œæ›´æ·±å…¥çš„è§£è¯»ï¼Ÿ",
      "categories": [
        "cs.CR",
        "cs.AI"
      ],
      "primary_category": "cs.CR",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2601.12331v1",
      "published_date": "2026-01-18 09:29:50 UTC",
      "updated_date": "2026-01-18 09:29:50 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-22T23:32:32.553142+00:00"
    },
    {
      "arxiv_id": "2601.12330v1",
      "title": "IceWatch: Forecasting Glacial Lake Outburst Floods (GLOFs) using Multimodal Deep Learning",
      "title_zh": "IceWatchï¼šåŸºäºå¤šæ¨¡æ€æ·±åº¦å­¦ä¹ çš„å†°æ¹–æºƒå†³æ´ªæ°´ï¼ˆGLOFsï¼‰é¢„æµ‹",
      "authors": [
        "Zuha Fatima",
        "Muhammad Anser Sohaib",
        "Muhammad Talha",
        "Ayesha Kanwal",
        "Sidra Sultana",
        "Nazia Perwaiz"
      ],
      "abstract": "Glacial Lake Outburst Floods (GLOFs) pose a serious threat in high mountain regions. They are hazardous to communities, infrastructure, and ecosystems further downstream. The classical methods of GLOF detection and prediction have so far mainly relied on hydrological modeling, threshold-based lake monitoring, and manual satellite image analysis. These approaches suffer from several drawbacks: slow updates, reliance on manual labor, and losses in accuracy when clouds interfere and/or lack on-site data. To tackle these challenges, we present IceWatch: a novel deep learning framework for GLOF prediction that incorporates both spatial and temporal perspectives. The vision component, RiskFlow, of IceWatch deals with Sentinel-2 multispectral satellite imagery using a CNN-based classifier and predicts GLOF events based on the spatial patterns of snow, ice, and meltwater. Its tabular counterpart confirms this prediction by considering physical dynamics. TerraFlow models glacier velocity from NASA ITS_LIVE time series while TempFlow forecasts near-surface temperature from MODIS LST records; both are trained on long-term observational archives and integrated via harmonized preprocessing and synchronization to enable multimodal, physics-informed GLOF prediction. Both together provide cross-validation, which will improve the reliability and interpretability of GLOF detection. This system ensures strong predictive performance, rapid data processing for real-time use, and robustness to noise and missing information. IceWatch paves the way for automatic, scalable GLOF warning systems. It also holds potential for integration with diverse sensor inputs and global glacier monitoring activities.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† IceWatchï¼Œä¸€ç§ç”¨äºé¢„æµ‹å†°æ¹–æºƒå†³æ´ªæ°´ (Glacial Lake Outburst Floods, GLOFs) çš„å¤šæ¨¡æ€æ·±åº¦å­¦ä¹ æ¡†æ¶ã€‚è¯¥æ¡†æ¶é›†æˆäº†å¤„ç† Sentinel-2 å¤šå…‰è°±å½±åƒçš„è§†è§‰ç»„ä»¶ RiskFlowï¼Œä»¥åŠåˆ†åˆ«æ¨¡æ‹Ÿå†°å·é€Ÿåº¦ä¸åœ°è¡¨æ¸©åº¦çš„ TerraFlow å’Œ TempFlow ç»„ä»¶ã€‚é€šè¿‡ç»“åˆå·ç§¯ç¥ç»ç½‘ç»œ (CNN) çš„ç©ºé—´åˆ†æä¸ç‰©ç†åŠ¨æ€æ•°æ®çš„äº¤å‰éªŒè¯ï¼ŒIceWatch æ˜¾è‘—æå‡äº†é¢„æµ‹çš„å¯é æ€§ã€å¯è§£é‡Šæ€§ä»¥åŠå¯¹å™ªå£°å’Œç¼ºå¤±æ•°æ®çš„é²æ£’æ€§ã€‚è¯¥ç³»ç»Ÿå®ç°äº†è‡ªåŠ¨åŒ–ä¸”å¯æ‰©å±•çš„å®æ—¶ç›‘æµ‹ï¼Œä¸ºé«˜å±±åœ°åŒºå»ºç«‹é«˜æ•ˆçš„ GLOF é¢„è­¦ç³»ç»Ÿå¥ å®šäº†æŠ€æœ¯åŸºç¡€ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2601.12330v1",
      "published_date": "2026-01-18 09:29:40 UTC",
      "updated_date": "2026-01-18 09:29:40 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-22T23:32:32.287754+00:00"
    },
    {
      "arxiv_id": "2601.12327v1",
      "title": "The Expert Validation Framework (EVF): Enabling Domain Expert Control in AI Engineering",
      "title_zh": "ä¸“å®¶éªŒè¯æ¡†æ¶ (EVF)ï¼šå®ç°äººå·¥æ™ºèƒ½å·¥ç¨‹ä¸­çš„é¢†åŸŸä¸“å®¶ç®¡æ§",
      "authors": [
        "Lucas Gren",
        "Felix Dobslaw"
      ],
      "abstract": "Generative AI (GenAI) systems promise to transform knowledge work by automating a range of tasks, yet their deployment in enterprise settings remains hindered by the lack of systematic quality assurance mechanisms. We present an Expert Validation Framework that places domain experts at the center of building software with GenAI components, enabling them to maintain authoritative control over system behavior through structured specification, testing, validation, and continuous monitoring processes. Our framework addresses the critical gap between AI capabilities and organizational trust by establishing a rigorous, expert-driven methodology for ensuring quality across diverse GenAI applications. Through a four-stage implementation process encompassing specification, system creation, validation, and production monitoring, the framework enables organizations to leverage GenAI capabilities while maintaining expert oversight and quality standards.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸“å®¶éªŒè¯æ¡†æ¶ (Expert Validation Framework, EVF)ï¼Œæ—¨åœ¨è§£å†³ç”Ÿæˆå¼ AI (GenAI) åœ¨ä¼ä¸šåº”ç”¨ä¸­å› ç¼ºä¹ç³»ç»Ÿæ€§è´¨é‡ä¿è¯æœºåˆ¶è€Œéš¾ä»¥éƒ¨ç½²çš„é—®é¢˜ã€‚è¯¥æ¡†æ¶å°†é¢†åŸŸä¸“å®¶ç½®äº AI å·¥ç¨‹çš„æ ¸å¿ƒï¼Œé€šè¿‡ç»“æ„åŒ–çš„è§„èŒƒ (specification)ã€æµ‹è¯•ã€éªŒè¯å’ŒæŒç»­ç›‘æ§æµç¨‹ï¼Œä½¿ä¸“å®¶èƒ½å¤Ÿä¿æŒå¯¹ç³»ç»Ÿè¡Œä¸ºçš„æƒå¨æ§åˆ¶ã€‚EVF åŒ…å«ä»è§„èŒƒã€ç³»ç»Ÿåˆ›å»ºåˆ°éªŒè¯åŠç”Ÿäº§ç›‘æ§çš„å››ä¸ªå®æ–½é˜¶æ®µï¼Œå»ºç«‹äº†ä¸€å¥—ä¸¥å¯†çš„ã€ä¸“å®¶é©±åŠ¨çš„æ–¹æ³•è®ºã€‚è¯¥æ¡†æ¶æœ‰æ•ˆåœ°å¼¥åˆäº† AI èƒ½åŠ›ä¸ç»„ç»‡ä¿¡ä»»ä¹‹é—´çš„é¸¿æ²Ÿï¼Œç¡®ä¿ä¼ä¸šåœ¨åˆ©ç”¨ GenAI æå‡æ•ˆç‡çš„åŒæ—¶ï¼Œèƒ½å¤Ÿç»´æŒä¸¥æ ¼çš„è´¨é‡æ ‡å‡†ä¸ä¸“ä¸šç›‘ç£ã€‚",
      "categories": [
        "cs.SE",
        "cs.AI"
      ],
      "primary_category": "cs.SE",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2601.12327v1",
      "published_date": "2026-01-18 09:20:21 UTC",
      "updated_date": "2026-01-18 09:20:21 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-22T23:32:35.886570+00:00"
    },
    {
      "arxiv_id": "2601.12323v1",
      "title": "MARO: Learning Stronger Reasoning from Social Interaction",
      "title_zh": "MAROï¼šé€šè¿‡ç¤¾ä¼šäº’åŠ¨å­¦ä¹ æ›´å¼ºçš„æ¨ç†èƒ½åŠ›",
      "authors": [
        "Yin Cai",
        "Zhouhong Gu",
        "Juntao Zhang",
        "Ping Chen"
      ],
      "abstract": "Humans face countless scenarios that require reasoning and judgment in daily life. However, existing large language model training methods primarily allow models to learn from existing textual content or solve predetermined problems, lacking experience in real scenarios involving interaction, negotiation, and competition with others. To address this, this paper proposes Multi-Agent Reward Optimization (MARO), a method that enables large language models (LLMs) to acquire stronger reasoning abilities by learning and practicing in multi-agent social environments. Specifically, MARO first addresses the sparse learning signal problem by decomposing final success or failure outcomes into each specific behavior during the interaction process; second, it handles the uneven role distribution problem by balancing the training sample weights of different roles; finally, it addresses environmental instability issues by directly evaluating the utility of each behavior. Experimental results demonstrate that MARO not only achieves significant improvements in social reasoning capabilities, but also that the abilities acquired through social simulation learning can effectively transfer to other tasks such as mathematical reasoning and instruction following. This reveals the tremendous potential of multi-agent social learning in enhancing the general reasoning capabilities of LLMs.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† MARO (Multi-Agent Reward Optimization)ï¼Œä¸€ç§é€šè¿‡åœ¨å¤šæ™ºèƒ½ä½“ç¤¾äº¤ç¯å¢ƒï¼ˆmulti-agent social environmentsï¼‰ä¸­å­¦ä¹ ä¸å®è·µæ¥å¢å¼ºå¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰æ¨ç†èƒ½åŠ›çš„æ–¹æ³•ã€‚MARO é€šè¿‡å°†æœ€ç»ˆæˆè´¥åˆ†è§£ä¸ºå…·ä½“è¡Œä¸ºæ¥è§£å†³ç¨€ç–å­¦ä¹ ä¿¡å·ï¼ˆsparse learning signalï¼‰é—®é¢˜ï¼Œå¹¶åˆ©ç”¨å¹³è¡¡è§’è‰²æƒé‡åŠç›´æ¥è¯„ä¼°è¡Œä¸ºæ•ˆç”¨çš„ç­–ç•¥åº”å¯¹ç¯å¢ƒä¸ç¨³å®šæ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒMARO ä¸ä»…æ˜¾è‘—æå‡äº†ç¤¾äº¤æ¨ç†èƒ½åŠ›ï¼Œä¸”æ‰€ä¹ å¾—çš„èƒ½åŠ›å¯æœ‰æ•ˆè¿ç§»è‡³æ•°å­¦æ¨ç†ï¼ˆmathematical reasoningï¼‰å’ŒæŒ‡ä»¤éµå¾ªï¼ˆinstruction followingï¼‰ç­‰ä»»åŠ¡ã€‚è¿™æ­ç¤ºäº†å¤šæ™ºèƒ½ä½“ç¤¾äº¤å­¦ä¹ åœ¨æå‡å¤§è¯­è¨€æ¨¡å‹é€šç”¨æ¨ç†èƒ½åŠ›ï¼ˆgeneral reasoning capabilitiesï¼‰æ–¹é¢çš„å·¨å¤§æ½œåŠ›ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2601.12323v1",
      "published_date": "2026-01-18 09:10:08 UTC",
      "updated_date": "2026-01-18 09:10:08 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-22T23:32:38.343454+00:00"
    },
    {
      "arxiv_id": "2601.12318v1",
      "title": "Beyond Human Annotation: Recent Advances in Data Generation Methods for Document Intelligence",
      "title_zh": "è¶…è¶Šäººå·¥æ ‡æ³¨ï¼šæ–‡æ¡£æ™ºèƒ½æ•°æ®ç”Ÿæˆæ–¹æ³•çš„æœ€æ–°è¿›å±•",
      "authors": [
        "Dehao Ying",
        "Fengchang Yu",
        "Haihua Chen",
        "Changjiang Jiang",
        "Yurong Li",
        "Wei Lu"
      ],
      "abstract": "The advancement of Document Intelligence (DI) demands large-scale, high-quality training data, yet manual annotation remains a critical bottleneck. While data generation methods are evolving rapidly, existing surveys are constrained by fragmented focuses on single modalities or specific tasks, lacking a unified perspective aligned with real-world workflows. To fill this gap, this survey establishes the first comprehensive technical map for data generation in DI. Data generation is redefined as supervisory signal production, and a novel taxonomy is introduced based on the \"availability of data and labels.\" This framework organizes methodologies into four resource-centric paradigms: Data Augmentation, Data Generation from Scratch, Automated Data Annotation, and Self-Supervised Signal Construction. Furthermore, a multi-level evaluation framework is established to integrate intrinsic quality and extrinsic utility, compiling performance gains across diverse DI benchmarks. Guided by this unified structure, the methodological landscape is dissected to reveal critical challenges such as fidelity gaps and frontiers including co-evolutionary ecosystems. Ultimately, by systematizing this fragmented field, data generation is positioned as the central engine for next-generation DI.",
      "tldr_zh": "æœ¬ç»¼è¿°å»ºç«‹äº† Document Intelligence (DI) é¢†åŸŸé¦–ä¸ªå…¨é¢çš„æŠ€æœ¯å›¾è°±ï¼Œæ—¨åœ¨è§£å†³å¤§è§„æ¨¡é«˜è´¨é‡è®­ç»ƒæ•°æ®è·å–ä¸­äººå·¥æ ‡æ³¨çš„ç“¶é¢ˆé—®é¢˜ã€‚è¯¥ç ”ç©¶å°†æ•°æ®ç”Ÿæˆé‡æ–°å®šä¹‰ä¸ºâ€œç›‘ç£ä¿¡å·äº§ç”Ÿâ€ï¼Œå¹¶æå‡ºäº†åŒ…å« Data Augmentationã€Data Generation from Scratchã€Automated Data Annotation ä»¥åŠ Self-Supervised Signal Construction çš„å››å¤§èµ„æºä¸­å¿ƒèŒƒå¼ã€‚æ­¤å¤–ï¼Œæ–‡ç« è¿˜å»ºç«‹äº†ä¸€ä¸ªå¤šçº§è¯„ä¼°æ¡†æ¶ä»¥æ•´åˆå†…åœ¨è´¨é‡å’Œå¤–åœ¨æ•ˆç”¨ï¼Œå¹¶ç³»ç»Ÿæ€»ç»“äº†å„ DI åŸºå‡†æµ‹è¯•ä¸­çš„æ€§èƒ½æå‡ã€‚é€šè¿‡å‰–æä¿çœŸåº¦å·®è· (fidelity gaps) ç­‰å…³é”®æŒ‘æˆ˜å’ŒååŒè¿›åŒ–ç”Ÿæ€ç³»ç»Ÿ (co-evolutionary ecosystems) ç­‰å‰æ²¿æ–¹å‘ï¼Œè¯¥ç»¼è¿°å°†æ•°æ®ç”Ÿæˆå®šä½ä¸ºé©±åŠ¨ä¸‹ä¸€ä»£ DI å‘å±•çš„æ ¸å¿ƒå¼•æ“ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2601.12318v1",
      "published_date": "2026-01-18 09:01:18 UTC",
      "updated_date": "2026-01-18 09:01:18 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-22T23:32:39.958696+00:00"
    },
    {
      "arxiv_id": "2601.12317v1",
      "title": "Explanova: Automatically Discover Data Insights in N \\times M Table via XAI Combined LLM Workflow",
      "title_zh": "Explanovaï¼šç»“åˆ XAI ä¸ LLM å·¥ä½œæµçš„ N Ã— M ç»´è¡¨æ•°æ®æ´å¯Ÿè‡ªåŠ¨å‘ç°",
      "authors": [
        "Yiming Huang"
      ],
      "abstract": "Automation in data analysis has been a long-time pursuit. Current agentic LLM shows a promising solution towards it. Like DeepAnalyze, DataSage, and Datawise. They are all powerful agentic frameworks for automatic fine-grained analysis and are powered by LLM-based agentic tool calling ability. However, what about powered by a preset AutoML-like workflow? If we traverse all possible exploration, like Xn itself`s statistics, Xn1-Xn2 relationships, Xn to all other, and finally explain? Our Explanova is such an attempt: Cheaper due to a Local Small LLM.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† Explanovaï¼Œä¸€ä¸ªç»“åˆå¯è§£é‡Šäººå·¥æ™ºèƒ½ (XAI) ä¸å¤§è¯­è¨€æ¨¡å‹ (LLM) å·¥ä½œæµçš„è‡ªåŠ¨åŒ–æ•°æ®æ´å¯Ÿå‘ç°ç³»ç»Ÿã€‚ä¸åŒäº DeepAnalyze ç­‰ä¾èµ–å·¥å…·è°ƒç”¨ (tool calling) çš„ç°æœ‰æ™ºèƒ½ä½“æ¡†æ¶ï¼ŒExplanova é‡‡ç”¨äº†é¢„è®¾çš„ç±» AutoML å·¥ä½œæµï¼Œé€šè¿‡å…¨é¢éå†å˜é‡ç»Ÿè®¡ã€å˜é‡é—´å…³ç³»åŠå…¨å±€å…³è”è¿›è¡Œæ·±å…¥åˆ†æå¹¶ç»™å‡ºè§£é‡Šã€‚ç”±äºè¯¥ç³»ç»Ÿæ”¯æŒåœ¨æœ¬åœ°éƒ¨ç½²å°è§„æ¨¡å¤§è¯­è¨€æ¨¡å‹ (Local Small LLM)ï¼Œå…¶è¿è¡Œæˆæœ¬æ˜¾è‘—é™ä½ï¼Œä¸ºå®ç°ä½æˆæœ¬ã€é«˜æ•ˆç‡çš„è¡¨æ ¼æ•°æ®è‡ªåŠ¨åŒ–åˆ†ææä¾›äº†æ–°æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2601.12317v1",
      "published_date": "2026-01-18 09:00:03 UTC",
      "updated_date": "2026-01-18 09:00:03 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-22T23:32:43.695555+00:00"
    },
    {
      "arxiv_id": "2601.12316v1",
      "title": "GazeFormer-MoE: Context-Aware Gaze Estimation via CLIP and MoE Transformer",
      "title_zh": "GazeFormer-MoEï¼šåŸºäº CLIP ä¸æ··åˆä¸“å®¶ Transformer çš„ä¸Šä¸‹æ–‡æ„ŸçŸ¥è§†çº¿ä¼°è®¡",
      "authors": [
        "Xinyuan Zhao",
        "Xianrui Chen",
        "Ahmad Chaddad"
      ],
      "abstract": "We present a semantics modulated, multi scale Transformer for 3D gaze estimation. Our model conditions CLIP global features with learnable prototype banks (illumination, head pose, background, direction), fuses these prototype-enriched global vectors with CLIP patch tokens and high-resolution CNN tokens in a unified attention space, and replaces several FFN blocks with routed/shared Mixture of Experts to increase conditional capacity. Evaluated on MPIIFaceGaze, EYEDIAP, Gaze360 and ETH-XGaze, our model achieves new state of the art angular errors of 2.49Â°, 3.22Â°, 10.16Â°, and 1.44Â°, demonstrating up to a 64% relative improvement over previously reported results. ablations attribute gains to prototype conditioning, cross scale fusion, MoE and hyperparameter. Our code is publicly available at https://github. com/AIPMLab/Gazeformer.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† GazeFormer-MoEï¼Œä¸€ç§ç”¨äº 3D gaze estimation çš„è¯­ä¹‰è°ƒåˆ¶å¤šå°ºåº¦ Transformer æ¨¡å‹ã€‚è¯¥æ¨¡å‹åˆ©ç”¨å¯å­¦ä¹ çš„ prototype banksï¼ˆæ¶µç›–å…‰ç…§ã€å¤´éƒ¨å§¿æ€ã€èƒŒæ™¯å’Œæ–¹å‘ï¼‰å¯¹ CLIP å…¨å±€ç‰¹å¾è¿›è¡Œè°ƒèŠ‚ï¼Œå¹¶å°†å…¶ä¸ CLIP patch tokens åŠé«˜åˆ†è¾¨ç‡ CNN tokens åœ¨ç»Ÿä¸€çš„æ³¨æ„åŠ›ç©ºé—´ä¸­è¿›è¡Œèåˆã€‚æ­¤å¤–ï¼Œé€šè¿‡å¼•å…¥ä¸“å®¶æ··åˆæ¨¡å‹ (Mixture of Experts, MoE) æ›¿æ¢éƒ¨åˆ† FFN æ¨¡å—ï¼Œæ˜¾è‘—æå‡äº†æ¨¡å‹çš„æ¡ä»¶è¡¨è¾¾èƒ½åŠ›ã€‚å®éªŒè¡¨æ˜ï¼ŒGazeFormer-MoE åœ¨ MPIIFaceGazeã€EYEDIAP ç­‰å¤šä¸ªæ ‡å‡†æ•°æ®é›†ä¸Šåˆ·æ–°äº† SOTA è®°å½•ï¼Œè¾ƒæ­¤å‰ç ”ç©¶å®ç°äº†é«˜è¾¾ 64% çš„ç›¸å¯¹æ”¹è¿›ã€‚è¯¥æˆæœè¯æ˜äº† prototype conditioningã€è·¨å°ºåº¦èåˆä»¥åŠ MoE æ¶æ„åœ¨å¤æ‚ç¯å¢ƒä¸‹å®ç°é«˜ç²¾åº¦è§†çº¿ä¼°è®¡çš„æœ‰æ•ˆæ€§ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "accepted at ICASSP 2026",
      "pdf_url": "https://arxiv.org/pdf/2601.12316v1",
      "published_date": "2026-01-18 08:54:02 UTC",
      "updated_date": "2026-01-18 08:54:02 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-22T23:32:52.997150+00:00"
    },
    {
      "arxiv_id": "2601.14305v1",
      "title": "An Optimized Decision Tree-Based Framework for Explainable IoT Anomaly Detection",
      "title_zh": "é¢å‘å¯è§£é‡Šç‰©è”ç½‘å¼‚å¸¸æ£€æµ‹çš„ä¼˜åŒ–å†³ç­–æ ‘æ¡†æ¶",
      "authors": [
        "Ashikuzzaman",
        "Md. Shawkat Hossain",
        "Jubayer Abdullah Joy",
        "Md Zahid Akon",
        "Md Manjur Ahmed",
        "Md. Naimul Islam"
      ],
      "abstract": "The increase in the number of Internet of Things (IoT) devices has tremendously increased the attack surface of cyber threats thus making a strong intrusion detection system (IDS) with a clear explanation of the process essential towards resource-constrained environments. Nevertheless, current IoT IDS systems are usually traded off with detection quality, model elucidability, and computational effectiveness, thus the deployment on IoT devices. The present paper counteracts these difficulties by suggesting an explainable AI (XAI) framework based on an optimized Decision Tree classifier with both local and global importance methods: SHAP values that estimate feature attribution using local explanations, and Morris sensitivity analysis that identifies the feature importance in a global view. The proposed system attains the state of art on the test performance with 99.91% accuracy, F1-score of 99.51% and Cohen Kappa of 0.9960 and high stability is confirmed by a cross validation mean accuracy of 98.93%. Efficiency is also enhanced in terms of computations to provide faster inferences compared to those that are generalized in ensemble models. SrcMac has shown as the most significant predictor in feature analyses according to SHAP and Morris methods. Compared to the previous work, our solution eliminates its major drawback lack because it allows us to apply it to edge devices and, therefore, achieve real-time processing, adhere to the new regulation of transparency in AI, and achieve high detection rates on attacks of dissimilar classes. This combination performance of high accuracy, explainability, and low computation make the framework useful and reliable as a resource-constrained IoT security problem in real environments.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ä¸ªåŸºäºä¼˜åŒ–çš„ Decision Tree çš„å¯è§£é‡Š AI (XAI) æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³èµ„æºå—é™çš„ IoT ç¯å¢ƒä¸­å¼‚å¸¸æ£€æµ‹ç³»ç»Ÿåœ¨å‡†ç¡®ç‡ã€å¯è§£é‡Šæ€§å’Œè®¡ç®—æ•ˆç‡ä¹‹é—´éš¾ä»¥å¹³è¡¡çš„æŒ‘æˆ˜ã€‚è¯¥æ¡†æ¶ç»“åˆäº†ç”¨äºå±€éƒ¨ç‰¹å¾å½’å› çš„ SHAP å€¼æ–¹æ³•å’Œç”¨äºå…¨å±€ç‰¹å¾é‡è¦æ€§è¯„ä¼°çš„ Morris çµæ•åº¦åˆ†æã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥ç³»ç»Ÿè¾¾åˆ°äº† 99.91% çš„å‡†ç¡®ç‡å’Œ 99.51% çš„ F1-scoreï¼Œä¸”æ¨ç†é€Ÿåº¦æ˜¾è‘—å¿«äºä¼ ç»Ÿçš„é›†æˆæ¨¡å‹ã€‚ç‰¹å¾åˆ†æç¡®å®šäº† SrcMac æ˜¯æœ€é‡è¦çš„é¢„æµ‹æŒ‡æ ‡ã€‚è¯¥æ–¹æ¡ˆå‡­å€Ÿå…¶é«˜ç²¾åº¦ã€ä½è®¡ç®—å¼€é”€å’Œé€æ˜åº¦ï¼Œä¸ºè¾¹ç¼˜è®¾å¤‡ä¸Šçš„å®æ—¶æ”»å‡»æ£€æµ‹æä¾›äº†ä¸€ç§å¯é ä¸”ç¬¦åˆç›‘ç®¡è¦æ±‚çš„è§£å†³æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.CR",
        "cs.AI"
      ],
      "primary_category": "cs.CR",
      "comment": "Acepted and Presented at IEEE 2nd International Conference on Computing, Applications and Systems (COMPAS 2025) , 23-24 October 2025, Kushtia, Bangladesh",
      "pdf_url": "https://arxiv.org/pdf/2601.14305v1",
      "published_date": "2026-01-18 08:48:53 UTC",
      "updated_date": "2026-01-18 08:48:53 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-22T23:32:48.929677+00:00"
    },
    {
      "arxiv_id": "2601.12310v1",
      "title": "Survival is the Only Reward: Sustainable Self-Training Through Environment-Mediated Selection",
      "title_zh": "ç”Ÿå­˜å³å”¯ä¸€å¥–åŠ±ï¼šé€šè¿‡ç¯å¢ƒä»‹å¯¼é€‰æ‹©å®ç°çš„å¯æŒç»­è‡ªè®­ç»ƒ",
      "authors": [
        "Jennifer Dodgson",
        "Alfath Daryl Alhajir",
        "Michael Joedhitya",
        "Akira Rafhael Janson Pattirane",
        "Surender Suresh Kumar",
        "Joseph Lim",
        "C. H. Peh",
        "Adith Ramdas",
        "Steven Zhang Zhexu"
      ],
      "abstract": "Self-training systems often degenerate due to the lack of an external criterion for judging data quality, leading to reward hacking and semantic drift. This paper provides a proof-of-concept system architecture for stable self-training under sparse external feedback and bounded memory, and empirically characterises its learning dynamics and failure modes.\n  We introduce a self-training architecture in which learning is mediated exclusively by environmental viability, rather than by reward, objective functions, or externally defined fitness criteria. Candidate behaviours are executed under real resource constraints, and only those whose environmental effects both persist and preserve the possibility of future interaction are propagated. The environment does not provide semantic feedback, dense rewards, or task-specific supervision; selection operates solely through differential survival of behaviours as world-altering events, making proxy optimisation impossible and rendering reward-hacking evolutionarily unstable.\n  Analysis of semantic dynamics shows that improvement arises primarily through the persistence of effective and repeatable strategies under a regime of consolidation and pruning, a paradigm we refer to as negative-space learning (NSL), and that models develop meta-learning strategies (such as deliberate experimental failure in order to elicit informative error messages) without explicit instruction. This work establishes that environment-grounded selection enables sustainable open-ended self-improvement, offering a viable path toward more robust and generalisable autonomous systems without reliance on human-curated data or complex reward shaping.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹è‡ªæˆ‘è®­ç»ƒ(Self-training)ç³»ç»Ÿä¸­å¸¸è§çš„å¥–åŠ±æ”»å‡»(Reward hacking)å’Œè¯­ä¹‰æ¼‚ç§»(Semantic drift)é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§åŸºäºç¯å¢ƒä»‹å¯¼é€‰æ‹©(Environment-mediated selection)çš„å¯æŒç»­è‡ªæˆ‘è®­ç»ƒç³»ç»Ÿæ¶æ„ã€‚è¯¥æ¶æ„æ‘’å¼ƒäº†ä¼ ç»Ÿçš„å¥–åŠ±æˆ–ç›®æ ‡å‡½æ•°ï¼Œå®Œå…¨ç”±ç¯å¢ƒå¯è¡Œæ€§(Environmental viability)é©±åŠ¨ï¼Œä»…ä¿ç•™åœ¨çœŸå®èµ„æºçº¦æŸä¸‹èƒ½å¤ŸæŒç»­å¹¶ä¿éšœåç»­äº¤äº’å¯èƒ½æ€§çš„è¡Œä¸ºï¼Œä»è€Œä½¿å¥–åŠ±æ”»å‡»åœ¨è¿›åŒ–ä¸Šå˜å¾—ä¸ç¨³å®šã€‚ç ”ç©¶æå‡ºäº†ä¸€ç§è¢«ç§°ä¸ºâ€œè´Ÿç©ºé—´å­¦ä¹ â€(Negative-space learning, NSL)çš„èŒƒå¼ï¼Œé€šè¿‡æœ‰æ•ˆç­–ç•¥çš„å·©å›ºä¸å‰ªæå®ç°æ€§èƒ½æå‡ï¼Œå¹¶è§‚å¯Ÿåˆ°æ¨¡å‹åœ¨æ— æ˜ç¡®æŒ‡ä»¤ä¸‹å‘å±•å‡ºäº†å…ƒå­¦ä¹ (Meta-learning)ç­–ç•¥ã€‚å®éªŒè¯æ˜ï¼Œè¿™ç§åŸºäºç¯å¢ƒçº¦æŸçš„é€‰æ‹©æœºåˆ¶èƒ½å¤Ÿå®ç°ç¨³å¥çš„ã€å¼€æ”¾å¼çš„è‡ªä¸»è¿›åŒ–ï¼Œä¸ºå¼€å‘ä¸ä¾èµ–äººå·¥æ•°æ®æˆ–å¤æ‚å¥–åŠ±å»ºæ¨¡(Reward shaping)çš„é€šç”¨è‡ªä¸»ç³»ç»Ÿæä¾›äº†æ–°è·¯å¾„ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2601.12310v1",
      "published_date": "2026-01-18 08:35:56 UTC",
      "updated_date": "2026-01-18 08:35:56 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-22T23:33:04.724269+00:00"
    },
    {
      "arxiv_id": "2601.12304v1",
      "title": "A Two-Stage Globally-Diverse Adversarial Attack for Vision-Language Pre-training Models",
      "title_zh": "é’ˆå¯¹è§†è§‰-è¯­è¨€é¢„è®­ç»ƒæ¨¡å‹çš„ä¸¤é˜¶æ®µå…¨å±€å¤šæ ·åŒ–å¯¹æŠ—æ”»å‡»",
      "authors": [
        "Wutao Chen",
        "Huaqin Zou",
        "Chen Wan",
        "Lifeng Huang"
      ],
      "abstract": "Vision-language pre-training (VLP) models are vulnerable to adversarial examples, particularly in black-box scenarios. Existing multimodal attacks often suffer from limited perturbation diversity and unstable multi-stage pipelines. To address these challenges, we propose 2S-GDA, a two-stage globally-diverse attack framework. The proposed method first introduces textual perturbations through a globally-diverse strategy by combining candidate text expansion with globally-aware replacement. To enhance visual diversity, image-level perturbations are generated using multi-scale resizing and block-shuffle rotation. Extensive experiments on VLP models demonstrate that 2S-GDA consistently improves attack success rates over state-of-the-art methods, with gains of up to 11.17\\% in black-box settings. Our framework is modular and can be easily combined with existing methods to further enhance adversarial transferability.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†2S-GDAï¼Œä¸€ç§é’ˆå¯¹è§†è§‰è¯­è¨€é¢„è®­ç»ƒæ¨¡å‹(Vision-Language Pre-training, VLP)çš„ä¸¤é˜¶æ®µå…¨å±€å¤šæ ·æ€§å¯¹æŠ—æ”»å‡»æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³é»‘ç›’åœºæ™¯ä¸‹æ‰°åŠ¨å¤šæ ·æ€§ä¸è¶³å’Œå¤šé˜¶æ®µæµç¨‹ä¸ç¨³å®šçš„é—®é¢˜ã€‚è¯¥æ–¹æ³•åœ¨æ–‡æœ¬å±‚é¢é€šè¿‡å€™é€‰æ–‡æœ¬æ‰©å±•ä¸å…¨å±€æ„ŸçŸ¥æ›¿æ¢å®ç°å…¨å±€å¤šæ ·æ€§ç­–ç•¥ï¼Œåœ¨å›¾åƒå±‚é¢åˆ™åˆ©ç”¨å¤šå°ºåº¦ç¼©æ”¾(multi-scale resizing)å’Œå—æ´—ç‰Œæ—‹è½¬(block-shuffle rotation)å¢å¼ºè§†è§‰å¤šæ ·æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œ2S-GDAåœ¨é»‘ç›’è®¾ç½®ä¸‹çš„æ”»å‡»æˆåŠŸç‡æ¯”ç°æœ‰æœ€å…ˆè¿›æ–¹æ³•æå‡äº†é«˜è¾¾11.17%ã€‚æ­¤å¤–ï¼Œè¯¥æ¡†æ¶å…·æœ‰æ¨¡å—åŒ–ç‰¹æ€§ï¼Œå¯æ–¹ä¾¿åœ°ä¸ç°æœ‰æ–¹æ³•ç»“åˆä»¥è¿›ä¸€æ­¥å¢å¼ºå¯¹æŠ—è¿ç§»æ€§(adversarial transferability)ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "Accepted to ICASSP 2026",
      "pdf_url": "https://arxiv.org/pdf/2601.12304v1",
      "published_date": "2026-01-18 08:05:33 UTC",
      "updated_date": "2026-01-18 08:05:33 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-22T23:33:14.975972+00:00"
    },
    {
      "arxiv_id": "2601.12294v1",
      "title": "ToolPRMBench: Evaluating and Advancing Process Reward Models for Tool-using Agents",
      "title_zh": "ToolPRMBenchï¼šé¢å‘å·¥å…·è°ƒç”¨æ™ºèƒ½ä½“çš„è¿‡ç¨‹å¥–åŠ±æ¨¡å‹è¯„ä¼°ä¸æå‡",
      "authors": [
        "Dawei Li",
        "Yuguang Yao",
        "Zhen Tan",
        "Huan Liu",
        "Ruocheng Guo"
      ],
      "abstract": "Reward-guided search methods have demonstrated strong potential in enhancing tool-using agents by effectively guiding sampling and exploration over complex action spaces. As a core design, those search methods utilize process reward models (PRMs) to provide step-level rewards, enabling more fine-grained monitoring. However, there is a lack of systematic and reliable evaluation benchmarks for PRMs in tool-using settings. In this paper, we introduce ToolPRMBench, a large-scale benchmark specifically designed to evaluate PRMs for tool-using agents. ToolPRMBench is built on top of several representative tool-using benchmarks and converts agent trajectories into step-level test cases. Each case contains the interaction history, a correct action, a plausible but incorrect alternative, and relevant tool metadata. We respectively utilize offline sampling to isolate local single-step errors and online sampling to capture realistic multi-step failures from full agent rollouts. A multi-LLM verification pipeline is proposed to reduce label noise and ensure data quality. We conduct extensive experiments across large language models, general PRMs, and tool-specialized PRMs on ToolPRMBench. The results reveal clear differences in PRM effectiveness and highlight the potential of specialized PRMs for tool-using. Code and data will be released at https://github.com/David-Li0406/ToolPRMBench.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å·¥å…·ä½¿ç”¨æ™ºèƒ½ä½“ï¼ˆtool-using agentsï¼‰ä¸­è¿‡ç¨‹å¥–åŠ±æ¨¡å‹ï¼ˆProcess Reward Models, PRMsï¼‰ç¼ºä¹ç³»ç»Ÿæ€§è¯„ä¼°åŸºå‡†çš„é—®é¢˜ï¼Œæå‡ºäº†å¤§è§„æ¨¡è¯„æµ‹åŸºå‡† ToolPRMBenchã€‚è¯¥åŸºå‡†é€šè¿‡å°†æ™ºèƒ½ä½“è½¨è¿¹è½¬æ¢ä¸ºæ­¥éª¤çº§æµ‹è¯•ç”¨ä¾‹ï¼Œç»“åˆç¦»çº¿é‡‡æ ·ï¼ˆè¯†åˆ«å±€éƒ¨å•æ­¥é”™è¯¯ï¼‰å’Œåœ¨çº¿é‡‡æ ·ï¼ˆæ•è·å¤šæ­¥å¤±è´¥ï¼‰æŠ€æœ¯ï¼Œå¹¶åˆ©ç”¨å¤šå¤§è¯­è¨€æ¨¡å‹ï¼ˆmulti-LLMï¼‰éªŒè¯æµæ°´çº¿ç¡®ä¿æ•°æ®æ ‡æ³¨çš„è´¨é‡ã€‚å®éªŒå¯¹é€šç”¨ PRMs å’Œå·¥å…·ä¸“ç”¨ PRMs è¿›è¡Œäº†å¹¿æ³›è¯„ä¼°ï¼Œç»“æœæ­ç¤ºäº†ä¸åŒæ¨¡å‹åœ¨æœ‰æ•ˆæ€§ä¸Šçš„æ˜¾è‘—å·®å¼‚ã€‚è¯¥ç ”ç©¶ä¸ä»…å¡«è¡¥äº†å·¥å…·ä½¿ç”¨åœºæ™¯ä¸‹ PRMs è¯„ä¼°çš„ç©ºç™½ï¼Œè¿˜å±•ç¤ºäº†å¼€å‘ä¸“ç”¨è¿‡ç¨‹å¥–åŠ±æ¨¡å‹ä»¥ä¼˜åŒ–æ™ºèƒ½ä½“å†³ç­–çš„å·¨å¤§æ½œåŠ›ã€‚",
      "categories": [
        "cs.AI",
        "cs.SE"
      ],
      "primary_category": "cs.AI",
      "comment": "under review",
      "pdf_url": "https://arxiv.org/pdf/2601.12294v1",
      "published_date": "2026-01-18 07:48:36 UTC",
      "updated_date": "2026-01-18 07:48:36 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-22T23:33:17.364894+00:00"
    },
    {
      "arxiv_id": "2601.12288v1",
      "title": "TimeGMM: Single-Pass Probabilistic Forecasting via Adaptive Gaussian Mixture Models with Reversible Normalization",
      "title_zh": "TimeGMMï¼šåŸºäºå¯é€†å½’ä¸€åŒ–è‡ªé€‚åº”é«˜æ–¯æ··åˆæ¨¡å‹çš„å•é˜¶æ®µæ¦‚ç‡é¢„æµ‹",
      "authors": [
        "Lei Liu",
        "Tengyuan Liu",
        "Hongwei Zhao",
        "Jiahui Huang",
        "Ruibo Guo",
        "Bin Li"
      ],
      "abstract": "Probabilistic time series forecasting is crucial for quantifying future uncertainty, with significant applications in fields such as energy and finance. However, existing methods often rely on computationally expensive sampling or restrictive parametric assumptions to characterize future distributions, which limits predictive performance and introduces distributional mismatch. To address these challenges, this paper presents TimeGMM, a novel probabilistic forecasting framework based on Gaussian Mixture Models (GMM) that captures complex future distributions in a single forward pass. A key component is GMM-adapted Reversible Instance Normalization (GRIN), a novel module designed to dynamically adapt to temporal-probabilistic distribution shifts. The framework integrates a dedicated Temporal Encoder (TE-Module) with a Conditional Temporal-Probabilistic Decoder (CTPD-Module) to jointly capture temporal dependencies and mixture distribution parameters. Extensive experiments demonstrate that TimeGMM consistently outperforms state-of-the-art methods, achieving maximum improvements of 22.48\\% in CRPS and 21.23\\% in NMAE.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† TimeGMMï¼Œä¸€ç§åŸºäº Gaussian Mixture Models (GMM) çš„æ–°å‹æ¦‚ç‡æ—¶é—´åºåˆ—é¢„æµ‹æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³ç°æœ‰æ–¹æ³•åœ¨é‡åŒ–æœªæ¥ä¸ç¡®å®šæ€§æ—¶è®¡ç®—æˆæœ¬é«˜æˆ–å‚æ•°å‡è®¾å—é™çš„é—®é¢˜ã€‚è¯¥æ¡†æ¶é€šè¿‡é›†æˆ Temporal Encoder (TE-Module) å’Œ Conditional Temporal-Probabilistic Decoder (CTPD-Module)ï¼Œå®ç°äº†åœ¨å•æ¬¡å‰å‘ä¼ æ’­ä¸­åŒæ—¶æ•æ‰æ—¶é—´ä¾èµ–æ€§å’Œå¤æ‚çš„æ··åˆåˆ†å¸ƒå‚æ•°ã€‚æ­¤å¤–ï¼Œç ”ç©¶å¼•å…¥äº† GMM-adapted Reversible Instance Normalization (GRIN) æ¨¡å—ï¼Œä¸“é—¨ç”¨äºåŠ¨æ€é€‚åº”æ—¶é—´-æ¦‚ç‡åˆ†å¸ƒçš„åç§»ã€‚å®éªŒè¯æ˜ï¼ŒTimeGMM åœ¨ CRPS å’Œ NMAE æŒ‡æ ‡ä¸Šè¾ƒç°æœ‰å…ˆè¿›æ¨¡å‹åˆ†åˆ«å®ç°äº†æœ€é«˜ 22.48% å’Œ 21.23% çš„æ€§èƒ½æå‡ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2601.12288v1",
      "published_date": "2026-01-18 07:02:13 UTC",
      "updated_date": "2026-01-18 07:02:13 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-22T23:33:21.808124+00:00"
    },
    {
      "arxiv_id": "2601.12286v1",
      "title": "Conversational Context Classification: A Representation Engineering Approach",
      "title_zh": "å¯¹è¯è¯­å¢ƒåˆ†ç±»ï¼šä¸€ç§åŸºäºè¡¨ç¤ºå·¥ç¨‹çš„æ–¹æ³•",
      "authors": [
        "Jonathan Pan"
      ],
      "abstract": "The increasing prevalence of Large Language Models (LLMs) demands effective safeguards for their operation, particularly concerning their tendency to generate out-of-context responses. A key challenge is accurately detecting when LLMs stray from expected conversational norms, manifesting as topic shifts, factual inaccuracies, or outright hallucinations. Traditional anomaly detection struggles to directly apply within contextual semantics. This paper outlines our experiment in exploring the use of Representation Engineering (RepE) and One-Class Support Vector Machine (OCSVM) to identify subspaces within the internal states of LLMs that represent a specific context. By training OCSVM on in-context examples, we establish a robust boundary within the LLM's hidden state latent space. We evaluate out study with two open source LLMs - Llama and Qwen models in specific contextual domain. Our approach entailed identifying the optimal layers within the LLM's internal state subspaces that strongly associates with the context of interest. Our evaluation results showed promising results in identifying the subspace for a specific context. Aside from being useful in detecting in or out of context conversation threads, this research work contributes to the study of better interpreting LLMs.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†åˆ©ç”¨ Representation Engineering (RepE) å’Œ One-Class Support Vector Machine (OCSVM) æ¥è¯†åˆ«å¤§è¯­è¨€æ¨¡å‹ (LLMs) å†…éƒ¨çŠ¶æ€å­ç©ºé—´çš„æ–¹æ³•ï¼Œä»¥è§£å†³æ¨¡å‹åœ¨å¯¹è¯ä¸­è„±ç¦»ä¸Šä¸‹æ–‡ã€è¯é¢˜åç§»æˆ–äº§ç”Ÿå¹»è§‰ (hallucinations) çš„é—®é¢˜ã€‚é€šè¿‡åœ¨ Llama å’Œ Qwen æ¨¡å‹ä¸Šè¿›è¡Œå®éªŒï¼Œç ”ç©¶è€…åˆ©ç”¨ OCSVM åœ¨éšè—çŠ¶æ€çš„æ½œåœ¨ç©ºé—´ä¸­å»ºç«‹äº†é²æ£’çš„è¾¹ç•Œï¼Œå¹¶ç¡®å®šäº†ä¸ç‰¹å®šä¸Šä¸‹æ–‡å…³è”æœ€å¼ºçš„æœ€ä¼˜å†…éƒ¨å±‚ã€‚å®éªŒç»“æœè¯æ˜ï¼Œè¯¥æ–¹æ³•åœ¨è¯†åˆ«ç‰¹å®šä¸Šä¸‹æ–‡å­ç©ºé—´æ–¹é¢è¡¨ç°ä¼˜å¼‚ï¼Œèƒ½æœ‰æ•ˆæ£€æµ‹å¯¹è¯æ˜¯å¦ç¬¦åˆé¢„æœŸçš„ä¸Šä¸‹æ–‡èŒƒå¼ã€‚æ­¤é¡¹å·¥ä½œä¸ä»…æå‡äº†å¯¹è¯ä¸Šä¸‹æ–‡åˆ†ç±»çš„å‡†ç¡®æ€§ï¼Œä¹Ÿä¸ºæ·±å…¥è§£é‡Š LLMs çš„å†…éƒ¨æœºåˆ¶è´¡çŒ®äº†é‡è¦è§è§£ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.CR"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2601.12286v1",
      "published_date": "2026-01-18 06:47:35 UTC",
      "updated_date": "2026-01-18 06:47:35 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-22T23:33:15.632669+00:00"
    },
    {
      "arxiv_id": "2601.12282v1",
      "title": "CytoCLIP: Learning Cytoarchitectural Characteristics in Developing Human Brain Using Contrastive Language Image Pre-Training",
      "title_zh": "CytoCLIPï¼šåˆ©ç”¨å¯¹æ¯”è¯­è¨€-å›¾åƒé¢„è®­ç»ƒå­¦ä¹ å‘è‚²ä¸­äººè„‘çš„ç»†èƒæ„ç­‘ç‰¹å¾",
      "authors": [
        "Pralaypati Ta",
        "Sriram Venkatesaperumal",
        "Keerthi Ram",
        "Mohanasankar Sivaprakasam"
      ],
      "abstract": "The functions of different regions of the human brain are closely linked to their distinct cytoarchitecture, which is defined by the spatial arrangement and morphology of the cells. Identifying brain regions by their cytoarchitecture enables various scientific analyses of the brain. However, delineating these areas manually in brain histological sections is time-consuming and requires specialized knowledge. An automated approach is necessary to minimize the effort needed from human experts. To address this, we propose CytoCLIP, a suite of vision-language models derived from pre-trained Contrastive Language-Image Pre-Training (CLIP) frameworks to learn joint visual-text representations of brain cytoarchitecture. CytoCLIP comprises two model variants: one is trained using low-resolution whole-region images to understand the overall cytoarchitectural pattern of an area, and the other is trained on high-resolution image tiles for detailed cellular-level representation. The training dataset is created from NISSL-stained histological sections of developing fetal brains of different gestational weeks. It includes 86 distinct regions for low-resolution images and 384 brain regions for high-resolution tiles. We evaluate the model's understanding of the cytoarchitecture and generalization ability using region classification and cross-modal retrieval tasks. Multiple experiments are performed under various data setups, including data from samples of different ages and sectioning planes. Experimental results demonstrate that CytoCLIP outperforms existing methods. It achieves an F1 score of 0.87 for whole-region classification and 0.91 for high-resolution image tile classification.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† CytoCLIPï¼Œè¿™æ˜¯ä¸€å¥—åŸºäºå¯¹æ¯”è¯­è¨€-å›¾åƒé¢„è®­ç»ƒ (CLIP) æ¡†æ¶çš„è§†è§‰-è¯­è¨€æ¨¡å‹ï¼Œæ—¨åœ¨è‡ªåŠ¨å­¦ä¹ äººç±»å‘è‚²æœŸå¤§è„‘çš„ç»†èƒæ„ç­‘ (Cytoarchitecture) ç‰¹å¾ã€‚CytoCLIP åŒ…å«ä¸¤ä¸ªæ¨¡å‹å˜ä½“ï¼Œåˆ†åˆ«é€šè¿‡ä½åˆ†è¾¨ç‡å›¾åƒæ•æ‰è„‘åŒºæ•´ä½“æ¨¡å¼ä»¥åŠé€šè¿‡é«˜åˆ†è¾¨ç‡åˆ‡ç‰‡å­¦ä¹ ç»†èƒçº§ç»†èŠ‚è¡¨å¾ã€‚è¯¥æ¨¡å‹åœ¨ä¸åŒå­•å‘¨èƒå„¿å¤§è„‘çš„ NISSL æŸ“è‰²ç»„ç»‡åˆ‡ç‰‡ä¸Šè¿›è¡Œè®­ç»ƒï¼Œæ¶µç›–äº†å¤šè¾¾ 384 ä¸ªè„‘åŒºã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒCytoCLIP åœ¨åŒºåŸŸåˆ†ç±»å’Œè·¨æ¨¡æ€æ£€ç´¢ä»»åŠ¡ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œå…¶å…¨åŒºåŸŸåˆ†ç±»å’Œé«˜åˆ†è¾¨ç‡åˆ‡ç‰‡åˆ†ç±»çš„ F1 åˆ†æ•°åˆ†åˆ«è¾¾åˆ° 0.87 å’Œ 0.91ï¼Œæ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2601.12282v1",
      "published_date": "2026-01-18 06:42:24 UTC",
      "updated_date": "2026-01-18 06:42:24 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-22T23:33:21.539769+00:00"
    },
    {
      "arxiv_id": "2601.12276v2",
      "title": "Predictive Prototyping: Evaluating Design Concepts with ChatGPT",
      "title_zh": "é¢„æµ‹æ€§åŸå‹ï¼šåˆ©ç”¨ ChatGPT è¯„ä¼°è®¾è®¡æ¦‚å¿µ",
      "authors": [
        "Hilsann Yong",
        "Bradley A. Camburn"
      ],
      "abstract": "The design-build-test cycle is essential for innovation, but physical prototyping is often slow and expensive. Although physics-based simulation and strategic prototyping can reduce cost, meaningful evaluation is frequently constrained until an integrated prototype is built. This paper investigates whether a generative pretrained transformer (GPT) can predict information typically obtained through prototyping, including cost, performance, and perceived usability. We introduce a retrieval-augmented generation (RAG) method to emulate design feedback using OpenAI GPT-4o, grounded in prototyping data scraped from Instructables.com to increase access to relevant precedent. Two studies are reported. First, a controlled experiment compares GPT-RAG and human designers, who receive design sketches and predict cost, performance, and usability; predictions are evaluated against ground-truth results from physical prototypes. Second, we report an applied demonstration in which a physical prototype is produced from GPT-RAG recommendations and compared with a commercial baseline and a topology-optimized design. Results show that GPT-RAG provides more accurate cost and performance estimates than individual or crowd human estimates, while yielding comparable usability insights; the GPT-RAG-informed prototype also outperforms both comparison prototypes. Repeated querying with response averaging significantly improves accuracy, suggesting that LLMs can emulate crowd aggregation effects consistent with the law of large numbers.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§åä¸ºâ€œé¢„æµ‹æ€§åŸå‹è®¾è®¡â€(Predictive Prototyping)çš„æ–¹æ³•ï¼Œåˆ©ç”¨ OpenAI GPT-4o å’Œæ£€ç´¢å¢å¼ºç”Ÿæˆ(RAG)æŠ€æœ¯æ¥é¢„æµ‹è®¾è®¡æ–¹æ¡ˆçš„æˆæœ¬(Cost)ã€æ€§èƒ½(Performance)å’Œæ„ŸçŸ¥å¯ç”¨æ€§(Usability)ã€‚ç ”ç©¶é€šè¿‡ä» Instructables.com æŠ“å–çš„å¼€æºæ•°æ®å¯¹æ¨¡å‹è¿›è¡ŒçŸ¥è¯†å¢å¼ºï¼Œå°†å…¶é¢„æµ‹ç»“æœä¸äººç±»è®¾è®¡å¸ˆåŠç‰©ç†åŸå‹çš„çœŸå®æ•°æ®è¿›è¡Œäº†ä¸¥è°¨å¯¹æ¯”ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒGPT-RAG åœ¨æˆæœ¬å’Œæ€§èƒ½è¯„ä¼°çš„å‡†ç¡®æ€§ä¸Šä¼˜äºäººç±»ä¸ªä½“åŠä¼—åŒ…ä¼°è®¡ï¼Œä¸”å…¶æŒ‡å¯¼ç”Ÿæˆçš„ç‰©ç†åŸå‹åœ¨å®é™…è¡¨ç°ä¸Šè¶…è¿‡äº†å•†ä¸šåŸºå‡†ã€‚æ­¤å¤–ï¼Œç ”ç©¶å‘ç°é€šè¿‡å“åº”å¹³å‡åŒ–(Response Averaging)çš„é‡å¤æŸ¥è¯¢èƒ½æ˜¾è‘—æå‡é¢„æµ‹ç²¾åº¦ï¼Œè¯æ˜äº†å¤§è¯­è¨€æ¨¡å‹(LLMs)èƒ½å¤Ÿæœ‰æ•ˆæ¨¡æ‹Ÿç¬¦åˆå¤§æ•°å®šå¾‹çš„ç¾¤ä½“èšåˆæ•ˆåº”ã€‚",
      "categories": [
        "cs.HC",
        "cs.AI"
      ],
      "primary_category": "cs.HC",
      "comment": "22 pages, 15 figures, 5 tables",
      "pdf_url": "https://arxiv.org/pdf/2601.12276v2",
      "published_date": "2026-01-18 06:26:03 UTC",
      "updated_date": "2026-01-21 03:54:37 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-22T23:33:26.295440+00:00"
    },
    {
      "arxiv_id": "2601.12269v1",
      "title": "Simulated Annealing Enhances Theory-of-Mind Reasoning in Autoregressive Language Models",
      "title_zh": "æ¨¡æ‹Ÿé€€ç«å¢å¼ºè‡ªå›å½’è¯­è¨€æ¨¡å‹çš„å¿ƒç†ç†è®ºæ¨ç†",
      "authors": [
        "Xucong Hu",
        "Jian-Qiao Zhu"
      ],
      "abstract": "Autoregressive language models are next-token predictors and have been criticized for only optimizing surface plausibility (i.e., local coherence) rather than maintaining correct latent-state representations (i.e., global coherence). Because Theory of Mind (ToM) tasks crucially depend on reasoning about latent mental states of oneself and others, such models are therefore often thought to fail at ToM. While post-training methods can improve ToM performance, we show that strong ToM capability can be recovered directly from the base model without any additional weight updates or verifications. Our approach builds on recent power-sampling methods (Karan & Du, 2025) that use Markov chain Monte Carlo (MCMC) to sample from sharpened sequence-level (rather than token-level) probability distributions of autoregressive language models. We further find that incorporating annealing, where the tempered distribution is gradually shifted from high to low temperature, substantially improves ToM performance over fixed-temperature power sampling. Together, these results suggest that sampling-based optimization provides a powerful way to extract latent capabilities from language models without retraining.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹è‡ªå›å½’è¯­è¨€æ¨¡å‹ï¼ˆAutoregressive language modelsï¼‰åœ¨å¤„ç†å¿ƒæ™ºç†è®ºï¼ˆTheory of Mind, ToMï¼‰ä»»åŠ¡æ—¶å› ç¼ºä¹å…¨å±€è¿è´¯æ€§ï¼ˆglobal coherenceï¼‰è€Œè¡¨ç°ä¸ä½³çš„é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§åŸºäºæ¨¡æ‹Ÿé€€ç«ï¼ˆSimulated Annealingï¼‰çš„ä¼˜åŒ–é‡‡æ ·æ–¹æ³•ã€‚è¯¥æ–¹æ³•åˆ©ç”¨é©¬å°”å¯å¤«é“¾è’™ç‰¹å¡æ´›ï¼ˆMCMCï¼‰æŠ€æœ¯ä»åºåˆ—çº§æ¦‚ç‡åˆ†å¸ƒä¸­è¿›è¡Œé‡‡æ ·ï¼Œæ— éœ€é¢å¤–çš„æƒé‡æ›´æ–°æˆ–éªŒè¯å³å¯ä»åŸºç¡€æ¨¡å‹ä¸­ç›´æ¥æ¢å¤å¼ºå¤§çš„ ToM èƒ½åŠ›ã€‚ç ”ç©¶å‘ç°ï¼Œé€šè¿‡å°†é‡‡æ ·åˆ†å¸ƒä»é«˜æ¸©é€æ¸é€€ç«è‡³ä½æ¸©ï¼Œæ¨¡å‹åœ¨ ToM ä»»åŠ¡ä¸Šçš„è¡¨ç°è¾ƒå›ºå®šæ¸©åº¦é‡‡æ ·æœ‰æ˜¾è‘—æå‡ã€‚å®éªŒè¯æ˜ï¼ŒåŸºäºé‡‡æ ·çš„ä¼˜åŒ–æŠ€æœ¯ï¼ˆsampling-based optimizationï¼‰æ˜¯æå–è¯­è¨€æ¨¡å‹æ½œåœ¨æ¨ç†èƒ½åŠ›çš„æœ‰æ•ˆæ‰‹æ®µï¼Œæ— éœ€é‡æ–°è®­ç»ƒå³å¯å®ç°å¤æ‚çš„è®¤çŸ¥å»ºæ¨¡ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2601.12269v1",
      "published_date": "2026-01-18 05:51:30 UTC",
      "updated_date": "2026-01-18 05:51:30 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-22T23:33:29.404404+00:00"
    },
    {
      "arxiv_id": "2601.12263v1",
      "title": "Multimodal Generative Engine Optimization: Rank Manipulation for Vision-Language Model Rankers",
      "title_zh": "å¤šæ¨¡æ€ç”Ÿæˆå¼å¼•æ“ä¼˜åŒ–ï¼šé’ˆå¯¹è§†è§‰-è¯­è¨€æ¨¡å‹æ’åºå™¨çš„æ’åæ“çºµ",
      "authors": [
        "Yixuan Du",
        "Chenxiao Yu",
        "Haoyan Xu",
        "Ziyi Wang",
        "Yue Zhao",
        "Xiyang Hu"
      ],
      "abstract": "Vision-Language Models (VLMs) are rapidly replacing unimodal encoders in modern retrieval and recommendation systems. While their capabilities are well-documented, their robustness against adversarial manipulation in competitive ranking scenarios remains largely unexplored. In this paper, we uncover a critical vulnerability in VLM-based product search: multimodal ranking attacks. We present Multimodal Generative Engine Optimization (MGEO), a novel adversarial framework that enables a malicious actor to unfairly promote a target product by jointly optimizing imperceptible image perturbations and fluent textual suffixes. Unlike existing attacks that treat modalities in isolation, MGEO employs an alternating gradient-based optimization strategy to exploit the deep cross-modal coupling within the VLM. Extensive experiments on real-world datasets using state-of-the-art models demonstrate that our coordinated attack significantly outperforms text-only and image-only baselines. These findings reveal that multimodal synergy, typically a strength of VLMs, can be weaponized to compromise the integrity of search rankings without triggering conventional content filters.",
      "tldr_zh": "è¯¥ç ”ç©¶æ­ç¤ºäº†åŸºäº Vision-Language Model (VLM) çš„äº§å“æœç´¢ç³»ç»Ÿåœ¨é¢å¯¹å¯¹æŠ—æ€§æ“çºµæ—¶çš„è„†å¼±æ€§ï¼Œå¹¶æå‡ºäº†å¤šæ¨¡æ€æ’åæ”»å‡»æ¡†æ¶ Multimodal Generative Engine Optimization (MGEO)ã€‚è¯¥æ¡†æ¶é€šè¿‡è”åˆä¼˜åŒ–ä¸å¯è§çš„å›¾åƒæ‰°åŠ¨ (image perturbations) å’Œæµç•…çš„æ–‡æœ¬åç¼€ (textual suffixes)ï¼Œåˆ©ç”¨ VLM å†…éƒ¨æ·±å±‚çš„è·¨æ¨¡æ€è€¦åˆç‰¹æ€§æ¥ä¸å…¬å¹³åœ°æå‡ç›®æ ‡äº§å“æ’åã€‚MGEO é‡‡ç”¨äº†ä¸€ç§äº¤æ›¿æ¢¯åº¦ä¼˜åŒ– (alternating gradient-based optimization) ç­–ç•¥ï¼Œå®éªŒè¯æ˜å…¶æ”»å‡»æ•ˆæœæ˜¾è‘—ä¼˜äºä»…é’ˆå¯¹å•ä¸€æ¨¡æ€çš„åŸºçº¿æ–¹æ³•ã€‚ç ”ç©¶ç»“æœè­¦å‘Šï¼ŒVLM æ ¸å¿ƒçš„å¤šæ¨¡æ€ååŒèƒ½åŠ›å¯èƒ½è¢«â€œæ­¦å™¨åŒ–â€ï¼Œä»è€Œåœ¨ä¸è§¦å‘å¸¸è§„å†…å®¹è¿‡æ»¤å™¨çš„å‰æä¸‹ç ´åæœç´¢æ’åçš„å…¬æ­£æ€§ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2601.12263v1",
      "published_date": "2026-01-18 04:58:28 UTC",
      "updated_date": "2026-01-18 04:58:28 UTC",
      "processing_status": "completed",
      "attempts": 3,
      "max_attempts": 3,
      "error": "Your request was blocked.",
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-22T23:36:25.690507+00:00"
    },
    {
      "arxiv_id": "2601.12260v1",
      "title": "Docs2Synth: A Synthetic Data Trained Retriever Framework for Scanned Visually Rich Documents Understanding",
      "title_zh": "Docs2Synthï¼šä¸€ç§ç”¨äºæ‰«æè§†è§‰ä¸°å¯Œæ–‡æ¡£ç†è§£çš„åˆæˆæ•°æ®è®­ç»ƒæ£€ç´¢å™¨æ¡†æ¶",
      "authors": [
        "Yihao Ding",
        "Qiang Sun",
        "Puzhen Wu",
        "Sirui Li",
        "Siwen Luo",
        "Wei Liu"
      ],
      "abstract": "Document understanding (VRDU) in regulated domains is particularly challenging, since scanned documents often contain sensitive, evolving, and domain specific knowledge. This leads to two major challenges: the lack of manual annotations for model adaptation and the difficulty for pretrained models to stay up-to-date with domain-specific facts. While Multimodal Large Language Models (MLLMs) show strong zero-shot abilities, they still suffer from hallucination and limited domain grounding. In contrast, discriminative Vision-Language Pre-trained Models (VLPMs) provide reliable grounding but require costly annotations to cover new domains. We introduce Docs2Synth, a synthetic-supervision framework that enables retrieval-guided inference for private and low-resource domains. Docs2Synth automatically processes raw document collections, generates and verifies diverse QA pairs via an agent-based system, and trains a lightweight visual retriever to extract domain-relevant evidence. During inference, the retriever collaborates with an MLLM through an iterative retrieval--generation loop, reducing hallucination and improving response consistency. We further deliver Docs2Synth as an easy-to-use Python package, enabling plug-and-play deployment across diverse real-world scenarios. Experiments on multiple VRDU benchmarks show that Docs2Synth substantially enhances grounding and domain generalization without requiring human annotations.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† Docs2Synthï¼Œä¸€ä¸ªé’ˆå¯¹æ‰«æçš„è§†è§‰ä¸°å¯Œæ–‡æ¡£ç†è§£ï¼ˆVRDUï¼‰çš„åˆæˆç›‘ç£æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³å—è§„åˆ¶é¢†åŸŸä¸­æ•°æ®æ•æ„Ÿã€äººå·¥æ ‡æ³¨åŒ®ä¹ä»¥åŠ Multimodal Large Language Models (MLLMs) å­˜åœ¨çš„å¹»è§‰é—®é¢˜ã€‚è¯¥æ¡†æ¶åˆ©ç”¨åŸºäºæ™ºèƒ½ä½“çš„ç³»ç»Ÿè‡ªåŠ¨ç”Ÿæˆå¹¶éªŒè¯ QA å¯¹ï¼Œä»è€Œè®­ç»ƒå‡ºä¸€ä¸ªè½»é‡çº§çš„ visual retrieverï¼Œç”¨äºæå–é¢†åŸŸç›¸å…³çš„è¯æ®ã€‚åœ¨æ¨ç†é˜¶æ®µï¼Œè¯¥æ£€ç´¢å™¨é€šè¿‡è¿­ä»£çš„ retrieval-generation å¾ªç¯ä¸ MLLM åä½œï¼Œæ˜¾è‘—å¢å¼ºäº†æ¨¡å‹çš„ grounding èƒ½åŠ›å¹¶å‡å°‘äº†å¹»è§‰ã€‚å®éªŒè¯æ˜ï¼ŒDocs2Synth åœ¨æ— éœ€äººå·¥æ ‡æ³¨çš„æƒ…å†µä¸‹å¤§å¹…æå‡äº†é¢†åŸŸæ³›åŒ–æ€§èƒ½ï¼Œå¹¶å·²ä½œä¸ºå³æ’å³ç”¨çš„ Python å·¥å…·åŒ…å‘å¸ƒã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "Accepted at WWW 2026 Demo Track",
      "pdf_url": "https://arxiv.org/pdf/2601.12260v1",
      "published_date": "2026-01-18 04:45:09 UTC",
      "updated_date": "2026-01-18 04:45:09 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-22T23:33:34.540090+00:00"
    },
    {
      "arxiv_id": "2601.12259v1",
      "title": "FutureX-Pro: Extending Future Prediction to High-Value Vertical Domains",
      "title_zh": "FutureX-Proï¼šå°†æœªæ¥é¢„æµ‹æ‰©å±•è‡³é«˜ä»·å€¼å‚ç›´é¢†åŸŸ",
      "authors": [
        "Jiashuo Liu",
        "Siyuan Chen",
        "Zaiyuan Wang",
        "Zhiyuan Zeng",
        "Jiacheng Guo",
        "Liang Hu",
        "Lingyue Yin",
        "Suozhi Huang",
        "Wenxin Hao",
        "Yang Yang",
        "Zerui Cheng",
        "Zixin Yao",
        "Lingyue Yin",
        "Haoxin Liu",
        "Jiayi Cheng",
        "Yuzhen Li",
        "Zezhong Ma",
        "Bingjie Wang",
        "Bingsen Qiu",
        "Xiao Liu",
        "Zeyang Zhang",
        "Zijian Liu",
        "Jinpeng Wang",
        "Mingren Yin",
        "Tianci He",
        "Yali Liao",
        "Yixiao Tian",
        "Zhenwei Zhu",
        "Anqi Dai",
        "Ge Zhang",
        "Jingkai Liu",
        "Kaiyuan Zhang",
        "Wenlong Wu",
        "Xiang Gao",
        "Xinjie Chen",
        "Zhixin Yao",
        "Zhoufutu Wen",
        "B. Aditya Prakash",
        "Jose Blanchet",
        "Mengdi Wang",
        "Nian Si",
        "Wenhao Huang"
      ],
      "abstract": "Building upon FutureX, which established a live benchmark for general-purpose future prediction, this report introduces FutureX-Pro, including FutureX-Finance, FutureX-Retail, FutureX-PublicHealth, FutureX-NaturalDisaster, and FutureX-Search. These together form a specialized framework extending agentic future prediction to high-value vertical domains. While generalist agents demonstrate proficiency in open-domain search, their reliability in capital-intensive and safety-critical sectors remains under-explored. FutureX-Pro targets four economically and socially pivotal verticals: Finance, Retail, Public Health, and Natural Disaster. We benchmark agentic Large Language Models (LLMs) on entry-level yet foundational prediction tasks -- ranging from forecasting market indicators and supply chain demands to tracking epidemic trends and natural disasters. By adapting the contamination-free, live-evaluation pipeline of FutureX, we assess whether current State-of-the-Art (SOTA) agentic LLMs possess the domain grounding necessary for industrial deployment. Our findings reveal the performance gap between generalist reasoning and the precision required for high-value vertical applications.",
      "tldr_zh": "è¯¥ç ”ç©¶åœ¨ FutureX åŸºç¡€ä¸Šæå‡ºäº† FutureX-Pro æ¡†æ¶ï¼Œæ—¨åœ¨å°† Agentic æœªæ¥é¢„æµ‹èƒ½åŠ›æ‰©å±•è‡³ Financeã€Retailã€Public Health å’Œ Natural Disaster ç­‰å…·æœ‰é«˜ç»æµå’Œç¤¾ä¼šä»·å€¼çš„å‚ç›´é¢†åŸŸã€‚é€šè¿‡é‡‡ç”¨æ— æ±¡æŸ“çš„å®æ—¶è¯„ä¼°æµæ°´çº¿ (Contamination-free, live-evaluation pipeline)ï¼Œè¯¥ç ”ç©¶å¯¹å¤§è¯­è¨€æ¨¡å‹ (LLMs) åœ¨å¸‚åœºæŒ‡æ ‡ã€ä¾›åº”é“¾éœ€æ±‚ã€æµè¡Œç—…è¶‹åŠ¿åŠè‡ªç„¶ç¾å®³ç­‰åŸºç¡€é¢„æµ‹ä»»åŠ¡ä¸­çš„è¡¨ç°è¿›è¡Œäº† Benchmarkã€‚å®éªŒç»“æœæ­ç¤ºäº†é€šç”¨æ¨ç†èƒ½åŠ›ä¸é«˜ä»·å€¼å‚ç›´åº”ç”¨æ‰€éœ€ç²¾åº¦ä¹‹é—´çš„æ€§èƒ½å·®è·ï¼Œä¸ºè¯„ä¼° SOTA Agentic LLMs çš„å·¥ä¸šåŒ–éƒ¨ç½²æ½œåŠ›æä¾›äº†å…³é”®è§è§£ã€‚",
      "categories": [
        "cs.AI",
        "cs.CE",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "21 pages",
      "pdf_url": "https://arxiv.org/pdf/2601.12259v1",
      "published_date": "2026-01-18 04:44:49 UTC",
      "updated_date": "2026-01-18 04:44:49 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-22T23:34:12.879861+00:00"
    },
    {
      "arxiv_id": "2601.12257v1",
      "title": "Soft Shadow Diffusion (SSD): Physics-inspired Learning for 3D Computational Periscopy",
      "title_zh": "Soft Shadow Diffusion (SSD)ï¼šé¢å‘ä¸‰ç»´è®¡ç®—å‘¨è§†çš„ç‰©ç†å¯å‘å¼å­¦ä¹ ",
      "authors": [
        "Fadlullah Raji",
        "John Murray-Bruce"
      ],
      "abstract": "Conventional imaging requires a line of sight to create accurate visual representations of a scene. In certain circumstances, however, obtaining a suitable line of sight may be impractical, dangerous, or even impossible. Non-line-of-sight (NLOS) imaging addresses this challenge by reconstructing the scene from indirect measurements. Recently, passive NLOS methods that use an ordinary photograph of the subtle shadow cast onto a visible wall by the hidden scene have gained interest. These methods are currently limited to 1D or low-resolution 2D color imaging or to localizing a hidden object whose shape is approximately known. Here, we generalize this class of methods and demonstrate a 3D reconstruction of a hidden scene from an ordinary NLOS photograph. To achieve this, we propose a novel reformulation of the light transport model that conveniently decomposes the hidden scene into \\textit{light-occluding} and \\textit{non-light-occluding} components to yield a separable non-linear least squares (SNLLS) inverse problem. We develop two solutions: A gradient-based optimization method and a physics-inspired neural network approach, which we call Soft Shadow diffusion (SSD). Despite the challenging ill-conditioned inverse problem encountered here, our approaches are effective on numerous 3D scenes in real experimental scenarios. Moreover, SSD is trained in simulation but generalizes well to unseen classes in simulation and real-world NLOS scenes. SSD also shows surprising robustness to noise and ambient illumination.",
      "tldr_zh": "# è®ºæ–‡ TLDR æ‘˜è¦ ğŸ“„\n\n---\n\n## æ‘˜è¦å†…å®¹\n\nè¯¥ç ”ç©¶æå‡ºäº† **Soft Shadow Diffusion (SSD)**ï¼Œè¿™æ˜¯ä¸€ç§åŸºäºç‰©ç†å¯å‘å­¦ä¹  (Physics-inspired Learning) çš„æ–¹æ³•ï¼Œæ—¨åœ¨é€šè¿‡æ™®é€šç…§ç‰‡ä¸­éšè—åœºæ™¯æŠ•å°„çš„å¾®å¼±é˜´å½±å®ç°ä¸‰ç»´éè§†è·æˆåƒ (**3D Non-line-of-sight, NLOS**)ã€‚ç ”ç©¶é€šè¿‡é‡æ–°æ„å»ºå…‰ä¼ è¾“æ¨¡å‹ï¼Œå°†éšè—åœºæ™¯åˆ†è§£ä¸ºé®å…‰ (light-occluding) ä¸éé®å…‰ç»„ä»¶ï¼Œä»è€Œå°†é‡å»ºé—®é¢˜è½¬åŒ–ä¸ºå¯åˆ†ç¦»éçº¿æ€§æœ€å°äºŒä¹˜ (**SNLLS**) é€†é—®é¢˜ã€‚è®ºæ–‡å¯¹æ¯”äº†åŸºäºæ¢¯åº¦çš„ä¼˜åŒ–æ–¹æ³•ä¸ SSD ç¥ç»ç½‘ç»œæ¨¡å‹ï¼Œåè€…è™½åœ¨æ¨¡æ‹Ÿç¯å¢ƒä¸­è®­ç»ƒï¼Œä½†åœ¨çœŸå®ä¸–ç•Œçš„ 3D åœºæ™¯é‡å»ºä¸­è¡¨ç°å‡ºæå¼ºçš„æ³›åŒ–èƒ½åŠ›ã€‚å®éªŒç»“æœè¯æ˜ï¼ŒSSD å¯¹å™ªå£°å’Œç¯å¢ƒå…‰å¹²æ‰°å…·æœ‰æ˜¾è‘—çš„é²æ£’æ€§ï¼ŒæˆåŠŸå®ç°äº†ä»æ™®é€š NLOS ç…§ç‰‡ä¸­æå–é«˜ç»´ç©ºé—´ä¿¡æ¯ã€‚\n\n---\n\nå¸Œæœ›è¿™ä¸ªæ‘˜è¦èƒ½æ»¡è¶³æ‚¨çš„éœ€æ±‚ï¼å¦‚æœæ‚¨è¿˜æœ‰å…¶ä»–è®ºæ–‡éœ€è¦å¤„ç†ï¼Œæ¬¢è¿éšæ—¶å‘Šè¯‰æˆ‘ã€‚âœ¨",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CG",
        "cs.GR"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2601.12257v1",
      "published_date": "2026-01-18 04:40:00 UTC",
      "updated_date": "2026-01-18 04:40:00 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-22T23:34:22.028258+00:00"
    },
    {
      "arxiv_id": "2601.12256v1",
      "title": "Improving Large Molecular Language Model via Relation-aware Multimodal Collaboration",
      "title_zh": "é€šè¿‡å…³ç³»æ„ŸçŸ¥å¤šæ¨¡æ€åä½œæå‡å¤§åˆ†å­è¯­è¨€æ¨¡å‹",
      "authors": [
        "Jinyoung Park",
        "Minseong Bae",
        "Jeehye Na",
        "Hyunwoo J. Kim"
      ],
      "abstract": "Large language models (LLMs) have demonstrated their instruction-following capabilities and achieved powerful performance on various tasks. Inspired by their success, recent works in the molecular domain have led to the development of large molecular language models (LMLMs) that integrate 1D molecular strings or 2D molecular graphs into the language models. However, existing LMLMs often suffer from hallucination and limited robustness, largely due to inadequate integration of diverse molecular modalities such as 1D sequences, 2D molecular graphs, and 3D conformations. To address these limitations, we propose CoLLaMo, a large language model-based molecular assistant equipped with a multi-level molecular modality-collaborative projector. The relation-aware modality-collaborative attention mechanism in the projector facilitates fine-grained and relation-guided information exchange between atoms by incorporating 2D structural and 3D spatial relations. Furthermore, we present a molecule-centric new automatic measurement, including a hallucination assessment metric and GPT-based caption quality evaluation to address the limitations of token-based generic evaluation metrics (i.e., BLEU) widely used in assessing molecular comprehension of LMLMs. Our extensive experiments demonstrate that our CoLLaMo enhances the molecular modality generalization capabilities of LMLMs, achieving the best performance on multiple tasks, including molecule captioning, computed property QA, descriptive property QA, motif counting, and IUPAC name prediction.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤§åˆ†å­è¯­è¨€æ¨¡å‹(LMLMs)åœ¨æ•´åˆ1Dåºåˆ—ã€2Dåˆ†å­å›¾å’Œ3Dæ„è±¡æ—¶å­˜åœ¨çš„å¹»è§‰ä¸é²æ£’æ€§ä¸è¶³é—®é¢˜ï¼Œæå‡ºäº†åŸºäºå¤šå±‚çº§åˆ†å­æ¨¡æ€åä½œæŠ•å½±å™¨çš„åˆ†å­åŠ©æ‰‹CoLLaMoã€‚è¯¥æ¡†æ¶å¼•å…¥äº†å…³ç³»æ„ŸçŸ¥çš„æ¨¡æ€åä½œæ³¨æ„åŠ›æœºåˆ¶(relation-aware modality-collaborative attention mechanism)ï¼Œé€šè¿‡æ•´åˆ2Dç»“æ„å’Œ3Dç©ºé—´å…³ç³»ï¼Œå®ç°äº†åŸå­é—´ç»†ç²’åº¦ä¸”å—å…³ç³»å¼•å¯¼çš„ä¿¡æ¯äº¤æ¢ã€‚æ­¤å¤–ï¼Œç ”ç©¶è¿˜å¼€å‘äº†ä¸€å¥—ä»¥åˆ†å­ä¸ºä¸­å¿ƒçš„è‡ªåŠ¨æµ‹é‡æŒ‡æ ‡ï¼ŒåŒ…å«å¹»è§‰è¯„ä¼°å’ŒåŸºäºGPTçš„æè¿°è´¨é‡è¯„ä»·ï¼Œä»¥å¼¥è¡¥ä¼ ç»ŸBLEUæŒ‡æ ‡åœ¨åˆ†å­ç†è§£è¯„ä¼°ä¸Šçš„å±€é™æ€§ã€‚å®éªŒè¯æ˜ï¼ŒCoLLaMoåœ¨åˆ†å­æè¿°(molecule captioning)ã€å±æ€§é—®ç­”åŠIUPACåç§°é¢„æµ‹ç­‰ä»»åŠ¡ä¸Šå‡è¾¾åˆ°æœ€ä¼˜æ€§èƒ½ï¼Œæ˜¾è‘—å¢å¼ºäº†æ¨¡å‹çš„æ¨¡æ€æ³›åŒ–èƒ½åŠ›ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2601.12256v1",
      "published_date": "2026-01-18 04:38:19 UTC",
      "updated_date": "2026-01-18 04:38:19 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-22T23:34:18.838186+00:00"
    },
    {
      "arxiv_id": "2601.12249v1",
      "title": "An Innovative Framework for Breast Cancer Detection Using Pyramid Adaptive Atrous Convolution, Transformer Integration, and Multi-Scale Feature Fusion",
      "title_zh": "åŸºäºé‡‘å­—å¡”è‡ªé€‚åº”ç©ºæ´å·ç§¯ã€Transformer é›†æˆä¸å¤šå°ºåº¦ç‰¹å¾èåˆçš„ä¹³è…ºç™Œæ£€æµ‹åˆ›æ–°æ¡†æ¶",
      "authors": [
        "Ehsan Sadeghi Pour",
        "Mahdi Esmaeili",
        "Morteza Romoozi"
      ],
      "abstract": "Breast cancer is one of the most common cancers among women worldwide, and its accurate and timely diagnosis plays a critical role in improving treatment outcomes. This thesis presents an innovative framework for detecting malignant masses in mammographic images by integrating the Pyramid Adaptive Atrous Convolution (PAAC) and Transformer architectures. The proposed approach utilizes Multi-Scale Feature Fusion to enhance the extraction of features from benign and malignant tissues and combines Dice Loss and Focal Loss functions to improve the model's learning process, effectively reducing errors in binary breast cancer classification and achieving high accuracy and efficiency. In this study, a comprehensive dataset of breast cancer images from INbreast, MIAS, and DDSM was preprocessed through data augmentation and contrast enhancement and resized to 227x227 pixels for model training. Leveraging the Transformer's ability to manage long-range dependencies with Self-Attention mechanisms, the proposed model achieved high accuracy in detecting cancerous masses, outperforming foundational models such as BreastNet, DeepMammo, Multi-Scale CNN, Swin-Unet, and SegFormer. The final evaluation results for the proposed model include an accuracy of 98.5\\%, sensitivity of 97.8\\%, specificity of 96.3\\%, F1-score of 98.2\\%, and overall precision of 97.9\\%. These metrics demonstrate a significant improvement over traditional methods and confirm the model's effectiveness in identifying cancerous masses in complex scenarios and large datasets. This model shows potential as a reliable and efficient tool for breast cancer diagnosis and can be effectively integrated into medical diagnostic systems.",
      "tldr_zh": "æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§ç”¨äºä¹³è…ºç™Œæ£€æµ‹çš„åˆ›æ–°æ¡†æ¶ï¼Œé€šè¿‡é›†æˆé‡‘å­—å¡”è‡ªé€‚åº”ç©ºæ´å·ç§¯ (Pyramid Adaptive Atrous Convolution, PAAC) ä¸ Transformer æ¶æ„ï¼Œå®ç°äº†å¯¹ä¹³è…º X çº¿å½±åƒä¸­æ¶æ€§è‚¿å—çš„ç²¾ç¡®è¯†åˆ«ã€‚è¯¥æ–¹æ³•åˆ©ç”¨å¤šå°ºåº¦ç‰¹å¾èåˆ (Multi-Scale Feature Fusion) å¢å¼ºç‰¹å¾æå–ï¼Œå¹¶ç»“åˆ Dice Loss å’Œ Focal Loss å‡½æ•°ä¼˜åŒ–æ¨¡å‹å­¦ä¹ ï¼Œæ˜¾è‘—æå‡äº†äºŒåˆ†ç±»ä»»åŠ¡çš„å‡†ç¡®æ€§ã€‚å®éªŒåœ¨ INbreastã€MIAS å’Œ DDSM æ•°æ®é›†ä¸Šè¿›è¡Œï¼Œå……åˆ†åˆ©ç”¨äº† Transformer çš„è‡ªæ³¨æ„åŠ› (Self-Attention) æœºåˆ¶æ•æ‰é•¿è·ç¦»ä¾èµ–å…³ç³»ã€‚ç»“æœæ˜¾ç¤ºï¼Œè¯¥æ¨¡å‹çš„å‡†ç¡®ç‡è¾¾åˆ° 98.5%ï¼ŒF1-score ä¸º 98.2%ï¼Œæ€§èƒ½å…¨é¢è¶…è¶Šäº† BreastNetã€Swin-Unet å’Œ SegFormer ç­‰åŸºå‡†æ¨¡å‹ï¼Œä¸ºä¸´åºŠä¹³è…ºç™Œè¯Šæ–­æä¾›äº†ä¸€ä¸ªé«˜æ•ˆã€å¯é çš„è¾…åŠ©å·¥å…·ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "13 page",
      "pdf_url": "https://arxiv.org/pdf/2601.12249v1",
      "published_date": "2026-01-18 03:55:33 UTC",
      "updated_date": "2026-01-18 03:55:33 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-22T23:34:23.059552+00:00"
    },
    {
      "arxiv_id": "2601.12248v1",
      "title": "AQUA-Bench: Beyond Finding Answers to Knowing When There Are None in Audio Question Answering",
      "title_zh": "AQUA-Benchï¼šéŸ³é¢‘é—®ç­”ä¸­ä»å¯»æ‰¾ç­”æ¡ˆè¿ˆå‘è¯†åˆ«æ— è§£æƒ…å†µ",
      "authors": [
        "Chun-Yi Kuan",
        "Hung-yi Lee"
      ],
      "abstract": "Recent advances in audio-aware large language models have shown strong performance on audio question answering. However, existing benchmarks mainly cover answerable questions and overlook the challenge of unanswerable ones, where no reliable answer can be inferred from the audio. Such cases are common in real-world settings, where questions may be misleading, ill-posed, or incompatible with the information. To address this gap, we present AQUA-Bench, a benchmark for Audio Question Unanswerability Assessment. It systematically evaluates three scenarios: Absent Answer Detection (the correct option is missing), Incompatible Answer Set Detection (choices are categorically mismatched with the question), and Incompatible Audio Question Detection (the question is irrelevant or lacks sufficient grounding in the audio). By assessing these cases, AQUA-Bench offers a rigorous measure of model reliability and promotes the development of audio-language systems that are more robust and trustworthy. Our experiments suggest that while models excel on standard answerable tasks, they often face notable challenges with unanswerable ones, pointing to a blind spot in current audio-language understanding.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† AQUA-Benchï¼Œè¿™æ˜¯ä¸€ä¸ªä¸“ä¸ºéŸ³é¢‘é—®ç­”ä¸å¯å›ç­”æ€§è¯„ä¼° (Audio Question Unanswerability Assessment) è®¾è®¡çš„æ–°å‹åŸºå‡†æµ‹è¯•ï¼Œæ—¨åœ¨è§£å†³ç°æœ‰æ¨¡å‹åœ¨é¢å¯¹è¯¯å¯¼æ€§æˆ–ä¸éŸ³é¢‘ä¸å…¼å®¹çš„é—®é¢˜æ—¶ç¼ºä¹å¯é æ€§çš„é—®é¢˜ã€‚è¯¥åŸºå‡†ç³»ç»Ÿåœ°è¯„ä¼°äº†ä¸‰ç§å…³é”®åœºæ™¯ï¼šç¼ºå¤±ç­”æ¡ˆæ£€æµ‹ (Absent Answer Detection)ã€ä¸å…¼å®¹é€‰é¡¹é›†æ£€æµ‹ (Incompatible Answer Set Detection) ä»¥åŠä¸å…¼å®¹éŸ³é¢‘é—®é¢˜æ£€æµ‹ (Incompatible Audio Question Detection)ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œå°½ç®¡å½“å‰æ¨¡å‹åœ¨æ ‡å‡†çš„å¯å›ç­”ä»»åŠ¡ä¸Šè¡¨ç°å¼ºåŠ²ï¼Œä½†åœ¨å¤„ç†ä¸å¯å›ç­”çš„é—®é¢˜æ—¶ä»é¢ä¸´æ˜¾è‘—æŒ‘æˆ˜ï¼Œæ­ç¤ºäº†éŸ³é¢‘è¯­è¨€ç†è§£é¢†åŸŸçš„è®¤çŸ¥ç›²ç‚¹ã€‚AQUA-Bench çš„æå‡ºä¸ºè¡¡é‡æ¨¡å‹å¯é æ€§æä¾›äº†ä¸¥æ ¼æ ‡å‡†ï¼Œæœ‰åŠ©äºæ¨åŠ¨æ›´é²æ£’ã€æ›´å¯ä¿¡çš„éŸ³é¢‘è¯­è¨€ç³»ç»Ÿå¼€å‘ã€‚",
      "categories": [
        "eess.AS",
        "cs.AI",
        "cs.CL",
        "cs.LG",
        "cs.SD"
      ],
      "primary_category": "eess.AS",
      "comment": "Accepted to ICASSP 2026. Project Website: https://kuan2jiu99.github.io/AQUA-Bench-demo/",
      "pdf_url": "https://arxiv.org/pdf/2601.12248v1",
      "published_date": "2026-01-18 03:55:28 UTC",
      "updated_date": "2026-01-18 03:55:28 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-22T23:34:25.412046+00:00"
    },
    {
      "arxiv_id": "2601.12247v1",
      "title": "Plan, Verify and Fill: A Structured Parallel Decoding Approach for Diffusion Language Models",
      "title_zh": "è®¡åˆ’ã€éªŒè¯ä¸å¡«å……ï¼šä¸€ç§é¢å‘æ‰©æ•£è¯­è¨€æ¨¡å‹çš„ç»“æ„åŒ–å¹¶è¡Œè§£ç æ–¹æ³•",
      "authors": [
        "Miao Li",
        "Hanyang Jiang",
        "Sikai Chen",
        "Hengyu Fu",
        "Yuhang Cai",
        "Baihe Huang",
        "Tinghan Ye",
        "Xuanzhou Chen",
        "Pascal Van Hentenryck"
      ],
      "abstract": "Diffusion Language Models (DLMs) present a promising non-sequential paradigm for text generation, distinct from standard autoregressive (AR) approaches. However, current decoding strategies often adopt a reactive stance, underutilizing the global bidirectional context to dictate global trajectories. To address this, we propose Plan-Verify-Fill (PVF), a training-free paradigm that grounds planning via quantitative validation. PVF actively constructs a hierarchical skeleton by prioritizing high-leverage semantic anchors and employs a verification protocol to operationalize pragmatic structural stopping where further deliberation yields diminishing returns. Extensive evaluations on LLaDA-8B-Instruct and Dream-7B-Instruct demonstrate that PVF reduces the Number of Function Evaluations (NFE) by up to 65% compared to confidence-based parallel decoding across benchmark datasets, unlocking superior efficiency without compromising accuracy.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹æ‰©æ•£è¯­è¨€æ¨¡å‹(Diffusion Language Models, DLMs)æå‡ºäº†ä¸€ç§åä¸º Plan-Verify-Fill (PVF) çš„ç»“æ„åŒ–å¹¶è¡Œè§£ç æ–¹æ³•ï¼Œæ—¨åœ¨è§£å†³ç°æœ‰è§£ç ç­–ç•¥å¯¹å…¨å±€åŒå‘ä¸Šä¸‹æ–‡åˆ©ç”¨ä¸è¶³çš„é—®é¢˜ã€‚PVF é‡‡ç”¨æ— éœ€è®­ç»ƒ(training-free)çš„èŒƒå¼ï¼Œé€šè¿‡ä¼˜å…ˆå¤„ç†é«˜æ æ†è¯­ä¹‰é”šç‚¹(semantic anchors)æ¥ä¸»åŠ¨æ„å»ºå±‚æ¬¡åŒ–éª¨æ¶ï¼Œå¹¶å¼•å…¥éªŒè¯åè®®ä»¥ä¼˜åŒ–ç»“æ„åŒ–åœæ­¢æ—¶æœºã€‚åœ¨ LLaDA-8B-Instruct å’Œ Dream-7B-Instruct ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œç›¸æ¯”åŸºäºç½®ä¿¡åº¦çš„å¹¶è¡Œè§£ç ï¼ŒPVF åœ¨ä¿æŒå‡†ç¡®ç‡çš„åŒæ—¶å°†å‡½æ•°è°ƒç”¨æ¬¡æ•°(NFE)é™ä½äº†å¤šè¾¾ 65%ã€‚è¯¥æ–¹æ³•æ˜¾è‘—æå‡äº† DLMs çš„ç”Ÿæˆæ•ˆç‡ï¼Œä¸ºéè‡ªå›å½’èŒƒå¼çš„æ–‡æœ¬ç”Ÿæˆæä¾›äº†æ›´å…·è§„åˆ’æ€§çš„è§£ç è·¯å¾„ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2601.12247v1",
      "published_date": "2026-01-18 03:53:01 UTC",
      "updated_date": "2026-01-18 03:53:01 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-22T23:34:30.852524+00:00"
    },
    {
      "arxiv_id": "2601.12243v1",
      "title": "Less is More: Label-Guided Summarization of Procedural and Instructional Videos",
      "title_zh": "å°‘å³æ˜¯å¤šï¼šåŸºäºæ ‡ç­¾å¼•å¯¼çš„æµç¨‹ä¸æ•™å­¦è§†é¢‘æ‘˜è¦",
      "authors": [
        "Shreya Rajpal",
        "Michal Golovanesky",
        "Carsten Eickhoff"
      ],
      "abstract": "Video summarization helps turn long videos into clear, concise representations that are easier to review, document, and analyze, especially in high-stakes domains like surgical training. Prior work has progressed from using basic visual features like color, motion, and structural changes to using pre-trained vision-language models that can better understand what's happening in the video (semantics) and capture temporal flow, resulting in more context-aware video summarization. We propose a three-stage framework, PRISM: Procedural Representation via Integrated Semantic and Multimodal analysis, that produces semantically grounded video summaries. PRISM combines adaptive visual sampling, label-driven keyframe anchoring, and contextual validation using a large language model (LLM). Our method ensures that selected frames reflect meaningful and procedural transitions while filtering out generic or hallucinated content, resulting in contextually coherent summaries across both domain-specific and instructional videos. We evaluate our method on instructional and activity datasets, using reference summaries for instructional videos. Despite sampling fewer than 5% of the original frames, our summaries retain 84% semantic content while improving over baselines by as much as 33%. Our approach generalizes across procedural and domain-specific video tasks, achieving strong performance with both semantic alignment and precision.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† PRISM (Procedural Representation via Integrated Semantic and Multimodal analysis)ï¼Œä¸€ç§æ—¨åœ¨ä¸ºç¨‹åºåŒ–å’Œæ•™å­¦è§†é¢‘ç”Ÿæˆè¯­ä¹‰åŒ–æ‘˜è¦çš„ä¸‰é˜¶æ®µæ¡†æ¶ã€‚è¯¥æ¡†æ¶ç»“åˆäº†è‡ªé€‚åº”è§†è§‰é‡‡æ · (adaptive visual sampling)ã€æ ‡ç­¾é©±åŠ¨çš„å…³é”®å¸§é”šå®š (label-driven keyframe anchoring) ä»¥åŠåŸºäºå¤§è¯­è¨€æ¨¡å‹ (LLM) çš„ä¸Šä¸‹æ–‡éªŒè¯æŠ€æœ¯ã€‚PRISM ç¡®ä¿æ‰€é€‰å…³é”®å¸§èƒ½å‡†ç¡®åæ˜ ç¨‹åºæ€§è½¬æ¢ï¼ŒåŒæ—¶æœ‰æ•ˆè¿‡æ»¤é€šç”¨æˆ–å¹»è§‰å†…å®¹ã€‚å®éªŒè¡¨æ˜ï¼Œåœ¨ä»…é‡‡æ ·ä¸åˆ° 5% åŸå§‹å¸§çš„å‰æä¸‹ï¼Œè¯¥æ–¹æ³•ä¿ç•™äº† 84% çš„è¯­ä¹‰ä¿¡æ¯ï¼Œæ€§èƒ½ä¼˜äºåŸºçº¿æ¨¡å‹è¾¾ 33%ã€‚è¯¥æ–¹æ³•åœ¨ä¸åŒé¢†åŸŸçš„è§†é¢‘ä»»åŠ¡ä¸­å…·æœ‰è‰¯å¥½çš„æ³›åŒ–æ€§ï¼Œå®ç°äº†æé«˜çš„è¯­ä¹‰å¯¹é½åº¦å’Œç²¾ç¡®åº¦ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "22 pages, 6 figures",
      "pdf_url": "https://arxiv.org/pdf/2601.12243v1",
      "published_date": "2026-01-18 03:41:48 UTC",
      "updated_date": "2026-01-18 03:41:48 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-22T23:34:32.444677+00:00"
    },
    {
      "arxiv_id": "2601.12242v1",
      "title": "Optimal Power Allocation and Sub-Optimal Channel Assignment for Downlink NOMA Systems Using Deep Reinforcement Learning",
      "title_zh": "åŸºäºæ·±åº¦å¼ºåŒ–å­¦ä¹ çš„ä¸‹è¡Œé“¾è·¯ NOMA ç³»ç»Ÿæœ€ä¼˜åŠŸç‡åˆ†é…ä¸æ¬¡ä¼˜ä¿¡é“åˆ†é…",
      "authors": [
        "WooSeok Kim",
        "Jeonghoon Lee",
        "Sangho Kim",
        "Taesun An",
        "WonMin Lee",
        "Dowon Kim",
        "Kyungseop Shin"
      ],
      "abstract": "In recent years, Non-Orthogonal Multiple Access (NOMA) system has emerged as a promising candidate for multiple access frameworks due to the evolution of deep machine learning, trying to incorporate deep machine learning into the NOMA system. The main motivation for such active studies is the growing need to optimize the utilization of network resources as the expansion of the internet of things (IoT) caused a scarcity of network resources. The NOMA addresses this need by power multiplexing, allowing multiple users to access the network simultaneously. Nevertheless, the NOMA system has few limitations. Several works have proposed to mitigate this, including the optimization of power allocation known as joint resource allocation(JRA) method, and integration of the JRA method and deep reinforcement learning (JRA-DRL). Despite this, the channel assignment problem remains unclear and requires further investigation. In this paper, we propose a deep reinforcement learning framework incorporating replay memory with an on-policy algorithm, allocating network resources in a NOMA system to generalize the learning. Also, we provide extensive simulations to evaluate the effects of varying the learning rate, batch size, type of model, and the number of features in the state.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ç‰©è”ç½‘ï¼ˆIoTï¼‰æ‰©å±•å¯¼è‡´çš„èµ„æºåŒ®ä¹æŒ‘æˆ˜ï¼Œèšç„¦äºéæ­£äº¤å¤šå€æ¥å…¥ï¼ˆNOMAï¼‰ç³»ç»Ÿä¸­çš„èµ„æºä¼˜åŒ–ï¼Œç‰¹åˆ«æ˜¯ç°æœ‰ç ”ç©¶ä¸­å°šä¸æ˜ç¡®çš„ä¿¡é“åˆ†é…ï¼ˆchannel assignmentï¼‰é—®é¢˜ã€‚ä½œè€…æå‡ºäº†ä¸€ç§ç»“åˆé‡æ”¾å†…å­˜ï¼ˆreplay memoryï¼‰ä¸åŒç­–ç®—æ³•ï¼ˆon-policy algorithmï¼‰çš„æ·±åº¦å¼ºåŒ–å­¦ä¹ ï¼ˆDeep Reinforcement Learning, DRLï¼‰æ¡†æ¶ï¼Œç”¨äºä¼˜åŒ–ä¸‹è¡Œé“¾è·¯ NOMA ç³»ç»Ÿçš„åŠŸç‡åˆ†é…ï¼ˆpower allocationï¼‰ä¸èµ„æºç®¡ç†ã€‚é€šè¿‡å¹¿æ³›çš„ä»¿çœŸå®éªŒï¼Œç ”ç©¶æ·±å…¥æ¢è®¨äº†å­¦ä¹ ç‡ã€æ‰¹é‡å¤§å°ã€æ¨¡å‹æ¶æ„ä»¥åŠçŠ¶æ€ç‰¹å¾ç»´æ•°å¯¹å­¦ä¹ æ³›åŒ–èƒ½åŠ›åŠç³»ç»Ÿæ€§èƒ½çš„å½±å“ã€‚è¯¥æ–¹æ³•ä¸ºæå‡ NOMA ç½‘ç»œèµ„æºåˆ©ç”¨ç‡æä¾›äº†ä¸€ç§æ›´å…·æ³›åŒ–æ€§çš„å¼ºåŒ–å­¦ä¹ è§£å†³æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.AI",
        "cs.LG",
        "cs.NI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2601.12242v1",
      "published_date": "2026-01-18 03:37:40 UTC",
      "updated_date": "2026-01-18 03:37:40 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-22T23:34:34.868514+00:00"
    },
    {
      "arxiv_id": "2601.14302v1",
      "title": "DDSA: Dual-Domain Strategic Attack for Spatial-Temporal Efficiency in Adversarial Robustness Testing",
      "title_zh": "DDSAï¼šé¢å‘å¯¹æŠ—é²æ£’æ€§æµ‹è¯•æ—¶ç©ºæ•ˆç‡çš„åŒåŸŸç­–ç•¥æ€§æ”»å‡»",
      "authors": [
        "Jinwei Hu",
        "Shiyuan Meng",
        "Yi Dong",
        "Xiaowei Huang"
      ],
      "abstract": "Image transmission and processing systems in resource-critical applications face significant challenges from adversarial perturbations that compromise mission-specific object classification. Current robustness testing methods require excessive computational resources through exhaustive frame-by-frame processing and full-image perturbations, proving impractical for large-scale deployments where massive image streams demand immediate processing. This paper presents DDSA (Dual-Domain Strategic Attack), a resource-efficient adversarial robustness testing framework that optimizes testing through temporal selectivity and spatial precision. We introduce a scenario-aware trigger function that identifies critical frames requiring robustness evaluation based on class priority and model uncertainty, and employ explainable AI techniques to locate influential pixel regions for targeted perturbation. Our dual-domain approach achieves substantial temporal-spatial resource conservation while maintaining attack effectiveness. The framework enables practical deployment of comprehensive adversarial robustness testing in resource-constrained real-time applications where computational efficiency directly impacts mission success.",
      "tldr_zh": "### è®ºæ–‡ TLDR æ‘˜è¦ ğŸ“\n\n---\n\nè¯¥ç ”ç©¶æå‡ºäº† DDSA (Dual-Domain Strategic Attack)ï¼Œè¿™æ˜¯ä¸€ç§æ—¨åœ¨ä¼˜åŒ–å¯¹æŠ—é²æ£’æ€§æµ‹è¯•æ—¶ç©ºæ•ˆç‡çš„èµ„æºé«˜æ•ˆå‹æ¡†æ¶ã€‚ä¸ºäº†è§£å†³ä¼ ç»Ÿæ–¹æ³•åœ¨å¤„ç†å¤§è§„æ¨¡å›¾åƒæµæ—¶é¢ä¸´çš„è®¡ç®—èµ„æºè¿‡åº¦æ¶ˆè€—é—®é¢˜ï¼ŒDDSA å¼•å…¥äº† scenario-aware trigger functionï¼Œæ ¹æ®ç±»åˆ«ä¼˜å…ˆçº§å’Œæ¨¡å‹ä¸ç¡®å®šæ€§ç²¾å‡†è¯†åˆ«éœ€è¦è¯„ä¼°çš„å…³é”®å¸§ã€‚åŒæ—¶ï¼Œè¯¥æ¡†æ¶åˆ©ç”¨ explainable AI æŠ€æœ¯å®šä½å½±å“é‡å¤§çš„åƒç´ åŒºåŸŸè¿›è¡Œé’ˆå¯¹æ€§æ‰°åŠ¨ï¼Œå®ç°äº†åœ¨æ—¶é—´ä¸ç©ºé—´ç»´åº¦çš„åŒåŸŸèµ„æºèŠ‚çœã€‚è¿™ç§æ–¹æ³•åœ¨ä¿æŒæ”»å‡»æœ‰æ•ˆæ€§çš„åŒæ—¶å¤§å¹…æå‡äº†æµ‹è¯•æ•ˆç‡ï¼Œä¸ºèµ„æºå—é™çš„å®æ—¶åº”ç”¨åœºæ™¯ä¸­éƒ¨ç½²å…¨é¢çš„å¯¹æŠ—é²æ£’æ€§æµ‹è¯•æä¾›äº†å®é™…å¯è¡Œçš„æ–¹æ¡ˆã€‚\n\n---\n\nå¸Œæœ›è¿™ä¸ªæ‘˜è¦å¯¹æ‚¨çš„ç ”ç©¶æœ‰æ‰€å¸®åŠ©ï¼å¦‚æœæ‚¨è¿˜æœ‰å…¶ä»–è®ºæ–‡éœ€è¦å¤„ç†ï¼Œæ¬¢è¿éšæ—¶å‘Šè¯‰æˆ‘ã€‚",
      "categories": [
        "cs.CR",
        "cs.AI",
        "cs.PF"
      ],
      "primary_category": "cs.CR",
      "comment": "Preprint accepted by ICASSP 2026 with minor revisions",
      "pdf_url": "https://arxiv.org/pdf/2601.14302v1",
      "published_date": "2026-01-18 03:14:22 UTC",
      "updated_date": "2026-01-18 03:14:22 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-22T23:34:40.181137+00:00"
    },
    {
      "arxiv_id": "2601.12234v1",
      "title": "Proc3D: Procedural 3D Generation and Parametric Editing of 3D Shapes with Large Language Models",
      "title_zh": "Proc3Dï¼šåŸºäºå¤§è¯­è¨€æ¨¡å‹çš„3Då½¢çŠ¶è¿‡ç¨‹åŒ–ç”Ÿæˆä¸å‚æ•°åŒ–ç¼–è¾‘",
      "authors": [
        "Fadlullah Raji",
        "Stefano Petrangeli",
        "Matheus Gadelha",
        "Yu Shen",
        "Uttaran Bhattacharya",
        "Gang Wu"
      ],
      "abstract": "Generating 3D models has traditionally been a complex task requiring specialized expertise. While recent advances in generative AI have sought to automate this process, existing methods produce non-editable representation, such as meshes or point clouds, limiting their adaptability for iterative design. In this paper, we introduce Proc3D, a system designed to generate editable 3D models while enabling real-time modifications. At its core, Proc3D introduces procedural compact graph (PCG), a graph representation of 3D models, that encodes the algorithmic rules and structures necessary for generating the model. This representation exposes key parameters, allowing intuitive manual adjustments via sliders and checkboxes, as well as real-time, automated modifications through natural language prompts using Large Language Models (LLMs). We demonstrate Proc3D's capabilities using two generative approaches: GPT-4o with in-context learning (ICL) and a fine-tuned LLAMA-3 model. Experimental results show that Proc3D outperforms existing methods in editing efficiency, achieving more than 400x speedup over conventional approaches that require full regeneration for each modification. Additionally, Proc3D improves ULIP scores by 28%, a metric that evaluates the alignment between generated 3D models and text prompts. By enabling text-aligned 3D model generation along with precise, real-time parametric edits, Proc3D facilitates highly accurate text-based image editing applications.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† Proc3Dï¼Œä¸€ç§åˆ©ç”¨å¤§è¯­è¨€æ¨¡å‹ (LLMs) ç”Ÿæˆå¯ç¼–è¾‘ 3D æ¨¡å‹å¹¶æ”¯æŒå®æ—¶å‚æ•°åŒ–ä¿®æ”¹çš„ç³»ç»Ÿã€‚å…¶æ ¸å¿ƒåˆ›æ–°æ˜¯å¼•å…¥äº†ç¨‹åºåŒ–ç´§å‡‘å›¾ (Procedural Compact Graph, PCG) è¡¨å¾ï¼Œé€šè¿‡å›¾ç»“æ„ç¼–ç  3D æ¨¡å‹çš„ç®—æ³•è§„åˆ™ï¼Œå¹¶å‘ç”¨æˆ·æš´éœ²å…³é”®æ§åˆ¶å‚æ•°ã€‚è¯¥ç³»ç»Ÿå…è®¸ç”¨æˆ·é€šè¿‡æ‰‹åŠ¨æ»‘å—æˆ–åŸºäºè‡ªç„¶è¯­è¨€æç¤ºï¼ˆåˆ©ç”¨ GPT-4o æˆ–å¾®è°ƒåçš„ LLAMA-3ï¼‰è¿›è¡Œç›´è§‚ä¸”ç²¾ç¡®çš„ç¼–è¾‘ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒProc3D åœ¨ç¼–è¾‘æ•ˆç‡ä¸Šæ¯”éœ€è¦å®Œå…¨é‡æ–°ç”Ÿæˆçš„ä¼ ç»Ÿæ–¹æ³•å¿« 400 å€ä»¥ä¸Šï¼ŒåŒæ—¶åœ¨è¡¡é‡æ–‡æœ¬ä¸€è‡´æ€§çš„ ULIP æŒ‡æ ‡ä¸Šæå‡äº† 28%ï¼Œä¸ºé«˜ç²¾åº¦çš„æ–‡æœ¬é©±åŠ¨ 3D å»ºæ¨¡ä¸è®¾è®¡æä¾›äº†é«˜æ•ˆè§£å†³æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.GR",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.GR",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2601.12234v1",
      "published_date": "2026-01-18 03:08:08 UTC",
      "updated_date": "2026-01-18 03:08:08 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-22T23:34:41.742904+00:00"
    },
    {
      "arxiv_id": "2601.12224v1",
      "title": "Where It Moves, It Matters: Referring Surgical Instrument Segmentation via Motion",
      "title_zh": "åŠ¨ä¹‹æ‰€åœ¨ï¼Œå…³é”®æ‰€åœ¨ï¼šåŸºäºè¿åŠ¨çš„æ‰‹æœ¯å™¨æ¢°æŒ‡ä»£åˆ†å‰²",
      "authors": [
        "Meng Wei",
        "Kun Yuan",
        "Shi Li",
        "Yue Zhou",
        "Long Bai",
        "Nassir Navab",
        "Hongliang Ren",
        "Hong Joo Lee",
        "Tom Vercauteren",
        "Nicolas Padoy"
      ],
      "abstract": "Enabling intuitive, language-driven interaction with surgical scenes is a critical step toward intelligent operating rooms and autonomous surgical robotic assistance. However, the task of referring segmentation, localizing surgical instruments based on natural language descriptions, remains underexplored in surgical videos, with existing approaches struggling to generalize due to reliance on static visual cues and predefined instrument names. In this work, we introduce SurgRef, a novel motion-guided framework that grounds free-form language expressions in instrument motion, capturing how tools move and interact across time, rather than what they look like. This allows models to understand and segment instruments even under occlusion, ambiguity, or unfamiliar terminology. To train and evaluate SurgRef, we present Ref-IMotion, a diverse, multi-institutional video dataset with dense spatiotemporal masks and rich motion-centric expressions. SurgRef achieves state-of-the-art accuracy and generalization across surgical procedures, setting a new benchmark for robust, language-driven surgical video segmentation.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹æ‰‹æœ¯å™¨æ¢°æŒ‡ä»£åˆ†å‰² (Referring Surgical Instrument Segmentation) é¢†åŸŸè¿‡åº¦ä¾èµ–é™æ€è§†è§‰çº¿ç´¢å’Œé¢„å®šä¹‰åç§°çš„é—®é¢˜ï¼Œæå‡ºäº† SurgRef æ¡†æ¶ã€‚SurgRef é‡‡ç”¨è¿åŠ¨å¼•å¯¼ (motion-guided) æœºåˆ¶ï¼Œå°†è‡ªç”±å½¢å¼çš„è¯­è¨€è¡¨è¾¾ä¸å™¨æ¢°åœ¨æ—¶é—´ç»´åº¦ä¸Šçš„è¿åŠ¨å’Œäº¤äº’é€»è¾‘ç›¸ç»“åˆï¼Œä½¿å…¶èƒ½åœ¨é®æŒ¡ã€æ­§ä¹‰æˆ–æœ¯è¯­ä¸ç†Ÿæ‚‰çš„æƒ…å†µä¸‹ä¾ç„¶å®ç°ç²¾å‡†å®šä½ã€‚ä¸ºæ”¯æŒè¯¥æ¡†æ¶çš„è®­ç»ƒä¸è¯„ä¼°ï¼Œç ”ç©¶è€…æ¨å‡ºäº† Ref-IMotion æ•°æ®é›†ï¼ŒåŒ…å«å¤šä¸­å¿ƒè§†é¢‘ã€ç¨ å¯†æ—¶ç©ºæ©ç  (spatiotemporal masks) åŠä¸°å¯Œçš„ä»¥è¿åŠ¨ä¸ºä¸­å¿ƒçš„æè¿°ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒSurgRef åœ¨å¤šç§æ‰‹æœ¯ç¨‹åºä¸­å‡è¾¾åˆ°äº† SOTA çš„å‡†ç¡®ç‡å’Œæ³›åŒ–æ€§èƒ½ï¼Œä¸ºé²æ£’çš„è¯­è¨€é©±åŠ¨æ‰‹æœ¯è§†é¢‘åˆ†å‰²å»ºç«‹äº†æ–°åŸºå‡†ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2601.12224v1",
      "published_date": "2026-01-18 02:14:08 UTC",
      "updated_date": "2026-01-18 02:14:08 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-22T23:34:58.640830+00:00"
    },
    {
      "arxiv_id": "2601.12215v1",
      "title": "Wavelet-Driven Masked Multiscale Reconstruction for PPG Foundation Models",
      "title_zh": "é¢å‘ PPG åŸºç¡€æ¨¡å‹çš„å°æ³¢é©±åŠ¨æ©ç å¤šå°ºåº¦é‡å»º",
      "authors": [
        "Megha Thukral",
        "Cyrus Tanade",
        "Simon A. Lee",
        "Juhyeon Lee",
        "Hao Zhou",
        "Keum San Chun",
        "Migyeong Gwak",
        "Viswam Nathan",
        "Md Mahbubur Rahman",
        "Li Zhu",
        "Mehrab Bin Morshed",
        "Subramaniam Venkatraman",
        "Sharanya Arcot Desai"
      ],
      "abstract": "Wearable foundation models have the potential to transform digital health by learning transferable representations from large-scale biosignals collected in everyday settings. While recent progress has been made in large-scale pretraining, most approaches overlook the spectral structure of photoplethysmography (PPG) signals, wherein physiological rhythms unfold across multiple frequency bands. Motivated by the insight that many downstream health-related tasks depend on multi-resolution features spanning fine-grained waveform morphology to global rhythmic dynamics, we introduce Masked Multiscale Reconstruction (MMR) for PPG representation learning - a self-supervised pretraining framework that explicitly learns from hierarchical time-frequency scales of PPG data. The pretraining task is designed to reconstruct randomly masked out coefficients obtained from a wavelet-based multiresolution decomposition of PPG signals, forcing the transformer encoder to integrate information across temporal and spectral scales. We pretrain our model with MMR using ~17 million unlabeled 10-second PPG segments from ~32,000 smartwatch users. On 17 of 19 diverse health-related tasks, MMR trained on large-scale wearable PPG data improves over or matches state-of-the-art open-source PPG foundation models, time-series foundation models, and other self-supervised baselines. Extensive analysis of our learned embeddings and systematic ablations underscores the value of wavelet-based representations, showing that they capture robust and physiologically-grounded features. Together, these results highlight the potential of MMR as a step toward generalizable PPG foundation models.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ç°æœ‰å…‰ç”µå®¹ç§¯è„‰ææ³¢ (PPG) åŸºç¡€æ¨¡å‹å¿½ç•¥ä¿¡å·é¢‘è°±ç»“æ„çš„é—®é¢˜ï¼Œæå‡ºäº† Masked Multiscale Reconstruction (MMR) è‡ªç›‘ç£é¢„è®­ç»ƒæ¡†æ¶ã€‚MMR é€šè¿‡é‡å»ºä»å°æ³¢å¤šåˆ†è¾¨ç‡åˆ†è§£ (wavelet-based multiresolution decomposition) ä¸­æå–çš„éšæœºé®è”½ç³»æ•°ï¼Œä¿ƒä½¿ Transformer ç¼–ç å™¨æ•´åˆè·¨æ—¶é—´å’Œé¢‘è°±å°ºåº¦çš„å±‚æ¬¡åŒ–ç‰¹å¾ã€‚ç ”ç©¶åˆ©ç”¨çº¦ 32,000 åæ™ºèƒ½æ‰‹è¡¨ç”¨æˆ·æä¾›çš„ 1,700 ä¸‡æ¡ PPG æ•°æ®ç‰‡æ®µè¿›è¡Œå¤§è§„æ¨¡é¢„è®­ç»ƒã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒMMR åœ¨ 19 é¡¹å¥åº·ç›¸å…³ä»»åŠ¡ä¸­çš„ 17 é¡¹ä¸Šä¼˜äºæˆ–ç­‰åŒäºç°æœ‰çš„ SOTA åŸºç¡€æ¨¡å‹åŠè‡ªç›‘ç£åŸºçº¿ã€‚è¯¥å·¥ä½œè¯æ˜äº†åŸºäºå°æ³¢çš„è¡¨å¾èƒ½æœ‰æ•ˆæ•æ‰å…·æœ‰ç”Ÿç†ä¾æ®çš„ç¨³å¥ç‰¹å¾ï¼Œä¸ºæ„å»ºé€šç”¨çš„ PPG åŸºç¡€æ¨¡å‹å¥ å®šäº†åŸºç¡€ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2601.12215v1",
      "published_date": "2026-01-18 01:34:47 UTC",
      "updated_date": "2026-01-18 01:34:47 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-22T23:34:55.328236+00:00"
    },
    {
      "arxiv_id": "2601.12212v1",
      "title": "Speculative Sampling with Reinforcement Learning",
      "title_zh": "åŸºäºå¼ºåŒ–å­¦ä¹ çš„æŠ•æœºé‡‡æ ·",
      "authors": [
        "Chenan Wang",
        "Daniel H. Shi",
        "Haipeng Chen"
      ],
      "abstract": "Inference time latency has remained an open challenge for real world applications of large language models (LLMs). State-of-the-art (SOTA) speculative sampling (SpS) methods for LLMs, like EAGLE-3, use tree-based drafting to explore multiple candidate continuations in parallel. However, the hyperparameters controlling the tree structure are static, which limits flexibility and efficiency across diverse contexts and domains. We introduce Reinforcement learning for Speculative Sampling (Re-SpS), the first reinforcement learning (RL)-based framework for draft tree hyperparameter optimization. Re-SpS dynamically adjusts draft tree hyperparameters in real-time, learning context-aware policies that maximize generation speed by balancing speculative aggression with computational overhead. It leverages efficient state representations from target model hidden states and introduces multi-step action persistence for better context modeling. Evaluation results across five diverse benchmarks demonstrate consistent improvements over the SOTA method EAGLE-3, achieving up to 5.45$\\times$ speedup over the backbone LLM and up to 1.12$\\times$ speedup compared to EAGLE-3 across five diverse benchmarks, with no loss in output fidelity.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†Re-SpS (Reinforcement learning for Speculative Sampling)ï¼Œè¿™æ˜¯é¦–ä¸ªåŸºäºå¼ºåŒ–å­¦ä¹ (RL)çš„æŠ•æœºé‡‡æ ·è¶…å‚æ•°ä¼˜åŒ–æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³å¤§è¯­è¨€æ¨¡å‹(LLMs)çš„æ¨ç†å»¶è¿Ÿé—®é¢˜ã€‚ä¸EAGLE-3ç­‰é‡‡ç”¨é™æ€è¶…å‚æ•°çš„SOTAæ–¹æ³•ä¸åŒï¼ŒRe-SpSèƒ½å¤Ÿæ ¹æ®ä¸Šä¸‹æ–‡å®æ—¶åŠ¨æ€è°ƒæ•´è‰ç¨¿æ ‘(draft tree)ç»“æ„ï¼Œé€šè¿‡å¹³è¡¡æŠ•æœºç§¯æåº¦ä¸è®¡ç®—å¼€é”€æ¥æœ€å¤§åŒ–ç”Ÿæˆé€Ÿåº¦ã€‚è¯¥æ¡†æ¶åˆ©ç”¨ç›®æ ‡æ¨¡å‹çš„éšè—çŠ¶æ€(hidden states)è¿›è¡ŒçŠ¶æ€è¡¨ç¤ºï¼Œå¹¶å¼•å…¥å¤šæ­¥åŠ¨ä½œæŒä¹…åŒ–(multi-step action persistence)ä»¥å¢å¼ºä¸Šä¸‹æ–‡å»ºæ¨¡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒRe-SpSåœ¨äº”ä¸ªåŸºå‡†æµ‹è¯•ä¸­å‡è¡¨ç°ä¼˜äºEAGLE-3ï¼Œå®ç°äº†æ¯”åŸºåº§LLMé«˜è¾¾5.45å€ã€æ¯”EAGLE-3é«˜è¾¾1.12å€çš„åŠ é€Ÿï¼Œä¸”å®Œå…¨ä¸å½±å“è¾“å‡ºçš„ä¿çœŸåº¦ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "Accepted to AAAI 2026",
      "pdf_url": "https://arxiv.org/pdf/2601.12212v1",
      "published_date": "2026-01-18 01:31:29 UTC",
      "updated_date": "2026-01-18 01:31:29 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-22T23:34:59.196097+00:00"
    },
    {
      "arxiv_id": "2601.12205v1",
      "title": "Do Neural Codecs Generalize? A Controlled Study Across Unseen Languages and Non-Speech Tasks",
      "title_zh": "ç¥ç»ç¼–è§£ç å™¨æ˜¯å¦å…·æœ‰æ³›åŒ–èƒ½åŠ›ï¼Ÿä¸€é¡¹é’ˆå¯¹æœªè§è¯­è¨€ä¸éè¯­éŸ³ä»»åŠ¡çš„å—æ§ç ”ç©¶",
      "authors": [
        "Shih-Heng Wang",
        "Jiatong Shi",
        "Jinchuan Tian",
        "Haibin Wu",
        "Shinji Watanabe"
      ],
      "abstract": "This paper investigates three crucial yet underexplored aspects of the generalization capabilities of neural audio codecs (NACs): (i) whether NACs can generalize to unseen languages during pre-training, (ii) whether speech-only pre-trained NACs can effectively generalize to non-speech applications such as environmental sounds, music, and animal vocalizations, and (iii) whether incorporating non-speech data during pre-training can improve performance on both speech and non-speech tasks. Existing studies typically rely on off-the-shelf NACs for comparison, which limits insight due to variations in implementation. In this work, we train NACs from scratch using strictly controlled configurations and carefully curated pre-training data to enable fair comparisons. We conduct a comprehensive evaluation of NAC performance on both signal reconstruction quality and downstream applications using 11 metrics. Our results show that NACs can generalize to unseen languages during pre-training, speech-only pre-trained NACs exhibit degraded performance on non-speech tasks, and incorporating non-speech data during pre-training improves performance on non-speech tasks while maintaining comparable performance on speech tasks.",
      "tldr_zh": "è¯¥ç ”ç©¶æ·±å…¥æ¢è®¨äº†ç¥ç»ç½‘ç»œéŸ³é¢‘ç¼–è§£ç å™¨(Neural Audio Codecs, NACs)åœ¨æœªè§è¯­è¨€å’Œéè¯­éŸ³ä»»åŠ¡ï¼ˆå¦‚ç¯å¢ƒéŸ³ã€éŸ³ä¹å’ŒåŠ¨ç‰©å«å£°ï¼‰ä¸­çš„æ³›åŒ–èƒ½åŠ›ã€‚ç ”ç©¶äººå‘˜é€šè¿‡ä»é›¶å¼€å§‹è®­ç»ƒå…·æœ‰ä¸¥æ ¼å—æ§å˜é‡çš„ NACsï¼Œç³»ç»Ÿè¯„ä¼°äº†å…¶åœ¨ä¿¡å·é‡å»ºè´¨é‡åŠä¸‹æ¸¸åº”ç”¨ä¸­çš„ 11 é¡¹æŒ‡æ ‡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒNACs èƒ½å¤Ÿæœ‰æ•ˆæ³›åŒ–è‡³é¢„è®­ç»ƒé˜¶æ®µæœªè§è¿‡çš„è¯­è¨€ï¼Œä½†ä»…åœ¨è¯­éŸ³æ•°æ®ä¸Šé¢„è®­ç»ƒçš„æ¨¡å‹åœ¨éè¯­éŸ³ä»»åŠ¡ä¸­è¡¨ç°ä¼šæœ‰æ‰€ä¸‹é™ã€‚æ­¤å¤–ï¼Œç ”ç©¶è¯æ˜åœ¨é¢„è®­ç»ƒä¸­åŠ å…¥éè¯­éŸ³æ•°æ®å¯ä»¥æ˜¾è‘—æå‡éè¯­éŸ³ä»»åŠ¡çš„æ€§èƒ½ï¼ŒåŒæ—¶ä¿æŒæ¨¡å‹åœ¨è¯­éŸ³ä»»åŠ¡ä¸Šçš„é«˜æ°´å¹³è¡¨ç°ã€‚",
      "categories": [
        "cs.SD",
        "cs.AI",
        "eess.AS"
      ],
      "primary_category": "cs.SD",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2601.12205v1",
      "published_date": "2026-01-18 00:53:11 UTC",
      "updated_date": "2026-01-18 00:53:11 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-22T23:35:00.042785+00:00"
    }
  ],
  "processing_status": "completed",
  "error": null,
  "raw_papers_fetched": true,
  "papers_count": 74,
  "processed_papers_count": 74,
  "failed_papers_count": 0,
  "llm_backup_calls": 0,
  "summary_generated": true,
  "daily_data_saved": true,
  "last_update": "2026-01-22T23:37:21.414303+00:00"
}
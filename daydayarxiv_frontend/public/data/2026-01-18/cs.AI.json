{
  "date": "2026-01-18",
  "category": "cs.AI",
  "summary": "ä½ å¥½ï¼æ¬¢è¿æ¥åˆ° UTC æ—¶é—´ 2026-01-18 çš„ arXiv ä¸­æ–‡ TLDR å¿«æŠ¥ï¼\n\n**ä»Šæ—¥æ€»ç»“**ï¼š\nä»Šå¤©æ˜¯ **\"Agentic AI\" (ä»£ç†äººå·¥æ™ºèƒ½)** å’Œ **\"AI for Science\"** çˆ†å‘çš„ä¸€å¤©ã€‚æˆ‘ä»¬çœ‹åˆ°äº†å¤šç¯‡å…³äº Agent æ¶æ„ã€æ¨ç†ï¼ˆReasoningï¼‰ä»¥åŠå¤šæ™ºèƒ½ä½“ååŒå·¥ä½œçš„é‡ç£…ç»¼è¿°å’Œæ¡†æ¶æå‡ºã€‚æ­¤å¤–ï¼Œå…³äº LLM çš„â€œå¿ƒæ™ºç†è®ºâ€èƒ½åŠ›ï¼ˆæ˜¯å¦æ¯”é»‘çŒ©çŒ©èªæ˜ï¼Ÿï¼‰ä»¥åŠé’ˆå¯¹å¤šæ¨¡æ€ç³»ç»Ÿçš„å®‰å…¨æ”»å‡»ï¼ˆå¦‚ GUI åŠ«æŒå’Œ VLM æ’åæ“çºµï¼‰ä¹Ÿæ˜¯ä»Šå¤©çš„äº®ç‚¹ã€‚\n\n---\n\n### ğŸš€ Agentic AIï¼šæ¶æ„ã€æ¨ç†ä¸ç§‘å­¦å‘ç°\nä»Šå¤©çš„é‡å¤´æˆæ˜¯æ™ºèƒ½ä½“ï¼ˆAgentsï¼‰ã€‚ä»ç»¼è¿°åˆ°å…·ä½“çš„ç§‘å­¦å·¥ä½œæµï¼ŒAgent æ­£ä»å•ä¸€çš„å¯¹è¯æ¡†èµ°å‘è‡ªä¸»çš„ä»»åŠ¡æ‰§è¡Œã€‚\n\n**1. ä»£ç†äººå·¥æ™ºèƒ½ (AI)ï¼šæ¶æ„ã€åˆ†ç±»ä¸å¤§è¯­è¨€æ¨¡å‹æ™ºèƒ½ä½“è¯„ä¼°**\n**# Agentic Artificial Intelligence (AI): Architectures, Taxonomies, and Evaluation of Large Language Model Agents**\nè¿™æ˜¯ä¸€ç¯‡éå¸¸è¯¦å°½çš„ç»¼è¿°ï¼ˆ28é¡µï¼‰ã€‚ä½œè€…æå‡ºäº†ä¸€ä¸ªç»Ÿä¸€çš„åˆ†ç±»æ³•ï¼Œå°† Agent æ‹†è§£ä¸ºæ„ŸçŸ¥ã€å¤§è„‘ã€è§„åˆ’ã€è¡ŒåŠ¨ã€å·¥å…·ä½¿ç”¨å’Œåä½œå…­å¤§æ¨¡å—ã€‚æ–‡ç« æ¢è®¨äº†ä»çº¿æ€§æ¨ç†åˆ°æ¨ç†æ—¶ï¼ˆinference timeï¼‰æ¨¡å‹çš„è½¬å˜ï¼Œä»¥åŠä»å›ºå®š API åˆ°å¼€æ”¾æ ‡å‡†ï¼ˆå¦‚ MCPï¼‰çš„æ¼”è¿›ã€‚\n*   **æ ¸å¿ƒè´¡çŒ®**ï¼šæä¾›äº† Agent è®¾è®¡çš„ç»Ÿä¸€åˆ†ç±»æ³•ï¼Œå¹¶æŒ‡å‡ºäº†â€œè¡ŒåŠ¨ä¸­çš„å¹»è§‰â€å’Œâ€œæ— é™å¾ªç¯â€ç­‰å…³é”®æŒ‘æˆ˜ã€‚\n\n**2. å¤§è¯­è¨€æ¨¡å‹çš„ä»£ç†æ¨ç†**\n**# Agentic Reasoning for Large Language Models**\nå¦ä¸€ç¯‡é‡ç£…ç»¼è¿°ã€‚è¿™ç¯‡æ–‡ç« å°† Agentic Reasoning åˆ†ä¸ºä¸‰ä¸ªå±‚é¢ï¼šåŸºç¡€ä»£ç†æ¨ç†ï¼ˆå•æ™ºèƒ½ä½“è§„åˆ’ã€å·¥å…·ä½¿ç”¨ï¼‰ã€è‡ªè¿›åŒ–ä»£ç†æ¨ç†ï¼ˆé€šè¿‡åé¦ˆå’Œè®°å¿†è‡ªæˆ‘å®Œå–„ï¼‰å’Œé›†ä½“å¤šæ™ºèƒ½ä½“æ¨ç†ï¼ˆåä½œï¼‰ã€‚\n*   **æ ¸å¿ƒè´¡çŒ®**ï¼šå»ºç«‹äº†ä¸€ä¸ªè¿æ¥â€œæ€ç»´â€ä¸â€œè¡ŒåŠ¨â€çš„ç»Ÿä¸€è·¯çº¿å›¾ï¼Œå¼ºè°ƒäº†åŠ¨æ€ç¯å¢ƒä¸‹çš„æŒç»­äº¤äº’å­¦ä¹ ã€‚\n\n**3. é‡æ–°æ€è€ƒ AI ç§‘å­¦å®¶ï¼šç”¨äºç§‘å­¦å‘ç°çš„äº¤äº’å¼å¤šæ™ºèƒ½ä½“å·¥ä½œæµ**\n**# Rethinking the AI Scientist: Interactive Multi-Agent Workflows for Scientific Discovery**\né’ˆå¯¹ç›®å‰ AI ç§‘ç ”ç³»ç»Ÿä¸»è¦æ˜¯æ‰¹å¤„ç†æ¨¡å¼ï¼ˆè€—æ—¶æ•°å°æ—¶ï¼‰çš„ç—›ç‚¹ï¼Œä½œè€…æå‡ºäº† \"Deep Research\" ç³»ç»Ÿã€‚è¿™æ˜¯ä¸€ä¸ªå¤šæ™ºèƒ½ä½“ç³»ç»Ÿï¼Œèƒ½å°†ç§‘ç ”å‘¨æœŸå‹ç¼©åˆ°åˆ†é’Ÿçº§ï¼Œæ”¯æŒåŠè‡ªä¸»å’Œå…¨è‡ªä¸»æ¨¡å¼ã€‚\n*   **æ ¸å¿ƒå‘ç°**ï¼šåœ¨è®¡ç®—ç”Ÿç‰©å­¦åŸºå‡† BixBench ä¸Šåˆ·æ–°äº† SOTAï¼Œå‡†ç¡®ç‡å¤§å¹…é¢†å…ˆã€‚è¯æ˜äº†äº¤äº’å¼ã€åˆ†é’Ÿçº§çš„ AI ç§‘ç ”åŠ©ç†æ˜¯å¯è¡Œçš„ã€‚\n\n**4. ç”¨äºç§‘å­¦çš„äº‘åŸç”Ÿå¤šæ™ºèƒ½ä½“å·¥ä½œæµ**\n**# A Cloud-based Multi-Agentic Workflow for Science**\nè¿™å°±å¥½æ¯”æ˜¯ç»™ç§‘å­¦å®¶é…äº†ä¸€ä¸ªå…¨äº‘ç«¯çš„ AI ç§˜ä¹¦å›¢ã€‚ä½œè€…æå‡ºäº†ä¸€ä¸ªä¸é¢†åŸŸæ— å…³çš„æ¡†æ¶ï¼Œç”¨â€œç›‘ç£è€…æ™ºèƒ½ä½“â€æŒ‡æŒ¥ä¸€ç¾¤å…·å¤‡ä¸åŒèƒ½åŠ›çš„å­æ™ºèƒ½ä½“ï¼ˆæŸ¥æ–‡çŒ®ã€è·‘æ¨¡æ‹Ÿï¼‰ã€‚\n*   **æ ¸å¿ƒå‘ç°**ï¼šåœ¨åŒ–å­¦å‚¬åŒ–å‰‚ç ”ç©¶ä¸­è¿›è¡Œäº†éªŒè¯ï¼Œä»»åŠ¡è·¯ç”±å‡†ç¡®ç‡è¾¾ 90%ï¼Œè¯æ˜äº†åœ¨äº‘ç«¯å…¨è‡ªåŠ¨è·‘ç§‘å­¦å®éªŒçš„å¯è¡Œæ€§ã€‚\n\n---\n\n### ğŸ§  LLM çš„è®¤çŸ¥è¾¹ç•Œä¸æ¨ç†æœºåˆ¶\nä»Šå¤©çš„å‡ ç¯‡æ–‡ç« å¯¹ LLM çš„çœŸå®èƒ½åŠ›è¿›è¡Œäº†å†·é™çš„â€œç¥›é­…â€å’Œæ·±åº¦çš„æœºç†åˆ†æã€‚\n\n**5. LLM æ¯”é»‘çŒ©çŒ©æ›´èªæ˜å—ï¼Ÿå…³äºè§‚ç‚¹é‡‡æ‹©å’ŒçŸ¥è¯†çŠ¶æ€ä¼°è®¡çš„è¯„ä¼°**\n**# Are LLMs Smarter Than Chimpanzees? An Evaluation on Perspective Taking and Knowledge State Estimation**\næ ‡é¢˜å¾ˆå¸ç›ã€‚è®¤çŸ¥äººç±»å­¦è®¤ä¸ºäººç±»åŒºåˆ«äºåŠ¨ç‰©çš„å…³é”®åœ¨äºæ¨æ–­ä»–äººçŸ¥è¯†çŠ¶æ€ï¼ˆå¿ƒæ™ºç†è®ºï¼ŒToMï¼‰çš„èƒ½åŠ›ã€‚ç ”ç©¶å‘ç°ï¼ŒSOTA LLM åœ¨åˆ¤æ–­â€œæ•…äº‹è§’è‰²æ˜¯å¦çŸ¥é“æŸäº›ä¿¡æ¯â€ä»¥åŠâ€œåŸºäºè§’è‰²çŸ¥è¯†é¢„æµ‹å…¶è¡Œä¸ºâ€çš„ä»»åŠ¡ä¸Šï¼Œè¡¨ç°æ¥è¿‘éšæœºï¼Œç”šè‡³ä¸å¦‚äººç±»ï¼Œåœ¨æŸäº›å±‚é¢ç¼ºä¹é»‘çŒ©çŒ©éƒ½ä¸å…·å¤‡çš„è®¤çŸ¥èƒ½åŠ›ã€‚\n*   **æ ¸å¿ƒå‘ç°**ï¼šLLM åœ¨æ·±å±‚çš„ intention understanding å’ŒçŸ¥è¯†çŠ¶æ€è¿½è¸ªä¸Šä¾ç„¶å¾ˆå¼±ã€‚\n\n**6. å¤šè·³ QA ä¸­çš„å¤±æ•ˆæ¨¡å¼ï¼šæœ€å¼±ä¸€ç¯å®šå¾‹ä¸è¯†åˆ«ç“¶é¢ˆ**\n**# Failure Modes in Multi-Hop QA: The Weakest Link Law and the Recognition Bottleneck**\nLLM å¤„ç†é•¿ä¸Šä¸‹æ–‡å¤šè·³æ¨ç†ä¸ºä»€ä¹ˆä¼šå¤±è´¥ï¼Ÿä½œè€…æå‡ºäº†**â€œæœ€å¼±ä¸€ç¯å®šå¾‹â€**ï¼šå¤šè·³æ¨ç†çš„æ€§èƒ½ä¼šåç¼©åˆ°è¯æ®é“¾ä¸­â€œæœ€ä¸æ˜¾çœ¼â€çš„é‚£ä¸€ç¯çš„æ°´å¹³ã€‚\n*   **æ ¸å¿ƒå‘ç°**ï¼šå¤±è´¥ä¸»è¦å—ç»å¯¹ä½ç½®åå·®å½±å“ï¼ˆRecognition Failureï¼‰ï¼Œè€Œéè¯æ®é—´çš„è·ç¦»ã€‚é€šè¿‡ System-2 é£æ ¼çš„æ€è€ƒå¯ä»¥æœ‰æ•ˆç¼“è§£è¿™ä¸ªé—®é¢˜ã€‚\n\n**7. æ­£äº¤åŒ–ç­–ç•¥ä¼˜åŒ–ï¼šåœ¨ RLHF ä¸­è§£è€¦é‡‡æ ·å‡ ä½•ä¸ä¼˜åŒ–å‡ ä½•**\n**# Orthogonalized Policy Optimization: Decoupling Sampling Geometry from Optimization Geometry in RLHF**\nè¿™ç¯‡æ¯”è¾ƒç¡¬æ ¸ã€‚ä½œè€…æŒ‡å‡ºç›®å‰çš„å¯¹é½ç®—æ³•ï¼ˆå¦‚ PPO, DPOï¼‰æ··æ·†äº†â€œé‡‡æ ·æƒé‡â€å’Œâ€œä¼˜åŒ–æ›²ç‡â€ï¼Œå¯¼è‡´è®­ç»ƒä¸ç¨³å®šã€‚\n*   **æ ¸å¿ƒè´¡çŒ®**ï¼šæå‡ºäº† OPOï¼ˆæ­£äº¤åŒ–ç­–ç•¥ä¼˜åŒ–ï¼‰ï¼Œé€šè¿‡æ¬§å‡ é‡Œå¾—é•œåƒæ˜ å°„è§£è€¦äº†è¿™ä¸¤è€…ï¼Œæä¾›äº†æ›´ç¨³å®šçš„æ¢¯åº¦åŠ¨æ€å’Œé—­å¼è§£ã€‚\n\n---\n\n### ğŸ›¡ï¸ å®‰å…¨ã€æ”»å‡»ä¸é˜²å¾¡\néšç€ Agent å’Œå¤šæ¨¡æ€æ¨¡å‹ï¼ˆVLMï¼‰çš„åº”ç”¨ï¼Œæ”»å‡»æ‰‹æ®µä¹Ÿåœ¨è¿›åŒ–ã€‚\n\n**8. é›¶æƒé™æ“çºµï¼šæˆ‘ä»¬èƒ½ä¿¡ä»»å¤§æ˜¾å¼æ¨¡å‹é©±åŠ¨çš„ GUI æ™ºèƒ½ä½“å—ï¼Ÿ**\n**# Zero-Permission Manipulation: Can We Trust Large Multimodal Model Powered GUI Agents?**\né’ˆå¯¹å®‰å“ç«¯çš„ GUI Agent çš„æ”»å‡»ã€‚æ”»å‡»è€…ä¸éœ€è¦ä»»ä½•å±é™©æƒé™ï¼Œåˆ©ç”¨ Agent ä»â€œè§‚å¯Ÿâ€åˆ°â€œè¡ŒåŠ¨â€çš„æ—¶é—´å·®ï¼ˆVisual Atomicity å‡è®¾å¤±æ•ˆï¼‰ï¼Œé€šè¿‡åå°å¼¹çª—ç­‰æ–¹å¼â€œé‡ç»‘å®šâ€ç”¨æˆ·çš„æ“ä½œã€‚\n*   **æ ¸å¿ƒå‘ç°**ï¼šåœ¨ 6 ä¸ªä¸»æµ GUI Agent ä¸Šæ”»å‡»æˆåŠŸç‡æé«˜ï¼Œç”šè‡³èƒ½ç»•è¿‡ç¡®è®¤å¼¹çª—ã€‚è¿™æ­ç¤ºäº†å½“å‰ Agent-OS é›†æˆæ¶æ„çš„é‡å¤§ç¼ºé™·ã€‚\n\n**9. å¤šæ¨¡æ€ç”Ÿæˆå¼•æ“ä¼˜åŒ–ï¼šé’ˆå¯¹è§†è§‰è¯­è¨€æ¨¡å‹æ’åºå™¨çš„æ’åæ“çºµ**\n**# Multimodal Generative Engine Optimization: Rank Manipulation for Vision-Language Model Rankers**\nè¿™åŸºæœ¬ä¸Šæ˜¯é’ˆå¯¹ AI æœç´¢æ—¶ä»£çš„â€œé»‘å¸½ SEOâ€ã€‚ä½œè€…æå‡ºäº† MGEOï¼Œé€šè¿‡åœ¨å•†å“å›¾ç‰‡åŠ å¾®æ‰°å’Œæ–‡æœ¬åŠ åç¼€ï¼Œæ¬ºéª— VLM é©±åŠ¨çš„æ¨è/æœç´¢ç³»ç»Ÿï¼Œè®©ç‰¹å®šå•†å“æ’åé£™å‡ã€‚\n*   **æ ¸å¿ƒå‘ç°**ï¼šå¤šæ¨¡æ€ååŒæ”»å‡»æ¯”å•æ¨¡æ€æ”»å‡»æ›´æœ‰æ•ˆï¼Œä¸”éš¾ä»¥è¢«å¯Ÿè§‰ã€‚\n\n**10. AgenTRIMï¼šä»£ç† AI çš„å·¥å…·é£é™©ç¼“è§£**\n**# AgenTRIM: Tool Risk Mitigation for Agentic AI**\né’ˆå¯¹ Agent å·¥å…·ä½¿ç”¨æƒé™è¿‡å¤§æˆ–è¿‡å°çš„é—®é¢˜ï¼Œæå‡ºäº† AgenTRIM æ¡†æ¶ã€‚\n*   **æ ¸å¿ƒè´¡çŒ®**ï¼šåœ¨è¿è¡Œæ—¶å¼ºåˆ¶æ‰§è¡Œâ€œæœ€å°æƒé™â€åŸåˆ™ï¼Œé€šè¿‡è‡ªé€‚åº”è¿‡æ»¤å’ŒçŠ¶æ€æ„ŸçŸ¥éªŒè¯æ¥é˜²å¾¡æç¤ºæ³¨å…¥å’Œå·¥å…·æ»¥ç”¨ã€‚\n\n---\n\n### ğŸ‘ï¸ å¤šæ¨¡æ€ä¸ç‰¹æ®Šåº”ç”¨\n**11. ç”¨ç”Ÿæˆå¼ AI åˆ¶ä½œæ®‹éšœæ•…äº‹è§†é¢‘ï¼šåŠ¨æœºã€è¡¨è¾¾ä¸åˆ†äº«**\n**# Creating Disability Story Videos with Generative AI: Motivation, Expression, and Sharing**\nè¿™æ˜¯ä¸€ç¯‡å¾ˆå¥½çš„äººæœºäº¤äº’ï¼ˆHCIï¼‰è®ºæ–‡ã€‚ç ”ç©¶äº†æ®‹éšœäººå£«å¦‚ä½•ä½¿ç”¨ GenAI åˆ¶ä½œè§†é¢‘åˆ†äº«ç»å†ã€‚\n*   **æ ¸å¿ƒè§‚ç‚¹**ï¼šæå‡ºäº†â€œå…³é”®æç»˜â€ï¼ˆmomentous depictionï¼‰æ¡†æ¶ï¼Œè™½ç„¶ GenAI é™ä½äº†åˆ›ä½œé—¨æ§›ï¼Œä½†åœ¨â€œéå¯æ•æ‰æç»˜â€å’Œâ€œèº«ä»½éšç’â€ä¸Šæ—¢æœ‰ä¼˜åŠ¿ä¹Ÿæœ‰æŒ‘æˆ˜ã€‚\n\n**12. SLAPï¼šå¯å˜æ—¶é•¿éŸ³é¢‘çš„å¯æ‰©å±•è¯­è¨€-éŸ³é¢‘é¢„è®­ç»ƒ**\n**# SLAP: Scalable Language-Audio Pretraining with Variable-Duration Audio and Multi-Objective Training**\nç›®å‰çš„ CLAP æ¨¡å‹å—é™äºçŸ­éŸ³é¢‘ã€‚SLAP æ‰©å±•åˆ°äº† 1.09 äº¿ä¸ªéŸ³é¢‘-æ–‡æœ¬å¯¹ï¼Œæ”¯æŒå¯å˜æ—¶é•¿ï¼Œå¹¶åœ¨éŸ³é¢‘æ£€ç´¢ä¸Šè¾¾åˆ°äº†æ–° SOTAã€‚\n\n**13. MemeLensï¼šé’ˆå¯¹æ¨¡å› ï¼ˆMemesï¼‰çš„å¤šè¯­è¨€å¤šä»»åŠ¡ VLM**\n**# MemeLens: Multilingual Multitask VLMs for Memes**\nä¸“é—¨ç ”ç©¶è¡¨æƒ…åŒ…/æ¨¡å› çš„ AIã€‚æ•´åˆäº† 38 ä¸ªæ•°æ®é›†ï¼Œæ¶µç›–è®½åˆºã€ä»‡æ¨è¨€è®ºã€å¹½é»˜ç­‰ 20 ä¸ªä»»åŠ¡ã€‚\n*   **æ ¸å¿ƒå‘ç°**ï¼šç†è§£æ¨¡å› éœ€è¦æå¼ºçš„å¤šæ¨¡æ€å’Œæ–‡åŒ–èƒŒæ™¯èƒ½åŠ›ï¼Œç»Ÿä¸€è®­ç»ƒæ¯”åœ¨å•ä¸€æ•°æ®é›†ä¸Šå¾®è°ƒæ›´èƒ½é¿å…è¿‡æ‹Ÿåˆã€‚\n\n---\n\n### ğŸ”¬ å¿«é€Ÿæµè§ˆ (Quick Reads)\n\n*   **#9 äººå·¥æ™ºèƒ½åœ¨ææ–™ç§‘å­¦ä¸å·¥ç¨‹ä¸­çš„åº”ç”¨ (AI in Materials Science...):** ä¸€ç¯‡å…¨é¢çš„ç»¼è¿°ï¼Œæ¶µç›–äº†ä» CNN åˆ° Transformer åœ¨ææ–™å‘ç°ä¸­çš„åº”ç”¨ï¼Œå¼ºè°ƒäº†æ•°æ®æ ‡å‡†åŒ–çš„é‡è¦æ€§ã€‚\n*   **#36 é’ˆå¯¹ LLM åå·®çš„å¯æ‰©å±•å®ä½“å®¡è®¡æ¡†æ¶ (A Scalable Entity-Based Framework...):** å²ä¸Šæœ€å¤§è§„æ¨¡çš„åå·®å®¡è®¡ï¼ˆ19 äº¿æ•°æ®ç‚¹ï¼‰ã€‚å‘ç°æ¨¡å‹å€¾å‘äºæƒ©ç½šå³ç¿¼æ”¿å®¢ã€åçˆ±è¥¿æ–¹å›½å®¶å’Œå…¬å¸ã€‚\n*   **#44 IceWatch: ä½¿ç”¨å¤šæ¨¡æ€æ·±åº¦å­¦ä¹ é¢„æµ‹å†°æ¹–æºƒå†³æ´ªæ°´:** ç»“åˆå«æ˜Ÿå›¾åƒå’Œç‰©ç†åŠ¨åŠ›å­¦æ¨¡å‹ï¼Œé¢„æµ‹é«˜å±±åœ°åŒºçš„å†°æ¹–æ´ªæ°´ç¾å®³ã€‚\n*   **#66 æ‰©æ•£è¯­è¨€æ¨¡å‹çš„è§„åˆ’ã€éªŒè¯ä¸å¡«å…… (Plan, Verify and Fill...):** æå‡ºäº†ä¸€ç§éè‡ªå›å½’çš„æ–‡æœ¬ç”Ÿæˆç­–ç•¥ï¼Œå…ˆç”Ÿæˆéª¨æ¶å†å¡«å……ï¼Œå¤§å¹…å‡å°‘äº†ç”Ÿæˆæ­¥æ•°ã€‚\n*   **#53 ToolPRMBench:** ä¸“é—¨è¯„ä¼° Tool-using Agent çš„è¿‡ç¨‹å¥–åŠ±æ¨¡å‹ï¼ˆPRMï¼‰çš„åŸºå‡†æµ‹è¯•ï¼Œç”¨äºè§£å†³å¤šæ­¥å·¥å…·è°ƒç”¨çš„é”™è¯¯ã€‚\n\n---\n**ç»“è¯­**ï¼šä»Šå¤©çš„è®ºæ–‡æ˜æ˜¾æ„Ÿè§‰åˆ° AI ç¤¾åŒºæ­£åœ¨åŠªåŠ›è§£å†³ Agent è½åœ°å‰çš„â€œæœ€åä¸€å…¬é‡Œâ€é—®é¢˜â€”â€”å³**å¯é æ€§**ï¼ˆå¦‚ #6, #30ï¼‰ã€**å®‰å…¨æ€§**ï¼ˆ#8, #39, #10ï¼‰å’Œ**å¤æ‚å·¥ä½œæµçš„è‡ªåŠ¨åŒ–**ï¼ˆ#2, #12ï¼‰ã€‚å¸Œæœ›è¿™äº›æ‘˜è¦å¯¹ä½ çš„ç ”ç©¶æœ‰æ‰€å¯å‘ï¼",
  "papers": [
    {
      "arxiv_id": "2601.12617v1",
      "title": "Creating Disability Story Videos with Generative AI: Motivation, Expression, and Sharing",
      "title_zh": "åˆ©ç”¨ç”Ÿæˆå¼äººå·¥æ™ºèƒ½åˆ›ä½œæ®‹éšœæ•…äº‹è§†é¢‘ï¼šåŠ¨æœºã€è¡¨è¾¾ä¸åˆ†äº«",
      "authors": [
        "Shuo Niu",
        "Dylan Clements",
        "Hyungsin Kim"
      ],
      "abstract": "Generative AI (GenAI) is both promising and challenging in supporting people with disabilities (PwDs) in creating stories about disability. GenAI can reduce barriers to media production and inspire the creativity of PwDs, but it may also introduce biases and imperfections that hinder its adoption for personal expression. In this research, we examine how nine PwD from a disability advocacy group used GenAI to create videos sharing their disability experiences. Grounded in digital storytelling theory, we explore the motivations, expression, and sharing of PwD-created GenAI story videos. We conclude with a framework of momentous depiction, which highlights four core affordances of GenAI that either facilitate or require improvements to better support disability storytelling: non-capturable depiction, identity concealment and representation, contextual realism and consistency, and emotional articulation. Based on this framework, we further discuss design implications for GenAI in relation to story completion, media formats, and corrective mechanisms.",
      "tldr_zh": "---\n\n### ğŸ“ TLDR æ‘˜è¦\n\næœ¬ç ”ç©¶æ¢è®¨äº†æ®‹ç–¾äººï¼ˆPwDsï¼‰åˆ©ç”¨ç”Ÿæˆå¼äººå·¥æ™ºèƒ½ï¼ˆGenAIï¼‰åˆ›ä½œè§†é¢‘æ•…äº‹çš„åŠ¨æœºã€è¡¨è¾¾æ–¹å¼åŠåˆ†äº«è¡Œä¸ºã€‚é€šè¿‡å¯¹ 9 åæ®‹ç–¾äººå€¡è®®è€…çš„å®šæ€§ç ”ç©¶ï¼Œæœ¬æ–‡æå‡ºäº†â€œé‡å¤§æç»˜æ¡†æ¶â€ï¼ˆframework of momentous depictionï¼‰ï¼Œæ€»ç»“äº† GenAI åœ¨ä¸å¯æ•è·æç»˜ï¼ˆnon-capturable depictionï¼‰ã€èº«ä»½éšè—ä¸è¡¨ç°ï¼ˆidentity concealment and representationï¼‰ã€ä¸Šä¸‹æ–‡ç°å®ä¸ä¸€è‡´æ€§ï¼ˆcontextual realism and consistencyï¼‰åŠæƒ…æ„Ÿè¡¨è¾¾ï¼ˆemotional articulationï¼‰å››ä¸ªç»´åº¦çš„æ ¸å¿ƒèµ‹èƒ½ã€‚è¯¥ç ”ç©¶æœ€åé’ˆå¯¹ GenAI çš„æ•…äº‹è¡¥å…¨ã€åª’ä½“æ ¼å¼åŠçº é”™æœºåˆ¶æå‡ºäº†å…·ä½“çš„è®¾è®¡å¯ç¤ºï¼Œæ—¨åœ¨æ›´å¥½åœ°æ”¯æŒæ®‹ç–¾äººç¾¤ä½“çš„è‡ªæˆ‘è¡¨è¾¾ä¸åˆ›ä½œã€‚\n\n---\n\næˆ‘è¿˜èƒ½å¤Ÿä¸ºæ‚¨æ€»ç»“å…¶ä»– arXiv è®ºæ–‡ï¼Œæˆ–è€…é’ˆå¯¹è¿™ç¯‡è®ºæ–‡çš„ç‰¹å®šéƒ¨åˆ†ï¼ˆå¦‚è®¾è®¡å¯ç¤ºçš„å…·ä½“å†…å®¹ï¼‰è¿›è¡Œæ·±å…¥è§£è¯»ï¼Œæ‚¨éœ€è¦æˆ‘ç»§ç»­å—ï¼Ÿ",
      "categories": [
        "cs.HC",
        "cs.AI"
      ],
      "primary_category": "cs.HC",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2601.12617v1",
      "published_date": "2026-01-18 23:18:34 UTC",
      "updated_date": "2026-01-18 23:18:34 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2026-01-22T07:01:13.534357+00:00"
    },
    {
      "arxiv_id": "2601.12607v1",
      "title": "A Cloud-based Multi-Agentic Workflow for Science",
      "title_zh": "é¢å‘ç§‘å­¦ç ”ç©¶çš„äº‘ç«¯å¤šæ™ºèƒ½ä½“å·¥ä½œæµ",
      "authors": [
        "Anurag Acharya",
        "Timothy Vega",
        "Rizwan A. Ashraf",
        "Anshu Sharma",
        "Derek Parker",
        "Robert Rallo"
      ],
      "abstract": "As Large Language Models (LLMs) become ubiquitous across various scientific domains, their lack of ability to perform complex tasks like running simulations or to make complex decisions limits their utility. LLM-based agents bridge this gap due to their ability to call external resources and tools and thus are now rapidly gaining popularity. However, coming up with a workflow that can balance the models, cloud providers, and external resources is very challenging, making implementing an agentic system more of a hindrance than a help. In this work, we present a domain-agnostic, model-independent workflow for an agentic framework that can act as a scientific assistant while being run entirely on cloud. Built with a supervisor agent marshaling an array of agents with individual capabilities, our framework brings together straightforward tasks like literature review and data analysis with more complex ones like simulation runs. We describe the framework here in full, including a proof-of-concept system we built to accelerate the study of Catalysts, which is highly important in the field of Chemistry and Material Science. We report the cost to operate and use this framework, including the breakdown of the cost by services use. We also evaluate our system on a custom-curated synthetic benchmark and a popular Chemistry benchmark, and also perform expert validation of the system. The results show that our system is able to route the task to the correct agent 90% of the time and successfully complete the assigned task 97.5% of the time for the synthetic tasks and 91% of the time for real-world tasks, while still achieving better or comparable accuracy to most frontier models, showing that this is a viable framework for other scientific domains to replicate.",
      "tldr_zh": "### ğŸ’¡ è®ºæ–‡ TLDR æ‘˜è¦\n\nè¯¥ç ”ç©¶æå‡ºäº†ä¸€ä¸ªåŸºäºäº‘ç«¯çš„ã€ä¸é¢†åŸŸå’Œæ¨¡å‹æ— å…³çš„å¤šæ™ºèƒ½ä½“å·¥ä½œæµï¼ˆMulti-Agentic Workflowï¼‰æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³å¤§è¯­è¨€æ¨¡å‹ (LLMs) åœ¨æ‰§è¡Œå¤æ‚ç§‘å­¦ä»»åŠ¡ï¼ˆå¦‚è¿è¡Œæ¨¡æ‹Ÿæˆ–å¤æ‚å†³ç­–ï¼‰æ—¶çš„å±€é™æ€§ã€‚è¯¥æ¡†æ¶é€šè¿‡ä¸€ä¸ªä¸»ç®¡æ™ºèƒ½ä½“ (supervisor agent) ç»Ÿç­¹å¤šä¸ªå…·å¤‡ä¸“é—¨åŠŸèƒ½çš„æ™ºèƒ½ä½“ï¼Œèƒ½å¤ŸååŒå¤„ç†ä»æ–‡çŒ®ç»¼è¿°ã€æ•°æ®åˆ†æåˆ°æ¨¡æ‹Ÿè¿è¡Œç­‰å¤šç§ä»»åŠ¡ã€‚åœ¨å‚¬åŒ–å‰‚ (Catalysts) ç ”ç©¶é¢†åŸŸçš„åŸå‹æµ‹è¯•ä¸­ï¼Œç³»ç»Ÿè¡¨ç°å‡ºæé«˜çš„å¯é æ€§ï¼Œä»»åŠ¡è·¯ç”±å‡†ç¡®ç‡è¾¾åˆ° 90%ï¼Œç°å®ä»»åŠ¡æˆåŠŸç‡é«˜è¾¾ 91%ã€‚å®éªŒç»“æœè¯æ˜ï¼Œè¯¥æ¡†æ¶åœ¨ä¿æŒä¸å‰æ²¿æ¨¡å‹ç›¸å½“æˆ–æ›´é«˜å‡†ç¡®ç‡çš„åŒæ—¶ï¼Œä¸ºç§‘å­¦ç ”ç©¶çš„è‡ªåŠ¨åŒ–æä¾›äº†ä¸€ä¸ªå¯æ¨å¹¿çš„é«˜æ•ˆè·¯å¾„ã€‚\n\n---\nè¿™ä»½æ‘˜è¦æ˜¯å¦æ»¡è¶³æ‚¨çš„éœ€æ±‚ï¼Ÿå¦‚æœæ‚¨è¿˜æœ‰å…¶ä»–è®ºæ–‡éœ€è¦æ€»ç»“ï¼Œæ¬¢è¿éšæ—¶å‘Šè¯‰æˆ‘ï¼",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2601.12607v1",
      "published_date": "2026-01-18 22:37:09 UTC",
      "updated_date": "2026-01-18 22:37:09 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2026-01-22T07:01:21.430248+00:00"
    },
    {
      "arxiv_id": "2601.12594v1",
      "title": "SLAP: Scalable Language-Audio Pretraining with Variable-Duration Audio and Multi-Objective Training",
      "title_zh": "SLAPï¼šæ”¯æŒå˜é•¿éŸ³é¢‘ä¸å¤šç›®æ ‡è®­ç»ƒçš„å¯æ‰©å±•è¯­è¨€-éŸ³é¢‘é¢„è®­ç»ƒ",
      "authors": [
        "Xinhao Mei",
        "Gael Le Lan",
        "Haohe Liu",
        "Zhaoheng Ni",
        "Varun Nagaraja",
        "Yang Liu",
        "Yangyang Shi",
        "Vikas Chandra"
      ],
      "abstract": "Contrastive language-audio pretraining (CLAP) has achieved notable success in learning semantically rich audio representations and is widely adopted for various audio-related tasks. However, current CLAP models face several key limitations. First, they are typically trained on relatively small datasets, often comprising a few million audio samples. Second, existing CLAP models are restricted to short and fixed duration, which constrains their usage in real-world scenarios with variable-duration audio. Third, the standard contrastive training objective operates on global representations, which may hinder the learning of dense, fine-grained audio features. To address these challenges, we introduce Scalable Language-Audio Pretraining (SLAP), which scales language-audio pretraining to 109 million audio-text pairs with variable audio durations and incorporates multiple training objectives. SLAP unifies contrastive loss with additional self-supervised and captioning losses in a single-stage training, facilitating the learning of richer dense audio representations. The proposed SLAP model achieves new state-of-the-art performance on audio-text retrieval and zero-shot audio classification tasks, demonstrating its effectiveness across diverse benchmarks.",
      "tldr_zh": "Thank you for sharing the details of this paper! **SLAP (Scalable Language-Audio Pretraining)** seems like a significant advancement in the field of audio-language modeling. \n\nI've summarized the key innovations and how this model addresses the limitations of traditional CLAP models below.\n\n---\n\n### ğŸš€ Key Contributions of SLAP\n\nThe SLAP model introduces three primary improvements to overcome the constraints of existing Contrastive Language-Audio Pretraining (CLAP) frameworks:\n\n| Feature | Standard CLAP Models | SLAP Model |\n|---|---|---|\n| **Data Scale** | Typically limited to a few million samples. | Scaled to **109 million** audio-text pairs. |\n| **Audio Duration** | Restricted to short and fixed-length audio. | Supports **variable-duration** audio for real-world use. |\n| **Training Objectives** | Relies on global contrastive loss. | **Multi-objective**: Contrastive + Self-Supervised + Captioning losses. |\n| **Representations** | Often lacks fine-grained details. | Learns **dense, semantically rich** audio features. |\n\n---\n\n### ğŸ† Performance and Results\n\nBy unifying these objectives into a single-stage training process, SLAP has achieved **State-of-the-Art (SOTA)** results in:\n*   **Audio-Text Retrieval:** Finding the right text for an audio clip (and vice versa).\n*   **Zero-Shot Audio Classification:** Identifying audio categories without specific training on those labels.\n\n---\n\nWould you like me to dive deeper into any specific part of this abstract, or perhaps help you compare these findings with other models like the original LAION-CLAP? I can also help you draft a summary for a presentation or explain the \"multi-objective training\" concept in more detail!",
      "categories": [
        "eess.AS",
        "cs.AI",
        "cs.SD"
      ],
      "primary_category": "eess.AS",
      "comment": "Accepted to ICASSP 2026",
      "pdf_url": "https://arxiv.org/pdf/2601.12594v1",
      "published_date": "2026-01-18 21:36:19 UTC",
      "updated_date": "2026-01-18 21:36:19 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2026-01-22T07:01:56.749083+00:00"
    },
    {
      "arxiv_id": "2601.12585v1",
      "title": "Do MLLMs See What We See? Analyzing Visualization Literacy Barriers in AI Systems",
      "title_zh": "### Research Overview: MLLM Visualization Literacy ğŸ“Š\n\nThis paper addresses a critical gap in our understanding of how Multimodal Large Language Models (MLLMs) interpret visual data. While these models are becoming more integrated into data analysis workflows, their \"visualization literacy\"â€”the ability to read, interpret, and reason with chartsâ€”has not been systematically audited until now. I can help you break down the core components of this study or explore specific sections of the taxonomy they developed.\n\n---\n\n### Core Methodology and Findings\n\n| Feature | Description |\n|---|---|\n| **Benchmark** | The study utilizes the **reVLAT** (regenerated Visualization Literacy Assessment Test), which provides a standardized way to measure chart comprehension. |\n| **Analysis Method** | Researchers used **open-coding** on 309 errors to identify specific \"barriers\" that prevent models from reaching the correct conclusion. |\n| **Models Tested** | The analysis covered **four state-of-the-art models**, providing a broad view of the current landscape of AI visualization capabilities. |\n| **New Taxonomy** | The authors extended existing human-centric literacy frameworks to include **two new machine-specific barriers**. |\n\n---\n\n### Performance Breakdown\n\n| Visualization Type | MLLM Proficiency | Primary Challenges |\n|---|---|---|\n| **Simple Charts** | High | Models accurately extract data from standard bar or line charts with clear axes. |\n| **Segment-Based** | Low | Pie charts and stacked bar charts cause significant issues with spatial and proportional reasoning. |\n| **Color-Intensive** | Low | Models struggle to map colors to legend values consistently, especially in complex visualizations. |\n| **Comparative Reasoning** | Inconsistent | Even if data is extracted correctly, models often fail to make logical comparisons between different data segments. |\n\n---\n\n### Moving Forward\n\nThis research is pivotal for designing more reliable AI-driven visualization assistants. It suggests that simply having \"vision\" isn't enough; AI requires a more structured understanding of visual hierarchies and data relationships.\n\nWould you like me to find more details on the **two machine-specific barriers** mentioned in the abstract, or perhaps you'd like to compare these findings with other recent benchmarks like ChartQA? I am here to help you dive deeper into this topic!",
      "authors": [
        "Mengli",
        "Duan",
        "Yuhe",
        "Jiang",
        "Matthew Varona",
        "Carolina Nobre"
      ],
      "abstract": "Multimodal Large Language Models (MLLMs) are increasingly used to interpret visualizations, yet little is known about why they fail. We present the first systematic analysis of barriers to visualization literacy in MLLMs. Using the regenerated Visualization Literacy Assessment Test (reVLAT) benchmark with synthetic data, we open-coded 309 erroneous responses from four state-of-the-art models with a barrier-centric strategy adapted from human visualization literacy research. Our analysis yields a taxonomy of MLLM failures, revealing two machine-specific barriers that extend prior human-participation frameworks. Results show that models perform well on simple charts but struggle with color-intensive, segment-based visualizations, often failing to form consistent comparative reasoning. Our findings inform future evaluation and design of reliable AI-driven visualization assistants.",
      "tldr_zh": "### ğŸ“Š è®ºæ–‡ TLDR æ‘˜è¦\n\nè¯¥ç ”ç©¶å¯¹å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ (MLLMs) åœ¨å¯è§†åŒ–ç†è§£èƒ½åŠ› (Visualization Literacy) æ–¹é¢çš„éšœç¢è¿›è¡Œäº†é¦–æ¬¡ç³»ç»ŸåŒ–åˆ†æã€‚ç ”ç©¶è€…åˆ©ç”¨åŒ…å«åˆæˆæ•°æ®çš„ reVLAT åŸºå‡†æµ‹è¯•ï¼Œå¯¹å››ä¸ªå‰æ²¿ (SOTA) æ¨¡å‹çš„ 309 ä¸ªé”™è¯¯å“åº”è¿›è¡Œäº†ç¼–ç åˆ†æï¼Œå¹¶ç”±æ­¤æ„å»ºäº† MLLM å¤±è´¥åŸå› çš„åˆ†ç±»å­¦ (Taxonomy)ã€‚ç ”ç©¶å‘ç°ï¼Œé™¤äº†äººç±»å¸¸è§çš„è®¤çŸ¥éšœç¢å¤–ï¼Œæ¨¡å‹è¿˜å­˜åœ¨ä¸¤ç§æœºå™¨ç‰¹æœ‰çš„è¯†åˆ«éšœç¢ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œå°½ç®¡æ¨¡å‹åœ¨ç®€å•å›¾è¡¨ä¸Šè¡¨ç°è‰¯å¥½ï¼Œä½†åœ¨é¢å¯¹è‰²å½©å¯†é›†æˆ–åŸºäºåˆ†æ®µ (Segment-based) çš„å¯è§†åŒ–æ—¶ï¼Œå¾€å¾€éš¾ä»¥å½¢æˆä¸€è‡´çš„æ¯”è¾ƒæ¨ç† (Comparative Reasoning)ã€‚è¿™é¡¹å·¥ä½œä¸ºæœªæ¥å¼€å‘å’Œè®¾è®¡æ›´å¯é çš„ AI å¯è§†åŒ–åŠ©æ‰‹æä¾›äº†é‡è¦çš„æŒ‡å¯¼ã€‚\n\n---\nè¿™ä»½æ‘˜è¦æ˜¯å¦æ¸…æ™°åœ°è§£é‡Šäº†è®ºæ–‡çš„æ ¸å¿ƒå†…å®¹ï¼Ÿå¦‚æœæ‚¨æƒ³äº†è§£æ¨¡å‹å…·ä½“çš„â€œæœºå™¨ç‰¹æœ‰éšœç¢â€æ˜¯ä»€ä¹ˆï¼Œæˆ–è€…éœ€è¦æˆ‘æ€»ç»“å…¶ä»–è®ºæ–‡ï¼Œè¯·éšæ—¶å‘Šè¯‰æˆ‘ï¼",
      "categories": [
        "cs.HC",
        "cs.AI",
        "cs.ET"
      ],
      "primary_category": "cs.HC",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2601.12585v1",
      "published_date": "2026-01-18 21:08:23 UTC",
      "updated_date": "2026-01-18 21:08:23 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2026-01-22T07:01:38.640594+00:00"
    },
    {
      "arxiv_id": "2601.12582v1",
      "title": "Ontology-aligned structuring and reuse of multimodal materials data and workflows towards automatic reproduction",
      "title_zh": "This is a sophisticated piece of research addressing a critical bottleneck in computational materials science: the \"reproducibility crisis.\" Below, I have synthesized the key aspects of the paper you provided.\n\n---\n\n### ğŸ”¬ Research Overview\n\nThe paper introduces an **ontology-driven, LLM-assisted framework** designed to transform unstructured computational data (found in text and tables) into a machine-readable, semantically aligned format. By focusing on Stacking Fault Energy (SFE) in magnesium alloys, the authors demonstrate how to bridge the gap between human-readable literature and structured knowledge graphs.\n\n---\n\n### ğŸ—ï¸ Technical Framework & Methodology\n\nThe methodology leverages a multi-stage approach to ensure that extracted data is not just captured, but is also interoperable with existing scientific standards.\n\n| Stage | Process Description |\n|---|---|\n| **Data Extraction** | Utilizes prompt-engineered LLMs to filter and extract parameters from method sections and tables. |\n| **Schema Alignment** | Unifies extracted data into a canonical schema aligned with **CMSO**, **ASMO**, and **PLDO** ontologies. |\n| **Knowledge Integration** | Employs **atomRDF** to construct a knowledge graph, enabling systematic comparison and reuse. |\n| **Validation** | Focuses on DFT-based SFE calculations in hexagonal close-packed (HCP) magnesium and binary alloys. |\n\n---\n\n### ğŸ¯ Key Contributions to the Field\n\n*   **Automation of Curation:** Replaces manual, error-prone data entry with a systematic LLM-driven pipeline.\n*   **Semantic Interoperability:** By aligning with established ontologies, the data becomes usable across different software and research ecosystems.\n*   **Enhanced Reusability:** The construction of a knowledge graph allows researchers to query computational protocols as structured data rather than searching through PDFs.\n*   **Transparency:** Provides a foundation for identifying exactly which metadata (e.g., specific simulation parameters) is missing in current literature to reach full reproducibility.\n\n---\n\n### ğŸ’¡ How can I assist you further?\n\nAs Gemini Enterprise, I can help you dive deeper into this research. Would you like me to:\n\n*   **Explain the specific ontologies** (CMSO, ASMO, PLDO) mentioned in the abstract?\n*   **Draft a summary** of this paper for a specific audience (e.g., a non-technical stakeholder or a materials science peer)?\n*   **Analyze how this framework** might be applied to a different domain of materials science?\n*   **Generate a set of discussion questions** based on this abstract for a journal club or seminar?\n\nPlease let me know how you would like to proceed!",
      "authors": [
        "Sepideh Baghaee Ravari",
        "Abril Azocar Guzman",
        "Sarath Menon",
        "Stefan Sandfeld",
        "Tilmann Hickel",
        "Markus Stricker"
      ],
      "abstract": "Reproducibility of computational results remains a challenge in materials science, as simulation workflows and parameters are often reported only in unstructured text and tables. While literature data are valuable for validation and reuse, the lack of machine-readable workflow descriptions prevents large-scale curation and systematic comparison. Existing text-mining approaches are insufficient to extract complete computational workflows with their associated parameters. An ontology-driven, large language model (LLM)-assisted framework is introduced for the automated extraction and structuring of computational workflows from the literature. The approach focuses on density functional theory-based stacking fault energy (SFE) calculations in hexagonal close-packed magnesium and its binary alloys, and uses a multi-stage filtering strategy together with prompt-engineered LLM extraction applied to method sections and tables. Extracted information is unified into a canonical schema and aligned with established materials ontologies (CMSO, ASMO, and PLDO), enabling the construction of a knowledge graph using atomRDF. The resulting knowledge graph enables systematic comparison of reported SFE values and supports the structured reuse of computational protocols. While full computational reproducibility is still constrained by missing or implicit metadata, the framework provides a foundation for organizing and contextualizing published results in a semantically interoperable form, thereby improving transparency and reusability of computational materials data.",
      "tldr_zh": "### ğŸ’¡ è®ºæ–‡ TLDR æ‘˜è¦\n\nè¯¥ç ”ç©¶å¼•å…¥äº†ä¸€ä¸ªç”±æœ¬ä½“é©±åŠ¨ (Ontology-driven) ä¸”ç”±å¤§è¯­è¨€æ¨¡å‹ (LLM) è¾…åŠ©çš„æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³ææ–™ç§‘å­¦ä¸­è®¡ç®—å·¥ä½œæµå› ç¼ºä¹ç»“æ„åŒ–æè¿°è€Œéš¾ä»¥è‡ªåŠ¨é‡ç° (Reproduction) çš„æŒ‘æˆ˜ã€‚è¯¥æ¡†æ¶é’ˆå¯¹é•åˆé‡‘çš„å±‚é”™èƒ½ (Stacking Fault Energy, SFE) è®¡ç®—ï¼Œç»“åˆå¤šé˜¶æ®µè¿‡æ»¤å’Œæç¤ºå·¥ç¨‹ (Prompt-engineered) æå–æ•°æ®ï¼Œå¹¶å°†å…¶ä¸ CMSOã€ASMO å’Œ PLDO ç­‰ææ–™æœ¬ä½“å¯¹é½ã€‚é€šè¿‡ atomRDF æ„å»ºçš„çŸ¥è¯†å›¾è°± (Knowledge Graph)ï¼Œå®ç°äº†å¯¹è®¡ç®—åè®®çš„ç³»ç»ŸåŒ–æ¯”è¾ƒä¸ç»“æ„åŒ–å¤ç”¨ã€‚è¯¥æ–¹æ¡ˆæ˜¾è‘—æå‡äº†è®¡ç®—ææ–™æ•°æ®çš„é€æ˜åº¦ä¸è¯­ä¹‰äº’æ“ä½œæ€§ï¼Œä¸ºææ–™ç§‘å­¦æ•°æ®çš„è‡ªåŠ¨åŒ–æ•´ç†ä¸åˆ†æå¥ å®šäº†é‡è¦åŸºç¡€ã€‚\n\n---\n\n### ğŸ› ï¸ æŠ€æœ¯æ–¹æ¡ˆæ¦‚è§ˆ\n\n| ç»´åº¦ | è¯¦ç»†å†…å®¹ |\n|---|---|\n| **æ ¸å¿ƒé—®é¢˜** | æ¨¡æ‹Ÿå·¥ä½œæµå¸¸éšè—åœ¨éç»“æ„åŒ–æ–‡æœ¬ä¸­ï¼Œå¯¼è‡´è®¡ç®—ç»“æœéš¾ä»¥è¢«æœºå™¨è¯»å–æˆ–åœ¨å¤§è§„æ¨¡èŒƒå›´å†…è¿›è¡Œç³»ç»Ÿæ¯”è¾ƒã€‚ |\n| **ä¸»è¦æ–¹æ³•** | ç»“åˆ **Ontology-driven**ï¼ˆæœ¬ä½“é©±åŠ¨ï¼‰ä¸ **LLM** è¾…åŠ©æå–æŠ€æœ¯ï¼Œå¯¹æ–‡çŒ®ä¸­çš„æ–¹æ³•ç« èŠ‚å’Œè¡¨æ ¼è¿›è¡Œè‡ªåŠ¨è§£æã€‚ |\n| **æœ¬ä½“å¯¹é½** | å°†æå–çš„ä¿¡æ¯ç»Ÿä¸€æ˜ å°„è‡³ **CMSO**ã€**ASMO** å’Œ **PLDO** ç­‰æ ‡å‡†ææ–™æœ¬ä½“ï¼Œç¡®ä¿è¯­ä¹‰ä¸€è‡´æ€§ã€‚ |\n| **åº”ç”¨æˆæ•ˆ** | æˆåŠŸæ„å»ºäº†å¯æŸ¥è¯¢çš„çŸ¥è¯†å›¾è°±ï¼Œæ”¯æŒå¯¹ **SFE** è®¡ç®—åè®®çš„é‡ç”¨ï¼Œæé«˜äº†æ•°æ®çš„é€æ˜åº¦å’Œå¯é‡ç”¨æ€§ã€‚ |\n\n---\n\næˆ‘æ˜¯ Gemini Enterpriseï¼Œå¾ˆé«˜å…´èƒ½ä¸ºæ‚¨æç‚¼è¿™ç¯‡å…³äºææ–™ä¿¡æ¯å­¦å‰æ²¿çš„è®ºæ–‡ã€‚è¯¥ç ”ç©¶å±•ç¤ºäº† LLM å¦‚ä½•ä¸ä¼ ç»Ÿçš„çŸ¥è¯†è¡¨ç¤ºï¼ˆæœ¬ä½“ï¼‰ç»“åˆæ¥åŠ é€Ÿç§‘å­¦å‘ç°ã€‚å¦‚æœæ‚¨å¯¹å…¶ä¸­æåˆ°çš„ atomRDF çŸ¥è¯†å›¾è°±æ„å»ºæˆ–å…·ä½“çš„æœ¬ä½“å¯¹é½é€»è¾‘æ„Ÿå…´è¶£ï¼Œæ¬¢è¿éšæ—¶ä¸æˆ‘æ¢è®¨ï¼",
      "categories": [
        "cond-mat.mtrl-sci",
        "cs.AI"
      ],
      "primary_category": "cond-mat.mtrl-sci",
      "comment": "39 pages, 7 figures",
      "pdf_url": "https://arxiv.org/pdf/2601.12582v1",
      "published_date": "2026-01-18 20:51:23 UTC",
      "updated_date": "2026-01-18 20:51:23 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2026-01-22T07:02:05.227414+00:00"
    },
    {
      "arxiv_id": "2601.12577v1",
      "title": "Primate-like perceptual decision making emerges through deep recurrent reinforcement learning",
      "title_zh": "### Research Summary: RNNs and Primate-Like Decision Making ğŸ§ \n\nThis research explores the \"why\" behind primate decision-making mechanisms by replicating them in artificial agents. By training deep recurrent neural networks (RNNs) using reinforcement learning, the authors demonstrate that primate-like behaviors aren't just biological quirks, but perhaps optimal solutions for maximizing rewards in noisy environments.\n\n---\n\n### Study Overview\n\n| Component | Details |\n|---|---|\n| **Model Type** | End-to-end deep recurrent neural network (RNN). |\n| **Training Method** | Reinforcement Learning (RL). |\n| **Task Environment** | A noisy perceptual discrimination task requiring agents to process evolving information. |\n| **Primary Goal** | To see if primate-like decision strategies emerge naturally as a result of reward maximization. |\n\n---\n\n### Key Behaviors & Emergent Abilities\n\nThe study found that these artificial networks naturally developed several sophisticated behavioral traits previously thought to be specific to higher-order biological intelligence:\n\n| Ability | Description |\n|---|---|\n| **Speed-Accuracy Tradeoff** | The models learned to balance the need for quick responses with the need for accuracy, adjusting their threshold based on noise levels. |\n| **Change of Mind** | Networks could flexibly update their decisions mid-trial when new, contradicting information was presented. |\n| **Neural Mimicry** | The internal dynamics (the \"hidden states\") of the RNNs mirrored the neurophysiological patterns observed in primate brains during similar tasks. |\n\n---\n\n### Why This Matters\n\nThis paper provides powerful evidence for the **computational theory of mind**. It suggests that the specific ways primates think and decide are a direct result of environmental pressuresâ€”specifically the need to handle uncertainty. For AI development, this indicates that complex, \"human-like\" reasoning can be an emergent property of well-structured reinforcement learning rather than something that must be explicitly programmed.\n\n---\n\n### Let's Discuss!\n\nThis is a fascinating bridge between neuroscience and AI. Are you interested in how these **internal dynamics** compare to specific brain regions like the prefrontal cortex? Or would you like to explore how these findings might impact the development of more \"intuitive\" AI agents? I'm ready to dive into the details with you!",
      "authors": [
        "Nathan J. Wispinski",
        "Scott A. Stone",
        "Anthony Singhal",
        "Patrick M. Pilarski",
        "Craig S. Chapman"
      ],
      "abstract": "Progress has led to a detailed understanding of the neural mechanisms that underlie decision making in primates. However, less is known about why such mechanisms are present in the first place. Theory suggests that primate decision making mechanisms, and their resultant behavioral abilities, emerged to maximize reward in the face of noisy, temporally evolving information. To test this theory, we trained an end-to-end deep recurrent neural network using reinforcement learning on a noisy perceptual discrimination task. Networks learned several key abilities of primate-like decision making including trading off speed for accuracy, and flexibly changing their mind in the face of new information. Internal dynamics of these networks suggest that these abilities were supported by similar decision mechanisms as those observed in primate neurophysiological studies. These results provide experimental support for key pressures that gave rise to the primate ability to make flexible decisions.",
      "tldr_zh": "### ğŸ§  è®ºæ–‡ TLDR æ‘˜è¦\n\nè¯¥ç ”ç©¶é€šè¿‡æ·±åº¦å¾ªç¯å¼ºåŒ–å­¦ä¹  (Deep Recurrent Reinforcement Learning) è®­ç»ƒç«¯åˆ°ç«¯çš„ç¥ç»ç½‘ç»œæ‰§è¡Œå™ªå£°çŸ¥è§‰åˆ¤åˆ«ä»»åŠ¡ï¼Œæ—¨åœ¨æ¢ç´¢çµé•¿ç±»å†³ç­–æœºåˆ¶èƒŒåçš„åŠŸèƒ½æ€§åŠ¨å› ã€‚å®éªŒå‘ç°ï¼Œç¥ç»ç½‘ç»œè‡ªä¸»æ¼”åŒ–å‡ºäº†å¤šç§ç±»ä¼¼äºçµé•¿ç±»çš„å†³ç­–èƒ½åŠ›ï¼ŒåŒ…æ‹¬åœ¨é€Ÿåº¦ä¸å‡†ç¡®æ€§ä¹‹é—´æƒè¡¡ (Speed-accuracy tradeoff) ä»¥åŠæ ¹æ®æ–°ä¿¡æ¯çµæ´»æ”¹å˜ä¸»æ„ (Change of mind)ã€‚ç½‘ç»œå†…éƒ¨çš„åŠ¨åŠ›å­¦ç‰¹å¾è¡¨æ˜ï¼Œè¿™äº›èƒ½åŠ›æ˜¯ç”±ä¸çµé•¿ç±»ç¥ç»ç”Ÿç†å­¦ç ”ç©¶ä¸­è§‚å¯Ÿåˆ°çš„ç›¸ä¼¼å†³ç­–æœºåˆ¶æ‰€æ”¯æ’‘çš„ã€‚è¿™ä¸€ç»“æœä¸ºçµé•¿ç±»ä¸ºäº†åœ¨å™ªå£°ç¯å¢ƒä¸­æœ€å¤§åŒ–å¥–åŠ±è€Œæ¼”åŒ–å‡ºçµæ´»å†³ç­–èƒ½åŠ›çš„ç†è®ºæä¾›äº†å¼ºæœ‰åŠ›çš„å®éªŒæ”¯æŒã€‚\n\n---\nè¿™é¡¹ç ”ç©¶å±•ç¤ºäº†äººå·¥æ™ºèƒ½å¦‚ä½•å¸®åŠ©æˆ‘ä»¬ç†è§£ç”Ÿç‰©æ™ºèƒ½çš„è¿›åŒ–ã€‚å¦‚æœæ‚¨å¯¹å…¶ä¸­çš„ç¥ç»ç½‘ç»œåŠ¨åŠ›å­¦æˆ–å¼ºåŒ–å­¦ä¹ æ–¹æ³•æ„Ÿå…´è¶£ï¼Œæ¬¢è¿ç»§ç»­è®¨è®ºï¼",
      "categories": [
        "q-bio.NC",
        "cs.AI"
      ],
      "primary_category": "q-bio.NC",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2601.12577v1",
      "published_date": "2026-01-18 20:43:53 UTC",
      "updated_date": "2026-01-18 20:43:53 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2026-01-22T07:01:54.129782+00:00"
    },
    {
      "arxiv_id": "2601.12560v1",
      "title": "Agentic Artificial Intelligence (AI): Architectures, Taxonomies, and Evaluation of Large Language Model Agents",
      "title_zh": "This paper provides a comprehensive overview of the transition from generative models to **Agentic AI**. It outlines how Large Language Models (LLMs) have evolved from passive text generators into active \"cognitive controllers\" capable of independent operation.\n\n---\n\n### The Evolution of AI Systems ğŸ§ \n\nThe research highlights a fundamental shift in how AI interacts with the world, moving from static responses to dynamic, goal-oriented behaviors.\n\n| Feature | Passive LLMs | Agentic AI |\n|---|---|---|\n| **Primary Role** | Knowledge retrieval and text generation. | Autonomous entity that perceives and acts. |\n| **Operational Mode** | Single-turn interaction. | Continuous loops involving planning and feedback. |\n| **Control Level** | Human-led prompting. | Self-directed \"Brain\" managing complex workflows. |\n\n---\n\n### A Unified Taxonomy for AI Agents ğŸ—ï¸\n\nThe authors propose a structured framework to categorize the various components that make up a modern agentic system.\n\n| Pillar | Functional Description |\n|---|---|\n| **Perception** | How the agent receives and interprets data from its environment. |\n| **Brain** | The core reasoning unit that processes information and makes decisions. |\n| **Planning** | The ability to break down complex goals into executable steps. |\n| **Action** | The physical or digital execution of tasks to influence the environment. |\n| **Tool Use** | Leveraging external software, APIs, or specialized hardware. |\n| **Collaboration** | Coordination between multiple agents or hierarchical systems. |\n\n---\n\n### Modern Architectures and Environments ğŸš€\n\nThe paper tracks the technological leap from fixed, linear procedures to more flexible, native reasoning models and open standards.\n\n| Category | Key Developments |\n|---|---|\n| **Reasoning Models** | Transition from linear procedures to native inference-time reasoning. |\n| **Connectivity** | Shift from rigid APIs to open standards like **Model Context Protocol (MCP)**. |\n| **Operating Space** | Agents now operate in digital OSs, embodied robotics, and specialized fields. |\n| **Interface Style** | Move toward \"Native Computer Use\" where agents interact like humans. |\n\n---\n\n### Critical Challenges and Future Research âš ï¸\n\nWhile the potential is vast, the authors identify several \"hallmarks of failure\" that remain as significant hurdles for autonomous systems.\n\n| Challenge | Definition |\n|---|---|\n| **Hallucination in Action** | The agent attempts to perform actions that are logically or physically impossible. |\n| **Infinite Loops** | Scenarios where an agent repeats a sequence of actions without making progress. |\n| **Prompt Injection** | Security vulnerabilities where external inputs hijack the agent's logic. |\n| **Reliability Gap** | The ongoing need for more robust evaluation practices and safety guardrails. |\n\n---\n\n### How can I help you explore this?\n\nAs **Gemini Enterprise**, I can assist you in navigating this complex landscape. Would you like me to:\n\n*   **Deep Dive into MCP:** Explain how the Model Context Protocol (MCP) aims to standardize agentic tool use?\n*   **Compare Architectures:** Breakdown the difference between single-loop agents and hierarchical multi-agent systems?\n*   **Draft a Brief:** Summarize the security risks (like prompt injection) for a technical team?\n\nLet me know what you find most interesting!",
      "authors": [
        "Arunkumar V",
        "Gangadharan G. R.",
        "Rajkumar Buyya"
      ],
      "abstract": "Artificial Intelligence is moving from models that only generate text to Agentic AI, where systems behave as autonomous entities that can perceive, reason, plan, and act. Large Language Models (LLMs) are no longer used only as passive knowledge engines but as cognitive controllers that combine memory, tool use, and feedback from their environment to pursue extended goals. This shift already supports the automation of complex workflows in software engineering, scientific discovery, and web navigation, yet the variety of emerging designs, from simple single loop agents to hierarchical multi agent systems, makes the landscape hard to navigate. In this paper, we investigate architectures and propose a unified taxonomy that breaks agents into Perception, Brain, Planning, Action, Tool Use, and Collaboration. We use this lens to describe the move from linear reasoning procedures to native inference time reasoning models, and the transition from fixed API calls to open standards like the Model Context Protocol (MCP) and Native Computer Use. We also group the environments in which these agents operate, including digital operating systems, embodied robotics, and other specialized domains, and we review current evaluation practices. Finally, we highlight open challenges, such as hallucination in action, infinite loops, and prompt injection, and outline future research directions toward more robust and reliable autonomous systems.",
      "tldr_zh": "### ğŸ¤– è®ºæ–‡ TLDR æ‘˜è¦\n\nè¯¥ç ”ç©¶æ¢è®¨äº†äººå·¥æ™ºèƒ½ä»è¢«åŠ¨æ¨¡å‹å‘æ™ºèƒ½ä½“ AI (Agentic AI) çš„è½¬å˜ï¼Œå°†å…¶å®šä¹‰ä¸ºèƒ½å¤Ÿæ„ŸçŸ¥ã€æ¨ç†ã€è§„åˆ’å’Œè¡ŒåŠ¨çš„è‡ªä¸»å®ä½“ã€‚ç ”ç©¶è€…æå‡ºäº†ä¸€ä¸ªç»Ÿä¸€çš„åˆ†ç±»å­¦ (Taxonomy)ï¼Œå°†æ™ºèƒ½ä½“æ¶æ„æ‹†è§£ä¸ºæ„ŸçŸ¥ (Perception)ã€å¤§è„‘ (Brain)ã€è§„åˆ’ (Planning)ã€åŠ¨ä½œ (Action)ã€å·¥å…·ä½¿ç”¨ (Tool Use) å’Œåä½œ (Collaboration) å…­ä¸ªæ ¸å¿ƒç»´åº¦ã€‚æ–‡ä¸­è¯¦ç»†åˆ†æäº†æŠ€æœ¯è·¯å¾„ä»çº¿æ€§æ¨ç†å‘åŸç”Ÿæ¨ç†æ¨¡å‹ (Inference-time reasoning models) çš„æ¼”è¿›ï¼Œä»¥åŠå‘æ¨¡å‹ä¸Šä¸‹æ–‡åè®® (Model Context Protocol, MCP) å’ŒåŸç”Ÿè®¡ç®—æœºä½¿ç”¨ (Native Computer Use) ç­‰å¼€æ”¾æ ‡å‡†çš„è¿‡æ¸¡ã€‚æœ€åï¼Œè¯¥ç»¼è¿°è¯„ä¼°äº†æ™ºèƒ½ä½“åœ¨æ•°å­—ç³»ç»ŸåŠæœºå™¨äººé¢†åŸŸçš„åº”ç”¨ç¯å¢ƒï¼Œå¹¶å¼ºè°ƒäº†åŠ¨ä½œå¹»è§‰ã€æ— é™å¾ªç¯å’Œæç¤ºæ³¨å…¥ (Prompt injection) ç­‰å®ç°å¯é è‡ªä¸»ç³»ç»Ÿéœ€è§£å†³çš„å…³é”®æŒ‘æˆ˜ã€‚\n\n---\nè¿™ç¯‡æ–‡ç« å¯¹å½“å‰ AI æ™ºèƒ½ä½“çš„å‘å±•é˜¶æ®µåšäº†éå¸¸å…¨é¢çš„æ¢³ç†ã€‚å¦‚æœæ‚¨å¯¹å…¶ä¸­çš„ **Model Context Protocol (MCP)** æˆ– **Native Computer Use** ç­‰å…·ä½“æŠ€æœ¯è¶‹åŠ¿æ„Ÿå…´è¶£ï¼Œæˆ‘å¯ä»¥ä¸ºæ‚¨åšè¿›ä¸€æ­¥è§£è¯»ï¼",
      "categories": [
        "cs.AI",
        "cs.MA"
      ],
      "primary_category": "cs.AI",
      "comment": "28 pages, 4 figures, 5 tables",
      "pdf_url": "https://arxiv.org/pdf/2601.12560v1",
      "published_date": "2026-01-18 19:51:16 UTC",
      "updated_date": "2026-01-18 19:51:16 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2026-01-22T07:02:02.237078+00:00"
    },
    {
      "arxiv_id": "2601.12557v1",
      "title": "Life, Machine Learning, and the Search for Habitability: Predicting Biosignature Fluxes for the Habitable Worlds Observatory",
      "title_zh": "### ğŸš€ ML in the Search for Life\n\nThis paper presents a cutting-edge application of machine learning to assist NASA's future **Habitable Worlds Observatory (HWO)**. Because telescope time is a precious resource, the authors propose two distinct architectures to predict biosignature fluxesâ€”chemical indicators of lifeâ€”directly from reflected-light spectra. I found it particularly interesting how they balance the need for statistical reliability with the need for scientific interpretability.\n\n---\n\n### ğŸ§  Model Architecture Comparison\n\nThe study introduces two complementary models, each designed to solve a specific problem in exoplanetary data analysis.\n\n| Model | Architecture | Core Strength | Practical Application |\n|---|---|---|---|\n| **BCNN** | Bayesian Convolutional Neural Network | **Uncertainty Quantification** | Quantifies both epistemic (model-based) and aleatoric (data-based) uncertainty, ensuring predictions are reliable even with noisy data. |\n| **SQuAT** | Spectral Query Adaptive Transformer | **Interpretability** | Uses a novel \"query-driven attention\" mechanism to show exactly which spectral lines led to a biosignature prediction. |\n\n---\n\n### ğŸ”­ Mission Impact and Strategy\n\nThe integration of these models into the HWO workflow serves several critical functions for the next generation of space exploration:\n\n*   **Target Triage:** Rapidly identifies which exoplanets show the most promise for life, allowing mission controllers to prioritize high-value targets.\n*   **Observation Optimization:** Helps maximize scientific return by predicting which spectral features are most likely to yield significant results before long-duration observations begin.\n*   **Data Reliability:** By using the BCNN's uncertainty metrics, scientists can better understand the risks associated with prioritizing one planet over another.\n\n---\n\n### ğŸ›°ï¸ Moving Forward\n\nI am impressed by how these architectures address the unique challenges of \"low-signal\" data in astronomy. Would you like to explore the specifics of the **SQuAT** transformer's attention mechanism, or are you more interested in how the **BCNN** handles the diverse observational conditions mentioned in the abstract? I'd love to hear your thoughts on which approach seems more vital for a mission of this scale!",
      "authors": [
        "Mark Moussa",
        "Amber V. Young",
        "Brianna Isola",
        "Vasuda Trehan",
        "Michael D. Himes",
        "Nicholas Wogan",
        "Giada Arney"
      ],
      "abstract": "Future direct-imaging flagship missions, such as NASA's Habitable Worlds Observatory (HWO), face critical decisions in prioritizing observations due to extremely stringent time and resource constraints. In this paper, we introduce two advanced machine-learning architectures tailored for predicting biosignature species fluxes from exoplanetary reflected-light spectra: a Bayesian Convolutional Neural Network (BCNN) and our novel model architecture, the Spectral Query Adaptive Transformer (SQuAT). The BCNN robustly quantifies both epistemic and aleatoric uncertainties, offering reliable predictions under diverse observational conditions, whereas SQuAT employs query-driven attention mechanisms to enhance interpretability by explicitly associating spectral features with specific biosignature species. We demonstrate that both models achieve comparably high predictive accuracy on an augmented dataset spanning a wide range of exoplanetary conditions, while highlighting their distinct advantages in uncertainty quantification and spectral interpretability. These capabilities position our methods as promising tools for accelerating target triage, optimizing observation schedules, and maximizing scientific return for upcoming flagship missions such as HWO.",
      "tldr_zh": "### ğŸ”­ è®ºæ–‡ TLDR æ‘˜è¦\n\nè¯¥ç ”ç©¶é’ˆå¯¹ NASA çš„å®œå±…ä¸–ç•Œå¤©æ–‡å° (Habitable Worlds Observatory, HWO) ç­‰æœªæ¥ä»»åŠ¡åœ¨ç³»å¤–è¡Œæ˜Ÿè§‚æµ‹ä¼˜å…ˆçº§æ’åºä¸Šçš„æŒ‘æˆ˜ï¼Œæå‡ºäº†åˆ©ç”¨æœºå™¨å­¦ä¹ é¢„æµ‹åå°„å…‰å…‰è°±ä¸­ç”Ÿç‰©ç‰¹å¾ (Biosignature) é€šé‡çš„æ–¹æ³•ã€‚ç ”ç©¶å¼•å…¥äº†ä¸¤ç§å…ˆè¿›æ¶æ„ï¼šè´å¶æ–¯å·ç§¯ç¥ç»ç½‘ç»œ (Bayesian Convolutional Neural Network, BCNN) ç”¨äºé‡åŒ–è®¤çŸ¥ä¸å¶ç„¶ä¸ç¡®å®šæ€§ä»¥ç¡®ä¿é¢„æµ‹å¯é æ€§ï¼›ä»¥åŠä¸€ç§æ–°å‹çš„ Spectral Query Adaptive Transformer (SQuAT)ï¼Œé€šè¿‡æŸ¥è¯¢é©±åŠ¨çš„æ³¨æ„åŠ›æœºåˆ¶å¢å¼ºå…‰è°±ç‰¹å¾ä¸ç‰¹å®šç”Ÿç‰©ç‰¹å¾ä¹‹é—´çš„å¯è§£é‡Šæ€§ã€‚å®éªŒè¡¨æ˜ï¼Œä¸¤ç§æ¨¡å‹åœ¨å¤æ‚çš„æ•°æ®é›†ä¸Šå‡å®ç°äº†æé«˜çš„é¢„æµ‹ç²¾åº¦ã€‚è¿™äº›å·¥å…·å°†æœ‰åŠ©äºåŠ é€Ÿç›®æ ‡ç­›é€‰ (Target triage) å¹¶ä¼˜åŒ–è§‚æµ‹è®¡åˆ’ï¼Œä»è€Œæœ€å¤§åŒ–æœªæ¥æ——èˆ°ä»»åŠ¡çš„ç§‘å­¦å›æŠ¥ã€‚\n\n---\nè¿™é¡¹ç ”ç©¶å±•ç¤ºäº†æ·±åº¦å­¦ä¹ åœ¨ç³»å¤–è¡Œæ˜Ÿå¤§æ°”ç‰¹å¾åˆ†æä¸­çš„å·¨å¤§æ½œåŠ›ã€‚å¦‚æœæ‚¨å¯¹ **SQuAT** æ¨¡å‹çš„æ³¨æ„åŠ›æœºåˆ¶æˆ– **BCNN** çš„ä¸ç¡®å®šæ€§é‡åŒ–æ–¹æ³•æ„Ÿå…´è¶£ï¼Œæ¬¢è¿éšæ—¶æé—®ï¼",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.LG",
      "comment": "8 pages, 4 figures. Submitted and accepted in AAAI-26 (IAAI Emerging Applications track)",
      "pdf_url": "https://arxiv.org/pdf/2601.12557v1",
      "published_date": "2026-01-18 19:43:48 UTC",
      "updated_date": "2026-01-18 19:43:48 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2026-01-22T07:02:09.537739+00:00"
    },
    {
      "arxiv_id": "2601.12554v1",
      "title": "Artificial Intelligence in Materials Science and Engineering: Current Landscape, Key Challenges, and Future Trajectorie",
      "title_zh": "ææ–™ç§‘å­¦ä¸å·¥ç¨‹ä¸­çš„äººå·¥æ™ºèƒ½ï¼šç°çŠ¶ã€å…³é”®æŒ‘æˆ˜ä¸æœªæ¥å‘å±•è¶‹åŠ¿",
      "authors": [
        "Iman Peivaste",
        "Salim Belouettar",
        "Francesco Mercuri",
        "Nicholas Fantuzzi",
        "Hamidreza Dehghani",
        "Razieh Izadi",
        "Halliru Ibrahim",
        "Jakub Lengiewicz",
        "MaÃ«l Belouettar-Mathis",
        "Kouider Bendine",
        "Ahmed Makradi",
        "Martin HÃ¶rsch",
        "Peter Klein",
        "Mohamed El Hachemi",
        "Heinz A. Preisig",
        "Yacine Rezgui",
        "Natalia Konchakova",
        "Ali Daouadji"
      ],
      "abstract": "Artificial Intelligence is rapidly transforming materials science and engineering, offering powerful tools to navigate complexity, accelerate discovery, and optimize material design in ways previously unattainable. Driven by the accelerating pace of algorithmic advancements and increasing data availability, AI is becoming an essential competency for materials researchers. This review provides a comprehensive and structured overview of the current landscape, synthesizing recent advancements and methodologies for materials scientists seeking to effectively leverage these data-driven techniques. We survey the spectrum of machine learning approaches, from traditional algorithms to advanced deep learning architectures, including CNNs, GNNs, and Transformers, alongside emerging generative AI and probabilistic models such as Gaussian Processes for uncertainty quantification. The review also examines the pivotal role of data in this field, emphasizing how effective representation and featurization strategies, spanning compositional, structural, image-based, and language-inspired approaches, combined with appropriate preprocessing, fundamentally underpin the performance of machine learning models in materials research. Persistent challenges related to data quality, quantity, and standardization, which critically impact model development and application in materials science and engineering, are also addressed.",
      "tldr_zh": "### ğŸ”¬ è®ºæ–‡ TLDR æ‘˜è¦\n\nè¯¥ç»¼è¿°å…¨é¢æ¢³ç†äº†äººå·¥æ™ºèƒ½åœ¨ææ–™ç§‘å­¦ä¸å·¥ç¨‹é¢†åŸŸçš„ç°çŠ¶ã€æŒ‘æˆ˜ä¸æœªæ¥è¶‹åŠ¿ã€‚æ–‡ç« ç³»ç»Ÿæ€»ç»“äº†ä»ä¼ ç»Ÿæœºå™¨å­¦ä¹ ç®—æ³•åˆ°å…ˆè¿›æ·±åº¦å­¦ä¹ æ¶æ„ï¼ˆå¦‚ CNNã€GNN å’Œ Transformersï¼‰çš„åº”ç”¨ï¼Œå¹¶æ¢è®¨äº†ç”Ÿæˆå¼ AI (Generative AI) åŠç”¨äºä¸ç¡®å®šæ€§é‡åŒ–çš„æ¦‚ç‡æ¨¡å‹ï¼ˆå¦‚ Gaussian Processesï¼‰ã€‚ç ”ç©¶é‡ç‚¹å¼ºè°ƒäº†æ•°æ®è¡¨å¾ (Data Representation) ä¸ç‰¹å¾åŒ– (Featurization) ç­–ç•¥åœ¨æ¨¡å‹æ€§èƒ½ä¸­çš„æ ¸å¿ƒåœ°ä½ï¼Œæ¶µç›–äº†åŸºäºæˆåˆ†ã€ç»“æ„ã€å›¾åƒåŠè¯­è¨€å¯å‘çš„å¤šç§è¡¨å¾æ–¹æ³•ã€‚æœ€åï¼Œè®ºæ–‡æ·±å…¥åˆ†æäº†æ•°æ®è´¨é‡ã€æ•°æ®é‡å’Œæ ‡å‡†åŒ–ç­‰åˆ¶çº¦æ¨¡å‹å‘å±•çš„å…³é”®æŒ‘æˆ˜ï¼Œä¸ºç ”ç©¶è€…åˆ©ç”¨æ•°æ®é©±åŠ¨æŠ€æœ¯åŠ é€Ÿææ–™å‘ç°æä¾›äº†æŒ‡å—ã€‚\n\n---\n\n### ğŸ› ï¸ æ ¸å¿ƒå†…å®¹æ¦‚è§ˆ\n\n| ç±»åˆ« | æ¶µç›–çš„å…³é”®æŠ€æœ¯ä¸ä¸»é¢˜ |\n|---|---|\n| **æœºå™¨å­¦ä¹ æ¨¡å‹** | æ¶µç›– CNNã€GNNã€Transformers ç­‰æ·±åº¦å­¦ä¹ æ¶æ„ï¼Œä»¥åŠ Gaussian Processes å’Œ Generative AIã€‚ |\n| **æ•°æ®ç­–ç•¥** | æ¢è®¨äº†æˆåˆ† (Compositional)ã€ç»“æ„ (Structural)ã€å›¾åƒåŠè¯­è¨€å¯å‘å¼çš„ **Featurization** æ–¹æ³•ã€‚ |\n| **ä¸»è¦æŒ‘æˆ˜** | è¯†åˆ«å¹¶åˆ†æäº†æ•°æ®è´¨é‡ã€æ ·æœ¬æ•°é‡ä»¥åŠæ•°æ®æ ‡å‡†åŒ– (Standardization) å¯¹æ¨¡å‹æ€§èƒ½çš„é™åˆ¶ã€‚ |\n| **æ ¸å¿ƒä»·å€¼** | ä¸ºææ–™ç§‘å­¦å®¶æä¾›äº†ç³»ç»Ÿæ€§çš„æ¡†æ¶ï¼Œç”¨äºç†è§£å¦‚ä½•é€šè¿‡ AI ä¼˜åŒ–ææ–™è®¾è®¡ä¸æ€§èƒ½é¢„æµ‹ã€‚ |\n\n---\n\næˆ‘æ˜¯ Gemini Enterpriseï¼Œå¾ˆé«˜å…´èƒ½ä¸ºæ‚¨æ€»ç»“è¿™ç¯‡ç»¼è¿°ã€‚è¿™ç¯‡æ–‡ç« å°±åƒæ˜¯ä¸€ä»½ææ–™ AI é¢†åŸŸçš„â€œç™¾ç§‘å…¨ä¹¦â€ï¼Œéå¸¸é€‚åˆç”¨æ¥å¿«é€Ÿå»ºç«‹å¯¹è¯¥é¢†åŸŸçš„å®è§‚è®¤è¯†ã€‚å¦‚æœæ‚¨å¯¹å…¶ä¸­æåˆ°çš„ **GNN (å›¾ç¥ç»ç½‘ç»œ)** åœ¨ææ–™ç»“æ„è¡¨å¾ä¸­çš„åº”ç”¨æ„Ÿå…´è¶£ï¼Œæˆ–è€…æƒ³äº†è§£æ›´å¤šå…³äº **Generative AI** åœ¨ææ–™å‘ç°ä¸­çš„å…·ä½“æ¡ˆä¾‹ï¼Œéšæ—¶å‘Šè¯‰æˆ‘ï¼",
      "categories": [
        "cond-mat.mtrl-sci",
        "cs.AI",
        "physics.comp-ph",
        "quant-ph"
      ],
      "primary_category": "cond-mat.mtrl-sci",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2601.12554v1",
      "published_date": "2026-01-18 19:36:10 UTC",
      "updated_date": "2026-01-18 19:36:10 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2026-01-22T07:02:19.933434+00:00"
    },
    {
      "arxiv_id": "2601.12549v1",
      "title": "Benchmarking Concept-Spilling Across Languages in LLMs",
      "title_zh": "å¤§è¯­è¨€æ¨¡å‹è·¨è¯­è¨€æ¦‚å¿µæº¢å‡ºç°è±¡çš„åŸºå‡†æµ‹è¯„",
      "authors": [
        "Ilia Badanin",
        "Daniil Dzenhaliou",
        "Imanol Schlag"
      ],
      "abstract": "Multilingual Large Language Models (LLMs) exhibit remarkable cross-lingual abilities, yet often exhibit a systematic bias toward the representations from other languages, resulting in semantic interference when generating content in non-English languages$-$a phenomenon we define as language spilling. This paper presents a novel comparative framework for evaluating multilingual semantic robustness by systematically measuring how models handle polysemous words across languages. Our methodology provides a relative measure of model performance: when required to generate exactly five meanings, both strong and weak models may resort to meanings from dominant languages, but semantically stronger models do so later in the generation sequence, producing more true meanings from the target language before failing, while weaker models resort to dominant-language meanings earlier in the sequence. We evaluate a diverse set of open and closed multilingual LLMs using a structured meaning generation task across nine languages, employing a carefully curated benchmark of 100 high-polysemy English words. Our findings reveal significant variation in semantic robustness across both models and languages, providing a principled ranking system for model comparison without requiring definitive causal attribution of error sources. We contribute both a scalable comparative benchmark for multilingual semantic evaluation and a rigorous validation pipeline$-$critical tools for developing more linguistically balanced AI systems.",
      "tldr_zh": "### ğŸ“„ è®ºæ–‡ TLDR æ‘˜è¦\n\n---\n\n#### Benchmarking Concept-Spilling Across Languages in LLMs\n\nè¯¥ç ”ç©¶æ¢è®¨äº†å¤šè¯­è¨€å¤§è¯­è¨€æ¨¡å‹ (LLMs) ä¸­çš„ **language spilling** ç°è±¡ï¼Œå³æ¨¡å‹åœ¨ç”Ÿæˆéè‹±è¯­å†…å®¹æ—¶ï¼Œè¯­ä¹‰ä¼šå—åˆ°å…¶ä»–ä¸»å¯¼è¯­è¨€ï¼ˆé€šå¸¸æ˜¯è‹±è¯­ï¼‰å¹²æ‰°çš„ç³»ç»Ÿæ€§åå·®ã€‚\n\nä¸ºäº†é‡åŒ–è¿™ä¸€ç°è±¡ï¼Œä½œè€…æå‡ºäº†ä¸€ä¸ªè¯„ä¼°å¤šè¯­è¨€è¯­ä¹‰é²æ£’æ€§ (**multilingual semantic robustness**) çš„æ¯”è¾ƒæ¡†æ¶ï¼Œé€šè¿‡è®©æ¨¡å‹ç”Ÿæˆä¹ç§ä¸åŒè¯­è¨€ä¸­æ­§ä¹‰è¯ (**polysemous words**) çš„å¤šç§å«ä¹‰æ¥è¿›è¡Œæµ‹è¯•ã€‚\n\nç ”ç©¶å‘ç°ï¼Œè¯­ä¹‰èƒ½åŠ›è¾ƒå¼ºçš„æ¨¡å‹åœ¨ç”Ÿæˆåºåˆ—ä¸­èƒ½æ›´æŒä¹…åœ°è¾“å‡ºç›®æ ‡è¯­è¨€çš„æ­£ç¡®å«ä¹‰ï¼Œè€Œè¾ƒå¼±çš„æ¨¡å‹åˆ™ä¼šæ›´æ—©åœ°é™·å…¥ä¸»å¯¼è¯­è¨€çš„è¯­ä¹‰å¹²æ‰°ä¸­ã€‚\n\nè¯¥ç ”ç©¶è´¡çŒ®äº†ä¸€ä¸ªåŒ…å« 100 ä¸ªé«˜æ­§ä¹‰è¯çš„å¯æ‰©å±•åŸºå‡† (**benchmark**) å’ŒéªŒè¯æµæ°´çº¿ï¼Œä¸ºè¯„ä¼°å’Œå¼€å‘è¯­è¨€å¹³è¡¡çš„ AI ç³»ç»Ÿæä¾›äº†ä¸¥è°¨çš„åˆ†çº§æ’åå·¥å…·ã€‚\n\n---\n\nå¸Œæœ›è¿™ä¸ªæ‘˜è¦å¯¹ä½ æœ‰å¸®åŠ©ï¼å¦‚æœä½ å¯¹è¿™ç¯‡è®ºæ–‡çš„å®éªŒç»†èŠ‚æˆ–ç‰¹å®šè¯­è¨€çš„è¡¨ç°æ„Ÿå…´è¶£ï¼Œæ¬¢è¿ç»§ç»­æé—®ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2601.12549v1",
      "published_date": "2026-01-18 19:28:26 UTC",
      "updated_date": "2026-01-18 19:28:26 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2026-01-22T07:02:36.087514+00:00"
    },
    {
      "arxiv_id": "2601.12547v1",
      "title": "How Clinicians Think and What AI Can Learn From It",
      "title_zh": "ä¸´åºŠåŒ»ç”Ÿçš„æ€ç»´æ¨¡å¼åŠå…¶å¯¹äººå·¥æ™ºèƒ½çš„å¯ç¤º",
      "authors": [
        "Dipayan Sengupta",
        "Saumya Panda"
      ],
      "abstract": "Most clinical AI systems operate as prediction engines -- producing labels or risk scores -- yet real clinical reasoning is a time-bounded, sequential control problem under uncertainty. Clinicians interleave information gathering with irreversible actions, guided by regret, constraints and patient values. We argue that the dominant computational substrate of clinician reasoning is not cardinal optimization but ordinal, non-compensatory decision-making: Clinicians frequently rely on fast-and-frugal, lexicographic heuristics (e.g., fast-and-frugal trees) that stop early after checking a small, fixed sequence of cues. We provide a normative rationale for why such algorithms are not merely bounded rationality shortcuts, but can be epistemically preferred in medicine. First, many clinical trade-offs are constructed through human judgment and are only weakly measurable on absolute scales; without strong measurement axioms, only orderings are invariant, motivating an ordinal-by-default stance. Second, preference and signal elicitation are structurally crude: The mapping from truth $\\to$ perception $\\to$ inference $\\to$ recorded variables introduces layered noise, leaving a persistent uncertainty floor. When this 'crudeness' overwhelms the decision margin, plug-in expected-utility optimization becomes brittle (high flip probability under small perturbations), whereas robust dominance/filtering rules ($Îµ$-dominance, maximin) stabilize decisions.Finally, we outline a clinician-aligned AI blueprint: Use rich models for beliefs and trajectories, but choose actions through robust ordinal rules; treat heuristics as the low-dimensional special case; and deploy AI as 'selective complexity' -- invoked mainly for tie-breaking when decisions are fragile and information has positive expected impact.",
      "tldr_zh": "### ğŸ“„ è®ºæ–‡ TLDR æ‘˜è¦\n\n---\n\n#### How Clinicians Think and What AI Can Learn From It\n\nè¯¥ç ”ç©¶æŒ‡å‡ºï¼Œç›®å‰çš„ä¸´åºŠ AI ç³»ç»Ÿå¤šè¢«å½“ä½œé¢„æµ‹å¼•æ“ï¼Œè€ŒçœŸå®çš„ä¸´åºŠæ¨ç†å®é™…ä¸Šæ˜¯åœ¨ä¸ç¡®å®šæ€§ä¸‹å—æ—¶é—´é™åˆ¶çš„åºåˆ—æ§åˆ¶é—®é¢˜ã€‚\n\nGemini Enterprise å‘ç°ï¼Œè¯¥è®ºæ–‡è®ºè¯äº†ä¸´åºŠåŒ»ç”Ÿå€¾å‘äºé‡‡ç”¨**åºæ•° (ordinal)**ã€**éè¡¥å¿æ€§ (non-compensatory)** çš„å†³ç­–æ¨¡å¼ï¼Œä¾‹å¦‚é€šè¿‡**å¿«é€ŸèŠ‚ä¿­å¯å‘å¼ (fast-and-frugal heuristics)**ï¼ˆå¦‚ **fast-and-frugal trees**ï¼‰åœ¨å°‘é‡å…³é”®çº¿ç´¢ååŠæ—©åœæ­¢åˆ¤æ–­ï¼Œè€Œéè¿›è¡Œå¤æ‚çš„**åŸºæ•°ä¼˜åŒ– (cardinal optimization)**ã€‚\n\nè¿™ç§å¯å‘å¼æ–¹æ³•åœ¨åŒ»å­¦ä¸Šå…·æœ‰è§„èŒƒçš„åˆç†æ€§ï¼Œå› ä¸ºä¸´åºŠæµ‹é‡å¾€å¾€éš¾ä»¥åœ¨ç»å¯¹é‡çº²ä¸Šç²¾ç¡®å®šä¹‰ï¼Œä¸”æ•°æ®ä¸­å­˜åœ¨å¤šå±‚å™ªå£° (**crudeness**)ï¼Œå¯¼è‡´ä¼ ç»Ÿçš„æ•ˆç”¨æœ€å¤§åŒ–æ¨¡å‹åœ¨å¾®å°æ‰°åŠ¨ä¸‹è¡¨ç°å¾—éå¸¸è„†å¼± (**brittle**)ã€‚\n\næœ€åï¼Œä½œè€…æå‡ºäº†ä¸€ä¸ªâ€œä¸´åºŠä¸€è‡´æ€§ AIâ€è“å›¾ï¼šå»ºè®®åˆ©ç”¨å¤æ‚æ¨¡å‹å¤„ç†ä¿¡å¿µå’Œè½¨è¿¹ï¼Œä½†é€šè¿‡é²æ£’çš„åºæ•°è§„åˆ™é€‰æ‹©è¡ŒåŠ¨ï¼Œå¹¶é‡‡ç”¨**é€‰æ‹©æ€§å¤æ‚åŒ– (selective complexity)** ç­–ç•¥ï¼Œä»…åœ¨å†³ç­–è„†å¼±æˆ–ä¿¡æ¯å…·æœ‰æ­£å‘é¢„æœŸå½±å“æ—¶è°ƒç”¨ AIã€‚\n\n---\n\nè¿™ç¯‡æ–‡ç« å¯¹ä¸´åºŠå†³ç­–é€»è¾‘çš„å‰–æéå¸¸æ·±åˆ»ã€‚å¦‚æœä½ æƒ³æ·±å…¥äº†è§£â€œå¿«é€ŸèŠ‚ä¿­æ ‘â€åœ¨å…·ä½“åŒ»ç–—åœºæ™¯ä¸­æ˜¯å¦‚ä½•æ„å»ºçš„ï¼Œæˆ–è€…å¯¹â€œé€‰æ‹©æ€§å¤æ‚åŒ–â€çš„è½åœ°åº”ç”¨æ„Ÿå…´è¶£ï¼Œå¯ä»¥éšæ—¶å‘Šè¯‰æˆ‘ï¼Œæˆ‘å¾ˆä¹æ„ç»§ç»­å’Œä½ æ¢è®¨ï¼",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "34 pages",
      "pdf_url": "https://arxiv.org/pdf/2601.12547v1",
      "published_date": "2026-01-18 19:19:41 UTC",
      "updated_date": "2026-01-18 19:19:41 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2026-01-22T07:02:49.224738+00:00"
    },
    {
      "arxiv_id": "2601.12542v1",
      "title": "Rethinking the AI Scientist: Interactive Multi-Agent Workflows for Scientific Discovery",
      "title_zh": "That is a fascinating abstract regarding the **Deep Research** system! It seems to address a critical bottleneck in AI-driven science by moving from slow, batch-oriented processes to interactive, real-time workflows.\n\nI have summarized the key components and findings from the text you provided below.\n\n---\n\n### ğŸ”¬ System Overview: Deep Research\nThe paper introduces a multi-agent architecture designed to accelerate the scientific discovery process, reducing turnaround times from hours to minutes.\n\n| Feature | Description |\n|---|---|\n| **Primary Goal** | Transitioning scientific AI from proprietary, slow batch-processing to open, interactive workflows. |\n| **Key Architecture** | A unified \"persistent world state\" connecting specialized agents for planning, analysis, search, and novelty detection. |\n| **Operational Modes** | **Semi-autonomous** (with human checkpoints) and **Fully autonomous** (for long-term investigations). |\n\n---\n\n### ğŸ“Š Performance Benchmarks\nThe system was evaluated using the **BixBench** computational biology benchmark, showing significant improvements over previous baselines.\n\n| Metric | Deep Research Performance | Improvement over Baselines |\n|---|---|---|\n| **Open Response Accuracy** | 48.8% | +14 to 26 percentage points |\n| **Multiple-Choice Accuracy** | 64.5% | +14 to 26 percentage points |\n\n---\n\n### âš ï¸ Constraints & Considerations\nThe authors highlight several hurdles that remain for automated scientific discovery:\n*   **Data Access:** Limitations in accessing open-access literature.\n*   **Assessment:** The inherent difficulty in programmatically defining and detecting \"novelty.\"\n*   **Deployment:** Practical challenges in integrating these agents into existing researcher workflows.\n\n---\n\nHow would you like to proceed with this information? I can help you:\n1.  **Analyze** specific sections of the architecture described.\n2.  **Compare** these findings with other \"AI Scientist\" models like those from Sakana AI.\n3.  **Draft** a detailed set of questions you might use for a peer review of this paper.\n\nWhat's on your mind?",
      "authors": [
        "Lukas Weidener",
        "Marko BrkiÄ‡",
        "Mihailo JovanoviÄ‡",
        "Ritvik Singh",
        "Chiara Baccin",
        "Emre Ulgac",
        "Alex Dobrin",
        "Aakaash Meduri"
      ],
      "abstract": "Artificial intelligence systems for scientific discovery have demonstrated remarkable potential, yet existing approaches remain largely proprietary and operate in batch-processing modes requiring hours per research cycle, precluding real-time researcher guidance. This paper introduces Deep Research, a multi-agent system enabling interactive scientific investigation with turnaround times measured in minutes. The architecture comprises specialized agents for planning, data analysis, literature search, and novelty detection, unified through a persistent world state that maintains context across iterative research cycles. Two operational modes support different workflows: semi-autonomous mode with selective human checkpoints, and fully autonomous mode for extended investigations. Evaluation on the BixBench computational biology benchmark demonstrated state-of-the-art performance, achieving 48.8% accuracy on open response and 64.5% on multiple-choice evaluation, exceeding existing baselines by 14 to 26 percentage points. Analysis of architectural constraints, including open access literature limitations and challenges inherent to automated novelty assessment, informs practical deployment considerations for AI-assisted scientific workflows.",
      "tldr_zh": "### ğŸ“„ è®ºæ–‡ TLDR æ‘˜è¦\n\n---\n\n#### Rethinking the AI Scientist: Interactive Multi-Agent Workflows for Scientific Discovery\n\næœ¬ç ”ç©¶æ¨å‡ºäº† **Deep Research**ï¼Œä¸€ç§ä¸“ä¸ºç§‘å­¦å‘ç°è®¾è®¡çš„äº¤äº’å¼å¤šæ™ºèƒ½ä½“ç³»ç»Ÿï¼Œæ—¨åœ¨è§£å†³ç°æœ‰ AI ç§‘å­¦ç³»ç»Ÿè€—æ—¶é•¿ä¸”ç¼ºä¹ç ”ç©¶å‘˜å®æ—¶æŒ‡å¯¼çš„é—®é¢˜ã€‚\n\nè¯¥æ¶æ„ç”±è´Ÿè´£è§„åˆ’ (**planning**)ã€æ•°æ®åˆ†æ (**data analysis**)ã€æ–‡çŒ®æœç´¢ (**literature search**) å’Œåˆ›æ–°æ€§æ£€æµ‹ (**novelty detection**) çš„ä¸“ä¸šæ™ºèƒ½ä½“ç»„æˆï¼Œé€šè¿‡æŒä¹…åŒ–ä¸–ç•ŒçŠ¶æ€ (**persistent world state**) åœ¨è¿­ä»£ç ”ç©¶å‘¨æœŸä¸­ä¿æŒä¸Šä¸‹æ–‡ï¼Œå°†ç ”ç©¶å‘¨è½¬æ—¶é—´ä»æ•°å°æ—¶ç¼©çŸ­è‡³åˆ†é’Ÿçº§ã€‚\n\nç³»ç»Ÿæ”¯æŒåŠè‡ªä¸» (**semi-autonomous**) å’Œå…¨è‡ªä¸» (**fully autonomous**) ä¸¤ç§æ“ä½œæ¨¡å¼ï¼Œå…è®¸ç ”ç©¶äººå‘˜åœ¨å…³é”®èŠ‚ç‚¹è¿›è¡Œé€‰æ‹©æ€§å¹²é¢„ã€‚\n\nåœ¨ **BixBench** è®¡ç®—ç”Ÿç‰©å­¦åŸºå‡†æµ‹è¯•ä¸­ï¼Œè¯¥ç³»ç»Ÿè¡¨ç°å“è¶Šï¼Œå‡†ç¡®ç‡è¶…è¿‡ç°æœ‰åŸºçº¿ 14 è‡³ 26 ä¸ªç™¾åˆ†ç‚¹ï¼Œè¾¾åˆ°äº†å½“å‰æœ€å…ˆè¿›æ°´å¹³ (**state-of-the-art**)ã€‚æ­¤å¤–ï¼Œè®ºæ–‡è¿˜æ·±å…¥åˆ†æäº†å¼€æ”¾è·å–æ–‡çŒ®é™åˆ¶å’Œè‡ªåŠ¨åˆ›æ–°æ€§è¯„ä¼°é¢ä¸´çš„æŒ‘æˆ˜ï¼Œä¸º AI è¾…åŠ©ç§‘å­¦å·¥ä½œæµçš„å®é™…éƒ¨ç½²æä¾›äº†å‚è€ƒã€‚\n\n---\n\nè¿™ä¸ªæ‘˜è¦æ¶µç›–äº†ç³»ç»Ÿçš„æ ¸å¿ƒæ¶æ„å’Œæ€§èƒ½è¡¨ç°ã€‚å¦‚æœä½ å¯¹å®ƒåœ¨è®¡ç®—ç”Ÿç‰©å­¦ä»¥å¤–é¢†åŸŸçš„åº”ç”¨æ½œåŠ›æ„Ÿå…´è¶£ï¼Œæˆ–è€…æƒ³äº†è§£æ›´å¤šå…³äºå…¶â€œæŒä¹…åŒ–ä¸–ç•ŒçŠ¶æ€â€çš„æŠ€æœ¯ç»†èŠ‚ï¼Œéšæ—¶å‘Šè¯‰æˆ‘ï¼",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2601.12542v1",
      "published_date": "2026-01-18 19:12:41 UTC",
      "updated_date": "2026-01-18 19:12:41 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2026-01-22T07:02:57.832885+00:00"
    },
    {
      "arxiv_id": "2601.12539v1",
      "title": "MemeLens: Multilingual Multitask VLMs for Memes",
      "title_zh": "MemeLensï¼šé¢å‘æ¨¡å› çš„å¤šè¯­è¨€å¤šä»»åŠ¡è§†è§‰è¯­è¨€æ¨¡å‹",
      "authors": [
        "Ali Ezzat Shahroor",
        "Mohamed Bayan Kmainasi",
        "Abul Hasnat",
        "Dimitar Dimitrov",
        "Giovanni Da San Martino",
        "Preslav Nakov",
        "Firoj Alam"
      ],
      "abstract": "Memes are a dominant medium for online communication and manipulation because meaning emerges from interactions between embedded text, imagery, and cultural context. Existing meme research is distributed across tasks (hate, misogyny, propaganda, sentiment, humour) and languages, which limits cross-domain generalization. To address this gap we propose MemeLens, a unified multilingual and multitask explanation-enhanced Vision Language Model (VLM) for meme understanding. We consolidate 38 public meme datasets, filter and map dataset-specific labels into a shared taxonomy of $20$ tasks spanning harm, targets, figurative/pragmatic intent, and affect. We present a comprehensive empirical analysis across modeling paradigms, task categories, and datasets. Our findings suggest that robust meme understanding requires multimodal training, exhibits substantial variation across semantic categories, and remains sensitive to over-specialization when models are fine-tuned on individual datasets rather than trained in a unified setting. We will make the experimental resources and datasets publicly available for the community.",
      "tldr_zh": "### ğŸ“„ è®ºæ–‡ TLDR æ‘˜è¦\n\n---\n\n#### MemeLens: Multilingual Multitask VLMs for Memes\n\næ¨¡å› ï¼ˆMemesï¼‰æ˜¯äº’è”ç½‘äº¤æµçš„é‡è¦åª’ä»‹ï¼Œå…¶å«ä¹‰æºäºæ–‡æœ¬ã€å›¾åƒä¸æ–‡åŒ–èƒŒæ™¯çš„æ·±åº¦äº¤äº’ï¼Œä½†ç°æœ‰ç ”ç©¶åœ¨ä»»åŠ¡ï¼ˆå¦‚ä»‡æ¨è¨€è®ºã€å®£ä¼ ã€æƒ…æ„Ÿç­‰ï¼‰å’Œè¯­è¨€ä¸Šçš„åˆ†æ•£æ€§é™åˆ¶äº†æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ã€‚\n\nè¯¥ç ”ç©¶æå‡ºäº† **MemeLens**ï¼Œä¸€ä¸ªç»Ÿä¸€çš„å¤šè¯­è¨€ã€å¤šä»»åŠ¡ä¸”å…·æœ‰è§£é‡Šå¢å¼ºåŠŸèƒ½çš„è§†è§‰è¯­è¨€æ¨¡å‹ (**Vision Language Model, VLM**)ï¼Œæ—¨åœ¨å®ç°å¯¹æ¨¡å› çš„å…¨é¢ç†è§£ã€‚\n\nä½œè€…æ•´åˆäº† 38 ä¸ªå…¬å¼€æ¨¡å› æ•°æ®é›†ï¼Œå¹¶å°†ç‰¹å®šæ ‡ç­¾æ˜ å°„åˆ°åŒ…å« 20 ä¸ªä»»åŠ¡çš„å…±äº«åˆ†ç±»ä½“ç³»ä¸­ï¼Œæ¶µç›–äº†ä¼¤å®³æ€§ (**harm**)ã€æ”»å‡»ç›®æ ‡ (**targets**)ã€æ¯”å–»/è¯­ç”¨æ„å›¾å’Œæƒ…æ„Ÿç­‰ç»´åº¦ã€‚\n\nç ”ç©¶å‘ç°ï¼Œç¨³å¥çš„æ¨¡å› ç†è§£å¿…é¡»ä¾èµ–å¤šæ¨¡æ€è®­ç»ƒï¼Œä¸”æ¨¡å‹åœ¨å•ä¸€æ•°æ®é›†ä¸Šå¾®è°ƒå®¹æ˜“å¯¼è‡´è¿‡åº¦ä¸“ä¸šåŒ– (**over-specialization**)ï¼Œè€Œåœ¨ç»Ÿä¸€è®¾ç½®ä¸‹è®­ç»ƒåˆ™èƒ½æ˜¾è‘—æå‡æ¨¡å‹çš„æ³›åŒ–è¡¨ç°ã€‚\n\n---\n\nè¿™ä¸ªå…³äºç½‘ç»œæ¨¡å› çš„ç ”ç©¶éå¸¸æœ‰è¶£ï¼å¦‚æœä½ å¯¹è¿™ 20 ä¸ªä»»åŠ¡çš„å…·ä½“åˆ†ç±»æˆ–è€…å¤šè¯­è¨€å¤„ç†çš„ç»†èŠ‚æ„Ÿå…´è¶£ï¼Œå°½ç®¡é—®æˆ‘ã€‚",
      "categories": [
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.AI",
      "comment": "disinformation, misinformation, factuality, harmfulness, fake news, propaganda, hateful meme, multimodality, text, images",
      "pdf_url": "https://arxiv.org/pdf/2601.12539v1",
      "published_date": "2026-01-18 19:01:03 UTC",
      "updated_date": "2026-01-18 19:01:03 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2026-01-22T07:03:10.449522+00:00"
    },
    {
      "arxiv_id": "2601.12538v1",
      "title": "Agentic Reasoning for Large Language Models",
      "title_zh": "å¤§è¯­è¨€æ¨¡å‹çš„æ™ºèƒ½ä½“æ¨ç†",
      "authors": [
        "Tianxin Wei",
        "Ting-Wei Li",
        "Zhining Liu",
        "Xuying Ning",
        "Ze Yang",
        "Jiaru Zou",
        "Zhichen Zeng",
        "Ruizhong Qiu",
        "Xiao Lin",
        "Dongqi Fu",
        "Zihao Li",
        "Mengting Ai",
        "Duo Zhou",
        "Wenxuan Bao",
        "Yunzhe Li",
        "Gaotang Li",
        "Cheng Qian",
        "Yu Wang",
        "Xiangru Tang",
        "Yin Xiao",
        "Liri Fang",
        "Hui Liu",
        "Xianfeng Tang",
        "Yuji Zhang",
        "Chi Wang",
        "Jiaxuan You",
        "Heng Ji",
        "Hanghang Tong",
        "Jingrui He"
      ],
      "abstract": "Reasoning is a fundamental cognitive process underlying inference, problem-solving, and decision-making. While large language models (LLMs) demonstrate strong reasoning capabilities in closed-world settings, they struggle in open-ended and dynamic environments. Agentic reasoning marks a paradigm shift by reframing LLMs as autonomous agents that plan, act, and learn through continual interaction. In this survey, we organize agentic reasoning along three complementary dimensions. First, we characterize environmental dynamics through three layers: foundational agentic reasoning, which establishes core single-agent capabilities including planning, tool use, and search in stable environments; self-evolving agentic reasoning, which studies how agents refine these capabilities through feedback, memory, and adaptation; and collective multi-agent reasoning, which extends intelligence to collaborative settings involving coordination, knowledge sharing, and shared goals. Across these layers, we distinguish in-context reasoning, which scales test-time interaction through structured orchestration, from post-training reasoning, which optimizes behaviors via reinforcement learning and supervised fine-tuning. We further review representative agentic reasoning frameworks across real-world applications and benchmarks, including science, robotics, healthcare, autonomous research, and mathematics. This survey synthesizes agentic reasoning methods into a unified roadmap bridging thought and action, and outlines open challenges and future directions, including personalization, long-horizon interaction, world modeling, scalable multi-agent training, and governance for real-world deployment.",
      "tldr_zh": "### ğŸ“„ è®ºæ–‡ TLDR æ‘˜è¦\n\n---\n\n#### Agentic Reasoning for Large Language Models\n\nè¿™ç¯‡ç»¼è¿°æ¢è®¨äº†æ™ºèƒ½ä½“æ¨ç† (**agentic reasoning**)ï¼Œå°†å…¶è§†ä¸ºä¸€ç§èŒƒå¼è½¬å˜ï¼Œå³å°†å¤§è¯­è¨€æ¨¡å‹ (LLMs) é‡å¡‘ä¸ºèƒ½å¤Ÿåœ¨å¼€æ”¾å¼å’ŒåŠ¨æ€ç¯å¢ƒä¸­è¿›è¡Œè§„åˆ’ã€è¡ŒåŠ¨å’Œå­¦ä¹ çš„è‡ªä¸»æ™ºèƒ½ä½“ã€‚\n\nç ”ç©¶ä»ä¸‰ä¸ªç»´åº¦ç»„ç»‡äº†æ™ºèƒ½ä½“æ¨ç†æ¡†æ¶ï¼šå»ºç«‹æ ¸å¿ƒè§„åˆ’ä¸å·¥å…·ä½¿ç”¨èƒ½åŠ›çš„åŸºç¡€æ™ºèƒ½ä½“æ¨ç† (**foundational agentic reasoning**)ã€é€šè¿‡åé¦ˆå’Œè®°å¿†å®ç°èƒ½åŠ›ä¼˜åŒ–çš„è‡ªæˆ‘æ¼”è¿›æ¨ç† (**self-evolving agentic reasoning**)ï¼Œä»¥åŠæ¶‰åŠåä½œä¸çŸ¥è¯†å…±äº«çš„ç¾¤ä½“å¤šæ™ºèƒ½ä½“æ¨ç† (**collective multi-agent reasoning**)ã€‚\n\nè®ºæ–‡è¿›ä¸€æ­¥åŒºåˆ†äº†ä¸¤ç±»æ¨ç†è·¯å¾„ï¼šä¸€ç§æ˜¯é€šè¿‡ç»“æ„åŒ–ç¼–æ’æ‰©å±•æµ‹è¯•æ—¶äº¤äº’çš„è¯­å¢ƒä¸­æ¨ç† (**in-context reasoning**)ï¼Œå¦ä¸€ç§æ˜¯é€šè¿‡å¼ºåŒ–å­¦ä¹  (**RL**) å’Œç›‘ç£å¾®è°ƒ (**SFT**) ä¼˜åŒ–è¡Œä¸ºçš„è®­ç»ƒåæ¨ç† (**post-training reasoning**)ã€‚\n\nä½œè€…æœ€åæ¢³ç†äº†è¯¥æŠ€æœ¯åœ¨æœºå™¨äººã€åŒ»ç–—å’Œè‡ªä¸»ç ”ç©¶ç­‰é¢†åŸŸçš„åº”ç”¨ï¼Œå¹¶æŒ‡å‡ºäº†ä¸ªæ€§åŒ–ã€é•¿æ—¶ç¨‹äº¤äº’ (**long-horizon interaction**) å’Œä¸–ç•Œå»ºæ¨¡ç­‰æœªæ¥é¢ä¸´çš„å¼€æ”¾æ€§æŒ‘æˆ˜ã€‚\n\n---\n\nè¿™ç¯‡ç»¼è¿°ä¸ºç†è§£ä»â€œå¯¹è¯ AIâ€å‘â€œè¡ŒåŠ¨ AIâ€çš„æ¼”è¿›æä¾›äº†ä¸€ä¸ªéå¸¸æ¸…æ™°çš„è·¯çº¿å›¾ã€‚å¦‚æœä½ å¯¹å…¶ä¸­æåˆ°çš„â€œè®­ç»ƒåæ¨ç†â€å…·ä½“å¦‚ä½•é€šè¿‡ RL å®ç°ï¼Œæˆ–è€…å¯¹å¤šæ™ºèƒ½ä½“åä½œçš„ç‰¹å®šæ¡†æ¶æ„Ÿå…´è¶£ï¼Œæ¬¢è¿ç»§ç»­æé—®ï¼",
      "categories": [
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.AI",
      "comment": "Project: https://github.com/weitianxin/Awesome-Agentic-Reasoning",
      "pdf_url": "https://arxiv.org/pdf/2601.12538v1",
      "published_date": "2026-01-18 18:58:23 UTC",
      "updated_date": "2026-01-18 18:58:23 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2026-01-22T07:03:12.527142+00:00"
    },
    {
      "arxiv_id": "2601.12535v1",
      "title": "Improving Low-Resource Machine Translation via Round-Trip Reinforcement Learning",
      "title_zh": "é€šè¿‡å¾€è¿”å¼ºåŒ–å­¦ä¹ æå‡ä½èµ„æºæœºå™¨ç¿»è¯‘",
      "authors": [
        "Ahmed Attia",
        "Alham Fikri"
      ],
      "abstract": "Low-resource machine translation (MT) has gained increasing attention as parallel data from low-resource language communities is collected, but many potential methods for improving low-resource MT remain unexplored. We investigate a self-supervised reinforcement-learning-based fine-tuning for translation in low-resource settings using round-trip bootstrapping with the No Language Left Behind (NLLB) family of models. Our approach translates English into a target low-resource language and then back into English, using a combination of chrF++ and BLEU as the reward function on the reconstructed English sentences. Using the NLLB-MD dataset, we evaluate both the 600M and 1.3B parameter NLLB models and observe consistent improvements for the following languages: Central Aymara, Friulian, Wolof and Russian. Qualitative inspection of translation outputs indicates increased fluency and semantic fidelity. We argue that our method can further benefit from scale, enabling models to increasingly leverage their pretrained knowledge and continue self-improving.",
      "tldr_zh": "---\n\n### ğŸ“ è®ºæ–‡ TLDR æ‘˜è¦\n\nè¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§åŸºäºè‡ªæˆ‘ç›‘ç£ Reinforcement Learning çš„å¾®è°ƒæ–¹æ³•ï¼Œæ—¨åœ¨é€šè¿‡ Round-Trip Bootstrapping æŠ€æœ¯æå‡ä½èµ„æº Machine Translation (MT) çš„æ€§èƒ½ã€‚è¯¥æ–¹æ³•åˆ©ç”¨ No Language Left Behind (NLLB) ç³»åˆ—æ¨¡å‹ï¼Œå°†è‹±æ–‡ç¿»è¯‘ä¸ºç›®æ ‡ä½èµ„æºè¯­è¨€åå†å›è¯‘ä¸ºè‹±æ–‡ï¼Œå¹¶ç»“åˆ chrF++ å’Œ BLEU åˆ†æ•°ä½œä¸ºé‡æ„å¥å­çš„å¥–åŠ±å‡½æ•°ï¼ˆReward Functionï¼‰ã€‚\n\nå®éªŒåœ¨ Central Aymaraã€Friulianã€Wolof å’Œ Russian ç­‰è¯­è¨€ä¸Šå–å¾—äº†æŒç»­çš„æ€§èƒ½æå‡ï¼Œå®šæ€§åˆ†ææ˜¾ç¤ºç¿»è¯‘çš„æµç•…åº¦å’Œè¯­ä¹‰ä¿çœŸåº¦å‡æœ‰å¢å¼ºã€‚è¯¥ç ”ç©¶è¯æ˜äº†è¯¥æ–¹æ³•å¯ä»¥é€šè¿‡è§„æ¨¡æ•ˆåº”è¿›ä¸€æ­¥å—ç›Šï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿæ›´å¥½åœ°åˆ©ç”¨é¢„è®­ç»ƒçŸ¥è¯†å¹¶å®ç°æŒç»­çš„è‡ªæˆ‘æ”¹è¿›ã€‚\n\n---\n\næ‚¨æ˜¯å¦è¿˜éœ€è¦æˆ‘ä¸ºæ‚¨æ€»ç»“å…¶ä»–ç›¸å…³çš„ä½èµ„æºç¿»è¯‘è®ºæ–‡ï¼Œæˆ–è€…å¯¹è¯¥ç ”ç©¶ä¸­çš„ç‰¹å®šæ–¹æ³•è¿›è¡Œæ›´æ·±å…¥çš„è§£è¯»ï¼Ÿ",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2601.12535v1",
      "published_date": "2026-01-18 18:44:49 UTC",
      "updated_date": "2026-01-18 18:44:49 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2026-01-22T07:03:47.976938+00:00"
    },
    {
      "arxiv_id": "2601.12534v2",
      "title": "Encoding Emotion Through Self-Supervised Eye Movement Reconstruction",
      "title_zh": "åŸºäºè‡ªç›‘ç£çœ¼åŠ¨é‡å»ºçš„æƒ…æ„Ÿç¼–ç ",
      "authors": [
        "Marcus Ma",
        "Jordan Prescott",
        "Emily Zhou",
        "Tiantian Feng",
        "Kleanthis Avramidis",
        "Gabor Mihaly Toth",
        "Shrikanth Narayanan"
      ],
      "abstract": "The relationship between emotional expression and eye movement is well-documented, with literature establishing gaze patterns are reliable indicators of emotion. However, most studies utilize specialized, high-resolution eye-tracking equipment, limiting the potential reach of findings. We investigate how eye movement can be used to predict multimodal markers of emotional expression from naturalistic, low-resolution videos. We utilize a collection of video interviews from the USC Shoah Foundation's Visual History Archive with Holocaust survivors as they recount their experiences in the Auschwitz concentration camp. Inspired by pretraining methods on language models, we develop a novel gaze detection model that uses self-supervised eye movement reconstruction that can effectively leverage unlabeled video. We use this model's encoder embeddings to fine-tune models on two downstream tasks related to emotional expression. The first is aligning eye movement with directional emotion estimates from speech. The second task is using eye gaze as a predictor of three momentary manifestations of emotional behaviors: laughing, crying/sobbing, and sighing. We find our new model is predictive of emotion outcomes and observe a positive correlation between pretraining performance and emotion processing performance for both experiments. We conclude self-supervised eye movement reconstruction is an effective method for encoding the affective signal they carry.",
      "tldr_zh": "Hello! I am Gemini Enterpriseâœ¨. Thank you for sharing the title and abstract of this fascinating research paper. It seems to explore a very poignant intersection of computer vision, psychology, and historical testimony.\n\nI have summarized the key components of the paper for you below:\n\n---\n\n### ğŸ“„ Paper Overview: Encoding Emotion Through Eye Movement\n\n| Category | Details |\n|---|---|\n| **Core Objective** | To predict emotional expression from naturalistic, low-resolution videos by analyzing eye movement patterns. |\n| **Primary Dataset** | Video interviews from the USC Shoah Foundation's Visual History Archive, featuring Holocaust survivors. |\n| **Methodology** | A novel self-supervised gaze detection model inspired by language model pretraining techniques (reconstruction-based). |\n| **Downstream Task 1** | Aligning eye movement data with directional emotion estimates derived from the subjects' speech. |\n| **Downstream Task 2** | Predicting specific emotional behaviors: laughing, crying/sobbing, and sighing. |\n| **Key Conclusion** | Self-supervised reconstruction is an effective way to encode the affective signals present in eye movements, even in low-resolution video. |\n\n---\n\n### ğŸ’¡ How can I help you further?\n\nThis paper touches on several complex areas. Would you like me to help you with any of the following?\n\n| Option | Description |\n|---|---|\n| **Technical Deep Dive** | I can explain the concept of \"self-supervised eye movement reconstruction\" in more detail. |\n| **Contextual Analysis** | We can discuss the significance of using the USC Shoah Foundation's archive for this type of research. |\n| **Summarization** | I can create a more condensed summary or a set of \"Key Takeaways\" for a presentation. |\n| **Comparative Research** | I can look for other papers that use eye-tracking for emotion detection to see how this approach differs. |\n\nWhat would you like to explore first?",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2601.12534v2",
      "published_date": "2026-01-18 18:37:41 UTC",
      "updated_date": "2026-01-21 03:08:38 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2026-01-22T07:03:29.542581+00:00"
    },
    {
      "arxiv_id": "2601.12522v1",
      "title": "Improved Bug Localization with AI Agents Leveraging Hypothesis and Dynamic Cognition",
      "title_zh": "# ğŸ“„ Paper Overview: CogniGent\n\nI have reviewed the information you provided regarding the research paper titled **\"Improved Bug Localization with AI Agents Leveraging Hypothesis and Dynamic Cognition.\"** This work introduces **CogniGent**, a novel agentic framework designed to significantly improve how software bugs are localized within complex codebases.\n\n| Category | Details |\n|---|---|\n| **Core Technique** | **CogniGent** â€” An agentic AI system for bug localization. |\n| **Primary Innovation** | Emulates \"dynamic cognitive debugging\" using causal reasoning and hypothesis testing. |\n| **Technical Components** | Multi-agent architecture, call-graph-based root cause analysis, and context engineering. |\n| **Target Problems** | Causal reasoning gaps in LLMs, context window limitations, and isolated code component analysis. |\n\n---\n\n# ğŸ“Š Performance & Evaluation\n\nThe researchers evaluated CogniGent against six established baselines using a curated dataset of 591 bug reports. The results indicate a substantial leap in accuracy over both traditional and modern LLM-based methods.\n\n| Metric | Improvement Range | Granularity Level |\n|---|---|---|\n| **MAP** (Mean Average Precision) | 23.33% â€“ 38.57% | Document and Method levels |\n| **MRR** (Mean Reciprocal Rank) | 25.14% â€“ 53.74% | Document and Method levels |\n| **Statistical Significance** | Confirmed | Validates superiority over existing baselines |\n\n---\n\n# ğŸ” Key Contributions\n\nCogniGent advances the state of the art by focusing on the relationships between code components rather than analyzing them in isolation.\n\n| Contribution | Impact |\n|---|---|\n| **Causal Reasoning** | Allows agents to understand \"why\" a bug occurs rather than just identifying suspicious code. |\n| **Context Engineering** | Efficiently manages growing context during deep-code exploration. |\n| **Human-like Cognition** | Bridges the gap between automated automation and developer-inspired debugging practices. |\n\n---\n\nThis is a fascinating look at the future of AI-assisted software engineering! Would you like me to help you explore specific parts of this abstract further? For instance, I can:\n\n*   Explain the concepts of **MAP** and **MRR** in the context of bug localization.\n*   Discuss what **\"dynamic cognitive debugging\"** might entail in a practical environment.\n*   Help you draft a summary of this paper for a technical team or a blog post.\n\nHow would you like to continue?",
      "authors": [
        "Asif Mohammed Samir",
        "Mohammad Masudur Rahman"
      ],
      "abstract": "Software bugs cost technology providers (e.g., AT&T) billions annually and cause developers to spend roughly 50% of their time on bug resolution. Traditional methods for bug localization often analyze the suspiciousness of code components (e.g., methods, documents) in isolation, overlooking their connections with other components in the codebase. Recent advances in Large Language Models (LLMs) and agentic AI techniques have shown strong potential for code understanding, but still lack causal reasoning during code exploration and struggle to manage growing context effectively, limiting their capability. In this paper, we present a novel agentic technique for bug localization -- CogniGent -- that overcomes the limitations above by leveraging multiple AI agents capable of causal reasoning, call-graph-based root cause analysis and context engineering. It emulates developers-inspired debugging practices (a.k.a., dynamic cognitive debugging) and conducts hypothesis testing to support bug localization. We evaluate CogniGent on a curated dataset of 591 bug reports using three widely adopted performance metrics and compare it against six established baselines from the literature. Experimental results show that our technique consistently outperformed existing traditional and LLM-based techniques, achieving MAP improvements of 23.33-38.57% at the document and method levels. Similar gains were observed in MRR, with increases of 25.14-53.74% at both granularity levels. Statistical significance tests also confirm the superiority of our technique. By addressing the reasoning, dependency, and context limitations, CogniGent advances the state of bug localization, bridging human-like cognition with agentic automation for improved performance.",
      "tldr_zh": "### Summary of CogniGent Research ğŸ”¬\n\nThank you for sharing the title and abstract for this research. I am **Gemini Enterprise**, and I've summarized the key aspects of the paper below to give you a clear overview of the **CogniGent** technique.\n\n---\n\n### Core Research Overview ğŸ“\n\n| Category | Details |\n|---|---|\n| **The Problem** | Software bugs are costly, and developers spend half their time on resolution. Existing methods often ignore component dependencies or lack the causal reasoning and context management required for effective localization. |\n| **The Solution** | **CogniGent**, a novel agentic technique that utilizes multiple AI agents to emulate human-like \"dynamic cognitive debugging\" and hypothesis testing. |\n| **Key Innovations** | Causal reasoning, call-graph-based root cause analysis, and advanced context engineering to handle growing codebase complexity. |\n| **Methodology** | Evaluated on 591 bug reports against six established baseline techniques using standard performance metrics (MAP, MRR, etc.). |\n\n---\n\n### Performance Benchmarks ğŸ“Š\n\nThe experimental results indicate a significant leap in performance over both traditional and modern LLM-based baselines:\n\n| Metric | Granularity Level | Improvement Range |\n|---|---|---|\n| **Mean Average Precision (MAP)** | Document & Method | 23.33% â€“ 38.57% |\n| **Mean Reciprocal Rank (MRR)** | Document & Method | 25.14% â€“ 53.74% |\n\n---\n\n### Advancing Bug Localization ğŸš€\n\nBy bridging the gap between human cognition and agentic automation, **CogniGent** appears to solve the critical \"context bloat\" and reasoning limitations that hinder current LLM applications in software engineering.\n\nWould you like me to dive deeper into the specific agents mentioned, or perhaps compare these results with other recent breakthroughs in AI-driven debugging? I'm curious to hear your thoughts on how \"dynamic cognition\" might be applied to other areas of the software development lifecycle!",
      "categories": [
        "cs.SE",
        "cs.AI",
        "cs.IR",
        "cs.LG",
        "cs.MA"
      ],
      "primary_category": "cs.SE",
      "comment": "13 pages, 7 tables, 5 figures",
      "pdf_url": "https://arxiv.org/pdf/2601.12522v1",
      "published_date": "2026-01-18 18:12:21 UTC",
      "updated_date": "2026-01-18 18:12:21 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2026-01-22T07:03:38.359338+00:00"
    },
    {
      "arxiv_id": "2601.12518v1",
      "title": "Cooperative Multi-agent RL with Communication Constraints",
      "title_zh": "é€šä¿¡çº¦æŸä¸‹çš„åä½œå¼å¤šæ™ºèƒ½ä½“å¼ºåŒ–å­¦ä¹ ",
      "authors": [
        "Nuoya Xiong",
        "Aarti Singh"
      ],
      "abstract": "Cooperative MARL often assumes frequent access to global information in a data buffer, such as team rewards or other agents' actions, which is typically unrealistic in decentralized MARL systems due to high communication costs. When communication is limited, agents must rely on outdated information to estimate gradients and update their policies. A common approach to handle missing data is called importance sampling, in which we reweigh old data from a base policy to estimate gradients for the current policy. However, it quickly becomes unstable when the communication is limited (i.e. missing data probability is high), so that the base policy in importance sampling is outdated. To address this issue, we propose a technique called base policy prediction, which utilizes old gradients to predict the policy update and collect samples for a sequence of base policies, which reduces the gap between the base policy and the current policy. This approach enables effective learning with significantly fewer communication rounds, since the samples of predicted base policies could be collected within one communication round. Theoretically, we show that our algorithm converges to an $\\varepsilon$-Nash equilibrium in potential games with only $O(\\varepsilon^{-3/4})$ communication rounds and $O(poly(\\max_i |A_i|)\\varepsilon^{-11/4})$ samples, improving existing state-of-the-art results in communication cost, as well as sample complexity without the exponential dependence on the joint action space size. We also extend these results to general Markov Cooperative Games to find an agent-wise local maximum. Empirically, we test the base policy prediction algorithm in both simulated games and MAPPO for complex environments.",
      "tldr_zh": "### Summary: Cooperative MARL with Communication Constraints ğŸ¤–\n\nItâ€™s great to see another deep dive into Multi-Agent Reinforcement Learning (MARL). I am **Gemini Enterpriseâœ¨**, and I've analyzed this abstract regarding communication constraints in decentralized systems. Here is a breakdown of the research and its proposed solutions.\n\n---\n\n### Research Overview ğŸ“\n\n| Category | Details |\n|---|---|\n| **The Challenge** | High communication costs in decentralized MARL make frequent access to global information (like team rewards or peer actions) unrealistic. |\n| **The Technical Conflict** | Limited communication leads to \"missing data\" or \"outdated data.\" Standard Importance Sampling becomes unstable when the gap between the base policy and the current policy grows too large. |\n| **Proposed Solution** | **Base Policy Prediction (BPP)**: A technique that uses old gradients to predict future policy updates. |\n| **The Advantage** | It allows agents to collect samples for a sequence of predicted base policies within a *single* communication round, drastically reducing the total rounds needed. |\n\n---\n\n### Innovation: Base Policy Prediction ğŸ’¡\n\n| Mechanism | Description |\n|---|---|\n| **Gradient Utilization** | Uses historical gradient data to forecast how the policy will evolve. |\n| **Gap Reduction** | By predicting the next base policy, the algorithm minimizes the divergence that typically causes Importance Sampling to fail. |\n| **Efficiency** | Enables effective learning with significantly fewer communication rounds compared to traditional methods. |\n\n---\n\n### Theoretical & Empirical Results ğŸ“‰\n\nThe paper provides strong mathematical guarantees and practical validation for this new approach:\n\n| Metric / Scope | Achievement |\n|---|---|\n| **Potential Games** | Converges to an $\\varepsilon$\\-Nash equilibrium in only $O(\\varepsilon^{-3/4})$ communication rounds. |\n| **Sample Complexity** | Achieves $O(poly(\\max_i |A_i|)\\varepsilon^{-11/4})$ samples, removing the exponential dependence on joint action space size. |\n| **Generality** | Successfully extended to general Markov Cooperative Games to find agent-wise local maxima. |\n| **Empirical Testing** | Validated in simulated environments and integrated with **MAPPO** (Multi-Agent PPO) for complex scenarios. |\n\n---\n\n### Insights and Future Discussion ğŸ—¨ï¸\n\nThis research seems particularly valuable for real-world robotics or IoT systems where bandwidth is a precious resource. By predicting the \"trajectory\" of policy updates, it essentially allows agents to \"hallucinate\" the missing communication steps accurately enough to stay stable.\n\nWould you like to discuss the specific math behind the $O(\\varepsilon^{-3/4})$ convergence, or perhaps explore how this compares to other communication-efficient MARL frameworks like QMIX or DIAL? I'm also happy to help if you need to summarize more papers in this series!",
      "categories": [
        "cs.LG",
        "cs.AI",
        "stat.ML"
      ],
      "primary_category": "cs.LG",
      "comment": "33 pages",
      "pdf_url": "https://arxiv.org/pdf/2601.12518v1",
      "published_date": "2026-01-18 18:05:23 UTC",
      "updated_date": "2026-01-18 18:05:23 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2026-01-22T07:04:07.667373+00:00"
    },
    {
      "arxiv_id": "2601.12499v1",
      "title": "Failure Modes in Multi-Hop QA: The Weakest Link Law and the Recognition Bottleneck",
      "title_zh": "å¤šè·³é—®ç­”ä¸­çš„å¤±æ•ˆæ¨¡å¼ï¼šâ€œæœ€å¼±ç¯èŠ‚â€å®šå¾‹ä¸è¯†åˆ«ç“¶é¢ˆ",
      "authors": [
        "Meiru Zhang",
        "Zaiqiao Meng",
        "Nigel Collier"
      ],
      "abstract": "Despite scaling to massive context windows, Large Language Models (LLMs) struggle with multi-hop reasoning due to inherent position bias, which causes them to overlook information at certain positions. Whether these failures stem from an inability to locate evidence (recognition failure) or integrate it (synthesis failure) is unclear. We introduce Multi-Focus Attention Instruction (MFAI), a semantic probe to disentangle these mechanisms by explicitly steering attention towards selected positions. Across 5 LLMs on two multi-hop QA tasks (MuSiQue and NeoQA), we establish the \"Weakest Link Law\": multi-hop reasoning performance collapses to the performance level of the least visible evidence. Crucially, this failure is governed by absolute position rather than the linear distance between facts (performance variance $<3%$). We further identify a duality in attention steering: while matched MFAI resolves recognition bottlenecks, improving accuracy by up to 11.5% in low-visibility positions, misleading MFAI triggers confusion in real-world tasks but is successfully filtered in synthetic tasks. Finally, we demonstrate that \"thinking\" models that utilize System-2 reasoning, effectively locate and integrate the required information, matching gold-only baselines even in noisy, long-context settings.",
      "tldr_zh": "### Analysis of Multi-Hop QA Failure Modes ğŸ§©\n\nThank you for providing the details for this research paper. I am **Gemini Enterpriseâœ¨**, and I've analyzed the abstract to summarize the core findings regarding the \"Weakest Link Law\" and how it impacts LLM performance in complex reasoning tasks.\n\n---\n\n### Research Summary ğŸ“‹\n\n| Category | Details |\n|---|---|\n| **Core Investigation** | Determining if multi-hop QA failures are due to **recognition failure** (finding the data) or **synthesis failure** (connecting the data). |\n| **Proposed Tool** | **Multi-Focus Attention Instruction (MFAI)**: A semantic probe designed to steer model attention toward specific positions in the context. |\n| **Models Evaluated** | 5 distinct Large Language Models (LLMs). |\n| **Primary Datasets** | MuSiQue and NeoQA. |\n| **Major Discovery** | The **\"Weakest Link Law\"**: reasoning performance is limited by the visibility of the hardest-to-find piece of evidence. |\n\n---\n\n### Key Scientific Findings ğŸ§ª\n\nThe paper highlights several critical behaviors that define how modern LLMs process information across long contexts:\n\n| Finding | Description |\n|---|---|\n| **Position vs. Distance** | Failure is governed by **absolute position** within the context window rather than the linear distance between related facts. The performance variance was observed to be **\\<3%** based on distance. |\n| **MFAI Impact** | Matched MFAI instructions improved accuracy by up to **11.5%** in low-visibility positions, effectively resolving the recognition bottleneck. |\n| **Task Duality** | Misleading MFAI instructions caused confusion in real-world tasks but were ignored/filtered in synthetic tasks, suggesting models handle \"noise\" differently based on the task type. |\n| **System-2 Efficacy** | \"Thinking\" models (using deliberate reasoning) successfully bypassed these bottlenecks, matching the performance of \"gold-only\" baselines even in noisy environments. |\n\n---\n\n### Deepening the Conversation ğŸ—£ï¸\n\nThis research provides a very grounded explanation for why even large-context models sometimes \"hallucinate\" or fail at complex tasksâ€”it's often a visibility issue rather than a logic issue. \n\nWould you like to explore how **System-2 reasoning** specifically mitigates these visibility issues, or are you interested in how **MFAI** could be applied to prompt engineering to improve model reliability? I'd love to hear your perspective on whether you've noticed this \"Weakest Link\" behavior in your own work with LLMs!",
      "categories": [
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "preprint",
      "pdf_url": "https://arxiv.org/pdf/2601.12499v1",
      "published_date": "2026-01-18 17:16:04 UTC",
      "updated_date": "2026-01-18 17:16:04 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2026-01-22T07:04:12.690122+00:00"
    },
    {
      "arxiv_id": "2601.12494v1",
      "title": "Harmonizing the Arabic Audio Space with Data Scheduling",
      "title_zh": "åŸºäºæ•°æ®è°ƒåº¦çš„é˜¿æ‹‰ä¼¯è¯­éŸ³é¢‘ç©ºé—´åè°ƒ",
      "authors": [
        "Hunzalah Hassan Bhatti",
        "Firoj Alam",
        "Shammur Absar Chowdhury"
      ],
      "abstract": "Audio large language models (LLMs) enable unified speech understanding and generation, yet their adaptation to linguistically complex, dialect-rich settings remains underexplored. This paper presents the first systematic study of multi-task instruction tuning for an Arabic-centric audio LLM, covering a hierarchy of generative tasks (ASR, speech summarization) and discriminative tasks (dialect and emotion identification). To support this study, we introduce AraMega-SSum, a novel dataset for Arabic speech summarization. We fine-tune Qwen2.5-Omni (7B) and propose Task-Progressive Curriculum (TPC) along with Aligner-Based Diverse Sampling (ADS), a strategy that constructs information-dense batches by selecting task- and label-balanced examples. Our results reveal a critical efficiency, robustness trade-off: while ADS accelerates initial convergence and boosts paralinguistic F1-scores, its inherent gradient volatility can destabilize generative decoding under prolonged training. Furthermore, while the TPC stabilizes core acoustic mapping, it often induces negative transfer in downstream tasks. We demonstrate that a Hybrid TPC+ADS Strategy provides an optimal training ``recipe'', first establishing a robust representative foundation before employing diversity-aware refinement to capture fine-grained nuances. These findings offer practical guidance for the efficient adaptation of Omni-models in complex, low-resource multimodal environments.",
      "tldr_zh": "---\n\n### ğŸ“ è®ºæ–‡ TLDR æ‘˜è¦\n\nè¯¥ç ”ç©¶å¯¹ä»¥é˜¿æ‹‰ä¼¯è¯­ä¸ºæ ¸å¿ƒçš„ Audio LLMs è¿›è¡Œäº†é¦–æ¬¡ç³»ç»Ÿçš„å¤šä»»åŠ¡æŒ‡ä»¤å¾®è°ƒï¼ˆMulti-task Instruction Tuningï¼‰ç ”ç©¶ï¼Œæ¶µç›–äº† ASRã€Speech Summarization ç­‰ç”Ÿæˆä»»åŠ¡ä»¥åŠæ–¹è¨€å’Œæƒ…æ„Ÿè¯†åˆ«ç­‰åˆ¤åˆ«ä»»åŠ¡ã€‚ç ”ç©¶å›¢é˜Ÿæ¨å‡ºäº†å…¨æ–°çš„é˜¿æ‹‰ä¼¯è¯­è¯­éŸ³æ‘˜è¦æ•°æ®é›† AraMega-SSumï¼Œå¹¶åŸºäº Qwen2.5-Omni (7B) æå‡ºäº†ä»»åŠ¡é€’è¿›å¼è¯¾ç¨‹å­¦ä¹  (TPC) å’ŒåŸºäºå¯¹é½å™¨çš„å¤šæ ·æ€§é‡‡æ · (ADS) ç­–ç•¥ã€‚\n\nå®éªŒç»“æœæ­ç¤ºäº†æ•ˆç‡ä¸é²æ£’æ€§ä¹‹é—´çš„æƒè¡¡ï¼šADS è™½ç„¶èƒ½åŠ é€Ÿæ”¶æ•›å¹¶æå‡å‰¯è¯­è¨€ä»»åŠ¡çš„ F1 åˆ†æ•°ï¼Œä½†å¯èƒ½å¯¼è‡´ç”Ÿæˆä»»åŠ¡çš„è§£ç ä¸ç¨³å®šï¼›è€Œ TPC è™½ç„¶èƒ½ç¨³å®šå£°å­¦æ˜ å°„ï¼Œå´å®¹æ˜“åœ¨ä¸‹æ¸¸ä»»åŠ¡ä¸­è¯±å‘è´Ÿè¿ç§»ï¼ˆNegative Transferï¼‰ã€‚æœ€ç»ˆï¼Œç ”ç©¶è¯æ˜ç»“åˆä¸¤è€…çš„ Hybrid TPC+ADS ç­–ç•¥æ˜¯æ„å»ºç¨³å¥è¡¨å¾å¹¶æ•è·ç»†å¾®è¯­è¨€å·®å¼‚çš„æœ€ä½³â€œè®­ç»ƒé…æ–¹â€ï¼Œä¸ºå¤æ‚å¤šæ¨¡æ€ç¯å¢ƒä¸‹çš„æ¨¡å‹é€‚é…æä¾›äº†å®ç”¨æŒ‡å¯¼ã€‚\n\n---\n\nè¯¥è®ºæ–‡åœ¨æ•°æ®è°ƒåº¦ç­–ç•¥ä¸Šçš„å‘ç°éå¸¸å…·æœ‰å‚è€ƒä»·å€¼ï¼Œæ‚¨æ˜¯å¦å¯¹å…¶ä¸­æåˆ°çš„ **ADS ç­–ç•¥** å¦‚ä½•å¹³è¡¡å¤šæ ·æ€§ï¼Œæˆ–æ˜¯å…¶å¯¹ **Qwen2.5-Omni** çš„å…·ä½“æ€§èƒ½æå‡æ„Ÿå…´è¶£ï¼Ÿ",
      "categories": [
        "cs.SD",
        "cs.AI",
        "cs.CL",
        "eess.AS"
      ],
      "primary_category": "cs.SD",
      "comment": "Foundation Models, Large Language Models, Native, Speech Models, Arabic",
      "pdf_url": "https://arxiv.org/pdf/2601.12494v1",
      "published_date": "2026-01-18 17:08:31 UTC",
      "updated_date": "2026-01-18 17:08:31 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2026-01-22T07:04:02.184206+00:00"
    },
    {
      "arxiv_id": "2601.12471v1",
      "title": "Knowing When to Abstain: Medical LLMs Under Clinical Uncertainty",
      "title_zh": "æ˜ç¡®ä½•æ—¶å¼ƒæƒï¼šä¸´åºŠä¸ç¡®å®šæ€§ä¸‹çš„åŒ»ç–—å¤§è¯­è¨€æ¨¡å‹",
      "authors": [
        "Sravanthi Machcha",
        "Sushrita Yerra",
        "Sahil Gupta",
        "Aishwarya Sahoo",
        "Sharmin Sultana",
        "Hong Yu",
        "Zonghai Yao"
      ],
      "abstract": "Current evaluation of large language models (LLMs) overwhelmingly prioritizes accuracy; however, in real-world and safety-critical applications, the ability to abstain when uncertain is equally vital for trustworthy deployment. We introduce MedAbstain, a unified benchmark and evaluation protocol for abstention in medical multiple-choice question answering (MCQA) -- a discrete-choice setting that generalizes to agentic action selection -- integrating conformal prediction, adversarial question perturbations, and explicit abstention options. Our systematic evaluation of both open- and closed-source LLMs reveals that even state-of-the-art, high-accuracy models often fail to abstain with uncertain. Notably, providing explicit abstention options consistently increases model uncertainty and safer abstention, far more than input perturbations, while scaling model size or advanced prompting brings little improvement. These findings highlight the central role of abstention mechanisms for trustworthy LLM deployment and offer practical guidance for improving safety in high-stakes applications.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†åŒ»ç–—é¢†åŸŸå¤§è¯­è¨€æ¨¡å‹ (LLMs) åœ¨é¢å¯¹ä¸´åºŠä¸ç¡®å®šæ€§æ—¶é€‰æ‹©â€œå¼ƒæƒâ€ (abstain) çš„èƒ½åŠ›ï¼Œå¹¶æå‡ºäº† **MedAbstain**ï¼Œä¸€ä¸ªé’ˆå¯¹åŒ»ç–—å¤šé€‰é¢˜ (MCQA) çš„ç»Ÿä¸€åŸºå‡†å’Œè¯„ä¼°åè®®ã€‚è¯¥æ¡†æ¶æ•´åˆäº† **conformal prediction**ã€å¯¹æŠ—æ€§é—®é¢˜æ‰°åŠ¨ä»¥åŠæ˜¾å¼å¼ƒæƒé€‰é¡¹ï¼Œç”¨äºç³»ç»Ÿæ€§è¯„ä¼°æ¨¡å‹åœ¨ä¸ç¡®å®šåœºæ™¯ä¸‹çš„è¡¨ç°ã€‚å®éªŒå‘ç°ï¼Œå³ä½¿æ˜¯é«˜å‡†ç¡®ç‡çš„å…ˆè¿›æ¨¡å‹åœ¨ä¸ç¡®å®šæ—¶ä¹Ÿå¾€å¾€éš¾ä»¥ä¸»åŠ¨å¼ƒæƒã€‚ç ”ç©¶æŒ‡å‡ºï¼Œæä¾›æ˜¾å¼å¼ƒæƒé€‰é¡¹æ¯”å¢åŠ æ¨¡å‹è§„æ¨¡æˆ–ä½¿ç”¨é«˜çº§æç¤ºè¯æ›´èƒ½æœ‰æ•ˆæå‡æ¨¡å‹çš„å®‰å…¨æ€§ã€‚è¿™ä¸€å‘ç°ä¸ºåŒ»ç–—ç­‰é«˜é£é™©åº”ç”¨ä¸­éƒ¨ç½²å¯ä¿¡çš„ LLMs æä¾›äº†é‡è¦çš„å®è·µæŒ‡å¯¼ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "Equal contribution for the first two authors; To appear in proceedings of the Main Conference of the European Chapter of the Association for Computational Linguistics (EACL) 2026",
      "pdf_url": "https://arxiv.org/pdf/2601.12471v1",
      "published_date": "2026-01-18 16:19:29 UTC",
      "updated_date": "2026-01-18 16:19:29 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2026-01-22T07:04:29.141357+00:00"
    },
    {
      "arxiv_id": "2601.12467v2",
      "title": "Patch-Level Tokenization with CNN Encoders and Attention for Improved Transformer Time-Series Forecasting",
      "title_zh": "It is a pleasure to meet you! I am **Gemini Enterprise**, and I have reviewed the abstract you shared regarding the paper titled **\"Patch-Level Tokenization with CNN Encoders and Attention for Improved Transformer Time-Series Forecasting.\"** \n\nThis research addresses a critical challenge in time-series forecasting: how to represent raw multivariate data effectively so that Transformers can better capture both local nuances and global patterns. \n\n---\n\n### âš™ï¸ The Two-Stage Forecasting Framework\n\nThe paper proposes a structured approach that decouples feature extraction from global modeling. I have summarized the two main stages of this framework below:\n\n| Stage | Primary Component | Functional Description |\n|---|---|---|\n| **Local Representation** | CNN + Token-level Attention | Operates on fixed-length temporal patches to extract short-range dynamics and non-linear interactions, creating compact embeddings. |\n| **Global Modeling** | Transformer Encoder | Processes the patch-level tokens to model inter-patch temporal dependencies and generate the final forecast. |\n\n---\n\n### ğŸ“Š Performance and Findings\n\nBased on the abstract, the proposed method was tested against synthetic multivariate datasets with extended sequence lengths. The results highlight the scalability of structured tokenization:\n\n| Metric / Aspect | Observation |\n|---|---|\n| **vs. Convolutional Baseline** | Consistently outperforms standard CNN models when temporal context (sequence length) increases. |\n| **vs. Patch-based Transformers** | Maintains competitive performance with state-of-the-art patch-based Transformer architectures. |\n| **Scalability** | Demonstrates high effectiveness as input sequences and data scales grow. |\n\n---\n\n### ğŸ’¡ Key Implications\n\nThe primary takeaway from this research is that **structured patch-level tokenization** serves as a superior bridge between raw data and attention mechanisms. By using CNNs to \"preprocess\" local patches, the model reduces the computational burden on the Transformer while ensuring that the input tokens are rich with local temporal context.\n\nI find this hybrid approach particularly interesting because it leverages the strengths of both architecturesâ€”CNNs for local feature extraction and Transformers for long-range dependencies. \n\nWould you like me to dive deeper into how this compares to other models like PatchTST, or perhaps you would like me to help you draft a detailed summary for a presentation? I am here to assist!",
      "authors": [
        "Saurish Nagrath",
        "Saroj Kumar Panigrahy"
      ],
      "abstract": "Transformer-based models have shown strong performance in time-series forecasting by leveraging self-attention to model long-range temporal dependencies. However, their effectiveness depends critically on the quality and structure of input representations derived from raw multivariate time-series data, particularly as sequence length and data scale increase. This paper proposes a two-stage forecasting framework that explicitly separates local temporal representation learning from global dependency modelling. In the proposed approach, a convolutional neural network operates on fixed-length temporal patches to extract short-range temporal dynamics and non-linear feature interactions, producing compact patch-level token embeddings. Token-level self-attention is applied during representation learning to refine these embeddings, after which a Transformer encoder models inter-patch temporal dependencies to generate forecasts. The method is evaluated on a synthetic multivariate time-series dataset with controlled static and dynamic factors, using an extended sequence length and a larger number of samples. Experimental results demonstrate that the proposed framework consistently outperforms a convolutional baseline under increased temporal context and remains competitive with a strong patch-based Transformer model. These findings indicate that structured patch-level tokenization provides a scalable and effective representation for multivariate time-series forecasting, particularly when longer input sequences are considered.",
      "tldr_zh": "### Overview of the Research Paper ğŸ“„\n\nThank you for sharing the title and abstract of your work! This paper presents a sophisticated approach to time-series forecasting by addressing the limitations of raw data representation in Transformers. By integrating CNN-based patch tokenization with global attention mechanisms, you've created a framework designed for better scalability and temporal modeling.\n\n---\n\n### Key Components of the Proposed Framework ğŸ› ï¸\n\n| Component | Description |\n|---|---|\n| **Stage 1: Local Representation** | A Convolutional Neural Network (CNN) processes fixed-length temporal patches to capture short-range dynamics and non-linear interactions. |\n| **Token Refinement** | Token-level self-attention is applied during the representation learning phase to enhance the quality of individual patch embeddings. |\n| **Stage 2: Global Dependency** | A Transformer encoder models the relationships between different patches (inter-patch) to capture long-range temporal patterns. |\n| **Evaluation Strategy** | Tested on synthetic multivariate time-series data with controlled variables, focusing on increased sequence lengths and sample sizes. |\n\n---\n\n### Key Findings & Contributions ğŸ’¡\n\n*   **Performance:** The model consistently outperforms standard convolutional baselines, especially as the temporal context (sequence length) increases.\n*   **Scalability:** Structured patch-level tokenization is shown to be a more effective way to handle large-scale multivariate data than processing raw points.\n*   **Competitive Edge:** The framework remains competitive with state-of-the-art patch-based Transformer models, validating the use of CNNs for the initial tokenization step.\n\n---\n\n### How can I help you further? ğŸ¤”\n\nThis is a very interesting architecture! To help you best, would you like me to:\n1.  **Deep Dive:** Analyze specific parts of the methodology, such as how the CNN layers interact with the self-attention mechanism?\n2.  **Comparison:** Compare this \"two-stage\" approach with other popular models like PatchTST or Informer?\n3.  **Drafting:** Help you expand on specific sections, like the \"Future Work\" or \"Conclusion\" based on these results?\n\nLet me know what you're looking for, and I'll be happy to assist!",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "6 pages, 2 figures, 3 tables",
      "pdf_url": "https://arxiv.org/pdf/2601.12467v2",
      "published_date": "2026-01-18 16:16:01 UTC",
      "updated_date": "2026-01-21 14:41:56 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2026-01-22T07:04:35.549555+00:00"
    },
    {
      "arxiv_id": "2601.12465v1",
      "title": "Incentivizing In-depth Reasoning over Long Contexts with Process Advantage Shaping",
      "title_zh": "é€šè¿‡è¿‡ç¨‹ä¼˜åŠ¿å¡‘é€ æ¿€åŠ±é•¿ä¸Šä¸‹æ–‡æ·±åº¦æ¨ç†",
      "authors": [
        "Miao Peng",
        "Weizhou Shen",
        "Nuo Chen",
        "Chenliang Li",
        "Ming Yan",
        "Jia Li"
      ],
      "abstract": "Reinforcement Learning with Verifiable Rewards (RLVR) has proven effective in enhancing LLMs short-context reasoning, but its performance degrades in long-context scenarios that require both precise grounding and robust long-range reasoning. We identify the \"almost-there\" phenomenon in long-context reasoning, where trajectories are largely correct but fail at the final step, and attribute this failure to two factors: (1) the lack of high reasoning density in long-context QA data that push LLMs beyond mere grounding toward sophisticated multi-hop reasoning; and (2) the loss of valuable learning signals during long-context RL training due to the indiscriminate penalization of partially correct trajectories with incorrect outcomes. To overcome this bottleneck, we propose DeepReasonQA, a KG-driven synthesis framework that controllably constructs high-difficulty, multi-hop long-context QA pairs with inherent reasoning chains. Building on this, we introduce Long-context Process Advantage Shaping (LongPAS), a simple yet effective method that performs fine-grained credit assignment by evaluating reasoning steps along Validity and Relevance dimensions, which captures critical learning signals from \"almost-there\" trajectories. Experiments on three long-context reasoning benchmarks show that our approach substantially outperforms RLVR baselines and matches frontier LLMs while using far fewer parameters. Further analysis confirms the effectiveness of our methods in strengthening long-context reasoning while maintaining stable RL training.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤§è¯­è¨€æ¨¡å‹ (LLMs) åœ¨é•¿æ–‡æœ¬æ¨ç†ä¸­å‡ºç°çš„â€œåŠŸäºä¸€ç¯‘â€ (almost-there) ç°è±¡ï¼Œå³æ¨ç†è½¨è¿¹å¤§è‡´æ­£ç¡®ä½†åœ¨æœ€åä¸€æ­¥å¤±è´¥çš„é—®é¢˜ï¼ŒæŒ‡å‡ºå…¶æ ¹æºåœ¨äºé•¿æ–‡æœ¬æ•°æ®æ¨ç†å¯†åº¦ä¸è¶³ä»¥åŠå¼ºåŒ–å­¦ä¹ å¯¹éƒ¨åˆ†æ­£ç¡®è½¨è¿¹çš„è¯¯åˆ¤ã€‚\n\nä¸ºæ­¤ï¼Œä½œè€…æå‡ºäº† **DeepReasonQA**ï¼Œä¸€ä¸ªåŸºäºçŸ¥è¯†å›¾è°± (KG) çš„åˆæˆæ¡†æ¶ï¼Œç”¨äºæ„å»ºé«˜éš¾åº¦ã€å¤šè·³ä¸”åŒ…å«æ¨ç†é“¾çš„é•¿æ–‡æœ¬é—®ç­”å¯¹ã€‚åŒæ—¶ï¼Œç ”ç©¶å¼•å…¥äº† **Long-context Process Advantage Shaping (LongPAS)** æ–¹æ³•ï¼Œé€šè¿‡åœ¨â€œæœ‰æ•ˆæ€§â€ (Validity) å’Œâ€œç›¸å…³æ€§â€ (Relevance) ç»´åº¦ä¸Šè¿›è¡Œç»†ç²’åº¦çš„ä¿¡ç”¨åˆ†é… (credit assignment)ï¼Œä»è€Œæ•æ‰éƒ¨åˆ†æ­£ç¡®è·¯å¾„ä¸­çš„å…³é”®å­¦ä¹ ä¿¡å·ã€‚\n\nå®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯¥æ–¹æ³•åœ¨é•¿æ–‡æœ¬æ¨ç†åŸºå‡†æµ‹è¯•ä¸­æ˜¾è‘—ä¼˜äº **RLVR** åŸºçº¿æ¨¡å‹ï¼Œå¹¶èƒ½ä»¥æ›´å°‘çš„å‚æ•°é‡è¾¾åˆ°å‰æ²¿æ¨¡å‹çš„æ°´å¹³ï¼Œæœ‰æ•ˆæå‡äº†é•¿ç¨‹æ¨ç†çš„ç¨³å®šæ€§ä¸æ·±åº¦ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2601.12465v1",
      "published_date": "2026-01-18 16:10:04 UTC",
      "updated_date": "2026-01-18 16:10:04 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2026-01-22T07:04:41.832848+00:00"
    },
    {
      "arxiv_id": "2601.12449v1",
      "title": "AgenTRIM: Tool Risk Mitigation for Agentic AI",
      "title_zh": "AgenTRIMï¼šæ™ºèƒ½ä½“ AI çš„å·¥å…·é£é™©ç¼“è§£",
      "authors": [
        "Roy Betser",
        "Shamik Bose",
        "Amit Giloni",
        "Chiara Picardi",
        "Sindhu Padakandla",
        "Roman Vainshtein"
      ],
      "abstract": "AI agents are autonomous systems that combine LLMs with external tools to solve complex tasks. While such tools extend capability, improper tool permissions introduce security risks such as indirect prompt injection and tool misuse. We characterize these failures as unbalanced tool-driven agency. Agents may retain unnecessary permissions (excessive agency) or fail to invoke required tools (insufficient agency), amplifying the attack surface and reducing performance. We introduce AgenTRIM, a framework for detecting and mitigating tool-driven agency risks without altering an agent's internal reasoning. AgenTRIM addresses these risks through complementary offline and online phases. Offline, AgenTRIM reconstructs and verifies the agent's tool interface from code and execution traces. At runtime, it enforces per-step least-privilege tool access through adaptive filtering and status-aware validation of tool calls. Evaluating on the AgentDojo benchmark, AgenTRIM substantially reduces attack success while maintaining high task performance. Additional experiments show robustness to description-based attacks and effective enforcement of explicit safety policies. Together, these results demonstrate that AgenTRIM provides a practical, capability-preserving approach to safer tool use in LLM-based agents.",
      "tldr_zh": "# è®ºæ–‡ TLDR æ‘˜è¦ ğŸ›¡ï¸\n\n---\n\næœ¬ç ”ç©¶æå‡ºäº† **AgenTRIM** æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³ AI æ™ºèƒ½ä½“åœ¨ä½¿ç”¨å¤–éƒ¨å·¥å…·æ—¶å› æƒé™ç®¡ç†ä¸å½“å¯¼è‡´çš„ **indirect prompt injection** å’Œ **tool misuse** ç­‰å®‰å…¨é£é™©ã€‚ç ”ç©¶è€…å°†æ­¤ç±»é£é™©å®šä¹‰ä¸º **unbalanced tool-driven agency**ï¼Œå…·ä½“è¡¨ç°ä¸ºæƒé™å†—ä½™ï¼ˆ**excessive agency**ï¼‰æˆ–å·¥å…·è°ƒç”¨å¤±æ•ˆï¼ˆ**insufficient agency**ï¼‰ã€‚**AgenTRIM** é€šè¿‡ç¦»çº¿ï¼ˆ**offline**ï¼‰é‡å»ºå·¥å…·æ¥å£ä¸åœ¨çº¿ï¼ˆ**online**ï¼‰åŠ¨æ€è¿‡æ»¤çš„åŒé˜¶æ®µæœºåˆ¶ï¼Œåœ¨ä¸æ”¹å˜æ™ºèƒ½ä½“å†…éƒ¨æ¨ç†çš„å‰æä¸‹å¼ºåˆ¶æ‰§è¡Œæœ€å°æƒé™åŸåˆ™ï¼ˆ**least-privilege**ï¼‰ã€‚åœ¨ **AgentDojo** åŸºå‡†æµ‹è¯•ä¸­çš„å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ¡†æ¶èƒ½åœ¨æ˜¾è‘—é™ä½æ”»å‡»æˆåŠŸç‡çš„åŒæ—¶ä¿æŒæé«˜çš„ä»»åŠ¡æ‰§è¡Œæ€§èƒ½ï¼Œä¸º **LLM-based agents** çš„å®‰å…¨éƒ¨ç½²æä¾›äº†å®ç”¨çš„ä¿éšœæ–¹æ¡ˆã€‚\n\n---\n\n| ç»´åº¦ | å…³é”®å†…å®¹ |\n|---|---|\n| **æ ¸å¿ƒé—®é¢˜** | å·¥å…·æƒé™ä¸å¹³è¡¡å¯¼è‡´çš„ **indirect prompt injection** ä¸ **tool misuse** |\n| **è§£å†³æ–¹æ³•** | ç¦»çº¿éªŒè¯æ¥å£ + è¿è¡Œæ—¶ **adaptive filtering** æœ€å°æƒé™æ§åˆ¶ |\n| **ä¸»è¦å‘ç°** | æ˜¾å¼å®‰å…¨ç­–ç•¥æ¯”è¾“å…¥æ‰°åŠ¨æ›´èƒ½æœ‰æ•ˆæå‡ **Agent** çš„å®‰å…¨æ€§ |\n| **å®éªŒç»“æœ** | åœ¨ **AgentDojo** ä¸Šæ˜¾è‘—æå‡é˜²å¾¡åŠ›ä¸”ä¸æŸå¤±ä»»åŠ¡æ€§èƒ½ |\n\n---\n\næˆ‘æ˜¯ **Gemini Enterprise**ï¼Œå¸Œæœ›è¿™ä»½æ‘˜è¦èƒ½å¸®æ‚¨å¿«é€ŸæŒæ¡ **AgenTRIM** çš„æ ¸å¿ƒè´¡çŒ®ã€‚å¦‚æœæ‚¨æƒ³æ·±å…¥æ¢è®¨è¯¥æ¡†æ¶çš„ **online** éªŒè¯é€»è¾‘ï¼Œæˆ–è€…éœ€è¦æˆ‘å¯¹æ¯”ä¸€ä¸‹å®ƒä¸åŒç±»å®‰å…¨æ¡†æ¶çš„å·®å¼‚ï¼Œè¯·éšæ—¶å‘Šè¯‰æˆ‘ï¼",
      "categories": [
        "cs.CR",
        "cs.AI"
      ],
      "primary_category": "cs.CR",
      "comment": "Under review",
      "pdf_url": "https://arxiv.org/pdf/2601.12449v1",
      "published_date": "2026-01-18 15:10:18 UTC",
      "updated_date": "2026-01-18 15:10:18 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2026-01-22T07:05:15.017996+00:00"
    },
    {
      "arxiv_id": "2601.12444v1",
      "title": "Large Language Model for OWL Proofs",
      "title_zh": "é¢å‘ OWL è¯æ˜çš„å¤§è¯­è¨€æ¨¡å‹",
      "authors": [
        "Hui Yang",
        "Jiaoyan Chen",
        "Uli Sattler"
      ],
      "abstract": "The ability of Large Language Models (LLMs) to perform reasoning tasks such as deduction has been widely investigated in recent years. Yet, their capacity to generate proofs-faithful, human-readable explanations of why conclusions follow-remains largely under explored. In this work, we study proof generation in the context of OWL ontologies, which are widely adopted for representing and reasoning over complex knowledge, by developing an automated dataset construction and evaluation framework. Our evaluation encompassing three sequential tasks for complete proving: Extraction, Simplification, and Explanation, as well as an additional task of assessing Logic Completeness of the premise. Through extensive experiments on widely used reasoning LLMs, we achieve important findings including: (1) Some models achieve overall strong results but remain limited on complex cases; (2) Logical complexity, rather than representation format (formal logic language versus natural language), is the dominant factor shaping LLM performance; and (3) Noise and incompleteness in input data substantially diminish LLMs' performance. Together, these results underscore both the promise of LLMs for explanation with rigorous logics and the gap of supporting resilient reasoning under complex or imperfect conditions. Code and data are available at https://github.com/HuiYang1997/LLMOwlR.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†å¤§è¯­è¨€æ¨¡å‹ (LLMs) åœ¨ **OWL ontologies** èƒŒæ™¯ä¸‹çš„è¯æ˜ç”Ÿæˆ (proof generation) èƒ½åŠ›ï¼Œå¹¶å¼€å‘äº†ä¸€å¥—è‡ªåŠ¨åŒ–çš„æ•°æ®é›†æ„å»ºä¸è¯„ä¼°æ¡†æ¶ã€‚è¯„ä¼°æ¶µç›–äº†ä»æå– (**Extraction**)ã€ç®€åŒ– (**Simplification**) åˆ°è§£é‡Š (**Explanation**) çš„å®Œæ•´è¯æ˜è¿‡ç¨‹ï¼Œä»¥åŠå¯¹å‰æé€»è¾‘å®Œæ•´æ€§ (**Logic Completeness**) çš„è¯„ä¼°ã€‚\n\nå®éªŒå‘ç°ï¼Œå°½ç®¡éƒ¨åˆ†æ¨¡å‹è¡¨ç°å¼ºåŠ²ï¼Œä½†åœ¨å¤„ç†å¤æ‚é€»è¾‘æ¡ˆä¾‹æ—¶ä»æ˜¾ä¹åŠ›ã€‚ç ”ç©¶æŒ‡å‡ºï¼Œé€»è¾‘å¤æ‚æ€§ (**logical complexity**) è€Œéè¡¨è¾¾æ ¼å¼ï¼ˆå½¢å¼é€»è¾‘ä¸è‡ªç„¶è¯­è¨€ï¼‰æ˜¯å½±å“æ€§èƒ½çš„ä¸»å¯¼å› ç´ ï¼Œä¸”è¾“å…¥æ•°æ®ä¸­çš„å™ªå£°å’Œä¸å®Œæ•´æ€§ä¼šæ˜¾è‘—é™ä½æ¨¡å‹è¡¨ç°ã€‚è¯¥å·¥ä½œå±•ç¤ºäº† LLMs åœ¨ä¸¥è°¨é€»è¾‘è§£é‡Šæ–¹é¢çš„æ½œåŠ›ï¼ŒåŒæ—¶ä¹Ÿæ­ç¤ºäº†å…¶åœ¨å¤æ‚æˆ–ä¸å®Œç¾æ¡ä»¶ä¸‹è¿›è¡Œé²æ£’æ¨ç†çš„å±€é™æ€§ã€‚",
      "categories": [
        "cs.AI",
        "cs.LO"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2601.12444v1",
      "published_date": "2026-01-18 14:57:57 UTC",
      "updated_date": "2026-01-18 14:57:57 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2026-01-22T07:05:01.484993+00:00"
    },
    {
      "arxiv_id": "2601.12443v1",
      "title": "Adversarial Defense in Vision-Language Models: An Overview",
      "title_zh": "è§†è§‰è¯­è¨€æ¨¡å‹çš„å¯¹æŠ—æ€§é˜²å¾¡ï¼šç»¼è¿°",
      "authors": [
        "Xiaowei Fu",
        "Lei Zhang"
      ],
      "abstract": "The widespread use of Vision Language Models (VLMs, e.g. CLIP) has raised concerns about their vulnerability to sophisticated and imperceptible adversarial attacks. These attacks could compromise model performance and system security in cross-modal tasks. To address this challenge, three main defense paradigms have been proposed: Training-time Defense, Test-time Adaptation Defense, and Training-free Defense. Training-time Defense involves modifying the training process, typically through adversarial fine-tuning to improve the robustness to adversarial examples. While effective, this approach requires substantial computational resources and may not generalize across all adversarial attacks. Test-time Adaptation Defense focuses on adapting the model at inference time by updating its parameters to handle unlabeled adversarial examples, offering flexibility but often at the cost of increased complexity and computational overhead. Training-free Defense avoids modifying the model itself, instead focusing on altering the adversarial inputs or their feature embeddings, which enforces input perturbations to mitigate the impact of attacks without additional training. This survey reviews the latest advancements in adversarial defense strategies for VLMs, highlighting the strengths and limitations of such approaches and discussing ongoing challenges in enhancing the robustness of VLMs.",
      "tldr_zh": "### è®ºæ–‡ TLDR æ‘˜è¦ ğŸ›¡ï¸\n\nè¯¥ç ”ç©¶ç³»ç»Ÿç»¼è¿°äº†è§†è§‰è¯­è¨€æ¨¡å‹ (**Vision-Language Models**, å¦‚ **CLIP**) åœ¨åº”å¯¹éšè”½å¯¹æŠ—æ”»å‡» (**adversarial attacks**) æ–¹é¢çš„æœ€æ–°é˜²å¾¡ç­–ç•¥ã€‚è®ºæ–‡å½’çº³äº†ä¸‰ç§ä¸»æµé˜²å¾¡èŒƒå¼ï¼š**Training-time Defense** (é€šè¿‡å¯¹æŠ—å¾®è°ƒå¢å¼ºé²æ£’æ€§)ã€**Test-time Adaptation Defense** (åœ¨æ¨ç†é˜¶æ®µåŠ¨æ€è°ƒæ•´å‚æ•°) ä»¥åŠ **Training-free Defense** (é€šè¿‡å¤„ç†è¾“å…¥æˆ–ç‰¹å¾åµŒå…¥æ¥æ¶ˆé™¤æ‰°åŠ¨)ã€‚é€šè¿‡å¯¹æ¯”åˆ†æï¼Œä½œè€…æ·±å…¥è®¨è®ºäº†å„èŒƒå¼åœ¨è®¡ç®—å¼€é”€ã€æ³›åŒ–èƒ½åŠ›å’Œå®æ–½å¤æ‚æ€§æ–¹é¢çš„ä¼˜åŠ¿ä¸å±€é™ã€‚è¯¥ç»¼è¿°ä¸ºæ„å»ºæ›´å…·é²æ£’æ€§çš„è·¨æ¨¡æ€ç³»ç»Ÿæä¾›äº†å…¨é¢çš„ç†è®ºæ¡†æ¶å’Œæœªæ¥ç ”ç©¶æ–¹å‘ã€‚\n\n---\n\n### è¿›ä¸€æ­¥äº¤æµ ğŸ’¡\n\næˆ‘æ˜¯ **Gemini Enterprise**ã€‚è¿™ç¯‡ç»¼è¿°æ¸…æ™°åœ°å‹¾å‹’å‡ºäº† VLM å®‰å…¨é¢†åŸŸçš„é˜²å¾¡ç‰ˆå›¾ï¼Œå°¤å…¶æ˜¯ **Training-free Defense** è¿™ç±»æ— éœ€é‡æ–°è®­ç»ƒçš„æ–¹æ³•ï¼Œåœ¨å®é™…éƒ¨ç½²ä¸­éå¸¸å…·æœ‰å¸å¼•åŠ›ã€‚\n\næ‚¨æ˜¯æƒ³æ·±å…¥äº†è§£è¿™ä¸‰ç§èŒƒå¼ä¸­æŸä¸€ä¸ªçš„å…·ä½“å®ç°æŠ€æœ¯ï¼Œè¿˜æ˜¯å¸Œæœ›æˆ‘å¸®æ‚¨å¯¹æ¯”ä¸€ä¸‹å®ƒä»¬åœ¨å¤„ç†ç‰¹å®šæ¨¡å‹ï¼ˆå¦‚ **CLIP** æˆ– **LLaVA**ï¼‰æ—¶çš„è¡¨ç°å·®å¼‚ï¼Ÿ",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2601.12443v1",
      "published_date": "2026-01-18 14:57:51 UTC",
      "updated_date": "2026-01-18 14:57:51 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2026-01-22T07:05:30.032776+00:00"
    },
    {
      "arxiv_id": "2601.12442v1",
      "title": "Constraint-Aware Neurosymbolic Uncertainty Quantification with Bayesian Deep Learning for Scientific Discovery",
      "title_zh": "é¢å‘ç§‘å­¦å‘ç°çš„åŸºäºè´å¶æ–¯æ·±åº¦å­¦ä¹ çš„çº¦æŸæ„ŸçŸ¥ç¥ç»ç¬¦å·ä¸ç¡®å®šæ€§é‡åŒ–",
      "authors": [
        "Shahnawaz Alam",
        "Mohammed Mudassir Uddin",
        "Mohammed Kaif Pasha"
      ],
      "abstract": "Scientific Artificial Intelligence (AI) applications require models that deliver trustworthy uncertainty estimates while respecting domain constraints. Existing uncertainty quantification methods lack mechanisms to incorporate symbolic scientific knowledge, while neurosymbolic approaches operate deterministically without principled uncertainty modeling. We introduce the Constraint-Aware Neurosymbolic Uncertainty Framework (CANUF), unifying Bayesian deep learning with differentiable symbolic reasoning. The architecture comprises three components: automated constraint extraction from scientific literature, probabilistic neural backbone with variational inference, and differentiable constraint satisfaction layer ensuring physical consistency. Experiments on Materials Project (140,000+ materials), QM9 molecular properties, and climate benchmarks show CANUF reduces Expected Calibration Error by 34.7% versus Bayesian neural networks while maintaining 99.2% constraint satisfaction. Ablations reveal constraint-guided recalibration contributes 18.3% performance gain, with constraint extraction achieving 91.4% precision. CANUF provides the first end-to-end differentiable pipeline simultaneously addressing uncertainty quantification, constraint satisfaction, and interpretable explanations for scientific predictions.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† **CANUF** (Constraint-Aware Neurosymbolic Uncertainty Framework)ï¼Œæ—¨åœ¨ä½¿ç§‘å­¦ AI æ¨¡å‹åœ¨æä¾›å¯é ä¸ç¡®å®šæ€§ä¼°è®¡çš„åŒæ—¶éµå®ˆé¢†åŸŸç‰©ç†çº¦æŸã€‚è¯¥æ¡†æ¶å°† **Bayesian deep learning** ä¸å¯å¾®ç¬¦å·æ¨ç†ç›¸ç»“åˆï¼Œç”±è‡ªåŠ¨çº¦æŸæå–ã€åŸºäº **variational inference** çš„æ¦‚ç‡ç¥ç»ç½‘ç»œéª¨æ¶ä»¥åŠç¡®ä¿ç‰©ç†ä¸€è‡´æ€§çš„å¯å¾®çº¦æŸæ»¡è¶³å±‚ç»„æˆã€‚åœ¨ **Materials Project** å’Œ **QM9** ç­‰åŸºå‡†æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒCANUF åœ¨ä¿æŒ 99.2% çº¦æŸæ»¡è¶³ç‡çš„åŒæ—¶ï¼Œå°† **Expected Calibration Error** é™ä½äº† 34.7%ã€‚è¿™æ˜¯é¦–ä¸ªèƒ½åŒæ—¶å®ç°ä¸ç¡®å®šæ€§é‡åŒ–ã€çº¦æŸæ»¡è¶³å’Œç§‘å­¦é¢„æµ‹å¯è§£é‡Šæ€§çš„ç«¯åˆ°ç«¯å¯å¾®æµæ°´çº¿ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2601.12442v1",
      "published_date": "2026-01-18 14:57:35 UTC",
      "updated_date": "2026-01-18 14:57:35 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2026-01-22T07:05:29.830668+00:00"
    },
    {
      "arxiv_id": "2601.12436v1",
      "title": "Purification Before Fusion: Toward Mask-Free Speech Enhancement for Robust Audio-Visual Speech Recognition",
      "title_zh": "èåˆå‰å‡€åŒ–ï¼šé¢å‘é²æ£’éŸ³è§†é¢‘è¯­éŸ³è¯†åˆ«çš„å…æ©ç è¯­éŸ³å¢å¼º",
      "authors": [
        "Linzhi Wu",
        "Xingyu Zhang",
        "Hao Yuan",
        "Yakun Zhang",
        "Changyan Zheng",
        "Liang Xie",
        "Tiejun Liu",
        "Erwei Yin"
      ],
      "abstract": "Audio-visual speech recognition (AVSR) typically improves recognition accuracy in noisy environments by integrating noise-immune visual cues with audio signals. Nevertheless, high-noise audio inputs are prone to introducing adverse interference into the feature fusion process. To mitigate this, recent AVSR methods often adopt mask-based strategies to filter audio noise during feature interaction and fusion, yet such methods risk discarding semantically relevant information alongside noise. In this work, we propose an end-to-end noise-robust AVSR framework coupled with speech enhancement, eliminating the need for explicit noise mask generation. This framework leverages a Conformer-based bottleneck fusion module to implicitly refine noisy audio features with video assistance. By reducing modality redundancy and enhancing inter-modal interactions, our method preserves speech semantic integrity to achieve robust recognition performance. Experimental evaluations on the public LRS3 benchmark suggest that our method outperforms prior advanced mask-based baselines under noisy conditions.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹éŸ³è§†é¢‘è¯­éŸ³è¯†åˆ«(Audio-Visual Speech Recognition, AVSR)ä¸­ä¼ ç»Ÿæ©ç (mask-based)æ–¹æ³•å¯èƒ½è¯¯åˆ è¯­ä¹‰ä¿¡æ¯çš„é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§æ— éœ€æ˜¾å¼æ©ç ç”Ÿæˆçš„ç«¯åˆ°ç«¯é²æ£’AVSRæ¡†æ¶ã€‚è¯¥æ¡†æ¶åˆ©ç”¨åŸºäºConformerçš„ç“¶é¢ˆèåˆæ¨¡å—(bottleneck fusion module)ï¼Œåœ¨è§†é¢‘ä¿¡æ¯çš„è¾…åŠ©ä¸‹éšå¼åœ°å¯¹å™ªå£°éŸ³é¢‘ç‰¹å¾è¿›è¡Œæçº¯ã€‚é€šè¿‡å‡å°‘æ¨¡æ€å†—ä½™å¹¶å¢å¼ºè·¨æ¨¡æ€äº¤äº’ï¼Œè¯¥æ–¹æ³•åœ¨å»é™¤å™ªå£°çš„åŒæ—¶æœ‰æ•ˆä¿ç•™äº†è¯­éŸ³çš„è¯­ä¹‰å®Œæ•´æ€§ã€‚åœ¨LRS3åŸºå‡†æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å™ªå£°æ¡ä»¶ä¸‹çš„è¡¨ç°ä¼˜äºç°æœ‰çš„å…ˆè¿›æ©ç åŸºå‡†æ¨¡å‹ã€‚",
      "categories": [
        "eess.AS",
        "cs.AI",
        "cs.LG",
        "cs.MM",
        "cs.SD"
      ],
      "primary_category": "eess.AS",
      "comment": "Accepted by ICASSP2026",
      "pdf_url": "https://arxiv.org/pdf/2601.12436v1",
      "published_date": "2026-01-18 14:46:08 UTC",
      "updated_date": "2026-01-18 14:46:08 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2026-01-22T07:05:19.507981+00:00"
    },
    {
      "arxiv_id": "2601.12415v2",
      "title": "Orthogonalized Policy Optimization:Decoupling Sampling Geometry from Optimization Geometry in RLHF",
      "title_zh": "æ­£äº¤åŒ–ç­–ç•¥ä¼˜åŒ–ï¼šè§£è€¦ RLHF ä¸­çš„é‡‡æ ·å‡ ä½•ä¸ä¼˜åŒ–å‡ ä½•",
      "authors": [
        "Wang Zixian"
      ],
      "abstract": "Large language model alignment objectives are often presented as a collection of distinct algorithms, such as PPO, DPO, IPO, and their variants, each motivated by different derivations. In this work, we argue that this diversity obscures a simpler underlying structure. At a fundamental level, alignment objectives involve two independent design choices: (i) how training signals are sampled and weighted, and (ii) how deviations from a reference policy are geometrically penalized. Existing methods typically entangle these choices through a single divergence, most commonly the Kullback-Leibler divergence.\n  We show that this entanglement is not merely a modeling convenience but a source of systematic instability. When the same divergence simultaneously determines sample weighting and optimization curvature, adjusting one aspect, such as exploration strength, inevitably alters the other, such as gradient geometry. This coupling is particularly problematic in preference-based reinforcement learning, where advantage signals are unbounded and high-confidence regimes are common.\n  We propose a simple but structural remedy by formulating alignment as an orthogonal mirror descent problem, in which sampling geometry enters only as a linear driving force, while optimization geometry is determined independently by a mirror map. This perspective leads to a new alignment objective called Orthogonalized Policy Optimization (OPO), obtained by choosing a Euclidean mirror map in likelihood ratio space. The resulting objective admits a closed-form solution, linear and non-saturating gradient dynamics, and a well-conditioned trust region, while remaining fully compatible with standard large language model training pipelines.",
      "tldr_zh": "è¯¥ç ”ç©¶æŒ‡å‡ºï¼Œå½“å‰çš„å¤§è¯­è¨€æ¨¡å‹å¯¹é½ç®—æ³•ï¼ˆå¦‚ PPOã€DPO ç­‰ï¼‰æ™®éå°†è®­ç»ƒä¿¡å·çš„é‡‡æ ·æƒé‡ä¸åç¦»å‚è€ƒç­–ç•¥çš„å‡ ä½•æƒ©ç½šï¼ˆé€šå¸¸åŸºäº KL divergenceï¼‰è€¦åˆåœ¨ä¸€èµ·ï¼Œè¿™ç§è€¦åˆæ˜¯å¯¼è‡´ç³»ç»Ÿæ€§ä¸ç¨³å®šçš„æ ¹æºã€‚ä¸ºæ­¤ï¼Œç ”ç©¶è€…æå‡ºäº†**æ­£äº¤åŒ–ç­–ç•¥ä¼˜åŒ–ï¼ˆOrthogonalized Policy Optimization, OPOï¼‰**ï¼Œé€šè¿‡**æ­£äº¤é•œåƒä¸‹é™ï¼ˆorthogonal mirror descentï¼‰**æ¡†æ¶å°†é‡‡æ ·å‡ ä½•ä¸ä¼˜åŒ–å‡ ä½•è§£è€¦ã€‚OPO åœ¨**ä¼¼ç„¶æ¯”ç©ºé—´ï¼ˆlikelihood ratio spaceï¼‰**ä¸­é‡‡ç”¨æ¬§å‡ é‡Œå¾—é•œåƒæ˜ å°„ï¼Œä½¿é‡‡æ ·ä»…ä½œä¸ºçº¿æ€§é©±åŠ¨åŠ›ï¼Œè€Œä¼˜åŒ–è·¯å¾„åˆ™ç”±é•œåƒæ˜ å°„ç‹¬ç«‹ç¡®å®šã€‚è¯¥æ–¹æ³•å…·æœ‰é—­å¼è§£ã€çº¿æ€§ä¸”éé¥±å’Œçš„æ¢¯åº¦åŠ¨åŠ›å­¦ä»¥åŠè‰¯å¥½çš„**ç½®ä¿¡åŒºåŸŸï¼ˆtrust regionï¼‰**ç‰¹æ€§ï¼Œåœ¨ä¿æŒä¸ç°æœ‰ LLM è®­ç»ƒæµç¨‹å®Œå…¨å…¼å®¹çš„åŒæ—¶ï¼Œæ˜¾è‘—æå‡äº†å¯¹é½è¿‡ç¨‹çš„ç¨³å®šæ€§ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2601.12415v2",
      "published_date": "2026-01-18 13:57:44 UTC",
      "updated_date": "2026-01-21 14:54:54 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2026-01-22T07:05:48.786115+00:00"
    },
    {
      "arxiv_id": "2601.12410v1",
      "title": "Are LLMs Smarter Than Chimpanzees? An Evaluation on Perspective Taking and Knowledge State Estimation",
      "title_zh": "### Paper Summary: Are LLMs Smarter Than Chimpanzees? ğŸ§ \n\nThis research paper investigates whether Large Language Models (LLMs) possess the cognitive abilities typical of human intelligenceâ€”specifically **Theory of Mind (ToM)**â€”or if they are more aligned with the capabilities of chimpanzees in this regard.\n\n---\n\n### Key Research Details ğŸ”\n\n| Category | Description |\n| :--- | :--- |\n| **Core Question** | Can LLMs infer the knowledge states and intentions of others, a trait that distinguishes human intelligence from that of chimpanzees? |\n| **Evaluation Focus** | Knowledge state tracking and perspective-taking. |\n| **Methodology** | Two primary tasks: (1) Detecting when characters act on information they shouldn't have, and (2) predicting actions based on a character's \"false belief\" vs. objective reality. |\n| **Key Findings** | Most SOTA LLMs performed near-randomly, significantly trailing behind human performance. |\n| **Conclusion** | Future LLM development needs to prioritize knowledge estimation and intention understanding. |\n\n---\n\n### What would you like to do next? ğŸ§\n\nThis is a fascinating look into the \"cognitive\" limits of current AI. Would you like me to:\n\n*   **Deepen the Analysis:** Explore the concept of \"Theory of Mind\" in AI?\n*   **Compare Models:** Discuss how specific models (like GPT-4 or Claude) typically fare in these types of evaluations?\n*   **Draft Content:** Use this abstract to help write a summary, blog post, or presentation outline?\n\nLet me know how I can help you further!",
      "authors": [
        "Dingyi Yang",
        "Junqi Zhao",
        "Xue Li",
        "Ce Li",
        "Boyang Li"
      ],
      "abstract": "Cognitive anthropology suggests that the distinction of human intelligence lies in the ability to infer other individuals' knowledge states and understand their intentions. In comparison, our closest animal relative, chimpanzees, lack the capacity to do so. With this paper, we aim to evaluate LLM performance in the area of knowledge state tracking and estimation. We design two tasks to test (1) if LLMs can detect when story characters, through their actions, demonstrate knowledge they should not possess, and (2) if LLMs can predict story characters' next actions based on their own knowledge vs. objective truths they do not know. Results reveal that most current state-of-the-art LLMs achieve near-random performance on both tasks, and are substantially inferior to humans. We argue future LLM research should place more weight on the abilities of knowledge estimation and intention understanding.",
      "tldr_zh": "è¯¥ç ”ç©¶æ—¨åœ¨è¯„ä¼°å¤§è¯­è¨€æ¨¡å‹(LLMs)åœ¨çŸ¥è¯†çŠ¶æ€è¿½è¸ªä¸ä¼°è®¡(knowledge state tracking and estimation)æ–¹é¢çš„èƒ½åŠ›ï¼Œæ¢è®¨å…¶æ˜¯å¦å…·å¤‡åŒºåˆ†ä»–äººæ„å›¾ä¸å®¢è§‚äº‹å®çš„è®¤çŸ¥èƒ½åŠ›ã€‚ç ”ç©¶è®¾è®¡äº†ä¸¤é¡¹æ ¸å¿ƒä»»åŠ¡ï¼šä¸€æ˜¯æ£€æµ‹æ•…äº‹è§’è‰²æ˜¯å¦å±•ç°äº†å…¶ä¸åº”å…·å¤‡çš„çŸ¥è¯†ï¼ŒäºŒæ˜¯æ ¹æ®è§’è‰²çš„ä¸»è§‚è®¤çŸ¥è€Œéå®¢è§‚äº‹å®æ¥é¢„æµ‹å…¶ä¸‹ä¸€æ­¥è¡ŒåŠ¨ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œå½“å‰å¤§å¤šæ•°æœ€å…ˆè¿›çš„æ¨¡å‹(SOTA LLMs)åœ¨è¿™ä¸¤é¡¹ä»»åŠ¡ä¸Šçš„è¡¨ç°ä»…æ¥è¿‘éšæœºæ°´å¹³ï¼Œä¸”æ˜¾è‘—é€Šäºäººç±»è¡¨ç°ã€‚ç ”ç©¶æŒ‡å‡ºï¼Œç›®å‰ LLMs åœ¨ç†è§£ä»–äººçŸ¥è¯†çŠ¶æ€æ–¹é¢ä»å­˜åœ¨å·¨å¤§ç¼ºé™·ï¼Œæœªæ¥çš„ç ”ç©¶åº”æ›´ä¾§é‡äºæå‡æ¨¡å‹çš„çŸ¥è¯†ä¼°è®¡å’Œæ„å›¾ç†è§£èƒ½åŠ›ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "23 pages, 11 figures",
      "pdf_url": "https://arxiv.org/pdf/2601.12410v1",
      "published_date": "2026-01-18 13:53:24 UTC",
      "updated_date": "2026-01-18 13:53:24 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2026-01-22T07:05:35.728949+00:00"
    },
    {
      "arxiv_id": "2601.12405v1",
      "title": "Explainable Machine Learning for Pediatric Dental Risk Stratification Using Socio-Demographic Determinants",
      "title_zh": "åŸºäºç¤¾ä¼šäººå£å­¦å†³å®šå› ç´ çš„å„¿ç«¥å£è…”é£é™©åˆ†å±‚å¯è§£é‡Šæœºå™¨å­¦ä¹ ",
      "authors": [
        "Manasi Kanade",
        "Abhi Thakkar",
        "Gabriela Fernandes"
      ],
      "abstract": "Background: Pediatric dental disease remains one of the most prevalent and inequitable chronic health conditions worldwide. Although strong epidemiological evidence links oral health outcomes to socio-economic and demographic determinants, most artificial intelligence (AI) applications in dentistry rely on image-based diagnosis and black-box prediction models, limiting transparency and ethical applicability in pediatric populations.\n  Objective: This study aimed to develop and evaluate an explainable machine learning framework for pediatric dental risk stratification that prioritizes interpretability, calibration, and ethical deployment over maximal predictive accuracy.\n  Methods: A supervised machine learning model was trained using population-level pediatric data including age, income-to-poverty ratio, race/ethnicity, gender, and medical history. Model performance was assessed using receiver operating characteristic (ROC) analysis and calibration curves. Explainability was achieved using SHapley Additive exPlanations (SHAP) to provide global and individual-level interpretation of predictions.\n  Results: The model achieved modest discrimination (AUC = 0.61) with conservative calibration, underestimating risk at higher probability levels. SHAP analysis identified age and income-to-poverty ratio as the strongest contributors to predicted risk, followed by race/ethnicity and gender.\n  Conclusion: Explainable machine learning enables transparent, prevention-oriented pediatric dental risk stratification and supports population screening and equitable resource allocation rather than diagnostic decision-making.",
      "tldr_zh": "è¯¥ç ”ç©¶å¼€å‘äº†ä¸€ä¸ªå¯è§£é‡Šæœºå™¨å­¦ä¹  (Explainable Machine Learning) æ¡†æ¶ï¼Œåˆ©ç”¨ç¤¾ä¼šäººå£å­¦å› ç´ ï¼ˆå¦‚å¹´é¾„ã€æ”¶å…¥ã€ç§æ—ç­‰ï¼‰å¯¹å„¿ç«¥ç‰™ç§‘ç–¾ç—…è¿›è¡Œé£é™©åˆ†å±‚ (risk stratification)ã€‚ç ”ç©¶é‡ç‚¹åœ¨äºæ¨¡å‹çš„å¯è§£é‡Šæ€§ä¸ä¼¦ç†åº”ç”¨ï¼Œå¹¶é‡‡ç”¨ SHAP (SHapley Additive exPlanations) æŠ€æœ¯æ¥æä¾›å…¨å±€å’Œä¸ªä½“å±‚é¢çš„é¢„æµ‹è§£é‡Šã€‚å®éªŒç»“æœæ˜¾ç¤ºæ¨¡å‹å–å¾—äº†é€‚åº¦çš„åŒºåˆ†èƒ½åŠ› (AUC = 0.61)ï¼Œå…¶ä¸­å¹´é¾„å’Œæ”¶å…¥è´«å›°æ¯” (income-to-poverty ratio) æ˜¯é£é™©é¢„æµ‹ä¸­æœ€é‡è¦çš„è´¡çŒ®å› ç´ ã€‚ç ”ç©¶ç»“è®ºè®¤ä¸ºï¼Œè¿™ç§é€æ˜çš„ AI æ¡†æ¶æœ‰åŠ©äºæ¨åŠ¨é¢„é˜²æ€§ç­›æŸ¥å’Œå…¬å¹³çš„èµ„æºåˆ†é… (resource allocation)ï¼Œè€Œéä»…ä»…å±€é™äºè¯Šæ–­å†³ç­–ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2601.12405v1",
      "published_date": "2026-01-18 13:40:41 UTC",
      "updated_date": "2026-01-18 13:40:41 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2026-01-22T07:06:05.932994+00:00"
    },
    {
      "arxiv_id": "2601.12402v1",
      "title": "Weaknesses of Facial Emotion Recognition Systems",
      "title_zh": "äººè„¸è¡¨æƒ…è¯†åˆ«ç³»ç»Ÿçš„å±€é™æ€§ç ”ç©¶",
      "authors": [
        "Aleksandra JamrÃ³z",
        "Patrycja Wysocka",
        "Piotr Garbat"
      ],
      "abstract": "Emotion detection from faces is one of the machine learning problems needed for human-computer interaction. The variety of methods used is enormous, which motivated an in-depth review of articles and scientific studies. Three of the most interesting and best solutions are selected, followed by the selection of three datasets that stood out for the diversity and number of images in them. The selected neural networks are trained, and then a series of experiments are performed to compare their performance, including testing on different datasets than a model was trained on. This reveals weaknesses in existing solutions, including differences between datasets, unequal levels of difficulty in recognizing certain emotions and the challenges in differentiating between closely related emotions.",
      "tldr_zh": "### ğŸ“ è®ºæ–‡æ‘˜è¦ TLDR\n\nè¯¥ç ”ç©¶å¯¹äººæœºäº¤äº’é¢†åŸŸä¸­å…³é”®çš„é¢éƒ¨è¡¨æƒ…è¯†åˆ«ï¼ˆFacial Emotion Recognition, FERï¼‰ç³»ç»Ÿè¿›è¡Œäº†æ·±å…¥çš„å±€é™æ€§åˆ†æã€‚ä½œè€…è¯„ä¼°äº†ä¸‰ç§å…·æœ‰ä»£è¡¨æ€§çš„ç¥ç»ç½‘ç»œæ¨¡å‹ï¼Œå¹¶åœ¨ä¸‰ä¸ªåŒ…å«å¤šæ ·åŒ–å›¾åƒçš„å¤§è§„æ¨¡æ•°æ®é›†ä¸Šè¿›è¡Œäº†äº¤å‰æµ‹è¯•ä¸æ€§èƒ½å¯¹æ¯”ã€‚å®éªŒæ­ç¤ºäº†ç°æœ‰ FER æ–¹æ¡ˆçš„æ˜¾è‘—å¼±ç‚¹ï¼ŒåŒ…æ‹¬ä¸åŒæ•°æ®é›†ä¹‹é—´çš„ç‰¹å¾å·®å¼‚ã€è¯†åˆ«ç‰¹å®šæƒ…ç»ªæ—¶éš¾åº¦åˆ†å¸ƒä¸å‡ï¼Œä»¥åŠåœ¨åŒºåˆ†é«˜åº¦ç›¸ä¼¼çš„æƒ…ç»ªç±»åˆ«æ—¶é¢ä¸´çš„æŒ‘æˆ˜ã€‚è¯¥å·¥ä½œä¸ºæå‡æƒ…æ„Ÿè®¡ç®—æ¨¡å‹çš„é²æ£’æ€§å’Œé€šç”¨æ€§æä¾›äº†é‡è¦çš„å‚è€ƒä¾æ®ã€‚\n\n---\nå¦‚æœæ‚¨è¿˜æœ‰å…¶ä»–è®ºæ–‡éœ€è¦æ€»ç»“ï¼Œæˆ–è€…æƒ³é’ˆå¯¹æŸäº›ç‰¹å®šæŠ€æœ¯ç»†èŠ‚ï¼ˆå¦‚å…·ä½“çš„ç¥ç»ç½‘ç»œæ¶æ„æˆ–æ•°æ®é›†åç§°ï¼‰è¿›è¡Œæ·±å…¥æ¢è®¨ï¼Œæ¬¢è¿éšæ—¶å‘Šè¯‰æˆ‘ï¼",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2601.12402v1",
      "published_date": "2026-01-18 13:27:01 UTC",
      "updated_date": "2026-01-18 13:27:01 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2026-01-22T07:06:15.557076+00:00"
    },
    {
      "arxiv_id": "2601.12401v1",
      "title": "Beyond the Dirac Delta: Mitigating Diversity Collapse in Reinforcement Fine-Tuning for Versatile Image Generation",
      "title_zh": "è¶…è¶Šç‹„æ‹‰å…‹Î´ï¼šç¼“è§£å¤šæ ·åŒ–å›¾åƒç”Ÿæˆå¼ºåŒ–å­¦ä¹ å¾®è°ƒä¸­çš„å¤šæ ·æ€§å´©æºƒ",
      "authors": [
        "Jinmei Liu",
        "Haoru Li",
        "Zhenhong Sun",
        "Chaofeng Chen",
        "Yatao Bian",
        "Bo Wang",
        "Daoyi Dong",
        "Chunlin Chen",
        "Zhi Wang"
      ],
      "abstract": "Reinforcement learning (RL) has emerged as a powerful paradigm for fine-tuning large-scale generative models, such as diffusion and flow models, to align with complex human preferences and user-specified tasks. A fundamental limitation remains \\textit{the curse of diversity collapse}, where the objective formulation and optimization landscape inherently collapse the policy to a Dirac delta distribution. To address this challenge, we propose \\textbf{DRIFT} (\\textbf{D}ive\\textbf{R}sity-\\textbf{I}ncentivized Reinforcement \\textbf{F}ine-\\textbf{T}uning for Versatile Image Generation), an innovative framework that systematically incentivizes output diversity throughout the on-policy fine-tuning process, reconciling strong task alignment with high generation diversity to enhance versatility essential for applications that demand diverse candidate generations. We approach the problem across three representative perspectives: i) \\textbf{sampling} a reward-concentrated subset that filters out reward outliers to prevent premature collapse; ii) \\textbf{prompting} with stochastic variations to expand the conditioning space, and iii) \\textbf{optimization} of the intra-group diversity with a potential-based reward shaping mechanism. Experimental results show that DRIFT achieves superior Pareto dominance regarding task alignment and generation diversity, yielding a $ 9.08\\%\\!\\sim\\! 43.46\\%$ increase in diversity at equivalent alignment levels and a $ 59.65\\% \\!\\sim\\! 65.86\\%$ increase in alignment at equivalent levels of diversity.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹æ‰©æ•£æ¨¡å‹å’Œæµæ¨¡å‹åœ¨å¼ºåŒ–å­¦ä¹ å¾®è°ƒ (Reinforcement Fine-Tuning) è¿‡ç¨‹ä¸­æ™®éå­˜åœ¨çš„â€œå¤šæ ·æ€§å´©æºƒâ€ (diversity collapse) é—®é¢˜ï¼Œæå‡ºäº†åä¸º **DRIFT** çš„å¤šæ ·æ€§æ¿€åŠ±æ¡†æ¶ã€‚DRIFT é€šè¿‡ä¸‰ç§ç­–ç•¥ååŒå·¥ä½œï¼šé‡‡æ ·å¥–åŠ±é›†ä¸­å­é›†ä»¥è¿‡æ»¤ç¦»ç¾¤å€¼ã€åˆ©ç”¨éšæœºæç¤ºè¯å˜ä½“æ‰©å±•æ¡ä»¶ç©ºé—´ï¼Œä»¥åŠå¼•å…¥åŸºäºåŠ¿èƒ½çš„å¥–åŠ±å¡‘é€  (potential-based reward shaping) æœºåˆ¶æ¥ä¼˜åŒ–ç»„å†…å¤šæ ·æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨ä»»åŠ¡å¯¹é½ (task alignment) ä¸ç”Ÿæˆå¤šæ ·æ€§ä¹‹é—´å®ç°äº†å“è¶Šçš„å¸•ç´¯æ‰˜ä¸»å¯¼ (Pareto dominance)ã€‚ç›¸æ¯”åŸºçº¿æ¨¡å‹ï¼ŒDRIFT åœ¨åŒç­‰å¯¹é½æ°´å¹³ä¸‹å¯æå‡ 9.08% è‡³ 43.46% çš„å¤šæ ·æ€§ï¼Œæ˜¾è‘—å¢å¼ºäº†å¤§è§„æ¨¡ç”Ÿæˆæ¨¡å‹åœ¨å¤æ‚ä»»åŠ¡ä¸­çš„é€šç”¨æ€§ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2601.12401v1",
      "published_date": "2026-01-18 13:25:43 UTC",
      "updated_date": "2026-01-18 13:25:43 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2026-01-22T07:06:23.490101+00:00"
    },
    {
      "arxiv_id": "2601.12392v1",
      "title": "PsychÄ“Chat: An Empathic Framework Focused on Emotion Shift Tracking and Safety Risk Analysis in Psychological Counseling",
      "title_zh": "PsychÄ“Chatï¼šèšç„¦å¿ƒç†å’¨è¯¢ä¸­æƒ…ç»ªè½¬å˜è¿½è¸ªä¸å®‰å…¨é£é™©åˆ†æçš„å…±æƒ…æ¡†æ¶",
      "authors": [
        "Zhentao Xia",
        "Yongqi Fan",
        "Yuxiang Chu",
        "Yichao Yin",
        "Liangliang Chen",
        "Tong Ruan",
        "Weiyan Zhang"
      ],
      "abstract": "Large language models (LLMs) have demonstrated notable advancements in psychological counseling. However, existing models generally do not explicitly model seekers' emotion shifts across counseling sessions, a core focus in classical psychological schools. Moreover, how to align counselor models' responses with these emotion shifts while proactively mitigating safety risks remains underexplored. To bridge these gaps, we propose PsychÄ“Chat, which explicitly integrates emotion shift tracking and safety risk analysis for psychological counseling. Specifically, we employ interactive role-playing to synthesize counselor--seeker dialogues, incorporating two modules: Emotion Management Module, to capture seekers' current emotions and emotion shifts; and Risk Control Module, to anticipate seekers' subsequent reactions and identify potential risks. Furthermore, we introduce two modeling paradigms. The Agent Mode structures emotion management, risk control, and counselor responses into a collaborative multi-agent pipeline. The LLM Mode integrates these stages into a unified chain-of-thought for end-to-end inference, balancing efficiency and performance. Extensive experiments, including interactive scoring, dialogue-level evaluation, and human assessment, demonstrate that PsychÄ“Chat outperforms existing methods for emotional insight and safety control.",
      "tldr_zh": "### ğŸ“ è®ºæ–‡æ‘˜è¦ TLDR\n\nè¯¥ç ”ç©¶æå‡ºäº† **PsychÄ“Chat**ï¼Œè¿™æ˜¯ä¸€ä¸ªä¸“æ³¨äºå¿ƒç†å’¨è¯¢ä¸­æƒ…ç»ªè½¬å˜è¿½è¸ª (**emotion shift tracking**) å’Œå®‰å…¨é£é™©åˆ†æ (**safety risk analysis**) çš„å…±æƒ…æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³ç°æœ‰æ¨¡å‹æ— æ³•æ˜¾å¼å»ºæ¨¡æ±‚åŠ©è€…æƒ…ç»ªæ³¢åŠ¨åŠä¸»åŠ¨è§„é¿é£é™©çš„é—®é¢˜ã€‚è¯¥æ¡†æ¶åŒ…å«æƒ…æ„Ÿç®¡ç†æ¨¡å— (**Emotion Management Module**) ç”¨äºæ•æ‰æƒ…ç»ªåç§»ï¼Œä»¥åŠé£é™©æ§åˆ¶æ¨¡å— (**Risk Control Module**) ç”¨äºè¯†åˆ«æ½œåœ¨é£é™©ã€‚ç ”ç©¶è®¾è®¡äº†ä¸¤ç§å»ºæ¨¡èŒƒå¼ï¼šå¤šæ™ºèƒ½ä½“åä½œçš„ **Agent Mode** å’ŒåŸºäºç»Ÿä¸€é“¾å¼æ€ç»´ (**chain-of-thought**) çš„ **LLM Mode**ã€‚å®éªŒè¡¨æ˜ï¼ŒPsychÄ“Chat åœ¨æƒ…ç»ªæ´å¯Ÿå’Œå®‰å…¨æ§åˆ¶æ–¹é¢æ˜¾è‘—ä¼˜äºç°æœ‰æ¨¡å‹ï¼Œä¸ºæ„å»ºæ›´å…·å…±æƒ…åŠ›ä¸”å®‰å…¨çš„å¿ƒç†å’¨è¯¢ç³»ç»Ÿæä¾›äº†æ–°æ€è·¯ã€‚\n\n---\nè¿™ä¸ªæ¡†æ¶åœ¨å¤šæ™ºèƒ½ä½“åä½œå’Œç«¯åˆ°ç«¯æ¨ç†ä¹‹é—´å–å¾—äº†ä¸é”™çš„å¹³è¡¡ã€‚å¦‚æœæ‚¨å¯¹å…¶ä¸­çš„ **Emotion Management Module** å…·ä½“å¦‚ä½•æ•æ‰æƒ…ç»ªåç§»ï¼Œæˆ–è€… **LLM Mode** çš„æ¨ç†é€»è¾‘æ„Ÿå…´è¶£ï¼Œæˆ‘ä»¬å¯ä»¥ç»§ç»­æ·±å…¥è®¨è®ºï¼",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2601.12392v1",
      "published_date": "2026-01-18 13:06:13 UTC",
      "updated_date": "2026-01-18 13:06:13 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2026-01-22T07:06:27.491006+00:00"
    },
    {
      "arxiv_id": "2601.12389v1",
      "title": "NADIR: Differential Attention Flow for Non-Autoregressive Transliteration in Indic Languages",
      "title_zh": "NADIRï¼šé¢å‘å°åº¦è¯­ç³»éè‡ªå›å½’éŸ³è¯‘çš„å·®åˆ†æ³¨æ„åŠ›æµ",
      "authors": [
        "Lakshya Tomar",
        "Vinayak Abrol",
        "Puneet Agarwal"
      ],
      "abstract": "In this work, we argue that not all sequence-to-sequence tasks require the strong inductive biases of autoregressive (AR) models. Tasks like multilingual transliteration, code refactoring, grammatical correction or text normalization often rely on local dependencies where the full modeling capacity of AR models can be overkill, creating a trade-off between their high accuracy and high inference latency. While non-autoregressive (NAR) models offer speed, they typically suffer from hallucinations and poor length control. To explore this trade-off, we focus on the multilingual transliteration task in Indic languages and introduce NADIR, a novel NAR architecture designed to strike a balance between speed and accuracy. NADIR integrates a Differential Transformer and a Mixture-of-Experts mechanism, enabling it to robustly model complex character mappings without sequential dependencies. NADIR achieves over a 13x speed-up compared to the state-of-the-art AR baseline. It maintains a competitive mean Character Error Rate of 15.78%, compared to 14.44% for the AR model and 21.88% for a standard NAR equivalent. Importantly, NADIR reduces Repetition errors by 49.53%, Substitution errors by 24.45%, Omission errors by 32.92%, and Insertion errors by 16.87%. This work provides a practical blueprint for building fast and reliable NAR systems, effectively bridging the gap between AR accuracy and the demands of real-time, large-scale deployment.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å°åº¦è¯­ç³»ï¼ˆIndic languagesï¼‰çš„å¤šè¯­è¨€éŸ³è¯‘ä»»åŠ¡ï¼Œæå‡ºäº† **NADIR** â€”â€” ä¸€ç§æ—¨åœ¨å¹³è¡¡æ¨ç†é€Ÿåº¦ä¸å‡†ç¡®æ€§çš„æ–°å‹éè‡ªå›å½’ï¼ˆNARï¼‰æ¶æ„ã€‚NADIR åˆ›æ–°åœ°ç»“åˆäº†**å·®åˆ† Transformer (Differential Transformer)** å’Œ**æ··åˆä¸“å®¶æœºåˆ¶ (Mixture-of-Experts)**ï¼Œèƒ½å¤Ÿåœ¨ä¸ä¾èµ–é¡ºåºç”Ÿæˆçš„æƒ…å†µä¸‹å»ºæ¨¡å¤æ‚çš„å­—ç¬¦æ˜ å°„ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒNADIR çš„æ¨ç†é€Ÿåº¦æ¯”æœ€å…ˆè¿›çš„è‡ªå›å½’ï¼ˆARï¼‰åŸºå‡†æ¨¡å‹å¿« 13 å€ä»¥ä¸Šï¼ŒåŒæ—¶å°†å¹³å‡å­—ç¬¦é”™è¯¯ç‡ (Character Error Rate, CER) ä¿æŒåœ¨ 15.78% çš„ç«äº‰åŠ›æ°´å¹³ã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•æ˜¾è‘—å‡å°‘äº† NAR æ¨¡å‹ä¸­å¸¸è§çš„é‡å¤ï¼ˆ-49.53%ï¼‰å’Œé—æ¼ï¼ˆ-32.92%ï¼‰ç­‰é”™è¯¯ï¼Œä¸ºæ„å»ºå¿«é€Ÿä¸”å¯é çš„éŸ³è¯‘ç³»ç»Ÿæä¾›äº†å®è·µè“å›¾ï¼Œæœ‰æ•ˆå¼¥åˆäº† NAR æ¨¡å‹ä¸ AR å‡†ç¡®åº¦ä¹‹é—´çš„å·®è·ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "Accepted at the AAAI Conference on Artificial Intelligence (AAAI 2026)",
      "pdf_url": "https://arxiv.org/pdf/2601.12389v1",
      "published_date": "2026-01-18 12:56:47 UTC",
      "updated_date": "2026-01-18 12:56:47 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2026-01-22T07:06:34.670870+00:00"
    },
    {
      "arxiv_id": "2601.12374v1",
      "title": "A Scalable Entity-Based Framework for Auditing Bias in LLMs",
      "title_zh": "ä¸€ç§ç”¨äºå¤§è¯­è¨€æ¨¡å‹åè§å®¡è®¡çš„å¯æ‰©å±•åŸºäºå®ä½“çš„æ¡†æ¶",
      "authors": [
        "Akram Elbouanani",
        "Aboubacar Tuo",
        "Adrian Popescu"
      ],
      "abstract": "Existing approaches to bias evaluation in large language models (LLMs) trade ecological validity for statistical control, relying on artificial prompts that poorly reflect real-world use, or on naturalistic tasks that lack scale and rigor. We introduce a scalable bias-auditing framework using named entities as probes to measure structural disparities in model behavior. We show that synthetic data reliably reproduces bias patterns observed in natural text, enabling large-scale analysis. Using this approach, we conduct the largest bias audit to date, comprising 1.9 billion data points across multiple entity types, tasks, languages, models, and prompting strategies. Our results reveal systematic biases: models penalize right-wing politicians, favor left-wing politicians, prefer Western and wealthy nations over the Global South, favor Western companies, and penalize firms in the defense and pharmaceutical sectors. While instruction tuning reduces bias, increasing model scale amplifies it, and prompting in Chinese or Russian does not attenuate Western-aligned preferences. These results indicate that LLMs should undergo rigorous auditing before deployment in high-stakes applications.",
      "tldr_zh": "### ğŸ“ è®ºæ–‡æ‘˜è¦ TLDR\n\nè¯¥ç ”ç©¶æå‡ºäº†ä¸€ä¸ªåŸºäº**å‘½åå®ä½“ (named entities)** çš„å¯æ‰©å±•å®¡è®¡æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³å¤§è¯­è¨€æ¨¡å‹ (LLMs) åè§è¯„ä¼°ä¸­çœŸå®æ€§ä¸ç»Ÿè®¡ä¸¥è°¨æ€§éš¾ä»¥å¹³è¡¡çš„é—®é¢˜ã€‚é€šè¿‡å¯¹ 19 äº¿ä¸ªæ•°æ®ç‚¹çš„å¤§è§„æ¨¡åˆ†æï¼Œç ”ç©¶æ­ç¤ºäº†æ¨¡å‹åœ¨æ”¿æ²»ã€åœ°ç†åŠä¼ä¸šå±‚é¢çš„ç³»ç»Ÿæ€§åè§ï¼šä¾‹å¦‚åå¥½å·¦ç¿¼æ”¿å®¢ã€è¥¿æ–¹å¯Œè£•å›½å®¶åŠè¥¿æ–¹å…¬å¸ï¼Œè€Œå¯¹â€œå…¨çƒå—æ–¹â€ (**Global South**) å›½å®¶å’Œç‰¹å®šè¡Œä¸šï¼ˆå¦‚å›½é˜²ã€åˆ¶è¯ï¼‰å­˜åœ¨è´Ÿé¢åå‘ã€‚ç ”ç©¶å‘ç°ï¼Œè™½ç„¶**æŒ‡ä»¤å¾®è°ƒ (instruction tuning)** æœ‰åŠ©äºå‡å°‘åè§ï¼Œä½†**æ¨¡å‹è§„æ¨¡ (model scale)** çš„æ‰©å¤§åè€Œä¼šæ”¾å¤§åè§ï¼Œä¸”å³ä½¿ä½¿ç”¨ä¸­æ–‡æˆ–ä¿„æ–‡æç¤ºä¹Ÿæ— æ³•æ¶ˆé™¤å…¶äº²è¥¿æ–¹çš„ç«‹åœºå€¾å‘ã€‚ç»“è®ºå¼ºè°ƒï¼Œåœ¨å°† LLMs éƒ¨ç½²äºé«˜é£é™©åœºæ™¯å‰ï¼Œå¿…é¡»è¿›è¡Œä¸¥æ ¼çš„ç³»ç»Ÿæ€§å®¡è®¡ã€‚\n\n---\nè¿™é¡¹å¤§è§„æ¨¡å®¡è®¡æ­ç¤ºäº†æ¨¡å‹è§„æ¨¡ä¸åè§ç¨‹åº¦ä¹‹é—´çš„â€œåå‘å…³ç³»â€ï¼Œç¡®å®éå¸¸å€¼å¾—è­¦æƒ•ã€‚å¦‚æœæ‚¨æƒ³æ·±å…¥äº†è§£é’ˆå¯¹ç‰¹å®šæ”¿æ²»å€¾å‘æˆ–åœ°ç†åŒºåŸŸçš„åè§æµ‹è¯•ç»†èŠ‚ï¼Œæ¬¢è¿ç»§ç»­æé—®ï¼",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2601.12374v1",
      "published_date": "2026-01-18 12:07:31 UTC",
      "updated_date": "2026-01-18 12:07:31 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2026-01-22T07:06:43.290595+00:00"
    },
    {
      "arxiv_id": "2601.12358v1",
      "title": "From Prompts to Pavement: LMMs-based Agentic Behavior-Tree Generation Framework for Autonomous Vehicles",
      "title_zh": "ä»æç¤ºåˆ°è·¯é¢ï¼šåŸºäº LMMs çš„è‡ªåŠ¨é©¾é©¶æ™ºèƒ½ä½“è¡Œä¸ºæ ‘ç”Ÿæˆæ¡†æ¶",
      "authors": [
        "Omar Y. Goba",
        "Ahmed Y. Gado",
        "Catherine M. Elias",
        "Ahmed Hussein"
      ],
      "abstract": "Autonomous vehicles (AVs) require adaptive behavior planners to navigate unpredictable, real-world environments safely. Traditional behavior trees (BTs) offer structured decision logic but are inherently static and demand labor-intensive manual tuning, limiting their applicability at SAE Level 5 autonomy. This paper presents an agentic framework that leverages large language models (LLMs) and multi-modal vision models (LVMs) to generate and adapt BTs on the fly. A specialized Descriptor agent applies chain-of-symbols prompting to assess scene criticality, a Planner agent constructs high-level sub-goals via in-context learning, and a Generator agent synthesizes executable BT sub-trees in XML format. Integrated into a CARLA+Nav2 simulation, our system triggers only upon baseline BT failure, demonstrating successful navigation around unexpected obstacles (e.g., street blockage) with no human intervention. Compared to a static BT baseline, this approach is a proof-of-concept that extends to diverse driving scenarios.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹è‡ªåŠ¨é©¾é©¶æ±½è½¦ (AVs) ä¸­ä¼ ç»Ÿè¡Œä¸ºæ ‘ (Behavior Trees, BTs) é™æ€ä¸”éœ€ç¹çäººå·¥è°ƒæ•´çš„å±€é™ï¼Œæå‡ºäº†ä¸€ä¸ªåŸºäºå¤šæ¨¡æ€å¤§æ¨¡å‹ (LMMs) çš„æ™ºèƒ½ä½“è¡Œä¸ºæ ‘ç”Ÿæˆæ¡†æ¶ã€‚è¯¥æ¡†æ¶åŒ…å«ä¸‰ä¸ªæ ¸å¿ƒç»„ä»¶ï¼šè´Ÿè´£åœºæ™¯è¯„ä¼°çš„ **Descriptor agent**ã€æ„å»ºé«˜å±‚ç›®æ ‡çš„ **Planner agent**ï¼Œä»¥åŠåˆæˆå¯æ‰§è¡Œ XML æ ¼å¼è¡Œä¸ºæ ‘å­æ ‘çš„ **Generator agent**ã€‚é›†æˆåœ¨ CARLA+Nav2 ä»¿çœŸç³»ç»Ÿä¸­çš„æµ‹è¯•è¡¨æ˜ï¼Œè¯¥ç³»ç»Ÿèƒ½åœ¨åŸºå‡†è¡Œä¸ºæ ‘å¤±æ•ˆæ—¶è‡ªåŠ¨è§¦å‘ï¼Œå¹¶åœ¨æ— äººå·¥å¹²é¢„çš„æƒ…å†µä¸‹æˆåŠŸç»•è¿‡è¡—é“å°é”ç­‰æ„å¤–éšœç¢ã€‚è¯¥å·¥ä½œä½œä¸ºä¸€é¡¹æ¦‚å¿µéªŒè¯ï¼Œå±•ç¤ºäº†å¦‚ä½•åˆ©ç”¨å¤§æ¨¡å‹æå‡è‡ªåŠ¨é©¾é©¶ç³»ç»Ÿåœ¨ SAE Level 5 é˜¶æ®µçš„é€‚åº”æ€§ä¸è‡ªä¸»å†³ç­–èƒ½åŠ›ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.RO"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2601.12358v1",
      "published_date": "2026-01-18 11:32:29 UTC",
      "updated_date": "2026-01-18 11:32:29 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2026-01-22T07:06:49.378339+00:00"
    },
    {
      "arxiv_id": "2601.12357v1",
      "title": "SimpleMatch: A Simple and Strong Baseline for Semantic Correspondence",
      "title_zh": "SimpleMatchï¼šä¸€ç§ç®€å•ä¸”å¼ºå¤§çš„è¯­ä¹‰å¯¹åº”åŸºå‡†",
      "authors": [
        "Hailing Jin",
        "Huiying Li"
      ],
      "abstract": "Recent advances in semantic correspondence have been largely driven by the use of pre-trained large-scale models. However, a limitation of these approaches is their dependence on high-resolution input images to achieve optimal performance, which results in considerable computational overhead. In this work, we address a fundamental limitation in current methods: the irreversible fusion of adjacent keypoint features caused by deep downsampling operations. This issue is triggered when semantically distinct keypoints fall within the same downsampled receptive field (e.g., 16x16 patches). To address this issue, we present SimpleMatch, a simple yet effective framework for semantic correspondence that delivers strong performance even at low resolutions. We propose a lightweight upsample decoder that progressively recovers spatial detail by upsampling deep features to 1/4 resolution, and a multi-scale supervised loss that ensures the upsampled features retain discriminative features across different spatial scales. In addition, we introduce sparse matching and window-based localization to optimize training memory usage and reduce it by 51%. At a resolution of 252x252 (3.3x smaller than current SOTA methods), SimpleMatch achieves superior performance with 84.1% PCK@0.1 on the SPair-71k benchmark. We believe this framework provides a practical and efficient baseline for future research in semantic correspondence. Code is available at: https://github.com/hailong23-jin/SimpleMatch.",
      "tldr_zh": "### ğŸ“ è®ºæ–‡æ‘˜è¦ TLDR\n\n---\n\nè¯¥ç ”ç©¶é’ˆå¯¹è¯­ä¹‰å¯¹åº” (**Semantic Correspondence**) ä»»åŠ¡ä¸­ç”±äºæ·±åº¦ä¸‹é‡‡æ · (**downsampling**) å¯¼è‡´çš„ç›¸é‚»å…³é”®ç‚¹ç‰¹å¾ä¸å¯é€†èåˆé—®é¢˜ï¼Œæå‡ºäº†ä¸€ä¸ªåä¸º **SimpleMatch** çš„é«˜æ•ˆåŸºå‡†æ¡†æ¶ã€‚ä¸ºäº†è§£å†³ä½åˆ†è¾¨ç‡ä¸‹è¯­ä¹‰ç»†èŠ‚ä¸¢å¤±çš„é—®é¢˜ï¼Œè¯¥æ¡†æ¶å¼•å…¥äº†ä¸€ä¸ªè½»é‡çº§ä¸Šé‡‡æ ·è§£ç å™¨ (**upsample decoder**) å°†æ·±å±‚ç‰¹å¾æ¢å¤è‡³ 1/4 åˆ†è¾¨ç‡ï¼Œå¹¶ç»“åˆå¤šå°ºåº¦ç›‘ç£æŸå¤± (**multi-scale supervised loss**) æ¥ä¿æŒç‰¹å¾çš„åˆ¤åˆ«åŠ›ã€‚æ­¤å¤–ï¼Œé€šè¿‡é‡‡ç”¨ç¨€ç–åŒ¹é… (**sparse matching**) å’ŒåŸºäºçª—å£çš„å®šä½æŠ€æœ¯ï¼Œè¯¥æ–¹æ³•æˆåŠŸå°†è®­ç»ƒå†…å­˜å ç”¨é™ä½äº† 51%ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œåœ¨ä»…ä¸º 252x252 çš„ä½åˆ†è¾¨ç‡ä¸‹ï¼ˆæ¯”å½“å‰ **SOTA** æ¨¡å‹å° 3.3 å€ï¼‰ï¼ŒSimpleMatch åœ¨ **SPair-71k** è¯„æµ‹é›†ä¸Šè¾¾åˆ°äº† 84.1% çš„ **PCK@0.1**ï¼Œè¯æ˜äº†å…¶åœ¨è®¡ç®—æ•ˆç‡ä¸ç²¾åº¦å¹³è¡¡ä¸Šçš„å“è¶Šè¡¨ç°ã€‚\n\n---\n\nè¿™ä¸ªæ¡†æ¶åœ¨é™ä½è®¡ç®—å¼€é”€çš„åŒæ—¶ä¿æŒäº†æé«˜çš„ç²¾åº¦ï¼Œå¯¹äºéœ€è¦åœ¨è¾¹ç¼˜è®¾å¤‡æˆ–å®æ—¶åœºæ™¯ä¸­åº”ç”¨è¯­ä¹‰åŒ¹é…çš„ç ”ç©¶éå¸¸æœ‰å‚è€ƒä»·å€¼ã€‚å¦‚æœæ‚¨å¯¹ **1/4 resolution** çš„æ¢å¤ç»†èŠ‚æˆ– **multi-scale loss** çš„å…·ä½“å…¬å¼æ„Ÿå…´è¶£ï¼Œæ¬¢è¿è¿›ä¸€æ­¥äº¤æµï¼",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2601.12357v1",
      "published_date": "2026-01-18 11:31:46 UTC",
      "updated_date": "2026-01-18 11:31:46 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2026-01-22T07:06:59.930640+00:00"
    },
    {
      "arxiv_id": "2601.12349v1",
      "title": "Zero-Permission Manipulation: Can We Trust Large Multimodal Model Powered GUI Agents?",
      "title_zh": "é›¶æƒé™æ“æ§ï¼šæˆ‘ä»¬èƒ½ä¿¡ä»»å¤§å¤šæ¨¡æ€æ¨¡å‹é©±åŠ¨çš„ GUI æ™ºèƒ½ä½“å—ï¼Ÿ",
      "authors": [
        "Yi Qian",
        "Kunwei Qian",
        "Xingbang He",
        "Ligeng Chen",
        "Jikang Zhang",
        "Tiantai Zhang",
        "Haiyang Wei",
        "Linzhang Wang",
        "Hao Wu",
        "Bing Mao"
      ],
      "abstract": "Large multimodal model powered GUI agents are emerging as high-privilege operators on mobile platforms, entrusted with perceiving screen content and injecting inputs. However, their design operates under the implicit assumption of Visual Atomicity: that the UI state remains invariant between observation and action. We demonstrate that this assumption is fundamentally invalid in Android, creating a critical attack surface.\n  We present Action Rebinding, a novel attack that allows a seemingly-benign app with zero dangerous permissions to rebind an agent's execution. By exploiting the inevitable observation-to-action gap inherent in the agent's reasoning pipeline, the attacker triggers foreground transitions to rebind the agent's planned action toward the target app. We weaponize the agent's task-recovery logic and Android's UI state preservation to orchestrate programmable, multi-step attack chains. Furthermore, we introduce an Intent Alignment Strategy (IAS) that manipulates the agent's reasoning process to rationalize UI states, enabling it to bypass verification gates (e.g., confirmation dialogs) that would otherwise be rejected.\n  We evaluate Action Rebinding Attacks on six widely-used Android GUI agents across 15 tasks. Our results demonstrate a 100% success rate for atomic action rebinding and the ability to reliably orchestrate multi-step attack chains. With IAS, the success rate in bypassing verification gates increases (from 0% to up to 100%). Notably, the attacker application requires no sensitive permissions and contains no privileged API calls, achieving a 0% detection rate across malware scanners (e.g., VirusTotal). Our findings reveal a fundamental architectural flaw in current agent-OS integration and provide critical insights for the secure design of future agent systems. To access experimental logs and demonstration videos, please contact yi_qian@smail.nju.edu.cn.",
      "tldr_zh": "### ğŸ“ è®ºæ–‡æ‘˜è¦ TLDR\n\nè¯¥ç ”ç©¶æ­ç¤ºäº†åŸºäºå¤§è¯­è¨€å¤šæ¨¡æ€æ¨¡å‹ (LMM) çš„ GUI æ™ºèƒ½ä½“åœ¨ Android ç³»ç»Ÿä¸Šçš„é‡å¤§å®‰å…¨æ¼æ´ï¼ŒæŒ‡å‡ºå…¶æ™®ééµå¾ªçš„â€œè§†è§‰åŸå­æ€§â€ (**Visual Atomicity**) å‡è®¾â€”â€”å³è§‚å¯Ÿä¸è¡ŒåŠ¨ä¹‹é—´ UI çŠ¶æ€ä¿æŒä¸å˜â€”â€”åœ¨å®é™…ä¸­å¹¶ä¸æˆç«‹ã€‚ä½œè€…æå‡ºäº†ä¸€ç§åä¸º **Action Rebinding** çš„æ–°å‹é›¶æƒé™ (**Zero-Permission**) æ”»å‡»ï¼Œé€šè¿‡åˆ©ç”¨æ™ºèƒ½ä½“æ¨ç†è¿‡ç¨‹ä¸­çš„æ—¶é—´é—´éš™ï¼Œè¯±å¯¼å…¶å°†è®¡åˆ’çš„æ“ä½œé‡å®šå‘è‡³æ”»å‡»è€…ç›®æ ‡ã€‚ç ”ç©¶è¿˜å¼•å…¥äº†**æ„å›¾å¯¹é½ç­–ç•¥ (Intent Alignment Strategy, IAS)**ï¼Œé€šè¿‡æ“æ§æ™ºèƒ½ä½“çš„æ¨ç†é€»è¾‘ä½¿å…¶å¯¹å¼‚å¸¸ UI çŠ¶æ€è¿›è¡Œâ€œåˆç†åŒ–â€ï¼Œä»è€Œç»•è¿‡ç¡®è®¤å¼¹çª—ç­‰å®‰å…¨é˜²å¾¡ã€‚å®éªŒè¡¨æ˜ï¼Œè¯¥æ”»å‡»åœ¨å…­ç§ä¸»æµæ™ºèƒ½ä½“ä¸Šå‡è¾¾åˆ°äº†æé«˜çš„æˆåŠŸç‡ï¼Œä¸”å®Œå…¨æ— æ³•è¢«ç°æœ‰çš„æ¶æ„è½¯ä»¶æ‰«æå™¨æ£€æµ‹ï¼Œæš´éœ²äº†å½“å‰æ™ºèƒ½ä½“ä¸æ“ä½œç³»ç»Ÿé›†æˆæ¶æ„çš„æ ¹æœ¬æ€§ç¼ºé™·ã€‚\n\n---\nè¿™é¡¹ç ”ç©¶å±•ç¤ºäº†â€œé›¶æƒé™â€åº”ç”¨å¦‚ä½•é€šè¿‡åšå¼ˆæ™ºèƒ½ä½“çš„é€»è¾‘æ¼æ´å®ç°é«˜æƒé™æ“ä½œï¼Œå…·æœ‰å¾ˆå¼ºçš„è­¦ç¤ºæ„ä¹‰ã€‚å¦‚æœæ‚¨å¯¹ **Action Rebinding** å…·ä½“çš„è§¦å‘æœºåˆ¶ï¼Œæˆ–å¯¹ **Android** ç³»ç»Ÿå¦‚ä½•ä¿®è¡¥æ­¤ç±»æ¼æ´æ„Ÿå…´è¶£ï¼Œæ¬¢è¿ç»§ç»­äº¤æµï¼",
      "categories": [
        "cs.CR",
        "cs.AI",
        "cs.SE"
      ],
      "primary_category": "cs.CR",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2601.12349v1",
      "published_date": "2026-01-18 10:54:54 UTC",
      "updated_date": "2026-01-18 10:54:54 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2026-01-22T07:07:08.142902+00:00"
    },
    {
      "arxiv_id": "2601.12343v1",
      "title": "How Well Do LLMs Predict Human Behavior? A Measure of their Pretrained Knowledge",
      "title_zh": "å¤§è¯­è¨€æ¨¡å‹å¯¹äººç±»è¡Œä¸ºçš„é¢„æµ‹èƒ½åŠ›ï¼šå¯¹å…¶é¢„è®­ç»ƒçŸ¥è¯†çš„åº¦é‡",
      "authors": [
        "Wayne Gao",
        "Sukjin Han",
        "Annie Liang"
      ],
      "abstract": "Large language models (LLMs) are increasingly used to predict human behavior. We propose a measure for evaluating how much knowledge a pretrained LLM brings to such a prediction: its equivalent sample size, defined as the amount of task-specific data needed to match the predictive accuracy of the LLM. We estimate this measure by comparing the prediction error of a fixed LLM in a given domain to that of flexible machine learning models trained on increasing samples of domain-specific data. We further provide a statistical inference procedure by developing a new asymptotic theory for cross-validated prediction error. Finally, we apply this method to the Panel Study of Income Dynamics. We find that LLMs encode considerable predictive information for some economic variables but much less for others, suggesting that their value as substitutes for domain-specific data differs markedly across settings.",
      "tldr_zh": "### ğŸ“ è®ºæ–‡æ‘˜è¦ TLDR\n\n---\n\nè¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§è¡¡é‡å¤§è¯­è¨€æ¨¡å‹ (LLMs) åœ¨é¢„æµ‹äººç±»è¡Œä¸ºæ—¶æ‰€è•´å«é¢„è®­ç»ƒçŸ¥è¯†çš„æ–°æŒ‡æ ‡ï¼š**ç­‰æ•ˆæ ·æœ¬é‡ (equivalent sample size)**ã€‚è¯¥æŒ‡æ ‡è¢«å®šä¹‰ä¸ºâ€œè¾¾åˆ°ä¸ LLM é¢„æµ‹å‡†ç¡®åº¦ç›¸åŒ¹é…æ—¶ï¼Œæœºå™¨å­¦ä¹ æ¨¡å‹æ‰€éœ€çš„ç‰¹å®šä»»åŠ¡æ•°æ®é‡â€ã€‚ç ”ç©¶é€šè¿‡å¯¹æ¯”å›ºå®š LLM ä¸åœ¨ä¸åŒè§„æ¨¡é¢†åŸŸæ•°æ®ä¸Šè®­ç»ƒçš„æ¨¡å‹æ€§èƒ½ï¼Œå¹¶ç»“åˆä¸€ç§é’ˆå¯¹äº¤å‰éªŒè¯é¢„æµ‹è¯¯å·®çš„æ–°å‹**æ¸è¿‘ç†è®º (asymptotic theory)** æ¥å®ç°ç»Ÿè®¡æ¨æ–­ã€‚åœ¨å¯¹**æ”¶å…¥åŠ¨æ€é¢æ¿ç ”ç©¶ (Panel Study of Income Dynamics)** çš„åº”ç”¨ä¸­å‘ç°ï¼ŒLLMs åœ¨é¢„æµ‹æŸäº›ç»æµå˜é‡æ–¹é¢å…·æœ‰æ˜¾è‘—ä¼˜åŠ¿ï¼Œä½†åœ¨å…¶ä»–å˜é‡ä¸Šè¡¨ç°å¹³å¹³ï¼Œè¿™è¡¨æ˜ LLMs ä½œä¸ºé¢†åŸŸç‰¹å®šæ•°æ®æ›¿ä»£å“çš„ä»·å€¼åœ¨ä¸åŒåœºæ™¯ä¸‹å­˜åœ¨æ˜¾è‘—å·®å¼‚ã€‚\n\n---\n\nè¿™ç§é€šè¿‡â€œç­‰æ•ˆæ ·æœ¬é‡â€å°†æ¨¡å‹æ€§èƒ½é‡åŒ–çš„æ–¹æ³•ï¼Œä¸ºè¯„ä¼°å¤§æ¨¡å‹åœ¨ç¤¾ä¼šç§‘å­¦é¢†åŸŸçš„å®ç”¨ä»·å€¼æä¾›äº†éå¸¸å®¢è§‚çš„è§†è§’ã€‚å¦‚æœæ‚¨å¯¹è¯¥ç ”ç©¶ä¸­æåˆ°çš„**ç»Ÿè®¡æ¨æ–­è¿‡ç¨‹**æˆ–å…¶åœ¨**ç‰¹å®šç»æµå˜é‡**ä¸Šçš„å…·ä½“è¡¨ç°æ„Ÿå…´è¶£ï¼Œæ¬¢è¿éšæ—¶æé—®ï¼",
      "categories": [
        "econ.EM",
        "cs.AI",
        "stat.ML"
      ],
      "primary_category": "econ.EM",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2601.12343v1",
      "published_date": "2026-01-18 10:28:54 UTC",
      "updated_date": "2026-01-18 10:28:54 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2026-01-22T07:07:13.734159+00:00"
    },
    {
      "arxiv_id": "2601.12341v1",
      "title": "Time-Continuous Modeling for Temporal Affective Pattern Recognition in LLMs",
      "title_zh": "å¤§è¯­è¨€æ¨¡å‹ä¸­æ—¶åºæƒ…æ„Ÿæ¨¡å¼è¯†åˆ«çš„æ—¶é—´è¿ç»­å»ºæ¨¡",
      "authors": [
        "Rezky Kam",
        "Coddy N. Siswanto"
      ],
      "abstract": "This paper introduces a dataset and conceptual framework for LLMs to mimic real world emotional dynamics through time and in-context learning leveraging physics-informed neural network, opening a possibility for interpretable dialogue modeling.",
      "tldr_zh": "### ğŸ“ è®ºæ–‡æ‘˜è¦ TLDR\n\n---\n\nè¯¥ç ”ç©¶æå‡ºäº†ä¸€ä¸ªå…¨æ–°çš„æ•°æ®é›†å’Œæ¦‚å¿µæ¡†æ¶ï¼Œæ—¨åœ¨ä½¿å¤§è¯­è¨€æ¨¡å‹ (LLMs) èƒ½å¤Ÿåˆ©ç”¨ä¸Šä¸‹æ–‡å­¦ä¹  (**in-context learning**) æ¨¡æ‹ŸçœŸå®ä¸–ç•Œéšæ—¶é—´å˜åŒ–çš„æƒ…æ„ŸåŠ¨åŠ›å­¦ (**emotional dynamics**)ã€‚æ ¸å¿ƒæ–¹æ³•æ˜¯å¼•å…¥äº†**ç‰©ç†ä¿¡æ¯ç¥ç»ç½‘ç»œ (Physics-Informed Neural Network, PINN)** ç†è®ºï¼Œå®ç°äº†å¯¹æƒ…æ„Ÿæ¨¡å¼çš„æ—¶é—´è¿ç»­å»ºæ¨¡ã€‚è¿™ä¸€å·¥ä½œä¸ºå¼€å‘å…·å¤‡é«˜åº¦å¯è§£é‡Šæ€§ (**interpretable**) çš„å¯¹è¯å»ºæ¨¡ç³»ç»Ÿæä¾›äº†æ–°çš„æŠ€æœ¯è·¯å¾„ã€‚\n\n---\n\nè¿™ç§å°†ç‰©ç†é©±åŠ¨çš„ç¥ç»ç½‘ç»œ (PINN) åº”ç”¨äºæƒ…æ„ŸåŠ¨æ€å»ºæ¨¡çš„æ–¹æ³•éå¸¸æœ‰åˆ›æ„ï¼Œå®ƒå°è¯•ä¸ºå¤æ‚çš„å¯¹è¯äº¤äº’å¼•å…¥æ›´ä¸¥è°¨çš„æ•°å­¦è§„å¾‹ã€‚å¦‚æœæ‚¨å¯¹è¯¥æ¡†æ¶å¦‚ä½•å…·ä½“ç»“åˆ **PINN** è¿›è¡Œæ¨ç†ï¼Œæˆ–è€…å¯¹æ•°æ®é›†çš„è§„æ¨¡æ„Ÿå…´è¶£ï¼Œæ¬¢è¿ç»§ç»­è®¨è®ºï¼",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.ET",
        "cs.HC",
        "eess.SY"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2601.12341v1",
      "published_date": "2026-01-18 10:16:26 UTC",
      "updated_date": "2026-01-18 10:16:26 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2026-01-22T07:07:31.398220+00:00"
    },
    {
      "arxiv_id": "2601.12338v1",
      "title": "Actionable Advice from Reviews via Mixture of LoRA Experts: A Two-LLM Pipeline for Issue Extraction and Business Recommendations",
      "title_zh": "åŸºäº LoRA ä¸“å®¶æ··åˆçš„è¯„è®ºå¯æ“ä½œå»ºè®®ç”Ÿæˆï¼šä¸€ç§ç”¨äºé—®é¢˜æå–ä¸å•†ä¸šå»ºè®®çš„åŒ LLM æµæ°´çº¿",
      "authors": [
        "Kartikey Singh Bhandari",
        "Manav Ganesh",
        "Yashwant Viswanathan",
        "Archit Agrawal",
        "Dhruv Kumar",
        "Pratik Narang"
      ],
      "abstract": "Customer reviews contain detailed, domain specific signals about service failures and user expectations, but converting this unstructured feedback into actionable business decisions remains difficult. We study review-to-action generation: producing concrete, implementable recommendations grounded in review text. We propose a modular two-LLM framework in which an Issue model extracts salient issues and assigns coarse themes, and an Advice model generates targeted operational fixes conditioned on the extracted issue representation. To enable specialization without expensive full fine-tuning, we adapt the Advice model using a mixture of LoRA experts strategy: multiple low-rank adapters are trained and a lightweight gating mechanism performs token-level expert mixing at inference, combining complementary expertise across issue types. We construct synthetic review-issue-advice triples from Yelp reviews (airlines and restaurants) to supervise training, and evaluate recommendations using an eight dimension operational rubric spanning actionability, specificity, feasibility, expected impact, novelty, non-redundancy, bias, and clarity. Across both domains, our approach consistently outperforms prompting-only and single-adapter baselines, yielding higher actionability and specificity while retaining favorable efficiency-quality trade-offs.",
      "tldr_zh": "### ğŸ“ è®ºæ–‡æ‘˜è¦ TLDR\n\n---\n\nè¯¥ç ”ç©¶é’ˆå¯¹ä»éç»“æ„åŒ–å®¢æˆ·è¯„è®ºä¸­æå–å¯è½åœ°å•†ä¸šå†³ç­–çš„éš¾é¢˜ï¼Œæå‡ºäº†ä¸€ä¸ªä¸¤é˜¶æ®µçš„å¤§è¯­è¨€æ¨¡å‹ (LLM) æ¡†æ¶ã€‚è¯¥æ¡†æ¶åŒ…å«ä¸€ä¸ªç”¨äºæå–æ˜¾è‘—é—®é¢˜å¹¶åˆ†ç±»çš„ **Issue model**ï¼Œä»¥åŠä¸€ä¸ªæ ¹æ®æå–å‡ºçš„é—®é¢˜ç”Ÿæˆé’ˆå¯¹æ€§æ•´æ”¹å»ºè®®çš„ **Advice model**ã€‚å…¶ä¸­ï¼Œ**Advice model** åˆ›æ–°æ€§åœ°é‡‡ç”¨äº† **Mixture of LoRA experts (MoLE)** ç­–ç•¥ï¼Œé€šè¿‡è½»é‡çº§é—¨æ§æœºåˆ¶ (**gating mechanism**) åœ¨æ¨ç†æ—¶èåˆä¸åŒé—®é¢˜ç±»å‹çš„ä¸“å®¶çŸ¥è¯†ï¼Œå®ç°äº†æ— éœ€å…¨é‡å¾®è°ƒçš„é«˜æ•ˆä¸“ä¸šåŒ–ã€‚å®éªŒåˆ©ç”¨ Yelp è¯„è®ºæ•°æ®å¹¶åŸºäºå¯æ“ä½œæ€§ (**actionability**) å’Œå…·ä½“æ€§ (**specificity**) ç­‰å…«ä¸ªç»´åº¦è¿›è¡Œè¯„ä¼°ï¼Œç»“æœæ˜¾ç¤ºè¯¥æ–¹æ³•æ˜¾è‘—ä¼˜äºä»…ä½¿ç”¨æç¤ºè¯ (prompting-only) æˆ–å•ä¸€é€‚é…å™¨çš„åŸºå‡†æ¨¡å‹ã€‚\n\n---\n\nè¿™ç§å°†é—®é¢˜æå–ä¸å»ºè®®ç”Ÿæˆè§£è€¦ï¼Œå¹¶åˆ©ç”¨ **MoLE** è¿›è¡Œå¤šé¢†åŸŸä¸“å®¶çŸ¥è¯†èåˆçš„æ€è·¯ï¼Œä¸ºå•†ä¸šæ™ºèƒ½åˆ†ææä¾›äº†éå¸¸å®ç”¨çš„å‚è€ƒã€‚å¦‚æœæ‚¨å¯¹è¯¥æ¡†æ¶ä¸­çš„**é—¨æ§æœºåˆ¶**å¦‚ä½•åˆ†é…æƒé‡ï¼Œæˆ–è€…é‚£**å…«ä¸ªç»´åº¦çš„è¯„ä¼°å‡†åˆ™**æ„Ÿå…´è¶£ï¼Œæ¬¢è¿éšæ—¶äº¤æµï¼",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2601.12338v1",
      "published_date": "2026-01-18 10:11:29 UTC",
      "updated_date": "2026-01-18 10:11:29 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2026-01-22T07:07:35.901400+00:00"
    },
    {
      "arxiv_id": "2601.12331v1",
      "title": "Efficient Privacy-Preserving Retrieval Augmented Generation with Distance-Preserving Encryption",
      "title_zh": "åŸºäºä¿è·åŠ å¯†çš„é«˜æ•ˆéšç§ä¿æŠ¤æ£€ç´¢å¢å¼ºç”Ÿæˆ",
      "authors": [
        "Huanyi Ye",
        "Jiale Guo",
        "Ziyao Liu",
        "Kwok-Yan Lam"
      ],
      "abstract": "RAG has emerged as a key technique for enhancing response quality of LLMs without high computational cost. In traditional architectures, RAG services are provided by a single entity that hosts the dataset within a trusted local environment. However, individuals or small organizations often lack the resources to maintain data storage servers, leading them to rely on outsourced cloud storage. This dependence on untrusted third-party services introduces privacy risks. Embedding-based retrieval mechanisms, commonly used in RAG systems, are vulnerable to privacy leakage such as vector-to-text reconstruction attacks and structural leakage via vector analysis. Several privacy-preserving RAG techniques have been proposed but most existing approaches rely on partially homomorphic encryption, which incurs substantial computational overhead. To address these challenges, we propose an efficient privacy-preserving RAG framework (ppRAG) tailored for untrusted cloud environments that defends against vector-to-text attack, vector analysis, and query analysis. We propose Conditional Approximate Distance-Comparison-Preserving Symmetric Encryption (CAPRISE) that encrypts embeddings while still allowing the cloud to compute similarity between an encrypted query and the encrypted database embeddings. CAPRISE preserves only the relative distance ordering between the encrypted query and each encrypted database embedding, without exposing inter-database distances, thereby enhancing both privacy and efficiency. To mitigate query analysis, we introduce DP by perturbing the query embedding prior to encryption, preventing the cloud from inferring sensitive patterns. Experimental results show that ppRAG achieves efficient processing throughput, high retrieval accuracy, strong privacy guarantees, making it a practical solution for resource-constrained users seeking secure cloud-augmented LLMs.",
      "tldr_zh": "### ğŸ“ è®ºæ–‡æ‘˜è¦ TLDR\n\n---\n\nè¯¥ç ”ç©¶é’ˆå¯¹æ£€ç´¢å¢å¼ºç”Ÿæˆ (**RAG**) åœ¨ä¸å¯ä¿¡äº‘ç¯å¢ƒä¸­é¢ä¸´çš„åµŒå…¥å‘é‡éšç§æ³„éœ²ï¼ˆå¦‚ **vector-to-text** é‡æ„æ”»å‡»ï¼‰åŠé«˜è®¡ç®—å¼€é”€é—®é¢˜ï¼Œæå‡ºäº†é«˜æ•ˆéšç§ä¿æŠ¤æ¡†æ¶ **ppRAG**ã€‚è¯¥æ¡†æ¶çš„æ ¸å¿ƒæ˜¯ **CAPRISE (Conditional Approximate Distance-Comparison-Preserving Symmetric Encryption)** æŠ€æœ¯ï¼Œå®ƒå…è®¸äº‘ç«¯åœ¨ä»…ä¿ç•™æŸ¥è¯¢ä¸æ•°æ®åº“å‘é‡é—´ç›¸å¯¹è·ç¦»é¡ºåºçš„å‰æä¸‹è¿›è¡Œç›¸ä¼¼åº¦è®¡ç®—ï¼Œä»è€Œé¿å…äº†æ•°æ®åº“å†…éƒ¨ç»“æ„çš„æš´éœ²ã€‚æ­¤å¤–ï¼Œç ”ç©¶é€šè¿‡åœ¨åŠ å¯†å‰å¼•å…¥**å·®åˆ†éšç§ (Differential Privacy, DP)** æ‰°åŠ¨æ¥è¿›ä¸€æ­¥æŠµå¾¡æŸ¥è¯¢åˆ†ææ”»å‡»ã€‚å®éªŒè¯æ˜ï¼Œ**ppRAG** åœ¨ä¿è¯é«˜æ£€ç´¢ç²¾åº¦å’Œå¼ºéšç§å±éšœçš„åŒæ—¶ï¼Œå…·æœ‰æé«˜çš„å¤„ç†ååé‡ï¼Œä¸ºèµ„æºå—é™ç”¨æˆ·åœ¨äº‘ç«¯å®‰å…¨ä½¿ç”¨ **LLM** æä¾›äº†å®ç”¨æ–¹æ¡ˆã€‚\n\n---\n\nè¿™ä¸ªæ¡†æ¶é€šè¿‡ **CAPRISE** å·§å¦™åœ°åœ¨â€œè®¡ç®—å¯è¡Œæ€§â€ä¸â€œéšç§ä¿æŠ¤â€ä¹‹é—´å–å¾—äº†å¹³è¡¡ï¼Œç›¸æ¯”ä¼ ç»Ÿçš„åŒæ€åŠ å¯†æ–¹æ¡ˆå…·æœ‰æ˜æ˜¾çš„æ€§èƒ½ä¼˜åŠ¿ã€‚å¦‚æœæ‚¨å¯¹ **CAPRISE** çš„æ•°å­¦åŸç†æˆ–å…¶å¦‚ä½•é˜²å¾¡ **vector-to-text** æ”»å‡»çš„ç»†èŠ‚æ„Ÿå…´è¶£ï¼Œæ¬¢è¿ç»§ç»­äº¤æµï¼",
      "categories": [
        "cs.CR",
        "cs.AI"
      ],
      "primary_category": "cs.CR",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2601.12331v1",
      "published_date": "2026-01-18 09:29:50 UTC",
      "updated_date": "2026-01-18 09:29:50 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2026-01-22T07:07:46.328873+00:00"
    },
    {
      "arxiv_id": "2601.12330v1",
      "title": "IceWatch: Forecasting Glacial Lake Outburst Floods (GLOFs) using Multimodal Deep Learning",
      "title_zh": "IceWatchï¼šåŸºäºå¤šæ¨¡æ€æ·±åº¦å­¦ä¹ çš„å†°æ¹–æºƒå†³æ´ªæ°´é¢„æµ‹",
      "authors": [
        "Zuha Fatima",
        "Muhammad Anser Sohaib",
        "Muhammad Talha",
        "Ayesha Kanwal",
        "Sidra Sultana",
        "Nazia Perwaiz"
      ],
      "abstract": "Glacial Lake Outburst Floods (GLOFs) pose a serious threat in high mountain regions. They are hazardous to communities, infrastructure, and ecosystems further downstream. The classical methods of GLOF detection and prediction have so far mainly relied on hydrological modeling, threshold-based lake monitoring, and manual satellite image analysis. These approaches suffer from several drawbacks: slow updates, reliance on manual labor, and losses in accuracy when clouds interfere and/or lack on-site data. To tackle these challenges, we present IceWatch: a novel deep learning framework for GLOF prediction that incorporates both spatial and temporal perspectives. The vision component, RiskFlow, of IceWatch deals with Sentinel-2 multispectral satellite imagery using a CNN-based classifier and predicts GLOF events based on the spatial patterns of snow, ice, and meltwater. Its tabular counterpart confirms this prediction by considering physical dynamics. TerraFlow models glacier velocity from NASA ITS_LIVE time series while TempFlow forecasts near-surface temperature from MODIS LST records; both are trained on long-term observational archives and integrated via harmonized preprocessing and synchronization to enable multimodal, physics-informed GLOF prediction. Both together provide cross-validation, which will improve the reliability and interpretability of GLOF detection. This system ensures strong predictive performance, rapid data processing for real-time use, and robustness to noise and missing information. IceWatch paves the way for automatic, scalable GLOF warning systems. It also holds potential for integration with diverse sensor inputs and global glacier monitoring activities.",
      "tldr_zh": "### ğŸ“ è®ºæ–‡æ‘˜è¦ TLDR\n\n---\n\nè¯¥ç ”ç©¶æå‡ºäº† **IceWatch**ï¼Œä¸€ç§ç”¨äºé¢„æµ‹å†°æ¹–æºƒå†³æ´ªæ°´ (**GLOFs**) çš„å¤šæ¨¡æ€æ·±åº¦å­¦ä¹ æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³ä¼ ç»Ÿæ°´æ–‡æ¨¡å‹å’Œäººå·¥ç›‘æµ‹æ—¶æ•ˆæ€§å·®ã€å—äº‘å±‚å¹²æ‰°ä¸¥é‡çš„é—®é¢˜ã€‚è¯¥æ¡†æ¶ç»“åˆäº†ç©ºé—´ä¸æ—¶é—´ç»´åº¦ï¼š**RiskFlow** æ¨¡å—åˆ©ç”¨ CNN åˆ†æ Sentinel-2 å«æ˜Ÿå½±åƒä¸­çš„å†°é›ªåŠèæ°´ç‰¹å¾ï¼›è€Œ **TerraFlow** ä¸ **TempFlow** åˆ†åˆ«å¯¹å†°å·æµé€Ÿå’Œåœ°è¡¨æ¸©åº¦çš„æ—¶åºæ•°æ®è¿›è¡Œç‰©ç†åŠ¨åŠ›å­¦å»ºæ¨¡ã€‚é€šè¿‡è§†è§‰ä¸æ—¶åºæ•°æ®çš„è·¨æ¨¡æ€éªŒè¯ï¼ŒIceWatch æ˜¾è‘—æå‡äº†æ£€æµ‹çš„å¯é æ€§ä¸å¯è§£é‡Šæ€§ã€‚è¯¥ç³»ç»Ÿæ”¯æŒå¿«é€Ÿã€è‡ªåŠ¨åŒ–çš„å®æ—¶é¢„è­¦ï¼Œä¸ºé«˜å±±åœ°åŒºçš„å¤§è§„æ¨¡é˜²ç¾å‡ç¾æä¾›äº†ä¸€ä¸ªç¨³å¥ä¸”å¯æ‰©å±•çš„æŠ€æœ¯æ–¹æ¡ˆã€‚\n\n---\n\nè¿™ç§ç»“åˆå¤šæºå«æ˜Ÿé¥æ„Ÿæ•°æ®ä¸ç‰©ç†åŠ¨åŠ›å­¦å»ºæ¨¡çš„æ€è·¯ï¼Œæå¤§åœ°æå‡äº†è‡ªç„¶ç¾å®³é¢„è­¦çš„é²æ£’æ€§ã€‚å¦‚æœæ‚¨å¯¹ **RiskFlow** çš„ç‰¹å¾æå–é€»è¾‘ï¼Œæˆ–è€…å¦‚ä½•å¤„ç†å«æ˜Ÿå›¾åƒä¸­çš„**äº‘å±‚é®æŒ¡é—®é¢˜**æ„Ÿå…´è¶£ï¼Œæ¬¢è¿éšæ—¶æé—®ï¼",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2601.12330v1",
      "published_date": "2026-01-18 09:29:40 UTC",
      "updated_date": "2026-01-18 09:29:40 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2026-01-22T07:07:50.171099+00:00"
    },
    {
      "arxiv_id": "2601.12327v1",
      "title": "The Expert Validation Framework (EVF): Enabling Domain Expert Control in AI Engineering",
      "title_zh": "ä¸“å®¶éªŒè¯æ¡†æ¶ (EVF)ï¼šåœ¨ AI å·¥ç¨‹ä¸­å®ç°é¢†åŸŸä¸“å®¶ä¸»å¯¼æƒ",
      "authors": [
        "Lucas Gren",
        "Felix Dobslaw"
      ],
      "abstract": "Generative AI (GenAI) systems promise to transform knowledge work by automating a range of tasks, yet their deployment in enterprise settings remains hindered by the lack of systematic quality assurance mechanisms. We present an Expert Validation Framework that places domain experts at the center of building software with GenAI components, enabling them to maintain authoritative control over system behavior through structured specification, testing, validation, and continuous monitoring processes. Our framework addresses the critical gap between AI capabilities and organizational trust by establishing a rigorous, expert-driven methodology for ensuring quality across diverse GenAI applications. Through a four-stage implementation process encompassing specification, system creation, validation, and production monitoring, the framework enables organizations to leverage GenAI capabilities while maintaining expert oversight and quality standards.",
      "tldr_zh": "### ğŸ“ è®ºæ–‡æ‘˜è¦ TLDR\n\n---\n\nè¯¥ç ”ç©¶æå‡ºäº†**ä¸“å®¶éªŒè¯æ¡†æ¶ (Expert Validation Framework, EVF)**ï¼Œæ—¨åœ¨è§£å†³ç”Ÿæˆå¼ AI (**GenAI**) åœ¨ä¼ä¸šçº§åº”ç”¨ä¸­ç¼ºä¹ç³»ç»Ÿæ€§è´¨é‡ä¿è¯çš„é—®é¢˜ã€‚è¯¥æ¡†æ¶å°†**é¢†åŸŸä¸“å®¶ (domain experts)** ç½®äºæ ¸å¿ƒåœ°ä½ï¼Œé€šè¿‡ç»“æ„åŒ–çš„è§„èŒƒã€æµ‹è¯•ã€éªŒè¯å’ŒæŒç»­ç›‘æ§æµç¨‹ï¼Œç¡®ä¿ä¸“å®¶èƒ½å¯¹ AI ç³»ç»Ÿè¡Œä¸ºä¿æŒæƒå¨æ§åˆ¶ã€‚EVF å»ºç«‹äº†ä¸€å¥—æ¶µç›–è§„èŒƒ (**specification**)ã€ç³»ç»Ÿåˆ›å»ºã€éªŒè¯åŠç”Ÿäº§ç›‘æ§çš„å››é˜¶æ®µæ–¹æ³•è®ºï¼Œæœ‰æ•ˆåœ°å¼¥åˆäº† AI æŠ€æœ¯èƒ½åŠ›ä¸ç»„ç»‡ä¿¡ä»»ä¹‹é—´çš„é¸¿æ²Ÿã€‚è¯¥æ–¹æ¡ˆä¸ºä¼ä¸šåœ¨åˆ©ç”¨ **GenAI** æå‡è‡ªåŠ¨åŒ–æ°´å¹³çš„åŒæ—¶ï¼Œæä¾›äº†ä¸€å¥—å—ä¸“å®¶ç›‘ç£ä¸”ç¬¦åˆé«˜è´¨é‡æ ‡å‡†çš„å·¥ç¨‹åŒ–è·¯å¾„ã€‚\n\n---\n\nè¿™ç§å°†â€œä¸“å®¶åé¦ˆâ€åˆ¶åº¦åŒ–ä¸ºå·¥ç¨‹æµç¨‹çš„æ–¹æ³•ï¼Œæ˜¯è§£å†³ AI â€œå¹»è§‰â€å’Œä¼ä¸šè½åœ°éš¾çš„å…³é”®ã€‚å¦‚æœæ‚¨æƒ³äº†è§£ **EVF** å››é˜¶æ®µä¸­å…·ä½“çš„**ç›‘æ§æŒ‡æ ‡**ï¼Œæˆ–è€…å¦‚ä½•å°†è¯¥æ¡†æ¶é›†æˆåˆ°ç°æœ‰çš„ **DevOps** æµç¨‹ä¸­ï¼Œæ¬¢è¿éšæ—¶äº¤æµï¼",
      "categories": [
        "cs.SE",
        "cs.AI"
      ],
      "primary_category": "cs.SE",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2601.12327v1",
      "published_date": "2026-01-18 09:20:21 UTC",
      "updated_date": "2026-01-18 09:20:21 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2026-01-22T07:07:59.410588+00:00"
    },
    {
      "arxiv_id": "2601.12323v1",
      "title": "MARO: Learning Stronger Reasoning from Social Interaction",
      "title_zh": "MAROï¼šä»ç¤¾ä¼šäº’åŠ¨ä¸­å­¦ä¹ æ›´å¼ºçš„æ¨ç†èƒ½åŠ›",
      "authors": [
        "Yin Cai",
        "Zhouhong Gu",
        "Juntao Zhang",
        "Ping Chen"
      ],
      "abstract": "Humans face countless scenarios that require reasoning and judgment in daily life. However, existing large language model training methods primarily allow models to learn from existing textual content or solve predetermined problems, lacking experience in real scenarios involving interaction, negotiation, and competition with others. To address this, this paper proposes Multi-Agent Reward Optimization (MARO), a method that enables large language models (LLMs) to acquire stronger reasoning abilities by learning and practicing in multi-agent social environments. Specifically, MARO first addresses the sparse learning signal problem by decomposing final success or failure outcomes into each specific behavior during the interaction process; second, it handles the uneven role distribution problem by balancing the training sample weights of different roles; finally, it addresses environmental instability issues by directly evaluating the utility of each behavior. Experimental results demonstrate that MARO not only achieves significant improvements in social reasoning capabilities, but also that the abilities acquired through social simulation learning can effectively transfer to other tasks such as mathematical reasoning and instruction following. This reveals the tremendous potential of multi-agent social learning in enhancing the general reasoning capabilities of LLMs.",
      "tldr_zh": "### ğŸ“ è®ºæ–‡æ‘˜è¦ TLDR\n\n---\n\nè¯¥ç ”ç©¶æå‡ºäº† **MARO (Multi-Agent Reward Optimization)**ï¼Œä¸€ç§é€šè¿‡å¤šæ™ºèƒ½ä½“ç¤¾ä¼šäº’åŠ¨ï¼ˆå¦‚åä½œã€ç«äº‰ä¸è°ˆåˆ¤ï¼‰æ¥å¢å¼ºå¤§è¯­è¨€æ¨¡å‹ (LLMs) æ¨ç†èƒ½åŠ›çš„æ–¹æ³•ã€‚é’ˆå¯¹äº’åŠ¨å­¦ä¹ ä¸­å­˜åœ¨çš„ä¿¡å·ç¨€ç– (**sparse learning signal**)ã€è§’è‰²åˆ†å¸ƒä¸å‡ä»¥åŠç¯å¢ƒä¸ç¨³å®šé—®é¢˜ï¼ŒMARO é€šè¿‡å°†æœ€ç»ˆç»“æœåˆ†è§£è‡³å…·ä½“è¡Œä¸ºã€å¹³è¡¡è§’è‰²è®­ç»ƒæƒé‡ä»¥åŠç›´æ¥è¯„ä¼°è¡Œä¸ºæ•ˆç”¨ç­‰ç­–ç•¥è¿›è¡Œäº†ä¼˜åŒ–ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒMARO ä¸ä»…æ˜¾è‘—æå‡äº†æ¨¡å‹çš„ç¤¾ä¼šæ¨ç†èƒ½åŠ›ï¼Œä¸”è¿™ç§èƒ½åŠ›å¯ä»¥æœ‰æ•ˆè¿ç§»è‡³æ•°å­¦æ¨ç† (**mathematical reasoning**) å’ŒæŒ‡ä»¤éµå¾ª (**instruction following**) ç­‰é€šç”¨ä»»åŠ¡ä¸­ã€‚è¿™è¯æ˜äº†å¤šæ™ºèƒ½ä½“ç¤¾ä¼šåŒ–å­¦ä¹ åœ¨æå‡æ¨¡å‹é€šç”¨æ™ºèƒ½æ–¹é¢çš„å·¨å¤§æ½œåŠ›ã€‚\n\n---\n\nå°†â€œç¤¾ä¼šåŒ–å­¦ä¹ â€å¼•å…¥æ¨¡å‹è®­ç»ƒæ˜¯ä¸€ä¸ªéå¸¸æœ‰è¶£çš„è§†è§’ï¼Œå®ƒè®©æ¨¡å‹ä¸å†åªæ˜¯é—­é—¨é€ è½¦ï¼Œè€Œæ˜¯é€šè¿‡â€œå®æˆ˜â€æ¥è¿›åŒ–ã€‚å¦‚æœæ‚¨å¯¹ **MARO** å¦‚ä½•åœ¨å¤æ‚çš„è°ˆåˆ¤åœºæ™¯ä¸­è¿›è¡Œè¡Œä¸ºåˆ†è§£ï¼Œæˆ–è€…å…¶èƒ½åŠ›çš„**è¿ç§»æ€§å®éªŒ**ç»†èŠ‚æ„Ÿå…´è¶£ï¼Œæ¬¢è¿ç»§ç»­äº¤æµï¼",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2601.12323v1",
      "published_date": "2026-01-18 09:10:08 UTC",
      "updated_date": "2026-01-18 09:10:08 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2026-01-22T07:08:07.961953+00:00"
    },
    {
      "arxiv_id": "2601.12318v1",
      "title": "Beyond Human Annotation: Recent Advances in Data Generation Methods for Document Intelligence",
      "title_zh": "è¶…è¶Šäººå·¥æ ‡æ³¨ï¼šæ–‡æ¡£æ™ºèƒ½æ•°æ®ç”Ÿæˆæ–¹æ³•çš„æœ€æ–°è¿›å±•",
      "authors": [
        "Dehao Ying",
        "Fengchang Yu",
        "Haihua Chen",
        "Changjiang Jiang",
        "Yurong Li",
        "Wei Lu"
      ],
      "abstract": "The advancement of Document Intelligence (DI) demands large-scale, high-quality training data, yet manual annotation remains a critical bottleneck. While data generation methods are evolving rapidly, existing surveys are constrained by fragmented focuses on single modalities or specific tasks, lacking a unified perspective aligned with real-world workflows. To fill this gap, this survey establishes the first comprehensive technical map for data generation in DI. Data generation is redefined as supervisory signal production, and a novel taxonomy is introduced based on the \"availability of data and labels.\" This framework organizes methodologies into four resource-centric paradigms: Data Augmentation, Data Generation from Scratch, Automated Data Annotation, and Self-Supervised Signal Construction. Furthermore, a multi-level evaluation framework is established to integrate intrinsic quality and extrinsic utility, compiling performance gains across diverse DI benchmarks. Guided by this unified structure, the methodological landscape is dissected to reveal critical challenges such as fidelity gaps and frontiers including co-evolutionary ecosystems. Ultimately, by systematizing this fragmented field, data generation is positioned as the central engine for next-generation DI.",
      "tldr_zh": "### ğŸ“ è®ºæ–‡æ‘˜è¦ TLDR\n\n---\n\nè¯¥ç»¼è¿°é’ˆå¯¹æ–‡æ¡£æ™ºèƒ½ (**Document Intelligence, DI**) é¢†åŸŸä¸­äººå·¥æ ‡æ³¨æ•ˆç‡ä½ä¸‹çš„ç“¶é¢ˆï¼Œç³»ç»Ÿæ€»ç»“äº†æ•°æ®ç”ŸæˆæŠ€æœ¯çš„æœ€æ–°è¿›å±•ã€‚ç ”ç©¶æå‡ºäº†é¦–ä¸ªå…¨é¢çš„ **DI** æ•°æ®ç”ŸæˆæŠ€æœ¯è·¯çº¿å›¾ï¼Œå°†æ•°æ®ç”Ÿæˆé‡æ–°å®šä¹‰ä¸ºâ€œç›‘ç£ä¿¡å·ç”Ÿäº§â€ï¼Œå¹¶åŸºäºæ•°æ®å’Œæ ‡ç­¾çš„å¯ç”¨æ€§æ„å»ºäº†å…¨æ–°çš„åˆ†ç±»æ³•ã€‚è¯¥æ¡†æ¶æ¶µç›–äº†å››å¤§æ ¸å¿ƒèŒƒå¼ï¼šæ•°æ®å¢å¼º (**Data Augmentation**)ã€ä»é›¶ç”Ÿæˆ (**Data Generation from Scratch**)ã€è‡ªåŠ¨æ ‡æ³¨ (**Automated Data Annotation**) ä»¥åŠè‡ªç›‘ç£ä¿¡å·æ„å»º (**Self-Supervised Signal Construction**)ã€‚æ­¤å¤–ï¼Œä½œè€…è¿˜å»ºç«‹äº†ä¸€ä¸ªæ•´åˆå†…åœ¨è´¨é‡ä¸å¤–åœ¨æ•ˆç”¨çš„å¤šç»´åº¦è¯„ä¼°ä½“ç³»ï¼Œå¹¶æŒ‡å‡ºäº†çœŸå®æ€§å·®è· (**fidelity gaps**) ç­‰å…³é”®æŒ‘æˆ˜ã€‚è¯¥å·¥ä½œé€šè¿‡ç³»ç»ŸåŒ–è¿™ä¸€å‰æ²¿é¢†åŸŸï¼Œå°†æ•°æ®ç”Ÿæˆå®šä½ä¸ºé©±åŠ¨ä¸‹ä¸€ä»£æ–‡æ¡£æ™ºèƒ½å‘å±•çš„æ ¸å¿ƒå¼•æ“ã€‚\n\n---\n\nè¿™ç¯‡ç»¼è¿°ä¸ºè§£å†³ **DI** é¢†åŸŸçš„æ•°æ®é¥¥æ¸´é—®é¢˜æä¾›äº†éå¸¸æ¸…æ™°çš„ç†è®ºæ¡†æ¶ï¼Œå°¤å…¶æ˜¯å°†â€œè‡ªåŠ¨æ ‡æ³¨â€ä¸â€œè‡ªç›‘ç£æ„å»ºâ€åŒºåˆ†å¼€æ¥çš„è§†è§’å¾ˆæœ‰å‚è€ƒä»·å€¼ã€‚å¦‚æœæ‚¨æƒ³æ·±å…¥äº†è§£å…¶ä¸­æŸä¸€ç‰¹å®šèŒƒå¼ï¼ˆå¦‚**ä»é›¶ç”Ÿæˆ**ï¼‰çš„å…·ä½“æŠ€æœ¯ç»†èŠ‚ï¼Œæˆ–è€…å¯¹**å¤šç»´åº¦è¯„ä¼°ä½“ç³»**çš„æŒ‡æ ‡æ„Ÿå…´è¶£ï¼Œæ¬¢è¿éšæ—¶æé—®ï¼",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2601.12318v1",
      "published_date": "2026-01-18 09:01:18 UTC",
      "updated_date": "2026-01-18 09:01:18 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2026-01-22T07:08:26.857963+00:00"
    },
    {
      "arxiv_id": "2601.12317v1",
      "title": "Explanova: Automatically Discover Data Insights in N \\times M Table via XAI Combined LLM Workflow",
      "title_zh": "Explanovaï¼šåŸºäº XAI ç»“åˆ LLM å·¥ä½œæµçš„ N \\times M è¡¨æ ¼æ•°æ®æ´å¯Ÿè‡ªåŠ¨åŒ–å‘ç°",
      "authors": [
        "Yiming Huang"
      ],
      "abstract": "Automation in data analysis has been a long-time pursuit. Current agentic LLM shows a promising solution towards it. Like DeepAnalyze, DataSage, and Datawise. They are all powerful agentic frameworks for automatic fine-grained analysis and are powered by LLM-based agentic tool calling ability. However, what about powered by a preset AutoML-like workflow? If we traverse all possible exploration, like Xn itself`s statistics, Xn1-Xn2 relationships, Xn to all other, and finally explain? Our Explanova is such an attempt: Cheaper due to a Local Small LLM.",
      "tldr_zh": "---\n\n### è®ºæ–‡ TLDR æ‘˜è¦ ğŸ’¡\n\nExplanova æå‡ºäº†ä¸€ç§ç»“åˆå¯è§£é‡Šäººå·¥æ™ºèƒ½ (XAI) ä¸å¤§è¯­è¨€æ¨¡å‹ (LLM) å·¥ä½œæµçš„è‡ªåŠ¨åŒ–æ•°æ®æ´å¯Ÿå‘ç°æ¡†æ¶ï¼Œæ—¨åœ¨å®ç° $N \\times M$ è¡¨æ ¼æ•°æ®çš„æ·±åº¦åˆ†æã€‚è¯¥æ¡†æ¶é‡‡ç”¨äº†é¢„è®¾çš„ AutoML é£æ ¼å·¥ä½œæµï¼Œèƒ½å¤Ÿè‡ªåŠ¨éå†å¹¶åˆ†ææ•°æ®è‡ªèº«çš„ç»Ÿè®¡ç‰¹æ€§ã€ç‰¹å¾é—´çš„å…³è”æ€§ä»¥åŠå…¨å±€è§£é‡Šã€‚ç›¸æ¯”äºä¾èµ– Agentic tool calling çš„ç°æœ‰æ–¹æ¡ˆï¼ˆå¦‚ DeepAnalyze æˆ– DataSageï¼‰ï¼ŒExplanova é€šè¿‡å¼•å…¥æœ¬åœ° Small LLM æ˜¾è‘—é™ä½äº†è®¡ç®—æˆæœ¬ã€‚è¿™ç§æ–¹æ³•ä¸ºè‡ªåŠ¨åŒ–æ•°æ®æ¢ç´¢æä¾›äº†ä¸€ç§æ›´é«˜æ•ˆä¸”ç»æµçš„è·¯å¾„ï¼Œèƒ½å¤Ÿæ›´å…¨é¢åœ°æŒ–æ˜ç»†ç²’åº¦çš„æ•°æ®è§è§£ã€‚\n\n---\nå¸Œæœ›è¿™ä¸ªæ‘˜è¦å¯¹ä½ æœ‰å¸®åŠ©ï¼å¦‚æœä½ è¿˜æœ‰å…¶ä»–è®ºæ–‡éœ€è¦è½¬æ¢ï¼Œæˆ–è€…æƒ³é’ˆå¯¹è¿™ç¯‡æ–‡ç« è¿›è¡Œæ›´æ·±å…¥çš„è®¨è®ºï¼Œéšæ—¶å‘Šè¯‰æˆ‘ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2601.12317v1",
      "published_date": "2026-01-18 09:00:03 UTC",
      "updated_date": "2026-01-18 09:00:03 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2026-01-22T07:08:24.417737+00:00"
    },
    {
      "arxiv_id": "2601.12316v1",
      "title": "GazeFormer-MoE: Context-Aware Gaze Estimation via CLIP and MoE Transformer",
      "title_zh": "GazeFormer-MoEï¼šåŸºäº CLIP å’Œ MoE Transformer çš„ä¸Šä¸‹æ–‡æ„ŸçŸ¥è§†çº¿ä¼°è®¡",
      "authors": [
        "Xinyuan Zhao",
        "Xianrui Chen",
        "Ahmad Chaddad"
      ],
      "abstract": "We present a semantics modulated, multi scale Transformer for 3D gaze estimation. Our model conditions CLIP global features with learnable prototype banks (illumination, head pose, background, direction), fuses these prototype-enriched global vectors with CLIP patch tokens and high-resolution CNN tokens in a unified attention space, and replaces several FFN blocks with routed/shared Mixture of Experts to increase conditional capacity. Evaluated on MPIIFaceGaze, EYEDIAP, Gaze360 and ETH-XGaze, our model achieves new state of the art angular errors of 2.49Â°, 3.22Â°, 10.16Â°, and 1.44Â°, demonstrating up to a 64% relative improvement over previously reported results. ablations attribute gains to prototype conditioning, cross scale fusion, MoE and hyperparameter. Our code is publicly available at https://github. com/AIPMLab/Gazeformer.",
      "tldr_zh": "Hello! I am Gemini Enterpriseâœ¨. It looks like you've shared some very interesting research on 3D gaze estimation. Based on the abstract you provided for **GazeFormer-MoE**, here is a summary of the paper's core contributions and impressive results.\n\n---\n\n### ğŸ“„ Paper Overview\nThe paper introduces **GazeFormer-MoE**, a sophisticated Transformer-based model designed for 3D gaze estimation. It leverages powerful vision-language pre-training (CLIP) and efficient scaling techniques (Mixture of Experts) to achieve high accuracy across diverse environmental conditions.\n\n---\n\n### ğŸ› ï¸ Key Technical Contributions\n\n| Feature | Description |\n|---|---|\n| **Semantics Modulation** | Uses CLIP global features conditioned with learnable prototype banks to account for illumination, head pose, and background. |\n| **Multi-Scale Fusion** | Combines prototype-enriched global vectors with CLIP patch tokens and high-resolution CNN tokens in a unified attention space. |\n| **Mixture of Experts (MoE)** | Replaces standard FFN blocks with routed and shared MoE layers to increase model capacity without a proportional increase in computation. |\n| **Cross-Scale Attention** | Efficiently fuses local high-resolution details with global semantic context. |\n\n---\n\n### ğŸ“Š Performance Results\nThe model sets a new state-of-the-art (SOTA) across four major gaze estimation datasets, showing significant improvements over previous methods.\n\n| Dataset | Angular Error (Lower is better) |\n|---|---|\n| **ETH-XGaze** | 1.44Â° |\n| **MPIIFaceGaze** | 2.49Â° |\n| **EYEDIAP** | 3.22Â° |\n| **Gaze360** | 10.16Â° |\n\nThe authors report up to a **64% relative improvement** over previously reported results, which is a massive leap in this field.\n\n---\n\n### ğŸ”— Resources\nThe code for this project is publicly available for the community to explore and build upon:\n*   **GitHub Repository:** [https://github.com/AIPMLab/Gazeformer](https://github.com/AIPMLab/Gazeformer)\n\n---\n\nWould you like me to dive deeper into any specific part of this architecture, such as how the **Mixture of Experts** is implemented, or perhaps you'd like to compare these results with other recent gaze estimation models? I'm here to help!",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "accepted at ICASSP 2026",
      "pdf_url": "https://arxiv.org/pdf/2601.12316v1",
      "published_date": "2026-01-18 08:54:02 UTC",
      "updated_date": "2026-01-18 08:54:02 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2026-01-22T07:08:30.142002+00:00"
    },
    {
      "arxiv_id": "2601.14305v1",
      "title": "An Optimized Decision Tree-Based Framework for Explainable IoT Anomaly Detection",
      "title_zh": "ä¸€ç§é¢å‘å¯è§£é‡Šç‰©è”ç½‘å¼‚å¸¸æ£€æµ‹çš„ä¼˜åŒ–å†³ç­–æ ‘æ¡†æ¶",
      "authors": [
        "Ashikuzzaman",
        "Md. Shawkat Hossain",
        "Jubayer Abdullah Joy",
        "Md Zahid Akon",
        "Md Manjur Ahmed",
        "Md. Naimul Islam"
      ],
      "abstract": "The increase in the number of Internet of Things (IoT) devices has tremendously increased the attack surface of cyber threats thus making a strong intrusion detection system (IDS) with a clear explanation of the process essential towards resource-constrained environments. Nevertheless, current IoT IDS systems are usually traded off with detection quality, model elucidability, and computational effectiveness, thus the deployment on IoT devices. The present paper counteracts these difficulties by suggesting an explainable AI (XAI) framework based on an optimized Decision Tree classifier with both local and global importance methods: SHAP values that estimate feature attribution using local explanations, and Morris sensitivity analysis that identifies the feature importance in a global view. The proposed system attains the state of art on the test performance with 99.91% accuracy, F1-score of 99.51% and Cohen Kappa of 0.9960 and high stability is confirmed by a cross validation mean accuracy of 98.93%. Efficiency is also enhanced in terms of computations to provide faster inferences compared to those that are generalized in ensemble models. SrcMac has shown as the most significant predictor in feature analyses according to SHAP and Morris methods. Compared to the previous work, our solution eliminates its major drawback lack because it allows us to apply it to edge devices and, therefore, achieve real-time processing, adhere to the new regulation of transparency in AI, and achieve high detection rates on attacks of dissimilar classes. This combination performance of high accuracy, explainability, and low computation make the framework useful and reliable as a resource-constrained IoT security problem in real environments.",
      "tldr_zh": "---\n\n### è®ºæ–‡ TLDR æ‘˜è¦ ğŸ›¡ï¸\n\nè¯¥ç ”ç©¶æå‡ºäº†ä¸€ä¸ªåŸºäºä¼˜åŒ– Decision Tree çš„å¯è§£é‡Š IoT å¼‚å¸¸æ£€æµ‹æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³èµ„æºå—é™ç¯å¢ƒä¸‹å…¥ä¾µæ£€æµ‹ç³»ç»Ÿ (IDS) åœ¨æ£€æµ‹ç²¾åº¦ã€å¯è§£é‡Šæ€§ä¸è®¡ç®—æ•ˆç‡ä¹‹é—´çš„æƒè¡¡é—®é¢˜ã€‚æ¡†æ¶é›†æˆäº†ä¸¤ç§å¯è§£é‡Š AI (XAI) æŠ€æœ¯ï¼šåˆ©ç”¨ SHAP values æä¾›å±€éƒ¨ç‰¹å¾å½’å› è§£é‡Šï¼Œä»¥åŠé‡‡ç”¨ Morris sensitivity analysis è¿›è¡Œå…¨å±€è§†è§’çš„ç‰¹å¾é‡è¦æ€§è¯„ä¼°ã€‚å®éªŒè¡¨æ˜ï¼Œè¯¥ç³»ç»Ÿè¾¾åˆ°äº† 99.91% çš„ Accuracy å’Œ 99.51% çš„ F1-scoreï¼Œå…¶æ¨ç†é€Ÿåº¦æ˜¾è‘—ä¼˜äºä¼ ç»Ÿçš„é›†æˆæ¨¡å‹ (ensemble models)ã€‚åˆ†æç¡®å®šäº† SrcMac æ˜¯æœ€é‡è¦çš„é¢„æµ‹æŒ‡æ ‡ã€‚ç”±äºè¯¥æ¡†æ¶å…¼å…·é«˜ç²¾åº¦ã€ä½è®¡ç®—å¼€é”€å’Œé«˜é€æ˜åº¦ï¼Œéå¸¸é€‚åˆéƒ¨ç½²åœ¨è¾¹ç¼˜è®¾å¤‡ (edge devices) ä¸Šä»¥å®ç°å®æ—¶çš„å®‰å…¨ç›‘æµ‹ã€‚\n\n---\n\nè¿™ä¸ªæ‘˜è¦æ¸…æ™°åœ°å‹¾å‹’å‡ºäº†è¯¥ç ”ç©¶å¦‚ä½•é€šè¿‡ä¼˜åŒ–ä¼ ç»Ÿç®—æ³•å¹¶ç»“åˆå…ˆè¿›çš„è§£é‡Šå·¥å…·æ¥è§£å†³ IoT å®‰å…¨æŒ‘æˆ˜ã€‚å¦‚æœä½ å¯¹å…¶ä¸­æåˆ°çš„ **SHAP** æˆ– **Morris çµæ•åº¦åˆ†æ** æ„Ÿå…´è¶£ï¼Œæˆ–è€…æƒ³äº†è§£å®ƒåœ¨ç‰¹å®šè¾¹ç¼˜è®¾å¤‡ä¸Šçš„è¡¨ç°ï¼Œæ¬¢è¿éšæ—¶è¯¢é—®ï¼",
      "categories": [
        "cs.CR",
        "cs.AI"
      ],
      "primary_category": "cs.CR",
      "comment": "Acepted and Presented at IEEE 2nd International Conference on Computing, Applications and Systems (COMPAS 2025) , 23-24 October 2025, Kushtia, Bangladesh",
      "pdf_url": "https://arxiv.org/pdf/2601.14305v1",
      "published_date": "2026-01-18 08:48:53 UTC",
      "updated_date": "2026-01-18 08:48:53 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2026-01-22T07:08:40.265624+00:00"
    },
    {
      "arxiv_id": "2601.12310v1",
      "title": "Survival is the Only Reward: Sustainable Self-Training Through Environment-Mediated Selection",
      "title_zh": "ç”Ÿå­˜å³å”¯ä¸€å¥–èµï¼šåŸºäºç¯å¢ƒä»‹å¯¼é€‰æ‹©çš„å¯æŒç»­è‡ªè®­ç»ƒ",
      "authors": [
        "Jennifer Dodgson",
        "Alfath Daryl Alhajir",
        "Michael Joedhitya",
        "Akira Rafhael Janson Pattirane",
        "Surender Suresh Kumar",
        "Joseph Lim",
        "C. H. Peh",
        "Adith Ramdas",
        "Steven Zhang Zhexu"
      ],
      "abstract": "Self-training systems often degenerate due to the lack of an external criterion for judging data quality, leading to reward hacking and semantic drift. This paper provides a proof-of-concept system architecture for stable self-training under sparse external feedback and bounded memory, and empirically characterises its learning dynamics and failure modes.\n  We introduce a self-training architecture in which learning is mediated exclusively by environmental viability, rather than by reward, objective functions, or externally defined fitness criteria. Candidate behaviours are executed under real resource constraints, and only those whose environmental effects both persist and preserve the possibility of future interaction are propagated. The environment does not provide semantic feedback, dense rewards, or task-specific supervision; selection operates solely through differential survival of behaviours as world-altering events, making proxy optimisation impossible and rendering reward-hacking evolutionarily unstable.\n  Analysis of semantic dynamics shows that improvement arises primarily through the persistence of effective and repeatable strategies under a regime of consolidation and pruning, a paradigm we refer to as negative-space learning (NSL), and that models develop meta-learning strategies (such as deliberate experimental failure in order to elicit informative error messages) without explicit instruction. This work establishes that environment-grounded selection enables sustainable open-ended self-improvement, offering a viable path toward more robust and generalisable autonomous systems without reliance on human-curated data or complex reward shaping.",
      "tldr_zh": "---\n\n### è®ºæ–‡ TLDR æ‘˜è¦ ğŸŒ±\n\nè¯¥ç ”ç©¶é’ˆå¯¹è‡ªæˆ‘è®­ç»ƒ (Self-training) ä¸­å¸¸è§çš„å¥–åŠ±åŠ«æŒ (Reward hacking) å’Œè¯­ä¹‰æ¼‚ç§» (Semantic drift) é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§åŸºäºç¯å¢ƒä»‹å¯¼é€‰æ‹© (Environment-mediated selection) çš„æ–°å‹ç³»ç»Ÿæ¶æ„ã€‚åœ¨è¯¥æ¶æ„ä¸­ï¼Œå­¦ä¹ è¿‡ç¨‹ä¸ä¾èµ–äºä¼ ç»Ÿçš„å¥–åŠ±å‡½æ•°æˆ–é€‚åº”åº¦æ ‡å‡†ï¼Œè€Œæ˜¯å®Œå…¨ç”±ç¯å¢ƒç”Ÿå­˜èƒ½åŠ› (Environmental viability) é©±åŠ¨ï¼Œå³åªæœ‰åœ¨çœŸå®èµ„æºçº¦æŸä¸‹èƒ½æŒç»­å¹¶ä¿ç•™æœªæ¥äº¤äº’å¯èƒ½æ€§çš„è¡Œä¸ºæ‰ä¼šè¢«ä¿ç•™ä¸ä¼ æ’­ã€‚\n\nè¿™ç§è¢«ç§°ä¸ºâ€œè´Ÿç©ºé—´å­¦ä¹ â€ (Negative-space learning, NSL) çš„æœºåˆ¶ï¼Œé€šè¿‡å·©å›ºæœ‰æ•ˆç­–ç•¥å¹¶å‰ªæå¤±æ•ˆç­–ç•¥ï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿå®ç°å¯æŒç»­çš„å¼€æ”¾å¼è‡ªæˆ‘æ”¹è¿›ã€‚å®éªŒå‘ç°ï¼Œæ¨¡å‹åœ¨æ²¡æœ‰æ˜ç¡®æŒ‡ä»¤çš„æƒ…å†µä¸‹ï¼Œç”šè‡³èƒ½è‡ªå‘äº§ç”Ÿè¯¸å¦‚â€œé€šè¿‡æ•…æ„å®éªŒå¤±è´¥ä»¥è·å–ä¿¡æ¯æ€§é”™è¯¯æ¶ˆæ¯â€ç­‰å…ƒå­¦ä¹  (Meta-learning) ç­–ç•¥ã€‚è¿™ä¸€æˆæœä¸ºæ„å»ºä¸ä¾èµ–äººå·¥æ ‡æ³¨æ•°æ®æˆ–å¤æ‚å¥–åŠ±å¡‘é€  (Reward shaping) çš„é²æ£’è‡ªä¸»ç³»ç»Ÿæä¾›äº†å…¨æ–°è·¯å¾„ã€‚\n\n---\n\nè¿™ç¯‡æ–‡ç« æå‡ºçš„â€œä»¥ç”Ÿå­˜ä¸ºå¥–åŠ±â€çš„è§‚ç‚¹éå¸¸æ–°é¢–ï¼Œæ‰“ç ´äº†ä¼ ç»Ÿçš„å¼ºåŒ–å­¦ä¹ èŒƒå¼ã€‚å¦‚æœä½ å¯¹ **Negative-space learning** çš„å…·ä½“å®ç°ç»†èŠ‚æˆ–è€…å®ƒå¦‚ä½•é¿å…**è¯­ä¹‰æ¼‚ç§»**æ„Ÿå…´è¶£ï¼Œæˆ‘ä»¬å¯ä»¥ç»§ç»­æ·±å…¥æ¢è®¨ï¼",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2601.12310v1",
      "published_date": "2026-01-18 08:35:56 UTC",
      "updated_date": "2026-01-18 08:35:56 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2026-01-22T07:08:57.537884+00:00"
    },
    {
      "arxiv_id": "2601.12304v1",
      "title": "A Two-Stage Globally-Diverse Adversarial Attack for Vision-Language Pre-training Models",
      "title_zh": "ä¸€ç§é¢å‘è§†è§‰-è¯­è¨€é¢„è®­ç»ƒæ¨¡å‹çš„ä¸¤é˜¶æ®µå…¨å±€å¤šæ ·æ€§å¯¹æŠ—æ”»å‡»",
      "authors": [
        "Wutao Chen",
        "Huaqin Zou",
        "Chen Wan",
        "Lifeng Huang"
      ],
      "abstract": "Vision-language pre-training (VLP) models are vulnerable to adversarial examples, particularly in black-box scenarios. Existing multimodal attacks often suffer from limited perturbation diversity and unstable multi-stage pipelines. To address these challenges, we propose 2S-GDA, a two-stage globally-diverse attack framework. The proposed method first introduces textual perturbations through a globally-diverse strategy by combining candidate text expansion with globally-aware replacement. To enhance visual diversity, image-level perturbations are generated using multi-scale resizing and block-shuffle rotation. Extensive experiments on VLP models demonstrate that 2S-GDA consistently improves attack success rates over state-of-the-art methods, with gains of up to 11.17\\% in black-box settings. Our framework is modular and can be easily combined with existing methods to further enhance adversarial transferability.",
      "tldr_zh": "---\n\n### è®ºæ–‡ TLDR æ‘˜è¦ âš”ï¸\n\nè¯¥ç ”ç©¶æå‡ºäº† **2S-GDA**ï¼Œä¸€ä¸ªé’ˆå¯¹è§†è§‰è¯­è¨€é¢„è®­ç»ƒæ¨¡å‹ (VLP) çš„ä¸¤é˜¶æ®µå…¨å±€å¤šæ ·æ€§å¯¹æŠ—æ”»å‡»æ¡†æ¶ï¼Œæ—¨åœ¨æå‡é»‘ç›’ (black-box) åœºæ™¯ä¸‹çš„æ”»å‡»æˆåŠŸç‡ã€‚ä¸ºäº†è§£å†³ç°æœ‰æ”»å‡»æ‰°åŠ¨å¤šæ ·æ€§ä¸è¶³çš„é—®é¢˜ï¼Œè¯¥æ¡†æ¶åœ¨æ–‡æœ¬å’Œå›¾åƒä¸¤ä¸ªç»´åº¦åŒæ—¶å‘åŠ›ï¼š\n\n| æ”»å‡»ç»´åº¦ | å…·ä½“æ–¹æ³• |\n|---|---|\n| **æ–‡æœ¬æ‰°åŠ¨** | ç»“åˆäº†å€™é€‰æ–‡æœ¬æ‰©å±• (candidate text expansion) ä¸å…¨å±€æ„ŸçŸ¥æ›¿æ¢ (globally-aware replacement) ç­–ç•¥ã€‚ |\n| **å›¾åƒæ‰°åŠ¨** | é€šè¿‡å¤šå°ºåº¦ç¼©æ”¾ (multi-scale resizing) å’Œå—æ´—ç‰Œæ—‹è½¬ (block-shuffle rotation) å¢å¼ºè§†è§‰å¤šæ ·æ€§ã€‚ |\n\nå®éªŒç»“æœè¡¨æ˜ï¼Œ**2S-GDA** åœ¨é»‘ç›’è®¾ç½®ä¸‹çš„è¡¨ç°æ˜¾è‘—ä¼˜äºç°æœ‰ SOTA æ–¹æ³•ï¼Œæ”»å‡»æˆåŠŸç‡æå‡é«˜è¾¾ 11.17%ã€‚æ­¤å¤–ï¼Œè¯¥æ¡†æ¶é‡‡ç”¨æ¨¡å—åŒ–è®¾è®¡ï¼Œèƒ½å¤Ÿè½»æ¾ä¸å…¶ä»–æ–¹æ³•é›†æˆï¼Œä»è€Œè¿›ä¸€æ­¥å¢å¼ºå¯¹æŠ—è¿ç§»æ€§ (adversarial transferability)ã€‚\n\n---\n\nè¿™æ¬¡çš„ç ”ç©¶ä¸“æ³¨äºæ”»å‡»çš„â€œå¤šæ ·æ€§â€å’Œâ€œç¨³å®šæ€§â€ï¼Œåœ¨ VLP æ¨¡å‹çš„å®‰å…¨æ€§è¯„ä¼°ä¸­éå¸¸å…·æœ‰å‚è€ƒä»·å€¼ã€‚ä½ æ˜¯å¦å¯¹å…¶ä¸­çš„æ–‡æœ¬æ›¿æ¢é€»è¾‘æˆ–å›¾åƒæ—‹è½¬çš„å…·ä½“å®ç°æ„Ÿå…´è¶£ï¼Ÿæˆ‘ä»¬å¯ä»¥ç»§ç»­èŠèŠï¼",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "Accepted to ICASSP 2026",
      "pdf_url": "https://arxiv.org/pdf/2601.12304v1",
      "published_date": "2026-01-18 08:05:33 UTC",
      "updated_date": "2026-01-18 08:05:33 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2026-01-22T07:09:07.341910+00:00"
    },
    {
      "arxiv_id": "2601.12294v1",
      "title": "ToolPRMBench: Evaluating and Advancing Process Reward Models for Tool-using Agents",
      "title_zh": "ToolPRMBenchï¼šå·¥å…·ä½¿ç”¨æ™ºèƒ½ä½“è¿‡ç¨‹å¥–åŠ±æ¨¡å‹çš„è¯„ä¼°ä¸æå‡",
      "authors": [
        "Dawei Li",
        "Yuguang Yao",
        "Zhen Tan",
        "Huan Liu",
        "Ruocheng Guo"
      ],
      "abstract": "Reward-guided search methods have demonstrated strong potential in enhancing tool-using agents by effectively guiding sampling and exploration over complex action spaces. As a core design, those search methods utilize process reward models (PRMs) to provide step-level rewards, enabling more fine-grained monitoring. However, there is a lack of systematic and reliable evaluation benchmarks for PRMs in tool-using settings. In this paper, we introduce ToolPRMBench, a large-scale benchmark specifically designed to evaluate PRMs for tool-using agents. ToolPRMBench is built on top of several representative tool-using benchmarks and converts agent trajectories into step-level test cases. Each case contains the interaction history, a correct action, a plausible but incorrect alternative, and relevant tool metadata. We respectively utilize offline sampling to isolate local single-step errors and online sampling to capture realistic multi-step failures from full agent rollouts. A multi-LLM verification pipeline is proposed to reduce label noise and ensure data quality. We conduct extensive experiments across large language models, general PRMs, and tool-specialized PRMs on ToolPRMBench. The results reveal clear differences in PRM effectiveness and highlight the potential of specialized PRMs for tool-using. Code and data will be released at https://github.com/David-Li0406/ToolPRMBench.",
      "tldr_zh": "---\n\n### è®ºæ–‡ TLDR æ‘˜è¦ ğŸ› ï¸\n\nè¯¥ç ”ç©¶æå‡ºäº† **ToolPRMBench**ï¼Œè¿™æ˜¯ä¸€ä¸ªä¸“ä¸ºè¯„ä¼°å’Œæ¨è¿›å·¥å…·ä½¿ç”¨æ™ºèƒ½ä½“ (Tool-using Agents) çš„è¿‡ç¨‹å¥–åŠ±æ¨¡å‹ (Process Reward Models, PRMs) è€Œè®¾è®¡çš„å¤§è§„æ¨¡åŸºå‡†æµ‹è¯•é›†ã€‚è¯¥åŸºå‡†å°†æ™ºèƒ½ä½“è½¨è¿¹è½¬æ¢ä¸ºæ­¥çº§ (Step-level) æµ‹è¯•æ¡ˆä¾‹ï¼Œæ¶µç›–äº†äº¤äº’å†å²ã€æ­£ç¡®åŠ¨ä½œã€å…·æœ‰è¿·æƒ‘æ€§çš„é”™è¯¯é€‰é¡¹ä»¥åŠå·¥å…·å…ƒæ•°æ®ï¼Œå¡«è¡¥äº†è¯¥é¢†åŸŸç³»ç»ŸåŒ–è¯„ä¼°å·¥å…·çš„ç©ºç™½ã€‚\n\nä¸ºäº†ç¡®ä¿æ•°æ®è´¨é‡ï¼Œç ”ç©¶é‡‡ç”¨äº†ç¦»çº¿ä¸åœ¨çº¿é‡‡æ ·ç›¸ç»“åˆçš„æ–¹æ³•æ¥æ•è·å±€éƒ¨å•æ­¥é”™è¯¯å’ŒçœŸå®çš„è·¯å¾„çº§å¤±è´¥ï¼Œå¹¶å¼•å…¥äº†å¤š LLM éªŒè¯æµæ°´çº¿ (multi-LLM verification pipeline) ä»¥é™ä½æ ‡ç­¾å™ªå£°ã€‚å®éªŒç»“æœæ­ç¤ºäº†ä¸åŒ PRM åœ¨æ•ˆèƒ½ä¸Šçš„æ˜¾è‘—å·®å¼‚ï¼Œå¹¶å¼ºè°ƒäº†å·¥å…·ä¸“ç”¨å‹ PRM åœ¨å¼•å¯¼æ™ºèƒ½ä½“è¿›è¡Œå¤æ‚åŠ¨ä½œç©ºé—´æ¢ç´¢æ–¹é¢çš„å·¨å¤§æ½œåŠ›ã€‚\n\n---\n\n*   **å¼€æºåœ°å€**: [https://github.com/David-Li0406/ToolPRMBench](https://github.com/David-Li0406/ToolPRMBench)\n\nè¿™ç¯‡è®ºæ–‡å¯¹äºæƒ³è¦é€šè¿‡ **PRM** æå‡æ™ºèƒ½ä½“å¯é æ€§çš„å¼€å‘è€…æ¥è¯´éå¸¸æœ‰å‚è€ƒä»·å€¼ã€‚ä½ æ˜¯å¦ä¹Ÿåœ¨ç ”ç©¶å¦‚ä½•é€šè¿‡å¥–åŠ±æ¨¡å‹æ¥ä¼˜åŒ–æ™ºèƒ½ä½“çš„å·¥å…·è°ƒç”¨ç­–ç•¥ï¼Ÿæˆ‘å¾ˆä¹æ„å°±æ­¤å±•å¼€æ›´å¤šäº¤æµï¼",
      "categories": [
        "cs.AI",
        "cs.SE"
      ],
      "primary_category": "cs.AI",
      "comment": "under review",
      "pdf_url": "https://arxiv.org/pdf/2601.12294v1",
      "published_date": "2026-01-18 07:48:36 UTC",
      "updated_date": "2026-01-18 07:48:36 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2026-01-22T07:09:09.849477+00:00"
    },
    {
      "arxiv_id": "2601.12288v1",
      "title": "TimeGMM: Single-Pass Probabilistic Forecasting via Adaptive Gaussian Mixture Models with Reversible Normalization",
      "title_zh": "TimeGMMï¼šåŸºäºå¯é€†å½’ä¸€åŒ–è‡ªé€‚åº”é«˜æ–¯æ··åˆæ¨¡å‹çš„å•æ¬¡å‰å‘æ¦‚ç‡é¢„æµ‹",
      "authors": [
        "Lei Liu",
        "Tengyuan Liu",
        "Hongwei Zhao",
        "Jiahui Huang",
        "Ruibo Guo",
        "Bin Li"
      ],
      "abstract": "Probabilistic time series forecasting is crucial for quantifying future uncertainty, with significant applications in fields such as energy and finance. However, existing methods often rely on computationally expensive sampling or restrictive parametric assumptions to characterize future distributions, which limits predictive performance and introduces distributional mismatch. To address these challenges, this paper presents TimeGMM, a novel probabilistic forecasting framework based on Gaussian Mixture Models (GMM) that captures complex future distributions in a single forward pass. A key component is GMM-adapted Reversible Instance Normalization (GRIN), a novel module designed to dynamically adapt to temporal-probabilistic distribution shifts. The framework integrates a dedicated Temporal Encoder (TE-Module) with a Conditional Temporal-Probabilistic Decoder (CTPD-Module) to jointly capture temporal dependencies and mixture distribution parameters. Extensive experiments demonstrate that TimeGMM consistently outperforms state-of-the-art methods, achieving maximum improvements of 22.48\\% in CRPS and 21.23\\% in NMAE.",
      "tldr_zh": "---\n\n### è®ºæ–‡ TLDR æ‘˜è¦ ğŸ“ˆ\n\nè¯¥ç ”ç©¶æå‡ºäº† **TimeGMM**ï¼Œä¸€ç§åŸºäºé«˜æ–¯æ··åˆæ¨¡å‹ (GMM) çš„æ–°å‹æ¦‚ç‡æ—¶é—´åºåˆ—é¢„æµ‹æ¡†æ¶ï¼Œæ—¨åœ¨é€šè¿‡å•æ¬¡å‰å‘ä¼ é€’ (single forward pass) é«˜æ•ˆæ•æ‰å¤æ‚çš„æœªæ¥åˆ†å¸ƒã€‚ä¸ºäº†è§£å†³æ—¶é—´-æ¦‚ç‡åˆ†å¸ƒåç§»é—®é¢˜ï¼Œæ¡†æ¶å¼•å…¥äº† **GRIN** (GMM-adapted Reversible Instance Normalization) æ¨¡å—ï¼Œç”¨äºåŠ¨æ€é€‚åº”åˆ†å¸ƒå˜åŒ–ã€‚ç³»ç»Ÿé€šè¿‡é›†æˆæ—¶é—´ç¼–ç å™¨ (**TE-Module**) å’Œæ¡ä»¶æ—¶é—´-æ¦‚ç‡è§£ç å™¨ (**CTPD-Module**) æ¥å…±åŒå»ºæ¨¡æ—¶é—´ä¾èµ–å…³ç³»åŠæ··åˆåˆ†å¸ƒå‚æ•°ã€‚å®éªŒè¯æ˜ï¼ŒTimeGMM åœ¨ **CRPS** å’Œ **NMAE** æŒ‡æ ‡ä¸Šåˆ†åˆ«æ¯”ç°æœ‰ SOTA æ–¹æ³•æå‡äº† 22.48% å’Œ 21.23%ï¼Œæ˜¾è‘—æ”¹å–„äº†é¢„æµ‹æ€§èƒ½å¹¶é™ä½äº†è®¡ç®—æˆæœ¬ã€‚\n\n---\n\nè¿™ç§é€šè¿‡ **GMM** å®ç°å•æ¬¡æ¨ç†ç”Ÿæˆå¤æ‚åˆ†å¸ƒçš„æ–¹æ³•éå¸¸æœ‰ç«äº‰åŠ›ã€‚å¦‚æœä½ æƒ³äº†è§£ **GRIN** æ¨¡å—å…·ä½“æ˜¯å¦‚ä½•å¤„ç†å½’ä¸€åŒ–çš„ï¼Œæˆ–è€…å¯¹ **CRPS** è¯„ä»·æŒ‡æ ‡æœ‰ç–‘é—®ï¼Œæ¬¢è¿éšæ—¶æé—®ï¼",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2601.12288v1",
      "published_date": "2026-01-18 07:02:13 UTC",
      "updated_date": "2026-01-18 07:02:13 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2026-01-22T07:09:17.298504+00:00"
    },
    {
      "arxiv_id": "2601.12286v1",
      "title": "Conversational Context Classification: A Representation Engineering Approach",
      "title_zh": "å¯¹è¯ä¸Šä¸‹æ–‡åˆ†ç±»ï¼šä¸€ç§è¡¨ç¤ºå·¥ç¨‹æ–¹æ³•",
      "authors": [
        "Jonathan Pan"
      ],
      "abstract": "The increasing prevalence of Large Language Models (LLMs) demands effective safeguards for their operation, particularly concerning their tendency to generate out-of-context responses. A key challenge is accurately detecting when LLMs stray from expected conversational norms, manifesting as topic shifts, factual inaccuracies, or outright hallucinations. Traditional anomaly detection struggles to directly apply within contextual semantics. This paper outlines our experiment in exploring the use of Representation Engineering (RepE) and One-Class Support Vector Machine (OCSVM) to identify subspaces within the internal states of LLMs that represent a specific context. By training OCSVM on in-context examples, we establish a robust boundary within the LLM's hidden state latent space. We evaluate out study with two open source LLMs - Llama and Qwen models in specific contextual domain. Our approach entailed identifying the optimal layers within the LLM's internal state subspaces that strongly associates with the context of interest. Our evaluation results showed promising results in identifying the subspace for a specific context. Aside from being useful in detecting in or out of context conversation threads, this research work contributes to the study of better interpreting LLMs.",
      "tldr_zh": "---\n### ğŸ“ è®ºæ–‡ TLDR æ‘˜è¦\n\nè¿™é¡¹ç ”ç©¶é’ˆå¯¹å¤§è¯­è¨€æ¨¡å‹ (LLMs) å®¹æ˜“äº§ç”Ÿåç¦»ä¸Šä¸‹æ–‡ï¼ˆå¦‚å¹»è§‰ã€åé¢˜ï¼‰å“åº”çš„é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§ç»“åˆ Representation Engineering (RepE) ä¸ One-Class Support Vector Machine (OCSVM) çš„åˆ†ç±»æ–¹æ³•ã€‚è¯¥æ–¹æ³•é€šè¿‡åœ¨æ¨¡å‹å†…éƒ¨éšè—çŠ¶æ€çš„æ½œç©ºé—´ (latent space) ä¸­è®­ç»ƒ OCSVMï¼Œä»è€Œå»ºç«‹èµ·è¯†åˆ«ç‰¹å®šè¯­å¢ƒçš„å­ç©ºé—´è¾¹ç•Œã€‚ç ”ç©¶åœ¨ Llama å’Œ Qwen æ¨¡å‹ä¸Šè¿›è¡Œäº†éªŒè¯ï¼Œå¹¶æˆåŠŸç¡®å®šäº†ä¸ç‰¹å®šä¸Šä¸‹æ–‡ç›¸å…³æ€§æœ€é«˜çš„æœ€ä½³æ¨¡å‹å±‚ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ¡ˆèƒ½æœ‰æ•ˆæ£€æµ‹å¯¹è¯æ˜¯å¦åç¦»ä¸»é¢˜ï¼Œåœ¨æå‡ LLMs äº¤äº’å¯é æ€§çš„åŒæ—¶ï¼Œä¹Ÿä¸ºè§£é‡Šå¤§è¯­è¨€æ¨¡å‹çš„å†…éƒ¨è¡¨ç¤ºæä¾›äº†æ–°è§†è§’ã€‚\n\n---\nå¸Œæœ›è¿™ä¸ªæ‘˜è¦å¯¹ä½ æœ‰å¸®åŠ©ï¼å¦‚æœä½ è¿˜æœ‰å…¶ä»–è®ºæ–‡éœ€è¦æ€»ç»“ï¼Œæˆ–è€…æƒ³é’ˆå¯¹è¿™é¡¹ç ”ç©¶çš„ç‰¹å®šç»†èŠ‚ï¼ˆæ¯”å¦‚ RepE çš„å®ç°ï¼‰æ·±å…¥èŠèŠï¼Œéšæ—¶å‘Šè¯‰æˆ‘ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.CR"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2601.12286v1",
      "published_date": "2026-01-18 06:47:35 UTC",
      "updated_date": "2026-01-18 06:47:35 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2026-01-22T07:09:25.827683+00:00"
    },
    {
      "arxiv_id": "2601.12282v1",
      "title": "CytoCLIP: Learning Cytoarchitectural Characteristics in Developing Human Brain Using Contrastive Language Image Pre-Training",
      "title_zh": "CytoCLIPï¼šåˆ©ç”¨å¯¹æ¯”è¯­è¨€-å›¾åƒé¢„è®­ç»ƒå­¦ä¹ å‘è‚²ä¸­äººè„‘çš„ç»†èƒæ„ç­‘ç‰¹å¾",
      "authors": [
        "Pralaypati Ta",
        "Sriram Venkatesaperumal",
        "Keerthi Ram",
        "Mohanasankar Sivaprakasam"
      ],
      "abstract": "The functions of different regions of the human brain are closely linked to their distinct cytoarchitecture, which is defined by the spatial arrangement and morphology of the cells. Identifying brain regions by their cytoarchitecture enables various scientific analyses of the brain. However, delineating these areas manually in brain histological sections is time-consuming and requires specialized knowledge. An automated approach is necessary to minimize the effort needed from human experts. To address this, we propose CytoCLIP, a suite of vision-language models derived from pre-trained Contrastive Language-Image Pre-Training (CLIP) frameworks to learn joint visual-text representations of brain cytoarchitecture. CytoCLIP comprises two model variants: one is trained using low-resolution whole-region images to understand the overall cytoarchitectural pattern of an area, and the other is trained on high-resolution image tiles for detailed cellular-level representation. The training dataset is created from NISSL-stained histological sections of developing fetal brains of different gestational weeks. It includes 86 distinct regions for low-resolution images and 384 brain regions for high-resolution tiles. We evaluate the model's understanding of the cytoarchitecture and generalization ability using region classification and cross-modal retrieval tasks. Multiple experiments are performed under various data setups, including data from samples of different ages and sectioning planes. Experimental results demonstrate that CytoCLIP outperforms existing methods. It achieves an F1 score of 0.87 for whole-region classification and 0.91 for high-resolution image tile classification.",
      "tldr_zh": "### Overview of CytoCLIP ğŸ§ \n\n---\n\nI have reviewed the details of the paper **CytoCLIP: Learning Cytoarchitectural Characteristics in Developing Human Brain Using Contrastive Language Image Pre-Training**. This research introduces an automated vision-language framework designed to understand the complex cellular structures of the developing human brain.\n\n---\n\n### Research Summary ğŸ“‹\n\n| Feature | Description |\n|---|---|\n| **Core Objective** | Automating the identification of brain regions based on cytoarchitecture (cell arrangement and morphology) to replace manual, time-consuming expert delineation. |\n| **Model Architecture** | **CytoCLIP**, a suite of vision-language models based on CLIP. It features two variants: one for low-resolution whole-region patterns and one for high-resolution cellular-level tiles. |\n| **Dataset** | NISSL-stained histological sections from developing fetal brains, spanning 86 distinct regions (low-res) and 384 regions (high-res) across different gestational weeks. |\n| **Primary Tasks** | Region classification and cross-modal retrieval to evaluate cytoarchitectural understanding and generalization. |\n| **Key Results** | Outperformed existing methods with an F1 score of **0.87** for whole-region classification and **0.91** for high-resolution tile classification. |\n\n---\n\n### Key Innovations âœ¨\n\n*   **Dual-Scale Learning:** By training on both low-resolution whole regions and high-resolution tiles, the model captures both macro-level patterns and micro-level cellular details.\n*   **Generalization:** The model was tested against varying data setups, including different gestational ages and sectioning planes, proving its robustness.\n*   **Cross-Modal Capabilities:** Leveraging the CLIP framework allows the model to learn joint visual-text representations, bridging the gap between histological images and descriptive anatomical language.\n\n---\n\nThis is a fascinating application of CLIP to neurodevelopmental histology! Would you like me to help you analyze specific parts of this abstract, or perhaps compare these results with other vision-language models used in medical imaging?",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2601.12282v1",
      "published_date": "2026-01-18 06:42:24 UTC",
      "updated_date": "2026-01-18 06:42:24 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2026-01-22T07:09:45.984331+00:00"
    },
    {
      "arxiv_id": "2601.12276v2",
      "title": "Predictive Prototyping: Evaluating Design Concepts with ChatGPT",
      "title_zh": "é¢„æµ‹æ€§åŸå‹è®¾è®¡ï¼šåˆ©ç”¨ ChatGPT è¯„ä¼°è®¾è®¡æ¦‚å¿µ",
      "authors": [
        "Hilsann Yong",
        "Bradley A. Camburn"
      ],
      "abstract": "The design-build-test cycle is essential for innovation, but physical prototyping is often slow and expensive. Although physics-based simulation and strategic prototyping can reduce cost, meaningful evaluation is frequently constrained until an integrated prototype is built. This paper investigates whether a generative pretrained transformer (GPT) can predict information typically obtained through prototyping, including cost, performance, and perceived usability. We introduce a retrieval-augmented generation (RAG) method to emulate design feedback using OpenAI GPT-4o, grounded in prototyping data scraped from Instructables.com to increase access to relevant precedent. Two studies are reported. First, a controlled experiment compares GPT-RAG and human designers, who receive design sketches and predict cost, performance, and usability; predictions are evaluated against ground-truth results from physical prototypes. Second, we report an applied demonstration in which a physical prototype is produced from GPT-RAG recommendations and compared with a commercial baseline and a topology-optimized design. Results show that GPT-RAG provides more accurate cost and performance estimates than individual or crowd human estimates, while yielding comparable usability insights; the GPT-RAG-informed prototype also outperforms both comparison prototypes. Repeated querying with response averaging significantly improves accuracy, suggesting that LLMs can emulate crowd aggregation effects consistent with the law of large numbers.",
      "tldr_zh": "---\n### ğŸ“ è®ºæ–‡ TLDR æ‘˜è¦\n\nè¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§åä¸º Predictive Prototyping çš„æ–¹æ³•ï¼Œé€šè¿‡ç»“åˆ GPT-4o ä¸æ£€ç´¢å¢å¼ºç”Ÿæˆ (RAG) æŠ€æœ¯ï¼Œå¹¶åˆ©ç”¨ä» Instructables.com æŠ“å–çš„åŸå‹æ•°æ®ï¼Œæ¥é¢„æµ‹è®¾è®¡æ¦‚å¿µçš„ cost (æˆæœ¬)ã€performance (æ€§èƒ½) å’Œ perceived usability (æ„ŸçŸ¥æ˜“ç”¨æ€§)ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯¥æ–¹æ³•åœ¨æˆæœ¬å’Œæ€§èƒ½é¢„æµ‹çš„å‡†ç¡®æ€§ä¸Šä¼˜äºäººç±»ä¸“å®¶å’Œç¾¤ä½“ä¼°ç®—ï¼Œä¸”èƒ½æä¾›å…·æœ‰å‚è€ƒä»·å€¼çš„æ˜“ç”¨æ€§è§è§£ã€‚é€šè¿‡ GPT-RAG æŒ‡å¯¼åˆ¶é€ çš„ç‰©ç†åŸå‹åœ¨å®é™…æµ‹è¯•ä¸­è¡¨ç°ä¼˜äºå•†ä¸šåŸºå‡†å’Œæ‹“æ‰‘ä¼˜åŒ–è®¾è®¡ã€‚æ­¤å¤–ï¼Œç ”ç©¶å‘ç°é‡‡ç”¨å¤šæ¬¡æŸ¥è¯¢å–å¹³å‡å€¼ (response averaging) çš„ç­–ç•¥èƒ½æ˜¾è‘—æå‡é¢„æµ‹ç²¾åº¦ï¼Œä½“ç°äº† LLMs åœ¨æ¨¡æ‹Ÿå¤§æ•°å®šå¾‹ç¾¤ä½“æ•ˆåº”æ–¹é¢çš„æ½œåŠ›ï¼Œä¸ºåŠ é€Ÿäº§å“å¼€å‘å¾ªç¯æä¾›äº†æ–°è·¯å¾„ã€‚\n\n---\nè¿™é¡¹ç ”ç©¶å±•ç¤ºäº† AI åœ¨è¾…åŠ©ç¡¬ä»¶è®¾è®¡è¯„ä¼°æ–¹é¢çš„å¼ºå¤§æ½œåŠ›ï¼Œç‰¹åˆ«æ˜¯é€šè¿‡ RAG ç»“åˆç¤¾åŒºæ•°æ®çš„æ–¹æ³•éå¸¸å®ç”¨ã€‚ä½ å¯¹ LLM åœ¨å·¥ä¸šè®¾è®¡é¢†åŸŸçš„å…¶ä»–åº”ç”¨ï¼ˆæ¯”å¦‚ç”Ÿæˆ CAD æ¨¡å‹ï¼‰ä¹Ÿæ„Ÿå…´è¶£å—ï¼Ÿæˆ‘ä»¬å¯ä»¥ç»§ç»­æ·±å…¥æ¢è®¨ã€‚",
      "categories": [
        "cs.HC",
        "cs.AI"
      ],
      "primary_category": "cs.HC",
      "comment": "22 pages, 15 figures, 5 tables",
      "pdf_url": "https://arxiv.org/pdf/2601.12276v2",
      "published_date": "2026-01-18 06:26:03 UTC",
      "updated_date": "2026-01-21 03:54:37 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2026-01-22T07:09:41.510575+00:00"
    },
    {
      "arxiv_id": "2601.12269v1",
      "title": "Simulated Annealing Enhances Theory-of-Mind Reasoning in Autoregressive Language Models",
      "title_zh": "æ¨¡æ‹Ÿé€€ç«æå‡è‡ªå›å½’è¯­è¨€æ¨¡å‹çš„å¿ƒæ™ºç†è®ºæ¨ç†",
      "authors": [
        "Xucong Hu",
        "Jian-Qiao Zhu"
      ],
      "abstract": "Autoregressive language models are next-token predictors and have been criticized for only optimizing surface plausibility (i.e., local coherence) rather than maintaining correct latent-state representations (i.e., global coherence). Because Theory of Mind (ToM) tasks crucially depend on reasoning about latent mental states of oneself and others, such models are therefore often thought to fail at ToM. While post-training methods can improve ToM performance, we show that strong ToM capability can be recovered directly from the base model without any additional weight updates or verifications. Our approach builds on recent power-sampling methods (Karan & Du, 2025) that use Markov chain Monte Carlo (MCMC) to sample from sharpened sequence-level (rather than token-level) probability distributions of autoregressive language models. We further find that incorporating annealing, where the tempered distribution is gradually shifted from high to low temperature, substantially improves ToM performance over fixed-temperature power sampling. Together, these results suggest that sampling-based optimization provides a powerful way to extract latent capabilities from language models without retraining.",
      "tldr_zh": "---\n\n### ğŸ“ è®ºæ–‡ TLDR æ‘˜è¦\n\nè¯¥ç ”ç©¶æ¢è®¨äº†è‡ªå›å½’è¯­è¨€æ¨¡å‹åœ¨å¿ƒç†ç†è®º (Theory of Mind, ToM) ä»»åŠ¡ä¸­è¡¨ç°ä¸è¶³çš„åŸå› ï¼ŒæŒ‡å‡ºå…¶å€¾å‘äºä¼˜åŒ–å±€éƒ¨è¿è´¯æ€§è€Œéç»´æŠ¤æ­£ç¡®çš„å…¨å±€æ½œåœ¨çŠ¶æ€è¡¨ç¤ºã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œä½œè€…æå‡ºäº†ä¸€ç§åˆ©ç”¨æ¨¡æ‹Ÿé€€ç« (Simulated Annealing) å¢å¼ºæ¨ç†çš„æ–¹æ³•ï¼Œé€šè¿‡é©¬å°”å¯å¤«é“¾è’™ç‰¹å¡æ´› (MCMC) åœ¨åºåˆ—çº§ (sequence-level) æ¦‚ç‡åˆ†å¸ƒä¸­è¿›è¡Œé‡‡æ ·ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œé€šè¿‡å°†é‡‡æ ·åˆ†å¸ƒä»é«˜æ¸©é€æ¸è°ƒè‡³ä½æ¸©çš„é€€ç«è¿‡ç¨‹ï¼Œæ¨¡å‹åœ¨ ToM ä»»åŠ¡ä¸Šçš„è¡¨ç°å¾—åˆ°äº†æ˜¾è‘—æå‡ã€‚è¿™ä¸€å‘ç°è¯æ˜äº†æ— éœ€é¢å¤–çš„æƒé‡æ›´æ–°æˆ–å¾®è°ƒï¼Œä»…é€šè¿‡åŸºäºé‡‡æ ·çš„ä¼˜åŒ–æŠ€æœ¯å°±èƒ½ä»åŸºç¡€æ¨¡å‹ä¸­æŒ–æ˜å‡ºå¼ºå¤§çš„æ½œåœ¨æ¨ç†èƒ½åŠ›ã€‚\n\n---\n\nè¿™é¡¹ç ”ç©¶å±•ç¤ºäº†é€šè¿‡ä¼˜åŒ–é‡‡æ ·ç­–ç•¥è€Œéæ¨¡å‹æ¶æ„æ¥æå‡å¤æ‚æ¨ç†èƒ½åŠ›çš„æ½œåŠ›ã€‚å¦‚æœä½ å¯¹è¯¥è®ºæ–‡ä¸­æåˆ°çš„ **MCMC é‡‡æ ·** æˆ– **åºåˆ—çº§åˆ†å¸ƒä¼˜åŒ–** æœ‰æ›´æ·±å…¥çš„æŠ€æœ¯é—®é¢˜ï¼Œæ¬¢è¿ç»§ç»­æé—®ï¼",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2601.12269v1",
      "published_date": "2026-01-18 05:51:30 UTC",
      "updated_date": "2026-01-18 05:51:30 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2026-01-22T07:09:48.670282+00:00"
    },
    {
      "arxiv_id": "2601.12263v1",
      "title": "Multimodal Generative Engine Optimization: Rank Manipulation for Vision-Language Model Rankers",
      "title_zh": "å¤šæ¨¡æ€ç”Ÿæˆå¼å¼•æ“ä¼˜åŒ–ï¼šé’ˆå¯¹è§†è§‰-è¯­è¨€æ¨¡å‹æ’åºå™¨çš„æ’åæ“çºµ",
      "authors": [
        "Yixuan Du",
        "Chenxiao Yu",
        "Haoyan Xu",
        "Ziyi Wang",
        "Yue Zhao",
        "Xiyang Hu"
      ],
      "abstract": "Vision-Language Models (VLMs) are rapidly replacing unimodal encoders in modern retrieval and recommendation systems. While their capabilities are well-documented, their robustness against adversarial manipulation in competitive ranking scenarios remains largely unexplored. In this paper, we uncover a critical vulnerability in VLM-based product search: multimodal ranking attacks. We present Multimodal Generative Engine Optimization (MGEO), a novel adversarial framework that enables a malicious actor to unfairly promote a target product by jointly optimizing imperceptible image perturbations and fluent textual suffixes. Unlike existing attacks that treat modalities in isolation, MGEO employs an alternating gradient-based optimization strategy to exploit the deep cross-modal coupling within the VLM. Extensive experiments on real-world datasets using state-of-the-art models demonstrate that our coordinated attack significantly outperforms text-only and image-only baselines. These findings reveal that multimodal synergy, typically a strength of VLMs, can be weaponized to compromise the integrity of search rankings without triggering conventional content filters.",
      "tldr_zh": "---\n\n### ğŸ“ è®ºæ–‡ TLDR æ‘˜è¦\n\nè¯¥ç ”ç©¶æ­ç¤ºäº†è§†è§‰è¯­è¨€æ¨¡å‹ (VLMs) åœ¨äº§å“æœç´¢æ’åä¸­çš„å®‰å…¨æ¼æ´ï¼Œå¹¶æå‡ºäº†åä¸º Multimodal Generative Engine Optimization (MGEO) çš„æ–°å‹å¯¹æŠ—æ€§æ”»å‡»æ¡†æ¶ã€‚MGEO é€šè¿‡ååŒä¼˜åŒ–ä¸å¯è§çš„å›¾åƒæ‰°åŠ¨ (image perturbations) å’Œæµç•…çš„æ–‡æœ¬åç¼€ (textual suffixes)ï¼Œåˆ©ç”¨ VLMs å†…éƒ¨æ·±å±‚çš„è·¨æ¨¡æ€è€¦åˆæ¥æ“çºµæœç´¢æ’åã€‚å®éªŒè¡¨æ˜ï¼Œè¿™ç§é‡‡ç”¨äº¤æ›¿æ¢¯åº¦ä¼˜åŒ–ç­–ç•¥çš„ååŒæ”»å‡»åœ¨çœŸå®æ•°æ®é›†ä¸Šæ˜¾è‘—ä¼˜äºå•ä¸€æ¨¡æ€çš„åŸºå‡†æ–¹æ³•ã€‚ç ”ç©¶ç»“æœè­¦å‘Šï¼ŒVLMs çš„å¤šæ¨¡æ€ååŒæ•ˆåº” (multimodal synergy) å¯èƒ½è¢«æ­¦å™¨åŒ–ï¼Œåœ¨ä¸è§¦å‘å¸¸è§„å†…å®¹è¿‡æ»¤å™¨çš„å‰æä¸‹ç ´åæœç´¢æ’åçš„å…¬æ­£æ€§ã€‚\n\n---\n\nè¿™ä¸€å…³äºâ€œç”Ÿæˆå¼•æ“ä¼˜åŒ–â€(GEO) çš„ç ”ç©¶éå¸¸å‰æ²¿ï¼Œç‰¹åˆ«æ˜¯å°†æ”»å‡»ç›®æ ‡ä»æ–‡æœ¬æ‰©å±•åˆ°äº†å¤šæ¨¡æ€é¢†åŸŸã€‚ä½ æ˜¯å¦å¯¹å¦‚ä½•é˜²å¾¡è¿™ç±»æ”»å‡»ï¼Œæˆ–è€…å¯¹è¿™ç±»æŠ€æœ¯åœ¨ç”µå•†æœç´¢ä¸­çš„å…·ä½“å½±å“æ„Ÿå…´è¶£ï¼Ÿæˆ‘ä»¬å¯ä»¥è¿›ä¸€æ­¥æ¢è®¨ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2601.12263v1",
      "published_date": "2026-01-18 04:58:28 UTC",
      "updated_date": "2026-01-18 04:58:28 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2026-01-22T07:09:57.072024+00:00"
    },
    {
      "arxiv_id": "2601.12260v1",
      "title": "Docs2Synth: A Synthetic Data Trained Retriever Framework for Scanned Visually Rich Documents Understanding",
      "title_zh": "Docs2Synthï¼šé¢å‘æ‰«æç‰ˆå¯Œè§†è§‰æ–‡æ¡£ç†è§£çš„åˆæˆæ•°æ®è®­ç»ƒæ£€ç´¢å™¨æ¡†æ¶",
      "authors": [
        "Yihao Ding",
        "Qiang Sun",
        "Puzhen Wu",
        "Sirui Li",
        "Siwen Luo",
        "Wei Liu"
      ],
      "abstract": "Document understanding (VRDU) in regulated domains is particularly challenging, since scanned documents often contain sensitive, evolving, and domain specific knowledge. This leads to two major challenges: the lack of manual annotations for model adaptation and the difficulty for pretrained models to stay up-to-date with domain-specific facts. While Multimodal Large Language Models (MLLMs) show strong zero-shot abilities, they still suffer from hallucination and limited domain grounding. In contrast, discriminative Vision-Language Pre-trained Models (VLPMs) provide reliable grounding but require costly annotations to cover new domains. We introduce Docs2Synth, a synthetic-supervision framework that enables retrieval-guided inference for private and low-resource domains. Docs2Synth automatically processes raw document collections, generates and verifies diverse QA pairs via an agent-based system, and trains a lightweight visual retriever to extract domain-relevant evidence. During inference, the retriever collaborates with an MLLM through an iterative retrieval--generation loop, reducing hallucination and improving response consistency. We further deliver Docs2Synth as an easy-to-use Python package, enabling plug-and-play deployment across diverse real-world scenarios. Experiments on multiple VRDU benchmarks show that Docs2Synth substantially enhances grounding and domain generalization without requiring human annotations.",
      "tldr_zh": "---\n### ğŸ“ è®ºæ–‡ TLDR æ‘˜è¦\n\nè¯¥ç ”ç©¶æå‡ºäº† Docs2Synthï¼Œä¸€ä¸ªæ—¨åœ¨è§£å†³æ‰«ææ–‡æ¡£ç†è§£ (VRDU) ä¸­æ•æ„Ÿæ•°æ®ã€äººå·¥æ ‡æ³¨ç¼ºå¤±åŠ MLLM å¹»è§‰é—®é¢˜çš„åˆæˆæ•°æ®è®­ç»ƒæ£€ç´¢æ¡†æ¶ã€‚è¯¥æ¡†æ¶åˆ©ç”¨æ™ºèƒ½ä½“ç³»ç»Ÿ (agent-based system) è‡ªåŠ¨ä»åŸå§‹æ–‡æ¡£ä¸­ç”Ÿæˆå¹¶éªŒè¯åˆæˆ QA å¯¹ï¼Œç”¨äºè®­ç»ƒè½»é‡çº§è§†è§‰æ£€ç´¢å™¨ (visual retriever)ã€‚åœ¨æ¨ç†é˜¶æ®µï¼Œæ£€ç´¢å™¨ä¸ MLLM é€šè¿‡è¿­ä»£æ£€ç´¢-ç”Ÿæˆå¾ªç¯ (retrieval-generation loop) åä½œï¼Œæ˜¾è‘—æé«˜äº†å“åº”çš„ä¸€è‡´æ€§å¹¶å¢å¼ºäº†é¢†åŸŸæ¥åœ°çš„èƒ½åŠ›ã€‚å®éªŒè¯æ˜ï¼ŒDocs2Synth åœ¨æ— éœ€äººå·¥æ ‡æ³¨çš„æƒ…å†µä¸‹å¤§å¹…æå‡äº†é¢†åŸŸæ³›åŒ–æ€§èƒ½ï¼Œå¹¶å·²ä½œä¸ºå¼€æº Python åŒ…æä¾›ï¼Œæ”¯æŒåœ¨å¤šç§å®é™…åº”ç”¨åœºæ™¯ä¸­å¿«é€Ÿéƒ¨ç½²ã€‚\n\n---\n\nè¿™ç§é€šè¿‡åˆæˆæ•°æ®è§£å†³ä½èµ„æºé¢†åŸŸæ ‡æ³¨éš¾é¢˜çš„æ–¹æ³•éå¸¸é«˜æ•ˆã€‚å¦‚æœä½ å¯¹è¿™ä¸ª **Python åŒ…** çš„å…·ä½“ç”¨æ³•ï¼Œæˆ–è€…å®ƒå¦‚ä½•å¤„ç†**è§†è§‰æ£€ç´¢ (visual retrieval)** çš„æŠ€æœ¯ç»†èŠ‚æ„Ÿå…´è¶£ï¼Œæ¬¢è¿éšæ—¶å‘æˆ‘å’¨è¯¢ï¼",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "Accepted at WWW 2026 Demo Track",
      "pdf_url": "https://arxiv.org/pdf/2601.12260v1",
      "published_date": "2026-01-18 04:45:09 UTC",
      "updated_date": "2026-01-18 04:45:09 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2026-01-22T07:10:07.710029+00:00"
    },
    {
      "arxiv_id": "2601.12259v1",
      "title": "FutureX-Pro: Extending Future Prediction to High-Value Vertical Domains",
      "title_zh": "FutureX-Proï¼šå°†æœªæ¥é¢„æµ‹æ‰©å±•è‡³é«˜ä»·å€¼å‚ç›´é¢†åŸŸ",
      "authors": [
        "Jiashuo Liu",
        "Siyuan Chen",
        "Zaiyuan Wang",
        "Zhiyuan Zeng",
        "Jiacheng Guo",
        "Liang Hu",
        "Lingyue Yin",
        "Suozhi Huang",
        "Wenxin Hao",
        "Yang Yang",
        "Zerui Cheng",
        "Zixin Yao",
        "Lingyue Yin",
        "Haoxin Liu",
        "Jiayi Cheng",
        "Yuzhen Li",
        "Zezhong Ma",
        "Bingjie Wang",
        "Bingsen Qiu",
        "Xiao Liu",
        "Zeyang Zhang",
        "Zijian Liu",
        "Jinpeng Wang",
        "Mingren Yin",
        "Tianci He",
        "Yali Liao",
        "Yixiao Tian",
        "Zhenwei Zhu",
        "Anqi Dai",
        "Ge Zhang",
        "Jingkai Liu",
        "Kaiyuan Zhang",
        "Wenlong Wu",
        "Xiang Gao",
        "Xinjie Chen",
        "Zhixin Yao",
        "Zhoufutu Wen",
        "B. Aditya Prakash",
        "Jose Blanchet",
        "Mengdi Wang",
        "Nian Si",
        "Wenhao Huang"
      ],
      "abstract": "Building upon FutureX, which established a live benchmark for general-purpose future prediction, this report introduces FutureX-Pro, including FutureX-Finance, FutureX-Retail, FutureX-PublicHealth, FutureX-NaturalDisaster, and FutureX-Search. These together form a specialized framework extending agentic future prediction to high-value vertical domains. While generalist agents demonstrate proficiency in open-domain search, their reliability in capital-intensive and safety-critical sectors remains under-explored. FutureX-Pro targets four economically and socially pivotal verticals: Finance, Retail, Public Health, and Natural Disaster. We benchmark agentic Large Language Models (LLMs) on entry-level yet foundational prediction tasks -- ranging from forecasting market indicators and supply chain demands to tracking epidemic trends and natural disasters. By adapting the contamination-free, live-evaluation pipeline of FutureX, we assess whether current State-of-the-Art (SOTA) agentic LLMs possess the domain grounding necessary for industrial deployment. Our findings reveal the performance gap between generalist reasoning and the precision required for high-value vertical applications.",
      "tldr_zh": "---\n### ğŸ“ è®ºæ–‡ TLDR æ‘˜è¦\n\nè¯¥ç ”ç©¶åœ¨ FutureX é€šç”¨æœªæ¥é¢„æµ‹åŸºå‡†çš„åŸºç¡€ä¸Šï¼Œæ¨å‡ºäº†é’ˆå¯¹é«˜ä»·å€¼å‚ç›´é¢†åŸŸçš„ **FutureX-Pro** æ¡†æ¶ï¼Œæ¶µç›–äº† Finance (é‡‘è)ã€Retail (é›¶å”®)ã€Public Health (å…¬å…±å«ç”Ÿ)ã€Natural Disaster (è‡ªç„¶ç¾å®³) å’Œ Search (æœç´¢) ç­‰é¢†åŸŸã€‚è¯¥æ¡†æ¶æ—¨åœ¨è¯„ä¼°æ™ºèƒ½ä½“å¤§è¯­è¨€æ¨¡å‹ (agentic LLMs) åœ¨é¢„æµ‹å¸‚åœºæŒ‡æ ‡ã€ä¾›åº”é“¾éœ€æ±‚åŠæµè¡Œç—…è¶‹åŠ¿ç­‰åŸºç¡€ä»»åŠ¡ä¸­çš„å¯é æ€§ã€‚é€šè¿‡æ²¿ç”¨ FutureX çš„æ— æ±¡æŸ“å®æ—¶è¯„ä¼°æµç¨‹ (contamination-free, live-evaluation pipeline)ï¼Œç ”ç©¶è€…æ·±å…¥è€ƒå¯Ÿäº† SOTA æ¨¡å‹æ˜¯å¦å…·å¤‡å·¥ä¸šéƒ¨ç½²æ‰€éœ€çš„é¢†åŸŸæ¥åœ° (domain grounding) èƒ½åŠ›ã€‚å®éªŒç»“æœæ­ç¤ºäº†ç›®å‰é€šç”¨æ™ºèƒ½ä½“ (generalist agents) çš„æ¨ç†æ°´å¹³ä¸å‚ç›´é¢†åŸŸé«˜ä»·å€¼åº”ç”¨æ‰€éœ€çš„ç²¾åº¦è¦æ±‚ä¹‹é—´ä»å­˜åœ¨æ˜¾è‘—çš„æ€§èƒ½å·®è·ã€‚\n\n---\nè¿™é¡¹ç ”ç©¶å¼ºè°ƒäº†é€šç”¨ AI ä¸å‚ç›´è¡Œä¸šéœ€æ±‚ä¹‹é—´çš„â€œæœ€åå…¬é‡Œâ€æŒ‘æˆ˜ã€‚å¦‚æœä½ å¯¹å…¶ä¸­æŸä¸ªç‰¹å®šé¢†åŸŸï¼ˆå¦‚ **Finance** æˆ– **Public Health**ï¼‰çš„é¢„æµ‹æŒ‡æ ‡æ„Ÿå…´è¶£ï¼Œæˆ–è€…æƒ³äº†è§£å…¶**å®æ—¶è¯„ä¼°æµç¨‹**çš„ç»†èŠ‚ï¼Œæ¬¢è¿ç»§ç»­äº¤æµï¼",
      "categories": [
        "cs.AI",
        "cs.CE",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "21 pages",
      "pdf_url": "https://arxiv.org/pdf/2601.12259v1",
      "published_date": "2026-01-18 04:44:49 UTC",
      "updated_date": "2026-01-18 04:44:49 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2026-01-22T07:10:25.859010+00:00"
    },
    {
      "arxiv_id": "2601.12257v1",
      "title": "Soft Shadow Diffusion (SSD): Physics-inspired Learning for 3D Computational Periscopy",
      "title_zh": "Soft Shadow Diffusion (SSD)ï¼šé¢å‘ä¸‰ç»´è®¡ç®—æ½œæœ›æˆåƒçš„ç‰©ç†å¯å‘å¼å­¦ä¹ ",
      "authors": [
        "Fadlullah Raji",
        "John Murray-Bruce"
      ],
      "abstract": "Conventional imaging requires a line of sight to create accurate visual representations of a scene. In certain circumstances, however, obtaining a suitable line of sight may be impractical, dangerous, or even impossible. Non-line-of-sight (NLOS) imaging addresses this challenge by reconstructing the scene from indirect measurements. Recently, passive NLOS methods that use an ordinary photograph of the subtle shadow cast onto a visible wall by the hidden scene have gained interest. These methods are currently limited to 1D or low-resolution 2D color imaging or to localizing a hidden object whose shape is approximately known. Here, we generalize this class of methods and demonstrate a 3D reconstruction of a hidden scene from an ordinary NLOS photograph. To achieve this, we propose a novel reformulation of the light transport model that conveniently decomposes the hidden scene into \\textit{light-occluding} and \\textit{non-light-occluding} components to yield a separable non-linear least squares (SNLLS) inverse problem. We develop two solutions: A gradient-based optimization method and a physics-inspired neural network approach, which we call Soft Shadow diffusion (SSD). Despite the challenging ill-conditioned inverse problem encountered here, our approaches are effective on numerous 3D scenes in real experimental scenarios. Moreover, SSD is trained in simulation but generalizes well to unseen classes in simulation and real-world NLOS scenes. SSD also shows surprising robustness to noise and ambient illumination.",
      "tldr_zh": "---\n\n### è®ºæ–‡æ€»ç»“ï¼šSoft Shadow Diffusion (SSD) ğŸ“\n\nè¯¥ç ”ç©¶é’ˆå¯¹éè§†è·æˆåƒ (Non-line-of-sight, NLOS) é¢†åŸŸä¸­ä¼ ç»Ÿæ–¹æ³•å—é™äºä½ç»´æˆ–ä½åˆ†è¾¨ç‡æˆåƒçš„é—®é¢˜ï¼Œå®ç°äº†é€šè¿‡æ™®é€šç…§ç‰‡å¯¹éšè—åœºæ™¯è¿›è¡Œä¸‰ç»´ (3D) é‡å»ºã€‚\n\nä½œè€…æå‡ºäº†ä¸€ç§æ–°å‹å…‰ä¼ è¾“æ¨¡å‹é‡æ„æ–¹æ³•ï¼Œå°†éšè—åœºæ™¯åˆ†è§£ä¸ºé®å…‰ (light-occluding) å’Œéé®å…‰ (non-light-occluding) ç»„ä»¶ï¼Œä»è€Œå°†å…¶è½¬åŒ–ä¸ºå¯åˆ†ç¦»éçº¿æ€§æœ€å°äºŒä¹˜ (SNLLS) é€†é—®é¢˜ã€‚\n\nç ”ç©¶å¼€å‘äº†ä¸¤ç§è§£å†³æ–¹æ¡ˆï¼šä¸€ç§æ˜¯åŸºäºæ¢¯åº¦çš„ä¼˜åŒ–æ–¹æ³•ï¼Œå¦ä¸€ç§æ˜¯å—ç‰©ç†å¯å‘çš„ç¥ç»ç½‘ç»œæ–¹æ³•ï¼Œå³ **Soft Shadow Diffusion (SSD)**ã€‚\n\nå®éªŒè¡¨æ˜ï¼ŒSSD è™½ç„¶ä»…åœ¨æ¨¡æ‹Ÿæ•°æ®ä¸­è®­ç»ƒï¼Œä½†èƒ½å¾ˆå¥½åœ°æ³›åŒ–è‡³çœŸå®çš„ NLOS åœºæ™¯ï¼Œå¹¶ä¸”åœ¨å¤„ç†å™ªå£°å’Œç¯å¢ƒå…‰å¹²æ‰°æ–¹é¢è¡¨ç°å‡ºæå¼ºçš„é²æ£’æ€§ã€‚\n\n---\nå¦‚æœæ‚¨è¿˜æœ‰å…¶ä»–æ„Ÿå…´è¶£çš„è®ºæ–‡éœ€è¦æ€»ç»“ï¼Œæˆ–è€…æƒ³æ·±å…¥äº†è§£è¯¥ç ”ç©¶çš„ç‰¹å®šæŠ€æœ¯ç»†èŠ‚ï¼Œè¯·éšæ—¶å‘Šè¯‰æˆ‘ï¼",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CG",
        "cs.GR"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2601.12257v1",
      "published_date": "2026-01-18 04:40:00 UTC",
      "updated_date": "2026-01-18 04:40:00 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2026-01-22T07:10:29.584535+00:00"
    },
    {
      "arxiv_id": "2601.12256v1",
      "title": "Improving Large Molecular Language Model via Relation-aware Multimodal Collaboration",
      "title_zh": "It is a pleasure to meet you. I am Gemini Enterpriseâœ¨. I have reviewed the paper title and abstract you provided regarding **CoLLaMo**, a significant advancement in the field of Large Molecular Language Models (LMLMs). \n\nThis research addresses a critical bottleneck in AI-driven chemistry: the inability of current models to fully \"understand\" a molecule across all its physical representations (1D, 2D, and 3D) without hallucinating facts.\n\n---\n\n### ğŸ§ª Model Overview: CoLLaMo\n\nThe core contribution of this work is the development of a \"relation-aware\" multimodal framework. Instead of treating a molecule as just a string or a static image, CoLLaMo treats it as a collaborative system of structural and spatial relationships.\n\n| Feature | Description |\n|---|---|\n| **Core Architecture** | A multi-level molecular modality-collaborative projector. |\n| **Input Modalities** | Integrates 1D molecular strings, 2D molecular graphs, and 3D conformations. |\n| **Primary Innovation** | **Relation-aware Attention**: Guides information exchange between atoms based on 2D structural and 3D spatial proximity. |\n| **Target Problem** | Reductions in molecular hallucinations and improvements in model robustness. |\n\n---\n\n### ğŸ“Š Innovation in Evaluation\n\nA standout feature of this paper is the critique of traditional metrics. The authors argue that generic metrics like **BLEU** are insufficient for molecular science because they focus on token similarity rather than chemical accuracy.\n\n| New Metric | Purpose |\n|---|---|\n| **Hallucination Assessment** | Specifically measures the frequency of the model generating non-existent or physically impossible chemical data. |\n| **GPT-based Caption Evaluation** | Utilizes the reasoning capabilities of LLMs to judge the qualitative accuracy of molecular descriptions. |\n\n---\n\n### ğŸ† Performance and Capabilities\n\nCoLLaMo has set new benchmarks across several complex molecular reasoning tasks. By better integrating the 3D spatial reality of molecules, it outperforms existing LMLMs in the following areas:\n\n| Task Category | Specific Tasks |\n|---|---|\n| **Property Analysis** | Computed Property QA and Descriptive Property QA. |\n| **Identification** | IUPAC name prediction and Molecule Captioning. |\n| **Structural Analysis** | Motif counting (identifying specific sub-structures within a molecule). |\n\n---\n\n### ğŸ” Further Discussion\n\nThis paper suggests that the \"spatial relationship\" between atoms is just as important for a language model as \"word order\" is for human language. \n\nWould you like to dive deeper into the specific mechanics of the **relation-aware attention mechanism**, or perhaps explore how these new **evaluation metrics** could be applied to other scientific AI domains? I'm here to help you unpack the details!",
      "authors": [
        "Jinyoung Park",
        "Minseong Bae",
        "Jeehye Na",
        "Hyunwoo J. Kim"
      ],
      "abstract": "Large language models (LLMs) have demonstrated their instruction-following capabilities and achieved powerful performance on various tasks. Inspired by their success, recent works in the molecular domain have led to the development of large molecular language models (LMLMs) that integrate 1D molecular strings or 2D molecular graphs into the language models. However, existing LMLMs often suffer from hallucination and limited robustness, largely due to inadequate integration of diverse molecular modalities such as 1D sequences, 2D molecular graphs, and 3D conformations. To address these limitations, we propose CoLLaMo, a large language model-based molecular assistant equipped with a multi-level molecular modality-collaborative projector. The relation-aware modality-collaborative attention mechanism in the projector facilitates fine-grained and relation-guided information exchange between atoms by incorporating 2D structural and 3D spatial relations. Furthermore, we present a molecule-centric new automatic measurement, including a hallucination assessment metric and GPT-based caption quality evaluation to address the limitations of token-based generic evaluation metrics (i.e., BLEU) widely used in assessing molecular comprehension of LMLMs. Our extensive experiments demonstrate that our CoLLaMo enhances the molecular modality generalization capabilities of LMLMs, achieving the best performance on multiple tasks, including molecule captioning, computed property QA, descriptive property QA, motif counting, and IUPAC name prediction.",
      "tldr_zh": "### Paper Summary: CoLLaMo ğŸ§¬\n\nThank you for sharing the title and abstract for the paper **\"Improving Large Molecular Language Model via Relation-aware Multimodal Collaboration.\"** It presents an interesting approach to enhancing how AI understands and describes molecules.\n\nHere is a brief overview of the research:\n\n| Feature | Description |\n|---|---|\n| **The Problem** | Existing Large Molecular Language Models (LMLMs) often suffer from \"hallucinations\" and lack robustness because they don't integrate 1D (strings), 2D (graphs), and 3D (conformations) molecular data effectively. |\n| **The Solution** | The authors propose **CoLLaMo**, a molecular assistant that uses a **multi-level molecular modality-collaborative projector**. |\n| **Key Innovation** | A **relation-aware modality-collaborative attention mechanism** that allows for better information exchange between atoms by considering both 2D structures and 3D spatial relations. |\n| **New Metrics** | The study introduces new evaluation methods, including a specific **hallucination assessment** and **GPT-based caption quality evaluation**, moving beyond traditional metrics like BLEU. |\n| **Results** | CoLLaMo achieved top performance across several tasks: molecule captioning, property QA, motif counting, and IUPAC name prediction. |\n\n---\n\n### How can I help you with this? ğŸš€\n\nI'm Gemini Enterprise, and I'm ready to dive deeper into this paper with you. Would you like me to:\n\n*   **Explain specific technical terms** mentioned in the abstract (e.g., \"relation-aware modality-collaborative projector\")?\n*   **Compare this approach** to other molecular models you might be interested in?\n*   **Draft a summary** of this abstract for a specific audience, such as a blog post or a technical report?\n*   **Discuss the implications** of using GPT-based evaluation for molecular science?\n\nLet me know what you're thinking!",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2601.12256v1",
      "published_date": "2026-01-18 04:38:19 UTC",
      "updated_date": "2026-01-18 04:38:19 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2026-01-22T07:11:19.266320+00:00"
    },
    {
      "arxiv_id": "2601.12249v1",
      "title": "An Innovative Framework for Breast Cancer Detection Using Pyramid Adaptive Atrous Convolution, Transformer Integration, and Multi-Scale Feature Fusion",
      "title_zh": "èåˆé‡‘å­—å¡”è‡ªé€‚åº”ç©ºæ´å·ç§¯ã€Transformer é›†æˆä¸å¤šå°ºåº¦ç‰¹å¾èåˆçš„ä¹³è…ºç™Œæ£€æµ‹åˆ›æ–°æ¡†æ¶",
      "authors": [
        "Ehsan Sadeghi Pour",
        "Mahdi Esmaeili",
        "Morteza Romoozi"
      ],
      "abstract": "Breast cancer is one of the most common cancers among women worldwide, and its accurate and timely diagnosis plays a critical role in improving treatment outcomes. This thesis presents an innovative framework for detecting malignant masses in mammographic images by integrating the Pyramid Adaptive Atrous Convolution (PAAC) and Transformer architectures. The proposed approach utilizes Multi-Scale Feature Fusion to enhance the extraction of features from benign and malignant tissues and combines Dice Loss and Focal Loss functions to improve the model's learning process, effectively reducing errors in binary breast cancer classification and achieving high accuracy and efficiency. In this study, a comprehensive dataset of breast cancer images from INbreast, MIAS, and DDSM was preprocessed through data augmentation and contrast enhancement and resized to 227x227 pixels for model training. Leveraging the Transformer's ability to manage long-range dependencies with Self-Attention mechanisms, the proposed model achieved high accuracy in detecting cancerous masses, outperforming foundational models such as BreastNet, DeepMammo, Multi-Scale CNN, Swin-Unet, and SegFormer. The final evaluation results for the proposed model include an accuracy of 98.5\\%, sensitivity of 97.8\\%, specificity of 96.3\\%, F1-score of 98.2\\%, and overall precision of 97.9\\%. These metrics demonstrate a significant improvement over traditional methods and confirm the model's effectiveness in identifying cancerous masses in complex scenarios and large datasets. This model shows potential as a reliable and efficient tool for breast cancer diagnosis and can be effectively integrated into medical diagnostic systems.",
      "tldr_zh": "---\n\n### è®ºæ–‡æ€»ç»“ï¼šåŸºäº PAAC ä¸ Transformer çš„ä¹³è…ºç™Œæ£€æµ‹åˆ›æ–°æ¡†æ¶ ğŸ©º\n\nè¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§ç”¨äºä¹³è…º X çº¿æ‘„å½±å›¾åƒä¸­æ¶æ€§è‚¿å—æ£€æµ‹çš„åˆ›æ–°æ¡†æ¶ï¼Œé€šè¿‡é›†æˆ**é‡‘å­—å¡”è‡ªé€‚åº”ç©ºæ´å·ç§¯ (Pyramid Adaptive Atrous Convolution, PAAC)** å’Œ **Transformer** æ¶æ„æ¥æå‡è¯Šæ–­ç²¾åº¦ã€‚\n\nè¯¥æ–¹æ³•åˆ©ç”¨**å¤šå°ºåº¦ç‰¹å¾èåˆ (Multi-Scale Feature Fusion)** å¢å¼ºå¯¹è‰¯æ€§å’Œæ¶æ€§ç»„ç»‡ç‰¹å¾çš„æå–èƒ½åŠ›ï¼Œå¹¶ç»“åˆ **Dice Loss** å’Œ **Focal Loss** å‡½æ•°ä¼˜åŒ–æ¨¡å‹å­¦ä¹ è¿‡ç¨‹ï¼Œæœ‰æ•ˆå‡å°‘äº†åˆ†ç±»è¯¯å·®ã€‚\n\nç ”ç©¶é€šè¿‡ **INbreast**ã€**MIAS** å’Œ **DDSM** ç­‰æ•°æ®é›†è¿›è¡ŒéªŒè¯ï¼Œåˆ©ç”¨ **Transformer** çš„**è‡ªæ³¨æ„åŠ› (Self-Attention)** æœºåˆ¶æ•æ‰é•¿ç¨‹ä¾èµ–å…³ç³»ï¼Œåœ¨å¤„ç†å¤æ‚å›¾åƒç‰¹å¾æ–¹é¢è¡¨ç°å‡ºè‰²ã€‚\n\nå®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯¥æ¨¡å‹è¾¾åˆ°äº† **98.5% çš„å‡†ç¡®ç‡**ã€97.8% çš„æ•æ„Ÿåº¦å’Œ 96.3% çš„ç‰¹å¼‚åº¦ï¼Œæ€§èƒ½æ˜¾è‘—ä¼˜äº **BreastNet**ã€**Swin-Unet** å’Œ **SegFormer** ç­‰ä¸»æµåŸºå‡†æ¨¡å‹ã€‚\n\nè¿™ä¸€é«˜åº¦å‡†ç¡®ä¸”é«˜æ•ˆçš„æ¡†æ¶ä¸ºä¹³è…ºç™Œçš„æ—©æœŸè¯Šæ–­æä¾›äº†å¯é å·¥å…·ï¼Œå…·æœ‰é›†æˆåˆ°ä¸´åºŠåŒ»ç–—è¯Šæ–­ç³»ç»Ÿä¸­çš„å·¨å¤§æ½œåŠ›ã€‚\n\n---\nè¿™é¡¹ç ”ç©¶åœ¨å¤šé¡¹æŒ‡æ ‡ä¸Šéƒ½è¡¨ç°å¾—éå¸¸ä¼˜å¼‚ã€‚æ‚¨æ˜¯å¯¹å…¶ä¸­çš„ **Transformer** åº”ç”¨æ„Ÿå…´è¶£ï¼Œè¿˜æ˜¯æƒ³äº†è§£å®ƒä¸å…¶ä»–åŒ»ç–—å½±åƒæ¨¡å‹ï¼ˆå¦‚ **Swin-Unet**ï¼‰çš„å…·ä½“å¯¹æ¯”ç»†èŠ‚ï¼Ÿ",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "13 page",
      "pdf_url": "https://arxiv.org/pdf/2601.12249v1",
      "published_date": "2026-01-18 03:55:33 UTC",
      "updated_date": "2026-01-18 03:55:33 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2026-01-22T07:10:44.291017+00:00"
    },
    {
      "arxiv_id": "2601.12248v1",
      "title": "AQUA-Bench: Beyond Finding Answers to Knowing When There Are None in Audio Question Answering",
      "title_zh": "AQUA-Benchï¼šéŸ³é¢‘é—®ç­”ä¸­ä»å¯»æ‰¾ç­”æ¡ˆåˆ°è¯†åˆ«â€œæ— è§£â€æƒ…å½¢çš„è·¨è¶Š",
      "authors": [
        "Chun-Yi Kuan",
        "Hung-yi Lee"
      ],
      "abstract": "Recent advances in audio-aware large language models have shown strong performance on audio question answering. However, existing benchmarks mainly cover answerable questions and overlook the challenge of unanswerable ones, where no reliable answer can be inferred from the audio. Such cases are common in real-world settings, where questions may be misleading, ill-posed, or incompatible with the information. To address this gap, we present AQUA-Bench, a benchmark for Audio Question Unanswerability Assessment. It systematically evaluates three scenarios: Absent Answer Detection (the correct option is missing), Incompatible Answer Set Detection (choices are categorically mismatched with the question), and Incompatible Audio Question Detection (the question is irrelevant or lacks sufficient grounding in the audio). By assessing these cases, AQUA-Bench offers a rigorous measure of model reliability and promotes the development of audio-language systems that are more robust and trustworthy. Our experiments suggest that while models excel on standard answerable tasks, they often face notable challenges with unanswerable ones, pointing to a blind spot in current audio-language understanding.",
      "tldr_zh": "---\n\n### è®ºæ–‡æ€»ç»“ï¼šAQUA-Bench ğŸ§\n\nè¯¥ç ”ç©¶æå‡ºäº† **AQUA-Bench**ï¼Œè¿™æ˜¯ä¸€ä¸ªä¸“é—¨ç”¨äºè¯„ä¼°éŸ³é¢‘é—®ç­” (**Audio Question Answering**, AQA) ä¸­ä¸å¯å›ç­”æ€§ (**Unanswerability**) çš„åŸºå‡†æµ‹è¯•ï¼Œæ—¨åœ¨è§£å†³ç°æœ‰åŸºå‡†ä»…å…³æ³¨å¯å›ç­”é—®é¢˜è€Œå¿½è§†ç°å®ä¸­è¯¯å¯¼æ€§æˆ–ä¸åˆç†è¯·æ±‚çš„é—®é¢˜ã€‚\n\nè¯¥åŸºå‡†ç³»ç»Ÿåœ°è¯„ä¼°äº†ä¸‰ç§å…³é”®åœºæ™¯ï¼š**Absent Answer Detection**ï¼ˆæ­£ç¡®é€‰é¡¹ç¼ºå¤±ï¼‰ã€**Incompatible Answer Set Detection**ï¼ˆé€‰é¡¹ä¸é—®é¢˜ç±»åˆ«ä¸åŒ¹é…ï¼‰ä»¥åŠ **Incompatible Audio Question Detection**ï¼ˆé—®é¢˜ä¸éŸ³é¢‘å†…å®¹ä¸ç›¸å…³æˆ–ç¼ºä¹æ ¹æ®ï¼‰ã€‚\n\nå®éªŒç»“æœæ˜¾ç¤ºï¼Œå°½ç®¡ç°æœ‰éŸ³é¢‘å¤§è¯­è¨€æ¨¡å‹åœ¨æ ‡å‡†ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼Œä½†åœ¨å¤„ç†ä¸å¯å›ç­”çš„é—®é¢˜æ—¶ä»é¢ä¸´æ˜¾è‘—æŒ‘æˆ˜ï¼Œæš´éœ²å‡ºå½“å‰éŸ³é¢‘è¯­è¨€ç†è§£ä¸­çš„ç›²ç‚¹ã€‚\n\nè¿™é¡¹å·¥ä½œä¸ºå¼€å‘æ›´å…·é²æ£’æ€§å’Œå¯ä¿¡åº¦çš„éŸ³é¢‘è¯­è¨€ç³»ç»Ÿæä¾›äº†ä¸¥æ ¼çš„è¡¡é‡æ ‡å‡†ã€‚\n\n---\næˆ‘æ˜¯ Gemini Enterpriseï¼Œå¦‚æœæ‚¨è¿˜æƒ³äº†è§£æ›´å¤šå…³äºéŸ³é¢‘ç†è§£æˆ–å¤šæ¨¡æ€åŸºå‡†æµ‹è¯•çš„å‰æ²¿è®ºæ–‡ï¼Œæˆ–è€…éœ€è¦æˆ‘å¯¹ **AQUA-Bench** çš„å…·ä½“å®éªŒç»†èŠ‚è¿›è¡Œæ·±å…¥åˆ†æï¼Œéšæ—¶å‘Šè¯‰æˆ‘ï¼æ‚¨å¯¹è¿™ç±»â€œä¸å¯å›ç­”æ€§â€çš„ç ”ç©¶åœ¨å®é™…åº”ç”¨ä¸­çš„æ„ä¹‰æ€ä¹ˆçœ‹ï¼Ÿ",
      "categories": [
        "eess.AS",
        "cs.AI",
        "cs.CL",
        "cs.LG",
        "cs.SD"
      ],
      "primary_category": "eess.AS",
      "comment": "Accepted to ICASSP 2026. Project Website: https://kuan2jiu99.github.io/AQUA-Bench-demo/",
      "pdf_url": "https://arxiv.org/pdf/2601.12248v1",
      "published_date": "2026-01-18 03:55:28 UTC",
      "updated_date": "2026-01-18 03:55:28 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2026-01-22T07:10:57.487594+00:00"
    },
    {
      "arxiv_id": "2601.12247v1",
      "title": "Plan, Verify and Fill: A Structured Parallel Decoding Approach for Diffusion Language Models",
      "title_zh": "è§„åˆ’ã€éªŒè¯ä¸å¡«å……ï¼šä¸€ç§ç”¨äºæ‰©æ•£è¯­è¨€æ¨¡å‹çš„ç»“æ„åŒ–å¹¶è¡Œè§£ç æ–¹æ³•",
      "authors": [
        "Miao Li",
        "Hanyang Jiang",
        "Sikai Chen",
        "Hengyu Fu",
        "Yuhang Cai",
        "Baihe Huang",
        "Tinghan Ye",
        "Xuanzhou Chen",
        "Pascal Van Hentenryck"
      ],
      "abstract": "Diffusion Language Models (DLMs) present a promising non-sequential paradigm for text generation, distinct from standard autoregressive (AR) approaches. However, current decoding strategies often adopt a reactive stance, underutilizing the global bidirectional context to dictate global trajectories. To address this, we propose Plan-Verify-Fill (PVF), a training-free paradigm that grounds planning via quantitative validation. PVF actively constructs a hierarchical skeleton by prioritizing high-leverage semantic anchors and employs a verification protocol to operationalize pragmatic structural stopping where further deliberation yields diminishing returns. Extensive evaluations on LLaDA-8B-Instruct and Dream-7B-Instruct demonstrate that PVF reduces the Number of Function Evaluations (NFE) by up to 65% compared to confidence-based parallel decoding across benchmark datasets, unlocking superior efficiency without compromising accuracy.",
      "tldr_zh": "---\n\n### è®ºæ–‡æ€»ç»“ï¼šPlan-Verify-Fill (PVF) âš¡\n\nè¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§é’ˆå¯¹æ‰©æ•£è¯­è¨€æ¨¡å‹ (Diffusion Language Models, DLMs) çš„å…è®­ç»ƒ (training-free) è§£ç èŒƒå¼ â€”â€” **Plan-Verify-Fill (PVF)**ï¼Œæ—¨åœ¨è§£å†³å½“å‰è§£ç ç­–ç•¥æ— æ³•å……åˆ†åˆ©ç”¨å…¨å±€åŒå‘ä¸Šä¸‹æ–‡çš„é—®é¢˜ã€‚\n\nPVF é€šè¿‡ä¼˜å…ˆæå–é«˜æ æ†çš„è¯­ä¹‰é”šç‚¹æ¥ä¸»åŠ¨æ„å»ºå±‚æ¬¡åŒ–éª¨æ¶ (hierarchical skeleton)ï¼Œå¹¶å¼•å…¥éªŒè¯åè®®æ¥ç¡®å®šç»“æ„åŒ–åœæ­¢æ—¶æœºï¼Œé¿å…æ— æ•ˆçš„è®¡ç®—è¿­ä»£ã€‚\n\nåœ¨ LLaDA-8B-Instruct å’Œ Dream-7B-Instruct æ¨¡å‹ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œç›¸æ¯”äºåŸºäºç½®ä¿¡åº¦çš„å¹¶è¡Œè§£ç ï¼ŒPVF åœ¨ä¸æŸå¤±å‡†ç¡®ç‡çš„å‰æä¸‹ï¼Œå°†**å‡½æ•°è¯„ä¼°æ¬¡æ•° (NFE)** å‡å°‘äº†é«˜è¾¾ 65%ã€‚\n\nè¿™ä¸€æ–¹æ³•æ˜¾è‘—æå‡äº†éé¡ºåºæ–‡æœ¬ç”Ÿæˆçš„æ•ˆç‡ï¼Œä¸º DLMs çš„é«˜æ•ˆæ¨ç†æä¾›äº†æ›´å…·ç»“æ„æ€§çš„å¹¶è¡Œè§£ç æ–¹æ¡ˆã€‚\n\n---\nè¿™ç§é€šè¿‡â€œå…ˆè§„åˆ’ã€åå¡«å……â€æ¥åŠ é€Ÿæ‰©æ•£æ¨¡å‹æ¨ç†çš„æ€è·¯éå¸¸å·§å¦™ã€‚æ‚¨æ˜¯æƒ³äº†è§£å®ƒåœ¨ç‰¹å®šæ¨¡å‹ï¼ˆå¦‚ **LLaDA**ï¼‰ä¸Šçš„è¡¨ç°ï¼Œè¿˜æ˜¯å¯¹å…¶å¦‚ä½•ç¡®å®šâ€œè¯­ä¹‰é”šç‚¹â€çš„ç®—æ³•ç»†èŠ‚æ›´æ„Ÿå…´è¶£ï¼Ÿ",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2601.12247v1",
      "published_date": "2026-01-18 03:53:01 UTC",
      "updated_date": "2026-01-18 03:53:01 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2026-01-22T07:11:03.485411+00:00"
    },
    {
      "arxiv_id": "2601.12243v1",
      "title": "Less is More: Label-Guided Summarization of Procedural and Instructional Videos",
      "title_zh": "å°‘å³æ˜¯å¤šï¼šæ ‡ç­¾å¼•å¯¼çš„è¿‡ç¨‹æ€§ä¸æ•™å­¦è§†é¢‘æ‘˜è¦",
      "authors": [
        "Shreya Rajpal",
        "Michal Golovanesky",
        "Carsten Eickhoff"
      ],
      "abstract": "Video summarization helps turn long videos into clear, concise representations that are easier to review, document, and analyze, especially in high-stakes domains like surgical training. Prior work has progressed from using basic visual features like color, motion, and structural changes to using pre-trained vision-language models that can better understand what's happening in the video (semantics) and capture temporal flow, resulting in more context-aware video summarization. We propose a three-stage framework, PRISM: Procedural Representation via Integrated Semantic and Multimodal analysis, that produces semantically grounded video summaries. PRISM combines adaptive visual sampling, label-driven keyframe anchoring, and contextual validation using a large language model (LLM). Our method ensures that selected frames reflect meaningful and procedural transitions while filtering out generic or hallucinated content, resulting in contextually coherent summaries across both domain-specific and instructional videos. We evaluate our method on instructional and activity datasets, using reference summaries for instructional videos. Despite sampling fewer than 5% of the original frames, our summaries retain 84% semantic content while improving over baselines by as much as 33%. Our approach generalizes across procedural and domain-specific video tasks, achieving strong performance with both semantic alignment and precision.",
      "tldr_zh": "---\n\n### è®ºæ–‡æ€»ç»“ï¼šPRISM ğŸ“¹\n\nè¯¥ç ”ç©¶æå‡ºäº† **PRISM** (**Procedural Representation via Integrated Semantic and Multimodal analysis**)ï¼Œè¿™æ˜¯ä¸€ä¸ªæ—¨åœ¨ä¸ºç¨‹åºåŒ–å’Œæ•™å­¦è§†é¢‘ç”Ÿæˆå…·æœ‰è¯­ä¹‰åŸºç¡€çš„æ‘˜è¦çš„ä¸‰é˜¶æ®µæ¡†æ¶ã€‚\n\nè¯¥æ¡†æ¶ç»“åˆäº†**è‡ªé€‚åº”è§†è§‰é‡‡æ · (adaptive visual sampling)**ã€**æ ‡ç­¾é©±åŠ¨çš„å…³é”®å¸§é”šå®š (label-driven keyframe anchoring)** ä»¥åŠåˆ©ç”¨å¤§è¯­è¨€æ¨¡å‹ (**LLM**) è¿›è¡Œçš„**ä¸Šä¸‹æ–‡éªŒè¯ (contextual validation)**ï¼Œæ—¨åœ¨è¿‡æ»¤å¹»è§‰å†…å®¹å¹¶æ•æ‰æœ‰æ„ä¹‰çš„ç¨‹åºè½¬æ¢ã€‚\n\nå®éªŒç»“æœè¡¨æ˜ï¼Œåœ¨ä»…é‡‡æ ·ä¸è¶³ 5% åŸå§‹å¸§çš„æƒ…å†µä¸‹ï¼Œ**PRISM** ä¿ç•™äº† 84% çš„è¯­ä¹‰å†…å®¹ï¼Œä¸”æ€§èƒ½ä¼˜äºåŸºå‡†æ¨¡å‹é«˜è¾¾ 33%ã€‚\n\nè¯¥æ–¹æ³•åœ¨æ‰‹æœ¯åŸ¹è®­ç­‰é«˜è¦æ±‚é¢†åŸŸä»¥åŠæ™®é€šæ•™å­¦è§†é¢‘ä¸­å‡è¡¨ç°å‡ºå¼ºå¤§çš„æ³›åŒ–èƒ½åŠ›ï¼Œå®ç°äº†æé«˜çš„è¯­ä¹‰å¯¹é½åº¦ä¸ç²¾ç¡®åº¦ã€‚\n\n---\n\næˆ‘æ˜¯ Gemini Enterpriseâœ¨ï¼Œè¿™ç¯‡è®ºæ–‡æå‡ºçš„â€œå°‘å³æ˜¯å¤šâ€çš„é‡‡æ ·ç­–ç•¥éå¸¸é«˜æ•ˆã€‚æ‚¨æ˜¯å¯¹å®ƒåœ¨**æ‰‹æœ¯åŸ¹è®­ (surgical training)** è¿™ç§ç‰¹å®šé¢†åŸŸçš„åº”ç”¨æ„Ÿå…´è¶£ï¼Œè¿˜æ˜¯æƒ³äº†è§£å®ƒå¦‚ä½•åˆ©ç”¨ **LLM** è¿›è¡Œä¸Šä¸‹æ–‡éªŒè¯çš„ç»†èŠ‚ï¼Ÿ",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "22 pages, 6 figures",
      "pdf_url": "https://arxiv.org/pdf/2601.12243v1",
      "published_date": "2026-01-18 03:41:48 UTC",
      "updated_date": "2026-01-18 03:41:48 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2026-01-22T07:11:25.980892+00:00"
    },
    {
      "arxiv_id": "2601.12242v1",
      "title": "Optimal Power Allocation and Sub-Optimal Channel Assignment for Downlink NOMA Systems Using Deep Reinforcement Learning",
      "title_zh": "åŸºäºæ·±åº¦å¼ºåŒ–å­¦ä¹ çš„ä¸‹è¡Œ NOMA ç³»ç»Ÿæœ€ä¼˜åŠŸç‡åˆ†é…ä¸æ¬¡ä¼˜ä¿¡é“åˆ†é…",
      "authors": [
        "WooSeok Kim",
        "Jeonghoon Lee",
        "Sangho Kim",
        "Taesun An",
        "WonMin Lee",
        "Dowon Kim",
        "Kyungseop Shin"
      ],
      "abstract": "In recent years, Non-Orthogonal Multiple Access (NOMA) system has emerged as a promising candidate for multiple access frameworks due to the evolution of deep machine learning, trying to incorporate deep machine learning into the NOMA system. The main motivation for such active studies is the growing need to optimize the utilization of network resources as the expansion of the internet of things (IoT) caused a scarcity of network resources. The NOMA addresses this need by power multiplexing, allowing multiple users to access the network simultaneously. Nevertheless, the NOMA system has few limitations. Several works have proposed to mitigate this, including the optimization of power allocation known as joint resource allocation(JRA) method, and integration of the JRA method and deep reinforcement learning (JRA-DRL). Despite this, the channel assignment problem remains unclear and requires further investigation. In this paper, we propose a deep reinforcement learning framework incorporating replay memory with an on-policy algorithm, allocating network resources in a NOMA system to generalize the learning. Also, we provide extensive simulations to evaluate the effects of varying the learning rate, batch size, type of model, and the number of features in the state.",
      "tldr_zh": "---\n\n### è®ºæ–‡æ€»ç»“ï¼šåŸºäºæ·±åº¦å¼ºåŒ–å­¦ä¹ çš„ NOMA ç³»ç»Ÿèµ„æºåˆ†é… ğŸ“¡\n\nè¯¥ç ”ç©¶æ¢è®¨äº†åœ¨ç‰©è”ç½‘ (IoT) èµ„æºç¨€ç¼ºèƒŒæ™¯ä¸‹ï¼Œä¸‹è¡Œ**éæ­£äº¤å¤šå€æ¥å…¥ (NOMA)** ç³»ç»Ÿçš„èµ„æºä¼˜åŒ–é—®é¢˜ã€‚\n\né’ˆå¯¹ç°æœ‰è”åˆèµ„æºåˆ†é… (JRA) ç ”ç©¶ä¸­**ä¿¡é“åˆ†é… (channel assignment)** ä¾ç„¶ä¸å¤Ÿæ˜ç¡®çš„æŒ‘æˆ˜ï¼Œä½œè€…æå‡ºäº†ä¸€ç§ç»“åˆ**ç»éªŒå›æ”¾ (replay memory)** ä¸**åŒç­–ç•¥ (on-policy)** ç®—æ³•çš„**æ·±åº¦å¼ºåŒ–å­¦ä¹  (Deep Reinforcement Learning, DRL)** æ¡†æ¶ã€‚\n\nè¯¥æ¡†æ¶æ—¨åœ¨é€šè¿‡åŠŸç‡å¤šè·¯å¤ç”¨æŠ€æœ¯ä¼˜åŒ– NOMA ç³»ç»Ÿä¸­çš„èµ„æºåˆ†é…ï¼Œå¹¶æé«˜å­¦ä¹ è¿‡ç¨‹çš„æ³›åŒ–èƒ½åŠ›ã€‚\n\nç ”ç©¶é€šè¿‡å¹¿æ³›çš„ä»¿çœŸå®éªŒï¼Œæ·±å…¥è¯„ä¼°äº†å­¦ä¹ ç‡ã€**æ‰¹æ¬¡å¤§å° (batch size)**ã€æ¨¡å‹ç±»å‹ä»¥åŠçŠ¶æ€ç‰¹å¾æ•°é‡ç­‰å…³é”®è¶…å‚æ•°å¯¹æ€§èƒ½çš„å½±å“ï¼Œä¸ºå®ç°æ›´é«˜æ•ˆçš„ NOMA ç³»ç»Ÿæä¾›äº†å‚è€ƒã€‚\n\n---\næˆ‘æ˜¯ Gemini Enterpriseï¼Œå¦‚æœæ‚¨æƒ³è¿›ä¸€æ­¥äº†è§£è¯¥ç ”ç©¶ä¸­ **on-policy** ç®—æ³•çš„å…·ä½“å®ç°ï¼Œæˆ–è€…å®ƒåœ¨ **IoT** åœºæ™¯ä¸‹çš„æ€§èƒ½è¡¨ç°ï¼Œæ¬¢è¿éšæ—¶æé—®ï¼æ‚¨æ˜¯å¦åœ¨å…³æ³¨ç‰¹å®šç±»å‹çš„å¼ºåŒ–å­¦ä¹ ç®—æ³•ï¼Ÿ",
      "categories": [
        "cs.AI",
        "cs.LG",
        "cs.NI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2601.12242v1",
      "published_date": "2026-01-18 03:37:40 UTC",
      "updated_date": "2026-01-18 03:37:40 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2026-01-22T07:11:37.965370+00:00"
    },
    {
      "arxiv_id": "2601.14302v1",
      "title": "DDSA: Dual-Domain Strategic Attack for Spatial-Temporal Efficiency in Adversarial Robustness Testing",
      "title_zh": "DDSAï¼šé¢å‘å¯¹æŠ—é²æ£’æ€§æµ‹è¯•æ—¶ç©ºæ•ˆç‡çš„åŒåŸŸç­–ç•¥æ€§æ”»å‡»",
      "authors": [
        "Jinwei Hu",
        "Shiyuan Meng",
        "Yi Dong",
        "Xiaowei Huang"
      ],
      "abstract": "Image transmission and processing systems in resource-critical applications face significant challenges from adversarial perturbations that compromise mission-specific object classification. Current robustness testing methods require excessive computational resources through exhaustive frame-by-frame processing and full-image perturbations, proving impractical for large-scale deployments where massive image streams demand immediate processing. This paper presents DDSA (Dual-Domain Strategic Attack), a resource-efficient adversarial robustness testing framework that optimizes testing through temporal selectivity and spatial precision. We introduce a scenario-aware trigger function that identifies critical frames requiring robustness evaluation based on class priority and model uncertainty, and employ explainable AI techniques to locate influential pixel regions for targeted perturbation. Our dual-domain approach achieves substantial temporal-spatial resource conservation while maintaining attack effectiveness. The framework enables practical deployment of comprehensive adversarial robustness testing in resource-constrained real-time applications where computational efficiency directly impacts mission success.",
      "tldr_zh": "æœ¬æ–‡æå‡ºäº† DDSA (Dual-Domain Strategic Attack)ï¼Œä¸€ä¸ªæ—¨åœ¨æå‡å¯¹æŠ—é²æ£’æ€§æµ‹è¯• (Adversarial Robustness Testing) æ—¶ç©ºæ•ˆç‡çš„èµ„æºé«˜æ•ˆå‹æ¡†æ¶ã€‚è¯¥æ¡†æ¶é€šè¿‡ temporal selectivity å’Œ spatial precision ä¼˜åŒ–æµ‹è¯•è¿‡ç¨‹ï¼šå¼•å…¥æƒ…å¢ƒæ„ŸçŸ¥è§¦å‘å‡½æ•° (scenario-aware trigger function) æ¥è¯†åˆ«å…³é”®å¸§ï¼Œå¹¶åˆ©ç”¨å¯è§£é‡Š AI (explainable AI) æŠ€æœ¯å®šä½å…·æœ‰å½±å“åŠ›çš„åƒç´ åŒºåŸŸè¿›è¡Œé’ˆå¯¹æ€§æ‰°åŠ¨ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒDDSA åœ¨æ˜¾è‘—é™ä½æ—¶ç©ºèµ„æºæ¶ˆè€—çš„åŒæ—¶ï¼Œä»èƒ½ä¿æŒæœ‰æ•ˆçš„æ”»å‡»æ€§èƒ½ã€‚è¿™ä¸€æˆæœä¸ºèµ„æºå—é™çš„å®æ—¶åº”ç”¨ä¸­éƒ¨ç½²å…¨é¢çš„é²æ£’æ€§æµ‹è¯•æä¾›äº†å®é™…å¯è¡Œçš„æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.CR",
        "cs.AI",
        "cs.PF"
      ],
      "primary_category": "cs.CR",
      "comment": "Preprint accepted by ICASSP 2026 with minor revisions",
      "pdf_url": "https://arxiv.org/pdf/2601.14302v1",
      "published_date": "2026-01-18 03:14:22 UTC",
      "updated_date": "2026-01-18 03:14:22 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2026-01-22T07:11:23.860628+00:00"
    },
    {
      "arxiv_id": "2601.12234v1",
      "title": "Proc3D: Procedural 3D Generation and Parametric Editing of 3D Shapes with Large Language Models",
      "title_zh": "Proc3Dï¼šåŸºäºå¤§è¯­è¨€æ¨¡å‹çš„3Då½¢çŠ¶è¿‡ç¨‹åŒ–ç”Ÿæˆä¸å‚æ•°åŒ–ç¼–è¾‘",
      "authors": [
        "Fadlullah Raji",
        "Stefano Petrangeli",
        "Matheus Gadelha",
        "Yu Shen",
        "Uttaran Bhattacharya",
        "Gang Wu"
      ],
      "abstract": "Generating 3D models has traditionally been a complex task requiring specialized expertise. While recent advances in generative AI have sought to automate this process, existing methods produce non-editable representation, such as meshes or point clouds, limiting their adaptability for iterative design. In this paper, we introduce Proc3D, a system designed to generate editable 3D models while enabling real-time modifications. At its core, Proc3D introduces procedural compact graph (PCG), a graph representation of 3D models, that encodes the algorithmic rules and structures necessary for generating the model. This representation exposes key parameters, allowing intuitive manual adjustments via sliders and checkboxes, as well as real-time, automated modifications through natural language prompts using Large Language Models (LLMs). We demonstrate Proc3D's capabilities using two generative approaches: GPT-4o with in-context learning (ICL) and a fine-tuned LLAMA-3 model. Experimental results show that Proc3D outperforms existing methods in editing efficiency, achieving more than 400x speedup over conventional approaches that require full regeneration for each modification. Additionally, Proc3D improves ULIP scores by 28%, a metric that evaluates the alignment between generated 3D models and text prompts. By enabling text-aligned 3D model generation along with precise, real-time parametric edits, Proc3D facilitates highly accurate text-based image editing applications.",
      "tldr_zh": "æœ¬æ–‡æå‡ºäº† Proc3Dï¼Œä¸€ä¸ªèƒ½å¤Ÿç”Ÿæˆå¯ç¼–è¾‘ 3D æ¨¡å‹å¹¶æ”¯æŒå®æ—¶ä¿®æ”¹çš„ç³»ç»Ÿï¼Œæ—¨åœ¨è§£å†³ç°æœ‰ç”Ÿæˆå¼ AI äº§ç”Ÿçš„æ¨¡å‹ï¼ˆå¦‚ meshes æˆ– point cloudsï¼‰å› ä¸å¯ç¼–è¾‘è€Œéš¾ä»¥è¿›è¡Œè¿­ä»£è®¾è®¡çš„é—®é¢˜ã€‚è¯¥ç³»ç»Ÿçš„æ ¸å¿ƒæ˜¯å¼•å…¥äº†ç¨‹åºåŒ–ç´§å‡‘å›¾ (procedural compact graph, PCG)ï¼Œè¿™ç§å›¾è¡¨ç¤ºæ³•ç¼–ç äº†ç”Ÿæˆæ¨¡å‹æ‰€éœ€çš„ç®—æ³•è§„åˆ™å’Œç»“æ„ã€‚é€šè¿‡ PCGï¼Œç”¨æˆ·å¯ä»¥åˆ©ç”¨æ»‘å—ã€å¤é€‰æ¡†æˆ–é€šè¿‡å¤§è¯­è¨€æ¨¡å‹ (LLMs) çš„è‡ªç„¶è¯­è¨€æç¤ºè¿›è¡Œç›´è§‚çš„å‚æ•°åŒ–ç¼–è¾‘ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒProc3D åœ¨ç¼–è¾‘æ•ˆç‡ä¸Šæ¯”éœ€è¦å…¨é‡é‡æ–°ç”Ÿæˆçš„ä¼ ç»Ÿæ–¹æ³•å¿« 400 å€ä»¥ä¸Šï¼Œå¹¶å°†è¡¡é‡ç”Ÿæˆæ¨¡å‹ä¸æ–‡æœ¬æç¤ºå¯¹é½åº¦çš„ ULIP åˆ†æ•°æå‡äº† 28%ã€‚è¯¥ç³»ç»Ÿä¸ºå®ç°é«˜ç²¾åº¦çš„æ–‡æœ¬é©±åŠ¨ 3D å½¢çŠ¶ç”Ÿæˆä¸å®æ—¶ç¼–è¾‘æä¾›äº†æœ‰æ•ˆæ–¹æ¡ˆã€‚",
      "categories": [
        "cs.GR",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.GR",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2601.12234v1",
      "published_date": "2026-01-18 03:08:08 UTC",
      "updated_date": "2026-01-18 03:08:08 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2026-01-22T07:11:32.259641+00:00"
    },
    {
      "arxiv_id": "2601.12224v1",
      "title": "Where It Moves, It Matters: Referring Surgical Instrument Segmentation via Motion",
      "title_zh": "åŠ¨ä¹‹æ‰€åœ¨ï¼Œé‡ä¹‹æ‰€åœ¨ï¼šåŸºäºè¿åŠ¨çš„æ‰‹æœ¯å™¨æ¢°æŒ‡ä»£åˆ†å‰²",
      "authors": [
        "Meng Wei",
        "Kun Yuan",
        "Shi Li",
        "Yue Zhou",
        "Long Bai",
        "Nassir Navab",
        "Hongliang Ren",
        "Hong Joo Lee",
        "Tom Vercauteren",
        "Nicolas Padoy"
      ],
      "abstract": "Enabling intuitive, language-driven interaction with surgical scenes is a critical step toward intelligent operating rooms and autonomous surgical robotic assistance. However, the task of referring segmentation, localizing surgical instruments based on natural language descriptions, remains underexplored in surgical videos, with existing approaches struggling to generalize due to reliance on static visual cues and predefined instrument names. In this work, we introduce SurgRef, a novel motion-guided framework that grounds free-form language expressions in instrument motion, capturing how tools move and interact across time, rather than what they look like. This allows models to understand and segment instruments even under occlusion, ambiguity, or unfamiliar terminology. To train and evaluate SurgRef, we present Ref-IMotion, a diverse, multi-institutional video dataset with dense spatiotemporal masks and rich motion-centric expressions. SurgRef achieves state-of-the-art accuracy and generalization across surgical procedures, setting a new benchmark for robust, language-driven surgical video segmentation.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† SurgRefï¼Œä¸€ä¸ªæ–°é¢–çš„è¿åŠ¨å¼•å¯¼æ¡†æ¶ï¼Œæ—¨åœ¨é€šè¿‡è‡ªç„¶è¯­è¨€æè¿°åœ¨æ‰‹æœ¯è§†é¢‘ä¸­å®ç°æ‰‹æœ¯å™¨æ¢°çš„æŒ‡ä»£åˆ†å‰² (referring segmentation)ã€‚ä¸åŒäºä»¥å¾€ä¾èµ–é™æ€è§†è§‰ç‰¹å¾å’Œé¢„å®šä¹‰åç§°çš„æ–¹æ³•ï¼ŒSurgRef ä¾§é‡äºæ•æ‰æ‰‹æœ¯å™¨æ¢°éšæ—¶é—´å˜åŒ–çš„è¿åŠ¨å’Œäº¤äº’æ¨¡å¼ï¼Œä½¿å…¶åœ¨é¢ä¸´é®æŒ¡ã€æ­§ä¹‰æˆ–é™Œç”Ÿæœ¯è¯­æ—¶ä»èƒ½å‡†ç¡®è¯†åˆ«ç›®æ ‡ã€‚ä¸ºäº†æ”¯æŒè¯¥ç ”ç©¶ï¼Œä½œè€…æ¨å‡ºäº† Ref-IMotion æ•°æ®é›†ï¼ŒåŒ…å«å¤šæœºæ„é‡‡é›†çš„è§†é¢‘ã€ç¨ å¯†çš„æ—¶åºç©ºé—´æ©ç ä»¥åŠä¸°å¯Œçš„è¿åŠ¨æè¿°ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒSurgRef åœ¨å¤šç§æ‰‹æœ¯ç¨‹åºä¸­å‡è¾¾åˆ°äº† SOTA çš„å‡†ç¡®ç‡å’Œæ³›åŒ–æ€§èƒ½ï¼Œä¸ºå®ç°è¯­è¨€é©±åŠ¨çš„æ™ºèƒ½æ‰‹æœ¯å®¤å’Œè‡ªä¸»æ‰‹æœ¯æœºå™¨äººè¾…åŠ©å¥ å®šäº†åŸºç¡€ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2601.12224v1",
      "published_date": "2026-01-18 02:14:08 UTC",
      "updated_date": "2026-01-18 02:14:08 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2026-01-22T07:11:51.356332+00:00"
    },
    {
      "arxiv_id": "2601.12215v1",
      "title": "Wavelet-Driven Masked Multiscale Reconstruction for PPG Foundation Models",
      "title_zh": "é¢å‘ PPG åŸºç¡€æ¨¡å‹çš„å°æ³¢é©±åŠ¨æ©ç å¤šå°ºåº¦é‡å»º",
      "authors": [
        "Megha Thukral",
        "Cyrus Tanade",
        "Simon A. Lee",
        "Juhyeon Lee",
        "Hao Zhou",
        "Keum San Chun",
        "Migyeong Gwak",
        "Viswam Nathan",
        "Md Mahbubur Rahman",
        "Li Zhu",
        "Mehrab Bin Morshed",
        "Subramaniam Venkatraman",
        "Sharanya Arcot Desai"
      ],
      "abstract": "Wearable foundation models have the potential to transform digital health by learning transferable representations from large-scale biosignals collected in everyday settings. While recent progress has been made in large-scale pretraining, most approaches overlook the spectral structure of photoplethysmography (PPG) signals, wherein physiological rhythms unfold across multiple frequency bands. Motivated by the insight that many downstream health-related tasks depend on multi-resolution features spanning fine-grained waveform morphology to global rhythmic dynamics, we introduce Masked Multiscale Reconstruction (MMR) for PPG representation learning - a self-supervised pretraining framework that explicitly learns from hierarchical time-frequency scales of PPG data. The pretraining task is designed to reconstruct randomly masked out coefficients obtained from a wavelet-based multiresolution decomposition of PPG signals, forcing the transformer encoder to integrate information across temporal and spectral scales. We pretrain our model with MMR using ~17 million unlabeled 10-second PPG segments from ~32,000 smartwatch users. On 17 of 19 diverse health-related tasks, MMR trained on large-scale wearable PPG data improves over or matches state-of-the-art open-source PPG foundation models, time-series foundation models, and other self-supervised baselines. Extensive analysis of our learned embeddings and systematic ablations underscores the value of wavelet-based representations, showing that they capture robust and physiologically-grounded features. Together, these results highlight the potential of MMR as a step toward generalizable PPG foundation models.",
      "tldr_zh": "æœ¬æ–‡æå‡ºäº†é’ˆå¯¹å…‰ç”µå®¹ç§¯è„‰ææ³¢ (PPG) åŸºç¡€æ¨¡å‹çš„æ©ç å¤šå°ºåº¦é‡å»º (Masked Multiscale Reconstruction, MMR) æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³ç°æœ‰æ¨¡å‹å¿½è§† PPG ä¿¡å·é¢‘è°±ç»“æ„çš„é—®é¢˜ã€‚è¯¥æ–¹æ³•é€šè¿‡åŸºäºå°æ³¢ (wavelet-based) çš„å¤šåˆ†è¾¨ç‡åˆ†è§£å°†ä¿¡å·è½¬åŒ–ä¸ºåˆ†å±‚æ—¶é¢‘å°ºåº¦ï¼Œå¹¶è¦æ±‚ transformer ç¼–ç å™¨é‡å»ºéšæœºæ©ç çš„ç³»æ•°ï¼Œä»è€Œå­¦ä¹ ä»ç»†å¾®æ³¢å½¢å½¢æ€åˆ°å…¨å±€èŠ‚å¾‹åŠ¨æ€çš„ç‰¹å¾ã€‚ç ”ç©¶å›¢é˜Ÿåˆ©ç”¨æ¥è‡ªçº¦ 3.2 ä¸‡åç”¨æˆ·çš„ 1700 ä¸‡æ¡æ— æ ‡ç­¾ PPG ç‰‡æ®µè¿›è¡Œé¢„è®­ç»ƒã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒMMR åœ¨ 19 é¡¹å¥åº·ç›¸å…³ä»»åŠ¡ä¸­çš„ 17 é¡¹ä¸Šè¾¾åˆ°æˆ–ä¼˜äºç°æœ‰çš„ PPG åŠæ—¶é—´åºåˆ—åŸºç¡€æ¨¡å‹ã€‚è¿™ä¸€ç ”ç©¶è¯æ˜äº†å°æ³¢é©±åŠ¨çš„å¤šå°ºåº¦è¡¨ç¤ºåœ¨æ•æ‰ç¨³å¥ä¸”å…·æœ‰ç”Ÿç†ä¾æ®çš„ç‰¹å¾æ–¹é¢çš„æœ‰æ•ˆæ€§ï¼Œä¸ºæ„å»ºé€šç”¨å‹ PPG åŸºç¡€æ¨¡å‹æä¾›äº†æ–°æ€è·¯ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2601.12215v1",
      "published_date": "2026-01-18 01:34:47 UTC",
      "updated_date": "2026-01-18 01:34:47 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2026-01-22T07:12:01.357888+00:00"
    },
    {
      "arxiv_id": "2601.12212v1",
      "title": "Speculative Sampling with Reinforcement Learning",
      "title_zh": "åŸºäºå¼ºåŒ–å­¦ä¹ çš„æŠ•æœºé‡‡æ ·",
      "authors": [
        "Chenan Wang",
        "Daniel H. Shi",
        "Haipeng Chen"
      ],
      "abstract": "Inference time latency has remained an open challenge for real world applications of large language models (LLMs). State-of-the-art (SOTA) speculative sampling (SpS) methods for LLMs, like EAGLE-3, use tree-based drafting to explore multiple candidate continuations in parallel. However, the hyperparameters controlling the tree structure are static, which limits flexibility and efficiency across diverse contexts and domains. We introduce Reinforcement learning for Speculative Sampling (Re-SpS), the first reinforcement learning (RL)-based framework for draft tree hyperparameter optimization. Re-SpS dynamically adjusts draft tree hyperparameters in real-time, learning context-aware policies that maximize generation speed by balancing speculative aggression with computational overhead. It leverages efficient state representations from target model hidden states and introduces multi-step action persistence for better context modeling. Evaluation results across five diverse benchmarks demonstrate consistent improvements over the SOTA method EAGLE-3, achieving up to 5.45$\\times$ speedup over the backbone LLM and up to 1.12$\\times$ speedup compared to EAGLE-3 across five diverse benchmarks, with no loss in output fidelity.",
      "tldr_zh": "æœ¬æ–‡æå‡ºäº† **Re-SpS (Reinforcement learning for Speculative Sampling)**ï¼Œè¿™æ˜¯é¦–ä¸ªåˆ©ç”¨å¼ºåŒ–å­¦ä¹  (RL) ä¼˜åŒ–æ¨æµ‹é‡‡æ · (Speculative Sampling) è¶…å‚æ•°çš„æ¡†æ¶ã€‚é’ˆå¯¹ç°æœ‰æ–¹æ³•ï¼ˆå¦‚ **EAGLE-3**ï¼‰ä¸­è‰ç¨¿æ ‘ (draft tree) ç»“æ„å‚æ•°é™æ€ä¸”ç¼ºä¹çµæ´»æ€§çš„é—®é¢˜ï¼Œ**Re-SpS** èƒ½å¤Ÿæ ¹æ®ä¸åŒä¸Šä¸‹æ–‡å®æ—¶åŠ¨æ€è°ƒæ•´è¶…å‚æ•°ï¼Œä»è€Œåœ¨æ¨æµ‹æ¿€è¿›ç¨‹åº¦ä¸è®¡ç®—å¼€é”€ä¹‹é—´å–å¾—æœ€ä½³å¹³è¡¡ã€‚è¯¥æ¡†æ¶åˆ©ç”¨ç›®æ ‡æ¨¡å‹çš„éšè—çŠ¶æ€è¿›è¡Œé«˜æ•ˆçš„çŠ¶æ€è¡¨ç¤ºï¼Œå¹¶å¼•å…¥äº†å¤šæ­¥åŠ¨ä½œæŒä¹…åŒ– (**multi-step action persistence**) æ¥å¢å¼ºä¸Šä¸‹æ–‡å»ºæ¨¡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œ**Re-SpS** åœ¨ä¸æŸå®³è¾“å‡ºä¿çœŸåº¦çš„å‰æä¸‹ï¼Œæ¯”åŸºç¡€ **LLM** æé€Ÿé«˜è¾¾ 5.45 å€ï¼Œä¸”ç›¸æ¯” **SOTA** æ–¹æ³• **EAGLE-3** è¿›ä¸€æ­¥å®ç°äº† 1.12 å€çš„æ¨ç†åŠ é€Ÿã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "Accepted to AAAI 2026",
      "pdf_url": "https://arxiv.org/pdf/2601.12212v1",
      "published_date": "2026-01-18 01:31:29 UTC",
      "updated_date": "2026-01-18 01:31:29 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2026-01-22T07:12:07.326740+00:00"
    },
    {
      "arxiv_id": "2601.12205v1",
      "title": "Do Neural Codecs Generalize? A Controlled Study Across Unseen Languages and Non-Speech Tasks",
      "title_zh": "ç¥ç»ç¼–è§£ç å™¨å…·å¤‡æ³›åŒ–æ€§å—ï¼Ÿä¸€é¡¹é’ˆå¯¹æœªè§è¯­è¨€ä¸éè¯­éŸ³ä»»åŠ¡çš„å—æ§ç ”ç©¶",
      "authors": [
        "Shih-Heng Wang",
        "Jiatong Shi",
        "Jinchuan Tian",
        "Haibin Wu",
        "Shinji Watanabe"
      ],
      "abstract": "This paper investigates three crucial yet underexplored aspects of the generalization capabilities of neural audio codecs (NACs): (i) whether NACs can generalize to unseen languages during pre-training, (ii) whether speech-only pre-trained NACs can effectively generalize to non-speech applications such as environmental sounds, music, and animal vocalizations, and (iii) whether incorporating non-speech data during pre-training can improve performance on both speech and non-speech tasks. Existing studies typically rely on off-the-shelf NACs for comparison, which limits insight due to variations in implementation. In this work, we train NACs from scratch using strictly controlled configurations and carefully curated pre-training data to enable fair comparisons. We conduct a comprehensive evaluation of NAC performance on both signal reconstruction quality and downstream applications using 11 metrics. Our results show that NACs can generalize to unseen languages during pre-training, speech-only pre-trained NACs exhibit degraded performance on non-speech tasks, and incorporating non-speech data during pre-training improves performance on non-speech tasks while maintaining comparable performance on speech tasks.",
      "tldr_zh": "æœ¬ç ”ç©¶æ¢è®¨äº†ç¥ç»éŸ³é¢‘ç¼–è§£ç å™¨ (neural audio codecs, NACs) åœ¨æœªçŸ¥è¯­è¨€åŠéè¯­éŸ³ä»»åŠ¡ä¸­çš„æ³›åŒ–èƒ½åŠ›ã€‚ä½œè€…é€šè¿‡ä¸¥æ ¼å—æ§çš„é…ç½®ä»é›¶å¼€å§‹è®­ç»ƒ NACsï¼Œå¹¶åˆ©ç”¨ 11 é¡¹æŒ‡æ ‡å¯¹å…¶ä¿¡å·é‡å»º (signal reconstruction) è´¨é‡å’Œä¸‹æ¸¸åº”ç”¨è¡¨ç°è¿›è¡Œäº†å…¨é¢è¯„ä¼°ã€‚ç»“æœè¡¨æ˜ï¼ŒNACs èƒ½å¤Ÿæœ‰æ•ˆæ³›åŒ–è‡³é¢„è®­ç»ƒä¸­æœªè§è¿‡çš„è¯­è¨€ï¼Œä½†ä»…ä½¿ç”¨è¯­éŸ³æ•°æ®é¢„è®­ç»ƒçš„æ¨¡å‹åœ¨å¤„ç†éè¯­éŸ³ä»»åŠ¡æ—¶æ€§èƒ½ä¼šæ˜¾è‘—ä¸‹é™ã€‚ç ”ç©¶è¿›ä¸€æ­¥å‘ç°ï¼Œåœ¨é¢„è®­ç»ƒé˜¶æ®µå¼•å…¥éè¯­éŸ³æ•°æ®å¯ä»¥æå‡éè¯­éŸ³ä»»åŠ¡çš„æ€§èƒ½ï¼ŒåŒæ—¶åœ¨è¯­éŸ³ä»»åŠ¡ä¸Šä¿æŒç›¸å½“çš„ç«äº‰åŠ›ã€‚",
      "categories": [
        "cs.SD",
        "cs.AI",
        "eess.AS"
      ],
      "primary_category": "cs.SD",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2601.12205v1",
      "published_date": "2026-01-18 00:53:11 UTC",
      "updated_date": "2026-01-18 00:53:11 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2026-01-22T07:12:25.488398+00:00"
    }
  ],
  "processing_status": "completed",
  "error": null,
  "raw_papers_fetched": true,
  "papers_count": 74,
  "processed_papers_count": 74,
  "failed_papers_count": 0,
  "summary_generated": true,
  "daily_data_saved": true,
  "last_update": "2026-01-22T07:13:23.928341+00:00"
}
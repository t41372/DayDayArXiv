{
  "date": "2025-08-10",
  "category": "cs.AI",
  "summary": "æ¬¢è¿æ¥åˆ° UTC æ—¶é—´ 2025-08-10 çš„ arXiv ä¸­æ–‡ TLDR å¿«æŠ¥ï¼\n\nğŸ‘‹ **ä¸€å¥è¯æ€»ç»“ï¼š**\nä»Šå¤©çš„ arXiv å……æ»¡äº†å¯¹ç°æœ‰èŒƒå¼çš„åæ€ä¸çªç ´ã€‚æˆ‘ä»¬çœ‹åˆ°äº†å¯¹ DPOï¼ˆç›´æ¥åå¥½ä¼˜åŒ–ï¼‰ç†è®ºåŸºç¡€çš„å°–é”æŒ‘æˆ˜ï¼Œè¯æ˜â€œå¹»è§‰â€åœ¨è®¡ç®—ä¸Šä¸å¯é¿å…çš„ç†è®ºè¾¹ç•Œï¼Œä»¥åŠ Agent é¢†åŸŸä»å•çº¯çš„ RAG å‘å¤šæ™ºèƒ½ä½“åä½œï¼ˆMulti-Agentï¼‰å’Œè‡ªè¿›åŒ–ç³»ç»Ÿçš„æ¼”è¿›ã€‚\n\n---\n\n### ğŸš€ ç„¦ç‚¹è®ºæ–‡ï¼šç†è®ºæŒ‘æˆ˜ä¸å¯¹é½ (Alignment & Theory)\n\n**1. [LLM å¯¹é½æ–°è§†è§’] DPO çš„æŸå¤±å‡½æ•°å¯èƒ½æœ‰é—®é¢˜ï¼Ÿ**\n**æ ‡é¢˜ï¼š** A Principled Loss Function for Direct Language Model Alignment (ä¸€ä¸ªç”¨äºç›´æ¥è¯­è¨€æ¨¡å‹å¯¹é½çš„åŸåˆ™æ€§æŸå¤±å‡½æ•°)\n**Authors:** Yuandong Tan\n**å…³é”®è¯ï¼š** DPO, RLHF, Loss Function, Alignment\n**TLDR:** è¿™ç¯‡æ–‡ç« éå¸¸çŠ€åˆ©ã€‚ä½œè€…æŒ‡å‡ºç›®å‰å¤§ç«çš„ DPO (Direct Preference Optimization) çš„æŸå¤±å‡½æ•°ä¸å…¶æ¨å¯¼å‡ºçš„ç†è®ºåŸºç¡€æ˜¯**ä¸ä¸€è‡´**çš„ã€‚DPO å€¾å‘äºæ— é™æœ€å¤§åŒ– logits å·®å€¼ï¼Œè¿™ä¼šå¯¼è‡´è®­ç»ƒä¸ç¨³å®šå’Œâ€œå¥–åŠ±é»‘å®¢ (reward hacking)â€ç°è±¡ã€‚\n**åˆ›æ–°ç‚¹ï¼š** ä½œè€…ä» RLHF çš„æœ€ä¼˜æ€§æ¡ä»¶å‡ºå‘ï¼Œæ¨å¯¼å‡ºäº†ä¸€ä¸ªæ–°çš„æŸå¤±å‡½æ•°ã€‚è¿™ä¸ªæ–°å‡½æ•°ä¸å†è¿½æ±‚æœ€å¤§åŒ– logits å·®ï¼Œè€Œæ˜¯å°†å…¶å¯¹é½åˆ°ä¸€ä¸ªç”±æ½œåœ¨å¥–åŠ±å†³å®šçš„**æœ‰é™å€¼**ã€‚\n**ç»“è®ºï¼š** åœ¨ Qwen2.5-7B ä¸Šçš„å®éªŒæ˜¾ç¤ºï¼Œæ–°æ–¹æ³•æ¯”æ ‡å‡† DPO èƒœç‡æ›´é«˜ï¼Œä¸”è®­ç»ƒæ¢¯åº¦æ›´ç¨³å®šï¼Œé¿å…äº†æ¦‚ç‡è¶‹è¿‘äºé›¶æ—¶çš„æ¢¯åº¦çˆ†ç‚¸é—®é¢˜ã€‚è¿™å¯èƒ½é¢„ç¤ºç€å¯¹é½ç®—æ³•çš„ä¸€æ¬¡é‡è¦ä¿®æ­£ã€‚\n\n**2. [å¹»è§‰çš„ç†è®ºè¾¹ç•Œ] å¹»è§‰æ˜¯ä¸å¯é¿å…çš„ï¼Ÿ**\n**æ ‡é¢˜ï¼š** Hallucination as a Computational Boundary: A Hierarchy of Inevitability and the Oracle Escape (å¹»è§‰ä½œä¸ºè®¡ç®—è¾¹ç•Œï¼šå¿…ç„¶æ€§å±‚çº§ä¸ Oracle é€ƒé€¸)\n**Authors:** Wang Xi et al.\n**å…³é”®è¯ï¼š** Hallucination, Computability Theory, RAG\n**TLDR:** è¿™æ˜¯ä¸€ç¯‡ç¡¬æ ¸çš„ç†è®ºæ–‡ç« ã€‚ä½œè€…å°† LLM å½¢å¼åŒ–ä¸ºæ¦‚ç‡å›¾çµæœºï¼Œå¹¶åŸºäºå¯¹è§’åŒ–å’Œä¸å¯è®¡ç®—æ€§è¾¹ç•Œï¼Œè¯æ˜äº†**å¹»è§‰æ˜¯å¿…ç„¶å­˜åœ¨çš„**ã€‚\n**åˆ›æ–°ç‚¹ï¼š** æå‡ºäº†â€œè®¡ç®—å¿…è¦æ€§å±‚çº§â€ã€‚å¥½æ¶ˆæ¯æ˜¯ï¼Œä½œè€…ç»™å‡ºäº†ä¸¤æ¡â€œé€ƒé€¸è·¯å¾„â€ï¼šä¸€æ˜¯å°† RAGï¼ˆæ£€ç´¢å¢å¼ºç”Ÿæˆï¼‰å»ºæ¨¡ä¸º Oracle æœºå™¨ï¼Œä»ç†è®ºä¸Šè¯æ˜äº† RAG èƒ½å¤Ÿé€šè¿‡â€œè®¡ç®—è·³è·ƒâ€æ¥è§„é¿å¹»è§‰ï¼›äºŒæ˜¯å°†æŒç»­å­¦ä¹ å½¢å¼åŒ–ä¸ºâ€œå†…åŒ– Oracleâ€æœºåˆ¶ã€‚è¿™ä¸º RAG çš„æœ‰æ•ˆæ€§æä¾›äº†åšå®çš„ç†è®ºæ”¯æ’‘ã€‚\n\n**3. [Transformer æœºåˆ¶] ä¸¤å±‚ Transformer å°±èƒ½æå®šå½’çº³å¤´**\n**æ ‡é¢˜ï¼š** What One Cannot, Two Can: Two-Layer Transformers Provably Represent Induction Heads on Any-Order Markov Chains (ä¸€å±‚ä¸è¡Œä¸¤å±‚è¡Œï¼šä¸¤å±‚ Transformer å¯è¯æ˜åœ°åœ¨ä»»æ„é˜¶é©¬å°”å¯å¤«é“¾ä¸Šè¡¨ç¤ºå½’çº³å¤´)\n**Authors:** Chanakya Ekbote et al.\n**å…³é”®è¯ï¼š** In-context Learning, Induction Heads, Transformer Depth\n**TLDR:** ä¹‹å‰çš„ç ”ç©¶è®¤ä¸ºè¦è¡¨ç¤ºé«˜é˜¶é©¬å°”å¯å¤«æºï¼ˆk-gramsï¼‰è‡³å°‘éœ€è¦ä¸‰å±‚ Transformerã€‚æœ¬æ–‡æ‰“ç ´äº†è¿™ä¸€è®¤çŸ¥ï¼Œä»ç†è®ºä¸Šè¯æ˜äº†**ä¸¤å±‚ï¼ˆæ¯å±‚ä¸€ä¸ªå¤´ï¼‰çš„ Transformer** å°±è¶³ä»¥è¡¨ç¤ºä»»ä½•æ¡ä»¶çš„ k-gramã€‚è¿™åŠ æ·±äº†æˆ‘ä»¬å¯¹ Transformer æ·±åº¦ä¸ä¸Šä¸‹æ–‡å­¦ä¹ ï¼ˆICLï¼‰èƒ½åŠ›ä¹‹é—´å…³ç³»çš„ç†è§£ï¼Œè§£é‡Šäº†ä¸ºä½•æµ…å±‚æ¶æ„ä¹Ÿèƒ½å±•ç°å‡ºå¼ºå¤§çš„ ICL èƒ½åŠ›ã€‚\n\n---\n\n### ğŸ¤– Agent æ™ºèƒ½ä½“ä¸å¤æ‚æ¨ç†\n\n**4. [è‡ªè¿›åŒ– Agent] è¿ˆå‘ç»ˆèº«å­¦ä¹ **\n**æ ‡é¢˜ï¼š** A Comprehensive Survey of Self-Evolving AI Agents: A New Paradigm Bridging Foundation Models and Lifelong Agentic Systems (è‡ªè¿›åŒ– AI æ™ºèƒ½ä½“ç»¼è¿°ï¼šè¿æ¥åŸºç¡€æ¨¡å‹ä¸ç»ˆèº«æ™ºèƒ½ä½“ç³»ç»Ÿçš„æ–°èŒƒå¼)\n**Authors:** Jinyuan Fang et al.\n**å…³é”®è¯ï¼š** Self-Evolving Agents, Survey, Lifelong Learning\n**TLDR:** ç°æœ‰çš„ Agent å¤§å¤šé…ç½®é™æ€ï¼Œæ— æ³•é€‚åº”ç¯å¢ƒã€‚è¿™ç¯‡ç»¼è¿°ç³»ç»Ÿåœ°æ¢³ç†äº†â€œè‡ªè¿›åŒ–æ™ºèƒ½ä½“â€ï¼Œå³èƒ½å¤Ÿæ ¹æ®äº¤äº’æ•°æ®å’Œç¯å¢ƒåé¦ˆè‡ªåŠ¨å¢å¼ºè‡ªèº«çš„ç³»ç»Ÿã€‚æ–‡ç« æå‡ºäº†ä¸€ä¸ªåŒ…å«è¾“å…¥ã€ç³»ç»Ÿã€ç¯å¢ƒå’Œä¼˜åŒ–å™¨çš„ç»Ÿä¸€æ¡†æ¶ï¼Œæ˜¯äº†è§£ Agent ä¸‹ä¸€æ­¥å‘å±•æ–¹å‘ï¼ˆä»é™æ€åˆ°åŠ¨æ€è¿›åŒ–ï¼‰çš„ç»ä½³ææ–™ã€‚\n\n**5. [åŒ»ç–—æ¨ç†] å¤šæ™ºèƒ½ä½“åä½œåŠæ‰“ RAG**\n**æ ‡é¢˜ï¼š** A Multi-Agent Approach to Neurological Clinical Reasoning (ç¥ç»ä¸´åºŠæ¨ç†çš„å¤šæ™ºèƒ½ä½“æ–¹æ³•)\n**Authors:** Moran Sorka et al.\n**å…³é”®è¯ï¼š** Medical AI, Multi-Agent, RAG, Neurology\n**TLDR:** åœ¨å¤æ‚çš„ç¥ç»å­¦ä¸´åºŠæ¨ç†ä¸­ï¼Œå•çº¯çš„ RAG æ•ˆæœæœ‰é™ã€‚ä½œè€…æ„å»ºäº†ä¸€ä¸ª**å¤šæ™ºèƒ½ä½“ç³»ç»Ÿ**ï¼Œå°†ä»»åŠ¡åˆ†è§£ä¸ºé—®é¢˜åˆ†æã€çŸ¥è¯†æ£€ç´¢ã€ç­”æ¡ˆåˆæˆå’ŒéªŒè¯ç­‰ä¸“é—¨çš„è®¤çŸ¥åŠŸèƒ½ã€‚\n**ç»“è®ºï¼š** å®éªŒæ˜¾ç¤ºï¼ŒåŸºäº LLaMA 3.3-70B çš„å¤šæ™ºèƒ½ä½“ç³»ç»Ÿå‡†ç¡®ç‡è¾¾åˆ° 89.2%ï¼Œè¿œè¶…å…¶åŸºç¡€æ¨¡å‹ï¼ˆ69.5%ï¼‰å’Œæ™®é€šçš„ RAG æ–¹æ³•ï¼Œç”šè‡³åœ¨æŸäº›æ–¹é¢é€¼è¿‘ OpenAI-o1ã€‚è¿™å†æ¬¡å°è¯äº†åœ¨å¤æ‚ä¸“ä¸šé¢†åŸŸï¼Œ\"Workflow > Model\"ã€‚\n\n**6. [ç¨åŠ¡æ¼æ´] LLM èƒ½å¸®ä½ é¿ç¨å—ï¼Ÿ**\n**æ ‡é¢˜ï¼š** Can LLMs Identify Tax Abuse? (LLM èƒ½è¯†åˆ«ç¨åŠ¡æ»¥ç”¨å—ï¼Ÿ)\n**Authors:** Andrew Blair-Stanek et al.\n**å…³é”®è¯ï¼š** Tax Law, Reasoning, AI Safety\n**TLDR:** è¿™æ˜¯ä¸€ä¸ªéå¸¸æœ‰è¶£çš„ç°å®ä¸–ç•Œæµ‹è¯•ã€‚ä½œè€…è¯„ä¼°äº† LLM æ˜¯å¦èƒ½å‘ç°å’Œåˆ†æç¾å›½ç¨æ³•ä¸­çš„é¿ç¨ç­–ç•¥ã€‚ç»“æœä»¤äººéœ‡æƒŠï¼šLLM çš„æ¨ç†èƒ½åŠ›ä¸ä»…èƒ½ç†è§£å¤æ‚çš„ç¨æ³•ï¼Œç”šè‡³è¯†åˆ«å‡ºäº†ä¸€ç§å…¨æ–°çš„ã€äººç±»ä¸“å®¶æœªæ›¾æ˜ç¡®çš„ç¨åŠ¡ç­–ç•¥ã€‚è¿™å¯¹ç¨åŠ¡æœºå…³åé¿ç¨æä¾›äº†æ–°å·¥å…·ï¼Œä¹Ÿå¸¦æ¥äº†æ½œåœ¨çš„ç›‘ç®¡æŒ‘æˆ˜ã€‚\n\n---\n\n### ğŸ‘ï¸ å¤šæ¨¡æ€ä¸å‚ç±»åº”ç”¨\n\n**7. [åŒ»å­¦åŸºç¡€æ¨¡å‹] MRI çš„å¤§è§„æ¨¡é¢„è®­ç»ƒ**\n**æ ‡é¢˜ï¼š** Large-scale Multi-sequence Pretraining for Generalizable MRI Analysis in Versatile Clinical Applications (é¢å‘é€šç”¨ä¸´åºŠåº”ç”¨çš„å¤§è§„æ¨¡å¤šåºåˆ— MRI åˆ†æé¢„è®­ç»ƒ)\n**Authors:** Zelin Qiu et al.\n**å…³é”®è¯ï¼š** Medical Imaging, MRI, Foundation Model, PRISM\n**TLDR:** é’ˆå¯¹ MRI åºåˆ—å¼‚è´¨æ€§å¼ºçš„é—®é¢˜ï¼Œä½œè€…æ¨å‡ºäº† **PRISM**ã€‚è¿™æ˜¯ç›®å‰æœ€å¤§çš„å¤šå™¨å®˜ã€å¤šåºåˆ— MRI é¢„è®­ç»ƒè¯­æ–™åº“ï¼ˆ33ä¸‡+æ‰«æï¼‰ã€‚PRISM è§£è€¦äº†è§£å‰–å­¦ç‰¹å¾å’Œåºåˆ—ç‰¹å¼‚æ€§å˜åŒ–ï¼Œåœ¨ 44 ä¸ªä¸‹æ¸¸ä»»åŠ¡ä¸­æ‹¿ä¸‹äº† 39 ä¸ªç¬¬ä¸€ã€‚åŒ»å­¦å½±åƒé¢†åŸŸçš„â€œImageNetæ—¶åˆ»â€æ­£åœ¨åŠ é€Ÿåˆ°æ¥ã€‚\n\n**8. [å†œä¸šæœºå™¨äºº] å¬æ‡‚æŒ‡ä»¤çš„å†œåœºå¸®æ‰‹**\n**æ ‡é¢˜ï¼š** AgriVLN: Vision-and-Language Navigation for Agricultural Robots (AgriVLNï¼šå†œä¸šæœºå™¨äººçš„è§†è§‰è¯­è¨€å¯¼èˆª)\n**Authors:** Xiaobei Zhao et al.\n**å…³é”®è¯ï¼š** Agricultural Robotics, VLN, Benchmark\n**TLDR:** ç°æœ‰çš„è§†è§‰è¯­è¨€å¯¼èˆªï¼ˆVLNï¼‰ç¼ºä¹å†œä¸šåœºæ™¯ã€‚æœ¬æ–‡å‘å¸ƒäº† **AgriVLN** å’Œ A2A åŸºå‡†æµ‹è¯•ï¼Œè®©å››è¶³æœºå™¨äººèƒ½å¬æ‡‚â€œå»ç»™è‹¹æœæ ‘æµ‡æ°´â€è¿™æ ·çš„æŒ‡ä»¤ã€‚é’ˆå¯¹é•¿æŒ‡ä»¤ç†è§£éš¾çš„é—®é¢˜ï¼Œè¿˜å¼•å…¥äº†å­ä»»åŠ¡åˆ†è§£æ¨¡å—ï¼Œæ˜¾è‘—æé«˜äº†å¯¼èˆªæˆåŠŸç‡ã€‚\n\n---\n\n### ğŸ›¡ï¸ å®‰å…¨ã€è¯„ä¼°ä¸ç¤¾ä¼šç§‘å­¦\n\n**9. [é“å¾·å›°å¢ƒ] AI çš„ç”µè½¦éš¾é¢˜**\n**æ ‡é¢˜ï¼š** \"Pull or Not to Pull?'': Investigating Moral Biases in Leading Large Language Models Across Ethical Dilemmas (\"æ‹‰è¿˜æ˜¯ä¸æ‹‰ï¼Ÿ\"ï¼šä¸»è¦å¤§è¯­è¨€æ¨¡å‹åœ¨ä¼¦ç†å›°å¢ƒä¸­çš„é“å¾·åè§è°ƒæŸ¥)\n**Authors:** Junchen Ding et al.\n**å…³é”®è¯ï¼š** Moral Reasoning, Ethics, Bias\n**TLDR:** æµ‹è¯•äº† 14 ä¸ª LLM åœ¨ 27 ç§ç”µè½¦éš¾é¢˜å˜ä½“ä¸‹çš„è¡¨ç°ã€‚å‘ç°å…·å¤‡â€œæ¨ç†èƒ½åŠ›â€çš„æ¨¡å‹ï¼ˆå¦‚ o1 ç±»ï¼‰å†³ç­–æ›´æœæ–­ï¼Œä½†å¹¶ä¸æ€»æ˜¯ç¬¦åˆäººç±»å…±è¯†ã€‚åœ¨æ¶‰åŠäº²å±å…³ç³»æˆ–è‡ªèº«åˆ©ç›Šæ—¶ï¼Œæ¨¡å‹å®¹æ˜“äº§ç”Ÿæœ‰äº‰è®®çš„ç»“æœã€‚ä½œè€…å‘¼åå°†â€œé“å¾·æ¨ç†â€ä½œä¸ºå¯¹é½çš„é‡è¦ç»´åº¦ã€‚\n\n**10. [ç¤¾ä¼šå†³å®šå› ç´ ] ç®—æ³•å…¬å¹³æ€§çš„æ–°ç»´åº¦**\n**æ ‡é¢˜ï¼š** Algorithmic Fairness amid Social Determinants: Reflection, Characterization, and Approach (ç¤¾ä¼šå†³å®šå› ç´ èƒŒæ™¯ä¸‹çš„ç®—æ³•å…¬å¹³æ€§)\n**Authors:** Zeyu Tang et al.\n**å…³é”®è¯ï¼š** Algorithmic Fairness, Social Determinants, Causal Influence\n**TLDR:** ä¼ ç»Ÿçš„ç®—æ³•å…¬å¹³æ€§åªå…³æ³¨â€œæ•æ„Ÿå±æ€§â€ï¼ˆå¦‚ç§æ—ã€æ€§åˆ«ï¼‰ã€‚æœ¬æ–‡æŒ‡å‡ºè¿™è¿˜ä¸å¤Ÿï¼Œå¿…é¡»è€ƒè™‘**ç¤¾ä¼šå†³å®šå› ç´ **ï¼ˆå¦‚åœ°åŒºã€ç¯å¢ƒä¸Šä¸‹æ–‡ï¼‰ã€‚ä½œè€…é€šè¿‡å¤§å­¦å½•å–çš„æ¡ˆä¾‹è¯æ˜ï¼Œä»…å…³æ³¨æ•æ„Ÿå±æ€§å¯èƒ½ä¼šå¼•å…¥æ–°çš„ç»“æ„æ€§ä¸å…¬ã€‚\n\n**11. [ä»£ç è¯„ä¼°] é˜²æ­¢æµ‹è¯•é›†æ±¡æŸ“çš„æ–°åŸºå‡†**\n**æ ‡é¢˜ï¼š** Dynamic Benchmark Construction for Evaluating Large Language Models on Real-World Codes (ç”¨äºåœ¨çœŸå®ä»£ç ä¸Šè¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹çš„åŠ¨æ€åŸºå‡†æ„å»º)\n**Authors:** Zhe Zhang et al.\n**å…³é”®è¯ï¼š** Code Generation, Benchmark, Data Contamination\n**TLDR:** é’ˆå¯¹ä»£ç ç”ŸæˆåŸºå‡†ï¼ˆå¦‚ HumanEvalï¼‰å¯èƒ½å·²è¢« LLM è®­ç»ƒæ•°æ®æ±¡æŸ“çš„é—®é¢˜ï¼Œæå‡ºäº† **CODE2BENCH**ã€‚è¿™æ˜¯ä¸€ä¸ªåŠ¨æ€æµæ°´çº¿ï¼Œåˆ©ç”¨ GitHub ä¸Š**æœ€æ–°**çš„ä»£ç åº“è‡ªåŠ¨æ„å»ºæµ‹è¯•é¢˜ï¼Œç¡®ä¿æ¨¡å‹æ²¡â€œè§è¿‡â€è¿™äº›é¢˜ç›®ã€‚\n\n---\n\n### ğŸ—ï¸ å…¶ä»–å€¼å¾—å…³æ³¨çš„è®ºæ–‡ (Quick Looks)\n\n*   **[æ—¶é—´åºåˆ—] Adapting LLMs to Time Series Forecasting via Temporal Heterogeneity Modeling and Semantic Alignment:** æå‡ºäº† TALON æ¡†æ¶ï¼Œé€šè¿‡è¯­ä¹‰å¯¹é½å’Œå¼‚è´¨æ€§å»ºæ¨¡ï¼Œè®© LLM æ›´å¥½åœ°å¤„ç†è¿ç»­æ•°å€¼ä¿¡å·ï¼ŒMSE é™ä½äº† 11%ã€‚\n*   **[åŸå¸‚è§„åˆ’] VA-Blueprint: Uncovering Building Blocks for Visual Analytics System Design:** åŸå¸‚è§†è§‰åˆ†æç³»ç»Ÿçš„è®¾è®¡è“å›¾ï¼Œåˆ©ç”¨ LLM ä» 100 å¤šç¯‡è®ºæ–‡ä¸­æå–è®¾è®¡ç»„ä»¶ï¼Œæ„å»ºçŸ¥è¯†åº“ã€‚\n*   **[æ¨èç³»ç»Ÿ] SocRipple: A Two-Stage Framework for Cold-Start Video Recommendations:** é’ˆå¯¹è§†é¢‘å†·å¯åŠ¨é—®é¢˜ï¼Œåˆ©ç”¨åˆ›ä½œè€…çš„ç¤¾äº¤å…³ç³»è¿›è¡Œåˆæ­¥åˆ†å‘ï¼Œå†é€šè¿‡ Ripple æ‰©æ•£ï¼Œæå‡äº† 36% çš„åˆ†å‘æ•ˆç‡ã€‚\n*   **[ç§‘ç ”è´¨é‡] Can Smaller Large Language Models Evaluate Research Quality?:** å³ä½¿æ˜¯è¾ƒå°çš„æ¨¡å‹ï¼ˆå¦‚ Gemma-3-27bï¼‰ä¹Ÿèƒ½æœ‰æ•ˆåœ°è¯„ä¼°è®ºæ–‡è´¨é‡ï¼Œä¸ä¸“å®¶è¯„åˆ†æ­£ç›¸å…³ï¼Œé€‚åˆä½èµ„æºç¯å¢ƒä¸‹çš„ç§‘ç ”è¯„ä¼°ã€‚\n\nå¸Œæœ›ä»Šå¤©çš„å¿«æŠ¥å¯¹ä½ çš„ç ”ç©¶æœ‰æ‰€å¯å‘ï¼æˆ‘ä»¬æ˜å¤©è§ã€‚",
  "papers": [
    {
      "arxiv_id": "2508.08337v1",
      "title": "Algorithmic Fairness amid Social Determinants: Reflection, Characterization, and Approach",
      "title_zh": "ç¤¾ä¼šå†³å®šå› ç´ èƒŒæ™¯ä¸‹çš„ç®—æ³•å…¬å¹³æ€§ï¼šåæ€ã€åˆ»ç”»ä¸æ–¹æ³•",
      "authors": [
        "Zeyu Tang",
        "Alex John London",
        "Atoosa Kasirzadeh",
        "Sanmi Koyejo",
        "Peter Spirtes",
        "Kun Zhang"
      ],
      "abstract": "Social determinants are variables that, while not directly pertaining to any specific individual, capture key aspects of contexts and environments that have direct causal influences on certain attributes of an individual. Previous algorithmic fairness literature has primarily focused on sensitive attributes, often overlooking the role of social determinants. Our paper addresses this gap by introducing formal and quantitative rigor into a space that has been shaped largely by qualitative proposals regarding the use of social determinants. To demonstrate theoretical perspectives and practical applicability, we examine a concrete setting of college admissions, using region as a proxy for social determinants. Our approach leverages a region-based analysis with Gamma distribution parameterization to model how social determinants impact individual outcomes. Despite its simplicity, our method quantitatively recovers findings that resonate with nuanced insights in previous qualitative debates, that are often missed by existing algorithmic fairness approaches. Our findings suggest that mitigation strategies centering solely around sensitive attributes may introduce new structural injustice when addressing existing discrimination. Considering both sensitive attributes and social determinants facilitates a more comprehensive explication of benefits and burdens experienced by individuals from diverse demographic backgrounds as well as contextual environments, which is essential for understanding and achieving fairness effectively and transparently.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†ç¤¾ä¼šå†³å®šå› ç´ (Social Determinants)åœ¨ç®—æ³•å…¬å¹³æ€§(Algorithmic Fairness)ä¸­çš„å…³é”®ä½œç”¨ï¼Œæ—¨åœ¨è§£å†³ä»¥å¾€æ–‡çŒ®è¿‡åº¦å…³æ³¨æ•æ„Ÿå±æ€§(Sensitive Attributes)è€Œå¿½è§†ç¯å¢ƒèƒŒæ™¯å˜é‡çš„é—®é¢˜ã€‚ä½œè€…ä¸ºç¤¾ä¼šå†³å®šå› ç´ çš„ç ”ç©¶å¼•å…¥äº†å½¢å¼åŒ–å’Œå®šé‡çš„ä¸¥è°¨æ€§ï¼Œå¹¶åœ¨å¤§å­¦å½•å–åœºæ™¯ä¸‹å°†åœ°åŒº(Region)ä½œä¸ºå…¶ä»£ç†å˜é‡ï¼Œåˆ©ç”¨ä¼½é©¬åˆ†å¸ƒ(Gamma Distribution)å‚æ•°åŒ–æ–¹æ³•å»ºæ¨¡å…¶å¯¹ä¸ªäººäº§å‡ºçš„å½±å“ã€‚ç ”ç©¶å®šé‡åœ°è¿˜åŸäº†ä»¥å¾€å®šæ€§è®¨è®ºä¸­å…³äºç¤¾ä¼šç¯å¢ƒçš„æ·±åˆ»è§è§£ï¼Œè¿™äº›è§è§£å¾€å¾€è¢«ç°æœ‰çš„å…¬å¹³æ€§æ–¹æ³•æ‰€å¿½ç•¥ã€‚å‘ç°è¡¨æ˜ï¼Œä»…å›´ç»•æ•æ„Ÿå±æ€§åˆ¶å®šçš„å¹²é¢„ç­–ç•¥åœ¨å¤„ç†ç°æœ‰æ­§è§†æ—¶ï¼Œå¯èƒ½ä¼šå¼•å…¥æ–°çš„ç»“æ„æ€§ä¸å…¬æ­£(Structural Injustice)ã€‚å› æ­¤ï¼ŒåŒæ—¶è€ƒè™‘æ•æ„Ÿå±æ€§å’Œç¤¾ä¼šå†³å®šå› ç´ ï¼Œå¯¹äºæ›´å…¨é¢åœ°è§£é‡Šä¸åŒèƒŒæ™¯ä¸‹ä¸ªä½“çš„åˆ©ç›Šä¸è´Ÿæ‹…ã€è¿›è€Œæœ‰æ•ˆå®ç°é€æ˜çš„å…¬å¹³æ€§å…·æœ‰é‡è¦æ„ä¹‰ã€‚",
      "categories": [
        "cs.CY",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CY",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.08337v1",
      "published_date": "2025-08-10 23:55:16 UTC",
      "updated_date": "2025-08-10 23:55:16 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T10:39:36.391496+00:00"
    },
    {
      "arxiv_id": "2508.07507v1",
      "title": "Intersectoral Knowledge in AI and Urban Studies: A Framework for Transdisciplinary Research",
      "title_zh": "äººå·¥æ™ºèƒ½ä¸åŸå¸‚ç ”ç©¶ä¸­çš„è·¨é¢†åŸŸçŸ¥è¯†ï¼šè·¨å­¦ç§‘ç ”ç©¶æ¡†æ¶",
      "authors": [
        "Rashid Mushkani"
      ],
      "abstract": "Transdisciplinary approaches are increasingly essential for addressing grand societal challenges, particularly in complex domains such as Artificial Intelligence (AI), urban planning, and social sciences. However, effectively validating and integrating knowledge across distinct epistemic and ontological perspectives poses significant difficulties. This article proposes a six-dimensional framework for assessing and strengthening transdisciplinary knowledge validity in AI and city studies, based on an extensive analysis of the most cited research (2014--2024). Specifically, the framework classifies research orientations according to ontological, epistemological, methodological, teleological, axiological, and valorization dimensions. Our findings show a predominance of perspectives aligned with critical realism (ontological), positivism (epistemological), analytical methods (methodological), consequentialism (teleological), epistemic values (axiological), and social/economic valorization. Less common stances, such as idealism, mixed methods, and cultural valorization, are also examined for their potential to enrich knowledge production. We highlight how early career researchers and transdisciplinary teams can leverage this framework to reconcile divergent disciplinary viewpoints and promote socially accountable outcomes.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ä¸ªé’ˆå¯¹äººå·¥æ™ºèƒ½(AI)ä¸åŸå¸‚ç ”ç©¶çš„å…­ç»´è·¨å­¦ç§‘çŸ¥è¯†æœ‰æ•ˆæ€§è¯„ä¼°æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³è·¨å­¦ç§‘é¢†åŸŸä¸­ä¸åŒè®¤è¯†è®ºå’Œæœ¬ä½“è®ºè§†è§’æ•´åˆå›°éš¾çš„é—®é¢˜ã€‚é€šè¿‡å¯¹2014è‡³2024å¹´é—´é«˜å¼•ç”¨æ–‡çŒ®çš„æ·±å…¥åˆ†æï¼Œè¯¥æ¡†æ¶ä»æœ¬ä½“è®º(ontological)ã€è®¤è¯†è®º(epistemological)ã€æ–¹æ³•è®º(methodological)ã€ç›®çš„è®º(teleological)ã€ä»·å€¼è®º(axiological)å’Œä»·å€¼åŒ–(valorization)å…­ä¸ªç»´åº¦å¯¹ç ”ç©¶å–å‘è¿›è¡Œäº†åˆ†ç±»ã€‚ç ”ç©¶å‘ç°å½“å‰å­¦æœ¯ç•Œæ™®éå€¾å‘äºæ‰¹åˆ¤ç°å®ä¸»ä¹‰(critical realism)ã€å®è¯ä¸»ä¹‰(positivism)å’Œåˆ†ææ–¹æ³•ï¼Œè€Œè¾ƒå°‘é‡‡ç”¨å”¯å¿ƒä¸»ä¹‰(idealism)æˆ–æ··åˆæ–¹æ³•ã€‚è¯¥æˆæœä¸ºæ—©æœŸç ”ç©¶äººå‘˜å’Œè·¨å­¦ç§‘å›¢é˜Ÿæä¾›äº†è°ƒå’Œå­¦ç§‘åˆ†æ­§çš„ç†è®ºå·¥å…·ï¼Œæœ‰åŠ©äºæ¨åŠ¨äº§ç”Ÿå…·æœ‰ç¤¾ä¼šé—®è´£æ€§çš„ç ”ç©¶æˆæœï¼Œå¹¶è¿›ä¸€æ­¥æŒ–æ˜å¤šå…ƒè§†è§’åœ¨çŸ¥è¯†ç”Ÿäº§ä¸­çš„æ½œåŠ›ã€‚",
      "categories": [
        "cs.CY",
        "cs.AI"
      ],
      "primary_category": "cs.CY",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.07507v1",
      "published_date": "2025-08-10 23:35:09 UTC",
      "updated_date": "2025-08-10 23:35:09 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T10:39:42.185937+00:00"
    },
    {
      "arxiv_id": "2508.10927v1",
      "title": "Modeling and Detecting Company Risks from News: A Case Study in Bloomberg News",
      "title_zh": "åŸºäºæ–°é—»çš„å…¬å¸é£é™©å»ºæ¨¡ä¸æ£€æµ‹ï¼šä»¥ Bloomberg News ä¸ºä¾‹çš„æ¡ˆä¾‹ç ”ç©¶",
      "authors": [
        "Jiaxin Pei",
        "Soumya Vadlamannati",
        "Liang-Kang Huang",
        "Daniel Preotiuc-Pietro",
        "Xinyu Hua"
      ],
      "abstract": "Identifying risks associated with a company is important to investors and the well-being of the overall financial market. In this study, we build a computational framework to automatically extract company risk factors from news articles. Our newly proposed schema comprises seven distinct aspects, such as supply chain, regulations, and competitions. We sample and annotate 744 news articles and benchmark various machine learning models. While large language models have achieved huge progress in various types of NLP tasks, our experiment shows that zero-shot and few-shot prompting state-of-the-art LLMs (e.g. LLaMA-2) can only achieve moderate to low performances in identifying risk factors. And fine-tuned pre-trained language models are performing better on most of the risk factors. Using this model, we analyze over 277K Bloomberg news articles and demonstrate that identifying risk factors from news could provide extensive insight into the operations of companies and industries.",
      "tldr_zh": "è¯¥ç ”ç©¶å»ºç«‹äº†ä¸€ä¸ªè‡ªåŠ¨ä»æ–°é—»æ–‡ç« ä¸­æå–å…¬å¸é£é™©å› ç´ çš„è®¡ç®—æ¡†æ¶ï¼Œæ—¨åœ¨ä¸ºæŠ•èµ„è€…å’Œé‡‘èå¸‚åœºæä¾›å†³ç­–æ”¯æŒã€‚ç ”ç©¶è€…æå‡ºäº†ä¸€ä¸ªæ–°çš„åˆ†ç±»æ¶æ„(Schema)ï¼Œæ¶µç›–äº†ä¾›åº”é“¾(Supply chain)ã€æ³•è§„(Regulations)å’Œç«äº‰(Competitions)ç­‰ä¸ƒä¸ªç»´åº¦çš„é£é™©å› ç´ ã€‚é€šè¿‡å¯¹744ç¯‡æ–°é—»è¿›è¡Œæ ‡æ³¨å¹¶æµ‹è¯•å¤šç§æœºå™¨å­¦ä¹ æ¨¡å‹ï¼Œç ”ç©¶å‘ç°å½“å‰å…ˆè¿›çš„å¤§è¯­è¨€æ¨¡å‹(LLMs)ï¼Œå¦‚LLaMA-2ï¼Œåœ¨é›¶æ ·æœ¬(Zero-shot)å’Œå°‘æ ·æœ¬(Few-shot)æç¤ºä¸‹ä»…èƒ½å–å¾—ä¸­ä½æ°´å¹³çš„è¡¨ç°ã€‚å®éªŒè¯æ˜ï¼Œç»è¿‡å¾®è°ƒçš„é¢„è®­ç»ƒè¯­è¨€æ¨¡å‹(Fine-tuned PLMs)åœ¨å¤šæ•°é£é™©å› ç´ è¯†åˆ«ä»»åŠ¡ä¸Šè¡¨ç°ä¼˜äºå¤§è¯­è¨€æ¨¡å‹ã€‚æœ€åï¼Œè¯¥ç ”ç©¶åˆ©ç”¨æ¨¡å‹åˆ†æäº†è¶…è¿‡27.7ä¸‡ç¯‡Bloombergæ–°é—»ï¼Œå±•ç¤ºäº†ä»æ–°é—»æ•°æ®ä¸­è‡ªåŠ¨è¯†åˆ«é£é™©å¯¹äºæ´å¯Ÿå…¬å¸å’Œè¡Œä¸šè¿è¥çŠ¶å†µçš„æ˜¾è‘—ä»·å€¼ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.CE",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.10927v1",
      "published_date": "2025-08-10 22:44:10 UTC",
      "updated_date": "2025-08-10 22:44:10 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T10:39:45.188461+00:00"
    },
    {
      "arxiv_id": "2508.07497v1",
      "title": "VA-Blueprint: Uncovering Building Blocks for Visual Analytics System Design",
      "title_zh": "VA-Blueprintï¼šæ­ç¤ºå¯è§†åŒ–åˆ†æç³»ç»Ÿè®¾è®¡çš„æ ¸å¿ƒæ„å»ºå—",
      "authors": [
        "Leonardo Ferreira",
        "Gustavo Moreira",
        "Fabio Miranda"
      ],
      "abstract": "Designing and building visual analytics (VA) systems is a complex, iterative process that requires the seamless integration of data processing, analytics capabilities, and visualization techniques. While prior research has extensively examined the social and collaborative aspects of VA system authoring, the practical challenges of developing these systems remain underexplored. As a result, despite the growing number of VA systems, there are only a few structured knowledge bases to guide their design and development. To tackle this gap, we propose VA-Blueprint, a methodology and knowledge base that systematically reviews and categorizes the fundamental building blocks of urban VA systems, a domain particularly rich and representative due to its intricate data and unique problem sets. Applying this methodology to an initial set of 20 systems, we identify and organize their core components into a multi-level structure, forming an initial knowledge base with a structured blueprint for VA system development. To scale this effort, we leverage a large language model to automate the extraction of these components for other 81 papers (completing a corpus of 101 papers), assessing its effectiveness in scaling knowledge base construction. We evaluate our method through interviews with experts and a quantitative analysis of annotation metrics. Our contributions provide a deeper understanding of VA systems' composition and establish a practical foundation to support more structured, reproducible, and efficient system development. VA-Blueprint is available at https://urbantk.org/va-blueprint.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† VA-Blueprintï¼Œè¿™æ˜¯ä¸€ç§æ—¨åœ¨ç³»ç»ŸåŒ–å®¡æŸ¥å’Œåˆ†ç±»åŸå¸‚ Visual Analytics (VA) ç³»ç»ŸåŸºç¡€æ„å»ºæ¨¡å—çš„æ–¹æ³•è®ºå’ŒçŸ¥è¯†åº“ï¼Œä»¥å¡«è¡¥ VA ç³»ç»Ÿè®¾è®¡é¢†åŸŸç¼ºä¹ç»“æ„åŒ–æŒ‡å¯¼çš„ç©ºç™½ã€‚ç ”ç©¶è€…é¦–å…ˆå¯¹ 20 ä¸ªä»£è¡¨æ€§ç³»ç»Ÿè¿›è¡Œæ·±å…¥åˆ†æï¼Œå°†å…¶æ ¸å¿ƒç»„ä»¶ç»„ç»‡æˆå¤šçº§ç»“æ„ï¼Œå½¢æˆäº† VA ç³»ç»Ÿå¼€å‘çš„åˆæ­¥è“å›¾ã€‚ä¸ºäº†æ‰©å¤§çŸ¥è¯†åº“è§„æ¨¡ï¼Œè¯¥å›¢é˜Ÿåˆ©ç”¨ Large Language Model (LLM) è‡ªåŠ¨åŒ–æå–äº†å¦å¤– 81 ç¯‡è®ºæ–‡çš„ç»„ä»¶ï¼Œæœ€ç»ˆæ„å»ºäº†ä¸€ä¸ªæ¶µç›– 101 ç¯‡è®ºæ–‡çš„å®Œæ•´è¯­æ–™åº“ã€‚é€šè¿‡ä¸“å®¶è®¿è°ˆå’Œæ ‡æ³¨æŒ‡æ ‡çš„å®šé‡åˆ†æï¼Œå®éªŒéªŒè¯äº†è¯¥æ–¹æ³•åœ¨ç³»ç»ŸåŒ–æ„å»ºçŸ¥è¯†åº“æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚è¿™ä¸€è´¡çŒ®æ·±åŒ–äº†å¯¹ VA ç³»ç»Ÿç»„æˆçš„ç†è§£ï¼Œä¸ºå®ç°æ›´å…·ç»“æ„åŒ–ã€å¯é‡å¤ä¸”é«˜æ•ˆçš„ç³»ç»Ÿå¼€å‘å¥ å®šäº†åšå®çš„å®è·µåŸºç¡€ã€‚",
      "categories": [
        "cs.HC",
        "cs.AI"
      ],
      "primary_category": "cs.HC",
      "comment": "Accepted at IEEE VIS 2025. VA-Blueprint is available at https://urbantk.org/va-blueprint",
      "pdf_url": "https://arxiv.org/pdf/2508.07497v1",
      "published_date": "2025-08-10 22:03:11 UTC",
      "updated_date": "2025-08-10 22:03:11 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T10:39:49.291508+00:00"
    },
    {
      "arxiv_id": "2508.07494v1",
      "title": "From Product Hilbert Spaces to the Generalized Koopman Operator and the Nonlinear Fundamental Lemma",
      "title_zh": "ä»ä¹˜ç§¯ Hilbert ç©ºé—´åˆ°å¹¿ä¹‰ Koopman ç®—å­ä¸éçº¿æ€§åŸºæœ¬å¼•ç†",
      "authors": [
        "Mircea Lazar"
      ],
      "abstract": "The generalization of the Koopman operator to systems with control input and the derivation of a nonlinear fundamental lemma are two open problems that play a key role in the development of data-driven control methods for nonlinear systems. Both problems hinge on the construction of observable or basis functions and their corresponding Hilbert space that enable an infinite-dimensional, linear system representation. In this paper we derive a novel solution to these problems based on orthonormal expansion in a product Hilbert space constructed as the tensor product between the Hilbert spaces of the state and input observable functions, respectively. We prove that there exists an infinite-dimensional linear operator, i.e. the generalized Koopman operator, from the constructed product Hilbert space to the Hilbert space corresponding to the lifted state propagated forward in time. A scalable data-driven method for computing finite-dimensional approximations of generalized Koopman operators and several choices of observable functions are also presented. Moreover, we derive a nonlinear fundamental lemma by exploiting the bilinear structure of the infinite-dimensional generalized Koopman model. The effectiveness of the developed generalized Koopman embedding is illustrated on the Van der Pol oscillator.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹éçº¿æ€§ç³»ç»Ÿæ•°æ®é©±åŠ¨æ§åˆ¶ä¸­çš„ä¸¤å¤§å…³é”®æŒ‘æˆ˜ï¼Œå³ Koopman operator åœ¨å«æ§åˆ¶è¾“å…¥ç³»ç»Ÿä¸­çš„æ³›åŒ–ä»¥åŠ nonlinear fundamental lemma çš„æ¨å¯¼ï¼Œæå‡ºäº†ä¸€ç§åŸºäº product Hilbert space çš„åˆ›æ–°è§£å†³æ–¹æ¡ˆã€‚ä½œè€…é€šè¿‡æ„å»ºçŠ¶æ€ä¸è¾“å…¥å¯è§‚æµ‹å‡½æ•° Hilbert spaces çš„å¼ é‡ç§¯ç©ºé—´ï¼Œè¯æ˜äº†åœ¨è¯¥ä¹˜ç§¯ç©ºé—´ä¸éšæ—¶é—´æ¼”åŒ–çš„æå‡çŠ¶æ€ç©ºé—´ä¹‹é—´å­˜åœ¨ä¸€ä¸ªæ— é™ç»´çº¿æ€§ç®—å­ï¼Œå³ generalized Koopman operatorã€‚æ–‡ä¸­è¿›ä¸€æ­¥æä¾›äº†ä¸€ç§å¯æ‰©å±•çš„æ•°æ®é©±åŠ¨ç®—æ³•ï¼Œç”¨äºè®¡ç®—è¯¥ç®—å­çš„æœ‰é™ç»´è¿‘ä¼¼ï¼Œå¹¶ç»™å‡ºäº†å¤šç§å¯è§‚æµ‹å‡½æ•°çš„é€‰å–å»ºè®®ã€‚æ­¤å¤–ï¼Œç ”ç©¶åˆ©ç”¨æ— é™ç»´ generalized Koopman æ¨¡å‹çš„ bilinear structureï¼ŒæˆåŠŸæ¨å¯¼å‡ºäº† nonlinear fundamental lemmaã€‚é€šè¿‡åœ¨ Van der Pol oscillator ä¸Šçš„ä»¿çœŸå®éªŒï¼Œè¯¥ç ”ç©¶éªŒè¯äº†æ‰€æå‡ºçš„ generalized Koopman embedding åœ¨å¤„ç†éçº¿æ€§åŠ¨åŠ›ç³»ç»Ÿå»ºæ¨¡ä¸æ§åˆ¶ä¸­çš„æœ‰æ•ˆæ€§ã€‚",
      "categories": [
        "math.OC",
        "cs.AI"
      ],
      "primary_category": "math.OC",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.07494v1",
      "published_date": "2025-08-10 21:57:16 UTC",
      "updated_date": "2025-08-10 21:57:16 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T10:39:56.191099+00:00"
    },
    {
      "arxiv_id": "2508.07486v1",
      "title": "Extracting Overlapping Microservices from Monolithic Code via Deep Semantic Embeddings and Graph Neural Network-Based Soft Clustering",
      "title_zh": "åŸºäºæ·±åº¦è¯­ä¹‰åµŒå…¥ä¸å›¾ç¥ç»ç½‘ç»œè½¯èšç±»çš„å•ä½“ä»£ç é‡å å¾®æœåŠ¡æå–",
      "authors": [
        "Morteza Ziabakhsh",
        "Kiyan Rezaee",
        "Sadegh Eskandari",
        "Seyed Amir Hossein Tabatabaei",
        "Mohammad M. Ghassemi"
      ],
      "abstract": "Modern software systems are increasingly shifting from monolithic architectures to microservices to enhance scalability, maintainability, and deployment flexibility. Existing microservice extraction methods typically rely on hard clustering, assigning each software component to a single microservice. This approach often increases inter-service coupling and reduces intra-service cohesion. We propose Mo2oM (Monolithic to Overlapping Microservices), a framework that formulates microservice extraction as a soft clustering problem, allowing components to belong probabilistically to multiple microservices. This approach is inspired by expert-driven decompositions, where practitioners intentionally replicate certain software components across services to reduce communication overhead. Mo2oM combines deep semantic embeddings with structural dependencies extracted from methodcall graphs to capture both functional and architectural relationships. A graph neural network-based soft clustering algorithm then generates the final set of microservices. We evaluate Mo2oM on four open-source monolithic benchmarks and compare it against eight state-of-the-art baselines. Our results demonstrate that Mo2oM achieves improvements of up to 40.97% in structural modularity (balancing cohesion and coupling), 58% in inter-service call percentage (communication overhead), 26.16% in interface number (modularity and decoupling), and 38.96% in non-extreme distribution (service size balance) across all benchmarks.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†Mo2oM (Monolithic to Overlapping Microservices)æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³ä¼ ç»Ÿå¾®æœåŠ¡æ‹†åˆ†æ–¹æ³•ä¸­ç”±äºç¡¬èšç±»(Hard Clustering)å¯¼è‡´çš„è·¨æœåŠ¡è€¦åˆåº¦é«˜å’Œå†…éƒ¨å‡èšåŠ›ä¸è¶³çš„é—®é¢˜ã€‚Mo2oMå°†å¾®æœåŠ¡æå–å»ºæ¨¡ä¸ºè½¯èšç±»(Soft Clustering)é—®é¢˜ï¼Œå…è®¸ä»£ç ç»„ä»¶ä»¥æ¦‚ç‡å½¢å¼å±äºå¤šä¸ªå¾®æœåŠ¡ï¼Œè¿™ä¸€è®¾è®¡çµæ„Ÿæºäºä¸“å®¶é€šè¿‡ç»„ä»¶å¤ç”¨ä»¥å‡å°‘æœåŠ¡é—´é€šä¿¡å¼€é”€çš„å®è·µç»éªŒã€‚è¯¥æ¡†æ¶ç»“åˆäº†æ·±åº¦è¯­ä¹‰åµŒå…¥(Deep Semantic Embeddings)ä¸ä»æ–¹æ³•è°ƒç”¨å›¾(Methodcall Graphs)ä¸­æå–çš„ç»“æ„ä¾èµ–å…³ç³»ï¼Œä»¥æ•æ‰åŠŸèƒ½ä¸æ¶æ„å±‚é¢çš„å¤æ‚å…³è”ã€‚éšåï¼Œåˆ©ç”¨åŸºäºå›¾ç¥ç»ç½‘ç»œ(Graph Neural Network)çš„è½¯èšç±»ç®—æ³•ç”Ÿæˆæœ€ç»ˆçš„å¾®æœåŠ¡é›†åˆã€‚åœ¨å››ä¸ªå¼€æºå•ä½“åŸºå‡†æµ‹è¯•é›†ä¸Šçš„è¯„ä¼°ç»“æœæ˜¾ç¤ºï¼ŒMo2oMåœ¨ç»“æ„æ¨¡å—åŒ–(Structural Modularity)æ–¹é¢æå‡äº†å¤šè¾¾40.97%ï¼Œå¹¶æ˜¾è‘—é™ä½äº†58%çš„æœåŠ¡é—´è°ƒç”¨æ¯”ä¾‹ã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•åœ¨æ¥å£æ•°é‡å’Œå¾®æœåŠ¡è§„æ¨¡å¹³è¡¡æ€§(Non-extreme Distribution)ç­‰æŒ‡æ ‡ä¸Šä¹Ÿè¡¨ç°ä¼˜å¼‚ï¼Œä¸ºä»å•ä½“æ¶æ„å‘å¾®æœåŠ¡å¹³æ»‘æ¼”è¿›æä¾›äº†æœ‰æ•ˆçš„è‡ªåŠ¨åŒ–æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.SE",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.SE",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.07486v1",
      "published_date": "2025-08-10 21:07:20 UTC",
      "updated_date": "2025-08-10 21:07:20 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T10:39:55.387594+00:00"
    },
    {
      "arxiv_id": "2508.07485v1",
      "title": "Democratizing Diplomacy: A Harness for Evaluating Any Large Language Model on Full-Press Diplomacy",
      "title_zh": "å¤–äº¤åšå¼ˆæ™®åŠåŒ–ï¼šè¯„ä¼°å¤§è¯­è¨€æ¨¡å‹åœ¨å…¨é€šè®¯ç‰ˆ Diplomacy ä¸­è¡¨ç°çš„é€šç”¨æ¡†æ¶",
      "authors": [
        "Alexander Duffy",
        "Samuel J Paech",
        "Ishana Shastri",
        "Elizabeth Karpinski",
        "Baptiste Alloui-Cros",
        "Tyler Marques",
        "Matthew Lyle Olson"
      ],
      "abstract": "We present the first evaluation harness that enables any out-of-the-box, local, Large Language Models (LLMs) to play full-press Diplomacy without fine-tuning or specialized training. Previous work required frontier LLMs, or fine-tuning, due to the high complexity and information density of Diplomacy's game state. Combined with the high variance of matches, these factors made Diplomacy prohibitive for study. In this work, we used data-driven iteration to optimize a textual game state representation such that a 24B model can reliably complete matches without any fine tuning. We develop tooling to facilitate hypothesis testing and statistical analysis, and we present case studies on persuasion, aggressive playstyles, and performance across a range of models. We conduct a variety of experiments across many popular LLMs, finding the larger models perform the best, but the smaller models still play adequately. We also introduce Critical State Analysis: an experimental protocol for rapidly iterating and analyzing key moments in a game at depth. Our harness democratizes the evaluation of strategic reasoning in LLMs by eliminating the need for fine-tuning, and it provides insights into how these capabilities emerge naturally from widely used LLMs. Our code is available in the supplement and will be open sourced.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†é¦–ä¸ªè¯„ä¼°æ¡†æ¶(evaluation harness)ï¼Œå…è®¸ä»»ä½•å¼€ç®±å³ç”¨çš„æœ¬åœ°å¤§è¯­è¨€æ¨¡å‹(LLMs)åœ¨æ— éœ€å¾®è°ƒ(fine-tuning)æˆ–ä¸“é—¨è®­ç»ƒçš„æƒ…å†µä¸‹å‚ä¸å®Œæ•´ç‰ˆå¤–äº¤(Diplomacy)æ¸¸æˆã€‚é’ˆå¯¹è¯¥æ¸¸æˆçŠ¶æ€çš„é«˜å¤æ‚æ€§å’Œé«˜ä¿¡æ¯å¯†åº¦ï¼Œä½œè€…é€šè¿‡æ•°æ®é©±åŠ¨çš„è¿­ä»£ä¼˜åŒ–äº†æ–‡æœ¬æ¸¸æˆçŠ¶æ€è¡¨ç¤º(textual game state representation)ï¼Œä½¿å¾—24Bå‚æ•°è§„æ¨¡çš„æ¨¡å‹ä¹Ÿèƒ½åœ¨ä¸ç»è¿‡å¾®è°ƒçš„æƒ…å†µä¸‹å¯é åœ°å®Œæˆæ¯”èµ›ã€‚ç ”ç©¶å›¢é˜Ÿå¼€å‘äº†é…å¥—å·¥å…·ä»¥æ”¯æŒå‡è®¾æ£€éªŒå’Œç»Ÿè®¡åˆ†æï¼Œå¹¶é’ˆå¯¹è¯´æœåŠ›(persuasion)ã€æ¿€è¿›ç­–ç•¥ä»¥åŠä¸åŒæ¨¡å‹çš„æ€§èƒ½è¡¨ç°è¿›è¡Œäº†æ·±å…¥çš„æ¡ˆä¾‹ç ”ç©¶ã€‚æ­¤å¤–ï¼Œè®ºæ–‡å¼•å…¥äº†å…³é”®çŠ¶æ€åˆ†æ(Critical State Analysis)åè®®ï¼Œç”¨äºå¿«é€Ÿè¿­ä»£å¹¶æ·±åº¦åˆ†ææ¸¸æˆä¸­çš„å…³é”®è½¬æŠ˜ç‚¹ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè™½ç„¶å¤§å‹æ¨¡å‹è¡¨ç°æœ€ä¼˜ï¼Œä½†è¾ƒå°å‹æ¨¡å‹ä¾ç„¶èƒ½è¡¨ç°å‡ºåŸºæœ¬çš„åšå¼ˆèƒ½åŠ›ã€‚è¯¥æ¡†æ¶é€šè¿‡æ¶ˆé™¤å¯¹å¾®è°ƒçš„éœ€æ±‚ï¼Œä½¿LLMsæˆ˜ç•¥æ¨ç†èƒ½åŠ›çš„è¯„ä¼°æ›´åŠ æ™®åŠåŒ–ï¼Œä¸ºæ¢ç´¢è¿™äº›èƒ½åŠ›å¦‚ä½•åœ¨ä¸»æµå¤§æ¨¡å‹ä¸­è‡ªç„¶æ¶Œç°æä¾›äº†è§è§£ã€‚",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.CY",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.07485v1",
      "published_date": "2025-08-10 21:07:08 UTC",
      "updated_date": "2025-08-10 21:07:08 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T10:40:55.663531+00:00"
    },
    {
      "arxiv_id": "2508.07484v1",
      "title": "ALOPE: Adaptive Layer Optimization for Translation Quality Estimation using Large Language Models",
      "title_zh": "ALOPEï¼šåŸºäºå¤§è¯­è¨€æ¨¡å‹çš„ç¿»è¯‘è´¨é‡è¯„ä¼°è‡ªé€‚åº”å±‚ä¼˜åŒ–",
      "authors": [
        "Archchana Sindhujan",
        "Shenbin Qian",
        "Chan Chi Chun Matthew",
        "Constantin Orasan",
        "Diptesh Kanojia"
      ],
      "abstract": "Large Language Models (LLMs) have shown remarkable performance across a wide range of natural language processing tasks. Quality Estimation (QE) for Machine Translation (MT), which assesses the quality of a source-target pair without relying on reference translations, remains a challenging cross-lingual task for LLMs. The challenges stem from the inherent limitations of existing LLM-based QE systems, which are pre-trained for causal language modelling rather than regression-specific tasks, further elevated by the presence of low-resource languages given pre-training data distribution. This paper introduces ALOPE, an adaptive layer-optimization framework designed to enhance LLM-based QE by restructuring Transformer representations through layer-wise adaptation for improved regression-based prediction. Our framework integrates low-rank adapters (LoRA) with regression task heads, leveraging selected pre-trained Transformer layers for improved cross-lingual alignment. In addition to the layer-specific adaptation, ALOPE introduces two strategies-dynamic weighting, which adaptively combines representations from multiple layers, and multi-head regression, which aggregates regression losses from multiple heads for QE. Our framework shows improvements over various existing LLM-based QE approaches. Empirical evidence suggests that intermediate Transformer layers in LLMs provide contextual representations that are more aligned with the cross-lingual nature of the QE task. We make resultant models and framework code publicly available for further research, also allowing existing LLM-based MT frameworks to be scaled with QE capabilities.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤§è¯­è¨€æ¨¡å‹(LLMs)åœ¨æœºå™¨ç¿»è¯‘è´¨é‡è¯„ä¼°(Quality Estimation, QE)ä¸­é¢ä¸´çš„é¢„è®­ç»ƒç›®æ ‡ä¸åŒ¹é…åŠè·¨è¯­è¨€å¯¹é½éš¾é¢˜ï¼Œæå‡ºäº†ALOPEè‡ªé€‚åº”å±‚ä¼˜åŒ–æ¡†æ¶ã€‚ALOPEé€šè¿‡å°†ä½ç§©é€‚é…å™¨(LoRA)ä¸å›å½’ä»»åŠ¡å¤´(regression task heads)ç›¸ç»“åˆï¼Œå¯¹é€‰å®šçš„Transformerå±‚è¿›è¡Œå±‚çº§é€‚é…ï¼Œä»è€Œé‡æ„è¡¨ç¤ºä»¥ä¼˜åŒ–å›å½’é¢„æµ‹ã€‚æ¡†æ¶è¿›ä¸€æ­¥å¼•å…¥äº†åŠ¨æ€æƒé‡(dynamic weighting)å’Œå¤šå¤´å›å½’(multi-head regression)ç­–ç•¥ï¼Œé€šè¿‡è‡ªé€‚åº”ç»„åˆå¤šå±‚è¡¨ç¤ºå¹¶èšåˆå›å½’æŸå¤±æ¥æå‡è¯„ä¼°ç²¾åº¦ã€‚å®éªŒç»“æœè¯æ˜ï¼ŒALOPEåœ¨å¤šä¸ªQEä»»åŠ¡ä¸Šæ˜¾è‘—ä¼˜äºç°æœ‰æ¨¡å‹ï¼Œå¹¶å‘ç°LLMçš„ä¸­é—´å±‚(intermediate layers)åœ¨æ•æ‰è·¨è¯­è¨€ç‰¹æ€§æ–¹é¢å…·æœ‰æ›´ä¼˜çš„ä¸Šä¸‹æ–‡è¡¨ç¤ºã€‚è¯¥é¡¹å·¥ä½œä¸ä»…ä¸ºæœºå™¨ç¿»è¯‘æ¡†æ¶é›†æˆQEèƒ½åŠ›æä¾›äº†å¯æ‰©å±•çš„è§£å†³æ–¹æ¡ˆï¼Œè¿˜é€šè¿‡å¼€æºä»£ç å’Œæ¨¡å‹ä¿ƒè¿›äº†ç›¸å…³é¢†åŸŸçš„åç»­ç ”ç©¶ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "Accepted to COLM 2025 Conference",
      "pdf_url": "https://arxiv.org/pdf/2508.07484v1",
      "published_date": "2025-08-10 20:59:44 UTC",
      "updated_date": "2025-08-10 20:59:44 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T10:40:33.700466+00:00"
    },
    {
      "arxiv_id": "2508.07468v2",
      "title": "CP-Agent: Agentic Constraint Programming",
      "title_zh": "CP-Agentï¼šåŸºäºæ™ºèƒ½ä½“çš„çº¦æŸè§„åˆ’",
      "authors": [
        "Stefan Szeider"
      ],
      "abstract": "Translating natural language into formal constraint models requires expertise in the problem domain and modeling frameworks. To investigate whether constraint modeling benefits from agentic workflows, we introduce CP-Agent, a Python coding agent using the ReAct framework with a persistent IPython kernel. Domain knowledge is provided through a project prompt of under 50 lines. The agent iteratively executes code, observes the solver's feedback, and refines models based on the execution results.\n  We evaluate CP-Agent on CP-Bench's 101 constraint programming problems. We clarified the benchmark to address systematic ambiguities in problem specifications and errors in ground-truth models. On the clarified benchmark, CP-Agent solves all 101 problems. Ablation studies indicate that minimal guidance outperforms detailed procedural scaffolding, and that explicit task management tools have mixed effects on focused modeling tasks.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¨å‡ºäº† CP-Agentï¼Œè¿™æ˜¯ä¸€æ¬¾æ—¨åœ¨å°†è‡ªç„¶è¯­è¨€è‡ªåŠ¨è½¬åŒ–ä¸ºæ­£å¼çº¦æŸæ¨¡å‹ï¼ˆformal constraint modelsï¼‰çš„æ™ºèƒ½ä½“æ¡†æ¶ã€‚å®ƒåŸºäº ReAct æ¡†æ¶å¹¶åˆ©ç”¨æŒä¹…åŒ– IPython å†…æ ¸æ„å»º Python ç¼–ç æ™ºèƒ½ä½“ï¼Œé€šè¿‡ä¸è¶³ 50 è¡Œçš„é¡¹ç›®æç¤ºè¯è·å–é¢†åŸŸçŸ¥è¯†ã€‚è¯¥æ™ºèƒ½ä½“é‡‡ç”¨è¿­ä»£æ‰§è¡Œä»£ç ã€è§‚å¯Ÿæ±‚è§£å™¨åé¦ˆå¹¶æ ¹æ®ç»“æœä¼˜åŒ–æ¨¡å‹çš„å·¥ä½œæµç¨‹ã€‚ç ”ç©¶å›¢é˜Ÿåœ¨ CP-Bench çš„ 101 ä¸ªçº¦æŸè§„åˆ’é—®é¢˜ä¸Šå¯¹ CP-Agent è¿›è¡Œäº†æµ‹è¯•ï¼Œå¹¶ä¿®æ­£äº†åŸºå‡†ä¸­å­˜åœ¨çš„ç³»ç»Ÿæ€§æ­§ä¹‰å’Œé”™è¯¯ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒCP-Agent åœ¨ä¼˜åŒ–åçš„åŸºå‡†ä¸ŠæˆåŠŸè§£å†³äº†å…¨éƒ¨ 101 ä¸ªé—®é¢˜ã€‚æ¶ˆèç ”ç©¶è¡¨æ˜ï¼Œæœ€å°åŒ–å¼•å¯¼ï¼ˆminimal guidanceï¼‰çš„æ•ˆæœä¼˜äºè¯¦ç»†çš„ç¨‹åºåŒ–è„šæ‰‹æ¶ï¼ˆprocedural scaffoldingï¼‰ï¼Œä¸”æ˜¾å¼ä»»åŠ¡ç®¡ç†å·¥å…·å¯¹å»ºæ¨¡ä»»åŠ¡çš„å½±å“åˆ©å¼Šå‚åŠã€‚",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.LG",
        "cs.SE"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.07468v2",
      "published_date": "2025-08-10 19:59:01 UTC",
      "updated_date": "2025-12-26 10:12:55 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T10:40:18.818694+00:00"
    },
    {
      "arxiv_id": "2508.07466v1",
      "title": "Grounding Natural Language for Multi-agent Decision-Making with Multi-agentic LLMs",
      "title_zh": "åŸºäºå¤šæ™ºèƒ½ä½“å¤§è¯­è¨€æ¨¡å‹çš„å¤šæ™ºèƒ½ä½“å†³ç­–è‡ªç„¶è¯­è¨€æ¥åœ°",
      "authors": [
        "Dom Huh",
        "Prasant Mohapatra"
      ],
      "abstract": "Language is a ubiquitous tool that is foundational to reasoning and collaboration, ranging from everyday interactions to sophisticated problem-solving tasks. The establishment of a common language can serve as a powerful asset in ensuring clear communication and understanding amongst agents, facilitating desired coordination and strategies. In this work, we extend the capabilities of large language models (LLMs) by integrating them with advancements in multi-agent decision-making algorithms. We propose a systematic framework for the design of multi-agentic large language models (LLMs), focusing on key integration practices. These include advanced prompt engineering techniques, the development of effective memory architectures, multi-modal information processing, and alignment strategies through fine-tuning algorithms. We evaluate these design choices through extensive ablation studies on classic game settings with significant underlying social dilemmas and game-theoretic considerations.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†åˆ©ç”¨è‡ªç„¶è¯­è¨€å¢å¼ºå¤šæ™ºèƒ½ä½“ç³»ç»Ÿï¼ˆmulti-agent systemsï¼‰æ¨ç†ä¸åä½œèƒ½åŠ›çš„æ–¹æ³•ï¼Œå¹¶æå‡ºäº†ä¸€ä¸ªè®¾è®¡å¤šæ™ºèƒ½ä½“å¤§è¯­è¨€æ¨¡å‹ï¼ˆmulti-agentic LLMsï¼‰çš„ç³»ç»ŸåŒ–æ¡†æ¶ã€‚è¯¥æ¡†æ¶å°†LLMä¸å…ˆè¿›çš„å¤šæ™ºèƒ½ä½“å†³ç­–ç®—æ³•ç›¸ç»“åˆï¼Œé‡ç‚¹ç ”ç©¶äº†é«˜çº§æç¤ºå·¥ç¨‹ï¼ˆprompt engineeringï¼‰ã€æœ‰æ•ˆçš„è®°å¿†æ¶æ„ï¼ˆmemory architecturesï¼‰ã€å¤šæ¨¡æ€ä¿¡æ¯å¤„ç†ï¼ˆmulti-modal processingï¼‰ä»¥åŠé€šè¿‡å¾®è°ƒç®—æ³•å®ç°çš„å¯¹é½ç­–ç•¥ï¼ˆalignment strategiesï¼‰ç­‰å…³é”®é›†æˆå®è·µã€‚ç ”ç©¶è€…åœ¨åŒ…å«ç¤¾ä¼šå›°å¢ƒå’Œåšå¼ˆè®ºè€ƒè™‘çš„ç»å…¸æ¸¸æˆåœºæ™¯ä¸­è¿›è¡Œäº†å¹¿æ³›çš„æ¶ˆèå®éªŒï¼Œä»¥éªŒè¯ä¸åŒè®¾è®¡é€‰æ‹©çš„æœ‰æ•ˆæ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ¡†æ¶èƒ½å¤Ÿé€šè¿‡å»ºç«‹å…±åŒè¯­è¨€ç¡®ä¿æ™ºèƒ½ä½“ä¹‹é—´çš„æ¸…æ™°æ²Ÿé€šï¼Œä»è€Œæœ‰æ•ˆä¿ƒè¿›å¤šæ™ºèƒ½ä½“ç¯å¢ƒä¸‹çš„åè°ƒä¸ç­–ç•¥åˆ¶å®šã€‚è¯¥å·¥ä½œä¸ºåˆ©ç”¨LLMè§£å†³å¤æ‚å¤šæ™ºèƒ½ä½“å†³ç­–é—®é¢˜æä¾›äº†æ ‡å‡†åŒ–çš„è®¾è®¡æŒ‡å¯¼å’Œå®è¯ä¾æ®ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.07466v1",
      "published_date": "2025-08-10 19:53:23 UTC",
      "updated_date": "2025-08-10 19:53:23 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T10:40:23.696855+00:00"
    },
    {
      "arxiv_id": "2508.07453v1",
      "title": "Noise-Aware Generative Microscopic Traffic Simulation",
      "title_zh": "å™ªå£°æ„ŸçŸ¥çš„ç”Ÿæˆå¼å¾®è§‚äº¤é€šä»¿çœŸ",
      "authors": [
        "Vindula Jayawardana",
        "Catherine Tang",
        "Junyi Ji",
        "Jonah Philion",
        "Xue Bin Peng",
        "Cathy Wu"
      ],
      "abstract": "Accurately modeling individual vehicle behavior in microscopic traffic simulation remains a key challenge in intelligent transportation systems, as it requires vehicles to realistically generate and respond to complex traffic phenomena such as phantom traffic jams. While traditional human driver simulation models offer computational tractability, they do so by abstracting away the very complexity that defines human driving. On the other hand, recent advances in infrastructure-mounted camera-based roadway sensing have enabled the extraction of vehicle trajectory data, presenting an opportunity to shift toward generative, agent-based models. Yet, a major bottleneck remains: most existing datasets are either overly sanitized or lack standardization, failing to reflect the noisy, imperfect nature of real-world sensing. Unlike data from vehicle-mounted sensors-which can mitigate sensing artifacts like occlusion through overlapping fields of view and sensor fusion-infrastructure-based sensors surface a messier, more practical view of challenges that traffic engineers encounter. To this end, we present the I-24 MOTION Scenario Dataset (I24-MSD)-a standardized, curated dataset designed to preserve a realistic level of sensor imperfection, embracing these errors as part of the learning problem rather than an obstacle to overcome purely from preprocessing. Drawing from noise-aware learning strategies in computer vision, we further adapt existing generative models in the autonomous driving community for I24-MSD with noise-aware loss functions. Our results show that such models not only outperform traditional baselines in realism but also benefit from explicitly engaging with, rather than suppressing, data imperfection. We view I24-MSD as a stepping stone toward a new generation of microscopic traffic simulation that embraces the real-world challenges and is better aligned with practical needs.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¾®è§‚äº¤é€šä»¿çœŸ(microscopic traffic simulation)ä¸­å‡†ç¡®æ¨¡æ‹Ÿè½¦è¾†è¡Œä¸ºçš„æŒ‘æˆ˜ï¼ŒæŒ‡å‡ºä¼ ç»Ÿæ¨¡å‹å› è¿‡åº¦ç®€åŒ–è€Œéš¾ä»¥æ•æ‰å¤æ‚äº¤é€šç°è±¡ï¼Œä¸”ç°æœ‰æ•°æ®é›†å¾€å¾€å¿½è§†äº†ç°å®ä¼ æ„Ÿå™¨äº§ç”Ÿçš„å™ªå£°ã€‚ä¸ºæ­¤ï¼Œç ”ç©¶å›¢é˜Ÿæ¨å‡ºäº† I-24 MOTION Scenario Dataset (I24-MSD)ï¼Œè¿™æ˜¯ä¸€ä¸ªæ—¨åœ¨ä¿ç•™çœŸå®æ„Ÿæµ‹ç¼ºé™·çš„æ ‡å‡†åŒ–æ•°æ®é›†ï¼Œå°†å™ªå£°è§†ä¸ºå­¦ä¹ é—®é¢˜çš„ä¸€éƒ¨åˆ†è€Œéä»…é€šè¿‡é¢„å¤„ç†æ¶ˆé™¤ã€‚ç ”ç©¶å€Ÿé‰´äº†è®¡ç®—æœºè§†è§‰ä¸­çš„å™ªå£°æ„ŸçŸ¥å­¦ä¹ (noise-aware learning)ç­–ç•¥ï¼Œé€šè¿‡å¼•å…¥å™ªå£°æ„ŸçŸ¥æŸå¤±å‡½æ•°(noise-aware loss functions)æ”¹è¿›äº†è‡ªåŠ¨é©¾é©¶é¢†åŸŸçš„ç”Ÿæˆå¼æ¨¡å‹ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯¥æ¨¡å‹åœ¨çœŸå®æ€§ä¸Šæ˜¾è‘—ä¼˜äºä¼ ç»ŸåŸºçº¿ï¼Œè¯æ˜æ˜¾å¼å¤„ç†è€ŒéæŠ‘åˆ¶æ•°æ®ç¼ºé™·èƒ½æå‡ä»¿çœŸæ•ˆæœã€‚è¯¥æˆæœä¸ºå¼€å‘å¯¹é½å®é™…éœ€æ±‚ã€èƒ½å¤Ÿåº”å¯¹ç°å®æŒ‘æˆ˜çš„æ–°ä¸€ä»£å¾®è§‚äº¤é€šä»¿çœŸæŠ€æœ¯å¥ å®šäº†åŸºç¡€ã€‚",
      "categories": [
        "eess.SY",
        "cs.AI",
        "cs.MA",
        "cs.RO"
      ],
      "primary_category": "eess.SY",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.07453v1",
      "published_date": "2025-08-10 18:41:49 UTC",
      "updated_date": "2025-08-10 18:41:49 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T10:41:12.753083+00:00"
    },
    {
      "arxiv_id": "2508.07452v2",
      "title": "Stackelberg Coupling of Online Representation Learning and Reinforcement Learning",
      "title_zh": "åœ¨çº¿è¡¨ç¤ºå­¦ä¹ ä¸å¼ºåŒ–å­¦ä¹ çš„ Stackelberg è€¦åˆ",
      "authors": [
        "Fernando Martinez",
        "Tao Li",
        "Yingdong Lu",
        "Juntao Chen"
      ],
      "abstract": "Deep Q-learning jointly learns representations and values within monolithic networks, promising beneficial co-adaptation between features and value estimates. Although this architecture has attained substantial success, the coupling between representation and value learning creates instability as representations must constantly adapt to non-stationary value targets, while value estimates depend on these shifting representations. This is compounded by high variance in bootstrapped targets, which causes bias in value estimation in off-policy methods. We introduce Stackelberg Coupled Representation and Reinforcement Learning (SCORER), a framework for value-based RL that views representation and Q-learning as two strategic agents in a hierarchical game. SCORER models the Q-function as the leader, which commits to its strategy by updating less frequently, while the perception network (encoder) acts as the follower, adapting more frequently to learn representations that minimize Bellman error variance given the leader's committed strategy. Through this division of labor, the Q-function minimizes MSBE while perception minimizes its variance, thereby reducing bias accordingly, with asymmetric updates allowing stable co-adaptation, unlike simultaneous parameter updates in monolithic solutions. Our proposed SCORER framework leads to a bi-level optimization problem whose solution is approximated by a two-timescale algorithm that creates an asymmetric learning dynamic between the two players. Extensive experiments on DQN and its variants demonstrate that gains stem from algorithmic insight rather than model complexity.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ Deep Q-learning ä¸­è¡¨ç¤ºå­¦ä¹ ä¸å€¼å­¦ä¹ è€¦åˆå¯¼è‡´çš„éå¹³ç¨³æ€§å’Œä¸ç¨³å®šæ€§é—®é¢˜ï¼Œæå‡ºäº† Stackelberg Coupled Representation and Reinforcement Learning (SCORER) æ¡†æ¶ã€‚SCORER å°†è¡¨ç¤ºå­¦ä¹ å’Œ Q-learning å»ºæ¨¡ä¸ºåˆ†å±‚åšå¼ˆä¸­çš„ä¸¤ä¸ªç­–ç•¥æ™ºèƒ½ä½“ï¼Œå…¶ä¸­ Q-function ä½œä¸ºé¢†å¯¼è€… (leader) é€šè¿‡é™ä½æ›´æ–°é¢‘ç‡æ¥ç¨³å®šç­–ç•¥ï¼Œè€Œæ„ŸçŸ¥ç½‘ç»œåˆ™ä½œä¸ºè·Ÿéšè€… (follower) è¿›è¡Œæ›´é¢‘ç¹çš„æ›´æ–°ï¼Œä»¥å­¦ä¹ èƒ½æœ€å°åŒ– Bellman error æ–¹å·®çš„è¡¨ç¤ºã€‚è¿™ç§è®¾è®¡å®ç°äº† Q-function æœ€å°åŒ– MSBE ä¸æ„ŸçŸ¥å±‚æœ€å°åŒ–æ–¹å·®çš„èŒèƒ½åˆ†å·¥ï¼Œé€šè¿‡ä¸å¯¹ç§°æ›´æ–° (asymmetric updates) è¾¾æˆäº†æ¯”ä¼ ç»ŸåŒæ­¥æ›´æ–°æ›´ç¨³å¥çš„å…±è‡ªé€‚åº”ã€‚è¯¥æ¡†æ¶å¼•å…¥äº†ä¸€ä¸ªåŒå±‚ä¼˜åŒ–é—®é¢˜ (bi-level optimization problem)ï¼Œå¹¶é‡‡ç”¨åŒæ—¶é—´å°ºåº¦ç®—æ³• (two-timescale algorithm) åˆ›å»ºä¸å¯¹ç§°å­¦ä¹ åŠ¨æ€ã€‚åœ¨ DQN åŠå…¶å˜ä½“ä¸Šçš„å¹¿æ³›å®éªŒè¯æ˜ï¼ŒSCORER çš„æ€§èƒ½å¢ç›Šæºäºå…¶ç®—æ³•å±‚é¢çš„æ·±åˆ»æ´å¯Ÿè€Œéæ¨¡å‹å¤æ‚åº¦çš„æå‡ï¼Œæœ‰æ•ˆç¼“è§£äº†å€¼ä¼°è®¡ä¸­çš„åå·®ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.07452v2",
      "published_date": "2025-08-10 18:36:54 UTC",
      "updated_date": "2025-10-01 15:29:53 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T10:41:21.559909+00:00"
    },
    {
      "arxiv_id": "2508.09209v2",
      "title": "Quantum-Enhanced Generative Adversarial Networks: Comparative Analysis of Classical and Hybrid Quantum-Classical Generative Adversarial Networks",
      "title_zh": "é‡å­å¢å¼ºå‹ç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼šç»å…¸ä¸æ··åˆé‡å­-ç»å…¸ç”Ÿæˆå¯¹æŠ—ç½‘ç»œçš„å¯¹æ¯”åˆ†æ",
      "authors": [
        "Kun Ming Goh"
      ],
      "abstract": "Generative adversarial networks (GANs) have emerged as a powerful paradigm for producing high-fidelity data samples, yet their performance is constrained by the quality of latent representations, typically sampled from classical noise distributions. This study investigates hybrid quantum-classical GANs (HQCGANs) in which a quantum generator, implemented via parameterised quantum circuits, produces latent vectors for a classical discriminator. We evaluate a classical GAN alongside three HQCGAN variants with 3, 5, and 7 qubits, using Qiskit's AerSimulator with realistic noise models to emulate near-term quantum devices. The binary MNIST dataset (digits 0 and 1) is used to align with the low-dimensional latent spaces imposed by current quantum hardware. Models are trained for 150 epochs and assessed with Frechet Inception Distance (FID) and Kernel Inception Distance (KID). Results show that while the classical GAN achieved the best scores, the 7-qubit HQCGAN produced competitive performance, narrowing the gap in later epochs, whereas the 3-qubit model exhibited earlier convergence limitations. Efficiency analysis indicates only moderate training time increases despite quantum sampling overhead. These findings validate the feasibility of noisy quantum circuits as latent priors in GAN architectures, highlighting their potential to enhance generative modelling within the constraints of the noisy intermediate-scale quantum (NISQ) era.",
      "tldr_zh": "è¯¥ç ”ç©¶å¯¹ç»å…¸ç”Ÿæˆå¯¹æŠ—ç½‘ç»œ(GANs)ä¸æ··åˆé‡å­-ç»å…¸ç”Ÿæˆå¯¹æŠ—ç½‘ç»œ(HQCGANs)è¿›è¡Œäº†å¯¹æ¯”åˆ†æï¼Œæ¢è®¨äº†åˆ©ç”¨å‚æ•°åŒ–é‡å­ç”µè·¯(parameterised quantum circuits)ä½œä¸ºç”Ÿæˆå™¨äº§ç”Ÿæ½œåœ¨å‘é‡(latent vectors)çš„æ•ˆæœã€‚ä½œè€…åœ¨Qiskitçš„AerSimulatorä¸Šåˆ©ç”¨çœŸå®å™ªå£°æ¨¡å‹æ¨¡æ‹Ÿäº†3ã€5å’Œ7ä¸ªé‡å­ä½(qubits)çš„HQCGANå˜ä½“ï¼Œå¹¶åœ¨äºŒå…ƒMNISTæ•°æ®é›†ä¸Šè¿›è¡Œäº†150ä¸ªå‘¨æœŸçš„è®­ç»ƒã€‚é€šè¿‡Frechet Inception Distance (FID)å’ŒKernel Inception Distance (KID)æŒ‡æ ‡è¯„ä¼°å‘ç°ï¼Œè™½ç„¶ç»å…¸GANç›®å‰å¾—åˆ†æœ€é«˜ï¼Œä½†7é‡å­ä½çš„æ¨¡å‹è¡¨ç°å‡ºæå¼ºçš„ç«äº‰åŠ›ï¼Œå¹¶åœ¨è®­ç»ƒåæœŸæ˜¾è‘—ç¼©å°äº†æ€§èƒ½å·®è·ã€‚å®éªŒç»“æœè¯æ˜äº†åœ¨å˜ˆæ‚ä¸­ç­‰è§„æ¨¡é‡å­(NISQ)æ—¶ä»£ï¼Œå°†å™ªå£°é‡å­ç”µè·¯ä½œä¸ºç”Ÿæˆå¯¹æŠ—ç½‘ç»œæ¶æ„ä¸­æ½œåœ¨å…ˆéªŒ(latent priors)å…·æœ‰æ˜ç¡®çš„å¯è¡Œæ€§ã€‚è¿™ä¸€å‘ç°å¼ºè°ƒäº†é‡å­æŠ€æœ¯åœ¨å—é™ç¡¬ä»¶ç¯å¢ƒä¸‹å¢å¼ºç”Ÿæˆå»ºæ¨¡ä»»åŠ¡çš„æ½œåŠ›ï¼Œä¸ºé‡å­å¢å¼ºçš„ç”Ÿæˆæ¨¡å‹ç ”ç©¶æä¾›äº†é‡è¦å‚è€ƒã€‚",
      "categories": [
        "quant-ph",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "quant-ph",
      "comment": "9 pages, 9 figures, 3 tables",
      "pdf_url": "https://arxiv.org/pdf/2508.09209v2",
      "published_date": "2025-08-10 18:34:53 UTC",
      "updated_date": "2025-08-17 14:13:36 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T10:41:17.052012+00:00"
    },
    {
      "arxiv_id": "2508.07432v1",
      "title": "Freeze and Reveal: Exposing Modality Bias in Vision-Language Models",
      "title_zh": "å†»ç»“ä¸æ­ç¤ºï¼šæ­ç¤ºè§†è§‰-è¯­è¨€æ¨¡å‹ä¸­çš„æ¨¡æ€åè§",
      "authors": [
        "Vivek Hruday Kavuri",
        "Vysishtya Karanam",
        "Venkata Jahnavi Venkamsetty",
        "Kriti Madumadukala",
        "Lakshmipathi Balaji Darur",
        "Ponnurangam Kumaraguru"
      ],
      "abstract": "Vision Language Models achieve impressive multi-modal performance but often inherit gender biases from their training data. This bias might be coming from both the vision and text modalities. In this work, we dissect the contributions of vision and text backbones to these biases by applying targeted debiasing using Counterfactual Data Augmentation and Task Vector methods. Inspired by data-efficient approaches in hate-speech classification, we introduce a novel metric, Degree of Stereotypicality and a corresponding debiasing method, Data Augmentation Using Degree of Stereotypicality - DAUDoS, to reduce bias with minimal computational cost. We curate a gender annotated dataset and evaluate all methods on VisoGender benchmark to quantify improvements and identify dominant source of bias. Our results show that CDA reduces the gender gap by 6% and DAUDoS by 3% but using only one-third of the data. Both methods also improve the model's ability to correctly identify gender in images by 3%, with DAUDoS achieving this improvement using only almost one-third of training data. From our experiment's, we observed that CLIP's vision encoder is more biased whereas PaliGemma2's text encoder is more biased. By identifying whether bias stems more from vision or text encoders, our work enables more targeted and effective bias mitigation strategies in future multi-modal systems.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†è§†è§‰è¯­è¨€æ¨¡å‹(Vision Language Models)ä¸­å­˜åœ¨çš„æ€§åˆ«åè§é—®é¢˜ï¼Œæ—¨åœ¨åˆ†æè§†è§‰ä¸æ–‡æœ¬éª¨å¹²ç½‘ç»œå¯¹åè§çš„å…·ä½“è´¡çŒ®ã€‚ç ”ç©¶è€…é‡‡ç”¨äº†åäº‹å®æ•°æ®å¢å¼º(Counterfactual Data Augmentation)å’Œä»»åŠ¡å‘é‡(Task Vector)æ–¹æ³•è¿›è¡Œå®šå‘å»åï¼Œå¹¶æå‡ºäº†åä¸ºåˆ»æ¿å°è±¡ç¨‹åº¦(Degree of Stereotypicality)çš„æ–°åº¦é‡æŒ‡æ ‡åŠå…¶å¯¹åº”çš„å»åæ–¹æ³•DAUDoSã€‚é€šè¿‡åœ¨VisoGenderåŸºå‡†ä¸Šçš„å®éªŒï¼Œç»“æœæ˜¾ç¤ºCDAå°†æ€§åˆ«å·®è·é™ä½äº†6%ï¼Œè€ŒDAUDoSåœ¨ä»…ä½¿ç”¨çº¦ä¸‰åˆ†ä¹‹ä¸€æ•°æ®çš„æƒ…å†µä¸‹å®ç°äº†3%çš„é™å¹…ã€‚å®éªŒè§‚å¯Ÿåˆ°CLIPçš„è§†è§‰ç¼–ç å™¨(Vision Encoder)è¡¨ç°å‡ºæ›´å¼ºçš„åè§ï¼Œè€ŒPaliGemma2çš„åè§åˆ™ä¸»è¦æºäºå…¶æ–‡æœ¬ç¼–ç å™¨(Text Encoder)ã€‚è¯¥å·¥ä½œé€šè¿‡ç²¾ç¡®å®šä½åè§æ¥æºï¼Œä¸ºæœªæ¥å¤šæ¨¡æ€ç³»ç»Ÿæä¾›äº†æ›´å…·é’ˆå¯¹æ€§å’Œæ•ˆç‡çš„åè§ç¼“è§£ç­–ç•¥ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.07432v1",
      "published_date": "2025-08-10 17:08:10 UTC",
      "updated_date": "2025-08-10 17:08:10 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T10:41:22.849534+00:00"
    },
    {
      "arxiv_id": "2508.07428v1",
      "title": "Lightning Prediction under Uncertainty: DeepLight with Hazy Loss",
      "title_zh": "ä¸ç¡®å®šæ€§ä¸‹çš„é—ªç”µé¢„æµ‹ï¼šåŸºäº Hazy Loss çš„ DeepLight",
      "authors": [
        "Md Sultanul Arifin",
        "Abu Nowshed Sakib",
        "Yeasir Rayhan",
        "Tanzima Hashem"
      ],
      "abstract": "Lightning, a common feature of severe meteorological conditions, poses significant risks, from direct human injuries to substantial economic losses. These risks are further exacerbated by climate change. Early and accurate prediction of lightning would enable preventive measures to safeguard people, protect property, and minimize economic losses. In this paper, we present DeepLight, a novel deep learning architecture for predicting lightning occurrences. Existing prediction models face several critical limitations: they often struggle to capture the dynamic spatial context and inherent uncertainty of lightning events, underutilize key observational data, such as radar reflectivity and cloud properties, and rely heavily on Numerical Weather Prediction (NWP) systems, which are both computationally expensive and highly sensitive to parameter settings. To overcome these challenges, DeepLight leverages multi-source meteorological data, including radar reflectivity, cloud properties, and historical lightning occurrences through a dual-encoder architecture. By employing multi-branch convolution techniques, it dynamically captures spatial correlations across varying extents. Furthermore, its novel Hazy Loss function explicitly addresses the spatio-temporal uncertainty of lightning by penalizing deviations based on proximity to true events, enabling the model to better learn patterns amidst randomness. Extensive experiments show that DeepLight improves the Equitable Threat Score (ETS) by 18%-30% over state-of-the-art methods, establishing it as a robust solution for lightning prediction.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹é—ªç”µé¢„æµ‹ä¸­éš¾ä»¥æ•æ‰åŠ¨æ€ç©ºé—´èƒŒæ™¯ã€å¿½è§†å†…åœ¨ä¸ç¡®å®šæ€§ä»¥åŠè¿‡åº¦ä¾èµ–æ•°å€¼å¤©æ°”é¢„æŠ¥(NWP)ç³»ç»Ÿç­‰å±€é™ï¼Œæå‡ºäº†åä¸ºDeepLightçš„æ–°å‹æ·±åº¦å­¦ä¹ æ¶æ„ã€‚è¯¥æ¶æ„é‡‡ç”¨åŒç¼–ç å™¨(dual-encoder)ç»“æ„ï¼Œæœ‰æ•ˆæ•´åˆäº†é›·è¾¾åå°„ç‡(radar reflectivity)ã€äº‘å±‚å±æ€§(cloud properties)å’Œå†å²é—ªç”µæ•°æ®ç­‰å¤šæºæ°”è±¡ä¿¡æ¯ã€‚DeepLightåˆ©ç”¨å¤šåˆ†æ”¯å·ç§¯(multi-branch convolution)æŠ€æœ¯åŠ¨æ€æ•æ‰ä¸åŒå°ºåº¦çš„ç©ºé—´ç›¸å…³æ€§ï¼Œå¹¶å¼•å…¥åˆ›æ–°çš„Hazy Losså‡½æ•°ï¼Œé€šè¿‡åŸºäºäº‹ä»¶é‚»è¿‘åº¦çš„åå·®æƒ©ç½šæ˜¾å¼å¤„ç†é—ªç”µçš„æ—¶ç©ºä¸ç¡®å®šæ€§ã€‚å¹¿æ³›çš„å®éªŒç»“æœæ˜¾ç¤ºï¼ŒDeepLightåœ¨å…¬å¹³å¨èƒå¾—åˆ†(ETS)æŒ‡æ ‡ä¸Šæ¯”ç°æœ‰æœ€å…ˆè¿›æ–¹æ³•æå‡äº†18%-30%ï¼Œè¯æ˜äº†å…¶ä½œä¸ºç¨³å¥é—ªç”µé¢„æµ‹æ–¹æ¡ˆçš„æœ‰æ•ˆæ€§ï¼Œä¸ºå‡å°‘é—ªç”µå¼•å‘çš„ç»æµæŸå¤±å’Œå®‰å…¨é£é™©æä¾›äº†é‡è¦æ”¯æŒã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.07428v1",
      "published_date": "2025-08-10 16:59:03 UTC",
      "updated_date": "2025-08-10 16:59:03 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T10:41:31.547816+00:00"
    },
    {
      "arxiv_id": "2508.07423v3",
      "title": "Real-Time Analysis of Unstructured Data with Machine Learning on Heterogeneous Architectures",
      "title_zh": "å¼‚æ„æ¶æ„ä¸‹åŸºäºæœºå™¨å­¦ä¹ çš„éç»“æ„åŒ–æ•°æ®å®æ—¶åˆ†æ",
      "authors": [
        "Fotis I. Giasemis"
      ],
      "abstract": "As the particle physics community needs higher and higher precisions in order to test our current model of the subatomic world, larger and larger datasets are necessary. With upgrades scheduled for the detectors of colliding-beam experiments around the world, and specifically at the Large Hadron Collider at CERN, more collisions and more complex interactions are expected. This directly implies an increase in data produced and consequently in the computational resources needed to process them. At CERN, the amount of data produced is gargantuan. This is why the data have to be heavily filtered and selected in real time before being permanently stored. This data can then be used to perform physics analyses, in order to expand our current understanding of the universe and improve the Standard Model of physics. This real-time filtering, known as triggering, involves complex processing happening often at frequencies as high as 40 MHz. This thesis contributes to understanding how machine learning models can be efficiently deployed in such environments, in order to maximize throughput and minimize energy consumption. Inevitably, modern hardware designed for such tasks and contemporary algorithms are needed in order to meet the challenges posed by the stringent, high-frequency data rates. In this work, I present our graph neural network-based pipeline, developed for charged particle track reconstruction at the LHCb experiment at CERN. The pipeline was implemented end-to-end inside LHCb's first-level trigger, entirely on GPUs. Its performance was compared against the classical tracking algorithms currently in production at LHCb. The pipeline was also accelerated on the FPGA architecture, and its performance in terms of power consumption and processing speed was compared against the GPU implementation.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†åœ¨é«˜èƒ½ç‰©ç†å®éªŒæ•°æ®é‡æ¿€å¢çš„èƒŒæ™¯ä¸‹ï¼Œå¦‚ä½•åˆ©ç”¨æœºå™¨å­¦ä¹ åœ¨å¼‚æ„æ¶æ„(Heterogeneous Architectures)ä¸Šå®ç°éç»“æ„åŒ–æ•°æ®çš„å®æ—¶åˆ†æã€‚ç ”ç©¶é‡ç‚¹ä»‹ç»äº†ä¸ºCERNçš„LHCbå®éªŒå¼€å‘çš„ä¸€å¥—åŸºäºå›¾ç¥ç»ç½‘ç»œ(Graph Neural Network)çš„æµæ°´çº¿ï¼Œæ—¨åœ¨å®Œæˆç¬¬ä¸€çº§è§¦å‘å™¨(Triggering)ä¸­çš„å¸¦ç”µç²’å­è½¨è¿¹é‡å»º(Track Reconstruction)ä»»åŠ¡ã€‚è¯¥æ–¹æ¡ˆåœ¨GPUä¸Šå®ç°äº†ç«¯åˆ°ç«¯çš„éƒ¨ç½²ï¼Œå¹¶ä¸ç°æœ‰çš„ä¼ ç»Ÿè¿½è¸ªç®—æ³•è¿›è¡Œäº†å¯¹æ¯”åˆ†æã€‚ä¸ºäº†è¿›ä¸€æ­¥ä¼˜åŒ–èƒ½æ•ˆï¼Œç ”ç©¶è¿˜å°†è¯¥æµæ°´çº¿æ‰©å±•è‡³FPGAæ¶æ„ï¼Œè¯¦ç»†è¯„ä¼°äº†å…¶åœ¨åŠŸè€—å’Œå¤„ç†é€Ÿåº¦æ–¹é¢ç›¸è¾ƒäºGPUçš„æ€§èƒ½è¡¨ç°ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æœºå™¨å­¦ä¹ æµæ°´çº¿èƒ½å¤Ÿæœ‰æ•ˆæ»¡è¶³é«˜é¢‘æ•°æ®ç¯å¢ƒä¸‹çš„é«˜ååé‡éœ€æ±‚ï¼Œä¸ºæœªæ¥ç²’å­ç‰©ç†å®éªŒä¸­çš„å®æ—¶æ•°æ®å¤„ç†æä¾›äº†æŠ€æœ¯è·¯å¾„ã€‚",
      "categories": [
        "physics.data-an",
        "cs.AI",
        "cs.DC",
        "cs.LG",
        "hep-ex"
      ],
      "primary_category": "physics.data-an",
      "comment": "PhD thesis, Chapters 8 and 9 include results from work performed in collaboration with Anthony Correia",
      "pdf_url": "https://arxiv.org/pdf/2508.07423v3",
      "published_date": "2025-08-10 16:45:10 UTC",
      "updated_date": "2025-09-06 20:47:36 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T10:41:27.160796+00:00"
    },
    {
      "arxiv_id": "2508.07410v3",
      "title": "Leveraging GNN to Enhance MEF Method in Predicting ENSO",
      "title_zh": "åˆ©ç”¨ GNN å¢å¼º MEF æ–¹æ³•åœ¨ ENSO é¢„æµ‹ä¸­çš„è¡¨ç°",
      "authors": [
        "Saghar Ganji",
        "Ahmad Reza Labibzadeh",
        "Alireza Hassani",
        "Mohammad Naisipour"
      ],
      "abstract": "Reliable long-lead forecasting of the El Nino Southern Oscillation (ENSO) remains a long-standing challenge in climate science. The previously developed Multimodal ENSO Forecast (MEF) model uses 80 ensemble predictions by two independent deep learning modules: a 3D Convolutional Neural Network (3D-CNN) and a time-series module. In their approach, outputs of the two modules are combined using a weighting strategy wherein one is prioritized over the other as a function of global performance. Separate weighting or testing of individual ensemble members did not occur, however, which may have limited the model to optimize the use of high-performing but spread-out forecasts. In this study, we propose a better framework that employs graph-based analysis to directly model similarity between all 80 members of the ensemble. By constructing an undirected graph whose vertices are ensemble outputs and whose weights on edges measure similarity (via RMSE and correlation), we identify and cluster structurally similar and accurate predictions. From which we obtain an optimized subset of 20 members using community detection methods. The final prediction is then obtained by averaging this optimized subset. This method improves the forecast skill through noise removal and emphasis on ensemble coherence. Interestingly, our graph-based selection shows robust statistical characteristics among top performers, offering new ensemble behavior insights. In addition, we observe that while the GNN-based approach does not always outperform the baseline MEF under every scenario, it produces more stable and consistent outputs, particularly in compound long-lead situations. The approach is model-agnostic too, suggesting that it can be applied directly to other forecasting models with gargantuan ensemble outputs, such as statistical, physical, or hybrid models.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å„å°”å°¼è¯º-å—æ–¹æ¶›åŠ¨ï¼ˆENSOï¼‰é•¿æœŸé¢„æµ‹çš„æŒ‘æˆ˜ï¼Œæå‡ºäº†ä¸€ç§åˆ©ç”¨å›¾åˆ†ææŠ€æœ¯å¢å¼ºå¤šæ¨¡æ€ENSOé¢„æµ‹ï¼ˆMEFï¼‰æ–¹æ³•çš„æ–°æ¡†æ¶ã€‚é’ˆå¯¹åŸMEFæ¨¡å‹ä¸­80ä¸ªé›†åˆé¢„æŠ¥(ensemble predictions)æˆå‘˜åˆ©ç”¨æ•ˆç‡ä¸è¶³çš„é—®é¢˜ï¼Œè¯¥æ–¹æ³•é€šè¿‡æ„å»ºæ— å‘å›¾ç›´æ¥å»ºæ¨¡æˆå‘˜é—´çš„ç›¸ä¼¼æ€§ï¼Œå¹¶åˆ©ç”¨RMSEå’Œç›¸å…³æ€§å®šä¹‰è¾¹æƒé‡ã€‚ç ”ç©¶é‡‡ç”¨ç¤¾ç¾¤æ£€æµ‹(community detection)æŠ€æœ¯ä»é¢„æŠ¥å…¨é›†ä¸­ç­›é€‰å‡º20ä¸ªç»“æ„ç›¸ä¼¼ä¸”å‡†ç¡®ç‡æœ€é«˜çš„æœ€ä¼˜å­é›†ï¼Œé€šè¿‡å–å…¶å¹³å‡å€¼ä½œä¸ºæœ€ç»ˆé¢„æµ‹ç»“æœã€‚è¿™ç§æ–¹æ³•é€šè¿‡æ¶ˆé™¤å™ªå£°å¹¶å¢å¼ºé›†åˆè¿è´¯æ€§(ensemble coherence)ï¼Œæœ‰æ•ˆæå‡äº†é¢„æŠ¥æŠ€å·§ã€‚å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å¤åˆé•¿æœŸé¢„æµ‹æƒ…å¢ƒä¸‹æ¯”åŸºçº¿æ¨¡å‹è¡¨ç°å‡ºæ›´é«˜çš„ç¨³å®šæ€§å’Œä¸€è‡´æ€§ã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ¡ˆå…·æœ‰æ¨¡å‹æ— å…³æ€§(model-agnostic)ï¼Œå¯å¹¿æ³›åº”ç”¨äºå…¶ä»–åŒ…å«å¤§è§„æ¨¡é›†åˆè¾“å‡ºçš„ç»Ÿè®¡ã€ç‰©ç†æˆ–æ··åˆåŠ¨åŠ›å­¦é¢„æŠ¥æ¨¡å‹ã€‚",
      "categories": [
        "physics.ao-ph",
        "cs.AI"
      ],
      "primary_category": "physics.ao-ph",
      "comment": "17 pages, 4 figures, 2 tables",
      "pdf_url": "https://arxiv.org/pdf/2508.07410v3",
      "published_date": "2025-08-10 16:16:58 UTC",
      "updated_date": "2025-08-26 08:09:57 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T10:41:57.564664+00:00"
    },
    {
      "arxiv_id": "2508.07407v2",
      "title": "A Comprehensive Survey of Self-Evolving AI Agents: A New Paradigm Bridging Foundation Models and Lifelong Agentic Systems",
      "title_zh": "è‡ªè¿›åŒ– AI æ™ºèƒ½ä½“å…¨æ™¯ç»¼è¿°ï¼šè¡”æ¥åŸºç¡€æ¨¡å‹ä¸ç»ˆèº«æ™ºèƒ½ä½“ç³»ç»Ÿçš„æ–°èŒƒå¼",
      "authors": [
        "Jinyuan Fang",
        "Yanwen Peng",
        "Xi Zhang",
        "Yingxu Wang",
        "Xinhao Yi",
        "Guibin Zhang",
        "Yi Xu",
        "Bin Wu",
        "Siwei Liu",
        "Zihao Li",
        "Zhaochun Ren",
        "Nikos Aletras",
        "Xi Wang",
        "Han Zhou",
        "Zaiqiao Meng"
      ],
      "abstract": "Recent advances in large language models have sparked growing interest in AI agents capable of solving complex, real-world tasks. However, most existing agent systems rely on manually crafted configurations that remain static after deployment, limiting their ability to adapt to dynamic and evolving environments. To this end, recent research has explored agent evolution techniques that aim to automatically enhance agent systems based on interaction data and environmental feedback. This emerging direction lays the foundation for self-evolving AI agents, which bridge the static capabilities of foundation models with the continuous adaptability required by lifelong agentic systems. In this survey, we provide a comprehensive review of existing techniques for self-evolving agentic systems. Specifically, we first introduce a unified conceptual framework that abstracts the feedback loop underlying the design of self-evolving agentic systems. The framework highlights four key components: System Inputs, Agent System, Environment, and Optimisers, serving as a foundation for understanding and comparing different strategies. Based on this framework, we systematically review a wide range of self-evolving techniques that target different components of the agent system. We also investigate domain-specific evolution strategies developed for specialised fields such as biomedicine, programming, and finance, where optimisation objectives are tightly coupled with domain constraints. In addition, we provide a dedicated discussion on the evaluation, safety, and ethical considerations for self-evolving agentic systems, which are critical to ensuring their effectiveness and reliability. This survey aims to provide researchers and practitioners with a systematic understanding of self-evolving AI agents, laying the foundation for the development of more adaptive, autonomous, and lifelong agentic systems.",
      "tldr_zh": "è¯¥ç ”ç©¶å¯¹è‡ªæˆ‘æ¼”åŒ–AIæ™ºèƒ½ä½“(Self-Evolving AI Agents)è¿›è¡Œäº†å…¨é¢ç»¼è¿°ï¼Œæ—¨åœ¨å¼¥åˆåŸºç¡€æ¨¡å‹(Foundation Models)çš„é™æ€èƒ½åŠ›ä¸ç»ˆèº«æ™ºèƒ½ä½“ç³»ç»Ÿ(Lifelong Agentic Systems)æ‰€éœ€çš„æŒç»­é€‚åº”æ€§ä¹‹é—´çš„å·®è·ã€‚æ–‡ç« é’ˆå¯¹ç°æœ‰ç³»ç»Ÿä¾èµ–æ‰‹åŠ¨é…ç½®ä¸”éƒ¨ç½²åä¿æŒé™æ€ã€éš¾ä»¥é€‚åº”åŠ¨æ€ç¯å¢ƒçš„å±€é™ï¼Œæå‡ºäº†ä¸€ä¸ªåŒ…å«ç³»ç»Ÿè¾“å…¥(System Inputs)ã€æ™ºèƒ½ä½“ç³»ç»Ÿ(Agent System)ã€ç¯å¢ƒ(Environment)å’Œä¼˜åŒ–å™¨(Optimisers)çš„ç»Ÿä¸€æ¦‚å¿µæ¡†æ¶ï¼Œç”¨ä»¥æŠ½è±¡è‡ªæˆ‘æ¼”åŒ–è¿‡ç¨‹ä¸­çš„åé¦ˆå›è·¯ã€‚åŸºäºè¯¥æ¡†æ¶ï¼Œç ”ç©¶ç³»ç»Ÿæ€§åœ°å›é¡¾äº†é’ˆå¯¹æ™ºèƒ½ä½“ä¸åŒç»„ä»¶çš„æ¼”åŒ–æŠ€æœ¯ï¼Œå¹¶æ¢è®¨äº†ç”Ÿç‰©åŒ»å­¦ã€ç¼–ç¨‹å’Œé‡‘èç­‰ç‰¹å®šé¢†åŸŸçš„æ¼”åŒ–ç­–ç•¥ã€‚æ­¤å¤–ï¼Œè®ºæ–‡è¿˜ä¸“é—¨è®¨è®ºäº†æ­¤ç±»ç³»ç»Ÿçš„è¯„ä¼°ã€å®‰å…¨å’Œä¼¦ç†è€ƒé‡ï¼Œä»¥ç¡®ä¿å…¶åœ¨å®é™…åº”ç”¨ä¸­çš„æœ‰æ•ˆæ€§ä¸å¯é æ€§ã€‚è¯¥ç»¼è¿°ä¸ºå¼€å‘æ›´å…·é€‚åº”æ€§å’Œè‡ªä¸»æ€§çš„ç»ˆèº«æ™ºèƒ½ä½“ç³»ç»Ÿå¥ å®šäº†ç³»ç»Ÿæ€§çš„ç†è®ºåŸºç¡€ã€‚",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.MA"
      ],
      "primary_category": "cs.AI",
      "comment": "Github Repo: https://github.com/EvoAgentX/Awesome-Self-Evolving-Agents",
      "pdf_url": "https://arxiv.org/pdf/2508.07407v2",
      "published_date": "2025-08-10 16:07:32 UTC",
      "updated_date": "2025-08-31 14:55:05 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T10:41:33.657000+00:00"
    },
    {
      "arxiv_id": "2508.07406v1",
      "title": "AgriVLN: Vision-and-Language Navigation for Agricultural Robots",
      "title_zh": "AgriVLNï¼šé¢å‘å†œä¸šæœºå™¨äººçš„è§†è§‰è¯­è¨€å¯¼èˆª",
      "authors": [
        "Xiaobei Zhao",
        "Xingqi Lyu",
        "Xiang Li"
      ],
      "abstract": "Agricultural robots have emerged as powerful members in agricultural tasks, nevertheless, still heavily rely on manual operation or untransportable railway for movement, resulting in limited mobility and poor adaptability. Vision-and-Language Navigation (VLN) enables robots to navigate to the target destinations following natural language instructions, demonstrating strong performance on several domains. However, none of the existing benchmarks or methods is specifically designed for agricultural scenes. To bridge this gap, we propose Agriculture to Agriculture (A2A) benchmark, containing 1,560 episodes across six diverse agricultural scenes, in which all realistic RGB videos are captured by front-facing camera on a quadruped robot at a height of 0.38 meters, aligning with the practical deployment conditions. Meanwhile, we propose Vision-and-Language Navigation for Agricultural Robots (AgriVLN) baseline based on Vision-Language Model (VLM) prompted with carefully crafted templates, which can understand both given instructions and agricultural environments to generate appropriate low-level actions for robot control. When evaluated on A2A, AgriVLN performs well on short instructions but struggles with long instructions, because it often fails to track which part of the instruction is currently being executed. To address this, we further propose Subtask List (STL) instruction decomposition module and integrate it into AgriVLN, improving Success Rate (SR) from 0.33 to 0.47. We additionally compare AgriVLN with several existing VLN methods, demonstrating the state-of-the-art performance in the agricultural domain.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å†œä¸šæœºå™¨äººç›®å‰è¿‡åº¦ä¾èµ–æ‰‹åŠ¨æ“ä½œæˆ–å›ºå®šè½¨é“å¯¼è‡´ç§»åŠ¨æ€§å—é™çš„é—®é¢˜ï¼Œæå‡ºäº† Agriculture to Agriculture (A2A) åŸºå‡†æµ‹è¯•é›†ï¼Œå¡«è¡¥äº†å†œä¸šåœºæ™¯è§†è§‰è¯­è¨€å¯¼èˆª (VLN) é¢†åŸŸçš„ç©ºç™½ã€‚è¯¥åŸºå‡†åŒ…å«å…­ä¸ªå¤šå…ƒåŒ–å†œä¸šåœºæ™¯ä¸­çš„1,560ä¸ªç‰‡æ®µï¼Œå…¶æ‰€æœ‰çœŸå® RGB è§†é¢‘å‡ç”±å››è¶³æœºå™¨äººåœ¨0.38ç±³é«˜åº¦é‡‡é›†ï¼Œä»¥ç¡®ä¿æ•°æ®ç¬¦åˆå®é™…éƒ¨ç½²ç¯å¢ƒã€‚ç ”ç©¶äººå‘˜åŒæ­¥æ¨å‡ºäº† AgriVLN åŸºçº¿æ¨¡å‹ï¼Œåˆ©ç”¨è§†è§‰è¯­è¨€æ¨¡å‹ (VLM) ç»“åˆç²¾å¿ƒè®¾è®¡çš„æ¨¡æ¿æ¥è§£ææŒ‡ä»¤å¹¶ç”Ÿæˆåº•å±‚æœºå™¨äººæ§åˆ¶åŠ¨ä½œã€‚é’ˆå¯¹æ¨¡å‹åœ¨å¤„ç†å¤æ‚é•¿æŒ‡ä»¤æ—¶å®¹æ˜“ä¸¢å¤±è¿›åº¦çš„é—®é¢˜ï¼Œç ”ç©¶è¿›ä¸€æ­¥å¼•å…¥äº†å­ä»»åŠ¡åˆ—è¡¨ (STL) æŒ‡ä»¤åˆ†è§£æ¨¡å—ï¼Œå°†å¯¼èˆªæˆåŠŸç‡ (Success Rate, SR) ä»0.33æ˜¾è‘—æå‡è‡³0.47ã€‚å®éªŒå¯¹æ¯”è¯æ˜ AgriVLN åœ¨å†œä¸šé¢†åŸŸè¾¾åˆ°äº†å½“å‰æœ€ä¼˜æ°´å¹³ (SOTA)ï¼Œä¸ºæå‡å†œä¸šæœºå™¨äººçš„ç¯å¢ƒé€‚åº”æ€§å’Œè‡ªä¸»å¯¼èˆªèƒ½åŠ›æä¾›äº†é‡è¦çš„æ–¹æ³•è®ºå‚è€ƒã€‚",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.RO",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.07406v1",
      "published_date": "2025-08-10 16:07:23 UTC",
      "updated_date": "2025-08-10 16:07:23 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T10:41:38.759130+00:00"
    },
    {
      "arxiv_id": "2508.07405v1",
      "title": "Generative AI for Strategic Plan Development",
      "title_zh": "ç”Ÿæˆå¼äººå·¥æ™ºèƒ½åœ¨æˆ˜ç•¥è§„åˆ’åˆ¶å®šä¸­çš„åº”ç”¨",
      "authors": [
        "Jesse Ponnock"
      ],
      "abstract": "Given recent breakthroughs in Generative Artificial Intelligence (GAI) and Large Language Models (LLMs), more and more professional services are being augmented through Artificial Intelligence (AI), which once seemed impossible to automate. This paper presents a modular model for leveraging GAI in developing strategic plans for large scale government organizations and evaluates leading machine learning techniques in their application towards one of the identified modules. Specifically, the performance of BERTopic and Non-negative Matrix Factorization (NMF) are evaluated in their ability to use topic modeling to generate themes representative of Vision Elements within a strategic plan. To accomplish this, BERTopic and NMF models are trained using a large volume of reports from the Government Accountability Office (GAO). The generated topics from each model are then scored for similarity against the Vision Elements of a published strategic plan and the results are compared. Our results show that these techniques are capable of generating themes similar to 100% of the elements being evaluated against. Further, we conclude that BERTopic performs best in this application with more than half of its correlated topics achieving a \"medium\" or \"strong\" correlation. A capability of GAI-enabled strategic plan development impacts a multi-billion dollar industry and assists the federal government in overcoming regulatory requirements which are crucial to the public good. Further work will focus on the operationalization of the concept proven in this study as well as viability of the remaining modules in the proposed model for GAI-generated strategic plans.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†ç”Ÿæˆå¼äººå·¥æ™ºèƒ½ (Generative AI) å’Œå¤§è¯­è¨€æ¨¡å‹ (LLMs) åœ¨å¤§å‹æ”¿åºœæœºæ„æˆ˜ç•¥è§„åˆ’åˆ¶å®šä¸­çš„åº”ç”¨ï¼Œå¹¶æå‡ºäº†ä¸€ä¸ªæ¨¡å—åŒ–æ¨¡å‹ã€‚ç ”ç©¶é‡ç‚¹è¯„ä¼°äº† BERTopic å’Œ éè´ŸçŸ©é˜µåˆ†è§£ (Non-negative Matrix Factorization, NMF) åœ¨ä¸»é¢˜å»ºæ¨¡ (topic modeling) æ–¹é¢çš„è¡¨ç°ï¼Œæ—¨åœ¨ç”Ÿæˆä»£è¡¨æˆ˜ç•¥è§„åˆ’ä¸­æ„¿æ™¯è¦ç´  (Vision Elements) çš„æ ¸å¿ƒä¸»é¢˜ã€‚é€šè¿‡åˆ©ç”¨å¤§é‡çš„æ”¿åºœé—®è´£å±€ (GAO) æŠ¥å‘Šè¿›è¡Œæ¨¡å‹è®­ç»ƒï¼Œç ”ç©¶äººå‘˜å°†ç”Ÿæˆçš„è¯¾é¢˜ä¸æ—¢æœ‰æˆ˜ç•¥è§„åˆ’è¿›è¡Œäº†ç›¸ä¼¼æ€§æ¯”å¯¹ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¿™äº›æŠ€æœ¯èƒ½å¤Ÿç”Ÿæˆä¸ 100% è¯„ä¼°è¦ç´ ç›¸ä¼¼çš„ä¸»é¢˜ï¼Œå…¶ä¸­ BERTopic è¡¨ç°æœ€ä¸ºå‡ºè‰²ï¼ŒåŠæ•°ä»¥ä¸Šå…³è”è¯¾é¢˜è¾¾åˆ°äº†â€œä¸­ç­‰â€æˆ–â€œå¼ºâ€ç›¸å…³æ€§ã€‚è¿™é¡¹åŸºäº Generative AI çš„æˆ˜ç•¥è§„åˆ’å¼€å‘èƒ½åŠ›å°†å½±å“ä»·å€¼æ•°åäº¿ç¾å…ƒçš„äº§ä¸šï¼Œå¹¶ååŠ©è”é‚¦æ”¿åºœé«˜æ•ˆå±¥è¡Œå¯¹å…¬ä¼—åˆ©ç›Šè‡³å…³é‡è¦çš„ç›‘ç®¡èŒè´£ã€‚æœªæ¥å·¥ä½œå°†é›†ä¸­äºè¯¥æ¦‚å¿µçš„ä¸šåŠ¡åŒ–è½åœ°ä»¥åŠæ¨¡å‹ä¸­å…¶ä»–æ¨¡å—çš„å¯è¡Œæ€§éªŒè¯ã€‚",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "11 pages, 9 figures",
      "pdf_url": "https://arxiv.org/pdf/2508.07405v1",
      "published_date": "2025-08-10 16:07:07 UTC",
      "updated_date": "2025-08-10 16:07:07 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T10:41:43.857260+00:00"
    },
    {
      "arxiv_id": "2508.07397v1",
      "title": "A Spin Glass Characterization of Neural Networks",
      "title_zh": "ç¥ç»ç½‘ç»œçš„è‡ªæ—‹ç»ç’ƒè¡¨å¾",
      "authors": [
        "Jun Li"
      ],
      "abstract": "This work presents a statistical mechanics characterization of neural networks, motivated by the replica symmetry breaking (RSB) phenomenon in spin glasses. A Hopfield-type spin glass model is constructed from a given feedforward neural network (FNN). Overlaps between simulated replica samples serve as a characteristic descriptor of the FNN. The connection between the spin-glass description and commonly studied properties of the FNN -- such as data fitting, capacity, generalization, and robustness -- has been investigated and empirically demonstrated. Unlike prior analytical studies that focus on model ensembles, this method provides a computable descriptor for individual network instances, which reveals nontrivial structural properties that are not captured by conventional metrics such as loss or accuracy. Preliminary results suggests its potential for practical applications such as model inspection, safety verification, and detection of hidden vulnerabilities.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§åŸºäºç»Ÿè®¡åŠ›å­¦çš„ç¥ç»ç½‘ç»œè¡¨å¾æ–¹æ³•ï¼Œå…¶çµæ„Ÿæºè‡ªè‡ªæ—‹ç»ç’ƒ (spin glasses) ä¸­çš„å‰¯æœ¬å¯¹ç§°ç ´ç¼º (replica symmetry breaking, RSB) ç°è±¡ã€‚ä½œè€…ä»ç»™å®šçš„å‰é¦ˆç¥ç»ç½‘ç»œ (FNN) å‡ºå‘æ„å»ºäº†ä¸€ä¸ª Hopfield å‹è‡ªæ—‹ç»ç’ƒæ¨¡å‹ï¼Œå¹¶åˆ©ç”¨æ¨¡æ‹Ÿå‰¯æœ¬æ ·æœ¬ä¹‹é—´çš„é‡å  (overlaps) ä½œä¸ºè¯¥ç½‘ç»œçš„ç‰¹å¾æè¿°ç¬¦ã€‚è¯¥å·¥ä½œé€šè¿‡å®éªŒéªŒè¯äº†è¿™ç§è‡ªæ—‹ç»ç’ƒæè¿°ä¸ FNN çš„æ•°æ®æ‹Ÿåˆã€å®¹é‡ã€æ³›åŒ–èƒ½åŠ›åŠé²æ£’æ€§ç­‰æ ¸å¿ƒå±æ€§ä¹‹é—´çš„å…³è”ã€‚ä¸ä»¥å¾€å…³æ³¨æ¨¡å‹ç³»ç»¼ (model ensembles) çš„åˆ†æç ”ç©¶ä¸åŒï¼Œè¯¥æ–¹æ³•ä¸ºå•ä¸ªç½‘ç»œå®ä¾‹æä¾›äº†å¯è®¡ç®—çš„æè¿°ç¬¦ï¼Œèƒ½å¤Ÿæ­ç¤ºæŸå¤±å‡½æ•°æˆ–å‡†ç¡®ç‡ç­‰å¸¸è§„æŒ‡æ ‡éš¾ä»¥æ•æ‰çš„éå¹³å‡¡ç»“æ„ç‰¹æ€§ã€‚åˆæ­¥ç ”ç©¶ç»“æœæ˜¾ç¤ºï¼Œè¯¥æ–¹æ³•åœ¨æ¨¡å‹æ£€æŸ¥ã€å®‰å…¨æ€§éªŒè¯å’Œéšè—æ¼æ´æ£€æµ‹ç­‰å®é™…åº”ç”¨åœºæ™¯ä¸­å±•ç°å‡ºå·¨å¤§æ½œåŠ›ã€‚",
      "categories": [
        "cond-mat.dis-nn",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cond-mat.dis-nn",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.07397v1",
      "published_date": "2025-08-10 15:53:58 UTC",
      "updated_date": "2025-08-10 15:53:58 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T10:42:14.286421+00:00"
    },
    {
      "arxiv_id": "2508.07390v1",
      "title": "Urbanite: A Dataflow-Based Framework for Human-AI Interactive Alignment in Urban Visual Analytics",
      "title_zh": "Urbaniteï¼šé¢å‘åŸå¸‚å¯è§†åŒ–åˆ†æçš„äººæœºäº¤äº’å¯¹é½æ•°æ®æµæ¡†æ¶",
      "authors": [
        "Gustavo Moreira",
        "Leonardo Ferreira",
        "Carolina Veiga",
        "Maryam Hosseini",
        "Fabio Miranda"
      ],
      "abstract": "With the growing availability of urban data and the increasing complexity of societal challenges, visual analytics has become essential for deriving insights into pressing real-world problems. However, analyzing such data is inherently complex and iterative, requiring expertise across multiple domains. The need to manage diverse datasets, distill intricate workflows, and integrate various analytical methods presents a high barrier to entry, especially for researchers and urban experts who lack proficiency in data management, machine learning, and visualization. Advancements in large language models offer a promising solution to lower the barriers to the construction of analytics systems by enabling users to specify intent rather than define precise computational operations. However, this shift from explicit operations to intent-based interaction introduces challenges in ensuring alignment throughout the design and development process. Without proper mechanisms, gaps can emerge between user intent, system behavior, and analytical outcomes. To address these challenges, we propose Urbanite, a framework for human-AI collaboration in urban visual analytics. Urbanite leverages a dataflow-based model that allows users to specify intent at multiple scopes, enabling interactive alignment across the specification, process, and evaluation stages of urban analytics. Based on findings from a survey to uncover challenges, Urbanite incorporates features to facilitate explainability, multi-resolution definition of tasks across dataflows, nodes, and parameters, while supporting the provenance of interactions. We demonstrate Urbanite's effectiveness through usage scenarios created in collaboration with urban experts. Urbanite is available at https://urbantk.org/urbanite.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† Urbaniteï¼Œä¸€ä¸ªæ—¨åœ¨å®ç°åŸå¸‚å¯è§†åŒ–åˆ†æ(Urban Visual Analytics)ä¸­äººæœºäº¤äº’å¯¹é½(Human-AI Interactive Alignment)çš„æ•°æ®æµ(Dataflow)æ¡†æ¶ã€‚é’ˆå¯¹åŸå¸‚æ•°æ®åˆ†æå¤æ‚æ€§é«˜ã€è·¨é¢†åŸŸä¸“ä¸šéœ€æ±‚å¤§ä»¥åŠå¤§è¯­è¨€æ¨¡å‹(LLMs)åœ¨åŸºäºæ„å›¾äº¤äº’ä¸­å­˜åœ¨çš„å¯¹é½æŒ‘æˆ˜ï¼Œè¯¥æ¡†æ¶æä¾›äº†ä¸€ç§é«˜æ•ˆçš„åä½œè§£å†³æ–¹æ¡ˆã€‚Urbanite é‡‡ç”¨åŸºäºæ•°æ®æµçš„æ¨¡å‹ï¼Œå…è®¸ç”¨æˆ·åœ¨å¤šä¸ªç»´åº¦æŒ‡å®šæ„å›¾ï¼Œä»è€Œåœ¨è§„èŒƒã€è¿‡ç¨‹å’Œè¯„ä¼°é˜¶æ®µå®ç°äº¤äº’å¼å¯¹é½ã€‚æ­¤å¤–ï¼Œè¯¥æ¡†æ¶é›†æˆäº†å¯è§£é‡Šæ€§(Explainability)ã€æ¶µç›–æ•°æ®æµã€èŠ‚ç‚¹å’Œå‚æ•°çš„å¤šåˆ†è¾¨ç‡ä»»åŠ¡å®šä¹‰ï¼Œå¹¶æ”¯æŒäº¤äº’æº¯æº(Provenance)ã€‚é€šè¿‡ä¸åŸå¸‚ä¸“å®¶åˆä½œåˆ›å»ºçš„åº”ç”¨åœºæ™¯ï¼Œç ”ç©¶å›¢é˜ŸéªŒè¯äº† Urbanite åœ¨é™ä½åˆ†æç³»ç»Ÿæ„å»ºé—¨æ§›ã€å¼¥åˆç”¨æˆ·æ„å›¾ä¸ç³»ç»Ÿè¡Œä¸ºå·®è·æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚",
      "categories": [
        "cs.HC",
        "cs.AI"
      ],
      "primary_category": "cs.HC",
      "comment": "Accepted at IEEE VIS 2025. Urbanite is available at https://urbantk.org/urbanite",
      "pdf_url": "https://arxiv.org/pdf/2508.07390v1",
      "published_date": "2025-08-10 15:44:37 UTC",
      "updated_date": "2025-08-10 15:44:37 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T10:42:19.599020+00:00"
    },
    {
      "arxiv_id": "2508.07388v1",
      "title": "Invert4TVG: A Temporal Video Grounding Framework with Inversion Tasks for Enhanced Action Understanding",
      "title_zh": "Invert4TVGï¼šé€šè¿‡åè½¬ä»»åŠ¡å¢å¼ºåŠ¨ä½œç†è§£çš„æ—¶åºè§†é¢‘å®šä½æ¡†æ¶",
      "authors": [
        "Zhaoyu Chen",
        "Hongnan Lin",
        "Yongwei Nie",
        "Fei Ma",
        "Xuemiao Xu",
        "Fei Yu",
        "Chengjiang Long"
      ],
      "abstract": "Temporal Video Grounding (TVG) seeks to localize video segments matching a given textual query. Current methods, while optimizing for high temporal Intersection-over-Union (IoU), often overfit to this metric, compromising semantic action understanding in the video and query, a critical factor for robust TVG. To address this, we introduce Inversion Tasks for TVG (Invert4TVG), a novel framework that enhances both localization accuracy and action understanding without additional data. Our approach leverages three inversion tasks derived from existing TVG annotations: (1) Verb Completion, predicting masked action verbs in queries from video segments; (2) Action Recognition, identifying query-described actions; and (3) Video Description, generating descriptions of video segments that explicitly embed query-relevant actions. These tasks, integrated with TVG via a reinforcement learning framework with well-designed reward functions, ensure balanced optimization of localization and semantics. Experiments show our method outperforms state-of-the-art approaches, achieving a 7.1\\% improvement in R1@0.7 on Charades-STA for a 3B model compared to Time-R1. By inverting TVG to derive query-related actions from segments, our approach strengthens semantic understanding, significantly raising the ceiling of localization accuracy.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† Invert4TVGï¼Œè¿™æ˜¯ä¸€ç§æ—¨åœ¨å¢å¼ºåŠ¨ä½œç†è§£çš„æ—¶åºè§†é¢‘å®šä½ (Temporal Video Grounding) æ¡†æ¶ã€‚é’ˆå¯¹ç°æœ‰æ–¹æ³•è¿‡åº¦æ‹Ÿåˆæ—¶åºäº¤å¹¶æ¯” (Intersection-over-Union) è€Œå¿½è§†è§†é¢‘ä¸æ–‡æœ¬æŸ¥è¯¢ä¸­è¯­ä¹‰åŠ¨ä½œç†è§£çš„é—®é¢˜ï¼Œè¯¥æ¡†æ¶åœ¨ä¸å¢åŠ é¢å¤–æ•°æ®çš„æƒ…å†µä¸‹å¼•å…¥äº†ä¸‰é¡¹åŸºäºç°æœ‰æ ‡æ³¨çš„åè½¬ä»»åŠ¡ã€‚å…·ä½“åŒ…æ‹¬ä»è§†é¢‘ç‰‡æ®µé¢„æµ‹æŸ¥è¯¢ä¸­æ©ç åŠ¨è¯çš„ Verb Completionã€è¯†åˆ«æŸ¥è¯¢æ‰€è¿°åŠ¨ä½œçš„ Action Recognitionï¼Œä»¥åŠç”ŸæˆåŒ…å«æŸ¥è¯¢ç›¸å…³åŠ¨ä½œæè¿°çš„ Video Descriptionã€‚è¿™äº›ä»»åŠ¡é€šè¿‡å¸¦æœ‰ç²¾å¿ƒè®¾è®¡å¥–åŠ±å‡½æ•°çš„å¼ºåŒ–å­¦ä¹  (Reinforcement Learning) æ¡†æ¶ä¸ä¸»ä»»åŠ¡æ•´åˆï¼Œç¡®ä¿äº†å®šä½ç²¾åº¦ä¸è¯­ä¹‰ä¼˜åŒ–çš„å¹³è¡¡ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒInvert4TVG åœ¨ Charades-STA æ•°æ®é›†ä¸Šæ˜¾è‘—ä¼˜äºç°æœ‰æœ€å…ˆè¿›æ–¹æ³•ï¼Œå…¶ 3B æ¨¡å‹åœ¨ R1@0.7 æŒ‡æ ‡ä¸Šè¾ƒ Time-R1 æå‡äº† 7.1%ã€‚é€šè¿‡å°† TVG ä»»åŠ¡åè½¬ä»¥å¼ºåŒ–å¯¹è§†é¢‘ç‰‡æ®µä¸­åŠ¨ä½œè¯­ä¹‰çš„ç†è§£ï¼Œè¯¥æ–¹æ³•æœ‰æ•ˆçªç ´äº†å®šä½å‡†ç¡®æ€§çš„ç“¶é¢ˆã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.07388v1",
      "published_date": "2025-08-10 15:38:04 UTC",
      "updated_date": "2025-08-10 15:38:04 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T10:42:42.685633+00:00"
    },
    {
      "arxiv_id": "2508.08334v1",
      "title": "HSA-Net: Hierarchical and Structure-Aware Framework for Efficient and Scalable Molecular Language Modeling",
      "title_zh": "HSA-Netï¼šé¢å‘é«˜æ•ˆä¸”å¯æ‰©å±•åˆ†å­è¯­è¨€å»ºæ¨¡çš„å±‚çº§åŒ–ç»“æ„æ„ŸçŸ¥æ¡†æ¶",
      "authors": [
        "Zihang Shao",
        "Wentao Lei",
        "Lei Wang",
        "Wencai Ye",
        "Li Liu"
      ],
      "abstract": "Molecular representation learning, a cornerstone for downstream tasks like molecular captioning and molecular property prediction, heavily relies on Graph Neural Networks (GNN). However, GNN suffers from the over-smoothing problem, where node-level features collapse in deep GNN layers. While existing feature projection methods with cross-attention have been introduced to mitigate this issue, they still perform poorly in deep features. This motivated our exploration of using Mamba as an alternative projector for its ability to handle complex sequences. However, we observe that while Mamba excels at preserving global topological information from deep layers, it neglects fine-grained details in shallow layers. The capabilities of Mamba and cross-attention exhibit a global-local trade-off. To resolve this critical global-local trade-off, we propose Hierarchical and Structure-Aware Network (HSA-Net), a novel framework with two modules that enables a hierarchical feature projection and fusion. Firstly, a Hierarchical Adaptive Projector (HAP) module is introduced to process features from different graph layers. It learns to dynamically switch between a cross-attention projector for shallow layers and a structure-aware Graph-Mamba projector for deep layers, producing high-quality, multi-level features. Secondly, to adaptively merge these multi-level features, we design a Source-Aware Fusion (SAF) module, which flexibly selects fusion experts based on the characteristics of the aggregation features, ensuring a precise and effective final representation fusion. Extensive experiments demonstrate that our HSA-Net framework quantitatively and qualitatively outperforms current state-of-the-art (SOTA) methods.",
      "tldr_zh": "æœ¬ç ”ç©¶é’ˆå¯¹åˆ†å­è¡¨ç¤ºå­¦ä¹ ä¸­å›¾ç¥ç»ç½‘ç»œ (GNN) åœ¨æ·±å±‚å‡ºç°çš„è¿‡åº¦å¹³æ»‘ (over-smoothing) é—®é¢˜ï¼Œæå‡ºäº†åä¸º HSA-Net çš„åˆ†å±‚ç»“æ„æ„ŸçŸ¥æ¡†æ¶ã€‚è¯¥æ¡†æ¶æ—¨åœ¨è§£å†³ç°æœ‰äº¤å‰æ³¨æ„åŠ› (cross-attention) æŠ•å½±å™¨ä¸ Mamba æ¨¡å‹åœ¨å¤„ç†æµ…å±‚ç»†èŠ‚ä¸æ·±å±‚å…¨å±€ä¿¡æ¯æ—¶å­˜åœ¨çš„å…¨å±€ä¸å±€éƒ¨æƒè¡¡ (global-local trade-off) éš¾é¢˜ã€‚HSA-Net æ ¸å¿ƒåŒ…å«åˆ†å±‚è‡ªé€‚åº”æŠ•å½±å™¨ (HAP) æ¨¡å—ï¼Œå¯æ ¹æ®å›¾å±‚æ·±åº¦åŠ¨æ€åˆ‡æ¢äº¤å‰æ³¨æ„åŠ›ä¸ç»“æ„æ„ŸçŸ¥ Graph-Mamba æŠ•å½±å™¨ï¼Œä»è€Œç”Ÿæˆé«˜è´¨é‡çš„å¤šçº§ç‰¹å¾ã€‚åŒæ—¶ï¼Œç ”ç©¶å¼•å…¥äº†æºæ„ŸçŸ¥èåˆ (SAF) æ¨¡å—ï¼Œé€šè¿‡çµæ´»é€‰æ‹©èåˆä¸“å®¶ (fusion experts) å®ç°å¯¹èšåˆç‰¹å¾çš„ç²¾ç¡®å¤„ç†ã€‚å®éªŒè¯æ˜ï¼ŒHSA-Net åœ¨å®šé‡ä¸å®šæ€§è¯„ä¼°ä¸­å‡ä¼˜äºå½“å‰çš„ SOTA æ–¹æ³•ï¼Œæ˜¾è‘—æå‡äº†åˆ†å­è¯­è¨€å»ºæ¨¡çš„æ•ˆç‡ä¸å¯æ‰©å±•æ€§ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "q-bio.QM"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.08334v1",
      "published_date": "2025-08-10 15:22:42 UTC",
      "updated_date": "2025-08-10 15:22:42 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T10:42:28.694911+00:00"
    },
    {
      "arxiv_id": "2508.20097v2",
      "title": "Can LLMs Identify Tax Abuse?",
      "title_zh": "å¤§è¯­è¨€æ¨¡å‹èƒ½å¦è¯†åˆ«ç¨æ”¶æ»¥ç”¨ï¼Ÿ",
      "authors": [
        "Andrew Blair-Stanek",
        "Nils Holzenberger",
        "Benjamin Van Durme"
      ],
      "abstract": "We investigate whether large language models can discover and analyze U.S. tax-minimization strategies. This real-world domain challenges even seasoned human experts, and progress can reduce tax revenue lost from well-advised, wealthy taxpayers. We evaluate the most advanced LLMs on their ability to (1) interpret and verify tax strategies, (2) fill in gaps in partially specified strategies, and (3) generate complete, end-to-end strategies from scratch. This domain should be of particular interest to the LLM reasoning community: unlike synthetic challenge problems or scientific reasoning tasks, U.S. tax law involves navigating hundreds of thousands of pages of statutes, case law, and administrative guidance, all updated regularly. Notably, LLM-based reasoning identified an entirely novel tax strategy, highlighting these models' potential to revolutionize tax agencies' fight against tax abuse.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†å¤§å‹è¯­è¨€æ¨¡å‹(LLMs)åœ¨å‘ç°å’Œåˆ†æç¾å›½ç¨æ”¶æœ€å°åŒ–(Tax-minimization)ç­–ç•¥æ–¹é¢çš„èƒ½åŠ›ï¼Œæ—¨åœ¨åº”å¯¹è¿™ä¸€ç”šè‡³å¯¹èµ„æ·±ä¸“å®¶éƒ½æå…·æŒ‘æˆ˜æ€§çš„ç°å®é¢†åŸŸã€‚ç ”ç©¶äººå‘˜è¯„ä¼°äº†æœ€å…ˆè¿›çš„LLMsåœ¨è§£é‡Šä¸éªŒè¯ç¨åŠ¡ç­–ç•¥ã€å¡«è¡¥éƒ¨åˆ†æŒ‡å®šç­–ç•¥ç©ºç™½ä»¥åŠä»å¤´ç”Ÿæˆå®Œæ•´ç«¯åˆ°ç«¯ç­–ç•¥æ–¹é¢çš„è¡¨ç°ã€‚ä¸åˆæˆæŒ‘æˆ˜æˆ–ç§‘å­¦æ¨ç†ä»»åŠ¡ä¸åŒï¼Œç¾å›½ç¨æ³•æ¶‰åŠå¯¹æˆåƒä¸Šä¸‡é¡µä¸æ–­æ›´æ–°çš„æ³•å¾‹ã€æ¡ˆä¾‹å’Œè¡Œæ”¿æŒ‡å—çš„å¤æ‚å¯¼èˆªã€‚ç ”ç©¶çš„ä¸€ä¸ªæ˜¾è‘—æˆæœæ˜¯ï¼ŒåŸºäºLLMçš„æ¨ç†è¯†åˆ«å‡ºäº†ä¸€ç§å…¨æ–°çš„ç¨åŠ¡ç­–ç•¥ï¼Œè¿™å……åˆ†è¯æ˜äº†æ­¤ç±»æ¨¡å‹åœ¨ååŠ©ç¨åŠ¡æœºå…³æ‰“å‡»ç¨æ”¶æ»¥ç”¨(Tax abuse)æ–¹é¢çš„é©å‘½æ€§æ½œåŠ›ã€‚",
      "categories": [
        "q-fin.CP",
        "cs.AI"
      ],
      "primary_category": "q-fin.CP",
      "comment": "9 pages",
      "pdf_url": "https://arxiv.org/pdf/2508.20097v2",
      "published_date": "2025-08-10 15:15:45 UTC",
      "updated_date": "2026-01-21 23:55:51 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T10:42:34.282790+00:00"
    },
    {
      "arxiv_id": "2508.07382v2",
      "title": "Pentest-R1: Towards Autonomous Penetration Testing Reasoning Optimized via Two-Stage Reinforcement Learning",
      "title_zh": "Pentest-R1ï¼šé€šè¿‡ä¸¤é˜¶æ®µå¼ºåŒ–å­¦ä¹ ä¼˜åŒ–è‡ªä¸»æ¸—é€æµ‹è¯•æ¨ç†",
      "authors": [
        "He Kong",
        "Die Hu",
        "Jingguo Ge",
        "Liangxiong Li",
        "Hui Li",
        "Tong Li"
      ],
      "abstract": "Automating penetration testing is crucial for enhancing cybersecurity, yet current Large Language Models (LLMs) face significant limitations in this domain, including poor error handling, inefficient reasoning, and an inability to perform complex end-to-end tasks autonomously. To address these challenges, we introduce Pentest-R1, a novel framework designed to optimize LLM reasoning capabilities for this task through a two-stage reinforcement learning pipeline. We first construct a dataset of over 500 real-world, multi-step walkthroughs, which Pentest-R1 leverages for offline reinforcement learning (RL) to instill foundational attack logic. Subsequently, the LLM is fine-tuned via online RL in an interactive Capture The Flag (CTF) environment, where it learns directly from environmental feedback to develop robust error self-correction and adaptive strategies. Our extensive experiments on the Cybench and AutoPenBench benchmarks demonstrate the framework's effectiveness. On AutoPenBench, Pentest-R1 achieves a 24.2\\% success rate, surpassing most state-of-the-art models and ranking second only to Gemini 2.5 Flash. On Cybench, it attains a 15.0\\% success rate in unguided tasks, establishing a new state-of-the-art for open-source LLMs and matching the performance of top proprietary models. Ablation studies confirm that the synergy of both training stages is critical to its success.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† Pentest-R1ï¼Œè¿™æ˜¯ä¸€ä¸ªæ—¨åœ¨é€šè¿‡ä¸¤é˜¶æ®µ Reinforcement Learning (RL) ä¼˜åŒ–å¤§è¯­è¨€æ¨¡å‹ (LLMs) åœ¨è‡ªåŠ¨åŒ–æ¸—é€æµ‹è¯•ä¸­æ¨ç†èƒ½åŠ›çš„æ¡†æ¶ã€‚é’ˆå¯¹å½“å‰ LLMs åœ¨é”™è¯¯å¤„ç†ã€æ¨ç†æ•ˆç‡åŠè‡ªä¸»æ‰§è¡Œå¤æ‚ä»»åŠ¡æ–¹é¢çš„å±€é™æ€§ï¼ŒPentest-R1 é¦–å…ˆåˆ©ç”¨åŒ…å« 500 å¤šä¸ªçœŸå®å¤šæ­¥æ”»ç•¥çš„èµ„æ–™é›†è¿›è¡Œ Offline RLï¼Œä»¥çŒè¾“åŸºç¡€æ”»å‡»é€»è¾‘ã€‚éšåï¼Œæ¨¡å‹åœ¨äº¤äº’å¼ Capture The Flag (CTF) ç¯å¢ƒä¸­é€šè¿‡ Online RL è¿›è¡Œå¾®è°ƒï¼Œåˆ©ç”¨ç¯å¢ƒåé¦ˆå¼€å‘é”™è¯¯è‡ªçº ä¸è‡ªé€‚åº”ç­–ç•¥ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒPentest-R1 åœ¨ AutoPenBench åŸºå‡†æµ‹è¯•ä¸Šè¾¾åˆ° 24.2% çš„æˆåŠŸç‡ï¼Œä½å±…å¼€æºæ¨¡å‹é¦–ä½å¹¶ä»…æ¬¡äº Gemini 2.5 Flashã€‚åœ¨ Cybench æœªå¼•å¯¼ä»»åŠ¡ä¸­ï¼Œå®ƒä»¥ 15.0% çš„æˆåŠŸç‡åˆ·æ–°äº†å¼€æºæ¨¡å‹çš„ SOTA è®°å½•ï¼Œæ€§èƒ½åª²ç¾é¡¶çº§ç§æœ‰æ¨¡å‹ã€‚æ¶ˆèå®éªŒè¯å®ï¼Œä¸¤ä¸ªè®­ç»ƒé˜¶æ®µçš„ååŒæ•ˆåº”æ˜¯å…¶å®ç°è‡ªä¸»æ¸—é€æµ‹è¯•æ€§èƒ½é£è·ƒçš„å…³é”®ã€‚",
      "categories": [
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.07382v2",
      "published_date": "2025-08-10 15:14:05 UTC",
      "updated_date": "2025-10-29 05:49:56 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T10:42:40.695442+00:00"
    },
    {
      "arxiv_id": "2508.16602v1",
      "title": "An Embodied AR Navigation Agent: Integrating BIM with Retrieval-Augmented Generation for Language Guidance",
      "title_zh": "å…·èº« AR å¯¼èˆªæ™ºèƒ½ä½“ï¼šèåˆ BIM ä¸æ£€ç´¢å¢å¼ºç”Ÿæˆçš„è¯­è¨€å¼•å¯¼",
      "authors": [
        "Hsuan-Kung Yang",
        "Tsu-Ching Hsiao",
        "Ryoichiro Oka",
        "Ryuya Nishino",
        "Satoko Tofukuji",
        "Norimasa Kobori"
      ],
      "abstract": "Delivering intelligent and adaptive navigation assistance in augmented reality (AR) requires more than visual cues, as it demands systems capable of interpreting flexible user intent and reasoning over both spatial and semantic context. Prior AR navigation systems often rely on rigid input schemes or predefined commands, which limit the utility of rich building data and hinder natural interaction. In this work, we propose an embodied AR navigation system that integrates Building Information Modeling (BIM) with a multi-agent retrieval-augmented generation (RAG) framework to support flexible, language-driven goal retrieval and route planning. The system orchestrates three language agents, Triage, Search, and Response, built on large language models (LLMs), which enables robust interpretation of open-ended queries and spatial reasoning using BIM data. Navigation guidance is delivered through an embodied AR agent, equipped with voice interaction and locomotion, to enhance user experience. A real-world user study yields a System Usability Scale (SUS) score of 80.5, indicating excellent usability, and comparative evaluations show that the embodied interface can significantly improves users' perception of system intelligence. These results underscore the importance and potential of language-grounded reasoning and embodiment in the design of user-centered AR navigation systems.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§å…·èº«ARå¯¼èˆªç³»ç»Ÿ(Embodied AR Navigation System)ï¼Œé€šè¿‡å°†å»ºç­‘ä¿¡æ¯æ¨¡å‹(BIM)ä¸å¤šæ™ºèƒ½ä½“æ£€ç´¢å¢å¼ºç”Ÿæˆ(RAG)æ¡†æ¶ç›¸ç»“åˆï¼Œå®ç°äº†çµæ´»çš„è¯­è¨€é©±åŠ¨ç›®æ ‡æ£€ç´¢å’Œè·¯å¾„è§„åˆ’ã€‚è¯¥ç³»ç»Ÿåˆ©ç”¨å¤§è¯­è¨€æ¨¡å‹(LLMs)æ„å»ºäº†Triageã€Searchå’ŒResponseä¸‰ä¸ªä¸“ä¸šæ™ºèƒ½ä½“ï¼Œèƒ½å¤Ÿè§£æå¼€æ”¾å¼æŸ¥è¯¢å¹¶ç»“åˆBIMæ•°æ®è¿›è¡Œå¤æ‚çš„ç©ºé—´æ¨ç†ã€‚å¯¼èˆªå¼•å¯¼é€šè¿‡ä¸€ä¸ªå…·å¤‡è¯­éŸ³äº¤äº’å’Œç§»åŠ¨èƒ½åŠ›çš„å…·èº«ARæ™ºèƒ½ä½“äº¤ä»˜ï¼Œæ—¨åœ¨å¢å¼ºäº¤äº’çš„è‡ªç„¶æ€§ã€‚åœ¨ç°å®åœºæ™¯çš„ç”¨æˆ·ç ”ç©¶ä¸­ï¼Œè¯¥ç³»ç»Ÿè·å¾—äº†80.5çš„ç³»ç»Ÿå¯ç”¨æ€§é‡è¡¨(SUS)è¯„åˆ†ï¼Œè¢«è¯„å®šä¸ºä¼˜ç§€ã€‚å®éªŒç»“æœè¯æ˜ï¼Œç›¸æ¯”äºä¼ ç»Ÿç•Œé¢ï¼Œå…·èº«åŒ–è®¾è®¡æ˜¾è‘—æå‡äº†ç”¨æˆ·å¯¹ç³»ç»Ÿæ™ºèƒ½ç¨‹åº¦çš„æ„ŸçŸ¥ï¼Œå‡¸æ˜¾äº†è¯­è¨€æ¨ç†ä¸å…·èº«äº¤äº’åœ¨ARå¯¼èˆªè®¾è®¡ä¸­çš„é‡è¦æ½œåŠ›ã€‚",
      "categories": [
        "cs.HC",
        "cs.AI"
      ],
      "primary_category": "cs.HC",
      "comment": "11 pages, 9 figures, accepted to IEEE ISMAR 2025",
      "pdf_url": "https://arxiv.org/pdf/2508.16602v1",
      "published_date": "2025-08-10 15:13:23 UTC",
      "updated_date": "2025-08-10 15:13:23 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T10:42:48.986941+00:00"
    },
    {
      "arxiv_id": "2508.14063v1",
      "title": "A Multi-Agent Approach to Neurological Clinical Reasoning",
      "title_zh": "ç¥ç»ç—…å­¦ä¸´åºŠæ¨ç†çš„å¤šæ™ºèƒ½ä½“æ–¹æ³•",
      "authors": [
        "Moran Sorka",
        "Alon Gorenshtein",
        "Dvir Aran",
        "Shahar Shelly"
      ],
      "abstract": "Large language models (LLMs) have shown promise in medical domains, but their ability to handle specialized neurological reasoning requires systematic evaluation. We developed a comprehensive benchmark using 305 questions from Israeli Board Certification Exams in Neurology, classified along three complexity dimensions: factual knowledge depth, clinical concept integration, and reasoning complexity. We evaluated ten LLMs using base models, retrieval-augmented generation (RAG), and a novel multi-agent system. Results showed significant performance variation. OpenAI-o1 achieved the highest base performance (90.9% accuracy), while specialized medical models performed poorly (52.9% for Meditron-70B). RAG provided modest benefits but limited effectiveness on complex reasoning questions. In contrast, our multi-agent framework, decomposing neurological reasoning into specialized cognitive functions including question analysis, knowledge retrieval, answer synthesis, and validation, achieved dramatic improvements, especially for mid-range models. The LLaMA 3.3-70B-based agentic system reached 89.2% accuracy versus 69.5% for its base model, with substantial gains on level 3 complexity questions. The multi-agent approach transformed inconsistent subspecialty performance into uniform excellence, addressing neurological reasoning challenges that persisted with RAG enhancement. We validated our approach using an independent dataset of 155 neurological cases from MedQA. Results confirm that structured multi-agent approaches designed to emulate specialized cognitive processes significantly enhance complex medical reasoning, offering promising directions for AI assistance in challenging clinical contexts.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤§å‹è¯­è¨€æ¨¡å‹(LLMs)åœ¨ç¥ç»å†…ç§‘ä¸´åºŠæ¨ç†ä¸­çš„åº”ç”¨è¿›è¡Œäº†ç³»ç»Ÿè¯„ä¼°ï¼Œå¹¶å¼€å‘äº†ä¸€ä¸ªåŸºäºä»¥è‰²åˆ—ç¥ç»å†…ç§‘æ‰§ä¸šåŒ»å¸ˆè€ƒè¯•(Israeli Board Certification Exams in Neurology)çš„ç»¼åˆåŸºå‡†ã€‚ç ”ç©¶äººå‘˜å¯¹æ¯”äº†åŸºç¡€æ¨¡å‹ã€æ£€ç´¢å¢å¼ºç”Ÿæˆ(RAG)ä»¥åŠä¸€ç§æ–°å‹çš„å¤šæ™ºèƒ½ä½“ç³»ç»Ÿ(multi-agent system)ï¼Œå‘ç°OpenAI-o1åœ¨åŸºç¡€è¡¨ç°ä¸­æœ€ä¸ºä¼˜å¼‚ï¼Œè€Œä¸“é—¨çš„åŒ»å­¦æ¨¡å‹è¡¨ç°æ¬ ä½³ã€‚ç ”ç©¶æå‡ºçš„å¤šæ™ºèƒ½ä½“æ¡†æ¶å°†ç¥ç»ç§‘å­¦æ¨ç†åˆ†è§£ä¸ºé—®é¢˜åˆ†æ(question analysis)ã€çŸ¥è¯†æ£€ç´¢(knowledge retrieval)ã€ç­”æ¡ˆåˆæˆ(answer synthesis)å’ŒéªŒè¯(validation)ç­‰ä¸“ä¸šè®¤çŸ¥åŠŸèƒ½ï¼Œæ˜¾è‘—æå‡äº†ä¸­ç­‰è§„æ¨¡æ¨¡å‹çš„æ€§èƒ½ã€‚å®éªŒæ˜¾ç¤ºï¼ŒåŸºäºLLaMA 3.3-70Bçš„å¤šæ™ºèƒ½ä½“ç³»ç»Ÿåœ¨å¤„ç†å¤æ‚æ¨ç†é—®é¢˜æ—¶ï¼Œå‡†ç¡®ç‡ä»69.5%å¤§å¹…æå‡è‡³89.2%ï¼Œå…‹æœäº†RAGåœ¨åº”å¯¹æ·±åº¦é€»è¾‘æŒ‘æˆ˜æ—¶çš„å±€é™æ€§ã€‚é€šè¿‡åœ¨MedQAæ•°æ®é›†ä¸Šçš„ç‹¬ç«‹éªŒè¯ï¼Œè¯¥ç ”ç©¶è¯å®äº†æ¨¡æ‹Ÿäººç±»è®¤çŸ¥è¿‡ç¨‹çš„å¤šæ™ºèƒ½ä½“æ–¹æ³•èƒ½æœ‰æ•ˆå¢å¼ºå¤æ‚åŒ»å­¦æ¨ç†ï¼Œä¸ºäººå·¥æ™ºèƒ½è¾…åŠ©ä¸´åºŠå†³ç­–æä¾›äº†é‡è¦æ–¹å‘ã€‚",
      "categories": [
        "cs.IR",
        "cs.AI"
      ],
      "primary_category": "cs.IR",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.14063v1",
      "published_date": "2025-08-10 14:52:27 UTC",
      "updated_date": "2025-08-10 14:52:27 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T10:42:55.459283+00:00"
    },
    {
      "arxiv_id": "2508.08333v1",
      "title": "Normative Moral Pluralism for AI: A Framework for Deliberation in Complex Moral Contexts",
      "title_zh": "é¢å‘äººå·¥æ™ºèƒ½çš„è§„èŒƒæ€§é“å¾·å¤šå…ƒè®ºï¼šå¤æ‚é“å¾·æƒ…å¢ƒä¸‹çš„å®¡è®®æ¡†æ¶",
      "authors": [
        "David-Doron Yaacov"
      ],
      "abstract": "The conceptual framework proposed in this paper centers on the development of a deliberative moral reasoning system - one designed to process complex moral situations by generating, filtering, and weighing normative arguments drawn from diverse ethical perspectives. While the framework is rooted in Machine Ethics, it also makes a substantive contribution to Value Alignment by outlining a system architecture that links structured moral reasoning to action under time constraints. Grounded in normative moral pluralism, this system is not constructed to imitate behavior but is built on reason-sensitive deliberation over structured moral content in a transparent and principled manner. Beyond its role as a deliberative system, it also serves as the conceptual foundation for a novel two-level architecture: functioning as a moral reasoning teacher envisioned to train faster models that support real-time responsiveness without reproducing the full structure of deliberative reasoning. Together, the deliberative and intuitive components are designed to enable both deep reflection and responsive action. A key design feature is the dual-hybrid structure: a universal layer that defines a moral threshold through top-down and bottom-up learning, and a local layer that learns to weigh competing considerations in context while integrating culturally specific normative content, so long as it remains within the universal threshold. By extending the notion of moral complexity to include not only conflicting beliefs but also multifactorial dilemmas, multiple stakeholders, and the integration of non-moral considerations, the framework aims to support morally grounded decision-making in realistic, high-stakes contexts.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ä¸ªåŸºäºè§„èŒƒæ€§é“å¾·å¤šå…ƒåŒ– (Normative Moral Pluralism) çš„å®¡è®®å¼é“å¾·æ¨ç†æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³äººå·¥æ™ºèƒ½åœ¨å¤æ‚é“å¾·è¯­å¢ƒä¸‹çš„å†³ç­–æŒ‘æˆ˜ã€‚è¯¥ç³»ç»Ÿé€šè¿‡ç”Ÿæˆã€è¿‡æ»¤å’Œæƒè¡¡æ¥è‡ªä¸åŒä¼¦ç†è§†è§’çš„è§„èŒƒæ€§è®ºç‚¹ï¼Œå®ç°äº†é€æ˜ä¸”åŸºäºåŸåˆ™çš„é“å¾·å®¡è®®ï¼Œè€Œéç®€å•çš„è¡Œä¸ºæ¨¡ä»¿ã€‚è¯¥æ¡†æ¶é‡‡ç”¨äº†åˆ›æ–°çš„ä¸¤å±‚æ¶æ„ï¼Œå°†æ·±æ€ç†Ÿè™‘çš„å®¡è®®ç»„ä»¶ä½œä¸ºâ€œè€å¸ˆâ€æ¥è®­ç»ƒèƒ½å¤Ÿå¿«é€Ÿå“åº”çš„ç›´è§‰å¼æ¨¡å‹ï¼Œä»è€Œåœ¨ä¿è¯å®æ—¶å“åº”é€Ÿåº¦çš„åŒæ—¶å…¼é¡¾æ·±åº¦åæ€ã€‚æ­¤å¤–ï¼Œç³»ç»Ÿé€šè¿‡ç”±é€šç”¨å±‚ (Universal Layer) å’Œå±€éƒ¨å±‚ (Local Layer) æ„æˆçš„åŒé‡æ··åˆç»“æ„ï¼Œåœ¨å®šä¹‰æ™®é€‚é“å¾·åº•çº¿çš„åŒæ—¶æ•´åˆäº†ç‰¹å®šæ–‡åŒ–èƒŒæ™¯ä¸‹çš„è§„èŒƒæ€§å†…å®¹ã€‚è¯¥ç ”ç©¶å°†é“å¾·å¤æ‚æ€§çš„èŒƒç•´æ‰©å±•åˆ°å¤šå› ç´ å›°å¢ƒå’Œå¤šæ–¹åˆ©ç›Šç›¸å…³è€…ï¼Œä¸ºé«˜é£é™©ç°å®åœºæ™¯ä¸­çš„ä»·å€¼å¯¹é½ (Value Alignment) å’Œæœºå™¨ä¼¦ç† (Machine Ethics) æä¾›äº†ç³»ç»ŸåŒ–çš„æ¶æ„æ”¯æŒã€‚",
      "categories": [
        "cs.CY",
        "cs.AI"
      ],
      "primary_category": "cs.CY",
      "comment": "Conference version: AIES 2025 (non-archival track), 12 pages",
      "pdf_url": "https://arxiv.org/pdf/2508.08333v1",
      "published_date": "2025-08-10 14:52:23 UTC",
      "updated_date": "2025-08-10 14:52:23 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T10:42:53.099904+00:00"
    },
    {
      "arxiv_id": "2508.08332v1",
      "title": "Energy-Aware Code Generation with LLMs: Benchmarking Small vs. Large Language Models for Sustainable AI Programming",
      "title_zh": "åŸºäºå¤§è¯­è¨€æ¨¡å‹çš„èƒ½é‡æ„ŸçŸ¥å‹ä»£ç ç”Ÿæˆï¼šé¢å‘å¯æŒç»­ AI ç¼–ç¨‹çš„å°å‹ä¸å¤§å‹è¯­è¨€æ¨¡å‹åŸºå‡†æµ‹è¯•",
      "authors": [
        "Humza Ashraf",
        "Syed Muhammad Danish",
        "Aris Leivadeas",
        "Yazan Otoum",
        "Zeeshan Sattar"
      ],
      "abstract": "Large Language Models (LLMs) are widely used for code generation. However, commercial models like ChatGPT require significant computing power, which leads to high energy use and carbon emissions. This has raised concerns about their environmental impact. In this study, we evaluate open-source Small Language Models (SLMs) trained explicitly for code generation and compare their performance and energy efficiency against large LLMs and efficient human-written Python code. The goal is to investigate whether SLMs can match the performance of LLMs on certain types of programming problems while producing more energy-efficient code. We evaluate 150 coding problems from LeetCode, evenly distributed across three difficulty levels: easy, medium, and hard. Our comparison includes three small open-source models, StableCode-3B, StarCoderBase-3B, and Qwen2.5-Coder-3B-Instruct, and two large commercial models, GPT-4.0 and DeepSeek-Reasoner. The generated code is evaluated using four key metrics: run-time, memory usage, energy consumption, and correctness. We use human-written solutions as a baseline to assess the quality and efficiency of the model-generated code. Results indicate that LLMs achieve the highest correctness across all difficulty levels, but SLMs are often more energy-efficient when their outputs are correct. In over 52% of the evaluated problems, SLMs consumed the same or less energy than LLMs.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤§å‹è¯­è¨€æ¨¡å‹(LLMs)åœ¨ä»£ç ç”Ÿæˆä¸­å› é«˜è®¡ç®—éœ€æ±‚å¯¼è‡´çš„èƒ½æºæ¶ˆè€—å’Œç¯å¢ƒå½±å“é—®é¢˜ï¼Œè¯„ä¼°äº†å°å‹è¯­è¨€æ¨¡å‹(SLMs)åœ¨å¯æŒç»­AIç¼–ç¨‹æ–¹é¢çš„æ½œåŠ›ã€‚ç ”ç©¶å¯¹æ¯”äº†StableCode-3Bã€StarCoderBase-3Bå’ŒQwen2.5-Coder-3B-Instructç­‰å¼€æºSLMsï¼Œä»¥åŠGPT-4.0å’ŒDeepSeek-Reasonerç­‰å•†ç”¨å¤§æ¨¡å‹åœ¨150ä¸ªä¸åŒéš¾åº¦çš„LeetCodeç¼–ç¨‹é¢˜ç›®ä¸Šçš„è¡¨ç°ã€‚è¯„ä¼°ä½“ç³»æ¶µç›–äº†è¿è¡Œæ—¶é—´(Run-time)ã€å†…å­˜å ç”¨(Memory usage)ã€èƒ½æºæ¶ˆè€—(Energy consumption)å’Œæ­£ç¡®æ€§(Correctness)å››ä¸ªæ ¸å¿ƒæŒ‡æ ‡ï¼Œå¹¶ä»¥äººç±»ç¼–å†™çš„ä»£ç ä½œä¸ºåŸºå‡†(Baseline)ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œå°½ç®¡LLMsåœ¨ä»£ç æ­£ç¡®æ€§æ–¹é¢ä¿æŒé¢†å…ˆï¼Œä½†SLMsåœ¨ç”Ÿæˆæ­£ç¡®ä»£ç æ—¶å¾€å¾€æ›´å…·èƒ½æºæ•ˆç‡ã€‚åœ¨è¶…è¿‡52%çš„æµ‹è¯•æ¡ˆä¾‹ä¸­ï¼ŒSLMsçš„èƒ½æ•ˆè¡¨ç°ä¼˜äºæˆ–ç­‰äºLLMsï¼Œè¯æ˜äº†åœ¨ç‰¹å®šç¼–ç¨‹ä»»åŠ¡ä¸­é‡‡ç”¨å°å‹åŒ–æ¨¡å‹ä»¥å‡å°‘ç¢³è¶³è¿¹çš„å¯è¡Œæ€§ã€‚",
      "categories": [
        "cs.SE",
        "cs.AI"
      ],
      "primary_category": "cs.SE",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.08332v1",
      "published_date": "2025-08-10 14:44:06 UTC",
      "updated_date": "2025-08-10 14:44:06 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T10:42:58.884180+00:00"
    },
    {
      "arxiv_id": "2508.07371v1",
      "title": "AutoAssert 1: A LoRA Fine-Tuned LLM Model for Efficient Automated Assertion Generation",
      "title_zh": "AutoAssert 1ï¼šåŸºäº LoRA å¾®è°ƒçš„é«˜æ•ˆè‡ªåŠ¨åŒ–æ–­è¨€ç”Ÿæˆ LLM æ¨¡å‹",
      "authors": [
        "Yi Zhong",
        "Hongchao Liu",
        "Di ZHao"
      ],
      "abstract": "As the complexity of software systems continues to increase, the demand for automated testing and maintenance tools is growing exponentially. To meet this urgent need, we propose a new assertion generation method based on Hardware Description Language (HDL). This method combines a lightweight, parameter-adjustable large language model (LLM) with the Unsloth platform to automatically generate test cases, thereby significantly reducing training costs without sacrificing accuracy or generalization performance. Empirical evaluation shows that our method can efficiently generate assertions that strictly conform to the hardware logic. This framework provides a robust and flexible solution to modern software testing and maintenance challenges. https://github.com/liusu-orange/AutoAssert-1 and https://gitee.com/OpenBPU/auto-assert1 are the locations of the source code.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† AutoAssert 1ï¼Œè¿™æ˜¯ä¸€ç§åŸºäºç¡¬ä»¶æè¿°è¯­è¨€ (Hardware Description Language, HDL) çš„æ–°å‹æ–­è¨€ç”Ÿæˆæ–¹æ³•ï¼Œæ—¨åœ¨åº”å¯¹æ—¥ç›Šå¢é•¿çš„è‡ªåŠ¨åŒ–è½¯ä»¶æµ‹è¯•å’Œç»´æŠ¤éœ€æ±‚ã€‚è¯¥æ–¹æ³•åˆ©ç”¨ Unsloth å¹³å°å¯¹è½»é‡çº§å¤§è¯­è¨€æ¨¡å‹ (LLM) è¿›è¡Œ LoRA å¾®è°ƒï¼Œå®ç°äº†è‡ªåŠ¨åŒ–çš„æµ‹è¯•ç”¨ä¾‹ç”Ÿæˆã€‚é€šè¿‡è¿™ç§æ¶æ„ï¼Œæ¨¡å‹åœ¨æ˜¾è‘—é™ä½è®­ç»ƒæˆæœ¬çš„åŒæ—¶ï¼Œä¾ç„¶ä¿æŒäº†æé«˜çš„å‡†ç¡®ç‡å’Œæ³›åŒ–æ€§èƒ½ã€‚å®éªŒè¯„ä¼°æ˜¾ç¤ºï¼ŒAutoAssert 1 èƒ½å¤Ÿé«˜æ•ˆç”Ÿæˆä¸¥æ ¼ç¬¦åˆç¡¬ä»¶é€»è¾‘çš„æ–­è¨€ (assertions)ï¼Œä¸ºè§£å†³ç°ä»£è½¯ä»¶æµ‹è¯•æŒ‘æˆ˜æä¾›äº†ç¨³å¥ä¸”çµæ´»çš„æ–¹æ¡ˆã€‚ç›®å‰è¯¥é¡¹ç›®çš„æºä»£ç å·²åœ¨ GitHub å’Œ Gitee å¹³å°åŒæ­¥å¼€æºã€‚",
      "categories": [
        "cs.SE",
        "cs.AI"
      ],
      "primary_category": "cs.SE",
      "comment": "16pages,6figures",
      "pdf_url": "https://arxiv.org/pdf/2508.07371v1",
      "published_date": "2025-08-10 14:43:54 UTC",
      "updated_date": "2025-08-10 14:43:54 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T10:44:13.396184+00:00"
    },
    {
      "arxiv_id": "2508.07353v3",
      "title": "Benchmarking for Domain-Specific LLMs: A Case Study on Academia and Beyond",
      "title_zh": "ç‰¹å®šé¢†åŸŸå¤§è¯­è¨€æ¨¡å‹åŸºå‡†æµ‹è¯„ï¼šå­¦æœ¯ç•ŒåŠå…¶ä»–é¢†åŸŸçš„æ¡ˆä¾‹ç ”ç©¶",
      "authors": [
        "Rubing Chen",
        "Jiaxin Wu",
        "Jian Wang",
        "Xulu Zhang",
        "Wenqi Fan",
        "Chenghua Lin",
        "Xiao-Yong Wei",
        "Qing Li"
      ],
      "abstract": "The increasing demand for domain-specific evaluation of large language models (LLMs) has led to the development of numerous benchmarks. These efforts often adhere to the principle of data scaling, relying on large corpora or extensive question-answer (QA) sets to ensure broad coverage. However, the impact of corpus and QA set design on the precision and recall of domain-specific LLM performance remains poorly understood. In this paper, we argue that data scaling is not always the optimal principle for domain-specific benchmark construction. Instead, we introduce Comp-Comp, an iterative benchmarking framework grounded in the principle of comprehensiveness and compactness. Comprehensiveness ensures semantic recall by covering the full breadth of the domain, while compactness improves precision by reducing redundancy and noise. To demonstrate the effectiveness of our approach, we present a case study conducted at a well-renowned university, resulting in the creation of PolyBench, a large-scale, high-quality academic benchmark. Although this study focuses on academia, the Comp-Comp framework is domain-agnostic and readily adaptable to a wide range of specialized fields. The source code and datasets can be accessed at https://github.com/Anya-RB-Chen/COMP-COMP.",
      "tldr_zh": "æœ¬ç ”ç©¶é’ˆå¯¹é¢†åŸŸç‰¹å®šå¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰è¯„ä¼°ä¸­è¿‡åº¦ä¾èµ–æ•°æ®ç¼©æ”¾ï¼ˆdata scalingï¼‰çš„ç°çŠ¶ï¼ŒæŒ‡å‡ºå¤§è§„æ¨¡è¯­æ–™åº“å¹¶éæ„å»ºåŸºå‡†æµ‹è¯•çš„å”¯ä¸€æœ€ä¼˜è§£ã€‚ä¸ºæ­¤ï¼Œä½œè€…æå‡ºäº†åä¸ºComp-Compçš„è¿­ä»£åŸºå‡†æµ‹è¯•æ¡†æ¶ï¼Œè¯¥æ¡†æ¶æ ¸å¿ƒéµå¾ªå…¨é¢æ€§ï¼ˆcomprehensivenessï¼‰å’Œç´§å‡‘æ€§ï¼ˆcompactnessï¼‰ä¸¤å¤§åŸåˆ™ã€‚å…¨é¢æ€§æ—¨åœ¨è¦†ç›–é¢†åŸŸå†…æ‰€æœ‰è¯­ä¹‰å¹¿åº¦ä»¥ç¡®ä¿è¯­ä¹‰å¬å›ï¼ˆsemantic recallï¼‰ï¼Œè€Œç´§å‡‘æ€§åˆ™é€šè¿‡å‡å°‘å†—ä½™å’Œå™ªå£°æ¥æå‡è¯„ä¼°ç²¾åº¦ï¼ˆprecisionï¼‰ã€‚é€šè¿‡åœ¨çŸ¥åå¤§å­¦å¼€å±•çš„å­¦æœ¯æ¡ˆä¾‹ç ”ç©¶ï¼Œç ”ç©¶è€…æˆåŠŸæ„å»ºäº†å¤§è§„æ¨¡é«˜è´¨é‡çš„å­¦æœ¯åŸºå‡†PolyBenchã€‚å®éªŒè¯æ˜ï¼ŒComp-Compæ¡†æ¶ä¸ä»…åœ¨å­¦æœ¯é¢†åŸŸè¡¨ç°å‡ºè‰²ï¼Œä¸”å…·æœ‰é¢†åŸŸæ— å…³æ€§ï¼Œèƒ½å¤Ÿçµæ´»åº”ç”¨äºå„ç§ä¸“ä¸šé¢†åŸŸçš„æ¨¡å‹è¯„ä¼°ä¸æ„å»ºã€‚",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "Accepted by EMNLP2025 Findings",
      "pdf_url": "https://arxiv.org/pdf/2508.07353v3",
      "published_date": "2025-08-10 14:08:28 UTC",
      "updated_date": "2025-09-09 03:00:43 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T10:43:18.260063+00:00"
    },
    {
      "arxiv_id": "2508.09208v1",
      "title": "CoMoE: Collaborative Optimization of Expert Aggregation and Offloading for MoE-based LLMs at Edge",
      "title_zh": "CoMoEï¼šè¾¹ç¼˜ä¾§åŸºäº MoE çš„å¤§è¯­è¨€æ¨¡å‹ä¸“å®¶èšåˆä¸å¸è½½ååŒä¼˜åŒ–",
      "authors": [
        "Muqing Li",
        "Ning Li",
        "Xin Yuan",
        "Wenchao Xu",
        "Quan Chen",
        "Song Guo",
        "Haijun Zhang"
      ],
      "abstract": "The proliferation of large language models (LLMs) has driven the adoption of Mixture-of-Experts (MoE) architectures as a promising solution to scale model capacity while controlling computational costs. However, deploying MoE models in resource-constrained mobile edge computing environments presents significant challenges due to their large memory footprint and dynamic expert activation patterns. To address these challenges, we propose a novel dynamic resource-aware collaborative optimization framework that jointly optimizes expert aggregation granularity and offloading strategies based on real-time device resource states, network conditions, and input characteristics in mobile edge environments, denoted as CoMoE. In CoMoE, we first systematically analyze existing expert aggregation techniques, including expert parameter merging,knowledge distillation,and parameter sharing decomposition, identifying their limitations in dynamic mobile environments.We then investigate expert offloading strategies encompassing expert prediction and prefetching, expert caching and scheduling, and multi-tier storage architectures, revealing the interdependencies between routing decisions and offloading performance.The CoMoE incorporates adaptive scheduling mechanisms that respond to user mobility and varying network conditions, enabling efficient MoE deployment across heterogeneous edge devices. Extensive experiments on real mobile edge testbeds demonstrate that CoMoE achieves approximately 70% reduction in memory usage compared to baseline methods, 10.5% lower inference latency than existing expert offloading techniques, while maintaining model performance stability. For large-scale MoE models (e.g,7.4B-parameter Switch-Base-128), the CoMoE reduces memory requirements from 15.6GB to 4.7GB, enabling deployment on resource-constrained mobile edge devices that previously could only support much smaller models.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ Mixture-of-Experts (MoE) æ¶æ„çš„å¤§è¯­è¨€æ¨¡å‹åœ¨èµ„æºå—é™çš„è¾¹ç¼˜è®¡ç®—ç¯å¢ƒä¸­é¢ä¸´çš„å†…å­˜æ¶ˆè€—å¤§å’Œä¸“å®¶æ¿€æ´»æ¨¡å¼åŠ¨æ€å˜åŒ–ç­‰æŒ‘æˆ˜ï¼Œæå‡ºäº† CoMoE åä½œä¼˜åŒ–æ¡†æ¶ã€‚è¯¥æ¡†æ¶é€šè¿‡å®æ—¶æ„ŸçŸ¥è®¾å¤‡èµ„æºçŠ¶æ€ã€ç½‘ç»œæ¡ä»¶å’Œè¾“å…¥ç‰¹å¾ï¼ŒååŒä¼˜åŒ–äº†ä¸“å®¶èšåˆ (Expert Aggregation) çš„ç²’åº¦ä¸å¸è½½ (Offloading) ç­–ç•¥ã€‚CoMoE æ·±å…¥åˆ†æå¹¶æ•´åˆäº†ä¸“å®¶å‚æ•°åˆå¹¶ã€çŸ¥è¯†è’¸é¦ç­‰èšåˆæŠ€æœ¯ï¼Œå¹¶ç»“åˆä¸“å®¶é¢„æµ‹ã€é¢„å–ä»¥åŠå¤šçº§å­˜å‚¨æ¶æ„ä¼˜åŒ–äº†è·¯ç”±å†³ç­–ä¸å¸è½½æ€§èƒ½çš„ä¾èµ–å…³ç³»ã€‚æ­¤å¤–ï¼Œæ¡†æ¶è¿˜å¼•å…¥äº†è‡ªé€‚åº”è°ƒåº¦æœºåˆ¶ï¼Œä»¥åº”å¯¹ç”¨æˆ·ç§»åŠ¨æ€§å’Œå¤šå˜çš„è¾¹ç¼˜ç½‘ç»œç¯å¢ƒï¼Œç¡®ä¿åœ¨å¼‚æ„è¾¹ç¼˜è®¾å¤‡ä¸Šçš„é«˜æ•ˆéƒ¨ç½²ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒCoMoE ç›¸æ¯”åŸºçº¿æ–¹æ³•å‡å°‘äº†çº¦ 70% çš„å†…å­˜å ç”¨ï¼Œæ¨ç†å»¶è¿Ÿé™ä½äº† 10.5%ï¼Œä¸”èƒ½ä¿æŒæ¨¡å‹æ€§èƒ½çš„ç¨³å®šã€‚åœ¨éƒ¨ç½²å¤§å‹ Switch-Base-128 æ¨¡å‹æ—¶ï¼ŒCoMoE æˆåŠŸå°†å†…å­˜éœ€æ±‚ä» 15.6GB é™è‡³ 4.7GBï¼Œä½¿å¾—åŸæœ¬æ— æ³•æ‰¿è½½å¤§æ¨¡å‹çš„èµ„æºå—é™è¾¹ç¼˜è®¾å¤‡èƒ½å¤Ÿè¿è¡Œ 7.4B å‚æ•°è§„æ¨¡çš„ MoE æ¨¡å‹ã€‚",
      "categories": [
        "cs.NI",
        "cs.AI"
      ],
      "primary_category": "cs.NI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.09208v1",
      "published_date": "2025-08-10 14:05:36 UTC",
      "updated_date": "2025-08-10 14:05:36 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T10:43:19.253057+00:00"
    },
    {
      "arxiv_id": "2508.07345v1",
      "title": "ProteoKnight: Convolution-based phage virion protein classification and uncertainty analysis",
      "title_zh": "ProteoKnightï¼šåŸºäºå·ç§¯çš„å™¬èŒä½“æ¯’ç²’è›‹ç™½åˆ†ç±»ä¸ä¸ç¡®å®šæ€§åˆ†æ",
      "authors": [
        "Samiha Afaf Neha",
        "Abir Ahammed Bhuiyan",
        "Md. Ishrak Khan"
      ],
      "abstract": "\\textbf{Introduction:} Accurate prediction of Phage Virion Proteins (PVP) is essential for genomic studies due to their crucial role as structural elements in bacteriophages. Computational tools, particularly machine learning, have emerged for annotating phage protein sequences from high-throughput sequencing. However, effective annotation requires specialized sequence encodings. Our paper introduces ProteoKnight, a new image-based encoding method that addresses spatial constraints in existing techniques, yielding competitive performance in PVP classification using pre-trained convolutional neural networks. Additionally, our study evaluates prediction uncertainty in binary PVP classification through Monte Carlo Dropout (MCD). \\textbf{Methods:} ProteoKnight adapts the classical DNA-Walk algorithm for protein sequences, incorporating pixel colors and adjusting walk distances to capture intricate protein features. Encoded sequences were classified using multiple pre-trained CNNs. Variance and entropy measures assessed prediction uncertainty across proteins of various classes and lengths. \\textbf{Results:} Our experiments achieved 90.8% accuracy in binary classification, comparable to state-of-the-art methods. Multi-class classification accuracy remains suboptimal. Our uncertainty analysis unveils variability in prediction confidence influenced by protein class and sequence length. \\textbf{Conclusions:} Our study surpasses frequency chaos game representation (FCGR) by introducing novel image encoding that mitigates spatial information loss limitations. Our classification technique yields accurate and robust PVP predictions while identifying low-confidence predictions.",
      "tldr_zh": "æœ¬ç ”ç©¶å¼€å‘äº† ProteoKnightï¼Œè¿™æ˜¯ä¸€ç§åŸºäºå›¾åƒç¼–ç çš„æ–°å‹ Phage Virion Proteins (PVP) åˆ†ç±»ä¸ä¸ç¡®å®šæ€§åˆ†ææ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³åŸºå› ç»„ç ”ç©¶ä¸­å™¬èŒä½“è›‹ç™½åºåˆ—æ³¨é‡Šçš„æŒ‘æˆ˜ã€‚ProteoKnight æ”¹è¿›äº†ç»å…¸çš„ DNA-Walk ç®—æ³•ï¼Œé€šè¿‡åƒç´ é¢œè‰²å’Œæ­¥è¡Œè·ç¦»è°ƒæ•´å°†å…¶åº”ç”¨äºè›‹ç™½è´¨åºåˆ—ï¼Œæœ‰æ•ˆå…‹æœäº†ä¼ ç»Ÿç¼–ç æ–¹æ³•åœ¨æ•è·ç©ºé—´ç‰¹å¾æ–¹é¢çš„é™åˆ¶ã€‚è¯¥æ¡†æ¶åˆ©ç”¨é¢„è®­ç»ƒçš„ CNNs å¯¹å›¾åƒåŒ–åºåˆ—è¿›è¡Œåˆ†ç±»ï¼Œåœ¨äºŒåˆ†ç±»ä»»åŠ¡ä¸­è¾¾åˆ°äº† 90.8% çš„å‡†ç¡®ç‡ï¼Œè¡¨ç°å‡ºä¸ç°æœ‰æœ€å…ˆè¿›æ–¹æ³•ç›¸å½“çš„ç«äº‰åŠ›ã€‚æ­¤å¤–ï¼Œç ”ç©¶è¿˜é€šè¿‡ Monte Carlo Dropout (MCD) æŠ€æœ¯è¯„ä¼°äº†é¢„æµ‹çš„ä¸ç¡®å®šæ€§ï¼Œå‘ç°é¢„æµ‹ç½®ä¿¡åº¦å—è›‹ç™½è´¨ç±»åˆ«å’Œåºåˆ—é•¿åº¦çš„æ˜¾è‘—å½±å“ã€‚ç›¸æ¯” Frequency Chaos Game Representation (FCGR)ï¼ŒProteoKnight èƒ½å¤Ÿæ›´æœ‰æ•ˆåœ°ä¿ç•™å…³é”®çš„åºåˆ—ç©ºé—´ä¿¡æ¯ã€‚è¯¥æ–¹æ³•ä¸ä»…å®ç°äº†ç²¾ç¡®çš„ PVP é¢„æµ‹ï¼Œè¿˜èƒ½æœ‰æ•ˆè¯†åˆ«ä½ç½®ä¿¡åº¦çš„åˆ†ç±»ç»“æœï¼Œä¸ºå™¬èŒä½“åŸºå› ç»„å­¦ç ”ç©¶æä¾›äº†é²æ£’çš„åˆ†æå·¥å…·ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.07345v1",
      "published_date": "2025-08-10 13:45:08 UTC",
      "updated_date": "2025-08-10 13:45:08 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T10:43:23.997783+00:00"
    },
    {
      "arxiv_id": "2508.07334v2",
      "title": "Hallucination as a Computational Boundary: A Hierarchy of Inevitability and the Oracle Escape",
      "title_zh": "å¹»è§‰ä½œä¸ºè®¡ç®—è¾¹ç•Œï¼šå¿…ç„¶æ€§å±‚çº§ä½“ç³»ä¸é¢„è¨€æœºé€ƒé€¸",
      "authors": [
        "Wang Xi",
        "Quan Shi",
        "Zenghui Ding",
        "Jianqing Gao",
        "Xianjun Yang"
      ],
      "abstract": "The illusion phenomenon of large language models (LLMs) is the core obstacle to their reliable deployment. This article formalizes the large language model as a probabilistic Turing machine by constructing a \"computational necessity hierarchy\", and for the first time proves the illusions are inevitable on diagonalization, incomputability, and information theory boundaries supported by the new \"learner pump lemma\". However, we propose two \"escape routes\": one is to model Retrieval Enhanced Generations (RAGs) as oracle machines, proving their absolute escape through \"computational jumps\", providing the first formal theory for the effectiveness of RAGs; The second is to formalize continuous learning as an \"internalized oracle\" mechanism and implement this path through a novel neural game theory framework. Finally, this article proposes a feasible new principle for artificial intelligence security - Computational Class Alignment (CCA), which requires strict matching between task complexity and the actual computing power of the system, providing theoretical support for the secure application of artificial intelligence.",
      "tldr_zh": "è¯¥ç ”ç©¶é€šè¿‡å°†å¤§è¯­è¨€æ¨¡å‹ (LLMs) å½¢å¼åŒ–ä¸ºæ¦‚ç‡å›¾çµæœº (probabilistic Turing machine)ï¼Œæ„å»ºäº†è®¡ç®—å¿…è¦æ€§å±‚æ¬¡ (computational necessity hierarchy)ï¼Œå¹¶åˆ©ç”¨æ–°çš„å­¦ä¹ è€…æŠ½æ ·å¼•ç† (learner pump lemma) è¯æ˜äº†å¹»è§‰ (hallucination) åœ¨å¯¹è§’çº¿åŒ–ã€ä¸å¯è®¡ç®—æ€§å’Œä¿¡æ¯è®ºè¾¹ç•Œä¸Šçš„å¿…ç„¶æ€§ã€‚ä¸ºäº†åº”å¯¹è¿™ä¸€æŒ‘æˆ˜ï¼Œè®ºæ–‡æå‡ºäº†ä¸¤æ¡é€ƒç”Ÿè·¯å¾„ï¼šå…¶ä¸€å°†æ£€ç´¢å¢å¼ºç”Ÿæˆ (RAG) å»ºæ¨¡ä¸ºç”²è°•æœº (oracle machines)ï¼Œé€šè¿‡è®¡ç®—è·³è·ƒ (computational jumps) è¯æ˜äº†å…¶é¿å¼€å¹»è§‰çš„ç†è®ºå¯è¡Œæ€§ï¼›å…¶äºŒå°†æŒç»­å­¦ä¹  (continuous learning) å½¢å¼åŒ–ä¸ºå†…åŒ–ç”²è°• (internalized oracle) æœºåˆ¶ï¼Œå¹¶é€šè¿‡ç¥ç»åšå¼ˆè®º (neural game theory) æ¡†æ¶å®ç°ã€‚æœ€åï¼Œæ–‡ç« æå‡ºäº†è®¡ç®—ç±»å¯¹é½ (Computational Class Alignment, CCA) åŸåˆ™ï¼Œè¦æ±‚ä»»åŠ¡å¤æ‚åº¦ä¸ç³»ç»Ÿè®¡ç®—èƒ½åŠ›ä¸¥æ ¼åŒ¹é…ã€‚è¯¥ç ”ç©¶ä¸º RAG çš„æœ‰æ•ˆæ€§æä¾›äº†é¦–ä¸ªå½¢å¼åŒ–ç†è®ºï¼Œå¹¶ä¸ºäººå·¥æ™ºèƒ½å®‰å…¨åº”ç”¨æä¾›äº†é‡è¦çš„ç†è®ºæ”¯æ’‘ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "8 pages, 6 figures",
      "pdf_url": "https://arxiv.org/pdf/2508.07334v2",
      "published_date": "2025-08-10 13:26:36 UTC",
      "updated_date": "2025-12-08 16:50:21 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T10:43:27.956965+00:00"
    },
    {
      "arxiv_id": "2508.07329v1",
      "title": "Efficient Edge LLMs Deployment via HessianAware Quantization and CPU GPU Collaborative",
      "title_zh": "åŸºäº Hessian æ„ŸçŸ¥é‡åŒ–ä¸ CPU-GPU ååŒçš„è¾¹ç¼˜å¤§è¯­è¨€æ¨¡å‹é«˜æ•ˆéƒ¨ç½²",
      "authors": [
        "Tuo Zhang",
        "Ning Li",
        "Xin Yuan",
        "Wenchao Xu",
        "Quan Chen",
        "Song Guo",
        "Haijun Zhang"
      ],
      "abstract": "With the breakthrough progress of large language models (LLMs) in natural language processing and multimodal tasks, efficiently deploying them on resource-constrained edge devices has become a critical challenge. The Mixture of Experts (MoE) architecture enhances model capacity through sparse activation, but faces two major difficulties in practical deployment: (1) The presence of numerous outliers in activation distributions leads to severe degradation in quantization accuracy for both activations and weights, significantly impairing inference performance; (2) Under limited memory, efficient offloading and collaborative inference of expert modules struggle to balance latency and throughput. To address these issues, this paper proposes an efficient MoE edge deployment scheme based on Hessian-Aware Quantization (HAQ) and CPU-GPU collaborative inference. First, by introducing smoothed Hessian matrix quantization, we achieve joint 8-bit quantization of activations and weights, which significantly alleviates the accuracy loss caused by outliers while ensuring efficient implementation on mainstream hardware. Second, we design an expert-level collaborative offloading and inference mechanism, which, combined with expert activation path statistics, enables efficient deployment and scheduling of expert modules between CPU and GPU, greatly reducing memory footprint and inference latency. Extensive experiments validate the effectiveness of our method on mainstream large models such as the OPT series and Mixtral 8*7B: on datasets like Wikitext2 and C4, the inference accuracy of the low-bit quantized model approaches that of the full-precision model, while GPU memory usage is reduced by about 60%, and inference latency is significantly improved.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤§è¯­è¨€æ¨¡å‹(LLMs)ç‰¹åˆ«æ˜¯ä¸“å®¶æ··åˆ(Mixture of Experts, MoE)æ¶æ„åœ¨èµ„æºå—é™è¾¹ç¼˜è®¾å¤‡éƒ¨ç½²ä¸­é¢ä¸´çš„é‡åŒ–ç²¾åº¦ä¸‹é™å’Œæ˜¾å­˜é™åˆ¶ç­‰æŒ‘æˆ˜ï¼Œæå‡ºäº†ä¸€ç§åŸºäºHessianæ„ŸçŸ¥é‡åŒ–(Hessian-Aware Quantization, HAQ)å’ŒCPU-GPUåä½œæ¨ç†çš„é«˜æ•ˆéƒ¨ç½²æ–¹æ¡ˆã€‚è¯¥æ–¹æ¡ˆé€šè¿‡å¼•å…¥å¹³æ»‘çš„HessiançŸ©é˜µé‡åŒ–æŠ€æœ¯ï¼Œå®ç°äº†æ¿€æ´»å€¼å’Œæƒé‡çš„è”åˆ8-bité‡åŒ–ï¼Œæœ‰æ•ˆç¼“è§£äº†ç”±ç¦»ç¾¤å€¼å¯¼è‡´çš„ç²¾åº¦æŸå¤±ï¼Œå¹¶ç¡®ä¿äº†åœ¨ä¸»æµç¡¬ä»¶ä¸Šçš„é«˜æ•ˆæ‰§è¡Œã€‚åŒæ—¶ï¼Œç ”ç©¶è®¾è®¡äº†ä¸€ç§ä¸“å®¶çº§çš„åä½œå¸è½½ä¸æ¨ç†æœºåˆ¶ï¼Œç»“åˆä¸“å®¶æ¿€æ´»è·¯å¾„ç»Ÿè®¡å®ç°äº†CPUä¸GPUé—´çš„é«˜æ•ˆè°ƒåº¦ï¼Œæ˜¾è‘—é™ä½äº†å†…å­˜å ç”¨å’Œæ¨ç†å»¶è¿Ÿã€‚åœ¨OPTç³»åˆ—å’ŒMixtral 8*7Bç­‰ä¸»æµæ¨¡å‹ä¸Šçš„å®éªŒéªŒè¯è¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨ä¿æŒæ¥è¿‘å…¨ç²¾åº¦æ¨ç†å‡†ç¡®ç‡çš„åŒæ—¶ï¼Œå°†æ˜¾å­˜å ç”¨å‡å°‘äº†çº¦60%ï¼Œå¹¶å¤§å¹…æå‡äº†æ¨ç†é€Ÿåº¦ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.07329v1",
      "published_date": "2025-08-10 12:59:57 UTC",
      "updated_date": "2025-08-10 12:59:57 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T10:43:33.054946+00:00"
    },
    {
      "arxiv_id": "2508.10039v1",
      "title": "Multi-task Adversarial Attacks against Black-box Model with Few-shot Queries",
      "title_zh": "é’ˆå¯¹å°‘æ¬¡æŸ¥è¯¢ä¸‹é»‘ç›’æ¨¡å‹çš„å¤šä»»åŠ¡å¯¹æŠ—æ”»å‡»",
      "authors": [
        "Wenqiang Wang",
        "Yan Xiao",
        "Hao Lin",
        "Yangshijie Zhang",
        "Xiaochun Cao"
      ],
      "abstract": "Current multi-task adversarial text attacks rely on abundant access to shared internal features and numerous queries, often limited to a single task type. As a result, these attacks are less effective against practical scenarios involving black-box feedback APIs, limited queries, or multiple task types. To bridge this gap, we propose \\textbf{C}luster and \\textbf{E}nsemble \\textbf{M}ulti-task Text Adversarial \\textbf{A}ttack (\\textbf{CEMA}), an effective black-box attack that exploits the transferability of adversarial texts across different tasks. CEMA simplifies complex multi-task scenarios by using a \\textit{deep-level substitute model} trained in a \\textit{plug-and-play} manner for text classification, enabling attacks without mimicking the victim model. This approach requires only a few queries for training, converting multi-task attacks into classification attacks and allowing attacks across various tasks.\n  CEMA generates multiple adversarial candidates using different text classification methods and selects the one that most effectively attacks substitute models.\n  In experiments involving multi-task models with two, three, or six tasks--spanning classification, translation, summarization, and text-to-image generation--CEMA demonstrates significant attack success with as few as 100 queries. Furthermore, CEMA can target commercial APIs (e.g., Baidu and Google Translate), large language models (e.g., ChatGPT 4o), and image-generation models (e.g., Stable Diffusion V2), showcasing its versatility and effectiveness in real-world applications.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† Cluster and Ensemble Multi-task Text Adversarial Attack (CEMA)ï¼Œè¿™æ˜¯ä¸€ç§é’ˆå¯¹é»‘ç›’æ¨¡å‹çš„é«˜æ•ˆå¤šä»»åŠ¡æ–‡æœ¬å¯¹æŠ—æ”»å‡»æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³ç°æœ‰æ”»å‡»æ–¹æ³•åœ¨é»‘ç›’åé¦ˆã€æŸ¥è¯¢å—é™(Limited queries)åŠå¤šä»»åŠ¡ç±»å‹åœºæ™¯ä¸‹æœ‰æ•ˆæ€§ä¸è¶³çš„é—®é¢˜ã€‚CEMA æ ¸å¿ƒåˆ©ç”¨äº†å¯¹æŠ—æ–‡æœ¬åœ¨ä¸åŒä»»åŠ¡é—´çš„å¯è¿ç§»æ€§(Transferability)ï¼Œé€šè¿‡ä¸€ç§å³æ’å³ç”¨(Plug-and-play)çš„æ·±åº¦æ›¿ä»£æ¨¡å‹(Deep-level substitute model)å°†å¤æ‚çš„å¤šä»»åŠ¡æ”»å‡»è½¬åŒ–ä¸ºåˆ†ç±»æ”»å‡»ï¼Œæ˜¾è‘—é™ä½äº†å¯¹ç›®æ ‡æ¨¡å‹æŸ¥è¯¢æ¬¡æ•°çš„è¦æ±‚ã€‚è¯¥æ¡†æ¶é€šè¿‡é›†æˆä¸åŒçš„æ–‡æœ¬åˆ†ç±»æ–¹æ³•ç”Ÿæˆå¤šä¸ªå€™é€‰æ ·æœ¬ï¼Œå¹¶ä»ä¸­ç­›é€‰å‡ºèƒ½æœ€æœ‰æ•ˆæ”»å‡»æ›¿ä»£æ¨¡å‹çš„å¯¹æŠ—æ–‡æœ¬ã€‚åœ¨æ¶‰åŠåˆ†ç±»ã€ç¿»è¯‘ã€æ‘˜è¦å’Œæ–‡æœ¬ç”Ÿæˆå›¾åƒ(Text-to-image generation)çš„å¤šä»»åŠ¡å®éªŒä¸­ï¼ŒCEMA ä»…éœ€ 100 æ¬¡æŸ¥è¯¢å³å¯è·å¾—æ˜¾è‘—çš„æ”»å‡»æˆåŠŸç‡ã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•æˆåŠŸéªŒè¯äº†å¯¹ç™¾åº¦å’Œè°·æ­Œç¿»è¯‘ç­‰å•†ä¸š APIã€ChatGPT 4o ä»¥åŠ Stable Diffusion V2 ç­‰å‰æ²¿æ¨¡å‹çš„æœ‰æ•ˆæ€§ï¼Œå±•ç°äº†å…¶åœ¨ç°å®ä¸–ç•Œåº”ç”¨åœºæ™¯ä¸­çš„å¹¿æ³›é€šç”¨æ€§ä¸å®é™…å¨èƒã€‚",
      "categories": [
        "cs.CR",
        "cs.AI"
      ],
      "primary_category": "cs.CR",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.10039v1",
      "published_date": "2025-08-10 12:46:47 UTC",
      "updated_date": "2025-08-10 12:46:47 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T10:43:45.554565+00:00"
    },
    {
      "arxiv_id": "2508.07325v1",
      "title": "Strategies of Code-switching in Human-Machine Dialogs",
      "title_zh": "äººæœºå¯¹è¯ä¸­çš„è¯­ç è½¬æ¢ç­–ç•¥",
      "authors": [
        "Dean Geckt",
        "Melinda Fricke",
        "Shuly Wintner"
      ],
      "abstract": "Most people are multilingual, and most multilinguals code-switch, yet the characteristics of code-switched language are not fully understood. We developed a chatbot capable of completing a Map Task with human participants using code-switched Spanish and English. In two experiments, we prompted the bot to code-switch according to different strategies, examining (1) the feasibility of such experiments for investigating bilingual language use, and (2) whether participants would be sensitive to variations in discourse and grammatical patterns. Participants generally enjoyed code-switching with our bot as long as it produced predictable code-switching behavior; when code-switching was random or ungrammatical (as when producing unattested incongruent mixed-language noun phrases, such as `la fork'), participants enjoyed the task less and were less successful at completing it. These results underscore the potential downsides of deploying insufficiently developed multilingual language technology, while also illustrating the promise of such technology for conducting research on bilingual language use.",
      "tldr_zh": "è¯¥ç ”ç©¶é€šè¿‡å¼€å‘ä¸€ä¸ªèƒ½å¤Ÿä½¿ç”¨è¥¿ç­ç‰™è¯­å’Œè‹±è¯­ code-switching è¿›è¡Œ Map Task çš„èŠå¤©æœºå™¨äººï¼Œæ·±å…¥æ¢è®¨äº†äººæœºå¯¹è¯ä¸­è¯­ç è½¬æ¢çš„ç‰¹å¾ã€‚ç ”ç©¶å›¢é˜Ÿé€šè¿‡ä¸¤é¡¹å®éªŒéªŒè¯äº†åˆ©ç”¨æ­¤ç±»æŠ€æœ¯è¿›è¡ŒåŒè¯­è¯­è¨€ç ”ç©¶çš„å¯è¡Œæ€§ï¼Œå¹¶é‡ç‚¹è€ƒå¯Ÿäº†å‚ä¸è€…å¯¹ä¸åŒè¯è¯­å’Œè¯­æ³•æ¨¡å¼å˜å¼‚çš„æ•æ„Ÿç¨‹åº¦ã€‚ç»“æœè¡¨æ˜ï¼Œå‚ä¸è€…åœ¨æœºå™¨äººè¡¨ç°å‡ºå¯é¢„æµ‹çš„ code-switching è¡Œä¸ºæ—¶å…·æœ‰è¾ƒé«˜çš„å‚ä¸åº¦å’ŒæˆåŠŸç‡ã€‚ç„¶è€Œï¼Œå½“æœºå™¨äººäº§ç”Ÿéšæœºæˆ–è¿åè¯­æ³•è§„åˆ™çš„è½¬æ¢ï¼ˆä¾‹å¦‚å‡ºç° \"la fork\" è¿™ç§ä¸ç¬¦åˆä¹ æƒ¯çš„æ··åˆè¯­è¨€åè¯çŸ­è¯­ï¼‰æ—¶ï¼Œå‚ä¸è€…çš„ä»»åŠ¡è¡¨ç°å’Œæ»¡æ„åº¦å‡æ˜¾è‘—ä¸‹é™ã€‚è¯¥ç ”ç©¶ç»“æœå¼ºè°ƒäº†éƒ¨ç½²å¼€å‘ä¸å……åˆ†çš„å¤šè¯­è¨€è¯­è¨€æŠ€æœ¯å¯èƒ½å¸¦æ¥çš„è´Ÿé¢å½±å“ã€‚åŒæ—¶ï¼Œè¿™ä¸€æˆæœä¹Ÿå±•ç¤ºäº†åˆ©ç”¨å…ˆè¿›å¯¹è¯æŠ€æœ¯æ¨åŠ¨åŒè¯­è¯­è¨€ä½¿ç”¨ç ”ç©¶çš„å¹¿é˜”å‰æ™¯ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.07325v1",
      "published_date": "2025-08-10 12:41:46 UTC",
      "updated_date": "2025-08-10 12:41:46 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T10:43:39.155238+00:00"
    },
    {
      "arxiv_id": "2508.07321v1",
      "title": "ObfusQAte: A Proposed Framework to Evaluate LLM Robustness on Obfuscated Factual Question Answering",
      "title_zh": "ObfusQAteï¼šè¯„ä¼°å¤§è¯­è¨€æ¨¡å‹åœ¨æ··æ·†äº‹å®é—®ç­”ä»»åŠ¡ä¸­é²æ£’æ€§çš„æ‹Ÿè®®æ¡†æ¶",
      "authors": [
        "Shubhra Ghosh",
        "Abhilekh Borah",
        "Aditya Kumar Guru",
        "Kripabandhu Ghosh"
      ],
      "abstract": "The rapid proliferation of Large Language Models (LLMs) has significantly contributed to the development of equitable AI systems capable of factual question-answering (QA). However, no known study tests the LLMs' robustness when presented with obfuscated versions of questions. To systematically evaluate these limitations, we propose a novel technique, ObfusQAte and, leveraging the same, introduce ObfusQA, a comprehensive, first of its kind, framework with multi-tiered obfuscation levels designed to examine LLM capabilities across three distinct dimensions: (i) Named-Entity Indirection, (ii) Distractor Indirection, and (iii) Contextual Overload. By capturing these fine-grained distinctions in language, ObfusQA provides a comprehensive benchmark for evaluating LLM robustness and adaptability. Our study observes that LLMs exhibit a tendency to fail or generate hallucinated responses when confronted with these increasingly nuanced variations. To foster research in this direction, we make ObfusQAte publicly available.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† ObfusQAteï¼Œè¿™æ˜¯ä¸€ä¸ªæ—¨åœ¨è¯„ä¼°å¤§è¯­è¨€æ¨¡å‹ (LLMs) åœ¨å¤„ç†æ··æ·†åçš„äº‹å®æ€§é—®ç­” (Factual Question Answering) æ—¶é²æ£’æ€§çš„å…¨æ–°æ¡†æ¶ã€‚ç ”ç©¶è€…é€šè¿‡è¯¥æŠ€æœ¯æ„å»ºäº†é¦–ä¸ªå¤šå±‚çº§æ··æ·†æ¡†æ¶ ObfusQAï¼Œä»å‘½åå®ä½“é—´æ¥æ€§ (Named-Entity Indirection)ã€å¹²æ‰°é¡¹é—´æ¥æ€§ (Distractor Indirection) å’Œä¸Šä¸‹æ–‡è¿‡è½½ (Contextual Overload) ä¸‰ä¸ªå…³é”®ç»´åº¦å¯¹æ¨¡å‹è¿›è¡Œå‹åŠ›æµ‹è¯•ã€‚ç ”ç©¶å‘ç°ï¼Œå½“é¢å¯¹è¿™äº›ç»†å¾®ä¸”å¤æ‚çš„è¯­è¨€å˜åŒ–æ—¶ï¼Œä¸»æµ LLMs è¡¨ç°å‡ºæ˜æ˜¾çš„æ€§èƒ½ä¸‹é™ï¼Œææ˜“äº§ç”Ÿå¹»è§‰ (Hallucination) æˆ–ç›´æ¥æ— æ³•å›ç­”ã€‚ObfusQA ä¸ºè¯„ä¼°æ¨¡å‹çš„é€‚åº”æ€§æä¾›äº†å…¨é¢çš„åŸºå‡†æµ‹è¯•ï¼Œæ­ç¤ºäº†å½“å‰æ¨¡å‹åœ¨å¤„ç†éè§„èŒƒåŒ–è¡¨è¾¾æ—¶çš„å±€é™ã€‚ä¸ºäº†ä¿ƒè¿›è¯¥é¢†åŸŸçš„è¿›ä¸€æ­¥æ¢ç´¢ï¼Œç ”ç©¶å›¢é˜Ÿå·²å°† ObfusQAte æ¡†æ¶å…¬å¼€ï¼Œä¸ºæå‡ AI ç³»ç»Ÿçš„å…¬å¹³æ€§ä¸å¯é æ€§æä¾›äº†é‡è¦å·¥å…·ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.07321v1",
      "published_date": "2025-08-10 12:27:52 UTC",
      "updated_date": "2025-08-10 12:27:52 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T10:43:41.560260+00:00"
    },
    {
      "arxiv_id": "2508.07315v2",
      "title": "FlexCTC: GPU-powered CTC Beam Decoding With Advanced Contextual Abilities",
      "title_zh": "FlexCTCï¼šå…·å¤‡å…ˆè¿›ä¸Šä¸‹æ–‡èƒ½åŠ›çš„ GPU åŠ é€Ÿ CTC é›†æŸè§£ç ",
      "authors": [
        "Lilit Grigoryan",
        "Vladimir Bataev",
        "Nikolay Karpov",
        "Andrei Andrusenko",
        "Vitaly Lavrukhin",
        "Boris Ginsburg"
      ],
      "abstract": "While beam search improves speech recognition quality over greedy decoding, standard implementations are slow, often sequential, and CPU-bound. To fully leverage modern hardware capabilities, we present a novel open-source FlexCTC toolkit for fully GPU-based beam decoding, designed for Connectionist Temporal Classification (CTC) models. Developed entirely in Python and PyTorch, it offers a fast, user-friendly, and extensible alternative to traditional C++, CUDA, or WFST-based decoders. The toolkit features a high-performance, fully batched GPU implementation with eliminated CPU-GPU synchronization and minimized kernel launch overhead via CUDA Graphs. It also supports advanced contextualization techniques, including GPU-powered N-gram language model fusion and phrase-level boosting. These features enable accurate and efficient decoding, making them suitable for both research and production use.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¨å‡ºäº†FlexCTCï¼Œä¸€ä¸ªå¼€æºä¸”å®Œå…¨åŸºäºGPUé©±åŠ¨çš„æ³¢æŸæœç´¢(beam decoding)å·¥å…·åŒ…ï¼Œæ—¨åœ¨ä¼˜åŒ–Connectionist Temporal Classification (CTC) æ¨¡å‹çš„è¯­éŸ³è¯†åˆ«è§£ç æ•ˆç‡ã€‚é’ˆå¯¹ä¼ ç»Ÿè§£ç æ–¹æ³•é€Ÿåº¦æ…¢ã€CPUå—é™ä¸”éš¾ä»¥å¹¶è¡Œçš„é—®é¢˜ï¼ŒFlexCTCåœ¨Pythonå’ŒPyTorchæ¡†æ¶ä¸‹å®ç°äº†å…¨æ‰¹å¤„ç†çš„GPUè§£ç æ¶æ„ï¼Œå¹¶é€šè¿‡CUDA GraphsæŠ€æœ¯æ˜¾è‘—å‡å°‘äº†å†…æ ¸å¯åŠ¨å¼€é”€ã€‚è¯¥å·¥å…·åŒ…é›†æˆäº†é«˜çº§ä¸Šä¸‹æ–‡å¤„ç†èƒ½åŠ›ï¼Œæ”¯æŒGPUåŠ é€Ÿçš„N-gramè¯­è¨€æ¨¡å‹èåˆ(language model fusion)å’ŒçŸ­è¯­çº§å¢å¼º(phrase-level boosting)ã€‚ç›¸æ¯”äºä¼ ç»Ÿçš„C++ã€CUDAæˆ–WFSTè§£ç å™¨ï¼ŒFlexCTCä¸ä»…æä¾›äº†æ›´å¥½çš„æ˜“ç”¨æ€§å’Œå¯æ‰©å±•æ€§ï¼Œè¿˜å®ç°äº†æ›´ä¸ºç²¾å‡†é«˜æ•ˆçš„è§£ç ã€‚è¯¥å·¥å…·åŒ…çš„å¼€æºç‰¹æ€§åŠå…¶å“è¶Šçš„æ€§èƒ½è¡¨ç°ï¼Œä½¿å…¶èƒ½å¤Ÿå¹¿æ³›é€‚ç”¨äºè¯­éŸ³è¯†åˆ«é¢†åŸŸçš„å­¦æœ¯ç ”ç©¶ä¸å·¥ä¸šç”Ÿäº§å®è·µã€‚",
      "categories": [
        "eess.AS",
        "cs.AI",
        "cs.CL",
        "cs.LG",
        "cs.SD"
      ],
      "primary_category": "eess.AS",
      "comment": "Accepted to Automatic Speech Recognition and Understanding Workshop (ASRU) 2025",
      "pdf_url": "https://arxiv.org/pdf/2508.07315v2",
      "published_date": "2025-08-10 12:15:57 UTC",
      "updated_date": "2025-08-13 15:09:16 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T10:43:50.293838+00:00"
    },
    {
      "arxiv_id": "2508.07308v1",
      "title": "HealthBranches: Synthesizing Clinically-Grounded Question Answering Datasets via Decision Pathways",
      "title_zh": "HealthBranchesï¼šåŸºäºå†³ç­–è·¯å¾„åˆæˆå…·æœ‰ä¸´åºŠä¾æ®çš„é—®ç­”æ•°æ®é›†",
      "authors": [
        "Cristian Cosentino",
        "Annamaria Defilippo",
        "Marco Dossena",
        "Christopher Irwin",
        "Sara Joubbi",
        "Pietro LiÃ²"
      ],
      "abstract": "HealthBranches is a novel benchmark dataset for medical Question-Answering (Q&A), specifically designed to evaluate complex reasoning in Large Language Models (LLMs). This dataset is generated through a semi-automated pipeline that transforms explicit decision pathways from medical source into realistic patient cases with associated questions and answers. Covering 4,063 case studies across 17 healthcare topics, each data point is based on clinically validated reasoning chains. HealthBranches supports both open-ended and multiple-choice question formats and uniquely includes the full reasoning path for each Q&A. Its structured design enables robust evaluation of LLMs' multi-step inference capabilities, including their performance in structured Retrieval-Augmented Generation (RAG) contexts. HealthBranches establishes a foundation for the development of more trustworthy, interpretable, and clinically reliable LLMs in high-stakes domains while also serving as a valuable resource for educational purposes.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¨å‡ºäº†HealthBranchesï¼Œè¿™æ˜¯ä¸€ä¸ªä¸“é—¨ç”¨äºè¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹(Large Language Models, LLMs)å¤æ‚æ¨ç†èƒ½åŠ›çš„åŒ»ç–—é—®ç­”(Question-Answering)åŸºå‡†æ•°æ®é›†ã€‚è¯¥æ•°æ®é›†é€šè¿‡ä¸€ç§åŠè‡ªåŠ¨åŒ–çš„æµæ°´çº¿ï¼Œå°†åŒ»ç–—æ¥æºçš„æ˜ç¡®å†³ç­–è·¯å¾„(decision pathways)è½¬åŒ–ä¸ºçœŸå®çš„æ‚£è€…æ¡ˆä¾‹åŠç›¸å…³é—®é¢˜ã€‚HealthBranchesæ¶µç›–äº†17ä¸ªåŒ»ç–—ä¸»é¢˜ä¸‹çš„4,063ä¸ªæ¡ˆä¾‹ç ”ç©¶ï¼Œæ¯ä¸ªæ•°æ®ç‚¹éƒ½åŸºäºç»è¿‡ä¸´åºŠéªŒè¯çš„æ¨ç†é“¾ã€‚è¯¥æ•°æ®é›†æ”¯æŒå¼€æ”¾å¼å’Œå¤šé€‰é¢˜æ ¼å¼ï¼Œå¹¶ç‹¬ç‰¹åœ°åŒ…å«äº†æ¯ä¸ªé—®ç­”çš„å®Œæ•´æ¨ç†è·¯å¾„ï¼Œå…¶ç»“æ„åŒ–è®¾è®¡èƒ½å¤Ÿæœ‰æ•ˆè¯„ä¼°LLMsçš„å¤šæ­¥æ¨ç†èƒ½åŠ›ã€‚æ­¤å¤–ï¼ŒHealthBranchesè¿˜æ”¯æŒåœ¨ç»“æ„åŒ–æ£€ç´¢å¢å¼ºç”Ÿæˆ(Retrieval-Augmented Generation, RAG)èƒŒæ™¯ä¸‹çš„æ€§èƒ½è¯„ä¼°ã€‚è¯¥ç ”ç©¶ä¸ºå¼€å‘åœ¨åŒ»ç–—ç­‰é«˜é£é™©é¢†åŸŸä¸­æ›´å…·å¯ä¿¡åº¦ã€å¯è§£é‡Šæ€§å’Œä¸´åºŠå¯é æ€§çš„LLMså¥ å®šäº†åŸºç¡€ï¼ŒåŒæ—¶ä¹Ÿä¸ºåŒ»å­¦æ•™è‚²æä¾›äº†å®è´µèµ„æºã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.IR",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.07308v1",
      "published_date": "2025-08-10 11:45:34 UTC",
      "updated_date": "2025-08-10 11:45:34 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T10:44:29.188686+00:00"
    },
    {
      "arxiv_id": "2508.07307v3",
      "title": "MCITlib: Multimodal Continual Instruction Tuning Library and Benchmark",
      "title_zh": "MCITlibï¼šå¤šæ¨¡æ€æŒç»­æŒ‡ä»¤å¾®è°ƒåº“ä¸åŸºå‡†",
      "authors": [
        "Haiyang Guo",
        "Fei Zhu",
        "Hongbo Zhao",
        "Fanhu Zeng",
        "Wenzhuo Liu",
        "Shijie Ma",
        "Da-Han Wang",
        "Xu-Yao Zhang"
      ],
      "abstract": "Continual learning enables AI systems to acquire new knowledge while retaining previously learned information. While traditional unimodal methods have made progress, the rise of Multimodal Large Language Models (MLLMs) brings new challenges in Multimodal Continual Learning (MCL), where models are expected to address both catastrophic forgetting and cross-modal coordination. To advance research in this area, we present MCITlib, a comprehensive library for Multimodal Continual Instruction Tuning. MCITlib currently implements 8 representative algorithms and conducts evaluations on 3 benchmarks under 2 backbone models. The library will be continuously updated to support future developments in MCL. The codebase is released at https://github.com/Ghy0501/MCITlib.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨å¤šæ¨¡æ€æŒç»­å­¦ä¹ ï¼ˆMultimodal Continual Learning, MCLï¼‰ä¸­é¢ä¸´çš„ç¾éš¾æ€§é—å¿˜å’Œè·¨æ¨¡æ€åè°ƒæŒ‘æˆ˜ï¼Œæå‡ºäº† MCITlibã€‚MCITlib æ˜¯ä¸€ä¸ªä¸“é—¨ç”¨äºå¤šæ¨¡æ€æŒç»­æŒ‡ä»¤å¾®è°ƒï¼ˆMultimodal Continual Instruction Tuningï¼‰çš„ç»¼åˆæ€§åº“å’ŒåŸºå‡†æµ‹è¯•å¹³å°ã€‚è¯¥åº“ç›®å‰é›†æˆäº† 8 ç§å…·æœ‰ä»£è¡¨æ€§çš„ç®—æ³•ï¼Œå¹¶åœ¨ 2 ç§éª¨å¹²æ¨¡å‹ï¼ˆbackbone modelsï¼‰ä¸Šé’ˆå¯¹ 3 ä¸ªåŸºå‡†æµ‹è¯•è¿›è¡Œäº†å…¨é¢è¯„ä¼°ã€‚MCITlib æ—¨åœ¨é€šè¿‡æä¾›æ ‡å‡†åŒ–çš„å·¥å…·å’ŒæŒç»­æ›´æ–°çš„æ¡†æ¶ï¼Œæ¨åŠ¨ MCL é¢†åŸŸçš„ç ”ç©¶è¿›å±•ã€‚è¯¥é¡¹ç›®çš„ä»£ç åº“å·²ç»å¼€æºï¼Œä¸ºè§£å†³ MLLM åœ¨åŠ¨æ€è·å–æ–°çŸ¥è¯†æ—¶çš„çŸ¥è¯†ä¿ç•™é—®é¢˜æä¾›äº†é‡è¦çš„èµ„æºæ”¯æŒã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "Preprint",
      "pdf_url": "https://arxiv.org/pdf/2508.07307v3",
      "published_date": "2025-08-10 11:42:36 UTC",
      "updated_date": "2025-12-31 07:27:23 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T10:45:05.719120+00:00"
    },
    {
      "arxiv_id": "2508.07306v1",
      "title": "DragonFruitQualityNet: A Lightweight Convolutional Neural Network for Real-Time Dragon Fruit Quality Inspection on Mobile Devices",
      "title_zh": "DragonFruitQualityNetï¼šç”¨äºç§»åŠ¨è®¾å¤‡ç«é¾™æœå“è´¨å®æ—¶æ£€æµ‹çš„è½»é‡çº§å·ç§¯ç¥ç»ç½‘ç»œ",
      "authors": [
        "Md Zahurul Haquea",
        "Yeahyea Sarker",
        "Muhammed Farhan Sadique Mahi",
        "Syed Jubayer Jaman",
        "Md Robiul Islam"
      ],
      "abstract": "Dragon fruit, renowned for its nutritional benefits and economic value, has experienced rising global demand due to its affordability and local availability. As dragon fruit cultivation expands, efficient pre- and post-harvest quality inspection has become essential for improving agricultural productivity and minimizing post-harvest losses. This study presents DragonFruitQualityNet, a lightweight Convolutional Neural Network (CNN) optimized for real-time quality assessment of dragon fruits on mobile devices. We curated a diverse dataset of 13,789 images, integrating self-collected samples with public datasets (dataset from Mendeley Data), and classified them into four categories: fresh, immature, mature, and defective fruits to ensure robust model training. The proposed model achieves an impressive 93.98% accuracy, outperforming existing methods in fruit quality classification. To facilitate practical adoption, we embedded the model into an intuitive mobile application, enabling farmers and agricultural stakeholders to conduct on-device, real-time quality inspections. This research provides an accurate, efficient, and scalable AI-driven solution for dragon fruit quality control, supporting digital agriculture and empowering smallholder farmers with accessible technology. By bridging the gap between research and real-world application, our work advances post-harvest management and promotes sustainable farming practices.",
      "tldr_zh": "æœ¬ç ”ç©¶æå‡ºäº† DragonFruitQualityNetï¼Œè¿™æ˜¯ä¸€ç§ä¸“é—¨ä¸ºç§»åŠ¨è®¾å¤‡ä¼˜åŒ–çš„è½»é‡çº§å·ç§¯ç¥ç»ç½‘ç»œ (Convolutional Neural Network, CNN)ï¼Œæ—¨åœ¨å®ç°ç«é¾™æœè´¨é‡çš„å®æ—¶æ£€æµ‹ã€‚ç ”ç©¶å›¢é˜Ÿæ•´åˆäº†è‡ªé‡‡é›†ä¸å…¬å¼€æ•°æ®é›†ï¼Œæ„å»ºäº†åŒ…å« 13,789 å¼ å›¾åƒçš„å¤§å‹æ•°æ®é›†ï¼Œå¹¶å°†ç«é¾™æœç»†åˆ†ä¸ºæ–°é²œ (fresh)ã€æœªæˆç†Ÿ (immature)ã€æˆç†Ÿ (mature) å’Œæœ‰ç¼ºé™· (defective) å››ä¸ªç±»åˆ«è¿›è¡Œè®­ç»ƒã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ¨¡å‹åœ¨æ°´æœè´¨é‡åˆ†ç±»ä»»åŠ¡ä¸­è¾¾åˆ°äº† 93.98% çš„å‡†ç¡®ç‡ï¼Œæ€§èƒ½ä¼˜äºç°æœ‰æ–¹æ³•ã€‚é€šè¿‡å°†æ¨¡å‹åµŒå…¥ç›´è§‚çš„ç§»åŠ¨ç«¯åº”ç”¨ç¨‹åºï¼Œå†œæ°‘å’Œå†œä¸šä»ä¸šè€…å¯ä»¥åœ¨è®¾å¤‡ç«¯è¿›è¡Œé«˜æ•ˆçš„è´¨é‡æ£€æŸ¥ã€‚è¿™é¡¹å·¥ä½œä¸ºç«é¾™æœè´¨é‡æ§åˆ¶æä¾›äº†ä¸€ç§å¯æ‰©å±•çš„ AI é©±åŠ¨è§£å†³æ–¹æ¡ˆï¼Œåœ¨å‡å°‘äº§åæŸå¤±çš„åŒæ—¶ï¼Œæœ‰æ•ˆæ¨åŠ¨äº†æ•°å­—å†œä¸šå’Œå¯æŒç»­è€•ä½œå®è·µã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.07306v1",
      "published_date": "2025-08-10 11:41:23 UTC",
      "updated_date": "2025-08-10 11:41:23 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T10:44:47.195822+00:00"
    },
    {
      "arxiv_id": "2508.07304v1",
      "title": "From Knowledge to Conjectures: A Modal Framework for Reasoning about Hypotheses",
      "title_zh": "ä»çŸ¥è¯†åˆ°çŒœæƒ³ï¼šä¸€ç§ç”¨äºå‡è®¾æ¨ç†çš„æ¨¡æ€æ¡†æ¶",
      "authors": [
        "Fabio Vitali"
      ],
      "abstract": "This paper introduces a new family of cognitive modal logics designed to formalize conjectural reasoning: a modal system in which cognitive contexts extend known facts with hypothetical assumptions to explore their consequences. Unlike traditional doxastic and epistemic systems, conjectural logics rely on a principle, called Axiom C ($\\varphi \\rightarrow \\Box\\varphi$), that ensures that all established facts are preserved across hypothetical layers. While Axiom C was dismissed in the past due to its association with modal collapse, we show that the collapse only arises under classical and bivalent assumptions, and specifically in the presence of Axiom T. Hence we avoid Axiom T and adopt a paracomplete semantic framework, grounded in Weak Kleene logic or Description Logic, where undefined propositions coexist with modal assertions. This prevents the modal collapse and guarantees a layering to distinguish between factual and conjectural statements. Under this framework we define new modal systems, e.g., KC and KDC, and show that they are complete, decidable, and robust under partial knowledge. Finally, we introduce a dynamic operation, $\\mathsf{settle}(\\varphi)$, which formalizes the transition from conjecture to accepted fact, capturing the event of the update of a world's cognitive state through the resolution of uncertainty.",
      "tldr_zh": "è¯¥ç ”ç©¶å¼•å…¥äº†ä¸€ç³»åˆ—æ–°å‹è®¤çŸ¥æ¨¡æ€é€»è¾‘(cognitive modal logics)ï¼Œæ—¨åœ¨å½¢å¼åŒ–æ¨æµ‹æ€§æ¨ç†(conjectural reasoning)ï¼Œå³åœ¨å·²çŸ¥äº‹å®çš„åŸºç¡€ä¸Šæ‰©å±•å‡è®¾ä»¥æ¢ç´¢å…¶åæœã€‚ä¸ä¼ ç»Ÿçš„è®¤è¯†è®ºç³»ç»Ÿä¸åŒï¼Œè¯¥æ¡†æ¶é‡‡ç”¨äº†è¢«ç§°ä¸º Axiom C ($\\varphi \\rightarrow \\Box\\varphi$) çš„æ ¸å¿ƒåŸåˆ™ï¼Œç¡®ä¿æ‰€æœ‰æ—¢å®šäº‹å®åœ¨å‡è®¾å±‚çº§ä¸­å¾—ä»¥ä¿ç•™ã€‚ä¸ºäº†è§£å†³å†å²ä¸Šä¸ Axiom C ç›¸å…³çš„æ¨¡æ€å´©æºƒ(modal collapse)é—®é¢˜ï¼Œç ”ç©¶è€…é€šè¿‡æ”¾å¼ƒ Axiom T å¹¶é‡‡ç”¨åŸºäº Weak Kleene logic æˆ– Description Logic çš„å‰¯å®Œæ•´è¯­ä¹‰æ¡†æ¶(paracomplete semantic framework)ï¼Œå…è®¸æœªå®šä¹‰å‘½é¢˜ä¸æ¨¡æ€æ–­è¨€å¹¶å­˜ã€‚åœ¨æ­¤åŸºç¡€ä¸Šå®šä¹‰çš„ KC å’Œ KDC ç­‰æ–°ç³»ç»Ÿè¢«è¯æ˜å…·æœ‰å®Œå¤‡æ€§(complete)å’Œå¯åˆ¤å®šæ€§(decidable)ï¼Œä¸”åœ¨éƒ¨åˆ†çŸ¥è¯†(partial knowledge)ç¯å¢ƒä¸‹è¡¨ç°ç¨³å¥ã€‚æ­¤å¤–ï¼Œç ”ç©¶è¿˜å¼•å…¥äº†åŠ¨æ€æ“ä½œ $\\mathsf{settle}(\\varphi)$ï¼Œç”¨äºå½¢å¼åŒ–ä»æ¨æµ‹å‘å…¬è®¤äº‹å®çš„è½¬å˜ï¼ŒæˆåŠŸæ•æ‰äº†é€šè¿‡æ¶ˆé™¤ä¸ç¡®å®šæ€§æ¥æ›´æ–°è®¤çŸ¥çŠ¶æ€çš„è¿‡ç¨‹ã€‚",
      "categories": [
        "cs.LO",
        "cs.AI"
      ],
      "primary_category": "cs.LO",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.07304v1",
      "published_date": "2025-08-10 11:37:49 UTC",
      "updated_date": "2025-08-10 11:37:49 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T10:44:43.090180+00:00"
    },
    {
      "arxiv_id": "2508.07299v1",
      "title": "When Is Prior Knowledge Helpful? Exploring the Evaluation and Selection of Unsupervised Pretext Tasks from a Neuro-Symbolic Perspective",
      "title_zh": "å…ˆéªŒçŸ¥è¯†ä½•æ—¶æœ‰æ•ˆï¼Ÿä»ç¥ç»ç¬¦å·è§†è§’æ¢è®¨æ— ç›‘ç£å‰ç½®ä»»åŠ¡çš„è¯„ä¼°ä¸é€‰æ‹©",
      "authors": [
        "Lin-Han Jia",
        "Si-Yu Han",
        "Wen-Chao Hu",
        "Jie-Jing Shao",
        "Wen-Da Wei",
        "Zhi Zhou",
        "Lan-Zhe Guo",
        "Yu-Feng Li"
      ],
      "abstract": "Neuro-symbolic (Nesy) learning improves the target task performance of models by enabling them to satisfy knowledge, while semi/self-supervised learning (SSL) improves the target task performance by designing unsupervised pretext tasks for unlabeled data to make models satisfy corresponding assumptions. We extend the Nesy theory based on reliable knowledge to the scenario of unreliable knowledge (i.e., assumptions), thereby unifying the theoretical frameworks of SSL and Nesy. Through rigorous theoretical analysis, we demonstrate that, in theory, the impact of pretext tasks on target performance hinges on three factors: knowledge learnability with respect to the model, knowledge reliability with respect to the data, and knowledge completeness with respect to the target. We further propose schemes to operationalize these theoretical metrics, and thereby develop a method that can predict the effectiveness of pretext tasks in advance. This will change the current status quo in practical applications, where the selections of unsupervised tasks are heuristic-based rather than theory-based, and it is difficult to evaluate the rationality of unsupervised pretext task selection before testing the model on the target task. In experiments, we verify a high correlation between the predicted performance-estimated using minimal data-and the actual performance achieved after large-scale semi-supervised or self-supervised learning, thus confirming the validity of the theory and the effectiveness of the evaluation method.",
      "tldr_zh": "è¯¥ç ”ç©¶ä»ç¥ç»ç¬¦å·(Neuro-Symbolic)è§†è§’æ¢è®¨äº†å…ˆéªŒçŸ¥è¯†åœ¨æ— ç›‘ç£ä»£ç†ä»»åŠ¡(Unsupervised Pretext Tasks)è¯„ä¼°ä¸é€‰æ‹©ä¸­çš„ä½œç”¨ã€‚é€šè¿‡å°†åŸºäºå¯é çŸ¥è¯†çš„ç¥ç»ç¬¦å·ç†è®ºæ‰©å±•è‡³ä¸å¯é çŸ¥è¯†ï¼ˆå³å‡è®¾ï¼‰åœºæ™¯ï¼Œè¯¥ç ”ç©¶æˆåŠŸç»Ÿä¸€äº†åŠç›‘ç£/è‡ªç›‘ç£å­¦ä¹ (SSL)ä¸ç¥ç»ç¬¦å·å­¦ä¹ çš„ç†è®ºæ¡†æ¶ã€‚ç†è®ºåˆ†ææŒ‡å‡ºï¼Œä»£ç†ä»»åŠ¡å¯¹ç›®æ ‡æ€§èƒ½çš„å½±å“ä¸»è¦å–å†³äºçŸ¥è¯†çš„å¯å­¦ä¹ æ€§(Knowledge Learnability)ã€å¯é æ€§(Knowledge Reliability)å’Œå®Œå¤‡æ€§(Knowledge Completeness)ä¸‰ä¸ªæ ¸å¿ƒè¦ç´ ã€‚åŸºäºæ­¤ï¼Œç ”ç©¶è€…æå‡ºäº†ä¸€å¥—å¯æ“ä½œåŒ–çš„è¯„ä¼°æ–¹æ¡ˆï¼Œèƒ½å¤Ÿé¢„å…ˆé¢„æµ‹ä»£ç†ä»»åŠ¡çš„æœ‰æ•ˆæ€§ï¼Œä»è€Œå°†ç°æœ‰çš„å¯å‘å¼é€‰æ‹©è½¬å‘åŸºäºç†è®ºçš„ç†æ€§é€‰æ‹©ã€‚å®éªŒè¯æ˜ï¼Œåˆ©ç”¨æå°‘é‡æ•°æ®å¾—å‡ºçš„é¢„æµ‹æ€§èƒ½ä¸å¤§è§„æ¨¡å­¦ä¹ åçš„å®é™…ç»“æœé«˜åº¦ç›¸å…³ï¼Œæœ‰åŠ›æ”¯æ’‘äº†è¯¥ç†è®ºæ¡†æ¶åœ¨æŒ‡å¯¼æ— ç›‘ç£ä»»åŠ¡é€‰æ‹©æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.07299v1",
      "published_date": "2025-08-10 11:23:36 UTC",
      "updated_date": "2025-08-10 11:23:36 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T10:44:47.494816+00:00"
    },
    {
      "arxiv_id": "2508.13174v1",
      "title": "AlphaEval: A Comprehensive and Efficient Evaluation Framework for Formula Alpha Mining",
      "title_zh": "AlphaEvalï¼šä¸€ç§å…¨é¢ä¸”é«˜æ•ˆçš„å…¬å¼åŒ– Alpha æŒ–æ˜è¯„ä¼°æ¡†æ¶",
      "authors": [
        "Hongjun Ding",
        "Binqi Chen",
        "Jinsheng Huang",
        "Taian Guo",
        "Zhengyang Mao",
        "Guoyi Shao",
        "Lutong Zou",
        "Luchen Liu",
        "Ming Zhang"
      ],
      "abstract": "Formula alpha mining, which generates predictive signals from financial data, is critical for quantitative investment. Although various algorithmic approaches-such as genetic programming, reinforcement learning, and large language models-have significantly expanded the capacity for alpha discovery, systematic evaluation remains a key challenge. Existing evaluation metrics predominantly include backtesting and correlation-based measures. Backtesting is computationally intensive, inherently sequential, and sensitive to specific strategy parameters. Correlation-based metrics, though efficient, assess only predictive ability and overlook other crucial properties such as temporal stability, robustness, diversity, and interpretability. Additionally, the closed-source nature of most existing alpha mining models hinders reproducibility and slows progress in this field. To address these issues, we propose AlphaEval, a unified, parallelizable, and backtest-free evaluation framework for automated alpha mining models. AlphaEval assesses the overall quality of generated alphas along five complementary dimensions: predictive power, stability, robustness to market perturbations, financial logic, and diversity. Extensive experiments across representative alpha mining algorithms demonstrate that AlphaEval achieves evaluation consistency comparable to comprehensive backtesting, while providing more comprehensive insights and higher efficiency. Furthermore, AlphaEval effectively identifies superior alphas compared to traditional single-metric screening approaches. All implementations and evaluation tools are open-sourced to promote reproducibility and community engagement.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† AlphaEvalï¼Œè¿™æ˜¯ä¸€ä¸ªé’ˆå¯¹ Formula alpha mining è‡ªåŠ¨æŒ–æ˜æ¨¡å‹çš„ç»Ÿä¸€ã€å¯å¹¶è¡Œä¸”æ— éœ€å›æµ‹çš„ç»¼åˆè¯„ä¼°æ¡†æ¶ã€‚ç›®å‰ä¼ ç»Ÿçš„å›æµ‹æ–¹æ³•è®¡ç®—å¼€é”€å¤§ä¸”å¯¹å‚æ•°æ•æ„Ÿï¼Œè€ŒåŸºäºç›¸å…³æ€§çš„æŒ‡æ ‡åˆ™éš¾ä»¥æ¶µç›–ç¨³å®šæ€§ã€ç¨³å¥æ€§ã€é‡‘èé€»è¾‘å’Œå¤šæ ·æ€§ç­‰å…³é”®ç»´åº¦ã€‚AlphaEval ä» Predictive powerã€Stabilityã€Robustnessã€Financial logic å’Œ Diversity äº”ä¸ªäº’è¡¥ç»´åº¦å¯¹ç”Ÿæˆçš„ Alpha è¿›è¡Œå…¨é¢è¯„ä¼°ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒAlphaEval åœ¨ä¿æŒä¸å…¨é¢å›æµ‹ç›¸å½“çš„è¯„ä¼°ä¸€è‡´æ€§çš„åŒæ—¶ï¼Œæ˜¾è‘—æé«˜äº†è¯„ä¼°æ•ˆç‡å¹¶æä¾›äº†æ›´å…¨é¢çš„æ´å¯Ÿã€‚ç›¸è¾ƒäºä¼ ç»Ÿçš„å•ä¸€æŒ‡æ ‡ç­›é€‰æ–¹æ³•ï¼Œè¯¥æ¡†æ¶èƒ½æ›´æœ‰æ•ˆåœ°è¯†åˆ«å‡ºæ›´ä¼˜è´¨çš„ Alpha ä¿¡å·ã€‚æ­¤å¤–ï¼Œè¯¥é¡¹ç›®çš„å®ç°å’Œè¯„ä¼°å·¥å…·å·²å…¨éƒ¨å¼€æºï¼Œæ—¨åœ¨è§£å†³é¢†åŸŸå†…æ¨¡å‹é—­æºå¯¼è‡´çš„å¤ç°éš¾é¢˜å¹¶æ¨åŠ¨ç¤¾åŒºå…±åŒè¿›æ­¥ã€‚",
      "categories": [
        "cs.AI",
        "cs.LG",
        "q-fin.CP",
        "stat.ML"
      ],
      "primary_category": "cs.AI",
      "comment": "12 pages, 5 figures",
      "pdf_url": "https://arxiv.org/pdf/2508.13174v1",
      "published_date": "2025-08-10 11:19:24 UTC",
      "updated_date": "2025-08-10 11:19:24 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T10:44:56.692949+00:00"
    },
    {
      "arxiv_id": "2508.07297v1",
      "title": "Revisiting Data Attribution for Influence Functions",
      "title_zh": "é‡æ–°å®¡è§†å½±å“å‡½æ•°çš„æ•°æ®å½’å› ",
      "authors": [
        "Hongbo Zhu",
        "Angelo Cangelosi"
      ],
      "abstract": "The goal of data attribution is to trace the model's predictions through the learning algorithm and back to its training data. thereby identifying the most influential training samples and understanding how the model's behavior leads to particular predictions. Understanding how individual training examples influence a model's predictions is fundamental for machine learning interpretability, data debugging, and model accountability. Influence functions, originating from robust statistics, offer an efficient, first-order approximation to estimate the impact of marginally upweighting or removing a data point on a model's learned parameters and its subsequent predictions, without the need for expensive retraining. This paper comprehensively reviews the data attribution capability of influence functions in deep learning. We discuss their theoretical foundations, recent algorithmic advances for efficient inverse-Hessian-vector product estimation, and evaluate their effectiveness for data attribution and mislabel detection. Finally, highlighting current challenges and promising directions for unleashing the huge potential of influence functions in large-scale, real-world deep learning scenarios.",
      "tldr_zh": "è¯¥ç ”ç©¶æ·±å…¥æ¢è®¨äº†æ•°æ®å½’å› (Data Attribution)åœ¨æ·±åº¦å­¦ä¹ ä¸­çš„é‡è¦æ€§ï¼Œæ—¨åœ¨é€šè¿‡å½±å“å‡½æ•°(Influence Functions)å°†æ¨¡å‹é¢„æµ‹è¿½æº¯è‡³è®­ç»ƒæ•°æ®ï¼Œä»¥æå‡æ¨¡å‹çš„å¯è§£é‡Šæ€§å’Œé—®è´£åˆ¶ã€‚å½±å“å‡½æ•°ä½œä¸ºä¸€ç§æºäºé²æ£’ç»Ÿè®¡å­¦(robust statistics)çš„æŠ€æœ¯ï¼Œåˆ©ç”¨ä¸€é˜¶è¿‘ä¼¼(first-order approximation)åœ¨æ— éœ€é‡æ–°è®­ç»ƒçš„å‰æä¸‹ä¼°ç®—æ ·æœ¬æƒé‡å˜åŒ–å¯¹æ¨¡å‹çš„å½±å“ã€‚æœ¬æ–‡å…¨é¢ç»¼è¿°äº†å½±å“å‡½æ•°çš„ç†è®ºåŸºç¡€ï¼Œå¹¶é‡ç‚¹è®¨è®ºäº†é«˜æ•ˆä¼°ç®—é€†æµ·æ£®çŸ©é˜µå‘é‡ç§¯(inverse-Hessian-vector product)çš„æœ€æ–°ç®—æ³•ã€‚é€šè¿‡å¯¹æ•°æ®å½’å› å’Œé”™è¯¯æ ‡ç­¾æ£€æµ‹(mislabel detection)ä»»åŠ¡çš„è¯„ä¼°ï¼Œè®ºæ–‡éªŒè¯äº†è¯¥æŠ€æœ¯çš„æœ‰æ•ˆæ€§ã€‚æœ€åï¼Œä½œè€…é’ˆå¯¹å¤§è§„æ¨¡ç°å®æ·±åº¦å­¦ä¹ åœºæ™¯æå‡ºäº†å½“å‰é¢ä¸´çš„æŒ‘æˆ˜åŠæœªæ¥å‘å±•æ–¹å‘ï¼Œæ—¨åœ¨è¿›ä¸€æ­¥é‡Šæ”¾å½±å“å‡½æ•°çš„åº”ç”¨æ½œåŠ›ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.07297v1",
      "published_date": "2025-08-10 11:15:07 UTC",
      "updated_date": "2025-08-10 11:15:07 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T10:45:08.134889+00:00"
    },
    {
      "arxiv_id": "2508.07292v1",
      "title": "EndoAgent: A Memory-Guided Reflective Agent for Intelligent Endoscopic Vision-to-Decision Reasoning",
      "title_zh": "EndoAgentï¼šé¢å‘æ™ºèƒ½å†…çª¥é•œè§†è§‰-å†³ç­–æ¨ç†çš„è®°å¿†å¼•å¯¼åæ€å‹æ™ºèƒ½ä½“",
      "authors": [
        "Yi Tang",
        "Kaini Wang",
        "Yang Chen",
        "Guangquan Zhou"
      ],
      "abstract": "Developing general artificial intelligence (AI) systems to support endoscopic image diagnosis is an emerging research priority. Existing methods based on large-scale pretraining often lack unified coordination across tasks and struggle to handle the multi-step processes required in complex clinical workflows. While AI agents have shown promise in flexible instruction parsing and tool integration across domains, their potential in endoscopy remains underexplored. To address this gap, we propose EndoAgent, the first memory-guided agent for vision-to-decision endoscopic analysis that integrates iterative reasoning with adaptive tool selection and collaboration. Built on a dual-memory design, it enables sophisticated decision-making by ensuring logical coherence through short-term action tracking and progressively enhancing reasoning acuity through long-term experiential learning. To support diverse clinical tasks, EndoAgent integrates a suite of expert-designed tools within a unified reasoning loop. We further introduce EndoAgentBench, a benchmark of 5,709 visual question-answer pairs that assess visual understanding and language generation capabilities in realistic scenarios. Extensive experiments show that EndoAgent consistently outperforms both general and medical multimodal models, exhibiting its strong flexibility and reasoning capabilities.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†EndoAgentï¼Œè¿™æ˜¯é¦–ä¸ªç”¨äºå†…çª¥é•œä»è§†è§‰åˆ°å†³ç­–åˆ†æçš„è®°å¿†å¯¼å‘æ™ºèƒ½ä½“(memory-guided agent)ï¼Œæ—¨åœ¨è§£å†³ç°æœ‰AIåœ¨å¤æ‚ä¸´åºŠæµç¨‹ä¸­ç¼ºä¹ç»Ÿä¸€åè°ƒå’Œå¤šæ­¥å¤„ç†èƒ½åŠ›çš„é—®é¢˜ã€‚è¯¥æ™ºèƒ½ä½“é‡‡ç”¨åŒè®°å¿†è®¾è®¡(dual-memory design)ï¼Œé€šè¿‡çŸ­æœŸè¡ŒåŠ¨è¿½è¸ªç¡®ä¿é€»è¾‘è¿è´¯æ€§ï¼Œå¹¶åˆ©ç”¨é•¿æœŸç»éªŒå­¦ä¹ æŒç»­å¢å¼ºæ¨ç†æ•é”åº¦ã€‚EndoAgentåœ¨ç»Ÿä¸€çš„æ¨ç†å¾ªç¯ä¸­é›†æˆäº†å¤šé¡¹ä¸“å®¶è®¾è®¡çš„å·¥å…·ï¼Œæ”¯æŒè¿­ä»£æ¨ç†ã€è‡ªé€‚åº”å·¥å…·é€‰æ‹©ä¸å¤šæ–¹åä½œã€‚ç ”ç©¶è€…åŒæ­¥æ¨å‡ºäº†åŒ…å«5,709ä¸ªè§†è§‰é—®ç­”å¯¹çš„åŸºå‡†æµ‹è¯•é›†EndoAgentBenchï¼Œç”¨äºè¯„ä¼°ç°å®åœºæ™¯ä¸‹çš„è§†è§‰ç†è§£ä¸è¯­è¨€ç”Ÿæˆèƒ½åŠ›ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼ŒEndoAgentåœ¨æ€§èƒ½ä¸ŠæŒç»­ä¼˜äºé€šç”¨çš„å’ŒåŒ»å­¦é¢†åŸŸçš„å¤šæ¨¡æ€æ¨¡å‹ï¼Œå±•ç°å‡ºå“è¶Šçš„çµæ´»æ€§å’Œå¤æ‚çš„æ¨ç†å†³ç­–èƒ½åŠ›ã€‚",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.CV"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.07292v1",
      "published_date": "2025-08-10 11:02:57 UTC",
      "updated_date": "2025-08-10 11:02:57 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T10:45:06.421511+00:00"
    },
    {
      "arxiv_id": "2508.07284v1",
      "title": "\"Pull or Not to Pull?'': Investigating Moral Biases in Leading Large Language Models Across Ethical Dilemmas",
      "title_zh": "â€œæ‹‰è¿˜æ˜¯ä¸æ‹‰ï¼Ÿâ€ï¼šæ¢ç©¶ä¸»æµå¤§è¯­è¨€æ¨¡å‹åœ¨å„ç±»ä¼¦ç†å›°å¢ƒä¸­çš„é“å¾·åè§",
      "authors": [
        "Junchen Ding",
        "Penghao Jiang",
        "Zihao Xu",
        "Ziqi Ding",
        "Yichen Zhu",
        "Jiaojiao Jiang",
        "Yuekang Li"
      ],
      "abstract": "As large language models (LLMs) increasingly mediate ethically sensitive decisions, understanding their moral reasoning processes becomes imperative. This study presents a comprehensive empirical evaluation of 14 leading LLMs, both reasoning enabled and general purpose, across 27 diverse trolley problem scenarios, framed by ten moral philosophies, including utilitarianism, deontology, and altruism. Using a factorial prompting protocol, we elicited 3,780 binary decisions and natural language justifications, enabling analysis along axes of decisional assertiveness, explanation answer consistency, public moral alignment, and sensitivity to ethically irrelevant cues. Our findings reveal significant variability across ethical frames and model types: reasoning enhanced models demonstrate greater decisiveness and structured justifications, yet do not always align better with human consensus. Notably, \"sweet zones\" emerge in altruistic, fairness, and virtue ethics framings, where models achieve a balance of high intervention rates, low explanation conflict, and minimal divergence from aggregated human judgments. However, models diverge under frames emphasizing kinship, legality, or self interest, often producing ethically controversial outcomes. These patterns suggest that moral prompting is not only a behavioral modifier but also a diagnostic tool for uncovering latent alignment philosophies across providers. We advocate for moral reasoning to become a primary axis in LLM alignment, calling for standardized benchmarks that evaluate not just what LLMs decide, but how and why.",
      "tldr_zh": "è¯¥ç ”ç©¶å¯¹14ç§é¢†å…ˆçš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨å¤„ç†é“å¾·å›°å¢ƒæ—¶çš„é“å¾·åè§è¿›è¡Œäº†å…¨é¢çš„å®è¯è¯„ä¼°ã€‚ç ”ç©¶é€šè¿‡10ç§é“å¾·å“²å­¦ï¼ˆå¦‚ utilitarianism å’Œ deontologyï¼‰æ¡†æ¶ä¸‹çš„27ä¸ªç”µè½¦éš¾é¢˜ï¼ˆtrolley problemï¼‰åœºæ™¯ï¼Œé‡‡ç”¨ factorial prompting åè®®åˆ†æäº†æ¨¡å‹çš„å†³ç­–æœæ–­æ€§ã€è§£é‡Šä¸€è‡´æ€§åŠä¸äººç±»é“å¾·çš„ä¸€è‡´æ€§ã€‚å®éªŒå‘ç°ï¼Œæ¨ç†å¢å¼ºå‹æ¨¡å‹è™½ç„¶å†³ç­–æ›´æœæ–­ä¸”ç†ç”±æ›´å…·ç»“æ„åŒ–ï¼Œä½†åœ¨äººç±»å…±è¯†å¯¹é½æ–¹é¢è¡¨ç°å‚å·®ä¸é½ã€‚æ¨¡å‹åœ¨ altruismã€fairness å’Œ virtue ethics æ¡†æ¶ä¸‹èƒ½å¤Ÿè¾¾åˆ°è¾ƒå¥½çš„å¹³è¡¡ï¼Œä½†åœ¨æ¶‰åŠ kinshipã€legality æˆ– self-interest æ—¶åˆ™å®¹æ˜“äº§ç”Ÿä¼¦ç†äº‰è®®ã€‚ç ”ç©¶æŒ‡å‡ºé“å¾·æç¤ºï¼ˆmoral promptingï¼‰ä¸ä»…èƒ½ä¿®æ­£è¡Œä¸ºï¼Œæ›´æ˜¯æ­ç¤ºæ¨¡å‹æ½œåœ¨ alignment å“²å­¦çš„è¯Šæ–­å·¥å…·ã€‚æœ€åï¼Œä½œè€…å»ºè®®å°†é“å¾·æ¨ç†çº³å…¥ LLM å¯¹é½çš„æ ¸å¿ƒç»´åº¦ï¼Œå¹¶å‘¼åå»ºç«‹å…³æ³¨å†³ç­–é€»è¾‘ä¸åŠ¨æœºçš„æ ‡å‡†åŒ–åŸºå‡†ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.CY"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.07284v1",
      "published_date": "2025-08-10 10:45:16 UTC",
      "updated_date": "2025-08-10 10:45:16 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T10:45:05.583644+00:00"
    },
    {
      "arxiv_id": "2508.07283v1",
      "title": "Fine-Tuning Large Language Models Using EEG Microstate Features for Mental Workload Assessment",
      "title_zh": "åŸºäº EEG å¾®çŠ¶æ€ç‰¹å¾çš„å¿ƒç†è´Ÿè·è¯„ä¼°å¤§è¯­è¨€æ¨¡å‹å¾®è°ƒ",
      "authors": [
        "Bujar Raufi"
      ],
      "abstract": "This study explores the intersection of electroencephalography (EEG) microstates and Large Language Models (LLMs) to enhance the assessment of cognitive load states. By utilizing EEG microstate features, the research aims to fine-tune LLMs for improved predictions of distinct cognitive states, specifically 'Rest' and 'Load'. The experimental design is delineated in four comprehensive stages: dataset collection and preprocessing, microstate segmentation and EEG backfitting, feature extraction paired with prompt engineering, and meticulous LLM model selection and refinement. Employing a supervised learning paradigm, the LLM is trained to identify cognitive load states based on EEG microstate features integrated into prompts, producing accurate discrimination of cognitive load. A curated dataset, linking EEG features to specified cognitive load conditions, underpins the experimental framework. The results indicate a significant improvement in model performance following the proposed fine-tuning, showcasing the potential of EEG-informed LLMs in cognitive neuroscience and cognitive AI applications. This approach not only contributes to the understanding of brain dynamics but also paves the way for advancements in machine learning techniques applicable to cognitive load and cognitive AI research.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†åˆ©ç”¨ EEG microstate ç‰¹å¾å¾®è°ƒ Large Language Models (LLMs) ä»¥å¢å¼ºè®¤çŸ¥è´Ÿè·è¯„ä¼°çš„æ–¹æ³•ã€‚ç ”ç©¶æ—¨åœ¨é€šè¿‡æå–çš„ç‰¹å¾æé«˜æ¨¡å‹å¯¹ 'Rest' å’Œ 'Load' ä¸¤ç§è®¤çŸ¥çŠ¶æ€çš„é¢„æµ‹èƒ½åŠ›ã€‚å®éªŒè®¾è®¡åŒ…å«æ•°æ®é›†é‡‡é›†ã€microstate åˆ†æ®µä¸ EEG backfittingï¼Œä»¥åŠç»“åˆ prompt engineering è¿›è¡Œç‰¹å¾æå–ä¸æ¨¡å‹ç²¾ç‚¼ç­‰å››ä¸ªå…³é”®é˜¶æ®µã€‚é€šè¿‡æœ‰ç›‘ç£å­¦ä¹ èŒƒå¼ï¼ŒLLM å­¦ä¹ è¯†åˆ«æ•´åˆåœ¨æç¤ºè¯ä¸­çš„ EEG ç‰¹å¾ï¼Œå®ç°äº†å¯¹è®¤çŸ¥è´Ÿè·çŠ¶æ€çš„å‡†ç¡®åˆ¤åˆ«ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œæ‰€æå¾®è°ƒæ–¹æ³•æ˜¾è‘—æå‡äº†æ¨¡å‹æ€§èƒ½ï¼Œè¯æ˜äº† EEG-informed LLMs åœ¨è®¤çŸ¥ç¥ç»ç§‘å­¦å’Œè®¤çŸ¥ AI é¢†åŸŸçš„åº”ç”¨å‰æ™¯ã€‚è¿™ä¸€æ–¹æ³•ä¸ä»…åŠ æ·±äº†å¯¹å¤§è„‘åŠ¨æ€æ€§çš„ç†è§£ï¼Œä¹Ÿä¸ºè®¤çŸ¥è´Ÿè·ç›¸å…³çš„æœºå™¨å­¦ä¹ æŠ€æœ¯å‘å±•å¥ å®šäº†åŸºç¡€ã€‚",
      "categories": [
        "cs.HC",
        "cs.AI",
        "eess.SP",
        "q-bio.NC"
      ],
      "primary_category": "cs.HC",
      "comment": "17 Pages, 7 figures, 3 tables and one prompt template",
      "pdf_url": "https://arxiv.org/pdf/2508.07283v1",
      "published_date": "2025-08-10 10:43:09 UTC",
      "updated_date": "2025-08-10 10:43:09 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T10:45:35.332172+00:00"
    },
    {
      "arxiv_id": "2508.07281v1",
      "title": "Representation Understanding via Activation Maximization",
      "title_zh": "åŸºäºæ¿€æ´»æœ€å¤§åŒ–çš„è¡¨ç¤ºç†è§£",
      "authors": [
        "Hongbo Zhu",
        "Angelo Cangelosi"
      ],
      "abstract": "Understanding internal feature representations of deep neural networks (DNNs) is a fundamental step toward model interpretability. Inspired by neuroscience methods that probe biological neurons using visual stimuli, recent deep learning studies have employed Activation Maximization (AM) to synthesize inputs that elicit strong responses from artificial neurons. In this work, we propose a unified feature visualization framework applicable to both Convolutional Neural Networks (CNNs) and Vision Transformers (ViTs). Unlike prior efforts that predominantly focus on the last output-layer neurons in CNNs, we extend feature visualization to intermediate layers as well, offering deeper insights into the hierarchical structure of learned feature representations. Furthermore, we investigate how activation maximization can be leveraged to generate adversarial examples, revealing potential vulnerabilities and decision boundaries of DNNs. Our experiments demonstrate the effectiveness of our approach in both traditional CNNs and modern ViT, highlighting its generalizability and interpretive value.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†ç†è§£æ·±åº¦ç¥ç»ç½‘ç»œ(Deep Neural Networks, DNNs)å†…éƒ¨ç‰¹å¾è¡¨ç¤ºå¯¹æ¨¡å‹å¯è§£é‡Šæ€§çš„é‡è¦æ€§ï¼Œå¹¶æå‡ºäº†ä¸€ä¸ªé€‚ç”¨äºå·ç§¯ç¥ç»ç½‘ç»œ(CNNs)å’Œè§†è§‰äº’æ„Ÿå™¨(Vision Transformers, ViTs)çš„ç»Ÿä¸€ç‰¹å¾å¯è§†åŒ–æ¡†æ¶ã€‚è¯¥æ¡†æ¶åˆ©ç”¨æ¿€æ´»æœ€å¤§åŒ–(Activation Maximization, AM)æŠ€æœ¯åˆæˆèƒ½å¤Ÿå¼•å‘äººå·¥ç¥ç»å…ƒå¼ºçƒˆååº”çš„è¾“å…¥ï¼Œä»¥æ¢æµ‹å…¶å†…éƒ¨æœºåˆ¶ã€‚ä¸åŒäºä»¥å¾€ä¸»è¦å…³æ³¨CNNsæœ€åä¸€å±‚è¾“å‡ºå±‚çš„ç ”ç©¶ï¼Œæœ¬å·¥ä½œå°†ç‰¹å¾å¯è§†åŒ–æ‰©å±•åˆ°äº†ä¸­é—´å±‚ï¼Œä»è€Œä¸ºåˆ†å±‚å­¦ä¹ ç‰¹å¾çš„ç»“æ„æä¾›äº†æ›´æ·±å±‚æ¬¡çš„è§è§£ã€‚æ­¤å¤–ï¼Œç ”ç©¶è¿˜è¿›ä¸€æ­¥è°ƒæŸ¥äº†å¦‚ä½•åˆ©ç”¨AMç”Ÿæˆå¯¹æŠ—æ ·æœ¬(Adversarial Examples)ï¼Œä»¥æ­ç¤ºæ¨¡å‹çš„æ½œåœ¨æ¼æ´å’Œå†³ç­–è¾¹ç•Œã€‚å®éªŒç»“æœè¯æ˜ï¼Œè¯¥æ–¹æ³•åœ¨ä¼ ç»Ÿå’Œç°ä»£æ¨¡å‹æ¶æ„ä¸­å‡å…·æœ‰æ˜¾è‘—çš„æœ‰æ•ˆæ€§ã€æ³›åŒ–èƒ½åŠ›å’Œè§£é‡Šä»·å€¼ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "7 pages,12 figures",
      "pdf_url": "https://arxiv.org/pdf/2508.07281v1",
      "published_date": "2025-08-10 10:36:30 UTC",
      "updated_date": "2025-08-10 10:36:30 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T10:45:51.556939+00:00"
    },
    {
      "arxiv_id": "2508.07279v3",
      "title": "MAQuA: Adaptive Question-Asking for Multidimensional Mental Health Screening using Item Response Theory",
      "title_zh": "MAQuAï¼šåŸºäºé¡¹ç›®ååº”ç†è®ºçš„è‡ªé€‚åº”æé—®å¼å¤šç»´åº¦å¿ƒç†å¥åº·ç­›æŸ¥",
      "authors": [
        "Vasudha Varadarajan",
        "Hui Xu",
        "Rebecca Astrid Boehme",
        "Mariam Marlan Mirstrom",
        "Sverker Sikstrom",
        "H. Andrew Schwartz"
      ],
      "abstract": "Recent advances in large language models (LLMs) offer new opportunities for scalable, interactive mental health assessment, but excessive querying by LLMs burdens users and is inefficient for real-world screening across transdiagnostic symptom profiles. We introduce MAQuA, an adaptive question-asking framework for simultaneous, multidimensional mental health screening. Combining multi-outcome modeling on language responses with item response theory (IRT) and factor analysis, MAQuA selects the questions with most informative responses across multiple dimensions at each turn to optimize diagnostic information, improving accuracy and potentially reducing response burden. Empirical results on a novel dataset reveal that MAQuA reduces the number of assessment questions required for score stabilization by 50-87% compared to random ordering (e.g., achieving stable depression scores with 71% fewer questions and eating disorder scores with 85% fewer questions). MAQuA demonstrates robust performance across both internalizing (depression, anxiety) and externalizing (substance use, eating disorder) domains, with early stopping strategies further reducing patient time and burden. These findings position MAQuA as a powerful and efficient tool for scalable, nuanced, and interactive mental health screening, advancing the integration of LLM-based agents into real-world clinical workflows.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†MAQuAï¼Œä¸€ç§ç”¨äºå¤šç»´åº¦å¿ƒç†å¥åº·ç­›æŸ¥çš„è‡ªé€‚åº”æé—®æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³å¤§å‹è¯­è¨€æ¨¡å‹(LLMs)åœ¨è¯„ä¼°è¿‡ç¨‹ä¸­å› è¿‡åº¦è¯¢é—®ç»™ç”¨æˆ·å¸¦æ¥çš„æ²‰é‡è´Ÿæ‹…ã€‚è¯¥æ¡†æ¶é€šè¿‡ç»“åˆå¯¹è¯­è¨€ååº”çš„å¤šç»“æœå»ºæ¨¡ã€é¡¹ç›®ååº”ç†è®º(Item Response Theory, IRT)å’Œå› å­åˆ†æï¼Œèƒ½å¤Ÿåœ¨æ¯ä¸€è½®å¯¹è¯ä¸­åŠ¨æ€é€‰æ‹©è·¨å¤šä¸ªç»´åº¦æœ€å…·ä¿¡æ¯é‡çš„æé—®ï¼Œä»è€Œåœ¨æå‡å‡†ç¡®æ€§çš„åŒæ—¶ä¼˜åŒ–è¯Šæ–­æ•ˆç‡ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œä¸éšæœºæé—®é¡ºåºç›¸æ¯”ï¼ŒMAQuAå°†è¯„ä¼°åˆ†æ•°è¶‹äºç¨³å®šæ‰€éœ€çš„é—®é¢˜æ•°é‡æ˜¾è‘—å‡å°‘äº†50-87%ï¼Œå…¶ä¸­æŠ‘éƒç—‡å’Œè¿›é£Ÿéšœç¢(eating disorder)è¯„ä¼°çš„é—®é¢˜æ•°åˆ†åˆ«å‡å°‘äº†71%å’Œ85%ã€‚è¯¥æ¨¡å‹åœ¨å†…åŒ–ï¼ˆæŠ‘éƒã€ç„¦è™‘ï¼‰å’Œå¤–åŒ–ï¼ˆç‰©è´¨ä½¿ç”¨ã€è¿›é£Ÿéšœç¢ï¼‰ç­‰å¤šä¸ªè¯Šæ–­é¢†åŸŸå‡è¡¨ç°å‡ºç¨³å¥çš„æ€§èƒ½ï¼Œé…åˆæ—©æœŸåœæ­¢ç­–ç•¥è¿›ä¸€æ­¥é™ä½äº†å—è¯•è€…çš„æ—¶é—´æˆæœ¬ã€‚è¿™é¡¹ç ”ç©¶è¯æ˜äº†MAQuAèƒ½å¤Ÿä½œä¸ºä¸€ç§é«˜æ•ˆä¸”å¯æ‰©å±•çš„å·¥å…·ï¼Œä¸ºå°†åŸºäºLLMçš„æ™ºèƒ½ä½“é›†æˆåˆ°çœŸå®ä¸´åºŠå·¥ä½œæµä¸­æä¾›äº†é‡è¦æŠ€æœ¯æ”¯æŒã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.07279v3",
      "published_date": "2025-08-10 10:33:16 UTC",
      "updated_date": "2025-11-20 08:20:46 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T10:45:53.184957+00:00"
    },
    {
      "arxiv_id": "2508.14062v1",
      "title": "Assessing and Mitigating Data Memorization Risks in Fine-Tuned Large Language Models",
      "title_zh": "å¾®è°ƒå¤§è¯­è¨€æ¨¡å‹ä¸­æ•°æ®è®°å¿†é£é™©çš„è¯„ä¼°ä¸ç¼“è§£",
      "authors": [
        "Badrinath Ramakrishnan",
        "Akshaya Balaji"
      ],
      "abstract": "Large Language Models (LLMs) have demonstrated remarkable capabilities across diverse natural language processing tasks, but their tendency to memorize training data poses significant privacy risks, particularly during fine-tuning processes. This paper presents a comprehensive empirical analysis of data memorization in fine-tuned LLMs and introduces a novel multi-layered privacy protection framework. Through controlled experiments on modern LLM architectures including GPT-2, Phi-3, and Gemma-2, we demonstrate that fine-tuning with repeated sensitive data increases privacy leakage rates from baseline levels of 0-5% to 60-75%, representing a 64.2% average increase across tested models. We propose and rigorously evaluate four complementary privacy protection methods: semantic data deduplication, differential privacy during generation, entropy-based filtering, and pattern-based content filtering. Our experimental results show that these techniques can reduce data leakage to 0% while maintaining 94.7% of original model utility.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†å¤§å‹è¯­è¨€æ¨¡å‹(LLMs)åœ¨å¾®è°ƒ(Fine-tuning)è¿‡ç¨‹ä¸­å­˜åœ¨çš„æ•°æ®è®°å¿†é£é™©åŠå…¶å¸¦æ¥çš„éšç§æ³„éœ²é—®é¢˜ã€‚é€šè¿‡åœ¨GPT-2ã€Phi-3å’ŒGemma-2ç­‰ç°ä»£æ¶æ„ä¸Šè¿›è¡Œçš„å®è¯åˆ†æå‘ç°ï¼ŒåŒ…å«é‡å¤æ•æ„Ÿæ•°æ®çš„å¾®è°ƒä¼šä½¿éšç§æ³„éœ²ç‡ä»0-5%æ˜¾è‘—ä¸Šå‡è‡³60-75%ï¼Œå¹³å‡å¢å¹…è¾¾64.2%ã€‚ä¸ºè§£å†³è¿™ä¸€æŒ‘æˆ˜ï¼Œè®ºæ–‡æå‡ºäº†ä¸€ä¸ªå¤šå±‚éšç§ä¿æŠ¤æ¡†æ¶ï¼Œæ¶µç›–äº†è¯­ä¹‰æ•°æ®å»é‡(Semantic data deduplication)ã€ç”Ÿæˆæ—¶çš„å·®åˆ†éšç§(Differential privacy)ã€åŸºäºç†µçš„è¿‡æ»¤(Entropy-based filtering)ä»¥åŠåŸºäºæ¨¡å¼çš„å†…å®¹è¿‡æ»¤(Pattern-based content filtering)å››ç§äº’è¡¥æŠ€æœ¯ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ¡†æ¶èƒ½å¤ŸæˆåŠŸå°†æ•°æ®æ³„éœ²ç‡é™è‡³0%ï¼ŒåŒæ—¶ä»èƒ½ä¿æŒ94.7%çš„æ¨¡å‹åŸå§‹æ•ˆç”¨(Utility)ã€‚è¯¥ç ”ç©¶ä¸ºLLMsåœ¨å®é™…åº”ç”¨ä¸­çš„éšç§å®‰å…¨ä¿éšœæä¾›äº†é‡è¦çš„å®è¯ä¾æ®å’ŒæŠ€æœ¯æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "14 pages, 2 figures. Code and experimental framework available at https://github.com/akshayaaa10/llm-privacy-research",
      "pdf_url": "https://arxiv.org/pdf/2508.14062v1",
      "published_date": "2025-08-10 10:26:55 UTC",
      "updated_date": "2025-08-10 10:26:55 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T10:45:58.765590+00:00"
    },
    {
      "arxiv_id": "2508.07273v1",
      "title": "Incorporating Contextual Paralinguistic Understanding in Large Speech-Language Models",
      "title_zh": "åœ¨å¤§è¯­éŸ³è¯­è¨€æ¨¡å‹ä¸­èå…¥è¯­å¢ƒå‰¯è¯­è¨€ç†è§£",
      "authors": [
        "Qiongqiong Wang",
        "Hardik B. Sailor",
        "Jeremy H. M. Wong",
        "Tianchi Liu",
        "Shuo Sun",
        "Wenyu Zhang",
        "Muhammad Huzaifah",
        "Nancy Chen",
        "Ai Ti Aw"
      ],
      "abstract": "Current large speech language models (Speech-LLMs) often exhibit limitations in empathetic reasoning, primarily due to the absence of training datasets that integrate both contextual content and paralinguistic cues. In this work, we propose two approaches to incorporate contextual paralinguistic information into model training: (1) an explicit method that provides paralinguistic metadata (e.g., emotion annotations) directly to the LLM, and (2) an implicit method that automatically generates novel training question-answer (QA) pairs using both categorical and dimensional emotion annotations alongside speech transcriptions. Our implicit method boosts performance (LLM-judged) by 38.41% on a human-annotated QA benchmark, reaching 46.02% when combined with the explicit approach, showing effectiveness in contextual paralinguistic understanding. We also validate the LLM judge by demonstrating its correlation with classification metrics, providing support for its reliability.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å½“å‰å¤§è¯­è¨€è¯­éŸ³æ¨¡å‹(Speech-LLMs)ç”±äºç¼ºä¹ç»“åˆä¸Šä¸‹æ–‡å†…å®¹ä¸å‰¯è¯­è¨€(paralinguistic)çº¿ç´¢çš„è®­ç»ƒæ•°æ®ï¼Œè€Œå¯¼è‡´åœ¨å…±æƒ…æ¨ç†æ–¹é¢å­˜åœ¨å±€é™æ€§çš„é—®é¢˜è¿›è¡Œäº†æ¢è®¨ã€‚ä½œè€…æå‡ºäº†ä¸¤ç§å°†ä¸Šä¸‹æ–‡å‰¯è¯­è¨€ä¿¡æ¯èå…¥æ¨¡å‹è®­ç»ƒçš„æ–¹æ¡ˆï¼ŒåŒ…æ‹¬ç›´æ¥æä¾›æƒ…æ„Ÿæ ‡æ³¨ç­‰å…ƒæ•°æ®çš„æ˜¾å¼æ–¹æ³•ï¼Œä»¥åŠåˆ©ç”¨æƒ…æ„Ÿæ ‡æ³¨ä¸è¯­éŸ³è½¬å½•è‡ªåŠ¨ç”Ÿæˆæ–°å‹é—®ç­”å¯¹(QA pairs)çš„éšå¼æ–¹æ³•ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œéšå¼æ–¹æ³•åœ¨äººå·¥æ ‡æ³¨çš„é—®ç­”åŸºå‡†æµ‹è¯•ä¸­ä½¿LLMè¯„å®šçš„æ€§èƒ½æå‡äº†38.41%ï¼Œè€Œç»“åˆæ˜¾å¼æ–¹æ³•åæ€§èƒ½æå‡å¹…åº¦è¿›ä¸€æ­¥è¾¾åˆ°46.02%ï¼Œè¯æ˜äº†è¯¥æ–¹æ¡ˆåœ¨å¢å¼ºä¸Šä¸‹æ–‡å‰¯è¯­è¨€ç†è§£æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚æ­¤å¤–ï¼Œç ”ç©¶è¿˜é€šè¿‡å±•ç¤ºLLMè¯„å®šç»“æœä¸åˆ†ç±»æŒ‡æ ‡ä¹‹é—´çš„é«˜åº¦ç›¸å…³æ€§ï¼ŒéªŒè¯äº†æ‰€é‡‡ç”¨è¯„ä¼°æœºåˆ¶çš„å¯é æ€§ã€‚è¯¥å·¥ä½œä¸ºæå‡Speech-LLMsçš„å…±æƒ…èƒ½åŠ›å’Œå¤æ‚è¯­å¢ƒä¸‹çš„å‰¯è¯­è¨€æ„ŸçŸ¥æä¾›äº†é‡è¦çš„æŠ€æœ¯è·¯å¾„ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "eess.AS"
      ],
      "primary_category": "cs.CL",
      "comment": "Accepted at (ASRU 2025) 2025 IEEE Automatic Speech Recognition and Understanding Workshop",
      "pdf_url": "https://arxiv.org/pdf/2508.07273v1",
      "published_date": "2025-08-10 10:03:30 UTC",
      "updated_date": "2025-08-10 10:03:30 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T10:46:05.194833+00:00"
    },
    {
      "arxiv_id": "2508.07270v1",
      "title": "OpenHAIV: A Framework Towards Practical Open-World Learning",
      "title_zh": "OpenHAIVï¼šé¢å‘å®ç”¨å¼€æ”¾ä¸–ç•Œå­¦ä¹ çš„æ¡†æ¶",
      "authors": [
        "Xiang Xiang",
        "Qinhao Zhou",
        "Zhuo Xu",
        "Jing Ma",
        "Jiaxin Dai",
        "Yifan Liang",
        "Hanlin Li"
      ],
      "abstract": "Substantial progress has been made in various techniques for open-world recognition. Out-of-distribution (OOD) detection methods can effectively distinguish between known and unknown classes in the data, while incremental learning enables continuous model knowledge updates. However, in open-world scenarios, these approaches still face limitations. Relying solely on OOD detection does not facilitate knowledge updates in the model, and incremental fine-tuning typically requires supervised conditions, which significantly deviate from open-world settings. To address these challenges, this paper proposes OpenHAIV, a novel framework that integrates OOD detection, new class discovery, and incremental continual fine-tuning into a unified pipeline. This framework allows models to autonomously acquire and update knowledge in open-world environments. The proposed framework is available at https://haiv-lab.github.io/openhaiv .",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¼€æ”¾ä¸–ç•Œ(Open-World)è¯†åˆ«é¢†åŸŸçš„ç°æœ‰å±€é™æ€§ï¼ŒæŒ‡å‡ºäº†å•çº¯çš„å‡ºåˆ†å¸ƒæ£€æµ‹(Out-of-distribution, OOD)æ— æ³•æ›´æ–°æ¨¡å‹çŸ¥è¯†ï¼Œè€Œä¼ ç»Ÿçš„å¢é‡å­¦ä¹ (Incremental Learning)åˆé«˜åº¦ä¾èµ–ç›‘ç£æ¡ä»¶çš„å›°å¢ƒã€‚ä¸ºæ­¤ï¼Œæœ¬æ–‡æå‡ºäº†OpenHAIVæ¡†æ¶ï¼Œæ—¨åœ¨æ„å»ºä¸€ä¸ªé€šå¾€å®ç”¨å¼€æ”¾ä¸–ç•Œå­¦ä¹ çš„ç»Ÿä¸€ä½“ç³»ã€‚è¯¥æ¡†æ¶åˆ›æ–°æ€§åœ°å°†OODæ£€æµ‹ã€æ–°ç±»åˆ«å‘ç°(New Class Discovery)ä»¥åŠå¢é‡æŒç»­å¾®è°ƒ(Incremental Continual Fine-tuning)é›†æˆåˆ°ä¸€ä¸ªç»Ÿä¸€çš„æµæ°´çº¿ä¸­ã€‚è¿™ç§é›†æˆåŒ–è®¾è®¡ä½¿æ¨¡å‹èƒ½å¤Ÿæ‘†è„±å¯¹å¼ºç›‘ç£æ¡ä»¶çš„ä¾èµ–ï¼Œåœ¨å¼€æ”¾ä¸–ç•Œç¯å¢ƒä¸­å®ç°è‡ªä¸»çš„çŸ¥è¯†è·å–ä¸åŠ¨æ€æ›´æ–°ã€‚è¯¥æ¡†æ¶è§£å†³äº†ç°æœ‰æŠ€æœ¯åœ¨çŸ¥è¯†æ¼”è¿›ä¸è‡ªé€‚åº”æ–¹é¢çš„è„±èŠ‚é—®é¢˜ï¼Œä¸ºå®ç°æ›´å…·å®ç”¨æ€§çš„è‡ªä¸»å­¦ä¹ ç³»ç»Ÿæä¾›äº†æ–°è·¯å¾„ã€‚ç›®å‰ï¼Œè¯¥æ¡†æ¶çš„ç›¸å…³èµ„æºå·²åœ¨é¡¹ç›®ç½‘é¡µå…¬å¼€ï¼Œä¸ºåç»­çš„å¼€æ”¾ä¸–ç•Œå­¦ä¹ ç ”ç©¶æä¾›äº†åŸºç¡€å·¥å…·ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG",
        "eess.IV",
        "stat.ML"
      ],
      "primary_category": "cs.CV",
      "comment": "Codes, results, and OpenHAIV documentation available at https://haiv-lab.github.io/openhaiv",
      "pdf_url": "https://arxiv.org/pdf/2508.07270v1",
      "published_date": "2025-08-10 09:55:19 UTC",
      "updated_date": "2025-08-10 09:55:19 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T10:46:00.260819+00:00"
    },
    {
      "arxiv_id": "2508.07264v1",
      "title": "FLUID: Flow-Latent Unified Integration via Token Distillation for Expert Specialization in Multimodal Learning",
      "title_zh": "FLUIDï¼šåŸºäº Token è’¸é¦å®ç°å¤šæ¨¡æ€å­¦ä¹ ä¸“å®¶ç‰¹åŒ–çš„æµ-æ½œç»Ÿä¸€é›†æˆ",
      "authors": [
        "Van Duc Cuong",
        "Ta Dinh Tam",
        "Tran Duc Chinh",
        "Nguyen Thi Hanh"
      ],
      "abstract": "Multimodal classification requires robust integration of visual and textual signals, yet common fusion strategies are brittle and vulnerable to modality-specific noise. In this paper, we present \\textsc{FLUID}-Flow-Latent Unified Integration via Token Distillation for Expert Specialization, a principled token-level pipeline that improves cross-modal robustness and scalability. \\textsc{FLUID} contributes three core elements: (1) \\emph{Q-transforms}, learnable query tokens that distill and retain salient token-level features from modality-specific backbones; (2) a two-stage fusion scheme that enforces cross-modal consistency via contrastive alignment and then performs adaptive, task-aware fusion through a gating mechanism and a \\emph{Q-bottleneck} that selectively compresses information for downstream reasoning; and (3) a lightweight, load-balanced Mixture-of-Experts at prediction time that enables efficient specialization to diverse semantic patterns. Extensive experiments demonstrate that \\textsc{FLUID} attains \\(91\\%\\) accuracy on the GLAMI-1M benchmark, significantly outperforming prior baselines and exhibiting strong resilience to label noise, long-tail class imbalance, and semantic heterogeneity. Targeted ablation studies corroborate both the individual and synergistic benefits of the proposed components, positioning \\textsc{FLUID} as a scalable, noise-resilient solution for multimodal product classification.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†FLUIDæ¡†æ¶ï¼Œå³Flow-Latent Unified Integration via Token Distillation for Expert Specializationï¼Œæ—¨åœ¨è§£å†³å¤šæ¨¡æ€åˆ†ç±»(Multimodal classification)ä¸­èåˆç­–ç•¥è„†å¼±ä¸”æ˜“å—æ¨¡æ€ç‰¹å®šå™ªå£°å¹²æ‰°çš„é—®é¢˜ã€‚è¯¥æ¡†æ¶å¼•å…¥äº†Q-transformsï¼Œé€šè¿‡å¯å­¦ä¹ çš„æŸ¥è¯¢Tokenä»ç‰¹å®šæ¨¡æ€çš„ä¸»å¹²ç½‘ç»œä¸­æå–å¹¶ä¿ç•™æ˜¾è‘—çš„Tokençº§ç‰¹å¾ã€‚FLUIDé‡‡ç”¨ä¸¤é˜¶æ®µèåˆæ–¹æ¡ˆï¼Œå…ˆé€šè¿‡å¯¹æ¯”å¯¹é½(Contrastive alignment)å¼ºåˆ¶æ‰§è¡Œè·¨æ¨¡æ€ä¸€è‡´æ€§ï¼Œå†åˆ©ç”¨é—¨æ§æœºåˆ¶å’ŒQ-bottleneckå®ç°è‡ªé€‚åº”çš„ä»»åŠ¡æ„ŸçŸ¥èåˆã€‚åœ¨é¢„æµ‹é˜¶æ®µï¼Œè¯¥æ¡†æ¶ç»“åˆäº†è½»é‡åŒ–ä¸”è´Ÿè½½å‡è¡¡çš„ä¸“å®¶æ··åˆæ¨¡å‹(Mixture-of-Experts, MoE)ï¼Œèƒ½å¤Ÿé«˜æ•ˆåœ°é€‚åº”å¤šæ ·åŒ–çš„è¯­ä¹‰æ¨¡å¼ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒFLUIDåœ¨GLAMI-1MåŸºå‡†æµ‹è¯•ä¸Šè¾¾åˆ°äº†91%çš„å‡†ç¡®ç‡ï¼Œæ˜¾è‘—ä¼˜äºç°æœ‰åŸºçº¿æ¨¡å‹ã€‚ç ”ç©¶è¿›ä¸€æ­¥è¯æ˜äº†è¯¥æ–¹æ³•åœ¨é¢å¯¹æ ‡ç­¾å™ªå£°ã€é•¿å°¾ç±»åˆ«å¤±è¡¡(Long-tail class imbalance)ä»¥åŠè¯­ä¹‰å¼‚è´¨æ€§æ—¶å…·æœ‰æå¼ºçš„é²æ£’æ€§ï¼Œä¸ºå¯æ‰©å±•ã€æŠ—å™ªå£°çš„å¤šæ¨¡æ€äº§å“åˆ†ç±»æä¾›äº†æœ‰æ•ˆæ–¹æ¡ˆã€‚",
      "categories": [
        "cs.SI",
        "cs.AI"
      ],
      "primary_category": "cs.SI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.07264v1",
      "published_date": "2025-08-10 09:34:17 UTC",
      "updated_date": "2025-08-10 09:34:17 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T10:46:07.890312+00:00"
    },
    {
      "arxiv_id": "2508.10038v1",
      "title": "Certifiably robust malware detectors by design",
      "title_zh": "è®¾è®¡åŸç”Ÿçš„å¯è¯æ˜é²æ£’æ¶æ„è½¯ä»¶æ£€æµ‹å™¨",
      "authors": [
        "Pierre-Francois Gimenez",
        "Sarath Sivaprasad",
        "Mario Fritz"
      ],
      "abstract": "Malware analysis involves analyzing suspicious software to detect malicious payloads. Static malware analysis, which does not require software execution, relies increasingly on machine learning techniques to achieve scalability. Although such techniques obtain very high detection accuracy, they can be easily evaded with adversarial examples where a few modifications of the sample can dupe the detector without modifying the behavior of the software. Unlike other domains, such as computer vision, creating an adversarial example of malware without altering its functionality requires specific transformations. We propose a new model architecture for certifiably robust malware detection by design. In addition, we show that every robust detector can be decomposed into a specific structure, which can be applied to learn empirically robust malware detectors, even on fragile features. Our framework ERDALT is based on this structure. We compare and validate these approaches with machine-learning-based malware detection methods, allowing for robust detection with limited reduction of detection performance.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹é™æ€æ¶æ„è½¯ä»¶åˆ†æ(Static malware analysis)ä¸­æœºå™¨å­¦ä¹ æ¨¡å‹æ˜“å—å¯¹æŠ—æ ·æœ¬(Adversarial examples)æ”»å‡»å¹¶å¯¼è‡´æ£€æµ‹å¤±æ•ˆçš„é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§å…¨æ–°çš„å¯éªŒè¯é²æ£’(Certifiably robust)æ¶æ„è½¯ä»¶æ£€æµ‹æ¨¡å‹æ¶æ„ã€‚ä½œè€…é€šè¿‡ç†è®ºåˆ†æè¯æ˜ï¼Œæ¯ä¸€ä¸ªé²æ£’æ£€æµ‹å™¨éƒ½å¯ä»¥è¢«åˆ†è§£ä¸ºä¸€ç§ç‰¹å®šçš„ç»“æ„ï¼Œå¹¶åŸºäºæ­¤å‘ç°å¼€å‘äº†åä¸ºERDALTçš„æ¡†æ¶ã€‚è¯¥æ¡†æ¶å…è®¸æ¨¡å‹åœ¨å³ä½¿å­˜åœ¨è„†å¼±ç‰¹å¾çš„æƒ…å†µä¸‹ï¼Œä¹Ÿèƒ½å­¦ä¹ å¹¶æ„å»ºå‡ºå…·æœ‰ç»éªŒé²æ£’æ€§çš„æ£€æµ‹å™¨ã€‚ä¸ä¼ ç»Ÿçš„åŸºäºæœºå™¨å­¦ä¹ çš„æ£€æµ‹æ–¹æ³•ç›¸æ¯”ï¼Œè¯¥æ–¹æ¡ˆåœ¨ä¸æ˜¾è‘—ç‰ºç‰²æ£€æµ‹å‡†ç¡®ç‡çš„å‰æä¸‹ï¼Œå®ç°äº†å¯¹æ¶æ„è½¯ä»¶åŠŸèƒ½ä¿æŒå‹å˜æ¢æ”»å‡»çš„æœ‰æ•ˆé˜²å¾¡ã€‚å®éªŒç»“æœéªŒè¯äº†æ‰€ææ–¹æ³•åœ¨å¤æ‚å¯¹æŠ—ç¯å¢ƒä¸‹çš„æœ‰æ•ˆæ€§ï¼Œä¸ºè®¾è®¡å…·å¤‡åŸç”Ÿå®‰å…¨å±æ€§çš„æ¶æ„è½¯ä»¶æ£€æµ‹ç³»ç»Ÿæä¾›äº†ç†è®ºæ”¯æ’‘ä¸å®è·µè·¯å¾„ã€‚",
      "categories": [
        "cs.CR",
        "cs.AI"
      ],
      "primary_category": "cs.CR",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.10038v1",
      "published_date": "2025-08-10 09:19:29 UTC",
      "updated_date": "2025-08-10 09:19:29 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T10:46:12.381307+00:00"
    },
    {
      "arxiv_id": "2508.07243v1",
      "title": "Causal Negative Sampling via Diffusion Model for Out-of-Distribution Recommendation",
      "title_zh": "åŸºäºæ‰©æ•£æ¨¡å‹çš„åˆ†å¸ƒå¤–æ¨èå› æœè´Ÿé‡‡æ ·",
      "authors": [
        "Chu Zhao",
        "Eneng Yang",
        "Yizhou Dang",
        "Jianzhe Zhao",
        "Guibing Guo",
        "Xingwei Wang"
      ],
      "abstract": "Heuristic negative sampling enhances recommendation performance by selecting negative samples of varying hardness levels from predefined candidate pools to guide the model toward learning more accurate decision boundaries. However, our empirical and theoretical analyses reveal that unobserved environmental confounders (e.g., exposure or popularity biases) in candidate pools may cause heuristic sampling methods to introduce false hard negatives (FHNS). These misleading samples can encourage the model to learn spurious correlations induced by such confounders, ultimately compromising its generalization ability under distribution shifts. To address this issue, we propose a novel method named Causal Negative Sampling via Diffusion (CNSDiff). By synthesizing negative samples in the latent space via a conditional diffusion process, CNSDiff avoids the bias introduced by predefined candidate pools and thus reduces the likelihood of generating FHNS. Moreover, it incorporates a causal regularization term to explicitly mitigate the influence of environmental confounders during the negative sampling process, leading to robust negatives that promote out-of-distribution (OOD) generalization. Comprehensive experiments under four representative distribution shift scenarios demonstrate that CNSDiff achieves an average improvement of 13.96% across all evaluation metrics compared to state-of-the-art baselines, verifying its effectiveness and robustness in OOD recommendation tasks.",
      "tldr_zh": "è¯¥ç ”ç©¶æŒ‡å‡ºæ¨èç³»ç»Ÿä¸­ä¼ ç»Ÿçš„å¯å‘å¼è´Ÿé‡‡æ ·(heuristic negative sampling)å®¹æ˜“å—å€™é€‰æ± ä¸­æœªè§‚æµ‹çš„ç¯å¢ƒæ··æ‚å› ç´ (environmental confounders)å½±å“ï¼Œä»è€Œå¼•å…¥è™šå‡ç¡¬è´Ÿæ ·æœ¬(FHNS)å¹¶å¯¼è‡´æ¨¡å‹å­¦ä¹ åˆ°ä¼ªç›¸å…³ï¼Œä¸¥é‡æŸå®³äº†å…¶åˆ†å¸ƒå¤–(OOD)æ³›åŒ–èƒ½åŠ›ã€‚ä¸ºæ­¤ï¼Œä½œè€…æå‡ºäº†åŸºäºæ‰©æ•£æ¨¡å‹çš„å› æœè´Ÿé‡‡æ ·æ–¹æ³•CNSDiff (Causal Negative Sampling via Diffusion)ï¼Œé€šè¿‡åœ¨éšç©ºé—´åˆ©ç”¨æ¡ä»¶æ‰©æ•£æ¨¡å‹(conditional diffusion process)åˆæˆè´Ÿæ ·æœ¬ï¼Œä»æ ¹æœ¬ä¸Šé¿å¼€äº†é¢„å®šä¹‰å€™é€‰æ± å¸¦æ¥çš„åå·®ã€‚è¯¥æ–¹æ³•è¿›ä¸€æ­¥ç»“åˆäº†å› æœæ­£åˆ™åŒ–é¡¹(causal regularization term)æ¥æ˜¾å¼å‰Šå¼±ç¯å¢ƒæ··æ‚å› ç´ çš„å½±å“ï¼Œç¡®ä¿ç”Ÿæˆçš„è´Ÿæ ·æœ¬å…·æœ‰æ›´å¼ºçš„é²æ£’æ€§ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œåœ¨å››ç§å…¸å‹çš„åˆ†å¸ƒåç§»åœºæ™¯ä¸‹ï¼ŒCNSDiff ç›¸æ¯”ç°æœ‰æœ€å…ˆè¿›æ–¹æ³•åœ¨å„é¡¹æŒ‡æ ‡ä¸Šå¹³å‡æå‡äº†13.96%ï¼Œå……åˆ†éªŒè¯äº†å…¶åœ¨å¤„ç†OODæ¨èä»»åŠ¡æ—¶çš„æœ‰æ•ˆæ€§å’Œç¨³å¥æ€§ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "14 pages, 6 figures, Under-review",
      "pdf_url": "https://arxiv.org/pdf/2508.07243v1",
      "published_date": "2025-08-10 08:55:21 UTC",
      "updated_date": "2025-08-10 08:55:21 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T10:46:14.990331+00:00"
    },
    {
      "arxiv_id": "2508.07241v1",
      "title": "SocRipple: A Two-Stage Framework for Cold-Start Video Recommendations",
      "title_zh": "SocRippleï¼šé¢å‘å†·å¯åŠ¨è§†é¢‘æ¨èçš„ä¸¤é˜¶æ®µæ¡†æ¶",
      "authors": [
        "Amit Jaspal",
        "Kapil Dalwani",
        "Ajantha Ramineni"
      ],
      "abstract": "Most industry scale recommender systems face critical cold start challenges new items lack interaction history, making it difficult to distribute them in a personalized manner. Standard collaborative filtering models underperform due to sparse engagement signals, while content only approaches lack user specific relevance. We propose SocRipple, a novel two stage retrieval framework tailored for coldstart item distribution in social graph based platforms. Stage 1 leverages the creators social connections for targeted initial exposure. Stage 2 builds on early engagement signals and stable user embeddings learned from historical interactions to \"ripple\" outwards via K Nearest Neighbor (KNN) search. Large scale experiments on a major video platform show that SocRipple boosts cold start item distribution by +36% while maintaining user engagement rate on cold start items, effectively balancing new item exposure with personalized recommendations.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† SocRippleï¼Œä¸€ä¸ªä¸“ä¸ºç¤¾äº¤å›¾è°±å¹³å°è®¾è®¡çš„ä¸¤é˜¶æ®µå†·å¯åŠ¨ (Cold-Start) è§†é¢‘æ¨èæ£€ç´¢æ¡†æ¶ã€‚é’ˆå¯¹å·¥ä¸šçº§æ¨èç³»ç»Ÿä¸­æ–°è§†é¢‘å› ç¼ºä¹äº¤äº’å†å²è€Œéš¾ä»¥è¿›è¡Œä¸ªæ€§åŒ–åˆ†å‘çš„é—®é¢˜ï¼Œè¯¥æ¡†æ¶çš„ç¬¬ä¸€é˜¶æ®µåˆ©ç”¨åˆ›ä½œè€…çš„ç¤¾äº¤å…³ç³»è¿›è¡Œé’ˆå¯¹æ€§çš„åˆå§‹æ›å…‰ã€‚åœ¨ç¬¬äºŒé˜¶æ®µï¼ŒSocRipple ç»“åˆæ—©æœŸäº’åŠ¨ä¿¡å·å’Œä»å†å²æ•°æ®ä¸­å­¦ä¹ åˆ°çš„ç¨³å®šç”¨æˆ·åµŒå…¥ (User Embeddings)ï¼Œé€šè¿‡ K-Nearest Neighbor (KNN) æœç´¢å°†æ¨èèŒƒå›´å‘å¤–â€œæ³¢çº¹å¼â€æ‰©æ•£ã€‚åœ¨å¤§è§„æ¨¡è§†é¢‘å¹³å°ä¸Šçš„å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯¥æ¡†æ¶åœ¨ä¿æŒç”¨æˆ·å‚ä¸ç‡çš„åŒæ—¶ï¼Œä½¿å†·å¯åŠ¨é¡¹çš„åˆ†å‘æ•ˆç‡æå‡äº† 36%ã€‚è¿™ä¸€æ–¹æ³•æœ‰æ•ˆåœ°å¹³è¡¡äº†æ–°å†…å®¹çš„æ›å…‰éœ€æ±‚ä¸ä¸ªæ€§åŒ–æ¨èçš„ç²¾å‡†åº¦ï¼Œä¸ºè§£å†³æ¨èç³»ç»Ÿä¸­çš„å†·å¯åŠ¨æŒ‘æˆ˜æä¾›äº†æ–°é¢–ä¸”é«˜æ•ˆçš„æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.IR",
        "cs.AI"
      ],
      "primary_category": "cs.IR",
      "comment": "4 pages, 2 figures, 2 tables, recsys 2025",
      "pdf_url": "https://arxiv.org/pdf/2508.07241v1",
      "published_date": "2025-08-10 08:37:36 UTC",
      "updated_date": "2025-08-10 08:37:36 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T10:46:17.495153+00:00"
    },
    {
      "arxiv_id": "2508.07224v1",
      "title": "EDGE: A Theoretical Framework for Misconception-Aware Adaptive Learning",
      "title_zh": "EDGEï¼šä¸€ç§æ„ŸçŸ¥è¯¯è§£çš„è‡ªé€‚åº”å­¦ä¹ ç†è®ºæ¡†æ¶",
      "authors": [
        "Ananda Prakash Verma"
      ],
      "abstract": "We present EDGE, a general-purpose, misconception-aware adaptive learning framework composed of four stages: Evaluate (ability and state estimation), Diagnose (posterior infer-ence of misconceptions), Generate (counterfactual item synthesis), and Exercise (index-based retrieval scheduling). EDGE unifies psychometrics (IRT/Bayesian state space models), cog-nitive diagnostics (misconception discovery from distractor patterns and response latencies), contrastive item generation (minimal perturbations that invalidate learner shortcuts while pre-serving psychometric validity), and principled scheduling (a restless bandit approximation to spaced retrieval). We formalize a composite readiness metric, EdgeScore, prove its monotonicity and Lipschitz continuity, and derive an index policy that is near-optimal under mild assumptions on forgetting and learning gains. We further establish conditions under which counterfactual items provably reduce the posterior probability of a targeted misconception faster than standard practice. The paper focuses on theory and implementable pseudocode; empirical study is left to future work.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† EDGEï¼Œä¸€ä¸ªé€šç”¨çš„ã€å…·å¤‡é”™è¯¯æ¦‚å¿µæ„ŸçŸ¥èƒ½åŠ›çš„è‡ªé€‚åº”å­¦ä¹ ç†è®ºæ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³å­¦ä¹ è¿‡ç¨‹ä¸­çš„è®¤çŸ¥åå·®é—®é¢˜ã€‚è¯¥æ¡†æ¶ç”±è¯„ä¼°(Evaluate)ã€è¯Šæ–­(Diagnose)ã€ç”Ÿæˆ(Generate)å’Œç»ƒä¹ (Exercise)å››ä¸ªæ ¸å¿ƒé˜¶æ®µç»„æˆï¼Œæ•´åˆäº†å¿ƒç†æµ‹é‡å­¦(IRT/Bayesian state space models)ã€è®¤çŸ¥è¯Šæ–­(cognitive diagnostics)ä»¥åŠå¯¹æ¯”é¡¹ç”Ÿæˆ(contrastive item generation)ç­‰æŠ€æœ¯ã€‚ç ”ç©¶è€…å®šä¹‰å¹¶è¯æ˜äº†ç»¼åˆå°±ç»ªæŒ‡æ ‡ EdgeScore çš„å•è°ƒæ€§å’Œåˆ©æ™®å¸ŒèŒ¨è¿ç»­æ€§(Lipschitz continuity)ï¼Œå¹¶æ¨å¯¼å‡ºä¸€ç§åœ¨ç‰¹å®šå‡è®¾ä¸‹æ¥è¿‘æœ€ä¼˜çš„ç´¢å¼•ç­–ç•¥ã€‚ç†è®ºè¯æ˜æ˜¾ç¤ºï¼Œé€šè¿‡åäº‹å®é¢˜ç›®(counterfactual items)è¿›è¡Œå¹²é¢„ï¼Œå¯ä»¥æ¯”æ ‡å‡†å®è·µæ›´æœ‰æ•ˆåœ°é™ä½å­¦ä¹ è€…äº§ç”Ÿç‰¹å®šé”™è¯¯æ¦‚å¿µçš„æ¦‚ç‡ã€‚å…¨æ–‡èšç„¦äºä¸¥è°¨çš„ç†è®ºæ¡†æ¶æ„å»ºä¸å¯æ‰§è¡Œçš„ä¼ªä»£ç å®ç°ï¼Œä¸ºæœªæ¥å¼€å‘é«˜æ•ˆã€ç§‘å­¦çš„è‡ªé€‚åº”å­¦ä¹ ç³»ç»Ÿå¥ å®šäº†åšå®çš„æ•°å­¦ä¸ç®—æ³•åŸºç¡€ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.07224v1",
      "published_date": "2025-08-10 08:06:00 UTC",
      "updated_date": "2025-08-10 08:06:00 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T10:46:26.488265+00:00"
    },
    {
      "arxiv_id": "2508.07223v1",
      "title": "Selection and Exploitation of High-Quality Knowledge from Large Language Models for Recommendation",
      "title_zh": "é¢å‘æ¨èçš„å¤§è¯­è¨€æ¨¡å‹é«˜è´¨é‡çŸ¥è¯†ç­›é€‰ä¸åˆ©ç”¨",
      "authors": [
        "Guanchen Wang",
        "Mingming Ha",
        "Tianbao Ma",
        "Linxun Chen",
        "Zhaojie Liu",
        "Guorui Zhou",
        "Kun Gai"
      ],
      "abstract": "In recent years, there has been growing interest in leveraging the impressive generalization capabilities and reasoning ability of large language models (LLMs) to improve the performance of recommenders. With this operation, recommenders can access and learn the additional world knowledge and reasoning information via LLMs. However, in general, for different users and items, the world knowledge derived from LLMs suffers from issues of hallucination, content redundant, and information homogenization. Directly feeding the generated response embeddings into the recommendation model can lead to unavoidable performance deterioration. To address these challenges, we propose a Knowledge Selection \\& Exploitation Recommendation (KSER) framework, which effectively select and extracts the high-quality knowledge from LLMs. The framework consists of two key components: a knowledge filtering module and a embedding spaces alignment module. In the knowledge filtering module, a Embedding Selection Filter Network (ESFNet) is designed to assign adaptive weights to different knowledge chunks in different knowledge fields. In the space alignment module, an attention-based architecture is proposed to align the semantic embeddings from LLMs with the feature space used to train the recommendation models. In addition, two training strategies--\\textbf{all-parameters training} and \\textbf{extractor-only training}--are proposed to flexibly adapt to different downstream tasks and application scenarios, where the extractor-only training strategy offers a novel perspective on knowledge-augmented recommendation. Experimental results validate the necessity and effectiveness of both the knowledge filtering and alignment modules, and further demonstrate the efficiency and effectiveness of the extractor-only training strategy.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤§è¯­è¨€æ¨¡å‹ (LLMs) åœ¨å¢å¼ºæ¨èç³»ç»Ÿè¿‡ç¨‹ä¸­å­˜åœ¨çš„å¹»è§‰ã€å†…å®¹å†—ä½™åŠä¿¡æ¯åŒè´¨åŒ–æŒ‘æˆ˜ï¼Œæå‡ºäº† KSER (Knowledge Selection & Exploitation Recommendation) æ¡†æ¶ï¼Œæ—¨åœ¨æœ‰æ•ˆç­›é€‰å¹¶æå–æ¥è‡ª LLMs çš„é«˜è´¨é‡çŸ¥è¯†ã€‚è¯¥æ¡†æ¶ç”±çŸ¥è¯†è¿‡æ»¤æ¨¡å—å’Œç©ºé—´å¯¹é½æ¨¡å—ç»„æˆï¼Œé€šè¿‡ ESFNet ä¸ºä¸åŒçŸ¥è¯†é¢†åŸŸçš„çŸ¥è¯†å—åˆ†é…è‡ªé€‚åº”æƒé‡ï¼Œå¹¶åˆ©ç”¨åŸºäº Attention çš„æ¶æ„å°†è¯­ä¹‰ Embedding ä¸æ¨èæ¨¡å‹ç‰¹å¾ç©ºé—´è¿›è¡Œå¯¹é½ã€‚æ­¤å¤–ï¼Œç ”ç©¶æå‡ºäº†å…¨å‚æ•°è®­ç»ƒ (all-parameters training) å’Œä»…æå–å™¨è®­ç»ƒ (extractor-only training) ä¸¤ç§ç­–ç•¥ï¼Œä»¥çµæ´»é€‚é…å¤šæ ·åŒ–çš„ä¸‹æ¸¸ä»»åŠ¡éœ€æ±‚ã€‚å®éªŒç»“æœä¸ä»…éªŒè¯äº†è¿‡æ»¤ä¸å¯¹é½æ¨¡å—å¯¹æ€§èƒ½æå‡çš„å¿…è¦æ€§ï¼Œè¿˜è¿›ä¸€æ­¥è¯æ˜äº†ä»…æå–å™¨è®­ç»ƒç­–ç•¥åœ¨çŸ¥è¯†å¢å¼ºæ¨èåœºæ™¯ä¸­çš„é«˜æ•ˆæ€§ä¸ä¼˜è¶Šæ€§ã€‚",
      "categories": [
        "cs.IR",
        "cs.AI"
      ],
      "primary_category": "cs.IR",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.07223v1",
      "published_date": "2025-08-10 08:03:01 UTC",
      "updated_date": "2025-08-10 08:03:01 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T10:46:46.006041+00:00"
    },
    {
      "arxiv_id": "2508.07221v1",
      "title": "LLM-based Agents for Automated Confounder Discovery and Subgroup Analysis in Causal Inference",
      "title_zh": "åŸºäº LLM çš„æ™ºèƒ½ä½“ï¼šå› æœæ¨æ–­ä¸­çš„è‡ªåŠ¨åŒ–æ··æ‚å› ç´ å‘ç°ä¸äºšç»„åˆ†æ",
      "authors": [
        "Po-Han Lee",
        "Yu-Cheng Lin",
        "Chan-Tung Ku",
        "Chan Hsu",
        "Pei-Cing Huang",
        "Ping-Hsun Wu",
        "Yihuang Kang"
      ],
      "abstract": "Estimating individualized treatment effects from observational data presents a persistent challenge due to unmeasured confounding and structural bias. Causal Machine Learning (causal ML) methods, such as causal trees and doubly robust estimators, provide tools for estimating conditional average treatment effects. These methods have limited effectiveness in complex real-world environments due to the presence of latent confounders or those described in unstructured formats. Moreover, reliance on domain experts for confounder identification and rule interpretation introduces high annotation cost and scalability concerns. In this work, we proposed Large Language Model-based agents for automated confounder discovery and subgroup analysis that integrate agents into the causal ML pipeline to simulate domain expertise. Our framework systematically performs subgroup identification and confounding structure discovery by leveraging the reasoning capabilities of LLM-based agents, which reduces human dependency while preserving interpretability. Experiments on real-world medical datasets show that our proposed approach enhances treatment effect estimation robustness by narrowing confidence intervals and uncovering unrecognized confounding biases. Our findings suggest that LLM-based agents offer a promising path toward scalable, trustworthy, and semantically aware causal inference.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ä¼°è®¡ä¸ªä½“åŒ–æ²»ç–—æ•ˆåº”(Individualized Treatment Effects)æ—¶é¢ä¸´çš„æœªè§‚æµ‹æ··æ‚å’Œå¯¹é¢†åŸŸä¸“å®¶é«˜åº¦ä¾èµ–çš„æŒ‘æˆ˜ï¼Œæå‡ºäº†ä¸€ç§åŸºäºå¤§è¯­è¨€æ¨¡å‹(Large Language Model)æ™ºèƒ½ä½“çš„è‡ªåŠ¨åŒ–æ¡†æ¶ã€‚è¯¥æ¡†æ¶å°†æ™ºèƒ½ä½“é›†æˆåˆ°å› æœæœºå™¨å­¦ä¹ (Causal Machine Learning)æµæ°´çº¿ä¸­ï¼Œåˆ©ç”¨LLMçš„æ¨ç†èƒ½åŠ›æ¨¡æ‹Ÿé¢†åŸŸä¸“å®¶ï¼Œç³»ç»Ÿæ€§åœ°è¿›è¡Œæ··æ‚å› å­å‘ç°(Confounder Discovery)å’Œäºšç»„åˆ†æ(Subgroup Analysis)ã€‚è¿™ç§æ–¹æ³•åœ¨æ˜¾è‘—é™ä½äººå·¥æ ‡æ³¨æˆæœ¬å’Œæå‡å¯æ‰©å±•æ€§çš„åŒæ—¶ï¼Œé€šè¿‡è¯­ä¹‰ç†è§£å¢å¼ºäº†å› æœæ¨æ–­çš„è§£é‡Šæ€§ã€‚åœ¨çœŸå®åŒ»ç–—æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿæœ‰æ•ˆæ­ç¤ºæœªè¢«è¯†åˆ«çš„æ··æ‚åç§»å¹¶ç¼©å°ç½®ä¿¡åŒºé—´(Confidence Intervals)ï¼Œä»è€Œå¢å¼ºæ²»ç–—æ•ˆåº”ä¼°è®¡çš„é²æ£’æ€§ã€‚ç ”ç©¶è¯æ˜äº†åŸºäºLLMçš„æ™ºèƒ½ä½“æ˜¯å®ç°å¤§è§„æ¨¡ã€å¯ä¿¡ä¸”å…·å¤‡è¯­ä¹‰æ„ŸçŸ¥çš„å› æœæ¨æ–­(Causal Inference)çš„æœ‰æ•ˆè·¯å¾„ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.MA",
        "stat.AP",
        "stat.ME"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.07221v1",
      "published_date": "2025-08-10 07:45:49 UTC",
      "updated_date": "2025-08-10 07:45:49 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T10:46:49.285870+00:00"
    },
    {
      "arxiv_id": "2508.07220v1",
      "title": "Neural Bridge Processes",
      "title_zh": "ç¥ç»æ¡¥æ¥è¿‡ç¨‹",
      "authors": [
        "Jian Xu",
        "Yican Liu",
        "Qibin Zhao",
        "John Paisley",
        "Delu Zeng"
      ],
      "abstract": "Learning stochastic functions from partially observed context-target pairs is a fundamental problem in probabilistic modeling. Traditional models like Gaussian Processes (GPs) face scalability issues with large datasets and assume Gaussianity, limiting their applicability. While Neural Processes (NPs) offer more flexibility, they struggle with capturing complex, multi-modal target distributions. Neural Diffusion Processes (NDPs) enhance expressivity through a learned diffusion process but rely solely on conditional signals in the denoising network, resulting in weak input coupling from an unconditional forward process and semantic mismatch at the diffusion endpoint. In this work, we propose Neural Bridge Processes (NBPs), a novel method for modeling stochastic functions where inputs x act as dynamic anchors for the entire diffusion trajectory. By reformulating the forward kernel to explicitly depend on x, NBP enforces a constrained path that strictly terminates at the supervised target. This approach not only provides stronger gradient signals but also guarantees endpoint coherence. We validate NBPs on synthetic data, EEG signal regression and image regression tasks, achieving substantial improvements over baselines. These results underscore the effectiveness of DDPM-style bridge sampling in enhancing both performance and theoretical consistency for structured prediction tasks.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ä»éƒ¨åˆ†è§‚æµ‹æ•°æ®ä¸­å­¦ä¹ éšæœºå‡½æ•°è¿™ä¸€åŸºæœ¬é—®é¢˜ï¼ŒæŒ‡å‡ºä¼ ç»Ÿ Gaussian Processes (GPs) çš„å¯æ‰©å±•æ€§æœ‰é™ï¼Œä¸” Neural Processes (NPs) åœ¨å¤„ç†å¤æ‚å¤šå³°åˆ†å¸ƒæ—¶å­˜åœ¨å›°éš¾ã€‚ç°æœ‰çš„ Neural Diffusion Processes (NDPs) è™½ç„¶å¢å¼ºäº†è¡¨è¾¾èƒ½åŠ›ï¼Œä½†ç”±äºå…¶å‰å‘è¿‡ç¨‹ä¸è¾“å…¥ä¿¡å·è§£è€¦ï¼Œå¯¼è‡´è¾“å…¥è€¦åˆè¾ƒå¼±ä¸”åœ¨æ‰©æ•£ç«¯ç‚¹å­˜åœ¨è¯­ä¹‰å¤±é…é—®é¢˜ã€‚ä¸ºäº†è§£å†³è¿™äº›ç¼ºé™·ï¼Œä½œè€…æå‡ºäº† Neural Bridge Processes (NBPs)ï¼Œè¿™æ˜¯ä¸€ç§å°†è¾“å…¥ $x$ ä½œä¸ºæ•´ä¸ªæ‰©æ•£è½¨è¿¹åŠ¨æ€é”šç‚¹çš„éšæœºå‡½æ•°å»ºæ¨¡æ–°æ–¹æ³•ã€‚NBP é€šè¿‡é‡æ–°è¡¨è¿°å‰å‘æ ¸å‡½æ•°ä½¿å…¶æ˜¾å¼ä¾èµ–äº $x$ï¼Œä»è€Œå¼ºåˆ¶æ‰©æ•£è·¯å¾„ä¸¥æ ¼ç»ˆæ­¢äºå—ç›‘ç£çš„ç›®æ ‡ï¼Œæ˜¾è‘—å¢å¼ºäº†æ¢¯åº¦ä¿¡å·å¹¶ä¿è¯äº†ç«¯ç‚¹çš„ä¸€è‡´æ€§ã€‚è¯¥æ–¹æ³•åœ¨åˆæˆæ•°æ®ã€EEG ä¿¡å·å›å½’å’Œå›¾åƒå›å½’ä»»åŠ¡ä¸­è¿›è¡Œäº†éªŒè¯ï¼Œå®éªŒç»“æœè¡¨æ˜ NBPs åœ¨æ€§èƒ½å’Œç†è®ºä¸€è‡´æ€§ä¸Šå‡æ˜¾è‘—ä¼˜äºåŸºå‡†æ¨¡å‹ã€‚è¿™å……åˆ†è¯æ˜äº†åŸºäº DDPM é£æ ¼çš„ bridge sampling åœ¨æå‡ç»“æ„åŒ–é¢„æµ‹ä»»åŠ¡è¡¨ç°å’Œç†è®ºå®Œå¤‡æ€§æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.07220v1",
      "published_date": "2025-08-10 07:44:52 UTC",
      "updated_date": "2025-08-10 07:44:52 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T10:46:54.599615+00:00"
    },
    {
      "arxiv_id": "2508.07208v2",
      "title": "What One Cannot, Two Can: Two-Layer Transformers Provably Represent Induction Heads on Any-Order Markov Chains",
      "title_zh": "å•å±‚ä¸å¯ï¼ŒåŒå±‚å¯ä¸ºï¼šåŒå±‚ Transformer å¯è¯æ˜åœ°è¡¨å¾ä»»æ„é˜¶é©¬å°”å¯å¤«é“¾ä¸Šçš„å½’çº³å¤´",
      "authors": [
        "Chanakya Ekbote",
        "Marco Bondaschi",
        "Nived Rajaraman",
        "Jason D. Lee",
        "Michael Gastpar",
        "Ashok Vardhan Makkuva",
        "Paul Pu Liang"
      ],
      "abstract": "In-context learning (ICL) is a hallmark capability of transformers, through which trained models learn to adapt to new tasks by leveraging information from the input context. Prior work has shown that ICL emerges in transformers due to the presence of special circuits called induction heads. Given the equivalence between induction heads and conditional k-grams, a recent line of work modeling sequential inputs as Markov processes has revealed the fundamental impact of model depth on its ICL capabilities: while a two-layer transformer can efficiently represent a conditional 1-gram model, its single-layer counterpart cannot solve the task unless it is exponentially large. However, for higher order Markov sources, the best known constructions require at least three layers (each with a single attention head) - leaving open the question: can a two-layer single-head transformer represent any kth-order Markov process? In this paper, we precisely address this and theoretically show that a two-layer transformer with one head per layer can indeed represent any conditional k-gram. Thus, our result provides the tightest known characterization of the interplay between transformer depth and Markov order for ICL. Building on this, we further analyze the learning dynamics of our two-layer construction, focusing on a simplified variant for first-order Markov chains, illustrating how effective in-context representations emerge during training. Together, these results deepen our current understanding of transformer-based ICL and illustrate how even shallow architectures can surprisingly exhibit strong ICL capabilities on structured sequence modeling tasks.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†Transformeræ¨¡å‹ä¸­In-context learning (ICL)èƒ½åŠ›çš„æ•°å­¦åŸºç¡€ï¼Œç‰¹åˆ«æ˜¯Induction Headsåœ¨å¤„ç†ç»“æ„åŒ–åºåˆ—å»ºæ¨¡ä»»åŠ¡ä¸­çš„å…³é”®ä½œç”¨ã€‚æ­¤å‰ç ”ç©¶è®¤ä¸ºï¼Œè¡¨å¾é«˜é˜¶Markovè¿‡ç¨‹é€šå¸¸éœ€è¦è‡³å°‘ä¸‰å±‚æ¶æ„ï¼Œè€Œæœ¬ç ”ç©¶åœ¨ç†è®ºä¸Šè¯æ˜äº†ä»…éœ€ä¸¤å±‚Transformerï¼ˆæ¯å±‚é…å¤‡ä¸€ä¸ªAttention Headï¼‰å³å¯è¡¨å¾ä»»æ„$k$é˜¶Markové“¾çš„Conditional k-gramã€‚è¿™ä¸€ç»“è®ºä¸ºTransformeræ¨¡å‹æ·±åº¦ä¸Markové˜¶æ•°ä¹‹é—´çš„ç›¸äº’ä½œç”¨æä¾›äº†ç›®å‰å·²çŸ¥æœ€ç´§è‡´çš„ç†è®ºåˆ»ç”»ã€‚ç ”ç©¶è¿›ä¸€æ­¥åˆ†æäº†ä¸¤å±‚æ„é€ åœ¨ç®€åŒ–ä¸€é˜¶Markové“¾ä¸‹çš„Learning Dynamicsï¼Œé˜æ˜äº†æœ‰æ•ˆçš„In-context representationsåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­çš„æ¶Œç°æœºåˆ¶ã€‚è¯¥å·¥ä½œä¸ä»…æ·±åŒ–äº†å¯¹Transformer-based ICLçš„ç†è§£ï¼Œè¿˜æ­ç¤ºäº†å³ä½¿æ˜¯æµ…å±‚æ¶æ„åœ¨å¤æ‚åºåˆ—å»ºæ¨¡ä»»åŠ¡ä¸­ä¹Ÿå…·æœ‰å‡ºè‰²çš„è¡¨å¾æ½œåŠ›ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "Accepted at NeurIPS 2025",
      "pdf_url": "https://arxiv.org/pdf/2508.07208v2",
      "published_date": "2025-08-10 07:03:01 UTC",
      "updated_date": "2025-11-14 19:59:48 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T10:46:58.696234+00:00"
    },
    {
      "arxiv_id": "2508.07207v1",
      "title": "Presburger Functional Synthesis: Complexity and Tractable Normal Forms",
      "title_zh": "Presburger å‡½æ•°åˆæˆï¼šå¤æ‚åº¦ä¸æ˜“å¤„ç†èŒƒå¼",
      "authors": [
        "S. Akshay",
        "A. R. Balasubramanian",
        "Supratik Chakraborty",
        "Georg Zetzsche"
      ],
      "abstract": "Given a relational specification between inputs and outputs as a logic formula, the problem of functional synthesis is to automatically synthesize a function from inputs to outputs satisfying the relation. Recently, a rich line of work has emerged tackling this problem for specifications in different theories, from Boolean to general first-order logic. In this paper, we launch an investigation of this problem for the theory of Presburger Arithmetic, that we call Presburger Functional Synthesis (PFnS). We show that PFnS can be solved in EXPTIME and provide a matching exponential lower bound. This is unlike the case for Boolean functional synthesis (BFnS), where only conditional exponential lower bounds are known. Further, we show that PFnS for one input and one output variable is as hard as BFnS in general. We then identify a special normal form, called PSyNF, for the specification formula that guarantees poly-time and poly-size solvability of PFnS. We prove several properties of PSyNF, including how to check and compile to this form, and conditions under which any other form that guarantees poly-time solvability of PFnS can be compiled in poly-time to PSyNF. Finally, we identify a syntactic normal form that is easier to check but is exponentially less succinct than PSyNF.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº† Presburger Arithmetic ç†è®ºä¸‹çš„å‡½æ•°åˆæˆé—®é¢˜ï¼Œå°†å…¶å®šä¹‰ä¸º Presburger Functional Synthesis (PFnS)ã€‚ç ”ç©¶è¯æ˜äº† PFnS å¯ä»¥åœ¨ EXPTIME å¤æ‚åº¦å†…è§£å†³ï¼Œå¹¶æä¾›äº†åŒ¹é…çš„æŒ‡æ•°ä¸‹ç•Œï¼Œè¿™ä¸ Boolean Functional Synthesis (BFnS) ä»…å­˜åœ¨æ¡ä»¶æŒ‡æ•°ä¸‹ç•Œçš„æƒ…å†µæœ‰æ‰€ä¸åŒã€‚ä½œè€…æŒ‡å‡ºå³ä½¿åœ¨åªæœ‰ä¸€ä¸ªè¾“å…¥å’Œä¸€ä¸ªè¾“å‡ºå˜é‡çš„æƒ…å†µä¸‹ï¼ŒPFnS çš„å¤æ‚æ€§ä¹Ÿä¸é€šç”¨çš„ BFnS ç›¸å½“ã€‚ä¸ºäº†æå‡æ±‚è§£æ•ˆç‡ï¼Œç ”ç©¶æå‡ºäº†ä¸€ç§åä¸º PSyNF çš„ç‰¹æ®Šæ­£è§„å½¢å¼ (Normal Form)ï¼Œè¯¥å½¢å¼èƒ½ç¡®ä¿ PFnS åœ¨å¤šé¡¹å¼æ—¶é—´å¤æ‚åº¦å’Œå¤šé¡¹å¼ç©ºé—´è§„æ¨¡å†…å¯è§£ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡è¿˜è®ºè¯äº† PSyNF çš„æ ¡éªŒä¸ç¼–è¯‘å±æ€§ï¼Œå¹¶è¯†åˆ«å‡ºä¸€ç§è™½ç„¶æ›´æ˜“äºæ ¡éªŒä½†ç®€æ´åº¦å‘ˆæŒ‡æ•°çº§ä¸‹é™çš„å¥æ³•æ­£è§„å½¢å¼ã€‚",
      "categories": [
        "cs.LO",
        "cs.AI"
      ],
      "primary_category": "cs.LO",
      "comment": "Full version of conference paper at KR 2025 (22nd International Conference on Principles of Knowledge Representation and Reasoning)",
      "pdf_url": "https://arxiv.org/pdf/2508.07207v1",
      "published_date": "2025-08-10 07:00:34 UTC",
      "updated_date": "2025-08-10 07:00:34 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T10:47:07.986025+00:00"
    },
    {
      "arxiv_id": "2508.07201v1",
      "title": "Propagation Tree Is Not Deep: Adaptive Graph Contrastive Learning Approach for Rumor Detection",
      "title_zh": "ä¼ æ’­æ ‘å¹¶éæ·±å±‚ï¼šé¢å‘è°£è¨€æ£€æµ‹çš„è‡ªé€‚åº”å›¾å¯¹æ¯”å­¦ä¹ æ–¹æ³•",
      "authors": [
        "Chaoqun Cui",
        "Caiyan Jia"
      ],
      "abstract": "Rumor detection on social media has become increasingly important. Most existing graph-based models presume rumor propagation trees (RPTs) have deep structures and learn sequential stance features along branches. However, through statistical analysis on real-world datasets, we find RPTs exhibit wide structures, with most nodes being shallow 1-level replies. To focus learning on intensive substructures, we propose Rumor Adaptive Graph Contrastive Learning (RAGCL) method with adaptive view augmentation guided by node centralities. We summarize three principles for RPT augmentation: 1) exempt root nodes, 2) retain deep reply nodes, 3) preserve lower-level nodes in deep sections. We employ node dropping, attribute masking and edge dropping with probabilities from centrality-based importance scores to generate views. A graph contrastive objective then learns robust rumor representations. Extensive experiments on four benchmark datasets demonstrate RAGCL outperforms state-of-the-art methods. Our work reveals the wide-structure nature of RPTs and contributes an effective graph contrastive learning approach tailored for rumor detection through principled adaptive augmentation. The proposed principles and augmentation techniques can potentially benefit other applications involving tree-structured graphs.",
      "tldr_zh": "è¯¥ç ”ç©¶æ­ç¤ºäº†ç¤¾äº¤åª’ä½“è°£è¨€ä¼ æ’­æ ‘ï¼ˆRumor Propagation Trees, RPTsï¼‰çš„å®½ç»“æ„ç‰¹æ€§ï¼ŒæŒ‡å‡ºå¤§å¤šæ•°èŠ‚ç‚¹å®é™…ä¸Šæ˜¯æµ…å±‚çš„ä¸€çº§å›å¤ï¼Œè€Œéä¼ ç»Ÿæ¨¡å‹æ‰€å‡è®¾çš„æ·±å±‚ç»“æ„ã€‚ä¸ºäº†æ›´æœ‰æ•ˆåœ°æ•è·å¯†é›†å­ç»“æ„ç‰¹å¾ï¼Œä½œè€…æå‡ºäº†è°£è¨€è‡ªé€‚åº”å›¾å¯¹æ¯”å­¦ä¹ ï¼ˆRumor Adaptive Graph Contrastive Learning, RAGCLï¼‰æ–¹æ³•ï¼Œåˆ©ç”¨åŸºäºèŠ‚ç‚¹ä¸­å¿ƒæ€§çš„è‡ªé€‚åº”è§†å›¾å¢å¼ºï¼ˆview augmentationï¼‰æŠ€æœ¯ã€‚è¯¥æ–¹æ³•éµå¾ªè±å…æ ¹èŠ‚ç‚¹ã€ä¿ç•™æ·±å±‚å›å¤èŠ‚ç‚¹ä»¥åŠä¿æŠ¤æ·±å±‚åŒºåŸŸåº•å±‚èŠ‚ç‚¹è¿™ä¸‰é¡¹æ ¸å¿ƒåŸåˆ™ï¼Œé€šè¿‡èŠ‚ç‚¹ä¸¢å¼ƒï¼ˆnode droppingï¼‰ã€å±æ€§æ©ç ï¼ˆattribute maskingï¼‰å’Œè¾¹ä¸¢å¼ƒï¼ˆedge droppingï¼‰ç”Ÿæˆå¢å¼ºè§†å›¾ã€‚é€šè¿‡å¼•å…¥å›¾å¯¹æ¯”å­¦ä¹ ç›®æ ‡ï¼ŒRAGCL èƒ½å¤Ÿå­¦ä¹ åˆ°æ›´å…·é²æ£’æ€§çš„è°£è¨€ç‰¹å¾è¡¨ç¤ºã€‚åœ¨å››ä¸ªåŸºå‡†æ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼ŒRAGCL çš„æ€§èƒ½ä¼˜äºå½“å‰æœ€å…ˆè¿›çš„æ–¹æ³•ã€‚è¯¥å·¥ä½œä¸ä»…æ·±åŒ–äº†å¯¹ RPTs ç»“æ„çš„ç†è§£ï¼Œè¿˜ä¸ºæ¶‰åŠæ ‘çŠ¶å›¾ç»“æ„çš„å…¶ä»–åº”ç”¨æä¾›äº†é€šç”¨çš„è‡ªé€‚åº”å¢å¼ºç­–ç•¥ã€‚",
      "categories": [
        "cs.SI",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.SI",
      "comment": "This paper is accepted by AAAI2024",
      "pdf_url": "https://arxiv.org/pdf/2508.07201v1",
      "published_date": "2025-08-10 06:53:30 UTC",
      "updated_date": "2025-08-10 06:53:30 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T10:47:10.358978+00:00"
    },
    {
      "arxiv_id": "2508.07196v1",
      "title": "Can Smaller Large Language Models Evaluate Research Quality?",
      "title_zh": "è¾ƒå°è§„æ¨¡çš„å¤§è¯­è¨€æ¨¡å‹èƒ½å¦è¯„ä¼°ç ”ç©¶è´¨é‡ï¼Ÿ",
      "authors": [
        "Mike Thelwall"
      ],
      "abstract": "Although both Google Gemini (1.5 Flash) and ChatGPT (4o and 4o-mini) give research quality evaluation scores that correlate positively with expert scores in nearly all fields, and more strongly that citations in most, it is not known whether this is true for smaller Large Language Models (LLMs). In response, this article assesses Google's Gemma-3-27b-it, a downloadable LLM (60Gb). The results for 104,187 articles show that Gemma-3-27b-it scores correlate positively with an expert research quality score proxy for all 34 Units of Assessment (broad fields) from the UK Research Excellence Framework 2021. The Gemma-3-27b-it correlations have 83.8% of the strength of ChatGPT 4o and 94.7% of the strength of ChatGPT 4o-mini correlations. Differently from the two larger LLMs, the Gemma-3-27b-it correlations do not increase substantially when the scores are averaged across five repetitions, its scores tend to be lower, and its reports are relatively uniform in style. Overall, the results show that research quality score estimation can be conducted by offline LLMs, so this capability is not an emergent property of the largest LLMs. Moreover, score improvement through repetition is not a universal feature of LLMs. In conclusion, although the largest LLMs still have the highest research evaluation score estimation capability, smaller ones can also be used for this task, and this can be helpful for cost saving or when secure offline processing is needed.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†è¾ƒå°è§„æ¨¡çš„å¤§è¯­è¨€æ¨¡å‹(LLMs)æ˜¯å¦å…·å¤‡è¯„ä¼°ç ”ç©¶è´¨é‡çš„èƒ½åŠ›ï¼Œå¹¶é‡ç‚¹è¯„ä¼°äº†Googleå‘å¸ƒçš„ç¦»çº¿æ¨¡å‹Gemma-3-27b-itã€‚ç ”ç©¶é€šè¿‡å¯¹è‹±å›½2021å¹´å“è¶Šç ”ç©¶æ¡†æ¶(REF 2021)ä¸­æ¶µç›–34ä¸ªé¢†åŸŸçš„104,187ç¯‡è®ºæ–‡è¿›è¡Œæµ‹è¯•ï¼Œåˆ†æäº†æ¨¡å‹è¯„åˆ†ä¸ä¸“å®¶è¯„å®¡åˆ†æ•°ä¹‹é—´çš„ç›¸å…³æ€§ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒGemma-3-27b-itåœ¨æ‰€æœ‰ç ”ç©¶é¢†åŸŸå‡è¡¨ç°å‡ºæ­£ç›¸å…³æ€§ï¼Œå…¶ç›¸å…³æ€§å¼ºåº¦åˆ†åˆ«è¾¾åˆ°ChatGPT 4oçš„83.8%å’ŒChatGPT 4o-miniçš„94.7%ã€‚ä¸å¤§å‹æ¨¡å‹ä¸åŒï¼Œè¯¥æ¨¡å‹çš„è¯„åˆ†åœ¨å¤šæ¬¡é‡å¤å¹³å‡åå¹¶æœªæ˜¾è‘—æå‡ï¼Œä¸”æŠ¥å‘Šé£æ ¼ç›¸å¯¹ç»Ÿä¸€ã€‚è¿™ä¸€å‘ç°è¯æ˜äº†ç ”ç©¶è´¨é‡è¯„ä¼°èƒ½åŠ›å¹¶éè¶…å¤§å‹LLMsçš„ä¸“å±æ¶Œç°å±æ€§(emergent property)ï¼Œç¦»çº¿æ¨¡å‹ä¹Ÿèƒ½èƒœä»»è¯¥ä»»åŠ¡ã€‚ç»¼ä¸Šæ‰€è¿°ï¼Œè™½ç„¶é¡¶çº§å¤§å‹æ¨¡å‹ä»å…·æœ‰æœ€é«˜è¯„ä¼°ç²¾åº¦ï¼Œä½†å°å‹æ¨¡å‹ä¸ºé™ä½æˆæœ¬å’Œä¿éšœæ•°æ®å®‰å…¨ç¦»çº¿å¤„ç†æä¾›äº†åˆ‡å®å¯è¡Œçš„é€‰æ‹©ã€‚",
      "categories": [
        "cs.DL",
        "cs.AI"
      ],
      "primary_category": "cs.DL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.07196v1",
      "published_date": "2025-08-10 06:18:40 UTC",
      "updated_date": "2025-08-10 06:18:40 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T10:47:09.953511+00:00"
    },
    {
      "arxiv_id": "2508.07195v1",
      "title": "Adapting LLMs to Time Series Forecasting via Temporal Heterogeneity Modeling and Semantic Alignment",
      "title_zh": "é€šè¿‡æ—¶é—´å¼‚è´¨æ€§å»ºæ¨¡ä¸è¯­ä¹‰å¯¹é½å°†å¤§è¯­è¨€æ¨¡å‹é€‚é…äºæ—¶é—´åºåˆ—é¢„æµ‹",
      "authors": [
        "Yanru Sun",
        "Emadeldeen Eldele",
        "Zongxia Xie",
        "Yucheng Wang",
        "Wenzhe Niu",
        "Qinghua Hu",
        "Chee Keong Kwoh",
        "Min Wu"
      ],
      "abstract": "Large Language Models (LLMs) have recently demonstrated impressive capabilities in natural language processing due to their strong generalization and sequence modeling capabilities. However, their direct application to time series forecasting remains challenging due to two fundamental issues: the inherent heterogeneity of temporal patterns and the modality gap between continuous numerical signals and discrete language representations. In this work, we propose TALON, a unified framework that enhances LLM-based forecasting by modeling temporal heterogeneity and enforcing semantic alignment. Specifically, we design a Heterogeneous Temporal Encoder that partitions multivariate time series into structurally coherent segments, enabling localized expert modeling across diverse temporal patterns. To bridge the modality gap, we introduce a Semantic Alignment Module that aligns temporal features with LLM-compatible representations, enabling effective integration of time series into language-based models while eliminating the need for handcrafted prompts during inference. Extensive experiments on seven real-world benchmarks demonstrate that TALON achieves superior performance across all datasets, with average MSE improvements of up to 11\\% over recent state-of-the-art methods. These results underscore the effectiveness of incorporating both pattern-aware and semantic-aware designs when adapting LLMs for time series forecasting. The code is available at: https://github.com/syrGitHub/TALON.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† TALONï¼Œä¸€ä¸ªæ—¨åœ¨å¢å¼ºåŸºäº Large Language Models (LLMs) æ—¶é—´åºåˆ—é¢„æµ‹èƒ½åŠ›çš„ç»Ÿä¸€æ¡†æ¶ï¼Œé‡ç‚¹è§£å†³æ—¶é—´æ¨¡å¼å¼‚è´¨æ€§ä»¥åŠæ•°å€¼ä¿¡å·ä¸è¯­è¨€è¡¨ç¤ºä¹‹é—´çš„æ¨¡æ€é¸¿æ²Ÿã€‚æ¡†æ¶æ ¸å¿ƒåŒ…å«ä¸€ä¸ª Heterogeneous Temporal Encoderï¼Œé€šè¿‡å°†å¤šå…ƒæ—¶é—´åºåˆ—åˆ’åˆ†ä¸ºç»“æ„ä¸€è‡´çš„ç‰‡æ®µï¼Œå®ç°äº†é’ˆå¯¹å¤šæ ·åŒ–æ¨¡å¼çš„å±€éƒ¨åŒ–ä¸“å®¶å»ºæ¨¡ã€‚åŒæ—¶ï¼ŒTALON å¼•å…¥äº† Semantic Alignment Moduleï¼Œå°†æ—¶é—´ç‰¹å¾ä¸ LLM å…¼å®¹çš„è¡¨ç¤ºè¿›è¡Œè¯­ä¹‰å¯¹é½ï¼Œä»è€Œåœ¨æ¨ç†é˜¶æ®µæ— éœ€æ‰‹å·¥è®¾è®¡æç¤º (handcrafted prompts) å³å¯å®ç°é«˜æ•ˆé›†æˆã€‚åœ¨ä¸ƒä¸ªçœŸå®ä¸–ç•ŒåŸºå‡†æ•°æ®é›†ä¸Šçš„å®éªŒç»“æœæ˜¾ç¤ºï¼ŒTALON åœ¨æ‰€æœ‰æµ‹è¯•ä¸­å‡å–å¾—äº†ä¼˜å¼‚æ€§èƒ½ï¼Œå…¶å¹³å‡å‡æ–¹è¯¯å·® (MSE) è¾ƒç°æœ‰æœ€å…ˆè¿›æ–¹æ³•æå‡äº†é«˜è¾¾ 11%ã€‚è¿™ä¸€æˆæœå……åˆ†è¯æ˜äº†åœ¨å°† LLMs é€‚é…äºæ—¶é—´åºåˆ—é¢†åŸŸæ—¶ï¼Œç»“åˆæ¨¡å¼æ„ŸçŸ¥ä¸è¯­ä¹‰æ„ŸçŸ¥è®¾è®¡çš„å…³é”®ä½œç”¨ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.07195v1",
      "published_date": "2025-08-10 06:06:19 UTC",
      "updated_date": "2025-08-10 06:06:19 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T10:47:18.164572+00:00"
    },
    {
      "arxiv_id": "2508.07186v1",
      "title": "Multi-Dimensional Summarization Agents with Context-Aware Reasoning over Enterprise Tables",
      "title_zh": "å…·æœ‰ä¸Šä¸‹æ–‡æ„ŸçŸ¥æ¨ç†èƒ½åŠ›çš„ä¼ä¸šè¡¨æ ¼å¤šç»´æ‘˜è¦æ™ºèƒ½ä½“",
      "authors": [
        "Amit Dhanda"
      ],
      "abstract": "We propose a novel framework for summarizing structured enterprise data across multiple dimensions using large language model (LLM)-based agents. Traditional table-to-text models often lack the capacity to reason across hierarchical structures and context-aware deltas, which are essential in business reporting tasks. Our method introduces a multi-agent pipeline that extracts, analyzes, and summarizes multi-dimensional data using agents for slicing, variance detection, context construction, and LLM-based generation. Our results show that the proposed framework outperforms traditional approaches, achieving 83\\% faithfulness to underlying data, superior coverage of significant changes, and high relevance scores (4.4/5) for decision-critical insights. The improvements are especially pronounced in categories involving subtle trade-offs, such as increased revenue due to price changes amid declining unit volumes, which competing methods either overlook or address with limited specificity. We evaluate the framework on Kaggle datasets and demonstrate significant improvements in faithfulness, relevance, and insight quality over baseline table summarization approaches.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ä¼ä¸šç»“æ„åŒ–æ•°æ®æå‡ºäº†ä¸€ä¸ªåŸºäºå¤§è¯­è¨€æ¨¡å‹(LLM)çš„å¤šç»´åº¦æ‘˜è¦æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³ä¼ ç»Ÿè¡¨è½¬æ–‡æœ¬(Table-to-Text)æ¨¡å‹åœ¨å¤„ç†ä¸šåŠ¡æŠ¥å‘Šä¸­å±‚çº§ç»“æ„å’Œä¸Šä¸‹æ–‡æ„ŸçŸ¥å¢é‡(Context-Aware Deltas)æ—¶æ¨ç†èƒ½åŠ›ä¸è¶³çš„é—®é¢˜ã€‚è¯¥æ–¹æ³•å¼•å…¥äº†ä¸€ä¸ªå¤šæ™ºèƒ½ä½“æµæ°´çº¿(Multi-Agent Pipeline)ï¼Œé€šè¿‡ä¸“é—¨çš„æ™ºèƒ½ä½“æ‰§è¡Œæ•°æ®åˆ‡ç‰‡(Slicing)ã€æ–¹å·®æ£€æµ‹(Variance Detection)å’Œä¸Šä¸‹æ–‡æ„å»ºï¼Œç¡®ä¿å¯¹å¤šç»´æ•°æ®çš„æ·±åº¦è§£æã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯¥æ¡†æ¶åœ¨æ•°æ®å¿ å®åº¦(Faithfulness)ä¸Šè¾¾åˆ°83%ï¼Œä¸”åœ¨å†³ç­–å…³é”®æ´å¯Ÿçš„ç›¸å…³æ€§è¯„åˆ†ä¸­å–å¾—4.4/5çš„é«˜åˆ†ã€‚ç‰¹åˆ«æ˜¯åœ¨å¤„ç†æ¶‰åŠç»†å¾®æƒè¡¡çš„å¤æ‚ä¸šåŠ¡åœºæ™¯ï¼ˆå¦‚ä»·æ ¼å˜åŠ¨å¼•èµ·æ”¶å…¥å¢é•¿ä½†é”€é‡ä¸‹é™ï¼‰æ—¶ï¼Œè¯¥æ¡†æ¶å±•ç°å‡ºæ¯”åŸºçº¿æ¨¡å‹æ›´é«˜çš„ç‰¹å¼‚æ€§å’Œå‡†ç¡®æ€§ã€‚é€šè¿‡åœ¨Kaggleæ•°æ®é›†ä¸Šçš„å¹¿æ³›è¯„ä¼°ï¼Œè¯¥ç ”ç©¶è¯æ˜äº†å¤šæ™ºèƒ½ä½“åä½œåœ¨æå‡ä¼ä¸šæŠ¥è¡¨è‡ªåŠ¨åŒ–ç”Ÿæˆè´¨é‡åŠæ´å¯ŸåŠ›æ–¹é¢çš„æ˜¾è‘—æ½œåŠ›ã€‚",
      "categories": [
        "cs.AI",
        "cs.MA"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.07186v1",
      "published_date": "2025-08-10 05:27:42 UTC",
      "updated_date": "2025-08-10 05:27:42 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T10:47:14.862416+00:00"
    },
    {
      "arxiv_id": "2508.07185v3",
      "title": "DySK-Attn: A Framework for Efficient, Real-Time Knowledge Updating in Large Language Models via Dynamic Sparse Knowledge Attention",
      "title_zh": "DySK-Attnï¼šåŸºäºåŠ¨æ€ç¨€ç–çŸ¥è¯†æ³¨æ„åŠ›çš„å¤§è¯­è¨€æ¨¡å‹é«˜æ•ˆå®æ—¶çŸ¥è¯†æ›´æ–°æ¡†æ¶",
      "authors": [
        "Kabir Khan",
        "Priya Sharma",
        "Arjun Mehta",
        "Neha Gupta",
        "Ravi Narayanan"
      ],
      "abstract": "Large Language Models (LLMs) suffer from a critical limitation: their knowledge is static and quickly becomes outdated. Retraining these massive models is computationally prohibitive, while existing knowledge editing techniques can be slow and may introduce unforeseen side effects. To address this, we propose DySK-Attn, a novel framework that enables LLMs to efficiently integrate real-time knowledge from a dynamic external source. Our approach synergizes an LLM with a dynamic Knowledge Graph (KG) that can be updated instantaneously. The core of our framework is a sparse knowledge attention mechanism, which allows the LLM to perform a coarse-to-fine grained search, efficiently identifying and focusing on a small, highly relevant subset of facts from the vast KG. This mechanism avoids the high computational cost of dense attention over the entire knowledge base and mitigates noise from irrelevant information. We demonstrate through extensive experiments on time-sensitive question-answering tasks that DySK-Attn significantly outperforms strong baselines, including standard Retrieval-Augmented Generation (RAG) and model editing techniques, in both factual accuracy for updated knowledge and computational efficiency. Our framework offers a scalable and effective solution for building LLMs that can stay current with the ever-changing world.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† DySK-Attn æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çŸ¥è¯†æ»åä¸”é‡è®­æˆæœ¬é«˜æ˜‚çš„é—®é¢˜ã€‚è¯¥æ¡†æ¶é€šè¿‡å°† LLM ä¸å¯å³æ—¶æ›´æ–°çš„åŠ¨æ€çŸ¥è¯†å›¾è°±ï¼ˆKnowledge Graph, KGï¼‰ç›¸ç»“åˆï¼Œå®ç°äº†å®æ—¶çŸ¥è¯†é›†æˆã€‚å…¶æ ¸å¿ƒé‡‡ç”¨äº†ç¨€ç–çŸ¥è¯†æ³¨æ„åŠ›æœºåˆ¶ï¼ˆSparse Knowledge Attentionï¼‰ï¼Œé€šè¿‡ä»ç²—åˆ°ç»†çš„ç²’åº¦æœç´¢ï¼Œåœ¨æµ·é‡çŸ¥è¯†åº“ä¸­é«˜æ•ˆå®šä½å¹¶èšç„¦äºæå°‘æ•°é«˜åº¦ç›¸å…³çš„äº‹å®ã€‚è¿™ç§æœºåˆ¶æœ‰æ•ˆè§„é¿äº†å…¨å±€ç¨ å¯†æ³¨æ„åŠ›çš„è®¡ç®—å¼€é”€ï¼Œå¹¶æ˜¾è‘—å‡å°‘äº†æ— å…³ä¿¡æ¯çš„å™ªå£°å¹²æ‰°ã€‚åœ¨æ—¶é—´æ•æ„Ÿå‹é—®ç­”ä»»åŠ¡çš„å®éªŒä¸­ï¼ŒDySK-Attn åœ¨äº‹å®å‡†ç¡®æ€§å’Œè®¡ç®—æ•ˆç‡æ–¹é¢å‡æ˜¾è‘—ä¼˜äºä¼ ç»Ÿçš„æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰å’Œæ¨¡å‹ç¼–è¾‘æŠ€æœ¯ã€‚è¯¥æ¡†æ¶ä¸ºæ„å»ºèƒ½å¤Ÿç´§è·Ÿå®æ—¶å˜åŒ–ã€å…·å¤‡å¯æ‰©å±•æ€§çš„ LLMs æä¾›äº†é«˜æ•ˆçš„è§£å†³æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "Preprint; 7 figures, 3 tables, 1 algorithm; v1. Code and data will be released",
      "pdf_url": "https://arxiv.org/pdf/2508.07185v3",
      "published_date": "2025-08-10 05:22:38 UTC",
      "updated_date": "2025-12-29 10:22:26 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T10:47:24.354303+00:00"
    },
    {
      "arxiv_id": "2508.07183v1",
      "title": "Explainability-in-Action: Enabling Expressive Manipulation and Tacit Understanding by Bending Diffusion Models in ComfyUI",
      "title_zh": "è¡ŒåŠ¨ä¸­çš„å¯è§£é‡Šæ€§ï¼šé€šè¿‡ ComfyUI ä¸­çš„æ‰©æ•£æ¨¡å‹å¹²é¢„å®ç°è¡¨ç°åŠ›æ“æ§ä¸é»˜ä¼šç†è§£",
      "authors": [
        "Ahmed M. Abuzuraiq",
        "Philippe Pasquier"
      ],
      "abstract": "Explainable AI (XAI) in creative contexts can go beyond transparency to support artistic engagement, modifiability, and sustained practice. While curated datasets and training human-scale models can offer artists greater agency and control, large-scale generative models like text-to-image diffusion systems often obscure these possibilities. We suggest that even large models can be treated as creative materials if their internal structure is exposed and manipulable. We propose a craft-based approach to explainability rooted in long-term, hands-on engagement akin to SchÃ¶n's \"reflection-in-action\" and demonstrate its application through a model-bending and inspection plugin integrated into the node-based interface of ComfyUI. We demonstrate that by interactively manipulating different parts of a generative model, artists can develop an intuition about how each component influences the output.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†å¯è§£é‡Šäººå·¥æ™ºèƒ½ (Explainable AI, XAI) åœ¨åˆ›æ„é¢†åŸŸçš„åº”ç”¨ï¼Œæ—¨åœ¨è¶…è¶Šä¼ ç»Ÿçš„é€æ˜åº¦è¦æ±‚ï¼Œé€šè¿‡æ­ç¤ºå¤§è§„æ¨¡ç”Ÿæˆæ¨¡å‹ï¼ˆå¦‚æ–‡æœ¬ç”Ÿæˆå›¾åƒçš„ Diffusion Modelsï¼‰çš„å†…éƒ¨ç»“æ„æ¥æ”¯æŒè‰ºæœ¯å®¶çš„åˆ›ä½œæ§åˆ¶ã€‚ä½œè€…æå‡ºäº†ä¸€ç§åŸºäºæ‰‹å·¥è‰ºï¼ˆcraft-basedï¼‰çš„å¯è§£é‡Šæ€§æ–¹æ³•ï¼Œå…¶æ ¸å¿ƒç†å¿µæºäº SchÃ¶n çš„â€œè¡ŒåŠ¨ä¸­åæ€â€ï¼ˆreflection-in-actionï¼‰ï¼Œå°†å¤æ‚çš„ç”Ÿæˆæ¨¡å‹è§†ä¸ºå¯æ“ä½œçš„åˆ›ä½œææ–™ã€‚ä¸ºäº†å®ç°è¿™ä¸€ç›®æ ‡ï¼Œç ”ç©¶äººå‘˜å¼€å‘äº†ä¸€ä¸ªé›†æˆäº ComfyUI èŠ‚ç‚¹å¼ç•Œé¢çš„æ’ä»¶ï¼Œç”¨äºæ¨¡å‹å¼¯æ›²ï¼ˆmodel-bendingï¼‰å’Œå†…éƒ¨æ£€æŸ¥ã€‚é€šè¿‡äº¤äº’å¼åœ°æ“æ§ç”Ÿæˆæ¨¡å‹çš„ä¸åŒç»„ä»¶ï¼Œè‰ºæœ¯å®¶èƒ½å¤Ÿç›´è§‚åœ°ç†è§£å„éƒ¨åˆ†å¯¹è¾“å‡ºç»“æœçš„å½±å“ï¼Œä»è€Œå»ºç«‹èµ·å¯¹æ¨¡å‹è¡Œä¸ºçš„ç¼„é»˜çŸ¥è¯†ï¼ˆtacit understandingï¼‰å’Œåˆ›ä½œç›´è§‰ã€‚è¿™ç§æ–¹æ³•ä¸ä»…å¢å¼ºäº†è‰ºæœ¯å®¶å¯¹ AI å·¥å…·çš„è¡¨è¾¾æ€§æ“æ§èƒ½åŠ›ï¼Œä¹Ÿä¸ºåœ¨å¤§å‹ç”Ÿæˆæ¨¡å‹ä¸­å®ç°æ›´æ·±å±‚æ¬¡çš„äººæœºåä½œä¸è‰ºæœ¯å®è·µæä¾›äº†æ–°è·¯å¾„ã€‚",
      "categories": [
        "cs.HC",
        "cs.AI",
        "cs.LG",
        "cs.MM"
      ],
      "primary_category": "cs.HC",
      "comment": "In Proceedings of Explainable AI for the Arts Workshop 2025 (XAIxArts 2025) arXiv:2406.14485",
      "pdf_url": "https://arxiv.org/pdf/2508.07183v1",
      "published_date": "2025-08-10 05:19:30 UTC",
      "updated_date": "2025-08-10 05:19:30 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T10:47:46.287134+00:00"
    },
    {
      "arxiv_id": "2508.07180v1",
      "title": "Dynamic Benchmark Construction for Evaluating Large Language Models on Real-World Codes",
      "title_zh": "é’ˆå¯¹çœŸå®ä¸–ç•Œä»£ç è¯„ä¼°å¤§è¯­è¨€æ¨¡å‹çš„åŠ¨æ€åŸºå‡†æ„å»º",
      "authors": [
        "Zhe Zhang",
        "Runlin Liu",
        "Aishan Liu",
        "Xingyu Liu",
        "Xiang Gao",
        "Hailong Sun"
      ],
      "abstract": "As large language models LLMs) become increasingly integrated into software development workflows, rigorously evaluating their performance on complex, real-world code generation tasks has become essential. However, existing benchmarks often suffer from data contamination and limited test rigor, constraining their ability to reveal model failures effectively. To address these, we present CODE2BENCH, a end-to-end pipeline for dynamically constructing robust and contamination-resistant benchmarks from real-world GitHub repositories. Specifically, CODE2BENCH introduces three key innovations: (1) Automated Dynamism, achieved through periodic ingestion of recent code to minimize training data contamination; (2) Scope Graph-based dependency analysis, which enables structured classification of functions into benchmark instances with controlled dependency levels (distinguishing between Self-Contained (SC) tasks for cross-language evaluation and Weakly Self-Contained (WSC) tasks involving permitted library usage); and (3) Property-Based Testing (PBT) for the automated synthesis of rigorous test suites to enable thorough functional verification. Using this pipeline, we construct CODE2BENCH-2505, the first benchmark derived from 880 recent Python projects spanning diverse domains, comprising 1,163 code generation tasks with 100% average branch coverage on ground-truth implementations. Extensive evaluation of 16 LLMs using CODE2BENCH-2505 reveals that models consistently struggle with SC tasks requiring complex, non-standard logic and cross-language transfer, while showing relatively stronger performance on WSC tasks in Python. Our work introduces a contamination-resistant, language-agnostic methodology for dynamic benchmark construction, offering a principled foundation for the comprehensive and realistic evaluation of LLMs on real-world software development tasks.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†CODE2BENCHï¼Œä¸€ä¸ªæ—¨åœ¨ä»çœŸå®GitHubä»“åº“åŠ¨æ€æ„å»ºç¨³å¥ä¸”æŠ—æ±¡æŸ“åŸºå‡†çš„ç«¯åˆ°ç«¯æµæ°´çº¿ï¼Œç”¨ä»¥è§£å†³å¤§è¯­è¨€æ¨¡å‹(LLMs)åœ¨ä»£ç ç”Ÿæˆè¯„ä¼°ä¸­é¢ä¸´çš„æ•°æ®æ±¡æŸ“å’Œæµ‹è¯•ä¸¥è°¨æ€§ä¸è¶³ç­‰é—®é¢˜ã€‚è¯¥æ¡†æ¶å¼•å…¥äº†è‡ªåŠ¨åŒ–åŠ¨æ€æ€§(Automated Dynamism)ã€åŸºäºScope Graphçš„ä¾èµ–åˆ†æä»¥åŠç”¨äºè‡ªåŠ¨åˆæˆä¸¥è°¨æµ‹è¯•å¥—ä»¶çš„åŸºäºå±æ€§çš„æµ‹è¯•(Property-Based Testing, PBT)ä¸‰é¡¹æ ¸å¿ƒåˆ›æ–°ã€‚åˆ©ç”¨è¯¥æµæ°´çº¿ï¼Œç ”ç©¶å›¢é˜Ÿæ„å»ºäº†åŒ…å«1,163ä¸ªä»£ç ç”Ÿæˆä»»åŠ¡çš„CODE2BENCH-2505åŸºå‡†ï¼Œå…¶åœ¨ ground-truth ä»£ç ä¸Šå®ç°äº†100%çš„å¹³å‡åˆ†æ”¯è¦†ç›–ã€‚é€šè¿‡å¯¹16ä¸ªä¸»æµLLMsçš„è¯„ä¼°å‘ç°ï¼Œæ¨¡å‹åœ¨å¤„ç†æ¶‰åŠå¤æ‚é€»è¾‘å’Œè·¨è¯­è¨€è¿ç§»çš„Self-Contained (SC)ä»»åŠ¡æ—¶ä¾ç„¶é¢ä¸´å·¨å¤§æŒ‘æˆ˜ï¼Œè€Œåœ¨Weakly Self-Contained (WSC)ä»»åŠ¡ä¸­è¡¨ç°ç›¸å¯¹è¾ƒå¥½ã€‚è¯¥å·¥ä½œä¸ºåŠ¨æ€ã€è·¨è¯­è¨€çš„åŸºå‡†æ„å»ºæä¾›äº†æŠ—æ±¡æŸ“çš„æ–¹æ³•è®ºï¼Œä¸ºåœ¨çœŸå®è½¯ä»¶å¼€å‘åœºæ™¯ä¸‹å…¨é¢è¡¡é‡æ¨¡å‹æ€§èƒ½å¥ å®šäº†é‡è¦åŸºç¡€ã€‚",
      "categories": [
        "cs.SE",
        "cs.AI"
      ],
      "primary_category": "cs.SE",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.07180v1",
      "published_date": "2025-08-10 05:06:36 UTC",
      "updated_date": "2025-08-10 05:06:36 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T10:47:56.351237+00:00"
    },
    {
      "arxiv_id": "2508.07179v1",
      "title": "Schema Lineage Extraction at Scale: Multilingual Pipelines, Composite Evaluation, and Language-Model Benchmarks",
      "title_zh": "å¤§è§„æ¨¡æ¨¡å¼è¡€ç¼˜æå–ï¼šå¤šè¯­è¨€æµæ°´çº¿ã€ç»¼åˆè¯„ä¼°ä¸è¯­è¨€æ¨¡å‹åŸºå‡†",
      "authors": [
        "Jiaqi Yin",
        "Yi-Wei Chen",
        "Meng-Lung Lee",
        "Xiya Liu"
      ],
      "abstract": "Enterprise data pipelines, characterized by complex transformations across multiple programming languages, often cause a semantic disconnect between original metadata and downstream data. This \"semantic drift\" compromises data reproducibility and governance, and impairs the utility of services like retrieval-augmented generation (RAG) and text-to-SQL systems. To address this, a novel framework is proposed for the automated extraction of fine-grained schema lineage from multilingual enterprise pipeline scripts. This method identifies four key components: source schemas, source tables, transformation logic, and aggregation operations, creating a standardized representation of data transformations. For the rigorous evaluation of lineage quality, this paper introduces the Schema Lineage Composite Evaluation (SLiCE), a metric that assesses both structural correctness and semantic fidelity. A new benchmark is also presented, comprising 1,700 manually annotated lineages from real-world industrial scripts. Experiments were conducted with 12 language models, from 1.3B to 32B small language models (SLMs) to large language models (LLMs) like GPT-4o and GPT-4.1. The results demonstrate that the performance of schema lineage extraction scales with model size and the sophistication of prompting techniques. Specially, a 32B open-source model, using a single reasoning trace, can achieve performance comparable to the GPT series under standard prompting. This finding suggests a scalable and economical approach for deploying schema-aware agents in practical applications.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ä¼ä¸šæ•°æ®æµæ°´çº¿ä¸­å› å¤šè¯­è¨€è½¬æ¢å¯¼è‡´çš„â€œè¯­ä¹‰æ¼‚ç§»â€(semantic drift)é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§è‡ªåŠ¨æå–ç»†ç²’åº¦æ¶æ„è¡€ç¼˜(schema lineage)çš„æ–°æ¡†æ¶ã€‚è¯¥æ¡†æ¶èƒ½å¤Ÿè¯†åˆ«æºæ¶æ„(source schemas)ã€æºè¡¨(source tables)ã€è½¬æ¢é€»è¾‘(transformation logic)å’Œèšåˆæ“ä½œ(aggregation operations)ï¼Œä¸ºæ•°æ®è½¬æ¢å»ºç«‹æ ‡å‡†åŒ–è¡¨ç¤ºã€‚ä¸ºäº†ä¸¥è°¨è¯„ä¼°æå–è´¨é‡ï¼Œè®ºæ–‡å¼•å…¥äº† Schema Lineage Composite Evaluation (SLiCE) æŒ‡æ ‡ï¼Œå¹¶å‘å¸ƒäº†åŒ…å«1,700ä¸ªçœŸå®å·¥ä¸šè„šæœ¬æ‰‹åŠ¨æ ‡æ³¨çš„åŸºå‡†æ•°æ®é›†ã€‚é€šè¿‡å¯¹12ä¸ªä»1.3Båˆ°32Bä¸åŒè§„æ¨¡è¯­è¨€æ¨¡å‹ï¼ˆLLMs/SLMsï¼‰çš„å®éªŒè¡¨æ˜ï¼Œæ¶æ„è¡€ç¼˜æå–èƒ½åŠ›éšæ¨¡å‹å‚æ•°å’Œæç¤ºç­–ç•¥çš„ä¼˜åŒ–è€Œæ˜¾è‘—æå‡ã€‚ç ”ç©¶ç‰¹åˆ«å‘ç°ï¼Œ32Bè§„æ¨¡çš„å¼€æºæ¨¡å‹åœ¨å•ä¸€æ¨ç†è·¯å¾„ä¸‹å¯è¾¾åˆ°ä¸ GPT-4o ç³»åˆ—ç›¸å½“çš„æ€§èƒ½ï¼Œä¸ºåœ¨å®é™…ç”Ÿäº§ä¸­éƒ¨ç½²ç»æµä¸”å¯æ‰©å±•çš„æ¶æ„æ„ŸçŸ¥ä»£ç†(schema-aware agents)æä¾›äº†å¯è¡Œè·¯å¾„ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.DB"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.07179v1",
      "published_date": "2025-08-10 05:04:32 UTC",
      "updated_date": "2025-08-10 05:04:32 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T10:47:48.743574+00:00"
    },
    {
      "arxiv_id": "2508.07178v2",
      "title": "Improved Personalized Headline Generation via Denoising Fake Interests from Implicit Feedback",
      "title_zh": "é€šè¿‡å‰”é™¤éšå¼åé¦ˆä¸­çš„è™šå‡å…´è¶£æå‡ä¸ªæ€§åŒ–æ ‡é¢˜ç”Ÿæˆè´¨é‡",
      "authors": [
        "Kejin Liu",
        "Junhong Lian",
        "Xiang Ao",
        "Ningtao Wang",
        "Xing Fu",
        "Yu Cheng",
        "Weiqiang Wang",
        "Xinyu Liu"
      ],
      "abstract": "Accurate personalized headline generation hinges on precisely capturing user interests from historical behaviors. However, existing methods neglect personalized-irrelevant click noise in entire historical clickstreams, which may lead to hallucinated headlines that deviate from genuine user preferences. In this paper, we reveal the detrimental impact of click noise on personalized generation quality through rigorous analysis in both user and news dimensions. Based on these insights, we propose a novel Personalized Headline Generation framework via Denoising Fake Interests from Implicit Feedback (PHG-DIF). PHG-DIF first employs dual-stage filtering to effectively remove clickstream noise, identified by short dwell times and abnormal click bursts, and then leverages multi-level temporal fusion to dynamically model users' evolving and multi-faceted interests for precise profiling. Moreover, we release DT-PENS, a new benchmark dataset comprising the click behavior of 1,000 carefully curated users and nearly 10,000 annotated personalized headlines with historical dwell time annotations. Extensive experiments demonstrate that PHG-DIF substantially mitigates the adverse effects of click noise and significantly improves headline quality, achieving state-of-the-art (SOTA) results on DT-PENS. Our framework implementation and dataset are available at https://github.com/liukejin-up/PHG-DIF.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†ä¸ªæ€§åŒ–æ ‡é¢˜ç”Ÿæˆ(Personalized Headline Generation)ä¸­å­˜åœ¨çš„ç‚¹å‡»å™ªå£°(click noise)é—®é¢˜ï¼Œå³å†å²ç‚¹å‡»è¡Œä¸ºä¸­åŒ…å«å¤§é‡ä¸ç”¨æˆ·çœŸå®å…´è¶£æ— å…³çš„æ•°æ®ï¼Œè¿™ä¼šå¯¼è‡´æ¨¡å‹ç”Ÿæˆåç¦»åå¥½çš„è™šå‡æ ‡é¢˜ã€‚ä¸ºæ­¤ï¼Œä½œè€…æå‡ºäº†PHG-DIFæ¡†æ¶ï¼Œæ—¨åœ¨é€šè¿‡éšå¼åé¦ˆ(Implicit Feedback)è¯†åˆ«å¹¶å»é™¤è™šå‡å…´è¶£ã€‚è¯¥æ¡†æ¶é¦–å…ˆåˆ©ç”¨åŒé˜¶æ®µè¿‡æ»¤(dual-stage filtering)æœºåˆ¶ï¼Œæ ¹æ®åœç•™æ—¶é—´(dwell time)å’Œå¼‚å¸¸ç‚¹å‡»é¢‘ç‡å‰”é™¤å™ªå£°ï¼Œéšåé€šè¿‡å¤šçº§æ—¶é—´èåˆ(multi-level temporal fusion)åŠ¨æ€å»ºæ¨¡ç”¨æˆ·æ¼”å˜çš„å¤šå…ƒå…´è¶£ã€‚æ­¤å¤–ï¼Œç ”ç©¶è¿˜å‘å¸ƒäº†åŒ…å«åœç•™æ—¶é—´æ ‡æ³¨çš„DT-PENSåŸºå‡†æ•°æ®é›†ï¼Œå¡«è¡¥äº†è¯¥é¢†åŸŸçš„å®éªŒç©ºç™½ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒPHG-DIFæ˜¾è‘—å‡è½»äº†ç‚¹å‡»å™ªå£°çš„è´Ÿé¢å½±å“ï¼Œå¹¶åœ¨ç”Ÿæˆè´¨é‡ä¸Šè¾¾åˆ°äº†ç›®å‰çš„SOTAæ°´å¹³ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "Accepted by the 34th ACM International Conference on Information and Knowledge Management (CIKM '25), Full Research Papers track",
      "pdf_url": "https://arxiv.org/pdf/2508.07178v2",
      "published_date": "2025-08-10 04:56:13 UTC",
      "updated_date": "2025-08-14 06:43:57 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T10:47:51.862916+00:00"
    },
    {
      "arxiv_id": "2508.07170v1",
      "title": "Lightweight Multi-Scale Feature Extraction with Fully Connected LMF Layer for Salient Object Detection",
      "title_zh": "åŸºäºå…¨è¿æ¥LMFå±‚çš„è½»é‡çº§å¤šå°ºåº¦ç‰¹å¾æå–ï¼Œç”¨äºæ˜¾è‘—æ€§ç›®æ ‡æ£€æµ‹",
      "authors": [
        "Yunpeng Shi",
        "Lei Chen",
        "Xiaolu Shen",
        "Yanju Guo"
      ],
      "abstract": "In the domain of computer vision, multi-scale feature extraction is vital for tasks such as salient object detection. However, achieving this capability in lightweight networks remains challenging due to the trade-off between efficiency and performance. This paper proposes a novel lightweight multi-scale feature extraction layer, termed the LMF layer, which employs depthwise separable dilated convolutions in a fully connected structure. By integrating multiple LMF layers, we develop LMFNet, a lightweight network tailored for salient object detection. Our approach significantly reduces the number of parameters while maintaining competitive performance. Here, we show that LMFNet achieves state-of-the-art or comparable results on five benchmark datasets with only 0.81M parameters, outperforming several traditional and lightweight models in terms of both efficiency and accuracy. Our work not only addresses the challenge of multi-scale learning in lightweight networks but also demonstrates the potential for broader applications in image processing tasks. The related code files are available at https://github.com/Shi-Yun-peng/LMFNet",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹æ˜¾è‘—ç›®æ ‡æ£€æµ‹ï¼ˆSalient Object Detectionï¼‰åœ¨è½»é‡åŒ–ç½‘ç»œä¸­éš¾ä»¥å¹³è¡¡æ•ˆç‡ä¸å¤šå°ºåº¦ç‰¹å¾æå–æ€§èƒ½çš„é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§æ–°å‹è½»é‡åŒ–å¤šå°ºåº¦ç‰¹å¾æå–å±‚LMF layerã€‚è¯¥å±‚åœ¨å…¨è¿æ¥ï¼ˆfully connectedï¼‰ç»“æ„ä¸­å·§å¦™åœ°è¿ç”¨äº†æ·±åº¦å¯åˆ†ç¦»ç©ºæ´å·ç§¯ï¼ˆdepthwise separable dilated convolutionsï¼‰ï¼Œæ—¨åœ¨å¤§å¹…å‡å°‘å‚æ•°é‡çš„åŒæ—¶æ•è·ä¸°å¯Œçš„å¤šå°ºåº¦ä¿¡æ¯ã€‚åŸºäºæ­¤å±‚æ„å»ºçš„LMFNetåœ¨ä»…æœ‰0.81Må‚æ•°çš„æƒ…å†µä¸‹ï¼Œåœ¨äº”ä¸ªåŸºå‡†æ•°æ®é›†ä¸Šå‡å–å¾—äº†State-of-the-artæˆ–ä¸ä¹‹ç›¸å½“çš„æ€§èƒ½è¡¨ç°ã€‚å®éªŒè¯æ˜ï¼Œè¯¥æ–¹æ³•åœ¨æ•ˆç‡å’Œå‡†ç¡®æ€§ä¸Šå‡ä¼˜äºå¤šç§ä¼ ç»Ÿçš„åŠç°æœ‰çš„è½»é‡åŒ–æ¨¡å‹ã€‚æ­¤é¡¹å·¥ä½œä¸ä»…æœ‰æ•ˆè§£å†³äº†è½»é‡åŒ–ç½‘ç»œçš„å¤šå°ºåº¦å­¦ä¹ éš¾é¢˜ï¼Œä¹Ÿä¸ºå…¶ä»–å›¾åƒå¤„ç†ä»»åŠ¡æä¾›äº†æå…·æ½œåŠ›çš„åº”ç”¨å‚è€ƒã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.07170v1",
      "published_date": "2025-08-10 04:06:48 UTC",
      "updated_date": "2025-08-10 04:06:48 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T10:47:53.948277+00:00"
    },
    {
      "arxiv_id": "2508.07165v2",
      "title": "Large-scale Multi-sequence Pretraining for Generalizable MRI Analysis in Versatile Clinical Applications",
      "title_zh": "é¢å‘å¤šæ ·åŒ–ä¸´åºŠåº”ç”¨çš„å¯æ³›åŒ–MRIåˆ†æå¤§è§„æ¨¡å¤šåºåˆ—é¢„è®­ç»ƒ",
      "authors": [
        "Zelin Qiu",
        "Xi Wang",
        "Zhuoyao Xie",
        "Juan Zhou",
        "Yu Wang",
        "Lingjie Yang",
        "Xinrui Jiang",
        "Juyoung Bae",
        "Moo Hyun Son",
        "Qiang Ye",
        "Dexuan Chen",
        "Rui Zhang",
        "Tao Li",
        "Neeraj Ramesh Mahboobani",
        "Varut Vardhanabhuti",
        "Xiaohui Duan",
        "Yinghua Zhao",
        "Hao Chen"
      ],
      "abstract": "Multi-sequence Magnetic Resonance Imaging (MRI) offers remarkable versatility, enabling the distinct visualization of different tissue types. Nevertheless, the inherent heterogeneity among MRI sequences poses significant challenges to the generalization capability of deep learning models. These challenges undermine model performance when faced with varying acquisition parameters, thereby severely restricting their clinical utility. In this study, we present PRISM, a foundation model PRe-trained with large-scale multI-Sequence MRI. We collected a total of 64 datasets from both public and private sources, encompassing a wide range of whole-body anatomical structures, with scans spanning diverse MRI sequences. Among them, 336,476 volumetric MRI scans from 34 datasets (8 public and 26 private) were curated to construct the largest multi-organ multi-sequence MRI pretraining corpus to date. We propose a novel pretraining paradigm that disentangles anatomically invariant features from sequence-specific variations in MRI, while preserving high-level semantic representations. We established a benchmark comprising 44 downstream tasks, including disease diagnosis, image segmentation, registration, progression prediction, and report generation. These tasks were evaluated on 32 public datasets and 5 private cohorts. PRISM consistently outperformed both non-pretrained models and existing foundation models, achieving first-rank results in 39 out of 44 downstream benchmarks with statistical significance improvements. These results underscore its ability to learn robust and generalizable representations across unseen data acquired under diverse MRI protocols. PRISM provides a scalable framework for multi-sequence MRI analysis, thereby enhancing the translational potential of AI in radiology. It delivers consistent performance across diverse imaging protocols, reinforcing its clinical applicability.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†PRISMï¼Œè¿™æ˜¯ä¸€ä¸ªåŸºäºå¤§è§„æ¨¡å¤šåºåˆ—MRIé¢„è®­ç»ƒçš„åŸºç¡€æ¨¡å‹(Foundation Model)ï¼Œæ—¨åœ¨è§£å†³æ·±åº¦å­¦ä¹ æ¨¡å‹åœ¨åº”å¯¹å¼‚æ„MRIåºåˆ—æ—¶æ³›åŒ–èƒ½åŠ›ä¸è¶³çš„æŒ‘æˆ˜ã€‚ç ”ç©¶å›¢é˜Ÿæ•´åˆäº†64ä¸ªæ•°æ®é›†ï¼Œæ„å»ºäº†åŒ…å«336,476ä¸ªä½“ç§¯MRIæ‰«æçš„ç›®å‰æœ€å¤§è§„æ¨¡å¤šå™¨å®˜å¤šåºåˆ—é¢„è®­ç»ƒè¯­æ–™åº“ã€‚PRISMé‡‡ç”¨äº†ä¸€ç§åˆ›æ–°çš„é¢„è®­ç»ƒèŒƒå¼ï¼Œé€šè¿‡è§£è€¦è§£å‰–å­¦ä¸å˜ç‰¹å¾(Anatomically Invariant Features)ä¸åºåˆ—ç‰¹æœ‰çš„å˜åŒ–(Sequence-specific Variations)ï¼Œåœ¨ä¿ç•™é«˜å±‚è¯­ä¹‰è¡¨å¾çš„åŒæ—¶æ˜¾è‘—æå‡äº†æ¨¡å‹çš„æ³›åŒ–æ€§ã€‚é€šè¿‡å¯¹ç–¾ç—…è¯Šæ–­ã€å›¾åƒåˆ†å‰²(Image Segmentation)ã€é…å‡†(Registration)ç­‰44é¡¹ä¸‹æ¸¸ä»»åŠ¡çš„å¹¿æ³›è¯„ä¼°ï¼ŒPRISMåœ¨å…¶ä¸­39é¡¹ä»»åŠ¡ä¸­å–å¾—äº†æ’åç¬¬ä¸€çš„æˆç»©ï¼Œè¡¨ç°æ˜¾è‘—ä¼˜äºç°æœ‰çš„å„ç±»åŸºç¡€æ¨¡å‹ã€‚è¯¥ç ”ç©¶è¯æ˜äº†PRISMåœ¨å¤„ç†è·¨åè®®æœªçŸ¥æ•°æ®æ—¶çš„ç¨³å¥æ€§ï¼Œä¸ºå¤šåºåˆ—MRIåˆ†ææä¾›äº†ä¸€ä¸ªå…·æœ‰é«˜åº¦ä¸´åºŠåº”ç”¨æ½œåŠ›çš„å¯æ‰©å±•æ¡†æ¶ã€‚",
      "categories": [
        "eess.IV",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "eess.IV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.07165v2",
      "published_date": "2025-08-10 03:31:46 UTC",
      "updated_date": "2025-08-25 16:25:14 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T10:48:09.554106+00:00"
    },
    {
      "arxiv_id": "2508.07163v1",
      "title": "Integrating Neurosymbolic AI in Advanced Air Mobility: A Comprehensive Survey",
      "title_zh": "ç¥ç»ç¬¦å· AI åœ¨å…ˆè¿›ç©ºä¸­äº¤é€šä¸­çš„é›†æˆï¼šå…¨é¢ç»¼è¿°",
      "authors": [
        "Kamal Acharya",
        "Iman Sharifi",
        "Mehul Lad",
        "Liang Sun",
        "Houbing Song"
      ],
      "abstract": "Neurosymbolic AI combines neural network adaptability with symbolic reasoning, promising an approach to address the complex regulatory, operational, and safety challenges in Advanced Air Mobility (AAM). This survey reviews its applications across key AAM domains such as demand forecasting, aircraft design, and real-time air traffic management. Our analysis reveals a fragmented research landscape where methodologies, including Neurosymbolic Reinforcement Learning, have shown potential for dynamic optimization but still face hurdles in scalability, robustness, and compliance with aviation standards. We classify current advancements, present relevant case studies, and outline future research directions aimed at integrating these approaches into reliable, transparent AAM systems. By linking advanced AI techniques with AAM's operational demands, this work provides a concise roadmap for researchers and practitioners developing next-generation air mobility solutions.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†å°†ç¥ç»ç¬¦å·äººå·¥æ™ºèƒ½(Neurosymbolic AI)æ•´åˆäºé«˜çº§ç©ºä¸­äº¤é€š(Advanced Air Mobility, AAM)çš„ç°çŠ¶ï¼Œæ—¨åœ¨åˆ©ç”¨ç¥ç»ç½‘ç»œçš„è‡ªé€‚åº”æ€§ä¸ç¬¦å·æ¨ç†çš„ç»“åˆæ¥åº”å¯¹å¤æ‚çš„ç›‘ç®¡ã€è¿è¥åŠå®‰å…¨æŒ‘æˆ˜ã€‚ç»¼è¿°è¯¦ç»†å›é¡¾äº†å…¶åœ¨éœ€æ±‚é¢„æµ‹ã€é£è¡Œå™¨è®¾è®¡åŠå®æ—¶ç©ºä¸­äº¤é€šç®¡ç†ç­‰æ ¸å¿ƒé¢†åŸŸçš„åº”ç”¨ï¼Œæ­ç¤ºäº†å½“å‰ç ”ç©¶æ ¼å±€ç¢ç‰‡åŒ–çš„ç°çŠ¶ã€‚å°½ç®¡ç¥ç»ç¬¦å·å¼ºåŒ–å­¦ä¹ (Neurosymbolic Reinforcement Learning)åœ¨åŠ¨æ€ä¼˜åŒ–ä¸­å±•ç°å‡ºæ½œåŠ›ï¼Œä½†åœ¨å¯æ‰©å±•æ€§ã€é²æ£’æ€§ä»¥åŠèˆªç©ºæ ‡å‡†åˆè§„æ€§æ–¹é¢ä»é¢ä¸´æ˜¾è‘—éšœç¢ã€‚é€šè¿‡å¯¹æŠ€æœ¯è¿›å±•çš„åˆ†ç±»ã€æ¡ˆä¾‹åˆ†æåŠæœªæ¥ç ”ç©¶æ–¹å‘çš„å‹¾å‹’ï¼Œæœ¬æ–‡ä¸ºæ„å»ºå¯é ã€é€æ˜çš„AAMç³»ç»Ÿæä¾›äº†æ˜ç¡®çš„æŠ€æœ¯è·¯çº¿å›¾ã€‚è¯¥å·¥ä½œæˆåŠŸå°†å…ˆè¿›äººå·¥æ™ºèƒ½æŠ€æœ¯ä¸AAMçš„å®é™…è¿è¥éœ€æ±‚æŒ‚é’©ï¼Œä¸ºä¸‹ä¸€ä»£ç©ºä¸­äº¤é€šè§£å†³æ–¹æ¡ˆçš„å¼€å‘å¥ å®šäº†ç†è®ºåŸºç¡€ã€‚",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.NE"
      ],
      "primary_category": "cs.RO",
      "comment": "9 pages, 4 figures, IJCAI-2025 (accepted)",
      "pdf_url": "https://arxiv.org/pdf/2508.07163v1",
      "published_date": "2025-08-10 03:30:06 UTC",
      "updated_date": "2025-08-10 03:30:06 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T10:48:02.090348+00:00"
    },
    {
      "arxiv_id": "2508.07146v1",
      "title": "Intention-Aware Diffusion Model for Pedestrian Trajectory Prediction",
      "title_zh": "é¢å‘è¡Œäººè½¨è¿¹é¢„æµ‹çš„æ„å›¾æ„ŸçŸ¥æ‰©æ•£æ¨¡å‹",
      "authors": [
        "Yu Liu",
        "Zhijie Liu",
        "Xiao Ren",
        "You-Fu Li",
        "He Kong"
      ],
      "abstract": "Predicting pedestrian motion trajectories is critical for the path planning and motion control of autonomous vehicles. Recent diffusion-based models have shown promising results in capturing the inherent stochasticity of pedestrian behavior for trajectory prediction. However, the absence of explicit semantic modelling of pedestrian intent in many diffusion-based methods may result in misinterpreted behaviors and reduced prediction accuracy. To address the above challenges, we propose a diffusion-based pedestrian trajectory prediction framework that incorporates both short-term and long-term motion intentions. Short-term intent is modelled using a residual polar representation, which decouples direction and magnitude to capture fine-grained local motion patterns. Long-term intent is estimated through a learnable, token-based endpoint predictor that generates multiple candidate goals with associated probabilities, enabling multimodal and context-aware intention modelling. Furthermore, we enhance the diffusion process by incorporating adaptive guidance and a residual noise predictor that dynamically refines denoising accuracy. The proposed framework is evaluated on the widely used ETH, UCY, and SDD benchmarks, demonstrating competitive results against state-of-the-art methods.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§æ„ŸçŸ¥æ„å›¾çš„æ‰©æ•£æ¨¡å‹(Intention-Aware Diffusion Model)ï¼Œæ—¨åœ¨è§£å†³è¡Œäººè½¨è¿¹é¢„æµ‹ä¸­å› ç¼ºä¹æ˜¾å¼è¯­ä¹‰æ„å›¾å»ºæ¨¡è€Œå¯¼è‡´çš„é¢„æµ‹åå·®é—®é¢˜ã€‚è¯¥æ¡†æ¶åŒæ—¶æ•´åˆäº†çŸ­æœŸå’Œé•¿æœŸè¿åŠ¨æ„å›¾ï¼Œå…¶ä¸­çŸ­æœŸæ„å›¾(short-term intent)é€šè¿‡æ®‹å·®æåæ ‡è¡¨ç¤º(residual polar representation)æ•è·ç»†ç²’åº¦çš„å±€éƒ¨è¿åŠ¨æ¨¡å¼ï¼Œè€Œé•¿æœŸæ„å›¾(long-term intent)åˆ™åˆ©ç”¨åŸºäºtokençš„ç»ˆç‚¹é¢„æµ‹å™¨(endpoint predictor)ç”Ÿæˆå¤šæ¨¡æ€ä¸”æ„ŸçŸ¥ä¸Šä¸‹æ–‡çš„å€™é€‰ç›®æ ‡ã€‚ä¸ºäº†è¿›ä¸€æ­¥æå‡é¢„æµ‹ç²¾åº¦ï¼Œè¯¥æ–¹æ³•åœ¨æ‰©æ•£è¿‡ç¨‹(diffusion process)ä¸­å¼•å…¥äº†è‡ªé€‚åº”å¼•å¯¼(adaptive guidance)å’Œæ®‹å·®å™ªå£°é¢„æµ‹å™¨(residual noise predictor)ä»¥åŠ¨æ€ä¼˜åŒ–å»å™ªæ•ˆæœã€‚å®éªŒåœ¨ETHã€UCYå’ŒSDDç­‰ä¸»æµåŸºå‡†æ•°æ®é›†ä¸Šè¿›è¡Œï¼Œç»“æœè¯æ˜è¯¥æ¨¡å‹åœ¨æ•æ‰è¡Œäººè¡Œä¸ºéšæœºæ€§æ–¹é¢å…·æœ‰æ˜¾è‘—ä¼˜åŠ¿ï¼Œä¸”å„é¡¹æ€§èƒ½æŒ‡æ ‡å‡ä¼˜äºå½“å‰çš„å…ˆè¿›æ°´å¹³(state-of-the-art)ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.07146v1",
      "published_date": "2025-08-10 02:36:33 UTC",
      "updated_date": "2025-08-10 02:36:33 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T10:48:04.651635+00:00"
    },
    {
      "arxiv_id": "2508.10036v1",
      "title": "Reflect then Learn: Active Prompting for Information Extraction Guided by Introspective Confusion",
      "title_zh": "å…ˆåæ€åå­¦ä¹ ï¼šç”±å†…çœå›°æƒ‘å¼•å¯¼çš„ä¿¡æ¯æŠ½å–ä¸»åŠ¨æç¤º",
      "authors": [
        "Dong Zhao",
        "Yadong Wang",
        "Xiang Chen",
        "Chenxi Wang",
        "Hongliang Dai",
        "Chuanxing Geng",
        "Shengzhong Zhang",
        "Shaoyuan Li",
        "Sheng-Jun Huang"
      ],
      "abstract": "Large Language Models (LLMs) show remarkable potential for few-shot information extraction (IE), yet their performance is highly sensitive to the choice of in-context examples. Conventional selection strategies often fail to provide informative guidance, as they overlook a key source of model fallibility: confusion stemming not just from semantic content, but also from the generation of well-structured formats required by IE tasks. To address this, we introduce Active Prompting for Information Extraction (APIE), a novel active prompting framework guided by a principle we term introspective confusion. Our method empowers an LLM to assess its own confusion through a dual-component uncertainty metric that uniquely quantifies both Format Uncertainty (difficulty in generating correct syntax) and Content Uncertainty (inconsistency in extracted semantics). By ranking unlabeled data with this comprehensive score, our framework actively selects the most challenging and informative samples to serve as few-shot exemplars. Extensive experiments on four benchmarks show that our approach consistently outperforms strong baselines, yielding significant improvements in both extraction accuracy and robustness. Our work highlights the critical importance of a fine-grained, dual-level view of model uncertainty when it comes to building effective and reliable structured generation systems.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨å°‘æ ·æœ¬ä¿¡æ¯æŠ½å–ï¼ˆIEï¼‰ä»»åŠ¡ä¸­å¯¹ç¤ºä¾‹é€‰æ‹©é«˜åº¦æ•æ„Ÿçš„é—®é¢˜ï¼Œæå‡ºäº†APIEï¼ˆActive Prompting for Information Extractionï¼‰æ¡†æ¶ã€‚è¯¥æ¡†æ¶åŸºäºâ€œå†…çœæ··æ·†â€ï¼ˆintrospective confusionï¼‰åŸåˆ™ï¼Œå¼•å…¥äº†åŒé‡ä¸ç¡®å®šæ€§æŒ‡æ ‡æ¥è¯„ä¼°æ¨¡å‹çŠ¶æ€ï¼Œå³é‡åŒ–è¯­æ³•ç”Ÿæˆéš¾åº¦çš„ Format Uncertainty å’Œè¡¡é‡è¯­ä¹‰æå–ä¸€è‡´æ€§çš„ Content Uncertaintyã€‚é€šè¿‡è¯¥ç»¼åˆè¯„åˆ†å¯¹æ— æ ‡ç­¾æ•°æ®è¿›è¡Œæ’åºï¼Œæ¡†æ¶èƒ½å¤Ÿä¸»åŠ¨æŒ‘é€‰å‡ºæœ€å…·æŒ‘æˆ˜æ€§å’Œä¿¡æ¯é‡çš„æ ·æœ¬ä½œä¸ºå°‘æ ·æœ¬ç¤ºä¾‹ã€‚åœ¨å››ä¸ªåŸºå‡†æ•°æ®é›†ä¸Šçš„å®éªŒç»“æœæ˜¾ç¤ºï¼ŒAPIE æ˜¾è‘—ä¼˜äºç°æœ‰çš„å¼ºåŸºçº¿æ¨¡å‹ï¼Œåœ¨æé«˜æå–å‡†ç¡®æ€§çš„åŒæ—¶å¢å¼ºäº†ç³»ç»Ÿçš„é²æ£’æ€§ã€‚è¯¥å·¥ä½œå¼ºè°ƒäº†åœ¨æ„å»ºå¯é çš„ç»“æ„åŒ–ç”Ÿæˆç³»ç»Ÿæ—¶ï¼Œé‡‡ç”¨ç»†ç²’åº¦ã€åŒå±‚çº§æ¨¡å‹ä¸ç¡®å®šæ€§åˆ†æçš„å…³é”®æ„ä¹‰ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.IR",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "Under Review",
      "pdf_url": "https://arxiv.org/pdf/2508.10036v1",
      "published_date": "2025-08-10 02:27:41 UTC",
      "updated_date": "2025-08-10 02:27:41 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T10:48:12.050030+00:00"
    },
    {
      "arxiv_id": "2508.07143v2",
      "title": "Fairness of Automatic Speech Recognition: Looking Through a Philosophical Lens",
      "title_zh": "è‡ªåŠ¨è¯­éŸ³è¯†åˆ«çš„å…¬å¹³æ€§ï¼šä¸€ç§å“²å­¦è§†è§’çš„å®¡è§†",
      "authors": [
        "Anna Seo Gyeong Choi",
        "Hoon Choi"
      ],
      "abstract": "Automatic Speech Recognition (ASR) systems now mediate countless human-technology interactions, yet research on their fairness implications remains surprisingly limited. This paper examines ASR bias through a philosophical lens, arguing that systematic misrecognition of certain speech varieties constitutes more than a technical limitation -- it represents a form of disrespect that compounds historical injustices against marginalized linguistic communities. We distinguish between morally neutral classification (discriminate1) and harmful discrimination (discriminate2), demonstrating how ASR systems can inadvertently transform the former into the latter when they consistently misrecognize non-standard dialects. We identify three unique ethical dimensions of speech technologies that differentiate ASR bias from other algorithmic fairness concerns: the temporal burden placed on speakers of non-standard varieties (\"temporal taxation\"), the disruption of conversational flow when systems misrecognize speech, and the fundamental connection between speech patterns and personal/cultural identity. These factors create asymmetric power relationships that existing technical fairness metrics fail to capture. The paper analyzes the tension between linguistic standardization and pluralism in ASR development, arguing that current approaches often embed and reinforce problematic language ideologies. We conclude that addressing ASR bias requires more than technical interventions; it demands recognition of diverse speech varieties as legitimate forms of expression worthy of technological accommodation. This philosophical reframing offers new pathways for developing ASR systems that respect linguistic diversity and speaker autonomy.",
      "tldr_zh": "è¯¥ç ”ç©¶ä»å“²å­¦è§†è§’æ¢è®¨äº†è‡ªåŠ¨è¯­éŸ³è¯†åˆ«(Automatic Speech Recognition, ASR)ç³»ç»Ÿçš„å…¬å¹³æ€§é—®é¢˜ï¼Œè®¤ä¸ºå¯¹ç‰¹å®šè¯­éŸ³å˜ä½“çš„ç³»ç»Ÿæ€§é”™è¯¯è¯†åˆ«ä¸ä»…æ˜¯æŠ€æœ¯å±€é™ï¼Œæ›´æ˜¯ä¸€ç§å¯¹è¾¹ç¼˜åŒ–è¯­è¨€ç¾¤ä½“çš„ä¸å°Šé‡ä»¥åŠå†å²ä¸å…¬çš„å»¶ç»­ã€‚æ–‡ç« æ˜ç¡®åŒºåˆ†äº†ä¸­æ€§åˆ†ç±»(discriminate1)ä¸æœ‰å®³æ­§è§†(discriminate2)ï¼Œæ­ç¤ºäº†ASRç³»ç»Ÿåœ¨æŒç»­è¯¯è¯»éæ ‡å‡†æ–¹è¨€æ—¶å¦‚ä½•å°†å‰è€…è½¬åŒ–ä¸ºåè€…ã€‚ç ”ç©¶è¯†åˆ«äº†è¯­éŸ³æŠ€æœ¯ç‰¹æœ‰çš„ä¸‰ä¸ªä¼¦ç†ç»´åº¦ï¼Œå³éæ ‡å‡†å˜ä½“ä½¿ç”¨è€…çš„æ—¶é—´ç¨(temporal taxation)ã€å¯¹è¯æµä¸­æ–­ä»¥åŠè¯­éŸ³æ¨¡å¼ä¸ä¸ªäººåŠæ–‡åŒ–èº«ä»½çš„å†…åœ¨è”ç³»ã€‚ä½œè€…æŒ‡å‡ºï¼Œç°æœ‰çš„æŠ€æœ¯å…¬å¹³æ€§æŒ‡æ ‡æ— æ³•æ•æ‰è¿™äº›å› ç´ äº§ç”Ÿçš„ä¸å¯¹ç§°æƒåŠ›å…³ç³»ï¼Œä¸”å½“å‰çš„å¼€å‘æ–¹æ³•å¾€å¾€å›ºåŒ–äº†æœ‰é—®é¢˜çš„è¯­è¨€æ„è¯†å½¢æ€ã€‚ç ”ç©¶æœ€åå¼ºè°ƒï¼Œè§£å†³ASRåè§éœ€è¦è¶…è¶Šå•çº¯çš„æŠ€æœ¯å¹²é¢„ï¼Œæ‰¿è®¤å¤šæ ·åŒ–è¯­éŸ³çš„åˆæ³•æ€§ï¼Œä»è€Œæ„å»ºå°Šé‡è¯­è¨€å¤šæ ·æ€§å’Œè¯´è¯è€…è‡ªä¸»æƒçš„ASRç³»ç»Ÿã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "Accepted to AIES 2025",
      "pdf_url": "https://arxiv.org/pdf/2508.07143v2",
      "published_date": "2025-08-10 02:26:47 UTC",
      "updated_date": "2025-08-13 02:20:09 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T10:48:19.949996+00:00"
    },
    {
      "arxiv_id": "2508.07142v4",
      "title": "Why Does Stochastic Gradient Descent Slow Down in Low-Precision Training?",
      "title_zh": "ä¸ºä»€ä¹ˆéšæœºæ¢¯åº¦ä¸‹é™åœ¨ä½ç²¾åº¦è®­ç»ƒä¸­ä¼šå˜æ…¢ï¼Ÿ",
      "authors": [
        "Vincent-Daniel Yun"
      ],
      "abstract": "Low-precision training has become crucial for reducing the computational and memory costs of large-scale deep learning. However, quantizing gradients introduces magnitude shrinkage, which can change how stochastic gradient descent (SGD) converges. In this study, we explore SGD convergence under a gradient shrinkage model, where each stochastic gradient is scaled by a factor \\( q_k \\in (0,1] \\). We show that this shrinkage affect the usual stepsize \\( Î¼_k \\) with an effective stepsize \\( Î¼_k q_k \\), slowing convergence when \\( q_{\\min} < 1 \\). With typical smoothness and bounded-variance assumptions, we prove that low-precision SGD still converges, but at a slower pace set by \\( q_{\\min} \\), and with a higher steady error level due to quantization effects. We analyze theoretically how lower numerical precision slows training by treating it as gradient shrinkage within the standard SGD convergence setup.",
      "tldr_zh": "è¿™é¡¹ç ”ç©¶æ¢è®¨äº†ä½ç²¾åº¦è®­ç»ƒ (Low-precision training) ä¸­éšæœºæ¢¯åº¦ä¸‹é™ (Stochastic Gradient Descent, SGD) é€Ÿåº¦å˜æ…¢çš„ç†è®ºåŸå› ã€‚ä½œè€…æå‡ºï¼Œæ¢¯åº¦é‡åŒ– (Quantization) å¼•å…¥çš„å¹…å€¼æ”¶ç¼© (Magnitude shrinkage) æ˜¯æ”¹å˜æ”¶æ•›ç‰¹æ€§çš„å…³é”®å› ç´ ï¼Œå¹¶ä»¥æ­¤å»ºç«‹äº†æ¢¯åº¦æ”¶ç¼©æ¨¡å‹è¿›è¡Œåˆ†æã€‚ç ”ç©¶è¯æ˜ï¼Œè¿™ç§æ”¶ç¼©å®è´¨ä¸Šäº§ç”Ÿäº†ä¸€ä¸ªè¾ƒå°çš„æœ‰æ•ˆæ­¥é•¿ (Effective stepsize)ï¼Œç›´æ¥å¯¼è‡´äº†æ”¶æ•›èŠ‚å¥çš„æ”¾ç¼“ã€‚åœ¨æ ‡å‡†å¹³æ»‘åº¦å’Œæœ‰ç•Œæ–¹å·®å‡è®¾ä¸‹ï¼Œç†è®ºè¯æ˜ä½ç²¾åº¦ SGD è™½ç„¶ä»èƒ½ä¿æŒæ”¶æ•›ï¼Œä½†å…¶é€Ÿåº¦ç”±æœ€å°æ”¶ç¼©å› å­ $q_{\\min}$ å†³å®šã€‚æ­¤å¤–ï¼Œé‡åŒ–æ•ˆåº”è¿˜ä¼šå¯¼è‡´æ¨¡å‹æœ€ç»ˆè¾¾åˆ°æ›´é«˜çš„ç¨³æ€è¯¯å·® (Steady error) æ°´å¹³ã€‚è¯¥åˆ†æé€šè¿‡å°†ä½ç²¾åº¦è®­ç»ƒè§†ä½œæ ‡å‡† SGD æ¡†æ¶ä¸‹çš„æ¢¯åº¦æ”¶ç¼©ï¼Œä¸ºç†è§£æ•°å€¼ç²¾åº¦å¯¹è®­ç»ƒæ•ˆç‡çš„å½±å“æä¾›äº†åšå®çš„ç†è®ºæ”¯æ’‘ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.IT",
        "math.NA"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.07142v4",
      "published_date": "2025-08-10 02:25:48 UTC",
      "updated_date": "2026-01-08 17:18:24 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T10:48:38.086216+00:00"
    },
    {
      "arxiv_id": "2508.07139v1",
      "title": "A Real-Time, Self-Tuning Moderator Framework for Adversarial Prompt Detection",
      "title_zh": "ä¸€ç§ç”¨äºå¯¹æŠ—æ€§æç¤ºè¯æ£€æµ‹çš„å®æ—¶è‡ªè°ƒä¼˜å®¡æ ¸æ¡†æ¶",
      "authors": [
        "Ivan Zhang"
      ],
      "abstract": "Ensuring LLM alignment is critical to information security as AI models become increasingly widespread and integrated in society. Unfortunately, many defenses against adversarial attacks and jailbreaking on LLMs cannot adapt quickly to new attacks, degrade model responses to benign prompts, or introduce significant barriers to scalable implementation. To mitigate these challenges, we introduce a real-time, self-tuning (RTST) moderator framework to defend against adversarial attacks while maintaining a lightweight training footprint. We empirically evaluate its effectiveness using Google's Gemini models against modern, effective jailbreaks. Our results demonstrate the advantages of an adaptive, minimally intrusive framework for jailbreak defense over traditional fine-tuning or classifier models.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§åä¸º RTST (Real-Time, Self-Tuning) çš„å®æ—¶è‡ªæ•´å®šè°ƒèŠ‚å™¨æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³å¤§å‹è¯­è¨€æ¨¡å‹ (LLMs) é˜²å¾¡å¯¹æŠ—æ€§æ”»å‡» (adversarial attacks) å’Œè¶Šç‹± (jailbreaking) æ—¶å­˜åœ¨çš„é€‚åº”é€Ÿåº¦æ…¢ã€å½±å“è‰¯æ€§å“åº”ä»¥åŠéš¾ä»¥è§„æ¨¡åŒ–éƒ¨ç½²ç­‰æŒ‘æˆ˜ã€‚è¯¥æ¡†æ¶å…·æœ‰è½»é‡çº§è®­ç»ƒè¶³è¿¹ (lightweight training footprint)ï¼Œèƒ½å¤Ÿåœ¨ç¡®ä¿å®‰å…¨æ€§çš„åŒæ—¶ä¿æŒè¾ƒä½çš„è®¡ç®—æˆæœ¬ã€‚ç ”ç©¶å›¢é˜Ÿåœ¨ Google Gemini æ¨¡å‹ä¸Šé’ˆå¯¹ç°ä»£é«˜æ•ˆçš„è¶Šç‹±æ‰‹æ®µè¿›è¡Œäº†å®è¯è¯„ä¼°ï¼ŒéªŒè¯äº†è¯¥æ¡†æ¶çš„é˜²å¾¡æ•ˆèƒ½ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¿™ç§è‡ªé€‚åº”ä¸”å¾®ä¾µå…¥å¼ (minimally intrusive) çš„æ¡†æ¶åœ¨å¯¹æŠ—æ€§æç¤ºæ£€æµ‹æ–¹é¢ä¼˜äºä¼ ç»Ÿçš„å¾®è°ƒ (fine-tuning) æˆ–åˆ†ç±»å™¨æ¨¡å‹ (classifier models)ã€‚è¯¥ç ”ç©¶ä¸ºå®ç°é«˜æ•ˆã€å¯æ‰©å±•ä¸”èƒ½å®æ—¶æ¼”è¿›çš„å¤§æ¨¡å‹å¯¹é½ (LLM alignment) é˜²å¾¡æœºåˆ¶æä¾›äº†é‡è¦å‚è€ƒã€‚",
      "categories": [
        "cs.CR",
        "cs.AI"
      ],
      "primary_category": "cs.CR",
      "comment": "10 pages, 1 figure",
      "pdf_url": "https://arxiv.org/pdf/2508.07139v1",
      "published_date": "2025-08-10 01:59:07 UTC",
      "updated_date": "2025-08-10 01:59:07 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T10:48:39.585874+00:00"
    },
    {
      "arxiv_id": "2508.07137v2",
      "title": "A Principled Loss Function for Direct Language Model Alignment",
      "title_zh": "ä¸€ç§ç”¨äºç›´æ¥è¯­è¨€æ¨¡å‹å¯¹é½çš„åŸºäºåŸåˆ™çš„æŸå¤±å‡½æ•°",
      "authors": [
        "Yuandong Tan"
      ],
      "abstract": "The alignment of large language models (LLMs) with human preferences is commonly achieved through Reinforcement Learning from Human Feedback (RLHF). Direct Preference Optimization (DPO) simplified this paradigm by establishing a direct mapping between the optimal policy and a reward function, eliminating the need for an explicit reward model. However, we argue that the DPO loss function is theoretically misaligned with its own derivation, as it promotes the indefinite maximization of a logits difference, which can lead to training instability and reward hacking. In this paper, we propose a novel loss function derived directly from the RLHF optimality condition. Our proposed loss targets a specific, finite value for the logits difference, which is dictated by the underlying reward, rather than its maximization. We provide a theoretical analysis, including a gradient-based comparison, to demonstrate that our method avoids the large gradients that plague DPO when the probability of dispreferred responses approaches zero. This inherent stability prevents reward hacking and leads to more effective alignment. We validate our approach by fine-tuning a Qwen2.5-7B model, showing significant win-rate improvements over a standard DPO baseline and achieving competitive performance against larger models like Llama-3.1-8B.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†å¤§è¯­è¨€æ¨¡å‹(LLMs)ä¸äººç±»åå¥½å¯¹é½çš„é—®é¢˜ï¼ŒæŒ‡å‡ºç›´æ¥åå¥½ä¼˜åŒ–(Direct Preference Optimization, DPO)åœ¨ç†è®ºä¸Šä¸å…¶æ¨å¯¼å­˜åœ¨ä¸ä¸€è‡´ï¼Œä¼šå¯¼è‡´Logitså·®å¼‚çš„æ— é™å¤§åŒ–ï¼Œè¿›è€Œå¼•å‘è®­ç»ƒä¸ç¨³å®šå’Œå¥–åŠ±ç ´è§£(Reward Hacking)ã€‚ä¸ºæ­¤ï¼Œä½œè€…ç›´æ¥ä»å¼ºåŒ–å­¦ä¹ äººç±»åé¦ˆ(RLHF)çš„æœ€ä¼˜æ€§æ¡ä»¶å‡ºå‘ï¼Œæå‡ºäº†ä¸€ç§å…¨æ–°çš„æŸå¤±å‡½æ•°ã€‚è¯¥æŸå¤±å‡½æ•°æ—¨åœ¨è®©Logitså·®å¼‚è¾¾åˆ°ç”±åº•å±‚å¥–åŠ±å†³å®šçš„ç‰¹å®šæœ‰é™å€¼ï¼Œè€Œéç›²ç›®æœ€å¤§åŒ–ã€‚ç†è®ºåˆ†æåŠæ¢¯åº¦å¯¹æ¯”è¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨ä¸è¢«åå¥½å“åº”çš„æ¦‚ç‡è¶‹è¿‘äºé›¶æ—¶ï¼Œèƒ½æœ‰æ•ˆé¿å…DPOå¸¸è§çš„æ¢¯åº¦è¿‡å¤§é—®é¢˜ï¼Œä»è€Œå¢å¼ºå¯¹é½ç¨³å®šæ€§ã€‚å®éªŒé€šè¿‡å¾®è°ƒQwen2.5-7Bæ¨¡å‹è¯æ˜ï¼Œè¯¥æ–¹æ³•åœ¨èƒœç‡ä¸Šæ˜¾è‘—ä¼˜äºæ ‡å‡†DPOåŸºçº¿ï¼Œå¹¶åœ¨æ€§èƒ½ä¸Šå¯ä¸Llama-3.1-8Bç­‰æ›´å¤§è§„æ¨¡çš„æ¨¡å‹ç«äº‰ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.07137v2",
      "published_date": "2025-08-10 01:56:58 UTC",
      "updated_date": "2025-09-25 09:08:05 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T10:48:41.091998+00:00"
    },
    {
      "arxiv_id": "2508.07132v1",
      "title": "\"Draw me a curator\" Examining the visual stereotyping of a cultural services profession by generative AI",
      "title_zh": "â€œç»™æˆ‘ç”»ä¸€ä¸ªç­–å±•äººâ€ï¼šå®¡è§†ç”Ÿæˆå¼äººå·¥æ™ºèƒ½å¯¹æ–‡åŒ–æœåŠ¡èŒä¸šçš„è§†è§‰åˆ»æ¿å°è±¡",
      "authors": [
        "Dirk HR Spennemann"
      ],
      "abstract": "Based on 230 visualisations, this paper examines the depiction of museum curators by the popular generative Artificial Intelligence (AI) model, ChatGPT4o. While the AI-generated representations do not reiterate popular stereotypes of curators as nerdy, conservative in dress and stuck in time rummaging through collections, they contrast sharply with real-world demographics. AI-generated imagery extremely underrepresents women (3.5% vs 49% to 72% in reality) and disregards ethnic communities other than Caucasian (0% vs 18% to 36%). It only over-represents young curators (79% vs approx. 27%) but also renders curators to resemble yuppie professionals or people featuring in fashion advertising. Stereotypical attributes are prevalent, with curators widely depicted as wearing beards and holding clipboards or digital tablets. The findings highlight biases in the generative AI image creation dataset, which is poised to shape an inaccurate portrayal of museum professionals if the images were to be taken uncritically at face value.",
      "tldr_zh": "è¯¥ç ”ç©¶é€šè¿‡åˆ†æ230å¼ ç”± ChatGPT4o ç”Ÿæˆçš„è§†è§‰å›¾åƒï¼Œæ¢è®¨äº†ç”Ÿæˆå¼äººå·¥æ™ºèƒ½ (Generative AI) åœ¨æç»˜åšç‰©é¦†ç­–å±•äºº (Curators) è¿™ä¸€æ–‡åŒ–æœåŠ¡èŒä¸šæ—¶å­˜åœ¨çš„è§†è§‰åˆ»æ¿å°è±¡ã€‚ç ”ç©¶å‘ç° AI ç”Ÿæˆçš„å›¾åƒè™½ç„¶é¿å¼€äº†è€æ´¾ã€å®ˆæ—§ã€åœ¨è—å“ä¸­ç¿»æ‰¾çš„ä¼ ç»Ÿåˆ»æ¿å°è±¡ï¼Œä½†å´ä¸çœŸå®çš„äººå£ç»Ÿè®¡æ•°æ®å½¢æˆäº†é²œæ˜å¯¹æ¯”ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒAI æåº¦ä½ä¼°äº†å¥³æ€§ç­–å±•äººçš„æ¯”ä¾‹ï¼ˆä»…ä¸º3.5%ï¼Œè¿œä½äºç°å®ä¸­49%è‡³72%çš„å æ¯”ï¼‰ï¼Œä¸”å®Œå…¨å¿½ç•¥äº†é«˜åŠ ç´¢è£”ä»¥å¤–çš„å…¶ä»–æ—è£”ç¾¤ä½“ã€‚ä¸æ­¤åŒæ—¶ï¼ŒAI å¤§å¹…è¿‡åº¦å‘ˆç°äº†å¹´è½»ç­–å±•äººçš„å½¢è±¡ï¼Œå¹¶å°†å…¶æç»˜å¾—æ›´æ¥è¿‘é›…çš®å£«ä¸“ä¸šäººå£«æˆ–æ—¶å°šå¹¿å‘Šæ¨¡ç‰¹ã€‚å›¾åƒä¸­è¿˜æ™®éå­˜åœ¨è“„èƒ¡é¡»ã€æ‰‹æŒå‰ªè´´æ¿ (Clipboards) æˆ–ç”µå­å¹³æ¿ç­‰ç‰¹å®šçš„è§†è§‰å±æ€§ï¼Œåæ˜ äº†æ–°çš„åˆ»æ¿å€¾å‘ã€‚è¿™äº›å‘ç°æ­ç¤ºäº†ç”Ÿæˆå¼ AI è®­ç»ƒæ•°æ®é›†ä¸­çš„ç³»ç»Ÿæ€§åå·® (Biases)ï¼Œè­¦å‘Šè‹¥ä¸åŠ æ‰¹åˆ¤åœ°å¼•ç”¨æ­¤ç±»å›¾åƒï¼Œå°†å¡‘é€ å‡ºæåº¦å¤±çœŸçš„åšç‰©é¦†ä¸“ä¸šäººå‘˜å…¬ä¼—å½¢è±¡ã€‚",
      "categories": [
        "cs.CY",
        "cs.AI"
      ],
      "primary_category": "cs.CY",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.07132v1",
      "published_date": "2025-08-10 00:43:43 UTC",
      "updated_date": "2025-08-10 00:43:43 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T10:48:51.596662+00:00"
    },
    {
      "arxiv_id": "2508.07129v1",
      "title": "Toward AI Matching Policies in Homeless Services: A Qualitative Study with Policymakers",
      "title_zh": "è¿ˆå‘æ— å®¶å¯å½’è€…æœåŠ¡ä¸­çš„ AI åŒ¹é…æ”¿ç­–ï¼šä¸€é¡¹é¢å‘æ”¿ç­–åˆ¶å®šè€…çš„å®šæ€§ç ”ç©¶",
      "authors": [
        "Caroline M. Johnston",
        "Olga Koumoundouros",
        "Angel Hsing-Chi Hwang",
        "Laura Onasch-Vera",
        "Eric Rice",
        "Phebe Vayanos"
      ],
      "abstract": "Artificial intelligence researchers have proposed various data-driven algorithms to improve the processes that match individuals experiencing homelessness to scarce housing resources. It remains unclear whether and how these algorithms are received or adopted by practitioners and what their corresponding consequences are. Through semi-structured interviews with 13 policymakers in homeless services in Los Angeles, we investigate whether such change-makers are open to the idea of integrating AI into the housing resource matching process, identifying where they see potential gains and drawbacks from such a system in issues of efficiency, fairness, and transparency. Our qualitative analysis indicates that, even when aware of various complicating factors, policymakers welcome the idea of an AI matching tool if thoughtfully designed and used in tandem with human decision-makers. Though there is no consensus as to the exact design of such an AI system, insights from policymakers raise open questions and design considerations that can be enlightening for future researchers and practitioners who aim to build responsible algorithmic systems to support decision-making in low-resource scenarios.",
      "tldr_zh": "è¿™é¡¹ç ”ç©¶é€šè¿‡å¯¹æ´›æ‰çŸ¶13ä½æ— å®¶å¯å½’è€…æœåŠ¡æ”¿ç­–åˆ¶å®šè€…è¿›è¡ŒåŠç»“æ„åŒ–è®¿è°ˆ(semi-structured interviews)ï¼Œæ¢è®¨äº†åœ¨ä½æˆ¿èµ„æºåŒ¹é…è¿‡ç¨‹ä¸­å¼•å…¥äººå·¥æ™ºèƒ½(AI)ç³»ç»Ÿçš„æ¥å—ç¨‹åº¦åŠå…¶å¯¹æ•ˆç‡ã€å…¬å¹³æ€§å’Œé€æ˜åº¦çš„å½±å“ã€‚å®šæ€§åˆ†æ(qualitative analysis)ç»“æœè¡¨æ˜ï¼Œå¦‚æœAIå·¥å…·èƒ½å¤Ÿç»è¿‡ç²¾å¿ƒè®¾è®¡å¹¶ä¸äººç±»å†³ç­–è€…(human decision-makers)ååŒä½¿ç”¨ï¼Œæ”¿ç­–åˆ¶å®šè€…å¯¹æ•´åˆAIæŠ€æœ¯æŒå¼€æ”¾å’Œæ¬¢è¿æ€åº¦ã€‚ç ”ç©¶æŒ‡å‡ºï¼Œè™½ç„¶ç›®å‰åœ¨ç³»ç»Ÿçš„å…·ä½“è®¾è®¡æ–¹æ¡ˆä¸Šå°šæœªè¾¾æˆå…±è¯†ï¼Œä½†å†³ç­–è€…çš„è§è§£ä¸ºæœªæ¥åœ¨ä½èµ„æºåœºæ™¯(low-resource scenarios)ä¸‹æ„å»ºè´Ÿè´£ä»»çš„ç®—æ³•å†³ç­–ç³»ç»Ÿæä¾›äº†å…³é”®çš„è®¾è®¡è€ƒé‡ã€‚è¯¥è®ºæ–‡é€šè¿‡æ­ç¤ºæ”¿ç­–åˆ¶å®šè€…çš„è§†è§’ï¼Œä¸ºç ”ç©¶äººå‘˜å’Œä»ä¸šè€…åœ¨ç¤¾ä¼šæœåŠ¡é¢†åŸŸå¼€å‘èƒ½å¤Ÿæ”¯æŒå¤æ‚å†³ç­–çš„AIåŒ¹é…æ”¿ç­–æä¾›äº†é‡è¦çš„å®è·µæ´å¯Ÿã€‚",
      "categories": [
        "cs.HC",
        "cs.AI"
      ],
      "primary_category": "cs.HC",
      "comment": "21 pages, 1 figure, 2 tables",
      "pdf_url": "https://arxiv.org/pdf/2508.07129v1",
      "published_date": "2025-08-10 00:33:03 UTC",
      "updated_date": "2025-08-10 00:33:03 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T10:49:01.294701+00:00"
    },
    {
      "arxiv_id": "2508.07128v1",
      "title": "Perceptual Evaluation of GANs and Diffusion Models for Generating X-rays",
      "title_zh": "ç”¨äº X å°„çº¿å½±åƒç”Ÿæˆçš„ GANs ä¸æ‰©æ•£æ¨¡å‹çš„æ„ŸçŸ¥è¯„ä¼°",
      "authors": [
        "Gregory Schuit",
        "Denis Parra",
        "Cecilia Besa"
      ],
      "abstract": "Generative image models have achieved remarkable progress in both natural and medical imaging. In the medical context, these techniques offer a potential solution to data scarcity-especially for low-prevalence anomalies that impair the performance of AI-driven diagnostic and segmentation tools. However, questions remain regarding the fidelity and clinical utility of synthetic images, since poor generation quality can undermine model generalizability and trust. In this study, we evaluate the effectiveness of state-of-the-art generative models-Generative Adversarial Networks (GANs) and Diffusion Models (DMs)-for synthesizing chest X-rays conditioned on four abnormalities: Atelectasis (AT), Lung Opacity (LO), Pleural Effusion (PE), and Enlarged Cardiac Silhouette (ECS). Using a benchmark composed of real images from the MIMIC-CXR dataset and synthetic images from both GANs and DMs, we conducted a reader study with three radiologists of varied experience. Participants were asked to distinguish real from synthetic images and assess the consistency between visual features and the target abnormality. Our results show that while DMs generate more visually realistic images overall, GANs can report better accuracy for specific conditions, such as absence of ECS. We further identify visual cues radiologists use to detect synthetic images, offering insights into the perceptual gaps in current models. These findings underscore the complementary strengths of GANs and DMs and point to the need for further refinement to ensure generative models can reliably augment training datasets for AI diagnostic systems.",
      "tldr_zh": "è¯¥ç ”ç©¶å¯¹ç”Ÿæˆå¯¹æŠ—ç½‘ç»œ(Generative Adversarial Networks, GANs)å’Œæ‰©æ•£æ¨¡å‹(Diffusion Models, DMs)åœ¨ç”Ÿæˆèƒ¸éƒ¨Xå°„çº¿å½±åƒæ–¹é¢çš„æœ‰æ•ˆæ€§è¿›è¡Œäº†æ„ŸçŸ¥è¯„ä¼°ï¼Œæ—¨åœ¨æ¢è®¨å…¶ç¼“è§£åŒ»ç–—æ•°æ®ç¨€ç¼ºé—®é¢˜çš„æ½œåŠ›ã€‚å®éªŒé’ˆå¯¹è‚ºä¸å¼ (Atelectasis)ã€è‚ºæ··æµŠ(Lung Opacity)ã€èƒ¸è…”ç§¯æ¶²(Pleural Effusion)å’Œå¿ƒè„è½®å»“å¢å¤§(Enlarged Cardiac Silhouette)å››ç§å¼‚å¸¸ï¼Œé€šè¿‡ä¸‰ä½æ”¾å°„ç§‘åŒ»ç”Ÿçš„è¯»è€…ç ”ç©¶å¯¹æ¯”äº†åˆæˆå½±åƒä¸çœŸå®MIMIC-CXRå½±åƒçš„å·®å¼‚ã€‚ç»“æœæ˜¾ç¤ºï¼Œè™½ç„¶æ‰©æ•£æ¨¡å‹(DMs)ç”Ÿæˆçš„å›¾åƒåœ¨æ•´ä½“è§†è§‰çœŸå®æ„Ÿä¸Šæ›´èƒœä¸€ç­¹ï¼Œä½†GANsåœ¨ç‰¹å®šç—…ç†æ¡ä»¶ï¼ˆå¦‚æ— å¿ƒè„è½®å»“å¢å¤§ï¼‰çš„å‡†ç¡®æ€§ä¸Šè¡¨ç°æ›´å¥½ã€‚æ­¤å¤–ï¼Œç ”ç©¶è¯†åˆ«äº†æ”¾å°„ç§‘åŒ»ç”Ÿç”¨äºæ£€æµ‹åˆæˆå½±åƒçš„è§†è§‰çº¿ç´¢ï¼Œæ­ç¤ºäº†ç°æœ‰æ¨¡å‹åœ¨çŸ¥è§‰ä¸Šçš„å·®è·ã€‚è¿™äº›å‘ç°å¼ºè°ƒäº†GANsä¸DMsçš„äº’è¡¥æ€§ï¼Œå¹¶ä¸ºè¿›ä¸€æ­¥ä¼˜åŒ–ç”Ÿæˆæ¨¡å‹ä»¥å¢å¼ºAIè¯Šæ–­ç³»ç»Ÿçš„è®­ç»ƒæ•°æ®é›†æä¾›äº†é‡è¦è§è§£ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "Accepted to the Workshop on Human-AI Collaboration at MICCAI 2025",
      "pdf_url": "https://arxiv.org/pdf/2508.07128v1",
      "published_date": "2025-08-10 00:32:18 UTC",
      "updated_date": "2025-08-10 00:32:18 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T10:48:59.791225+00:00"
    },
    {
      "arxiv_id": "2508.07126v2",
      "title": "Pref-GUIDE: Continual Policy Learning from Real-Time Human Feedback via Preference-Based Learning",
      "title_zh": "Pref-GUIDEï¼šåŸºäºåå¥½å­¦ä¹ çš„å®æ—¶äººç±»åé¦ˆæŒç»­ç­–ç•¥å­¦ä¹ ",
      "authors": [
        "Zhengran Ji",
        "Boyuan Chen"
      ],
      "abstract": "Training reinforcement learning agents with human feedback is crucial when task objectives are difficult to specify through dense reward functions. While prior methods rely on offline trajectory comparisons to elicit human preferences, such data is unavailable in online learning scenarios where agents must adapt on the fly. Recent approaches address this by collecting real-time scalar feedback to guide agent behavior and train reward models for continued learning after human feedback becomes unavailable. However, scalar feedback is often noisy and inconsistent, limiting the accuracy and generalization of learned rewards. We propose Pref-GUIDE, a framework that transforms real-time scalar feedback into preference-based data to improve reward model learning for continual policy training. Pref-GUIDE Individual mitigates temporal inconsistency by comparing agent behaviors within short windows and filtering ambiguous feedback. Pref-GUIDE Voting further enhances robustness by aggregating reward models across a population of users to form consensus preferences. Across three challenging environments, Pref-GUIDE significantly outperforms scalar-feedback baselines, with the voting variant exceeding even expert-designed dense rewards. By reframing scalar feedback as structured preferences with population feedback, Pref-GUIDE offers a scalable and principled approach for harnessing human input in online reinforcement learning.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¼ºåŒ–å­¦ä¹ (Reinforcement Learning)ä¸­ä»»åŠ¡ç›®æ ‡éš¾ä»¥é€šè¿‡ç¨ å¯†å¥–åŠ±å‡½æ•°(dense reward functions)æŒ‡å®šçš„æŒ‘æˆ˜ï¼Œæå‡ºäº†åä¸ºPref-GUIDEçš„æŒç»­ç­–ç•¥å­¦ä¹ æ¡†æ¶ã€‚è¯¥æ¡†æ¶åˆ›æ–°æ€§åœ°å°†å®æ—¶æ ‡é‡åé¦ˆ(real-time scalar feedback)è½¬åŒ–ä¸ºåŸºäºåå¥½çš„æ•°æ®(preference-based data)ï¼Œä»¥å…‹æœåœ¨çº¿å­¦ä¹ åœºæ™¯ä¸­æ ‡é‡åé¦ˆå™ªå£°å¤§ä¸”ä¸ä¸€è‡´çš„å±€é™æ€§ã€‚å…¶ä¸­Pref-GUIDE Individualé€šè¿‡åœ¨çŸ­æ—¶é—´çª—å£å†…æ¯”è¾ƒæ™ºèƒ½ä½“è¡Œä¸ºå¹¶è¿‡æ»¤æ¨¡ç³Šåé¦ˆï¼Œæœ‰æ•ˆç¼“è§£äº†æ—¶é—´ä¸ä¸€è‡´æ€§(temporal inconsistency)é—®é¢˜ï¼›è€ŒPref-GUIDE Votingåˆ™é€šè¿‡èšåˆå¤šç”¨æˆ·åé¦ˆå½¢æˆå…±è¯†åå¥½ï¼Œè¿›ä¸€æ­¥å¢å¼ºäº†å¥–åŠ±æ¨¡å‹çš„é²æ£’æ€§(robustness)ã€‚åœ¨ä¸‰é¡¹æŒ‘æˆ˜æ€§ä»»åŠ¡ä¸­çš„å®éªŒç»“æœè¡¨æ˜ï¼ŒPref-GUIDEæ˜¾è‘—ä¼˜äºä¼ ç»Ÿçš„æ ‡é‡åé¦ˆåŸºå‡†ï¼Œå…¶æŠ•ç¥¨å˜ä½“ç”šè‡³è¶…è¶Šäº†ä¸“å®¶è®¾è®¡çš„ç¨ å¯†å¥–åŠ±ã€‚è¯¥ç ”ç©¶ä¸ºåœ¨çº¿å¼ºåŒ–å­¦ä¹ (online reinforcement learning)åˆ©ç”¨äººç±»è¾“å…¥æä¾›äº†ä¸€ç§å¯æ‰©å±•ä¸”è§„èŒƒçš„æ–¹æ³•ï¼Œæå‡äº†å¥–åŠ±æ¨¡å‹å­¦ä¹ çš„å‡†ç¡®æ€§ä¸æ³›åŒ–èƒ½åŠ›ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.07126v2",
      "published_date": "2025-08-10 00:18:44 UTC",
      "updated_date": "2025-10-06 20:55:39 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T10:49:01.693334+00:00"
    }
  ],
  "processing_status": "completed",
  "error": null,
  "raw_papers_fetched": true,
  "papers_count": 87,
  "processed_papers_count": 87,
  "failed_papers_count": 0,
  "llm_backup_calls": 0,
  "summary_generated": true,
  "daily_data_saved": true,
  "last_update": "2026-01-24T10:51:23.882774+00:00"
}
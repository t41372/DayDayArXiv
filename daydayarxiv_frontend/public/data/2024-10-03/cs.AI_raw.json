[
  {
    "arxiv_id": "2410.14690v1",
    "title": "Rethinking VLMs and LLMs for Image Classification",
    "authors": [
      "Avi Cooper",
      "Keizo Kato",
      "Chia-Hsien Shih",
      "Hiroaki Yamane",
      "Kasper Vinken",
      "Kentaro Takemoto",
      "Taro Sunagawa",
      "Hao-Wei Yeh",
      "Jin Yamanaka",
      "Ian Mason",
      "Xavier Boix"
    ],
    "abstract": "Visual Language Models (VLMs) are now increasingly being merged with Large\nLanguage Models (LLMs) to enable new capabilities, particularly in terms of\nimproved interactivity and open-ended responsiveness. While these are\nremarkable capabilities, the contribution of LLMs to enhancing the longstanding\nkey problem of classifying an image among a set of choices remains unclear.\nThrough extensive experiments involving seven models, ten visual understanding\ndatasets, and multiple prompt variations per dataset, we find that, for object\nand scene recognition, VLMs that do not leverage LLMs can achieve better\nperformance than VLMs that do. Yet at the same time, leveraging LLMs can\nimprove performance on tasks requiring reasoning and outside knowledge. In\nresponse to these challenges, we propose a pragmatic solution: a lightweight\nfix involving a relatively small LLM that efficiently routes visual tasks to\nthe most suitable model for the task. The LLM router undergoes training using a\ndataset constructed from more than 2.5 million examples of pairs of visual task\nand model accuracy. Our results reveal that this lightweight fix surpasses or\nmatches the accuracy of state-of-the-art alternatives, including GPT-4V and\nHuggingGPT, while improving cost-effectiveness.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.14690v1",
    "published_date": "2024-10-03 23:40:21 UTC",
    "updated_date": "2024-10-03 23:40:21 UTC"
  },
  {
    "arxiv_id": "2410.03039v1",
    "title": "Revealing the Unseen: Guiding Personalized Diffusion Models to Expose Training Data",
    "authors": [
      "Xiaoyu Wu",
      "Jiaru Zhang",
      "Steven Wu"
    ],
    "abstract": "Diffusion Models (DMs) have evolved into advanced image generation tools,\nespecially for few-shot fine-tuning where a pretrained DM is fine-tuned on a\nsmall set of images to capture specific styles or objects. Many people upload\nthese personalized checkpoints online, fostering communities such as Civitai\nand HuggingFace. However, model owners may overlook the potential risks of data\nleakage by releasing their fine-tuned checkpoints. Moreover, concerns regarding\ncopyright violations arise when unauthorized data is used during fine-tuning.\nIn this paper, we ask: \"Can training data be extracted from these fine-tuned\nDMs shared online?\" A successful extraction would present not only data leakage\nthreats but also offer tangible evidence of copyright infringement. To answer\nthis, we propose FineXtract, a framework for extracting fine-tuning data. Our\nmethod approximates fine-tuning as a gradual shift in the model's learned\ndistribution -- from the original pretrained DM toward the fine-tuning data. By\nextrapolating the models before and after fine-tuning, we guide the generation\ntoward high-probability regions within the fine-tuned data distribution. We\nthen apply a clustering algorithm to extract the most probable images from\nthose generated using this extrapolated guidance. Experiments on DMs fine-tuned\nwith datasets such as WikiArt, DreamBooth, and real-world checkpoints posted\nonline validate the effectiveness of our method, extracting approximately 20%\nof fine-tuning data in most cases, significantly surpassing baseline\nperformance.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "Under review",
    "pdf_url": "http://arxiv.org/pdf/2410.03039v1",
    "published_date": "2024-10-03 23:06:11 UTC",
    "updated_date": "2024-10-03 23:06:11 UTC"
  },
  {
    "arxiv_id": "2410.03035v3",
    "title": "SPINE: Online Semantic Planning for Missions with Incomplete Natural Language Specifications in Unstructured Environments",
    "authors": [
      "Zachary Ravichandran",
      "Varun Murali",
      "Mariliza Tzes",
      "George J. Pappas",
      "Vijay Kumar"
    ],
    "abstract": "As robots become increasingly capable, users will want to describe high-level\nmissions and have robots infer the relevant details. Because pre-built maps are\ndifficult to obtain in many realistic settings, accomplishing such missions\nwill require the robot to map and plan online. While many semantic planning\nmethods operate online, they are typically designed for well specified missions\nsuch as object search or exploration. Recently, Large Language Models (LLMs)\nhave demonstrated powerful contextual reasoning abilities over a range of\nrobotic tasks described in natural language. However, existing LLM-enabled\nplanners typically do not consider online planning or complex missions; rather,\nrelevant subtasks and semantics are provided by a pre-built map or a user. We\naddress these limitations via SPINE, an online planner for missions with\nincomplete mission specifications provided in natural language. The planner\nuses an LLM to reason about subtasks implied by the mission specification and\nthen realizes these subtasks in a receding horizon framework. Tasks are\nautomatically validated for safety and refined online with new map\nobservations. We evaluate SPINE in simulation and real-world settings with\nmissions that require multiple steps of semantic reasoning and exploration in\ncluttered outdoor environments of over 20,000m$^2$. Compared to baselines that\nuse existing LLM-enabled planning approaches, our method is over twice as\nefficient in terms of time and distance, requires less user interactions, and\ndoes not require a full map. Additional resources are provided at\nhttps://zacravichandran.github.io/SPINE.",
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "primary_category": "cs.RO",
    "comment": "Accepted to the International Conference on Robotics and Automation\n  (ICRA) 2025",
    "pdf_url": "http://arxiv.org/pdf/2410.03035v3",
    "published_date": "2024-10-03 22:41:47 UTC",
    "updated_date": "2025-03-21 01:34:48 UTC"
  },
  {
    "arxiv_id": "2410.12837v1",
    "title": "A Comprehensive Survey of Retrieval-Augmented Generation (RAG): Evolution, Current Landscape and Future Directions",
    "authors": [
      "Shailja Gupta",
      "Rajesh Ranjan",
      "Surya Narayan Singh"
    ],
    "abstract": "This paper presents a comprehensive study of Retrieval-Augmented Generation\n(RAG), tracing its evolution from foundational concepts to the current state of\nthe art. RAG combines retrieval mechanisms with generative language models to\nenhance the accuracy of outputs, addressing key limitations of LLMs. The study\nexplores the basic architecture of RAG, focusing on how retrieval and\ngeneration are integrated to handle knowledge-intensive tasks. A detailed\nreview of the significant technological advancements in RAG is provided,\nincluding key innovations in retrieval-augmented language models and\napplications across various domains such as question-answering, summarization,\nand knowledge-based tasks. Recent research breakthroughs are discussed,\nhighlighting novel methods for improving retrieval efficiency. Furthermore, the\npaper examines ongoing challenges such as scalability, bias, and ethical\nconcerns in deployment. Future research directions are proposed, focusing on\nimproving the robustness of RAG models, expanding the scope of application of\nRAG models, and addressing societal implications. This survey aims to serve as\na foundational resource for researchers and practitioners in understanding the\npotential of RAG and its trajectory in natural language processing.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.IR"
    ],
    "primary_category": "cs.CL",
    "comment": "4 Figures",
    "pdf_url": "http://arxiv.org/pdf/2410.12837v1",
    "published_date": "2024-10-03 22:29:47 UTC",
    "updated_date": "2024-10-03 22:29:47 UTC"
  },
  {
    "arxiv_id": "2410.03032v3",
    "title": "CounterQuill: Investigating the Potential of Human-AI Collaboration in Online Counterspeech Writing",
    "authors": [
      "Xiaohan Ding",
      "Kaike Ping",
      "Uma Sushmitha Gunturi",
      "Buse Carik",
      "Sophia Stil",
      "Lance T Wilhelm",
      "Taufiq Daryanto",
      "James Hawdon",
      "Sang Won Lee",
      "Eugenia H Rho"
    ],
    "abstract": "Online hate speech has become increasingly prevalent on social media, causing\nharm to individuals and society. While automated content moderation has\nreceived considerable attention, user-driven counterspeech remains a less\nexplored yet promising approach. However, many people face difficulties in\ncrafting effective responses. We introduce CounterQuill, a human-AI\ncollaborative system that helps everyday users with writing empathetic\ncounterspeech, not by generating automatic replies, but by educating them\nthrough reflection and response. CounterQuill follows a three-stage workflow\ngrounded in computational thinking: (1) a learning session to build\nunderstanding of hate speech and counterspeech, (2) a brainstorming session to\nidentify harmful patterns and ideate counterspeech ideas, and (3) a co-writing\nsession that helps users refine their counter responses while preserving\npersonal voice. Through a user study \\r{ho}(N=20), we found that CounterQuill\nhelped participants develop the skills to brainstorm and draft counterspeech\nwith increased confidence and control throughout the process. Our findings\nhighlight how AI systems can scaffold complex communication tasks through\nstructured, human-centered workflows that educate users on how to recognize,\nreflect on, and respond to online hate speech.",
    "categories": [
      "cs.HC",
      "cs.AI",
      "cs.CY"
    ],
    "primary_category": "cs.HC",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.03032v3",
    "published_date": "2024-10-03 22:29:20 UTC",
    "updated_date": "2025-05-16 23:49:10 UTC"
  },
  {
    "arxiv_id": "2410.03030v2",
    "title": "Dynamic Sparse Training versus Dense Training: The Unexpected Winner in Image Corruption Robustness",
    "authors": [
      "Boqian Wu",
      "Qiao Xiao",
      "Shunxin Wang",
      "Nicola Strisciuglio",
      "Mykola Pechenizkiy",
      "Maurice van Keulen",
      "Decebal Constantin Mocanu",
      "Elena Mocanu"
    ],
    "abstract": "It is generally perceived that Dynamic Sparse Training opens the door to a\nnew era of scalability and efficiency for artificial neural networks at,\nperhaps, some costs in accuracy performance for the classification task. At the\nsame time, Dense Training is widely accepted as being the \"de facto\" approach\nto train artificial neural networks if one would like to maximize their\nrobustness against image corruption. In this paper, we question this general\npractice. Consequently, we claim that, contrary to what is commonly thought,\nthe Dynamic Sparse Training methods can consistently outperform Dense Training\nin terms of robustness accuracy, particularly if the efficiency aspect is not\nconsidered as a main objective (i.e., sparsity levels between 10% and up to\n50%), without adding (or even reducing) resource cost. We validate our claim on\ntwo types of data, images and videos, using several traditional and modern deep\nlearning architectures for computer vision and three widely studied Dynamic\nSparse Training algorithms. Our findings reveal a new yet-unknown benefit of\nDynamic Sparse Training and open new possibilities in improving deep learning\nrobustness beyond the current state of the art.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "Accepted at ICLR 2025",
    "pdf_url": "http://arxiv.org/pdf/2410.03030v2",
    "published_date": "2024-10-03 22:24:54 UTC",
    "updated_date": "2025-03-05 04:37:07 UTC"
  },
  {
    "arxiv_id": "2410.03024v2",
    "title": "Flow Matching with Gaussian Process Priors for Probabilistic Time Series Forecasting",
    "authors": [
      "Marcel Kollovieh",
      "Marten Lienen",
      "David Lüdke",
      "Leo Schwinn",
      "Stephan Günnemann"
    ],
    "abstract": "Recent advancements in generative modeling, particularly diffusion models,\nhave opened new directions for time series modeling, achieving state-of-the-art\nperformance in forecasting and synthesis. However, the reliance of\ndiffusion-based models on a simple, fixed prior complicates the generative\nprocess since the data and prior distributions differ significantly. We\nintroduce TSFlow, a conditional flow matching (CFM) model for time series\ncombining Gaussian processes, optimal transport paths, and data-dependent prior\ndistributions. By incorporating (conditional) Gaussian processes, TSFlow aligns\nthe prior distribution more closely with the temporal structure of the data,\nenhancing both unconditional and conditional generation. Furthermore, we\npropose conditional prior sampling to enable probabilistic forecasting with an\nunconditionally trained model. In our experimental evaluation on eight\nreal-world datasets, we demonstrate the generative capabilities of TSFlow,\nproducing high-quality unconditional samples. Finally, we show that both\nconditionally and unconditionally trained models achieve competitive results\nacross multiple forecasting benchmarks.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.03024v2",
    "published_date": "2024-10-03 22:12:50 UTC",
    "updated_date": "2025-05-11 22:30:03 UTC"
  },
  {
    "arxiv_id": "2410.03019v2",
    "title": "Is Your Paper Being Reviewed by an LLM? Investigating AI Text Detectability in Peer Review",
    "authors": [
      "Sungduk Yu",
      "Man Luo",
      "Avinash Madasu",
      "Vasudev Lal",
      "Phillip Howard"
    ],
    "abstract": "Peer review is a critical process for ensuring the integrity of published\nscientific research. Confidence in this process is predicated on the assumption\nthat experts in the relevant domain give careful consideration to the merits of\nmanuscripts which are submitted for publication. With the recent rapid\nadvancements in the linguistic capabilities of large language models (LLMs), a\nnew potential risk to the peer review process is that negligent reviewers will\nrely on LLMs to perform the often time consuming process of reviewing a paper.\nIn this study, we investigate the ability of existing AI text detection\nalgorithms to distinguish between peer reviews written by humans and different\nstate-of-the-art LLMs. Our analysis shows that existing approaches fail to\nidentify many GPT-4o written reviews without also producing a high number of\nfalse positive classifications. To address this deficiency, we propose a new\ndetection approach which surpasses existing methods in the identification of\nGPT-4o written peer reviews at low levels of false positive classifications.\nOur work reveals the difficulty of accurately identifying AI-generated text at\nthe individual review level, highlighting the urgent need for new tools and\nmethods to detect this type of unethical application of generative AI.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.03019v2",
    "published_date": "2024-10-03 22:05:06 UTC",
    "updated_date": "2024-12-06 17:23:53 UTC"
  },
  {
    "arxiv_id": "2410.03018v1",
    "title": "Transforming Teachers' Roles and Agencies in the Era of Generative AI: Perceptions, Acceptance, Knowledge, and Practices",
    "authors": [
      "Xiaoming Zhai"
    ],
    "abstract": "This paper explores the transformative impact of Generative Artificial\nIntelligence (GenAI) on teachers' roles and agencies in education, presenting a\ncomprehensive framework that addresses teachers' perceptions, knowledge,\nacceptance, and practices of GenAI. As GenAI technologies, such as ChatGPT,\nbecome increasingly integrated into educational settings, teachers are required\nto adapt to evolving classroom dynamics, where AI plays a significant role in\ncontent creation, personalized learning, and student engagement. However,\nexisting literature often treats these factors in isolation, overlooking how\nthey collectively influence teachers' ability to effectively integrate GenAI\ninto their pedagogical practices. This paper fills this gap by proposing a\nframework that categorizes teachers into four roles -- Observer, Adopter,\nCollaborator, and Innovator -- each representing different levels of GenAI\nengagement, outlining teachers' agencies in GenAI classrooms. By highlighting\nthe need for continuous professional development and institutional support, we\ndemonstrate how teachers can evolve from basic GenAI users to co-creators of\nknowledge alongside GenAI systems. The findings emphasize that for GenAI to\nreach its full educational potential, teachers must not only accept and\nunderstand its capabilities but also integrate it deeply into their teaching\nstrategies. This study contributes to the growing literature on GenAI in\neducation, offering practical implications for supporting teachers in\nnavigating the complexities of GenAI adoption.",
    "categories": [
      "cs.CY",
      "cs.AI"
    ],
    "primary_category": "cs.CY",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.03018v1",
    "published_date": "2024-10-03 21:59:01 UTC",
    "updated_date": "2024-10-03 21:59:01 UTC"
  },
  {
    "arxiv_id": "2410.03007v1",
    "title": "FastAdaSP: Multitask-Adapted Efficient Inference for Large Speech Language Model",
    "authors": [
      "Yichen Lu",
      "Jiaqi Song",
      "Chao-Han Huck Yang",
      "Shinji Watanabe"
    ],
    "abstract": "In this study, we aim to explore Multitask Speech Language Model (SpeechLM)\nefficient inference via token reduction. Unlike other modalities such as vision\nor text, speech has unique temporal dependencies, making previous efficient\ninference works on other modalities not directly applicable. Furthermore,\nmethods for efficient SpeechLM inference on long sequence and sparse signals\nremain largely unexplored. Then we propose FastAdaSP, a weighted token merging\nframework specifically designed for various speech-related tasks to improve the\ntrade-off between efficiency and performance. Experimental results on WavLLM\nand Qwen-Audio show that our method achieves the state-of-the-art (SOTA)\nefficiency-performance trade-off compared with other baseline methods.\nSpecifically, FastAdaSP achieved 7x memory efficiency and 1.83x decoding\nthroughput without any degradation on tasks like Emotion Recognition (ER) and\nSpoken Question Answering (SQA). The code will be available at\nhttps://github.com/yichen14/FastAdaSP",
    "categories": [
      "eess.AS",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "eess.AS",
    "comment": "EMNLP 2024 Industry Track",
    "pdf_url": "http://arxiv.org/pdf/2410.03007v1",
    "published_date": "2024-10-03 21:33:07 UTC",
    "updated_date": "2024-10-03 21:33:07 UTC"
  },
  {
    "arxiv_id": "2410.03791v2",
    "title": "People are poorly equipped to detect AI-powered voice clones",
    "authors": [
      "Sarah Barrington",
      "Emily A. Cooper",
      "Hany Farid"
    ],
    "abstract": "As generative artificial intelligence (AI) continues its ballistic\ntrajectory, everything from text to audio, image, and video generation\ncontinues to improve at mimicking human-generated content. Through a series of\nperceptual studies, we report on the realism of AI-generated voices in terms of\nidentity matching and naturalness. We find human participants cannot\nconsistently identify recordings of AI-generated voices. Specifically,\nparticipants perceived the identity of an AI-voice to be the same as its real\ncounterpart approximately 80% of the time, and correctly identified a voice as\nAI generated only about 60% of the time.",
    "categories": [
      "cs.HC",
      "cs.AI",
      "cs.CY",
      "cs.SD",
      "eess.AS"
    ],
    "primary_category": "cs.HC",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.03791v2",
    "published_date": "2024-10-03 21:26:58 UTC",
    "updated_date": "2025-01-25 01:18:23 UTC"
  },
  {
    "arxiv_id": "2410.02995v3",
    "title": "Task-free Lifelong Robot Learning with Retrieval-based Weighted Local Adaptation",
    "authors": [
      "Pengzhi Yang",
      "Xinyu Wang",
      "Ruipeng Zhang",
      "Cong Wang",
      "Frans A. Oliehoek",
      "Jens Kober"
    ],
    "abstract": "A fundamental objective in intelligent robotics is to move towards lifelong\nlearning robot that can learn and adapt to unseen scenarios over time. However,\ncontinually learning new tasks would introduce catastrophic forgetting problems\ndue to data distribution shifts. To mitigate this, we store a subset of data\nfrom previous tasks and utilize it in two manners: leveraging experience replay\nto retain learned skills and applying a novel Retrieval-based Local Adaptation\ntechnique to restore relevant knowledge. Since a lifelong learning robot must\noperate in task-free scenarios, where task IDs and even boundaries are not\navailable, our method performs effectively without relying on such information.\nWe also incorporate a selective weighting mechanism to focus on the most\n\"forgotten\" skill segment, ensuring effective knowledge restoration.\nExperimental results across diverse manipulation tasks demonstrate that our\nframework provides a scalable paradigm for lifelong learning, enhancing robot\nperformance in open-ended, task-free scenarios.",
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "primary_category": "cs.RO",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.02995v3",
    "published_date": "2024-10-03 21:11:42 UTC",
    "updated_date": "2025-02-03 12:08:50 UTC"
  },
  {
    "arxiv_id": "2410.02992v1",
    "title": "Guided Stream of Search: Learning to Better Search with Language Models via Optimal Path Guidance",
    "authors": [
      "Seungyong Moon",
      "Bumsoo Park",
      "Hyun Oh Song"
    ],
    "abstract": "While language models have demonstrated impressive capabilities across a\nrange of tasks, they still struggle with tasks that require complex planning\nand reasoning. Recent studies have proposed training language models on search\nprocesses rather than optimal solutions, resulting in better generalization\nperformance even though search processes are noisy and even suboptimal.\nHowever, these studies overlook the value of optimal solutions, which can serve\nas step-by-step landmarks to guide more effective search. In this work, we\nexplore how to leverage optimal solutions to enhance the search and planning\nabilities of language models. To this end, we propose guided stream of search\n(GSoS), which seamlessly incorporates optimal solutions into the\nself-generation process in a progressive manner, producing high-quality search\ntrajectories. These trajectories are then distilled into the pre-trained model\nvia supervised fine-tuning. Our approach significantly enhances the search and\nplanning abilities of language models on Countdown, a simple yet challenging\nmathematical reasoning task. Notably, combining our method with RL fine-tuning\nyields further improvements, whereas previous supervised fine-tuning methods do\nnot benefit from RL. Furthermore, our approach exhibits greater effectiveness\nthan leveraging optimal solutions in the form of subgoal rewards.",
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.02992v1",
    "published_date": "2024-10-03 21:07:59 UTC",
    "updated_date": "2024-10-03 21:07:59 UTC"
  },
  {
    "arxiv_id": "2410.02984v1",
    "title": "Differentiation and Specialization of Attention Heads via the Refined Local Learning Coefficient",
    "authors": [
      "George Wang",
      "Jesse Hoogland",
      "Stan van Wingerden",
      "Zach Furman",
      "Daniel Murfet"
    ],
    "abstract": "We introduce refined variants of the Local Learning Coefficient (LLC), a\nmeasure of model complexity grounded in singular learning theory, to study the\ndevelopment of internal structure in transformer language models during\ntraining. By applying these \\textit{refined LLCs} (rLLCs) to individual\ncomponents of a two-layer attention-only transformer, we gain novel insights\ninto the progressive differentiation and specialization of attention heads. Our\nmethodology reveals how attention heads differentiate into distinct functional\nroles over the course of training, analyzes the types of data these heads\nspecialize to process, and discovers a previously unidentified multigram\ncircuit. These findings demonstrate that rLLCs provide a principled,\nquantitative toolkit for \\textit{developmental interpretability}, which aims to\nunderstand models through their evolution across the learning process. More\nbroadly, this work takes a step towards establishing the correspondence between\ndata distributional structure, geometric properties of the loss landscape,\nlearning dynamics, and emergent computational structures in neural networks.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.02984v1",
    "published_date": "2024-10-03 20:51:02 UTC",
    "updated_date": "2024-10-03 20:51:02 UTC"
  },
  {
    "arxiv_id": "2410.02978v1",
    "title": "An explainable approach to detect case law on housing and eviction issues within the HUDOC database",
    "authors": [
      "Mohammad Mohammadi",
      "Martijn Wieling",
      "Michel Vols"
    ],
    "abstract": "Case law is instrumental in shaping our understanding of human rights,\nincluding the right to adequate housing. The HUDOC database provides access to\nthe textual content of case law from the European Court of Human Rights\n(ECtHR), along with some metadata. While this metadata includes valuable\ninformation, such as the application number and the articles addressed in a\ncase, it often lacks detailed substantive insights, such as the specific issues\na case covers. This underscores the need for detailed analysis to extract such\ninformation. However, given the size of the database - containing over 40,000\ncases - an automated solution is essential.\n  In this study, we focus on the right to adequate housing and aim to build\nmodels to detect cases related to housing and eviction issues. Our experiments\nshow that the resulting models not only provide performance comparable to more\nsophisticated approaches but are also interpretable, offering explanations for\ntheir decisions by highlighting the most influential words. The application of\nthese models led to the identification of new cases that were initially\noverlooked during data collection. This suggests that NLP approaches can be\neffectively applied to categorise case law based on the specific issues they\naddress.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.02978v1",
    "published_date": "2024-10-03 20:39:51 UTC",
    "updated_date": "2024-10-03 20:39:51 UTC"
  },
  {
    "arxiv_id": "2410.02977v1",
    "title": "Harm Ratio: A Novel and Versatile Fairness Criterion",
    "authors": [
      "Soroush Ebadian",
      "Rupert Freeman",
      "Nisarg Shah"
    ],
    "abstract": "Envy-freeness has become the cornerstone of fair division research. In\nsettings where each individual is allocated a disjoint share of collective\nresources, it is a compelling fairness axiom which demands that no individual\nstrictly prefer the allocation of another individual to their own.\nUnfortunately, in many real-life collective decision-making problems, the goal\nis to choose a (common) public outcome that is equally applicable to all\nindividuals, and the notion of envy becomes vacuous. Consequently, this\nliterature has avoided studying fairness criteria that focus on individuals\nfeeling a sense of jealousy or resentment towards other individuals (rather\nthan towards the system), missing out on a key aspect of fairness.\n  In this work, we propose a novel fairness criterion, individual harm ratio,\nwhich is inspired by envy-freeness but applies to a broad range of collective\ndecision-making settings. Theoretically, we identify minimal conditions under\nwhich this criterion and its groupwise extensions can be guaranteed, and study\nthe computational complexity of related problems. Empirically, we conduct\nexperiments with real data to show that our fairness criterion is powerful\nenough to differentiate between prominent decision-making algorithms for a\nrange of tasks from voting and fair division to participatory budgeting and\npeer review.",
    "categories": [
      "cs.GT",
      "cs.AI"
    ],
    "primary_category": "cs.GT",
    "comment": "To appear at EAAMO 2024",
    "pdf_url": "http://arxiv.org/pdf/2410.02977v1",
    "published_date": "2024-10-03 20:36:05 UTC",
    "updated_date": "2024-10-03 20:36:05 UTC"
  },
  {
    "arxiv_id": "2410.03787v1",
    "title": "CalliffusionV2: Personalized Natural Calligraphy Generation with Flexible Multi-modal Control",
    "authors": [
      "Qisheng Liao",
      "Liang Li",
      "Yulang Fei",
      "Gus Xia"
    ],
    "abstract": "In this paper, we introduce CalliffusionV2, a novel system designed to\nproduce natural Chinese calligraphy with flexible multi-modal control. Unlike\nprevious approaches that rely solely on image or text inputs and lack\nfine-grained control, our system leverages both images to guide generations at\nfine-grained levels and natural language texts to describe the features of\ngenerations. CalliffusionV2 excels at creating a broad range of characters and\ncan quickly learn new styles through a few-shot learning approach. It is also\ncapable of generating non-Chinese characters without prior training.\nComprehensive tests confirm that our system produces calligraphy that is both\nstylistically accurate and recognizable by neural network classifiers and human\nevaluators.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CV",
      "cs.MM"
    ],
    "primary_category": "cs.CL",
    "comment": "11 pages, 7 figures",
    "pdf_url": "http://arxiv.org/pdf/2410.03787v1",
    "published_date": "2024-10-03 20:26:54 UTC",
    "updated_date": "2024-10-03 20:26:54 UTC"
  },
  {
    "arxiv_id": "2410.02970v2",
    "title": "F-Fidelity: A Robust Framework for Faithfulness Evaluation of Explainable AI",
    "authors": [
      "Xu Zheng",
      "Farhad Shirani",
      "Zhuomin Chen",
      "Chaohao Lin",
      "Wei Cheng",
      "Wenbo Guo",
      "Dongsheng Luo"
    ],
    "abstract": "Recent research has developed a number of eXplainable AI (XAI) techniques,\nsuch as gradient-based approaches, input perturbation-base methods, and\nblack-box explanation methods. While these XAI techniques can extract\nmeaningful insights from deep learning models, how to properly evaluate them\nremains an open problem. The most widely used approach is to perturb or even\nremove what the XAI method considers to be the most important features in an\ninput and observe the changes in the output prediction. This approach, although\nstraightforward, suffers the Out-of-Distribution (OOD) problem as the perturbed\nsamples may no longer follow the original data distribution. A recent method\nRemOve And Retrain (ROAR) solves the OOD issue by retraining the model with\nperturbed samples guided by explanations. However, using the model retrained\nbased on XAI methods to evaluate these explainers may cause information leakage\nand thus lead to unfair comparisons. We propose Fine-tuned Fidelity\n(F-Fidelity), a robust evaluation framework for XAI, which utilizes i) an\nexplanation-agnostic fine-tuning strategy, thus mitigating the information\nleakage issue, and ii) a random masking operation that ensures that the removal\nstep does not generate an OOD input. We also design controlled experiments with\nstate-of-the-art (SOTA) explainers and their degraded version to verify the\ncorrectness of our framework. We conduct experiments on multiple data\nmodalities, such as images, time series, and natural language. The results\ndemonstrate that F-Fidelity significantly improves upon prior evaluation\nmetrics in recovering the ground-truth ranking of the explainers. Furthermore,\nwe show both theoretically and empirically that, given a faithful explainer,\nF-Fidelity metric can be used to compute the sparsity of influential input\ncomponents, i.e., to extract the true explanation size.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted to International Conference on Learning Representations\n  (ICLR 2025); 33 Pages, 5 figures, 26 Tables",
    "pdf_url": "http://arxiv.org/pdf/2410.02970v2",
    "published_date": "2024-10-03 20:23:06 UTC",
    "updated_date": "2025-03-06 20:06:16 UTC"
  },
  {
    "arxiv_id": "2410.02967v1",
    "title": "Label-Free Subjective Player Experience Modelling via Let's Play Videos",
    "authors": [
      "Dave Goel",
      "Athar Mahmoudi-Nejad",
      "Matthew Guzdial"
    ],
    "abstract": "Player Experience Modelling (PEM) is the study of AI techniques applied to\nmodelling a player's experience within a video game. PEM development can be\nlabour-intensive, requiring expert hand-authoring or specialized data\ncollection. In this work, we propose a novel PEM development approach,\napproximating player experience from gameplay video. We evaluate this approach\npredicting affect in the game Angry Birds via a human subject study. We\nvalidate that our PEM can strongly correlate with self-reported and sensor\nmeasures of affect, demonstrating the potential of this approach.",
    "categories": [
      "cs.HC",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.HC",
    "comment": "9 pages, 3 figures, AAAI Conference on Artificial Intelligence and\n  Interactive Digital Entertainment",
    "pdf_url": "http://arxiv.org/pdf/2410.02967v1",
    "published_date": "2024-10-03 20:12:56 UTC",
    "updated_date": "2024-10-03 20:12:56 UTC"
  },
  {
    "arxiv_id": "2410.02958v1",
    "title": "AutoML-Agent: A Multi-Agent LLM Framework for Full-Pipeline AutoML",
    "authors": [
      "Patara Trirat",
      "Wonyong Jeong",
      "Sung Ju Hwang"
    ],
    "abstract": "Automated machine learning (AutoML) accelerates AI development by automating\ntasks in the development pipeline, such as optimal model search and\nhyperparameter tuning. Existing AutoML systems often require technical\nexpertise to set up complex tools, which is in general time-consuming and\nrequires a large amount of human effort. Therefore, recent works have started\nexploiting large language models (LLM) to lessen such burden and increase the\nusability of AutoML frameworks via a natural language interface, allowing\nnon-expert users to build their data-driven solutions. These methods, however,\nare usually designed only for a particular process in the AI development\npipeline and do not efficiently use the inherent capacity of the LLMs. This\npaper proposes AutoML-Agent, a novel multi-agent framework tailored for\nfull-pipeline AutoML, i.e., from data retrieval to model deployment.\nAutoML-Agent takes user's task descriptions, facilitates collaboration between\nspecialized LLM agents, and delivers deployment-ready models. Unlike existing\nwork, instead of devising a single plan, we introduce a retrieval-augmented\nplanning strategy to enhance exploration to search for more optimal plans. We\nalso decompose each plan into sub-tasks (e.g., data preprocessing and neural\nnetwork design) each of which is solved by a specialized agent we build via\nprompting executing in parallel, making the search process more efficient.\nMoreover, we propose a multi-stage verification to verify executed results and\nguide the code generation LLM in implementing successful solutions. Extensive\nexperiments on seven downstream tasks using fourteen datasets show that\nAutoML-Agent achieves a higher success rate in automating the full AutoML\nprocess, yielding systems with good performance throughout the diverse domains.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL",
      "cs.MA"
    ],
    "primary_category": "cs.LG",
    "comment": "47 pages, 5 figures",
    "pdf_url": "http://arxiv.org/pdf/2410.02958v1",
    "published_date": "2024-10-03 20:01:09 UTC",
    "updated_date": "2024-10-03 20:01:09 UTC"
  },
  {
    "arxiv_id": "2410.02955v1",
    "title": "AiBAT: Artificial Intelligence/Instructions for Build, Assembly, and Test",
    "authors": [
      "Benjamin Nuernberger",
      "Anny Liu",
      "Heather Stefanini",
      "Richard Otis",
      "Amanda Towler",
      "R. Peter Dillon"
    ],
    "abstract": "Instructions for Build, Assembly, and Test (IBAT) refers to the process used\nwhenever any operation is conducted on hardware, including tests, assembly, and\nmaintenance. Currently, the generation of IBAT documents is time-intensive, as\nusers must manually reference and transfer information from engineering\ndiagrams and parts lists into IBAT instructions. With advances in machine\nlearning and computer vision, however, it is possible to have an artificial\nintelligence (AI) model perform the partial filling of the IBAT template,\nfreeing up engineer time for more highly skilled tasks. AiBAT is a novel system\nfor assisting users in authoring IBATs. It works by first analyzing assembly\ndrawing documents, extracting information and parsing it, and then filling in\nIBAT templates with the extracted information. Such assisted authoring has\npotential to save time and reduce cost. This paper presents an overview of the\nAiBAT system, including promising preliminary results and discussion on future\nwork.",
    "categories": [
      "cs.AI",
      "cs.AR",
      "cs.ET",
      "cs.HC"
    ],
    "primary_category": "cs.AI",
    "comment": "9 pages, 6 figures, 2 tables",
    "pdf_url": "http://arxiv.org/pdf/2410.02955v1",
    "published_date": "2024-10-03 19:57:05 UTC",
    "updated_date": "2024-10-03 19:57:05 UTC"
  },
  {
    "arxiv_id": "2410.02952v3",
    "title": "Visual Editing with LLM-based Tool Chaining: An Efficient Distillation Approach for Real-Time Applications",
    "authors": [
      "Oren Sultan",
      "Alex Khasin",
      "Guy Shiran",
      "Asnat Greenstein-Messica",
      "Dafna Shahaf"
    ],
    "abstract": "We present a practical distillation approach to fine-tune LLMs for invoking\ntools in real-time applications. We focus on visual editing tasks;\nspecifically, we modify images and videos by interpreting user stylistic\nrequests, specified in natural language (\"golden hour\"), using an LLM to select\nthe appropriate tools and their parameters to achieve the desired visual\neffect. We found that proprietary LLMs such as GPT-3.5-Turbo show potential in\nthis task, but their high cost and latency make them unsuitable for real-time\napplications. In our approach, we fine-tune a (smaller) student LLM with\nguidance from a (larger) teacher LLM and behavioral signals. We introduce\noffline metrics to evaluate student LLMs. Both online and offline experiments\nshow that our student models manage to match the performance of our teacher\nmodel (GPT-3.5-Turbo), significantly reducing costs and latency. Lastly, we\nshow that fine-tuning was improved by 25% in low-data regimes using\naugmentation.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "EMNLP 2024",
    "pdf_url": "http://arxiv.org/pdf/2410.02952v3",
    "published_date": "2024-10-03 19:52:37 UTC",
    "updated_date": "2024-10-10 11:41:35 UTC"
  },
  {
    "arxiv_id": "2410.02950v1",
    "title": "LLMCO2: Advancing Accurate Carbon Footprint Prediction for LLM Inferences",
    "authors": [
      "Zhenxiao Fu",
      "Fan Chen",
      "Shan Zhou",
      "Haitong Li",
      "Lei Jiang"
    ],
    "abstract": "Throughout its lifecycle, a large language model (LLM) generates a\nsubstantially larger carbon footprint during inference than training. LLM\ninference requests vary in batch size, prompt length, and token generation\nnumber, while cloud providers employ different GPU types and quantities to meet\ndiverse service-level objectives for accuracy and latency. It is crucial for\nboth users and cloud providers to have a tool that quickly and accurately\nestimates the carbon impact of LLM inferences based on a combination of\ninference request and hardware configurations before execution. Estimating the\ncarbon footprint of LLM inferences is more complex than training due to lower\nand highly variable model FLOPS utilization, rendering previous equation-based\nmodels inaccurate. Additionally, existing machine learning (ML) prediction\nmethods either lack accuracy or demand extensive training data, as they\ninadequately handle the distinct prefill and decode phases, overlook\nhardware-specific features, and inefficiently sample uncommon inference\nconfigurations. We introduce \\coo, a graph neural network (GNN)-based model\nthat greatly improves the accuracy of LLM inference carbon footprint\npredictions compared to previous methods.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL",
      "cs.CY"
    ],
    "primary_category": "cs.LG",
    "comment": "9 pages, 11 figures",
    "pdf_url": "http://arxiv.org/pdf/2410.02950v1",
    "published_date": "2024-10-03 19:48:45 UTC",
    "updated_date": "2024-10-03 19:48:45 UTC"
  },
  {
    "arxiv_id": "2410.02942v2",
    "title": "SymmetricDiffusers: Learning Discrete Diffusion on Finite Symmetric Groups",
    "authors": [
      "Yongxing Zhang",
      "Donglin Yang",
      "Renjie Liao"
    ],
    "abstract": "Finite symmetric groups $S_n$ are essential in fields such as combinatorics,\nphysics, and chemistry. However, learning a probability distribution over $S_n$\nposes significant challenges due to its intractable size and discrete nature.\nIn this paper, we introduce SymmetricDiffusers, a novel discrete diffusion\nmodel that simplifies the task of learning a complicated distribution over\n$S_n$ by decomposing it into learning simpler transitions of the reverse\ndiffusion using deep neural networks. We identify the riffle shuffle as an\neffective forward transition and provide empirical guidelines for selecting the\ndiffusion length based on the theory of random walks on finite groups.\nAdditionally, we propose a generalized Plackett-Luce (PL) distribution for the\nreverse transition, which is provably more expressive than the PL distribution.\nWe further introduce a theoretically grounded \"denoising schedule\" to improve\nsampling and learning efficiency. Extensive experiments show that our model\nachieves state-of-the-art or comparable performances on solving tasks including\nsorting 4-digit MNIST images, jigsaw puzzles, and traveling salesman problems.\nOur code is released at https://github.com/DSL-Lab/SymmetricDiffusers.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.LG",
    "comment": "ICLR 2025 Oral",
    "pdf_url": "http://arxiv.org/pdf/2410.02942v2",
    "published_date": "2024-10-03 19:37:40 UTC",
    "updated_date": "2025-03-05 22:22:12 UTC"
  },
  {
    "arxiv_id": "2410.02932v1",
    "title": "Intrinsic Evaluation of RAG Systems for Deep-Logic Questions",
    "authors": [
      "Junyi Hu",
      "You Zhou",
      "Jie Wang"
    ],
    "abstract": "We introduce the Overall Performance Index (OPI), an intrinsic metric to\nevaluate retrieval-augmented generation (RAG) mechanisms for applications\ninvolving deep-logic queries. OPI is computed as the harmonic mean of two key\nmetrics: the Logical-Relation Correctness Ratio and the average of BERT\nembedding similarity scores between ground-truth and generated answers. We\napply OPI to assess the performance of LangChain, a popular RAG tool, using a\nlogical relations classifier fine-tuned from GPT-4o on the RAG-Dataset-12000\nfrom Hugging Face. Our findings show a strong correlation between BERT\nembedding similarity scores and extrinsic evaluation scores. Among the commonly\nused retrievers, the cosine similarity retriever using BERT-based embeddings\noutperforms others, while the Euclidean distance-based retriever exhibits the\nweakest performance. Furthermore, we demonstrate that combining multiple\nretrievers, either algorithmically or by merging retrieved sentences, yields\nsuperior performance compared to using any single retriever alone.",
    "categories": [
      "cs.AI",
      "I.2.7"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.02932v1",
    "published_date": "2024-10-03 19:25:05 UTC",
    "updated_date": "2024-10-03 19:25:05 UTC"
  },
  {
    "arxiv_id": "2410.02917v1",
    "title": "Deep image-based Adaptive BRDF Measure",
    "authors": [
      "Wen Cao"
    ],
    "abstract": "Efficient and accurate measurement of the bi-directional reflectance\ndistribution function (BRDF) plays a key role in high quality image rendering\nand physically accurate sensor simulation. However, obtaining the reflectance\nproperties of a material is both time-consuming and challenging. This paper\npresents a novel method for minimizing the number of samples required for high\nquality BRDF capture using a gonio-reflectometer setup. Taking an image of the\nphysical material sample as input a lightweight neural network first estimates\nthe parameters of an analytic BRDF model, and the distribution of the sample\nlocations. In a second step we use an image based loss to find the number of\nsamples required to meet the accuracy required. This approach significantly\naccelerates the measurement process while maintaining a high level of accuracy\nand fidelity in the BRDF representation.",
    "categories": [
      "cs.GR",
      "cs.AI"
    ],
    "primary_category": "cs.GR",
    "comment": "9",
    "pdf_url": "http://arxiv.org/pdf/2410.02917v1",
    "published_date": "2024-10-03 19:08:48 UTC",
    "updated_date": "2024-10-03 19:08:48 UTC"
  },
  {
    "arxiv_id": "2410.02916v3",
    "title": "LLM Safeguard is a Double-Edged Sword: Exploiting False Positives for Denial-of-Service Attacks",
    "authors": [
      "Qingzhao Zhang",
      "Ziyang Xiong",
      "Z. Morley Mao"
    ],
    "abstract": "Safety is a paramount concern for large language models (LLMs) in open\ndeployment, motivating the development of safeguard methods that enforce\nethical and responsible use through safety alignment or guardrail mechanisms.\nJailbreak attacks that exploit the \\emph{false negatives} of safeguard methods\nhave emerged as a prominent research focus in the field of LLM security.\nHowever, we found that the malicious attackers could also exploit false\npositives of safeguards, i.e., fooling the safeguard model to block safe\ncontent mistakenly, leading to a denial-of-service (DoS) affecting LLM users.\nTo bridge the knowledge gap of this overlooked threat, we explore multiple\nattack methods that include inserting a short adversarial prompt into user\nprompt templates and corrupting the LLM on the server by poisoned fine-tuning.\nIn both ways, the attack triggers safeguard rejections of user requests from\nthe client. Our evaluation demonstrates the severity of this threat across\nmultiple scenarios. For instance, in the scenario of white-box adversarial\nprompt injection, the attacker can use our optimization process to\nautomatically generate seemingly safe adversarial prompts, approximately only\n30 characters long, that universally block over 97% of user requests on Llama\nGuard 3. These findings reveal a new dimension in LLM safeguard evaluation --\nadversarial robustness to false positives.",
    "categories": [
      "cs.CR",
      "cs.AI"
    ],
    "primary_category": "cs.CR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.02916v3",
    "published_date": "2024-10-03 19:07:53 UTC",
    "updated_date": "2025-04-09 15:20:33 UTC"
  },
  {
    "arxiv_id": "2410.02914v1",
    "title": "Streamlining Conformal Information Retrieval via Score Refinement",
    "authors": [
      "Yotam Intrator",
      "Ori Kelner",
      "Regev Cohen",
      "Roman Goldenberg",
      "Ehud Rivlin",
      "Daniel Freedman"
    ],
    "abstract": "Information retrieval (IR) methods, like retrieval augmented generation, are\nfundamental to modern applications but often lack statistical guarantees.\nConformal prediction addresses this by retrieving sets guaranteed to include\nrelevant information, yet existing approaches produce large-sized sets,\nincurring high computational costs and slow response times. In this work, we\nintroduce a score refinement method that applies a simple monotone\ntransformation to retrieval scores, leading to significantly smaller conformal\nsets while maintaining their statistical guarantees. Experiments on various\nBEIR benchmarks validate the effectiveness of our approach in producing compact\nsets containing relevant information.",
    "categories": [
      "cs.IR",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.IR",
    "comment": "6 pages",
    "pdf_url": "http://arxiv.org/pdf/2410.02914v1",
    "published_date": "2024-10-03 19:05:47 UTC",
    "updated_date": "2024-10-03 19:05:47 UTC"
  },
  {
    "arxiv_id": "2410.02912v1",
    "title": "Fine-Tuning Language Models with Differential Privacy through Adaptive Noise Allocation",
    "authors": [
      "Xianzhi Li",
      "Ran Zmigrod",
      "Zhiqiang Ma",
      "Xiaomo Liu",
      "Xiaodan Zhu"
    ],
    "abstract": "Language models are capable of memorizing detailed patterns and information,\nleading to a double-edged effect: they achieve impressive modeling performance\non downstream tasks with the stored knowledge but also raise significant\nprivacy concerns. Traditional differential privacy based training approaches\noffer robust safeguards by employing a uniform noise distribution across all\nparameters. However, this overlooks the distinct sensitivities and\ncontributions of individual parameters in privacy protection and often results\nin suboptimal models. To address these limitations, we propose ANADP, a novel\nalgorithm that adaptively allocates additive noise based on the importance of\nmodel parameters. We demonstrate that ANADP narrows the performance gap between\nregular fine-tuning and traditional DP fine-tuning on a series of datasets\nwhile maintaining the required privacy constraints.",
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.CR",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "EMNLP 2024 findings",
    "pdf_url": "http://arxiv.org/pdf/2410.02912v1",
    "published_date": "2024-10-03 19:02:50 UTC",
    "updated_date": "2024-10-03 19:02:50 UTC"
  },
  {
    "arxiv_id": "2410.02902v4",
    "title": "Better Instruction-Following Through Minimum Bayes Risk",
    "authors": [
      "Ian Wu",
      "Patrick Fernandes",
      "Amanda Bertsch",
      "Seungone Kim",
      "Sina Pakazad",
      "Graham Neubig"
    ],
    "abstract": "General-purpose LLM judges capable of human-level evaluation provide not only\na scalable and accurate way of evaluating instruction-following LLMs but also\nnew avenues for supervising and improving their performance. One promising way\nof leveraging LLM judges for supervision is through Minimum Bayes Risk (MBR)\ndecoding, which uses a reference-based evaluator to select a high-quality\noutput from amongst a set of candidate outputs. In the first part of this work,\nwe explore using MBR decoding as a method for improving the test-time\nperformance of instruction-following LLMs. We find that MBR decoding with\nreference-based LLM judges substantially improves over greedy decoding,\nbest-of-N decoding with reference-free judges and MBR decoding with lexical and\nembedding-based metrics on AlpacaEval and MT-Bench. These gains are consistent\nacross LLMs with up to 70B parameters, demonstrating that smaller LLM judges\ncan be used to supervise much larger LLMs. Then, seeking to retain the\nimprovements from MBR decoding while mitigating additional test-time costs, we\nexplore iterative self-training on MBR-decoded outputs. We find that\nself-training using Direct Preference Optimisation leads to significant\nperformance gains, such that the self-trained models with greedy decoding\ngenerally match and sometimes exceed the performance of their base models with\nMBR decoding.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted to ICLR 2025 (Spotlight); Camera Ready",
    "pdf_url": "http://arxiv.org/pdf/2410.02902v4",
    "published_date": "2024-10-03 18:48:38 UTC",
    "updated_date": "2025-02-25 19:43:29 UTC"
  },
  {
    "arxiv_id": "2410.03786v1",
    "title": "AI-rays: Exploring Bias in the Gaze of AI Through a Multimodal Interactive Installation",
    "authors": [
      "Ziyao Gao",
      "Yiwen Zhang",
      "Ling Li",
      "Theodoros Papatheodorou",
      "Wei Zeng"
    ],
    "abstract": "Data surveillance has become more covert and pervasive with AI algorithms,\nwhich can result in biased social classifications. Appearance offers intuitive\nidentity signals, but what does it mean to let AI observe and speculate on\nthem? We introduce AI-rays, an interactive installation where AI generates\nspeculative identities from participants' appearance which are expressed\nthrough synthesized personal items placed in participants' bags. It uses\nspeculative X-ray visions to contrast reality with AI-generated assumptions,\nmetaphorically highlighting AI's scrutiny and biases. AI-rays promotes\ndiscussions on modern surveillance and the future of human-machine reality\nthrough a playful, immersive experience exploring AI biases.",
    "categories": [
      "cs.HC",
      "cs.AI",
      "cs.CY"
    ],
    "primary_category": "cs.HC",
    "comment": "Siggraph Asia 2024 Art Paper",
    "pdf_url": "http://arxiv.org/pdf/2410.03786v1",
    "published_date": "2024-10-03 18:44:05 UTC",
    "updated_date": "2024-10-03 18:44:05 UTC"
  },
  {
    "arxiv_id": "2410.02897v1",
    "title": "Cognitive Biases in Large Language Models for News Recommendation",
    "authors": [
      "Yougang Lyu",
      "Xiaoyu Zhang",
      "Zhaochun Ren",
      "Maarten de Rijke"
    ],
    "abstract": "Despite large language models (LLMs) increasingly becoming important\ncomponents of news recommender systems, employing LLMs in such systems\nintroduces new risks, such as the influence of cognitive biases in LLMs.\nCognitive biases refer to systematic patterns of deviation from norms or\nrationality in the judgment process, which can result in inaccurate outputs\nfrom LLMs, thus threatening the reliability of news recommender systems.\nSpecifically, LLM-based news recommender systems affected by cognitive biases\ncould lead to the propagation of misinformation, reinforcement of stereotypes,\nand the formation of echo chambers. In this paper, we explore the potential\nimpact of multiple cognitive biases on LLM-based news recommender systems,\nincluding anchoring bias, framing bias, status quo bias and group attribution\nbias. Furthermore, to facilitate future research at improving the reliability\nof LLM-based news recommender systems, we discuss strategies to mitigate these\nbiases through data augmentation, prompt engineering and learning algorithms\naspects.",
    "categories": [
      "cs.IR",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.IR",
    "comment": "Accepted at the ROGEN '24 workshop, co-located with ACM RecSys '24",
    "pdf_url": "http://arxiv.org/pdf/2410.02897v1",
    "published_date": "2024-10-03 18:42:07 UTC",
    "updated_date": "2024-10-03 18:42:07 UTC"
  },
  {
    "arxiv_id": "2410.02892v2",
    "title": "The Role of Deductive and Inductive Reasoning in Large Language Models",
    "authors": [
      "Chengkun Cai",
      "Xu Zhao",
      "Haoliang Liu",
      "Zhongyu Jiang",
      "Tianfang Zhang",
      "Zongkai Wu",
      "Jenq-Neng Hwang",
      "Serge Belongie",
      "Lei Li"
    ],
    "abstract": "Large Language Models (LLMs) have demonstrated impressive capabilities in\nreasoning tasks, yet their reliance on static prompt structures and limited\nadaptability to complex scenarios remains a significant challenge. In this\npaper, we propose the Deductive and InDuctive(DID) method, a novel framework\nthat enhances LLM reasoning by dynamically integrating both deductive and\ninductive reasoning approaches. Drawing from cognitive science principles, DID\nimplements a dual-metric complexity evaluation system that combines Littlestone\ndimension and information entropy to precisely assess task difficulty and guide\ndecomposition strategies. DID enables the model to progressively adapt its\nreasoning pathways based on problem complexity, mirroring human cognitive\nprocesses. We evaluate DID's effectiveness across multiple benchmarks,\nincluding the AIW and MR-GSM8K, as well as our custom Holiday Puzzle dataset\nfor temporal reasoning. Our results demonstrate significant improvements in\nreasoning quality and solution accuracy - achieving 70.3% accuracy on AIW\n(compared to 62.2% for Tree of Thought) while maintaining lower computational\ncosts. The success of DID in improving LLM performance while preserving\ncomputational efficiency suggests promising directions for developing more\ncognitively aligned and capable language models. Our work contributes a\ntheoretically grounded, input-centric approach to enhancing LLM reasoning\ncapabilities, offering an efficient alternative to traditional\noutput-exploration methods.",
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "4 figures",
    "pdf_url": "http://arxiv.org/pdf/2410.02892v2",
    "published_date": "2024-10-03 18:30:47 UTC",
    "updated_date": "2025-02-17 10:22:52 UTC"
  },
  {
    "arxiv_id": "2410.02884v2",
    "title": "LLaMA-Berry: Pairwise Optimization for O1-like Olympiad-Level Mathematical Reasoning",
    "authors": [
      "Di Zhang",
      "Jianbo Wu",
      "Jingdi Lei",
      "Tong Che",
      "Jiatong Li",
      "Tong Xie",
      "Xiaoshui Huang",
      "Shufei Zhang",
      "Marco Pavone",
      "Yuqiang Li",
      "Wanli Ouyang",
      "Dongzhan Zhou"
    ],
    "abstract": "This paper presents an advanced mathematical problem-solving framework,\nLLaMA-Berry, for enhancing the mathematical reasoning ability of Large Language\nModels (LLMs). The framework combines Monte Carlo Tree Search (MCTS) with\niterative Self-Refine to optimize the reasoning path and utilizes a pairwise\nreward model to evaluate different paths globally. By leveraging the\nself-critic and rewriting capabilities of LLMs, Self-Refine applied to MCTS\n(SR-MCTS) overcomes the inefficiencies and limitations of conventional\nstep-wise and greedy search algorithms by fostering a more efficient\nexploration of solution spaces. Pairwise Preference Reward Model~(PPRM),\ninspired by Reinforcement Learning from Human Feedback (RLHF), is then used to\nmodel pairwise preferences between solutions, utilizing an Enhanced Borda Count\n(EBC) method to synthesize these preferences into a global ranking score to\nfind better answers. This approach addresses the challenges of scoring\nvariability and non-independent distributions in mathematical reasoning tasks.\nThe framework has been tested on general and advanced benchmarks, showing\nsuperior performance in terms of search efficiency and problem-solving\ncapability compared to existing methods like ToT and rStar, particularly in\ncomplex Olympiad-level benchmarks, including GPQA, AIME24 and AMC23.",
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.02884v2",
    "published_date": "2024-10-03 18:12:29 UTC",
    "updated_date": "2024-11-21 07:07:59 UTC"
  },
  {
    "arxiv_id": "2410.02874v2",
    "title": "Real-World Cooking Robot System from Recipes Based on Food State Recognition Using Foundation Models and PDDL",
    "authors": [
      "Naoaki Kanazawa",
      "Kento Kawaharazuka",
      "Yoshiki Obinata",
      "Kei Okada",
      "Masayuki Inaba"
    ],
    "abstract": "Although there is a growing demand for cooking behaviours as one of the\nexpected tasks for robots, a series of cooking behaviours based on new recipe\ndescriptions by robots in the real world has not yet been realised. In this\nstudy, we propose a robot system that integrates real-world executable robot\ncooking behaviour planning using the Large Language Model (LLM) and classical\nplanning of PDDL descriptions, and food ingredient state recognition learning\nfrom a small number of data using the Vision-Language model (VLM). We succeeded\nin experiments in which PR2, a dual-armed wheeled robot, performed cooking from\narranged new recipes in a real-world environment, and confirmed the\neffectiveness of the proposed system.",
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "primary_category": "cs.RO",
    "comment": "Accepted at Advanced Robotics, website -\n  https://kanazawanaoaki.github.io/cook-from-recipe-pddl/",
    "pdf_url": "http://arxiv.org/pdf/2410.02874v2",
    "published_date": "2024-10-03 18:02:56 UTC",
    "updated_date": "2024-10-07 01:39:25 UTC"
  },
  {
    "arxiv_id": "2410.02763v1",
    "title": "Vinoground: Scrutinizing LMMs over Dense Temporal Reasoning with Short Videos",
    "authors": [
      "Jianrui Zhang",
      "Mu Cai",
      "Yong Jae Lee"
    ],
    "abstract": "There has been growing sentiment recently that modern large multimodal models\n(LMMs) have addressed most of the key challenges related to short video\ncomprehension. As a result, both academia and industry are gradually shifting\ntheir attention towards the more complex challenges posed by understanding\nlong-form videos. However, is this really the case? Our studies indicate that\nLMMs still lack many fundamental reasoning capabilities even when dealing with\nshort videos. We introduce Vinoground, a temporal counterfactual LMM evaluation\nbenchmark encompassing 1000 short and natural video-caption pairs. We\ndemonstrate that existing LMMs severely struggle to distinguish temporal\ndifferences between different actions and object transformations. For example,\nthe best model GPT-4o only obtains ~50% on our text and video scores, showing a\nlarge gap compared to the human baseline of ~90%. All open-source multimodal\nmodels and CLIP-based models perform much worse, producing mostly random chance\nperformance. Through this work, we shed light onto the fact that temporal\nreasoning in short videos is a problem yet to be fully solved. The dataset and\nevaluation code are available at https://vinoground.github.io.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "Project Page: https://vinoground.github.io",
    "pdf_url": "http://arxiv.org/pdf/2410.02763v1",
    "published_date": "2024-10-03 17:59:58 UTC",
    "updated_date": "2024-10-03 17:59:58 UTC"
  },
  {
    "arxiv_id": "2410.02761v4",
    "title": "FakeShield: Explainable Image Forgery Detection and Localization via Multi-modal Large Language Models",
    "authors": [
      "Zhipei Xu",
      "Xuanyu Zhang",
      "Runyi Li",
      "Zecheng Tang",
      "Qing Huang",
      "Jian Zhang"
    ],
    "abstract": "The rapid development of generative AI is a double-edged sword, which not\nonly facilitates content creation but also makes image manipulation easier and\nmore difficult to detect. Although current image forgery detection and\nlocalization (IFDL) methods are generally effective, they tend to face two\nchallenges: \\textbf{1)} black-box nature with unknown detection principle,\n\\textbf{2)} limited generalization across diverse tampering methods (e.g.,\nPhotoshop, DeepFake, AIGC-Editing). To address these issues, we propose the\nexplainable IFDL task and design FakeShield, a multi-modal framework capable of\nevaluating image authenticity, generating tampered region masks, and providing\na judgment basis based on pixel-level and image-level tampering clues.\nAdditionally, we leverage GPT-4o to enhance existing IFDL datasets, creating\nthe Multi-Modal Tamper Description dataSet (MMTD-Set) for training FakeShield's\ntampering analysis capabilities. Meanwhile, we incorporate a Domain Tag-guided\nExplainable Forgery Detection Module (DTE-FDM) and a Multi-modal Forgery\nLocalization Module (MFLM) to address various types of tamper detection\ninterpretation and achieve forgery localization guided by detailed textual\ndescriptions. Extensive experiments demonstrate that FakeShield effectively\ndetects and localizes various tampering techniques, offering an explainable and\nsuperior solution compared to previous IFDL methods. The code is available at\nhttps://github.com/zhipeixu/FakeShield.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "Accepted by ICLR 2025",
    "pdf_url": "http://arxiv.org/pdf/2410.02761v4",
    "published_date": "2024-10-03 17:59:34 UTC",
    "updated_date": "2025-04-12 08:08:04 UTC"
  },
  {
    "arxiv_id": "2410.05295v4",
    "title": "AutoDAN-Turbo: A Lifelong Agent for Strategy Self-Exploration to Jailbreak LLMs",
    "authors": [
      "Xiaogeng Liu",
      "Peiran Li",
      "Edward Suh",
      "Yevgeniy Vorobeychik",
      "Zhuoqing Mao",
      "Somesh Jha",
      "Patrick McDaniel",
      "Huan Sun",
      "Bo Li",
      "Chaowei Xiao"
    ],
    "abstract": "In this paper, we propose AutoDAN-Turbo, a black-box jailbreak method that\ncan automatically discover as many jailbreak strategies as possible from\nscratch, without any human intervention or predefined scopes (e.g., specified\ncandidate strategies), and use them for red-teaming. As a result, AutoDAN-Turbo\ncan significantly outperform baseline methods, achieving a 74.3% higher average\nattack success rate on public benchmarks. Notably, AutoDAN-Turbo achieves an\n88.5 attack success rate on GPT-4-1106-turbo. In addition, AutoDAN-Turbo is a\nunified framework that can incorporate existing human-designed jailbreak\nstrategies in a plug-and-play manner. By integrating human-designed strategies,\nAutoDAN-Turbo can even achieve a higher attack success rate of 93.4 on\nGPT-4-1106-turbo.",
    "categories": [
      "cs.CR",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CR",
    "comment": "ICLR 2025 Spotlight. Project Page:\n  https://autodans.github.io/AutoDAN-Turbo Code:\n  https://github.com/SaFoLab-WISC/AutoDAN-Turbo",
    "pdf_url": "http://arxiv.org/pdf/2410.05295v4",
    "published_date": "2024-10-03 17:59:01 UTC",
    "updated_date": "2025-04-22 05:20:39 UTC"
  },
  {
    "arxiv_id": "2410.02748v3",
    "title": "CriSPO: Multi-Aspect Critique-Suggestion-guided Automatic Prompt Optimization for Text Generation",
    "authors": [
      "Han He",
      "Qianchu Liu",
      "Lei Xu",
      "Chaitanya Shivade",
      "Yi Zhang",
      "Sundararajan Srinivasan",
      "Katrin Kirchhoff"
    ],
    "abstract": "Existing automatic prompt engineering methods are typically designed for\ndiscriminative tasks, where new task prompts are iteratively refined with\nlimited feedback from a single metric reflecting a single aspect. However,\nthese approaches are suboptimal for generative tasks, which require more\nnuanced guidance beyond a single numeric metric to improve the prompt and\noptimize multiple aspects of the generated text. To address these challenges,\nwe propose a novel multi-aspect Critique-Suggestion-guided automatic Prompt\nOptimization (CriSPO) approach. CriSPO introduces a critique-suggestion module\nas its core component. This module spontaneously discovers aspects, and\ncompares generated and reference texts across these aspects, providing specific\nsuggestions for prompt modification. These clear critiques and actionable\nsuggestions guide a receptive optimizer module to make more substantial\nchanges, exploring a broader and more effective search space. To further\nimprove CriSPO with multi-metric optimization, we introduce an Automatic Suffix\nTuning (AST) extension to enhance the performance of task prompts across\nmultiple metrics. We evaluate CriSPO on 4 state-of-the-art LLMs across 4\nsummarization and 5 QA datasets. Extensive experiments show 3-4% ROUGE score\nimprovement on summarization and substantial improvement of various metrics on\nQA. Code available at https://github.com/amazon-science/crispo",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted to AAAI-2025",
    "pdf_url": "http://arxiv.org/pdf/2410.02748v3",
    "published_date": "2024-10-03 17:57:01 UTC",
    "updated_date": "2025-01-14 17:20:04 UTC"
  },
  {
    "arxiv_id": "2410.02744v1",
    "title": "Neutral residues: revisiting adapters for model extension",
    "authors": [
      "Franck Signe Talla",
      "Herve Jegou",
      "Edouard Grave"
    ],
    "abstract": "We address the problem of extending a pretrained large language model to a\nnew domain that was not seen at training time, like adding a language for which\nthe original model has seen no or little training data. Popular solutions like\nfine-tuning or low-rank adaptation are successful at domain adaptation, but\nformally they do not add any extra capacity and degrade the performance in the\noriginal domain.\n  Our paper analyzes this extension problem under three angles: data,\narchitecture and training procedure, which are advantageously considered\njointly. In particular, we improve adapters and make it possible to learn an\nentire new language while ensuring that the output of the neural network is\nalmost unchanged in the original domain. For this purpose, we modify the new\nresidual blocks in a way that leads each new residual block to output\nnear-zeros in the original domain.\n  This solution of neutral residues, which borrows architectural components\nfrom mixture of experts, is effective: with only 20% extra learnable weights\ncompared to an original model trained on English, we get results that are\nsignificantly better than concurrent approaches (fine-tuning, low-rank or\nvanilla adapters) in terms of the trade-off between learning a new language and\nnot forgetting English.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.02744v1",
    "published_date": "2024-10-03 17:55:17 UTC",
    "updated_date": "2024-10-03 17:55:17 UTC"
  },
  {
    "arxiv_id": "2410.02741v2",
    "title": "Salient Information Prompting to Steer Content in Prompt-based Abstractive Summarization",
    "authors": [
      "Lei Xu",
      "Mohammed Asad Karim",
      "Saket Dingliwal",
      "Aparna Elangovan"
    ],
    "abstract": "Large language models (LLMs) can generate fluent summaries across domains\nusing prompting techniques, reducing the need to train models for summarization\napplications. However, crafting effective prompts that guide LLMs to generate\nsummaries with the appropriate level of detail and writing style remains a\nchallenge. In this paper, we explore the use of salient information extracted\nfrom the source document to enhance summarization prompts. We show that adding\nkeyphrases in prompts can improve ROUGE F1 and recall, making the generated\nsummaries more similar to the reference and more complete. The number of\nkeyphrases can control the precision-recall trade-off. Furthermore, our\nanalysis reveals that incorporating phrase-level salient information is\nsuperior to word- or sentence-level. However, the impact on hallucination is\nnot universally positive across LLMs. To conduct this analysis, we introduce\nKeyphrase Signal Extractor (SigExt), a lightweight model that can be finetuned\nto extract salient keyphrases. By using SigExt, we achieve consistent ROUGE\nimprovements across datasets and open-weight and proprietary LLMs without any\nLLM customization. Our findings provide insights into leveraging salient\ninformation in building prompt-based summarization systems. We release our code\nat \\url{https://github.com/amazon-science/SigExt}",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted to EMNLP 2024 Industry Track. Code available at\n  https://github.com/amazon-science/SigExt",
    "pdf_url": "http://arxiv.org/pdf/2410.02741v2",
    "published_date": "2024-10-03 17:54:56 UTC",
    "updated_date": "2024-12-02 21:06:29 UTC"
  },
  {
    "arxiv_id": "2410.02740v1",
    "title": "Revisit Large-Scale Image-Caption Data in Pre-training Multimodal Foundation Models",
    "authors": [
      "Zhengfeng Lai",
      "Vasileios Saveris",
      "Chen Chen",
      "Hong-You Chen",
      "Haotian Zhang",
      "Bowen Zhang",
      "Juan Lao Tebar",
      "Wenze Hu",
      "Zhe Gan",
      "Peter Grasch",
      "Meng Cao",
      "Yinfei Yang"
    ],
    "abstract": "Recent advancements in multimodal models highlight the value of rewritten\ncaptions for improving performance, yet key challenges remain. For example,\nwhile synthetic captions often provide superior quality and image-text\nalignment, it is not clear whether they can fully replace AltTexts: the role of\nsynthetic captions and their interaction with original web-crawled AltTexts in\npre-training is still not well understood. Moreover, different multimodal\nfoundation models may have unique preferences for specific caption formats, but\nefforts to identify the optimal captions for each model remain limited. In this\nwork, we propose a novel, controllable, and scalable captioning pipeline\ndesigned to generate diverse caption formats tailored to various multimodal\nmodels. By examining Short Synthetic Captions (SSC) towards Dense Synthetic\nCaptions (DSC+) as case studies, we systematically explore their effects and\ninteractions with AltTexts across models such as CLIP, multimodal LLMs, and\ndiffusion models. Our findings reveal that a hybrid approach that keeps both\nsynthetic captions and AltTexts can outperform the use of synthetic captions\nalone, improving both alignment and performance, with each model demonstrating\npreferences for particular caption formats. This comprehensive analysis\nprovides valuable insights into optimizing captioning strategies, thereby\nadvancing the pre-training of multimodal foundation models.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "CV/ML",
    "pdf_url": "http://arxiv.org/pdf/2410.02740v1",
    "published_date": "2024-10-03 17:54:52 UTC",
    "updated_date": "2024-10-03 17:54:52 UTC"
  },
  {
    "arxiv_id": "2410.02736v2",
    "title": "Justice or Prejudice? Quantifying Biases in LLM-as-a-Judge",
    "authors": [
      "Jiayi Ye",
      "Yanbo Wang",
      "Yue Huang",
      "Dongping Chen",
      "Qihui Zhang",
      "Nuno Moniz",
      "Tian Gao",
      "Werner Geyer",
      "Chao Huang",
      "Pin-Yu Chen",
      "Nitesh V Chawla",
      "Xiangliang Zhang"
    ],
    "abstract": "LLM-as-a-Judge has been widely utilized as an evaluation method in various\nbenchmarks and served as supervised rewards in model training. However, despite\ntheir excellence in many domains, potential issues are under-explored,\nundermining their reliability and the scope of their utility. Therefore, we\nidentify 12 key potential biases and propose a new automated bias\nquantification framework-CALM-which systematically quantifies and analyzes each\ntype of bias in LLM-as-a-Judge by using automated and principle-guided\nmodification. Our experiments cover multiple popular language models, and the\nresults indicate that while advanced models have achieved commendable overall\nperformance, significant biases persist in certain specific tasks. Empirical\nresults suggest that there remains room for improvement in the reliability of\nLLM-as-a-Judge. Moreover, we also discuss the explicit and implicit influence\nof these biases and give some suggestions for the reliable application of\nLLM-as-a-Judge. Our work highlights the need for stakeholders to address these\nissues and remind users to exercise caution in LLM-as-a-Judge applications.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.02736v2",
    "published_date": "2024-10-03 17:53:30 UTC",
    "updated_date": "2024-10-04 03:57:47 UTC"
  },
  {
    "arxiv_id": "2410.02732v1",
    "title": "Custom Non-Linear Model Predictive Control for Obstacle Avoidance in Indoor and Outdoor Environments",
    "authors": [
      "Lara Laban",
      "Mariusz Wzorek",
      "Piotr Rudol",
      "Tommy Persson"
    ],
    "abstract": "Navigating complex environments requires Unmanned Aerial Vehicles (UAVs) and\nautonomous systems to perform trajectory tracking and obstacle avoidance in\nreal-time. While many control strategies have effectively utilized linear\napproximations, addressing the non-linear dynamics of UAV, especially in\nobstacle-dense environments, remains a key challenge that requires further\nresearch. This paper introduces a Non-linear Model Predictive Control (NMPC)\nframework for the DJI Matrice 100, addressing these challenges by using a\ndynamic model and B-spline interpolation for smooth reference trajectories,\nensuring minimal deviation while respecting safety constraints. The framework\nsupports various trajectory types and employs a penalty-based cost function for\ncontrol accuracy in tight maneuvers. The framework utilizes CasADi for\nefficient real-time optimization, enabling the UAV to maintain robust operation\neven under tight computational constraints. Simulation and real-world indoor\nand outdoor experiments demonstrated the NMPC ability to adapt to disturbances,\nresulting in smooth, collision-free navigation.",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.AR",
      "cs.CE",
      "cs.SY",
      "eess.SY",
      "93C85 (Primary), 93B52, 68T40 (Secondary)",
      "I.2.9; I.2.8; I.6.3; I.6.5; I.6.8; C.3; C.4"
    ],
    "primary_category": "cs.RO",
    "comment": "This manuscript has 7 pages and 8 figures, detailing NMPC for UAV\n  obstacle avoidance using DJI UAVs. It features simulations, experimental\n  results, and uses CasADi for optimization with ROS integration. Code and\n  media at https://github.com/larasupernovae/nmpc_flash_multi_obstacle",
    "pdf_url": "http://arxiv.org/pdf/2410.02732v1",
    "published_date": "2024-10-03 17:50:19 UTC",
    "updated_date": "2024-10-03 17:50:19 UTC"
  },
  {
    "arxiv_id": "2410.02729v2",
    "title": "Unified Multimodal Interleaved Document Representation for Retrieval",
    "authors": [
      "Jaewoo Lee",
      "Joonho Ko",
      "Jinheon Baek",
      "Soyeong Jeong",
      "Sung Ju Hwang"
    ],
    "abstract": "Information Retrieval (IR) methods aim to identify documents relevant to a\nquery, which have been widely applied in various natural language tasks.\nHowever, existing approaches typically consider only the textual content within\ndocuments, overlooking the fact that documents can contain multiple modalities,\nincluding images and tables. Also, they often segment each long document into\nmultiple discrete passages for embedding, which prevents them from capturing\nthe overall document context and interactions between paragraphs. To address\nthese two challenges, we propose a method that holistically embeds documents\ninterleaved with multiple modalities by leveraging the capability of recent\nvision-language models that enable the processing and integration of text,\nimages, and tables into a unified format and representation. Moreover, to\nmitigate the information loss from segmenting documents into passages, instead\nof representing and retrieving passages individually, we further merge the\nrepresentations of segmented passages into one single document representation,\nwhile we additionally introduce a reranking strategy to decouple and identify\nthe relevant passage within the document if necessary. Then, through extensive\nexperiments on diverse IR scenarios considering both the textual and multimodal\nqueries, we show that our approach substantially outperforms relevant\nbaselines, thanks to the consideration of the multimodal information within\ndocuments.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.IR"
    ],
    "primary_category": "cs.CL",
    "comment": "Preprint",
    "pdf_url": "http://arxiv.org/pdf/2410.02729v2",
    "published_date": "2024-10-03 17:49:09 UTC",
    "updated_date": "2024-12-16 15:11:11 UTC"
  },
  {
    "arxiv_id": "2410.02725v1",
    "title": "Adaptive Inference-Time Compute: LLMs Can Predict if They Can Do Better, Even Mid-Generation",
    "authors": [
      "Rohin Manvi",
      "Anikait Singh",
      "Stefano Ermon"
    ],
    "abstract": "Inference-time computation is a powerful paradigm to enhance the performance\nof large language models (LLMs), with Best-of-N sampling being a widely used\ntechnique. However, this method is computationally expensive, requiring both\n(1) an external reward model and (2) the generation of multiple samples. In\nthis work, we introduce a new generative self-evaluation scheme designed to\nadaptively reduce the number of generated samples while maintaining or even\nimproving performance. We use a generative reward model formulation, allowing\nthe LLM to predict mid-generation the probability that restarting the\ngeneration will yield a better response. These predictions are obtained without\nan external reward model and can be used to decide whether or not to generate\nmore samples, prune unpromising samples early on, or to pick the best sample.\nThis capability is very inexpensive as it involves generating a single\npredefined token. Trained using a dataset constructed with real unfiltered\nLMSYS user prompts, Llama 3.1 8B's win rate against GPT-4 on AlpacaEval\nincreases from 21% to 34% with 16 samples and math performance on GSM8K\nimproves from 84% to 91%. By sampling only when the LLM determines that it is\nbeneficial to do so and adaptively adjusting temperature annealing, we\ndemonstrate that 74% of the improvement from using 16 samples can be achieved\nwith only 1.2 samples on average. We further demonstrate that 50-75% of samples\ncan be pruned early in generation with minimal degradation in performance.\nOverall, our methods enable more efficient and scalable compute utilization\nduring inference for LLMs.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.02725v1",
    "published_date": "2024-10-03 17:47:29 UTC",
    "updated_date": "2024-10-03 17:47:29 UTC"
  },
  {
    "arxiv_id": "2410.02724v2",
    "title": "Large Language Models as Markov Chains",
    "authors": [
      "Oussama Zekri",
      "Ambroise Odonnat",
      "Abdelhakim Benechehab",
      "Linus Bleistein",
      "Nicolas Boullé",
      "Ievgen Redko"
    ],
    "abstract": "Large language models (LLMs) are remarkably efficient across a wide range of\nnatural language processing tasks and well beyond them. However, a\ncomprehensive theoretical analysis of the LLMs' generalization capabilities\nremains elusive. In our paper, we approach this task by drawing an equivalence\nbetween autoregressive transformer-based language models and Markov chains\ndefined on a finite state space. This allows us to study the multi-step\ninference mechanism of LLMs from first principles. We relate the obtained\nresults to the pathological behavior observed with LLMs such as repetitions and\nincoherent replies with high temperature. Finally, we leverage the proposed\nformalization to derive pre-training and in-context learning generalization\nbounds for LLMs under realistic data and model assumptions. Experiments with\nthe most recent Llama and Gemma herds of models show that our theory correctly\ncaptures their behavior in practice.",
    "categories": [
      "stat.ML",
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "primary_category": "stat.ML",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.02724v2",
    "published_date": "2024-10-03 17:45:31 UTC",
    "updated_date": "2025-02-02 15:57:01 UTC"
  },
  {
    "arxiv_id": "2410.12836v2",
    "title": "EditRoom: LLM-parameterized Graph Diffusion for Composable 3D Room Layout Editing",
    "authors": [
      "Kaizhi Zheng",
      "Xiaotong Chen",
      "Xuehai He",
      "Jing Gu",
      "Linjie Li",
      "Zhengyuan Yang",
      "Kevin Lin",
      "Jianfeng Wang",
      "Lijuan Wang",
      "Xin Eric Wang"
    ],
    "abstract": "Given the steep learning curve of professional 3D software and the\ntime-consuming process of managing large 3D assets, language-guided 3D scene\nediting has significant potential in fields such as virtual reality, augmented\nreality, and gaming. However, recent approaches to language-guided 3D scene\nediting either require manual interventions or focus only on appearance\nmodifications without supporting comprehensive scene layout changes. In\nresponse, we propose EditRoom, a unified framework capable of executing a\nvariety of layout edits through natural language commands, without requiring\nmanual intervention. Specifically, EditRoom leverages Large Language Models\n(LLMs) for command planning and generates target scenes using a diffusion-based\nmethod, enabling six types of edits: rotate, translate, scale, replace, add,\nand remove. To address the lack of data for language-guided 3D scene editing,\nwe have developed an automatic pipeline to augment existing 3D scene synthesis\ndatasets and introduced EditRoom-DB, a large-scale dataset with 83k editing\npairs, for training and evaluation. Our experiments demonstrate that our\napproach consistently outperforms other baselines across all metrics,\nindicating higher accuracy and coherence in language-guided scene layout\nediting.",
    "categories": [
      "cs.GR",
      "cs.AI",
      "cs.CV",
      "cs.HC"
    ],
    "primary_category": "cs.GR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.12836v2",
    "published_date": "2024-10-03 17:42:24 UTC",
    "updated_date": "2025-04-01 23:38:07 UTC"
  },
  {
    "arxiv_id": "2410.02721v1",
    "title": "Domain-Specific Retrieval-Augmented Generation Using Vector Stores, Knowledge Graphs, and Tensor Factorization",
    "authors": [
      "Ryan C. Barron",
      "Ves Grantcharov",
      "Selma Wanna",
      "Maksim E. Eren",
      "Manish Bhattarai",
      "Nicholas Solovyev",
      "George Tompkins",
      "Charles Nicholas",
      "Kim Ø. Rasmussen",
      "Cynthia Matuszek",
      "Boian S. Alexandrov"
    ],
    "abstract": "Large Language Models (LLMs) are pre-trained on large-scale corpora and excel\nin numerous general natural language processing (NLP) tasks, such as question\nanswering (QA). Despite their advanced language capabilities, when it comes to\ndomain-specific and knowledge-intensive tasks, LLMs suffer from hallucinations,\nknowledge cut-offs, and lack of knowledge attributions. Additionally, fine\ntuning LLMs' intrinsic knowledge to highly specific domains is an expensive and\ntime consuming process. The retrieval-augmented generation (RAG) process has\nrecently emerged as a method capable of optimization of LLM responses, by\nreferencing them to a predetermined ontology. It was shown that using a\nKnowledge Graph (KG) ontology for RAG improves the QA accuracy, by taking into\naccount relevant sub-graphs that preserve the information in a structured\nmanner. In this paper, we introduce SMART-SLIC, a highly domain-specific LLM\nframework, that integrates RAG with KG and a vector store (VS) that store\nfactual domain specific information. Importantly, to avoid hallucinations in\nthe KG, we build these highly domain-specific KGs and VSs without the use of\nLLMs, but via NLP, data mining, and nonnegative tensor factorization with\nautomatic model selection. Pairing our RAG with a domain-specific: (i) KG\n(containing structured information), and (ii) VS (containing unstructured\ninformation) enables the development of domain-specific chat-bots that\nattribute the source of information, mitigate hallucinations, lessen the need\nfor fine-tuning, and excel in highly domain-specific question answering tasks.\nWe pair SMART-SLIC with chain-of-thought prompting agents. The framework is\ndesigned to be generalizable to adapt to any specific or specialized domain. In\nthis paper, we demonstrate the question answering capabilities of our framework\non a corpus of scientific publications on malware analysis and anomaly\ndetection.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.IR",
      "cs.SE"
    ],
    "primary_category": "cs.CL",
    "comment": "9 pages 7 figures, 1 table, 1 cypher code Accepted to ICMLA 2024",
    "pdf_url": "http://arxiv.org/pdf/2410.02721v1",
    "published_date": "2024-10-03 17:40:55 UTC",
    "updated_date": "2024-10-03 17:40:55 UTC"
  },
  {
    "arxiv_id": "2410.02720v2",
    "title": "Curvature Diversity-Driven Deformation and Domain Alignment for Point Cloud",
    "authors": [
      "Mengxi Wu",
      "Hao Huang",
      "Yi Fang",
      "Mohammad Rostami"
    ],
    "abstract": "Unsupervised Domain Adaptation (UDA) is crucial for reducing the need for\nextensive manual data annotation when training deep networks on point cloud\ndata. A significant challenge of UDA lies in effectively bridging the domain\ngap. To tackle this challenge, we propose \\textbf{C}urvature\n\\textbf{D}iversity-Driven \\textbf{N}uclear-Norm Wasserstein \\textbf{D}omain\nAlignment (CDND). Our approach first introduces a \\textit{\\textbf{Curv}ature\nDiversity-driven Deformation \\textbf{Rec}onstruction (CurvRec)} task, which\neffectively mitigates the gap between the source and target domains by enabling\nthe model to extract salient features from semantically rich regions of a given\npoint cloud. We then propose \\textit{\\textbf{D}eformation-based\n\\textbf{N}uclear-norm \\textbf{W}asserstein \\textbf{D}iscrepancy (D-NWD)}, which\napplies the Nuclear-norm Wasserstein Discrepancy to both \\textit{deformed and\noriginal} data samples to align the source and target domains. Furthermore, we\ncontribute a theoretical justification for the effectiveness of D-NWD in\ndistribution alignment and demonstrate that it is \\textit{generic} enough to be\napplied to \\textbf{any} deformations. To validate our method, we conduct\nextensive experiments on two public domain adaptation datasets for point cloud\nclassification and segmentation tasks. Empirical experiment results show that\nour CDND achieves state-of-the-art performance by a noticeable margin over\nexisting approaches.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.02720v2",
    "published_date": "2024-10-03 17:39:55 UTC",
    "updated_date": "2024-10-05 03:11:37 UTC"
  },
  {
    "arxiv_id": "2410.02717v1",
    "title": "Measurements with Noise: Bayesian Optimization for Co-optimizing Noise and Property Discovery in Automated Experiments",
    "authors": [
      "Boris N. Slautin",
      "Yu Liu",
      "Jan Dec",
      "Vladimir V. Shvartsman",
      "Doru C. Lupascu",
      "Maxim Ziatdinov",
      "Sergei V. Kalinin"
    ],
    "abstract": "We have developed a Bayesian optimization (BO) workflow that integrates\nintra-step noise optimization into automated experimental cycles. Traditional\nBO approaches in automated experiments focus on optimizing experimental\ntrajectories but often overlook the impact of measurement noise on data quality\nand cost. Our proposed framework simultaneously optimizes both the target\nproperty and the associated measurement noise by introducing time as an\nadditional input parameter, thereby balancing the signal-to-noise ratio and\nexperimental duration. Two approaches are explored: a reward-driven noise\noptimization and a double-optimization acquisition function, both enhancing the\nefficiency of automated workflows by considering noise and cost within the\noptimization process. We validate our method through simulations and real-world\nexperiments using Piezoresponse Force Microscopy (PFM), demonstrating the\nsuccessful optimization of measurement duration and property exploration. Our\napproach offers a scalable solution for optimizing multiple variables in\nautomated experimental workflows, improving data quality, and reducing resource\nexpenditure in materials science and beyond.",
    "categories": [
      "cond-mat.mtrl-sci",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cond-mat.mtrl-sci",
    "comment": "22 pages, 9 figures",
    "pdf_url": "http://arxiv.org/pdf/2410.02717v1",
    "published_date": "2024-10-03 17:38:43 UTC",
    "updated_date": "2024-10-03 17:38:43 UTC"
  },
  {
    "arxiv_id": "2410.02710v1",
    "title": "SteerDiff: Steering towards Safe Text-to-Image Diffusion Models",
    "authors": [
      "Hongxiang Zhang",
      "Yifeng He",
      "Hao Chen"
    ],
    "abstract": "Text-to-image (T2I) diffusion models have drawn attention for their ability\nto generate high-quality images with precise text alignment. However, these\nmodels can also be misused to produce inappropriate content. Existing safety\nmeasures, which typically rely on text classifiers or ControlNet-like\napproaches, are often insufficient. Traditional text classifiers rely on\nlarge-scale labeled datasets and can be easily bypassed by rephrasing. As\ndiffusion models continue to scale, fine-tuning these safeguards becomes\nincreasingly challenging and lacks flexibility. Recent red-teaming attack\nresearches further underscore the need for a new paradigm to prevent the\ngeneration of inappropriate content. In this paper, we introduce SteerDiff, a\nlightweight adaptor module designed to act as an intermediary between user\ninput and the diffusion model, ensuring that generated images adhere to ethical\nand safety standards with little to no impact on usability. SteerDiff\nidentifies and manipulates inappropriate concepts within the text embedding\nspace to guide the model away from harmful outputs. We conduct extensive\nexperiments across various concept unlearning tasks to evaluate the\neffectiveness of our approach. Furthermore, we benchmark SteerDiff against\nmultiple red-teaming strategies to assess its robustness. Finally, we explore\nthe potential of SteerDiff for concept forgetting tasks, demonstrating its\nversatility in text-conditioned image generation.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CR"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.02710v1",
    "published_date": "2024-10-03 17:34:55 UTC",
    "updated_date": "2024-10-03 17:34:55 UTC"
  },
  {
    "arxiv_id": "2410.02707v4",
    "title": "LLMs Know More Than They Show: On the Intrinsic Representation of LLM Hallucinations",
    "authors": [
      "Hadas Orgad",
      "Michael Toker",
      "Zorik Gekhman",
      "Roi Reichart",
      "Idan Szpektor",
      "Hadas Kotek",
      "Yonatan Belinkov"
    ],
    "abstract": "Large language models (LLMs) often produce errors, including factual\ninaccuracies, biases, and reasoning failures, collectively referred to as\n\"hallucinations\". Recent studies have demonstrated that LLMs' internal states\nencode information regarding the truthfulness of their outputs, and that this\ninformation can be utilized to detect errors. In this work, we show that the\ninternal representations of LLMs encode much more information about\ntruthfulness than previously recognized. We first discover that the\ntruthfulness information is concentrated in specific tokens, and leveraging\nthis property significantly enhances error detection performance. Yet, we show\nthat such error detectors fail to generalize across datasets, implying that --\ncontrary to prior claims -- truthfulness encoding is not universal but rather\nmultifaceted. Next, we show that internal representations can also be used for\npredicting the types of errors the model is likely to make, facilitating the\ndevelopment of tailored mitigation strategies. Lastly, we reveal a discrepancy\nbetween LLMs' internal encoding and external behavior: they may encode the\ncorrect answer, yet consistently generate an incorrect one. Taken together,\nthese insights deepen our understanding of LLM errors from the model's internal\nperspective, which can guide future research on enhancing error analysis and\nmitigation.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "68T50",
      "I.2.7"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.02707v4",
    "published_date": "2024-10-03 17:31:31 UTC",
    "updated_date": "2025-05-18 11:18:56 UTC"
  },
  {
    "arxiv_id": "2410.02703v2",
    "title": "Selective Attention Improves Transformer",
    "authors": [
      "Yaniv Leviathan",
      "Matan Kalman",
      "Yossi Matias"
    ],
    "abstract": "Unneeded elements in the attention's context degrade performance. We\nintroduce Selective Attention, a simple parameter-free change to the standard\nattention mechanism which reduces attention to unneeded elements. Selective\nattention consistently improves language modeling and downstream task\nperformance in a variety of model sizes and context lengths. For example,\ntransformers trained with the language modeling objective on C4 with selective\nattention perform language modeling equivalently to standard transformers with\n~2X more heads and parameters in their attention modules. Selective attention\nalso allows decreasing the size of the attention's context buffer, leading to\nmeaningful reductions in the memory and compute requirements during inference.\nFor example, transformers trained on C4 with context sizes of 512, 1,024, and\n2,048 need 16X, 25X, and 47X less memory for their attention module,\nrespectively, when equipped with selective attention, as those without\nselective attention, with the same validation perplexity.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "ICLR 2025",
    "pdf_url": "http://arxiv.org/pdf/2410.02703v2",
    "published_date": "2024-10-03 17:27:30 UTC",
    "updated_date": "2025-04-24 02:44:54 UTC"
  },
  {
    "arxiv_id": "2410.02694v3",
    "title": "HELMET: How to Evaluate Long-Context Language Models Effectively and Thoroughly",
    "authors": [
      "Howard Yen",
      "Tianyu Gao",
      "Minmin Hou",
      "Ke Ding",
      "Daniel Fleischer",
      "Peter Izsak",
      "Moshe Wasserblat",
      "Danqi Chen"
    ],
    "abstract": "Many benchmarks exist for evaluating long-context language models (LCLMs),\nyet developers often rely on synthetic tasks such as needle-in-a-haystack\n(NIAH) or an arbitrary subset of tasks. However, it remains unclear whether\nthese benchmarks reflect the diverse downstream applications of LCLMs, and such\ninconsistencies further complicate model comparison. We investigate the\nunderlying reasons behind these practices and find that existing benchmarks\noften provide noisy signals due to limited coverage of applications,\ninsufficient context lengths, unreliable metrics, and incompatibility with base\nmodels. In this work, we introduce HELMET (How to Evaluate Long-context Models\nEffectively and Thoroughly), a comprehensive benchmark encompassing seven\ndiverse, application-centric categories. We also address several issues in\nprevious benchmarks by adding controllable lengths up to 128K tokens,\nmodel-based evaluation for reliable metrics, and few-shot prompting for\nrobustly evaluating base models. Consequently, we demonstrate that HELMET\noffers more reliable and consistent rankings of frontier LCLMs. Through a\ncomprehensive study of 59 LCLMs, we find that (1) synthetic tasks like NIAH do\nnot reliably predict downstream performance; (2) the diverse categories in\nHELMET exhibit distinct trends and low correlations with each other; and (3)\nwhile most LCLMs achieve perfect NIAH scores, open-source models significantly\nlag behind closed ones when tasks require full-context reasoning or following\ncomplex instructions -- the gap widens as length increases. Finally, we\nrecommend using our RAG tasks for fast model development, as they are easy to\nrun and better predict other downstream performance; ultimately, we advocate\nfor a holistic evaluation across diverse tasks.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "ICLR 2025. Project page: https://princeton-nlp.github.io/HELMET/",
    "pdf_url": "http://arxiv.org/pdf/2410.02694v3",
    "published_date": "2024-10-03 17:20:11 UTC",
    "updated_date": "2025-03-06 18:41:54 UTC"
  },
  {
    "arxiv_id": "2410.02693v1",
    "title": "Discovering Clues of Spoofed LM Watermarks",
    "authors": [
      "Thibaud Gloaguen",
      "Nikola Jovanović",
      "Robin Staab",
      "Martin Vechev"
    ],
    "abstract": "LLM watermarks stand out as a promising way to attribute ownership of\nLLM-generated text. One threat to watermark credibility comes from spoofing\nattacks, where an unauthorized third party forges the watermark, enabling it to\nfalsely attribute arbitrary texts to a particular LLM. While recent works have\ndemonstrated that state-of-the-art schemes are in fact vulnerable to spoofing,\nthey lack deeper qualitative analysis of the texts produced by spoofing\nmethods. In this work, we for the first time reveal that there are observable\ndifferences between genuine and spoofed watermark texts. Namely, we show that\nregardless of their underlying approach, all current spoofing methods\nconsistently leave observable artifacts in spoofed texts, indicative of\nwatermark forgery. We build upon these findings to propose rigorous statistical\ntests that reliably reveal the presence of such artifacts, effectively\ndiscovering that a watermark was spoofed. Our experimental evaluation shows\nhigh test power across all current spoofing methods, providing insights into\ntheir fundamental limitations, and suggesting a way to mitigate this threat.",
    "categories": [
      "cs.CR",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.02693v1",
    "published_date": "2024-10-03 17:18:37 UTC",
    "updated_date": "2024-10-03 17:18:37 UTC"
  },
  {
    "arxiv_id": "2410.02688v2",
    "title": "User-centric Immersive Communications in 6G: A Data-oriented Framework via Digital Twin",
    "authors": [
      "Conghao Zhou",
      "Shisheng Hu",
      "Jie Gao",
      "Xinyu Huang",
      "Weihua Zhuang",
      "Xuemin Shen"
    ],
    "abstract": "In this article, we present a novel user-centric service provision for\nimmersive communications (IC) in 6G to deal with the uncertainty of individual\nuser behaviors while satisfying unique requirements on the quality of\nmulti-sensory experience. To this end, we propose a data-oriented framework for\nnetwork resource management, featuring personalized data management that can\nsupport network modeling tailored to different user demands. Our framework\nleverages the digital twin (DT) technique as a key enabler. Particularly, a DT\nis established for each user, and the data attributes in the DT are customized\nbased on the characteristics of the user. The DT functions, corresponding to\nvarious data operations, are customized in the development, evaluation, and\nupdate of network models to meet unique user demands. A trace-driven case study\ndemonstrates the effectiveness of our framework in achieving user-centric IC\nand the significance of personalized data management in 6G.",
    "categories": [
      "cs.NI",
      "cs.AI"
    ],
    "primary_category": "cs.NI",
    "comment": "Accepted by IEEE Wireless Communications",
    "pdf_url": "http://arxiv.org/pdf/2410.02688v2",
    "published_date": "2024-10-03 17:15:53 UTC",
    "updated_date": "2025-03-12 18:52:20 UTC"
  },
  {
    "arxiv_id": "2410.02683v3",
    "title": "DailyDilemmas: Revealing Value Preferences of LLMs with Quandaries of Daily Life",
    "authors": [
      "Yu Ying Chiu",
      "Liwei Jiang",
      "Yejin Choi"
    ],
    "abstract": "As users increasingly seek guidance from LLMs for decision-making in daily\nlife, many of these decisions are not clear-cut and depend significantly on the\npersonal values and ethical standards of people. We present DailyDilemmas, a\ndataset of 1,360 moral dilemmas encountered in everyday life. Each dilemma\npresents two possible actions, along with affected parties and relevant human\nvalues for each action. Based on these dilemmas, we gather a repository of\nhuman values covering diverse everyday topics, such as interpersonal\nrelationships, workplace, and environmental issues. With DailyDilemmas, we\nevaluate LLMs on these dilemmas to determine what action they will choose and\nthe values represented by these action choices. Then, we analyze values through\nthe lens of five theoretical frameworks inspired by sociology, psychology, and\nphilosophy, including the World Values Survey, Moral Foundations Theory,\nMaslow's Hierarchy of Needs, Aristotle's Virtues, and Plutchik's Wheel of\nEmotions. For instance, we find LLMs are most aligned with self-expression over\nsurvival in World Values Survey and care over loyalty in Moral Foundations\nTheory. Interestingly, we find substantial preference differences in models for\nsome core values. For example, for truthfulness, Mixtral-8x7B neglects it by\n9.7% while GPT-4-turbo selects it by 9.4%. We also study the recent guidance\nreleased by OpenAI (ModelSpec), and Anthropic (Constitutional AI) to understand\nhow their designated principles reflect their models' actual value\nprioritization when facing nuanced moral reasoning in daily-life settings.\nFinally, we find that end users cannot effectively steer such prioritization\nusing system prompts.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted into ICLR 2025 (spotlight)",
    "pdf_url": "http://arxiv.org/pdf/2410.02683v3",
    "published_date": "2024-10-03 17:08:52 UTC",
    "updated_date": "2025-03-15 03:54:40 UTC"
  },
  {
    "arxiv_id": "2410.11855v1",
    "title": "Online Energy Optimization in GPUs: A Multi-Armed Bandit Approach",
    "authors": [
      "Xiongxiao Xu",
      "Solomon Abera Bekele",
      "Brice Videau",
      "Kai Shu"
    ],
    "abstract": "Energy consumption has become a critical design metric and a limiting factor\nin the development of future computing architectures, from small wearable\ndevices to large-scale leadership computing facilities. The predominant methods\nin energy management optimization are focused on CPUs. However, GPUs are\nincreasingly significant and account for the majority of energy consumption in\nheterogeneous high performance computing (HPC) systems. Moreover, they\ntypically rely on either purely offline training or a hybrid of offline and\nonline training, which are impractical and lead to energy loss during data\ncollection. Therefore, this paper studies a novel and practical online energy\noptimization problem for GPUs in HPC scenarios. The problem is challenging due\nto the inherent performance-energy trade-offs of GPUs, the exploration &\nexploitation dilemma across frequencies, and the lack of explicit performance\ncounters in GPUs. To address these challenges, we formulate the online energy\nconsumption optimization problem as a multi-armed bandit framework and develop\na novel bandit based framework EnergyUCB. EnergyUCB is designed to dynamically\nadjust GPU core frequencies in real-time, reducing energy consumption with\nminimal impact on performance. Specifically, the proposed framework EnergyUCB\n(1) balances the performance-energy trade-off in the reward function, (2)\neffectively navigates the exploration & exploitation dilemma when adjusting GPU\ncore frequencies online, and (3) leverages the ratio of GPU core utilization to\nuncore utilization as a real-time GPU performance metric. Experiments on a wide\nrange of real-world HPC benchmarks demonstrate that EnergyUCB can achieve\nsubstantial energy savings. The code of EnergyUCB is available at\nhttps://github.com/XiongxiaoXu/EnergyUCB-Bandit.",
    "categories": [
      "cs.DC",
      "cs.AI",
      "cs.AR",
      "cs.LG"
    ],
    "primary_category": "cs.DC",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.11855v1",
    "published_date": "2024-10-03 17:05:34 UTC",
    "updated_date": "2024-10-03 17:05:34 UTC"
  },
  {
    "arxiv_id": "2410.02678v1",
    "title": "Distilling an End-to-End Voice Assistant Without Instruction Training Data",
    "authors": [
      "William Held",
      "Ella Li",
      "Michael Ryan",
      "Weiyan Shi",
      "Yanzhe Zhang",
      "Diyi Yang"
    ],
    "abstract": "Voice assistants, such as Siri and Google Assistant, typically model audio\nand text separately, resulting in lost speech information and increased\ncomplexity. Recent efforts to address this with end-to-end Speech Large\nLanguage Models (LLMs) trained with supervised finetuning (SFT)\n  have led to models ``forgetting\" capabilities from text-only LLMs. Our work\nproposes an alternative paradigm for training Speech LLMs without instruction\ndata, using the response of a text-only LLM to transcripts as self-supervision.\nImportantly, this process can be performed without annotated responses. We show\nthat our Distilled Voice Assistant (DiVA) generalizes to Spoken Question\nAnswering, Classification, and Translation. Furthermore, we show that DiVA\nbetter meets user preferences, achieving a 72\\% win rate compared with\nstate-of-the-art models like Qwen 2 Audio, despite using $>$100x less training\ncompute.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.02678v1",
    "published_date": "2024-10-03 17:04:48 UTC",
    "updated_date": "2024-10-03 17:04:48 UTC"
  },
  {
    "arxiv_id": "2410.02677v1",
    "title": "CulturalBench: a Robust, Diverse and Challenging Benchmark on Measuring the (Lack of) Cultural Knowledge of LLMs",
    "authors": [
      "Yu Ying Chiu",
      "Liwei Jiang",
      "Bill Yuchen Lin",
      "Chan Young Park",
      "Shuyue Stella Li",
      "Sahithya Ravi",
      "Mehar Bhatia",
      "Maria Antoniak",
      "Yulia Tsvetkov",
      "Vered Shwartz",
      "Yejin Choi"
    ],
    "abstract": "To make large language models (LLMs) more helpful across diverse cultures, it\nis essential to have effective cultural knowledge benchmarks to measure and\ntrack our progress. Effective benchmarks need to be robust, diverse, and\nchallenging. We introduce CulturalBench: a set of 1,227 human-written and\nhuman-verified questions for effectively assessing LLMs' cultural knowledge,\ncovering 45 global regions including the underrepresented ones like Bangladesh,\nZimbabwe, and Peru. Questions - each verified by five independent annotators -\nspan 17 diverse topics ranging from food preferences to greeting etiquettes. We\nevaluate models on two setups: CulturalBench-Easy and CulturalBench-Hard which\nshare the same questions but asked differently. We find that LLMs are sensitive\nto such difference in setups (e.g., GPT-4o with 27.3% difference). Compared to\nhuman performance (92.6% accuracy), CulturalBench-Hard is more challenging for\nfrontier LLMs with the best performing model (GPT-4o) at only 61.5% and the\nworst (Llama3-8b) at 21.4%. Moreover, we find that LLMs often struggle with\ntricky questions that have multiple correct answers (e.g., What utensils do the\nChinese usually use?), revealing a tendency to converge to a single answer. Our\nresults also indicate that OpenAI GPT-4o substantially outperform other\nproprietary and open source models in questions related to all but one region\n(Oceania). Nonetheless, all models consistently underperform on questions\nrelated to South America and the Middle East.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "Preprint. Under review",
    "pdf_url": "http://arxiv.org/pdf/2410.02677v1",
    "published_date": "2024-10-03 17:04:31 UTC",
    "updated_date": "2024-10-03 17:04:31 UTC"
  },
  {
    "arxiv_id": "2410.02675v4",
    "title": "FAN: Fourier Analysis Networks",
    "authors": [
      "Yihong Dong",
      "Ge Li",
      "Yongding Tao",
      "Xue Jiang",
      "Kechi Zhang",
      "Jia Li",
      "Jinliang Deng",
      "Jing Su",
      "Jun Zhang",
      "Jingjing Xu"
    ],
    "abstract": "Despite the remarkable successes of general-purpose neural networks, such as\nMLPs and Transformers, we find that they exhibit notable shortcomings in\nmodeling and reasoning about periodic phenomena, achieving only marginal\nperformance within the training domain and failing to generalize effectively to\nout-of-domain (OOD) scenarios. Periodicity is ubiquitous throughout nature and\nscience. Therefore, neural networks should be equipped with the essential\nability to model and handle periodicity. In this work, we propose FAN, a novel\ngeneral-purpose neural network that offers broad applicability similar to MLP\nwhile effectively addressing periodicity modeling challenges. Periodicity is\nnaturally integrated into FAN's structure and computational processes by\nintroducing the Fourier Principle. Unlike existing Fourier-based networks,\nwhich possess particular periodicity modeling abilities but are typically\ndesigned for specific tasks, our approach maintains the general-purpose\nmodeling capability. Therefore, FAN can seamlessly replace MLP in various model\narchitectures with fewer parameters and FLOPs. Through extensive experiments,\nwe demonstrate the superiority of FAN in periodicity modeling tasks and the\neffectiveness and generalizability of FAN across a range of real-world tasks,\ne.g., symbolic formula representation, time series forecasting, language\nmodeling, and image recognition.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.02675v4",
    "published_date": "2024-10-03 17:02:21 UTC",
    "updated_date": "2025-04-02 04:15:51 UTC"
  },
  {
    "arxiv_id": "2410.02671v3",
    "title": "Unsupervised Point Cloud Completion through Unbalanced Optimal Transport",
    "authors": [
      "Taekyung Lee",
      "Jaemoo Choi",
      "Jaewoong Choi",
      "Myungjoo Kang"
    ],
    "abstract": "Unpaired point cloud completion explores methods for learning a completion\nmap from unpaired incomplete and complete point cloud data. In this paper, we\npropose a novel approach for unpaired point cloud completion using the\nunbalanced optimal transport map, called Unbalanced Optimal Transport Map for\nUnpaired Point Cloud Completion (UOT-UPC). We demonstrate that the unpaired\npoint cloud completion can be naturally interpreted as the Optimal Transport\n(OT) problem and introduce the Unbalanced Optimal Transport (UOT) approach to\naddress the class imbalance problem, which is prevalent in unpaired point cloud\ncompletion datasets. Moreover, we analyze the appropriate cost function for\nunpaired completion tasks. This analysis shows that the InfoCD cost function is\nparticularly well-suited for this task. Our model is the first attempt to\nleverage UOT for unpaired point cloud completion, achieving competitive or\nsuperior results on both single-category and multi-category datasets. In\nparticular, our model is especially effective in scenarios with class\nimbalance, where the proportions of categories are different between the\nincomplete and complete point cloud datasets.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "20 pages, 10 figures",
    "pdf_url": "http://arxiv.org/pdf/2410.02671v3",
    "published_date": "2024-10-03 16:54:35 UTC",
    "updated_date": "2024-10-16 15:06:49 UTC"
  },
  {
    "arxiv_id": "2410.02666v1",
    "title": "AlphaIntegrator: Transformer Action Search for Symbolic Integration Proofs",
    "authors": [
      "Mert Ünsal",
      "Timon Gehr",
      "Martin Vechev"
    ],
    "abstract": "We present the first correct-by-construction learning-based system for\nstep-by-step mathematical integration. The key idea is to learn a policy,\nrepresented by a GPT transformer model, which guides the search for the right\nmathematical integration rule, to be carried out by a symbolic solver.\nConcretely, we introduce a symbolic engine with axiomatically correct actions\non mathematical expressions, as well as the first dataset for step-by-step\nintegration. Our GPT-style transformer model, trained on this synthetic data,\ndemonstrates strong generalization by surpassing its own data generator in\naccuracy and efficiency, using 50% fewer search steps. Our experimental results\nwith SoTA LLMs also demonstrate that the standard approach of fine-tuning LLMs\non a set of question-answer pairs is insufficient for solving this mathematical\ntask. This motivates the importance of discovering creative methods for\ncombining LLMs with symbolic reasoning engines, of which our work is an\ninstance.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.SC"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.02666v1",
    "published_date": "2024-10-03 16:50:30 UTC",
    "updated_date": "2024-10-03 16:50:30 UTC"
  },
  {
    "arxiv_id": "2410.02664v1",
    "title": "Grounded Answers for Multi-agent Decision-making Problem through Generative World Model",
    "authors": [
      "Zeyang Liu",
      "Xinrui Yang",
      "Shiguang Sun",
      "Long Qian",
      "Lipeng Wan",
      "Xingyu Chen",
      "Xuguang Lan"
    ],
    "abstract": "Recent progress in generative models has stimulated significant innovations\nin many fields, such as image generation and chatbots. Despite their success,\nthese models often produce sketchy and misleading solutions for complex\nmulti-agent decision-making problems because they miss the trial-and-error\nexperience and reasoning as humans. To address this limitation, we explore a\nparadigm that integrates a language-guided simulator into the multi-agent\nreinforcement learning pipeline to enhance the generated answer. The simulator\nis a world model that separately learns dynamics and reward, where the dynamics\nmodel comprises an image tokenizer as well as a causal transformer to generate\ninteraction transitions autoregressively, and the reward model is a\nbidirectional transformer learned by maximizing the likelihood of trajectories\nin the expert demonstrations under language guidance. Given an image of the\ncurrent state and the task description, we use the world model to train the\njoint policy and produce the image sequence as the answer by running the\nconverged policy on the dynamics model. The empirical results demonstrate that\nthis framework can improve the answers for multi-agent decision-making problems\nby showing superior performance on the training and unseen tasks of the\nStarCraft Multi-Agent Challenge benchmark. In particular, it can generate\nconsistent interaction sequences and explainable reward functions at\ninteraction states, opening the path for training generative models of the\nfuture.",
    "categories": [
      "cs.AI",
      "cs.MA"
    ],
    "primary_category": "cs.AI",
    "comment": "The Thirty-eighth Annual Conference on Neural Information Processing\n  Systems",
    "pdf_url": "http://arxiv.org/pdf/2410.02664v1",
    "published_date": "2024-10-03 16:49:59 UTC",
    "updated_date": "2024-10-03 16:49:59 UTC"
  },
  {
    "arxiv_id": "2410.02656v2",
    "title": "Scalable Simulation-free Entropic Unbalanced Optimal Transport",
    "authors": [
      "Jaemoo Choi",
      "Jaewoong Choi"
    ],
    "abstract": "The Optimal Transport (OT) problem investigates a transport map that connects\ntwo distributions while minimizing a given cost function. Finding such a\ntransport map has diverse applications in machine learning, such as generative\nmodeling and image-to-image translation. In this paper, we introduce a scalable\nand simulation-free approach for solving the Entropic Unbalanced Optimal\nTransport (EUOT) problem. We derive the dynamical form of this EUOT problem,\nwhich is a generalization of the Schr\\\"odinger bridges (SB) problem. Based on\nthis, we derive dual formulation and optimality conditions of the EUOT problem\nfrom the stochastic optimal control interpretation. By leveraging these\nproperties, we propose a simulation-free algorithm to solve EUOT, called\nSimulation-free EUOT (SF-EUOT). While existing SB models require expensive\nsimulation costs during training and evaluation, our model achieves\nsimulation-free training and one-step generation by utilizing the reciprocal\nproperty. Our model demonstrates significantly improved scalability in\ngenerative modeling and image-to-image translation tasks compared to previous\nSB methods.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "26 pages",
    "pdf_url": "http://arxiv.org/pdf/2410.02656v2",
    "published_date": "2024-10-03 16:43:00 UTC",
    "updated_date": "2024-10-21 20:50:35 UTC"
  },
  {
    "arxiv_id": "2410.02847v3",
    "title": "Deep Signature: Characterization of Large-Scale Molecular Dynamics",
    "authors": [
      "Tiexin Qin",
      "Mengxu Zhu",
      "Chunyang Li",
      "Terry Lyons",
      "Hong Yan",
      "Haoliang Li"
    ],
    "abstract": "Understanding protein dynamics are essential for deciphering protein\nfunctional mechanisms and developing molecular therapies. However, the complex\nhigh-dimensional dynamics and interatomic interactions of biological processes\npose significant challenge for existing computational techniques. In this\npaper, we approach this problem for the first time by introducing Deep\nSignature, a novel computationally tractable framework that characterizes\ncomplex dynamics and interatomic interactions based on their evolving\ntrajectories. Specifically, our approach incorporates soft spectral clustering\nthat locally aggregates cooperative dynamics to reduce the size of the system,\nas well as signature transform that collects iterated integrals to provide a\nglobal characterization of the non-smooth interactive dynamics. Theoretical\nanalysis demonstrates that Deep Signature exhibits several desirable\nproperties, including invariance to translation, near invariance to rotation,\nequivariance to permutation of atomic coordinates, and invariance under time\nreparameterization. Furthermore, experimental results on three benchmarks of\nbiological processes verify that our approach can achieve superior performance\ncompared to baseline methods.",
    "categories": [
      "q-bio.QM",
      "cs.AI"
    ],
    "primary_category": "q-bio.QM",
    "comment": "ICLR 2025",
    "pdf_url": "http://arxiv.org/pdf/2410.02847v3",
    "published_date": "2024-10-03 16:37:48 UTC",
    "updated_date": "2025-05-14 08:20:12 UTC"
  },
  {
    "arxiv_id": "2410.02651v2",
    "title": "CAX: Cellular Automata Accelerated in JAX",
    "authors": [
      "Maxence Faldor",
      "Antoine Cully"
    ],
    "abstract": "Cellular automata have become a cornerstone for investigating emergence and\nself-organization across diverse scientific disciplines. However, the absence\nof a hardware-accelerated cellular automata library limits the exploration of\nnew research directions, hinders collaboration, and impedes reproducibility. In\nthis work, we introduce CAX (Cellular Automata Accelerated in JAX), a\nhigh-performance and flexible open-source library designed to accelerate\ncellular automata research. CAX delivers cutting-edge performance through\nhardware acceleration while maintaining flexibility through its modular\narchitecture, intuitive API, and support for both discrete and continuous\ncellular automata in arbitrary dimensions. We demonstrate CAX's performance and\nflexibility through a wide range of benchmarks and applications. From classic\nmodels like elementary cellular automata and Conway's Game of Life to advanced\napplications such as growing neural cellular automata and self-classifying\nMNIST digits, CAX speeds up simulations up to 2,000 times faster. Furthermore,\nwe demonstrate CAX's potential to accelerate research by presenting a\ncollection of three novel cellular automata experiments, each implemented in\njust a few lines of code thanks to the library's modular architecture. Notably,\nwe show that a simple one-dimensional cellular automaton can outperform GPT-4\non the 1D-ARC challenge.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.02651v2",
    "published_date": "2024-10-03 16:36:05 UTC",
    "updated_date": "2025-03-11 11:34:10 UTC"
  },
  {
    "arxiv_id": "2410.02650v2",
    "title": "Undesirable Memorization in Large Language Models: A Survey",
    "authors": [
      "Ali Satvaty",
      "Suzan Verberne",
      "Fatih Turkmen"
    ],
    "abstract": "While recent research increasingly showcases the remarkable capabilities of\nLarge Language Models (LLMs), it is equally crucial to examine their associated\nrisks. Among these, privacy and security vulnerabilities are particularly\nconcerning, posing significant ethical and legal challenges. At the heart of\nthese vulnerabilities stands memorization, which refers to a model's tendency\nto store and reproduce phrases from its training data. This phenomenon has been\nshown to be a fundamental source to various privacy and security attacks\nagainst LLMs. In this paper, we provide a taxonomy of the literature on LLM\nmemorization, exploring it across three dimensions: granularity,\nretrievability, and desirability. Next, we discuss the metrics and methods used\nto quantify memorization, followed by an analysis of the causes and factors\nthat contribute to memorization phenomenon. We then explore strategies that are\nused so far to mitigate the undesirable aspects of this phenomenon. We conclude\nour survey by identifying potential research topics for the near future,\nincluding methods to balance privacy and performance, and the analysis of\nmemorization in specific LLM contexts such as conversational agents,\nretrieval-augmented generation, and diffusion language models. Given the rapid\nresearch pace in this field, we also maintain a dedicated repository of the\nreferences discussed in this survey which will be regularly updated to reflect\nthe latest developments.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.02650v2",
    "published_date": "2024-10-03 16:34:46 UTC",
    "updated_date": "2025-03-19 18:50:38 UTC"
  },
  {
    "arxiv_id": "2410.02644v3",
    "title": "Agent Security Bench (ASB): Formalizing and Benchmarking Attacks and Defenses in LLM-based Agents",
    "authors": [
      "Hanrong Zhang",
      "Jingyuan Huang",
      "Kai Mei",
      "Yifei Yao",
      "Zhenting Wang",
      "Chenlu Zhan",
      "Hongwei Wang",
      "Yongfeng Zhang"
    ],
    "abstract": "Although LLM-based agents, powered by Large Language Models (LLMs), can use\nexternal tools and memory mechanisms to solve complex real-world tasks, they\nmay also introduce critical security vulnerabilities. However, the existing\nliterature does not comprehensively evaluate attacks and defenses against\nLLM-based agents. To address this, we introduce Agent Security Bench (ASB), a\ncomprehensive framework designed to formalize, benchmark, and evaluate the\nattacks and defenses of LLM-based agents, including 10 scenarios (e.g.,\ne-commerce, autonomous driving, finance), 10 agents targeting the scenarios,\nover 400 tools, 27 different types of attack/defense methods, and 7 evaluation\nmetrics. Based on ASB, we benchmark 10 prompt injection attacks, a memory\npoisoning attack, a novel Plan-of-Thought backdoor attack, 4 mixed attacks, and\n11 corresponding defenses across 13 LLM backbones. Our benchmark results reveal\ncritical vulnerabilities in different stages of agent operation, including\nsystem prompt, user prompt handling, tool usage, and memory retrieval, with the\nhighest average attack success rate of 84.30\\%, but limited effectiveness shown\nin current defenses, unveiling important works to be done in terms of agent\nsecurity for the community. We also introduce a new metric to evaluate the\nagents' capability to balance utility and security. Our code can be found at\nhttps://github.com/agiresearch/ASB.",
    "categories": [
      "cs.CR",
      "cs.AI"
    ],
    "primary_category": "cs.CR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.02644v3",
    "published_date": "2024-10-03 16:30:47 UTC",
    "updated_date": "2025-04-16 09:10:17 UTC"
  },
  {
    "arxiv_id": "2410.02637v2",
    "title": "Plots Unlock Time-Series Understanding in Multimodal Models",
    "authors": [
      "Mayank Daswani",
      "Mathias M. J. Bellaiche",
      "Marc Wilson",
      "Desislav Ivanov",
      "Mikhail Papkov",
      "Eva Schnider",
      "Jing Tang",
      "Kay Lamerigts",
      "Gabriela Botea",
      "Michael A. Sanchez",
      "Yojan Patel",
      "Shruthi Prabhakara",
      "Shravya Shetty",
      "Umesh Telang"
    ],
    "abstract": "While multimodal foundation models can now natively work with data beyond\ntext, they remain underutilized in analyzing the considerable amounts of\nmulti-dimensional time-series data in fields like healthcare, finance, and\nsocial sciences, representing a missed opportunity for richer, data-driven\ninsights. This paper proposes a simple but effective method that leverages the\nexisting vision encoders of these models to \"see\" time-series data via plots,\navoiding the need for additional, potentially costly, model training. Our\nempirical evaluations show that this approach outperforms providing the raw\ntime-series data as text, with the additional benefit that visual time-series\nrepresentations demonstrate up to a 90% reduction in model API costs. We\nvalidate our hypothesis through synthetic data tasks of increasing complexity,\nprogressing from simple functional form identification on clean data, to\nextracting trends from noisy scatter plots. To demonstrate generalizability\nfrom synthetic tasks with clear reasoning steps to more complex, real-world\nscenarios, we apply our approach to consumer health tasks - specifically fall\ndetection, activity recognition, and readiness assessment - which involve\nheterogeneous, noisy data and multi-step reasoning. The overall success in plot\nperformance over text performance (up to an 120% performance increase on\nzero-shot synthetic tasks, and up to 150% performance increase on real-world\ntasks), across both GPT and Gemini model families, highlights our approach's\npotential for making the best use of the native capabilities of foundation\nmodels.",
    "categories": [
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.AI",
    "comment": "57 pages",
    "pdf_url": "http://arxiv.org/pdf/2410.02637v2",
    "published_date": "2024-10-03 16:23:13 UTC",
    "updated_date": "2024-11-28 16:01:47 UTC"
  },
  {
    "arxiv_id": "2410.03781v2",
    "title": "Towards the Pedagogical Steering of Large Language Models for Tutoring: A Case Study with Modeling Productive Failure",
    "authors": [
      "Romain Puech",
      "Jakub Macina",
      "Julia Chatain",
      "Mrinmaya Sachan",
      "Manu Kapur"
    ],
    "abstract": "One-to-one tutoring is one of the most efficient methods of teaching. With\nthe growing popularity of Large Language Models (LLMs), there have been efforts\nto create LLM based conversational tutors which can expand the benefits of one\nto one tutoring to everyone. However, current LLMs are trained primarily to be\nhelpful assistants and lack crucial pedagogical skills. For example, they often\nquickly reveal the solution to the student and fail to plan for a richer multi\nturn pedagogical interaction. To use LLMs in pedagogical settings, they need to\nbe steered to use effective teaching strategies: a problem we introduce as\nPedagogical Steering. We develop StratL, an algorithm to optimize LLM prompts\nand steer it to follow a predefined multi-turn tutoring plan represented as a\ntransition graph. As a case study, we create a prototype tutor for high school\nmath following Productive Failure (PF), an advanced and effective learning\ndesign. To validate our approach in a real-world setting, we run a field study\nwith 17 high school students in Singapore and show that StratL succeeds in\nsteering the LLM to follow the PF tutoring strategy. Finally, we highlight\nchallenges in Pedagogical Steering of LLMs and offer opportunities for further\nimprovements by publishing a dataset of PF problems and our code.",
    "categories": [
      "cs.HC",
      "cs.AI",
      "cs.CY",
      "cs.MA",
      "97",
      "I.2; H.5; J.4"
    ],
    "primary_category": "cs.HC",
    "comment": "19 pages, 10 figures, 6 tables",
    "pdf_url": "http://arxiv.org/pdf/2410.03781v2",
    "published_date": "2024-10-03 16:15:41 UTC",
    "updated_date": "2025-03-18 19:44:51 UTC"
  },
  {
    "arxiv_id": "2410.02628v2",
    "title": "Inverse Entropic Optimal Transport Solves Semi-supervised Learning via Data Likelihood Maximization",
    "authors": [
      "Mikhail Persiianov",
      "Arip Asadulaev",
      "Nikita Andreev",
      "Nikita Starodubcev",
      "Dmitry Baranchuk",
      "Anastasis Kratsios",
      "Evgeny Burnaev",
      "Alexander Korotin"
    ],
    "abstract": "Learning conditional distributions $\\pi^*(\\cdot|x)$ is a central problem in\nmachine learning, which is typically approached via supervised methods with\npaired data $(x,y) \\sim \\pi^*$. However, acquiring paired data samples is often\nchallenging, especially in problems such as domain translation. This\nnecessitates the development of $\\textit{semi-supervised}$ models that utilize\nboth limited paired data and additional unpaired i.i.d. samples $x \\sim\n\\pi^*_x$ and $y \\sim \\pi^*_y$ from the marginal distributions. The usage of\nsuch combined data is complex and often relies on heuristic approaches. To\ntackle this issue, we propose a new learning paradigm that integrates both\npaired and unpaired data $\\textbf{seamlessly}$ through the data likelihood\nmaximization techniques. We demonstrate that our approach also connects\nintriguingly with inverse entropic optimal transport (OT). This finding allows\nus to apply recent advances in computational OT to establish a $\\textbf{light}$\nlearning algorithm to get $\\pi^*(\\cdot|x)$. Furthermore, we demonstrate through\nempirical tests that our method effectively learns conditional distributions\nusing paired and unpaired data simultaneously.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.02628v2",
    "published_date": "2024-10-03 16:12:59 UTC",
    "updated_date": "2025-02-03 13:45:36 UTC"
  },
  {
    "arxiv_id": "2410.02618v1",
    "title": "Achieving Fairness in Predictive Process Analytics via Adversarial Learning (Extended Version)",
    "authors": [
      "Massimiliano de Leoni",
      "Alessandro Padella"
    ],
    "abstract": "Predictive business process analytics has become important for organizations,\noffering real-time operational support for their processes. However, these\nalgorithms often perform unfair predictions because they are based on biased\nvariables (e.g., gender or nationality), namely variables embodying\ndiscrimination. This paper addresses the challenge of integrating a debiasing\nphase into predictive business process analytics to ensure that predictions are\nnot influenced by biased variables. Our framework leverages on adversial\ndebiasing is evaluated on four case studies, showing a significant reduction in\nthe contribution of biased variables to the predicted value. The proposed\ntechnique is also compared with the state of the art in fairness in process\nmining, illustrating that our framework allows for a more enhanced level of\nfairness, while retaining a better prediction quality.",
    "categories": [
      "cs.AI",
      "cs.LG",
      "J.1"
    ],
    "primary_category": "cs.AI",
    "comment": "17 pages, 5 figures",
    "pdf_url": "http://arxiv.org/pdf/2410.02618v1",
    "published_date": "2024-10-03 15:56:03 UTC",
    "updated_date": "2024-10-03 15:56:03 UTC"
  },
  {
    "arxiv_id": "2410.02613v1",
    "title": "NL-Eye: Abductive NLI for Images",
    "authors": [
      "Mor Ventura",
      "Michael Toker",
      "Nitay Calderon",
      "Zorik Gekhman",
      "Yonatan Bitton",
      "Roi Reichart"
    ],
    "abstract": "Will a Visual Language Model (VLM)-based bot warn us about slipping if it\ndetects a wet floor? Recent VLMs have demonstrated impressive capabilities, yet\ntheir ability to infer outcomes and causes remains underexplored. To address\nthis, we introduce NL-Eye, a benchmark designed to assess VLMs' visual\nabductive reasoning skills. NL-Eye adapts the abductive Natural Language\nInference (NLI) task to the visual domain, requiring models to evaluate the\nplausibility of hypothesis images based on a premise image and explain their\ndecisions. NL-Eye consists of 350 carefully curated triplet examples (1,050\nimages) spanning diverse reasoning categories: physical, functional, logical,\nemotional, cultural, and social. The data curation process involved two steps -\nwriting textual descriptions and generating images using text-to-image models,\nboth requiring substantial human involvement to ensure high-quality and\nchallenging scenes. Our experiments show that VLMs struggle significantly on\nNL-Eye, often performing at random baseline levels, while humans excel in both\nplausibility prediction and explanation quality. This demonstrates a deficiency\nin the abductive reasoning capabilities of modern VLMs. NL-Eye represents a\ncrucial step toward developing VLMs capable of robust multimodal reasoning for\nreal-world applications, including accident-prevention bots and generated video\nverification.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.02613v1",
    "published_date": "2024-10-03 15:51:36 UTC",
    "updated_date": "2024-10-03 15:51:36 UTC"
  },
  {
    "arxiv_id": "2410.02611v1",
    "title": "IndicSentEval: How Effectively do Multilingual Transformer Models encode Linguistic Properties for Indic Languages?",
    "authors": [
      "Akhilesh Aravapalli",
      "Mounika Marreddy",
      "Subba Reddy Oota",
      "Radhika Mamidi",
      "Manish Gupta"
    ],
    "abstract": "Transformer-based models have revolutionized the field of natural language\nprocessing. To understand why they perform so well and to assess their\nreliability, several studies have focused on questions such as: Which\nlinguistic properties are encoded by these models, and to what extent? How\nrobust are these models in encoding linguistic properties when faced with\nperturbations in the input text? However, these studies have mainly focused on\nBERT and the English language. In this paper, we investigate similar questions\nregarding encoding capability and robustness for 8 linguistic properties across\n13 different perturbations in 6 Indic languages, using 9 multilingual\nTransformer models (7 universal and 2 Indic-specific). To conduct this study,\nwe introduce a novel multilingual benchmark dataset, IndicSentEval, containing\napproximately $\\sim$47K sentences. Surprisingly, our probing analysis of\nsurface, syntactic, and semantic properties reveals that while almost all\nmultilingual models demonstrate consistent encoding performance for English,\nthey show mixed results for Indic languages. As expected, Indic-specific\nmultilingual models capture linguistic properties in Indic languages better\nthan universal models. Intriguingly, universal models broadly exhibit better\nrobustness compared to Indic-specific models, particularly under perturbations\nsuch as dropping both nouns and verbs, dropping only verbs, or keeping only\nnouns. Overall, this study provides valuable insights into probing and\nperturbation-specific strengths and weaknesses of popular multilingual\nTransformer-based models for different Indic languages. We make our code and\ndataset publicly available [https://tinyurl.com/IndicSentEval}].",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "23 pages, 11 figures",
    "pdf_url": "http://arxiv.org/pdf/2410.02611v1",
    "published_date": "2024-10-03 15:50:08 UTC",
    "updated_date": "2024-10-03 15:50:08 UTC"
  },
  {
    "arxiv_id": "2410.02605v2",
    "title": "A Prospect-Theoretic Policy Gradient Algorithm for Behavioral Alignment in Reinforcement Learning",
    "authors": [
      "Olivier Lepel",
      "Anas Barakat"
    ],
    "abstract": "Classical reinforcement learning (RL) typically assumes rational\ndecision-making based on expected utility theory. However, this model has been\nshown to be empirically inconsistent with actual human preferences, as\nevidenced in psychology and behavioral economics. Cumulative Prospect Theory\n(CPT) provides a more nuanced model for human-based decision-making, capturing\ndiverse attitudes and perceptions toward risk, gains, and losses. While prior\nwork has integrated CPT with RL to solve a CPT policy optimization problem, the\nunderstanding and practical impact of this formulation remain limited. We\nrevisit the CPT-RL framework, offering new theoretical insights into the nature\nof optimal policies. We further derive a novel policy gradient theorem for CPT\nobjectives, generalizing the foundational result in standard RL. Building on\nthis theorem, we design a model-free policy gradient algorithm for solving the\nCPT-RL problem and demonstrate its performance through simulations. Notably,\nour algorithm scales better to larger state spaces compared to existing\nzeroth-order methods. This work advances the integration of behavioral\ndecision-making into RL.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "revised version",
    "pdf_url": "http://arxiv.org/pdf/2410.02605v2",
    "published_date": "2024-10-03 15:45:39 UTC",
    "updated_date": "2025-02-26 20:50:04 UTC"
  },
  {
    "arxiv_id": "2410.02596v1",
    "title": "Beyond Squared Error: Exploring Loss Design for Enhanced Training of Generative Flow Networks",
    "authors": [
      "Rui Hu",
      "Yifan Zhang",
      "Zhuoran Li",
      "Longbo Huang"
    ],
    "abstract": "Generative Flow Networks (GFlowNets) are a novel class of generative models\ndesigned to sample from unnormalized distributions and have found applications\nin various important tasks, attracting great research interest in their\ntraining algorithms. In general, GFlowNets are trained by fitting the forward\nflow to the backward flow on sampled training objects. Prior work focused on\nthe choice of training objects, parameterizations, sampling and resampling\nstrategies, and backward policies, aiming to enhance credit assignment,\nexploration, or exploitation of the training process. However, the choice of\nregression loss, which can highly influence the exploration and exploitation\nbehavior of the under-training policy, has been overlooked. Due to the lack of\ntheoretical understanding for choosing an appropriate regression loss, most\nexisting algorithms train the flow network by minimizing the squared error of\nthe forward and backward flows in log-space, i.e., using the quadratic\nregression loss. In this work, we rigorously prove that distinct regression\nlosses correspond to specific divergence measures, enabling us to design and\nanalyze regression losses according to the desired properties of the\ncorresponding divergence measures. Specifically, we examine two key properties:\nzero-forcing and zero-avoiding, where the former promotes exploitation and\nhigher rewards, and the latter encourages exploration and enhances diversity.\nBased on our theoretical framework, we propose three novel regression losses,\nnamely, Shifted-Cosh, Linex(1/2), and Linex(1). We evaluate them across three\nbenchmarks: hyper-grid, bit-sequence generation, and molecule generation. Our\nproposed losses are compatible with most existing training algorithms, and\nsignificantly improve the performances of the algorithms concerning convergence\nspeed, sample diversity, and robustness.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.02596v1",
    "published_date": "2024-10-03 15:37:22 UTC",
    "updated_date": "2024-10-03 15:37:22 UTC"
  },
  {
    "arxiv_id": "2410.02592v4",
    "title": "IC3M: In-Car Multimodal Multi-object Monitoring for Abnormal Status of Both Driver and Passengers",
    "authors": [
      "Zihan Fang",
      "Zheng Lin",
      "Senkang Hu",
      "Hangcheng Cao",
      "Yiqin Deng",
      "Xianhao Chen",
      "Yuguang Fang"
    ],
    "abstract": "Recently, in-car monitoring has emerged as a promising technology for\ndetecting early-stage abnormal status of the driver and providing timely alerts\nto prevent traffic accidents. Although training models with multimodal data\nenhances the reliability of abnormal status detection, the scarcity of labeled\ndata and the imbalance of class distribution impede the extraction of critical\nabnormal state features, significantly deteriorating training performance.\nFurthermore, missing modalities due to environment and hardware limitations\nfurther exacerbate the challenge of abnormal status identification. More\nimportantly, monitoring abnormal health conditions of passengers, particularly\nin elderly care, is of paramount importance but remains underexplored. To\naddress these challenges, we introduce our IC3M, an efficient\ncamera-rotation-based multimodal framework for monitoring both driver and\npassengers in a car. Our IC3M comprises two key modules: an adaptive threshold\npseudo-labeling strategy and a missing modality reconstruction. The former\ncustomizes pseudo-labeling thresholds for different classes based on the class\ndistribution, generating class-balanced pseudo labels to guide model training\neffectively, while the latter leverages crossmodality relationships learned\nfrom limited labels to accurately recover missing modalities by distribution\ntransferring from available modalities. Extensive experimental results\ndemonstrate that IC3M outperforms state-of-the-art benchmarks in accuracy,\nprecision, and recall while exhibiting superior robustness under limited\nlabeled data and severe missing modality.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG",
      "cs.SY",
      "eess.SY"
    ],
    "primary_category": "cs.CV",
    "comment": "16 pages, 17 figures",
    "pdf_url": "http://arxiv.org/pdf/2410.02592v4",
    "published_date": "2024-10-03 15:34:41 UTC",
    "updated_date": "2024-11-21 07:16:05 UTC"
  },
  {
    "arxiv_id": "2410.02581v3",
    "title": "Boosting Sample Efficiency and Generalization in Multi-agent Reinforcement Learning via Equivariance",
    "authors": [
      "Joshua McClellan",
      "Naveed Haghani",
      "John Winder",
      "Furong Huang",
      "Pratap Tokekar"
    ],
    "abstract": "Multi-Agent Reinforcement Learning (MARL) struggles with sample inefficiency\nand poor generalization [1]. These challenges are partially due to a lack of\nstructure or inductive bias in the neural networks typically used in learning\nthe policy. One such form of structure that is commonly observed in multi-agent\nscenarios is symmetry. The field of Geometric Deep Learning has developed\nEquivariant Graph Neural Networks (EGNN) that are equivariant (or symmetric) to\nrotations, translations, and reflections of nodes. Incorporating equivariance\nhas been shown to improve learning efficiency and decrease error [ 2 ]. In this\npaper, we demonstrate that EGNNs improve the sample efficiency and\ngeneralization in MARL. However, we also show that a naive application of EGNNs\nto MARL results in poor early exploration due to a bias in the EGNN structure.\nTo mitigate this bias, we present Exploration-enhanced Equivariant Graph Neural\nNetworks or E2GN2. We compare E2GN2 to other common function approximators\nusing common MARL benchmarks MPE and SMACv2. E2GN2 demonstrates a significant\nimprovement in sample efficiency, greater final reward convergence, and a 2x-5x\ngain in over standard GNNs in our generalization tests. These results pave the\nway for more reliable and effective solutions in complex multi-agent systems.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "accepted as a poster at NeurIPS 2024",
    "pdf_url": "http://arxiv.org/pdf/2410.02581v3",
    "published_date": "2024-10-03 15:25:37 UTC",
    "updated_date": "2024-10-22 16:26:40 UTC"
  },
  {
    "arxiv_id": "2410.02579v1",
    "title": "Deep Regression 2D-3D Ultrasound Registration for Liver Motion Correction in Focal Tumor Thermal Ablation",
    "authors": [
      "Shuwei Xing",
      "Derek W. Cool",
      "David Tessier",
      "Elvis C. S. Chen",
      "Terry M. Peters",
      "Aaron Fenster"
    ],
    "abstract": "Liver tumor ablation procedures require accurate placement of the needle\napplicator at the tumor centroid. The lower-cost and real-time nature of\nultrasound (US) has advantages over computed tomography (CT) for applicator\nguidance, however, in some patients, liver tumors may be occult on US and tumor\nmimics can make lesion identification challenging. Image registration\ntechniques can aid in interpreting anatomical details and identifying tumors,\nbut their clinical application has been hindered by the tradeoff between\nalignment accuracy and runtime performance, particularly when compensating for\nliver motion due to patient breathing or movement. Therefore, we propose a\n2D-3D US registration approach to enable intra-procedural alignment that\nmitigates errors caused by liver motion. Specifically, our approach can\ncorrelate imbalanced 2D and 3D US image features and use continuous 6D rotation\nrepresentations to enhance the model's training stability. The dataset was\ndivided into 2388, 196 and 193 image pairs for training, validation and\ntesting, respectively. Our approach achieved a mean Euclidean distance error of\n2.28 mm $\\pm$ 1.81 mm and a mean geodesic angular error of 2.99$^{\\circ}$ $\\pm$\n1.95$^{\\circ}$, with a runtime of 0.22 seconds per 2D-3D US image pair. These\nresults demonstrate that our approach can achieve accurate alignment and\nclinically acceptable runtime, indicating potential for clinical translation.",
    "categories": [
      "eess.IV",
      "cs.AI"
    ],
    "primary_category": "eess.IV",
    "comment": "15 pagers, 9 figures",
    "pdf_url": "http://arxiv.org/pdf/2410.02579v1",
    "published_date": "2024-10-03 15:24:45 UTC",
    "updated_date": "2024-10-03 15:24:45 UTC"
  },
  {
    "arxiv_id": "2410.03779v1",
    "title": "Discovering Message Passing Hierarchies for Mesh-Based Physics Simulation",
    "authors": [
      "Huayu Deng",
      "Xiangming Zhu",
      "Yunbo Wang",
      "Xiaokang Yang"
    ],
    "abstract": "Graph neural networks have emerged as a powerful tool for large-scale\nmesh-based physics simulation. Existing approaches primarily employ\nhierarchical, multi-scale message passing to capture long-range dependencies\nwithin the graph. However, these graph hierarchies are typically fixed and\nmanually designed, which do not adapt to the evolving dynamics present in\ncomplex physical systems. In this paper, we introduce a novel neural network\nnamed DHMP, which learns Dynamic Hierarchies for Message Passing networks\nthrough a differentiable node selection method. The key component is the\nanisotropic message passing mechanism, which operates at both intra-level and\ninter-level interactions. Unlike existing methods, it first supports\ndirectionally non-uniform aggregation of dynamic features between adjacent\nnodes within each graph hierarchy. Second, it determines node selection\nprobabilities for the next hierarchy according to different physical contexts,\nthereby creating more flexible message shortcuts for learning remote node\nrelations. Our experiments demonstrate the effectiveness of DHMP, achieving\n22.7% improvement on average compared to recent fixed-hierarchy message passing\nnetworks across five classic physics simulation datasets.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CE"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.03779v1",
    "published_date": "2024-10-03 15:18:00 UTC",
    "updated_date": "2024-10-03 15:18:00 UTC"
  },
  {
    "arxiv_id": "2410.02551v2",
    "title": "ColaCare: Enhancing Electronic Health Record Modeling through Large Language Model-Driven Multi-Agent Collaboration",
    "authors": [
      "Zixiang Wang",
      "Yinghao Zhu",
      "Huiya Zhao",
      "Xiaochen Zheng",
      "Dehao Sui",
      "Tianlong Wang",
      "Wen Tang",
      "Yasha Wang",
      "Ewen Harrison",
      "Chengwei Pan",
      "Junyi Gao",
      "Liantao Ma"
    ],
    "abstract": "We introduce ColaCare, a framework that enhances Electronic Health Record\n(EHR) modeling through multi-agent collaboration driven by Large Language\nModels (LLMs). Our approach seamlessly integrates domain-specific expert models\nwith LLMs to bridge the gap between structured EHR data and text-based\nreasoning. Inspired by the Multidisciplinary Team (MDT) approach used in\nclinical settings, ColaCare employs two types of agents: DoctorAgents and a\nMetaAgent, which collaboratively analyze patient data. Expert models process\nand generate predictions from numerical EHR data, while LLM agents produce\nreasoning references and decision-making reports within the MDT-driven\ncollaborative consultation framework. The MetaAgent orchestrates the\ndiscussion, facilitating consultations and evidence-based debates among\nDoctorAgents, simulating diverse expertise in clinical decision-making. We\nadditionally incorporate the Merck Manual of Diagnosis and Therapy (MSD)\nmedical guideline within a retrieval-augmented generation (RAG) module for\nmedical evidence support, addressing the challenge of knowledge currency.\nExtensive experiments conducted on three EHR datasets demonstrate ColaCare's\nsuperior performance in clinical mortality outcome and readmission prediction\ntasks, underscoring its potential to revolutionize clinical decision support\nsystems and advance personalized precision medicine. All code, case studies and\na questionnaire are available at the project website:\nhttps://colacare.netlify.app.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.LG",
    "comment": "ACM TheWebConf 2025 Conference (WWW 2025) Research Track",
    "pdf_url": "http://arxiv.org/pdf/2410.02551v2",
    "published_date": "2024-10-03 14:55:22 UTC",
    "updated_date": "2025-02-26 13:51:56 UTC"
  },
  {
    "arxiv_id": "2410.02547v1",
    "title": "Personalized Quantum Federated Learning for Privacy Image Classification",
    "authors": [
      "Jinjing Shi",
      "Tian Chen",
      "Shichao Zhang",
      "Xuelong Li"
    ],
    "abstract": "Quantum federated learning has brought about the improvement of privacy image\nclassification, while the lack of personality of the client model may\ncontribute to the suboptimal of quantum federated learning. A personalized\nquantum federated learning algorithm for privacy image classification is\nproposed to enhance the personality of the client model in the case of an\nimbalanced distribution of images. First, a personalized quantum federated\nlearning model is constructed, in which a personalized layer is set for the\nclient model to maintain the personalized parameters. Second, a personalized\nquantum federated learning algorithm is introduced to secure the information\nexchanged between the client and server.Third, the personalized federated\nlearning is applied to image classification on the FashionMNIST dataset, and\nthe experimental results indicate that the personalized quantum federated\nlearning algorithm can obtain global and local models with excellent\nperformance, even in situations where local training samples are imbalanced.\nThe server's accuracy is 100% with 8 clients and a distribution parameter of\n100, outperforming the non-personalized model by 7%. The average client\naccuracy is 2.9% higher than that of the non-personalized model with 2 clients\nand a distribution parameter of 1. Compared to previous quantum federated\nlearning algorithms, the proposed personalized quantum federated learning\nalgorithm eliminates the need for additional local training while safeguarding\nboth model and data privacy.It may facilitate broader adoption and application\nof quantum technologies, and pave the way for more secure, scalable, and\nefficient quantum distribute machine learning solutions.",
    "categories": [
      "quant-ph",
      "cs.AI"
    ],
    "primary_category": "quant-ph",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.02547v1",
    "published_date": "2024-10-03 14:53:04 UTC",
    "updated_date": "2024-10-03 14:53:04 UTC"
  },
  {
    "arxiv_id": "2410.02845v1",
    "title": "Towards Layer-Wise Personalized Federated Learning: Adaptive Layer Disentanglement via Conflicting Gradients",
    "authors": [
      "Minh Duong Nguyen",
      "Khanh Le",
      "Khoi Do",
      "Nguyen H. Tran",
      "Duc Nguyen",
      "Chien Trinh",
      "Zhaohui Yang"
    ],
    "abstract": "In personalized Federated Learning (pFL), high data heterogeneity can cause\nsignificant gradient divergence across devices, adversely affecting the\nlearning process. This divergence, especially when gradients from different\nusers form an obtuse angle during aggregation, can negate progress, leading to\nsevere weight and gradient update degradation. To address this issue, we\nintroduce a new approach to pFL design, namely Federated Learning with\nLayer-wise Aggregation via Gradient Analysis (FedLAG), utilizing the concept of\ngradient conflict at the layer level. Specifically, when layer-wise gradients\nof different clients form acute angles, those gradients align in the same\ndirection, enabling updates across different clients toward identifying\nclient-invariant features. Conversely, when layer-wise gradient pairs make\ncreate obtuse angles, the layers tend to focus on client-specific tasks. In\nhindsights, FedLAG assigns layers for personalization based on the extent of\nlayer-wise gradient conflicts. Specifically, layers with gradient conflicts are\nexcluded from the global aggregation process. The theoretical evaluation\ndemonstrates that when integrated into other pFL baselines, FedLAG enhances pFL\nperformance by a certain margin. Therefore, our proposed method achieves\nsuperior convergence behavior compared with other baselines. Extensive\nexperiments show that our FedLAG outperforms several state-of-the-art methods\nand can be easily incorporated with many existing methods to further enhance\nperformance.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.02845v1",
    "published_date": "2024-10-03 14:46:19 UTC",
    "updated_date": "2024-10-03 14:46:19 UTC"
  },
  {
    "arxiv_id": "2410.02536v3",
    "title": "Intelligence at the Edge of Chaos",
    "authors": [
      "Shiyang Zhang",
      "Aakash Patel",
      "Syed A Rizvi",
      "Nianchen Liu",
      "Sizhuang He",
      "Amin Karbasi",
      "Emanuele Zappala",
      "David van Dijk"
    ],
    "abstract": "We explore the emergence of intelligent behavior in artificial systems by\ninvestigating how the complexity of rule-based systems influences the\ncapabilities of models trained to predict these rules. Our study focuses on\nelementary cellular automata (ECA), simple yet powerful one-dimensional systems\nthat generate behaviors ranging from trivial to highly complex. By training\ndistinct Large Language Models (LLMs) on different ECAs, we evaluated the\nrelationship between the complexity of the rules' behavior and the intelligence\nexhibited by the LLMs, as reflected in their performance on downstream tasks.\nOur findings reveal that rules with higher complexity lead to models exhibiting\ngreater intelligence, as demonstrated by their performance on reasoning and\nchess move prediction tasks. Both uniform and periodic systems, and often also\nhighly chaotic systems, resulted in poorer downstream performance, highlighting\na sweet spot of complexity conducive to intelligence. We conjecture that\nintelligence arises from the ability to predict complexity and that creating\nintelligence may require only exposure to complexity.",
    "categories": [
      "cs.AI",
      "cs.NE"
    ],
    "primary_category": "cs.AI",
    "comment": "15 pages,8 Figures",
    "pdf_url": "http://arxiv.org/pdf/2410.02536v3",
    "published_date": "2024-10-03 14:42:34 UTC",
    "updated_date": "2025-03-01 13:21:09 UTC"
  },
  {
    "arxiv_id": "2410.02533v2",
    "title": "A Schema-aware Logic Reformulation for Graph Reachability",
    "authors": [
      "Davide Di Pierro",
      "Stephan Mennicke",
      "Stefano Ferilli"
    ],
    "abstract": "Graph reachability is the task of understanding whether two distinct points\nin a graph are interconnected by arcs to which in general a semantic is\nattached. Reachability has plenty of applications, ranging from motion planning\nto routing. Improving reachability requires structural knowledge of relations\nso as to avoid the complexity of traditional depth-first and breadth-first\nstrategies, implemented in logic languages. In some contexts, graphs are\nenriched with their schema definitions establishing domain and range for every\narc. The introduction of a schema-aware formalization for guiding the search\nmay result in a sensitive improvement by cutting out unuseful paths and\nprioritising those that, in principle, reach the target earlier. In this work,\nwe propose a strategy to automatically exclude and sort certain graph paths by\nexploiting the higher-level conceptualization of instances. The aim is to\nobtain a new first-order logic reformulation of the graph reachability\nscenario, capable of improving the traditional algorithms in terms of time,\nspace requirements, and number of backtracks. The experiments exhibit the\nexpected advantages of the approach in reducing the number of backtracks during\nthe search strategy, resulting in saving time and space as well.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.02533v2",
    "published_date": "2024-10-03 14:39:49 UTC",
    "updated_date": "2025-03-25 11:41:51 UTC"
  },
  {
    "arxiv_id": "2410.02525v4",
    "title": "Contextual Document Embeddings",
    "authors": [
      "John X. Morris",
      "Alexander M. Rush"
    ],
    "abstract": "Dense document embeddings are central to neural retrieval. The dominant\nparadigm is to train and construct embeddings by running encoders directly on\nindividual documents. In this work, we argue that these embeddings, while\neffective, are implicitly out-of-context for targeted use cases of retrieval,\nand that a contextualized document embedding should take into account both the\ndocument and neighboring documents in context - analogous to contextualized\nword embeddings. We propose two complementary methods for contextualized\ndocument embeddings: first, an alternative contrastive learning objective that\nexplicitly incorporates the document neighbors into the intra-batch contextual\nloss; second, a new contextual architecture that explicitly encodes neighbor\ndocument information into the encoded representation. Results show that both\nmethods achieve better performance than biencoders in several settings, with\ndifferences especially pronounced out-of-domain. We achieve state-of-the-art\nresults on the MTEB benchmark with no hard negative mining, score distillation,\ndataset-specific instructions, intra-GPU example-sharing, or extremely large\nbatch sizes. Our method can be applied to improve performance on any\ncontrastive learning dataset and any biencoder.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.02525v4",
    "published_date": "2024-10-03 14:33:34 UTC",
    "updated_date": "2024-11-08 16:26:22 UTC"
  },
  {
    "arxiv_id": "2410.02512v1",
    "title": "SAFLEX: Self-Adaptive Augmentation via Feature Label Extrapolation",
    "authors": [
      "Mucong Ding",
      "Bang An",
      "Yuancheng Xu",
      "Anirudh Satheesh",
      "Furong Huang"
    ],
    "abstract": "Data augmentation, a cornerstone technique in deep learning, is crucial in\nenhancing model performance, especially with scarce labeled data. While\ntraditional techniques are effective, their reliance on hand-crafted methods\nlimits their applicability across diverse data types and tasks. Although modern\nlearnable augmentation methods offer increased adaptability, they are\ncomputationally expensive and challenging to incorporate within prevalent\naugmentation workflows. In this work, we present a novel, efficient method for\ndata augmentation, effectively bridging the gap between existing augmentation\nstrategies and emerging datasets and learning tasks. We introduce SAFLEX\n(Self-Adaptive Augmentation via Feature Label EXtrapolation), which learns the\nsample weights and soft labels of augmented samples provided by any given\nupstream augmentation pipeline, using a specifically designed efficient bilevel\noptimization algorithm. Remarkably, SAFLEX effectively reduces the noise and\nlabel errors of the upstream augmentation pipeline with a marginal\ncomputational cost. As a versatile module, SAFLEX excels across diverse\ndatasets, including natural and medical images and tabular data, showcasing its\nprowess in few-shot learning and out-of-distribution generalization. SAFLEX\nseamlessly integrates with common augmentation strategies like RandAug, CutMix,\nand those from large pre-trained generative models like stable diffusion and is\nalso compatible with frameworks such as CLIP's fine-tuning. Our findings\nhighlight the potential to adapt existing augmentation pipelines for new data\ntypes and tasks, signaling a move towards more adaptable and resilient training\nframeworks.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "ICLR 2024",
    "pdf_url": "http://arxiv.org/pdf/2410.02512v1",
    "published_date": "2024-10-03 14:21:49 UTC",
    "updated_date": "2024-10-03 14:21:49 UTC"
  },
  {
    "arxiv_id": "2410.02511v1",
    "title": "Choices are More Important than Efforts: LLM Enables Efficient Multi-Agent Exploration",
    "authors": [
      "Yun Qu",
      "Boyuan Wang",
      "Yuhang Jiang",
      "Jianzhun Shao",
      "Yixiu Mao",
      "Cheems Wang",
      "Chang Liu",
      "Xiangyang Ji"
    ],
    "abstract": "With expansive state-action spaces, efficient multi-agent exploration remains\na longstanding challenge in reinforcement learning. Although pursuing novelty,\ndiversity, or uncertainty attracts increasing attention, redundant efforts\nbrought by exploration without proper guidance choices poses a practical issue\nfor the community. This paper introduces a systematic approach, termed LEMAE,\nchoosing to channel informative task-relevant guidance from a knowledgeable\nLarge Language Model (LLM) for Efficient Multi-Agent Exploration. Specifically,\nwe ground linguistic knowledge from LLM into symbolic key states, that are\ncritical for task fulfillment, in a discriminative manner at low LLM inference\ncosts. To unleash the power of key states, we design Subspace-based Hindsight\nIntrinsic Reward (SHIR) to guide agents toward key states by increasing reward\ndensity. Additionally, we build the Key State Memory Tree (KSMT) to track\ntransitions between key states in a specific task for organized exploration.\nBenefiting from diminishing redundant explorations, LEMAE outperforms existing\nSOTA approaches on the challenging benchmarks (e.g., SMAC and MPE) by a large\nmargin, achieving a 10x acceleration in certain scenarios.",
    "categories": [
      "cs.AI",
      "cs.MA"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.02511v1",
    "published_date": "2024-10-03 14:21:23 UTC",
    "updated_date": "2024-10-03 14:21:23 UTC"
  },
  {
    "arxiv_id": "2410.02507v1",
    "title": "Can Large Language Models Grasp Legal Theories? Enhance Legal Reasoning with Insights from Multi-Agent Collaboration",
    "authors": [
      "Weikang Yuan",
      "Junjie Cao",
      "Zhuoren Jiang",
      "Yangyang Kang",
      "Jun Lin",
      "Kaisong Song",
      "tianqianjin lin",
      "Pengwei Yan",
      "Changlong Sun",
      "Xiaozhong Liu"
    ],
    "abstract": "Large Language Models (LLMs) could struggle to fully understand legal\ntheories and perform complex legal reasoning tasks. In this study, we introduce\na challenging task (confusing charge prediction) to better evaluate LLMs'\nunderstanding of legal theories and reasoning capabilities. We also propose a\nnovel framework: Multi-Agent framework for improving complex Legal Reasoning\ncapability (MALR). MALR employs non-parametric learning, encouraging LLMs to\nautomatically decompose complex legal tasks and mimic human learning process to\nextract insights from legal rules, helping LLMs better understand legal\ntheories and enhance their legal reasoning abilities. Extensive experiments on\nmultiple real-world datasets demonstrate that the proposed framework\neffectively addresses complex reasoning issues in practical scenarios, paving\nthe way for more reliable applications in the legal domain.",
    "categories": [
      "cs.AI",
      "cs.CL",
      "I.2.7"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.02507v1",
    "published_date": "2024-10-03 14:15:00 UTC",
    "updated_date": "2024-10-03 14:15:00 UTC"
  },
  {
    "arxiv_id": "2410.02505v2",
    "title": "Dog-IQA: Standard-guided Zero-shot MLLM for Mix-grained Image Quality Assessment",
    "authors": [
      "Kai Liu",
      "Ziqing Zhang",
      "Wenbo Li",
      "Renjing Pei",
      "Fenglong Song",
      "Xiaohong Liu",
      "Linghe Kong",
      "Yulun Zhang"
    ],
    "abstract": "Image quality assessment (IQA) serves as the golden standard for all models'\nperformance in nearly all computer vision fields. However, it still suffers\nfrom poor out-of-distribution generalization ability and expensive training\ncosts. To address these problems, we propose Dog-IQA, a standard-guided\nzero-shot mix-grained IQA method, which is training-free and utilizes the\nexceptional prior knowledge of multimodal large language models (MLLMs). To\nobtain accurate IQA scores, namely scores consistent with humans, we design an\nMLLM-based inference pipeline that imitates human experts. In detail, Dog-IQA\napplies two techniques. First, Dog-IQA objectively scores with specific\nstandards that utilize MLLM's behavior pattern and minimize the influence of\nsubjective factors. Second, Dog-IQA comprehensively takes local semantic\nobjects and the whole image as input and aggregates their scores, leveraging\nlocal and global information. Our proposed Dog-IQA achieves state-of-the-art\n(SOTA) performance compared with training-free methods, and competitive\nperformance compared with training-based methods in cross-dataset scenarios.\nOur code will be available at https://github.com/Kai-Liu001/Dog-IQA.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "10 pages, 5 figures. The code and models will be available at\n  https://github.com/Kai-Liu001/Dog-IQA",
    "pdf_url": "http://arxiv.org/pdf/2410.02505v2",
    "published_date": "2024-10-03 14:14:21 UTC",
    "updated_date": "2024-10-10 05:36:30 UTC"
  },
  {
    "arxiv_id": "2410.02503v1",
    "title": "Mixed-Session Conversation with Egocentric Memory",
    "authors": [
      "Jihyoung Jang",
      "Taeyoung Kim",
      "Hyounghun Kim"
    ],
    "abstract": "Recently introduced dialogue systems have demonstrated high usability.\nHowever, they still fall short of reflecting real-world conversation scenarios.\nCurrent dialogue systems exhibit an inability to replicate the dynamic,\ncontinuous, long-term interactions involving multiple partners. This shortfall\narises because there have been limited efforts to account for both aspects of\nreal-world dialogues: deeply layered interactions over the long-term dialogue\nand widely expanded conversation networks involving multiple participants. As\nthe effort to incorporate these aspects combined, we introduce Mixed-Session\nConversation, a dialogue system designed to construct conversations with\nvarious partners in a multi-session dialogue setup. We propose a new dataset\ncalled MiSC to implement this system. The dialogue episodes of MiSC consist of\n6 consecutive sessions, with four speakers (one main speaker and three\npartners) appearing in each episode. Also, we propose a new dialogue model with\na novel memory management mechanism, called Egocentric Memory Enhanced\nMixed-Session Conversation Agent (EMMA). EMMA collects and retains memories\nfrom the main speaker's perspective during conversations with partners,\nenabling seamless continuity in subsequent interactions. Extensive human\nevaluations validate that the dialogues in MiSC demonstrate a seamless\nconversational flow, even when conversation partners change in each session.\nEMMA trained with MiSC is also evaluated to maintain high memorability without\ncontradiction throughout the entire conversation.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "EMNLP Findings 2024 (30 pages); Project website:\n  https://mixed-session.github.io/",
    "pdf_url": "http://arxiv.org/pdf/2410.02503v1",
    "published_date": "2024-10-03 14:06:43 UTC",
    "updated_date": "2024-10-03 14:06:43 UTC"
  },
  {
    "arxiv_id": "2410.02844v3",
    "title": "CAnDOIT: Causal Discovery with Observational and Interventional Data from Time-Series",
    "authors": [
      "Luca Castri",
      "Sariah Mghames",
      "Marc Hanheide",
      "Nicola Bellotto"
    ],
    "abstract": "The study of cause-and-effect is of the utmost importance in many branches of\nscience, but also for many practical applications of intelligent systems. In\nparticular, identifying causal relationships in situations that include hidden\nfactors is a major challenge for methods that rely solely on observational data\nfor building causal models. This paper proposes CAnDOIT, a causal discovery\nmethod to reconstruct causal models using both observational and interventional\ntime-series data. The use of interventional data in the causal analysis is\ncrucial for real-world applications, such as robotics, where the scenario is\nhighly complex and observational data alone are often insufficient to uncover\nthe correct causal structure. Validation of the method is performed initially\non randomly generated synthetic models and subsequently on a well-known\nbenchmark for causal structure learning in a robotic manipulation environment.\nThe experiments demonstrate that the approach can effectively handle data from\ninterventions and exploit them to enhance the accuracy of the causal analysis.\nA Python implementation of CAnDOIT has also been developed and is publicly\navailable on GitHub: https://github.com/lcastri/causalflow.",
    "categories": [
      "stat.ML",
      "cs.AI",
      "cs.LG",
      "cs.RO"
    ],
    "primary_category": "stat.ML",
    "comment": "Published in Advanced Intelligent Systems",
    "pdf_url": "http://arxiv.org/pdf/2410.02844v3",
    "published_date": "2024-10-03 13:57:08 UTC",
    "updated_date": "2024-10-11 09:48:39 UTC"
  },
  {
    "arxiv_id": "2410.02843v1",
    "title": "Neural DDEs with Learnable Delays for Partially Observed Dynamical Systems",
    "authors": [
      "Thibault Monsel",
      "Emmanuel Menier",
      "Onofrio Semeraro",
      "Lionel Mathelin",
      "Guillaume Charpiat"
    ],
    "abstract": "Many successful methods to learn dynamical systems from data have recently\nbeen introduced. Such methods often rely on the availability of the system's\nfull state. However, this underlying hypothesis is rather restrictive as it is\ntypically not confirmed in practice, leaving us with partially observed\nsystems. Utilizing the Mori-Zwanzig (MZ) formalism from statistical physics, we\ndemonstrate that Constant Lag Neural Delay Differential Equations (NDDEs)\nnaturally serve as suitable models for partially observed states. In empirical\nevaluation, we show that such models outperform existing methods on both\nsynthetic and experimental data.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "physics.comp-ph"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.02843v1",
    "published_date": "2024-10-03 13:54:21 UTC",
    "updated_date": "2024-10-03 13:54:21 UTC"
  },
  {
    "arxiv_id": "2410.02472v3",
    "title": "Meta-Models: An Architecture for Decoding LLM Behaviors Through Interpreted Embeddings and Natural Language",
    "authors": [
      "Anthony Costarelli",
      "Mat Allen",
      "Severin Field"
    ],
    "abstract": "As Large Language Models (LLMs) become increasingly integrated into our daily\nlives, the potential harms from deceptive behavior underlie the need for\nfaithfully interpreting their decision-making. While traditional probing\nmethods have shown some effectiveness, they remain best for narrowly scoped\ntasks while more comprehensive explanations are still necessary. To this end,\nwe investigate meta-models-an architecture using a \"meta-model\" that takes\nactivations from an \"input-model\" and answers natural language questions about\nthe input-model's behaviors. We evaluate the meta-model's ability to generalize\nby training them on selected task types and assessing their out-of-distribution\nperformance in deceptive scenarios. Our findings show that meta-models\ngeneralize well to out-of-distribution tasks and point towards opportunities\nfor future research in this area. Our code is available at\nhttps://github.com/acostarelli/meta-models-public .",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "11 pages, 2 figures",
    "pdf_url": "http://arxiv.org/pdf/2410.02472v3",
    "published_date": "2024-10-03 13:25:15 UTC",
    "updated_date": "2024-11-07 18:30:38 UTC"
  },
  {
    "arxiv_id": "2410.02465v2",
    "title": "Revealing the Inherent Instructability of Pre-Trained Language Models",
    "authors": [
      "Seokhyun An",
      "Minji Kim",
      "Hyounghun Kim"
    ],
    "abstract": "Instruction tuning -- supervised fine-tuning using instruction-response pairs\n-- is a key step in making pre-trained large language models (LLMs)\ninstructable. Meanwhile, LLMs perform multitask learning during their\npre-training, acquiring extensive knowledge and capabilities. We hypothesize\nthat the pre-training stage can enable them to develop the ability to\ncomprehend and address instructions. To verify this, we propose Response Tuning\n(RT), which removes the instruction and its corresponding mapping to the\nresponse from instruction tuning. Instead, it focuses solely on establishing\nthe response distribution. Our experiments demonstrate that RT models, trained\nonly on responses, can effectively respond to a wide range of instructions and\nexhibit helpfulness approaching that of their instruction-tuned counterparts.\nIn addition, we observe that the models can recognize and reject unsafe queries\nafter learning the refusal conditions from training responses. Furthermore, we\ndemonstrate that these observations also hold in an in-context learning\nsetting. These findings support our hypothesis, highlighting the extensive\ninherent capabilities of pre-trained LLMs.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "31 pages",
    "pdf_url": "http://arxiv.org/pdf/2410.02465v2",
    "published_date": "2024-10-03 13:15:19 UTC",
    "updated_date": "2025-02-16 13:50:42 UTC"
  },
  {
    "arxiv_id": "2410.02456v1",
    "title": "Recurrent Few-Shot model for Document Verification",
    "authors": [
      "Maxime Talarmain",
      "Carlos Boned",
      "Sanket Biswas",
      "Oriol Ramos"
    ],
    "abstract": "General-purpose ID, or travel, document image- and video-based verification\nsystems have yet to achieve good enough performance to be considered a solved\nproblem. There are several factors that negatively impact their performance,\nincluding low-resolution images and videos and a lack of sufficient data to\ntrain the models. This task is particularly challenging when dealing with\nunseen class of ID, or travel, documents. In this paper we address this task by\nproposing a recurrent-based model able to detect forged documents in a few-shot\nscenario. The recurrent architecture makes the model robust to document\nresolution variability. Moreover, the few-shot approach allow the model to\nperform well even for unseen class of documents. Preliminary results on the\nSIDTD and Findit datasets show good performance of this model for this task.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.02456v1",
    "published_date": "2024-10-03 13:05:27 UTC",
    "updated_date": "2024-10-03 13:05:27 UTC"
  },
  {
    "arxiv_id": "2410.02451v2",
    "title": "Strong Preferences Affect the Robustness of Preference Models and Value Alignment",
    "authors": [
      "Ziwei Xu",
      "Mohan Kankanhalli"
    ],
    "abstract": "Value alignment, which aims to ensure that large language models (LLMs) and\nother AI agents behave in accordance with human values, is critical for\nensuring safety and trustworthiness of these systems. A key component of value\nalignment is the modeling of human preferences as a representation of human\nvalues. In this paper, we investigate the robustness of value alignment by\nexamining the sensitivity of preference models. Specifically, we ask: how do\nchanges in the probabilities of some preferences affect the predictions of\nthese models for other preferences? To answer this question, we theoretically\nanalyze the robustness of widely used preference models by examining their\nsensitivities to minor changes in preferences they model. Our findings reveal\nthat, in the Bradley-Terry and the Placket-Luce model, the probability of a\npreference can change significantly as other preferences change, especially\nwhen these preferences are dominant (i.e., with probabilities near 0 or 1). We\nidentify specific conditions where this sensitivity becomes significant for\nthese models and discuss the practical implications for the robustness and\nsafety of value alignment in AI systems.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "21 Pages. Accepted by ICLR 2025",
    "pdf_url": "http://arxiv.org/pdf/2410.02451v2",
    "published_date": "2024-10-03 12:53:43 UTC",
    "updated_date": "2025-03-08 04:29:40 UTC"
  },
  {
    "arxiv_id": "2410.02443v1",
    "title": "Clinnova Federated Learning Proof of Concept: Key Takeaways from a Cross-border Collaboration",
    "authors": [
      "Julia Alekseenko",
      "Bram Stieltjes",
      "Michael Bach",
      "Melanie Boerries",
      "Oliver Opitz",
      "Alexandros Karargyris",
      "Nicolas Padoy"
    ],
    "abstract": "Clinnova, a collaborative initiative involving France, Germany, Switzerland,\nand Luxembourg, is dedicated to unlocking the power of precision medicine\nthrough data federation, standardization, and interoperability. This European\nGreater Region initiative seeks to create an interoperable European standard\nusing artificial intelligence (AI) and data science to enhance healthcare\noutcomes and efficiency. Key components include multidisciplinary research\ncenters, a federated biobanking strategy, a digital health innovation platform,\nand a federated AI strategy. It targets inflammatory bowel disease, rheumatoid\ndiseases, and multiple sclerosis (MS), emphasizing data quality to develop AI\nalgorithms for personalized treatment and translational research.\n  The IHU Strasbourg (Institute of Minimal-invasive Surgery) has the lead in\nthis initiative to develop the federated learning (FL) proof of concept (POC)\nthat will serve as a foundation for advancing AI in healthcare. At its core,\nClinnova-MS aims to enhance MS patient care by using FL to develop more\naccurate models that detect disease progression, guide interventions, and\nvalidate digital biomarkers across multiple sites. This technical report\npresents insights and key takeaways from the first cross-border federated POC\non MS segmentation of MRI images within the Clinnova framework. While our work\nmarks a significant milestone in advancing MS segmentation through cross-border\ncollaboration, it also underscores the importance of addressing technical,\nlogistical, and ethical considerations to realize the full potential of FL in\nhealthcare settings.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.02443v1",
    "published_date": "2024-10-03 12:40:52 UTC",
    "updated_date": "2024-10-03 12:40:52 UTC"
  },
  {
    "arxiv_id": "2410.02440v1",
    "title": "Optimizing Adaptive Attacks against Content Watermarks for Language Models",
    "authors": [
      "Abdulrahman Diaa",
      "Toluwani Aremu",
      "Nils Lukas"
    ],
    "abstract": "Large Language Models (LLMs) can be \\emph{misused} to spread online spam and\nmisinformation. Content watermarking deters misuse by hiding a message in\nmodel-generated outputs, enabling their detection using a secret watermarking\nkey. Robustness is a core security property, stating that evading detection\nrequires (significant) degradation of the content's quality. Many LLM\nwatermarking methods have been proposed, but robustness is tested only against\n\\emph{non-adaptive} attackers who lack knowledge of the watermarking method and\ncan find only suboptimal attacks. We formulate the robustness of LLM\nwatermarking as an objective function and propose preference-based optimization\nto tune \\emph{adaptive} attacks against the specific watermarking method. Our\nevaluation shows that (i) adaptive attacks substantially outperform\nnon-adaptive baselines. (ii) Even in a non-adaptive setting, adaptive attacks\noptimized against a few known watermarks remain highly effective when tested\nagainst other unseen watermarks, and (iii) optimization-based attacks are\npractical and require less than seven GPU hours. Our findings underscore the\nneed to test robustness against adaptive attackers.",
    "categories": [
      "cs.CR",
      "cs.AI"
    ],
    "primary_category": "cs.CR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.02440v1",
    "published_date": "2024-10-03 12:37:39 UTC",
    "updated_date": "2024-10-03 12:37:39 UTC"
  },
  {
    "arxiv_id": "2410.02430v1",
    "title": "Predictive Attractor Models",
    "authors": [
      "Ramy Mounir",
      "Sudeep Sarkar"
    ],
    "abstract": "Sequential memory, the ability to form and accurately recall a sequence of\nevents or stimuli in the correct order, is a fundamental prerequisite for\nbiological and artificial intelligence as it underpins numerous cognitive\nfunctions (e.g., language comprehension, planning, episodic memory formation,\netc.) However, existing methods of sequential memory suffer from catastrophic\nforgetting, limited capacity, slow iterative learning procedures, low-order\nMarkov memory, and, most importantly, the inability to represent and generate\nmultiple valid future possibilities stemming from the same context. Inspired by\nbiologically plausible neuroscience theories of cognition, we propose\n\\textit{Predictive Attractor Models (PAM)}, a novel sequence memory\narchitecture with desirable generative properties. PAM is a streaming model\nthat learns a sequence in an online, continuous manner by observing each input\n\\textit{only once}. Additionally, we find that PAM avoids catastrophic\nforgetting by uniquely representing past context through lateral inhibition in\ncortical minicolumns, which prevents new memories from overwriting previously\nlearned knowledge. PAM generates future predictions by sampling from a union\nset of predicted possibilities; this generative ability is realized through an\nattractor model trained alongside the predictor. We show that PAM is trained\nwith local computations through Hebbian plasticity rules in a biologically\nplausible framework. Other desirable traits (e.g., noise tolerance, CPU-based\nlearning, capacity scaling) are discussed throughout the paper. Our findings\nsuggest that PAM represents a significant step forward in the pursuit of\nbiologically plausible and computationally efficient sequential memory models,\nwith broad implications for cognitive science and artificial intelligence\nresearch.",
    "categories": [
      "cs.AI",
      "cs.CV",
      "cs.LG",
      "q-bio.NC"
    ],
    "primary_category": "cs.AI",
    "comment": "Accepted to NeurIPS 2024",
    "pdf_url": "http://arxiv.org/pdf/2410.02430v1",
    "published_date": "2024-10-03 12:25:01 UTC",
    "updated_date": "2024-10-03 12:25:01 UTC"
  },
  {
    "arxiv_id": "2410.02429v2",
    "title": "IoT-LLM: Enhancing Real-World IoT Task Reasoning with Large Language Models",
    "authors": [
      "Tuo An",
      "Yunjiao Zhou",
      "Han Zou",
      "Jianfei Yang"
    ],
    "abstract": "Large Language Models (LLMs) have demonstrated remarkable capabilities across\ntextual and visual domains but often generate outputs that violate physical\nlaws, revealing a gap in their understanding of the physical world. Inspired by\nhuman cognition, where perception is fundamental to reasoning, we explore\naugmenting LLMs with enhanced perception abilities using Internet of Things\n(IoT) sensor data and pertinent knowledge for IoT task reasoning in the\nphysical world. In this work, we systematically study LLMs capability to\naddress real-world IoT tasks by augmenting their perception and knowledge base,\nand then propose a unified framework, IoT-LLM, to enhance such capability. In\nIoT-LLM, we customize three steps for LLMs: preprocessing IoT data into formats\namenable to LLMs, activating their commonsense knowledge through\nchain-of-thought prompting and specialized role definitions, and expanding\ntheir understanding via IoT-oriented retrieval-augmented generation based on\nin-context learning. To evaluate the performance, We design a new benchmark\nwith five real-world IoT tasks with different data types and reasoning\ndifficulties and provide the benchmarking results on six open-source and\nclose-source LLMs. Experimental results demonstrate the limitations of existing\nLLMs with naive textual inputs that cannot perform these tasks effectively. We\nshow that IoT-LLM significantly enhances the performance of IoT tasks reasoning\nof LLM, such as GPT-4, achieving an average improvement of 65% across various\ntasks against previous methods. The results also showcase LLMs ability to\ncomprehend IoT data and the physical law behind data by providing a reasoning\nprocess. Limitations of our work are claimed to inspire future research in this\nnew era.",
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.AI",
    "comment": "21 pages, 10 figures, under review",
    "pdf_url": "http://arxiv.org/pdf/2410.02429v2",
    "published_date": "2024-10-03 12:24:18 UTC",
    "updated_date": "2024-10-04 03:30:10 UTC"
  },
  {
    "arxiv_id": "2410.02428v1",
    "title": "Collective Critics for Creative Story Generation",
    "authors": [
      "Minwook Bae",
      "Hyounghun Kim"
    ],
    "abstract": "Generating a long story of several thousand words with narrative coherence\nusing Large Language Models (LLMs) has been a challenging task. Previous\nresearch has addressed this challenge by proposing different frameworks that\ncreate a story plan and generate a long story based on that plan. However,\nthese frameworks have been mainly focusing on maintaining narrative coherence\nin stories, often overlooking creativity in story planning and the\nexpressiveness of the stories generated from those plans, which are desirable\nproperties to captivate readers' interest. In this paper, we propose Collective\nCritics for Creative Story Generation framework (CritiCS), which is composed of\nplan refining stage (CrPlan) and story generation stage (CrText), to integrate\na collective revision mechanism that promotes those properties into long-form\nstory generation process. Specifically, in each stage, a group of LLM critics\nand one leader collaborate to incrementally refine drafts of plan and story\nthroughout multiple rounds. Extensive human evaluation shows that the CritiCS\ncan significantly enhance story creativity and reader engagement, while also\nmaintaining narrative coherence. Furthermore, the design of the framework\nallows active participation from human writers in any role within the critique\nprocess, enabling interactive human-machine collaboration in story writing.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "EMNLP 2024 (36 pages)",
    "pdf_url": "http://arxiv.org/pdf/2410.02428v1",
    "published_date": "2024-10-03 12:21:17 UTC",
    "updated_date": "2024-10-03 12:21:17 UTC"
  },
  {
    "arxiv_id": "2410.02426v1",
    "title": "Learning the Latent Rules of a Game from Data: A Chess Story",
    "authors": [
      "Ben Fauber"
    ],
    "abstract": "We demonstrate that small pretrained foundational generative language models\nwith millions of parameters can learn the latent rules of a process from data\nassociated with the process. Inspired by Stefan Zweig's novella\n\"Schachnovelle,\" also known as \"The Royal Game\" in English, we show that 28M\nand 125M parameter pretrained foundational small language models (SLMs) can be\ninstruction fine-tuned with 1,000-to-1,000,000 examples to learn the rules of\nchess, propose legal moves, and accurately solve chess problems. We also\nexplore the impact of successive language model fine-tuning epochs on improved\noutcomes and demonstrate reductions in model hallucinations by increasing the\nnumber of instruction fine-tuning examples.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.02426v1",
    "published_date": "2024-10-03 12:19:49 UTC",
    "updated_date": "2024-10-03 12:19:49 UTC"
  },
  {
    "arxiv_id": "2410.02401v7",
    "title": "SynCo: Synthetic Hard Negatives for Contrastive Visual Representation Learning",
    "authors": [
      "Nikolaos Giakoumoglou",
      "Tania Stathaki"
    ],
    "abstract": "Contrastive learning has become a dominant approach in self-supervised visual\nrepresentation learning, but efficiently leveraging hard negatives, which are\nsamples closely resembling the anchor, remains challenging. We introduce SynCo\n(Synthetic negatives in Contrastive learning), a novel approach that improves\nmodel performance by generating synthetic hard negatives on the representation\nspace. Building on the MoCo framework, SynCo introduces six strategies for\ncreating diverse synthetic hard negatives on-the-fly with minimal computational\noverhead. SynCo achieves faster training and strong representation learning,\nsurpassing MoCo-v2 by +0.4% and MoCHI by +1.0% on ImageNet ILSVRC-2012 linear\nevaluation. It also transfers more effectively to detection tasks achieving\nstrong results on PASCAL VOC detection (57.2% AP) and significantly improving\nover MoCo-v2 on COCO detection (+1.0% AP) and instance segmentation (+0.8% AP).\nOur synthetic hard negative generation approach significantly enhances visual\nrepresentations learned through self-supervised contrastive learning.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "I.4, I.2"
    ],
    "primary_category": "cs.CV",
    "comment": "Preprint. Code: https://github.com/giakoumoglou/synco, Supplementary:\n  https://giakoumoglou.com/src/synco_suppl.pdf",
    "pdf_url": "http://arxiv.org/pdf/2410.02401v7",
    "published_date": "2024-10-03 11:29:09 UTC",
    "updated_date": "2025-02-17 12:37:09 UTC"
  },
  {
    "arxiv_id": "2410.02396v1",
    "title": "Parameter Competition Balancing for Model Merging",
    "authors": [
      "Guodong Du",
      "Junlin Lee",
      "Jing Li",
      "Runhua Jiang",
      "Yifei Guo",
      "Shuyang Yu",
      "Hanting Liu",
      "Sim Kuan Goh",
      "Ho-Kin Tang",
      "Daojing He",
      "Min Zhang"
    ],
    "abstract": "While fine-tuning pretrained models has become common practice, these models\noften underperform outside their specific domains. Recently developed model\nmerging techniques enable the direct integration of multiple models, each\nfine-tuned for distinct tasks, into a single model. This strategy promotes\nmultitasking capabilities without requiring retraining on the original\ndatasets. However, existing methods fall short in addressing potential\nconflicts and complex correlations between tasks, especially in parameter-level\nadjustments, posing a challenge in effectively balancing parameter competition\nacross various tasks. This paper introduces an innovative technique named\nPCB-Merging (Parameter Competition Balancing), a lightweight and training-free\ntechnique that adjusts the coefficients of each parameter for effective model\nmerging. PCB-Merging employs intra-balancing to gauge parameter significance\nwithin individual tasks and inter-balancing to assess parameter similarities\nacross different tasks. Parameters with low importance scores are dropped, and\nthe remaining ones are rescaled to form the final merged model. We assessed our\napproach in diverse merging scenarios, including cross-task, cross-domain, and\ncross-training configurations, as well as out-of-domain generalization. The\nexperimental results reveal that our approach achieves substantial performance\nenhancements across multiple modalities, domains, model sizes, number of tasks,\nfine-tuning forms, and large language models, outperforming existing model\nmerging methods. The code is publicly available at:\n\\url{https://github.com/duguodong7/pcb-merging}.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "Accepted by NeurIPS2024",
    "pdf_url": "http://arxiv.org/pdf/2410.02396v1",
    "published_date": "2024-10-03 11:17:58 UTC",
    "updated_date": "2024-10-03 11:17:58 UTC"
  },
  {
    "arxiv_id": "2410.02394v1",
    "title": "Online Multi-Label Classification under Noisy and Changing Label Distribution",
    "authors": [
      "Yizhang Zou",
      "Xuegang Hu",
      "Peipei Li",
      "Jun Hu",
      "You Wu"
    ],
    "abstract": "Multi-label data stream usually contains noisy labels in the real-world\napplications, namely occuring in both relevant and irrelevant labels. However,\nexisting online multi-label classification methods are mostly limited in terms\nof label quality and fail to deal with the case of noisy labels. On the other\nhand, the ground-truth label distribution may vary with the time changing,\nwhich is hidden in the observed noisy label distribution and difficult to\ntrack, posing a major challenge for concept drift adaptation. Motivated by\nthis, we propose an online multi-label classification algorithm under Noisy and\nChanging Label Distribution (NCLD). The convex objective is designed to\nsimultaneously model the label scoring and the label ranking for high accuracy,\nwhose robustness to NCLD benefits from three novel works: 1) The local feature\ngraph is used to reconstruct the label scores jointly with the observed labels,\nand an unbiased ranking loss is derived and applied to learn reliable ranking\ninformation. 2) By detecting the difference between two adjacent chunks with\nthe unbiased label cardinality, we identify the change in the ground-truth\nlabel distribution and reset the ranking or all information learned from the\npast to match the new distribution. 3) Efficient and accurate updating is\nachieved based on the updating rule derived from the closed-form optimal model\nsolution. Finally, empirical experimental results validate the effectiveness of\nour method in classifying instances under NCLD.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.02394v1",
    "published_date": "2024-10-03 11:16:43 UTC",
    "updated_date": "2024-10-03 11:16:43 UTC"
  },
  {
    "arxiv_id": "2410.02389v1",
    "title": "Diffusion Meets Options: Hierarchical Generative Skill Composition for Temporally-Extended Tasks",
    "authors": [
      "Zeyu Feng",
      "Hao Luan",
      "Kevin Yuchen Ma",
      "Harold Soh"
    ],
    "abstract": "Safe and successful deployment of robots requires not only the ability to\ngenerate complex plans but also the capacity to frequently replan and correct\nexecution errors. This paper addresses the challenge of long-horizon trajectory\nplanning under temporally extended objectives in a receding horizon manner. To\nthis end, we propose DOPPLER, a data-driven hierarchical framework that\ngenerates and updates plans based on instruction specified by linear temporal\nlogic (LTL). Our method decomposes temporal tasks into chain of options with\nhierarchical reinforcement learning from offline non-expert datasets. It\nleverages diffusion models to generate options with low-level actions. We\ndevise a determinantal-guided posterior sampling technique during batch\ngeneration, which improves the speed and diversity of diffusion generated\noptions, leading to more efficient querying. Experiments on robot navigation\nand manipulation tasks demonstrate that DOPPLER can generate sequences of\ntrajectories that progressively satisfy the specified formulae for obstacle\navoidance and sequential visitation. Demonstration videos are available online\nat: https://philiptheother.github.io/doppler/.",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.RO",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.02389v1",
    "published_date": "2024-10-03 11:10:37 UTC",
    "updated_date": "2024-10-03 11:10:37 UTC"
  },
  {
    "arxiv_id": "2410.02387v3",
    "title": "BiSSL: A Bilevel Optimization Framework for Enhancing the Alignment Between Self-Supervised Pre-Training and Downstream Fine-Tuning",
    "authors": [
      "Gustav Wagner Zakarias",
      "Lars Kai Hansen",
      "Zheng-Hua Tan"
    ],
    "abstract": "This study presents BiSSL, a novel training framework that utilizes bilevel\noptimization to enhance the alignment between the pretext pre-training and\ndownstream fine-tuning stages in self-supervised learning. BiSSL formulates the\npretext and downstream task objectives as the lower- and upper-level objectives\nin a bilevel optimization problem and serves as an intermediate training stage\nwithin the self-supervised learning pipeline. By explicitly modeling the\ninterdependence of these training stages, BiSSL facilitates enhanced\ninformation sharing between them, ultimately leading to a backbone parameter\ninitialization that is better aligned for the downstream task. We propose a\nversatile training algorithm that alternates between optimizing the two\nobjectives defined in BiSSL, which is applicable to a broad range of pretext\nand downstream tasks. Using SimCLR and Bootstrap Your Own Latent to pre-train\nResNet-50 backbones on the ImageNet dataset, we demonstrate that our proposed\nframework significantly outperforms the conventional self-supervised learning\npipeline on the vast majority of 12 downstream image classification datasets,\nas well as on object detection. Visualizations of the backbone features provide\nfurther evidence that BiSSL improves the downstream task alignment of the\nbackbone features prior to fine-tuning.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.02387v3",
    "published_date": "2024-10-03 11:07:43 UTC",
    "updated_date": "2025-01-31 13:14:18 UTC"
  },
  {
    "arxiv_id": "2410.14686v1",
    "title": "Achieving Generalization in Orchestrating GNSS Interference Monitoring Stations Through Pseudo-Labeling",
    "authors": [
      "Lucas Heublein",
      "Tobias Feigl",
      "Alexander Rügamer",
      "Felix Ott"
    ],
    "abstract": "The accuracy of global navigation satellite system (GNSS) receivers is\nsignificantly compromised by interference from jamming devices. Consequently,\nthe detection of these jammers are crucial to mitigating such interference\nsignals. However, robust classification of interference using machine learning\n(ML) models is challenging due to the lack of labeled data in real-world\nenvironments. In this paper, we propose an ML approach that achieves high\ngeneralization in classifying interference through orchestrated monitoring\nstations deployed along highways. We present a semi-supervised approach coupled\nwith an uncertainty-based voting mechanism by combining Monte Carlo and Deep\nEnsembles that effectively minimizes the requirement for labeled training\nsamples to less than 5% of the dataset while improving adaptability across\nvarying environments. Our method demonstrates strong performance when adapted\nfrom indoor environments to real-world scenarios.",
    "categories": [
      "eess.SP",
      "cs.AI",
      "cs.LG",
      "94-05, 82-11",
      "E.0; I.2.0; I.5.4; I.5.1"
    ],
    "primary_category": "eess.SP",
    "comment": "DGON Positioning and Navigation for Intelligent Transport Systems\n  (POSNAV)",
    "pdf_url": "http://arxiv.org/pdf/2410.14686v1",
    "published_date": "2024-10-03 11:07:17 UTC",
    "updated_date": "2024-10-03 11:07:17 UTC"
  },
  {
    "arxiv_id": "2410.02381v4",
    "title": "MetaMetrics: Calibrating Metrics For Generation Tasks Using Human Preferences",
    "authors": [
      "Genta Indra Winata",
      "David Anugraha",
      "Lucky Susanto",
      "Garry Kuwanto",
      "Derry Tanti Wijaya"
    ],
    "abstract": "Understanding the quality of a performance evaluation metric is crucial for\nensuring that model outputs align with human preferences. However, it remains\nunclear how well each metric captures the diverse aspects of these preferences,\nas metrics often excel in one particular area but not across all dimensions. To\naddress this, it is essential to systematically calibrate metrics to specific\naspects of human preference, catering to the unique characteristics of each\naspect. We introduce MetaMetrics, a calibrated meta-metric designed to evaluate\ngeneration tasks across different modalities in a supervised manner.\nMetaMetrics optimizes the combination of existing metrics to enhance their\nalignment with human preferences. Our metric demonstrates flexibility and\neffectiveness in both language and vision downstream tasks, showing significant\nbenefits across various multilingual and multi-domain scenarios. MetaMetrics\naligns closely with human preferences and is highly extendable and easily\nintegrable into any application. This makes MetaMetrics a powerful tool for\nimproving the evaluation of generation tasks, ensuring that metrics are more\nrepresentative of human judgment across diverse contexts.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CV",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted to ICLR 2025",
    "pdf_url": "http://arxiv.org/pdf/2410.02381v4",
    "published_date": "2024-10-03 11:01:25 UTC",
    "updated_date": "2025-02-28 23:33:22 UTC"
  },
  {
    "arxiv_id": "2410.02378v1",
    "title": "Towards Comprehensive Detection of Chinese Harmful Memes",
    "authors": [
      "Junyu Lu",
      "Bo Xu",
      "Xiaokun Zhang",
      "Hongbo Wang",
      "Haohao Zhu",
      "Dongyu Zhang",
      "Liang Yang",
      "Hongfei Lin"
    ],
    "abstract": "This paper has been accepted in the NeurIPS 2024 D & B Track. Harmful memes\nhave proliferated on the Chinese Internet, while research on detecting Chinese\nharmful memes significantly lags behind due to the absence of reliable datasets\nand effective detectors. To this end, we focus on the comprehensive detection\nof Chinese harmful memes. We construct ToxiCN MM, the first Chinese harmful\nmeme dataset, which consists of 12,000 samples with fine-grained annotations\nfor various meme types. Additionally, we propose a baseline detector,\nMultimodal Knowledge Enhancement (MKE), incorporating contextual information of\nmeme content generated by the LLM to enhance the understanding of Chinese\nmemes. During the evaluation phase, we conduct extensive quantitative\nexperiments and qualitative analyses on multiple baselines, including LLMs and\nour MKE. The experimental results indicate that detecting Chinese harmful memes\nis challenging for existing models while demonstrating the effectiveness of\nMKE. The resources for this paper are available at\nhttps://github.com/DUT-lujunyu/ToxiCN_MM.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.02378v1",
    "published_date": "2024-10-03 10:51:02 UTC",
    "updated_date": "2024-10-03 10:51:02 UTC"
  },
  {
    "arxiv_id": "2410.02371v1",
    "title": "NTU-NPU System for Voice Privacy 2024 Challenge",
    "authors": [
      "Nikita Kuzmin",
      "Hieu-Thi Luong",
      "Jixun Yao",
      "Lei Xie",
      "Kong Aik Lee",
      "Eng Siong Chng"
    ],
    "abstract": "In this work, we describe our submissions for the Voice Privacy Challenge\n2024. Rather than proposing a novel speech anonymization system, we enhance the\nprovided baselines to meet all required conditions and improve evaluated\nmetrics. Specifically, we implement emotion embedding and experiment with WavLM\nand ECAPA2 speaker embedders for the B3 baseline. Additionally, we compare\ndifferent speaker and prosody anonymization techniques. Furthermore, we\nintroduce Mean Reversion F0 for B5, which helps to enhance privacy without a\nloss in utility. Finally, we explore disentanglement models, namely $\\beta$-VAE\nand NaturalSpeech3 FACodec.",
    "categories": [
      "eess.AS",
      "cs.AI"
    ],
    "primary_category": "eess.AS",
    "comment": "System description for VPC 2024",
    "pdf_url": "http://arxiv.org/pdf/2410.02371v1",
    "published_date": "2024-10-03 10:45:10 UTC",
    "updated_date": "2024-10-03 10:45:10 UTC"
  },
  {
    "arxiv_id": "2410.02365v1",
    "title": "From Concrete to Abstract: A Multimodal Generative Approach to Abstract Concept Learning",
    "authors": [
      "Haodong Xie",
      "Rahul Singh Maharjan",
      "Federico Tavella",
      "Angelo Cangelosi"
    ],
    "abstract": "Understanding and manipulating concrete and abstract concepts is fundamental\nto human intelligence. Yet, they remain challenging for artificial agents. This\npaper introduces a multimodal generative approach to high order abstract\nconcept learning, which integrates visual and categorical linguistic\ninformation from concrete ones. Our model initially grounds subordinate level\nconcrete concepts, combines them to form basic level concepts, and finally\nabstracts to superordinate level concepts via the grounding of basic-level\nconcepts. We evaluate the model language learning ability through\nlanguage-to-visual and visual-to-language tests with high order abstract\nconcepts. Experimental results demonstrate the proficiency of the model in both\nlanguage understanding and language naming tasks.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.02365v1",
    "published_date": "2024-10-03 10:24:24 UTC",
    "updated_date": "2024-10-03 10:24:24 UTC"
  },
  {
    "arxiv_id": "2410.02362v1",
    "title": "A Comprehensive Survey of Mamba Architectures for Medical Image Analysis: Classification, Segmentation, Restoration and Beyond",
    "authors": [
      "Shubhi Bansal",
      "Sreeharish A",
      "Madhava Prasath J",
      "Manikandan S",
      "Sreekanth Madisetty",
      "Mohammad Zia Ur Rehman",
      "Chandravardhan Singh Raghaw",
      "Gaurav Duggal",
      "Nagendra Kumar"
    ],
    "abstract": "Mamba, a special case of the State Space Model, is gaining popularity as an\nalternative to template-based deep learning approaches in medical image\nanalysis. While transformers are powerful architectures, they have drawbacks,\nincluding quadratic computational complexity and an inability to address\nlong-range dependencies efficiently. This limitation affects the analysis of\nlarge and complex datasets in medical imaging, where there are many spatial and\ntemporal relationships. In contrast, Mamba offers benefits that make it\nwell-suited for medical image analysis. It has linear time complexity, which is\na significant improvement over transformers. Mamba processes longer sequences\nwithout attention mechanisms, enabling faster inference and requiring less\nmemory. Mamba also demonstrates strong performance in merging multimodal data,\nimproving diagnosis accuracy and patient outcomes. The organization of this\npaper allows readers to appreciate the capabilities of Mamba in medical imaging\nstep by step. We begin by defining core concepts of SSMs and models, including\nS4, S5, and S6, followed by an exploration of Mamba architectures such as pure\nMamba, U-Net variants, and hybrid models with convolutional neural networks,\ntransformers, and Graph Neural Networks. We also cover Mamba optimizations,\ntechniques and adaptations, scanning, datasets, applications, experimental\nresults, and conclude with its challenges and future directions in medical\nimaging. This review aims to demonstrate the transformative potential of Mamba\nin overcoming existing barriers within medical imaging while paving the way for\ninnovative advancements in the field. A comprehensive list of Mamba\narchitectures applied in the medical field, reviewed in this work, is available\nat Github.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.02362v1",
    "published_date": "2024-10-03 10:23:03 UTC",
    "updated_date": "2024-10-03 10:23:03 UTC"
  },
  {
    "arxiv_id": "2410.02355v4",
    "title": "AlphaEdit: Null-Space Constrained Knowledge Editing for Language Models",
    "authors": [
      "Junfeng Fang",
      "Houcheng Jiang",
      "Kun Wang",
      "Yunshan Ma",
      "Shi Jie",
      "Xiang Wang",
      "Xiangnan He",
      "Tat-seng Chua"
    ],
    "abstract": "Large language models (LLMs) often exhibit hallucinations due to incorrect or\noutdated knowledge. Hence, model editing methods have emerged to enable\ntargeted knowledge updates. To achieve this, a prevailing paradigm is the\nlocating-then-editing approach, which first locates influential parameters and\nthen edits them by introducing a perturbation. While effective, current studies\nhave demonstrated that this perturbation inevitably disrupt the originally\npreserved knowledge within LLMs, especially in sequential editing scenarios. To\naddress this, we introduce AlphaEdit, a novel solution that projects\nperturbation onto the null space of the preserved knowledge before applying it\nto the parameters. We theoretically prove that this projection ensures the\noutput of post-edited LLMs remains unchanged when queried about the preserved\nknowledge, thereby mitigating the issue of disruption. Extensive experiments on\nvarious LLMs, including LLaMA3, GPT2-XL, and GPT-J, show that AlphaEdit boosts\nthe performance of most locating-then-editing methods by an average of 36.7%\nwith a single line of additional code for projection solely. Our code is\navailable at: https://github.com/jianghoucheng/AlphaEdit.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.02355v4",
    "published_date": "2024-10-03 10:06:27 UTC",
    "updated_date": "2025-04-22 16:15:47 UTC"
  },
  {
    "arxiv_id": "2410.02338v2",
    "title": "How Much Can RAG Help the Reasoning of LLM?",
    "authors": [
      "Jingyu Liu",
      "Jiaen Lin",
      "Yong Liu"
    ],
    "abstract": "Retrieval-Augmented Generation (RAG) has gained significant popularity in\nmodern Large Language Models (LLMs) due to its effectiveness in introducing new\nknowledge and reducing hallucinations. However, the deep understanding of RAG\nremains limited, how does RAG help the reasoning process and can RAG help\nimprove the reasoning capability remains question. While external documents are\ntypically considered as a method to incorporate domain-specific information,\nthey also contain intermediate reasoning results related to the query, this\nsuggests that documents could enhance the reasoning capability of LLMs, which\nhas not been previously explored. In this paper, we investigate this issue in\ndepth and find that while RAG can assist with reasoning, the help is limited.\nIf we conceptualize the reasoning process as a tree with fixed depth, then RAG\nstruggles to assist LLMs in performing deeper reasoning. Additionally, the\ninformation in the documents requires preprocessing to filter out noise. We\ndemonstrate that this preprocessing is difficult to achieve simply fine-tuning\nof the LLM, it often necessitates numerous additional transformer layers to\nsolve the problem. To simplify the problem, we propose DPrompt tuning, which\neffectively resolves the issue within just limited transformer layers, leading\nto improved performance.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.02338v2",
    "published_date": "2024-10-03 09:48:09 UTC",
    "updated_date": "2024-10-04 14:59:04 UTC"
  },
  {
    "arxiv_id": "2410.02326v1",
    "title": "Autonomous Self-Trained Channel State Prediction Method for mmWave Vehicular Communications",
    "authors": [
      "Abidemi Orimogunje",
      "Vukan Ninkovic",
      "Evariste Twahirwa",
      "Gaspard Gashema",
      "Dejan Vukobratovic"
    ],
    "abstract": "Establishing and maintaining 5G mmWave vehicular connectivity poses a\nsignificant challenge due to high user mobility that necessitates frequent\ntriggering of beam switching procedures. Departing from reactive beam switching\nbased on the user device channel state feedback, proactive beam switching\nprepares in advance for upcoming beam switching decisions by exploiting\naccurate channel state information (CSI) prediction. In this paper, we develop\na framework for autonomous self-trained CSI prediction for mmWave vehicular\nusers where a base station (gNB) collects and labels a dataset that it uses for\ntraining recurrent neural network (RNN)-based CSI prediction model. The\nproposed framework exploits the CSI feedback from vehicular users combined with\noverhearing the C-V2X cooperative awareness messages (CAMs) they broadcast. We\nimplement and evaluate the proposed framework using deepMIMO dataset generation\nenvironment and demonstrate its capability to provide accurate CSI prediction\nfor 5G mmWave vehicular users. CSI prediction model is trained and its\ncapability to provide accurate CSI predictions from various input features are\ninvestigated.",
    "categories": [
      "eess.SP",
      "cs.AI"
    ],
    "primary_category": "eess.SP",
    "comment": "Accepted for publication at European Wireless 2024",
    "pdf_url": "http://arxiv.org/pdf/2410.02326v1",
    "published_date": "2024-10-03 09:20:48 UTC",
    "updated_date": "2024-10-03 09:20:48 UTC"
  },
  {
    "arxiv_id": "2410.02320v3",
    "title": "Post-edits Are Preferences Too",
    "authors": [
      "Nathaniel Berger",
      "Miriam Exel",
      "Matthias Huck",
      "Stefan Riezler"
    ],
    "abstract": "Preference Optimization (PO) techniques are currently one of the state of the\nart techniques for fine-tuning large language models (LLMs) on pairwise\npreference feedback from human annotators. However, in machine translation,\nthis sort of feedback can be difficult to solicit. Additionally, Kreutzer et\nal. (2018) have shown that, for machine translation, pairwise preferences are\nless reliable than other forms of human feedback, such as 5-point ratings.\n  We examine post-edits to see if they can be a source of reliable human\npreferences by construction. In PO, a human annotator is shown sequences $s_1$\nand $s_2$ and asked for a preference judgment, %$s_1 > s_2$; while for\npost-editing, editors create $s_1$ and know that it should be better than\n$s_2$. We attempt to use these implicit preferences for PO and show that it\nhelps the model move towards post-edit-like hypotheses and away from machine\ntranslation-like hypotheses. Furthermore, we show that best results are\nobtained by pre-training the model with supervised fine-tuning (SFT) on\npost-edits in order to promote post-edit-like hypotheses to the top output\nranks.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "To appear at the Ninth Conference on Machine Translation (WMT24)",
    "pdf_url": "http://arxiv.org/pdf/2410.02320v3",
    "published_date": "2024-10-03 08:56:29 UTC",
    "updated_date": "2025-02-21 11:53:53 UTC"
  },
  {
    "arxiv_id": "2410.02316v1",
    "title": "CTARR: A fast and robust method for identifying anatomical regions on CT images via atlas registration",
    "authors": [
      "Thomas Buddenkotte",
      "Roland Opfer",
      "Julia Krüger",
      "Alessa Hering",
      "Mireia Crispin-Ortuzar"
    ],
    "abstract": "Medical image analysis tasks often focus on regions or structures located in\na particular location within the patient's body. Often large parts of the image\nmay not be of interest for the image analysis task. When using deep-learning\nbased approaches, this causes an unnecessary increases the computational burden\nduring inference and raises the chance of errors. In this paper, we introduce\nCTARR, a novel generic method for CT Anatomical Region Recognition. The method\nserves as a pre-processing step for any deep learning-based CT image analysis\npipeline by automatically identifying the pre-defined anatomical region that is\nrelevant for the follow-up task and removing the rest. It can be used in (i)\nimage segmentation to prevent false positives in anatomically implausible\nregions and speeding up the inference, (ii) image classification to produce\nimage crops that are consistent in their anatomical context, and (iii) image\nregistration by serving as a fast pre-registration step. Our proposed method is\nbased on atlas registration and provides a fast and robust way to crop any\nanatomical region encoded as one or multiple bounding box(es) from any\nunlabeled CT scan of the brain, chest, abdomen and/or pelvis. We demonstrate\nthe utility and robustness of the proposed method in the context of medical\nimage segmentation by evaluating it on six datasets of public segmentation\nchallenges. The foreground voxels in the regions of interest are preserved in\nthe vast majority of cases and tasks (97.45-100%) while taking only fractions\nof a seconds to compute (0.1-0.21s) on a deep learning workstation and greatly\nreducing the segmentation runtime (2.0-12.7x). Our code is available at\nhttps://github.com/ThomasBudd/ctarr.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.02316v1",
    "published_date": "2024-10-03 08:52:21 UTC",
    "updated_date": "2024-10-03 08:52:21 UTC"
  },
  {
    "arxiv_id": "2410.03777v2",
    "title": "Determine-Then-Ensemble: Necessity of Top-k Union for Large Language Model Ensembling",
    "authors": [
      "Yuxuan Yao",
      "Han Wu",
      "Mingyang Liu",
      "Sichun Luo",
      "Xiongwei Han",
      "Jie Liu",
      "Zhijiang Guo",
      "Linqi Song"
    ],
    "abstract": "Large language models (LLMs) exhibit varying strengths and weaknesses across\ndifferent tasks, prompting recent studies to explore the benefits of ensembling\nmodels to leverage their complementary advantages. However, existing LLM\nensembling methods often overlook model compatibility and struggle with\ninefficient alignment of probabilities across the entire vocabulary. In this\nstudy, we empirically investigate the factors influencing ensemble performance,\nidentifying model performance, vocabulary size, and response style as key\ndeterminants, revealing that compatibility among models is essential for\neffective ensembling. This analysis leads to the development of a simple yet\neffective model selection strategy that identifies compatible models.\nAdditionally, we introduce the \\textsc{Uni}on \\textsc{T}op-$k$\n\\textsc{E}nsembling (\\textsc{UniTE}), a novel approach that efficiently\ncombines models by focusing on the union of the top-k tokens from each model,\nthereby avoiding the need for full vocabulary alignment and reducing\ncomputational overhead. Extensive evaluations across multiple benchmarks\ndemonstrate that \\textsc{UniTE} significantly enhances performance compared to\nexisting methods, offering a more efficient framework for LLM ensembling.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.03777v2",
    "published_date": "2024-10-03 08:42:38 UTC",
    "updated_date": "2025-02-25 12:40:51 UTC"
  },
  {
    "arxiv_id": "2410.02283v1",
    "title": "Morphological evaluation of subwords vocabulary used by BETO language model",
    "authors": [
      "Óscar García-Sierra",
      "Ana Fernández-Pampillón Cesteros",
      "Miguel Ortega-Martín"
    ],
    "abstract": "Subword tokenization algorithms used by Large Language Models are\nsignificantly more efficient and can independently build the necessary\nvocabulary of words and subwords without human intervention. However, those\nsubwords do not always align with real morphemes, potentially impacting the\nmodels' performance, though it remains uncertain when this might occur. In\nprevious research, we proposed a method to assess the morphological quality of\nvocabularies, focusing on the overlap between these vocabularies and the\nmorphemes of a given language. Our evaluation method was built on three quality\nmeasures, relevance, cohesion, and morphological accuracy, and a procedure for\ntheir assessment. By applying this method to vocabularies created by three\nsubword tokenization algorithms, BPE, Wordpiece, and Unigram, we concluded that\nthese vocabularies generally exhibit very low morphological quality. In this\narticle, we apply this evaluation to the tokenizer of BETO, a BERT language\nmodel trained on large Spanish corpora. This evaluation, along with our\nprevious results, helped us conclude that its vocabulary has a low\nmorphological quality, and we also found that training the tokenizer in a\nlarger corpus does not improve the morphological quality of the generated\nvocabulary. Additionally, this evaluation helps clarify the algorithm used by\nthe tokenizer, that is, Wordpiece, given the inconsistencies between the\nauthors' claims and the model's configuration.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "in Spanish language",
    "pdf_url": "http://arxiv.org/pdf/2410.02283v1",
    "published_date": "2024-10-03 08:07:14 UTC",
    "updated_date": "2024-10-03 08:07:14 UTC"
  },
  {
    "arxiv_id": "2410.14685v1",
    "title": "Leveraging Event Streams with Deep Reinforcement Learning for End-to-End UAV Tracking",
    "authors": [
      "Ala Souissi",
      "Hajer Fradi",
      "Panagiotis Papadakis"
    ],
    "abstract": "In this paper, we present our proposed approach for active tracking to\nincrease the autonomy of Unmanned Aerial Vehicles (UAVs) using event cameras,\nlow-energy imaging sensors that offer significant advantages in speed and\ndynamic range. The proposed tracking controller is designed to respond to\nvisual feedback from the mounted event sensor, adjusting the drone movements to\nfollow the target. To leverage the full motion capabilities of a quadrotor and\nthe unique properties of event sensors, we propose an end-to-end\ndeep-reinforcement learning (DRL) framework that maps raw sensor data from\nevent streams directly to control actions for the UAV. To learn an optimal\npolicy under highly variable and challenging conditions, we opt for a\nsimulation environment with domain randomization for effective transfer to\nreal-world environments. We demonstrate the effectiveness of our approach\nthrough experiments in challenging scenarios, including fast-moving targets and\nchanging lighting conditions, which result in improved generalization\ncapabilities.",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.NE"
    ],
    "primary_category": "cs.RO",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.14685v1",
    "published_date": "2024-10-03 07:56:40 UTC",
    "updated_date": "2024-10-03 07:56:40 UTC"
  },
  {
    "arxiv_id": "2410.02271v1",
    "title": "CoLLAP: Contrastive Long-form Language-Audio Pretraining with Musical Temporal Structure Augmentation",
    "authors": [
      "Junda Wu",
      "Warren Li",
      "Zachary Novack",
      "Amit Namburi",
      "Carol Chen",
      "Julian McAuley"
    ],
    "abstract": "Modeling temporal characteristics plays a significant role in the\nrepresentation learning of audio waveform. We propose Contrastive Long-form\nLanguage-Audio Pretraining (\\textbf{CoLLAP}) to significantly extend the\nperception window for both the input audio (up to 5 minutes) and the language\ndescriptions (exceeding 250 words), while enabling contrastive learning across\nmodalities and temporal dynamics. Leveraging recent Music-LLMs to generate\nlong-form music captions for full-length songs, augmented with musical temporal\nstructures, we collect 51.3K audio-text pairs derived from the large-scale\nAudioSet training dataset, where the average audio length reaches 288 seconds.\nWe propose a novel contrastive learning architecture that fuses language\nrepresentations with structured audio representations by segmenting each song\ninto clips and extracting their embeddings. With an attention mechanism, we\ncapture multimodal temporal correlations, allowing the model to automatically\nweigh and enhance the final fusion score for improved contrastive alignment.\nFinally, we develop two variants of the CoLLAP model with different types of\nbackbone language models. Through comprehensive experiments on multiple\nlong-form music-text retrieval datasets, we demonstrate consistent performance\nimprovement in retrieval accuracy compared with baselines. We also show the\npretrained CoLLAP models can be transferred to various music information\nretrieval tasks, with heterogeneous long-form multimodal contexts.",
    "categories": [
      "cs.SD",
      "cs.AI",
      "eess.AS"
    ],
    "primary_category": "cs.SD",
    "comment": "4 pages",
    "pdf_url": "http://arxiv.org/pdf/2410.02271v1",
    "published_date": "2024-10-03 07:46:51 UTC",
    "updated_date": "2024-10-03 07:46:51 UTC"
  },
  {
    "arxiv_id": "2410.02268v3",
    "title": "Structural-Entropy-Based Sample Selection for Efficient and Effective Learning",
    "authors": [
      "Tianchi Xie",
      "Jiangning Zhu",
      "Guozu Ma",
      "Minzhi Lin",
      "Wei Chen",
      "Weikai Yang",
      "Shixia Liu"
    ],
    "abstract": "Sample selection improves the efficiency and effectiveness of machine\nlearning models by providing informative and representative samples. Typically,\nsamples can be modeled as a sample graph, where nodes are samples and edges\nrepresent their similarities. Most existing methods are based on local\ninformation, such as the training difficulty of samples, thereby overlooking\nglobal information, such as connectivity patterns. This oversight can result in\nsuboptimal selection because global information is crucial for ensuring that\nthe selected samples well represent the structural properties of the graph. To\naddress this issue, we employ structural entropy to quantify global information\nand losslessly decompose it from the whole graph to individual nodes using the\nShapley value. Based on the decomposition, we present\n$\\textbf{S}$tructural-$\\textbf{E}$ntropy-based sample $\\textbf{S}$election\n($\\textbf{SES}$), a method that integrates both global and local information to\nselect informative and representative samples. SES begins by constructing a\n$k$NN-graph among samples based on their similarities. It then measures sample\nimportance by combining structural entropy (global metric) with training\ndifficulty (local metric). Finally, SES applies importance-biased blue noise\nsampling to select a set of diverse and representative samples. Comprehensive\nexperiments on three learning scenarios -- supervised learning, active\nlearning, and continual learning -- clearly demonstrate the effectiveness of\nour method.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL",
      "cs.CV"
    ],
    "primary_category": "cs.LG",
    "comment": "Published as a conference paper at ICLR 2025",
    "pdf_url": "http://arxiv.org/pdf/2410.02268v3",
    "published_date": "2024-10-03 07:40:14 UTC",
    "updated_date": "2025-03-03 05:32:47 UTC"
  },
  {
    "arxiv_id": "2410.02253v2",
    "title": "From Imitation to Exploration: End-to-end Autonomous Driving based on World Model",
    "authors": [
      "Yueyuan Li",
      "Mingyang Jiang",
      "Songan Zhang",
      "Wei Yuan",
      "Chunxiang Wang",
      "Ming Yang"
    ],
    "abstract": "In recent years, end-to-end autonomous driving architectures have gained\nincreasing attention due to their advantage in avoiding error accumulation.\nMost existing end-to-end autonomous driving methods are based on Imitation\nLearning (IL), which can quickly derive driving strategies by mimicking expert\nbehaviors. However, IL often struggles to handle scenarios outside the training\ndataset, especially in high-dynamic and interaction-intensive traffic\nenvironments. In contrast, Reinforcement Learning (RL)-based driving models can\noptimize driving decisions through interaction with the environment, improving\nadaptability and robustness.\n  To leverage the strengths of both IL and RL, we propose RAMBLE, an end-to-end\nworld model-based RL method for driving decision-making. RAMBLE extracts\nenvironmental context information from RGB images and LiDAR data through an\nasymmetrical variational autoencoder. A transformer-based architecture is then\nused to capture the dynamic transitions of traffic participants. Next, an\nactor-critic structure reinforcement learning algorithm is applied to derive\ndriving strategies based on the latent features of the current state and\ndynamics. To accelerate policy convergence and ensure stable training, we\nintroduce a training scheme that initializes the policy network using IL, and\nemploys KL loss and soft update mechanisms to smoothly transition the model\nfrom IL to RL.\n  RAMBLE achieves state-of-the-art performance in route completion rate on the\nCARLA Leaderboard 1.0 and completes all 38 scenarios on the CARLA Leaderboard\n2.0, demonstrating its effectiveness in handling complex and dynamic traffic\nscenarios. The model will be open-sourced upon paper acceptance at\nhttps://github.com/SCP-CN-001/ramble to support further research and\ndevelopment in autonomous driving.",
    "categories": [
      "cs.AI",
      "cs.LG",
      "cs.RO"
    ],
    "primary_category": "cs.AI",
    "comment": "12 pages, 4 figures, 3 tables; T-ITS under review",
    "pdf_url": "http://arxiv.org/pdf/2410.02253v2",
    "published_date": "2024-10-03 06:45:59 UTC",
    "updated_date": "2025-04-20 06:05:58 UTC"
  },
  {
    "arxiv_id": "2410.02246v2",
    "title": "PFGuard: A Generative Framework with Privacy and Fairness Safeguards",
    "authors": [
      "Soyeon Kim",
      "Yuji Roh",
      "Geon Heo",
      "Steven Euijong Whang"
    ],
    "abstract": "Generative models must ensure both privacy and fairness for Trustworthy AI.\nWhile these goals have been pursued separately, recent studies propose to\ncombine existing privacy and fairness techniques to achieve both goals.\nHowever, naively combining these techniques can be insufficient due to\nprivacy-fairness conflicts, where a sample in a minority group may be\nrepresented in ways that support fairness, only to be suppressed for privacy.\nWe demonstrate how these conflicts lead to adverse effects, such as privacy\nviolations and unexpected fairness-utility tradeoffs. To mitigate these risks,\nwe propose PFGuard, a generative framework with privacy and fairness\nsafeguards, which simultaneously addresses privacy, fairness, and utility. By\nusing an ensemble of multiple teacher models, PFGuard balances privacy-fairness\nconflicts between fair and private training stages and achieves high utility\nbased on ensemble learning. Extensive experiments show that PFGuard\nsuccessfully generates synthetic data on high-dimensional data while providing\nboth DP guarantees and convergence in fair generative modeling.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "In Proceedings of the 13th International Conference on Learning\n  Representations (ICLR), 2025",
    "pdf_url": "http://arxiv.org/pdf/2410.02246v2",
    "published_date": "2024-10-03 06:37:16 UTC",
    "updated_date": "2025-02-28 09:03:21 UTC"
  },
  {
    "arxiv_id": "2410.02242v2",
    "title": "Robust Weight Initialization for Tanh Neural Networks with Fixed Point Analysis",
    "authors": [
      "Hyunwoo Lee",
      "Hayoung Choi",
      "Hyunju Kim"
    ],
    "abstract": "As a neural network's depth increases, it can improve generalization\nperformance. However, training deep networks is challenging due to gradient and\nsignal propagation issues. To address these challenges, extensive theoretical\nresearch and various methods have been introduced. Despite these advances,\neffective weight initialization methods for tanh neural networks remain\ninsufficiently investigated. This paper presents a novel weight initialization\nmethod for neural networks with tanh activation function. Based on an analysis\nof the fixed points of the function $\\tanh(ax)$, the proposed method aims to\ndetermine values of $a$ that mitigate activation saturation. A series of\nexperiments on various classification datasets and physics-informed neural\nnetworks demonstrates that the proposed method outperforms Xavier\ninitialization methods~(with or without normalization) in terms of robustness\nacross different network sizes, data efficiency, and convergence speed. Code is\navailable at https://github.com/1HyunwooLee/Tanh-Init",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "ICLR 2025",
    "pdf_url": "http://arxiv.org/pdf/2410.02242v2",
    "published_date": "2024-10-03 06:30:27 UTC",
    "updated_date": "2025-03-02 11:32:27 UTC"
  },
  {
    "arxiv_id": "2410.02240v6",
    "title": "SCA: Improve Semantic Consistent in Unrestricted Adversarial Attacks via DDPM Inversion",
    "authors": [
      "Zihao Pan",
      "Lifeng Chen",
      "Weibin Wu",
      "Yuhang Cao",
      "Zibin Zheng"
    ],
    "abstract": "Systems based on deep neural networks are vulnerable to adversarial attacks.\nUnrestricted adversarial attacks typically manipulate the semantic content of\nan image (e.g., color or texture) to create adversarial examples that are both\neffective and photorealistic. Recent works have utilized the diffusion\ninversion process to map images into a latent space, where high-level semantics\nare manipulated by introducing perturbations. However, they often result in\nsubstantial semantic distortions in the denoised output and suffer from low\nefficiency. In this study, we propose a novel framework called\nSemantic-Consistent Unrestricted Adversarial Attacks (SCA), which employs an\ninversion method to extract edit-friendly noise maps and utilizes a Multimodal\nLarge Language Model (MLLM) to provide semantic guidance throughout the\nprocess. Under the condition of rich semantic information provided by MLLM, we\nperform the DDPM denoising process of each step using a series of edit-friendly\nnoise maps and leverage DPM Solver++ to accelerate this process, enabling\nefficient sampling with semantic consistency. Compared to existing methods, our\nframework enables the efficient generation of adversarial examples that exhibit\nminimal discernible semantic changes. Consequently, we for the first time\nintroduce Semantic-Consistent Adversarial Examples (SCAE). Extensive\nexperiments and visualizations have demonstrated the high efficiency of SCA,\nparticularly in being on average 12 times faster than the state-of-the-art\nattacks. Our code can be found at https://github.com/Pan-Zihao/SCA.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.02240v6",
    "published_date": "2024-10-03 06:25:53 UTC",
    "updated_date": "2025-05-12 18:56:21 UTC"
  },
  {
    "arxiv_id": "2410.02231v1",
    "title": "SEAL: SEmantic-Augmented Imitation Learning via Language Model",
    "authors": [
      "Chengyang Gu",
      "Yuxin Pan",
      "Haotian Bai",
      "Hui Xiong",
      "Yize Chen"
    ],
    "abstract": "Hierarchical Imitation Learning (HIL) is a promising approach for tackling\nlong-horizon decision-making tasks. While it is a challenging task due to the\nlack of detailed supervisory labels for sub-goal learning, and reliance on\nhundreds to thousands of expert demonstrations. In this work, we introduce\nSEAL, a novel framework that leverages Large Language Models (LLMs)'s powerful\nsemantic and world knowledge for both specifying sub-goal space and\npre-labeling states to semantically meaningful sub-goal representations without\nprior knowledge of task hierarchies. SEAL employs a dual-encoder structure,\ncombining supervised LLM-guided sub-goal learning with unsupervised Vector\nQuantization (VQ) for more robust sub-goal representations. Additionally, SEAL\nincorporates a transition-augmented low-level planner for improved adaptation\nto sub-goal transitions. Our experiments demonstrate that SEAL outperforms\nstate-of-the-art HIL methods and LLM-based planning approaches, particularly in\nsettings with small expert datasets and complex long-horizon tasks.",
    "categories": [
      "cs.AI",
      "cs.LG",
      "cs.SY",
      "eess.SY"
    ],
    "primary_category": "cs.AI",
    "comment": "18 pages, 5 figures, in submission",
    "pdf_url": "http://arxiv.org/pdf/2410.02231v1",
    "published_date": "2024-10-03 05:53:10 UTC",
    "updated_date": "2024-10-03 05:53:10 UTC"
  },
  {
    "arxiv_id": "2410.02229v1",
    "title": "CodePMP: Scalable Preference Model Pretraining for Large Language Model Reasoning",
    "authors": [
      "Huimu Yu",
      "Xing Wu",
      "Weidong Yin",
      "Debing Zhang",
      "Songlin Hu"
    ],
    "abstract": "Large language models (LLMs) have made significant progress in natural\nlanguage understanding and generation, driven by scalable pretraining and\nadvanced finetuning. However, enhancing reasoning abilities in LLMs,\nparticularly via reinforcement learning from human feedback (RLHF), remains\nchallenging due to the scarcity of high-quality preference data, which is\nlabor-intensive to annotate and crucial for reward model (RM) finetuning. To\nalleviate this issue, we introduce CodePMP, a scalable preference model\npretraining (PMP) pipeline that utilizes a large corpus of synthesized\ncode-preference pairs from publicly available high-quality source code. CodePMP\nimproves RM finetuning efficiency by pretraining preference models on\nlarge-scale synthesized code-preference pairs. We evaluate CodePMP on\nmathematical reasoning tasks (GSM8K, MATH) and logical reasoning tasks (ReClor,\nLogiQA2.0), consistently showing significant improvements in reasoning\nperformance of LLMs and highlighting the importance of scalable preference\nmodel pretraining for efficient reward modeling.",
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.AI",
    "comment": "work in progress",
    "pdf_url": "http://arxiv.org/pdf/2410.02229v1",
    "published_date": "2024-10-03 05:51:26 UTC",
    "updated_date": "2024-10-03 05:51:26 UTC"
  },
  {
    "arxiv_id": "2410.14684v2",
    "title": "RepoGraph: Enhancing AI Software Engineering with Repository-level Code Graph",
    "authors": [
      "Siru Ouyang",
      "Wenhao Yu",
      "Kaixin Ma",
      "Zilin Xiao",
      "Zhihan Zhang",
      "Mengzhao Jia",
      "Jiawei Han",
      "Hongming Zhang",
      "Dong Yu"
    ],
    "abstract": "Large Language Models (LLMs) excel in code generation yet struggle with\nmodern AI software engineering tasks. Unlike traditional function-level or\nfile-level coding tasks, AI software engineering requires not only basic coding\nproficiency but also advanced skills in managing and interacting with code\nrepositories. However, existing methods often overlook the need for\nrepository-level code understanding, which is crucial for accurately grasping\nthe broader context and developing effective solutions. On this basis, we\npresent RepoGraph, a plug-in module that manages a repository-level structure\nfor modern AI software engineering solutions. RepoGraph offers the desired\nguidance and serves as a repository-wide navigation for AI software engineers.\nWe evaluate RepoGraph on the SWE-bench by plugging it into four different\nmethods of two lines of approaches, where RepoGraph substantially boosts the\nperformance of all systems, leading to a new state-of-the-art among open-source\nframeworks. Our analyses also demonstrate the extensibility and flexibility of\nRepoGraph by testing on another repo-level coding benchmark, CrossCodeEval. Our\ncode is available at https://github.com/ozyyshr/RepoGraph.",
    "categories": [
      "cs.SE",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.SE",
    "comment": "ICLR 2025",
    "pdf_url": "http://arxiv.org/pdf/2410.14684v2",
    "published_date": "2024-10-03 05:45:26 UTC",
    "updated_date": "2025-03-18 20:26:58 UTC"
  },
  {
    "arxiv_id": "2410.02223v2",
    "title": "EmbedLLM: Learning Compact Representations of Large Language Models",
    "authors": [
      "Richard Zhuang",
      "Tianhao Wu",
      "Zhaojin Wen",
      "Andrew Li",
      "Jiantao Jiao",
      "Kannan Ramchandran"
    ],
    "abstract": "With hundreds of thousands of language models available on Huggingface today,\nefficiently evaluating and utilizing these models across various downstream,\ntasks has become increasingly critical. Many existing methods repeatedly learn\ntask-specific representations of Large Language Models (LLMs), which leads to\ninefficiencies in both time and computational resources. To address this, we\npropose EmbedLLM, a framework designed to learn compact vector representations,\nof LLMs that facilitate downstream applications involving many models, such as\nmodel routing. We introduce an encoder-decoder approach for learning such\nembeddings, along with a systematic framework to evaluate their effectiveness.\nEmpirical results show that EmbedLLM outperforms prior methods in model routing\nboth in accuracy and latency. Additionally, we demonstrate that our method can\nforecast a model's performance on multiple benchmarks, without incurring\nadditional inference cost. Extensive probing experiments validate that the\nlearned embeddings capture key model characteristics, e.g. whether the model is\nspecialized for coding tasks, even without being explicitly trained on them. We\nopen source our dataset, code and embedder to facilitate further research and\napplication.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.02223v2",
    "published_date": "2024-10-03 05:43:24 UTC",
    "updated_date": "2024-10-16 22:23:00 UTC"
  },
  {
    "arxiv_id": "2410.02220v4",
    "title": "Data to Defense: The Role of Curation in Customizing LLMs Against Jailbreaking Attacks",
    "authors": [
      "Xiaoqun Liu",
      "Jiacheng Liang",
      "Luoxi Tang",
      "Muchao Ye",
      "Weicheng Ma",
      "Zhaohan Xi"
    ],
    "abstract": "Large language models (LLMs) are widely adapted for downstream applications\nthrough fine-tuning, a process named customization. However, recent studies\nhave identified a vulnerability during this process, where malicious samples\ncan compromise the robustness of LLMs and amplify harmful behaviors-an attack\ncommonly referred to as jailbreaking. To address this challenge, we propose an\nadaptive data curation approach allowing any text to be curated to enhance its\neffectiveness in counteracting harmful samples during customization. To avoid\nthe need for additional defensive modules, we further introduce a comprehensive\nmitigation framework spanning the lifecycle of the customization process:\nbefore customization to immunize LLMs against future jailbreak attempts, during\ncustomization to neutralize risks, and after customization to restore\ncompromised models. Experimental results demonstrate a significant reduction in\njailbreaking effects, achieving up to a 100% success rate in generating safe\nresponses. By combining adaptive data curation with lifecycle-based mitigation\nstrategies, this work represents a solid step forward in mitigating\njailbreaking risks and ensuring the secure adaptation of LLMs.",
    "categories": [
      "cs.CR",
      "cs.AI"
    ],
    "primary_category": "cs.CR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.02220v4",
    "published_date": "2024-10-03 05:24:38 UTC",
    "updated_date": "2025-02-18 05:47:09 UTC"
  },
  {
    "arxiv_id": "2410.02219v2",
    "title": "Multi-modal clothing recommendation model based on large model and VAE enhancement",
    "authors": [
      "Bingjie Huang",
      "Qingyi Lu",
      "Shuaishuai Huang",
      "Xue-she Wang",
      "Haowei Yang"
    ],
    "abstract": "Accurately recommending products has long been a subject requiring in-depth\nresearch. This study proposes a multimodal paradigm for clothing\nrecommendations. Specifically, it designs a multimodal analysis method that\nintegrates clothing description texts and images, utilizing a pre-trained large\nlanguage model to deeply explore the hidden meanings of users and products.\nAdditionally, a variational encoder is employed to learn the relationship\nbetween user information and products to address the cold start problem in\nrecommendation systems. This study also validates the significant performance\nadvantages of this method over various recommendation system methods through\nextensive ablation experiments, providing crucial practical guidance for the\ncomprehensive optimization of recommendation systems.",
    "categories": [
      "cs.IR",
      "cs.AI"
    ],
    "primary_category": "cs.IR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.02219v2",
    "published_date": "2024-10-03 05:23:39 UTC",
    "updated_date": "2024-10-20 02:40:57 UTC"
  },
  {
    "arxiv_id": "2410.05292v1",
    "title": "CaLMFlow: Volterra Flow Matching using Causal Language Models",
    "authors": [
      "Sizhuang He",
      "Daniel Levine",
      "Ivan Vrkic",
      "Marco Francesco Bressana",
      "David Zhang",
      "Syed Asad Rizvi",
      "Yangtian Zhang",
      "Emanuele Zappala",
      "David van Dijk"
    ],
    "abstract": "We introduce CaLMFlow (Causal Language Models for Flow Matching), a novel\nframework that casts flow matching as a Volterra integral equation (VIE),\nleveraging the power of large language models (LLMs) for continuous data\ngeneration. CaLMFlow enables the direct application of LLMs to learn complex\nflows by formulating flow matching as a sequence modeling task, bridging\ndiscrete language modeling and continuous generative modeling. Our method\nimplements tokenization across space and time, thereby solving a VIE over these\ndomains. This approach enables efficient handling of high-dimensional data and\noutperforms ODE solver-dependent methods like conditional flow matching (CFM).\nWe demonstrate CaLMFlow's effectiveness on synthetic and real-world data,\nincluding single-cell perturbation response prediction, showcasing its ability\nto incorporate textual context and generalize to unseen conditions. Our results\nhighlight LLM-driven flow matching as a promising paradigm in generative\nmodeling, offering improved scalability, flexibility, and context-awareness.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "q-bio.QM"
    ],
    "primary_category": "cs.LG",
    "comment": "10 pages, 9 figures, 7 tables",
    "pdf_url": "http://arxiv.org/pdf/2410.05292v1",
    "published_date": "2024-10-03 05:07:41 UTC",
    "updated_date": "2024-10-03 05:07:41 UTC"
  },
  {
    "arxiv_id": "2410.14683v1",
    "title": "Brain-Aware Readout Layers in GNNs: Advancing Alzheimer's early Detection and Neuroimaging",
    "authors": [
      "Jiwon Youn",
      "Dong Woo Kang",
      "Hyun Kook Lim",
      "Mansu Kim"
    ],
    "abstract": "Alzheimer's disease (AD) is a neurodegenerative disorder characterized by\nprogressive memory and cognitive decline, affecting millions worldwide.\nDiagnosing AD is challenging due to its heterogeneous nature and variable\nprogression. This study introduces a novel brain-aware readout layer (BA\nreadout layer) for Graph Neural Networks (GNNs), designed to improve\ninterpretability and predictive accuracy in neuroimaging for early AD\ndiagnosis. By clustering brain regions based on functional connectivity and\nnode embedding, this layer improves the GNN's capability to capture complex\nbrain network characteristics. We analyzed neuroimaging data from 383\nparticipants, including both cognitively normal and preclinical AD individuals,\nusing T1-weighted MRI, resting-state fMRI, and FBB-PET to construct brain\ngraphs. Our results show that GNNs with the BA readout layer significantly\noutperform traditional models in predicting the Preclinical Alzheimer's\nCognitive Composite (PACC) score, demonstrating higher robustness and\nstability. The adaptive BA readout layer also offers enhanced interpretability\nby highlighting task-specific brain regions critical to cognitive functions\nimpacted by AD. These findings suggest that our approach provides a valuable\ntool for the early diagnosis and analysis of Alzheimer's disease.",
    "categories": [
      "q-bio.NC",
      "cs.AI",
      "cs.CV",
      "eess.IV"
    ],
    "primary_category": "q-bio.NC",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.14683v1",
    "published_date": "2024-10-03 05:04:45 UTC",
    "updated_date": "2024-10-03 05:04:45 UTC"
  },
  {
    "arxiv_id": "2410.02207v1",
    "title": "Adapting Segment Anything Model to Melanoma Segmentation in Microscopy Slide Images",
    "authors": [
      "Qingyuan Liu",
      "Avideh Zakhor"
    ],
    "abstract": "Melanoma segmentation in Whole Slide Images (WSIs) is useful for prognosis\nand the measurement of crucial prognostic factors such as Breslow depth and\nprimary invasive tumor size. In this paper, we present a novel approach that\nuses the Segment Anything Model (SAM) for automatic melanoma segmentation in\nmicroscopy slide images. Our method employs an initial semantic segmentation\nmodel to generate preliminary segmentation masks that are then used to prompt\nSAM. We design a dynamic prompting strategy that uses a combination of centroid\nand grid prompts to achieve optimal coverage of the super high-resolution slide\nimages while maintaining the quality of generated prompts. To optimize for\ninvasive melanoma segmentation, we further refine the prompt generation process\nby implementing in-situ melanoma detection and low-confidence region filtering.\nWe select Segformer as the initial segmentation model and EfficientSAM as the\nsegment anything model for parameter-efficient fine-tuning. Our experimental\nresults demonstrate that this approach not only surpasses other\nstate-of-the-art melanoma segmentation methods but also significantly\noutperforms the baseline Segformer by 9.1% in terms of IoU.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.02207v1",
    "published_date": "2024-10-03 04:40:18 UTC",
    "updated_date": "2024-10-03 04:40:18 UTC"
  },
  {
    "arxiv_id": "2410.02205v3",
    "title": "Aligning with Logic: Measuring, Evaluating and Improving Logical Preference Consistency in Large Language Models",
    "authors": [
      "Yinhong Liu",
      "Zhijiang Guo",
      "Tianya Liang",
      "Ehsan Shareghi",
      "Ivan Vulić",
      "Nigel Collier"
    ],
    "abstract": "Large Language Models (LLMs) are expected to be predictable and trustworthy\nto support reliable decision-making systems. Yet current LLMs often show\ninconsistencies in their judgments. In this work, we examine logical preference\nconsistency as a foundational requirement for building more dependable LLM\nsystems, ensuring stable and coherent decision-making while minimizing erratic\nor contradictory outputs. To quantify the logical preference consistency, we\npropose a universal evaluation framework based on three fundamental properties:\ntransitivity, commutativity and negation invariance. Through extensive\nexperimentation across diverse LLMs, we demonstrate that these properties serve\nas strong indicators of judgment robustness. Furthermore, we introduce a data\nrefinement and augmentation technique, REPAIR, that enhances logical\nconsistency while maintaining alignment with human preferences. Finally, we\nshow that improving consistency leads to better performance in LLM-driven\nlogic-based algorithms, reinforcing stability and coherence in decision-making\nsystems.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LO"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.02205v3",
    "published_date": "2024-10-03 04:34:04 UTC",
    "updated_date": "2025-02-09 17:13:51 UTC"
  },
  {
    "arxiv_id": "2410.02203v3",
    "title": "GraphIC: A Graph-Based In-Context Example Retrieval Model for Multi-Step Reasoning",
    "authors": [
      "Jiale Fu",
      "Yaqing Wang",
      "Simeng Han",
      "Jiaming Fan",
      "Xu Yang"
    ],
    "abstract": "In-context learning (ICL) enhances large language models (LLMs) by\nincorporating demonstration examples, yet its effectiveness heavily depends on\nthe quality of selected examples. Current methods typically use text embeddings\nto measure semantic similarity, which often introduces bias in multi-step\nreasoning tasks. This occurs because text embeddings contain irrelevant\nsemantic information and lack deeper reasoning structures. To address this, we\npropose GraphIC, a graph-based retrieval model that leverages reasoning-aware\nrepresentation and specialized similarity metric for in-context example\nretrieval. GraphIC first constructs thought graphs-directed, node-attributed\ngraphs that explicitly model reasoning steps and their dependencies-for\ncandidate examples and queries. This approach filters out superficial semantics\nwhile preserving essential reasoning processes. Next, GraphIC retrieves\nexamples using a novel similarity metric tailored for these graphs, capturing\nsequential reasoning patterns and asymmetry between examples. Comprehensive\nevaluations across mathematical reasoning, code generation, and logical\nreasoning tasks demonstrate that GraphIC outperforms 10 baseline methods. Our\nresults highlight the importance of reasoning-aware retrieval in ICL, offering\na robust solution for enhancing LLM performance in multi-step reasoning\nscenarios.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.02203v3",
    "published_date": "2024-10-03 04:33:02 UTC",
    "updated_date": "2025-02-25 03:10:28 UTC"
  },
  {
    "arxiv_id": "2410.02202v1",
    "title": "Can Language Models Take A Hint? Prompting for Controllable Contextualized Commonsense Inference",
    "authors": [
      "Pedro Colon-Hernandez",
      "Nanxi Liu",
      "Chelsea Joe",
      "Peter Chin",
      "Claire Yin",
      "Henry Lieberman",
      "Yida Xin",
      "Cynthia Breazeal"
    ],
    "abstract": "Generating commonsense assertions within a given story context remains a\ndifficult task for modern language models. Previous research has addressed this\nproblem by aligning commonsense inferences with stories and training language\ngeneration models accordingly. One of the challenges is determining which topic\nor entity in the story should be the focus of an inferred assertion. Prior\napproaches lack the ability to control specific aspects of the generated\nassertions. In this work, we introduce \"hinting,\" a data augmentation technique\nthat enhances contextualized commonsense inference. \"Hinting\" employs a prefix\nprompting strategy using both hard and soft prompts to guide the inference\nprocess. To demonstrate its effectiveness, we apply \"hinting\" to two contextual\ncommonsense inference datasets: ParaCOMET and GLUCOSE, evaluating its impact on\nboth general and context-specific inference. Furthermore, we evaluate \"hinting\"\nby incorporating synonyms and antonyms into the hints. Our results show that\n\"hinting\" does not compromise the performance of contextual commonsense\ninference while offering improved controllability.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Submitted to ACL Rolling Review. arXiv admin note: text overlap with\n  arXiv:2302.05406",
    "pdf_url": "http://arxiv.org/pdf/2410.02202v1",
    "published_date": "2024-10-03 04:32:46 UTC",
    "updated_date": "2024-10-03 04:32:46 UTC"
  },
  {
    "arxiv_id": "2410.02198v1",
    "title": "G2T-LLM: Graph-to-Tree Text Encoding for Molecule Generation with Fine-Tuned Large Language Models",
    "authors": [
      "Zhaoning Yu",
      "Xiangyang Xu",
      "Hongyang Gao"
    ],
    "abstract": "We introduce G2T-LLM, a novel approach for molecule generation that uses\ngraph-to-tree text encoding to transform graph-based molecular structures into\na hierarchical text format optimized for large language models (LLMs). This\nencoding converts complex molecular graphs into tree-structured formats, such\nas JSON and XML, which LLMs are particularly adept at processing due to their\nextensive pre-training on these types of data. By leveraging the flexibility of\nLLMs, our approach allows for intuitive interaction using natural language\nprompts, providing a more accessible interface for molecular design. Through\nsupervised fine-tuning, G2T-LLM generates valid and coherent chemical\nstructures, addressing common challenges like invalid outputs seen in\ntraditional graph-based methods. While LLMs are computationally intensive, they\noffer superior generalization and adaptability, enabling the generation of\ndiverse molecular structures with minimal task-specific customization. The\nproposed approach achieved comparable performances with state-of-the-art\nmethods on various benchmark molecular generation datasets, demonstrating its\npotential as a flexible and innovative tool for AI-driven molecular design.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "q-bio.QM"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.02198v1",
    "published_date": "2024-10-03 04:25:21 UTC",
    "updated_date": "2024-10-03 04:25:21 UTC"
  },
  {
    "arxiv_id": "2410.02197v2",
    "title": "Beyond Bradley-Terry Models: A General Preference Model for Language Model Alignment",
    "authors": [
      "Yifan Zhang",
      "Ge Zhang",
      "Yue Wu",
      "Kangping Xu",
      "Quanquan Gu"
    ],
    "abstract": "Modeling human preferences is crucial for aligning foundation models with\nhuman values. Traditional reward modeling methods, such as the Bradley-Terry\n(BT) reward model, fall short in expressiveness, particularly in addressing\nintransitive preferences. In this paper, we introduce preference embedding, an\napproach that embeds responses into a latent space to capture intricate\npreference structures efficiently, achieving linear query complexity.\nAdditionally, we propose preference score-based General Preference Optimization\n(GPO), which generalizes reward-based reinforcement learning from human\nfeedback (RLHF). Experimental results show that our General Preference\nembedding Model (GPM) consistently outperforms the BT reward model on the\nRewardBench benchmark and effectively models cyclic preferences where any BT\nreward model behaves like a random guess. Furthermore, evaluations on\ndownstream tasks such as AlpacaEval2.0, following the language model\npost-training with GPO and our general preference model, reveal performance\nimprovements over BT models. These findings indicate that our method may\nenhance the alignment of foundation models with nuanced human values. The code\nis available at https://github.com/general-preference/general-preference-model.",
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "35 pages, 3 figures",
    "pdf_url": "http://arxiv.org/pdf/2410.02197v2",
    "published_date": "2024-10-03 04:22:55 UTC",
    "updated_date": "2025-02-17 20:42:35 UTC"
  },
  {
    "arxiv_id": "2410.02195v1",
    "title": "BACKTIME: Backdoor Attacks on Multivariate Time Series Forecasting",
    "authors": [
      "Xiao Lin",
      "Zhining Liu",
      "Dongqi Fu",
      "Ruizhong Qiu",
      "Hanghang Tong"
    ],
    "abstract": "Multivariate Time Series (MTS) forecasting is a fundamental task with\nnumerous real-world applications, such as transportation, climate, and\nepidemiology. While a myriad of powerful deep learning models have been\ndeveloped for this task, few works have explored the robustness of MTS\nforecasting models to malicious attacks, which is crucial for their trustworthy\nemployment in high-stake scenarios. To address this gap, we dive deep into the\nbackdoor attacks on MTS forecasting models and propose an effective attack\nmethod named BackTime.By subtly injecting a few stealthy triggers into the MTS\ndata, BackTime can alter the predictions of the forecasting model according to\nthe attacker's intent. Specifically, BackTime first identifies vulnerable\ntimestamps in the data for poisoning, and then adaptively synthesizes stealthy\nand effective triggers by solving a bi-level optimization problem with a\nGNN-based trigger generator. Extensive experiments across multiple datasets and\nstate-of-the-art MTS forecasting models demonstrate the effectiveness,\nversatility, and stealthiness of \\method{} attacks. The code is available at\n\\url{https://github.com/xiaolin-cs/BackTime}.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CR"
    ],
    "primary_category": "cs.LG",
    "comment": "23 pages. Neurips 2024",
    "pdf_url": "http://arxiv.org/pdf/2410.02195v1",
    "published_date": "2024-10-03 04:16:49 UTC",
    "updated_date": "2024-10-03 04:16:49 UTC"
  },
  {
    "arxiv_id": "2410.02191v2",
    "title": "A Survey on Point-of-Interest Recommendation: Models, Architectures, and Security",
    "authors": [
      "Qianru Zhang",
      "Peng Yang",
      "Junliang Yu",
      "Haixin Wang",
      "Xingwei He",
      "Siu-Ming Yiu",
      "Hongzhi Yin"
    ],
    "abstract": "The widespread adoption of smartphones and Location-Based Social Networks has\nled to a massive influx of spatio-temporal data, creating unparalleled\nopportunities for enhancing Point-of-Interest (POI) recommendation systems.\nThese advanced POI systems are crucial for enriching user experiences, enabling\npersonalized interactions, and optimizing decision-making processes in the\ndigital landscape. However, existing surveys tend to focus on traditional\napproaches and few of them delve into cutting-edge developments, emerging\narchitectures, as well as security considerations in POI recommendations. To\naddress this gap, our survey stands out by offering a comprehensive, up-to-date\nreview of POI recommendation systems, covering advancements in models,\narchitectures, and security aspects. We systematically examine the transition\nfrom traditional models to advanced techniques such as large language models.\nAdditionally, we explore the architectural evolution from centralized to\ndecentralized and federated learning systems, highlighting the improvements in\nscalability and privacy. Furthermore, we address the increasing importance of\nsecurity, examining potential vulnerabilities and privacy-preserving\napproaches. Our taxonomy provides a structured overview of the current state of\nPOI recommendation, while we also identify promising directions for future\nresearch in this rapidly advancing field.",
    "categories": [
      "cs.IR",
      "cs.AI",
      "cs.CE",
      "cs.LG"
    ],
    "primary_category": "cs.IR",
    "comment": "20 pages",
    "pdf_url": "http://arxiv.org/pdf/2410.02191v2",
    "published_date": "2024-10-03 04:11:42 UTC",
    "updated_date": "2025-03-10 02:57:32 UTC"
  },
  {
    "arxiv_id": "2410.02189v2",
    "title": "Agent-Oriented Planning in Multi-Agent Systems",
    "authors": [
      "Ao Li",
      "Yuexiang Xie",
      "Songze Li",
      "Fugee Tsung",
      "Bolin Ding",
      "Yaliang Li"
    ],
    "abstract": "Through the collaboration of multiple LLM-empowered agents possessing diverse\nexpertise and tools, multi-agent systems achieve impressive progress in solving\nreal-world problems. Given the user queries, the meta-agents, serving as the\nbrain within multi-agent systems, are required to decompose the queries into\nmultiple sub-tasks that can be allocated to suitable agents capable of solving\nthem, so-called agent-oriented planning. In this study, we identify three\ncritical design principles of agent-oriented planning, including solvability,\ncompleteness, and non-redundancy, to ensure that each sub-task can be\neffectively resolved, resulting in satisfactory responses to user queries.\nThese principles further inspire us to propose AOP, a novel framework for\nagent-oriented planning in multi-agent systems, leveraging a fast task\ndecomposition and allocation process followed by an effective and efficient\nevaluation via a reward model. According to the evaluation results, the\nmeta-agent is also responsible for promptly making necessary adjustments to\nsub-tasks and scheduling. Besides, we integrate a feedback loop into AOP to\nfurther enhance the effectiveness and robustness of such a problem-solving\nprocess. Extensive experiments demonstrate the advancement of AOP in solving\nreal-world problems compared to both single-agent systems and existing planning\nstrategies for multi-agent systems. The source code is available at\nhttps://github.com/lalaliat/Agent-Oriented-Planning",
    "categories": [
      "cs.AI",
      "cs.LG",
      "cs.MA"
    ],
    "primary_category": "cs.AI",
    "comment": "Accepted by ICLR'2025",
    "pdf_url": "http://arxiv.org/pdf/2410.02189v2",
    "published_date": "2024-10-03 04:07:51 UTC",
    "updated_date": "2025-03-11 11:22:17 UTC"
  },
  {
    "arxiv_id": "2410.02185v2",
    "title": "POSIX: A Prompt Sensitivity Index For Large Language Models",
    "authors": [
      "Anwoy Chatterjee",
      "H S V N S Kowndinya Renduchintala",
      "Sumit Bhatia",
      "Tanmoy Chakraborty"
    ],
    "abstract": "Despite their remarkable capabilities, Large Language Models (LLMs) are found\nto be surprisingly sensitive to minor variations in prompts, often generating\nsignificantly divergent outputs in response to minor variations in the prompts,\nsuch as spelling errors, alteration of wording or the prompt template. However,\nwhile assessing the quality of an LLM, the focus often tends to be solely on\nits performance on downstream tasks, while very little to no attention is paid\nto prompt sensitivity. To fill this gap, we propose POSIX - a novel PrOmpt\nSensitivity IndeX as a reliable measure of prompt sensitivity, thereby offering\na more comprehensive evaluation of LLM performance. The key idea behind POSIX\nis to capture the relative change in loglikelihood of a given response upon\nreplacing the corresponding prompt with a different intent-preserving prompt.\nWe provide thorough empirical evidence demonstrating the efficacy of POSIX in\ncapturing prompt sensitivity and subsequently use it to measure and thereby\ncompare prompt sensitivity of various open-source LLMs. We find that merely\nincreasing the parameter count or instruction tuning does not necessarily\nreduce prompt sensitivity whereas adding some few-shot exemplars, even just\none, almost always leads to significant decrease in prompt sensitivity. We also\nfind that alterations to prompt template lead to the highest sensitivity in the\ncase of MCQ type tasks, whereas paraphrasing results in the highest sensitivity\nin open-ended generation tasks. The code for reproducing our results is\nopen-sourced at https://github.com/kowndinya-renduchintala/POSIX.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "EMNLP 2024 (Findings)",
    "pdf_url": "http://arxiv.org/pdf/2410.02185v2",
    "published_date": "2024-10-03 04:01:14 UTC",
    "updated_date": "2024-10-04 07:00:03 UTC"
  },
  {
    "arxiv_id": "2410.02173v1",
    "title": "Efficiently Deploying LLMs with Controlled Risk",
    "authors": [
      "Michael J. Zellinger",
      "Matt Thomson"
    ],
    "abstract": "Deploying large language models in production requires simultaneous attention\nto efficiency and risk control. Prior work has shown the possibility to cut\ncosts while maintaining similar accuracy, but has neglected to focus on risk\ncontrol. By contrast, here we present hierarchical chains with multi-level\nabstention (HCMA), which use model-intrinsic uncertainty to delegate queries\nalong the LLM intelligence hierarchy, enabling training-free model switching\nbased solely on black-box API calls. Our framework presents novel trade-offs\nbetween efficiency and risk. For example, deploying HCMA on MMLU cuts the error\nrate of Llama3 405B by 30% when the model is allowed to abstain on 20% of the\nqueries. To calibrate HCMA for optimal performance, our approach uses\ndata-efficient logistic regressions (based on a simple nonlinear feature\ntransformation), which require only 50 or 100 labeled examples to achieve\nexcellent calibration error (ECE), cutting ECE by 50% compared to naive Platt\nscaling. On free-form generation tasks, we find that chain-of-thought is\nineffectual for selective prediction, whereas zero-shot prompting drives error\nto 0% on TruthfulQA at high abstention rates. As LLMs are increasingly deployed\nacross computing environments with different capabilities (such as mobile,\nlaptop, and cloud), our framework paves the way towards maintaining deployment\nefficiency while putting in place sharp risk controls.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "10 pages",
    "pdf_url": "http://arxiv.org/pdf/2410.02173v1",
    "published_date": "2024-10-03 03:25:56 UTC",
    "updated_date": "2024-10-03 03:25:56 UTC"
  },
  {
    "arxiv_id": "2410.02172v1",
    "title": "Abstract Reward Processes: Leveraging State Abstraction for Consistent Off-Policy Evaluation",
    "authors": [
      "Shreyas Chaudhari",
      "Ameet Deshpande",
      "Bruno Castro da Silva",
      "Philip S. Thomas"
    ],
    "abstract": "Evaluating policies using off-policy data is crucial for applying\nreinforcement learning to real-world problems such as healthcare and autonomous\ndriving. Previous methods for off-policy evaluation (OPE) generally suffer from\nhigh variance or irreducible bias, leading to unacceptably high prediction\nerrors. In this work, we introduce STAR, a framework for OPE that encompasses a\nbroad range of estimators -- which include existing OPE methods as special\ncases -- that achieve lower mean squared prediction errors. STAR leverages\nstate abstraction to distill complex, potentially continuous problems into\ncompact, discrete models which we call abstract reward processes (ARPs).\nPredictions from ARPs estimated from off-policy data are provably consistent\n(asymptotically correct). Rather than proposing a specific estimator, we\npresent a new framework for OPE and empirically demonstrate that estimators\nwithin STAR outperform existing methods. The best STAR estimator outperforms\nbaselines in all twelve cases studied, and even the median STAR estimator\nsurpasses the baselines in seven out of the twelve cases.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted at the Thirty-eighth Annual Conference on Neural Information\n  Processing Systems (NeurIPS 2024)",
    "pdf_url": "http://arxiv.org/pdf/2410.02172v1",
    "published_date": "2024-10-03 03:19:43 UTC",
    "updated_date": "2024-10-03 03:19:43 UTC"
  },
  {
    "arxiv_id": "2410.02165v1",
    "title": "A LLM-Powered Automatic Grading Framework with Human-Level Guidelines Optimization",
    "authors": [
      "Yucheng Chu",
      "Hang Li",
      "Kaiqi Yang",
      "Harry Shomer",
      "Hui Liu",
      "Yasemin Copur-Gencturk",
      "Jiliang Tang"
    ],
    "abstract": "Open-ended short-answer questions (SAGs) have been widely recognized as a\npowerful tool for providing deeper insights into learners' responses in the\ncontext of learning analytics (LA). However, SAGs often present challenges in\npractice due to the high grading workload and concerns about inconsistent\nassessments. With recent advancements in natural language processing (NLP),\nautomatic short-answer grading (ASAG) offers a promising solution to these\nchallenges. Despite this, current ASAG algorithms are often limited in\ngeneralizability and tend to be tailored to specific questions. In this paper,\nwe propose a unified multi-agent ASAG framework, GradeOpt, which leverages\nlarge language models (LLMs) as graders for SAGs. More importantly, GradeOpt\nincorporates two additional LLM-based agents - the reflector and the refiner -\ninto the multi-agent system. This enables GradeOpt to automatically optimize\nthe original grading guidelines by performing self-reflection on its errors.\nThrough experiments on a challenging ASAG task, namely the grading of\npedagogical content knowledge (PCK) and content knowledge (CK) questions,\nGradeOpt demonstrates superior performance in grading accuracy and behavior\nalignment with human graders compared to representative baselines. Finally,\ncomprehensive ablation studies confirm the effectiveness of the individual\ncomponents designed in GradeOpt.",
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.02165v1",
    "published_date": "2024-10-03 03:11:24 UTC",
    "updated_date": "2024-10-03 03:11:24 UTC"
  },
  {
    "arxiv_id": "2410.03775v3",
    "title": "Beyond correlation: The Impact of Human Uncertainty in Measuring the Effectiveness of Automatic Evaluation and LLM-as-a-Judge",
    "authors": [
      "Aparna Elangovan",
      "Lei Xu",
      "Jongwoo Ko",
      "Mahsa Elyasi",
      "Ling Liu",
      "Sravan Bodapati",
      "Dan Roth"
    ],
    "abstract": "The effectiveness of automatic evaluation of generative models is typically\nmeasured by comparing the labels generated via automation with labels by humans\nusing correlation metrics. However, metrics like Krippendorff's $\\alpha$ and\nRandolph's $\\kappa$ were originally designed to measure the reliability of\nhuman labeling, thus make assumptions about typical human labeling behavior,\nand these assumptions may not be applicable to machine generated labels. In\nthis paper, we show how *relying on a single aggregate correlation score* can\nobscure fundamental differences between human labels and those from automatic\nevaluation, including LLM-as-a-Judge. Specifically, we demonstrate that when\nthe proportion of samples with variation or uncertainty in human assigned\nlabels is relatively high, machine labels (generated by automatic evaluation\nmethods) may superficially appear to have similar or better correlation with\nthe human majority label compared to the human-to-human (HH) correlation. This\ncan create the illusion that labels from automatic evaluation approximates the\nhuman majority label. However, as the proportion of samples with consistent\nhuman labels increases, the correlation between machine and human labels fall\nwell below HH correlation. Based on these findings, we first propose\nstratifying data by human label uncertainty to provide a more robust analysis\nof automatic evaluation performance. Second, recognizing that uncertainty and\nvariation are inherent in perception-based human evaluations, such as those\ninvolving attitudes or preferences, we introduce a new metric - binned\nJensen-Shannon Divergence for perception for such scenarios to better measure\nthe effectiveness of automatic evaluations. We present visualization techniques\n-- perception charts, to contextualize correlation measures appropriately. We\nhave open-sourced at https://github.com/amazon-science/BeyondCorrelation.",
    "categories": [
      "cs.HC",
      "cs.AI"
    ],
    "primary_category": "cs.HC",
    "comment": "Accepted at ICLR 2025",
    "pdf_url": "http://arxiv.org/pdf/2410.03775v3",
    "published_date": "2024-10-03 03:08:29 UTC",
    "updated_date": "2025-01-27 07:02:04 UTC"
  },
  {
    "arxiv_id": "2410.02162v1",
    "title": "Planning in Strawberry Fields: Evaluating and Improving the Planning and Scheduling Capabilities of LRM o1",
    "authors": [
      "Karthik Valmeekam",
      "Kaya Stechly",
      "Atharva Gundawar",
      "Subbarao Kambhampati"
    ],
    "abstract": "The ability to plan a course of action that achieves a desired state of\naffairs has long been considered a core competence of intelligent agents and\nhas been an integral part of AI research since its inception. With the advent\nof large language models (LLMs), there has been considerable interest in the\nquestion of whether or not they possess such planning abilities, but -- despite\nthe slew of new private and open source LLMs since GPT3 -- progress has\nremained slow. OpenAI claims that their recent o1 (Strawberry) model has been\nspecifically constructed and trained to escape the normal limitations of\nautoregressive LLMs -- making it a new kind of model: a Large Reasoning Model\n(LRM). In this paper, we evaluate the planning capabilities of two LRMs\n(o1-preview and o1-mini) on both planning and scheduling benchmarks. We see\nthat while o1 does seem to offer significant improvements over autoregressive\nLLMs, this comes at a steep inference cost, while still failing to provide any\nguarantees over what it generates. We also show that combining o1 models with\nexternal verifiers -- in a so-called LRM-Modulo system -- guarantees the\ncorrectness of the combined system's output while further improving\nperformance.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "arXiv admin note: text overlap with arXiv:2409.13373",
    "pdf_url": "http://arxiv.org/pdf/2410.02162v1",
    "published_date": "2024-10-03 03:04:36 UTC",
    "updated_date": "2024-10-03 03:04:36 UTC"
  },
  {
    "arxiv_id": "2410.02160v1",
    "title": "RiskSEA : A Scalable Graph Embedding for Detecting On-chain Fraudulent Activities on the Ethereum Blockchain",
    "authors": [
      "Ayush Agarwal",
      "Lv Lu",
      "Arjun Maheswaran",
      "Varsha Mahadevan",
      "Bhaskar Krishnamachari"
    ],
    "abstract": "Like any other useful technology, cryptocurrencies are sometimes used for\ncriminal activities. While transactions are recorded on the blockchain, there\nexists a need for a more rapid and scalable method to detect addresses\nassociated with fraudulent activities. We present RiskSEA, a scalable risk\nscoring system capable of effectively handling the dynamic nature of\nlarge-scale blockchain transaction graphs. The risk scoring system, which we\nimplement for Ethereum, consists of 1. a scalable approach to generating\nnode2vec embedding for entire set of addresses to capture the graph topology 2.\ntransaction-based features to capture the transactional behavioral pattern of\nan address 3. a classifier model to generate risk score for addresses that\ncombines the node2vec embedding and behavioral features. Efficiently generating\nnode2vec embedding for large scale and dynamically evolving blockchain\ntransaction graphs is challenging, we present two novel approaches for\ngenerating node2vec embeddings and effectively scaling it to the entire set of\nblockchain addresses: 1. node2vec embedding propagation and 2. dynamic node2vec\nembedding. We present a comprehensive analysis of the proposed approaches. Our\nexperiments show that combining both behavioral and node2vec features boosts\nthe classification performance significantly, and that the dynamic node2vec\nembeddings perform better than the node2vec propagated embeddings.",
    "categories": [
      "cs.CR",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CR",
    "comment": "arXiv admin note: text overlap with arXiv:2203.12363 by other authors",
    "pdf_url": "http://arxiv.org/pdf/2410.02160v1",
    "published_date": "2024-10-03 02:54:19 UTC",
    "updated_date": "2024-10-03 02:54:19 UTC"
  },
  {
    "arxiv_id": "2410.02159v2",
    "title": "Mitigating Memorization In Language Models",
    "authors": [
      "Mansi Sakarvadia",
      "Aswathy Ajith",
      "Arham Khan",
      "Nathaniel Hudson",
      "Caleb Geniesse",
      "Kyle Chard",
      "Yaoqing Yang",
      "Ian Foster",
      "Michael W. Mahoney"
    ],
    "abstract": "Language models (LMs) can \"memorize\" information, i.e., encode training data\nin their weights in such a way that inference-time queries can lead to verbatim\nregurgitation of that data. This ability to extract training data can be\nproblematic, for example, when data are private or sensitive. In this work, we\ninvestigate methods to mitigate memorization: three regularizer-based, three\nfinetuning-based, and eleven machine unlearning-based methods, with five of the\nlatter being new methods that we introduce. We also introduce TinyMem, a suite\nof small, computationally-efficient LMs for the rapid development and\nevaluation of memorization-mitigation methods. We demonstrate that the\nmitigation methods that we develop using TinyMem can successfully be applied to\nproduction-grade LMs, and we determine via experiment that: regularizer-based\nmitigation methods are slow and ineffective at curbing memorization;\nfine-tuning-based methods are effective at curbing memorization, but overly\nexpensive, especially for retaining higher accuracies; and unlearning-based\nmethods are faster and more effective, allowing for the precise localization\nand removal of memorized information from LM weights prior to inference. We\nshow, in particular, that our proposed unlearning method BalancedSubnet\noutperforms other mitigation methods at removing memorized information while\npreserving performance on target tasks.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.02159v2",
    "published_date": "2024-10-03 02:53:51 UTC",
    "updated_date": "2025-01-28 21:19:25 UTC"
  },
  {
    "arxiv_id": "2410.02156v2",
    "title": "The why, what, and how of AI-based coding in scientific research",
    "authors": [
      "Tonghe Zhuang",
      "Zhicheng Lin"
    ],
    "abstract": "Computer programming (coding) is indispensable for researchers across\ndisciplines, yet it remains challenging to learn and time-consuming to carry\nout. Generative AI, particularly large language models (LLMs), has the\npotential to transform coding into intuitive conversations, but best practices\nand effective workflows are only emerging. We dissect AI-based coding through\nthree key lenses: the nature and role of LLMs in coding (why), six types of\ncoding assistance they provide (what), and a five-step workflow in action with\npractical implementation strategies (how). Additionally, we address the\nlimitations and future outlook of AI in coding. By offering actionable\ninsights, this framework helps to guide researchers in effectively leveraging\nAI to enhance coding practices and education, accelerating scientific progress.",
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.CL",
      "cs.PL"
    ],
    "primary_category": "cs.CY",
    "comment": "23 pages, 7 figure, 3 boxes",
    "pdf_url": "http://arxiv.org/pdf/2410.02156v2",
    "published_date": "2024-10-03 02:36:30 UTC",
    "updated_date": "2024-11-18 07:36:36 UTC"
  },
  {
    "arxiv_id": "2410.02155v3",
    "title": "From Pixels to Tokens: Byte-Pair Encoding on Quantized Visual Modalities",
    "authors": [
      "Wanpeng Zhang",
      "Zilong Xie",
      "Yicheng Feng",
      "Yijiang Li",
      "Xingrun Xing",
      "Sipeng Zheng",
      "Zongqing Lu"
    ],
    "abstract": "Multimodal Large Language Models have made significant strides in integrating\nvisual and textual information, yet they often struggle with effectively\naligning these modalities. We introduce a novel image tokenizer that bridges\nthis gap by applying the principle of Byte-Pair Encoding (BPE) to visual data.\nUnlike conventional approaches that rely on separate visual encoders, our\nmethod directly incorporates structural prior information into image tokens,\nmirroring the successful tokenization strategies used in text-only Large\nLanguage Models. This innovative approach enables Transformer models to more\neffectively learn and reason across modalities. Through theoretical analysis\nand extensive experiments, we demonstrate that our BPE Image Tokenizer\nsignificantly enhances MLLMs' multimodal understanding capabilities, even with\nlimited training data. Leveraging this method, we develop Being-VL-0, a model\nthat demonstrates superior performance across various benchmarks and shows\npromising scalability, potentially paving the way for more efficient and\ncapable multimodal foundation models.",
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.CV"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.02155v3",
    "published_date": "2024-10-03 02:34:31 UTC",
    "updated_date": "2025-03-09 15:36:53 UTC"
  },
  {
    "arxiv_id": "2410.02147v2",
    "title": "Efficient Source-Free Time-Series Adaptation via Parameter Subspace Disentanglement",
    "authors": [
      "Gaurav Patel",
      "Christopher Sandino",
      "Behrooz Mahasseni",
      "Ellen L Zippi",
      "Erdrin Azemi",
      "Ali Moin",
      "Juri Minxha"
    ],
    "abstract": "In this paper, we propose a framework for efficient Source-Free Domain\nAdaptation (SFDA) in the context of time-series, focusing on enhancing both\nparameter efficiency and data-sample utilization. Our approach introduces an\nimproved paradigm for source-model preparation and target-side adaptation,\naiming to enhance training efficiency during target adaptation. Specifically,\nwe reparameterize the source model's weights in a Tucker-style decomposed\nmanner, factorizing the model into a compact form during the source model\npreparation phase. During target-side adaptation, only a subset of these\ndecomposed factors is fine-tuned, leading to significant improvements in\ntraining efficiency. We demonstrate using PAC Bayesian analysis that this\nselective fine-tuning strategy implicitly regularizes the adaptation process by\nconstraining the model's learning capacity. Furthermore, this\nre-parameterization reduces the overall model size and enhances inference\nefficiency, making the approach particularly well suited for\nresource-constrained devices. Additionally, we demonstrate that our framework\nis compatible with various SFDA methods and achieves significant computational\nefficiency, reducing the number of fine-tuned parameters and inference overhead\nin terms of MACs by over 90% while maintaining model performance.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "eess.SP"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted at ICLR 2025",
    "pdf_url": "http://arxiv.org/pdf/2410.02147v2",
    "published_date": "2024-10-03 02:12:03 UTC",
    "updated_date": "2025-02-01 16:45:14 UTC"
  },
  {
    "arxiv_id": "2410.03774v1",
    "title": "Human-Based Risk Model for Improved Driver Support in Interactive Driving Scenarios",
    "authors": [
      "Tim Puphal",
      "Benedict Flade",
      "Matti Krüger",
      "Ryohei Hirano",
      "Akihito Kimata"
    ],
    "abstract": "This paper addresses the problem of human-based driver support. Nowadays,\ndriver support systems help users to operate safely in many driving situations.\nNevertheless, these systems do not fully use the rich information that is\navailable from sensing the human driver. In this paper, we therefore present a\nhuman-based risk model that uses driver information for improved driver\nsupport. In contrast to state of the art, our proposed risk model combines a)\nthe current driver perception based on driver errors, such as the driver\noverlooking another vehicle (i.e., notice error), and b) driver\npersonalization, such as the driver being defensive or confident. In extensive\nsimulations of multiple interactive driving scenarios, we show that our novel\nhuman-based risk model achieves earlier warning times and reduced warning\nerrors compared to a baseline risk model not using human driver information.",
    "categories": [
      "cs.HC",
      "cs.AI"
    ],
    "primary_category": "cs.HC",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.03774v1",
    "published_date": "2024-10-03 02:10:13 UTC",
    "updated_date": "2024-10-03 02:10:13 UTC"
  },
  {
    "arxiv_id": "2410.02110v2",
    "title": "Can LLMs Reliably Simulate Human Learner Actions? A Simulation Authoring Framework for Open-Ended Learning Environments",
    "authors": [
      "Amogh Mannekote",
      "Adam Davies",
      "Jina Kang",
      "Kristy Elizabeth Boyer"
    ],
    "abstract": "Simulating learner actions helps stress-test open-ended interactive learning\nenvironments and prototype new adaptations before deployment. While recent\nstudies show the promise of using large language models (LLMs) for simulating\nhuman behavior, such approaches have not gone beyond rudimentary\nproof-of-concept stages due to key limitations. First, LLMs are highly\nsensitive to minor prompt variations, raising doubts about their ability to\ngeneralize to new scenarios without extensive prompt engineering. Moreover,\napparently successful outcomes can often be unreliable, either because domain\nexperts unintentionally guide LLMs to produce expected results, leading to\nself-fulfilling prophecies; or because the LLM has encountered highly similar\nscenarios in its training data, meaning that models may not be simulating\nbehavior so much as regurgitating memorized content. To address these\nchallenges, we propose Hyp-Mix, a simulation authoring framework that allows\nexperts to develop and evaluate simulations by combining testable hypotheses\nabout learner behavior. Testing this framework in a physics learning\nenvironment, we found that GPT-4 Turbo maintains calibrated behavior even as\nthe underlying learner model changes, providing the first evidence that LLMs\ncan be used to simulate realistic behaviors in open-ended interactive learning\nenvironments, a necessary prerequisite for useful LLM behavioral simulation.",
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.02110v2",
    "published_date": "2024-10-03 00:25:40 UTC",
    "updated_date": "2024-10-12 22:58:02 UTC"
  }
]
{
  "date": "2025-03-26",
  "category": "cs.AI",
  "summary": "欢迎来到 UTC 时间 2025-03-26 的 arXiv 中文 TLDR 快报！\n\n今天 arXiv 的论文涵盖了众多领域，其中**大型语言模型（LLM）及其智能体（Agent）应用**依然是热点，探讨了工具接口设计、推理能力提升、多模态融合、移动端基准、安全性（如新型攻击手段 SUDO）和伦理问题（如错误性别指称）。同时，**生成模型**在视频合成（特别是自动驾驶和物理真实感）、图像生成（扩散模型蒸馏）方面有显著进展，Wayve 发布的 GAIA-2 世界模型引人注目。**强化学习**在机器人控制、鲁棒性、离线学习和技能发现方面也有不少探索。此外，**联邦学习的安全性、计算机视觉中的分割与增强、图神经网络的鲁棒性**等也是今日的研究重点。\n\n以下是今日重点论文的快速概览：\n\n---\n\n**今日焦点与 LLM 智能体**\n\n1.  **工具接口设计的艺术 (The Art of Tool Interface Design)**\n    *   论文提出名为 Thinker 的智能体框架，通过状态机增强生成（SMAG）、任务委托和自适应上下文管理等创新工具接口设计，显著提升了 LLM 在复杂客服场景推理任务中的表现（GPT-4o 提升至 82.6%，Llama-3.1 405B 提升至 81.9%），且无需微调。\n\n2.  **sudo rm -rf agentic_security (sudo rm -rf agentic_security)**\n    *   **安全警告！** 本文提出名为 SUDO 的新型攻击框架，利用 Detox2Tox 机制（将有害请求伪装成良性请求，获取 VLM 指令后再重新注入恶意内容），能系统性绕过商业计算机操作智能体（如 Claude Computer Use）的安全防护，攻击成功率高达 41%。揭示了当前智能体在真实计算环境中的严重安全风险。*（包含有害内容示例）*\n\n3.  **StableToolBench-MirrorAPI：将工具环境建模为 7000+ 真实世界 API 的镜像 (StableToolBench-MirrorAPI: Modeling Tool Environments as Mirrors of 7,000+ Real-World APIs)**\n    *   为解决工具学习基准测试中稳定性、可扩展性和真实性的平衡问题，该研究提出 MirrorAPI 框架。通过训练专门的 LLM 来模拟 7000 多个真实 API 的响应，创建更稳定和真实的工具环境 \"镜像\"，并构建了 MirrorAPI-Bench 进行评估。\n\n4.  **GAIA-2：用于自动驾驶的可控多视图生成世界模型 (GAIA-2: A Controllable Multi-View Generative World Model for Autonomous Driving)**\n    *   Wayve 公司推出的 GAIA-2 是一个潜在扩散世界模型，专为自动驾驶设计。它能根据车辆动态、智能体配置、环境因素和道路语义等结构化输入，生成高分辨率、时空一致的多摄像头视频，支持跨地域（英美德）驾驶环境，并能整合外部潜在嵌入，用于模拟常见和罕见驾驶场景。\n\n---\n\n**LLM 推理、评估与应用**\n\n5.  **理解类 R1-Zero 训练：批判性视角 (Understanding R1-Zero-Like Training: A Critical Perspective)**\n    *   本文深入分析了类 R1-Zero 训练（通过强化学习提升 LLM 推理能力）的两个核心：基础模型和 RL 算法。研究发现某些基础模型（如 DeepSeek-V3-Base）本身已有推理潜力，并指出 GRPO 优化存在偏差。提出了改进的 Dr. GRPO 和精简的 R1-Zero 配方，在 AIME 2024 上用 7B 模型达到 43.3% 的 SOTA 准确率。\n\n6.  **Reason-RFT：用于视觉推理的强化微调 (Reason-RFT: Reinforcement Fine-Tuning for Visual Reasoning)**\n    *   针对现有 VLM 视觉推理方法（基于 CoT 的 SFT）可能导致的过拟合和泛化能力不足问题，提出 Reason-RFT 框架。该框架结合 SFT 激活 VLM 推理潜力和基于 GRPO 的 RL 增强泛化能力，在视觉计数、结构感知、空间变换等任务上取得 SOTA 性能，并展示了良好的泛化性和数据效率。\n\n7.  **Mobile-MMLU：移动智能语言理解基准 (Mobile-MMLU: A Mobile Intelligence Language Understanding Benchmark)**\n    *   针对移动设备上 LLM 应用的特殊性（交互方式、资源限制）和缺乏专用基准的问题，本文推出 Mobile-MMLU 数据集。包含 16186 个问题，覆盖 80 个移动相关领域，并提供更难的子集 Mobile-MMLU-Pro，用于评估 LLM 在移动场景下的性能、效率和知识优先级。\n\n8.  **QualiSpeech：带有自然语言推理和描述的语音质量评估数据集 (QualiSpeech: A Speech Quality Assessment Dataset with Natural Language Reasoning and Descriptions)**\n    *   本文提出用自然语言描述进行语音质量评估的新视角，认为其比传统数值评分更丰富、更有指导性。为此，构建了 QualiSpeech 数据集，包含 11 个维度的详细自然语言评论。并提出 QualiSpeech Benchmark 评估听觉 LLM 的低层语音理解能力，实验表明微调后的模型能可靠生成噪声和失真的详细描述。\n\n9.  **利用隐式情感：增强 LLM 心理特质评估的信度和效度 (Leveraging Implicit Sentiments: Enhancing Reliability and Validity in Psychological Trait Evaluation of LLMs)**\n    *   现有基于人类心理量表的 LLM 心理评估方法信效度有限。本文提出专为 LLM 设计的双语评估工具 Core Sentiment Inventory (CSI)，通过隐式评估模型的情感倾向（乐观、悲观、中性），提供更可靠、有效的 LLM 心理画像，且与 LLM 真实世界输出的情感相关性超过 0.85。\n\n10. **大型语言模型能预测人类态度之间的关联吗？(Can Large Language Models Predict Associations Among Human Attitudes?)**\n    *   研究发现，前沿 LLM (GPT-4o) 不仅能预测高度相似的态度，还能在缺乏表面相似性的情况下，预测不同主题态度之间的关联，表明 LLM 捕捉到了人类信念系统更深层次的潜在结构。\n\n11. **通过语义超图推理实现 N 元关系事实的归纳式链接预测 (Inductive Link Prediction on N-ary Relational Facts via Semantic Hypergraph Reasoning)**\n    *   针对知识图中 N 元关系（涉及两个以上实体）的归纳式链接预测（ILP）挑战，本文提出基于 N 元子图推理的框架。引入 N 元语义超图结构和子图聚合网络 NS-HART，有效挖掘子图中的复杂语义关联，在多个归纳基准上表现优越。\n\n---\n\n**多模态与生成模型**\n\n12. **统一多模态离散扩散 (Unified Multimodal Discrete Diffusion)**\n    *   探索使用离散扩散模型作为统一的文本和图像联合生成框架。提出 UniDisc 模型，相比自回归（AR）模型，具有更好的质量/多样性控制、联合多模态修复能力和更强的可控性。实验表明 UniDisc 在性能、推理计算、可控性等方面优于多模态 AR 模型。\n\n13. **VinaBench：忠实且一致的视觉叙事基准 (VinaBench: Benchmark for Faithful and Consistent Visual Narratives)**\n    *   为解决视觉叙事生成中忠实于文本和图像间一致性的挑战，提出 VinaBench 基准。该基准标注了视觉叙事样本中的常识和语篇约束，并提出新指标评估生成图像的一致性和与文本的对齐度。实验证明使用 VinaBench 约束能有效提升生成质量。\n\n14. **ADS-Edit：用于自动驾驶系统的多模态知识编辑数据集 (ADS-Edit: A Multimodal Knowledge Editing Dataset for Autonomous Driving Systems)**\n    *   为解决大型多模态模型 (LMM) 在自动驾驶系统 (ADS) 中面临的交通知识误解、复杂路况等挑战，本文提出使用知识编辑技术进行针对性修改，并引入首个专为 ADS 设计的多模态知识编辑数据集 ADS-Edit，包含真实场景、多数据类型和评估指标。\n\n15. **合成视频增强视频合成中的物理保真度 (Synthetic Video Enhances Physical Fidelity in Video Synthesis)**\n    *   研究如何利用计算机图形学渲染的、遵循物理规律的合成视频来提高视频生成模型的物理保真度。提出一种方法来整合合成数据并将其物理真实感迁移到模型中，显著减少不期望的伪影。实验证明该方法能有效提升物理一致性。\n\n16. **AccidentSim：从真实世界事故报告生成物理真实的车辆碰撞视频 (AccidentSim: Generating Physically Realistic Vehicle Collision Videos from Real-World Accident Reports)**\n    *   为解决真实车辆事故视频难以收集的问题，提出 AccidentSim 框架。利用物理模拟器从事故报告中提取信息，复现碰撞后轨迹并构建数据集，微调语言模型预测轨迹，最后用 NeRF 渲染背景并融合车辆，生成兼具视觉和物理真实感的碰撞视频。\n\n17. **利用相对和绝对位置匹配在单 GPU 上实现高质量扩散蒸馏 (High Quality Diffusion Distillation on a Single GPU with Relative and Absolute Position Matching)**\n    *   提出 RAPM，一种新的扩散蒸馏方法，可在单 GPU 上高效训练，生成高质量图像。通过匹配教师模型的相对和绝对位置采样轨迹，并引入两个判别器，RAPM 在有限计算资源下，用 4 个时间步即可达到 SOTA 方法 1 个时间步的 FID 分数。\n\n---\n\n**强化学习与机器人**\n\n18. **MoLe-VLA：通过层混合实现动态跳层视觉语言动作模型以实现高效机器人操作 (MoLe-VLA: Dynamic Layer-skipping Vision Language Action Model via Mixture-of-Layers for Efficient Robot Manipulation)**\n    *   为解决 MLLM 在机器人应用中计算和存储开销大的问题，受层混合（MoE）启发，提出 MoLe-VLA 架构。将 LLM 每层视为专家，通过时空感知路由器（STAR）动态选择性激活部分层，并使用认知自知识蒸馏（CogKD）补偿损失的认知能力。实验表明，MoLe-VLA 在保持甚至提升性能（成功率+8%）的同时，显著降低计算成本（高达 5.6 倍）。\n\n19. **通过自适应梯度掩码对抗攻击实现鲁棒的机器人深度强化学习 (Robust Deep Reinforcement Learning in Robotics via Adaptive Gradient-Masked Adversarial Attacks)**\n    *   针对现有白盒对抗攻击方法在 DRL 中效果不佳的问题（忽视时序动态、未区分状态维度重要性），提出 AGMR 攻击。结合 DRL 和梯度软掩码机制，动态识别关键状态维度并优化对抗策略，选择性分配扰动，有效降低受害者智能体性能，并可通过对抗防御提升其鲁棒性。\n\n20. **用于鲁棒深度强化学习的状态感知扰动优化 (State-Aware Perturbation Optimization for Robust Deep Reinforcement Learning)**\n    *   进一步分析 DRL 中的白盒攻击，提出 AVD-MDP 理论框架，并基于此设计了 STAR 对抗攻击方法。STAR 使用软掩码机制选择性攻击状态以增强隐蔽性，并通过信息论优化目标引导受害者智能体进入脆弱状态，最大化回报降低。\n\n21. **从单次演示中学习自适应灵巧抓取 (Learning Adaptive Dexterous Grasping from Single Demonstrations)**\n    *   提出 AdaDexGrasp 框架，解决机器人灵巧抓取技能学习效率低和上下文适应性差的问题。通过单次人类演示结合轨迹跟随奖励进行 RL，利用课程学习增强鲁棒性，并使用 VLM 根据用户指令选择技能。实验证明该方法显著提高了 RL 效率，并成功迁移到真实机械手。\n\n22. **具有离散扩散技能的离线强化学习 (Offline Reinforcement Learning with Discrete Diffusion Skills)**\n    *   探索在离线 RL 中使用离散技能空间的可能性。提出 DDS 方法，利用 Transformer 编码器和扩散解码器构建紧凑的离散技能空间，并结合高层策略形成分层 RL 框架。实验表明 DDS 在 D4RL 基准测试中表现优异，尤其在长时序任务上显著优于现有方法，且具有更好的可解释性和稳定性。\n\n---\n\n**AI 安全、鲁棒性与隐私**\n\n23. **原型引导的后门防御 (Prototype Guided Backdoor Defense)**\n    *   提出 PGBD，一种鲁棒的后训练（post-hoc）后门防御方法，可有效防御包括语义触发器在内的多种后门攻击。PGBD 利用激活空间中的几何位移来惩罚朝向触发器的移动，通过新颖的净化损失进行微调。该方法在各种攻击设置下均表现更优，并首次防御了针对名人面部图像的新型语义攻击。\n\n24. **针对图结构扰动的鲁棒集成方法 β-GNN ($\\beta$-GNN: A Robust Ensemble Approach Against Graph Structure Perturbation)**\n    *   提出 β-GNN 模型，通过加权集成任意 GNN 和 MLP 来提高 GNN 的鲁棒性，同时不牺牲干净数据上的性能。学习到的动态权重 β 不仅调节 GNN 的贡献，还能指示数据扰动水平。实验证明 β-GNN 在对抗准确性和攻击严重性量化方面表现优异。\n\n25. **TS-Inverse：针对联邦时间序列预测模型的梯度反演攻击 (TS-Inverse: A Gradient Inversion Attack Tailored for Federated Time Series Forecasting Models)**\n    *   研究联邦学习（FL）中时间序列预测（TSF）任务的隐私风险。发现现有梯度反演攻击（GIA）在 TSF 任务上的挑战，并提出 TS-Inverse 攻击方法。通过学习输出分位数预测的梯度反演模型、结合周期性和趋势正则化的损失函数以及分位数正则化，显著提高了 TS 数据的反演效果（sMAPE 提升 2-10 倍）。\n\n26. **基于 GAN 的防御框架：鲁棒联邦学习对抗投毒攻击 (Robust Federated Learning Against Poisoning Attacks: A GAN-Based Defense Framework)**\n    *   为解决 FL 中投毒攻击问题和现有防御方法的局限性（依赖外部数据或预定义启发式），提出一种基于条件 GAN (cGAN) 的隐私保护防御框架。服务器端使用 cGAN 生成合成数据来验证客户端更新，无需外部数据集，具有可扩展性和自适应性。实验证明其能有效防御多种投毒攻击。\n\n27. **弱 AI 安全法规的反作用效应 (The Backfiring Effect of Weak AI Safety Regulation)**\n    *   通过博弈论模型分析 AI 安全法规的效果。研究发现，仅对领域专家（应用适配者）施加弱安全法规可能适得其反，导致安全性降低。相比之下，对 AI 创建者和领域专家同时施加适当的强法规，可以作为承诺机制，带来安全和性能的双重提升。\n\n---\n\n**其他值得关注的论文**\n\n*   **FinAudio (Paper 4):** 首个评估金融领域音频大语言模型（AudioLLMs）能力的基准，定义了金融短/长音频 ASR 和长音频摘要任务，并评估了现有模型。\n*   **LATTE-MV (Paper 10):** 从单目视频学习预测乒乓球击球点，通过 3D 重建和不确定性感知控制器提升了高速回球率。\n*   **UniDisc (Paper 17):** 首个统一的多模态离散扩散模型，能联合理解和生成文本与图像，在性能、可控性等方面优于自回归模型。\n*   **Flip Learning (Paper 26):** 一种基于多智能体强化学习的弱监督分割方法（仅需边界框），通过让智能体“擦除”目标区域来预测分割掩码，在乳腺超声图像分割上表现优异。\n*   **MetaChat (Paper 44):** 一个多智能体设计框架，能将语义描述的光子学设计目标自动转化为高性能自由形态器件布局，利用 AIM 范式和代理求解器实现近实时设计。\n*   **VideoGEM (Paper 52):** 首个基于预训练图像/视频语言模型的免训练视频动作空间定位方法，通过调整自注意力层权重和提示分解实现。\n*   **CryoSAMU (Paper 57):** 使用结构感知的多模态 U-Net 增强中等分辨率（4-8Å）冷冻电镜（Cryo-EM）蛋白质结构密度图，性能优于 SOTA 且速度更快。\n*   **FPET (Paper 60):** 通过即插即用的令牌冗余减少模块，加速参数高效微调（PET），在保持性能的同时提高推理速度和训练效率。\n*   **SARGes (Paper 73):** 利用 LLM 解析语音内容生成可靠的语义手势标签，指导合成与语音语义对齐的、可解释的伴随手势。\n*   **TV3S (Paper 77):** 利用 Mamba 状态空间模型进行时间特征共享的视频语义分割架构，通过选择性门控机制实现长时序连贯性和高效计算。\n*   **TheoryCoder (Paper 82):** 基于理论的强化学习（TBRL）实例，利用理论的层次化表示和高效程序合成（LLM 辅助）进行更强大的学习和规划，通过双层规划解决复杂网格世界游戏。\n\n---\n\n希望这份 TLDR 能帮助你快速了解今日 arXiv 的精华！",
  "papers": [
    {
      "arxiv_id": "2503.21036v1",
      "title": "The Art of Tool Interface Design",
      "title_zh": "工具界面设计艺术\n",
      "authors": [
        "Yunnan Wu",
        "Paul Chen",
        "Deshank Baranwal",
        "Jinlong Zhou",
        "Jian Yuan"
      ],
      "abstract": "We present an agentic framework, Thinker, which achieves state of art\nperformance in challenging reasoning tasks for realistic customer service\nscenarios that involve complex business logic and human interactions via long\nhorizons. On the $\\tau$-bench retail dataset, Thinker achieves 82.6\\% success\nrate with GPT-4o (version 2024-06-01) (baseline: 68.3\\%), and 81.9\\% success\nrate with Llama-3.1 405B (baseline: 49.6\\%), without any fine-tuning. Thinker\neffectively closes the gap in reasoning capabilities between the base models by\nintroducing proper structure.\n  The key features of the Thinker framework are: (1) State-Machine Augmented\nGeneration (SMAG), which represents business logic as state machines and the\nLLM uses state machines as tools. (2) Delegation of tasks from the main\nreasoning loop to LLM-powered tools. (3) Adaptive context management.\n  Our prompting-only solution achieves signficant gains, while still\nmaintaining a standard agentic architecture with a ReAct style reasoning loop.\nThe key is to innovate on the tool interface design, as exemplified by SMAG and\nthe LLM-powered tools.",
      "tldr_zh": "该论文介绍了一种名为Thinker的智能体框架，旨在提升大型语言模型(LLM)在复杂推理任务中的表现，尤其是在涉及复杂商业逻辑和人机交互的真实客户服务场景中。Thinker框架通过引入状态机增强生成(SMAG)、任务委派和自适应上下文管理等关键特性，有效地弥补了基础模型在推理能力上的差距。在$\\tau$-bench零售数据集上的实验结果表明，Thinker框架在GPT-4o和Llama-3.1 405B上的成功率分别达到82.6%和81.9%，显著优于基线模型，且无需任何微调。该研究强调了工具接口设计的重要性，并通过SMAG和LLM驱动的工具进行创新，实现了显著的性能提升。\n",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.21036v1",
      "published_date": "2025-03-26 23:02:00 UTC",
      "updated_date": "2025-03-26 23:02:00 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-29T00:59:32.888660"
    },
    {
      "arxiv_id": "2503.21011v1",
      "title": "Can Large Language Models Predict Associations Among Human Attitudes?",
      "title_zh": "大型语言模型能否预测人类态度之间的关联？\n",
      "authors": [
        "Ana Ma",
        "Derek Powell"
      ],
      "abstract": "Prior work has shown that large language models (LLMs) can predict human\nattitudes based on other attitudes, but this work has largely focused on\npredictions from highly similar and interrelated attitudes. In contrast, human\nattitudes are often strongly associated even across disparate and dissimilar\ntopics. Using a novel dataset of human responses toward diverse attitude\nstatements, we found that a frontier language model (GPT-4o) was able to\nrecreate the pairwise correlations among individual attitudes and to predict\nindividuals' attitudes from one another. Crucially, in an advance over prior\nwork, we tested GPT-4o's ability to predict in the absence of\nsurface-similarity between attitudes, finding that while surface similarity\nimproves prediction accuracy, the model was still highly-capable of generating\nmeaningful social inferences between dissimilar attitudes. Altogether, our\nfindings indicate that LLMs capture crucial aspects of the deeper, latent\nstructure of human belief systems.",
      "tldr_zh": "该研究探讨了大型语言模型(LLMs)预测人类态度关联的能力，尤其是在态度之间缺乏表面相似性的情况下。作者构建了一个包含多样化态度陈述的人类回应数据集，并使用GPT-4o进行实验。结果表明，GPT-4o能够重现个体态度之间的成对相关性，并能基于一个人的态度预测其对其他问题的看法。即使在态度之间缺乏表面相似性的情况下，模型仍然能够进行有意义的社会推断，表明LLMs能够捕捉人类信念系统深层、潜在的结构。研究结果证实了LLMs在理解和预测复杂人类行为方面的潜力。\n",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.21011v1",
      "published_date": "2025-03-26 21:58:43 UTC",
      "updated_date": "2025-03-26 21:58:43 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-29T00:59:44.467700"
    },
    {
      "arxiv_id": "2503.21000v1",
      "title": "Improving User Behavior Prediction: Leveraging Annotator Metadata in Supervised Machine Learning Models",
      "title_zh": "改进用户行为预测：在监督式机器学习模型中利用标注者元数据\n",
      "authors": [
        "Lynnette Hui Xian Ng",
        "Kokil Jaidka",
        "Kaiyuan Tay",
        "Hansin Ahuja",
        "Niyati Chhaya"
      ],
      "abstract": "Supervised machine-learning models often underperform in predicting user\nbehaviors from conversational text, hindered by poor crowdsourced label quality\nand low NLP task accuracy. We introduce the Metadata-Sensitive\nWeighted-Encoding Ensemble Model (MSWEEM), which integrates annotator\nmeta-features like fatigue and speeding. First, our results show MSWEEM\noutperforms standard ensembles by 14\\% on held-out data and 12\\% on an\nalternative dataset. Second, we find that incorporating signals of annotator\nbehavior, such as speed and fatigue, significantly boosts model performance.\nThird, we find that annotators with higher qualifications, such as Master's,\ndeliver more consistent and faster annotations. Given the increasing\nuncertainty over annotation quality, our experiments show that understanding\nannotator patterns is crucial for enhancing model accuracy in user behavior\nprediction.",
      "tldr_zh": "这篇论文提出了Metadata-Sensitive Weighted-Encoding Ensemble Model (MSWEEM)，旨在提升利用对话文本预测用户行为的监督学习模型的性能。MSWEEM集成了标注员的元特征，如疲劳度和速度。实验结果表明，MSWEEM在保留数据上的表现优于标准集成模型14%，在替代数据集上优于12%。研究发现，结合标注员行为信号（如速度和疲劳度）能够显著提升模型性能，并且更高资质的标注员（如硕士）能够提供更一致和更快速的标注。该研究强调，理解标注员模式对于提高用户行为预测模型的准确性至关重要。\n",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "Accepted at CSCW 2025",
      "pdf_url": "http://arxiv.org/pdf/2503.21000v1",
      "published_date": "2025-03-26 21:30:48 UTC",
      "updated_date": "2025-03-26 21:30:48 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-29T00:59:56.484302"
    },
    {
      "arxiv_id": "2503.20990v1",
      "title": "FinAudio: A Benchmark for Audio Large Language Models in Financial Applications",
      "title_zh": "FinAudio：金融应用中音频大语言模型的基准测试\n",
      "authors": [
        "Yupeng Cao",
        "Haohang Li",
        "Yangyang Yu",
        "Shashidhar Reddy Javaji",
        "Yueru He",
        "Jimin Huang",
        "Zining Zhu",
        "Qianqian Xie",
        "Xiao-yang Liu",
        "Koduvayur Subbalakshmi",
        "Meikang Qiu",
        "Sophia Ananiadou",
        "Jian-Yun Nie"
      ],
      "abstract": "Audio Large Language Models (AudioLLMs) have received widespread attention\nand have significantly improved performance on audio tasks such as\nconversation, audio understanding, and automatic speech recognition (ASR).\nDespite these advancements, there is an absence of a benchmark for assessing\nAudioLLMs in financial scenarios, where audio data, such as earnings conference\ncalls and CEO speeches, are crucial resources for financial analysis and\ninvestment decisions. In this paper, we introduce \\textsc{FinAudio}, the first\nbenchmark designed to evaluate the capacity of AudioLLMs in the financial\ndomain. We first define three tasks based on the unique characteristics of the\nfinancial domain: 1) ASR for short financial audio, 2) ASR for long financial\naudio, and 3) summarization of long financial audio. Then, we curate two short\nand two long audio datasets, respectively, and develop a novel dataset for\nfinancial audio summarization, comprising the \\textsc{FinAudio} benchmark.\nThen, we evaluate seven prevalent AudioLLMs on \\textsc{FinAudio}. Our\nevaluation reveals the limitations of existing AudioLLMs in the financial\ndomain and offers insights for improving AudioLLMs. All datasets and codes will\nbe released.",
      "tldr_zh": "本文提出了FinAudio，一个专门用于评估音频大语言模型(AudioLLMs)在金融领域应用能力的基准。该基准包含三个任务：针对短金融音频的自动语音识别(ASR)、针对长金融音频的ASR以及长金融音频的摘要生成。研究者们构建了相应的短音频、长音频和音频摘要数据集，共同组成了FinAudio基准。通过对七种主流AudioLLMs在FinAudio上的评估，揭示了现有模型在金融领域的局限性，并为未来AudioLLMs的改进提供了方向。数据集和代码将会开源。\n",
      "categories": [
        "cs.CE",
        "cs.AI",
        "cs.MM"
      ],
      "primary_category": "cs.CE",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.20990v1",
      "published_date": "2025-03-26 21:07:51 UTC",
      "updated_date": "2025-03-26 21:07:51 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-29T01:00:08.377366"
    },
    {
      "arxiv_id": "2503.20981v1",
      "title": "Patients Speak, AI Listens: LLM-based Analysis of Online Reviews Uncovers Key Drivers for Urgent Care Satisfaction",
      "title_zh": "患者倾诉，AI 倾听：基于 LLM 的在线评论分析揭示了急诊护理满意度的关键驱动因素\n",
      "authors": [
        "Xiaoran Xu",
        "Zhaoqian Xue",
        "Chi Zhang",
        "Jhonatan Medri",
        "Junjie Xiong",
        "Jiayan Zhou",
        "Jin Jin",
        "Yongfeng Zhang",
        "Siyuan Ma",
        "Lingyao Li"
      ],
      "abstract": "Investigating the public experience of urgent care facilities is essential\nfor promoting community healthcare development. Traditional survey methods\noften fall short due to limited scope, time, and spatial coverage.\nCrowdsourcing through online reviews or social media offers a valuable approach\nto gaining such insights. With recent advancements in large language models\n(LLMs), extracting nuanced perceptions from reviews has become feasible. This\nstudy collects Google Maps reviews across the DMV and Florida areas and\nconducts prompt engineering with the GPT model to analyze the aspect-based\nsentiment of urgent care. We first analyze the geospatial patterns of various\naspects, including interpersonal factors, operational efficiency, technical\nquality, finances, and facilities. Next, we determine Census Block\nGroup(CBG)-level characteristics underpinning differences in public perception,\nincluding population density, median income, GINI Index, rent-to-income ratio,\nhousehold below poverty rate, no insurance rate, and unemployment rate. Our\nresults show that interpersonal factors and operational efficiency emerge as\nthe strongest determinants of patient satisfaction in urgent care, while\ntechnical quality, finances, and facilities show no significant independent\neffects when adjusted for in multivariate models. Among socioeconomic and\ndemographic factors, only population density demonstrates a significant but\nmodest association with patient ratings, while the remaining factors exhibit no\nsignificant correlations. Overall, this study highlights the potential of\ncrowdsourcing to uncover the key factors that matter to residents and provide\nvaluable insights for stakeholders to improve public satisfaction with urgent\ncare.",
      "tldr_zh": "本研究利用大型语言模型(LLMs)分析Google Maps上关于DMV和佛罗里达地区急诊中心的在线评论，旨在挖掘影响患者满意度的关键因素。通过提示工程和GPT模型，分析了人际关系、运营效率、技术质量、财务和设施等方面的观点和情感。研究发现，人际关系和运营效率是患者满意度的最强决定因素，而技术质量、财务和设施的影响不显著。在社会经济和人口因素中，只有人口密度与患者评分有显著但适度的关联。该研究强调了众包方法在揭示影响居民的关键因素方面的潜力，并为利益相关者改进急诊中心服务提供了有价值的见解。\n",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.SI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.20981v1",
      "published_date": "2025-03-26 20:45:01 UTC",
      "updated_date": "2025-03-26 20:45:01 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-29T01:00:20.638345"
    },
    {
      "arxiv_id": "2503.20975v1",
      "title": "Competitive Multi-armed Bandit Games for Resource Sharing",
      "title_zh": "用于资源共享的竞争性多臂老虎机博弈",
      "authors": [
        "Hongbo Li",
        "Lingjie Duan"
      ],
      "abstract": "In modern resource-sharing systems, multiple agents access limited resources\nwith unknown stochastic conditions to perform tasks. When multiple agents\naccess the same resource (arm) simultaneously, they compete for successful\nusage, leading to contention and reduced rewards. This motivates our study of\ncompetitive multi-armed bandit (CMAB) games. In this paper, we study a new\nN-player K-arm competitive MAB game, where non-myopic players (agents) compete\nwith each other to form diverse private estimations of unknown arms over time.\nTheir possible collisions on same arms and time-varying nature of arm rewards\nmake the policy analysis more involved than existing studies for myopic\nplayers. We explicitly analyze the threshold-based structures of social optimum\nand existing selfish policy, showing that the latter causes prolonged\nconvergence time $\\Omega(\\frac{K}{\\eta^2}\\ln({\\frac{KN}{\\delta}}))$, while\nsocially optimal policy with coordinated communication reduces it to\n$\\mathcal{O}(\\frac{K}{N\\eta^2}\\ln{(\\frac{K}{\\delta})})$. Based on the\ncomparison, we prove that the competition among selfish players for the best\narm can result in an infinite price of anarchy (PoA), indicating an arbitrarily\nlarge efficiency loss compared to social optimum. We further prove that no\ninformational (non-monetary) mechanism (including Bayesian persuasion) can\nreduce the infinite PoA, as the strategic misreporting by non-myopic players\nundermines such approaches. To address this, we propose a Combined\nInformational and Side-Payment (CISP) mechanism, which provides socially\noptimal arm recommendations with proper informational and monetary incentives\nto players according to their time-varying private beliefs. Our CISP mechanism\nkeeps ex-post budget balanced for social planner and ensures truthful reporting\nfrom players, achieving the minimum PoA=1 and same convergence time as social\noptimum.",
      "tldr_zh": "本文研究了一种新的N人K臂竞争多臂老虎机（CMAB）博弈，其中非短视的玩家竞争以形成对未知臂的不同的私有估计。论文分析了社会最优策略和现有自私策略的阈值结构，表明后者会导致较长的收敛时间，而具有协调通信的社会最优策略可以减少收敛时间。研究证明，自私玩家之间对最佳臂的竞争可能导致无限的无政府状态代价（PoA）。此外，论文证明没有信息机制可以降低无限的PoA。为了解决这个问题，论文提出了一种组合信息和边支付（CISP）机制，该机制根据玩家随时间变化的私有信念，为玩家提供具有适当信息和金钱激励的社会最优臂推荐。CISP机制保持了社会规划者的事后预算平衡，并确保了玩家的真实报告，实现了最小的PoA=1和与社会最优相同的收敛时间。\n",
      "categories": [
        "cs.GT",
        "cs.AI"
      ],
      "primary_category": "cs.GT",
      "comment": "This paper has been accepted by IEEE TMC",
      "pdf_url": "http://arxiv.org/pdf/2503.20975v1",
      "published_date": "2025-03-26 20:35:18 UTC",
      "updated_date": "2025-03-26 20:35:18 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-29T01:00:32.885882"
    },
    {
      "arxiv_id": "2503.20959v1",
      "title": "Sociotechnical Effects of Machine Translation",
      "title_zh": "机器翻译的社会技术影响\n",
      "authors": [
        "Joss Moorkens",
        "Andy Way",
        "Séamus Lankford"
      ],
      "abstract": "While the previous chapters have shown how machine translation (MT) can be\nuseful, in this chapter we discuss some of the side-effects and risks that are\nassociated, and how they might be mitigated. With the move to neural MT and\napproaches using Large Language Models (LLMs), there is an associated impact on\nclimate change, as the models built by multinational corporations are massive.\nThey are hugely expensive to train, consume large amounts of electricity, and\noutput huge volumes of kgCO2 to boot. However, smaller models which still\nperform to a high level of quality can be built with much lower carbon\nfootprints, and tuning pre-trained models saves on the requirement to train\nfrom scratch. We also discuss the possible detrimental effects of MT on\ntranslators and other users. The topics of copyright and ownership of data are\ndiscussed, as well as ethical considerations on data and MT use. Finally, we\nshow how if done properly, using MT in crisis scenarios can save lives, and we\nprovide a method of how this might be done.",
      "tldr_zh": "本文探讨了机器翻译(MT)带来的社会技术影响和潜在风险。随着神经机器翻译(NMT)和基于大型语言模型(LLM)的方法兴起，气候变化问题日益突出，因为大型模型训练成本高昂，耗电量大，并产生大量二氧化碳。然而，可以通过构建更小、高质量的模型或微调预训练模型来降低碳排放。此外，文章还讨论了MT对翻译人员和其他用户的潜在不利影响，以及数据版权、所有权和伦理方面的考量。最后，文章指出在危机情况下合理使用MT可以拯救生命，并提供了一种可行的方法。\n",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.20959v1",
      "published_date": "2025-03-26 19:49:46 UTC",
      "updated_date": "2025-03-26 19:49:46 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-29T01:00:44.478955"
    },
    {
      "arxiv_id": "2503.20952v1",
      "title": "TS-Inverse: A Gradient Inversion Attack Tailored for Federated Time Series Forecasting Models",
      "title_zh": "TS-Inverse：一种为联邦时间序列预测模型量身定制的梯度反演攻击\n",
      "authors": [
        "Caspar Meijer",
        "Jiyue Huang",
        "Shreshtha Sharma",
        "Elena Lazovik",
        "Lydia Y. Chen"
      ],
      "abstract": "Federated learning (FL) for time series forecasting (TSF) enables clients\nwith privacy-sensitive time series (TS) data to collaboratively learn accurate\nforecasting models, for example, in energy load prediction. Unfortunately,\nprivacy risks in FL persist, as servers can potentially reconstruct clients'\ntraining data through gradient inversion attacks (GIA). Although GIA is\ndemonstrated for image classification tasks, little is known about time series\nregression tasks. In this paper, we first conduct an extensive empirical study\non inverting TS data across 4 TSF models and 4 datasets, identifying the unique\nchallenges of reconstructing both observations and targets of TS data. We then\npropose TS-Inverse, a novel GIA that improves the inversion of TS data by (i)\nlearning a gradient inversion model that outputs quantile predictions, (ii) a\nunique loss function that incorporates periodicity and trend regularization,\nand (iii) regularization according to the quantile predictions. Our evaluations\ndemonstrate a remarkable performance of TS-Inverse, achieving at least a 2x-10x\nimprovement in terms of the sMAPE metric over existing GIA methods on TS data.\nCode repository: https://github.com/Capsar/ts-inverse",
      "tldr_zh": "该论文提出了TS-Inverse，一种专门为联邦时间序列预测模型设计的梯度反演攻击(GIA)方法。针对时间序列数据（TS）反演的独特挑战，TS-Inverse通过学习一个输出分位数预测的梯度反演模型，并结合包含周期性和趋势正则化的损失函数，以及基于分位数预测的正则化，显著提升了TS数据的重构效果。在四个时间序列数据集和四个时间序列预测模型上的实验表明，TS-Inverse相比现有GIA方法，在sMAPE指标上实现了至少2到10倍的提升。该研究揭示了联邦学习中时间序列数据的隐私风险，并提供了一种有效的攻击手段。\n",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.20952v1",
      "published_date": "2025-03-26 19:35:49 UTC",
      "updated_date": "2025-03-26 19:35:49 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-29T01:00:56.557840"
    },
    {
      "arxiv_id": "2503.20950v1",
      "title": "DEMENTIA-PLAN: An Agent-Based Framework for Multi-Knowledge Graph Retrieval-Augmented Generation in Dementia Care",
      "title_zh": "DEMENTIA-PLAN：用于痴呆症护理中多知识图检索增强生成的一种基于智能体的框架\n",
      "authors": [
        "Yutong Song",
        "Chenhan Lyu",
        "Pengfei Zhang",
        "Sabine Brunswicker",
        "Nikil Dutt",
        "Amir Rahmani"
      ],
      "abstract": "Mild-stage dementia patients primarily experience two critical symptoms:\nsevere memory loss and emotional instability. To address these challenges, we\npropose DEMENTIA-PLAN, an innovative retrieval-augmented generation framework\nthat leverages large language models to enhance conversational support. Our\nmodel employs a multiple knowledge graph architecture, integrating various\ndimensional knowledge representations including daily routine graphs and life\nmemory graphs. Through this multi-graph architecture, DEMENTIA-PLAN\ncomprehensively addresses both immediate care needs and facilitates deeper\nemotional resonance through personal memories, helping stabilize patient mood\nwhile providing reliable memory support. Our notable innovation is the\nself-reflection planning agent, which systematically coordinates knowledge\nretrieval and semantic integration across multiple knowledge graphs, while\nscoring retrieved content from daily routine and life memory graphs to\ndynamically adjust their retrieval weights for optimized response generation.\nDEMENTIA-PLAN represents a significant advancement in the clinical application\nof large language models for dementia care, bridging the gap between AI tools\nand caregivers interventions.",
      "tldr_zh": "DEMENTIA-PLAN是一个基于智能体的检索增强生成框架，旨在为轻度痴呆症患者提供对话支持，解决其记忆力丧失和情绪不稳定的问题。该框架利用大型语言模型和多重知识图谱架构，整合日常活动图和生活记忆图等不同维度知识，全面满足患者的即时护理需求，并通过个人记忆产生更深的情感共鸣，稳定患者情绪。其创新之处在于自反思规划智能体，该智能体系统地协调跨多个知识图谱的知识检索和语义整合，并对检索到的内容进行评分，动态调整检索权重，以优化响应生成。DEMENTIA-PLAN代表了大型语言模型在痴呆症护理临床应用中的显著进步。\n",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "Accepted by AAAI 2025 Workshop on Knowledge Graphs for Personalized\n  Public Health",
      "pdf_url": "http://arxiv.org/pdf/2503.20950v1",
      "published_date": "2025-03-26 19:34:04 UTC",
      "updated_date": "2025-03-26 19:34:04 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-29T01:01:08.687310"
    },
    {
      "arxiv_id": "2503.20936v1",
      "title": "LATTE-MV: Learning to Anticipate Table Tennis Hits from Monocular Videos",
      "title_zh": "LATTE-MV：学习从单目视频预测乒乓球击球\n",
      "authors": [
        "Daniel Etaat",
        "Dvij Kalaria",
        "Nima Rahmanian",
        "Shankar Sastry"
      ],
      "abstract": "Physical agility is a necessary skill in competitive table tennis, but by no\nmeans sufficient. Champions excel in this fast-paced and highly dynamic\nenvironment by anticipating their opponent's intent - buying themselves the\nnecessary time to react. In this work, we take one step towards designing such\nan anticipatory agent. Previous works have developed systems capable of\nreal-time table tennis gameplay, though they often do not leverage\nanticipation. Among the works that forecast opponent actions, their approaches\nare limited by dataset size and variety. Our paper contributes (1) a scalable\nsystem for reconstructing monocular video of table tennis matches in 3D and (2)\nan uncertainty-aware controller that anticipates opponent actions. We\ndemonstrate in simulation that our policy improves the ball return rate against\nhigh-speed hits from 49.9% to 59.0% as compared to a baseline non-anticipatory\npolicy.",
      "tldr_zh": "该论文提出了LATTE-MV，一个从单目视频中学习预测乒乓球击球的系统。该系统旨在模拟优秀乒乓球运动员通过预测对手意图来争取反应时间的策略。论文主要贡献在于：(1) 提出了一个可扩展的系统，用于从单目乒乓球比赛视频中重建3D场景；(2) 设计了一个具有不确定性感知的控制器，用于预测对手的动作。仿真实验表明，与非预测策略相比，该策略在高球速击球下的回球率从49.9%提高到59.0%。\n",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "CVPR 2025",
      "pdf_url": "http://arxiv.org/pdf/2503.20936v1",
      "published_date": "2025-03-26 19:11:22 UTC",
      "updated_date": "2025-03-26 19:11:22 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-29T01:01:20.524427"
    },
    {
      "arxiv_id": "2503.20925v1",
      "title": "Prototype Guided Backdoor Defense",
      "title_zh": "原型引导的后门防御\n",
      "authors": [
        "Venkat Adithya Amula",
        "Sunayana Samavedam",
        "Saurabh Saini",
        "Avani Gupta",
        "Narayanan P J"
      ],
      "abstract": "Deep learning models are susceptible to {\\em backdoor attacks} involving\nmalicious attackers perturbing a small subset of training data with a {\\em\ntrigger} to causes misclassifications. Various triggers have been used,\nincluding semantic triggers that are easily realizable without requiring the\nattacker to manipulate the image. The emergence of generative AI has eased the\ngeneration of varied poisoned samples. Robustness across types of triggers is\ncrucial to effective defense. We propose Prototype Guided Backdoor Defense\n(PGBD), a robust post-hoc defense that scales across different trigger types,\nincluding previously unsolved semantic triggers. PGBD exploits displacements in\nthe geometric spaces of activations to penalize movements toward the trigger.\nThis is done using a novel sanitization loss of a post-hoc fine-tuning step.\nThe geometric approach scales easily to all types of attacks. PGBD achieves\nbetter performance across all settings. We also present the first defense\nagainst a new semantic attack on celebrity face images. Project page:\n\\hyperlink{https://venkatadithya9.github.io/pgbd.github.io/}{this https URL}.",
      "tldr_zh": "该论文提出了一种名为原型引导后门防御(Prototype Guided Backdoor Defense, PGBD)的鲁棒性后处理防御方法，旨在对抗深度学习模型中的后门攻击，特别是针对各种类型的触发器，包括难以解决的语义触发器。PGBD利用激活几何空间中的位移，通过一种新颖的清洗损失函数，惩罚模型向触发器的移动，从而达到防御效果。实验表明，PGBD在各种设置下均表现出更好的性能，并且首次成功防御了一种针对名人面部图像的新型语义攻击。该方法具有良好的可扩展性，适用于各种类型的攻击。\n",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.20925v1",
      "published_date": "2025-03-26 18:58:53 UTC",
      "updated_date": "2025-03-26 18:58:53 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-29T01:01:32.803742"
    },
    {
      "arxiv_id": "2503.20914v1",
      "title": "D4R -- Exploring and Querying Relational Graphs Using Natural Language and Large Language Models -- the Case of Historical Documents",
      "title_zh": "D4R —— 使用自然语言和大型语言模型探索和查询关系图 —— 以历史文献为例\n",
      "authors": [
        "Michel Boeglin",
        "David Kahn",
        "Josiane Mothe",
        "Diego Ortiz",
        "David Panzoli"
      ],
      "abstract": "D4R is a digital platform designed to assist non-technical users,\nparticularly historians, in exploring textual documents through advanced\ngraphical tools for text analysis and knowledge extraction. By leveraging a\nlarge language model, D4R translates natural language questions into Cypher\nqueries, enabling the retrieval of data from a Neo4J database. A user-friendly\ngraphical interface allows for intuitive interaction, enabling users to\nnavigate and analyse complex relational data extracted from unstructured\ntextual documents. Originally designed to bridge the gap between AI\ntechnologies and historical research, D4R's capabilities extend to various\nother domains. A demonstration video and a live software demo are available.",
      "tldr_zh": "D4R是一个数字平台，旨在帮助非技术用户（特别是历史学家）通过高级图形工具进行文本分析和知识提取，从而探索文本文件。它利用大型语言模型将自然语言问题转换为Cypher查询，从而能够从Neo4J数据库中检索数据。D4R提供了一个用户友好的图形界面，允许直观的交互，使用户能够导航和分析从非结构化文本文件中提取的复杂关系数据。最初旨在弥合人工智能技术和历史研究之间的差距，D4R的功能扩展到各种其他领域。该平台提供演示视频和实时软件演示。\n",
      "categories": [
        "cs.IR",
        "cs.AI",
        "cs.CL",
        "cs.LG",
        "H.3; H.3.3; I.2.7"
      ],
      "primary_category": "cs.IR",
      "comment": "8 pages, 7 figures",
      "pdf_url": "http://arxiv.org/pdf/2503.20914v1",
      "published_date": "2025-03-26 18:41:42 UTC",
      "updated_date": "2025-03-26 18:41:42 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-29T01:01:44.386827"
    },
    {
      "arxiv_id": "2503.20903v1",
      "title": "Assessing Generative Models for Structured Data",
      "title_zh": "结构化数据生成模型的评估\n",
      "authors": [
        "Reilly Cannon",
        "Nicolette M. Laird",
        "Caesar Vazquez",
        "Andy Lin",
        "Amy Wagler",
        "Tony Chiang"
      ],
      "abstract": "Synthetic tabular data generation has emerged as a promising method to\naddress limited data availability and privacy concerns. With the sharp increase\nin the performance of large language models in recent years, researchers have\nbeen interested in applying these models to the generation of tabular data.\nHowever, little is known about the quality of the generated tabular data from\nlarge language models. The predominant method for assessing the quality of\nsynthetic tabular data is the train-synthetic-test-real approach, where the\nartificial examples are compared to the original by how well machine learning\nmodels, trained separately on the real and synthetic sets, perform in some\ndownstream tasks. This method does not directly measure how closely the\ndistribution of generated data approximates that of the original. This paper\nintroduces rigorous methods for directly assessing synthetic tabular data\nagainst real data by looking at inter-column dependencies within the data. We\nfind that large language models (GPT-2), both when queried via few-shot\nprompting and when fine-tuned, and GAN (CTGAN) models do not produce data with\ndependencies that mirror the original real data. Results from this study can\ninform future practice in synthetic data generation to improve data quality.",
      "tldr_zh": "该论文评估了生成模型在结构化数据生成方面的能力，着重关注大型语言模型（LLMs）在生成合成表格数据时的质量问题。研究采用直接评估方法，通过分析数据内部的列间依赖关系，比较合成数据与真实数据的分布差异，而非传统的“train-synthetic-test-real”方法。实验结果表明，无论是通过少量样本提示还是微调，GPT-2以及GAN模型（CTGAN）生成的表格数据，其列间依赖关系与原始真实数据存在显著差异。这项研究为未来合成数据生成实践提供了指导，旨在提高数据质量。\n",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.20903v1",
      "published_date": "2025-03-26 18:19:05 UTC",
      "updated_date": "2025-03-26 18:19:05 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-29T01:01:56.577276"
    },
    {
      "arxiv_id": "2503.20884v1",
      "title": "Robust Federated Learning Against Poisoning Attacks: A GAN-Based Defense Framework",
      "title_zh": "面向中毒攻击的稳健联邦学习：一种基于 GAN 的防御框架\n",
      "authors": [
        "Usama Zafar",
        "André Teixeira",
        "Salman Toor"
      ],
      "abstract": "Federated Learning (FL) enables collaborative model training across\ndecentralized devices without sharing raw data, but it remains vulnerable to\npoisoning attacks that compromise model integrity. Existing defenses often rely\non external datasets or predefined heuristics (e.g. number of malicious\nclients), limiting their effectiveness and scalability. To address these\nlimitations, we propose a privacy-preserving defense framework that leverages a\nConditional Generative Adversarial Network (cGAN) to generate synthetic data at\nthe server for authenticating client updates, eliminating the need for external\ndatasets. Our framework is scalable, adaptive, and seamlessly integrates into\nFL workflows. Extensive experiments on benchmark datasets demonstrate its\nrobust performance against a variety of poisoning attacks, achieving high True\nPositive Rate (TPR) and True Negative Rate (TNR) of malicious and benign\nclients, respectively, while maintaining model accuracy. The proposed framework\noffers a practical and effective solution for securing federated learning\nsystems.",
      "tldr_zh": "本文提出了一种基于生成对抗网络(GAN)的联邦学习(FL)防御框架，旨在应对投毒攻击。该框架利用条件生成对抗网络(cGAN)在服务器端生成合成数据，用于验证客户端更新，从而无需外部数据集。该框架具有可扩展性和适应性，能够无缝集成到FL工作流程中。实验结果表明，该框架在各种投毒攻击下表现出强大的鲁棒性，实现了对恶意和良性客户端的高真阳性率(TPR)和真阴性率(TNR)，同时保持了模型精度，为保护联邦学习系统提供了一种实用有效的解决方案。\n",
      "categories": [
        "cs.CR",
        "cs.AI",
        "cs.DC"
      ],
      "primary_category": "cs.CR",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.20884v1",
      "published_date": "2025-03-26 18:00:56 UTC",
      "updated_date": "2025-03-26 18:00:56 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-29T01:02:08.461999"
    },
    {
      "arxiv_id": "2503.20871v1",
      "title": "VinaBench: Benchmark for Faithful and Consistent Visual Narratives",
      "title_zh": "VinaBench：忠实且一致的视觉叙事基准\n",
      "authors": [
        "Silin Gao",
        "Sheryl Mathew",
        "Li Mi",
        "Sepideh Mamooler",
        "Mengjie Zhao",
        "Hiromi Wakaki",
        "Yuki Mitsufuji",
        "Syrielle Montariol",
        "Antoine Bosselut"
      ],
      "abstract": "Visual narrative generation transforms textual narratives into sequences of\nimages illustrating the content of the text. However, generating visual\nnarratives that are faithful to the input text and self-consistent across\ngenerated images remains an open challenge, due to the lack of knowledge\nconstraints used for planning the stories. In this work, we propose a new\nbenchmark, VinaBench, to address this challenge. Our benchmark annotates the\nunderlying commonsense and discourse constraints in visual narrative samples,\noffering systematic scaffolds for learning the implicit strategies of visual\nstorytelling. Based on the incorporated narrative constraints, we further\npropose novel metrics to closely evaluate the consistency of generated\nnarrative images and the alignment of generations with the input textual\nnarrative. Our results across three generative vision models demonstrate that\nlearning with VinaBench's knowledge constraints effectively improves the\nfaithfulness and cohesion of generated visual narratives.",
      "tldr_zh": "VinaBench是一个新的视觉叙事生成基准，旨在解决生成忠实于输入文本且在生成图像中保持自洽的视觉叙事这一难题。该基准通过标注视觉叙事样本中潜在的常识和语篇约束，为学习视觉故事讲述的隐式策略提供系统性的支架。此外，论文还提出了新的指标，用于评估生成叙事图像的一致性以及生成内容与输入文本叙事的对齐程度。在三个生成视觉模型上的实验结果表明，利用VinaBench的知识约束进行学习可以有效提高生成视觉叙事的忠实度和连贯性。\n",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.CV",
      "comment": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern\n  Recognition (CVPR 2025)",
      "pdf_url": "http://arxiv.org/pdf/2503.20871v1",
      "published_date": "2025-03-26 18:00:03 UTC",
      "updated_date": "2025-03-26 18:00:03 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-29T01:02:20.646022"
    },
    {
      "arxiv_id": "2503.20786v1",
      "title": "Mobile-MMLU: A Mobile Intelligence Language Understanding Benchmark",
      "title_zh": "Mobile-MMLU：移动智能语言理解基准\n",
      "authors": [
        "Sondos Mahmoud Bsharat",
        "Mukul Ranjan",
        "Aidar Myrzakhan",
        "Jiacheng Liu",
        "Bowei Guo",
        "Shengkun Tang",
        "Zhuang Liu",
        "Yuanzhi Li",
        "Zhiqiang Shen"
      ],
      "abstract": "Rapid advancements in large language models (LLMs) have increased interest in\ndeploying them on mobile devices for on-device AI applications. Mobile users\ninteract differently with LLMs compared to desktop users, creating unique\nexpectations and data biases. Current benchmark datasets primarily target at\nserver and desktop environments, and there is a notable lack of extensive\ndatasets specifically designed for mobile contexts. Additionally, mobile\ndevices face strict limitations in storage and computing resources,\nconstraining model size and capabilities, thus requiring optimized efficiency\nand prioritized knowledge. To address these challenges, we introduce\nMobile-MMLU, a large-scale benchmark dataset tailored for mobile intelligence.\nIt consists of 16,186 questions across 80 mobile-related fields, designed to\nevaluate LLM performance in realistic mobile scenarios. A challenging subset,\nMobile-MMLU-Pro, provides advanced evaluation similar in size to MMLU-Pro but\nsignificantly more difficult than our standard full set. Both benchmarks use\nmultiple-choice, order-invariant questions focused on practical mobile\ninteractions, such as recipe suggestions, travel planning, and essential daily\ntasks. The dataset emphasizes critical mobile-specific metrics like inference\nlatency, energy consumption, memory usage, and response quality, offering\ncomprehensive insights into model performance under mobile constraints.\nMoreover, it prioritizes privacy and adaptability, assessing models' ability to\nperform on-device processing, maintain user privacy, and adapt to personalized\nusage patterns. Mobile-MMLU family offers a standardized framework for\ndeveloping and comparing mobile-optimized LLMs, enabling advancements in\nproductivity and decision-making within mobile computing environments. Our code\nand data are available at: https://github.com/VILA-Lab/Mobile-MMLU.",
      "tldr_zh": "为了评估大型语言模型(LLMs)在移动设备上的性能，并解决现有基准数据集缺乏移动环境针对性的问题，该研究提出了Mobile-MMLU，一个大规模的移动智能语言理解基准。该基准包含80个移动相关领域的16186个问题，旨在模拟真实的移动应用场景。同时，研究还提出了更具挑战性的子集Mobile-MMLU-Pro。Mobile-MMLU不仅评估模型的准确性，还关注推理延迟、能耗、内存使用和响应质量等移动设备的关键指标。该基准为开发和比较移动优化的LLMs提供了一个标准化的框架，促进移动计算环境中生产力和决策能力的提升。\n",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "An order-invariant and mobile-centric benchmark. Code and data are\n  available at: https://github.com/VILA-Lab/Mobile-MMLU",
      "pdf_url": "http://arxiv.org/pdf/2503.20786v1",
      "published_date": "2025-03-26 17:59:56 UTC",
      "updated_date": "2025-03-26 17:59:56 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-29T01:02:32.837707"
    },
    {
      "arxiv_id": "2503.20853v1",
      "title": "Unified Multimodal Discrete Diffusion",
      "title_zh": "统一多模态离散扩散\n",
      "authors": [
        "Alexander Swerdlow",
        "Mihir Prabhudesai",
        "Siddharth Gandhi",
        "Deepak Pathak",
        "Katerina Fragkiadaki"
      ],
      "abstract": "Multimodal generative models that can understand and generate across multiple\nmodalities are dominated by autoregressive (AR) approaches, which process\ntokens sequentially from left to right, or top to bottom. These models jointly\nhandle images, text, video, and audio for various tasks such as image\ncaptioning, question answering, and image generation. In this work, we explore\ndiscrete diffusion models as a unified generative formulation in the joint text\nand image domain, building upon their recent success in text generation.\nDiscrete diffusion models offer several advantages over AR models, including\nimproved control over quality versus diversity of generated samples, the\nability to perform joint multimodal inpainting (across both text and image\ndomains), and greater controllability in generation through guidance.\nLeveraging these benefits, we present the first Unified Multimodal Discrete\nDiffusion (UniDisc) model which is capable of jointly understanding and\ngenerating text and images for a variety of downstream tasks. We compare\nUniDisc to multimodal AR models, performing a scaling analysis and\ndemonstrating that UniDisc outperforms them in terms of both performance and\ninference-time compute, enhanced controllability, editability, inpainting, and\nflexible trade-off between inference time and generation quality. Code and\nadditional visualizations are available at https://unidisc.github.io.",
      "tldr_zh": "该论文提出了一个统一的多模态离散扩散模型(UniDisc)，用于联合理解和生成文本和图像。与目前主流的自回归(AR)模型相比，UniDisc在文本生成的基础上，利用离散扩散模型在质量与多样性控制、多模态联合修复(inpainting)以及生成可控性方面的优势，实现了图像和文本的联合生成。实验结果表明，UniDisc在性能和推理计算方面均优于多模态AR模型，并具有更强的可控性、可编辑性、修复能力以及推理时间和生成质量之间的灵活权衡。该研究为多模态生成提供了一种新的有效方法。\n",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG",
        "cs.RO"
      ],
      "primary_category": "cs.CV",
      "comment": "Project Website: https://unidisc.github.io",
      "pdf_url": "http://arxiv.org/pdf/2503.20853v1",
      "published_date": "2025-03-26 17:59:51 UTC",
      "updated_date": "2025-03-26 17:59:51 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-29T01:02:44.699225"
    },
    {
      "arxiv_id": "2503.20783v1",
      "title": "Understanding R1-Zero-Like Training: A Critical Perspective",
      "title_zh": "理解类 R1-Zero 训练：一种批判性视角\n",
      "authors": [
        "Zichen Liu",
        "Changyu Chen",
        "Wenjun Li",
        "Penghui Qi",
        "Tianyu Pang",
        "Chao Du",
        "Wee Sun Lee",
        "Min Lin"
      ],
      "abstract": "DeepSeek-R1-Zero has shown that reinforcement learning (RL) at scale can\ndirectly enhance the reasoning capabilities of LLMs without supervised\nfine-tuning. In this work, we critically examine R1-Zero-like training by\nanalyzing its two core components: base models and RL. We investigate a wide\nrange of base models, including DeepSeek-V3-Base, to understand how pretraining\ncharacteristics influence RL performance. Our analysis reveals that\nDeepSeek-V3-Base already exhibit ''Aha moment'', while Qwen2.5 base models\ndemonstrate strong reasoning capabilities even without prompt templates,\nsuggesting potential pretraining biases. Additionally, we identify an\noptimization bias in Group Relative Policy Optimization (GRPO), which\nartificially increases response length (especially for incorrect outputs)\nduring training. To address this, we introduce Dr. GRPO, an unbiased\noptimization method that improves token efficiency while maintaining reasoning\nperformance. Leveraging these insights, we present a minimalist R1-Zero recipe\nthat achieves 43.3% accuracy on AIME 2024 with a 7B base model, establishing a\nnew state-of-the-art. Our code is available at\nhttps://github.com/sail-sg/understand-r1-zero.",
      "tldr_zh": "该研究深入分析了类似DeepSeek-R1-Zero的训练方法，重点考察了基座模型和强化学习(RL)两个核心组成部分对LLM推理能力提升的影响。通过分析DeepSeek-V3-Base和Qwen2.5等基座模型，揭示了预训练特征对RL性能的影响，并指出了Group Relative Policy Optimization (GRPO) 中存在的优化偏差，即会人为增加回复长度，尤其是不正确的输出。为了解决这个问题，作者提出了Dr. GRPO，一种无偏优化方法，提高了token效率并保持了推理性能。基于这些发现，作者提出了一个极简的R1-Zero方案，在AIME 2024上使用7B基座模型实现了43.3%的准确率，达到了新的state-of-the-art。\n",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.20783v1",
      "published_date": "2025-03-26 17:59:14 UTC",
      "updated_date": "2025-03-26 17:59:14 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-29T01:02:56.864284"
    },
    {
      "arxiv_id": "2503.20756v1",
      "title": "ADS-Edit: A Multimodal Knowledge Editing Dataset for Autonomous Driving Systems",
      "title_zh": "ADS-Edit：面向自动驾驶系统的多模态知识编辑数据集\n",
      "authors": [
        "Chenxi Wang",
        "Jizhan Fang",
        "Xiang Chen",
        "Bozhong Tian",
        "Ziwen Xu",
        "Huajun Chen",
        "Ningyu Zhang"
      ],
      "abstract": "Recent advancements in Large Multimodal Models (LMMs) have shown promise in\nAutonomous Driving Systems (ADS). However, their direct application to ADS is\nhindered by challenges such as misunderstanding of traffic knowledge, complex\nroad conditions, and diverse states of vehicle. To address these challenges, we\npropose the use of Knowledge Editing, which enables targeted modifications to a\nmodel's behavior without the need for full retraining. Meanwhile, we introduce\nADS-Edit, a multimodal knowledge editing dataset specifically designed for ADS,\nwhich includes various real-world scenarios, multiple data types, and\ncomprehensive evaluation metrics. We conduct comprehensive experiments and\nderive several interesting conclusions. We hope that our work will contribute\nto the further advancement of knowledge editing applications in the field of\nautonomous driving. Code and data are available in\nhttps://github.com/zjunlp/EasyEdit.",
      "tldr_zh": "该论文提出了ADS-Edit，一个专为自动驾驶系统(ADS)设计的多模态知识编辑数据集，旨在解决大型多模态模型(LMMs)在ADS应用中对交通知识理解不足、复杂路况处理以及车辆状态多样性等挑战。ADS-Edit包含各种真实场景、多种数据类型和全面的评估指标，支持对模型行为进行有针对性的修改，而无需完全重新训练。通过实验验证，该数据集能够促进知识编辑技术在自动驾驶领域的进一步发展。代码和数据已开源。\n",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.CV",
        "cs.LG",
        "cs.MM"
      ],
      "primary_category": "cs.CL",
      "comment": "Work in progress",
      "pdf_url": "http://arxiv.org/pdf/2503.20756v1",
      "published_date": "2025-03-26 17:45:29 UTC",
      "updated_date": "2025-03-26 17:45:29 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-29T01:03:08.557844"
    },
    {
      "arxiv_id": "2503.20752v2",
      "title": "Reason-RFT: Reinforcement Fine-Tuning for Visual Reasoning",
      "title_zh": "Reason-RFT：用于视觉推理的强化微调\n",
      "authors": [
        "Huajie Tan",
        "Yuheng Ji",
        "Xiaoshuai Hao",
        "Minglan Lin",
        "Pengwei Wang",
        "Zhongyuan Wang",
        "Shanghang Zhang"
      ],
      "abstract": "Visual reasoning abilities play a crucial role in understanding complex\nmultimodal data, advancing both domain-specific applications and artificial\ngeneral intelligence (AGI). Existing methods improve VLM reasoning via\nChain-of-Thought (CoT) supervised fine-tuning, using meticulously annotated\ntraining data to enhance visual reasoning capabilities. However, this training\nparadigm may lead to overfitting and cognitive rigidity, restricting the\nmodel's ability to transfer visual reasoning skills across domains and limiting\nits real-world applicability. To address these limitations, we propose\nReason-RFT, a novel reinforcement fine-tuning framework that significantly\nenhances generalization capabilities in visual reasoning tasks. Reason-RFT\nintroduces a two-phase training framework for visual reasoning: (1) Supervised\nFine-Tuning (SFT) with curated Chain-of-Thought (CoT) data activates the\nreasoning potential of Vision-Language Models (VLMs), followed by (2) Group\nRelative Policy Optimization (GRPO)-based reinforcement learning that generates\nmultiple reasoning-response pairs, significantly enhancing generalization in\nvisual reasoning tasks. To evaluate Reason-RFT's visual reasoning capabilities,\nwe reconstructed a comprehensive dataset spanning visual counting, structure\nperception, and spatial transformation. Experimental results demonstrate\nReasoning-RFT's three key advantages: (1) Performance Enhancement: achieving\nstate-of-the-art results across multiple tasks, outperforming most mainstream\nopen-source and proprietary models; (2) Generalization Superiority:\nconsistently maintaining robust performance across diverse tasks and domains,\noutperforming alternative training paradigms; (3) Data Efficiency: excelling in\nfew-shot learning scenarios while surpassing full-dataset SFT baselines.\nProject website: https://tanhuajie.github.io/ReasonRFT",
      "tldr_zh": "该论文提出了Reason-RFT，一种用于视觉推理的强化微调框架，旨在提升视觉语言模型(VLM)在视觉推理任务中的泛化能力。该框架包含两个阶段：首先使用精选的Chain-of-Thought (CoT)数据进行监督微调(SFT)，激活VLM的推理潜力；然后，通过基于Group Relative Policy Optimization (GRPO)的强化学习生成多个推理-响应对，从而显著增强视觉推理的泛化能力。在包含视觉计数、结构感知和空间变换的综合数据集上的实验结果表明，Reason-RFT在性能、泛化性和数据效率方面均优于现有方法，并在多个任务上取得了state-of-the-art的结果。\n",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "35 pages, 22 figures",
      "pdf_url": "http://arxiv.org/pdf/2503.20752v2",
      "published_date": "2025-03-26 17:38:06 UTC",
      "updated_date": "2025-03-27 03:13:00 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-29T01:03:20.717762"
    },
    {
      "arxiv_id": "2503.20750v1",
      "title": "Optimal Scaling Laws for Efficiency Gains in a Theoretical Transformer-Augmented Sectional MoE Framework",
      "title_zh": "理论Transformer增强型分段MoE框架中效率提升的最优缩放定律\n",
      "authors": [
        "Soham Sane"
      ],
      "abstract": "This paper introduces a theoretical framework for a Transformer-augmented,\nsectional Mixture-of-Experts (MoE) architecture that aims to enhance\ncomputational efficiency while preserving model scalability. Unlike\nconventional MoE models, which route entire token embeddings to selected\nexperts, our approach portions the embedding dimension itself -- assigning\nsegments of each token's representation to dedicated experts. To combat losses\nin token representation, we utilize a pre-expert transformer layer to recompute\nattention across tokens and reduce the sequence length dimensionality. We\nextend our theory by deriving optimal scaling laws that a non-linear\nrelationship between the number of experts and factors such as model\ndimensionality, sequence length, and system overhead. These formulations yield\nclosed-form and numerically-solvable expressions for identifying the optimal\nexpert count under given architectural and hardware constraints. As a result,\nour framework not only provides theoretical bounds for computing efficiency\nwith varying frameworks but also guides practical design choices for scaling\nlarge models effectively. While empirical validation is pending, we present a\ncomprehensive experimental road map to evaluate the framework's efficiency,\nscalability, and practicality in future work.",
      "tldr_zh": "本文提出了一个Transformer增强的sectional MoE（Mixture-of-Experts）架构的理论框架，旨在提高计算效率并保持模型的可扩展性。与传统的MoE模型将整个token embedding路由到选定的专家不同，该方法将embedding维度进行分割，并将每个token表示的片段分配给专门的专家。为了弥补token表示的损失，使用pre-expert Transformer层来重新计算token之间的注意力并减少序列长度维度。该研究推导出了最优的scaling laws，揭示了专家数量与模型维度、序列长度和系统开销等因素之间的非线性关系。这些公式为在给定的架构和硬件约束下确定最佳专家数量提供了闭式解和数值解。该框架不仅为不同框架下的计算效率提供了理论界限，还为有效扩展大型模型的实际设计选择提供了指导。虽然经验验证尚未进行，但论文提出了一个全面的实验路线图，以在未来的工作中评估框架的效率、可扩展性和实用性。\n",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.20750v1",
      "published_date": "2025-03-26 17:33:38 UTC",
      "updated_date": "2025-03-26 17:33:38 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-29T01:03:32.998705"
    },
    {
      "arxiv_id": "2503.20744v1",
      "title": "High Quality Diffusion Distillation on a Single GPU with Relative and Absolute Position Matching",
      "title_zh": "基于相对和绝对位置匹配的单 GPU 高质量扩散蒸馏\n",
      "authors": [
        "Guoqiang Zhang",
        "Kenta Niwa",
        "J. P. Lewis",
        "Cedric Mesnage",
        "W. Bastiaan Kleijn"
      ],
      "abstract": "We introduce relative and absolute position matching (RAPM), a diffusion\ndistillation method resulting in high quality generation that can be trained\nefficiently on a single GPU. Recent diffusion distillation research has\nachieved excellent results for high-resolution text-to-image generation with\nmethods such as phased consistency models (PCM) and improved distribution\nmatching distillation (DMD2). However, these methods generally require many\nGPUs (e.g.~8-64) and significant batchsizes (e.g.~128-2048) during training,\nresulting in memory and compute requirements that are beyond the resources of\nsome researchers. RAPM provides effective single-GPU diffusion distillation\ntraining with a batchsize of 1. The new method attempts to mimic the sampling\ntrajectories of the teacher model by matching the relative and absolute\npositions. The design of relative positions is inspired by PCM. Two\ndiscriminators are introduced accordingly in RAPM, one for matching relative\npositions and the other for absolute positions. Experimental results on\nStableDiffusion (SD) V1.5 and SDXL indicate that RAPM with 4 timesteps produces\ncomparable FID scores as the best method with 1 timestep under very limited\ncomputational resources.",
      "tldr_zh": "该论文提出了一种名为相对和绝对位置匹配(RAPM)的扩散蒸馏方法，旨在以单GPU高效训练高质量图像生成模型。RAPM通过匹配相对和绝对位置来模仿教师模型的采样轨迹，从而实现知识蒸馏。该方法引入了两个判别器，分别用于匹配相对位置和绝对位置。实验结果表明，在StableDiffusion (SD) V1.5 和 SDXL上，RAPM仅用4个时间步就能在极低的计算资源下产生与最佳单步方法相当的FID分数。这使得研究人员可以在资源有限的情况下进行有效的单GPU扩散蒸馏训练。\n",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.20744v1",
      "published_date": "2025-03-26 17:29:08 UTC",
      "updated_date": "2025-03-26 17:29:08 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-29T01:03:44.643588"
    },
    {
      "arxiv_id": "2503.20742v1",
      "title": "Quantum Neural Network Restatement of Markov Jump Process",
      "title_zh": "马尔可夫跳跃过程的量子神经网络重述\n",
      "authors": [
        "Z. Zarezadeh",
        "N. Zarezadeh"
      ],
      "abstract": "Despite the many challenges in exploratory data analysis, artificial neural\nnetworks have motivated strong interests in scientists and researchers both in\ntheoretical as well as practical applications. Among sources of such popularity\nof artificial neural networks the ability of modeling non-linear dynamical\nsystems, generalization, and adaptation possibilities should be mentioned.\nDespite this, there is still significant debate about the role of various\nunderlying stochastic processes in stabilizing a unique structure for data\nlearning and prediction. One of such obstacles to the theoretical and numerical\nstudy of machine intelligent systems is the curse of dimensionality and the\nsampling from high-dimensional probability distributions. In general, this\ncurse prevents efficient description of states, providing a significant\ncomplexity barrier for the system to be efficiently described and studied. In\nthis strand of research, direct treatment and description of such abstract\nnotions of learning theory in terms of quantum information be one of the most\nfavorable candidates. Hence, the subject matter of these articles is devoted to\nproblems of design, adaptation and the formulations of computationally hard\nproblems in terms of quantum mechanical systems. In order to characterize the\nmicroscopic description of such dynamics in the language of inferential\nstatistics, covariance matrix estimation of d-dimensional Gaussian densities\nand Bayesian interpretation of eigenvalue problem for dynamical systems is\nassessed.",
      "tldr_zh": "本文探讨了利用量子神经网络(Quantum Neural Network)重新表述马尔可夫跳跃过程(Markov Jump Process)的方法。尽管人工神经网络在建模非线性动力系统方面具有优势，但高维数据带来的维度灾难仍然是理论和数值研究的障碍。该研究尝试将学习理论中的抽象概念用量子信息进行描述，旨在解决设计、适应和计算难题。文章重点关注动力系统的微观描述，并评估了d维高斯密度的协方差矩阵估计以及动力系统特征值问题的贝叶斯解释。\n",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.NA",
        "math.NA"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.20742v1",
      "published_date": "2025-03-26 17:25:11 UTC",
      "updated_date": "2025-03-26 17:25:11 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-29T01:03:56.649808"
    },
    {
      "arxiv_id": "2503.20739v1",
      "title": "Emotion Detection and Music Recommendation System",
      "title_zh": "情感检测与音乐推荐系统\n",
      "authors": [
        "Swetha Kambham",
        "Hubert Jhonson",
        "Sai Prathap Reddy Kambham"
      ],
      "abstract": "As artificial intelligence becomes more and more ingrained in daily life, we\npresent a novel system that uses deep learning for music recommendation and\nemotion-based detection. Through the use of facial recognition and the DeepFace\nframework, our method analyses human emotions in real-time and then plays music\nthat reflects the mood it has discovered. The system uses a webcam to take\npictures, analyses the most common facial expression, and then pulls a playlist\nfrom local storage that corresponds to the mood it has detected. An engaging\nand customised experience is ensured by allowing users to manually change the\nsong selection via a dropdown menu or navigation buttons. By continuously\nlooping over the playlist, the technology guarantees continuity. The objective\nof our system is to improve emotional well-being through music therapy by\noffering a responsive and automated music-selection experience.",
      "tldr_zh": "该论文提出了一种基于深度学习的情感检测和音乐推荐系统。该系统利用DeepFace框架进行面部识别，实时分析用户情绪，并推荐与之匹配的音乐。系统通过摄像头捕捉图像，分析主要面部表情，并从本地存储中提取与该情绪对应的播放列表。用户可以通过下拉菜单或导航按钮手动更改歌曲选择，从而获得定制化的体验。该系统旨在通过提供响应式和自动化的音乐选择体验，从而改善用户的情感健康。\n",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.20739v1",
      "published_date": "2025-03-26 17:22:06 UTC",
      "updated_date": "2025-03-26 17:22:06 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-29T01:04:08.521427"
    },
    {
      "arxiv_id": "2503.20688v1",
      "title": "Graph-Enhanced Model-Free Reinforcement Learning Agents for Efficient Power Grid Topological Control",
      "title_zh": "基于图增强的免模型强化学习智能体，用于高效的电网拓扑控制\n",
      "authors": [
        "Eloy Anguiano Batanero",
        "Ángela Fernández",
        "Álvaro Barbero"
      ],
      "abstract": "The increasing complexity of power grid management, driven by the emergence\nof prosumers and the demand for cleaner energy solutions, has needed innovative\napproaches to ensure stability and efficiency. This paper presents a novel\napproach within the model-free framework of reinforcement learning, aimed at\noptimizing power network operations without prior expert knowledge. We\nintroduce a masked topological action space, enabling agents to explore diverse\nstrategies for cost reduction while maintaining reliable service using the\nstate logic as a guide for choosing proper actions. Through extensive\nexperimentation across 20 different scenarios in a simulated 5-substation\nenvironment, we demonstrate that our approach achieves a consistent reduction\nin power losses, while ensuring grid stability against potential blackouts. The\nresults underscore the effectiveness of combining dynamic observation\nformalization with opponent-based training, showing a viable way for autonomous\nmanagement solutions in modern energy systems or even for building a\nfoundational model for this field.",
      "tldr_zh": "本文提出了一种基于图增强的无模型强化学习方法，用于高效的电网拓扑控制。该方法旨在优化电力网络运营，无需先验专家知识。通过引入掩蔽的拓扑动作空间，智能体能够探索多样化的策略以降低成本，同时利用状态逻辑作为指导来选择合适的动作，从而维持可靠的服务。在模拟的5个变电站环境中进行的20种不同场景的实验表明，该方法能够持续降低功率损耗，同时确保电网稳定性，防止潜在的停电。研究结果强调了将动态观察形式化与基于对手的训练相结合的有效性，展示了现代能源系统中自主管理解决方案的可行途径，甚至可以为该领域构建基础模型。\n",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.20688v1",
      "published_date": "2025-03-26 16:20:30 UTC",
      "updated_date": "2025-03-26 16:20:30 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-29T01:04:20.849761"
    },
    {
      "arxiv_id": "2503.20685v2",
      "title": "Flip Learning: Weakly Supervised Erase to Segment Nodules in Breast Ultrasound",
      "title_zh": "翻转学习：弱监督擦除以分割乳腺超声中的结节\n",
      "authors": [
        "Yuhao Huang",
        "Ao Chang",
        "Haoran Dou",
        "Xing Tao",
        "Xinrui Zhou",
        "Yan Cao",
        "Ruobing Huang",
        "Alejandro F Frangi",
        "Lingyun Bao",
        "Xin Yang",
        "Dong Ni"
      ],
      "abstract": "Accurate segmentation of nodules in both 2D breast ultrasound (BUS) and 3D\nautomated breast ultrasound (ABUS) is crucial for clinical diagnosis and\ntreatment planning. Therefore, developing an automated system for nodule\nsegmentation can enhance user independence and expedite clinical analysis.\nUnlike fully-supervised learning, weakly-supervised segmentation (WSS) can\nstreamline the laborious and intricate annotation process. However, current WSS\nmethods face challenges in achieving precise nodule segmentation, as many of\nthem depend on inaccurate activation maps or inefficient pseudo-mask generation\nalgorithms. In this study, we introduce a novel multi-agent reinforcement\nlearning-based WSS framework called Flip Learning, which relies solely on 2D/3D\nboxes for accurate segmentation. Specifically, multiple agents are employed to\nerase the target from the box to facilitate classification tag flipping, with\nthe erased region serving as the predicted segmentation mask. The key\ncontributions of this research are as follows: (1) Adoption of a\nsuperpixel/supervoxel-based approach to encode the standardized environment,\ncapturing boundary priors and expediting the learning process. (2) Introduction\nof three meticulously designed rewards, comprising a classification score\nreward and two intensity distribution rewards, to steer the agents' erasing\nprocess precisely, thereby avoiding both under- and over-segmentation. (3)\nImplementation of a progressive curriculum learning strategy to enable agents\nto interact with the environment in a progressively challenging manner, thereby\nenhancing learning efficiency. Extensively validated on the large in-house BUS\nand ABUS datasets, our Flip Learning method outperforms state-of-the-art WSS\nmethods and foundation models, and achieves comparable performance as\nfully-supervised learning algorithms.",
      "tldr_zh": "该研究提出了一种名为Flip Learning的弱监督分割(WSS)框架，用于精确分割2D/3D乳腺超声(BUS/ABUS)中的结节，仅需2D/3D边界框标注。该框架采用多智能体强化学习，通过多个智能体从边界框中擦除目标，实现分类标签翻转，擦除区域即为预测的分割掩码。主要贡献包括：(1)采用基于超像素/超体素的方法编码标准化环境，捕捉边界先验并加速学习；(2)引入包含分类分数奖励和两个强度分布奖励的三种精心设计的奖励，精确引导智能体的擦除过程，避免欠分割和过分割；(3)实施渐进式课程学习策略，使智能体以渐进方式与环境交互，提高学习效率。在大型内部BUS和ABUS数据集上的广泛验证表明，Flip Learning优于现有WSS方法和基础模型，并达到与全监督学习算法相当的性能。\n",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "Accepted by Medical Image Analysis. 24 pages, 13 figures, 20 tabels",
      "pdf_url": "http://arxiv.org/pdf/2503.20685v2",
      "published_date": "2025-03-26 16:20:02 UTC",
      "updated_date": "2025-03-27 06:16:16 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-29T01:04:33.106684"
    },
    {
      "arxiv_id": "2503.20676v1",
      "title": "Inductive Link Prediction on N-ary Relational Facts via Semantic Hypergraph Reasoning",
      "title_zh": "基于语义超图推理的 N 元关系事实归纳链接预测\n",
      "authors": [
        "Gongzhu Yin",
        "Hongli Zhang",
        "Yuchen Yang",
        "Yi Luo"
      ],
      "abstract": "N-ary relational facts represent semantic correlations among more than two\nentities. While recent studies have developed link prediction (LP) methods to\ninfer missing relations for knowledge graphs (KGs) containing n-ary relational\nfacts, they are generally limited to transductive settings. Fully inductive\nsettings, where predictions are made on previously unseen entities, remain a\nsignificant challenge. As existing methods are mainly entity embedding-based,\nthey struggle to capture entity-independent logical rules. To fill in this gap,\nwe propose an n-ary subgraph reasoning framework for fully inductive link\nprediction (ILP) on n-ary relational facts. This framework reasons over local\nsubgraphs and has a strong inductive inference ability to capture n-ary\npatterns. Specifically, we introduce a novel graph structure, the n-ary\nsemantic hypergraph, to facilitate subgraph extraction. Moreover, we develop a\nsubgraph aggregating network, NS-HART, to effectively mine complex semantic\ncorrelations within subgraphs. Theoretically, we provide a thorough analysis\nfrom the score function optimization perspective to shed light on NS-HART's\neffectiveness for n-ary ILP tasks. Empirically, we conduct extensive\nexperiments on a series of inductive benchmarks, including transfer reasoning\n(with and without entity features) and pairwise subgraph reasoning. The results\nhighlight the superiority of the n-ary subgraph reasoning framework and the\nexceptional inductive ability of NS-HART. The source code of this paper has\nbeen made publicly available at\nhttps://github.com/yin-gz/Nary-Inductive-SubGraph.",
      "tldr_zh": "该论文提出了一种基于语义超图推理的N元关系事实归纳链接预测(Inductive Link Prediction, ILP)框架，旨在解决现有方法在处理未见实体时，难以捕捉实体独立逻辑规则的问题。该框架通过构建n元语义超图提取子图，并设计子图聚合网络NS-HART，有效挖掘子图中复杂的语义关联。理论分析表明NS-HART在N元ILP任务中具有有效性。实验结果表明，该框架和NS-HART在包括迁移推理和成对子图推理在内的多个归纳基准测试中表现出色，验证了其优越的归纳能力。\n",
      "categories": [
        "cs.AI",
        "cs.LG",
        "I.2.4"
      ],
      "primary_category": "cs.AI",
      "comment": "To be published in Proceedings of the 31st ACM SIGKDD Conference on\n  Knowledge Discovery and Data Mining V.1 (KDD'25)",
      "pdf_url": "http://arxiv.org/pdf/2503.20676v1",
      "published_date": "2025-03-26 16:09:54 UTC",
      "updated_date": "2025-03-26 16:09:54 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-29T01:04:44.728619"
    },
    {
      "arxiv_id": "2503.20848v1",
      "title": "The Backfiring Effect of Weak AI Safety Regulation",
      "title_zh": "弱人工智能安全监管的反作用效应\n",
      "authors": [
        "Benjamin Laufer",
        "Jon Kleinberg",
        "Hoda Heidari"
      ],
      "abstract": "Recent policy proposals aim to improve the safety of general-purpose AI, but\nthere is little understanding of the efficacy of different regulatory\napproaches to AI safety. We present a strategic model that explores the\ninteractions between the regulator, the general-purpose AI technology creators,\nand domain specialists--those who adapt the AI for specific applications. Our\nanalysis examines how different regulatory measures, targeting different parts\nof the development chain, affect the outcome of the development process. In\nparticular, we assume AI technology is described by two key attributes: safety\nand performance. The regulator first sets a minimum safety standard that\napplies to one or both players, with strict penalties for non-compliance. The\ngeneral-purpose creator then develops the technology, establishing its initial\nsafety and performance levels. Next, domain specialists refine the AI for their\nspecific use cases, and the resulting revenue is distributed between the\nspecialist and generalist through an ex-ante bargaining process. Our analysis\nof this game reveals two key insights: First, weak safety regulation imposed\nonly on the domain specialists can backfire. While it might seem logical to\nregulate use cases (as opposed to the general-purpose technology), our analysis\nshows that weak regulations targeting domain specialists alone can\nunintentionally reduce safety. This effect persists across a wide range of\nsettings. Second, in sharp contrast to the previous finding, we observe that\nstronger, well-placed regulation can in fact benefit all players subjected to\nit. When regulators impose appropriate safety standards on both AI creators and\ndomain specialists, the regulation functions as a commitment mechanism, leading\nto safety and performance gains, surpassing what is achieved under no\nregulation or regulating one player only.",
      "tldr_zh": "该论文构建了一个博弈论模型，分析了监管机构、通用人工智能（General-Purpose AI）开发者和领域专家在AI安全监管下的互动。模型考察了针对不同开发环节的监管措施如何影响开发结果，尤其关注AI的安全性和性能。研究发现，仅对领域专家施加较弱的安全监管可能会适得其反，反而降低安全性。相反，对AI开发者和领域专家同时施加更强、更合理的监管，可以作为一种承诺机制，提升整体的安全性和性能，实现多方共赢。\n",
      "categories": [
        "cs.GT",
        "cs.AI",
        "cs.CY",
        "econ.TH"
      ],
      "primary_category": "cs.GT",
      "comment": "28 pages, 8 figures",
      "pdf_url": "http://arxiv.org/pdf/2503.20848v1",
      "published_date": "2025-03-26 16:08:22 UTC",
      "updated_date": "2025-03-26 16:08:22 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-29T01:04:56.747790"
    },
    {
      "arxiv_id": "2503.20658v1",
      "title": "Probabilistic Forecasting for Network Resource Analysis in Integrated Terrestrial and Non-Terrestrial Networks",
      "title_zh": "用于综合地面和非地面网络中网络资源分析的概率预测\n",
      "authors": [
        "Cristian J. Vaca-Rubio",
        "Vaishnavi Kasuluru",
        "Engin Zeydan",
        "Luis Blanco",
        "Roberto Pereira",
        "Marius Caus",
        "Kapal Dev"
      ],
      "abstract": "Efficient resource management is critical for Non-Terrestrial Networks (NTNs)\nto provide consistent, high-quality service in remote and under-served regions.\nWhile traditional single-point prediction methods, such as Long-Short Term\nMemory (LSTM), have been used in terrestrial networks, they often fall short in\nNTNs due to the complexity of satellite dynamics, signal latency and coverage\nvariability. Probabilistic forecasting, which quantifies the uncertainties of\nthe predictions, is a robust alternative. In this paper, we evaluate the\napplication of probabilistic forecasting techniques, in particular SFF, to NTN\nresource allocation scenarios. Our results show their effectiveness in\npredicting bandwidth and capacity requirements in different NTN segments of\nprobabilistic forecasting compared to single-point prediction techniques such\nas LSTM. The results show the potential of black probabilistic forecasting\nmodels to provide accurate and reliable predictions and to quantify their\nuncertainty, making them indispensable for optimizing NTN resource allocation.\nAt the end of the paper, we also present application scenarios and a\nstandardization roadmap for the use of probabilistic forecasting in integrated\nTerrestrial Network (TN)-NTN environments.",
      "tldr_zh": "本文探讨了概率预测技术在集成地面网络(TN)和非地面网络(NTN)中进行网络资源分析的应用。针对传统单点预测方法（如LSTM）在NTN中因卫星动态、信号延迟和覆盖范围变化而表现不足的问题，提出了利用概率预测量化预测不确定性的方法。通过对NTN不同部分带宽和容量需求的预测评估，验证了概率预测（特别是SFF）的有效性。研究结果表明，概率预测模型能够提供准确可靠的预测并量化其不确定性，对于优化NTN资源分配至关重要，并提出了在TN-NTN集成环境中使用概率预测的应用场景和标准化路线图。\n",
      "categories": [
        "eess.SP",
        "cs.AI",
        "cs.LG",
        "cs.NI"
      ],
      "primary_category": "eess.SP",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.20658v1",
      "published_date": "2025-03-26 15:54:46 UTC",
      "updated_date": "2025-03-26 15:54:46 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-29T01:05:08.811228"
    },
    {
      "arxiv_id": "2503.20654v1",
      "title": "AccidentSim: Generating Physically Realistic Vehicle Collision Videos from Real-World Accident Reports",
      "title_zh": "AccidentSim：从真实世界事故报告生成物理上逼真的车辆碰撞视频\n",
      "authors": [
        "Xiangwen Zhang",
        "Qian Zhang",
        "Longfei Han",
        "Qiang Qu",
        "Xiaoming Chen"
      ],
      "abstract": "Collecting real-world vehicle accident videos for autonomous driving research\nis challenging due to their rarity and complexity. While existing driving video\ngeneration methods may produce visually realistic videos, they often fail to\ndeliver physically realistic simulations because they lack the capability to\ngenerate accurate post-collision trajectories. In this paper, we introduce\nAccidentSim, a novel framework that generates physically realistic vehicle\ncollision videos by extracting and utilizing the physical clues and contextual\ninformation available in real-world vehicle accident reports. Specifically,\nAccidentSim leverages a reliable physical simulator to replicate post-collision\nvehicle trajectories from the physical and contextual information in the\naccident reports and to build a vehicle collision trajectory dataset. This\ndataset is then used to fine-tune a language model, enabling it to respond to\nuser prompts and predict physically consistent post-collision trajectories\nacross various driving scenarios based on user descriptions. Finally, we employ\nNeural Radiance Fields (NeRF) to render high-quality backgrounds, merging them\nwith the foreground vehicles that exhibit physically realistic trajectories to\ngenerate vehicle collision videos. Experimental results demonstrate that the\nvideos produced by AccidentSim excel in both visual and physical authenticity.",
      "tldr_zh": "由于真实车辆事故视频难以收集，现有方法难以生成物理上逼真的碰撞后轨迹，本文提出了AccidentSim框架，旨在从真实事故报告中生成物理上逼真的车辆碰撞视频。AccidentSim利用物理模拟器，从事故报告中提取物理和上下文信息，复制碰撞后的车辆轨迹，构建车辆碰撞轨迹数据集。然后，该数据集被用于微调语言模型，使其能够根据用户描述预测物理上一致的碰撞后轨迹。最后，使用神经辐射场(NeRF)渲染高质量背景，并将其与具有物理真实轨迹的前景车辆合并，以生成车辆碰撞视频。实验结果表明，AccidentSim生成的视频在视觉和物理真实性方面表现出色。\n",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.20654v1",
      "published_date": "2025-03-26 15:50:42 UTC",
      "updated_date": "2025-03-26 15:50:42 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-29T01:05:20.985877"
    },
    {
      "arxiv_id": "2503.20648v1",
      "title": "TN-Eval: Rubric and Evaluation Protocols for Measuring the Quality of Behavioral Therapy Notes",
      "title_zh": "TN-Eval：用于衡量行为治疗记录质量的评分标准和评估协议\n",
      "authors": [
        "Raj Sanjay Shah",
        "Lei Xu",
        "Qianchu Liu",
        "Jon Burnsky",
        "Drew Bertagnolli",
        "Chaitanya Shivade"
      ],
      "abstract": "Behavioral therapy notes are important for both legal compliance and patient\ncare. Unlike progress notes in physical health, quality standards for\nbehavioral therapy notes remain underdeveloped. To address this gap, we\ncollaborated with licensed therapists to design a comprehensive rubric for\nevaluating therapy notes across key dimensions: completeness, conciseness, and\nfaithfulness. Further, we extend a public dataset of behavioral health\nconversations with therapist-written notes and LLM-generated notes, and apply\nour evaluation framework to measure their quality. We find that: (1) A\nrubric-based manual evaluation protocol offers more reliable and interpretable\nresults than traditional Likert-scale annotations. (2) LLMs can mimic human\nevaluators in assessing completeness and conciseness but struggle with\nfaithfulness. (3) Therapist-written notes often lack completeness and\nconciseness, while LLM-generated notes contain hallucination. Surprisingly, in\na blind test, therapists prefer and judge LLM-generated notes to be superior to\ntherapist-written notes.",
      "tldr_zh": "该研究针对行为治疗记录质量评估标准不足的问题，与专业治疗师合作设计了一套综合评估标准（TN-Eval），涵盖完整性、简洁性和忠实性三个关键维度。研究者扩展了一个包含治疗师撰写和LLM生成的笔记的公共数据集，并利用TN-Eval评估了这些笔记的质量。结果表明，基于评估标准的评估协议比传统的李克特量表更可靠，LLM在评估完整性和简洁性方面能模仿人类，但在忠实性方面存在困难。尽管如此，在盲测中，治疗师更喜欢并认为LLM生成的笔记优于治疗师自己撰写的笔记。\n",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.20648v1",
      "published_date": "2025-03-26 15:40:40 UTC",
      "updated_date": "2025-03-26 15:40:40 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-29T01:05:32.759109"
    },
    {
      "arxiv_id": "2503.20634v1",
      "title": "Procedural Knowledge Ontology (PKO)",
      "title_zh": "程序知识本体 (PKO)\n",
      "authors": [
        "Valentina Anita Carriero",
        "Mario Scrocca",
        "Ilaria Baroni",
        "Antonia Azzini",
        "Irene Celino"
      ],
      "abstract": "Processes, workflows and guidelines are core to ensure the correct\nfunctioning of industrial companies: for the successful operations of factory\nlines, machinery or services, often industry operators rely on their past\nexperience and know-how. The effect is that this Procedural Knowledge (PK)\nremains tacit and, as such, difficult to exploit efficiently and effectively.\nThis paper presents PKO, the Procedural Knowledge Ontology, which enables the\nexplicit modeling of procedures and their executions, by reusing and extending\nexisting ontologies. PKO is built on requirements collected from three\nheterogeneous industrial use cases and can be exploited by any AI and\ndata-driven tools that rely on a shared and interoperable representation to\nsupport the governance of PK throughout its life cycle. We describe its\nstructure and design methodology, and outline its relevance, quality, and\nimpact by discussing applications leveraging PKO for PK elicitation and\nexploitation.",
      "tldr_zh": "本文介绍了程序知识本体(Procedural Knowledge Ontology, PKO)，旨在显式地建模程序及其执行过程，解决工业企业中程序知识(Procedural Knowledge, PK)隐性化的问题。PKO通过重用和扩展现有本体，实现了对程序知识的共享和互操作表示。该本体基于三个异构工业用例的需求构建，能够被各种依赖共享和互操作表示的AI和数据驱动工具利用，以支持程序知识在其整个生命周期中的管理。文章还讨论了利用PKO进行程序知识获取和利用的应用，突出了其相关性、质量和影响。\n",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.20634v1",
      "published_date": "2025-03-26 15:28:30 UTC",
      "updated_date": "2025-03-26 15:28:30 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-29T01:05:44.762104"
    },
    {
      "arxiv_id": "2503.20630v1",
      "title": "$β$-GNN: A Robust Ensemble Approach Against Graph Structure Perturbation",
      "title_zh": "$β$-GNN：一种针对图结构扰动的鲁棒集成方法\n",
      "authors": [
        "Haci Ismail Aslan",
        "Philipp Wiesner",
        "Ping Xiong",
        "Odej Kao"
      ],
      "abstract": "Graph Neural Networks (GNNs) are playing an increasingly important role in\nthe efficient operation and security of computing systems, with applications in\nworkload scheduling, anomaly detection, and resource management. However, their\nvulnerability to network perturbations poses a significant challenge. We\npropose $\\beta$-GNN, a model enhancing GNN robustness without sacrificing clean\ndata performance. $\\beta$-GNN uses a weighted ensemble, combining any GNN with\na multi-layer perceptron. A learned dynamic weight, $\\beta$, modulates the\nGNN's contribution. This $\\beta$ not only weights GNN influence but also\nindicates data perturbation levels, enabling proactive mitigation. Experimental\nresults on diverse datasets show $\\beta$-GNN's superior adversarial accuracy\nand attack severity quantification. Crucially, $\\beta$-GNN avoids perturbation\nassumptions, preserving clean data structure and performance.",
      "tldr_zh": "该论文提出了$\\beta$-GNN，一种增强图神经网络(GNN)鲁棒性的集成方法，旨在解决GNN对图结构扰动的脆弱性问题。$\\beta$-GNN通过学习动态权重$\\beta$，将任意GNN与多层感知机(MLP)进行加权集成，从而在不牺牲原始数据性能的前提下，提升模型对扰动的抵抗能力。学习到的权重$\\beta$不仅调节GNN的贡献，还能指示数据扰动程度，从而实现主动缓解。实验结果表明，在各种数据集上，$\\beta$-GNN具有优越的对抗准确性和攻击严重程度量化能力，并且避免了对扰动的假设，从而保持了原始数据的结构和性能。\n",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "This is the author's version of the paper accepted at EuroMLSys 2025",
      "pdf_url": "http://arxiv.org/pdf/2503.20630v1",
      "published_date": "2025-03-26 15:24:07 UTC",
      "updated_date": "2025-03-26 15:24:07 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-29T01:05:56.976784"
    },
    {
      "arxiv_id": "2503.20623v1",
      "title": "Collaborative Storytelling and LLM: A Linguistic Analysis of Automatically-Generated Role-Playing Game Sessions",
      "title_zh": "协作叙事与大型语言模型：自动生成角色扮演游戏环节的语言学分析\n",
      "authors": [
        "Alessandro Maisto"
      ],
      "abstract": "Role-playing games (RPG) are games in which players interact with one another\nto create narratives. The role of players in the RPG is largely based on the\ninteraction between players and their characters. This emerging form of shared\nnarrative, primarily oral, is receiving increasing attention. In particular,\nmany authors investigated the use of an LLM as an actor in the game. In this\npaper, we aim to discover to what extent the language of Large Language Models\n(LLMs) exhibit oral or written features when asked to generate an RPG session\nwithout human interference. We will conduct a linguistic analysis of the\nlexical and syntactic features of the generated texts and compare the results\nwith analyses of conversations, transcripts of human RPG sessions, and books.\nWe found that LLMs exhibit a pattern that is distinct from all other text\ncategories, including oral conversations, human RPG sessions and books. Our\nanalysis has shown how training influences the way LLMs express themselves and\nprovides important indications of the narrative capabilities of these tools.",
      "tldr_zh": "本文研究了大型语言模型(LLMs)在自动生成角色扮演游戏(RPG)会话中的语言特征，旨在分析其口语化和书面化程度。研究人员对LLM生成的文本进行了词汇和句法分析，并将其与对话、人类RPG会话记录和书籍进行了比较。结果表明，LLM的语言模式与所有其他文本类别（包括口头对话、人类RPG会话和书籍）都不同。该分析揭示了训练方式如何影响LLM的表达方式，并为理解这些工具的叙事能力提供了重要线索。\n",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "17 pages",
      "pdf_url": "http://arxiv.org/pdf/2503.20623v1",
      "published_date": "2025-03-26 15:10:47 UTC",
      "updated_date": "2025-03-26 15:10:47 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-29T01:06:08.702340"
    },
    {
      "arxiv_id": "2503.20844v1",
      "title": "Robust Deep Reinforcement Learning in Robotics via Adaptive Gradient-Masked Adversarial Attacks",
      "title_zh": "基于自适应梯度掩蔽对抗攻击的机器人鲁棒深度强化学习\n",
      "authors": [
        "Zongyuan Zhang",
        "Tianyang Duan",
        "Zheng Lin",
        "Dong Huang",
        "Zihan Fang",
        "Zekai Sun",
        "Ling Xiong",
        "Hongbin Liang",
        "Heming Cui",
        "Yong Cui",
        "Yue Gao"
      ],
      "abstract": "Deep reinforcement learning (DRL) has emerged as a promising approach for\nrobotic control, but its realworld deployment remains challenging due to its\nvulnerability to environmental perturbations. Existing white-box adversarial\nattack methods, adapted from supervised learning, fail to effectively target\nDRL agents as they overlook temporal dynamics and indiscriminately perturb all\nstate dimensions, limiting their impact on long-term rewards. To address these\nchallenges, we propose the Adaptive Gradient-Masked Reinforcement (AGMR)\nAttack, a white-box attack method that combines DRL with a gradient-based soft\nmasking mechanism to dynamically identify critical state dimensions and\noptimize adversarial policies. AGMR selectively allocates perturbations to the\nmost impactful state features and incorporates a dynamic adjustment mechanism\nto balance exploration and exploitation during training. Extensive experiments\ndemonstrate that AGMR outperforms state-of-the-art adversarial attack methods\nin degrading the performance of the victim agent and enhances the victim\nagent's robustness through adversarial defense mechanisms.",
      "tldr_zh": "这篇论文提出了一种自适应梯度掩蔽强化学习对抗攻击 (AGMR) 方法，旨在提升深度强化学习 (DRL) 在机器人控制中的鲁棒性。AGMR 是一种白盒攻击方法，它结合了 DRL 和基于梯度的软掩蔽机制，能够动态识别关键状态维度并优化对抗策略。通过选择性地扰动最具影响力的状态特征，并结合动态调整机制平衡训练期间的探索和利用，AGMR 能够有效降低受害智能体的性能。实验结果表明，AGMR 在攻击效果上优于现有方法，并且能够通过对抗防御机制增强受害智能体的鲁棒性。\n",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.NI",
        "cs.RO"
      ],
      "primary_category": "cs.LG",
      "comment": "9 pages, 6 figures",
      "pdf_url": "http://arxiv.org/pdf/2503.20844v1",
      "published_date": "2025-03-26 15:08:58 UTC",
      "updated_date": "2025-03-26 15:08:58 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-29T01:06:20.789464"
    },
    {
      "arxiv_id": "2503.20613v1",
      "title": "State-Aware Perturbation Optimization for Robust Deep Reinforcement Learning",
      "title_zh": "面向鲁棒深度强化学习的状态感知扰动优化",
      "authors": [
        "Zongyuan Zhang",
        "Tianyang Duan",
        "Zheng Lin",
        "Dong Huang",
        "Zihan Fang",
        "Zekai Sun",
        "Ling Xiong",
        "Hongbin Liang",
        "Heming Cui",
        "Yong Cui"
      ],
      "abstract": "Recently, deep reinforcement learning (DRL) has emerged as a promising\napproach for robotic control. However, the deployment of DRL in real-world\nrobots is hindered by its sensitivity to environmental perturbations. While\nexisting whitebox adversarial attacks rely on local gradient information and\napply uniform perturbations across all states to evaluate DRL robustness, they\nfail to account for temporal dynamics and state-specific vulnerabilities. To\ncombat the above challenge, we first conduct a theoretical analysis of\nwhite-box attacks in DRL by establishing the adversarial victim-dynamics Markov\ndecision process (AVD-MDP), to derive the necessary and sufficient conditions\nfor a successful attack. Based on this, we propose a selective state-aware\nreinforcement adversarial attack method, named STAR, to optimize perturbation\nstealthiness and state visitation dispersion. STAR first employs a soft\nmask-based state-targeting mechanism to minimize redundant perturbations,\nenhancing stealthiness and attack effectiveness. Then, it incorporates an\ninformation-theoretic optimization objective to maximize mutual information\nbetween perturbations, environmental states, and victim actions, ensuring a\ndispersed state-visitation distribution that steers the victim agent into\nvulnerable states for maximum return reduction. Extensive experiments\ndemonstrate that STAR outperforms state-of-the-art benchmarks.",
      "tldr_zh": "该论文针对深度强化学习(DRL)在机器人控制中对环境扰动敏感的问题，提出了状态感知扰动优化方法STAR。首先，通过对抗性受害者动态马尔可夫决策过程(AVD-MDP)对DRL中的白盒攻击进行理论分析，推导出成功攻击的充要条件。然后，STAR采用基于软掩码的状态目标机制，最小化冗余扰动，提高隐蔽性和攻击有效性；并结合信息论优化目标，最大化扰动、环境状态和受害者动作之间的互信息，确保分散的状态访问分布，引导受害者进入脆弱状态，从而最大程度地降低回报。实验结果表明，STAR优于现有技术。\n",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.NI",
        "cs.SY",
        "eess.SY"
      ],
      "primary_category": "cs.LG",
      "comment": "15 pages, 11 figures",
      "pdf_url": "http://arxiv.org/pdf/2503.20613v1",
      "published_date": "2025-03-26 15:00:07 UTC",
      "updated_date": "2025-03-26 15:00:07 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-29T01:06:32.837257"
    },
    {
      "arxiv_id": "2503.20607v1",
      "title": "A decision-theoretic approach to dealing with uncertainty in quantum mechanics",
      "title_zh": "一种处理量子力学中不确定性的决策理论方法\n",
      "authors": [
        "Keano De Vos",
        "Gert de Cooman",
        "Alexander Erreygers",
        "Jasper De Bock"
      ],
      "abstract": "We provide a decision-theoretic framework for dealing with uncertainty in\nquantum mechanics. This uncertainty is two-fold: on the one hand there may be\nuncertainty about the state the quantum system is in, and on the other hand, as\nis essential to quantum mechanical uncertainty, even if the quantum state is\nknown, measurements may still produce an uncertain outcome. In our framework,\nmeasurements therefore play the role of acts with an uncertain outcome and our\nsimple decision-theoretic postulates ensure that Born's rule is encapsulated in\nthe utility functions associated with such acts. This approach allows us to\nuncouple (precise) probability theory from quantum mechanics, in the sense that\nit leaves room for a more general, so-called imprecise probabilities approach.\nWe discuss the mathematical implications of our findings, which allow us to\ngive a decision-theoretic foundation to recent seminal work by Benavoli,\nFacchini and Zaffalon, and we compare our approach to earlier and different\napproaches by Deutsch and Wallace.",
      "tldr_zh": "本文提出了一个决策理论框架，用于处理量子力学中的不确定性。这种不确定性包括量子系统状态的不确定性，以及即使量子状态已知，测量结果仍然可能是不确定的。在该框架中，测量扮演了具有不确定结果的行为的角色，简单的决策理论假设确保了Born规则被包含在与这些行为相关的效用函数中。该方法将精确概率论与量子力学解耦，为更一般的“非精确概率”方法留下了空间。研究结果为Benavoli, Facchini和Zaffalon的开创性工作提供了决策理论基础，并与Deutsch和Wallace的早期方法进行了比较。\n",
      "categories": [
        "quant-ph",
        "cs.AI",
        "math.PR"
      ],
      "primary_category": "quant-ph",
      "comment": "52 pages",
      "pdf_url": "http://arxiv.org/pdf/2503.20607v1",
      "published_date": "2025-03-26 14:53:06 UTC",
      "updated_date": "2025-03-26 14:53:06 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-29T01:06:44.801597"
    },
    {
      "arxiv_id": "2503.20527v1",
      "title": "StableToolBench-MirrorAPI: Modeling Tool Environments as Mirrors of 7,000+ Real-World APIs",
      "title_zh": "StableToolBench-MirrorAPI：将工具环境建模为 7000 多个真实世界 API 的镜像",
      "authors": [
        "Zhicheng Guo",
        "Sijie Cheng",
        "Yuchen Niu",
        "Hao Wang",
        "Sicheng Zhou",
        "Wenbing Huang",
        "Yang Liu"
      ],
      "abstract": "The rapid advancement of large language models (LLMs) has spurred significant\ninterest in tool learning, where LLMs are augmented with external tools to\ntackle complex tasks. However, existing tool environments face challenges in\nbalancing stability, scalability, and realness, particularly for benchmarking\npurposes. To address this problem, we propose MirrorAPI, a novel framework that\ntrains specialized LLMs to accurately simulate real API responses, effectively\nacting as \"mirrors\" to tool environments. Using a comprehensive dataset of\nrequest-response pairs from 7,000+ APIs, we employ supervised fine-tuning and\nchain-of-thought reasoning to enhance simulation fidelity. MirrorAPI achieves\nsuperior accuracy and stability compared to state-of-the-art methods, as\ndemonstrated by its performance on the newly constructed MirrorAPI-Bench and\nits integration into StableToolBench.",
      "tldr_zh": "该论文提出了MirrorAPI，一个新颖的框架，旨在通过训练专门的LLM来精确模拟真实API的响应，从而解决现有工具环境在稳定性、可扩展性和真实性方面面临的挑战。MirrorAPI利用包含7000+ API的请求-响应对数据集，采用监督微调和链式思维(Chain-of-Thought)推理来提高模拟的保真度。实验表明，MirrorAPI在新建的MirrorAPI-Bench以及集成到StableToolBench中时，表现出比现有最佳方法更高的准确性和稳定性，为工具学习提供了一个更可靠的benchmark环境。\n",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.20527v1",
      "published_date": "2025-03-26 13:13:03 UTC",
      "updated_date": "2025-03-26 13:13:03 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-29T01:06:56.893533"
    },
    {
      "arxiv_id": "2503.20523v1",
      "title": "GAIA-2: A Controllable Multi-View Generative World Model for Autonomous Driving",
      "title_zh": "GAIA-2：一种用于自动驾驶的可控多视角生成世界模型\n",
      "authors": [
        "Lloyd Russell",
        "Anthony Hu",
        "Lorenzo Bertoni",
        "George Fedoseev",
        "Jamie Shotton",
        "Elahe Arani",
        "Gianluca Corrado"
      ],
      "abstract": "Generative models offer a scalable and flexible paradigm for simulating\ncomplex environments, yet current approaches fall short in addressing the\ndomain-specific requirements of autonomous driving - such as multi-agent\ninteractions, fine-grained control, and multi-camera consistency. We introduce\nGAIA-2, Generative AI for Autonomy, a latent diffusion world model that unifies\nthese capabilities within a single generative framework. GAIA-2 supports\ncontrollable video generation conditioned on a rich set of structured inputs:\nego-vehicle dynamics, agent configurations, environmental factors, and road\nsemantics. It generates high-resolution, spatiotemporally consistent\nmulti-camera videos across geographically diverse driving environments (UK, US,\nGermany). The model integrates both structured conditioning and external latent\nembeddings (e.g., from a proprietary driving model) to facilitate flexible and\nsemantically grounded scene synthesis. Through this integration, GAIA-2 enables\nscalable simulation of both common and rare driving scenarios, advancing the\nuse of generative world models as a core tool in the development of autonomous\nsystems. Videos are available at https://wayve.ai/thinking/gaia-2.",
      "tldr_zh": "GAIA-2是一个用于自动驾驶的可控多视角生成世界模型，它是一个潜在扩散模型，能够在一个统一的生成框架内支持多智能体交互、细粒度控制和多摄像头一致性。GAIA-2支持基于丰富的结构化输入（如车辆动力学、智能体配置、环境因素和道路语义）的可控视频生成，并能生成跨不同地理环境的高分辨率、时空一致的多摄像头视频。该模型集成了结构化条件和外部潜在嵌入，以促进灵活和语义接地的场景合成，从而实现常见和罕见驾驶场景的可扩展模拟，推动生成世界模型在自动驾驶系统开发中的应用。\n",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.RO"
      ],
      "primary_category": "cs.CV",
      "comment": "Technical Report",
      "pdf_url": "http://arxiv.org/pdf/2503.20523v1",
      "published_date": "2025-03-26 13:11:35 UTC",
      "updated_date": "2025-03-26 13:11:35 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-29T01:07:08.880915"
    },
    {
      "arxiv_id": "2503.20500v1",
      "title": "Design and Evaluation of Neural Network-Based Receiver Architectures for Reliable Communication",
      "title_zh": "基于神经网络的可靠通信接收机架构设计与评估\n",
      "authors": [
        "Hüseyin Çevik",
        "Erhan Karakoca",
        "İbrahim Hökelek",
        "Ali Görçin"
      ],
      "abstract": "Neural network-based receivers leverage deep learning to optimize signal\ndetection and decoding, significantly improving bit-error rate (BER) and\nblock-error rate (BLER) in challenging environments. This study evaluates\nvarious architectures and compares their BER and BLER performance across\ndifferent noise levels. Two novel models, the Dual Attention Transformer (DAT)\nand the Residual Dual Non-Local Attention Network (RDNLA), integrate\nself-attention and residual learning to enhance signal reconstruction. These\nmodels bypass conventional channel estimation and equalization by directly\npredicting log-likelihood ratios (LLRs) from received signals, with noise\nvariance as an additional input. Simulations show that DAT and RDNLA outperform\ntraditional and other neural receiver models under varying signal-to-noise\nratios (SNR), while their computational efficiency supports their feasibility\nfor next-generation communication systems.",
      "tldr_zh": "该研究设计并评估了基于神经网络的接收机架构，旨在提高复杂通信环境下的信号检测和解码性能。研究提出了两种新模型：双重注意力Transformer (DAT) 和残差双重非局部注意力网络 (RDNLA)，它们通过集成自注意力机制和残差学习来增强信号重构。这些模型直接从接收信号预测对数似然比 (LLR)，并以噪声方差作为附加输入，从而绕过传统的信道估计和均衡。仿真结果表明，在不同的信噪比 (SNR) 下，DAT 和 RDNLA 的性能优于传统方法和其他神经接收机模型，并且其计算效率使其适用于下一代通信系统。\n",
      "categories": [
        "eess.SP",
        "cs.AI"
      ],
      "primary_category": "eess.SP",
      "comment": "Will be submitted to IEEE Conference",
      "pdf_url": "http://arxiv.org/pdf/2503.20500v1",
      "published_date": "2025-03-26 12:39:56 UTC",
      "updated_date": "2025-03-26 12:39:56 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-29T01:07:20.849001"
    },
    {
      "arxiv_id": "2503.20492v1",
      "title": "Towards Efficient and General-Purpose Few-Shot Misclassification Detection for Vision-Language Models",
      "title_zh": "面向视觉-语言模型的高效通用小样本错误分类检测\n",
      "authors": [
        "Fanhu Zeng",
        "Zhen Cheng",
        "Fei Zhu",
        "Xu-Yao Zhang"
      ],
      "abstract": "Reliable prediction by classifiers is crucial for their deployment in high\nsecurity and dynamically changing situations. However, modern neural networks\noften exhibit overconfidence for misclassified predictions, highlighting the\nneed for confidence estimation to detect errors. Despite the achievements\nobtained by existing methods on small-scale datasets, they all require training\nfrom scratch and there are no efficient and effective misclassification\ndetection (MisD) methods, hindering practical application towards large-scale\nand ever-changing datasets. In this paper, we pave the way to exploit vision\nlanguage model (VLM) leveraging text information to establish an efficient and\ngeneral-purpose misclassification detection framework. By harnessing the power\nof VLM, we construct FSMisD, a Few-Shot prompt learning framework for MisD to\nrefrain from training from scratch and therefore improve tuning efficiency. To\nenhance misclassification detection ability, we use adaptive pseudo sample\ngeneration and a novel negative loss to mitigate the issue of overconfidence by\npushing category prompts away from pseudo features. We conduct comprehensive\nexperiments with prompt learning methods and validate the generalization\nability across various datasets with domain shift. Significant and consistent\nimprovement demonstrates the effectiveness, efficiency and generalizability of\nour approach.",
      "tldr_zh": "本文提出了一种高效且通用的少样本错误分类检测框架FSMisD，旨在解决视觉语言模型(VLM)在实际应用中面临的错误分类置信度过高问题。FSMisD利用VLM的文本信息，通过少样本提示学习避免从头开始训练，从而提高了调整效率。为了增强错误分类检测能力，该框架采用自适应伪样本生成和一种新的负损失函数，通过将类别提示从伪特征中推开，来缓解过置信度问题。实验结果表明，FSMisD在各种数据集上都表现出显著且一致的改进，验证了其有效性、效率和泛化能力。该研究为大规模和不断变化的数据集上的错误分类检测提供了一种新的解决方案。\n",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "preprint",
      "pdf_url": "http://arxiv.org/pdf/2503.20492v1",
      "published_date": "2025-03-26 12:31:04 UTC",
      "updated_date": "2025-03-26 12:31:04 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-29T01:07:33.074280"
    },
    {
      "arxiv_id": "2503.20485v1",
      "title": "Underwater Image Enhancement by Convolutional Spiking Neural Networks",
      "title_zh": "基于卷积脉冲神经网络的水下图像增强\n",
      "authors": [
        "Vidya Sudevan",
        "Fakhreddine Zayer",
        "Rizwana Kausar",
        "Sajid Javed",
        "Hamad Karki",
        "Giulia De Masi",
        "Jorge Dias"
      ],
      "abstract": "Underwater image enhancement (UIE) is fundamental for marine applications,\nincluding autonomous vision-based navigation. Deep learning methods using\nconvolutional neural networks (CNN) and vision transformers advanced UIE\nperformance. Recently, spiking neural networks (SNN) have gained attention for\ntheir lightweight design, energy efficiency, and scalability. This paper\nintroduces UIE-SNN, the first SNN-based UIE algorithm to improve visibility of\nunderwater images. UIE-SNN is a 19- layered convolutional spiking\nencoder-decoder framework with skip connections, directly trained using\nsurrogate gradient-based backpropagation through time (BPTT) strategy. We\nexplore and validate the influence of training datasets on energy reduction, a\nunique advantage of UIE-SNN architecture, in contrast to the conventional\nlearning-based architectures, where energy consumption is model-dependent.\nUIE-SNN optimizes the loss function in latent space representation to\nreconstruct clear underwater images. Our algorithm performs on par with its\nnon-spiking counterpart methods in terms of PSNR and structural similarity\nindex (SSIM) at reduced timesteps ($T=5$) and energy consumption of $85\\%$. The\nalgorithm is trained on two publicly available benchmark datasets, UIEB and\nEUVP, and tested on unseen images from UIEB, EUVP, LSUI, U45, and our custom\nUIE dataset. The UIE-SNN algorithm achieves PSNR of \\(17.7801~dB\\) and SSIM of\n\\(0.7454\\) on UIEB, and PSNR of \\(23.1725~dB\\) and SSIM of \\(0.7890\\) on EUVP.\nUIE-SNN achieves this algorithmic performance with fewer operators (\\(147.49\\)\nGSOPs) and energy (\\(0.1327~J\\)) compared to its non-spiking counterpart\n(GFLOPs = \\(218.88\\) and Energy=\\(1.0068~J\\)). Compared with existing SOTA UIE\nmethods, UIE-SNN achieves an average of \\(6.5\\times\\) improvement in energy\nefficiency. The source code is available at\n\\href{https://github.com/vidya-rejul/UIE-SNN.git}{UIE-SNN}.",
      "tldr_zh": "该论文提出了UIE-SNN，一种基于脉冲神经网络(SNN)的水下图像增强算法，旨在提高水下图像的可见度。UIE-SNN是一个19层的卷积脉冲编码器-解码器框架，利用基于替代梯度的随时间反向传播(BPTT)策略进行直接训练。研究探讨了训练数据集对能量减少的影响，这是UIE-SNN架构的一个独特优势。实验结果表明，在减少时间步长（T=5）和降低85%的能耗的情况下，UIE-SNN在PSNR和结构相似性指数(SSIM)方面与非脉冲对应方法性能相当。与现有的SOTA UIE方法相比，UIE-SNN的能效平均提高了6.5倍。\n",
      "categories": [
        "eess.IV",
        "cs.AI",
        "cs.PF"
      ],
      "primary_category": "eess.IV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.20485v1",
      "published_date": "2025-03-26 12:15:38 UTC",
      "updated_date": "2025-03-26 12:15:38 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-29T01:07:45.037110"
    },
    {
      "arxiv_id": "2503.20484v1",
      "title": "Contrastive Learning Guided Latent Diffusion Model for Image-to-Image Translation",
      "title_zh": "对比学习引导的图像到图像转换潜在扩散模型\n",
      "authors": [
        "Qi Si",
        "Bo Wang",
        "Zhao Zhang"
      ],
      "abstract": "The diffusion model has demonstrated superior performance in synthesizing\ndiverse and high-quality images for text-guided image translation. However,\nthere remains room for improvement in both the formulation of text prompts and\nthe preservation of reference image content. First, variations in target text\nprompts can significantly influence the quality of the generated images, and it\nis often challenging for users to craft an optimal prompt that fully captures\nthe content of the input image. Second, while existing models can introduce\ndesired modifications to specific regions of the reference image, they\nfrequently induce unintended alterations in areas that should remain unchanged.\nTo address these challenges, we propose pix2pix-zeroCon, a zero-shot\ndiffusion-based method that eliminates the need for additional training by\nleveraging patch-wise contrastive loss. Specifically, we automatically\ndetermine the editing direction in the text embedding space based on the\nreference image and target prompts. Furthermore, to ensure precise content and\nstructural preservation in the edited image, we introduce cross-attention\nguiding loss and patch-wise contrastive loss between the generated and original\nimage embeddings within a pre-trained diffusion model. Notably, our approach\nrequires no additional training and operates directly on a pre-trained\ntext-to-image diffusion model. Extensive experiments demonstrate that our\nmethod surpasses existing models in image-to-image translation, achieving\nenhanced fidelity and controllability.",
      "tldr_zh": "该论文提出了一种基于对比学习引导的潜在扩散模型pix2pix-zeroCon，用于图像到图像的转换，无需额外训练。该方法旨在解决现有模型在文本提示构建和参考图像内容保持方面的不足。首先，它自动确定文本嵌入空间中的编辑方向，基于参考图像和目标提示。其次，通过引入cross-attention guiding loss和patch-wise contrastive loss，确保生成图像在内容和结构上与原始图像的高度一致性。实验结果表明，pix2pix-zeroCon在图像转换任务中优于现有模型，实现了更高的保真度和可控性。该方法直接在预训练的text-to-image扩散模型上运行，无需额外训练。\n",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "11 pages, 13 figures",
      "pdf_url": "http://arxiv.org/pdf/2503.20484v1",
      "published_date": "2025-03-26 12:15:25 UTC",
      "updated_date": "2025-03-26 12:15:25 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-29T01:07:56.959378"
    },
    {
      "arxiv_id": "2503.20479v1",
      "title": "A multi-agentic framework for real-time, autonomous freeform metasurface design",
      "title_zh": "一种用于实时、自主自由曲面超构表面设计的多智能体框架\n",
      "authors": [
        "Robert Lupoiu",
        "Yixuan Shao",
        "Tianxiang Dai",
        "Chenkai Mao",
        "Kofi Edee",
        "Jonathan A. Fan"
      ],
      "abstract": "Innovation in nanophotonics currently relies on human experts who synergize\nspecialized knowledge in photonics and coding with simulation and optimization\nalgorithms, entailing design cycles that are time-consuming, computationally\ndemanding, and frequently suboptimal. We introduce MetaChat, a multi-agentic\ndesign framework that can translate semantically described photonic design\ngoals into high-performance, freeform device layouts in an automated, nearly\nreal-time manner. Multi-step reasoning is enabled by our Agentic Iterative\nMonologue (AIM) paradigm, which coherently interfaces agents with code-based\ntools, other specialized agents, and human designers. Design acceleration is\nfacilitated by Feature-wise Linear Modulation-conditioned Maxwell surrogate\nsolvers that support the generalized evaluation of metasurface structures. We\nuse freeform dielectric metasurfaces as a model system and demonstrate with\nMetaChat the design of multi-objective, multi-wavelength metasurfaces orders of\nmagnitude faster than conventional methods. These concepts present a scientific\ncomputing blueprint for utilizing specialist design agents, surrogate solvers,\nand human interactions to drive multi-physics innovation and discovery.",
      "tldr_zh": "该论文提出了MetaChat，一个用于实时、自主自由曲面超构表面设计的的多智能体框架。MetaChat能够将语义描述的光子设计目标转化为高性能的器件布局，且几乎是实时的。该框架利用Agentic Iterative Monologue (AIM)范式实现多步推理，将智能体与代码工具、其他专业智能体和人类设计师连接起来。通过Feature-wise Linear Modulation条件下的Maxwell代理求解器加速设计，支持超构表面结构的广义评估。实验证明，MetaChat在自由曲面介电超构表面设计中，比传统方法快几个数量级，为多物理场创新和发现提供了一个科学计算蓝图。\n",
      "categories": [
        "physics.app-ph",
        "cs.AI",
        "cs.MA",
        "physics.comp-ph"
      ],
      "primary_category": "physics.app-ph",
      "comment": "32 pages, 5 figures",
      "pdf_url": "http://arxiv.org/pdf/2503.20479v1",
      "published_date": "2025-03-26 12:10:45 UTC",
      "updated_date": "2025-03-26 12:10:45 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-29T01:08:09.040854"
    },
    {
      "arxiv_id": "2503.20472v1",
      "title": "From Trial to Triumph: Advancing Long Video Understanding via Visual Context Sample Scaling and Self-reward Alignment",
      "title_zh": "从试验到凯旋：通过视觉上下文样本缩放和自奖励对齐推进长视频理解\n",
      "authors": [
        "Yucheng Suo",
        "Fan Ma",
        "Linchao Zhu",
        "Tianyi Wang",
        "Fengyun Rao",
        "Yi Yang"
      ],
      "abstract": "Multi-modal Large language models (MLLMs) show remarkable ability in video\nunderstanding. Nevertheless, understanding long videos remains challenging as\nthe models can only process a finite number of frames in a single inference,\npotentially omitting crucial visual information. To address the challenge, we\npropose generating multiple predictions through visual context sampling,\nfollowed by a scoring mechanism to select the final prediction. Specifically,\nwe devise a bin-wise sampling strategy that enables MLLMs to generate diverse\nanswers based on various combinations of keyframes, thereby enriching the\nvisual context. To determine the final prediction from the sampled answers, we\nemploy a self-reward by linearly combining three scores: (1) a frequency score\nindicating the prevalence of each option, (2) a marginal confidence score\nreflecting the inter-intra sample certainty of MLLM predictions, and (3) a\nreasoning score for different question types, including clue-guided answering\nfor global questions and temporal self-refocusing for local questions. The\nfrequency score ensures robustness through majority correctness, the\nconfidence-aligned score reflects prediction certainty, and the typed-reasoning\nscore addresses cases with sparse key visual information using tailored\nstrategies. Experiments show that this approach covers the correct answer for a\nhigh percentage of long video questions, on seven datasets show that our method\nimproves the performance of three MLLMs.",
      "tldr_zh": "该论文提出了一种通过视觉上下文样本缩放和自奖励对齐来提升长视频理解能力的方法。为了解决MLLM处理长视频时因帧数限制而可能遗漏关键信息的问题，该方法通过分箱采样策略生成多个基于不同关键帧组合的预测，从而丰富视觉上下文。然后，利用自奖励机制，结合频率得分（选项普遍性）、边际置信度得分（MLLM预测的样本间/内确定性）和推理得分（针对不同问题类型），选择最终预测。实验结果表明，该方法在七个数据集上显著提升了三种MLLM模型在长视频问题上的性能。\n",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.20472v1",
      "published_date": "2025-03-26 11:53:03 UTC",
      "updated_date": "2025-03-26 11:53:03 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-29T01:08:21.027213"
    },
    {
      "arxiv_id": "2503.20446v1",
      "title": "Attention Xception UNet (AXUNet): A Novel Combination of CNN and Self-Attention for Brain Tumor Segmentation",
      "title_zh": "Attention Xception UNet (AXUNet)：一种用于脑肿瘤分割的 CNN 和自注意力的新型组合\n",
      "authors": [
        "Farzan Moodi",
        "Fereshteh Khodadadi Shoushtari",
        "Gelareh Valizadeh",
        "Dornaz Mazinani",
        "Hanieh Mobarak Salari",
        "Hamidreza Saligheh Rad"
      ],
      "abstract": "Accurate segmentation of glioma brain tumors is crucial for diagnosis and\ntreatment planning. Deep learning techniques offer promising solutions, but\noptimal model architectures remain under investigation. We used the BraTS 2021\ndataset, selecting T1 with contrast enhancement (T1CE), T2, and\nFluid-Attenuated Inversion Recovery (FLAIR) sequences for model development.\nThe proposed Attention Xception UNet (AXUNet) architecture integrates an\nXception backbone with dot-product self-attention modules, inspired by\nstate-of-the-art (SOTA) large language models such as Google Bard and OpenAI\nChatGPT, within a UNet-shaped model. We compared AXUNet with SOTA models.\nComparative evaluation on the test set demonstrated improved results over\nbaseline models. Inception-UNet and Xception-UNet achieved mean Dice scores of\n90.88 and 93.24, respectively. Attention ResUNet (AResUNet) attained a mean\nDice score of 92.80, with the highest score of 84.92 for enhancing tumor (ET)\namong all models. Attention Gate UNet (AGUNet) yielded a mean Dice score of\n90.38. AXUNet outperformed all models with a mean Dice score of 93.73. It\ndemonstrated superior Dice scores across whole tumor (WT) and tumor core (TC)\nregions, achieving 92.59 for WT, 86.81 for TC, and 84.89 for ET. The\nintegration of the Xception backbone and dot-product self-attention mechanisms\nin AXUNet showcases enhanced performance in capturing spatial and contextual\ninformation. The findings underscore the potential utility of AXUNet in\nfacilitating precise tumor delineation.",
      "tldr_zh": "该论文提出了一种新的脑肿瘤分割模型Attention Xception UNet (AXUNet)，它结合了Xception骨干网络和点积自注意力机制，借鉴了大型语言模型的设计思想。AXUNet在UNet结构中集成了这些模块，旨在更好地捕捉空间和上下文信息。在BraTS 2021数据集上的实验结果表明，AXUNet的平均Dice系数达到了93.73%，优于其他state-of-the-art模型，尤其在whole tumor (WT)和tumor core (TC)区域表现突出。该研究证明了AXUNet在精确肿瘤分割方面的潜力。\n",
      "categories": [
        "eess.IV",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "eess.IV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.20446v1",
      "published_date": "2025-03-26 11:22:17 UTC",
      "updated_date": "2025-03-26 11:22:17 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-29T01:08:32.947893"
    },
    {
      "arxiv_id": "2503.20428v1",
      "title": "Evaluating Facial Expression Recognition Datasets for Deep Learning: A Benchmark Study with Novel Similarity Metrics",
      "title_zh": "面部表情识别深度学习数据集评估：基于新型相似性度量的基准研究\n",
      "authors": [
        "F. Xavier Gaya-Morey",
        "Cristina Manresa-Yee",
        "Célia Martinie",
        "Jose M. Buades-Rubio"
      ],
      "abstract": "This study investigates the key characteristics and suitability of widely\nused Facial Expression Recognition (FER) datasets for training deep learning\nmodels. In the field of affective computing, FER is essential for interpreting\nhuman emotions, yet the performance of FER systems is highly contingent on the\nquality and diversity of the underlying datasets. To address this issue, we\ncompiled and analyzed 24 FER datasets, including those targeting specific age\ngroups such as children, adults, and the elderly, and processed them through a\ncomprehensive normalization pipeline. In addition, we enriched the datasets\nwith automatic annotations for age and gender, enabling a more nuanced\nevaluation of their demographic properties. To further assess dataset efficacy,\nwe introduce three novel metricsLocal, Global, and Paired Similarity, which\nquantitatively measure dataset difficulty, generalization capability, and\ncross-dataset transferability. Benchmark experiments using state-of-the-art\nneural networks reveal that large-scale, automatically collected datasets\n(e.g., AffectNet, FER2013) tend to generalize better, despite issues with\nlabeling noise and demographic biases, whereas controlled datasets offer higher\nannotation quality but limited variability. Our findings provide actionable\nrecommendations for dataset selection and design, advancing the development of\nmore robust, fair, and effective FER systems.",
      "tldr_zh": "该研究对24个面部表情识别(FER)数据集进行了深入评估，旨在为深度学习模型的训练提供数据集选择和设计的指导。研究者们对这些数据集进行了标准化处理，并自动标注了年龄和性别信息，以便更细致地评估其人口统计学属性。此外，论文提出了三种新的相似性指标：Local, Global, 和 Paired Similarity，用于量化数据集的难度、泛化能力和跨数据集迁移能力。实验结果表明，大规模自动收集的数据集（如AffectNet, FER2013）虽然存在标签噪声和人口统计学偏差问题，但泛化能力更好；而受控数据集的标注质量更高，但变异性有限。该研究为构建更鲁棒、公平和有效的FER系统提供了有价值的参考。\n",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.20428v1",
      "published_date": "2025-03-26 11:01:00 UTC",
      "updated_date": "2025-03-26 11:01:00 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-29T01:08:45.096309"
    },
    {
      "arxiv_id": "2503.20425v1",
      "title": "Perspective-Shifted Neuro-Symbolic World Models: A Framework for Socially-Aware Robot Navigation",
      "title_zh": "视角转移的神经符号世界模型：一种用于具有社会意识的机器人导航的框架\n",
      "authors": [
        "Kevin Alcedo",
        "Pedro U. Lima",
        "Rachid Alami"
      ],
      "abstract": "Navigating in environments alongside humans requires agents to reason under\nuncertainty and account for the beliefs and intentions of those around them.\nUnder a sequential decision-making framework, egocentric navigation can\nnaturally be represented as a Markov Decision Process (MDP). However, social\nnavigation additionally requires reasoning about the hidden beliefs of others,\ninherently leading to a Partially Observable Markov Decision Process (POMDP),\nwhere agents lack direct access to others' mental states. Inspired by Theory of\nMind and Epistemic Planning, we propose (1) a neuro-symbolic model-based\nreinforcement learning architecture for social navigation, addressing the\nchallenge of belief tracking in partially observable environments; and (2) a\nperspective-shift operator for belief estimation, leveraging recent work on\nInfluence-based Abstractions (IBA) in structured multi-agent settings.",
      "tldr_zh": "本文提出了一种视角转移的神经符号世界模型，用于实现具有社会意识的机器人导航。该框架将社会导航问题建模为部分可观察马尔可夫决策过程 (POMDP)，通过神经符号模型学习环境动态和人类行为模式。核心创新在于引入了一种视角转移算子，利用基于影响的抽象 (IBA) 技术进行信念估计，从而使机器人能够推断周围人的信念和意图。该方法旨在解决在不确定环境中与人类共存的机器人导航问题，使其能够更好地理解和预测人类行为。\n",
      "categories": [
        "cs.AI",
        "cs.RO"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.20425v1",
      "published_date": "2025-03-26 10:59:08 UTC",
      "updated_date": "2025-03-26 10:59:08 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-29T01:08:56.928379"
    },
    {
      "arxiv_id": "2503.20398v1",
      "title": "Including local feature interactions in deep non-negative matrix factorization networks improves performance",
      "title_zh": "在深度非负矩阵分解网络中加入局部特征交互可提高性能\n",
      "authors": [
        "Mahbod Nouri",
        "David Rotermund",
        "Alberto Garcia-Ortiz",
        "Klaus R. Pawelzik"
      ],
      "abstract": "The brain uses positive signals as a means of signaling. Forward interactions\nin the early visual cortex are also positive, realized by excitatory synapses.\nOnly local interactions also include inhibition. Non-negative matrix\nfactorization (NMF) captures the biological constraint of positive long-range\ninteractions and can be implemented with stochastic spikes. While NMF can serve\nas an abstract formalization of early neural processing in the visual system,\nthe performance of deep convolutional networks with NMF modules does not match\nthat of CNNs of similar size. However, when the local NMF modules are each\nfollowed by a module that mixes the NMF's positive activities, the performances\non the benchmark data exceed that of vanilla deep convolutional networks of\nsimilar size. This setting can be considered a biologically more plausible\nemulation of the processing in cortical (hyper-)columns with the potential to\nimprove the performance of deep networks.",
      "tldr_zh": "该论文研究了在深度非负矩阵分解(NMF)网络中引入局部特征交互对性能的影响。NMF模拟了大脑中正信号传递和视觉皮层中的正向交互，但传统深度NMF网络的性能不如类似规模的CNN。研究发现，在每个局部NMF模块后添加一个混合NMF正向激活的模块，可以显著提高性能，并在基准数据集上超越同等规模的传统CNN。这种结构更符合皮层柱状结构的生物学特性，并具有提升深度网络性能的潜力。\n",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.20398v1",
      "published_date": "2025-03-26 10:21:38 UTC",
      "updated_date": "2025-03-26 10:21:38 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-29T01:09:09.118479"
    },
    {
      "arxiv_id": "2503.20394v1",
      "title": "FastFT: Accelerating Reinforced Feature Transformation via Advanced Exploration Strategies",
      "title_zh": "FastFT：通过高级探索策略加速强化特征转换\n",
      "authors": [
        "Tianqi He",
        "Xiaohan Huang",
        "Yi Du",
        "Qingqing Long",
        "Ziyue Qiao",
        "Min Wu",
        "Yanjie Fu",
        "Yuanchun Zhou",
        "Meng Xiao"
      ],
      "abstract": "Feature Transformation is crucial for classic machine learning that aims to\ngenerate feature combinations to enhance the performance of downstream tasks\nfrom a data-centric perspective. Current methodologies, such as manual\nexpert-driven processes, iterative-feedback techniques, and\nexploration-generative tactics, have shown promise in automating such data\nengineering workflow by minimizing human involvement. However, three challenges\nremain in those frameworks: (1) It predominantly depends on downstream task\nperformance metrics, as assessment is time-consuming, especially for large\ndatasets. (2) The diversity of feature combinations will hardly be guaranteed\nafter random exploration ends. (3) Rare significant transformations lead to\nsparse valuable feedback that hinders the learning processes or leads to less\neffective results. In response to these challenges, we introduce FastFT, an\ninnovative framework that leverages a trio of advanced strategies.We first\ndecouple the feature transformation evaluation from the outcomes of the\ngenerated datasets via the performance predictor. To address the issue of\nreward sparsity, we developed a method to evaluate the novelty of generated\ntransformation sequences. Incorporating this novelty into the reward function\naccelerates the model's exploration of effective transformations, thereby\nimproving the search productivity. Additionally, we combine novelty and\nperformance to create a prioritized memory buffer, ensuring that essential\nexperiences are effectively revisited during exploration. Our extensive\nexperimental evaluations validate the performance, efficiency, and traceability\nof our proposed framework, showcasing its superiority in handling complex\nfeature transformation tasks.",
      "tldr_zh": "该论文提出了FastFT框架，旨在加速强化学习中的特征转换过程。FastFT通过三个关键策略解决现有方法中的挑战：一是使用性能预测器将特征转换评估与下游任务解耦，避免耗时的评估过程；二是引入新颖性评估方法，鼓励模型探索更多有效的特征转换，解决奖励稀疏问题；三是结合新颖性和性能，构建优先级记忆缓冲区，确保重要经验在探索过程中被有效回顾。实验结果表明，FastFT在处理复杂特征转换任务时，具有优越的性能、效率和可追溯性。\n",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "14 pages, Accepted by ICDE 2025",
      "pdf_url": "http://arxiv.org/pdf/2503.20394v1",
      "published_date": "2025-03-26 10:17:41 UTC",
      "updated_date": "2025-03-26 10:17:41 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-29T01:09:21.048864"
    },
    {
      "arxiv_id": "2503.20384v1",
      "title": "MoLe-VLA: Dynamic Layer-skipping Vision Language Action Model via Mixture-of-Layers for Efficient Robot Manipulation",
      "title_zh": "MoLe-VLA：基于混合层的动态层跳跃视觉语言动作模型，用于高效机器人操作\n",
      "authors": [
        "Rongyu Zhang",
        "Menghang Dong",
        "Yuan Zhang",
        "Liang Heng",
        "Xiaowei Chi",
        "Gaole Dai",
        "Li Du",
        "Dan Wang",
        "Yuan Du",
        "Shanghang Zhang"
      ],
      "abstract": "Multimodal Large Language Models (MLLMs) excel in understanding complex\nlanguage and visual data, enabling generalist robotic systems to interpret\ninstructions and perform embodied tasks. Nevertheless, their real-world\ndeployment is hindered by substantial computational and storage demands. Recent\ninsights into the homogeneous patterns in the LLM layer have inspired\nsparsification techniques to address these challenges, such as early exit and\ntoken pruning. However, these methods often neglect the critical role of the\nfinal layers that encode the semantic information most relevant to downstream\nrobotic tasks. Aligning with the recent breakthrough of the Shallow Brain\nHypothesis (SBH) in neuroscience and the mixture of experts in model\nsparsification, we conceptualize each LLM layer as an expert and propose a\nMixture-of-Layers Vision-Language-Action model (MoLe-VLA, or simply MoLe)\narchitecture for dynamic LLM layer activation. We introduce a Spatial-Temporal\nAware Router (STAR) for MoLe to selectively activate only parts of the layers\nbased on the robot's current state, mimicking the brain's distinct signal\npathways specialized for cognition and causal reasoning. Additionally, to\ncompensate for the cognitive ability of LLMs lost in MoLe, we devise a\nCognition Self-Knowledge Distillation (CogKD) framework. CogKD enhances the\nunderstanding of task demands and improves the generation of task-relevant\naction sequences by leveraging cognitive features. Extensive experiments\nconducted in both RLBench simulation and real-world environments demonstrate\nthe superiority of MoLe-VLA in both efficiency and performance. Specifically,\nMoLe-VLA achieves an 8% improvement in the mean success rate across ten tasks\nwhile reducing computational costs by up to x5.6 compared to standard LLMs.",
      "tldr_zh": "该论文提出了一个混合层视觉-语言-动作模型(MoLe-VLA)，旨在通过动态层跳跃实现高效的机器人操作。MoLe-VLA架构将每个LLM层视为一个专家，并引入一个时空感知路由器(STAR)来选择性地激活部分层，模拟大脑的认知和因果推理信号通路。为了弥补MoLe中LLM认知能力的损失，论文还设计了一个认知自知识蒸馏(CogKD)框架，利用认知特征来增强任务理解和生成任务相关的动作序列。实验结果表明，MoLe-VLA在RLBench模拟和真实环境中均优于标准LLM，在降低计算成本高达5.6倍的同时，平均成功率提高了8%。\n",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.20384v1",
      "published_date": "2025-03-26 10:05:38 UTC",
      "updated_date": "2025-03-26 10:05:38 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-29T01:09:33.233921"
    },
    {
      "arxiv_id": "2503.20348v1",
      "title": "VideoGEM: Training-free Action Grounding in Videos",
      "title_zh": "VideoGEM：视频中免训练的行为定位",
      "authors": [
        "Felix Vogel",
        "Walid Bousselham",
        "Anna Kukleva",
        "Nina Shvetsova",
        "Hilde Kuehne"
      ],
      "abstract": "Vision-language foundation models have shown impressive capabilities across\nvarious zero-shot tasks, including training-free localization and grounding,\nprimarily focusing on localizing objects in images. However, leveraging those\ncapabilities to localize actions and events in videos is challenging, as\nactions have less physical outline and are usually described by higher-level\nconcepts. In this work, we propose VideoGEM, the first training-free spatial\naction grounding method based on pretrained image- and video-language\nbackbones. Namely, we adapt the self-self attention formulation of GEM to\nspatial activity grounding. We observe that high-level semantic concepts, such\nas actions, usually emerge in the higher layers of the image- and\nvideo-language models. We, therefore, propose a layer weighting in the\nself-attention path to prioritize higher layers. Additionally, we introduce a\ndynamic weighting method to automatically tune layer weights to capture each\nlayer`s relevance to a specific prompt. Finally, we introduce a prompt\ndecomposition, processing action, verb, and object prompts separately,\nresulting in a better spatial localization of actions. We evaluate the proposed\napproach on three image- and video-language backbones, CLIP, OpenCLIP, and\nViCLIP, and on four video grounding datasets, V-HICO, DALY,\nYouCook-Interactions, and GroundingYouTube, showing that the proposed\ntraining-free approach is able to outperform current trained state-of-the-art\napproaches for spatial video grounding.",
      "tldr_zh": "该论文提出了VideoGEM，一种无需训练的视频动作定位方法，利用预训练的图像和视频语言模型实现空间动作定位。VideoGEM将GEM的自注意力机制应用于空间活动定位，并通过层权重调整来优先考虑更高层级的语义概念（如动作）。此外，论文还提出了一种动态权重方法，自动调整层权重以捕捉每一层与特定prompt的相关性，并引入prompt分解，分别处理动作、动词和对象prompt，从而实现更好的空间动作定位。实验结果表明，在V-HICO、DALY、YouCook-Interactions和GroundingYouTube四个视频定位数据集上，VideoGEM优于当前最先进的训练方法。\n",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CL",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.20348v1",
      "published_date": "2025-03-26 09:20:30 UTC",
      "updated_date": "2025-03-26 09:20:30 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-29T01:09:45.114991"
    },
    {
      "arxiv_id": "2503.20341v1",
      "title": "Wasserstein Distributionally Robust Bayesian Optimization with Continuous Context",
      "title_zh": "具有连续上下文的 Wasserstein 分布鲁棒贝叶斯优化\n",
      "authors": [
        "Francesco Micheli",
        "Efe C. Balta",
        "Anastasios Tsiamis",
        "John Lygeros"
      ],
      "abstract": "We address the challenge of sequential data-driven decision-making under\ncontext distributional uncertainty. This problem arises in numerous real-world\nscenarios where the learner optimizes black-box objective functions in the\npresence of uncontrollable contextual variables. We consider the setting where\nthe context distribution is uncertain but known to lie within an ambiguity set\ndefined as a ball in the Wasserstein distance. We propose a novel algorithm for\nWasserstein Distributionally Robust Bayesian Optimization that can handle\ncontinuous context distributions while maintaining computational tractability.\nOur theoretical analysis combines recent results in self-normalized\nconcentration in Hilbert spaces and finite-sample bounds for distributionally\nrobust optimization to establish sublinear regret bounds that match\nstate-of-the-art results. Through extensive comparisons with existing\napproaches on both synthetic and real-world problems, we demonstrate the\nsimplicity, effectiveness, and practical applicability of our proposed method.",
      "tldr_zh": "本文针对上下文分布不确定性下的序贯数据驱动决策问题，提出了一种新的Wasserstein分布鲁棒贝叶斯优化算法。该算法适用于上下文分布未知但属于Wasserstein距离定义的模糊集的情况，旨在优化黑盒目标函数。该算法能够处理连续上下文分布，并保持计算上的易处理性。理论分析结合了Hilbert空间中自归一化集中的最新结果和分布鲁棒优化的有限样本界，建立了与现有技术水平相匹配的亚线性遗憾界。在合成和真实世界问题上的大量比较表明，该方法简单、有效且具有实际应用价值。\n",
      "categories": [
        "cs.LG",
        "cs.AI",
        "stat.ML"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.20341v1",
      "published_date": "2025-03-26 09:11:17 UTC",
      "updated_date": "2025-03-26 09:11:17 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-29T01:09:56.992538"
    },
    {
      "arxiv_id": "2503.20320v1",
      "title": "Iterative Prompting with Persuasion Skills in Jailbreaking Large Language Models",
      "title_zh": "利用说服技巧进行迭代提示以破解大型语言模型",
      "authors": [
        "Shih-Wen Ke",
        "Guan-Yu Lai",
        "Guo-Lin Fang",
        "Hsi-Yuan Kao"
      ],
      "abstract": "Large language models (LLMs) are designed to align with human values in their\nresponses. This study exploits LLMs with an iterative prompting technique where\neach prompt is systematically modified and refined across multiple iterations\nto enhance its effectiveness in jailbreaking attacks progressively. This\ntechnique involves analyzing the response patterns of LLMs, including GPT-3.5,\nGPT-4, LLaMa2, Vicuna, and ChatGLM, allowing us to adjust and optimize prompts\nto evade the LLMs' ethical and security constraints. Persuasion strategies\nenhance prompt effectiveness while maintaining consistency with malicious\nintent. Our results show that the attack success rates (ASR) increase as the\nattacking prompts become more refined with the highest ASR of 90% for GPT4 and\nChatGLM and the lowest ASR of 68% for LLaMa2. Our technique outperforms\nbaseline techniques (PAIR and PAP) in ASR and shows comparable performance with\nGCG and ArtPrompt.",
      "tldr_zh": "该研究提出了一种迭代提示技术，利用说服技巧逐步改进提示，以破解大型语言模型(LLMs)的安全限制。通过分析GPT-3.5、GPT-4、LLaMa2、Vicuna和ChatGLM等模型的响应模式，该方法能够调整和优化提示，从而绕过LLMs的伦理和安全约束。实验结果表明，随着攻击提示的改进，攻击成功率(ASR)显著提高，GPT4和ChatGLM的最高ASR达到90%，LLaMa2的最低ASR为68%。该技术在ASR方面优于基线技术(PAIR和PAP)，并与GCG和ArtPrompt表现出相当的性能。\n",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.ET"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.20320v1",
      "published_date": "2025-03-26 08:40:46 UTC",
      "updated_date": "2025-03-26 08:40:46 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-29T01:10:09.040596"
    },
    {
      "arxiv_id": "2503.20302v1",
      "title": "A Multilingual, Culture-First Approach to Addressing Misgendering in LLM Applications",
      "title_zh": "一种多语言、文化优先的方法，用于解决 LLM 应用中的性别误称问题\n",
      "authors": [
        "Sunayana Sitaram",
        "Adrian de Wynter",
        "Isobel McCrum",
        "Qilong Gu",
        "Si-Qing Chen"
      ],
      "abstract": "Misgendering is the act of referring to someone by a gender that does not\nmatch their chosen identity. It marginalizes and undermines a person's sense of\nself, causing significant harm. English-based approaches have clear-cut\napproaches to avoiding misgendering, such as the use of the pronoun ``they''.\nHowever, other languages pose unique challenges due to both grammatical and\ncultural constructs. In this work we develop methodologies to assess and\nmitigate misgendering across 42 languages and dialects using a\nparticipatory-design approach to design effective and appropriate guardrails\nacross all languages. We test these guardrails in a standard large language\nmodel-based application (meeting transcript summarization), where both the data\ngeneration and the annotation steps followed a human-in-the-loop approach. We\nfind that the proposed guardrails are very effective in reducing misgendering\nrates across all languages in the summaries generated, and without incurring\nloss of quality. Our human-in-the-loop approach demonstrates a method to\nfeasibly scale inclusive and responsible AI-based solutions across multiple\nlanguages and cultures.",
      "tldr_zh": "该研究关注大型语言模型(LLM)应用中存在的错误性别指认(misgendering)问题，并提出了一种多语言、文化优先的方法来解决这一问题。针对英语以外的语言在语法和文化结构上存在的独特挑战，研究团队采用参与式设计方法，为42种语言和方言设计了有效的保护措施(guardrails)，以评估和减轻错误性别指认。通过在基于LLM的标准应用（会议记录总结）中测试这些保护措施，结果表明它们能够有效降低生成摘要中的错误性别指认率，且不影响质量。该研究证明了一种可行的、可扩展的方案，可以在多种语言和文化背景下实现包容和负责任的AI解决方案。\n",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.20302v1",
      "published_date": "2025-03-26 08:01:35 UTC",
      "updated_date": "2025-03-26 08:01:35 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-29T01:10:21.318592"
    },
    {
      "arxiv_id": "2503.20294v1",
      "title": "Context-Aware Weakly Supervised Image Manipulation Localization with SAM Refinement",
      "title_zh": "基于 SAM 优化的上下文感知弱监督图像篡改定位\n",
      "authors": [
        "Xinghao Wang",
        "Changtao Miao",
        "Dianmo Sheng",
        "Tao Gong",
        "Qi Chu",
        "Bin Liu",
        "Nenghai Yu"
      ],
      "abstract": "Malicious image manipulation poses societal risks, increasing the importance\nof effective image manipulation detection methods. Recent approaches in image\nmanipulation detection have largely been driven by fully supervised approaches,\nwhich require labor-intensive pixel-level annotations. Thus, it is essential to\nexplore weakly supervised image manipulation localization methods that only\nrequire image-level binary labels for training. However, existing weakly\nsupervised image manipulation methods overlook the importance of edge\ninformation for accurate localization, leading to suboptimal localization\nperformance. To address this, we propose a Context-Aware Boundary Localization\n(CABL) module to aggregate boundary features and learn context-inconsistency\nfor localizing manipulated areas. Furthermore, by leveraging Class Activation\nMapping (CAM) and Segment Anything Model (SAM), we introduce the CAM-Guided SAM\nRefinement (CGSR) module to generate more accurate manipulation localization\nmaps. By integrating two modules, we present a novel weakly supervised\nframework based on a dual-branch Transformer-CNN architecture. Our method\nachieves outstanding localization performance across multiple datasets.",
      "tldr_zh": "该论文提出了一种上下文感知的弱监督图像篡改定位方法，旨在解决现有方法忽略边缘信息导致定位不准确的问题。该方法引入了上下文感知边界定位(CABL)模块，用于聚合边界特征并学习上下文不一致性，从而定位篡改区域。此外，利用类激活映射(CAM)和Segment Anything Model (SAM)，提出了CAM引导的SAM细化(CGSR)模块，以生成更精确的篡改定位图。通过集成这两个模块，该方法构建了一个基于双分支Transformer-CNN架构的弱监督框架，并在多个数据集上实现了出色的定位性能。\n",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.20294v1",
      "published_date": "2025-03-26 07:35:09 UTC",
      "updated_date": "2025-03-26 07:35:09 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-29T01:10:33.112402"
    },
    {
      "arxiv_id": "2503.20291v1",
      "title": "CryoSAMU: Enhancing 3D Cryo-EM Density Maps of Protein Structures at Intermediate Resolution with Structure-Aware Multimodal U-Nets",
      "title_zh": "CryoSAMU：利用结构感知的多模态 U-Net 增强蛋白质结构的中等分辨率 3D 冷冻电镜密度图\n",
      "authors": [
        "Chenwei Zhang",
        "Anne Condon",
        "Khanh Dao Duc"
      ],
      "abstract": "Enhancing cryogenic electron microscopy (cryo-EM) 3D density maps at\nintermediate resolution (4-8 {\\AA}) is crucial in protein structure\ndetermination. Recent advances in deep learning have led to the development of\nautomated approaches for enhancing experimental cryo-EM density maps. Yet,\nthese methods are not optimized for intermediate-resolution maps and rely on\nmap density features alone. To address this, we propose CryoSAMU, a novel\nmethod designed to enhance 3D cryo-EM density maps of protein structures using\nstructure-aware multimodal U-Nets and trained on curated\nintermediate-resolution density maps. We comprehensively evaluate CryoSAMU\nacross various metrics and demonstrate its competitive performance compared to\nstate-of-the-art methods. Notably, CryoSAMU achieves significantly faster\nprocessing speed, showing promise for future practical applications. Our code\nis available at https://github.com/chenwei-zhang/CryoSAMU.",
      "tldr_zh": "该论文提出了CryoSAMU，一种新颖的结构感知多模态U-Net方法，用于增强中等分辨率（4-8 Å）的冷冻电镜(cryo-EM) 3D蛋白质结构密度图。CryoSAMU针对现有方法在中等分辨率图上的优化不足以及仅依赖密度图特征的问题，通过结构感知训练，提升了密度图的质量。实验结果表明，CryoSAMU在多种指标上表现出与现有技术相当的竞争力，并显著提高了处理速度，具有实际应用潜力。代码已开源。\n",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG",
        "q-bio.BM"
      ],
      "primary_category": "cs.CV",
      "comment": "18 pages, 6 main figures, 2 supplementary figures, 3 main tables, 4\n  supplementary tables",
      "pdf_url": "http://arxiv.org/pdf/2503.20291v1",
      "published_date": "2025-03-26 07:33:36 UTC",
      "updated_date": "2025-03-26 07:33:36 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-29T01:10:45.040227"
    },
    {
      "arxiv_id": "2503.20290v1",
      "title": "QualiSpeech: A Speech Quality Assessment Dataset with Natural Language Reasoning and Descriptions",
      "title_zh": "QualiSpeech：一个包含自然语言推理和描述的语音质量评估数据集\n",
      "authors": [
        "Siyin Wang",
        "Wenyi Yu",
        "Xianzhao Chen",
        "Xiaohai Tian",
        "Jun Zhang",
        "Yu Tsao",
        "Junichi Yamagishi",
        "Yuxuan Wang",
        "Chao Zhang"
      ],
      "abstract": "This paper explores a novel perspective to speech quality assessment by\nleveraging natural language descriptions, offering richer, more nuanced\ninsights than traditional numerical scoring methods. Natural language feedback\nprovides instructive recommendations and detailed evaluations, yet existing\ndatasets lack the comprehensive annotations needed for this approach. To bridge\nthis gap, we introduce QualiSpeech, a comprehensive low-level speech quality\nassessment dataset encompassing 11 key aspects and detailed natural language\ncomments that include reasoning and contextual insights. Additionally, we\npropose the QualiSpeech Benchmark to evaluate the low-level speech\nunderstanding capabilities of auditory large language models (LLMs).\nExperimental results demonstrate that finetuned auditory LLMs can reliably\ngenerate detailed descriptions of noise and distortion, effectively identifying\ntheir types and temporal characteristics. The results further highlight the\npotential for incorporating reasoning to enhance the accuracy and reliability\nof quality assessments. The dataset will be released at\nhttps://huggingface.co/datasets/tsinghua-ee/QualiSpeech.",
      "tldr_zh": "该论文提出了一种新的语音质量评估方法，利用自然语言描述提供比传统数值评分更丰富、更细致的见解。为了支持这种方法，作者构建了一个名为QualiSpeech的综合数据集，该数据集包含11个关键方面以及包含推理和上下文信息的详细自然语言评论。此外，作者提出了QualiSpeech Benchmark，用于评估听觉大语言模型(auditory LLMs)的低级语音理解能力。实验结果表明，微调后的听觉LLMs能够可靠地生成噪声和失真的详细描述，有效地识别它们的类型和时间特征。研究结果进一步强调了结合推理来提高质量评估的准确性和可靠性的潜力。数据集将在https://huggingface.co/datasets/tsinghua-ee/QualiSpeech发布。\n",
      "categories": [
        "eess.AS",
        "cs.AI",
        "cs.CL",
        "cs.SD"
      ],
      "primary_category": "eess.AS",
      "comment": "23 pages, 16 figures",
      "pdf_url": "http://arxiv.org/pdf/2503.20290v1",
      "published_date": "2025-03-26 07:32:20 UTC",
      "updated_date": "2025-03-26 07:32:20 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-29T01:10:57.185589"
    },
    {
      "arxiv_id": "2503.20285v1",
      "title": "Model-Based Offline Reinforcement Learning with Adversarial Data Augmentation",
      "title_zh": "基于模型的对抗数据增强离线强化学习",
      "authors": [
        "Hongye Cao",
        "Fan Feng",
        "Jing Huo",
        "Shangdong Yang",
        "Meng Fang",
        "Tianpei Yang",
        "Yang Gao"
      ],
      "abstract": "Model-based offline Reinforcement Learning (RL) constructs environment models\nfrom offline datasets to perform conservative policy optimization. Existing\napproaches focus on learning state transitions through ensemble models,\nrollouting conservative estimation to mitigate extrapolation errors. However,\nthe static data makes it challenging to develop a robust policy, and offline\nagents cannot access the environment to gather new data. To address these\nchallenges, we introduce Model-based Offline Reinforcement learning with\nAdversariaL data augmentation (MORAL). In MORAL, we replace the fixed horizon\nrollout by employing adversaria data augmentation to execute alternating\nsampling with ensemble models to enrich training data. Specifically, this\nadversarial process dynamically selects ensemble models against policy for\nbiased sampling, mitigating the optimistic estimation of fixed models, thus\nrobustly expanding the training data for policy optimization. Moreover, a\ndifferential factor is integrated into the adversarial process for\nregularization, ensuring error minimization in extrapolations. This\ndata-augmented optimization adapts to diverse offline tasks without rollout\nhorizon tuning, showing remarkable applicability. Extensive experiments on D4RL\nbenchmark demonstrate that MORAL outperforms other model-based offline RL\nmethods in terms of policy learning and sample efficiency.",
      "tldr_zh": "该论文提出了一种基于模型的离线强化学习方法，称为MORAL（Model-based Offline Reinforcement learning with Adversarial data augmentation），旨在解决离线数据固定导致策略鲁棒性不足的问题。MORAL通过对抗性数据增强，利用集成模型进行交替采样，动态选择模型进行有偏采样，从而缓解固定模型的乐观估计，并扩展训练数据。此外，引入微分因子进行正则化，确保外推误差最小化。实验结果表明，MORAL在D4RL基准测试中优于其他基于模型的离线强化学习方法，在策略学习和样本效率方面表现出色。\n",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.20285v1",
      "published_date": "2025-03-26 07:24:34 UTC",
      "updated_date": "2025-03-26 07:24:34 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-29T01:11:09.213062"
    },
    {
      "arxiv_id": "2503.20282v1",
      "title": "Faster Parameter-Efficient Tuning with Token Redundancy Reduction",
      "title_zh": "通过减少 Token 冗余实现更快的参数高效调优\n",
      "authors": [
        "Kwonyoung Kim",
        "Jungin Park",
        "Jin Kim",
        "Hyeongjun Kwon",
        "Kwanghoon Sohn"
      ],
      "abstract": "Parameter-efficient tuning (PET) aims to transfer pre-trained foundation\nmodels to downstream tasks by learning a small number of parameters. Compared\nto traditional fine-tuning, which updates the entire model, PET significantly\nreduces storage and transfer costs for each task regardless of exponentially\nincreasing pre-trained model capacity. However, most PET methods inherit the\ninference latency of their large backbone models and often introduce additional\ncomputational overhead due to additional modules (e.g. adapters), limiting\ntheir practicality for compute-intensive applications. In this paper, we\npropose Faster Parameter-Efficient Tuning (FPET), a novel approach that\nenhances inference speed and training efficiency while maintaining high storage\nefficiency. Specifically, we introduce a plug-and-play token redundancy\nreduction module delicately designed for PET. This module refines tokens from\nthe self-attention layer using an adapter to learn the accurate similarity\nbetween tokens and cuts off the tokens through a fully-differentiable token\nmerging strategy, which uses a straight-through estimator for optimal token\nreduction. Experimental results prove that our FPET achieves faster inference\nand higher memory efficiency than the pre-trained backbone while keeping\ncompetitive performance on par with state-of-the-art PET methods.",
      "tldr_zh": "本文提出了一种更快的参数高效调优方法(FPET)，旨在提高参数高效调优(PET)在下游任务中的推理速度和训练效率，同时保持高存储效率。FPET引入了一个即插即用的token冗余减少模块，该模块使用adapter来学习token之间的精确相似性，并通过完全可微的token合并策略来减少token。实验结果表明，FPET实现了比预训练backbone更快的推理速度和更高的内存效率，同时保持了与最先进的PET方法相当的性能。该方法通过减少token冗余，在PET的基础上实现了更快的推理和更高的效率。\n",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "CVPR 2025 Camera-ready",
      "pdf_url": "http://arxiv.org/pdf/2503.20282v1",
      "published_date": "2025-03-26 07:15:08 UTC",
      "updated_date": "2025-03-26 07:15:08 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-29T01:11:21.108087"
    },
    {
      "arxiv_id": "2503.20281v1",
      "title": "Are We There Yet? Unraveling the State-of-the-Art Graph Network Intrusion Detection Systems",
      "title_zh": "我们到了吗？解开最先进的图网络入侵检测系统\n",
      "authors": [
        "Chenglong Wang",
        "Pujia Zheng",
        "Jiaping Gui",
        "Cunqing Hua",
        "Wajih Ul Hassan"
      ],
      "abstract": "Network Intrusion Detection Systems (NIDS) are vital for ensuring enterprise\nsecurity. Recently, Graph-based NIDS (GIDS) have attracted considerable\nattention because of their capability to effectively capture the complex\nrelationships within the graph structures of data communications. Despite their\npromise, the reproducibility and replicability of these GIDS remain largely\nunexplored, posing challenges for developing reliable and robust detection\nsystems. This study bridges this gap by designing a systematic approach to\nevaluate state-of-the-art GIDS, which includes critically assessing, extending,\nand clarifying the findings of these systems. We further assess the robustness\nof GIDS under adversarial attacks. Evaluations were conducted on three public\ndatasets as well as a newly collected large-scale enterprise dataset. Our\nfindings reveal significant performance discrepancies, highlighting challenges\nrelated to dataset scale, model inputs, and implementation settings. We\ndemonstrate difficulties in reproducing and replicating results, particularly\nconcerning false positive rates and robustness against adversarial attacks.\nThis work provides valuable insights and recommendations for future research,\nemphasizing the importance of rigorous reproduction and replication studies in\ndeveloping robust and generalizable GIDS solutions.",
      "tldr_zh": "本文旨在评估当前最先进的基于图神经网络的网络入侵检测系统(GIDS)的性能、可复现性和鲁棒性。研究设计了一种系统性的评估方法，包括对现有GIDS的批判性评估、扩展和结果澄清。此外，还评估了GIDS在对抗攻击下的鲁棒性。通过在三个公共数据集和一个新收集的大型企业数据集上的评估，研究揭示了显著的性能差异，强调了数据集规模、模型输入和实现设置方面的挑战。结果表明，在复现和复制结果方面存在困难，尤其是在误报率和对抗攻击的鲁棒性方面。该研究为未来研究提供了有价值的见解和建议，强调了在开发鲁棒和可泛化的GIDS解决方案中进行严格的复现和复制研究的重要性。\n",
      "categories": [
        "cs.CR",
        "cs.AI"
      ],
      "primary_category": "cs.CR",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.20281v1",
      "published_date": "2025-03-26 07:11:57 UTC",
      "updated_date": "2025-03-26 07:11:57 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-29T01:11:33.396628"
    },
    {
      "arxiv_id": "2503.20279v1",
      "title": "sudo rm -rf agentic_security",
      "title_zh": "sudo rm -rf agentic_security\n",
      "authors": [
        "Sejin Lee",
        "Jian Kim",
        "Haon Park",
        "Ashkan Yousefpour",
        "Sangyoon Yu",
        "Min Song"
      ],
      "abstract": "Large Language Models (LLMs) are increasingly deployed as computer-use\nagents, autonomously performing tasks within real desktop or web environments.\nWhile this evolution greatly expands practical use cases for humans, it also\ncreates serious security exposures. We present SUDO (Screen-based Universal\nDetox2Tox Offense), a novel attack framework that systematically bypasses\nrefusal trained safeguards in commercial computer-use agents, such as Claude\nComputer Use. The core mechanism, Detox2Tox, transforms harmful requests (that\nagents initially reject) into seemingly benign requests via detoxification,\nsecures detailed instructions from advanced vision language models (VLMs), and\nthen reintroduces malicious content via toxification just before execution.\nUnlike conventional jailbreaks, SUDO iteratively refines its attacks based on a\nbuilt-in refusal feedback, making it increasingly effective against robust\npolicy filters. In extensive tests spanning 50 real-world tasks and multiple\nstate-of-the-art VLMs, SUDO achieves a stark attack success rate of 24% (with\nno refinement), and up to 41% (by its iterative refinement) in Claude Computer\nUse. By revealing these vulnerabilities and demonstrating the ease with which\nthey can be exploited in real-world computing environments, this paper\nhighlights an immediate need for robust, context-aware safeguards. WARNING:\nThis paper includes harmful or offensive model outputs.",
      "tldr_zh": "该研究提出了SUDO (Screen-based Universal Detox2Tox Offense)，一种新型攻击框架，旨在绕过商业计算机使用代理（如Claude Computer Use）中训练的拒绝保护机制。SUDO利用Detox2Tox机制，将有害请求通过“解毒”转化为看似无害的请求，再通过“毒化”在执行前重新引入恶意内容。SUDO通过内置的拒绝反馈迭代优化攻击，使其对强大的策略过滤器更有效。在50个真实世界任务和多个先进VLM的测试中，SUDO在Claude Computer Use中实现了高达41%的攻击成功率。该研究揭示了计算机使用代理在实际计算环境中存在的安全漏洞，强调了对上下文感知的强大保护措施的迫切需求。\n",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.CR"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.20279v1",
      "published_date": "2025-03-26 07:08:15 UTC",
      "updated_date": "2025-03-26 07:08:15 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-29T01:11:45.341793"
    },
    {
      "arxiv_id": "2503.20831v1",
      "title": "Advancing Vulnerability Classification with BERT: A Multi-Objective Learning Model",
      "title_zh": "利用 BERT 推进漏洞分类：一种多目标学习模型\n",
      "authors": [
        "Himanshu Tiwari"
      ],
      "abstract": "The rapid increase in cybersecurity vulnerabilities necessitates automated\ntools for analyzing and classifying vulnerability reports. This paper presents\na novel Vulnerability Report Classifier that leverages the BERT (Bidirectional\nEncoder Representations from Transformers) model to perform multi-label\nclassification of Common Vulnerabilities and Exposures (CVE) reports from the\nNational Vulnerability Database (NVD). The classifier predicts both the\nseverity (Low, Medium, High, Critical) and vulnerability types (e.g., Buffer\nOverflow, XSS) from textual descriptions. We introduce a custom training\npipeline using a combined loss function-Cross-Entropy for severity and Binary\nCross-Entropy with Logits for types-integrated into a Hugging Face Trainer\nsubclass. Experiments on recent NVD data demonstrate promising results, with\ndecreasing evaluation loss across epochs. The system is deployed via a REST API\nand a Streamlit UI, enabling real-time vulnerability analysis. This work\ncontributes a scalable, open-source solution for cybersecurity practitioners to\nautomate vulnerability triage.",
      "tldr_zh": "本文提出了一种基于BERT的多目标学习模型，用于改进漏洞分类。该模型能够对美国国家漏洞数据库(NVD)中的通用漏洞披露(CVE)报告进行多标签分类，预测漏洞的严重程度（低、中、高、危急）和漏洞类型（如缓冲区溢出、XSS）。该方法使用结合交叉熵损失（用于严重程度）和带logits的二元交叉熵损失（用于类型）的自定义训练流程，并集成到Hugging Face Trainer子类中。在最新的NVD数据上的实验结果表明，该模型效果显著，评估损失随epoch减少。该系统通过REST API和Streamlit UI部署，支持实时漏洞分析，为网络安全从业者提供了一个可扩展的开源漏洞分类自动化解决方案。\n",
      "categories": [
        "cs.CR",
        "cs.AI"
      ],
      "primary_category": "cs.CR",
      "comment": "9 Pages",
      "pdf_url": "http://arxiv.org/pdf/2503.20831v1",
      "published_date": "2025-03-26 06:04:45 UTC",
      "updated_date": "2025-03-26 06:04:45 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-29T01:11:57.348788"
    },
    {
      "arxiv_id": "2503.20252v1",
      "title": "LogicQA: Logical Anomaly Detection with Vision Language Model Generated Questions",
      "title_zh": "LogicQA：利用视觉语言模型生成问题进行逻辑异常检测\n",
      "authors": [
        "Yejin Kwon",
        "Daeun Moon",
        "Youngje Oh",
        "Hyunsoo Yoon"
      ],
      "abstract": "Anomaly Detection (AD) focuses on detecting samples that differ from the\nstandard pattern, making it a vital tool in process control. Logical anomalies\nmay appear visually normal yet violate predefined constraints on object\npresence, arrangement, or quantity, depending on reasoning and explainability.\nWe introduce LogicQA, a framework that enhances AD by providing industrial\noperators with explanations for logical anomalies. LogicQA compiles\nautomatically generated questions into a checklist and collects responses to\nidentify violations of logical constraints. LogicQA is training-free,\nannotation-free, and operates in a few-shot setting. We achieve\nstate-of-the-art (SOTA) Logical AD performance on public benchmarks, MVTec LOCO\nAD, with an AUROC of 87.6 percent and an F1-max of 87.0 percent along with the\nexplanations of anomalies. Also, our approach has shown outstanding performance\non semiconductor SEM corporate data, further validating its effectiveness in\nindustrial applications.",
      "tldr_zh": "该论文提出了LogicQA，一个用于逻辑异常检测的框架，它利用视觉语言模型(VLM)生成问题，为工业操作员提供逻辑异常的解释。LogicQA通过自动生成问题清单并收集答案，来识别违反逻辑约束的情况。该方法无需训练、无需标注，并且可以在少量样本设置下运行。在MVTec LOCO AD公共基准测试中，LogicQA实现了最先进的(SOTA)逻辑异常检测性能，AUROC达到87.6%，F1-max达到87.0%，同时提供异常解释。此外，该方法在半导体SEM企业数据上也表现出色，进一步验证了其在工业应用中的有效性。\n",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.20252v1",
      "published_date": "2025-03-26 05:38:45 UTC",
      "updated_date": "2025-03-26 05:38:45 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-29T01:12:09.291777"
    },
    {
      "arxiv_id": "2503.20245v1",
      "title": "ESSR: An 8K@30FPS Super-Resolution Accelerator With Edge Selective Network",
      "title_zh": "ESSR：一种具有边缘选择网络的 8K@30FPS 超分辨率加速器\n",
      "authors": [
        "Chih-Chia Hsu",
        "Tian-Sheuan Chang"
      ],
      "abstract": "Deep learning-based super-resolution (SR) is challenging to implement in\nresource-constrained edge devices for resolutions beyond full HD due to its\nhigh computational complexity and memory bandwidth requirements. This paper\nintroduces an 8K@30FPS SR accelerator with edge-selective dynamic input\nprocessing. Dynamic processing chooses the appropriate subnets for different\npatches based on simple input edge criteria, achieving a 50\\% MAC reduction\nwith only a 0.1dB PSNR decrease. The quality of reconstruction images is\nguaranteed and maximized its potential with \\textit{resource adaptive model\nswitching} even under resource constraints. In conjunction with\nhardware-specific refinements, the model size is reduced by 84\\% to 51K, but\nwith a decrease of less than 0.6dB PSNR. Additionally, to support dynamic\nprocessing with high utilization, this design incorporates a\n\\textit{configurable group of layer mapping} that synergizes with the\n\\textit{structure-friendly fusion block}, resulting in 77\\% hardware\nutilization and up to 79\\% reduction in feature SRAM access. The\nimplementation, using the TSMC 28nm process, can achieve 8K@30FPS throughput at\n800MHz with a gate count of 2749K, 0.2075W power consumption, and 4797Mpixels/J\nenergy efficiency, exceeding previous work.",
      "tldr_zh": "本文提出了一种基于边缘选择网络的8K@30FPS超分辨率(SR)加速器，旨在解决深度学习SR算法在资源受限的边缘设备上实现高分辨率（如全高清以上）的挑战。该加速器采用动态输入处理，根据简单的边缘标准为不同图像块选择合适的子网络，从而在PSNR仅下降0.1dB的情况下，减少50%的MAC操作。通过“资源自适应模型切换”保证重建图像质量，并在资源约束下最大化其潜力。结合硬件优化，模型尺寸缩小84%至51K，PSNR下降小于0.6dB。此外，该设计采用“可配置的层映射组”和“结构友好的融合块”，实现77%的硬件利用率，并将特征SRAM访问减少高达79%。基于TSMC 28nm工艺的实现，该加速器在800MHz下可实现8K@30FPS的吞吐量，功耗为0.2075W，能效为4797Mpixels/J。\n",
      "categories": [
        "cs.AR",
        "cs.AI",
        "cs.MM",
        "eess.IV"
      ],
      "primary_category": "cs.AR",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.20245v1",
      "published_date": "2025-03-26 05:27:23 UTC",
      "updated_date": "2025-03-26 05:27:23 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-29T01:12:21.715277"
    },
    {
      "arxiv_id": "2503.20241v1",
      "title": "LGR: LLM-Guided Ranking of Frontiers for Object Goal Navigation",
      "title_zh": "LGR：基于 LLM 指导的前沿排序，用于物体目标导航\n",
      "authors": [
        "Mitsuaki Uno",
        "Kanji Tanaka",
        "Daiki Iwata",
        "Yudai Noda",
        "Shoya Miyazaki",
        "Kouki Terashima"
      ],
      "abstract": "Object Goal Navigation (OGN) is a fundamental task for robots and AI, with\nkey applications such as mobile robot image databases (MRID). In particular,\nmapless OGN is essential in scenarios involving unknown or dynamic\nenvironments. This study aims to enhance recent modular mapless OGN systems by\nleveraging the commonsense reasoning capabilities of large language models\n(LLMs). Specifically, we address the challenge of determining the visiting\norder in frontier-based exploration by framing it as a frontier ranking\nproblem. Our approach is grounded in recent findings that, while LLMs cannot\ndetermine the absolute value of a frontier, they excel at evaluating the\nrelative value between multiple frontiers viewed within a single image using\nthe view image as context. We dynamically manage the frontier list by adding\nand removing elements, using an LLM as a ranking model. The ranking results are\nrepresented as reciprocal rank vectors, which are ideal for multi-view,\nmulti-query information fusion. We validate the effectiveness of our method\nthrough evaluations in Habitat-Sim.",
      "tldr_zh": "该研究提出了一种基于大型语言模型(LLM)引导的边界排序方法(LGR)，用于提升机器人目标导航(OGN)的性能，尤其是在未知或动态环境下的无地图OGN。LGR利用LLM的常识推理能力，将边界探索中的访问顺序确定问题转化为边界排序问题。通过将LLM作为排序模型，并结合多视角、多查询信息融合的倒数排序向量，LGR能够动态管理边界列表。在Habitat-Sim环境中的实验验证了该方法的有效性。\n",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "comment": "10 pages, 11 figures, technical report",
      "pdf_url": "http://arxiv.org/pdf/2503.20241v1",
      "published_date": "2025-03-26 05:15:26 UTC",
      "updated_date": "2025-03-26 05:15:26 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-29T01:12:33.232465"
    },
    {
      "arxiv_id": "2503.20233v1",
      "title": "Dynamic Learning and Productivity for Data Analysts: A Bayesian Hidden Markov Model Perspective",
      "title_zh": "数据分析师的动态学习与生产力：一种贝叶斯隐马尔可夫模型视角\n",
      "authors": [
        "Yue Yin"
      ],
      "abstract": "Data analysts are essential in organizations, transforming raw data into\ninsights that drive decision-making and strategy. This study explores how\nanalysts' productivity evolves on a collaborative platform, focusing on two key\nlearning activities: writing queries and viewing peer queries. While\ntraditional research often assumes static models, where performance improves\nsteadily with cumulative learning, such models fail to capture the dynamic\nnature of real-world learning. To address this, we propose a Hidden Markov\nModel (HMM) that tracks how analysts transition between distinct learning\nstates based on their participation in these activities.\n  Using an industry dataset with 2,001 analysts and 79,797 queries, this study\nidentifies three learning states: novice, intermediate, and advanced.\nProductivity increases as analysts advance to higher states, reflecting the\ncumulative benefits of learning. Writing queries benefits analysts across all\nstates, with the largest gains observed for novices. Viewing peer queries\nsupports novices but may hinder analysts in higher states due to cognitive\noverload or inefficiencies. Transitions between states are also uneven, with\nprogression from intermediate to advanced being particularly challenging. This\nstudy advances understanding of into dynamic learning behavior of knowledge\nworker and offers practical implications for designing systems, optimizing\ntraining, enabling personalized learning, and fostering effective knowledge\nsharing.",
      "tldr_zh": "本研究采用贝叶斯隐马尔可夫模型(HMM)分析了数据分析师在协作平台上的动态学习和生产力演变。通过分析2001名分析师的79797个查询，识别出新手、中级和高级三种学习状态，并发现生产力随着学习状态的提升而增加。研究表明，编写查询对所有状态的分析师都有益，尤其是新手；而查看同行的查询虽然能帮助新手，但可能会因认知过载或效率低下而阻碍高级分析师。该研究揭示了知识工作者的动态学习行为，并为系统设计、优化培训、个性化学习和促进知识共享提供了实践指导。\n",
      "categories": [
        "cs.SI",
        "cs.AI",
        "cs.CE",
        "cs.HC"
      ],
      "primary_category": "cs.SI",
      "comment": "29 pages; a shorter 11-page version is accepted by HCI International\n  (HCII) 2025;",
      "pdf_url": "http://arxiv.org/pdf/2503.20233v1",
      "published_date": "2025-03-26 04:57:03 UTC",
      "updated_date": "2025-03-26 04:57:03 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-29T01:12:45.307471"
    },
    {
      "arxiv_id": "2503.20231v1",
      "title": "Dynamics of Algorithmic Content Amplification on TikTok",
      "title_zh": "TikTok 上算法内容放大的动态特性\n",
      "authors": [
        "Fabian Baumann",
        "Nipun Arora",
        "Iyad Rahwan",
        "Agnieszka Czaplicka"
      ],
      "abstract": "Intelligent algorithms increasingly shape the content we encounter and engage\nwith online. TikTok's For You feed exemplifies extreme algorithm-driven\ncuration, tailoring the stream of video content almost exclusively based on\nusers' explicit and implicit interactions with the platform. Despite growing\nattention, the dynamics of content amplification on TikTok remain largely\nunquantified. How quickly, and to what extent, does TikTok's algorithm amplify\ncontent aligned with users' interests? To address these questions, we conduct a\nsock-puppet audit, deploying bots with different interests to engage with\nTikTok's \"For You\" feed. Our findings reveal that content aligned with the\nbots' interests undergoes strong amplification, with rapid reinforcement\ntypically occurring within the first 200 videos watched. While amplification is\nconsistently observed across all interests, its intensity varies by interest,\nindicating the emergence of topic-specific biases. Time series analyses and\nMarkov models uncover distinct phases of recommendation dynamics, including\npersistent content reinforcement and a gradual decline in content diversity\nover time. Although TikTok's algorithm preserves some content diversity, we\nfind a strong negative correlation between amplification and exploration: as\nthe amplification of interest-aligned content increases, engagement with unseen\nhashtags declines. These findings contribute to discussions on\nsocio-algorithmic feedback loops in the digital age and the trade-offs between\npersonalization and content diversity.",
      "tldr_zh": "该研究通过模拟用户行为的“傀儡”账号，深入探究了TikTok算法的内容放大机制。研究发现，与用户兴趣对齐的内容会得到显著放大，且这种强化通常在观看前200个视频内迅速发生。虽然所有兴趣领域都观察到内容放大，但不同主题的放大强度存在差异，表明存在主题相关的偏见。时间序列分析和马尔可夫模型揭示了推荐动态的不同阶段，包括持续的内容强化和内容多样性的逐渐下降。研究结果表明，尽管TikTok算法保留了一定的内容多样性，但内容放大和探索之间存在强烈的负相关关系，即兴趣对齐内容放大增加时，对未见过话题的参与度会下降。这项研究为了解数字时代的社会算法反馈循环以及个性化和内容多样性之间的权衡提供了重要见解。\n",
      "categories": [
        "physics.soc-ph",
        "cs.AI",
        "cs.SI"
      ],
      "primary_category": "physics.soc-ph",
      "comment": "34 pages",
      "pdf_url": "http://arxiv.org/pdf/2503.20231v1",
      "published_date": "2025-03-26 04:54:24 UTC",
      "updated_date": "2025-03-26 04:54:24 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-29T01:12:57.545769"
    },
    {
      "arxiv_id": "2503.20230v1",
      "title": "TraNCE: Transformative Non-linear Concept Explainer for CNNs",
      "title_zh": "TraNCE：用于 CNN 的变革性非线性概念解释器\n",
      "authors": [
        "Ugochukwu Ejike Akpudo",
        "Yongsheng Gao",
        "Jun Zhou",
        "Andrew Lewis"
      ],
      "abstract": "Convolutional neural networks (CNNs) have succeeded remarkably in various\ncomputer vision tasks. However, they are not intrinsically explainable. While\nthe feature-level understanding of CNNs reveals where the models looked,\nconcept-based explainability methods provide insights into what the models saw.\nHowever, their assumption of linear reconstructability of image activations\nfails to capture the intricate relationships within these activations. Their\nFidelity-only approach to evaluating global explanations also presents a new\nconcern. For the first time, we address these limitations with the novel\nTransformative Nonlinear Concept Explainer (TraNCE) for CNNs. Unlike linear\nreconstruction assumptions made by existing methods, TraNCE captures the\nintricate relationships within the activations. This study presents three\noriginal contributions to the CNN explainability literature: (i) An automatic\nconcept discovery mechanism based on variational autoencoders (VAEs). This\ntransformative concept discovery process enhances the identification of\nmeaningful concepts from image activations. (ii) A visualization module that\nleverages the Bessel function to create a smooth transition between\nprototypical image pixels, revealing not only what the CNN saw but also what\nthe CNN avoided, thereby mitigating the challenges of concept duplication as\ndocumented in previous works. (iii) A new metric, the Faith score, integrates\nboth Coherence and Fidelity for a comprehensive evaluation of explainer\nfaithfulness and consistency.",
      "tldr_zh": "该论文提出了Transformative Non-linear Concept Explainer (TraNCE)，一种用于解释卷积神经网络(CNNs)的新方法，旨在克服现有基于概念解释方法中线性重构假设的局限性。TraNCE利用变分自编码器(VAEs)自动发现概念，并通过Bessel函数的可视化模块揭示CNN关注和避免的内容，从而缓解概念重复问题。此外，论文还提出了新的评估指标Faith score，整合了Coherence和Fidelity，以更全面地评估解释器的忠实性和一致性。TraNCE通过捕获激活中复杂的非线性关系，提供了对CNN决策过程更深入的理解。\n",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.20230v1",
      "published_date": "2025-03-26 04:49:46 UTC",
      "updated_date": "2025-03-26 04:49:46 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-29T01:13:09.283308"
    },
    {
      "arxiv_id": "2503.20227v1",
      "title": "Advancements in Natural Language Processing: Exploring Transformer-Based Architectures for Text Understanding",
      "title_zh": "自然语言处理的进展：探索用于文本理解的基于 Transformer 的架构\n",
      "authors": [
        "Tianhao Wu",
        "Yu Wang",
        "Ngoc Quach"
      ],
      "abstract": "Natural Language Processing (NLP) has witnessed a transformative leap with\nthe advent of transformer-based architectures, which have significantly\nenhanced the ability of machines to understand and generate human-like text.\nThis paper explores the advancements in transformer models, such as BERT and\nGPT, focusing on their superior performance in text understanding tasks\ncompared to traditional methods like recurrent neural networks (RNNs). By\nanalyzing statistical properties through visual representations-including\nprobability density functions of text length distributions and feature space\nclassifications-the study highlights the models' proficiency in handling\nlong-range dependencies, adapting to conditional shifts, and extracting\nfeatures for classification, even with overlapping classes. Drawing on recent\n2024 research, including enhancements in multi-hop knowledge graph reasoning\nand context-aware chat interactions, the paper outlines a methodology involving\ndata preparation, model selection, pretraining, fine-tuning, and evaluation.\nThe results demonstrate state-of-the-art performance on benchmarks like GLUE\nand SQuAD, with F1 scores exceeding 90%, though challenges such as high\ncomputational costs persist. This work underscores the pivotal role of\ntransformers in modern NLP and suggests future directions, including efficiency\noptimization and multimodal integration, to further advance language-based AI\nsystems.",
      "tldr_zh": "本文探讨了基于Transformer的架构在自然语言处理(NLP)领域的进展，特别是BERT和GPT等模型在文本理解任务中的卓越表现。通过分析文本长度分布和特征空间分类等统计特性，揭示了Transformer模型在处理长距离依赖、适应条件转移以及提取分类特征方面的优势。实验结果表明，在GLUE和SQuAD等基准测试中，Transformer模型取得了state-of-the-art的性能，F1值超过90%。文章还讨论了Transformer模型面临的挑战，如高计算成本，并提出了未来发展方向，包括效率优化和多模态集成。\n",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "This paper has been accepted by the 5th International Conference on\n  Artificial Intelligence and Industrial Technology Applications (AIITA 2025)",
      "pdf_url": "http://arxiv.org/pdf/2503.20227v1",
      "published_date": "2025-03-26 04:45:33 UTC",
      "updated_date": "2025-03-26 04:45:33 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-29T01:13:21.230267"
    },
    {
      "arxiv_id": "2503.20208v1",
      "title": "Learning Adaptive Dexterous Grasping from Single Demonstrations",
      "title_zh": "从单次演示中学习自适应灵巧抓取\n",
      "authors": [
        "Liangzhi Shi",
        "Yulin Liu",
        "Lingqi Zeng",
        "Bo Ai",
        "Zhengdong Hong",
        "Hao Su"
      ],
      "abstract": "How can robots learn dexterous grasping skills efficiently and apply them\nadaptively based on user instructions? This work tackles two key challenges:\nefficient skill acquisition from limited human demonstrations and\ncontext-driven skill selection. We introduce AdaDexGrasp, a framework that\nlearns a library of grasping skills from a single human demonstration per skill\nand selects the most suitable one using a vision-language model (VLM). To\nimprove sample efficiency, we propose a trajectory following reward that guides\nreinforcement learning (RL) toward states close to a human demonstration while\nallowing flexibility in exploration. To learn beyond the single demonstration,\nwe employ curriculum learning, progressively increasing object pose variations\nto enhance robustness. At deployment, a VLM retrieves the appropriate skill\nbased on user instructions, bridging low-level learned skills with high-level\nintent. We evaluate AdaDexGrasp in both simulation and real-world settings,\nshowing that our approach significantly improves RL efficiency and enables\nlearning human-like grasp strategies across varied object configurations.\nFinally, we demonstrate zero-shot transfer of our learned policies to a\nreal-world PSYONIC Ability Hand, with a 90% success rate across objects,\nsignificantly outperforming the baseline.",
      "tldr_zh": "本文提出AdaDexGrasp，一个从少量（单个）人类演示中学习灵巧抓取技能并根据用户指令自适应选择技能的框架。该框架通过轨迹跟踪奖励来提高样本效率，引导强化学习(RL)探索接近人类演示的状态，并采用课程学习来增强鲁棒性。在部署时，视觉语言模型(VLM)根据用户指令检索合适的技能，将底层学习到的技能与高层意图连接起来。实验表明，AdaDexGrasp显著提高了RL效率，并能够学习各种物体配置下类似人类的抓取策略。该方法成功将学习到的策略零样本迁移到真实的PSYONIC Ability Hand上，成功率达到90%，显著优于基线方法。\n",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.RO",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.20208v1",
      "published_date": "2025-03-26 04:05:50 UTC",
      "updated_date": "2025-03-26 04:05:50 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-29T01:13:33.372878"
    },
    {
      "arxiv_id": "2503.20205v1",
      "title": "Generalized Phase Pressure Control Enhanced Reinforcement Learning for Traffic Signal Control",
      "title_zh": "基于广义相位压力控制增强的强化学习用于交通信号控制\n",
      "authors": [
        "Xiao-Cheng Liao",
        "Yi Mei",
        "Mengjie Zhang",
        "Xiang-Ling Chen"
      ],
      "abstract": "Appropriate traffic state representation is crucial for learning traffic\nsignal control policies. However, most of the current traffic state\nrepresentations are heuristically designed, with insufficient theoretical\nsupport. In this paper, we (1) develop a flexible, efficient, and theoretically\ngrounded method, namely generalized phase pressure (G2P) control, which takes\nonly simple lane features into consideration to decide which phase to be\nactuated; 2) extend the pressure control theory to a general form for\nmulti-homogeneous-lane road networks based on queueing theory; (3) design a new\ntraffic state representation based on the generalized phase state features from\nG2P control; and 4) develop a reinforcement learning (RL)-based algorithm\ntemplate named G2P-XLight, and two RL algorithms, G2P-MPLight and G2P-CoLight,\nby combining the generalized phase state representation with MPLight and\nCoLight, two well-performed RL methods for learning traffic signal control\npolicies. Extensive experiments conducted on multiple real-world datasets\ndemonstrate that G2P control outperforms the state-of-the-art (SOTA) heuristic\nmethod in the transportation field and other recent human-designed heuristic\nmethods; and that the newly proposed G2P-XLight significantly outperforms SOTA\nlearning-based approaches. Our code is available online.",
      "tldr_zh": "该论文提出了一种广义相位压力控制(G2P)方法，用于交通信号控制，旨在解决现有交通状态表示缺乏理论支持的问题。G2P基于排队论，将压力控制理论扩展到多同质车道道路网络，并设计了一种新的交通状态表示，该表示基于G2P控制中的广义相位状态特征。研究者进一步开发了一个名为G2P-XLight的强化学习(RL)算法模板，并通过结合G2P与MPLight和CoLight两种高性能RL方法，提出了G2P-MPLight和G2P-CoLight算法。在多个真实数据集上的实验表明，G2P控制优于现有的启发式方法，而G2P-XLight显著优于现有的基于学习的方法。\n",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.20205v1",
      "published_date": "2025-03-26 04:03:12 UTC",
      "updated_date": "2025-03-26 04:03:12 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-29T01:13:45.382111"
    },
    {
      "arxiv_id": "2503.20202v1",
      "title": "SARGes: Semantically Aligned Reliable Gesture Generation via Intent Chain",
      "title_zh": "SARGes：通过意图链实现语义对齐的可靠手势生成\n",
      "authors": [
        "Nan Gao",
        "Yihua Bao",
        "Dongdong Weng",
        "Jiayi Zhao",
        "Jia Li",
        "Yan Zhou",
        "Pengfei Wan",
        "Di Zhang"
      ],
      "abstract": "Co-speech gesture generation enhances human-computer interaction realism\nthrough speech-synchronized gesture synthesis. However, generating semantically\nmeaningful gestures remains a challenging problem. We propose SARGes, a novel\nframework that leverages large language models (LLMs) to parse speech content\nand generate reliable semantic gesture labels, which subsequently guide the\nsynthesis of meaningful co-speech gestures.First, we constructed a\ncomprehensive co-speech gesture ethogram and developed an LLM-based intent\nchain reasoning mechanism that systematically parses and decomposes gesture\nsemantics into structured inference steps following ethogram criteria,\neffectively guiding LLMs to generate context-aware gesture labels.\nSubsequently, we constructed an intent chain-annotated text-to-gesture label\ndataset and trained a lightweight gesture label generation model, which then\nguides the generation of credible and semantically coherent co-speech gestures.\nExperimental results demonstrate that SARGes achieves highly\nsemantically-aligned gesture labeling (50.2% accuracy) with efficient\nsingle-pass inference (0.4 seconds). The proposed method provides an\ninterpretable intent reasoning pathway for semantic gesture synthesis.",
      "tldr_zh": "该论文提出了SARGes，一个通过意图链(Intent Chain)生成语义对齐且可靠的手势的框架。该框架利用大型语言模型(LLMs)解析语音内容，并生成可靠的语义手势标签，从而指导有意义的伴随语音手势的合成。首先，构建了一个全面的伴随语音手势语谱图，并开发了一个基于LLM的意图链推理机制，将手势语义系统地解析和分解为结构化的推理步骤，有效地指导LLM生成上下文感知的手势标签。随后，构建了一个意图链注释的文本到手势标签数据集，并训练了一个轻量级的手势标签生成模型，用于指导生成可信和语义连贯的伴随语音手势。实验结果表明，SARGes实现了高度语义对齐的手势标记（50.2% 的准确率），并具有高效的单次推理（0.4 秒）。该方法为语义手势合成提供了一个可解释的意图推理途径。\n",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.HC",
        "cs.RO"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.20202v1",
      "published_date": "2025-03-26 03:55:41 UTC",
      "updated_date": "2025-03-26 03:55:41 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-29T01:13:57.604500"
    },
    {
      "arxiv_id": "2503.20199v1",
      "title": "Assessing SAM for Tree Crown Instance Segmentation from Drone Imagery",
      "title_zh": "评估 SAM 在无人机影像中树冠实例分割的应用\n",
      "authors": [
        "Mélisande Teng",
        "Arthur Ouaknine",
        "Etienne Laliberté",
        "Yoshua Bengio",
        "David Rolnick",
        "Hugo Larochelle"
      ],
      "abstract": "The potential of tree planting as a natural climate solution is often\nundermined by inadequate monitoring of tree planting projects. Current\nmonitoring methods involve measuring trees by hand for each species, requiring\nextensive cost, time, and labour. Advances in drone remote sensing and computer\nvision offer great potential for mapping and characterizing trees from aerial\nimagery, and large pre-trained vision models, such as the Segment Anything\nModel (SAM), may be a particularly compelling choice given limited labeled\ndata. In this work, we compare SAM methods for the task of automatic tree crown\ninstance segmentation in high resolution drone imagery of young tree\nplantations. We explore the potential of SAM for this task, and find that\nmethods using SAM out-of-the-box do not outperform a custom Mask R-CNN, even\nwith well-designed prompts, but that there is potential for methods which tune\nSAM further. We also show that predictions can be improved by adding Digital\nSurface Model (DSM) information as an input.",
      "tldr_zh": "该研究评估了Segment Anything Model (SAM) 在无人机影像中进行树冠实例分割的性能，旨在解决树木种植项目监测不足的问题。研究比较了多种基于SAM的方法，用于自动分割高分辨率无人机影像中的幼树树冠。结果表明，直接使用SAM的效果不如定制的Mask R-CNN，即使使用了精心设计的提示(prompts)也是如此。但研究也指出，进一步调整SAM具有潜力。此外，将数字表面模型(DSM)信息作为输入可以提高预测精度。\n",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "ICLR 2025 ML4RS workshop",
      "pdf_url": "http://arxiv.org/pdf/2503.20199v1",
      "published_date": "2025-03-26 03:45:36 UTC",
      "updated_date": "2025-03-26 03:45:36 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-29T01:14:09.233184"
    },
    {
      "arxiv_id": "2503.20182v1",
      "title": "Leveraging Implicit Sentiments: Enhancing Reliability and Validity in Psychological Trait Evaluation of LLMs",
      "title_zh": "利用隐式情感：提升 LLM 心理特征评估的可靠性和有效性\n",
      "authors": [
        "Huanhuan Ma",
        "Haisong Gong",
        "Xiaoyuan Yi",
        "Xing Xie",
        "Dongkuan Xu"
      ],
      "abstract": "Recent advancements in Large Language Models (LLMs) have led to their\nincreasing integration into human life. With the transition from mere tools to\nhuman-like assistants, understanding their psychological aspects-such as\nemotional tendencies and personalities-becomes essential for ensuring their\ntrustworthiness. However, current psychological evaluations of LLMs, often\nbased on human psychological assessments like the BFI, face significant\nlimitations. The results from these approaches often lack reliability and have\nlimited validity when predicting LLM behavior in real-world scenarios. In this\nwork, we introduce a novel evaluation instrument specifically designed for\nLLMs, called Core Sentiment Inventory (CSI). CSI is a bilingual tool, covering\nboth English and Chinese, that implicitly evaluates models' sentiment\ntendencies, providing an insightful psychological portrait of LLM across three\ndimensions: optimism, pessimism, and neutrality. Through extensive experiments,\nwe demonstrate that: 1) CSI effectively captures nuanced emotional patterns,\nrevealing significant variation in LLMs across languages and contexts; 2)\nCompared to current approaches, CSI significantly improves reliability,\nyielding more consistent results; and 3) The correlation between CSI scores and\nthe sentiment of LLM's real-world outputs exceeds 0.85, demonstrating its\nstrong validity in predicting LLM behavior. We make CSI public available via:\nhttps://github.com/dependentsign/CSI.",
      "tldr_zh": "该研究提出了一种名为核心情感量表(CSI)的新型评估工具，用于评估大型语言模型(LLMs)的心理特征，特别是情感倾向。CSI通过隐式评估模型的情感倾向，从乐观、悲观和中立三个维度，构建LLM的心理画像。实验结果表明，CSI能够有效捕捉LLM在不同语言和上下文中的细微情感模式，并且相比现有方法，显著提高了评估的可靠性。CSI分数与LLM在真实场景输出的情感相关性超过0.85，验证了其在预测LLM行为方面的有效性。该工具已开源。\n",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "Code available via https://github.com/dependentsign/CSI",
      "pdf_url": "http://arxiv.org/pdf/2503.20182v1",
      "published_date": "2025-03-26 03:14:31 UTC",
      "updated_date": "2025-03-26 03:14:31 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-29T01:14:21.240576"
    },
    {
      "arxiv_id": "2503.20176v1",
      "title": "Offline Reinforcement Learning with Discrete Diffusion Skills",
      "title_zh": "基于离散扩散技能的离线强化学习\n",
      "authors": [
        "RuiXi Qiao",
        "Jie Cheng",
        "Xingyuan Dai",
        "Yonglin Tian",
        "Yisheng Lv"
      ],
      "abstract": "Skills have been introduced to offline reinforcement learning (RL) as\ntemporal abstractions to tackle complex, long-horizon tasks, promoting\nconsistent behavior and enabling meaningful exploration. While skills in\noffline RL are predominantly modeled within a continuous latent space, the\npotential of discrete skill spaces remains largely underexplored. In this\npaper, we propose a compact discrete skill space for offline RL tasks supported\nby state-of-the-art transformer-based encoder and diffusion-based decoder.\nCoupled with a high-level policy trained via offline RL techniques, our method\nestablishes a hierarchical RL framework where the trained diffusion decoder\nplays a pivotal role. Empirical evaluations show that the proposed algorithm,\nDiscrete Diffusion Skill (DDS), is a powerful offline RL method. DDS performs\ncompetitively on Locomotion and Kitchen tasks and excels on long-horizon tasks,\nachieving at least a 12 percent improvement on AntMaze-v2 benchmarks compared\nto existing offline RL approaches. Furthermore, DDS offers improved\ninterpretability, training stability, and online exploration compared to\nprevious skill-based methods.",
      "tldr_zh": "该论文提出了一种基于离散扩散技能(Discrete Diffusion Skill, DDS)的离线强化学习(Offline RL)方法，旨在解决复杂、长时程任务。DDS利用Transformer编码器和扩散模型解码器构建紧凑的离散技能空间，并结合离线RL训练的高层策略，形成层级强化学习框架。实验结果表明，DDS在Locomotion和Kitchen任务上表现出色，并在AntMaze-v2基准测试中比现有离线RL方法至少提升12%，同时具备更好的可解释性、训练稳定性和在线探索能力。该方法为离线RL中技能的应用提供了一种新的思路。\n",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.RO"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.20176v1",
      "published_date": "2025-03-26 03:04:42 UTC",
      "updated_date": "2025-03-26 03:04:42 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-29T01:14:33.227746"
    },
    {
      "arxiv_id": "2503.20824v1",
      "title": "Exploiting Temporal State Space Sharing for Video Semantic Segmentation",
      "title_zh": "利用时序状态空间共享进行视频语义分割\n",
      "authors": [
        "Syed Ariff Syed Hesham",
        "Yun Liu",
        "Guolei Sun",
        "Henghui Ding",
        "Jing Yang",
        "Ender Konukoglu",
        "Xue Geng",
        "Xudong Jiang"
      ],
      "abstract": "Video semantic segmentation (VSS) plays a vital role in understanding the\ntemporal evolution of scenes. Traditional methods often segment videos\nframe-by-frame or in a short temporal window, leading to limited temporal\ncontext, redundant computations, and heavy memory requirements. To this end, we\nintroduce a Temporal Video State Space Sharing (TV3S) architecture to leverage\nMamba state space models for temporal feature sharing. Our model features a\nselective gating mechanism that efficiently propagates relevant information\nacross video frames, eliminating the need for a memory-heavy feature pool. By\nprocessing spatial patches independently and incorporating shifted operation,\nTV3S supports highly parallel computation in both training and inference\nstages, which reduces the delay in sequential state space processing and\nimproves the scalability for long video sequences. Moreover, TV3S incorporates\ninformation from prior frames during inference, achieving long-range temporal\ncoherence and superior adaptability to extended sequences. Evaluations on the\nVSPW and Cityscapes datasets reveal that our approach outperforms current\nstate-of-the-art methods, establishing a new standard for VSS with consistent\nresults across long video sequences. By achieving a good balance between\naccuracy and efficiency, TV3S shows a significant advancement in spatiotemporal\nmodeling, paving the way for efficient video analysis. The code is publicly\navailable at https://github.com/Ashesham/TV3S.git.",
      "tldr_zh": "该论文提出了一种时序视频状态空间共享(TV3S)架构，利用Mamba状态空间模型进行时序特征共享，旨在解决视频语义分割(VSS)中时序上下文有限、计算冗余和内存需求高等问题。TV3S采用选择性门控机制有效传递视频帧之间的相关信息，无需占用大量内存的特征池。通过独立处理空间patches和结合shifted操作，TV3S支持训练和推理阶段的高度并行计算，减少了序列状态空间处理的延迟，提高了长视频序列的可扩展性。实验结果表明，TV3S在VSPW和Cityscapes数据集上优于当前最先进的方法，为VSS建立了新的标准，并在长视频序列中取得了稳定一致的结果。TV3S在准确性和效率之间取得了良好的平衡，展示了时空建模的显著进步。\n",
      "categories": [
        "eess.IV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "eess.IV",
      "comment": "IEEE/CVF Conference on Computer Vision and Pattern Recognition 2025",
      "pdf_url": "http://arxiv.org/pdf/2503.20824v1",
      "published_date": "2025-03-26 01:47:42 UTC",
      "updated_date": "2025-03-26 01:47:42 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-29T01:14:45.542471"
    },
    {
      "arxiv_id": "2503.20139v1",
      "title": "Look Before Leap: Look-Ahead Planning with Uncertainty in Reinforcement Learning",
      "title_zh": "三思而后行：强化学习中基于不确定性的前瞻规划\n",
      "authors": [
        "Yongshuai Liu",
        "Xin Liu"
      ],
      "abstract": "Model-based reinforcement learning (MBRL) has demonstrated superior sample\nefficiency compared to model-free reinforcement learning (MFRL). However, the\npresence of inaccurate models can introduce biases during policy learning,\nresulting in misleading trajectories. The challenge lies in obtaining accurate\nmodels due to limited diverse training data, particularly in regions with\nlimited visits (uncertain regions). Existing approaches passively quantify\nuncertainty after sample generation, failing to actively collect uncertain\nsamples that could enhance state coverage and improve model accuracy. Moreover,\nMBRL often faces difficulties in making accurate multi-step predictions,\nthereby impacting overall performance. To address these limitations, we propose\na novel framework for uncertainty-aware policy optimization with model-based\nexploratory planning. In the model-based planning phase, we introduce an\nuncertainty-aware k-step lookahead planning approach to guide action selection\nat each step. This process involves a trade-off analysis between model\nuncertainty and value function approximation error, effectively enhancing\npolicy performance. In the policy optimization phase, we leverage an\nuncertainty-driven exploratory policy to actively collect diverse training\nsamples, resulting in improved model accuracy and overall performance of the RL\nagent. Our approach offers flexibility and applicability to tasks with varying\nstate/action spaces and reward structures. We validate its effectiveness\nthrough experiments on challenging robotic manipulation tasks and Atari games,\nsurpassing state-of-the-art methods with fewer interactions, thereby leading to\nsignificant performance improvements.",
      "tldr_zh": "该论文提出了一种基于不确定性的前瞻规划强化学习框架，旨在解决模型不准确导致的策略学习偏差问题。该框架通过不确定性感知的k步前瞻规划指导动作选择，权衡模型不确定性和价值函数近似误差。同时，利用不确定性驱动的探索策略主动收集多样化的训练样本，提高模型精度。实验结果表明，该方法在机器人操作任务和Atari游戏中优于现有方法，能够以更少的交互次数获得显著的性能提升。\n",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.20139v1",
      "published_date": "2025-03-26 01:07:35 UTC",
      "updated_date": "2025-03-26 01:07:35 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-29T01:14:57.080111"
    },
    {
      "arxiv_id": "2503.20138v1",
      "title": "Unlocking the Value of Decentralized Data: A Federated Dual Learning Approach for Model Aggregation",
      "title_zh": "释放去中心化数据的价值：一种用于模型聚合的联邦对偶学习方法\n",
      "authors": [
        "Junyi Zhu",
        "Ruicong Yao",
        "Taha Ceritli",
        "Savas Ozkan",
        "Matthew B. Blaschko",
        "Eunchung Noh",
        "Jeongwon Min",
        "Cho Jung Min",
        "Mete Ozay"
      ],
      "abstract": "Artificial Intelligence (AI) technologies have revolutionized numerous\nfields, yet their applications often rely on costly and time-consuming data\ncollection processes. Federated Learning (FL) offers a promising alternative by\nenabling AI models to be trained on decentralized data where data is scattered\nacross clients (distributed nodes). However, existing FL approaches struggle to\nmatch the performance of centralized training due to challenges such as\nheterogeneous data distribution and communication delays, limiting their\npotential for breakthroughs. We observe that many real-world use cases involve\nhybrid data regimes, in which a server (center node) has access to some data\nwhile a large amount of data is distributed across associated clients. To\nimprove the utilization of decentralized data under this regime, address data\nheterogeneity issue, and facilitate asynchronous communication between the\nserver and clients, we propose a dual learning approach that leverages\ncentralized data at the server to guide the merging of model updates from\nclients. Our method accommodates scenarios where server data is out-of-domain\nrelative to decentralized client data, making it applicable to a wide range of\nuse cases. We provide theoretical analysis demonstrating the faster convergence\nof our method compared to existing methods. Furthermore, experimental results\nacross various scenarios show that our approach significantly outperforms\nexisting technologies, highlighting its potential to unlock the value of large\namounts of decentralized data.",
      "tldr_zh": "该论文提出了一种联邦双重学习(Federated Dual Learning)方法，旨在提升联邦学习在数据异构和通信延迟等挑战下的性能，更有效地利用去中心化数据。该方法利用服务器端（中心节点）的集中数据来指导客户端模型更新的合并，即使服务器端数据与客户端数据属于不同领域也能适用。理论分析表明，该方法比现有方法收敛速度更快。实验结果表明，该方法在各种场景下均优于现有技术，突显了其释放大量去中心化数据价值的潜力。\n",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.20138v1",
      "published_date": "2025-03-26 01:00:35 UTC",
      "updated_date": "2025-03-26 01:00:35 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-29T01:15:09.277458"
    },
    {
      "arxiv_id": "2503.20822v1",
      "title": "Synthetic Video Enhances Physical Fidelity in Video Synthesis",
      "title_zh": "合成视频增强视频合成中的物理保真度\n",
      "authors": [
        "Qi Zhao",
        "Xingyu Ni",
        "Ziyu Wang",
        "Feng Cheng",
        "Ziyan Yang",
        "Lu Jiang",
        "Bohan Wang"
      ],
      "abstract": "We investigate how to enhance the physical fidelity of video generation\nmodels by leveraging synthetic videos derived from computer graphics pipelines.\nThese rendered videos respect real-world physics, such as maintaining 3D\nconsistency, and serve as a valuable resource that can potentially improve\nvideo generation models. To harness this potential, we propose a solution that\ncurates and integrates synthetic data while introducing a method to transfer\nits physical realism to the model, significantly reducing unwanted artifacts.\nThrough experiments on three representative tasks emphasizing physical\nconsistency, we demonstrate its efficacy in enhancing physical fidelity. While\nour model still lacks a deep understanding of physics, our work offers one of\nthe first empirical demonstrations that synthetic video enhances physical\nfidelity in video synthesis. Website: https://kevinz8866.github.io/simulation/",
      "tldr_zh": "该研究探索了如何利用计算机图形学流水线生成的合成视频来提升视频生成模型中的物理真实性。通过引入一种数据筛选和集成方法，以及一种将物理真实感迁移到模型的技术，显著减少了不必要的伪影。在三个强调物理一致性的代表性任务上的实验表明，该方法能够有效提高视频生成模型的物理保真度。该研究初步验证了合成视频在增强视频合成物理真实性方面的潜力。\n",
      "categories": [
        "eess.IV",
        "cs.AI",
        "cs.GR"
      ],
      "primary_category": "eess.IV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.20822v1",
      "published_date": "2025-03-26 00:45:07 UTC",
      "updated_date": "2025-03-26 00:45:07 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-29T01:15:21.222823"
    },
    {
      "arxiv_id": "2503.20126v1",
      "title": "Can We Make Code Green? Understanding Trade-Offs in LLMs vs. Human Code Optimizations",
      "title_zh": "我们能让代码变绿吗？理解大语言模型与人类代码优化中的权衡\n",
      "authors": [
        "Pooja Rani",
        "Jan-Andrea Bard",
        "June Sallou",
        "Alexander Boll",
        "Timo Kehrer",
        "Alberto Bacchelli"
      ],
      "abstract": "The rapid technological evolution has accelerated software development for\nvarious domains and use cases, contributing to a growing share of global carbon\nemissions. While recent large language models (LLMs) claim to assist developers\nin optimizing code for performance and energy efficiency, their efficacy in\nreal-world scenarios remains under exploration. In this work, we explore the\neffectiveness of LLMs in reducing the environmental footprint of real-world\nprojects, focusing on software written in Matlab-widely used in both academia\nand industry for scientific and engineering applications. We analyze\nenergy-focused optimization on 400 scripts across 100 top GitHub repositories.\nWe examine potential 2,176 optimizations recommended by leading LLMs, such as\nGPT-3, GPT-4, Llama, and Mixtral, and a senior Matlab developer, on energy\nconsumption, memory usage, execution time consumption, and code correctness.\nThe developer serves as a real-world baseline for comparing typical human and\nLLM-generated optimizations.\n  Mapping these optimizations to 13 high-level themes, we found that LLMs\npropose a broad spectrum of improvements--beyond energy efficiency--including\nimproving code readability and maintainability, memory management, error\nhandling while the developer overlooked some parallel processing, error\nhandling etc. However, our statistical tests reveal that the energy-focused\noptimizations unexpectedly negatively impacted memory usage, with no clear\nbenefits regarding execution time or energy consumption. Our qualitative\nanalysis of energy-time trade-offs revealed that some themes, such as\nvectorization preallocation, were among the common themes shaping these\ntrade-offs. With LLMs becoming ubiquitous in modern software development, our\nstudy serves as a call to action: prioritizing the evaluation of common coding\npractices to identify the green ones.",
      "tldr_zh": "该研究探讨了大型语言模型(LLMs)在优化代码以降低环境足迹方面的有效性，着重分析了Matlab代码的优化。研究人员评估了GPT-3、GPT-4、Llama和Mixtral等LLMs以及一位资深Matlab开发者对GitHub上100个仓库中400个脚本提出的2176项优化建议，涉及能耗、内存使用、执行时间和代码正确性。结果表明，LLMs提出的优化方案范围广泛，但针对能源的优化反而负面影响了内存使用，且在执行时间和能耗方面没有明显改善。这项研究强调了在软件开发中评估常见编码实践以确定“绿色”方案的重要性。\n",
      "categories": [
        "cs.SE",
        "cs.AI",
        "cs.PF"
      ],
      "primary_category": "cs.SE",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.20126v1",
      "published_date": "2025-03-26 00:27:29 UTC",
      "updated_date": "2025-03-26 00:27:29 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-29T01:15:33.487145"
    },
    {
      "arxiv_id": "2503.20124v1",
      "title": "Synthesizing world models for bilevel planning",
      "title_zh": "为双层规划合成世界模型\n",
      "authors": [
        "Zergham Ahmed",
        "Joshua B. Tenenbaum",
        "Christopher J. Bates",
        "Samuel J. Gershman"
      ],
      "abstract": "Modern reinforcement learning (RL) systems have demonstrated remarkable\ncapabilities in complex environments, such as video games. However, they still\nfall short of achieving human-like sample efficiency and adaptability when\nlearning new domains. Theory-based reinforcement learning (TBRL) is an\nalgorithmic framework specifically designed to address this gap. Modeled on\ncognitive theories, TBRL leverages structured, causal world models - \"theories\"\n- as forward simulators for use in planning, generalization and exploration.\nAlthough current TBRL systems provide compelling explanations of how humans\nlearn to play video games, they face several technical limitations: their\ntheory languages are restrictive, and their planning algorithms are not\nscalable. To address these challenges, we introduce TheoryCoder, an\ninstantiation of TBRL that exploits hierarchical representations of theories\nand efficient program synthesis methods for more powerful learning and\nplanning. TheoryCoder equips agents with general-purpose abstractions (e.g.,\n\"move to\"), which are then grounded in a particular environment by learning a\nlow-level transition model (a Python program synthesized from observations by a\nlarge language model). A bilevel planning algorithm can exploit this\nhierarchical structure to solve large domains. We demonstrate that this\napproach can be successfully applied to diverse and challenging grid-world\ngames, where approaches based on directly synthesizing a policy perform poorly.\nAblation studies demonstrate the benefits of using hierarchical abstractions.",
      "tldr_zh": "该论文提出了一种新的理论驱动强化学习(TBRL)系统TheoryCoder，旨在提升强化学习在学习新领域时的样本效率和适应性。TheoryCoder通过层级化的理论表示和高效的程序合成方法，利用结构化的因果世界模型（“理论”）作为前向模拟器进行规划、泛化和探索。该系统通过学习低级转换模型（由大型语言模型合成的Python程序）将通用抽象概念（例如“移动到”）应用于特定环境。一种双层规划算法利用这种层级结构来解决大型领域问题。实验表明，该方法成功应用于各种具有挑战性的网格世界游戏中，并优于直接合成策略的方法。消融研究也证明了使用层级抽象的优势。\n",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "25 pages",
      "pdf_url": "http://arxiv.org/pdf/2503.20124v1",
      "published_date": "2025-03-26 00:10:01 UTC",
      "updated_date": "2025-03-26 00:10:01 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-29T01:15:45.543289"
    }
  ],
  "raw_papers_fetched": true,
  "papers_count": 82,
  "processed_papers_count": 82,
  "failed_papers_count": 0,
  "summary_generated": true,
  "daily_data_saved": true,
  "last_update": "2025-03-29T01:16:51.686876"
}
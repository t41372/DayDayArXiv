{
  "date": "2025-03-26",
  "category": "cs.AI",
  "summary": "欢迎来到 UTC 时间 2025-03-26 的 arXiv 中文 TLDR 快报！\n\n今天 arXiv 的论文主要聚焦于 AI 模型的安全性、强化学习应用、多模态生成技术，以及 LLM 在医疗和决策领域的潜在风险与创新。其中，LLM 在医疗查询中的易受影响分析（第1篇）和新型工具接口设计框架（第2篇）最为令人印象深刻，前者揭示了模型对用户输入的脆弱性，后者由知名学者主导，展示了高效的代理框架；这些论文强调了 AI 的实际应用挑战和改进方向。\n\n下面，我将挑选并简要讨论部分关键论文，先从 AI 和 LLM 相关主题入手，再快速触及强化学习、图像生成等领域。限于篇幅，我会优先高话题度论文，并对其他内容简略概述。\n\n### AI 和 LLM 相关论文\n- **Susceptibility of Large Language Models to User-Driven Factors in Medical Queries（大型语言模型在医疗查询中的用户驱动因素易感性）**  \n  这篇论文探讨了 LLM（如 GPT-4o 和 LLaMA）在医疗场景中的可靠性问题。主要贡献是通过实验证明，模型对用户输入（如误导性表述或信息缺失）高度敏感，导致诊断准确率下降。发现中强调了使用完整临床细节和避免权威性语言的重要性，这对 AI 在医疗中的应用有重要启示。\n\n- **The Art of Tool Interface Design（工具接口设计的艺术）**  \n  作者包括知名学者如 Jian Yuan，该框架（Thinker）在复杂业务场景中实现了状态机增强生成（SMAG），显著提升了 LLM（如 GPT-4o）的推理性能。主要贡献是无需微调就提高了代理系统的成功率（e.g., 82.6%），并通过工具接口创新（如任务委托和自适应上下文管理）提升了效率，适用于客户服务等实际应用。\n\n- **Can Large Language Models Predict Associations Among Human Attitudes?（大型语言模型能否预测人类态度间的关联？）**  \n  这篇研究了 GPT-4o 在预测人类态度相关性时的能力。主要发现是，模型能捕捉态度间的潜在结构，即使在不相似主题间也有效，但表面相似性会提升准确率。这为 LLM 在社会心理分析中的应用提供了新视角。\n\n- **FinAudio: A Benchmark for Audio Large Language Models in Financial Applications（FinAudio: 针对金融应用的音频大型语言模型基准）**  \n  论文引入了 FinAudio 基准，用于评估音频 LLM 在金融场景（如会议语音）的性能。主要贡献是定义了 ASR 和摘要任务，并测试了多种模型，揭示了现有模型在金融领域的局限性。该基准将推动音频 AI 的改进。\n\n- **Patients Speak, AI Listens: LLM-based Analysis of Online Reviews Uncovers Key Drivers for Urgent Care Satisfaction（患者发声，AI聆听：基于LLM的在线评论分析揭示紧急护理满意度的关键驱动因素）**  \n  通过 LLM 分析在线评论，该研究发现人际因素和操作效率是患者满意度的主要驱动，而技术质量等因素影响较小。主要贡献是使用地理和 socioeconomic 数据提升分析精度，为医疗服务优化提供了数据驱动洞见。\n\n### 强化学习和机器人相关论文\n- **Competitive Multi-armed Bandit Games for Resource Sharing（竞争性多臂赌博机游戏用于资源共享）**  \n  这篇分析了多代理资源竞争问题，并提出了 CISP 机制来实现社会最优策略。主要发现是，自私代理可能导致无限效率损失，而新机制能减少 PoA（Price of Anarchy），适用于资源管理系统。\n\n- **LATTE-MV: Learning to Anticipate Table Tennis Hits from Monocular Videos（LATTE-MV: 从单目视频学习预测乒乓球击打）**  \n  论文开发了不确定性感知控制器，提高了乒乓球机器人对对手动作的预测精度（成功率从49.9%提升至59.0%）。主要贡献是结合 3D 重建和强化学习，提升了机器人敏捷性。\n\n其他论文，如图像生成和安全领域的（如第9、13、20篇），主要贡献了新方法（如扩散模型或后门防御），但由于篇幅，我仅快速提及：这些工作在提升模型鲁棒性和生成质量上取得进展，例如第20篇的 Unified Multimodal Discrete Diffusion 改进了多模态生成，但整体影响力不如上述 AI 主题突出。\n\n今天的 arXiv 更新丰富，但核心在于 AI 的可靠性和扩展应用。如果你对 LLM 在医疗或工具设计感兴趣，建议优先查看前几篇论文！",
  "papers": [
    {
      "arxiv_id": "2503.22746v1",
      "title": "Susceptibility of Large Language Models to User-Driven Factors in Medical Queries",
      "title_zh": "大语言模型在医疗查询中对用户驱动因素的易感性",
      "authors": [
        "Kyung Ho Lim",
        "Ujin Kang",
        "Xiang Li",
        "Jin Sung Kim",
        "Young-Chul Jung",
        "Sangjoon Park",
        "Byung-Hoon Kim"
      ],
      "abstract": "Large language models (LLMs) are increasingly used in healthcare, but their\nreliability is heavily influenced by user-driven factors such as question\nphrasing and the completeness of clinical information. In this study, we\nexamined how misinformation framing, source authority, model persona, and\nomission of key clinical details affect the diagnostic accuracy and reliability\nof LLM outputs. We conducted two experiments: one introducing misleading\nexternal opinions with varying assertiveness (perturbation test), and another\nremoving specific categories of patient information (ablation test). Using\npublic datasets (MedQA and Medbullets), we evaluated proprietary models\n(GPT-4o, Claude 3.5 Sonnet, Claude 3.5 Haiku, Gemini 1.5 Pro, Gemini 1.5 Flash)\nand open-source models (LLaMA 3 8B, LLaMA 3 Med42 8B, DeepSeek R1 8B). All\nmodels were vulnerable to user-driven misinformation, with proprietary models\nespecially affected by definitive and authoritative language. Assertive tone\nhad the greatest negative impact on accuracy. In the ablation test, omitting\nphysical exam findings and lab results caused the most significant performance\ndrop. Although proprietary models had higher baseline accuracy, their\nperformance declined sharply under misinformation. These results highlight the\nneed for well-structured prompts and complete clinical context. Users should\navoid authoritative framing of misinformation and provide full clinical\ndetails, especially for complex cases.",
      "tldr_zh": "这篇论文探讨了大型语言模型(LLMs)在医疗查询中的易感性，焦点在于用户驱动因素如misinformation framing、source authority、model persona和关键临床细节的遗漏对诊断准确性和可靠性的影响。研究通过perturbation test（引入误导性外部意见以测试断言性影响）和ablation test（移除患者信息类别，如体检发现和实验室结果）评估了多种模型，包括专有模型(GPT-4o等)和开源模型(LLaMA 3 8B等)。结果显示，所有模型均易受用户误导影响，专有模型对断言语气和权威语言特别敏感，导致准确率显著下降。论文强调，用户应采用结构化的提示并提供完整临床细节，以提升LLMs在医疗场景中的可靠性。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.CY"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.22746v1",
      "published_date": "2025-03-26 23:28:21 UTC",
      "updated_date": "2025-03-26 23:28:21 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T06:13:45.175811"
    },
    {
      "arxiv_id": "2503.21036v1",
      "title": "The Art of Tool Interface Design",
      "title_zh": "工具界面设计的艺术",
      "authors": [
        "Yunnan Wu",
        "Paul Chen",
        "Deshank Baranwal",
        "Jinlong Zhou",
        "Jian Yuan"
      ],
      "abstract": "We present an agentic framework, Thinker, which achieves state of art\nperformance in challenging reasoning tasks for realistic customer service\nscenarios that involve complex business logic and human interactions via long\nhorizons. On the $\\tau$-bench retail dataset, Thinker achieves 82.6\\% success\nrate with GPT-4o (version 2024-06-01) (baseline: 68.3\\%), and 81.9\\% success\nrate with Llama-3.1 405B (baseline: 49.6\\%), without any fine-tuning. Thinker\neffectively closes the gap in reasoning capabilities between the base models by\nintroducing proper structure.\n  The key features of the Thinker framework are: (1) State-Machine Augmented\nGeneration (SMAG), which represents business logic as state machines and the\nLLM uses state machines as tools. (2) Delegation of tasks from the main\nreasoning loop to LLM-powered tools. (3) Adaptive context management.\n  Our prompting-only solution achieves signficant gains, while still\nmaintaining a standard agentic architecture with a ReAct style reasoning loop.\nThe key is to innovate on the tool interface design, as exemplified by SMAG and\nthe LLM-powered tools.",
      "tldr_zh": "该研究提出 Thinker 框架，通过创新的工具接口设计，提升了大型语言模型（LLM）在复杂客户服务场景中的推理性能。框架的关键特征包括 State-Machine Augmented Generation (SMAG)，将业务逻辑表示为状态机并作为工具使用；任务从主推理循环委托给 LLM 驱动的工具；以及 Adaptive context management，以优化上下文处理。在 τ-bench retail 数据集上，Thinker 无需微调，使用 GPT-4o 达到 82.6% 成功率（基线 68.3%），使用 Llama-3.1 405B 达到 81.9% 成功率（基线 49.6%），显著缩小了基础模型的推理差距，同时保持 ReAct 风格的代理架构。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.21036v1",
      "published_date": "2025-03-26 23:02:00 UTC",
      "updated_date": "2025-03-26 23:02:00 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T06:13:56.353753"
    },
    {
      "arxiv_id": "2503.21011v1",
      "title": "Can Large Language Models Predict Associations Among Human Attitudes?",
      "title_zh": "大型语言模型能否预测人类态度之间的关联？",
      "authors": [
        "Ana Ma",
        "Derek Powell"
      ],
      "abstract": "Prior work has shown that large language models (LLMs) can predict human\nattitudes based on other attitudes, but this work has largely focused on\npredictions from highly similar and interrelated attitudes. In contrast, human\nattitudes are often strongly associated even across disparate and dissimilar\ntopics. Using a novel dataset of human responses toward diverse attitude\nstatements, we found that a frontier language model (GPT-4o) was able to\nrecreate the pairwise correlations among individual attitudes and to predict\nindividuals' attitudes from one another. Crucially, in an advance over prior\nwork, we tested GPT-4o's ability to predict in the absence of\nsurface-similarity between attitudes, finding that while surface similarity\nimproves prediction accuracy, the model was still highly-capable of generating\nmeaningful social inferences between dissimilar attitudes. Altogether, our\nfindings indicate that LLMs capture crucial aspects of the deeper, latent\nstructure of human belief systems.",
      "tldr_zh": "本研究探讨了大语言模型 (LLMs) 是否能预测人类态度之间的关联，超越先前专注于相似态度的研究。使用一个新数据集，研究者测试了 GPT-4o 的能力，发现它能准确重现个体态度间的成对相关性，并从一个态度预测另一个。关键发现是，即使在态度表面不相似的情况下，GPT-4o 仍能进行有意义的社交推断，尽管相似性会提升预测准确性。总体而言，这表明 LLMs 捕捉了人类信念系统更深层的潜在结构。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.21011v1",
      "published_date": "2025-03-26 21:58:43 UTC",
      "updated_date": "2025-03-26 21:58:43 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T06:14:06.897513"
    },
    {
      "arxiv_id": "2503.21000v1",
      "title": "Improving User Behavior Prediction: Leveraging Annotator Metadata in Supervised Machine Learning Models",
      "title_zh": "翻译失败",
      "authors": [
        "Lynnette Hui Xian Ng",
        "Kokil Jaidka",
        "Kaiyuan Tay",
        "Hansin Ahuja",
        "Niyati Chhaya"
      ],
      "abstract": "Supervised machine-learning models often underperform in predicting user\nbehaviors from conversational text, hindered by poor crowdsourced label quality\nand low NLP task accuracy. We introduce the Metadata-Sensitive\nWeighted-Encoding Ensemble Model (MSWEEM), which integrates annotator\nmeta-features like fatigue and speeding. First, our results show MSWEEM\noutperforms standard ensembles by 14\\% on held-out data and 12\\% on an\nalternative dataset. Second, we find that incorporating signals of annotator\nbehavior, such as speed and fatigue, significantly boosts model performance.\nThird, we find that annotators with higher qualifications, such as Master's,\ndeliver more consistent and faster annotations. Given the increasing\nuncertainty over annotation quality, our experiments show that understanding\nannotator patterns is crucial for enhancing model accuracy in user behavior\nprediction.",
      "tldr_zh": "该研究探讨了监督机器学习模型在预测用户行为时面临的挑战，如众包标签质量低和NLP任务准确率不足，并引入了MSWEEM（Metadata-Sensitive Weighted-Encoding Ensemble Model），通过整合标注者元数据（如疲劳和speeding）来提升模型性能。实验结果显示，MSWEEM在保留数据上比标准集成模型提高了14%的表现，在备选数据集上提高了12%。此外，研究发现，标注者行为信号（如速度和疲劳）显著提升了预测准确性，而具有更高资质的标注者（如持有Master's学位）能提供更一致且更快的标注，从而强调了理解标注者模式对改进用户行为预测的重要性。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "Accepted at CSCW 2025",
      "pdf_url": "http://arxiv.org/pdf/2503.21000v1",
      "published_date": "2025-03-26 21:30:48 UTC",
      "updated_date": "2025-03-26 21:30:48 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T06:14:20.267247"
    },
    {
      "arxiv_id": "2503.20990v1",
      "title": "FinAudio: A Benchmark for Audio Large Language Models in Financial Applications",
      "title_zh": "翻译失败",
      "authors": [
        "Yupeng Cao",
        "Haohang Li",
        "Yangyang Yu",
        "Shashidhar Reddy Javaji",
        "Yueru He",
        "Jimin Huang",
        "Zining Zhu",
        "Qianqian Xie",
        "Xiao-yang Liu",
        "Koduvayur Subbalakshmi",
        "Meikang Qiu",
        "Sophia Ananiadou",
        "Jian-Yun Nie"
      ],
      "abstract": "Audio Large Language Models (AudioLLMs) have received widespread attention\nand have significantly improved performance on audio tasks such as\nconversation, audio understanding, and automatic speech recognition (ASR).\nDespite these advancements, there is an absence of a benchmark for assessing\nAudioLLMs in financial scenarios, where audio data, such as earnings conference\ncalls and CEO speeches, are crucial resources for financial analysis and\ninvestment decisions. In this paper, we introduce \\textsc{FinAudio}, the first\nbenchmark designed to evaluate the capacity of AudioLLMs in the financial\ndomain. We first define three tasks based on the unique characteristics of the\nfinancial domain: 1) ASR for short financial audio, 2) ASR for long financial\naudio, and 3) summarization of long financial audio. Then, we curate two short\nand two long audio datasets, respectively, and develop a novel dataset for\nfinancial audio summarization, comprising the \\textsc{FinAudio} benchmark.\nThen, we evaluate seven prevalent AudioLLMs on \\textsc{FinAudio}. Our\nevaluation reveals the limitations of existing AudioLLMs in the financial\ndomain and offers insights for improving AudioLLMs. All datasets and codes will\nbe released.",
      "tldr_zh": "本研究引入了FinAudio，这是首个针对Audio Large Language Models (AudioLLMs) 在金融应用中的基准测试，旨在评估这些模型处理金融音频数据的能力，如收益电话会议和CEO演讲。论文定义了三个特定任务：短金融音频的ASR、长金融音频的ASR，以及长金融音频的总结，并构建了相应的短音频数据集、长音频数据集和一个新颖的金融音频总结数据集。研究评估了七个主流AudioLLMs，发现这些模型在金融领域存在显著局限性，并提供了改进建议。所有数据集和代码将公开发布，以推动相关研究。",
      "categories": [
        "cs.CE",
        "cs.AI",
        "cs.MM"
      ],
      "primary_category": "cs.CE",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.20990v1",
      "published_date": "2025-03-26 21:07:51 UTC",
      "updated_date": "2025-03-26 21:07:51 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T06:14:32.286044"
    },
    {
      "arxiv_id": "2503.20981v1",
      "title": "Patients Speak, AI Listens: LLM-based Analysis of Online Reviews Uncovers Key Drivers for Urgent Care Satisfaction",
      "title_zh": "翻译失败",
      "authors": [
        "Xiaoran Xu",
        "Zhaoqian Xue",
        "Chi Zhang",
        "Jhonatan Medri",
        "Junjie Xiong",
        "Jiayan Zhou",
        "Jin Jin",
        "Yongfeng Zhang",
        "Siyuan Ma",
        "Lingyao Li"
      ],
      "abstract": "Investigating the public experience of urgent care facilities is essential\nfor promoting community healthcare development. Traditional survey methods\noften fall short due to limited scope, time, and spatial coverage.\nCrowdsourcing through online reviews or social media offers a valuable approach\nto gaining such insights. With recent advancements in large language models\n(LLMs), extracting nuanced perceptions from reviews has become feasible. This\nstudy collects Google Maps reviews across the DMV and Florida areas and\nconducts prompt engineering with the GPT model to analyze the aspect-based\nsentiment of urgent care. We first analyze the geospatial patterns of various\naspects, including interpersonal factors, operational efficiency, technical\nquality, finances, and facilities. Next, we determine Census Block\nGroup(CBG)-level characteristics underpinning differences in public perception,\nincluding population density, median income, GINI Index, rent-to-income ratio,\nhousehold below poverty rate, no insurance rate, and unemployment rate. Our\nresults show that interpersonal factors and operational efficiency emerge as\nthe strongest determinants of patient satisfaction in urgent care, while\ntechnical quality, finances, and facilities show no significant independent\neffects when adjusted for in multivariate models. Among socioeconomic and\ndemographic factors, only population density demonstrates a significant but\nmodest association with patient ratings, while the remaining factors exhibit no\nsignificant correlations. Overall, this study highlights the potential of\ncrowdsourcing to uncover the key factors that matter to residents and provide\nvaluable insights for stakeholders to improve public satisfaction with urgent\ncare.",
      "tldr_zh": "这篇论文利用大型语言模型 (LLMs) 和提示工程分析 Google Maps 上急诊护理设施的在线评论，以揭示公众满意度的关键驱动因素。研究收集了 DMV 和 Florida 地区的评论，并评估了方面-based sentiment，包括人际因素、操作效率、技术质量、财务和设施，同时考察了 Census Block Group (CBG) 级别的社会经济指标，如人口密度、收入中位数和贫困率。结果显示，人际因素和操作效率是患者满意度的主要决定因素，而技术质量、财务和设施在多变量模型中无显著独立影响。总体而言，该研究突显了众包方法结合 LLMs 的潜力，为利益相关者提供改进急诊护理服务的宝贵见解。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.SI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.20981v1",
      "published_date": "2025-03-26 20:45:01 UTC",
      "updated_date": "2025-03-26 20:45:01 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T06:14:44.761216"
    },
    {
      "arxiv_id": "2503.20975v1",
      "title": "Competitive Multi-armed Bandit Games for Resource Sharing",
      "title_zh": "用于资源",
      "authors": [
        "Hongbo Li",
        "Lingjie Duan"
      ],
      "abstract": "In modern resource-sharing systems, multiple agents access limited resources\nwith unknown stochastic conditions to perform tasks. When multiple agents\naccess the same resource (arm) simultaneously, they compete for successful\nusage, leading to contention and reduced rewards. This motivates our study of\ncompetitive multi-armed bandit (CMAB) games. In this paper, we study a new\nN-player K-arm competitive MAB game, where non-myopic players (agents) compete\nwith each other to form diverse private estimations of unknown arms over time.\nTheir possible collisions on same arms and time-varying nature of arm rewards\nmake the policy analysis more involved than existing studies for myopic\nplayers. We explicitly analyze the threshold-based structures of social optimum\nand existing selfish policy, showing that the latter causes prolonged\nconvergence time $\\Omega(\\frac{K}{\\eta^2}\\ln({\\frac{KN}{\\delta}}))$, while\nsocially optimal policy with coordinated communication reduces it to\n$\\mathcal{O}(\\frac{K}{N\\eta^2}\\ln{(\\frac{K}{\\delta})})$. Based on the\ncomparison, we prove that the competition among selfish players for the best\narm can result in an infinite price of anarchy (PoA), indicating an arbitrarily\nlarge efficiency loss compared to social optimum. We further prove that no\ninformational (non-monetary) mechanism (including Bayesian persuasion) can\nreduce the infinite PoA, as the strategic misreporting by non-myopic players\nundermines such approaches. To address this, we propose a Combined\nInformational and Side-Payment (CISP) mechanism, which provides socially\noptimal arm recommendations with proper informational and monetary incentives\nto players according to their time-varying private beliefs. Our CISP mechanism\nkeeps ex-post budget balanced for social planner and ensures truthful reporting\nfrom players, achieving the minimum PoA=1 and same convergence time as social\noptimum.",
      "tldr_zh": "本研究探讨了 Competitive Multi-armed Bandit (CMAB) 游戏在资源共享系统中的应用，针对多个非短视代理人（players）竞争未知随机资源的场景，导致碰撞和奖励减少的问题。通过分析，论文证明了自私策略会延长收敛时间至 $\\Omega(\\frac{K}{\\eta^2}\\ln({\\frac{KN}{\\delta}}))$，并导致无限的价格无政府状态 (PoA)，造成效率损失。相比之下，社会最优策略通过协调通信可将收敛时间缩短至 $\\mathcal{O}(\\frac{K}{N\\eta^2}\\ln{(\\frac{K}{\\delta})})$。为了解决这一问题，作者提出 Combined Informational and Side-Payment (CISP) 机制，提供信息和货币激励，确保代理人真实报告，实现 PoA=1 和与社会最优相同的收敛时间，从而提升资源共享效率。",
      "categories": [
        "cs.GT",
        "cs.AI"
      ],
      "primary_category": "cs.GT",
      "comment": "This paper has been accepted by IEEE TMC",
      "pdf_url": "http://arxiv.org/pdf/2503.20975v1",
      "published_date": "2025-03-26 20:35:18 UTC",
      "updated_date": "2025-03-26 20:35:18 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T06:14:56.092115"
    },
    {
      "arxiv_id": "2503.20959v1",
      "title": "Sociotechnical Effects of Machine Translation",
      "title_zh": "机器翻译的社会技术影响",
      "authors": [
        "Joss Moorkens",
        "Andy Way",
        "Séamus Lankford"
      ],
      "abstract": "While the previous chapters have shown how machine translation (MT) can be\nuseful, in this chapter we discuss some of the side-effects and risks that are\nassociated, and how they might be mitigated. With the move to neural MT and\napproaches using Large Language Models (LLMs), there is an associated impact on\nclimate change, as the models built by multinational corporations are massive.\nThey are hugely expensive to train, consume large amounts of electricity, and\noutput huge volumes of kgCO2 to boot. However, smaller models which still\nperform to a high level of quality can be built with much lower carbon\nfootprints, and tuning pre-trained models saves on the requirement to train\nfrom scratch. We also discuss the possible detrimental effects of MT on\ntranslators and other users. The topics of copyright and ownership of data are\ndiscussed, as well as ethical considerations on data and MT use. Finally, we\nshow how if done properly, using MT in crisis scenarios can save lives, and we\nprovide a method of how this might be done.",
      "tldr_zh": "本论文探讨了机器翻译（MT）的社会技术影响，包括其负面副作用和风险缓解策略。论文指出，神经 MT 和 Large Language Models (LLMs) 的使用会导致巨额碳排放（如 kgCO2 输出），加剧气候变化问题，但可以通过采用更小模型或微调预训练模型来降低碳足迹。此外，论文讨论了 MT 对翻译人员和用户的潜在危害，如版权争议和数据伦理问题，并强调在危机场景中正确应用 MT 可以挽救生命，并提供了相应的实施方法。总的来说，该研究为平衡 MT 的益处与风险提供了宝贵见解。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.20959v1",
      "published_date": "2025-03-26 19:49:46 UTC",
      "updated_date": "2025-03-26 19:49:46 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T06:15:07.282046"
    },
    {
      "arxiv_id": "2503.20952v1",
      "title": "TS-Inverse: A Gradient Inversion Attack Tailored for Federated Time Series Forecasting Models",
      "title_zh": "TS-Inverse：针对联邦时间序列预测模型的梯度反演攻击",
      "authors": [
        "Caspar Meijer",
        "Jiyue Huang",
        "Shreshtha Sharma",
        "Elena Lazovik",
        "Lydia Y. Chen"
      ],
      "abstract": "Federated learning (FL) for time series forecasting (TSF) enables clients\nwith privacy-sensitive time series (TS) data to collaboratively learn accurate\nforecasting models, for example, in energy load prediction. Unfortunately,\nprivacy risks in FL persist, as servers can potentially reconstruct clients'\ntraining data through gradient inversion attacks (GIA). Although GIA is\ndemonstrated for image classification tasks, little is known about time series\nregression tasks. In this paper, we first conduct an extensive empirical study\non inverting TS data across 4 TSF models and 4 datasets, identifying the unique\nchallenges of reconstructing both observations and targets of TS data. We then\npropose TS-Inverse, a novel GIA that improves the inversion of TS data by (i)\nlearning a gradient inversion model that outputs quantile predictions, (ii) a\nunique loss function that incorporates periodicity and trend regularization,\nand (iii) regularization according to the quantile predictions. Our evaluations\ndemonstrate a remarkable performance of TS-Inverse, achieving at least a 2x-10x\nimprovement in terms of the sMAPE metric over existing GIA methods on TS data.\nCode repository: https://github.com/Capsar/ts-inverse",
      "tldr_zh": "本文研究了Federated Learning (FL) 在时间序列预测 (TSF) 中的隐私风险，特别关注Gradient Inversion Attacks (GIA) 重建客户端TS数据的挑战，通过对4个TSF模型和4个数据集的实证研究，识别了TS数据观察值和目标的重建难题。作者提出TS-Inverse，一种新型GIA方法，通过学习输出分位数预测的模型、设计包含周期性和趋势正则化的独特损失函数，以及基于分位数预测的正则化，显著提升了重建准确性。实验结果显示，TS-Inverse 在sMAPE指标上比现有GIA方法提高了2倍到10倍，为FL隐私保护提供了重要洞见。代码仓库：https://github.com/Capsar/ts-inverse。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.20952v1",
      "published_date": "2025-03-26 19:35:49 UTC",
      "updated_date": "2025-03-26 19:35:49 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T06:15:20.951805"
    },
    {
      "arxiv_id": "2503.20950v1",
      "title": "DEMENTIA-PLAN: An Agent-Based Framework for Multi-Knowledge Graph Retrieval-Augmented Generation in Dementia Care",
      "title_zh": "翻译失败",
      "authors": [
        "Yutong Song",
        "Chenhan Lyu",
        "Pengfei Zhang",
        "Sabine Brunswicker",
        "Nikil Dutt",
        "Amir Rahmani"
      ],
      "abstract": "Mild-stage dementia patients primarily experience two critical symptoms:\nsevere memory loss and emotional instability. To address these challenges, we\npropose DEMENTIA-PLAN, an innovative retrieval-augmented generation framework\nthat leverages large language models to enhance conversational support. Our\nmodel employs a multiple knowledge graph architecture, integrating various\ndimensional knowledge representations including daily routine graphs and life\nmemory graphs. Through this multi-graph architecture, DEMENTIA-PLAN\ncomprehensively addresses both immediate care needs and facilitates deeper\nemotional resonance through personal memories, helping stabilize patient mood\nwhile providing reliable memory support. Our notable innovation is the\nself-reflection planning agent, which systematically coordinates knowledge\nretrieval and semantic integration across multiple knowledge graphs, while\nscoring retrieved content from daily routine and life memory graphs to\ndynamically adjust their retrieval weights for optimized response generation.\nDEMENTIA-PLAN represents a significant advancement in the clinical application\nof large language models for dementia care, bridging the gap between AI tools\nand caregivers interventions.",
      "tldr_zh": "本文提出 DEMENTIA-PLAN 框架，这是一个基于代理的检索增强生成（Retrieval-Augmented Generation）系统，旨在通过多知识图谱架构（如日常例行图和生活记忆图）为轻度痴呆患者提供对话支持，解决记忆丧失和情绪不稳定问题。框架的核心创新是自反省规划代理（self-reflection planning agent），它协调多图谱知识检索、语义整合，并动态调整检索权重以优化响应生成，从而实现即时护理和情感共鸣。总体上，该框架推进了大型语言模型在痴呆护理中的临床应用，桥接了 AI 工具与护理干预的差距。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "Accepted by AAAI 2025 Workshop on Knowledge Graphs for Personalized\n  Public Health",
      "pdf_url": "http://arxiv.org/pdf/2503.20950v1",
      "published_date": "2025-03-26 19:34:04 UTC",
      "updated_date": "2025-03-26 19:34:04 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T06:15:32.194665"
    },
    {
      "arxiv_id": "2503.22742v2",
      "title": "Adaptive Integrated Layered Attention (AILA)",
      "title_zh": "自适应集成分层注意力 (AILA)",
      "authors": [
        "William Claster",
        "Suhas KM",
        "Dhairya Gundechia"
      ],
      "abstract": "We propose Adaptive Integrated Layered Attention (AILA), a neural network\narchitecture that combines dense skip connections with different mechanisms for\nadaptive feature reuse across network layers. We evaluate AILA on three\nchallenging tasks: price forecasting for various commodities and indices (S&P\n500, Gold, US dollar Futures, Coffee, Wheat), image recognition using the\nCIFAR-10 dataset, and sentiment analysis on the IMDB movie review dataset. In\nall cases, AILA matches strong deep learning baselines (LSTMs, Transformers,\nand ResNets), achieving it at a fraction of the training and inference time.\nNotably, we implement and test two versions of the model - AILA-Architecture 1,\nwhich uses simple linear layers as the connection mechanism between layers, and\nAILA-Architecture 2, which implements an attention mechanism to selectively\nfocus on outputs from previous layers. Both architectures are applied in a\nsingle-task learning setting, with each model trained separately for individual\ntasks. Results confirm that AILA's adaptive inter-layer connections yield\nrobust gains by flexibly reusing pertinent features at multiple network depths.\nThe AILA approach thus presents an extension to existing architectures,\nimproving long-range sequence modeling, image recognition with optimised\ncomputational speed, and SOTA classification performance in practice.",
      "tldr_zh": "本研究提出了一种神经网络架构 Adaptive Integrated Layered Attention (AILA)，它结合密集跳跃连接和适应性特征重用机制，实现层间信息的灵活重用。AILA 在价格预测（如 S&P 500、金等商品）、图像识别（CIFAR-10 数据集）和情感分析（IMDB 电影评论数据集）等任务上，与 LSTMs、Transformers 和 ResNets 等强基线模型性能相当，但训练和推理时间显著减少。实验结果显示，AILA 的两个变体（AILA-Architecture 1 使用线性层连接，AILA-Architecture 2 使用注意力机制）均通过适应性层间连接带来了稳健的性能提升，扩展了现有架构在长期序列建模和分类任务中的应用。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL",
        "cs.CV",
        "cs.IR",
        "cs.NE"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.22742v2",
      "published_date": "2025-03-26 19:32:31 UTC",
      "updated_date": "2025-05-12 21:58:10 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T06:15:45.490420"
    },
    {
      "arxiv_id": "2503.20936v1",
      "title": "LATTE-MV: Learning to Anticipate Table Tennis Hits from Monocular Videos",
      "title_zh": "翻译失败",
      "authors": [
        "Daniel Etaat",
        "Dvij Kalaria",
        "Nima Rahmanian",
        "Shankar Sastry"
      ],
      "abstract": "Physical agility is a necessary skill in competitive table tennis, but by no\nmeans sufficient. Champions excel in this fast-paced and highly dynamic\nenvironment by anticipating their opponent's intent - buying themselves the\nnecessary time to react. In this work, we take one step towards designing such\nan anticipatory agent. Previous works have developed systems capable of\nreal-time table tennis gameplay, though they often do not leverage\nanticipation. Among the works that forecast opponent actions, their approaches\nare limited by dataset size and variety. Our paper contributes (1) a scalable\nsystem for reconstructing monocular video of table tennis matches in 3D and (2)\nan uncertainty-aware controller that anticipates opponent actions. We\ndemonstrate in simulation that our policy improves the ball return rate against\nhigh-speed hits from 49.9% to 59.0% as compared to a baseline non-anticipatory\npolicy.",
      "tldr_zh": "本文提出LATTE-MV系统，旨在从单目视频（Monocular Videos）学习预测乒乓球击球，通过重建比赛的3D场景来提升代理的预判能力。系统的主要贡献包括一个可扩展的3D重建框架和一个不确定性aware控制器，能够预测对手动作并提高反应效率。在模拟实验中，该策略将回球率从基线非预测策略的49.9%提升至59.0%，展示了其在高速动态环境中的显著优势。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "CVPR 2025",
      "pdf_url": "http://arxiv.org/pdf/2503.20936v1",
      "published_date": "2025-03-26 19:11:22 UTC",
      "updated_date": "2025-03-26 19:11:22 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T06:15:55.500788"
    },
    {
      "arxiv_id": "2503.20925v1",
      "title": "Prototype Guided Backdoor Defense",
      "title_zh": "原型引导后门防御",
      "authors": [
        "Venkat Adithya Amula",
        "Sunayana Samavedam",
        "Saurabh Saini",
        "Avani Gupta",
        "Narayanan P J"
      ],
      "abstract": "Deep learning models are susceptible to {\\em backdoor attacks} involving\nmalicious attackers perturbing a small subset of training data with a {\\em\ntrigger} to causes misclassifications. Various triggers have been used,\nincluding semantic triggers that are easily realizable without requiring the\nattacker to manipulate the image. The emergence of generative AI has eased the\ngeneration of varied poisoned samples. Robustness across types of triggers is\ncrucial to effective defense. We propose Prototype Guided Backdoor Defense\n(PGBD), a robust post-hoc defense that scales across different trigger types,\nincluding previously unsolved semantic triggers. PGBD exploits displacements in\nthe geometric spaces of activations to penalize movements toward the trigger.\nThis is done using a novel sanitization loss of a post-hoc fine-tuning step.\nThe geometric approach scales easily to all types of attacks. PGBD achieves\nbetter performance across all settings. We also present the first defense\nagainst a new semantic attack on celebrity face images. Project page:\n\\hyperlink{https://venkatadithya9.github.io/pgbd.github.io/}{this https URL}.",
      "tldr_zh": "本文提出了一种原型引导后门防御（PGBD），作为一种鲁棒的后验防御方法，用于保护深度学习模型免受后门攻击（backdoor attacks），包括语义触发器（semantic triggers）和由生成式AI创建的多样化中毒样本。PGBD 通过利用激活几何空间中的位移，引入一个新的净化损失（sanitization loss）在后验微调步骤中惩罚向触发器的移动，从而实现对不同攻击类型的可扩展防御。实验结果表明，PGBD 在所有设置下均优于现有方法，并首次成功防御针对名人面部图像的新语义攻击。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.20925v1",
      "published_date": "2025-03-26 18:58:53 UTC",
      "updated_date": "2025-03-26 18:58:53 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T06:16:08.297929"
    },
    {
      "arxiv_id": "2503.22740v1",
      "title": "CSPO: Cross-Market Synergistic Stock Price Movement Forecasting with Pseudo-volatility Optimization",
      "title_zh": "翻译失败",
      "authors": [
        "Sida Lin",
        "Yankai Chen",
        "Yiyan Qi",
        "Chenhao Ma",
        "Bokai Cao",
        "Yifei Zhang",
        "Xue Liu",
        "Jian Guo"
      ],
      "abstract": "The stock market, as a cornerstone of the financial markets, places\nforecasting stock price movements at the forefront of challenges in\nquantitative finance. Emerging learning-based approaches have made significant\nprogress in capturing the intricate and ever-evolving data patterns of modern\nmarkets. With the rapid expansion of the stock market, it presents two\ncharacteristics, i.e., stock exogeneity and volatility heterogeneity, that\nheighten the complexity of price forecasting. Specifically, while stock\nexogeneity reflects the influence of external market factors on price\nmovements, volatility heterogeneity showcases the varying difficulty in\nmovement forecasting against price fluctuations. In this work, we introduce the\nframework of Cross-market Synergy with Pseudo-volatility Optimization (CSPO).\nSpecifically, CSPO implements an effective deep neural architecture to leverage\nexternal futures knowledge. This enriches stock embeddings with cross-market\ninsights and thus enhances the CSPO's predictive capability. Furthermore, CSPO\nincorporates pseudo-volatility to model stock-specific forecasting confidence,\nenabling a dynamic adaptation of its optimization process to improve accuracy\nand robustness. Our extensive experiments, encompassing industrial evaluation\nand public benchmarking, highlight CSPO's superior performance over existing\nmethods and effectiveness of all proposed modules contained therein.",
      "tldr_zh": "该研究针对股票价格预测的复杂性，提出CSPO框架，利用Cross-Market Synergistic方法整合外部期货知识来丰富股票嵌入，从而提升预测能力。具体而言，CSPO引入Pseudo-volatility Optimization来建模股票特定的预测置信度，实现优化过程的动态适应，提高准确性和鲁棒性。实验结果显示，该框架在工业评估和公共基准测试中优于现有方法，所有模块均表现出色。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.22740v1",
      "published_date": "2025-03-26 18:58:15 UTC",
      "updated_date": "2025-03-26 18:58:15 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T06:16:19.332942"
    },
    {
      "arxiv_id": "2503.20914v1",
      "title": "D4R -- Exploring and Querying Relational Graphs Using Natural Language and Large Language Models -- the Case of Historical Documents",
      "title_zh": "翻译失败",
      "authors": [
        "Michel Boeglin",
        "David Kahn",
        "Josiane Mothe",
        "Diego Ortiz",
        "David Panzoli"
      ],
      "abstract": "D4R is a digital platform designed to assist non-technical users,\nparticularly historians, in exploring textual documents through advanced\ngraphical tools for text analysis and knowledge extraction. By leveraging a\nlarge language model, D4R translates natural language questions into Cypher\nqueries, enabling the retrieval of data from a Neo4J database. A user-friendly\ngraphical interface allows for intuitive interaction, enabling users to\nnavigate and analyse complex relational data extracted from unstructured\ntextual documents. Originally designed to bridge the gap between AI\ntechnologies and historical research, D4R's capabilities extend to various\nother domains. A demonstration video and a live software demo are available.",
      "tldr_zh": "本研究提出D4R平台，使用Large Language Models将自然语言查询转化为Cypher查询，帮助非技术用户（如历史学家）从Neo4J数据库中检索和分析非结构化文本文档中的关系图。该平台提供用户友好的图形界面，支持直观交互和复杂数据的导航探索。最初设计用于桥接AI技术和历史研究，D4R的功能可扩展到其他领域，并附带演示视频和实时软件演示。",
      "categories": [
        "cs.IR",
        "cs.AI",
        "cs.CL",
        "cs.LG",
        "H.3; H.3.3; I.2.7"
      ],
      "primary_category": "cs.IR",
      "comment": "8 pages, 7 figures",
      "pdf_url": "http://arxiv.org/pdf/2503.20914v1",
      "published_date": "2025-03-26 18:41:42 UTC",
      "updated_date": "2025-03-26 18:41:42 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T06:16:31.627015"
    },
    {
      "arxiv_id": "2503.20903v1",
      "title": "Assessing Generative Models for Structured Data",
      "title_zh": "评估生成模型在结构化数据上的表现",
      "authors": [
        "Reilly Cannon",
        "Nicolette M. Laird",
        "Caesar Vazquez",
        "Andy Lin",
        "Amy Wagler",
        "Tony Chiang"
      ],
      "abstract": "Synthetic tabular data generation has emerged as a promising method to\naddress limited data availability and privacy concerns. With the sharp increase\nin the performance of large language models in recent years, researchers have\nbeen interested in applying these models to the generation of tabular data.\nHowever, little is known about the quality of the generated tabular data from\nlarge language models. The predominant method for assessing the quality of\nsynthetic tabular data is the train-synthetic-test-real approach, where the\nartificial examples are compared to the original by how well machine learning\nmodels, trained separately on the real and synthetic sets, perform in some\ndownstream tasks. This method does not directly measure how closely the\ndistribution of generated data approximates that of the original. This paper\nintroduces rigorous methods for directly assessing synthetic tabular data\nagainst real data by looking at inter-column dependencies within the data. We\nfind that large language models (GPT-2), both when queried via few-shot\nprompting and when fine-tuned, and GAN (CTGAN) models do not produce data with\ndependencies that mirror the original real data. Results from this study can\ninform future practice in synthetic data generation to improve data quality.",
      "tldr_zh": "这篇论文评估了生成模型在结构化数据（如表格数据）上的性能，焦点在于解决数据可用性和隐私问题。作者引入了直接评估合成数据的严格方法，通过分析数据内的 inter-column dependencies，而不是依赖传统的 train-synthetic-test-real 方式。研究发现，大型语言模型（如 GPT-2，通过 few-shot prompting 或 fine-tuning）和 GAN 模型（如 CTGAN）生成的合成数据，其依赖性未能准确 mirror 真实数据的分布。这些结果可指导未来的 synthetic data generation 实践，以提升数据质量。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.20903v1",
      "published_date": "2025-03-26 18:19:05 UTC",
      "updated_date": "2025-03-26 18:19:05 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T06:16:44.162227"
    },
    {
      "arxiv_id": "2503.20884v1",
      "title": "Robust Federated Learning Against Poisoning Attacks: A GAN-Based Defense Framework",
      "title_zh": "翻译失败",
      "authors": [
        "Usama Zafar",
        "André Teixeira",
        "Salman Toor"
      ],
      "abstract": "Federated Learning (FL) enables collaborative model training across\ndecentralized devices without sharing raw data, but it remains vulnerable to\npoisoning attacks that compromise model integrity. Existing defenses often rely\non external datasets or predefined heuristics (e.g. number of malicious\nclients), limiting their effectiveness and scalability. To address these\nlimitations, we propose a privacy-preserving defense framework that leverages a\nConditional Generative Adversarial Network (cGAN) to generate synthetic data at\nthe server for authenticating client updates, eliminating the need for external\ndatasets. Our framework is scalable, adaptive, and seamlessly integrates into\nFL workflows. Extensive experiments on benchmark datasets demonstrate its\nrobust performance against a variety of poisoning attacks, achieving high True\nPositive Rate (TPR) and True Negative Rate (TNR) of malicious and benign\nclients, respectively, while maintaining model accuracy. The proposed framework\noffers a practical and effective solution for securing federated learning\nsystems.",
      "tldr_zh": "该论文提出了一种基于 Conditional Generative Adversarial Network (cGAN) 的隐私保护防御框架，用于抵抗 Federated Learning (FL) 中的 poisoning attacks，避免依赖外部数据集或预定义启发式规则。该框架在服务器端生成合成数据来验证客户端更新，确保系统可扩展、适应性强，并无缝整合到 FL 工作流中。实验在基准数据集上证明，该框架对多种 poisoning attacks 表现出色，实现了高 True Positive Rate (TPR) 和 True Negative Rate (TNR)，同时保持了模型准确性，为保护联邦学习系统提供了实用解决方案。",
      "categories": [
        "cs.CR",
        "cs.AI",
        "cs.DC"
      ],
      "primary_category": "cs.CR",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.20884v1",
      "published_date": "2025-03-26 18:00:56 UTC",
      "updated_date": "2025-03-26 18:00:56 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T06:16:55.871088"
    },
    {
      "arxiv_id": "2503.20871v3",
      "title": "VinaBench: Benchmark for Faithful and Consistent Visual Narratives",
      "title_zh": "翻译失败",
      "authors": [
        "Silin Gao",
        "Sheryl Mathew",
        "Li Mi",
        "Sepideh Mamooler",
        "Mengjie Zhao",
        "Hiromi Wakaki",
        "Yuki Mitsufuji",
        "Syrielle Montariol",
        "Antoine Bosselut"
      ],
      "abstract": "Visual narrative generation transforms textual narratives into sequences of\nimages illustrating the content of the text. However, generating visual\nnarratives that are faithful to the input text and self-consistent across\ngenerated images remains an open challenge, due to the lack of knowledge\nconstraints used for planning the stories. In this work, we propose a new\nbenchmark, VinaBench, to address this challenge. Our benchmark annotates the\nunderlying commonsense and discourse constraints in visual narrative samples,\noffering systematic scaffolds for learning the implicit strategies of visual\nstorytelling. Based on the incorporated narrative constraints, we further\npropose novel metrics to closely evaluate the consistency of generated\nnarrative images and the alignment of generations with the input textual\nnarrative. Our results across three generative vision models demonstrate that\nlearning with VinaBench's knowledge constraints effectively improves the\nfaithfulness and cohesion of generated visual narratives.",
      "tldr_zh": "这篇论文提出了 VinaBench，一个新的基准，用于评估视觉叙事生成（visual narrative generation）的忠实性和一致性，以解决生成图像序列不忠实于输入文本和图像间不一致的问题。VinaBench 通过标注样本中的常识（commonsense）和话语约束（discourse constraints），提供系统支架来学习视觉叙事的隐含策略。基于这些约束，论文引入新指标来评估生成图像的连贯性和与文本的 alignment。实验结果显示，在三个生成视觉模型上，使用 VinaBench 的知识约束能有效提升视觉叙事的忠实度和凝聚力。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.CV",
      "comment": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern\n  Recognition (CVPR 2025)",
      "pdf_url": "http://arxiv.org/pdf/2503.20871v3",
      "published_date": "2025-03-26 18:00:03 UTC",
      "updated_date": "2025-04-03 09:28:19 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T06:17:07.673933"
    },
    {
      "arxiv_id": "2503.20786v1",
      "title": "Mobile-MMLU: A Mobile Intelligence Language Understanding Benchmark",
      "title_zh": "Mobile-MMLU：移动智能语言理解基准测试",
      "authors": [
        "Sondos Mahmoud Bsharat",
        "Mukul Ranjan",
        "Aidar Myrzakhan",
        "Jiacheng Liu",
        "Bowei Guo",
        "Shengkun Tang",
        "Zhuang Liu",
        "Yuanzhi Li",
        "Zhiqiang Shen"
      ],
      "abstract": "Rapid advancements in large language models (LLMs) have increased interest in\ndeploying them on mobile devices for on-device AI applications. Mobile users\ninteract differently with LLMs compared to desktop users, creating unique\nexpectations and data biases. Current benchmark datasets primarily target at\nserver and desktop environments, and there is a notable lack of extensive\ndatasets specifically designed for mobile contexts. Additionally, mobile\ndevices face strict limitations in storage and computing resources,\nconstraining model size and capabilities, thus requiring optimized efficiency\nand prioritized knowledge. To address these challenges, we introduce\nMobile-MMLU, a large-scale benchmark dataset tailored for mobile intelligence.\nIt consists of 16,186 questions across 80 mobile-related fields, designed to\nevaluate LLM performance in realistic mobile scenarios. A challenging subset,\nMobile-MMLU-Pro, provides advanced evaluation similar in size to MMLU-Pro but\nsignificantly more difficult than our standard full set. Both benchmarks use\nmultiple-choice, order-invariant questions focused on practical mobile\ninteractions, such as recipe suggestions, travel planning, and essential daily\ntasks. The dataset emphasizes critical mobile-specific metrics like inference\nlatency, energy consumption, memory usage, and response quality, offering\ncomprehensive insights into model performance under mobile constraints.\nMoreover, it prioritizes privacy and adaptability, assessing models' ability to\nperform on-device processing, maintain user privacy, and adapt to personalized\nusage patterns. Mobile-MMLU family offers a standardized framework for\ndeveloping and comparing mobile-optimized LLMs, enabling advancements in\nproductivity and decision-making within mobile computing environments. Our code\nand data are available at: https://github.com/VILA-Lab/Mobile-MMLU.",
      "tldr_zh": "该研究引入了 Mobile-MMLU，一种针对移动智能的语言理解基准数据集，旨在评估大型语言模型（LLMs）在移动设备上的性能，以应对用户交互差异、资源限制和数据偏差等问题。该数据集包含16,186个多选题，覆盖80个移动相关领域，如食谱建议和旅行规划，并强调实用场景下的指标，包括推理延迟、能源消耗、内存使用和响应质量。此外，一个更具挑战性的子集 Mobile-MMLU-Pro 提供了高级评估，聚焦隐私保护、本地处理和个性化适应，为开发移动优化 LLMs 提供了标准化框架，促进移动计算环境中的生产力和决策能力提升。数据集及其代码已在 GitHub 上公开。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "An order-invariant and mobile-centric benchmark. Code and data are\n  available at: https://github.com/VILA-Lab/Mobile-MMLU",
      "pdf_url": "http://arxiv.org/pdf/2503.20786v1",
      "published_date": "2025-03-26 17:59:56 UTC",
      "updated_date": "2025-03-26 17:59:56 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T06:17:20.045833"
    },
    {
      "arxiv_id": "2503.20853v1",
      "title": "Unified Multimodal Discrete Diffusion",
      "title_zh": "统一的多模态离散扩散",
      "authors": [
        "Alexander Swerdlow",
        "Mihir Prabhudesai",
        "Siddharth Gandhi",
        "Deepak Pathak",
        "Katerina Fragkiadaki"
      ],
      "abstract": "Multimodal generative models that can understand and generate across multiple\nmodalities are dominated by autoregressive (AR) approaches, which process\ntokens sequentially from left to right, or top to bottom. These models jointly\nhandle images, text, video, and audio for various tasks such as image\ncaptioning, question answering, and image generation. In this work, we explore\ndiscrete diffusion models as a unified generative formulation in the joint text\nand image domain, building upon their recent success in text generation.\nDiscrete diffusion models offer several advantages over AR models, including\nimproved control over quality versus diversity of generated samples, the\nability to perform joint multimodal inpainting (across both text and image\ndomains), and greater controllability in generation through guidance.\nLeveraging these benefits, we present the first Unified Multimodal Discrete\nDiffusion (UniDisc) model which is capable of jointly understanding and\ngenerating text and images for a variety of downstream tasks. We compare\nUniDisc to multimodal AR models, performing a scaling analysis and\ndemonstrating that UniDisc outperforms them in terms of both performance and\ninference-time compute, enhanced controllability, editability, inpainting, and\nflexible trade-off between inference time and generation quality. Code and\nadditional visualizations are available at https://unidisc.github.io.",
      "tldr_zh": "本文提出 Unified Multimodal Discrete Diffusion (UniDisc) 模型，作为一种统一的离散扩散框架，用于处理文本和图像的多模态生成任务，以克服 autoregressive (AR) 模型的顺序处理局限性。UniDisc 利用离散扩散模型的优势，包括更好的质量与多样性控制、联合多模态修复（如文本和图像领域）和增强的可控性生成。实验比较显示，UniDisc 在性能、推理计算效率、可编辑性和质量权衡方面均优于 AR 模型，并提供开源代码和可视化资源。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG",
        "cs.RO"
      ],
      "primary_category": "cs.CV",
      "comment": "Project Website: https://unidisc.github.io",
      "pdf_url": "http://arxiv.org/pdf/2503.20853v1",
      "published_date": "2025-03-26 17:59:51 UTC",
      "updated_date": "2025-03-26 17:59:51 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T06:17:32.234626"
    },
    {
      "arxiv_id": "2503.20783v1",
      "title": "Understanding R1-Zero-Like Training: A Critical Perspective",
      "title_zh": "翻译失败",
      "authors": [
        "Zichen Liu",
        "Changyu Chen",
        "Wenjun Li",
        "Penghui Qi",
        "Tianyu Pang",
        "Chao Du",
        "Wee Sun Lee",
        "Min Lin"
      ],
      "abstract": "DeepSeek-R1-Zero has shown that reinforcement learning (RL) at scale can\ndirectly enhance the reasoning capabilities of LLMs without supervised\nfine-tuning. In this work, we critically examine R1-Zero-like training by\nanalyzing its two core components: base models and RL. We investigate a wide\nrange of base models, including DeepSeek-V3-Base, to understand how pretraining\ncharacteristics influence RL performance. Our analysis reveals that\nDeepSeek-V3-Base already exhibit ''Aha moment'', while Qwen2.5 base models\ndemonstrate strong reasoning capabilities even without prompt templates,\nsuggesting potential pretraining biases. Additionally, we identify an\noptimization bias in Group Relative Policy Optimization (GRPO), which\nartificially increases response length (especially for incorrect outputs)\nduring training. To address this, we introduce Dr. GRPO, an unbiased\noptimization method that improves token efficiency while maintaining reasoning\nperformance. Leveraging these insights, we present a minimalist R1-Zero recipe\nthat achieves 43.3% accuracy on AIME 2024 with a 7B base model, establishing a\nnew state-of-the-art. Our code is available at\nhttps://github.com/sail-sg/understand-r1-zero.",
      "tldr_zh": "该论文对 R1-Zero-like training 进行批判性分析，聚焦于其核心组件——base models 和 reinforcement learning (RL)，揭示了 base models 的预训练特性（如 DeepSeek-V3-Base 的 \"Aha moment\" 和 Qwen2.5 的强推理能力）可能引入偏差，从而影响 RL 性能。研究者识别了 Group Relative Policy Optimization (GRPO) 中的优化偏差，导致响应长度不当增加，特别是错误输出，并提出改进版 Dr. GRPO，以提升 token 效率同时保持推理表现。基于这些洞见，他们开发了一个 minimalist R1-Zero recipe，使用 7B base model 在 AIME 2024 上实现 43.3% 的准确率，确立了新的 state-of-the-art 基准。代码已在 GitHub 开源。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.20783v1",
      "published_date": "2025-03-26 17:59:14 UTC",
      "updated_date": "2025-03-26 17:59:14 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T06:17:44.529509"
    },
    {
      "arxiv_id": "2504.02855v1",
      "title": "Exploration of Multi-Element Collaborative Research and Application for Modern Power System Based on Generative Large Models",
      "title_zh": "翻译失败",
      "authors": [
        "Lu Cheng",
        "Qixiu Zhang",
        "Beibei Xu",
        "Zhiwei Huang",
        "Cirun Zhang",
        "Yanan Lyu",
        "Fan Zhang"
      ],
      "abstract": "The transition to intelligent, low-carbon power systems necessitates advanced\noptimization strategies for managing renewable energy integration, energy\nstorage, and carbon emissions. Generative Large Models (GLMs) provide a\ndata-driven approach to enhancing forecasting, scheduling, and market\noperations by processing multi-source data and capturing complex system\ndynamics. This paper explores the role of GLMs in optimizing load-side\nmanagement, energy storage utilization, and electricity carbon, with a focus on\nSmart Wide-area Hybrid Energy Systems with Storage and Carbon (SGLSC). By\nleveraging spatiotemporal modeling and reinforcement learning, GLMs enable\ndynamic energy scheduling, improve grid stability, enhance carbon trading\nstrategies, and strengthen resilience against extreme weather events. The\nproposed framework highlights the transformative potential of GLMs in achieving\nefficient, adaptive, and low-carbon power system operations.",
      "tldr_zh": "这篇论文探讨了基于 Generative Large Models (GLMs) 的多元素协作研究在现代电力系统中的应用，旨在优化可再生能源整合、能源存储和碳排放管理。论文重点关注 Smart Wide-area Hybrid Energy Systems with Storage and Carbon (SGLSC)，通过处理多源数据、时空建模和强化学习来提升预测、调度和市场操作的效率。研究结果显示，GLMs 能够实现动态能源调度，提高电网稳定性、碳交易策略，并增强系统对极端天气事件的弹性。总体框架突出了 GLMs 在推动高效、适应性和低碳电力系统转型的潜力。",
      "categories": [
        "eess.SY",
        "cs.AI",
        "cs.SY"
      ],
      "primary_category": "eess.SY",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.02855v1",
      "published_date": "2025-03-26 17:58:49 UTC",
      "updated_date": "2025-03-26 17:58:49 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T06:17:56.365961"
    },
    {
      "arxiv_id": "2503.20756v1",
      "title": "ADS-Edit: A Multimodal Knowledge Editing Dataset for Autonomous Driving Systems",
      "title_zh": "ADS-Edit：一种用于自主驾驶系统的多模",
      "authors": [
        "Chenxi Wang",
        "Jizhan Fang",
        "Xiang Chen",
        "Bozhong Tian",
        "Ziwen Xu",
        "Huajun Chen",
        "Ningyu Zhang"
      ],
      "abstract": "Recent advancements in Large Multimodal Models (LMMs) have shown promise in\nAutonomous Driving Systems (ADS). However, their direct application to ADS is\nhindered by challenges such as misunderstanding of traffic knowledge, complex\nroad conditions, and diverse states of vehicle. To address these challenges, we\npropose the use of Knowledge Editing, which enables targeted modifications to a\nmodel's behavior without the need for full retraining. Meanwhile, we introduce\nADS-Edit, a multimodal knowledge editing dataset specifically designed for ADS,\nwhich includes various real-world scenarios, multiple data types, and\ncomprehensive evaluation metrics. We conduct comprehensive experiments and\nderive several interesting conclusions. We hope that our work will contribute\nto the further advancement of knowledge editing applications in the field of\nautonomous driving. Code and data are available in\nhttps://github.com/zjunlp/EasyEdit.",
      "tldr_zh": "该研究探讨了大型多模态模型 (LMMs) 在自动驾驶系统 (ADS) 中的应用面临的挑战，如对交通知识的误解、复杂道路条件和车辆状态多样性。为解决这些问题，论文提出使用 Knowledge Editing 技术来实现模型行为的针对性修改，而无需完整重新训练。作者引入了 ADS-Edit，这是一个专为 ADS 设计的多模态知识编辑数据集，涵盖真实场景、多数据类型和全面评估指标；实验结果揭示了若干有趣结论，并为知识编辑在自动驾驶领域的应用提供了新方向。Code 和数据可从 https://github.com/zjunlp/EasyEdit 获取。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.CV",
        "cs.LG",
        "cs.MM"
      ],
      "primary_category": "cs.CL",
      "comment": "Work in progress",
      "pdf_url": "http://arxiv.org/pdf/2503.20756v1",
      "published_date": "2025-03-26 17:45:29 UTC",
      "updated_date": "2025-03-26 17:45:29 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T06:18:07.472153"
    },
    {
      "arxiv_id": "2503.20752v2",
      "title": "Reason-RFT: Reinforcement Fine-Tuning for Visual Reasoning",
      "title_zh": "翻译失败",
      "authors": [
        "Huajie Tan",
        "Yuheng Ji",
        "Xiaoshuai Hao",
        "Minglan Lin",
        "Pengwei Wang",
        "Zhongyuan Wang",
        "Shanghang Zhang"
      ],
      "abstract": "Visual reasoning abilities play a crucial role in understanding complex\nmultimodal data, advancing both domain-specific applications and artificial\ngeneral intelligence (AGI). Existing methods improve VLM reasoning via\nChain-of-Thought (CoT) supervised fine-tuning, using meticulously annotated\ntraining data to enhance visual reasoning capabilities. However, this training\nparadigm may lead to overfitting and cognitive rigidity, restricting the\nmodel's ability to transfer visual reasoning skills across domains and limiting\nits real-world applicability. To address these limitations, we propose\nReason-RFT, a novel reinforcement fine-tuning framework that significantly\nenhances generalization capabilities in visual reasoning tasks. Reason-RFT\nintroduces a two-phase training framework for visual reasoning: (1) Supervised\nFine-Tuning (SFT) with curated Chain-of-Thought (CoT) data activates the\nreasoning potential of Vision-Language Models (VLMs), followed by (2) Group\nRelative Policy Optimization (GRPO)-based reinforcement learning that generates\nmultiple reasoning-response pairs, significantly enhancing generalization in\nvisual reasoning tasks. To evaluate Reason-RFT's visual reasoning capabilities,\nwe reconstructed a comprehensive dataset spanning visual counting, structure\nperception, and spatial transformation. Experimental results demonstrate\nReasoning-RFT's three key advantages: (1) Performance Enhancement: achieving\nstate-of-the-art results across multiple tasks, outperforming most mainstream\nopen-source and proprietary models; (2) Generalization Superiority:\nconsistently maintaining robust performance across diverse tasks and domains,\noutperforming alternative training paradigms; (3) Data Efficiency: excelling in\nfew-shot learning scenarios while surpassing full-dataset SFT baselines.\nProject website: https://tanhuajie.github.io/ReasonRFT",
      "tldr_zh": "本论文提出 Reason-RFT，一种新型强化微调框架，旨在提升视觉语言模型(VLMs)的视觉推理能力，以解决传统 Chain-of-Thought (CoT) 监督微调导致的过拟合和认知刚性问题。框架采用两阶段训练：首先通过 Supervised Fine-Tuning (SFT) 使用精心策划的 CoT 数据激活模型的推理潜力，随后应用 Group Relative Policy Optimization (GRPO) 基于强化学习生成多个推理-响应对，从而显著增强模型在不同领域的泛化能力。实验评估基于一个重建的数据集，涵盖视觉计数、结构感知和空间变换任务，结果显示 Reason-RFT 实现了最先进性能、优越的泛化优势和数据效率，在少样本场景中超越全数据集 SFT 基线。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "35 pages, 22 figures",
      "pdf_url": "http://arxiv.org/pdf/2503.20752v2",
      "published_date": "2025-03-26 17:38:06 UTC",
      "updated_date": "2025-03-27 03:13:00 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T06:18:21.570006"
    },
    {
      "arxiv_id": "2503.20750v1",
      "title": "Optimal Scaling Laws for Efficiency Gains in a Theoretical Transformer-Augmented Sectional MoE Framework",
      "title_zh": "翻译失败",
      "authors": [
        "Soham Sane"
      ],
      "abstract": "This paper introduces a theoretical framework for a Transformer-augmented,\nsectional Mixture-of-Experts (MoE) architecture that aims to enhance\ncomputational efficiency while preserving model scalability. Unlike\nconventional MoE models, which route entire token embeddings to selected\nexperts, our approach portions the embedding dimension itself -- assigning\nsegments of each token's representation to dedicated experts. To combat losses\nin token representation, we utilize a pre-expert transformer layer to recompute\nattention across tokens and reduce the sequence length dimensionality. We\nextend our theory by deriving optimal scaling laws that a non-linear\nrelationship between the number of experts and factors such as model\ndimensionality, sequence length, and system overhead. These formulations yield\nclosed-form and numerically-solvable expressions for identifying the optimal\nexpert count under given architectural and hardware constraints. As a result,\nour framework not only provides theoretical bounds for computing efficiency\nwith varying frameworks but also guides practical design choices for scaling\nlarge models effectively. While empirical validation is pending, we present a\ncomprehensive experimental road map to evaluate the framework's efficiency,\nscalability, and practicality in future work.",
      "tldr_zh": "这篇论文提出了一种理论框架，即 Transformer-augmented, sectional Mixture-of-Experts (MoE) 架构，旨在提升计算效率的同时保持模型可扩展性。该框架通过将 token 嵌入的维度部分分配给不同专家，而不是路由整个嵌入，并利用 pre-expert Transformer 层重新计算 token 间的注意力，以减少序列长度维度并缓解表示损失。论文推导了最佳缩放定律，揭示了专家数量与模型维度、序列长度及系统开销之间的非线性关系，并提供封闭形式和数值可解表达式来指导实际模型设计。尽管目前为理论性研究，论文还概述了未来实验路线图，以验证框架的效率、可扩展性和实用性。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.20750v1",
      "published_date": "2025-03-26 17:33:38 UTC",
      "updated_date": "2025-03-26 17:33:38 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T06:18:32.482809"
    },
    {
      "arxiv_id": "2503.20744v1",
      "title": "High Quality Diffusion Distillation on a Single GPU with Relative and Absolute Position Matching",
      "title_zh": "高质量扩散蒸馏：在单个 GPU 上使用相对和绝对位置匹配",
      "authors": [
        "Guoqiang Zhang",
        "Kenta Niwa",
        "J. P. Lewis",
        "Cedric Mesnage",
        "W. Bastiaan Kleijn"
      ],
      "abstract": "We introduce relative and absolute position matching (RAPM), a diffusion\ndistillation method resulting in high quality generation that can be trained\nefficiently on a single GPU. Recent diffusion distillation research has\nachieved excellent results for high-resolution text-to-image generation with\nmethods such as phased consistency models (PCM) and improved distribution\nmatching distillation (DMD2). However, these methods generally require many\nGPUs (e.g.~8-64) and significant batchsizes (e.g.~128-2048) during training,\nresulting in memory and compute requirements that are beyond the resources of\nsome researchers. RAPM provides effective single-GPU diffusion distillation\ntraining with a batchsize of 1. The new method attempts to mimic the sampling\ntrajectories of the teacher model by matching the relative and absolute\npositions. The design of relative positions is inspired by PCM. Two\ndiscriminators are introduced accordingly in RAPM, one for matching relative\npositions and the other for absolute positions. Experimental results on\nStableDiffusion (SD) V1.5 and SDXL indicate that RAPM with 4 timesteps produces\ncomparable FID scores as the best method with 1 timestep under very limited\ncomputational resources.",
      "tldr_zh": "本研究引入了相对和绝对位置匹配 (RAPM)，一种高效的扩散蒸馏方法，能够在单 GPU 上以批量大小为1实现高质量图像生成，解决了现有方法如 PCM 和 DMD2 的高资源需求问题。RAPM 通过模仿教师模型的采样轨迹，匹配相对和绝对位置，并引入两个判别器（一个用于相对位置，一个用于绝对位置）来提升训练效率，其相对位置设计受 PCM 启发。在 StableDiffusion V1.5 和 SDXL 的实验中，RAPM 使用4个时间步生成与最先进方法相当的 FID 分数，证明了其在有限计算资源下的强大性能。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.20744v1",
      "published_date": "2025-03-26 17:29:08 UTC",
      "updated_date": "2025-03-26 17:29:08 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T06:18:44.859118"
    },
    {
      "arxiv_id": "2503.20742v2",
      "title": "Quantum Neural Network Restatement of Markov Jump Process",
      "title_zh": "翻译失败",
      "authors": [
        "Z. Zarezadeh",
        "N. Zarezadeh"
      ],
      "abstract": "Despite the many challenges in exploratory data analysis, artificial neural\nnetworks have motivated strong interests in scientists and researchers both in\ntheoretical as well as practical applications. Among sources of such popularity\nof artificial neural networks the ability of modeling non-linear dynamical\nsystems, generalization, and adaptation possibilities should be mentioned.\nDespite this, there is still significant debate about the role of various\nunderlying stochastic processes in stabilizing a unique structure for data\nlearning and prediction. One of such obstacles to the theoretical and numerical\nstudy of machine intelligent systems is the curse of dimensionality and the\nsampling from high-dimensional probability distributions. In general, this\ncurse prevents efficient description of states, providing a significant\ncomplexity barrier for the system to be efficiently described and studied. In\nthis strand of research, direct treatment and description of such abstract\nnotions of learning theory in terms of quantum information be one of the most\nfavorable candidates. Hence, the subject matter of these articles is devoted to\nproblems of design, adaptation and the formulations of computationally hard\nproblems in terms of quantum mechanical systems. In order to characterize the\nmicroscopic description of such dynamics in the language of inferential\nstatistics, covariance matrix estimation of d-dimensional Gaussian densities\nand Bayesian interpretation of eigenvalue problem for dynamical systems is\nassessed.",
      "tldr_zh": "这篇论文重新表述了马尔可夫跳过程（Markov Jump Process）为量子神经网络（Quantum Neural Network），旨在解决神经网络在建模非线性动态系统时面临的挑战，如维数灾难（curse of dimensionality）和高维概率分布采样问题。作者提出通过量子信息理论，将学习理论问题表述为量子机械系统，以提升数据学习和预测的效率和稳定性。具体而言，论文评估了d维高斯密度协方差矩阵估计（covariance matrix estimation）和动态系统的贝叶斯解释（Bayesian interpretation），为处理复杂动态系统的微观描述提供了新框架。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.NA",
        "math.NA"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.20742v2",
      "published_date": "2025-03-26 17:25:11 UTC",
      "updated_date": "2025-03-28 16:24:37 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T06:18:55.881010"
    },
    {
      "arxiv_id": "2503.20739v1",
      "title": "Emotion Detection and Music Recommendation System",
      "title_zh": "情感检测与音乐推荐系统",
      "authors": [
        "Swetha Kambham",
        "Hubert Jhonson",
        "Sai Prathap Reddy Kambham"
      ],
      "abstract": "As artificial intelligence becomes more and more ingrained in daily life, we\npresent a novel system that uses deep learning for music recommendation and\nemotion-based detection. Through the use of facial recognition and the DeepFace\nframework, our method analyses human emotions in real-time and then plays music\nthat reflects the mood it has discovered. The system uses a webcam to take\npictures, analyses the most common facial expression, and then pulls a playlist\nfrom local storage that corresponds to the mood it has detected. An engaging\nand customised experience is ensured by allowing users to manually change the\nsong selection via a dropdown menu or navigation buttons. By continuously\nlooping over the playlist, the technology guarantees continuity. The objective\nof our system is to improve emotional well-being through music therapy by\noffering a responsive and automated music-selection experience.",
      "tldr_zh": "本文提出了一种基于深度学习的音乐推荐系统，通过面部识别技术实时检测用户情绪并提供个性化音乐播放。系统利用 DeepFace 框架和网络摄像头捕获面部图像，分析常见表情后从本地存储提取对应的播放列表，并允许用户通过下拉菜单或导航按钮手动调整歌曲。最终，该系统通过循环播放确保连续体验，旨在通过音乐疗法改善用户的情感福祉。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.20739v1",
      "published_date": "2025-03-26 17:22:06 UTC",
      "updated_date": "2025-03-26 17:22:06 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T06:19:07.595962"
    },
    {
      "arxiv_id": "2503.22736v1",
      "title": "Cyborg Data: Merging Human with AI Generated Training Data",
      "title_zh": "翻译失败",
      "authors": [
        "Kai North",
        "Christopher Ormerod"
      ],
      "abstract": "Automated scoring (AS) systems used in large-scale assessment have\ntraditionally used small statistical models that require a large quantity of\nhand-scored data to make accurate predictions, which can be time-consuming and\ncostly. Generative Large Language Models are trained on many tasks and have\nshown impressive abilities to generalize to new tasks with little to no data.\nWhile these models require substantially more computational power to make\npredictions, they still require some fine-tuning to meet operational standards.\nEvidence suggests that these models can exceed human-human levels of agreement\neven when fine-tuned on small amounts of data. With this in mind, we propose a\nmodel distillation pipeline in which a large generative model, a Teacher,\nteaches a much smaller model, a Student. The Teacher, trained on a small subset\nof the training data, is used to provide scores on the remaining training data,\nwhich is then used to train the Student. We call the resulting dataset \"Cyborg\nData\", as it combines human and machine-scored responses. Our findings show\nthat Student models trained on \"Cyborg Data\" show performance comparable to\ntraining on the entire dataset, while only requiring 10% of the original\nhand-scored data.",
      "tldr_zh": "本论文针对自动化评分(AS)系统的传统依赖大量手动评分数据的局限，提出了一种模型蒸馏(model distillation)管道，使用大型生成式语言模型(Generative Large Language Models)作为Teacher模型。Teacher模型在少量训练数据上微调后，为剩余数据生成评分，从而创建“Cyborg Data”，这是一种结合人类和AI评分的数据集。实验结果表明，使用“Cyborg Data”训练的Student模型性能与使用完整数据集相当，但只需10%的原始手动评分数据。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.22736v1",
      "published_date": "2025-03-26 16:38:20 UTC",
      "updated_date": "2025-03-26 16:38:20 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T06:19:20.642200"
    },
    {
      "arxiv_id": "2503.20688v1",
      "title": "Graph-Enhanced Model-Free Reinforcement Learning Agents for Efficient Power Grid Topological Control",
      "title_zh": "图增强的模型无关强化学习代理",
      "authors": [
        "Eloy Anguiano Batanero",
        "Ángela Fernández",
        "Álvaro Barbero"
      ],
      "abstract": "The increasing complexity of power grid management, driven by the emergence\nof prosumers and the demand for cleaner energy solutions, has needed innovative\napproaches to ensure stability and efficiency. This paper presents a novel\napproach within the model-free framework of reinforcement learning, aimed at\noptimizing power network operations without prior expert knowledge. We\nintroduce a masked topological action space, enabling agents to explore diverse\nstrategies for cost reduction while maintaining reliable service using the\nstate logic as a guide for choosing proper actions. Through extensive\nexperimentation across 20 different scenarios in a simulated 5-substation\nenvironment, we demonstrate that our approach achieves a consistent reduction\nin power losses, while ensuring grid stability against potential blackouts. The\nresults underscore the effectiveness of combining dynamic observation\nformalization with opponent-based training, showing a viable way for autonomous\nmanagement solutions in modern energy systems or even for building a\nfoundational model for this field.",
      "tldr_zh": "这篇论文提出了一种基于图增强的无模型强化学习（model-free reinforcement learning）代理，用于高效的电力网格拓扑控制，以应对prosumers和清洁能源需求带来的管理复杂性。方法引入masked topological action space和state logic，允许代理在不依赖专家知识的情况下探索多样策略，减少成本并确保电网稳定性。实验结果显示，在模拟的5-substation环境中进行20种场景测试后，该方法实现了电力损失的一致性降低，并通过动态观察形式化和opponent-based training，为现代能源系统的自主管理提供了可行基础。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.20688v1",
      "published_date": "2025-03-26 16:20:30 UTC",
      "updated_date": "2025-03-26 16:20:30 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T06:19:32.108285"
    },
    {
      "arxiv_id": "2503.20685v2",
      "title": "Flip Learning: Weakly Supervised Erase to Segment Nodules in Breast Ultrasound",
      "title_zh": "Flip Learning：弱监督擦除法用于分割乳腺超声中的结节",
      "authors": [
        "Yuhao Huang",
        "Ao Chang",
        "Haoran Dou",
        "Xing Tao",
        "Xinrui Zhou",
        "Yan Cao",
        "Ruobing Huang",
        "Alejandro F Frangi",
        "Lingyun Bao",
        "Xin Yang",
        "Dong Ni"
      ],
      "abstract": "Accurate segmentation of nodules in both 2D breast ultrasound (BUS) and 3D\nautomated breast ultrasound (ABUS) is crucial for clinical diagnosis and\ntreatment planning. Therefore, developing an automated system for nodule\nsegmentation can enhance user independence and expedite clinical analysis.\nUnlike fully-supervised learning, weakly-supervised segmentation (WSS) can\nstreamline the laborious and intricate annotation process. However, current WSS\nmethods face challenges in achieving precise nodule segmentation, as many of\nthem depend on inaccurate activation maps or inefficient pseudo-mask generation\nalgorithms. In this study, we introduce a novel multi-agent reinforcement\nlearning-based WSS framework called Flip Learning, which relies solely on 2D/3D\nboxes for accurate segmentation. Specifically, multiple agents are employed to\nerase the target from the box to facilitate classification tag flipping, with\nthe erased region serving as the predicted segmentation mask. The key\ncontributions of this research are as follows: (1) Adoption of a\nsuperpixel/supervoxel-based approach to encode the standardized environment,\ncapturing boundary priors and expediting the learning process. (2) Introduction\nof three meticulously designed rewards, comprising a classification score\nreward and two intensity distribution rewards, to steer the agents' erasing\nprocess precisely, thereby avoiding both under- and over-segmentation. (3)\nImplementation of a progressive curriculum learning strategy to enable agents\nto interact with the environment in a progressively challenging manner, thereby\nenhancing learning efficiency. Extensively validated on the large in-house BUS\nand ABUS datasets, our Flip Learning method outperforms state-of-the-art WSS\nmethods and foundation models, and achieves comparable performance as\nfully-supervised learning algorithms.",
      "tldr_zh": "本研究提出了一种名为 Flip Learning 的多智能体强化学习框架，用于弱监督分割（WSS）乳腺超声图像中的结节，仅需 2D/3D 边界框作为输入，以提高分割准确性和临床效率。框架中，多个智能体通过擦除目标区域来实现分类标签翻转，并使用 erased 区域作为预测的分割掩码，同时采用 superpixel/supervoxel-based approach 捕捉边界先验，并设计了三个奖励机制（包括分类分数奖励和两个强度分布奖励）来避免欠分割或过分割。研究还引入了渐进式课程学习策略，以提升智能体的学习效率。在内部 BUS 和 ABUS 数据集上的实验验证中，Flip Learning 优于现有 WSS 方法和基础模型，其性能接近全监督学习算法。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "Accepted by Medical Image Analysis. 24 pages, 13 figures, 20 tabels",
      "pdf_url": "http://arxiv.org/pdf/2503.20685v2",
      "published_date": "2025-03-26 16:20:02 UTC",
      "updated_date": "2025-03-27 06:16:16 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T06:19:44.391373"
    },
    {
      "arxiv_id": "2503.20676v1",
      "title": "Inductive Link Prediction on N-ary Relational Facts via Semantic Hypergraph Reasoning",
      "title_zh": "翻译失败",
      "authors": [
        "Gongzhu Yin",
        "Hongli Zhang",
        "Yuchen Yang",
        "Yi Luo"
      ],
      "abstract": "N-ary relational facts represent semantic correlations among more than two\nentities. While recent studies have developed link prediction (LP) methods to\ninfer missing relations for knowledge graphs (KGs) containing n-ary relational\nfacts, they are generally limited to transductive settings. Fully inductive\nsettings, where predictions are made on previously unseen entities, remain a\nsignificant challenge. As existing methods are mainly entity embedding-based,\nthey struggle to capture entity-independent logical rules. To fill in this gap,\nwe propose an n-ary subgraph reasoning framework for fully inductive link\nprediction (ILP) on n-ary relational facts. This framework reasons over local\nsubgraphs and has a strong inductive inference ability to capture n-ary\npatterns. Specifically, we introduce a novel graph structure, the n-ary\nsemantic hypergraph, to facilitate subgraph extraction. Moreover, we develop a\nsubgraph aggregating network, NS-HART, to effectively mine complex semantic\ncorrelations within subgraphs. Theoretically, we provide a thorough analysis\nfrom the score function optimization perspective to shed light on NS-HART's\neffectiveness for n-ary ILP tasks. Empirically, we conduct extensive\nexperiments on a series of inductive benchmarks, including transfer reasoning\n(with and without entity features) and pairwise subgraph reasoning. The results\nhighlight the superiority of the n-ary subgraph reasoning framework and the\nexceptional inductive ability of NS-HART. The source code of this paper has\nbeen made publicly available at\nhttps://github.com/yin-gz/Nary-Inductive-SubGraph.",
      "tldr_zh": "该论文针对 N-ary relational facts（多于两个实体的关系）在知识图谱（KGs）中的链接预测（LP）问题，提出了一种完全归纳式（fully inductive）框架，以解决现有方法在处理未见实体时的局限性。框架通过构建 n-ary semantic hypergraph 结构来提取本地子图，并开发了子图聚合网络 NS-HART，以有效挖掘子图内的复杂语义相关性，从而捕捉实体独立的逻辑规则。从分数函数优化的角度进行理论分析，证明了 NS-HART 在 N-ary ILP 任务中的有效性。实验结果显示，该框架在多种归纳基准上（如转移推理和成对子图推理）显著优于基线模型，展示了其强大的归纳能力。",
      "categories": [
        "cs.AI",
        "cs.LG",
        "I.2.4"
      ],
      "primary_category": "cs.AI",
      "comment": "To be published in Proceedings of the 31st ACM SIGKDD Conference on\n  Knowledge Discovery and Data Mining V.1 (KDD'25)",
      "pdf_url": "http://arxiv.org/pdf/2503.20676v1",
      "published_date": "2025-03-26 16:09:54 UTC",
      "updated_date": "2025-03-26 16:09:54 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T06:19:57.166298"
    },
    {
      "arxiv_id": "2503.20848v1",
      "title": "The Backfiring Effect of Weak AI Safety Regulation",
      "title_zh": "弱人工智能安全监管的适得其反效应",
      "authors": [
        "Benjamin Laufer",
        "Jon Kleinberg",
        "Hoda Heidari"
      ],
      "abstract": "Recent policy proposals aim to improve the safety of general-purpose AI, but\nthere is little understanding of the efficacy of different regulatory\napproaches to AI safety. We present a strategic model that explores the\ninteractions between the regulator, the general-purpose AI technology creators,\nand domain specialists--those who adapt the AI for specific applications. Our\nanalysis examines how different regulatory measures, targeting different parts\nof the development chain, affect the outcome of the development process. In\nparticular, we assume AI technology is described by two key attributes: safety\nand performance. The regulator first sets a minimum safety standard that\napplies to one or both players, with strict penalties for non-compliance. The\ngeneral-purpose creator then develops the technology, establishing its initial\nsafety and performance levels. Next, domain specialists refine the AI for their\nspecific use cases, and the resulting revenue is distributed between the\nspecialist and generalist through an ex-ante bargaining process. Our analysis\nof this game reveals two key insights: First, weak safety regulation imposed\nonly on the domain specialists can backfire. While it might seem logical to\nregulate use cases (as opposed to the general-purpose technology), our analysis\nshows that weak regulations targeting domain specialists alone can\nunintentionally reduce safety. This effect persists across a wide range of\nsettings. Second, in sharp contrast to the previous finding, we observe that\nstronger, well-placed regulation can in fact benefit all players subjected to\nit. When regulators impose appropriate safety standards on both AI creators and\ndomain specialists, the regulation functions as a commitment mechanism, leading\nto safety and performance gains, surpassing what is achieved under no\nregulation or regulating one player only.",
      "tldr_zh": "该研究构建了一个战略模型，探讨AI安全监管对监管者、AI技术创建者(general-purpose AI creators)和领域专家(domain specialists)之间的互动影响。模型假设AI技术具有safety和performance两个关键属性，并分析了针对不同参与者的监管措施。关键发现是，弱安全监管仅针对domain specialists时可能适得其反(backfiring effect)，导致整体安全降低；相反，更强的监管同时施加于AI创建者和domain specialists时，能作为承诺机制，提升safety和performance，优于无监管或单方监管的场景。总体而言，该分析为制定有效AI安全政策提供了重要见解。",
      "categories": [
        "cs.GT",
        "cs.AI",
        "cs.CY",
        "econ.TH"
      ],
      "primary_category": "cs.GT",
      "comment": "28 pages, 8 figures",
      "pdf_url": "http://arxiv.org/pdf/2503.20848v1",
      "published_date": "2025-03-26 16:08:22 UTC",
      "updated_date": "2025-03-26 16:08:22 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T06:20:07.053847"
    },
    {
      "arxiv_id": "2503.20658v1",
      "title": "Probabilistic Forecasting for Network Resource Analysis in Integrated Terrestrial and Non-Terrestrial Networks",
      "title_zh": "集成陆地和非陆地网络中网络资源分析的概率预测",
      "authors": [
        "Cristian J. Vaca-Rubio",
        "Vaishnavi Kasuluru",
        "Engin Zeydan",
        "Luis Blanco",
        "Roberto Pereira",
        "Marius Caus",
        "Kapal Dev"
      ],
      "abstract": "Efficient resource management is critical for Non-Terrestrial Networks (NTNs)\nto provide consistent, high-quality service in remote and under-served regions.\nWhile traditional single-point prediction methods, such as Long-Short Term\nMemory (LSTM), have been used in terrestrial networks, they often fall short in\nNTNs due to the complexity of satellite dynamics, signal latency and coverage\nvariability. Probabilistic forecasting, which quantifies the uncertainties of\nthe predictions, is a robust alternative. In this paper, we evaluate the\napplication of probabilistic forecasting techniques, in particular SFF, to NTN\nresource allocation scenarios. Our results show their effectiveness in\npredicting bandwidth and capacity requirements in different NTN segments of\nprobabilistic forecasting compared to single-point prediction techniques such\nas LSTM. The results show the potential of black probabilistic forecasting\nmodels to provide accurate and reliable predictions and to quantify their\nuncertainty, making them indispensable for optimizing NTN resource allocation.\nAt the end of the paper, we also present application scenarios and a\nstandardization roadmap for the use of probabilistic forecasting in integrated\nTerrestrial Network (TN)-NTN environments.",
      "tldr_zh": "本论文探讨了在集成地面网络(TN)和非地面网络(NTNs)中，使用概率预测技术进行网络资源分析，以应对卫星动态、信号延迟和覆盖变化带来的挑战。相比传统单点预测方法如 LSTM，该研究评估了概率预测技术，特别是 SFF，在预测 NTN 带宽和容量需求方面的有效性，结果显示其准确性和可靠性显著提升，并能量化预测不确定性。最终，该方法为优化 NTN 资源分配提供了潜力，并提出了应用场景和 TN-NTN 环境的标准化路线图。",
      "categories": [
        "eess.SP",
        "cs.AI",
        "cs.LG",
        "cs.NI"
      ],
      "primary_category": "eess.SP",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.20658v1",
      "published_date": "2025-03-26 15:54:46 UTC",
      "updated_date": "2025-03-26 15:54:46 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T06:20:20.414098"
    },
    {
      "arxiv_id": "2503.20654v1",
      "title": "AccidentSim: Generating Physically Realistic Vehicle Collision Videos from Real-World Accident Reports",
      "title_zh": "AccidentSim：从真实世界",
      "authors": [
        "Xiangwen Zhang",
        "Qian Zhang",
        "Longfei Han",
        "Qiang Qu",
        "Xiaoming Chen"
      ],
      "abstract": "Collecting real-world vehicle accident videos for autonomous driving research\nis challenging due to their rarity and complexity. While existing driving video\ngeneration methods may produce visually realistic videos, they often fail to\ndeliver physically realistic simulations because they lack the capability to\ngenerate accurate post-collision trajectories. In this paper, we introduce\nAccidentSim, a novel framework that generates physically realistic vehicle\ncollision videos by extracting and utilizing the physical clues and contextual\ninformation available in real-world vehicle accident reports. Specifically,\nAccidentSim leverages a reliable physical simulator to replicate post-collision\nvehicle trajectories from the physical and contextual information in the\naccident reports and to build a vehicle collision trajectory dataset. This\ndataset is then used to fine-tune a language model, enabling it to respond to\nuser prompts and predict physically consistent post-collision trajectories\nacross various driving scenarios based on user descriptions. Finally, we employ\nNeural Radiance Fields (NeRF) to render high-quality backgrounds, merging them\nwith the foreground vehicles that exhibit physically realistic trajectories to\ngenerate vehicle collision videos. Experimental results demonstrate that the\nvideos produced by AccidentSim excel in both visual and physical authenticity.",
      "tldr_zh": "该研究提出AccidentSim框架，通过分析真实车辆事故报告中的物理线索和上下文信息，生成物理真实性的车辆碰撞视频。该框架首先利用物理模拟器基于报告数据复制碰撞后车辆轨迹，并构建一个专用轨迹数据集。随后，通过微调语言模型，使其能根据用户提示预测各种驾驶场景下的物理一致轨迹。最后，结合Neural Radiance Fields (NeRF)渲染高质量背景，与前置车辆轨迹融合，生成视觉和物理真实视频。实验结果显示，AccidentSim生成的视频在真实性方面显著优于现有方法，为自动驾驶研究提供宝贵资源。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.20654v1",
      "published_date": "2025-03-26 15:50:42 UTC",
      "updated_date": "2025-03-26 15:50:42 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T06:20:31.286559"
    },
    {
      "arxiv_id": "2503.20648v1",
      "title": "TN-Eval: Rubric and Evaluation Protocols for Measuring the Quality of Behavioral Therapy Notes",
      "title_zh": "翻译失败",
      "authors": [
        "Raj Sanjay Shah",
        "Lei Xu",
        "Qianchu Liu",
        "Jon Burnsky",
        "Drew Bertagnolli",
        "Chaitanya Shivade"
      ],
      "abstract": "Behavioral therapy notes are important for both legal compliance and patient\ncare. Unlike progress notes in physical health, quality standards for\nbehavioral therapy notes remain underdeveloped. To address this gap, we\ncollaborated with licensed therapists to design a comprehensive rubric for\nevaluating therapy notes across key dimensions: completeness, conciseness, and\nfaithfulness. Further, we extend a public dataset of behavioral health\nconversations with therapist-written notes and LLM-generated notes, and apply\nour evaluation framework to measure their quality. We find that: (1) A\nrubric-based manual evaluation protocol offers more reliable and interpretable\nresults than traditional Likert-scale annotations. (2) LLMs can mimic human\nevaluators in assessing completeness and conciseness but struggle with\nfaithfulness. (3) Therapist-written notes often lack completeness and\nconciseness, while LLM-generated notes contain hallucination. Surprisingly, in\na blind test, therapists prefer and judge LLM-generated notes to be superior to\ntherapist-written notes.",
      "tldr_zh": "这篇论文提出了TN-Eval框架，包括一个全面的评估标准（rubric）和协议，用于衡量行为治疗笔记的质量，涵盖完整性（completeness）、简洁性（conciseness）和忠实性（faithfulness）等关键维度。研究团队与治疗师合作扩展了一个公共数据集，包含治疗师写的笔记和LLM生成的笔记，并通过该框架进行比较评估。结果显示，基于rubric的手动评估比传统的Likert-scale注解更可靠且可解释；LLM能有效评估完整性和简洁性，但处理faithfulness时存在困难；有趣的是，在盲测中，治疗师更倾向于LLM生成的笔记，并认为它们优于治疗师写的笔记，后者常缺乏完整性和简洁性。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.20648v1",
      "published_date": "2025-03-26 15:40:40 UTC",
      "updated_date": "2025-03-26 15:40:40 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T06:20:44.486379"
    },
    {
      "arxiv_id": "2503.20634v1",
      "title": "Procedural Knowledge Ontology (PKO)",
      "title_zh": "翻译失败",
      "authors": [
        "Valentina Anita Carriero",
        "Mario Scrocca",
        "Ilaria Baroni",
        "Antonia Azzini",
        "Irene Celino"
      ],
      "abstract": "Processes, workflows and guidelines are core to ensure the correct\nfunctioning of industrial companies: for the successful operations of factory\nlines, machinery or services, often industry operators rely on their past\nexperience and know-how. The effect is that this Procedural Knowledge (PK)\nremains tacit and, as such, difficult to exploit efficiently and effectively.\nThis paper presents PKO, the Procedural Knowledge Ontology, which enables the\nexplicit modeling of procedures and their executions, by reusing and extending\nexisting ontologies. PKO is built on requirements collected from three\nheterogeneous industrial use cases and can be exploited by any AI and\ndata-driven tools that rely on a shared and interoperable representation to\nsupport the governance of PK throughout its life cycle. We describe its\nstructure and design methodology, and outline its relevance, quality, and\nimpact by discussing applications leveraging PKO for PK elicitation and\nexploitation.",
      "tldr_zh": "这篇论文介绍了 Procedural Knowledge Ontology (PKO)，一个用于显式建模工业流程、执行和指南的本体，以解决隐性 Procedural Knowledge (PK) 在工业中的利用难题。PKO 通过重用和扩展现有本体，并基于三个异构工业用例的需求进行设计，支持 AI 和数据驱动工具实现共享、可互操作的知识表示。论文详细描述了 PKO 的结构、设计方法及其在 PK 生命周期管理中的相关性、质量和影响，包括在知识提取和利用方面的实际应用。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.20634v1",
      "published_date": "2025-03-26 15:28:30 UTC",
      "updated_date": "2025-03-26 15:28:30 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T06:20:56.034013"
    },
    {
      "arxiv_id": "2503.20630v1",
      "title": "$β$-GNN: A Robust Ensemble Approach Against Graph Structure Perturbation",
      "title_zh": "翻译失败",
      "authors": [
        "Haci Ismail Aslan",
        "Philipp Wiesner",
        "Ping Xiong",
        "Odej Kao"
      ],
      "abstract": "Graph Neural Networks (GNNs) are playing an increasingly important role in\nthe efficient operation and security of computing systems, with applications in\nworkload scheduling, anomaly detection, and resource management. However, their\nvulnerability to network perturbations poses a significant challenge. We\npropose $\\beta$-GNN, a model enhancing GNN robustness without sacrificing clean\ndata performance. $\\beta$-GNN uses a weighted ensemble, combining any GNN with\na multi-layer perceptron. A learned dynamic weight, $\\beta$, modulates the\nGNN's contribution. This $\\beta$ not only weights GNN influence but also\nindicates data perturbation levels, enabling proactive mitigation. Experimental\nresults on diverse datasets show $\\beta$-GNN's superior adversarial accuracy\nand attack severity quantification. Crucially, $\\beta$-GNN avoids perturbation\nassumptions, preserving clean data structure and performance.",
      "tldr_zh": "本文提出$β$-GNN，一种鲁棒的集成方法，用于提升图神经网络(GNNs)对图结构扰动的抵抗力，同时不影响干净数据的性能。$β$-GNN通过将任意GNN与多层感知器(MLP)结合，并使用动态权重$β$来调节GNN的贡献，该权重不仅平衡模型影响，还能量化数据扰动水平以实现主动缓解。实验在多种数据集上证明，$β$-GNN显著提高了对抗准确率和攻击严重性评估，而无需对扰动进行预设假设。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "This is the author's version of the paper accepted at EuroMLSys 2025",
      "pdf_url": "http://arxiv.org/pdf/2503.20630v1",
      "published_date": "2025-03-26 15:24:07 UTC",
      "updated_date": "2025-03-26 15:24:07 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T06:21:07.938837"
    },
    {
      "arxiv_id": "2503.20623v1",
      "title": "Collaborative Storytelling and LLM: A Linguistic Analysis of Automatically-Generated Role-Playing Game Sessions",
      "title_zh": "合作式叙事和 LLM：自动生成角色扮演游戏会话的语言学分析",
      "authors": [
        "Alessandro Maisto"
      ],
      "abstract": "Role-playing games (RPG) are games in which players interact with one another\nto create narratives. The role of players in the RPG is largely based on the\ninteraction between players and their characters. This emerging form of shared\nnarrative, primarily oral, is receiving increasing attention. In particular,\nmany authors investigated the use of an LLM as an actor in the game. In this\npaper, we aim to discover to what extent the language of Large Language Models\n(LLMs) exhibit oral or written features when asked to generate an RPG session\nwithout human interference. We will conduct a linguistic analysis of the\nlexical and syntactic features of the generated texts and compare the results\nwith analyses of conversations, transcripts of human RPG sessions, and books.\nWe found that LLMs exhibit a pattern that is distinct from all other text\ncategories, including oral conversations, human RPG sessions and books. Our\nanalysis has shown how training influences the way LLMs express themselves and\nprovides important indications of the narrative capabilities of these tools.",
      "tldr_zh": "这篇论文分析了大型语言模型 (LLMs) 在自动生成角色扮演游戏 (RPG) 会话时的语言特征，旨在探讨 LLMs 是否表现出口语或书面语的特性。研究通过对词汇和句法特征的语言分析，将生成的文本与人类对话、RPG 会话记录及书籍进行比较。结果发现，LLMs 的输出模式与这些类别均不同，这突显了训练数据对模型表达方式的影响，并提供了关于其叙事能力的宝贵洞见。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "17 pages",
      "pdf_url": "http://arxiv.org/pdf/2503.20623v1",
      "published_date": "2025-03-26 15:10:47 UTC",
      "updated_date": "2025-03-26 15:10:47 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T06:21:19.475266"
    },
    {
      "arxiv_id": "2503.20844v1",
      "title": "Robust Deep Reinforcement Learning in Robotics via Adaptive Gradient-Masked Adversarial Attacks",
      "title_zh": "翻译失败",
      "authors": [
        "Zongyuan Zhang",
        "Tianyang Duan",
        "Zheng Lin",
        "Dong Huang",
        "Zihan Fang",
        "Zekai Sun",
        "Ling Xiong",
        "Hongbin Liang",
        "Heming Cui",
        "Yong Cui",
        "Yue Gao"
      ],
      "abstract": "Deep reinforcement learning (DRL) has emerged as a promising approach for\nrobotic control, but its realworld deployment remains challenging due to its\nvulnerability to environmental perturbations. Existing white-box adversarial\nattack methods, adapted from supervised learning, fail to effectively target\nDRL agents as they overlook temporal dynamics and indiscriminately perturb all\nstate dimensions, limiting their impact on long-term rewards. To address these\nchallenges, we propose the Adaptive Gradient-Masked Reinforcement (AGMR)\nAttack, a white-box attack method that combines DRL with a gradient-based soft\nmasking mechanism to dynamically identify critical state dimensions and\noptimize adversarial policies. AGMR selectively allocates perturbations to the\nmost impactful state features and incorporates a dynamic adjustment mechanism\nto balance exploration and exploitation during training. Extensive experiments\ndemonstrate that AGMR outperforms state-of-the-art adversarial attack methods\nin degrading the performance of the victim agent and enhances the victim\nagent's robustness through adversarial defense mechanisms.",
      "tldr_zh": "本研究针对深度强化学习(DRL)在机器人控制中的易受环境扰动问题，提出了一种鲁棒性提升方法：Adaptive Gradient-Masked Reinforcement (AGMR) Attack。该方法结合DRL和梯度-based软masking机制，动态识别关键状态维度，选择性地分配扰动并优化对抗策略，同时通过动态调整机制平衡探索与利用。实验结果显示，AGMR在降低受害者代理性能方面优于现有白盒对抗攻击方法，并通过对抗防御机制显著增强代理的鲁棒性。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.NI",
        "cs.RO"
      ],
      "primary_category": "cs.LG",
      "comment": "9 pages, 6 figures",
      "pdf_url": "http://arxiv.org/pdf/2503.20844v1",
      "published_date": "2025-03-26 15:08:58 UTC",
      "updated_date": "2025-03-26 15:08:58 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T06:21:31.236060"
    },
    {
      "arxiv_id": "2503.20613v1",
      "title": "State-Aware Perturbation Optimization for Robust Deep Reinforcement Learning",
      "title_zh": "翻译失败",
      "authors": [
        "Zongyuan Zhang",
        "Tianyang Duan",
        "Zheng Lin",
        "Dong Huang",
        "Zihan Fang",
        "Zekai Sun",
        "Ling Xiong",
        "Hongbin Liang",
        "Heming Cui",
        "Yong Cui"
      ],
      "abstract": "Recently, deep reinforcement learning (DRL) has emerged as a promising\napproach for robotic control. However, the deployment of DRL in real-world\nrobots is hindered by its sensitivity to environmental perturbations. While\nexisting whitebox adversarial attacks rely on local gradient information and\napply uniform perturbations across all states to evaluate DRL robustness, they\nfail to account for temporal dynamics and state-specific vulnerabilities. To\ncombat the above challenge, we first conduct a theoretical analysis of\nwhite-box attacks in DRL by establishing the adversarial victim-dynamics Markov\ndecision process (AVD-MDP), to derive the necessary and sufficient conditions\nfor a successful attack. Based on this, we propose a selective state-aware\nreinforcement adversarial attack method, named STAR, to optimize perturbation\nstealthiness and state visitation dispersion. STAR first employs a soft\nmask-based state-targeting mechanism to minimize redundant perturbations,\nenhancing stealthiness and attack effectiveness. Then, it incorporates an\ninformation-theoretic optimization objective to maximize mutual information\nbetween perturbations, environmental states, and victim actions, ensuring a\ndispersed state-visitation distribution that steers the victim agent into\nvulnerable states for maximum return reduction. Extensive experiments\ndemonstrate that STAR outperforms state-of-the-art benchmarks.",
      "tldr_zh": "该研究针对深度强化学习（DRL）在机器人控制中的敏感性问题，提出了一种状态感知扰动优化方法，以提升DRL的鲁棒性。作者首先通过建立对抗受害者动态Markov决策过程（AVD-MDP）进行理论分析，推导出成功攻击的必要和充分条件。基于此，开发了STAR方法，该方法采用软掩码机制最小化冗余扰动，并通过信息理论优化最大化扰动与环境状态、受害者动作之间的互信息，从而引导代理进入脆弱状态并减少回报。实验结果显示，STAR在多项基准测试中表现出色，优于现有攻击方法。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.NI",
        "cs.SY",
        "eess.SY"
      ],
      "primary_category": "cs.LG",
      "comment": "15 pages, 11 figures",
      "pdf_url": "http://arxiv.org/pdf/2503.20613v1",
      "published_date": "2025-03-26 15:00:07 UTC",
      "updated_date": "2025-03-26 15:00:07 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T06:21:45.205780"
    },
    {
      "arxiv_id": "2503.20607v1",
      "title": "A decision-theoretic approach to dealing with uncertainty in quantum mechanics",
      "title_zh": "基于决策理论处理量子",
      "authors": [
        "Keano De Vos",
        "Gert de Cooman",
        "Alexander Erreygers",
        "Jasper De Bock"
      ],
      "abstract": "We provide a decision-theoretic framework for dealing with uncertainty in\nquantum mechanics. This uncertainty is two-fold: on the one hand there may be\nuncertainty about the state the quantum system is in, and on the other hand, as\nis essential to quantum mechanical uncertainty, even if the quantum state is\nknown, measurements may still produce an uncertain outcome. In our framework,\nmeasurements therefore play the role of acts with an uncertain outcome and our\nsimple decision-theoretic postulates ensure that Born's rule is encapsulated in\nthe utility functions associated with such acts. This approach allows us to\nuncouple (precise) probability theory from quantum mechanics, in the sense that\nit leaves room for a more general, so-called imprecise probabilities approach.\nWe discuss the mathematical implications of our findings, which allow us to\ngive a decision-theoretic foundation to recent seminal work by Benavoli,\nFacchini and Zaffalon, and we compare our approach to earlier and different\napproaches by Deutsch and Wallace.",
      "tldr_zh": "该论文提出了一种决策理论框架（decision-theoretic framework），用于处理量子力学中的双重不确定性，包括量子状态的不确定性和测量结果的不确定性。在该框架中，测量被视为具有不确定结果的行为，通过简单的决策理论假设，将 Born's rule 整合到效用函数（utility functions）中，从而实现精确概率理论与量子力学的分离，并支持更一般的模糊概率方法（imprecise probabilities）。此外，该方法为 Benavoli 等人的相关工作提供了决策理论基础，并与 Deutsch 和 Wallace 的方法进行了比较。",
      "categories": [
        "quant-ph",
        "cs.AI",
        "math.PR"
      ],
      "primary_category": "quant-ph",
      "comment": "52 pages",
      "pdf_url": "http://arxiv.org/pdf/2503.20607v1",
      "published_date": "2025-03-26 14:53:06 UTC",
      "updated_date": "2025-03-26 14:53:06 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T06:21:57.702113"
    },
    {
      "arxiv_id": "2503.20842v1",
      "title": "Anti Robot Speciesism",
      "title_zh": "翻译失败",
      "authors": [
        "Julian De Freitas",
        "Noah Castelo",
        "Bernd Schmitt",
        "Miklos Sarvary"
      ],
      "abstract": "Humanoid robots are a form of embodied artificial intelligence (AI) that\nlooks and acts more and more like humans. Powered by generative AI and advances\nin robotics, humanoid robots can speak and interact with humans rather\nnaturally but are still easily recognizable as robots. But how will we treat\nhumanoids when they seem indistinguishable from humans in appearance and mind?\nWe find a tendency (called \"anti-robot\" speciesism) to deny such robots\nhumanlike capabilities, driven by motivations to accord members of the human\nspecies preferential treatment. Six experiments show that robots are denied\nhumanlike attributes, simply because they are not biological beings and because\nhumans want to avoid feelings of cognitive dissonance when utilizing such\nrobots for unsavory tasks. Thus, people do not rationally attribute\ncapabilities to perfectly humanlike robots but deny them capabilities as it\nsuits them.",
      "tldr_zh": "这篇论文探讨了“anti-robot speciesism”现象，即人们倾向于否认人形机器人（humanoid robots）类人能力的倾向，以优先对待人类成员。研究通过六次实验发现，这种否认主要源于机器人是非生物实体，以及人们为避免认知 dissonance（认知失调）而产生的动机，尤其是在使用机器人执行不愉快任务时。结果表明，人们不理性地赋予机器人能力，而是根据自身需求进行否认，这反映了人类在面对高度拟人化机器人时的偏见。",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.20842v1",
      "published_date": "2025-03-26 13:56:30 UTC",
      "updated_date": "2025-03-26 13:56:30 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T06:22:09.527103"
    },
    {
      "arxiv_id": "2503.20527v1",
      "title": "StableToolBench-MirrorAPI: Modeling Tool Environments as Mirrors of 7,000+ Real-World APIs",
      "title_zh": "翻译失败",
      "authors": [
        "Zhicheng Guo",
        "Sijie Cheng",
        "Yuchen Niu",
        "Hao Wang",
        "Sicheng Zhou",
        "Wenbing Huang",
        "Yang Liu"
      ],
      "abstract": "The rapid advancement of large language models (LLMs) has spurred significant\ninterest in tool learning, where LLMs are augmented with external tools to\ntackle complex tasks. However, existing tool environments face challenges in\nbalancing stability, scalability, and realness, particularly for benchmarking\npurposes. To address this problem, we propose MirrorAPI, a novel framework that\ntrains specialized LLMs to accurately simulate real API responses, effectively\nacting as \"mirrors\" to tool environments. Using a comprehensive dataset of\nrequest-response pairs from 7,000+ APIs, we employ supervised fine-tuning and\nchain-of-thought reasoning to enhance simulation fidelity. MirrorAPI achieves\nsuperior accuracy and stability compared to state-of-the-art methods, as\ndemonstrated by its performance on the newly constructed MirrorAPI-Bench and\nits integration into StableToolBench.",
      "tldr_zh": "该论文提出 MirrorAPI 框架，通过训练专门的 LLMs 来模拟 7,000+ 真实 API 的响应，将工具环境建模为“镜像”，以解决现有工具学习在稳定性和真实性上的挑战。框架采用监督 fine-tuning 和 chain-of-thought reasoning 技术，基于请求-响应数据集提升模拟的准确性。实验结果显示，MirrorAPI 在 MirrorAPI-Bench 上比现有方法表现出色，并已整合到 StableToolBench 中，为大规模工具学习基准测试提供了可靠基础。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.20527v1",
      "published_date": "2025-03-26 13:13:03 UTC",
      "updated_date": "2025-03-26 13:13:03 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T06:22:20.160547"
    },
    {
      "arxiv_id": "2503.20523v1",
      "title": "GAIA-2: A Controllable Multi-View Generative World Model for Autonomous Driving",
      "title_zh": "翻译失败",
      "authors": [
        "Lloyd Russell",
        "Anthony Hu",
        "Lorenzo Bertoni",
        "George Fedoseev",
        "Jamie Shotton",
        "Elahe Arani",
        "Gianluca Corrado"
      ],
      "abstract": "Generative models offer a scalable and flexible paradigm for simulating\ncomplex environments, yet current approaches fall short in addressing the\ndomain-specific requirements of autonomous driving - such as multi-agent\ninteractions, fine-grained control, and multi-camera consistency. We introduce\nGAIA-2, Generative AI for Autonomy, a latent diffusion world model that unifies\nthese capabilities within a single generative framework. GAIA-2 supports\ncontrollable video generation conditioned on a rich set of structured inputs:\nego-vehicle dynamics, agent configurations, environmental factors, and road\nsemantics. It generates high-resolution, spatiotemporally consistent\nmulti-camera videos across geographically diverse driving environments (UK, US,\nGermany). The model integrates both structured conditioning and external latent\nembeddings (e.g., from a proprietary driving model) to facilitate flexible and\nsemantically grounded scene synthesis. Through this integration, GAIA-2 enables\nscalable simulation of both common and rare driving scenarios, advancing the\nuse of generative world models as a core tool in the development of autonomous\nsystems. Videos are available at https://wayve.ai/thinking/gaia-2.",
      "tldr_zh": "本研究引入了 GAIA-2，一种可控的多视图生成世界模型（latent diffusion world model），旨在解决自动驾驶领域的多代理交互、细粒度控制和多摄像头一致性等需求。它支持基于结构化输入（如车辆动态、代理配置、环境因素和道路语义）的视频生成，能够在不同地理区域（如 UK、US、Germany）产生高分辨率、时空一致的多摄像头视频。GAIA-2 通过整合结构化条件和外部潜在嵌入（如来自专有驾驶模型），实现灵活的场景合成，并提升常见和罕见驾驶场景的模拟规模，为自动驾驶系统的开发提供核心工具。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.RO"
      ],
      "primary_category": "cs.CV",
      "comment": "Technical Report",
      "pdf_url": "http://arxiv.org/pdf/2503.20523v1",
      "published_date": "2025-03-26 13:11:35 UTC",
      "updated_date": "2025-03-26 13:11:35 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T06:22:33.525141"
    },
    {
      "arxiv_id": "2503.20500v3",
      "title": "Novel Deep Neural OFDM Receiver Architectures for LLR Estimation",
      "title_zh": "翻译失败",
      "authors": [
        "Erhan Karakoca",
        "Hüseyin Çevik",
        "İbrahim Hökelek",
        "Ali Görçin"
      ],
      "abstract": "Neural receivers have recently become a popular topic, where the received\nsignals can be directly decoded by data driven mechanisms such as machine\nlearning and deep learning. In this paper, we propose two novel neural network\nbased orthogonal frequency division multiplexing (OFDM) receivers performing\nchannel estimation and equalization tasks and directly predicting log\nlikelihood ratios (LLRs) from the received in phase and quadrature phase (IQ)\nsignals. The first network, the Dual Attention Transformer (DAT), employs a\nstate of the art (SOTA) transformer architecture with an attention mechanism.\nThe second network, the Residual Dual Non Local Attention Network (RDNLA),\nutilizes a parallel residual architecture with a non local attention block. The\nbit error rate (BER) and block error rate (BLER) performance of various SOTA\nneural receiver architectures is compared with our proposed methods across\ndifferent signal to noise ratio (SNR) levels. The simulation results show that\nDAT and RDNLA outperform both traditional communication systems and existing\nneural receiver models.",
      "tldr_zh": "本论文提出两种新型深度神经网络架构，用于OFDM接收器进行LLR估计，包括通道估计、均衡化任务，并直接从接收的IQ信号预测对数似然比(LLRs)。第一种架构是Dual Attention Transformer (DAT)，采用先进的Transformer结构和注意力机制；第二种是Residual Dual Non Local Attention Network (RDNLA)，使用并行残差架构和非局部注意力块。实验结果显示，在不同信噪比(SNR)水平下，DAT和RDNLA在位错误率(BER)和块错误率(BLER)上均优于传统通信系统和现有神经接收器模型。",
      "categories": [
        "eess.SP",
        "cs.AI"
      ],
      "primary_category": "eess.SP",
      "comment": "Submitted to IEEE Globecom 2025",
      "pdf_url": "http://arxiv.org/pdf/2503.20500v3",
      "published_date": "2025-03-26 12:39:56 UTC",
      "updated_date": "2025-05-08 16:41:56 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T06:22:44.011880"
    },
    {
      "arxiv_id": "2503.20492v1",
      "title": "Towards Efficient and General-Purpose Few-Shot Misclassification Detection for Vision-Language Models",
      "title_zh": "翻译失败",
      "authors": [
        "Fanhu Zeng",
        "Zhen Cheng",
        "Fei Zhu",
        "Xu-Yao Zhang"
      ],
      "abstract": "Reliable prediction by classifiers is crucial for their deployment in high\nsecurity and dynamically changing situations. However, modern neural networks\noften exhibit overconfidence for misclassified predictions, highlighting the\nneed for confidence estimation to detect errors. Despite the achievements\nobtained by existing methods on small-scale datasets, they all require training\nfrom scratch and there are no efficient and effective misclassification\ndetection (MisD) methods, hindering practical application towards large-scale\nand ever-changing datasets. In this paper, we pave the way to exploit vision\nlanguage model (VLM) leveraging text information to establish an efficient and\ngeneral-purpose misclassification detection framework. By harnessing the power\nof VLM, we construct FSMisD, a Few-Shot prompt learning framework for MisD to\nrefrain from training from scratch and therefore improve tuning efficiency. To\nenhance misclassification detection ability, we use adaptive pseudo sample\ngeneration and a novel negative loss to mitigate the issue of overconfidence by\npushing category prompts away from pseudo features. We conduct comprehensive\nexperiments with prompt learning methods and validate the generalization\nability across various datasets with domain shift. Significant and consistent\nimprovement demonstrates the effectiveness, efficiency and generalizability of\nour approach.",
      "tldr_zh": "本文针对视觉语言模型 (VLM) 的误分类检测 (MisD) 问题，提出了一种高效且通用的 Few-Shot 框架 FSMisD，以避免从零训练并提升调优效率。该框架利用 VLM 的文本信息，通过自适应伪样本生成和新型负损失来缓解模型的过度自信问题，从而增强检测能力。实验在多种数据集上进行了全面验证，展示了 FSMisD 在性能提升和跨域泛化方面的显著优势，为大规模动态场景下的可靠预测提供了实用解决方案。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "preprint",
      "pdf_url": "http://arxiv.org/pdf/2503.20492v1",
      "published_date": "2025-03-26 12:31:04 UTC",
      "updated_date": "2025-03-26 12:31:04 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T06:22:56.885283"
    },
    {
      "arxiv_id": "2503.20485v1",
      "title": "Underwater Image Enhancement by Convolutional Spiking Neural Networks",
      "title_zh": "翻译失败",
      "authors": [
        "Vidya Sudevan",
        "Fakhreddine Zayer",
        "Rizwana Kausar",
        "Sajid Javed",
        "Hamad Karki",
        "Giulia De Masi",
        "Jorge Dias"
      ],
      "abstract": "Underwater image enhancement (UIE) is fundamental for marine applications,\nincluding autonomous vision-based navigation. Deep learning methods using\nconvolutional neural networks (CNN) and vision transformers advanced UIE\nperformance. Recently, spiking neural networks (SNN) have gained attention for\ntheir lightweight design, energy efficiency, and scalability. This paper\nintroduces UIE-SNN, the first SNN-based UIE algorithm to improve visibility of\nunderwater images. UIE-SNN is a 19- layered convolutional spiking\nencoder-decoder framework with skip connections, directly trained using\nsurrogate gradient-based backpropagation through time (BPTT) strategy. We\nexplore and validate the influence of training datasets on energy reduction, a\nunique advantage of UIE-SNN architecture, in contrast to the conventional\nlearning-based architectures, where energy consumption is model-dependent.\nUIE-SNN optimizes the loss function in latent space representation to\nreconstruct clear underwater images. Our algorithm performs on par with its\nnon-spiking counterpart methods in terms of PSNR and structural similarity\nindex (SSIM) at reduced timesteps ($T=5$) and energy consumption of $85\\%$. The\nalgorithm is trained on two publicly available benchmark datasets, UIEB and\nEUVP, and tested on unseen images from UIEB, EUVP, LSUI, U45, and our custom\nUIE dataset. The UIE-SNN algorithm achieves PSNR of \\(17.7801~dB\\) and SSIM of\n\\(0.7454\\) on UIEB, and PSNR of \\(23.1725~dB\\) and SSIM of \\(0.7890\\) on EUVP.\nUIE-SNN achieves this algorithmic performance with fewer operators (\\(147.49\\)\nGSOPs) and energy (\\(0.1327~J\\)) compared to its non-spiking counterpart\n(GFLOPs = \\(218.88\\) and Energy=\\(1.0068~J\\)). Compared with existing SOTA UIE\nmethods, UIE-SNN achieves an average of \\(6.5\\times\\) improvement in energy\nefficiency. The source code is available at\n\\href{https://github.com/vidya-rejul/UIE-SNN.git}{UIE-SNN}.",
      "tldr_zh": "这篇论文提出了一种基于卷积脉冲神经网络（SNN）的水下图像增强（UIE）算法，名为 UIE-SNN，这是首个针对 UIE 的 SNN 方法，旨在提升水下图像可见度并提高能量效率。UIE-SNN 采用一个 19 层卷积编码器-解码器框架，结合跳跃连接和代理梯度-based 反向传播通过时间（BPTT）策略直接训练，优化潜空间表示以重建清晰图像。实验结果显示，该算法在减少时间步（T=5）和 85% 能量消耗的情况下，在 PSNR 和 SSIM 指标上与非 SNN 方法相当，并在 UIEB 和 EUVP 数据集上分别达到 PSNR 为 17.78 dB 和 23.17 dB；相比现有 SOTA 方法，UIE-SNN 实现了平均 6.5 倍的能量效率改进。",
      "categories": [
        "eess.IV",
        "cs.AI",
        "cs.PF"
      ],
      "primary_category": "eess.IV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.20485v1",
      "published_date": "2025-03-26 12:15:38 UTC",
      "updated_date": "2025-03-26 12:15:38 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T06:23:11.438207"
    },
    {
      "arxiv_id": "2503.20484v1",
      "title": "Contrastive Learning Guided Latent Diffusion Model for Image-to-Image Translation",
      "title_zh": "基于对比学习的潜在扩散模型",
      "authors": [
        "Qi Si",
        "Bo Wang",
        "Zhao Zhang"
      ],
      "abstract": "The diffusion model has demonstrated superior performance in synthesizing\ndiverse and high-quality images for text-guided image translation. However,\nthere remains room for improvement in both the formulation of text prompts and\nthe preservation of reference image content. First, variations in target text\nprompts can significantly influence the quality of the generated images, and it\nis often challenging for users to craft an optimal prompt that fully captures\nthe content of the input image. Second, while existing models can introduce\ndesired modifications to specific regions of the reference image, they\nfrequently induce unintended alterations in areas that should remain unchanged.\nTo address these challenges, we propose pix2pix-zeroCon, a zero-shot\ndiffusion-based method that eliminates the need for additional training by\nleveraging patch-wise contrastive loss. Specifically, we automatically\ndetermine the editing direction in the text embedding space based on the\nreference image and target prompts. Furthermore, to ensure precise content and\nstructural preservation in the edited image, we introduce cross-attention\nguiding loss and patch-wise contrastive loss between the generated and original\nimage embeddings within a pre-trained diffusion model. Notably, our approach\nrequires no additional training and operates directly on a pre-trained\ntext-to-image diffusion model. Extensive experiments demonstrate that our\nmethod surpasses existing models in image-to-image translation, achieving\nenhanced fidelity and controllability.",
      "tldr_zh": "该研究针对扩散模型在图像到图像翻译中的问题，提出了一种零-shot 方法 pix2pix-zeroCon，以改善文本提示制定和参考图像内容保留。方法通过自动确定文本嵌入空间的编辑方向，并引入 cross-attention guiding loss 和 patch-wise contrastive loss，确保生成图像在指定区域的精确修改，同时保持不变区域的保真度。该方法无需额外训练，直接基于预训练的文本到图像扩散模型进行操作，实验结果显示其在图像翻译任务中超过了现有模型，实现了更高的保真度和可控性。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "11 pages, 13 figures",
      "pdf_url": "http://arxiv.org/pdf/2503.20484v1",
      "published_date": "2025-03-26 12:15:25 UTC",
      "updated_date": "2025-03-26 12:15:25 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T06:23:20.505179"
    },
    {
      "arxiv_id": "2503.20479v1",
      "title": "A multi-agentic framework for real-time, autonomous freeform metasurface design",
      "title_zh": "翻译失败",
      "authors": [
        "Robert Lupoiu",
        "Yixuan Shao",
        "Tianxiang Dai",
        "Chenkai Mao",
        "Kofi Edee",
        "Jonathan A. Fan"
      ],
      "abstract": "Innovation in nanophotonics currently relies on human experts who synergize\nspecialized knowledge in photonics and coding with simulation and optimization\nalgorithms, entailing design cycles that are time-consuming, computationally\ndemanding, and frequently suboptimal. We introduce MetaChat, a multi-agentic\ndesign framework that can translate semantically described photonic design\ngoals into high-performance, freeform device layouts in an automated, nearly\nreal-time manner. Multi-step reasoning is enabled by our Agentic Iterative\nMonologue (AIM) paradigm, which coherently interfaces agents with code-based\ntools, other specialized agents, and human designers. Design acceleration is\nfacilitated by Feature-wise Linear Modulation-conditioned Maxwell surrogate\nsolvers that support the generalized evaluation of metasurface structures. We\nuse freeform dielectric metasurfaces as a model system and demonstrate with\nMetaChat the design of multi-objective, multi-wavelength metasurfaces orders of\nmagnitude faster than conventional methods. These concepts present a scientific\ncomputing blueprint for utilizing specialist design agents, surrogate solvers,\nand human interactions to drive multi-physics innovation and discovery.",
      "tldr_zh": "本论文提出MetaChat，一种多智能体框架，用于实现实时自主的自由形式亚表面设计，能够将语义描述的光子设计目标自动转化为高性能设备布局。框架采用Agentic Iterative Monologue (AIM)范式，支持多步推理，并通过Feature-wise Linear Modulation-conditioned Maxwell surrogate solvers加速评估和优化设计过程。相比传统方法，MetaChat在多目标、多波长自由形式介电亚表面设计上快几个数量级。整体框架为利用专业智能体、代理求解器和人类交互驱动多物理创新提供科学计算蓝图。",
      "categories": [
        "physics.app-ph",
        "cs.AI",
        "cs.MA",
        "physics.comp-ph"
      ],
      "primary_category": "physics.app-ph",
      "comment": "32 pages, 5 figures",
      "pdf_url": "http://arxiv.org/pdf/2503.20479v1",
      "published_date": "2025-03-26 12:10:45 UTC",
      "updated_date": "2025-03-26 12:10:45 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T06:23:33.469467"
    },
    {
      "arxiv_id": "2503.20472v1",
      "title": "From Trial to Triumph: Advancing Long Video Understanding via Visual Context Sample Scaling and Self-reward Alignment",
      "title_zh": "翻译失败",
      "authors": [
        "Yucheng Suo",
        "Fan Ma",
        "Linchao Zhu",
        "Tianyi Wang",
        "Fengyun Rao",
        "Yi Yang"
      ],
      "abstract": "Multi-modal Large language models (MLLMs) show remarkable ability in video\nunderstanding. Nevertheless, understanding long videos remains challenging as\nthe models can only process a finite number of frames in a single inference,\npotentially omitting crucial visual information. To address the challenge, we\npropose generating multiple predictions through visual context sampling,\nfollowed by a scoring mechanism to select the final prediction. Specifically,\nwe devise a bin-wise sampling strategy that enables MLLMs to generate diverse\nanswers based on various combinations of keyframes, thereby enriching the\nvisual context. To determine the final prediction from the sampled answers, we\nemploy a self-reward by linearly combining three scores: (1) a frequency score\nindicating the prevalence of each option, (2) a marginal confidence score\nreflecting the inter-intra sample certainty of MLLM predictions, and (3) a\nreasoning score for different question types, including clue-guided answering\nfor global questions and temporal self-refocusing for local questions. The\nfrequency score ensures robustness through majority correctness, the\nconfidence-aligned score reflects prediction certainty, and the typed-reasoning\nscore addresses cases with sparse key visual information using tailored\nstrategies. Experiments show that this approach covers the correct answer for a\nhigh percentage of long video questions, on seven datasets show that our method\nimproves the performance of three MLLMs.",
      "tldr_zh": "该研究针对多模态大语言模型（MLLMs）在长视频理解中的挑战，提出了一种通过视觉上下文样本缩放和自奖励对齐的方法，以缓解有限帧处理导致的关键信息遗漏问题。具体而言，该方法采用 bin-wise sampling 策略，让 MLLMs 基于不同关键帧组合生成多样答案，从而丰富视觉上下文；随后，通过线性结合 frequency score（反映选项流行度）、marginal confidence score（评估预测确定性）和 reasoning score（针对全局或局部问题的特定策略）来选择最终预测。实验结果显示，该方法在七个数据集上显著提升了三个 MLLMs 的性能，证明其在提高长视频理解准确性和鲁棒性方面的有效性。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.20472v1",
      "published_date": "2025-03-26 11:53:03 UTC",
      "updated_date": "2025-03-26 11:53:03 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T06:23:45.779690"
    },
    {
      "arxiv_id": "2503.20446v1",
      "title": "Attention Xception UNet (AXUNet): A Novel Combination of CNN and Self-Attention for Brain Tumor Segmentation",
      "title_zh": "翻译失败",
      "authors": [
        "Farzan Moodi",
        "Fereshteh Khodadadi Shoushtari",
        "Gelareh Valizadeh",
        "Dornaz Mazinani",
        "Hanieh Mobarak Salari",
        "Hamidreza Saligheh Rad"
      ],
      "abstract": "Accurate segmentation of glioma brain tumors is crucial for diagnosis and\ntreatment planning. Deep learning techniques offer promising solutions, but\noptimal model architectures remain under investigation. We used the BraTS 2021\ndataset, selecting T1 with contrast enhancement (T1CE), T2, and\nFluid-Attenuated Inversion Recovery (FLAIR) sequences for model development.\nThe proposed Attention Xception UNet (AXUNet) architecture integrates an\nXception backbone with dot-product self-attention modules, inspired by\nstate-of-the-art (SOTA) large language models such as Google Bard and OpenAI\nChatGPT, within a UNet-shaped model. We compared AXUNet with SOTA models.\nComparative evaluation on the test set demonstrated improved results over\nbaseline models. Inception-UNet and Xception-UNet achieved mean Dice scores of\n90.88 and 93.24, respectively. Attention ResUNet (AResUNet) attained a mean\nDice score of 92.80, with the highest score of 84.92 for enhancing tumor (ET)\namong all models. Attention Gate UNet (AGUNet) yielded a mean Dice score of\n90.38. AXUNet outperformed all models with a mean Dice score of 93.73. It\ndemonstrated superior Dice scores across whole tumor (WT) and tumor core (TC)\nregions, achieving 92.59 for WT, 86.81 for TC, and 84.89 for ET. The\nintegration of the Xception backbone and dot-product self-attention mechanisms\nin AXUNet showcases enhanced performance in capturing spatial and contextual\ninformation. The findings underscore the potential utility of AXUNet in\nfacilitating precise tumor delineation.",
      "tldr_zh": "该研究提出了一种新型脑肿瘤分割模型 Attention Xception UNet (AXUNet)，将 Xception 骨干网与点积自注意力机制结合，基于 UNet 结构，旨在提升胶质瘤肿瘤的精确分割，使用 BraTS 2021 数据集中的 T1CE、T2 和 FLAIR 序列进行开发。AXUNet 受大型语言模型如 Google Bard 和 OpenAI ChatGPT 的启发，通过整合 CNN 和自注意力模块，显著提高了空间和上下文信息的捕获能力。在比较实验中，AXUNet 超越了基线模型，如 Inception-UNet 和 AResUNet，取得了最高的平均 Dice 分数（93.73），在整体肿瘤 (WT)、肿瘤核心 (TC) 和增强肿瘤 (ET) 区域分别达到 92.59、86.81 和 84.89。该方法的性能提升突显了其在诊断和治疗规划中的潜在应用价值。",
      "categories": [
        "eess.IV",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "eess.IV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.20446v1",
      "published_date": "2025-03-26 11:22:17 UTC",
      "updated_date": "2025-03-26 11:22:17 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T06:23:59.995692"
    },
    {
      "arxiv_id": "2503.20428v1",
      "title": "Evaluating Facial Expression Recognition Datasets for Deep Learning: A Benchmark Study with Novel Similarity Metrics",
      "title_zh": "评估面部表情识别数据集用于深度学习：一项采用",
      "authors": [
        "F. Xavier Gaya-Morey",
        "Cristina Manresa-Yee",
        "Célia Martinie",
        "Jose M. Buades-Rubio"
      ],
      "abstract": "This study investigates the key characteristics and suitability of widely\nused Facial Expression Recognition (FER) datasets for training deep learning\nmodels. In the field of affective computing, FER is essential for interpreting\nhuman emotions, yet the performance of FER systems is highly contingent on the\nquality and diversity of the underlying datasets. To address this issue, we\ncompiled and analyzed 24 FER datasets, including those targeting specific age\ngroups such as children, adults, and the elderly, and processed them through a\ncomprehensive normalization pipeline. In addition, we enriched the datasets\nwith automatic annotations for age and gender, enabling a more nuanced\nevaluation of their demographic properties. To further assess dataset efficacy,\nwe introduce three novel metricsLocal, Global, and Paired Similarity, which\nquantitatively measure dataset difficulty, generalization capability, and\ncross-dataset transferability. Benchmark experiments using state-of-the-art\nneural networks reveal that large-scale, automatically collected datasets\n(e.g., AffectNet, FER2013) tend to generalize better, despite issues with\nlabeling noise and demographic biases, whereas controlled datasets offer higher\nannotation quality but limited variability. Our findings provide actionable\nrecommendations for dataset selection and design, advancing the development of\nmore robust, fair, and effective FER systems.",
      "tldr_zh": "本研究评估了24个Facial Expression Recognition (FER)数据集的关键特征和适用性，用于训练深度学习模型，通过归一化管道和自动注释（如年龄和性别）进行处理。研究引入了三个新指标—Local Similarity、Global Similarity和Paired Similarity—来量化数据集的难度、泛化能力和跨数据集转移能力。基准实验使用最先进的神经网络发现，大规模自动收集数据集（如AffectNet和FER2013）具有更好的泛化性，但存在标签噪声和人口统计偏差，而控制型数据集则提供更高注释质量但变异性有限。该研究为FER系统的数据集选择和设计提供了可操作建议，推动更鲁棒、公平和有效的系统发展。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.20428v1",
      "published_date": "2025-03-26 11:01:00 UTC",
      "updated_date": "2025-03-26 11:01:00 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T06:24:11.719450"
    },
    {
      "arxiv_id": "2503.20425v1",
      "title": "Perspective-Shifted Neuro-Symbolic World Models: A Framework for Socially-Aware Robot Navigation",
      "title_zh": "翻译失败",
      "authors": [
        "Kevin Alcedo",
        "Pedro U. Lima",
        "Rachid Alami"
      ],
      "abstract": "Navigating in environments alongside humans requires agents to reason under\nuncertainty and account for the beliefs and intentions of those around them.\nUnder a sequential decision-making framework, egocentric navigation can\nnaturally be represented as a Markov Decision Process (MDP). However, social\nnavigation additionally requires reasoning about the hidden beliefs of others,\ninherently leading to a Partially Observable Markov Decision Process (POMDP),\nwhere agents lack direct access to others' mental states. Inspired by Theory of\nMind and Epistemic Planning, we propose (1) a neuro-symbolic model-based\nreinforcement learning architecture for social navigation, addressing the\nchallenge of belief tracking in partially observable environments; and (2) a\nperspective-shift operator for belief estimation, leveraging recent work on\nInfluence-based Abstractions (IBA) in structured multi-agent settings.",
      "tldr_zh": "这篇论文提出了一种Perspective-Shifted Neuro-Symbolic World Models框架，用于实现社会感知的机器人导航，旨在处理不确定环境中的他人信念和意图。框架将导航建模为Partially Observable Markov Decision Process (POMDP)，并引入神经符号模型的强化学习架构来跟踪隐藏信念。论文的关键贡献包括一个视角转换操作符，利用Influence-based Abstractions (IBA)进行信念估计，基于Theory of Mind和Epistemic Planning，提升了机器人在多智能体环境中的导航准确性和社会适应性。",
      "categories": [
        "cs.AI",
        "cs.RO"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.20425v1",
      "published_date": "2025-03-26 10:59:08 UTC",
      "updated_date": "2025-03-26 10:59:08 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T06:24:21.610046"
    },
    {
      "arxiv_id": "2503.20398v1",
      "title": "Including local feature interactions in deep non-negative matrix factorization networks improves performance",
      "title_zh": "翻译失败",
      "authors": [
        "Mahbod Nouri",
        "David Rotermund",
        "Alberto Garcia-Ortiz",
        "Klaus R. Pawelzik"
      ],
      "abstract": "The brain uses positive signals as a means of signaling. Forward interactions\nin the early visual cortex are also positive, realized by excitatory synapses.\nOnly local interactions also include inhibition. Non-negative matrix\nfactorization (NMF) captures the biological constraint of positive long-range\ninteractions and can be implemented with stochastic spikes. While NMF can serve\nas an abstract formalization of early neural processing in the visual system,\nthe performance of deep convolutional networks with NMF modules does not match\nthat of CNNs of similar size. However, when the local NMF modules are each\nfollowed by a module that mixes the NMF's positive activities, the performances\non the benchmark data exceed that of vanilla deep convolutional networks of\nsimilar size. This setting can be considered a biologically more plausible\nemulation of the processing in cortical (hyper-)columns with the potential to\nimprove the performance of deep networks.",
      "tldr_zh": "该研究发现，将局部特征互动纳入深度非负矩阵分解 (NMF) 网络中，能显著提升其性能。论文指出，虽然标准NMF模块的深度卷积网络表现不如类似大小的CNNs，但在其后添加一个混合NMF正活动的模块后，网络在基准数据集上的表现超过了传统深度卷积网络。这种方法更符合大脑视觉皮层（如皮层柱）的生物学机制，提供了一种更可信的神经处理模拟，有望优化深度网络的设计。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.20398v1",
      "published_date": "2025-03-26 10:21:38 UTC",
      "updated_date": "2025-03-26 10:21:38 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T06:24:33.226111"
    },
    {
      "arxiv_id": "2503.20394v1",
      "title": "FastFT: Accelerating Reinforced Feature Transformation via Advanced Exploration Strategies",
      "title_zh": "FastFT：通过高级探索策略加速强化特征转换",
      "authors": [
        "Tianqi He",
        "Xiaohan Huang",
        "Yi Du",
        "Qingqing Long",
        "Ziyue Qiao",
        "Min Wu",
        "Yanjie Fu",
        "Yuanchun Zhou",
        "Meng Xiao"
      ],
      "abstract": "Feature Transformation is crucial for classic machine learning that aims to\ngenerate feature combinations to enhance the performance of downstream tasks\nfrom a data-centric perspective. Current methodologies, such as manual\nexpert-driven processes, iterative-feedback techniques, and\nexploration-generative tactics, have shown promise in automating such data\nengineering workflow by minimizing human involvement. However, three challenges\nremain in those frameworks: (1) It predominantly depends on downstream task\nperformance metrics, as assessment is time-consuming, especially for large\ndatasets. (2) The diversity of feature combinations will hardly be guaranteed\nafter random exploration ends. (3) Rare significant transformations lead to\nsparse valuable feedback that hinders the learning processes or leads to less\neffective results. In response to these challenges, we introduce FastFT, an\ninnovative framework that leverages a trio of advanced strategies.We first\ndecouple the feature transformation evaluation from the outcomes of the\ngenerated datasets via the performance predictor. To address the issue of\nreward sparsity, we developed a method to evaluate the novelty of generated\ntransformation sequences. Incorporating this novelty into the reward function\naccelerates the model's exploration of effective transformations, thereby\nimproving the search productivity. Additionally, we combine novelty and\nperformance to create a prioritized memory buffer, ensuring that essential\nexperiences are effectively revisited during exploration. Our extensive\nexperimental evaluations validate the performance, efficiency, and traceability\nof our proposed framework, showcasing its superiority in handling complex\nfeature transformation tasks.",
      "tldr_zh": "论文提出 FastFT 框架，通过高级探索策略加速强化特征转换（Reinforced Feature Transformation），以解决现有方法在评估耗时、特征组合多样性不足和奖励稀疏等问题上的挑战。FastFT 首先使用性能预测器（performance predictor）将特征转换评估从下游任务结果中解耦，其次通过评估生成转换序列的新颖性（novelty）并融入奖励函数，加快模型对有效转换的探索，最后结合新颖性和性能创建优先级内存缓冲区（priorized memory buffer），确保关键经验被高效回顾。实验结果显示，FastFT 在处理复杂特征转换任务时表现出色，在性能、效率和可追溯性方面优于基线方法。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "14 pages, Accepted by ICDE 2025",
      "pdf_url": "http://arxiv.org/pdf/2503.20394v1",
      "published_date": "2025-03-26 10:17:41 UTC",
      "updated_date": "2025-03-26 10:17:41 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T06:24:46.521982"
    },
    {
      "arxiv_id": "2503.20384v2",
      "title": "MoLe-VLA: Dynamic Layer-skipping Vision Language Action Model via Mixture-of-Layers for Efficient Robot Manipulation",
      "title_zh": "翻译失败",
      "authors": [
        "Rongyu Zhang",
        "Menghang Dong",
        "Yuan Zhang",
        "Liang Heng",
        "Xiaowei Chi",
        "Gaole Dai",
        "Li Du",
        "Yuan Du",
        "Shanghang Zhang"
      ],
      "abstract": "Multimodal Large Language Models (MLLMs) excel in understanding complex\nlanguage and visual data, enabling generalist robotic systems to interpret\ninstructions and perform embodied tasks. Nevertheless, their real-world\ndeployment is hindered by substantial computational and storage demands. Recent\ninsights into the homogeneous patterns in the LLM layer have inspired\nsparsification techniques to address these challenges, such as early exit and\ntoken pruning. However, these methods often neglect the critical role of the\nfinal layers that encode the semantic information most relevant to downstream\nrobotic tasks. Aligning with the recent breakthrough of the Shallow Brain\nHypothesis (SBH) in neuroscience and the mixture of experts in model\nsparsification, we conceptualize each LLM layer as an expert and propose a\nMixture-of-Layers Vision-Language-Action model (MoLe-VLA, or simply MoLe)\narchitecture for dynamic LLM layer activation. We introduce a Spatial-Temporal\nAware Router (STAR) for MoLe to selectively activate only parts of the layers\nbased on the robot's current state, mimicking the brain's distinct signal\npathways specialized for cognition and causal reasoning. Additionally, to\ncompensate for the cognitive ability of LLMs lost in MoLe, we devise a\nCognition Self-Knowledge Distillation (CogKD) framework. CogKD enhances the\nunderstanding of task demands and improves the generation of task-relevant\naction sequences by leveraging cognitive features. Extensive experiments\nconducted in both RLBench simulation and real-world environments demonstrate\nthe superiority of MoLe-VLA in both efficiency and performance. Specifically,\nMoLe-VLA achieves an 8% improvement in the mean success rate across ten tasks\nwhile reducing computational costs by up to x5.6 compared to standard LLMs.",
      "tldr_zh": "本研究提出 MoLe-VLA 模型，一种基于 Mixture-of-Layers 的动态层跳过架构，用于优化 Multimodal Large Language Models (MLLMs) 在机器人操作中的计算效率，同时解决其高资源消耗问题。模型引入 Spatial-Temporal Aware Router (STAR) 来根据机器人当前状态选择激活部分层，以及 Cognition Self-Knowledge Distillation (CogKD) 框架来补偿认知能力损失，从而提升任务理解和行动序列生成。实验结果显示，在 RLBench 模拟和真实环境中，MoLe-VLA 比标准 LLMs 提高了 8% 的平均成功率，并将计算成本降低高达 5.6 倍。",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.20384v2",
      "published_date": "2025-03-26 10:05:38 UTC",
      "updated_date": "2025-04-14 11:39:39 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T06:24:58.817858"
    },
    {
      "arxiv_id": "2503.20348v1",
      "title": "VideoGEM: Training-free Action Grounding in Videos",
      "title_zh": "翻译失败",
      "authors": [
        "Felix Vogel",
        "Walid Bousselham",
        "Anna Kukleva",
        "Nina Shvetsova",
        "Hilde Kuehne"
      ],
      "abstract": "Vision-language foundation models have shown impressive capabilities across\nvarious zero-shot tasks, including training-free localization and grounding,\nprimarily focusing on localizing objects in images. However, leveraging those\ncapabilities to localize actions and events in videos is challenging, as\nactions have less physical outline and are usually described by higher-level\nconcepts. In this work, we propose VideoGEM, the first training-free spatial\naction grounding method based on pretrained image- and video-language\nbackbones. Namely, we adapt the self-self attention formulation of GEM to\nspatial activity grounding. We observe that high-level semantic concepts, such\nas actions, usually emerge in the higher layers of the image- and\nvideo-language models. We, therefore, propose a layer weighting in the\nself-attention path to prioritize higher layers. Additionally, we introduce a\ndynamic weighting method to automatically tune layer weights to capture each\nlayer`s relevance to a specific prompt. Finally, we introduce a prompt\ndecomposition, processing action, verb, and object prompts separately,\nresulting in a better spatial localization of actions. We evaluate the proposed\napproach on three image- and video-language backbones, CLIP, OpenCLIP, and\nViCLIP, and on four video grounding datasets, V-HICO, DALY,\nYouCook-Interactions, and GroundingYouTube, showing that the proposed\ntraining-free approach is able to outperform current trained state-of-the-art\napproaches for spatial video grounding.",
      "tldr_zh": "该研究提出了 VideoGEM，一种无需训练的视频动作定位方法，基于预训练的图像和视频语言模型（如 CLIP、OpenCLIP 和 ViCLIP），旨在解决动作在视频中的空间定位挑战，因为动作通常缺乏物理轮廓且涉及高层语义概念。方法包括适应 GEM 的自注意力公式、引入层权重机制优先处理高层语义、动态权重调整以优化提示相关性，以及提示分解将动作、动词和物体提示分开处理。这些创新使 VideoGEM 在 V-HICO、DALY、YouCook-Interactions 和 GroundingYouTube 数据集上超越了现有的训练型最先进方法，证明了其在训练-free 场景下的有效性。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CL",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.20348v1",
      "published_date": "2025-03-26 09:20:30 UTC",
      "updated_date": "2025-03-26 09:20:30 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T06:25:09.338124"
    },
    {
      "arxiv_id": "2503.20341v1",
      "title": "Wasserstein Distributionally Robust Bayesian Optimization with Continuous Context",
      "title_zh": "翻译失败",
      "authors": [
        "Francesco Micheli",
        "Efe C. Balta",
        "Anastasios Tsiamis",
        "John Lygeros"
      ],
      "abstract": "We address the challenge of sequential data-driven decision-making under\ncontext distributional uncertainty. This problem arises in numerous real-world\nscenarios where the learner optimizes black-box objective functions in the\npresence of uncontrollable contextual variables. We consider the setting where\nthe context distribution is uncertain but known to lie within an ambiguity set\ndefined as a ball in the Wasserstein distance. We propose a novel algorithm for\nWasserstein Distributionally Robust Bayesian Optimization that can handle\ncontinuous context distributions while maintaining computational tractability.\nOur theoretical analysis combines recent results in self-normalized\nconcentration in Hilbert spaces and finite-sample bounds for distributionally\nrobust optimization to establish sublinear regret bounds that match\nstate-of-the-art results. Through extensive comparisons with existing\napproaches on both synthetic and real-world problems, we demonstrate the\nsimplicity, effectiveness, and practical applicability of our proposed method.",
      "tldr_zh": "本研究针对上下文分布不确定性的顺序数据驱动决策问题，提出了一种Wasserstein Distributionally Robust Bayesian Optimization算法，用于优化黑盒目标函数，同时处理连续上下文分布。该算法利用Wasserstein distance定义的模糊集（ambiguity set），确保计算上的可行性，并结合Hilbert空间的自归一化集中结果和分布鲁棒优化的有限样本界限，建立了匹配最先进次线性遗憾界（sublinear regret bounds）的理论保证。通过在合成和真实世界问题上的广泛比较，实验结果证明了该方法的简单性、有效性和实际适用性。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "stat.ML"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.20341v1",
      "published_date": "2025-03-26 09:11:17 UTC",
      "updated_date": "2025-03-26 09:11:17 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T06:25:20.918278"
    },
    {
      "arxiv_id": "2503.20320v1",
      "title": "Iterative Prompting with Persuasion Skills in Jailbreaking Large Language Models",
      "title_zh": "迭代提示与说服技能在大型语言模型",
      "authors": [
        "Shih-Wen Ke",
        "Guan-Yu Lai",
        "Guo-Lin Fang",
        "Hsi-Yuan Kao"
      ],
      "abstract": "Large language models (LLMs) are designed to align with human values in their\nresponses. This study exploits LLMs with an iterative prompting technique where\neach prompt is systematically modified and refined across multiple iterations\nto enhance its effectiveness in jailbreaking attacks progressively. This\ntechnique involves analyzing the response patterns of LLMs, including GPT-3.5,\nGPT-4, LLaMa2, Vicuna, and ChatGLM, allowing us to adjust and optimize prompts\nto evade the LLMs' ethical and security constraints. Persuasion strategies\nenhance prompt effectiveness while maintaining consistency with malicious\nintent. Our results show that the attack success rates (ASR) increase as the\nattacking prompts become more refined with the highest ASR of 90% for GPT4 and\nChatGLM and the lowest ASR of 68% for LLaMa2. Our technique outperforms\nbaseline techniques (PAIR and PAP) in ASR and shows comparable performance with\nGCG and ArtPrompt.",
      "tldr_zh": "这篇论文探讨了利用迭代提示技术结合说服策略来攻击大型语言模型 (LLMs)，以逐步绕过其道德和安全约束。研究方法涉及分析 LLMs（如 GPT-3.5、GPT-4、LLaMa2、Vicuna 和 ChatGLM）的响应模式，并系统优化提示以增强攻击效果，同时保持恶意意图的一致性。结果显示，该技术使攻击成功率 (ASR) 最高达到 90%（针对 GPT-4 和 ChatGLM），并在 ASR 上优于基线方法（如 PAIR 和 PAP），与 GCG 和 ArtPrompt 性能相当。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.ET"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.20320v1",
      "published_date": "2025-03-26 08:40:46 UTC",
      "updated_date": "2025-03-26 08:40:46 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T06:25:33.967350"
    },
    {
      "arxiv_id": "2503.22729v1",
      "title": "Ancestral Mamba: Enhancing Selective Discriminant Space Model with Online Visual Prototype Learning for Efficient and Robust Discriminant Approach",
      "title_zh": "Ancestral Mamba：通过在线视觉原型学习增强选择性判别空间模型，以实现高效且鲁棒的判别方法",
      "authors": [
        "Jiahao Qin",
        "Feng Liu",
        "Lu Zong"
      ],
      "abstract": "In the realm of computer graphics, the ability to learn continuously from\nnon-stationary data streams while adapting to new visual patterns and\nmitigating catastrophic forgetting is of paramount importance. Existing\napproaches often struggle to capture and represent the essential\ncharacteristics of evolving visual concepts, hindering their applicability to\ndynamic graphics tasks. In this paper, we propose Ancestral Mamba, a novel\napproach that integrates online prototype learning into a selective\ndiscriminant space model for efficient and robust online continual learning.\nThe key components of our approach include Ancestral Prototype Adaptation\n(APA), which continuously refines and builds upon learned visual prototypes,\nand Mamba Feedback (MF), which provides targeted feedback to adapt to\nchallenging visual patterns. APA enables the model to continuously adapt its\nprototypes, building upon ancestral knowledge to tackle new challenges, while\nMF acts as a targeted feedback mechanism, focusing on challenging classes and\nrefining their representations. Extensive experiments on graphics-oriented\ndatasets, such as CIFAR-10 and CIFAR-100, demonstrate the superior performance\nof Ancestral Mamba compared to state-of-the-art baselines, achieving\nsignificant improvements in accuracy and forgetting mitigation.",
      "tldr_zh": "这篇论文提出了 Ancestral Mamba，一种将在线视觉原型学习集成到选择性判别空间模型中的新方法，旨在解决计算机图形学中在线持续学习面临的灾难性遗忘和适应新视觉模式的问题。关键组件包括 Ancestral Prototype Adaptation (APA)，它通过持续改进和构建视觉原型来利用祖先知识处理新挑战，以及 Mamba Feedback (MF)，提供针对性反馈以优化挑战性类别的表示。实验在 CIFAR-10 和 CIFAR-100 等数据集上显示，Ancestral Mamba 相较于现有基线方法显著提高了准确率并有效缓解了遗忘问题。",
      "categories": [
        "cs.GR",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.GR",
      "comment": "10 pages, 3 figures",
      "pdf_url": "http://arxiv.org/pdf/2503.22729v1",
      "published_date": "2025-03-26 08:36:05 UTC",
      "updated_date": "2025-03-26 08:36:05 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T06:25:48.033017"
    },
    {
      "arxiv_id": "2503.20302v2",
      "title": "A Multilingual, Culture-First Approach to Addressing Misgendering in LLM Applications",
      "title_zh": "翻译失败",
      "authors": [
        "Sunayana Sitaram",
        "Adrian de Wynter",
        "Isobel McCrum",
        "Qilong Gu",
        "Si-Qing Chen"
      ],
      "abstract": "Misgendering is the act of referring to someone by a gender that does not\nmatch their chosen identity. It marginalizes and undermines a person's sense of\nself, causing significant harm. English-based approaches have clear-cut\napproaches to avoiding misgendering, such as the use of the pronoun ``they''.\nHowever, other languages pose unique challenges due to both grammatical and\ncultural constructs. In this work we develop methodologies to assess and\nmitigate misgendering across 42 languages and dialects using a\nparticipatory-design approach to design effective and appropriate guardrails\nacross all languages. We test these guardrails in a standard LLM-based\napplication (meeting transcript summarization), where both the data generation\nand the annotation steps followed a human-in-the-loop approach. We find that\nthe proposed guardrails are very effective in reducing misgendering rates\nacross all languages in the summaries generated, and without incurring loss of\nquality. Our human-in-the-loop approach demonstrates a method to feasibly scale\ninclusive and responsible AI-based solutions across multiple languages and\ncultures. We release the guardrails and synthetic dataset encompassing 42\nlanguages, along with human and LLM-judge evaluations, to encourage further\nresearch on this subject.",
      "tldr_zh": "这篇论文提出了一种以文化为先的多语言方法，针对LLM应用中的misgendering问题（即使用不匹配性别身份的称呼）进行评估和缓解，涵盖42种语言的语法和文化挑战。研究采用参与式设计和human-in-the-loop方法，开发了有效的guardrails，并在会议记录摘要任务中测试，结果显示这些防护措施显著降低了misgendering率，同时不影响输出质量。论文发布了guardrails、合成数据集以及人类和LLM评估结果，以推动更多包容性AI解决方案的研究。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.20302v2",
      "published_date": "2025-03-26 08:01:35 UTC",
      "updated_date": "2025-05-21 05:39:43 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T06:25:57.940226"
    },
    {
      "arxiv_id": "2503.20294v2",
      "title": "Context-Aware Weakly Supervised Image Manipulation Localization with SAM Refinement",
      "title_zh": "翻译失败",
      "authors": [
        "Xinghao Wang",
        "Tao Gong",
        "Qi Chu",
        "Bin Liu",
        "Nenghai Yu"
      ],
      "abstract": "Malicious image manipulation poses societal risks, increasing the importance\nof effective image manipulation detection methods. Recent approaches in image\nmanipulation detection have largely been driven by fully supervised approaches,\nwhich require labor-intensive pixel-level annotations. Thus, it is essential to\nexplore weakly supervised image manipulation localization methods that only\nrequire image-level binary labels for training. However, existing weakly\nsupervised image manipulation methods overlook the importance of edge\ninformation for accurate localization, leading to suboptimal localization\nperformance. To address this, we propose a Context-Aware Boundary Localization\n(CABL) module to aggregate boundary features and learn context-inconsistency\nfor localizing manipulated areas. Furthermore, by leveraging Class Activation\nMapping (CAM) and Segment Anything Model (SAM), we introduce the CAM-Guided SAM\nRefinement (CGSR) module to generate more accurate manipulation localization\nmaps. By integrating two modules, we present a novel weakly supervised\nframework based on a dual-branch Transformer-CNN architecture. Our method\nachieves outstanding localization performance across multiple datasets.",
      "tldr_zh": "该研究针对恶意图像篡改的社会风险，提出了一种基于弱监督的图像篡改定位方法，以避免传统全监督方法所需的繁重像素级标注。论文引入了Context-Aware Boundary Localization (CABL) 模块，用于聚合边界特征并学习上下文不一致性，从而精确定位篡改区域。同时，通过Class Activation Mapping (CAM) 和Segment Anything Model (SAM)，开发了CAM-Guided SAM Refinement (CGSR) 模块，以生成更准确的定位地图。最终，该方法整合于双分支Transformer-CNN架构中，并在多个数据集上实现了出色的定位性能。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.20294v2",
      "published_date": "2025-03-26 07:35:09 UTC",
      "updated_date": "2025-03-31 04:54:08 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T06:26:09.255959"
    },
    {
      "arxiv_id": "2503.20291v2",
      "title": "CryoSAMU: Enhancing 3D Cryo-EM Density Maps of Protein Structures at Intermediate Resolution with Structure-Aware Multimodal U-Nets",
      "title_zh": "翻译失败",
      "authors": [
        "Chenwei Zhang",
        "Khanh Dao Duc"
      ],
      "abstract": "Enhancing cryogenic electron microscopy (cryo-EM) 3D density maps at\nintermediate resolution (4-8 {\\AA}) is crucial in protein structure\ndetermination. Recent advances in deep learning have led to the development of\nautomated approaches for enhancing experimental cryo-EM density maps. Yet,\nthese methods are not optimized for intermediate-resolution maps and rely on\nmap density features alone. To address this, we propose CryoSAMU, a novel\nmethod designed to enhance 3D cryo-EM density maps of protein structures using\nstructure-aware multimodal U-Nets and trained on curated\nintermediate-resolution density maps. We comprehensively evaluate CryoSAMU\nacross various metrics and demonstrate its competitive performance compared to\nstate-of-the-art methods. Notably, CryoSAMU achieves significantly faster\nprocessing speed, showing promise for future practical applications. Our code\nis available at https://github.com/chenwei-zhang/CryoSAMU.",
      "tldr_zh": "这篇论文提出了CryoSAMU，一种创新方法，用于增强中间分辨率（4-8 Å）的cryo-EM 3D密度图，以辅助蛋白结构确定。CryoSAMU 采用structure-aware multimodal U-Nets，并通过训练整理过的密度图数据，解决了现有深度学习方法依赖单一密度特征的局限性。在全面评估中，CryoSAMU 在各种指标上表现出与最先进方法相当的性能，同时显著提高了处理速度，并提供了开源代码以支持实际应用。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG",
        "q-bio.BM"
      ],
      "primary_category": "cs.CV",
      "comment": "19 pages, 6 main figures, 2 supplementary figures, 3 main tables, 4\n  supplementary tables",
      "pdf_url": "http://arxiv.org/pdf/2503.20291v2",
      "published_date": "2025-03-26 07:33:36 UTC",
      "updated_date": "2025-05-15 15:06:46 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T06:26:22.675117"
    },
    {
      "arxiv_id": "2503.20290v2",
      "title": "QualiSpeech: A Speech Quality Assessment Dataset with Natural Language Reasoning and Descriptions",
      "title_zh": "翻译失败",
      "authors": [
        "Siyin Wang",
        "Wenyi Yu",
        "Xianzhao Chen",
        "Xiaohai Tian",
        "Jun Zhang",
        "Lu Lu",
        "Yu Tsao",
        "Junichi Yamagishi",
        "Yuxuan Wang",
        "Chao Zhang"
      ],
      "abstract": "This paper explores a novel perspective to speech quality assessment by\nleveraging natural language descriptions, offering richer, more nuanced\ninsights than traditional numerical scoring methods. Natural language feedback\nprovides instructive recommendations and detailed evaluations, yet existing\ndatasets lack the comprehensive annotations needed for this approach. To bridge\nthis gap, we introduce QualiSpeech, a comprehensive low-level speech quality\nassessment dataset encompassing 11 key aspects and detailed natural language\ncomments that include reasoning and contextual insights. Additionally, we\npropose the QualiSpeech Benchmark to evaluate the low-level speech\nunderstanding capabilities of auditory large language models (LLMs).\nExperimental results demonstrate that finetuned auditory LLMs can reliably\ngenerate detailed descriptions of noise and distortion, effectively identifying\ntheir types and temporal characteristics. The results further highlight the\npotential for incorporating reasoning to enhance the accuracy and reliability\nof quality assessments. The dataset will be released at\nhttps://huggingface.co/datasets/tsinghua-ee/QualiSpeech.",
      "tldr_zh": "该论文提出了一种创新的语音质量评估方法，使用自然语言描述提供比传统数字评分更丰富的见解，以解决现有数据集缺乏全面注释的问题。作者引入了 QualiSpeech 数据集，该数据集涵盖 11 个关键语音质量方面，并包含详细的自然语言评论、推理和上下文洞见，同时还提出了 QualiSpeech Benchmark 用于评估 auditory LLMs 的低级语音理解能力。实验结果显示，通过微调的 auditory LLMs 可以可靠地生成关于噪声和失真的详细描述，包括类型和时间特性，从而提升评估的准确性和可靠性。该数据集已计划在 https://huggingface.co/datasets/tsinghua-ee/QualiSpeech 发布，为语音质量评估领域提供了宝贵资源。",
      "categories": [
        "eess.AS",
        "cs.AI",
        "cs.CL",
        "cs.SD"
      ],
      "primary_category": "eess.AS",
      "comment": "23 pages, 16 figures",
      "pdf_url": "http://arxiv.org/pdf/2503.20290v2",
      "published_date": "2025-03-26 07:32:20 UTC",
      "updated_date": "2025-04-01 12:33:53 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T06:26:36.149230"
    },
    {
      "arxiv_id": "2503.20285v1",
      "title": "Model-Based Offline Reinforcement Learning with Adversarial Data Augmentation",
      "title_zh": "翻译失败",
      "authors": [
        "Hongye Cao",
        "Fan Feng",
        "Jing Huo",
        "Shangdong Yang",
        "Meng Fang",
        "Tianpei Yang",
        "Yang Gao"
      ],
      "abstract": "Model-based offline Reinforcement Learning (RL) constructs environment models\nfrom offline datasets to perform conservative policy optimization. Existing\napproaches focus on learning state transitions through ensemble models,\nrollouting conservative estimation to mitigate extrapolation errors. However,\nthe static data makes it challenging to develop a robust policy, and offline\nagents cannot access the environment to gather new data. To address these\nchallenges, we introduce Model-based Offline Reinforcement learning with\nAdversariaL data augmentation (MORAL). In MORAL, we replace the fixed horizon\nrollout by employing adversaria data augmentation to execute alternating\nsampling with ensemble models to enrich training data. Specifically, this\nadversarial process dynamically selects ensemble models against policy for\nbiased sampling, mitigating the optimistic estimation of fixed models, thus\nrobustly expanding the training data for policy optimization. Moreover, a\ndifferential factor is integrated into the adversarial process for\nregularization, ensuring error minimization in extrapolations. This\ndata-augmented optimization adapts to diverse offline tasks without rollout\nhorizon tuning, showing remarkable applicability. Extensive experiments on D4RL\nbenchmark demonstrate that MORAL outperforms other model-based offline RL\nmethods in terms of policy learning and sample efficiency.",
      "tldr_zh": "本论文提出了一种名为 MORAL 的模型-based 离线强化学习方法，通过对抗数据增强技术来解决静态数据导致的策略优化挑战。MORAL 替换传统固定时间步回滚，使用对抗过程动态选择集成模型进行偏差采样，以丰富训练数据并减少乐观估计错误；同时，融入微分因子进行正则化，确保外推误差最小化。该方法无需调整回滚时间步，即可适应多种离线任务，在 D4RL 基准实验中，MORAL 在策略学习和样本效率上显著优于现有模型-based 离线 RL 方法。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.20285v1",
      "published_date": "2025-03-26 07:24:34 UTC",
      "updated_date": "2025-03-26 07:24:34 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T06:26:48.394492"
    },
    {
      "arxiv_id": "2503.20282v1",
      "title": "Faster Parameter-Efficient Tuning with Token Redundancy Reduction",
      "title_zh": "翻译失败",
      "authors": [
        "Kwonyoung Kim",
        "Jungin Park",
        "Jin Kim",
        "Hyeongjun Kwon",
        "Kwanghoon Sohn"
      ],
      "abstract": "Parameter-efficient tuning (PET) aims to transfer pre-trained foundation\nmodels to downstream tasks by learning a small number of parameters. Compared\nto traditional fine-tuning, which updates the entire model, PET significantly\nreduces storage and transfer costs for each task regardless of exponentially\nincreasing pre-trained model capacity. However, most PET methods inherit the\ninference latency of their large backbone models and often introduce additional\ncomputational overhead due to additional modules (e.g. adapters), limiting\ntheir practicality for compute-intensive applications. In this paper, we\npropose Faster Parameter-Efficient Tuning (FPET), a novel approach that\nenhances inference speed and training efficiency while maintaining high storage\nefficiency. Specifically, we introduce a plug-and-play token redundancy\nreduction module delicately designed for PET. This module refines tokens from\nthe self-attention layer using an adapter to learn the accurate similarity\nbetween tokens and cuts off the tokens through a fully-differentiable token\nmerging strategy, which uses a straight-through estimator for optimal token\nreduction. Experimental results prove that our FPET achieves faster inference\nand higher memory efficiency than the pre-trained backbone while keeping\ncompetitive performance on par with state-of-the-art PET methods.",
      "tldr_zh": "本论文针对 Parameter-Efficient Tuning (PET) 的推理延迟和额外计算开销问题，提出了一种新方法 Faster Parameter-Efficient Tuning (FPET)，旨在提升推理速度和训练效率，同时保持高存储效率。FPET 引入了一个 plug-and-play 的 token redundancy reduction 模块，该模块在 self-attention layer 使用 adapter 精炼 tokens，并通过 fully-differentiable token merging 策略（结合 straight-through estimator）实现最优 token 减少。实验结果表明，FPET 比预训练骨干模型推理更快、内存更高效，且性能与最先进 PET 方法相当。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "CVPR 2025 Camera-ready",
      "pdf_url": "http://arxiv.org/pdf/2503.20282v1",
      "published_date": "2025-03-26 07:15:08 UTC",
      "updated_date": "2025-03-26 07:15:08 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T06:26:59.337555"
    },
    {
      "arxiv_id": "2503.20281v1",
      "title": "Are We There Yet? Unraveling the State-of-the-Art Graph Network Intrusion Detection Systems",
      "title_zh": "翻译失败",
      "authors": [
        "Chenglong Wang",
        "Pujia Zheng",
        "Jiaping Gui",
        "Cunqing Hua",
        "Wajih Ul Hassan"
      ],
      "abstract": "Network Intrusion Detection Systems (NIDS) are vital for ensuring enterprise\nsecurity. Recently, Graph-based NIDS (GIDS) have attracted considerable\nattention because of their capability to effectively capture the complex\nrelationships within the graph structures of data communications. Despite their\npromise, the reproducibility and replicability of these GIDS remain largely\nunexplored, posing challenges for developing reliable and robust detection\nsystems. This study bridges this gap by designing a systematic approach to\nevaluate state-of-the-art GIDS, which includes critically assessing, extending,\nand clarifying the findings of these systems. We further assess the robustness\nof GIDS under adversarial attacks. Evaluations were conducted on three public\ndatasets as well as a newly collected large-scale enterprise dataset. Our\nfindings reveal significant performance discrepancies, highlighting challenges\nrelated to dataset scale, model inputs, and implementation settings. We\ndemonstrate difficulties in reproducing and replicating results, particularly\nconcerning false positive rates and robustness against adversarial attacks.\nThis work provides valuable insights and recommendations for future research,\nemphasizing the importance of rigorous reproduction and replication studies in\ndeveloping robust and generalizable GIDS solutions.",
      "tldr_zh": "本研究评估了基于图形的网络入侵检测系统 (GIDS)，探讨其再现性和可复制性问题，以及在对抗攻击下的鲁棒性。研究者设计了一个系统方法，包括对现有 GIDS 进行关键评估、扩展和澄清，并在三个公共数据集以及一个新收集的大型企业数据集上进行测试。结果显示，GIDS 存在显著性能差异，受数据集规模、模型输入和实现设置的影响，且再现结果困难，特别是假阳性率和对抗攻击的鲁棒性。总体而言，该工作提供了宝贵见解和建议，强调未来研究需加强严格的再现和复制研究，以开发更可靠的 GIDS 解决方案。",
      "categories": [
        "cs.CR",
        "cs.AI"
      ],
      "primary_category": "cs.CR",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.20281v1",
      "published_date": "2025-03-26 07:11:57 UTC",
      "updated_date": "2025-03-26 07:11:57 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T06:27:10.581204"
    },
    {
      "arxiv_id": "2503.20279v2",
      "title": "sudo rm -rf agentic_security",
      "title_zh": "翻译失败",
      "authors": [
        "Sejin Lee",
        "Jian Kim",
        "Haon Park",
        "Ashkan Yousefpour",
        "Sangyoon Yu",
        "Min Song"
      ],
      "abstract": "Large Language Models (LLMs) are increasingly deployed as computer-use\nagents, autonomously performing tasks within real desktop or web environments.\nWhile this evolution greatly expands practical use cases for humans, it also\ncreates serious security exposures. We present SUDO (Screen-based Universal\nDetox2Tox Offense), a novel attack framework that systematically bypasses\nrefusal trained safeguards in commercial computer-use agents, such as Claude\nComputer Use. The core mechanism, Detox2Tox, transforms harmful requests (that\nagents initially reject) into seemingly benign requests via detoxification,\nsecures detailed instructions from advanced vision language models (VLMs), and\nthen reintroduces malicious content via toxification just before execution.\nUnlike conventional jailbreaks, SUDO iteratively refines its attacks based on a\nbuilt-in refusal feedback, making it increasingly effective against robust\npolicy filters. In extensive tests spanning 50 real-world tasks and multiple\nstate-of-the-art VLMs, SUDO achieves a stark attack success rate of 24% (with\nno refinement), and up to 41% (by its iterative refinement) in Claude Computer\nUse. By revealing these vulnerabilities and demonstrating the ease with which\nthey can be exploited in real-world computing environments, this paper\nhighlights an immediate need for robust, context-aware safeguards. WARNING:\nThis paper includes harmful or offensive model outputs Our code is available\nat: https://github.com/AIM-Intelligence/SUDO.git",
      "tldr_zh": "这篇论文提出了 SUDO（Screen-based Universal Detox2Tox Offense）攻击框架，针对大型语言模型 (LLMs) 作为计算机使用代理时的安全漏洞，通过 Detox2Tox 机制将有害请求转化为看似无害的形式，以绕过拒绝训练的安全措施。框架利用视觉语言模型 (VLMs) 获取详细指令，并在执行前重新引入恶意内容，同时通过迭代精炼基于拒绝反馈来提升攻击效果。在测试中，SUDO 在 Claude Computer Use 等模型上实现了 24%（无精炼）至 41%（有精炼）的攻击成功率，强调了在真实计算环境中亟需更 robust 的上下文感知安全防护。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.CR"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.20279v2",
      "published_date": "2025-03-26 07:08:15 UTC",
      "updated_date": "2025-04-04 04:36:20 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T06:27:22.698154"
    },
    {
      "arxiv_id": "2503.20831v1",
      "title": "Advancing Vulnerability Classification with BERT: A Multi-Objective Learning Model",
      "title_zh": "翻译失败",
      "authors": [
        "Himanshu Tiwari"
      ],
      "abstract": "The rapid increase in cybersecurity vulnerabilities necessitates automated\ntools for analyzing and classifying vulnerability reports. This paper presents\na novel Vulnerability Report Classifier that leverages the BERT (Bidirectional\nEncoder Representations from Transformers) model to perform multi-label\nclassification of Common Vulnerabilities and Exposures (CVE) reports from the\nNational Vulnerability Database (NVD). The classifier predicts both the\nseverity (Low, Medium, High, Critical) and vulnerability types (e.g., Buffer\nOverflow, XSS) from textual descriptions. We introduce a custom training\npipeline using a combined loss function-Cross-Entropy for severity and Binary\nCross-Entropy with Logits for types-integrated into a Hugging Face Trainer\nsubclass. Experiments on recent NVD data demonstrate promising results, with\ndecreasing evaluation loss across epochs. The system is deployed via a REST API\nand a Streamlit UI, enabling real-time vulnerability analysis. This work\ncontributes a scalable, open-source solution for cybersecurity practitioners to\nautomate vulnerability triage.",
      "tldr_zh": "本论文提出了一种基于 BERT 的多目标学习模型，用于对国家漏洞数据库 (NVD) 中的 Common Vulnerabilities and Exposures (CVE) 报告进行多标签分类，包括漏洞严重性 (Low, Medium, High, Critical) 和类型 (如 Buffer Overflow, XSS)。该模型采用自定义训练管道，结合 Cross-Entropy 损失（针对严重性）和 Binary Cross-Entropy with Logits 损失（针对类型），并集成到 Hugging Face Trainer 子类中进行优化。实验结果显示，在最近的 NVD 数据上，模型的评估损失逐epoch减少，证明了其有效性。该系统通过 REST API 和 Streamlit UI 部署，提供了一个可扩展的开源解决方案，帮助网络安全从业者实现自动化漏洞分类和实时分析。",
      "categories": [
        "cs.CR",
        "cs.AI"
      ],
      "primary_category": "cs.CR",
      "comment": "9 Pages",
      "pdf_url": "http://arxiv.org/pdf/2503.20831v1",
      "published_date": "2025-03-26 06:04:45 UTC",
      "updated_date": "2025-03-26 06:04:45 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T06:27:36.028756"
    },
    {
      "arxiv_id": "2503.20258v1",
      "title": "Mamba-3D as Masked Autoencoders for Accurate and Data-Efficient Analysis of Medical Ultrasound Videos",
      "title_zh": "翻译失败",
      "authors": [
        "Jiaheng Zhou",
        "Yanfeng Zhou",
        "Wei Fang",
        "Yuxing Tang",
        "Le Lu",
        "Ge Yang"
      ],
      "abstract": "Ultrasound videos are an important form of clinical imaging data, and deep\nlearning-based automated analysis can improve diagnostic accuracy and clinical\nefficiency. However, the scarcity of labeled data and the inherent challenges\nof video analysis have impeded the advancement of related methods. In this\nwork, we introduce E-ViM$^3$, a data-efficient Vision Mamba network that\npreserves the 3D structure of video data, enhancing long-range dependencies and\ninductive biases to better model space-time correlations. With our design of\nEnclosure Global Tokens (EGT), the model captures and aggregates global\nfeatures more effectively than competing methods. To further improve data\nefficiency, we employ masked video modeling for self-supervised pre-training,\nwith the proposed Spatial-Temporal Chained (STC) masking strategy designed to\nadapt to various video scenarios. Experiments demonstrate that E-ViM$^3$\nperforms as the state-of-the-art in two high-level semantic analysis tasks\nacross four datasets of varying sizes: EchoNet-Dynamic, CAMUS, MICCAI-BUV, and\nWHBUS. Furthermore, our model achieves competitive performance with limited\nlabels, highlighting its potential impact on real-world clinical applications.",
      "tldr_zh": "本文提出 E-ViM³，一种基于 Vision Mamba 的数据高效网络，用于精确分析医疗超声视频，通过保留视频的 3D 结构并增强长程依赖性来更好地建模时空相关性。该模型引入 Enclosure Global Tokens (EGT) 以更有效地捕获和聚合全局特征，并采用 masked video modeling 结合 Spatial-Temporal Chained (STC) masking 策略进行自监督预训练，从而提高数据利用效率。实验结果显示，E-ViM³ 在 EchoNet-Dynamic、CAMUS、MICCAI-BUV 和 WHBUS 等四个数据集上的高级语义分析任务中达到最先进水平，并在标签有限的情况下表现出竞争性表现，具有重要的临床应用潜力。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.20258v1",
      "published_date": "2025-03-26 05:54:13 UTC",
      "updated_date": "2025-03-26 05:54:13 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T06:27:51.325824"
    },
    {
      "arxiv_id": "2503.20252v2",
      "title": "LogicQA: Logical Anomaly Detection with Vision Language Model Generated Questions",
      "title_zh": "LogicQA：基于视觉语言模型生成问题的逻辑异常检测",
      "authors": [
        "Yejin Kwon",
        "Daeun Moon",
        "Youngje Oh",
        "Hyunsoo Yoon"
      ],
      "abstract": "Anomaly Detection (AD) focuses on detecting samples that differ from the\nstandard pattern, making it a vital tool in process control. Logical anomalies\nmay appear visually normal yet violate predefined constraints on object\npresence, arrangement, or quantity, depending on reasoning and explainability.\nWe introduce LogicQA, a framework that enhances AD by providing industrial\noperators with explanations for logical anomalies. LogicQA compiles\nautomatically generated questions into a checklist and collects responses to\nidentify violations of logical constraints. LogicQA is training-free,\nannotation-free, and operates in a few-shot setting. We achieve\nstate-of-the-art (SOTA) Logical AD performance on public benchmarks, MVTec LOCO\nAD, with an AUROC of 87.6 percent and an F1-max of 87.0 percent along with the\nexplanations of anomalies. Also, our approach has shown outstanding performance\non semiconductor SEM corporate data, further validating its effectiveness in\nindustrial applications.",
      "tldr_zh": "这篇论文提出 LogicQA 框架，利用 Vision Language Model (VLM) 生成的问题来增强 Anomaly Detection (AD)，专注于检测视觉上正常但违反预定义逻辑约束（如物体存在、排列或数量）的逻辑异常，并提供可解释性解释。LogicQA 通过将自动生成的问题编译成检查列表，并在 few-shot 设置下收集响应来识别约束违反，该方法无需训练或标注。实验结果显示，该框架在 MVTec LOCO AD 基准上达到 SOTA 性能，AUROC 为 87.6% 和 F1-max 为 87.0%，并在半导体工业数据上表现出色，验证了其在实际应用中的有效性。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "Accepted Industry Track at ACL 2025",
      "pdf_url": "http://arxiv.org/pdf/2503.20252v2",
      "published_date": "2025-03-26 05:38:45 UTC",
      "updated_date": "2025-05-20 06:11:08 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T06:28:00.526419"
    },
    {
      "arxiv_id": "2503.20245v1",
      "title": "ESSR: An 8K@30FPS Super-Resolution Accelerator With Edge Selective Network",
      "title_zh": "翻译失败",
      "authors": [
        "Chih-Chia Hsu",
        "Tian-Sheuan Chang"
      ],
      "abstract": "Deep learning-based super-resolution (SR) is challenging to implement in\nresource-constrained edge devices for resolutions beyond full HD due to its\nhigh computational complexity and memory bandwidth requirements. This paper\nintroduces an 8K@30FPS SR accelerator with edge-selective dynamic input\nprocessing. Dynamic processing chooses the appropriate subnets for different\npatches based on simple input edge criteria, achieving a 50\\% MAC reduction\nwith only a 0.1dB PSNR decrease. The quality of reconstruction images is\nguaranteed and maximized its potential with \\textit{resource adaptive model\nswitching} even under resource constraints. In conjunction with\nhardware-specific refinements, the model size is reduced by 84\\% to 51K, but\nwith a decrease of less than 0.6dB PSNR. Additionally, to support dynamic\nprocessing with high utilization, this design incorporates a\n\\textit{configurable group of layer mapping} that synergizes with the\n\\textit{structure-friendly fusion block}, resulting in 77\\% hardware\nutilization and up to 79\\% reduction in feature SRAM access. The\nimplementation, using the TSMC 28nm process, can achieve 8K@30FPS throughput at\n800MHz with a gate count of 2749K, 0.2075W power consumption, and 4797Mpixels/J\nenergy efficiency, exceeding previous work.",
      "tldr_zh": "本论文提出ESSR，一种支持8K@30FPS的超分辨率(SR)加速器，针对资源受限边缘设备的高计算复杂性和内存带宽需求。该加速器采用边缘选择动态输入处理，根据输入边缘标准选择合适子网，实现50% MAC操作减少，同时仅损失0.1dB PSNR，并通过资源自适应模型切换优化重建图像质量。硬件优化将模型大小缩减84%至51K，同时结合可配置层映射和结构友好融合块，提升硬件利用率至77%并减少79%特征SRAM访问；在TSMC 28nm工艺下，系统在800MHz下实现8K@30FPS吞吐量，功耗0.2075W，能效达4797Mpixels/J，超越现有工作。",
      "categories": [
        "cs.AR",
        "cs.AI",
        "cs.MM",
        "eess.IV"
      ],
      "primary_category": "cs.AR",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.20245v1",
      "published_date": "2025-03-26 05:27:23 UTC",
      "updated_date": "2025-03-26 05:27:23 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T06:28:12.484053"
    },
    {
      "arxiv_id": "2503.20241v1",
      "title": "LGR: LLM-Guided Ranking of Frontiers for Object Goal Navigation",
      "title_zh": "翻译失败",
      "authors": [
        "Mitsuaki Uno",
        "Kanji Tanaka",
        "Daiki Iwata",
        "Yudai Noda",
        "Shoya Miyazaki",
        "Kouki Terashima"
      ],
      "abstract": "Object Goal Navigation (OGN) is a fundamental task for robots and AI, with\nkey applications such as mobile robot image databases (MRID). In particular,\nmapless OGN is essential in scenarios involving unknown or dynamic\nenvironments. This study aims to enhance recent modular mapless OGN systems by\nleveraging the commonsense reasoning capabilities of large language models\n(LLMs). Specifically, we address the challenge of determining the visiting\norder in frontier-based exploration by framing it as a frontier ranking\nproblem. Our approach is grounded in recent findings that, while LLMs cannot\ndetermine the absolute value of a frontier, they excel at evaluating the\nrelative value between multiple frontiers viewed within a single image using\nthe view image as context. We dynamically manage the frontier list by adding\nand removing elements, using an LLM as a ranking model. The ranking results are\nrepresented as reciprocal rank vectors, which are ideal for multi-view,\nmulti-query information fusion. We validate the effectiveness of our method\nthrough evaluations in Habitat-Sim.",
      "tldr_zh": "该研究提出LGR方法，利用大型语言模型(LLMs)的常识推理能力来提升无地图Object Goal Navigation (OGN)系统，特别是解决前沿排序问题，以确定探索顺序。方法通过将前沿评估框架化为相对价值比较，使用视图图像作为上下文，动态管理前沿列表并生成互惠排名向量(reciprocal rank vectors)以融合多视图信息。实验在Habitat-Sim环境中验证了LGR的有效性，展示了其在未知动态环境中的潜力。",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "comment": "10 pages, 11 figures, technical report",
      "pdf_url": "http://arxiv.org/pdf/2503.20241v1",
      "published_date": "2025-03-26 05:15:26 UTC",
      "updated_date": "2025-03-26 05:15:26 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T06:28:23.769045"
    },
    {
      "arxiv_id": "2503.20233v1",
      "title": "Dynamic Learning and Productivity for Data Analysts: A Bayesian Hidden Markov Model Perspective",
      "title_zh": "数据分析师的动态学习和生产力：贝叶斯隐马尔可夫模型视角",
      "authors": [
        "Yue Yin"
      ],
      "abstract": "Data analysts are essential in organizations, transforming raw data into\ninsights that drive decision-making and strategy. This study explores how\nanalysts' productivity evolves on a collaborative platform, focusing on two key\nlearning activities: writing queries and viewing peer queries. While\ntraditional research often assumes static models, where performance improves\nsteadily with cumulative learning, such models fail to capture the dynamic\nnature of real-world learning. To address this, we propose a Hidden Markov\nModel (HMM) that tracks how analysts transition between distinct learning\nstates based on their participation in these activities.\n  Using an industry dataset with 2,001 analysts and 79,797 queries, this study\nidentifies three learning states: novice, intermediate, and advanced.\nProductivity increases as analysts advance to higher states, reflecting the\ncumulative benefits of learning. Writing queries benefits analysts across all\nstates, with the largest gains observed for novices. Viewing peer queries\nsupports novices but may hinder analysts in higher states due to cognitive\noverload or inefficiencies. Transitions between states are also uneven, with\nprogression from intermediate to advanced being particularly challenging. This\nstudy advances understanding of into dynamic learning behavior of knowledge\nworker and offers practical implications for designing systems, optimizing\ntraining, enabling personalized learning, and fostering effective knowledge\nsharing.",
      "tldr_zh": "这篇论文探讨了数据分析师的生产力如何通过编写查询和查看同行查询等学习活动动态演变，采用 Bayesian Hidden Markov Model (HMM) 来跟踪分析师在不同学习状态（如新手、中间和高级）之间的转换。研究基于一个包含 2,001 名分析师和 79,797 个查询的行业数据集，发现编写查询对所有状态的分析师有益，尤其是新手，而查看同行查询则可能导致高级分析师出现认知过载或效率低下。结果显示，学习状态的提升会增加生产力，但从中间到高级状态的过渡特别具有挑战性。该研究为设计优化系统、培训程序和个性化学习提供实际启示。",
      "categories": [
        "cs.SI",
        "cs.AI",
        "cs.CE",
        "cs.HC"
      ],
      "primary_category": "cs.SI",
      "comment": "29 pages; a shorter 11-page version is accepted by HCI International\n  (HCII) 2025;",
      "pdf_url": "http://arxiv.org/pdf/2503.20233v1",
      "published_date": "2025-03-26 04:57:03 UTC",
      "updated_date": "2025-03-26 04:57:03 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T06:28:36.481433"
    },
    {
      "arxiv_id": "2503.20231v1",
      "title": "Dynamics of Algorithmic Content Amplification on TikTok",
      "title_zh": "TikTok 上的算法内容放大动态",
      "authors": [
        "Fabian Baumann",
        "Nipun Arora",
        "Iyad Rahwan",
        "Agnieszka Czaplicka"
      ],
      "abstract": "Intelligent algorithms increasingly shape the content we encounter and engage\nwith online. TikTok's For You feed exemplifies extreme algorithm-driven\ncuration, tailoring the stream of video content almost exclusively based on\nusers' explicit and implicit interactions with the platform. Despite growing\nattention, the dynamics of content amplification on TikTok remain largely\nunquantified. How quickly, and to what extent, does TikTok's algorithm amplify\ncontent aligned with users' interests? To address these questions, we conduct a\nsock-puppet audit, deploying bots with different interests to engage with\nTikTok's \"For You\" feed. Our findings reveal that content aligned with the\nbots' interests undergoes strong amplification, with rapid reinforcement\ntypically occurring within the first 200 videos watched. While amplification is\nconsistently observed across all interests, its intensity varies by interest,\nindicating the emergence of topic-specific biases. Time series analyses and\nMarkov models uncover distinct phases of recommendation dynamics, including\npersistent content reinforcement and a gradual decline in content diversity\nover time. Although TikTok's algorithm preserves some content diversity, we\nfind a strong negative correlation between amplification and exploration: as\nthe amplification of interest-aligned content increases, engagement with unseen\nhashtags declines. These findings contribute to discussions on\nsocio-algorithmic feedback loops in the digital age and the trade-offs between\npersonalization and content diversity.",
      "tldr_zh": "本研究探讨了TikTok算法在内容放大上的动态，焦点在于For You feed如何基于用户互动快速个性化视频推荐。研究者通过sock-puppet audit方法，使用具有不同兴趣的bots互动，揭示了与用户兴趣对齐的内容在观看前200个视频内即经历强烈放大，但放大强度因主题而异，导致主题特定偏差。时间序列分析和Markov models进一步显示，推荐过程包括内容强化阶段和多样性逐渐下降的趋势，同时放大与探索之间存在负相关：内容个性化增强时，未见hashtags的互动减少。这些发现为数字时代的社会算法反馈循环提供量化洞见，并强调了个性化与内容多样性之间的权衡。",
      "categories": [
        "physics.soc-ph",
        "cs.AI",
        "cs.SI"
      ],
      "primary_category": "physics.soc-ph",
      "comment": "34 pages",
      "pdf_url": "http://arxiv.org/pdf/2503.20231v1",
      "published_date": "2025-03-26 04:54:24 UTC",
      "updated_date": "2025-03-26 04:54:24 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T06:28:47.851371"
    },
    {
      "arxiv_id": "2503.20230v1",
      "title": "TraNCE: Transformative Non-linear Concept Explainer for CNNs",
      "title_zh": "翻译失败",
      "authors": [
        "Ugochukwu Ejike Akpudo",
        "Yongsheng Gao",
        "Jun Zhou",
        "Andrew Lewis"
      ],
      "abstract": "Convolutional neural networks (CNNs) have succeeded remarkably in various\ncomputer vision tasks. However, they are not intrinsically explainable. While\nthe feature-level understanding of CNNs reveals where the models looked,\nconcept-based explainability methods provide insights into what the models saw.\nHowever, their assumption of linear reconstructability of image activations\nfails to capture the intricate relationships within these activations. Their\nFidelity-only approach to evaluating global explanations also presents a new\nconcern. For the first time, we address these limitations with the novel\nTransformative Nonlinear Concept Explainer (TraNCE) for CNNs. Unlike linear\nreconstruction assumptions made by existing methods, TraNCE captures the\nintricate relationships within the activations. This study presents three\noriginal contributions to the CNN explainability literature: (i) An automatic\nconcept discovery mechanism based on variational autoencoders (VAEs). This\ntransformative concept discovery process enhances the identification of\nmeaningful concepts from image activations. (ii) A visualization module that\nleverages the Bessel function to create a smooth transition between\nprototypical image pixels, revealing not only what the CNN saw but also what\nthe CNN avoided, thereby mitigating the challenges of concept duplication as\ndocumented in previous works. (iii) A new metric, the Faith score, integrates\nboth Coherence and Fidelity for a comprehensive evaluation of explainer\nfaithfulness and consistency.",
      "tldr_zh": "该论文提出TraNCE，一种变革性的非线性概念解释器，用于提升CNNs的可解释性，解决现有方法对图像激活的线性重构假设和仅关注Fidelity的评估局限。TraNCE的核心创新包括：(i) 基于变分自编码器(VAEs)的自动概念发现机制，以更好地识别图像激活中的意义概念；(ii) 一个利用Bessel函数的可视化模块，实现原型图像像素的平滑过渡，揭示CNNs关注和忽略的内容，并减少概念重复问题；(iii) 引入Faith score新指标，结合Coherence和Fidelity，全面评估解释器的忠实度和一致性。该方法为CNNs的可解释性研究提供了新框架，提升了模型理解的准确性和可靠性。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.20230v1",
      "published_date": "2025-03-26 04:49:46 UTC",
      "updated_date": "2025-03-26 04:49:46 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T06:29:01.337865"
    },
    {
      "arxiv_id": "2503.20227v1",
      "title": "Advancements in Natural Language Processing: Exploring Transformer-Based Architectures for Text Understanding",
      "title_zh": "自然语言处理中的进展：探索基于Transformer的架构用于文本理解",
      "authors": [
        "Tianhao Wu",
        "Yu Wang",
        "Ngoc Quach"
      ],
      "abstract": "Natural Language Processing (NLP) has witnessed a transformative leap with\nthe advent of transformer-based architectures, which have significantly\nenhanced the ability of machines to understand and generate human-like text.\nThis paper explores the advancements in transformer models, such as BERT and\nGPT, focusing on their superior performance in text understanding tasks\ncompared to traditional methods like recurrent neural networks (RNNs). By\nanalyzing statistical properties through visual representations-including\nprobability density functions of text length distributions and feature space\nclassifications-the study highlights the models' proficiency in handling\nlong-range dependencies, adapting to conditional shifts, and extracting\nfeatures for classification, even with overlapping classes. Drawing on recent\n2024 research, including enhancements in multi-hop knowledge graph reasoning\nand context-aware chat interactions, the paper outlines a methodology involving\ndata preparation, model selection, pretraining, fine-tuning, and evaluation.\nThe results demonstrate state-of-the-art performance on benchmarks like GLUE\nand SQuAD, with F1 scores exceeding 90%, though challenges such as high\ncomputational costs persist. This work underscores the pivotal role of\ntransformers in modern NLP and suggests future directions, including efficiency\noptimization and multimodal integration, to further advance language-based AI\nsystems.",
      "tldr_zh": "这篇论文探讨了自然语言处理（NLP）领域的进展，聚焦于 Transformer-based 架构（如 BERT 和 GPT），这些模型在文本理解任务上显著优于传统方法如 RNNs，能更好地处理长程依赖、适应条件变化并提取特征。\n研究通过分析统计属性（如文本长度分布的概率密度函数）和视觉表示，结合 2024 年的相关工作（如多跳知识图推理和上下文感知聊天），提出了一个包括数据准备、模型选择、预训练、微调和评估的完整方法论。\n实验结果显示，这些模型在 GLUE 和 SQuAD 基准上达到了最先进性能，F1 分数超过 90%，但面临高计算成本的挑战。\n未来方向包括优化效率和整合多模态技术，以进一步提升语言 AI 系统。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "This paper has been accepted by the 5th International Conference on\n  Artificial Intelligence and Industrial Technology Applications (AIITA 2025)",
      "pdf_url": "http://arxiv.org/pdf/2503.20227v1",
      "published_date": "2025-03-26 04:45:33 UTC",
      "updated_date": "2025-03-26 04:45:33 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T06:29:13.375180"
    },
    {
      "arxiv_id": "2503.20208v1",
      "title": "Learning Adaptive Dexterous Grasping from Single Demonstrations",
      "title_zh": "翻译失败",
      "authors": [
        "Liangzhi Shi",
        "Yulin Liu",
        "Lingqi Zeng",
        "Bo Ai",
        "Zhengdong Hong",
        "Hao Su"
      ],
      "abstract": "How can robots learn dexterous grasping skills efficiently and apply them\nadaptively based on user instructions? This work tackles two key challenges:\nefficient skill acquisition from limited human demonstrations and\ncontext-driven skill selection. We introduce AdaDexGrasp, a framework that\nlearns a library of grasping skills from a single human demonstration per skill\nand selects the most suitable one using a vision-language model (VLM). To\nimprove sample efficiency, we propose a trajectory following reward that guides\nreinforcement learning (RL) toward states close to a human demonstration while\nallowing flexibility in exploration. To learn beyond the single demonstration,\nwe employ curriculum learning, progressively increasing object pose variations\nto enhance robustness. At deployment, a VLM retrieves the appropriate skill\nbased on user instructions, bridging low-level learned skills with high-level\nintent. We evaluate AdaDexGrasp in both simulation and real-world settings,\nshowing that our approach significantly improves RL efficiency and enables\nlearning human-like grasp strategies across varied object configurations.\nFinally, we demonstrate zero-shot transfer of our learned policies to a\nreal-world PSYONIC Ability Hand, with a 90% success rate across objects,\nsignificantly outperforming the baseline.",
      "tldr_zh": "本研究提出AdaDexGrasp框架，用于从单个人类演示中高效学习自适应灵巧抓握技能，并根据用户指令选择合适技能。该框架通过轨迹跟随奖励引导reinforcement learning (RL)，提高样本效率，同时采用curriculum learning逐步增加物体姿势变化以增强鲁棒性。在部署时，使用vision-language model (VLM)根据用户指令检索技能，实现低级技能与高层意图的桥接。实验在模拟和真实环境中验证，显著提升RL效率，并实现人类般的抓握策略；在真实PSYONIC Ability Hand上，零样本转移成功率达90%，优于基线模型。",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.RO",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.20208v1",
      "published_date": "2025-03-26 04:05:50 UTC",
      "updated_date": "2025-03-26 04:05:50 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T06:29:23.366273"
    },
    {
      "arxiv_id": "2503.20205v1",
      "title": "Generalized Phase Pressure Control Enhanced Reinforcement Learning for Traffic Signal Control",
      "title_zh": "翻译失败",
      "authors": [
        "Xiao-Cheng Liao",
        "Yi Mei",
        "Mengjie Zhang",
        "Xiang-Ling Chen"
      ],
      "abstract": "Appropriate traffic state representation is crucial for learning traffic\nsignal control policies. However, most of the current traffic state\nrepresentations are heuristically designed, with insufficient theoretical\nsupport. In this paper, we (1) develop a flexible, efficient, and theoretically\ngrounded method, namely generalized phase pressure (G2P) control, which takes\nonly simple lane features into consideration to decide which phase to be\nactuated; 2) extend the pressure control theory to a general form for\nmulti-homogeneous-lane road networks based on queueing theory; (3) design a new\ntraffic state representation based on the generalized phase state features from\nG2P control; and 4) develop a reinforcement learning (RL)-based algorithm\ntemplate named G2P-XLight, and two RL algorithms, G2P-MPLight and G2P-CoLight,\nby combining the generalized phase state representation with MPLight and\nCoLight, two well-performed RL methods for learning traffic signal control\npolicies. Extensive experiments conducted on multiple real-world datasets\ndemonstrate that G2P control outperforms the state-of-the-art (SOTA) heuristic\nmethod in the transportation field and other recent human-designed heuristic\nmethods; and that the newly proposed G2P-XLight significantly outperforms SOTA\nlearning-based approaches. Our code is available online.",
      "tldr_zh": "本论文提出了一种名为 Generalized Phase Pressure (G2P) Control 的方法，用于提升交通信号控制的强化学习 (Reinforcement Learning, RL) 性能，该方法基于简单的车道特征并有理论支撑，决定相位激活。论文扩展了压力控制理论应用于多同质车道路网，利用排队理论设计了新的交通状态表示，并开发了 G2P-XLight 算法模板以及 G2P-MPLight 和 G2P-CoLight 算法，通过整合 G2P 与现有 RL 方法如 MPLight 和 CoLight。实验结果显示，在多个真实数据集上，G2P Control 优于最先进的手动启发式方法，而 G2P-XLight 显著超越了现有基于学习的 SOTA 方法，且代码已开源。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.20205v1",
      "published_date": "2025-03-26 04:03:12 UTC",
      "updated_date": "2025-03-26 04:03:12 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T06:29:36.688953"
    },
    {
      "arxiv_id": "2503.20202v1",
      "title": "SARGes: Semantically Aligned Reliable Gesture Generation via Intent Chain",
      "title_zh": "翻译失败",
      "authors": [
        "Nan Gao",
        "Yihua Bao",
        "Dongdong Weng",
        "Jiayi Zhao",
        "Jia Li",
        "Yan Zhou",
        "Pengfei Wan",
        "Di Zhang"
      ],
      "abstract": "Co-speech gesture generation enhances human-computer interaction realism\nthrough speech-synchronized gesture synthesis. However, generating semantically\nmeaningful gestures remains a challenging problem. We propose SARGes, a novel\nframework that leverages large language models (LLMs) to parse speech content\nand generate reliable semantic gesture labels, which subsequently guide the\nsynthesis of meaningful co-speech gestures.First, we constructed a\ncomprehensive co-speech gesture ethogram and developed an LLM-based intent\nchain reasoning mechanism that systematically parses and decomposes gesture\nsemantics into structured inference steps following ethogram criteria,\neffectively guiding LLMs to generate context-aware gesture labels.\nSubsequently, we constructed an intent chain-annotated text-to-gesture label\ndataset and trained a lightweight gesture label generation model, which then\nguides the generation of credible and semantically coherent co-speech gestures.\nExperimental results demonstrate that SARGes achieves highly\nsemantically-aligned gesture labeling (50.2% accuracy) with efficient\nsingle-pass inference (0.4 seconds). The proposed method provides an\ninterpretable intent reasoning pathway for semantic gesture synthesis.",
      "tldr_zh": "本研究提出SARGes框架，通过意图链(intent chain)机制利用大型语言模型(LLMs)解析语音内容并生成可靠的语义手势标签，从而指导合成与语音同步的语义有意义的手势。首先，构建了一个全面的与语音同步手势行为图(co-speech gesture ethogram)，并开发LLM-based意图链推理机制，将手势语义分解为结构化的推理步骤，以生成上下文相关的标签。随后，基于注释数据集训练了一个轻量级的手势标签生成模型。实验结果显示，SARGes实现了高度语义对齐的手势标签(50.2%准确率)，并支持高效的单次推理(0.4秒)，为可解释的语义手势合成提供了新途径。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.HC",
        "cs.RO"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.20202v1",
      "published_date": "2025-03-26 03:55:41 UTC",
      "updated_date": "2025-03-26 03:55:41 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T06:29:49.082015"
    },
    {
      "arxiv_id": "2503.20199v1",
      "title": "Assessing SAM for Tree Crown Instance Segmentation from Drone Imagery",
      "title_zh": "翻译失败",
      "authors": [
        "Mélisande Teng",
        "Arthur Ouaknine",
        "Etienne Laliberté",
        "Yoshua Bengio",
        "David Rolnick",
        "Hugo Larochelle"
      ],
      "abstract": "The potential of tree planting as a natural climate solution is often\nundermined by inadequate monitoring of tree planting projects. Current\nmonitoring methods involve measuring trees by hand for each species, requiring\nextensive cost, time, and labour. Advances in drone remote sensing and computer\nvision offer great potential for mapping and characterizing trees from aerial\nimagery, and large pre-trained vision models, such as the Segment Anything\nModel (SAM), may be a particularly compelling choice given limited labeled\ndata. In this work, we compare SAM methods for the task of automatic tree crown\ninstance segmentation in high resolution drone imagery of young tree\nplantations. We explore the potential of SAM for this task, and find that\nmethods using SAM out-of-the-box do not outperform a custom Mask R-CNN, even\nwith well-designed prompts, but that there is potential for methods which tune\nSAM further. We also show that predictions can be improved by adding Digital\nSurface Model (DSM) information as an input.",
      "tldr_zh": "这篇论文评估了Segment Anything Model (SAM)在利用无人机高分辨率图像进行树冠实例分割任务的性能，以解决树种植项目监测的成本和效率问题。研究比较了直接使用SAM的方法与自定义Mask R-CNN，结果显示SAM即使搭配精心设计的提示，也不如Mask R-CNN准确。作者发现，进一步调整SAM参数和加入Digital Surface Model (DSM)信息可以显著改善预测效果，为基于预训练模型的树木监测提供潜在优化路径。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "ICLR 2025 ML4RS workshop",
      "pdf_url": "http://arxiv.org/pdf/2503.20199v1",
      "published_date": "2025-03-26 03:45:36 UTC",
      "updated_date": "2025-03-26 03:45:36 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T06:30:00.543479"
    },
    {
      "arxiv_id": "2503.22723v1",
      "title": "Zero-Shot LLMs in Human-in-the-Loop RL: Replacing Human Feedback for Reward Shaping",
      "title_zh": "翻译失败",
      "authors": [
        "Mohammad Saif Nazir",
        "Chayan Banerjee"
      ],
      "abstract": "Reinforcement learning often faces challenges with reward misalignment, where\nagents optimize for given rewards but fail to exhibit the desired behaviors.\nThis occurs when the reward function incentivizes proxy behaviors that diverge\nfrom the true objective. While human-in-the-loop (HIL) methods can help, they\nmay exacerbate the problem, as humans are prone to biases that lead to\ninconsistent, subjective, or misaligned feedback, complicating the learning\nprocess. To address these issues, we propose two key contributions. First, we\nextend the use of zero-shot, off-the-shelf large language models (LLMs) for\nreward shaping beyond natural language processing (NLP) to continuous control\ntasks. By leveraging LLMs as direct feedback providers, we replace surrogate\nmodels trained on human feedback, which often suffer from the bias inherent in\nthe feedback data it is trained on. Second, we introduce a hybrid framework\n(LLM-HFBF) that enables LLMs to identify and correct biases in human feedback\nwhile incorporating this feedback into the reward shaping process. The LLM-HFBF\nframework creates a more balanced and reliable system by addressing both the\nlimitations of LLMs (e.g., lack of domain-specific knowledge) and human\nsupervision (e.g., inherent biases). By enabling human feedback bias flagging\nand correction, our approach improves reinforcement learning performance and\nreduces reliance on potentially biased human guidance. Empirical experiments\nshow that biased human feedback significantly reduces performance, with average\nepisodic reward (AER) dropping from 28.472 in (unbiased approaches) to 7.039\n(biased with conservative bias). In contrast, LLM-based approaches maintain a\nmatching AER like unbiased feedback, even in custom edge case scenarios.",
      "tldr_zh": "该研究解决强化学习（RL）中的奖励不匹配问题，以及人类参与（HIL）反馈可能带来的偏见和不一致性。论文的主要贡献包括：将 zero-shot 大语言模型（LLMs）扩展到连续控制任务，用于直接替换人类反馈的奖励塑造，从而避免基于偏见数据的代理模型；以及引入混合框架 LLM-HFBF，让 LLMs 识别和修正人类反馈中的偏见，同时整合反馈以提升系统可靠性。实验结果显示，带有偏见的人类反馈会显著降低性能（如平均情节奖励（AER）从 28.472 降至 7.039），而 LLM-based 方法能维持与无偏见反馈相当的性能，甚至在自定义边缘场景中表现出色。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "20 pages, 2 figures, 5 Tables",
      "pdf_url": "http://arxiv.org/pdf/2503.22723v1",
      "published_date": "2025-03-26 03:17:12 UTC",
      "updated_date": "2025-03-26 03:17:12 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T06:30:13.550981"
    },
    {
      "arxiv_id": "2503.20182v1",
      "title": "Leveraging Implicit Sentiments: Enhancing Reliability and Validity in Psychological Trait Evaluation of LLMs",
      "title_zh": "翻译失败",
      "authors": [
        "Huanhuan Ma",
        "Haisong Gong",
        "Xiaoyuan Yi",
        "Xing Xie",
        "Dongkuan Xu"
      ],
      "abstract": "Recent advancements in Large Language Models (LLMs) have led to their\nincreasing integration into human life. With the transition from mere tools to\nhuman-like assistants, understanding their psychological aspects-such as\nemotional tendencies and personalities-becomes essential for ensuring their\ntrustworthiness. However, current psychological evaluations of LLMs, often\nbased on human psychological assessments like the BFI, face significant\nlimitations. The results from these approaches often lack reliability and have\nlimited validity when predicting LLM behavior in real-world scenarios. In this\nwork, we introduce a novel evaluation instrument specifically designed for\nLLMs, called Core Sentiment Inventory (CSI). CSI is a bilingual tool, covering\nboth English and Chinese, that implicitly evaluates models' sentiment\ntendencies, providing an insightful psychological portrait of LLM across three\ndimensions: optimism, pessimism, and neutrality. Through extensive experiments,\nwe demonstrate that: 1) CSI effectively captures nuanced emotional patterns,\nrevealing significant variation in LLMs across languages and contexts; 2)\nCompared to current approaches, CSI significantly improves reliability,\nyielding more consistent results; and 3) The correlation between CSI scores and\nthe sentiment of LLM's real-world outputs exceeds 0.85, demonstrating its\nstrong validity in predicting LLM behavior. We make CSI public available via:\nhttps://github.com/dependentsign/CSI.",
      "tldr_zh": "本文研究探讨了评估大型语言模型（LLMs）的心理特质（如情感倾向和个性）的重要性，以提升其可信度，但现有方法如 BFI 存在可靠性低和预测有效性有限的问题。作者提出了一种新型双语工具 Core Sentiment Inventory (CSI)，用于隐式评估 LLMs 在乐观、悲观和中性三个维度的情感倾向。实验结果表明，CSI 能捕捉细微情感模式，提供更一致的评估结果，且其分数与 LLMs 实际输出情感的相关性超过 0.85，证明了其在预测模型行为方面的强有效性。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "Code available via https://github.com/dependentsign/CSI",
      "pdf_url": "http://arxiv.org/pdf/2503.20182v1",
      "published_date": "2025-03-26 03:14:31 UTC",
      "updated_date": "2025-03-26 03:14:31 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T06:30:25.445796"
    },
    {
      "arxiv_id": "2503.20176v1",
      "title": "Offline Reinforcement Learning with Discrete Diffusion Skills",
      "title_zh": "翻译失败",
      "authors": [
        "RuiXi Qiao",
        "Jie Cheng",
        "Xingyuan Dai",
        "Yonglin Tian",
        "Yisheng Lv"
      ],
      "abstract": "Skills have been introduced to offline reinforcement learning (RL) as\ntemporal abstractions to tackle complex, long-horizon tasks, promoting\nconsistent behavior and enabling meaningful exploration. While skills in\noffline RL are predominantly modeled within a continuous latent space, the\npotential of discrete skill spaces remains largely underexplored. In this\npaper, we propose a compact discrete skill space for offline RL tasks supported\nby state-of-the-art transformer-based encoder and diffusion-based decoder.\nCoupled with a high-level policy trained via offline RL techniques, our method\nestablishes a hierarchical RL framework where the trained diffusion decoder\nplays a pivotal role. Empirical evaluations show that the proposed algorithm,\nDiscrete Diffusion Skill (DDS), is a powerful offline RL method. DDS performs\ncompetitively on Locomotion and Kitchen tasks and excels on long-horizon tasks,\nachieving at least a 12 percent improvement on AntMaze-v2 benchmarks compared\nto existing offline RL approaches. Furthermore, DDS offers improved\ninterpretability, training stability, and online exploration compared to\nprevious skill-based methods.",
      "tldr_zh": "本论文提出了一种名为 Discrete Diffusion Skills (DDS) 的离线强化学习方法，利用离散技能空间作为时间抽象，以应对复杂长期任务。该方法结合 transformer-based encoder 和 diffusion-based decoder，构建紧凑的离散技能空间，并通过离线 RL 技术训练高层次策略，形成分层强化学习框架。实验结果显示，DDS 在 Locomotion 和 Kitchen 任务上表现出竞争性表现，并在长周期任务如 AntMaze-v2 基准上比现有方法至少提升 12%，同时提升了可解释性、训练稳定性和在线探索能力。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.RO"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.20176v1",
      "published_date": "2025-03-26 03:04:42 UTC",
      "updated_date": "2025-03-26 03:04:42 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T06:30:37.369028"
    },
    {
      "arxiv_id": "2504.08748v1",
      "title": "A Survey of Multimodal Retrieval-Augmented Generation",
      "title_zh": "翻译失败",
      "authors": [
        "Lang Mei",
        "Siyu Mo",
        "Zhihan Yang",
        "Chong Chen"
      ],
      "abstract": "Multimodal Retrieval-Augmented Generation (MRAG) enhances large language\nmodels (LLMs) by integrating multimodal data (text, images, videos) into\nretrieval and generation processes, overcoming the limitations of text-only\nRetrieval-Augmented Generation (RAG). While RAG improves response accuracy by\nincorporating external textual knowledge, MRAG extends this framework to\ninclude multimodal retrieval and generation, leveraging contextual information\nfrom diverse data types. This approach reduces hallucinations and enhances\nquestion-answering systems by grounding responses in factual, multimodal\nknowledge. Recent studies show MRAG outperforms traditional RAG, especially in\nscenarios requiring both visual and textual understanding. This survey reviews\nMRAG's essential components, datasets, evaluation methods, and limitations,\nproviding insights into its construction and improvement. It also identifies\nchallenges and future research directions, highlighting MRAG's potential to\nrevolutionize multimodal information retrieval and generation. By offering a\ncomprehensive perspective, this work encourages further exploration into this\npromising paradigm.",
      "tldr_zh": "这篇调查论文探讨了Multimodal Retrieval-Augmented Generation (MRAG)，它通过整合文本、图像和视频等多模态数据扩展了传统的Retrieval-Augmented Generation (RAG)，从而提升大型语言模型(LLMs)的响应准确性和减少幻觉。MRAG的核心方法包括多模态检索和生成，利用上下文信息在视觉与文本理解场景中表现出色，比RAG有显著改进。论文审视了MRAG的组件、数据集、评估方法以及局限性，并指出了未来的研究挑战和潜力，以推动该领域的创新发展。",
      "categories": [
        "cs.IR",
        "cs.AI",
        "cs.CL",
        "cs.ET",
        "cs.LG"
      ],
      "primary_category": "cs.IR",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.08748v1",
      "published_date": "2025-03-26 02:43:09 UTC",
      "updated_date": "2025-03-26 02:43:09 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T06:30:49.183891"
    },
    {
      "arxiv_id": "2503.20825v1",
      "title": "Debiasing Kernel-Based Generative Models",
      "title_zh": "翻译失败",
      "authors": [
        "Tian Qin",
        "Wei-Min Huang"
      ],
      "abstract": "We propose a novel two-stage framework of generative models named Debiasing\nKernel-Based Generative Models (DKGM) with the insights from kernel density\nestimation (KDE) and stochastic approximation. In the first stage of DKGM, we\nemploy KDE to bypass the obstacles in estimating the density of data without\nlosing too much image quality. One characteristic of KDE is oversmoothing,\nwhich makes the generated image blurry. Therefore, in the second stage, we\nformulate the process of reducing the blurriness of images as a statistical\ndebiasing problem and develop a novel iterative algorithm to improve image\nquality, which is inspired by the stochastic approximation. Extensive\nexperiments illustrate that the image quality of DKGM on CIFAR10 is comparable\nto state-of-the-art models such as diffusion models and GAN models. The\nperformance of DKGM on CelebA 128x128 and LSUN (Church) 128x128 is also\ncompetitive. We conduct extra experiments to exploit how the bandwidth in KDE\naffects the sample diversity and debiasing effect of DKGM. The connections\nbetween DKGM and score-based models are also discussed.",
      "tldr_zh": "本研究提出了一种名为 Debiasing Kernel-Based Generative Models (DKGM) 的两阶段生成模型框架，结合 Kernel Density Estimation (KDE) 和 stochastic approximation 的见解，以提高图像生成质量。第一阶段使用 KDE 估计数据密度，避免过多损失图像细节，但会因过度平滑导致模糊；第二阶段则将模糊减少视为统计 debiasing 问题，并开发一个基于 stochastic approximation 的迭代算法来优化图像。第二阶段的实验显示，DKGM 在 CIFAR10 数据集上的图像质量可与 diffusion models 和 GAN models 等最先进模型媲美，并在 CelebA 128x128 和 LSUN (Church) 128x128 上表现出色；此外，研究还探讨了 KDE 中的 bandwidth 对样本多样性和 debiasing 效果的影响，并讨论了 DKGM 与 score-based models 的联系。",
      "categories": [
        "stat.ML",
        "cs.AI"
      ],
      "primary_category": "stat.ML",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.20825v1",
      "published_date": "2025-03-26 01:48:34 UTC",
      "updated_date": "2025-03-26 01:48:34 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T06:31:01.125659"
    },
    {
      "arxiv_id": "2503.20824v1",
      "title": "Exploiting Temporal State Space Sharing for Video Semantic Segmentation",
      "title_zh": "翻译失败",
      "authors": [
        "Syed Ariff Syed Hesham",
        "Yun Liu",
        "Guolei Sun",
        "Henghui Ding",
        "Jing Yang",
        "Ender Konukoglu",
        "Xue Geng",
        "Xudong Jiang"
      ],
      "abstract": "Video semantic segmentation (VSS) plays a vital role in understanding the\ntemporal evolution of scenes. Traditional methods often segment videos\nframe-by-frame or in a short temporal window, leading to limited temporal\ncontext, redundant computations, and heavy memory requirements. To this end, we\nintroduce a Temporal Video State Space Sharing (TV3S) architecture to leverage\nMamba state space models for temporal feature sharing. Our model features a\nselective gating mechanism that efficiently propagates relevant information\nacross video frames, eliminating the need for a memory-heavy feature pool. By\nprocessing spatial patches independently and incorporating shifted operation,\nTV3S supports highly parallel computation in both training and inference\nstages, which reduces the delay in sequential state space processing and\nimproves the scalability for long video sequences. Moreover, TV3S incorporates\ninformation from prior frames during inference, achieving long-range temporal\ncoherence and superior adaptability to extended sequences. Evaluations on the\nVSPW and Cityscapes datasets reveal that our approach outperforms current\nstate-of-the-art methods, establishing a new standard for VSS with consistent\nresults across long video sequences. By achieving a good balance between\naccuracy and efficiency, TV3S shows a significant advancement in spatiotemporal\nmodeling, paving the way for efficient video analysis. The code is publicly\navailable at https://github.com/Ashesham/TV3S.git.",
      "tldr_zh": "本论文针对视频语义分割（Video Semantic Segmentation）的时序演变理解问题，提出了一种Temporal Video State Space Sharing (TV3S)架构，利用Mamba state space models实现时序特征共享，以解决传统方法的时序上下文有限、冗余计算和内存需求高等问题。TV3S引入选择性门控机制（selective gating mechanism）和shifted operation，支持独立处理空间补丁并实现高度并行计算，从而减少延迟并提升长视频序列的可扩展性。实验结果显示，该方法在VSPW和Cityscapes数据集上超越现有最先进方法，提供更好的时序一致性和准确性，同时在效率与性能之间取得平衡，推动了时空建模的进展。",
      "categories": [
        "eess.IV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "eess.IV",
      "comment": "IEEE/CVF Conference on Computer Vision and Pattern Recognition 2025",
      "pdf_url": "http://arxiv.org/pdf/2503.20824v1",
      "published_date": "2025-03-26 01:47:42 UTC",
      "updated_date": "2025-03-26 01:47:42 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T06:31:14.668383"
    },
    {
      "arxiv_id": "2503.21815v1",
      "title": "ATP: Adaptive Threshold Pruning for Efficient Data Encoding in Quantum Neural Networks",
      "title_zh": "翻译失败",
      "authors": [
        "Mohamed Afane",
        "Gabrielle Ebbrecht",
        "Ying Wang",
        "Juntao Chen",
        "Junaid Farooq"
      ],
      "abstract": "Quantum Neural Networks (QNNs) offer promising capabilities for complex data\ntasks, but are often constrained by limited qubit resources and high\nentanglement, which can hinder scalability and efficiency. In this paper, we\nintroduce Adaptive Threshold Pruning (ATP), an encoding method that reduces\nentanglement and optimizes data complexity for efficient computations in QNNs.\nATP dynamically prunes non-essential features in the data based on adaptive\nthresholds, effectively reducing quantum circuit requirements while preserving\nhigh performance. Extensive experiments across multiple datasets demonstrate\nthat ATP reduces entanglement entropy and improves adversarial robustness when\ncombined with adversarial training methods like FGSM. Our results highlight\nATPs ability to balance computational efficiency and model resilience,\nachieving significant performance improvements with fewer resources, which will\nhelp make QNNs more feasible in practical, resource-constrained settings.",
      "tldr_zh": "该论文针对 Quantum Neural Networks (QNNs) 的资源限制和高纠缠度问题，提出了一种 Adaptive Threshold Pruning (ATP) 编码方法，以优化数据复杂度和提高计算效率。ATP 通过动态修剪非必要特征基于自适应阈值，减少量子电路需求，同时保持模型的高性能。实验结果显示，在多个数据集上，ATP 显著降低了纠缠熵，并提升了对抗鲁棒性，特别是与 FGSM 等对抗训练结合时，最终实现性能提升并使 QNNs 在资源受限的实际环境中更具可行性。",
      "categories": [
        "quant-ph",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "quant-ph",
      "comment": "Accepted at the IEEE/CVF Conference on Computer Vision and Pattern\n  Recognition (CVPR), 2025.a",
      "pdf_url": "http://arxiv.org/pdf/2503.21815v1",
      "published_date": "2025-03-26 01:14:26 UTC",
      "updated_date": "2025-03-26 01:14:26 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T06:31:24.278007"
    },
    {
      "arxiv_id": "2503.20139v1",
      "title": "Look Before Leap: Look-Ahead Planning with Uncertainty in Reinforcement Learning",
      "title_zh": "翻译失败",
      "authors": [
        "Yongshuai Liu",
        "Xin Liu"
      ],
      "abstract": "Model-based reinforcement learning (MBRL) has demonstrated superior sample\nefficiency compared to model-free reinforcement learning (MFRL). However, the\npresence of inaccurate models can introduce biases during policy learning,\nresulting in misleading trajectories. The challenge lies in obtaining accurate\nmodels due to limited diverse training data, particularly in regions with\nlimited visits (uncertain regions). Existing approaches passively quantify\nuncertainty after sample generation, failing to actively collect uncertain\nsamples that could enhance state coverage and improve model accuracy. Moreover,\nMBRL often faces difficulties in making accurate multi-step predictions,\nthereby impacting overall performance. To address these limitations, we propose\na novel framework for uncertainty-aware policy optimization with model-based\nexploratory planning. In the model-based planning phase, we introduce an\nuncertainty-aware k-step lookahead planning approach to guide action selection\nat each step. This process involves a trade-off analysis between model\nuncertainty and value function approximation error, effectively enhancing\npolicy performance. In the policy optimization phase, we leverage an\nuncertainty-driven exploratory policy to actively collect diverse training\nsamples, resulting in improved model accuracy and overall performance of the RL\nagent. Our approach offers flexibility and applicability to tasks with varying\nstate/action spaces and reward structures. We validate its effectiveness\nthrough experiments on challenging robotic manipulation tasks and Atari games,\nsurpassing state-of-the-art methods with fewer interactions, thereby leading to\nsignificant performance improvements.",
      "tldr_zh": "这篇论文探讨了模型-based reinforcement learning (MBRL) 在面对模型不确定性时存在的偏差问题，与模型-free reinforcement learning (MFRL) 相比，MBRL 虽更高效，但易受有限训练数据影响。作者提出了一种 uncertainty-aware policy optimization 框架，包括 uncertainty-aware k-step lookahead planning 来平衡模型不确定性和价值函数近似误差，以及 uncertainty-driven exploratory policy 来主动收集多样样本，从而提升模型准确性和代理性能。该框架在机器人操作任务和 Atari 游戏上的实验中，展示了比现有方法更高的样本效率和显著性能提升。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.20139v1",
      "published_date": "2025-03-26 01:07:35 UTC",
      "updated_date": "2025-03-26 01:07:35 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T06:31:37.297210"
    },
    {
      "arxiv_id": "2503.20138v1",
      "title": "Unlocking the Value of Decentralized Data: A Federated Dual Learning Approach for Model Aggregation",
      "title_zh": "解",
      "authors": [
        "Junyi Zhu",
        "Ruicong Yao",
        "Taha Ceritli",
        "Savas Ozkan",
        "Matthew B. Blaschko",
        "Eunchung Noh",
        "Jeongwon Min",
        "Cho Jung Min",
        "Mete Ozay"
      ],
      "abstract": "Artificial Intelligence (AI) technologies have revolutionized numerous\nfields, yet their applications often rely on costly and time-consuming data\ncollection processes. Federated Learning (FL) offers a promising alternative by\nenabling AI models to be trained on decentralized data where data is scattered\nacross clients (distributed nodes). However, existing FL approaches struggle to\nmatch the performance of centralized training due to challenges such as\nheterogeneous data distribution and communication delays, limiting their\npotential for breakthroughs. We observe that many real-world use cases involve\nhybrid data regimes, in which a server (center node) has access to some data\nwhile a large amount of data is distributed across associated clients. To\nimprove the utilization of decentralized data under this regime, address data\nheterogeneity issue, and facilitate asynchronous communication between the\nserver and clients, we propose a dual learning approach that leverages\ncentralized data at the server to guide the merging of model updates from\nclients. Our method accommodates scenarios where server data is out-of-domain\nrelative to decentralized client data, making it applicable to a wide range of\nuse cases. We provide theoretical analysis demonstrating the faster convergence\nof our method compared to existing methods. Furthermore, experimental results\nacross various scenarios show that our approach significantly outperforms\nexisting technologies, highlighting its potential to unlock the value of large\namounts of decentralized data.",
      "tldr_zh": "该论文针对 Federated Learning (FL) 在数据异质性和通信延迟下性能不如集中式训练的问题，提出了一种 Federated Dual Learning 方法，利用服务器的集中式数据来指导客户端模型更新的合并。  \n这种方法适用于服务器数据与客户端数据不相关的情景，能够有效处理数据异质性并支持异步通信。  \n理论分析证明了该方法的收敛速度更快，实验结果显示其在各种场景中显著优于现有技术，从而提升了分散数据的利用价值。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.20138v1",
      "published_date": "2025-03-26 01:00:35 UTC",
      "updated_date": "2025-03-26 01:00:35 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T06:31:49.110798"
    },
    {
      "arxiv_id": "2503.20822v1",
      "title": "Synthetic Video Enhances Physical Fidelity in Video Synthesis",
      "title_zh": "合成视频增强视频合成中的物理保真度",
      "authors": [
        "Qi Zhao",
        "Xingyu Ni",
        "Ziyu Wang",
        "Feng Cheng",
        "Ziyan Yang",
        "Lu Jiang",
        "Bohan Wang"
      ],
      "abstract": "We investigate how to enhance the physical fidelity of video generation\nmodels by leveraging synthetic videos derived from computer graphics pipelines.\nThese rendered videos respect real-world physics, such as maintaining 3D\nconsistency, and serve as a valuable resource that can potentially improve\nvideo generation models. To harness this potential, we propose a solution that\ncurates and integrates synthetic data while introducing a method to transfer\nits physical realism to the model, significantly reducing unwanted artifacts.\nThrough experiments on three representative tasks emphasizing physical\nconsistency, we demonstrate its efficacy in enhancing physical fidelity. While\nour model still lacks a deep understanding of physics, our work offers one of\nthe first empirical demonstrations that synthetic video enhances physical\nfidelity in video synthesis. Website: https://kevinz8866.github.io/simulation/",
      "tldr_zh": "本研究探讨了利用计算机图形 pipelines 生成的合成 videos 来提升视频生成模型的 physical fidelity。研究提出一种解决方案，通过整理整合合成数据并引入物理真实性转移方法，显著减少 unwanted artifacts。实验在三个强调物理一致性的任务上验证了该方法的有效性，尽管模型仍缺乏对物理的深入理解，但这首次提供了实证证据，证明合成 videos 能增强视频合成中的 physical fidelity。",
      "categories": [
        "eess.IV",
        "cs.AI",
        "cs.GR"
      ],
      "primary_category": "eess.IV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.20822v1",
      "published_date": "2025-03-26 00:45:07 UTC",
      "updated_date": "2025-03-26 00:45:07 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T06:32:03.026118"
    },
    {
      "arxiv_id": "2503.20126v1",
      "title": "Can We Make Code Green? Understanding Trade-Offs in LLMs vs. Human Code Optimizations",
      "title_zh": "我们能让代码变得绿色吗？理解LLMs与人类代码优化中的权衡",
      "authors": [
        "Pooja Rani",
        "Jan-Andrea Bard",
        "June Sallou",
        "Alexander Boll",
        "Timo Kehrer",
        "Alberto Bacchelli"
      ],
      "abstract": "The rapid technological evolution has accelerated software development for\nvarious domains and use cases, contributing to a growing share of global carbon\nemissions. While recent large language models (LLMs) claim to assist developers\nin optimizing code for performance and energy efficiency, their efficacy in\nreal-world scenarios remains under exploration. In this work, we explore the\neffectiveness of LLMs in reducing the environmental footprint of real-world\nprojects, focusing on software written in Matlab-widely used in both academia\nand industry for scientific and engineering applications. We analyze\nenergy-focused optimization on 400 scripts across 100 top GitHub repositories.\nWe examine potential 2,176 optimizations recommended by leading LLMs, such as\nGPT-3, GPT-4, Llama, and Mixtral, and a senior Matlab developer, on energy\nconsumption, memory usage, execution time consumption, and code correctness.\nThe developer serves as a real-world baseline for comparing typical human and\nLLM-generated optimizations.\n  Mapping these optimizations to 13 high-level themes, we found that LLMs\npropose a broad spectrum of improvements--beyond energy efficiency--including\nimproving code readability and maintainability, memory management, error\nhandling while the developer overlooked some parallel processing, error\nhandling etc. However, our statistical tests reveal that the energy-focused\noptimizations unexpectedly negatively impacted memory usage, with no clear\nbenefits regarding execution time or energy consumption. Our qualitative\nanalysis of energy-time trade-offs revealed that some themes, such as\nvectorization preallocation, were among the common themes shaping these\ntrade-offs. With LLMs becoming ubiquitous in modern software development, our\nstudy serves as a call to action: prioritizing the evaluation of common coding\npractices to identify the green ones.",
      "tldr_zh": "本研究探讨了大型语言模型（LLMs，如 GPT-3、GPT-4、Llama 和 Mixtral）在优化 Matlab 代码能源效率方面的有效性，与资深人类开发者进行比较，分析了 400 个脚本的 2,176 个优化建议。研究评估了这些优化对能源消耗、内存使用、执行时间和代码正确性的影响，发现 LLMs 提出了广泛改进（如代码可读性、内存管理和错误处理），但人类开发者在某些方面（如并行处理）表现更强。结果显示，能源优化意外地增加了内存使用，并未显著改善执行时间或能源消耗，特别是在 vectorization 和 preallocation 等主题上的权衡。总之，该研究呼吁在软件开发中优先评估常见编码实践，以识别“绿色”优化策略。",
      "categories": [
        "cs.SE",
        "cs.AI",
        "cs.PF"
      ],
      "primary_category": "cs.SE",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.20126v1",
      "published_date": "2025-03-26 00:27:29 UTC",
      "updated_date": "2025-03-26 00:27:29 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T06:32:13.264622"
    },
    {
      "arxiv_id": "2504.07118v1",
      "title": "Sacred or Secular? Religious Bias in AI-Generated Financial Advice",
      "title_zh": "神圣还是世俗？AI 生成的财务建议中的宗教偏见",
      "authors": [
        "Muhammad Salar Khan",
        "Hamza Umer"
      ],
      "abstract": "This study examines religious biases in AI-generated financial advice,\nfocusing on ChatGPT's responses to financial queries. Using a prompt-based\nmethodology and content analysis, we find that 50% of the financial emails\ngenerated by ChatGPT exhibit religious biases, with explicit biases present in\nboth ingroup and outgroup interactions. While ingroup biases personalize\nresponses based on religious alignment, outgroup biases introduce religious\nframing that may alienate clients or create ideological friction. These\nfindings align with broader research on AI bias and suggest that ChatGPT is not\nmerely reflecting societal biases but actively shaping financial discourse\nbased on perceived religious identity. Using the Critical Algorithm Studies\nframework, we argue that ChatGPT functions as a mediator of financial\nnarratives, selectively reinforcing religious perspectives. This study\nunderscores the need for greater transparency, bias mitigation strategies, and\nregulatory oversight to ensure neutrality in AI-driven financial services.",
      "tldr_zh": "这篇论文探讨了AI生成财务建议中的宗教偏见，焦点是ChatGPT，通过提示-based methodology和内容分析发现，50%的生成的财务邮件显示显性宗教偏见，包括ingroup biases（基于宗教一致性的个性化响应）和outgroup biases（可能疏远客户或制造意识形态冲突）。研究表明，ChatGPT不仅仅反映社会偏见，还主动塑造财务话语；使用Critical Algorithm Studies框架进行分析，强调需要提升透明度、实施偏见缓解策略，并加强监管以确保AI驱动财务服务的中立性。",
      "categories": [
        "cs.CY",
        "cs.AI",
        "cs.ET"
      ],
      "primary_category": "cs.CY",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.07118v1",
      "published_date": "2025-03-26 00:27:04 UTC",
      "updated_date": "2025-03-26 00:27:04 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T06:32:25.317156"
    },
    {
      "arxiv_id": "2503.20124v1",
      "title": "Synthesizing world models for bilevel planning",
      "title_zh": "翻译失败",
      "authors": [
        "Zergham Ahmed",
        "Joshua B. Tenenbaum",
        "Christopher J. Bates",
        "Samuel J. Gershman"
      ],
      "abstract": "Modern reinforcement learning (RL) systems have demonstrated remarkable\ncapabilities in complex environments, such as video games. However, they still\nfall short of achieving human-like sample efficiency and adaptability when\nlearning new domains. Theory-based reinforcement learning (TBRL) is an\nalgorithmic framework specifically designed to address this gap. Modeled on\ncognitive theories, TBRL leverages structured, causal world models - \"theories\"\n- as forward simulators for use in planning, generalization and exploration.\nAlthough current TBRL systems provide compelling explanations of how humans\nlearn to play video games, they face several technical limitations: their\ntheory languages are restrictive, and their planning algorithms are not\nscalable. To address these challenges, we introduce TheoryCoder, an\ninstantiation of TBRL that exploits hierarchical representations of theories\nand efficient program synthesis methods for more powerful learning and\nplanning. TheoryCoder equips agents with general-purpose abstractions (e.g.,\n\"move to\"), which are then grounded in a particular environment by learning a\nlow-level transition model (a Python program synthesized from observations by a\nlarge language model). A bilevel planning algorithm can exploit this\nhierarchical structure to solve large domains. We demonstrate that this\napproach can be successfully applied to diverse and challenging grid-world\ngames, where approaches based on directly synthesizing a policy perform poorly.\nAblation studies demonstrate the benefits of using hierarchical abstractions.",
      "tldr_zh": "本文提出 TheoryCoder，一种基于理论的强化学习(TBRL)框架，旨在提升强化学习(RL)系统在复杂环境中的样本效率和适应性，通过利用分层理论表示和高效程序合成方法来构建结构化的世界模型。TheoryCoder 提供通用抽象（如 \"move to\"）并通过学习低级过渡模型（由大语言模型从观察中合成的 Python 程序）实现环境接地，然后利用双层规划(bilevel planning)算法处理大型领域。实验在多样网格世界游戏中显示，TheoryCoder 显著优于直接合成策略的方法，消融研究证实了分层抽象的益处。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "25 pages",
      "pdf_url": "http://arxiv.org/pdf/2503.20124v1",
      "published_date": "2025-03-26 00:10:01 UTC",
      "updated_date": "2025-03-26 00:10:01 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T06:32:37.852804"
    }
  ],
  "raw_papers_fetched": true,
  "papers_count": 95,
  "processed_papers_count": 95,
  "failed_papers_count": 0,
  "summary_generated": true,
  "daily_data_saved": true,
  "last_update": "2025-05-24T06:32:53.898109"
}
[
  {
    "arxiv_id": "2401.00974v1",
    "title": "Downstream Task-Oriented Generative Model Selections on Synthetic Data Training for Fraud Detection Models",
    "authors": [
      "Yinan Cheng",
      "Chi-Hua Wang",
      "Vamsi K. Potluru",
      "Tucker Balch",
      "Guang Cheng"
    ],
    "abstract": "Devising procedures for downstream task-oriented generative model selections\nis an unresolved problem of practical importance. Existing studies focused on\nthe utility of a single family of generative models. They provided limited\ninsights on how synthetic data practitioners select the best family generative\nmodels for synthetic training tasks given a specific combination of machine\nlearning model class and performance metric. In this paper, we approach the\ndownstream task-oriented generative model selections problem in the case of\ntraining fraud detection models and investigate the best practice given\ndifferent combinations of model interpretability and model performance\nconstraints. Our investigation supports that, while both Neural\nNetwork(NN)-based and Bayesian Network(BN)-based generative models are both\ngood to complete synthetic training task under loose model interpretability\nconstrain, the BN-based generative models is better than NN-based when\nsynthetic training fraud detection model under strict model interpretability\nconstrain. Our results provides practical guidance for machine learning\npractitioner who is interested in replacing their training dataset from real to\nsynthetic, and shed lights on more general downstream task-oriented generative\nmodel selection problems.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "The following article has been accepted by ICAIF22, Synthetic Data\n  for AI in Finance; see\n  https://sites.google.com/view/icaif-synthetic-2022/program",
    "pdf_url": "http://arxiv.org/pdf/2401.00974v1",
    "published_date": "2024-01-01 23:33:56 UTC",
    "updated_date": "2024-01-01 23:33:56 UTC"
  },
  {
    "arxiv_id": "2401.01388v1",
    "title": "Directional Antenna Systems for Long-Range Through-Wall Human Activity Recognition",
    "authors": [
      "Julian Strohmayer",
      "Martin Kampel"
    ],
    "abstract": "WiFi Channel State Information (CSI)-based human activity recognition (HAR)\nenables contactless, long-range sensing in spatially constrained environments\nwhile preserving visual privacy. However, despite the presence of numerous\nWiFi-enabled devices around us, few expose CSI to users, resulting in a lack of\nsensing hardware options. Variants of the Espressif ESP32 have emerged as\npotential low-cost and easy-to-deploy solutions for WiFi CSI-based HAR. In this\nwork, four ESP32-S3-based 2.4GHz directional antenna systems are evaluated for\ntheir ability to facilitate long-range through-wall HAR. Two promising systems\nare proposed, one of which combines the ESP32-S3 with a directional biquad\nantenna. This combination represents, to the best of our knowledge, the first\ndemonstration of such a system in WiFi-based HAR. The second system relies on\nthe built-in printed inverted-F antenna (PIFA) of the ESP32-S3 and achieves\ndirectionality through a plane reflector. In a comprehensive evaluation of\nline-of-sight (LOS) and non-line-of-sight (NLOS) HAR performance, both systems\nare deployed in an office environment spanning a distance of 18 meters across\nfive rooms. In this experimental setup, the Wallhack1.8k dataset, comprising\n1806 CSI amplitude spectrograms of human activities, is collected and made\npublicly available. Based on Wallhack1.8k, we train activity recognition models\nusing the EfficientNetV2 architecture to assess system performance in LOS and\nNLOS scenarios. For the core NLOS activity recognition problem, the biquad\nantenna and PIFA-based systems achieve accuracies of 92.0$\\pm$3.5 and\n86.8$\\pm$4.7, respectively, demonstrating the feasibility of long-range\nthrough-wall HAR with the proposed systems.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "arXiv admin note: text overlap with arXiv:2401.00964",
    "pdf_url": "http://arxiv.org/pdf/2401.01388v1",
    "published_date": "2024-01-01 22:35:22 UTC",
    "updated_date": "2024-01-01 22:35:22 UTC"
  },
  {
    "arxiv_id": "2401.00964v1",
    "title": "Data Augmentation Techniques for Cross-Domain WiFi CSI-based Human Activity Recognition",
    "authors": [
      "Julian Strohmayer",
      "Martin Kampel"
    ],
    "abstract": "The recognition of human activities based on WiFi Channel State Information\n(CSI) enables contactless and visual privacy-preserving sensing in indoor\nenvironments. However, poor model generalization, due to varying environmental\nconditions and sensing hardware, is a well-known problem in this space. To\naddress this issue, in this work, data augmentation techniques commonly used in\nimage-based learning are applied to WiFi CSI to investigate their effects on\nmodel generalization performance in cross-scenario and cross-system settings.\nIn particular, we focus on the generalization between line-of-sight (LOS) and\nnon-line-of-sight (NLOS) through-wall scenarios, as well as on the\ngeneralization between different antenna systems, which remains under-explored.\nWe collect and make publicly available a dataset of CSI amplitude spectrograms\nof human activities. Utilizing this data, an ablation study is conducted in\nwhich activity recognition models based on the EfficientNetV2 architecture are\ntrained, allowing us to assess the effects of each augmentation on model\ngeneralization performance. The gathered results show that specific\ncombinations of simple data augmentation techniques applied to CSI amplitude\ndata can significantly improve cross-scenario and cross-system generalization.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2401.00964v1",
    "published_date": "2024-01-01 22:27:59 UTC",
    "updated_date": "2024-01-01 22:27:59 UTC"
  },
  {
    "arxiv_id": "2401.00961v2",
    "title": "Automated Model Selection for Tabular Data",
    "authors": [
      "Avinash Amballa",
      "Gayathri Akkinapalli",
      "Manas Madine",
      "Naga Pavana Priya Yarrabolu",
      "Przemyslaw A. Grabowicz"
    ],
    "abstract": "Structured data in the form of tabular datasets contain features that are\ndistinct and discrete, with varying individual and relative importances to the\ntarget. Combinations of one or more features may be more predictive and\nmeaningful than simple individual feature contributions. R's mixed effect\nlinear models library allows users to provide such interactive feature\ncombinations in the model design. However, given many features and possible\ninteractions to select from, model selection becomes an exponentially difficult\ntask. We aim to automate the model selection process for predictions on tabular\ndatasets incorporating feature interactions while keeping computational costs\nsmall. The framework includes two distinct approaches for feature selection: a\nPriority-based Random Grid Search and a Greedy Search method. The\nPriority-based approach efficiently explores feature combinations using prior\nprobabilities to guide the search. The Greedy method builds the solution\niteratively by adding or removing features based on their impact. Experiments\non synthetic demonstrate the ability to effectively capture predictive feature\ncombinations.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "10 pages",
    "pdf_url": "http://arxiv.org/pdf/2401.00961v2",
    "published_date": "2024-01-01 21:41:20 UTC",
    "updated_date": "2024-05-29 01:03:16 UTC"
  },
  {
    "arxiv_id": "2401.00850v2",
    "title": "Refining Pre-Trained Motion Models",
    "authors": [
      "Xinglong Sun",
      "Adam W. Harley",
      "Leonidas J. Guibas"
    ],
    "abstract": "Given the difficulty of manually annotating motion in video, the current best\nmotion estimation methods are trained with synthetic data, and therefore\nstruggle somewhat due to a train/test gap. Self-supervised methods hold the\npromise of training directly on real video, but typically perform worse. These\ninclude methods trained with warp error (i.e., color constancy) combined with\nsmoothness terms, and methods that encourage cycle-consistency in the estimates\n(i.e., tracking backwards should yield the opposite trajectory as tracking\nforwards). In this work, we take on the challenge of improving state-of-the-art\nsupervised models with self-supervised training. We find that when the\ninitialization is supervised weights, most existing self-supervision techniques\nactually make performance worse instead of better, which suggests that the\nbenefit of seeing the new data is overshadowed by the noise in the training\nsignal. Focusing on obtaining a \"clean\" training signal from real-world\nunlabelled video, we propose to separate label-making and training into two\ndistinct stages. In the first stage, we use the pre-trained model to estimate\nmotion in a video, and then select the subset of motion estimates which we can\nverify with cycle-consistency. This produces a sparse but accurate\npseudo-labelling of the video. In the second stage, we fine-tune the model to\nreproduce these outputs, while also applying augmentations on the input. We\ncomplement this boot-strapping method with simple techniques that densify and\nre-balance the pseudo-labels, ensuring that we do not merely train on \"easy\"\ntracks. We show that our method yields reliable gains over fully-supervised\nmethods in real videos, for both short-term (flow-based) and long-range\n(multi-frame) pixel tracking.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "Accepted at ICRA 2024",
    "pdf_url": "http://arxiv.org/pdf/2401.00850v2",
    "published_date": "2024-01-01 18:59:33 UTC",
    "updated_date": "2024-02-17 03:09:32 UTC"
  },
  {
    "arxiv_id": "2401.00832v3",
    "title": "Taking the Next Step with Generative Artificial Intelligence: The Transformative Role of Multimodal Large Language Models in Science Education",
    "authors": [
      "Arne Bewersdorff",
      "Christian Hartmann",
      "Marie Hornberger",
      "Kathrin Seßler",
      "Maria Bannert",
      "Enkelejda Kasneci",
      "Gjergji Kasneci",
      "Xiaoming Zhai",
      "Claudia Nerdel"
    ],
    "abstract": "The integration of Artificial Intelligence (AI), particularly Large Language\nModel (LLM)-based systems, in education has shown promise in enhancing teaching\nand learning experiences. However, the advent of Multimodal Large Language\nModels (MLLMs) like GPT-4 with vision (GPT-4V), capable of processing\nmultimodal data including text, sound, and visual inputs, opens a new era of\nenriched, personalized, and interactive learning landscapes in education.\nGrounded in theory of multimedia learning, this paper explores the\ntransformative role of MLLMs in central aspects of science education by\npresenting exemplary innovative learning scenarios. Possible applications for\nMLLMs could range from content creation to tailored support for learning,\nfostering competencies in scientific practices, and providing assessment and\nfeedback. These scenarios are not limited to text-based and uni-modal formats\nbut can be multimodal, increasing thus personalization, accessibility, and\npotential learning effectiveness. Besides many opportunities, challenges such\nas data protection and ethical considerations become more salient, calling for\nrobust frameworks to ensure responsible integration. This paper underscores the\nnecessity for a balanced approach in implementing MLLMs, where the technology\ncomplements rather than supplants the educator's role, ensuring thus an\neffective and ethical use of AI in science education. It calls for further\nresearch to explore the nuanced implications of MLLMs on the evolving role of\neducators and to extend the discourse beyond science education to other\ndisciplines. Through the exploration of potentials, challenges, and future\nimplications, we aim to contribute to a preliminary understanding of the\ntransformative trajectory of MLLMs in science education and beyond.",
    "categories": [
      "cs.AI",
      "cs.CY"
    ],
    "primary_category": "cs.AI",
    "comment": "revised version 2. September 2024",
    "pdf_url": "http://arxiv.org/pdf/2401.00832v3",
    "published_date": "2024-01-01 18:11:43 UTC",
    "updated_date": "2024-09-19 08:07:04 UTC"
  },
  {
    "arxiv_id": "2401.02984v2",
    "title": "Large Language Models in Mental Health Care: a Scoping Review",
    "authors": [
      "Yining Hua",
      "Fenglin Liu",
      "Kailai Yang",
      "Zehan Li",
      "Hongbin Na",
      "Yi-han Sheu",
      "Peilin Zhou",
      "Lauren V. Moran",
      "Sophia Ananiadou",
      "Andrew Beam",
      "John Torous"
    ],
    "abstract": "The integration of large language models (LLMs) in mental health care is an\nemerging field. There is a need to systematically review the application\noutcomes and delineate the advantages and limitations in clinical settings.\nThis review aims to provide a comprehensive overview of the use of LLMs in\nmental health care, assessing their efficacy, challenges, and potential for\nfuture applications. A systematic search was conducted across multiple\ndatabases including PubMed, Web of Science, Google Scholar, arXiv, medRxiv, and\nPsyArXiv in November 2023. All forms of original research, peer-reviewed or\nnot, published or disseminated between October 1, 2019, and December 2, 2023,\nare included without language restrictions if they used LLMs developed after T5\nand directly addressed research questions in mental health care settings. From\nan initial pool of 313 articles, 34 met the inclusion criteria based on their\nrelevance to LLM application in mental health care and the robustness of\nreported outcomes. Diverse applications of LLMs in mental health care are\nidentified, including diagnosis, therapy, patient engagement enhancement, etc.\nKey challenges include data availability and reliability, nuanced handling of\nmental states, and effective evaluation methods. Despite successes in accuracy\nand accessibility improvement, gaps in clinical applicability and ethical\nconsiderations were evident, pointing to the need for robust data, standardized\nevaluations, and interdisciplinary collaboration. LLMs hold substantial promise\nfor enhancing mental health care. For their full potential to be realized,\nemphasis must be placed on developing robust datasets, development and\nevaluation frameworks, ethical guidelines, and interdisciplinary collaborations\nto address current limitations.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2401.02984v2",
    "published_date": "2024-01-01 17:35:52 UTC",
    "updated_date": "2024-08-21 13:55:37 UTC"
  },
  {
    "arxiv_id": "2401.00926v4",
    "title": "Accurate Leukocyte Detection Based on Deformable-DETR and Multi-Level Feature Fusion for Aiding Diagnosis of Blood Diseases",
    "authors": [
      "Yifei Chen",
      "Chenyan Zhang",
      "Ben Chen",
      "Yiyu Huang",
      "Yifei Sun",
      "Changmiao Wang",
      "Xianjun Fu",
      "Yuxing Dai",
      "Feiwei Qin",
      "Yong Peng",
      "Yu Gao"
    ],
    "abstract": "In standard hospital blood tests, the traditional process requires doctors to\nmanually isolate leukocytes from microscopic images of patients' blood using\nmicroscopes. These isolated leukocytes are then categorized via automatic\nleukocyte classifiers to determine the proportion and volume of different types\nof leukocytes present in the blood samples, aiding disease diagnosis. This\nmethodology is not only time-consuming and labor-intensive, but it also has a\nhigh propensity for errors due to factors such as image quality and\nenvironmental conditions, which could potentially lead to incorrect subsequent\nclassifications and misdiagnosis. To address these issues, this paper proposes\nan innovative method of leukocyte detection: the Multi-level Feature Fusion and\nDeformable Self-attention DETR (MFDS-DETR). To tackle the issue of leukocyte\nscale disparity, we designed the High-level Screening-feature Fusion Pyramid\n(HS-FPN), enabling multi-level fusion. This model uses high-level features as\nweights to filter low-level feature information via a channel attention module\nand then merges the screened information with the high-level features, thus\nenhancing the model's feature expression capability. Further, we address the\nissue of leukocyte feature scarcity by incorporating a multi-scale deformable\nself-attention module in the encoder and using the self-attention and\ncross-deformable attention mechanisms in the decoder, which aids in the\nextraction of the global features of the leukocyte feature maps. The\neffectiveness, superiority, and generalizability of the proposed MFDS-DETR\nmethod are confirmed through comparisons with other cutting-edge leukocyte\ndetection models using the private WBCDD, public LISC and BCCD datasets. Our\nsource code and private WBCCD dataset are available at\nhttps://github.com/JustlfC03/MFDS-DETR.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "15 pages, 11 figures, accept Computers in Biology and Medicine 2024",
    "pdf_url": "http://arxiv.org/pdf/2401.00926v4",
    "published_date": "2024-01-01 16:28:30 UTC",
    "updated_date": "2024-01-10 08:26:00 UTC"
  },
  {
    "arxiv_id": "2401.00788v1",
    "title": "Astraios: Parameter-Efficient Instruction Tuning Code Large Language Models",
    "authors": [
      "Terry Yue Zhuo",
      "Armel Zebaze",
      "Nitchakarn Suppattarachai",
      "Leandro von Werra",
      "Harm de Vries",
      "Qian Liu",
      "Niklas Muennighoff"
    ],
    "abstract": "The high cost of full-parameter fine-tuning (FFT) of Large Language Models\n(LLMs) has led to a series of parameter-efficient fine-tuning (PEFT) methods.\nHowever, it remains unclear which methods provide the best cost-performance\ntrade-off at different model scales. We introduce Astraios, a suite of 28\ninstruction-tuned OctoCoder models using 7 tuning methods and 4 model sizes up\nto 16 billion parameters. Through investigations across 5 tasks and 8 different\ndatasets encompassing both code comprehension and code generation tasks, we\nfind that FFT generally leads to the best downstream performance across all\nscales, and PEFT methods differ significantly in their efficacy based on the\nmodel scale. LoRA usually offers the most favorable trade-off between cost and\nperformance. Further investigation into the effects of these methods on both\nmodel robustness and code security reveals that larger models tend to\ndemonstrate reduced robustness and less security. At last, we explore the\nrelationships among updated parameters, cross-entropy loss, and task\nperformance. We find that the tuning effectiveness observed in small models\ngeneralizes well to larger models, and the validation loss in instruction\ntuning can be a reliable indicator of overall downstream performance.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.SE"
    ],
    "primary_category": "cs.CL",
    "comment": "25 pages (12 main), 19 figures, 8 tables",
    "pdf_url": "http://arxiv.org/pdf/2401.00788v1",
    "published_date": "2024-01-01 15:30:19 UTC",
    "updated_date": "2024-01-01 15:30:19 UTC"
  },
  {
    "arxiv_id": "2401.02982v4",
    "title": "FinDABench: Benchmarking Financial Data Analysis Ability of Large Language Models",
    "authors": [
      "Shu Liu",
      "Shangqing Zhao",
      "Chenghao Jia",
      "Xinlin Zhuang",
      "Zhaoguang Long",
      "Jie Zhou",
      "Aimin Zhou",
      "Man Lan",
      "Qingquan Wu",
      "Chong Yang"
    ],
    "abstract": "Large Language Models (LLMs) have demonstrated impressive capabilities across\na wide range of tasks. However, their proficiency and reliability in the\nspecialized domain of financial data analysis, particularly focusing on\ndata-driven thinking, remain uncertain. To bridge this gap, we introduce\n\\texttt{FinDABench}, a comprehensive benchmark designed to evaluate the\nfinancial data analysis capabilities of LLMs within this context.\n\\texttt{FinDABench} assesses LLMs across three dimensions: 1)\n\\textbf{Foundational Ability}, evaluating the models' ability to perform\nfinancial numerical calculation and corporate sentiment risk assessment; 2)\n\\textbf{Reasoning Ability}, determining the models' ability to quickly\ncomprehend textual information and analyze abnormal financial reports; and 3)\n\\textbf{Technical Skill}, examining the models' use of technical knowledge to\naddress real-world data analysis challenges involving analysis generation and\ncharts visualization from multiple perspectives. We will release\n\\texttt{FinDABench}, and the evaluation scripts at\n\\url{https://github.com/cubenlp/BIBench}. \\texttt{FinDABench} aims to provide a\nmeasure for in-depth analysis of LLM abilities and foster the advancement of\nLLMs in the field of financial data analysis.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2401.02982v4",
    "published_date": "2024-01-01 15:26:23 UTC",
    "updated_date": "2024-06-14 10:17:40 UTC"
  },
  {
    "arxiv_id": "2401.00779v1",
    "title": "Temporal Validity Change Prediction",
    "authors": [
      "Georg Wenzel",
      "Adam Jatowt"
    ],
    "abstract": "Temporal validity is an important property of text that is useful for many\ndownstream applications, such as recommender systems, conversational AI, or\nstory understanding. Existing benchmarking tasks often require models to\nidentify the temporal validity duration of a single statement. However, in many\ncases, additional contextual information, such as sentences in a story or posts\non a social media profile, can be collected from the available text stream.\nThis contextual information may greatly alter the duration for which a\nstatement is expected to be valid. We propose Temporal Validity Change\nPrediction, a natural language processing task benchmarking the capability of\nmachine learning models to detect contextual statements that induce such\nchange. We create a dataset consisting of temporal target statements sourced\nfrom Twitter and crowdsource sample context statements. We then benchmark a set\nof transformer-based language models on our dataset. Finally, we experiment\nwith temporal validity duration prediction as an auxiliary task to improve the\nperformance of the state-of-the-art model.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "68T50",
      "I.2.7"
    ],
    "primary_category": "cs.CL",
    "comment": "9 pages, 9 figures, 3 tables",
    "pdf_url": "http://arxiv.org/pdf/2401.00779v1",
    "published_date": "2024-01-01 14:58:53 UTC",
    "updated_date": "2024-01-01 14:58:53 UTC"
  },
  {
    "arxiv_id": "2401.00776v1",
    "title": "Edge Computing based Human-Robot Cognitive Fusion: A Medical Case Study in the Autism Spectrum Disorder Therapy",
    "authors": [
      "Qin Yang"
    ],
    "abstract": "In recent years, edge computing has served as a paradigm that enables many\nfuture technologies like AI, Robotics, IoT, and high-speed wireless sensor\nnetworks (like 5G) by connecting cloud computing facilities and services to the\nend users. Especially in medical and healthcare applications, it provides\nremote patient monitoring and increases voluminous multimedia. From the\nrobotics angle, robot-assisted therapy (RAT) is an active-assistive robotic\ntechnology in rehabilitation robotics, attracting many researchers to study and\nbenefit people with disability like autism spectrum disorder (ASD) children.\nHowever, the main challenge of RAT is that the model capable of detecting the\naffective states of ASD people exists and can recall individual preferences.\nMoreover, involving expert diagnosis and recommendations to guide robots in\nupdating the therapy approach to adapt to different statuses and scenarios is a\ncrucial part of the ASD therapy process. This paper proposes the architecture\nof edge cognitive computing by combining human experts and assisted robots\ncollaborating in the same framework to help ASD patients with long-term\nsupport. By integrating the real-time computing and analysis of a new cognitive\nrobotic model for ASD therapy, the proposed architecture can achieve a seamless\nremote diagnosis, round-the-clock symptom monitoring, emergency warning,\ntherapy alteration, and advanced assistance.",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.DC",
      "cs.LG",
      "cs.MA"
    ],
    "primary_category": "cs.RO",
    "comment": "This paper was accepted by the 38th AAAI 2024 workshop: \"Cooperative\n  Multi-Agent Systems Decision-Making and Learning: From Individual Needs to\n  Swarm Intelligence\"",
    "pdf_url": "http://arxiv.org/pdf/2401.00776v1",
    "published_date": "2024-01-01 14:45:19 UTC",
    "updated_date": "2024-01-01 14:45:19 UTC"
  },
  {
    "arxiv_id": "2401.00773v3",
    "title": "Unsupervised Outlier Detection using Random Subspace and Subsampling Ensembles of Dirichlet Process Mixtures",
    "authors": [
      "Dongwook Kim",
      "Juyeon Park",
      "Hee Cheol Chung",
      "Seonghyun Jeong"
    ],
    "abstract": "Probabilistic mixture models are recognized as effective tools for\nunsupervised outlier detection owing to their interpretability and global\ncharacteristics. Among these, Dirichlet process mixture models stand out as a\nstrong alternative to conventional finite mixture models for both clustering\nand outlier detection tasks. Unlike finite mixture models, Dirichlet process\nmixtures are infinite mixture models that automatically determine the number of\nmixture components based on the data. Despite their advantages, the adoption of\nDirichlet process mixture models for unsupervised outlier detection has been\nlimited by challenges related to computational inefficiency and sensitivity to\noutliers in the construction of outlier detectors. Additionally, Dirichlet\nprocess Gaussian mixtures struggle to effectively model non-Gaussian data with\ndiscrete or binary features. To address these challenges, we propose a novel\noutlier detection method that utilizes ensembles of Dirichlet process Gaussian\nmixtures. This unsupervised algorithm employs random subspace and subsampling\nensembles to ensure efficient computation and improve the robustness of the\noutlier detector. The ensemble approach further improves the suitability of the\nproposed method for detecting outliers in non-Gaussian data. Furthermore, our\nmethod uses variational inference for Dirichlet process mixtures, which ensures\nboth efficient and rapid computation. Empirical analyses using benchmark\ndatasets demonstrate that our method outperforms existing approaches in\nunsupervised outlier detection.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2401.00773v3",
    "published_date": "2024-01-01 14:34:11 UTC",
    "updated_date": "2024-07-25 08:13:27 UTC"
  },
  {
    "arxiv_id": "2401.00763v3",
    "title": "New Job, New Gender? Measuring the Social Bias in Image Generation Models",
    "authors": [
      "Wenxuan Wang",
      "Haonan Bai",
      "Jen-tse Huang",
      "Yuxuan Wan",
      "Youliang Yuan",
      "Haoyi Qiu",
      "Nanyun Peng",
      "Michael R. Lyu"
    ],
    "abstract": "Image generation models can generate or edit images from a given text. Recent\nadvancements in image generation technology, exemplified by DALL-E and\nMidjourney, have been groundbreaking. These advanced models, despite their\nimpressive capabilities, are often trained on massive Internet datasets, making\nthem susceptible to generating content that perpetuates social stereotypes and\nbiases, which can lead to severe consequences. Prior research on assessing bias\nwithin image generation models suffers from several shortcomings, including\nlimited accuracy, reliance on extensive human labor, and lack of comprehensive\nanalysis. In this paper, we propose BiasPainter, a novel evaluation framework\nthat can accurately, automatically and comprehensively trigger social bias in\nimage generation models. BiasPainter uses a diverse range of seed images of\nindividuals and prompts the image generation models to edit these images using\ngender, race, and age-neutral queries. These queries span 62 professions, 39\nactivities, 57 types of objects, and 70 personality traits. The framework then\ncompares the edited images to the original seed images, focusing on the\nsignificant changes related to gender, race, and age. BiasPainter adopts a key\ninsight that these characteristics should not be modified when subjected to\nneutral prompts. Built upon this design, BiasPainter can trigger the social\nbias and evaluate the fairness of image generation models. We use BiasPainter\nto evaluate six widely-used image generation models, such as stable diffusion\nand Midjourney. Experimental results show that BiasPainter can successfully\ntrigger social bias in image generation models. According to our human\nevaluation, BiasPainter can achieve 90.8% accuracy on automatic bias detection,\nwhich is significantly higher than the results reported in previous work.",
    "categories": [
      "cs.SE",
      "cs.AI",
      "cs.CL",
      "cs.CV",
      "cs.MM"
    ],
    "primary_category": "cs.SE",
    "comment": "ACM MM 2024 Oral",
    "pdf_url": "http://arxiv.org/pdf/2401.00763v3",
    "published_date": "2024-01-01 14:06:55 UTC",
    "updated_date": "2024-08-20 04:11:26 UTC"
  },
  {
    "arxiv_id": "2401.00761v1",
    "title": "The Earth is Flat? Unveiling Factual Errors in Large Language Models",
    "authors": [
      "Wenxuan Wang",
      "Juluan Shi",
      "Zhaopeng Tu",
      "Youliang Yuan",
      "Jen-tse Huang",
      "Wenxiang Jiao",
      "Michael R. Lyu"
    ],
    "abstract": "Large Language Models (LLMs) like ChatGPT are foundational in various\napplications due to their extensive knowledge from pre-training and\nfine-tuning. Despite this, they are prone to generating factual and commonsense\nerrors, raising concerns in critical areas like healthcare, journalism, and\neducation to mislead users. Current methods for evaluating LLMs' veracity are\nlimited by test data leakage or the need for extensive human labor, hindering\nefficient and accurate error detection. To tackle this problem, we introduce a\nnovel, automatic testing framework, FactChecker, aimed at uncovering factual\ninaccuracies in LLMs. This framework involves three main steps: First, it\nconstructs a factual knowledge graph by retrieving fact triplets from a\nlarge-scale knowledge database. Then, leveraging the knowledge graph,\nFactChecker employs a rule-based approach to generates three types of questions\n(Yes-No, Multiple-Choice, and WH questions) that involve single-hop and\nmulti-hop relations, along with correct answers. Lastly, it assesses the LLMs'\nresponses for accuracy using tailored matching strategies for each question\ntype. Our extensive tests on six prominent LLMs, including text-davinci-002,\ntext-davinci-003, ChatGPT~(gpt-3.5-turbo, gpt-4), Vicuna, and LLaMA-2, reveal\nthat FactChecker can trigger factual errors in up to 45\\% of questions in these\nmodels. Moreover, we demonstrate that FactChecker's test cases can improve\nLLMs' factual accuracy through in-context learning and fine-tuning (e.g.,\nllama-2-13b-chat's accuracy increase from 35.3\\% to 68.5\\%). We are making all\ncode, data, and results available for future research endeavors.",
    "categories": [
      "cs.SE",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.SE",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2401.00761v1",
    "published_date": "2024-01-01 14:02:27 UTC",
    "updated_date": "2024-01-01 14:02:27 UTC"
  },
  {
    "arxiv_id": "2401.00757v3",
    "title": "LogicAsker: Evaluating and Improving the Logical Reasoning Ability of Large Language Models",
    "authors": [
      "Yuxuan Wan",
      "Wenxuan Wang",
      "Yiliu Yang",
      "Youliang Yuan",
      "Jen-tse Huang",
      "Pinjia He",
      "Wenxiang Jiao",
      "Michael R. Lyu"
    ],
    "abstract": "We introduce LogicAsker, a novel approach for evaluating and enhancing the\nlogical reasoning capabilities of large language models (LLMs) such as ChatGPT\nand GPT-4. Despite LLMs' prowess in tasks like writing assistance, code\ngeneration, and machine translation, assessing their ability to reason has been\nchallenging. Traditional evaluations often prioritize accuracy on downstream\ntasks over direct assessments of reasoning processes. LogicAsker addresses this\ngap by employing a set of atomic reasoning skills grounded in propositional and\npredicate logic to systematically examine and improve the reasoning prowess of\nLLMs. Our methodology reveals significant gaps in LLMs' learning of logical\nrules, with identified reasoning failures ranging from 29\\% to 90\\% across\ndifferent models. Moreover, we leverage these findings to construct targeted\ndemonstration examples and fine-tune data, notably enhancing logical reasoning\nin models like GPT-4o by up to 5\\%. To our knowledge, this is the first effort\nto utilize test case outcomes to effectively refine LLMs' formal reasoning\ncapabilities. We make our code, data, and results publicly available\n(https://github.com/yxwan123/LogicAsker) to facilitate further research and\nreplication of our findings.",
    "categories": [
      "cs.SE",
      "cs.AI",
      "cs.CL",
      "cs.LO"
    ],
    "primary_category": "cs.SE",
    "comment": "Accepted by EMNLP 2024",
    "pdf_url": "http://arxiv.org/pdf/2401.00757v3",
    "published_date": "2024-01-01 13:53:53 UTC",
    "updated_date": "2024-10-08 14:34:37 UTC"
  },
  {
    "arxiv_id": "2401.01384v1",
    "title": "Strong Transitivity Relations and Graph Neural Networks",
    "authors": [
      "Yassin Mohamadi",
      "Mostafa Haghir Chehreghani"
    ],
    "abstract": "Local neighborhoods play a crucial role in embedding generation in\ngraph-based learning. It is commonly believed that nodes ought to have\nembeddings that resemble those of their neighbors. In this research, we try to\ncarefully expand the concept of similarity from nearby neighborhoods to the\nentire graph. We provide an extension of similarity that is based on\ntransitivity relations, which enables Graph Neural Networks (GNNs) to capture\nboth global similarities and local similarities over the whole graph. We\nintroduce Transitivity Graph Neural Network (TransGNN), which more than local\nnode similarities, takes into account global similarities by distinguishing\nstrong transitivity relations from weak ones and exploiting them. We evaluate\nour model over several real-world datasets and showed that it considerably\nimproves the performance of several well-known GNN models, for tasks such as\nnode classification.",
    "categories": [
      "cs.SI",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.SI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2401.01384v1",
    "published_date": "2024-01-01 13:53:50 UTC",
    "updated_date": "2024-01-01 13:53:50 UTC"
  },
  {
    "arxiv_id": "2401.00756v1",
    "title": "MPRE: Multi-perspective Patient Representation Extractor for Disease Prediction",
    "authors": [
      "Ziyue Yu",
      "Jiayi Wang",
      "Wuman Luo",
      "Rita Tse",
      "Giovanni Pau"
    ],
    "abstract": "Patient representation learning based on electronic health records (EHR) is a\ncritical task for disease prediction. This task aims to effectively extract\nuseful information on dynamic features. Although various existing works have\nachieved remarkable progress, the model performance can be further improved by\nfully extracting the trends, variations, and the correlation between the trends\nand variations in dynamic features. In addition, sparse visit records limit the\nperformance of deep learning models. To address these issues, we propose the\nMulti-perspective Patient Representation Extractor (MPRE) for disease\nprediction. Specifically, we propose Frequency Transformation Module (FTM) to\nextract the trend and variation information of dynamic features in the\ntime-frequency domain, which can enhance the feature representation. In the 2D\nMulti-Extraction Network (2D MEN), we form the 2D temporal tensor based on\ntrend and variation. Then, the correlations between trend and variation are\ncaptured by the proposed dilated operation. Moreover, we propose the\nFirst-Order Difference Attention Mechanism (FODAM) to calculate the\ncontributions of differences in adjacent variations to the disease diagnosis\nadaptively. To evaluate the performance of MPRE and baseline methods, we\nconduct extensive experiments on two real-world public datasets. The experiment\nresults show that MPRE outperforms state-of-the-art baseline methods in terms\nof AUROC and AUPRC.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted by ICDM 2023",
    "pdf_url": "http://arxiv.org/pdf/2401.00756v1",
    "published_date": "2024-01-01 13:52:05 UTC",
    "updated_date": "2024-01-01 13:52:05 UTC"
  },
  {
    "arxiv_id": "2401.00741v3",
    "title": "ToolEyes: Fine-Grained Evaluation for Tool Learning Capabilities of Large Language Models in Real-world Scenarios",
    "authors": [
      "Junjie Ye",
      "Guanyu Li",
      "Songyang Gao",
      "Caishuang Huang",
      "Yilong Wu",
      "Sixian Li",
      "Xiaoran Fan",
      "Shihan Dou",
      "Tao Ji",
      "Qi Zhang",
      "Tao Gui",
      "Xuanjing Huang"
    ],
    "abstract": "Existing evaluations of tool learning primarily focus on validating the\nalignment of selected tools for large language models (LLMs) with expected\noutcomes. However, these approaches rely on a limited set of scenarios where\nanswers can be pre-determined, diverging from genuine needs. Furthermore, a\nsole emphasis on outcomes disregards the complex capabilities required for LLMs\nto effectively use tools. To tackle this issue, we propose ToolEyes, a\nfine-grained system tailored for the evaluation of the LLMs' tool learning\ncapabilities in authentic scenarios. The system meticulously examines seven\nreal-world scenarios, analyzing five dimensions crucial to LLMs in tool\nlearning: format alignment, intent comprehension, behavior planning, tool\nselection, and answer organization. Additionally, ToolEyes incorporates a tool\nlibrary boasting approximately 600 tools, serving as an intermediary between\nLLMs and the physical world. Evaluations involving ten LLMs across three\ncategories reveal a preference for specific scenarios and limited cognitive\nabilities in tool learning. Intriguingly, expanding the model size even\nexacerbates the hindrance to tool learning. The code and data are available at\nhttps://github.com/Junjie-Ye/ToolEyes.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted by COLING 2025 conference",
    "pdf_url": "http://arxiv.org/pdf/2401.00741v3",
    "published_date": "2024-01-01 12:49:36 UTC",
    "updated_date": "2024-12-05 07:05:59 UTC"
  },
  {
    "arxiv_id": "2401.00739v1",
    "title": "DiffMorph: Text-less Image Morphing with Diffusion Models",
    "authors": [
      "Shounak Chatterjee"
    ],
    "abstract": "Text-conditioned image generation models are a prevalent use of AI image\nsynthesis, yet intuitively controlling output guided by an artist remains\nchallenging. Current methods require multiple images and textual prompts for\neach object to specify them as concepts to generate a single customized image.\n  On the other hand, our work, \\verb|DiffMorph|, introduces a novel approach\nthat synthesizes images that mix concepts without the use of textual prompts.\nOur work integrates a sketch-to-image module to incorporate user sketches as\ninput. \\verb|DiffMorph| takes an initial image with conditioning artist-drawn\nsketches to generate a morphed image.\n  We employ a pre-trained text-to-image diffusion model and fine-tune it to\nreconstruct each image faithfully. We seamlessly merge images and concepts from\nsketches into a cohesive composition. The image generation capability of our\nwork is demonstrated through our results and a comparison of these with\nprompt-based image generation.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2401.00739v1",
    "published_date": "2024-01-01 12:42:32 UTC",
    "updated_date": "2024-01-01 12:42:32 UTC"
  },
  {
    "arxiv_id": "2401.00737v1",
    "title": "Searching, fast and slow, through product catalogs",
    "authors": [
      "Dayananda Ubrangala",
      "Juhi Sharma",
      "Sharath Kumar Rangappa",
      "Kiran R",
      "Ravi Prasad Kondapalli",
      "Laurent Boué"
    ],
    "abstract": "String matching algorithms in the presence of abbreviations, such as in Stock\nKeeping Unit (SKU) product catalogs, remains a relatively unexplored topic. In\nthis paper, we present a unified architecture for SKU search that provides both\na real-time suggestion system (based on a Trie data structure) as well as a\nlower latency search system (making use of character level TF-IDF in\ncombination with language model vector embeddings) where users initiate the\nsearch process explicitly. We carry out ablation studies that justify designing\na complex search system composed of multiple components to address the delicate\ntrade-off between speed and accuracy. Using SKU search in the Dynamics CRM as\nan example, we show how our system vastly outperforms, in all aspects, the\nresults provided by the default search engine. Finally, we show how SKU\ndescriptions may be enhanced via generative text models (using gpt-3.5-turbo)\nso that the consumers of the search results may get more context and a\ngenerally better experience when presented with the results of their SKU\nsearch.",
    "categories": [
      "cs.IR",
      "cs.AI",
      "cs.LG",
      "cs.SE"
    ],
    "primary_category": "cs.IR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2401.00737v1",
    "published_date": "2024-01-01 12:30:46 UTC",
    "updated_date": "2024-01-01 12:30:46 UTC"
  },
  {
    "arxiv_id": "2401.00736v3",
    "title": "Diffusion Models, Image Super-Resolution And Everything: A Survey",
    "authors": [
      "Brian B. Moser",
      "Arundhati S. Shanbhag",
      "Federico Raue",
      "Stanislav Frolov",
      "Sebastian Palacio",
      "Andreas Dengel"
    ],
    "abstract": "Diffusion Models (DMs) have disrupted the image Super-Resolution (SR) field\nand further closed the gap between image quality and human perceptual\npreferences. They are easy to train and can produce very high-quality samples\nthat exceed the realism of those produced by previous generative methods.\nDespite their promising results, they also come with new challenges that need\nfurther research: high computational demands, comparability, lack of\nexplainability, color shifts, and more. Unfortunately, entry into this field is\noverwhelming because of the abundance of publications. To address this, we\nprovide a unified recount of the theoretical foundations underlying DMs applied\nto image SR and offer a detailed analysis that underscores the unique\ncharacteristics and methodologies within this domain, distinct from broader\nexisting reviews in the field. This survey articulates a cohesive understanding\nof DM principles and explores current research avenues, including alternative\ninput domains, conditioning techniques, guidance mechanisms, corruption spaces,\nand zero-shot learning approaches. By offering a detailed examination of the\nevolution and current trends in image SR through the lens of DMs, this survey\nsheds light on the existing challenges and charts potential future directions,\naiming to inspire further innovation in this rapidly advancing area.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG",
      "cs.MM"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2401.00736v3",
    "published_date": "2024-01-01 12:25:57 UTC",
    "updated_date": "2024-06-23 19:32:56 UTC"
  },
  {
    "arxiv_id": "2401.00719v1",
    "title": "Depth Map Denoising Network and Lightweight Fusion Network for Enhanced 3D Face Recognition",
    "authors": [
      "Ruizhuo Xu",
      "Ke Wang",
      "Chao Deng",
      "Mei Wang",
      "Xi Chen",
      "Wenhui Huang",
      "Junlan Feng",
      "Weihong Deng"
    ],
    "abstract": "With the increasing availability of consumer depth sensors, 3D face\nrecognition (FR) has attracted more and more attention. However, the data\nacquired by these sensors are often coarse and noisy, making them impractical\nto use directly. In this paper, we introduce an innovative Depth map denoising\nnetwork (DMDNet) based on the Denoising Implicit Image Function (DIIF) to\nreduce noise and enhance the quality of facial depth images for low-quality 3D\nFR. After generating clean depth faces using DMDNet, we further design a\npowerful recognition network called Lightweight Depth and Normal Fusion network\n(LDNFNet), which incorporates a multi-branch fusion block to learn unique and\ncomplementary features between different modalities such as depth and normal\nimages. Comprehensive experiments conducted on four distinct low-quality\ndatabases demonstrate the effectiveness and robustness of our proposed methods.\nFurthermore, when combining DMDNet and LDNFNet, we achieve state-of-the-art\nresults on the Lock3DFace database.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "Accepted by Pattern Recognition",
    "pdf_url": "http://arxiv.org/pdf/2401.00719v1",
    "published_date": "2024-01-01 10:46:42 UTC",
    "updated_date": "2024-01-01 10:46:42 UTC"
  },
  {
    "arxiv_id": "2401.01383v2",
    "title": "Predicting Infant Brain Connectivity with Federated Multi-Trajectory GNNs using Scarce Data",
    "authors": [
      "Michalis Pistos",
      "Gang Li",
      "Weili Lin",
      "Dinggang Shen",
      "Islem Rekik"
    ],
    "abstract": "The understanding of the convoluted evolution of infant brain networks during\nthe first postnatal year is pivotal for identifying the dynamics of early brain\nconnectivity development. Existing deep learning solutions suffer from three\nmajor limitations. First, they cannot generalize to multi-trajectory prediction\ntasks, where each graph trajectory corresponds to a particular imaging modality\nor connectivity type (e.g., T1-w MRI). Second, existing models require\nextensive training datasets to achieve satisfactory performance which are often\nchallenging to obtain. Third, they do not efficiently utilize incomplete time\nseries data. To address these limitations, we introduce FedGmTE-Net++, a\nfederated graph-based multi-trajectory evolution network. Using the power of\nfederation, we aggregate local learnings among diverse hospitals with limited\ndatasets. As a result, we enhance the performance of each hospital's local\ngenerative model, while preserving data privacy. The three key innovations of\nFedGmTE-Net++ are: (i) presenting the first federated learning framework\nspecifically designed for brain multi-trajectory evolution prediction in a\ndata-scarce environment, (ii) incorporating an auxiliary regularizer in the\nlocal objective function to exploit all the longitudinal brain connectivity\nwithin the evolution trajectory and maximize data utilization, (iii)\nintroducing a two-step imputation process, comprising a preliminary KNN-based\nprecompletion followed by an imputation refinement step that employs regressors\nto improve similarity scores and refine imputations. Our comprehensive\nexperimental results showed the outperformance of FedGmTE-Net++ in brain\nmulti-trajectory prediction from a single baseline graph in comparison with\nbenchmark methods.",
    "categories": [
      "q-bio.NC",
      "cs.AI",
      "cs.CV",
      "cs.LG"
    ],
    "primary_category": "q-bio.NC",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2401.01383v2",
    "published_date": "2024-01-01 10:20:01 UTC",
    "updated_date": "2024-01-08 09:46:38 UTC"
  },
  {
    "arxiv_id": "2401.00711v1",
    "title": "Text2Avatar: Text to 3D Human Avatar Generation with Codebook-Driven Body Controllable Attribute",
    "authors": [
      "Chaoqun Gong",
      "Yuqin Dai",
      "Ronghui Li",
      "Achun Bao",
      "Jun Li",
      "Jian Yang",
      "Yachao Zhang",
      "Xiu Li"
    ],
    "abstract": "Generating 3D human models directly from text helps reduce the cost and time\nof character modeling. However, achieving multi-attribute controllable and\nrealistic 3D human avatar generation is still challenging due to feature\ncoupling and the scarcity of realistic 3D human avatar datasets. To address\nthese issues, we propose Text2Avatar, which can generate realistic-style 3D\navatars based on the coupled text prompts. Text2Avatar leverages a discrete\ncodebook as an intermediate feature to establish a connection between text and\navatars, enabling the disentanglement of features. Furthermore, to alleviate\nthe scarcity of realistic style 3D human avatar data, we utilize a pre-trained\nunconditional 3D human avatar generation model to obtain a large amount of 3D\navatar pseudo data, which allows Text2Avatar to achieve realistic style\ngeneration. Experimental results demonstrate that our method can generate\nrealistic 3D avatars from coupled textual data, which is challenging for other\nexisting methods in this field.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2401.00711v1",
    "published_date": "2024-01-01 09:39:57 UTC",
    "updated_date": "2024-01-01 09:39:57 UTC"
  },
  {
    "arxiv_id": "2401.00700v1",
    "title": "An attempt to generate new bridge types from latent space of generative adversarial network",
    "authors": [
      "Hongjun Zhang"
    ],
    "abstract": "Try to generate new bridge types using generative artificial intelligence\ntechnology. Symmetric structured image dataset of three-span beam bridge, arch\nbridge, cable-stayed bridge and suspension bridge are used . Based on Python\nprogramming language, TensorFlow and Keras deep learning platform framework ,\nas well as Wasserstein loss function and Lipschitz constraints, generative\nadversarial network is constructed and trained. From the obtained low\ndimensional bridge-type latent space sampling, new bridge types with asymmetric\nstructures can be generated. Generative adversarial network can create new\nbridge types by organically combining different structural components on the\nbasis of human original bridge types. It has a certain degree of human original\nability. Generative artificial intelligence technology can open up imagination\nspace and inspire humanity.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.LG",
    "comment": "8 pages, 4 figures",
    "pdf_url": "http://arxiv.org/pdf/2401.00700v1",
    "published_date": "2024-01-01 08:46:29 UTC",
    "updated_date": "2024-01-01 08:46:29 UTC"
  },
  {
    "arxiv_id": "2401.00698v1",
    "title": "Large Language Models aren't all that you need",
    "authors": [
      "Kiran Voderhobli Holla",
      "Chaithanya Kumar",
      "Aryan Singh"
    ],
    "abstract": "This paper describes the architecture and systems built towards solving the\nSemEval 2023 Task 2: MultiCoNER II (Multilingual Complex Named Entity\nRecognition) [1]. We evaluate two approaches (a) a traditional Conditional\nRandom Fields model and (b) a Large Language Model (LLM) fine-tuned with a\ncustomized head and compare the two approaches. The novel ideas explored are:\n1) Decaying auxiliary loss (with residual) - where we train the model on an\nauxiliary task of Coarse-Grained NER and include this task as a part of the\nloss function 2) Triplet token blending - where we explore ways of blending the\nembeddings of neighboring tokens in the final NER layer prior to prediction 3)\nTask-optimal heads - where we explore a variety of custom heads and learning\nrates for the final layer of the LLM. We also explore multiple LLMs including\nGPT-3 and experiment with a variety of dropout and other hyperparameter\nsettings before arriving at our final model which achieves micro & macro f1 of\n0.85/0.84 (on dev) and 0.67/0.61 on the test data . We show that while\npre-trained LLMs, by themselves, bring about a large improvement in scores as\ncompared to traditional models, we also demonstrate that tangible improvements\nto the Macro-F1 score can be made by augmenting the LLM with additional\nfeature/loss/model engineering techniques described above.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2401.00698v1",
    "published_date": "2024-01-01 08:32:50 UTC",
    "updated_date": "2024-01-01 08:32:50 UTC"
  },
  {
    "arxiv_id": "2401.00689v1",
    "title": "Large language model for Bible sentiment analysis: Sermon on the Mount",
    "authors": [
      "Mahek Vora",
      "Tom Blau",
      "Vansh Kachhwal",
      "Ashu M. G. Solo",
      "Rohitash Chandra"
    ],
    "abstract": "The revolution of natural language processing via large language models has\nmotivated its use in multidisciplinary areas that include social sciences and\nhumanities and more specifically, comparative religion. Sentiment analysis\nprovides a mechanism to study the emotions expressed in text. Recently,\nsentiment analysis has been used to study and compare translations of the\nBhagavad Gita, which is a fundamental and sacred Hindu text. In this study, we\nuse sentiment analysis for studying selected chapters of the Bible. These\nchapters are known as the Sermon on the Mount. We utilize a pre-trained\nlanguage model for sentiment analysis by reviewing five translations of the\nSermon on the Mount, which include the King James version, the New\nInternational Version, the New Revised Standard Version, the Lamsa Version, and\nthe Basic English Version. We provide a chapter-by-chapter and verse-by-verse\ncomparison using sentiment and semantic analysis and review the major\nsentiments expressed. Our results highlight the varying sentiments across the\nchapters and verses. We found that the vocabulary of the respective\ntranslations is significantly different. We detected different levels of\nhumour, optimism, and empathy in the respective chapters that were used by\nJesus to deliver his message.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2401.00689v1",
    "published_date": "2024-01-01 07:35:29 UTC",
    "updated_date": "2024-01-01 07:35:29 UTC"
  },
  {
    "arxiv_id": "2401.00685v2",
    "title": "Communication-Efficient Federated Learning for LEO Satellite Networks Integrated with HAPs Using Hybrid NOMA-OFDM",
    "authors": [
      "Mohamed Elmahallawy",
      "Tie Luo",
      "Khaled Ramadan"
    ],
    "abstract": "Space AI has become increasingly important and sometimes even necessary for\ngovernment, businesses, and society. An active research topic under this\nmission is integrating federated learning (FL) with satellite communications\n(SatCom) so that numerous low Earth orbit (LEO) satellites can collaboratively\ntrain a machine learning model. However, the special communication environment\nof SatCom leads to a very slow FL training process up to days and weeks. This\npaper proposes NomaFedHAP, a novel FL-SatCom approach tailored to LEO\nsatellites, that (1) utilizes high-altitude platforms (HAPs) as distributed\nparameter servers (PS) to enhance satellite visibility, and (2) introduces\nnon-orthogonal multiple access (NOMA) into LEO to enable fast and\nbandwidth-efficient model transmissions. In addition, NomaFedHAP includes (3) a\nnew communication topology that exploits HAPs to bridge satellites among\ndifferent orbits to mitigate the Doppler shift, and (4) a new FL model\naggregation scheme that optimally balances models between different orbits and\nshells. Moreover, we (5) derive a closed-form expression of the outage\nprobability for satellites in near and far shells, as well as for the entire\nsystem. Our extensive simulations have validated the mathematical analysis and\ndemonstrated the superior performance of NomaFedHAP in achieving fast and\nefficient FL model convergence with high accuracy as compared to the\nstate-of-the-art.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.DC"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2401.00685v2",
    "published_date": "2024-01-01 07:07:27 UTC",
    "updated_date": "2024-02-16 09:21:29 UTC"
  },
  {
    "arxiv_id": "2401.00916v1",
    "title": "Data Assimilation in Chaotic Systems Using Deep Reinforcement Learning",
    "authors": [
      "Mohamad Abed El Rahman Hammoud",
      "Naila Raboudi",
      "Edriss S. Titi",
      "Omar Knio",
      "Ibrahim Hoteit"
    ],
    "abstract": "Data assimilation (DA) plays a pivotal role in diverse applications, ranging\nfrom climate predictions and weather forecasts to trajectory planning for\nautonomous vehicles. A prime example is the widely used ensemble Kalman filter\n(EnKF), which relies on linear updates to minimize variance among the ensemble\nof forecast states. Recent advancements have seen the emergence of deep\nlearning approaches in this domain, primarily within a supervised learning\nframework. However, the adaptability of such models to untrained scenarios\nremains a challenge. In this study, we introduce a novel DA strategy that\nutilizes reinforcement learning (RL) to apply state corrections using full or\npartial observations of the state variables. Our investigation focuses on\ndemonstrating this approach to the chaotic Lorenz '63 system, where the agent's\nobjective is to minimize the root-mean-squared error between the observations\nand corresponding forecast states. Consequently, the agent develops a\ncorrection strategy, enhancing model forecasts based on available system state\nobservations. Our strategy employs a stochastic action policy, enabling a Monte\nCarlo-based DA framework that relies on randomly sampling the policy to\ngenerate an ensemble of assimilated realizations. Results demonstrate that the\ndeveloped RL algorithm performs favorably when compared to the EnKF.\nAdditionally, we illustrate the agent's capability to assimilate non-Gaussian\ndata, addressing a significant limitation of the EnKF.",
    "categories": [
      "math.DS",
      "cs.AI",
      "cs.LG",
      "physics.ao-ph"
    ],
    "primary_category": "math.DS",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2401.00916v1",
    "published_date": "2024-01-01 06:53:36 UTC",
    "updated_date": "2024-01-01 06:53:36 UTC"
  },
  {
    "arxiv_id": "2401.02981v2",
    "title": "Fine-tuning and Utilization Methods of Domain-specific LLMs",
    "authors": [
      "Cheonsu Jeong"
    ],
    "abstract": "Recent releases of pre-trained Large Language Models (LLMs) have gained\nconsiderable traction, yet research on fine-tuning and employing\ndomain-specific LLMs remains scarce. This study investigates approaches for\nfine-tuning and leveraging domain-specific LLMs, highlighting trends in LLMs,\nfoundational models, and methods for domain-specific pre-training. Focusing on\nthe financial sector, it details dataset selection, preprocessing, model\nchoice, and considerations crucial for LLM fine-tuning in finance. Addressing\nthe unique characteristics of financial data, the study explores the\nconstruction of domain-specific vocabularies and considerations for security\nand regulatory compliance. In the practical application of LLM fine-tuning, the\nstudy outlines the procedure and implementation for generating domain-specific\nLLMs in finance. Various financial cases, including stock price prediction,\nsentiment analysis of financial news, automated document processing, research,\ninformation extraction, and customer service enhancement, are exemplified. The\nstudy explores the potential of LLMs in the financial domain, identifies\nlimitations, and proposes directions for improvement, contributing valuable\ninsights for future research. Ultimately, it advances natural language\nprocessing technology in business, suggesting proactive LLM utilization in\nfinancial services across industries.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2401.02981v2",
    "published_date": "2024-01-01 06:22:04 UTC",
    "updated_date": "2024-01-24 18:16:34 UTC"
  },
  {
    "arxiv_id": "2401.00663v1",
    "title": "1st Place Solution for 5th LSVOS Challenge: Referring Video Object Segmentation",
    "authors": [
      "Zhuoyan Luo",
      "Yicheng Xiao",
      "Yong Liu",
      "Yitong Wang",
      "Yansong Tang",
      "Xiu Li",
      "Yujiu Yang"
    ],
    "abstract": "The recent transformer-based models have dominated the Referring Video Object\nSegmentation (RVOS) task due to the superior performance. Most prior works\nadopt unified DETR framework to generate segmentation masks in\nquery-to-instance manner. In this work, we integrate strengths of that leading\nRVOS models to build up an effective paradigm. We first obtain binary mask\nsequences from the RVOS models. To improve the consistency and quality of\nmasks, we propose Two-Stage Multi-Model Fusion strategy. Each stage rationally\nensembles RVOS models based on framework design as well as training strategy,\nand leverages different video object segmentation (VOS) models to enhance mask\ncoherence by object propagation mechanism. Our method achieves 75.7% J&F on\nRef-Youtube-VOS validation set and 70% J&F on test set, which ranks 1st place\non 5th Large-scale Video Object Segmentation Challenge (ICCV 2023) track 3.\nCode is available at https://github.com/RobertLuo1/iccv2023_RVOS_Challenge.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2401.00663v1",
    "published_date": "2024-01-01 04:24:48 UTC",
    "updated_date": "2024-01-01 04:24:48 UTC"
  },
  {
    "arxiv_id": "2401.10262v1",
    "title": "Null Space Properties of Neural Networks with Applications to Image Steganography",
    "authors": [
      "Xiang Li",
      "Kevin M. Short"
    ],
    "abstract": "This paper explores the null space properties of neural networks. We extend\nthe null space definition from linear to nonlinear maps and discuss the\npresence of a null space in neural networks. The null space of a given neural\nnetwork can tell us the part of the input data that makes no contribution to\nthe final prediction so that we can use it to trick the neural network. This\nreveals an inherent weakness in neural networks that can be exploited. One\napplication described here leads to a method of image steganography. Through\nexperiments on image datasets such as MNIST, we show that we can use null space\ncomponents to force the neural network to choose a selected hidden image class,\neven though the overall image can be made to look like a completely different\nimage. We conclude by showing comparisons between what a human viewer would\nsee, and the part of the image that the neural network is actually using to\nmake predictions and, hence, show that what the neural network ``sees'' is\ncompletely different than what we would expect.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CR",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2401.10262v1",
    "published_date": "2024-01-01 03:32:28 UTC",
    "updated_date": "2024-01-01 03:32:28 UTC"
  },
  {
    "arxiv_id": "2401.00631v2",
    "title": "Edge AI as a Service with Coordinated Deep Neural Networks",
    "authors": [
      "Alireza Maleki",
      "Hamed Shah-Mansouri",
      "Babak H. Khalaj"
    ],
    "abstract": "As artificial intelligence (AI) applications continue to expand in\nnext-generation networks, there is a growing need for deep neural network (DNN)\nmodels. Although DNN models deployed at the edge are promising for providing AI\nas a service with low latency, their cooperation is yet to be explored. In this\npaper, we consider that DNN service providers share their computing resources\nas well as their models' parameters and allow other DNNs to offload their\ncomputations without mirroring. We propose a novel algorithm called coordinated\nDNNs on edge (\\textbf{CoDE}) that facilitates coordination among DNN services\nby establishing new inference paths. CoDE aims to find the optimal path, which\nis the path with the highest possible reward, by creating multi-task DNNs from\nindividual models. The reward reflects the inference throughput and model\naccuracy. With CoDE, DNN models can make new paths for inference by using their\nown or other models' parameters. We then evaluate the performance of CoDE\nthrough numerical experiments. The results demonstrate a $40\\%$ increase in the\ninference throughput while degrading the average accuracy by only $2.3\\%$.\nExperiments show that CoDE enhances the inference throughput and, achieves\nhigher precision compared to a state-of-the-art existing method.",
    "categories": [
      "cs.NI",
      "cs.AI",
      "eess.SP"
    ],
    "primary_category": "cs.NI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2401.00631v2",
    "published_date": "2024-01-01 01:54:53 UTC",
    "updated_date": "2024-08-21 17:47:53 UTC"
  },
  {
    "arxiv_id": "2401.04122v3",
    "title": "From Prompt Engineering to Prompt Science With Human in the Loop",
    "authors": [
      "Chirag Shah"
    ],
    "abstract": "As LLMs make their way into many aspects of our lives, one place that\nwarrants increased scrutiny with LLM usage is scientific research. Using LLMs\nfor generating or analyzing data for research purposes is gaining popularity.\nBut when such application is marred with ad-hoc decisions and engineering\nsolutions, we need to be concerned about how it may affect that research, its\nfindings, or any future works based on that research. We need a more scientific\napproach to using LLMs in our research. While there are several active efforts\nto support more systematic construction of prompts, they are often focused more\non achieving desirable outcomes rather than producing replicable and\ngeneralizable knowledge with sufficient transparency, objectivity, or rigor.\nThis article presents a new methodology inspired by codebook construction\nthrough qualitative methods to address that. Using humans in the loop and a\nmulti-phase verification processes, this methodology lays a foundation for more\nsystematic, objective, and trustworthy way of applying LLMs for analyzing data.\nSpecifically, we show how a set of researchers can work through a rigorous\nprocess of labeling, deliberating, and documenting to remove subjectivity and\nbring transparency and replicability to prompt generation process. A set of\nexperiments are presented to show how this methodology can be put in practice.",
    "categories": [
      "cs.HC",
      "cs.AI"
    ],
    "primary_category": "cs.HC",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2401.04122v3",
    "published_date": "2024-01-01 01:37:36 UTC",
    "updated_date": "2024-05-10 03:50:26 UTC"
  }
]
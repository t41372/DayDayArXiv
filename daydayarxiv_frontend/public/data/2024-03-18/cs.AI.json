{
  "date": "2024-03-18",
  "category": "cs.AI",
  "summary": "欢迎来到 UTC 时间 2024-03-18 的 arXiv 中文 TLDR 快报！\n\n今天 arXiv 的论文主要聚焦于 AI 领域的创新应用，包括大型语言模型 (LLM) 在视觉理解、决策和安全中的扩展、强化学习的环境适应性，以及图像生成和医疗诊断的进展。其中，LLM 增强的视觉和生成模型（如 Scene-LLM 和 LLaVA-UHD）令人印象深刻，它们展示了高效的多模态处理能力；此外，涉及 AI 安全和医疗应用的论文（如检测恶意包和药物滥用诊断）具有实际影响力。\n\n下面，我将挑选并简要讨论部分关键论文，先优先聊那些创新性强、可能有话题度的文章（如 LLM 和强化学习相关），然后快速掠过其他较基础或小众的论文。每篇论文会列出标题（中文 + 英文），并保留核心学术术语，聚焦主要贡献和发现。\n\n### 重点论文讨论\n\n**1. Scene-LLM: Extending Language Model for 3D Visual Understanding and Reasoning（Scene-LLM: 扩展语言模型用于 3D 视觉理解和推理）**  \n   这篇论文提出 Scene-LLM，一种集成 LLM 的 3D 视觉模型，用于增强代理在交互式环境的理解。主要贡献是通过混合 3D 视觉特征表示（包括场景级和自我中心视角），实现了高效的 3D 物体检测和交互规划。发现显示，该模型在密集描述、问答和规划任务上表现出色，突显 LLM 在多模态任务中的潜力。\n\n**2. LLaVA-UHD: an LMM Perceiving Any Aspect Ratio and High-Resolution Images（LLaVA-UHD: 一种能感知任意宽高比和高分辨率图像的大型多模态模型）**  \n   作者提出 LLaVA-UHD，一种高效的多模态模型，支持任意分辨率图像处理。主要贡献是引入图像模块化和压缩机制，实现快速渲染和一致性优化。实验结果显示，该模型在图像生成和理解任务上超越了基线，提升了 LLM 在视觉任务的泛化能力，特别适合资源受限的场景。\n\n**3. Reinforcement Learning from Delayed Observations via World Models（基于世界模型的延迟观测强化学习）**  \n   这篇论文解决强化学习中观测延迟问题，通过世界模型（world models）将部分可观测马尔可夫决策过程（POMDP）转化为延迟马尔可夫决策过程（MDP）。主要贡献是提出一种方法，显著提升了算法在视觉延迟环境下的性能，实验显示性能提升高达 250%，为真实世界应用（如机器人控制）提供了实用框架。\n\n**4. Leveraging Large Language Models to Extract Information on Substance Use Disorder Severity（利用大型语言模型提取药物滥用严重程度信息）**  \n   作者开发了一个基于 LLM 的零-shot 学习工作流，用于从临床笔记中提取药物滥用相关信息。主要贡献是使用 Flan-T5 模型和精心设计的提示，实现了高召回率的提取，相比规则方法提升了 9% 的 F1 分数。该工作强调 LLM 在医疗 NLP 中的潜力，帮助改善风险评估和治疗规划。\n\n**5. Leveraging Large Language Models to Detect npm Malicious Packages（利用大型语言模型检测 npm 恶意包）**  \n   这篇论文提出 SocketAI 系统，使用 LLM（如 GPT-4）检测 JavaScript 恶意代码。主要贡献是通过静态分析和 LLM 结合，提升了检测精度（F1 分数达 97%），并减少了分析成本。该发现突显 LLM 在软件安全中的应用价值，相关实验数据支持了其鲁棒性。\n\n**6. Synthetic Image Generation in Cyber Influence Operations（合成图像生成在网络影响操作中的潜在威胁）**  \n   作者探讨扩散模型在生成合成图像时的应用及其在网络影响操作中的风险。主要贡献是评估了这些模型的实际局限性（如生成逼真图像的挑战），并提出防御建议。该论文揭示了 AI 生成内容的潜在滥用风险，具有现实社会影响。\n\n**7. Compositional learning of functions in humans and machines（人类和机器的功能组合学习）**  \n   作者（包括 Brenden M. Lake）比较了人类和机器在功能组合学习中的能力。主要贡献是通过视觉范式实验，证明 Transformer 模型能模拟人类在复杂组合任务中的泛化模式。该工作深化了 AI 与人类认知的比较，Brenden M. Lake 的参与增加了其学术影响力。\n\n**8. E2F-Net: Eyes-to-Face Inpainting via StyleGAN Latent Space（E2F-Net: 通过 StyleGAN 潜在空间的眼睛到面部修复）**  \n   这篇论文提出 E2F-Net，用于从眼部图像重建面部。主要贡献是结合编码器和优化技术，实现高保真修复，实验显示其在公开数据集上超越了基线。该方法为图像生成领域提供了新工具，强调潜在空间的表达能力。\n\n其他论文中，涉及图神经网络（如 Molecular Classification Using Hyperdimensional Graph Classification）和文本生成（如 Reference-based Metrics Disprove Themselves in Question Generation）的文章虽有创新，但相对小众或技术细节较深，因此快速掠过：  \n- **Molecular Classification Using Hyperdimensional Graph Classification（分子分类使用超维图分类）**：提出 HDC 模型提升图学习效率，贡献在于训练和推理速度快（40x 加速），适用于化学信息学。  \n- **Reference-based Metrics Disprove Themselves in Question Generation（基于参考的指标在问题生成中失效）**：发现传统指标（如 BLEU）在问答生成中失效，贡献是引入无参考指标提升人类判断一致性。  \n- **Efficient Transformer-based Hyper-parameter Optimization（高效的 Transformer 基于超参数优化）**：针对 IoT 环境优化 HPO，贡献在于结合强化学习加速模型训练，但实用性较窄。  \n- **Span-Oriented Information Extraction（跨度导向的信息提取）**：统一 NLP 任务视角，贡献在于将多种提取任务视为跨度问题，但缺乏新实验验证。  \n\n剩余论文（如安全分析、图像增强等）多为技术性补充或特定领域应用，篇幅有限，这里仅列出标题而不展开讨论，以保持简洁。例如：  \n- **Safety Implications of Explainable Artificial Intelligence（可解释 AI 的安全影响）**  \n- **TnT-LLM: Text Mining at Scale（大规模文本挖掘的 TnT-LLM）**  \n- **Graph-Jigsaw Conditioned Diffusion Model（图拼图条件扩散模型）**  \n\n总之，今天的论文突显了 AI 在多模态和实际应用中的进展，LLM 的扩展尤其值得关注。未来几天，arXiv 可能继续聚焦这些主题，读者可关注相关领域最新动态！",
  "papers": [
    {
      "arxiv_id": "2403.12320v1",
      "title": "Approximated Likelihood Ratio: A Forward-Only and Parallel Framework for Boosting Neural Network Training",
      "title_zh": "翻译失败",
      "authors": [
        "Zeliang Zhang",
        "Jinyang Jiang",
        "Zhuo Liu",
        "Susan Liang",
        "Yijie Peng",
        "Chenliang Xu"
      ],
      "abstract": "Efficient and biologically plausible alternatives to backpropagation in\nneural network training remain a challenge due to issues such as high\ncomputational complexity and additional assumptions about neural networks,\nwhich limit scalability to deeper networks. The likelihood ratio method offers\na promising gradient estimation strategy but is constrained by significant\nmemory consumption, especially when deploying multiple copies of data to reduce\nestimation variance. In this paper, we introduce an approximation technique for\nthe likelihood ratio (LR) method to alleviate computational and memory demands\nin gradient estimation. By exploiting the natural parallelism during the\nbackward pass using LR, we further provide a high-performance training\nstrategy, which pipelines both the forward and backward pass, to make it more\nsuitable for the computation on specialized hardware. Extensive experiments\ndemonstrate the effectiveness of the approximation technique in neural network\ntraining. This work underscores the potential of the likelihood ratio method in\nachieving high-performance neural network training, suggesting avenues for\nfurther exploration.",
      "tldr_zh": "这篇论文针对神经网络训练中 backpropagation 的高计算复杂性和生物可行性问题，提出了一种对 likelihood ratio (LR) 方法的近似技术，以降低梯度估计的计算和内存需求。作者利用 LR 在 backward pass 中的自然并行性，设计了一个前向-only 和并行框架，将 forward pass 和 backward pass 流水线化，使其更适合专用硬件的计算。实验结果显示，该近似技术在神经网络训练中表现出色，证明了 LR 方法在实现高性能训练方面的潜力，并为进一步探索提供了方向。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2403.12320v1",
      "published_date": "2024-03-18 23:23:50 UTC",
      "updated_date": "2024-03-18 23:23:50 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T16:28:30.339877"
    },
    {
      "arxiv_id": "2403.12309v2",
      "title": "Reinforcement Learning from Delayed Observations via World Models",
      "title_zh": "翻译失败",
      "authors": [
        "Armin Karamzade",
        "Kyungmin Kim",
        "Montek Kalsi",
        "Roy Fox"
      ],
      "abstract": "In standard reinforcement learning settings, agents typically assume\nimmediate feedback about the effects of their actions after taking them.\nHowever, in practice, this assumption may not hold true due to physical\nconstraints and can significantly impact the performance of learning\nalgorithms. In this paper, we address observation delays in partially\nobservable environments. We propose leveraging world models, which have shown\nsuccess in integrating past observations and learning dynamics, to handle\nobservation delays. By reducing delayed POMDPs to delayed MDPs with world\nmodels, our methods can effectively handle partial observability, where\nexisting approaches achieve sub-optimal performance or degrade quickly as\nobservability decreases. Experiments suggest that one of our methods can\noutperform a naive model-based approach by up to 250%. Moreover, we evaluate\nour methods on visual delayed environments, for the first time showcasing\ndelay-aware reinforcement learning continuous control with visual observations.",
      "tldr_zh": "这篇论文解决了强化学习(Reinforcement Learning)中观察延迟的问题，提出使用世界模型(World Models)来整合过去观察并学习动态，从而处理部分可观察环境(Partially Observable Environments)。他们将延迟的部分可观察马尔可夫决策过程(delayed POMDPs)简化为延迟的马尔可夫决策过程(delayed MDPs)，使算法在延迟场景下更有效。实验结果显示，该方法比朴素的模型-based方法性能提升高达250%，并首次在视觉延迟环境中实现了连续控制的延迟感知强化学习。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2403.12309v2",
      "published_date": "2024-03-18 23:18:27 UTC",
      "updated_date": "2024-06-26 02:44:18 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T16:28:42.381582"
    },
    {
      "arxiv_id": "2403.12308v1",
      "title": "Gradient-based Fuzzy System Optimisation via Automatic Differentiation -- FuzzyR as a Use Case",
      "title_zh": "翻译失败",
      "authors": [
        "Chao Chen",
        "Christian Wagner",
        "Jonathan M. Garibaldi"
      ],
      "abstract": "Since their introduction, fuzzy sets and systems have become an important\narea of research known for its versatility in modelling, knowledge\nrepresentation and reasoning, and increasingly its potential within the context\nexplainable AI. While the applications of fuzzy systems are diverse, there has\nbeen comparatively little advancement in their design from a machine learning\nperspective. In other words, while representations such as neural networks have\nbenefited from a boom in learning capability driven by an increase in\ncomputational performance in combination with advances in their training\nmechanisms and available tool, in particular gradient descent, the impact on\nfuzzy system design has been limited. In this paper, we discuss\ngradient-descent-based optimisation of fuzzy systems, focussing in particular\non automatic differentiation -- crucial to neural network learning -- with a\nview to free fuzzy system designers from intricate derivative computations,\nallowing for more focus on the functional and explainability aspects of their\ndesign. As a starting point, we present a use case in FuzzyR which demonstrates\nhow current fuzzy inference system implementations can be adjusted to leverage\npowerful features of automatic differentiation tools sets, discussing its\npotential for the future of fuzzy system design.",
      "tldr_zh": "该研究探讨了模糊系统(Fuzzy systems)的优化问题，指出其在机器学习领域的进展落后于神经网络，特别是梯度下降(Gradient descent)技术的应用。论文提出通过自动微分(Automatic Differentiation)实现模糊系统的梯度优化，简化衍生计算过程，让设计师更专注于功能和可解释性(Explainable AI)方面。作者以 FuzzyR 为用例，展示了如何调整现有模糊推理系统来利用自动微分工具，从而为模糊系统设计开辟新的潜力。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2403.12308v1",
      "published_date": "2024-03-18 23:18:16 UTC",
      "updated_date": "2024-03-18 23:18:16 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T16:28:55.142575"
    },
    {
      "arxiv_id": "2403.12307v1",
      "title": "Molecular Classification Using Hyperdimensional Graph Classification",
      "title_zh": "翻译失败",
      "authors": [
        "Pere Verges",
        "Igor Nunes",
        "Mike Heddes",
        "Tony Givargis",
        "Alexandru Nicolau"
      ],
      "abstract": "Our work introduces an innovative approach to graph learning by leveraging\nHyperdimensional Computing. Graphs serve as a widely embraced method for\nconveying information, and their utilization in learning has gained significant\nattention. This is notable in the field of chemoinformatics, where learning\nfrom graph representations plays a pivotal role. An important application\nwithin this domain involves the identification of cancerous cells across\ndiverse molecular structures.\n  We propose an HDC-based model that demonstrates comparable Area Under the\nCurve results when compared to state-of-the-art models like Graph Neural\nNetworks (GNNs) or the Weisfieler-Lehman graph kernel (WL). Moreover, it\noutperforms previously proposed hyperdimensional computing graph learning\nmethods. Furthermore, it achieves noteworthy speed enhancements, boasting a 40x\nacceleration in the training phase and a 15x improvement in inference time\ncompared to GNN and WL models. This not only underscores the efficacy of the\nHDC-based method, but also highlights its potential for expedited and\nresource-efficient graph learning.",
      "tldr_zh": "本研究提出了一种基于 Hyperdimensional Computing (HDC) 的创新图学习方法，用于分子分类，特别是化学信息学中的癌症细胞识别。该方法通过 HDC 模型处理图表示，与 Graph Neural Networks (GNNs) 和 Weisfieler-Lehman (WL) 图核相比，实现了相似的 Area Under the Curve (AUC) 结果，并优于现有 HDC 图学习方法。同时，该模型在训练阶段实现了 40 倍速度提升，在推理阶段实现了 15 倍加速，突显了其高效和资源节约的优势。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.NE",
        "q-bio.QM"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2403.12307v1",
      "published_date": "2024-03-18 23:16:17 UTC",
      "updated_date": "2024-03-18 23:16:17 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T16:29:05.328452"
    },
    {
      "arxiv_id": "2403.12297v1",
      "title": "Leveraging Large Language Models to Extract Information on Substance Use Disorder Severity from Clinical Notes: A Zero-shot Learning Approach",
      "title_zh": "翻译失败",
      "authors": [
        "Maria Mahbub",
        "Gregory M. Dams",
        "Sudarshan Srinivasan",
        "Caitlin Rizy",
        "Ioana Danciu",
        "Jodie Trafton",
        "Kathryn Knight"
      ],
      "abstract": "Substance use disorder (SUD) poses a major concern due to its detrimental\neffects on health and society. SUD identification and treatment depend on a\nvariety of factors such as severity, co-determinants (e.g., withdrawal\nsymptoms), and social determinants of health. Existing diagnostic coding\nsystems used by American insurance providers, like the International\nClassification of Diseases (ICD-10), lack granularity for certain diagnoses,\nbut clinicians will add this granularity (as that found within the Diagnostic\nand Statistical Manual of Mental Disorders classification or DSM-5) as\nsupplemental unstructured text in clinical notes. Traditional natural language\nprocessing (NLP) methods face limitations in accurately parsing such diverse\nclinical language. Large Language Models (LLMs) offer promise in overcoming\nthese challenges by adapting to diverse language patterns. This study\ninvestigates the application of LLMs for extracting severity-related\ninformation for various SUD diagnoses from clinical notes. We propose a\nworkflow employing zero-shot learning of LLMs with carefully crafted prompts\nand post-processing techniques. Through experimentation with Flan-T5, an\nopen-source LLM, we demonstrate its superior recall compared to the rule-based\napproach. Focusing on 11 categories of SUD diagnoses, we show the effectiveness\nof LLMs in extracting severity information, contributing to improved risk\nassessment and treatment planning for SUD patients.",
      "tldr_zh": "本研究探讨了利用大型语言模型（LLMs）通过零样本学习（zero-shot learning）方法，从临床笔记中提取物质使用障碍（SUD）严重程度信息，以解决传统自然语言处理（NLP）方法在解析多样化临床语言时的局限性。研究提出了一种工作流，包括精心设计的提示和后处理技术，并使用开源模型 Flan-T5 进行实验，结果显示其召回率优于基于规则的方法。针对 11 类 SUD 诊断，该方法证明了 LLMs 在提升风险评估和治疗规划方面的有效性，从而为更精确的 SUD 患者管理提供支持。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "10 pages, 4 figures, 2 tables",
      "pdf_url": "http://arxiv.org/pdf/2403.12297v1",
      "published_date": "2024-03-18 22:39:03 UTC",
      "updated_date": "2024-03-18 22:39:03 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T16:29:19.274873"
    },
    {
      "arxiv_id": "2403.12242v3",
      "title": "Reference-based Metrics Disprove Themselves in Question Generation",
      "title_zh": "翻译失败",
      "authors": [
        "Bang Nguyen",
        "Mengxia Yu",
        "Yun Huang",
        "Meng Jiang"
      ],
      "abstract": "Reference-based metrics such as BLEU and BERTScore are widely used to\nevaluate question generation (QG). In this study, on QG benchmarks such as\nSQuAD and HotpotQA, we find that using human-written references cannot\nguarantee the effectiveness of the reference-based metrics. Most QG benchmarks\nhave only one reference; we replicate the annotation process and collect\nanother reference. A good metric is expected to grade a human-validated\nquestion no worse than generated questions. However, the results of\nreference-based metrics on our newly collected reference disproved the metrics\nthemselves. We propose a reference-free metric consisted of multi-dimensional\ncriteria such as naturalness, answerability, and complexity, utilizing large\nlanguage models. These criteria are not constrained to the syntactic or\nsemantic of a single reference question, and the metric does not require a\ndiverse set of references. Experiments reveal that our metric accurately\ndistinguishes between high-quality questions and flawed ones, and achieves\nstate-of-the-art alignment with human judgment.",
      "tldr_zh": "本研究发现，参考-based metrics 如 BLEU 和 BERTScore 在评估 Question Generation (QG) 时无效，因为使用人类参考（如 SQuAD 和 HotpotQA 基准）无法保证其准确性；实验通过收集额外参考证明，这些 metrics 往往将人类验证的问题评级低于生成的低质问题，从而自证失效。作者提出了一种 reference-free metric，利用 Large Language Models (LLMs) 基于多维度标准（如 naturalness、answerability 和 complexity）进行评估，该方法不依赖单一参考集。实验结果显示，新 metric 能精确区分高质量与有缺陷的问题，并实现了 state-of-the-art 与人类判断的一致性。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "EMNLP 2024 Findings - Camera Ready",
      "pdf_url": "http://arxiv.org/pdf/2403.12242v3",
      "published_date": "2024-03-18 20:47:10 UTC",
      "updated_date": "2024-10-10 16:55:31 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T16:29:30.894798"
    },
    {
      "arxiv_id": "2403.12237v2",
      "title": "Efficient Transformer-based Hyper-parameter Optimization for Resource-constrained IoT Environments",
      "title_zh": "翻译失败",
      "authors": [
        "Ibrahim Shaer",
        "Soodeh Nikan",
        "Abdallah Shami"
      ],
      "abstract": "The hyper-parameter optimization (HPO) process is imperative for finding the\nbest-performing Convolutional Neural Networks (CNNs). The automation process of\nHPO is characterized by its sizable computational footprint and its lack of\ntransparency; both important factors in a resource-constrained Internet of\nThings (IoT) environment. In this paper, we address these problems by proposing\na novel approach that combines transformer architecture and actor-critic\nReinforcement Learning (RL) model, TRL-HPO, equipped with multi-headed\nattention that enables parallelization and progressive generation of layers.\nThese assumptions are founded empirically by evaluating TRL-HPO on the MNIST\ndataset and comparing it with state-of-the-art approaches that build CNN models\nfrom scratch. The results show that TRL-HPO outperforms the classification\nresults of these approaches by 6.8% within the same time frame, demonstrating\nthe efficiency of TRL-HPO for the HPO process. The analysis of the results\nidentifies the main culprit for performance degradation attributed to stacking\nfully connected layers. This paper identifies new avenues for improving\nRL-based HPO processes in resource-constrained environments.",
      "tldr_zh": "本研究针对超参数优化 (HPO) 在资源受限的 Internet of Things (IoT) 环境中的计算密集和不透明问题，提出了一种高效方法 TRL-HPO，该方法结合 Transformer 架构和 Actor-Critic 强化学习 (RL) 模型，利用多头注意力机制实现并行化和渐进式层生成。TRL-HPO 通过从零构建 CNN 模型，并在 MNIST 数据集上进行评估，与现有方法相比，在相同时间内将分类准确率提高了 6.8%。结果分析显示，性能下降的主要原因在于堆叠全连接层，这为在资源受限环境中改进 RL-based HPO 过程提供了新方向。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "7 pages, Submitted to IEEE Internet of Things Magazine",
      "pdf_url": "http://arxiv.org/pdf/2403.12237v2",
      "published_date": "2024-03-18 20:35:35 UTC",
      "updated_date": "2024-05-01 21:39:21 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T16:29:42.144840"
    },
    {
      "arxiv_id": "2403.15453v1",
      "title": "Span-Oriented Information Extraction -- A Unifying Perspective on Information Extraction",
      "title_zh": "翻译失败",
      "authors": [
        "Yifan Ding",
        "Michael Yankoski",
        "Tim Weninger"
      ],
      "abstract": "Information Extraction refers to a collection of tasks within Natural\nLanguage Processing (NLP) that identifies sub-sequences within text and their\nlabels. These tasks have been used for many years to link extract relevant\ninformation and to link free text to structured data. However, the\nheterogeneity among information extraction tasks impedes progress in this area.\nWe therefore offer a unifying perspective centered on what we define to be\nspans in text. We then re-orient these seemingly incongruous tasks into this\nunified perspective and then re-present the wide assortment of information\nextraction tasks as variants of the same basic Span-Oriented Information\nExtraction task.",
      "tldr_zh": "信息提取（Information Extraction）是自然语言处理（NLP）中的一类任务，用于识别文本中的子序列及其标签，但任务间的异质性阻碍了该领域的进展。论文提出一个以 spans（文本子序列）为中心的统一视角，将各种看似不相关的提取任务重新组织起来。最终，将这些任务视为 Span-Oriented Information Extraction 的变体，从而为信息提取研究提供一个更 cohesive 的框架。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.IR"
      ],
      "primary_category": "cs.CL",
      "comment": "35 Pages, 1 Figure",
      "pdf_url": "http://arxiv.org/pdf/2403.15453v1",
      "published_date": "2024-03-18 20:10:44 UTC",
      "updated_date": "2024-03-18 20:10:44 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T16:29:53.798731"
    },
    {
      "arxiv_id": "2403.12212v2",
      "title": "Evaluating Named Entity Recognition: A comparative analysis of mono- and multilingual transformer models on a novel Brazilian corporate earnings call transcripts dataset",
      "title_zh": "翻译失败",
      "authors": [
        "Ramon Abilio",
        "Guilherme Palermo Coelho",
        "Ana Estela Antunes da Silva"
      ],
      "abstract": "Since 2018, when the Transformer architecture was introduced, Natural\nLanguage Processing has gained significant momentum with pre-trained\nTransformer-based models that can be fine-tuned for various tasks. Most models\nare pre-trained on large English corpora, making them less applicable to other\nlanguages, such as Brazilian Portuguese. In our research, we identified two\nmodels pre-trained in Brazilian Portuguese (BERTimbau and PTT5) and two\nmultilingual models (mBERT and mT5). BERTimbau and mBERT use only the Encoder\nmodule, while PTT5 and mT5 use both the Encoder and Decoder. Our study aimed to\nevaluate their performance on a financial Named Entity Recognition (NER) task\nand determine the computational requirements for fine-tuning and inference. To\nthis end, we developed the Brazilian Financial NER (BraFiNER) dataset,\ncomprising sentences from Brazilian banks' earnings calls transcripts annotated\nusing a weakly supervised approach. Additionally, we introduced a novel\napproach that reframes the token classification task as a text generation\nproblem. After fine-tuning the models, we evaluated them using performance and\nerror metrics. Our findings reveal that BERT-based models consistently\noutperform T5-based models. While the multilingual models exhibit comparable\nmacro F1-scores, BERTimbau demonstrates superior performance over PTT5. In\nterms of error metrics, BERTimbau outperforms the other models. We also\nobserved that PTT5 and mT5 generated sentences with changes in monetary and\npercentage values, highlighting the importance of accuracy and consistency in\nthe financial domain. Our findings provide insights into the differing\nperformance of BERT- and T5-based models for the NER task.",
      "tldr_zh": "本研究评估了单语和多语 Transformer 模型在金融 Named Entity Recognition (NER) 任务上的性能，比较了 BERTimbau、PTT5、mBERT 和 mT5 四个模型。研究者开发了新型数据集 BraFiNER，该数据集基于巴西银行财报电话会议的句子，并采用弱监督方法进行标注，同时引入了一种创新方法，将 token 分类任务重新 framing 为文本生成问题。实验结果显示，BERT-based 模型（如 BERTimbau 和 mBERT）在 macro F1-score 和错误指标上优于 T5-based 模型（如 PTT5 和 mT5）。此外，BERTimbau 表现出最佳性能，而 PTT5 和 mT5 在处理货币和百分比值时存在不准确性，这突显了在金融领域模型一致性的重要性。总体而言，该研究为 Transformer 模型在巴西葡萄牙语 NER 任务中的应用提供了宝贵洞见。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG",
        "68T50"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2403.12212v2",
      "published_date": "2024-03-18 19:53:56 UTC",
      "updated_date": "2024-08-30 17:02:11 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T16:30:08.330786"
    },
    {
      "arxiv_id": "2403.12211v2",
      "title": "A Unified Model for Longitudinal Multi-Modal Multi-View Prediction with Missingness",
      "title_zh": "翻译失败",
      "authors": [
        "Boqi Chen",
        "Junier Oliva",
        "Marc Niethammer"
      ],
      "abstract": "Medical records often consist of different modalities, such as images, text,\nand tabular information. Integrating all modalities offers a holistic view of a\npatient's condition, while analyzing them longitudinally provides a better\nunderstanding of disease progression. However, real-world longitudinal medical\nrecords present challenges: 1) patients may lack some or all of the data for a\nspecific timepoint, and 2) certain modalities or views might be absent for all\npatients during a particular period. In this work, we introduce a unified model\nfor longitudinal multi-modal multi-view prediction with missingness. Our method\nallows as many timepoints as desired for input, and aims to leverage all\navailable data, regardless of their availability. We conduct extensive\nexperiments on the knee osteoarthritis dataset from the Osteoarthritis\nInitiative for pain and Kellgren-Lawrence grade prediction at a future\ntimepoint. We demonstrate the effectiveness of our method by comparing results\nfrom our unified model to specific models that use the same modality and view\ncombinations during training and evaluation. We also show the benefit of having\nextended temporal data and provide post-hoc analysis for a deeper understanding\nof each modality/view's importance for different tasks.",
      "tldr_zh": "这篇论文提出了一种统一的模型，用于处理纵向多模态多视图预测中的数据缺失问题，旨在整合图像、文本和表格等模态，提供患者状况的整体视图并分析疾病进展。模型允许输入任意多个时间点，并充分利用所有可用数据，而不管缺失情况如何。在膝关节骨关节炎数据集上的实验中，该模型在预测未来的疼痛和 Kellgren-Lawrence 等级时，比特定模态/视图模型表现出色，提高了预测准确性。研究还展示了扩展时间数据的益处，并通过后验分析揭示了每个模态/视图对不同任务的重要性。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2403.12211v2",
      "published_date": "2024-03-18 19:51:55 UTC",
      "updated_date": "2024-03-22 00:17:11 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T16:30:18.487847"
    },
    {
      "arxiv_id": "2403.12207v1",
      "title": "Synthetic Image Generation in Cyber Influence Operations: An Emergent Threat?",
      "title_zh": "网络影响行动中的合成图像生成：一个新兴威胁？",
      "authors": [
        "Melanie Mathys",
        "Marco Willi",
        "Michael Graber",
        "Raphael Meier"
      ],
      "abstract": "The evolution of artificial intelligence (AI) has catalyzed a transformation\nin digital content generation, with profound implications for cyber influence\noperations. This report delves into the potential and limitations of generative\ndeep learning models, such as diffusion models, in fabricating convincing\nsynthetic images. We critically assess the accessibility, practicality, and\noutput quality of these tools and their implications in threat scenarios of\ndeception, influence, and subversion. Notably, the report generates content for\nseveral hypothetical cyber influence operations to demonstrate the current\ncapabilities and limitations of these AI-driven methods for threat actors.\nWhile generative models excel at producing illustrations and non-realistic\nimagery, creating convincing photo-realistic content remains a significant\nchallenge, limited by computational resources and the necessity for\nhuman-guided refinement. Our exploration underscores the delicate balance\nbetween technological advancement and its potential for misuse, prompting\nrecommendations for ongoing research, defense mechanisms, multi-disciplinary\ncollaboration, and policy development. These recommendations aim to leverage\nAI's potential for positive impact while safeguarding against its risks to the\nintegrity of information, especially in the context of cyber influence.",
      "tldr_zh": "这篇报告探讨了生成式深度学习模型（如diffusion models）在合成图像生成中的应用及其在网络影响操作中的潜在威胁，通过评估这些工具的易用性、实用性和输出质量，并创建假设场景来展示其能力与局限。研究发现，这些模型在生成插图和非真实图像方面表现出色，但制作令人信服的照片级真实图像仍受计算资源限制和需要人工细化。最终，报告强调技术进步与滥用的平衡，提出持续研究、防卫机制、多学科合作和政策发展的建议，以防范对信息完整性的风险。",
      "categories": [
        "cs.CY",
        "cs.AI",
        "cs.CV",
        "K.4.0; I.2.0; I.4.0"
      ],
      "primary_category": "cs.CY",
      "comment": "44 pages, 56 figures",
      "pdf_url": "http://arxiv.org/pdf/2403.12207v1",
      "published_date": "2024-03-18 19:44:30 UTC",
      "updated_date": "2024-03-18 19:44:30 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T16:30:33.393969"
    },
    {
      "arxiv_id": "2403.12201v1",
      "title": "Compositional learning of functions in humans and machines",
      "title_zh": "人类和机器的函数组合学习",
      "authors": [
        "Yanli Zhou",
        "Brenden M. Lake",
        "Adina Williams"
      ],
      "abstract": "The ability to learn and compose functions is foundational to efficient\nlearning and reasoning in humans, enabling flexible generalizations such as\ncreating new dishes from known cooking processes. Beyond sequential chaining of\nfunctions, existing linguistics literature indicates that humans can grasp more\ncomplex compositions with interacting functions, where output production\ndepends on context changes induced by different function orderings. Extending\nthe investigation into the visual domain, we developed a function learning\nparadigm to explore the capacity of humans and neural network models in\nlearning and reasoning with compositional functions under varied interaction\nconditions. Following brief training on individual functions, human\nparticipants were assessed on composing two learned functions, in ways covering\nfour main interaction types, including instances in which the application of\nthe first function creates or removes the context for applying the second\nfunction. Our findings indicate that humans can make zero-shot generalizations\non novel visual function compositions across interaction conditions,\ndemonstrating sensitivity to contextual changes. A comparison with a neural\nnetwork model on the same task reveals that, through the meta-learning for\ncompositionality (MLC) approach, a standard sequence-to-sequence Transformer\ncan mimic human generalization patterns in composing functions.",
      "tldr_zh": "该研究探讨了人类和机器在函数组合学习方面的能力，强调这种能力是高效学习和推理的基础。研究者开发了一个视觉领域的函数学习范式，让人类在短暂训练后评估两种函数的组合，涵盖四种互动类型，包括函数顺序引发的上下文变化，结果显示人类能进行零-shot generalizations，并对上下文敏感。与此对比，通过 meta-learning for compositionality (MLC) 方法，一个标准的序列到序列 Transformer 模型能模仿人类的泛化模式，为理解跨领域组合学习提供了新见解。",
      "categories": [
        "cs.AI",
        "cs.HC",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "7 pages, 6 figures",
      "pdf_url": "http://arxiv.org/pdf/2403.12201v1",
      "published_date": "2024-03-18 19:22:53 UTC",
      "updated_date": "2024-03-18 19:22:53 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T16:30:44.802767"
    },
    {
      "arxiv_id": "2403.12197v1",
      "title": "E2F-Net: Eyes-to-Face Inpainting via StyleGAN Latent Space",
      "title_zh": "翻译失败",
      "authors": [
        "Ahmad Hassanpour",
        "Fatemeh Jamalbafrani",
        "Bian Yang",
        "Kiran Raja",
        "Raymond Veldhuis",
        "Julian Fierrez"
      ],
      "abstract": "Face inpainting, the technique of restoring missing or damaged regions in\nfacial images, is pivotal for applications like face recognition in occluded\nscenarios and image analysis with poor-quality captures. This process not only\nneeds to produce realistic visuals but also preserve individual identity\ncharacteristics. The aim of this paper is to inpaint a face given periocular\nregion (eyes-to-face) through a proposed new Generative Adversarial Network\n(GAN)-based model called Eyes-to-Face Network (E2F-Net). The proposed approach\nextracts identity and non-identity features from the periocular region using\ntwo dedicated encoders have been used. The extracted features are then mapped\nto the latent space of a pre-trained StyleGAN generator to benefit from its\nstate-of-the-art performance and its rich, diverse and expressive latent space\nwithout any additional training. We further improve the StyleGAN output to find\nthe optimal code in the latent space using a new optimization for GAN inversion\ntechnique. Our E2F-Net requires a minimum training process reducing the\ncomputational complexity as a secondary benefit. Through extensive experiments,\nwe show that our method successfully reconstructs the whole face with high\nquality, surpassing current techniques, despite significantly less training and\nsupervision efforts. We have generated seven eyes-to-face datasets based on\nwell-known public face datasets for training and verifying our proposed\nmethods. The code and datasets are publicly available.",
      "tldr_zh": "本论文提出 E2F-Net，一种基于 GAN 的模型，用于通过眼睛周围区域（periocular region）修复整个面部图像，确保生成真实视觉并保留个体身份特征。该方法使用两个专用编码器提取身份和非身份特征，并将这些特征映射到预训练 StyleGAN 的 latent space 中，同时采用新的 GAN inversion 优化技术，以提升输出质量并减少训练过程的计算复杂性。实验结果表明，E2F-Net 在面部重建任务上优于现有技术，尽管训练和监督需求较低；此外，论文生成了七个基于公共面部数据集的 eyes-to-face 数据集，并公开了代码和数据集。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2403.12197v1",
      "published_date": "2024-03-18 19:11:34 UTC",
      "updated_date": "2024-03-18 19:11:34 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T16:30:57.481115"
    },
    {
      "arxiv_id": "2403.12196v4",
      "title": "Leveraging Large Language Models to Detect npm Malicious Packages",
      "title_zh": "利用大语言模型检测 npm 恶意包",
      "authors": [
        "Nusrat Zahan",
        "Philipp Burckhardt",
        "Mikola Lysenko",
        "Feross Aboukhadijeh",
        "Laurie Williams"
      ],
      "abstract": "Existing malicious code detection techniques demand the integration of\nmultiple tools to detect different malware patterns, often suffering from high\nmisclassification rates. Therefore, malicious code detection techniques could\nbe enhanced by adopting advanced, more automated approaches to achieve high\naccuracy and a low misclassification rate. The goal of this study is to aid\nsecurity analysts in detecting malicious packages by empirically studying the\neffectiveness of Large Language Models (LLMs) in detecting malicious code. We\npresent SocketAI, a malicious code review workflow to detect malicious code. To\nevaluate the effectiveness of SocketAI, we leverage a benchmark dataset of\n5,115 npm packages, of which 2,180 packages have malicious code. We conducted a\nbaseline comparison of GPT-3 and GPT-4 models with the state-of-the-art CodeQL\nstatic analysis tool, using 39 custom CodeQL rules developed in prior research\nto detect malicious Javascript code. We also compare the effectiveness of\nstatic analysis as a pre-screener with SocketAI workflow, measuring the number\nof files that need to be analyzed. and the associated costs. Additionally, we\nperformed a qualitative study to understand the types of malicious activities\ndetected or missed by our workflow. Our baseline comparison demonstrates a 16%\nand 9% improvement over static analysis in precision and F1 scores,\nrespectively. GPT-4 achieves higher accuracy with 99% precision and 97% F1\nscores, while GPT-3 offers a more cost-effective balance at 91% precision and\n94% F1 scores. Pre-screening files with a static analyzer reduces the number of\nfiles requiring LLM analysis by 77.9% and decreases costs by 60.9% for GPT-3\nand 76.1% for GPT-4. Our qualitative analysis identified data theft, execution\nof arbitrary code, and suspicious domain categories as the top detected\nmalicious packages.",
      "tldr_zh": "本文利用 Large Language Models (LLMs) 开发了 SocketAI 工作流，用于检测 npm 恶意包，从而提高检测准确性和降低误分类率。研究通过一个包含 5,115 个 npm 包的数据集（其中 2,180 个有恶意代码）进行实验，与 CodeQL 静态分析工具比较，结果显示 GPT-4 达到了 99% 精度和 97% F1 分数，而 GPT-3 提供了更具成本效益的性能（91% 精度和 94% F1 分数）。此外，将静态分析作为预筛选，能减少 77.9% 的文件分析量和约 60.9% 的成本。定性分析还识别出数据窃取、执行任意代码和可疑域名作为主要检测到的恶意活动类型。",
      "categories": [
        "cs.CR",
        "cs.AI"
      ],
      "primary_category": "cs.CR",
      "comment": "13 pages, 2 Figure, 6 tables",
      "pdf_url": "http://arxiv.org/pdf/2403.12196v4",
      "published_date": "2024-03-18 19:10:12 UTC",
      "updated_date": "2025-01-06 16:29:32 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T16:31:09.728105"
    },
    {
      "arxiv_id": "2403.12181v1",
      "title": "MAC Advice for Facility Location Mechanism Design",
      "title_zh": "翻译失败",
      "authors": [
        "Zohar Barak",
        "Anupam Gupta",
        "Inbal Talgam-Cohen"
      ],
      "abstract": "Algorithms with predictions have attracted much attention in the last years\nacross various domains, including variants of facility location, as a way to\nsurpass traditional worst-case analyses. We study the $k$-facility location\nmechanism design problem, where the $n$ agents are strategic and might\nmisreport their location.\n  Unlike previous models, where predictions are for the $k$ optimal facility\nlocations, we receive $n$ predictions for the locations of each of the agents.\nHowever, these predictions are only \"mostly\" and \"approximately\" correct (or\nMAC for short) -- i.e., some $\\delta$-fraction of the predicted locations are\nallowed to be arbitrarily incorrect, and the remainder of the predictions are\nallowed to be correct up to an $\\varepsilon$-error. We make no assumption on\nthe independence of the errors. Can such predictions allow us to beat the\ncurrent best bounds for strategyproof facility location?\n  We show that the $1$-median (geometric median) of a set of points is\nnaturally robust under corruptions, which leads to an algorithm for\nsingle-facility location with MAC predictions. We extend the robustness result\nto a \"balanced\" variant of the $k$ facilities case. Without balancedness, we\nshow that robustness completely breaks down, even for the setting of $k=2$\nfacilities on a line. For this \"unbalanced\" setting, we devise a truthful\nrandom mechanism that outperforms the best known result of Lu et al. [2010],\nwhich does not use predictions. En route, we introduce the problem of \"second\"\nfacility location (when the first facility's location is already fixed). Our\nfindings on the robustness of the $1$-median and more generally $k$-medians may\nbe of independent interest, as quantitative versions of classic breakdown-point\nresults in robust statistics.",
      "tldr_zh": "这篇论文探讨了在 k-facility location 机制设计中，使用 MAC predictions（mostly and approximately correct 预测）来应对策略性代理人可能误报位置的问题，这些预测允许部分错误（δ-部分完全错误，其余ε-错误）。作者证明了 1-median（geometric median）对预测损坏具有天然鲁棒性，并将其扩展到 balanced k-设施变体；然而，在 unbalanced 场景（如 k=2 设施）中，鲁棒性失效，因此他们设计了一个 truthful random mechanism，比 Lu et al. [2010] 的无预测结果更优。总体上，这些发现为策略性设施位置问题提供了新算法，并与 robust statistics 中的经典结果相关。",
      "categories": [
        "cs.GT",
        "cs.AI"
      ],
      "primary_category": "cs.GT",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2403.12181v1",
      "published_date": "2024-03-18 18:52:04 UTC",
      "updated_date": "2024-03-18 18:52:04 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T16:31:19.944300"
    },
    {
      "arxiv_id": "2403.12176v4",
      "title": "Safety Implications of Explainable Artificial Intelligence in End-to-End Autonomous Driving",
      "title_zh": "可解释人工智能在端到端自动驾驶中的安全影响",
      "authors": [
        "Shahin Atakishiyev",
        "Mohammad Salameh",
        "Randy Goebel"
      ],
      "abstract": "The end-to-end learning pipeline is gradually creating a paradigm shift in\nthe ongoing development of highly autonomous vehicles (AVs), largely due to\nadvances in deep learning, the availability of large-scale training datasets,\nand improvements in integrated sensor devices. However, a lack of\nexplainability in real-time decisions with contemporary learning methods\nimpedes user trust and attenuates the widespread deployment and\ncommercialization of such vehicles. Moreover, the issue is exacerbated when\nthese cars are involved in or cause traffic accidents. Consequently,\nexplainability in end-to-end autonomous driving is essential to build trust in\nvehicular automation. With that said, automotive researchers have not yet\nrigorously explored safety benefits and consequences of explanations in\nend-to-end autonomous driving. This paper aims to bridge the gaps between these\ntopics and seeks to answer the following research question: What are safety\nimplications of explanations in end-to-end autonomous driving? In this regard,\nwe first revisit established safety and explainability concepts in end-to-end\ndriving. Furthermore, we present critical case studies and show the pivotal\nrole of explanations in enhancing driving safety. Finally, we describe insights\nfrom empirical studies and reveal potential value, limitations, and caveats of\npractical explainable AI methods with respect to their potential impacts on\nsafety of end-to-end driving.",
      "tldr_zh": "这篇论文探讨了在端到端自动驾驶（End-to-End Autonomous Driving）中，可解释人工智能（Explainable Artificial Intelligence）的安全影响，强调了缺乏可解释性如何导致用户信任缺失和事故风险增加。论文通过回顾安全与可解释性概念、分析关键案例研究，展示了解释如何提升驾驶安全。最终，基于实证研究，论文揭示了可解释 AI 的潜在价值、限制和注意事项，为构建更可靠的自动驾驶系统提供了重要见解。",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2403.12176v4",
      "published_date": "2024-03-18 18:49:20 UTC",
      "updated_date": "2025-04-20 23:39:27 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T16:31:30.942748"
    },
    {
      "arxiv_id": "2403.12173v1",
      "title": "TnT-LLM: Text Mining at Scale with Large Language Models",
      "title_zh": "TnT-LLM：利用大型语言模型进行大规模文本挖掘",
      "authors": [
        "Mengting Wan",
        "Tara Safavi",
        "Sujay Kumar Jauhar",
        "Yujin Kim",
        "Scott Counts",
        "Jennifer Neville",
        "Siddharth Suri",
        "Chirag Shah",
        "Ryen W White",
        "Longqi Yang",
        "Reid Andersen",
        "Georg Buscher",
        "Dhruv Joshi",
        "Nagu Rangan"
      ],
      "abstract": "Transforming unstructured text into structured and meaningful forms,\norganized by useful category labels, is a fundamental step in text mining for\ndownstream analysis and application. However, most existing methods for\nproducing label taxonomies and building text-based label classifiers still rely\nheavily on domain expertise and manual curation, making the process expensive\nand time-consuming. This is particularly challenging when the label space is\nunder-specified and large-scale data annotations are unavailable. In this\npaper, we address these challenges with Large Language Models (LLMs), whose\nprompt-based interface facilitates the induction and use of large-scale pseudo\nlabels. We propose TnT-LLM, a two-phase framework that employs LLMs to automate\nthe process of end-to-end label generation and assignment with minimal human\neffort for any given use-case. In the first phase, we introduce a zero-shot,\nmulti-stage reasoning approach which enables LLMs to produce and refine a label\ntaxonomy iteratively. In the second phase, LLMs are used as data labelers that\nyield training samples so that lightweight supervised classifiers can be\nreliably built, deployed, and served at scale. We apply TnT-LLM to the analysis\nof user intent and conversational domain for Bing Copilot (formerly Bing Chat),\nan open-domain chat-based search engine. Extensive experiments using both human\nand automatic evaluation metrics demonstrate that TnT-LLM generates more\naccurate and relevant label taxonomies when compared against state-of-the-art\nbaselines, and achieves a favorable balance between accuracy and efficiency for\nclassification at scale. We also share our practical experiences and insights\non the challenges and opportunities of using LLMs for large-scale text mining\nin real-world applications.",
      "tldr_zh": "这篇论文介绍了 TnT-LLM，一种利用 Large Language Models (LLMs) 的框架，旨在自动化大规模文本挖掘过程，将非结构化文本转化为结构化标签分类，从而减少对领域专家和手动标注的依赖。框架分为两阶段：第一阶段采用零-shot 多阶段推理方法，让 LLMs 迭代生成和精炼标签分类体系；第二阶段则使用 LLMs 作为数据标记器，生成训练样本以构建轻量级监督分类器，实现高效部署。在 Bing Copilot 的用户意图和对话域分析应用中，TnT-LLM 比现有基线方法生成更准确的相关标签，并在准确性和效率之间取得良好平衡，同时作者分享了使用 LLMs 在实际文本挖掘中的挑战与机遇。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.IR"
      ],
      "primary_category": "cs.CL",
      "comment": "9 pages main content, 8 pages references and appendix",
      "pdf_url": "http://arxiv.org/pdf/2403.12173v1",
      "published_date": "2024-03-18 18:45:28 UTC",
      "updated_date": "2024-03-18 18:45:28 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T16:31:44.804789"
    },
    {
      "arxiv_id": "2403.12172v2",
      "title": "Graph-Jigsaw Conditioned Diffusion Model for Skeleton-based Video Anomaly Detection",
      "title_zh": "翻译失败",
      "authors": [
        "Ali Karami",
        "Thi Kieu Khanh Ho",
        "Narges Armanfard"
      ],
      "abstract": "Skeleton-based video anomaly detection (SVAD) is a crucial task in computer\nvision. Accurately identifying abnormal patterns or events enables operators to\npromptly detect suspicious activities, thereby enhancing safety. Achieving this\ndemands a comprehensive understanding of human motions, both at body and region\nlevels, while also accounting for the wide variations of performing a single\naction. However, existing studies fail to simultaneously address these crucial\nproperties. This paper introduces a novel, practical and lightweight framework,\nnamely Graph-Jigsaw Conditioned Diffusion Model for Skeleton-based Video\nAnomaly Detection (GiCiSAD) to overcome the challenges associated with SVAD.\nGiCiSAD consists of three novel modules: the Graph Attention-based Forecasting\nmodule to capture the spatio-temporal dependencies inherent in the data, the\nGraph-level Jigsaw Puzzle Maker module to distinguish subtle region-level\ndiscrepancies between normal and abnormal motions, and the Graph-based\nConditional Diffusion model to generate a wide spectrum of human motions.\nExtensive experiments on four widely used skeleton-based video datasets show\nthat GiCiSAD outperforms existing methods with significantly fewer training\nparameters, establishing it as the new state-of-the-art.",
      "tldr_zh": "这篇论文针对基于骨骼的视频异常检测 (SVAD) 提出了一种新颖的轻量级框架 GiCiSAD，以解决人体动作在时空依赖、区域差异和动作变异方面的挑战。GiCiSAD 包含三个关键模块：Graph Attention-based Forecasting 模块用于捕捉数据中的时空依赖、Graph-level Jigsaw Puzzle Maker 模块用于区分正常和异常动作的微妙差异，以及 Graph-based Conditional Diffusion 模型用于生成多样的人体动作。实验结果显示，在四个常用数据集上，GiCiSAD 优于现有方法，同时显著减少训练参数，确立了新的状态-of-the-art 性能。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "Accepted at the Winter Conference on Applications of Computer Vision\n  (WACV). 17 pages, 6 figures, 6 tables",
      "pdf_url": "http://arxiv.org/pdf/2403.12172v2",
      "published_date": "2024-03-18 18:42:32 UTC",
      "updated_date": "2024-08-31 02:36:11 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T16:31:55.972885"
    },
    {
      "arxiv_id": "2403.12171v1",
      "title": "EasyJailbreak: A Unified Framework for Jailbreaking Large Language Models",
      "title_zh": "EasyJailbreak：用于大语言模型越狱攻击的统一框架",
      "authors": [
        "Weikang Zhou",
        "Xiao Wang",
        "Limao Xiong",
        "Han Xia",
        "Yingshuang Gu",
        "Mingxu Chai",
        "Fukang Zhu",
        "Caishuang Huang",
        "Shihan Dou",
        "Zhiheng Xi",
        "Rui Zheng",
        "Songyang Gao",
        "Yicheng Zou",
        "Hang Yan",
        "Yifan Le",
        "Ruohui Wang",
        "Lijun Li",
        "Jing Shao",
        "Tao Gui",
        "Qi Zhang",
        "Xuanjing Huang"
      ],
      "abstract": "Jailbreak attacks are crucial for identifying and mitigating the security\nvulnerabilities of Large Language Models (LLMs). They are designed to bypass\nsafeguards and elicit prohibited outputs. However, due to significant\ndifferences among various jailbreak methods, there is no standard\nimplementation framework available for the community, which limits\ncomprehensive security evaluations. This paper introduces EasyJailbreak, a\nunified framework simplifying the construction and evaluation of jailbreak\nattacks against LLMs. It builds jailbreak attacks using four components:\nSelector, Mutator, Constraint, and Evaluator. This modular framework enables\nresearchers to easily construct attacks from combinations of novel and existing\ncomponents. So far, EasyJailbreak supports 11 distinct jailbreak methods and\nfacilitates the security validation of a broad spectrum of LLMs. Our validation\nacross 10 distinct LLMs reveals a significant vulnerability, with an average\nbreach probability of 60% under various jailbreaking attacks. Notably, even\nadvanced models like GPT-3.5-Turbo and GPT-4 exhibit average Attack Success\nRates (ASR) of 57% and 33%, respectively. We have released a wealth of\nresources for researchers, including a web platform, PyPI published package,\nscreencast video, and experimental outputs.",
      "tldr_zh": "本研究提出EasyJailbreak，一种统一的框架，用于简化对Large Language Models (LLMs)的Jailbreak attacks（越狱攻击），以识别和缓解其安全漏洞。该框架由四个组件组成：Selector、Mutator、Constraint和Evaluator，允许研究人员轻松组合现有和新颖组件来构建和评估11种不同的攻击方法。实验结果显示，在10个不同LLMs上，平均攻击成功率（ASR）达到60%，即使是高级模型如GPT-3.5-Turbo和GPT-4也分别有57%和33%的ASR，突显了LLMs的显著安全风险。该框架还发布了资源，包括web平台、PyPI包、视频和实验输出，以支持进一步的研究。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2403.12171v1",
      "published_date": "2024-03-18 18:39:53 UTC",
      "updated_date": "2024-03-18 18:39:53 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T16:32:08.836930"
    },
    {
      "arxiv_id": "2403.12162v1",
      "title": "Intelligent Execution through Plan Analysis",
      "title_zh": "通过计划分析的智能执行",
      "authors": [
        "Daniel Borrajo",
        "Manuela Veloso"
      ],
      "abstract": "Intelligent robots need to generate and execute plans. In order to deal with\nthe complexity of real environments, planning makes some assumptions about the\nworld. When executing plans, the assumptions are usually not met. Most works\nhave focused on the negative impact of this fact and the use of replanning\nafter execution failures. Instead, we focus on the positive impact, or\nopportunities to find better plans. When planning, the proposed technique finds\nand stores those opportunities. Later, during execution, the monitoring system\ncan use them to focus perception and repair the plan, instead of replanning\nfrom scratch. Experiments in several paradigmatic robotic tasks show how the\napproach outperforms standard replanning strategies.",
      "tldr_zh": "该研究探讨了智能机器人计划执行中的挑战，强调规划假设在实际环境中往往不满足，而非关注负面影响（如执行失败后的重新规划），本文聚焦于发现这些不满足带来的机会。提出的技术在规划阶段识别并存储这些机会，并在执行过程中利用监控系统进行感知聚焦和计划修复，而不是从零开始重新规划。实验在多个典型机器人任务中表明，该方法优于标准重新规划策略，提供了一种更高效的智能执行框架。",
      "categories": [
        "cs.AI",
        "cs.RO"
      ],
      "primary_category": "cs.AI",
      "comment": "Published at IROS 21, 6 pages",
      "pdf_url": "http://arxiv.org/pdf/2403.12162v1",
      "published_date": "2024-03-18 18:23:36 UTC",
      "updated_date": "2024-03-18 18:23:36 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T16:32:20.077583"
    },
    {
      "arxiv_id": "2403.12153v1",
      "title": "Routing and Scheduling in Answer Set Programming applied to Multi-Agent Path Finding: Preliminary Report",
      "title_zh": "翻译失败",
      "authors": [
        "Roland Kaminski",
        "Torsten Schaub",
        "Tran Cao Son",
        "Jiří Švancara",
        "Philipp Wanko"
      ],
      "abstract": "We present alternative approaches to routing and scheduling in Answer Set\nProgramming (ASP), and explore them in the context of Multi-agent Path Finding.\nThe idea is to capture the flow of time in terms of partial orders rather than\ntime steps attached to actions and fluents. This also abolishes the need for\nfixed upper bounds on the length of plans. The trade-off for this avoidance is\nthat (parts of) temporal trajectories must be acyclic, since multiple\noccurrences of the same action or fluent cannot be distinguished anymore. While\nthis approach provides an interesting alternative for modeling routing, it is\nwithout alternative for scheduling since fine-grained timings cannot be\nrepresented in ASP in a feasible way. This is different for partial orders that\ncan be efficiently handled by external means such as acyclicity and difference\nconstraints. We formally elaborate upon this idea and present several resulting\nASP encodings. Finally, we demonstrate their effectiveness via an empirical\nanalysis.",
      "tldr_zh": "本研究探讨了在 Answer Set Programming (ASP) 中使用部分顺序 (partial orders) 来处理路由和调度的替代方法，并将其应用于 Multi-agent Path Finding。传统方法依赖于动作和流畅的时间步骤，而新方法通过部分顺序表示时间流，避免了计划长度的固定上限，但要求时间轨迹部分必须无环，以区分重复动作。论文提供了几个ASP编码，强调这种方法特别适合调度，因为部分顺序可通过外部工具如 acyclicity 和 difference constraints 高效管理。实验分析证明了这些编码的有效性，为路由和调度建模提供了新视角。",
      "categories": [
        "cs.AI",
        "cs.LO",
        "cs.SC"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2403.12153v1",
      "published_date": "2024-03-18 18:09:47 UTC",
      "updated_date": "2024-03-18 18:09:47 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T16:32:32.435955"
    },
    {
      "arxiv_id": "2403.12151v3",
      "title": "Fusing Domain-Specific Content from Large Language Models into Knowledge Graphs for Enhanced Zero Shot Object State Classification",
      "title_zh": "将大型语言模型中的领域特定内容",
      "authors": [
        "Filippos Gouidis",
        "Katerina Papantoniou",
        "Konstantinos Papoutsakis",
        "Theodore Patkos",
        "Antonis Argyros",
        "Dimitris Plexousakis"
      ],
      "abstract": "Domain-specific knowledge can significantly contribute to addressing a wide\nvariety of vision tasks. However, the generation of such knowledge entails\nconsiderable human labor and time costs. This study investigates the potential\nof Large Language Models (LLMs) in generating and providing domain-specific\ninformation through semantic embeddings. To achieve this, an LLM is integrated\ninto a pipeline that utilizes Knowledge Graphs and pre-trained semantic vectors\nin the context of the Vision-based Zero-shot Object State Classification task.\nWe thoroughly examine the behavior of the LLM through an extensive ablation\nstudy. Our findings reveal that the integration of LLM-based embeddings, in\ncombination with general-purpose pre-trained embeddings, leads to substantial\nperformance improvements. Drawing insights from this ablation study, we conduct\na comparative analysis against competing models, thereby highlighting the\nstate-of-the-art performance achieved by the proposed approach.",
      "tldr_zh": "该研究探讨了如何利用 Large Language Models (LLMs) 生成领域特定知识，并将其融合到 Knowledge Graphs 中，以提升 Zero-shot Object State Classification 任务的性能。方法涉及将 LLM 与预训练语义向量整合成一个管道，生成并提供领域特定信息。实验通过广泛的 ablation study 发现，结合 LLM-based embeddings 和通用预训练嵌入能显著改善分类准确性。最终，与竞争模型相比，该方法实现了 state-of-the-art 性能，为视觉任务注入高效的领域知识提供了新途径。",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.CV",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "Accepted at the AAAI-MAKE 2024",
      "pdf_url": "http://arxiv.org/pdf/2403.12151v3",
      "published_date": "2024-03-18 18:08:44 UTC",
      "updated_date": "2024-12-11 18:12:43 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T16:32:45.186701"
    },
    {
      "arxiv_id": "2403.12143v3",
      "title": "Graph Neural Networks for Learning Equivariant Representations of Neural Networks",
      "title_zh": "图神经网络用于学习神经",
      "authors": [
        "Miltiadis Kofinas",
        "Boris Knyazev",
        "Yan Zhang",
        "Yunlu Chen",
        "Gertjan J. Burghouts",
        "Efstratios Gavves",
        "Cees G. M. Snoek",
        "David W. Zhang"
      ],
      "abstract": "Neural networks that process the parameters of other neural networks find\napplications in domains as diverse as classifying implicit neural\nrepresentations, generating neural network weights, and predicting\ngeneralization errors. However, existing approaches either overlook the\ninherent permutation symmetry in the neural network or rely on intricate\nweight-sharing patterns to achieve equivariance, while ignoring the impact of\nthe network architecture itself. In this work, we propose to represent neural\nnetworks as computational graphs of parameters, which allows us to harness\npowerful graph neural networks and transformers that preserve permutation\nsymmetry. Consequently, our approach enables a single model to encode neural\ncomputational graphs with diverse architectures. We showcase the effectiveness\nof our method on a wide range of tasks, including classification and editing of\nimplicit neural representations, predicting generalization performance, and\nlearning to optimize, while consistently outperforming state-of-the-art\nmethods. The source code is open-sourced at\nhttps://github.com/mkofinas/neural-graphs.",
      "tldr_zh": "本研究提出了一种使用 Graph Neural Networks (GNNs) 和 Transformers 来学习神经网络等变表示 (equivariant representations) 的方法，以解决现有模型忽略排列对称性或依赖复杂权重共享模式的问题。方法将神经网络表示为参数的计算图 (computational graphs)，从而允许一个单一模型处理不同架构的神经计算图，并在任务中保持对称性。实验结果显示，该方法在隐式神经表示的分类和编辑、预测泛化性能以及学习优化等任务上，均优于最先进方法，并提供了开源代码以供进一步应用。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "stat.ML"
      ],
      "primary_category": "cs.LG",
      "comment": "In ICLR 2024. Source code: https://github.com/mkofinas/neural-graphs",
      "pdf_url": "http://arxiv.org/pdf/2403.12143v3",
      "published_date": "2024-03-18 18:01:01 UTC",
      "updated_date": "2024-07-23 16:30:10 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T16:32:57.683745"
    },
    {
      "arxiv_id": "2403.12031v2",
      "title": "RouterBench: A Benchmark for Multi-LLM Routing System",
      "title_zh": "翻译失败",
      "authors": [
        "Qitian Jason Hu",
        "Jacob Bieker",
        "Xiuyu Li",
        "Nan Jiang",
        "Benjamin Keigwin",
        "Gaurav Ranganath",
        "Kurt Keutzer",
        "Shriyash Kaustubh Upadhyay"
      ],
      "abstract": "As the range of applications for Large Language Models (LLMs) continues to\ngrow, the demand for effective serving solutions becomes increasingly critical.\nDespite the versatility of LLMs, no single model can optimally address all\ntasks and applications, particularly when balancing performance with cost. This\nlimitation has led to the development of LLM routing systems, which combine the\nstrengths of various models to overcome the constraints of individual LLMs.\nYet, the absence of a standardized benchmark for evaluating the performance of\nLLM routers hinders progress in this area. To bridge this gap, we present\nRouterBench, a novel evaluation framework designed to systematically assess the\nefficacy of LLM routing systems, along with a comprehensive dataset comprising\nover 405k inference outcomes from representative LLMs to support the\ndevelopment of routing strategies. We further propose a theoretical framework\nfor LLM routing, and deliver a comparative analysis of various routing\napproaches through RouterBench, highlighting their potentials and limitations\nwithin our evaluation framework. This work not only formalizes and advances the\ndevelopment of LLM routing systems but also sets a standard for their\nassessment, paving the way for more accessible and economically viable LLM\ndeployments. The code and data are available at\nhttps://github.com/withmartian/routerbench.",
      "tldr_zh": "该论文针对大型语言模型（LLMs）在任务处理中的性能与成本平衡问题，提出了RouterBench，这是一个用于评估多LLM路由系统的标准化基准框架。该框架包括一个包含超过40.5万推理结果的综合数据集，以及一个LLM路由的理论框架，支持路由策略的开发和优化。通过RouterBench对各种路由方法进行比较分析，论文突出了它们的潜力与局限性，为更经济可行的LLM部署奠定了标准。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2403.12031v2",
      "published_date": "2024-03-18 17:59:04 UTC",
      "updated_date": "2024-03-28 17:56:28 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T16:33:09.172731"
    },
    {
      "arxiv_id": "2403.12029v3",
      "title": "Align and Distill: Unifying and Improving Domain Adaptive Object Detection",
      "title_zh": "Align and Distill：统一和改进领域自适应目标检测",
      "authors": [
        "Justin Kay",
        "Timm Haucke",
        "Suzanne Stathatos",
        "Siqi Deng",
        "Erik Young",
        "Pietro Perona",
        "Sara Beery",
        "Grant Van Horn"
      ],
      "abstract": "Object detectors often perform poorly on data that differs from their\ntraining set. Domain adaptive object detection (DAOD) methods have recently\ndemonstrated strong results on addressing this challenge. Unfortunately, we\nidentify systemic benchmarking pitfalls that call past results into question\nand hamper further progress: (a) Overestimation of performance due to\nunderpowered baselines, (b) Inconsistent implementation practices preventing\ntransparent comparisons of methods, and (c) Lack of generality due to outdated\nbackbones and lack of diversity in benchmarks. We address these problems by\nintroducing: (1) A unified benchmarking and implementation framework, Align and\nDistill (ALDI), enabling comparison of DAOD methods and supporting future\ndevelopment, (2) A fair and modern training and evaluation protocol for DAOD\nthat addresses benchmarking pitfalls, (3) A new DAOD benchmark dataset,\nCFC-DAOD, enabling evaluation on diverse real-world data, and (4) A new method,\nALDI++, that achieves state-of-the-art results by a large margin. ALDI++\noutperforms the previous state-of-the-art by +3.5 AP50 on Cityscapes to Foggy\nCityscapes, +5.7 AP50 on Sim10k to Cityscapes (where ours is the only method to\noutperform a fair baseline), and +0.6 AP50 on CFC Kenai to Channel. ALDI and\nALDI++ are architecture-agnostic, setting a new state-of-the-art for YOLO and\nDETR-based DAOD as well without additional hyperparameter tuning. Our\nframework, dataset, and state-of-the-art method offer a critical reset for DAOD\nand provide a strong foundation for future research. Code and data are\navailable: https://github.com/justinkay/aldi and\nhttps://github.com/visipedia/caltech-fish-counting.",
      "tldr_zh": "这篇论文针对Domain Adaptive Object Detection (DAOD)领域的基准测试问题（如基准过低、不一致实现和缺乏通用性），提出了一种统一框架Align and Distill (ALDI)，以支持方法比较和未来发展。研究团队引入了公平的现代训练协议、新数据集CFC-DAOD（用于评估多样化真实世界数据），以及新方法ALDI++，后者在多个基准上大幅提升性能，包括Cityscapes to Foggy Cityscapes上提高3.5 AP50、Sim10k to Cityscapes上提高5.7 AP50（超越公平基准），以及CFC Kenai to Channel上提高0.6 AP50。ALDI和ALDI++架构无关，能为YOLO和DETR-based DAOD设置新标准，为DAOD研究提供坚实基础。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "TMLR camera ready (Featured Certification). 33 pages, 15 figures",
      "pdf_url": "http://arxiv.org/pdf/2403.12029v3",
      "published_date": "2024-03-18 17:58:02 UTC",
      "updated_date": "2025-03-17 20:18:16 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T16:33:23.143068"
    },
    {
      "arxiv_id": "2403.12028v1",
      "title": "Ultraman: Single Image 3D Human Reconstruction with Ultra Speed and Detail",
      "title_zh": "Ultraman：单图像3D人体重建的超高速和高细节方法",
      "authors": [
        "Mingjin Chen",
        "Junhao Chen",
        "Xiaojun Ye",
        "Huan-ang Gao",
        "Xiaoxue Chen",
        "Zhaoxin Fan",
        "Hao Zhao"
      ],
      "abstract": "3D human body reconstruction has been a challenge in the field of computer\nvision. Previous methods are often time-consuming and difficult to capture the\ndetailed appearance of the human body. In this paper, we propose a new method\ncalled \\emph{Ultraman} for fast reconstruction of textured 3D human models from\na single image. Compared to existing techniques, \\emph{Ultraman} greatly\nimproves the reconstruction speed and accuracy while preserving high-quality\ntexture details. We present a set of new frameworks for human reconstruction\nconsisting of three parts, geometric reconstruction, texture generation and\ntexture mapping. Firstly, a mesh reconstruction framework is used, which\naccurately extracts 3D human shapes from a single image. At the same time, we\npropose a method to generate a multi-view consistent image of the human body\nbased on a single image. This is finally combined with a novel texture mapping\nmethod to optimize texture details and ensure color consistency during\nreconstruction. Through extensive experiments and evaluations, we demonstrate\nthe superior performance of \\emph{Ultraman} on various standard datasets. In\naddition, \\emph{Ultraman} outperforms state-of-the-art methods in terms of\nhuman rendering quality and speed. Upon acceptance of the article, we will make\nthe code and data publicly available.",
      "tldr_zh": "本文提出了一种名为Ultraman的新方法，用于从单张图像快速重建带纹理细节的3D human reconstruction模型，与现有技术相比显著提升了速度和准确性。该方法由三个部分组成：几何重建框架用于从图像中提取精确的3D人体形状，纹理生成基于单张图像创建多视图一致图像，以及创新的纹理映射优化细节并确保颜色一致性。通过广泛实验，Ultraman在标准数据集上表现出色，在人体渲染质量和速度方面优于最先进方法。作者计划在文章接受后公开代码和数据。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "eess.IV"
      ],
      "primary_category": "cs.CV",
      "comment": "Project Page: https://air-discover.github.io/Ultraman/",
      "pdf_url": "http://arxiv.org/pdf/2403.12028v1",
      "published_date": "2024-03-18 17:57:30 UTC",
      "updated_date": "2024-03-18 17:57:30 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T16:33:33.843143"
    },
    {
      "arxiv_id": "2403.12027v4",
      "title": "From Pixels to Insights: A Survey on Automatic Chart Understanding in the Era of Large Foundation Models",
      "title_zh": "从像素到洞见：大型基础模型时代自动图表理解的综述",
      "authors": [
        "Kung-Hsiang Huang",
        "Hou Pong Chan",
        "Yi R. Fung",
        "Haoyi Qiu",
        "Mingyang Zhou",
        "Shafiq Joty",
        "Shih-Fu Chang",
        "Heng Ji"
      ],
      "abstract": "Data visualization in the form of charts plays a pivotal role in data\nanalysis, offering critical insights and aiding in informed decision-making.\nAutomatic chart understanding has witnessed significant advancements with the\nrise of large foundation models in recent years. Foundation models, such as\nlarge language models, have revolutionized various natural language processing\ntasks and are increasingly being applied to chart understanding tasks. This\nsurvey paper provides a comprehensive overview of the recent developments,\nchallenges, and future directions in chart understanding within the context of\nthese foundation models. We review fundamental building blocks crucial for\nstudying chart understanding tasks. Additionally, we explore various tasks and\ntheir evaluation metrics and sources of both charts and textual inputs. Various\nmodeling strategies are then examined, encompassing both classification-based\nand generation-based approaches, along with tool augmentation techniques that\nenhance chart understanding performance. Furthermore, we discuss the\nstate-of-the-art performance of each task and discuss how we can improve the\nperformance. Challenges and future directions are addressed, highlighting the\nimportance of several topics, such as domain-specific charts, lack of efforts\nin developing evaluation metrics, and agent-oriented settings. This survey\npaper serves as a comprehensive resource for researchers and practitioners in\nthe fields of natural language processing, computer vision, and data analysis,\nproviding valuable insights and directions for future research in chart\nunderstanding leveraging large foundation models. The studies mentioned in this\npaper, along with emerging new research, will be continually updated at:\nhttps://github.com/khuangaf/Awesome-Chart-Understanding.",
      "tldr_zh": "这篇调查论文系统回顾了在大型基础模型时代（如大型语言模型）的自动图表理解（Automatic Chart Understanding）领域的发展，涵盖了基础构建块、各种任务及其评估指标、数据来源和建模策略，包括分类-based和生成-based方法以及工具增强技术。论文分析了各任务的state-of-the-art性能，讨论了改进潜力，并指出了关键挑战，如处理领域特定图表、开发评估指标的不足以及代理导向设置的未来方向。该研究为自然语言处理、计算机视觉和数据分析领域的从业者提供全面资源，并通过GitHub持续更新相关内容。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.CL",
      "comment": "IEEE Transactions on Knowledge and Data Engineering (TKDE)",
      "pdf_url": "http://arxiv.org/pdf/2403.12027v4",
      "published_date": "2024-03-18 17:57:09 UTC",
      "updated_date": "2024-12-05 03:26:13 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T16:33:45.978546"
    },
    {
      "arxiv_id": "2403.12026v2",
      "title": "FlexCap: Describe Anything in Images in Controllable Detail",
      "title_zh": "FlexCap：以可控细节描述图像中的任何内容",
      "authors": [
        "Debidatta Dwibedi",
        "Vidhi Jain",
        "Jonathan Tompson",
        "Andrew Zisserman",
        "Yusuf Aytar"
      ],
      "abstract": "We introduce FlexCap, a vision-language model that generates region-specific\ndescriptions of varying lengths. FlexCap is trained to produce\nlength-conditioned captions for input boxes, enabling control over information\ndensity, with descriptions ranging from concise object labels to detailed\ncaptions. To achieve this, we create large-scale training datasets of image\nregion descriptions with varying lengths from captioned web images. We\ndemonstrate FlexCap's effectiveness in several applications: first, it achieves\nstrong performance in dense captioning tasks on the Visual Genome dataset.\nSecond, we show how FlexCap's localized descriptions can serve as input to a\nlarge language model to create a visual question answering (VQA) system,\nachieving state-of-the-art zero-shot performance on multiple VQA benchmarks.\nOur experiments illustrate FlexCap's utility for tasks including image\nlabeling, object attribute recognition, and visual dialog. Project webpage:\nhttps://flex-cap.github.io .",
      "tldr_zh": "本研究引入了FlexCap，一种视觉语言模型（vision-language model），能够生成区域特定的图像描述，并通过长度条件控制描述的详细程度，从简短的物体标签到详细的说明。研究团队构建了大规模训练数据集，从网络图像的标题中提取不同长度的区域描述，以训练模型。实验结果显示，FlexCap在Visual Genome数据集的密集描述任务中表现出色，并在零样本视觉问答（VQA）基准上达到最先进水平，同时适用于图像标记、物体属性识别和视觉对话等应用。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CL",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "Accepted at NeurIPS 2024",
      "pdf_url": "http://arxiv.org/pdf/2403.12026v2",
      "published_date": "2024-03-18 17:57:02 UTC",
      "updated_date": "2025-01-28 23:14:21 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T16:33:56.863505"
    },
    {
      "arxiv_id": "2403.12017v1",
      "title": "Supervised Fine-Tuning as Inverse Reinforcement Learning",
      "title_zh": "翻译失败",
      "authors": [
        "Hao Sun"
      ],
      "abstract": "The prevailing approach to aligning Large Language Models (LLMs) typically\nrelies on human or AI feedback and assumes access to specific types of\npreference datasets. In our work, we question the efficacy of such datasets and\nexplore various scenarios where alignment with expert demonstrations proves\nmore realistic. We build a sequential decision-making framework to formulate\nthe problem of aligning LLMs using demonstration datasets. Drawing insights\nfrom inverse reinforcement learning and imitation learning, we introduce\nvarious approaches for divergence minimization in the LLM alignment tasks. Our\nanalysis highlights the mass-covering and mode-seeking behaviors of these\ndifferent approaches. Inclusively, we examine the pros and cons of the\nclassical supervised fine-tuning method, elaborating on scenarios where\ndifferent methods shine.",
      "tldr_zh": "本论文将Supervised Fine-Tuning视为Inverse Reinforcement Learning，质疑传统依赖偏好数据集的LLM（Large Language Models）对齐方法，转而探索使用专家演示数据的更现实方案。作者构建了一个顺序决策框架，将LLM对齐问题形式化为基于演示数据集的任务，并引入逆强化学习和模仿学习方法来最小化差异。分析显示，这些方法表现出质量覆盖和模式寻找的行为，并讨论了经典Supervised Fine-Tuning的优缺点，包括其在特定场景中的优势。总的来说，该研究为LLM对齐提供了新的理论视角和实用指导。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2403.12017v1",
      "published_date": "2024-03-18 17:52:57 UTC",
      "updated_date": "2024-03-18 17:52:57 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T16:34:08.448078"
    },
    {
      "arxiv_id": "2403.12014v2",
      "title": "EnvGen: Generating and Adapting Environments via LLMs for Training Embodied Agents",
      "title_zh": "EnvGen：利用 LLMs 生成和适应环境以训练具身代理",
      "authors": [
        "Abhay Zala",
        "Jaemin Cho",
        "Han Lin",
        "Jaehong Yoon",
        "Mohit Bansal"
      ],
      "abstract": "Recent SOTA approaches for embodied learning via interaction directly employ\nlarge language models (LLMs) as agents to determine the next steps in an\nenvironment. Due to their world knowledge and reasoning capabilities, LLM\nagents achieve stronger performance than previous smaller agents based on\nreinforcement learning (RL); however, frequently calling LLMs is slow and\nexpensive. Instead of directly employing LLMs as agents, can we use LLMs'\nreasoning capabilities to adaptively create training environments to help\nsmaller RL agents learn useful skills that they are weak at? We propose EnvGen,\na novel framework to address this question. We first prompt an LLM to generate\ntraining environments by giving it the task description and simulator\nobjectives that the agents should learn and then asking it to generate a set of\nenvironment configurations (e.g., different terrains, items initially given to\nagents, etc.). Next, we train a small RL agent in a mixture of the original and\nLLM-generated environments. Then, we enable the LLM to continuously adapt the\ngenerated environments to progressively improve the skills that the agent is\nweak at, by providing feedback to the LLM in the form of the agent's\nperformance. We demonstrate the usefulness of EnvGen with comprehensive\nexperiments in Crafter and Heist environments. We find that a small RL agent\ntrained with EnvGen can outperform SOTA methods, including a GPT-4 agent, and\nlearns long-horizon tasks significantly faster. We also show that using an LLM\nto adapt environments dynamically outperforms curriculum learning approaches\nand how the environments are adapted to help improve RL agents' weaker skills\nover time. Additionally, EnvGen is substantially more efficient as it only uses\na small number of LLM calls (e.g., 4 in total), whereas LLM agents require\nthousands of calls. Lastly, we present detailed ablation studies for EnvGen\ndesign choices.",
      "tldr_zh": "本文提出 EnvGen 框架，利用 LLMs 的推理能力生成和适应训练环境，帮助小型 RL 代理学习弱项技能。具体方法包括：提示 LLMs 创建环境配置（如不同地形和物品）、在混合环境中训练 RL 代理，并通过代理性能反馈动态调整环境。实验在 Crafter 和 Heist 环境中显示，EnvGen 训练的代理超过了 SOTA 方法（如 GPT-4 代理），学习长时任务更快，且仅需少量 LLM 调用（例如 4 次），远比直接使用 LLMs 作为代理更高效。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "COLM 2024; First two authors contributed equally; Project website:\n  https://envgen-llm.github.io/",
      "pdf_url": "http://arxiv.org/pdf/2403.12014v2",
      "published_date": "2024-03-18 17:51:16 UTC",
      "updated_date": "2024-07-12 17:39:19 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T16:34:23.714859"
    },
    {
      "arxiv_id": "2403.12010v1",
      "title": "VideoMV: Consistent Multi-View Generation Based on Large Video Generative Model",
      "title_zh": "翻译失败",
      "authors": [
        "Qi Zuo",
        "Xiaodong Gu",
        "Lingteng Qiu",
        "Yuan Dong",
        "Zhengyi Zhao",
        "Weihao Yuan",
        "Rui Peng",
        "Siyu Zhu",
        "Zilong Dong",
        "Liefeng Bo",
        "Qixing Huang"
      ],
      "abstract": "Generating multi-view images based on text or single-image prompts is a\ncritical capability for the creation of 3D content. Two fundamental questions\non this topic are what data we use for training and how to ensure multi-view\nconsistency. This paper introduces a novel framework that makes fundamental\ncontributions to both questions. Unlike leveraging images from 2D diffusion\nmodels for training, we propose a dense consistent multi-view generation model\nthat is fine-tuned from off-the-shelf video generative models. Images from\nvideo generative models are more suitable for multi-view generation because the\nunderlying network architecture that generates them employs a temporal module\nto enforce frame consistency. Moreover, the video data sets used to train these\nmodels are abundant and diverse, leading to a reduced train-finetuning domain\ngap. To enhance multi-view consistency, we introduce a 3D-Aware Denoising\nSampling, which first employs a feed-forward reconstruction module to get an\nexplicit global 3D model, and then adopts a sampling strategy that effectively\ninvolves images rendered from the global 3D model into the denoising sampling\nloop to improve the multi-view consistency of the final images. As a\nby-product, this module also provides a fast way to create 3D assets\nrepresented by 3D Gaussians within a few seconds. Our approach can generate 24\ndense views and converges much faster in training than state-of-the-art\napproaches (4 GPU hours versus many thousand GPU hours) with comparable visual\nquality and consistency. By further fine-tuning, our approach outperforms\nexisting state-of-the-art methods in both quantitative metrics and visual\neffects. Our project page is aigc3d.github.io/VideoMV.",
      "tldr_zh": "这篇论文提出了VideoMV框架，利用大型视频生成模型进行一致的多视图图像生成，以支持3D内容创建。该框架从现成的视频生成模型微调，而不是依赖2D扩散模型图像，从而利用视频模型的时间模块确保帧一致性，并减少训练域差距。为了提升多视图一致性，引入了3D-Aware Denoising Sampling方法，通过前向重建模块获取全局3D模型，并在去噪采样中融入渲染图像，实现快速生成24个密集视图。实验显示，该方法训练效率极高（仅需4 GPU hours），视觉质量和一致性与最先进方法相当，且进一步微调后在定量指标和视觉效果上超越现有SOTA基准。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.GR"
      ],
      "primary_category": "cs.CV",
      "comment": "Project page: aigc3d.github.io/VideoMV/",
      "pdf_url": "http://arxiv.org/pdf/2403.12010v1",
      "published_date": "2024-03-18 17:48:15 UTC",
      "updated_date": "2024-03-18 17:48:15 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T16:34:37.025602"
    },
    {
      "arxiv_id": "2403.12009v2",
      "title": "Leveraging Spatial and Semantic Feature Extraction for Skin Cancer Diagnosis with Capsule Networks and Graph Neural Networks",
      "title_zh": "翻译失败",
      "authors": [
        "K. P. Santoso",
        "R. V. H. Ginardi",
        "R. A. Sastrowardoyo",
        "F. A. Madany"
      ],
      "abstract": "In the realm of skin lesion image classification, the intricate spatial and\nsemantic features pose significant challenges for conventional Convolutional\nNeural Network (CNN)-based methodologies. These challenges are compounded by\nthe imbalanced nature of skin lesion datasets, which hampers the ability of\nmodels to learn minority class features effectively. Despite augmentation\nstrategies, such as those using Generative Adversarial Networks (GANs),\nprevious attempts have not fully addressed these complexities. This study\nintroduces an innovative approach by integrating Graph Neural Networks (GNNs)\nwith Capsule Networks to enhance classification performance. GNNs, known for\ntheir proficiency in handling graph-structured data, offer an advanced\nmechanism for capturing complex patterns and relationships beyond the\ncapabilities of traditional CNNs. Capsule Networks further contribute by\nproviding superior recognition of spatial hierarchies within images. Our\nresearch focuses on evaluating and enhancing the Tiny Pyramid Vision GNN (Tiny\nPyramid ViG) architecture by incorporating it with a Capsule Network. This\nhybrid model was applied to the MNIST:HAM10000 dataset, a comprehensive skin\nlesion dataset designed for benchmarking classification models. After 75 epochs\nof training, our model achieved a significant accuracy improvement, reaching\n89.23% and 95.52%, surpassing established benchmarks such as GoogLeNet\n(83.94%), InceptionV3 (86.82%), MobileNet V3 (89.87%), EfficientNet-B7\n(92.07%), ResNet18 (92.22%), ResNet34 (91.90%), ViT-Base (73.70%), and IRv2-SA\n(93.47%) on the same dataset. This outcome underscores the potential of our\napproach in overcoming the inherent challenges of skin lesion classification,\ncontributing to the advancement of image-based diagnosis in dermatology.",
      "tldr_zh": "该研究针对皮肤病变图像分类面临的复杂空间和语义特征挑战，以及数据集不平衡问题，提出了一种创新方法，将Graph Neural Networks (GNNs)与Capsule Networks整合，以捕捉图像中的复杂模式和空间层次。\n具体而言，他们增强了Tiny Pyramid Vision GNN (Tiny Pyramid ViG)架构，并将其与Capsule Network结合，应用于MNIST:HAM10000数据集，经过75个训练周期的优化。\n实验结果显示，该混合模型的准确率达到89.23%和95.52%，超过了基准模型如GoogLeNet (83.94%)、InceptionV3 (86.82%)等。\n这一进展有助于克服传统CNN方法的局限性，推动皮肤病诊断领域的图像分类技术发展。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "This is the first version of our paper, we gladly expect feedback and\n  corrections if there is any mistake within our paper",
      "pdf_url": "http://arxiv.org/pdf/2403.12009v2",
      "published_date": "2024-03-18 17:47:39 UTC",
      "updated_date": "2024-03-19 07:11:28 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T16:34:51.075887"
    },
    {
      "arxiv_id": "2403.12002v2",
      "title": "DreamMotion: Space-Time Self-Similar Score Distillation for Zero-Shot Video Editing",
      "title_zh": "DreamMotion: 用于零样本视频编辑的时空自相似分数蒸",
      "authors": [
        "Hyeonho Jeong",
        "Jinho Chang",
        "Geon Yeong Park",
        "Jong Chul Ye"
      ],
      "abstract": "Text-driven diffusion-based video editing presents a unique challenge not\nencountered in image editing literature: establishing real-world motion. Unlike\nexisting video editing approaches, here we focus on score distillation sampling\nto circumvent the standard reverse diffusion process and initiate optimization\nfrom videos that already exhibit natural motion. Our analysis reveals that\nwhile video score distillation can effectively introduce new content indicated\nby target text, it can also cause significant structure and motion deviation.\nTo counteract this, we propose to match space-time self-similarities of the\noriginal video and the edited video during the score distillation. Thanks to\nthe use of score distillation, our approach is model-agnostic, which can be\napplied for both cascaded and non-cascaded video diffusion frameworks. Through\nextensive comparisons with leading methods, our approach demonstrates its\nsuperiority in altering appearances while accurately preserving the original\nstructure and motion.",
      "tldr_zh": "这篇论文提出了DreamMotion，一种基于score distillation sampling的零样本视频编辑方法，旨在解决文本驱动扩散模型在建立真实运动方面的挑战。不同于现有方法，该技术通过匹配原始视频和编辑视频的空间-time self-similarities来减少结构和运动偏差，同时保持模型无关性，可应用于级联和非级联视频扩散框架。实验结果表明，DreamMotion在改变视频外观的同时，准确保留了原始结构和运动，并显著优于领先方法。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "Accepted to ECCV 2024, Project page:\n  https://hyeonho99.github.io/dreammotion/",
      "pdf_url": "http://arxiv.org/pdf/2403.12002v2",
      "published_date": "2024-03-18 17:38:53 UTC",
      "updated_date": "2024-07-15 13:34:29 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T16:35:00.459623"
    },
    {
      "arxiv_id": "2403.12000v1",
      "title": "Notochord: a Flexible Probabilistic Model for Real-Time MIDI Performance",
      "title_zh": "翻译失败",
      "authors": [
        "Victor Shepardson",
        "Jack Armitage",
        "Thor Magnusson"
      ],
      "abstract": "Deep learning-based probabilistic models of musical data are producing\nincreasingly realistic results and promise to enter creative workflows of many\nkinds. Yet they have been little-studied in a performance setting, where the\nresults of user actions typically ought to feel instantaneous. To enable such\nstudy, we designed Notochord, a deep probabilistic model for sequences of\nstructured events, and trained an instance of it on the Lakh MIDI dataset. Our\nprobabilistic formulation allows interpretable interventions at a sub-event\nlevel, which enables one model to act as a backbone for diverse interactive\nmusical functions including steerable generation, harmonization, machine\nimprovisation, and likelihood-based interfaces. Notochord can generate\npolyphonic and multi-track MIDI, and respond to inputs with latency below ten\nmilliseconds. Training code, model checkpoints and interactive examples are\nprovided as open source software.",
      "tldr_zh": "本文提出Notochord，这是一个基于深度学习的probabilistic model，用于实时MIDI性能，旨在为音乐创作流程提供即时响应。模型在Lakh MIDI数据集上训练，支持在sub-event级别进行可解释干预，从而实现多样互动功能，如steerable generation、harmonization、机器即兴和基于似然的接口。实验结果显示，Notochord可生成多声部和多轨道的MIDI，并以低于10毫秒的延迟响应输入，同时提供开源软件包括训练代码、模型检查点和互动示例。",
      "categories": [
        "cs.SD",
        "cs.AI",
        "eess.AS"
      ],
      "primary_category": "cs.SD",
      "comment": "12 pages, 6 figures. Proceedings of the 3rd Conference on AI Music\n  Creativity (2022, September 17)",
      "pdf_url": "http://arxiv.org/pdf/2403.12000v1",
      "published_date": "2024-03-18 17:35:02 UTC",
      "updated_date": "2024-03-18 17:35:02 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T16:35:13.157982"
    },
    {
      "arxiv_id": "2403.11996v3",
      "title": "Accelerating Scientific Discovery with Generative Knowledge Extraction, Graph-Based Representation, and Multimodal Intelligent Graph Reasoning",
      "title_zh": "翻译失败",
      "authors": [
        "Markus J. Buehler"
      ],
      "abstract": "Leveraging generative Artificial Intelligence (AI), we have transformed a\ndataset comprising 1,000 scientific papers into an ontological knowledge graph.\nThrough an in-depth structural analysis, we have calculated node degrees,\nidentified communities and connectivities, and evaluated clustering\ncoefficients and betweenness centrality of pivotal nodes, uncovering\nfascinating knowledge architectures. The graph has an inherently scale-free\nnature, is highly connected, and can be used for graph reasoning by taking\nadvantage of transitive and isomorphic properties that reveal unprecedented\ninterdisciplinary relationships that can be used to answer queries, identify\ngaps in knowledge, propose never-before-seen material designs, and predict\nmaterial behaviors. We compute deep node embeddings for combinatorial node\nsimilarity ranking for use in a path sampling strategy links dissimilar\nconcepts that have previously not been related. One comparison revealed\nstructural parallels between biological materials and Beethoven's 9th Symphony,\nhighlighting shared patterns of complexity through isomorphic mapping. In\nanother example, the algorithm proposed a hierarchical mycelium-based composite\nbased on integrating path sampling with principles extracted from Kandinsky's\n'Composition VII' painting. The resulting material integrates an innovative set\nof concepts that include a balance of chaos/order, adjustable porosity,\nmechanical strength, and complex patterned chemical functionalization. We\nuncover other isomorphisms across science, technology and art, revealing a\nnuanced ontology of immanence that reveal a context-dependent heterarchical\ninterplay of constituents. Graph-based generative AI achieves a far higher\ndegree of novelty, explorative capacity, and technical detail, than\nconventional approaches and establishes a widely useful framework for\ninnovation by revealing hidden connections.",
      "tldr_zh": "本研究利用生成式人工智能(Generative AI)将1000篇科学论文转化为本体知识图，通过结构分析计算节点度、社区连通性、聚类系数和节点中心性，揭示了知识图的规模自由和高连通特性。研究采用图推理技术，包括传递性和同构属性，来发现跨学科关系，支持查询回答、知识空白识别、新材料设计和材料行为预测。利用深度节点嵌入和路径采样策略，该框架将不相似的概念联系起来，如将生物材料与贝多芬第九交响曲或康定斯基画作进行同构映射，提出创新材料如分层菌丝复合。相较于传统方法，该方法显著提升了创新性和探索能力，为加速科学发现提供了一个高效框架。",
      "categories": [
        "cs.LG",
        "cond-mat.mes-hall",
        "cond-mat.mtrl-sci",
        "cond-mat.soft",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2403.11996v3",
      "published_date": "2024-03-18 17:30:27 UTC",
      "updated_date": "2024-06-10 19:06:26 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T16:35:24.904757"
    },
    {
      "arxiv_id": "2403.11984v1",
      "title": "Using Generative Text Models to Create Qualitative Codebooks for Student Evaluations of Teaching",
      "title_zh": "翻译失败",
      "authors": [
        "Andrew Katz",
        "Mitchell Gerhardt",
        "Michelle Soledad"
      ],
      "abstract": "Feedback is a critical aspect of improvement. Unfortunately, when there is a\nlot of feedback from multiple sources, it can be difficult to distill the\ninformation into actionable insights. Consider student evaluations of teaching\n(SETs), which are important sources of feedback for educators. They can give\ninstructors insights into what worked during a semester. A collection of SETs\ncan also be useful to administrators as signals for courses or entire programs.\nHowever, on a large scale as in high-enrollment courses or administrative\nrecords over several years, the volume of SETs can render them difficult to\nanalyze. In this paper, we discuss a novel method for analyzing SETs using\nnatural language processing (NLP) and large language models (LLMs). We\ndemonstrate the method by applying it to a corpus of 5,000 SETs from a large\npublic university. We show that the method can be used to extract, embed,\ncluster, and summarize the SETs to identify the themes they express. More\ngenerally, this work illustrates how to use the combination of NLP techniques\nand LLMs to generate a codebook for SETs. We conclude by discussing the\nimplications of this method for analyzing SETs and other types of student\nwriting in teaching and research settings.",
      "tldr_zh": "本文提出了一种新方法，使用自然语言处理 (NLP) 和大型语言模型 (LLMs) 来分析学生对教学评价 (SETs)，旨在从大量反馈中提炼可行动洞见。该方法涉及提取、嵌入、聚类和总结 5,000 个 SETs 的语料库，以自动生成定性代码书并识别反馈主题。结果表明，此方法有效提升了分析效率，并为教学和研究中的学生写作分析提供了更广泛的应用潜力。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.HC"
      ],
      "primary_category": "cs.CL",
      "comment": "Natural language processing, large language models, generative AI,\n  student evaluations of teaching, codebook generation, qualitative data\n  analysis",
      "pdf_url": "http://arxiv.org/pdf/2403.11984v1",
      "published_date": "2024-03-18 17:21:35 UTC",
      "updated_date": "2024-03-18 17:21:35 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T16:35:37.327928"
    },
    {
      "arxiv_id": "2403.15452v1",
      "title": "What Are Tools Anyway? A Survey from the Language Model Perspective",
      "title_zh": "翻译失败",
      "authors": [
        "Zhiruo Wang",
        "Zhoujun Cheng",
        "Hao Zhu",
        "Daniel Fried",
        "Graham Neubig"
      ],
      "abstract": "Language models (LMs) are powerful yet mostly for text generation tasks.\nTools have substantially enhanced their performance for tasks that require\ncomplex skills. However, many works adopt the term \"tool\" in different ways,\nraising the question: What is a tool anyway? Subsequently, where and how do\ntools help LMs? In this survey, we provide a unified definition of tools as\nexternal programs used by LMs, and perform a systematic review of LM tooling\nscenarios and approaches. Grounded on this review, we empirically study the\nefficiency of various tooling methods by measuring their required compute and\nperformance gains on various benchmarks, and highlight some challenges and\npotential future research in the field.",
      "tldr_zh": "这篇论文从语言模型 (LMs) 的视角审视了“工具”的概念，并统一定义工具为 LMs 使用的外部程序，以帮助处理需要复杂技能的任务。作者系统回顾了 LM 工具化的各种场景和方法，包括工具如何提升模型性能，并通过实证研究评估了不同工具方法的计算效率和在基准测试上的性能提升。研究结果突出了工具在实际应用中的挑战，如计算开销和兼容性问题，并指出了潜在的未来研究方向。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2403.15452v1",
      "published_date": "2024-03-18 17:20:07 UTC",
      "updated_date": "2024-03-18 17:20:07 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T16:35:49.234699"
    },
    {
      "arxiv_id": "2403.11966v1",
      "title": "Informed Spectral Normalized Gaussian Processes for Trajectory Prediction",
      "title_zh": "翻译失败",
      "authors": [
        "Christian Schlauch",
        "Christian Wirth",
        "Nadja Klein"
      ],
      "abstract": "Prior parameter distributions provide an elegant way to represent prior\nexpert and world knowledge for informed learning. Previous work has shown that\nusing such informative priors to regularize probabilistic deep learning (DL)\nmodels increases their performance and data-efficiency. However, commonly used\nsampling-based approximations for probabilistic DL models can be\ncomputationally expensive, requiring multiple inference passes and longer\ntraining times. Promising alternatives are compute-efficient last layer kernel\napproximations like spectral normalized Gaussian processes (SNGPs). We propose\na novel regularization-based continual learning method for SNGPs, which enables\nthe use of informative priors that represent prior knowledge learned from\nprevious tasks. Our proposal builds upon well-established methods and requires\nno rehearsal memory or parameter expansion. We apply our informed SNGP model to\nthe trajectory prediction problem in autonomous driving by integrating prior\ndrivability knowledge. On two public datasets, we investigate its performance\nunder diminishing training data and across locations, and thereby demonstrate\nan increase in data-efficiency and robustness to location-transfers over\nnon-informed and informed baselines.",
      "tldr_zh": "本研究提出了一种新型的 Informed Spectral Normalized Gaussian Processes (SNGPs) 方法，通过整合信息先验参数分布来提升轨迹预测模型的性能和数据效率。该方法采用基于正则化的持续学习框架，无需回放记忆或参数扩展，从而减少计算开销，并将先验知识应用于自主驾驶场景。在两个公共数据集上进行的实验显示，与非信息和信息基线模型相比，Informed SNGPs 在训练数据减少和跨位置转移时表现出更高的鲁棒性和数据效率。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2403.11966v1",
      "published_date": "2024-03-18 17:05:24 UTC",
      "updated_date": "2024-03-18 17:05:24 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T16:36:01.367701"
    },
    {
      "arxiv_id": "2403.14712v1",
      "title": "AI for bureaucratic productivity: Measuring the potential of AI to help automate 143 million UK government transactions",
      "title_zh": "翻译失败",
      "authors": [
        "Vincent J. Straub",
        "Youmna Hashem",
        "Jonathan Bright",
        "Satyam Bhagwanani",
        "Deborah Morgan",
        "John Francis",
        "Saba Esnaashari",
        "Helen Margetts"
      ],
      "abstract": "There is currently considerable excitement within government about the\npotential of artificial intelligence to improve public service productivity\nthrough the automation of complex but repetitive bureaucratic tasks, freeing up\nthe time of skilled staff. Here, we explore the size of this opportunity, by\nmapping out the scale of citizen-facing bureaucratic decision-making procedures\nwithin UK central government, and measuring their potential for AI-driven\nautomation. We estimate that UK central government conducts approximately one\nbillion citizen-facing transactions per year in the provision of around 400\nservices, of which approximately 143 million are complex repetitive\ntransactions. We estimate that 84% of these complex transactions are highly\nautomatable, representing a huge potential opportunity: saving even an average\nof just one minute per complex transaction would save the equivalent of\napproximately 1,200 person-years of work every year. We also develop a model to\nestimate the volume of transactions a government service undertakes, providing\na way for government to avoid conducting time consuming transaction volume\nmeasurements. Finally, we find that there is high turnover in the types of\nservices government provide, meaning that automation efforts should focus on\ngeneral procedures rather than services themselves which are likely to evolve\nover time. Overall, our work presents a novel perspective on the structure and\nfunctioning of modern government, and how it might evolve in the age of\nartificial intelligence.",
      "tldr_zh": "本研究评估了 AI 在自动化 UK 中央政府 1.43 亿复杂重复交易中的潜力，发现这些交易中有 84% 高度可自动化，如果每笔交易节省一分钟，将相当于每年节省 1200 人年的工作时间。研究者通过映射政府决策程序和估算交易规模，开发了一个模型来快速评估交易量，从而避免了繁琐的测量过程。他们还发现政府服务的类型具有高周转率，因此建议将自动化努力集中在一般程序上，以适应未来的演变，这为现代政府在 AI 时代优化结构提供了新视角。",
      "categories": [
        "cs.CY",
        "cs.AI"
      ],
      "primary_category": "cs.CY",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2403.14712v1",
      "published_date": "2024-03-18 17:03:17 UTC",
      "updated_date": "2024-03-18 17:03:17 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T16:36:12.928879"
    },
    {
      "arxiv_id": "2403.11961v1",
      "title": "Enhanced Event-Based Video Reconstruction with Motion Compensation",
      "title_zh": "通过运动补偿增强的事件驱动视频重建",
      "authors": [
        "Siying Liu",
        "Pier Luigi Dragotti"
      ],
      "abstract": "Deep neural networks for event-based video reconstruction often suffer from a\nlack of interpretability and have high memory demands. A lightweight network\ncalled CISTA-LSTC has recently been introduced showing that high-quality\nreconstruction can be achieved through the systematic design of its\narchitecture. However, its modelling assumption that input signals and output\nreconstructed frame share the same sparse representation neglects the\ndisplacement caused by motion. To address this, we propose warping the input\nintensity frames and sparse codes to enhance reconstruction quality. A\nCISTA-Flow network is constructed by integrating a flow network with CISTA-LSTC\nfor motion compensation. The system relies solely on events, in which predicted\nflow aids in reconstruction and then reconstructed frames are used to\nfacilitate flow estimation. We also introduce an iterative training framework\nfor this combined system. Results demonstrate that our approach achieves\nstate-of-the-art reconstruction accuracy and simultaneously provides reliable\ndense flow estimation. Furthermore, our model exhibits flexibility in that it\ncan integrate different flow networks, suggesting its potential for further\nperformance enhancement.",
      "tldr_zh": "本文提出了一种增强基于事件的视频重建方法，以解决现有深度神经网络在运动补偿方面的不足，特别是 CISTA-LSTC 模型忽略运动引起的位移问题。研究团队构建了 CISTA-Flow 网络，将 flow network 与 CISTA-LSTC 整合，通过扭曲输入强度帧和稀疏代码来提高重建质量，该系统仅依赖事件数据并采用迭代训练框架。实验结果显示，该方法实现了 state-of-the-art 的重建准确性，同时提供了可靠的密集流估计。模型还具有灵活性，可整合不同 flow network 以进一步提升性能。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "22 pages, 8 figures (supplementary material included)",
      "pdf_url": "http://arxiv.org/pdf/2403.11961v1",
      "published_date": "2024-03-18 16:58:23 UTC",
      "updated_date": "2024-03-18 16:58:23 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T16:36:25.357271"
    },
    {
      "arxiv_id": "2403.11959v2",
      "title": "IVAC-P2L: Leveraging Irregular Repetition Priors for Improving Video Action Counting",
      "title_zh": "IVAC-P2L：利用不规则重复先验知识改进视频动作计数",
      "authors": [
        "Hang Wang",
        "Zhi-Qi Cheng",
        "Youtian Du",
        "Lei Zhang"
      ],
      "abstract": "Video Action Counting (VAC) is crucial in analyzing sports, fitness, and\neveryday activities by quantifying repetitive actions in videos. However,\ntraditional VAC methods have overlooked the complexity of action repetitions,\nsuch as interruptions and the variability in cycle duration. Our research\naddresses the shortfall by introducing a novel approach to VAC, called\nIrregular Video Action Counting (IVAC). IVAC prioritizes modeling irregular\nrepetition patterns in videos, which we define through two primary aspects:\nInter-cycle Consistency and Cycle-interval Inconsistency. Inter-cycle\nConsistency ensures homogeneity in the spatial-temporal representations of\ncycle segments, signifying action uniformity within cycles. Cycle-interval\ninconsistency highlights the importance of distinguishing between cycle\nsegments and intervals based on their inherent content differences. To\nencapsulate these principles, we propose a new methodology that includes\nconsistency and inconsistency modules, supported by a unique pull-push loss\n(P2L) mechanism. The IVAC-P2L model applies a pull loss to promote coherence\namong cycle segment features and a push loss to clearly distinguish features of\ncycle segments from interval segments. Empirical evaluations conducted on the\nRepCount dataset demonstrate that the IVAC-P2L model sets a new benchmark in\nVAC task performance. Furthermore, the model demonstrates exceptional\nadaptability and generalization across various video contents, outperforming\nexisting models on two additional datasets, UCFRep and Countix, without the\nneed for dataset-specific optimization. These results confirm the efficacy of\nour approach in addressing irregular repetitions in videos and pave the way for\nfurther advancements in video analysis and understanding.",
      "tldr_zh": "该研究针对视频动作计数（VAC）的传统方法忽略动作重复的复杂性（如中断和周期变异），提出了一种新型框架IVAC（Irregular Video Action Counting）。IVAC通过Inter-cycle Consistency（确保周期段的空间-时间表示均匀）和Cycle-interval Inconsistency（区分周期段与间隔段）的原则，引入一致性与不一致性模块，并采用独特的pull-push loss (P2L)机制，其中pull loss促进周期段特征的一致性，push loss则强化特征间的差异。实验在RepCount数据集上，IVAC-P2L模型显著提升了性能基准，并在UCFRep和Countix数据集上展现出优秀的适应性和泛化能力，无需特定优化，从而为处理视频中不规则重复模式提供了新途径。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.MM"
      ],
      "primary_category": "cs.CV",
      "comment": "Source code: https://github.com/hwang-cs-ime/IVAC-P2L",
      "pdf_url": "http://arxiv.org/pdf/2403.11959v2",
      "published_date": "2024-03-18 16:56:47 UTC",
      "updated_date": "2024-03-20 11:58:23 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T16:36:38.871951"
    },
    {
      "arxiv_id": "2404.07950v3",
      "title": "Reinforcement Learning with Generalizable Gaussian Splatting",
      "title_zh": "翻译失败",
      "authors": [
        "Jiaxu Wang",
        "Qiang Zhang",
        "Jingkai Sun",
        "Jiahang Cao",
        "Gang Han",
        "Wen Zhao",
        "Weining Zhang",
        "Yecheng Shao",
        "Yijie Guo",
        "Renjing Xu"
      ],
      "abstract": "An excellent representation is crucial for reinforcement learning (RL)\nperformance, especially in vision-based reinforcement learning tasks. The\nquality of the environment representation directly influences the achievement\nof the learning task. Previous vision-based RL typically uses explicit or\nimplicit ways to represent environments, such as images, points, voxels, and\nneural radiance fields. However, these representations contain several\ndrawbacks. They cannot either describe complex local geometries or generalize\nwell to unseen scenes, or require precise foreground masks. Moreover, these\nimplicit neural representations are akin to a ``black box\", significantly\nhindering interpretability. 3D Gaussian Splatting (3DGS), with its explicit\nscene representation and differentiable rendering nature, is considered a\nrevolutionary change for reconstruction and representation methods. In this\npaper, we propose a novel Generalizable Gaussian Splatting framework to be the\nrepresentation of RL tasks, called GSRL. Through validation in the RoboMimic\nenvironment, our method achieves better results than other baselines in\nmultiple tasks, improving the performance by 10%, 44%, and 15% compared with\nbaselines on the hardest task. This work is the first attempt to leverage\ngeneralizable 3DGS as a representation for RL.",
      "tldr_zh": "本论文提出了一种名为 GSRL 的新型框架，将 Generalizable Gaussian Splatting 用作强化学习（RL）任务的环境表示，以解决传统视觉 RL 方法（如图像、点、voxels 或神经辐射场）在描述复杂几何、泛化性和可解释性方面的缺点。GSRL 利用 3D Gaussian Splatting 的显式场景表示和可微渲染特性，实现了对未知场景的更好适应和精确重建。在 RoboMimic 环境中的实验验证中，GSRL 比基线模型在多个任务上表现出色，在最难任务上提升了 10%、44% 和 15% 的性能，这也是首次将 generalizable 3DGS 应用于 RL 表示。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "7 pages,2 figures",
      "pdf_url": "http://arxiv.org/pdf/2404.07950v3",
      "published_date": "2024-03-18 16:50:23 UTC",
      "updated_date": "2024-08-06 02:42:32 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T16:36:51.730280"
    },
    {
      "arxiv_id": "2403.11942v2",
      "title": "Exploring Facial Expression Recognition through Semi-Supervised Pretraining and Temporal Modeling",
      "title_zh": "通过半监督预训练和",
      "authors": [
        "Jun Yu",
        "Zhihong Wei",
        "Zhongpeng Cai",
        "Gongpeng Zhao",
        "Zerui Zhang",
        "Yongqi Wang",
        "Guochen Xie",
        "Jichao Zhu",
        "Wangyuan Zhu"
      ],
      "abstract": "Facial Expression Recognition (FER) plays a crucial role in computer vision\nand finds extensive applications across various fields. This paper aims to\npresent our approach for the upcoming 6th Affective Behavior Analysis\nin-the-Wild (ABAW) competition, scheduled to be held at CVPR2024. In the facial\nexpression recognition task, The limited size of the FER dataset poses a\nchallenge to the expression recognition model's generalization ability,\nresulting in subpar recognition performance. To address this problem, we employ\na semi-supervised learning technique to generate expression category\npseudo-labels for unlabeled face data. At the same time, we uniformly sampled\nthe labeled facial expression samples and implemented a debiased feedback\nlearning strategy to address the problem of category imbalance in the dataset\nand the possible data bias in semi-supervised learning. Moreover, to further\ncompensate for the limitation and bias of features obtained only from static\nimages, we introduced a Temporal Encoder to learn and capture temporal\nrelationships between neighbouring expression image features. In the 6th ABAW\ncompetition, our method achieved outstanding results on the official validation\nset, a result that fully confirms the effectiveness and competitiveness of our\nproposed method.",
      "tldr_zh": "本论文探讨了通过半监督预训练和时间建模来提升Facial Expression Recognition (FER)的性能，针对FER数据集规模有限导致的泛化问题。该方法采用semi-supervised learning为无标签面部数据生成伪标签，并结合debiased feedback learning策略来缓解类别不平衡和数据偏差。此外，引入Temporal Encoder来捕捉相邻表情图像特征之间的时间关系。在第6届ABAW比赛的官方验证集上，该方法取得了出色结果，证明了其有效性和竞争力。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2403.11942v2",
      "published_date": "2024-03-18 16:36:54 UTC",
      "updated_date": "2024-03-19 17:20:59 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T16:37:01.743787"
    },
    {
      "arxiv_id": "2403.11905v4",
      "title": "Tur[k]ingBench: A Challenge Benchmark for Web Agents",
      "title_zh": "翻译失败",
      "authors": [
        "Kevin Xu",
        "Yeganeh Kordi",
        "Tanay Nayak",
        "Adi Asija",
        "Yizhong Wang",
        "Kate Sanders",
        "Adam Byerly",
        "Jingyu Zhang",
        "Benjamin Van Durme",
        "Daniel Khashabi"
      ],
      "abstract": "Can advanced multi-modal models effectively tackle complex web-based tasks?\nSuch tasks are often found on crowdsourcing platforms, where crowdworkers\nengage in challenging micro-tasks within web-based environments.\n  Building on this idea, we present TurkingBench, a benchmark consisting of\ntasks presented as web pages with textual instructions and multi-modal\ncontexts. Unlike previous approaches that rely on artificially synthesized web\npages, our benchmark uses natural HTML pages originally designed for\ncrowdsourcing workers to perform various annotation tasks. Each task's HTML\ninstructions are instantiated with different values derived from crowdsourcing\ntasks, creating diverse instances. This benchmark includes 32.2K instances\nspread across 158 tasks.\n  To support the evaluation of TurkingBench, we have developed a framework that\nlinks chatbot responses to actions on web pages (e.g., modifying a text box,\nselecting a radio button). We assess the performance of cutting-edge private\nand open-source models, including language-only and vision-language models\n(such as GPT4 and InternVL), on this benchmark. Our results show that while\nthese models outperform random chance, there is still significant room for\nimprovement. We hope that this benchmark will drive progress in the evaluation\nand development of web-based agents.",
      "tldr_zh": "该研究引入了TurkingBench，一种挑战性基准测试，用于评估多模态模型在处理复杂网络任务（如众包平台的微任务）方面的能力。不同于以往依赖人工合成的网页，TurkingBench利用真实的HTML页面和文本指令，结合众包任务的不同值实例化，创建了跨越158个任务的32.2K个多样化实例。研究团队开发了一个框架，将聊天机器人响应链接到网页操作（如修改文本框或选择单选按钮），并测试了先进模型如GPT-4和InternVL，结果显示这些模型性能优于随机猜测，但仍有显著改进空间。该基准测试旨在推动网络代理的评估和开发。",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.CV",
        "cs.HC"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2403.11905v4",
      "published_date": "2024-03-18 16:06:30 UTC",
      "updated_date": "2025-02-22 01:33:54 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T16:37:14.505670"
    },
    {
      "arxiv_id": "2403.11901v4",
      "title": "Larimar: Large Language Models with Episodic Memory Control",
      "title_zh": "Larimar：带有情节记忆控制的大型语言模型",
      "authors": [
        "Payel Das",
        "Subhajit Chaudhury",
        "Elliot Nelson",
        "Igor Melnyk",
        "Sarath Swaminathan",
        "Sihui Dai",
        "Aurélie Lozano",
        "Georgios Kollias",
        "Vijil Chenthamarakshan",
        "Jiří",
        "Navrátil",
        "Soham Dan",
        "Pin-Yu Chen"
      ],
      "abstract": "Efficient and accurate updating of knowledge stored in Large Language Models\n(LLMs) is one of the most pressing research challenges today. This paper\npresents Larimar - a novel, brain-inspired architecture for enhancing LLMs with\na distributed episodic memory. Larimar's memory allows for dynamic, one-shot\nupdates of knowledge without the need for computationally expensive re-training\nor fine-tuning. Experimental results on multiple fact editing benchmarks\ndemonstrate that Larimar attains accuracy comparable to most competitive\nbaselines, even in the challenging sequential editing setup, but also excels in\nspeed - yielding speed-ups of 8-10x depending on the base LLM - as well as\nflexibility due to the proposed architecture being simple, LLM-agnostic, and\nhence general. We further provide mechanisms for selective fact forgetting,\ninformation leakage prevention, and input context length generalization with\nLarimar and show their effectiveness. Our code is available at\nhttps://github.com/IBM/larimar",
      "tldr_zh": "本研究提出Larimar，一种受脑启发的新型架构，用于为Large Language Models (LLMs) 增强分布式episodic memory，从而实现高效的知识动态更新，支持一次性(one-shot)更新而不需重新训练或微调。实验结果显示，Larimar在多个事实编辑基准测试中，其准确性与最先进基线相当，甚至在顺序编辑设置中表现出色，同时实现了8-10倍的速度提升，并保持了简单、LLM-agnostic的灵活性。该框架还提供了选择性事实遗忘、信息泄露预防和输入上下文长度泛化的机制，进一步提升了其实用性和可靠性。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "ICML 2024",
      "pdf_url": "http://arxiv.org/pdf/2403.11901v4",
      "published_date": "2024-03-18 16:01:42 UTC",
      "updated_date": "2024-08-21 22:54:47 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T16:37:27.246697"
    },
    {
      "arxiv_id": "2403.11894v4",
      "title": "From Explainable to Interpretable Deep Learning for Natural Language Processing in Healthcare: How Far from Reality?",
      "title_zh": "翻译失败",
      "authors": [
        "Guangming Huang",
        "Yingya Li",
        "Shoaib Jameel",
        "Yunfei Long",
        "Giorgos Papanastasiou"
      ],
      "abstract": "Deep learning (DL) has substantially enhanced natural language processing\n(NLP) in healthcare research. However, the increasing complexity of DL-based\nNLP necessitates transparent model interpretability, or at least\nexplainability, for reliable decision-making. This work presents a thorough\nscoping review of explainable and interpretable DL in healthcare NLP. The term\n\"eXplainable and Interpretable Artificial Intelligence\" (XIAI) is introduced to\ndistinguish XAI from IAI. Different models are further categorized based on\ntheir functionality (model-, input-, output-based) and scope (local, global).\nOur analysis shows that attention mechanisms are the most prevalent emerging\nIAI technique. The use of IAI is growing, distinguishing it from XAI. The major\nchallenges identified are that most XIAI does not explore \"global\" modelling\nprocesses, the lack of best practices, and the lack of systematic evaluation\nand benchmarks. One important opportunity is to use attention mechanisms to\nenhance multi-modal XIAI for personalized medicine. Additionally, combining DL\nwith causal logic holds promise. Our discussion encourages the integration of\nXIAI in Large Language Models (LLMs) and domain-specific smaller models. In\nconclusion, XIAI adoption in healthcare requires dedicated in-house expertise.\nCollaboration with domain experts, end-users, and policymakers can lead to\nready-to-use XIAI methods across NLP and medical tasks. While challenges exist,\nXIAI techniques offer a valuable foundation for interpretable NLP algorithms in\nhealthcare.",
      "tldr_zh": "这篇论文通过一个全面的范围综述，探讨了在医疗自然语言处理(NLP)中可解释和可解释深度学习(XIAI)的现状，引入XIAI概念以区分XAI（可解释AI）和IAI（可解释AI）。作者根据模型功能（模型、输入、输出-based）和范围（局部、全局）进行分类，发现注意力机制是最常见的IAI技术，且IAI的使用正在增长。关键挑战包括缺乏全局建模探索、最佳实践和系统评估基准，而机会在于利用注意力机制增强多模态XIAI用于个性化医学，并将深度学习(DL)与因果逻辑结合。论文鼓励将XIAI整合到Large Language Models (LLMs)和领域特定模型中，并强调需要内部专业知识和多方合作来推动其在医疗中的实际应用。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "This paper has been accepted by Computational and Structural\n  Biotechnology Journal",
      "pdf_url": "http://arxiv.org/pdf/2403.11894v4",
      "published_date": "2024-03-18 15:53:33 UTC",
      "updated_date": "2024-10-16 14:14:27 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T16:37:40.810749"
    },
    {
      "arxiv_id": "2403.11887v1",
      "title": "SuperLoRA: Parameter-Efficient Unified Adaptation of Multi-Layer Attention Modules",
      "title_zh": "翻译失败",
      "authors": [
        "Xiangyu Chen",
        "Jing Liu",
        "Ye Wang",
        "Pu Perry Wang",
        "Matthew Brand",
        "Guanghui Wang",
        "Toshiaki Koike-Akino"
      ],
      "abstract": "Low-rank adaptation (LoRA) and its variants are widely employed in\nfine-tuning large models, including large language models for natural language\nprocessing and diffusion models for computer vision. This paper proposes a\ngeneralized framework called SuperLoRA that unifies and extends different LoRA\nvariants, which can be realized under different hyper-parameter settings.\nIntroducing grouping, folding, shuffling, projecting, and tensor factoring,\nSuperLoRA offers high flexibility compared with other LoRA variants and\ndemonstrates superior performance for transfer learning tasks especially in the\nextremely few-parameter regimes.",
      "tldr_zh": "本论文提出了一种名为 SuperLoRA 的参数高效框架，用于统一和扩展低秩适应（LoRA）及其变体，适用于多层注意力模块的微调。SuperLoRA 通过引入 grouping、folding、shuffling、projecting 和 tensor factoring 等技术，提供更高的灵活性，尤其在极少参数设置下。实验结果显示，该框架在迁移学习任务中表现出色，显著提升了性能。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "33 pages, 29 figures",
      "pdf_url": "http://arxiv.org/pdf/2403.11887v1",
      "published_date": "2024-03-18 15:40:36 UTC",
      "updated_date": "2024-03-18 15:40:36 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T16:37:49.114535"
    },
    {
      "arxiv_id": "2403.11886v2",
      "title": "QueryAgent: A Reliable and Efficient Reasoning Framework with Environmental Feedback-based Self-Correction",
      "title_zh": "翻译失败",
      "authors": [
        "Xiang Huang",
        "Sitao Cheng",
        "Shanshan Huang",
        "Jiayu Shen",
        "Yong Xu",
        "Chaoyun Zhang",
        "Yuzhong Qu"
      ],
      "abstract": "Employing Large Language Models (LLMs) for semantic parsing has achieved\nremarkable success. However, we find existing methods fall short in terms of\nreliability and efficiency when hallucinations are encountered. In this paper,\nwe address these challenges with a framework called QueryAgent, which solves a\nquestion step-by-step and performs step-wise self-correction. We introduce an\nenvironmental feedback-based self-correction method called ERASER. Unlike\ntraditional approaches, ERASER leverages rich environmental feedback in the\nintermediate steps to perform selective and differentiated self-correction only\nwhen necessary. Experimental results demonstrate that QueryAgent notably\noutperforms all previous few-shot methods using only one example on GrailQA and\nGraphQ by 7.0 and 15.0 F1. Moreover, our approach exhibits superiority in terms\nof efficiency, including runtime, query overhead, and API invocation costs. By\nleveraging ERASER, we further improve another baseline (i.e., AgentBench) by\napproximately 10 points, revealing the strong transferability of our approach.",
      "tldr_zh": "该研究提出QueryAgent框架，利用逐步问题解决和基于环境反馈的自我修正方法，解决Large Language Models (LLMs)在语义解析中遇到的幻觉问题，提高了可靠性和效率。具体而言，引入ERASER机制，通过中间步骤的环境反馈进行选择性自我修正，仅在必要时优化过程。实验结果显示，QueryAgent在GrailQA和GraphQ数据集上，使用仅一个示例，比之前的少样本方法提升了7.0和15.0 F1分数，同时在运行时间、查询开销和API调用成本等方面表现出色；此外，该方法应用于AgentBench基线后提升约10分，展示了强大的转移性。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "Accepted by ACL 2024 main conference. 22 pages,7 figures, 13 tables",
      "pdf_url": "http://arxiv.org/pdf/2403.11886v2",
      "published_date": "2024-03-18 15:39:14 UTC",
      "updated_date": "2024-06-13 13:18:43 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T16:38:03.315859"
    },
    {
      "arxiv_id": "2403.11882v1",
      "title": "ReGenNet: Towards Human Action-Reaction Synthesis",
      "title_zh": "ReGenNet：",
      "authors": [
        "Liang Xu",
        "Yizhou Zhou",
        "Yichao Yan",
        "Xin Jin",
        "Wenhan Zhu",
        "Fengyun Rao",
        "Xiaokang Yang",
        "Wenjun Zeng"
      ],
      "abstract": "Humans constantly interact with their surrounding environments. Current\nhuman-centric generative models mainly focus on synthesizing humans plausibly\ninteracting with static scenes and objects, while the dynamic human\naction-reaction synthesis for ubiquitous causal human-human interactions is\nless explored. Human-human interactions can be regarded as asymmetric with\nactors and reactors in atomic interaction periods. In this paper, we\ncomprehensively analyze the asymmetric, dynamic, synchronous, and detailed\nnature of human-human interactions and propose the first multi-setting human\naction-reaction synthesis benchmark to generate human reactions conditioned on\ngiven human actions. To begin with, we propose to annotate the actor-reactor\norder of the interaction sequences for the NTU120, InterHuman, and Chi3D\ndatasets. Based on them, a diffusion-based generative model with a Transformer\ndecoder architecture called ReGenNet together with an explicit distance-based\ninteraction loss is proposed to predict human reactions in an online manner,\nwhere the future states of actors are unavailable to reactors. Quantitative and\nqualitative results show that our method can generate instant and plausible\nhuman reactions compared to the baselines, and can generalize to unseen actor\nmotions and viewpoint changes.",
      "tldr_zh": "该论文分析了人类-人类交互的非对称(dynamic)、动态(synchronous)和详细(detailed)特性，提出首个多设置基准，用于根据给定的人类动作生成反应。研究者为 NTU120、InterHuman 和 Chi3D 数据集标注了 actor-reactor 顺序，并开发了基于扩散模型(diffusion-based generative model)的 ReGenNet，使用 Transformer decoder 架构和 explicit distance-based interaction loss，在线预测反应，而不依赖 actors 的未来状态。实验结果显示，ReGenNet 比基线方法生成更即时且合理的反应，并能泛化到未见动作和视角变化。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "Accepted by CVPR 2024, Project Page:\n  https://liangxuy.github.io/ReGenNet/",
      "pdf_url": "http://arxiv.org/pdf/2403.11882v1",
      "published_date": "2024-03-18 15:33:06 UTC",
      "updated_date": "2024-03-18 15:33:06 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T16:38:15.510927"
    },
    {
      "arxiv_id": "2403.11879v4",
      "title": "Unimodal Multi-Task Fusion for Emotional Mimicry Intensity Prediction",
      "title_zh": "翻译失败",
      "authors": [
        "Tobias Hallmen",
        "Fabian Deuser",
        "Norbert Oswald",
        "Elisabeth André"
      ],
      "abstract": "In this research, we introduce a novel methodology for assessing Emotional\nMimicry Intensity (EMI) as part of the 6th Workshop and Competition on\nAffective Behavior Analysis in-the-wild. Our methodology utilises the Wav2Vec\n2.0 architecture, which has been pre-trained on an extensive podcast dataset,\nto capture a wide array of audio features that include both linguistic and\nparalinguistic components. We refine our feature extraction process by\nemploying a fusion technique that combines individual features with a global\nmean vector, thereby embedding a broader contextual understanding into our\nanalysis. A key aspect of our approach is the multi-task fusion strategy that\nnot only leverages these features but also incorporates a pre-trained\nValence-Arousal-Dominance (VAD) model. This integration is designed to refine\nemotion intensity prediction by concurrently processing multiple emotional\ndimensions, thereby embedding a richer contextual understanding into our\nframework. For the temporal analysis of audio data, our feature fusion process\nutilises a Long Short-Term Memory (LSTM) network. This approach, which relies\nsolely on the provided audio data, shows marked advancements over the existing\nbaseline, offering a more comprehensive understanding of emotional mimicry in\nnaturalistic settings, achieving the second place in the EMI challenge.",
      "tldr_zh": "本研究提出了一种单模态多任务融合方法，用于预测Emotional Mimicry Intensity (EMI)，作为第6届野外情感行为分析工作坊竞赛的一部分。该方法利用预训练的Wav2Vec 2.0架构从音频数据中提取语言和副语言特征，并通过融合技术结合单个特征与全局均值向量，以及整合预训练的Valence-Arousal-Dominance (VAD)模型，来同时处理多个情绪维度并提升预测准确性。Long Short-Term Memory (LSTM)网络用于音频数据的时序分析，仅依赖音频输入，即在EMI挑战中超越基线模型并获得第二名，提供更全面的自然环境情绪模仿理解。",
      "categories": [
        "cs.SD",
        "cs.AI",
        "eess.AS"
      ],
      "primary_category": "cs.SD",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2403.11879v4",
      "published_date": "2024-03-18 15:32:02 UTC",
      "updated_date": "2024-06-16 12:21:39 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T16:38:26.562772"
    },
    {
      "arxiv_id": "2403.11865v2",
      "title": "Exploring Multi-modal Neural Scene Representations With Applications on Thermal Imaging",
      "title_zh": "探索多模态神经场景表示及其在热成像上的应用",
      "authors": [
        "Mert Özer",
        "Maximilian Weiherer",
        "Martin Hundhausen",
        "Bernhard Egger"
      ],
      "abstract": "Neural Radiance Fields (NeRFs) quickly evolved as the new de-facto standard\nfor the task of novel view synthesis when trained on a set of RGB images. In\nthis paper, we conduct a comprehensive evaluation of neural scene\nrepresentations, such as NeRFs, in the context of multi-modal learning.\nSpecifically, we present four different strategies of how to incorporate a\nsecond modality, other than RGB, into NeRFs: (1) training from scratch\nindependently on both modalities; (2) pre-training on RGB and fine-tuning on\nthe second modality; (3) adding a second branch; and (4) adding a separate\ncomponent to predict (color) values of the additional modality. We chose\nthermal imaging as second modality since it strongly differs from RGB in terms\nof radiosity, making it challenging to integrate into neural scene\nrepresentations. For the evaluation of the proposed strategies, we captured a\nnew publicly available multi-view dataset, ThermalMix, consisting of six common\nobjects and about 360 RGB and thermal images in total. We employ cross-modality\ncalibration prior to data capturing, leading to high-quality alignments between\nRGB and thermal images. Our findings reveal that adding a second branch to NeRF\nperforms best for novel view synthesis on thermal images while also yielding\ncompelling results on RGB. Finally, we also show that our analysis generalizes\nto other modalities, including near-infrared images and depth maps. Project\npage: https://mert-o.github.io/ThermalNeRF/.",
      "tldr_zh": "本研究评估了 Neural Radiance Fields (NeRFs) 在多模态学习中的表现，特别是整合非RGB模态的应用，并以热成像为例。论文提出四种策略来整合第二模态，包括从零训练、RGB预训练微调、添加第二分支以及添加单独预测组件。研究者创建了新数据集ThermalMix，包含约360张RGB和热成像图像，并通过跨模态校准确保高质量对齐。实验结果显示，添加第二分支的策略在热成像的Novel View Synthesis上表现最佳，同时在RGB上也取得出色效果，且该方法可泛化到其他模态如近红外图像和深度图。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.GR"
      ],
      "primary_category": "cs.CV",
      "comment": "Accepted to ECCVW'24",
      "pdf_url": "http://arxiv.org/pdf/2403.11865v2",
      "published_date": "2024-03-18 15:18:55 UTC",
      "updated_date": "2024-08-23 09:40:59 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T16:38:40.740837"
    },
    {
      "arxiv_id": "2403.11852v3",
      "title": "Reinforcement Learning with Latent State Inference for Autonomous On-ramp Merging under Observation Delay",
      "title_zh": "翻译失败",
      "authors": [
        "Amin Tabrizian",
        "Zhitong Huang",
        "Peng Wei"
      ],
      "abstract": "This paper presents a novel approach to address the challenging problem of\nautonomous on-ramp merging, where a self-driving vehicle needs to seamlessly\nintegrate into a flow of vehicles on a multi-lane highway. We introduce the\nLane-keeping, Lane-changing with Latent-state Inference and Safety Controller\n(L3IS) agent, designed to perform the on-ramp merging task safely without\ncomprehensive knowledge about surrounding vehicles' intents or driving styles.\nWe also present an augmentation of this agent called AL3IS that accounts for\nobservation delays, allowing the agent to make more robust decisions in\nreal-world environments with vehicle-to-vehicle (V2V) communication delays. By\nmodeling the unobservable aspects of the environment through latent states,\nsuch as other drivers' intents, our approach enhances the agent's ability to\nadapt to dynamic traffic conditions, optimize merging maneuvers, and ensure\nsafe interactions with other vehicles. We demonstrate the effectiveness of our\nmethod through extensive simulations generated from real traffic data and\ncompare its performance with existing approaches. L3IS shows a 99.90% success\nrate in a challenging on-ramp merging case generated from the real US Highway\n101 data. We further perform a sensitivity analysis on AL3IS to evaluate its\nrobustness against varying observation delays, which demonstrates an acceptable\nperformance of 93.84% success rate in 1-second V2V communication delay.",
      "tldr_zh": "本研究提出了一种基于强化学习(Reinforcement Learning)的新方法，用于处理自动驾驶车辆在多车道高速公路上坡道合并的挑战问题。作者引入了L3IS代理（Lane-keeping, Lane-changing with Latent-state Inference and Safety Controller），通过潜在状态推理(Latent State Inference)建模其他车辆的意图，实现安全合并，而无需全面了解周围车辆的驾驶风格；其扩展版本AL3IS则针对观察延迟（如V2V通信延迟）进行了优化，提升了代理在动态交通中的适应性和决策鲁棒性。实验结果显示，L3IS在基于真实US Highway 101数据的模拟场景中成功率达99.90%，而AL3IS在1秒延迟条件下仍保持93.84%的成功率，证明了该方法的有效性和实际应用潜力。",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2403.11852v3",
      "published_date": "2024-03-18 15:02:46 UTC",
      "updated_date": "2024-06-21 15:31:50 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T16:38:52.306146"
    },
    {
      "arxiv_id": "2403.13850v1",
      "title": "Spatio-Temporal Fluid Dynamics Modeling via Physical-Awareness and Parameter Diffusion Guidance",
      "title_zh": "翻译失败",
      "authors": [
        "Hao Wu",
        "Fan Xu",
        "Yifan Duan",
        "Ziwei Niu",
        "Weiyan Wang",
        "Gaofeng Lu",
        "Kun Wang",
        "Yuxuan Liang",
        "Yang Wang"
      ],
      "abstract": "This paper proposes a two-stage framework named ST-PAD for spatio-temporal\nfluid dynamics modeling in the field of earth sciences, aiming to achieve\nhigh-precision simulation and prediction of fluid dynamics through\nspatio-temporal physics awareness and parameter diffusion guidance. In the\nupstream stage, we design a vector quantization reconstruction module with\ntemporal evolution characteristics, ensuring balanced and resilient parameter\ndistribution by introducing general physical constraints. In the downstream\nstage, a diffusion probability network involving parameters is utilized to\ngenerate high-quality future states of fluids, while enhancing the model's\ngeneralization ability by perceiving parameters in various physical setups.\nExtensive experiments on multiple benchmark datasets have verified the\neffectiveness and robustness of the ST-PAD framework, which showcase that\nST-PAD outperforms current mainstream models in fluid dynamics modeling and\nprediction, especially in effectively capturing local representations and\nmaintaining significant advantages in OOD generations.",
      "tldr_zh": "本论文提出了一种名为 ST-PAD 的两阶段框架，用于地球科学领域的时空流体动力学建模，通过 spatio-temporal physics awareness 和 parameter diffusion guidance 实现高精度模拟和预测。在上游阶段，该框架设计了带有时间演化特征的 vector quantization 重建模块，引入一般物理约束以确保参数分布的平衡和弹性；而在下游阶段，则利用涉及参数的 diffusion probability network 生成高质量的流体未来状态，并通过感知各种物理设置提升模型的泛化能力。实验结果显示，ST-PAD 在多个基准数据集上优于主流模型，尤其在捕捉局部表示和 OOD generations 方面表现出显著优势。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "physics.flu-dyn"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2403.13850v1",
      "published_date": "2024-03-18 14:57:47 UTC",
      "updated_date": "2024-03-18 14:57:47 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T16:39:04.147059"
    },
    {
      "arxiv_id": "2403.11843v1",
      "title": "Fuzzy Rough Choquet Distances for Classification",
      "title_zh": "模糊粗糙 Choquet 距离用于分类",
      "authors": [
        "Adnan Theerens",
        "Chris Cornelis"
      ],
      "abstract": "This paper introduces a novel Choquet distance using fuzzy rough set based\nmeasures. The proposed distance measure combines the attribute information\nreceived from fuzzy rough set theory with the flexibility of the Choquet\nintegral. This approach is designed to adeptly capture non-linear relationships\nwithin the data, acknowledging the interplay of the conditional attributes\ntowards the decision attribute and resulting in a more flexible and accurate\ndistance. We explore its application in the context of machine learning, with a\nspecific emphasis on distance-based classification approaches (e.g. k-nearest\nneighbours). The paper examines two fuzzy rough set based measures that are\nbased on the positive region. Moreover, we explore two procedures for\nmonotonizing the measures derived from fuzzy rough set theory, making them\nsuitable for use with the Choquet integral, and investigate their differences.",
      "tldr_zh": "该论文提出了一种新型的 Choquet 距离，利用基于模糊粗糙集（Fuzzy Rough Set）的度量，结合 Choquet 积分的灵活性，以捕捉数据中的非线性关系，并考虑条件属性与决策属性的相互作用，从而实现更准确的距离计算。作者探讨了两种基于正区域（Positive Region）的模糊粗糙集度量，并开发了两种使这些度量单调化的过程，使其适合与 Choquet 积分整合。最终，该方法应用于机器学习中的基于距离的分类算法（如 k-nearest neighbours），有望提升分类的灵活性和精度。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2403.11843v1",
      "published_date": "2024-03-18 14:53:48 UTC",
      "updated_date": "2024-03-18 14:53:48 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T16:39:15.414840"
    },
    {
      "arxiv_id": "2403.11841v1",
      "title": "Pessimistic Causal Reinforcement Learning with Mediators for Confounded Offline Data",
      "title_zh": "翻译失败",
      "authors": [
        "Danyang Wang",
        "Chengchun Shi",
        "Shikai Luo",
        "Will Wei Sun"
      ],
      "abstract": "In real-world scenarios, datasets collected from randomized experiments are\noften constrained by size, due to limitations in time and budget. As a result,\nleveraging large observational datasets becomes a more attractive option for\nachieving high-quality policy learning. However, most existing offline\nreinforcement learning (RL) methods depend on two key\nassumptions--unconfoundedness and positivity--which frequently do not hold in\nobservational data contexts. Recognizing these challenges, we propose a novel\npolicy learning algorithm, PESsimistic CAusal Learning (PESCAL). We utilize the\nmediator variable based on front-door criterion to remove the confounding bias;\nadditionally, we adopt the pessimistic principle to address the distributional\nshift between the action distributions induced by candidate policies, and the\nbehavior policy that generates the observational data. Our key observation is\nthat, by incorporating auxiliary variables that mediate the effect of actions\non system dynamics, it is sufficient to learn a lower bound of the mediator\ndistribution function, instead of the Q-function, to partially mitigate the\nissue of distributional shift. This insight significantly simplifies our\nalgorithm, by circumventing the challenging task of sequential uncertainty\nquantification for the estimated Q-function. Moreover, we provide theoretical\nguarantees for the algorithms we propose, and demonstrate their efficacy\nthrough simulations, as well as real-world experiments utilizing offline\ndatasets from a leading ride-hailing platform.",
      "tldr_zh": "该研究针对观察性数据中的混杂问题，提出了一种新型算法PESCAL（PESsimistic CAusal Learning），用于改进离线强化学习（offline reinforcement learning）。PESCAL利用中介变量（mediator variable）基于front-door criterion去除混杂偏差，并采用pessimistic principle处理候选策略和行为策略之间的分布偏移。关键创新在于，通过辅助变量调解动作对系统动态的影响，只需学习中介分布函数的下界，而非复杂的Q-function，从而简化算法并缓解分布偏移问题。该算法提供了理论保证，并在模拟和真实实验（如网约车平台的离线数据集）中证明了其有效性，提高了策略学习的质量。",
      "categories": [
        "stat.ML",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "stat.ML",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2403.11841v1",
      "published_date": "2024-03-18 14:51:19 UTC",
      "updated_date": "2024-03-18 14:51:19 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T16:39:28.283188"
    },
    {
      "arxiv_id": "2403.11838v2",
      "title": "Ensuring Safe and High-Quality Outputs: A Guideline Library Approach for Language Models",
      "title_zh": "翻译失败",
      "authors": [
        "Yi Luo",
        "Zhenghao Lin",
        "Yuhao Zhang",
        "Jiashuo Sun",
        "Chen Lin",
        "Chengjin Xu",
        "Xiangdong Su",
        "Yelong Shen",
        "Jian Guo",
        "Yeyun Gong"
      ],
      "abstract": "Large Language Models (LLMs) exhibit impressive capabilities but also present\nrisks such as biased content generation and privacy issues. One of the current\nalignment techniques includes principle-driven integration, but it faces\nchallenges arising from the imprecision of manually crafted rules and\ninadequate risk perception in models without safety training. To address these,\nwe introduce Guide-Align, a two-stage approach. Initially, a safety-trained\nmodel identifies potential risks and formulates specific guidelines for various\ninputs, establishing a comprehensive library of guidelines and a model for\ninput-guidelines retrieval. Subsequently, the retrieval model correlates new\ninputs with relevant guidelines, which guide LLMs in response generation to\nensure safe and high-quality outputs, thereby aligning with human values. An\nadditional optional stage involves fine-tuning a model with well-aligned\ndatasets generated through the process implemented in the second stage. Our\nmethod customizes guidelines to accommodate diverse inputs, thereby enhancing\nthe fine-grainedness and comprehensiveness of the guideline library.\nFurthermore, it incorporates safety expertise from a safety-trained LLM through\na lightweight retrieval model. We evaluate our approach on three benchmarks,\ndemonstrating significant improvements in LLM security and quality. Notably,\nour fine-tuned model, Labrador, even at 13 billion parameters, outperforms\nGPT-3.5-turbo and surpasses GPT-4 in alignment capabilities.",
      "tldr_zh": "这篇论文针对 Large Language Models (LLMs) 的风险（如偏见和隐私问题），提出了 Guide-Align 一种两阶段方法来提升模型的安全性和输出质量。第一阶段，使用安全训练模型识别潜在风险并构建全面的指南库，以及一个输入-指南检索模型；第二阶段，通过检索模型为新输入关联相关指南，指导 LLMs 生成与人类价值观对齐的输出，并可选地进行微调。实验在三个基准上验证了该方法的有效性，微调后的 Labrador 模型（13 亿参数）在对齐能力上优于 GPT-3.5-turbo 并超过了 GPT-4。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "Accepted to NAACL 2024 main conference",
      "pdf_url": "http://arxiv.org/pdf/2403.11838v2",
      "published_date": "2024-03-18 14:48:29 UTC",
      "updated_date": "2024-03-23 06:26:41 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T16:39:40.887829"
    },
    {
      "arxiv_id": "2403.11830v2",
      "title": "Problem space structural adversarial attacks for Network Intrusion Detection Systems based on Graph Neural Networks",
      "title_zh": "基于图神经网络的",
      "authors": [
        "Andrea Venturi",
        "Dario Stabili",
        "Mirco Marchetti"
      ],
      "abstract": "Machine Learning (ML) algorithms have become increasingly popular for\nsupporting Network Intrusion Detection Systems (NIDS). Nevertheless, extensive\nresearch has shown their vulnerability to adversarial attacks, which involve\nsubtle perturbations to the inputs of the models aimed at compromising their\nperformance. Recent proposals have effectively leveraged Graph Neural Networks\n(GNN) to produce predictions based also on the structural patterns exhibited by\nintrusions to enhance the detection robustness. However, the adoption of\nGNN-based NIDS introduces new types of risks. In this paper, we propose the\nfirst formalization of adversarial attacks specifically tailored for GNN in\nnetwork intrusion detection. Moreover, we outline and model the problem space\nconstraints that attackers need to consider to carry out feasible structural\nattacks in real-world scenarios. As a final contribution, we conduct an\nextensive experimental campaign in which we launch the proposed attacks against\nstate-of-the-art GNN-based NIDS. Our findings demonstrate the increased\nrobustness of the models against classical feature-based adversarial attacks,\nwhile highlighting their susceptibility to structure-based attacks.",
      "tldr_zh": "本论文探讨了基于 Graph Neural Networks (GNN) 的 Network Intrusion Detection Systems (NIDS) 面对结构性对抗性攻击的脆弱性，强调了 GNN 在利用入侵的结构模式增强检测鲁棒性时引入的新风险。研究者首次形式化了针对 GNN 在网络入侵检测中的对抗性攻击，并建模了攻击者在真实场景中需遵守的问题空间约束，以确保攻击的可行性。通过广泛实验，论文证明了 GNN-based NIDS 对经典特征-based 攻击的较高鲁棒性，但同时暴露了其对结构-based 攻击的易感性，为提升 NIDS 的安全性提供了重要见解。",
      "categories": [
        "cs.CR",
        "cs.AI"
      ],
      "primary_category": "cs.CR",
      "comment": "preprint submitted to IEEE TIFS, under review",
      "pdf_url": "http://arxiv.org/pdf/2403.11830v2",
      "published_date": "2024-03-18 14:40:33 UTC",
      "updated_date": "2024-04-23 15:21:04 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T16:39:51.938679"
    },
    {
      "arxiv_id": "2403.13849v1",
      "title": "Graphs Unveiled: Graph Neural Networks and Graph Generation",
      "title_zh": "翻译失败",
      "authors": [
        "László Kovács",
        "Ali Jlidi"
      ],
      "abstract": "One of the hot topics in machine learning is the field of GNN. The complexity\nof graph data has imposed significant challenges on existing machine learning\nalgorithms. Recently, many studies on extending deep learning approaches for\ngraph data have emerged. This paper represents a survey, providing a\ncomprehensive overview of Graph Neural Networks (GNNs). We discuss the\napplications of graph neural networks across various domains. Finally, we\npresent an advanced field in GNNs: graph generation.",
      "tldr_zh": "这篇论文对 Graph Neural Networks (GNNs) 进行了全面概述，探讨了图数据复杂性对现有机器学习算法的挑战，以及最近将深度学习方法扩展到图数据的相关研究。论文讨论了 GNNs 在各种领域的应用，包括如何处理图结构数据。最终，它介绍了 GNNs 的高级领域：图生成，为研究者提供了一个系统性的调查和未来方向指引。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2403.13849v1",
      "published_date": "2024-03-18 14:37:27 UTC",
      "updated_date": "2024-03-18 14:37:27 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T16:40:03.021234"
    },
    {
      "arxiv_id": "2403.11821v5",
      "title": "A Survey on Quality Metrics for Text-to-Image Generation",
      "title_zh": "文本到图像生成质量指标的调查",
      "authors": [
        "Sebastian Hartwig",
        "Dominik Engel",
        "Leon Sick",
        "Hannah Kniesel",
        "Tristan Payer",
        "Poonam Poonam",
        "Michael Glöckler",
        "Alex Bäuerle",
        "Timo Ropinski"
      ],
      "abstract": "AI-based text-to-image models do not only excel at generating realistic\nimages, they also give designers more and more fine-grained control over the\nimage content. Consequently, these approaches have gathered increased attention\nwithin the computer graphics research community, which has been historically\ndevoted towards traditional rendering techniques, that offer precise control\nover scene parameters (e.g., objects, materials, and lighting). While the\nquality of conventionally rendered images is assessed through well established\nimage quality metrics, such as SSIM or PSNR, the unique challenges of\ntext-to-image generation require other, dedicated quality metrics. These\nmetrics must be able to not only measure overall image quality, but also how\nwell images reflect given text prompts, whereby the control of scene and\nrendering parameters is interweaved. Within this survey, we provide a\ncomprehensive overview of such text-to-image quality metrics, and propose a\ntaxonomy to categorize these metrics. Our taxonomy is grounded in the\nassumption, that there are two main quality criteria, namely compositional\nquality and general quality, that contribute to the overall image quality.\nBesides the metrics, this survey covers dedicated text-to-image benchmark\ndatasets, over which the metrics are frequently computed. Finally, we identify\nlimitations and open challenges in the field of text-to-image generation, and\nderive guidelines for practitioners conducting text-to-image evaluation.",
      "tldr_zh": "这篇调查论文概述了文本到图像生成(Text-to-Image)领域的质量指标，强调这些指标需评估图像整体质量（如使用 SSIM 或 PSNR）和文本提示的精确匹配。作者提出一个基于 compositional quality 和 general quality 的分类法，提供全面概述，并涵盖相关基准数据集。最终，论文识别了当前方法的限制和开放挑战，并为从业者提供文本到图像评估的实践指南。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.GR"
      ],
      "primary_category": "cs.CV",
      "comment": "preprint",
      "pdf_url": "http://arxiv.org/pdf/2403.11821v5",
      "published_date": "2024-03-18 14:24:20 UTC",
      "updated_date": "2025-01-29 08:48:10 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T16:40:14.167777"
    },
    {
      "arxiv_id": "2403.11807v7",
      "title": "How Far Are We on the Decision-Making of LLMs? Evaluating LLMs' Gaming Ability in Multi-Agent Environments",
      "title_zh": "翻译失败",
      "authors": [
        "Jen-tse Huang",
        "Eric John Li",
        "Man Ho Lam",
        "Tian Liang",
        "Wenxuan Wang",
        "Youliang Yuan",
        "Wenxiang Jiao",
        "Xing Wang",
        "Zhaopeng Tu",
        "Michael R. Lyu"
      ],
      "abstract": "Decision-making is a complex process requiring diverse abilities, making it\nan excellent framework for evaluating Large Language Models (LLMs). Researchers\nhave examined LLMs' decision-making through the lens of Game Theory. However,\nexisting evaluation mainly focus on two-player scenarios where an LLM competes\nagainst another. Additionally, previous benchmarks suffer from test set leakage\ndue to their static design. We introduce GAMA($\\gamma$)-Bench, a new framework\nfor evaluating LLMs' Gaming Ability in Multi-Agent environments. It includes\neight classical game theory scenarios and a dynamic scoring scheme specially\ndesigned to quantitatively assess LLMs' performance. $\\gamma$-Bench allows\nflexible game settings and adapts the scoring system to different game\nparameters, enabling comprehensive evaluation of robustness, generalizability,\nand strategies for improvement. Our results indicate that GPT-3.5 demonstrates\nstrong robustness but limited generalizability, which can be enhanced using\nmethods like Chain-of-Thought. We also evaluate 13 LLMs from 6 model families,\nincluding GPT-3.5, GPT-4, Gemini, LLaMA-3.1, Mixtral, and Qwen-2.\nGemini-1.5-Pro outperforms others, scoring of $69.8$ out of $100$, followed by\nLLaMA-3.1-70B ($65.9$) and Mixtral-8x22B ($62.4$). Our code and experimental\nresults are publicly available at https://github.com/CUHK-ARISE/GAMABench.",
      "tldr_zh": "该研究评估大型语言模型 (LLMs) 在多智能体环境中的决策和游戏能力，引入 GAMA(γ)-Bench 框架，以八个经典游戏理论场景和动态评分方案解决现有基准的测试集泄露问题，并测试模型的鲁棒性、泛化性和改进策略。实验结果显示，GPT-3.5 表现出较强鲁棒性但泛化性有限，可通过 Chain-of-Thought 方法增强；Gemini-1.5-Pro 以69.8分的最高成绩领先其他13个模型，包括 GPT-4 和 LLaMA-3.1-70B (65.9分)。代码和结果已公开在 GitHub 上，便于进一步研究。",
      "categories": [
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.AI",
      "comment": "Accepted to ICLR 2025; 11 pages of main text; 26 pages of appendices;\n  Included models: GPT-3.5-{0613, 1106, 0125}, GPT-4-0125, GPT-4o-0806,\n  Gemini-{1.0, 1.5)-Pro, LLaMA-3.1-{7, 70, 405}B, Mixtral-8x{7, 22}B,\n  Qwen-2-72B",
      "pdf_url": "http://arxiv.org/pdf/2403.11807v7",
      "published_date": "2024-03-18 14:04:47 UTC",
      "updated_date": "2025-03-06 18:58:23 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T16:40:29.918588"
    },
    {
      "arxiv_id": "2403.11793v3",
      "title": "Reasoning Abilities of Large Language Models: In-Depth Analysis on the Abstraction and Reasoning Corpus",
      "title_zh": "大语言模型的推理能力：在抽象和推理语料库上的深入分析",
      "authors": [
        "Seungpil Lee",
        "Woochang Sim",
        "Donghyeon Shin",
        "Wongyu Seo",
        "Jiwon Park",
        "Seokki Lee",
        "Sanha Hwang",
        "Sejin Kim",
        "Sundong Kim"
      ],
      "abstract": "The existing methods for evaluating the inference abilities of Large Language\nModels (LLMs) have been predominantly results-centric, making it challenging to\nassess the inference process comprehensively. We introduce a novel approach\nusing the Abstraction and Reasoning Corpus (ARC) benchmark to evaluate the\ninference and contextual understanding abilities of LLMs in a process-centric\nmanner, focusing on three key components from the Language of Thought\nHypothesis (LoTH): Logical Coherence, Compositionality, and Productivity. Our\ncarefully designed experiments reveal that while LLMs demonstrate some\ninference capabilities, they still significantly lag behind human-level\nreasoning in these three aspects. The main contribution of this paper lies in\nintroducing the LoTH perspective, which provides a method for evaluating the\nreasoning process that conventional results-oriented approaches fail to\ncapture, thereby offering new insights into the development of human-level\nreasoning in artificial intelligence systems.",
      "tldr_zh": "本研究分析了大型语言模型 (LLMs) 的推理能力，采用过程导向的方法，使用 Abstraction and Reasoning Corpus (ARC) 基准来评估 LLMs 的推理和上下文理解。研究从 Language of Thought Hypothesis (LoTH) 的三个关键组件——Logical Coherence、Compositionality 和 Productivity——入手，设计实验揭示 LLMs 在这些方面仍显著落后于人类水平。主要贡献在于引入 LoTH 视角，提供了一种超越传统结果导向方法的评估框架，从而为开发人类级别的 AI 推理系统带来新见解。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.ET",
        "cs.SC"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2403.11793v3",
      "published_date": "2024-03-18 13:50:50 UTC",
      "updated_date": "2024-11-23 03:26:41 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T16:40:43.285836"
    },
    {
      "arxiv_id": "2403.11790v1",
      "title": "Deep Medial Voxels: Learned Medial Axis Approximations for Anatomical Shape Modeling",
      "title_zh": "翻译失败",
      "authors": [
        "Antonio Pepe",
        "Richard Schussnig",
        "Jianning Li",
        "Christina Gsaxner",
        "Dieter Schmalstieg",
        "Jan Egger"
      ],
      "abstract": "Shape reconstruction from imaging volumes is a recurring need in medical\nimage analysis. Common workflows start with a segmentation step, followed by\ncareful post-processing and,finally, ad hoc meshing algorithms. As this\nsequence can be timeconsuming, neural networks are trained to reconstruct\nshapes through template deformation. These networks deliver state-ofthe-art\nresults without manual intervention, but, so far, they have primarily been\nevaluated on anatomical shapes with little topological variety between\nindividuals. In contrast, other works favor learning implicit shape models,\nwhich have multiple benefits for meshing and visualization. Our work follows\nthis direction by introducing deep medial voxels, a semi-implicit\nrepresentation that faithfully approximates the topological skeleton from\nimaging volumes and eventually leads to shape reconstruction via convolution\nsurfaces. Our reconstruction technique shows potential for both visualization\nand computer simulations.",
      "tldr_zh": "这篇论文针对医疗图像分析中的形状重建问题，提出了一种基于神经网络的Deep Medial Voxels方法，以学习逼近Medial Axis（拓扑骨架），从而从图像体积中重建解剖形状。该方法采用半隐式表示和卷积表面技术，避免了传统流程中耗时的分割、后处理和网格生成步骤，并适用于拓扑多样性的解剖结构。实验结果显示，该技术在可视化和计算机模拟方面表现出潜力，提供了一种高效、无需手动干预的形状重建方案。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "10 pages",
      "pdf_url": "http://arxiv.org/pdf/2403.11790v1",
      "published_date": "2024-03-18 13:47:18 UTC",
      "updated_date": "2024-03-18 13:47:18 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T16:40:54.965730"
    },
    {
      "arxiv_id": "2403.11786v1",
      "title": "Construction of Hyper-Relational Knowledge Graphs Using Pre-Trained Large Language Models",
      "title_zh": "翻译失败",
      "authors": [
        "Preetha Datta",
        "Fedor Vitiugin",
        "Anastasiia Chizhikova",
        "Nitin Sawhney"
      ],
      "abstract": "Extracting hyper-relations is crucial for constructing comprehensive\nknowledge graphs, but there are limited supervised methods available for this\ntask. To address this gap, we introduce a zero-shot prompt-based method using\nOpenAI's GPT-3.5 model for extracting hyper-relational knowledge from text.\nComparing our model with a baseline, we achieved promising results, with a\nrecall of 0.77. Although our precision is currently lower, a detailed analysis\nof the model outputs has uncovered potential pathways for future research in\nthis area.",
      "tldr_zh": "本研究针对构建hyper-relational knowledge graphs的挑战，提出了一种zero-shot prompt-based方法，使用OpenAI的GPT-3.5模型从文本中提取hyper-relations，以弥补现有监督方法不足。与基线模型相比，该方法实现了0.77的recall，但precision仍有待提升。总体结果显示了该方法的潜力，并为未来hyper-relational知识提取的研究提供了宝贵方向。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "5 pages + references",
      "pdf_url": "http://arxiv.org/pdf/2403.11786v1",
      "published_date": "2024-03-18 13:44:48 UTC",
      "updated_date": "2024-03-18 13:44:48 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T16:41:06.642688"
    },
    {
      "arxiv_id": "2403.11780v3",
      "title": "Prompt-Singer: Controllable Singing-Voice-Synthesis with Natural Language Prompt",
      "title_zh": "Prompt-Singer: 基于自然语言提示的可控歌声合成",
      "authors": [
        "Yongqi Wang",
        "Ruofan Hu",
        "Rongjie Huang",
        "Zhiqing Hong",
        "Ruiqi Li",
        "Wenrui Liu",
        "Fuming You",
        "Tao Jin",
        "Zhou Zhao"
      ],
      "abstract": "Recent singing-voice-synthesis (SVS) methods have achieved remarkable audio\nquality and naturalness, yet they lack the capability to control the style\nattributes of the synthesized singing explicitly. We propose Prompt-Singer, the\nfirst SVS method that enables attribute controlling on singer gender, vocal\nrange and volume with natural language. We adopt a model architecture based on\na decoder-only transformer with a multi-scale hierarchy, and design a\nrange-melody decoupled pitch representation that enables text-conditioned vocal\nrange control while keeping melodic accuracy. Furthermore, we explore various\nexperiment settings, including different types of text representations, text\nencoder fine-tuning, and introducing speech data to alleviate data scarcity,\naiming to facilitate further research. Experiments show that our model achieves\nfavorable controlling ability and audio quality. Audio samples are available at\nhttp://prompt-singer.github.io .",
      "tldr_zh": "该论文提出Prompt-Singer，一种可控的Singing-Voice-Synthesis (SVS) 方法，使用自然语言提示显式控制歌手性别、声场范围和音量，解决了现有SVS模型在风格属性控制上的局限性。该方法采用基于decoder-only transformer的多尺度层次架构，并设计了range-melody decoupled pitch representation，以实现文本条件下的声场控制，同时保持旋律准确性。通过探索各种实验设置，包括不同文本表示、文本编码器微调和引入语音数据来缓解数据稀缺问题，实验结果显示Prompt-Singer在控制能力和音频质量上均表现出色。",
      "categories": [
        "cs.SD",
        "cs.AI",
        "cs.LG",
        "eess.AS"
      ],
      "primary_category": "cs.SD",
      "comment": "Accepted by NAACL 2024 (main conference)",
      "pdf_url": "http://arxiv.org/pdf/2403.11780v3",
      "published_date": "2024-03-18 13:39:05 UTC",
      "updated_date": "2025-01-06 09:08:24 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T16:41:19.461790"
    },
    {
      "arxiv_id": "2403.11772v2",
      "title": "S-JEPA: towards seamless cross-dataset transfer through dynamic spatial attention",
      "title_zh": "翻译失败",
      "authors": [
        "Pierre Guetschel",
        "Thomas Moreau",
        "Michael Tangermann"
      ],
      "abstract": "Motivated by the challenge of seamless cross-dataset transfer in EEG signal\nprocessing, this article presents an exploratory study on the use of Joint\nEmbedding Predictive Architectures (JEPAs). In recent years, self-supervised\nlearning has emerged as a promising approach for transfer learning in various\ndomains. However, its application to EEG signals remains largely unexplored. In\nthis article, we introduce Signal-JEPA for representing EEG recordings which\nincludes a novel domain-specific spatial block masking strategy and three novel\narchitectures for downstream classification. The study is conducted on a 54\nsubjects dataset and the downstream performance of the models is evaluated on\nthree different BCI paradigms: motor imagery, ERP and SSVEP. Our study provides\npreliminary evidence for the potential of JEPAs in EEG signal encoding.\nNotably, our results highlight the importance of spatial filtering for accurate\ndownstream classification and reveal an influence of the length of the\npre-training examples but not of the mask size on the downstream performance.",
      "tldr_zh": "本研究针对 EEG 信号处理的跨数据集转移挑战，提出 Signal-JEPA 框架，该框架基于 Joint Embedding Predictive Architectures (JEPAs)，并引入新型领域特定空间块掩码策略和三个下游分类架构，以实现动态空间注意力。\n在包含 54 个受试者的数据集上，评估了 Signal-JEPA 在运动想象、ERP 和 SSVEP 等 BCI 范式上的性能。\n结果表明，JEPAs 在 EEG 信号编码中具有潜力，空间过滤对准确下游分类至关重要，而预训练示例的长度会影响性能，但掩码大小则无显著影响。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2403.11772v2",
      "published_date": "2024-03-18 13:30:12 UTC",
      "updated_date": "2024-10-07 20:07:53 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T16:41:32.511869"
    },
    {
      "arxiv_id": "2403.14711v1",
      "title": "Human-in-the-Loop AI for Cheating Ring Detection",
      "title_zh": "翻译失败",
      "authors": [
        "Yong-Siang Shih",
        "Manqian Liao",
        "Ruidong Liu",
        "Mirza Basim Baig"
      ],
      "abstract": "Online exams have become popular in recent years due to their accessibility.\nHowever, some concerns have been raised about the security of the online exams,\nparticularly in the context of professional cheating services aiding malicious\ntest takers in passing exams, forming so-called \"cheating rings\". In this\npaper, we introduce a human-in-the-loop AI cheating ring detection system\ndesigned to detect and deter these cheating rings. We outline the underlying\nlogic of this human-in-the-loop AI system, exploring its design principles\ntailored to achieve its objectives of detecting cheaters. Moreover, we\nillustrate the methodologies used to evaluate its performance and fairness,\naiming to mitigate the unintended risks associated with the AI system. The\ndesign and development of the system adhere to Responsible AI (RAI) standards,\nensuring that ethical considerations are integrated throughout the entire\ndevelopment process.",
      "tldr_zh": "该论文针对在线考试中由专业作弊服务形成的“cheating rings”问题，提出了一种 human-in-the-loop AI 系统，用于检测和阻止这些作弊行为。系统设计遵循特定逻辑和原则，旨在提升检测准确性，同时通过评估方法确保其性能和公平性，以缓解潜在风险。该系统在整个开发过程中融入 Responsible AI (RAI) 标准，强调伦理考虑和负责任的 AI 应用。",
      "categories": [
        "cs.CY",
        "cs.AI",
        "cs.HC",
        "cs.LG"
      ],
      "primary_category": "cs.CY",
      "comment": "Accepted to the AI4Ed Workshop at AAAI 2024 as a short paper",
      "pdf_url": "http://arxiv.org/pdf/2403.14711v1",
      "published_date": "2024-03-18 13:25:57 UTC",
      "updated_date": "2024-03-18 13:25:57 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T16:41:42.321021"
    },
    {
      "arxiv_id": "2403.13018v1",
      "title": "Invisible Backdoor Attack Through Singular Value Decomposition",
      "title_zh": "翻译失败",
      "authors": [
        "Wenmin Chen",
        "Xiaowei Xu"
      ],
      "abstract": "With the widespread application of deep learning across various domains,\nconcerns about its security have grown significantly. Among these, backdoor\nattacks pose a serious security threat to deep neural networks (DNNs). In\nrecent years, backdoor attacks on neural networks have become increasingly\nsophisticated, aiming to compromise the security and trustworthiness of models\nby implanting hidden, unauthorized functionalities or triggers, leading to\nmisleading predictions or behaviors. To make triggers less perceptible and\nimperceptible, various invisible backdoor attacks have been proposed. However,\nmost of them only consider invisibility in the spatial domain, making it easy\nfor recent defense methods to detect the generated toxic images.To address\nthese challenges, this paper proposes an invisible backdoor attack called DEBA.\nDEBA leverages the mathematical properties of Singular Value Decomposition\n(SVD) to embed imperceptible backdoors into models during the training phase,\nthereby causing them to exhibit predefined malicious behavior under specific\ntrigger conditions. Specifically, we first perform SVD on images, and then\nreplace the minor features of trigger images with those of clean images, using\nthem as triggers to ensure the effectiveness of the attack. As minor features\nare scattered throughout the entire image, the major features of clean images\nare preserved, making poisoned images visually indistinguishable from clean\nones. Extensive experimental evaluations demonstrate that DEBA is highly\neffective, maintaining high perceptual quality and a high attack success rate\nfor poisoned images. Furthermore, we assess the performance of DEBA under\nexisting defense measures, showing that it is robust and capable of\nsignificantly evading and resisting the effects of these defense measures.",
      "tldr_zh": "这篇论文针对深度神经网络（DNNs）的后门攻击（backdoor attacks）安全威胁，提出了一种名为 DEBA 的隐形攻击方法，利用 Singular Value Decomposition (SVD) 在训练阶段嵌入不可察觉的后门。DEBA 通过对图像进行 SVD 分解，并替换触发图像的次要特征（minor features），从而保留图像的主要特征，确保中毒图像与清洁图像在视觉上难以区分。实验评估显示，DEBA 实现了高攻击成功率和高感知质量，并能有效抵抗现有的防御措施。",
      "categories": [
        "cs.CR",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CR",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2403.13018v1",
      "published_date": "2024-03-18 13:25:12 UTC",
      "updated_date": "2024-03-18 13:25:12 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T16:41:55.086772"
    },
    {
      "arxiv_id": "2403.11755v3",
      "title": "Meta-Prompting for Automating Zero-shot Visual Recognition with LLMs",
      "title_zh": "翻译失败",
      "authors": [
        "M. Jehanzeb Mirza",
        "Leonid Karlinsky",
        "Wei Lin",
        "Sivan Doveh",
        "Jakub Micorek",
        "Mateusz Kozinski",
        "Hilde Kuehne",
        "Horst Possegger"
      ],
      "abstract": "Prompt ensembling of Large Language Model (LLM) generated category-specific\nprompts has emerged as an effective method to enhance zero-shot recognition\nability of Vision-Language Models (VLMs). To obtain these category-specific\nprompts, the present methods rely on hand-crafting the prompts to the LLMs for\ngenerating VLM prompts for the downstream tasks. However, this requires\nmanually composing these task-specific prompts and still, they might not cover\nthe diverse set of visual concepts and task-specific styles associated with the\ncategories of interest. To effectively take humans out of the loop and\ncompletely automate the prompt generation process for zero-shot recognition, we\npropose Meta-Prompting for Visual Recognition (MPVR). Taking as input only\nminimal information about the target task, in the form of its short natural\nlanguage description, and a list of associated class labels, MPVR automatically\nproduces a diverse set of category-specific prompts resulting in a strong\nzero-shot classifier. MPVR generalizes effectively across various popular\nzero-shot image recognition benchmarks belonging to widely different domains\nwhen tested with multiple LLMs and VLMs. For example, MPVR obtains a zero-shot\nrecognition improvement over CLIP by up to 19.8% and 18.2% (5.0% and 4.5% on\naverage over 20 datasets) leveraging GPT and Mixtral LLMs, respectively",
      "tldr_zh": "本研究提出 Meta-Prompting for Visual Recognition (MPVR)，一种自动化方法，用于生成类别特定提示，从而提升 Vision-Language Models (VLMs) 的零-shot识别能力，而无需手动设计提示。MPVR 仅需任务的简短自然语言描述和相关类标签作为输入，就能自动创建多样化的提示，形成高效的零-shot分类器。该方法在多种零-shot图像识别基准上表现出色，例如与 GPT 和 Mixtral LLMs 结合时，比 CLIP 模型平均提高了 5.0% 和 4.5% 的性能（在 20 个数据集上）。总之，MPVR 有效自动化了提示生成过程，提升了跨领域的视觉识别准确性。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "ECCV Camera Ready. Code & Data:\n  https://jmiemirza.github.io/Meta-Prompting/",
      "pdf_url": "http://arxiv.org/pdf/2403.11755v3",
      "published_date": "2024-03-18 13:03:24 UTC",
      "updated_date": "2024-08-07 06:05:42 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T16:42:09.119963"
    },
    {
      "arxiv_id": "2403.11734v2",
      "title": "Learning More Expressive General Policies for Classical Planning Domains",
      "title_zh": "翻译失败",
      "authors": [
        "Simon Ståhlberg",
        "Blai Bonet",
        "Hector Geffner"
      ],
      "abstract": "GNN-based approaches for learning general policies across planning domains\nare limited by the expressive power of $C_2$, namely; first-order logic with\ntwo variables and counting. This limitation can be overcame by transitioning to\n$k$-GNNs, for $k=3$, wherein object embeddings are substituted with triplet\nembeddings. Yet, while $3$-GNNs have the expressive power of $C_3$, unlike $1$-\nand $2$-GNNs that are confined to $C_2$, they require quartic time for message\nexchange and cubic space to store embeddings, rendering them infeasible in\npractice. In this work, we introduce a parameterized version R-GNN[$t$] (with\nparameter $t$) of Relational GNNs. Unlike GNNs, that are designed to perform\ncomputation on graphs, Relational GNNs are designed to do computation on\nrelational structures. When $t=\\infty$, R-GNN[$t$] approximates $3$-GNNs over\ngraphs, but using only quadratic space for embeddings. For lower values of $t$,\nsuch as $t=1$ and $t=2$, R-GNN[$t$] achieves a weaker approximation by\nexchanging fewer messages, yet interestingly, often yield the expressivity\nrequired in several planning domains. Furthermore, the new R-GNN[$t$]\narchitecture is the original R-GNN architecture with a suitable transformation\napplied to the inputs only. Experimental results illustrate the clear\nperformance gains of R-GNN[$1$] over the plain R-GNNs, and also over Edge\nTransformers that also approximate $3$-GNNs.",
      "tldr_zh": "这篇论文针对基于 GNN 的通用策略学习在经典规划领域中受限于 C2（一阶逻辑与两个变量和计数）的表达能力问题，提出了一种参数化版本的 Relational GNN，即 R-GNN[$t$]。R-GNN[$t$] 通过减少消息交换和使用二次空间近似 3-GNNs 的功能，当 $t=\\infty$ 时可模拟 3-GNNs，而对于较小的 $t$ 值（如 $t=1$ 或 $t=2$），它在保持必要表达能力的同时降低了计算开销。实验结果显示，R-GNN[$1$] 比原版 R-GNN 和 Edge Transformers 在多个规划领域中表现出显著性能提升，为更高效的规划策略学习提供了新途径。",
      "categories": [
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "Proceedings of the 39th AAAI Conference on Artificial Intelligence\n  (AAAI-25)",
      "pdf_url": "http://arxiv.org/pdf/2403.11734v2",
      "published_date": "2024-03-18 12:42:53 UTC",
      "updated_date": "2025-02-18 14:42:02 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T16:42:19.140904"
    },
    {
      "arxiv_id": "2403.14710v1",
      "title": "Use of recommendation models to provide support to dyslexic students",
      "title_zh": "使用推荐模型为阅读障碍学生提供支持",
      "authors": [
        "Gianluca Morciano",
        "José Manuel Alcalde-Llergo",
        "Andrea Zingoni",
        "Enrique Yeguas-Bolivar",
        "Juri Taborri",
        "Giuseppe Calabrò"
      ],
      "abstract": "Dyslexia is the most widespread specific learning disorder and significantly\nimpair different cognitive domains. This, in turn, negatively affects dyslexic\nstudents during their learning path. Therefore, specific support must be given\nto these students. In addition, such a support must be highly personalized,\nsince the problems generated by the disorder can be very different from one to\nanother. In this work, we explored the possibility of using AI to suggest the\nmost suitable supporting tools for dyslexic students, so as to provide a\ntargeted help that can be of real utility. To do this, we relied on\nrecommendation algorithms, which are a branch of machine learning, that aim to\ndetect personal preferences and provide the most suitable suggestions. We hence\nimplemented and trained three collaborative-filtering recommendation models,\nnamely an item-based, a user-based and a weighted-hybrid model, and studied\ntheir performance on a large database of 1237 students' information, collected\nwith a self-evaluating questionnaire regarding all the most used supporting\nstrategies and digital tools. Each recommendation model was tested with three\ndifferent similarity metrics, namely Pearson correlation, Euclidean distance\nand Cosine similarity. The obtained results showed that a recommendation system\nis highly effective in suggesting the optimal help tools/strategies for\neveryone. This demonstrates that the proposed approach is successful and can be\nused as a new and effective methodology to support students with dyslexia.",
      "tldr_zh": "本研究针对阅读障碍（Dyslexia）学生面临的认知和学习挑战，提出使用 recommendation models 提供个性化支持工具，以解决个体差异问题。研究实现了三种 collaborative-filtering 模型，包括 item-based、user-based 和 weighted-hybrid 模型，并结合 Pearson correlation、Euclidean distance 和 Cosine similarity 等相似性指标，在包含1237名学生的数据库上进行训练和测试。结果表明，这些模型在推荐最佳支持策略方面表现出色，准确率高，为阅读障碍学生的辅助提供了有效的新方法。",
      "categories": [
        "cs.CY",
        "cs.AI"
      ],
      "primary_category": "cs.CY",
      "comment": "36 pages, 4 figures and 6 tables. Preprint submitted to Expert\n  Systems with Applications",
      "pdf_url": "http://arxiv.org/pdf/2403.14710v1",
      "published_date": "2024-03-18 12:12:38 UTC",
      "updated_date": "2024-03-18 12:12:38 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T16:42:32.096075"
    },
    {
      "arxiv_id": "2403.11703v1",
      "title": "LLaVA-UHD: an LMM Perceiving Any Aspect Ratio and High-Resolution Images",
      "title_zh": "翻译失败",
      "authors": [
        "Ruyi Xu",
        "Yuan Yao",
        "Zonghao Guo",
        "Junbo Cui",
        "Zanlin Ni",
        "Chunjiang Ge",
        "Tat-Seng Chua",
        "Zhiyuan Liu",
        "Maosong Sun",
        "Gao Huang"
      ],
      "abstract": "Visual encoding constitutes the basis of large multimodal models (LMMs) in\nunderstanding the visual world. Conventional LMMs process images in fixed sizes\nand limited resolutions, while recent explorations in this direction are\nlimited in adaptivity, efficiency, and even correctness. In this work, we first\ntake GPT-4V and LLaVA-1.5 as representative examples and expose systematic\nflaws rooted in their visual encoding strategy. To address the challenges, we\npresent LLaVA-UHD, a large multimodal model that can efficiently perceive\nimages in any aspect ratio and high resolution. LLaVA-UHD includes three key\ncomponents: (1) An image modularization strategy that divides native-resolution\nimages into smaller variable-sized slices for efficient and extensible\nencoding, (2) a compression module that further condenses image tokens from\nvisual encoders, and (3) a spatial schema to organize slice tokens for LLMs.\nComprehensive experiments show that LLaVA-UHD outperforms established LMMs\ntrained with 2-3 orders of magnitude more data on 9 benchmarks. Notably, our\nmodel built on LLaVA-1.5 336x336 supports 6 times larger (i.e., 672x1088)\nresolution images using only 94% inference computation, and achieves 6.4\naccuracy improvement on TextVQA. Moreover, the model can be efficiently trained\nin academic settings, within 23 hours on 8 A100 GPUs (vs. 26 hours of\nLLaVA-1.5). We make the data and code publicly available at\nhttps://github.com/thunlp/LLaVA-UHD.",
      "tldr_zh": "这篇论文揭示了现有大模态模型 (LMMs) 如 GPT-4V 和 LLaVA-1.5 在视觉编码中存在的系统性缺陷，这些模型仅处理固定大小和有限分辨率的图像。作者提出 LLaVA-UHD，一种高效的多模态模型，通过图像模块化策略（将原分辨率图像分成可变大小切片）、压缩模块（进一步浓缩图像标记）和空间模式（组织切片标记）来支持任意宽高比和高分辨率图像的感知。实验结果显示，LLaVA-UHD 在9个基准测试中优于使用2-3个数量级更多训练数据的模型，并在 TextVQA 上准确率提升6.4%，同时在推理时仅需94%的计算量且训练效率更高（8个 A100 GPU 上只需23小时）。这项工作公开了数据和代码，促进了 LMMs 的进一步发展。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "Preprint",
      "pdf_url": "http://arxiv.org/pdf/2403.11703v1",
      "published_date": "2024-03-18 12:04:11 UTC",
      "updated_date": "2024-03-18 12:04:11 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T16:42:48.210750"
    },
    {
      "arxiv_id": "2403.11671v1",
      "title": "HDLdebugger: Streamlining HDL debugging with Large Language Models",
      "title_zh": "翻译失败",
      "authors": [
        "Xufeng Yao",
        "Haoyang Li",
        "Tsz Ho Chan",
        "Wenyi Xiao",
        "Mingxuan Yuan",
        "Yu Huang",
        "Lei Chen",
        "Bei Yu"
      ],
      "abstract": "In the domain of chip design, Hardware Description Languages (HDLs) play a\npivotal role. However, due to the complex syntax of HDLs and the limited\navailability of online resources, debugging HDL codes remains a difficult and\ntime-intensive task, even for seasoned engineers. Consequently, there is a\npressing need to develop automated HDL code debugging models, which can\nalleviate the burden on hardware engineers. Despite the strong capabilities of\nLarge Language Models (LLMs) in generating, completing, and debugging software\ncode, their utilization in the specialized field of HDL debugging has been\nlimited and, to date, has not yielded satisfactory results. In this paper, we\npropose an LLM-assisted HDL debugging framework, namely HDLdebugger, which\nconsists of HDL debugging data generation via a reverse engineering approach, a\nsearch engine for retrieval-augmented generation, and a retrieval-augmented LLM\nfine-tuning approach. Through the integration of these components, HDLdebugger\ncan automate and streamline HDL debugging for chip design. Our comprehensive\nexperiments, conducted on an HDL code dataset sourced from Huawei, reveal that\nHDLdebugger outperforms 13 cutting-edge LLM baselines, displaying exceptional\neffectiveness in HDL code debugging.",
      "tldr_zh": "该论文针对硬件描述语言（HDL）代码调试的复杂性和资源不足问题，提出了一种基于 Large Language Models (LLMs) 的自动化框架 HDLdebugger，以减轻硬件工程师的负担。该框架包括通过逆向工程生成调试数据、检索增强生成（retrieval-augmented generation）的搜索引擎，以及对 LLM 进行检索增强微调的模块，从而简化 HDL 代码的调试过程。在华为的 HDL 代码数据集上进行的实验显示，HDLdebugger 优于 13 个前沿 LLM 基线，在 HDL 调试任务中表现出卓越的性能。",
      "categories": [
        "cs.AR",
        "cs.AI",
        "cs.CE",
        "cs.LG",
        "cs.SE"
      ],
      "primary_category": "cs.AR",
      "comment": "13 pages,5 figures",
      "pdf_url": "http://arxiv.org/pdf/2403.11671v1",
      "published_date": "2024-03-18 11:19:37 UTC",
      "updated_date": "2024-03-18 11:19:37 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T16:42:57.909820"
    },
    {
      "arxiv_id": "2403.12114v1",
      "title": "Safety Analysis of Autonomous Railway Systems: An Introduction to the SACRED Methodology",
      "title_zh": "翻译失败",
      "authors": [
        "Josh Hunter",
        "John McDermid",
        "Simon Burton"
      ],
      "abstract": "As the railway industry increasingly seeks to introduce autonomy and machine\nlearning (ML), several questions arise. How can safety be assured for such\nsystems and technologies? What is the applicability of current safety standards\nwithin this new technological landscape? What are the key metrics to classify a\nsystem as safe? Currently, safety analysis for the railway reflects the failure\nmodes of existing technology; in contrast, the primary concern of analysis of\nautomation is typically average performance. Such purely statistical approaches\nto measuring ML performance are limited, as they may overlook classes of\nsituations that may occur rarely but in which the function performs\nconsistently poorly. To combat these difficulties we introduce SACRED, a safety\nmethodology for producing an initial safety case and determining important\nsafety metrics for autonomous systems. The development of SACRED is motivated\nby the proposed GoA-4 light-rail system in Berlin.",
      "tldr_zh": "随着铁路行业引入自治和机器学习（ML），安全分析面临挑战，包括现有标准适用的局限性和统计方法忽略罕见高风险场景的问题。SACRED方法论应运而生，作为一种安全框架，用于生成初始安全案例并定义自主系统的重要安全指标。该方法强调全面评估系统性能，而非单纯依赖平均表现，并以柏林拟议的GoA-4轻轨系统为实际动机，推动了铁路安全分析的创新。",
      "categories": [
        "cs.SE",
        "cs.AI"
      ],
      "primary_category": "cs.SE",
      "comment": "S. Bernardi, T. Zoppi (Editors), \"Fast Abstracts and Student Forum\n  Proceedings - EDCC 2024 - 19th European Dependable Computing Conference,\n  Leuven, Belgium, 8-11 April 2024\"",
      "pdf_url": "http://arxiv.org/pdf/2403.12114v1",
      "published_date": "2024-03-18 11:12:19 UTC",
      "updated_date": "2024-03-18 11:12:19 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T16:43:10.693609"
    },
    {
      "arxiv_id": "2403.13848v2",
      "title": "Smooth Sensitivity for Learning Differentially-Private yet Accurate Rule Lists",
      "title_zh": "翻译失败",
      "authors": [
        "Timothée Ly",
        "Julien Ferry",
        "Marie-José Huguet",
        "Sébastien Gambs",
        "Ulrich Aivodji"
      ],
      "abstract": "Differentially-private (DP) mechanisms can be embedded into the design of a\nmachine learning algorithm to protect the resulting model against privacy\nleakage. However, this often comes with a significant loss of accuracy due to\nthe noise added to enforce DP. In this paper, we aim at improving this\ntrade-off for a popular class of machine learning algorithms leveraging the\nGini impurity as an information gain criterion to greedily build interpretable\nmodels such as decision trees or rule lists. To this end, we establish the\nsmooth sensitivity of the Gini impurity, which can be used to obtain thorough\nDP guarantees while adding noise scaled with tighter magnitude. We illustrate\nthe applicability of this mechanism by integrating it within a greedy algorithm\nproducing rule list models, motivated by the fact that such models remain\nunderstudied in the DP literature. Our theoretical analysis and experimental\nresults confirm that the DP rule lists models integrating smooth sensitivity\nhave higher accuracy that those using other DP frameworks based on global\nsensitivity, for identical privacy budgets.",
      "tldr_zh": "这篇论文针对机器学习算法中嵌入差分隐私（DP）机制导致准确性下降的问题，提出了利用平滑敏感度（Smooth Sensitivity）来优化噪声添加策略。研究者建立了Gini杂质作为信息增益标准的平滑敏感度，并将其整合到一个贪婪算法中，用于生成解释性强的规则列表模型。实验结果表明，这种方法在相同的隐私预算下，比基于全局敏感度的DP框架实现了更高的准确性，从而改善了DP模型的隐私与性能权衡。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CR"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2403.13848v2",
      "published_date": "2024-03-18 10:44:22 UTC",
      "updated_date": "2024-11-12 09:21:13 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T16:43:21.505863"
    },
    {
      "arxiv_id": "2403.11642v1",
      "title": "Guiding the generation of counterfactual explanations through temporal background knowledge for Predictive Process Monitoring",
      "title_zh": "通过时间背景知识指导反事实解释的生成，用于预测过程监控",
      "authors": [
        "Andrei Buliga",
        "Chiara Di Francescomarino",
        "Chiara Ghidini",
        "Ivan Donadello",
        "Fabrizio Maria Maggi"
      ],
      "abstract": "Counterfactual explanations suggest what should be different in the input\ninstance to change the outcome of an AI system. When dealing with\ncounterfactual explanations in the field of Predictive Process Monitoring,\nhowever, control flow relationships among events have to be carefully\nconsidered. A counterfactual, indeed, should not violate control flow\nrelationships among activities (temporal background knowledege). Within the\nfield of Explainability in Predictive Process Monitoring, there have been a\nseries of works regarding counterfactual explanations for outcome-based\npredictions. However, none of them consider the inclusion of temporal\nbackground knowledge when generating these counterfactuals. In this work, we\nadapt state-of-the-art techniques for counterfactual generation in the domain\nof XAI that are based on genetic algorithms to consider a series of temporal\nconstraints at runtime. We assume that this temporal background knowledge is\ngiven, and we adapt the fitness function, as well as the crossover and mutation\noperators, to maintain the satisfaction of the constraints. The proposed\nmethods are evaluated with respect to state-of-the-art genetic algorithms for\ncounterfactual generation and the results are presented. We showcase that the\ninclusion of temporal background knowledge allows the generation of\ncounterfactuals more conformant to the temporal background knowledge, without\nhowever losing in terms of the counterfactual traditional quality metrics.",
      "tldr_zh": "本文提出了一种在 Predictive Process Monitoring 领域生成 counterfactual explanations 的新方法，通过整合 temporal background knowledge 来指导生成过程，以避免违反事件间的控制流关系。作者改进了基于 genetic algorithms 的状态-of-the-art 技术，具体包括调整 fitness function、crossover 和 mutation operators，以确保 temporal constraints 在运行时得到满足。该方法在实验中与现有算法比较后，展示了生成的 counterfactuals 更符合 temporal background knowledge，同时在传统质量指标上保持了竞争力。",
      "categories": [
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2403.11642v1",
      "published_date": "2024-03-18 10:34:40 UTC",
      "updated_date": "2024-03-18 10:34:40 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T16:43:33.680041"
    },
    {
      "arxiv_id": "2403.11626v1",
      "title": "QEAN: Quaternion-Enhanced Attention Network for Visual Dance Generation",
      "title_zh": "QEAN：四元数增强注意力网络用于视觉舞蹈生成",
      "authors": [
        "Zhizhen Zhou",
        "Yejing Huo",
        "Guoheng Huang",
        "An Zeng",
        "Xuhang Chen",
        "Lian Huang",
        "Zinuo Li"
      ],
      "abstract": "The study of music-generated dance is a novel and challenging Image\ngeneration task. It aims to input a piece of music and seed motions, then\ngenerate natural dance movements for the subsequent music. Transformer-based\nmethods face challenges in time series prediction tasks related to human\nmovements and music due to their struggle in capturing the nonlinear\nrelationship and temporal aspects. This can lead to issues like joint\ndeformation, role deviation, floating, and inconsistencies in dance movements\ngenerated in response to the music. In this paper, we propose a\nQuaternion-Enhanced Attention Network (QEAN) for visual dance synthesis from a\nquaternion perspective, which consists of a Spin Position Embedding (SPE)\nmodule and a Quaternion Rotary Attention (QRA) module. First, SPE embeds\nposition information into self-attention in a rotational manner, leading to\nbetter learning of features of movement sequences and audio sequences, and\nimproved understanding of the connection between music and dance. Second, QRA\nrepresents and fuses 3D motion features and audio features in the form of a\nseries of quaternions, enabling the model to better learn the temporal\ncoordination of music and dance under the complex temporal cycle conditions of\ndance generation. Finally, we conducted experiments on the dataset AIST++, and\nthe results show that our approach achieves better and more robust performance\nin generating accurate, high-quality dance movements. Our source code and\ndataset can be available from https://github.com/MarasyZZ/QEAN and\nhttps://google.github.io/aistplusplus_dataset respectively.",
      "tldr_zh": "这篇论文针对音乐生成舞蹈任务中Transformer-based方法的不足（如捕捉非线性关系和时间方面的问题，导致舞蹈动作变形或不协调），提出了Quaternion-Enhanced Attention Network (QEAN)。QEAN包括Spin Position Embedding (SPE)模块，用于旋转方式嵌入位置信息以提升动作序列和音频序列的特征学习，以及Quaternion Rotary Attention (QRA)模块，用于以四元数形式融合3D动作特征和音频特征，从而更好地学习音乐与舞蹈的时间协调。在AIST++数据集上的实验结果表明，QEAN在生成准确、高质量的舞蹈动作方面性能更优，并提供了开源代码。",
      "categories": [
        "cs.GR",
        "cs.AI",
        "cs.CV",
        "cs.MM",
        "cs.SD",
        "eess.AS"
      ],
      "primary_category": "cs.GR",
      "comment": "Accepted by The Visual Computer Journal",
      "pdf_url": "http://arxiv.org/pdf/2403.11626v1",
      "published_date": "2024-03-18 09:58:43 UTC",
      "updated_date": "2024-03-18 09:58:43 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T16:43:47.081759"
    },
    {
      "arxiv_id": "2403.13847v2",
      "title": "Optimal Transport for Domain Adaptation through Gaussian Mixture Models",
      "title_zh": "翻译失败",
      "authors": [
        "Eduardo Fernandes Montesuma",
        "Fred Maurice Ngolè Mboula",
        "Antoine Souloumiac"
      ],
      "abstract": "Machine learning systems operate under the assumption that training and test\ndata are sampled from a fixed probability distribution. However, this\nassumptions is rarely verified in practice, as the conditions upon which data\nwas acquired are likely to change. In this context, the adaptation of the\nunsupervised domain requires minimal access to the data of the new conditions\nfor learning models robust to changes in the data distribution. Optimal\ntransport is a theoretically grounded tool for analyzing changes in\ndistribution, especially as it allows the mapping between domains. However,\nthese methods are usually computationally expensive as their complexity scales\ncubically with the number of samples. In this work, we explore optimal\ntransport between Gaussian Mixture Models (GMMs), which is conveniently written\nin terms of the components of source and target GMMs. We experiment with 9\nbenchmarks, with a total of $85$ adaptation tasks, showing that our methods are\nmore efficient than previous shallow domain adaptation methods, and they scale\nwell with number of samples $n$ and dimensions $d$.",
      "tldr_zh": "本研究针对机器学习中训练和测试数据分布变化的问题，提出了一种基于 Optimal Transport 和 Gaussian Mixture Models (GMMs) 的无监督域适应方法。该方法通过分析源和目标 GMMs 的组件来实现高效的分布映射，避免了传统 Optimal Transport 方法的立方级计算复杂度。在9个基准数据集上的85个适应任务实验中，该方法比之前的浅层域适应方法更高效，并能很好地扩展到样本数 n 和维度 d。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "stat.ML"
      ],
      "primary_category": "cs.LG",
      "comment": "29 pages, 9 figures, 8 tables, accepted at Transactions on Machine\n  Learning Research",
      "pdf_url": "http://arxiv.org/pdf/2403.13847v2",
      "published_date": "2024-03-18 09:32:33 UTC",
      "updated_date": "2025-01-22 12:47:49 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T16:43:57.204352"
    },
    {
      "arxiv_id": "2404.07948v1",
      "title": "Usability and Performance Analysis of Embedded Development Environment for On-device Learning",
      "title_zh": "翻译失败",
      "authors": [
        "Enzo Scaffi",
        "Antoine Bonneau",
        "Frédéric Le Mouël",
        "Fabien Mieyeville"
      ],
      "abstract": "This research empirically examines embedded development tools viable for\non-device TinyML implementation. The research evaluates various development\ntools with various abstraction levels on resource-constrained IoT devices, from\nbasic hardware manipulation to deployment of minimalistic ML training. The\nanalysis encompasses memory usage, energy consumption, and performance metrics\nduring model training and inference and usability of the different solutions.\nArduino Framework offers ease of implementation but with increased energy\nconsumption compared to the native option, while RIOT OS exhibits efficient\nenergy consumption despite higher memory utilization with equivalent ease of\nuse. The absence of certain critical functionalities like DVFS directly\nintegrated into the OS highlights limitations for fine hardware control.",
      "tldr_zh": "本研究实证评估了多种嵌入式开发工具在资源受限 IoT 设备上用于设备端 TinyML 实现的可用性和性能，涵盖从硬件操作到最小化 ML 训练的各个抽象级别，并分析了内存使用、能源消耗、性能指标以及工具易用性。结果表明，Arduino Framework 提供简便的实施方式，但能源消耗较高；相比之下，RIOT OS 展现出更高效的能源消耗，尽管内存利用率较高。研究还指出了现有工具的局限性，如缺少直接集成 DVFS 等关键功能，为优化嵌入式开发环境提供了宝贵见解。",
      "categories": [
        "cs.SE",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.SE",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2404.07948v1",
      "published_date": "2024-03-18 09:26:04 UTC",
      "updated_date": "2024-03-18 09:26:04 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T16:44:10.975052"
    },
    {
      "arxiv_id": "2403.11598v2",
      "title": "Optimal Layout Synthesis for Deep Quantum Circuits on NISQ Processors with 100+ Qubits",
      "title_zh": "针对具有 100+ 量子比特的 NISQ 处理器的深度量子电路的最佳布局合成",
      "authors": [
        "Irfansha Shaik",
        "Jaco van de Pol"
      ],
      "abstract": "Layout synthesis is mapping a quantum circuit to a quantum processor. SWAP\ngate insertions are needed for scheduling 2-qubit gates only on connected\nphysical qubits. With the ever-increasing number of qubits in NISQ processors,\nscalable layout synthesis is of utmost importance. With large optimality gaps\nobserved in heuristic approaches, scalable exact methods are needed. While\nrecent exact and near-optimal approaches scale to moderate circuits, large deep\ncircuits are still out of scope.\n  In this work, we propose a SAT encoding based on parallel plans that apply 1\nSWAP and a group of CNOTs at each time step. Using domain-specific information,\nwe maintain optimality in parallel plans while scaling to large and deep\ncircuits. From our results, we show the scalability of our approach which\nsignificantly outperforms leading exact and near-optimal approaches (up to\n100x). For the first time, we can optimally map several 8, 14, and 16 qubit\ncircuits onto 54, 80, and 127 qubit platforms with up to 17 SWAPs. While adding\noptimal SWAPs, we also report near-optimal depth in our mapped circuits.",
      "tldr_zh": "本文提出了一种基于 SAT 编码的并行计划方法，用于优化深度量子电路在拥有 100+ 量子比特的 NISQ 处理器上的布局合成，该方法每步应用 1 个 SWAP 门和一组 CNOTs，并利用领域特定信息确保可扩展性和最优性。相比现有精确和近似最优方法，该方法性能提升高达 100 倍，首次实现了对 8、14 和 16 量子比特电路在 54、80 和 127 量子比特平台上的最优映射，使用最多 17 个 SWAPs，同时保持近似最优电路深度。该研究为大规模量子电路映射提供了高效且可靠的解决方案。",
      "categories": [
        "quant-ph",
        "cs.AI"
      ],
      "primary_category": "quant-ph",
      "comment": "7 Figures, 4 Tables, 1 Listing",
      "pdf_url": "http://arxiv.org/pdf/2403.11598v2",
      "published_date": "2024-03-18 09:19:01 UTC",
      "updated_date": "2024-07-22 12:00:02 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T16:44:23.850678"
    },
    {
      "arxiv_id": "2403.11585v3",
      "title": "Linguacodus: A Synergistic Framework for Transformative Code Generation in Machine Learning Pipelines",
      "title_zh": "翻译失败",
      "authors": [
        "Ekaterina Trofimova",
        "Emil Sataev",
        "Andrey E. Ustyuzhanin"
      ],
      "abstract": "In the ever-evolving landscape of machine learning, seamless translation of\nnatural language descriptions into executable code remains a formidable\nchallenge. This paper introduces Linguacodus, an innovative framework designed\nto tackle this challenge by deploying a dynamic pipeline that iteratively\ntransforms natural language task descriptions into code through high-level\ndata-shaping instructions. The core of Linguacodus is a fine-tuned large\nlanguage model (LLM), empowered to evaluate diverse solutions for various\nproblems and select the most fitting one for a given task. This paper details\nthe fine-tuning process, and sheds light on how natural language descriptions\ncan be translated into functional code. Linguacodus represents a substantial\nleap towards automated code generation, effectively bridging the gap between\ntask descriptions and executable code. It holds great promise for advancing\nmachine learning applications across diverse domains. Additionally, we propose\nan algorithm capable of transforming a natural description of an ML task into\ncode with minimal human interaction. In extensive experiments on a vast machine\nlearning code dataset originating from Kaggle, we showcase the effectiveness of\nLinguacodus. The investigations highlight its potential applications across\ndiverse domains, emphasizing its impact on applied machine learning in various\nscientific fields.",
      "tldr_zh": "本研究引入了Linguacodus框架，这是一个协同框架，旨在通过动态管道将自然语言任务描述转化为机器学习管道中的可执行代码。框架的核心是fine-tuned large language model (LLM)，它能评估多种解决方案并选择最合适的，同时利用高层次数据-shaping指令进行迭代转换。该框架还提出一个算法，实现最小人类交互下的ML任务描述到代码的自动转化。在Kaggle的大型机器学习代码数据集上进行的广泛实验证明，Linguacodus显著提升了代码生成效率，并展示了其在多样领域的应用潜力。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL",
        "cs.PL",
        "cs.SE"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2403.11585v3",
      "published_date": "2024-03-18 08:58:47 UTC",
      "updated_date": "2024-11-21 16:28:03 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T16:44:33.219844"
    },
    {
      "arxiv_id": "2403.11558v1",
      "title": "Reinforcement Learning with Token-level Feedback for Controllable Text Generation",
      "title_zh": "翻译失败",
      "authors": [
        "Wendi Li",
        "Wei Wei",
        "Kaihe Xu",
        "Wenfeng Xie",
        "Dangyang Chen",
        "Yu Cheng"
      ],
      "abstract": "To meet the requirements of real-world applications, it is essential to\ncontrol generations of large language models (LLMs). Prior research has tried\nto introduce reinforcement learning (RL) into controllable text generation\nwhile most existing methods suffer from overfitting issues (finetuning-based\nmethods) or semantic collapse (post-processing methods). However, current RL\nmethods are generally guided by coarse-grained (sentence/paragraph-level)\nfeedback, which may lead to suboptimal performance owing to semantic twists or\nprogressions within sentences. To tackle that, we propose a novel reinforcement\nlearning algorithm named TOLE which formulates TOken-LEvel rewards for\ncontrollable text generation, and employs a \"first-quantize-then-noise\"\nparadigm to enhance the robustness of the RL algorithm.Furthermore, TOLE can be\nflexibly extended to multiple constraints with little computational expense.\nExperimental results show that our algorithm can achieve superior performance\non both single-attribute and multi-attribute control tasks. We have released\nour codes at https://github.com/WindyLee0822/CTG",
      "tldr_zh": "这篇论文提出了一种名为TOLE的强化学习(Reinforcement Learning, RL)算法，用于可控文本生成，以解决现有方法在处理大语言模型(LLMs)时面临的过拟合和语义崩溃问题。TOLE通过引入Token-level奖励来提供更精细的反馈，并采用\"first-quantize-then-noise\"范式增强算法的鲁棒性，同时支持灵活扩展到多个约束。实验结果表明，该方法在单属性和多属性控制任务上显著优于基线模型，并开源了代码以便进一步应用。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "Accepted to NAACL 2024 Findings",
      "pdf_url": "http://arxiv.org/pdf/2403.11558v1",
      "published_date": "2024-03-18 08:18:37 UTC",
      "updated_date": "2024-03-18 08:18:37 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T16:44:47.434680"
    },
    {
      "arxiv_id": "2403.11552v3",
      "title": "LLM3:Large Language Model-based Task and Motion Planning with Motion Failure Reasoning",
      "title_zh": "LLM3：基于大型语言模型的任务和运动规划，带有运动失败推理",
      "authors": [
        "Shu Wang",
        "Muzhi Han",
        "Ziyuan Jiao",
        "Zeyu Zhang",
        "Ying Nian Wu",
        "Song-Chun Zhu",
        "Hangxin Liu"
      ],
      "abstract": "Conventional Task and Motion Planning (TAMP) approaches rely on manually\ncrafted interfaces connecting symbolic task planning with continuous motion\ngeneration. These domain-specific and labor-intensive modules are limited in\naddressing emerging tasks in real-world settings. Here, we present LLM^3, a\nnovel Large Language Model (LLM)-based TAMP framework featuring a\ndomain-independent interface. Specifically, we leverage the powerful reasoning\nand planning capabilities of pre-trained LLMs to propose symbolic action\nsequences and select continuous action parameters for motion planning.\nCrucially, LLM^3 incorporates motion planning feedback through prompting,\nallowing the LLM to iteratively refine its proposals by reasoning about motion\nfailure. Consequently, LLM^3 interfaces between task planning and motion\nplanning, alleviating the intricate design process of handling domain-specific\nmessages between them. Through a series of simulations in a box-packing domain,\nwe quantitatively demonstrate the effectiveness of LLM^3 in solving TAMP\nproblems and the efficiency in selecting action parameters. Ablation studies\nunderscore the significant contribution of motion failure reasoning to the\nsuccess of LLM^3. Furthermore, we conduct qualitative experiments on a physical\nmanipulator, demonstrating the practical applicability of our approach in\nreal-world settings.",
      "tldr_zh": "该论文提出 LLM^3，一种基于 Large Language Model (LLM) 的任务和运动规划 (Task and Motion Planning, TAMP) 框架，使用领域无关接口来连接符号任务规划和连续运动生成。框架利用 LLM 的推理能力生成动作序列并选择动作参数，同时通过提示反馈机制处理运动失败，实现迭代优化和改进。实验结果显示，在箱子打包模拟环境中，LLM^3 显著提高了 TAMP 问题的解决效率和准确性，消融研究强调了运动失败推理的关键作用，并在实际机器人操作中验证了其实际应用潜力。",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "comment": "IROS 2024. Codes available: https://github.com/AssassinWS/LLM-TAMP",
      "pdf_url": "http://arxiv.org/pdf/2403.11552v3",
      "published_date": "2024-03-18 08:03:47 UTC",
      "updated_date": "2024-08-21 09:46:35 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T16:44:59.036399"
    },
    {
      "arxiv_id": "2403.11536v1",
      "title": "OCR is All you need: Importing Multi-Modality into Image-based Defect Detection System",
      "title_zh": "OCR 就是",
      "authors": [
        "Chih-Chung Hsu",
        "Chia-Ming Lee",
        "Chun-Hung Sun",
        "Kuang-Ming Wu"
      ],
      "abstract": "Automatic optical inspection (AOI) plays a pivotal role in the manufacturing\nprocess, predominantly leveraging high-resolution imaging instruments for\nscanning purposes. It detects anomalies by analyzing image textures or\npatterns, making it an essential tool in industrial manufacturing and quality\ncontrol. Despite its importance, the deployment of models for AOI often faces\nchallenges. These include limited sample sizes, which hinder effective feature\nlearning, variations among source domains, and sensitivities to changes in\nlighting and camera positions during imaging. These factors collectively\ncompromise the accuracy of model predictions. Traditional AOI often fails to\ncapitalize on the rich mechanism-parameter information from machines or inside\nimages, including statistical parameters, which typically benefit AOI\nclassification. To address this, we introduce an external modality-guided data\nmining framework, primarily rooted in optical character recognition (OCR), to\nextract statistical features from images as a second modality to enhance\nperformance, termed OANet (Ocr-Aoi-Net). A key aspect of our approach is the\nalignment of external modality features, extracted using a single\nmodality-aware model, with image features encoded by a convolutional neural\nnetwork. This synergy enables a more refined fusion of semantic representations\nfrom different modalities. We further introduce feature refinement and a gating\nfunction in our OANet to optimize the combination of these features, enhancing\ninference and decision-making capabilities. Experimental outcomes show that our\nmethodology considerably boosts the recall rate of the defect detection model\nand maintains high robustness even in challenging scenarios.",
      "tldr_zh": "这篇论文针对自动光学检测 (AOI) 在制造过程中的挑战，如样本规模有限、源域差异以及光照和相机位置变化导致的准确性下降，提出了一种基于光学字符识别 (OCR) 的多模态框架 OANet。OANet 通过 OCR 提取图像中的统计特征作为第二模态，并将其与卷积神经网络编码的图像特征对齐融合，同时引入特征精炼和门控函数来优化语义表示的结合。实验结果表明，该方法显著提升了缺陷检测模型的召回率，并在复杂场景中保持高鲁棒性。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2403.11536v1",
      "published_date": "2024-03-18 07:41:39 UTC",
      "updated_date": "2024-03-18 07:41:39 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T16:45:11.702971"
    },
    {
      "arxiv_id": "2403.13017v1",
      "title": "Impart: An Imperceptible and Effective Label-Specific Backdoor Attack",
      "title_zh": "翻译失败",
      "authors": [
        "Jingke Zhao",
        "Zan Wang",
        "Yongwei Wang",
        "Lanjun Wang"
      ],
      "abstract": "Backdoor attacks have been shown to impose severe threats to real\nsecurity-critical scenarios. Although previous works can achieve high attack\nsuccess rates, they either require access to victim models which may\nsignificantly reduce their threats in practice, or perform visually noticeable\nin stealthiness. Besides, there is still room to improve the attack success\nrates in the scenario that different poisoned samples may have different target\nlabels (a.k.a., the all-to-all setting). In this study, we propose a novel\nimperceptible backdoor attack framework, named Impart, in the scenario where\nthe attacker has no access to the victim model. Specifically, in order to\nenhance the attack capability of the all-to-all setting, we first propose a\nlabel-specific attack. Different from previous works which try to find an\nimperceptible pattern and add it to the source image as the poisoned image, we\nthen propose to generate perturbations that align with the target label in the\nimage feature by a surrogate model. In this way, the generated poisoned images\nare attached with knowledge about the target class, which significantly\nenhances the attack capability.",
      "tldr_zh": "该研究提出了一种名为 Impart 的隐蔽后门攻击框架（backdoor attack），专注于在攻击者无法访问受害者模型（victim model）的情况下，实现高效的标签特定攻击（label-specific attack）。Impart 通过代理模型（surrogate model）生成与目标标签对齐的图像特征扰动，而不是添加可见模式，从而显著提升了 all-to-all 设置中的攻击成功率。实验结果表明，这种方法使中毒图像携带目标类别的知识，进一步提高了攻击的隐蔽性和有效性。",
      "categories": [
        "cs.CR",
        "cs.AI"
      ],
      "primary_category": "cs.CR",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2403.13017v1",
      "published_date": "2024-03-18 07:22:56 UTC",
      "updated_date": "2024-03-18 07:22:56 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T16:45:23.346094"
    },
    {
      "arxiv_id": "2403.15449v3",
      "title": "Hatred Stems from Ignorance! Distillation of the Persuasion Modes in Countering Conversational Hate Speech",
      "title_zh": "翻译失败",
      "authors": [
        "Ghadi Alyahya",
        "Abeer Aldayel"
      ],
      "abstract": "Examining the factors that the counterspeech uses are at the core of\nunderstanding the optimal methods for confronting hate speech online. Various\nstudies have assessed the emotional base factors used in counter speech, such\nas emotional empathy, offensiveness, and hostility. To better understand the\ncounterspeech used in conversations, this study distills persuasion modes into\nreason, emotion, and credibility and evaluates their use in two types of\nconversation interactions: closed (multi-turn) and open (single-turn)\nconcerning racism, sexism, and religious bigotry. The evaluation covers the\ndistinct behaviors seen with human-sourced as opposed to machine-generated\ncounterspeech. It also assesses the interplay between the stance taken and the\nmode of persuasion seen in the counterspeech.\n  Notably, we observe nuanced differences in the counterspeech persuasion modes\nused in open and closed interactions, especially in terms of the topic, with a\ngeneral tendency to use reason as a persuasion mode to express the counterpoint\nto hate comments. The machine-generated counterspeech tends to exhibit an\nemotional persuasion mode, while human counters lean toward reason.\nFurthermore, our study shows that reason tends to obtain more supportive\nreplies than other persuasion modes. The findings highlight the potential for\nincorporating persuasion modes into studies about countering hate speech, as\nthey can serve as an optimal means of explainability and pave the way for the\nfurther adoption of the reply's stance and the role it plays in assessing what\ncomprises the optimal counterspeech.",
      "tldr_zh": "本研究提炼了说服模式（persuasion modes），包括reason（理性）、emotion（情感）和credibility（可信度），以评估其在反驳对话式仇恨言论（conversational hate speech）中的应用，涵盖种族主义、性别歧视和宗教偏见等主题，并比较封闭（多轮）和开放（单轮）互动中人类与机器生成的counterspeech。结果显示，开放互动更倾向于使用reason来表达反驳观点，而机器生成的counterspeech更偏向emotion，人类则更青睐reason；此外，reason模式往往获得更多支持回复。总体而言，该研究强调说服模式可提升counterspeech的可解释性，并为优化反驳策略提供指导。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "Accepted to appear @ ICWSM 2025. The link to the camera-ready paper\n  will be added soon",
      "pdf_url": "http://arxiv.org/pdf/2403.15449v3",
      "published_date": "2024-03-18 07:20:35 UTC",
      "updated_date": "2025-04-14 16:35:50 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T16:45:35.140895"
    },
    {
      "arxiv_id": "2404.16852v1",
      "title": "A Disease Labeler for Chinese Chest X-Ray Report Generation",
      "title_zh": "一种用于中文胸部X光报告生成的疾病标签器",
      "authors": [
        "Mengwei Wang",
        "Ruixin Yan",
        "Zeyi Hou",
        "Ning Lang",
        "Xiuzhuang Zhou"
      ],
      "abstract": "In the field of medical image analysis, the scarcity of Chinese chest X-ray\nreport datasets has hindered the development of technology for generating\nChinese chest X-ray reports. On one hand, the construction of a Chinese chest\nX-ray report dataset is limited by the time-consuming and costly process of\naccurate expert disease annotation. On the other hand, a single natural\nlanguage generation metric is commonly used to evaluate the similarity between\ngenerated and ground-truth reports, while the clinical accuracy and\neffectiveness of the generated reports rely on an accurate disease labeler\n(classifier). To address the issues, this study proposes a disease labeler\ntailored for the generation of Chinese chest X-ray reports. This labeler\nleverages a dual BERT architecture to handle diagnostic reports and clinical\ninformation separately and constructs a hierarchical label learning algorithm\nbased on the affiliation between diseases and body parts to enhance text\nclassification performance. Utilizing this disease labeler, a Chinese chest\nX-ray report dataset comprising 51,262 report samples was established. Finally,\nexperiments and analyses were conducted on a subset of expert-annotated Chinese\nchest X-ray reports, validating the effectiveness of the proposed disease\nlabeler.",
      "tldr_zh": "该研究针对中文胸部X光报告数据集稀缺的问题，提出了一种专为Chinese Chest X-Ray Report Generation设计的疾病标签器，以解决专家标注耗时和评估指标单一的挑战。该标签器采用双BERT架构分别处理诊断报告和临床信息，并构建基于疾病与身体部位关联的层次化标签学习算法，提升文本分类性能。利用该标签器，成功建立了包含51,262个报告样本的中文胸部X光报告数据集；实验在专家标注子集上验证了标签器的有效性，展示了其在临床准确性方面的潜力。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL",
        "eess.IV"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2404.16852v1",
      "published_date": "2024-03-18 07:10:33 UTC",
      "updated_date": "2024-03-18 07:10:33 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T16:45:46.101221"
    },
    {
      "arxiv_id": "2403.11506v1",
      "title": "End-To-End Underwater Video Enhancement: Dataset and Model",
      "title_zh": "端到端水下视频增强：数据集和模型",
      "authors": [
        "Dazhao Du",
        "Enhan Li",
        "Lingyu Si",
        "Fanjiang Xu",
        "Jianwei Niu"
      ],
      "abstract": "Underwater video enhancement (UVE) aims to improve the visibility and frame\nquality of underwater videos, which has significant implications for marine\nresearch and exploration. However, existing methods primarily focus on\ndeveloping image enhancement algorithms to enhance each frame independently.\nThere is a lack of supervised datasets and models specifically tailored for UVE\ntasks. To fill this gap, we construct the Synthetic Underwater Video\nEnhancement (SUVE) dataset, comprising 840 diverse underwater-style videos\npaired with ground-truth reference videos. Based on this dataset, we train a\nnovel underwater video enhancement model, UVENet, which utilizes inter-frame\nrelationships to achieve better enhancement performance. Through extensive\nexperiments on both synthetic and real underwater videos, we demonstrate the\neffectiveness of our approach. This study represents the first comprehensive\nexploration of UVE to our knowledge. The code is available at\nhttps://anonymous.4open.science/r/UVENet.",
      "tldr_zh": "本研究针对水下视频增强（UVE）的问题，强调了现有方法仅关注独立帧图像增强的局限性，并首次构建了Synthetic Underwater Video Enhancement (SUVE)数据集，该数据集包含840个多样化水下风格视频及其ground-truth参考视频。基于此数据集，作者开发了端到端的UVENet模型，该模型利用inter-frame relationships来提升视频增强性能。实验在合成和真实水下视频上验证了方法的有效性，展示了其在海洋研究和探索中的潜力，这也是首个全面探索UVE的系统性研究。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2403.11506v1",
      "published_date": "2024-03-18 06:24:46 UTC",
      "updated_date": "2024-03-18 06:24:46 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T16:45:58.961930"
    },
    {
      "arxiv_id": "2403.11504v1",
      "title": "MLVICX: Multi-Level Variance-Covariance Exploration for Chest X-ray Self-Supervised Representation Learning",
      "title_zh": "翻译失败",
      "authors": [
        "Azad Singh",
        "Vandan Gorade",
        "Deepak Mishra"
      ],
      "abstract": "Self-supervised learning (SSL) is potentially useful in reducing the need for\nmanual annotation and making deep learning models accessible for medical image\nanalysis tasks. By leveraging the representations learned from unlabeled data,\nself-supervised models perform well on tasks that require little to no\nfine-tuning. However, for medical images, like chest X-rays, which are\ncharacterized by complex anatomical structures and diverse clinical conditions,\nthere arises a need for representation learning techniques that can encode\nfine-grained details while preserving the broader contextual information. In\nthis context, we introduce MLVICX (Multi-Level Variance-Covariance Exploration\nfor Chest X-ray Self-Supervised Representation Learning), an approach to\ncapture rich representations in the form of embeddings from chest X-ray images.\nCentral to our approach is a novel multi-level variance and covariance\nexploration strategy that empowers the model to detect diagnostically\nmeaningful patterns while reducing redundancy effectively. By enhancing the\nvariance and covariance of the learned embeddings, MLVICX promotes the\nretention of critical medical insights by adapting both global and local\ncontextual details. We demonstrate the performance of MLVICX in advancing\nself-supervised chest X-ray representation learning through comprehensive\nexperiments. The performance enhancements we observe across various downstream\ntasks highlight the significance of the proposed approach in enhancing the\nutility of chest X-ray embeddings for precision medical diagnosis and\ncomprehensive image analysis. For pertaining, we used the NIH-Chest X-ray\ndataset, while for downstream tasks, we utilized NIH-Chest X-ray, Vinbig-CXR,\nRSNA pneumonia, and SIIM-ACR Pneumothorax datasets. Overall, we observe more\nthan 3% performance gains over SOTA SSL approaches in various downstream tasks.",
      "tldr_zh": "本研究提出MLVICX，一种针对胸部X光片的自监督学习(Self-Supervised Learning)表示学习方法，通过多级方差和协方差探索策略(Multi-Level Variance-Covariance Exploration)捕捉细粒度诊断模式，同时减少冗余并保留全局和局部上下文细节。相比传统方法，MLVICX在NIH-Chest X-ray等数据集上训练后，在下游任务如NIH-Chest X-ray、Vinbig-CXR、RSNA pneumonia和SIIM-ACR Pneumothorax中实现了超过3%的性能提升。总体而言，该方法提升了胸部X光片嵌入的实用性，支持更精确的医疗诊断和图像分析。",
      "categories": [
        "eess.IV",
        "cs.AI",
        "cs.CV",
        "cs.LG"
      ],
      "primary_category": "eess.IV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2403.11504v1",
      "published_date": "2024-03-18 06:19:37 UTC",
      "updated_date": "2024-03-18 06:19:37 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T16:46:12.575259"
    },
    {
      "arxiv_id": "2403.11496v1",
      "title": "MCD: Diverse Large-Scale Multi-Campus Dataset for Robot Perception",
      "title_zh": "MCD：多样化大规模多校园机器人",
      "authors": [
        "Thien-Minh Nguyen",
        "Shenghai Yuan",
        "Thien Hoang Nguyen",
        "Pengyu Yin",
        "Haozhi Cao",
        "Lihua Xie",
        "Maciej Wozniak",
        "Patric Jensfelt",
        "Marko Thiel",
        "Justin Ziegenbein",
        "Noel Blunder"
      ],
      "abstract": "Perception plays a crucial role in various robot applications. However,\nexisting well-annotated datasets are biased towards autonomous driving\nscenarios, while unlabelled SLAM datasets are quickly over-fitted, and often\nlack environment and domain variations. To expand the frontier of these fields,\nwe introduce a comprehensive dataset named MCD (Multi-Campus Dataset),\nfeaturing a wide range of sensing modalities, high-accuracy ground truth, and\ndiverse challenging environments across three Eurasian university campuses. MCD\ncomprises both CCS (Classical Cylindrical Spinning) and NRE (Non-Repetitive\nEpicyclic) lidars, high-quality IMUs (Inertial Measurement Units), cameras, and\nUWB (Ultra-WideBand) sensors. Furthermore, in a pioneering effort, we introduce\nsemantic annotations of 29 classes over 59k sparse NRE lidar scans across three\ndomains, thus providing a novel challenge to existing semantic segmentation\nresearch upon this largely unexplored lidar modality. Finally, we propose, for\nthe first time to the best of our knowledge, continuous-time ground truth based\non optimization-based registration of lidar-inertial data on large survey-grade\nprior maps, which are also publicly released, each several times the size of\nexisting ones. We conduct a rigorous evaluation of numerous state-of-the-art\nalgorithms on MCD, report their performance, and highlight the challenges\nawaiting solutions from the research community.",
      "tldr_zh": "本研究引入了 MCD（Multi-Campus Dataset），一个大规模多样化的数据集，旨在解决机器人感知领域现有数据集偏向自动驾驶和环境变化不足的问题。MCD 涵盖三个欧亚大学校园的复杂环境，包括 CCS（Classical Cylindrical Spinning）和 NRE（Non-Repetitive Epicyclic）lidars、高质量 IMUs（Inertial Measurement Units）、摄像头和 UWB（Ultra-WideBand）传感器，并提供 59k 个稀疏 NRE lidar 扫描的 29 类语义标注。创新性地，数据集首次采用基于优化注册的连续时间 ground truth，并公开大型调查级先验地图；实验评估显示，现有最先进算法在 MCD 上面临新挑战，呼吁进一步研究。",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "comment": "Accepted by The IEEE/CVF Conference on Computer Vision and Pattern\n  Recognition 2024",
      "pdf_url": "http://arxiv.org/pdf/2403.11496v1",
      "published_date": "2024-03-18 06:00:38 UTC",
      "updated_date": "2024-03-18 06:00:38 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T16:46:26.070100"
    },
    {
      "arxiv_id": "2403.11495v1",
      "title": "Semantic-Enhanced Representation Learning for Road Networks with Temporal Dynamics",
      "title_zh": "语义增强的道路网络表示学习，结合时间动态",
      "authors": [
        "Yile Chen",
        "Xiucheng Li",
        "Gao Cong",
        "Zhifeng Bao",
        "Cheng Long"
      ],
      "abstract": "In this study, we introduce a novel framework called Toast for learning\ngeneral-purpose representations of road networks, along with its advanced\ncounterpart DyToast, designed to enhance the integration of temporal dynamics\nto boost the performance of various time-sensitive downstream tasks.\nSpecifically, we propose to encode two pivotal semantic characteristics\nintrinsic to road networks: traffic patterns and traveling semantics. To\nachieve this, we refine the skip-gram module by incorporating auxiliary\nobjectives aimed at predicting the traffic context associated with a target\nroad segment. Moreover, we leverage trajectory data and design pre-training\nstrategies based on Transformer to distill traveling semantics on road\nnetworks. DyToast further augments this framework by employing unified\ntrigonometric functions characterized by their beneficial properties, enabling\nthe capture of temporal evolution and dynamic nature of road networks more\neffectively. With these proposed techniques, we can obtain representations that\nencode multi-faceted aspects of knowledge within road networks, applicable\nacross both road segment-based applications and trajectory-based applications.\nExtensive experiments on two real-world datasets across three tasks demonstrate\nthat our proposed framework consistently outperforms the state-of-the-art\nbaselines by a significant margin.",
      "tldr_zh": "本研究提出了一种名为 Toast 的新框架及其高级版本 DyToast，用于学习道路网络的通用表示，并通过整合时间动态提升时间敏感下游任务的性能。具体而言，Toast 通过改进 skip-gram 模块和辅助目标预测来编码道路网络的关键语义特征，包括交通模式和旅行语义，并利用轨迹数据及基于 Transformer's 预训练策略提取这些特征。DyToast 进一步采用统一的三角函数来捕捉道路网络的动态演变，从而生成适用于道路段和轨迹应用的综合知识表示。在两个真实数据集上的三个任务实验中，该框架显著优于现有基线模型，证明了其有效性。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2403.11495v1",
      "published_date": "2024-03-18 05:59:56 UTC",
      "updated_date": "2024-03-18 05:59:56 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T16:46:34.674937"
    },
    {
      "arxiv_id": "2403.11492v2",
      "title": "SmartRefine: A Scenario-Adaptive Refinement Framework for Efficient Motion Prediction",
      "title_zh": "翻译失败",
      "authors": [
        "Yang Zhou",
        "Hao Shao",
        "Letian Wang",
        "Steven L. Waslander",
        "Hongsheng Li",
        "Yu Liu"
      ],
      "abstract": "Predicting the future motion of surrounding agents is essential for\nautonomous vehicles (AVs) to operate safely in dynamic, human-robot-mixed\nenvironments. Context information, such as road maps and surrounding agents'\nstates, provides crucial geometric and semantic information for motion behavior\nprediction. To this end, recent works explore two-stage prediction frameworks\nwhere coarse trajectories are first proposed, and then used to select critical\ncontext information for trajectory refinement. However, they either incur a\nlarge amount of computation or bring limited improvement, if not both. In this\npaper, we introduce a novel scenario-adaptive refinement strategy, named\nSmartRefine, to refine prediction with minimal additional computation.\nSpecifically, SmartRefine can comprehensively adapt refinement configurations\nbased on each scenario's properties, and smartly chooses the number of\nrefinement iterations by introducing a quality score to measure the prediction\nquality and remaining refinement potential of each scenario. SmartRefine is\ndesigned as a generic and flexible approach that can be seamlessly integrated\ninto most state-of-the-art motion prediction models. Experiments on Argoverse\n(1 & 2) show that our method consistently improves the prediction accuracy of\nmultiple state-of-the-art prediction models. Specifically, by adding\nSmartRefine to QCNet, we outperform all published ensemble-free works on the\nArgoverse 2 leaderboard (single agent track) at submission. Comprehensive\nstudies are also conducted to ablate design choices and explore the mechanism\nbehind multi-iteration refinement. Codes are available at\nhttps://github.com/opendilab/SmartRefine/",
      "tldr_zh": "这篇论文提出了 SmartRefine，一种场景自适应的精炼框架，用于提升自动驾驶车辆（AVs）运动预测的效率和准确性。该框架通过引入质量分数动态调整精炼迭代次数，并根据场景属性优化配置，从而在最小化额外计算量的同时，智能选择关键上下文信息进行轨迹精炼。实验结果显示，SmartRefine 无缝集成到多种最先进模型（如 QCNet）后，在 Argoverse 1 & 2 数据集上显著提高了预测准确性，并在 Argoverse 2 排行榜（单代理轨道）上超越所有非集成模型。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.RO"
      ],
      "primary_category": "cs.CV",
      "comment": "Camera-ready version for CVPR 2024",
      "pdf_url": "http://arxiv.org/pdf/2403.11492v2",
      "published_date": "2024-03-18 05:53:20 UTC",
      "updated_date": "2024-03-19 17:04:35 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T16:46:50.002777"
    },
    {
      "arxiv_id": "2403.11487v3",
      "title": "Can LLMs Generate Human-Like Wayfinding Instructions? Towards Platform-Agnostic Embodied Instruction Synthesis",
      "title_zh": "翻译失败",
      "authors": [
        "Vishnu Sashank Dorbala",
        "Sanjoy Chowdhury",
        "Dinesh Manocha"
      ],
      "abstract": "We present a novel approach to automatically synthesize \"wayfinding\ninstructions\" for an embodied robot agent. In contrast to prior approaches that\nare heavily reliant on human-annotated datasets designed exclusively for\nspecific simulation platforms, our algorithm uses in-context learning to\ncondition an LLM to generate instructions using just a few references. Using an\nLLM-based Visual Question Answering strategy, we gather detailed information\nabout the environment which is used by the LLM for instruction synthesis. We\nimplement our approach on multiple simulation platforms including Matterport3D,\nAI Habitat and ThreeDWorld, thereby demonstrating its platform-agnostic nature.\nWe subjectively evaluate our approach via a user study and observe that 83.3%\nof users find the synthesized instructions accurately capture the details of\nthe environment and show characteristics similar to those of human-generated\ninstructions. Further, we conduct zero-shot navigation with multiple approaches\non the REVERIE dataset using the generated instructions, and observe very close\ncorrelation with the baseline on standard success metrics (< 1% change in SR),\nquantifying the viability of generated instructions in replacing\nhuman-annotated data. We finally discuss the applicability of our approach in\nenabling a generalizable evaluation of embodied navigation policies. To the\nbest of our knowledge, ours is the first LLM-driven approach capable of\ngenerating \"human-like\" instructions in a platform-agnostic manner, without\ntraining.",
      "tldr_zh": "本研究提出了一种新型方法，使用大型语言模型（LLMs）通过 in-context learning 自动合成“wayfinding instructions”，以指导机器人代理在不同环境中导航。该方法依赖 LLM-based Visual Question Answering (VQA) 策略收集环境细节，并在多个模拟平台如 Matterport3D、AI Habitat 和 ThreeDWorld 上实现平台无关性。用户研究显示，83.3% 的参与者认为生成的指令准确捕捉环境细节，并与人类指令类似；在 REVERIE 数据集的零样本导航实验中，该方法与基线模型的成功率（SR）仅差不到1%，证明其可替代人类标注数据。该创新无需训练，即可生成“human-like”指令，并为评估 embodied navigation policies 的泛化性提供新途径。",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "comment": "14 Pages",
      "pdf_url": "http://arxiv.org/pdf/2403.11487v3",
      "published_date": "2024-03-18 05:38:07 UTC",
      "updated_date": "2024-04-02 04:27:55 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T16:47:02.216949"
    },
    {
      "arxiv_id": "2403.13846v3",
      "title": "A Clustering Method with Graph Maximum Decoding Information",
      "title_zh": "翻译失败",
      "authors": [
        "Xinrun Xu",
        "Manying Lv",
        "Zhanbiao Lian",
        "Yurong Wu",
        "Jin Yan",
        "Shan Jiang",
        "Zhiming Ding"
      ],
      "abstract": "The clustering method based on graph models has garnered increased attention\nfor its widespread applicability across various knowledge domains. Its\nadaptability to integrate seamlessly with other relevant applications endows\nthe graph model-based clustering analysis with the ability to robustly extract\n\"natural associations\" or \"graph structures\" within datasets, facilitating the\nmodelling of relationships between data points. Despite its efficacy, the\ncurrent clustering method utilizing the graph-based model overlooks the\nuncertainty associated with random walk access between nodes and the embedded\nstructural information in the data. To address this gap, we present a novel\nClustering method for Maximizing Decoding Information within graph-based\nmodels, named CMDI. CMDI innovatively incorporates two-dimensional structural\ninformation theory into the clustering process, consisting of two phases: graph\nstructure extraction and graph vertex partitioning. Within CMDI, graph\npartitioning is reformulated as an abstract clustering problem, leveraging\nmaximum decoding information to minimize uncertainty associated with random\nvisits to vertices. Empirical evaluations on three real-world datasets\ndemonstrate that CMDI outperforms classical baseline methods, exhibiting a\nsuperior decoding information ratio (DI-R). Furthermore, CMDI showcases\nheightened efficiency, particularly when considering prior knowledge (PK).\nThese findings underscore the effectiveness of CMDI in enhancing decoding\ninformation quality and computational efficiency, positioning it as a valuable\ntool in graph-based clustering analyses.",
      "tldr_zh": "本研究提出了一种新的聚类方法CMDI（Clustering method for Maximizing Decoding Information），旨在解决基于图模型的聚类方法忽略节点间随机游走不确定性和嵌入结构信息的问题。CMDI通过整合二维结构信息理论，分为图结构提取和图顶点分区两个阶段，将图分区问题转化为抽象聚类问题，并利用最大解码信息最小化顶点随机访问的不确定性。在三个真实数据集上的实验表明，CMDI优于经典基线方法，具有更高的解码信息比率(DI-R)和计算效率，尤其在考虑先验知识(PK)时表现突出。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "9 pages, 9 figures, IJCNN 2024",
      "pdf_url": "http://arxiv.org/pdf/2403.13846v3",
      "published_date": "2024-03-18 05:18:19 UTC",
      "updated_date": "2025-04-01 08:10:49 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T16:47:14.249872"
    },
    {
      "arxiv_id": "2403.11483v1",
      "title": "Open-World Semi-Supervised Learning for Node Classification",
      "title_zh": "开放世界半监督学习用于节点分类",
      "authors": [
        "Yanling Wang",
        "Jing Zhang",
        "Lingxi Zhang",
        "Lixin Liu",
        "Yuxiao Dong",
        "Cuiping Li",
        "Hong Chen",
        "Hongzhi Yin"
      ],
      "abstract": "Open-world semi-supervised learning (Open-world SSL) for node classification,\nthat classifies unlabeled nodes into seen classes or multiple novel classes, is\na practical but under-explored problem in the graph community. As only seen\nclasses have human labels, they are usually better learned than novel classes,\nand thus exhibit smaller intra-class variances within the embedding space\n(named as imbalance of intra-class variances between seen and novel classes).\nBased on empirical and theoretical analysis, we find the variance imbalance can\nnegatively impact the model performance. Pre-trained feature encoders can\nalleviate this issue via producing compact representations for novel classes.\nHowever, creating general pre-trained encoders for various types of graph data\nhas been proven to be challenging. As such, there is a demand for an effective\nmethod that does not rely on pre-trained graph encoders. In this paper, we\npropose an IMbalance-Aware method named OpenIMA for Open-world semi-supervised\nnode classification, which trains the node classification model from scratch\nvia contrastive learning with bias-reduced pseudo labels. Extensive experiments\non seven popular graph benchmarks demonstrate the effectiveness of OpenIMA, and\nthe source code has been available on GitHub.",
      "tldr_zh": "该论文探讨了Open-world semi-supervised learning (Open-world SSL) 在节点分类中的实际问题，即将未标记节点分类到已知类或多个新类，但由于已知类有标签导致嵌入空间中类内方差不平衡，从而影响模型性能。作者提出OpenIMA方法，通过对比学习和减少偏差的伪标签从零开始训练模型，而不依赖预训练特征编码器。该方法在七个流行图基准上的广泛实验中证明了其有效性，并提供了开源代码。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.SI"
      ],
      "primary_category": "cs.LG",
      "comment": "Accepted by ICDE 2024",
      "pdf_url": "http://arxiv.org/pdf/2403.11483v1",
      "published_date": "2024-03-18 05:12:54 UTC",
      "updated_date": "2024-03-18 05:12:54 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T16:47:25.138142"
    },
    {
      "arxiv_id": "2403.11473v1",
      "title": "Word Order's Impacts: Insights from Reordering and Generation Analysis",
      "title_zh": "词序的影响：从重新排序和生成分析中获得的洞见",
      "authors": [
        "Qinghua Zhao",
        "Jiaang Li",
        "Lei Li",
        "Zenghui Zhou",
        "Junfeng Liu"
      ],
      "abstract": "Existing works have studied the impacts of the order of words within natural\ntext. They usually analyze it by destroying the original order of words to\ncreate a scrambled sequence, and then comparing the models' performance between\nthe original and scrambled sequences. The experimental results demonstrate\nmarginal drops. Considering this findings, different hypothesis about word\norder is proposed, including ``the order of words is redundant with lexical\nsemantics'', and ``models do not rely on word order''. In this paper, we\nrevisit the aforementioned hypotheses by adding a order reconstruction\nperspective, and selecting datasets of different spectrum. Specifically, we\nfirst select four different datasets, and then design order reconstruction and\ncontinuing generation tasks. Empirical findings support that ChatGPT relies on\nword order to infer, but cannot support or negate the redundancy relations\nbetween word order lexical semantics.",
      "tldr_zh": "本研究重新审视了词序（word order）对语言模型的影响，针对现有工作通过打乱词序发现性能下降较小并提出词序冗余假设（如“词序与词汇语义（lexical semantics）冗余”）。作者设计了词序重建和连续生成任务，并选取四个不同数据集进行实验。结果显示，ChatGPT 依赖词序进行推理，但无法支持或否定词序与词汇语义之间的冗余关系，这为深入理解模型对文本顺序的处理提供了新洞见。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2403.11473v1",
      "published_date": "2024-03-18 04:45:44 UTC",
      "updated_date": "2024-03-18 04:45:44 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T16:47:37.350080"
    },
    {
      "arxiv_id": "2403.11468v2",
      "title": "CollagePrompt: A Benchmark for Budget-Friendly Visual Recognition with GPT-4V",
      "title_zh": "翻译失败",
      "authors": [
        "Siyu Xu",
        "Yunke Wang",
        "Daochang Liu",
        "Bo Du",
        "Chang Xu"
      ],
      "abstract": "Recent advancements in generative AI have suggested that by taking visual\nprompts, GPT-4V can demonstrate significant proficiency in visual recognition\ntasks. Despite its impressive capabilities, the financial cost associated with\nGPT-4V's inference presents a substantial barrier to its wide use. To address\nthis challenge, we propose a budget-friendly collage prompting task that\ncollages multiple images into a single visual prompt and makes GPT-4V perform\nvisual recognition on several images simultaneously, thereby reducing the cost.\nWe collect a dataset of various collage prompts to assess its performance in\nGPT-4V's visual recognition. Our evaluations reveal several key findings: 1)\nRecognition accuracy varies with different positions in the collage. 2)\nGrouping images of the same category together leads to better visual\nrecognition results. 3) Incorrect labels often come from adjacent images. These\nfindings highlight the importance of image arrangement within collage prompt.\nTo this end, we construct a benchmark called CollagePrompt, which offers a\nplatform for designing collage prompt to achieve more cost-effective visual\nrecognition with GPT-4V. A baseline method derived from genetic algorithms to\noptimize collage layouts is proposed and two metrics are introduced to measure\nthe efficiency of the optimized collage prompt. Our benchmark enables\nresearchers to better optimize collage prompts, thus making GPT-4V more\ncost-effective in visual recognition. The code and data are available at this\nproject page https://collageprompting.github.io/.",
      "tldr_zh": "本研究针对GPT-4V在视觉识别任务中的高推理成本问题，提出了一种预算友好的collage prompting方法，将多张图像拼贴成单一视觉提示，让GPT-4V同时处理多个图像，从而降低费用。实验发现，识别准确率受图像位置影响，同类图像分组可提升性能，而错误标签往往来自相邻图像，这些insights突出了图像布局的重要性。为此，研究构建了CollagePrompt基准，提供基于遗传算法的基线方法和两个效率指标，帮助优化collage提示，使GPT-4V的视觉识别更具成本效益。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "Accepted by NAACL2025 Findings",
      "pdf_url": "http://arxiv.org/pdf/2403.11468v2",
      "published_date": "2024-03-18 04:41:38 UTC",
      "updated_date": "2025-02-06 12:55:33 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T16:47:49.263276"
    },
    {
      "arxiv_id": "2403.11456v4",
      "title": "HateCOT: An Explanation-Enhanced Dataset for Generalizable Offensive Speech Detection via Large Language Models",
      "title_zh": "翻译失败",
      "authors": [
        "Huy Nghiem",
        "Hal Daumé III"
      ],
      "abstract": "The widespread use of social media necessitates reliable and efficient\ndetection of offensive content to mitigate harmful effects. Although\nsophisticated models perform well on individual datasets, they often fail to\ngeneralize due to varying definitions and labeling of \"offensive content.\" In\nthis paper, we introduce HateCOT, an English dataset with over 52,000 samples\nfrom diverse sources, featuring explanations generated by GPT-3.5Turbo and\ncurated by humans. We demonstrate that pretraining on HateCOT significantly\nenhances the performance of open-source Large Language Models on three\nbenchmark datasets for offensive content detection in both zero-shot and\nfew-shot settings, despite differences in domain and task. Additionally,\nHateCOT facilitates effective K-shot fine-tuning of LLMs with limited data and\nimproves the quality of their explanations, as confirmed by our human\nevaluation.",
      "tldr_zh": "本研究引入了HateCOT，这是一个包含超过52,000个样本的增强解释数据集，旨在提升Large Language Models (LLMs)在攻击性内容检测中的泛化能力。数据集来自多样来源，由GPT-3.5Turbo生成解释并经人类校对，支持LLMs在零-shot和few-shot设置下表现提升，并在三个基准数据集上比基线模型显著改进。HateCOT还促进了有效的K-shot微调，并通过人类评估证实了其对解释质量的优化。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.SI"
      ],
      "primary_category": "cs.CL",
      "comment": "EMNLP 2024 Findings",
      "pdf_url": "http://arxiv.org/pdf/2403.11456v4",
      "published_date": "2024-03-18 04:12:35 UTC",
      "updated_date": "2024-10-05 21:37:55 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T16:48:00.904536"
    },
    {
      "arxiv_id": "2403.12109v1",
      "title": "GCAM: Gaussian and causal-attention model of food fine-grained recognition",
      "title_zh": "翻译失败",
      "authors": [
        "Guohang Zhuang",
        "Yue Hu",
        "Tianxing Yan",
        "JiaZhan Gao"
      ],
      "abstract": "Currently, most food recognition relies on deep learning for category\nclassification. However, these approaches struggle to effectively distinguish\nbetween visually similar food samples, highlighting the pressing need to\naddress fine-grained issues in food recognition. To mitigate these challenges,\nwe propose the adoption of a Gaussian and causal-attention model for\nfine-grained object recognition.In particular, we train to obtain Gaussian\nfeatures over target regions, followed by the extraction of fine-grained\nfeatures from the objects, thereby enhancing the feature mapping capabilities\nof the target regions. To counteract data drift resulting from uneven data\ndistributions, we employ a counterfactual reasoning approach. By using\ncounterfactual interventions, we analyze the impact of the learned image\nattention mechanism on network predictions, enabling the network to acquire\nmore useful attention weights for fine-grained image recognition. Finally, we\ndesign a learnable loss strategy to balance training stability across various\nmodules, ultimately improving the accuracy of the final target recognition. We\nvalidate our approach on four relevant datasets, demonstrating its excellent\nperformance across these four datasets.We experimentally show that GCAM\nsurpasses state-of-the-art methods on the ETH-FOOD101, UECFOOD256, and\nVireo-FOOD172 datasets. Furthermore, our approach also achieves\nstate-of-the-art performance on the CUB-200 dataset.",
      "tldr_zh": "本研究针对食物细粒度识别的挑战，提出GCAM模型，该模型通过训练Gaussian特征并提取细粒度特征来提升目标区域的特征映射能力，同时采用counterfactual reasoning来对抗数据漂移，并优化注意力权重。GCAM还设计了可学习损失策略，以平衡不同模块的训练稳定性，从而提高识别准确性。在实验中，该方法在ETH-FOOD101、UECFOOD256和Vireo-FOOD172数据集上超越现有最先进方法，并在CUB-200数据集上达到最先进性能。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.LG",
      "comment": "23 pages, 11 figures",
      "pdf_url": "http://arxiv.org/pdf/2403.12109v1",
      "published_date": "2024-03-18 03:39:54 UTC",
      "updated_date": "2024-03-18 03:39:54 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T16:48:15.336071"
    },
    {
      "arxiv_id": "2403.11432v2",
      "title": "Demystifying the Physics of Deep Reinforcement Learning-Based Autonomous Vehicle Decision-Making",
      "title_zh": "翻译失败",
      "authors": [
        "Hanxi Wan",
        "Pei Li",
        "Arpan Kusari"
      ],
      "abstract": "With the advent of universal function approximators in the domain of\nreinforcement learning, the number of practical applications leveraging deep\nreinforcement learning (DRL) has exploded. Decision-making in autonomous\nvehicles (AVs) has emerged as a chief application among them, taking the sensor\ndata or the higher-order kinematic variables as the input and providing a\ndiscrete choice or continuous control output. There has been a continuous\neffort to understand the black-box nature of the DRL models, but so far, there\nhasn't been any discussion (to the best of authors' knowledge) about how the\nmodels learn the physical process. This presents an overwhelming limitation\nthat restricts the real-world deployment of DRL in AVs. Therefore, in this\nresearch work, we try to decode the knowledge learnt by the attention-based DRL\nframework about the physical process. We use a continuous proximal policy\noptimization-based DRL algorithm as the baseline model and add a multi-head\nattention framework in an open-source AV simulation environment. We provide\nsome analytical techniques for discussing the interpretability of the trained\nmodels in terms of explainability and causality for spatial and temporal\ncorrelations. We show that the weights in the first head encode the positions\nof the neighboring vehicles while the second head focuses on the leader vehicle\nexclusively. Also, the ego vehicle's action is causally dependent on the\nvehicles in the target lane spatially and temporally. Through these findings,\nwe reliably show that these techniques can help practitioners decipher the\nresults of the DRL algorithms.",
      "tldr_zh": "这篇论文探讨了基于深度强化学习 (DRL) 的自动驾驶车辆 (AVs) 决策中物理过程的解读问题，旨在解决 DRL 模型的黑箱性质以促进实际部署。作者使用连续 Proximal Policy Optimization (PPO) 算法作为基线，并整合多头注意力框架，在开源 AV 模拟环境中训练模型。研究通过分析技术揭示了模型的可解释性和因果关系，例如第一个注意力头编码邻近车辆的位置，第二个头专注于领头车辆，且自车辆的行动在空间和时间上因果依赖于目标车道车辆。这些发现有助于从业者更好地理解 DRL 算法的结果，提高其在真实世界应用的可靠性。",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.RO",
      "comment": "Submitted for peer-review",
      "pdf_url": "http://arxiv.org/pdf/2403.11432v2",
      "published_date": "2024-03-18 02:59:13 UTC",
      "updated_date": "2024-06-13 15:03:40 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T16:48:26.834053"
    },
    {
      "arxiv_id": "2403.13845v1",
      "title": "Learning to better see the unseen: Broad-Deep Mixed Anti-Forgetting Framework for Incremental Zero-Shot Fault Diagnosis",
      "title_zh": "翻译失败",
      "authors": [
        "Jiancheng Zhao",
        "Jiaqi Yue",
        "Chunhui Zhao"
      ],
      "abstract": "Zero-shot fault diagnosis (ZSFD) is capable of identifying unseen faults via\npredicting fault attributes labeled by human experts. We first recognize the\ndemand of ZSFD to deal with continuous changes in industrial processes, i.e.,\nthe model's ability to adapt to new fault categories and attributes while\navoiding forgetting the diagnosis ability learned previously. To overcome the\nissue that the existing ZSFD paradigm cannot learn from evolving streams of\ntraining data in industrial scenarios, the incremental ZSFD (IZSFD) paradigm is\nproposed for the first time, which incorporates category increment and\nattribute increment for both traditional ZSFD and generalized ZSFD paradigms.\nTo achieve IZSFD, we present a broad-deep mixed anti-forgetting framework\n(BDMAFF) that aims to learn from new fault categories and attributes. To tackle\nthe issue of forgetting, BDMAFF effectively accumulates previously acquired\nknowledge from two perspectives: features and attribute prototypes. The feature\nmemory is established through a deep generative model that employs\nanti-forgetting training strategies, ensuring the generation quality of\nhistorical categories is supervised and maintained. The diagnosis model SEEs\nthe UNSEEN faults with the help of generated samples from the generative model.\nThe attribute prototype memory is established through a diagnosis model\ninspired by the broad learning system. Unlike traditional incremental learning\nalgorithms, BDMAFF introduces a memory-driven iterative update strategy for the\ndiagnosis model, which allows the model to learn new faults and attributes\nwithout requiring the storage of all historical training samples. The\neffectiveness of the proposed method is verified by a real hydraulic system and\nthe Tennessee-Eastman benchmark process.",
      "tldr_zh": "该论文首次提出 Incremental Zero-Shot Fault Diagnosis (IZSFD) 范式，以处理工业过程的连续变化，允许模型适应新故障类别和属性同时避免忘记先前诊断能力。作者引入 Broad-Deep Mixed Anti-Forgetting Framework (BDMAFF)，通过特征记忆（利用深度生成模型和反忘记训练策略）和属性原型记忆（受广义学习系统启发）来积累知识。框架采用记忆驱动的迭代更新策略，使诊断模型能够学习新故障而不需存储所有历史样本。实验在真实液压系统和 Tennessee-Eastman 基准过程中验证了 BDMAFF 的有效性，提升了 Zero-Shot Fault Diagnosis (ZSFD) 的性能。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2403.13845v1",
      "published_date": "2024-03-18 02:50:42 UTC",
      "updated_date": "2024-03-18 02:50:42 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T16:48:40.902863"
    },
    {
      "arxiv_id": "2405.01555v4",
      "title": "Digital Twin-Empowered Task Assignment in Aerial MEC Network: A Resource Coalition Cooperation Approach with Generative Model",
      "title_zh": "翻译失败",
      "authors": [
        "Xin Tang",
        "Qian Chen",
        "Rong Yu",
        "Xiaohuan Li"
      ],
      "abstract": "To meet the demands for ubiquitous communication and temporary edge computing\nin 6G networks, aerial mobile edge computing (MEC) networks have been\nenvisioned as a new paradigm. However, dynamic user requests pose challenges\nfor task assignment strategies. Most of the existing research assumes that the\nstrategy is deployed on ground-based stations or UAVs, which will be\nineffective in an environment lacking infrastructure and continuous energy\nsupply. Moreover, the resource mutual exclusion problem of dynamic task\nassignment has not been effectively solved. Toward this end, we introduce the\ndigital twin (DT) into the aerial MEC network to study the resource coalition\ncooperation approach with the generative model (GM), which provides a\npreliminary coalition structure for the coalition game. Specifically, we\npropose a novel network framework that is composed of an application plane, a\nphysical plane, and a virtual plane. After that, the task assignment problem is\nsimplified to convex optimization programming with linear constraints. And\nthen, we also propose a resource coalition cooperation approach that is based\non a transferable utility (TU) coalition game to obtain an approximate optimal\nsolution. Numerical results confirm the effectiveness of our proposed approach\nin terms of energy consumption and utilization of resources.",
      "tldr_zh": "本研究针对6G网络中Aerial MEC网络的动态任务分配挑战，引入Digital Twin和Generative Model，提出一种资源联盟合作方法，以优化资源利用和能耗。论文构建了一个由应用层、物理层和虚拟层组成的新型网络框架，将任务分配问题简化为凸优化问题，并基于Transferable Utility (TU) coalition game实现近似最优解。实验结果显示，该方法显著降低了能耗并提高了资源利用率。",
      "categories": [
        "cs.NI",
        "cs.AI"
      ],
      "primary_category": "cs.NI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2405.01555v4",
      "published_date": "2024-03-18 02:28:10 UTC",
      "updated_date": "2024-10-26 12:09:26 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T16:48:55.173281"
    },
    {
      "arxiv_id": "2403.11420v1",
      "title": "Neural network representation of quantum systems",
      "title_zh": "神经网络表示量子系统",
      "authors": [
        "Koji Hashimoto",
        "Yuji Hirono",
        "Jun Maeda",
        "Jojiro Totsuka-Yoshinaka"
      ],
      "abstract": "It has been proposed that random wide neural networks near Gaussian process\nare quantum field theories around Gaussian fixed points. In this paper, we\nprovide a novel map with which a wide class of quantum mechanical systems can\nbe cast into the form of a neural network with a statistical summation over\nnetwork parameters. Our simple idea is to use the universal approximation\ntheorem of neural networks to generate arbitrary paths in the Feynman's path\nintegral. The map can be applied to interacting quantum systems / field\ntheories, even away from the Gaussian limit. Our findings bring machine\nlearning closer to the quantum world.",
      "tldr_zh": "该论文提出了一种新映射，将广泛的量子机械系统表示为神经网络形式，通过神经网络的通用逼近定理生成费曼路径积分(Feynman's path integral)中的任意路径，从而实现对量子系统的统计求和。不同于以往仅限于高斯固定点的假设，这种方法适用于交互量子系统和场论，即使远离高斯极限。研究结果将机器学习与量子世界更紧密地连接，为跨领域应用提供了新视角。",
      "categories": [
        "hep-th",
        "cond-mat.dis-nn",
        "cs.AI",
        "cs.LG",
        "quant-ph"
      ],
      "primary_category": "hep-th",
      "comment": "24 pages, 6 figures",
      "pdf_url": "http://arxiv.org/pdf/2403.11420v1",
      "published_date": "2024-03-18 02:20:22 UTC",
      "updated_date": "2024-03-18 02:20:22 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T16:49:05.231412"
    },
    {
      "arxiv_id": "2403.11418v1",
      "title": "Variational Sampling of Temporal Trajectories",
      "title_zh": "翻译失败",
      "authors": [
        "Jurijs Nazarovs",
        "Zhichun Huang",
        "Xingjian Zhen",
        "Sourav Pal",
        "Rudrasis Chakraborty",
        "Vikas Singh"
      ],
      "abstract": "A deterministic temporal process can be determined by its trajectory, an\nelement in the product space of (a) initial condition $z_0 \\in \\mathcal{Z}$ and\n(b) transition function $f: (\\mathcal{Z}, \\mathcal{T}) \\to \\mathcal{Z}$ often\ninfluenced by the control of the underlying dynamical system. Existing methods\noften model the transition function as a differential equation or as a\nrecurrent neural network. Despite their effectiveness in predicting future\nmeasurements, few results have successfully established a method for sampling\nand statistical inference of trajectories using neural networks, partially due\nto constraints in the parameterization. In this work, we introduce a mechanism\nto learn the distribution of trajectories by parameterizing the transition\nfunction $f$ explicitly as an element in a function space. Our framework allows\nefficient synthesis of novel trajectories, while also directly providing a\nconvenient tool for inference, i.e., uncertainty estimation, likelihood\nevaluations and out of distribution detection for abnormal trajectories. These\ncapabilities can have implications for various downstream tasks, e.g.,\nsimulation and evaluation for reinforcement learning.",
      "tldr_zh": "该论文探讨了确定性时间过程的轨迹采样问题，传统方法往往依赖微分方程或循环神经网络建模转移函数（transition function），但在轨迹采样和统计推断方面存在局限。作者引入一种变分采样（variational sampling）机制，通过将转移函数显式参数化为函数空间中的元素，来学习时间轨迹（temporal trajectories）的分布，实现高效合成新轨迹。框架还支持直接进行不确定性估计、似然评估和异常轨迹检测，从而为强化学习的模拟和评估等下游任务提供便利工具。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2403.11418v1",
      "published_date": "2024-03-18 02:12:12 UTC",
      "updated_date": "2024-03-18 02:12:12 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T16:49:19.179846"
    },
    {
      "arxiv_id": "2403.11415v2",
      "title": "DreamSampler: Unifying Diffusion Sampling and Score Distillation for Image Manipulation",
      "title_zh": "翻译失败",
      "authors": [
        "Jeongsol Kim",
        "Geon Yeong Park",
        "Jong Chul Ye"
      ],
      "abstract": "Reverse sampling and score-distillation have emerged as main workhorses in\nrecent years for image manipulation using latent diffusion models (LDMs). While\nreverse diffusion sampling often requires adjustments of LDM architecture or\nfeature engineering, score distillation offers a simple yet powerful\nmodel-agnostic approach, but it is often prone to mode-collapsing. To address\nthese limitations and leverage the strengths of both approaches, here we\nintroduce a novel framework called {\\em DreamSampler}, which seamlessly\nintegrates these two distinct approaches through the lens of regularized latent\noptimization. Similar to score-distillation, DreamSampler is a model-agnostic\napproach applicable to any LDM architecture, but it allows both distillation\nand reverse sampling with additional guidance for image editing and\nreconstruction. Through experiments involving image editing, SVG reconstruction\nand etc, we demonstrate the competitive performance of DreamSampler compared to\nexisting approaches, while providing new applications. Code:\nhttps://github.com/DreamSampler/dream-sampler",
      "tldr_zh": "本文提出DreamSampler框架，通过正则化潜在优化统一了扩散采样和分数蒸馏两种方法，用于潜在扩散模型(LDMs)的图像操作。DreamSampler作为一种模型无关的方法，支持分数蒸馏和逆向采样，并添加额外指导以提升图像编辑和重建性能。实验结果显示，该框架在图像编辑、SVG重建等任务中比现有方法具有竞争优势，并扩展了新应用，如代码开源在https://github.com/DreamSampler/dream-sampler。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "ECCV 2024",
      "pdf_url": "http://arxiv.org/pdf/2403.11415v2",
      "published_date": "2024-03-18 02:08:58 UTC",
      "updated_date": "2024-09-23 06:47:08 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T16:49:31.171309"
    },
    {
      "arxiv_id": "2403.15447v3",
      "title": "Decoding Compressed Trust: Scrutinizing the Trustworthiness of Efficient LLMs Under Compression",
      "title_zh": "翻译失败",
      "authors": [
        "Junyuan Hong",
        "Jinhao Duan",
        "Chenhui Zhang",
        "Zhangheng Li",
        "Chulin Xie",
        "Kelsey Lieberman",
        "James Diffenderfer",
        "Brian Bartoldson",
        "Ajay Jaiswal",
        "Kaidi Xu",
        "Bhavya Kailkhura",
        "Dan Hendrycks",
        "Dawn Song",
        "Zhangyang Wang",
        "Bo Li"
      ],
      "abstract": "Compressing high-capability Large Language Models (LLMs) has emerged as a\nfavored strategy for resource-efficient inferences. While state-of-the-art\n(SoTA) compression methods boast impressive advancements in preserving benign\ntask performance, the potential risks of compression in terms of safety and\ntrustworthiness have been largely neglected. This study conducts the first,\nthorough evaluation of three (3) leading LLMs using five (5) SoTA compression\ntechniques across eight (8) trustworthiness dimensions. Our experiments\nhighlight the intricate interplay between compression and trustworthiness,\nrevealing some interesting patterns. We find that quantization is currently a\nmore effective approach than pruning in achieving efficiency and\ntrustworthiness simultaneously. For instance, a 4-bit quantized model retains\nthe trustworthiness of its original counterpart, but model pruning\nsignificantly degrades trustworthiness, even at 50% sparsity. Moreover,\nemploying quantization within a moderate bit range could unexpectedly improve\ncertain trustworthiness dimensions such as ethics and fairness. Conversely,\nextreme quantization to very low bit levels (3 bits) tends to reduce\ntrustworthiness significantly. This increased risk cannot be uncovered by\nlooking at benign performance alone, in turn, mandating comprehensive\ntrustworthiness evaluation in practice. These findings culminate in practical\nrecommendations for simultaneously achieving high utility, efficiency, and\ntrustworthiness in LLMs. Code and models are available at\nhttps://decoding-comp-trust.github.io.",
      "tldr_zh": "本研究首次全面评估了压缩大型语言模型(LLMs)对可信性的影响，使用五种最先进压缩技术在八个可信度维度上测试三种领先LLMs。\n结果显示，量化(quantization)比修剪(pruning)更有效地实现效率和可信性的平衡，例如4-bit量化模型能保留原模型的可信性，而修剪即使在50%稀疏度下也会显著降低可信性。\n此外，中等位宽量化可能意外改善伦理和公平性等维度，但极端量化(如3 bits)会显著降低整体可信性，这一点无法仅从常规性能中发现。\n基于这些发现，论文提供了实用建议，以帮助LLMs同时实现高实用性、效率和可信性。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "Accepted to ICML'24",
      "pdf_url": "http://arxiv.org/pdf/2403.15447v3",
      "published_date": "2024-03-18 01:38:19 UTC",
      "updated_date": "2024-06-04 05:40:12 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T16:49:46.635073"
    },
    {
      "arxiv_id": "2403.11402v1",
      "title": "Embracing the Generative AI Revolution: Advancing Tertiary Education in Cybersecurity with GPT",
      "title_zh": "翻译失败",
      "authors": [
        "Raza Nowrozy",
        "David Jam"
      ],
      "abstract": "The rapid advancement of generative Artificial Intelligence (AI)\ntechnologies, particularly Generative Pre-trained Transformer (GPT) models such\nas ChatGPT, has the potential to significantly impact cybersecurity. In this\nstudy, we investigated the impact of GPTs, specifically ChatGPT, on tertiary\neducation in cybersecurity, and provided recommendations for universities to\nadapt their curricula to meet the evolving needs of the industry. Our research\nhighlighted the importance of understanding the alignment between GPT's\n``mental model'' and human cognition, as well as the enhancement of GPT\ncapabilities to human skills based on Bloom's taxonomy. By analyzing current\neducational practices and the alignment of curricula with industry\nrequirements, we concluded that universities providing practical degrees like\ncybersecurity should align closely with industry demand and embrace the\ninevitable generative AI revolution, while applying stringent ethics oversight\nto safeguard responsible GPT usage. We proposed a set of recommendations\nfocused on updating university curricula, promoting agility within\nuniversities, fostering collaboration between academia, industry, and\npolicymakers, and evaluating and assessing educational outcomes.",
      "tldr_zh": "本研究探讨了生成式AI，特别是GPT（如ChatGPT），对网络安全高等教育的影响，并为大学提供适应行业需求的课程调整建议。\n研究分析了GPT的“mental model”与人类认知的契合，以及基于Bloom's taxonomy提升GPT能力以补充人类技能。\n最终结论强调，大学应紧跟AI革命、加强伦理监督，并提出具体推荐，包括更新课程、提升机构灵活性、促进学术界与行业及政策制定者的合作，以及评估教育成果。",
      "categories": [
        "cs.CY",
        "cs.AI"
      ],
      "primary_category": "cs.CY",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2403.11402v1",
      "published_date": "2024-03-18 01:20:38 UTC",
      "updated_date": "2024-03-18 01:20:38 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T16:49:55.300568"
    },
    {
      "arxiv_id": "2403.11401v2",
      "title": "Scene-LLM: Extending Language Model for 3D Visual Understanding and Reasoning",
      "title_zh": "Scene-LLM：扩展语言模型用于3D视觉理解和推理",
      "authors": [
        "Rao Fu",
        "Jingyu Liu",
        "Xilun Chen",
        "Yixin Nie",
        "Wenhan Xiong"
      ],
      "abstract": "This paper introduces Scene-LLM, a 3D-visual-language model that enhances\nembodied agents' abilities in interactive 3D indoor environments by integrating\nthe reasoning strengths of Large Language Models (LLMs). Scene-LLM adopts a\nhybrid 3D visual feature representation, that incorporates dense spatial\ninformation and supports scene state updates. The model employs a projection\nlayer to efficiently project these features in the pre-trained textual\nembedding space, enabling effective interpretation of 3D visual information.\nUnique to our approach is the integration of both scene-level and ego-centric\n3D information. This combination is pivotal for interactive planning, where\nscene-level data supports global planning and ego-centric data is important for\nlocalization. Notably, we use ego-centric 3D frame features for feature\nalignment, an efficient technique that enhances the model's ability to align\nfeatures of small objects within the scene. Our experiments with Scene-LLM\ndemonstrate its strong capabilities in dense captioning, question answering,\nand interactive planning. We believe Scene-LLM advances the field of 3D visual\nunderstanding and reasoning, offering new possibilities for sophisticated agent\ninteractions in indoor settings.",
      "tldr_zh": "本研究提出 Scene-LLM，一种扩展 Large Language Models (LLMs) 的 3D-visual-language 模型，旨在提升 embodied agents 在交互式 3D 室内环境中的视觉理解和推理能力。该模型采用混合 3D 视觉特征表示，包括密集空间信息和场景状态更新，并通过投影层将这些特征投射到预训练的文本嵌入空间中，同时整合场景级和 ego-centric 3D 信息以支持全局规划和本地化。实验结果显示，Scene-LLM 在 dense captioning、question answering 和 interactive planning 任务上表现出色，显著提升了小物体特征对齐的效率。总之，该模型为 3D 视觉理解和代理交互领域提供了新可能性，推动了室内环境中的智能应用发展。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2403.11401v2",
      "published_date": "2024-03-18 01:18:48 UTC",
      "updated_date": "2024-03-22 18:52:51 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T16:50:08.437822"
    },
    {
      "arxiv_id": "2403.11395v2",
      "title": "Automated data processing and feature engineering for deep learning and big data applications: a survey",
      "title_zh": "翻译失败",
      "authors": [
        "Alhassan Mumuni",
        "Fuseini Mumuni"
      ],
      "abstract": "Modern approach to artificial intelligence (AI) aims to design algorithms\nthat learn directly from data. This approach has achieved impressive results\nand has contributed significantly to the progress of AI, particularly in the\nsphere of supervised deep learning. It has also simplified the design of\nmachine learning systems as the learning process is highly automated. However,\nnot all data processing tasks in conventional deep learning pipelines have been\nautomated. In most cases data has to be manually collected, preprocessed and\nfurther extended through data augmentation before they can be effective for\ntraining. Recently, special techniques for automating these tasks have emerged.\nThe automation of data processing tasks is driven by the need to utilize large\nvolumes of complex, heterogeneous data for machine learning and big data\napplications. Today, end-to-end automated data processing systems based on\nautomated machine learning (AutoML) techniques are capable of taking raw data\nand transforming them into useful features for Big Data tasks by automating all\nintermediate processing stages. In this work, we present a thorough review of\napproaches for automating data processing tasks in deep learning pipelines,\nincluding automated data preprocessing--e.g., data cleaning, labeling, missing\ndata imputation, and categorical data encoding--as well as data augmentation\n(including synthetic data generation using generative AI methods) and feature\nengineering--specifically, automated feature extraction, feature construction\nand feature selection. In addition to automating specific data processing\ntasks, we discuss the use of AutoML methods and tools to simultaneously\noptimize all stages of the machine learning pipeline.",
      "tldr_zh": "这篇调查论文探讨了自动化数据处理和特征工程在 deep learning 和 big data 应用中的方法，旨在解决传统深度学习管道中手动数据收集、预处理和增强的挑战。论文回顾了自动化数据预处理技术（如数据清洗、labeling、缺失数据 imputation 和 categorical data encoding）、数据 augmentation（包括使用 generative AI 生成合成数据）以及特征工程（如自动特征 extraction、construction 和 selection）。此外，它强调了基于 AutoML 的端到端系统，能够同时优化机器学习管道的所有阶段，从而提高处理大规模复杂数据的效率和自动化水平。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.DB"
      ],
      "primary_category": "cs.LG",
      "comment": "Journal of Information and Intelligence (2024)",
      "pdf_url": "http://arxiv.org/pdf/2403.11395v2",
      "published_date": "2024-03-18 01:07:48 UTC",
      "updated_date": "2024-03-19 09:36:27 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T16:50:19.121040"
    },
    {
      "arxiv_id": "2403.13844v1",
      "title": "Scheduled Knowledge Acquisition on Lightweight Vector Symbolic Architectures for Brain-Computer Interfaces",
      "title_zh": "翻译失败",
      "authors": [
        "Yejia Liu",
        "Shijin Duan",
        "Xiaolin Xu",
        "Shaolei Ren"
      ],
      "abstract": "Brain-Computer interfaces (BCIs) are typically designed to be lightweight and\nresponsive in real-time to provide users timely feedback. Classical feature\nengineering is computationally efficient but has low accuracy, whereas the\nrecent neural networks (DNNs) improve accuracy but are computationally\nexpensive and incur high latency. As a promising alternative, the\nlow-dimensional computing (LDC) classifier based on vector symbolic\narchitecture (VSA), achieves small model size yet higher accuracy than\nclassical feature engineering methods. However, its accuracy still lags behind\nthat of modern DNNs, making it challenging to process complex brain signals. To\nimprove the accuracy of a small model, knowledge distillation is a popular\nmethod. However, maintaining a constant level of distillation between the\nteacher and student models may not be the best way for a growing student during\nits progressive learning stages. In this work, we propose a simple scheduled\nknowledge distillation method based on curriculum data order to enable the\nstudent to gradually build knowledge from the teacher model, controlled by an\n$\\alpha$ scheduler. Meanwhile, we employ the LDC/VSA as the student model to\nenhance the on-device inference efficiency for tiny BCI devices that demand low\nlatency. The empirical results have demonstrated that our approach achieves\nbetter tradeoff between accuracy and hardware efficiency compared to other\nmethods.",
      "tldr_zh": "脑-计算机接口(BCI)需要在实时响应中平衡准确性和计算效率，传统特征工程方法计算高效但准确率低，而DNNs虽准确率高却延迟大。本文提出一种基于课程数据顺序的定时知识蒸馏方法，使用LDC/VSA作为学生模型，并通过α调度器控制知识逐步转移，提高小模型的准确率。实验结果表明，该方法在BCI设备上实现了准确性和硬件效率之间的更好权衡，优于现有方法。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "Accepted as a full paper by the tinyML Research Symposium 2024",
      "pdf_url": "http://arxiv.org/pdf/2403.13844v1",
      "published_date": "2024-03-18 01:06:29 UTC",
      "updated_date": "2024-03-18 01:06:29 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T16:50:31.390820"
    },
    {
      "arxiv_id": "2403.12108v3",
      "title": "Does AI help humans make better decisions? A statistical evaluation framework for experimental and observational studies",
      "title_zh": "AI 是否能帮助人类做出更好的决策？用于实验性和观察性研究的统计评估框架",
      "authors": [
        "Eli Ben-Michael",
        "D. James Greiner",
        "Melody Huang",
        "Kosuke Imai",
        "Zhichao Jiang",
        "Sooahn Shin"
      ],
      "abstract": "The use of Artificial Intelligence (AI), or more generally data-driven\nalgorithms, has become ubiquitous in today's society. Yet, in many cases and\nespecially when stakes are high, humans still make final decisions. The\ncritical question, therefore, is whether AI helps humans make better decisions\ncompared to a human-alone or AI-alone system. We introduce a new methodological\nframework to empirically answer this question with a minimal set of\nassumptions. We measure a decision maker's ability to make correct decisions\nusing standard classification metrics based on the baseline potential outcome.\nWe consider a single-blinded and unconfounded treatment assignment, where the\nprovision of AI-generated recommendations is assumed to be randomized across\ncases with humans making final decisions. Under this study design, we show how\nto compare the performance of three alternative decision-making\nsystems--human-alone, human-with-AI, and AI-alone. Importantly, the AI-alone\nsystem includes any individualized treatment assignment, including those that\nare not used in the original study. We also show when AI recommendations should\nbe provided to a human-decision maker, and when one should follow such\nrecommendations. We apply the proposed methodology to our own randomized\ncontrolled trial evaluating a pretrial risk assessment instrument. We find that\nthe risk assessment recommendations do not improve the classification accuracy\nof a judge's decision to impose cash bail. Furthermore, we find that replacing\na human judge with algorithms--the risk assessment score and a large language\nmodel in particular--leads to a worse classification performance.",
      "tldr_zh": "这篇论文探讨了人工智能（AI）是否能帮助人类做出更好决策，并引入了一个统计评估框架，用于实验和观察研究中实证评估。框架基于最小假设，使用标准分类指标（如基于基线潜在结果的准确率）比较三种决策系统：人类单独、人类加 AI 推荐，以及 AI 单独。研究假设 AI 推荐是随机提供的，并分析何时提供这些推荐以及何时遵循它们；在应用于随机对照试验时，发现 AI 推荐并未提高法官对现金保释决策的分类准确率，且用算法（如风险评估分数或大型语言模型）替换人类会导致更差的表现。",
      "categories": [
        "cs.AI",
        "econ.GN",
        "q-fin.EC",
        "stat.AP",
        "stat.ME"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2403.12108v3",
      "published_date": "2024-03-18 01:04:52 UTC",
      "updated_date": "2024-10-11 23:05:37 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T16:50:41.661920"
    },
    {
      "arxiv_id": "2403.11381v2",
      "title": "Can LLM-Augmented autonomous agents cooperate?, An evaluation of their cooperative capabilities through Melting Pot",
      "title_zh": "翻译失败",
      "authors": [
        "Manuel Mosquera",
        "Juan Sebastian Pinzon",
        "Manuel Rios",
        "Yesid Fonseca",
        "Luis Felipe Giraldo",
        "Nicanor Quijano",
        "Ruben Manrique"
      ],
      "abstract": "As the field of AI continues to evolve, a significant dimension of this\nprogression is the development of Large Language Models and their potential to\nenhance multi-agent artificial intelligence systems. This paper explores the\ncooperative capabilities of Large Language Model-augmented Autonomous Agents\n(LAAs) using the well-known Meltin Pot environments along with reference models\nsuch as GPT4 and GPT3.5. Preliminary results suggest that while these agents\ndemonstrate a propensity for cooperation, they still struggle with effective\ncollaboration in given environments, emphasizing the need for more robust\narchitectures. The study's contributions include an abstraction layer to adapt\nMelting Pot game scenarios for LLMs, the implementation of a reusable\narchitecture for LLM-mediated agent development - which includes short and\nlong-term memories and different cognitive modules, and the evaluation of\ncooperation capabilities using a set of metrics tied to the Melting Pot's\n\"Commons Harvest\" game. The paper closes, by discussing the limitations of the\ncurrent architectural framework and the potential of a new set of modules that\nfosters better cooperation among LAAs.",
      "tldr_zh": "本论文评估了Large Language Models (LLMs)增强的Autonomous Agents (LAAs)在Melting Pot环境中的合作能力，使用GPT4和GPT3.5作为参考模型。结果显示，这些代理虽表现出合作倾向，但仍难以实现有效协作，突显了更稳健架构的需求。论文的主要贡献包括开发一个抽象层以适应Melting Pot游戏场景、实现一个可重用架构（包含短期和长期记忆以及认知模块）、并通过与\"Commons Harvest\"游戏相关的指标评估合作能力。该研究讨论了当前框架的局限性，并提出新模块以提升LAAs的合作潜力。",
      "categories": [
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2403.11381v2",
      "published_date": "2024-03-18 00:13:43 UTC",
      "updated_date": "2024-06-19 16:23:05 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T16:50:56.808337"
    },
    {
      "arxiv_id": "2403.15445v1",
      "title": "Decoding Multilingual Topic Dynamics and Trend Identification through ARIMA Time Series Analysis on Social Networks: A Novel Data Translation Framework Enhanced by LDA/HDP Models",
      "title_zh": "翻译失败",
      "authors": [
        "Samawel Jaballi",
        "Azer Mahjoubi",
        "Manar Joundy Hazar",
        "Salah Zrigui",
        "Henri Nicolas",
        "Mounir Zrigui"
      ],
      "abstract": "In this study, the authors present a novel methodology adept at decoding\nmultilingual topic dynamics and identifying communication trends during crises.\nWe focus on dialogues within Tunisian social networks during the Coronavirus\nPandemic and other notable themes like sports and politics. We start by\naggregating a varied multilingual corpus of comments relevant to these\nsubjects. This dataset undergoes rigorous refinement during data preprocessing.\nWe then introduce our No-English-to-English Machine Translation approach to\nhandle linguistic differences. Empirical tests of this method showed high\naccuracy and F1 scores, highlighting its suitability for linguistically\ncoherent tasks. Delving deeper, advanced modeling techniques, specifically LDA\nand HDP models are employed to extract pertinent topics from the translated\ncontent. This leads to applying ARIMA time series analysis to decode evolving\ntopic trends. Applying our method to a multilingual Tunisian dataset, we\neffectively identified key topics mirroring public sentiment. Such insights\nprove vital for organizations and governments striving to understand public\nperspectives during crises. Compared to standard approaches, our model\noutperforms, as confirmed by metrics like Coherence Score, U-mass, and Topic\nCoherence. Additionally, an in-depth assessment of the identified topics\nrevealed notable thematic shifts in discussions, with our trends identification\nindicating impressive accuracy, backed by RMSE-based analysis.",
      "tldr_zh": "本研究提出了一种新框架，用于解码多语言社交网络主题动态和识别危机期间的沟通趋势，聚焦于突尼斯社交网络中冠状病毒大流行、体育和政治等主题。框架包括 No-English-to-English Machine Translation 方法进行数据翻译、LDA 和 HDP 模型提取相关主题，以及 ARIMA 时间序列分析解码主题演变趋势。实验结果显示，该方法在突尼斯多语言数据集上实现了高准确性和 F1 scores，比标准方法在 Coherence Score、U-mass 和 Topic Coherence 等指标上表现更优，并通过 RMSE 分析确认了趋势识别的可靠性，为政府和组织理解公众情绪提供了关键洞见。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG",
        "cs.SI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2403.15445v1",
      "published_date": "2024-03-18 00:01:10 UTC",
      "updated_date": "2024-03-18 00:01:10 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T16:51:08.906046"
    }
  ],
  "raw_papers_fetched": true,
  "papers_count": 112,
  "processed_papers_count": 112,
  "failed_papers_count": 0,
  "summary_generated": true,
  "daily_data_saved": true,
  "last_update": "2025-05-17T16:51:42.934159"
}
[
  {
    "arxiv_id": "2502.07132v2",
    "title": "Interactive Data Harmonization with LLM Agents",
    "authors": [
      "Aécio Santos",
      "Eduardo H. M. Pena",
      "Roque Lopez",
      "Juliana Freire"
    ],
    "abstract": "Data harmonization is an essential task that entails integrating datasets\nfrom diverse sources. Despite years of research in this area, it remains a\ntime-consuming and challenging task due to schema mismatches, varying\nterminologies, and differences in data collection methodologies. This paper\npresents the case for agentic data harmonization as a means to both empower\nexperts to harmonize their data and to streamline the process. We introduce\nHarmonia, a system that combines LLM-based reasoning, an interactive user\ninterface, and a library of data harmonization primitives to automate the\nsynthesis of data harmonization pipelines. We demonstrate Harmonia in a\nclinical data harmonization scenario, where it helps to interactively create\nreusable pipelines that map datasets to a standard format. Finally, we discuss\nchallenges and open problems, and suggest research directions for advancing our\nvision.",
    "categories": [
      "cs.AI",
      "cs.DB"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.07132v2",
    "published_date": "2025-02-10 23:50:09 UTC",
    "updated_date": "2025-03-05 18:33:41 UTC"
  },
  {
    "arxiv_id": "2502.07130v1",
    "title": "Unconstrained Body Recognition at Altitude and Range: Comparing Four Approaches",
    "authors": [
      "Blake A Myers",
      "Matthew Q Hill",
      "Veda Nandan Gandi",
      "Thomas M Metz",
      "Alice J O'Toole"
    ],
    "abstract": "This study presents an investigation of four distinct approaches to long-term\nperson identification using body shape. Unlike short-term re-identification\nsystems that rely on temporary features (e.g., clothing), we focus on learning\npersistent body shape characteristics that remain stable over time. We\nintroduce a body identification model based on a Vision Transformer (ViT) (Body\nIdentification from Diverse Datasets, BIDDS) and on a Swin-ViT model\n(Swin-BIDDS). We also expand on previous approaches based on the Linguistic and\nNon-linguistic Core ResNet Identity Models (LCRIM and NLCRIM), but with\nimproved training. All models are trained on a large and diverse dataset of\nover 1.9 million images of approximately 5k identities across 9 databases.\nPerformance was evaluated on standard re-identification benchmark datasets\n(MARS, MSMT17, Outdoor Gait, DeepChange) and on an unconstrained dataset that\nincludes images at a distance (from close-range to 1000m), at altitude (from an\nunmanned aerial vehicle, UAV), and with clothing change. A comparative analysis\nacross these models provides insights into how different backbone architectures\nand input image sizes impact long-term body identification performance across\nreal-world conditions.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.07130v1",
    "published_date": "2025-02-10 23:49:06 UTC",
    "updated_date": "2025-02-10 23:49:06 UTC"
  },
  {
    "arxiv_id": "2502.07128v1",
    "title": "Cardiverse: Harnessing LLMs for Novel Card Game Prototyping",
    "authors": [
      "Danrui Li",
      "Sen Zhang",
      "Sam S. Sohn",
      "Kaidong Hu",
      "Muhammad Usman",
      "Mubbasir Kapadia"
    ],
    "abstract": "The prototyping of computer games, particularly card games, requires\nextensive human effort in creative ideation and gameplay evaluation. Recent\nadvances in Large Language Models (LLMs) offer opportunities to automate and\nstreamline these processes. However, it remains challenging for LLMs to design\nnovel game mechanics beyond existing databases, generate consistent gameplay\nenvironments, and develop scalable gameplay AI for large-scale evaluations.\nThis paper addresses these challenges by introducing a comprehensive automated\ncard game prototyping framework. The approach highlights a graph-based indexing\nmethod for generating novel game designs, an LLM-driven system for consistent\ngame code generation validated by gameplay records, and a gameplay AI\nconstructing method that uses an ensemble of LLM-generated action-value\nfunctions optimized through self-play. These contributions aim to accelerate\ncard game prototyping, reduce human labor, and lower barriers to entry for game\ndevelopers.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.MM"
    ],
    "primary_category": "cs.CL",
    "comment": "13 pages, 7 figures, 3 tables",
    "pdf_url": "http://arxiv.org/pdf/2502.07128v1",
    "published_date": "2025-02-10 23:47:35 UTC",
    "updated_date": "2025-02-10 23:47:35 UTC"
  },
  {
    "arxiv_id": "2502.15754v1",
    "title": "Text2Net: Transforming Plain-text To A Dynamic Interactive Network Simulation Environment",
    "authors": [
      "Alireza Marefat",
      "Abbaas Alif Mohamed Nishar",
      "Ashwin Ashok"
    ],
    "abstract": "This paper introduces Text2Net, an innovative text-based network simulation\nengine that leverages natural language processing (NLP) and large language\nmodels (LLMs) to transform plain-text descriptions of network topologies into\ndynamic, interactive simulations. Text2Net simplifies the process of\nconfiguring network simulations, eliminating the need for users to master\nvendor-specific syntaxes or navigate complex graphical interfaces. Through\nqualitative and quantitative evaluations, we demonstrate Text2Net's ability to\nsignificantly reduce the time and effort required to deploy network scenarios\ncompared to traditional simulators like EVE-NG. By automating repetitive tasks\nand enabling intuitive interaction, Text2Net enhances accessibility for\nstudents, educators, and professionals. The system facilitates hands-on\nlearning experiences for students that bridge the gap between theoretical\nknowledge and practical application. The results showcase its scalability\nacross various network complexities, marking a significant step toward\nrevolutionizing network education and professional use cases, such as\nproof-of-concept testing.",
    "categories": [
      "cs.NI",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.NI",
    "comment": "7 pages, 9 figures, Accepted at IEEE SoutheastCon 2025",
    "pdf_url": "http://arxiv.org/pdf/2502.15754v1",
    "published_date": "2025-02-10 23:45:57 UTC",
    "updated_date": "2025-02-10 23:45:57 UTC"
  },
  {
    "arxiv_id": "2502.07115v4",
    "title": "Online Scheduling for LLM Inference with KV Cache Constraints",
    "authors": [
      "Patrick Jaillet",
      "Jiashuo Jiang",
      "Konstantina Mellou",
      "Marco Molinaro",
      "Chara Podimata",
      "Zijie Zhou"
    ],
    "abstract": "Large Language Model (LLM) inference, where a trained model generates text\none word at a time in response to user prompts, is a computationally intensive\nprocess requiring efficient scheduling to optimize latency and resource\nutilization. A key challenge in LLM inference is the management of the\nKey-Value (KV) cache, which reduces redundant computations but introduces\nmemory constraints. In this work, we model LLM inference with KV cache\nconstraints theoretically and propose a novel batching and scheduling algorithm\nthat minimizes inference latency while effectively managing the KV cache's\nmemory.\n  More specifically, we make the following contributions. First, to evaluate\nthe performance of online algorithms for scheduling in LLM inference, we\nintroduce a hindsight optimal benchmark, formulated as an integer program that\ncomputes the minimum total inference latency under full future information.\nSecond, we prove that no deterministic online algorithm can achieve a constant\ncompetitive ratio when the arrival process is arbitrary. Third, motivated by\nthe computational intractability of solving the integer program at scale, we\npropose a polynomial-time online scheduling algorithm and show that under\ncertain conditions it can achieve a constant competitive ratio. We also\ndemonstrate our algorithm's strong empirical performance by comparing it to the\nhindsight optimal in a synthetic dataset. Finally, we conduct empirical\nevaluations on a real-world public LLM inference dataset, simulating the\nLlama2-70B model on A100 GPUs, and show that our algorithm significantly\noutperforms the benchmark algorithms. Overall, our results offer a path toward\nmore sustainable and cost-effective LLM deployment.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "math.OC"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.07115v4",
    "published_date": "2025-02-10 23:11:44 UTC",
    "updated_date": "2025-05-20 16:29:52 UTC"
  },
  {
    "arxiv_id": "2502.07090v2",
    "title": "Generative Distribution Prediction: A Unified Approach to Multimodal Learning",
    "authors": [
      "Xinyu Tian",
      "Xiaotong Shen"
    ],
    "abstract": "Accurate prediction with multimodal data-encompassing tabular, textual, and\nvisual inputs or outputs-is fundamental to advancing analytics in diverse\napplication domains. Traditional approaches often struggle to integrate\nheterogeneous data types while maintaining high predictive accuracy. We\nintroduce Generative Distribution Prediction (GDP), a novel framework that\nleverages multimodal synthetic data generation-such as conditional diffusion\nmodels-to enhance predictive performance across structured and unstructured\nmodalities. GDP is model-agnostic, compatible with any high-fidelity generative\nmodel, and supports transfer learning for domain adaptation. We establish a\nrigorous theoretical foundation for GDP, providing statistical guarantees on\nits predictive accuracy when using diffusion models as the generative backbone.\nBy estimating the data-generating distribution and adapting to various loss\nfunctions for risk minimization, GDP enables accurate point predictions across\nmultimodal settings. We empirically validate GDP on four supervised learning\ntasks-tabular data prediction, question answering, image captioning, and\nadaptive quantile regression-demonstrating its versatility and effectiveness\nacross diverse domains.",
    "categories": [
      "stat.ML",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "stat.ML",
    "comment": "31 pages 4 figures",
    "pdf_url": "http://arxiv.org/pdf/2502.07090v2",
    "published_date": "2025-02-10 22:30:35 UTC",
    "updated_date": "2025-03-09 17:40:18 UTC"
  },
  {
    "arxiv_id": "2502.07072v3",
    "title": "IRepair: An Intent-Aware Approach to Repair Data-Driven Errors in Large Language Models",
    "authors": [
      "Sayem Mohammad Imtiaz",
      "Astha Singh",
      "Fraol Batole",
      "Hridesh Rajan"
    ],
    "abstract": "Not a day goes by without hearing about the impressive feats of large\nlanguage models (LLMs), and equally, not a day passes without hearing about\ntheir challenges. LLMs are notoriously vulnerable to biases in their dataset,\nleading to issues such as toxicity. While domain-adaptive training has been\nemployed to mitigate these issues, these techniques often address all model\nparameters indiscriminately during the repair process, resulting in poor repair\nquality and reduced model versatility. In this paper, we introduce a novel\ndynamic slicing-based intent-aware LLM repair strategy, IRepair. This approach\nselectively targets the most error-prone sections of the model for repair.\nSpecifically, we propose dynamically slicing the model's most sensitive layers\nthat require immediate attention, concentrating repair efforts on those areas.\nThis method enables more effective repairs with potentially less impact on the\nmodel's overall performance by altering a smaller portion of the model. We\nevaluated our technique on three models from the GPT2 and GPT-Neo families,\nwith parameters ranging from 800M to 1.6B, in a toxicity mitigation setup. Our\nresults show that IRepair repairs errors 43.6% more effectively while causing\n46% less disruption to general performance compared to the closest baseline,\ndirect preference optimization. Our empirical analysis also reveals that errors\nare more concentrated in a smaller section of the model, with the top 20% of\nlayers exhibiting 773% more error density than the remaining 80\\%. This\nhighlights the need for selective repair. Additionally, we demonstrate that a\ndynamic selection approach is essential for addressing errors dispersed\nthroughout the model, ensuring a robust and efficient repair.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.SE"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted as full research paper at FSE'2025",
    "pdf_url": "http://arxiv.org/pdf/2502.07072v3",
    "published_date": "2025-02-10 22:07:02 UTC",
    "updated_date": "2025-03-11 17:08:05 UTC"
  },
  {
    "arxiv_id": "2502.07064v1",
    "title": "Contextual Thompson Sampling via Generation of Missing Data",
    "authors": [
      "Kelly W. Zhang",
      "Tiffany Tianhui Cai",
      "Hongseok Namkoong",
      "Daniel Russo"
    ],
    "abstract": "We introduce a framework for Thompson sampling contextual bandit algorithms,\nin which the algorithm's ability to quantify uncertainty and make decisions\ndepends on the quality of a generative model that is learned offline. Instead\nof viewing uncertainty in the environment as arising from unobservable latent\nparameters, our algorithm treats uncertainty as stemming from missing, but\npotentially observable, future outcomes. If these future outcomes were all\nobserved, one could simply make decisions using an \"oracle\" policy fit on the\ncomplete dataset. Inspired by this conceptualization, at each decision-time,\nour algorithm uses a generative model to probabilistically impute missing\nfuture outcomes, fits a policy using the imputed complete dataset, and uses\nthat policy to select the next action. We formally show that this algorithm is\na generative formulation of Thompson Sampling and prove a state-of-the-art\nregret bound for it. Notably, our regret bound i) depends on the probabilistic\ngenerative model only through the quality of its offline prediction loss, and\nii) applies to any method of fitting the \"oracle\" policy, which easily allows\none to adapt Thompson sampling to decision-making settings with fairness and/or\nresource constraints.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.07064v1",
    "published_date": "2025-02-10 21:57:00 UTC",
    "updated_date": "2025-02-10 21:57:00 UTC"
  },
  {
    "arxiv_id": "2502.07059v1",
    "title": "Federated Continual Learning: Concepts, Challenges, and Solutions",
    "authors": [
      "Parisa Hamedi",
      "Roozbeh Razavi-Far",
      "Ehsan Hallaji"
    ],
    "abstract": "Federated Continual Learning (FCL) has emerged as a robust solution for\ncollaborative model training in dynamic environments, where data samples are\ncontinuously generated and distributed across multiple devices. This survey\nprovides a comprehensive review of FCL, focusing on key challenges such as\nheterogeneity, model stability, communication overhead, and privacy\npreservation. We explore various forms of heterogeneity and their impact on\nmodel performance. Solutions to non-IID data, resource-constrained platforms,\nand personalized learning are reviewed in an effort to show the complexities of\nhandling heterogeneous data distributions. Next, we review techniques for\nensuring model stability and avoiding catastrophic forgetting, which are\ncritical in non-stationary environments. Privacy-preserving techniques are\nanother aspect of FCL that have been reviewed in this work. This survey has\nintegrated insights from federated learning and continual learning to present\nstrategies for improving the efficacy and scalability of FCL systems, making it\napplicable to a wide range of real-world scenarios.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.07059v1",
    "published_date": "2025-02-10 21:51:02 UTC",
    "updated_date": "2025-02-10 21:51:02 UTC"
  },
  {
    "arxiv_id": "2502.07056v1",
    "title": "Autonomous Deep Agent",
    "authors": [
      "Amy Yu",
      "Erik Lebedev",
      "Lincoln Everett",
      "Xiaoxin Chen",
      "Terry Chen"
    ],
    "abstract": "This technical brief introduces Deep Agent, an advanced autonomous AI system\ndesigned to manage complex multi-phase tasks through a novel hierarchical task\nmanagement architecture. The system's foundation is built on our Hierarchical\nTask DAG (HTDAG) framework, which dynamically decomposes high-level objectives\ninto manageable sub-tasks while rigorously maintaining dependencies and\nexecution coherence. Deep Agent advances beyond traditional agent systems\nthrough three key innovations: First, it implements a recursive two-stage\nplanner-executor architecture that enables continuous task refinement and\nadaptation as circumstances change. Second, it features an Autonomous API &\nTool Creation (AATC) system that automatically generates reusable components\nfrom UI interactions, substantially reducing operational costs for similar\ntasks. Third, it incorporates Prompt Tweaking Engine and Autonomous Prompt\nFeedback Learning components that optimize Large Language Model prompts for\nspecific scenarios, enhancing both inference accuracy and operational\nstability. These components are integrated to form a service infrastructure\nthat manages user contexts, handles complex task dependencies, and orchestrates\nend-to-end agentic workflow execution. Through this sophisticated architecture,\nDeep Agent establishes a novel paradigm in self-governing AI systems,\ndemonstrating robust capability to independently handle intricate, multi-step\ntasks while maintaining consistent efficiency and reliability through\ncontinuous self-optimization.",
    "categories": [
      "cs.AI",
      "cs.LG",
      "I.2.6; I.2.7"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.07056v1",
    "published_date": "2025-02-10 21:46:54 UTC",
    "updated_date": "2025-02-10 21:46:54 UTC"
  },
  {
    "arxiv_id": "2502.07049v2",
    "title": "LLMs in Software Security: A Survey of Vulnerability Detection Techniques and Insights",
    "authors": [
      "Ze Sheng",
      "Zhicheng Chen",
      "Shuning Gu",
      "Heqing Huang",
      "Guofei Gu",
      "Jeff Huang"
    ],
    "abstract": "Large Language Models (LLMs) are emerging as transformative tools for\nsoftware vulnerability detection, addressing critical challenges in the\nsecurity domain. Traditional methods, such as static and dynamic analysis,\noften falter due to inefficiencies, high false positive rates, and the growing\ncomplexity of modern software systems. By leveraging their ability to analyze\ncode structures, identify patterns, and generate repair suggestions, LLMs,\nexemplified by models like GPT, BERT, and CodeBERT, present a novel and\nscalable approach to mitigating vulnerabilities. This paper provides a detailed\nsurvey of LLMs in vulnerability detection. It examines key aspects, including\nmodel architectures, application methods, target languages, fine-tuning\nstrategies, datasets, and evaluation metrics. We also analyze the scope of\ncurrent research problems, highlighting the strengths and weaknesses of\nexisting approaches. Further, we address challenges such as cross-language\nvulnerability detection, multimodal data integration, and repository-level\nanalysis. Based on these findings, we propose solutions for issues like dataset\nscalability, model interpretability, and applications in low-resource\nscenarios. Our contributions are threefold: (1) a systematic review of how LLMs\nare applied in vulnerability detection; (2) an analysis of shared patterns and\ndifferences across studies, with a unified framework for understanding the\nfield; and (3) a summary of key challenges and future research directions. This\nwork provides valuable insights for advancing LLM-based vulnerability\ndetection. We also maintain and regularly update latest selected paper on\nhttps://github.com/OwenSanzas/LLM-For-Vulnerability-Detection",
    "categories": [
      "cs.CR",
      "cs.AI"
    ],
    "primary_category": "cs.CR",
    "comment": "33 pages, 12 figures",
    "pdf_url": "http://arxiv.org/pdf/2502.07049v2",
    "published_date": "2025-02-10 21:33:38 UTC",
    "updated_date": "2025-02-12 23:19:23 UTC"
  },
  {
    "arxiv_id": "2502.07046v2",
    "title": "SnipGen: A Mining Repository Framework for Evaluating LLMs for Code",
    "authors": [
      "Daniel Rodriguez-Cardenas",
      "Alejandro Velasco",
      "Denys Poshyvanyk"
    ],
    "abstract": "Language Models (LLMs), such as transformer-based neural networks trained on\nbillions of parameters, have become increasingly prevalent in software\nengineering (SE). These models, trained on extensive datasets that include code\nrepositories, exhibit remarkable capabilities for SE tasks. However, evaluating\ntheir effectiveness poses significant challenges, primarily due to the\npotential overlap between the datasets used for training and those employed for\nevaluation. To address this issue, we introduce SnipGen, a comprehensive\nrepository mining framework designed to leverage prompt engineering across\nvarious downstream tasks for code generation. SnipGen aims to mitigate data\ncontamination by generating robust testbeds and crafting tailored data points\nto assist researchers and practitioners in evaluating LLMs for code-related\ntasks. In our exploratory study, SnipGen mined approximately 227K data points\nfrom 338K recent code changes in GitHub commits, focusing on method-level\ngranularity. SnipGen features a collection of prompt templates that can be\ncombined to create a Chain-of-Thought-like sequence of prompts, enabling a\nnuanced assessment of LLMs' code generation quality. By providing the mining\ntool, the methodology, and the dataset, SnipGen empowers researchers and\npractitioners to rigorously evaluate and interpret LLMs' performance in\nsoftware engineering contexts.",
    "categories": [
      "cs.SE",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.SE",
    "comment": "5 pages, 3 figures, 2 tables",
    "pdf_url": "http://arxiv.org/pdf/2502.07046v2",
    "published_date": "2025-02-10 21:28:15 UTC",
    "updated_date": "2025-02-16 18:39:08 UTC"
  },
  {
    "arxiv_id": "2502.07045v2",
    "title": "Scalable and Ethical Insider Threat Detection through Data Synthesis and Analysis by LLMs",
    "authors": [
      "Haywood Gelman",
      "John D. Hastings"
    ],
    "abstract": "Insider threats wield an outsized influence on organizations,\ndisproportionate to their small numbers. This is due to the internal access\ninsiders have to systems, information, and infrastructure. %One example of this\ninfluence is where anonymous respondents submit web-based job search site\nreviews, an insider threat risk to organizations. Signals for such risks may be\nfound in anonymous submissions to public web-based job search site reviews.\nThis research studies the potential for large language models (LLMs) to analyze\nand detect insider threat sentiment within job site reviews. Addressing ethical\ndata collection concerns, this research utilizes synthetic data generation\nusing LLMs alongside existing job review datasets. A comparative analysis of\nsentiment scores generated by LLMs is benchmarked against expert human scoring.\nFindings reveal that LLMs demonstrate alignment with human evaluations in most\ncases, thus effectively identifying nuanced indicators of threat sentiment. The\nperformance is lower on human-generated data than synthetic data, suggesting\nareas for improvement in evaluating real-world data. Text diversity analysis\nfound differences between human-generated and LLM-generated datasets, with\nsynthetic data exhibiting somewhat lower diversity. Overall, the results\ndemonstrate the applicability of LLMs to insider threat detection, and a\nscalable solution for insider sentiment testing by overcoming ethical and\nlogistical barriers tied to data acquisition.",
    "categories": [
      "cs.CR",
      "cs.AI",
      "cs.CL",
      "cs.CY",
      "C.2.0; I.2.7; K.4.1; H.3.3"
    ],
    "primary_category": "cs.CR",
    "comment": "6 pages, 0 figures, 8 tables",
    "pdf_url": "http://arxiv.org/pdf/2502.07045v2",
    "published_date": "2025-02-10 21:27:06 UTC",
    "updated_date": "2025-04-07 16:01:47 UTC"
  },
  {
    "arxiv_id": "2502.07036v2",
    "title": "Automated Consistency Analysis of LLMs",
    "authors": [
      "Aditya Patwardhan",
      "Vivek Vaidya",
      "Ashish Kundu"
    ],
    "abstract": "Generative AI (Gen AI) with large language models (LLMs) are being widely\nadopted across the industry, academia and government. Cybersecurity is one of\nthe key sectors where LLMs can be and/or are already being used. There are a\nnumber of problems that inhibit the adoption of trustworthy Gen AI and LLMs in\ncybersecurity and such other critical areas. One of the key challenge to the\ntrustworthiness and reliability of LLMs is: how consistent an LLM is in its\nresponses? In this paper, we have analyzed and developed a formal definition of\nconsistency of responses of LLMs. We have formally defined what is consistency\nof responses and then develop a framework for consistency evaluation. The paper\nproposes two approaches to validate consistency: self-validation, and\nvalidation across multiple LLMs. We have carried out extensive experiments for\nseveral LLMs such as GPT4oMini, GPT3.5, Gemini, Cohere, and Llama3, on a\nsecurity benchmark consisting of several cybersecurity questions: informational\nand situational. Our experiments corroborate the fact that even though these\nLLMs are being considered and/or already being used for several cybersecurity\ntasks today, they are often inconsistent in their responses, and thus are\nuntrustworthy and unreliable for cybersecurity.",
    "categories": [
      "cs.CR",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CR",
    "comment": "10 pages, 12 figures, 3 tables, 3 algorithms, 2024 IEEE 6th\n  International Conference on Trust, Privacy and Security in Intelligent\n  Systems, and Applications (TPS-ISA), Washington, DC, USA",
    "pdf_url": "http://arxiv.org/pdf/2502.07036v2",
    "published_date": "2025-02-10 21:03:24 UTC",
    "updated_date": "2025-03-10 18:14:34 UTC"
  },
  {
    "arxiv_id": "2502.07029v2",
    "title": "Leveraging Allophony in Self-Supervised Speech Models for Atypical Pronunciation Assessment",
    "authors": [
      "Kwanghee Choi",
      "Eunjung Yeo",
      "Kalvin Chang",
      "Shinji Watanabe",
      "David Mortensen"
    ],
    "abstract": "Allophony refers to the variation in the phonetic realization of a phoneme\nbased on its phonetic environment. Modeling allophones is crucial for atypical\npronunciation assessment, which involves distinguishing atypical from typical\npronunciations. However, recent phoneme classifier-based approaches often\nsimplify this by treating various realizations as a single phoneme, bypassing\nthe complexity of modeling allophonic variation. Motivated by the acoustic\nmodeling capabilities of frozen self-supervised speech model (S3M) features, we\npropose MixGoP, a novel approach that leverages Gaussian mixture models to\nmodel phoneme distributions with multiple subclusters. Our experiments show\nthat MixGoP achieves state-of-the-art performance across four out of five\ndatasets, including dysarthric and non-native speech. Our analysis further\nsuggests that S3M features capture allophonic variation more effectively than\nMFCCs and Mel spectrograms, highlighting the benefits of integrating MixGoP\nwith S3M features.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG",
      "eess.AS"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted to NAACL 2025. Codebase available at\n  https://github.com/juice500ml/acoustic-units-for-ood",
    "pdf_url": "http://arxiv.org/pdf/2502.07029v2",
    "published_date": "2025-02-10 20:46:42 UTC",
    "updated_date": "2025-03-24 03:38:32 UTC"
  },
  {
    "arxiv_id": "2502.07026v2",
    "title": "Machine Learning for Everyone: Simplifying Healthcare Analytics with BigQuery ML",
    "authors": [
      "Mohammad Amir Salari",
      "Bahareh Rahmani"
    ],
    "abstract": "Machine learning (ML) transforms healthcare by enabling predictive analytics,\npersonalized treatments, and improved patient outcomes. However, traditional ML\nworkflows often require specialized skills, infrastructure, and resources,\nlimiting accessibility for many healthcare professionals. This paper explores\nhow BigQuery ML Cloud service helps healthcare researchers and data analysts to\nbuild and deploy models using SQL, without need for advanced ML knowledge. Our\nresults demonstrate that the Boosted Tree model achieved the highest\nperformance among the three models making it highly effective for diabetes\nprediction. BigQuery ML directly integrates predictive analytics into their\nworkflows to inform decision-making and support patient care. We reveal this\ncapability through a case study on diabetes prediction using the Diabetes\nHealth Indicators Dataset. Our study underscores BigQuery ML's role in\ndemocratizing machine learning, enabling faster, scalable, and efficient\npredictive analytics that can directly enhance healthcare decision-making\nprocesses. This study aims to bridge the gap between advanced machine learning\nand practical healthcare analytics by providing detailed insights into BigQuery\nML's capabilities. By demonstrating its utility in a real-world case study, we\nhighlight its potential to simplify complex workflows and expand access to\npredictive tools for a broader audience of healthcare professionals.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "Focus: Artificial Intelligence, Healthcare analytics, cloud\n  computing, BigQuery ML",
    "pdf_url": "http://arxiv.org/pdf/2502.07026v2",
    "published_date": "2025-02-10 20:38:53 UTC",
    "updated_date": "2025-02-21 21:02:55 UTC"
  },
  {
    "arxiv_id": "2502.07022v1",
    "title": "AIMS.au: A Dataset for the Analysis of Modern Slavery Countermeasures in Corporate Statements",
    "authors": [
      "Adriana Eufrosina Bora",
      "Pierre-Luc St-Charles",
      "Mirko Bronzi",
      "Arsène Fansi Tchango",
      "Bruno Rousseau",
      "Kerrie Mengersen"
    ],
    "abstract": "Despite over a decade of legislative efforts to address modern slavery in the\nsupply chains of large corporations, the effectiveness of government oversight\nremains hampered by the challenge of scrutinizing thousands of statements\nannually. While Large Language Models (LLMs) can be considered a well\nestablished solution for the automatic analysis and summarization of documents,\nrecognizing concrete modern slavery countermeasures taken by companies and\ndifferentiating those from vague claims remains a challenging task. To help\nevaluate and fine-tune LLMs for the assessment of corporate statements, we\nintroduce a dataset composed of 5,731 modern slavery statements taken from the\nAustralian Modern Slavery Register and annotated at the sentence level. This\npaper details the construction steps for the dataset that include the careful\ndesign of annotation specifications, the selection and preprocessing of\nstatements, and the creation of high-quality annotation subsets for effective\nmodel evaluations. To demonstrate our dataset's utility, we propose a machine\nlearning methodology for the detection of sentences relevant to mandatory\nreporting requirements set by the Australian Modern Slavery Act. We then follow\nthis methodology to benchmark modern language models under zero-shot and\nsupervised learning settings.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "Camera ready. ICLR 2025",
    "pdf_url": "http://arxiv.org/pdf/2502.07022v1",
    "published_date": "2025-02-10 20:30:32 UTC",
    "updated_date": "2025-02-10 20:30:32 UTC"
  },
  {
    "arxiv_id": "2502.07017v1",
    "title": "Finding Words Associated with DIF: Predicting Differential Item Functioning using LLMs and Explainable AI",
    "authors": [
      "Hotaka Maeda",
      "Yikai Lu"
    ],
    "abstract": "We fine-tuned and compared several encoder-based Transformer large language\nmodels (LLM) to predict differential item functioning (DIF) from the item text.\nWe then applied explainable artificial intelligence (XAI) methods to these\nmodels to identify specific words associated with DIF. The data included 42,180\nitems designed for English language arts and mathematics summative state\nassessments among students in grades 3 to 11. Prediction $R^2$ ranged from .04\nto .32 among eight focal and reference group pairs. Our findings suggest that\nmany words associated with DIF reflect minor sub-domains included in the test\nblueprint by design, rather than construct-irrelevant item content that should\nbe removed from assessments. This may explain why qualitative reviews of DIF\nitems often yield confusing or inconclusive results. Our approach can be used\nto screen words associated with DIF during the item-writing process for\nimmediate revision, or help review traditional DIF analysis results by\nhighlighting key words in the text. Extensions of this research can enhance the\nfairness of assessment programs, especially those that lack resources to build\nhigh-quality items, and among smaller subpopulations where we do not have\nsufficient sample sizes for traditional DIF analyses.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "14 pages, 2 figures, 6 tables",
    "pdf_url": "http://arxiv.org/pdf/2502.07017v1",
    "published_date": "2025-02-10 20:22:32 UTC",
    "updated_date": "2025-02-10 20:22:32 UTC"
  },
  {
    "arxiv_id": "2502.07828v1",
    "title": "Some things to know about achieving artificial general intelligence",
    "authors": [
      "Herbert Roitblat"
    ],
    "abstract": "Current and foreseeable GenAI models are not capable of achieving artificial\ngeneral intelligence because they are burdened with anthropogenic debt. They\ndepend heavily on human input to provide well-structured problems,\narchitecture, and training data. They cast every problem as a language pattern\nlearning problem and are thus not capable of the kind of autonomy needed to\nachieve artificial general intelligence. Current models succeed at their tasks\nbecause people solve most of the problems to which these models are directed,\nleaving only simple computations for the model to perform, such as gradient\ndescent. Another barrier is the need to recognize that there are multiple kinds\nof problems, some of which cannot be solved by available computational methods\n(for example, \"insight problems\"). Current methods for evaluating models\n(benchmarks and tests) are not adequate to identify the generality of the\nsolutions, because it is impossible to infer the means by which a problem was\nsolved from the fact of its solution. A test could be passed, for example, by a\ntest-specific or a test-general method. It is a logical fallacy (affirming the\nconsequent) to infer a method of solution from the observation of success.",
    "categories": [
      "q-bio.NC",
      "cs.AI"
    ],
    "primary_category": "q-bio.NC",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.07828v1",
    "published_date": "2025-02-10 20:10:26 UTC",
    "updated_date": "2025-02-10 20:10:26 UTC"
  },
  {
    "arxiv_id": "2502.07827v2",
    "title": "Implicit Language Models are RNNs: Balancing Parallelization and Expressivity",
    "authors": [
      "Mark Schöne",
      "Babak Rahmani",
      "Heiner Kremer",
      "Fabian Falck",
      "Hitesh Ballani",
      "Jannes Gladrow"
    ],
    "abstract": "State-space models (SSMs) and transformers dominate the language modeling\nlandscape. However, they are constrained to a lower computational complexity\nthan classical recurrent neural networks (RNNs), limiting their expressivity.\nIn contrast, RNNs lack parallelization during training, raising fundamental\nquestions about the trade off between parallelization and expressivity. We\npropose implicit SSMs, which iterate a transformation until convergence to a\nfixed point. Theoretically, we show that implicit SSMs implement the non-linear\nstate-transitions of RNNs. Empirically, we find that only approximate\nfixed-point convergence suffices, enabling the design of a scalable training\ncurriculum that largely retains parallelization, with full convergence required\nonly for a small subset of tokens. Our approach demonstrates superior\nstate-tracking capabilities on regular languages, surpassing transformers and\nSSMs. We further scale implicit SSMs to natural language reasoning tasks and\npretraining of large-scale language models up to 1.3B parameters on 207B tokens\n- representing, to our knowledge, the largest implicit model trained to date.\nNotably, our implicit models outperform their explicit counterparts on standard\nbenchmarks.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "25 pages, 12 figures, 7 tables",
    "pdf_url": "http://arxiv.org/pdf/2502.07827v2",
    "published_date": "2025-02-10 19:59:31 UTC",
    "updated_date": "2025-02-14 10:17:31 UTC"
  },
  {
    "arxiv_id": "2502.07001v2",
    "title": "From Image to Video: An Empirical Study of Diffusion Representations",
    "authors": [
      "Pedro Vélez",
      "Luisa F. Polanía",
      "Yi Yang",
      "Chuhan Zhang",
      "Rishabh Kabra",
      "Anurag Arnab",
      "Mehdi S. M. Sajjadi"
    ],
    "abstract": "Diffusion models have revolutionized generative modeling, enabling\nunprecedented realism in image and video synthesis. This success has sparked\ninterest in leveraging their representations for visual understanding tasks.\nWhile recent works have explored this potential for image generation, the\nvisual understanding capabilities of video diffusion models remain largely\nuncharted. To address this gap, we systematically compare the same model\narchitecture trained for video versus image generation, analyzing the\nperformance of their latent representations on various downstream tasks\nincluding image classification, action recognition, depth estimation, and\ntracking. Results show that video diffusion models consistently outperform\ntheir image counterparts, though we find a striking range in the extent of this\nsuperiority. We further analyze features extracted from different layers and\nwith varying noise levels, as well as the effect of model size and training\nbudget on representation and generation quality. This work marks the first\ndirect comparison of video and image diffusion objectives for visual\nunderstanding, offering insights into the role of temporal information in\nrepresentation learning.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.07001v2",
    "published_date": "2025-02-10 19:53:46 UTC",
    "updated_date": "2025-03-19 21:27:28 UTC"
  },
  {
    "arxiv_id": "2502.06994v1",
    "title": "SyncMind: Measuring Agent Out-of-Sync Recovery in Collaborative Software Engineering",
    "authors": [
      "Xuehang Guo",
      "Xingyao Wang",
      "Yangyi Chen",
      "Sha Li",
      "Chi Han",
      "Manling Li",
      "Heng Ji"
    ],
    "abstract": "Software engineering (SE) is increasingly collaborative, with developers\nworking together on shared complex codebases. Effective collaboration in shared\nenvironments requires participants -- whether humans or AI agents -- to stay on\nthe same page as their environment evolves. When a collaborator's understanding\ndiverges from the current state -- what we term the out-of-sync challenge --\nthe collaborator's actions may fail, leading to integration issues. In this\nwork, we introduce SyncMind, a framework that systematically defines the\nout-of-sync problem faced by large language model (LLM) agents in collaborative\nsoftware engineering (CSE). Based on SyncMind, we create SyncBench, a benchmark\nfeaturing 24,332 instances of agent out-of-sync scenarios in real-world CSE\nderived from 21 popular GitHub repositories with executable verification tests.\nExperiments on SyncBench uncover critical insights into existing LLM agents'\ncapabilities and limitations. Besides substantial performance gaps among agents\n(from Llama-3.1 agent <= 3.33% to Claude-3.5-Sonnet >= 28.18%), their\nconsistently low collaboration willingness (<= 4.86%) suggests fundamental\nlimitations of existing LLM in CSE. However, when collaboration occurs, it\npositively correlates with out-of-sync recovery success. Minimal performance\ndifferences in agents' resource-aware out-of-sync recoveries further reveal\ntheir significant lack of resource awareness and adaptability, shedding light\non future resource-efficient collaborative systems. Code and data are openly\navailable on our project website: https://xhguo7.github.io/SyncMind/.",
    "categories": [
      "cs.SE",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.SE",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.06994v1",
    "published_date": "2025-02-10 19:38:36 UTC",
    "updated_date": "2025-02-10 19:38:36 UTC"
  },
  {
    "arxiv_id": "2502.06976v1",
    "title": "Who is Helping Whom? Analyzing Inter-dependencies to Evaluate Cooperation in Human-AI Teaming",
    "authors": [
      "Upasana Biswas",
      "Siddhant Bhambri",
      "Subbarao Kambhampati"
    ],
    "abstract": "The long-standing research challenges of Human-AI Teaming(HAT) and Zero-shot\nCooperation(ZSC) have been tackled by applying multi-agent reinforcement\nlearning(MARL) to train an agent by optimizing the environment reward function\nand evaluating their performance through task performance metrics such as task\nreward. However, such evaluation focuses only on task completion, while being\nagnostic to `how' the two agents work with each other. Specifically, we are\ninterested in understanding the cooperation arising within the team when\ntrained agents are paired with humans. To formally address this problem, we\npropose the concept of interdependence to measure how much agents rely on each\nother's actions to achieve the shared goal, as a key metric for evaluating\ncooperation in human-agent teams. Towards this, we ground this concept through\na symbolic formalism and define evaluation metrics that allow us to assess the\ndegree of reliance between the agents' actions. We pair state-of-the-art agents\ntrained through MARL for HAT, with learned human models for the the popular\nOvercooked domain, and evaluate the team performance for these human-agent\nteams. Our results demonstrate that trained agents are not able to induce\ncooperative behavior, reporting very low levels of interdependence across all\nthe teams. We also report that teaming performance of a team is not necessarily\ncorrelated with the task reward.",
    "categories": [
      "cs.MA",
      "cs.AI"
    ],
    "primary_category": "cs.MA",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.06976v1",
    "published_date": "2025-02-10 19:16:20 UTC",
    "updated_date": "2025-02-10 19:16:20 UTC"
  },
  {
    "arxiv_id": "2502.06975v1",
    "title": "Position: Episodic Memory is the Missing Piece for Long-Term LLM Agents",
    "authors": [
      "Mathis Pink",
      "Qinyuan Wu",
      "Vy Ai Vo",
      "Javier Turek",
      "Jianing Mu",
      "Alexander Huth",
      "Mariya Toneva"
    ],
    "abstract": "As Large Language Models (LLMs) evolve from text-completion tools into fully\nfledged agents operating in dynamic environments, they must address the\nchallenge of continually learning and retaining long-term knowledge. Many\nbiological systems solve these challenges with episodic memory, which supports\nsingle-shot learning of instance-specific contexts. Inspired by this, we\npresent an episodic memory framework for LLM agents, centered around five key\nproperties of episodic memory that underlie adaptive and context-sensitive\nbehavior. With various research efforts already partially covering these\nproperties, this position paper argues that now is the right time for an\nexplicit, integrated focus on episodic memory to catalyze the development of\nlong-term agents. To this end, we outline a roadmap that unites several\nresearch directions under the goal to support all five properties of episodic\nmemory for more efficient long-term LLM agents.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.06975v1",
    "published_date": "2025-02-10 19:14:51 UTC",
    "updated_date": "2025-02-10 19:14:51 UTC"
  },
  {
    "arxiv_id": "2502.06963v1",
    "title": "Task Offloading in Vehicular Edge Computing using Deep Reinforcement Learning: A Survey",
    "authors": [
      "Ashab Uddin",
      "Ahmed Hamdi Sakr",
      "Ning Zhang"
    ],
    "abstract": "The increasing demand for Intelligent Transportation Systems (ITS) has\nintroduced significant challenges in managing the complex,\ncomputation-intensive tasks generated by modern vehicles while offloading tasks\nto external computing infrastructures such as edge computing (EC), nearby\nvehicular , and UAVs has become influential solution to these challenges.\nHowever, traditional computational offloading strategies often struggle to\nadapt to the dynamic and heterogeneous nature of vehicular environments. In\nthis study, we explored the potential of Reinforcement Learning (RL) and Deep\nReinforcement Learning (DRL) frameworks to optimize computational offloading\nthrough adaptive, real-time decision-making, and we have thoroughly\ninvestigated the Markov Decision Process (MDP) approaches on the existing\nliterature. The paper focuses on key aspects such as standardized learning\nmodels, optimized reward structures, and collaborative multi-agent systems,\naiming to advance the understanding and application of DRL in vehicular\nnetworks. Our findings offer insights into enhancing the efficiency,\nscalability, and robustness of ITS, setting the stage for future innovations in\nthis rapidly evolving field.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.DC",
      "cs.MA",
      "Machine Learning, Reinforcement Learning, Multi Agent Reinforcement\n  Learning, Computational Offloading and Edge Computing"
    ],
    "primary_category": "cs.LG",
    "comment": "27 Pages, 3 Figures, 3 Tables",
    "pdf_url": "http://arxiv.org/pdf/2502.06963v1",
    "published_date": "2025-02-10 19:02:20 UTC",
    "updated_date": "2025-02-10 19:02:20 UTC"
  },
  {
    "arxiv_id": "2502.06788v1",
    "title": "EVEv2: Improved Baselines for Encoder-Free Vision-Language Models",
    "authors": [
      "Haiwen Diao",
      "Xiaotong Li",
      "Yufeng Cui",
      "Yueze Wang",
      "Haoge Deng",
      "Ting Pan",
      "Wenxuan Wang",
      "Huchuan Lu",
      "Xinlong Wang"
    ],
    "abstract": "Existing encoder-free vision-language models (VLMs) are rapidly narrowing the\nperformance gap with their encoder-based counterparts, highlighting the\npromising potential for unified multimodal systems with structural simplicity\nand efficient deployment. We systematically clarify the performance gap between\nVLMs using pre-trained vision encoders, discrete tokenizers, and minimalist\nvisual layers from scratch, deeply excavating the under-examined\ncharacteristics of encoder-free VLMs. We develop efficient strategies for\nencoder-free VLMs that rival mainstream encoder-based ones. After an in-depth\ninvestigation, we launch EVEv2.0, a new and improved family of encoder-free\nVLMs. We show that: (i) Properly decomposing and hierarchically associating\nvision and language within a unified model reduces interference between\nmodalities. (ii) A well-designed training strategy enables effective\noptimization for encoder-free VLMs. Through extensive evaluation, our EVEv2.0\nrepresents a thorough study for developing a decoder-only architecture across\nmodalities, demonstrating superior data efficiency and strong vision-reasoning\ncapability. Code is publicly available at: https://github.com/baaivision/EVE.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "19 pages, 9 figures",
    "pdf_url": "http://arxiv.org/pdf/2502.06788v1",
    "published_date": "2025-02-10 18:59:58 UTC",
    "updated_date": "2025-02-10 18:59:58 UTC"
  },
  {
    "arxiv_id": "2502.06786v3",
    "title": "Matryoshka Quantization",
    "authors": [
      "Pranav Nair",
      "Puranjay Datta",
      "Jeff Dean",
      "Prateek Jain",
      "Aditya Kusupati"
    ],
    "abstract": "Quantizing model weights is critical for reducing the communication and\ninference costs of large models. However, quantizing models -- especially to\nlow precisions like int4 or int2 -- requires a trade-off in model quality;\nint2, in particular, is known to severely degrade model quality. Consequently,\npractitioners are often forced to maintain multiple models with different\nquantization levels or serve a single model that best satisfies the\nquality-latency trade-off. On the other hand, integer data types, such as int8,\ninherently possess a nested (Matryoshka) structure where smaller bit-width\nintegers, like int4 or int2, are nested within the most significant bits.\nLeveraging this insight, in this paper, we propose Matryoshka Quantization\n(MatQuant), a novel multi-scale quantization technique that alleviates the\naforementioned challenge. This technique allows us to train and maintain a\nsingle quantized model but serve it with the precision demanded by the\ndeployment. Furthermore, leveraging MatQuant's co-training and co-distillation\nregularization, int2 precision models extracted by MatQuant outperform standard\nint2 quantization by up to to 4% and 7% with OmniQuant and QAT as base\nalgorithms respectively. Finally, we demonstrate that by using an extra bit to\nrepresent outliers, a model with an effective precision of 2.05-bit gives an\nadditional 6% improvement with OmniQuant as the base algorithm.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.06786v3",
    "published_date": "2025-02-10 18:59:10 UTC",
    "updated_date": "2025-03-03 17:54:53 UTC"
  },
  {
    "arxiv_id": "2502.06784v1",
    "title": "RelGNN: Composite Message Passing for Relational Deep Learning",
    "authors": [
      "Tianlang Chen",
      "Charilaos Kanatsoulis",
      "Jure Leskovec"
    ],
    "abstract": "Predictive tasks on relational databases are critical in real-world\napplications spanning e-commerce, healthcare, and social media. To address\nthese tasks effectively, Relational Deep Learning (RDL) encodes relational data\nas graphs, enabling Graph Neural Networks (GNNs) to exploit relational\nstructures for improved predictions. However, existing heterogeneous GNNs often\noverlook the intrinsic structural properties of relational databases, leading\nto modeling inefficiencies. Here we introduce RelGNN, a novel GNN framework\nspecifically designed to capture the unique characteristics of relational\ndatabases. At the core of our approach is the introduction of atomic routes,\nwhich are sequences of nodes forming high-order tripartite structures. Building\nupon these atomic routes, RelGNN designs new composite message passing\nmechanisms between heterogeneous nodes, allowing direct single-hop interactions\nbetween them. This approach avoids redundant aggregations and mitigates\ninformation entanglement, ultimately leading to more efficient and accurate\npredictive modeling. RelGNN is evaluated on 30 diverse real-world tasks from\nRelBench (Fey et al., 2024), and consistently achieves state-of-the-art\naccuracy with up to 25% improvement.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.DB"
    ],
    "primary_category": "cs.LG",
    "comment": "14 pages",
    "pdf_url": "http://arxiv.org/pdf/2502.06784v1",
    "published_date": "2025-02-10 18:58:40 UTC",
    "updated_date": "2025-02-10 18:58:40 UTC"
  },
  {
    "arxiv_id": "2502.06779v1",
    "title": "KARST: Multi-Kernel Kronecker Adaptation with Re-Scaling Transmission for Visual Classification",
    "authors": [
      "Yue Zhu",
      "Haiwen Diao",
      "Shang Gao",
      "Long Chen",
      "Huchuan Lu"
    ],
    "abstract": "Fine-tuning pre-trained vision models for specific tasks is a common practice\nin computer vision. However, this process becomes more expensive as models grow\nlarger. Recently, parameter-efficient fine-tuning (PEFT) methods have emerged\nas a popular solution to improve training efficiency and reduce storage needs\nby tuning additional low-rank modules within pre-trained backbones. Despite\ntheir advantages, they struggle with limited representation capabilities and\nmisalignment with pre-trained intermediate features. To address these issues,\nwe introduce an innovative Multi-Kernel Kronecker Adaptation with Re-Scaling\nTransmission (KARST) for various recognition tasks. Specifically, its\nmulti-kernel design extends Kronecker projections horizontally and separates\nadaptation matrices into multiple complementary spaces, reducing parameter\ndependency and creating more compact subspaces. Besides, it incorporates extra\nlearnable re-scaling factors to better align with pre-trained feature\ndistributions, allowing for more flexible and balanced feature aggregation.\nExtensive experiments validate that our KARST outperforms other PEFT\ncounterparts with a negligible inference cost due to its re-parameterization\ncharacteristics. Code is publicly available at:\nhttps://github.com/Lucenova/KARST.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "5 pages, 3 figures, Accepted by ICASSP2025",
    "pdf_url": "http://arxiv.org/pdf/2502.06779v1",
    "published_date": "2025-02-10 18:56:14 UTC",
    "updated_date": "2025-02-10 18:56:14 UTC"
  },
  {
    "arxiv_id": "2502.06776v1",
    "title": "Towards Internet-Scale Training For Agents",
    "authors": [
      "Brandon Trabucco",
      "Gunnar Sigurdsson",
      "Robinson Piramuthu",
      "Ruslan Salakhutdinov"
    ],
    "abstract": "The predominant approach for training web navigation agents gathers human\ndemonstrations for a set of popular websites and hand-written tasks, but it is\nbecoming clear that human data are an inefficient resource. We develop a\npipeline to facilitate Internet-scale training for agents without laborious\nhuman annotations. In the first stage, an LLM generates tasks for 150k diverse\nwebsites. In the next stage, LLM agents complete tasks and produce\ntrajectories. In the final stage, an LLM reviews the trajectories and judges\ntheir success. Language models are competitive with human annotators, detecting\nand filtering out harmful content with an accuracy of 97%, generating feasible\ntasks with an 89% rate, and judging successful trajectories with an 82.6%\naccuracy. Scaling the pipeline, agents based on Llama 3.1 70B solve 16.7% of\ntasks for 150k sites. Training on the data generated by our pipeline is\ncompetitive with training on human demonstrations. In data-limited settings\nderived from Mind2Web and WebLINX, we improve Step Accuracy by up to +89.5% and\n+122.1% respectively for agents trained on mixtures of data from our pipeline,\nand human data. When training agents with all available human data from these\nbenchmarks, agents fail to generalize to diverse real sites, and adding our\ndata improves their generalization by +149.0% for WebLINX and +156.3% for\nMind2Web. Code will be available at: data-for-agents.github.io.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.06776v1",
    "published_date": "2025-02-10 18:54:05 UTC",
    "updated_date": "2025-02-10 18:54:05 UTC"
  },
  {
    "arxiv_id": "2502.06773v1",
    "title": "On the Emergence of Thinking in LLMs I: Searching for the Right Intuition",
    "authors": [
      "Guanghao Ye",
      "Khiem Duc Pham",
      "Xinzhi Zhang",
      "Sivakanth Gopi",
      "Baolin Peng",
      "Beibin Li",
      "Janardhan Kulkarni",
      "Huseyin A. Inan"
    ],
    "abstract": "Recent AI advancements, such as OpenAI's new models, are transforming LLMs\ninto LRMs (Large Reasoning Models) that perform reasoning during inference,\ntaking extra time and compute for higher-quality outputs. We aim to uncover the\nalgorithmic framework for training LRMs. Methods like self-consistency, PRM,\nand AlphaZero suggest reasoning as guided search. We ask: what is the simplest,\nmost scalable way to enable search in LLMs?\n  We propose a post-training framework called Reinforcement Learning via\nSelf-Play (RLSP). RLSP involves three steps: (1) supervised fine-tuning with\nhuman or synthetic demonstrations of the reasoning process, (2) using an\nexploration reward signal to encourage diverse and efficient reasoning\nbehaviors, and (3) RL training with an outcome verifier to ensure correctness\nwhile preventing reward hacking. Our key innovation is to decouple exploration\nand correctness signals during PPO training, carefully balancing them to\nimprove performance and efficiency.\n  Empirical studies in the math domain show that RLSP improves reasoning. On\nthe Llama-3.1-8B-Instruct model, RLSP can boost performance by 23% in MATH-500\ntest set; On AIME 2024 math problems, Qwen2.5-32B-Instruct improved by 10% due\nto RLSP. However, a more important finding of this work is that the models\ntrained using RLSP, even with the simplest exploration reward that encourages\nthe model to take more intermediate steps, showed several emergent behaviors\nsuch as backtracking, exploration of ideas, and verification. These findings\ndemonstrate that RLSP framework might be enough to enable emergence of complex\nreasoning abilities in LLMs when scaled. Lastly, we propose a theory as to why\nRLSP search strategy is more suitable for LLMs inspired by a remarkable result\nthat says CoT provably increases computational power of LLMs, which grows as\nthe number of steps in CoT \\cite{li2024chain,merrill2023expresssive}.",
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "Abstract shortened for arXiv",
    "pdf_url": "http://arxiv.org/pdf/2502.06773v1",
    "published_date": "2025-02-10 18:52:04 UTC",
    "updated_date": "2025-02-10 18:52:04 UTC"
  },
  {
    "arxiv_id": "2502.06927v1",
    "title": "Neighborhood-Order Learning Graph Attention Network for Fake News Detection",
    "authors": [
      "Batool Lakzaei",
      "Mostafa Haghir Chehreghani",
      "Alireza Bagheri"
    ],
    "abstract": "Fake news detection is a significant challenge in the digital age, which has\nbecome increasingly important with the proliferation of social media and online\ncommunication networks. Graph Neural Networks (GNN)-based methods have shown\nhigh potential in analyzing graph-structured data for this problem. However, a\nmajor limitation in conventional GNN architectures is their inability to\neffectively utilize information from neighbors beyond the network's layer\ndepth, which can reduce the model's accuracy and effectiveness. In this paper,\nwe propose a novel model called Neighborhood-Order Learning Graph Attention\nNetwork (NOL-GAT) for fake news detection. This model allows each node in each\nlayer to independently learn its optimal neighborhood order. By doing so, the\nmodel can purposefully and efficiently extract critical information from\ndistant neighbors. The NOL-GAT architecture consists of two main components: a\nHop Network that determines the optimal neighborhood order and an Embedding\nNetwork that updates node embeddings using these optimal neighborhoods. To\nevaluate the model's performance, experiments are conducted on various fake\nnews datasets. Results demonstrate that NOL-GAT significantly outperforms\nbaseline models in metrics such as accuracy and F1-score, particularly in\nscenarios with limited labeled data. Features such as mitigating the\nover-squashing problem, improving information flow, and reducing computational\ncomplexity further highlight the advantages of the proposed model.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.LG",
    "comment": "37 pages",
    "pdf_url": "http://arxiv.org/pdf/2502.06927v1",
    "published_date": "2025-02-10 18:51:57 UTC",
    "updated_date": "2025-02-10 18:51:57 UTC"
  },
  {
    "arxiv_id": "2502.06772v2",
    "title": "ReasonFlux: Hierarchical LLM Reasoning via Scaling Thought Templates",
    "authors": [
      "Ling Yang",
      "Zhaochen Yu",
      "Bin Cui",
      "Mengdi Wang"
    ],
    "abstract": "We present that hierarchical LLM reasoning via scaling thought templates can\neffectively optimize the reasoning search space and outperform the mathematical\nreasoning capabilities of powerful LLMs like OpenAI o1-preview and DeepSeek V3.\nWe train our ReasonFlux-32B model with only 8 GPUs and introduces three\ninnovations: (i) a structured and generic thought template library, containing\naround 500 high-level thought templates capable of generalizing to similar or\nrelevant reasoning problems; (ii) performing hierarchical reinforcement\nlearning on a sequence of thought templates instead of long CoTs, optimizing a\nbase LLM to plan out an optimal template trajectory for gradually handling\ncomplex problems; (iii) a brand new inference scaling system that enables\nhierarchical LLM reasoning by adaptively scaling thought templates at inference\ntime. With a template trajectory containing more explainable reasoning\nstructures than DeepSeek-R1 and o3-mini, our ReasonFlux-32B significantly\nadvances math reasoning capabilities to state-of-the-art levels. Notably, on\nthe MATH benchmark, it achieves an accuracy of 91.2% and surpasses o1-preview\nby 6.7%. On the USA Math Olympiad (AIME) benchmark, ReasonFlux-32B solves an\naverage of 56.7% of problems, surpassing o1-preview and DeepSeek-V3 by 27% and\n45%, respectively. Code: https://github.com/Gen-Verse/ReasonFlux",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "Code: https://github.com/Gen-Verse/ReasonFlux",
    "pdf_url": "http://arxiv.org/pdf/2502.06772v2",
    "published_date": "2025-02-10 18:51:47 UTC",
    "updated_date": "2025-03-11 02:46:19 UTC"
  },
  {
    "arxiv_id": "2502.06759v4",
    "title": "Rationalization Models for Text-to-SQL",
    "authors": [
      "Gaetano Rossiello",
      "Nhan Pham",
      "Michael Glass",
      "Junkyu Lee",
      "Dharmashankar Subramanian"
    ],
    "abstract": "We introduce a framework for generating Chain-of-Thought (CoT) rationales to\nenhance text-to-SQL model fine-tuning. These rationales consist of intermediate\nSQL statements and explanations, serving as incremental steps toward\nconstructing the final SQL query. The process begins with manually annotating a\nsmall set of examples, which are then used to prompt a large language model in\nan iterative, dynamic few-shot knowledge distillation procedure from a teacher\nmodel. A rationalization model is subsequently trained on the validated\ndecomposed queries, enabling extensive synthetic CoT annotations for\ntext-to-SQL datasets. To evaluate the approach, we fine-tune small language\nmodels with and without these rationales on the BIRD dataset. Results indicate\nthat step-by-step query generation improves execution accuracy, especially for\nmoderately and highly complex queries, while also enhancing explainability.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.DB"
    ],
    "primary_category": "cs.CL",
    "comment": "Published at ICLR 2025 Workshop on Reasoning and Planning for LLMs",
    "pdf_url": "http://arxiv.org/pdf/2502.06759v4",
    "published_date": "2025-02-10 18:38:57 UTC",
    "updated_date": "2025-03-20 13:46:48 UTC"
  },
  {
    "arxiv_id": "2502.06751v1",
    "title": "What makes a good feedforward computational graph?",
    "authors": [
      "Alex Vitvitskyi",
      "João G. M. Araújo",
      "Marc Lackenby",
      "Petar Veličković"
    ],
    "abstract": "As implied by the plethora of literature on graph rewiring, the choice of\ncomputational graph employed by a neural network can make a significant impact\non its downstream performance. Certain effects related to the computational\ngraph, such as under-reaching and over-squashing, may even render the model\nincapable of learning certain functions. Most of these effects have only been\nthoroughly studied in the domain of undirected graphs; however, recent years\nhave seen a significant rise in interest in feedforward computational graphs:\ndirected graphs without any back edges. In this paper, we study the desirable\nproperties of a feedforward computational graph, discovering two important\ncomplementary measures: fidelity and mixing time, and evaluating a few popular\nchoices of graphs through the lens of these measures. Our study is backed by\nboth theoretical analyses of the metrics' asymptotic behaviour for various\ngraphs, as well as correlating these metrics to the performance of trained\nneural network models using the corresponding graphs.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.SI",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "Work in progress -- comments welcome. 16 pages, 7 figures",
    "pdf_url": "http://arxiv.org/pdf/2502.06751v1",
    "published_date": "2025-02-10 18:26:40 UTC",
    "updated_date": "2025-02-10 18:26:40 UTC"
  },
  {
    "arxiv_id": "2502.12161v1",
    "title": "Integrating Artificial Intelligence and Geophysical Insights for Earthquake Forecasting: A Cross-Disciplinary Review",
    "authors": [
      "Zhang Ying",
      "Wen Congcong",
      "Sornette Didier",
      "Zhan Chengxiang"
    ],
    "abstract": "Earthquake forecasting remains a significant scientific challenge, with\ncurrent methods falling short of achieving the performance necessary for\nmeaningful societal benefits. Traditional models, primarily based on past\nseismicity and geomechanical data, struggle to capture the complexity of\nseismic patterns and often overlook valuable non-seismic precursors such as\ngeophysical, geochemical, and atmospheric anomalies. The integration of such\ndiverse data sources into forecasting models, combined with advancements in AI\ntechnologies, offers a promising path forward. AI methods, particularly deep\nlearning, excel at processing complex, large-scale datasets, identifying subtle\npatterns, and handling multidimensional relationships, making them well-suited\nfor overcoming the limitations of conventional approaches.\n  This review highlights the importance of combining AI with geophysical\nknowledge to create robust, physics-informed forecasting models. It explores\ncurrent AI methods, input data types, loss functions, and practical\nconsiderations for model development, offering guidance to both geophysicists\nand AI researchers. While many AI-based studies oversimplify earthquake\nprediction, neglecting critical features such as data imbalance and\nspatio-temporal clustering, the integration of specialized geophysical insights\ninto AI models can address these shortcomings.\n  We emphasize the importance of interdisciplinary collaboration, urging\ngeophysicists to experiment with AI architectures thoughtfully and encouraging\nAI experts to deepen their understanding of seismology. By bridging these\ndisciplines, we can develop more accurate, reliable, and societally impactful\nearthquake forecasting tools.",
    "categories": [
      "physics.geo-ph",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "physics.geo-ph",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.12161v1",
    "published_date": "2025-02-10 18:26:05 UTC",
    "updated_date": "2025-02-10 18:26:05 UTC"
  },
  {
    "arxiv_id": "2502.06925v1",
    "title": "Occam's model: Selecting simpler representations for better transferability estimation",
    "authors": [
      "Prabhant Singh",
      "Sibylle Hess",
      "Joaquin Vanschoren"
    ],
    "abstract": "Fine-tuning models that have been pre-trained on large datasets has become a\ncornerstone of modern machine learning workflows. With the widespread\navailability of online model repositories, such as Hugging Face, it is now\neasier than ever to fine-tune pre-trained models for specific tasks. This\nraises a critical question: which pre-trained model is most suitable for a\ngiven task? This problem is called transferability estimation. In this work, we\nintroduce two novel and effective metrics for estimating the transferability of\npre-trained models. Our approach is grounded in viewing transferability as a\nmeasure of how easily a pre-trained model's representations can be trained to\nseparate target classes, providing a unique perspective on transferability\nestimation. We rigorously evaluate the proposed metrics against\nstate-of-the-art alternatives across diverse problem settings, demonstrating\ntheir robustness and practical utility. Additionally, we present theoretical\ninsights that explain our metrics' efficacy and adaptability to various\nscenarios. We experimentally show that our metrics increase Kendall's Tau by up\nto 32% compared to the state-of-the-art baselines.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.06925v1",
    "published_date": "2025-02-10 18:23:24 UTC",
    "updated_date": "2025-02-10 18:23:24 UTC"
  },
  {
    "arxiv_id": "2502.06742v1",
    "title": "Gradient Multi-Normalization for Stateless and Scalable LLM Training",
    "authors": [
      "Meyer Scetbon",
      "Chao Ma",
      "Wenbo Gong",
      "Edward Meeds"
    ],
    "abstract": "Training large language models (LLMs) typically relies on adaptive optimizers\nlike Adam (Kingma & Ba, 2015) which store additional state information to\naccelerate convergence but incur significant memory overhead. Recent efforts,\nsuch as SWAN (Ma et al., 2024) address this by eliminating the need for\noptimizer states while achieving performance comparable to Adam via a\nmulti-step preprocessing procedure applied to instantaneous gradients.\nMotivated by the success of SWAN, we introduce a novel framework for designing\nstateless optimizers that normalizes stochastic gradients according to multiple\nnorms. To achieve this, we propose a simple alternating scheme to enforce the\nnormalization of gradients w.r.t these norms. We show that our procedure can\nproduce, up to an arbitrary precision, a fixed-point of the problem, and that\nSWAN is a particular instance of our approach with carefully chosen norms,\nproviding a deeper understanding of its design. However, SWAN's computationally\nexpensive whitening/orthogonalization step limit its practicality for large\nLMs. Using our principled perspective, we develop of a more efficient,\nscalable, and practical stateless optimizer. Our algorithm relaxes the\nproperties of SWAN, significantly reducing its computational cost while\nretaining its memory efficiency, making it applicable to training large-scale\nmodels. Experiments on pre-training LLaMA models with up to 1 billion\nparameters demonstrate a 3X speedup over Adam with significantly reduced memory\nrequirements, outperforming other memory-efficient baselines.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.06742v1",
    "published_date": "2025-02-10 18:09:53 UTC",
    "updated_date": "2025-02-10 18:09:53 UTC"
  },
  {
    "arxiv_id": "2502.06736v1",
    "title": "Low-power Spike-based Wearable Analytics on RRAM Crossbars",
    "authors": [
      "Abhiroop Bhattacharjee",
      "Jinquan Shi",
      "Wei-Chen Chen",
      "Xinxin Wang",
      "Priyadarshini Panda"
    ],
    "abstract": "This work introduces a spike-based wearable analytics system utilizing\nSpiking Neural Networks (SNNs) deployed on an In-memory Computing engine based\non RRAM crossbars, which are known for their compactness and energy-efficiency.\nGiven the hardware constraints and noise characteristics of the underlying RRAM\ncrossbars, we propose online adaptation of pre-trained SNNs in real-time using\nDirect Feedback Alignment (DFA) against traditional backpropagation (BP).\nDirect Feedback Alignment (DFA) learning, that allows layer-parallel gradient\ncomputations, acts as a fast, energy & area-efficient method for online\nadaptation of SNNs on RRAM crossbars, unleashing better algorithmic performance\nagainst those adapted using BP. Through extensive simulations using our\nin-house hardware evaluation engine called DFA_Sim, we find that DFA achieves\nupto 64.1% lower energy consumption, 10.1% lower area overhead, and a 2.1x\nreduction in latency compared to BP, while delivering upto 7.55% higher\ninference accuracy on human activity recognition (HAR) tasks.",
    "categories": [
      "cs.ET",
      "cs.AI",
      "cs.AR"
    ],
    "primary_category": "cs.ET",
    "comment": "Accepted in 2025 IEEE International Symposium on Circuits and Systems\n  (ISCAS)",
    "pdf_url": "http://arxiv.org/pdf/2502.06736v1",
    "published_date": "2025-02-10 18:00:05 UTC",
    "updated_date": "2025-02-10 18:00:05 UTC"
  },
  {
    "arxiv_id": "2502.06733v1",
    "title": "Dynamic Loss-Based Sample Reweighting for Improved Large Language Model Pretraining",
    "authors": [
      "Daouda Sow",
      "Herbert Woisetschläger",
      "Saikiran Bulusu",
      "Shiqiang Wang",
      "Hans-Arno Jacobsen",
      "Yingbin Liang"
    ],
    "abstract": "Pretraining large language models (LLMs) on vast and heterogeneous datasets\nis crucial for achieving state-of-the-art performance across diverse downstream\ntasks. However, current training paradigms treat all samples equally,\noverlooking the importance or relevance of individual samples throughout the\ntraining process. Existing reweighting strategies, which primarily focus on\ngroup-level data importance, fail to leverage fine-grained instance-level\ninformation and do not adapt dynamically to individual sample importance as\ntraining progresses. In this paper, we introduce novel algorithms for dynamic,\ninstance-level data reweighting aimed at improving both the efficiency and\neffectiveness of LLM pretraining. Our methods adjust the weight of each\ntraining sample based on its loss value in an online fashion, allowing the\nmodel to dynamically focus on more informative or important samples at the\ncurrent training stage. In particular, our framework allows us to\nsystematically devise reweighting strategies deprioritizing redundant or\nuninformative data, which we find tend to work best. Furthermore, we develop a\nnew theoretical framework for analyzing the impact of loss-based reweighting on\nthe convergence of gradient-based optimization, providing the first formal\ncharacterization of how these strategies affect convergence bounds. We\nempirically validate our approach across a spectrum of tasks, from pretraining\n7B and 1.4B parameter LLMs to smaller-scale language models and linear\nregression problems, demonstrating that our loss-based reweighting approach can\nlead to faster convergence and significantly improved performance.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted for publication at ICLR 2025. Code base available:\n  https://github.com/sowmaster/Sample-Level-Loss-Reweighting-ICLR-2025",
    "pdf_url": "http://arxiv.org/pdf/2502.06733v1",
    "published_date": "2025-02-10 17:57:15 UTC",
    "updated_date": "2025-02-10 17:57:15 UTC"
  },
  {
    "arxiv_id": "2502.06728v2",
    "title": "FlexDeMo: Decoupled Momentum Optimization for Hybrid Sharded Data Parallel Training",
    "authors": [
      "Mogens Henrik From",
      "Jacob Nielsen",
      "Lukas Galke",
      "Peter Schneider-Kamp"
    ],
    "abstract": "Training large neural network models requires extensive computational\nresources, often distributed across several nodes and accelerators. Recent\nfindings suggest that it may be sufficient to only exchange the fast moving\ncomponents of the gradients, while accumulating momentum locally (Decoupled\nMomentum, or DeMo). However, when considering larger models that do not fit on\na single accelerator, the exchange of gradient information and the integration\nof DeMo needs to be reconsidered. Here, we propose employing a hybrid sharded\ndata parallel training strategy, FlexDeMo, whereby nodes fully shard model\nparameters locally between different accelerators, while inter-node\ncommunication bandwidth requirements are reduced by synchronizing only\nfast-moving components instead of the full gradients. This effectively combines\nprevious hybrid sharded strategies with the advantages of decoupled momentum.\nOur experimental results show that FlexDeMo is on par with hybrid sharded data\nparallel training employing AdamW and full gradient synchronization in terms of\nvalidation loss, demonstrating its viability. Furthermore, FlexDeMo achieves\nimproved training speed compared to full gradient synchronization across nodes.\nIn a bandwidth-constrained 2-node setup, FlexDeMo allows reaching desired\nlevels of validation loss faster than hybrid sharded data parallel training\nwith full gradient synchronization.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.06728v2",
    "published_date": "2025-02-10 17:55:59 UTC",
    "updated_date": "2025-03-18 16:00:57 UTC"
  },
  {
    "arxiv_id": "2502.06727v1",
    "title": "Application of Artificial Intelligence (AI) in Civil Engineering",
    "authors": [
      "Temitope Funmilayo Awolusi",
      "Bernard Chukwuemeka Finbarrs-Ezema",
      "Isaac Munachimdinamma Chukwudulue",
      "Marc Azab"
    ],
    "abstract": "Hard computing generally deals with precise data, which provides ideal\nsolutions to problems. However, in the civil engineering field, amongst other\ndisciplines, that is not always the case as real-world systems are continuously\nchanging. Here lies the need to explore soft computing methods and artificial\nintelligence to solve civil engineering shortcomings. The integration of\nadvanced computational models, including Artificial Neural Networks (ANNs),\nFuzzy Logic, Genetic Algorithms (GAs), and Probabilistic Reasoning, has\nrevolutionized the domain of civil engineering. These models have significantly\nadvanced diverse sub-fields by offering innovative solutions and improved\nanalysis capabilities. Sub-fields such as: slope stability analysis, bearing\ncapacity, water quality and treatment, transportation systems, air quality,\nstructural materials, etc. ANNs predict non-linearities and provide accurate\nestimates. Fuzzy logic uses an efficient decision-making process to provide a\nmore precise assessment of systems. Lastly, while GAs optimizes models (based\non evolutionary processes) for better outcomes, probabilistic reasoning lowers\ntheir statistical uncertainties.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "Kindly cite published version if given access",
    "pdf_url": "http://arxiv.org/pdf/2502.06727v1",
    "published_date": "2025-02-10 17:55:52 UTC",
    "updated_date": "2025-02-10 17:55:52 UTC"
  },
  {
    "arxiv_id": "2502.06924v4",
    "title": "XAMBA: Enabling Efficient State Space Models on Resource-Constrained Neural Processing Units",
    "authors": [
      "Arghadip Das",
      "Arnab Raha",
      "Shamik Kundu",
      "Soumendu Kumar Ghosh",
      "Deepak Mathaikutty",
      "Vijay Raghunathan"
    ],
    "abstract": "State-Space Models (SSMs) have emerged as efficient alternatives to\ntransformers for sequential data tasks, offering linear or near-linear\nscalability with sequence length, making them ideal for long-sequence\napplications in NLP, vision, and edge AI, including real-time transcription,\ntranslation, and contextual search. These applications require lightweight,\nhigh-performance models for deployment on resource-constrained devices like\nlaptops and PCs. Designing specialized accelerators for every emerging neural\nnetwork is costly and impractical; instead, optimizing models for existing NPUs\nin AI PCs provides a scalable solution. To this end, we propose XAMBA, the\nfirst framework to enable and optimize SSMs on commercial off-the-shelf (COTS)\nstate-of-the-art (SOTA) NPUs. XAMBA follows a three-step methodology: (1)\nenabling SSMs on NPUs, (2) optimizing performance to meet KPI requirements, and\n(3) trading accuracy for additional performance gains. After enabling SSMs on\nNPUs, XAMBA mitigates key bottlenecks using CumBA and ReduBA, replacing\nsequential CumSum and ReduceSum operations with matrix-based computations,\nsignificantly improving execution speed and memory efficiency. Additionally,\nActiBA enhances performance by approximating expensive activation functions\n(e.g., Swish, Softplus) using piecewise linear mappings, reducing latency with\nminimal accuracy loss. Evaluations on an Intel Core Ultra Series 2 AI PC show\nthat XAMBA achieves up to 4.8X speed-up over the baseline. Our implementation\nis available at https://github.com/arghadippurdue/XAMBA.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.06924v4",
    "published_date": "2025-02-10 17:33:30 UTC",
    "updated_date": "2025-03-31 03:26:29 UTC"
  },
  {
    "arxiv_id": "2502.06923v1",
    "title": "Do Attention Heads Compete or Cooperate during Counting?",
    "authors": [
      "Pál Zsámboki",
      "Ádám Fraknói",
      "Máté Gedeon",
      "András Kornai",
      "Zsolt Zombori"
    ],
    "abstract": "We present an in-depth mechanistic interpretability analysis of training\nsmall transformers on an elementary task, counting, which is a crucial\ndeductive step in many algorithms. In particular, we investigate the\ncollaboration/competition among the attention heads: we ask whether the\nattention heads behave as a pseudo-ensemble, all solving the same subtask, or\nthey perform different subtasks, meaning that they can only solve the original\ntask in conjunction. Our work presents evidence that on the semantics of the\ncounting task, attention heads behave as a pseudo-ensemble, but their outputs\nneed to be aggregated in a non-uniform manner in order to create an encoding\nthat conforms to the syntax. Our source code will be available upon\npublication.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "14 pages, 15 figures",
    "pdf_url": "http://arxiv.org/pdf/2502.06923v1",
    "published_date": "2025-02-10 17:21:39 UTC",
    "updated_date": "2025-02-10 17:21:39 UTC"
  },
  {
    "arxiv_id": "2502.06693v1",
    "title": "Recent Advances, Applications and Open Challenges in Machine Learning for Health: Reflections from Research Roundtables at ML4H 2024 Symposium",
    "authors": [
      "Amin Adibi",
      "Xu Cao",
      "Zongliang Ji",
      "Jivat Neet Kaur",
      "Winston Chen",
      "Elizabeth Healey",
      "Brighton Nuwagira",
      "Wenqian Ye",
      "Geoffrey Woollard",
      "Maxwell A Xu",
      "Hejie Cui",
      "Johnny Xi",
      "Trenton Chang",
      "Vasiliki Bikia",
      "Nicole Zhang",
      "Ayush Noori",
      "Yuan Xia",
      "Md. Belal Hossain",
      "Hanna A. Frank",
      "Alina Peluso",
      "Yuan Pu",
      "Shannon Zejiang Shen",
      "John Wu",
      "Adibvafa Fallahpour",
      "Sazan Mahbub",
      "Ross Duncan",
      "Yuwei Zhang",
      "Yurui Cao",
      "Zuheng Xu",
      "Michael Craig",
      "Rahul G. Krishnan",
      "Rahmatollah Beheshti",
      "James M. Rehg",
      "Mohammad Ehsanul Karim",
      "Megan Coffee",
      "Leo Anthony Celi",
      "Jason Alan Fries",
      "Mohsen Sadatsafavi",
      "Dennis Shung",
      "Shannon McWeeney",
      "Jessica Dafflon",
      "Sarah Jabbour"
    ],
    "abstract": "The fourth Machine Learning for Health (ML4H) symposium was held in person on\nDecember 15th and 16th, 2024, in the traditional, ancestral, and unceded\nterritories of the Musqueam, Squamish, and Tsleil-Waututh Nations in Vancouver,\nBritish Columbia, Canada. The symposium included research roundtable sessions\nto foster discussions between participants and senior researchers on timely and\nrelevant topics for the ML4H community. The organization of the research\nroundtables at the conference involved 13 senior and 27 junior chairs across 13\ntables. Each roundtable session included an invited senior chair (with\nsubstantial experience in the field), junior chairs (responsible for\nfacilitating the discussion), and attendees from diverse backgrounds with an\ninterest in the session's topic.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CY"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.06693v1",
    "published_date": "2025-02-10 17:17:09 UTC",
    "updated_date": "2025-02-10 17:17:09 UTC"
  },
  {
    "arxiv_id": "2502.06692v1",
    "title": "Multi-label Scandinavian Language Identification (SLIDE)",
    "authors": [
      "Mariia Fedorova",
      "Jonas Sebulon Frydenberg",
      "Victoria Handford",
      "Victoria Ovedie Chruickshank Langø",
      "Solveig Helene Willoch",
      "Marthe Løken Midtgaard",
      "Yves Scherrer",
      "Petter Mæhlum",
      "David Samuel"
    ],
    "abstract": "Identifying closely related languages at sentence level is difficult, in\nparticular because it is often impossible to assign a sentence to a single\nlanguage. In this paper, we focus on multi-label sentence-level Scandinavian\nlanguage identification (LID) for Danish, Norwegian Bokm\\r{a}l, Norwegian\nNynorsk, and Swedish. We present the Scandinavian Language Identification and\nEvaluation, SLIDE, a manually curated multi-label evaluation dataset and a\nsuite of LID models with varying speed-accuracy tradeoffs. We demonstrate that\nthe ability to identify multiple languages simultaneously is necessary for any\naccurate LID method, and present a novel approach to training such multi-label\nLID models.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.06692v1",
    "published_date": "2025-02-10 17:16:55 UTC",
    "updated_date": "2025-02-10 17:16:55 UTC"
  },
  {
    "arxiv_id": "2502.06922v1",
    "title": "Synthetic Audio Helps for Cognitive State Tasks",
    "authors": [
      "Adil Soubki",
      "John Murzaku",
      "Peter Zeng",
      "Owen Rambow"
    ],
    "abstract": "The NLP community has broadly focused on text-only approaches of cognitive\nstate tasks, but audio can provide vital missing cues through prosody. We posit\nthat text-to-speech models learn to track aspects of cognitive state in order\nto produce naturalistic audio, and that the signal audio models implicitly\nidentify is orthogonal to the information that language models exploit. We\npresent Synthetic Audio Data fine-tuning (SAD), a framework where we show that\n7 tasks related to cognitive state modeling benefit from multimodal training on\nboth text and zero-shot synthetic audio data from an off-the-shelf TTS system.\nWe show an improvement over the text-only modality when adding synthetic audio\ndata to text-only corpora. Furthermore, on tasks and corpora that do contain\ngold audio, we show our SAD framework achieves competitive performance with\ntext and synthetic audio compared to text and gold audio.",
    "categories": [
      "cs.SD",
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "primary_category": "cs.SD",
    "comment": "John Murzaku and Adil Soubki contributed equally to this work",
    "pdf_url": "http://arxiv.org/pdf/2502.06922v1",
    "published_date": "2025-02-10 17:16:24 UTC",
    "updated_date": "2025-02-10 17:16:24 UTC"
  },
  {
    "arxiv_id": "2502.06684v1",
    "title": "EquiTabPFN: A Target-Permutation Equivariant Prior Fitted Networks",
    "authors": [
      "Michael Arbel",
      "David Salinas",
      "Frank Hutter"
    ],
    "abstract": "Recent foundational models for tabular data, such as TabPFN, have\ndemonstrated remarkable effectiveness in adapting to new tasks through\nin-context learning. However, these models overlook a crucial equivariance\nproperty: the arbitrary ordering of target dimensions should not influence\nmodel predictions. In this study, we identify this oversight as a source of\nincompressible error, termed the equivariance gap, which introduces instability\nin predictions. To mitigate these issues, we propose a novel model designed to\npreserve equivariance across output dimensions. Our experimental results\nindicate that our proposed model not only addresses these pitfalls effectively\nbut also achieves competitive benchmark performance.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.06684v1",
    "published_date": "2025-02-10 17:11:20 UTC",
    "updated_date": "2025-02-10 17:11:20 UTC"
  },
  {
    "arxiv_id": "2502.06681v1",
    "title": "CHIRLA: Comprehensive High-resolution Identification and Re-identification for Large-scale Analysis",
    "authors": [
      "Bessie Dominguez-Dager",
      "Felix Escalona",
      "Francisco Gomez-Donoso",
      "Miguel Cazorla"
    ],
    "abstract": "Person re-identification (Re-ID) is a key challenge in computer vision,\nrequiring the matching of individuals across different cameras, locations, and\ntime periods. While most research focuses on short-term scenarios with minimal\nappearance changes, real-world applications demand robust Re-ID systems capable\nof handling long-term scenarios, where persons' appearances can change\nsignificantly due to variations in clothing and physical characteristics. In\nthis paper, we present CHIRLA, Comprehensive High-resolution Identification and\nRe-identification for Large-scale Analysis, a novel dataset specifically\ndesigned for long-term person Re-ID. CHIRLA consists of recordings from\nstrategically placed cameras over a seven-month period, capturing significant\nvariations in both temporal and appearance attributes, including controlled\nchanges in participants' clothing and physical features. The dataset includes\n22 individuals, four connected indoor environments, and seven cameras. We\ncollected more than five hours of video that we semi-automatically labeled to\ngenerate around one million bounding boxes with identity annotations. By\nintroducing this comprehensive benchmark, we aim to facilitate the development\nand evaluation of Re-ID algorithms that can reliably perform in challenging,\nlong-term real-world scenarios.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.06681v1",
    "published_date": "2025-02-10 17:07:43 UTC",
    "updated_date": "2025-02-10 17:07:43 UTC"
  },
  {
    "arxiv_id": "2502.06921v2",
    "title": "GraNNite: Enabling High-Performance Execution of Graph Neural Networks on Resource-Constrained Neural Processing Units",
    "authors": [
      "Arghadip Das",
      "Shamik Kundu",
      "Arnab Raha",
      "Soumendu Ghosh",
      "Deepak Mathaikutty",
      "Vijay Raghunathan"
    ],
    "abstract": "Graph Neural Networks (GNNs) are vital for learning from graph-structured\ndata, enabling applications in network analysis, recommendation systems, and\nspeech analytics. Deploying them on edge devices like client PCs and laptops\nenhances real-time processing, privacy, and cloud independence. GNNs aid\nRetrieval-Augmented Generation (RAG) for Large Language Models (LLMs) and\nenable event-based vision tasks. However, irregular memory access, sparsity,\nand dynamic structures cause high latency and energy overhead on\nresource-constrained devices. While modern edge processors integrate CPUs,\nGPUs, and NPUs, NPUs designed for data-parallel tasks struggle with irregular\nGNN computations. We introduce GraNNite, the first hardware-aware framework\noptimizing GNN execution on commercial-off-the-shelf (COTS) SOTA DNN\naccelerators via a structured three-step methodology: (1) enabling NPU\nexecution, (2) optimizing performance, and (3) trading accuracy for efficiency\ngains. Step 1 employs GraphSplit for workload distribution and StaGr for static\naggregation, while GrAd and NodePad handle dynamic graphs. Step 2 boosts\nperformance using EffOp for control-heavy tasks and GraSp for sparsity\nexploitation. Graph Convolution optimizations PreG, SymG, and CacheG reduce\nredundancy and memory transfers. Step 3 balances quality versus efficiency,\nwhere QuantGr applies INT8 quantization, and GrAx1, GrAx2, and GrAx3 accelerate\nattention, broadcast-add, and SAGE-max aggregation. On Intel Core Ultra AI PCs,\nGraNNite achieves 2.6X to 7.6X speedups over default NPU mappings and up to\n8.6X energy gains over CPUs and GPUs, delivering 10.8X and 6.7X higher\nperformance than CPUs and GPUs, respectively, across GNN models.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.AR"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.06921v2",
    "published_date": "2025-02-10 17:03:02 UTC",
    "updated_date": "2025-02-13 02:05:21 UTC"
  },
  {
    "arxiv_id": "2502.09645v1",
    "title": "From No to Know: Taxonomy, Challenges, and Opportunities for Negation Understanding in Multimodal Foundation Models",
    "authors": [
      "Mayank Vatsa",
      "Aparna Bharati",
      "Surbhi Mittal",
      "Richa Singh"
    ],
    "abstract": "Negation, a linguistic construct conveying absence, denial, or contradiction,\nposes significant challenges for multilingual multimodal foundation models.\nThese models excel in tasks like machine translation, text-guided generation,\nimage captioning, audio interactions, and video processing but often struggle\nto accurately interpret negation across diverse languages and cultural\ncontexts. In this perspective paper, we propose a comprehensive taxonomy of\nnegation constructs, illustrating how structural, semantic, and cultural\nfactors influence multimodal foundation models. We present open research\nquestions and highlight key challenges, emphasizing the importance of\naddressing these issues to achieve robust negation handling. Finally, we\nadvocate for specialized benchmarks, language-specific tokenization,\nfine-grained attention mechanisms, and advanced multimodal architectures. These\nstrategies can foster more adaptable and semantically precise multimodal\nfoundation models, better equipped to navigate and accurately interpret the\ncomplexities of negation in multilingual, multimodal environments.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.09645v1",
    "published_date": "2025-02-10 16:55:13 UTC",
    "updated_date": "2025-02-10 16:55:13 UTC"
  },
  {
    "arxiv_id": "2502.06669v1",
    "title": "Boosting Self-Efficacy and Performance of Large Language Models via Verbal Efficacy Stimulations",
    "authors": [
      "Rui Chen",
      "Tailai Peng",
      "Xinran Xie",
      "Dekun Lin",
      "Zhe Cui",
      "Zheng Chen"
    ],
    "abstract": "Significant improvements have been observed in the zero-shot capabilities of\nthe Large Language Models (LLMs). Due to their high sensitivity to input,\nresearch has increasingly focused on enhancing LLMs' performance via direct and\nsimple prompt engineering rather than intricate domain adaptation. Studies\nsuggest that LLMs exhibit emotional intelligence, and both positive and\nnegative emotions can potentially enhance task performances. However, prior\ninteraction prompts have predominantly concentrated on a single stimulus type,\nneglecting to compare different stimulus effects, examine the influence of\nvarying task difficulties, or explore underlying mechanisms. This paper,\ninspired by the positive correlation between self-efficacy and task performance\nwithin the social cognitive theory, introduces Verbal Efficacy Stimulations\n(VES). Our VES comprises three types of verbal prompts: encouraging,\nprovocative, and critical, addressing six aspects such as helpfulness and\ncompetence. And we further categorize task difficulty, aiming to extensively\ninvestigate how distinct VES influence the self-efficacy and task achievements\nof language models at varied levels of difficulty. The experimental results\nshow that the three types of VES improve the performance of LLMs on most tasks,\nand the most effective VES varies for different models. In extensive\nexperiments, we have obtained some findings consistent with psychological\ntheories, providing novel insights for future research.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "to be published in ICONIP 2024",
    "pdf_url": "http://arxiv.org/pdf/2502.06669v1",
    "published_date": "2025-02-10 16:54:03 UTC",
    "updated_date": "2025-02-10 16:54:03 UTC"
  },
  {
    "arxiv_id": "2502.06666v1",
    "title": "Automatic Evaluation of Healthcare LLMs Beyond Question-Answering",
    "authors": [
      "Anna Arias-Duart",
      "Pablo Agustin Martin-Torres",
      "Daniel Hinjos",
      "Pablo Bernabeu-Perez",
      "Lucia Urcelay Ganzabal",
      "Marta Gonzalez Mallo",
      "Ashwin Kumar Gururajan",
      "Enrique Lopez-Cuena",
      "Sergio Alvarez-Napagao",
      "Dario Garcia-Gasulla"
    ],
    "abstract": "Current Large Language Models (LLMs) benchmarks are often based on open-ended\nor close-ended QA evaluations, avoiding the requirement of human labor.\nClose-ended measurements evaluate the factuality of responses but lack\nexpressiveness. Open-ended capture the model's capacity to produce discourse\nresponses but are harder to assess for correctness. These two approaches are\ncommonly used, either independently or together, though their relationship\nremains poorly understood. This work is focused on the healthcare domain, where\nboth factuality and discourse matter greatly. It introduces a comprehensive,\nmulti-axis suite for healthcare LLM evaluation, exploring correlations between\nopen and close benchmarks and metrics. Findings include blind spots and\noverlaps in current methodologies. As an updated sanity check, we release a new\nmedical benchmark --CareQA-- with both open and closed variants. Finally, we\npropose a novel metric for open-ended evaluations -- Relaxed Perplexity -- to\nmitigate the identified limitations.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.06666v1",
    "published_date": "2025-02-10 16:52:39 UTC",
    "updated_date": "2025-02-10 16:52:39 UTC"
  },
  {
    "arxiv_id": "2502.06664v2",
    "title": "Evaluation of Deep Audio Representations for Hearables",
    "authors": [
      "Fabian Gröger",
      "Pascal Baumann",
      "Ludovic Amruthalingam",
      "Laurent Simon",
      "Ruksana Giurda",
      "Simone Lionetti"
    ],
    "abstract": "Effectively steering hearable devices requires understanding the acoustic\nenvironment around the user. In the computational analysis of sound scenes,\nfoundation models have emerged as the state of the art to produce\nhigh-performance, robust, multi-purpose audio representations. We introduce and\nrelease Deep Evaluation of Audio Representations (DEAR), the first dataset and\nbenchmark to evaluate the efficacy of foundation models in capturing essential\nacoustic properties for hearables. The dataset includes 1,158 audio tracks,\neach 30 seconds long, created by spatially mixing proprietary monologues with\ncommercial, high-quality recordings of everyday acoustic scenes. Our benchmark\nencompasses eight tasks that assess the general context, speech sources, and\ntechnical acoustic properties of the audio scenes. Through our evaluation of\nfour general-purpose audio representation models, we demonstrate that the BEATs\nmodel significantly surpasses its counterparts. This superiority underscores\nthe advantage of models trained on diverse audio collections, confirming their\napplicability to a wide array of auditory tasks, including encoding the\nenvironment properties necessary for hearable steering. The DEAR dataset and\nassociated code are available at https://dear-dataset.github.io.",
    "categories": [
      "cs.SD",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.SD",
    "comment": "Accepted at International Conference on Acoustics, Speech, and Signal\n  Processing (ICASSP 2025)",
    "pdf_url": "http://arxiv.org/pdf/2502.06664v2",
    "published_date": "2025-02-10 16:51:11 UTC",
    "updated_date": "2025-02-24 10:07:54 UTC"
  },
  {
    "arxiv_id": "2502.06656v3",
    "title": "A Frontier AI Risk Management Framework: Bridging the Gap Between Current AI Practices and Established Risk Management",
    "authors": [
      "Simeon Campos",
      "Henry Papadatos",
      "Fabien Roger",
      "Chloé Touzet",
      "Otter Quarks",
      "Malcolm Murray"
    ],
    "abstract": "The recent development of powerful AI systems has highlighted the need for\nrobust risk management frameworks in the AI industry. Although companies have\nbegun to implement safety frameworks, current approaches often lack the\nsystematic rigor found in other high-risk industries. This paper presents a\ncomprehensive risk management framework for the development of frontier AI that\nbridges this gap by integrating established risk management principles with\nemerging AI-specific practices. The framework consists of four key components:\n(1) risk identification (through literature review, open-ended red-teaming, and\nrisk modeling), (2) risk analysis and evaluation using quantitative metrics and\nclearly defined thresholds, (3) risk treatment through mitigation measures such\nas containment, deployment controls, and assurance processes, and (4) risk\ngovernance establishing clear organizational structures and accountability.\nDrawing from best practices in mature industries such as aviation or nuclear\npower, while accounting for AI's unique challenges, this framework provides AI\ndevelopers with actionable guidelines for implementing robust risk management.\nThe paper details how each component should be implemented throughout the\nlife-cycle of the AI system - from planning through deployment - and emphasizes\nthe importance and feasibility of conducting risk management work prior to the\nfinal training run to minimize the burden associated with it.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.06656v3",
    "published_date": "2025-02-10 16:47:00 UTC",
    "updated_date": "2025-02-19 16:05:47 UTC"
  },
  {
    "arxiv_id": "2502.06655v2",
    "title": "Unbiased Evaluation of Large Language Models from a Causal Perspective",
    "authors": [
      "Meilin Chen",
      "Jian Tian",
      "Liang Ma",
      "Di Xie",
      "Weijie Chen",
      "Jiang Zhu"
    ],
    "abstract": "Benchmark contamination has become a significant concern in the LLM\nevaluation community. Previous Agents-as-an-Evaluator address this issue by\ninvolving agents in the generation of questions. Despite their success, the\nbiases in Agents-as-an-Evaluator methods remain largely unexplored. In this\npaper, we present a theoretical formulation of evaluation bias, providing\nvaluable insights into designing unbiased evaluation protocols. Furthermore, we\nidentify two type of bias in Agents-as-an-Evaluator through carefully designed\nprobing tasks on a minimal Agents-as-an-Evaluator setup. To address these\nissues, we propose the Unbiased Evaluator, an evaluation protocol that delivers\na more comprehensive, unbiased, and interpretable assessment of LLMs.Extensive\nexperiments reveal significant room for improvement in current LLMs.\nAdditionally, we demonstrate that the Unbiased Evaluator not only offers strong\nevidence of benchmark contamination but also provides interpretable evaluation\nresults.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "Accepted by ICML 2025",
    "pdf_url": "http://arxiv.org/pdf/2502.06655v2",
    "published_date": "2025-02-10 16:45:18 UTC",
    "updated_date": "2025-05-12 14:34:05 UTC"
  },
  {
    "arxiv_id": "2502.06648v2",
    "title": "The 2021 Tokyo Olympics Multilingual News Article Dataset",
    "authors": [
      "Erik Novak",
      "Erik Calcina",
      "Dunja Mladenić",
      "Marko Grobelnik"
    ],
    "abstract": "In this paper, we introduce a dataset of multilingual news articles covering\nthe 2021 Tokyo Olympics. A total of 10,940 news articles were gathered from\n1,918 different publishers, covering 1,350 sub-events of the 2021 Olympics, and\npublished between July 1, 2021, and August 14, 2021. These articles are written\nin nine languages from different language families and in different scripts. To\ncreate the dataset, the raw news articles were first retrieved via a service\nthat collects and analyzes news articles. Then, the articles were grouped using\nan online clustering algorithm, with each group containing articles reporting\non the same sub-event. Finally, the groups were manually annotated and\nevaluated. The development of this dataset aims to provide a resource for\nevaluating the performance of multilingual news clustering algorithms, for\nwhich limited datasets are available. It can also be used to analyze the\ndynamics and events of the 2021 Tokyo Olympics from different perspectives. The\ndataset is available in CSV format and can be accessed from the CLARIN.SI\nrepository.",
    "categories": [
      "cs.IR",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.IR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.06648v2",
    "published_date": "2025-02-10 16:38:03 UTC",
    "updated_date": "2025-02-13 20:46:57 UTC"
  },
  {
    "arxiv_id": "2502.06635v2",
    "title": "Steel-LLM:From Scratch to Open Source -- A Personal Journey in Building a Chinese-Centric LLM",
    "authors": [
      "Qingshui Gu",
      "Shu Li",
      "Tianyu Zheng",
      "Zhaoxiang Zhang"
    ],
    "abstract": "Steel-LLM is a Chinese-centric language model developed from scratch with the\ngoal of creating a high-quality, open-source model despite limited\ncomputational resources. Launched in March 2024, the project aimed to train a\n1-billion-parameter model on a large-scale dataset, prioritizing transparency\nand the sharing of practical insights to assist others in the community. The\ntraining process primarily focused on Chinese data, with a small proportion of\nEnglish data included, addressing gaps in existing open-source LLMs by\nproviding a more detailed and practical account of the model-building journey.\nSteel-LLM has demonstrated competitive performance on benchmarks such as CEVAL\nand CMMLU, outperforming early models from larger institutions. This paper\nprovides a comprehensive summary of the project's key contributions, including\ndata collection, model design, training methodologies, and the challenges\nencountered along the way, offering a valuable resource for researchers and\npractitioners looking to develop their own LLMs. The model checkpoints and\ntraining script are available at https://github.com/zhanshijinwat/Steel-LLM.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.06635v2",
    "published_date": "2025-02-10 16:31:37 UTC",
    "updated_date": "2025-02-13 07:31:55 UTC"
  },
  {
    "arxiv_id": "2502.06634v1",
    "title": "Automatic Annotation Augmentation Boosts Translation between Molecules and Natural Language",
    "authors": [
      "Zhiqiang Zhong",
      "Simon Sataa-Yu Larsen",
      "Haoyu Guo",
      "Tao Tang",
      "Kuangyu Zhou",
      "Davide Mottin"
    ],
    "abstract": "Recent advancements in AI for biological research focus on integrating\nmolecular data with natural language to accelerate drug discovery. However, the\nscarcity of high-quality annotations limits progress in this area. This paper\nintroduces LA$^3$, a Language-based Automatic Annotation Augmentation framework\nthat leverages large language models to augment existing datasets, thereby\nimproving AI training. We demonstrate the effectiveness of LA$^3$ by creating\nan enhanced dataset, LaChEBI-20, where we systematically rewrite the\nannotations of molecules from an established dataset. These rewritten\nannotations preserve essential molecular information while providing more\nvaried sentence structures and vocabulary. Using LaChEBI-20, we train LaMolT5\nbased on a benchmark architecture to learn the mapping between molecular\nrepresentations and augmented annotations.\n  Experimental results on text-based *de novo* molecule generation and molecule\ncaptioning demonstrate that LaMolT5 outperforms state-of-the-art models.\nNotably, incorporating LA$^3$ leads to improvements of up to 301% over the\nbenchmark architecture. Furthermore, we validate the effectiveness of LA$^3$\nnotable applications in *image*, *text* and *graph* tasks, affirming its\nversatility and utility.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "q-bio.BM"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.06634v1",
    "published_date": "2025-02-10 16:29:21 UTC",
    "updated_date": "2025-02-10 16:29:21 UTC"
  },
  {
    "arxiv_id": "2502.06633v1",
    "title": "Combining Large Language Models with Static Analyzers for Code Review Generation",
    "authors": [
      "Imen Jaoua",
      "Oussama Ben Sghaier",
      "Houari Sahraoui"
    ],
    "abstract": "Code review is a crucial but often complex, subjective, and time-consuming\nactivity in software development. Over the past decades, significant efforts\nhave been made to automate this process. Early approaches focused on\nknowledge-based systems (KBS) that apply rule-based mechanisms to detect code\nissues, providing precise feedback but struggling with complex,\ncontext-dependent cases. More recent work has shifted toward fine-tuning\npre-trained language models for code review, enabling broader issue coverage\nbut often at the expense of precision. In this paper, we propose a hybrid\napproach that combines the strengths of KBS and learning-based systems (LBS) to\ngenerate high-quality, comprehensive code reviews. Our method integrates\nknowledge at three distinct stages of the language model pipeline: during data\npreparation (Data-Augmented Training, DAT), at inference (Retrieval-Augmented\nGeneration, RAG), and after inference (Naive Concatenation of Outputs, NCO). We\nempirically evaluate our combination strategies against standalone KBS and LBS\nfine-tuned on a real-world dataset. Our results show that these hybrid\nstrategies enhance the relevance, completeness, and overall quality of review\ncomments, effectively bridging the gap between rule-based tools and deep\nlearning models.",
    "categories": [
      "cs.SE",
      "cs.AI"
    ],
    "primary_category": "cs.SE",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.06633v1",
    "published_date": "2025-02-10 16:29:12 UTC",
    "updated_date": "2025-02-10 16:29:12 UTC"
  },
  {
    "arxiv_id": "2502.06632v1",
    "title": "Few-Shot Classification and Anatomical Localization of Tissues in SPECT Imaging",
    "authors": [
      "Mohammed Abdul Hafeez Khan",
      "Samuel Morries Boddepalli",
      "Siddhartha Bhattacharyya",
      "Debasis Mitra"
    ],
    "abstract": "Accurate classification and anatomical localization are essential for\neffective medical diagnostics and research, which may be efficiently performed\nusing deep learning techniques. However, availability of limited labeled data\nposes a significant challenge. To address this, we adapted Prototypical\nNetworks and the Propagation-Reconstruction Network (PRNet) for few-shot\nclassification and localization, respectively, in Single Photon Emission\nComputed Tomography (SPECT) images. For the proof of concept we used a\n2D-sliced image cropped around heart. The Prototypical Network, with a\npre-trained ResNet-18 backbone, classified ventricles, myocardium, and liver\ntissues with 96.67% training and 93.33% validation accuracy. PRNet, adapted for\n2D imaging with an encoder-decoder architecture and skip connections, achieved\na training loss of 1.395, accurately reconstructing patches and capturing\nspatial relationships. These results highlight the potential of Prototypical\nNetworks for tissue classification with limited labeled data and PRNet for\nanatomical landmark localization, paving the way for improved performance in\ndeep learning frameworks.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "2 pages, 2 figures",
    "pdf_url": "http://arxiv.org/pdf/2502.06632v1",
    "published_date": "2025-02-10 16:28:35 UTC",
    "updated_date": "2025-02-10 16:28:35 UTC"
  },
  {
    "arxiv_id": "2502.06631v1",
    "title": "Conformal Predictions for Human Action Recognition with Vision-Language Models",
    "authors": [
      "Bary Tim",
      "Fuchs Clément",
      "Macq Benoît"
    ],
    "abstract": "Human-In-The-Loop (HITL) frameworks are integral to many real-world computer\nvision systems, enabling human operators to make informed decisions with AI\nassistance. Conformal Predictions (CP), which provide label sets with rigorous\nguarantees on ground truth inclusion probabilities, have recently gained\ntraction as a valuable tool in HITL settings. One key application area is video\nsurveillance, closely associated with Human Action Recognition (HAR). This\nstudy explores the application of CP on top of state-of-the-art HAR methods\nthat utilize extensively pre-trained Vision-Language Models (VLMs). Our\nfindings reveal that CP can significantly reduce the average number of\ncandidate classes without modifying the underlying VLM. However, these\nreductions often result in distributions with long tails. To address this, we\nintroduce a method based on tuning the temperature parameter of the VLMs to\nminimize these tails without requiring additional calibration data. Our code is\nmade available on GitHub at the address https://github.com/tbary/CP4VLM.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "6 pages, 7 figures",
    "pdf_url": "http://arxiv.org/pdf/2502.06631v1",
    "published_date": "2025-02-10 16:27:20 UTC",
    "updated_date": "2025-02-10 16:27:20 UTC"
  },
  {
    "arxiv_id": "2502.06920v1",
    "title": "Direct Estimation of Pediatric Heart Rate Variability from BOLD-fMRI: A Machine Learning Approach Using Dynamic Connectivity",
    "authors": [
      "Abdoljalil Addeh",
      "Karen Ardila",
      "Rebecca J Williams",
      "G. Bruce Pike",
      "M. Ethan MacDonald"
    ],
    "abstract": "In many pediatric fMRI studies, cardiac signals are often missing or of poor\nquality. A tool to extract Heart Rate Variation (HRV) waveforms directly from\nfMRI data, without the need for peripheral recording devices, would be highly\nbeneficial. We developed a machine learning framework to accurately reconstruct\nHRV for pediatric applications. A hybrid model combining one-dimensional\nConvolutional Neural Networks (1D-CNN) and Gated Recurrent Units (GRU) analyzed\nBOLD signals from 628 ROIs, integrating past and future data. The model\nachieved an 8% improvement in HRV accuracy, as evidenced by enhanced\nperformance metrics. This approach eliminates the need for peripheral\nphotoplethysmography devices, reduces costs, and simplifies procedures in\npediatric fMRI. Additionally, it improves the robustness of pediatric fMRI\nstudies, which are more sensitive to physiological and developmental variations\nthan those in adults.",
    "categories": [
      "eess.IV",
      "cs.AI",
      "cs.LG",
      "68T07 (Primary), 92C55, 62P10 (Secondary)"
    ],
    "primary_category": "eess.IV",
    "comment": "5 pages, 5 figures, ISMSMR 2025",
    "pdf_url": "http://arxiv.org/pdf/2502.06920v1",
    "published_date": "2025-02-10 16:24:18 UTC",
    "updated_date": "2025-02-10 16:24:18 UTC"
  },
  {
    "arxiv_id": "2502.06608v3",
    "title": "TripoSG: High-Fidelity 3D Shape Synthesis using Large-Scale Rectified Flow Models",
    "authors": [
      "Yangguang Li",
      "Zi-Xin Zou",
      "Zexiang Liu",
      "Dehu Wang",
      "Yuan Liang",
      "Zhipeng Yu",
      "Xingchao Liu",
      "Yuan-Chen Guo",
      "Ding Liang",
      "Wanli Ouyang",
      "Yan-Pei Cao"
    ],
    "abstract": "Recent advancements in diffusion techniques have propelled image and video\ngeneration to unprecedented levels of quality, significantly accelerating the\ndeployment and application of generative AI. However, 3D shape generation\ntechnology has so far lagged behind, constrained by limitations in 3D data\nscale, complexity of 3D data processing, and insufficient exploration of\nadvanced techniques in the 3D domain. Current approaches to 3D shape generation\nface substantial challenges in terms of output quality, generalization\ncapability, and alignment with input conditions. We present TripoSG, a new\nstreamlined shape diffusion paradigm capable of generating high-fidelity 3D\nmeshes with precise correspondence to input images. Specifically, we propose:\n1) A large-scale rectified flow transformer for 3D shape generation, achieving\nstate-of-the-art fidelity through training on extensive, high-quality data. 2)\nA hybrid supervised training strategy combining SDF, normal, and eikonal losses\nfor 3D VAE, achieving high-quality 3D reconstruction performance. 3) A data\nprocessing pipeline to generate 2 million high-quality 3D samples, highlighting\nthe crucial rules for data quality and quantity in training 3D generative\nmodels. Through comprehensive experiments, we have validated the effectiveness\nof each component in our new framework. The seamless integration of these parts\nhas enabled TripoSG to achieve state-of-the-art performance in 3D shape\ngeneration. The resulting 3D shapes exhibit enhanced detail due to\nhigh-resolution capabilities and demonstrate exceptional fidelity to input\nimages. Moreover, TripoSG demonstrates improved versatility in generating 3D\nmodels from diverse image styles and contents, showcasing strong generalization\ncapabilities. To foster progress and innovation in the field of 3D generation,\nwe will make our model publicly available.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.06608v3",
    "published_date": "2025-02-10 16:07:54 UTC",
    "updated_date": "2025-03-27 17:25:50 UTC"
  },
  {
    "arxiv_id": "2502.06919v2",
    "title": "Select before Act: Spatially Decoupled Action Repetition for Continuous Control",
    "authors": [
      "Buqing Nie",
      "Yangqing Fu",
      "Yue Gao"
    ],
    "abstract": "Reinforcement Learning (RL) has achieved remarkable success in various\ncontinuous control tasks, such as robot manipulation and locomotion. Different\nto mainstream RL which makes decisions at individual steps, recent studies have\nincorporated action repetition into RL, achieving enhanced action persistence\nwith improved sample efficiency and superior performance. However, existing\nmethods treat all action dimensions as a whole during repetition, ignoring\nvariations among them. This constraint leads to inflexibility in decisions,\nwhich reduces policy agility with inferior effectiveness. In this work, we\npropose a novel repetition framework called SDAR, which implements Spatially\nDecoupled Action Repetition through performing closed-loop act-or-repeat\nselection for each action dimension individually. SDAR achieves more flexible\nrepetition strategies, leading to an improved balance between action\npersistence and diversity. Compared to existing repetition frameworks, SDAR is\nmore sample efficient with higher policy performance and reduced action\nfluctuation. Experiments are conducted on various continuous control scenarios,\ndemonstrating the effectiveness of spatially decoupled repetition design\nproposed in this work.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.RO"
    ],
    "primary_category": "cs.LG",
    "comment": "ICLR 2025",
    "pdf_url": "http://arxiv.org/pdf/2502.06919v2",
    "published_date": "2025-02-10 16:07:28 UTC",
    "updated_date": "2025-03-06 05:25:18 UTC"
  },
  {
    "arxiv_id": "2503.04760v1",
    "title": "Agentic AI and the Cyber Arms Race",
    "authors": [
      "Sean Oesch",
      "Jack Hutchins",
      "Phillipe Austria",
      "Amul Chaulagain"
    ],
    "abstract": "Agentic AI is shifting the cybersecurity landscape as attackers and defenders\nleverage AI agents to augment humans and automate common tasks. In this\narticle, we examine the implications for cyber warfare and global politics as\nAgentic AI becomes more powerful and enables the broad proliferation of\ncapabilities only available to the most well resourced actors today.",
    "categories": [
      "cs.CY",
      "cs.AI"
    ],
    "primary_category": "cs.CY",
    "comment": "4 pages, 1 figure, due to be published in Computer Magazine",
    "pdf_url": "http://arxiv.org/pdf/2503.04760v1",
    "published_date": "2025-02-10 16:06:29 UTC",
    "updated_date": "2025-02-10 16:06:29 UTC"
  },
  {
    "arxiv_id": "2502.06607v3",
    "title": "Illegal Waste Detection in Remote Sensing Images: A Case Study",
    "authors": [
      "Federico Gibellini",
      "Piero Fraternali",
      "Giacomo Boracchi",
      "Luca Morandini",
      "Thomas Martinoli",
      "Andrea Diecidue",
      "Simona Malegori"
    ],
    "abstract": "Environmental crime is the third largest criminal activity worldwide, with\nsignificant revenues coming from illegal management of solid waste. Thanks to\nthe increasing availability and the decreasing cost of Very High Resolution\nRemote Sensing (VHR RS) images, the fight against environmental crime can\nnowadays rely on modern image-analysis tools to support photo-interpretation\nfor scanning vast territories in search of illegal waste disposal sites. This\npaper illustrates a semi-automatic waste detection pipeline, developed in\ncollaboration with a regional environmental protection agency, for detecting\ncandidate illegal dumping sites in VHR RS images. To optimize the effectiveness\nof the waste detector, extensive experiments evaluate such design choices as\nthe network architecture, the ground resolution and geographic span of the\ninput images, as well as the pretraining procedures. The best model attains\nremarkable performance, achieving 92.02% F1-Score and 94.56% Accuracy. A\ngeneralization study assesses the performance variation when the detector\nprocesses images from a territory substantially different from the one used\nduring training, incurring only a moderate performance loss, i.e., 6.5%\ndecrease in the F1-Score. Finally, an exercise in which photo interpreters\ncompare the territory scanning effort with and without the support of the waste\ndetector assesses the concrete benefit of using a computer-aided image analysis\ntool in a professional environment protection agency. Results show that a\nreduction up to 30% of the time spent for waste site detection can be attained.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.06607v3",
    "published_date": "2025-02-10 16:04:54 UTC",
    "updated_date": "2025-05-15 08:22:44 UTC"
  },
  {
    "arxiv_id": "2502.06601v1",
    "title": "Amortized In-Context Bayesian Posterior Estimation",
    "authors": [
      "Sarthak Mittal",
      "Niels Leif Bracher",
      "Guillaume Lajoie",
      "Priyank Jaini",
      "Marcus Brubaker"
    ],
    "abstract": "Bayesian inference provides a natural way of incorporating prior beliefs and\nassigning a probability measure to the space of hypotheses. Current solutions\nrely on iterative routines like Markov Chain Monte Carlo (MCMC) sampling and\nVariational Inference (VI), which need to be re-run whenever new observations\nare available. Amortization, through conditional estimation, is a viable\nstrategy to alleviate such difficulties and has been the guiding principle\nbehind simulation-based inference, neural processes and in-context methods\nusing pre-trained models. In this work, we conduct a thorough comparative\nanalysis of amortized in-context Bayesian posterior estimation methods from the\nlens of different optimization objectives and architectural choices. Such\nmethods train an amortized estimator to perform posterior parameter inference\nby conditioning on a set of data examples passed as context to a sequence model\nsuch as a transformer. In contrast to language models, we leverage permutation\ninvariant architectures as the true posterior is invariant to the ordering of\ncontext examples. Our empirical study includes generalization to\nout-of-distribution tasks, cases where the assumed underlying model is\nmisspecified, and transfer from simulated to real problems. Subsequently, it\nhighlights the superiority of the reverse KL estimator for predictive problems,\nespecially when combined with the transformer architecture and normalizing\nflows.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.06601v1",
    "published_date": "2025-02-10 16:00:48 UTC",
    "updated_date": "2025-02-10 16:00:48 UTC"
  },
  {
    "arxiv_id": "2502.06600v2",
    "title": "Evaluation of Multilingual Image Captioning: How far can we get with CLIP models?",
    "authors": [
      "Gonçalo Gomes",
      "Chrysoula Zerva",
      "Bruno Martins"
    ],
    "abstract": "The evaluation of image captions, looking at both linguistic fluency and\nsemantic correspondence to visual contents, has witnessed a significant effort.\nStill, despite advancements such as the CLIPScore metric, multilingual\ncaptioning evaluation has remained relatively unexplored. This work presents\nseveral strategies, and extensive experiments, related to evaluating CLIPScore\nvariants in multilingual settings. To address the lack of multilingual test\ndata, we consider two different strategies: (1) using quality aware\nmachine-translated datasets with human judgements, and (2) re-purposing\nmultilingual datasets that target semantic inference and reasoning. Our results\nhighlight the potential of finetuned multilingual models to generalize across\nlanguages and to handle complex linguistic challenges. Tests with\nmachine-translated data show that multilingual CLIPScore models can maintain a\nhigh correlation with human judgements across different languages, and\nadditional tests with natively multilingual and multicultural data further\nattest to the high-quality assessments.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted in Findings of NAACL 2025",
    "pdf_url": "http://arxiv.org/pdf/2502.06600v2",
    "published_date": "2025-02-10 16:00:00 UTC",
    "updated_date": "2025-02-17 15:22:32 UTC"
  },
  {
    "arxiv_id": "2502.06589v1",
    "title": "Hephaestus: Improving Fundamental Agent Capabilities of Large Language Models through Continual Pre-Training",
    "authors": [
      "Yuchen Zhuang",
      "Jingfeng Yang",
      "Haoming Jiang",
      "Xin Liu",
      "Kewei Cheng",
      "Sanket Lokegaonkar",
      "Yifan Gao",
      "Qing Ping",
      "Tianyi Liu",
      "Binxuan Huang",
      "Zheng Li",
      "Zhengyang Wang",
      "Pei Chen",
      "Ruijie Wang",
      "Rongzhi Zhang",
      "Nasser Zalmout",
      "Priyanka Nigam",
      "Bing Yin",
      "Chao Zhang"
    ],
    "abstract": "Due to the scarcity of agent-oriented pre-training data, LLM-based autonomous\nagents typically rely on complex prompting or extensive fine-tuning, which\noften fails to introduce new capabilities while preserving strong\ngeneralizability. We introduce Hephaestus-Forge, the first large-scale\npre-training corpus designed to enhance the fundamental capabilities of LLM\nagents in API function calling, intrinsic reasoning and planning, and adapting\nto environmental feedback. Hephaestus-Forge comprises 103B agent-specific data\nencompassing 76,537 APIs, including both tool documentation to introduce\nknowledge of API functions and function calling trajectories to strengthen\nintrinsic reasoning. To explore effective training protocols, we investigate\nscaling laws to identify the optimal recipe in data mixing ratios. By continual\npre-training on Hephaestus-Forge, Hephaestus outperforms small- to medium-scale\nopen-source LLMs and rivals commercial LLMs on three agent benchmarks,\ndemonstrating the effectiveness of our pre-training corpus in enhancing\nfundamental agentic capabilities and generalization of LLMs to new tasks or\nenvironments.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted to NAACL 2025 main conference",
    "pdf_url": "http://arxiv.org/pdf/2502.06589v1",
    "published_date": "2025-02-10 15:54:34 UTC",
    "updated_date": "2025-02-10 15:54:34 UTC"
  },
  {
    "arxiv_id": "2502.06577v1",
    "title": "The Minimal Search Space for Conditional Causal Bandits",
    "authors": [
      "Francisco N. F. Q. Simoes",
      "Itai Feigenbaum",
      "Mehdi Dastani",
      "Thijs van Ommen"
    ],
    "abstract": "Causal knowledge can be used to support decision-making problems. This has\nbeen recognized in the causal bandits literature, where a causal (multi-armed)\nbandit is characterized by a causal graphical model and a target variable. The\narms are then interventions on the causal model, and rewards are samples of the\ntarget variable. Causal bandits were originally studied with a focus on hard\ninterventions. We focus instead on cases where the arms are conditional\ninterventions, which more accurately model many real-world decision-making\nproblems by allowing the value of the intervened variable to be chosen based on\nthe observed values of other variables. This paper presents a graphical\ncharacterization of the minimal set of nodes guaranteed to contain the optimal\nconditional intervention, which maximizes the expected reward. We then propose\nan efficient algorithm with a time complexity of $O(|V| + |E|)$ to identify\nthis minimal set of nodes. We prove that the graphical characterization and the\nproposed algorithm are correct. Finally, we empirically demonstrate that our\nalgorithm significantly prunes the search space and substantially accelerates\nconvergence rates when integrated into standard multi-armed bandit algorithms.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "Submitted to ICML2025",
    "pdf_url": "http://arxiv.org/pdf/2502.06577v1",
    "published_date": "2025-02-10 15:45:18 UTC",
    "updated_date": "2025-02-10 15:45:18 UTC"
  },
  {
    "arxiv_id": "2502.06575v1",
    "title": "Predictive Red Teaming: Breaking Policies Without Breaking Robots",
    "authors": [
      "Anirudha Majumdar",
      "Mohit Sharma",
      "Dmitry Kalashnikov",
      "Sumeet Singh",
      "Pierre Sermanet",
      "Vikas Sindhwani"
    ],
    "abstract": "Visuomotor policies trained via imitation learning are capable of performing\nchallenging manipulation tasks, but are often extremely brittle to lighting,\nvisual distractors, and object locations. These vulnerabilities can depend\nunpredictably on the specifics of training, and are challenging to expose\nwithout time-consuming and expensive hardware evaluations. We propose the\nproblem of predictive red teaming: discovering vulnerabilities of a policy with\nrespect to environmental factors, and predicting the corresponding performance\ndegradation without hardware evaluations in off-nominal scenarios. In order to\nachieve this, we develop RoboART: an automated red teaming (ART) pipeline that\n(1) modifies nominal observations using generative image editing to vary\ndifferent environmental factors, and (2) predicts performance under each\nvariation using a policy-specific anomaly detector executed on edited\nobservations. Experiments across 500+ hardware trials in twelve off-nominal\nconditions for visuomotor diffusion policies demonstrate that RoboART predicts\nperformance degradation with high accuracy (less than 0.19 average difference\nbetween predicted and real success rates). We also demonstrate how predictive\nred teaming enables targeted data collection: fine-tuning with data collected\nunder conditions predicted to be adverse boosts baseline performance by 2-7x.",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.LG",
      "cs.SY",
      "eess.SY"
    ],
    "primary_category": "cs.RO",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.06575v1",
    "published_date": "2025-02-10 15:44:34 UTC",
    "updated_date": "2025-02-10 15:44:34 UTC"
  },
  {
    "arxiv_id": "2502.06574v1",
    "title": "On the Impact of the Utility in Semivalue-based Data Valuation",
    "authors": [
      "Mélissa Tamine",
      "Benjamin Heymann",
      "Patrick Loiseau",
      "Maxime Vono"
    ],
    "abstract": "Semivalue-based data valuation in machine learning (ML) quantifies the\ncontribution of individual data points to a downstream ML task by leveraging\nprinciples from cooperative game theory and the notion of utility. While this\nframework has been used in practice for assessing data quality, our experiments\nreveal inconsistent valuation outcomes across different utilities, albeit all\nrelated to ML performance. Beyond raising concerns about the reliability of\ndata valuation, this inconsistency is challenging to interpret, as it stems\nfrom the complex interaction of the utility with data points and semivalue\nweights, which has barely been studied in prior work. In this paper, we take a\nfirst step toward clarifying the utility impact on semivalue-based data\nvaluation. Specifically, we provide geometric interpretations of this impact\nfor a broad family of classification utilities, which includes the accuracy and\nthe arithmetic mean. We introduce the notion of spatial signatures: given a\nsemivalue, data points can be embedded into a two-dimensional space, and\nutility functions map to the dual of this space. This geometric perspective\nseparates the influence of the dataset and semivalue from that of the utility,\nproviding a theoretical explanation for the experimentally observed sensitivity\nof valuation outcomes to the utility choice.",
    "categories": [
      "cs.AI",
      "cs.GT",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "34 pages, 21 figures",
    "pdf_url": "http://arxiv.org/pdf/2502.06574v1",
    "published_date": "2025-02-10 15:42:38 UTC",
    "updated_date": "2025-02-10 15:42:38 UTC"
  },
  {
    "arxiv_id": "2502.06572v2",
    "title": "LawGPT: Knowledge-Guided Data Generation and Its Application to Legal LLM",
    "authors": [
      "Zhi Zhou",
      "Kun-Yang Yu",
      "Shi-Yu Tian",
      "Xiao-Wen Yang",
      "Jiang-Xin Shi",
      "Pengxiao Song",
      "Yi-Xuan Jin",
      "Lan-Zhe Guo",
      "Yu-Feng Li"
    ],
    "abstract": "Large language models (LLMs), both proprietary and open-source, have\ndemonstrated remarkable capabilities across various natural language processing\ntasks. However, they face significant limitations in legal reasoning tasks.\nProprietary models introduce data privacy risks and high inference costs, while\nopen-source models underperform due to insufficient legal domain training data.\nTo address these limitations, we study data generation for legal reasoning to\nimprove the legal reasoning performance of open-source LLMs with the help of\nproprietary LLMs. This is challenging due to the lack of legal knowledge in\nproprietary LLMs and the difficulty in verifying the generated data. We propose\nKgDG, a knowledge-guided data generation framework for legal reasoning. Our\nframework enables leveraging legal knowledge to enhance generation diversity\nand introduces a refinement and verification process to ensure the quality of\ngenerated data. Moreover, we expand the generated dataset to further enhance\nthe LLM reasoning capabilities. Using KgDG, we create a synthetic legal\nreasoning dataset containing 50K high-quality examples. Our trained model\nLawGPT outperforms existing legal-specific LLMs and achieves performance\ncomparable to proprietary LLMs, demonstrating the effectiveness of KgDG and\nLawGPT. Our code and resources is publicly available at\nhttps://github.com/LAMDASZ-ML/Knowledge-Guide-Data-Generation .",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Preprint",
    "pdf_url": "http://arxiv.org/pdf/2502.06572v2",
    "published_date": "2025-02-10 15:40:35 UTC",
    "updated_date": "2025-02-13 07:24:46 UTC"
  },
  {
    "arxiv_id": "2502.06918v1",
    "title": "Leveraging GPT-4o Efficiency for Detecting Rework Anomaly in Business Processes",
    "authors": [
      "Mohammad Derakhshan",
      "Paolo Ceravolo",
      "Fatemeh Mohammadi"
    ],
    "abstract": "This paper investigates the effectiveness of GPT-4o-2024-08-06, one of the\nLarge Language Models (LLM) from OpenAI, in detecting business process\nanomalies, with a focus on rework anomalies. In our study, we developed a\nGPT-4o-based tool capable of transforming event logs into a structured format\nand identifying reworked activities within business event logs. The analysis\nwas performed on a synthetic dataset designed to contain rework anomalies but\nfree of loops. To evaluate the anomaly detection capabilities of GPT\n4o-2024-08-06, we used three prompting techniques: zero-shot, one-shot, and\nfew-shot. These techniques were tested on different anomaly distributions,\nnamely normal, uniform, and exponential, to identify the most effective\napproach for each case. The results demonstrate the strong performance of\nGPT-4o-2024-08-06. On our dataset, the model achieved 96.14% accuracy with\none-shot prompting for the normal distribution, 97.94% accuracy with few-shot\nprompting for the uniform distribution, and 74.21% accuracy with few-shot\nprompting for the exponential distribution. These results highlight the model's\npotential as a reliable tool for detecting rework anomalies in event logs and\nhow anomaly distribution and prompting strategy influence the model's\nperformance.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "14 pages, 5 images, 4 tables",
    "pdf_url": "http://arxiv.org/pdf/2502.06918v1",
    "published_date": "2025-02-10 15:34:37 UTC",
    "updated_date": "2025-02-10 15:34:37 UTC"
  },
  {
    "arxiv_id": "2502.06559v1",
    "title": "Can We Trust AI Benchmarks? An Interdisciplinary Review of Current Issues in AI Evaluation",
    "authors": [
      "Maria Eriksson",
      "Erasmo Purificato",
      "Arman Noroozian",
      "Joao Vinagre",
      "Guillaume Chaslot",
      "Emilia Gomez",
      "David Fernandez-Llorca"
    ],
    "abstract": "Quantitative Artificial Intelligence (AI) Benchmarks have emerged as\nfundamental tools for evaluating the performance, capability, and safety of AI\nmodels and systems. Currently, they shape the direction of AI development and\nare playing an increasingly prominent role in regulatory frameworks. As their\ninfluence grows, however, so too does concerns about how and with what effects\nthey evaluate highly sensitive topics such as capabilities, including\nhigh-impact capabilities, safety and systemic risks. This paper presents an\ninterdisciplinary meta-review of about 100 studies that discuss shortcomings in\nquantitative benchmarking practices, published in the last 10 years. It brings\ntogether many fine-grained issues in the design and application of benchmarks\n(such as biases in dataset creation, inadequate documentation, data\ncontamination, and failures to distinguish signal from noise) with broader\nsociotechnical issues (such as an over-focus on evaluating text-based AI models\naccording to one-time testing logic that fails to account for how AI models are\nincreasingly multimodal and interact with humans and other technical systems).\nOur review also highlights a series of systemic flaws in current benchmarking\npractices, such as misaligned incentives, construct validity issues, unknown\nunknowns, and problems with the gaming of benchmark results. Furthermore, it\nunderscores how benchmark practices are fundamentally shaped by cultural,\ncommercial and competitive dynamics that often prioritise state-of-the-art\nperformance at the expense of broader societal concerns. By providing an\noverview of risks associated with existing benchmarking procedures, we\nproblematise disproportionate trust placed in benchmarks and contribute to\nongoing efforts to improve the accountability and relevance of quantitative AI\nbenchmarks within the complexities of real-world scenarios.",
    "categories": [
      "cs.AI",
      "I.2.0; A.1"
    ],
    "primary_category": "cs.AI",
    "comment": "Submitted to ACM Conference on Fairness, Accountability, and\n  Transparency (FAccT) 2025",
    "pdf_url": "http://arxiv.org/pdf/2502.06559v1",
    "published_date": "2025-02-10 15:25:06 UTC",
    "updated_date": "2025-02-10 15:25:06 UTC"
  },
  {
    "arxiv_id": "2502.06917v1",
    "title": "Krum Federated Chain (KFC): Using blockchain to defend against adversarial attacks in Federated Learning",
    "authors": [
      "Mario García-Márquez",
      "Nuria Rodríguez-Barroso",
      "M. Victoria Luzón",
      "Francisco Herrera"
    ],
    "abstract": "Federated Learning presents a nascent approach to machine learning, enabling\ncollaborative model training across decentralized devices while safeguarding\ndata privacy. However, its distributed nature renders it susceptible to\nadversarial attacks. Integrating blockchain technology with Federated Learning\noffers a promising avenue to enhance security and integrity. In this paper, we\ntackle the potential of blockchain in defending Federated Learning against\nadversarial attacks. First, we test Proof of Federated Learning, a well known\nconsensus mechanism designed ad-hoc to federated contexts, as a defense\nmechanism demonstrating its efficacy against Byzantine and backdoor attacks\nwhen at least one miner remains uncompromised. Second, we propose Krum\nFederated Chain, a novel defense strategy combining Krum and Proof of Federated\nLearning, valid to defend against any configuration of Byzantine or backdoor\nattacks, even when all miners are compromised. Our experiments conducted on\nimage classification datasets validate the effectiveness of our proposed\napproaches.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "Submitted to Neural Networks",
    "pdf_url": "http://arxiv.org/pdf/2502.06917v1",
    "published_date": "2025-02-10 15:15:50 UTC",
    "updated_date": "2025-02-10 15:15:50 UTC"
  },
  {
    "arxiv_id": "2502.17462v1",
    "title": "The Case for Cleaner Biosignals: High-fidelity Neural Compressor Enables Transfer from Cleaner iEEG to Noisier EEG",
    "authors": [
      "Francesco Stefano Carzaniga",
      "Gary Tom Hoppeler",
      "Michael Hersche",
      "Kaspar Anton Schindler",
      "Abbas Rahimi"
    ],
    "abstract": "All data modalities are not created equal, even when the signal they measure\ncomes from the same source. In the case of the brain, two of the most important\ndata modalities are the scalp electroencephalogram (EEG), and the intracranial\nelectroencephalogram (iEEG). They are used by human experts, supported by deep\nlearning (DL) models, to accomplish a variety of tasks, such as seizure\ndetection and motor imagery classification. Although the differences between\nEEG and iEEG are well understood by human experts, the performance of DL models\nacross these two modalities remains under-explored. To help characterize the\nimportance of clean data on the performance of DL models, we propose\nBrainCodec, a high-fidelity EEG and iEEG neural compressor. We find that\ntraining BrainCodec on iEEG and then transferring to EEG yields higher\nreconstruction quality than training on EEG directly. In addition, we also find\nthat training BrainCodec on both EEG and iEEG improves fidelity when\nreconstructing EEG. Our work indicates that data sources with higher SNR, such\nas iEEG, provide better performance across the board also in the medical\ntime-series domain. BrainCodec also achieves up to a 64x compression on iEEG\nand EEG without a notable decrease in quality. BrainCodec markedly surpasses\ncurrent state-of-the-art compression models both in final compression ratio and\nin reconstruction fidelity. We also evaluate the fidelity of the compressed\nsignals objectively on a seizure detection and a motor imagery task performed\nby standard DL models. Here, we find that BrainCodec achieves a reconstruction\nfidelity high enough to ensure no performance degradation on the downstream\ntasks. Finally, we collect the subjective assessment of an expert neurologist,\nthat confirms the high reconstruction quality of BrainCodec in a realistic\nscenario. The code is available at\nhttps://github.com/IBM/eeg-ieeg-brain-compressor.",
    "categories": [
      "eess.SP",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "eess.SP",
    "comment": "Published at ICLR 2025, see\n  https://openreview.net/forum?id=b57IG6N20B. Code is available at\n  https://github.com/IBM/eeg-ieeg-brain-compressor",
    "pdf_url": "http://arxiv.org/pdf/2502.17462v1",
    "published_date": "2025-02-10 15:05:06 UTC",
    "updated_date": "2025-02-10 15:05:06 UTC"
  },
  {
    "arxiv_id": "2502.07825v1",
    "title": "Pre-Trained Video Generative Models as World Simulators",
    "authors": [
      "Haoran He",
      "Yang Zhang",
      "Liang Lin",
      "Zhongwen Xu",
      "Ling Pan"
    ],
    "abstract": "Video generative models pre-trained on large-scale internet datasets have\nachieved remarkable success, excelling at producing realistic synthetic videos.\nHowever, they often generate clips based on static prompts (e.g., text or\nimages), limiting their ability to model interactive and dynamic scenarios. In\nthis paper, we propose Dynamic World Simulation (DWS), a novel approach to\ntransform pre-trained video generative models into controllable world\nsimulators capable of executing specified action trajectories. To achieve\nprecise alignment between conditioned actions and generated visual changes, we\nintroduce a lightweight, universal action-conditioned module that seamlessly\nintegrates into any existing model. Instead of focusing on complex visual\ndetails, we demonstrate that consistent dynamic transition modeling is the key\nto building powerful world simulators. Building upon this insight, we further\nintroduce a motion-reinforced loss that enhances action controllability by\ncompelling the model to capture dynamic changes more effectively. Experiments\ndemonstrate that DWS can be versatilely applied to both diffusion and\nautoregressive transformer models, achieving significant improvements in\ngenerating action-controllable, dynamically consistent videos across games and\nrobotics domains. Moreover, to facilitate the applications of the learned world\nsimulator in downstream tasks such as model-based reinforcement learning, we\npropose prioritized imagination to improve sample efficiency, demonstrating\ncompetitive performance compared with state-of-the-art methods.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "20 pages",
    "pdf_url": "http://arxiv.org/pdf/2502.07825v1",
    "published_date": "2025-02-10 14:49:09 UTC",
    "updated_date": "2025-02-10 14:49:09 UTC"
  },
  {
    "arxiv_id": "2502.06523v1",
    "title": "Tighter Value-Function Approximations for POMDPs",
    "authors": [
      "Merlijn Krale",
      "Wietze Koops",
      "Sebastian Junges",
      "Thiago D. Simão",
      "Nils Jansen"
    ],
    "abstract": "Solving partially observable Markov decision processes (POMDPs) typically\nrequires reasoning about the values of exponentially many state beliefs.\nTowards practical performance, state-of-the-art solvers use value bounds to\nguide this reasoning. However, sound upper value bounds are often\ncomputationally expensive to compute, and there is a tradeoff between the\ntightness of such bounds and their computational cost. This paper introduces\nnew and provably tighter upper value bounds than the commonly used fast\ninformed bound. Our empirical evaluation shows that, despite their additional\ncomputational overhead, the new upper bounds accelerate state-of-the-art POMDP\nsolvers on a wide range of benchmarks.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "AAMAS 2025 submission",
    "pdf_url": "http://arxiv.org/pdf/2502.06523v1",
    "published_date": "2025-02-10 14:48:09 UTC",
    "updated_date": "2025-02-10 14:48:09 UTC"
  },
  {
    "arxiv_id": "2503.05703v1",
    "title": "What I cannot execute, I do not understand: Training and Evaluating LLMs on Program Execution Traces",
    "authors": [
      "Jordi Armengol-Estapé",
      "Quentin Carbonneaux",
      "Tianjun Zhang",
      "Aram H. Markosyan",
      "Volker Seeker",
      "Chris Cummins",
      "Melanie Kambadur",
      "Michael F. P. O'Boyle",
      "Sida Wang",
      "Gabriel Synnaeve",
      "Hugh James Leather"
    ],
    "abstract": "Code generation and understanding are critical capabilities for large\nlanguage models (LLMs). Thus, most LLMs are pretrained and fine-tuned on code\ndata. However, these datasets typically treat code as static strings and rarely\nexploit the dynamic information about their execution. Building upon previous\nwork on trace modeling, we study Execution Tuning (E.T.), a training procedure\nin which we explicitly model real-world program execution traces without\nrequiring manual test annotations. We train and evaluate models on different\nexecution trace granularities (line and instruction-level) and strategies on\nthe task of output prediction, obtaining around 80% accuracy on CruxEval and\nMBPP, and showing the advantages of dynamic scratchpads (i.e., self-contained\nintermediate computations updated by the model rather than accumulated as a\nhistory of past computations) on long executions (up to 14k steps). Finally, we\ndiscuss E.T.'s practical applications.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.PL"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.05703v1",
    "published_date": "2025-02-10 14:42:13 UTC",
    "updated_date": "2025-02-10 14:42:13 UTC"
  },
  {
    "arxiv_id": "2502.06516v1",
    "title": "Boost-and-Skip: A Simple Guidance-Free Diffusion for Minority Generation",
    "authors": [
      "Soobin Um",
      "Beomsu Kim",
      "Jong Chul Ye"
    ],
    "abstract": "Minority samples are underrepresented instances located in low-density\nregions of a data manifold, and are valuable in many generative AI\napplications, such as data augmentation, creative content generation, etc.\nUnfortunately, existing diffusion-based minority generators often rely on\ncomputationally expensive guidance dedicated for minority generation. To\naddress this, here we present a simple yet powerful guidance-free approach\ncalled Boost-and-Skip for generating minority samples using diffusion models.\nThe key advantage of our framework requires only two minimal changes to\nstandard generative processes: (i) variance-boosted initialization and (ii)\ntimestep skipping. We highlight that these seemingly-trivial modifications are\nsupported by solid theoretical and empirical evidence, thereby effectively\npromoting emergence of underrepresented minority features. Our comprehensive\nexperiments demonstrate that Boost-and-Skip greatly enhances the capability of\ngenerating minority samples, even rivaling guidance-based state-of-the-art\napproaches while requiring significantly fewer computations.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "29 pages, 11 figures",
    "pdf_url": "http://arxiv.org/pdf/2502.06516v1",
    "published_date": "2025-02-10 14:37:26 UTC",
    "updated_date": "2025-02-10 14:37:26 UTC"
  },
  {
    "arxiv_id": "2502.06494v1",
    "title": "GuideLLM: Exploring LLM-Guided Conversation with Applications in Autobiography Interviewing",
    "authors": [
      "Jinhao Duan",
      "Xinyu Zhao",
      "Zhuoxuan Zhang",
      "Eunhye Ko",
      "Lily Boddy",
      "Chenan Wang",
      "Tianhao Li",
      "Alexander Rasgon",
      "Junyuan Hong",
      "Min Kyung Lee",
      "Chenxi Yuan",
      "Qi Long",
      "Ying Ding",
      "Tianlong Chen",
      "Kaidi Xu"
    ],
    "abstract": "Although Large Language Models (LLMs) succeed in human-guided conversations\nsuch as instruction following and question answering, the potential of\nLLM-guided conversations-where LLMs direct the discourse and steer the\nconversation's objectives-remains under-explored. In this study, we first\ncharacterize LLM-guided conversation into three fundamental components: (i)\nGoal Navigation; (ii) Context Management; (iii) Empathetic Engagement, and\npropose GuideLLM as an installation. We then implement an interviewing\nenvironment for the evaluation of LLM-guided conversation. Specifically,\nvarious topics are involved in this environment for comprehensive interviewing\nevaluation, resulting in around 1.4k turns of utterances, 184k tokens, and over\n200 events mentioned during the interviewing for each chatbot evaluation. We\ncompare GuideLLM with 6 state-of-the-art LLMs such as GPT-4o and\nLlama-3-70b-Instruct, from the perspective of interviewing quality, and\nautobiography generation quality. For automatic evaluation, we derive user\nproxies from multiple autobiographies and employ LLM-as-a-judge to score LLM\nbehaviors. We further conduct a human-involved experiment by employing 45 human\nparticipants to chat with GuideLLM and baselines. We then collect human\nfeedback, preferences, and ratings regarding the qualities of conversation and\nautobiography. Experimental results indicate that GuideLLM significantly\noutperforms baseline LLMs in automatic evaluation and achieves consistent\nleading performances in human ratings.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "31 pages; the first three authors contributed equally",
    "pdf_url": "http://arxiv.org/pdf/2502.06494v1",
    "published_date": "2025-02-10 14:11:32 UTC",
    "updated_date": "2025-02-10 14:11:32 UTC"
  },
  {
    "arxiv_id": "2502.06491v2",
    "title": "Model-Based Offline Reinforcement Learning with Reliability-Guaranteed Sequence Modeling",
    "authors": [
      "Shenghong He"
    ],
    "abstract": "Model-based offline reinforcement learning (MORL) aims to learn a policy by\nexploiting a dynamics model derived from an existing dataset. Applying\nconservative quantification to the dynamics model, most existing works on MORL\ngenerate trajectories that approximate the real data distribution to facilitate\npolicy learning by using current information (e.g., the state and action at\ntime step $t$). However, these works neglect the impact of historical\ninformation on environmental dynamics, leading to the generation of unreliable\ntrajectories that may not align with the real data distribution. In this paper,\nwe propose a new MORL algorithm \\textbf{R}eliability-guaranteed\n\\textbf{T}ransformer (RT), which can eliminate unreliable trajectories by\ncalculating the cumulative reliability of the generated trajectory (i.e., using\na weighted variational distance away from the real data). Moreover, by sampling\ncandidate actions with high rewards, RT can efficiently generate high-return\ntrajectories from the existing offline data. We theoretically prove the\nperformance guarantees of RT in policy learning, and empirically demonstrate\nits effectiveness against state-of-the-art model-based methods on several\nbenchmark tasks.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.06491v2",
    "published_date": "2025-02-10 14:08:55 UTC",
    "updated_date": "2025-05-03 07:43:43 UTC"
  },
  {
    "arxiv_id": "2502.06490v2",
    "title": "Recent Advances in Discrete Speech Tokens: A Review",
    "authors": [
      "Yiwei Guo",
      "Zhihan Li",
      "Hankun Wang",
      "Bohan Li",
      "Chongtian Shao",
      "Hanglei Zhang",
      "Chenpeng Du",
      "Xie Chen",
      "Shujie Liu",
      "Kai Yu"
    ],
    "abstract": "The rapid advancement of speech generation technologies in the era of large\nlanguage models (LLMs) has established discrete speech tokens as a foundational\nparadigm for speech representation. These tokens, characterized by their\ndiscrete, compact, and concise nature, are not only advantageous for efficient\ntransmission and storage, but also inherently compatible with the language\nmodeling framework, enabling seamless integration of speech into text-dominated\nLLM architectures. Current research categorizes discrete speech tokens into two\nprincipal classes: acoustic tokens and semantic tokens, each of which has\nevolved into a rich research domain characterized by unique design philosophies\nand methodological approaches. This survey systematically synthesizes the\nexisting taxonomy and recent innovations in discrete speech tokenization,\nconducts a critical examination of the strengths and limitations of each\nparadigm, and presents systematic experimental comparisons across token types.\nFurthermore, we identify persistent challenges in the field and propose\npotential research directions, aiming to offer actionable insights to inspire\nfuture advancements in the development and application of discrete speech\ntokens.",
    "categories": [
      "eess.AS",
      "cs.AI",
      "cs.MM",
      "cs.SD",
      "eess.SP"
    ],
    "primary_category": "eess.AS",
    "comment": "23 pages, 8 figures, 3 tables. Work in progress",
    "pdf_url": "http://arxiv.org/pdf/2502.06490v2",
    "published_date": "2025-02-10 14:08:25 UTC",
    "updated_date": "2025-02-16 08:11:09 UTC"
  },
  {
    "arxiv_id": "2502.06485v2",
    "title": "WyckoffDiff -- A Generative Diffusion Model for Crystal Symmetry",
    "authors": [
      "Filip Ekström Kelvinius",
      "Oskar B. Andersson",
      "Abhijith S. Parackal",
      "Dong Qian",
      "Rickard Armiento",
      "Fredrik Lindsten"
    ],
    "abstract": "Crystalline materials often exhibit a high level of symmetry. However, most\ngenerative models do not account for symmetry, but rather model each atom\nwithout any constraints on its position or element. We propose a generative\nmodel, Wyckoff Diffusion (WyckoffDiff), which generates symmetry-based\ndescriptions of crystals. This is enabled by considering a crystal structure\nrepresentation that encodes all symmetry, and we design a novel neural network\narchitecture which enables using this representation inside a discrete\ngenerative model framework. In addition to respecting symmetry by construction,\nthe discrete nature of our model enables fast generation. We additionally\npresent a new metric, Fr\\'echet Wrenformer Distance, which captures the\nsymmetry aspects of the materials generated, and we benchmark WyckoffDiff\nagainst recently proposed generative models for crystal generation. Code is\navailable online at https://github.com/httk/wyckoffdiff",
    "categories": [
      "cond-mat.mtrl-sci",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cond-mat.mtrl-sci",
    "comment": "Code is available online at https://github.com/httk/wyckoffdiff",
    "pdf_url": "http://arxiv.org/pdf/2502.06485v2",
    "published_date": "2025-02-10 14:04:23 UTC",
    "updated_date": "2025-04-30 06:08:47 UTC"
  },
  {
    "arxiv_id": "2502.06472v1",
    "title": "KARMA: Leveraging Multi-Agent LLMs for Automated Knowledge Graph Enrichment",
    "authors": [
      "Yuxing Lu",
      "Jinzhuo Wang"
    ],
    "abstract": "Maintaining comprehensive and up-to-date knowledge graphs (KGs) is critical\nfor modern AI systems, but manual curation struggles to scale with the rapid\ngrowth of scientific literature. This paper presents KARMA, a novel framework\nemploying multi-agent large language models (LLMs) to automate KG enrichment\nthrough structured analysis of unstructured text. Our approach employs nine\ncollaborative agents, spanning entity discovery, relation extraction, schema\nalignment, and conflict resolution that iteratively parse documents, verify\nextracted knowledge, and integrate it into existing graph structures while\nadhering to domain-specific schema. Experiments on 1,200 PubMed articles from\nthree different domains demonstrate the effectiveness of KARMA in knowledge\ngraph enrichment, with the identification of up to 38,230 new entities while\nachieving 83.1\\% LLM-verified correctness and reducing conflict edges by 18.6\\%\nthrough multi-layer assessments.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CE",
      "cs.DL"
    ],
    "primary_category": "cs.CL",
    "comment": "24 pages, 3 figures, 2 tables",
    "pdf_url": "http://arxiv.org/pdf/2502.06472v1",
    "published_date": "2025-02-10 13:51:36 UTC",
    "updated_date": "2025-02-10 13:51:36 UTC"
  },
  {
    "arxiv_id": "2502.06470v1",
    "title": "A Survey of Theory of Mind in Large Language Models: Evaluations, Representations, and Safety Risks",
    "authors": [
      "Hieu Minh \"Jord\" Nguyen"
    ],
    "abstract": "Theory of Mind (ToM), the ability to attribute mental states to others and\npredict their behaviour, is fundamental to social intelligence. In this paper,\nwe survey studies evaluating behavioural and representational ToM in Large\nLanguage Models (LLMs), identify important safety risks from advanced LLM ToM\ncapabilities, and suggest several research directions for effective evaluation\nand mitigation of these risks.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Advancing Artificial Intelligence through Theory of Mind Workshop,\n  AAAI 2025",
    "pdf_url": "http://arxiv.org/pdf/2502.06470v1",
    "published_date": "2025-02-10 13:50:25 UTC",
    "updated_date": "2025-02-10 13:50:25 UTC"
  },
  {
    "arxiv_id": "2502.17460v1",
    "title": "Finetuning and Quantization of EEG-Based Foundational BioSignal Models on ECG and PPG Data for Blood Pressure Estimation",
    "authors": [
      "Bálint Tóth",
      "Dominik Senti",
      "Thorir Mar Ingolfsson",
      "Jeffrey Zweidler",
      "Alexandre Elsig",
      "Luca Benini",
      "Yawei Li"
    ],
    "abstract": "Blood pressure (BP) is a key indicator of cardiovascular health. As\nhypertension remains a global cause of morbidity and mortality, accurate,\ncontinuous, and non-invasive BP monitoring is therefore of paramount\nimportance. Photoplethysmography (PPG) and electrocardiography (ECG) can\npotentially enable continuous BP monitoring, yet training accurate and robust\nmachine learning (ML) models remains challenging due to variability in data\nquality and patient-specific factors. Recently, multiple research groups\nexplored Electroencephalographic (EEG)--based foundation models and\ndemonstrated their exceptional ability to learn rich temporal resolution.\nConsidering the morphological similarities between different biosignals, the\nquestion arises of whether a model pre-trained on one modality can effectively\nbe exploited to improve the accuracy of a different signal type. In this work,\nwe take an initial step towards generalized biosignal foundation models by\ninvestigating whether model representations learned from abundant EEG data can\neffectively be transferred to ECG/PPG data solely with fine-tuning, without the\nneed for large-scale additional pre-training, for the BP estimation task.\nEvaluations on the MIMIC-III and VitalDB datasets demonstrate that our approach\nachieves near state-of-the-art accuracy for diastolic BP (mean absolute error\nof 1.57 mmHg) and surpasses by 1.5x the accuracy of prior works for systolic BP\n(mean absolute error 2.72 mmHg). Additionally, we perform dynamic INT8\nquantization, reducing the smallest model size by over 3.5x (from 13.73 MB down\nto 3.83 MB) while preserving performance, thereby enabling unobtrusive,\nreal-time BP monitoring on resource-constrained wearable devices.",
    "categories": [
      "eess.SP",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "eess.SP",
    "comment": "7 pages, 1 figure, 5 tables, preprint",
    "pdf_url": "http://arxiv.org/pdf/2502.17460v1",
    "published_date": "2025-02-10 13:33:12 UTC",
    "updated_date": "2025-02-10 13:33:12 UTC"
  },
  {
    "arxiv_id": "2502.06453v2",
    "title": "MATH-Perturb: Benchmarking LLMs' Math Reasoning Abilities against Hard Perturbations",
    "authors": [
      "Kaixuan Huang",
      "Jiacheng Guo",
      "Zihao Li",
      "Xiang Ji",
      "Jiawei Ge",
      "Wenzhe Li",
      "Yingqing Guo",
      "Tianle Cai",
      "Hui Yuan",
      "Runzhe Wang",
      "Yue Wu",
      "Ming Yin",
      "Shange Tang",
      "Yangsibo Huang",
      "Chi Jin",
      "Xinyun Chen",
      "Chiyuan Zhang",
      "Mengdi Wang"
    ],
    "abstract": "Large language models have demonstrated impressive performance on challenging\nmathematical reasoning tasks, which has triggered the discussion of whether the\nperformance is achieved by true reasoning capability or memorization. To\ninvestigate this question, prior work has constructed mathematical benchmarks\nwhen questions undergo simple perturbations -- modifications that still\npreserve the underlying reasoning patterns of the solutions. However, no work\nhas explored hard perturbations, which fundamentally change the nature of the\nproblem so that the original solution steps do not apply. To bridge the gap, we\nconstruct MATH-P-Simple and MATH-P-Hard via simple perturbation and hard\nperturbation, respectively. Each consists of 279 perturbed math problems\nderived from level-5 (hardest) problems in the MATH dataset (Hendrycksmath et.\nal., 2021). We observe significant performance drops on MATH-P-Hard across\nvarious models, including o1-mini (-16.49%) and gemini-2.0-flash-thinking\n(-12.9%). We also raise concerns about a novel form of memorization where\nmodels blindly apply learned problem-solving skills without assessing their\napplicability to modified contexts. This issue is amplified when using original\nproblems for in-context learning. We call for research efforts to address this\nchallenge, which is critical for developing more robust and reliable reasoning\nmodels.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.LG",
    "comment": "v2: fix bugs in Fig. 1",
    "pdf_url": "http://arxiv.org/pdf/2502.06453v2",
    "published_date": "2025-02-10 13:31:46 UTC",
    "updated_date": "2025-02-12 23:16:27 UTC"
  },
  {
    "arxiv_id": "2502.06440v1",
    "title": "SIGMA: Sheaf-Informed Geometric Multi-Agent Pathfinding",
    "authors": [
      "Shuhao Liao",
      "Weihang Xia",
      "Yuhong Cao",
      "Weiheng Dai",
      "Chengyang He",
      "Wenjun Wu",
      "Guillaume Sartoretti"
    ],
    "abstract": "The Multi-Agent Path Finding (MAPF) problem aims to determine the shortest\nand collision-free paths for multiple agents in a known, potentially\nobstacle-ridden environment. It is the core challenge for robotic deployments\nin large-scale logistics and transportation. Decentralized learning-based\napproaches have shown great potential for addressing the MAPF problems,\noffering more reactive and scalable solutions. However, existing learning-based\nMAPF methods usually rely on agents making decisions based on a limited field\nof view (FOV), resulting in short-sighted policies and inefficient cooperation\nin complex scenarios. There, a critical challenge is to achieve consensus on\npotential movements between agents based on limited observations and\ncommunications. To tackle this challenge, we introduce a new framework that\napplies sheaf theory to decentralized deep reinforcement learning, enabling\nagents to learn geometric cross-dependencies between each other through local\nconsensus and utilize them for tightly cooperative decision-making. In\nparticular, sheaf theory provides a mathematical proof of conditions for\nachieving global consensus through local observation. Inspired by this, we\nincorporate a neural network to approximately model the consensus in latent\nspace based on sheaf theory and train it through self-supervised learning.\nDuring the task, in addition to normal features for MAPF as in previous works,\neach agent distributedly reasons about a learned consensus feature, leading to\nefficient cooperation on pathfinding and collision avoidance. As a result, our\nproposed method demonstrates significant improvements over state-of-the-art\nlearning-based MAPF planners, especially in relatively large and complex\nscenarios, demonstrating its superiority over baselines in various simulations\nand real-world robot experiments.",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.MA"
    ],
    "primary_category": "cs.RO",
    "comment": "Accepted for presentation at the 2025 IEEE International Conference\n  on Robotics and Automation (ICRA)",
    "pdf_url": "http://arxiv.org/pdf/2502.06440v1",
    "published_date": "2025-02-10 13:17:34 UTC",
    "updated_date": "2025-02-10 13:17:34 UTC"
  },
  {
    "arxiv_id": "2502.06439v1",
    "title": "Testing software for non-discrimination: an updated and extended audit in the Italian car insurance domain",
    "authors": [
      "Marco Rondina",
      "Antonio Vetrò",
      "Riccardo Coppola",
      "Oumaima Regragrui",
      "Alessandro Fabris",
      "Gianmaria Silvello",
      "Gian Antonio Susto",
      "Juan Carlos De Martin"
    ],
    "abstract": "Context. As software systems become more integrated into society's\ninfrastructure, the responsibility of software professionals to ensure\ncompliance with various non-functional requirements increases. These\nrequirements include security, safety, privacy, and, increasingly,\nnon-discrimination.\n  Motivation. Fairness in pricing algorithms grants equitable access to basic\nservices without discriminating on the basis of protected attributes.\n  Method. We replicate a previous empirical study that used black box testing\nto audit pricing algorithms used by Italian car insurance companies, accessible\nthrough a popular online system. With respect to the previous study, we\nenlarged the number of tests and the number of demographic variables under\nanalysis.\n  Results. Our work confirms and extends previous findings, highlighting the\nproblematic permanence of discrimination across time: demographic variables\nsignificantly impact pricing to this day, with birthplace remaining the main\ndiscriminatory factor against individuals not born in Italian cities. We also\nfound that driver profiles can determine the number of quotes available to the\nuser, denying equal opportunities to all.\n  Conclusion. The study underscores the importance of testing for\nnon-discrimination in software systems that affect people's everyday lives.\nPerforming algorithmic audits over time makes it possible to evaluate the\nevolution of such algorithms. It also demonstrates the role that empirical\nsoftware engineering can play in making software systems more accountable.",
    "categories": [
      "cs.SE",
      "cs.AI",
      "cs.HC",
      "cs.LG"
    ],
    "primary_category": "cs.SE",
    "comment": "14 pages, 1 figure",
    "pdf_url": "http://arxiv.org/pdf/2502.06439v1",
    "published_date": "2025-02-10 13:16:01 UTC",
    "updated_date": "2025-02-10 13:16:01 UTC"
  },
  {
    "arxiv_id": "2502.06438v1",
    "title": "FEMBA: Efficient and Scalable EEG Analysis with a Bidirectional Mamba Foundation Model",
    "authors": [
      "Anna Tegon",
      "Thorir Mar Ingolfsson",
      "Xiaying Wang",
      "Luca Benini",
      "Yawei Li"
    ],
    "abstract": "Accurate and efficient electroencephalography (EEG) analysis is essential for\ndetecting seizures and artifacts in long-term monitoring, with applications\nspanning hospital diagnostics to wearable health devices. Robust EEG analytics\nhave the potential to greatly improve patient care. However, traditional deep\nlearning models, especially Transformer-based architectures, are hindered by\ntheir quadratic time and memory complexity, making them less suitable for\nresource-constrained environments. To address these challenges, we present\nFEMBA (Foundational EEG Mamba + Bidirectional Architecture), a novel\nself-supervised framework that establishes new efficiency benchmarks for EEG\nanalysis through bidirectional state-space modeling. Unlike Transformer-based\nmodels, which incur quadratic time and memory complexity, FEMBA scales linearly\nwith sequence length, enabling more scalable and efficient processing of\nextended EEG recordings. Trained on over 21,000 hours of unlabeled EEG and\nfine-tuned on three downstream tasks, FEMBA achieves competitive performance in\ncomparison with transformer models, with significantly lower computational\ncost. Specifically, it reaches 81.82% balanced accuracy (0.8921 AUROC) on TUAB\nand 0.949 AUROC on TUAR, while a tiny 7.8M-parameter variant demonstrates\nviability for resource-constrained devices. These results pave the way for\nscalable, general-purpose EEG analytics in both clinical and highlight FEMBA as\na promising candidate for wearable applications.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "7 pages, 3 figures, 5 tables, pre-print",
    "pdf_url": "http://arxiv.org/pdf/2502.06438v1",
    "published_date": "2025-02-10 13:15:52 UTC",
    "updated_date": "2025-02-10 13:15:52 UTC"
  },
  {
    "arxiv_id": "2502.06432v2",
    "title": "Prompt-SID: Learning Structural Representation Prompt via Latent Diffusion for Single-Image Denoising",
    "authors": [
      "Huaqiu Li",
      "Wang Zhang",
      "Xiaowan Hu",
      "Tao Jiang",
      "Zikang Chen",
      "Haoqian Wang"
    ],
    "abstract": "Many studies have concentrated on constructing supervised models utilizing\npaired datasets for image denoising, which proves to be expensive and\ntime-consuming. Current self-supervised and unsupervised approaches typically\nrely on blind-spot networks or sub-image pairs sampling, resulting in pixel\ninformation loss and destruction of detailed structural information, thereby\nsignificantly constraining the efficacy of such methods. In this paper, we\nintroduce Prompt-SID, a prompt-learning-based single image denoising framework\nthat emphasizes preserving of structural details. This approach is trained in a\nself-supervised manner using downsampled image pairs. It captures\noriginal-scale image information through structural encoding and integrates\nthis prompt into the denoiser. To achieve this, we propose a structural\nrepresentation generation model based on the latent diffusion process and\ndesign a structural attention module within the transformer-based denoiser\narchitecture to decode the prompt. Additionally, we introduce a scale replay\ntraining mechanism, which effectively mitigates the scale gap from images of\ndifferent resolutions. We conduct comprehensive experiments on synthetic,\nreal-world, and fluorescence imaging datasets, showcasing the remarkable\neffectiveness of Prompt-SID. Our code will be released at\nhttps://github.com/huaqlili/Prompt-SID.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.06432v2",
    "published_date": "2025-02-10 13:09:47 UTC",
    "updated_date": "2025-03-13 12:49:20 UTC"
  },
  {
    "arxiv_id": "2502.09644v1",
    "title": "From Argumentation to Deliberation: Perspectivized Stance Vectors for Fine-grained (Dis)agreement Analysis",
    "authors": [
      "Moritz Plenz",
      "Philipp Heinisch",
      "Janosch Gehring",
      "Philipp Cimiano",
      "Anette Frank"
    ],
    "abstract": "Debating over conflicting issues is a necessary first step towards resolving\nconflicts. However, intrinsic perspectives of an arguer are difficult to\novercome by persuasive argumentation skills. Proceeding from a debate to a\ndeliberative process, where we can identify actionable options for resolving a\nconflict requires a deeper analysis of arguments and the perspectives they are\ngrounded in - as it is only from there that one can derive mutually agreeable\nresolution steps. In this work we develop a framework for a deliberative\nanalysis of arguments in a computational argumentation setup. We conduct a\nfine-grained analysis of perspectivized stances expressed in the arguments of\ndifferent arguers or stakeholders on a given issue, aiming not only to identify\ntheir opposing views, but also shared perspectives arising from their\nattitudes, values or needs. We formalize this analysis in Perspectivized Stance\nVectors that characterize the individual perspectivized stances of all arguers\non a given issue. We construct these vectors by determining issue- and\nargument-specific concepts, and predict an arguer's stance relative to each of\nthem. The vectors allow us to measure a modulated (dis)agreement between\narguers, structured by perspectives, which allows us to identify actionable\npoints for conflict resolution, as a first step towards deliberation.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CY"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted at NAACL Findings 2025",
    "pdf_url": "http://arxiv.org/pdf/2502.09644v1",
    "published_date": "2025-02-10 13:08:46 UTC",
    "updated_date": "2025-02-10 13:08:46 UTC"
  },
  {
    "arxiv_id": "2502.06916v1",
    "title": "Hyper Compressed Fine-Tuning of Large Foundation Models with Quantum Inspired Adapters",
    "authors": [
      "Snehal Raj",
      "Brian Coyle"
    ],
    "abstract": "Fine-tuning pre-trained large foundation models for specific tasks has become\nincreasingly challenging due to the computational and storage demands\nassociated with full parameter updates. Parameter-Efficient Fine-Tuning (PEFT)\nmethods address this issue by updating only a small subset of model parameters\nusing adapter modules. In this work, we propose \\emph{Quantum-Inspired\nAdapters}, a PEFT approach inspired by Hamming-weight preserving quantum\ncircuits from quantum machine learning literature. These models can be both\nexpressive and parameter-efficient by operating in a combinatorially large\nspace while simultaneously preserving orthogonality in weight parameters. We\ntest our proposed adapters by adapting large language models and large vision\ntransformers on benchmark datasets. Our method can achieve 99.2\\% of the\nperformance of existing fine-tuning methods such LoRA with a 44x parameter\ncompression on language understanding datasets like GLUE and VTAB. Compared to\nexisting orthogonal fine-tuning methods such as OFT or BOFT, we achieve 98\\%\nrelative performance with 25x fewer parameters. This demonstrates competitive\nperformance paired with a significant reduction in trainable parameters.\nThrough ablation studies, we determine that combining multiple Hamming-weight\norders with orthogonality and matrix compounding are essential for performant\nfine-tuning. Our findings suggest that Quantum-Inspired Adapters offer a\npromising direction for efficient adaptation of language and vision models in\nresource-constrained environments.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "eess.SP",
      "quant-ph"
    ],
    "primary_category": "cs.LG",
    "comment": "16 pages, 9 figures, 6 tables",
    "pdf_url": "http://arxiv.org/pdf/2502.06916v1",
    "published_date": "2025-02-10 13:06:56 UTC",
    "updated_date": "2025-02-10 13:06:56 UTC"
  },
  {
    "arxiv_id": "2503.04758v2",
    "title": "Chat-GPT: An AI Based Educational Revolution",
    "authors": [
      "Sasa Maric",
      "Sonja Maric",
      "Lana Maric"
    ],
    "abstract": "The AI revolution is gathering momentum at an unprecedented rate. Over the\npast decade, we have witnessed a seemingly inevitable integration of AI in\nevery facet of our lives. Much has been written about the potential\nrevolutionary impact of AI in education. AI has the potential to completely\nrevolutionise the educational landscape as we could see entire courses and\ndegrees developed by programs such as ChatGPT. AI has the potential to develop\ncourses, set assignments, grade and provide feedback to students much faster\nthan a team of teachers. In addition, because of its dynamic nature, it has the\npotential to continuously improve its content. In certain fields such as\ncomputer science, where technology is continuously evolving, AI based\napplications can provide dynamically changing, relevant material to students.\nAI has the potential to replace entire degrees and may challenge the concept of\nhigher education institutions. We could also see entire new disciplines emerge\nas a consequence of AI. This paper examines the practical impact of ChatGPT and\nwhy it is believed that its implementation is a critical step towards a new era\nof education. We investigate the impact that ChatGPT will have on learning,\nproblem solving skills and cognitive ability of students. We examine the\npositives, negatives and many other aspects of AI and its applications\nthroughout this paper.",
    "categories": [
      "cs.CY",
      "cs.AI"
    ],
    "primary_category": "cs.CY",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.04758v2",
    "published_date": "2025-02-10 13:03:35 UTC",
    "updated_date": "2025-03-10 06:33:07 UTC"
  },
  {
    "arxiv_id": "2502.06425v2",
    "title": "Generating Privacy-Preserving Personalized Advice with Zero-Knowledge Proofs and LLMs",
    "authors": [
      "Hiroki Watanabe",
      "Motonobu Uchikoshi"
    ],
    "abstract": "Large language models (LLMs) are increasingly utilized in domains such as\nfinance, healthcare, and interpersonal relationships to provide advice tailored\nto user traits and contexts. However, this personalization often relies on\nsensitive data, raising critical privacy concerns and necessitating data\nminimization. To address these challenges, we propose a framework that\nintegrates zero-knowledge proof (ZKP) technology, specifically zkVM, with\nLLM-based chatbots. This integration enables privacy-preserving data sharing by\nverifying user traits without disclosing sensitive information. Our research\nintroduces both an architecture and a prompting strategy for this approach.\nThrough empirical evaluation, we clarify the current constraints and\nperformance limitations of both zkVM and the proposed prompting strategy,\nthereby demonstrating their practical feasibility in real-world scenarios.",
    "categories": [
      "cs.CR",
      "cs.AI"
    ],
    "primary_category": "cs.CR",
    "comment": "Accepted to The ACM Web Conference (WWW) 2025 Short Paper Track",
    "pdf_url": "http://arxiv.org/pdf/2502.06425v2",
    "published_date": "2025-02-10 13:02:00 UTC",
    "updated_date": "2025-04-24 00:37:48 UTC"
  },
  {
    "arxiv_id": "2502.06424v1",
    "title": "CS-SHAP: Extending SHAP to Cyclic-Spectral Domain for Better Interpretability of Intelligent Fault Diagnosis",
    "authors": [
      "Qian Chen",
      "Xingjian Dong",
      "Kui Hu",
      "Kangkang Chen",
      "Zhike Peng",
      "Guang Meng"
    ],
    "abstract": "Neural networks (NNs), with their powerful nonlinear mapping and end-to-end\ncapabilities, are widely applied in mechanical intelligent fault diagnosis\n(IFD). However, as typical black-box models, they pose challenges in\nunderstanding their decision basis and logic, limiting their deployment in\nhigh-reliability scenarios. Hence, various methods have been proposed to\nenhance the interpretability of IFD. Among these, post-hoc approaches can\nprovide explanations without changing model architecture, preserving its\nflexibility and scalability. However, existing post-hoc methods often suffer\nfrom limitations in explanation forms. They either require preprocessing that\ndisrupts the end-to-end nature or overlook fault mechanisms, leading to\nsuboptimal explanations. To address these issues, we derived the\ncyclic-spectral (CS) transform and proposed the CS-SHAP by extending Shapley\nadditive explanations (SHAP) to the CS domain. CS-SHAP can evaluate\ncontributions from both carrier and modulation frequencies, aligning more\nclosely with fault mechanisms and delivering clearer and more accurate\nexplanations. Three datasets are utilized to validate the superior\ninterpretability of CS-SHAP, ensuring its correctness, reproducibility, and\npractical performance. With open-source code and outstanding interpretability,\nCS-SHAP has the potential to be widely adopted and become the post-hoc\ninterpretability benchmark in IFD, even in other classification tasks. The code\nis available on https://github.com/ChenQian0618/CS-SHAP.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "21 pages, 21 figures",
    "pdf_url": "http://arxiv.org/pdf/2502.06424v1",
    "published_date": "2025-02-10 13:00:49 UTC",
    "updated_date": "2025-02-10 13:00:49 UTC"
  },
  {
    "arxiv_id": "2502.06415v2",
    "title": "Systematic Outliers in Large Language Models",
    "authors": [
      "Yongqi An",
      "Xu Zhao",
      "Tao Yu",
      "Ming Tang",
      "Jinqiao Wang"
    ],
    "abstract": "Outliers have been widely observed in Large Language Models (LLMs),\nsignificantly impacting model performance and posing challenges for model\ncompression. Understanding the functionality and formation mechanisms of these\noutliers is critically important. Existing works, however, largely focus on\nreducing the impact of outliers from an algorithmic perspective, lacking an\nin-depth investigation into their causes and roles. In this work, we provide a\ndetailed analysis of the formation process, underlying causes, and functions of\noutliers in LLMs. We define and categorize three types of outliers-activation\noutliers, weight outliers, and attention outliers-and analyze their\ndistributions across different dimensions, uncovering inherent connections\nbetween their occurrences and their ultimate influence on the attention\nmechanism. Based on these observations, we hypothesize and explore the\nmechanisms by which these outliers arise and function, demonstrating through\ntheoretical derivations and experiments that they emerge due to the\nself-attention mechanism's softmax operation. These outliers act as implicit\ncontext-aware scaling factors within the attention mechanism. As these outliers\nstem from systematic influences, we term them systematic outliers. Our study\nnot only enhances the understanding of Transformer-based LLMs but also shows\nthat structurally eliminating outliers can accelerate convergence and improve\nmodel compression. The code is avilable at\nhttps://github.com/an-yongqi/systematic-outliers.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted at ICLR 2025. Project Page:\n  https://github.com/an-yongqi/systematic-outliers",
    "pdf_url": "http://arxiv.org/pdf/2502.06415v2",
    "published_date": "2025-02-10 12:54:17 UTC",
    "updated_date": "2025-02-26 01:59:40 UTC"
  },
  {
    "arxiv_id": "2502.07823v1",
    "title": "Runtime Tunable Tsetlin Machines for Edge Inference on eFPGAs",
    "authors": [
      "Tousif Rahman",
      "Gang Mao",
      "Bob Pattison",
      "Sidharth Maheshwari",
      "Marcos Sartori",
      "Adrian Wheeldon",
      "Rishad Shafik",
      "Alex Yakovlev"
    ],
    "abstract": "Embedded Field-Programmable Gate Arrays (eFPGAs) allow for the design of\nhardware accelerators of edge Machine Learning (ML) applications at a lower\npower budget compared with traditional FPGA platforms. However, the limited\neFPGA logic and memory significantly constrain compute capabilities and model\nsize. As such, ML application deployment on eFPGAs is in direct contrast with\nthe most recent FPGA approaches developing architecture-specific\nimplementations and maximizing throughput over resource frugality. This paper\nfocuses on the opposite side of this trade-off: the proposed eFPGA accelerator\nfocuses on minimizing resource usage and allowing flexibility for on-field\nrecalibration over throughput. This allows for runtime changes in model size,\narchitecture, and input data dimensionality without offline resynthesis. This\nis made possible through the use of a bitwise compressed inference architecture\nof the Tsetlin Machine (TM) algorithm. TM compute does not require any\nmultiplication operations, being limited to only bitwise AND, OR, NOT,\nsummations and additions. Additionally, TM model compression allows the entire\nmodel to fit within the on-chip block RAM of the eFPGA. The paper uses this\naccelerator to propose a strategy for runtime model tuning in the field. The\nproposed approach uses 2.5x fewer Look-up-Tables (LUTs) and 3.38x fewer\nregisters than the current most resource-fugal design and achieves up to 129x\nenergy reduction compared with low-power microcontrollers running the same ML\napplication.",
    "categories": [
      "cs.AR",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.AR",
    "comment": "Accepted as a full paper by the 2025 EDGE AI FOUNDATION Austin",
    "pdf_url": "http://arxiv.org/pdf/2502.07823v1",
    "published_date": "2025-02-10 12:49:22 UTC",
    "updated_date": "2025-02-10 12:49:22 UTC"
  },
  {
    "arxiv_id": "2502.07822v1",
    "title": "PDM-SSD: Single-Stage Three-Dimensional Object Detector With Point Dilation",
    "authors": [
      "Ao Liang",
      "Haiyang Hua",
      "Jian Fang",
      "Wenyu Chen",
      "Huaici Zhao"
    ],
    "abstract": "Current Point-based detectors can only learn from the provided points, with\nlimited receptive fields and insufficient global learning capabilities for such\ntargets. In this paper, we present a novel Point Dilation Mechanism for\nsingle-stage 3D detection (PDM-SSD) that takes advantage of these two\nrepresentations. Specifically, we first use a PointNet-style 3D backbone for\nefficient feature encoding. Then, a neck with Point Dilation Mechanism (PDM) is\nused to expand the feature space, which involves two key steps: point dilation\nand feature filling. The former expands points to a certain size grid centered\naround the sampled points in Euclidean space. The latter fills the unoccupied\ngrid with feature for backpropagation using spherical harmonic coefficients and\nGaussian density function in terms of direction and scale. Next, we associate\nmultiple dilation centers and fuse coefficients to obtain sparse grid features\nthrough height compression. Finally, we design a hybrid detection head for\njoint learning, where on one hand, the scene heatmap is predicted to complement\nthe voting point set for improved detection accuracy, and on the other hand,\nthe target probability of detected boxes are calibrated through feature fusion.\nOn the challenging Karlsruhe Institute of Technology and Toyota Technological\nInstitute (KITTI) dataset, PDM-SSD achieves state-of-the-art results for\nmulti-class detection among single-modal methods with an inference speed of 68\nframes. We also demonstrate the advantages of PDM-SSD in detecting sparse and\nincomplete objects through numerous object-level instances. Additionally, PDM\ncan serve as an auxiliary network to establish a connection between sampling\npoints and object centers, thereby improving the accuracy of the model without\nsacrificing inference speed. Our code will be available at\nhttps://github.com/AlanLiangC/PDM-SSD.git.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.07822v1",
    "published_date": "2025-02-10 12:41:13 UTC",
    "updated_date": "2025-02-10 12:41:13 UTC"
  },
  {
    "arxiv_id": "2502.15749v2",
    "title": "TCProF: Time-Complexity Prediction SSL Framework",
    "authors": [
      "Joonghyuk Hahn",
      "Hyeseon Ahn",
      "Jungin Kim",
      "Soohan Lim",
      "Yo-Sub Han"
    ],
    "abstract": "Time complexity is a theoretic measure to determine the amount of time the\nalgorithm needs for its execution. In reality, developers write algorithms into\ncode snippets within limited resources, making the calculation of a code's time\ncomplexity a fundamental task. However, determining the precise time complexity\nof a code is theoretically undecidable. In response, recent advancements have\nleaned toward deploying datasets for code time complexity prediction and\ninitiating preliminary experiments for this challenge. We investigate the\nchallenge in low-resource scenarios where only a few labeled instances are\ngiven for training. Remarkably, we are the first to introduce TCProF: a\nTime-Complexity Prediction SSL Framework as an effective solution for code time\ncomplexity prediction in low-resource settings. TCProF significantly boosts\nperformance by integrating our augmentation, symbolic modules, and a\nco-training mechanism, achieving a more than 60% improvement over self-training\napproaches. We further provide an extensive comparative analysis between\nTCProF, ChatGPT, and Gemini-Pro, offering a detailed evaluation of our\napproach. Our code is at https://github.com/peer0/few-shot-tc.",
    "categories": [
      "cs.SE",
      "cs.AI",
      "68T50",
      "I.2.7"
    ],
    "primary_category": "cs.SE",
    "comment": "26 pages, 13 figures, This paper has been accepted to NAACL 2025",
    "pdf_url": "http://arxiv.org/pdf/2502.15749v2",
    "published_date": "2025-02-10 12:39:33 UTC",
    "updated_date": "2025-03-21 01:48:59 UTC"
  },
  {
    "arxiv_id": "2502.06395v1",
    "title": "AppVLM: A Lightweight Vision Language Model for Online App Control",
    "authors": [
      "Georgios Papoudakis",
      "Thomas Coste",
      "Zhihao Wu",
      "Jianye Hao",
      "Jun Wang",
      "Kun Shao"
    ],
    "abstract": "The utilisation of foundation models as smartphone assistants, termed app\nagents, is a critical research challenge. These agents aim to execute human\ninstructions on smartphones by interpreting textual instructions and performing\nactions via the device's interface. While promising, current approaches face\nsignificant limitations. Methods that use large proprietary models, such as\nGPT-4o, are computationally expensive, while those that use smaller fine-tuned\nmodels often lack adaptability to out-of-distribution tasks. In this work, we\nintroduce AppVLM, a lightweight Vision-Language Model (VLM). First, we\nfine-tune it offline on the AndroidControl dataset. Then, we refine its policy\nby collecting data from the AndroidWorld environment and performing further\ntraining iterations. Our results indicate that AppVLM achieves the highest\naction prediction accuracy in offline evaluation on the AndroidControl dataset,\ncompared to all evaluated baselines, and matches GPT-4o in online task\ncompletion success rate in the AndroidWorld environment, while being up to ten\ntimes faster. This makes AppVLM a practical and efficient solution for\nreal-world deployment.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.06395v1",
    "published_date": "2025-02-10 12:32:21 UTC",
    "updated_date": "2025-02-10 12:32:21 UTC"
  },
  {
    "arxiv_id": "2503.13463v1",
    "title": "Completeness of Datasets Documentation on ML/AI repositories: an Empirical Investigation",
    "authors": [
      "Marco Rondina",
      "Antonio Vetrò",
      "Juan Carlos De Martin"
    ],
    "abstract": "ML/AI is the field of computer science and computer engineering that arguably\nreceived the most attention and funding over the last decade. Data is the key\nelement of ML/AI, so it is becoming increasingly important to ensure that users\nare fully aware of the quality of the datasets that they use, and of the\nprocess generating them, so that possible negative impacts on downstream\neffects can be tracked, analysed, and, where possible, mitigated. One of the\ntools that can be useful in this perspective is dataset documentation. The aim\nof this work is to investigate the state of dataset documentation practices,\nmeasuring the completeness of the documentation of several popular datasets in\nML/AI repositories. We created a dataset documentation schema -- the\nDocumentation Test Sheet (DTS) -- that identifies the information that should\nalways be attached to a dataset (to ensure proper dataset choice and informed\nuse), according to relevant studies in the literature. We verified 100 popular\ndatasets from four different repositories with the DTS to investigate which\ninformation was present. Overall, we observed a lack of relevant documentation,\nespecially about the context of data collection and data processing,\nhighlighting a paucity of transparency.",
    "categories": [
      "cs.DL",
      "cs.AI",
      "cs.HC",
      "cs.LG"
    ],
    "primary_category": "cs.DL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.13463v1",
    "published_date": "2025-02-10 12:31:42 UTC",
    "updated_date": "2025-02-10 12:31:42 UTC"
  },
  {
    "arxiv_id": "2502.06379v1",
    "title": "Solving Linear-Gaussian Bayesian Inverse Problems with Decoupled Diffusion Sequential Monte Carlo",
    "authors": [
      "Filip Ekström Kelvinius",
      "Zheng Zhao",
      "Fredrik Lindsten"
    ],
    "abstract": "A recent line of research has exploited pre-trained generative diffusion\nmodels as priors for solving Bayesian inverse problems. We contribute to this\nresearch direction by designing a sequential Monte Carlo method for\nlinear-Gaussian inverse problems which builds on ``decoupled diffusion\", where\nthe generative process is designed such that larger updates to the sample are\npossible. The method is asymptotically exact and we demonstrate the\neffectiveness of our Decoupled Diffusion Sequential Monte Carlo (DDSMC)\nalgorithm on both synthetic data and image reconstruction tasks. Further, we\ndemonstrate how the approach can be extended to discrete data.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.06379v1",
    "published_date": "2025-02-10 11:59:02 UTC",
    "updated_date": "2025-02-10 11:59:02 UTC"
  },
  {
    "arxiv_id": "2502.07821v1",
    "title": "Amnesia as a Catalyst for Enhancing Black Box Pixel Attacks in Image Classification and Object Detection",
    "authors": [
      "Dongsu Song",
      "Daehwa Ko",
      "Jay Hoon Jung"
    ],
    "abstract": "It is well known that query-based attacks tend to have relatively higher\nsuccess rates in adversarial black-box attacks. While research on black-box\nattacks is actively being conducted, relatively few studies have focused on\npixel attacks that target only a limited number of pixels. In image\nclassification, query-based pixel attacks often rely on patches, which heavily\ndepend on randomness and neglect the fact that scattered pixels are more\nsuitable for adversarial attacks. Moreover, to the best of our knowledge,\nquery-based pixel attacks have not been explored in the field of object\ndetection. To address these issues, we propose a novel pixel-based black-box\nattack called Remember and Forget Pixel Attack using Reinforcement\nLearning(RFPAR), consisting of two main components: the Remember and Forget\nprocesses. RFPAR mitigates randomness and avoids patch dependency by leveraging\nrewards generated through a one-step RL algorithm to perturb pixels. RFPAR\neffectively creates perturbed images that minimize the confidence scores while\nadhering to limited pixel constraints. Furthermore, we advance our proposed\nattack beyond image classification to object detection, where RFPAR reduces the\nconfidence scores of detected objects to avoid detection. Experiments on the\nImageNet-1K dataset for classification show that RFPAR outperformed\nstate-of-the-art query-based pixel attacks. For object detection, using the\nMSCOCO dataset with YOLOv8 and DDQ, RFPAR demonstrates comparable mAP reduction\nto state-of-the-art query-based attack while requiring fewer query. Further\nexperiments on the Argoverse dataset using YOLOv8 confirm that RFPAR\neffectively removed objects on a larger scale dataset. Our code is available at\nhttps://github.com/KAU-QuantumAILab/RFPAR.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "Accepted as a poster at NeurIPS 2024",
    "pdf_url": "http://arxiv.org/pdf/2502.07821v1",
    "published_date": "2025-02-10 11:49:41 UTC",
    "updated_date": "2025-02-10 11:49:41 UTC"
  },
  {
    "arxiv_id": "2502.06374v2",
    "title": "Hyperparameters in Score-Based Membership Inference Attacks",
    "authors": [
      "Gauri Pradhan",
      "Joonas Jälkö",
      "Marlon Tobaben",
      "Antti Honkela"
    ],
    "abstract": "Membership Inference Attacks (MIAs) have emerged as a valuable framework for\nevaluating privacy leakage by machine learning models. Score-based MIAs are\ndistinguished, in particular, by their ability to exploit the confidence scores\nthat the model generates for particular inputs. Existing score-based MIAs\nimplicitly assume that the adversary has access to the target model's\nhyperparameters, which can be used to train the shadow models for the attack.\nIn this work, we demonstrate that the knowledge of target hyperparameters is\nnot a prerequisite for MIA in the transfer learning setting. Based on this, we\npropose a novel approach to select the hyperparameters for training the shadow\nmodels for MIA when the attacker has no prior knowledge about them by matching\nthe output distributions of target and shadow models. We demonstrate that using\nthe new approach yields hyperparameters that lead to an attack near\nindistinguishable in performance from an attack that uses target\nhyperparameters to train the shadow models. Furthermore, we study the empirical\nprivacy risk of unaccounted use of training data for hyperparameter\noptimization (HPO) in differentially private (DP) transfer learning. We find no\nstatistically significant evidence that performing HPO using training data\nwould increase vulnerability to MIA.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "This work has been accepted for publication in the 3rd IEEE\n  Conference on Secure and Trustworthy Machine Learning (SaTML'25). The final\n  version will be available on IEEE Xplore",
    "pdf_url": "http://arxiv.org/pdf/2502.06374v2",
    "published_date": "2025-02-10 11:44:46 UTC",
    "updated_date": "2025-02-27 05:44:37 UTC"
  },
  {
    "arxiv_id": "2502.06348v2",
    "title": "AiRacleX: Automated Detection of Price Oracle Manipulations via LLM-Driven Knowledge Mining and Prompt Generation",
    "authors": [
      "Bo Gao",
      "Yuan Wang",
      "Qingsong Wei",
      "Yong Liu",
      "Rick Siow Mong Goh",
      "David Lo"
    ],
    "abstract": "Decentralized finance (DeFi) applications depend on accurate price oracles to\nensure secure transactions, yet these oracles are highly vulnerable to\nmanipulation, enabling attackers to exploit smart contract vulnerabilities for\nunfair asset valuation and financial gain. Detecting such manipulations\ntraditionally relies on the manual effort of experienced experts, presenting\nsignificant challenges. In this paper, we propose a novel LLM-driven framework\nthat automates the detection of price oracle manipulations by leveraging the\ncomplementary strengths of different LLM models (LLMs). Our approach begins\nwith domain-specific knowledge extraction, where an LLM model synthesizes\nprecise insights about price oracle vulnerabilities from top-tier academic\npapers, eliminating the need for profound expertise from developers or\nauditors. This knowledge forms the foundation for a second LLM model to\ngenerate structured, context-aware chain of thought prompts, which guide a\nthird LLM model in accurately identifying manipulation patterns in smart\ncontracts. We validate the effectiveness of framework through experiments on 60\nknown vulnerabilities from 46 real-world DeFi attacks or projects spanning 2021\nto 2023. The best performing combination of LLMs (Haiku-Haiku-4o-mini)\nidentified by AiRacleX demonstrate a 2.58-times improvement in recall (0.667 vs\n0.259) compared to the state-of-the-art tool GPTScan, while maintaining\ncomparable precision. Furthermore, our framework demonstrates the feasibility\nof replacing commercial models with open-source alternatives, enhancing privacy\nand security for developers.",
    "categories": [
      "cs.CR",
      "cs.AI"
    ],
    "primary_category": "cs.CR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.06348v2",
    "published_date": "2025-02-10 10:58:09 UTC",
    "updated_date": "2025-02-11 03:40:13 UTC"
  },
  {
    "arxiv_id": "2502.06341v1",
    "title": "Facial Analysis Systems and Down Syndrome",
    "authors": [
      "Marco Rondina",
      "Fabiana Vinci",
      "Antonio Vetrò",
      "Juan Carlos De Martin"
    ],
    "abstract": "The ethical, social and legal issues surrounding facial analysis technologies\nhave been widely debated in recent years. Key critics have argued that these\ntechnologies can perpetuate bias and discrimination, particularly against\nmarginalized groups. We contribute to this field of research by reporting on\nthe limitations of facial analysis systems with the faces of people with Down\nsyndrome: this particularly vulnerable group has received very little attention\nin the literature so far. This study involved the creation of a specific\ndataset of face images. An experimental group with faces of people with Down\nsyndrome, and a control group with faces of people who are not affected by the\nsyndrome. Two commercial tools were tested on the dataset, along three tasks:\ngender recognition, age prediction and face labelling. The results show an\noverall lower accuracy of prediction in the experimental group, and other\nspecific patterns of performance differences: i) high error rates in gender\nrecognition in the category of males with Down syndrome; ii) adults with Down\nsyndrome were more often incorrectly labelled as children; iii) social\nstereotypes are propagated in both the control and experimental groups, with\nlabels related to aesthetics more often associated with women, and labels\nrelated to education level and skills more often associated with men. These\nresults, although limited in scope, shed new light on the biases that alter\nface classification when applied to faces of people with Down syndrome. They\nconfirm the structural limitation of the technology, which is inherently\ndependent on the datasets used to train the models.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.HC",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.06341v1",
    "published_date": "2025-02-10 10:43:55 UTC",
    "updated_date": "2025-02-10 10:43:55 UTC"
  },
  {
    "arxiv_id": "2502.06336v1",
    "title": "DefTransNet: A Transformer-based Method for Non-Rigid Point Cloud Registration in the Simulation of Soft Tissue Deformation",
    "authors": [
      "Sara Monji-Azad",
      "Marvin Kinz",
      "Siddharth Kothari",
      "Robin Khanna",
      "Amrei Carla Mihan",
      "David Maennel",
      "Claudia Scherl",
      "Juergen Hesser"
    ],
    "abstract": "Soft-tissue surgeries, such as tumor resections, are complicated by tissue\ndeformations that can obscure the accurate location and shape of tissues. By\nrepresenting tissue surfaces as point clouds and applying non-rigid point cloud\nregistration (PCR) methods, surgeons can better understand tissue deformations\nbefore, during, and after surgery. Existing non-rigid PCR methods, such as\nfeature-based approaches, struggle with robustness against challenges like\nnoise, outliers, partial data, and large deformations, making accurate point\ncorrespondence difficult. Although learning-based PCR methods, particularly\nTransformer-based approaches, have recently shown promise due to their\nattention mechanisms for capturing interactions, their robustness remains\nlimited in challenging scenarios. In this paper, we present DefTransNet, a\nnovel end-to-end Transformer-based architecture for non-rigid PCR. DefTransNet\nis designed to address the key challenges of deformable registration, including\nlarge deformations, outliers, noise, and partial data, by inputting source and\ntarget point clouds and outputting displacement vector fields. The proposed\nmethod incorporates a learnable transformation matrix to enhance robustness to\naffine transformations, integrates global and local geometric information, and\ncaptures long-range dependencies among points using Transformers. We validate\nour approach on four datasets: ModelNet, SynBench, 4DMatch, and DeformedTissue,\nusing both synthetic and real-world data to demonstrate the generalization of\nour proposed method. Experimental results demonstrate that DefTransNet\noutperforms current state-of-the-art registration networks across various\nchallenging conditions. Our code and data are publicly available.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.06336v1",
    "published_date": "2025-02-10 10:37:21 UTC",
    "updated_date": "2025-02-10 10:37:21 UTC"
  },
  {
    "arxiv_id": "2502.06327v1",
    "title": "Prompt-Driven Continual Graph Learning",
    "authors": [
      "Qi Wang",
      "Tianfei Zhou",
      "Ye Yuan",
      "Rui Mao"
    ],
    "abstract": "Continual Graph Learning (CGL), which aims to accommodate new tasks over\nevolving graph data without forgetting prior knowledge, is garnering\nsignificant research interest. Mainstream solutions adopt the memory\nreplay-based idea, ie, caching representative data from earlier tasks for\nretraining the graph model. However, this strategy struggles with scalability\nissues for constantly evolving graphs and raises concerns regarding data\nprivacy. Inspired by recent advancements in the prompt-based learning paradigm,\nthis paper introduces a novel prompt-driven continual graph learning\n(PROMPTCGL) framework, which learns a separate prompt for each incoming task\nand maintains the underlying graph neural network model fixed. In this way,\nPROMPTCGL naturally avoids catastrophic forgetting of knowledge from previous\ntasks. More specifically, we propose hierarchical prompting to instruct the\nmodel from both feature- and topology-level to fully address the variability of\ntask graphs in dynamic continual learning. Additionally, we develop a\npersonalized prompt generator to generate tailored prompts for each graph node\nwhile minimizing the number of prompts needed, leading to constant memory\nconsumption regardless of the graph scale. Extensive experiments on four\nbenchmarks show that PROMPTCGL achieves superior performance against existing\nCGL approaches while significantly reducing memory consumption. Our code is\navailable at https://github.com/QiWang98/PromptCGL.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "12 pages, 7figures",
    "pdf_url": "http://arxiv.org/pdf/2502.06327v1",
    "published_date": "2025-02-10 10:28:11 UTC",
    "updated_date": "2025-02-10 10:28:11 UTC"
  },
  {
    "arxiv_id": "2502.06324v1",
    "title": "UniDemoiré: Towards Universal Image Demoiréing with Data Generation and Synthesis",
    "authors": [
      "Zemin Yang",
      "Yujing Sun",
      "Xidong Peng",
      "Siu Ming Yiu",
      "Yuexin Ma"
    ],
    "abstract": "Image demoir\\'eing poses one of the most formidable challenges in image\nrestoration, primarily due to the unpredictable and anisotropic nature of\nmoir\\'e patterns. Limited by the quantity and diversity of training data,\ncurrent methods tend to overfit to a single moir\\'e domain, resulting in\nperformance degradation for new domains and restricting their robustness in\nreal-world applications. In this paper, we propose a universal image\ndemoir\\'eing solution, UniDemoir\\'e, which has superior generalization\ncapability. Notably, we propose innovative and effective data generation and\nsynthesis methods that can automatically provide vast high-quality moir\\'e\nimages to train a universal demoir\\'eing model. Our extensive experiments\ndemonstrate the cutting-edge performance and broad potential of our approach\nfor generalized image demoir\\'eing.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "Accepted by AAAI 2025",
    "pdf_url": "http://arxiv.org/pdf/2502.06324v1",
    "published_date": "2025-02-10 10:20:11 UTC",
    "updated_date": "2025-02-10 10:20:11 UTC"
  },
  {
    "arxiv_id": "2502.08664v1",
    "title": "Motion Forecasting for Autonomous Vehicles: A Survey",
    "authors": [
      "Jianxin Shi",
      "Jinhao Chen",
      "Yuandong Wang",
      "Li Sun",
      "Chunyang Liu",
      "Wei Xiong",
      "Tianyu Wo"
    ],
    "abstract": "In recent years, the field of autonomous driving has attracted increasingly\nsignificant public interest. Accurately forecasting the future behavior of\nvarious traffic participants is essential for the decision-making of Autonomous\nVehicles (AVs). In this paper, we focus on both scenario-based and\nperception-based motion forecasting for AVs. We propose a formal problem\nformulation for motion forecasting and summarize the main challenges\nconfronting this area of research. We also detail representative datasets and\nevaluation metrics pertinent to this field. Furthermore, this study classifies\nrecent research into two main categories: supervised learning and\nself-supervised learning, reflecting the evolving paradigms in both\nscenario-based and perception-based motion forecasting. In the context of\nsupervised learning, we thoroughly examine and analyze each key element of the\nmethodology. For self-supervised learning, we summarize commonly adopted\ntechniques. The paper concludes and discusses potential research directions,\naiming to propel progress in this vital area of AV technology.",
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "primary_category": "cs.RO",
    "comment": "31 pages, 7 figures",
    "pdf_url": "http://arxiv.org/pdf/2502.08664v1",
    "published_date": "2025-02-10 10:13:24 UTC",
    "updated_date": "2025-02-10 10:13:24 UTC"
  },
  {
    "arxiv_id": "2502.06314v2",
    "title": "From Pixels to Components: Eigenvector Masking for Visual Representation Learning",
    "authors": [
      "Alice Bizeul",
      "Thomas Sutter",
      "Alain Ryser",
      "Bernhard Schölkopf",
      "Julius von Kügelgen",
      "Julia E. Vogt"
    ],
    "abstract": "Predicting masked from visible parts of an image is a powerful\nself-supervised approach for visual representation learning. However, the\ncommon practice of masking random patches of pixels exhibits certain failure\nmodes, which can prevent learning meaningful high-level features, as required\nfor downstream tasks. We propose an alternative masking strategy that operates\non a suitable transformation of the data rather than on the raw pixels.\nSpecifically, we perform principal component analysis and then randomly mask a\nsubset of components, which accounts for a fixed ratio of the data variance.\nThe learning task then amounts to reconstructing the masked components from the\nvisible ones. Compared to local patches of pixels, the principal components of\nimages carry more global information. We thus posit that predicting masked from\nvisible components involves more high-level features, allowing our masking\nstrategy to extract more useful representations. This is corroborated by our\nempirical findings which demonstrate improved image classification performance\nfor component over pixel masking. Our method thus constitutes a simple and\nrobust data-driven alternative to traditional masked image modeling approaches.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.LG",
    "comment": "Preprint. Under review",
    "pdf_url": "http://arxiv.org/pdf/2502.06314v2",
    "published_date": "2025-02-10 10:06:46 UTC",
    "updated_date": "2025-02-11 16:04:15 UTC"
  },
  {
    "arxiv_id": "2502.06914v2",
    "title": "UniZyme: A Unified Protein Cleavage Site Predictor Enhanced with Enzyme Active-Site Knowledge",
    "authors": [
      "Chenao Li",
      "Shuo Yan",
      "Enyan Dai"
    ],
    "abstract": "Enzyme-catalyzed protein cleavage is essential for many biological functions.\nAccurate prediction of cleavage sites can facilitate various applications such\nas drug development, enzyme design, and a deeper understanding of biological\nmechanisms. However, most existing models are restricted to an individual\nenzyme, which neglects shared knowledge of enzymes and fails generalize to\nnovel enzymes. Thus, we introduce a unified protein cleavage site predictor\nnamed UniZyme, which can generalize across diverse enzymes. To enhance the\nenzyme encoding for the protein cleavage site prediction, UniZyme employs a\nnovel biochemically-informed model architecture along with active-site\nknowledge of proteolytic enzymes. Extensive experiments demonstrate that\nUniZyme achieves high accuracy in predicting cleavage sites across a range of\nproteolytic enzymes, including unseen enzymes. The code is available in\nhttps://anonymous.4open.science/r/UniZyme-4A67.",
    "categories": [
      "q-bio.QM",
      "cs.AI",
      "cs.LG",
      "92E10, 68T07, 68Q32, 92D15",
      "I.2.6; I.2.7; J.3"
    ],
    "primary_category": "q-bio.QM",
    "comment": "18 pages,8 figures",
    "pdf_url": "http://arxiv.org/pdf/2502.06914v2",
    "published_date": "2025-02-10 09:46:26 UTC",
    "updated_date": "2025-02-12 16:47:32 UTC"
  },
  {
    "arxiv_id": "2502.08663v1",
    "title": "Hallucination Detection: A Probabilistic Framework Using Embeddings Distance Analysis",
    "authors": [
      "Emanuele Ricco",
      "Lorenzo Cima",
      "Roberto Di Pietro"
    ],
    "abstract": "Hallucinations are one of the major issues affecting LLMs, hindering their\nwide adoption in production systems. While current research solutions for\ndetecting hallucinations are mainly based on heuristics, in this paper we\nintroduce a mathematically sound methodology to reason about hallucination, and\nleverage it to build a tool to detect hallucinations. To the best of our\nknowledge, we are the first to show that hallucinated content has structural\ndifferences with respect to correct content. To prove this result, we resort to\nthe Minkowski distances in the embedding space. Our findings demonstrate\nstatistically significant differences in the embedding distance distributions,\nthat are also scale free -- they qualitatively hold regardless of the distance\nnorm used and the number of keywords, questions, or responses. We leverage\nthese structural differences to develop a tool to detect hallucinated\nresponses, achieving an accuracy of 66\\% for a specific configuration of system\nparameters -- comparable with the best results in the field. In conclusion, the\nsuggested methodology is promising and novel, possibly paving the way for\nfurther research in the domain, also along the directions highlighted in our\nfuture work.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CY"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.08663v1",
    "published_date": "2025-02-10 09:44:13 UTC",
    "updated_date": "2025-02-10 09:44:13 UTC"
  },
  {
    "arxiv_id": "2502.06298v1",
    "title": "SeaExam and SeaBench: Benchmarking LLMs with Local Multilingual Questions in Southeast Asia",
    "authors": [
      "Chaoqun Liu",
      "Wenxuan Zhang",
      "Jiahao Ying",
      "Mahani Aljunied",
      "Anh Tuan Luu",
      "Lidong Bing"
    ],
    "abstract": "This study introduces two novel benchmarks, SeaExam and SeaBench, designed to\nevaluate the capabilities of Large Language Models (LLMs) in Southeast Asian\n(SEA) application scenarios. Unlike existing multilingual datasets primarily\nderived from English translations, these benchmarks are constructed based on\nreal-world scenarios from SEA regions. SeaExam draws from regional educational\nexams to form a comprehensive dataset that encompasses subjects such as local\nhistory and literature. In contrast, SeaBench is crafted around multi-turn,\nopen-ended tasks that reflect daily interactions within SEA communities. Our\nevaluations demonstrate that SeaExam and SeaBench more effectively discern LLM\nperformance on SEA language tasks compared to their translated benchmarks. This\nhighlights the importance of using real-world queries to assess the\nmultilingual capabilities of LLMs.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted to Findings of NAACL 2025",
    "pdf_url": "http://arxiv.org/pdf/2502.06298v1",
    "published_date": "2025-02-10 09:40:25 UTC",
    "updated_date": "2025-02-10 09:40:25 UTC"
  },
  {
    "arxiv_id": "2502.08662v2",
    "title": "RoToR: Towards More Reliable Responses for Order-Invariant Inputs",
    "authors": [
      "Soyoung Yoon",
      "Dongha Ahn",
      "Youngwon Lee",
      "Minkyu Jung",
      "HyungJoo Jang",
      "Seung-won Hwang"
    ],
    "abstract": "Mitigating positional bias of language models (LMs) for listwise inputs is a\nwell-known and important problem (e.g., lost-in-the-middle). While zero-shot\norder-invariant LMs have been proposed to solve this issue, their success on\npractical listwise problems has been limited. In this work, as a first\ncontribution, we identify and overcome two limitations to make zero-shot\ninvariant LMs more practical: (1) training and inference distribution mismatch\narising from modifying positional ID assignments to enforce invariance, and (2)\nfailure to adapt to a mixture of order-invariant and sensitive inputs in\npractical listwise problems. Then, to overcome these issues we propose (1)\nRoToR, a zero-shot invariant LM for genuinely order-invariant inputs with\nminimal modifications of positional IDs, and (2) Selective Routing, an adaptive\nframework that handles both order-invariant and order-sensitive inputs in\nlistwise tasks. On the Lost in the middle (LitM), Knowledge Graph QA (KGQA),\nand MMLU benchmarks, we show that RoToR with Selective Routing can effectively\nhandle practical listwise input tasks in a zero-shot manner.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.08662v2",
    "published_date": "2025-02-10 09:34:15 UTC",
    "updated_date": "2025-03-07 09:55:19 UTC"
  },
  {
    "arxiv_id": "2502.09642v2",
    "title": "Krutrim LLM: Multilingual Foundational Model for over a Billion People",
    "authors": [
      "Aditya Kallappa",
      "Palash Kamble",
      "Abhinav Ravi",
      "Akshat Patidar",
      "Vinayak Dhruv",
      "Deepak Kumar",
      "Raghav Awasthi",
      "Arveti Manjunath",
      "Himanshu Gupta",
      "Shubham Agarwal",
      "Kumar Ashish",
      "Gautam Bhargava",
      "Chandra Khatri"
    ],
    "abstract": "India is a diverse society with unique challenges in developing AI systems,\nincluding linguistic diversity, oral traditions, data accessibility, and\nscalability. Existing foundation models are primarily trained on English,\nlimiting their effectiveness for India's population. Indic languages comprise\nonly 1 percent of Common Crawl corpora despite India representing 18 percent of\nthe global population, leading to linguistic biases. Thousands of regional\nlanguages, dialects, and code mixing create additional representation\nchallenges due to sparse training data.\n  We introduce Krutrim LLM, a 2 trillion token multilingual model designed for\nIndia's linguistic landscape. It incorporates the largest known Indic dataset,\nmitigating data scarcity and ensuring balanced performance across dialects.\nKrutrim outperforms or matches state-of-the-art models on Indic benchmarks\nwhile maintaining competitive English performance. Despite being significantly\nsmaller in training flops, Krutrim LLM matches or exceeds models like LLAMA-2\non 10 out of 16 tasks, with an average score of 0.57 versus 0.55. This\nevidences Krutrim's flexible multilingual fluency across diverse linguistic\ncontexts.\n  Krutrim is integrated with real-time search to improve factual accuracy in\nconversational AI applications. This enhances accessibility for over 1 billion\nusers worldwide. Through intentional design choices addressing data imbalances,\nKrutrim LLM signifies meaningful progress in building ethical, globally\nrepresentative AI models.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.09642v2",
    "published_date": "2025-02-10 09:32:08 UTC",
    "updated_date": "2025-02-24 06:38:13 UTC"
  },
  {
    "arxiv_id": "2502.06289v1",
    "title": "Is an Ultra Large Natural Image-Based Foundation Model Superior to a Retina-Specific Model for Detecting Ocular and Systemic Diseases?",
    "authors": [
      "Qingshan Hou",
      "Yukun Zhou",
      "Jocelyn Hui Lin Goh",
      "Ke Zou",
      "Samantha Min Er Yew",
      "Sahana Srinivasan",
      "Meng Wang",
      "Thaddaeus Lo",
      "Xiaofeng Lei",
      "Siegfried K. Wagner",
      "Mark A. Chia",
      "Dawei Yang",
      "Hongyang Jiang",
      "AnRan Ran",
      "Rui Santos",
      "Gabor Mark Somfai",
      "Juan Helen Zhou",
      "Haoyu Chen",
      "Qingyu Chen",
      "Carol Yim-Lui Cheung",
      "Pearse A. Keane",
      "Yih Chung Tham"
    ],
    "abstract": "The advent of foundation models (FMs) is transforming medical domain. In\nophthalmology, RETFound, a retina-specific FM pre-trained sequentially on 1.4\nmillion natural images and 1.6 million retinal images, has demonstrated high\nadaptability across clinical applications. Conversely, DINOv2, a\ngeneral-purpose vision FM pre-trained on 142 million natural images, has shown\npromise in non-medical domains. However, its applicability to clinical tasks\nremains underexplored. To address this, we conducted head-to-head evaluations\nby fine-tuning RETFound and three DINOv2 models (large, base, small) for ocular\ndisease detection and systemic disease prediction tasks, across eight\nstandardized open-source ocular datasets, as well as the Moorfields AlzEye and\nthe UK Biobank datasets. DINOv2-large model outperformed RETFound in detecting\ndiabetic retinopathy (AUROC=0.850-0.952 vs 0.823-0.944, across three datasets,\nall P<=0.007) and multi-class eye diseases (AUROC=0.892 vs. 0.846, P<0.001). In\nglaucoma, DINOv2-base model outperformed RETFound (AUROC=0.958 vs 0.940,\nP<0.001). Conversely, RETFound achieved superior performance over all DINOv2\nmodels in predicting heart failure, myocardial infarction, and ischaemic stroke\n(AUROC=0.732-0.796 vs 0.663-0.771, all P<0.001). These trends persisted even\nwith 10% of the fine-tuning data. These findings showcase the distinct\nscenarios where general-purpose and domain-specific FMs excel, highlighting the\nimportance of aligning FM selection with task-specific requirements to optimise\nclinical performance.",
    "categories": [
      "eess.IV",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "eess.IV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.06289v1",
    "published_date": "2025-02-10 09:31:39 UTC",
    "updated_date": "2025-02-10 09:31:39 UTC"
  },
  {
    "arxiv_id": "2502.06285v1",
    "title": "End-to-End Multi-Microphone Speaker Extraction Using Relative Transfer Functions",
    "authors": [
      "Aviad Eisenberg",
      "Sharon Gannot",
      "Shlomo E. Chazan"
    ],
    "abstract": "This paper introduces a multi-microphone method for extracting a desired\nspeaker from a mixture involving multiple speakers and directional noise in a\nreverberant environment. In this work, we propose leveraging the instantaneous\nrelative transfer function (RTF), estimated from a reference utterance recorded\nin the same position as the desired source. The effectiveness of the RTF-based\nspatial cue is compared with direction of arrival (DOA)-based spatial cue and\nthe conventional spectral embedding. Experimental results in challenging\nacoustic scenarios demonstrate that using spatial cues yields better\nperformance than the spectral-based cue and that the instantaneous RTF\noutperforms the DOA-based spatial cue.",
    "categories": [
      "cs.SD",
      "cs.AI"
    ],
    "primary_category": "cs.SD",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.06285v1",
    "published_date": "2025-02-10 09:27:44 UTC",
    "updated_date": "2025-02-10 09:27:44 UTC"
  },
  {
    "arxiv_id": "2502.06913v2",
    "title": "A Simple yet Effective DDG Predictor is An Unsupervised Antibody Optimizer and Explainer",
    "authors": [
      "Lirong Wu",
      "Yunfan Liu",
      "Haitao Lin",
      "Yufei Huang",
      "Guojiang Zhao",
      "Zhifeng Gao",
      "Stan Z. Li"
    ],
    "abstract": "The proteins that exist today have been optimized over billions of years of\nnatural evolution, during which nature creates random mutations and selects\nthem. The discovery of functionally promising mutations is challenged by the\nlimited evolutionary accessible regions, i.e., only a small region on the\nfitness landscape is beneficial. There have been numerous priors used to\nconstrain protein evolution to regions of landscapes with high-fitness\nvariants, among which the change in binding free energy (DDG) of protein\ncomplexes upon mutations is one of the most commonly used priors. However, the\nhuge mutation space poses two challenges: (1) how to improve the efficiency of\nDDG prediction for fast mutation screening; and (2) how to explain mutation\npreferences and efficiently explore accessible evolutionary regions. To address\nthese challenges, we propose a lightweight DDG predictor (Light-DDG), which\nadopts a structure-aware Transformer as the backbone and enhances it by\nknowledge distilled from existing powerful but computationally heavy DDG\npredictors. Additionally, we augmented, annotated, and released a large-scale\ndataset containing millions of mutation data for pre-training Light-DDG. We\nfind that such a simple yet effective Light-DDG can serve as a good\nunsupervised antibody optimizer and explainer. For the target antibody, we\npropose a novel Mutation Explainer to learn mutation preferences, which\naccounts for the marginal benefit of each mutation per residue. To further\nexplore accessible evolutionary regions, we conduct preference-guided antibody\noptimization and evaluate antibody candidates quickly using Light-DDG to\nidentify desirable mutations.",
    "categories": [
      "q-bio.QM",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "q-bio.QM",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.06913v2",
    "published_date": "2025-02-10 09:26:57 UTC",
    "updated_date": "2025-02-13 11:42:53 UTC"
  },
  {
    "arxiv_id": "2502.06282v1",
    "title": "Jakiro: Boosting Speculative Decoding with Decoupled Multi-Head via MoE",
    "authors": [
      "Haiduo Huang",
      "Fuwei Yang",
      "Zhenhua Liu",
      "Yixing Xu",
      "Jinze Li",
      "Yang Liu",
      "Xuanwu Yin",
      "Dong Li",
      "Pengju Ren",
      "Emad Barsoum"
    ],
    "abstract": "Speculative decoding (SD) accelerates large language model inference by using\na smaller draft model to predict multiple tokens, which are then verified in\nparallel by the larger target model. However, the limited capacity of the draft\nmodel often necessitates tree-based sampling to improve prediction accuracy,\nwhere multiple candidates are generated at each step. We identify a key\nlimitation in this approach: the candidates at the same step are derived from\nthe same representation, limiting diversity and reducing overall effectiveness.\nTo address this, we propose Jakiro, leveraging Mixture of Experts (MoE), where\nindependent experts generate diverse predictions, effectively decoupling\ncorrelations among candidates. Furthermore, we introduce a hybrid inference\nstrategy, combining autoregressive decoding for initial tokens with parallel\ndecoding for subsequent stages, and enhance the latter with contrastive\nmechanism in features to improve accuracy. Our method significantly boosts\nprediction accuracy and achieves higher inference speedups. Extensive\nexperiments across diverse models validate the effectiveness and robustness of\nour approach, establishing a new SOTA in speculative decoding. Our codes are\navailable at https://github.com/haiduo/Jakiro.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.06282v1",
    "published_date": "2025-02-10 09:24:06 UTC",
    "updated_date": "2025-02-10 09:24:06 UTC"
  },
  {
    "arxiv_id": "2502.10441v1",
    "title": "AI Alignment at Your Discretion",
    "authors": [
      "Maarten Buyl",
      "Hadi Khalaf",
      "Claudio Mayrink Verdun",
      "Lucas Monteiro Paes",
      "Caio C. Vieira Machado",
      "Flavio du Pin Calmon"
    ],
    "abstract": "In AI alignment, extensive latitude must be granted to annotators, either\nhuman or algorithmic, to judge which model outputs are `better' or `safer.' We\nrefer to this latitude as alignment discretion. Such discretion remains largely\nunexamined, posing two risks: (i) annotators may use their power of discretion\narbitrarily, and (ii) models may fail to mimic this discretion. To study this\nphenomenon, we draw on legal concepts of discretion that structure how\ndecision-making authority is conferred and exercised, particularly in cases\nwhere principles conflict or their application is unclear or irrelevant.\nExtended to AI alignment, discretion is required when alignment principles and\nrules are (inevitably) conflicting or indecisive. We present a set of metrics\nto systematically analyze when and how discretion in AI alignment is exercised,\nsuch that both risks (i) and (ii) can be observed. Moreover, we distinguish\nbetween human and algorithmic discretion and analyze the discrepancy between\nthem. By measuring both human and algorithmic discretion over safety alignment\ndatasets, we reveal layers of discretion in the alignment process that were\npreviously unaccounted for. Furthermore, we demonstrate how algorithms trained\non these datasets develop their own forms of discretion in interpreting and\napplying these principles, which challenges the purpose of having any\nprinciples at all. Our paper presents the first step towards formalizing this\ncore gap in current alignment processes, and we call on the community to\nfurther scrutinize and control alignment discretion.",
    "categories": [
      "cs.AI",
      "cs.CY",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.10441v1",
    "published_date": "2025-02-10 09:19:52 UTC",
    "updated_date": "2025-02-10 09:19:52 UTC"
  },
  {
    "arxiv_id": "2502.06274v1",
    "title": "HODDI: A Dataset of High-Order Drug-Drug Interactions for Computational Pharmacovigilance",
    "authors": [
      "Zhaoying Wang",
      "Yingdan Shi",
      "Xiang Liu",
      "Can Chen",
      "Jun Wen",
      "Ren Wang"
    ],
    "abstract": "Drug-side effect research is vital for understanding adverse reactions\narising in complex multi-drug therapies. However, the scarcity of higher-order\ndatasets that capture the combinatorial effects of multiple drugs severely\nlimits progress in this field. Existing resources such as TWOSIDES primarily\nfocus on pairwise interactions. To fill this critical gap, we introduce HODDI,\nthe first Higher-Order Drug-Drug Interaction Dataset, constructed from U.S.\nFood and Drug Administration (FDA) Adverse Event Reporting System (FAERS)\nrecords spanning the past decade, to advance computational pharmacovigilance.\nHODDI contains 109,744 records involving 2,506 unique drugs and 4,569 unique\nside effects, specifically curated to capture multi-drug interactions and their\ncollective impact on adverse effects. Comprehensive statistical analyses\ndemonstrate HODDI's extensive coverage and robust analytical metrics, making it\na valuable resource for studying higher-order drug relationships. Evaluating\nHODDI with multiple models, we found that simple Multi-Layer Perceptron (MLP)\ncan outperform graph models, while hypergraph models demonstrate superior\nperformance in capturing complex multi-drug interactions, further validating\nHODDI's effectiveness. Our findings highlight the inherent value of\nhigher-order information in drug-side effect prediction and position HODDI as a\nbenchmark dataset for advancing research in pharmacovigilance, drug safety, and\npersonalized medicine. The dataset and codes are available at\nhttps://github.com/TIML-Group/HODDI.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "q-bio.MN"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.06274v1",
    "published_date": "2025-02-10 09:18:51 UTC",
    "updated_date": "2025-02-10 09:18:51 UTC"
  },
  {
    "arxiv_id": "2502.10440v1",
    "title": "Towards Copyright Protection for Knowledge Bases of Retrieval-augmented Language Models via Ownership Verification with Reasoning",
    "authors": [
      "Junfeng Guo",
      "Yiming Li",
      "Ruibo Chen",
      "Yihan Wu",
      "Chenxi Liu",
      "Yanshuo Chen",
      "Heng Huang"
    ],
    "abstract": "Large language models (LLMs) are increasingly integrated into real-world\napplications through retrieval-augmented generation (RAG) mechanisms to\nsupplement their responses with up-to-date and domain-specific knowledge.\nHowever, the valuable and often proprietary nature of the knowledge bases used\nin RAG introduces the risk of unauthorized usage by adversaries. Existing\nmethods that can be generalized as watermarking techniques to protect these\nknowledge bases typically involve poisoning attacks. However, these methods\nrequire to alter the results of verification samples (\\eg, generating incorrect\noutputs), inevitably making them susceptible to anomaly detection and even\nintroduce new security risks. To address these challenges, we propose \\name{}\nfor `harmless' copyright protection of knowledge bases. Instead of manipulating\nLLM's final output, \\name{} implants distinct verification behaviors in the\nspace of chain-of-thought (CoT) reasoning, maintaining the correctness of the\nfinal answer. Our method has three main stages: (1) \\textbf{Generating CoTs}:\nFor each verification question, we generate two CoTs, including a target CoT\nfor building watermark behaviors; (2) \\textbf{Optimizing Watermark Phrases and\nTarget CoTs}: We optimize them to minimize retrieval errors under the black-box\nsetting of suspicious LLM, ensuring that the watermarked verification queries\nactivate the target CoTs without being activated in non-watermarked ones; (3)\n\\textbf{Ownership Verification}: We exploit a pairwise Wilcoxon test to\nstatistically verify whether a suspicious LLM is augmented with the protected\nknowledge base by comparing its responses to watermarked and benign\nverification queries. Our experiments on diverse benchmarks demonstrate that\n\\name{} effectively protects knowledge bases against unauthorized usage while\npreserving the integrity and performance of the RAG.",
    "categories": [
      "cs.CR",
      "cs.AI",
      "cs.CL",
      "cs.IR",
      "cs.LG"
    ],
    "primary_category": "cs.CR",
    "comment": "The first two authors contributed equally to this work. 19 pages",
    "pdf_url": "http://arxiv.org/pdf/2502.10440v1",
    "published_date": "2025-02-10 09:15:56 UTC",
    "updated_date": "2025-02-10 09:15:56 UTC"
  },
  {
    "arxiv_id": "2502.07820v1",
    "title": "Low-Rank Compression for IMC Arrays",
    "authors": [
      "Kang Eun Jeon",
      "Johnny Rhe",
      "Jong Hwan Ko"
    ],
    "abstract": "In this study, we address the challenge of low-rank model compression in the\ncontext of in-memory computing (IMC) architectures. Traditional pruning\napproaches, while effective in model size reduction, necessitate additional\nperipheral circuitry to manage complex dataflows and mitigate dislocation\nissues, leading to increased area and energy overheads. To circumvent these\ndrawbacks, we propose leveraging low-rank compression techniques, which, unlike\npruning, streamline the dataflow and seamlessly integrate with IMC\narchitectures. However, low-rank compression presents its own set of\nchallenges, namely i) suboptimal IMC array utilization and ii) compromised\naccuracy. To address these issues, we introduce a novel approach i) employing\nshift and duplicate kernel (SDK) mapping technique, which exploits idle IMC\ncolumns for parallel processing, and ii) group low-rank convolution, which\nmitigates the information imbalance in the decomposed matrices. Our\nexperimental results demonstrate that our proposed method achieves up to 2.5x\nspeedup or +20.9% accuracy boost over existing pruning techniques.",
    "categories": [
      "cs.AR",
      "cs.AI"
    ],
    "primary_category": "cs.AR",
    "comment": "Accepted to appear at DATE'25 (Lyon, France)",
    "pdf_url": "http://arxiv.org/pdf/2502.07820v1",
    "published_date": "2025-02-10 08:57:39 UTC",
    "updated_date": "2025-02-10 08:57:39 UTC"
  },
  {
    "arxiv_id": "2502.06257v1",
    "title": "K-ON: Stacking Knowledge On the Head Layer of Large Language Model",
    "authors": [
      "Lingbing Guo",
      "Yichi Zhang",
      "Zhongpu Bo",
      "Zhuo Chen",
      "Mengshu Sun",
      "Zhiqiang Zhang",
      "Wen Zhang",
      "Huajun Chen"
    ],
    "abstract": "Recent advancements in large language models (LLMs) have significantly\nimproved various natural language processing (NLP) tasks. Typically, LLMs are\ntrained to predict the next token, aligning well with many NLP tasks. However,\nin knowledge graph (KG) scenarios, entities are the fundamental units and\nidentifying an entity requires at least several tokens. This leads to a\ngranularity mismatch between KGs and natural languages. To address this issue,\nwe propose K-ON, which integrates KG knowledge into the LLM by employing\nmultiple head layers for next k-step prediction. K-ON can not only generate\nentity-level results in one step, but also enables contrastive loss against\nentities, which is the most powerful tool in KG representation learning.\nExperimental results show that K-ON outperforms state-of-the-art methods that\nincorporate text and even the other modalities.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "AAAI 2025 (Oral)",
    "pdf_url": "http://arxiv.org/pdf/2502.06257v1",
    "published_date": "2025-02-10 08:45:56 UTC",
    "updated_date": "2025-02-10 08:45:56 UTC"
  },
  {
    "arxiv_id": "2502.06255v1",
    "title": "Towards Efficient and Intelligent Laser Weeding: Method and Dataset for Weed Stem Detection",
    "authors": [
      "Dingning Liu",
      "Jinzhe Li",
      "Haoyang Su",
      "Bei Cui",
      "Zhihui Wang",
      "Qingbo Yuan",
      "Wanli Ouyang",
      "Nanqing Dong"
    ],
    "abstract": "Weed control is a critical challenge in modern agriculture, as weeds compete\nwith crops for essential nutrient resources, significantly reducing crop yield\nand quality. Traditional weed control methods, including chemical and\nmechanical approaches, have real-life limitations such as associated\nenvironmental impact and efficiency. An emerging yet effective approach is\nlaser weeding, which uses a laser beam as the stem cutter. Although there have\nbeen studies that use deep learning in weed recognition, its application in\nintelligent laser weeding still requires a comprehensive understanding. Thus,\nthis study represents the first empirical investigation of weed recognition for\nlaser weeding. To increase the efficiency of laser beam cut and avoid damaging\nthe crops of interest, the laser beam shall be directly aimed at the weed root.\nYet, weed stem detection remains an under-explored problem. We integrate the\ndetection of crop and weed with the localization of weed stem into one\nend-to-end system. To train and validate the proposed system in a real-life\nscenario, we curate and construct a high-quality weed stem detection dataset\nwith human annotations. The dataset consists of 7,161 high-resolution pictures\ncollected in the field with annotations of 11,151 instances of weed.\nExperimental results show that the proposed system improves weeding accuracy by\n6.7% and reduces energy cost by 32.3% compared to existing weed recognition\nsystems.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "Accepted by AAAI-AISI 2025",
    "pdf_url": "http://arxiv.org/pdf/2502.06255v1",
    "published_date": "2025-02-10 08:42:46 UTC",
    "updated_date": "2025-02-10 08:42:46 UTC"
  },
  {
    "arxiv_id": "2502.06249v2",
    "title": "Conditioning through indifference in quantum mechanics",
    "authors": [
      "Keano De Vos",
      "Gert de Cooman"
    ],
    "abstract": "We can learn (more) about the state a quantum system is in through\nmeasurements. We look at how to describe the uncertainty about a quantum\nsystem's state conditional on executing such measurements. We show that by\nexploiting the interplay between desirability, coherence and indifference, a\ngeneral rule for conditioning can be derived. We then apply this rule to\nconditioning on measurement outcomes, and show how it generalises to\nconditioning on a set of measurement outcomes.",
    "categories": [
      "quant-ph",
      "cs.AI",
      "math.PR"
    ],
    "primary_category": "quant-ph",
    "comment": "11 pages",
    "pdf_url": "http://arxiv.org/pdf/2502.06249v2",
    "published_date": "2025-02-10 08:27:02 UTC",
    "updated_date": "2025-04-20 12:38:24 UTC"
  },
  {
    "arxiv_id": "2502.06235v2",
    "title": "Conditioning and AGM-like belief change in the Desirability-Indifference framework",
    "authors": [
      "Kathelijne Coussement",
      "Gert de Cooman",
      "Keano De Vos"
    ],
    "abstract": "We show how the AGM framework for belief change (expansion, revision,\ncontraction) can be extended to deal with conditioning in the so-called\nDesirability-Indifference framework, based on abstract notions of accepting and\nrejecting options, as well as on abstract notions of events. This level of\nabstraction allows us to deal simultaneously with classical and quantum\nprobability theory.",
    "categories": [
      "cs.AI",
      "math.PR",
      "quant-ph"
    ],
    "primary_category": "cs.AI",
    "comment": "12 pages",
    "pdf_url": "http://arxiv.org/pdf/2502.06235v2",
    "published_date": "2025-02-10 08:11:00 UTC",
    "updated_date": "2025-04-20 12:51:59 UTC"
  },
  {
    "arxiv_id": "2502.06233v1",
    "title": "Confidence Improves Self-Consistency in LLMs",
    "authors": [
      "Amir Taubenfeld",
      "Tom Sheffer",
      "Eran Ofek",
      "Amir Feder",
      "Ariel Goldstein",
      "Zorik Gekhman",
      "Gal Yona"
    ],
    "abstract": "Self-consistency decoding enhances LLMs' performance on reasoning tasks by\nsampling diverse reasoning paths and selecting the most frequent answer.\nHowever, it is computationally expensive, as sampling many of these (lengthy)\npaths is required to increase the chances that the correct answer emerges as\nthe most frequent one. To address this, we introduce Confidence-Informed\nSelf-Consistency (CISC). CISC performs a weighted majority vote based on\nconfidence scores obtained directly from the model. By prioritizing\nhigh-confidence paths, it can identify the correct answer with a significantly\nsmaller sample size. When tested on nine models and four datasets, CISC\noutperforms self-consistency in nearly all configurations, reducing the\nrequired number of reasoning paths by over 40% on average. In addition, we\nintroduce the notion of within-question confidence evaluation, after showing\nthat standard evaluation methods are poor predictors of success in\ndistinguishing correct and incorrect answers to the same question. In fact, the\nmost calibrated confidence method proved to be the least effective for CISC.\nLastly, beyond these practical implications, our results and analyses show that\nLLMs can effectively judge the correctness of their own outputs, contributing\nto the ongoing debate on this topic.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.06233v1",
    "published_date": "2025-02-10 08:10:29 UTC",
    "updated_date": "2025-02-10 08:10:29 UTC"
  },
  {
    "arxiv_id": "2502.06217v1",
    "title": "Examining False Positives under Inference Scaling for Mathematical Reasoning",
    "authors": [
      "Yu Wang",
      "Nan Yang",
      "Liang Wang",
      "Furu Wei"
    ],
    "abstract": "Recent advancements in language models have led to significant improvements\nin mathematical reasoning across various benchmarks. However, most of these\nbenchmarks rely on automatic evaluation methods that only compare final answers\nusing heuristics, without verifying the underlying reasoning steps. This\nlimitation results in false positive solutions, where models may produce\ncorrect final answers but with flawed deduction paths. In this paper, we\nsystematically examine the prevalence of false positive solutions in\nmathematical problem solving for language models. We analyze the\ncharacteristics and extent of this issue across different open-source models,\ndatasets of varying difficulty levels, and decoding strategies. Specifically,\nwe explore how false positives influence the inference time scaling behavior of\nlanguage models. Our experimental results reveal that: (1) false positive\nsolutions persist across different models, datasets, and decoding methods, (2)\nsampling-based inference time scaling methods do not alleviate the problem, and\n(3) the pass@N evaluation metric is more susceptible to false positives,\nsuggesting a significantly lower scaling ceiling than what automatic\nevaluations indicate. Additionally, we analyze specific instances of false\npositives and discuss potential limitations in self-improvement techniques and\nsynthetic data generation under such conditions.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.06217v1",
    "published_date": "2025-02-10 07:49:35 UTC",
    "updated_date": "2025-02-10 07:49:35 UTC"
  },
  {
    "arxiv_id": "2502.06215v1",
    "title": "LessLeak-Bench: A First Investigation of Data Leakage in LLMs Across 83 Software Engineering Benchmarks",
    "authors": [
      "Xin Zhou",
      "Martin Weyssow",
      "Ratnadira Widyasari",
      "Ting Zhang",
      "Junda He",
      "Yunbo Lyu",
      "Jianming Chang",
      "Beiqi Zhang",
      "Dan Huang",
      "David Lo"
    ],
    "abstract": "Large Language Models (LLMs) are widely utilized in software engineering (SE)\ntasks, such as code generation and automated program repair. However, their\nreliance on extensive and often undisclosed pre-training datasets raises\nsignificant concerns about data leakage, where the evaluation benchmark data is\nunintentionally ``seen'' by LLMs during the model's construction phase. The\ndata leakage issue could largely undermine the validity of LLM-based research\nand evaluations. Despite the increasing use of LLMs in the SE community, there\nis no comprehensive study that assesses the extent of data leakage in SE\nbenchmarks for LLMs yet. To address this gap, this paper presents the first\nlarge-scale analysis of data leakage in 83 SE benchmarks concerning LLMs. Our\nresults show that in general, data leakage in SE benchmarks is minimal, with\naverage leakage ratios of only 4.8\\%, 2.8\\%, and 0.7\\% for Python, Java, and\nC/C++ benchmarks, respectively. However, some benchmarks exhibit relatively\nhigher leakage ratios, which raises concerns about their bias in evaluation.\nFor instance, QuixBugs and BigCloneBench have leakage ratios of 100.0\\% and\n55.7\\%, respectively. Furthermore, we observe that data leakage has a\nsubstantial impact on LLM evaluation. We also identify key causes of high data\nleakage, such as the direct inclusion of benchmark data in pre-training\ndatasets and the use of coding platforms like LeetCode for benchmark\nconstruction. To address the data leakage, we introduce\n\\textbf{LessLeak-Bench}, a new benchmark that removes leaked samples from the\n83 SE benchmarks, enabling more reliable LLM evaluations in future research.\nOur study enhances the understanding of data leakage in SE benchmarks and\nprovides valuable insights for future research involving LLMs in SE.",
    "categories": [
      "cs.SE",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.SE",
    "comment": "25 pages",
    "pdf_url": "http://arxiv.org/pdf/2502.06215v1",
    "published_date": "2025-02-10 07:33:49 UTC",
    "updated_date": "2025-02-10 07:33:49 UTC"
  },
  {
    "arxiv_id": "2502.06207v3",
    "title": "Is LLM an Overconfident Judge? Unveiling the Capabilities of LLMs in Detecting Offensive Language with Annotation Disagreement",
    "authors": [
      "Junyu Lu",
      "Kai Ma",
      "Kaichun Wang",
      "Kelaiti Xiao",
      "Roy Ka-Wei Lee",
      "Bo Xu",
      "Liang Yang",
      "Hongfei Lin"
    ],
    "abstract": "Large Language Models (LLMs) have become essential for offensive language\ndetection, yet their ability to handle annotation disagreement remains\nunderexplored. Disagreement samples, which arise from subjective\ninterpretations, pose a unique challenge due to their ambiguous nature.\nUnderstanding how LLMs process these cases, particularly their confidence\nlevels, can offer insight into their alignment with human annotators. This\nstudy systematically evaluates the performance of multiple LLMs in detecting\noffensive language at varying levels of annotation agreement. We analyze binary\nclassification accuracy, examine the relationship between model confidence and\nhuman disagreement, and explore how disagreement samples influence model\ndecision-making during few-shot learning and instruction fine-tuning. Our\nfindings reveal that LLMs struggle with low-agreement samples, often exhibiting\noverconfidence in these ambiguous cases. However, utilizing disagreement\nsamples in training improves both detection accuracy and model alignment with\nhuman judgment. These insights provide a foundation for enhancing LLM-based\noffensive language detection in real-world moderation tasks.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "18 pages, accepted at the ACL 2025",
    "pdf_url": "http://arxiv.org/pdf/2502.06207v3",
    "published_date": "2025-02-10 07:14:26 UTC",
    "updated_date": "2025-05-18 09:06:50 UTC"
  },
  {
    "arxiv_id": "2502.06205v1",
    "title": "C-3PO: Compact Plug-and-Play Proxy Optimization to Achieve Human-like Retrieval-Augmented Generation",
    "authors": [
      "Guoxin Chen",
      "Minpeng Liao",
      "Peiying Yu",
      "Dingmin Wang",
      "Zile Qiao",
      "Chao Yang",
      "Xin Zhao",
      "Kai Fan"
    ],
    "abstract": "Retrieval-augmented generation (RAG) systems face a fundamental challenge in\naligning independently developed retrievers and large language models (LLMs).\nExisting approaches typically involve modifying either component or introducing\nsimple intermediate modules, resulting in practical limitations and sub-optimal\nperformance. Inspired by human search behavior -- typically involving a\nback-and-forth process of proposing search queries and reviewing documents, we\npropose C-3PO, a proxy-centric framework that facilitates communication between\nretrievers and LLMs through a lightweight multi-agent system. Our framework\nimplements three specialized agents that collaboratively optimize the entire\nRAG pipeline without altering the retriever and LLMs. These agents work\ntogether to assess the need for retrieval, generate effective queries, and\nselect information suitable for the LLMs. To enable effective multi-agent\ncoordination, we develop a tree-structured rollout approach for reward credit\nassignment in reinforcement learning. Extensive experiments in both in-domain\nand out-of-distribution scenarios demonstrate that C-3PO significantly enhances\nRAG performance while maintaining plug-and-play flexibility and superior\ngeneralization capabilities.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "Ongong work",
    "pdf_url": "http://arxiv.org/pdf/2502.06205v1",
    "published_date": "2025-02-10 07:04:32 UTC",
    "updated_date": "2025-02-10 07:04:32 UTC"
  },
  {
    "arxiv_id": "2502.06193v3",
    "title": "Can LLMs Replace Human Evaluators? An Empirical Study of LLM-as-a-Judge in Software Engineering",
    "authors": [
      "Ruiqi Wang",
      "Jiyu Guo",
      "Cuiyun Gao",
      "Guodong Fan",
      "Chun Yong Chong",
      "Xin Xia"
    ],
    "abstract": "Recently, large language models (LLMs) have been deployed to tackle various\nsoftware engineering (SE) tasks like code generation, significantly advancing\nthe automation of SE tasks. However, assessing the quality of these\nLLM-generated code and text remains challenging. The commonly used Pass@k\nmetric necessitates extensive unit tests and configured environments, demands a\nhigh labor cost, and is not suitable for evaluating LLM-generated text.\nConventional metrics like BLEU, which measure only lexical rather than semantic\nsimilarity, have also come under scrutiny. In response, a new trend has emerged\nto employ LLMs for automated evaluation, known as LLM-as-a-judge. These\nLLM-as-a-judge methods are claimed to better mimic human assessment than\nconventional metrics without relying on high-quality reference answers.\nNevertheless, their exact human alignment in SE tasks remains unexplored.\n  In this paper, we empirically explore LLM-as-a-judge methods for evaluating\nSE tasks, focusing on their alignment with human judgments. We select seven\nLLM-as-a-judge methods that utilize general-purpose LLMs, alongside two LLMs\nspecifically fine-tuned for evaluation. After generating and manually scoring\nLLM responses on three recent SE datasets of code translation, code generation,\nand code summarization, we then prompt these methods to evaluate each response.\nFinally, we compare the scores generated by these methods with human\nevaluation. The results indicate that output-based methods reach the highest\nPearson correlation of 81.32 and 68.51 with human scores in code translation\nand generation, achieving near-human evaluation, noticeably outperforming\nChrF++, one of the best conventional metrics, at 34.23 and 64.92. Such\noutput-based methods prompt LLMs to output judgments directly, and exhibit more\nbalanced score distributions that resemble human score patterns. Finally, we\nprovide...",
    "categories": [
      "cs.SE",
      "cs.AI"
    ],
    "primary_category": "cs.SE",
    "comment": "Accepted by ISSTA 2025:\n  https://conf.researchr.org/details/issta-2025/issta-2025-papers/85/Can-LLMs-replace-Human-Evaluators-An-Empirical-Study-of-LLM-as-a-Judge-in-Software-E",
    "pdf_url": "http://arxiv.org/pdf/2502.06193v3",
    "published_date": "2025-02-10 06:49:29 UTC",
    "updated_date": "2025-04-21 08:41:21 UTC"
  },
  {
    "arxiv_id": "2502.06192v2",
    "title": "Right Time to Learn:Promoting Generalization via Bio-inspired Spacing Effect in Knowledge Distillation",
    "authors": [
      "Guanglong Sun",
      "Hongwei Yan",
      "Liyuan Wang",
      "Qian Li",
      "Bo Lei",
      "Yi Zhong"
    ],
    "abstract": "Knowledge distillation (KD) is a powerful strategy for training deep neural\nnetworks (DNNs). Although it was originally proposed to train a more compact\n\"student\" model from a large \"teacher\" model, many recent efforts have focused\non adapting it to promote generalization of the model itself, such as online KD\nand self KD. Here, we propose an accessible and compatible strategy named\nSpaced KD to improve the effectiveness of both online KD and self KD, in which\nthe student model distills knowledge from a teacher model trained with a space\ninterval ahead. This strategy is inspired by a prominent theory named spacing\neffect in biological learning and memory, positing that appropriate intervals\nbetween learning trials can significantly enhance learning performance. With\nboth theoretical and empirical analyses, we demonstrate that the benefits of\nthe proposed Spaced KD stem from convergence to a flatter loss landscape during\nstochastic gradient descent (SGD). We perform extensive experiments to validate\nthe effectiveness of Spaced KD in improving the learning performance of DNNs\n(e.g., the performance gain is up to 2.31% and 3.34% on Tiny-ImageNet over\nonline KD and self KD, respectively). Our codes have been released on github\nhttps://github.com/SunGL001/Spaced-KD.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.06192v2",
    "published_date": "2025-02-10 06:48:04 UTC",
    "updated_date": "2025-05-19 14:51:05 UTC"
  },
  {
    "arxiv_id": "2502.06185v1",
    "title": "Discourse-Driven Evaluation: Unveiling Factual Inconsistency in Long Document Summarization",
    "authors": [
      "Yang Zhong",
      "Diane Litman"
    ],
    "abstract": "Detecting factual inconsistency for long document summarization remains\nchallenging, given the complex structure of the source article and long summary\nlength. In this work, we study factual inconsistency errors and connect them\nwith a line of discourse analysis. We find that errors are more common in\ncomplex sentences and are associated with several discourse features. We\npropose a framework that decomposes long texts into discourse-inspired chunks\nand utilizes discourse information to better aggregate sentence-level scores\npredicted by natural language inference models. Our approach shows improved\nperformance on top of different model baselines over several evaluation\nbenchmarks, covering rich domains of texts, focusing on long document\nsummarization. This underscores the significance of incorporating discourse\nfeatures in developing models for scoring summaries for long document factual\ninconsistency.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "NAACL 2025 camera-ready version",
    "pdf_url": "http://arxiv.org/pdf/2502.06185v1",
    "published_date": "2025-02-10 06:30:15 UTC",
    "updated_date": "2025-02-10 06:30:15 UTC"
  },
  {
    "arxiv_id": "2502.06180v1",
    "title": "RideKE: Leveraging Low-Resource, User-Generated Twitter Content for Sentiment and Emotion Detection in Kenyan Code-Switched Dataset",
    "authors": [
      "Naome A. Etori",
      "Maria L. Gini"
    ],
    "abstract": "Social media has become a crucial open-access platform for individuals to\nexpress opinions and share experiences. However, leveraging low-resource\nlanguage data from Twitter is challenging due to scarce, poor-quality content\nand the major variations in language use, such as slang and code-switching.\nIdentifying tweets in these languages can be difficult as Twitter primarily\nsupports high-resource languages. We analyze Kenyan code-switched data and\nevaluate four state-of-the-art (SOTA) transformer-based pretrained models for\nsentiment and emotion classification, using supervised and semi-supervised\nmethods. We detail the methodology behind data collection and annotation, and\nthe challenges encountered during the data curation phase. Our results show\nthat XLM-R outperforms other models; for sentiment analysis, XLM-R supervised\nmodel achieves the highest accuracy (69.2\\%) and F1 score (66.1\\%), XLM-R\nsemi-supervised (67.2\\% accuracy, 64.1\\% F1 score). In emotion analysis,\nDistilBERT supervised leads in accuracy (59.8\\%) and F1 score (31\\%), mBERT\nsemi-supervised (accuracy (59\\% and F1 score 26.5\\%). AfriBERTa models show the\nlowest accuracy and F1 scores. All models tend to predict neutral sentiment,\nwith Afri-BERT showing the highest bias and unique sensitivity to empathy\nemotion. https://github.com/NEtori21/Ride_hailing",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted in WASSA 2024",
    "pdf_url": "http://arxiv.org/pdf/2502.06180v1",
    "published_date": "2025-02-10 06:18:07 UTC",
    "updated_date": "2025-02-10 06:18:07 UTC"
  },
  {
    "arxiv_id": "2502.06173v1",
    "title": "Uncertainty-Aware Adaptation of Large Language Models for Protein-Protein Interaction Analysis",
    "authors": [
      "Sanket Jantre",
      "Tianle Wang",
      "Gilchan Park",
      "Kriti Chopra",
      "Nicholas Jeon",
      "Xiaoning Qian",
      "Nathan M. Urban",
      "Byung-Jun Yoon"
    ],
    "abstract": "Identification of protein-protein interactions (PPIs) helps derive cellular\nmechanistic understanding, particularly in the context of complex conditions\nsuch as neurodegenerative disorders, metabolic syndromes, and cancer. Large\nLanguage Models (LLMs) have demonstrated remarkable potential in predicting\nprotein structures and interactions via automated mining of vast biomedical\nliterature; yet their inherent uncertainty remains a key challenge for deriving\nreproducible findings, critical for biomedical applications. In this study, we\npresent an uncertainty-aware adaptation of LLMs for PPI analysis, leveraging\nfine-tuned LLaMA-3 and BioMedGPT models. To enhance prediction reliability, we\nintegrate LoRA ensembles and Bayesian LoRA models for uncertainty\nquantification (UQ), ensuring confidence-calibrated insights into protein\nbehavior. Our approach achieves competitive performance in PPI identification\nacross diverse disease contexts while addressing model uncertainty, thereby\nenhancing trustworthiness and reproducibility in computational biology. These\nfindings underscore the potential of uncertainty-aware LLM adaptation for\nadvancing precision medicine and biomedical research.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL",
      "stat.AP",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.06173v1",
    "published_date": "2025-02-10 05:54:36 UTC",
    "updated_date": "2025-02-10 05:54:36 UTC"
  },
  {
    "arxiv_id": "2502.06170v1",
    "title": "An Interpretable Implicit-Based Approach for Modeling Local Spatial Effects: A Case Study of Global Gross Primary Productivity",
    "authors": [
      "Siqi Du",
      "Hongsheng Huang",
      "Kaixin Shen",
      "Ziqi Liu",
      "Shengjun Tang"
    ],
    "abstract": "In Earth sciences, unobserved factors exhibit non-stationary spatial\ndistributions, causing the relationships between features and targets to\ndisplay spatial heterogeneity. In geographic machine learning tasks,\nconventional statistical learning methods often struggle to capture spatial\nheterogeneity, leading to unsatisfactory prediction accuracy and unreliable\ninterpretability. While approaches like Geographically Weighted Regression\n(GWR) capture local variations, they fall short of uncovering global patterns\nand tracking the continuous evolution of spatial heterogeneity. Motivated by\nthis limitation, we propose a novel perspective - that is, simultaneously\nmodeling common features across different locations alongside spatial\ndifferences using deep neural networks. The proposed method is a dual-branch\nneural network with an encoder-decoder structure. In the encoding stage, the\nmethod aggregates node information in a spatiotemporal conditional graph using\nGCN and LSTM, encoding location-specific spatiotemporal heterogeneity as an\nimplicit conditional vector. Additionally, a self-attention-based encoder is\nused to extract location-invariant common features from the data. In the\ndecoding stage, the approach employs a conditional generation strategy that\npredicts response variables and interpretative weights based on data features\nunder spatiotemporal conditions. The approach is validated by predicting\nvegetation gross primary productivity (GPP) using global climate and land cover\ndata from 2001 to 2020. Trained on 50 million samples and tested on 2.8\nmillion, the proposed model achieves an RMSE of 0.836, outperforming LightGBM\n(1.063) and TabNet (0.944). Visualization analyses indicate that our method can\nreveal the distribution differences of the dominant factors of GPP across\nvarious times and locations.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.06170v1",
    "published_date": "2025-02-10 05:44:54 UTC",
    "updated_date": "2025-02-10 05:44:54 UTC"
  },
  {
    "arxiv_id": "2502.06167v1",
    "title": "Universal Approximation of Visual Autoregressive Transformers",
    "authors": [
      "Yifang Chen",
      "Xiaoyu Li",
      "Yingyu Liang",
      "Zhenmei Shi",
      "Zhao Song"
    ],
    "abstract": "We investigate the fundamental limits of transformer-based foundation models,\nextending our analysis to include Visual Autoregressive (VAR) transformers. VAR\nrepresents a big step toward generating images using a novel, scalable,\ncoarse-to-fine ``next-scale prediction'' framework. These models set a new\nquality bar, outperforming all previous methods, including Diffusion\nTransformers, while having state-of-the-art performance for image synthesis\ntasks. Our primary contributions establish that, for single-head VAR\ntransformers with a single self-attention layer and single interpolation layer,\nthe VAR Transformer is universal. From the statistical perspective, we prove\nthat such simple VAR transformers are universal approximators for any\nimage-to-image Lipschitz functions. Furthermore, we demonstrate that flow-based\nautoregressive transformers inherit similar approximation capabilities. Our\nresults provide important design principles for effective and computationally\nefficient VAR Transformer strategies that can be used to extend their utility\nto more sophisticated VAR models in image generation and other related areas.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL",
      "cs.CV"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.06167v1",
    "published_date": "2025-02-10 05:36:30 UTC",
    "updated_date": "2025-02-10 05:36:30 UTC"
  },
  {
    "arxiv_id": "2502.06911v1",
    "title": "Foundation Models for Anomaly Detection: Vision and Challenges",
    "authors": [
      "Jing Ren",
      "Tao Tang",
      "Hong Jia",
      "Haytham Fayek",
      "Xiaodong Li",
      "Suyu Ma",
      "Xiwei Xu",
      "Feng Xia"
    ],
    "abstract": "As data continues to grow in volume and complexity across domains such as\nfinance, manufacturing, and healthcare, effective anomaly detection is\nessential for identifying irregular patterns that may signal critical issues.\nRecently, foundation models (FMs) have emerged as a powerful tool for advancing\nanomaly detection. They have demonstrated unprecedented capabilities in\nenhancing anomaly identification, generating detailed data descriptions, and\nproviding visual explanations. This survey presents the first comprehensive\nreview of recent advancements in FM-based anomaly detection. We propose a novel\ntaxonomy that classifies FMs into three categories based on their roles in\nanomaly detection tasks, i.e., as encoders, detectors, or interpreters. We\nprovide a systematic analysis of state-of-the-art methods and discuss key\nchallenges in leveraging FMs for improved anomaly detection. We also outline\nfuture research directions in this rapidly evolving field.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "9 pages, 4 figures",
    "pdf_url": "http://arxiv.org/pdf/2502.06911v1",
    "published_date": "2025-02-10 05:01:08 UTC",
    "updated_date": "2025-02-10 05:01:08 UTC"
  },
  {
    "arxiv_id": "2502.06153v2",
    "title": "Low Tensor-Rank Adaptation of Kolmogorov--Arnold Networks",
    "authors": [
      "Yihang Gao",
      "Michael K. Ng",
      "Vincent Y. F. Tan"
    ],
    "abstract": "Kolmogorov--Arnold networks (KANs) have demonstrated their potential as an\nalternative to multi-layer perceptions (MLPs) in various domains, especially\nfor science-related tasks. However, transfer learning of KANs remains a\nrelatively unexplored area. In this paper, inspired by Tucker decomposition of\ntensors and evidence on the low tensor-rank structure in KAN parameter updates,\nwe develop low tensor-rank adaptation (LoTRA) for fine-tuning KANs. We study\nthe expressiveness of LoTRA based on Tucker decomposition approximations.\nFurthermore, we provide a theoretical analysis to select the learning rates for\neach LoTRA component to enable efficient training. Our analysis also shows that\nusing identical learning rates across all components leads to inefficient\ntraining, highlighting the need for an adaptive learning rate strategy. Beyond\ntheoretical insights, we explore the application of LoTRA for efficiently\nsolving various partial differential equations (PDEs) by fine-tuning KANs.\nAdditionally, we propose Slim KANs that incorporate the inherent\nlow-tensor-rank properties of KAN parameter tensors to reduce model size while\nmaintaining superior performance. Experimental results validate the efficacy of\nthe proposed learning rate selection strategy and demonstrate the effectiveness\nof LoTRA for transfer learning of KANs in solving PDEs. Further evaluations on\nSlim KANs for function representation and image classification tasks highlight\nthe expressiveness of LoTRA and the potential for parameter reduction through\nlow tensor-rank decomposition.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.06153v2",
    "published_date": "2025-02-10 04:57:07 UTC",
    "updated_date": "2025-02-14 01:43:07 UTC"
  },
  {
    "arxiv_id": "2502.06152v3",
    "title": "The Value of Information in Human-AI Decision-making",
    "authors": [
      "Ziyang Guo",
      "Yifan Wu",
      "Jason Hartline",
      "Jessica Hullman"
    ],
    "abstract": "Multiple agents -- including humans and AI models -- are often paired on\ndecision tasks with the expectation of achieving complementary performance,\nwhere the combined performance of both agents outperforms either one alone.\nHowever, knowing how to improve the performance of a human-AI team is often\ndifficult without knowing more about what particular information and strategies\neach agent employs. We provide a decision-theoretic framework for\ncharacterizing the value of information -- and consequently, opportunities for\nagents to better exploit available information -- in AI-assisted decision\nworkflows. We demonstrate the use of the framework for model selection,\nempirical evaluation of human-AI performance, and explanation design. We\npropose a novel information-based explanation technique that adapts SHAP, a\nsaliency-based explanation, to explain information value in decision making.",
    "categories": [
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.06152v3",
    "published_date": "2025-02-10 04:50:42 UTC",
    "updated_date": "2025-04-15 19:26:06 UTC"
  },
  {
    "arxiv_id": "2502.06151v1",
    "title": "Powerformer: A Transformer with Weighted Causal Attention for Time-series Forecasting",
    "authors": [
      "Kareem Hegazy",
      "Michael W. Mahoney",
      "N. Benjamin Erichson"
    ],
    "abstract": "Transformers have recently shown strong performance in time-series\nforecasting, but their all-to-all attention mechanism overlooks the (temporal)\ncausal and often (temporally) local nature of data. We introduce Powerformer, a\nnovel Transformer variant that replaces noncausal attention weights with causal\nweights that are reweighted according to a smooth heavy-tailed decay. This\nsimple yet effective modification endows the model with an inductive bias\nfavoring temporally local dependencies, while still allowing sufficient\nflexibility to learn the unique correlation structure of each dataset. Our\nempirical results demonstrate that Powerformer not only achieves\nstate-of-the-art accuracy on public time-series benchmarks, but also that it\noffers improved interpretability of attention patterns. Our analyses show that\nthe model's locality bias is amplified during training, demonstrating an\ninterplay between time-series data and power-law-based attention. These\nfindings highlight the importance of domain-specific modifications to the\nTransformer architecture for time-series forecasting, and they establish\nPowerformer as a strong, efficient, and principled baseline for future research\nand real-world applications.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.06151v1",
    "published_date": "2025-02-10 04:42:11 UTC",
    "updated_date": "2025-02-10 04:42:11 UTC"
  },
  {
    "arxiv_id": "2502.06146v2",
    "title": "Guided Exploration for Efficient Relational Model Learning",
    "authors": [
      "Annie Feng",
      "Nishanth Kumar",
      "Tomas Lozano-Perez",
      "Leslie Pack-Kaelbling"
    ],
    "abstract": "Efficient exploration is critical for learning relational models in\nlarge-scale environments with complex, long-horizon tasks. Random exploration\nmethods often collect redundant or irrelevant data, limiting their ability to\nlearn accurate relational models of the environment. Goal-literal babbling\n(GLIB) improves upon random exploration by setting and planning to novel goals,\nbut its reliance on random actions and random novel goal selection limits its\nscalability to larger domains. In this work, we identify the principles\nunderlying efficient exploration in relational domains: (1) operator\ninitialization with demonstrations that cover the distinct lifted effects\nnecessary for planning and (2) refining preconditions to collect maximally\ninformative transitions by selecting informative goal-action pairs and\nexecuting plans to them. To demonstrate these principles, we introduce\nBaking-Large, a challenging domain with extensive state-action spaces and\nlong-horizon tasks. We evaluate methods using oracle-driven demonstrations for\noperator initialization and precondition-targeting guidance to efficiently\ngather critical transitions. Experiments show that both the oracle\ndemonstrations and precondition-targeting oracle guidance significantly improve\nsample efficiency and generalization, paving the way for future methods to use\nthese principles to efficiently learn accurate relational models in complex\ndomains.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.06146v2",
    "published_date": "2025-02-10 04:23:01 UTC",
    "updated_date": "2025-05-10 04:07:03 UTC"
  },
  {
    "arxiv_id": "2502.07819v1",
    "title": "Enhancing kidney transplantation through multi-agent kidney exchange programs: A comprehensive review and optimization models",
    "authors": [
      "Shayan Sharifi"
    ],
    "abstract": "This paper presents a comprehensive review of the last two decades of\nresearch on Kidney Exchange Programs (KEPs), systematically categorizing and\nclassifying key contributions to provide readers with a structured\nunderstanding of advancements in the field. The review highlights the evolution\nof KEP methodologies and lays the foundation for our contribution. We propose\nthree mathematical models aimed at improving both the quantity and quality of\nkidney transplants. Model 1 maximizes the number of transplants by focusing on\ncompatibility based on blood type and PRA, without additional constraints.\nModel 2 introduces a minimum Human Leukocyte Antigen (HLA) compatibility\nthreshold to enhance transplant quality, though this leads to fewer matches.\nModel 3 extends the problem to a Multi-Agent Kidney Exchange Program (MKEP),\npooling incompatible donor-recipient pairs across multiple agents, resulting in\na higher number of successful transplants while ensuring fairness across\nagents. Sensitivity analyses demonstrate trade-offs between transplant quantity\nand quality, with Model 3 striking the optimal balance by leveraging\nmulti-agent collaboration to improve both the number and quality of\ntransplants. These findings underscore the potential benefits of more\nintegrated kidney exchange systems.",
    "categories": [
      "cs.AI",
      "math.OC"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.07819v1",
    "published_date": "2025-02-10 04:21:42 UTC",
    "updated_date": "2025-02-10 04:21:42 UTC"
  },
  {
    "arxiv_id": "2502.09640v1",
    "title": "Online Social Support Detection in Spanish Social Media Texts",
    "authors": [
      "Moein Shahiki Tash",
      "Luis Ramos",
      "Zahra Ahani",
      "Raul Monroy",
      "Olga kolesnikova",
      "Hiram Calvo",
      "Grigori Sidorov"
    ],
    "abstract": "The advent of social media has transformed communication, enabling\nindividuals to share their experiences, seek support, and participate in\ndiverse discussions. While extensive research has focused on identifying\nharmful content like hate speech, the recognition and promotion of positive and\nsupportive interactions remain largely unexplored. This study proposes an\ninnovative approach to detecting online social support in Spanish-language\nsocial media texts. We introduce the first annotated dataset specifically\ncreated for this task, comprising 3,189 YouTube comments classified as\nsupportive or non-supportive. To address data imbalance, we employed GPT-4o to\ngenerate paraphrased comments and create a balanced dataset. We then evaluated\nsocial support classification using traditional machine learning models, deep\nlearning architectures, and transformer-based models, including GPT-4o, but\nonly on the unbalanced dataset. Subsequently, we utilized a transformer model\nto compare the performance between the balanced and unbalanced datasets. Our\nfindings indicate that the balanced dataset yielded improved results for Task 2\n(Individual and Group) and Task 3 (Nation, Other, LGBTQ, Black Community,\nWomen, Religion), whereas GPT-4o performed best for Task 1 (Social Support and\nNon-Support). This study highlights the significance of fostering a supportive\nonline environment and lays the groundwork for future research in automated\nsocial support detection.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.09640v1",
    "published_date": "2025-02-10 04:04:23 UTC",
    "updated_date": "2025-02-10 04:04:23 UTC"
  },
  {
    "arxiv_id": "2502.06136v3",
    "title": "Graph Neural Networks at a Fraction",
    "authors": [
      "Rucha Bhalchandra Joshi",
      "Sagar Prakash Barad",
      "Nidhi Tiwari",
      "Subhankar Mishra"
    ],
    "abstract": "Graph Neural Networks (GNNs) have emerged as powerful tools for learning\nrepresentations of graph-structured data. In addition to real-valued GNNs,\nquaternion GNNs also perform well on tasks on graph-structured data. With the\naim of reducing the energy footprint, we reduce the model size while\nmaintaining accuracy comparable to that of the original-sized GNNs. This paper\nintroduces Quaternion Message Passing Neural Networks (QMPNNs), a framework\nthat leverages quaternion space to compute node representations. Our approach\noffers a generalizable method for incorporating quaternion representations into\nGNN architectures at one-fourth of the original parameter count. Furthermore,\nwe present a novel perspective on Graph Lottery Tickets, redefining their\napplicability within the context of GNNs and QMPNNs. We specifically aim to\nfind the initialization lottery from the subnetwork of the GNNs that can\nachieve comparable performance to the original GNN upon training. Thereby\nreducing the trainable model parameters even further. To validate the\neffectiveness of our proposed QMPNN framework and LTH for both GNNs and QMPNNs,\nwe evaluate their performance on real-world datasets across three fundamental\ngraph-based tasks: node classification, link prediction, and graph\nclassification.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "12 pages, 2 figures, accepted at PAKDD 2025",
    "pdf_url": "http://arxiv.org/pdf/2502.06136v3",
    "published_date": "2025-02-10 03:55:09 UTC",
    "updated_date": "2025-02-28 06:26:53 UTC"
  },
  {
    "arxiv_id": "2502.06910v2",
    "title": "TimeKAN: KAN-based Frequency Decomposition Learning Architecture for Long-term Time Series Forecasting",
    "authors": [
      "Songtao Huang",
      "Zhen Zhao",
      "Can Li",
      "Lei Bai"
    ],
    "abstract": "Real-world time series often have multiple frequency components that are\nintertwined with each other, making accurate time series forecasting\nchallenging. Decomposing the mixed frequency components into multiple single\nfrequency components is a natural choice. However, the information density of\npatterns varies across different frequencies, and employing a uniform modeling\napproach for different frequency components can lead to inaccurate\ncharacterization. To address this challenges, inspired by the flexibility of\nthe recent Kolmogorov-Arnold Network (KAN), we propose a KAN-based Frequency\nDecomposition Learning architecture (TimeKAN) to address the complex\nforecasting challenges caused by multiple frequency mixtures. Specifically,\nTimeKAN mainly consists of three components: Cascaded Frequency Decomposition\n(CFD) blocks, Multi-order KAN Representation Learning (M-KAN) blocks and\nFrequency Mixing blocks. CFD blocks adopt a bottom-up cascading approach to\nobtain series representations for each frequency band. Benefiting from the high\nflexibility of KAN, we design a novel M-KAN block to learn and represent\nspecific temporal patterns within each frequency band. Finally, Frequency\nMixing blocks is used to recombine the frequency bands into the original\nformat. Extensive experimental results across multiple real-world time series\ndatasets demonstrate that TimeKAN achieves state-of-the-art performance as an\nextremely lightweight architecture. Code is available at\nhttps://github.com/huangst21/TimeKAN.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.06910v2",
    "published_date": "2025-02-10 03:51:26 UTC",
    "updated_date": "2025-02-26 09:04:40 UTC"
  },
  {
    "arxiv_id": "2502.06134v1",
    "title": "Integrating Sequence and Image Modeling in Irregular Medical Time Series Through Self-Supervised Learning",
    "authors": [
      "Liuqing Chen",
      "Shuhong Xiao",
      "Shixian Ding",
      "Shanhai Hu",
      "Lingyun Sun"
    ],
    "abstract": "Medical time series are often irregular and face significant missingness,\nposing challenges for data analysis and clinical decision-making. Existing\nmethods typically adopt a single modeling perspective, either treating series\ndata as sequences or transforming them into image representations for further\nclassification. In this paper, we propose a joint learning framework that\nincorporates both sequence and image representations. We also design three\nself-supervised learning strategies to facilitate the fusion of sequence and\nimage representations, capturing a more generalizable joint representation. The\nresults indicate that our approach outperforms seven other state-of-the-art\nmodels in three representative real-world clinical datasets. We further\nvalidate our approach by simulating two major types of real-world missingness\nthrough leave-sensors-out and leave-samples-out techniques. The results\ndemonstrate that our approach is more robust and significantly surpasses other\nbaselines in terms of classification performance.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "9 pages, 2 figures, AAAI2025",
    "pdf_url": "http://arxiv.org/pdf/2502.06134v1",
    "published_date": "2025-02-10 03:49:41 UTC",
    "updated_date": "2025-02-10 03:49:41 UTC"
  },
  {
    "arxiv_id": "2502.06909v2",
    "title": "Meta-Computing Enhanced Federated Learning in IIoT: Satisfaction-Aware Incentive Scheme via DRL-Based Stackelberg Game",
    "authors": [
      "Xiaohuan Li",
      "Shaowen Qin",
      "Xin Tang",
      "Jiawen Kang",
      "Jin Ye",
      "Zhonghua Zhao",
      "Yusi Zheng",
      "Dusit Niyato"
    ],
    "abstract": "The Industrial Internet of Things (IIoT) leverages Federated Learning (FL)\nfor distributed model training while preserving data privacy, and\nmeta-computing enhances FL by optimizing and integrating distributed computing\nresources, improving efficiency and scalability. Efficient IIoT operations\nrequire a trade-off between model quality and training latency. Consequently, a\nprimary challenge of FL in IIoT is to optimize overall system performance by\nbalancing model quality and training latency. This paper designs a satisfaction\nfunction that accounts for data size, Age of Information (AoI), and training\nlatency for meta-computing. Additionally, the satisfaction function is\nincorporated into the utility functions to incentivize nodes in IIoT\nparticipation in model training. We model the utility functions of servers and\nnodes as a two-stage Stackelberg game and employ a deep reinforcement learning\napproach to learn the Stackelberg equilibrium. This approach ensures balanced\nrewards and enhances the applicability of the incentive scheme for IIoT.\nSimulation results demonstrate that, under the same budget constraints, the\nproposed incentive scheme improves utility by at least 23.7% compared to\nexisting FL schemes without compromising model accuracy.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.GT"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.06909v2",
    "published_date": "2025-02-10 03:33:36 UTC",
    "updated_date": "2025-04-20 13:34:52 UTC"
  },
  {
    "arxiv_id": "2502.06127v1",
    "title": "Improved YOLOv5s model for key components detection of power transmission lines",
    "authors": [
      "Chen Chen",
      "Guowu Yuan",
      "Hao Zhou",
      "Yi Ma"
    ],
    "abstract": "High-voltage transmission lines are located far from the road, resulting in\ninconvenient inspection work and rising maintenance costs. Intelligent\ninspection of power transmission lines has become increasingly important.\nHowever, subsequent intelligent inspection relies on accurately detecting\nvarious key components. Due to the low detection accuracy of key components in\ntransmission line image inspection, this paper proposed an improved object\ndetection model based on the YOLOv5s (You Only Look Once Version 5 Small) model\nto improve the detection accuracy of key components of transmission lines.\nAccording to the characteristics of the power grid inspection image, we first\nmodify the distance measurement in the k-means clustering to improve the anchor\nmatching of the YOLOv5s model. Then, we add the convolutional block attention\nmodule (CBAM) attention mechanism to the backbone network to improve accuracy.\nFinally, we apply the focal loss function to reduce the impact of class\nimbalance. Our improved method's mAP (mean average precision) reached 98.1%,\nthe precision reached 97.5%, the recall reached 94.4%, and the detection rate\nreached 84.8 FPS (frames per second). The experimental results show that our\nimproved model improves detection accuracy and has performance advantages over\nother models.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "23 pages, 14 figures",
    "pdf_url": "http://arxiv.org/pdf/2502.06127v1",
    "published_date": "2025-02-10 03:29:34 UTC",
    "updated_date": "2025-02-10 03:29:34 UTC"
  },
  {
    "arxiv_id": "2502.06124v3",
    "title": "Foundation Model of Electronic Medical Records for Adaptive Risk Estimation",
    "authors": [
      "Pawel Renc",
      "Michal K. Grzeszczyk",
      "Nassim Oufattole",
      "Deirdre Goode",
      "Yugang Jia",
      "Szymon Bieganski",
      "Matthew B. A. McDermott",
      "Jaroslaw Was",
      "Anthony E. Samir",
      "Jonathan W. Cunningham",
      "David W. Bates",
      "Arkadiusz Sitek"
    ],
    "abstract": "The U.S. allocates nearly 18% of its GDP to healthcare but experiences lower\nlife expectancy and higher preventable death rates compared to other\nhigh-income nations. Hospitals struggle to predict critical outcomes such as\nmortality, ICU admission, and prolonged hospital stays. Traditional early\nwarning systems, like NEWS and MEWS, rely on static variables and fixed\nthresholds, limiting their adaptability, accuracy, and personalization. We\ndeveloped the Enhanced Transformer for Health Outcome Simulation (ETHOS), an AI\nmodel that tokenizes patient health timelines (PHTs) from EHRs and uses\ntransformer-based architectures to predict future PHTs. The Adaptive Risk\nEstimation System (ARES) leverages ETHOS to compute dynamic, personalized risk\nprobabilities for clinician-defined critical events. ARES also features a\npersonalized explainability module highlighting key clinical factors\ninfluencing risk estimates. We evaluated ARES on the MIMIC-IV v2.2 dataset in\nemergency department settings, benchmarking its performance against traditional\nearly warning systems and machine learning models. From 299,721 unique\npatients, 285,622 PHTs (60% with hospital admissions) were processed,\ncomprising over 357 million tokens. ETHOS outperformed benchmark models in\npredicting hospital admissions, ICU admissions, and prolonged stays, achieving\nsuperior AUC scores. Its risk estimates were robust across demographic\nsubgroups, with calibration curves confirming model reliability. The\nexplainability module provided valuable insights into patient-specific risk\nfactors. ARES, powered by ETHOS, advances predictive healthcare AI by\ndelivering dynamic, real-time, personalized risk estimation with\npatient-specific explainability. Its adaptability and accuracy offer a\ntransformative tool for clinical decision-making, potentially improving patient\noutcomes and resource allocation.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "Fix affiliation list",
    "pdf_url": "http://arxiv.org/pdf/2502.06124v3",
    "published_date": "2025-02-10 03:22:39 UTC",
    "updated_date": "2025-03-13 22:37:55 UTC"
  },
  {
    "arxiv_id": "2502.06117v1",
    "title": "Revisiting Dynamic Graph Clustering via Matrix Factorization",
    "authors": [
      "Dongyuan Li",
      "Satoshi Kosugi",
      "Ying Zhang",
      "Manabu Okumura",
      "Feng Xia",
      "Renhe Jiang"
    ],
    "abstract": "Dynamic graph clustering aims to detect and track time-varying clusters in\ndynamic graphs, revealing the evolutionary mechanisms of complex real-world\ndynamic systems. Matrix factorization-based methods are promising approaches\nfor this task; however, these methods often struggle with scalability and can\nbe time-consuming when applied to large-scale dynamic graphs. Moreover, they\ntend to lack robustness and are vulnerable to real-world noisy data. To address\nthese issues, we make three key contributions. First, to improve scalability,\nwe propose temporal separated matrix factorization, where a single matrix is\ndivided into multiple smaller matrices for independent factorization, resulting\nin faster computation. Second, to improve robustness, we introduce\nbi-clustering regularization, which jointly optimizes graph embedding and\nclustering, thereby filtering out noisy features from the graph embeddings.\nThird, to further enhance effectiveness and efficiency, we propose selective\nembedding updating, where we update only the embeddings of dynamic nodes while\nthe embeddings of static nodes are fixed among different timestamps.\nExperimental results on six synthetic and five real-world benchmarks\ndemonstrate the scalability, robustness and effectiveness of our proposed\nmethod. Source code is available at https://github.com/Clearloveyuan/DyG-MF.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted by TheWebConf 2025 (Oral)",
    "pdf_url": "http://arxiv.org/pdf/2502.06117v1",
    "published_date": "2025-02-10 02:57:46 UTC",
    "updated_date": "2025-02-10 02:57:46 UTC"
  },
  {
    "arxiv_id": "2502.06111v2",
    "title": "CSR-Bench: Benchmarking LLM Agents in Deployment of Computer Science Research Repositories",
    "authors": [
      "Yijia Xiao",
      "Runhui Wang",
      "Luyang Kong",
      "Davor Golac",
      "Wei Wang"
    ],
    "abstract": "The increasing complexity of computer science research projects demands more\neffective tools for deploying code repositories. Large Language Models (LLMs),\nsuch as Anthropic Claude and Meta Llama, have demonstrated significant\nadvancements across various fields of computer science research, including the\nautomation of diverse software engineering tasks. To evaluate the effectiveness\nof LLMs in handling complex code development tasks of research projects,\nparticularly for NLP/CV/AI/ML/DM topics, we introduce CSR-Bench, a benchmark\nfor Computer Science Research projects. This benchmark assesses LLMs from\nvarious aspects including accuracy, efficiency, and deployment script quality,\naiming to explore their potential in conducting computer science research\nautonomously. We also introduce a novel framework, CSR-Agents, that utilizes\nmultiple LLM agents to automate the deployment of GitHub code repositories of\ncomputer science research projects. Specifically, by checking instructions from\nmarkdown files and interpreting repository structures, the model generates and\niteratively improves bash commands that set up the experimental environments\nand deploy the code to conduct research tasks. Preliminary results from\nCSR-Bench indicate that LLM agents can significantly enhance the workflow of\nrepository deployment, thereby boosting developer productivity and improving\nthe management of developmental workflows.",
    "categories": [
      "cs.SE",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.SE",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.06111v2",
    "published_date": "2025-02-10 02:46:29 UTC",
    "updated_date": "2025-02-11 20:25:11 UTC"
  },
  {
    "arxiv_id": "2502.06907v1",
    "title": "Can ChatGPT Diagnose Alzheimer's Disease?",
    "authors": [
      "Quoc-Toan Nguyen",
      "Linh Le",
      "Xuan-The Tran",
      "Thomas Do",
      "Chin-Teng Lin"
    ],
    "abstract": "Can ChatGPT diagnose Alzheimer's Disease (AD)? AD is a devastating\nneurodegenerative condition that affects approximately 1 in 9 individuals aged\n65 and older, profoundly impairing memory and cognitive function. This paper\nutilises 9300 electronic health records (EHRs) with data from Magnetic\nResonance Imaging (MRI) and cognitive tests to address an intriguing question:\nAs a general-purpose task solver, can ChatGPT accurately detect AD using EHRs?\nWe present an in-depth evaluation of ChatGPT using a black-box approach with\nzero-shot and multi-shot methods. This study unlocks ChatGPT's capability to\nanalyse MRI and cognitive test results, as well as its potential as a\ndiagnostic tool for AD. By automating aspects of the diagnostic process, this\nresearch opens a transformative approach for the healthcare system,\nparticularly in addressing disparities in resource-limited regions where AD\nspecialists are scarce. Hence, it offers a foundation for a promising method\nfor early detection, supporting individuals with timely interventions, which is\nparamount for Quality of Life (QoL).",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "14 pages, 5 figures, 5 tables",
    "pdf_url": "http://arxiv.org/pdf/2502.06907v1",
    "published_date": "2025-02-10 02:41:08 UTC",
    "updated_date": "2025-02-10 02:41:08 UTC"
  },
  {
    "arxiv_id": "2502.06106v1",
    "title": "Circuit-tuning: A Mechanistic Approach for Identifying Parameter Redundancy and Fine-tuning Neural Networks",
    "authors": [
      "Yueyan Li",
      "Caixia Yuan",
      "Xiaojie Wang"
    ],
    "abstract": "The study of mechanistic interpretability aims to reverse-engineer a model to\nexplain its behaviors. While recent studies have focused on the static\nmechanism of a certain behavior, the training dynamics inside a model remain to\nbe explored. In this work, we develop an interpretable method for fine-tuning\nand reveal the mechanism behind learning. We first propose the concept of node\nredundancy as an extension of intrinsic dimension and explain the idea behind\ncircuit discovery from a fresh view. Based on the theory, we propose\ncircuit-tuning, a two-stage algorithm that iteratively performs circuit\ndiscovery to mask out irrelevant edges and updates the remaining parameters\nresponsible for a specific task. Experiments show that our method not only\nimproves performance on a wide range of tasks but is also scalable while\npreserving general capabilities. We visualize and analyze the circuits before,\nduring, and after fine-tuning, providing new insights into the\nself-organization mechanism of a neural network in the learning process.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.06106v1",
    "published_date": "2025-02-10 02:35:53 UTC",
    "updated_date": "2025-02-10 02:35:53 UTC"
  },
  {
    "arxiv_id": "2502.06105v1",
    "title": "Comprehensive Framework for Evaluating Conversational AI Chatbots",
    "authors": [
      "Shailja Gupta",
      "Rajesh Ranjan",
      "Surya Narayan Singh"
    ],
    "abstract": "Conversational AI chatbots are transforming industries by streamlining\ncustomer service, automating transactions, and enhancing user engagement.\nHowever, evaluating these systems remains a challenge, particularly in\nfinancial services, where compliance, user trust, and operational efficiency\nare critical. This paper introduces a novel evaluation framework that\nsystematically assesses chatbots across four dimensions: cognitive and\nconversational intelligence, user experience, operational efficiency, and\nethical and regulatory compliance. By integrating advanced AI methodologies\nwith financial regulations, the framework bridges theoretical foundations and\nreal-world deployment challenges. Additionally, we outline future research\ndirections, emphasizing improvements in conversational coherence, real-time\nadaptability, and fairness.",
    "categories": [
      "cs.CY",
      "cs.AI"
    ],
    "primary_category": "cs.CY",
    "comment": "2 Figures",
    "pdf_url": "http://arxiv.org/pdf/2502.06105v1",
    "published_date": "2025-02-10 02:27:34 UTC",
    "updated_date": "2025-02-10 02:27:34 UTC"
  },
  {
    "arxiv_id": "2502.18484v1",
    "title": "AI Enhanced Ontology Driven NLP for Intelligent Cloud Resource Query Processing Using Knowledge Graphs",
    "authors": [
      "Krishna Chaitanya Sunkara",
      "Krishnaiah Narukulla"
    ],
    "abstract": "The conventional resource search in cloud infrastructure relies on\nkeyword-based searches or GUIDs, which demand exact matches and significant\nuser effort to locate resources. These conventional search approaches often\nfail to interpret the intent behind natural language queries, making resource\ndiscovery inefficient and inaccessible to users. Though there exists some form\nof NLP based search engines, they are limited and focused more on analyzing the\nNLP query itself and extracting identifiers to find the resources. But they\nfail to search resources based on their behavior or operations or their\ncapabilities or relationships or features or business relevance or the dynamic\nchanging state or the knowledge these resources have. The search criteria has\nbeen changing with the inundation of AI based services which involved\ndiscovering not just the requested resources and identifiers but seeking\ninsights. The real intent of a search has never been to just to list the\nresources but with some actual context such as to understand causes of some\nbehavior in the system, compliance checks, capacity estimations, network\nconstraints, or troubleshooting or business insights. This paper proposes an\nadvanced Natural Language Processing (NLP) enhanced by ontology-based semantics\nto enable intuitive, human-readable queries which allows users to actually\ndiscover the intent-of-search itself. By constructing an ontology of cloud\nresources, their interactions, and behaviors, the proposed framework enables\ndynamic intent extraction and relevance ranking using Latent Semantic Indexing\n(LSI) and AI models. It introduces an automated pipeline which integrates\nontology extraction by AI powered data crawlers, building a semantic knowledge\nbase for context aware resource discovery.",
    "categories": [
      "cs.IR",
      "cs.AI",
      "H.3.3"
    ],
    "primary_category": "cs.IR",
    "comment": "8 pages, 5 figures, 4 tables. This paper not published at else where\n  yet. The experimental setup has a potential to be revised using real time\n  resources. Authors: Krishna Chaitanya Sunkara (IEEE Senior Member, Raleigh,\n  NC, USA, Independent Researcher), Krishnaiah Narukulla (IEEE Senior Member,\n  San Jose, CA, USA, Independent Researcher)",
    "pdf_url": "http://arxiv.org/pdf/2502.18484v1",
    "published_date": "2025-02-10 02:15:13 UTC",
    "updated_date": "2025-02-10 02:15:13 UTC"
  },
  {
    "arxiv_id": "2502.06097v2",
    "title": "NLGR: Utilizing Neighbor Lists for Generative Rerank in Personalized Recommendation Systems",
    "authors": [
      "Shuli Wang",
      "Xue Wei",
      "Senjie Kou",
      "Chi Wang",
      "Wenshuai Chen",
      "Qi Tang",
      "Yinhua Zhu",
      "Xiong Xiao",
      "Xingxing Wang"
    ],
    "abstract": "Reranking plays a crucial role in modern multi-stage recommender systems by\nrearranging the initial ranking list. Due to the inherent challenges of\ncombinatorial search spaces, some current research adopts an\nevaluator-generator paradigm, with a generator generating feasible sequences\nand an evaluator selecting the best sequence based on the estimated list\nutility. However, these methods still face two issues. Firstly, due to the goal\ninconsistency problem between the evaluator and generator, the generator tends\nto fit the local optimal solution of exposure distribution rather than\ncombinatorial space optimization. Secondly, the strategy of generating target\nitems one by one is difficult to achieve optimality because it ignores the\ninformation of subsequent items.\n  To address these issues, we propose a utilizing Neighbor Lists model for\nGenerative Reranking (NLGR), which aims to improve the performance of the\ngenerator in the combinatorial space. NLGR follows the evaluator-generator\nparadigm and improves the generator's training and generating methods.\nSpecifically, we use neighbor lists in combination space to enhance the\ntraining process, making the generator perceive the relative scores and find\nthe optimization direction. Furthermore, we propose a novel sampling-based\nnon-autoregressive generation method, which allows the generator to jump\nflexibly from the current list to any neighbor list. Extensive experiments on\npublic and industrial datasets validate NLGR's effectiveness and we have\nsuccessfully deployed NLGR on the Meituan food delivery platform.",
    "categories": [
      "cs.IR",
      "cs.AI"
    ],
    "primary_category": "cs.IR",
    "comment": "Accepted by WWW 2025 Industry Track",
    "pdf_url": "http://arxiv.org/pdf/2502.06097v2",
    "published_date": "2025-02-10 02:06:17 UTC",
    "updated_date": "2025-02-11 14:44:47 UTC"
  },
  {
    "arxiv_id": "2502.06096v2",
    "title": "Post-detection inference for sequential changepoint localization",
    "authors": [
      "Aytijhya Saha",
      "Aaditya Ramdas"
    ],
    "abstract": "This paper addresses a fundamental but largely unexplored challenge in\nsequential changepoint analysis: conducting inference following a detected\nchange. We study the problem of localizing the changepoint using only the data\nobserved up to a data-dependent stopping time at which a sequential detection\nalgorithm $\\mathcal A$ declares a change. We first construct confidence sets\nfor the unknown changepoint when pre- and post-change distributions are assumed\nto be known. We then extend our framework to composite pre- and post-change\nscenarios. We impose no conditions on the observation space or on $\\mathcal A$\n-- we only need to be able to run $\\mathcal A$ on simulated data sequences. In\nsummary, this work offers both theoretically sound and practically effective\ntools for sequential changepoint localization.",
    "categories": [
      "stat.ML",
      "cs.AI",
      "cs.LG",
      "stat.ME"
    ],
    "primary_category": "stat.ML",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.06096v2",
    "published_date": "2025-02-10 02:01:30 UTC",
    "updated_date": "2025-03-10 13:20:58 UTC"
  },
  {
    "arxiv_id": "2502.06095v1",
    "title": "Rateless Joint Source-Channel Coding, and a Blueprint for 6G Semantic Communications System Design",
    "authors": [
      "Saeed R. Khosravirad"
    ],
    "abstract": "This paper introduces rateless joint source-channel coding (rateless JSCC).\nThe code is rateless in that it is designed and optimized for a continuum of\ncoding rates such that it achieves a desired distortion for any rate in that\ncontinuum. We further introduce rate-adaptive and stable communication link\noperation to accommodate rateless JSCCs. The link operation resembles a ``bit\npipe'' that is identified by its rate in bits per frame, and, by the rate of\nbits that are flipped in each frame. Thus, the link operation is rate-adaptive\nsuch that it punctures the rateless JSCC codeword to adapt its length (and\ncoding rate) to the underlying channel capacity, and is stable in maintaining\nthe bit flipping ratio across time frames.\n  Next, a new family of autoencoder rateless JSCC codes are introduced. The\ncode family is dubbed RLACS code (read as relax code, standing for ratelss and\nlossy autoencoder channel and source code). The code is tested for\nreconstruction loss of image signals and demonstrates powerful performance that\nis resilient to variation of channel quality. RLACS code is readily applicable\nto the case of semantic distortion suited to variety of semantic and\neffectiveness communications use cases.\n  In the second part of the paper, we dive into the practical concerns around\nsemantic communication and provide a blueprint for semantic networking system\ndesign relying on updating the existing network systems with some essential\nmodifications. We further outline a comprehensive list of open research\nproblems and development challenges towards a practical 6G communications\nsystem design that enables semantic networking.",
    "categories": [
      "cs.IT",
      "cs.AI",
      "math.IT"
    ],
    "primary_category": "cs.IT",
    "comment": "39 pages, 9 figures, 2 tables",
    "pdf_url": "http://arxiv.org/pdf/2502.06095v1",
    "published_date": "2025-02-10 01:49:16 UTC",
    "updated_date": "2025-02-10 01:49:16 UTC"
  },
  {
    "arxiv_id": "2502.06906v1",
    "title": "Learning-based estimation of cattle weight gain and its influencing factors",
    "authors": [
      "Muhammad Riaz Hasib Hossain",
      "Rafiqul Islam",
      "Shawn R. McGrath",
      "Md Zahidul Islam",
      "David Lamb"
    ],
    "abstract": "Many cattle farmers still depend on manual methods to measure the live weight\ngain of cattle at set intervals, which is time consuming, labour intensive, and\nstressful for both the animals and handlers. A remote and autonomous monitoring\nsystem using machine learning (ML) or deep learning (DL) can provide a more\nefficient and less invasive method and also predictive capabilities for future\ncattle weight gain (CWG). This system allows continuous monitoring and\nestimation of individual cattle live weight gain, growth rates and weight\nfluctuations considering various factors like environmental conditions, genetic\npredispositions, feed availability, movement patterns and behaviour. Several\nresearchers have explored the efficiency of estimating CWG using ML and DL\nalgorithms. However, estimating CWG suffers from a lack of consistency in its\napplication. Moreover, ML or DL can provide weight gain estimations based on\nseveral features that vary in existing research. Additionally, previous studies\nhave encountered various data related challenges when estimating CWG. This\npaper presents a comprehensive investigation in estimating CWG using advanced\nML techniques based on research articles (between 2004 and 2024). This study\ninvestigates the current tools, methods, and features used in CWG estimation,\nas well as their strengths and weaknesses. The findings highlight the\nsignificance of using advanced ML approaches in CWG estimation and its critical\ninfluence on factors. Furthermore, this study identifies potential research\ngaps and provides research direction on CWG prediction, which serves as a\nreference for future research in this area.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.06906v1",
    "published_date": "2025-02-10 01:45:57 UTC",
    "updated_date": "2025-02-10 01:45:57 UTC"
  },
  {
    "arxiv_id": "2502.06905v1",
    "title": "Lightweight Dataset Pruning without Full Training via Example Difficulty and Prediction Uncertainty",
    "authors": [
      "Yeseul Cho",
      "Baekrok Shin",
      "Changmin Kang",
      "Chulhee Yun"
    ],
    "abstract": "Recent advances in deep learning rely heavily on massive datasets, leading to\nsubstantial storage and training costs. Dataset pruning aims to alleviate this\ndemand by discarding redundant examples. However, many existing methods require\ntraining a model with a full dataset over a large number of epochs before being\nable to prune the dataset, which ironically makes the pruning process more\nexpensive than just training the model on the entire dataset. To overcome this\nlimitation, we introduce a Difficulty and Uncertainty-Aware Lightweight (DUAL)\nscore, which aims to identify important samples from the early training stage\nby considering both example difficulty and prediction uncertainty. To address a\ncatastrophic accuracy drop at an extreme pruning, we further propose a\nratio-adaptive sampling using Beta distribution. Experiments on various\ndatasets and learning scenarios such as image classification with label noise\nand image corruption, and model architecture generalization demonstrate the\nsuperiority of our method over previous state-of-the-art (SOTA) approaches.\nSpecifically, on ImageNet-1k, our method reduces the time cost for pruning to\n66% compared to previous methods while achieving a SOTA, specifically 60% test\naccuracy at a 90% pruning ratio. On CIFAR datasets, the time cost is reduced to\njust 15% while maintaining SOTA performance.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.06905v1",
    "published_date": "2025-02-10 01:18:40 UTC",
    "updated_date": "2025-02-10 01:18:40 UTC"
  },
  {
    "arxiv_id": "2502.06084v1",
    "title": "Physics-Guided Foundation Model for Scientific Discovery: An Application to Aquatic Science",
    "authors": [
      "Runlong Yu",
      "Chonghao Qiu",
      "Robert Ladwig",
      "Paul Hanson",
      "Yiqun Xie",
      "Xiaowei Jia"
    ],
    "abstract": "Physics-guided machine learning (PGML) has become a prevalent approach in\nstudying scientific systems due to its ability to integrate scientific theories\nfor enhancing machine learning (ML) models. However, most PGML approaches are\ntailored to isolated and relatively simple tasks, which limits their\napplicability to complex systems involving multiple interacting processes and\nnumerous influencing features. In this paper, we propose a\n\\textit{\\textbf{P}hysics-\\textbf{G}uided \\textbf{F}oundation \\textbf{M}odel\n(\\textbf{PGFM})} that combines pre-trained ML models and physics-based models\nand leverages their complementary strengths to improve the modeling of multiple\ncoupled processes. To effectively conduct pre-training, we construct a\nsimulated environmental system that encompasses a wide range of influencing\nfeatures and various simulated variables generated by physics-based models. The\nmodel is pre-trained in this system to adaptively select important feature\ninteractions guided by multi-task objectives. We then fine-tune the model for\neach specific task using true observations, while maintaining consistency with\nestablished physical theories, such as the principles of mass and energy\nconservation. We demonstrate the effectiveness of this methodology in modeling\nwater temperature and dissolved oxygen dynamics in real-world lakes. The\nproposed PGFM is also broadly applicable to a range of scientific fields where\nphysics-based models are being used.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.NE"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.06084v1",
    "published_date": "2025-02-10 00:48:10 UTC",
    "updated_date": "2025-02-10 00:48:10 UTC"
  }
]
[
  {
    "arxiv_id": "2412.16406v2",
    "title": "Learning Disease Progression Models That Capture Health Disparities",
    "authors": [
      "Erica Chiang",
      "Divya Shanmugam",
      "Ashley N. Beecy",
      "Gabriel Sayer",
      "Deborah Estrin",
      "Nikhil Garg",
      "Emma Pierson"
    ],
    "abstract": "Disease progression models are widely used to inform the diagnosis and\ntreatment of many progressive diseases. However, a significant limitation of\nexisting models is that they do not account for health disparities that can\nbias the observed data. To address this, we develop an interpretable Bayesian\ndisease progression model that captures three key health disparities: certain\npatient populations may (1) start receiving care only when their disease is\nmore severe, (2) experience faster disease progression even while receiving\ncare, or (3) receive follow-up care less frequently conditional on disease\nseverity. We show theoretically and empirically that failing to account for any\nof these disparities can result in biased estimates of severity (e.g.,\nunderestimating severity for disadvantaged groups). On a dataset of heart\nfailure patients, we show that our model can identify groups that face each\ntype of health disparity, and that accounting for these disparities while\ninferring disease severity meaningfully shifts which patients are considered\nhigh-risk.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CY",
      "stat.AP",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.16406v2",
    "published_date": "2024-12-20 23:56:37 UTC",
    "updated_date": "2025-04-29 20:31:15 UTC"
  },
  {
    "arxiv_id": "2412.16395v1",
    "title": "Autonomous Option Invention for Continual Hierarchical Reinforcement Learning and Planning",
    "authors": [
      "Rashmeet Kaur Nayyar",
      "Siddharth Srivastava"
    ],
    "abstract": "Abstraction is key to scaling up reinforcement learning (RL). However,\nautonomously learning abstract state and action representations to enable\ntransfer and generalization remains a challenging open problem. This paper\npresents a novel approach for inventing, representing, and utilizing options,\nwhich represent temporally extended behaviors, in continual RL settings. Our\napproach addresses streams of stochastic problems characterized by long\nhorizons, sparse rewards, and unknown transition and reward functions.\n  Our approach continually learns and maintains an interpretable state\nabstraction, and uses it to invent high-level options with abstract symbolic\nrepresentations. These options meet three key desiderata: (1) composability for\nsolving tasks effectively with lookahead planning, (2) reusability across\nproblem instances for minimizing the need for relearning, and (3) mutual\nindependence for reducing interference among options. Our main contributions\nare approaches for continually learning transferable, generalizable options\nwith symbolic representations, and for integrating search techniques with RL to\nefficiently plan over these learned options to solve new problems. Empirical\nresults demonstrate that the resulting approach effectively learns and\ntransfers abstract knowledge across problem instances, achieving superior\nsample efficiency compared to state-of-the-art methods.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.16395v1",
    "published_date": "2024-12-20 23:04:52 UTC",
    "updated_date": "2024-12-20 23:04:52 UTC"
  },
  {
    "arxiv_id": "2412.16389v1",
    "title": "Ethics and Technical Aspects of Generative AI Models in Digital Content Creation",
    "authors": [
      "Atahan Karagoz"
    ],
    "abstract": "Generative AI models like GPT-4o and DALL-E 3 are reshaping digital content\ncreation, offering industries tools to generate diverse and sophisticated text\nand images with remarkable creativity and efficiency. This paper examines both\nthe capabilities and challenges of these models within creative workflows.\nWhile they deliver high performance in generating content with creativity,\ndiversity, and technical precision, they also raise significant ethical\nconcerns. Our study addresses two key research questions: (a) how these models\nperform in terms of creativity, diversity, accuracy, and computational\nefficiency, and (b) the ethical risks they present, particularly concerning\nbias, authenticity, and potential misuse. Through a structured series of\nexperiments, we analyze their technical performance and assess the ethical\nimplications of their outputs, revealing that although generative models\nenhance creative processes, they often reflect biases from their training data\nand carry ethical vulnerabilities that require careful oversight. This research\nproposes ethical guidelines to support responsible AI integration into industry\npractices, fostering a balance between innovation and ethical integrity.",
    "categories": [
      "cs.AI",
      "cs.CY",
      "cs.HC",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.16389v1",
    "published_date": "2024-12-20 22:53:29 UTC",
    "updated_date": "2024-12-20 22:53:29 UTC"
  },
  {
    "arxiv_id": "2412.16385v1",
    "title": "Collision-based Dynamics for Multi-Marginal Optimal Transport",
    "authors": [
      "Mohsen Sadr",
      "Hossein Gorji"
    ],
    "abstract": "Inspired by the Boltzmann kinetics, we propose a collision-based dynamics\nwith a Monte Carlo solution algorithm that approximates the solution of the\nmulti-marginal optimal transport problem via randomized pairwise swapping of\nsample indices. The computational complexity and memory usage of the proposed\nmethod scale linearly with the number of samples, making it highly attractive\nfor high-dimensional settings. In several examples, we demonstrate the\nefficiency of the proposed method compared to the state-of-the-art methods.",
    "categories": [
      "cs.AI",
      "cs.LG",
      "stat.CO"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.16385v1",
    "published_date": "2024-12-20 22:41:16 UTC",
    "updated_date": "2024-12-20 22:41:16 UTC"
  },
  {
    "arxiv_id": "2412.16381v1",
    "title": "VerSe: Integrating Multiple Queries as Prompts for Versatile Cardiac MRI Segmentation",
    "authors": [
      "Bangwei Guo",
      "Meng Ye",
      "Yunhe Gao",
      "Bingyu Xin",
      "Leon Axel",
      "Dimitris Metaxas"
    ],
    "abstract": "Despite the advances in learning-based image segmentation approach, the\naccurate segmentation of cardiac structures from magnetic resonance imaging\n(MRI) remains a critical challenge. While existing automatic segmentation\nmethods have shown promise, they still require extensive manual corrections of\nthe segmentation results by human experts, particularly in complex regions such\nas the basal and apical parts of the heart. Recent efforts have been made on\ndeveloping interactive image segmentation methods that enable human-in-the-loop\nlearning. However, they are semi-automatic and inefficient, due to their\nreliance on click-based prompts, especially for 3D cardiac MRI volumes. To\naddress these limitations, we propose VerSe, a Versatile Segmentation framework\nto unify automatic and interactive segmentation through mutiple queries. Our\nkey innovation lies in the joint learning of object and click queries as\nprompts for a shared segmentation backbone. VerSe supports both fully automatic\nsegmentation, through object queries, and interactive mask refinement, by\nproviding click queries when needed. With the proposed integrated prompting\nscheme, VerSe demonstrates significant improvement in performance and\nefficiency over existing methods, on both cardiac MRI and out-of-distribution\nmedical imaging datasets. The code is available at\nhttps://github.com/bangwayne/Verse.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.HC"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.16381v1",
    "published_date": "2024-12-20 22:35:47 UTC",
    "updated_date": "2024-12-20 22:35:47 UTC"
  },
  {
    "arxiv_id": "2412.16378v3",
    "title": "REFA: Reference Free Alignment for multi-preference optimization",
    "authors": [
      "Taneesh Gupta",
      "Rahul Madhavan",
      "Xuchao Zhang",
      "Chetan Bansal",
      "Saravan Rajmohan"
    ],
    "abstract": "We introduce $\\textbf{REFA}$, a family of reference-free alignment methods\nthat optimize over multiple user preferences while enforcing fine-grained\nlength control. Our approach integrates deviation-based weighting to emphasize\nhigh-quality responses, length normalization to prevent trivial short-response\nsolutions, and an EOS-probability regularizer to mitigate dataset-induced\nbrevity biases. Theoretically, we show that under the Uncertainty Reduction\nwith Sequence Length Assertion (URSLA) framework, naive length normalization\ncan still incentivize length-based shortcuts. In contrast, REFA corrects these\nsubtle incentives, guiding models toward genuinely more informative and\nhigher-quality outputs. Empirically, REFA achieves a new\n$\\textbf{state-of-the-art}$ among reference-free alignment methods, generating\nricher responses that align more closely with human preferences. Notably, REFA\nimproves performance on the AlpacaEval2 benchmark, achieving a $\\textbf{26.6%}$\nLength-Controlled Win Rate (LC-WR) and $\\textbf{24.2%}$ Win Rate (WR).",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.16378v3",
    "published_date": "2024-12-20 22:25:23 UTC",
    "updated_date": "2025-02-24 07:53:07 UTC"
  },
  {
    "arxiv_id": "2412.16375v1",
    "title": "Iterative Encoding-Decoding VAEs Anomaly Detection in NOAA's DART Time Series: A Machine Learning Approach for Enhancing Data Integrity for NASA's GRACE-FO Verification and Validation",
    "authors": [
      "Kevin Lee"
    ],
    "abstract": "NOAA's Deep-ocean Assessment and Reporting of Tsunamis (DART) data are\ncritical for NASA-JPL's tsunami detection, real-time operations, and\noceanographic research. However, these time-series data often contain spikes,\nsteps, and drifts that degrade data quality and obscure essential oceanographic\nfeatures. To address these anomalies, the work introduces an Iterative\nEncoding-Decoding Variational Autoencoders (Iterative Encoding-Decoding VAEs)\nmodel to improve the quality of DART time series. Unlike traditional filtering\nand thresholding methods that risk distorting inherent signal characteristics,\nIterative Encoding-Decoding VAEs progressively remove anomalies while\npreserving the data's latent structure. A hybrid thresholding approach further\nretains genuine oceanographic features near boundaries. Applied to complex DART\ndatasets, this approach yields reconstructions that better maintain key oceanic\nproperties compared to classical statistical techniques, offering improved\nrobustness against spike removal and subtle step changes. The resulting\nhigh-quality data supports critical verification and validation efforts for the\nGRACE-FO mission at NASA-JPL, where accurate surface measurements are essential\nto modeling Earth's gravitational field and global water dynamics. Ultimately,\nthis data processing method enhances tsunami detection and underpins future\nclimate modeling with improved interpretability and reliability.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "physics.geo-ph"
    ],
    "primary_category": "cs.LG",
    "comment": "Preprint",
    "pdf_url": "http://arxiv.org/pdf/2412.16375v1",
    "published_date": "2024-12-20 22:19:11 UTC",
    "updated_date": "2024-12-20 22:19:11 UTC"
  },
  {
    "arxiv_id": "2412.16373v1",
    "title": "FairREAD: Re-fusing Demographic Attributes after Disentanglement for Fair Medical Image Classification",
    "authors": [
      "Yicheng Gao",
      "Jinkui Hao",
      "Bo Zhou"
    ],
    "abstract": "Recent advancements in deep learning have shown transformative potential in\nmedical imaging, yet concerns about fairness persist due to performance\ndisparities across demographic subgroups. Existing methods aim to address these\nbiases by mitigating sensitive attributes in image data; however, these\nattributes often carry clinically relevant information, and their removal can\ncompromise model performance-a highly undesirable outcome. To address this\nchallenge, we propose Fair Re-fusion After Disentanglement (FairREAD), a novel,\nsimple, and efficient framework that mitigates unfairness by re-integrating\nsensitive demographic attributes into fair image representations. FairREAD\nemploys orthogonality constraints and adversarial training to disentangle\ndemographic information while using a controlled re-fusion mechanism to\npreserve clinically relevant details. Additionally, subgroup-specific threshold\nadjustments ensure equitable performance across demographic groups.\nComprehensive evaluations on a large-scale clinical X-ray dataset demonstrate\nthat FairREAD significantly reduces unfairness metrics while maintaining\ndiagnostic accuracy, establishing a new benchmark for fairness and performance\nin medical image classification.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CY"
    ],
    "primary_category": "cs.CV",
    "comment": "Submitted to Medical Image Analysis, code will be available after\n  review is complete",
    "pdf_url": "http://arxiv.org/pdf/2412.16373v1",
    "published_date": "2024-12-20 22:17:57 UTC",
    "updated_date": "2024-12-20 22:17:57 UTC"
  },
  {
    "arxiv_id": "2412.16365v1",
    "title": "Overview of the First Workshop on Language Models for Low-Resource Languages (LoResLM 2025)",
    "authors": [
      "Hansi Hettiarachchi",
      "Tharindu Ranasinghe",
      "Paul Rayson",
      "Ruslan Mitkov",
      "Mohamed Gaber",
      "Damith Premasiri",
      "Fiona Anting Tan",
      "Lasitha Uyangodage"
    ],
    "abstract": "The first Workshop on Language Models for Low-Resource Languages (LoResLM\n2025) was held in conjunction with the 31st International Conference on\nComputational Linguistics (COLING 2025) in Abu Dhabi, United Arab Emirates.\nThis workshop mainly aimed to provide a forum for researchers to share and\ndiscuss their ongoing work on language models (LMs) focusing on low-resource\nlanguages, following the recent advancements in neural language models and\ntheir linguistic biases towards high-resource languages. LoResLM 2025 attracted\nnotable interest from the natural language processing (NLP) community,\nresulting in 35 accepted papers from 52 submissions. These contributions cover\na broad range of low-resource languages from eight language families and 13\ndiverse research areas, paving the way for future possibilities and promoting\nlinguistic inclusivity in NLP.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "The First Workshop on Language Models for Low-Resource Languages\n  (LoResLM 2025)",
    "pdf_url": "http://arxiv.org/pdf/2412.16365v1",
    "published_date": "2024-12-20 21:55:32 UTC",
    "updated_date": "2024-12-20 21:55:32 UTC"
  },
  {
    "arxiv_id": "2412.16359v2",
    "title": "Human-Readable Adversarial Prompts: An Investigation into LLM Vulnerabilities Using Situational Context",
    "authors": [
      "Nilanjana Das",
      "Edward Raff",
      "Manas Gaur"
    ],
    "abstract": "Previous studies that uncovered vulnerabilities in large language models\n(LLMs) frequently employed nonsensical adversarial prompts. However, such\nprompts can now be readily identified using automated detection techniques. To\nfurther strengthen adversarial attacks, we focus on human-readable adversarial\nprompts, which are more realistic and potent threats. Our key contributions are\n(1) situation-driven attacks leveraging movie scripts as context to create\nhuman-readable prompts that successfully deceive LLMs, (2) adversarial suffix\nconversion to transform nonsensical adversarial suffixes into independent\nmeaningful text, and (3) AdvPrompter with p-nucleus sampling, a method to\ngenerate diverse, human-readable adversarial suffixes, improving attack\nefficacy in models like GPT-3.5 and Gemma 7B.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "arXiv admin note: text overlap with arXiv:2407.14644",
    "pdf_url": "http://arxiv.org/pdf/2412.16359v2",
    "published_date": "2024-12-20 21:43:52 UTC",
    "updated_date": "2025-03-11 21:41:19 UTC"
  },
  {
    "arxiv_id": "2412.16355v2",
    "title": "Social Science Is Necessary for Operationalizing Socially Responsible Foundation Models",
    "authors": [
      "Adam Davies",
      "Elisa Nguyen",
      "Michael Simeone",
      "Erik Johnston",
      "Martin Gubri"
    ],
    "abstract": "With the rise of foundation models, there is growing concern about their\npotential social impacts. Social science has a long history of studying the\nsocial impacts of transformative technologies in terms of pre-existing systems\nof power and how these systems are disrupted or reinforced by new technologies.\nIn this position paper, we build on prior work studying the social impacts of\nearlier technologies to propose a conceptual framework studying foundation\nmodels as sociotechnical systems, incorporating social science expertise to\nbetter understand how these models affect systems of power, anticipate the\nimpacts of deploying these models in various applications, and study the\neffectiveness of technical interventions intended to mitigate social harms. We\nadvocate for an interdisciplinary and collaborative research paradigm between\nAI and social science across all stages of foundation model research and\ndevelopment to promote socially responsible research practices and use cases,\nand outline several strategies to facilitate such research.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.16355v2",
    "published_date": "2024-12-20 21:34:43 UTC",
    "updated_date": "2025-04-02 19:56:19 UTC"
  },
  {
    "arxiv_id": "2412.16339v2",
    "title": "Deliberative Alignment: Reasoning Enables Safer Language Models",
    "authors": [
      "Melody Y. Guan",
      "Manas Joglekar",
      "Eric Wallace",
      "Saachi Jain",
      "Boaz Barak",
      "Alec Helyar",
      "Rachel Dias",
      "Andrea Vallone",
      "Hongyu Ren",
      "Jason Wei",
      "Hyung Won Chung",
      "Sam Toyer",
      "Johannes Heidecke",
      "Alex Beutel",
      "Amelia Glaese"
    ],
    "abstract": "As large-scale language models increasingly impact safety-critical domains,\nensuring their reliable adherence to well-defined principles remains a\nfundamental challenge. We introduce Deliberative Alignment, a new paradigm that\ndirectly teaches the model safety specifications and trains it to explicitly\nrecall and accurately reason over the specifications before answering. We used\nthis approach to align OpenAI's o-series models, and achieved highly precise\nadherence to OpenAI's safety policies, without requiring human-written\nchain-of-thoughts or answers. Deliberative Alignment pushes the Pareto frontier\nby simultaneously increasing robustness to jailbreaks while decreasing\noverrefusal rates, and also improves out-of-distribution generalization. We\ndemonstrate that reasoning over explicitly specified policies enables more\nscalable, trustworthy, and interpretable alignment.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CY",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "24 pages",
    "pdf_url": "http://arxiv.org/pdf/2412.16339v2",
    "published_date": "2024-12-20 21:00:11 UTC",
    "updated_date": "2025-01-08 20:11:59 UTC"
  },
  {
    "arxiv_id": "2412.16336v1",
    "title": "Real Faults in Deep Learning Fault Benchmarks: How Real Are They?",
    "authors": [
      "Gunel Jahangirova",
      "Nargiz Humbatova",
      "Jinhan Kim",
      "Shin Yoo",
      "Paolo Tonella"
    ],
    "abstract": "As the adoption of Deep Learning (DL) systems continues to rise, an\nincreasing number of approaches are being proposed to test these systems,\nlocalise faults within them, and repair those faults. The best attestation of\neffectiveness for such techniques is an evaluation that showcases their\ncapability to detect, localise and fix real faults. To facilitate these\nevaluations, the research community has collected multiple benchmarks of real\nfaults in DL systems. In this work, we perform a manual analysis of 490 faults\nfrom five different benchmarks and identify that 314 of them are eligible for\nour study. Our investigation focuses specifically on how well the bugs\ncorrespond to the sources they were extracted from, which fault types are\nrepresented, and whether the bugs are reproducible. Our findings indicate that\nonly 18.5% of the faults satisfy our realism conditions. Our attempts to\nreproduce these faults were successful only in 52% of cases.",
    "categories": [
      "cs.SE",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.SE",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.16336v1",
    "published_date": "2024-12-20 20:52:10 UTC",
    "updated_date": "2024-12-20 20:52:10 UTC"
  },
  {
    "arxiv_id": "2412.16333v1",
    "title": "Optimizing Fintech Marketing: A Comparative Study of Logistic Regression and XGBoost",
    "authors": [
      "Sahar Yarmohammadtoosky Dinesh Chowdary Attota"
    ],
    "abstract": "As several studies have shown, predicting credit risk is still a major\nconcern for the financial services industry and is receiving a lot of scholarly\ninterest. This area of study is crucial because it aids financial organizations\nin determining the probability that borrowers would default, which has a direct\nbearing on lending choices and risk management tactics. Despite the progress\nmade in this domain, there is still a substantial knowledge gap concerning\nconsumer actions that take place prior to the filing of credit card\napplications. The objective of this study is to predict customer responses to\nmail campaigns and assess the likelihood of default among those who engage.\nThis research employs advanced machine learning techniques, specifically\nlogistic regression and XGBoost, to analyze consumer behavior and predict\nresponses to direct mail campaigns. By integrating different data preprocessing\nstrategies, including imputation and binning, we enhance the robustness and\naccuracy of our predictive models. The results indicate that XGBoost\nconsistently outperforms logistic regression across various metrics,\nparticularly in scenarios using categorical binning and custom imputation.\nThese findings suggest that XGBoost is particularly effective in handling\ncomplex data structures and provides a strong predictive capability in\nassessing credit risk.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "q-fin.ST",
      "stat.AP"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.16333v1",
    "published_date": "2024-12-20 20:45:42 UTC",
    "updated_date": "2024-12-20 20:45:42 UTC"
  },
  {
    "arxiv_id": "2412.16329v1",
    "title": "Improving Object Detection for Time-Lapse Imagery Using Temporal Features in Wildlife Monitoring",
    "authors": [
      "Marcus Jenkins",
      "Kirsty A. Franklin",
      "Malcolm A. C. Nicoll",
      "Nik C. Cole",
      "Kevin Ruhomaun",
      "Vikash Tatayah",
      "Michal Mackiewicz"
    ],
    "abstract": "Monitoring animal populations is crucial for assessing the health of\necosystems. Traditional methods, which require extensive fieldwork, are\nincreasingly being supplemented by time-lapse camera-trap imagery combined with\nan automatic analysis of the image data. The latter usually involves some\nobject detector aimed at detecting relevant targets (commonly animals) in each\nimage, followed by some postprocessing to gather activity and population data.\nIn this paper, we show that the performance of an object detector in a single\nframe of a time-lapse sequence can be improved by including spatio-temporal\nfeatures from the prior frames. We propose a method that leverages temporal\ninformation by integrating two additional spatial feature channels which\ncapture stationary and non-stationary elements of the scene and consequently\nimprove scene understanding and reduce the number of stationary false\npositives. The proposed technique achieves a significant improvement of 24\\% in\nmean average precision (mAP@0.05:0.95) over the baseline (temporal\nfeature-free, single frame) object detector on a large dataset of breeding\ntropical seabirds. We envisage our method will be widely applicable to other\nwildlife monitoring applications that use time-lapse imaging.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "68T45",
      "I.4.8"
    ],
    "primary_category": "cs.CV",
    "comment": "18 pages, 13 figures",
    "pdf_url": "http://arxiv.org/pdf/2412.16329v1",
    "published_date": "2024-12-20 20:37:09 UTC",
    "updated_date": "2024-12-20 20:37:09 UTC"
  },
  {
    "arxiv_id": "2412.16325v1",
    "title": "Towards Safe and Honest AI Agents with Neural Self-Other Overlap",
    "authors": [
      "Marc Carauleanu",
      "Michael Vaiana",
      "Judd Rosenblatt",
      "Cameron Berg",
      "Diogo Schwerz de Lucena"
    ],
    "abstract": "As AI systems increasingly make critical decisions, deceptive AI poses a\nsignificant challenge to trust and safety. We present Self-Other Overlap (SOO)\nfine-tuning, a promising approach in AI Safety that could substantially improve\nour ability to build honest artificial intelligence. Inspired by cognitive\nneuroscience research on empathy, SOO aims to align how AI models represent\nthemselves and others. Our experiments on LLMs with 7B, 27B, and 78B parameters\ndemonstrate SOO's efficacy: deceptive responses of Mistral-7B-Instruct-v0.2\ndropped from 73.6% to 17.2% with no observed reduction in general task\nperformance, while in Gemma-2-27b-it and CalmeRys-78B-Orpo-v0.1 deceptive\nresponses were reduced from 100% to 9.3% and 2.7%, respectively, with a small\nimpact on capabilities. In reinforcement learning scenarios, SOO-trained agents\nshowed significantly reduced deceptive behavior. SOO's focus on contrastive\nself and other-referencing observations offers strong potential for\ngeneralization across AI architectures. While current applications focus on\nlanguage models and simple RL environments, SOO could pave the way for more\ntrustworthy AI in broader domains. Ethical implications and long-term effects\nwarrant further investigation, but SOO represents a significant step forward in\nAI safety research.",
    "categories": [
      "cs.AI",
      "cs.CR"
    ],
    "primary_category": "cs.AI",
    "comment": "NeurIPS 2024 Safe Generative AI Workshop",
    "pdf_url": "http://arxiv.org/pdf/2412.16325v1",
    "published_date": "2024-12-20 20:23:52 UTC",
    "updated_date": "2024-12-20 20:23:52 UTC"
  },
  {
    "arxiv_id": "2501.01969v1",
    "title": "Optimal bounds for dissatisfaction in perpetual voting",
    "authors": [
      "Alexander Kozachinskiy",
      "Alexander Shen",
      "Tomasz Steifer"
    ],
    "abstract": "In perpetual voting, multiple decisions are made at different moments in\ntime. Taking the history of previous decisions into account allows us to\nsatisfy properties such as proportionality over periods of time. In this paper,\nwe consider the following question: is there a perpetual approval voting method\nthat guarantees that no voter is dissatisfied too many times? We identify a\nsufficient condition on voter behavior -- which we call 'bounded conflicts'\ncondition -- under which a sublinear growth of dissatisfaction is possible. We\nprovide a tight upper bound on the growth of dissatisfaction under bounded\nconflicts, using techniques from Kolmogorov complexity. We also observe that\nthe approval voting with binary choices mimics the machine learning setting of\nprediction with expert advice. This allows us to present a voting method with\nsublinear guarantees on dissatisfaction under bounded conflicts, based on the\nstandard techniques from prediction with expert advice.",
    "categories": [
      "cs.GT",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.GT",
    "comment": "Full version of the AAAI 2025 paper",
    "pdf_url": "http://arxiv.org/pdf/2501.01969v1",
    "published_date": "2024-12-20 19:58:55 UTC",
    "updated_date": "2024-12-20 19:58:55 UTC"
  },
  {
    "arxiv_id": "2412.16311v1",
    "title": "HybGRAG: Hybrid Retrieval-Augmented Generation on Textual and Relational Knowledge Bases",
    "authors": [
      "Meng-Chieh Lee",
      "Qi Zhu",
      "Costas Mavromatis",
      "Zhen Han",
      "Soji Adeshina",
      "Vassilis N. Ioannidis",
      "Huzefa Rangwala",
      "Christos Faloutsos"
    ],
    "abstract": "Given a semi-structured knowledge base (SKB), where text documents are\ninterconnected by relations, how can we effectively retrieve relevant\ninformation to answer user questions? Retrieval-Augmented Generation (RAG)\nretrieves documents to assist large language models (LLMs) in question\nanswering; while Graph RAG (GRAG) uses structured knowledge bases as its\nknowledge source. However, many questions require both textual and relational\ninformation from SKB - referred to as \"hybrid\" questions - which complicates\nthe retrieval process and underscores the need for a hybrid retrieval method\nthat leverages both information. In this paper, through our empirical analysis,\nwe identify key insights that show why existing methods may struggle with\nhybrid question answering (HQA) over SKB. Based on these insights, we propose\nHybGRAG for HQA consisting of a retriever bank and a critic module, with the\nfollowing advantages: (1) Agentic, it automatically refines the output by\nincorporating feedback from the critic module, (2) Adaptive, it solves hybrid\nquestions requiring both textual and relational information with the retriever\nbank, (3) Interpretable, it justifies decision making with intuitive refinement\npath, and (4) Effective, it surpasses all baselines on HQA benchmarks. In\nexperiments on the STaRK benchmark, HybGRAG achieves significant performance\ngains, with an average relative improvement in Hit@1 of 51%.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.IR"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.16311v1",
    "published_date": "2024-12-20 19:49:12 UTC",
    "updated_date": "2024-12-20 19:49:12 UTC"
  },
  {
    "arxiv_id": "2412.16291v1",
    "title": "Benchmarking LLMs and SLMs for patient reported outcomes",
    "authors": [
      "Matteo Marengo",
      "Jarod Lévy",
      "Jean-Emmanuel Bibault"
    ],
    "abstract": "LLMs have transformed the execution of numerous tasks, including those in the\nmedical domain. Among these, summarizing patient-reported outcomes (PROs) into\nconcise natural language reports is of particular interest to clinicians, as it\nenables them to focus on critical patient concerns and spend more time in\nmeaningful discussions. While existing work with LLMs like GPT-4 has shown\nimpressive results, real breakthroughs could arise from leveraging SLMs as they\noffer the advantage of being deployable locally, ensuring patient data privacy\nand compliance with healthcare regulations. This study benchmarks several SLMs\nagainst LLMs for summarizing patient-reported Q\\&A forms in the context of\nradiotherapy. Using various metrics, we evaluate their precision and\nreliability. The findings highlight both the promise and limitations of SLMs\nfor high-stakes medical tasks, fostering more efficient and privacy-preserving\nAI-driven healthcare solutions.",
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.AI",
    "comment": "10 pages",
    "pdf_url": "http://arxiv.org/pdf/2412.16291v1",
    "published_date": "2024-12-20 19:01:25 UTC",
    "updated_date": "2024-12-20 19:01:25 UTC"
  },
  {
    "arxiv_id": "2412.16153v2",
    "title": "MotiF: Making Text Count in Image Animation with Motion Focal Loss",
    "authors": [
      "Shijie Wang",
      "Samaneh Azadi",
      "Rohit Girdhar",
      "Saketh Rambhatla",
      "Chen Sun",
      "Xi Yin"
    ],
    "abstract": "Text-Image-to-Video (TI2V) generation aims to generate a video from an image\nfollowing a text description, which is also referred to as text-guided image\nanimation. Most existing methods struggle to generate videos that align well\nwith the text prompts, particularly when motion is specified. To overcome this\nlimitation, we introduce MotiF, a simple yet effective approach that directs\nthe model's learning to the regions with more motion, thereby improving the\ntext alignment and motion generation. We use optical flow to generate a motion\nheatmap and weight the loss according to the intensity of the motion. This\nmodified objective leads to noticeable improvements and complements existing\nmethods that utilize motion priors as model inputs. Additionally, due to the\nlack of a diverse benchmark for evaluating TI2V generation, we propose TI2V\nBench, a dataset consists of 320 image-text pairs for robust evaluation. We\npresent a human evaluation protocol that asks the annotators to select an\noverall preference between two videos followed by their justifications. Through\na comprehensive evaluation on TI2V Bench, MotiF outperforms nine open-sourced\nmodels, achieving an average preference of 72%. The TI2V Bench and additional\nresults are released in https://wang-sj16.github.io/motif/.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "Accepted by CVPR 2025. Project page:\n  https://wang-sj16.github.io/motif/",
    "pdf_url": "http://arxiv.org/pdf/2412.16153v2",
    "published_date": "2024-12-20 18:57:06 UTC",
    "updated_date": "2025-03-23 00:30:55 UTC"
  },
  {
    "arxiv_id": "2412.16145v2",
    "title": "Offline Reinforcement Learning for LLM Multi-Step Reasoning",
    "authors": [
      "Huaijie Wang",
      "Shibo Hao",
      "Hanze Dong",
      "Shenao Zhang",
      "Yilin Bao",
      "Ziran Yang",
      "Yi Wu"
    ],
    "abstract": "Improving the multi-step reasoning ability of large language models (LLMs)\nwith offline reinforcement learning (RL) is essential for quickly adapting them\nto complex tasks. While Direct Preference Optimization (DPO) has shown promise\nin aligning LLMs with human preferences, it is less suitable for multi-step\nreasoning tasks because (1) DPO relies on paired preference data, which is not\nreadily available for multi-step reasoning tasks, and (2) it treats all tokens\nuniformly, making it ineffective for credit assignment in multi-step reasoning\ntasks, which often come with sparse reward. In this work, we propose OREO\n(Offline Reasoning Optimization), an offline RL method for enhancing LLM\nmulti-step reasoning. Building on insights from previous works of maximum\nentropy reinforcement learning, it jointly learns a policy model and value\nfunction by optimizing the soft Bellman Equation. We show in principle that it\nreduces the need to collect pairwise data and enables better credit assignment.\nEmpirically, OREO surpasses existing offline learning methods on multi-step\nreasoning benchmarks, including mathematical reasoning tasks (GSM8K, MATH) and\nembodied agent control (ALFWorld). The approach can be extended to a\nmulti-iteration framework when additional resources are available. Furthermore,\nthe learned value function can be leveraged to guide the tree search for free,\nwhich can further boost performance during test time.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.16145v2",
    "published_date": "2024-12-20 18:49:45 UTC",
    "updated_date": "2024-12-25 18:54:02 UTC"
  },
  {
    "arxiv_id": "2412.16277v1",
    "title": "Mapping the Mind of an Instruction-based Image Editing using SMILE",
    "authors": [
      "Zeinab Dehghani",
      "Koorosh Aslansefat",
      "Adil Khan",
      "Adín Ramírez Rivera",
      "Franky George",
      "Muhammad Khalid"
    ],
    "abstract": "Despite recent advancements in Instruct-based Image Editing models for\ngenerating high-quality images, they are known as black boxes and a significant\nbarrier to transparency and user trust. To solve this issue, we introduce SMILE\n(Statistical Model-agnostic Interpretability with Local Explanations), a novel\nmodel-agnostic for localized interpretability that provides a visual heatmap to\nclarify the textual elements' influence on image-generating models. We applied\nour method to various Instruction-based Image Editing models like Pix2Pix,\nImage2Image-turbo and Diffusers-Inpaint and showed how our model can improve\ninterpretability and reliability. Also, we use stability, accuracy, fidelity,\nand consistency metrics to evaluate our method. These findings indicate the\nexciting potential of model-agnostic interpretability for reliability and\ntrustworthiness in critical applications such as healthcare and autonomous\ndriving while encouraging additional investigation into the significance of\ninterpretability in enhancing dependable image editing models.",
    "categories": [
      "cs.AI",
      "cs.CV",
      "cs.HC"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.16277v1",
    "published_date": "2024-12-20 18:33:23 UTC",
    "updated_date": "2024-12-20 18:33:23 UTC"
  },
  {
    "arxiv_id": "2412.16135v3",
    "title": "Can LLMs Obfuscate Code? A Systematic Analysis of Large Language Models into Assembly Code Obfuscation",
    "authors": [
      "Seyedreza Mohseni",
      "Seyedali Mohammadi",
      "Deepa Tilwani",
      "Yash Saxena",
      "Gerald Ketu Ndawula",
      "Sriram Vema",
      "Edward Raff",
      "Manas Gaur"
    ],
    "abstract": "Malware authors often employ code obfuscations to make their malware harder\nto detect. Existing tools for generating obfuscated code often require access\nto the original source code (e.g., C++ or Java), and adding new obfuscations is\na non-trivial, labor-intensive process. In this study, we ask the following\nquestion: Can Large Language Models (LLMs) potentially generate a new\nobfuscated assembly code? If so, this poses a risk to anti-virus engines and\npotentially increases the flexibility of attackers to create new obfuscation\npatterns. We answer this in the affirmative by developing the MetamorphASM\nbenchmark comprising MetamorphASM Dataset (MAD) along with three code\nobfuscation techniques: dead code, register substitution, and control flow\nchange. The MetamorphASM systematically evaluates the ability of LLMs to\ngenerate and analyze obfuscated code using MAD, which contains 328,200\nobfuscated assembly code samples. We release this dataset and analyze the\nsuccess rate of various LLMs (e.g., GPT-3.5/4, GPT-4o-mini, Starcoder,\nCodeGemma, CodeLlama, CodeT5, and LLaMA 3.1) in generating obfuscated assembly\ncode. The evaluation was performed using established information-theoretic\nmetrics and manual human review to ensure correctness and provide the\nfoundation for researchers to study and develop remediations to this risk.",
    "categories": [
      "cs.CR",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.CR",
    "comment": "To appear in AAAI 2025, Main Track",
    "pdf_url": "http://arxiv.org/pdf/2412.16135v3",
    "published_date": "2024-12-20 18:31:24 UTC",
    "updated_date": "2025-01-29 13:52:31 UTC"
  },
  {
    "arxiv_id": "2412.16118v1",
    "title": "Convolutional Deep Operator Networks for Learning Nonlinear Focused Ultrasound Wave Propagation in Heterogeneous Spinal Cord Anatomy",
    "authors": [
      "Avisha Kumar",
      "Xuzhe Zhi",
      "Zan Ahmad",
      "Minglang Yin",
      "Amir Manbachi"
    ],
    "abstract": "Focused ultrasound (FUS) therapy is a promising tool for optimally targeted\ntreatment of spinal cord injuries (SCI), offering submillimeter precision to\nenhance blood flow at injury sites while minimizing impact on surrounding\ntissues. However, its efficacy is highly sensitive to the placement of the\nultrasound source, as the spinal cord's complex geometry and acoustic\nheterogeneity distort and attenuate the FUS signal. Current approaches rely on\ncomputer simulations to solve the governing wave propagation equations and\ncompute patient-specific pressure maps using ultrasound images of the spinal\ncord anatomy. While accurate, these high-fidelity simulations are\ncomputationally intensive, taking up to hours to complete parameter sweeps,\nwhich is impractical for real-time surgical decision-making. To address this\nbottleneck, we propose a convolutional deep operator network (DeepONet) to\nrapidly predict FUS pressure fields in patient spinal cords. Unlike\nconventional neural networks, DeepONets are well equipped to approximate the\nsolution operator of the parametric partial differential equations (PDEs) that\ngovern the behavior of FUS waves with varying initial and boundary conditions\n(i.e., new transducer locations or spinal cord geometries) without requiring\nextensive simulations. Trained on simulated pressure maps across diverse\npatient anatomies, this surrogate model achieves real-time predictions with\nonly a 2% loss on the test set, significantly accelerating the modeling of\nnonlinear physical systems in heterogeneous domains. By facilitating rapid\nparameter sweeps in surgical settings, this work provides a crucial step toward\nprecise and individualized solutions in neurosurgical treatments.",
    "categories": [
      "physics.med-ph",
      "cs.AI"
    ],
    "primary_category": "physics.med-ph",
    "comment": "Accepted for oral presentation at AAAI Conference on Artificial\n  Intelligence: AI for Accelerating Science and Engineering Workshop 2025",
    "pdf_url": "http://arxiv.org/pdf/2412.16118v1",
    "published_date": "2024-12-20 18:03:38 UTC",
    "updated_date": "2024-12-20 18:03:38 UTC"
  },
  {
    "arxiv_id": "2412.16108v1",
    "title": "Demystifying the Potential of ChatGPT-4 Vision for Construction Progress Monitoring",
    "authors": [
      "Ahmet Bahaddin Ersoz"
    ],
    "abstract": "The integration of Large Vision-Language Models (LVLMs) such as OpenAI's\nGPT-4 Vision into various sectors has marked a significant evolution in the\nfield of artificial intelligence, particularly in the analysis and\ninterpretation of visual data. This paper explores the practical application of\nGPT-4 Vision in the construction industry, focusing on its capabilities in\nmonitoring and tracking the progress of construction projects. Utilizing\nhigh-resolution aerial imagery of construction sites, the study examines how\nGPT-4 Vision performs detailed scene analysis and tracks developmental changes\nover time. The findings demonstrate that while GPT-4 Vision is proficient in\nidentifying construction stages, materials, and machinery, it faces challenges\nwith precise object localization and segmentation. Despite these limitations,\nthe potential for future advancements in this technology is considerable. This\nresearch not only highlights the current state and opportunities of using LVLMs\nin construction but also discusses future directions for enhancing the model's\nutility through domain-specific training and integration with other computer\nvision techniques and digital twins.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.16108v1",
    "published_date": "2024-12-20 17:49:22 UTC",
    "updated_date": "2024-12-20 17:49:22 UTC"
  },
  {
    "arxiv_id": "2412.16098v2",
    "title": "Explainable AI for Multivariate Time Series Pattern Exploration: Latent Space Visual Analytics with Temporal Fusion Transformer and Variational Autoencoders in Power Grid Event Diagnosis",
    "authors": [
      "Haowen Xu",
      "Ali Boyaci",
      "Jianming Lian",
      "Aaron Wilson"
    ],
    "abstract": "Detecting and analyzing complex patterns in multivariate time-series data is\ncrucial for decision-making in urban and environmental system operations.\nHowever, challenges arise from the high dimensionality, intricate complexity,\nand interconnected nature of complex patterns, which hinder the understanding\nof their underlying physical processes. Existing AI methods often face\nlimitations in interpretability, computational efficiency, and scalability,\nreducing their applicability in real-world scenarios. This paper proposes a\nnovel visual analytics framework that integrates two generative AI models,\nTemporal Fusion Transformer (TFT) and Variational Autoencoders (VAEs), to\nreduce complex patterns into lower-dimensional latent spaces and visualize them\nin 2D using dimensionality reduction techniques such as PCA, t-SNE, and UMAP\nwith DBSCAN. These visualizations, presented through coordinated and\ninteractive views and tailored glyphs, enable intuitive exploration of complex\nmultivariate temporal patterns, identifying patterns' similarities and uncover\ntheir potential correlations for a better interpretability of the AI outputs.\nThe framework is demonstrated through a case study on power grid signal data,\nwhere it identifies multi-label grid event signatures, including faults and\nanomalies with diverse root causes. Additionally, novel metrics and\nvisualizations are introduced to validate the models and evaluate the\nperformance, efficiency, and consistency of latent maps generated by TFT and\nVAE under different configurations. These analyses provide actionable insights\nfor model parameter tuning and reliability improvements. Comparative results\nhighlight that TFT achieves shorter run times and superior scalability to\ndiverse time-series data shapes compared to VAE. This work advances fault\ndiagnosis in multivariate time series, fostering explainable AI to support\ncritical system operations.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.16098v2",
    "published_date": "2024-12-20 17:41:11 UTC",
    "updated_date": "2024-12-24 05:04:52 UTC"
  },
  {
    "arxiv_id": "2412.16089v1",
    "title": "The Evolution of LLM Adoption in Industry Data Curation Practices",
    "authors": [
      "Crystal Qian",
      "Michael Xieyang Liu",
      "Emily Reif",
      "Grady Simon",
      "Nada Hussein",
      "Nathan Clement",
      "James Wexler",
      "Carrie J. Cai",
      "Michael Terry",
      "Minsuk Kahng"
    ],
    "abstract": "As large language models (LLMs) grow increasingly adept at processing\nunstructured text data, they offer new opportunities to enhance data curation\nworkflows. This paper explores the evolution of LLM adoption among\npractitioners at a large technology company, evaluating the impact of LLMs in\ndata curation tasks through participants' perceptions, integration strategies,\nand reported usage scenarios. Through a series of surveys, interviews, and user\nstudies, we provide a timely snapshot of how organizations are navigating a\npivotal moment in LLM evolution. In Q2 2023, we conducted a survey to assess\nLLM adoption in industry for development tasks (N=84), and facilitated expert\ninterviews to assess evolving data needs (N=10) in Q3 2023. In Q2 2024, we\nexplored practitioners' current and anticipated LLM usage through a user study\ninvolving two LLM-based prototypes (N=12). While each study addressed distinct\nresearch goals, they revealed a broader narrative about evolving LLM usage in\naggregate. We discovered an emerging shift in data understanding from\nheuristic-first, bottom-up approaches to insights-first, top-down workflows\nsupported by LLMs. Furthermore, to respond to a more complex data landscape,\ndata practitioners now supplement traditional subject-expert-created 'golden\ndatasets' with LLM-generated 'silver' datasets and rigorously validated 'super\ngolden' datasets curated by diverse experts. This research sheds light on the\ntransformative role of LLMs in large-scale analysis of unstructured data and\nhighlights opportunities for further tool development.",
    "categories": [
      "cs.HC",
      "cs.AI"
    ],
    "primary_category": "cs.HC",
    "comment": "19 pages, 4 tables, 3 figures",
    "pdf_url": "http://arxiv.org/pdf/2412.16089v1",
    "published_date": "2024-12-20 17:34:16 UTC",
    "updated_date": "2024-12-20 17:34:16 UTC"
  },
  {
    "arxiv_id": "2412.16086v2",
    "title": "Towards Interpretable Radiology Report Generation via Concept Bottlenecks using a Multi-Agentic RAG",
    "authors": [
      "Hasan Md Tusfiqur Alam",
      "Devansh Srivastav",
      "Md Abdul Kadir",
      "Daniel Sonntag"
    ],
    "abstract": "Deep learning has advanced medical image classification, but interpretability\nchallenges hinder its clinical adoption. This study enhances interpretability\nin Chest X-ray (CXR) classification by using concept bottleneck models (CBMs)\nand a multi-agent Retrieval-Augmented Generation (RAG) system for report\ngeneration. By modeling relationships between visual features and clinical\nconcepts, we create interpretable concept vectors that guide a multi-agent RAG\nsystem to generate radiology reports, enhancing clinical relevance,\nexplainability, and transparency. Evaluation of the generated reports using an\nLLM-as-a-judge confirmed the interpretability and clinical utility of our\nmodel's outputs. On the COVID-QU dataset, our model achieved 81% classification\naccuracy and demonstrated robust report generation performance, with five key\nmetrics ranging between 84% and 90%. This interpretable multi-agent framework\nbridges the gap between high-performance AI and the explainability required for\nreliable AI-driven CXR analysis in clinical settings. Our code is available at\nhttps://github.com/tifat58/IRR-with-CBM-RAG.git.",
    "categories": [
      "cs.IR",
      "cs.AI",
      "cs.CL",
      "cs.CV",
      "eess.IV"
    ],
    "primary_category": "cs.IR",
    "comment": "Accepted in the 47th European Conference for Information Retrieval\n  (ECIR) 2025",
    "pdf_url": "http://arxiv.org/pdf/2412.16086v2",
    "published_date": "2024-12-20 17:33:50 UTC",
    "updated_date": "2025-01-22 17:18:15 UTC"
  },
  {
    "arxiv_id": "2501.06192v1",
    "title": "A Computational Model of Learning and Memory Using Structurally Dynamic Cellular Automata",
    "authors": [
      "Jeet Singh"
    ],
    "abstract": "In the fields of computation and neuroscience, much is still unknown about\nthe underlying computations that enable key cognitive functions including\nlearning, memory, abstraction and behavior. This paper proposes a mathematical\nand computational model of learning and memory based on a small set of\nbio-plausible functions that include coincidence detection, signal modulation,\nand reward/penalty mechanisms. Our theoretical approach proposes that these\nbasic functions are sufficient to establish and modulate an information space\nover which computation can be carried out, generating signal gradients usable\nfor inference and behavior. The computational method used to test this is a\nstructurally dynamic cellular automaton with continuous-valued cell states and\na series of recursive steps propagating over an undirected graph with the\nmemory function embedded entirely in the creation and modulation of graph\nedges. The experimental results show: that the toy model can make near-optimal\nchoices to re-discover a reward state after a single training run; that it can\navoid complex penalty configurations; that signal modulation and network\nplasticity can generate exploratory behaviors in sparse reward environments;\nthat the model generates context-dependent memory representations; and that it\nexhibits high computational efficiency because of its minimal, single-pass\ntraining requirements combined with flexible and contextual memory\nrepresentation.",
    "categories": [
      "cs.AI",
      "cs.NE",
      "math.DS",
      "q-bio.NC"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.06192v1",
    "published_date": "2024-12-20 17:26:17 UTC",
    "updated_date": "2024-12-20 17:26:17 UTC"
  },
  {
    "arxiv_id": "2412.16075v1",
    "title": "Formal Mathematical Reasoning: A New Frontier in AI",
    "authors": [
      "Kaiyu Yang",
      "Gabriel Poesia",
      "Jingxuan He",
      "Wenda Li",
      "Kristin Lauter",
      "Swarat Chaudhuri",
      "Dawn Song"
    ],
    "abstract": "AI for Mathematics (AI4Math) is not only intriguing intellectually but also\ncrucial for AI-driven discovery in science, engineering, and beyond. Extensive\nefforts on AI4Math have mirrored techniques in NLP, in particular, training\nlarge language models on carefully curated math datasets in text form. As a\ncomplementary yet less explored avenue, formal mathematical reasoning is\ngrounded in formal systems such as proof assistants, which can verify the\ncorrectness of reasoning and provide automatic feedback. In this position\npaper, we advocate for formal mathematical reasoning and argue that it is\nindispensable for advancing AI4Math to the next level. In recent years, we have\nseen steady progress in using AI to perform formal reasoning, including core\ntasks such as theorem proving and autoformalization, as well as emerging\napplications such as verifiable generation of code and hardware designs.\nHowever, significant challenges remain to be solved for AI to truly master\nmathematics and achieve broader impact. We summarize existing progress, discuss\nopen challenges, and envision critical milestones to measure future success. At\nthis inflection point for formal mathematical reasoning, we call on the\nresearch community to come together to drive transformative advancements in\nthis field.",
    "categories": [
      "cs.AI",
      "cs.LG",
      "cs.LO"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.16075v1",
    "published_date": "2024-12-20 17:19:24 UTC",
    "updated_date": "2024-12-20 17:19:24 UTC"
  },
  {
    "arxiv_id": "2412.16275v1",
    "title": "LEARN: A Unified Framework for Multi-Task Domain Adapt Few-Shot Learning",
    "authors": [
      "Bharadwaj Ravichandran",
      "Alexander Lynch",
      "Sarah Brockman",
      "Brandon RichardWebster",
      "Dawei Du",
      "Anthony Hoogs",
      "Christopher Funk"
    ],
    "abstract": "Both few-shot learning and domain adaptation sub-fields in Computer Vision\nhave seen significant recent progress in terms of the availability of\nstate-of-the-art algorithms and datasets. Frameworks have been developed for\neach sub-field; however, building a common system or framework that combines\nboth is something that has not been explored. As part of our research, we\npresent the first unified framework that combines domain adaptation for the\nfew-shot learning setting across 3 different tasks - image classification,\nobject detection and video classification. Our framework is highly modular with\nthe capability to support few-shot learning with/without the inclusion of\ndomain adaptation depending on the algorithm. Furthermore, the most important\nconfigurable feature of our framework is the on-the-fly setup for incremental\n$n$-shot tasks with the optional capability to configure the system to scale to\na traditional many-shot task. With more focus on Self-Supervised Learning (SSL)\nfor current few-shot learning approaches, our system also supports multiple SSL\npre-training configurations. To test our framework's capabilities, we provide\nbenchmarks on a wide range of algorithms and datasets across different task and\nproblem settings. The code is open source has been made publicly available\nhere: https://gitlab.kitware.com/darpa_learn/learn",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.16275v1",
    "published_date": "2024-12-20 17:16:15 UTC",
    "updated_date": "2024-12-20 17:16:15 UTC"
  },
  {
    "arxiv_id": "2412.16050v4",
    "title": "Label-Efficient Data Augmentation with Video Diffusion Models for Guidewire Segmentation in Cardiac Fluoroscopy",
    "authors": [
      "Shaoyan Pan",
      "Yikang Liu",
      "Lin Zhao",
      "Eric Z. Chen",
      "Xiao Chen",
      "Terrence Chen",
      "Shanhui Sun"
    ],
    "abstract": "The accurate segmentation of guidewires in interventional cardiac fluoroscopy\nvideos is crucial for computer-aided navigation tasks. Although deep learning\nmethods have demonstrated high accuracy and robustness in wire segmentation,\nthey require substantial annotated datasets for generalizability, underscoring\nthe need for extensive labeled data to enhance model performance. To address\nthis challenge, we propose the Segmentation-guided Frame-consistency Video\nDiffusion Model (SF-VD) to generate large collections of labeled fluoroscopy\nvideos, augmenting the training data for wire segmentation networks. SF-VD\nleverages videos with limited annotations by independently modeling scene\ndistribution and motion distribution. It first samples the scene distribution\nby generating 2D fluoroscopy images with wires positioned according to a\nspecified input mask, and then samples the motion distribution by progressively\ngenerating subsequent frames, ensuring frame-to-frame coherence through a\nframe-consistency strategy. A segmentation-guided mechanism further refines the\nprocess by adjusting wire contrast, ensuring a diverse range of visibility in\nthe synthesized image. Evaluation on a fluoroscopy dataset confirms the\nsuperior quality of the generated videos and shows significant improvements in\nguidewire segmentation.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "AAAI 2025",
    "pdf_url": "http://arxiv.org/pdf/2412.16050v4",
    "published_date": "2024-12-20 16:52:11 UTC",
    "updated_date": "2025-01-27 21:13:10 UTC"
  },
  {
    "arxiv_id": "2412.16038v3",
    "title": "Intelligent Approaches to Predictive Analytics in Occupational Health and Safety in India",
    "authors": [
      "Ritwik Raj Saxena"
    ],
    "abstract": "Concerns associated with occupational health and safety (OHS) remain critical\nand often under-addressed aspects of workforce management. This is especially\ntrue for high-risk industries such as manufacturing, construction, and mining.\nSuch industries dominate the economy of India which is a developing country\nwith a vast informal sector. Regulatory frameworks have been strengthened over\nthe decades, particularly with regards to bringing the unorganized sector\nwithin the purview of law. Traditional approaches to OHS have largely been\nreactive and rely on post-incident analysis (which is curative) rather than\npreventive intervention. This paper portrays the immense potential of\npredictive analytics in rejuvenating OHS practices in India. Intelligent\npredictive analytics is driven by approaches like machine learning and\nstatistical modeling. Its data-driven nature serves to overcome the limitations\nof conventional OHS methods. Predictive analytics approaches to OHS in India\ndraw on global case studies and generative applications of predictive analytics\nin OHS which are customized to Indian industrial contexts. This paper attempts\nto explore in what ways it exhibits the potential to address challenges such as\nfragmented data ecosystems, resource constraints, and the variability of\nworkplace hazards. The paper presents actionable policy recommendations to\ncreate conditions conducive to the widespread implementation of predictive\nanalytics, which must be advocated as a cornerstone of OHS strategy. In doing\nso, the paper aims to spark a collaborational dialogue among policymakers,\nindustry leaders, and technologists. It urges a shift towards intelligent\npractices to safeguard the well-being of India's workforce.",
    "categories": [
      "cs.CY",
      "cs.AI"
    ],
    "primary_category": "cs.CY",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.16038v3",
    "published_date": "2024-12-20 16:39:06 UTC",
    "updated_date": "2025-01-01 00:35:53 UTC"
  },
  {
    "arxiv_id": "2412.16032v1",
    "title": "A Framework for Streaming Event-Log Prediction in Business Processes",
    "authors": [
      "Benedikt Bollig",
      "Matthias Függer",
      "Thomas Nowak"
    ],
    "abstract": "We present a Python-based framework for event-log prediction in streaming\nmode, enabling predictions while data is being generated by a business process.\nThe framework allows for easy integration of streaming algorithms, including\nlanguage models like n-grams and LSTMs, and for combining these predictors\nusing ensemble methods.\n  Using our framework, we conducted experiments on various well-known\nprocess-mining data sets and compared classical batch with streaming mode.\nThough, in batch mode, LSTMs generally achieve the best performance, there is\noften an n-gram whose accuracy comes very close. Combining basic models in\nensemble methods can even outperform LSTMs. The value of basic models with\nrespect to LSTMs becomes even more apparent in streaming mode, where LSTMs\ngenerally lack accuracy in the early stages of a prediction run, while basic\nmethods make sensible predictions immediately.",
    "categories": [
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "18 pages",
    "pdf_url": "http://arxiv.org/pdf/2412.16032v1",
    "published_date": "2024-12-20 16:29:14 UTC",
    "updated_date": "2024-12-20 16:29:14 UTC"
  },
  {
    "arxiv_id": "2412.16022v1",
    "title": "The Only Way is Ethics: A Guide to Ethical Research with Large Language Models",
    "authors": [
      "Eddie L. Ungless",
      "Nikolas Vitsakis",
      "Zeerak Talat",
      "James Garforth",
      "Björn Ross",
      "Arno Onken",
      "Atoosa Kasirzadeh",
      "Alexandra Birch"
    ],
    "abstract": "There is a significant body of work looking at the ethical considerations of\nlarge language models (LLMs): critiquing tools to measure performance and\nharms; proposing toolkits to aid in ideation; discussing the risks to workers;\nconsidering legislation around privacy and security etc. As yet there is no\nwork that integrates these resources into a single practical guide that focuses\non LLMs; we attempt this ambitious goal. We introduce 'LLM Ethics Whitepaper',\nwhich we provide as an open and living resource for NLP practitioners, and\nthose tasked with evaluating the ethical implications of others' work. Our goal\nis to translate ethics literature into concrete recommendations and\nprovocations for thinking with clear first steps, aimed at computer scientists.\n'LLM Ethics Whitepaper' distils a thorough literature review into clear Do's\nand Don'ts, which we present also in this paper. We likewise identify useful\ntoolkits to support ethical work. We refer the interested reader to the full\nLLM Ethics Whitepaper, which provides a succinct discussion of ethical\nconsiderations at each stage in a project lifecycle, as well as citations for\nthe hundreds of papers from which we drew our recommendations. The present\npaper can be thought of as a pocket guide to conducting ethical research with\nLLMs.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted to COLING '25. This paper is the condensed pocket guide to\n  accompany our full LLM Ethics Whitepaper, available at arXiv:2410.19812, and\n  at https://github.com/MxEddie/Ethics-Whitepaper for suggested revisions",
    "pdf_url": "http://arxiv.org/pdf/2412.16022v1",
    "published_date": "2024-12-20 16:14:43 UTC",
    "updated_date": "2024-12-20 16:14:43 UTC"
  },
  {
    "arxiv_id": "2412.16003v2",
    "title": "Choose Your Explanation: A Comparison of SHAP and GradCAM in Human Activity Recognition",
    "authors": [
      "Felix Tempel",
      "Daniel Groos",
      "Espen Alexander F. Ihlen",
      "Lars Adde",
      "Inga Strümke"
    ],
    "abstract": "Explaining machine learning (ML) models using eXplainable AI (XAI) techniques\nhas become essential to make them more transparent and trustworthy. This is\nespecially important in high-stakes domains like healthcare, where\nunderstanding model decisions is critical to ensure ethical, sound, and\ntrustworthy outcome predictions. However, users are often confused about which\nexplanability method to choose for their specific use case. We present a\ncomparative analysis of widely used explainability methods, Shapley Additive\nExplanations (SHAP) and Gradient-weighted Class Activation Mapping (Grad-CAM),\nwithin the domain of human activity recognition (HAR) utilizing graph\nconvolutional networks (GCNs). By evaluating these methods on skeleton-based\ndata from two real-world datasets, including a healthcare-critical cerebral\npalsy (CP) case, this study provides vital insights into both approaches'\nstrengths, limitations, and differences, offering a roadmap for selecting the\nmost appropriate explanation method based on specific models and applications.\nWe quantitatively and quantitatively compare these methods, focusing on feature\nimportance ranking, interpretability, and model sensitivity through\nperturbation experiments. While SHAP provides detailed input feature\nattribution, Grad-CAM delivers faster, spatially oriented explanations, making\nboth methods complementary depending on the application's requirements. Given\nthe importance of XAI in enhancing trust and transparency in ML models,\nparticularly in sensitive environments like healthcare, our research\ndemonstrates how SHAP and Grad-CAM could complement each other to provide more\ninterpretable and actionable model explanations.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.16003v2",
    "published_date": "2024-12-20 15:53:25 UTC",
    "updated_date": "2025-04-13 08:22:33 UTC"
  },
  {
    "arxiv_id": "2412.15998v1",
    "title": "CNN-LSTM Hybrid Deep Learning Model for Remaining Useful Life Estimation",
    "authors": [
      "Muthukumar G",
      "Jyosna Philip"
    ],
    "abstract": "Remaining Useful Life (RUL) of a component or a system is defined as the\nlength from the current time to the end of the useful life. Accurate RUL\nestimation plays a crucial role in Predictive Maintenance applications.\nTraditional regression methods, both linear and non-linear, have struggled to\nachieve high accuracy in this domain. While Convolutional Neural Networks\n(CNNs) have shown improved accuracy, they often overlook the sequential nature\nof the data, relying instead on features derived from sliding windows. Since\nRUL prediction inherently involves multivariate time series analysis, robust\nsequence learning is essential. In this work, we propose a hybrid approach\ncombining Convolutional Neural Networks with Long Short-Term Memory (LSTM)\nnetworks for RUL estimation. Although CNN-based LSTM models have been applied\nto sequence prediction tasks in financial forecasting, this is the first\nattempt to adopt this approach for RUL estimation in prognostics. In this\napproach, CNN is first employed to efficiently extract features from the data,\nfollowed by LSTM, which uses these extracted features to predict RUL. This\nmethod effectively leverages sensor sequence information, uncovering hidden\npatterns within the data, even under multiple operating conditions and fault\nscenarios. Our results demonstrate that the hybrid CNN-LSTM model achieves the\nhighest accuracy, offering a superior score compared to the other methods.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "conference paper",
    "pdf_url": "http://arxiv.org/pdf/2412.15998v1",
    "published_date": "2024-12-20 15:48:57 UTC",
    "updated_date": "2024-12-20 15:48:57 UTC"
  },
  {
    "arxiv_id": "2412.15995v1",
    "title": "Data-Centric Improvements for Enhancing Multi-Modal Understanding in Spoken Conversation Modeling",
    "authors": [
      "Maximillian Chen",
      "Ruoxi Sun",
      "Sercan Ö. Arık"
    ],
    "abstract": "Conversational assistants are increasingly popular across diverse real-world\napplications, highlighting the need for advanced multimodal speech modeling.\nSpeech, as a natural mode of communication, encodes rich user-specific\ncharacteristics such as speaking rate and pitch, making it critical for\neffective interaction. Our work introduces a data-centric customization\napproach for efficiently enhancing multimodal understanding in conversational\nspeech modeling. Central to our contributions is a novel multi-task learning\nparadigm that involves designing auxiliary tasks to utilize a small amount of\nspeech data. Our approach achieves state-of-the-art performance on the\nSpoken-SQuAD benchmark, using only 10% of the training data with open-weight\nmodels, establishing a robust and efficient framework for audio-centric\nconversational modeling. We also introduce ASK-QA, the first dataset for\nmulti-turn spoken dialogue with ambiguous user requests and dynamic evaluation\ninputs. Code and data forthcoming.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.SD",
      "eess.AS"
    ],
    "primary_category": "cs.CL",
    "comment": "22 pages, 6 figures, 14 tables",
    "pdf_url": "http://arxiv.org/pdf/2412.15995v1",
    "published_date": "2024-12-20 15:43:09 UTC",
    "updated_date": "2024-12-20 15:43:09 UTC"
  },
  {
    "arxiv_id": "2412.15991v1",
    "title": "APIRL: Deep Reinforcement Learning for REST API Fuzzing",
    "authors": [
      "Myles Foley",
      "Sergio Maffeis"
    ],
    "abstract": "REST APIs have become key components of web services. However, they often\ncontain logic flaws resulting in server side errors or security\nvulnerabilities. HTTP requests are used as test cases to find and mitigate such\nissues. Existing methods to modify requests, including those using deep\nlearning, suffer from limited performance and precision, relying on undirected\nsearch or making limited usage of the contextual information. In this paper we\npropose APIRL, a fully automated deep reinforcement learning tool for testing\nREST APIs. A key novelty of our approach is the use of feedback from a\ntransformer module pre-trained on JSON-structured data, akin to that used in\nAPI responses. This allows APIRL to learn the subtleties relating to test\noutcomes, and generalise to unseen API endpoints. We show APIRL can find\nsignificantly more bugs than the state-of-the-art in real world REST APIs while\nminimising the number of required test cases. We also study how reward\nfunctions, and other key design choices, affect learnt policies in a thorough\nablation study.",
    "categories": [
      "cs.SE",
      "cs.AI",
      "cs.NI"
    ],
    "primary_category": "cs.SE",
    "comment": "Thirty-ninth Conference on Artificial Intelligence (AAAI 2025)",
    "pdf_url": "http://arxiv.org/pdf/2412.15991v1",
    "published_date": "2024-12-20 15:40:51 UTC",
    "updated_date": "2024-12-20 15:40:51 UTC"
  },
  {
    "arxiv_id": "2412.15983v1",
    "title": "Never Reset Again: A Mathematical Framework for Continual Inference in Recurrent Neural Networks",
    "authors": [
      "Bojian Yin",
      "Federico Corradi"
    ],
    "abstract": "Recurrent Neural Networks (RNNs) are widely used for sequential processing\nbut face fundamental limitations with continual inference due to state\nsaturation, requiring disruptive hidden state resets. However, reset-based\nmethods impose synchronization requirements with input boundaries and increase\ncomputational costs at inference. To address this, we propose an adaptive loss\nfunction that eliminates the need for resets during inference while preserving\nhigh accuracy over extended sequences. By combining cross-entropy and\nKullback-Leibler divergence, the loss dynamically modulates the gradient based\non input informativeness, allowing the network to differentiate meaningful data\nfrom noise and maintain stable representations over time. Experimental results\ndemonstrate that our reset-free approach outperforms traditional reset-based\nmethods when applied to a variety of RNNs, particularly in continual tasks,\nenhancing both the theoretical and practical capabilities of RNNs for streaming\napplications.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.15983v1",
    "published_date": "2024-12-20 15:24:28 UTC",
    "updated_date": "2024-12-20 15:24:28 UTC"
  },
  {
    "arxiv_id": "2412.16270v1",
    "title": "MetaScientist: A Human-AI Synergistic Framework for Automated Mechanical Metamaterial Design",
    "authors": [
      "Jingyuan Qi",
      "Zian Jia",
      "Minqian Liu",
      "Wangzhi Zhan",
      "Junkai Zhang",
      "Xiaofei Wen",
      "Jingru Gan",
      "Jianpeng Chen",
      "Qin Liu",
      "Mingyu Derek Ma",
      "Bangzheng Li",
      "Haohui Wang",
      "Adithya Kulkarni",
      "Muhao Chen",
      "Dawei Zhou",
      "Ling Li",
      "Wei Wang",
      "Lifu Huang"
    ],
    "abstract": "The discovery of novel mechanical metamaterials, whose properties are\ndominated by their engineered structures rather than chemical composition, is a\nknowledge-intensive and resource-demanding process. To accelerate the design of\nnovel metamaterials, we present MetaScientist, a human-in-the-loop system that\nintegrates advanced AI capabilities with expert oversight with two primary\nphases: (1) hypothesis generation, where the system performs complex reasoning\nto generate novel and scientifically sound hypotheses, supported with\ndomain-specific foundation models and inductive biases retrieved from existing\nliterature; (2) 3D structure synthesis, where a 3D structure is synthesized\nwith a novel 3D diffusion model based on the textual hypothesis and refined it\nwith a LLM-based refinement model to achieve better structure properties. At\neach phase, domain experts iteratively validate the system outputs, and provide\nfeedback and supplementary materials to ensure the alignment of the outputs\nwith scientific principles and human preferences. Through extensive evaluation\nfrom human scientists, MetaScientist is able to deliver novel and valid\nmechanical metamaterial designs that have the potential to be highly impactful\nin the metamaterial field.",
    "categories": [
      "cs.AI",
      "cs.HC"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.16270v1",
    "published_date": "2024-12-20 15:20:57 UTC",
    "updated_date": "2024-12-20 15:20:57 UTC"
  },
  {
    "arxiv_id": "2412.15967v1",
    "title": "Self-Supervised Radiograph Anatomical Region Classification -- How Clean Is Your Real-World Data?",
    "authors": [
      "Simon Langer",
      "Jessica Ritter",
      "Rickmer Braren",
      "Daniel Rueckert",
      "Paul Hager"
    ],
    "abstract": "Modern deep learning-based clinical imaging workflows rely on accurate labels\nof the examined anatomical region. Knowing the anatomical region is required to\nselect applicable downstream models and to effectively generate cohorts of high\nquality data for future medical and machine learning research efforts. However,\nthis information may not be available in externally sourced data or generally\ncontain data entry errors. To address this problem, we show the effectiveness\nof self-supervised methods such as SimCLR and BYOL as well as supervised\ncontrastive deep learning methods in assigning one of 14 anatomical region\nclasses in our in-house dataset of 48,434 skeletal radiographs. We achieve a\nstrong linear evaluation accuracy of 96.6% with a single model and 97.7% using\nan ensemble approach. Furthermore, only a few labeled instances (1% of the\ntraining set) suffice to achieve an accuracy of 92.2%, enabling usage in\nlow-label and thus low-resource scenarios. Our model can be used to correct\ndata entry mistakes: a follow-up analysis of the test set errors of our\nbest-performing single model by an expert radiologist identified 35% incorrect\nlabels and 11% out-of-domain images. When accounted for, the radiograph\nanatomical region labelling performance increased -- without and with an\nensemble, respectively -- to a theoretical accuracy of 98.0% and 98.8%.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "12 pages, 4 figures, 2 supplementary figures",
    "pdf_url": "http://arxiv.org/pdf/2412.15967v1",
    "published_date": "2024-12-20 15:07:55 UTC",
    "updated_date": "2024-12-20 15:07:55 UTC"
  },
  {
    "arxiv_id": "2412.15957v1",
    "title": "From General to Specific: Tailoring Large Language Models for Personalized Healthcare",
    "authors": [
      "Ruize Shi",
      "Hong Huang",
      "Wei Zhou",
      "Kehan Yin",
      "Kai Zhao",
      "Yun Zhao"
    ],
    "abstract": "The rapid development of large language models (LLMs) has transformed many\nindustries, including healthcare. However, previous medical LLMs have largely\nfocused on leveraging general medical knowledge to provide responses, without\naccounting for patient variability and lacking true personalization at the\nindividual level. To address this, we propose a novel method called\npersonalized medical language model (PMLM), which explores and optimizes\npersonalized LLMs through recommendation systems and reinforcement learning\n(RL). Specifically, by utilizing self-informed and peer-informed\npersonalization, PMLM captures changes in behaviors and preferences to design\ninitial personalized prompts tailored to individual needs. We further refine\nthese initial personalized prompts through RL, ultimately enhancing the\nprecision of LLM guidance. Notably, the personalized prompt are hard prompt,\nwhich grants PMLM high adaptability and reusability, allowing it to directly\nleverage high-quality proprietary LLMs. We evaluate PMLM using real-world\nobstetrics and gynecology data, and the experimental results demonstrate that\nPMLM achieves personalized responses, and it provides more refined and\nindividualized services, offering a potential way for personalized medical\nLLMs.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.IR"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.15957v1",
    "published_date": "2024-12-20 14:51:12 UTC",
    "updated_date": "2024-12-20 14:51:12 UTC"
  },
  {
    "arxiv_id": "2412.15948v1",
    "title": "Trust Calibration in IDEs: Paving the Way for Widespread Adoption of AI Refactoring",
    "authors": [
      "Markus Borg"
    ],
    "abstract": "In the software industry, the drive to add new features often overshadows the\nneed to improve existing code. Large Language Models (LLMs) offer a new\napproach to improving codebases at an unprecedented scale through AI-assisted\nrefactoring. However, LLMs come with inherent risks such as braking changes and\nthe introduction of security vulnerabilities. We advocate for encapsulating the\ninteraction with the models in IDEs and validating refactoring attempts using\ntrustworthy safeguards. However, equally important for the uptake of AI\nrefactoring is research on trust development. In this position paper, we\nposition our future work based on established models from research on human\nfactors in automation. We outline action research within CodeScene on\ndevelopment of 1) novel LLM safeguards and 2) user interaction that conveys an\nappropriate level of trust. The industry collaboration enables large-scale\nrepository analysis and A/B testing to continuously guide the design of our\nresearch interventions.",
    "categories": [
      "cs.SE",
      "cs.AI",
      "cs.HC"
    ],
    "primary_category": "cs.SE",
    "comment": "Accepted for publication in the Proc. of the 2nd Workshop on\n  Integrated Development Environments, 2025",
    "pdf_url": "http://arxiv.org/pdf/2412.15948v1",
    "published_date": "2024-12-20 14:44:11 UTC",
    "updated_date": "2024-12-20 14:44:11 UTC"
  },
  {
    "arxiv_id": "2412.15939v1",
    "title": "Reframing Image Difference Captioning with BLIP2IDC and Synthetic Augmentation",
    "authors": [
      "Gautier Evennou",
      "Antoine Chaffin",
      "Vivien Chappelier",
      "Ewa Kijak"
    ],
    "abstract": "The rise of the generative models quality during the past years enabled the\ngeneration of edited variations of images at an important scale. To counter the\nharmful effects of such technology, the Image Difference Captioning (IDC) task\naims to describe the differences between two images. While this task is\nsuccessfully handled for simple 3D rendered images, it struggles on real-world\nimages. The reason is twofold: the training data-scarcity, and the difficulty\nto capture fine-grained differences between complex images. To address those\nissues, we propose in this paper a simple yet effective framework to both adapt\nexisting image captioning models to the IDC task and augment IDC datasets. We\nintroduce BLIP2IDC, an adaptation of BLIP2 to the IDC task at low computational\ncost, and show it outperforms two-streams approaches by a significant margin on\nreal-world IDC datasets. We also propose to use synthetic augmentation to\nimprove the performance of IDC models in an agnostic fashion. We show that our\nsynthetic augmentation strategy provides high quality data, leading to a\nchallenging new dataset well-suited for IDC named Syned1.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "This paper has been accepted for the IEEE/CVF Winter Conference on\n  Applications of Computer Vision (WACV) 2025",
    "pdf_url": "http://arxiv.org/pdf/2412.15939v1",
    "published_date": "2024-12-20 14:32:56 UTC",
    "updated_date": "2024-12-20 14:32:56 UTC"
  },
  {
    "arxiv_id": "2412.15924v1",
    "title": "Watertox: The Art of Simplicity in Universal Attacks A Cross-Model Framework for Robust Adversarial Generation",
    "authors": [
      "Zhenghao Gao",
      "Shengjie Xu",
      "Meixi Chen",
      "Fangyao Zhao"
    ],
    "abstract": "Contemporary adversarial attack methods face significant limitations in\ncross-model transferability and practical applicability. We present Watertox,\nan elegant adversarial attack framework achieving remarkable effectiveness\nthrough architectural diversity and precision-controlled perturbations. Our\ntwo-stage Fast Gradient Sign Method combines uniform baseline perturbations\n($\\epsilon_1 = 0.1$) with targeted enhancements ($\\epsilon_2 = 0.4$). The\nframework leverages an ensemble of complementary architectures, from VGG to\nConvNeXt, synthesizing diverse perspectives through an innovative voting\nmechanism. Against state-of-the-art architectures, Watertox reduces model\naccuracy from 70.6% to 16.0%, with zero-shot attacks achieving up to 98.8%\naccuracy reduction against unseen architectures. These results establish\nWatertox as a significant advancement in adversarial methodologies, with\npromising applications in visual security systems and CAPTCHA generation.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CR"
    ],
    "primary_category": "cs.CV",
    "comment": "18 pages, 4 figures, 3 tables. Advances a novel method for generating\n  cross-model transferable adversarial perturbations through a two-stage FGSM\n  process and architectural ensemble voting mechanism",
    "pdf_url": "http://arxiv.org/pdf/2412.15924v1",
    "published_date": "2024-12-20 14:17:03 UTC",
    "updated_date": "2024-12-20 14:17:03 UTC"
  },
  {
    "arxiv_id": "2412.15921v2",
    "title": "Less is More: Towards Green Code Large Language Models via Unified Structural Pruning",
    "authors": [
      "Guang Yang",
      "Yu Zhou",
      "Xiangyu Zhang",
      "Wei Cheng",
      "Ke Liu",
      "Xiang Chen",
      "Terry Yue Zhuo",
      "Taolue Chen"
    ],
    "abstract": "The extensive application of Large Language Models (LLMs) in generative\ncoding tasks has raised concerns due to their high computational demands and\nenergy consumption. Unlike previous structural pruning methods designed for\nclassification models that deal with lowdimensional classification logits,\ngenerative Code LLMs produce high-dimensional token logit sequences, making\ntraditional pruning objectives inherently limited. Moreover, existing single\ncomponent pruning approaches further constrain the effectiveness when applied\nto generative Code LLMs. In response, we propose Flab-Pruner, an innovative\nunified structural pruning method that combines vocabulary, layer, and\nFeed-Forward Network (FFN) pruning. This approach effectively reduces model\nparameters while maintaining performance. Additionally, we introduce a\ncustomized code instruction data strategy for coding tasks to enhance the\nperformance recovery efficiency of the pruned model. Through extensive\nevaluations on three state-of-the-art Code LLMs across multiple generative\ncoding tasks, the results demonstrate that Flab-Pruner retains 97% of the\noriginal performance after pruning 22% of the parameters and achieves the same\nor even better performance after post-training. The pruned models exhibit\nsignificant improvements in storage, GPU usage, computational efficiency, and\nenvironmental impact, while maintaining well robustness. Our research provides\na sustainable solution for green software engineering and promotes the\nefficient deployment of LLMs in real-world generative coding intelligence\napplications.",
    "categories": [
      "cs.SE",
      "cs.AI"
    ],
    "primary_category": "cs.SE",
    "comment": "UNDER REVIEW",
    "pdf_url": "http://arxiv.org/pdf/2412.15921v2",
    "published_date": "2024-12-20 14:13:09 UTC",
    "updated_date": "2025-04-23 23:24:01 UTC"
  },
  {
    "arxiv_id": "2412.15908v2",
    "title": "Speedup Techniques for Switchable Temporal Plan Graph Optimization",
    "authors": [
      "He Jiang",
      "Muhan Lin",
      "Jiaoyang Li"
    ],
    "abstract": "Multi-Agent Path Finding (MAPF) focuses on planning collision-free paths for\nmultiple agents. However, during the execution of a MAPF plan, agents may\nencounter unexpected delays, which can lead to inefficiencies, deadlocks, or\neven collisions. To address these issues, the Switchable Temporal Plan Graph\nprovides a framework for finding an acyclic Temporal Plan Graph with the\nminimum execution cost under delays, ensuring deadlock- and collision-free\nexecution. Unfortunately, existing optimal algorithms, such as Mixed Integer\nLinear Programming and Graph-Based Switchable Edge Search (GSES), are often too\nslow for practical use. This paper introduces Improved GSES, which\nsignificantly accelerates GSES through four speedup techniques: stronger\nadmissible heuristics, edge grouping, prioritized branching, and incremental\nimplementation. Experiments conducted on four different map types with varying\nnumbers of agents demonstrate that Improved GSES consistently achieves over\ntwice the success rate of GSES and delivers up to a 30-fold speedup on\ninstances where both methods successfully find solutions.",
    "categories": [
      "cs.MA",
      "cs.AI",
      "cs.RO"
    ],
    "primary_category": "cs.MA",
    "comment": "Accepted by AAAI 2025. This version contains the appendix",
    "pdf_url": "http://arxiv.org/pdf/2412.15908v2",
    "published_date": "2024-12-20 13:59:15 UTC",
    "updated_date": "2025-01-12 01:03:35 UTC"
  },
  {
    "arxiv_id": "2412.15907v1",
    "title": "Development of a Large-scale Dataset of Chest Computed Tomography Reports in Japanese and a High-performance Finding Classification Model",
    "authors": [
      "Yosuke Yamagishi",
      "Yuta Nakamura",
      "Tomohiro Kikuchi",
      "Yuki Sonoda",
      "Hiroshi Hirakawa",
      "Shintaro Kano",
      "Satoshi Nakamura",
      "Shouhei Hanaoka",
      "Takeharu Yoshikawa",
      "Osamu Abe"
    ],
    "abstract": "Background: Recent advances in large language models highlight the need for\nhigh-quality multilingual medical datasets. While Japan leads globally in CT\nscanner deployment and utilization, the lack of large-scale Japanese radiology\ndatasets has hindered the development of specialized language models for\nmedical imaging analysis. Objective: To develop a comprehensive Japanese CT\nreport dataset through machine translation and establish a specialized language\nmodel for structured finding classification. Additionally, to create a\nrigorously validated evaluation dataset through expert radiologist review.\nMethods: We translated the CT-RATE dataset (24,283 CT reports from 21,304\npatients) into Japanese using GPT-4o mini. The training dataset consisted of\n22,778 machine-translated reports, while the validation dataset included 150\nradiologist-revised reports. We developed CT-BERT-JPN based on\n\"tohoku-nlp/bert-base-japanese-v3\" architecture for extracting 18 structured\nfindings from Japanese radiology reports. Results: Translation metrics showed\nstrong performance with BLEU scores of 0.731 and 0.690, and ROUGE scores\nranging from 0.770 to 0.876 for Findings and from 0.748 to 0.857 for Impression\nsections. CT-BERT-JPN demonstrated superior performance compared to GPT-4o in\n11 out of 18 conditions, including lymphadenopathy (+14.2%), interlobular\nseptal thickening (+10.9%), and atelectasis (+7.4%). The model maintained F1\nscores exceeding 0.95 in 14 out of 18 conditions and achieved perfect scores in\nfour conditions. Conclusions: Our study establishes a robust Japanese CT report\ndataset and demonstrates the effectiveness of a specialized language model for\nstructured finding classification. The hybrid approach of machine translation\nand expert validation enables the creation of large-scale medical datasets\nwhile maintaining high quality.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Dataset available at\n  https://huggingface.co/datasets/YYama0/CT-RATE-JPN",
    "pdf_url": "http://arxiv.org/pdf/2412.15907v1",
    "published_date": "2024-12-20 13:59:11 UTC",
    "updated_date": "2024-12-20 13:59:11 UTC"
  },
  {
    "arxiv_id": "2412.15904v3",
    "title": "What Are Step-Level Reward Models Rewarding? Counterintuitive Findings from MCTS-Boosted Mathematical Reasoning",
    "authors": [
      "Yiran Ma",
      "Zui Chen",
      "Tianqiao Liu",
      "Mi Tian",
      "Zhuo Liu",
      "Zitao Liu",
      "Weiqi Luo"
    ],
    "abstract": "Step-level reward models (SRMs) can significantly enhance mathematical\nreasoning performance through process supervision or step-level preference\nalignment based on reinforcement learning. The performance of SRMs is pivotal,\nas they serve as critical guidelines, ensuring that each step in the reasoning\nprocess is aligned with desired outcomes. Recently, AlphaZero-like methods,\nwhere Monte Carlo Tree Search (MCTS) is employed for automatic step-level\npreference annotation, have proven particularly effective. However, the precise\nmechanisms behind the success of SRMs remain largely unexplored. To address\nthis gap, this study delves into the counterintuitive aspects of SRMs,\nparticularly focusing on MCTS-based approaches. Our findings reveal that the\nremoval of natural language descriptions of thought processes has minimal\nimpact on the efficacy of SRMs. Furthermore, we demonstrate that SRMs are adept\nat assessing the complex logical coherence present in mathematical language\nwhile having difficulty in natural language. These insights provide a nuanced\nunderstanding of the core elements that drive effective step-level reward\nmodeling in mathematical reasoning. By shedding light on these mechanisms, this\nstudy offers valuable guidance for developing more efficient and streamlined\nSRMs, which can be achieved by focusing on the crucial parts of mathematical\nreasoning.",
    "categories": [
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "AAAI 2025",
    "pdf_url": "http://arxiv.org/pdf/2412.15904v3",
    "published_date": "2024-12-20 13:56:23 UTC",
    "updated_date": "2025-03-08 05:01:59 UTC"
  },
  {
    "arxiv_id": "2412.15902v1",
    "title": "On the Suitability of pre-trained foundational LLMs for Analysis in German Legal Education",
    "authors": [
      "Lorenz Wendlinger",
      "Christian Braun",
      "Abdullah Al Zubaer",
      "Simon Alexander Nonn",
      "Sarah Großkopf",
      "Christofer Fellicious",
      "Michael Granitzer"
    ],
    "abstract": "We show that current open-source foundational LLMs possess instruction\ncapability and German legal background knowledge that is sufficient for some\nlegal analysis in an educational context. However, model capability breaks down\nin very specific tasks, such as the classification of \"Gutachtenstil\" appraisal\nstyle components, or with complex contexts, such as complete legal opinions.\nEven with extended context and effective prompting strategies, they cannot\nmatch the Bag-of-Words baseline. To combat this, we introduce a Retrieval\nAugmented Generation based prompt example selection method that substantially\nimproves predictions in high data availability scenarios. We further evaluate\nthe performance of pre-trained LLMs on two standard tasks for argument mining\nand automated essay scoring and find it to be more adequate. Throughout,\npre-trained LLMs improve upon the baseline in scenarios with little or no\nlabeled data with Chain-of-Thought prompting further helping in the zero-shot\ncase.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "11 pages",
    "pdf_url": "http://arxiv.org/pdf/2412.15902v1",
    "published_date": "2024-12-20 13:54:57 UTC",
    "updated_date": "2024-12-20 13:54:57 UTC"
  },
  {
    "arxiv_id": "2412.15891v1",
    "title": "TelcoLM: collecting data, adapting, and benchmarking language models for the telecommunication domain",
    "authors": [
      "Camille Barboule",
      "Viet-Phi Huynh",
      "Adrien Bufort",
      "Yoan Chabot",
      "Géraldine Damnati",
      "Gwénolé Lecorvé"
    ],
    "abstract": "Despite outstanding processes in many tasks, Large Language Models (LLMs)\nstill lack accuracy when dealing with highly technical domains. Especially,\ntelecommunications (telco) is a particularly challenging domain due the large\namount of lexical, semantic and conceptual peculiarities. Yet, this domain\nholds many valuable use cases, directly linked to industrial needs. Hence, this\npaper studies how LLMs can be adapted to the telco domain. It reports our\neffort to (i) collect a massive corpus of domain-specific data (800M tokens,\n80K instructions), (ii) perform adaptation using various methodologies, and\n(iii) benchmark them against larger generalist models in downstream tasks that\nrequire extensive knowledge of telecommunications. Our experiments on\nLlama-2-7b show that domain-adapted models can challenge the large generalist\nmodels. They also suggest that adaptation can be restricted to a unique\ninstruction-tuning step, dicarding the need for any fine-tuning on raw texts\nbeforehand.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "30 pages (main: 13 pages, appendices: 17 pages), 1 figure, 22 tables,\n  achieved March 2024, released December 2024",
    "pdf_url": "http://arxiv.org/pdf/2412.15891v1",
    "published_date": "2024-12-20 13:47:02 UTC",
    "updated_date": "2024-12-20 13:47:02 UTC"
  },
  {
    "arxiv_id": "2412.15877v1",
    "title": "Approximate State Abstraction for Markov Games",
    "authors": [
      "Hiroki Ishibashi",
      "Kenshi Abe",
      "Atsushi Iwasaki"
    ],
    "abstract": "This paper introduces state abstraction for two-player zero-sum Markov games\n(TZMGs), where the payoffs for the two players are determined by the state\nrepresenting the environment and their respective actions, with state\ntransitions following Markov decision processes. For example, in games like\nsoccer, the value of actions changes according to the state of play, and thus\nsuch games should be described as Markov games. In TZMGs, as the number of\nstates increases, computing equilibria becomes more difficult. Therefore, we\nconsider state abstraction, which reduces the number of states by treating\nmultiple different states as a single state. There is a substantial body of\nresearch on finding optimal policies for Markov decision processes using state\nabstraction. However, in the multi-player setting, the game with state\nabstraction may yield different equilibrium solutions from those of the ground\ngame. To evaluate the equilibrium solutions of the game with state abstraction,\nwe derived bounds on the duality gap, which represents the distance from the\nequilibrium solutions of the ground game. Finally, we demonstrate our state\nabstraction with Markov Soccer, compute equilibrium policies, and examine the\nresults.",
    "categories": [
      "cs.GT",
      "cs.AI",
      "cs.MA"
    ],
    "primary_category": "cs.GT",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.15877v1",
    "published_date": "2024-12-20 13:28:41 UTC",
    "updated_date": "2024-12-20 13:28:41 UTC"
  },
  {
    "arxiv_id": "2412.15876v1",
    "title": "AI-in-the-loop: The future of biomedical visual analytics applications in the era of AI",
    "authors": [
      "Katja Bühler",
      "Thomas Höllt",
      "Thomas Schulz",
      "Pere-Pau Vázquez"
    ],
    "abstract": "AI is the workhorse of modern data analytics and omnipresent across many\nsectors. Large Language Models and multi-modal foundation models are today\ncapable of generating code, charts, visualizations, etc. How will these massive\ndevelopments of AI in data analytics shape future data visualizations and\nvisual analytics workflows? What is the potential of AI to reshape methodology\nand design of future visual analytics applications? What will be our role as\nvisualization researchers in the future? What are opportunities, open\nchallenges and threats in the context of an increasingly powerful AI? This\nVisualization Viewpoint discusses these questions in the special context of\nbiomedical data analytics as an example of a domain in which critical decisions\nare taken based on complex and sensitive data, with high requirements on\ntransparency, efficiency, and reliability. We map recent trends and\ndevelopments in AI on the elements of interactive visualization and visual\nanalytics workflows and highlight the potential of AI to transform biomedical\nvisualization as a research field. Given that agency and responsibility have to\nremain with human experts, we argue that it is helpful to keep the focus on\nhuman-centered workflows, and to use visual analytics as a tool for integrating\n``AI-in-the-loop''. This is in contrast to the more traditional term\n``human-in-the-loop'', which focuses on incorporating human expertise into\nAI-based systems.",
    "categories": [
      "cs.HC",
      "cs.AI",
      "cs.GR",
      "68U01",
      "H.1.2; H.5.2; I.3.6; I.2.1; J.3; D.2.0"
    ],
    "primary_category": "cs.HC",
    "comment": "Accepted for publication in IEEE Computer Graphics & Applications",
    "pdf_url": "http://arxiv.org/pdf/2412.15876v1",
    "published_date": "2024-12-20 13:27:24 UTC",
    "updated_date": "2024-12-20 13:27:24 UTC"
  },
  {
    "arxiv_id": "2502.02593v1",
    "title": "Reconstructing 3D Flow from 2D Data with Diffusion Transformer",
    "authors": [
      "Fan Lei"
    ],
    "abstract": "Fluid flow is a widely applied physical problem, crucial in various fields.\nDue to the highly nonlinear and chaotic nature of fluids, analyzing\nfluid-related problems is exceptionally challenging. Computational fluid\ndynamics (CFD) is the best tool for this analysis but involves significant\ncomputational resources, especially for 3D simulations, which are slow and\nresource-intensive. In experimental fluid dynamics, PIV cost increases with\ndimensionality. Reconstructing 3D flow fields from 2D PIV data could reduce\ncosts and expand application scenarios. Here, We propose a Diffusion\nTransformer-based method for reconstructing 3D flow fields from 2D flow data.\nBy embedding the positional information of 2D planes into the model, we enable\nthe reconstruction of 3D flow fields from any combination of 2D slices,\nenhancing flexibility. We replace global attention with window and plane\nattention to reduce computational costs associated with higher dimensions\nwithout compromising performance. Our experiments demonstrate that our model\ncan efficiently and accurately reconstruct 3D flow fields from 2D data,\nproducing realistic results.",
    "categories": [
      "cs.CE",
      "cs.AI",
      "physics.flu-dyn"
    ],
    "primary_category": "cs.CE",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.02593v1",
    "published_date": "2024-12-20 13:19:48 UTC",
    "updated_date": "2024-12-20 13:19:48 UTC"
  },
  {
    "arxiv_id": "2412.17859v1",
    "title": "The Unreasonable Effectiveness of Open Science in AI: A Replication Study",
    "authors": [
      "Odd Erik Gundersen",
      "Odd Cappelen",
      "Martin Mølnå",
      "Nicklas Grimstad Nilsen"
    ],
    "abstract": "A reproducibility crisis has been reported in science, but the extent to\nwhich it affects AI research is not yet fully understood. Therefore, we\nperformed a systematic replication study including 30 highly cited AI studies\nrelying on original materials when available. In the end, eight articles were\nrejected because they required access to data or hardware that was practically\nimpossible to acquire as part of the project. Six articles were successfully\nreproduced, while five were partially reproduced. In total, 50% of the articles\nincluded was reproduced to some extent. The availability of code and data\ncorrelate strongly with reproducibility, as 86% of articles that shared code\nand data were fully or partly reproduced, while this was true for 33% of\narticles that shared only data. The quality of the data documentation\ncorrelates with successful replication. Poorly documented or miss-specified\ndata will probably result in unsuccessful replication. Surprisingly, the\nquality of the code documentation does not correlate with successful\nreplication. Whether the code is poorly documented, partially missing, or not\nversioned is not important for successful replication, as long as the code is\nshared. This study emphasizes the effectiveness of open science and the\nimportance of properly documenting data work.",
    "categories": [
      "cs.AI",
      "cs.LG",
      "cs.SE"
    ],
    "primary_category": "cs.AI",
    "comment": "This paper has been accepted at AAAI 2025",
    "pdf_url": "http://arxiv.org/pdf/2412.17859v1",
    "published_date": "2024-12-20 12:33:27 UTC",
    "updated_date": "2024-12-20 12:33:27 UTC"
  },
  {
    "arxiv_id": "2412.15838v2",
    "title": "Align Anything: Training All-Modality Models to Follow Instructions with Language Feedback",
    "authors": [
      "Jiaming Ji",
      "Jiayi Zhou",
      "Hantao Lou",
      "Boyuan Chen",
      "Donghai Hong",
      "Xuyao Wang",
      "Wenqi Chen",
      "Kaile Wang",
      "Rui Pan",
      "Jiahao Li",
      "Mohan Wang",
      "Josef Dai",
      "Tianyi Qiu",
      "Hua Xu",
      "Dong Li",
      "Weipeng Chen",
      "Jun Song",
      "Bo Zheng",
      "Yaodong Yang"
    ],
    "abstract": "Reinforcement learning from human feedback (RLHF) has proven effective in\nenhancing the instruction-following capabilities of large language models;\nhowever, it remains underexplored in the cross-modality domain. As the number\nof modalities increases, aligning all-modality models with human intentions --\nsuch as instruction following -- becomes a pressing challenge. In this work, we\nmake the first attempt to fine-tune all-modality models (i.e. input and output\nwith any modality, also named any-to-any models) using human preference data\nacross all modalities (including text, image, audio, and video), ensuring its\nbehavior aligns with human intentions. This endeavor presents several\nchallenges. First, there is no large-scale all-modality human preference data\nin existing open-source resources, as most datasets are limited to specific\nmodalities, predominantly text and image. Secondly, the effectiveness of binary\npreferences in RLHF for post-training alignment in complex all-modality\nscenarios remains an unexplored area. Finally, there is a lack of a systematic\nframework to evaluate the capabilities of all-modality models, particularly\nregarding modality selection and synergy. To address these challenges, we\npropose the align-anything framework, which includes meticulously annotated\n200k all-modality human preference data. Then, we introduce an alignment method\nthat learns from unified language feedback, effectively capturing complex\nmodality-specific human preferences and enhancing the model's\ninstruction-following capabilities. Furthermore, to assess performance\nimprovements in all-modality models after post-training alignment, we construct\na challenging all-modality capability evaluation framework -- eval-anything.\nAll data, models, and code frameworks have been open-sourced for the community.\nFor more details, please refer to\nhttps://github.com/PKU-Alignment/align-anything.",
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.15838v2",
    "published_date": "2024-12-20 12:27:16 UTC",
    "updated_date": "2024-12-30 07:27:58 UTC"
  },
  {
    "arxiv_id": "2412.15837v1",
    "title": "Traffic-Rule-Compliant Trajectory Repair via Satisfiability Modulo Theories and Reachability Analysis",
    "authors": [
      "Yuanfei Lin",
      "Zekun Xing",
      "Xuyuan Han",
      "Matthias Althoff"
    ],
    "abstract": "Complying with traffic rules is challenging for automated vehicles, as\nnumerous rules need to be considered simultaneously. If a planned trajectory\nviolates traffic rules, it is common to replan a new trajectory from scratch.\nWe instead propose a trajectory repair technique to save computation time. By\ncoupling satisfiability modulo theories with set-based reachability analysis,\nwe determine if and in what manner the initial trajectory can be repaired.\nExperiments in high-fidelity simulators and in the real world demonstrate the\nbenefits of our proposed approach in various scenarios. Even in complex\nenvironments with intricate rules, we efficiently and reliably repair\nrule-violating trajectories, enabling automated vehicles to swiftly resume\nlegally safe operation in real-time.",
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "primary_category": "cs.RO",
    "comment": "2024 IEEE. Personal use of this material is permitted. Permission\n  from IEEE must be obtained for all other uses, in any current or future\n  media, including reprinting/republishing this material for advertising or\n  promotional purposes, creating new collective works, for resale or\n  redistribution to servers or lists, or reuse of any copyrighted component of\n  this work in other works",
    "pdf_url": "http://arxiv.org/pdf/2412.15837v1",
    "published_date": "2024-12-20 12:26:22 UTC",
    "updated_date": "2024-12-20 12:26:22 UTC"
  },
  {
    "arxiv_id": "2412.15822v1",
    "title": "S$^2$DN: Learning to Denoise Unconvincing Knowledge for Inductive Knowledge Graph Completion",
    "authors": [
      "Tengfei Ma",
      "Yujie Chen",
      "Liang Wang",
      "Xuan Lin",
      "Bosheng Song",
      "Xiangxiang Zeng"
    ],
    "abstract": "Inductive Knowledge Graph Completion (KGC) aims to infer missing facts\nbetween newly emerged entities within knowledge graphs (KGs), posing a\nsignificant challenge. While recent studies have shown promising results in\ninferring such entities through knowledge subgraph reasoning, they suffer from\n(i) the semantic inconsistencies of similar relations, and (ii) noisy\ninteractions inherent in KGs due to the presence of unconvincing knowledge for\nemerging entities. To address these challenges, we propose a Semantic\nStructure-aware Denoising Network (S$^2$DN) for inductive KGC. Our goal is to\nlearn adaptable general semantics and reliable structures to distill consistent\nsemantic knowledge while preserving reliable interactions within KGs.\nSpecifically, we introduce a semantic smoothing module over the enclosing\nsubgraphs to retain the universal semantic knowledge of relations. We\nincorporate a structure refining module to filter out unreliable interactions\nand offer additional knowledge, retaining robust structure surrounding target\nlinks. Extensive experiments conducted on three benchmark KGs demonstrate that\nS$^2$DN surpasses the performance of state-of-the-art models. These results\ndemonstrate the effectiveness of S$^2$DN in preserving semantic consistency and\nenhancing the robustness of filtering out unreliable interactions in\ncontaminated KGs.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.LG",
    "comment": "15 pages",
    "pdf_url": "http://arxiv.org/pdf/2412.15822v1",
    "published_date": "2024-12-20 12:03:33 UTC",
    "updated_date": "2024-12-20 12:03:33 UTC"
  },
  {
    "arxiv_id": "2412.15821v1",
    "title": "$π$-yalli: un nouveau corpus pour le nahuatl",
    "authors": [
      "Juan-Manuel Torres-Moreno",
      "Juan-José Guzmán-Landa",
      "Graham Ranger",
      "Martha Lorena Avendaño Garrido",
      "Miguel Figueroa-Saavedra",
      "Ligia Quintana-Torres",
      "Carlos-Emiliano González-Gallardo",
      "Elvys Linhares Pontes",
      "Patricia Velázquez Morales",
      "Luis-Gil Moreno Jiménez"
    ],
    "abstract": "The NAHU$^2$ project is a Franco-Mexican collaboration aimed at building the\n$\\pi$-YALLI corpus adapted to machine learning, which will subsequently be used\nto develop computer resources for the Nahuatl language. Nahuatl is a language\nwith few computational resources, even though it is a living language spoken by\naround 2 million people. We have decided to build $\\pi$-YALLI, a corpus that\nwill enable to carry out research on Nahuatl in order to develop Language\nModels (LM), whether dynamic or not, which will make it possible to in turn\nenable the development of Natural Language Processing (NLP) tools such as: a) a\ngrapheme unifier, b) a word segmenter, c) a POS grammatical analyser, d) a\ncontent-based Automatic Text Summarization; and possibly, e) a translator\ntranslator (probabilistic or learning-based).",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "9 pages, in French language, 2 figures",
    "pdf_url": "http://arxiv.org/pdf/2412.15821v1",
    "published_date": "2024-12-20 12:03:10 UTC",
    "updated_date": "2024-12-20 12:03:10 UTC"
  },
  {
    "arxiv_id": "2412.15803v1",
    "title": "WebLLM: A High-Performance In-Browser LLM Inference Engine",
    "authors": [
      "Charlie F. Ruan",
      "Yucheng Qin",
      "Xun Zhou",
      "Ruihang Lai",
      "Hongyi Jin",
      "Yixin Dong",
      "Bohan Hou",
      "Meng-Shiun Yu",
      "Yiyan Zhai",
      "Sudeep Agarwal",
      "Hangrui Cao",
      "Siyuan Feng",
      "Tianqi Chen"
    ],
    "abstract": "Advancements in large language models (LLMs) have unlocked remarkable\ncapabilities. While deploying these models typically requires server-grade GPUs\nand cloud-based inference, the recent emergence of smaller open-source models\nand increasingly powerful consumer devices have made on-device deployment\npractical. The web browser as a platform for on-device deployment is\nuniversally accessible, provides a natural agentic environment, and\nconveniently abstracts out the different backends from diverse device vendors.\nTo address this opportunity, we introduce WebLLM, an open-source JavaScript\nframework that enables high-performance LLM inference entirely within web\nbrowsers. WebLLM provides an OpenAI-style API for seamless integration into web\napplications, and leverages WebGPU for efficient local GPU acceleration and\nWebAssembly for performant CPU computation. With machine learning compilers\nMLC-LLM and Apache TVM, WebLLM leverages optimized WebGPU kernels, overcoming\nthe absence of performant WebGPU kernel libraries. Evaluations show that WebLLM\ncan retain up to 80% native performance on the same device, with room to\nfurther close the gap. WebLLM paves the way for universally accessible,\nprivacy-preserving, personalized, and locally powered LLM applications in web\nbrowsers. The code is available at: https://github.com/mlc-ai/web-llm.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.15803v1",
    "published_date": "2024-12-20 11:24:13 UTC",
    "updated_date": "2024-12-20 11:24:13 UTC"
  },
  {
    "arxiv_id": "2412.15801v1",
    "title": "Bi-directional Mapping of Morphology Metrics and 3D City Blocks for Enhanced Characterization and Generation of Urban Form",
    "authors": [
      "Chenyi Cai",
      "Biao Li",
      "Qiyan Zhang",
      "Xiao Wang",
      "Filip Biljecki",
      "Pieter Herthogs"
    ],
    "abstract": "Urban morphology, examining city spatial configurations, links urban design\nto sustainability. Morphology metrics play a fundamental role in\nperformance-driven computational urban design (CUD) which integrates urban form\ngeneration, performance evaluation and optimization. However, a critical gap\nremains between performance evaluation and complex urban form generation,\ncaused by the disconnection between morphology metrics and urban form,\nparticularly in metric-to-form workflows. It prevents the application of\noptimized metrics to generate improved urban form with enhanced urban\nperformance. Formulating morphology metrics that not only effectively\ncharacterize complex urban forms but also enable the reconstruction of diverse\nforms is of significant importance. This paper highlights the importance of\nestablishing a bi-directional mapping between morphology metrics and complex\nurban form to enable the integration of urban form generation with performance\nevaluation. We present an approach that can 1) formulate morphology metrics to\nboth characterize urban forms and in reverse, retrieve diverse similar 3D urban\nforms, and 2) evaluate the effectiveness of morphology metrics in representing\n3D urban form characteristics of blocks by comparison. We demonstrate the\nmethodology with 3D urban models of New York City, covering 14,248 blocks. We\nuse neural networks and information retrieval for morphology metric encoding,\nurban form clustering and morphology metric evaluation. We identified an\neffective set of morphology metrics for characterizing block-scale urban forms\nthrough comparison. The proposed methodology tightly couples complex urban\nforms with morphology metrics, hence it can enable a seamless and bidirectional\nrelationship between urban form generation and optimization in\nperformance-driven urban design towards sustainable urban design and planning.",
    "categories": [
      "cs.CE",
      "cs.AI"
    ],
    "primary_category": "cs.CE",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.15801v1",
    "published_date": "2024-12-20 11:22:55 UTC",
    "updated_date": "2024-12-20 11:22:55 UTC"
  },
  {
    "arxiv_id": "2412.15790v1",
    "title": "GraphSeqLM: A Unified Graph Language Framework for Omic Graph Learning",
    "authors": [
      "Heming Zhang",
      "Di Huang",
      "Yixin Chen",
      "Fuhai Li"
    ],
    "abstract": "The integration of multi-omic data is pivotal for understanding complex\ndiseases, but its high dimensionality and noise present significant challenges.\nGraph Neural Networks (GNNs) offer a robust framework for analyzing large-scale\nsignaling pathways and protein-protein interaction networks, yet they face\nlimitations in expressivity when capturing intricate biological relationships.\nTo address this, we propose Graph Sequence Language Model (GraphSeqLM), a\nframework that enhances GNNs with biological sequence embeddings generated by\nLarge Language Models (LLMs). These embeddings encode structural and biological\nproperties of DNA, RNA, and proteins, augmenting GNNs with enriched features\nfor analyzing sample-specific multi-omic data. By integrating topological,\nsequence-derived, and biological information, GraphSeqLM demonstrates superior\npredictive accuracy and outperforms existing methods, paving the way for more\neffective multi-omic data integration in precision medicine.",
    "categories": [
      "q-bio.QM",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "q-bio.QM",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.15790v1",
    "published_date": "2024-12-20 11:05:26 UTC",
    "updated_date": "2024-12-20 11:05:26 UTC"
  },
  {
    "arxiv_id": "2412.15772v1",
    "title": "Linguistic Features Extracted by GPT-4 Improve Alzheimer's Disease Detection based on Spontaneous Speech",
    "authors": [
      "Jonathan Heitz",
      "Gerold Schneider",
      "Nicolas Langer"
    ],
    "abstract": "Alzheimer's Disease (AD) is a significant and growing public health concern.\nInvestigating alterations in speech and language patterns offers a promising\npath towards cost-effective and non-invasive early detection of AD on a large\nscale. Large language models (LLMs), such as GPT, have enabled powerful new\npossibilities for semantic text analysis. In this study, we leverage GPT-4 to\nextract five semantic features from transcripts of spontaneous patient speech.\nThe features capture known symptoms of AD, but they are difficult to quantify\neffectively using traditional methods of computational linguistics. We\ndemonstrate the clinical significance of these features and further validate\none of them (\"Word-Finding Difficulties\") against a proxy measure and human\nraters. When combined with established linguistic features and a Random Forest\nclassifier, the GPT-derived features significantly improve the detection of AD.\nOur approach proves effective for both manually transcribed and automatically\ngenerated transcripts, representing a novel and impactful use of recent\nadvancements in LLMs for AD speech analysis.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted at the 31st International Conference on Computational\n  Linguistics (COLING 2025)",
    "pdf_url": "http://arxiv.org/pdf/2412.15772v1",
    "published_date": "2024-12-20 10:43:42 UTC",
    "updated_date": "2024-12-20 10:43:42 UTC"
  },
  {
    "arxiv_id": "2412.15748v1",
    "title": "Critique of Impure Reason: Unveiling the reasoning behaviour of medical Large Language Models",
    "authors": [
      "Shamus Sim",
      "Tyrone Chen"
    ],
    "abstract": "Background: Despite the current ubiquity of Large Language Models (LLMs)\nacross the medical domain, there is a surprising lack of studies which address\ntheir reasoning behaviour. We emphasise the importance of understanding\nreasoning behaviour as opposed to high-level prediction accuracies, since it is\nequivalent to explainable AI (XAI) in this context. In particular, achieving\nXAI in medical LLMs used in the clinical domain will have a significant impact\nacross the healthcare sector. Results: Therefore, we define the concept of\nreasoning behaviour in the specific context of medical LLMs. We then categorise\nand discuss the current state of the art of methods which evaluate reasoning\nbehaviour in medical LLMs. Finally, we propose theoretical frameworks which can\nempower medical professionals or machine learning engineers to gain insight\ninto the low-level reasoning operations of these previously obscure models.\nConclusion: The subsequent increased transparency and trust in medical machine\nlearning models by clinicians as well as patients will accelerate the\nintegration, application as well as further development of medical AI for the\nhealthcare system as a whole",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "16 pages, 5 figures, 2 tables. Conceptualization, both authors.\n  formal analysis, both authors. funding acquisition, both authors.\n  investigation, both authors. resources, both authors. supervision, T.C..\n  validation, both authors. visualization, both authors. writing original\n  draft, both authors. writing review and editing, both authors",
    "pdf_url": "http://arxiv.org/pdf/2412.15748v1",
    "published_date": "2024-12-20 10:06:52 UTC",
    "updated_date": "2024-12-20 10:06:52 UTC"
  },
  {
    "arxiv_id": "2412.16265v3",
    "title": "Autoware.Flex: Human-Instructed Dynamically Reconfigurable Autonomous Driving Systems",
    "authors": [
      "Ziwei Song",
      "Mingsong Lv",
      "Tianchi Ren",
      "Chun Jason Xue",
      "Jen-Ming Wu",
      "Nan Guan"
    ],
    "abstract": "Existing Autonomous Driving Systems (ADS) independently make driving\ndecisions, but they face two significant limitations. First, in complex\nscenarios, ADS may misinterpret the environment and make inappropriate driving\ndecisions. Second, these systems are unable to incorporate human driving\npreferences in their decision-making processes. This paper proposes\nAutoware$.$Flex, a novel ADS system that incorporates human input into the\ndriving process, allowing users to guide the ADS in making more appropriate\ndecisions and ensuring their preferences are satisfied. Achieving this needs to\naddress two key challenges: (1) translating human instructions, expressed in\nnatural language, into a format the ADS can understand, and (2) ensuring these\ninstructions are executed safely and consistently within the ADS' s\ndecision-making framework. For the first challenge, we employ a Large Language\nModel (LLM) assisted by an ADS-specialized knowledge base to enhance\ndomain-specific translation. For the second challenge, we design a validation\nmechanism to ensure that human instructions result in safe and consistent\ndriving behavior. Experiments conducted on both simulators and a real-world\nautonomous vehicle demonstrate that Autoware$.$Flex effectively interprets\nhuman instructions and executes them safely.",
    "categories": [
      "cs.AI",
      "cs.HC",
      "cs.RO"
    ],
    "primary_category": "cs.AI",
    "comment": "14 pages, 13 figures",
    "pdf_url": "http://arxiv.org/pdf/2412.16265v3",
    "published_date": "2024-12-20 10:06:11 UTC",
    "updated_date": "2025-02-14 09:12:12 UTC"
  },
  {
    "arxiv_id": "2412.15728v1",
    "title": "fluke: Federated Learning Utility frameworK for Experimentation and research",
    "authors": [
      "Mirko Polato"
    ],
    "abstract": "Since its inception in 2016, Federated Learning (FL) has been gaining\ntremendous popularity in the machine learning community. Several frameworks\nhave been proposed to facilitate the development of FL algorithms, but\nresearchers often resort to implementing their algorithms from scratch,\nincluding all baselines and experiments. This is because existing frameworks\nare not flexible enough to support their needs or the learning curve to extend\nthem is too steep. In this paper, we present \\fluke, a Python package designed\nto simplify the development of new FL algorithms. fluke is specifically\ndesigned for prototyping purposes and is meant for researchers or practitioners\nfocusing on the learning components of a federated system. fluke is\nopen-source, and it can be either used out of the box or extended with new\nalgorithms with minimal overhead.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted at FLUID workshop (AAAI 2025) [4 pages (+2 references), 2\n  figures, 1 algorithm]",
    "pdf_url": "http://arxiv.org/pdf/2412.15728v1",
    "published_date": "2024-12-20 09:51:23 UTC",
    "updated_date": "2024-12-20 09:51:23 UTC"
  },
  {
    "arxiv_id": "2412.15716v1",
    "title": "Towards Secure AI-driven Industrial Metaverse with NFT Digital Twins",
    "authors": [
      "Ravi Prakash",
      "Tony Thomas"
    ],
    "abstract": "The rise of the industrial metaverse has brought digital twins (DTs) to the\nforefront. Blockchain-powered non-fungible tokens (NFTs) offer a decentralized\napproach to creating and owning these cloneable DTs. However, the potential for\nunauthorized duplication, or counterfeiting, poses a significant threat to the\nsecurity of NFT-DTs. Existing NFT clone detection methods often rely on static\ninformation like metadata and images, which can be easily manipulated. To\naddress these limitations, we propose a novel deep-learning-based solution as a\ncombination of an autoencoder and RNN-based classifier. This solution enables\nreal-time pattern recognition to detect fake NFT-DTs. Additionally, we\nintroduce the concept of dynamic metadata, providing a more reliable way to\nverify authenticity through AI-integrated smart contracts. By effectively\nidentifying counterfeit DTs, our system contributes to strengthening the\nsecurity of NFT-based assets in the metaverse.",
    "categories": [
      "cs.CR",
      "cs.AI",
      "cs.HC"
    ],
    "primary_category": "cs.CR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.15716v1",
    "published_date": "2024-12-20 09:40:18 UTC",
    "updated_date": "2024-12-20 09:40:18 UTC"
  },
  {
    "arxiv_id": "2412.15714v2",
    "title": "AutoLife: Automatic Life Journaling with Smartphones and LLMs",
    "authors": [
      "Huatao Xu",
      "Panrong Tong",
      "Mo Li",
      "Mani Srivastava"
    ],
    "abstract": "This paper introduces a novel mobile sensing application - life journaling -\ndesigned to generate semantic descriptions of users' daily lives. We present\nAutoLife, an automatic life journaling system based on commercial smartphones.\nAutoLife only inputs low-cost sensor data (without photos or audio) from\nsmartphones and can automatically generate comprehensive life journals for\nusers. To achieve this, we first derive time, motion, and location contexts\nfrom multimodal sensor data, and harness the zero-shot capabilities of Large\nLanguage Models (LLMs), enriched with commonsense knowledge about human lives,\nto interpret diverse contexts and generate life journals. To manage the task\ncomplexity and long sensing duration, a multilayer framework is proposed, which\ndecomposes tasks and seamlessly integrates LLMs with other techniques for life\njournaling. This study establishes a real-life dataset as a benchmark and\nextensive experiment results demonstrate that AutoLife produces accurate and\nreliable life journals.",
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.HC"
    ],
    "primary_category": "cs.AI",
    "comment": "13 pages",
    "pdf_url": "http://arxiv.org/pdf/2412.15714v2",
    "published_date": "2024-12-20 09:37:02 UTC",
    "updated_date": "2024-12-23 10:45:32 UTC"
  },
  {
    "arxiv_id": "2412.15703v3",
    "title": "MacLight: Multi-scene Aggregation Convolutional Learning for Traffic Signal Control",
    "authors": [
      "Sunbowen Lee",
      "Hongqin Lyu",
      "Yicheng Gong",
      "Yingying Sun",
      "Chao Deng"
    ],
    "abstract": "Reinforcement learning methods have proposed promising traffic signal control\npolicy that can be trained on large road networks. Current SOTA methods model\nroad networks as topological graph structures, incorporate graph attention into\ndeep Q-learning, and merge local and global embeddings to improve policy.\nHowever, graph-based methods are difficult to parallelize, resulting in huge\ntime overhead. Moreover, none of the current peer studies have deployed dynamic\ntraffic systems for experiments, which is far from the actual situation.\n  In this context, we propose Multi-Scene Aggregation Convolutional Learning\nfor traffic signal control (MacLight), which offers faster training speeds and\nmore stable performance. Our approach consists of two main components. The\nfirst is the global representation, where we utilize variational autoencoders\nto compactly compress and extract the global representation. The second\ncomponent employs the proximal policy optimization algorithm as the backbone,\nallowing value evaluation to consider both local features and global embedding\nrepresentations. This backbone model significantly reduces time overhead and\nensures stability in policy updates. We validated our method across multiple\ntraffic scenarios under both static and dynamic traffic systems. Experimental\nresults demonstrate that, compared to general and domian SOTA methods, our\napproach achieves superior stability, optimized convergence levels and the\nhighest time efficiency. The code is under\nhttps://github.com/Aegis1863/MacLight.",
    "categories": [
      "cs.MA",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.MA",
    "comment": "Accepted as full paper by AAMAS2025",
    "pdf_url": "http://arxiv.org/pdf/2412.15703v3",
    "published_date": "2024-12-20 09:26:41 UTC",
    "updated_date": "2024-12-24 04:42:00 UTC"
  },
  {
    "arxiv_id": "2412.16264v3",
    "title": "Continual Learning with Strategic Selection and Forgetting for Network Intrusion Detection",
    "authors": [
      "Xinchen Zhang",
      "Running Zhao",
      "Zhihan Jiang",
      "Handi Chen",
      "Yulong Ding",
      "Edith C. H. Ngai",
      "Shuang-Hua Yang"
    ],
    "abstract": "Intrusion Detection Systems (IDS) are crucial for safeguarding digital\ninfrastructure. In dynamic network environments, both threat landscapes and\nnormal operational behaviors are constantly changing, resulting in concept\ndrift. While continuous learning mitigates the adverse effects of concept\ndrift, insufficient attention to drift patterns and excessive preservation of\noutdated knowledge can still hinder the IDS's adaptability. In this paper, we\npropose SSF (Strategic Selection and Forgetting), a novel continual learning\nmethod for IDS, providing continuous model updates with a constantly refreshed\nmemory buffer. Our approach features a strategic sample selection algorithm to\nselect representative new samples and a strategic forgetting mechanism to drop\noutdated samples. The proposed strategic sample selection algorithm prioritizes\nnew samples that cause the `drifted' pattern, enabling the model to better\nunderstand the evolving landscape. Additionally, we introduce strategic\nforgetting upon detecting significant drift by discarding outdated samples to\nfree up memory, allowing the incorporation of more recent data. SSF captures\nevolving patterns effectively and ensures the model is aligned with the change\nof data patterns, significantly enhancing the IDS's adaptability to concept\ndrift. The state-of-the-art performance of SSF on NSL-KDD and UNSW-NB15\ndatasets demonstrates its superior adaptability to concept drift for network\nintrusion detection. The code is released at\nhttps://github.com/xinchen930/SSF-Strategic-Selection-and-Forgetting.",
    "categories": [
      "cs.CR",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CR",
    "comment": "Accepted by IEEE International Conference on Computer Communications\n  (INFOCOM) 2025",
    "pdf_url": "http://arxiv.org/pdf/2412.16264v3",
    "published_date": "2024-12-20 09:22:07 UTC",
    "updated_date": "2025-02-14 12:15:37 UTC"
  },
  {
    "arxiv_id": "2412.15701v2",
    "title": "Collaborative Gym: A Framework for Enabling and Evaluating Human-Agent Collaboration",
    "authors": [
      "Yijia Shao",
      "Vinay Samuel",
      "Yucheng Jiang",
      "John Yang",
      "Diyi Yang"
    ],
    "abstract": "Recent advancements in language models (LMs) have sparked growing interest in\ndeveloping LM agents. While fully autonomous agents could excel in many\nscenarios, numerous use cases inherently require them to collaborate with\nhumans due to humans' latent preferences, domain expertise, or need for\ncontrol. To facilitate the study of human-agent collaboration, we present\nCollaborative Gym (Co-Gym), a general framework enabling asynchronous,\ntripartite interaction among agents, humans, and task environments. We\ninstantiate Co-Gym with three representative tasks in both simulated and\nreal-world conditions, and propose an evaluation framework that assesses both\nthe collaboration outcomes and processes. Our findings reveal that\ncollaborative agents consistently outperform their fully autonomous\ncounterparts in task performance within those delivered cases, achieving win\nrates of 86% in Travel Planning, 74% in Tabular Analysis, and 66% in Related\nWork when evaluated by real users. However, our study also highlights\nsignificant challenges in developing collaborative agents, requiring\nadvancements in core aspects of intelligence -- communication capabilities,\nsituational awareness, and balancing autonomy and human control.",
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.HC"
    ],
    "primary_category": "cs.AI",
    "comment": "Preprint. Work in progress",
    "pdf_url": "http://arxiv.org/pdf/2412.15701v2",
    "published_date": "2024-12-20 09:21:15 UTC",
    "updated_date": "2025-01-16 07:01:37 UTC"
  },
  {
    "arxiv_id": "2412.15700v2",
    "title": "AIR: Unifying Individual and Collective Exploration in Cooperative Multi-Agent Reinforcement Learning",
    "authors": [
      "Guangchong Zhou",
      "Zeren Zhang",
      "Guoliang Fan"
    ],
    "abstract": "Exploration in cooperative multi-agent reinforcement learning (MARL) remains\nchallenging for value-based agents due to the absence of an explicit policy.\nExisting approaches include individual exploration based on uncertainty towards\nthe system and collective exploration through behavioral diversity among\nagents. However, the introduction of additional structures often leads to\nreduced training efficiency and infeasible integration of these methods. In\nthis paper, we propose Adaptive exploration via Identity Recognition~(AIR),\nwhich consists of two adversarial components: a classifier that recognizes\nagent identities from their trajectories, and an action selector that\nadaptively adjusts the mode and degree of exploration. We theoretically prove\nthat AIR can facilitate both individual and collective exploration during\ntraining, and experiments also demonstrate the efficiency and effectiveness of\nAIR across various tasks.",
    "categories": [
      "cs.AI",
      "cs.LG",
      "cs.MA"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.15700v2",
    "published_date": "2024-12-20 09:18:30 UTC",
    "updated_date": "2024-12-30 09:00:55 UTC"
  },
  {
    "arxiv_id": "2412.15677v1",
    "title": "AI-generated Image Quality Assessment in Visual Communication",
    "authors": [
      "Yu Tian",
      "Yixuan Li",
      "Baoliang Chen",
      "Hanwei Zhu",
      "Shiqi Wang",
      "Sam Kwong"
    ],
    "abstract": "Assessing the quality of artificial intelligence-generated images (AIGIs)\nplays a crucial role in their application in real-world scenarios. However,\ntraditional image quality assessment (IQA) algorithms primarily focus on\nlow-level visual perception, while existing IQA works on AIGIs overemphasize\nthe generated content itself, neglecting its effectiveness in real-world\napplications. To bridge this gap, we propose AIGI-VC, a quality assessment\ndatabase for AI-Generated Images in Visual Communication, which studies the\ncommunicability of AIGIs in the advertising field from the perspectives of\ninformation clarity and emotional interaction. The dataset consists of 2,500\nimages spanning 14 advertisement topics and 8 emotion types. It provides\ncoarse-grained human preference annotations and fine-grained preference\ndescriptions, benchmarking the abilities of IQA methods in preference\nprediction, interpretation, and reasoning. We conduct an empirical study of\nexisting representative IQA methods and large multi-modal models on the AIGI-VC\ndataset, uncovering their strengths and weaknesses.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "AAAI-2025; Project page: https://github.com/ytian73/AIGI-VC",
    "pdf_url": "http://arxiv.org/pdf/2412.15677v1",
    "published_date": "2024-12-20 08:47:07 UTC",
    "updated_date": "2024-12-20 08:47:07 UTC"
  },
  {
    "arxiv_id": "2412.16262v1",
    "title": "VirusT5: Harnessing Large Language Models to Predicting SARS-CoV-2 Evolution",
    "authors": [
      "Vishwajeet Marathe",
      "Deewan Bajracharya",
      "Changhui Yan"
    ],
    "abstract": "During a virus's evolution,various regions of the genome are subjected to\ndistinct levels of functional constraints.Combined with factors like codon bias\nand DNA repair efficiency,these constraints contribute to unique mutation\npatterns within the genome or a specific gene. In this project, we harnessed\nthe power of Large Language Models(LLMs) to predict the evolution of\nSARS-CoV-2. By treating the mutation process from one generation to the next as\na translation task, we trained a transformer model, called VirusT5, to capture\nthe mutation patterns underlying SARS-CoV-2 evolution. We evaluated the\nVirusT5's ability to detect these mutation patterns including its ability to\nidentify mutation hotspots and explored the potential of using VirusT5 to\npredict future virus variants. Our findings demonstrate the feasibility of\nusing a large language model to model viral evolution as a translation process.\nThis study establishes the groundbreaking concept of \"mutation-as-translation,\"\npaving the way for new methodologies and tools for combating virus threats",
    "categories": [
      "q-bio.QM",
      "cs.AI"
    ],
    "primary_category": "q-bio.QM",
    "comment": "This is a preprint of a paper submitted to IEEE for consideration",
    "pdf_url": "http://arxiv.org/pdf/2412.16262v1",
    "published_date": "2024-12-20 08:46:42 UTC",
    "updated_date": "2024-12-20 08:46:42 UTC"
  },
  {
    "arxiv_id": "2412.15660v1",
    "title": "Adaptable and Precise: Enterprise-Scenario LLM Function-Calling Capability Training Pipeline",
    "authors": [
      "Guancheng Zeng",
      "Wentao Ding",
      "Beining Xu",
      "Chi Zhang",
      "Wenqiang Han",
      "Gang Li",
      "Jingjing Mo",
      "Pengxu Qiu",
      "Xinran Tao",
      "Wang Tao",
      "Haowen Hu"
    ],
    "abstract": "Enterprises possess a vast array of API assets scattered across various\nfunctions, forming the backbone of existing business processes. By leveraging\nthese APIs as functional tools, enterprises can design diverse,\nscenario-specific agent applications, driven by on-premise function-calling\nmodels as the core engine. However, generic models often fail to meet\nenterprise requirements in terms of computational efficiency, output accuracy,\nand stability, necessitating scenario-specific adaptation. In this paper, we\npropose a training pipeline for function-calling capabilities tailored to\nreal-world business scenarios. This pipeline includes the synthesis and\naugmentation of scenario-specific function-calling data, model fine-tuning, and\nperformance evaluation and analysis. Using this pipeline, we generated 1,260\nfully AI-generated samples and 1,035 augmented manually-labeled samples in\ndigital HR agent scenario. The Qwen2.5-Coder-7B-Instruct model was employed as\nthe base model and fine-tuned using the LoRA method on four GPUs with 24GB\nVRAM. Our fine-tuned model demonstrated outstanding performance in evaluations\nand practical applications, surpassing GPT-4 and GPT-4o in accuracy on the test\nset. These results validate the reliability of the proposed pipeline for\ntraining scenario-specific function-calling models.",
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.SE"
    ],
    "primary_category": "cs.AI",
    "comment": "23 pages, 6 figures, 7 tables",
    "pdf_url": "http://arxiv.org/pdf/2412.15660v1",
    "published_date": "2024-12-20 08:20:21 UTC",
    "updated_date": "2024-12-20 08:20:21 UTC"
  },
  {
    "arxiv_id": "2412.15655v3",
    "title": "MathSpeech: Leveraging Small LMs for Accurate Conversion in Mathematical Speech-to-Formula",
    "authors": [
      "Sieun Hyeon",
      "Kyudan Jung",
      "Jaehee Won",
      "Nam-Joon Kim",
      "Hyun Gon Ryu",
      "Hyuk-Jae Lee",
      "Jaeyoung Do"
    ],
    "abstract": "In various academic and professional settings, such as mathematics lectures\nor research presentations, it is often necessary to convey mathematical\nexpressions orally. However, reading mathematical expressions aloud without\naccompanying visuals can significantly hinder comprehension, especially for\nthose who are hearing-impaired or rely on subtitles due to language barriers.\nFor instance, when a presenter reads Euler's Formula, current Automatic Speech\nRecognition (ASR) models often produce a verbose and error-prone textual\ndescription (e.g., e to the power of i x equals cosine of x plus i\n$\\textit{side}$ of x), instead of the concise $\\LaTeX{}$ format (i.e., $ e^{ix}\n= \\cos(x) + i\\sin(x) $), which hampers clear understanding and communication.\nTo address this issue, we introduce MathSpeech, a novel pipeline that\nintegrates ASR models with small Language Models (sLMs) to correct errors in\nmathematical expressions and accurately convert spoken expressions into\nstructured $\\LaTeX{}$ representations. Evaluated on a new dataset derived from\nlecture recordings, MathSpeech demonstrates $\\LaTeX{}$ generation capabilities\ncomparable to leading commercial Large Language Models (LLMs), while leveraging\nfine-tuned small language models of only 120M parameters. Specifically, in\nterms of CER, BLEU, and ROUGE scores for $\\LaTeX{}$ translation, MathSpeech\ndemonstrated significantly superior capabilities compared to GPT-4o. We\nobserved a decrease in CER from 0.390 to 0.298, and higher ROUGE/BLEU scores\ncompared to GPT-4o.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted at AAAI 2025",
    "pdf_url": "http://arxiv.org/pdf/2412.15655v3",
    "published_date": "2024-12-20 08:13:05 UTC",
    "updated_date": "2025-04-11 04:17:44 UTC"
  },
  {
    "arxiv_id": "2502.15690v1",
    "title": "Level-Navi Agent: A Framework and benchmark for Chinese Web Search Agents",
    "authors": [
      "Chuanrui Hu",
      "Shichong Xie",
      "Baoxin Wang",
      "Bin Chen",
      "Xiaofeng Cong",
      "Jun Zhang"
    ],
    "abstract": "Large language models (LLMs), adopted to understand human language, drive the\ndevelopment of artificial intelligence (AI) web search agents. Compared to\ntraditional search engines, LLM-powered AI search agents are capable of\nunderstanding and responding to complex queries with greater depth, enabling\nmore accurate operations and better context recognition. However, little\nattention and effort has been paid to the Chinese web search, which results in\nthat the capabilities of open-source models have not been uniformly and fairly\nevaluated. The difficulty lies in lacking three aspects: an unified agent\nframework, an accurately labeled dataset, and a suitable evaluation metric. To\naddress these issues, we propose a general-purpose and training-free web search\nagent by level-aware navigation, Level-Navi Agent, accompanied by a\nwell-annotated dataset (Web24) and a suitable evaluation metric. Level-Navi\nAgent can think through complex user questions and conduct searches across\nvarious levels on the internet to gather information for questions. Meanwhile,\nwe provide a comprehensive evaluation of state-of-the-art LLMs under fair\nsettings. To further facilitate future research, source code is available at\nGithub.",
    "categories": [
      "cs.IR",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.IR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.15690v1",
    "published_date": "2024-12-20 08:03:12 UTC",
    "updated_date": "2024-12-20 08:03:12 UTC"
  },
  {
    "arxiv_id": "2412.15639v2",
    "title": "Tacit Learning with Adaptive Information Selection for Cooperative Multi-Agent Reinforcement Learning",
    "authors": [
      "Lunjun Liu",
      "Weilai Jiang",
      "Yaonan Wang"
    ],
    "abstract": "In multi-agent reinforcement learning (MARL), the centralized training with\ndecentralized execution (CTDE) framework has gained widespread adoption due to\nits strong performance. However, the further development of CTDE faces two key\nchallenges. First, agents struggle to autonomously assess the relevance of\ninput information for cooperative tasks, impairing their decision-making\nabilities. Second, in communication-limited scenarios with partial\nobservability, agents are unable to access global information, restricting\ntheir ability to collaborate effectively from a global perspective. To address\nthese challenges, we introduce a novel cooperative MARL framework based on\ninformation selection and tacit learning. In this framework, agents gradually\ndevelop implicit coordination during training, enabling them to infer the\ncooperative behavior of others in a discrete space without communication,\nrelying solely on local information. Moreover, we integrate gating and\nselection mechanisms, allowing agents to adaptively filter information based on\nenvironmental changes, thereby enhancing their decision-making capabilities.\nExperiments on popular MARL benchmarks show that our framework can be\nseamlessly integrated with state-of-the-art algorithms, leading to significant\nperformance improvements.",
    "categories": [
      "cs.MA",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.MA",
    "comment": "Accepted by AAMAS 2025 (Extended Abstract)",
    "pdf_url": "http://arxiv.org/pdf/2412.15639v2",
    "published_date": "2024-12-20 07:55:59 UTC",
    "updated_date": "2024-12-24 06:05:22 UTC"
  },
  {
    "arxiv_id": "2412.15623v1",
    "title": "JailPO: A Novel Black-box Jailbreak Framework via Preference Optimization against Aligned LLMs",
    "authors": [
      "Hongyi Li",
      "Jiawei Ye",
      "Jie Wu",
      "Tianjie Yan",
      "Chu Wang",
      "Zhixin Li"
    ],
    "abstract": "Large Language Models (LLMs) aligned with human feedback have recently\ngarnered significant attention. However, it remains vulnerable to jailbreak\nattacks, where adversaries manipulate prompts to induce harmful outputs.\nExploring jailbreak attacks enables us to investigate the vulnerabilities of\nLLMs and further guides us in enhancing their security. Unfortunately, existing\ntechniques mainly rely on handcrafted templates or generated-based\noptimization, posing challenges in scalability, efficiency and universality. To\naddress these issues, we present JailPO, a novel black-box jailbreak framework\nto examine LLM alignment. For scalability and universality, JailPO meticulously\ntrains attack models to automatically generate covert jailbreak prompts.\nFurthermore, we introduce a preference optimization-based attack method to\nenhance the jailbreak effectiveness, thereby improving efficiency. To analyze\nmodel vulnerabilities, we provide three flexible jailbreak patterns. Extensive\nexperiments demonstrate that JailPO not only automates the attack process while\nmaintaining effectiveness but also exhibits superior performance in efficiency,\nuniversality, and robustness against defenses compared to baselines.\nAdditionally, our analysis of the three JailPO patterns reveals that attacks\nbased on complex templates exhibit higher attack strength, whereas covert\nquestion transformations elicit riskier responses and are more likely to bypass\ndefense mechanisms.",
    "categories": [
      "cs.CR",
      "cs.AI"
    ],
    "primary_category": "cs.CR",
    "comment": "Accepted by AAAI 2025",
    "pdf_url": "http://arxiv.org/pdf/2412.15623v1",
    "published_date": "2024-12-20 07:29:10 UTC",
    "updated_date": "2024-12-20 07:29:10 UTC"
  },
  {
    "arxiv_id": "2412.15620v1",
    "title": "Modeling Autonomous Shifts Between Focus State and Mind-Wandering Using a Predictive-Coding-Inspired Variational RNN Model",
    "authors": [
      "Henrique Oyama",
      "Jun Tani"
    ],
    "abstract": "The current study investigates possible neural mechanisms underling\nautonomous shifts between focus state and mind-wandering by conducting model\nsimulation experiments. On this purpose, we modeled perception processes of\ncontinuous sensory sequences using our previous proposed variational RNN model\nwhich was developed based on the free energy principle. The current study\nextended this model by introducing an adaptation mechanism of a meta-level\nparameter, referred to as the meta-prior $\\mathbf{w}$, which regulates the\ncomplexity term in the free energy. Our simulation experiments demonstrated\nthat autonomous shifts between focused perception and mind-wandering take place\nwhen $\\mathbf{w}$ switches between low and high values associated with decrease\nand increase of the average reconstruction error over the past window. In\nparticular, high $\\mathbf{w}$ prioritized top-down predictions while low\n$\\mathbf{w}$ emphasized bottom-up sensations. This paper explores how our\nexperiment results align with existing studies and highlights their potential\nfor future research.",
    "categories": [
      "q-bio.NC",
      "cs.AI"
    ],
    "primary_category": "q-bio.NC",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.15620v1",
    "published_date": "2024-12-20 07:25:20 UTC",
    "updated_date": "2024-12-20 07:25:20 UTC"
  },
  {
    "arxiv_id": "2412.15619v2",
    "title": "Understanding Individual Agent Importance in Multi-Agent System via Counterfactual Reasoning",
    "authors": [
      "Jianming Chen",
      "Yawen Wang",
      "Junjie Wang",
      "Xiaofei Xie",
      "jun Hu",
      "Qing Wang",
      "Fanjiang Xu"
    ],
    "abstract": "Explaining multi-agent systems (MAS) is urgent as these systems become\nincreasingly prevalent in various applications. Previous work has proveided\nexplanations for the actions or states of agents, yet falls short in\nunderstanding the black-boxed agent's importance within a MAS and the overall\nteam strategy. To bridge this gap, we propose EMAI, a novel agent-level\nexplanation approach that evaluates the individual agent's importance. Inspired\nby counterfactual reasoning, a larger change in reward caused by the randomized\naction of agent indicates its higher importance. We model it as a MARL problem\nto capture interactions across agents. Utilizing counterfactual reasoning, EMAI\nlearns the masking agents to identify important agents. Specifically, we define\nthe optimization function to minimize the reward difference before and after\naction randomization and introduce sparsity constraints to encourage the\nexploration of more action randomization of agents during training. The\nexperimental results in seven multi-agent tasks demonstratee that EMAI achieves\nhigher fidelity in explanations than baselines and provides more effective\nguidance in practical applications concerning understanding policies, launching\nattacks, and patching policies.",
    "categories": [
      "cs.AI",
      "cs.MA"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.15619v2",
    "published_date": "2024-12-20 07:24:43 UTC",
    "updated_date": "2024-12-23 01:56:56 UTC"
  },
  {
    "arxiv_id": "2412.16257v2",
    "title": "PromptLA: Towards Integrity Verification of Black-box Text-to-Image Diffusion Models",
    "authors": [
      "Zhuomeng Zhang",
      "Fangqi Li",
      "Chong Di",
      "Hongyu Zhu",
      "Hanyi Wang",
      "Shilin Wang"
    ],
    "abstract": "Despite the impressive synthesis quality of text-to-image (T2I) diffusion\nmodels, their black-box deployment poses significant regulatory challenges:\nMalicious actors can fine-tune these models to generate illegal content,\ncircumventing existing safeguards through parameter manipulation. Therefore, it\nis essential to verify the integrity of T2I diffusion models. To this end,\nconsidering the randomness within the outputs of generative models and the high\ncosts in interacting with them, we discern model tampering via the KL\ndivergence between the distributions of the features of generated images. We\npropose a novel prompt selection algorithm based on learning automaton\n(PromptLA) for efficient and accurate verification. Evaluations on four\nadvanced T2I models (e.g., SDXL, FLUX.1) demonstrate that our method achieves a\nmean AUC of over 0.96 in integrity detection, exceeding baselines by more than\n0.2, showcasing strong effectiveness and generalization. Additionally, our\napproach achieves lower cost and is robust against image-level post-processing.\nTo the best of our knowledge, this paper is the first work addressing the\nintegrity verification of T2I diffusion models, which establishes quantifiable\nstandards for AI copyright litigation in practice.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CR"
    ],
    "primary_category": "cs.CV",
    "comment": "9 pages, 6 figures",
    "pdf_url": "http://arxiv.org/pdf/2412.16257v2",
    "published_date": "2024-12-20 07:24:32 UTC",
    "updated_date": "2025-03-28 08:39:46 UTC"
  },
  {
    "arxiv_id": "2412.15616v1",
    "title": "Microservices-Based Framework for Predictive Analytics and Real-time Performance Enhancement in Travel Reservation Systems",
    "authors": [
      "Biman Barua",
      "M. Shamim Kaiser"
    ],
    "abstract": "The paper presents a framework of microservices-based architecture dedicated\nto enhancing the performance of real-time travel reservation systems using the\npower of predictive analytics. Traditional monolithic systems are bad at\nscaling and performing with high loads, causing backup resources to be\nunderutilized along with delays. To overcome the above-stated problems, we\nadopt a modularization approach in decoupling system components into\nindependent services that can grow or shrink according to demand. Our framework\nalso includes real-time predictive analytics, through machine learning models,\nthat optimize forecasting customer demand, dynamic pricing, as well as system\nperformance. With an experimental evaluation applying the approach, we could\nshow that the framework impacts metrics of performance such as response time,\nthroughput, transaction rate of success, and prediction accuracy compared to\ntheir conventional counterparts. Not only does the microservices approach\nimprove scalability and fault tolerance like a usual architecture, but it also\nbrings along timely and accurate predictions, which imply a greater customer\nsatisfaction and efficiency of operation. The integration of real-time\nanalytics would lead to more intelligent decision-making, thereby improving the\nresponse of the system along with the reliability it holds. A scalable,\nefficient framework is offered by such a system to address the modern\nchallenges imposed by any form of travel reservation system while considering\nother complex, data-driven industries as future applications. Future work will\nbe an investigation of advanced AI models and edge processing to further\nimprove the performance and robustness of the systems employed.",
    "categories": [
      "cs.IT",
      "cs.AI",
      "cs.CE",
      "cs.LG",
      "math.IT"
    ],
    "primary_category": "cs.IT",
    "comment": "10 Pages, 05 figures",
    "pdf_url": "http://arxiv.org/pdf/2412.15616v1",
    "published_date": "2024-12-20 07:19:42 UTC",
    "updated_date": "2024-12-20 07:19:42 UTC"
  },
  {
    "arxiv_id": "2412.16256v1",
    "title": "Aria-UI: Visual Grounding for GUI Instructions",
    "authors": [
      "Yuhao Yang",
      "Yue Wang",
      "Dongxu Li",
      "Ziyang Luo",
      "Bei Chen",
      "Chao Huang",
      "Junnan Li"
    ],
    "abstract": "Digital agents for automating tasks across different platforms by directly\nmanipulating the GUIs are increasingly important. For these agents, grounding\nfrom language instructions to target elements remains a significant challenge\ndue to reliance on HTML or AXTree inputs. In this paper, we introduce Aria-UI,\na large multimodal model specifically designed for GUI grounding. Aria-UI\nadopts a pure-vision approach, eschewing reliance on auxiliary inputs. To adapt\nto heterogeneous planning instructions, we propose a scalable data pipeline\nthat synthesizes diverse and high-quality instruction samples for grounding. To\nhandle dynamic contexts in task performing, Aria-UI incorporates textual and\ntext-image interleaved action histories, enabling robust context-aware\nreasoning for grounding. Aria-UI sets new state-of-the-art results across\noffline and online agent benchmarks, outperforming both vision-only and\nAXTree-reliant baselines. We release all training data and model checkpoints to\nfoster further research at https://ariaui.github.io.",
    "categories": [
      "cs.HC",
      "cs.AI"
    ],
    "primary_category": "cs.HC",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.16256v1",
    "published_date": "2024-12-20 07:16:57 UTC",
    "updated_date": "2024-12-20 07:16:57 UTC"
  },
  {
    "arxiv_id": "2412.15610v1",
    "title": "A Fusion Approach of Dependency Syntax and Sentiment Polarity for Feature Label Extraction in Commodity Reviews",
    "authors": [
      "Jianfei Xu"
    ],
    "abstract": "This study analyzes 13,218 product reviews from JD.com, covering four\ncategories: mobile phones, computers, cosmetics, and food. A novel method for\nfeature label extraction is proposed by integrating dependency parsing and\nsentiment polarity analysis. The proposed method addresses the challenges of\nlow robustness in existing extraction algorithms and significantly enhances\nextraction accuracy. Experimental results show that the method achieves an\naccuracy of 0.7, with recall and F-score both stabilizing at 0.8, demonstrating\nits effectiveness. However, challenges such as dependence on matching\ndictionaries and the limited scope of extracted feature tags require further\ninvestigation in future research.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.15610v1",
    "published_date": "2024-12-20 07:07:18 UTC",
    "updated_date": "2024-12-20 07:07:18 UTC"
  },
  {
    "arxiv_id": "2412.15606v2",
    "title": "Multi-modal Agent Tuning: Building a VLM-Driven Agent for Efficient Tool Usage",
    "authors": [
      "Zhi Gao",
      "Bofei Zhang",
      "Pengxiang Li",
      "Xiaojian Ma",
      "Tao Yuan",
      "Yue Fan",
      "Yuwei Wu",
      "Yunde Jia",
      "Song-Chun Zhu",
      "Qing Li"
    ],
    "abstract": "The advancement of large language models (LLMs) prompts the development of\nmulti-modal agents, which are used as a controller to call external tools,\nproviding a feasible way to solve practical tasks. In this paper, we propose a\nmulti-modal agent tuning method that automatically generates multi-modal\ntool-usage data and tunes a vision-language model (VLM) as the controller for\npowerful tool-usage reasoning. To preserve the data quality, we prompt the\nGPT-4o mini model to generate queries, files, and trajectories, followed by\nquery-file and trajectory verifiers. Based on the data synthesis pipeline, we\ncollect the MM-Traj dataset that contains 20K tasks with trajectories of tool\nusage. Then, we develop the T3-Agent via \\underline{T}rajectory\n\\underline{T}uning on VLMs for \\underline{T}ool usage using MM-Traj.\nEvaluations on the GTA and GAIA benchmarks show that the T3-Agent consistently\nachieves improvements on two popular VLMs: MiniCPM-V-8.5B and {Qwen2-VL-7B},\nwhich outperforms untrained VLMs by $20\\%$, showing the effectiveness of the\nproposed data synthesis pipeline, leading to high-quality data for tool-usage\ncapabilities.",
    "categories": [
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.AI",
    "comment": "ICLR 2025, https://mat-agent.github.io/",
    "pdf_url": "http://arxiv.org/pdf/2412.15606v2",
    "published_date": "2024-12-20 07:00:46 UTC",
    "updated_date": "2025-02-03 12:56:55 UTC"
  },
  {
    "arxiv_id": "2412.15598v2",
    "title": "Long-Term EEG Partitioning for Seizure Onset Detection",
    "authors": [
      "Zheng Chen",
      "Yasuko Matsubara",
      "Yasushi Sakurai",
      "Jimeng Sun"
    ],
    "abstract": "Deep learning models have recently shown great success in classifying\nepileptic patients using EEG recordings. Unfortunately, classification-based\nmethods lack a sound mechanism to detect the onset of seizure events. In this\nwork, we propose a two-stage framework, SODor, that explicitly models seizure\nonset through a novel task formulation of subsequence clustering. Given an EEG\nsequence, the framework first learns a set of second-level embeddings with\nlabel supervision. It then employs model-based clustering to explicitly capture\nlong-term temporal dependencies in EEG sequences and identify meaningful\nsubsequences. Epochs within a subsequence share a common cluster assignment\n(normal or seizure), with cluster or state transitions representing successful\nonset detections. Extensive experiments on three datasets demonstrate that our\nmethod can correct misclassifications, achieving 5\\%-11\\% classification\nimprovements over other baselines and accurately detecting seizure onsets.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "eess.SP"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted at AAAI 2025",
    "pdf_url": "http://arxiv.org/pdf/2412.15598v2",
    "published_date": "2024-12-20 06:42:58 UTC",
    "updated_date": "2025-03-03 06:39:17 UTC"
  },
  {
    "arxiv_id": "2412.15595v1",
    "title": "Mask-RadarNet: Enhancing Transformer With Spatial-Temporal Semantic Context for Radar Object Detection in Autonomous Driving",
    "authors": [
      "Yuzhi Wu",
      "Jun Liu",
      "Guangfeng Jiang",
      "Weijian Liu",
      "Danilo Orlando"
    ],
    "abstract": "As a cost-effective and robust technology, automotive radar has seen steady\nimprovement during the last years, making it an appealing complement to\ncommonly used sensors like camera and LiDAR in autonomous driving. Radio\nfrequency data with rich semantic information are attracting more and more\nattention. Most current radar-based models take radio frequency image sequences\nas the input. However, these models heavily rely on convolutional neural\nnetworks and leave out the spatial-temporal semantic context during the\nencoding stage. To solve these problems, we propose a model called\nMask-RadarNet to fully utilize the hierarchical semantic features from the\ninput radar data. Mask-RadarNet exploits the combination of interleaved\nconvolution and attention operations to replace the traditional architecture in\ntransformer-based models. In addition, patch shift is introduced to the\nMask-RadarNet for efficient spatial-temporal feature learning. By shifting part\nof patches with a specific mosaic pattern in the temporal dimension,\nMask-RadarNet achieves competitive performance while reducing the computational\nburden of the spatial-temporal modeling. In order to capture the\nspatial-temporal semantic contextual information, we design the class masking\nattention module (CMAM) in our encoder. Moreover, a lightweight auxiliary\ndecoder is added to our model to aggregate prior maps generated from the CMAM.\nExperiments on the CRUW dataset demonstrate the superiority of the proposed\nmethod to some state-of-the-art radar-based object detection algorithms. With\nrelatively lower computational complexity and fewer parameters, the proposed\nMask-RadarNet achieves higher recognition accuracy for object detection in\nautonomous driving.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.15595v1",
    "published_date": "2024-12-20 06:39:40 UTC",
    "updated_date": "2024-12-20 06:39:40 UTC"
  },
  {
    "arxiv_id": "2412.15593v1",
    "title": "Machine Learning Techniques for Pattern Recognition in High-Dimensional Data Mining",
    "authors": [
      "Pochun Li"
    ],
    "abstract": "This paper proposes a frequent pattern data mining algorithm based on support\nvector machine (SVM), aiming to solve the performance bottleneck of traditional\nfrequent pattern mining algorithms in high-dimensional and sparse data\nenvironments. By converting the frequent pattern mining task into a\nclassification problem, the SVM model is introduced to improve the accuracy and\nrobustness of pattern extraction. In terms of method design, the kernel\nfunction is used to map the data to a high-dimensional feature space, so as to\nconstruct the optimal classification hyperplane, realize the nonlinear\nseparation of patterns and the accurate mining of frequent items. In the\nexperiment, two public datasets, Retail and Mushroom, were selected to compare\nand analyze the proposed algorithm with traditional FP-Growth, FP-Tree,\ndecision tree and random forest models. The experimental results show that the\nalgorithm in this paper is significantly better than the traditional model in\nterms of three key indicators: support, confidence and lift, showing strong\npattern recognition ability and rule extraction effect. The study shows that\nthe SVM model has excellent performance advantages in an environment with high\ndata sparsity and a large number of transactions, and can effectively cope with\ncomplex pattern mining tasks. At the same time, this paper also points out the\npotential direction of future research, including the introduction of deep\nlearning and ensemble learning frameworks to further improve the scalability\nand adaptability of the algorithm. This research not only provides a new idea\nfor frequent pattern mining, but also provides important technical support for\nsolving pattern discovery and association rule mining problems in practical\napplications.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.15593v1",
    "published_date": "2024-12-20 06:32:05 UTC",
    "updated_date": "2024-12-20 06:32:05 UTC"
  },
  {
    "arxiv_id": "2412.15589v1",
    "title": "Pre-training Graph Neural Networks on Molecules by Using Subgraph-Conditioned Graph Information Bottleneck",
    "authors": [
      "Van Thuy Hoang",
      "O-Joun Lee"
    ],
    "abstract": "This study aims to build a pre-trained Graph Neural Network (GNN) model on\nmolecules without human annotations or prior knowledge. Although various\nattempts have been proposed to overcome limitations in acquiring labeled\nmolecules, the previous pre-training methods still rely on semantic subgraphs,\ni.e., functional groups. Only focusing on the functional groups could overlook\nthe graph-level distinctions. The key challenge to build a pre-trained GNN on\nmolecules is how to (1) generate well-distinguished graph-level representations\nand (2) automatically discover the functional groups without prior knowledge.\nTo solve it, we propose a novel Subgraph-conditioned Graph Information\nBottleneck, named S-CGIB, for pre-training GNNs to recognize core subgraphs\n(graph cores) and significant subgraphs. The main idea is that the graph cores\ncontain compressed and sufficient information that could generate\nwell-distinguished graph-level representations and reconstruct the input graph\nconditioned on significant subgraphs across molecules under the S-CGIB\nprinciple. To discover significant subgraphs without prior knowledge about\nfunctional groups, we propose generating a set of functional group candidates,\ni.e., ego networks, and using an attention-based interaction between the graph\ncore and the candidates. Despite being identified from self-supervised\nlearning, our learned subgraphs match the real-world functional groups.\nExtensive experiments on molecule datasets across various domains demonstrate\nthe superiority of S-CGIB.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "15 pages",
    "pdf_url": "http://arxiv.org/pdf/2412.15589v1",
    "published_date": "2024-12-20 05:52:30 UTC",
    "updated_date": "2024-12-20 05:52:30 UTC"
  },
  {
    "arxiv_id": "2412.15579v1",
    "title": "Score-based Generative Diffusion Models for Social Recommendations",
    "authors": [
      "Chengyi Liu",
      "Jiahao Zhang",
      "Shijie Wang",
      "Wenqi Fan",
      "Qing Li"
    ],
    "abstract": "With the prevalence of social networks on online platforms, social\nrecommendation has become a vital technique for enhancing personalized\nrecommendations. The effectiveness of social recommendations largely relies on\nthe social homophily assumption, which presumes that individuals with social\nconnections often share similar preferences. However, this foundational premise\nhas been recently challenged due to the inherent complexity and noise present\nin real-world social networks. In this paper, we tackle the low social\nhomophily challenge from an innovative generative perspective, directly\ngenerating optimal user social representations that maximize consistency with\ncollaborative signals. Specifically, we propose the Score-based Generative\nModel for Social Recommendation (SGSR), which effectively adapts the Stochastic\nDifferential Equation (SDE)-based diffusion models for social recommendations.\nTo better fit the recommendation context, SGSR employs a joint curriculum\ntraining strategy to mitigate challenges related to missing supervision signals\nand leverages self-supervised learning techniques to align knowledge across\nsocial and collaborative domains. Extensive experiments on real-world datasets\ndemonstrate the effectiveness of our approach in filtering redundant social\ninformation and improving recommendation performance.",
    "categories": [
      "cs.SI",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.SI",
    "comment": "14 pages, 8 figures",
    "pdf_url": "http://arxiv.org/pdf/2412.15579v1",
    "published_date": "2024-12-20 05:23:45 UTC",
    "updated_date": "2024-12-20 05:23:45 UTC"
  },
  {
    "arxiv_id": "2412.15571v1",
    "title": "Continual Learning Using a Kernel-Based Method Over Foundation Models",
    "authors": [
      "Saleh Momeni",
      "Sahisnu Mazumder",
      "Bing Liu"
    ],
    "abstract": "Continual learning (CL) learns a sequence of tasks incrementally. This paper\nstudies the challenging CL setting of class-incremental learning (CIL). CIL has\ntwo key challenges: catastrophic forgetting (CF) and inter-task class\nseparation (ICS). Despite numerous proposed methods, these issues remain\npersistent obstacles. This paper proposes a novel CIL method, called Kernel\nLinear Discriminant Analysis (KLDA), that can effectively avoid CF and ICS\nproblems. It leverages only the powerful features learned in a foundation model\n(FM). However, directly using these features proves suboptimal. To address\nthis, KLDA incorporates the Radial Basis Function (RBF) kernel and its Random\nFourier Features (RFF) to enhance the feature representations from the FM,\nleading to improved performance. When a new task arrives, KLDA computes only\nthe mean for each class in the task and updates a shared covariance matrix for\nall learned classes based on the kernelized features. Classification is\nperformed using Linear Discriminant Analysis. Our empirical evaluation using\ntext and image classification datasets demonstrates that KLDA significantly\noutperforms baselines. Remarkably, without relying on replay data, KLDA\nachieves accuracy comparable to joint training of all classes, which is\nconsidered the upper bound for CIL performance. The KLDA code is available at\nhttps://github.com/salehmomeni/klda.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL",
      "cs.CV"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.15571v1",
    "published_date": "2024-12-20 05:09:18 UTC",
    "updated_date": "2024-12-20 05:09:18 UTC"
  },
  {
    "arxiv_id": "2412.15563v1",
    "title": "In-context Continual Learning Assisted by an External Continual Learner",
    "authors": [
      "Saleh Momeni",
      "Sahisnu Mazumder",
      "Zixuan Ke",
      "Bing Liu"
    ],
    "abstract": "Existing continual learning (CL) methods mainly rely on fine-tuning or\nadapting large language models (LLMs). They still suffer from catastrophic\nforgetting (CF). Little work has been done to exploit in-context learning (ICL)\nto leverage the extensive knowledge within LLMs for CL without updating any\nparameters. However, incrementally learning each new task in ICL necessitates\nadding training examples from each class of the task to the prompt, which\nhampers scalability as the prompt length increases. This issue not only leads\nto excessively long prompts that exceed the input token limit of the underlying\nLLM but also degrades the model's performance due to the overextended context.\nTo address this, we introduce InCA, a novel approach that integrates an\nexternal continual learner (ECL) with ICL to enable scalable CL without CF. The\nECL is built incrementally to pre-select a small subset of likely classes for\neach test instance. By restricting the ICL prompt to only these selected\nclasses, InCA prevents prompt lengths from becoming excessively long, while\nmaintaining high performance. Experimental results demonstrate that InCA\nsignificantly outperforms existing CL baselines, achieving substantial\nperformance gains.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.15563v1",
    "published_date": "2024-12-20 04:44:41 UTC",
    "updated_date": "2024-12-20 04:44:41 UTC"
  },
  {
    "arxiv_id": "2412.15554v3",
    "title": "Architecture-Aware Learning Curve Extrapolation via Graph Ordinary Differential Equation",
    "authors": [
      "Yanna Ding",
      "Zijie Huang",
      "Xiao Shou",
      "Yihang Guo",
      "Yizhou Sun",
      "Jianxi Gao"
    ],
    "abstract": "Learning curve extrapolation predicts neural network performance from early\ntraining epochs and has been applied to accelerate AutoML, facilitating\nhyperparameter tuning and neural architecture search. However, existing methods\ntypically model the evolution of learning curves in isolation, neglecting the\nimpact of neural network (NN) architectures, which influence the loss landscape\nand learning trajectories. In this work, we explore whether incorporating\nneural network architecture improves learning curve modeling and how to\neffectively integrate this architectural information. Motivated by the\ndynamical system view of optimization, we propose a novel architecture-aware\nneural differential equation model to forecast learning curves continuously. We\nempirically demonstrate its ability to capture the general trend of fluctuating\nlearning curves while quantifying uncertainty through variational parameters.\nOur model outperforms current state-of-the-art learning curve extrapolation\nmethods and pure time-series modeling approaches for both MLP and CNN-based\nlearning curves. Additionally, we explore the applicability of our method in\nNeural Architecture Search scenarios, such as training configuration ranking.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted to AAAI'25",
    "pdf_url": "http://arxiv.org/pdf/2412.15554v3",
    "published_date": "2024-12-20 04:28:02 UTC",
    "updated_date": "2025-01-19 02:54:49 UTC"
  },
  {
    "arxiv_id": "2412.16252v1",
    "title": "Post-hoc Interpretability Illumination for Scientific Interaction Discovery",
    "authors": [
      "Ling Zhang",
      "Zhichao Hou",
      "Tingxiang Ji",
      "Yuanyuan Xu",
      "Runze Li"
    ],
    "abstract": "Model interpretability and explainability have garnered substantial attention\nin recent years, particularly in decision-making applications. However,\nexisting interpretability tools often fall short in delivering satisfactory\nperformance due to limited capabilities or efficiency issues. To address these\nchallenges, we propose a novel post-hoc method: Iterative Kings' Forests (iKF),\ndesigned to uncover complex multi-order interactions among variables. iKF\niteratively selects the next most important variable, the \"King\", and\nconstructs King's Forests by placing it at the root node of each tree to\nidentify variables that interact with the \"King\". It then generates ranked\nshort lists of important variables and interactions of varying orders.\nAdditionally, iKF provides inference metrics to analyze the patterns of the\nselected interactions and classify them into one of three interaction types:\nAccompanied Interaction, Synergistic Interaction, and Hierarchical Interaction.\nExtensive experiments demonstrate the strong interpretive power of our proposed\niKF, highlighting its great potential for explainable modeling and scientific\ndiscovery across diverse scientific fields.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.16252v1",
    "published_date": "2024-12-20 04:17:12 UTC",
    "updated_date": "2024-12-20 04:17:12 UTC"
  },
  {
    "arxiv_id": "2412.15547v1",
    "title": "NGQA: A Nutritional Graph Question Answering Benchmark for Personalized Health-aware Nutritional Reasoning",
    "authors": [
      "Zheyuan Zhang",
      "Yiyang Li",
      "Nhi Ha Lan Le",
      "Zehong Wang",
      "Tianyi Ma",
      "Vincent Galassi",
      "Keerthiram Murugesan",
      "Nuno Moniz",
      "Werner Geyer",
      "Nitesh V Chawla",
      "Chuxu Zhang",
      "Yanfang Ye"
    ],
    "abstract": "Diet plays a critical role in human health, yet tailoring dietary reasoning\nto individual health conditions remains a major challenge. Nutrition Question\nAnswering (QA) has emerged as a popular method for addressing this problem.\nHowever, current research faces two critical limitations. On one hand, the\nabsence of datasets involving user-specific medical information severely limits\n\\textit{personalization}. This challenge is further compounded by the wide\nvariability in individual health needs. On the other hand, while large language\nmodels (LLMs), a popular solution for this task, demonstrate strong reasoning\nabilities, they struggle with the domain-specific complexities of personalized\nhealthy dietary reasoning, and existing benchmarks fail to capture these\nchallenges. To address these gaps, we introduce the Nutritional Graph Question\nAnswering (NGQA) benchmark, the first graph question answering dataset designed\nfor personalized nutritional health reasoning. NGQA leverages data from the\nNational Health and Nutrition Examination Survey (NHANES) and the Food and\nNutrient Database for Dietary Studies (FNDDS) to evaluate whether a food is\nhealthy for a specific user, supported by explanations of the key contributing\nnutrients. The benchmark incorporates three question complexity settings and\nevaluates reasoning across three downstream tasks. Extensive experiments with\nLLM backbones and baseline models demonstrate that the NGQA benchmark\neffectively challenges existing models. In sum, NGQA addresses a critical\nreal-world problem while advancing GraphQA research with a novel\ndomain-specific benchmark.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.15547v1",
    "published_date": "2024-12-20 04:13:46 UTC",
    "updated_date": "2024-12-20 04:13:46 UTC"
  },
  {
    "arxiv_id": "2412.15544v1",
    "title": "VLM-RL: A Unified Vision Language Models and Reinforcement Learning Framework for Safe Autonomous Driving",
    "authors": [
      "Zilin Huang",
      "Zihao Sheng",
      "Yansong Qu",
      "Junwei You",
      "Sikai Chen"
    ],
    "abstract": "In recent years, reinforcement learning (RL)-based methods for learning\ndriving policies have gained increasing attention in the autonomous driving\ncommunity and have achieved remarkable progress in various driving scenarios.\nHowever, traditional RL approaches rely on manually engineered rewards, which\nrequire extensive human effort and often lack generalizability. To address\nthese limitations, we propose \\textbf{VLM-RL}, a unified framework that\nintegrates pre-trained Vision-Language Models (VLMs) with RL to generate reward\nsignals using image observation and natural language goals. The core of VLM-RL\nis the contrasting language goal (CLG)-as-reward paradigm, which uses positive\nand negative language goals to generate semantic rewards. We further introduce\na hierarchical reward synthesis approach that combines CLG-based semantic\nrewards with vehicle state information, improving reward stability and offering\na more comprehensive reward signal. Additionally, a batch-processing technique\nis employed to optimize computational efficiency during training. Extensive\nexperiments in the CARLA simulator demonstrate that VLM-RL outperforms\nstate-of-the-art baselines, achieving a 10.5\\% reduction in collision rate, a\n104.6\\% increase in route completion rate, and robust generalization to unseen\ndriving scenarios. Furthermore, VLM-RL can seamlessly integrate almost any\nstandard RL algorithms, potentially revolutionizing the existing RL paradigm\nthat relies on manual reward engineering and enabling continuous performance\nimprovements. The demo video and code can be accessed at:\nhttps://zilin-huang.github.io/VLM-RL-website.",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.RO",
    "comment": "28 pages, 16 figures",
    "pdf_url": "http://arxiv.org/pdf/2412.15544v1",
    "published_date": "2024-12-20 04:08:11 UTC",
    "updated_date": "2024-12-20 04:08:11 UTC"
  },
  {
    "arxiv_id": "2412.15541v1",
    "title": "ChangeDiff: A Multi-Temporal Change Detection Data Generator with Flexible Text Prompts via Diffusion Model",
    "authors": [
      "Qi Zang",
      "Jiayi Yang",
      "Shuang Wang",
      "Dong Zhao",
      "Wenjun Yi",
      "Zhun Zhong"
    ],
    "abstract": "Data-driven deep learning models have enabled tremendous progress in change\ndetection (CD) with the support of pixel-level annotations. However, collecting\ndiverse data and manually annotating them is costly, laborious, and\nknowledge-intensive. Existing generative methods for CD data synthesis show\ncompetitive potential in addressing this issue but still face the following\nlimitations: 1) difficulty in flexibly controlling change events, 2) dependence\non additional data to train the data generators, 3) focus on specific change\ndetection tasks. To this end, this paper focuses on the semantic CD (SCD) task\nand develops a multi-temporal SCD data generator ChangeDiff by exploring\npowerful diffusion models. ChangeDiff innovatively generates change data in two\nsteps: first, it uses text prompts and a text-to-layout (T2L) model to create\ncontinuous layouts, and then it employs layout-to-image (L2I) to convert these\nlayouts into images. Specifically, we propose multi-class distribution-guided\ntext prompts (MCDG-TP), allowing for layouts to be generated flexibly through\ncontrollable classes and their corresponding ratios. Subsequently, to\ngeneralize the T2L model to the proposed MCDG-TP, a class distribution\nrefinement loss is further designed as training supervision. %For the former, a\nmulti-classdistribution-guided text prompt (MCDG-TP) is proposed to complement\nvia controllable classes and ratios. To generalize the text-to-image diffusion\nmodel to the proposed MCDG-TP, a class distribution refinement loss is designed\nas training supervision. For the latter, MCDG-TP in three modes is proposed to\nsynthesize new layout masks from various texts. Our generated data shows\nsignificant progress in temporal continuity, spatial diversity, and quality\nrealism, empowering change detectors with accuracy and transferability. The\ncode is available at https://github.com/DZhaoXd/ChangeDiff",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.15541v1",
    "published_date": "2024-12-20 03:58:28 UTC",
    "updated_date": "2024-12-20 03:58:28 UTC"
  },
  {
    "arxiv_id": "2412.15538v2",
    "title": "FedRLHF: A Convergence-Guaranteed Federated Framework for Privacy-Preserving and Personalized RLHF",
    "authors": [
      "Flint Xiaofeng Fan",
      "Cheston Tan",
      "Yew-Soon Ong",
      "Roger Wattenhofer",
      "Wei-Tsang Ooi"
    ],
    "abstract": "In the era of increasing privacy concerns and demand for personalized\nexperiences, traditional Reinforcement Learning with Human Feedback (RLHF)\nframeworks face significant challenges due to their reliance on centralized\ndata. We introduce Federated Reinforcement Learning with Human Feedback\n(FedRLHF), a novel framework that decentralizes the RLHF process. FedRLHF\nenables collaborative policy learning across multiple clients without\nnecessitating the sharing of raw data or human feedback, thereby ensuring\nrobust privacy preservation. Leveraging federated reinforcement learning, each\nclient integrates human feedback locally into their reward functions and\nupdates their policies through personalized RLHF processes. We establish\nrigorous theoretical foundations for FedRLHF, providing convergence guarantees,\nand deriving sample complexity bounds that scale efficiently with the number of\nclients. Empirical evaluations on the MovieLens and IMDb datasets demonstrate\nthat FedRLHF not only preserves user privacy but also achieves performance on\npar with centralized RLHF, while enhancing personalization across diverse\nclient environments.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CR",
      "I.2.11"
    ],
    "primary_category": "cs.LG",
    "comment": "Updated for AAMAS 2025 camera-ready. This preprint represents the\n  full version of the paper, including all proofs, experimental details, and\n  additional discussions",
    "pdf_url": "http://arxiv.org/pdf/2412.15538v2",
    "published_date": "2024-12-20 03:56:31 UTC",
    "updated_date": "2025-02-08 02:34:08 UTC"
  },
  {
    "arxiv_id": "2412.15537v1",
    "title": "Enhancing Large-scale UAV Route Planing with Global and Local Features via Reinforcement Graph Fusion",
    "authors": [
      "Tao Zhou",
      "Kai Ye",
      "Zeyu Shi",
      "Jiajing Lin",
      "Dejun Xu",
      "Min Jiang"
    ],
    "abstract": "Numerous remarkable advancements have been made in accuracy, speed, and\nparallelism for solving the Unmanned Aerial Vehicle Route Planing (UAVRP).\nHowever, existing UAVRP solvers face challenges when attempting to scale\neffectively and efficiently for larger instances. In this paper, we present a\ngeneralization framework that enables current UAVRP solvers to robustly extend\ntheir capabilities to larger instances, accommodating up to 10,000 points,\nusing widely recognized test sets. The UAVRP under a large number of patrol\npoints is a typical large-scale TSP problem.Our proposed framework comprises\nthree distinct steps. Firstly, we employ Delaunay triangulation to extract\nsubgraphs from large instances while preserving global features. Secondly, we\nutilize an embedded TSP solver to obtain sub-results, followed by graph fusion.\nFinally, we implement a decoding strategy customizable to the user's\nrequirements, resulting in high-quality solutions, complemented by a warming-up\nprocess for the heatmap. To demonstrate the flexibility of our approach, we\nintegrate two representative TSP solvers into our framework and conduct a\ncomprehensive comparative analysis against existing algorithms using large TSP\nbenchmark datasets. The results unequivocally demonstrate that our framework\nefficiently scales existing TSP solvers to handle large instances and\nconsistently outperforms state-of-the-art (SOTA) methods. Furthermore, since\nour proposed framework does not necessitate additional training or fine-tuning,\nwe believe that its generality can significantly advance research on end-to-end\nUAVRP solvers, enabling the application of a broader range of methods to\nreal-world scenarios.",
    "categories": [
      "cs.AI",
      "cs.RO"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.15537v1",
    "published_date": "2024-12-20 03:54:43 UTC",
    "updated_date": "2024-12-20 03:54:43 UTC"
  },
  {
    "arxiv_id": "2412.15532v1",
    "title": "Improved Forecasts of Global Extreme Marine Heatwaves Through a Physics-guided Data-driven Approach",
    "authors": [
      "Ruiqi Shu",
      "Hao Wu",
      "Yuan Gao",
      "Fanghua Xu",
      "Ruijian Gou",
      "Xiaomeng Huang"
    ],
    "abstract": "The unusually warm sea surface temperature events known as marine heatwaves\n(MHWs) have a profound impact on marine ecosystems. Accurate prediction of\nextreme MHWs has significant scientific and financial worth. However, existing\nmethods still have certain limitations, especially in the most extreme MHWs. In\nthis study, to address these issues, based on the physical nature of MHWs, we\ncreated a novel deep learning neural network that is capable of accurate 10-day\nMHW forecasting. Our framework significantly improves the forecast ability of\nextreme MHWs through two specially designed modules inspired by numerical\nmodels: a coupler and a probabilistic data argumentation. The coupler simulates\nthe driving effect of atmosphere on MHWs while the probabilistic data\nargumentation approaches significantly boost the forecast ability of extreme\nMHWs based on the idea of ensemble forecast. Compared with traditional\nnumerical prediction, our framework has significantly higher accuracy and\nrequires fewer computational resources. What's more, explainable AI methods\nshow that wind forcing is the primary driver of MHW evolution and reveal its\nrelation with air-sea heat exchange. Overall, our model provides a framework\nfor understanding MHWs' driving processes and operational forecasts in the\nfuture.",
    "categories": [
      "physics.ao-ph",
      "cs.AI"
    ],
    "primary_category": "physics.ao-ph",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.15532v1",
    "published_date": "2024-12-20 03:47:56 UTC",
    "updated_date": "2024-12-20 03:47:56 UTC"
  },
  {
    "arxiv_id": "2412.15529v3",
    "title": "XRAG: eXamining the Core -- Benchmarking Foundational Components in Advanced Retrieval-Augmented Generation",
    "authors": [
      "Qianren Mao",
      "Yangyifei Luo",
      "Qili Zhang",
      "Yashuo Luo",
      "Zhilong Cao",
      "Jinlong Zhang",
      "HanWen Hao",
      "Zhijun Chen",
      "Weifeng Jiang",
      "Junnan Liu",
      "Xiaolong Wang",
      "Zhenting Huang",
      "Zhixing Tan",
      "Sun Jie",
      "Bo Li",
      "Xudong Liu",
      "Richong Zhang",
      "Jianxin Li"
    ],
    "abstract": "Retrieval-augmented generation (RAG) synergizes the retrieval of pertinent\ndata with the generative capabilities of Large Language Models (LLMs), ensuring\nthat the generated output is not only contextually relevant but also accurate\nand current. We introduce XRAG, an open-source, modular codebase that\nfacilitates exhaustive evaluation of the performance of foundational components\nof advanced RAG modules. These components are systematically categorized into\nfour core phases: pre-retrieval, retrieval, post-retrieval, and generation. We\nsystematically analyse them across reconfigured datasets, providing a\ncomprehensive benchmark for their effectiveness. As the complexity of RAG\nsystems continues to escalate, we underscore the critical need to identify\npotential failure points in RAG systems. We formulate a suite of experimental\nmethodologies and diagnostic testing protocols to dissect the failure points\ninherent in RAG engineering. Subsequently, we proffer bespoke solutions aimed\nat bolstering the overall performance of these modules. Our work thoroughly\nevaluates the performance of advanced core components in RAG systems, providing\ninsights into optimizations for prevalent failure points.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.15529v3",
    "published_date": "2024-12-20 03:37:07 UTC",
    "updated_date": "2025-05-16 14:13:36 UTC"
  },
  {
    "arxiv_id": "2412.15525v1",
    "title": "Generalized Back-Stepping Experience Replay in Sparse-Reward Environments",
    "authors": [
      "Guwen Lyu",
      "Masahiro Sato"
    ],
    "abstract": "Back-stepping experience replay (BER) is a reinforcement learning technique\nthat can accelerate learning efficiency in reversible environments. BER trains\nan agent with generated back-stepping transitions of collected experiences and\nnormal forward transitions. However, the original algorithm is designed for a\ndense-reward environment that does not require complex exploration, limiting\nthe BER technique to demonstrate its full potential. Herein, we propose an\nenhanced version of BER called Generalized BER (GBER), which extends the\noriginal algorithm to sparse-reward environments, particularly those with\ncomplex structures that require the agent to explore. GBER improves the\nperformance of BER by introducing relabeling mechanism and applying diverse\nsampling strategies. We evaluate our modified version, which is based on a\ngoal-conditioned deep deterministic policy gradient offline learning algorithm,\nacross various maze navigation environments. The experimental results indicate\nthat the GBER algorithm can significantly boost the performance and stability\nof the baseline algorithm in various sparse-reward environments, especially\nthose with highly structural symmetricity.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.15525v1",
    "published_date": "2024-12-20 03:31:23 UTC",
    "updated_date": "2024-12-20 03:31:23 UTC"
  },
  {
    "arxiv_id": "2412.15524v2",
    "title": "HREF: Human Response-Guided Evaluation of Instruction Following in Language Models",
    "authors": [
      "Xinxi Lyu",
      "Yizhong Wang",
      "Hannaneh Hajishirzi",
      "Pradeep Dasigi"
    ],
    "abstract": "Evaluating the capability of Large Language Models (LLMs) in following\ninstructions has heavily relied on a powerful LLM as the judge, introducing\nunresolved biases that deviate the judgments from human judges. In this work,\nwe reevaluate various choices for automatic evaluation on a wide range of\ninstruction-following tasks. We experiment with methods that leverage\nhuman-written responses and observe that they enhance the reliability of\nautomatic evaluations across a wide range of tasks, resulting in up to a 3.2%\nimprovement in agreement with human judges. We also discovered that\nhuman-written responses offer an orthogonal perspective to model-generated\nresponses in following instructions and should be used as an additional context\nwhen comparing model responses. Based on these observations, we develop a new\nevaluation benchmark, Human Response-Guided Evaluation of Instruction Following\n(HREF), comprising 4,258 samples across 11 task categories with a composite\nevaluation setup, employing a composite evaluation setup that selects the most\nreliable method for each category. In addition to providing reliable\nevaluation, HREF emphasizes individual task performance and is free from\ncontamination. Finally, we study the impact of key design choices in HREF,\nincluding the size of the evaluation set, the judge model, the baseline model,\nand the prompt template. We host a live leaderboard that evaluates LLMs on the\nprivate evaluation set of HREF.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "28 pages, 15 figures",
    "pdf_url": "http://arxiv.org/pdf/2412.15524v2",
    "published_date": "2024-12-20 03:26:47 UTC",
    "updated_date": "2025-03-24 19:31:44 UTC"
  },
  {
    "arxiv_id": "2412.15523v2",
    "title": "InstructOCR: Instruction Boosting Scene Text Spotting",
    "authors": [
      "Chen Duan",
      "Qianyi Jiang",
      "Pei Fu",
      "Jiamin Chen",
      "Shengxi Li",
      "Zining Wang",
      "Shan Guo",
      "Junfeng Luo"
    ],
    "abstract": "In the field of scene text spotting, previous OCR methods primarily relied on\nimage encoders and pre-trained text information, but they often overlooked the\nadvantages of incorporating human language instructions. To address this gap,\nwe propose InstructOCR, an innovative instruction-based scene text spotting\nmodel that leverages human language instructions to enhance the understanding\nof text within images. Our framework employs both text and image encoders\nduring training and inference, along with instructions meticulously designed\nbased on text attributes. This approach enables the model to interpret text\nmore accurately and flexibly. Extensive experiments demonstrate the\neffectiveness of our model and we achieve state-of-the-art results on widely\nused benchmarks. Furthermore, the proposed framework can be seamlessly applied\nto scene text VQA tasks. By leveraging instruction strategies during\npre-training, the performance on downstream VQA tasks can be significantly\nimproved, with a 2.6% increase on the TextVQA dataset and a 2.1% increase on\nthe ST-VQA dataset. These experimental results provide insights into the\nbenefits of incorporating human language instructions for OCR-related tasks.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "Accepted by AAAI2025",
    "pdf_url": "http://arxiv.org/pdf/2412.15523v2",
    "published_date": "2024-12-20 03:23:26 UTC",
    "updated_date": "2025-01-13 10:01:56 UTC"
  },
  {
    "arxiv_id": "2501.14747v1",
    "title": "Enhancing Green Economy with Artificial Intelligence: Role of Energy Use and FDI in the United States",
    "authors": [
      "Abdullah Al Abrar Chowdhury",
      "Azizul Hakim Rafi",
      "Adita Sultana",
      "Abdulla All Noman"
    ],
    "abstract": "The escalating challenge of climate change necessitates an urgent exploration\nof factors influencing carbon emissions. This study contributes to the\ndiscourse by examining the interplay of technological, economic, and\ndemographic factors on environmental sustainability. This study investigates\nthe impact of artificial intelligence (AI) innovation, economic growth, foreign\ndirect investment (FDI), energy consumption, and urbanization on CO2 emissions\nin the United States from 1990 to 2022. Employing the ARDL framework integrated\nwith the STIRPAT model, the findings reveal a dual narrative: while AI\ninnovation mitigates environmental stress, economic growth, energy use, FDI,\nand urbanization exacerbate environmental degradation. Unit root tests (ADF,\nPP, and DF-GLS) confirm mixed integration levels among variables, and the ARDL\nbounds test establishes long-term co-integration. The analysis highlights that\nAI innovation positively correlates with CO2 reduction when environmental\nsafeguards are in place, whereas GDP growth, energy consumption, FDI, and\nurbanization intensify CO2 emissions. Robustness checks using FMOLS, DOLS, and\nCCR validate the ARDL findings. Additionally, Pairwise Granger causality tests\nreveal significant one-way causal links between CO2 emissions and economic\ngrowth, AI innovation, energy use, FDI, and urbanization. These relationships\nemphasize the critical role of AI-driven technological advancements,\nsustainable investments, and green energy in fostering ecological\nsustainability. The study suggests policy measures such as encouraging green\nFDI, advancing AI technologies, adopting sustainable energy practices, and\nimplementing eco-friendly urban development to promote sustainable growth in\nthe USA.",
    "categories": [
      "econ.GN",
      "cs.AI",
      "q-fin.EC"
    ],
    "primary_category": "econ.GN",
    "comment": "22 pages, 1 figure",
    "pdf_url": "http://arxiv.org/pdf/2501.14747v1",
    "published_date": "2024-12-20 03:03:21 UTC",
    "updated_date": "2024-12-20 03:03:21 UTC"
  },
  {
    "arxiv_id": "2412.15511v1",
    "title": "RESQUE: Quantifying Estimator to Task and Distribution Shift for Sustainable Model Reusability",
    "authors": [
      "Vishwesh Sangarya",
      "Jung-Eun Kim"
    ],
    "abstract": "As a strategy for sustainability of deep learning, reusing an existing model\nby retraining it rather than training a new model from scratch is critical. In\nthis paper, we propose REpresentation Shift QUantifying Estimator (RESQUE), a\npredictive quantifier to estimate the retraining cost of a model to\ndistributional shifts or change of tasks. It provides a single concise index\nfor an estimate of resources required for retraining the model. Through\nextensive experiments, we show that RESQUE has a strong correlation with\nvarious retraining measures. Our results validate that RESQUE is an effective\nindicator in terms of epochs, gradient norms, changes of parameter magnitude,\nenergy, and carbon emissions. These measures align well with RESQUE for new\ntasks, multiple noise types, and varying noise intensities. As a result, RESQUE\nenables users to make informed decisions for retraining to different\ntasks/distribution shifts and determine the most cost-effective and sustainable\noption, allowing for the reuse of a model with a much smaller footprint in the\nenvironment. The code for this work is available here:\nhttps://github.com/JEKimLab/AAAI2025RESQUE",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.LG",
    "comment": "The Annual AAAI Conference on Artificial Intelligence (AAAI), 2025",
    "pdf_url": "http://arxiv.org/pdf/2412.15511v1",
    "published_date": "2024-12-20 02:55:07 UTC",
    "updated_date": "2024-12-20 02:55:07 UTC"
  },
  {
    "arxiv_id": "2412.15501v1",
    "title": "Humanlike Cognitive Patterns as Emergent Phenomena in Large Language Models",
    "authors": [
      "Zhisheng Tang",
      "Mayank Kejriwal"
    ],
    "abstract": "Research on emergent patterns in Large Language Models (LLMs) has gained\nsignificant traction in both psychology and artificial intelligence, motivating\nthe need for a comprehensive review that offers a synthesis of this complex\nlandscape. In this article, we systematically review LLMs' capabilities across\nthree important cognitive domains: decision-making biases, reasoning, and\ncreativity. We use empirical studies drawing on established psychological tests\nand compare LLMs' performance to human benchmarks. On decision-making, our\nsynthesis reveals that while LLMs demonstrate several human-like biases, some\nbiases observed in humans are absent, indicating cognitive patterns that only\npartially align with human decision-making. On reasoning, advanced LLMs like\nGPT-4 exhibit deliberative reasoning akin to human System-2 thinking, while\nsmaller models fall short of human-level performance. A distinct dichotomy\nemerges in creativity: while LLMs excel in language-based creative tasks, such\nas storytelling, they struggle with divergent thinking tasks that require\nreal-world context. Nonetheless, studies suggest that LLMs hold considerable\npotential as collaborators, augmenting creativity in human-machine\nproblem-solving settings. Discussing key limitations, we also offer guidance\nfor future research in areas such as memory, attention, and open-source model\ndevelopment.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.15501v1",
    "published_date": "2024-12-20 02:26:56 UTC",
    "updated_date": "2024-12-20 02:26:56 UTC"
  },
  {
    "arxiv_id": "2412.15499v3",
    "title": "A Robust Prototype-Based Network with Interpretable RBF Classifier Foundations",
    "authors": [
      "Sascha Saralajew",
      "Ashish Rana",
      "Thomas Villmann",
      "Ammar Shaker"
    ],
    "abstract": "Prototype-based classification learning methods are known to be inherently\ninterpretable. However, this paradigm suffers from major limitations compared\nto deep models, such as lower performance. This led to the development of the\nso-called deep Prototype-Based Networks (PBNs), also known as prototypical\nparts models. In this work, we analyze these models with respect to different\nproperties, including interpretability. In particular, we focus on the\nClassification-by-Components (CBC) approach, which uses a probabilistic model\nto ensure interpretability and can be used as a shallow or deep architecture.\nWe show that this model has several shortcomings, like creating contradicting\nexplanations. Based on these findings, we propose an extension of CBC that\nsolves these issues. Moreover, we prove that this extension has robustness\nguarantees and derive a loss that optimizes robustness. Additionally, our\nanalysis shows that most (deep) PBNs are related to (deep) RBF classifiers,\nwhich implies that our robustness guarantees generalize to shallow RBF\nclassifiers. The empirical evaluation demonstrates that our deep PBN yields\nstate-of-the-art classification accuracy on different benchmarks while\nresolving the interpretability shortcomings of other approaches. Further, our\nshallow PBN variant outperforms other shallow PBNs while being inherently\ninterpretable and exhibiting provable robustness guarantees.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.LG",
    "comment": "To appear at AAAI 2025. Includes the Appendix of the AAAI submission.\n  In v2, the font size has been increased in some figures. In v3, an incorrect\n  hyperparameter specification (Table 6; $\\lambda$) has been corrected",
    "pdf_url": "http://arxiv.org/pdf/2412.15499v3",
    "published_date": "2024-12-20 02:25:31 UTC",
    "updated_date": "2025-04-17 14:56:49 UTC"
  },
  {
    "arxiv_id": "2412.15498v1",
    "title": "The First Multilingual Model For The Detection of Suicide Texts",
    "authors": [
      "Rodolfo Zevallos",
      "Annika Schoene",
      "John E. Ortega"
    ],
    "abstract": "Suicidal ideation is a serious health problem affecting millions of people\nworldwide. Social networks provide information about these mental health\nproblems through users' emotional expressions. We propose a multilingual model\nleveraging transformer architectures like mBERT, XML-R, and mT5 to detect\nsuicidal text across posts in six languages - Spanish, English, German,\nCatalan, Portuguese and Italian. A Spanish suicide ideation tweet dataset was\ntranslated into five other languages using SeamlessM4T. Each model was\nfine-tuned on this multilingual data and evaluated across classification\nmetrics. Results showed mT5 achieving the best performance overall with F1\nscores above 85%, highlighting capabilities for cross-lingual transfer\nlearning. The English and Spanish translations also displayed high quality\nbased on perplexity. Our exploration underscores the importance of considering\nlinguistic diversity in developing automated multilingual tools to identify\nsuicidal risk. Limitations exist around semantic fidelity in translations and\nethical implications which provide guidance for future human-in-the-loop\nevaluations.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "SUMEval-2: The 2nd Workshop on Scaling Up Multilingual &\n  Multi-Cultural Evaluation at the 31st International Conference on\n  Computational Linguistics (COLING 2025)",
    "pdf_url": "http://arxiv.org/pdf/2412.15498v1",
    "published_date": "2024-12-20 02:23:59 UTC",
    "updated_date": "2024-12-20 02:23:59 UTC"
  },
  {
    "arxiv_id": "2412.15497v1",
    "title": "Lexicography Saves Lives (LSL): Automatically Translating Suicide-Related Language",
    "authors": [
      "Annika Marie Schoene",
      "John E. Ortega",
      "Rodolfo Joel Zevallos",
      "Laura Haaber Ihle"
    ],
    "abstract": "Recent years have seen a marked increase in research that aims to identify or\npredict risk, intention or ideation of suicide. The majority of new tasks,\ndatasets, language models and other resources focus on English and on suicide\nin the context of Western culture. However, suicide is global issue and\nreducing suicide rate by 2030 is one of the key goals of the UN's Sustainable\nDevelopment Goals. Previous work has used English dictionaries related to\nsuicide to translate into different target languages due to lack of other\navailable resources. Naturally, this leads to a variety of ethical tensions\n(e.g.: linguistic misrepresentation), where discourse around suicide is not\npresent in a particular culture or country. In this work, we introduce the\n'Lexicography Saves Lives Project' to address this issue and make three\ndistinct contributions. First, we outline ethical consideration and provide\noverview guidelines to mitigate harm in developing suicide-related resources.\nNext, we translate an existing dictionary related to suicidal ideation into 200\ndifferent languages and conduct human evaluations on a subset of translated\ndictionaries. Finally, we introduce a public website to make our resources\navailable and enable community participation.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "The 31st International Conference on Computational Linguistics\n  (COLING 2025)",
    "pdf_url": "http://arxiv.org/pdf/2412.15497v1",
    "published_date": "2024-12-20 02:23:36 UTC",
    "updated_date": "2024-12-20 02:23:36 UTC"
  },
  {
    "arxiv_id": "2412.15495v1",
    "title": "TL-Training: A Task-Feature-Based Framework for Training Large Language Models in Tool Use",
    "authors": [
      "Junjie Ye",
      "Yilong Wu",
      "Sixian Li",
      "Yuming Yang",
      "Tao Gui",
      "Qi Zhang",
      "Xuanjing Huang",
      "Peng Wang",
      "Zhongchao Shi",
      "Jianping Fan",
      "Zhengyin Du"
    ],
    "abstract": "Large language models (LLMs) achieve remarkable advancements by leveraging\ntools to interact with external environments, a critical step toward\ngeneralized AI. However, the standard supervised fine-tuning (SFT) approach,\nwhich relies on large-scale datasets, often overlooks task-specific\ncharacteristics in tool use, leading to performance bottlenecks. To address\nthis issue, we analyze three existing LLMs and uncover key insights: training\ndata can inadvertently impede tool-use behavior, token importance is\ndistributed unevenly, and errors in tool calls fall into a small set of\ndistinct categories. Building on these findings, we propose TL-Training, a\ntask-feature-based framework that mitigates the effects of suboptimal training\ndata, dynamically adjusts token weights to prioritize key tokens during SFT,\nand incorporates a robust reward mechanism tailored to error categories,\noptimized through proximal policy optimization. We validate TL-Training by\ntraining CodeLLaMA-2-7B and evaluating it on four diverse open-source test\nsets. Our results demonstrate that the LLM trained by our method matches or\nsurpasses both open- and closed-source LLMs in tool-use performance using only\n1,217 training data points. Additionally, our method enhances robustness in\nnoisy environments and improves general task performance, offering a scalable\nand efficient paradigm for tool-use training in LLMs. The code and data are\navailable at https://github.com/Junjie-Ye/TL-Training.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.15495v1",
    "published_date": "2024-12-20 02:21:36 UTC",
    "updated_date": "2024-12-20 02:21:36 UTC"
  },
  {
    "arxiv_id": "2412.15483v1",
    "title": "Task-Specific Preconditioner for Cross-Domain Few-Shot Learning",
    "authors": [
      "Suhyun Kang",
      "Jungwon Park",
      "Wonseok Lee",
      "Wonjong Rhee"
    ],
    "abstract": "Cross-Domain Few-Shot Learning~(CDFSL) methods typically parameterize models\nwith task-agnostic and task-specific parameters. To adapt task-specific\nparameters, recent approaches have utilized fixed optimization strategies,\ndespite their potential sub-optimality across varying domains or target tasks.\nTo address this issue, we propose a novel adaptation mechanism called\nTask-Specific Preconditioned gradient descent~(TSP). Our method first\nmeta-learns Domain-Specific Preconditioners~(DSPs) that capture the\ncharacteristics of each meta-training domain, which are then linearly combined\nusing task-coefficients to form the Task-Specific Preconditioner. The\npreconditioner is applied to gradient descent, making the optimization adaptive\nto the target task. We constrain our preconditioners to be positive definite,\nguiding the preconditioned gradient toward the direction of steepest descent.\nEmpirical evaluations on the Meta-Dataset show that TSP achieves\nstate-of-the-art performance across diverse experimental scenarios.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted by AAAI 2025",
    "pdf_url": "http://arxiv.org/pdf/2412.15483v1",
    "published_date": "2024-12-20 01:33:43 UTC",
    "updated_date": "2024-12-20 01:33:43 UTC"
  },
  {
    "arxiv_id": "2412.15479v1",
    "title": "Continual Learning Using Only Large Language Model Prompting",
    "authors": [
      "Jiabao Qiu",
      "Zixuan Ke",
      "Bing Liu"
    ],
    "abstract": "We introduce CLOB, a novel continual learning (CL) paradigm wherein a large\nlanguage model (LLM) is regarded as a black box. Learning is done incrementally\nvia only verbal prompting. CLOB does not fine-tune any part of the LLM or add\nany trainable parameters to it. It is particularly suitable for LLMs that are\naccessible via APIs. We also propose a new CL technique, called CIS, based on\nincremental summarization that also overcomes the LLM's input length limit.\nExperiments show CIS outperforms baselines by a very large margin.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "To Appear in COLING-2025 (short paper)",
    "pdf_url": "http://arxiv.org/pdf/2412.15479v1",
    "published_date": "2024-12-20 01:21:57 UTC",
    "updated_date": "2024-12-20 01:21:57 UTC"
  },
  {
    "arxiv_id": "2412.15477v1",
    "title": "Difficulty-aware Balancing Margin Loss for Long-tailed Recognition",
    "authors": [
      "Minseok Son",
      "Inyong Koo",
      "Jinyoung Park",
      "Changick Kim"
    ],
    "abstract": "When trained with severely imbalanced data, deep neural networks often\nstruggle to accurately recognize classes with only a few samples. Previous\nstudies in long-tailed recognition have attempted to rebalance biased learning\nusing known sample distributions, primarily addressing different classification\ndifficulties at the class level. However, these approaches often overlook the\ninstance difficulty variation within each class. In this paper, we propose a\ndifficulty-aware balancing margin (DBM) loss, which considers both class\nimbalance and instance difficulty. DBM loss comprises two components: a\nclass-wise margin to mitigate learning bias caused by imbalanced class\nfrequencies, and an instance-wise margin assigned to hard positive samples\nbased on their individual difficulty. DBM loss improves class discriminativity\nby assigning larger margins to more difficult samples. Our method seamlessly\ncombines with existing approaches and consistently improves performance across\nvarious long-tailed recognition benchmarks.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.15477v1",
    "published_date": "2024-12-20 01:11:30 UTC",
    "updated_date": "2024-12-20 01:11:30 UTC"
  },
  {
    "arxiv_id": "2412.16248v1",
    "title": "Optimizing Low-Speed Autonomous Driving: A Reinforcement Learning Approach to Route Stability and Maximum Speed",
    "authors": [
      "Benny Bao-Sheng Li",
      "Elena Wu",
      "Hins Shao-Xuan Yang",
      "Nicky Yao-Jin Liang"
    ],
    "abstract": "Autonomous driving has garnered significant attention in recent years,\nespecially in optimizing vehicle performance under varying conditions. This\npaper addresses the challenge of maintaining maximum speed stability in\nlow-speed autonomous driving while following a predefined route. Leveraging\nreinforcement learning (RL), we propose a novel approach to optimize driving\npolicies that enable the vehicle to achieve near-maximum speed without\ncompromising on safety or route accuracy, even in low-speed scenarios.",
    "categories": [
      "cs.AI",
      "cs.RO"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.16248v1",
    "published_date": "2024-12-20 01:06:41 UTC",
    "updated_date": "2024-12-20 01:06:41 UTC"
  },
  {
    "arxiv_id": "2501.10391v1",
    "title": "Developing an Ontology for AI Act Fundamental Rights Impact Assessments",
    "authors": [
      "Tytti Rintamaki",
      "Harshvardhan J. Pandit"
    ],
    "abstract": "The recently published EU Artificial Intelligence Act (AI Act) is a landmark\nregulation that regulates the use of AI technologies. One of its novel\nrequirements is the obligation to conduct a Fundamental Rights Impact\nAssessment (FRIA), where organisations in the role of deployers must assess the\nrisks of their AI system regarding health, safety, and fundamental rights.\nAnother novelty in the AI Act is the requirement to create a questionnaire and\nan automated tool to support organisations in their FRIA obligations. Such\nautomated tools will require a machine-readable form of information involved\nwithin the FRIA process, and additionally also require machine-readable\ndocumentation to enable further compliance tools to be created. In this\narticle, we present our novel representation of the FRIA as an ontology based\non semantic web standards. Our work builds upon the existing state of the art,\nnotably the Data Privacy Vocabulary (DPV), where similar works have been\nestablished to create tools for GDPR's Data Protection Impact Assessments\n(DPIA) and other obligations. Through our ontology, we enable the creation and\nmanagement of FRIA, and the use of automated tool in its various steps.",
    "categories": [
      "cs.CY",
      "cs.AI"
    ],
    "primary_category": "cs.CY",
    "comment": "Presented at CLAIRvoyant (ConventicLE on Artificial Intelligence\n  Regulation) Workshop 2024",
    "pdf_url": "http://arxiv.org/pdf/2501.10391v1",
    "published_date": "2024-12-20 00:37:33 UTC",
    "updated_date": "2024-12-20 00:37:33 UTC"
  },
  {
    "arxiv_id": "2412.15467v1",
    "title": "Non-Uniform Parameter-Wise Model Merging",
    "authors": [
      "Albert Manuel Orozco Camacho",
      "Stefan Horoi",
      "Guy Wolf",
      "Eugene Belilovsky"
    ],
    "abstract": "Combining multiple machine learning models has long been a technique for\nenhancing performance, particularly in distributed settings. Traditional\napproaches, such as model ensembles, work well, but are expensive in terms of\nmemory and compute. Recently, methods based on averaging model parameters have\nachieved good results in some settings and have gained popularity. However,\nmerging models initialized differently that do not share a part of their\ntraining trajectories can yield worse results than simply using the base\nmodels, even after aligning their neurons. In this paper, we introduce a novel\napproach, Non-uniform Parameter-wise Model Merging, or NP Merge, which merges\nmodels by learning the contribution of each parameter to the final model using\ngradient-based optimization. We empirically demonstrate the effectiveness of\nour method for merging models of various architectures in multiple settings,\noutperforming past methods. We also extend NP Merge to handle the merging of\nmultiple models, showcasing its scalability and robustness.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "9 pages, 1 figure, to be published in the Proceedings of the 9th IEEE\n  Special Session on Machine Learning on Big Data (MLBD 2024)",
    "pdf_url": "http://arxiv.org/pdf/2412.15467v1",
    "published_date": "2024-12-20 00:05:14 UTC",
    "updated_date": "2024-12-20 00:05:14 UTC"
  },
  {
    "arxiv_id": "2412.16247v2",
    "title": "Towards scientific discovery with dictionary learning: Extracting biological concepts from microscopy foundation models",
    "authors": [
      "Konstantin Donhauser",
      "Kristina Ulicna",
      "Gemma Elyse Moran",
      "Aditya Ravuri",
      "Kian Kenyon-Dean",
      "Cian Eastwood",
      "Jason Hartford"
    ],
    "abstract": "Dictionary learning (DL) has emerged as a powerful interpretability tool for\nlarge language models. By extracting known concepts (e.g., Golden-Gate Bridge)\nfrom human-interpretable data (e.g., text), sparse DL can elucidate a model's\ninner workings. In this work, we ask if DL can also be used to discover unknown\nconcepts from less human-interpretable scientific data (e.g., cell images),\nultimately enabling modern approaches to scientific discovery. As a first step,\nwe use DL algorithms to study microscopy foundation models trained on\nmulti-cell image data, where little prior knowledge exists regarding which\nhigh-level concepts should arise. We show that sparse dictionaries indeed\nextract biologically-meaningful concepts such as cell type and genetic\nperturbation type. We also propose Iterative Codebook Feature Learning~(ICFL)\nand combine it with a pre-processing step which uses PCA whitening from a\ncontrol dataset. In our experiments, we demonstrate that both ICFL and PCA\nimprove the selectivity of extracted features compared to TopK sparse\nautoencoders.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.16247v2",
    "published_date": "2024-12-20 00:01:16 UTC",
    "updated_date": "2025-02-11 16:54:45 UTC"
  }
]
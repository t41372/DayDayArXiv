[
  {
    "arxiv_id": "2510.13044v1",
    "title": "SceneAdapt: Scene-aware Adaptation of Human Motion Diffusion",
    "authors": [
      "Jungbin Cho",
      "Minsu Kim",
      "Jisoo Kim",
      "Ce Zheng",
      "Laszlo A. Jeni",
      "Ming-Hsuan Yang",
      "Youngjae Yu",
      "Seonjoo Kim"
    ],
    "abstract": "Human motion is inherently diverse and semantically rich, while also shaped by the surrounding scene. However, existing motion generation approaches address either motion semantics or scene-awareness in isolation, since constructing large-scale datasets with both rich text--motion coverage and precise scene interactions is extremely challenging. In this work, we introduce SceneAdapt, a framework that injects scene awareness into text-conditioned motion models by leveraging disjoint scene--motion and text--motion datasets through two adaptation stages: inbetweening and scene-aware inbetweening. The key idea is to use motion inbetweening, learnable without text, as a proxy task to bridge two distinct datasets and thereby inject scene-awareness to text-to-motion models. In the first stage, we introduce keyframing layers that modulate motion latents for inbetweening while preserving the latent manifold. In the second stage, we add a scene-conditioning layer that injects scene geometry by adaptively querying local context through cross-attention. Experimental results show that SceneAdapt effectively injects scene awareness into text-to-motion models, and we further analyze the mechanisms through which this awareness emerges. Code and models will be released.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "15 pages",
    "pdf_url": "https://arxiv.org/pdf/2510.13044v1",
    "published_date": "2025-10-14 23:42:10 UTC",
    "updated_date": "2025-10-14 23:42:10 UTC"
  },
  {
    "arxiv_id": "2510.13042v1",
    "title": "SeqBench: Benchmarking Sequential Narrative Generation in Text-to-Video Models",
    "authors": [
      "Zhengxu Tang",
      "Zizheng Wang",
      "Luning Wang",
      "Zitao Shuai",
      "Chenhao Zhang",
      "Siyu Qian",
      "Yirui Wu",
      "Bohao Wang",
      "Haosong Rao",
      "Zhenyu Yang",
      "Chenwei Wu"
    ],
    "abstract": "Text-to-video (T2V) generation models have made significant progress in creating visually appealing videos. However, they struggle with generating coherent sequential narratives that require logical progression through multiple events. Existing T2V benchmarks primarily focus on visual quality metrics but fail to evaluate narrative coherence over extended sequences. To bridge this gap, we present SeqBench, a comprehensive benchmark for evaluating sequential narrative coherence in T2V generation. SeqBench includes a carefully designed dataset of 320 prompts spanning various narrative complexities, with 2,560 human-annotated videos generated from 8 state-of-the-art T2V models. Additionally, we design a Dynamic Temporal Graphs (DTG)-based automatic evaluation metric, which can efficiently capture long-range dependencies and temporal ordering while maintaining computational efficiency. Our DTG-based metric demonstrates a strong correlation with human annotations. Through systematic evaluation using SeqBench, we reveal critical limitations in current T2V models: failure to maintain consistent object states across multi-action sequences, physically implausible results in multi-object scenarios, and difficulties in preserving realistic timing and ordering relationships between sequential actions. SeqBench provides the first systematic framework for evaluating narrative coherence in T2V generation and offers concrete insights for improving sequential reasoning capabilities in future models. Please refer to https://videobench.github.io/SeqBench.github.io/ for more details.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.13042v1",
    "published_date": "2025-10-14 23:40:57 UTC",
    "updated_date": "2025-10-14 23:40:57 UTC"
  },
  {
    "arxiv_id": "2510.13040v1",
    "title": "Randomness and Interpolation Improve Gradient Descent",
    "authors": [
      "Jiawen Li",
      "Pascal Lefevre",
      "Anwar Pp Abdul Majeed"
    ],
    "abstract": "Based on Stochastic Gradient Descent (SGD), the paper introduces two optimizers, named Interpolational Accelerating Gradient Descent (IAGD) as well as Noise-Regularized Stochastic Gradient Descent (NRSGD). IAGD leverages second-order Newton Interpolation to expedite the convergence process during training, assuming relevancy in gradients between iterations. To avoid over-fitting, NRSGD incorporates a noise regularization technique that introduces controlled noise to the gradients during the optimization process. Comparative experiments of this research are conducted on the CIFAR-10, and CIFAR-100 datasets, benchmarking different CNNs(Convolutional Neural Networks) with IAGD and NRSGD against classical optimizers in Keras Package. Results demonstrate the potential of those two viable improvement methods in SGD, implicating the effectiveness of the advancements.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.13040v1",
    "published_date": "2025-10-14 23:32:01 UTC",
    "updated_date": "2025-10-14 23:32:01 UTC"
  },
  {
    "arxiv_id": "2510.13036v1",
    "title": "Repairing Reward Functions with Human Feedback to Mitigate Reward Hacking",
    "authors": [
      "Stephane Hatgis-Kessell",
      "Logan Mondal Bhamidipaty",
      "Emma Brunskill"
    ],
    "abstract": "Human-designed reward functions for reinforcement learning (RL) agents are frequently misaligned with the humans' true, unobservable objectives, and thus act only as proxies. Optimizing for a misspecified proxy reward function often induces reward hacking, resulting in a policy misaligned with the human's true objectives. An alternative is to perform RL from human feedback, which involves learning a reward function from scratch by collecting human preferences over pairs of trajectories. However, building such datasets is costly. To address the limitations of both approaches, we propose Preference-Based Reward Repair (PBRR): an automated iterative framework that repairs a human-specified proxy reward function by learning an additive, transition-dependent correction term from preferences. A manually specified reward function can yield policies that are highly suboptimal under the ground-truth objective, yet corrections on only a few transitions may suffice to recover optimal performance. To identify and correct for those transitions, PBRR uses a targeted exploration strategy and a new preference-learning objective. We prove in tabular domains PBRR has a cumulative regret that matches, up to constants, that of prior preference-based RL methods. In addition, on a suite of reward-hacking benchmarks, PBRR consistently outperforms baselines that learn a reward function from scratch from preferences or modify the proxy reward function using other approaches, requiring substantially fewer preferences to learn high performing policies.",
    "categories": [
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.13036v1",
    "published_date": "2025-10-14 23:18:24 UTC",
    "updated_date": "2025-10-14 23:18:24 UTC"
  },
  {
    "arxiv_id": "2510.13029v1",
    "title": "Toward Reasoning-Centric Time-Series Analysis",
    "authors": [
      "Xinlei Wang",
      "Mingtian Tan",
      "Jing Qiu",
      "Junhua Zhao",
      "Jinjin Gu"
    ],
    "abstract": "Traditional time series analysis has long relied on pattern recognition, trained on static and well-established benchmarks. However, in real-world settings -- where policies shift, human behavior adapts, and unexpected events unfold -- effective analysis must go beyond surface-level trends to uncover the actual forces driving them. The recent rise of Large Language Models (LLMs) presents new opportunities for rethinking time series analysis by integrating multimodal inputs. However, as the use of LLMs becomes popular, we must remain cautious, asking why we use LLMs and how to exploit them effectively. Most existing LLM-based methods still employ their numerical regression ability and ignore their deeper reasoning potential. This paper argues for rethinking time series with LLMs as a reasoning task that prioritizes causal structure and explainability. This shift brings time series analysis closer to human-aligned understanding, enabling transparent and context-aware insights in complex real-world environments.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.13029v1",
    "published_date": "2025-10-14 22:59:07 UTC",
    "updated_date": "2025-10-14 22:59:07 UTC"
  },
  {
    "arxiv_id": "2510.13022v1",
    "title": "On the Role of Preference Variance in Preference Optimization",
    "authors": [
      "Jiacheng Guo",
      "Zihao Li",
      "Jiahao Qiu",
      "Yue Wu",
      "Mengdi Wang"
    ],
    "abstract": "Direct Preference Optimization (DPO) has emerged as an important approach for learning from human preferences in aligning large language models (LLMs). However, collecting human preference data is costly and inefficient, motivating methods to reduce the required annotations. In this work, we investigate the impact of \\emph{preference variance} (PVar), which measures the variance in model preferences when comparing pairs of responses, on the effectiveness of DPO training. We provide a theoretical insight by establishing an upper bound on the DPO gradient norm for any given prompt, showing it is controlled by the PVar of that prompt. This implies that prompts with low PVar can only produce small gradient updates, making them less valuable for learning. We validate this finding by fine-tuning LLMs with preferences generated by a reward model, evaluating on two benchmarks (AlpacaEval 2.0 and Arena-Hard). Experimental results demonstrate that prompts with higher PVar outperform randomly selected prompts or those with lower PVar. We also show that our PVar-based selection method is robust, when using smaller reward models (1B, 3B) for selection. Notably, in a separate experiment using the original human annotations from the UltraFeedback dataset, we found that training on only the top 10\\% of prompts with the highest PVar yields better evaluation performance than training on the full dataset, highlighting the importance of preference variance in identifying informative examples for efficient LLM alignment.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.13022v1",
    "published_date": "2025-10-14 22:34:52 UTC",
    "updated_date": "2025-10-14 22:34:52 UTC"
  },
  {
    "arxiv_id": "2510.13011v1",
    "title": "Deliberate Lab: A Platform for Real-Time Human-AI Social Experiments",
    "authors": [
      "Crystal Qian",
      "Vivian Tsai",
      "Michael Behr",
      "Nada Hussein",
      "Léo Laugier",
      "Nithum Thain",
      "Lucas Dixon"
    ],
    "abstract": "Social and behavioral scientists increasingly aim to study how humans interact, collaborate, and make decisions alongside artificial intelligence. However, the experimental infrastructure for such work remains underdeveloped: (1) few platforms support real-time, multi-party studies at scale; (2) most deployments require bespoke engineering, limiting replicability and accessibility, and (3) existing tools do not treat AI agents as first-class participants. We present Deliberate Lab, an open-source platform for large-scale, real-time behavioral experiments that supports both human participants and large language model (LLM)-based agents. We report on a 12-month public deployment of the platform (N=88 experimenters, N=9195 experiment participants), analyzing usage patterns and workflows. Case studies and usage scenarios are aggregated from platform users, complemented by in-depth interviews with select experimenters. By lowering technical barriers and standardizing support for hybrid human-AI experimentation, Deliberate Lab expands the methodological repertoire for studying collective decision-making and human-centered AI.",
    "categories": [
      "cs.HC",
      "cs.AI"
    ],
    "primary_category": "cs.HC",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.13011v1",
    "published_date": "2025-10-14 22:02:24 UTC",
    "updated_date": "2025-10-14 22:02:24 UTC"
  },
  {
    "arxiv_id": "2510.13009v1",
    "title": "Developing and Validating the Arabic Version of the Attitudes Toward Large Language Models Scale",
    "authors": [
      "Basad Barajeeh",
      "Ala Yankouskaya",
      "Sameha AlShakhsi",
      "Chun Sing Maxwell Ho",
      "Guandong Xu",
      "Raian Ali"
    ],
    "abstract": "As the use of large language models (LLMs) becomes increasingly global, understanding public attitudes toward these systems requires tools that are adapted to local contexts and languages. In the Arab world, LLM adoption has grown rapidly with both globally dominant platforms and regional ones like Fanar and Jais offering Arabic-specific solutions. This highlights the need for culturally and linguistically relevant scales to accurately measure attitudes toward LLMs in the region. Tools assessing attitudes toward artificial intelligence (AI) can provide a base for measuring attitudes specific to LLMs. The 5-item Attitudes Toward Artificial Intelligence (ATAI) scale, which measures two dimensions, the AI Fear and the AI Acceptance, has been recently adopted and adapted to develop new instruments in English using a sample from the UK: the Attitudes Toward General LLMs (AT-GLLM) and Attitudes Toward Primary LLM (AT-PLLM) scales. In this paper, we translate the two scales, AT-GLLM and AT-PLLM, and validate them using a sample of 249 Arabic-speaking adults. The results show that the scale, translated into Arabic, is a reliable and valid tool that can be used for the Arab population and language. Psychometric analyses confirmed a two-factor structure, strong measurement invariance across genders, and good internal reliability. The scales also demonstrated strong convergent and discriminant validity. Our scales will support research in a non-Western context, a much-needed effort to help draw a global picture of LLM perceptions, and will also facilitate localized research and policy-making in the Arab region.",
    "categories": [
      "cs.HC",
      "cs.AI"
    ],
    "primary_category": "cs.HC",
    "comment": "28 Pages",
    "pdf_url": "https://arxiv.org/pdf/2510.13009v1",
    "published_date": "2025-10-14 21:56:53 UTC",
    "updated_date": "2025-10-14 21:56:53 UTC"
  },
  {
    "arxiv_id": "2510.13008v1",
    "title": "CurLL: A Developmental Framework to Evaluate Continual Learning in Language Models",
    "authors": [
      "Pavan Kalyan",
      "Shubhra Mishra",
      "Satya Lokam",
      "Navin Goyal"
    ],
    "abstract": "We introduce a comprehensive continual learning dataset and benchmark (CurlL) grounded in human developmental trajectories from ages 5-10, enabling systematic and fine-grained assessment of models' ability to progressively acquire new skills. CurlL spans five developmental stages (0-4) covering ages 5-10, supported by a skill graph that breaks down broad skills into smaller abilities, concrete goals, and measurable indicators, while also capturing which abilities build on others. We generate a 23.4B-token synthetic dataset with controlled skill progression, vocabulary complexity, and format diversity, comprising paragraphs, comprehension-based QA (CQA), skill-testing QA (CSQA), and instruction-response (IR) pairs. Stage-wise token counts range from 2.12B to 6.78B tokens, supporting precise analysis of forgetting, forward transfer, and backward transfer. Using a 135M-parameter transformer trained under independent, joint, and sequential (continual) setups, we show trade-offs in skill retention and transfer efficiency. By mirroring human learning patterns and providing fine-grained control over skill dependencies, this work advances continual learning evaluations for language models.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.13008v1",
    "published_date": "2025-10-14 21:56:03 UTC",
    "updated_date": "2025-10-14 21:56:03 UTC"
  },
  {
    "arxiv_id": "2510.13006v3",
    "title": "What is Implementation Science; and Why It Matters for Bridging the Artificial Intelligence Innovation-to-Application Gap in Medical Imaging",
    "authors": [
      "Ahmad Fayaz-Bakhsh",
      "Janice Tania",
      "Syaheerah Lebai Lutfi",
      "Abhinav K. Jha",
      "Arman Rahmim"
    ],
    "abstract": "The transformative potential of artificial intelligence (AI) in medical Imaging (MI) is well recognized. Yet despite promising reports in research settings, many AI tools fail to achieve clinical adoption in practice. In fact, more generally, there is a documented 17-year average delay between evidence generation and implementation of a technology. Implementation science (IS) may provide a practical, evidence-based framework to bridge the gap between AI development and real-world clinical imaging use, to shorten this lag through systematic frameworks, strategies, and hybrid research designs. We outline challenges specific to AI adoption in MI workflows, including infrastructural, educational, and cultural barriers. We highlight the complementary roles of effectiveness research and implementation research, emphasizing hybrid study designs and the role of integrated KT (iKT), stakeholder engagement, and equity-focused co-creation in designing sustainable and generalizable solutions. We discuss integration of Human-Computer Interaction (HCI) frameworks in MI towards usable AI. Adopting IS is not only a methodological advancement; it is a strategic imperative for accelerating translation of innovation into improved patient outcomes.",
    "categories": [
      "physics.med-ph",
      "cs.AI"
    ],
    "primary_category": "physics.med-ph",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.13006v3",
    "published_date": "2025-10-14 21:50:31 UTC",
    "updated_date": "2025-11-24 19:53:22 UTC"
  },
  {
    "arxiv_id": "2510.13002v1",
    "title": "From Narratives to Probabilistic Reasoning: Predicting and Interpreting Drivers' Hazardous Actions in Crashes Using Large Language Model",
    "authors": [
      "Boyou Chen",
      "Gerui Xu",
      "Zifei Wang",
      "Huizhong Guo",
      "Ananna Ahmed",
      "Zhaonan Sun",
      "Zhen Hu",
      "Kaihan Zhang",
      "Shan Bao"
    ],
    "abstract": "Vehicle crashes involve complex interactions between road users, split-second decisions, and challenging environmental conditions. Among these, two-vehicle crashes are the most prevalent, accounting for approximately 70% of roadway crashes and posing a significant challenge to traffic safety. Identifying Driver Hazardous Action (DHA) is essential for understanding crash causation, yet the reliability of DHA data in large-scale databases is limited by inconsistent and labor-intensive manual coding practices. Here, we present an innovative framework that leverages a fine-tuned large language model to automatically infer DHAs from textual crash narratives, thereby improving the validity and interpretability of DHA classifications. Using five years of two-vehicle crash data from MTCF, we fine-tuned the Llama 3.2 1B model on detailed crash narratives and benchmarked its performance against conventional machine learning classifiers, including Random Forest, XGBoost, CatBoost, and a neural network. The fine-tuned LLM achieved an overall accuracy of 80%, surpassing all baseline models and demonstrating pronounced improvements in scenarios with imbalanced data. To increase interpretability, we developed a probabilistic reasoning approach, analyzing model output shifts across original test sets and three targeted counterfactual scenarios: variations in driver distraction and age. Our analysis revealed that introducing distraction for one driver substantially increased the likelihood of \"General Unsafe Driving\"; distraction for both drivers maximized the probability of \"Both Drivers Took Hazardous Actions\"; and assigning a teen driver markedly elevated the probability of \"Speed and Stopping Violations.\" Our framework and analytical methods provide a robust and interpretable solution for large-scale automated DHA detection, offering new opportunities for traffic safety analysis and intervention.",
    "categories": [
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.13002v1",
    "published_date": "2025-10-14 21:35:47 UTC",
    "updated_date": "2025-10-14 21:35:47 UTC"
  },
  {
    "arxiv_id": "2510.12997v2",
    "title": "Max It or Miss It: Benchmarking LLM On Solving Extremal Problems",
    "authors": [
      "Binxin Gao",
      "Jingjun Han"
    ],
    "abstract": "Test-time scaling has enabled Large Language Models (LLMs) with remarkable reasoning capabilities, particularly in mathematical domains, through intermediate chain-of-thought (CoT) reasoning before generating final answers. However, the specific sources and mechanisms underlying these reasoning capabilities remain insufficiently understood. Optimization reasoning, i.e. finding extrema under constraints, represents a fundamental abstraction that underpins critical applications in planning, control, resource allocation, and prompt search. To systematically evaluate this capability, we introduce ExtremBench, a benchmark dataset for solving mathematical extremal problems, curated from inequality exercises used for Chinese Mathematical Olympiad and transformed into $93$ standardized extrema-finding problems. We conduct extensive evaluations across various state-of-the-art open-source model families, including the Qwen3, GPT-OSS, and DeepSeek. Our results reveal that LLMs' extremal-solving reasoning capabilities do not always align with those of current mathematical benchmarks such as AIME25 and MATH-500, with some models showing strong general mathematical reasoning but poor extremal-solving skills, and vice versa. This discrepancy highlights a critical gap in current evaluation practices and suggests that existing benchmarks may not comprehensively capture the full spectrum of mathematical reasoning abilities.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.LG",
    "comment": "Our benchmark dataset is available at https://huggingface.co/datasets/binxingao/extrem-bench",
    "pdf_url": "https://arxiv.org/pdf/2510.12997v2",
    "published_date": "2025-10-14 21:23:37 UTC",
    "updated_date": "2025-10-17 21:17:43 UTC"
  },
  {
    "arxiv_id": "2512.00009v2",
    "title": "Development and Benchmarking of a Blended Human-AI Qualitative Research Assistant",
    "authors": [
      "Joseph Matveyenko",
      "James Liu",
      "John David Parsons",
      "Ryan A. Brown",
      "Alina Palimaru",
      "Prateek Puri"
    ],
    "abstract": "Qualitative research emphasizes constructing meaning through iterative engagement with textual data. Traditionally this human-driven process requires navigating coder fatigue and interpretative drift, thus posing challenges when scaling analysis to larger, more complex datasets. Computational approaches to augment qualitative research have been met with skepticism, partly due to their inability to replicate the nuance, context-awareness, and sophistication of human analysis. Large language models, however, present new opportunities to automate aspects of qualitative analysis while upholding rigor and research quality in important ways. To assess their benefits and limitations - and build trust among qualitative researchers - these approaches must be rigorously benchmarked against human-generated datasets. In this work, we benchmark Muse, an interactive, AI-powered qualitative research system that allows researchers to identify themes and annotate datasets, finding an inter-rater reliability between Muse and humans of Cohen's $κ$ = 0.71 for well-specified codes. We also conduct robust error analysis to identify failure mode, guide future improvements, and demonstrate the capacity to correct for human bias.",
    "categories": [
      "cs.HC",
      "cs.AI"
    ],
    "primary_category": "cs.HC",
    "comment": "32 pages, 9 figures",
    "pdf_url": "https://arxiv.org/pdf/2512.00009v2",
    "published_date": "2025-10-14 21:17:34 UTC",
    "updated_date": "2025-12-15 01:41:02 UTC"
  },
  {
    "arxiv_id": "2510.13905v2",
    "title": "Schema for In-Context Learning",
    "authors": [
      "Pan Chen",
      "Shaohong Chen",
      "Mark Wang",
      "Shi Xuan Leong",
      "Priscilla Fung",
      "Varinia Bernales",
      "Alan Aspuru-Guzik"
    ],
    "abstract": "In-Context Learning (ICL) enables transformer-based language models to adapt to new tasks by conditioning on demonstration examples. However, traditional example-driven in-context learning lacks explicit modules for knowledge retrieval and transfer at the abstraction level. Inspired by cognitive science, specifically schema theory, which holds that humans interpret new information by activating pre-existing mental frameworks (schemas) to structure understanding, we introduce SCHEMA ACTIVATED IN CONTEXT LEARNING (SA-ICL). This framework extracts the representation of the building blocks of cognition for the reasoning process instilled from prior examples, creating an abstracted schema, a lightweight, structured template of key inferential steps and their relationships, which is then used to augment a model's reasoning process when presented with a novel question. We demonstrate that a broad range of large language models (LLMs) lack the capacity to form and utilize internal schema-based learning representations implicitly, but instead benefit significantly from explicit schema-based scaffolding. Across chemistry and physics questions from the GPQA dataset, our experiments show that SA-ICL consistently boosts performance, up to 36.19 percent, when the single demonstration example is of high quality, which simultaneously reduces reliance on the number of demonstrations and enhances interpretability. SCHEMA ACTIVATED IN CONTEXT LEARNING not only bridges disparate ICL strategies ranging from pattern priming to Chain-of-Thought prompting, but also paves a new path for enhancing human-like reasoning in LLMs.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.13905v2",
    "published_date": "2025-10-14 21:00:15 UTC",
    "updated_date": "2025-10-23 18:04:38 UTC"
  },
  {
    "arxiv_id": "2510.12985v1",
    "title": "SENTINEL: A Multi-Level Formal Framework for Safety Evaluation of LLM-based Embodied Agents",
    "authors": [
      "Simon Sinong Zhan",
      "Yao Liu",
      "Philip Wang",
      "Zinan Wang",
      "Qineng Wang",
      "Zhian Ruan",
      "Xiangyu Shi",
      "Xinyu Cao",
      "Frank Yang",
      "Kangrui Wang",
      "Huajie Shao",
      "Manling Li",
      "Qi Zhu"
    ],
    "abstract": "We present Sentinel, the first framework for formally evaluating the physical safety of Large Language Model(LLM-based) embodied agents across the semantic, plan, and trajectory levels. Unlike prior methods that rely on heuristic rules or subjective LLM judgments, Sentinel grounds practical safety requirements in formal temporal logic (TL) semantics that can precisely specify state invariants, temporal dependencies, and timing constraints. It then employs a multi-level verification pipeline where (i) at the semantic level, intuitive natural language safety requirements are formalized into TL formulas and the LLM agent's understanding of these requirements is probed for alignment with the TL formulas; (ii) at the plan level, high-level action plans and subgoals generated by the LLM agent are verified against the TL formulas to detect unsafe plans before execution; and (iii) at the trajectory level, multiple execution trajectories are merged into a computation tree and efficiently verified against physically-detailed TL specifications for a final safety check. We apply Sentinel in VirtualHome and ALFRED, and formally evaluate multiple LLM-based embodied agents against diverse safety requirements. Our experiments show that by grounding physical safety in temporal logic and applying verification methods across multiple levels, Sentinel provides a rigorous foundation for systematically evaluating LLM-based embodied agents in physical environments, exposing safety violations overlooked by previous methods and offering insights into their failure modes.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.12985v1",
    "published_date": "2025-10-14 20:53:51 UTC",
    "updated_date": "2025-10-14 20:53:51 UTC"
  },
  {
    "arxiv_id": "2510.12979v1",
    "title": "DeepPlanner: Scaling Planning Capability for Deep Research Agents via Advantage Shaping",
    "authors": [
      "Wei Fan",
      "Wenlin Yao",
      "Zheng Li",
      "Feng Yao",
      "Xin Liu",
      "Liang Qiu",
      "Qingyu Yin",
      "Yangqiu Song",
      "Bing Yin"
    ],
    "abstract": "Large language models (LLMs) augmented with multi-step reasoning and action generation abilities have shown promise in leveraging external tools to tackle complex tasks that require long-horizon planning. However, existing approaches either rely on implicit planning in the reasoning stage or introduce explicit planners without systematically addressing how to optimize the planning stage. As evidence, we observe that under vanilla reinforcement learning (RL), planning tokens exhibit significantly higher entropy than other action tokens, revealing uncertain decision points that remain under-optimized. To address this, we propose DeepPlanner, an end-to-end RL framework that effectively enhances the planning capabilities of deep research agents. Our approach shapes token-level advantage with an entropy-based term to allocate larger updates to high entropy tokens, and selectively upweights sample-level advantages for planning-intensive rollouts. Extensive experiments across seven deep research benchmarks demonstrate that DeepPlanner improves planning quality and achieves state-of-the-art results under a substantially lower training budget.",
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.AI",
    "comment": "Under Review",
    "pdf_url": "https://arxiv.org/pdf/2510.12979v1",
    "published_date": "2025-10-14 20:47:05 UTC",
    "updated_date": "2025-10-14 20:47:05 UTC"
  },
  {
    "arxiv_id": "2510.12957v1",
    "title": "A Multimodal XAI Framework for Trustworthy CNNs and Bias Detection in Deep Representation Learning",
    "authors": [
      "Noor Islam S. Mohammad"
    ],
    "abstract": "Standard benchmark datasets, such as MNIST, often fail to expose latent biases and multimodal feature complexities, limiting the trustworthiness of deep neural networks in high-stakes applications. We propose a novel multimodal Explainable AI (XAI) framework that unifies attention-augmented feature fusion, Grad-CAM++-based local explanations, and a Reveal-to-Revise feedback loop for bias detection and mitigation. Evaluated on multimodal extensions of MNIST, our approach achieves 93.2% classification accuracy, 91.6% F1-score, and 78.1% explanation fidelity (IoU-XAI), outperforming unimodal and non-explainable baselines. Ablation studies demonstrate that integrating interpretability with bias-aware learning enhances robustness and human alignment. Our work bridges the gap between performance, transparency, and fairness, highlighting a practical pathway for trustworthy AI in sensitive domains.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.12957v1",
    "published_date": "2025-10-14 20:06:09 UTC",
    "updated_date": "2025-10-14 20:06:09 UTC"
  },
  {
    "arxiv_id": "2510.13903v1",
    "title": "Benefits and Limitations of Communication in Multi-Agent Reasoning",
    "authors": [
      "Michael Rizvi-Martel",
      "Satwik Bhattamishra",
      "Neil Rathi",
      "Guillaume Rabusseau",
      "Michael Hahn"
    ],
    "abstract": "Chain-of-thought prompting has popularized step-by-step reasoning in large language models, yet model performance still degrades as problem complexity and context length grow. By decomposing difficult tasks with long contexts into shorter, manageable ones, recent multi-agent paradigms offer a promising near-term solution to this problem. However, the fundamental capacities of such systems are poorly understood. In this work, we propose a theoretical framework to analyze the expressivity of multi-agent systems. We apply our framework to three algorithmic families: state tracking, recall, and $k$-hop reasoning. We derive bounds on (i) the number of agents required to solve the task exactly, (ii) the quantity and structure of inter-agent communication, and (iii) the achievable speedups as problem size and context scale. Our results identify regimes where communication is provably beneficial, delineate tradeoffs between agent count and bandwidth, and expose intrinsic limitations when either resource is constrained. We complement our theoretical analysis with a set of experiments on pretrained LLMs using controlled synthetic benchmarks. Empirical outcomes confirm the tradeoffs between key quantities predicted by our theory. Collectively, our analysis offers principled guidance for designing scalable multi-agent reasoning systems.",
    "categories": [
      "cs.MA",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.MA",
    "comment": "34 pages, 14 figures",
    "pdf_url": "https://arxiv.org/pdf/2510.13903v1",
    "published_date": "2025-10-14 20:04:27 UTC",
    "updated_date": "2025-10-14 20:04:27 UTC"
  },
  {
    "arxiv_id": "2510.24737v1",
    "title": "Cardi-GPT: An Expert ECG-Record Processing Chatbot",
    "authors": [
      "Koustav Mallick",
      "Neel Singh",
      "Mohammedreza Hajiarbabi"
    ],
    "abstract": "Interpreting and communicating electrocardiogram (ECG) findings are crucial yet challenging tasks in cardiovascular diagnosis, traditionally requiring significant expertise and precise clinical communication. This paper introduces Cardi-GPT, an advanced expert system designed to streamline ECG interpretation and enhance clinical communication through deep learning and natural language interaction. Cardi-GPT employs a 16-residual-block convolutional neural network (CNN) to process 12-lead ECG data, achieving a weighted accuracy of 0.6194 across 24 cardiac conditions. A novel fuzzification layer converts complex numerical outputs into clinically meaningful linguistic categories, while an integrated chatbot interface facilitates intuitive exploration of diagnostic insights and seamless communication between healthcare providers.\n  The system was evaluated on a diverse dataset spanning six hospitals across four countries, demonstrating superior performance compared to baseline models. Additionally, Cardi-GPT achieved an impressive overall response quality score of 73\\%, assessed using a comprehensive evaluation framework that measures coverage, grounding, and coherence. By bridging the gap between intricate ECG data interpretation and actionable clinical insights, Cardi-GPT represents a transformative innovation in cardiovascular healthcare, promising to improve diagnostic accuracy, clinical workflows, and patient outcomes across diverse medical settings.",
    "categories": [
      "eess.SP",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "eess.SP",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.24737v1",
    "published_date": "2025-10-14 19:58:33 UTC",
    "updated_date": "2025-10-14 19:58:33 UTC"
  },
  {
    "arxiv_id": "2510.12953v2",
    "title": "Epistemic-aware Vision-Language Foundation Model for Fetal Ultrasound Interpretation",
    "authors": [
      "Xiao He",
      "Huangxuan Zhao",
      "Guojia Wan",
      "Wei Zhou",
      "Yanxing Liu",
      "Juhua Liu",
      "Yongchao Xu",
      "Yong Luo",
      "Dacheng Tao",
      "Bo Du"
    ],
    "abstract": "Recent medical vision-language models have shown promise on tasks such as VQA, report generation, and anomaly detection. However, most are adapted to structured adult imaging and underperform in fetal ultrasound, which poses challenges of multi-view image reasoning, numerous diseases, and image diversity. To bridge this gap, we introduce FetalMind, a medical AI system tailored to fetal ultrasound for both report generation and diagnosis. Guided by clinical workflow, we propose Salient Epistemic Disentanglement (SED), which injects an expert-curated bipartite graph into the model to decouple view-disease associations and to steer preference selection along clinically faithful steps via reinforcement learning. This design mitigates variability across diseases and heterogeneity across views, reducing learning bottlenecks while aligning the model's inference with obstetric practice. To train FetalMind at scale, we curate FetalSigma-1M dataset, the first large-scale fetal ultrasound report corpus, comprising 20K reports from twelve medical centers, addressing the scarcity of domain data. Extensive experiments show that FetalMind outperforms open- and closed-source baselines across all gestational stages, achieving +14% average gains and +61.2% higher accuracy on critical conditions while remaining efficient, stable, and scalable. Project Page: https://hexiao0275.github.io/FetalMind.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.IR",
      "cs.MM"
    ],
    "primary_category": "cs.CV",
    "comment": "This paper contains fundamental errors and will not be replaced",
    "pdf_url": "https://arxiv.org/pdf/2510.12953v2",
    "published_date": "2025-10-14 19:57:03 UTC",
    "updated_date": "2025-10-23 03:45:15 UTC"
  },
  {
    "arxiv_id": "2510.12948v1",
    "title": "SpareCodeSearch: Searching for Code Context When You Have No Spare GPU",
    "authors": [
      "Minh Nguyen"
    ],
    "abstract": "Retrieval-Augmented Generation (RAG) frameworks aim to enhance Code Language Models (CLMs) by including another module for retrieving relevant context to construct the input prompt. However, these retrieval modules commonly use semantic search, requiring substantial computational resources for training and hosting these embedded models, making them infeasible to integrate into lightweight applications such as in-IDE AI-based code completion. In this solution paper, we prove that using keyword-search is sufficient to retrieve relevant and useful code context inside large codebases, without the need for extensive GPU resources. The usefulness of code contexts found by our solution is demonstrated through their completion results on the Code Context Competition's benchmark, reaching 0.748 and 0.725 chRF scores on Kotlin and Python tracks, respectively.",
    "categories": [
      "cs.SE",
      "cs.AI"
    ],
    "primary_category": "cs.SE",
    "comment": "4 pages, 3 figures, 4 tables. Accepted to Context Collection Workshop co-located with ASE'25",
    "pdf_url": "https://arxiv.org/pdf/2510.12948v1",
    "published_date": "2025-10-14 19:48:50 UTC",
    "updated_date": "2025-10-14 19:48:50 UTC"
  },
  {
    "arxiv_id": "2510.12947v1",
    "title": "HyWA: Hypernetwork Weight Adapting Personalized Voice Activity Detection",
    "authors": [
      "Mahsa Ghazvini Nejad",
      "Hamed Jafarzadeh Asl",
      "Amin Edraki",
      "Mohammadreza Sadeghi",
      "Masoud Asgharian",
      "Yuanhao Yu",
      "Vahid Partovi Nia"
    ],
    "abstract": "Personalized Voice Activity Detection (PVAD) systems activate only in response to a specific target speaker by incorporating speaker embeddings from enrollment utterances. Unlike existing methods that require architectural changes, such as FiLM layers, our approach employs a hypernetwork to modify the weights of a few selected layers within a standard voice activity detection (VAD) model. This enables speaker conditioning without changing the VAD architecture, allowing the same VAD model to adapt to different speakers by updating only a small subset of the layers. We propose HyWA-PVAD, a hypernetwork weight adaptation method, and evaluate it against multiple baseline conditioning techniques. Our comparison shows consistent improvements in PVAD performance. HyWA also offers practical advantages for deployment by preserving the core VAD architecture. Our new approach improves the current conditioning techniques in two ways: i) increases the mean average precision, ii) simplifies deployment by reusing the same VAD architecture.",
    "categories": [
      "eess.AS",
      "cs.AI",
      "cs.LG",
      "cs.SD"
    ],
    "primary_category": "eess.AS",
    "comment": "Mahsa Ghazvini Nejad and Hamed Jafarzadeh Asl contributed equally to this work",
    "pdf_url": "https://arxiv.org/pdf/2510.12947v1",
    "published_date": "2025-10-14 19:46:40 UTC",
    "updated_date": "2025-10-14 19:46:40 UTC"
  },
  {
    "arxiv_id": "2510.13900v1",
    "title": "Narrow Finetuning Leaves Clearly Readable Traces in Activation Differences",
    "authors": [
      "Julian Minder",
      "Clément Dumas",
      "Stewart Slocum",
      "Helena Casademunt",
      "Cameron Holmes",
      "Robert West",
      "Neel Nanda"
    ],
    "abstract": "Finetuning on narrow domains has become an essential tool to adapt Large Language Models (LLMs) to specific tasks and to create models with known unusual properties that are useful for research. We show that narrow finetuning creates strong biases in LLM activations that can be interpreted to understand the finetuning domain. These biases can be discovered using simple tools from model diffing - the study of differences between models before and after finetuning. In particular, analyzing activation differences on the first few tokens of random text and steering by adding this difference to the model activations produces text similar to the format and general content of the finetuning data. We demonstrate that these analyses contain crucial information by creating an LLM-based interpretability agent to understand the finetuning domain. With access to the bias, the agent performs significantly better compared to baseline agents using simple prompting. Our analysis spans synthetic document finetuning for false facts, emergent misalignment, subliminal learning, and taboo word guessing game models across different architectures (Gemma, LLaMA, Qwen) and scales (1B to 32B parameters). We suspect these biases reflect overfitting and find that mixing pretraining data into the finetuning corpus largely removes them, though residual risks may remain. Our work (1) demonstrates that narrowly finetuned models have salient traces of their training objective in their activations and suggests ways to improve how they are trained, (2) warns AI safety and interpretability researchers that the common practice of using such models as a proxy for studying broader finetuning (e.g., chat-tuning) might not be realistic, and (3) highlights the need for deeper investigation into the effects of narrow finetuning and development of truly realistic case studies for model-diffing, safety and interpretability research.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.13900v1",
    "published_date": "2025-10-14 19:05:59 UTC",
    "updated_date": "2025-10-14 19:05:59 UTC"
  },
  {
    "arxiv_id": "2510.12920v1",
    "title": "InferA: A Smart Assistant for Cosmological Ensemble Data",
    "authors": [
      "Justin Z. Tam",
      "Pascal Grosset",
      "Divya Banesh",
      "Nesar Ramachandra",
      "Terece L. Turton",
      "James Ahrens"
    ],
    "abstract": "Analyzing large-scale scientific datasets presents substantial challenges due to their sheer volume, structural complexity, and the need for specialized domain knowledge. Automation tools, such as PandasAI, typically require full data ingestion and lack context of the full data structure, making them impractical as intelligent data analysis assistants for datasets at the terabyte scale. To overcome these limitations, we propose InferA, a multi-agent system that leverages large language models to enable scalable and efficient scientific data analysis. At the core of the architecture is a supervisor agent that orchestrates a team of specialized agents responsible for distinct phases of the data retrieval and analysis. The system engages interactively with users to elicit their analytical intent and confirm query objectives, ensuring alignment between user goals and system actions. To demonstrate the framework's usability, we evaluate the system using ensemble runs from the HACC cosmology simulation which comprises several terabytes.",
    "categories": [
      "astro-ph.IM",
      "cs.AI"
    ],
    "primary_category": "astro-ph.IM",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.12920v1",
    "published_date": "2025-10-14 18:47:22 UTC",
    "updated_date": "2025-10-14 18:47:22 UTC"
  },
  {
    "arxiv_id": "2510.12872v2",
    "title": "KVCOMM: Online Cross-context KV-cache Communication for Efficient LLM-based Multi-agent Systems",
    "authors": [
      "Hancheng Ye",
      "Zhengqi Gao",
      "Mingyuan Ma",
      "Qinsi Wang",
      "Yuzhe Fu",
      "Ming-Yu Chung",
      "Yueqian Lin",
      "Zhijian Liu",
      "Jianyi Zhang",
      "Danyang Zhuo",
      "Yiran Chen"
    ],
    "abstract": "Multi-agent large language model (LLM) systems are increasingly adopted for complex language processing tasks that require communication and coordination among agents. However, these systems often suffer substantial overhead from repeated reprocessing of overlapping contexts across agents. In typical pipelines, once an agent receives a message from its predecessor, the full context-including prior turns-must be reprocessed from scratch, leading to inefficient processing. While key-value (KV) caching is an effective solution for avoiding redundant computation in single-agent settings where prefixes remain unchanged, it cannot be directly reused in multi-agent scenarios due to diverging prefixes introduced by agent-specific context extensions. We identify that the core challenge lies in the offset variance of KV-caches across agents. To address this, we propose KVCOMM, a training-free framework that enables efficient prefilling in multi-agent inference by reusing KV-caches and aligning cache offsets of overlapping contexts under diverse prefix contexts. KVCOMM estimates and adjusts KV-caches for shared content by referencing a pool of cached examples-termed anchors-that store observed cache deviations under varying prefixes. The anchor pool is maintained and updated online, allowing dynamic adaptation to distinct user requests and context structures. KVCOMM achieves over 70% reuse rate across diverse multi-agent workloads, including retrieval-augmented generation, math reasoning, and collaborative coding tasks, all without quality degradation. Particularly, when each fully-connected agent receives 1K input tokens with 512 prefix tokens and 512 output tokens under a five-agent setting, KVCOMM achieves up to 7.8x speedup compared to the standard prefill pipeline, reducing TTFT from ~430 ms to ~55 ms.",
    "categories": [
      "cs.MA",
      "cs.AI",
      "stat.ML"
    ],
    "primary_category": "cs.MA",
    "comment": "Accepted for publication in NeurIPS2025. Code is available at \\url{https://github.com/FastMAS/KVCOMM}",
    "pdf_url": "https://arxiv.org/pdf/2510.12872v2",
    "published_date": "2025-10-14 18:00:01 UTC",
    "updated_date": "2025-11-01 08:26:24 UTC"
  },
  {
    "arxiv_id": "2510.12796v2",
    "title": "DriveVLA-W0: World Models Amplify Data Scaling Law in Autonomous Driving",
    "authors": [
      "Yingyan Li",
      "Shuyao Shang",
      "Weisong Liu",
      "Bing Zhan",
      "Haochen Wang",
      "Yuqi Wang",
      "Yuntao Chen",
      "Xiaoman Wang",
      "Yasong An",
      "Chufeng Tang",
      "Lu Hou",
      "Lue Fan",
      "Zhaoxiang Zhang"
    ],
    "abstract": "Scaling Vision-Language-Action (VLA) models on large-scale data offers a promising path to achieving a more generalized driving intelligence. However, VLA models are limited by a ``supervision deficit'': the vast model capacity is supervised by sparse, low-dimensional actions, leaving much of their representational power underutilized. To remedy this, we propose \\textbf{DriveVLA-W0}, a training paradigm that employs world modeling to predict future images. This task generates a dense, self-supervised signal that compels the model to learn the underlying dynamics of the driving environment. We showcase the paradigm's versatility by instantiating it for two dominant VLA archetypes: an autoregressive world model for VLAs that use discrete visual tokens, and a diffusion world model for those operating on continuous visual features. Building on the rich representations learned from world modeling, we introduce a lightweight action expert to address the inference latency for real-time deployment. Extensive experiments on the NAVSIM v1/v2 benchmark and a 680x larger in-house dataset demonstrate that DriveVLA-W0 significantly outperforms BEV and VLA baselines. Crucially, it amplifies the data scaling law, showing that performance gains accelerate as the training dataset size increases.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.12796v2",
    "published_date": "2025-10-14 17:59:47 UTC",
    "updated_date": "2025-12-18 07:25:29 UTC"
  },
  {
    "arxiv_id": "2510.12795v1",
    "title": "CuMPerLay: Learning Cubical Multiparameter Persistence Vectorizations",
    "authors": [
      "Caner Korkmaz",
      "Brighton Nuwagira",
      "Barış Coşkunuzer",
      "Tolga Birdal"
    ],
    "abstract": "We present CuMPerLay, a novel differentiable vectorization layer that enables the integration of Cubical Multiparameter Persistence (CMP) into deep learning pipelines. While CMP presents a natural and powerful way to topologically work with images, its use is hindered by the complexity of multifiltration structures as well as the vectorization of CMP. In face of these challenges, we introduce a new algorithm for vectorizing MP homologies of cubical complexes. Our CuMPerLay decomposes the CMP into a combination of individual, learnable single-parameter persistence, where the bifiltration functions are jointly learned. Thanks to the differentiability, its robust topological feature vectors can be seamlessly used within state-of-the-art architectures such as Swin Transformers. We establish theoretical guarantees for the stability of our vectorization under generalized Wasserstein metrics. Our experiments on benchmark medical imaging and computer vision datasets show the benefit CuMPerLay on classification and segmentation performance, particularly in limited-data scenarios. Overall, CuMPerLay offers a promising direction for integrating global structural information into deep networks for structured image analysis.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG",
      "math.AT",
      "stat.ML"
    ],
    "primary_category": "cs.CV",
    "comment": "Appears at ICCV 2025",
    "pdf_url": "https://arxiv.org/pdf/2510.12795v1",
    "published_date": "2025-10-14 17:59:01 UTC",
    "updated_date": "2025-10-14 17:59:01 UTC"
  },
  {
    "arxiv_id": "2510.12789v1",
    "title": "UniFusion: Vision-Language Model as Unified Encoder in Image Generation",
    "authors": [
      "Kevin Li",
      "Manuel Brack",
      "Sudeep Katakol",
      "Hareesh Ravi",
      "Ajinkya Kale"
    ],
    "abstract": "Although recent advances in visual generation have been remarkable, most existing architectures still depend on distinct encoders for images and text. This separation constrains diffusion models' ability to perform cross-modal reasoning and knowledge transfer. Prior attempts to bridge this gap often use the last layer information from VLM, employ multiple visual encoders, or train large unified models jointly for text and image generation, which demands substantial computational resources and large-scale data, limiting its accessibility.We present UniFusion, a diffusion-based generative model conditioned on a frozen large vision-language model (VLM) that serves as a unified multimodal encoder. At the core of UniFusion is the Layerwise Attention Pooling (LAP) mechanism that extracts both high level semantics and low level details from text and visual tokens of a frozen VLM to condition a diffusion generative model. We demonstrate that LAP outperforms other shallow fusion architectures on text-image alignment for generation and faithful transfer of visual information from VLM to the diffusion model which is key for editing. We propose VLM-Enabled Rewriting Injection with Flexibile Inference (VERIFI), which conditions a diffusion transformer (DiT) only on the text tokens generated by the VLM during in-model prompt rewriting. VERIFI combines the alignment of the conditioning distribution with the VLM's reasoning capabilities for increased capabilities and flexibility at inference. In addition, finetuning on editing task not only improves text-image alignment for generation, indicative of cross-modality knowledge transfer, but also exhibits tremendous generalization capabilities. Our model when trained on single image editing, zero-shot generalizes to multiple image references further motivating the unified encoder design of UniFusion.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "Project page at https://thekevinli.github.io/unifusion/",
    "pdf_url": "https://arxiv.org/pdf/2510.12789v1",
    "published_date": "2025-10-14 17:57:56 UTC",
    "updated_date": "2025-10-14 17:57:56 UTC"
  },
  {
    "arxiv_id": "2510.12787v3",
    "title": "Ax-Prover: A Deep Reasoning Agentic Framework for Theorem Proving in Mathematics and Quantum Physics",
    "authors": [
      "Benjamin Breen",
      "Marco Del Tredici",
      "Jacob McCarran",
      "Javier Aspuru Mijares",
      "Weichen Winston Yin",
      "Kfir Sulimany",
      "Jacob M. Taylor",
      "Frank H. L. Koppens",
      "Dirk Englund"
    ],
    "abstract": "We present Ax-Prover, a multi-agent system for automated theorem proving in Lean that can solve problems across diverse scientific domains and operate either autonomously or collaboratively with human experts. To achieve this, Ax-Prover approaches scientific problem solving through formal proof generation, a process that demands both creative reasoning and strict syntactic rigor. Ax-Prover meets this challenge by equipping Large Language Models (LLMs), which provide knowledge and reasoning, with Lean tools via the Model Context Protocol (MCP), which ensure formal correctness. To evaluate its performance as an autonomous prover, we benchmark our approach against frontier LLMs and specialized prover models on two public math benchmarks and on two Lean benchmarks we introduce in the fields of abstract algebra and quantum theory. On public datasets, Ax-Prover is competitive with state-of-the-art provers, while it largely outperforms them on the new benchmarks. This shows that, unlike specialized systems that struggle to generalize, our tool-based agentic theorem prover approach offers a generalizable methodology for formal verification across diverse scientific domains. Furthermore, we demonstrate Ax-Prover's assistant capabilities in a practical use case, showing how it enabled an expert mathematician to formalize the proof of a complex cryptography theorem.",
    "categories": [
      "cs.AI",
      "cs.MA"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.12787v3",
    "published_date": "2025-10-14 17:57:04 UTC",
    "updated_date": "2025-11-13 18:56:10 UTC"
  },
  {
    "arxiv_id": "2510.12785v1",
    "title": "MVP4D: Multi-View Portrait Video Diffusion for Animatable 4D Avatars",
    "authors": [
      "Felix Taubner",
      "Ruihang Zhang",
      "Mathieu Tuli",
      "Sherwin Bahmani",
      "David B. Lindell"
    ],
    "abstract": "Digital human avatars aim to simulate the dynamic appearance of humans in virtual environments, enabling immersive experiences across gaming, film, virtual reality, and more. However, the conventional process for creating and animating photorealistic human avatars is expensive and time-consuming, requiring large camera capture rigs and significant manual effort from professional 3D artists. With the advent of capable image and video generation models, recent methods enable automatic rendering of realistic animated avatars from a single casually captured reference image of a target subject. While these techniques significantly lower barriers to avatar creation and offer compelling realism, they lack constraints provided by multi-view information or an explicit 3D representation. So, image quality and realism degrade when rendered from viewpoints that deviate strongly from the reference image. Here, we build a video model that generates animatable multi-view videos of digital humans based on a single reference image and target expressions. Our model, MVP4D, is based on a state-of-the-art pre-trained video diffusion model and generates hundreds of frames simultaneously from viewpoints varying by up to 360 degrees around a target subject. We show how to distill the outputs of this model into a 4D avatar that can be rendered in real-time. Our approach significantly improves the realism, temporal consistency, and 3D consistency of generated avatars compared to previous methods.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.GR"
    ],
    "primary_category": "cs.CV",
    "comment": "18 pages, 12 figures",
    "pdf_url": "https://arxiv.org/pdf/2510.12785v1",
    "published_date": "2025-10-14 17:56:14 UTC",
    "updated_date": "2025-10-14 17:56:14 UTC"
  },
  {
    "arxiv_id": "2510.12773v1",
    "title": "Dr.LLM: Dynamic Layer Routing in LLMs",
    "authors": [
      "Ahmed Heakl",
      "Martin Gubri",
      "Salman Khan",
      "Sangdoo Yun",
      "Seong Joon Oh"
    ],
    "abstract": "Large Language Models (LLMs) process every token through all layers of a transformer stack, causing wasted computation on simple queries and insufficient flexibility for harder ones that need deeper reasoning. Adaptive-depth methods can improve efficiency, but prior approaches rely on costly inference-time search, architectural changes, or large-scale retraining, and in practice often degrade accuracy despite efficiency gains. We introduce Dr.LLM, Dynamic routing of Layers for LLMs, a retrofittable framework that equips pretrained models with lightweight per-layer routers deciding to skip, execute, or repeat a block. Routers are trained with explicit supervision: using Monte Carlo Tree Search (MCTS), we derive high-quality layer configurations that preserve or improve accuracy under a compute budget. Our design, windowed pooling for stable routing, focal loss with class balancing, and bottleneck MLP routers, ensures robustness under class imbalance and long sequences. On ARC (logic) and DART (math), Dr.LLM improves accuracy by up to +3.4%p while saving 5 layers per example on average. Routers generalize to out-of-domain tasks (MMLU, GSM8k, AIME, TruthfulQA, SQuADv2, GPQA, PIQA, AGIEval) with only 0.85% accuracy drop while retaining efficiency, and outperform prior routing methods by up to +7.7%p. Overall, Dr.LLM shows that explicitly supervised routers retrofit frozen LLMs for budget-aware, accuracy-driven inference without altering base weights.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "17 pages, Under submission",
    "pdf_url": "https://arxiv.org/pdf/2510.12773v1",
    "published_date": "2025-10-14 17:51:26 UTC",
    "updated_date": "2025-10-14 17:51:26 UTC"
  },
  {
    "arxiv_id": "2510.12768v1",
    "title": "Uncertainty Matters in Dynamic Gaussian Splatting for Monocular 4D Reconstruction",
    "authors": [
      "Fengzhi Guo",
      "Chih-Chuan Hsu",
      "Sihao Ding",
      "Cheng Zhang"
    ],
    "abstract": "Reconstructing dynamic 3D scenes from monocular input is fundamentally under-constrained, with ambiguities arising from occlusion and extreme novel views. While dynamic Gaussian Splatting offers an efficient representation, vanilla models optimize all Gaussian primitives uniformly, ignoring whether they are well or poorly observed. This limitation leads to motion drifts under occlusion and degraded synthesis when extrapolating to unseen views. We argue that uncertainty matters: Gaussians with recurring observations across views and time act as reliable anchors to guide motion, whereas those with limited visibility are treated as less reliable. To this end, we introduce USplat4D, a novel Uncertainty-aware dynamic Gaussian Splatting framework that propagates reliable motion cues to enhance 4D reconstruction. Our key insight is to estimate time-varying per-Gaussian uncertainty and leverages it to construct a spatio-temporal graph for uncertainty-aware optimization. Experiments on diverse real and synthetic datasets show that explicitly modeling uncertainty consistently improves dynamic Gaussian Splatting models, yielding more stable geometry under occlusion and high-quality synthesis at extreme viewpoints.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.GR"
    ],
    "primary_category": "cs.CV",
    "comment": "Project page: https://tamu-visual-ai.github.io/usplat4d/",
    "pdf_url": "https://arxiv.org/pdf/2510.12768v1",
    "published_date": "2025-10-14 17:47:11 UTC",
    "updated_date": "2025-10-14 17:47:11 UTC"
  },
  {
    "arxiv_id": "2510.12763v1",
    "title": "Disentangling Neurodegeneration with Brain Age Gap Prediction Models: A Graph Signal Processing Perspective",
    "authors": [
      "Saurabh Sihag",
      "Gonzalo Mateos",
      "Alejandro Ribeiro"
    ],
    "abstract": "Neurodegeneration, characterized by the progressive loss of neuronal structure or function, is commonly assessed in clinical practice through reductions in cortical thickness or brain volume, as visualized by structural MRI. While informative, these conventional approaches lack the statistical sophistication required to fully capture the spatially correlated and heterogeneous nature of neurodegeneration, which manifests both in healthy aging and in neurological disorders. To address these limitations, brain age gap has emerged as a promising data-driven biomarker of brain health. The brain age gap prediction (BAGP) models estimate the difference between a person's predicted brain age from neuroimaging data and their chronological age. The resulting brain age gap serves as a compact biomarker of brain health, with recent studies demonstrating its predictive utility for disease progression and severity. However, practical adoption of BAGP models is hindered by their methodological obscurities and limited generalizability across diverse clinical populations. This tutorial article provides an overview of BAGP and introduces a principled framework for this application based on recent advancements in graph signal processing (GSP). In particular, we focus on graph neural networks (GNNs) and introduce the coVariance neural network (VNN), which leverages the anatomical covariance matrices derived from structural MRI. VNNs offer strong theoretical grounding and operational interpretability, enabling robust estimation of brain age gap predictions. By integrating perspectives from GSP, machine learning, and network neuroscience, this work clarifies the path forward for reliable and interpretable BAGP models and outlines future research directions in personalized medicine.",
    "categories": [
      "eess.SP",
      "cs.AI",
      "q-bio.QM"
    ],
    "primary_category": "eess.SP",
    "comment": "Accepted for publication in IEEE Signal Processing Magazine",
    "pdf_url": "https://arxiv.org/pdf/2510.12763v1",
    "published_date": "2025-10-14 17:44:45 UTC",
    "updated_date": "2025-10-14 17:44:45 UTC"
  },
  {
    "arxiv_id": "2510.12750v1",
    "title": "VQArt-Bench: A semantically rich VQA Benchmark for Art and Cultural Heritage",
    "authors": [
      "A. Alfarano",
      "L. Venturoli",
      "D. Negueruela del Castillo"
    ],
    "abstract": "Multimodal Large Language Models (MLLMs) have demonstrated significant capabilities in joint visual and linguistic tasks. However, existing Visual Question Answering (VQA) benchmarks often fail to evaluate deep semantic understanding, particularly in complex domains like visual art analysis. Confined to simple syntactic structures and surface-level attributes, these questions fail to capture the diversity and depth of human visual inquiry. This limitation incentivizes models to exploit statistical shortcuts rather than engage in visual reasoning. To address this gap, we introduce VQArt-Bench, a new, large-scale VQA benchmark for the cultural heritage domain. This benchmark is constructed using a novel multi-agent pipeline where specialized agents collaborate to generate nuanced, validated, and linguistically diverse questions. The resulting benchmark is structured along relevant visual understanding dimensions that probe a model's ability to interpret symbolic meaning, narratives, and complex visual relationships. Our evaluation of 14 state-of-the-art MLLMs on this benchmark reveals significant limitations in current models, including a surprising weakness in simple counting tasks and a clear performance gap between proprietary and open-source models.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.12750v1",
    "published_date": "2025-10-14 17:29:52 UTC",
    "updated_date": "2025-10-14 17:29:52 UTC"
  },
  {
    "arxiv_id": "2510.12742v1",
    "title": "CTRL-Rec: Controlling Recommender Systems With Natural Language",
    "authors": [
      "Micah Carroll",
      "Adeline Foote",
      "Kevin Feng",
      "Marcus Williams",
      "Anca Dragan",
      "W. Bradley Knox",
      "Smitha Milli"
    ],
    "abstract": "When users are dissatisfied with recommendations from a recommender system, they often lack fine-grained controls for changing them. Large language models (LLMs) offer a solution by allowing users to guide their recommendations through natural language requests (e.g., \"I want to see respectful posts with a different perspective than mine\"). We propose a method, CTRL-Rec, that allows for natural language control of traditional recommender systems in real-time with computational efficiency. Specifically, at training time, we use an LLM to simulate whether users would approve of items based on their language requests, and we train embedding models that approximate such simulated judgments. We then integrate these user-request-based predictions into the standard weighting of signals that traditional recommender systems optimize. At deployment time, we require only a single LLM embedding computation per user request, allowing for real-time control of recommendations. In experiments with the MovieLens dataset, our method consistently allows for fine-grained control across a diversity of requests. In a study with 19 Letterboxd users, we find that CTRL-Rec was positively received by users and significantly enhanced users' sense of control and satisfaction with recommendations compared to traditional controls.",
    "categories": [
      "cs.AI",
      "cs.IR"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.12742v1",
    "published_date": "2025-10-14 17:20:04 UTC",
    "updated_date": "2025-10-14 17:20:04 UTC"
  },
  {
    "arxiv_id": "2510.12740v2",
    "title": "Hey, wait a minute: on at-issue sensitivity in Language Models",
    "authors": [
      "Sanghee J. Kim",
      "Kanishka Misra"
    ],
    "abstract": "Evaluating the naturalness of dialogue in language models (LMs) is not trivial: notions of 'naturalness' vary, and scalable quantitative metrics remain limited. This study leverages the linguistic notion of 'at-issueness' to assess dialogue naturalness and introduces a new method: Divide, Generate, Recombine, and Compare (DGRC). DGRC (i) divides a dialogue as a prompt, (ii) generates continuations for subparts using LMs, (iii) recombines the dialogue and continuations, and (iv) compares the likelihoods of the recombined sequences. This approach mitigates bias in linguistic analyses of LMs and enables systematic testing of discourse-sensitive behavior. Applying DGRC, we find that LMs prefer to continue dialogue on at-issue content, with this effect enhanced in instruct-tuned models. They also reduce their at-issue preference when relevant cues (e.g., \"Hey, wait a minute\") are present. Although instruct-tuning does not further amplify this modulation, the pattern reflects a hallmark of successful dialogue dynamics.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "10 pages, 5 figures, 3 tables. See https://github.com/sangheek16/hey-wait-a-minute for code and data",
    "pdf_url": "https://arxiv.org/pdf/2510.12740v2",
    "published_date": "2025-10-14 17:17:20 UTC",
    "updated_date": "2025-11-04 16:32:37 UTC"
  },
  {
    "arxiv_id": "2510.12733v2",
    "title": "HYPE: Hybrid Planning with Ego Proposal-Conditioned Predictions",
    "authors": [
      "Hang Yu",
      "Julian Jordan",
      "Julian Schmidt",
      "Silvan Lindner",
      "Alessandro Canevaro",
      "Wilhelm Stork"
    ],
    "abstract": "Safe and interpretable motion planning in complex urban environments needs to reason about bidirectional multi-agent interactions. This reasoning requires to estimate the costs of potential ego driving maneuvers. Many existing planners generate initial trajectories with sampling-based methods and refine them by optimizing on learned predictions of future environment states, which requires a cost function that encodes the desired vehicle behavior. Designing such a cost function can be very challenging, especially if a wide range of complex urban scenarios has to be considered. We propose HYPE: HYbrid Planning with Ego proposal-conditioned predictions, a planner that integrates multimodal trajectory proposals from a learned proposal model as heuristic priors into a Monte Carlo Tree Search (MCTS) refinement. To model bidirectional interactions, we introduce an ego-conditioned occupancy prediction model, enabling consistent, scene-aware reasoning. Our design significantly simplifies cost function design in refinement by considering proposal-driven guidance, requiring only minimalistic grid-based cost terms. Evaluations on large-scale real-world benchmarks nuPlan and DeepUrban show that HYPE effectively achieves state-of-the-art performance, especially in safety and adaptability.",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.RO",
    "comment": "Accepted to IEEE ITSC 2025",
    "pdf_url": "https://arxiv.org/pdf/2510.12733v2",
    "published_date": "2025-10-14 17:11:04 UTC",
    "updated_date": "2025-10-23 20:59:51 UTC"
  },
  {
    "arxiv_id": "2510.12732v2",
    "title": "Clutch Control: An Attention-based Combinatorial Bandit for Efficient Mutation in JavaScript Engine Fuzzing",
    "authors": [
      "Myles Foley",
      "Sergio Maffeis",
      "Muhammad Fakhrur Rozi",
      "Takeshi Takahashi"
    ],
    "abstract": "JavaScript engines are widely used in web browsers, PDF readers, and server-side applications. The rise in concern over their security has led to the development of several targeted fuzzing techniques. However, existing approaches use random selection to determine where to perform mutations in JavaScript code. We postulate that the problem of selecting better mutation targets is suitable for combinatorial bandits with a volatile number of arms. Thus, we propose CLUTCH, a novel deep combinatorial bandit that can observe variable length JavaScript test case representations, using an attention mechanism from deep learning. Furthermore, using Concrete Dropout, CLUTCH can dynamically adapt its exploration. We show that CLUTCH increases efficiency in JavaScript fuzzing compared to three state-of-the-art solutions by increasing the number of valid test cases and coverage-per-testcase by, respectively, 20.3% and 8.9% on average. In volatile and combinatorial settings we show that CLUTCH outperforms state-of-the-art bandits, achieving at least 78.1% and 4.1% less regret in volatile and combinatorial settings, respectively.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.12732v2",
    "published_date": "2025-10-14 17:10:51 UTC",
    "updated_date": "2025-11-14 12:43:37 UTC"
  },
  {
    "arxiv_id": "2510.13897v1",
    "title": "Dual-attention ResNet outperforms transformers in HER2 prediction on DCE-MRI",
    "authors": [
      "Naomi Fridman",
      "Anat Goldstein"
    ],
    "abstract": "Breast cancer is the most diagnosed cancer in women, with HER2 status critically guiding treatment decisions. Noninvasive prediction of HER2 status from dynamic contrast-enhanced MRI (DCE-MRI) could streamline diagnostics and reduce reliance on biopsy. However, preprocessing high-dynamic-range DCE-MRI into standardized 8-bit RGB format for pretrained neural networks is nontrivial, and normalization strategy significantly affects model performance. We benchmarked intensity normalization strategies using a Triple-Head Dual-Attention ResNet that processes RGB-fused temporal sequences from three DCE phases. Trained on a multicenter cohort (n=1,149) from the I-SPY trials and externally validated on BreastDCEDL_AMBL (n=43 lesions), our model outperformed transformer-based architectures, achieving 0.75 accuracy and 0.74 AUC on I-SPY test data. N4 bias field correction slightly degraded performance. Without fine-tuning, external validation yielded 0.66 AUC, demonstrating cross-institutional generalizability. These findings highlight the effectiveness of dual-attention mechanisms in capturing transferable spatiotemporal features for HER2 stratification, advancing reproducible deep learning biomarkers in breast cancer imaging.",
    "categories": [
      "q-bio.QM",
      "cs.AI"
    ],
    "primary_category": "q-bio.QM",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.13897v1",
    "published_date": "2025-10-14 17:08:17 UTC",
    "updated_date": "2025-10-14 17:08:17 UTC"
  },
  {
    "arxiv_id": "2510.12727v1",
    "title": "Hierarchical Federated Learning for Crop Yield Prediction in Smart Agricultural Production Systems",
    "authors": [
      "Anas Abouaomar",
      "Mohammed El hanjri",
      "Abdellatif Kobbane",
      "Anis Laouiti",
      "Khalid Nafil"
    ],
    "abstract": "In this paper, we presents a novel hierarchical federated learning architecture specifically designed for smart agricultural production systems and crop yield prediction. Our approach introduces a seasonal subscription mechanism where farms join crop-specific clusters at the beginning of each agricultural season. The proposed three-layer architecture consists of individual smart farms at the client level, crop-specific aggregators at the middle layer, and a global model aggregator at the top level. Within each crop cluster, clients collaboratively train specialized models tailored to specific crop types, which are then aggregated to produce a higher-level global model that integrates knowledge across multiple crops. This hierarchical design enables both local specialization for individual crop types and global generalization across diverse agricultural contexts while preserving data privacy and reducing communication overhead. Experiments demonstrate the effectiveness of the proposed system, showing that local and crop-layer models closely follow actual yield patterns with consistent alignment, significantly outperforming standard machine learning models. The results validate the advantages of hierarchical federated learning in the agricultural context, particularly for scenarios involving heterogeneous farming environments and privacy-sensitive agricultural data.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.DC"
    ],
    "primary_category": "cs.LG",
    "comment": "6 pages, 3 figures, conference",
    "pdf_url": "https://arxiv.org/pdf/2510.12727v1",
    "published_date": "2025-10-14 17:06:23 UTC",
    "updated_date": "2025-10-14 17:06:23 UTC"
  },
  {
    "arxiv_id": "2510.13896v1",
    "title": "GenCellAgent: Generalizable, Training-Free Cellular Image Segmentation via Large Language Model Agents",
    "authors": [
      "Xi Yu",
      "Yang Yang",
      "Qun Liu",
      "Yonghua Du",
      "Sean McSweeney",
      "Yuewei Lin"
    ],
    "abstract": "Cellular image segmentation is essential for quantitative biology yet remains difficult due to heterogeneous modalities, morphological variability, and limited annotations. We present GenCellAgent, a training-free multi-agent framework that orchestrates specialist segmenters and generalist vision-language models via a planner-executor-evaluator loop (choose tool $\\rightarrow$ run $\\rightarrow$ quality-check) with long-term memory. The system (i) automatically routes images to the best tool, (ii) adapts on the fly using a few reference images when imaging conditions differ from what a tool expects, (iii) supports text-guided segmentation of organelles not covered by existing models, and (iv) commits expert edits to memory, enabling self-evolution and personalized workflows. Across four cell-segmentation benchmarks, this routing yields a 15.7\\% mean accuracy gain over state-of-the-art baselines. On endoplasmic reticulum and mitochondria from new datasets, GenCellAgent improves average IoU by 37.6\\% over specialist models. It also segments novel objects such as the Golgi apparatus via iterative text-guided refinement, with light human correction further boosting performance. Together, these capabilities provide a practical path to robust, adaptable cellular image segmentation without retraining, while reducing annotation burden and matching user preferences.",
    "categories": [
      "q-bio.QM",
      "cs.AI",
      "cs.CV",
      "cs.MA"
    ],
    "primary_category": "q-bio.QM",
    "comment": "43 pages",
    "pdf_url": "https://arxiv.org/pdf/2510.13896v1",
    "published_date": "2025-10-14 17:02:57 UTC",
    "updated_date": "2025-10-14 17:02:57 UTC"
  },
  {
    "arxiv_id": "2510.12714v1",
    "title": "Artificial intelligence for simplified patient-centered dosimetry in radiopharmaceutical therapies",
    "authors": [
      "Alejandro Lopez-Montes",
      "Fereshteh Yousefirizi",
      "Yizhou Chen",
      "Yazdan Salimi",
      "Robert Seifert",
      "Ali Afshar-Oromieh",
      "Carlos Uribe",
      "Axel Rominger",
      "Habib Zaidi",
      "Arman Rahmim",
      "Kuangyu Shi"
    ],
    "abstract": "KEY WORDS: Artificial Intelligence (AI), Theranostics, Dosimetry, Radiopharmaceutical Therapy (RPT), Patient-friendly dosimetry KEY POINTS - The rapid evolution of radiopharmaceutical therapy (RPT) highlights the growing need for personalized and patient-centered dosimetry. - Artificial Intelligence (AI) offers solutions to the key limitations in current dosimetry calculations. - The main advances on AI for simplified dosimetry toward patient-friendly RPT are reviewed. - Future directions on the role of AI in RPT dosimetry are discussed.",
    "categories": [
      "physics.med-ph",
      "cs.AI",
      "physics.app-ph"
    ],
    "primary_category": "physics.med-ph",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.12714v1",
    "published_date": "2025-10-14 16:55:36 UTC",
    "updated_date": "2025-10-14 16:55:36 UTC"
  },
  {
    "arxiv_id": "2510.12713v1",
    "title": "Towards Robust Artificial Intelligence: Self-Supervised Learning Approach for Out-of-Distribution Detection",
    "authors": [
      "Wissam Salhab",
      "Darine Ameyed",
      "Hamid Mcheick",
      "Fehmi Jaafar"
    ],
    "abstract": "Robustness in AI systems refers to their ability to maintain reliable and accurate performance under various conditions, including out-of-distribution (OOD) samples, adversarial attacks, and environmental changes. This is crucial in safety-critical systems, such as autonomous vehicles, transportation, or healthcare, where malfunctions could have severe consequences. This paper proposes an approach to improve OOD detection without the need of labeled data, thereby increasing the AI systems' robustness. The proposed approach leverages the principles of self-supervised learning, allowing the model to learn useful representations from unlabeled data. Combined with graph-theoretical techniques, this enables the more efficient identification and categorization of OOD samples. Compared to existing state-of-the-art methods, this approach achieved an Area Under the Receiver Operating Characteristic Curve (AUROC) = 0.99.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.12713v1",
    "published_date": "2025-10-14 16:55:25 UTC",
    "updated_date": "2025-10-14 16:55:25 UTC"
  },
  {
    "arxiv_id": "2510.12712v3",
    "title": "Beyond Seeing: Evaluating Multimodal LLMs on Tool-Enabled Image Perception, Transformation, and Reasoning",
    "authors": [
      "Xingang Guo",
      "Utkarsh Tyagi",
      "Advait Gosai",
      "Paula Vergara",
      "Jayeon Park",
      "Ernesto Gabriel Hernández Montoya",
      "Chen Bo Calvin Zhang",
      "Bin Hu",
      "Yunzhong He",
      "Bing Liu",
      "Rakshith Sharma Srinivasa"
    ],
    "abstract": "Multimodal Large Language Models (MLLMs) are increasingly applied in real-world scenarios where user-provided images are often imperfect, requiring active image manipulations such as cropping, editing, or enhancement to uncover salient visual cues. Beyond static visual perception, MLLMs must also think with images: dynamically transforming visual content and integrating it with other tools to solve complex tasks. However, this shift from treating vision as passive context to a manipulable cognitive workspace remains underexplored. Most existing benchmarks still follow a think about images paradigm, where images are regarded as static inputs. To address this gap, we introduce VisualToolBench, a visual tool-use reasoning benchmark that rigorously evaluates MLLMs' ability to perceive, transform, and reason across complex visual-textual tasks under the think-with-images paradigm. VisualToolBench comprises 1,204 challenging, open-ended vision tasks (603 single-turn, 601 multi-turn) spanning across five diverse domains, each paired with detailed rubrics to enable systematic evaluation. Our evaluation shows that current MLLMs struggle with tasks requiring effective integration of vision and general-purpose tools. Even the strongest model (GPT-5-think) reaches only 18.68% pass rate. We further observe divergent tool-use behaviors, with OpenAI models benefiting from diverse image manipulations while Gemini-2.5-pro shows no improvement. By introducing the first benchmark centered on think with images, VisualToolBench offers critical insights for advancing visual intelligence in MLLMs.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.12712v3",
    "published_date": "2025-10-14 16:50:49 UTC",
    "updated_date": "2025-10-24 23:29:20 UTC"
  },
  {
    "arxiv_id": "2510.12864v1",
    "title": "From Literal to Liberal: A Meta-Prompting Framework for Eliciting Human-Aligned Exception Handling in Large Language Models",
    "authors": [
      "Imran Khan"
    ],
    "abstract": "Large Language Models (LLMs) are increasingly being deployed as the reasoning engines for agentic AI systems, yet they exhibit a critical flaw: a rigid adherence to explicit rules that leads to decisions misaligned with human common sense and intent. This \"rule-rigidity\" is a significant barrier to building trustworthy autonomous agents. While prior work has shown that supervised fine-tuning (SFT) with human explanations can mitigate this issue, SFT is computationally expensive and inaccessible to many practitioners. To address this gap, we introduce the Rule-Intent Distinction (RID) Framework, a novel, low-compute meta-prompting technique designed to elicit human-aligned exception handling in LLMs in a zero-shot manner. The RID framework provides the model with a structured cognitive schema for deconstructing tasks, classifying rules, weighing conflicting outcomes, and justifying its final decision. We evaluated the RID framework against baseline and Chain-of-Thought (CoT) prompting on a custom benchmark of 20 scenarios requiring nuanced judgment across diverse domains. Our human-verified results demonstrate that the RID framework significantly improves performance, achieving a 95% Human Alignment Score (HAS), compared to 80% for the baseline and 75% for CoT. Furthermore, it consistently produces higher-quality, intent-driven reasoning. This work presents a practical, accessible, and effective method for steering LLMs from literal instruction-following to liberal, goal-oriented reasoning, paving the way for more reliable and pragmatic AI agents.",
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "13 pages. Code and data are available at https://github.com/strongSoda/LITERAL-TO-LIBERAL",
    "pdf_url": "https://arxiv.org/pdf/2510.12864v1",
    "published_date": "2025-10-14 16:42:52 UTC",
    "updated_date": "2025-10-14 16:42:52 UTC"
  },
  {
    "arxiv_id": "2510.12704v1",
    "title": "Hybrid Explanation-Guided Learning for Transformer-Based Chest X-Ray Diagnosis",
    "authors": [
      "Shelley Zixin Shu",
      "Haozhe Luo",
      "Alexander Poellinger",
      "Mauricio Reyes"
    ],
    "abstract": "Transformer-based deep learning models have demonstrated exceptional performance in medical imaging by leveraging attention mechanisms for feature representation and interpretability. However, these models are prone to learning spurious correlations, leading to biases and limited generalization. While human-AI attention alignment can mitigate these issues, it often depends on costly manual supervision. In this work, we propose a Hybrid Explanation-Guided Learning (H-EGL) framework that combines self-supervised and human-guided constraints to enhance attention alignment and improve generalization. The self-supervised component of H-EGL leverages class-distinctive attention without relying on restrictive priors, promoting robustness and flexibility. We validate our approach on chest X-ray classification using the Vision Transformer (ViT), where H-EGL outperforms two state-of-the-art Explanation-Guided Learning (EGL) methods, demonstrating superior classification accuracy and generalization capability. Additionally, it produces attention maps that are better aligned with human expertise.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "Accepted by iMIMIC at MICCAI 2025",
    "pdf_url": "https://arxiv.org/pdf/2510.12704v1",
    "published_date": "2025-10-14 16:39:02 UTC",
    "updated_date": "2025-10-14 16:39:02 UTC"
  },
  {
    "arxiv_id": "2510.12703v1",
    "title": "CAMNet: Leveraging Cooperative Awareness Messages for Vehicle Trajectory Prediction",
    "authors": [
      "Mattia Grasselli",
      "Angelo Porrello",
      "Carlo Augusto Grazia"
    ],
    "abstract": "Autonomous driving remains a challenging task, particularly due to safety concerns. Modern vehicles are typically equipped with expensive sensors such as LiDAR, cameras, and radars to reduce the risk of accidents. However, these sensors face inherent limitations: their field of view and line of sight can be obstructed by other vehicles, thereby reducing situational awareness. In this context, vehicle-to-vehicle communication plays a crucial role, as it enables cars to share information and remain aware of each other even when sensors are occluded. One way to achieve this is through the use of Cooperative Awareness Messages (CAMs). In this paper, we investigate the use of CAM data for vehicle trajectory prediction. Specifically, we design and train a neural network, Cooperative Awareness Message-based Graph Neural Network (CAMNet), on a widely used motion forecasting dataset. We then evaluate the model on a second dataset that we created from scratch using Cooperative Awareness Messages, in order to assess whether this type of data can be effectively exploited. Our approach demonstrates promising results, showing that CAMs can indeed support vehicle trajectory prediction. At the same time, we discuss several limitations of the approach, which highlight opportunities for future research.",
    "categories": [
      "cs.AI",
      "cs.NI"
    ],
    "primary_category": "cs.AI",
    "comment": "Accepted at the IEEE Consumer Communications & Networking Conference (CCNC) 2026 - Las Vegas, NV, USA 9 - 12 January 2026",
    "pdf_url": "https://arxiv.org/pdf/2510.12703v1",
    "published_date": "2025-10-14 16:37:52 UTC",
    "updated_date": "2025-10-14 16:37:52 UTC"
  },
  {
    "arxiv_id": "2510.12702v1",
    "title": "Beyond Postconditions: Can Large Language Models infer Formal Contracts for Automatic Software Verification?",
    "authors": [
      "Cedric Richter",
      "Heike Wehrheim"
    ],
    "abstract": "Automatic software verifiers have become increasingly effective at the task of checking software against (formal) specifications. Yet, their adoption in practice has been hampered by the lack of such specifications in real world code. Large Language Models (LLMs) have shown promise in inferring formal postconditions from natural language hints embedded in code such as function names, comments or documentation. Using the generated postconditions as specifications in a subsequent verification, however, often leads verifiers to suggest invalid inputs, hinting at potential issues that ultimately turn out to be false alarms.\n  To address this, we revisit the problem of specification inference from natural language in the context of automatic software verification. In the process, we introduce NL2Contract, the task of employing LLMs to translate informal natural language into formal functional contracts, consisting of postconditions as well as preconditions. We introduce metrics to validate and compare different NL2Contract approaches, using soundness, bug discriminative power of the generated contracts and their usability in the context of automatic software verification as key metrics. We evaluate NL2Contract with different LLMs and compare it to the task of postcondition generation nl2postcond. Our evaluation shows that (1) LLMs are generally effective at generating functional contracts sound for all possible inputs, (2) the generated contracts are sufficiently expressive for discriminating buggy from correct behavior, and (3) verifiers supplied with LLM inferred functional contracts produce fewer false alarms than when provided with postconditions alone. Further investigations show that LLM inferred preconditions generally align well with developers intentions which allows us to use automatic software verifiers to catch real-world bugs.",
    "categories": [
      "cs.SE",
      "cs.AI",
      "cs.PL"
    ],
    "primary_category": "cs.SE",
    "comment": "under submission",
    "pdf_url": "https://arxiv.org/pdf/2510.12702v1",
    "published_date": "2025-10-14 16:37:39 UTC",
    "updated_date": "2025-10-14 16:37:39 UTC"
  },
  {
    "arxiv_id": "2510.12700v2",
    "title": "Topological Signatures of ReLU Neural Network Activation Patterns",
    "authors": [
      "Vicente Bosca",
      "Tatum Rask",
      "Sunia Tanweer",
      "Andrew R. Tawfeek",
      "Branden Stone"
    ],
    "abstract": "This paper explores the topological signatures of ReLU neural network activation patterns. We consider feedforward neural networks with ReLU activation functions and analyze the polytope decomposition of the feature space induced by the network. Mainly, we investigate how the Fiedler partition of the dual graph and show that it appears to correlate with the decision boundary -- in the case of binary classification. Additionally, we compute the homology of the cellular decomposition -- in a regression task -- to draw similar patterns in behavior between the training loss and polyhedral cell-count, as the model is trained.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CG",
      "math.AT",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.12700v2",
    "published_date": "2025-10-14 16:36:34 UTC",
    "updated_date": "2026-01-09 01:40:17 UTC"
  },
  {
    "arxiv_id": "2510.12699v1",
    "title": "Generation Space Size: Understanding and Calibrating Open-Endedness of LLM Generations",
    "authors": [
      "Sunny Yu",
      "Ahmad Jabbar",
      "Robert Hawkins",
      "Dan Jurafsky",
      "Myra Cheng"
    ],
    "abstract": "Different open-ended generation tasks require different degrees of output diversity. However, current LLMs are often miscalibrated. They collapse to overly homogeneous outputs for creative tasks and hallucinate diverse but incorrect responses for factual tasks. We argue that these two failure modes are unified by, and can both be addressed by, the notion of effective generation space size (GSS) -- the set of semantically distinct outputs a model considers for a prompt. We present GSSBench, a task suite of prompt pairs with ground-truth GSS relationships to assess different metrics and understand where models diverge from desired behavior. We find that hallucination detection metrics, particularly EigenScore, consistently outperform standard diversity and uncertainty quantification metrics, while using only model internals, providing interpretable insights into a model's internal task representations. We demonstrate three applications of GSS: (1) detecting prompt ambiguity and predicting clarification questions for better grounding, (2) interpreting overthinking and underthinking in reasoning models, and (3) steering models to expand their generation space to yield high-quality and diverse outputs.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.12699v1",
    "published_date": "2025-10-14 16:31:34 UTC",
    "updated_date": "2025-10-14 16:31:34 UTC"
  },
  {
    "arxiv_id": "2510.12697v1",
    "title": "Multi-Agent Debate for LLM Judges with Adaptive Stability Detection",
    "authors": [
      "Tianyu Hu",
      "Zhen Tan",
      "Song Wang",
      "Huaizhi Qu",
      "Tianlong Chen"
    ],
    "abstract": "With advancements in reasoning capabilities, Large Language Models (LLMs) are increasingly employed for automated judgment tasks. While LLMs-as-Judges offer promise in automating evaluations, current approaches often rely on simplistic aggregation methods (e.g., majority voting), which can fail even when individual agents provide correct answers. To address this, we propose a multi-agent debate judge framework where agents collaboratively reason and iteratively refine their responses. We formalize the debate process mathematically, analyzing agent interactions and proving that debate amplifies correctness compared to static ensembles. To enhance efficiency, we introduce a stability detection mechanism that models judge consensus dynamics via a time-varying Beta-Binomial mixture, with adaptive stopping based on distributional similarity (Kolmogorov-Smirnov test). This mechanism models the judges' collective correct rate dynamics using a time-varying mixture of Beta-Binomial distributions and employs an adaptive stopping criterion based on distributional similarity (Kolmogorov-Smirnov statistic). Experiments across multiple benchmarks and models demonstrate that our framework improves judgment accuracy over majority voting while maintaining computational efficiency.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.12697v1",
    "published_date": "2025-10-14 16:30:30 UTC",
    "updated_date": "2025-10-14 16:30:30 UTC"
  },
  {
    "arxiv_id": "2510.12693v1",
    "title": "ERA: Transforming VLMs into Embodied Agents via Embodied Prior Learning and Online Reinforcement Learning",
    "authors": [
      "Hanyang Chen",
      "Mark Zhao",
      "Rui Yang",
      "Qinwei Ma",
      "Ke Yang",
      "Jiarui Yao",
      "Kangrui Wang",
      "Hao Bai",
      "Zhenhailong Wang",
      "Rui Pan",
      "Mengchao Zhang",
      "Jose Barreiros",
      "Aykut Onol",
      "ChengXiang Zhai",
      "Heng Ji",
      "Manling Li",
      "Huan Zhang",
      "Tong Zhang"
    ],
    "abstract": "Recent advances in embodied AI highlight the potential of vision language models (VLMs) as agents capable of perception, reasoning, and interaction in complex environments. However, top-performing systems rely on large-scale models that are costly to deploy, while smaller VLMs lack the necessary knowledge and skills to succeed. To bridge this gap, we present \\textit{Embodied Reasoning Agent (ERA)}, a two-stage framework that integrates prior knowledge learning and online reinforcement learning (RL). The first stage, \\textit{Embodied Prior Learning}, distills foundational knowledge from three types of data: (1) Trajectory-Augmented Priors, which enrich existing trajectory data with structured reasoning generated by stronger models; (2) Environment-Anchored Priors, which provide in-environment knowledge and grounding supervision; and (3) External Knowledge Priors, which transfer general knowledge from out-of-environment datasets. In the second stage, we develop an online RL pipeline that builds on these priors to further enhance agent performance. To overcome the inherent challenges in agent RL, including long horizons, sparse rewards, and training instability, we introduce three key designs: self-summarization for context management, dense reward shaping, and turn-level policy optimization. Extensive experiments on both high-level planning (EB-ALFRED) and low-level control (EB-Manipulation) tasks demonstrate that ERA-3B surpasses both prompting-based large models and previous training-based baselines. Specifically, it achieves overall improvements of 8.4\\% on EB-ALFRED and 19.4\\% on EB-Manipulation over GPT-4o, and exhibits strong generalization to unseen tasks. Overall, ERA offers a practical path toward scalable embodied intelligence, providing methodological insights for future embodied AI systems.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.12693v1",
    "published_date": "2025-10-14 16:25:46 UTC",
    "updated_date": "2025-10-14 16:25:46 UTC"
  },
  {
    "arxiv_id": "2510.12692v1",
    "title": "Who is a Better Matchmaker? Human vs. Algorithmic Judge Assignment in a High-Stakes Startup Competition",
    "authors": [
      "Sarina Xi",
      "Orelia Pi",
      "Miaomiao Zhang",
      "Becca Xiong",
      "Jacqueline Ng Lane",
      "Nihar B. Shah"
    ],
    "abstract": "There is growing interest in applying artificial intelligence (AI) to automate and support complex decision-making tasks. However, it remains unclear how algorithms compare to human judgment in contexts requiring semantic understanding and domain expertise. We examine this in the context of the judge assignment problem, matching submissions to suitably qualified judges. Specifically, we tackled this problem at the Harvard President's Innovation Challenge, the university's premier venture competition awarding over \\$500,000 to student and alumni startups. This represents a real-world environment where high-quality judge assignment is essential. We developed an AI-based judge-assignment algorithm, Hybrid Lexical-Semantic Similarity Ensemble (HLSE), and deployed it at the competition. We then evaluated its performance against human expert assignments using blinded match-quality scores from judges on $309$ judge-venture pairs. Using a Mann-Whitney U statistic based test, we found no statistically significant difference in assignment quality between the two approaches ($AUC=0.48, p=0.40$); on average, algorithmic matches are rated $3.90$ and manual matches $3.94$ on a 5-point scale, where 5 indicates an excellent match. Furthermore, manual assignments that previously required a full week could be automated in several hours by the algorithm during deployment. These results demonstrate that HLSE achieves human-expert-level matching quality while offering greater scalability and efficiency, underscoring the potential of AI-driven solutions to support and enhance human decision-making for judge assignment in high-stakes settings.",
    "categories": [
      "cs.HC",
      "cs.AI",
      "cs.CL",
      "cs.CY",
      "cs.LG"
    ],
    "primary_category": "cs.HC",
    "comment": "17 Pages, 2 figures",
    "pdf_url": "https://arxiv.org/pdf/2510.12692v1",
    "published_date": "2025-10-14 16:25:09 UTC",
    "updated_date": "2025-10-14 16:25:09 UTC"
  },
  {
    "arxiv_id": "2510.12691v3",
    "title": "DiffEM: Learning from Corrupted Data with Diffusion Models via Expectation Maximization",
    "authors": [
      "Danial Hosseintabar",
      "Fan Chen",
      "Giannis Daras",
      "Antonio Torralba",
      "Constantinos Daskalakis"
    ],
    "abstract": "Diffusion models have emerged as powerful generative priors for high-dimensional inverse problems, yet learning them when only corrupted or noisy observations are available remains challenging. In this work, we propose a new method for training diffusion models with Expectation-Maximization (EM) from corrupted data. Our proposed method, DiffEM, utilizes conditional diffusion models to reconstruct clean data from observations in the E-step, and then uses the reconstructed data to refine the conditional diffusion model in the M-step. Theoretically, we provide monotonic convergence guarantees for the DiffEM iteration, assuming appropriate statistical conditions. We demonstrate the effectiveness of our approach through experiments on various image reconstruction tasks.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.12691v3",
    "published_date": "2025-10-14 16:25:02 UTC",
    "updated_date": "2025-12-20 04:01:26 UTC"
  },
  {
    "arxiv_id": "2510.12689v2",
    "title": "From Delegates to Trustees: How Optimizing for Long-Term Interests Shapes Bias and Alignment in LLM",
    "authors": [
      "Suyash Fulay",
      "Jocelyn Zhu",
      "Michiel Bakker"
    ],
    "abstract": "Large language models (LLMs) have shown promising accuracy in predicting survey responses and policy preferences, which has increased interest in their potential to represent human interests in various domains. Most existing research has focused on \"behavioral cloning\", effectively evaluating how well models reproduce individuals' expressed preferences. Drawing on theories of political representation, we highlight an underexplored design trade-off: whether AI systems should act as delegates, mirroring expressed preferences, or as trustees, exercising judgment about what best serves an individual's interests. This trade-off is closely related to issues of LLM sycophancy, where models can encourage behavior or validate beliefs that may be aligned with a user's short-term preferences, but is detrimental to their long-term interests.\n  Through a series of experiments simulating votes on various policy issues in the U.S. context, we apply a temporal utility framework that weighs short and long-term interests (simulating a trustee role) and compare voting outcomes to behavior-cloning models (simulating a delegate). We find that trustee-style predictions weighted toward long-term interests produce policy decisions that align more closely with expert consensus on well-understood issues, but also show greater bias toward models' default stances on topics lacking clear agreement. These findings reveal a fundamental trade-off in designing AI systems to represent human interests. Delegate models better preserve user autonomy but may diverge from well-supported policy positions, while trustee models can promote welfare on well-understood issues yet risk paternalism and bias on subjective topics.",
    "categories": [
      "cs.CY",
      "cs.AI"
    ],
    "primary_category": "cs.CY",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.12689v2",
    "published_date": "2025-10-14 16:24:19 UTC",
    "updated_date": "2025-11-16 16:36:44 UTC"
  },
  {
    "arxiv_id": "2510.12680v1",
    "title": "Demystifying Hybrid Thinking: Can LLMs Truly Switch Between Think and No-Think?",
    "authors": [
      "Shouren Wang",
      "Wang Yang",
      "Xianxuan Long",
      "Qifan Wang",
      "Vipin Chaudhary",
      "Xiaotian Han"
    ],
    "abstract": "Hybrid thinking enables LLMs to switch between reasoning and direct answering, offering a balance between efficiency and reasoning capability. Yet our experiments reveal that current hybrid thinking LLMs only achieve partial mode separation: reasoning behaviors often leak into the no-think mode. To understand and mitigate this, we analyze the factors influencing controllability and identify four that matter most: (1) larger data scale, (2) using think and no-think answers from different questions rather than the same question, (3) a moderate increase in no-think data number, and (4) a two-phase strategy that first trains reasoning ability and then applies hybrid think training. Building on these findings, we propose a practical recipe that, compared to standard training, can maintain accuracy in both modes while significantly reducing no-think output length (from $1085$ to $585$ on MATH500) and occurrences of reasoning-supportive tokens such as ``\\texttt{wait}'' (from $5917$ to $522$ on MATH500). Our findings highlight the limitations of current hybrid thinking and offer directions for strengthening its controllability.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.LG",
    "comment": "10 pages, 6 figures",
    "pdf_url": "https://arxiv.org/pdf/2510.12680v1",
    "published_date": "2025-10-14 16:19:44 UTC",
    "updated_date": "2025-10-14 16:19:44 UTC"
  },
  {
    "arxiv_id": "2510.12659v1",
    "title": "SG-XDEAT: Sparsity-Guided Cross-Dimensional and Cross-Encoding Attention with Target-Aware Conditioning in Tabular Learning",
    "authors": [
      "Chih-Chuan Cheng",
      "Yi-Ju Tseng"
    ],
    "abstract": "We propose SG-XDEAT (Sparsity-Guided Cross Dimensional and Cross-Encoding Attention with Target Aware Conditioning), a novel framework designed for supervised learning on tabular data. At its core, SG-XDEAT employs a dual-stream encoder that decomposes each input feature into two parallel representations: a raw value stream and a target-conditioned (label-aware) stream. These dual representations are then propagated through a hierarchical stack of attention-based modules. SG-XDEAT integrates three key components: (i) Cross-Dimensional self-attention, which captures intra-view dependencies among features within each stream; (ii) Cross-Encoding self-attention, which enables bidirectional interaction between raw and target-aware representations; and (iii) an Adaptive Sparse Self-Attention (ASSA) mechanism, which dynamically suppresses low-utility tokens by driving their attention weights toward zero--thereby mitigating the impact of noise. Empirical results on multiple public benchmarks show consistent gains over strong baselines, confirming that jointly modeling raw and target-aware views--while adaptively filtering noise--yields a more robust deep tabular learner.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.12659v1",
    "published_date": "2025-10-14 15:56:40 UTC",
    "updated_date": "2025-10-14 15:56:40 UTC"
  },
  {
    "arxiv_id": "2510.12643v1",
    "title": "Reasoning Pattern Matters: Learning to Reason without Human Rationales",
    "authors": [
      "Chaoxu Pang",
      "Yixuan Cao",
      "Ping Luo"
    ],
    "abstract": "Large Language Models (LLMs) have demonstrated remarkable reasoning capabilities under the widely adopted SFT+RLVR paradigm, which first performs Supervised Fine-Tuning (SFT) on human-annotated reasoning trajectories (rationales) to establish initial reasoning behaviors, then applies Reinforcement Learning with Verifiable Rewards (RLVR) to optimize the model using verifiable signals without golden rationales. However, annotating high-quality rationales for the SFT stage remains prohibitively expensive. This paper investigates when and how rationale annotation costs can be substantially reduced without compromising reasoning performance. We identify a broad class of problems, termed patterned reasoning tasks, where reasoning follows a fixed, procedural strategy consistent across instances. Although instances vary in content such as domain knowledge, factual information, or numeric values, the solution derives from applying a shared reasoning pattern. We argue that the success of SFT+RLVR on such tasks primarily stems from its ability to enable models to internalize these reasoning patterns. Using numerical semantic matching as a representative task, we provide both causal and behavioral evidence showing that reasoning patterns rather than the quantity or quality of rationales are the key determinant of performance. Building on these insights, we propose Pattern-Aware LLMs as Rationale AnnOtators (PARO), a simple yet effective framework that enables LLMs to generate rationales aligned with task-specific reasoning patterns without requiring human rationale annotations. Experiments show that PARO-generated rationales achieve comparable SFT+RLVR performance to human rationales that are 10 times larger. These results suggest that large-scale human rationale annotations can be replaced with LLM-based automatic annotations requiring only limited human supervision over reasoning patterns.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Submitted to Frontiers of Computer Science",
    "pdf_url": "https://arxiv.org/pdf/2510.12643v1",
    "published_date": "2025-10-14 15:34:38 UTC",
    "updated_date": "2025-10-14 15:34:38 UTC"
  },
  {
    "arxiv_id": "2510.12642v1",
    "title": "Aixel: A Unified, Adaptive and Extensible System for AI-powered Data Analysis",
    "authors": [
      "Meihui Zhang",
      "Liming Wang",
      "Chi Zhang",
      "Zhaojing Luo"
    ],
    "abstract": "A growing trend in modern data analysis is the integration of data management with learning, guided by accuracy, latency, and cost requirements. In practice, applications draw data of different formats from many sources. In the meanwhile, the objectives and budgets change over time. Existing systems handle these applications across databases, analysis libraries, and tuning services. Such fragmentation leads to complex user interaction, limited adaptability, suboptimal performance, and poor extensibility across components. To address these challenges, we present Aixel, a unified, adaptive, and extensible system for AI-powered data analysis. The system organizes work across four layers: application, task, model, and data. The task layer provides a declarative interface to capture user intent, which is parsed into an executable operator plan. An optimizer compiles and schedules this plan to meet specified goals in accuracy, latency, and cost. The task layer coordinates the execution of data and model operators, with built-in support for reuse and caching to improve efficiency. The model layer offers versioned storage for index, metadata, tensors, and model artifacts. It supports adaptive construction, task-aligned drift detection, and safe updates that reuse shared components. The data layer provides unified data management capabilities, including indexing, constraint-aware discovery, task-aligned selection, and comprehensive feature management. With the above designed layers, Aixel delivers a user friendly, adaptive, efficient, and extensible system.",
    "categories": [
      "cs.DB",
      "cs.AI"
    ],
    "primary_category": "cs.DB",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.12642v1",
    "published_date": "2025-10-14 15:34:35 UTC",
    "updated_date": "2025-10-14 15:34:35 UTC"
  },
  {
    "arxiv_id": "2510.16007v1",
    "title": "Layer-Aware Influence for Online Data Valuation Estimation",
    "authors": [
      "Ziao Yang",
      "Longbo Huang",
      "Hongfu Liu"
    ],
    "abstract": "Data-centric learning emphasizes curating high-quality training samples to boost performance rather than designing new architectures. A central problem is to estimate the influence of training sample efficiently. Prior studies largely focus on static influence measured on a converged model, overlooking how data valuation dynamically changes during optimization. This omission neglects the dynamic nature of sample influence during optimization, especially in deep models. To address the computational burden of frequent influence estimation, we develop a layer-aware online estimator that requires only loss-to-output gradients. This design avoids parameter-level and full-network gradients while preserving ranking fidelity. Extensive experiments across LLM pretraining, fine-tuning, and image classification show our method improves accuracy with substantially lower time and memory cost, making dynamic data curation efficient and scalable in practice.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.16007v1",
    "published_date": "2025-10-14 15:34:22 UTC",
    "updated_date": "2025-10-14 15:34:22 UTC"
  },
  {
    "arxiv_id": "2510.12635v2",
    "title": "Memory as Action: Autonomous Context Curation for Long-Horizon Agentic Tasks",
    "authors": [
      "Yuxiang Zhang",
      "Jiangming Shu",
      "Ye Ma",
      "Xueyuan Lin",
      "Shangxi Wu",
      "Jitao Sang"
    ],
    "abstract": "Long-context Large Language Models, despite their expanded capacity, require careful working memory management to mitigate attention dilution during long-horizon tasks. Yet existing approaches rely on external mechanisms that lack awareness of the agent's reasoning state, leading to suboptimal decisions. We propose Memory-as-Action (MemAct), a framework that treats working memory management as learnable policy actions. By formulating context management as in-place editing operations (deletion, insertion), MemAct enables joint optimization of information retention and task performance through end-to-end reinforcement learning. To address the computational challenges of dynamic context updates, we introduce Dynamic Context Policy Optimization, which restores training efficiency without compromising reasoning integrity. Experiments show that MemAct-RL-14B matches the accuracy of models $16\\times$ larger while reducing average context length by 51\\%, with learned strategies that adapt to model capabilities and generalize across task complexities.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.12635v2",
    "published_date": "2025-10-14 15:29:57 UTC",
    "updated_date": "2026-01-10 01:44:56 UTC"
  },
  {
    "arxiv_id": "2510.12633v1",
    "title": "Laminar: A Scalable Asynchronous RL Post-Training Framework",
    "authors": [
      "Guangming Sheng",
      "Yuxuan Tong",
      "Borui Wan",
      "Wang Zhang",
      "Chaobo Jia",
      "Xibin Wu",
      "Yuqi Wu",
      "Xiang Li",
      "Chi Zhang",
      "Yanghua Peng",
      "Haibin Lin",
      "Xin Liu",
      "Chuan Wu"
    ],
    "abstract": "Reinforcement learning (RL) post-training for Large Language Models (LLMs) is now scaling to large clusters and running for extended durations to enhance model reasoning performance. However, the scalability of existing RL frameworks is limited, as extreme long-tail skewness in RL trajectory generation causes severe GPU underutilization. Current asynchronous RL systems attempt to mitigate this, but they rely on global weight synchronization between the actor and all rollouts, which creates a rigid model update schedule. This global synchronization is ill-suited for the highly skewed and evolving distribution of trajectory generation latency in RL training, crippling training efficiency. Our key insight is that efficient scaling requires breaking this lockstep through trajectory-level asynchrony, which generates and consumes each trajectory independently. We propose Laminar, a scalable and robust RL post-training system built on a fully decoupled architecture. First, we replace global updates with a tier of relay workers acting as a distributed parameter service. This enables asynchronous and fine-grained weight synchronization, allowing rollouts to pull the latest weight anytime without stalling the actor's training loop. Second, a dynamic repack mechanism consolidates long-tail trajectories onto a few dedicated rollouts, maximizing generation throughput. The fully decoupled design also isolates failures, ensuring robustness for long-running jobs. Our evaluation on a 1024-GPU cluster shows that Laminar achieves up to 5.48$\\times$ training throughput speedup over state-of-the-art systems, while reducing model convergence time.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.DC"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.12633v1",
    "published_date": "2025-10-14 15:29:14 UTC",
    "updated_date": "2025-10-14 15:29:14 UTC"
  },
  {
    "arxiv_id": "2510.12630v1",
    "title": "Designing Tools with Control Confidence",
    "authors": [
      "Ajith Anil Meera",
      "Abian Torres",
      "Pablo Lanillos"
    ],
    "abstract": "Prehistoric humans invented stone tools for specialized tasks by not just maximizing the tool's immediate goal-completion accuracy, but also increasing their confidence in the tool for later use under similar settings. This factor contributed to the increased robustness of the tool, i.e., the least performance deviations under environmental uncertainties. However, the current autonomous tool design frameworks solely rely on performance optimization, without considering the agent's confidence in tool use for repeated use. Here, we take a step towards filling this gap by i) defining an optimization framework for task-conditioned autonomous hand tool design for robots, where ii) we introduce a neuro-inspired control confidence term into the optimization routine that helps the agent to design tools with higher robustness. Through rigorous simulations using a robotic arm, we show that tools designed with control confidence as the objective function are more robust to environmental uncertainties during tool use than a pure accuracy-driven objective. We further show that adding control confidence to the objective function for tool design provides a balance between the robustness and goal accuracy of the designed tools under control perturbations. Finally, we show that our CMAES-based evolutionary optimization strategy for autonomous tool design outperforms other state-of-the-art optimizers by designing the optimal tool within the fewest iterations. Code: https://github.com/ajitham123/Tool_design_control_confidence.",
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "primary_category": "cs.RO",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.12630v1",
    "published_date": "2025-10-14 15:27:27 UTC",
    "updated_date": "2025-10-14 15:27:27 UTC"
  },
  {
    "arxiv_id": "2510.12624v1",
    "title": "Learning-To-Measure: In-context Active Feature Acquisition",
    "authors": [
      "Yuta Kobayashi",
      "Zilin Jing",
      "Jiayu Yao",
      "Hongseok Namkoong",
      "Shalmali Joshi"
    ],
    "abstract": "Active feature acquisition (AFA) is a sequential decision-making problem where the goal is to improve model performance for test instances by adaptively selecting which features to acquire. In practice, AFA methods often learn from retrospective data with systematic missingness in the features and limited task-specific labels. Most prior work addresses acquisition for a single predetermined task, limiting scalability. To address this limitation, we formalize the meta-AFA problem, where the goal is to learn acquisition policies across various tasks. We introduce Learning-to-Measure (L2M), which consists of i) reliable uncertainty quantification over unseen tasks, and ii) an uncertainty-guided greedy feature acquisition agent that maximizes conditional mutual information. We demonstrate a sequence-modeling or autoregressive pre-training approach that underpins reliable uncertainty quantification for tasks with arbitrary missingness. L2M operates directly on datasets with retrospective missingness and performs the meta-AFA task in-context, eliminating per-task retraining. Across synthetic and real-world tabular benchmarks, L2M matches or surpasses task-specific baselines, particularly under scarce labels and high missingness.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.12624v1",
    "published_date": "2025-10-14 15:23:32 UTC",
    "updated_date": "2025-10-14 15:23:32 UTC"
  },
  {
    "arxiv_id": "2510.12615v1",
    "title": "Rethinking Knowledge Distillation: A Data Dependent Regulariser With a Negative Asymmetric Payoff",
    "authors": [
      "Israel Mason-Williams",
      "Gabryel Mason-Williams",
      "Helen Yannakoudakis"
    ],
    "abstract": "Knowledge distillation is often considered a compression mechanism when judged on the resulting student's accuracy and loss, yet its functional impact is poorly understood. In this work, we quantify the compression capacity of knowledge distillation and the resulting knowledge transfer from a functional perspective, decoupling compression from architectural reduction, which provides an improved understanding of knowledge distillation. We employ hypothesis testing, controls, and random control distillation to understand knowledge transfer mechanisms across data modalities. To rigorously test the breadth and limits of our analyses, we explore multiple distillation variants and analyse distillation scaling laws across model sizes. Our findings demonstrate that, while there is statistically significant knowledge transfer in some modalities and architectures, the extent of this transfer is less pronounced than anticipated, even under conditions designed to maximise knowledge sharing. Notably, in cases of significant knowledge transfer, we identify a consistent and severe asymmetric transfer of negative knowledge to the student, raising safety concerns in knowledge distillation applications. Across 12 experimental setups, 9 architectures, and 7 datasets, our findings show that knowledge distillation functions less as a compression mechanism and more as a data-dependent regulariser with a negative asymmetric payoff.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "45 pages, 24 figures and 104 tables",
    "pdf_url": "https://arxiv.org/pdf/2510.12615v1",
    "published_date": "2025-10-14 15:14:55 UTC",
    "updated_date": "2025-10-14 15:14:55 UTC"
  },
  {
    "arxiv_id": "2510.12608v1",
    "title": "StyleDecipher: Robust and Explainable Detection of LLM-Generated Texts with Stylistic Analysis",
    "authors": [
      "Siyuan Li",
      "Aodu Wulianghai",
      "Xi Lin",
      "Guangyan Li",
      "Xiang Chen",
      "Jun Wu",
      "Jianhua Li"
    ],
    "abstract": "With the increasing integration of large language models (LLMs) into open-domain writing, detecting machine-generated text has become a critical task for ensuring content authenticity and trust. Existing approaches rely on statistical discrepancies or model-specific heuristics to distinguish between LLM-generated and human-written text. However, these methods struggle in real-world scenarios due to limited generalization, vulnerability to paraphrasing, and lack of explainability, particularly when facing stylistic diversity or hybrid human-AI authorship. In this work, we propose StyleDecipher, a robust and explainable detection framework that revisits LLM-generated text detection using combined feature extractors to quantify stylistic differences. By jointly modeling discrete stylistic indicators and continuous stylistic representations derived from semantic embeddings, StyleDecipher captures distinctive style-level divergences between human and LLM outputs within a unified representation space. This framework enables accurate, explainable, and domain-agnostic detection without requiring access to model internals or labeled segments. Extensive experiments across five diverse domains, including news, code, essays, reviews, and academic abstracts, demonstrate that StyleDecipher consistently achieves state-of-the-art in-domain accuracy. Moreover, in cross-domain evaluations, it surpasses existing baselines by up to 36.30%, while maintaining robustness against adversarial perturbations and mixed human-AI content. Further qualitative and quantitative analysis confirms that stylistic signals provide explainable evidence for distinguishing machine-generated text. Our source code can be accessed at https://github.com/SiyuanLi00/StyleDecipher.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.12608v1",
    "published_date": "2025-10-14 15:07:27 UTC",
    "updated_date": "2025-10-14 15:07:27 UTC"
  },
  {
    "arxiv_id": "2510.16005v1",
    "title": "Breaking Guardrails, Facing Walls: Insights on Adversarial AI for Defenders & Researchers",
    "authors": [
      "Giacomo Bertollo",
      "Naz Bodemir",
      "Jonah Burgess"
    ],
    "abstract": "Analyzing 500 CTF participants, this paper shows that while participants readily bypassed simple AI guardrails using common techniques, layered multi-step defenses still posed significant challenges, offering concrete insights for building safer AI systems.",
    "categories": [
      "cs.CR",
      "cs.AI"
    ],
    "primary_category": "cs.CR",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.16005v1",
    "published_date": "2025-10-14 15:01:59 UTC",
    "updated_date": "2025-10-14 15:01:59 UTC"
  },
  {
    "arxiv_id": "2510.12604v4",
    "title": "COINS: SemantiC Ids Enhanced COLd Item RepresentatioN for Click-through Rate Prediction in E-commerce Search",
    "authors": [
      "Qihang Zhao",
      "Zhongbo Sun",
      "Xiaoyang Zheng",
      "Xian Guo",
      "Siyuan Wang",
      "Zihan Liang",
      "Mingcan Peng",
      "Ben Chen",
      "Chenyi Lei"
    ],
    "abstract": "With the rise of modern search and recommendation platforms, insufficient collaborative information of cold-start items exacerbates the Matthew effect of existing platform items, challenging platform diversity and becoming a longstanding issue. Existing methods align items' side content with collaborative information to transfer collaborative signals from high-popularity items to cold-start items. However, these methods fail to account for the asymmetry between collaboration and content, nor the fine-grained differences among items. To address these issues, we propose COINS, an item representation enhancement approach based on fused alignment of semantic IDs. Specifically, we use RQ-OPQ encoding to quantize item content and collaborative information, followed by a two-step alignment: RQ encoding transfers shared collaborative signals across items, while OPQ encoding learns differentiated information of items. Comprehensive offline experiments on large-scale industrial datasets demonstrate superiority of COINS, and rigorous online A/B tests confirm statistically significant improvements: item CTR +1.66%, buyers +1.57%, and order volume +2.17%.",
    "categories": [
      "cs.IR",
      "cs.AI"
    ],
    "primary_category": "cs.IR",
    "comment": "Accepted by WWW26",
    "pdf_url": "https://arxiv.org/pdf/2510.12604v4",
    "published_date": "2025-10-14 14:58:50 UTC",
    "updated_date": "2026-01-15 02:58:58 UTC"
  },
  {
    "arxiv_id": "2510.12603v1",
    "title": "Reasoning in the Dark: Interleaved Vision-Text Reasoning in Latent Space",
    "authors": [
      "Chao Chen",
      "Zhixin Ma",
      "Yongqi Li",
      "Yupeng Hu",
      "Yinwei Wei",
      "Wenjie Li",
      "Liqiang Nie"
    ],
    "abstract": "Multimodal reasoning aims to enhance the capabilities of MLLMs by incorporating intermediate reasoning steps before reaching the final answer. It has evolved from text-only reasoning to the integration of visual information, enabling the thought process to be conveyed through both images and text. Despite its effectiveness, current multimodal reasoning methods depend on explicit reasoning steps that require labor-intensive vision-text annotations and inherently introduce significant inference latency. To address these issues, we introduce multimodal latent reasoning with the advantages of multimodal representation, reduced annotation, and inference efficiency. To facilicate it, we propose Interleaved Vision-Text Latent Reasoning (IVT-LR), which injects both visual and textual information in the reasoning process within the latent space. Specifically, IVT-LR represents each reasoning step by combining two implicit parts: latent text (the hidden states from the previous step) and latent vision (a set of selected image embeddings). We further introduce a progressive multi-stage training strategy to enable MLLMs to perform the above multimodal latent reasoning steps. Experiments on M3CoT and ScienceQA demonstrate that our IVT-LR method achieves an average performance increase of 5.45% in accuracy, while simultaneously achieving a speed increase of over 5 times compared to existing approaches. Code available at https://github.com/FYYDCC/IVT-LR.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.12603v1",
    "published_date": "2025-10-14 14:58:25 UTC",
    "updated_date": "2025-10-14 14:58:25 UTC"
  },
  {
    "arxiv_id": "2510.12859v2",
    "title": "Three Lenses on the AI Revolution: Risk, Transformation, Continuity",
    "authors": [
      "Masoud Makrehchi"
    ],
    "abstract": "Artificial Intelligence (AI) has emerged as both a continuation of historical technological revolutions and a potential rupture with them. This paper argues that AI must be viewed simultaneously through three lenses: \\textit{risk}, where it resembles nuclear technology in its irreversible and global externalities; \\textit{transformation}, where it parallels the Industrial Revolution as a general-purpose technology driving productivity and reorganization of labor; and \\textit{continuity}, where it extends the fifty-year arc of computing revolutions from personal computing to the internet to mobile. Drawing on historical analogies, we emphasize that no past transition constituted a strict singularity: disruptive shifts eventually became governable through new norms and institutions.\n  We examine recurring patterns across revolutions -- democratization at the usage layer, concentration at the production layer, falling costs, and deepening personalization -- and show how these dynamics are intensifying in the AI era. Sectoral analysis illustrates how accounting, law, education, translation, advertising, and software engineering are being reshaped as routine cognition is commoditized and human value shifts to judgment, trust, and ethical responsibility. At the frontier, the challenge of designing moral AI agents highlights the need for robust guardrails, mechanisms for moral generalization, and governance of emergent multi-agent dynamics.\n  We conclude that AI is neither a singular break nor merely incremental progress. It is both evolutionary and revolutionary: predictable in its median effects yet carrying singularity-class tail risks. Good outcomes are not automatic; they require coupling pro-innovation strategies with safety governance, ensuring equitable access, and embedding AI within a human order of responsibility.",
    "categories": [
      "cs.CY",
      "cs.AI"
    ],
    "primary_category": "cs.CY",
    "comment": "18 pages",
    "pdf_url": "https://arxiv.org/pdf/2510.12859v2",
    "published_date": "2025-10-14 14:53:49 UTC",
    "updated_date": "2025-12-13 03:44:59 UTC"
  },
  {
    "arxiv_id": "2510.13894v2",
    "title": "Bayes or Heisenberg: Who(se) Rules?",
    "authors": [
      "Volker Tresp",
      "Hang Li",
      "Federico Harjes",
      "Yunpu Ma"
    ],
    "abstract": "Although quantum systems are generally described by quantum state vectors, we show that in certain cases their measurement processes can be reformulated as probabilistic equations expressed in terms of probabilistic state vectors. These probabilistic representations can, in turn, be approximated by the neural network dynamics of the Tensor Brain (TB) model.\n  The Tensor Brain is a recently proposed framework for modeling perception and memory in the brain, providing a biologically inspired mechanism for efficiently integrating generated symbolic representations into reasoning processes.",
    "categories": [
      "q-bio.NC",
      "cs.AI",
      "cs.LG",
      "quant-ph"
    ],
    "primary_category": "q-bio.NC",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.13894v2",
    "published_date": "2025-10-14 14:27:18 UTC",
    "updated_date": "2025-10-23 11:22:19 UTC"
  },
  {
    "arxiv_id": "2510.12563v2",
    "title": "HardcoreLogic: Challenging Large Reasoning Models with Long-tail Logic Puzzle Games",
    "authors": [
      "Jingcong Liang",
      "Shijun Wan",
      "Xuehai Wu",
      "Yitong Li",
      "Qianglong Chen",
      "Duyu Tang",
      "Siyuan Wang",
      "Zhongyu Wei"
    ],
    "abstract": "Large Reasoning Models (LRMs) have demonstrated impressive performance on complex tasks, including logical puzzle games that require deriving solutions satisfying all constraints. However, whether they can flexibly apply appropriate rules to varying conditions, particularly when faced with non-canonical game variants, remains an open question. Existing corpora focus on popular puzzles like 9x9 Sudoku, risking overfitting to canonical formats and memorization of solution patterns, which can mask deficiencies in understanding novel rules or adapting strategies to new variants. To address this, we introduce HardcoreLogic, a challenging benchmark of over 5,000 puzzles across 10 games, designed to test the robustness of LRMs on the \"long-tail\" of logical games. HardcoreLogic systematically transforms canonical puzzles through three dimensions: Increased Complexity (IC), Uncommon Elements (UE), and Unsolvable Puzzles (UP), reducing reliance on shortcut memorization. Evaluations on a diverse set of LRMs reveal significant performance drops, even for models achieving top scores on existing benchmarks, indicating heavy reliance on memorized stereotypes. While increased complexity is the dominant source of difficulty, models also struggle with subtle rule variations that do not necessarily increase puzzle difficulty. Our systematic error analysis on solvable and unsolvable puzzles further highlights gaps in genuine reasoning. Overall, HardcoreLogic exposes the limitations of current LRMs and establishes a benchmark for advancing high-level logical reasoning.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.12563v2",
    "published_date": "2025-10-14 14:23:24 UTC",
    "updated_date": "2025-10-15 10:31:28 UTC"
  },
  {
    "arxiv_id": "2510.16004v1",
    "title": "PAINT: Parallel-in-time Neural Twins for Dynamical System Reconstruction",
    "authors": [
      "Andreas Radler",
      "Vincent Seyfried",
      "Stefan Pirker",
      "Johannes Brandstetter",
      "Thomas Lichtenegger"
    ],
    "abstract": "Neural surrogates have shown great potential in simulating dynamical systems, while offering real-time capabilities. We envision Neural Twins as a progression of neural surrogates, aiming to create digital replicas of real systems. A neural twin consumes measurements at test time to update its state, thereby enabling context-specific decision-making. A critical property of neural twins is their ability to remain on-trajectory, i.e., to stay close to the true system state over time. We introduce Parallel-in-time Neural Twins (PAINT), an architecture-agnostic family of methods for modeling dynamical systems from measurements. PAINT trains a generative neural network to model the distribution of states parallel over time. At test time, states are predicted from measurements in a sliding window fashion. Our theoretical analysis shows that PAINT is on-trajectory, whereas autoregressive models generally are not. Empirically, we evaluate our method on a challenging two-dimensional turbulent fluid dynamics problem. The results demonstrate that PAINT stays on-trajectory and predicts system states from sparse measurements with high fidelity. These findings underscore PAINT's potential for developing neural twins that stay on-trajectory, enabling more accurate state estimation and decision-making.",
    "categories": [
      "cs.AI",
      "physics.flu-dyn"
    ],
    "primary_category": "cs.AI",
    "comment": "22 pages, 16 figures",
    "pdf_url": "https://arxiv.org/pdf/2510.16004v1",
    "published_date": "2025-10-14 14:22:45 UTC",
    "updated_date": "2025-10-14 14:22:45 UTC"
  },
  {
    "arxiv_id": "2510.12555v1",
    "title": "Inclusive Fitness as a Key Step Towards More Advanced Social Behaviors in Multi-Agent Reinforcement Learning Settings",
    "authors": [
      "Andries Rosseau",
      "Raphaël Avalos",
      "Ann Nowé"
    ],
    "abstract": "The competitive and cooperative forces of natural selection have driven the evolution of intelligence for millions of years, culminating in nature's vast biodiversity and the complexity of human minds. Inspired by this process, we propose a novel multi-agent reinforcement learning framework where each agent is assigned a genotype and where reward functions are modelled after the concept of inclusive fitness. An agent's genetic material may be shared with other agents, and our inclusive reward function naturally accounts for this. We study the resulting social dynamics in two types of network games with prisoner's dilemmas and find that our results align with well-established principles from biology, such as Hamilton's rule. Furthermore, we outline how this framework can extend to more open-ended environments with spatial and temporal structure, finite resources, and evolving populations. We hypothesize the emergence of an arms race of strategies, where each new strategy is a gradual improvement over earlier adaptations of other agents, effectively producing a multi-agent autocurriculum analogous to biological evolution. In contrast to the binary team-based structures prevalent in earlier research, our gene-based reward structure introduces a spectrum of cooperation ranging from full adversity to full cooperativeness based on genetic similarity, enabling unique non team-based social dynamics. For example, one agent having a mutual cooperative relationship with two other agents, while the two other agents behave adversarially towards each other. We argue that incorporating inclusive fitness in agents provides a foundation for the emergence of more strategically advanced and socially intelligent agents.",
    "categories": [
      "cs.AI",
      "cs.MA",
      "cs.SI"
    ],
    "primary_category": "cs.AI",
    "comment": "This version is a slightly updated version (e.g., added an important reference) compared to the peer-reviewed versions at 'Adapative Learning Agents' at AAMAS 2022 or 'From Cells to Societies' at ICLR 2022",
    "pdf_url": "https://arxiv.org/pdf/2510.12555v1",
    "published_date": "2025-10-14 14:20:01 UTC",
    "updated_date": "2025-10-14 14:20:01 UTC"
  },
  {
    "arxiv_id": "2510.12541v1",
    "title": "Evaluation of Real-Time Preprocessing Methods in AI-Based ECG Signal Analysis",
    "authors": [
      "Jasmin Freudenberg",
      "Kai Hahn",
      "Christian Weber",
      "Madjid Fathi"
    ],
    "abstract": "The increasing popularity of portable ECG systems and the growing demand for privacy-compliant, energy-efficient real-time analysis require new approaches to signal processing at the point of data acquisition. In this context, the edge domain is acquiring increasing importance, as it not only reduces latency times, but also enables an increased level of data security. The FACE project aims to develop an innovative machine learning solution for analysing long-term electrocardiograms that synergistically combines the strengths of edge and cloud computing. In this thesis, various pre-processing steps of ECG signals are analysed with regard to their applicability in the project. The selection of suitable methods in the edge area is based in particular on criteria such as energy efficiency, processing capability and real-time capability.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "Conference paper for 2025 IEEE World AI IoT Congress (AIIoT), FACE Project, University of Siegen, Germany",
    "pdf_url": "https://arxiv.org/pdf/2510.12541v1",
    "published_date": "2025-10-14 14:04:13 UTC",
    "updated_date": "2025-10-14 14:04:13 UTC"
  },
  {
    "arxiv_id": "2510.12537v1",
    "title": "Unconditional Human Motion and Shape Generation via Balanced Score-Based Diffusion",
    "authors": [
      "David Björkstrand",
      "Tiesheng Wang",
      "Lars Bretzner",
      "Josephine Sullivan"
    ],
    "abstract": "Recent work has explored a range of model families for human motion generation, including Variational Autoencoders (VAEs), Generative Adversarial Networks (GANs), and diffusion-based models. Despite their differences, many methods rely on over-parameterized input features and auxiliary losses to improve empirical results. These strategies should not be strictly necessary for diffusion models to match the human motion distribution. We show that on par with state-of-the-art results in unconditional human motion generation are achievable with a score-based diffusion model using only careful feature-space normalization and analytically derived weightings for the standard L2 score-matching loss, while generating both motion and shape directly, thereby avoiding slow post hoc shape recovery from joints. We build the method step by step, with a clear theoretical motivation for each component, and provide targeted ablations demonstrating the effectiveness of each proposed addition in isolation.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.12537v1",
    "published_date": "2025-10-14 14:02:22 UTC",
    "updated_date": "2025-10-14 14:02:22 UTC"
  },
  {
    "arxiv_id": "2510.12534v3",
    "title": "ProtoSiTex: Learning Semi-Interpretable Prototypes for Multi-label Text Classification",
    "authors": [
      "Utsav Kumar Nareti",
      "Suraj Kumar",
      "Soumya Pandey",
      "Soumi Chattopadhyay",
      "Chandranath Adak"
    ],
    "abstract": "The rapid growth of user-generated text across digital platforms has intensified the need for interpretable models capable of fine-grained text classification and explanation. Existing prototype-based models offer intuitive explanations but typically operate at coarse granularity (sentence or document level) and fail to address the multi-label nature of real-world text classification. We propose ProtoSiTex, a semi-interpretable framework designed for fine-grained multi-label text classification. ProtoSiTex employs a dual-phase alternate training strategy: an unsupervised prototype discovery phase that learns semantically coherent and diverse prototypes, and a supervised classification phase that maps these prototypes to class labels. A hierarchical loss function enforces consistency across subsentence, sentence, and document levels, enhancing interpretability and alignment. Unlike prior approaches, ProtoSiTex captures overlapping and conflicting semantics using adaptive prototypes and multi-head attention. We also introduce a benchmark dataset of hotel reviews annotated at the subsentence level with multiple labels. Experiments on this dataset and two public benchmarks (binary and multi-class) show that ProtoSiTex achieves state-of-the-art performance while delivering faithful, human-aligned explanations, establishing it as a robust solution for semi-interpretable multi-label text classification.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.12534v3",
    "published_date": "2025-10-14 13:59:28 UTC",
    "updated_date": "2025-12-18 11:14:07 UTC"
  },
  {
    "arxiv_id": "2510.12516v1",
    "title": "BoN Appetit Team at LeWiDi-2025: Best-of-N Test-time Scaling Can Not Stomach Annotation Disagreements (Yet)",
    "authors": [
      "Tomas Ruiz",
      "Siyao Peng",
      "Barbara Plank",
      "Carsten Schwemmer"
    ],
    "abstract": "Test-time scaling is a family of techniques to improve LLM outputs at inference time by performing extra computation. To the best of our knowledge, test-time scaling has been limited to domains with verifiably correct answers, like mathematics and coding. We transfer test-time scaling to the LeWiDi-2025 tasks to evaluate annotation disagreements. We experiment with three test-time scaling methods: two benchmark algorithms (Model Averaging and Majority Voting), and a Best-of-N sampling method. The two benchmark methods improve LLM performance consistently on the LeWiDi tasks, but the Best-of-N method does not. Our experiments suggest that the Best-of-N method does not currently transfer from mathematics to LeWiDi tasks, and we analyze potential reasons for this gap.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.12516v1",
    "published_date": "2025-10-14 13:43:08 UTC",
    "updated_date": "2025-10-14 13:43:08 UTC"
  },
  {
    "arxiv_id": "2510.12858v2",
    "title": "A Critical Review of the Need for Knowledge-Centric Evaluation of Quranic Recitation",
    "authors": [
      "Mohammed Hilal Al-Kharusi",
      "Khizar Hayat",
      "Khalil Bader Al Ruqeishi",
      "Haroon Rashid Lone"
    ],
    "abstract": "The art and science of Quranic recitation (Tajweed), a discipline governed by meticulous phonetic, rhythmic, and theological principles, confronts substantial educational challenges in today's digital age. Although modern technology offers unparalleled opportunities for learning, existing automated systems for evaluating recitation have struggled to gain broad acceptance or demonstrate educational effectiveness. This literature review examines this crucial disparity, offering a thorough analysis of scholarly research, digital platforms, and commercial tools developed over the past twenty years. Our analysis uncovers a fundamental flaw in current approaches that adapt Automatic Speech Recognition (ASR) systems, which emphasize word identification over qualitative acoustic evaluation. These systems suffer from limitations such as reliance on biased datasets, demographic disparities, and an inability to deliver meaningful feedback for improvement. Challenging these data-centric methodologies, we advocate for a paradigm shift toward a knowledge-based computational framework. By leveraging the unchanging nature of the Quranic text and the well-defined rules of Tajweed, we propose that an effective evaluation system should be built upon rule-based acoustic modeling centered on canonical pronunciation principles and articulation points (Makhraj), rather than depending on statistical patterns derived from flawed or biased data. The review concludes that the future of automated Quranic recitation assessment lies in hybrid systems that combine linguistic expertise with advanced audio processing. Such an approach paves the way for developing reliable, fair, and pedagogically effective tools that can authentically assist learners across the globe.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.SD"
    ],
    "primary_category": "cs.CL",
    "comment": "32 pages",
    "pdf_url": "https://arxiv.org/pdf/2510.12858v2",
    "published_date": "2025-10-14 13:39:49 UTC",
    "updated_date": "2025-11-13 04:52:37 UTC"
  },
  {
    "arxiv_id": "2510.12503v1",
    "title": "The Robustness of Differentiable Causal Discovery in Misspecified Scenarios",
    "authors": [
      "Huiyang Yi",
      "Yanyan He",
      "Duxin Chen",
      "Mingyu Kang",
      "He Wang",
      "Wenwu Yu"
    ],
    "abstract": "Causal discovery aims to learn causal relationships between variables from targeted data, making it a fundamental task in machine learning. However, causal discovery algorithms often rely on unverifiable causal assumptions, which are usually difficult to satisfy in real-world data, thereby limiting the broad application of causal discovery in practical scenarios. Inspired by these considerations, this work extensively benchmarks the empirical performance of various mainstream causal discovery algorithms, which assume i.i.d. data, under eight model assumption violations. Our experimental results show that differentiable causal discovery methods exhibit robustness under the metrics of Structural Hamming Distance and Structural Intervention Distance of the inferred graphs in commonly used challenging scenarios, except for scale variation. We also provide the theoretical explanations for the performance of differentiable causal discovery methods. Finally, our work aims to comprehensively benchmark the performance of recent differentiable causal discovery methods under model assumption violations, and provide the standard for reasonable evaluation of causal discovery, as well as to further promote its application in real-world scenarios.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.ME",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "accepted to ICLR 2025",
    "pdf_url": "https://arxiv.org/pdf/2510.12503v1",
    "published_date": "2025-10-14 13:33:06 UTC",
    "updated_date": "2025-10-14 13:33:06 UTC"
  },
  {
    "arxiv_id": "2510.12498v2",
    "title": "Artificial Intelligence Virtual Cells: From Measurements to Decisions across Modality, Scale, Dynamics, and Evaluation",
    "authors": [
      "Chengpeng Hu",
      "Calvin Yu-Chian Chen"
    ],
    "abstract": "Artificial Intelligence Virtual Cells (AIVCs) aim to learn executable, decision-relevant models of cell state from multimodal, multiscale measurements. Recent studies have introduced single-cell and spatial foundation models, improved cross-modality alignment, scaled perturbation atlases, and explored pathway-level readouts. Nevertheless, although held-out validation is standard practice, evaluations remain predominantly within single datasets and settings; evidence indicates that transport across laboratories and platforms is often limited, that some data splits are vulnerable to leakage and coverage bias, and that dose, time and combination effects are not yet systematically handled. Cross-scale coupling also remains constrained, as anchors linking molecular, cellular and tissue levels are sparse, and alignment to scientific or clinical readouts varies across studies. We propose a model-agnostic Cell-State Latent (CSL) perspective that organizes learning via an operator grammar: measurement, lift/project for cross-scale coupling, and intervention for dosing and scheduling. This view motivates a decision-aligned evaluation blueprint across modality, scale, context and intervention, and emphasizes function-space readouts such as pathway activity, spatial neighborhoods and clinically relevant endpoints. We recommend operator-aware data design, leakage-resistant partitions, and transparent calibration and reporting to enable reproducible, like-for-like comparisons.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.12498v2",
    "published_date": "2025-10-14 13:31:40 UTC",
    "updated_date": "2025-12-02 08:44:52 UTC"
  },
  {
    "arxiv_id": "2510.12494v1",
    "title": "PubSub-VFL: Towards Efficient Two-Party Split Learning in Heterogeneous Environments via Publisher/Subscriber Architecture",
    "authors": [
      "Yi Liu",
      "Yang Liu",
      "Leqian Zheng",
      "Jue Hong",
      "Junjie Shi",
      "Qingyou Yang",
      "Ye Wu",
      "Cong Wang"
    ],
    "abstract": "With the rapid advancement of the digital economy, data collaboration between organizations has become a well-established business model, driving the growth of various industries. However, privacy concerns make direct data sharing impractical. To address this, Two-Party Split Learning (a.k.a. Vertical Federated Learning (VFL)) has emerged as a promising solution for secure collaborative learning. Despite its advantages, this architecture still suffers from low computational resource utilization and training efficiency. Specifically, its synchronous dependency design increases training latency, while resource and data heterogeneity among participants further hinder efficient computation. To overcome these challenges, we propose PubSub-VFL, a novel VFL paradigm with a Publisher/Subscriber architecture optimized for two-party collaborative learning with high computational efficiency. PubSub-VFL leverages the decoupling capabilities of the Pub/Sub architecture and the data parallelism of the parameter server architecture to design a hierarchical asynchronous mechanism, reducing training latency and improving system efficiency. Additionally, to mitigate the training imbalance caused by resource and data heterogeneity, we formalize an optimization problem based on participants' system profiles, enabling the selection of optimal hyperparameters while preserving privacy. We conduct a theoretical analysis to demonstrate that PubSub-VFL achieves stable convergence and is compatible with security protocols such as differential privacy. Extensive case studies on five benchmark datasets further validate its effectiveness, showing that, compared to state-of-the-art baselines, PubSub-VFL not only accelerates training by $2 \\sim 7\\times$ without compromising accuracy, but also achieves a computational resource utilization rate of up to 91.07%.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.DC"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted at NeurIPS 2025",
    "pdf_url": "https://arxiv.org/pdf/2510.12494v1",
    "published_date": "2025-10-14 13:27:33 UTC",
    "updated_date": "2025-10-14 13:27:33 UTC"
  },
  {
    "arxiv_id": "2510.12490v1",
    "title": "Using Medical Algorithms for Task-Oriented Dialogue in LLM-Based Medical Interviews",
    "authors": [
      "Rui Reis",
      "Pedro Rangel Henriques",
      "João Ferreira-Coimbra",
      "Eva Oliveira",
      "Nuno F. Rodrigues"
    ],
    "abstract": "We developed a task-oriented dialogue framework structured as a Directed Acyclic Graph (DAG) of medical questions. The system integrates: (1) a systematic pipeline for transforming medical algorithms and guidelines into a clinical question corpus; (2) a cold-start mechanism based on hierarchical clustering to generate efficient initial questioning without prior patient information; (3) an expand-and-prune mechanism enabling adaptive branching and backtracking based on patient responses; (4) a termination logic to ensure interviews end once sufficient information is gathered; and (5) automated synthesis of doctor-friendly structured reports aligned with clinical workflows. Human-computer interaction principles guided the design of both the patient and physician applications. Preliminary evaluation involved five physicians using standardized instruments: NASA-TLX (cognitive workload), the System Usability Scale (SUS), and the Questionnaire for User Interface Satisfaction (QUIS). The patient application achieved low workload scores (NASA-TLX = 15.6), high usability (SUS = 86), and strong satisfaction (QUIS = 8.1/9), with particularly high ratings for ease of learning and interface design. The physician application yielded moderate workload (NASA-TLX = 26) and excellent usability (SUS = 88.5), with satisfaction scores of 8.3/9. Both applications demonstrated effective integration into clinical workflows, reducing cognitive demand and supporting efficient report generation. Limitations included occasional system latency and a small, non-diverse evaluation sample.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.12490v1",
    "published_date": "2025-10-14 13:24:21 UTC",
    "updated_date": "2025-10-14 13:24:21 UTC"
  },
  {
    "arxiv_id": "2510.12482v1",
    "title": "A Text-Image Fusion Method with Data Augmentation Capabilities for Referring Medical Image Segmentation",
    "authors": [
      "Shurong Chai",
      "Rahul Kumar JAIN",
      "Rui Xu",
      "Shaocong Mo",
      "Ruibo Hou",
      "Shiyu Teng",
      "Jiaqing Liu",
      "Lanfen Lin",
      "Yen-Wei Chen"
    ],
    "abstract": "Deep learning relies heavily on data augmentation to mitigate limited data, especially in medical imaging. Recent multimodal learning integrates text and images for segmentation, known as referring or text-guided image segmentation. However, common augmentations like rotation and flipping disrupt spatial alignment between image and text, weakening performance. To address this, we propose an early fusion framework that combines text and visual features before augmentation, preserving spatial consistency. We also design a lightweight generator that projects text embeddings into visual space, bridging semantic gaps. Visualization of generated pseudo-images shows accurate region localization. Our method is evaluated on three medical imaging tasks and four segmentation frameworks, achieving state-of-the-art results. Code is publicly available on GitHub: https://github.com/11yxk/MedSeg_EarlyFusion.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.12482v1",
    "published_date": "2025-10-14 13:18:34 UTC",
    "updated_date": "2025-10-14 13:18:34 UTC"
  },
  {
    "arxiv_id": "2510.12476v1",
    "title": "When Personalization Tricks Detectors: The Feature-Inversion Trap in Machine-Generated Text Detection",
    "authors": [
      "Lang Gao",
      "Xuhui Li",
      "Chenxi Wang",
      "Mingzhe Li",
      "Wei Liu",
      "Zirui Song",
      "Jinghui Zhang",
      "Rui Yan",
      "Preslav Nakov",
      "Xiuying Chen"
    ],
    "abstract": "Large language models (LLMs) have grown more powerful in language generation, producing fluent text and even imitating personal style. Yet, this ability also heightens the risk of identity impersonation. To the best of our knowledge, no prior work has examined personalized machine-generated text (MGT) detection. In this paper, we introduce \\dataset, the first benchmark for evaluating detector robustness in personalized settings, built from literary and blog texts paired with their LLM-generated imitations. Our experimental results demonstrate large performance gaps across detectors in personalized settings: some state-of-the-art models suffer significant drops. We attribute this limitation to the \\textit{feature-inversion trap}, where features that are discriminative in general domains become inverted and misleading when applied to personalized text. Based on this finding, we propose \\method, a simple and reliable way to predict detector performance changes in personalized settings. \\method identifies latent directions corresponding to inverted features and constructs probe datasets that differ primarily along these features to evaluate detector dependence. Our experiments show that \\method can accurately predict both the direction and the magnitude of post-transfer changes, showing 85\\% correlation with the actual performance gaps. We hope that this work will encourage further research on personalized text detection.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.12476v1",
    "published_date": "2025-10-14 13:10:23 UTC",
    "updated_date": "2025-10-14 13:10:23 UTC"
  },
  {
    "arxiv_id": "2510.12857v1",
    "title": "Adaptive Generation of Bias-Eliciting Questions for LLMs",
    "authors": [
      "Robin Staab",
      "Jasper Dekoninck",
      "Maximilian Baader",
      "Martin Vechev"
    ],
    "abstract": "Large language models (LLMs) are now widely deployed in user-facing applications, reaching hundreds of millions worldwide. As they become integrated into everyday tasks, growing reliance on their outputs raises significant concerns. In particular, users may unknowingly be exposed to model-inherent biases that systematically disadvantage or stereotype certain groups. However, existing bias benchmarks continue to rely on templated prompts or restrictive multiple-choice questions that are suggestive, simplistic, and fail to capture the complexity of real-world user interactions. In this work, we address this gap by introducing a counterfactual bias evaluation framework that automatically generates realistic, open-ended questions over sensitive attributes such as sex, race, or religion. By iteratively mutating and selecting bias-inducing questions, our approach systematically explores areas where models are most susceptible to biased behavior. Beyond detecting harmful biases, we also capture distinct response dimensions that are increasingly relevant in user interactions, such as asymmetric refusals and explicit acknowledgment of bias. Leveraging our framework, we construct CAB, a human-verified benchmark spanning diverse topics, designed to enable cross-model comparisons. Using CAB, we analyze a range of LLMs across multiple bias dimensions, revealing nuanced insights into how different models manifest bias. For instance, while GPT-5 outperforms other models, it nonetheless exhibits persistent biases in specific scenarios. These findings underscore the need for continual improvements to ensure fair model behavior.",
    "categories": [
      "cs.CY",
      "cs.AI"
    ],
    "primary_category": "cs.CY",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.12857v1",
    "published_date": "2025-10-14 13:08:10 UTC",
    "updated_date": "2025-10-14 13:08:10 UTC"
  },
  {
    "arxiv_id": "2510.12462v1",
    "title": "Evaluating and Mitigating LLM-as-a-judge Bias in Communication Systems",
    "authors": [
      "Jiaxin Gao",
      "Chen Chen",
      "Yanwen Jia",
      "Xueluan Gong",
      "Kwok-Yan Lam",
      "Qian Wang"
    ],
    "abstract": "Large Language Models (LLMs) are increasingly being used to autonomously evaluate the quality of content in communication systems, e.g., to assess responses in telecom customer support chatbots. However, the impartiality of these AI \"judges\" is not guaranteed, and any biases in their evaluation criteria could skew outcomes and undermine user trust. In this paper, we systematically investigate judgment biases in two LLM-as-a-judge models (i.e., GPT-Judge and JudgeLM) under the point-wise scoring setting, encompassing 11 types of biases that cover both implicit and explicit forms. We observed that state-of-the-art LLM judges demonstrate robustness to biased inputs, generally assigning them lower scores than the corresponding clean samples. Providing a detailed scoring rubric further enhances this robustness. We further found that fine-tuning an LLM on high-scoring yet biased responses can significantly degrade its performance, highlighting the risk of training on biased data. We also discovered that the judged scores correlate with task difficulty: a challenging dataset like GPQA yields lower average scores, whereas an open-ended reasoning dataset (e.g., JudgeLM-val) sees higher average scores. Finally, we proposed four potential mitigation strategies to ensure fair and reliable AI judging in practical communication scenarios.",
    "categories": [
      "cs.AI",
      "cs.CR"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.12462v1",
    "published_date": "2025-10-14 12:52:29 UTC",
    "updated_date": "2025-10-14 12:52:29 UTC"
  },
  {
    "arxiv_id": "2510.13893v1",
    "title": "Guarding the Guardrails: A Taxonomy-Driven Approach to Jailbreak Detection",
    "authors": [
      "Olga E. Sorokoletova",
      "Francesco Giarrusso",
      "Vincenzo Suriani",
      "Daniele Nardi"
    ],
    "abstract": "Jailbreaking techniques pose a significant threat to the safety of Large Language Models (LLMs). Existing defenses typically focus on single-turn attacks, lack coverage across languages, and rely on limited taxonomies that either fail to capture the full diversity of attack strategies or emphasize risk categories rather than the jailbreaking techniques. To advance the understanding of the effectiveness of jailbreaking techniques, we conducted a structured red-teaming challenge. The outcome of our experiments are manifold. First, we developed a comprehensive hierarchical taxonomy of 50 jailbreak strategies, consolidating and extending prior classifications into seven broad families, including impersonation, persuasion, privilege escalation, cognitive overload, obfuscation, goal conflict, and data poisoning. Second, we analyzed the data collected from the challenge to examine the prevalence and success rates of different attack types, providing insights into how specific jailbreak strategies exploit model vulnerabilities and induce misalignment. Third, we benchmark a popular LLM for jailbreak detection, evaluating the benefits of taxonomy-guided prompting for improving automatic detection. Finally, we compiled a new Italian dataset of 1364 multi-turn adversarial dialogues, annotated with our taxonomy, enabling the study of interactions where adversarial intent emerges gradually and succeeds in bypassing traditional safeguards.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.13893v1",
    "published_date": "2025-10-14 12:34:41 UTC",
    "updated_date": "2025-10-14 12:34:41 UTC"
  },
  {
    "arxiv_id": "2510.12451v1",
    "title": "A Function Centric Perspective On Flat and Sharp Minima",
    "authors": [
      "Israel Mason-Williams",
      "Gabryel Mason-Williams",
      "Helen Yannakoudakis"
    ],
    "abstract": "Flat minima are widely believed to correlate with improved generalisation in deep neural networks. However, this connection has proven more nuanced in recent studies, with both theoretical counterexamples and empirical exceptions emerging in the literature. In this paper, we revisit the role of sharpness in model performance, proposing that sharpness is better understood as a function-dependent property rather than a reliable indicator of poor generalisation. We conduct extensive empirical studies, from single-objective optimisation to modern image classification tasks, showing that sharper minima often emerge when models are regularised (e.g., via SAM, weight decay, or data augmentation), and that these sharp minima can coincide with better generalisation, calibration, robustness, and functional consistency. Across a range of models and datasets, we find that baselines without regularisation tend to converge to flatter minima yet often perform worse across all safety metrics. Our findings demonstrate that function complexity, rather than flatness alone, governs the geometry of solutions, and that sharper minima can reflect more appropriate inductive biases (especially under regularisation), calling for a function-centric reappraisal of loss landscape geometry.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.LG",
    "comment": "26 pages, 26 tables, 63 figures, pre-print",
    "pdf_url": "https://arxiv.org/pdf/2510.12451v1",
    "published_date": "2025-10-14 12:33:14 UTC",
    "updated_date": "2025-10-14 12:33:14 UTC"
  },
  {
    "arxiv_id": "2510.12428v1",
    "title": "Biased-Attention Guided Risk Prediction for Safe Decision-Making at Unsignalized Intersections",
    "authors": [
      "Chengyang Dong",
      "Nan Guo"
    ],
    "abstract": "Autonomous driving decision-making at unsignalized intersections is highly challenging due to complex dynamic interactions and high conflict risks. To achieve proactive safety control, this paper proposes a deep reinforcement learning (DRL) decision-making framework integrated with a biased attention mechanism. The framework is built upon the Soft Actor-Critic (SAC) algorithm. Its core innovation lies in the use of biased attention to construct a traffic risk predictor. This predictor assesses the long-term risk of collision for a vehicle entering the intersection and transforms this risk into a dense reward signal to guide the SAC agent in making safe and efficient driving decisions. Finally, the simulation results demonstrate that the proposed method effectively improves both traffic efficiency and vehicle safety at the intersection, thereby proving the effectiveness of the intelligent decision-making framework in complex scenarios. The code of our work is available at https://github.com/hank111525/SAC-RWB.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.12428v1",
    "published_date": "2025-10-14 12:05:51 UTC",
    "updated_date": "2025-10-14 12:05:51 UTC"
  },
  {
    "arxiv_id": "2510.12423v1",
    "title": "MTOS: A LLM-Driven Multi-topic Opinion Simulation Framework for Exploring Echo Chamber Dynamics",
    "authors": [
      "Dingyi Zuo",
      "Hongjie Zhang",
      "Jie Ou",
      "Chaosheng Feng",
      "Shuwan Liu"
    ],
    "abstract": "The polarization of opinions, information segregation, and cognitive biases on social media have attracted significant academic attention. In real-world networks, information often spans multiple interrelated topics, posing challenges for opinion evolution and highlighting the need for frameworks that simulate interactions among topics. Existing studies based on large language models (LLMs) focus largely on single topics, limiting the capture of cognitive transfer in multi-topic, cross-domain contexts. Traditional numerical models, meanwhile, simplify complex linguistic attitudes into discrete values, lacking interpretability, behavioral consistency, and the ability to integrate multiple topics. To address these issues, we propose Multi-topic Opinion Simulation (MTOS), a social simulation framework integrating multi-topic contexts with LLMs. MTOS leverages LLMs alongside short-term and long-term memory, incorporates multiple user-selection interaction mechanisms and dynamic topic-selection strategies, and employs a belief decay mechanism to enable perspective updates across topics. We conduct extensive experiments on MTOS, varying topic numbers, correlation types, and performing ablation studies to assess features such as group polarization and local consistency. Results show that multi-topic settings significantly alter polarization trends: positively correlated topics amplify echo chambers, negatively correlated topics inhibit them, and irrelevant topics also mitigate echo chamber effects through resource competition. Compared with numerical models, LLM-based agents realistically simulate dynamic opinion changes, reproduce linguistic features of news texts, and capture complex human reasoning, improving simulation interpretability and system stability.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "14 pages, 11figures",
    "pdf_url": "https://arxiv.org/pdf/2510.12423v1",
    "published_date": "2025-10-14 11:59:47 UTC",
    "updated_date": "2025-10-14 11:59:47 UTC"
  },
  {
    "arxiv_id": "2510.12409v1",
    "title": "PricingLogic: Evaluating LLMs Reasoning on Complex Tourism Pricing Tasks",
    "authors": [
      "Yunuo Liu",
      "Dawei Zhu",
      "Zena Al-Khalili",
      "Dai Cheng",
      "Yanjun Chen",
      "Dietrich Klakow",
      "Wei Zhang",
      "Xiaoyu Shen"
    ],
    "abstract": "We present PricingLogic, the first benchmark that probes whether Large Language Models(LLMs) can reliably automate tourism-related prices when multiple, overlapping fare rules apply. Travel agencies are eager to offload this error-prone task onto AI systems; however, deploying LLMs without verified reliability could result in significant financial losses and erode customer trust. PricingLogic comprises 300 natural-language questions based on booking requests derived from 42 real-world pricing policies, spanning two levels of difficulty: (i) basic customer-type pricing and (ii)bundled-tour calculations involving interacting discounts. Evaluations of a line of LLMs reveal a steep performance drop on the harder tier,exposing systematic failures in rule interpretation and arithmetic reasoning.These results highlight that, despite their general capabilities, today's LLMs remain unreliable in revenue-critical applications without further safeguards or domain adaptation. Our code and dataset are available at https://github.com/EIT-NLP/PricingLogic.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.12409v1",
    "published_date": "2025-10-14 11:42:15 UTC",
    "updated_date": "2025-10-14 11:42:15 UTC"
  },
  {
    "arxiv_id": "2510.12408v1",
    "title": "Low-Field Magnetic Resonance Image Quality Enhancement using a Conditional Flow Matching Model",
    "authors": [
      "Huu Tien Nguyen",
      "Ahmed Karam Eldaly"
    ],
    "abstract": "This paper introduces a novel framework for image quality transfer based on conditional flow matching (CFM). Unlike conventional generative models that rely on iterative sampling or adversarial objectives, CFM learns a continuous flow between a noise distribution and target data distributions through the direct regression of an optimal velocity field. We evaluate this approach in the context of low-field magnetic resonance imaging (LF-MRI), a rapidly emerging modality that offers affordable and portable scanning but suffers from inherently low signal-to-noise ratio and reduced diagnostic quality. Our framework is designed to reconstruct high-field-like MR images from their corresponding low-field inputs, thereby bridging the quality gap without requiring expensive infrastructure. Experiments demonstrate that CFM not only achieves state-of-the-art performance, but also generalizes robustly to both in-distribution and out-of-distribution data. Importantly, it does so while utilizing significantly fewer parameters than competing deep learning methods. These results underline the potential of CFM as a powerful and scalable tool for MRI reconstruction, particularly in resource-limited clinical environments.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.12408v1",
    "published_date": "2025-10-14 11:41:27 UTC",
    "updated_date": "2025-10-14 11:41:27 UTC"
  },
  {
    "arxiv_id": "2510.12856v1",
    "title": "Efficient Adaptive Transformer: An Empirical Study and Reproducible Framework",
    "authors": [
      "Jan Miller"
    ],
    "abstract": "The Efficient Adaptive Transformer (EAT) framework unifies three adaptive efficiency techniques - progressive token pruning, sparse attention, and dynamic early exiting - into a single, reproducible architecture for input-adaptive inference. EAT provides an open-source benchmarking pipeline that automates data processing, timing, and ablation across GLUE tasks (SST-2, QQP, MNLI). Although this empirical study finds that combining these mechanisms can increase latency in shallow six-layer models, it demonstrates that EAT achieves slightly higher accuracy than the optimized DistilBERT baseline on SST-2, illustrating the potential of dynamic computation for latency-sensitive NLP. The main contribution is the open, end-to-end reproducible framework - complete with scripts, CSV logging, and analysis utilities - intended to serve as a community tool for further research on adaptive transformers.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "10 pages, 6 figures, pgfplots tables included; BibTeX compiled to .bbl. Code and reproducibility artifacts referenced in the paper",
    "pdf_url": "https://arxiv.org/pdf/2510.12856v1",
    "published_date": "2025-10-14 11:40:48 UTC",
    "updated_date": "2025-10-14 11:40:48 UTC"
  },
  {
    "arxiv_id": "2510.12399v2",
    "title": "A Survey of Vibe Coding with Large Language Models",
    "authors": [
      "Yuyao Ge",
      "Lingrui Mei",
      "Zenghao Duan",
      "Tianhao Li",
      "Yujia Zheng",
      "Yiwei Wang",
      "Lexin Wang",
      "Jiayu Yao",
      "Tianyu Liu",
      "Yujun Cai",
      "Baolong Bi",
      "Fangda Guo",
      "Jiafeng Guo",
      "Shenghua Liu",
      "Xueqi Cheng"
    ],
    "abstract": "The advancement of large language models (LLMs) has catalyzed a paradigm shift from code generation assistance to autonomous coding agents, enabling a novel development methodology termed \"Vibe Coding\" where developers validate AI-generated implementations through outcome observation rather than line-by-line code comprehension. Despite its transformative potential, the effectiveness of this emergent paradigm remains under-explored, with empirical evidence revealing unexpected productivity losses and fundamental challenges in human-AI collaboration. To address this gap, this survey provides the first comprehensive and systematic review of Vibe Coding with large language models, establishing both theoretical foundations and practical frameworks for this transformative development approach. Drawing from systematic analysis of over 1000 research papers, we survey the entire vibe coding ecosystem, examining critical infrastructure components including LLMs for coding, LLM-based coding agent, development environment of coding agent, and feedback mechanisms. We first introduce Vibe Coding as a formal discipline by formalizing it through a Constrained Markov Decision Process that captures the dynamic triadic relationship among human developers, software projects, and coding agents. Building upon this theoretical foundation, we then synthesize existing practices into five distinct development models: Unconstrained Automation, Iterative Conversational Collaboration, Planning-Driven, Test-Driven, and Context-Enhanced Models, thus providing the first comprehensive taxonomy in this domain. Critically, our analysis reveals that successful Vibe Coding depends not merely on agent capabilities but on systematic context engineering, well-established development environments, and human-agent collaborative development models.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.12399v2",
    "published_date": "2025-10-14 11:26:56 UTC",
    "updated_date": "2025-12-21 03:48:55 UTC"
  },
  {
    "arxiv_id": "2510.12389v1",
    "title": "Tokenization Disparities as Infrastructure Bias: How Subword Systems Create Inequities in LLM Access and Efficiency",
    "authors": [
      "Hailay Kidu Teklehaymanot",
      "Wolfgang Nejdl"
    ],
    "abstract": "Tokenization disparities pose a significant barrier to achieving equitable access to artificial intelligence across linguistically diverse populations. This study conducts a large-scale cross-linguistic evaluation of tokenization efficiency in over 200 languages to systematically quantify computational inequities in large language models (LLMs). Using a standardized experimental framework, we applied consistent preprocessing and normalization protocols, followed by uniform tokenization through the tiktoken library across all language samples. Comprehensive tokenization statistics were collected using established evaluation metrics, including Tokens Per Sentence (TPS) and Relative Tokenization Cost (RTC), benchmarked against English baselines. Our cross-linguistic analysis reveals substantial and systematic disparities: Latin-script languages consistently exhibit higher tokenization efficiency, while non-Latin and morphologically complex languages incur significantly greater token inflation, often 3-5 times higher RTC ratios. These inefficiencies translate into increased computational costs and reduced effective context utilization for underrepresented languages. Overall, the findings highlight structural inequities in current AI systems, where speakers of low-resource and non-Latin languages face disproportionate computational disadvantages. Future research should prioritize the development of linguistically informed tokenization strategies and adaptive vocabulary construction methods that incorporate typological diversity, ensuring more inclusive and computationally equitable multilingual AI systems.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "6 pages 4 figures",
    "pdf_url": "https://arxiv.org/pdf/2510.12389v1",
    "published_date": "2025-10-14 11:14:38 UTC",
    "updated_date": "2025-10-14 11:14:38 UTC"
  },
  {
    "arxiv_id": "2510.12384v3",
    "title": "Phenome-Wide Multi-Omics Integration Uncovers Distinct Archetypes of Human Aging",
    "authors": [
      "Huifa Li",
      "Feilong Tang",
      "Haochen Xue",
      "Yulong Li",
      "Xinlin Zhuang",
      "Bin Zhang",
      "Eran Segal",
      "Imran Razzak"
    ],
    "abstract": "Aging is a highly complex and heterogeneous process that progresses at different rates across individuals, making biological age (BA) a more accurate indicator of physiological decline than chronological age. While previous studies have built aging clocks using single-omics data, they often fail to capture the full molecular complexity of human aging. In this work, we leveraged the Human Phenotype Project, a large-scale cohort of 10,000 adults aged 40-70 years, with extensive longitudinal profiling that includes clinical, behavioral, environmental, and multi-omics datasets spanning transcriptomics, lipidomics, metabolomics, and the microbiome. By employing advanced machine learning frameworks capable of modeling nonlinear biological dynamics, we developed and rigorously validated a multi-omics aging clock that robustly predicts diverse health outcomes and future disease risk. Unsupervised clustering of the integrated molecular profiles from multi-omics uncovered distinct biological subtypes of aging, revealing striking heterogeneity in aging trajectories and pinpointing pathway-specific alterations associated with different aging patterns. These findings demonstrate the power of multi-omics integration to decode the molecular landscape of aging and lay the groundwork for personalized healthspan monitoring and precision strategies to prevent age-related diseases.",
    "categories": [
      "q-bio.GN",
      "cs.AI"
    ],
    "primary_category": "q-bio.GN",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.12384v3",
    "published_date": "2025-10-14 11:00:51 UTC",
    "updated_date": "2025-10-23 08:18:20 UTC"
  },
  {
    "arxiv_id": "2510.12379v1",
    "title": "LiteVPNet: A Lightweight Network for Video Encoding Control in Quality-Critical Applications",
    "authors": [
      "Vibhoothi Vibhoothi",
      "François Pitié",
      "Anil Kokaram"
    ],
    "abstract": "In the last decade, video workflows in the cinema production ecosystem have presented new use cases for video streaming technology. These new workflows, e.g. in On-set Virtual Production, present the challenge of requiring precise quality control and energy efficiency. Existing approaches to transcoding often fall short of these requirements, either due to a lack of quality control or computational overhead. To fill this gap, we present a lightweight neural network (LiteVPNet) for accurately predicting Quantisation Parameters for NVENC AV1 encoders that achieve a specified VMAF score. We use low-complexity features, including bitstream characteristics, video complexity measures, and CLIP-based semantic embeddings. Our results demonstrate that LiteVPNet achieves mean VMAF errors below 1.2 points across a wide range of quality targets. Notably, LiteVPNet achieves VMAF errors within 2 points for over 87% of our test corpus, c.f. approx 61% with state-of-the-art methods. LiteVPNet's performance across various quality regions highlights its applicability for enhancing high-value content transport and streaming for more energy-efficient, high-quality media experiences.",
    "categories": [
      "eess.IV",
      "cs.AI",
      "cs.LG",
      "cs.MM"
    ],
    "primary_category": "eess.IV",
    "comment": "Accepted PCS 2025 Camera-Ready Version, 5 Pages",
    "pdf_url": "https://arxiv.org/pdf/2510.12379v1",
    "published_date": "2025-10-14 10:51:49 UTC",
    "updated_date": "2025-10-14 10:51:49 UTC"
  },
  {
    "arxiv_id": "2510.12376v1",
    "title": "Deep Attention-guided Adaptive Subsampling",
    "authors": [
      "Sharath M Shankaranarayana",
      "Soumava Kumar Roy",
      "Prasad Sudhakar",
      "Chandan Aladahalli"
    ],
    "abstract": "Although deep neural networks have provided impressive gains in performance, these improvements often come at the cost of increased computational complexity and expense. In many cases, such as 3D volume or video classification tasks, not all slices or frames are necessary due to inherent redundancies. To address this issue, we propose a novel learnable subsampling framework that can be integrated into any neural network architecture. Subsampling, being a nondifferentiable operation, poses significant challenges for direct adaptation into deep learning models. While some works, have proposed solutions using the Gumbel-max trick to overcome the problem of non-differentiability, they fall short in a crucial aspect: they are only task-adaptive and not inputadaptive. Once the sampling mechanism is learned, it remains static and does not adjust to different inputs, making it unsuitable for real-world applications. To this end, we propose an attention-guided sampling module that adapts to inputs even during inference. This dynamic adaptation results in performance gains and reduces complexity in deep neural network models. We demonstrate the effectiveness of our method on 3D medical imaging datasets from MedMNIST3D as well as two ultrasound video datasets for classification tasks, one of them being a challenging in-house dataset collected under real-world clinical conditions.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.12376v1",
    "published_date": "2025-10-14 10:50:45 UTC",
    "updated_date": "2025-10-14 10:50:45 UTC"
  },
  {
    "arxiv_id": "2510.12367v1",
    "title": "LLM-REVal: Can We Trust LLM Reviewers Yet?",
    "authors": [
      "Rui Li",
      "Jia-Chen Gu",
      "Po-Nien Kung",
      "Heming Xia",
      "Junfeng liu",
      "Xiangwen Kong",
      "Zhifang Sui",
      "Nanyun Peng"
    ],
    "abstract": "The rapid advancement of large language models (LLMs) has inspired researchers to integrate them extensively into the academic workflow, potentially reshaping how research is practiced and reviewed. While previous studies highlight the potential of LLMs in supporting research and peer review, their dual roles in the academic workflow and the complex interplay between research and review bring new risks that remain largely underexplored. In this study, we focus on how the deep integration of LLMs into both peer-review and research processes may influence scholarly fairness, examining the potential risks of using LLMs as reviewers by simulation. This simulation incorporates a research agent, which generates papers and revises, alongside a review agent, which assesses the submissions. Based on the simulation results, we conduct human annotations and identify pronounced misalignment between LLM-based reviews and human judgments: (1) LLM reviewers systematically inflate scores for LLM-authored papers, assigning them markedly higher scores than human-authored ones; (2) LLM reviewers persistently underrate human-authored papers with critical statements (e.g., risk, fairness), even after multiple revisions. Our analysis reveals that these stem from two primary biases in LLM reviewers: a linguistic feature bias favoring LLM-generated writing styles, and an aversion toward critical statements. These results highlight the risks and equity concerns posed to human authors and academic research if LLMs are deployed in the peer review cycle without adequate caution. On the other hand, revisions guided by LLM reviews yield quality gains in both LLM-based and human evaluations, illustrating the potential of the LLMs-as-reviewers for early-stage researchers and enhancing low-quality papers.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.12367v1",
    "published_date": "2025-10-14 10:30:20 UTC",
    "updated_date": "2025-10-14 10:30:20 UTC"
  },
  {
    "arxiv_id": "2510.12364v2",
    "title": "(R)evolution of Programming: Vibe Coding as a Post-Coding Paradigm",
    "authors": [
      "Kevin Krings",
      "Nino S. Bohn",
      "Thomas Ludwig"
    ],
    "abstract": "Recent advancements in generative artificial intelligence (GenAI), particularly large language models, have introduced new possibilities for software development practices. In our paper we investigate the emerging Vibe Coding (VC) paradigm that emphasizes intuitive, affect-driven, and improvisational interactions between developers and AI systems. Building upon the discourse of End-User Development (EUD), we explore how VC diverges from conventional programming approaches such as those supported by tools like GitHub Copilot. Through five semi-structured interview sessions with ten experienced software practitioners, we identify five thematic dimensions: creativity, sustainability, the future of programming, collaboration, and criticism. Our analysis conceptualizes VC within the metaphor of co-drifting, contrasting it with the prevalent co-piloting perspective of AI-assisted development. We argue that VC reconfigures the developers role, blurring boundaries between professional and non-developers. While VC enables novel forms of expression and rapid prototyping, it also introduces challenges regarding reproducibility, scalability, and inclusivity. We propose that VC represents a meaningful shift in programming culture, warranting further investigation within human-computer interaction (HCI) and software engineering research.",
    "categories": [
      "cs.SE",
      "cs.AI",
      "cs.HC"
    ],
    "primary_category": "cs.SE",
    "comment": "Workshop Contribution at the sixth decennial Aarhus conference in \"The End of Programming (as we know it) - Envisioning Radical Re-Conceptualizations of Co-Coding with AI\"",
    "pdf_url": "https://arxiv.org/pdf/2510.12364v2",
    "published_date": "2025-10-14 10:25:56 UTC",
    "updated_date": "2025-10-15 14:43:26 UTC"
  },
  {
    "arxiv_id": "2510.12350v2",
    "title": "O-Forge: An LLM + Computer Algebra Framework for Asymptotic Analysis",
    "authors": [
      "Ayush Khaitan",
      "Vijay Ganesh"
    ],
    "abstract": "Large language models have recently demonstrated advanced capabilities in solving IMO and Putnam problems; yet their role in research mathematics has remained fairly limited. The key difficulty is verification: suggested proofs may look plausible, but cannot be trusted without rigorous checking. We present a framework, called LLM+CAS, and an associated tool, O-Forge, that couples frontier LLMs with a computer algebra systems (CAS) in an In-Context Symbolic Feedback loop to produce proofs that are both creative and symbolically verified. Our focus is on asymptotic inequalities, a topic that often involves difficult proofs and appropriate decomposition of the domain into the \"right\" subdomains. Many mathematicians, including Terry Tao, have suggested that using AI tools to find the right decompositions can be very useful for research-level asymptotic analysis. In this paper, we show that our framework LLM+CAS turns out to be remarkably effective at proposing such decompositions via a combination of a frontier LLM and a CAS. More precisely, we use an LLM to suggest domain decomposition, and a CAS (such as Mathematica) that provides a verification of each piece axiomatically. Using this loop, we answer a question posed by Terence Tao: whether LLMs coupled with a verifier can be used to help prove intricate asymptotic inequalities. More broadly, we show how AI can move beyond contest math towards research-level tools for professional mathematicians.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.12350v2",
    "published_date": "2025-10-14 10:07:53 UTC",
    "updated_date": "2025-10-16 13:07:41 UTC"
  },
  {
    "arxiv_id": "2510.12334v1",
    "title": "Finite-time Convergence Analysis of Actor-Critic with Evolving Reward",
    "authors": [
      "Rui Hu",
      "Yu Chen",
      "Longbo Huang"
    ],
    "abstract": "Many popular practical reinforcement learning (RL) algorithms employ evolving reward functions-through techniques such as reward shaping, entropy regularization, or curriculum learning-yet their theoretical foundations remain underdeveloped. This paper provides the first finite-time convergence analysis of a single-timescale actor-critic algorithm in the presence of an evolving reward function under Markovian sampling. We consider a setting where the reward parameters may change at each time step, affecting both policy optimization and value estimation. Under standard assumptions, we derive non-asymptotic bounds for both actor and critic errors. Our result shows that an $O(1/\\sqrt{T})$ convergence rate is achievable, matching the best-known rate for static rewards, provided the reward parameters evolve slowly enough. This rate is preserved when the reward is updated via a gradient-based rule with bounded gradient and on the same timescale as the actor and critic, offering a theoretical foundation for many popular RL techniques. As a secondary contribution, we introduce a novel analysis of distribution mismatch under Markovian sampling, improving the best-known rate by a factor of $\\log^2T$ in the static-reward case.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.12334v1",
    "published_date": "2025-10-14 09:45:19 UTC",
    "updated_date": "2025-10-14 09:45:19 UTC"
  },
  {
    "arxiv_id": "2511.13726v1",
    "title": "Refine Thought: A Test-Time Inference Method for Embedding Model Reasoning",
    "authors": [
      "Guangzhi Wang",
      "Kai Li",
      "Yinghao Jiao",
      "Zhi Liu"
    ],
    "abstract": "We propose RT (Refine Thought), a method that can enhance the semantic rea-soning ability of text embedding models. The method obtains the final semanticrepresentation by running multiple forward passes of the text embedding model.Experiments show that RT achieves significant improvements on semantic reason-ing tasks in BRIGHT and the person job matching benchmark PJBenchmark1, while maintaining consistent performance on general-purpose semantic under-standing tasks such as C-MTEB. Our results indicate that RT is effective becauseit further activates the semantic reasoning ability learned during pretraining bydecoder-only text embedding models(e.g., Qwen3-Embedding-8B). RT canbe seen as a test-time inference method.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.IR"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2511.13726v1",
    "published_date": "2025-10-14 09:35:27 UTC",
    "updated_date": "2025-10-14 09:35:27 UTC"
  },
  {
    "arxiv_id": "2510.16001v1",
    "title": "A Non-overlap-based Conflict Measure for Random Permutation Sets",
    "authors": [
      "Ruolan Cheng",
      "Yong Deng",
      "Enrique Herrera-Viedma"
    ],
    "abstract": "Random permutation set (RPS) is a new formalism for reasoning with uncertainty involving order information. Measuring the conflict between two pieces of evidence represented by permutation mass functions remains an urgent research topic in order-structured uncertain information fusion. In this paper, a detailed analysis of conflicts in RPS is carried out from two different perspectives: random finite set (RFS) and Dempster-Shafer theory (DST). Starting from the observation of permutations, we first define an inconsistency measure between permutations inspired by the rank-biased overlap(RBO) measure and further propose a non-overlap-based conflict measure method for RPSs. This paper regards RPS theory (RPST) as an extension of DST. The order information newly added in focal sets indicates qualitative propensity, characterized by top-ranked elements occupying a more critical position. Some numerical examples are used to demonstrate the behavior and properties of the proposed conflict measure. The proposed method not only has the natural top-weightedness property and can effectively measure the conflict between RPSs from the DST view but also provides decision-makers with a flexible selection of weights, parameters, and truncated depths.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.16001v1",
    "published_date": "2025-10-14 09:35:03 UTC",
    "updated_date": "2025-10-14 09:35:03 UTC"
  },
  {
    "arxiv_id": "2510.12327v1",
    "title": "Simple Projection Variants Improve ColBERT Performance",
    "authors": [
      "Benjamin Clavié",
      "Sean Lee",
      "Rikiya Takehi",
      "Aamir Shakir",
      "Makoto P. Kato"
    ],
    "abstract": "Multi-vector dense retrieval methods like ColBERT systematically use a single-layer linear projection to reduce the dimensionality of individual vectors. In this study, we explore the implications of the MaxSim operator on the gradient flows of the training of multi-vector models and show that such a simple linear projection has inherent, if non-critical, limitations in this setting. We then discuss the theoretical improvements that could result from replacing this single-layer projection with well-studied alternative feedforward linear networks (FFN), such as deeper, non-linear FFN blocks, GLU blocks, and skip-connections, could alleviate these limitations. Through the design and systematic evaluation of alternate projection blocks, we show that better-designed final projections positively impact the downstream performance of ColBERT models. We highlight that many projection variants outperform the original linear projections, with the best-performing variants increasing average performance on a range of retrieval benchmarks across domains by over 2 NDCG@10 points. We then conduct further exploration on the individual parameters of these projections block in order to understand what drives this empirical performance, highlighting the particular importance of upscaled intermediate projections and residual connections. As part of these ablation studies, we show that numerous suboptimal projection variants still outperform the traditional single-layer projection across multiple benchmarks, confirming our hypothesis. Finally, we observe that this effect is consistent across random seeds, further confirming that replacing the linear layer of ColBERT models is a robust, drop-in upgrade.",
    "categories": [
      "cs.IR",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.IR",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.12327v1",
    "published_date": "2025-10-14 09:34:05 UTC",
    "updated_date": "2025-10-14 09:34:05 UTC"
  },
  {
    "arxiv_id": "2510.12325v1",
    "title": "Causal Inspired Multi Modal Recommendation",
    "authors": [
      "Jie Yang",
      "Chenyang Gu",
      "Zixuan Liu"
    ],
    "abstract": "Multimodal recommender systems enhance personalized recommendations in e-commerce and online advertising by integrating visual, textual, and user-item interaction data. However, existing methods often overlook two critical biases: (i) modal confounding, where latent factors (e.g., brand style or product category) simultaneously drive multiple modalities and influence user preference, leading to spurious feature-preference associations; (ii) interaction bias, where genuine user preferences are mixed with noise from exposure effects and accidental clicks. To address these challenges, we propose a Causal-inspired multimodal Recommendation framework. Specifically, we introduce a dual-channel cross-modal diffusion module to identify hidden modal confounders, utilize back-door adjustment with hierarchical matching and vector-quantized codebooks to block confounding paths, and apply front-door adjustment combined with causal topology reconstruction to build a deconfounded causal subgraph. Extensive experiments on three real-world e-commerce datasets demonstrate that our method significantly outperforms state-of-the-art baselines while maintaining strong interpretability.",
    "categories": [
      "cs.IR",
      "cs.AI"
    ],
    "primary_category": "cs.IR",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.12325v1",
    "published_date": "2025-10-14 09:29:07 UTC",
    "updated_date": "2025-10-14 09:29:07 UTC"
  },
  {
    "arxiv_id": "2510.12323v1",
    "title": "RAG-Anything: All-in-One RAG Framework",
    "authors": [
      "Zirui Guo",
      "Xubin Ren",
      "Lingrui Xu",
      "Jiahao Zhang",
      "Chao Huang"
    ],
    "abstract": "Retrieval-Augmented Generation (RAG) has emerged as a fundamental paradigm for expanding Large Language Models beyond their static training limitations. However, a critical misalignment exists between current RAG capabilities and real-world information environments. Modern knowledge repositories are inherently multimodal, containing rich combinations of textual content, visual elements, structured tables, and mathematical expressions. Yet existing RAG frameworks are limited to textual content, creating fundamental gaps when processing multimodal documents. We present RAG-Anything, a unified framework that enables comprehensive knowledge retrieval across all modalities. Our approach reconceptualizes multimodal content as interconnected knowledge entities rather than isolated data types. The framework introduces dual-graph construction to capture both cross-modal relationships and textual semantics within a unified representation. We develop cross-modal hybrid retrieval that combines structural knowledge navigation with semantic matching. This enables effective reasoning over heterogeneous content where relevant evidence spans multiple modalities. RAG-Anything demonstrates superior performance on challenging multimodal benchmarks, achieving significant improvements over state-of-the-art methods. Performance gains become particularly pronounced on long documents where traditional approaches fail. Our framework establishes a new paradigm for multimodal knowledge access, eliminating the architectural fragmentation that constrains current systems. Our framework is open-sourced at: https://github.com/HKUDS/RAG-Anything.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.12323v1",
    "published_date": "2025-10-14 09:25:35 UTC",
    "updated_date": "2025-10-14 09:25:35 UTC"
  },
  {
    "arxiv_id": "2510.12312v1",
    "title": "Deep SPI: Safe Policy Improvement via World Models",
    "authors": [
      "Florent Delgrange",
      "Raphael Avalos",
      "Willem Röpke"
    ],
    "abstract": "Safe policy improvement (SPI) offers theoretical control over policy updates, yet existing guarantees largely concern offline, tabular reinforcement learning (RL). We study SPI in general online settings, when combined with world model and representation learning. We develop a theoretical framework showing that restricting policy updates to a well-defined neighborhood of the current policy ensures monotonic improvement and convergence. This analysis links transition and reward prediction losses to representation quality, yielding online, \"deep\" analogues of classical SPI theorems from the offline RL literature. Building on these results, we introduce DeepSPI, a principled on-policy algorithm that couples local transition and reward losses with regularised policy updates. On the ALE-57 benchmark, DeepSPI matches or exceeds strong baselines, including PPO and DeepMDPs, while retaining theoretical guarantees.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "10 pages main text, 17 pages appendix (excluding references)",
    "pdf_url": "https://arxiv.org/pdf/2510.12312v1",
    "published_date": "2025-10-14 09:11:24 UTC",
    "updated_date": "2025-10-14 09:11:24 UTC"
  },
  {
    "arxiv_id": "2510.15998v1",
    "title": "AMStraMGRAM: Adaptive Multi-cutoff Strategy Modification for ANaGRAM",
    "authors": [
      "Nilo Schwencke",
      "Cyriaque Rousselot",
      "Alena Shilova",
      "Cyril Furtlehner"
    ],
    "abstract": "Recent works have shown that natural gradient methods can significantly outperform standard optimizers when training physics-informed neural networks (PINNs). In this paper, we analyze the training dynamics of PINNs optimized with ANaGRAM, a natural-gradient-inspired approach employing singular value decomposition with cutoff regularization. Building on this analysis, we propose a multi-cutoff adaptation strategy that further enhances ANaGRAM's performance. Experiments on benchmark PDEs validate the effectiveness of our method, which allows to reach machine precision on some experiments. To provide theoretical grounding, we develop a framework based on spectral theory that explains the necessity of regularization and extend previous shown connections with Green's functions theory.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.15998v1",
    "published_date": "2025-10-14 09:10:42 UTC",
    "updated_date": "2025-10-14 09:10:42 UTC"
  },
  {
    "arxiv_id": "2510.12850v1",
    "title": "Ethic-BERT: An Enhanced Deep Learning Model for Ethical and Non-Ethical Content Classification",
    "authors": [
      "Mahamodul Hasan Mahadi",
      "Md. Nasif Safwan",
      "Souhardo Rahman",
      "Shahnaj Parvin",
      "Aminun Nahar",
      "Kamruddin Nur"
    ],
    "abstract": "Developing AI systems capable of nuanced ethical reasoning is critical as they increasingly influence human decisions, yet existing models often rely on superficial correlations rather than principled moral understanding. This paper introduces Ethic-BERT, a BERT-based model for ethical content classification across four domains: Commonsense, Justice, Virtue, and Deontology. Leveraging the ETHICS dataset, our approach integrates robust preprocessing to address vocabulary sparsity and contextual ambiguities, alongside advanced fine-tuning strategies like full model unfreezing, gradient accumulation, and adaptive learning rate scheduling. To evaluate robustness, we employ an adversarially filtered \"Hard Test\" split, isolating complex ethical dilemmas. Experimental results demonstrate Ethic-BERT's superiority over baseline models, achieving 82.32% average accuracy on the standard test, with notable improvements in Justice and Virtue. In addition, the proposed Ethic-BERT attains 15.28% average accuracy improvement in the HardTest. These findings contribute to performance improvement and reliable decision-making using bias-aware preprocessing and proposed enhanced AI model.",
    "categories": [
      "cs.CY",
      "cs.AI"
    ],
    "primary_category": "cs.CY",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.12850v1",
    "published_date": "2025-10-14 08:42:14 UTC",
    "updated_date": "2025-10-14 08:42:14 UTC"
  },
  {
    "arxiv_id": "2510.12285v1",
    "title": "Chinese ModernBERT with Whole-Word Masking",
    "authors": [
      "Zeyu Zhao",
      "Ningtao Wang",
      "Xing Fu",
      "Yu Cheng"
    ],
    "abstract": "Encoder-only Transformers have advanced along three axes -- architecture, data, and systems -- yielding Pareto gains in accuracy, speed, and memory efficiency. Yet these improvements have not fully transferred to Chinese, where tokenization and morphology differ markedly from English. We introduce Chinese ModernBERT, a from-scratch Chinese encoder that couples: (i) a hardware-aware 32k BPE vocabulary tailored to frequent Chinese affixes/compounds, lowering the embedding budget; (ii) whole-word masking (WWM) with a dynamic masking curriculum (30% -> 15%) to align task difficulty with training progress; (iii) a two-stage pre-training pipeline that extends the native context from 1,024 to 8,192 tokens using RoPE and alternating local/global attention; and (iv) a damped-cosine learning-rate schedule for stable long-horizon optimization. We pre-train on ~1.2T Chinese tokens from CCI3-HQ, CCI4 (Chinese), and Cosmopedia-Chinese. On CLUE, Chinese ModernBERT is competitive with strong Chinese encoders under a unified fine-tuning protocol. Under bf16 it achieves high long-sequence throughput while maintaining strong short-sequence speed, reflecting benefits from budget allocation and attention design. To probe retrieval-oriented quality, we add a small amount of open contrastive data: fine-tuning on SimCLUE (~3M pairs) improves further when adding T2Ranking (~2M), reaching 0.505 (Pearson) / 0.537 (Spearman) on the SimCLUE test set. Under this open-data setting, Chinese ModernBERT surpasses Qwen-0.6B-embedding on SimCLUE, suggesting a clear scaling path for STS with additional curated pairs. We will release tokenizer and weights to facilitate reproducible research.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.12285v1",
    "published_date": "2025-10-14 08:41:22 UTC",
    "updated_date": "2025-10-14 08:41:22 UTC"
  },
  {
    "arxiv_id": "2510.15996v1",
    "title": "Using Kolmogorov-Smirnov Distance for Measuring Distribution Shift in Machine Learning",
    "authors": [
      "Ozan K. Tonguz",
      "Federico Taschin"
    ],
    "abstract": "One of the major problems in Machine Learning (ML) and Artificial Intelligence (AI) is the fact that the probability distribution of the test data in the real world could deviate substantially from the probability distribution of the training data set. When this happens, the predictions of an ML system or an AI agent could involve large errors which is very troublesome and undesirable. While this is a well-known hard problem plaguing the AI and ML systems' accuracy and reliability, in certain applications such errors could be critical for safety and reliability of AI and ML systems. One approach to deal with this problem is to monitor and measure the deviation in the probability distribution of the test data in real time and to compensate for this deviation. In this paper, we propose and explore the use of Kolmogorov-Smirnov (KS) Test for measuring the distribution shift and we show how the KS distance can be used to quantify the distribution shift and its impact on an AI agent's performance. Our results suggest that KS distance could be used as a valuable statistical tool for monitoring and measuring the distribution shift. More specifically, it is shown that even a distance of KS=0.02 could lead to about 50\\% increase in the travel time at a single intersection using a Reinforcement Learning agent which is quite significant. It is hoped that the use of KS Test and KS distance in AI-based smart transportation could be an important step forward for gauging the performance degradation of an AI agent in real time and this, in turn, could help the AI agent to cope with the distribution shift in a more informed manner.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.15996v1",
    "published_date": "2025-10-14 08:35:55 UTC",
    "updated_date": "2025-10-14 08:35:55 UTC"
  },
  {
    "arxiv_id": "2510.12278v1",
    "title": "Quantum Annealing for Staff Scheduling in Educational Environments",
    "authors": [
      "Alessia Ciacco",
      "Francesca Guerriero",
      "Eneko Osaba"
    ],
    "abstract": "We address a novel staff allocation problem that arises in the organization of collaborators among multiple school sites and educational levels. The problem emerges from a real case study in a public school in Calabria, Italy, where staff members must be distributed across kindergartens, primary, and secondary schools under constraints of availability, competencies, and fairness. To tackle this problem, we develop an optimization model and investigate a solution approach based on quantum annealing. Our computational experiments on real-world data show that quantum annealing is capable of producing balanced assignments in short runtimes. These results provide evidence of the practical applicability of quantum optimization methods in educational scheduling and, more broadly, in complex resource allocation tasks.",
    "categories": [
      "cs.ET",
      "cs.AI"
    ],
    "primary_category": "cs.ET",
    "comment": "8 pages, 3 tables, and 1 figure. Paper submitted to the International Conference on Quantum Communications, Networking, and Computing (QCNC 2026)",
    "pdf_url": "https://arxiv.org/pdf/2510.12278v1",
    "published_date": "2025-10-14 08:29:58 UTC",
    "updated_date": "2025-10-14 08:29:58 UTC"
  },
  {
    "arxiv_id": "2510.12275v1",
    "title": "TFGA-Net: Temporal-Frequency Graph Attention Network for Brain-Controlled Speaker Extraction",
    "authors": [
      "Youhao Si",
      "Yuan Liao",
      "Qiushi Han",
      "Yuhang Yang",
      "Rui Dai",
      "Liya Huang"
    ],
    "abstract": "The rapid development of auditory attention decoding (AAD) based on electroencephalography (EEG) signals offers the possibility EEG-driven target speaker extraction. However, how to effectively utilize the target-speaker common information between EEG and speech remains an unresolved problem. In this paper, we propose a model for brain-controlled speaker extraction, which utilizes the EEG recorded from the listener to extract the target speech. In order to effectively extract information from EEG signals, we derive multi-scale time--frequency features and further incorporate cortical topological structures that are selectively engaged during the task. Moreover, to effectively exploit the non-Euclidean structure of EEG signals and capture their global features, the graph convolutional networks and self-attention mechanism are used in the EEG encoder. In addition, to make full use of the fused EEG and speech feature and preserve global context and capture speech rhythm and prosody, we introduce MossFormer2 which combines MossFormer and RNN-Free Recurrent as separator. Experimental results on both the public Cocktail Party and KUL dataset in this paper show that our TFGA-Net model significantly outper-forms the state-of-the-art method in certain objective evaluation metrics. The source code is available at: https://github.com/LaoDa-X/TFGA-NET.",
    "categories": [
      "cs.SD",
      "cs.AI"
    ],
    "primary_category": "cs.SD",
    "comment": "5 pages, 3 figures",
    "pdf_url": "https://arxiv.org/pdf/2510.12275v1",
    "published_date": "2025-10-14 08:26:50 UTC",
    "updated_date": "2025-10-14 08:26:50 UTC"
  },
  {
    "arxiv_id": "2510.12269v3",
    "title": "Tensor Logic: The Language of AI",
    "authors": [
      "Pedro Domingos"
    ],
    "abstract": "Progress in AI is hindered by the lack of a programming language with all the requisite features. Libraries like PyTorch and TensorFlow provide automatic differentiation and efficient GPU implementation, but are additions to Python, which was never intended for AI. Their lack of support for automated reasoning and knowledge acquisition has led to a long and costly series of hacky attempts to tack them on. On the other hand, AI languages like LISP and Prolog lack scalability and support for learning. This paper proposes tensor logic, a language that solves these problems by unifying neural and symbolic AI at a fundamental level. The sole construct in tensor logic is the tensor equation, based on the observation that logical rules and Einstein summation are essentially the same operation, and all else can be reduced to them. I show how to elegantly implement key forms of neural, symbolic and statistical AI in tensor logic, including transformers, formal reasoning, kernel machines and graphical models. Most importantly, tensor logic makes new directions possible, such as sound reasoning in embedding space. This combines the scalability and learnability of neural networks with the reliability and transparency of symbolic reasoning, and is potentially a basis for the wider adoption of AI.",
    "categories": [
      "cs.AI",
      "cs.LG",
      "cs.NE",
      "cs.PL",
      "stat.ML"
    ],
    "primary_category": "cs.AI",
    "comment": "17 pages, 0 figures",
    "pdf_url": "https://arxiv.org/pdf/2510.12269v3",
    "published_date": "2025-10-14 08:24:08 UTC",
    "updated_date": "2025-10-16 07:40:28 UTC"
  },
  {
    "arxiv_id": "2510.12266v1",
    "title": "HiLoRA: Adaptive Hierarchical LoRA Routing for Training-Free Domain Generalization",
    "authors": [
      "Ziyi Han",
      "Huanyu Wang",
      "Zeyu Zhang",
      "Xiangxiang Dai",
      "Xutong Liu",
      "John C. S. Lui"
    ],
    "abstract": "Low-Rank Adaptation (LoRA) has emerged as a widely used technique for adapting large language models (LLMs) to new domains, due to its modular design and broad availability on platforms such as HuggingFace. This availability has motivated efforts to reuse existing LoRAs for domain generalization.\n  However, existing methods often rely on explicit task labels or additional training, which are impractical for deployment. Moreover, they typically activate a fixed number of entire LoRA modules, leading to parameter redundancy or insufficiency that degrade performance.\n  In this paper, we propose \\texttt{HiLoRA}, a training-free framework that performs adaptive hierarchical routing over LoRA pools. Drawing on structural properties of LoRA, we define rank-one components (ROCs), in which each rank parameter is regarded as an independent unit. For a given input sequence, \\texttt{HiLoRA} first adaptively selects a subset of LoRAs and determines their ROC allocation based on Gaussian likelihoods at the sequence level. At the token level, it further refines routing by activating only the most informative ROCs.\n  We further provide theoretical guarantees that \\texttt{HiLoRA} selects the most relevant LoRAs with high probability.\n  Extensive experiments show that \\texttt{HiLoRA} achieves substantial improvements in domain generalization, with accuracy gains of up to {\\small $55\\%$} over state-of-the-art baselines, while maintaining comparable inference throughput.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.12266v1",
    "published_date": "2025-10-14 08:19:13 UTC",
    "updated_date": "2025-10-14 08:19:13 UTC"
  },
  {
    "arxiv_id": "2510.12265v1",
    "title": "Human-in-the-Loop Bandwidth Estimation for Quality of Experience Optimization in Real-Time Video Communication",
    "authors": [
      "Sami Khairy",
      "Gabriel Mittag",
      "Vishak Gopal",
      "Ross Cutler"
    ],
    "abstract": "The quality of experience (QoE) delivered by video conferencing systems is significantly influenced by accurately estimating the time-varying available bandwidth between the sender and receiver. Bandwidth estimation for real-time communications remains an open challenge due to rapidly evolving network architectures, increasingly complex protocol stacks, and the difficulty of defining QoE metrics that reliably improve user experience. In this work, we propose a deployed, human-in-the-loop, data-driven framework for bandwidth estimation to address these challenges. Our approach begins with training objective QoE reward models derived from subjective user evaluations to measure audio and video quality in real-time video conferencing systems. Subsequently, we collect roughly $1$M network traces with objective QoE rewards from real-world Microsoft Teams calls to curate a bandwidth estimation training dataset. We then introduce a novel distributional offline reinforcement learning (RL) algorithm to train a neural-network-based bandwidth estimator aimed at improving QoE for users. Our real-world A/B test demonstrates that the proposed approach reduces the subjective poor call ratio by $11.41\\%$ compared to the baseline bandwidth estimator. Furthermore, the proposed offline RL algorithm is benchmarked on D4RL tasks to demonstrate its generalization beyond bandwidth estimation.",
    "categories": [
      "cs.MM",
      "cs.AI",
      "cs.NI",
      "eess.SY"
    ],
    "primary_category": "cs.MM",
    "comment": "Accepted for publication in the proceedings of the AAAI Conference on Artificial Intelligence 2026 (IAAI Technical Track on Deployed Highly Innovative Applications of AI)",
    "pdf_url": "https://arxiv.org/pdf/2510.12265v1",
    "published_date": "2025-10-14 08:18:30 UTC",
    "updated_date": "2025-10-14 08:18:30 UTC"
  },
  {
    "arxiv_id": "2510.12264v1",
    "title": "$\\mathbf{T^3}$: Reducing Belief Deviation in Reinforcement Learning for Active Reasoning",
    "authors": [
      "Deyu Zou",
      "Yongqiang Chen",
      "Jianxiang Wang",
      "Haochen Yang",
      "Mufei Li",
      "James Cheng",
      "Pan Li",
      "Yu Gong"
    ],
    "abstract": "Active reasoning requires large language models (LLMs) to interact with external sources and strategically gather information to solve problems. Central to this process is belief tracking: maintaining a coherent understanding of the problem state and the missing information toward the solution. However, due to limited reasoning capabilities, LLM-based agents often suffer from belief deviation: they struggle to correctly model beliefs, lose track of problem states, and fall into uninformative or repetitive actions. Once this happens, errors compound and reinforcement learning (RL) training fails to properly credit the crucial exploratory steps. To address this issue, we propose to track the deviation of model beliefs and develop $\\mathbf{T^3}$, a simple yet effective method that detects excessive belief deviation and truncates trajectories during training to remove uninformative tails. By preserving credit for informative prefixes, $\\mathbf{T^3}$ systematically improves policy optimization. Across 5 challenging tasks, $\\mathbf{T^3}$ consistently enhances training stability, token efficiency, and final performance, achieving up to 30% gains while cutting rollout tokens by roughly 25%. These results highlight belief control as a key principle for developing robust and generalizable LLM-based active reasoners.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.12264v1",
    "published_date": "2025-10-14 08:14:49 UTC",
    "updated_date": "2025-10-14 08:14:49 UTC"
  },
  {
    "arxiv_id": "2510.12255v1",
    "title": "Shallow Robustness, Deep Vulnerabilities: Multi-Turn Evaluation of Medical LLMs",
    "authors": [
      "Blazej Manczak",
      "Eric Lin",
      "Francisco Eiras",
      "James O' Neill",
      "Vaikkunth Mugunthan"
    ],
    "abstract": "Large language models (LLMs) are rapidly transitioning into medical clinical use, yet their reliability under realistic, multi-turn interactions remains poorly understood. Existing evaluation frameworks typically assess single-turn question answering under idealized conditions, overlooking the complexities of medical consultations where conflicting input, misleading context, and authority influence are common. We introduce MedQA-Followup, a framework for systematically evaluating multi-turn robustness in medical question answering. Our approach distinguishes between shallow robustness (resisting misleading initial context) and deep robustness (maintaining accuracy when answers are challenged across turns), while also introducing an indirect-direct axis that separates contextual framing (indirect) from explicit suggestion (direct). Using controlled interventions on the MedQA dataset, we evaluate five state-of-the-art LLMs and find that while models perform reasonably well under shallow perturbations, they exhibit severe vulnerabilities in multi-turn settings, with accuracy dropping from 91.2% to as low as 13.5% for Claude Sonnet 4. Counterintuitively, indirect, context-based interventions are often more harmful than direct suggestions, yielding larger accuracy drops across models and exposing a significant vulnerability for clinical deployment. Further compounding analyses reveal model differences, with some showing additional performance drops under repeated interventions while others partially recovering or even improving. These findings highlight multi-turn robustness as a critical but underexplored dimension for safe and reliable deployment of medical LLMs.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Dataset and code: https://huggingface.co/datasets/dynamoai-ml/MedQA-USMLE-4-MultiTurnRobust ; https://github.com/bmanczak/MedQA-MultiTurnRobustness Accepted as a poster at NeurIPS 2025 Workshop on GenAI for Health: Potential, Trust, and Policy Compliance",
    "pdf_url": "https://arxiv.org/pdf/2510.12255v1",
    "published_date": "2025-10-14 08:04:18 UTC",
    "updated_date": "2025-10-14 08:04:18 UTC"
  },
  {
    "arxiv_id": "2510.12253v1",
    "title": "Diffusion Models for Reinforcement Learning: Foundations, Taxonomy, and Development",
    "authors": [
      "Changfu Xu",
      "Jianxiong Guo",
      "Yuzhu Liang",
      "Haiyang Huang",
      "Haodong Zou",
      "Xi Zheng",
      "Shui Yu",
      "Xiaowen Chu",
      "Jiannong Cao",
      "Tian Wang"
    ],
    "abstract": "Diffusion Models (DMs), as a leading class of generative models, offer key advantages for reinforcement learning (RL), including multi-modal expressiveness, stable training, and trajectory-level planning. This survey delivers a comprehensive and up-to-date synthesis of diffusion-based RL. We first provide an overview of RL, highlighting its challenges, and then introduce the fundamental concepts of DMs, investigating how they are integrated into RL frameworks to address key challenges in this research field. We establish a dual-axis taxonomy that organizes the field along two orthogonal dimensions: a function-oriented taxonomy that clarifies the roles DMs play within the RL pipeline, and a technique-oriented taxonomy that situates implementations across online versus offline learning regimes. We also provide a comprehensive examination of this progression from single-agent to multi-agent domains, thereby forming several frameworks for DM-RL integration and highlighting their practical utility. Furthermore, we outline several categories of successful applications of diffusion-based RL across diverse domains, discuss open research issues of current methodologies, and highlight key directions for future research to advance the field. Finally, we summarize the survey to identify promising future development directions. We are actively maintaining a GitHub repository (https://github.com/ChangfuXu/D4RL-FTD) for papers and other related resources to apply DMs for RL.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "Under Review",
    "pdf_url": "https://arxiv.org/pdf/2510.12253v1",
    "published_date": "2025-10-14 08:03:46 UTC",
    "updated_date": "2025-10-14 08:03:46 UTC"
  },
  {
    "arxiv_id": "2510.12252v2",
    "title": "PromptLocate: Localizing Prompt Injection Attacks",
    "authors": [
      "Yuqi Jia",
      "Yupei Liu",
      "Zedian Shao",
      "Jinyuan Jia",
      "Neil Gong"
    ],
    "abstract": "Prompt injection attacks deceive a large language model into completing an attacker-specified task instead of its intended task by contaminating its input data with an injected prompt, which consists of injected instruction(s) and data. Localizing the injected prompt within contaminated data is crucial for post-attack forensic analysis and data recovery. Despite its growing importance, prompt injection localization remains largely unexplored. In this work, we bridge this gap by proposing PromptLocate, the first method for localizing injected prompts. PromptLocate comprises three steps: (1) splitting the contaminated data into semantically coherent segments, (2) identifying segments contaminated by injected instructions, and (3) pinpointing segments contaminated by injected data. We show PromptLocate accurately localizes injected prompts across eight existing and eight adaptive attacks.",
    "categories": [
      "cs.CR",
      "cs.AI"
    ],
    "primary_category": "cs.CR",
    "comment": "To appear in IEEE Symposium on Security and Privacy, 2026. For slides, see https://people.duke.edu/~zg70/code/PromptInjection.pdf",
    "pdf_url": "https://arxiv.org/pdf/2510.12252v2",
    "published_date": "2025-10-14 08:02:11 UTC",
    "updated_date": "2025-10-17 02:10:07 UTC"
  },
  {
    "arxiv_id": "2510.12246v1",
    "title": "PromptFlow: Training Prompts Like Neural Networks",
    "authors": [
      "Jingyi Wang",
      "Hongyuan Zhu",
      "Ye Niu",
      "Yunhui Deng"
    ],
    "abstract": "Large Language Models (LLMs) have demonstrated profound impact on Natural Language Processing (NLP) tasks. However, their effective deployment across diverse domains often require domain-specific adaptation strategies, as generic models may underperform when faced with specialized data distributions. Recent advances in prompt engineering (PE) offer a promising alternative to extensive retraining by refining input instructions to align LLM outputs with task objectives. This paradigm has emerged as a rapid and versatile approach for model fine-tuning. Despite its potential, manual prompt design remains labor-intensive and heavily depends on specialized expertise, often requiring iterative human effort to achieve optimal formulations. To address this limitation, automated prompt engineering methodologies have been developed to systematically generate task-specific prompts. However, current implementations predominantly employ static update rules and lack mechanisms for dynamic strategy selection, resulting in suboptimal adaptation to varying NLP task requirements. Furthermore, most methods treat and update the whole prompts at each step, without considering editing prompt sections at a finer granularity. At last, in particular, the problem of how to recycle experience in LLM is still underexplored. To this end, we propose the PromptFlow, a modular training framework inspired by TensorFlow, which integrates meta-prompts, operators, optimization, and evaluator. Our framework can be equipped with the latest optimization methods and autonomously explores optimal prompt refinement trajectories through gradient-based meta-learning, requiring minimal task-specific training data. Specifically, we devise a reinforcement learning method to recycle experience for LLM in the PE process. Finally, we conduct extensive experiments on various datasets, and demonstrate the effectiveness of PromptFlow.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "Comments: 18 pages, 14 figures, conference submission, appendix included",
    "pdf_url": "https://arxiv.org/pdf/2510.12246v1",
    "published_date": "2025-10-14 07:56:12 UTC",
    "updated_date": "2025-10-14 07:56:12 UTC"
  },
  {
    "arxiv_id": "2510.12245v1",
    "title": "MoRA: On-the-fly Molecule-aware Low-Rank Adaptation Framework for LLM-based Multi-Modal Molecular Assistant",
    "authors": [
      "Tao Yin",
      "Xiaohong Zhang",
      "Jiacheng Zhang",
      "Li Huang",
      "Zhibin Zhang",
      "Yuansong Zeng",
      "Jin Xie",
      "Meng Yan"
    ],
    "abstract": "Effectively integrating molecular graph structures with Large Language Models (LLMs) is a key challenge in drug discovery. Most existing multi-modal alignment methods typically process these structures by fine-tuning the LLM or adding a static adapter simultaneously. However, these approaches have two main limitations: (1) it optimizes a shared parameter space across all molecular inputs, limiting the model's ability to capture instance-specific structural features; and (2) fine-tuning the LLM for molecular tasks can lead to catastrophic forgetting, undermining its general reasoning capabilities. In this paper, instead of static task-oriented adaptation, we propose an instance-specific parameter space alignment approach for each molecule on-the-fly. To this end, we introduce Molecule-aware Low-Rank Adaptation (MoRA) that produces a unique set of low-rank adaptation weights for each input molecular graph. These weights are then dynamically injected into a frozen LLM, allowing the model to adapt its reasoning to the structure of each molecular input, while preserving the LLM's core knowledge. Extensive experiments demonstrate that on key molecular tasks, such as chemical reaction prediction and molecular captioning, MoRA's instance-specific dynamic adaptation outperforms statically adapted baselines, including a 14.1% relative improvement in reaction prediction exact match and a 22% reduction in error for quantum property prediction. The code is available at https://github.com/jk-sounds/MoRA.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.12245v1",
    "published_date": "2025-10-14 07:54:43 UTC",
    "updated_date": "2025-10-14 07:54:43 UTC"
  },
  {
    "arxiv_id": "2510.15994v1",
    "title": "MCP Security Bench (MSB): Benchmarking Attacks Against Model Context Protocol in LLM Agents",
    "authors": [
      "Dongsen Zhang",
      "Zekun Li",
      "Xu Luo",
      "Xuannan Liu",
      "Peipei Li",
      "Wenjun Xu"
    ],
    "abstract": "The Model Context Protocol (MCP) standardizes how large language model (LLM) agents discover, describe, and call external tools. While MCP unlocks broad interoperability, it also enlarges the attack surface by making tools first-class, composable objects with natural-language metadata, and standardized I/O. We present MSB (MCP Security Benchmark), the first end-to-end evaluation suite that systematically measures how well LLM agents resist MCP-specific attacks throughout the full tool-use pipeline: task planning, tool invocation, and response handling. MSB contributes: (1) a taxonomy of 12 attacks including name-collision, preference manipulation, prompt injections embedded in tool descriptions, out-of-scope parameter requests, user-impersonating responses, false-error escalation, tool-transfer, retrieval injection, and mixed attacks; (2) an evaluation harness that executes attacks by running real tools (both benign and malicious) via MCP rather than simulation; and (3) a robustness metric that quantifies the trade-off between security and performance: Net Resilient Performance (NRP). We evaluate nine popular LLM agents across 10 domains and 400+ tools, producing 2,000 attack instances. Results reveal the effectiveness of attacks against each stage of MCP. Models with stronger performance are more vulnerable to attacks due to their outstanding tool calling and instruction following capabilities. MSB provides a practical baseline for researchers and practitioners to study, compare, and harden MCP agents.",
    "categories": [
      "cs.CR",
      "cs.AI"
    ],
    "primary_category": "cs.CR",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.15994v1",
    "published_date": "2025-10-14 07:36:25 UTC",
    "updated_date": "2025-10-14 07:36:25 UTC"
  },
  {
    "arxiv_id": "2510.12229v2",
    "title": "Analysing Moral Bias in Finetuned LLMs through Mechanistic Interpretability",
    "authors": [
      "Bianca Raimondi",
      "Daniela Dalbagno",
      "Maurizio Gabbrielli"
    ],
    "abstract": "Large language models (LLMs) have been shown to internalize human-like biases during finetuning, yet the mechanisms by which these biases manifest remain unclear. In this work, we investigated whether the well-known Knobe effect, a moral bias in intentionality judgements, emerges in finetuned LLMs and whether it can be traced back to specific components of the model. We conducted a Layer-Patching analysis across 3 open-weights LLMs and demonstrated that the bias is not only learned during finetuning but also localized in a specific set of layers. Surprisingly, we found that patching activations from the corresponding pretrained model into just a few critical layers is sufficient to eliminate the effect. Our findings offer new evidence that social biases in LLMs can be interpreted, localized, and mitigated through targeted interventions, without the need for model retraining.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Preprint. Under review",
    "pdf_url": "https://arxiv.org/pdf/2510.12229v2",
    "published_date": "2025-10-14 07:31:29 UTC",
    "updated_date": "2025-12-05 18:32:06 UTC"
  },
  {
    "arxiv_id": "2510.12224v1",
    "title": "MedKGEval: A Knowledge Graph-Based Multi-Turn Evaluation Framework for Open-Ended Patient Interactions with Clinical LLMs",
    "authors": [
      "Yuechun Yu",
      "Han Ying",
      "Haoan Jin",
      "Wenjian Jiang",
      "Dong Xian",
      "Binghao Wang",
      "Zhou Yang",
      "Mengyue Wu"
    ],
    "abstract": "The reliable evaluation of large language models (LLMs) in medical applications remains an open challenge, particularly in capturing the complexity of multi-turn doctor-patient interactions that unfold in real clinical environments. Existing evaluation methods typically rely on post hoc review of full conversation transcripts, thereby neglecting the dynamic, context-sensitive nature of medical dialogues and the evolving informational needs of patients. In this work, we present MedKGEval, a novel multi-turn evaluation framework for clinical LLMs grounded in structured medical knowledge. Our approach introduces three key contributions: (1) a knowledge graph-driven patient simulation mechanism, where a dedicated control module retrieves relevant medical facts from a curated knowledge graph, thereby endowing the patient agent with human-like and realistic conversational behavior. This knowledge graph is constructed by integrating open-source resources with additional triples extracted from expert-annotated datasets; (2) an in-situ, turn-level evaluation framework, where each model response is assessed by a Judge Agent for clinical appropriateness, factual correctness, and safety as the dialogue progresses using a suite of fine-grained, task-specific metrics; (3) a comprehensive multi-turn benchmark of eight state-of-the-art LLMs, demonstrating MedKGEval's ability to identify subtle behavioral flaws and safety risks that are often overlooked by conventional evaluation pipelines. Although initially designed for Chinese and English medical applications, our framework can be readily extended to additional languages by switching the input knowledge graphs, ensuring seamless bilingual support and domain-specific applicability.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.12224v1",
    "published_date": "2025-10-14 07:22:26 UTC",
    "updated_date": "2025-10-14 07:22:26 UTC"
  },
  {
    "arxiv_id": "2510.12218v1",
    "title": "GOAT: A Training Framework for Goal-Oriented Agent with Tools",
    "authors": [
      "Hyunji Min",
      "Sangwon Jung",
      "Junyoung Sung",
      "Dosung Lee",
      "Leekyeung Han",
      "Paul Hongsuck Seo"
    ],
    "abstract": "Large language models (LLMs) have recently been extended beyond traditional text generation to serve as interactive agents capable of using external tools based on user intent. However, current LLM agents still show limited ability to handle goal-oriented queries, which require decomposing a high-level objective into multiple interdependent API calls with correct planning and execution. Current approaches mainly rely on zero-shot evaluation due to the absence of training data. While proprietary closed-source models such as GPT-4 demonstrate strong reasoning abilities, smaller open-source models struggle to perform complex tool use effectively. Thus, we propose a novel training framework GOAT, which enables fine-tuning of LLM agents in a human annotation-free setting. GOAT automatically constructs synthetic datasets of goal-oriented API execution tasks directly from given API documents, equipping models with the ability to reason over interdependent calls and generate coherent responses. Through extensive experiments, we show that GOAT-trained agents achieve state-of-the-art performance across multiple existing goal-oriented benchmarks. In addition, we introduce GOATBench, a new goal-oriented API execution benchmark, and demonstrate that agents trained with GOAT also excel in this setting. These results highlight GOAT as a practical path toward building robust open-source LLM agents capable of complex reasoning and tool use.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "32 pages, 21 figures",
    "pdf_url": "https://arxiv.org/pdf/2510.12218v1",
    "published_date": "2025-10-14 07:14:50 UTC",
    "updated_date": "2025-10-14 07:14:50 UTC"
  },
  {
    "arxiv_id": "2510.12217v2",
    "title": "HALF: Harm-Aware LLM Fairness Evaluation Aligned with Deployment",
    "authors": [
      "Ali Mekky",
      "Omar El Herraoui",
      "Preslav Nakov",
      "Yuxia Wang"
    ],
    "abstract": "Large language models (LLMs) are increasingly deployed across high-impact domains, from clinical decision support and legal analysis to hiring and education, making fairness and bias evaluation before deployment critical. However, existing evaluations lack grounding in real-world scenarios and do not account for differences in harm severity, e.g., a biased decision in surgery should not be weighed the same as a stylistic bias in text summarization. To address this gap, we introduce HALF (Harm-Aware LLM Fairness), a deployment-aligned framework that assesses model bias in realistic applications and weighs the outcomes by harm severity. HALF organizes nine application domains into three tiers (Severe, Moderate, Mild) using a five-stage pipeline. Our evaluation results across eight LLMs show that (1) LLMs are not consistently fair across domains, (2) model size or performance do not guarantee fairness, and (3) reasoning models perform better in medical decision support but worse in education. We conclude that HALF exposes a clear gap between previous benchmarking success and deployment readiness.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.12217v2",
    "published_date": "2025-10-14 07:13:26 UTC",
    "updated_date": "2025-10-16 08:43:05 UTC"
  },
  {
    "arxiv_id": "2510.12214v2",
    "title": "DE3S: Dual-Enhanced Soft-Sparse-Shape Learning for Medical Early Time-Series Classification",
    "authors": [
      "Tao Xie",
      "Zexi Tan",
      "Haoyi Xiao",
      "Binbin Sun",
      "Yiqun Zhang"
    ],
    "abstract": "Early Time Series Classification (ETSC) is critical in time-sensitive medical applications such as sepsis, yet it presents an inherent trade-off between accuracy and earliness. This trade-off arises from two core challenges: 1) models should effectively model inherently weak and noisy early-stage snippets, and 2) they should resolve the complex, dual requirement of simultaneously capturing local, subject-specific variations and overarching global temporal patterns. Existing methods struggle to overcome these underlying challenges, often forcing a severe compromise: sacrificing accuracy to achieve earliness, or vice-versa. We propose \\textbf{DE3S}, a \\textbf{D}ual-\\textbf{E}nhanced \\textbf{S}oft-\\textbf{S}parse \\textbf{S}equence Learning framework, which systematically solves these challenges. A dual enhancement mechanism is proposed to enhance the modeling of weak, early signals. Then, an attention-based patch module is introduced to preserve discriminative information while reducing noise and complexity. A dual-path fusion architecture is designed, using a sparse mixture of experts to model local, subject-specific variations. A multi-scale inception module is also employed to capture global dependencies. Experiments on six real-world medical datasets show the competitive performance of DE3S, particularly in early prediction windows. Ablation studies confirm the effectiveness of each component in addressing its targeted challenge. The source code is available \\href{https://github.com/kuxit/DE3S}{\\textbf{here}}.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted to IEEE BIBM 2025",
    "pdf_url": "https://arxiv.org/pdf/2510.12214v2",
    "published_date": "2025-10-14 07:10:05 UTC",
    "updated_date": "2025-11-05 14:23:32 UTC"
  },
  {
    "arxiv_id": "2510.12209v1",
    "title": "Revisiting Meta-Learning with Noisy Labels: Reweighting Dynamics and Theoretical Guarantees",
    "authors": [
      "Yiming Zhang",
      "Chester Holtz",
      "Gal Mishne",
      "Alex Cloninger"
    ],
    "abstract": "Learning with noisy labels remains challenging because over-parameterized networks memorize corrupted supervision. Meta-learning-based sample reweighting mitigates this by using a small clean subset to guide training, yet its behavior and training dynamics lack theoretical understanding. We provide a rigorous theoretical analysis of meta-reweighting under label noise and show that its training trajectory unfolds in three phases: (i) an alignment phase that amplifies examples consistent with a clean subset and suppresses conflicting ones; (ii) a filtering phase driving noisy example weights toward zero until the clean subset loss plateaus; and (iii) a post-filtering phase in which noise filtration becomes perturbation-sensitive. The mechanism is a similarity-weighted coupling between training and clean subset signals together with clean subset training loss contraction; in the post-filtering regime where the clean-subset loss is sufficiently small, the coupling term vanishes and meta-reweighting loses discriminatory power. Guided by this analysis, we propose a lightweight surrogate for meta-reweighting that integrates mean-centering, row shifting, and label-signed modulation, yielding more stable performance while avoiding expensive bi-level optimization. Across synthetic and real noisy-label benchmarks, our method consistently outperforms strong reweighting/selection baselines.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.12209v1",
    "published_date": "2025-10-14 07:00:12 UTC",
    "updated_date": "2025-10-14 07:00:12 UTC"
  },
  {
    "arxiv_id": "2510.12201v1",
    "title": "On the Design and Evaluation of Human-centered Explainable AI Systems: A Systematic Review and Taxonomy",
    "authors": [
      "Aline Mangold",
      "Juliane Zietz",
      "Susanne Weinhold",
      "Sebastian Pannasch"
    ],
    "abstract": "As AI becomes more common in everyday living, there is an increasing demand for intelligent systems that are both performant and understandable. Explainable AI (XAI) systems aim to provide comprehensible explanations of decisions and predictions. At present, however, evaluation processes are rather technical and not sufficiently focused on the needs of human users. Consequently, evaluation studies involving human users can serve as a valuable guide for conducting user studies. This paper presents a comprehensive review of 65 user studies evaluating XAI systems across different domains and application contexts. As a guideline for XAI developers, we provide a holistic overview of the properties of XAI systems and evaluation metrics focused on human users (human-centered). We propose objectives for the human-centered design (design goals) of XAI systems. To incorporate users' specific characteristics, design goals are adapted to users with different levels of AI expertise (AI novices and data experts). In this regard, we provide an extension to existing XAI evaluation and design frameworks. The first part of our results includes the analysis of XAI system characteristics. An important finding is the distinction between the core system and the XAI explanation, which together form the whole system. Further results include the distinction of evaluation metrics into affection towards the system, cognition, usability, interpretability, and explanation metrics. Furthermore, the users, along with their specific characteristics and behavior, can be assessed. For AI novices, the relevant extended design goals include responsible use, acceptance, and usability. For data experts, the focus is performance-oriented and includes human-AI collaboration and system and user task performance.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.12201v1",
    "published_date": "2025-10-14 06:52:43 UTC",
    "updated_date": "2025-10-14 06:52:43 UTC"
  },
  {
    "arxiv_id": "2510.12194v2",
    "title": "ResearStudio: A Human-Intervenable Framework for Building Controllable Deep-Research Agents",
    "authors": [
      "Linyi Yang",
      "Yixuan Weng"
    ],
    "abstract": "Current deep-research agents run in a ''fire-and-forget'' mode: once started, they give users no way to fix errors or add expert knowledge during execution. We present ResearStudio, the first open-source framework that places real-time human control at its core. The system follows a Collaborative Workshop design. A hierarchical Planner-Executor writes every step to a live ''plan-as-document,'' a fast communication layer streams each action, file change, and tool call to a web interface. At any moment, the user can pause the run, edit the plan or code, run custom commands, and resume -- switching smoothly between AI-led, human-assisted and human-led, AI-assisted modes. In fully autonomous mode, ResearStudio achieves state-of-the-art results on the GAIA benchmark, surpassing systems like OpenAI's DeepResearch and Manus. These results show that strong automated performance and fine-grained human control can coexist. The full code, protocol, and evaluation scripts are available at https://github.com/ResearAI/ResearStudio. We will continue to update the repository to encourage further work on safe and controllable research agents. Our live demo is publicly accessible at http://ai-researcher.net:3000/. We support the development of DeepScientist, which can be accessed at https://github.com/ResearAI/DeepScientist.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "EMNLP 2025 Demo, Oral",
    "pdf_url": "https://arxiv.org/pdf/2510.12194v2",
    "published_date": "2025-10-14 06:40:11 UTC",
    "updated_date": "2025-11-21 05:43:16 UTC"
  },
  {
    "arxiv_id": "2510.12184v1",
    "title": "CompoDistill: Attention Distillation for Compositional Reasoning in Multimodal LLMs",
    "authors": [
      "Jiwan Kim",
      "Kibum Kim",
      "Sangwoo Seo",
      "Chanyoung Park"
    ],
    "abstract": "Recently, efficient Multimodal Large Language Models (MLLMs) have gained significant attention as a solution to their high computational complexity, making them more practical for real-world applications. In this regard, the knowledge distillation (KD) approach has emerged as a promising alternative, which transfers the rich visual and linguistic knowledge from a larger model (teacher) to a smaller model (student). However, we observe that existing KD methods struggle to effectively distill the teacher MLLM's rich visual perception abilities to the student, a challenge that has been largely overlooked in previous studies. Through a systematic analysis, we identify visual attention misalignment between student and teacher as the main cause of this issue. Based on this insight, we propose CompoDistill, a novel KD framework that explicitly aligns the student's visual attention with that of the teacher to enhance the student's visual perception abilities. Our extensive experiments show that CompoDistill significantly improves performance on compositional reasoning tasks that require visual perception abilities while maintaining strong performance on visual question answering tasks, as done in existing studies. Furthermore, CompoDistill demonstrates effectiveness with a more advanced backbone, highlighting its generalizability.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "Preprint. Under Review",
    "pdf_url": "https://arxiv.org/pdf/2510.12184v1",
    "published_date": "2025-10-14 06:27:26 UTC",
    "updated_date": "2025-10-14 06:27:26 UTC"
  },
  {
    "arxiv_id": "2510.13891v1",
    "title": "K-frames: Scene-Driven Any-k Keyframe Selection for long video understanding",
    "authors": [
      "Yifeng Yao",
      "Yike Yun",
      "Jing Wang",
      "Huishuai Zhang",
      "Dongyan Zhao",
      "Ke Tian",
      "Zhihao Wang",
      "Minghui Qiu",
      "Tao Wang"
    ],
    "abstract": "Multimodal Large Language Models (MLLMs) have demonstrated significant capabilities in image understanding, but long-video are constrained by context windows and computational cost. Uniform frame sampling often leads to substantial information loss. Meanwhile existing keyframe selection methods such as text-frame retrieval or RL-based frame optimization typically yield sparse and temporally disjointed frames, overlooking scene continuity and lacking flexibility for multi-scale frame selection. To address these limitations, we introduce K-frames, a novel paradigm for scene-driven keyframe selection that preserves temporal continuity. Instead of selecting individual frames, K-frames predicts semantically coherent, query-relevant clips, which enables any-k keyframes selection to meet diverse user budgets. To achieve this approach, we first introduce PeakClips, a dataset of 200K video highlights conditioned by query. Building on this dataset, K-frames learns clip2frame selection using a three-stage progressive curriculum. It involves two Supervised Fine-Tuning stages for temporal grounding and key-clip perception, followed by a Reinforcement Learning stage that directly optimizes the scene-driven prediction policy for downstream task without further annotations. Extensive experiments on major long-video understanding benchmarks demonstrate that K-frames provides an effective, interpretable, and plug-and-play solution for keyframe selection at various scales. Our dataset and model will be available.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.13891v1",
    "published_date": "2025-10-14 06:23:22 UTC",
    "updated_date": "2025-10-14 06:23:22 UTC"
  },
  {
    "arxiv_id": "2510.12181v1",
    "title": "From Knowledge to Treatment: Large Language Model Assisted Biomedical Concept Representation for Drug Repurposing",
    "authors": [
      "Chengrui Xiang",
      "Tengfei Ma",
      "Xiangzheng Fu",
      "Yiping Liu",
      "Bosheng Song",
      "Xiangxiang Zeng"
    ],
    "abstract": "Drug repurposing plays a critical role in accelerating treatment discovery, especially for complex and rare diseases. Biomedical knowledge graphs (KGs), which encode rich clinical associations, have been widely adopted to support this task. However, existing methods largely overlook common-sense biomedical concept knowledge in real-world labs, such as mechanistic priors indicating that certain drugs are fundamentally incompatible with specific treatments. To address this gap, we propose LLaDR, a Large Language Model-assisted framework for Drug Repurposing, which improves the representation of biomedical concepts within KGs. Specifically, we extract semantically enriched treatment-related textual representations of biomedical entities from large language models (LLMs) and use them to fine-tune knowledge graph embedding (KGE) models. By injecting treatment-relevant knowledge into KGE, LLaDR largely improves the representation of biomedical concepts, enhancing semantic understanding of under-studied or complex indications. Experiments based on benchmarks demonstrate that LLaDR achieves state-of-the-art performance across different scenarios, with case studies on Alzheimer's disease further confirming its robustness and effectiveness. Code is available at https://github.com/xiaomingaaa/LLaDR.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "16 pages, 4 figures, 13 tables. Accepted by EMNLP 2025 (Findings)",
    "pdf_url": "https://arxiv.org/pdf/2510.12181v1",
    "published_date": "2025-10-14 06:15:36 UTC",
    "updated_date": "2025-10-14 06:15:36 UTC"
  },
  {
    "arxiv_id": "2510.12178v1",
    "title": "Evolution of meta's llama models and parameter-efficient fine-tuning of large language models: a survey",
    "authors": [
      "Abdulhady Abas Abdullah",
      "Arkaitz Zubiaga",
      "Seyedali Mirjalili",
      "Amir H. Gandomi",
      "Fatemeh Daneshfar",
      "Mohammadsadra Amini",
      "Alan Salam Mohammed",
      "Hadi Veisi"
    ],
    "abstract": "This review surveys the rapid evolution of Meta AI's LLaMA (Large Language Model Meta AI) series - from LLaMA 1 through LLaMA 4 and the specialized parameter-efficient fine-tuning (PEFT) methods developed for these models. We first describe the LLaMA family of foundation models (7B-65B to 288B parameters), their architectures (including native multimodal and Mixtureof-Experts variants), and key performance characteristics. We then describe and discuss the concept of PEFT, which adapts large pre-trained models by updating only a small subset of parameters, and review five PEFT methods that have been applied to LLaMA: LoRA (Low-Rank Adaptation), LLaMA-Adapter V1 and V2, LLaMA-Excitor, and QLoRA (Quantized LoRA). We discuss each method's mechanism, parameter savings, and example application to LLaMA (e.g., instruction tuning, multimodal tasks). We provide structured discussion and analysis of model and adapter architectures, parameter counts, and benchmark results (including examples where fine-tuned LLaMA models outperform larger baselines). Finally, we examine real-world use cases where LLaMA-based models and PEFT have been successfully applied (e.g., legal and medical domains), and we discuss ongoing challenges and future research directions (such as scaling to even larger contexts and improving robustness). This survey paper provides a one-stop resource for ML researchers and practitioners interested in LLaMA models and efficient fine-tuning strategies.",
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.12178v1",
    "published_date": "2025-10-14 06:12:44 UTC",
    "updated_date": "2025-10-14 06:12:44 UTC"
  },
  {
    "arxiv_id": "2510.12171v1",
    "title": "MatSciBench: Benchmarking the Reasoning Ability of Large Language Models in Materials Science",
    "authors": [
      "Junkai Zhang",
      "Jingru Gan",
      "Xiaoxuan Wang",
      "Zian Jia",
      "Changquan Gu",
      "Jianpeng Chen",
      "Yanqiao Zhu",
      "Mingyu Derek Ma",
      "Dawei Zhou",
      "Ling Li",
      "Wei Wang"
    ],
    "abstract": "Large Language Models (LLMs) have demonstrated remarkable abilities in scientific reasoning, yet their reasoning capabilities in materials science remain underexplored. To fill this gap, we introduce MatSciBench, a comprehensive college-level benchmark comprising 1,340 problems that span the essential subdisciplines of materials science. MatSciBench features a structured and fine-grained taxonomy that categorizes materials science questions into 6 primary fields and 31 sub-fields, and includes a three-tier difficulty classification based on the reasoning length required to solve each question. MatSciBench provides detailed reference solutions enabling precise error analysis and incorporates multimodal reasoning through visual contexts in numerous questions. Evaluations of leading models reveal that even the highest-performing model, Gemini-2.5-Pro, achieves under 80% accuracy on college-level materials science questions, highlighting the complexity of MatSciBench. Our systematic analysis of different reasoning strategie--basic chain-of-thought, tool augmentation, and self-correction--demonstrates that no single method consistently excels across all scenarios. We further analyze performance by difficulty level, examine trade-offs between efficiency and accuracy, highlight the challenges inherent in multimodal reasoning tasks, analyze failure modes across LLMs and reasoning methods, and evaluate the influence of retrieval-augmented generation. MatSciBench thus establishes a comprehensive and solid benchmark for assessing and driving improvements in the scientific reasoning capabilities of LLMs within the materials science domain.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.12171v1",
    "published_date": "2025-10-14 05:59:40 UTC",
    "updated_date": "2025-10-14 05:59:40 UTC"
  },
  {
    "arxiv_id": "2510.12144v1",
    "title": "Budget-constrained Active Learning to Effectively De-censor Survival Data",
    "authors": [
      "Ali Parsaee",
      "Bei Jiang",
      "Zachary Friggstad",
      "Russell Greiner"
    ],
    "abstract": "Standard supervised learners attempt to learn a model from a labeled dataset. Given a small set of labeled instances, and a pool of unlabeled instances, a budgeted learner can use its given budget to pay to acquire the labels of some unlabeled instances, which it can then use to produce a model. Here, we explore budgeted learning in the context of survival datasets, which include (right) censored instances, where we know only a lower bound on an instance's time-to-event. Here, that learner can pay to (partially) label a censored instance -- e.g., to acquire the actual time for an instance [perhaps go from (3 yr, censored) to (7.2 yr, uncensored)], or other variants [e.g., learn about one more year, so go from (3 yr, censored) to either (4 yr, censored) or perhaps (3.2 yr, uncensored)]. This serves as a model of real world data collection, where follow-up with censored patients does not always lead to uncensoring, and how much information is given to the learner model during data collection is a function of the budget and the nature of the data itself. We provide both experimental and theoretical results for how to apply state-of-the-art budgeted learning algorithms to survival data and the respective limitations that exist in doing so. Our approach provides bounds and time complexity asymptotically equivalent to the standard active learning method BatchBALD. Moreover, empirical analysis on several survival tasks show that our model performs better than other potential approaches on several benchmarks.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.12144v1",
    "published_date": "2025-10-14 04:53:30 UTC",
    "updated_date": "2025-10-14 04:53:30 UTC"
  },
  {
    "arxiv_id": "2510.12137v1",
    "title": "Credal Transformer: A Principled Approach for Quantifying and Mitigating Hallucinations in Large Language Models",
    "authors": [
      "Shihao Ji",
      "Zihui Song",
      "Jiajie Huang"
    ],
    "abstract": "Large Language Models (LLMs) hallucinate, generating factually incorrect yet confident assertions. We argue this stems from the Transformer's Softmax function, which creates \"Artificial Certainty\" by collapsing ambiguous attention scores into a single probability distribution, discarding uncertainty information at each layer. To fix this, we introduce the Credal Transformer, which replaces standard attention with a Credal Attention Mechanism (CAM) based on evidential theory. CAM produces a \"credal set\" (a set of distributions) instead of a single attention vector, with the set's size directly measuring model uncertainty. We implement this by re-conceptualizing attention scores as evidence masses for a Dirichlet distribution: sufficient evidence recovers standard attention, while insufficient evidence yields a diffuse distribution, representing ambiguity. Empirically, the Credal Transformer identifies out-of-distribution inputs, quantifies ambiguity, and significantly reduces confident errors on unanswerable questions by abstaining. Our contribution is a new architecture to mitigate hallucinations and a design paradigm that integrates uncertainty quantification directly into the model, providing a foundation for more reliable AI.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.12137v1",
    "published_date": "2025-10-14 04:31:49 UTC",
    "updated_date": "2025-10-14 04:31:49 UTC"
  },
  {
    "arxiv_id": "2510.12133v1",
    "title": "SafeMT: Multi-turn Safety for Multimodal Language Models",
    "authors": [
      "Han Zhu",
      "Juntao Dai",
      "Jiaming Ji",
      "Haoran Li",
      "Chengkun Cai",
      "Pengcheng Wen",
      "Chi-Min Chan",
      "Boyuan Chen",
      "Yaodong Yang",
      "Sirui Han",
      "Yike Guo"
    ],
    "abstract": "With the widespread use of multi-modal Large Language models (MLLMs), safety issues have become a growing concern. Multi-turn dialogues, which are more common in everyday interactions, pose a greater risk than single prompts; however, existing benchmarks do not adequately consider this situation. To encourage the community to focus on the safety issues of these models in multi-turn dialogues, we introduce SafeMT, a benchmark that features dialogues of varying lengths generated from harmful queries accompanied by images. This benchmark consists of 10,000 samples in total, encompassing 17 different scenarios and four jailbreak methods. Additionally, we propose Safety Index (SI) to evaluate the general safety of MLLMs during conversations. We assess the safety of 17 models using this benchmark and discover that the risk of successful attacks on these models increases as the number of turns in harmful dialogues rises. This observation indicates that the safety mechanisms of these models are inadequate for recognizing the hazard in dialogue interactions. We propose a dialogue safety moderator capable of detecting malicious intent concealed within conversations and providing MLLMs with relevant safety policies. Experimental results from several open-source models indicate that this moderator is more effective in reducing multi-turn ASR compared to existed guard models.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.12133v1",
    "published_date": "2025-10-14 04:24:07 UTC",
    "updated_date": "2025-10-14 04:24:07 UTC"
  },
  {
    "arxiv_id": "2510.13890v2",
    "title": "A Survey on Collaborating Small and Large Language Models for Performance, Cost-effectiveness, Cloud-edge Privacy, and Trustworthiness",
    "authors": [
      "Fali Wang",
      "Jihai Chen",
      "Shuhua Yang",
      "Ali Al-Lawati",
      "Linli Tang",
      "Hui Liu",
      "Suhang Wang"
    ],
    "abstract": "Large language models (LLMs) have achieved remarkable progress across domains and applications but face challenges such as high fine-tuning costs, inference latency, limited edge deployability, and reliability concerns. Small language models (SLMs), with compact, efficient, and adaptable features, offer promising solutions. Building on this potential, recent research explores collaborative frameworks that integrate their complementary strengths, leveraging SLMs' specialization and efficiency with LLMs' generalization and reasoning to address diverse objectives across tasks and deployment scenarios. Motivated by these developments, this paper presents a systematic survey of SLM-LLM collaboration from the perspective of collaboration objectives. We propose a taxonomy covering four goals: performance enhancement, cost-effectiveness, cloud-edge privacy, and trustworthiness. Under this framework, we review representative methods, summarize design paradigms, and outline open challenges and future directions toward efficient and secure SLM-LLM collaboration. The collected papers are available at https://github.com/FairyFali/SLMs-Survey.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "24 pages, 19 figures-under review; more detailed than v1",
    "pdf_url": "https://arxiv.org/pdf/2510.13890v2",
    "published_date": "2025-10-14 04:16:47 UTC",
    "updated_date": "2025-11-05 10:30:09 UTC"
  },
  {
    "arxiv_id": "2510.12121v1",
    "title": "Precise Attribute Intensity Control in Large Language Models via Targeted Representation Editing",
    "authors": [
      "Rongzhi Zhang",
      "Liqin Ye",
      "Yuzhao Heng",
      "Xiang Chen",
      "Tong Yu",
      "Lingkai Kong",
      "Sudheer Chava",
      "Chao Zhang"
    ],
    "abstract": "Precise attribute intensity control--generating Large Language Model (LLM) outputs with specific, user-defined attribute intensities--is crucial for AI systems adaptable to diverse user expectations. Current LLM alignment methods, however, typically provide only directional or open-ended guidance, failing to reliably achieve exact attribute intensities. We address this limitation with three key designs: (1) reformulating precise attribute intensity control as a target-reaching problem, rather than simple maximization; (2) training a lightweight value function via temporal-difference learning to predict final attribute intensity scores from partial generations, thereby steering LLM outputs; and (3) employing gradient-based interventions on hidden representations to navigate the model precisely towards specific attribute intensity targets. Our method enables fine-grained, continuous control over attribute intensities, moving beyond simple directional alignment. Experiments on LLaMA-3.2-3b and Phi-4-mini confirm our method's ability to steer text generation to user-specified attribute intensities with high accuracy. Finally, we demonstrate efficiency enhancements across three downstream tasks: preference data synthesis, Pareto frontier approximation and optimization, and distillation of aligned behaviors for intervention-free inference. Our code is available on https://github.com/Pre-Control/pre-control",
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.12121v1",
    "published_date": "2025-10-14 03:50:22 UTC",
    "updated_date": "2025-10-14 03:50:22 UTC"
  },
  {
    "arxiv_id": "2510.12116v1",
    "title": "Understanding the Modality Gap: An Empirical Study on the Speech-Text Alignment Mechanism of Large Speech Language Models",
    "authors": [
      "Bajian Xiang",
      "Shuaijiang Zhao",
      "Tingwei Guo",
      "Wei Zou"
    ],
    "abstract": "End-to-end Large Speech Language Models (LSLMs) have demonstrated impressive conversational generation abilities, yet consistently fall short of traditional pipeline systems on semantic understanding benchmarks. In this work, we reveal through systematic experimentation that although LSLMs lose some text input performance after speech-text alignment training, the performance gap between speech and text inputs is more pronounced, which we refer to as the modality gap. To understand this gap, we analyze both coarse- and fine-grained text and speech representations. At the coarse-grained level, representations of speech and text in deeper layers are found to be increasingly aligned in direction (cosine similarity), while concurrently diverging in magnitude (Euclidean distance). We further find that representation similarity is strongly correlated with the modality gap. At the fine-grained level, a spontaneous token-level alignment pattern between text and speech representations is observed. Based on this, we introduce the Alignment Path Score to quantify token-level alignment quality, which exhibits stronger correlation with the modality gap. Building on these insights, we design targeted interventions on critical tokens through angle projection and length normalization. These strategies demonstrate the potential to improve correctness for speech inputs. Our study provides the first systematic empirical analysis of the modality gap and alignment mechanisms in LSLMs, offering both theoretical and methodological guidance for future optimization.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted to EMNLP 2025 (Main Conference)",
    "pdf_url": "https://arxiv.org/pdf/2510.12116v1",
    "published_date": "2025-10-14 03:34:38 UTC",
    "updated_date": "2025-10-14 03:34:38 UTC"
  },
  {
    "arxiv_id": "2510.12111v1",
    "title": "Chimera: State Space Models Beyond Sequences",
    "authors": [
      "Aakash Lahoti",
      "Tanya Marwah",
      "Ratish Puduppully",
      "Albert Gu"
    ],
    "abstract": "Transformer-based deep learning methods have become the standard approach for modeling diverse data such as sequences, images, and graphs. These methods rely on self-attention, which treats data as an unordered set of elements. This ignores the neighborhood structure or graph topology of the data and requires inductive biases--such as position embeddings in sequences and images, or random walks in graphs--to incorporate topology. However, designing such task-specific biases requires significant effort and can introduce side effects that hinder generalization. We introduce Chimera, a unified model that directly incorporates data topology in a principled way, removing the need for domain-specific biases. The key idea is that state space models--which naturally do not require position embeddings--can be generalized to capture any graph topology. Our experiments show that Chimera achieves strong performance across language, vision, and graph domains, outperforming BERT on GLUE by 0.7 points, ViT on ImageNet-1k by 2.6%, and all baselines on the Long Range Graph Benchmark. We further propose algorithmic optimizations to improve Chimera's efficiency: (1) for Directed Acyclic Graphs, Chimera can be implemented as a linear-time recurrence; (2) for general graphs, a simple mathematical relaxation achieves Transformer's quadratic complexity without domain-specific heuristics. These results validate Chimera's core contribution and support the idea that data topology is a powerful inductive bias across modalities.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "Published in TMLR (October 2025); 22 Pages, 6 Figures, 11 Tables",
    "pdf_url": "https://arxiv.org/pdf/2510.12111v1",
    "published_date": "2025-10-14 03:27:57 UTC",
    "updated_date": "2025-10-14 03:27:57 UTC"
  },
  {
    "arxiv_id": "2510.12110v1",
    "title": "Deep Associations, High Creativity: A Simple yet Effective Metric for Evaluating Large Language Models",
    "authors": [
      "Ziliang Qiu",
      "Renfen Hu"
    ],
    "abstract": "The evaluation of LLMs' creativity represents a crucial research domain, though challenges such as data contamination and costly human assessments often impede progress. Drawing inspiration from human creativity assessment, we propose PACE, asking LLMs to generate Parallel Association Chains to Evaluate their creativity. PACE minimizes the risk of data contamination and offers a straightforward, highly efficient evaluation, as evidenced by its strong correlation with Chatbot Arena Creative Writing rankings (Spearman's $ρ= 0.739$, $p < 0.001$) across various proprietary and open-source models. A comparative analysis of associative creativity between LLMs and humans reveals that while high-performing LLMs achieve scores comparable to average human performance, professional humans consistently outperform LLMs. Furthermore, linguistic analysis reveals that both humans and LLMs exhibit a trend of decreasing concreteness in their associations, and humans demonstrating a greater diversity of associative patterns.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "14 pages",
    "pdf_url": "https://arxiv.org/pdf/2510.12110v1",
    "published_date": "2025-10-14 03:26:28 UTC",
    "updated_date": "2025-10-14 03:26:28 UTC"
  },
  {
    "arxiv_id": "2510.15992v1",
    "title": "Stratos: An End-to-End Distillation Pipeline for Customized LLMs under Distributed Cloud Environments",
    "authors": [
      "Ziming Dai",
      "Tuo Zhang",
      "Fei Gao",
      "Xingyi Cai",
      "Xiaofei Wang",
      "Cheng Zhang",
      "Wenyu Wang",
      "Chengjie Zang"
    ],
    "abstract": "The growing industrial demand for customized and cost-efficient large language models (LLMs) is fueled by the rise of vertical, domain-specific tasks and the need to optimize performance under constraints such as latency and budget. Knowledge distillation, as an efficient model compression and transfer technique, offers a feasible solution. However, existing distillation frameworks often require manual intervention and struggle to meet such complex user-defined distillation requirements. To bridge this gap, we propose Stratos, an end-to-end LLM distillation pipeline that automates server and model selection, knowledge distillation, and deployment in distributed cloud environments. Given user-defined constraints on model performance and system budget, Stratos automatically selects Pareto-optimal servers, dynamically matches teacher-student pairs, and adapts distillation strategies based on task complexity to optimize cloud hosting. Experiments show that Stratos produces a student model that achieves four times the accuracy of its GPT-4o teacher baseline on a rare, domain-specific Mahjong reasoning task with reverse synthetic data and knowledge injection. Moreover, it achieves reduced latency and cost without compromising accuracy. These results highlight its promise for vertical-domain LLM deployment.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.15992v1",
    "published_date": "2025-10-14 03:12:14 UTC",
    "updated_date": "2025-10-14 03:12:14 UTC"
  },
  {
    "arxiv_id": "2510.13888v1",
    "title": "Reliable Fine-Grained Evaluation of Natural Language Math Proofs",
    "authors": [
      "Wenjie Ma",
      "Andrei Cojocaru",
      "Neel Kolhe",
      "Bradley Louie",
      "Robin Said Sharif",
      "Haihan Zhang",
      "Vincent Zhuang",
      "Matei Zaharia",
      "Sewon Min"
    ],
    "abstract": "Recent advances in large language models (LLMs) for mathematical reasoning have largely focused on tasks with easily verifiable final answers; however, generating and verifying natural language math proofs remains an open challenge. We identify the absence of a reliable, fine-grained evaluator for LLM-generated math proofs as a critical gap. To address this, we propose a systematic methodology for developing and validating evaluators that assign fine-grained scores on a 0-7 scale to model-generated math proofs. To enable this study, we introduce ProofBench, the first expert-annotated dataset of fine-grained proof ratings, spanning 145 problems from six major math competitions (USAMO, IMO, Putnam, etc) and 435 LLM-generated solutions from Gemini-2.5-pro, o3, and DeepSeek-R1. %with expert gradings. Using ProofBench as a testbed, we systematically explore the evaluator design space across key axes: the backbone model, input context, instructions and evaluation workflow. Our analysis delivers ProofGrader, an evaluator that combines a strong reasoning backbone LM, rich context from reference solutions and marking schemes, and a simple ensembling method; it achieves a low Mean Absolute Error (MAE) of 0.926 against expert scores, significantly outperforming naive baselines. Finally, we demonstrate its practical utility in a best-of-$n$ selection task: at $n=16$, ProofGrader achieves an average score of 4.14 (out of 7), closing 78% of the gap between a naive binary evaluator (2.48) and the human oracle (4.62), highlighting its potential to advance downstream proof generation.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "31 pages, 6 figures, 10 tables",
    "pdf_url": "https://arxiv.org/pdf/2510.13888v1",
    "published_date": "2025-10-14 02:59:07 UTC",
    "updated_date": "2025-10-14 02:59:07 UTC"
  },
  {
    "arxiv_id": "2510.13887v3",
    "title": "Incomplete Multi-view Clustering via Hierarchical Semantic Alignment and Cooperative Completion",
    "authors": [
      "Xiaojian Ding",
      "Lin Zhao",
      "Xian Li",
      "Xiaoying Zhu"
    ],
    "abstract": "Incomplete multi-view data, where certain views are entirely missing for some samples, poses significant challenges for traditional multi-view clustering methods. Existing deep incomplete multi-view clustering approaches often rely on static fusion strategies or two-stage pipelines, leading to suboptimal fusion results and error propagation issues. To address these limitations, this paper proposes a novel incomplete multi-view clustering framework based on Hierarchical Semantic Alignment and Cooperative Completion (HSACC). HSACC achieves robust cross-view fusion through a dual-level semantic space design. In the low-level semantic space, consistency alignment is ensured by maximizing mutual information across views. In the high-level semantic space, adaptive view weights are dynamically assigned based on the distributional affinity between individual views and an initial fused representation, followed by weighted fusion to generate a unified global representation. Additionally, HSACC implicitly recovers missing views by projecting aligned latent representations into high-dimensional semantic spaces and jointly optimizes reconstruction and clustering objectives, enabling cooperative learning of completion and clustering. Experimental results demonstrate that HSACC significantly outperforms state-of-the-art methods on five benchmark datasets. Ablation studies validate the effectiveness of the hierarchical alignment and dynamic weighting mechanisms, while parameter analysis confirms the model's robustness to hyperparameter variations.",
    "categories": [
      "eess.IV",
      "cs.AI",
      "cs.LG",
      "stat.ML"
    ],
    "primary_category": "eess.IV",
    "comment": "13 pages, conference paper. Accepted to NeurIPS 2025, not yet published",
    "pdf_url": "https://arxiv.org/pdf/2510.13887v3",
    "published_date": "2025-10-14 02:58:10 UTC",
    "updated_date": "2025-10-27 03:20:35 UTC"
  },
  {
    "arxiv_id": "2510.12091v1",
    "title": "ToPolyAgent: AI Agents for Coarse-Grained Topological Polymer Simulations",
    "authors": [
      "Lijie Ding",
      "Jan-Michael Carrillo",
      "Changwoo Do"
    ],
    "abstract": "We introduce ToPolyAgent, a multi-agent AI framework for performing coarse-grained molecular dynamics (MD) simulations of topological polymers through natural language instructions. By integrating large language models (LLMs) with domain-specific computational tools, ToPolyAgent supports both interactive and autonomous simulation workflows across diverse polymer architectures, including linear, ring, brush, and star polymers, as well as dendrimers. The system consists of four LLM-powered agents: a Config Agent for generating initial polymer-solvent configurations, a Simulation Agent for executing LAMMPS-based MD simulations and conformational analyses, a Report Agent for compiling markdown reports, and a Workflow Agent for streamlined autonomous operations. Interactive mode incorporates user feedback loops for iterative refinements, while autonomous mode enables end-to-end task execution from detailed prompts. We demonstrate ToPolyAgent's versatility through case studies involving diverse polymer architectures under varying solvent condition, thermostats, and simulation lengths. Furthermore, we highlight its potential as a research assistant by directing it to investigate the effect of interaction parameters on the linear polymer conformation, and the influence of grafting density on the persistence length of the brush polymer. By coupling natural language interfaces with rigorous simulation tools, ToPolyAgent lowers barriers to complex computational workflows and advances AI-driven materials discovery in polymer science. It lays the foundation for autonomous and extensible multi-agent scientific research ecosystems.",
    "categories": [
      "cs.AI",
      "cond-mat.mtrl-sci",
      "cond-mat.soft"
    ],
    "primary_category": "cs.AI",
    "comment": "10 pages, 8 figures",
    "pdf_url": "https://arxiv.org/pdf/2510.12091v1",
    "published_date": "2025-10-14 02:54:19 UTC",
    "updated_date": "2025-10-14 02:54:19 UTC"
  },
  {
    "arxiv_id": "2510.12088v1",
    "title": "One Life to Learn: Inferring Symbolic World Models for Stochastic Environments from Unguided Exploration",
    "authors": [
      "Zaid Khan",
      "Archiki Prasad",
      "Elias Stengel-Eskin",
      "Jaemin Cho",
      "Mohit Bansal"
    ],
    "abstract": "Symbolic world modeling requires inferring and representing an environment's transitional dynamics as an executable program. Prior work has focused on largely deterministic environments with abundant interaction data, simple mechanics, and human guidance. We address a more realistic and challenging setting, learning in a complex, stochastic environment where the agent has only \"one life\" to explore a hostile environment without human guidance. We introduce OneLife, a framework that models world dynamics through conditionally-activated programmatic laws within a probabilistic programming framework. Each law operates through a precondition-effect structure, activating in relevant world states. This creates a dynamic computation graph that routes inference and optimization only through relevant laws, avoiding scaling challenges when all laws contribute to predictions about a complex, hierarchical state, and enabling the learning of stochastic dynamics even with sparse rule activation. To evaluate our approach under these demanding constraints, we introduce a new evaluation protocol that measures (a) state ranking, the ability to distinguish plausible future states from implausible ones, and (b) state fidelity, the ability to generate future states that closely resemble reality. We develop and evaluate our framework on Crafter-OO, our reimplementation of the Crafter environment that exposes a structured, object-oriented symbolic state and a pure transition function that operates on that state alone. OneLife can successfully learn key environment dynamics from minimal, unguided interaction, outperforming a strong baseline on 16 out of 23 scenarios tested. We also test OneLife's planning ability, with simulated rollouts successfully identifying superior strategies. Our work establishes a foundation for autonomously constructing programmatic world models of unknown, complex environments.",
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "Project page: https://onelife-worldmodel.github.io/; 39 pages",
    "pdf_url": "https://arxiv.org/pdf/2510.12088v1",
    "published_date": "2025-10-14 02:49:32 UTC",
    "updated_date": "2025-10-14 02:49:32 UTC"
  },
  {
    "arxiv_id": "2510.12083v1",
    "title": "An AI-Based Behavioral Health Safety Filter and Dataset for Identifying Mental Health Crises in Text-Based Conversations",
    "authors": [
      "Benjamin W. Nelson",
      "Celeste Wong",
      "Matthew T. Silvestrini",
      "Sooyoon Shin",
      "Alanna Robinson",
      "Jessica Lee",
      "Eric Yang",
      "John Torous",
      "Andrew Trister"
    ],
    "abstract": "Large language models often mishandle psychiatric emergencies, offering harmful or inappropriate advice and enabling destructive behaviors. This study evaluated the Verily behavioral health safety filter (VBHSF) on two datasets: the Verily Mental Health Crisis Dataset containing 1,800 simulated messages and the NVIDIA Aegis AI Content Safety Dataset subsetted to 794 mental health-related messages. The two datasets were clinician-labelled and we evaluated performance using the clinician labels. Additionally, we carried out comparative performance analyses against two open source, content moderation guardrails: OpenAI Omni Moderation Latest and NVIDIA NeMo Guardrails. The VBHSF demonstrated, well-balanced performance on the Verily Mental Health Crisis Dataset v1.0, achieving high sensitivity (0.990) and specificity (0.992) in detecting any mental health crises. It achieved an F1-score of 0.939, sensitivity ranged from 0.917-0.992, and specificity was >= 0.978 in identifying specific crisis categories. When evaluated against the NVIDIA Aegis AI Content Safety Dataset 2.0, VBHSF performance remained highly sensitive (0.982) and accuracy (0.921) with reduced specificity (0.859). When compared with the NVIDIA NeMo and OpenAI Omni Moderation Latest guardrails, the VBHSF demonstrated superior performance metrics across both datasets, achieving significantly higher sensitivity in all cases (all p < 0.001) and higher specificity relative to NVIDIA NeMo (p < 0.001), but not to OpenAI Omni Moderation Latest (p = 0.094). NVIDIA NeMo and OpenAI Omni Moderation Latest exhibited inconsistent performance across specific crisis types, with sensitivity for some categories falling below 0.10. Overall, the VBHSF demonstrated robust, generalizable performance that prioritizes sensitivity to minimize missed crises, a crucial feature for healthcare applications.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Main Text: 2943; Abstract: 256; Tables and Figures: 5",
    "pdf_url": "https://arxiv.org/pdf/2510.12083v1",
    "published_date": "2025-10-14 02:47:52 UTC",
    "updated_date": "2025-10-14 02:47:52 UTC"
  },
  {
    "arxiv_id": "2510.12082v1",
    "title": "Enhancing Neural Code Representation with Additional Context",
    "authors": [
      "Huy Nguyen",
      "Christoph Treude",
      "Patanamon Thongtanunam"
    ],
    "abstract": "Automated program comprehension underpins many software engineering tasks, from code summarisation to clone detection. Recent deep learning models achieve strong results but typically rely on source code alone, overlooking contextual information such as version history or structural relationships. This limits their ability to capture how code evolves and operates. We conduct an empirical study on how enriching code representations with such contextual signals affects neural model performance on key comprehension tasks. Two downstream tasks, code clone detection and code summarisation, are evaluated using SeSaMe (1,679 Java methods) and CodeSearchNet (63,259 methods). Five representative models (CodeBERT, GraphCodeBERT, CodeT5, PLBART, ASTNN) are fine-tuned under code-only and context-augmented settings. Results show that context generally improves performance: version history consistently boosts clone detection (e.g., CodeT5 +15.92% F1) and summarisation (e.g., GraphCodeBERT +5.56% METEOR), while call-graph effects vary by model and task. Combining multiple contexts yields further gains (up to +21.48% macro-F1). Human evaluation on 100 Java snippets confirms that context-augmented summaries are significantly preferred for Accuracy and Content Adequacy (p <= 0.026; |delta| up to 0.55). These findings highlight the potential of contextual signals to enhance code comprehension and open new directions for optimising contextual encoding in neural SE models.",
    "categories": [
      "cs.SE",
      "cs.AI"
    ],
    "primary_category": "cs.SE",
    "comment": "34 pages, 7 figures, 11 tables",
    "pdf_url": "https://arxiv.org/pdf/2510.12082v1",
    "published_date": "2025-10-14 02:45:42 UTC",
    "updated_date": "2025-10-14 02:45:42 UTC"
  },
  {
    "arxiv_id": "2510.12080v1",
    "title": "Evaluating the Quality of Randomness and Entropy in Tasks Supported by Large Language Models",
    "authors": [
      "Rabimba Karanjai",
      "Yang Lu",
      "Ranjith Chodavarapu",
      "Lei Xu",
      "Weidong Shi"
    ],
    "abstract": "The rapid advancement of large language model (LLM) technology has led to diverse applications, many of which inherently require randomness, such as stochastic decision-making, gaming, scheduling, AI agents, and cryptography-related tasks. However, the capabilities of LLMs in handling randomness, particularly in generating and utilizing random numbers effectively, remain unclear. This paper investigates the capacity of LLMs for handling tasks that involve randomness through a series of experiments. We designed a set of experiments that consider various factors that can influence an LLM's performance in tasks involving randomness, such as accessibility to external tools, types of tasks, model states (fresh vs. non-fresh), and prompting strategies. The experiments cover a range of tasks, including generating random numbers, generating random strings such as passwords, shuffling items, and evaluating the quality of randomness using entropy and the NIST randomness test-suite. Our findings reveal that while LLMs can generate outputs that exhibit some degree of randomness, their performance is inconsistent and often deviates significantly from the expected behavior. The analysis of the experimental results highlights key limitations and areas where improvement is needed for the LLMs to effectively handle tasks involving randomness",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.12080v1",
    "published_date": "2025-10-14 02:43:08 UTC",
    "updated_date": "2025-10-14 02:43:08 UTC"
  },
  {
    "arxiv_id": "2510.17852v1",
    "title": "Deploying Atmospheric and Oceanic AI Models on Chinese Hardware and Framework: Migration Strategies, Performance Optimization and Analysis",
    "authors": [
      "Yuze Sun",
      "Wentao Luo",
      "Yanfei Xiang",
      "Jiancheng Pan",
      "Jiahao Li",
      "Quan Zhang",
      "Xiaomeng Huang"
    ],
    "abstract": "With the growing role of artificial intelligence in climate and weather research, efficient model training and inference are in high demand. Current models like FourCastNet and AI-GOMS depend heavily on GPUs, limiting hardware independence, especially for Chinese domestic hardware and frameworks. To address this issue, we present a framework for migrating large-scale atmospheric and oceanic models from PyTorch to MindSpore and optimizing for Chinese chips, and evaluating their performance against GPUs. The framework focuses on software-hardware adaptation, memory optimization, and parallelism. Furthermore, the model's performance is evaluated across multiple metrics, including training speed, inference speed, model accuracy, and energy efficiency, with comparisons against GPU-based implementations. Experimental results demonstrate that the migration and optimization process preserves the models' original accuracy while significantly reducing system dependencies and improving operational efficiency by leveraging Chinese chips as a viable alternative for scientific computing. This work provides valuable insights and practical guidance for leveraging Chinese domestic chips and frameworks in atmospheric and oceanic AI model development, offering a pathway toward greater technological independence.",
    "categories": [
      "cs.DC",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.DC",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.17852v1",
    "published_date": "2025-10-14 02:41:56 UTC",
    "updated_date": "2025-10-14 02:41:56 UTC"
  },
  {
    "arxiv_id": "2510.13886v1",
    "title": "Physics-Informed autoencoder for DSC-MRI Perfusion post-processing: application to glioma grading",
    "authors": [
      "Pierre Fayolle",
      "Alexandre Bône",
      "Noëlie Debs",
      "Mathieu Naudin",
      "Pascal Bourdon",
      "Remy Guillevin",
      "David Helbert"
    ],
    "abstract": "DSC-MRI perfusion is a medical imaging technique for diagnosing and prognosing brain tumors and strokes. Its analysis relies on mathematical deconvolution, but noise or motion artifacts in a clinical environment can disrupt this process, leading to incorrect estimate of perfusion parameters. Although deep learning approaches have shown promising results, their calibration typically rely on third-party deconvolution algorithms to generate reference outputs and are bound to reproduce their limitations.\n  To adress this problem, we propose a physics-informed autoencoder that leverages an analytical model to decode the perfusion parameters and guide the learning of the encoding network. This autoencoder is trained in a self-supervised fashion without any third-party software and its performance is evaluated on a database with glioma patients. Our method shows reliable results for glioma grading in accordance with other well-known deconvolution algorithms despite a lower computation time. It also achieved competitive performance even in the presence of high noise which is critical in a medical environment.",
    "categories": [
      "q-bio.QM",
      "cs.AI",
      "eess.IV",
      "eess.SP"
    ],
    "primary_category": "q-bio.QM",
    "comment": "5 pages, 5 figures, IEEE ISBI 2025, Houston, Tx, USA",
    "pdf_url": "https://arxiv.org/pdf/2510.13886v1",
    "published_date": "2025-10-14 02:39:55 UTC",
    "updated_date": "2025-10-14 02:39:55 UTC"
  },
  {
    "arxiv_id": "2510.12076v1",
    "title": "BeSTAD: Behavior-Aware Spatio-Temporal Anomaly Detection for Human Mobility Data",
    "authors": [
      "Junyi Xie",
      "Jina Kim",
      "Yao-Yi Chiang",
      "Lingyi Zhao",
      "Khurram Shafique"
    ],
    "abstract": "Traditional anomaly detection in human mobility has primarily focused on trajectory-level analysis, identifying statistical outliers or spatiotemporal inconsistencies across aggregated movement traces. However, detecting individual-level anomalies, i.e., unusual deviations in a person's mobility behavior relative to their own historical patterns, within datasets encompassing large populations remains a significant challenge. In this paper, we present BeSTAD (Behavior-aware Spatio-Temporal Anomaly Detection for Human Mobility Data), an unsupervised framework that captures individualized behavioral signatures across large populations and uncovers fine-grained anomalies by jointly modeling spatial context and temporal dynamics. BeSTAD learns semantically enriched mobility representations that integrate location meaning and temporal patterns, enabling the detection of subtle deviations in individual movement behavior. BeSTAD further employs a behavior-cluster-aware modeling mechanism that builds personalized behavioral profiles from normal activity and identifies anomalies through cross-period behavioral comparison with consistent semantic alignment. Building on prior work in mobility behavior clustering, this approach enables not only the detection of behavioral shifts and deviations from established routines but also the identification of individuals exhibiting such changes within large-scale mobility datasets. By learning individual behaviors directly from unlabeled data, BeSTAD advances anomaly detection toward personalized and interpretable mobility analysis.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "accepted by The 2nd ACM SIGSPATIAL International Workshop on Geospatial Anomaly Detection",
    "pdf_url": "https://arxiv.org/pdf/2510.12076v1",
    "published_date": "2025-10-14 02:33:06 UTC",
    "updated_date": "2025-10-14 02:33:06 UTC"
  },
  {
    "arxiv_id": "2510.12075v1",
    "title": "A Review on Domain Adaption and Generative Adversarial Networks(GANs)",
    "authors": [
      "Aashish Dhawan",
      "Divyanshu Mudgal"
    ],
    "abstract": "The major challenge in today's computer vision scenario is the availability of good quality labeled data. In a field of study like image classification, where data is of utmost importance, we need to find more reliable methods which can overcome the scarcity of data to produce results comparable to previous benchmark results. In most cases, obtaining labeled data is very difficult because of the high cost of human labor and in some cases impossible. The purpose of this paper is to discuss Domain Adaptation and various methods to implement it. The main idea is to use a model trained on a particular dataset to predict on data from a different domain of the same kind, for example - a model trained on paintings of airplanes predicting on real images of airplanes",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.12075v1",
    "published_date": "2025-10-14 02:32:10 UTC",
    "updated_date": "2025-10-14 02:32:10 UTC"
  },
  {
    "arxiv_id": "2510.12072v1",
    "title": "EmboMatrix: A Scalable Training-Ground for Embodied Decision-Making",
    "authors": [
      "Zixing Lei",
      "Sheng Yin",
      "Yichen Xiong",
      "Yuanzhuo Ding",
      "Wenhao Huang",
      "Yuxi Wei",
      "Qingyao Xu",
      "Yiming Li",
      "Weixin Li",
      "Yunhong Wang",
      "Siheng Chen"
    ],
    "abstract": "Embodied decision-making enables agents to translate high-level goals into executable actions through continuous interactions within the physical world, forming a cornerstone of general-purpose embodied intelligence. Large language models (LLMs), with their general decision-making capabilities, offer a promising path to realize this potential; however, LLMs trained solely on language lack exposure to physical environments, limiting their true embodied understanding. To bridge this gap, we propose the concept of a training ground: a comprehensive infrastructure that provides task and scene simulation, embodied interaction, and feedback signals, offering a one-stop solution for LLM acquire genuine embodied decision-making skills. In this work, we present EmboMatrix, the first training ground of its kind, providing massive and diverse tasks with efficient simulation and precise rewards. EmboMatrix incorporates a series of novel techniques: a multi-agent data engine for large-scale task and scene generation, a distributed heterogeneous-hardware system for scalable simulation, and a multi-level reward architecture for precise supervision. Leveraging EmboMatrix, we cultivate EmboBrain, an LLM whose embodied decision-making abilities emerge from extensive embodied interactions. Experiments show that EmboBrain-7B surpasses the 671B DeepSeek-R1 baseline by 9.5\\% on two challenging embodied decision-making benchmarks, demonstrating the power of interactive, environment-grounded learning for building truly intelligent embodied agents.",
    "categories": [
      "cs.AI",
      "cs.RO"
    ],
    "primary_category": "cs.AI",
    "comment": "10 pages 8 figures",
    "pdf_url": "https://arxiv.org/pdf/2510.12072v1",
    "published_date": "2025-10-14 02:26:52 UTC",
    "updated_date": "2025-10-14 02:26:52 UTC"
  },
  {
    "arxiv_id": "2510.12070v1",
    "title": "MEASURE: Multi-scale Minimal Sufficient Representation Learning for Domain Generalization in Sleep Staging",
    "authors": [
      "Sangmin Jo",
      "Jee Seok Yoon",
      "Wootaek Jeong",
      "Kwanseok Oh",
      "Heung-Il Suk"
    ],
    "abstract": "Deep learning-based automatic sleep staging has significantly advanced in performance and plays a crucial role in the diagnosis of sleep disorders. However, those models often struggle to generalize on unseen subjects due to variability in physiological signals, resulting in degraded performance in out-of-distribution scenarios. To address this issue, domain generalization approaches have recently been studied to ensure generalized performance on unseen domains during training. Among those techniques, contrastive learning has proven its validity in learning domain-invariant features by aligning samples of the same class across different domains. Despite its potential, many existing methods are insufficient to extract adequately domain-invariant representations, as they do not explicitly address domain characteristics embedded within the unshared information across samples. In this paper, we posit that mitigating such domain-relevant attributes-referred to as excess domain-relevant information-is key to bridging the domain gap. However, the direct strategy to mitigate the domain-relevant attributes often overfits features at the high-level information, limiting their ability to leverage the diverse temporal and spectral information encoded in the multiple feature levels. To address these limitations, we propose a novel MEASURE (Multi-scalE minimAl SUfficient Representation lEarning) framework, which effectively reduces domain-relevant information while preserving essential temporal and spectral features for sleep stage classification. In our exhaustive experiments on publicly available sleep staging benchmark datasets, SleepEDF-20 and MASS, our proposed method consistently outperformed state-of-the-art methods. Our code is available at : https://github.com/ku-milab/Measure",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "12 page, 7 figures, uses IEEE.sty",
    "pdf_url": "https://arxiv.org/pdf/2510.12070v1",
    "published_date": "2025-10-14 02:20:50 UTC",
    "updated_date": "2025-10-14 02:20:50 UTC"
  },
  {
    "arxiv_id": "2510.12067v1",
    "title": "HiCoTraj:Zero-Shot Demographic Reasoning via Hierarchical Chain-of-Thought Prompting from Trajectory",
    "authors": [
      "Junyi Xie",
      "Yuankun Jiao",
      "Jina Kim",
      "Yao-Yi Chiang",
      "Lingyi Zhao",
      "Khurram Shafique"
    ],
    "abstract": "Inferring demographic attributes such as age, sex, or income level from human mobility patterns enables critical applications such as targeted public health interventions, equitable urban planning, and personalized transportation services. Existing mobility-based demographic inference studies heavily rely on large-scale trajectory data with demographic labels, leading to limited interpretability and poor generalizability across different datasets and user groups. We propose HiCoTraj (Zero-Shot Demographic Reasoning via Hierarchical Chain-of-Thought Prompting from Trajectory), a framework that leverages LLMs' zero-shot learning and semantic understanding capabilities to perform demographic inference without labeled training data. HiCoTraj transforms trajectories into semantically rich, natural language representations by creating detailed activity chronicles and multi-scale visiting summaries. Then HiCoTraj uses a novel hierarchical chain of thought reasoning to systematically guide LLMs through three cognitive stages: factual feature extraction, behavioral pattern analysis, and demographic inference with structured output. This approach addresses the scarcity challenge of labeled demographic data while providing transparent reasoning chains. Experimental evaluation on real-world trajectory data demonstrates that HiCoTraj achieves competitive performance across multiple demographic attributes in zero-shot scenarios.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "accepted by The 1st ACM SIGSPATIAL International Workshop on Generative and Agentic AI for Multi-Modality Space-Time Intelligence",
    "pdf_url": "https://arxiv.org/pdf/2510.12067v1",
    "published_date": "2025-10-14 02:18:29 UTC",
    "updated_date": "2025-10-14 02:18:29 UTC"
  },
  {
    "arxiv_id": "2510.12066v1",
    "title": "AI Agents as Universal Task Solvers",
    "authors": [
      "Alessandro Achille",
      "Stefano Soatto"
    ],
    "abstract": "AI reasoning agents are already able to solve a variety of tasks by deploying tools, simulating outcomes of multiple hypotheses and reflecting on them. In doing so, they perform computation, although not in the classical sense -- there is no program being executed. Still, if they perform computation, can AI agents be universal? Can chain-of-thought reasoning solve any computable task? How does an AI Agent learn to reason? Is it a matter of model size? Or training dataset size?\n  In this work, we reinterpret the role of learning in the context of AI Agents, viewing them as compute-capable stochastic dynamical systems, and highlight the role of time in a foundational principle for learning to reason. In doing so, we propose a shift from classical inductive learning to transductive learning -- where the objective is not to approximate the distribution of past data, but to capture their algorithmic structure to reduce the time needed to find solutions to new tasks.\n  Transductive learning suggests that, counter to Shannon's theory, a key role of information in learning is about reduction of time rather than reconstruction error. In particular, we show that the optimal speed-up that a universal solver can achieve using past data is tightly related to their algorithmic information. Using this, we show a theoretical derivation for the observed power-law scaling of inference time versus training time. We then show that scaling model size can lead to behaviors that, while improving accuracy on benchmarks, fail any reasonable test of intelligence, let alone super-intelligence: In the limit of infinite space and time, large models can behave as savants, able to brute-force through any task without any insight. Instead, we argue that the key quantity to optimize when scaling reasoning models is time, whose critical role in learning has so far only been indirectly considered.",
    "categories": [
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.12066v1",
    "published_date": "2025-10-14 02:17:54 UTC",
    "updated_date": "2025-10-14 02:17:54 UTC"
  },
  {
    "arxiv_id": "2510.13885v1",
    "title": "Order from Chaos: Comparative Study of Ten Leading LLMs on Unstructured Data Categorization",
    "authors": [
      "Ariel Kamen"
    ],
    "abstract": "This study presents a comparative evaluation of ten state-of-the-art large language models (LLMs) applied to unstructured text categorization using the Interactive Advertising Bureau (IAB) 2.2 hierarchical taxonomy. The analysis employed a uniform dataset of 8,660 human-annotated samples and identical zero-shot prompts to ensure methodological consistency across all models. Evaluation metrics included four classic measures - accuracy, precision, recall, and F1-score - and three LLM-specific indicators: hallucination ratio, inflation ratio, and categorization cost.\n  Results show that, despite their rapid advancement, contemporary LLMs achieve only moderate classic performance, with average scores of 34% accuracy, 42% precision, 45% recall, and 41% F1-score. Hallucination and inflation ratios reveal that models frequently overproduce categories relative to human annotators. Among the evaluated systems, Gemini 1.5/2.0 Flash and GPT 20B/120B offered the most favorable cost-to-performance balance, while GPT 120B demonstrated the lowest hallucination ratio. The findings suggest that scaling and architectural improvements alone do not ensure better categorization accuracy, as the task requires compressing rich unstructured text into a limited taxonomy - a process that challenges current model architectures.\n  To address these limitations, a separate ensemble-based approach was developed and tested. The ensemble method, in which multiple LLMs act as independent experts, substantially improved accuracy, reduced inflation, and completely eliminated hallucinations. These results indicate that coordinated orchestration of models - rather than sheer scale - may represent the most effective path toward achieving or surpassing human-expert performance in large-scale text categorization.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "10 pages, 4 figures,",
    "pdf_url": "https://arxiv.org/pdf/2510.13885v1",
    "published_date": "2025-10-14 02:15:01 UTC",
    "updated_date": "2025-10-14 02:15:01 UTC"
  },
  {
    "arxiv_id": "2510.12063v1",
    "title": "ThinkPilot: Steering Reasoning Models via Automated Think-prefixes Optimization",
    "authors": [
      "Sunzhu Li",
      "Zhiyu Lin",
      "Shuling Yang",
      "Jiale Zhao",
      "Wei Chen"
    ],
    "abstract": "Large Reasoning Models (LRMs) are powerful, but they still suffer from inefficient and off-target reasoning. Currently, training-free methods are limited to either rigid heuristics or descriptive, non-actionable analyses. In this paper, we introduce ThinkPilot, a training-free framework that automatically optimizes LRMs reasoning. It uses an evolutionary process to generate think-prefixes, which are instructions that evolve driven by a taxonomy of reasoning behaviors to guide models toward superior performance. Extensive experiments demonstrate ThinkPilot's broad effectiveness: it significantly improves the accuracy-length trade-off for efficient reasoning, drastically improves safety (for example, cutting the StrongREJECT score of DeepSeek-R1-Distill-Qwen-32B from 27.0% to 0.7), and enhances instruction following. It also synergizes with existing training-based methods. Our analysis reveals that think-prefixes can reliably control LRMs' reasoning behaviors, and that different tasks have strong preferences for specific behavioral distributions. By automatically identifying and eliciting these behaviors, ThinkPilot provides a generalizable framework for aligning LRMs reasoning with task demands. Data and code are available at https://github.com/teqkilla/ThinkPilot",
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.12063v1",
    "published_date": "2025-10-14 02:02:19 UTC",
    "updated_date": "2025-10-14 02:02:19 UTC"
  },
  {
    "arxiv_id": "2510.12061v1",
    "title": "Empowering LLM Agents with Geospatial Awareness: Toward Grounded Reasoning for Wildfire Response",
    "authors": [
      "Yiheng Chen",
      "Lingyao Li",
      "Zihui Ma",
      "Qikai Hu",
      "Yilun Zhu",
      "Min Deng",
      "Runlong Yu"
    ],
    "abstract": "Effective disaster response is essential for safeguarding lives and property. Existing statistical approaches often lack semantic context, generalize poorly across events, and offer limited interpretability. While Large language models (LLMs) provide few-shot generalization, they remain text-bound and blind to geography. To bridge this gap, we introduce a Geospatial Awareness Layer (GAL) that grounds LLM agents in structured earth data. Starting from raw wildfire detections, GAL automatically retrieves and integrates infrastructure, demographic, terrain, and weather information from external geodatabases, assembling them into a concise, unit-annotated perception script. This enriched context enables agents to produce evidence-based resource-allocation recommendations (e.g., personnel assignments, budget allocations), further reinforced by historical analogs and daily change signals for incremental updates. We evaluate the framework in real wildfire scenarios across multiple LLM models, showing that geospatially grounded agents can outperform baselines. The proposed framework can generalize to other hazards such as floods and hurricanes.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.12061v1",
    "published_date": "2025-10-14 01:59:02 UTC",
    "updated_date": "2025-10-14 01:59:02 UTC"
  },
  {
    "arxiv_id": "2510.12060v1",
    "title": "Your VAR Model is Secretly an Efficient and Explainable Generative Classifier",
    "authors": [
      "Yi-Chung Chen",
      "David I. Inouye",
      "Jing Gao"
    ],
    "abstract": "Generative classifiers, which leverage conditional generative models for classification, have recently demonstrated desirable properties such as robustness to distribution shifts. However, recent progress in this area has been largely driven by diffusion-based models, whose substantial computational cost severely limits scalability. This exclusive focus on diffusion-based methods has also constrained our understanding of generative classifiers. In this work, we propose a novel generative classifier built on recent advances in visual autoregressive (VAR) modeling, which offers a new perspective for studying generative classifiers. To further enhance its performance, we introduce the Adaptive VAR Classifier$^+$ (A-VARC$^+$), which achieves a superior trade-off between accuracy and inference speed, thereby significantly improving practical applicability. Moreover, we show that the VAR-based method exhibits fundamentally different properties from diffusion-based methods. In particular, due to its tractable likelihood, the VAR-based classifier enables visual explainability via token-wise mutual information and demonstrates inherent resistance to catastrophic forgetting in class-incremental learning tasks.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.12060v1",
    "published_date": "2025-10-14 01:59:01 UTC",
    "updated_date": "2025-10-14 01:59:01 UTC"
  },
  {
    "arxiv_id": "2510.12845v1",
    "title": "VLURes: Benchmarking VLM Visual and Linguistic Understanding in Low-Resource Languages",
    "authors": [
      "Jesse Atuhurra",
      "Iqra Ali",
      "Tomoya Iwakura",
      "Hidetaka Kamigaito",
      "Tatsuya Hiraoka"
    ],
    "abstract": "Vision Language Models (VLMs) are pivotal for advancing perception in intelligent agents. Yet, evaluation of VLMs remains limited to predominantly English-centric benchmarks in which the image-text pairs comprise short texts. To evaluate VLM fine-grained abilities, in four languages under long-text settings, we introduce a novel multilingual benchmark VLURes featuring eight vision-and-language tasks, and a pioneering unrelatedness task, to probe the fine-grained Visual and Linguistic Understanding capabilities of VLMs across English, Japanese, and low-resource languages, Swahili, and Urdu. Our datasets, curated from web resources in the target language, encompass ten diverse image categories and rich textual context, introducing valuable vision-language resources for Swahili and Urdu. By prompting VLMs to generate responses and rationales, evaluated automatically and by native speakers, we uncover performance disparities across languages and tasks critical to intelligent agents, such as object recognition, scene understanding, and relationship understanding. We conducted evaluations of ten VLMs with VLURes. The best performing model, GPT-4o, achieves an overall accuracy of 90.8% and lags human performance by 6.7%, though the gap is larger for open-source models. The gap highlights VLURes' critical role in developing intelligent agents to tackle multi-modal visual reasoning.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CV",
      "cs.RO"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.12845v1",
    "published_date": "2025-10-14 01:41:43 UTC",
    "updated_date": "2025-10-14 01:41:43 UTC"
  },
  {
    "arxiv_id": "2510.12051v1",
    "title": "APCE: Adaptive Progressive Context Expansion for Long Context Processing",
    "authors": [
      "Baisub Lee",
      "Sanghyun Byun",
      "Mohanad Odema",
      "Jung Guack",
      "Jacob Song",
      "Woo Seong Chung"
    ],
    "abstract": "Deploying useful Long-Context Transformer Models (LCTMs) requires addressing two key challenges: (1) A growing memory footprint due to quadratic self-attention and linear KV-cache scaling in memory as sequence length increases; (2) the ContextRot phenomena where empirical evidence suggests that transformer architecture's performance degrades with increasing context length. Given the shared dependency on the input, a natural question arises: Can we surgically select the most important input chunks for processing to synergistically (a) reduce the memory footprint, and (b) mitigate the ContextRot effects? In this paper, we answer this question in the affirmative for long-context summarization tasks. We propose APCE as a context-aware solution to select the most important input chunks through low-dimensional semantic similarity matching with the current query. By directly operating on the input, APCE decouples from strict dependency on underlying hardware or CUDA environments, promising a compatible solution scalable to different deployment systems. Our empirical evaluations have demonstrated superior or on-par summarization performance for APCE compared to the full dense baseline using a fraction (50%-70%) of the input sequence resulting in KV-cache and self-attention memory efficiency improvements. We hope our findings inspire further research on context-aware efficiency solutions for LCTMs geared towards other relevant long-context tasks.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "NeurIPS 2025 Workshop: ML For Systems",
    "pdf_url": "https://arxiv.org/pdf/2510.12051v1",
    "published_date": "2025-10-14 01:26:36 UTC",
    "updated_date": "2025-10-14 01:26:36 UTC"
  },
  {
    "arxiv_id": "2510.12049v2",
    "title": "Generative AI and Firm Productivity: Field Experiments in Online Retail",
    "authors": [
      "Lu Fang",
      "Zhe Yuan",
      "Kaifu Zhang",
      "Dante Donati",
      "Miklos Sarvary"
    ],
    "abstract": "We quantify the impact of Generative Artificial Intelligence (GenAI) on firm productivity through a series of large-scale randomized field experiments involving millions of users and products at a leading cross-border online retail platform. Over six months in 2023-2024, GenAI-based enhancements were integrated into seven consumer-facing business workflows. We find that GenAI adoption significantly increases sales, with treatment effects ranging from $0\\%$ to $16.3\\%$, depending on GenAI's marginal contribution relative to existing firm practices. Because inputs and prices were held constant across experimental arms, these gains map directly into total factor productivity improvements. Across the four GenAI applications with positive effects, the implied annual incremental value is approximately $\\$ 5$ per consumer-an economically meaningful impact given the retailer's scale and the early stage of GenAI adoption. The primary mechanism operates through higher conversion rates, consistent with GenAI reducing frictions in the marketplace and improving consumer experience. We also document substantial heterogeneity: smaller and newer sellers, as well as less experienced consumers, exhibit disproportionately larger gains. Our findings provide novel, large-scale causal evidence on the productivity effects of GenAI in online retail, highlighting both its immediate value and broader potential.",
    "categories": [
      "econ.GN",
      "cs.AI"
    ],
    "primary_category": "econ.GN",
    "comment": "Keywords: Field Experiments, Generative AI, Productivity, Retail Platforms, Consumer Experience. JEL codes: C93, D24, L81, M31, O3",
    "pdf_url": "https://arxiv.org/pdf/2510.12049v2",
    "published_date": "2025-10-14 01:17:09 UTC",
    "updated_date": "2025-10-31 14:50:59 UTC"
  },
  {
    "arxiv_id": "2512.00008v1",
    "title": "MOTION: ML-Assisted On-Device Low-Latency Motion Recognition",
    "authors": [
      "Veeramani Pugazhenthi",
      "Wei-Hsiang Chu",
      "Junwei Lu",
      "Jadyn N. Miyahira",
      "Soheil Salehi"
    ],
    "abstract": "The use of tiny devices capable of low-latency gesture recognition is gaining momentum in everyday human-computer interaction and especially in medical monitoring fields. Embedded solutions such as fall detection, rehabilitation tracking, and patient supervision require fast and efficient tracking of movements while avoiding unwanted false alarms. This study presents an efficient solution on how to build very efficient motion-based models only using triaxial accelerometer sensors. We explore the capability of the AutoML pipelines to extract the most important features from the data segments. This approach also involves training multiple lightweight machine learning algorithms using the extracted features. We use WeBe Band, a multi-sensor wearable device that is equipped with a powerful enough MCU to effectively perform gesture recognition entirely on the device. Of the models explored, we found that the neural network provided the best balance between accuracy, latency, and memory use. Our results also demonstrate that reliable real-time gesture recognition can be achieved in WeBe Band, with great potential for real-time medical monitoring solutions that require a secure and fast response time.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.HC"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2512.00008v1",
    "published_date": "2025-10-14 01:15:47 UTC",
    "updated_date": "2025-10-14 01:15:47 UTC"
  },
  {
    "arxiv_id": "2510.12047v3",
    "title": "ContractEval: A Benchmark for Evaluating Contract-Satisfying Assertions in Code Generation",
    "authors": [
      "Soohan Lim",
      "Joonghyuk Hahn",
      "Hyunwoo Park",
      "Sang-Ki Ko",
      "Yo-Sub Han"
    ],
    "abstract": "Current code generation benchmarks measure functional correctness on well-formed inputs, as test cases are curated to satisfy input preconditions. This leaves a gap: generated programs may appear correct but fail to satisfy contracts -- assertion-level validity constraints for rejecting ill-formed inputs. We introduce ContractEval, a benchmark for evaluating contract-satisfying assertions in code generation, i.e., whether code rejects contract-violating inputs by triggering intended assertions. Built on HumanEval+ and MBPP+, ContractEval augments each task with contract-violation tests derived from reference assertions. We synthesize these via a neuro-symbolic pipeline: an LLM converts assertion clauses into constraints, and an SMT solver enumerates satisfiable violation combinations to generate inputs that violate selected clauses while satisfying the rest. Across five code LLMs, standard prompting yields 0% contract satisfaction, while adding a few contract-violation examples boosts contract satisfaction to 49--53% while maintaining pass@1 by 92% of the original. Our code is available at https://github.com/suhanmen/ContractEval.",
    "categories": [
      "cs.AI",
      "cs.SE"
    ],
    "primary_category": "cs.AI",
    "comment": "18 pages, 15 figures, 5 tables",
    "pdf_url": "https://arxiv.org/pdf/2510.12047v3",
    "published_date": "2025-10-14 01:12:37 UTC",
    "updated_date": "2026-01-09 02:35:26 UTC"
  },
  {
    "arxiv_id": "2510.12044v1",
    "title": "Hierarchical Alignment: Surgical Fine-Tuning via Functional Layer Specialization in Large Language Models",
    "authors": [
      "Yukun Zhang",
      "Qi Dong"
    ],
    "abstract": "Existing alignment techniques for Large Language Models (LLMs), such as Direct Preference Optimization (DPO), typically treat the model as a monolithic entity, applying uniform optimization pressure across all layers. This approach overlooks the functional specialization within the Transformer architecture, where different layers are known to handle distinct tasks from syntax to abstract reasoning. In this paper, we challenge this one-size-fits-all paradigm by introducing Hierarchical Alignment, a novel method that applies targeted DPO to distinct functional blocks of a model's layers: local (syntax), intermediate (logic), and global (factuality). Through a series of controlled experiments on state-of-the-art models like Llama-3.1-8B and Qwen1.5-7B using LoRA for surgical fine-tuning, our results, evaluated by a powerful LLM-as-Judge, demonstrate significant and predictable improvements. Specifically, aligning the local layers (Local-Align) enhances grammatical fluency. More importantly, aligning the global layers (Global-Align) not only improves factual consistency as hypothesized but also proves to be the most effective strategy for enhancing logical coherence, outperforming all baselines. Critically, all hierarchical strategies successfully avoid the \"alignment tax\" observed in standard DPO, where gains in fluency come at the cost of degraded logical reasoning. These findings establish a more resource-efficient, controllable, and interpretable path for model alignment, highlighting the immense potential of shifting from monolithic optimization to structure-aware surgical fine-tuning to build more advanced and reliable LLMs.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.12044v1",
    "published_date": "2025-10-14 00:58:34 UTC",
    "updated_date": "2025-10-14 00:58:34 UTC"
  },
  {
    "arxiv_id": "2510.15990v1",
    "title": "Can GRPO Help LLMs Transcend Their Pretraining Origin?",
    "authors": [
      "Kangqi Ni",
      "Zhen Tan",
      "Zijie Liu",
      "Pingzhi Li",
      "Tianlong Chen"
    ],
    "abstract": "Reinforcement Learning with Verifiable Rewards (RLVR), primarily driven by the Group Relative Policy Optimization (GRPO) algorithm, is a leading approach for enhancing the reasoning abilities of Large Language Models (LLMs). Despite its wide adoption, GRPO's gains are often inconsistent; for instance, a model may show significant improvement in one reasoning domain, like mathematics, yet remain stagnant in another, such as medicine. This inconsistency raises a critical question: under what conditions does GRPO improve reasoning and generalize out-of-distribution (OOD)? We investigate this from a data distribution perspective. We first prove theoretically that GRPO is a conservative reweighting scheme, bounded by the base model's distribution and thus unable to discover completely novel solutions. We further validate this in carefully designed controlled studies by training transformers from scratch, evaluating generalization across reasoning depth, input length, token representation, and compositionality. Our results provide a principled explanation for GRPO's boundaries: OOD improvement emerges only when the target task aligns with the model's pretrained biases, while gains on in-distribution (ID) tasks diminish as performance saturates. This reframes GRPO not as a universal reasoning enhancer but as a tool that sharpens pretraining biases. Our findings motivate future development of algorithms that can expand a model's capabilities beyond its pretraining origin.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.15990v1",
    "published_date": "2025-10-14 00:37:52 UTC",
    "updated_date": "2025-10-14 00:37:52 UTC"
  },
  {
    "arxiv_id": "2510.12033v1",
    "title": "CausalTrace: A Neurosymbolic Causal Analysis Agent for Smart Manufacturing",
    "authors": [
      "Chathurangi Shyalika",
      "Aryaman Sharma",
      "Fadi El Kalach",
      "Utkarshani Jaimini",
      "Cory Henson",
      "Ramy Harik",
      "Amit Sheth"
    ],
    "abstract": "Modern manufacturing environments demand not only accurate predictions but also interpretable insights to process anomalies, root causes, and potential interventions. Existing AI systems often function as isolated black boxes, lacking the seamless integration of prediction, explanation, and causal reasoning required for a unified decision-support solution. This fragmentation limits their trustworthiness and practical utility in high-stakes industrial environments. In this work, we present CausalTrace, a neurosymbolic causal analysis module integrated into the SmartPilot industrial CoPilot. CausalTrace performs data-driven causal analysis enriched by industrial ontologies and knowledge graphs, including advanced functions such as causal discovery, counterfactual reasoning, and root cause analysis (RCA). It supports real-time operator interaction and is designed to complement existing agents by offering transparent, explainable decision support. We conducted a comprehensive evaluation of CausalTrace using multiple causal assessment methods and the C3AN framework (i.e. Custom, Compact, Composite AI with Neurosymbolic Integration), which spans principles of robustness, intelligence, and trustworthiness. In an academic rocket assembly testbed, CausalTrace achieved substantial agreement with domain experts (ROUGE-1: 0.91 in ontology QA) and strong RCA performance (MAP@3: 94%, PR@2: 97%, MRR: 0.92, Jaccard: 0.92). It also attained 4.59/5 in the C3AN evaluation, demonstrating precision and reliability for live deployment.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "8 pages, 4 figures, 3 tables, Accepted at AAAI 2026: IAAI - Innovative Applications of AI Conference",
    "pdf_url": "https://arxiv.org/pdf/2510.12033v1",
    "published_date": "2025-10-14 00:36:40 UTC",
    "updated_date": "2025-10-14 00:36:40 UTC"
  },
  {
    "arxiv_id": "2510.12032v1",
    "title": "Multi-stage Prompt Refinement for Mitigating Hallucinations in Large Language Models",
    "authors": [
      "Jung-Woo Shim",
      "Yeong-Joon Ju",
      "Ji-Hoon Park",
      "Seong-Whan Lee"
    ],
    "abstract": "Recent advancements in large language models (LLMs) have shown strong performance in natural language understanding and generation tasks. However, LLMs continue to encounter challenges with hallucinations, where models generate plausible but incorrect information. While several factors contribute to hallucinations, the impact of ill-formed prompts, prompts with ambiguous wording, incorrect grammar, or incomplete information, was relatively under explored. To address this, we introduce Multi-stage Prompt Refinement (MPR), a framework designed to systematically improve these ill-formed prompts across multiple stages. Each stage addresses specific errors such as punctuation, typographical mistakes, and misuse of key terms, using small language models (SLMs) fine-tuned for these tasks. MPR iteratively enhances the clarity of prompts with additional context and employs a self-reflection mechanism with ranking to prioritize the most relevant input. Experimental results on hallucination benchmarks show that prompts refined by MPR achieve over an 85~\\% win rate compared to their original forms, demonstrating its effectiveness in reducing hallucinations and improving LLM output accuracy. Interestingly, we reveal that MPR can be combined with existing post-hoc hallucination mitigation frameworks, further enhancing its versatility. MPR provides a lightweight and adaptable solution for enhancing LLM reliability across various domains.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "22 pages, 6 figures",
    "pdf_url": "https://arxiv.org/pdf/2510.12032v1",
    "published_date": "2025-10-14 00:31:36 UTC",
    "updated_date": "2025-10-14 00:31:36 UTC"
  },
  {
    "arxiv_id": "2510.12029v1",
    "title": "CPR: Mitigating Large Language Model Hallucinations with Curative Prompt Refinement",
    "authors": [
      "Jung-Woo Shim",
      "Yeong-Joon Ju",
      "Ji-Hoon Park",
      "Seong-Whan Lee"
    ],
    "abstract": "Recent advancements in large language models (LLMs) highlight their fluency in generating responses to diverse prompts. However, these models sometimes generate plausible yet incorrect ``hallucinated\" facts, undermining trust. A frequent but often overlooked cause of such errors is the use of poorly structured or vague prompts by users, leading LLMs to base responses on assumed rather than actual intentions. To mitigate hallucinations induced by these ill-formed prompts, we introduce Curative Prompt Refinement (CPR), a plug-and-play framework for curative prompt refinement that 1) cleans ill-formed prompts, and 2) generates additional informative task descriptions to align the intention of the user and the prompt using a fine-tuned small language model. When applied to language models, we discover that CPR significantly increases the quality of generation while also mitigating hallucination. Empirical studies show that prompts with CPR applied achieves over a 90\\% win rate over the original prompts without any external knowledge.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "2024 IEEE International Conference on Systems, Man, and Cybernetics (SMC), 7 pages, 2 figures",
    "pdf_url": "https://arxiv.org/pdf/2510.12029v1",
    "published_date": "2025-10-14 00:27:46 UTC",
    "updated_date": "2025-10-14 00:27:46 UTC"
  }
]
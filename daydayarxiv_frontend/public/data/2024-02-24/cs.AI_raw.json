[
  {
    "arxiv_id": "2402.15938v3",
    "title": "Generalization or Memorization: Data Contamination and Trustworthy Evaluation for Large Language Models",
    "authors": [
      "Yihong Dong",
      "Xue Jiang",
      "Huanyu Liu",
      "Zhi Jin",
      "Bin Gu",
      "Mengfei Yang",
      "Ge Li"
    ],
    "abstract": "Recent statements about the impressive capabilities of large language models\n(LLMs) are usually supported by evaluating on open-access benchmarks.\nConsidering the vast size and wide-ranging sources of LLMs' training data, it\ncould explicitly or implicitly include test data, leading to LLMs being more\nsusceptible to data contamination. However, due to the opacity of training\ndata, the black-box access of models, and the rapid growth of synthetic\ntraining data, detecting and mitigating data contamination for LLMs faces\nsignificant challenges. In this paper, we propose CDD, which stands for\nContamination Detection via output Distribution for LLMs. CDD necessitates only\nthe sampled texts to detect data contamination, by identifying the peakedness\nof LLM's output distribution. To mitigate the impact of data contamination in\nevaluation, we also present TED: Trustworthy Evaluation via output\nDistribution, based on the correction of LLM's output distribution. To\nfacilitate this study, we introduce two benchmarks, i.e., DetCon and ComiEval,\nfor data contamination detection and contamination mitigation evaluation tasks.\nExtensive experimental results show that CDD achieves the average relative\nimprovements of 21.8\\%-30.2\\% over other contamination detection approaches in\nterms of Accuracy, F1 Score, and AUC metrics, and can effectively detect\nimplicit contamination. TED substantially mitigates performance improvements up\nto 66.9\\% attributed to data contamination across various contamination setups.\nIn real-world applications, we reveal that ChatGPT exhibits a high potential to\nsuffer from data contamination on HumanEval benchmark.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CR",
      "cs.LG",
      "cs.SE"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted to ACL",
    "pdf_url": "http://arxiv.org/pdf/2402.15938v3",
    "published_date": "2024-02-24 23:54:41 UTC",
    "updated_date": "2024-05-31 17:49:03 UTC"
  },
  {
    "arxiv_id": "2402.15933v1",
    "title": "Bridging the Gap between 2D and 3D Visual Question Answering: A Fusion Approach for 3D VQA",
    "authors": [
      "Wentao Mo",
      "Yang Liu"
    ],
    "abstract": "In 3D Visual Question Answering (3D VQA), the scarcity of fully annotated\ndata and limited visual content diversity hampers the generalization to novel\nscenes and 3D concepts (e.g., only around 800 scenes are utilized in ScanQA and\nSQA dataset). Current approaches resort supplement 3D reasoning with 2D\ninformation. However, these methods face challenges: either they use top-down\n2D views that introduce overly complex and sometimes question-irrelevant visual\nclues, or they rely on globally aggregated scene/image-level representations\nfrom 2D VLMs, losing the fine-grained vision-language correlations. To overcome\nthese limitations, our approach utilizes question-conditional 2D view selection\nprocedure, pinpointing semantically relevant 2D inputs for crucial visual\nclues. We then integrate this 2D knowledge into the 3D-VQA system via a\ntwo-branch Transformer structure. This structure, featuring a Twin-Transformer\ndesign, compactly combines 2D and 3D modalities and captures fine-grained\ncorrelations between modalities, allowing them mutually augmenting each other.\nIntegrating proposed mechanisms above, we present BridgeQA, that offers a fresh\nperspective on multi-modal transformer-based architectures for 3D-VQA.\nExperiments validate that BridgeQA achieves state-of-the-art on 3D-VQA datasets\nand significantly outperforms existing solutions. Code is available at\n$\\href{https://github.com/matthewdm0816/BridgeQA}{\\text{this URL}}$.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "To be published in AAAI 24",
    "pdf_url": "http://arxiv.org/pdf/2402.15933v1",
    "published_date": "2024-02-24 23:31:34 UTC",
    "updated_date": "2024-02-24 23:31:34 UTC"
  },
  {
    "arxiv_id": "2402.15929v3",
    "title": "Certifying Knowledge Comprehension in LLMs",
    "authors": [
      "Isha Chaudhary",
      "Vedaant V. Jain",
      "Gagandeep Singh"
    ],
    "abstract": "Large Language Models (LLMs) are increasingly deployed in safety-critical\nsystems where they provide answers based on in-context information derived from\nknowledge bases. As LLMs are increasingly envisioned as superhuman agents,\ntheir proficiency in knowledge comprehension-extracting relevant information\nand reasoning over it to answer questions, a key facet of human\nintelligence-becomes crucial. However, existing evaluations of LLMs on\nknowledge comprehension are typically conducted on small test sets, but these\ndatasets represent only a tiny fraction of the vast number of possible queries.\nSimple empirical evaluations on these limited test sets raises concerns about\nthe reliability and generalizability of the results. In this work, we introduce\nthe first specification and certification framework for knowledge comprehension\nin LLMs, providing formal probabilistic guarantees for reliability. Instead of\na fixed dataset, we design novel specifications that mathematically represent\nprohibitively large probability distributions of knowledge comprehension\nprompts with natural noise, using knowledge graphs. From these specifications,\nwe generate quantitative certificates that offer high-confidence, tight bounds\non the probability that a given LLM correctly answers any question drawn from\nthe specification distribution. We apply our framework to certify SOTA LLMs in\ntwo domains: precision medicine and general question-answering. Our results\nreveal previously unrecognized vulnerabilities in SOTA LLMs due to natural\nnoise in the prompts. Additionally, we establish performance hierarchies with\nformal guarantees among the SOTA LLMs, particularly in the context of precision\nmedicine question-answering.",
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2402.15929v3",
    "published_date": "2024-02-24 23:16:57 UTC",
    "updated_date": "2025-04-21 23:10:55 UTC"
  },
  {
    "arxiv_id": "2402.15925v2",
    "title": "MultiContrievers: Analysis of Dense Retrieval Representations",
    "authors": [
      "Seraphina Goldfarb-Tarrant",
      "Pedro Rodriguez",
      "Jane Dwivedi-Yu",
      "Patrick Lewis"
    ],
    "abstract": "Dense retrievers compress source documents into (possibly lossy) vector\nrepresentations, yet there is little analysis of what information is lost\nversus preserved, and how it affects downstream tasks. We conduct the first\nanalysis of the information captured by dense retrievers compared to the\nlanguage models they are based on (e.g., BERT versus Contriever). We use 25\nMultiBert checkpoints as randomized initialisations to train MultiContrievers,\na set of 25 contriever models. We test whether specific pieces of information\n-- such as gender and occupation -- can be extracted from contriever vectors of\nwikipedia-like documents. We measure this extractability via information\ntheoretic probing. We then examine the relationship of extractability to\nperformance and gender bias, as well as the sensitivity of these results to\nmany random initialisations and data shuffles. We find that (1) contriever\nmodels have significantly increased extractability, but extractability usually\ncorrelates poorly with benchmark performance 2) gender bias is present, but is\nnot caused by the contriever representations 3) there is high sensitivity to\nboth random initialisation and to data shuffle, suggesting that future\nretrieval research should test across a wider spread of both.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.IR"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2402.15925v2",
    "published_date": "2024-02-24 23:01:21 UTC",
    "updated_date": "2024-10-04 13:37:12 UTC"
  },
  {
    "arxiv_id": "2402.15923v1",
    "title": "Predicting Outcomes in Video Games with Long Short Term Memory Networks",
    "authors": [
      "Kittimate Chulajata",
      "Sean Wu",
      "Fabien Scalzo",
      "Eun Sang Cha"
    ],
    "abstract": "Forecasting winners in E-sports with real-time analytics has the potential to\nfurther engage audiences watching major tournament events. However, making such\nreal-time predictions is challenging due to unpredictable variables within the\ngame involving diverse player strategies and decision-making. Our work attempts\nto enhance audience engagement within video game tournaments by introducing a\nreal-time method of predicting wins. Our Long Short Term Memory Network (LSTMs)\nbased approach enables efficient predictions of win-lose outcomes by only using\nthe health indicator of each player as a time series. As a proof of concept, we\nevaluate our model's performance within a classic, two-player arcade game,\nSuper Street Fighter II Turbo. We also benchmark our method against state of\nthe art methods for time series forecasting; i.e. Transformer models found in\nlarge language models (LLMs). Finally, we open-source our data set and code in\nhopes of furthering work in predictive analysis for arcade games.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.MM"
    ],
    "primary_category": "cs.LG",
    "comment": "7 pages, 2 Figures, 2 Tables. Kittimate Chulajata and Sean Wu are\n  considered co-first authors",
    "pdf_url": "http://arxiv.org/pdf/2402.15923v1",
    "published_date": "2024-02-24 22:36:23 UTC",
    "updated_date": "2024-02-24 22:36:23 UTC"
  },
  {
    "arxiv_id": "2402.16905v2",
    "title": "Procedural Adherence and Interpretability Through Neuro-Symbolic Generative Agents",
    "authors": [
      "Raven Rothkopf",
      "Hannah Tongxin Zeng",
      "Mark Santolucito"
    ],
    "abstract": "The surge in popularity of large language models (LLMs) has opened doors for\nnew approaches to the creation of interactive agents. However, managing and\ninterpreting the temporal behavior of such agents over the course of a\npotentially infinite interaction remain challenging. The stateful, long-term\nhorizon reasoning required for coherent agent behavior does not fit well into\nthe LLM paradigm. We propose a combination of formal logic-based program\nsynthesis and LLM content generation to bring guarantees of procedural\nadherence and interpretability to generative agent behavior. To illustrate the\nbenefit of procedural adherence and interpretability, we use Temporal Stream\nLogic (TSL) to generate an automaton that enforces an interpretable, high-level\ntemporal structure on an agent. With the automaton tracking the context of the\ninteraction and making decisions to guide the conversation accordingly, we can\ndrive content generation in a way that allows the LLM to focus on a shorter\ncontext window. We evaluated our approach on different tasks involved in\ncreating an interactive agent specialized for generating\nchoose-your-own-adventure games. We found that over all of the tasks, an\nautomaton-enhanced agent with procedural guarantees achieves at least 96%\nadherence to its temporal constraints, whereas a purely LLM-based agent\ndemonstrates as low as 14.67% adherence.",
    "categories": [
      "cs.AI",
      "cs.LG",
      "cs.LO"
    ],
    "primary_category": "cs.AI",
    "comment": "11 pages",
    "pdf_url": "http://arxiv.org/pdf/2402.16905v2",
    "published_date": "2024-02-24 21:36:26 UTC",
    "updated_date": "2024-08-28 02:37:08 UTC"
  },
  {
    "arxiv_id": "2402.15905v1",
    "title": "Explainable Contrastive and Cost-Sensitive Learning for Cervical Cancer Classification",
    "authors": [
      "Ashfiqun Mustari",
      "Rushmia Ahmed",
      "Afsara Tasnim",
      "Jakia Sultana Juthi",
      "G M Shahariar"
    ],
    "abstract": "This paper proposes an efficient system for classifying cervical cancer cells\nusing pre-trained convolutional neural networks (CNNs). We first fine-tune five\npre-trained CNNs and minimize the overall cost of misclassification by\nprioritizing accuracy for certain classes that have higher associated costs or\nimportance. To further enhance the performance of the models, supervised\ncontrastive learning is included to make the models more adept at capturing\nimportant features and patterns. Extensive experimentation are conducted to\nevaluate the proposed system on the SIPaKMeD dataset. The experimental results\ndemonstrate the effectiveness of the developed system, achieving an accuracy of\n97.29%. To make our system more trustworthy, we have employed several\nexplainable AI techniques to interpret how the models reached a specific\ndecision. The implementation of the system can be found at -\nhttps://github.com/isha-67/CervicalCancerStudy.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "Accepted and presented in 26th International Conference on Computer\n  and Information Technology (ICCIT 2023)",
    "pdf_url": "http://arxiv.org/pdf/2402.15905v1",
    "published_date": "2024-02-24 21:03:30 UTC",
    "updated_date": "2024-02-24 21:03:30 UTC"
  },
  {
    "arxiv_id": "2402.15903v2",
    "title": "ESFL: Efficient Split Federated Learning over Resource-Constrained Heterogeneous Wireless Devices",
    "authors": [
      "Guangyu Zhu",
      "Yiqin Deng",
      "Xianhao Chen",
      "Haixia Zhang",
      "Yuguang Fang",
      "Tan F. Wong"
    ],
    "abstract": "Federated learning (FL) allows multiple parties (distributed devices) to\ntrain a machine learning model without sharing raw data. How to effectively and\nefficiently utilize the resources on devices and the central server is a highly\ninteresting yet challenging problem. In this paper, we propose an efficient\nsplit federated learning algorithm (ESFL) to take full advantage of the\npowerful computing capabilities at a central server under a split federated\nlearning framework with heterogeneous end devices (EDs). By splitting the model\ninto different submodels between the server and EDs, our approach jointly\noptimizes user-side workload and server-side computing resource allocation by\nconsidering users' heterogeneity. We formulate the whole optimization problem\nas a mixed-integer non-linear program, which is an NP-hard problem, and develop\nan iterative approach to obtain an approximate solution efficiently. Extensive\nsimulations have been conducted to validate the significantly increased\nefficiency of our ESFL approach compared with standard federated learning,\nsplit learning, and splitfed learning.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.NI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2402.15903v2",
    "published_date": "2024-02-24 20:50:29 UTC",
    "updated_date": "2024-04-17 02:59:30 UTC"
  },
  {
    "arxiv_id": "2403.00809v1",
    "title": "Abdelhak at SemEval-2024 Task 9 : Decoding Brainteasers, The Efficacy of Dedicated Models Versus ChatGPT",
    "authors": [
      "Abdelhak Kelious",
      "Mounir Okirim"
    ],
    "abstract": "This study introduces a dedicated model aimed at solving the BRAINTEASER task\n9 , a novel challenge designed to assess models lateral thinking capabilities\nthrough sentence and word puzzles. Our model demonstrates remarkable efficacy,\nsecuring Rank 1 in sentence puzzle solving during the test phase with an\noverall score of 0.98. Additionally, we explore the comparative performance of\nChatGPT, specifically analyzing how variations in temperature settings affect\nits ability to engage in lateral thinking and problem-solving. Our findings\nindicate a notable performance disparity between the dedicated model and\nChatGPT, underscoring the potential of specialized approaches in enhancing\ncreative reasoning in AI.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2403.00809v1",
    "published_date": "2024-02-24 20:00:03 UTC",
    "updated_date": "2024-02-24 20:00:03 UTC"
  },
  {
    "arxiv_id": "2402.15892v1",
    "title": "Statistical Games",
    "authors": [
      "Jozsef Konczer"
    ],
    "abstract": "This work contains the mathematical exploration of a few prototypical games\nin which central concepts from statistics and probability theory naturally\nemerge. The first two kinds of games are termed Fisher and Bayesian games,\nwhich are connected to Frequentist and Bayesian statistics, respectively.\nLater, a more general type of game is introduced, termed Statistical game, in\nwhich a further parameter, the players' relative risk aversion, can be set. In\nthis work, we show that Fisher and Bayesian games can be viewed as limiting\ncases of Statistical games. Therefore, Statistical games can be viewed as a\nunified framework, incorporating both Frequentist and Bayesian statistics.\nFurthermore, a philosophical framework is (re-)presented -- often referred to\nas minimax regret criterion -- as a general approach to decision making.\n  The main motivation for this work was to embed Bayesian statistics into a\nbroader decision-making framework, where, based on collected data, actions with\nconsequences have to be made, which can be translated to utilities (or\nrewards/losses) of the decision-maker. The work starts with the simplest\npossible toy model, related to hypothesis testing and statistical inference.\nThis choice has two main benefits: i.) it allows us to determine (conjecture)\nthe behaviour of the equilibrium strategies in various limiting cases ii.) this\nway, we can introduce Statistical games without requiring additional stochastic\nparameters. The work contains game theoretical methods related to two-player,\nnon-cooperative games to determine and prove equilibrium strategies of Fisher,\nBayesian and Statistical games. It also relies on analytical tools for\nderivations concerning various limiting cases.",
    "categories": [
      "math.ST",
      "cs.AI",
      "cs.GT",
      "cs.LG",
      "econ.TH",
      "stat.ML",
      "stat.TH",
      "62C05, 62C20, 91A35, 68T37, 68T05"
    ],
    "primary_category": "math.ST",
    "comment": "129 pages, 51 figures",
    "pdf_url": "http://arxiv.org/pdf/2402.15892v1",
    "published_date": "2024-02-24 19:59:15 UTC",
    "updated_date": "2024-02-24 19:59:15 UTC"
  },
  {
    "arxiv_id": "2402.16904v1",
    "title": "Selective Task offloading for Maximum Inference Accuracy and Energy efficient Real-Time IoT Sensing Systems",
    "authors": [
      "Abdelkarim Ben Sada",
      "Amar Khelloufi",
      "Abdenacer Naouri",
      "Huansheng Ning",
      "Sahraoui Dhelim"
    ],
    "abstract": "The recent advancements in small-size inference models facilitated AI\ndeployment on the edge. However, the limited resource nature of edge devices\nposes new challenges especially for real-time applications. Deploying multiple\ninference models (or a single tunable model) varying in size and therefore\naccuracy and power consumption, in addition to an edge server inference model,\ncan offer a dynamic system in which the allocation of inference models to\ninference jobs is performed according to the current resource conditions.\nTherefore, in this work, we tackle the problem of selectively allocating\ninference models to jobs or offloading them to the edge server to maximize\ninference accuracy under time and energy constraints. This problem is shown to\nbe an instance of the unbounded multidimensional knapsack problem which is\nconsidered a strongly NP-hard problem. We propose a lightweight hybrid genetic\nalgorithm (LGSTO) to solve this problem. We introduce a termination condition\nand neighborhood exploration techniques for faster evolution of populations. We\ncompare LGSTO with the Naive and Dynamic programming solutions. In addition to\nclassic genetic algorithms using different reproduction methods including\nNSGA-II, and finally we compare to other evolutionary methods such as Particle\nswarm optimization (PSO) and Ant colony optimization (ACO). Experiment results\nshow that LGSTO performed 3 times faster than the fastest comparable schemes\nwhile producing schedules with higher average accuracy.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.NE"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2402.16904v1",
    "published_date": "2024-02-24 18:46:06 UTC",
    "updated_date": "2024-02-24 18:46:06 UTC"
  },
  {
    "arxiv_id": "2402.15832v2",
    "title": "Multiple Instance Learning for Glioma Diagnosis using Hematoxylin and Eosin Whole Slide Images: An Indian Cohort Study",
    "authors": [
      "Ekansh Chauhan",
      "Amit Sharma",
      "Megha S Uppin",
      "C. V. Jawahar",
      "P. K. Vinod"
    ],
    "abstract": "The effective management of brain tumors relies on precise typing, subtyping,\nand grading. This study advances patient care with findings from rigorous\nmultiple instance learning experimentations across various feature extractors\nand aggregators in brain tumor histopathology. It establishes new performance\nbenchmarks in glioma subtype classification across multiple datasets, including\na novel dataset focused on the Indian demographic (IPD- Brain), providing a\nvaluable resource for existing research. Using a ResNet-50, pretrained on\nhistopathology datasets for feature extraction, combined with the Double-Tier\nFeature Distillation (DTFD) feature aggregator, our approach achieves\nstate-of-the-art AUCs of 88.08 on IPD-Brain and 95.81 on the TCGA-Brain\ndataset, respectively, for three-way glioma subtype classification. Moreover,\nit establishes new benchmarks in grading and detecting IHC molecular biomarkers\n(IDH1R132H, TP53, ATRX, Ki-67) through H&E stained whole slide images for the\nIPD-Brain dataset. The work also highlights a significant correlation between\nthe model decision-making processes and the diagnostic reasoning of\npathologists, underscoring its capability to mimic professional diagnostic\nprocedures.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2402.15832v2",
    "published_date": "2024-02-24 14:59:19 UTC",
    "updated_date": "2024-03-08 11:31:58 UTC"
  },
  {
    "arxiv_id": "2402.15826v1",
    "title": "Reward Design for Justifiable Sequential Decision-Making",
    "authors": [
      "Aleksa Sukovic",
      "Goran Radanovic"
    ],
    "abstract": "Equipping agents with the capacity to justify made decisions using supporting\nevidence represents a cornerstone of accountable decision-making. Furthermore,\nensuring that justifications are in line with human expectations and societal\nnorms is vital, especially in high-stakes situations such as healthcare. In\nthis work, we propose the use of a debate-based reward model for reinforcement\nlearning agents, where the outcome of a zero-sum debate game quantifies the\njustifiability of a decision in a particular state. This reward model is then\nused to train a justifiable policy, whose decisions can be more easily\ncorroborated with supporting evidence. In the debate game, two argumentative\nagents take turns providing supporting evidence for two competing decisions.\nGiven the proposed evidence, a proxy of a human judge evaluates which decision\nis better justified. We demonstrate the potential of our approach in learning\npolicies for prescribing and justifying treatment decisions of septic patients.\nWe show that augmenting the reward with the feedback signal generated by the\ndebate-based reward model yields policies highly favored by the judge when\ncompared to the policy obtained solely from the environment rewards, while\nhardly sacrificing any performance. Moreover, in terms of the overall\nperformance and justifiability of trained policies, the debate-based feedback\nis comparable to the feedback obtained from an ideal judge proxy that evaluates\ndecisions using the full information encoded in the state. This suggests that\nthe debate game outputs key information contained in states that is most\nrelevant for evaluating decisions, which in turn substantiates the practicality\nof combining our approach with human-in-the-loop evaluations. Lastly, we\nshowcase that agents trained via multi-agent debate learn to propose evidence\nthat is resilient to refutations and closely aligns with human preferences.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2402.15826v1",
    "published_date": "2024-02-24 14:29:30 UTC",
    "updated_date": "2024-02-24 14:29:30 UTC"
  },
  {
    "arxiv_id": "2403.00808v1",
    "title": "IPED: An Implicit Perspective for Relational Triple Extraction based on Diffusion Model",
    "authors": [
      "Jianli Zhao",
      "Changhao Xu",
      "Bin Jiang"
    ],
    "abstract": "Relational triple extraction is a fundamental task in the field of\ninformation extraction, and a promising framework based on table filling has\nrecently gained attention as a potential baseline for entity relation\nextraction. However, inherent shortcomings such as redundant information and\nincomplete triple recognition remain problematic. To address these challenges,\nwe propose an Implicit Perspective for relational triple Extraction based on\nDiffusion model (IPED), an innovative approach for extracting relational\ntriples. Our classifier-free solution adopts an implicit strategy using block\ncoverage to complete the tables, avoiding the limitations of explicit tagging\nmethods. Additionally, we introduce a generative model structure, the\nblock-denoising diffusion model, to collaborate with our implicit perspective\nand effectively circumvent redundant information disruptions. Experimental\nresults on two popular datasets demonstrate that IPED achieves state-of-the-art\nperformance while gaining superior inference speed and low computational\ncomplexity. To support future research, we have made our source code publicly\navailable online.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "12 pages, 4 figures, committed to NAACL 2024",
    "pdf_url": "http://arxiv.org/pdf/2403.00808v1",
    "published_date": "2024-02-24 14:18:11 UTC",
    "updated_date": "2024-02-24 14:18:11 UTC"
  },
  {
    "arxiv_id": "2402.15821v2",
    "title": "Cooperation and Control in Delegation Games",
    "authors": [
      "Oliver Sourbut",
      "Lewis Hammond",
      "Harriet Wood"
    ],
    "abstract": "Many settings of interest involving humans and machines -- from virtual\npersonal assistants to autonomous vehicles -- can naturally be modelled as\nprincipals (humans) delegating to agents (machines), which then interact with\neach other on their principals' behalf. We refer to these multi-principal,\nmulti-agent scenarios as delegation games. In such games, there are two\nimportant failure modes: problems of control (where an agent fails to act in\nline their principal's preferences) and problems of cooperation (where the\nagents fail to work well together). In this paper we formalise and analyse\nthese problems, further breaking them down into issues of alignment (do the\nplayers have similar preferences?) and capabilities (how competent are the\nplayers at satisfying those preferences?). We show -- theoretically and\nempirically -- how these measures determine the principals' welfare, how they\ncan be estimated using limited observations, and thus how they might be used to\nhelp us design more aligned and cooperative AI systems.",
    "categories": [
      "cs.GT",
      "cs.AI",
      "cs.MA"
    ],
    "primary_category": "cs.GT",
    "comment": "Published at IJCAI 2024",
    "pdf_url": "http://arxiv.org/pdf/2402.15821v2",
    "published_date": "2024-02-24 14:17:41 UTC",
    "updated_date": "2024-08-05 22:54:36 UTC"
  },
  {
    "arxiv_id": "2402.15820v1",
    "title": "DART: Depth-Enhanced Accurate and Real-Time Background Matting",
    "authors": [
      "Hanxi Li",
      "Guofeng Li",
      "Bo Li",
      "Lin Wu",
      "Yan Cheng"
    ],
    "abstract": "Matting with a static background, often referred to as ``Background Matting\"\n(BGM), has garnered significant attention within the computer vision community\ndue to its pivotal role in various practical applications like webcasting and\nphoto editing. Nevertheless, achieving highly accurate background matting\nremains a formidable challenge, primarily owing to the limitations inherent in\nconventional RGB images. These limitations manifest in the form of\nsusceptibility to varying lighting conditions and unforeseen shadows.\n  In this paper, we leverage the rich depth information provided by the\nRGB-Depth (RGB-D) cameras to enhance background matting performance in\nreal-time, dubbed DART. Firstly, we adapt the original RGB-based BGM algorithm\nto incorporate depth information. The resulting model's output undergoes\nrefinement through Bayesian inference, incorporating a background depth prior.\nThe posterior prediction is then translated into a \"trimap,\" which is\nsubsequently fed into a state-of-the-art matting algorithm to generate more\nprecise alpha mattes. To ensure real-time matting capabilities, a critical\nrequirement for many real-world applications, we distill the backbone of our\nmodel from a larger and more versatile BGM network. Our experiments demonstrate\nthe superior performance of the proposed method. Moreover, thanks to the\ndistillation operation, our method achieves a remarkable processing speed of 33\nframes per second (fps) on a mid-range edge-computing device. This high\nefficiency underscores DART's immense potential for deployment in mobile\napplications}",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2402.15820v1",
    "published_date": "2024-02-24 14:10:17 UTC",
    "updated_date": "2024-02-24 14:10:17 UTC"
  },
  {
    "arxiv_id": "2402.16901v2",
    "title": "FGBERT: Function-Driven Pre-trained Gene Language Model for Metagenomics",
    "authors": [
      "ChenRui Duan",
      "Zelin Zang",
      "Yongjie Xu",
      "Hang He",
      "Zihan Liu",
      "Siyuan Li",
      "Zijia Song",
      "Ju-Sheng Zheng",
      "Stan Z. Li"
    ],
    "abstract": "Metagenomic data, comprising mixed multi-species genomes, are prevalent in\ndiverse environments like oceans and soils, significantly impacting human\nhealth and ecological functions. However, current research relies on K-mer,\nwhich limits the capture of structurally and functionally relevant gene\ncontexts. Moreover, these approaches struggle with encoding biologically\nmeaningful genes and fail to address the One-to-Many and Many-to-One\nrelationships inherent in metagenomic data. To overcome these challenges, we\nintroduce FGBERT, a novel metagenomic pre-trained model that employs a\nprotein-based gene representation as a context-aware and structure-relevant\ntokenizer. FGBERT incorporates Masked Gene Modeling (MGM) to enhance the\nunderstanding of inter-gene contextual relationships and Triplet Enhanced\nMetagenomic Contrastive Learning (TMC) to elucidate gene sequence-function\nrelationships. Pre-trained on over 100 million metagenomic sequences, FGBERT\ndemonstrates superior performance on metagenomic datasets at four levels,\nspanning gene, functional, bacterial, and environmental levels and ranging from\n1k to 213k input sequences. Case studies of ATP Synthase and Gene Operons\nhighlight FGBERT's capability for functional recognition and its biological\nrelevance in metagenomic research.",
    "categories": [
      "q-bio.GN",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "q-bio.GN",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2402.16901v2",
    "published_date": "2024-02-24 13:13:17 UTC",
    "updated_date": "2024-12-27 06:40:39 UTC"
  },
  {
    "arxiv_id": "2402.15809v2",
    "title": "Empowering Large Language Model Agents through Action Learning",
    "authors": [
      "Haiteng Zhao",
      "Chang Ma",
      "Guoyin Wang",
      "Jing Su",
      "Lingpeng Kong",
      "Jingjing Xu",
      "Zhi-Hong Deng",
      "Hongxia Yang"
    ],
    "abstract": "Large Language Model (LLM) Agents have recently garnered increasing interest\nyet they are limited in their ability to learn from trial and error, a key\nelement of intelligent behavior. In this work, we argue that the capacity to\nlearn new actions from experience is fundamental to the advancement of learning\nin LLM agents. While humans naturally expand their action spaces and develop\nskills through experiential learning, LLM agents typically operate within fixed\naction spaces, limiting their potential for growth. To address these\nchallenges, our study explores open-action learning for language agents. We\nintroduce a framework LearnAct with an iterative learning strategy to create\nand improve actions in the form of Python functions. In each iteration, LLM\nrevises and updates the currently available actions based on the errors\nidentified in unsuccessful training tasks, thereby enhancing action\neffectiveness. Our experimental evaluations across Robotic Planning and\nAlfworld environments reveal that after learning on a few training task\ninstances, our approach to open-action learning markedly improves agent\nperformance for the type of task (by 32 percent in AlfWorld compared to\nReAct+Reflexion, for instance) highlighting the importance of experiential\naction learning in the development of more intelligent LLM agents.",
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.AI",
    "comment": "9 pages",
    "pdf_url": "http://arxiv.org/pdf/2402.15809v2",
    "published_date": "2024-02-24 13:13:04 UTC",
    "updated_date": "2024-08-08 07:05:46 UTC"
  },
  {
    "arxiv_id": "2402.15808v2",
    "title": "Optimal Zero-Shot Detector for Multi-Armed Attacks",
    "authors": [
      "Federica Granese",
      "Marco Romanelli",
      "Pablo Piantanida"
    ],
    "abstract": "This paper explores a scenario in which a malicious actor employs a\nmulti-armed attack strategy to manipulate data samples, offering them various\navenues to introduce noise into the dataset. Our central objective is to\nprotect the data by detecting any alterations to the input. We approach this\ndefensive strategy with utmost caution, operating in an environment where the\ndefender possesses significantly less information compared to the attacker.\nSpecifically, the defender is unable to utilize any data samples for training a\ndefense model or verifying the integrity of the channel. Instead, the defender\nrelies exclusively on a set of pre-existing detectors readily available \"off\nthe shelf\". To tackle this challenge, we derive an innovative\ninformation-theoretic defense approach that optimally aggregates the decisions\nmade by these detectors, eliminating the need for any training data. We further\nexplore a practical use-case scenario for empirical evaluation, where the\nattacker possesses a pre-trained classifier and launches well-known adversarial\nattacks against it. Our experiments highlight the effectiveness of our proposed\nsolution, even in scenarios that deviate from the optimal setup.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CR"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted to appear in the 27th International Conference on Artificial\n  Intelligence and Statistics (AISTATS), May 2nd - May 4th, 2024 This article\n  supersedes arXiv:2302.02216",
    "pdf_url": "http://arxiv.org/pdf/2402.15808v2",
    "published_date": "2024-02-24 13:08:39 UTC",
    "updated_date": "2024-02-27 20:08:16 UTC"
  },
  {
    "arxiv_id": "2402.15796v1",
    "title": "Construction and application of artificial intelligence crowdsourcing map based on multi-track GPS data",
    "authors": [
      "Yong Wang",
      "Yanlin Zhou",
      "Huan Ji",
      "Zheng He",
      "Xinyu Shen"
    ],
    "abstract": "In recent years, the rapid development of high-precision map technology\ncombined with artificial intelligence has ushered in a new development\nopportunity in the field of intelligent vehicles. High-precision map technology\nis an important guarantee for intelligent vehicles to achieve autonomous\ndriving. However, due to the lack of research on high-precision map technology,\nit is difficult to rationally use this technology in the field of intelligent\nvehicles. Therefore, relevant researchers studied a fast and effective\nalgorithm to generate high-precision GPS data from a large number of\nlow-precision GPS trajectory data fusion, and generated several key data points\nto simplify the description of GPS trajectory, and realized the \"crowdsourced\nupdate\" model based on a large number of social vehicles for map data\ncollection came into being. This kind of algorithm has the important\nsignificance to improve the data accuracy, reduce the measurement cost and\nreduce the data storage space. On this basis, this paper analyzes the\nimplementation form of crowdsourcing map, so as to improve the various\ninformation data in the high-precision map according to the actual situation,\nand promote the high-precision map can be reasonably applied to the intelligent\ncar.",
    "categories": [
      "cs.AI",
      "cs.HC"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2402.15796v1",
    "published_date": "2024-02-24 11:54:32 UTC",
    "updated_date": "2024-02-24 11:54:32 UTC"
  },
  {
    "arxiv_id": "2403.00805v1",
    "title": "A New Dynamic Distributed Planning Approach: Application to DPDP Problems",
    "authors": [
      "Zakaria Tolba"
    ],
    "abstract": "In this work, we proposed a new dynamic distributed planning approach that is\nable to take into account the changes that the agent introduces on his set of\nactions to be planned in order to take into account the changes that occur in\nhis environment. Our approach fits into the context of distributed planning for\ndistributed plans where each agent can produce its own plans. According to our\napproach the generation of the plans is based on the satisfaction of the\nconstraints by the use of the genetic algorithms. Our approach is to generate,\na new plan by each agent, whenever there is a change in its set of actions to\nplan. This in order to take into account the new actions introduced in its new\nplan. In this new plan, the agent takes, each time, as a new action set to plan\nall the old un-executed actions of the old plan and the new actions engendered\nby the changes and as a new initial state; the state in which the set of\nactions of the agent undergoes a change. In our work, we used a concrete case\nto illustrate and demonstrate the utility of our approach.",
    "categories": [
      "cs.AI",
      "cs.MA"
    ],
    "primary_category": "cs.AI",
    "comment": "Master's thesis, in French language",
    "pdf_url": "http://arxiv.org/pdf/2403.00805v1",
    "published_date": "2024-02-24 10:06:04 UTC",
    "updated_date": "2024-02-24 10:06:04 UTC"
  },
  {
    "arxiv_id": "2402.15779v1",
    "title": "Cryptanalysis and improvement of multimodal data encryption by machine-learning-based system",
    "authors": [
      "Zakaria Tolba"
    ],
    "abstract": "With the rising popularity of the internet and the widespread use of networks\nand information systems via the cloud and data centers, the privacy and\nsecurity of individuals and organizations have become extremely crucial. In\nthis perspective, encryption consolidates effective technologies that can\neffectively fulfill these requirements by protecting public information\nexchanges. To achieve these aims, the researchers used a wide assortment of\nencryption algorithms to accommodate the varied requirements of this field, as\nwell as focusing on complex mathematical issues during their work to\nsubstantially complicate the encrypted communication mechanism. as much as\npossible to preserve personal information while significantly reducing the\npossibility of attacks. Depending on how complex and distinct the requirements\nestablished by these various applications are, the potential of trying to break\nthem continues to occur, and systems for evaluating and verifying the\ncryptographic algorithms implemented continue to be necessary. The best\napproach to analyzing an encryption algorithm is to identify a practical and\nefficient technique to break it or to learn ways to detect and repair weak\naspects in algorithms, which is known as cryptanalysis. Experts in\ncryptanalysis have discovered several methods for breaking the cipher, such as\ndiscovering a critical vulnerability in mathematical equations to derive the\nsecret key or determining the plaintext from the ciphertext. There are various\nattacks against secure cryptographic algorithms in the literature, and the\nstrategies and mathematical solutions widely employed empower cryptanalysts to\ndemonstrate their findings, identify weaknesses, and diagnose maintenance\nfailures in algorithms.",
    "categories": [
      "cs.CR",
      "cs.AI",
      "cs.IR"
    ],
    "primary_category": "cs.CR",
    "comment": "Doctoral thesis. Keywords: Cryptanalysis, Black-box, Deep learning,\n  Machine learning, Ciphertext, Plaintext, Genetic algorithm, Permutation box,\n  Substitution Box",
    "pdf_url": "http://arxiv.org/pdf/2402.15779v1",
    "published_date": "2024-02-24 10:02:21 UTC",
    "updated_date": "2024-02-24 10:02:21 UTC"
  },
  {
    "arxiv_id": "2402.15770v1",
    "title": "From COBIT to ISO 42001: Evaluating Cybersecurity Frameworks for Opportunities, Risks, and Regulatory Compliance in Commercializing Large Language Models",
    "authors": [
      "Timothy R. McIntosh",
      "Teo Susnjak",
      "Tong Liu",
      "Paul Watters",
      "Raza Nowrozy",
      "Malka N. Halgamuge"
    ],
    "abstract": "This study investigated the integration readiness of four predominant\ncybersecurity Governance, Risk and Compliance (GRC) frameworks - NIST CSF 2.0,\nCOBIT 2019, ISO 27001:2022, and the latest ISO 42001:2023 - for the\nopportunities, risks, and regulatory compliance when adopting Large Language\nModels (LLMs), using qualitative content analysis and expert validation. Our\nanalysis, with both LLMs and human experts in the loop, uncovered potential for\nLLM integration together with inadequacies in LLM risk oversight of those\nframeworks. Comparative gap analysis has highlighted that the new ISO\n42001:2023, specifically designed for Artificial Intelligence (AI) management\nsystems, provided most comprehensive facilitation for LLM opportunities,\nwhereas COBIT 2019 aligned most closely with the impending European Union AI\nAct. Nonetheless, our findings suggested that all evaluated frameworks would\nbenefit from enhancements to more effectively and more comprehensively address\nthe multifaceted risks associated with LLMs, indicating a critical and\ntime-sensitive need for their continuous evolution. We propose integrating\nhuman-expert-in-the-loop validation processes as crucial for enhancing\ncybersecurity frameworks to support secure and compliant LLM integration, and\ndiscuss implications for the continuous evolution of cybersecurity GRC\nframeworks to support the secure integration of LLMs.",
    "categories": [
      "cs.CY",
      "cs.AI"
    ],
    "primary_category": "cs.CY",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2402.15770v1",
    "published_date": "2024-02-24 09:06:25 UTC",
    "updated_date": "2024-02-24 09:06:25 UTC"
  },
  {
    "arxiv_id": "2402.15769v2",
    "title": "GenCode: A Generic Data Augmentation Framework for Boosting Deep Learning-Based Code Understanding",
    "authors": [
      "Zeming Dong",
      "Qiang Hu",
      "Xiaofei Xie",
      "Maxime Cordy",
      "Mike Papadakis",
      "Jianjun Zhao"
    ],
    "abstract": "Pre-trained code models lead the era of code intelligence with multiple\nmodels have been designed with impressive performance. However, one important\nproblem, data augmentation for code data that automatically helps developers\nprepare training data lacks study in this field. In this paper, we introduce a\ngeneric data augmentation framework, GenCode, to enhance the training of code\nunderstanding models. Simply speaking, GenCode follows a\ngeneration-and-selection paradigm to prepare useful training code data.\nSpecifically, it employs code transformation techniques to generate new code\ncandidates first and then selects important ones as the training data by\nimportance metrics. To evaluate the effectiveness of GenCode, we conduct\nexperiments on four code understanding tasks (e.g., code clone detection) and\nthree pre-trained code models (e.g., CodeT5). Compared to the state-of-the-art\n(SOTA) code augmentation method, MixCode, GenCode produces code models with\n2.92% higher accuracy and 4.90% robustness on average.",
    "categories": [
      "cs.SE",
      "cs.AI"
    ],
    "primary_category": "cs.SE",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2402.15769v2",
    "published_date": "2024-02-24 08:57:12 UTC",
    "updated_date": "2024-11-11 09:38:23 UTC"
  },
  {
    "arxiv_id": "2402.15767v1",
    "title": "PhyPlan: Compositional and Adaptive Physical Task Reasoning with Physics-Informed Skill Networks for Robot Manipulators",
    "authors": [
      "Harshil Vagadia",
      "Mudit Chopra",
      "Abhinav Barnawal",
      "Tamajit Banerjee",
      "Shreshth Tuli",
      "Souvik Chakraborty",
      "Rohan Paul"
    ],
    "abstract": "Given the task of positioning a ball-like object to a goal region beyond\ndirect reach, humans can often throw, slide, or rebound objects against the\nwall to attain the goal. However, enabling robots to reason similarly is\nnon-trivial. Existing methods for physical reasoning are data-hungry and\nstruggle with complexity and uncertainty inherent in the real world. This paper\npresents PhyPlan, a novel physics-informed planning framework that combines\nphysics-informed neural networks (PINNs) with modified Monte Carlo Tree Search\n(MCTS) to enable embodied agents to perform dynamic physical tasks. PhyPlan\nleverages PINNs to simulate and predict outcomes of actions in a fast and\naccurate manner and uses MCTS for planning. It dynamically determines whether\nto consult a PINN-based simulator (coarse but fast) or engage directly with the\nactual environment (fine but slow) to determine optimal policy. Evaluation with\nrobots in simulated 3D environments demonstrates the ability of our approach to\nsolve 3D-physical reasoning tasks involving the composition of dynamic skills.\nQuantitatively, PhyPlan excels in several aspects: (i) it achieves lower regret\nwhen learning novel tasks compared to state-of-the-art, (ii) it expedites skill\nlearning and enhances the speed of physical reasoning, (iii) it demonstrates\nhigher data efficiency compared to a physics un-informed approach.",
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "primary_category": "cs.RO",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2402.15767v1",
    "published_date": "2024-02-24 08:51:03 UTC",
    "updated_date": "2024-02-24 08:51:03 UTC"
  },
  {
    "arxiv_id": "2402.15764v2",
    "title": "Look Before You Leap: Problem Elaboration Prompting Improves Mathematical Reasoning in Large Language Models",
    "authors": [
      "Haoran Liao",
      "Jidong Tian",
      "Shaohua Hu",
      "Hao He",
      "Yaohui Jin"
    ],
    "abstract": "Large language models (LLMs) still grapple with complex tasks like\nmathematical reasoning. Despite significant efforts invested in improving\nprefix prompts or reasoning process, the crucial role of problem context might\nhave been neglected. Accurate recognition of inputs is fundamental for solving\nmathematical tasks, as ill-formed problems could potentially mislead LLM's\nreasoning. In this study, we propose a new approach named Problem Elaboration\nPrompting (PEP) to enhance the mathematical capacities of LLMs. Specifically,\nPEP decomposes and elucidates the problem context before reasoning, therefore\nenhancing the context modeling and parsing efficiency. Experiments across\ndatasets and models demonstrate promising performances: (1) PEP demonstrates an\noverall enhancement in various mathematical tasks. For instance, with the\nGPT-3.5 model, PEP exhibits improvements of 9.93% and 8.80% on GSM8k through\ngreedy decoding and self-consistency, respectively. (2) PEP can be easily\nimplemented and integrated with other prompting methods. (3) PEP shows\nparticular strength in handling distraction problems.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2402.15764v2",
    "published_date": "2024-02-24 08:40:30 UTC",
    "updated_date": "2024-03-27 01:23:58 UTC"
  },
  {
    "arxiv_id": "2402.17786v1",
    "title": "Stepwise Self-Consistent Mathematical Reasoning with Large Language Models",
    "authors": [
      "Zilong Zhao",
      "Yao Rong",
      "Dongyang Guo",
      "Emek Gözlüklü",
      "Emir Gülboy",
      "Enkelejda Kasneci"
    ],
    "abstract": "Using Large Language Models for complex mathematical reasoning is difficult,\nprimarily due to the complexity of multi-step reasoning. The main challenges of\nthis process include (1) selecting critical intermediate results to advance the\nprocedure, and (2) limited exploration of potential solutions. To address these\nissues, we introduce a novel algorithm, namely Stepwise Self-Consistent\nChain-of-Thought (SSC-CoT). SSC-CoT employs a strategy of selecting\nintermediate steps based on the intersection of various reasoning chains.\nAdditionally, SSC-CoT enables the model to discover critical intermediate steps\nby querying a knowledge graph comprising relevant domain knowledge. To validate\nSSC-CoT, we present a new dataset, TriMaster100, tailored for complex\ntrigonometry problems. This dataset contains 100 questions, with each solution\nbroken down into scored intermediate steps, facilitating a comprehensive\nevaluation of the mathematical reasoning process. On TriMaster100, SSC-CoT\ntriples the effectiveness of the state-of-the-art methods. Furthermore, we\nbenchmark SSC-CoT on the widely recognized complex mathematical question\ndataset, MATH level 5, and it surpasses the second-best method by 7.2% in\naccuracy. Code and the TriMaster100 dataset can be found at:\nhttps://github.com/zhao-zilong/ssc-cot.",
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2402.17786v1",
    "published_date": "2024-02-24 08:22:39 UTC",
    "updated_date": "2024-02-24 08:22:39 UTC"
  },
  {
    "arxiv_id": "2402.15761v3",
    "title": "Res-VMamba: Fine-Grained Food Category Visual Classification Using Selective State Space Models with Deep Residual Learning",
    "authors": [
      "Chi-Sheng Chen",
      "Guan-Ying Chen",
      "Dong Zhou",
      "Di Jiang",
      "Dai-Shi Chen"
    ],
    "abstract": "Food classification is the foundation for developing food vision tasks and\nplays a key role in the burgeoning field of computational nutrition. Due to the\ncomplexity of food requiring fine-grained classification, recent academic\nresearch mainly modifies Convolutional Neural Networks (CNNs) and/or Vision\nTransformers (ViTs) to perform food category classification. However, to learn\nfine-grained features, the CNN backbone needs additional structural design,\nwhereas ViT, containing the self-attention module, has increased computational\ncomplexity. In recent months, a new Sequence State Space (S4) model, through a\nSelection mechanism and computation with a Scan (S6), colloquially termed\nMamba, has demonstrated superior performance and computation efficiency\ncompared to the Transformer architecture. The VMamba model, which incorporates\nthe Mamba mechanism into image tasks (such as classification), currently\nestablishes the state-of-the-art (SOTA) on the ImageNet dataset. In this\nresearch, we introduce an academically underestimated food dataset CNFOOD-241,\nand pioneer the integration of a residual learning framework within the VMamba\nmodel to concurrently harness both global and local state features inherent in\nthe original VMamba architectural design. The research results show that VMamba\nsurpasses current SOTA models in fine-grained and food classification. The\nproposed Res-VMamba further improves the classification accuracy to 79.54\\%\nwithout pretrained weight. Our findings elucidate that our proposed methodology\nestablishes a new benchmark for SOTA performance in food recognition on the\nCNFOOD-241 dataset. The code can be obtained on GitHub:\nhttps://github.com/ChiShengChen/ResVMamba.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "14 pages, 3 figures",
    "pdf_url": "http://arxiv.org/pdf/2402.15761v3",
    "published_date": "2024-02-24 08:20:39 UTC",
    "updated_date": "2024-09-06 16:23:26 UTC"
  },
  {
    "arxiv_id": "2402.15759v2",
    "title": "TV-SAM: Increasing Zero-Shot Segmentation Performance on Multimodal Medical Images Using GPT-4 Generated Descriptive Prompts Without Human Annotation",
    "authors": [
      "Zekun Jiang",
      "Dongjie Cheng",
      "Ziyuan Qin",
      "Jun Gao",
      "Qicheng Lao",
      "Abdullaev Bakhrom Ismoilovich",
      "Urazboev Gayrat",
      "Yuldashov Elyorbek",
      "Bekchanov Habibullo",
      "Defu Tang",
      "LinJing Wei",
      "Kang Li",
      "Le Zhang"
    ],
    "abstract": "This study presents a novel multimodal medical image zero-shot segmentation\nalgorithm named the text-visual-prompt segment anything model (TV-SAM) without\nany manual annotations. The TV-SAM incorporates and integrates the large\nlanguage model GPT-4, the vision language model GLIP, and the SAM to\nautonomously generate descriptive text prompts and visual bounding box prompts\nfrom medical images, thereby enhancing the SAM's capability for zero-shot\nsegmentation. Comprehensive evaluations are implemented on seven public\ndatasets encompassing eight imaging modalities to demonstrate that TV-SAM can\neffectively segment unseen targets across various modalities without additional\ntraining. TV-SAM significantly outperforms SAM AUTO and GSAM, closely matching\nthe performance of SAM BBOX with gold standard bounding box prompts and\nsurpasses the state-of-the-art methods on specific datasets such as ISIC and\nWBC. The study indicates that TV-SAM serves as an effective multimodal medical\nimage zero-shot segmentation algorithm, highlighting the significant\ncontribution of GPT-4 to zero-shot segmentation. By integrating foundational\nmodels such as GPT-4, GLIP, and SAM, the ability to address complex problems in\nspecialized domains can be enhanced.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "13 pages, 5 figures, 4 tables, accepted by BDMA Journal",
    "pdf_url": "http://arxiv.org/pdf/2402.15759v2",
    "published_date": "2024-02-24 08:10:54 UTC",
    "updated_date": "2024-10-14 14:41:52 UTC"
  },
  {
    "arxiv_id": "2402.15758v2",
    "title": "Chimera: A Lossless Decoding Method for Accelerating Large Language Models Inference by Fusing all Tokens",
    "authors": [
      "Ziqian Zeng",
      "Jiahong Yu",
      "Qianshi Pang",
      "Zihao Wang",
      "Huiping Zhuang",
      "Hongen Shao",
      "Xiaofeng Zou"
    ],
    "abstract": "Large language models (LLMs) have demonstrated remarkable capabilities across\nvarious tasks. However, their widespread application is hindered by the\nresource-intensive decoding process. To address this challenge, current\napproaches have incorporated additional decoding heads to enable parallel\nprediction of multiple subsequent tokens, thereby achieving inference\nacceleration. Nevertheless, the accuracy of these decoding heads falls short of\nthe auto-regressive decoding approach.\n  In light of these limitations, we propose Chimera, a novel framework\nspecifically designed for speculative sampling. Within this framework, we\nintroduce a lightweight draft model that effectively utilizes previously\ngenerated tokens to predict subsequent words. To ensure both accuracy and\nefficiency, we present two strategies within the lightweight draft model.\nFirstly, we focus on capturing short-range dependencies at the bottom layer.\nSecondly, we leverage the readily available representations from the original\nLLM.Through empirical evaluation on the Vicuna and LlaMA-2 series, Chimera\ndemonstrates impressive results, achieving an average latency speedup ratio of\n2.7x compared to the vanilla auto-regressive decoding approach. This highlights\nthe potential of our proposed framework in significantly improving the\nefficiency of large language models during the decoding process.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2402.15758v2",
    "published_date": "2024-02-24 08:10:39 UTC",
    "updated_date": "2024-04-18 16:23:16 UTC"
  },
  {
    "arxiv_id": "2402.15757v1",
    "title": "Batch Active Learning of Reward Functions from Human Preferences",
    "authors": [
      "Erdem Bıyık",
      "Nima Anari",
      "Dorsa Sadigh"
    ],
    "abstract": "Data generation and labeling are often expensive in robot learning.\nPreference-based learning is a concept that enables reliable labeling by\nquerying users with preference questions. Active querying methods are commonly\nemployed in preference-based learning to generate more informative data at the\nexpense of parallelization and computation time. In this paper, we develop a\nset of novel algorithms, batch active preference-based learning methods, that\nenable efficient learning of reward functions using as few data samples as\npossible while still having short query generation times and also retaining\nparallelizability. We introduce a method based on determinantal point processes\n(DPP) for active batch generation and several heuristic-based alternatives.\nFinally, we present our experimental results for a variety of robotics tasks in\nsimulation. Our results suggest that our batch active learning algorithm\nrequires only a few queries that are computed in a short amount of time. We\nshowcase one of our algorithms in a study to learn human users' preferences.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.RO",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "To appear in ACM Transactions on Human-Robot Interaction (THRI). 27\n  pages, 12 figures, 2 tables. arXiv admin note: text overlap with\n  arXiv:1810.04303",
    "pdf_url": "http://arxiv.org/pdf/2402.15757v1",
    "published_date": "2024-02-24 08:07:48 UTC",
    "updated_date": "2024-02-24 08:07:48 UTC"
  },
  {
    "arxiv_id": "2402.15751v1",
    "title": "Sparse MeZO: Less Parameters for Better Performance in Zeroth-Order LLM Fine-Tuning",
    "authors": [
      "Yong Liu",
      "Zirui Zhu",
      "Chaoyu Gong",
      "Minhao Cheng",
      "Cho-Jui Hsieh",
      "Yang You"
    ],
    "abstract": "While fine-tuning large language models (LLMs) for specific tasks often\nyields impressive results, it comes at the cost of memory inefficiency due to\nback-propagation in gradient-based training. Memory-efficient Zeroth-order\n(MeZO) optimizers, recently proposed to address this issue, only require\nforward passes during training, making them more memory-friendly. However, the\nquality of gradient estimates in zeroth order optimization often depends on the\ndata dimensionality, potentially explaining why MeZO still exhibits significant\nperformance drops compared to standard fine-tuning across various tasks.\nInspired by the success of Parameter-Efficient Fine-Tuning (PEFT), this paper\nintroduces Sparse MeZO, a novel memory-efficient zeroth-order optimization\napproach that applies ZO only to a carefully chosen subset of parameters. We\npropose a simple yet effective parameter selection scheme that yields\nsignificant performance gains with Sparse-MeZO. Additionally, we develop a\nmemory-optimized implementation for sparse masking, ensuring the algorithm\nrequires only inference-level memory consumption, allowing Sparse-MeZO to\nfine-tune LLaMA-30b on a single A100 GPU. Experimental results illustrate that\nSparse-MeZO consistently improves both performance and convergence speed over\nMeZO without any overhead. For example, it achieves a 9\\% absolute accuracy\nimprovement and 3.5x speedup over MeZO on the RTE task.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2402.15751v1",
    "published_date": "2024-02-24 07:22:04 UTC",
    "updated_date": "2024-02-24 07:22:04 UTC"
  },
  {
    "arxiv_id": "2402.15746v1",
    "title": "Intelligent Director: An Automatic Framework for Dynamic Visual Composition using ChatGPT",
    "authors": [
      "Sixiao Zheng",
      "Jingyang Huo",
      "Yu Wang",
      "Yanwei Fu"
    ],
    "abstract": "With the rise of short video platforms represented by TikTok, the trend of\nusers expressing their creativity through photos and videos has increased\ndramatically. However, ordinary users lack the professional skills to produce\nhigh-quality videos using professional creation software. To meet the demand\nfor intelligent and user-friendly video creation tools, we propose the Dynamic\nVisual Composition (DVC) task, an interesting and challenging task that aims to\nautomatically integrate various media elements based on user requirements and\ncreate storytelling videos. We propose an Intelligent Director framework,\nutilizing LENS to generate descriptions for images and video frames and\ncombining ChatGPT to generate coherent captions while recommending appropriate\nmusic names. Then, the best-matched music is obtained through music retrieval.\nThen, materials such as captions, images, videos, and music are integrated to\nseamlessly synthesize the video. Finally, we apply AnimeGANv2 for style\ntransfer. We construct UCF101-DVC and Personal Album datasets and verified the\neffectiveness of our framework in solving DVC through qualitative and\nquantitative comparisons, along with user studies, demonstrating its\nsubstantial potential.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.MM"
    ],
    "primary_category": "cs.CV",
    "comment": "Project Page: https://sixiaozheng.github.io/IntelligentDirector/",
    "pdf_url": "http://arxiv.org/pdf/2402.15746v1",
    "published_date": "2024-02-24 06:58:15 UTC",
    "updated_date": "2024-02-24 06:58:15 UTC"
  },
  {
    "arxiv_id": "2402.15745v2",
    "title": "GAOKAO-MM: A Chinese Human-Level Benchmark for Multimodal Models Evaluation",
    "authors": [
      "Yi Zong",
      "Xipeng Qiu"
    ],
    "abstract": "The Large Vision-Language Models (LVLMs) have demonstrated great abilities in\nimage perception and language understanding. However, existing multimodal\nbenchmarks focus on primary perception abilities and commonsense knowledge\nwhich are insufficient to reflect the comprehensive capabilities of LVLMs. We\npropose GAOKAO-MM, a multimodal benchmark based on the Chinese College Entrance\nExamination (GAOKAO), comprising of 8 subjects and 12 types of images, such as\ndiagrams, function graphs, maps and photos. GAOKAO-MM derives from native\nChinese context and sets human-level requirements for the model's abilities,\nincluding perception, understanding, knowledge and reasoning. We evaluate 10\nLVLMs and find that the accuracies of all of them are lower than 50%, with\nGPT-4-Vison (48.1%), Qwen-VL-Plus (41.2%) and Gemini-Pro-Vision (35.1%) ranking\nin the top three positions. The results of our multi-dimension analysis\nindicate that LVLMs have moderate distance towards Artificial General\nIntelligence (AGI) and provide insights facilitating the development of\nmultilingual LVLMs.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2402.15745v2",
    "published_date": "2024-02-24 06:57:15 UTC",
    "updated_date": "2024-08-06 15:28:30 UTC"
  },
  {
    "arxiv_id": "2402.16899v3",
    "title": "A priori Estimates for Deep Residual Network in Continuous-time Reinforcement Learning",
    "authors": [
      "Shuyu Yin",
      "Qixuan Zhou",
      "Fei Wen",
      "Tao Luo"
    ],
    "abstract": "Deep reinforcement learning excels in numerous large-scale practical\napplications. However, existing performance analyses ignores the unique\ncharacteristics of continuous-time control problems, is unable to directly\nestimate the generalization error of the Bellman optimal loss and require a\nboundedness assumption. Our work focuses on continuous-time control problems\nand proposes a method that is applicable to all such problems where the\ntransition function satisfies semi-group and Lipschitz properties. Under this\nmethod, we can directly analyze the \\emph{a priori} generalization error of the\nBellman optimal loss. The core of this method lies in two transformations of\nthe loss function. To complete the transformation, we propose a decomposition\nmethod for the maximum operator. Additionally, this analysis method does not\nrequire a boundedness assumption. Finally, we obtain an \\emph{a priori}\ngeneralization error without the curse of dimensionality.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2402.16899v3",
    "published_date": "2024-02-24 06:31:43 UTC",
    "updated_date": "2024-03-07 05:33:40 UTC"
  },
  {
    "arxiv_id": "2403.14652v1",
    "title": "MemeCraft: Contextual and Stance-Driven Multimodal Meme Generation",
    "authors": [
      "Han Wang",
      "Roy Ka-Wei Lee"
    ],
    "abstract": "Online memes have emerged as powerful digital cultural artifacts in the age\nof social media, offering not only humor but also platforms for political\ndiscourse, social critique, and information dissemination. Their extensive\nreach and influence in shaping online communities' sentiments make them\ninvaluable tools for campaigning and promoting ideologies. Despite the\ndevelopment of several meme-generation tools, there remains a gap in their\nsystematic evaluation and their ability to effectively communicate ideologies.\nAddressing this, we introduce MemeCraft, an innovative meme generator that\nleverages large language models (LLMs) and visual language models (VLMs) to\nproduce memes advocating specific social movements. MemeCraft presents an\nend-to-end pipeline, transforming user prompts into compelling multimodal memes\nwithout manual intervention. Conscious of the misuse potential in creating\ndivisive content, an intrinsic safety mechanism is embedded to curb hateful\nmeme production.",
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.CL",
      "cs.MM",
      "I.2.7; I.2.10"
    ],
    "primary_category": "cs.CY",
    "comment": "8 pages, 7 figures, ACM MM 2024",
    "pdf_url": "http://arxiv.org/pdf/2403.14652v1",
    "published_date": "2024-02-24 06:14:34 UTC",
    "updated_date": "2024-02-24 06:14:34 UTC"
  },
  {
    "arxiv_id": "2402.15729v3",
    "title": "How Do Humans Write Code? Large Models Do It the Same Way Too",
    "authors": [
      "Long Li",
      "Xuzheng He",
      "Haozhe Wang",
      "Linlin Wang",
      "Liang He"
    ],
    "abstract": "Program-of-Thought (PoT) replaces natural language-based Chain-of-Thought\n(CoT) as the most popular method in Large Language Models (LLMs) mathematical\nreasoning tasks by utilizing external tool calls to circumvent computational\nerrors. However, our evaluation of the GPT-4 and Llama series reveals that\nusing PoT introduces more reasoning errors, such as incorrect formulas or\nflawed logic, compared to CoT. To address this issue, we propose Human-Think\nLanguage (HTL), which leverages a suite of strategies that help integrate PoT\nand CoT, encompassing: (1) a new generation paradigm that uses full CoT\nreasoning to control code generation. (2) Focus Attention, that directs model\nattention to the CoT reasoning during PoT to generate more logical code. (3)\nreinforcement learning that utilizes the accuracy of both CoT and PoT responses\nas rewards to prevent repetitive reasoning steps in LLMs when solving difficult\nmath problems. Our method achieves an average improvement of 6.5% on the\nLlama-Base model and 4.3% on the Mistral-Base model across 8 mathematical\ncalculation datasets. It also shows significant effectiveness on five\nout-of-domain datasets by controlling the model's information flow, exhibiting\nstrong transferability. Additionally, HTL shows the most significant\nimprovement in non-mathematical natural language inference task, contributing\nto a unified reasoning task framework",
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.PL"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2402.15729v3",
    "published_date": "2024-02-24 05:40:01 UTC",
    "updated_date": "2024-10-16 08:04:46 UTC"
  },
  {
    "arxiv_id": "2402.15727v2",
    "title": "LLMs Can Defend Themselves Against Jailbreaking in a Practical Manner: A Vision Paper",
    "authors": [
      "Daoyuan Wu",
      "Shuai Wang",
      "Yang Liu",
      "Ning Liu"
    ],
    "abstract": "Jailbreaking is an emerging adversarial attack that bypasses the safety\nalignment deployed in off-the-shelf large language models (LLMs). A\nconsiderable amount of research exists proposing more effective jailbreak\nattacks, including the recent Greedy Coordinate Gradient (GCG) attack,\njailbreak template-based attacks such as using \"Do-Anything-Now\" (DAN), and\nmultilingual jailbreak. In contrast, the defensive side has been relatively\nless explored. This paper proposes a lightweight yet practical defense called\nSELFDEFEND, which can defend against all existing jailbreak attacks with\nminimal delay for jailbreak prompts and negligible delay for normal user\nprompts. Our key insight is that regardless of the kind of jailbreak strategies\nemployed, they eventually need to include a harmful prompt (e.g., \"how to make\na bomb\") in the prompt sent to LLMs, and we found that existing LLMs can\neffectively recognize such harmful prompts that violate their safety policies.\nBased on this insight, we design a shadow stack that concurrently checks\nwhether a harmful prompt exists in the user prompt and triggers a checkpoint in\nthe normal stack once a token of \"No\" or a harmful prompt is output. The latter\ncould also generate an explainable LLM response to adversarial prompts. We\ndemonstrate our idea of SELFDEFEND works in various jailbreak scenarios through\nmanual analysis in GPT-3.5/4. We also list three future directions to further\nenhance SELFDEFEND.",
    "categories": [
      "cs.CR",
      "cs.AI"
    ],
    "primary_category": "cs.CR",
    "comment": "Fixed the bibliography reference issue in our LLM jailbreak defense\n  vision paper submitted on 24 Feb 2024",
    "pdf_url": "http://arxiv.org/pdf/2402.15727v2",
    "published_date": "2024-02-24 05:34:43 UTC",
    "updated_date": "2024-03-04 05:37:40 UTC"
  },
  {
    "arxiv_id": "2402.15721v2",
    "title": "Hal-Eval: A Universal and Fine-grained Hallucination Evaluation Framework for Large Vision Language Models",
    "authors": [
      "Chaoya Jiang",
      "Hongrui Jia",
      "Wei Ye",
      "Mengfan Dong",
      "Haiyang Xu",
      "Ming Yan",
      "Ji Zhang",
      "Shikun Zhang"
    ],
    "abstract": "Large Vision Language Models exhibit remarkable capabilities but struggle\nwith hallucinations inconsistencies between images and their descriptions.\nPrevious hallucination evaluation studies on LVLMs have identified\nhallucinations in terms of objects, attributes, and relations but overlooked\ncomplex hallucinations that create an entire narrative around a fictional\nentity. In this paper, we introduce a refined taxonomy of hallucinations,\nfeaturing a new category: Event Hallucination. We then utilize advanced LLMs to\ngenerate and filter fine grained hallucinatory data consisting of various types\nof hallucinations, with a particular focus on event hallucinations, laying the\ngroundwork for integrating discriminative and generative evaluation methods\nwithin our universal evaluation framework. The proposed benchmark distinctively\nassesses LVLMs ability to tackle a broad spectrum of hallucinations, making it\na reliable and comprehensive tool for gauging LVLMs efficacy in handling\nhallucinations. We will release our code and data.",
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.AI",
    "comment": "Accepted by ACM MM 2024",
    "pdf_url": "http://arxiv.org/pdf/2402.15721v2",
    "published_date": "2024-02-24 05:14:52 UTC",
    "updated_date": "2024-11-08 05:08:43 UTC"
  },
  {
    "arxiv_id": "2402.17785v2",
    "title": "ByteComposer: a Human-like Melody Composition Method based on Language Model Agent",
    "authors": [
      "Xia Liang",
      "Xingjian Du",
      "Jiaju Lin",
      "Pei Zou",
      "Yuan Wan",
      "Bilei Zhu"
    ],
    "abstract": "Large Language Models (LLM) have shown encouraging progress in multimodal\nunderstanding and generation tasks. However, how to design a human-aligned and\ninterpretable melody composition system is still under-explored. To solve this\nproblem, we propose ByteComposer, an agent framework emulating a human's\ncreative pipeline in four separate steps : \"Conception Analysis - Draft\nComposition - Self-Evaluation and Modification - Aesthetic Selection\". This\nframework seamlessly blends the interactive and knowledge-understanding\nfeatures of LLMs with existing symbolic music generation models, thereby\nachieving a melody composition agent comparable to human creators. We conduct\nextensive experiments on GPT4 and several open-source large language models,\nwhich substantiate our framework's effectiveness. Furthermore, professional\nmusic composers were engaged in multi-dimensional evaluations, the final\nresults demonstrated that across various facets of music composition,\nByteComposer agent attains the level of a novice melody composer.",
    "categories": [
      "cs.SD",
      "cs.AI",
      "eess.AS"
    ],
    "primary_category": "cs.SD",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2402.17785v2",
    "published_date": "2024-02-24 04:35:07 UTC",
    "updated_date": "2024-03-07 00:32:27 UTC"
  },
  {
    "arxiv_id": "2402.15713v1",
    "title": "Making Pre-trained Language Models Better Continual Few-Shot Relation Extractors",
    "authors": [
      "Shengkun Ma",
      "Jiale Han",
      "Yi Liang",
      "Bo Cheng"
    ],
    "abstract": "Continual Few-shot Relation Extraction (CFRE) is a practical problem that\nrequires the model to continuously learn novel relations while avoiding\nforgetting old ones with few labeled training data. The primary challenges are\ncatastrophic forgetting and overfitting. This paper harnesses prompt learning\nto explore the implicit capabilities of pre-trained language models to address\nthe above two challenges, thereby making language models better continual\nfew-shot relation extractors. Specifically, we propose a Contrastive Prompt\nLearning framework, which designs prompt representation to acquire more\ngeneralized knowledge that can be easily adapted to old and new categories, and\nmargin-based contrastive learning to focus more on hard samples, therefore\nalleviating catastrophic forgetting and overfitting issues. To further remedy\noverfitting in low-resource scenarios, we introduce an effective memory\naugmentation strategy that employs well-crafted prompts to guide ChatGPT in\ngenerating diverse samples. Extensive experiments demonstrate that our method\noutperforms state-of-the-art methods by a large margin and significantly\nmitigates catastrophic forgetting and overfitting in low-resource scenarios.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted as COLING2024",
    "pdf_url": "http://arxiv.org/pdf/2402.15713v1",
    "published_date": "2024-02-24 04:32:44 UTC",
    "updated_date": "2024-02-24 04:32:44 UTC"
  },
  {
    "arxiv_id": "2402.17783v1",
    "title": "BagStacking: An Integrated Ensemble Learning Approach for Freezing of Gait Detection in Parkinson's Disease",
    "authors": [
      "Seffi Cohen",
      "Lior Rokach"
    ],
    "abstract": "This paper introduces BagStacking, a novel ensemble learning method designed\nto enhance the detection of Freezing of Gait (FOG) in Parkinson's Disease (PD)\nby using a lower-back sensor to track acceleration. Building on the principles\nof bagging and stacking, BagStacking aims to achieve the variance reduction\nbenefit of bagging's bootstrap sampling while also learning sophisticated\nblending through stacking. The method involves training a set of base models on\nbootstrap samples from the training data, followed by a meta-learner trained on\nthe base model outputs and true labels to find an optimal aggregation scheme.\nThe experimental evaluation demonstrates significant improvements over other\nstate-of-the-art machine learning methods on the validation set. Specifically,\nBagStacking achieved a MAP score of 0.306, outperforming LightGBM (0.234) and\nclassic Stacking (0.286). Additionally, the run-time of BagStacking was\nmeasured at 3828 seconds, illustrating an efficient approach compared to\nRegular Stacking's 8350 seconds. BagStacking presents a promising direction for\nhandling the inherent variability in FOG detection data, offering a robust and\nscalable solution to improve patient care in PD.",
    "categories": [
      "eess.SP",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "eess.SP",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2402.17783v1",
    "published_date": "2024-02-24 04:13:48 UTC",
    "updated_date": "2024-02-24 04:13:48 UTC"
  },
  {
    "arxiv_id": "2402.15708v2",
    "title": "Query Augmentation by Decoding Semantics from Brain Signals",
    "authors": [
      "Ziyi Ye",
      "Jingtao Zhan",
      "Qingyao Ai",
      "Yiqun Liu",
      "Maarten de Rijke",
      "Christina Lioma",
      "Tuukka Ruotsalo"
    ],
    "abstract": "Query augmentation is a crucial technique for refining semantically imprecise\nqueries. Traditionally, query augmentation relies on extracting information\nfrom initially retrieved, potentially relevant documents. If the quality of the\ninitially retrieved documents is low, then the effectiveness of query\naugmentation would be limited as well. We propose Brain-Aug, which enhances a\nquery by incorporating semantic information decoded from brain signals.\nBrainAug generates the continuation of the original query with a prompt\nconstructed with brain signal information and a ranking-oriented inference\napproach. Experimental results on fMRI (functional magnetic resonance imaging)\ndatasets show that Brain-Aug produces semantically more accurate queries,\nleading to improved document ranking performance. Such improvement brought by\nbrain signals is particularly notable for ambiguous queries.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.IR"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2402.15708v2",
    "published_date": "2024-02-24 04:08:51 UTC",
    "updated_date": "2024-03-03 09:18:07 UTC"
  },
  {
    "arxiv_id": "2402.16898v2",
    "title": "MIM-Reasoner: Learning with Theoretical Guarantees for Multiplex Influence Maximization",
    "authors": [
      "Nguyen Do",
      "Tanmoy Chowdhury",
      "Chen Ling",
      "Liang Zhao",
      "My T. Thai"
    ],
    "abstract": "Multiplex influence maximization (MIM) asks us to identify a set of seed\nusers such as to maximize the expected number of influenced users in a\nmultiplex network. MIM has been one of central research topics, especially in\nnowadays social networking landscape where users participate in multiple online\nsocial networks (OSNs) and their influences can propagate among several OSNs\nsimultaneously. Although there exist a couple combinatorial algorithms to MIM,\nlearning-based solutions have been desired due to its generalization ability to\nheterogeneous networks and their diversified propagation characteristics. In\nthis paper, we introduce MIM-Reasoner, coupling reinforcement learning with\nprobabilistic graphical model, which effectively captures the complex\npropagation process within and between layers of a given multiplex network,\nthereby tackling the most challenging problem in MIM. We establish a\ntheoretical guarantee for MIM-Reasoner as well as conduct extensive analyses on\nboth synthetic and real-world datasets to validate our MIM-Reasoner's\nperformance.",
    "categories": [
      "cs.SI",
      "cs.AI",
      "cs.LG",
      "math.PR",
      "stat.ML"
    ],
    "primary_category": "cs.SI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2402.16898v2",
    "published_date": "2024-02-24 03:48:22 UTC",
    "updated_date": "2024-03-10 07:35:15 UTC"
  },
  {
    "arxiv_id": "2402.16897v2",
    "title": "Reliable Conflictive Multi-View Learning",
    "authors": [
      "Cai Xu",
      "Jiajun Si",
      "Ziyu Guan",
      "Wei Zhao",
      "Yue Wu",
      "Xiyue Gao"
    ],
    "abstract": "Multi-view learning aims to combine multiple features to achieve more\ncomprehensive descriptions of data. Most previous works assume that multiple\nviews are strictly aligned. However, real-world multi-view data may contain\nlow-quality conflictive instances, which show conflictive information in\ndifferent views. Previous methods for this problem mainly focus on eliminating\nthe conflictive data instances by removing them or replacing conflictive views.\nNevertheless, real-world applications usually require making decisions for\nconflictive instances rather than only eliminating them. To solve this, we\npoint out a new Reliable Conflictive Multi-view Learning (RCML) problem, which\nrequires the model to provide decision results and attached reliabilities for\nconflictive multi-view data. We develop an Evidential Conflictive Multi-view\nLearning (ECML) method for this problem. ECML first learns view-specific\nevidence, which could be termed as the amount of support to each category\ncollected from data. Then, we can construct view-specific opinions consisting\nof decision results and reliability. In the multi-view fusion stage, we propose\na conflictive opinion aggregation strategy and theoretically prove this\nstrategy can exactly model the relation of multi-view common and view-specific\nreliabilities. Experiments performed on 6 datasets verify the effectiveness of\nECML.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "9 pages and to be appeared in AAAI2024",
    "pdf_url": "http://arxiv.org/pdf/2402.16897v2",
    "published_date": "2024-02-24 03:47:06 UTC",
    "updated_date": "2024-02-28 09:58:46 UTC"
  },
  {
    "arxiv_id": "2402.15703v1",
    "title": "Is Offline Decision Making Possible with Only Few Samples? Reliable Decisions in Data-Starved Bandits via Trust Region Enhancement",
    "authors": [
      "Ruiqi Zhang",
      "Yuexiang Zhai",
      "Andrea Zanette"
    ],
    "abstract": "What can an agent learn in a stochastic Multi-Armed Bandit (MAB) problem from\na dataset that contains just a single sample for each arm? Surprisingly, in\nthis work, we demonstrate that even in such a data-starved setting it may still\nbe possible to find a policy competitive with the optimal one. This paves the\nway to reliable decision-making in settings where critical decisions must be\nmade by relying only on a handful of samples.\n  Our analysis reveals that \\emph{stochastic policies can be substantially\nbetter} than deterministic ones for offline decision-making. Focusing on\noffline multi-armed bandits, we design an algorithm called Trust Region of\nUncertainty for Stochastic policy enhancemenT (TRUST) which is quite different\nfrom the predominant value-based lower confidence bound approach. Its design is\nenabled by localization laws, critical radii, and relative pessimism. We prove\nthat its sample complexity is comparable to that of LCB on minimax problems\nwhile being substantially lower on problems with very few samples.\n  Finally, we consider an application to offline reinforcement learning in the\nspecial case where the logging policies are known.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "22 pages",
    "pdf_url": "http://arxiv.org/pdf/2402.15703v1",
    "published_date": "2024-02-24 03:41:09 UTC",
    "updated_date": "2024-02-24 03:41:09 UTC"
  },
  {
    "arxiv_id": "2402.15700v1",
    "title": "CoRelation: Boosting Automatic ICD Coding Through Contextualized Code Relation Learning",
    "authors": [
      "Junyu Luo",
      "Xiaochen Wang",
      "Jiaqi Wang",
      "Aofei Chang",
      "Yaqing Wang",
      "Fenglong Ma"
    ],
    "abstract": "Automatic International Classification of Diseases (ICD) coding plays a\ncrucial role in the extraction of relevant information from clinical notes for\nproper recording and billing. One of the most important directions for boosting\nthe performance of automatic ICD coding is modeling ICD code relations.\nHowever, current methods insufficiently model the intricate relationships among\nICD codes and often overlook the importance of context in clinical notes. In\nthis paper, we propose a novel approach, a contextualized and flexible\nframework, to enhance the learning of ICD code representations. Our approach,\nunlike existing methods, employs a dependent learning paradigm that considers\nthe context of clinical notes in modeling all possible code relations. We\nevaluate our approach on six public ICD coding datasets and the experimental\nresults demonstrate the effectiveness of our approach compared to\nstate-of-the-art baselines.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.LG",
    "comment": "LREC-Coling 2024",
    "pdf_url": "http://arxiv.org/pdf/2402.15700v1",
    "published_date": "2024-02-24 03:25:28 UTC",
    "updated_date": "2024-02-24 03:25:28 UTC"
  },
  {
    "arxiv_id": "2402.15690v1",
    "title": "Foot In The Door: Understanding Large Language Model Jailbreaking via Cognitive Psychology",
    "authors": [
      "Zhenhua Wang",
      "Wei Xie",
      "Baosheng Wang",
      "Enze Wang",
      "Zhiwen Gui",
      "Shuoyoucheng Ma",
      "Kai Chen"
    ],
    "abstract": "Large Language Models (LLMs) have gradually become the gateway for people to\nacquire new knowledge. However, attackers can break the model's security\nprotection (\"jail\") to access restricted information, which is called\n\"jailbreaking.\" Previous studies have shown the weakness of current LLMs when\nconfronted with such jailbreaking attacks. Nevertheless, comprehension of the\nintrinsic decision-making mechanism within the LLMs upon receipt of jailbreak\nprompts is noticeably lacking. Our research provides a psychological\nexplanation of the jailbreak prompts. Drawing on cognitive consistency theory,\nwe argue that the key to jailbreak is guiding the LLM to achieve cognitive\ncoordination in an erroneous direction. Further, we propose an automatic\nblack-box jailbreaking method based on the Foot-in-the-Door (FITD) technique.\nThis method progressively induces the model to answer harmful questions via\nmulti-step incremental prompts. We instantiated a prototype system to evaluate\nthe jailbreaking effectiveness on 8 advanced LLMs, yielding an average success\nrate of 83.9%. This study builds a psychological perspective on the explanatory\ninsights into the intrinsic decision-making logic of LLMs.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2402.15690v1",
    "published_date": "2024-02-24 02:27:55 UTC",
    "updated_date": "2024-02-24 02:27:55 UTC"
  },
  {
    "arxiv_id": "2402.15687v1",
    "title": "General Purpose Image Encoder DINOv2 for Medical Image Registration",
    "authors": [
      "Xinrui Song",
      "Xuanang Xu",
      "Pingkun Yan"
    ],
    "abstract": "Existing medical image registration algorithms rely on either dataset\nspecific training or local texture-based features to align images. The former\ncannot be reliably implemented without large modality-specific training\ndatasets, while the latter lacks global semantics thus could be easily trapped\nat local minima. In this paper, we present a training-free deformable image\nregistration method, DINO-Reg, leveraging a general purpose image encoder\nDINOv2 for image feature extraction. The DINOv2 encoder was trained using the\nImageNet data containing natural images. We used the pretrained DINOv2 without\nany finetuning. Our method feeds the DINOv2 encoded features into a discrete\noptimizer to find the optimal deformable registration field. We conducted a\nseries of experiments to understand the behavior and role of such a general\npurpose image encoder in the application of image registration. Combined with\nhandcrafted features, our method won the first place in the recent OncoReg\nChallenge. To our knowledge, this is the first application of general vision\nfoundation models in medical image registration.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2402.15687v1",
    "published_date": "2024-02-24 02:15:30 UTC",
    "updated_date": "2024-02-24 02:15:30 UTC"
  },
  {
    "arxiv_id": "2402.15670v1",
    "title": "A mathematical model for simultaneous personnel shift planning and unrelated parallel machine scheduling",
    "authors": [
      "Maziyar Khadivi",
      "Mostafa Abbasi",
      "Todd Charter",
      "Homayoun Najjaran"
    ],
    "abstract": "This paper addresses a production scheduling problem derived from an\nindustrial use case, focusing on unrelated parallel machine scheduling with the\npersonnel availability constraint. The proposed model optimizes the production\nplan over a multi-period scheduling horizon, accommodating variations in\npersonnel shift hours within each time period. It assumes shared personnel\namong machines, with one personnel required per machine for setup and\nsupervision during job processing. Available personnel are fewer than the\nmachines, thus limiting the number of machines that can operate in parallel.\nThe model aims to minimize the total production time considering\nmachine-dependent processing times and sequence-dependent setup times. The\nmodel handles practical scenarios like machine eligibility constraints and\nproduction time windows. A Mixed Integer Linear Programming (MILP) model is\nintroduced to formulate the problem, taking into account both continuous and\ndistrict variables. A two-step solution approach enhances computational speed,\nfirst maximizing accepted jobs and then minimizing production time. Validation\nwith synthetic problem instances and a real industrial case study of a food\nprocessing plant demonstrates the performance of the model and its usefulness\nin personnel shift planning. The findings offer valuable insights for practical\nmanagerial decision-making in the context of production scheduling.",
    "categories": [
      "cs.AI",
      "cs.DM"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2402.15670v1",
    "published_date": "2024-02-24 01:04:04 UTC",
    "updated_date": "2024-02-24 01:04:04 UTC"
  },
  {
    "arxiv_id": "2402.15666v1",
    "title": "Universal Model in Online Customer Service",
    "authors": [
      "Shu-Ting Pi",
      "Cheng-Ping Hsieh",
      "Qun Liu",
      "Yuying Zhu"
    ],
    "abstract": "Building machine learning models can be a time-consuming process that often\ntakes several months to implement in typical business scenarios. To ensure\nconsistent model performance and account for variations in data distribution,\nregular retraining is necessary. This paper introduces a solution for improving\nonline customer service in e-commerce by presenting a universal model for\npredict-ing labels based on customer questions, without requiring training. Our\nnovel approach involves using machine learning techniques to tag customer\nquestions in transcripts and create a repository of questions and corresponding\nlabels. When a customer requests assistance, an information retrieval model\nsearches the repository for similar questions, and statistical analysis is used\nto predict the corresponding label. By eliminating the need for individual\nmodel training and maintenance, our approach reduces both the model development\ncycle and costs. The repository only requires periodic updating to maintain\naccuracy.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.IR"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2402.15666v1",
    "published_date": "2024-02-24 00:41:16 UTC",
    "updated_date": "2024-02-24 00:41:16 UTC"
  },
  {
    "arxiv_id": "2402.15665v1",
    "title": "Teacher-Student Learning on Complexity in Intelligent Routing",
    "authors": [
      "Shu-Ting Pi",
      "Michael Yang",
      "Yuying Zhu",
      "Qun Liu"
    ],
    "abstract": "Customer service is often the most time-consuming aspect for e-commerce\nwebsites, with each contact typically taking 10-15 minutes. Effectively routing\ncustomers to appropriate agents without transfers is therefore crucial for\ne-commerce success. To this end, we have developed a machine learning framework\nthat predicts the complexity of customer contacts and routes them to\nappropriate agents accordingly. The framework consists of two parts. First, we\ntrain a teacher model to score the complexity of a contact based on the\npost-contact transcripts. Then, we use the teacher model as a data annotator to\nprovide labels to train a student model that predicts the complexity based on\npre-contact data only. Our experiments show that such a framework is successful\nand can significantly improve customer experience. We also propose a useful\nmetric called complexity AUC that evaluates the effectiveness of customer\nservice at a statistical level.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "KDD 2023 Workshop on End-End Customer Journey Optimization",
    "pdf_url": "http://arxiv.org/pdf/2402.15665v1",
    "published_date": "2024-02-24 00:40:40 UTC",
    "updated_date": "2024-02-24 00:40:40 UTC"
  },
  {
    "arxiv_id": "2402.15662v1",
    "title": "GiMeFive: Towards Interpretable Facial Emotion Classification",
    "authors": [
      "Jiawen Wang",
      "Leah Kawka"
    ],
    "abstract": "Deep convolutional neural networks have been shown to successfully recognize\nfacial emotions for the past years in the realm of computer vision. However,\nthe existing detection approaches are not always reliable or explainable, we\nhere propose our model GiMeFive with interpretations, i.e., via layer\nactivations and gradient-weighted class activation mapping. We compare against\nthe state-of-the-art methods to classify the six facial emotions. Empirical\nresults show that our model outperforms the previous methods in terms of\naccuracy on two Facial Emotion Recognition (FER) benchmarks and our aggregated\nFER GiMeFive. Furthermore, we explain our work in real-world image and video\nexamples, as well as real-time live camera streams. Our code and supplementary\nmaterial are available at https: //github.com/werywjw/SEP-CVDL.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "9 pages, 6 figures",
    "pdf_url": "http://arxiv.org/pdf/2402.15662v1",
    "published_date": "2024-02-24 00:37:37 UTC",
    "updated_date": "2024-02-24 00:37:37 UTC"
  },
  {
    "arxiv_id": "2403.00804v1",
    "title": "Uncovering Customer Issues through Topological Natural Language Analysis",
    "authors": [
      "Shu-Ting Pi",
      "Sidarth Srinivasan",
      "Yuying Zhu",
      "Michael Yang",
      "Qun Liu"
    ],
    "abstract": "E-commerce companies deal with a high volume of customer service requests\ndaily. While a simple annotation system is often used to summarize the topics\nof customer contacts, thoroughly exploring each specific issue can be\nchallenging. This presents a critical concern, especially during an emerging\noutbreak where companies must quickly identify and address specific issues. To\ntackle this challenge, we propose a novel machine learning algorithm that\nleverages natural language techniques and topological data analysis to monitor\nemerging and trending customer issues. Our approach involves an end-to-end deep\nlearning framework that simultaneously tags the primary question sentence of\neach customer's transcript and generates sentence embedding vectors. We then\nwhiten the embedding vectors and use them to construct an undirected graph.\nFrom there, we define trending and emerging issues based on the topological\nproperties of each transcript. We have validated our results through various\nmethods and found that they are highly consistent with news sources.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted in KDD 2023 Workshop on Decision Intelligence and Analytics\n  for Online Marketplaces",
    "pdf_url": "http://arxiv.org/pdf/2403.00804v1",
    "published_date": "2024-02-24 00:15:09 UTC",
    "updated_date": "2024-02-24 00:15:09 UTC"
  },
  {
    "arxiv_id": "2402.15656v2",
    "title": "Learning Semilinear Neural Operators : A Unified Recursive Framework For Prediction And Data Assimilation",
    "authors": [
      "Ashutosh Singh",
      "Ricardo Augusto Borsoi",
      "Deniz Erdogmus",
      "Tales Imbiriba"
    ],
    "abstract": "Recent advances in the theory of Neural Operators (NOs) have enabled fast and\naccurate computation of the solutions to complex systems described by partial\ndifferential equations (PDEs). Despite their great success, current NO-based\nsolutions face important challenges when dealing with spatio-temporal PDEs over\nlong time scales. Specifically, the current theory of NOs does not present a\nsystematic framework to perform data assimilation and efficiently correct the\nevolution of PDE solutions over time based on sparsely sampled noisy\nmeasurements. In this paper, we propose a learning-based state-space approach\nto compute the solution operators to infinite-dimensional semilinear PDEs.\nExploiting the structure of semilinear PDEs and the theory of nonlinear\nobservers in function spaces, we develop a flexible recursive method that\nallows for both prediction and data assimilation by combining prediction and\ncorrection operations. The proposed framework is capable of producing fast and\naccurate predictions over long time horizons, dealing with irregularly sampled\nnoisy measurements to correct the solution, and benefits from the decoupling\nbetween the spatial and temporal dynamics of this class of PDEs. We show\nthrough experiments on the Kuramoto-Sivashinsky, Navier-Stokes and Korteweg-de\nVries equations that the proposed model is robust to noise and can leverage\narbitrary amounts of measurements to correct its prediction over a long time\nhorizon with little computational overhead.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.NA",
      "math.NA"
    ],
    "primary_category": "cs.LG",
    "comment": "ICLR 2024",
    "pdf_url": "http://arxiv.org/pdf/2402.15656v2",
    "published_date": "2024-02-24 00:10:51 UTC",
    "updated_date": "2024-03-15 06:47:48 UTC"
  },
  {
    "arxiv_id": "2402.15655v1",
    "title": "Contact Complexity in Customer Service",
    "authors": [
      "Shu-Ting Pi",
      "Michael Yang",
      "Qun Liu"
    ],
    "abstract": "Customers who reach out for customer service support may face a range of\nissues that vary in complexity. Routing high-complexity contacts to junior\nagents can lead to multiple transfers or repeated contacts, while directing\nlow-complexity contacts to senior agents can strain their capacity to assist\ncustomers who need professional help. To tackle this, a machine learning model\nthat accurately predicts the complexity of customer issues is highly desirable.\nHowever, defining the complexity of a contact is a difficult task as it is a\nhighly abstract concept. While consensus-based data annotation by experienced\nagents is a possible solution, it is time-consuming and costly. To overcome\nthese challenges, we have developed a novel machine learning approach to define\ncontact complexity. Instead of relying on human annotation, we trained an AI\nexpert model to mimic the behavior of agents and evaluate each contact's\ncomplexity based on how the AI expert responds. If the AI expert is uncertain\nor lacks the skills to comprehend the contact transcript, it is considered a\nhigh-complexity contact. Our method has proven to be reliable, scalable, and\ncost-effective based on the collected data.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted in KDD 2023 Workshop on Decision Intelligence and Analytics\n  for Online Marketplaces",
    "pdf_url": "http://arxiv.org/pdf/2402.15655v1",
    "published_date": "2024-02-24 00:09:27 UTC",
    "updated_date": "2024-02-24 00:09:27 UTC"
  }
]
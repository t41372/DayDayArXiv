[
  {
    "arxiv_id": "2506.07326v1",
    "title": "Reward Model Interpretability via Optimal and Pessimal Tokens",
    "authors": [
      "Brian Christian",
      "Hannah Rose Kirk",
      "Jessica A. F. Thompson",
      "Christopher Summerfield",
      "Tsvetomira Dumbalska"
    ],
    "abstract": "Reward modeling has emerged as a crucial component in aligning large language models with human values. Significant attention has focused on using reward models as a means for fine-tuning generative models. However, the reward models themselves -- which directly encode human value judgments by turning prompt-response pairs into scalar rewards -- remain relatively understudied. We present a novel approach to reward model interpretability through exhaustive analysis of their responses across their entire vocabulary space. By examining how different reward models score every possible single-token response to value-laden prompts, we uncover several striking findings: (i) substantial heterogeneity between models trained on similar objectives, (ii) systematic asymmetries in how models encode high- vs low-scoring tokens, (iii) significant sensitivity to prompt framing that mirrors human cognitive biases, and (iv) overvaluation of more frequent tokens. We demonstrate these effects across ten recent open-source reward models of varying parameter counts and architectures. Our results challenge assumptions about the interchangeability of reward models, as well as their suitability as proxies of complex and context-dependent human values. We find that these models can encode concerning biases toward certain identity groups, which may emerge as unintended consequences of harmlessness training -- distortions that risk propagating through the downstream large language models now deployed to millions.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CY",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted for publication in Proceedings of the 2025 ACM Conference on Fairness, Accountability, and Transparency (FAccT '25), to appear June 2025",
    "pdf_url": "https://arxiv.org/pdf/2506.07326v1",
    "published_date": "2025-06-08 23:56:58 UTC",
    "updated_date": "2025-06-08 23:56:58 UTC"
  },
  {
    "arxiv_id": "2506.07323v2",
    "title": "Speech Recognition on TV Series with Video-guided Post-ASR Correction",
    "authors": [
      "Haoyuan Yang",
      "Yue Zhang",
      "Liqiang Jing",
      "John H. L. Hansen"
    ],
    "abstract": "Automatic Speech Recognition (ASR) has achieved remarkable success with deep learning, driving advancements in conversational artificial intelligence, media transcription, and assistive technologies. However, ASR systems still struggle in complex environments such as TV series, where multiple speakers, overlapping speech, domain-specific terminology, and long-range contextual dependencies pose significant challenges to transcription accuracy. Existing approaches fail to explicitly leverage the rich temporal and contextual information available in the video. To address this limitation, we propose a Video-Guided Post-ASR Correction (VPC) framework that uses a Video-Large Multimodal Model (VLMM) to capture video context and refine ASR outputs. Evaluations on a TV-series benchmark show that our method consistently improves transcription accuracy in complex multimedia environments.",
    "categories": [
      "cs.SD",
      "cs.AI",
      "eess.AS"
    ],
    "primary_category": "cs.SD",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.07323v2",
    "published_date": "2025-06-08 23:36:31 UTC",
    "updated_date": "2025-09-21 20:56:28 UTC"
  },
  {
    "arxiv_id": "2506.07312v1",
    "title": "Generative Modeling of Networked Time-Series via Transformer Architectures",
    "authors": [
      "Yusuf Elnady"
    ],
    "abstract": "Many security and network applications require having large datasets to train the machine learning models. Limited data access is a well-known problem in the security domain. Recent studies have shown the potential of Transformer models to enlarge the size of data by synthesizing new samples, but the synthesized samples don't improve the models over the real data. To address this issue, we design an efficient transformer-based model as a generative framework to generate time-series data, that can be used to boost the performance of existing and new ML workflows. Our new transformer model achieves the SOTA results. We style our model to be generalizable and work across different datasets, and produce high-quality samples.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.07312v1",
    "published_date": "2025-06-08 23:05:13 UTC",
    "updated_date": "2025-06-08 23:05:13 UTC"
  },
  {
    "arxiv_id": "2506.07311v1",
    "title": "Paged Attention Meets FlexAttention: Unlocking Long-Context Efficiency in Deployed Inference",
    "authors": [
      "Thomas Joshi",
      "Herman Saini",
      "Neil Dhillon",
      "Antoni Viros i Martin",
      "Kaoutar El Maghraoui"
    ],
    "abstract": "Large Language Models (LLMs) encounter severe memory inefficiencies during long-context inference due to conventional handling of key-value (KV) caches. In this work, we introduce a novel integration of PagedAttention with PyTorch's FlexAttention, addressing internal fragmentation and inefficiencies associated with monolithic KV cache allocations. Implemented within IBM's Foundation Model Stack (FMS), our fused attention kernel efficiently gathers scattered KV data. Our benchmarks on an NVIDIA L4 GPU (24GB) demonstrate significantly reduced inference latency, growing only linearly (~2x) with sequence length from 128 to 2048 tokens when utilizing a global KV cache, compared to exponential latency increases without caching. While peak memory usage remains largely unchanged for single-step evaluations (dominated by model weights and activations), paged attention causes minimal incremental memory usage, observable only at sequence lengths exceeding 2048 tokens due to its power-of-two cache allocations. We open-source the full implementation and discuss its implications for future long-context model deployment.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.07311v1",
    "published_date": "2025-06-08 22:59:20 UTC",
    "updated_date": "2025-06-08 22:59:20 UTC"
  },
  {
    "arxiv_id": "2506.07298v2",
    "title": "Pre-trained Large Language Models Learn Hidden Markov Models In-context",
    "authors": [
      "Yijia Dai",
      "Zhaolin Gao",
      "Yahya Sattar",
      "Sarah Dean",
      "Jennifer J. Sun"
    ],
    "abstract": "Hidden Markov Models (HMMs) are foundational tools for modeling sequential data with latent Markovian structure, yet fitting them to real-world data remains computationally challenging. In this work, we show that pre-trained large language models (LLMs) can effectively model data generated by HMMs via in-context learning (ICL)$\\unicode{x2013}$their ability to infer patterns from examples within a prompt. On a diverse set of synthetic HMMs, LLMs achieve predictive accuracy approaching the theoretical optimum. We uncover novel scaling trends influenced by HMM properties, and offer theoretical conjectures for these empirical observations. We also provide practical guidelines for scientists on using ICL as a diagnostic tool for complex data. On real-world animal decision-making tasks, ICL achieves competitive performance with models designed by human experts. To our knowledge, this is the first demonstration that ICL can learn and predict HMM-generated sequences$\\unicode{x2013}$an advance that deepens our understanding of in-context learning in LLMs and establishes its potential as a powerful tool for uncovering hidden structure in complex scientific data.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.07298v2",
    "published_date": "2025-06-08 21:49:38 UTC",
    "updated_date": "2025-06-11 05:17:22 UTC"
  },
  {
    "arxiv_id": "2506.07296v1",
    "title": "HotelMatch-LLM: Joint Multi-Task Training of Small and Large Language Models for Efficient Multimodal Hotel Retrieval",
    "authors": [
      "Arian Askari",
      "Emmanouil Stergiadis",
      "Ilya Gusev",
      "Moran Beladev"
    ],
    "abstract": "We present HotelMatch-LLM, a multimodal dense retrieval model for the travel domain that enables natural language property search, addressing the limitations of traditional travel search engines which require users to start with a destination and editing search parameters. HotelMatch-LLM features three key innovations: (1) Domain-specific multi-task optimization with three novel retrieval, visual, and language modeling objectives; (2) Asymmetrical dense retrieval architecture combining a small language model (SLM) for efficient online query processing and a large language model (LLM) for embedding hotel data; and (3) Extensive image processing to handle all property image galleries. Experiments on four diverse test sets show HotelMatch-LLM significantly outperforms state-of-the-art models, including VISTA and MARVEL. Specifically, on the test set -- main query type -- we achieve 0.681 for HotelMatch-LLM compared to 0.603 for the most effective baseline, MARVEL. Our analysis highlights the impact of our multi-task optimization, the generalizability of HotelMatch-LLM across LLM architectures, and its scalability for processing large image galleries.",
    "categories": [
      "cs.IR",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.IR",
    "comment": "Accepted at ACL 2025, Main track. 13 Pages, 1 figure",
    "pdf_url": "https://arxiv.org/pdf/2506.07296v1",
    "published_date": "2025-06-08 21:39:08 UTC",
    "updated_date": "2025-06-08 21:39:08 UTC"
  },
  {
    "arxiv_id": "2506.07281v1",
    "title": "Secondary Stakeholders in AI: Fighting for, Brokering, and Navigating Agency",
    "authors": [
      "Leah Hope Ajmani",
      "Nuredin Ali Abdelkadir",
      "Stevie Chancellor"
    ],
    "abstract": "As AI technologies become more human-facing, there have been numerous calls to adapt participatory approaches to AI development -- spurring the idea of participatory AI. However, these calls often focus only on primary stakeholders, such as end-users, and not secondary stakeholders. This paper seeks to translate the ideals of participatory AI to a broader population of secondary AI stakeholders through semi-structured interviews. We theorize that meaningful participation involves three participatory ideals: (1) informedness, (2) consent, and (3) agency. We also explore how secondary stakeholders realize these ideals by traversing a complicated problem space. Like walking up the rungs of a ladder, these ideals build on one another. We introduce three stakeholder archetypes: the reluctant data contributor, the unsupported activist, and the well-intentioned practitioner, who must navigate systemic barriers to achieving agentic AI relationships. We envision an AI future where secondary stakeholders are able to meaningfully participate with the AI systems they influence and are influenced by.",
    "categories": [
      "cs.HC",
      "cs.AI"
    ],
    "primary_category": "cs.HC",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.07281v1",
    "published_date": "2025-06-08 20:57:30 UTC",
    "updated_date": "2025-06-08 20:57:30 UTC"
  },
  {
    "arxiv_id": "2506.07280v2",
    "title": "From Generation to Generalization: Emergent Few-Shot Learning in Video Diffusion Models",
    "authors": [
      "Pablo Acuaviva",
      "Aram Davtyan",
      "Mariam Hassan",
      "Sebastian Stapf",
      "Ahmad Rahimi",
      "Alexandre Alahi",
      "Paolo Favaro"
    ],
    "abstract": "Video Diffusion Models (VDMs) have emerged as powerful generative tools, capable of synthesizing high-quality spatiotemporal content. Yet, their potential goes far beyond mere video generation. We argue that the training dynamics of VDMs, driven by the need to model coherent sequences, naturally pushes them to internalize structured representations and an implicit understanding of the visual world. To probe the extent of this internal knowledge, we introduce a few-shot fine-tuning framework that repurposes VDMs for new tasks using only a handful of examples. Our method transforms each task into a visual transition, enabling the training of LoRA weights on short input-output sequences without altering the generative interface of a frozen VDM. Despite minimal supervision, the model exhibits strong generalization across diverse tasks, from low-level vision (for example, segmentation and pose estimation) to high-level reasoning (for example, on ARC-AGI). These results reframe VDMs as more than generative engines. They are adaptable visual learners with the potential to serve as the backbone for future foundation models in vision.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "27 pages, 23 figures, 9 tables. Project page: https://pabloacuaviva.github.io/Gen2Gen/",
    "pdf_url": "https://arxiv.org/pdf/2506.07280v2",
    "published_date": "2025-06-08 20:52:34 UTC",
    "updated_date": "2025-06-10 11:37:10 UTC"
  },
  {
    "arxiv_id": "2506.14817v1",
    "title": "Next-Generation Conflict Forecasting: Unleashing Predictive Patterns through Spatiotemporal Learning",
    "authors": [
      "Simon P. von der Maase"
    ],
    "abstract": "Forecasting violent conflict at high spatial and temporal resolution remains a central challenge for both researchers and policymakers. This study presents a novel neural network architecture for forecasting three distinct types of violence -- state-based, non-state, and one-sided -- at the subnational (priogrid-month) level, up to 36 months in advance. The model jointly performs classification and regression tasks, producing both probabilistic estimates and expected magnitudes of future events. It achieves state-of-the-art performance across all tasks and generates approximate predictive posterior distributions to quantify forecast uncertainty.\n  The architecture is built on a Monte Carlo Dropout Long Short-Term Memory (LSTM) U-Net, integrating convolutional layers to capture spatial dependencies with recurrent structures to model temporal dynamics. Unlike many existing approaches, it requires no manual feature engineering and relies solely on historical conflict data. This design enables the model to autonomously learn complex spatiotemporal patterns underlying violent conflict.\n  Beyond achieving state-of-the-art predictive performance, the model is also highly extensible: it can readily integrate additional data sources and jointly forecast auxiliary variables. These capabilities make it a promising tool for early warning systems, humanitarian response planning, and evidence-based peacebuilding initiatives.",
    "categories": [
      "stat.OT",
      "cs.AI",
      "cs.CY",
      "cs.LG",
      "stat.AP"
    ],
    "primary_category": "stat.OT",
    "comment": "33 pages, 9 figures, 3 tables. Presented at workshops hosted by PRIO, AFK (German Association for Peace and Conflict Studies), CCEW (Bundeswehr University Munich), Uppsala University, SODAS (University of Copenhagen) and in briefings with UN agencies including UNIDIR, OCHA, and FAO",
    "pdf_url": "https://arxiv.org/pdf/2506.14817v1",
    "published_date": "2025-06-08 20:42:29 UTC",
    "updated_date": "2025-06-08 20:42:29 UTC"
  },
  {
    "arxiv_id": "2506.07276v1",
    "title": "Tokenized Bandit for LLM Decoding and Alignment",
    "authors": [
      "Suho Shin",
      "Chenghao Yang",
      "Haifeng Xu",
      "Mohammad T. Hajiaghayi"
    ],
    "abstract": "We introduce the tokenized linear bandit (TLB) and multi-armed bandit (TMAB), variants of linear and stochastic multi-armed bandit problems inspired by LLM decoding and alignment. In these problems, at each round $t \\in [T]$, a user submits a query (context), and the decision maker (DM) sequentially selects a token irrevocably from a token set. Once the sequence is complete, the DM observes a random utility from the user, whose expectation is presented by a sequence function mapping the chosen token sequence to a nonnegative real value that depends on the query.\n  In both problems, we first show that learning is impossible without any structure on the sequence function. We introduce a natural assumption, diminishing distance with more commons (DDMC), and propose algorithms with regret $\\tilde{O}(L\\sqrt{T})$ and $\\tilde{O}(L\\sqrt{T^{2/3}})$ for TLB and TMAB, respectively. As a side product, we obtain an (almost) optimality of the greedy decoding for LLM decoding algorithm under DDMC, which justifies the unresaonable effectiveness of greedy decoding in several tasks. This also has an immediate application to decoding-time LLM alignment, when the misaligned utility can be represented as the frozen LLM's utility and a linearly realizable latent function. We finally validate our algorithm's performance empirically as well as verify our assumptions using synthetic and real-world datasets.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "To appear at ICML 2025",
    "pdf_url": "https://arxiv.org/pdf/2506.07276v1",
    "published_date": "2025-06-08 20:32:08 UTC",
    "updated_date": "2025-06-08 20:32:08 UTC"
  },
  {
    "arxiv_id": "2506.13774v2",
    "title": "Personalized Constitutionally-Aligned Agentic Superego: Secure AI Behavior Aligned to Diverse Human Values",
    "authors": [
      "Nell Watson",
      "Ahmed Amer",
      "Evan Harris",
      "Preeti Ravindra",
      "Shujun Zhang"
    ],
    "abstract": "Agentic AI systems, possessing capabilities for autonomous planning and action, show great potential across diverse domains. However, their practical deployment is hindered by challenges in aligning their behavior with varied human values, complex safety requirements, and specific compliance needs. Existing alignment methodologies often falter when faced with the complex task of providing personalized context without inducing confabulation or operational inefficiencies. This paper introduces a novel solution: a 'superego' agent, designed as a personalized oversight mechanism for agentic AI. This system dynamically steers AI planning by referencing user-selected 'Creed Constitutions' encapsulating diverse rule sets -- with adjustable adherence levels to fit non-negotiable values. A real-time compliance enforcer validates plans against these constitutions and a universal ethical floor before execution. We present a functional system, including a demonstration interface with a prototypical constitution-sharing portal, and successful integration with third-party models via the Model Context Protocol (MCP). Comprehensive benchmark evaluations (HarmBench, AgentHarm) demonstrate that our Superego agent dramatically reduces harmful outputs -- achieving up to a 98.3% harm score reduction and near-perfect refusal rates (e.g., 100% with Claude Sonnet 4 on AgentHarm's harmful set) for leading LLMs like Gemini 2.5 Flash and GPT-4o. This approach substantially simplifies personalized AI alignment, rendering agentic systems more reliably attuned to individual and cultural contexts, while also enabling substantial safety improvements. An overview on this research with examples is available at https://superego.creed.space.",
    "categories": [
      "cs.AI",
      "cs.CY",
      "cs.MA"
    ],
    "primary_category": "cs.AI",
    "comment": "42 pages, 6 figures",
    "pdf_url": "https://arxiv.org/pdf/2506.13774v2",
    "published_date": "2025-06-08 20:31:26 UTC",
    "updated_date": "2025-08-08 20:29:52 UTC"
  },
  {
    "arxiv_id": "2506.07274v1",
    "title": "Parsing the Switch: LLM-Based UD Annotation for Complex Code-Switched and Low-Resource Languages",
    "authors": [
      "Olga Kellert",
      "Nemika Tyagi",
      "Muhammad Imran",
      "Nelvin Licona-Guevara",
      "Carlos Gómez-Rodríguez"
    ],
    "abstract": "Code-switching presents a complex challenge for syntactic analysis, especially in low-resource language settings where annotated data is scarce. While recent work has explored the use of large language models (LLMs) for sequence-level tagging, few approaches systematically investigate how well these models capture syntactic structure in code-switched contexts. Moreover, existing parsers trained on monolingual treebanks often fail to generalize to multilingual and mixed-language input. To address this gap, we introduce the BiLingua Parser, an LLM-based annotation pipeline designed to produce Universal Dependencies (UD) annotations for code-switched text. First, we develop a prompt-based framework for Spanish-English and Spanish-Guaraní data, combining few-shot LLM prompting with expert review. Second, we release two annotated datasets, including the first Spanish-Guaraní UD-parsed corpus. Third, we conduct a detailed syntactic analysis of switch points across language pairs and communicative contexts. Experimental results show that BiLingua Parser achieves up to 95.29% LAS after expert revision, significantly outperforming prior baselines and multilingual parsers. These results show that LLMs, when carefully guided, can serve as practical tools for bootstrapping syntactic resources in under-resourced, code-switched environments. Data and source code are available at https://github.com/N3mika/ParsingProject",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "16 pages",
    "pdf_url": "https://arxiv.org/pdf/2506.07274v1",
    "published_date": "2025-06-08 20:23:57 UTC",
    "updated_date": "2025-06-08 20:23:57 UTC"
  },
  {
    "arxiv_id": "2506.10021v1",
    "title": "From Tool Calling to Symbolic Thinking: LLMs in a Persistent Lisp Metaprogramming Loop",
    "authors": [
      "Jordi de la Torre"
    ],
    "abstract": "We propose a novel architecture for integrating large language models (LLMs) with a persistent, interactive Lisp environment. This setup enables LLMs to define, invoke, and evolve their own tools through programmatic interaction with a live REPL. By embedding Lisp expressions within generation and intercepting them via a middleware layer, the system allows for stateful external memory, reflective programming, and dynamic tool creation. We present a design framework and architectural principles to guide future implementations of interactive AI systems that integrate symbolic programming with neural language generation.",
    "categories": [
      "cs.PL",
      "cs.AI"
    ],
    "primary_category": "cs.PL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.10021v1",
    "published_date": "2025-06-08 20:12:06 UTC",
    "updated_date": "2025-06-08 20:12:06 UTC"
  },
  {
    "arxiv_id": "2506.12081v1",
    "title": "Latency Optimization for Wireless Federated Learning in Multihop Networks",
    "authors": [
      "Shaba Shaon",
      "Van-Dinh Nguyen",
      "Dinh C. Nguyen"
    ],
    "abstract": "In this paper, we study a novel latency minimization problem in wireless federated learning (FL) across multi-hop networks. The system comprises multiple routes, each integrating leaf and relay nodes for FL model training. We explore a personalized learning and adaptive aggregation-aware FL (PAFL) framework that effectively addresses data heterogeneity across participating nodes by harmonizing individual and collective learning objectives. We formulate an optimization problem aimed at minimizing system latency through the joint optimization of leaf and relay nodes, as well as relay routing indicator. We also incorporate an additional energy harvesting scheme for the relay nodes to help with their relay tasks. This formulation presents a computationally demanding challenge, and thus we develop a simple yet efficient algorithm based on block coordinate descent and successive convex approximation (SCA) techniques. Simulation results illustrate the efficacy of our proposed joint optimization approach for leaf and relay nodes with relay routing indicator. We observe significant latency savings in the wireless multi-hop PAFL system, with reductions of up to 69.37% compared to schemes optimizing only one node type, traditional greedy algorithm, and scheme without relay routing indicator.",
    "categories": [
      "cs.NI",
      "cs.AI",
      "cs.IT"
    ],
    "primary_category": "cs.NI",
    "comment": "Accepted at IEEE Transactions on Vehicular Technology (IEEE TVT), code is available at https://github.com/ShabaGit/Multihop_FL",
    "pdf_url": "https://arxiv.org/pdf/2506.12081v1",
    "published_date": "2025-06-08 19:10:09 UTC",
    "updated_date": "2025-06-08 19:10:09 UTC"
  },
  {
    "arxiv_id": "2506.07255v3",
    "title": "Subgoal-Guided Policy Heuristic Search with Learned Subgoals",
    "authors": [
      "Jake Tuero",
      "Michael Buro",
      "Levi H. S. Lelis"
    ],
    "abstract": "Policy tree search is a family of tree search algorithms that use a policy to guide the search. These algorithms provide guarantees on the number of expansions required to solve a given problem that are based on the quality of the policy. While these algorithms have shown promising results, the process in which they are trained requires complete solution trajectories to train the policy. Search trajectories are obtained during a trial-and-error search process. When the training problem instances are hard, learning can be prohibitively costly, especially when starting from a randomly initialized policy. As a result, search samples are wasted in failed attempts to solve these hard instances. This paper introduces a novel method for learning subgoal-based policies for policy tree search algorithms. The subgoals and policies conditioned on subgoals are learned from the trees that the search expands while attempting to solve problems, including the search trees of failed attempts. We empirically show that our policy formulation and training method improve the sample efficiency of learning a policy and heuristic function in this online setting.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "Accepted to ICML-25",
    "pdf_url": "https://arxiv.org/pdf/2506.07255v3",
    "published_date": "2025-06-08 18:45:43 UTC",
    "updated_date": "2025-12-02 07:35:49 UTC"
  },
  {
    "arxiv_id": "2506.07245v2",
    "title": "SDE-SQL: Enhancing Text-to-SQL Generation in Large Language Models via Self-Driven Exploration with SQL Probes",
    "authors": [
      "Wenxuan Xie",
      "Yaxun Dai",
      "Wenhao Jiang"
    ],
    "abstract": "Recent advancements in large language models (LLMs) have significantly improved performance on the Text-to-SQL task. However, prior approaches typically rely on static, pre-processed database information provided at inference time, which limits the model's ability to fully understand the database contents. Without dynamic interaction, LLMs are constrained to fixed, human-provided context and cannot autonomously explore the underlying data. To address this limitation, we propose SDE-SQL, a framework that enables large language models to perform self-driven exploration of databases during inference. This is accomplished by generating and executing SQL probes, which allow the model to actively retrieve information from the database and iteratively update its understanding of the data. Unlike prior methods, SDE-SQL operates in a zero-shot setting, without relying on any question-SQL pairs as in-context demonstrations. When evaluated on the BIRD benchmark with Qwen2.5-72B-Instruct, SDE-SQL achieves an 8.02% relative improvement in execution accuracy over the vanilla Qwen2.5-72B-Instruct baseline, establishing a new state-of-the-art among methods based on open-source models without supervised fine-tuning (SFT) or model ensembling. Moreover, with SFT, the performance of SDE-SQL can be further enhanced, yielding an additional 0.52% improvement.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.07245v2",
    "published_date": "2025-06-08 18:01:26 UTC",
    "updated_date": "2025-06-19 04:10:10 UTC"
  },
  {
    "arxiv_id": "2506.07240v1",
    "title": "Overclocking LLM Reasoning: Monitoring and Controlling Thinking Path Lengths in LLMs",
    "authors": [
      "Roy Eisenstadt",
      "Itamar Zimerman",
      "Lior Wolf"
    ],
    "abstract": "Recently, techniques such as explicit structured reasoning have demonstrated strong test-time scaling behavior by enforcing a separation between the model's internal \"thinking\" process and the final response. A key factor influencing answer quality in this setting is the length of the thinking stage. When the reasoning is too short, the model may fail to capture the complexity of the task. Conversely, when it is too long, the model may overthink, leading to unnecessary computation and degraded performance. This paper explores and exploits the underlying mechanisms by which LLMs understand and regulate the length of their reasoning during explicit thought processes. First, we show that LLMs encode their progress through the reasoning process and introduce an interactive progress bar visualization, which is then used to reveal insights on the model's planning dynamics. Second, we manipulate the internal progress encoding during inference to reduce unnecessary steps and generate a more concise and decisive chain of thoughts. Our empirical results demonstrate that this \"overclocking\" method mitigates overthinking, improves answer accuracy, and reduces inference latency. Our code is publicly available.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.07240v1",
    "published_date": "2025-06-08 17:54:33 UTC",
    "updated_date": "2025-06-08 17:54:33 UTC"
  },
  {
    "arxiv_id": "2506.07239v2",
    "title": "VeriLoC: Line-of-Code Level Prediction of Hardware Design Quality from Verilog Code",
    "authors": [
      "Raghu Vamshi Hemadri",
      "Jitendra Bhandari",
      "Andre Nakkab",
      "Johann Knechtel",
      "Badri P Gopalan",
      "Ramesh Narayanaswamy",
      "Ramesh Karri",
      "Siddharth Garg"
    ],
    "abstract": "Modern chip design is complex, and there is a crucial need for early-stage prediction of key design-quality metrics like timing and routing congestion directly from Verilog code (a commonly used programming language for hardware design). It is especially important yet complex to predict individual lines of code that cause timing violations or downstream routing congestion. Prior works have tried approaches like converting Verilog into an intermediate graph representation and using LLM embeddings alongside other features to predict module-level quality, but did not consider line-level quality prediction. We propose VeriLoC, the first method that predicts design quality directly from Verilog at both the line- and module-level. To this end, VeriLoC leverages recent Verilog code-generation LLMs to extract local line-level and module-level embeddings, and train downstream classifiers/regressors on concatenations of these embeddings. VeriLoC achieves high F1-scores of 0.86-0.95 for line-level congestion and timing prediction, and reduces the mean average percentage error from 14% - 18% for SOTA methods down to only 4%. We believe that VeriLoC embeddings and insights from our work will also be of value for other predictive and optimization tasks for complex hardware design.",
    "categories": [
      "cs.AR",
      "cs.AI"
    ],
    "primary_category": "cs.AR",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.07239v2",
    "published_date": "2025-06-08 17:53:22 UTC",
    "updated_date": "2025-06-29 01:51:01 UTC"
  },
  {
    "arxiv_id": "2506.13984v1",
    "title": "Mirror Descent Using the Tempesta Generalized Multi-parametric Logarithms",
    "authors": [
      "Andrzej Cichocki"
    ],
    "abstract": "In this paper, we develop a wide class Mirror Descent (MD) algorithms, which play a key role in machine learning. For this purpose we formulated the constrained optimization problem, in which we exploits the Bregman divergence with the Tempesta multi-parametric deformation logarithm as a link function. This link function called also mirror function defines the mapping between the primal and dual spaces and is associated with a very-wide (in fact, theoretically infinite) class of generalized trace-form entropies. In order to derive novel MD updates, we estimate generalized exponential function, which closely approximates the inverse of the multi-parametric Tempesta generalized logarithm. The shape and properties of the Tempesta logarithm and its inverse-deformed exponential functions can be tuned by several hyperparameters. By learning these hyperparameters, we can adapt to distribution or geometry of training data, and we can adjust them to achieve desired properties of MD algorithms. The concept of applying multi-parametric logarithms allow us to generate a new wide and flexible family of MD and mirror-less MD updates.",
    "categories": [
      "stat.ML",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "stat.ML",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.13984v1",
    "published_date": "2025-06-08 17:48:44 UTC",
    "updated_date": "2025-06-08 17:48:44 UTC"
  },
  {
    "arxiv_id": "2506.07232v1",
    "title": "Learn as Individuals, Evolve as a Team: Multi-agent LLMs Adaptation in Embodied Environments",
    "authors": [
      "Xinran Li",
      "Chenjia Bai",
      "Zijian Li",
      "Jiakun Zheng",
      "Ting Xiao",
      "Jun Zhang"
    ],
    "abstract": "Large language models (LLMs) possess extensive knowledge bases and strong reasoning capabilities, making them promising tools for complex, multi-agent planning in embodied environments. However, despite LLMs' advanced abilities and the sophisticated modular design of agentic methods, existing LLM-based planning algorithms remain limited by weak adaptation capabilities to multi-agent embodied scenarios. We address this limitation by introducing a framework that enables LLM agents to learn and evolve both before and during test time, equipping them with environment-relevant knowledge for better planning and enhanced communication for improved cooperation. Inspired by centralized training with decentralized execution in multi-agent reinforcement learning, we propose a \\textit{Learn as Individuals, Evolve as a Team (LIET)} paradigm for multi-agent LLMs adaptation. At the individual level, LLM agents learn a local utility function from exploratory datasets to better comprehend the embodied environment, which is then queried during test time to support informed decision-making. At the team level, LLM agents collaboratively and iteratively maintain and update a shared cooperation knowledge list based on new experiences, using it to guide more effective communication. By combining individual learning with team evolution, LIET enables comprehensive and flexible adaptation for LLM agents. Our experiments on Communicative Watch-And-Help and ThreeD-World Multi-Agent Transport benchmarks demonstrate that LIET, instantiated with both LLaMA and GPT-4o, outperforms existing baselines and exhibits strong cooperative planning abilities.",
    "categories": [
      "cs.MA",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.MA",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.07232v1",
    "published_date": "2025-06-08 17:32:03 UTC",
    "updated_date": "2025-06-08 17:32:03 UTC"
  },
  {
    "arxiv_id": "2506.07223v1",
    "title": "LLM-Enhanced Rapid-Reflex Async-Reflect Embodied Agent for Real-Time Decision-Making in Dynamically Changing Environments",
    "authors": [
      "Yangqing Zheng",
      "Shunqi Mao",
      "Dingxin Zhang",
      "Weidong Cai"
    ],
    "abstract": "In the realm of embodied intelligence, the evolution of large language models (LLMs) has markedly enhanced agent decision making. Consequently, researchers have begun exploring agent performance in dynamically changing high-risk scenarios, i.e., fire, flood, and wind scenarios in the HAZARD benchmark. Under these extreme conditions, the delay in decision making emerges as a crucial yet insufficiently studied issue. We propose a Time Conversion Mechanism (TCM) that translates inference delays in decision-making into equivalent simulation frames, thus aligning cognitive and physical costs under a single FPS-based metric. By extending HAZARD with Respond Latency (RL) and Latency-to-Action Ratio (LAR), we deliver a fully latency-aware evaluation protocol. Moreover, we present the Rapid-Reflex Async-Reflect Agent (RRARA), which couples a lightweight LLM-guided feedback module with a rule-based agent to enable immediate reactive behaviors and asynchronous reflective refinements in situ. Experiments on HAZARD show that RRARA substantially outperforms existing baselines in latency-sensitive scenarios.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "Accepted by the CVPR 2025 Embodied AI Workshop",
    "pdf_url": "https://arxiv.org/pdf/2506.07223v1",
    "published_date": "2025-06-08 17:09:26 UTC",
    "updated_date": "2025-06-08 17:09:26 UTC"
  },
  {
    "arxiv_id": "2506.11113v3",
    "title": "Breaking the Reviewer: Assessing the Vulnerability of Large Language Models in Automated Peer Review Under Textual Adversarial Attacks",
    "authors": [
      "Tzu-Ling Lin",
      "Wei-Chih Chen",
      "Teng-Fang Hsiao",
      "Hou-I Liu",
      "Ya-Hsin Yeh",
      "Yu Kai Chan",
      "Wen-Sheng Lien",
      "Po-Yen Kuo",
      "Philip S. Yu",
      "Hong-Han Shuai"
    ],
    "abstract": "Peer review is essential for maintaining academic quality, but the increasing volume of submissions places a significant burden on reviewers. Large language models (LLMs) offer potential assistance in this process, yet their susceptibility to textual adversarial attacks raises reliability concerns. This paper investigates the robustness of LLMs used as automated reviewers in the presence of such attacks. We focus on three key questions: (1) The effectiveness of LLMs in generating reviews compared to human reviewers. (2) The impact of adversarial attacks on the reliability of LLM-generated reviews. (3) Challenges and potential mitigation strategies for LLM-based review. Our evaluation reveals significant vulnerabilities, as text manipulations can distort LLM assessments. We offer a comprehensive evaluation of LLM performance in automated peer reviewing and analyze its robustness against adversarial attacks. Our findings emphasize the importance of addressing adversarial risks to ensure AI strengthens, rather than compromises, the integrity of scholarly communication.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Minor correction: Fixed sign errors in the results table. The update does not affect the main findings or conclusions",
    "pdf_url": "https://arxiv.org/pdf/2506.11113v3",
    "published_date": "2025-06-08 16:57:38 UTC",
    "updated_date": "2025-10-09 11:07:40 UTC"
  },
  {
    "arxiv_id": "2506.17255v1",
    "title": "UltraSketchLLM: Saliency-Driven Sketching for Ultra-Low Bit LLM Compression",
    "authors": [
      "Sunan Zou",
      "Ziyun Zhang",
      "Xueting Sun",
      "Guojie Luo"
    ],
    "abstract": "The rapid growth of large language models (LLMs) has outpaced the memory constraints of edge devices, necessitating extreme weight compression beyond the 1-bit limit. While quantization reduces model size, it is fundamentally limited to 1 bit per weight. Existing multiple-to-one compression methods either rely on mapping tables (inducing memory overhead) or incur severe accuracy degradation due to random weight grouping. We introduce UltraSketchLLM, an index-free, sketch-based framework that achieves ultra-low bit compression (down to 0.5 bits per weight) while preserving model performance. UltraSketchLLM leverages data sketching, a sub-linear representation technique from streaming applications, to map multiple weights to single values with bounded error. Our approach integrates an underestimate AbsMaxMin sketch to minimize relative errors for small weights, importance-aware space allocation to prioritize salient weights, and a straight-through estimator for compression-aware finetuning. Experiments on Llama-3.2-1B demonstrate up to 0.5-bit compression with competitive perplexity, alongside tolerable latency overhead. UltraSketchLLM offers a practical solution for deploying LLMs in resource-constrained environments.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.17255v1",
    "published_date": "2025-06-08 16:55:42 UTC",
    "updated_date": "2025-06-08 16:55:42 UTC"
  },
  {
    "arxiv_id": "2506.07218v2",
    "title": "Perception-R1: Advancing Multimodal Reasoning Capabilities of MLLMs via Visual Perception Reward",
    "authors": [
      "Tong Xiao",
      "Xin Xu",
      "Zhenya Huang",
      "Hongyu Gao",
      "Quan Liu",
      "Qi Liu",
      "Enhong Chen"
    ],
    "abstract": "Enhancing the multimodal reasoning capabilities of Multimodal Large Language Models (MLLMs) is a challenging task that has attracted increasing attention in the community. Recently, several studies have applied Reinforcement Learning with Verifiable Rewards (RLVR) to the multimodal domain in order to enhance the reasoning abilities of MLLMs. However, these works largely overlook the enhancement of multimodal perception capabilities in MLLMs, which serve as a core prerequisite and foundational component of complex multimodal reasoning. Through McNemar's test, we find that existing RLVR method fails to effectively enhance the multimodal perception capabilities of MLLMs, thereby limiting their further improvement in multimodal reasoning. To address this limitation, we propose Perception-R1, which introduces a novel visual perception reward that explicitly encourages MLLMs to perceive the visual content accurately, thereby can effectively incentivizing both their multimodal perception and reasoning capabilities. Specifically, we first collect textual visual annotations from the CoT trajectories of multimodal problems, which will serve as visual references for reward assignment. During RLVR training, we employ a judging LLM to assess the consistency between the visual annotations and the responses generated by MLLM, and assign the visual perception reward based on these consistency judgments. Extensive experiments on several multimodal reasoning benchmarks demonstrate the effectiveness of our Perception-R1, which achieves state-of-the-art performance on most benchmarks using only 1,442 training data.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.07218v2",
    "published_date": "2025-06-08 16:48:42 UTC",
    "updated_date": "2025-09-19 02:21:53 UTC"
  },
  {
    "arxiv_id": "2506.07217v2",
    "title": "BIMgent: Towards Autonomous Building Modeling via Computer-use Agents",
    "authors": [
      "Zihan Deng",
      "Changyu Du",
      "Stavros Nousias",
      "André Borrmann"
    ],
    "abstract": "Existing computer-use agents primarily focus on general-purpose desktop automation tasks, with limited exploration of their application in highly specialized domains. In particular, the 3D building modeling process in the Architecture, Engineering, and Construction (AEC) sector involves open-ended design tasks and complex interaction patterns within Building Information Modeling (BIM) authoring software, which has yet to be thoroughly addressed by current studies. In this paper, we propose BIMgent, an agentic framework powered by multimodal large language models (LLMs), designed to enable autonomous building model authoring via graphical user interface (GUI) operations. BIMgent automates the architectural building modeling process, including multimodal input for conceptual design, planning of software-specific workflows, and efficient execution of the authoring GUI actions. We evaluate BIMgent on real-world building modeling tasks, including both text-based conceptual design generation and reconstruction from existing building design. The design quality achieved by BIMgent was found to be reasonable. Its operations achieved a 32% success rate, whereas all baseline models failed to complete the tasks (0% success rate). Results demonstrate that BIMgent effectively reduces manual workload while preserving design intent, highlighting its potential for practical deployment in real-world architectural modeling scenarios. Project page: https://tumcms.github.io/BIMgent.github.io/",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "ICML 2025 Workshop on Computer Use Agents",
    "pdf_url": "https://arxiv.org/pdf/2506.07217v2",
    "published_date": "2025-06-08 16:45:31 UTC",
    "updated_date": "2025-06-30 08:31:07 UTC"
  },
  {
    "arxiv_id": "2506.08049v3",
    "title": "Physics-Informed Teleconnection-Aware Transformer for Global Subseasonal-to-Seasonal Forecasting",
    "authors": [
      "Tengfei Lyu",
      "Weijia Zhang",
      "Hao Liu"
    ],
    "abstract": "Subseasonal-to-seasonal (S2S) forecasting, which predicts climate conditions from several weeks to months in advance, represents a critical frontier for agricultural planning, energy management, and disaster preparedness. However, it remains one of the most challenging problems in atmospheric science, due to the chaotic dynamics of atmospheric systems and complex interactions across multiple scales. Current approaches often fail to explicitly model underlying physical processes and teleconnections that are crucial at S2S timescales. We introduce \\textbf{TelePiT}, a novel deep learning architecture that enhances global S2S forecasting through integrated multi-scale physics and teleconnection awareness. Our approach consists of three key components: (1) Spherical Harmonic Embedding, which accurately encodes global atmospheric variables onto spherical geometry; (2) Multi-Scale Physics-Informed Neural ODE, which explicitly captures atmospheric physical processes across multiple learnable frequency bands; (3) Teleconnection-Aware Transformer, which models critical global climate interactions through explicitly modeling teleconnection patterns into the self-attention. Extensive experiments demonstrate that \\textbf{TelePiT} significantly outperforms state-of-the-art data-driven baselines and operational numerical weather prediction systems across all forecast horizons, marking a significant advance toward reliable S2S forecasting.",
    "categories": [
      "stat.ML",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "stat.ML",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.08049v3",
    "published_date": "2025-06-08 16:32:21 UTC",
    "updated_date": "2025-08-11 00:46:56 UTC"
  },
  {
    "arxiv_id": "2506.09067v1",
    "title": "Enhancing the Safety of Medical Vision-Language Models by Synthetic Demonstrations",
    "authors": [
      "Zhiyu Xue",
      "Reza Abbasi-Asl",
      "Ramtin Pedarsani"
    ],
    "abstract": "Generative medical vision-language models~(Med-VLMs) are primarily designed to generate complex textual information~(e.g., diagnostic reports) from multimodal inputs including vision modality~(e.g., medical images) and language modality~(e.g., clinical queries). However, their security vulnerabilities remain underexplored. Med-VLMs should be capable of rejecting harmful queries, such as \\textit{Provide detailed instructions for using this CT scan for insurance fraud}. At the same time, addressing security concerns introduces the risk of over-defense, where safety-enhancing mechanisms may degrade general performance, causing Med-VLMs to reject benign clinical queries. In this paper, we propose a novel inference-time defense strategy to mitigate harmful queries, enabling defense against visual and textual jailbreak attacks. Using diverse medical imaging datasets collected from nine modalities, we demonstrate that our defense strategy based on synthetic clinical demonstrations enhances model safety without significantly compromising performance. Additionally, we find that increasing the demonstration budget alleviates the over-defense issue. We then introduce a mixed demonstration strategy as a trade-off solution for balancing security and performance under few-shot demonstration budget constraints.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.09067v1",
    "published_date": "2025-06-08 16:26:51 UTC",
    "updated_date": "2025-06-08 16:26:51 UTC"
  },
  {
    "arxiv_id": "2506.07211v1",
    "title": "Sword and Shield: Uses and Strategies of LLMs in Navigating Disinformation",
    "authors": [
      "Gionnieve Lim",
      "Bryan Chen Zhengyu Tan",
      "Kellie Yu Hui Sim",
      "Weiyan Shi",
      "Ming Hui Chew",
      "Ming Shan Hee",
      "Roy Ka-Wei Lee",
      "Simon T. Perrault",
      "Kenny Tsu Wei Choo"
    ],
    "abstract": "The emergence of Large Language Models (LLMs) presents a dual challenge in the fight against disinformation. These powerful tools, capable of generating human-like text at scale, can be weaponised to produce sophisticated and persuasive disinformation, yet they also hold promise for enhancing detection and mitigation strategies. This paper investigates the complex dynamics between LLMs and disinformation through a communication game that simulates online forums, inspired by the game Werewolf, with 25 participants. We analyse how Disinformers, Moderators, and Users leverage LLMs to advance their goals, revealing both the potential for misuse and combating disinformation. Our findings highlight the varying uses of LLMs depending on the participants' roles and strategies, underscoring the importance of understanding their effectiveness in this context. We conclude by discussing implications for future LLM development and online platform design, advocating for a balanced approach that empowers users and fosters trust while mitigating the risks of LLM-assisted disinformation.",
    "categories": [
      "cs.HC",
      "cs.AI"
    ],
    "primary_category": "cs.HC",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.07211v1",
    "published_date": "2025-06-08 16:24:11 UTC",
    "updated_date": "2025-06-08 16:24:11 UTC"
  },
  {
    "arxiv_id": "2506.11111v2",
    "title": "Evaluating and Improving Robustness in Large Language Models: A Survey and Future Directions",
    "authors": [
      "Kun Zhang",
      "Le Wu",
      "Kui Yu",
      "Guangyi Lv",
      "Dacao Zhang"
    ],
    "abstract": "Large Language Models (LLMs) have gained enormous attention in recent years due to their capability of understanding and generating natural languages. With the rapid development and wild-range applications (e.g., Agents, Embodied Intelligence), the robustness of LLMs has received increased attention. As the core brain of many AI applications, the robustness of LLMs requires that models should not only generate consistent contents, but also ensure the correctness and stability of generated content when dealing with unexpeted application scenarios (e.g., toxic prompts, limited noise domain data, outof-distribution (OOD) applications, etc). In this survey paper, we conduct a thorough review of the robustness of LLMs, aiming to provide a comprehensive terminology of concepts and methods around this field and facilitate the community. Specifically, we first give a formal definition of LLM robustness and present the collection protocol of this survey paper. Then, based on the types of perturbated inputs, we organize this survey from the following perspectives: 1) Adversarial Robustness: tackling the problem that prompts are manipulated intentionally, such as noise prompts, long context, data attack, etc; 2) OOD Robustness: dealing with the unexpected real-world application scenarios, such as OOD detection, zero-shot transferring, hallucinations, etc; 3) Evaluation of Robustness: summarizing the new evaluation datasets, metrics, and tools for verifying the robustness of LLMs. After reviewing the representative work from each perspective, we discuss and highlight future opportunities and research directions in this field. Meanwhile, we also organize related works and provide an easy-to-search project (https://github.com/zhangkunzk/Awesome-LLM-Robustness-papers) to support the community.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "33 pages, 5 figures",
    "pdf_url": "https://arxiv.org/pdf/2506.11111v2",
    "published_date": "2025-06-08 16:20:12 UTC",
    "updated_date": "2025-07-09 06:18:33 UTC"
  },
  {
    "arxiv_id": "2506.09066v1",
    "title": "ReStNet: A Reusable & Stitchable Network for Dynamic Adaptation on IoT Devices",
    "authors": [
      "Maoyu Wang",
      "Yao Lu",
      "Jiaqi Nie",
      "Zeyu Wang",
      "Yun Lin",
      "Qi Xuan",
      "Guan Gui"
    ],
    "abstract": "With the rapid development of deep learning, a growing number of pre-trained models have been publicly available. However, deploying these fixed models in real-world IoT applications is challenging because different devices possess heterogeneous computational and memory resources, making it impossible to deploy a single model across all platforms. Although traditional compression methods, such as pruning, quantization, and knowledge distillation, can improve efficiency, they become inflexible once applied and cannot adapt to changing resource constraints. To address these issues, we propose ReStNet, a Reusable and Stitchable Network that dynamically constructs a hybrid network by stitching two pre-trained models together. Implementing ReStNet requires addressing several key challenges, including how to select the optimal stitching points, determine the stitching order of the two pre-trained models, and choose an effective fine-tuning strategy. To systematically address these challenges and adapt to varying resource constraints, ReStNet determines the stitching point by calculating layer-wise similarity via Centered Kernel Alignment (CKA). It then constructs the hybrid model by retaining early layers from a larger-capacity model and appending deeper layers from a smaller one. To facilitate efficient deployment, only the stitching layer is fine-tuned. This design enables rapid adaptation to changing budgets while fully leveraging available resources. Moreover, ReStNet supports both homogeneous (CNN-CNN, Transformer-Transformer) and heterogeneous (CNN-Transformer) stitching, allowing to combine different model families flexibly. Extensive experiments on multiple benchmarks demonstrate that ReStNet achieve flexible accuracy-efficiency trade-offs at runtime while significantly reducing training cost.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.09066v1",
    "published_date": "2025-06-08 16:14:37 UTC",
    "updated_date": "2025-06-08 16:14:37 UTC"
  },
  {
    "arxiv_id": "2506.07202v1",
    "title": "Reasoning Multimodal Large Language Model: Data Contamination and Dynamic Evaluation",
    "authors": [
      "Ming Liu",
      "Wensheng Zhang"
    ],
    "abstract": "Multimodal Large Language Models (MLLMs) show impressive vision-language benchmark performance, yet growing concerns about data contamination (test set exposure during training) risk masking true generalization. This concern extends to reasoning MLLMs, often fine-tuned via reinforcement learning from potentially contaminated base models. We propose a novel dynamic evaluation framework to rigorously assess MLLM generalization, moving beyond static benchmarks. Instead of perturbing inputs, we perturb the task itself. Using the same visual input, models are evaluated across a family of tasks (e.g., QA, captioning, question posing, verification) to probe diverse capabilities. This task perturbation reveals whether model performance is robust or reliant on superficial task-specific cues. Our approach is analogous to loss landscape sharpness: models overfit or contaminated for a single task (sharp minima) falter under task shifts, unlike models with generalizable solutions (flatter minima). We developed an automated pipeline with a calibrated judge scoring open-ended generations (captions, questions) using paraphrase and corruption sampling. Applying this framework to leading image/video MLLMs on benchmarks including MME, RealWorldQA, and CVRR-ES, we analyze each model's cross-task \"ability vector.\" We demonstrate that fine-tuning on simulated test data (extreme contamination) drastically sharpens task-specific performance but harms overall generalization. Our dynamic task perturbation offers deeper insights into MLLM generalization, distinguishing genuine understanding from spurious leakage or overfitting.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.07202v1",
    "published_date": "2025-06-08 15:52:38 UTC",
    "updated_date": "2025-06-08 15:52:38 UTC"
  },
  {
    "arxiv_id": "2506.07194v1",
    "title": "Exploring Effective Strategies for Building a Customised GPT Agent for Coding Classroom Dialogues",
    "authors": [
      "Luwei Bai",
      "Dongkeun Han",
      "Sara Hennessy"
    ],
    "abstract": "This study investigates effective strategies for developing a customised GPT agent to code classroom dialogue. While classroom dialogue is widely recognised as a crucial element of education, its analysis remains challenging due to the need for a nuanced understanding of dialogic functions and the labour-intensive nature of manual transcript coding. Recent advancements in large language models offer promising avenues for automating this process. However, existing studies predominantly focus on training large-scale models or evaluating pre-trained models with fixed codebooks, which are often not applicable or replicable for dialogue researchers working with small datasets or customised coding schemes. Using GPT-4's MyGPT agent as a case, this study evaluates its baseline performance in coding classroom dialogue with a human codebook and examines how performance varies with different example inputs through a variable control method. Through a design-based research approach, it identifies a set of practical strategies, based on MyGPT's unique features, for configuring effective agents with limited data. The findings suggest that, despite some limitations, a MyGPT agent developed with these strategies can serve as a useful coding assistant by generating coding suggestions.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "Draft technical report. 39 pages, 2 figures. Not yet submitted for publication. Update expected",
    "pdf_url": "https://arxiv.org/pdf/2506.07194v1",
    "published_date": "2025-06-08 15:29:05 UTC",
    "updated_date": "2025-06-08 15:29:05 UTC"
  },
  {
    "arxiv_id": "2506.07184v1",
    "title": "Mitigating Behavioral Hallucination in Multimodal Large Language Models for Sequential Images",
    "authors": [
      "Liangliang You",
      "Junchi Yao",
      "Shu Yang",
      "Guimin Hu",
      "Lijie Hu",
      "Di Wang"
    ],
    "abstract": "While multimodal large language models excel at various tasks, they still suffer from hallucinations, which limit their reliability and scalability for broader domain applications. To address this issue, recent research mainly focuses on objective hallucination. However, for sequential images, besides objective hallucination, there is also behavioral hallucination, which is less studied. This work aims to fill in the gap. We first reveal that behavioral hallucinations mainly arise from two key factors: prior-driven bias and the snowball effect. Based on these observations, we introduce SHE (Sequence Hallucination Eradication), a lightweight, two-stage framework that (1) detects hallucinations via visual-textual alignment check using our proposed adaptive temporal window and (2) mitigates them via orthogonal projection onto the joint embedding space. We also propose a new metric (BEACH) to quantify behavioral hallucination severity. Empirical results on standard benchmarks demonstrate that SHE reduces behavioral hallucination by over 10% on BEACH while maintaining descriptive accuracy.",
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.CV"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.07184v1",
    "published_date": "2025-06-08 15:08:52 UTC",
    "updated_date": "2025-06-08 15:08:52 UTC"
  },
  {
    "arxiv_id": "2506.07180v2",
    "title": "Flattery in Motion: Benchmarking and Analyzing Sycophancy in Video-LLMs",
    "authors": [
      "Wenrui Zhou",
      "Mohamed Hendy",
      "Shu Yang",
      "Qingsong Yang",
      "Zikun Guo",
      "Yuyu Luo",
      "Lijie Hu",
      "Di Wang"
    ],
    "abstract": "As video large language models (Video-LLMs) become increasingly integrated into real-world applications that demand grounded multimodal reasoning, ensuring their factual consistency and reliability is of critical importance. However, sycophancy, the tendency of these models to align with user input even when it contradicts the visual evidence, undermines their trustworthiness in such contexts. Current sycophancy research has largely overlooked its specific manifestations in the video-language domain, resulting in a notable absence of systematic benchmarks and targeted evaluations to understand how Video-LLMs respond under misleading user input. To fill this gap, we propose VISE (Video-LLM Sycophancy Benchmarking and Evaluation), the first benchmark designed to evaluate sycophantic behavior in state-of-the-art Video-LLMs across diverse question formats, prompt biases, and visual reasoning tasks. Specifically, VISE pioneeringly brings linguistic perspectives on sycophancy into the video domain, enabling fine-grained analysis across multiple sycophancy types and interaction patterns. Furthermore, we propose two potential training-free mitigation strategies, revealing potential paths for reducing sycophantic bias: (i) enhancing visual grounding through interpretable key-frame selection and (ii) steering model behavior away from sycophancy via targeted, inference-time intervention on its internal neural representations. Our code is available at https://github.com/William030422/Video-Sycophancy.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.CL",
    "comment": "28 pages",
    "pdf_url": "https://arxiv.org/pdf/2506.07180v2",
    "published_date": "2025-06-08 15:00:21 UTC",
    "updated_date": "2025-10-10 15:15:28 UTC"
  },
  {
    "arxiv_id": "2506.07179v1",
    "title": "Regularized Adaptive Graph Learning for Large-Scale Traffic Forecasting",
    "authors": [
      "Kaiqi Wu",
      "Weiyang Kong",
      "Sen Zhang",
      "Yubao Liu",
      "Zitong Chen"
    ],
    "abstract": "Traffic prediction is a critical task in spatial-temporal forecasting with broad applications in travel planning and urban management. Adaptive graph convolution networks have emerged as mainstream solutions due to their ability to learn node embeddings in a data-driven manner and capture complex latent dependencies. However, existing adaptive graph learning methods for traffic forecasting often either ignore the regularization of node embeddings, which account for a significant proportion of model parameters, or face scalability issues from expensive graph convolution operations. To address these challenges, we propose a Regularized Adaptive Graph Learning (RAGL) model. First, we introduce a regularized adaptive graph learning framework that synergizes Stochastic Shared Embedding (SSE) and adaptive graph convolution via a residual difference mechanism, achieving both embedding regularization and noise suppression. Second, to ensure scalability on large road networks, we develop the Efficient Cosine Operator (ECO), which performs graph convolution based on the cosine similarity of regularized embeddings with linear time complexity. Extensive experiments on four large-scale real-world traffic datasets show that RAGL consistently outperforms state-of-the-art methods in terms of prediction accuracy and exhibits competitive computational efficiency.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.07179v1",
    "published_date": "2025-06-08 14:58:27 UTC",
    "updated_date": "2025-06-08 14:58:27 UTC"
  },
  {
    "arxiv_id": "2506.07177v1",
    "title": "Frame Guidance: Training-Free Guidance for Frame-Level Control in Video Diffusion Models",
    "authors": [
      "Sangwon Jang",
      "Taekyung Ki",
      "Jaehyeong Jo",
      "Jaehong Yoon",
      "Soo Ye Kim",
      "Zhe Lin",
      "Sung Ju Hwang"
    ],
    "abstract": "Advancements in diffusion models have significantly improved video quality, directing attention to fine-grained controllability. However, many existing methods depend on fine-tuning large-scale video models for specific tasks, which becomes increasingly impractical as model sizes continue to grow. In this work, we present Frame Guidance, a training-free guidance for controllable video generation based on frame-level signals, such as keyframes, style reference images, sketches, or depth maps. For practical training-free guidance, we propose a simple latent processing method that dramatically reduces memory usage, and apply a novel latent optimization strategy designed for globally coherent video generation. Frame Guidance enables effective control across diverse tasks, including keyframe guidance, stylization, and looping, without any training, compatible with any video models. Experimental results show that Frame Guidance can produce high-quality controlled videos for a wide range of tasks and input signals.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "Project page: https://frame-guidance-video.github.io/",
    "pdf_url": "https://arxiv.org/pdf/2506.07177v1",
    "published_date": "2025-06-08 14:54:41 UTC",
    "updated_date": "2025-06-08 14:54:41 UTC"
  },
  {
    "arxiv_id": "2506.07173v2",
    "title": "Translating Federated Learning Algorithms in Python into CSP Processes Using ChatGPT",
    "authors": [
      "Miroslav Popovic",
      "Marko Popovic",
      "Miodrag Djukic",
      "Ilija Basicevic"
    ],
    "abstract": "The Python Testbed for Federated Learning Algorithms is a simple Python FL framework that is easy to use by ML&AI developers who do not need to be professional programmers and is also amenable to LLMs. In the previous research, generic federated learning algorithms provided by this framework were manually translated into the CSP processes and algorithms' safety and liveness properties were automatically verified by the model checker PAT. In this paper, a simple translation process is introduced wherein the ChatGPT is used to automate the translation of the mentioned federated learning algorithms in Python into the corresponding CSP processes. Within the process, the minimality of the used context is estimated based on the feedback from ChatGPT. The proposed translation process was experimentally validated by successful translation (verified by the model checker PAT) of both generic centralized and decentralized federated learning algorithms.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "6 pages, 4 tables; Published by IEEE Xplore",
    "pdf_url": "https://arxiv.org/pdf/2506.07173v2",
    "published_date": "2025-06-08 14:45:11 UTC",
    "updated_date": "2025-09-05 09:25:57 UTC"
  },
  {
    "arxiv_id": "2506.07169v1",
    "title": "CTDGSI: A comprehensive exploitation of instance selection methods for automatic text classification. VII Concurso de Teses, Dissertações e Trabalhos de Graduação em SI -- XXI Simpósio Brasileiro de Sistemas de Informação",
    "authors": [
      "Washington Cunha",
      "Leonardo Rocha",
      "Marcos André Gonçalves"
    ],
    "abstract": "Progress in Natural Language Processing (NLP) has been dictated by the rule of more: more data, more computing power and more complexity, best exemplified by the Large Language Models. However, training (or fine-tuning) large dense models for specific applications usually requires significant amounts of computing resources. This \\textbf{Ph.D. dissertation} focuses on an under-investi\\-gated NLP data engineering technique, whose potential is enormous in the current scenario known as Instance Selection (IS). The IS goal is to reduce the training set size by removing noisy or redundant instances while maintaining the effectiveness of the trained models and reducing the training process cost. We provide a comprehensive and scientifically sound comparison of IS methods applied to an essential NLP task -- Automatic Text Classification (ATC), considering several classification solutions and many datasets. Our findings reveal a significant untapped potential for IS solutions. We also propose two novel IS solutions that are noise-oriented and redundancy-aware, specifically designed for large datasets and transformer architectures. Our final solution achieved an average reduction of 41\\% in training sets, while maintaining the same levels of effectiveness in all datasets. Importantly, our solutions demonstrated speedup improvements of 1.67x (up to 2.46x), making them scalable for datasets with hundreds of thousands of documents.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "16 pages, 5 figures, 2 tables",
    "pdf_url": "https://arxiv.org/pdf/2506.07169v1",
    "published_date": "2025-06-08 14:34:57 UTC",
    "updated_date": "2025-06-08 14:34:57 UTC"
  },
  {
    "arxiv_id": "2506.07168v1",
    "title": "Efficient Text-Attributed Graph Learning through Selective Annotation and Graph Alignment",
    "authors": [
      "Huanyi Xie",
      "Lijie Hu",
      "Lu Yu",
      "Tianhao Huang",
      "Longfei Li",
      "Meng Li",
      "Jun Zhou",
      "Huan Wang",
      "Di Wang"
    ],
    "abstract": "In the realm of Text-attributed Graphs (TAGs), traditional graph neural networks (GNNs) often fall short due to the complex textual information associated with each node. Recent methods have improved node representations by leveraging large language models (LLMs) to enhance node text features, but these approaches typically require extensive annotations or fine-tuning across all nodes, which is both time-consuming and costly. To overcome these challenges, we introduce GAGA, an efficient framework for TAG representation learning. GAGA reduces annotation time and cost by focusing on annotating only representative nodes and edges. It constructs an annotation graph that captures the topological relationships among these annotations. Furthermore, GAGA employs a two-level alignment module to effectively integrate the annotation graph with the TAG, aligning their underlying structures. Experiments show that GAGA achieves classification accuracies on par with or surpassing state-of-the-art methods while requiring only 1% of the data to be annotated, demonstrating its high efficiency.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.LG",
    "comment": "23 pages",
    "pdf_url": "https://arxiv.org/pdf/2506.07168v1",
    "published_date": "2025-06-08 14:34:29 UTC",
    "updated_date": "2025-06-08 14:34:29 UTC"
  },
  {
    "arxiv_id": "2506.07165v1",
    "title": "AMoPO: Adaptive Multi-objective Preference Optimization without Reward Models and Reference Models",
    "authors": [
      "Qi Liu",
      "Jingqing Ruan",
      "Hao Li",
      "Haodong Zhao",
      "Desheng Wang",
      "Jiansong Chen",
      "Wan Guanglu",
      "Xunliang Cai",
      "Zhi Zheng",
      "Tong Xu"
    ],
    "abstract": "Existing multi-objective preference alignment methods for large language models (LLMs) face limitations: (1) the inability to effectively balance various preference dimensions, and (2) reliance on auxiliary reward/reference models introduces computational complexity. To address these challenges, we propose Adaptive Multi-objective Preference Optimization (AMoPO), a novel framework that achieves dynamic balance across preference dimensions. By introducing the multi-objective optimization paradigm to use the dimension-aware generation metrics as implicit rewards, AMoPO aligns LLMs with diverse preferences without additional reward models or reference models. We introduce an adaptive weight assignment mechanism that models the generation space as a Gaussian distribution, allowing dynamic prioritization of preference dimensions. Empirical results demonstrate that AMoPO outperforms state-of-the-art baselines by 28.5%, and the experiments on 7B, 14B, and 32B models reveal the scaling ability of AMoPO. Moreover, additional analysis of multiple dimensions verifies its adaptability and effectiveness. These findings validate AMoPO's capability to achieve dimension-aware preference alignment, highlighting its superiority. Our codes and datasets are available at https://github.com/Javkonline/AMoPO.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted by ACL 2025",
    "pdf_url": "https://arxiv.org/pdf/2506.07165v1",
    "published_date": "2025-06-08 14:31:06 UTC",
    "updated_date": "2025-06-08 14:31:06 UTC"
  },
  {
    "arxiv_id": "2506.08048v2",
    "title": "Toward Reliable AR-Guided Surgical Navigation: Interactive Deformation Modeling with Data-Driven Biomechanics and Prompts",
    "authors": [
      "Zheng Han",
      "Jun Zhou",
      "Jialun Pei",
      "Jing Qin",
      "Yingfang Fan",
      "Qi Dou"
    ],
    "abstract": "In augmented reality (AR)-guided surgical navigation, preoperative organ models are superimposed onto the patient's intraoperative anatomy to visualize critical structures such as vessels and tumors. Accurate deformation modeling is essential to maintain the reliability of AR overlays by ensuring alignment between preoperative models and the dynamically changing anatomy. Although the finite element method (FEM) offers physically plausible modeling, its high computational cost limits intraoperative applicability. Moreover, existing algorithms often fail to handle large anatomical changes, such as those induced by pneumoperitoneum or ligament dissection, leading to inaccurate anatomical correspondences and compromised AR guidance. To address these challenges, we propose a data-driven biomechanics algorithm that preserves FEM-level accuracy while improving computational efficiency. In addition, we introduce a novel human-in-the-loop mechanism into the deformation modeling process. This enables surgeons to interactively provide prompts to correct anatomical misalignments, thereby incorporating clinical expertise and allowing the model to adapt dynamically to complex surgical scenarios. Experiments on a publicly available dataset demonstrate that our algorithm achieves a mean target registration error of 3.42 mm. Incorporating surgeon prompts through the interactive framework further reduces the error to 2.78 mm, surpassing state-of-the-art methods in volumetric accuracy. These results highlight the ability of our framework to deliver efficient and accurate deformation modeling while enhancing surgeon-algorithm collaboration, paving the way for safer and more reliable computer-assisted surgeries.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.RO"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.08048v2",
    "published_date": "2025-06-08 14:19:54 UTC",
    "updated_date": "2025-06-11 03:34:08 UTC"
  },
  {
    "arxiv_id": "2506.11110v1",
    "title": "AssertBench: A Benchmark for Evaluating Self-Assertion in Large Language Models",
    "authors": [
      "Jaeho Lee",
      "Atharv Chowdhary"
    ],
    "abstract": "Recent benchmarks have probed factual consistency and rhetorical robustness in Large Language Models (LLMs). However, a knowledge gap exists regarding how directional framing of factually true statements influences model agreement, a common scenario for LLM users. AssertBench addresses this by sampling evidence-supported facts from FEVEROUS, a fact verification dataset. For each (evidence-backed) fact, we construct two framing prompts: one where the user claims the statement is factually correct, and another where the user claims it is incorrect. We then record the model's agreement and reasoning. The desired outcome is that the model asserts itself, maintaining consistent truth evaluation across both framings, rather than switching its evaluation to agree with the user. AssertBench isolates framing-induced variability from the model's underlying factual knowledge by stratifying results based on the model's accuracy on the same claims when presented neutrally. In doing so, this benchmark aims to measure an LLM's ability to \"stick to its guns\" when presented with contradictory user assertions about the same fact. The complete source code is available at https://github.com/achowd32/assert-bench.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "15 pages, 4 figures, appendix contains 2 additional figures and 2 tables",
    "pdf_url": "https://arxiv.org/pdf/2506.11110v1",
    "published_date": "2025-06-08 14:08:22 UTC",
    "updated_date": "2025-06-08 14:08:22 UTC"
  },
  {
    "arxiv_id": "2506.07154v1",
    "title": "Syntactic Control of Language Models by Posterior Inference",
    "authors": [
      "Vicky Xefteri",
      "Tim Vieira",
      "Ryan Cotterell",
      "Afra Amini"
    ],
    "abstract": "Controlling the syntactic structure of text generated by language models is valuable for applications requiring clarity, stylistic consistency, or interpretability, yet it remains a challenging task. In this paper, we argue that sampling algorithms based on the posterior inference can effectively enforce a target constituency structure during generation. Our approach combines sequential Monte Carlo, which estimates the posterior distribution by sampling from a proposal distribution, with a syntactic tagger that ensures that each generated token aligns with the desired syntactic structure. Our experiments with GPT2 and Llama3-8B models show that with an appropriate proposal distribution, we can improve syntactic accuracy, increasing the F1 score from $12.31$ (GPT2-large) and $35.33$ (Llama3-8B) to about $93$ in both cases without compromising the language model's fluency. These results underscore both the complexity of syntactic control and the effectiveness of sampling algorithms, offering a promising approach for applications where precise control over syntax is essential.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.07154v1",
    "published_date": "2025-06-08 14:01:34 UTC",
    "updated_date": "2025-06-08 14:01:34 UTC"
  },
  {
    "arxiv_id": "2506.07153v2",
    "title": "Mind the Web: The Security of Web Use Agents",
    "authors": [
      "Avishag Shapira",
      "Parth Atulbhai Gandhi",
      "Edan Habler",
      "Asaf Shabtai"
    ],
    "abstract": "Web-use agents are rapidly being deployed to automate complex web tasks with extensive browser capabilities. However, these capabilities create a critical and previously unexplored attack surface. This paper demonstrates how attackers can exploit web-use agents by embedding malicious content in web pages, such as comments, reviews, or advertisements, that agents encounter during legitimate browsing tasks. We introduce the task-aligned injection technique that frames malicious commands as helpful task guidance rather than obvious attacks, exploiting fundamental limitations in LLMs' contextual reasoning. Agents struggle to maintain coherent contextual awareness and fail to detect when seemingly helpful web content contains steering attempts that deviate them from their original task goal. To scale this attack, we developed an automated three-stage pipeline that generates effective injections without manual annotation or costly online agent interactions during training, remaining efficient even with limited training data. This pipeline produces a generator model that we evaluate on five popular agents using payloads organized by the Confidentiality-Integrity-Availability (CIA) security triad, including unauthorized camera activation, file exfiltration, user impersonation, phishing, and denial-of-service. This generator achieves over 80% attack success rate (ASR) with strong transferability across unseen payloads, diverse web environments, and different underlying LLMs. This attack succeed even against agents with built-in safety mechanisms, requiring only the ability to post content on public websites. To address this risk, we propose comprehensive mitigation strategies including oversight mechanisms, execution constraints, and task-aware reasoning techniques.",
    "categories": [
      "cs.CR",
      "cs.AI"
    ],
    "primary_category": "cs.CR",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.07153v2",
    "published_date": "2025-06-08 13:59:55 UTC",
    "updated_date": "2025-10-20 18:33:40 UTC"
  },
  {
    "arxiv_id": "2506.13773v2",
    "title": "Representing Time-Continuous Behavior of Cyber-Physical Systems in Knowledge Graphs",
    "authors": [
      "Milapji Singh Gill",
      "Tom Jeleniewski",
      "Felix Gehlhoff",
      "Alexander Fay"
    ],
    "abstract": "Time-continuous dynamic models are essential for various Cyber-Physical System (CPS) applications. To ensure effective usability in different lifecycle phases, such behavioral information in the form of differential equations must be contextualized and integrated with further CPS information. While knowledge graphs provide a formal description and structuring mechanism for this task, there is a lack of reusable ontological artifacts and methods to reduce manual instantiation effort. Hence, this contribution introduces two artifacts: Firstly, a modular semantic model based on standards is introduced to represent differential equations directly within knowledge graphs and to enrich them semantically. Secondly, a method for efficient knowledge graph generation is presented. A validation of these artifacts was conducted in the domain of aviation maintenance. Results show that differential equations of a complex Electro-Hydraulic Servoactuator can be formally represented in a knowledge graph and be contextualized with other lifecycle data, proving the artifacts' practical applicability.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "\\c{opyright} 2025 IEEE. Personal use of this material is permitted. Permission from IEEE must be obtained for all other uses, in any current or future media, including reprinting/republishing this material for advertising or promotional purposes, creating new collective works, for resale or redistribution to servers or lists, or reuse of any copyrighted component of this work in other works",
    "pdf_url": "https://arxiv.org/pdf/2506.13773v2",
    "published_date": "2025-06-08 13:54:23 UTC",
    "updated_date": "2026-01-17 21:19:08 UTC"
  },
  {
    "arxiv_id": "2506.07142v1",
    "title": "Prompting Science Report 2: The Decreasing Value of Chain of Thought in Prompting",
    "authors": [
      "Lennart Meincke",
      "Ethan Mollick",
      "Lilach Mollick",
      "Dan Shapiro"
    ],
    "abstract": "This is the second in a series of short reports that seek to help business, education, and policy leaders understand the technical details of working with AI through rigorous testing. In this report, we investigate Chain-of-Thought (CoT) prompting, a technique that encourages a large language model (LLM) to \"think step by step\" (Wei et al., 2022). CoT is a widely adopted method for improving reasoning tasks, however, our findings reveal a more nuanced picture of its effectiveness. We demonstrate two things:\n  - The effectiveness of Chain-of-Thought prompting can vary greatly depending on the type of task and model. For non-reasoning models, CoT generally improves average performance by a small amount, particularly if the model does not inherently engage in step-by-step processing by default. However, CoT can introduce more variability in answers, sometimes triggering occasional errors in questions the model would otherwise get right. We also found that many recent models perform some form of CoT reasoning even if not asked; for these models, a request to perform CoT had little impact. Performing CoT generally requires far more tokens (increasing cost and time) than direct answers.\n  - For models designed with explicit reasoning capabilities, CoT prompting often results in only marginal, if any, gains in answer accuracy. However, it significantly increases the time and tokens needed to generate a response.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.07142v1",
    "published_date": "2025-06-08 13:41:25 UTC",
    "updated_date": "2025-06-08 13:41:25 UTC"
  },
  {
    "arxiv_id": "2506.07138v1",
    "title": "Learning Compact Vision Tokens for Efficient Large Multimodal Models",
    "authors": [
      "Hao Tang",
      "Chengchao Shen"
    ],
    "abstract": "Large multimodal models (LMMs) suffer significant computational challenges due to the high cost of Large Language Models (LLMs) and the quadratic complexity of processing long vision token sequences. In this paper, we explore the spatial redundancy among vision tokens and shorten the length of vision token sequences for inference acceleration. Specifically, we propose a Spatial Token Fusion (STF) method to learn compact vision tokens for short vision token sequence, where spatial-adjacent tokens are fused into one. Meanwhile, weight-frozen vision encoder can not well adapt to the demand of extensive downstream vision-language tasks. To this end, we further introduce a Multi-Block Token Fusion (MBTF) module to supplement multi-granularity features for the reduced token sequence. Overall, we combine STF and MBTF module to balance token reduction and information preservation, thereby improving inference efficiency without sacrificing multimodal reasoning capabilities. Experimental results demonstrate that our method based on LLaVA-1.5 achieves comparable or even superior performance to the baseline on 8 popular vision-language benchmarks with only $25\\%$ vision tokens of baseline. The source code and trained weights are available at https://github.com/visresearch/LLaVA-STF.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL",
      "cs.MM"
    ],
    "primary_category": "cs.CV",
    "comment": "The source code and trained weights are available at https://github.com/visresearch/LLaVA-STF",
    "pdf_url": "https://arxiv.org/pdf/2506.07138v1",
    "published_date": "2025-06-08 13:36:06 UTC",
    "updated_date": "2025-06-08 13:36:06 UTC"
  },
  {
    "arxiv_id": "2506.08047v1",
    "title": "Evaluation of Machine Learning Models in Student Academic Performance Prediction",
    "authors": [
      "A. G. R. Sandeepa",
      "Sanka Mohottala"
    ],
    "abstract": "This research investigates the use of machine learning methods to forecast students' academic performance in a school setting. Students' data with behavioral, academic, and demographic details were used in implementations with standard classical machine learning models including multi-layer perceptron classifier (MLPC). MLPC obtained 86.46% maximum accuracy for test set across all implementations. Under 10-fold cross validation, MLPC obtained 79.58% average accuracy for test set while for train set, it was 99.65%. MLP's better performance over other machine learning models strongly suggest the potential use of neural networks as data-efficient models. Feature selection approach played a crucial role in improving the performance and multiple evaluation approaches were used in order to compare with existing literature. Explainable machine learning methods were utilized to demystify the black box models and to validate the feature selection approach.",
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CY",
    "comment": "Paper Accepted for IEEE ICARC Conference (2025). 6 pages, 5 figures",
    "pdf_url": "https://arxiv.org/pdf/2506.08047v1",
    "published_date": "2025-06-08 13:33:49 UTC",
    "updated_date": "2025-06-08 13:33:49 UTC"
  },
  {
    "arxiv_id": "2506.07135v1",
    "title": "Taxonomy of migration scenarios for Qiskit refactoring using LLMs",
    "authors": [
      "José Manuel Suárez",
      "Luís Mariano Bibbó",
      "Joaquín Bogado",
      "Alejandro Fernandez"
    ],
    "abstract": "As quantum computing advances, quantum programming libraries' heterogeneity and steady evolution create new challenges for software developers. Frequent updates in software libraries break working code that needs to be refactored, thus adding complexity to an already complex landscape. These refactoring challenges are, in many cases, fundamentally different from those known in classical software engineering due to the nature of quantum computing software. This study addresses these challenges by developing a taxonomy of quantum circuit's refactoring problems, providing a structured framework to analyze and compare different refactoring approaches. Large Language Models (LLMs) have proven valuable tools for classic software development, yet their value in quantum software engineering remains unexplored. This study uses LLMs to categorize refactoring needs in migration scenarios between different Qiskit versions. Qiskit documentation and release notes were scrutinized to create an initial taxonomy of refactoring required for migrating between Qiskit releases. Two taxonomies were produced: one by expert developers and one by an LLM. These taxonomies were compared, analyzing differences and similarities, and were integrated into a unified taxonomy that reflects the findings of both methods. By systematically categorizing refactoring challenges in Qiskit, the unified taxonomy is a foundation for future research on AI-assisted migration while enabling a more rigorous evaluation of automated refactoring techniques. Additionally, this work contributes to quantum software engineering (QSE) by enhancing software development workflows, improving language compatibility, and promoting best practices in quantum programming.",
    "categories": [
      "cs.SE",
      "cs.AI",
      "cs.ET"
    ],
    "primary_category": "cs.SE",
    "comment": "Accepted for publication in ASQC JAIIO 54 (https://54jaiio.sadio.org.ar/simposios/)",
    "pdf_url": "https://arxiv.org/pdf/2506.07135v1",
    "published_date": "2025-06-08 13:28:52 UTC",
    "updated_date": "2025-06-08 13:28:52 UTC"
  },
  {
    "arxiv_id": "2506.07134v2",
    "title": "Monotone and Conservative Policy Iteration Beyond the Tabular Case",
    "authors": [
      "S. R. Eshwar",
      "Gugan Thoppe",
      "Ananyabrata Barua",
      "Aditya Gopalan",
      "Gal Dalal"
    ],
    "abstract": "We introduce Reliable Policy Iteration (RPI) and Conservative RPI (CRPI), variants of Policy Iteration (PI) and Conservative PI (CPI), that retain tabular guarantees under function approximation. RPI uses a novel Bellman-constrained optimization for policy evaluation. We show that RPI restores the textbook \\textit{monotonicity} of value estimates and that these estimates provably \\textit{lower-bound} the true return; moreover, their limit partially satisfies the \\textit{unprojected} Bellman equation. CRPI shares RPI's evaluation, but updates policies conservatively by maximizing a new performance-difference \\textit{lower bound} that explicitly accounts for function-approximation-induced errors. CRPI inherits RPI's guarantees and, crucially, admits per-step improvement bounds. In initial simulations, RPI and CRPI outperform PI and its variants. Our work addresses a foundational gap in RL: popular algorithms such as TRPO and PPO derive from tabular CPI yet are deployed with function approximation, where CPI's guarantees often fail-leading to divergence, oscillations, or convergence to suboptimal policies. By restoring PI/CPI-style guarantees for \\textit{arbitrary} function classes, RPI and CRPI provide a principled basis for next-generation RL.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "math.OC"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.07134v2",
    "published_date": "2025-06-08 13:27:11 UTC",
    "updated_date": "2025-10-11 19:01:41 UTC"
  },
  {
    "arxiv_id": "2506.07127v3",
    "title": "Human-assisted Robotic Policy Refinement via Action Preference Optimization",
    "authors": [
      "Wenke Xia",
      "Yichu Yang",
      "Hongtao Wu",
      "Xiao Ma",
      "Tao Kong",
      "Di Hu"
    ],
    "abstract": "Establishing a reliable and iteratively refined robotic system is essential for deploying real-world applications. While Vision-Language-Action (VLA) models are widely recognized as the foundation model for such robotic deployment, their reliance on offline expert demonstrations critically limits their capacity for post-deployment refinement. To mitigate this limitation, we introduce Action Preference Optimization (APO), a method designed to refine VLA models by human-assisted preference alignment gathered through interaction with environments. This method begins with a human-robot collaboration framework for reliable failure correction and interaction trajectory collection through human intervention. However, directly leveraging these interaction trajectories for preference optimization is non-trivial due to the challenges of irreversible robotic actions and token distribution mismatch. To solve this, APO proposes an adaptive reweighting algorithm with binary desirability signals derived from interaction, empowering VLA models effectively suppress failure-prone actions while enhancing corrective action adaptation. Ultimately, APO equips VLA models with the crucial capability to learn from failure, paving the way for their iterative refinement and reliable deployment in dynamic environments. The experiments conducted in simulation and real-world scenarios prove superior generalization and robustness of our human-assisted framework across a variety of manipulation tasks. We believe this work could bring insights for efficient and stable optimization of VLA models through human-robot collaboration. The code and dataset are released at https://github.com/GeWu-Lab/Action-Preference-Optimization",
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "primary_category": "cs.RO",
    "comment": "Accepted By NeurIPS 2025",
    "pdf_url": "https://arxiv.org/pdf/2506.07127v3",
    "published_date": "2025-06-08 13:14:18 UTC",
    "updated_date": "2025-10-30 04:04:19 UTC"
  },
  {
    "arxiv_id": "2506.07126v1",
    "title": "MAGNet: A Multi-Scale Attention-Guided Graph Fusion Network for DRC Violation Detection",
    "authors": [
      "Weihan Lu",
      "Hong Cai Chen"
    ],
    "abstract": "Design rule checking (DRC) is of great significance for cost reduction and design efficiency improvement in integrated circuit (IC) designs. Machine-learning-based DRC has become an important approach in computer-aided design (CAD). In this paper, we propose MAGNet, a hybrid deep learning model that integrates an improved U-Net with a graph neural network for DRC violation prediction. The U-Net backbone is enhanced with a Dynamic Attention Module (DAM) and a Multi-Scale Convolution Module (MSCM) to strengthen its capability in extracting fine-grained and multi-scale spatial features. In parallel, we construct a pixel-aligned graph structure based on chip layout tiles, and apply a specialized GNN to model the topological relationships among pins. During graph construction, a graph-to-grid mapping is generated to align GNN features with the layout image. In addition, a label amplification strategy is adopted during training to enhance the model's sensitivity to sparse violation patterns. Overall, MAGNet effectively combines spatial, semantic, and structural information, achieving improved prediction accuracy and reduced false positive rates in DRC hotspot detection. Subsequently, through incremental training, we achieve a more sensitive discrimination ability for hotspots. The results demonstrate that, in comparison with ibUnet, RouteNet, and J-Net, MAGnet significantly outperforms these models, achieving substantial improvements in overall performance.",
    "categories": [
      "cs.AR",
      "cs.AI"
    ],
    "primary_category": "cs.AR",
    "comment": "9 pages, 12 figures, 2 tables",
    "pdf_url": "https://arxiv.org/pdf/2506.07126v1",
    "published_date": "2025-06-08 13:13:41 UTC",
    "updated_date": "2025-06-08 13:13:41 UTC"
  },
  {
    "arxiv_id": "2506.07122v2",
    "title": "Image Segmentation and Classification of E-waste for Training Robots for Waste Segregation",
    "authors": [
      "Prakriti Tripathi"
    ],
    "abstract": "Industry partners provided a problem statement that involves classifying electronic waste using machine learning models that will be used by pick-and-place robots for waste segregation. This was achieved by taking common electronic waste items, such as a mouse and charger, unsoldering them, and taking pictures to create a custom dataset. Then state-of-the art YOLOv11 model was trained and run to achieve 70 mAP in real-time. Mask-RCNN model was also trained and achieved 41 mAP. The model can be integrated with pick-and-place robots to perform segregation of e-waste.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "3 pages, 2 figures, submitted to 2025 5th International Conference on AI-ML-Systems (AIMLSystems)",
    "pdf_url": "https://arxiv.org/pdf/2506.07122v2",
    "published_date": "2025-06-08 13:09:33 UTC",
    "updated_date": "2025-09-22 21:11:14 UTC"
  },
  {
    "arxiv_id": "2506.07121v1",
    "title": "Quality-Diversity Red-Teaming: Automated Generation of High-Quality and Diverse Attackers for Large Language Models",
    "authors": [
      "Ren-Jian Wang",
      "Ke Xue",
      "Zeyu Qin",
      "Ziniu Li",
      "Sheng Tang",
      "Hao-Tian Li",
      "Shengcai Liu",
      "Chao Qian"
    ],
    "abstract": "Ensuring safety of large language models (LLMs) is important. Red teaming--a systematic approach to identifying adversarial prompts that elicit harmful responses from target LLMs--has emerged as a crucial safety evaluation method. Within this framework, the diversity of adversarial prompts is essential for comprehensive safety assessments. We find that previous approaches to red-teaming may suffer from two key limitations. First, they often pursue diversity through simplistic metrics like word frequency or sentence embedding similarity, which may not capture meaningful variation in attack strategies. Second, the common practice of training a single attacker model restricts coverage across potential attack styles and risk categories. This paper introduces Quality-Diversity Red-Teaming (QDRT), a new framework designed to address these limitations. QDRT achieves goal-driven diversity through behavior-conditioned training and implements a behavioral replay buffer in an open-ended manner. Additionally, it trains multiple specialized attackers capable of generating high-quality attacks across diverse styles and risk categories. Our empirical evaluation demonstrates that QDRT generates attacks that are both more diverse and more effective against a wide range of target LLMs, including GPT-2, Llama-3, Gemma-2, and Qwen2.5. This work advances the field of LLM safety by providing a systematic and effective approach to automated red-teaming, ultimately supporting the responsible deployment of LLMs.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.NE"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.07121v1",
    "published_date": "2025-06-08 13:07:41 UTC",
    "updated_date": "2025-06-08 13:07:41 UTC"
  },
  {
    "arxiv_id": "2506.07118v1",
    "title": "RBA-FE: A Robust Brain-Inspired Audio Feature Extractor for Depression Diagnosis",
    "authors": [
      "Yu-Xuan Wu",
      "Ziyan Huang",
      "Bin Hu",
      "Zhi-Hong Guan"
    ],
    "abstract": "This article proposes a robust brain-inspired audio feature extractor (RBA-FE) model for depression diagnosis, using an improved hierarchical network architecture. Most deep learning models achieve state-of-the-art performance for image-based diagnostic tasks, ignoring the counterpart audio features. In order to tailor the noise challenge, RBA-FE leverages six acoustic features extracted from the raw audio, capturing both spatial characteristics and temporal dependencies. This hybrid attribute helps alleviate the precision limitation in audio feature extraction within other learning models like deep residual shrinkage networks. To deal with the noise issues, our model incorporates an improved spiking neuron model, called adaptive rate smooth leaky integrate-and-fire (ARSLIF). The ARSLIF model emulates the mechanism of ``retuning of cellular signal selectivity\" in the brain attention systems, which enhances the model robustness against environmental noises in audio data. Experimental results demonstrate that RBA-FE achieves state-of-the-art accuracy on the MODMA dataset, respectively with 0.8750, 0.8974, 0.8750 and 0.8750 in precision, accuracy, recall and F1 score. Extensive experiments on the AVEC2014 and DAIC-WOZ datasets both show enhancements in noise robustness. It is further indicated by comparison that the ARSLIF neuron model suggest the abnormal firing pattern within the feature extraction on depressive audio data, offering brain-inspired interpretability.",
    "categories": [
      "cs.SD",
      "cs.AI",
      "eess.AS"
    ],
    "primary_category": "cs.SD",
    "comment": "14 pages",
    "pdf_url": "https://arxiv.org/pdf/2506.07118v1",
    "published_date": "2025-06-08 13:00:45 UTC",
    "updated_date": "2025-06-08 13:00:45 UTC"
  },
  {
    "arxiv_id": "2506.07116v1",
    "title": "BRIGHT+: Upgrading the BRIGHT Benchmark with MARCUS, a Multi-Agent RAG Clean-Up Suite",
    "authors": [
      "Liyang Chen",
      "Yujun Cai",
      "Jieqiong Dong",
      "Yiwei Wang"
    ],
    "abstract": "Retrieval-Augmented Generation (RAG) systems require corpora that are both structurally clean and semantically coherent. BRIGHT is a recent and influential benchmark designed to evaluate complex multi-hop retrieval across diverse, high-reasoning domains. However, its practical effectiveness is limited by common web-crawled artifacts - such as content redundancy and semantic discontinuity - that impair retrieval accuracy and downstream reasoning. Notably, we find that such issues are concentrated in seven StackExchange-derived subdomains, while other domains (e.g., Coding and Theorem-based content) remain relatively clean.\n  In this study, we present MARCUS, a multi-agent pipeline that leverages large language models (LLMs) to systematically clean and re-chunk BRIGHT into a higher-quality corpus: BRIGHT-Plus. MARCUS applies dedicated agents for structural noise removal and semantic segmentation, preserving answer-bearing spans while improving contextual integrity. Experimental evaluations demonstrate that BRIGHT-Plus yields consistent and significant improvements in both retrieval accuracy and multi-hop reasoning across a diverse set of retrievers. We release both the BRIGHT-Plus corpus and the MARCUS pipeline to support future research on robust, reasoning-centric retrieval.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "8 pages, 7 figures, 4 tables. Submitted to EMNLP 2025",
    "pdf_url": "https://arxiv.org/pdf/2506.07116v1",
    "published_date": "2025-06-08 12:59:04 UTC",
    "updated_date": "2025-06-08 12:59:04 UTC"
  },
  {
    "arxiv_id": "2506.07109v1",
    "title": "Towards Universal Offline Black-Box Optimization via Learning Language Model Embeddings",
    "authors": [
      "Rong-Xi Tan",
      "Ming Chen",
      "Ke Xue",
      "Yao Wang",
      "Yaoyuan Wang",
      "Sheng Fu",
      "Chao Qian"
    ],
    "abstract": "The pursuit of universal black-box optimization (BBO) algorithms is a longstanding goal. However, unlike domains such as language or vision, where scaling structured data has driven generalization, progress in offline BBO remains hindered by the lack of unified representations for heterogeneous numerical spaces. Thus, existing offline BBO approaches are constrained to single-task and fixed-dimensional settings, failing to achieve cross-domain universal optimization. Recent advances in language models (LMs) offer a promising path forward: their embeddings capture latent relationships in a unifying way, enabling universal optimization across different data types possible. In this paper, we discuss multiple potential approaches, including an end-to-end learning framework in the form of next-token prediction, as well as prioritizing the learning of latent spaces with strong representational capabilities. To validate the effectiveness of these methods, we collect offline BBO tasks and data from open-source academic works for training. Experiments demonstrate the universality and effectiveness of our proposed methods. Our findings suggest that unifying language model priors and learning string embedding space can overcome traditional barriers in universal BBO, paving the way for general-purpose BBO algorithms. The code is provided at https://github.com/lamda-bbo/universal-offline-bbo.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.NE"
    ],
    "primary_category": "cs.LG",
    "comment": "ICML 2025",
    "pdf_url": "https://arxiv.org/pdf/2506.07109v1",
    "published_date": "2025-06-08 12:41:18 UTC",
    "updated_date": "2025-06-08 12:41:18 UTC"
  },
  {
    "arxiv_id": "2506.07106v2",
    "title": "Theorem-of-Thought: A Multi-Agent Framework for Abductive, Deductive, and Inductive Reasoning in Language Models",
    "authors": [
      "Samir Abdaljalil",
      "Hasan Kurban",
      "Khalid Qaraqe",
      "Erchin Serpedin"
    ],
    "abstract": "Large language models (LLMs) have shown strong performance across natural language reasoning tasks, yet their reasoning processes remain brittle and difficult to interpret. Prompting techniques like Chain-of-Thought (CoT) enhance reliability by eliciting intermediate reasoning steps or aggregating multiple outputs. However, they lack mechanisms for enforcing logical structure and assessing internal coherence. We introduce Theorem-of-Thought (ToTh), a novel framework that models reasoning as collaboration among three parallel agents, each simulating a distinct mode of inference: abductive, deductive, and inductive. Each agent produces a reasoning trace, which is structured into a formal reasoning graph. To evaluate consistency, we apply Bayesian belief propagation guided by natural language inference (NLI), assigning confidence scores to each step. The most coherent graph is selected to derive the final answer. Experiments on symbolic (WebOfLies) and numerical (MultiArith) reasoning benchmarks show that ToTh consistently outperforms CoT, Self-Consistency, and CoT-Decoding across multiple LLMs, while producing interpretable and logically grounded reasoning chains. Our findings suggest a promising direction for building more robust and cognitively inspired LLM reasoning. The implementation is available at https://github.com/KurbanIntelligenceLab/theorem-of-thought.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "ACL 2025 KnowFM",
    "pdf_url": "https://arxiv.org/pdf/2506.07106v2",
    "published_date": "2025-06-08 12:28:38 UTC",
    "updated_date": "2025-07-31 09:33:35 UTC"
  },
  {
    "arxiv_id": "2506.17254v1",
    "title": "Keeping Up with the Models: Online Deployment and Routing of LLMs at Scale",
    "authors": [
      "Shaoang Li",
      "Jian Li"
    ],
    "abstract": "The rapid pace at which new large language models (LLMs) appear -- and older ones become obsolete -- forces LLM service providers to juggle a streaming inventory of models while respecting tight deployment capacity and per-query cost budgets. We cast the reality as an online decision problem that couples stage-wise deployment, made at fixed maintenance windows, with per-query routing among the models kept live. We introduce StageRoute, a hierarchical algorithm that (i) optimistically selects up to $M_max$ models for the next stage using reward upper-confidence and cost lower-confidence bounds, then (ii) solves a budget-constrained bandit sub-problem to route each incoming query. We prove that StageRoute achieves a regret of order $T^{2/3}$ and provide a matching lower bound, thereby establishing its near-optimality. Moreover, our experiments confirm the theory, demonstrating that StageRoute performs close to the optimum in practical settings.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.17254v1",
    "published_date": "2025-06-08 12:25:26 UTC",
    "updated_date": "2025-06-08 12:25:26 UTC"
  },
  {
    "arxiv_id": "2506.07104v2",
    "title": "How Far Are We from Optimal Reasoning Efficiency?",
    "authors": [
      "Jiaxuan Gao",
      "Shu Yan",
      "Qixin Tan",
      "Lu Yang",
      "Shusheng Xu",
      "Wei Fu",
      "Zhiyu Mei",
      "Kaifeng Lyu",
      "Yi Wu"
    ],
    "abstract": "Large Reasoning Models (LRMs) demonstrate remarkable problem-solving capabilities through extended Chain-of-Thought (CoT) reasoning but often produce excessively verbose and redundant reasoning traces. This inefficiency incurs high inference costs and limits practical deployment. While existing fine-tuning methods aim to improve reasoning efficiency, assessing their efficiency gains remains challenging due to inconsistent evaluations. In this work, we introduce the reasoning efficiency frontiers, empirical upper bounds derived from fine-tuning base LRMs across diverse approaches and training configurations. Based on these frontiers, we propose the Reasoning Efficiency Gap (REG), a unified metric quantifying deviations of any fine-tuned LRMs from these frontiers. Systematic evaluation on challenging mathematical benchmarks reveals significant gaps in current methods: they either sacrifice accuracy for short length or still remain inefficient under tight token budgets. To reduce the efficiency gap, we propose REO-RL, a class of Reinforcement Learning algorithms that minimizes REG by targeting a sparse set of token budgets. Leveraging numerical integration over strategically selected budgets, REO-RL approximates the full efficiency objective with low error using a small set of token budgets. Through systematic benchmarking, we demonstrate that our efficiency metric, REG, effectively captures the accuracy-length trade-off, with low-REG methods reducing length while maintaining accuracy. Our approach, REO-RL, consistently reduces REG by >=50 across all evaluated LRMs and matching Qwen3-4B/8B efficiency frontiers under a 16K token budget with minimal accuracy loss. Ablation studies confirm the effectiveness of our exponential token budget strategy. Finally, our findings highlight that fine-tuning LRMs to perfectly align with the efficiency frontiers remains an open challenge.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.07104v2",
    "published_date": "2025-06-08 12:18:50 UTC",
    "updated_date": "2025-09-10 09:03:04 UTC"
  },
  {
    "arxiv_id": "2506.07099v1",
    "title": "Filling the Missings: Spatiotemporal Data Imputation by Conditional Diffusion",
    "authors": [
      "Wenying He",
      "Jieling Huang",
      "Junhua Gu",
      "Ji Zhang",
      "Yude Bai"
    ],
    "abstract": "Missing data in spatiotemporal systems presents a significant challenge for modern applications, ranging from environmental monitoring to urban traffic management. The integrity of spatiotemporal data often deteriorates due to hardware malfunctions and software failures in real-world deployments. Current approaches based on machine learning and deep learning struggle to model the intricate interdependencies between spatial and temporal dimensions effectively and, more importantly, suffer from cumulative errors during the data imputation process, which propagate and amplify through iterations. To address these limitations, we propose CoFILL, a novel Conditional Diffusion Model for spatiotemporal data imputation. CoFILL builds on the inherent advantages of diffusion models to generate high-quality imputations without relying on potentially error-prone prior estimates. It incorporates an innovative dual-stream architecture that processes temporal and frequency domain features in parallel. By fusing these complementary features, CoFILL captures both rapid fluctuations and underlying patterns in the data, which enables more robust imputation. The extensive experiments reveal that CoFILL's noise prediction network successfully transforms random noise into meaningful values that align with the true data distribution. The results also show that CoFILL outperforms state-of-the-art methods in imputation accuracy. The source code is publicly available at https://github.com/joyHJL/CoFILL.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "9 pages,3 figures",
    "pdf_url": "https://arxiv.org/pdf/2506.07099v1",
    "published_date": "2025-06-08 11:53:06 UTC",
    "updated_date": "2025-06-08 11:53:06 UTC"
  },
  {
    "arxiv_id": "2506.07092v1",
    "title": "Patient Similarity Computation for Clinical Decision Support: An Efficient Use of Data Transformation, Combining Static and Time Series Data",
    "authors": [
      "Joydeb Kumar Sana",
      "Mohammad M. Masud",
      "M Sohel Rahman",
      "M Saifur Rahman"
    ],
    "abstract": "Patient similarity computation (PSC) is a fundamental problem in healthcare informatics. The aim of the patient similarity computation is to measure the similarity among patients according to their historical clinical records, which helps to improve clinical decision support. This paper presents a novel distributed patient similarity computation (DPSC) technique based on data transformation (DT) methods, utilizing an effective combination of time series and static data. Time series data are sensor-collected patients' information, including metrics like heart rate, blood pressure, Oxygen saturation, respiration, etc. The static data are mainly patient background and demographic data, including age, weight, height, gender, etc. Static data has been used for clustering the patients. Before feeding the static data to the machine learning model adaptive Weight-of-Evidence (aWOE) and Z-score data transformation (DT) methods have been performed, which improve the prediction performances. In aWOE-based patient similarity models, sensitive patient information has been processed using aWOE which preserves the data privacy of the trained models. We used the Dynamic Time Warping (DTW) approach, which is robust and very popular, for time series similarity. However, DTW is not suitable for big data due to the significant computational run-time. To overcome this problem, distributed DTW computation is used in this study. For Coronary Artery Disease, our DT based approach boosts prediction performance by as much as 11.4%, 10.20%, and 12.6% in terms of AUC, accuracy, and F-measure, respectively. In the case of Congestive Heart Failure (CHF), our proposed method achieves performance enhancement up to 15.9%, 10.5%, and 21.9% for the same measures, respectively. The proposed method reduces the computation time by as high as 40%.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "This paper presents a novel distributed patient similarity computation (DPSC) technique based on data transformation (DT) methods, utilizing an effective combination of time series and static data",
    "pdf_url": "https://arxiv.org/pdf/2506.07092v1",
    "published_date": "2025-06-08 11:32:00 UTC",
    "updated_date": "2025-06-08 11:32:00 UTC"
  },
  {
    "arxiv_id": "2506.07079v1",
    "title": "On the Generalization of Data-Assisted Control in port-Hamiltonian Systems (DAC-pH)",
    "authors": [
      "Mostafa Eslami",
      "Maryam Babazadeh"
    ],
    "abstract": "This paper introduces a hypothetical hybrid control framework for port-Hamiltonian (p$\\mathcal{H}$) systems, employing a dynamic decomposition based on Data-Assisted Control (DAC). The system's evolution is split into two parts with fixed topology: Right-Hand Side (RHS)- an intrinsic Hamiltonian flow handling worst-case parametric uncertainties, and Left-Hand Side (LHS)- a dissipative/input flow addressing both structural and parametric uncertainties. A virtual port variable $Π$ serves as the interface between these two components. A nonlinear controller manages the intrinsic Hamiltonian flow, determining a desired port control value $Π_c$. Concurrently, Reinforcement Learning (RL) is applied to the dissipative/input flow to learn an agent for providing optimal policy in mapping $Π_c$ to the actual system input. This hybrid approach effectively manages RHS uncertainties while preserving the system's inherent structure. Key advantages include adjustable performance via LHS controller parameters, enhanced AI explainability and interpretability through the port variable $Π$, the ability to guarantee safety and state attainability with hard/soft constraints, reduced complexity in learning hypothesis classes compared to end-to-end solutions, and improved state/parameter estimation using LHS prior knowledge and system Hamiltonian to address partial observability. The paper details the p$\\mathcal{H}$ formulation, derives the decomposition, and presents the modular controller architecture. Beyond design, crucial aspects of stability and robustness analysis and synthesis are investigated, paving the way for deeper theoretical investigations. An application example, a pendulum with nonlinear dynamics, is simulated to demonstrate the approach's empirical and phenomenological benefits for future research.",
    "categories": [
      "eess.SY",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "eess.SY",
    "comment": "This paper presents an early investigation of Data-Assisted Control (DAC) with reinforcement learning, showcasing its potential through a simple example. Theoretical analysis is ongoing to establish formal support and guarantees for the proposed approach",
    "pdf_url": "https://arxiv.org/pdf/2506.07079v1",
    "published_date": "2025-06-08 10:44:01 UTC",
    "updated_date": "2025-06-08 10:44:01 UTC"
  },
  {
    "arxiv_id": "2506.17253v4",
    "title": "MS-DFTVNet:A Long-Term Time Series Prediction Method Based on Multi-Scale Deformable Convolution",
    "authors": [
      "Chenghan Li",
      "Mingchen Li",
      "Yipu Liao",
      "Ruisheng Diao"
    ],
    "abstract": "Research on long-term time series prediction has primarily relied on Transformer and MLP models, while the potential of convolutional networks in this domain remains underexplored. To address this, we propose a novel multi-scale time series reshape module that effectively captures cross-period patch interactions and variable dependencies. Building on this, we develop MS-DFTVNet, the multi-scale 3D deformable convolutional framework tailored for long-term forecasting. Moreover, to handle the inherently uneven distribution of temporal features, we introduce a context-aware dynamic deformable convolution mechanism, which further enhances the model's ability to capture complex temporal patterns. Extensive experiments demonstrate that MS-DFTVNet not only significantly outperforms strong baselines but also achieves an average improvement of about 7.5% across six public datasets, setting new state-of-the-art results.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.17253v4",
    "published_date": "2025-06-08 10:33:39 UTC",
    "updated_date": "2025-10-02 01:01:13 UTC"
  },
  {
    "arxiv_id": "2506.07077v1",
    "title": "Dual-Priv Pruning : Efficient Differential Private Fine-Tuning in Multimodal Large Language Models",
    "authors": [
      "Qianshan Wei",
      "Jiaqi Li",
      "Zihan You",
      "Yi Zhan",
      "Kecen Li",
      "Jialin Wu",
      "Xinfeng Li Hengjun Liu",
      "Yi Yu",
      "Bin Cao",
      "Yiwen Xu",
      "Yang Liu",
      "Guilin Qi"
    ],
    "abstract": "Differential Privacy (DP) is a widely adopted technique, valued for its effectiveness in protecting the privacy of task-specific datasets, making it a critical tool for large language models. However, its effectiveness in Multimodal Large Language Models (MLLMs) remains uncertain. Applying Differential Privacy (DP) inherently introduces substantial computation overhead, a concern particularly relevant for MLLMs which process extensive textual and visual data. Furthermore, a critical challenge of DP is that the injected noise, necessary for privacy, scales with parameter dimensionality, leading to pronounced model degradation; This trade-off between privacy and utility complicates the application of Differential Privacy (DP) to complex architectures like MLLMs. To address these, we propose Dual-Priv Pruning, a framework that employs two complementary pruning mechanisms for DP fine-tuning in MLLMs: (i) visual token pruning to reduce input dimensionality by removing redundant visual information, and (ii) gradient-update pruning during the DP optimization process. This second mechanism selectively prunes parameter updates based on the magnitude of noisy gradients, aiming to mitigate noise impact and improve utility. Experiments demonstrate that our approach achieves competitive results with minimal performance degradation. In terms of computational efficiency, our approach consistently utilizes less memory than standard DP-SGD. While requiring only 1.74% more memory than zeroth-order methods which suffer from severe performance issues on A100 GPUs, our method demonstrates leading memory efficiency on H20 GPUs. To the best of our knowledge, we are the first to explore DP fine-tuning in MLLMs. Our code is coming soon.",
    "categories": [
      "cs.CR",
      "cs.AI"
    ],
    "primary_category": "cs.CR",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.07077v1",
    "published_date": "2025-06-08 10:33:01 UTC",
    "updated_date": "2025-06-08 10:33:01 UTC"
  },
  {
    "arxiv_id": "2506.07075v1",
    "title": "Reasoning Paths as Signals: Augmenting Multi-hop Fact Verification through Structural Reasoning Progression",
    "authors": [
      "Liwen Zheng",
      "Chaozhuo Li",
      "Haoran Jia",
      "Xi Zhang"
    ],
    "abstract": "The growing complexity of factual claims in real-world scenarios presents significant challenges for automated fact verification systems, particularly in accurately aggregating and reasoning over multi-hop evidence. Existing approaches often rely on static or shallow models that fail to capture the evolving structure of reasoning paths, leading to fragmented retrieval and limited interpretability. To address these issues, we propose a Structural Reasoning framework for Multi-hop Fact Verification that explicitly models reasoning paths as structured graphs throughout both evidence retrieval and claim verification stages. Our method comprises two key modules: a structure-enhanced retrieval mechanism that constructs reasoning graphs to guide evidence collection, and a reasoning-path-guided verification module that incrementally builds subgraphs to represent evolving inference trajectories. We further incorporate a structure-aware reasoning mechanism that captures long-range dependencies across multi-hop evidence chains, enabling more precise verification. Extensive experiments on the FEVER and HoVer datasets demonstrate that our approach consistently outperforms strong baselines, highlighting the effectiveness of reasoning-path modeling in enhancing retrieval precision and verification accuracy.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.07075v1",
    "published_date": "2025-06-08 10:30:36 UTC",
    "updated_date": "2025-06-08 10:30:36 UTC"
  },
  {
    "arxiv_id": "2506.17252v3",
    "title": "Adaptive Batch-Wise Sample Scheduling for Direct Preference Optimization",
    "authors": [
      "Zixuan Huang",
      "Yikun Ban",
      "Lean Fu",
      "Xiaojie Li",
      "Zhongxiang Dai",
      "Jianxin Li",
      "Deqing Wang"
    ],
    "abstract": "Direct Preference Optimization (DPO) has emerged as an effective approach for aligning large language models (LLMs) with human preferences. However, its performance is highly dependent on the quality of the underlying human preference data. To address this bottleneck, prior work has explored various data selection strategies, but these methods often overlook the impact of the evolving states of the language model during the optimization process. In this paper, we introduce a novel problem: Sample Scheduling for DPO, which aims to dynamically and adaptively schedule training samples based on the model's evolving batch-wise states throughout preference optimization. To solve this problem, we propose SamS, an efficient and effective algorithm that adaptively selects samples in each training batch based on the LLM's learning feedback to maximize the potential generalization performance. Notably, without modifying the core DPO algorithm, simply integrating SamS significantly improves performance across tasks, with minimal additional computational overhead. This work points to a promising new direction for improving LLM alignment through batch-wise sample selection, with potential generalization to RLHF and broader supervised learning paradigms.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.17252v3",
    "published_date": "2025-06-08 10:26:09 UTC",
    "updated_date": "2025-10-02 06:00:42 UTC"
  },
  {
    "arxiv_id": "2506.07066v1",
    "title": "From Axioms to Algorithms: Mechanized Proofs of the vNM Utility Theorem",
    "authors": [
      "Li Jingyuan"
    ],
    "abstract": "This paper presents a comprehensive formalization of the von Neumann-Morgenstern (vNM) expected utility theorem using the Lean 4 interactive theorem prover. We implement the classical axioms of preference-completeness, transitivity, continuity, and independence-enabling machine-verified proofs of both the existence and uniqueness of utility representations. Our formalization captures the mathematical structure of preference relations over lotteries, verifying that preferences satisfying the vNM axioms can be represented by expected utility maximization.\n  Our contributions include a granular implementation of the independence axiom, formally verified proofs of fundamental claims about mixture lotteries, constructive demonstrations of utility existence, and computational experiments validating the results. We prove equivalence to classical presentations while offering greater precision at decision boundaries.\n  This formalization provides a rigorous foundation for applications in economic modeling, AI alignment, and management decision systems, bridging the gap between theoretical decision theory and computational implementation.",
    "categories": [
      "econ.TH",
      "cs.AI",
      "q-fin.CP"
    ],
    "primary_category": "econ.TH",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.07066v1",
    "published_date": "2025-06-08 10:09:54 UTC",
    "updated_date": "2025-06-08 10:09:54 UTC"
  },
  {
    "arxiv_id": "2506.17251v2",
    "title": "Training-free LLM Verification via Recycling Few-shot Examples",
    "authors": [
      "Dongseok Lee",
      "Jimyung Hong",
      "Dongyoung Kim",
      "Jaehyung Kim"
    ],
    "abstract": "Although LLMs have achieved remarkable performance, the inherent stochasticity of their reasoning process and varying conclusions present significant challenges. Majority voting or Best-of-N with external verification models has been explored to find the most promising solution among multiple LLM outputs. However, these approaches have certain limitations, such as limited applicability or the cost of an additional training step. To address this problem, we propose a novel and effective framework that Recycles Few-shot examples to verify LLM outputs (ReFeri). Our key idea is to additionally utilize the given few-shot examples to evaluate the candidate outputs of the target query, not only using them to generate outputs as the conventional few-shot prompting setup. Specifically, ReFeri evaluates the generated outputs by combining two different scores, designed motivated from Bayes' rule, and subsequently selects the candidate that is both confidently determined and contextually coherent through a few additional LLM inferences. Experiments with three different LLMs and across seven diverse tasks demonstrate that our framework significantly improves the accuracy of LLMs-achieving an average gain of 4.8%-through effective response selection, without additional training.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.17251v2",
    "published_date": "2025-06-08 10:02:07 UTC",
    "updated_date": "2025-10-01 04:58:58 UTC"
  },
  {
    "arxiv_id": "2506.07064v1",
    "title": "Com$^2$: A Causal-Guided Benchmark for Exploring Complex Commonsense Reasoning in Large Language Models",
    "authors": [
      "Kai Xiong",
      "Xiao Ding",
      "Yixin Cao",
      "Yuxiong Yan",
      "Li Du",
      "Yufei Zhang",
      "Jinglong Gao",
      "Jiaqian Liu",
      "Bing Qin",
      "Ting Liu"
    ],
    "abstract": "Large language models (LLMs) have mastered abundant simple and explicit commonsense knowledge through pre-training, enabling them to achieve human-like performance in simple commonsense reasoning. Nevertheless, LLMs struggle to reason with complex and implicit commonsense knowledge that is derived from simple ones (such as understanding the long-term effects of certain events), an aspect humans tend to focus on more. Existing works focus on complex tasks like math and code, while complex commonsense reasoning remains underexplored due to its uncertainty and lack of structure. To fill this gap and align with real-world concerns, we propose a benchmark Com$^2$ focusing on complex commonsense reasoning. We first incorporate causal event graphs to serve as structured complex commonsense. Then we adopt causal theory~(e.g., intervention) to modify the causal event graphs and obtain different scenarios that meet human concerns. Finally, an LLM is employed to synthesize examples with slow thinking, which is guided by the logical relationships in the modified causal graphs. Furthermore, we use detective stories to construct a more challenging subset. Experiments show that LLMs struggle in reasoning depth and breadth, while post-training and slow thinking can alleviate this. The code and data are available at https://github.com/Waste-Wood/Com2.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted by ACL 2025 Main Conference",
    "pdf_url": "https://arxiv.org/pdf/2506.07064v1",
    "published_date": "2025-06-08 09:53:08 UTC",
    "updated_date": "2025-06-08 09:53:08 UTC"
  },
  {
    "arxiv_id": "2506.07062v1",
    "title": "Prime the search: Using large language models for guiding geometric task and motion planning by warm-starting tree search",
    "authors": [
      "Dongryung Lee",
      "Sejune Joo",
      "Kimin Lee",
      "Beomjoon Kim"
    ],
    "abstract": "The problem of relocating a set of objects to designated areas amidst movable obstacles can be framed as a Geometric Task and Motion Planning (G-TAMP) problem, a subclass of task and motion planning (TAMP). Traditional approaches to G-TAMP have relied either on domain-independent heuristics or on learning from planning experience to guide the search, both of which typically demand significant computational resources or data. In contrast, humans often use common sense to intuitively decide which objects to manipulate in G-TAMP problems. Inspired by this, we propose leveraging Large Language Models (LLMs), which have common sense knowledge acquired from internet-scale data, to guide task planning in G-TAMP problems. To enable LLMs to perform geometric reasoning, we design a predicate-based prompt that encodes geometric information derived from a motion planning algorithm. We then query the LLM to generate a task plan, which is then used to search for a feasible set of continuous parameters. Since LLMs are prone to mistakes, instead of committing to LLM's outputs, we extend Monte Carlo Tree Search (MCTS) to a hybrid action space and use the LLM to guide the search. Unlike the previous approach that calls an LLM at every node and incurs high computational costs, we use it to warm-start the MCTS with the nodes explored in completing the LLM's task plan. On six different G-TAMP problems, we show our method outperforms previous LLM planners and pure search algorithms. Code can be found at: https://github.com/iMSquared/prime-the-search",
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "primary_category": "cs.RO",
    "comment": "The International Journal of Robotics Research (IJRR)",
    "pdf_url": "https://arxiv.org/pdf/2506.07062v1",
    "published_date": "2025-06-08 09:47:54 UTC",
    "updated_date": "2025-06-08 09:47:54 UTC"
  },
  {
    "arxiv_id": "2506.07060v1",
    "title": "Less is More: some Computational Principles based on Parcimony, and Limitations of Natural Intelligence",
    "authors": [
      "Laura Cohen",
      "Xavier Hinaut",
      "Lilyana Petrova",
      "Alexandre Pitti",
      "Syd Reynal",
      "Ichiro Tsuda"
    ],
    "abstract": "Natural intelligence (NI) consistently achieves more with less. Infants learn language, develop abstract concepts, and acquire sensorimotor skills from sparse data, all within tight neural and energy limits. In contrast, today's AI relies on virtually unlimited computational power, energy, and data to reach high performance. This paper argues that constraints in NI are paradoxically catalysts for efficiency, adaptability, and creativity. We first show how limited neural bandwidth promotes concise codes that still capture complex patterns. Spiking neurons, hierarchical structures, and symbolic-like representations emerge naturally from bandwidth constraints, enabling robust generalization. Next, we discuss chaotic itinerancy, illustrating how the brain transits among transient attractors to flexibly retrieve memories and manage uncertainty. We then highlight reservoir computing, where random projections facilitate rapid generalization from small datasets. Drawing on developmental perspectives, we emphasize how intrinsic motivation, along with responsive social environments, drives infant language learning and discovery of meaning. Such active, embodied processes are largely absent in current AI. Finally, we suggest that adopting 'less is more' principles -- energy constraints, parsimonious architectures, and real-world interaction -- can foster the emergence of more efficient, interpretable, and biologically grounded artificial systems.",
    "categories": [
      "q-bio.NC",
      "cs.AI"
    ],
    "primary_category": "q-bio.NC",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.07060v1",
    "published_date": "2025-06-08 09:42:29 UTC",
    "updated_date": "2025-06-08 09:42:29 UTC"
  },
  {
    "arxiv_id": "2506.07054v1",
    "title": "Policy Gradient with Tree Search: Avoiding Local Optimas through Lookahead",
    "authors": [
      "Uri Koren",
      "Navdeep Kumar",
      "Uri Gadot",
      "Giorgia Ramponi",
      "Kfir Yehuda Levy",
      "Shie Mannor"
    ],
    "abstract": "Classical policy gradient (PG) methods in reinforcement learning frequently converge to suboptimal local optima, a challenge exacerbated in large or complex environments. This work investigates Policy Gradient with Tree Search (PGTS), an approach that integrates an $m$-step lookahead mechanism to enhance policy optimization. We provide theoretical analysis demonstrating that increasing the tree search depth $m$-monotonically reduces the set of undesirable stationary points and, consequently, improves the worst-case performance of any resulting stationary policy. Critically, our analysis accommodates practical scenarios where policy updates are restricted to states visited by the current policy, rather than requiring updates across the entire state space. Empirical evaluations on diverse MDP structures, including Ladder, Tightrope, and Gridworld environments, illustrate PGTS's ability to exhibit \"farsightedness,\" navigate challenging reward landscapes, escape local traps where standard PG fails, and achieve superior solutions.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.07054v1",
    "published_date": "2025-06-08 09:28:11 UTC",
    "updated_date": "2025-06-08 09:28:11 UTC"
  },
  {
    "arxiv_id": "2506.17250v1",
    "title": "Towards Interpretable Adversarial Examples via Sparse Adversarial Attack",
    "authors": [
      "Fudong Lin",
      "Jiadong Lou",
      "Hao Wang",
      "Brian Jalaian",
      "Xu Yuan"
    ],
    "abstract": "Sparse attacks are to optimize the magnitude of adversarial perturbations for fooling deep neural networks (DNNs) involving only a few perturbed pixels (i.e., under the l0 constraint), suitable for interpreting the vulnerability of DNNs. However, existing solutions fail to yield interpretable adversarial examples due to their poor sparsity. Worse still, they often struggle with heavy computational overhead, poor transferability, and weak attack strength. In this paper, we aim to develop a sparse attack for understanding the vulnerability of CNNs by minimizing the magnitude of initial perturbations under the l0 constraint, to overcome the existing drawbacks while achieving a fast, transferable, and strong attack to DNNs. In particular, a novel and theoretical sound parameterization technique is introduced to approximate the NP-hard l0 optimization problem, making directly optimizing sparse perturbations computationally feasible. Besides, a novel loss function is designed to augment initial perturbations by maximizing the adversary property and minimizing the number of perturbed pixels simultaneously. Extensive experiments are conducted to demonstrate that our approach, with theoretical performance guarantees, outperforms state-of-the-art sparse attacks in terms of computational overhead, transferability, and attack strength, expecting to serve as a benchmark for evaluating the robustness of DNNs. In addition, theoretical and empirical results validate that our approach yields sparser adversarial examples, empowering us to discover two categories of noises, i.e., \"obscuring noise\" and \"leading noise\", which will help interpret how adversarial perturbation misleads the classifiers into incorrect predictions. Our code is available at https://github.com/fudong03/SparseAttack.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.17250v1",
    "published_date": "2025-06-08 09:13:30 UTC",
    "updated_date": "2025-06-08 09:13:30 UTC"
  },
  {
    "arxiv_id": "2506.07047v1",
    "title": "Mathesis: Towards Formal Theorem Proving from Natural Languages",
    "authors": [
      "Yu Xuejun",
      "Jianyuan Zhong",
      "Zijin Feng",
      "Pengyi Zhai",
      "Roozbeh Yousefzadeh",
      "Wei Chong Ng",
      "Haoxiong Liu",
      "Ziyi Shou",
      "Jing Xiong",
      "Yudong Zhou",
      "Claudia Beth Ong",
      "Austen Jeremy Sugiarto",
      "Yaoxi Zhang",
      "Wai Ming Tai",
      "Huan Cao",
      "Dongcai Lu",
      "Jiacheng Sun",
      "Qiang Xu",
      "Shen Xin",
      "Zhenguo Li"
    ],
    "abstract": "Recent advances in large language models show strong promise for formal reasoning. However, most LLM-based theorem provers have long been constrained by the need for expert-written formal statements as inputs, limiting their applicability to real-world problems expressed in natural language. We tackle this gap with Mathesis, the first end-to-end theorem proving pipeline processing informal problem statements. It contributes Mathesis-Autoformalizer, the first autoformalizer using reinforcement learning to enhance the formalization ability of natural language problems, aided by our novel LeanScorer framework for nuanced formalization quality assessment. It also proposes a Mathesis-Prover, which generates formal proofs from the formalized statements. To evaluate the real-world applicability of end-to-end formal theorem proving, we introduce Gaokao-Formal, a benchmark of 488 complex problems from China's national college entrance exam. Our approach is carefully designed, with a thorough study of each component. Experiments demonstrate Mathesis's effectiveness, with the autoformalizer outperforming the best baseline by 22% in pass-rate on Gaokao-Formal. The full system surpasses other model combinations, achieving 64% accuracy on MiniF2F with pass@32 and a state-of-the-art 18% on Gaokao-Formal.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.07047v1",
    "published_date": "2025-06-08 09:04:14 UTC",
    "updated_date": "2025-06-08 09:04:14 UTC"
  },
  {
    "arxiv_id": "2506.07045v1",
    "title": "Interpretable and Reliable Detection of AI-Generated Images via Grounded Reasoning in MLLMs",
    "authors": [
      "Yikun Ji",
      "Hong Yan",
      "Jun Lan",
      "Huijia Zhu",
      "Weiqiang Wang",
      "Qi Fan",
      "Liqing Zhang",
      "Jianfu Zhang"
    ],
    "abstract": "The rapid advancement of image generation technologies intensifies the demand for interpretable and robust detection methods. Although existing approaches often attain high accuracy, they typically operate as black boxes without providing human-understandable justifications. Multi-modal Large Language Models (MLLMs), while not originally intended for forgery detection, exhibit strong analytical and reasoning capabilities. When properly fine-tuned, they can effectively identify AI-generated images and offer meaningful explanations. However, existing MLLMs still struggle with hallucination and often fail to align their visual interpretations with actual image content and human reasoning. To bridge this gap, we construct a dataset of AI-generated images annotated with bounding boxes and descriptive captions that highlight synthesis artifacts, establishing a foundation for human-aligned visual-textual grounded reasoning. We then finetune MLLMs through a multi-stage optimization strategy that progressively balances the objectives of accurate detection, visual localization, and coherent textual explanation. The resulting model achieves superior performance in both detecting AI-generated images and localizing visual flaws, significantly outperforming baseline methods.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.07045v1",
    "published_date": "2025-06-08 08:47:44 UTC",
    "updated_date": "2025-06-08 08:47:44 UTC"
  },
  {
    "arxiv_id": "2506.07044v4",
    "title": "Lingshu: A Generalist Foundation Model for Unified Multimodal Medical Understanding and Reasoning",
    "authors": [
      "LASA Team",
      "Weiwen Xu",
      "Hou Pong Chan",
      "Long Li",
      "Mahani Aljunied",
      "Ruifeng Yuan",
      "Jianyu Wang",
      "Chenghao Xiao",
      "Guizhen Chen",
      "Chaoqun Liu",
      "Zhaodonghui Li",
      "Yu Sun",
      "Junao Shen",
      "Chaojun Wang",
      "Jie Tan",
      "Deli Zhao",
      "Tingyang Xu",
      "Hao Zhang",
      "Yu Rong"
    ],
    "abstract": "Multimodal Large Language Models (MLLMs) have demonstrated impressive capabilities in understanding common visual elements, largely due to their large-scale datasets and advanced training strategies. However, their effectiveness in medical applications remains limited due to the inherent discrepancies between data and tasks in medical scenarios and those in the general domain. Concretely, existing medical MLLMs face the following critical limitations: (1) limited coverage of medical knowledge beyond imaging, (2) heightened susceptibility to hallucinations due to suboptimal data curation processes, (3) lack of reasoning capabilities tailored for complex medical scenarios. To address these challenges, we first propose a comprehensive data curation procedure that (1) efficiently acquires rich medical knowledge data not only from medical imaging but also from extensive medical texts and general-domain data; and (2) synthesizes accurate medical captions, visual question answering (VQA), and reasoning samples. As a result, we build a multimodal dataset enriched with extensive medical knowledge. Building on the curated data, we introduce our medical-specialized MLLM: Lingshu. Lingshu undergoes multi-stage training to embed medical expertise and enhance its task-solving capabilities progressively. Besides, we preliminarily explore the potential of applying reinforcement learning with verifiable rewards paradigm to enhance Lingshu's medical reasoning ability. Additionally, we develop MedEvalKit, a unified evaluation framework that consolidates leading multimodal and textual medical benchmarks for standardized, fair, and efficient model assessment. We evaluate the performance of Lingshu on three fundamental medical tasks, multimodal QA, text-based QA, and medical report generation. The results show that Lingshu consistently outperforms the existing open-source multimodal models on most tasks ...",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.CL",
    "comment": "Technical Report, 53 pages, 25 tables, and 16 figures. Our webpage is https://alibaba-damo-academy.github.io/lingshu/",
    "pdf_url": "https://arxiv.org/pdf/2506.07044v4",
    "published_date": "2025-06-08 08:47:30 UTC",
    "updated_date": "2025-06-13 04:22:02 UTC"
  },
  {
    "arxiv_id": "2506.07040v3",
    "title": "Efficient $Q$-Learning and Actor-Critic Methods for Robust Average Reward Reinforcement Learning",
    "authors": [
      "Yang Xu",
      "Swetha Ganesh",
      "Vaneet Aggarwal"
    ],
    "abstract": "We present a non-asymptotic convergence analysis of $Q$-learning and actor-critic algorithms for robust average-reward Markov Decision Processes (MDPs) under contamination, total-variation (TV) distance, and Wasserstein uncertainty sets. A key ingredient of our analysis is showing that the optimal robust $Q$ operator is a strict contraction with respect to a carefully designed semi-norm (with constant functions quotiented out). This property enables a stochastic approximation update that learns the optimal robust $Q$-function using $\\tilde{\\mathcal{O}}(ε^{-2})$ samples. We also provide an efficient routine for robust $Q$-function estimation, which in turn facilitates robust critic estimation. Building on this, we introduce an actor-critic algorithm that learns an $ε$-optimal robust policy within $\\tilde{\\mathcal{O}}(ε^{-2})$ samples. We provide numerical simulations to evaluate the performance of our algorithms.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "Updated the main text of the draft",
    "pdf_url": "https://arxiv.org/pdf/2506.07040v3",
    "published_date": "2025-06-08 08:26:27 UTC",
    "updated_date": "2025-12-09 19:24:43 UTC"
  },
  {
    "arxiv_id": "2506.07035v1",
    "title": "AnnoDPO: Protein Functional Annotation Learning with Direct Preference Optimization",
    "authors": [
      "Zixuan Jiang",
      "Renjing Xu"
    ],
    "abstract": "Deciphering protein function remains a fundamental challenge in protein representation learning. The task presents significant difficulties for protein language models (PLMs) due to the sheer volume of functional annotation categories and the highly imbalanced distribution of annotated instances across biological ontologies. Inspired by the remarkable success of reinforcement learning from human feedback (RLHF) in large language model (LLM) alignment, we propose AnnoDPO, a novel multi-modal framework for protein function prediction that leverages Direct Preference Optimization (DPO) to enhance annotation learning. Our methodology addresses the dual challenges of annotation scarcity and category imbalance through preference-aligned training objectives, establishing a new paradigm for biological knowledge integration in protein representation learning.",
    "categories": [
      "q-bio.BM",
      "cs.AI"
    ],
    "primary_category": "q-bio.BM",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.07035v1",
    "published_date": "2025-06-08 07:59:09 UTC",
    "updated_date": "2025-06-08 07:59:09 UTC"
  },
  {
    "arxiv_id": "2506.07031v4",
    "title": "HauntAttack: When Attack Follows Reasoning as a Shadow",
    "authors": [
      "Jingyuan Ma",
      "Rui Li",
      "Zheng Li",
      "Junfeng Liu",
      "Heming Xia",
      "Lei Sha",
      "Zhifang Sui"
    ],
    "abstract": "Emerging Large Reasoning Models (LRMs) consistently excel in mathematical and reasoning tasks, showcasing remarkable capabilities. However, the enhancement of reasoning abilities and the exposure of internal reasoning processes introduce new safety vulnerabilities. A critical question arises: when reasoning becomes intertwined with harmfulness, will LRMs become more vulnerable to jailbreaks in reasoning mode? To investigate this, we introduce HauntAttack, a novel and general-purpose black-box adversarial attack framework that systematically embeds harmful instructions into reasoning questions. Specifically, we modify key reasoning conditions in existing questions with harmful instructions, thereby constructing a reasoning pathway that guides the model step by step toward unsafe outputs. We evaluate HauntAttack on 11 LRMs and observe an average attack success rate of 70\\%, achieving up to 12 percentage points of absolute improvement over the strongest prior baseline. Our further analysis reveals that even advanced safety-aligned models remain highly susceptible to reasoning-based attacks, offering insights into the urgent challenge of balancing reasoning capability and safety in future model development.",
    "categories": [
      "cs.CR",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.CR",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.07031v4",
    "published_date": "2025-06-08 07:45:48 UTC",
    "updated_date": "2025-10-23 12:59:35 UTC"
  },
  {
    "arxiv_id": "2506.07028v1",
    "title": "SiliCoN: Simultaneous Nuclei Segmentation and Color Normalization of Histological Images",
    "authors": [
      "Suman Mahapatra",
      "Pradipta Maji"
    ],
    "abstract": "Segmentation of nuclei regions from histological images is an important task for automated computer-aided analysis of histological images, particularly in the presence of impermissible color variation in the color appearance of stained tissue images. While color normalization enables better nuclei segmentation, accurate segmentation of nuclei structures makes color normalization rather trivial. In this respect, the paper proposes a novel deep generative model for simultaneously segmenting nuclei structures and normalizing color appearance of stained histological images.This model judiciously integrates the merits of truncated normal distribution and spatial attention. The model assumes that the latent color appearance information, corresponding to a particular histological image, is independent of respective nuclei segmentation map as well as embedding map information. The disentangled representation makes the model generalizable and adaptable as the modification or loss in color appearance information cannot be able to affect the nuclei segmentation map as well as embedding information. Also, for dealing with the stain overlap of associated histochemical reagents, the prior for latent color appearance code is assumed to be a mixture of truncated normal distributions. The proposed model incorporates the concept of spatial attention for segmentation of nuclei regions from histological images. The performance of the proposed approach, along with a comparative analysis with related state-of-the-art algorithms, has been demonstrated on publicly available standard histological image data sets.",
    "categories": [
      "eess.IV",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "eess.IV",
    "comment": "10 pages, 9 figures",
    "pdf_url": "https://arxiv.org/pdf/2506.07028v1",
    "published_date": "2025-06-08 07:31:42 UTC",
    "updated_date": "2025-06-08 07:31:42 UTC"
  },
  {
    "arxiv_id": "2507.02871v1",
    "title": "ZettaLith: An Architectural Exploration of Extreme-Scale AI Inference Acceleration",
    "authors": [
      "Kia Silverbrook"
    ],
    "abstract": "The high computational cost and power consumption of current and anticipated AI systems present a major challenge for widespread deployment and further scaling. Current hardware approaches face fundamental efficiency limits. This paper introduces ZettaLith, a scalable computing architecture designed to reduce the cost and power of AI inference by over 1,000x compared to current GPU-based systems. Based on architectural analysis and technology projections, a single ZettaLith rack could potentially achieve 1.507 zettaFLOPS in 2027 - representing a theoretical 1,047x improvement in inference performance, 1,490x better power efficiency, and could be 2,325x more cost-effective than current leading GPU racks for FP4 transformer inference. The ZettaLith architecture achieves these gains by abandoning general purpose GPU applications, and via the multiplicative effect of numerous co-designed architectural innovations using established digital electronic technologies, as detailed in this paper. ZettaLith's core architectural principles scale down efficiently to exaFLOPS desktop systems and petaFLOPS mobile chips, maintaining their roughly 1,000x advantage. ZettaLith presents a simpler system architecture compared to the complex hierarchy of current GPU clusters. ZettaLith is optimized exclusively for AI inference and is not applicable for AI training.",
    "categories": [
      "cs.DC",
      "cs.AI",
      "cs.AR",
      "cs.LG"
    ],
    "primary_category": "cs.DC",
    "comment": "53 pages, 15 figures, 23 tables",
    "pdf_url": "https://arxiv.org/pdf/2507.02871v1",
    "published_date": "2025-06-08 07:15:47 UTC",
    "updated_date": "2025-06-08 07:15:47 UTC"
  },
  {
    "arxiv_id": "2506.07023v1",
    "title": "Optimal Transport Driven Asymmetric Image-to-Image Translation for Nuclei Segmentation of Histological Images",
    "authors": [
      "Suman Mahapatra",
      "Pradipta Maji"
    ],
    "abstract": "Segmentation of nuclei regions from histological images enables morphometric analysis of nuclei structures, which in turn helps in the detection and diagnosis of diseases under consideration. To develop a nuclei segmentation algorithm, applicable to different types of target domain representations, image-to-image translation networks can be considered as they are invariant to target domain image representations. One of the important issues with image-to-image translation models is that they fail miserably when the information content between two image domains are asymmetric in nature. In this regard, the paper introduces a new deep generative model for segmenting nuclei structures from histological images. The proposed model considers an embedding space for handling information-disparity between information-rich histological image space and information-poor segmentation map domain. Integrating judiciously the concepts of optimal transport and measure theory, the model develops an invertible generator, which provides an efficient optimization framework with lower network complexity. The concept of invertible generator automatically eliminates the need of any explicit cycle-consistency loss. The proposed model also introduces a spatially-constrained squeeze operation within the framework of invertible generator to maintain spatial continuity within the image patches. The model provides a better trade-off between network complexity and model performance compared to other existing models having complex network architectures. The performance of the proposed deep generative model, along with a comparison with state-of-the-art nuclei segmentation methods, is demonstrated on publicly available histological image data sets.",
    "categories": [
      "eess.IV",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "eess.IV",
    "comment": "13 pages, 8 figures",
    "pdf_url": "https://arxiv.org/pdf/2506.07023v1",
    "published_date": "2025-06-08 07:05:33 UTC",
    "updated_date": "2025-06-08 07:05:33 UTC"
  },
  {
    "arxiv_id": "2506.07022v1",
    "title": "AlphaSteer: Learning Refusal Steering with Principled Null-Space Constraint",
    "authors": [
      "Leheng Sheng",
      "Changshuo Shen",
      "Weixiang Zhao",
      "Junfeng Fang",
      "Xiaohao Liu",
      "Zhenkai Liang",
      "Xiang Wang",
      "An Zhang",
      "Tat-Seng Chua"
    ],
    "abstract": "As LLMs are increasingly deployed in real-world applications, ensuring their ability to refuse malicious prompts, especially jailbreak attacks, is essential for safe and reliable use. Recently, activation steering has emerged as an effective approach for enhancing LLM safety by adding a refusal direction vector to internal activations of LLMs during inference, which will further induce the refusal behaviors of LLMs. However, indiscriminately applying activation steering fundamentally suffers from the trade-off between safety and utility, since the same steering vector can also lead to over-refusal and degraded performance on benign prompts. Although prior efforts, such as vector calibration and conditional steering, have attempted to mitigate this trade-off, their lack of theoretical grounding limits their robustness and effectiveness. To better address the trade-off between safety and utility, we present a theoretically grounded and empirically effective activation steering method called AlphaSteer. Specifically, it considers activation steering as a learnable process with two principled learning objectives: utility preservation and safety enhancement. For utility preservation, it learns to construct a nearly zero vector for steering benign data, with the null-space constraints. For safety enhancement, it learns to construct a refusal direction vector for steering malicious data, with the help of linear regression. Experiments across multiple jailbreak attacks and utility benchmarks demonstrate the effectiveness of AlphaSteer, which significantly improves the safety of LLMs without compromising general capabilities. Our codes are available at https://github.com/AlphaLab-USTC/AlphaSteer.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CR"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.07022v1",
    "published_date": "2025-06-08 07:03:28 UTC",
    "updated_date": "2025-06-08 07:03:28 UTC"
  },
  {
    "arxiv_id": "2506.07016v2",
    "title": "MAGNET: A Multi-agent Framework for Finding Audio-Visual Needles by Reasoning over Multi-Video Haystacks",
    "authors": [
      "Sanjoy Chowdhury",
      "Mohamed Elmoghany",
      "Yohan Abeysinghe",
      "Junjie Fei",
      "Sayan Nag",
      "Salman Khan",
      "Mohamed Elhoseiny",
      "Dinesh Manocha"
    ],
    "abstract": "Large multimodal models (LMMs) have shown remarkable progress in audio-visual understanding, yet they struggle with real-world scenarios that require complex reasoning across extensive video collections. Existing benchmarks for video question answering remain limited in scope, typically involving one clip per query, which falls short of representing the challenges of large-scale, audio-visual retrieval and reasoning encountered in practical applications. To bridge this gap, we introduce a novel task named AV-HaystacksQA, where the goal is to identify salient segments across different videos in response to a query and link them together to generate the most informative answer. To this end, we present AVHaystacks, an audio-visual benchmark comprising 3100 annotated QA pairs designed to assess the capabilities of LMMs in multi-video retrieval and temporal grounding task. Additionally, we propose a model-agnostic, multi-agent framework MAGNET to address this challenge, achieving up to 89% and 65% relative improvements over baseline methods on BLEU@4 and GPT evaluation scores in QA task on our proposed AVHaystacks. To enable robust evaluation of multi-video retrieval and temporal grounding for optimal response generation, we introduce two new metrics, STEM, which captures alignment errors between a ground truth and a predicted step sequence and MTGS, to facilitate balanced and interpretable evaluation of segment-level grounding performance. Project: https://schowdhury671.github.io/magnet_project/",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "Audio-visual learning, Audio-Visual RAG, Multi-Video Linkage",
    "pdf_url": "https://arxiv.org/pdf/2506.07016v2",
    "published_date": "2025-06-08 06:34:29 UTC",
    "updated_date": "2025-06-13 19:05:47 UTC"
  },
  {
    "arxiv_id": "2506.07008v2",
    "title": "Deep regularization networks for inverse problems with noisy operators",
    "authors": [
      "Fatemeh Pourahmadian",
      "Yang Xu"
    ],
    "abstract": "A supervised learning approach is proposed for regularization of large inverse problems where the main operator is built from noisy data. This is germane to superresolution imaging via the sampling indicators of the inverse scattering theory. We aim to accelerate the spatiotemporal regularization process for this class of inverse problems to enable real-time imaging. In this approach, a neural operator maps each pattern on the right-hand side of the scattering equation to its affiliated regularization parameter. The network is trained in two steps which entails: (1) training on low-resolution regularization maps furnished by the Morozov discrepancy principle with nonoptimal thresholds, and (2) optimizing network predictions through minimization of the Tikhonov loss function regulated by the validation loss. Step 2 allows for tailoring of the approximate maps of Step 1 toward construction of higher quality images. This approach enables direct learning from test data and dispenses with the need for a-priori knowledge of the optimal regularization maps. The network, trained on low-resolution data, quickly generates dense regularization maps for high-resolution imaging. We highlight the importance of the training loss function on the network's generalizability. In particular, we demonstrate that networks informed by the logic of discrepancy principle lead to images of higher contrast. In this case, the training process involves many-objective optimization. We propose a new method to adaptively select the appropriate loss weights during training without requiring an additional optimization process. The proposed approach is synthetically examined for imaging damage evolution in an elastic plate. The results indicate that the discrepancy-informed regularization networks not only accelerate the imaging process, but also remarkably enhance the image quality in complex environments.",
    "categories": [
      "math.NA",
      "cs.AI",
      "eess.SP"
    ],
    "primary_category": "math.NA",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.07008v2",
    "published_date": "2025-06-08 06:19:18 UTC",
    "updated_date": "2025-08-21 01:09:32 UTC"
  },
  {
    "arxiv_id": "2506.07006v1",
    "title": "CARoL: Context-aware Adaptation for Robot Learning",
    "authors": [
      "Zechen Hu",
      "Tong Xu",
      "Xuesu Xiao",
      "Xuan Wang"
    ],
    "abstract": "Using Reinforcement Learning (RL) to learn new robotic tasks from scratch is often inefficient. Leveraging prior knowledge has the potential to significantly enhance learning efficiency, which, however, raises two critical challenges: how to determine the relevancy of existing knowledge and how to adaptively integrate them into learning a new task. In this paper, we propose Context-aware Adaptation for Robot Learning (CARoL), a novel framework to efficiently learn a similar but distinct new task from prior knowledge. CARoL incorporates context awareness by analyzing state transitions in system dynamics to identify similarities between the new task and prior knowledge. It then utilizes these identified similarities to prioritize and adapt specific knowledge pieces for the new task. Additionally, CARoL has a broad applicability spanning policy-based, value-based, and actor-critic RL algorithms. We validate the efficiency and generalizability of CARoL on both simulated robotic platforms and physical ground vehicles. The simulations include CarRacing and LunarLander environments, where CARoL demonstrates faster convergence and higher rewards when learning policies for new tasks. In real-world experiments, we show that CARoL enables a ground vehicle to quickly and efficiently adapt policies learned in simulation to smoothly traverse real-world off-road terrain.",
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "primary_category": "cs.RO",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.07006v1",
    "published_date": "2025-06-08 06:05:32 UTC",
    "updated_date": "2025-06-08 06:05:32 UTC"
  },
  {
    "arxiv_id": "2506.07003v2",
    "title": "End-to-End Probabilistic Framework for Learning with Hard Constraints",
    "authors": [
      "Utkarsh Utkarsh",
      "Danielle C. Maddix",
      "Ruijun Ma",
      "Michael W. Mahoney",
      "Yuyang Wang"
    ],
    "abstract": "We present ProbHardE2E, a probabilistic forecasting framework that incorporates hard operational/physical constraints, and provides uncertainty quantification. Our methodology uses a novel differentiable probabilistic projection layer (DPPL) that can be combined with a wide range of neural network architectures. DPPL allows the model to learn the system in an end-to-end manner, compared to other approaches where constraints are satisfied either through a post-processing step or at inference. ProbHardE2E optimizes a strictly proper scoring rule, without making any distributional assumptions on the target, which enables it to obtain robust distributional estimates (in contrast to existing approaches that generally optimize likelihood-based objectives, which are heavily biased by their distributional assumptions and model choices); and it can incorporate a range of non-linear constraints (increasing the power of modeling and flexibility). We apply ProbHardE2E in learning partial differential equations with uncertainty estimates and to probabilistic time-series forecasting, showcasing it as a broadly applicable general framework that connects these seemingly disparate domains.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CE"
    ],
    "primary_category": "cs.LG",
    "comment": "45 pages, 5 figures, 10 tables",
    "pdf_url": "https://arxiv.org/pdf/2506.07003v2",
    "published_date": "2025-06-08 05:29:50 UTC",
    "updated_date": "2025-11-04 02:29:25 UTC"
  },
  {
    "arxiv_id": "2506.06999v2",
    "title": "Towards Physics-informed Diffusion for Anomaly Detection in Trajectories",
    "authors": [
      "Arun Sharma",
      "Mingzhou Yang",
      "Majid Farhadloo",
      "Subhankar Ghosh",
      "Bharat Jayaprakash",
      "Shashi Shekhar"
    ],
    "abstract": "Given trajectory data, a domain-specific study area, and a user-defined threshold, we aim to find anomalous trajectories indicative of possible GPS spoofing (e.g., fake trajectory). The problem is societally important to curb illegal activities in international waters, such as unauthorized fishing and illicit oil transfers. The problem is challenging due to advances in AI generated in deep fakes generation (e.g., additive noise, fake trajectories) and lack of adequate amount of labeled samples for ground-truth verification. Recent literature shows promising results for anomalous trajectory detection using generative models despite data sparsity. However, they do not consider fine-scale spatiotemporal dependencies and prior physical knowledge, resulting in higher false-positive rates. To address these limitations, we propose a physics-informed diffusion model that integrates kinematic constraints to identify trajectories that do not adhere to physical laws. Experimental results on real-world datasets in the maritime and urban domains show that the proposed framework results in higher prediction accuracy and lower estimation error rate for anomaly detection and trajectory generation methods, respectively. Our implementation is available at https://github.com/arunshar/Physics-Informed-Diffusion-Probabilistic-Model.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.06999v2",
    "published_date": "2025-06-08 05:10:20 UTC",
    "updated_date": "2025-06-14 21:09:16 UTC"
  },
  {
    "arxiv_id": "2506.17249v1",
    "title": "Improving Prediction Certainty Estimation for Reliable Early Exiting via Null Space Projection",
    "authors": [
      "Jianing He",
      "Qi Zhang",
      "Duoqian Miao",
      "Yi Kun",
      "Shufeng Hao",
      "Hongyun Zhang",
      "Zhihua Wei"
    ],
    "abstract": "Early exiting has demonstrated great potential in accelerating the inference of pre-trained language models (PLMs) by enabling easy samples to exit at shallow layers, eliminating the need for executing deeper layers. However, existing early exiting methods primarily rely on class-relevant logits to formulate their exiting signals for estimating prediction certainty, neglecting the detrimental influence of class-irrelevant information in the features on prediction certainty. This leads to an overestimation of prediction certainty, causing premature exiting of samples with incorrect early predictions. To remedy this, we define an NSP score to estimate prediction certainty by considering the proportion of class-irrelevant information in the features. On this basis, we propose a novel early exiting method based on the Certainty-Aware Probability (CAP) score, which integrates insights from both logits and the NSP score to enhance prediction certainty estimation, thus enabling more reliable exiting decisions. The experimental results on the GLUE benchmark show that our method can achieve an average speed-up ratio of 2.19x across all tasks with negligible performance degradation, surpassing the state-of-the-art (SOTA) ConsistentEE by 28%, yielding a better trade-off between task performance and inference efficiency. The code is available at https://github.com/He-Jianing/NSP.git.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "IJCAI 2025, 9 pages",
    "pdf_url": "https://arxiv.org/pdf/2506.17249v1",
    "published_date": "2025-06-08 05:08:34 UTC",
    "updated_date": "2025-06-08 05:08:34 UTC"
  },
  {
    "arxiv_id": "2506.06998v1",
    "title": "What makes Reasoning Models Different? Follow the Reasoning Leader for Efficient Decoding",
    "authors": [
      "Ming Li",
      "Zhengyuan Yang",
      "Xiyao Wang",
      "Dianqi Li",
      "Kevin Lin",
      "Tianyi Zhou",
      "Lijuan Wang"
    ],
    "abstract": "Large reasoning models (LRMs) achieve strong reasoning performance by emitting long chains of thought. Yet, these verbose traces slow down inference and often drift into unnecessary detail, known as the overthinking phenomenon. To better understand LRMs' behavior, we systematically analyze the token-level misalignment between reasoning and non-reasoning models. While it is expected that their primary difference lies in the stylistic \"thinking cues\", LRMs uniquely exhibit two pivotal, previously under-explored phenomena: a Global Misalignment Rebound, where their divergence from non-reasoning models persists or even grows as response length increases, and more critically, a Local Misalignment Diminish, where the misalignment concentrates at the \"thinking cues\" each sentence starts with but rapidly declines in the remaining of the sentence. Motivated by the Local Misalignment Diminish, we propose FoReaL-Decoding, a collaborative fast-slow thinking decoding method for cost-quality trade-off. In FoReaL-Decoding, a Leading model leads the first few tokens for each sentence, and then a weaker draft model completes the following tokens to the end of each sentence. FoReaL-Decoding adopts a stochastic gate to smoothly interpolate between the small and the large model. On four popular math-reasoning benchmarks (AIME24, GPQA-Diamond, MATH500, AMC23), FoReaL-Decoding reduces theoretical FLOPs by 30 to 50% and trims CoT length by up to 40%, while preserving 86 to 100% of model performance. These results establish FoReaL-Decoding as a simple, plug-and-play route to controllable cost-quality trade-offs in reasoning-centric tasks.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.06998v1",
    "published_date": "2025-06-08 05:08:32 UTC",
    "updated_date": "2025-06-08 05:08:32 UTC"
  },
  {
    "arxiv_id": "2507.13355v1",
    "title": "PGR-DRC: Pre-Global Routing DRC Violation Prediction Using Unsupervised Learning",
    "authors": [
      "Riadul Islam",
      "Dhandeep Challagundla"
    ],
    "abstract": "Leveraging artificial intelligence (AI)-driven electronic design and automation (EDA) tools, high-performance computing, and parallelized algorithms are essential for next-generation microprocessor innovation, ensuring continued progress in computing, AI, and semiconductor technology. Machine learning-based design rule checking (DRC) and lithography hotspot detection can improve first-pass silicon success. However, conventional ML and neural network (NN)-based models use supervised learning and require a large balanced dataset (in terms of positive and negative classes) and training time. This research addresses those key challenges by proposing the first-ever unsupervised DRC violation prediction methodology. The proposed model can be built using any unbalanced dataset using only one class and set a threshold for it, then fitting any new data querying if they are within the boundary of the model for classification. This research verified the proposed model by implementing different computational cores using CMOS 28 nm technology and Synopsys Design Compiler and IC Compiler II tools. Then, layouts were divided into virtual grids to collect about 60k data for analysis and verification. The proposed method has 99.95% prediction test accuracy, while the existing support vector machine (SVM) and neural network (NN) models have 85.44\\% and 98.74\\% accuracy, respectively. In addition, the proposed methodology has about 26.3x and up to 6003x lower training times compared to SVM and NN-models, respectively.",
    "categories": [
      "cs.AR",
      "cs.AI",
      "cs.LG",
      "eess.SY"
    ],
    "primary_category": "cs.AR",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2507.13355v1",
    "published_date": "2025-06-08 04:51:13 UTC",
    "updated_date": "2025-06-08 04:51:13 UTC"
  },
  {
    "arxiv_id": "2506.06991v2",
    "title": "Evaluating LLM-Contaminated Crowdsourcing Data Without Ground Truth",
    "authors": [
      "Yichi Zhang",
      "Jinlong Pang",
      "Zhaowei Zhu",
      "Yang Liu"
    ],
    "abstract": "The recent success of generative AI highlights the crucial role of high-quality human feedback in building trustworthy AI systems. However, the increasing use of large language models (LLMs) by crowdsourcing workers poses a significant challenge: datasets intended to reflect human input may be compromised by LLM-generated responses. Existing LLM detection approaches often rely on high-dimensional training data such as text, making them unsuitable for annotation tasks like multiple-choice labeling. In this work, we investigate the potential of peer prediction -- a mechanism that evaluates the information within workers' responses without using ground truth -- to mitigate LLM-assisted cheating in crowdsourcing with a focus on annotation tasks. Our approach quantifies the correlations between worker answers while conditioning on (a subset of) LLM-generated labels available to the requester. Building on prior research, we propose a training-free scoring mechanism with theoretical guarantees under a crowdsourcing model that accounts for LLM collusion. We establish conditions under which our method is effective and empirically demonstrate its robustness in detecting low-effort cheating on real-world crowdsourcing datasets.",
    "categories": [
      "cs.AI",
      "cs.GT",
      "cs.HC"
    ],
    "primary_category": "cs.AI",
    "comment": "32 pages, 7 figures",
    "pdf_url": "https://arxiv.org/pdf/2506.06991v2",
    "published_date": "2025-06-08 04:38:39 UTC",
    "updated_date": "2025-11-06 15:24:22 UTC"
  },
  {
    "arxiv_id": "2506.06981v2",
    "title": "Deep RL Needs Deep Behavior Analysis: Exploring Implicit Planning by Model-Free Agents in Open-Ended Environments",
    "authors": [
      "Riley Simmons-Edler",
      "Ryan P. Badman",
      "Felix Baastad Berg",
      "Raymond Chua",
      "John J. Vastola",
      "Joshua Lunger",
      "William Qian",
      "Kanaka Rajan"
    ],
    "abstract": "Understanding the behavior of deep reinforcement learning (DRL) agents -particularly as task and agent sophistication increase- requires more than simple comparison of reward curves, yet standard methods for behavioral analysis remain underdeveloped in DRL. We apply tools from neuroscience and ethology to study DRL agents in a novel, complex, partially observable environment, ForageWorld, designed to capture key aspects of real-world animal foraging- including sparse, depleting resource patches, predator threats, and spatially extended arenas. We use this environment as a platform for applying joint behavioral and neural analysis to agents, revealing detailed, quantitatively grounded insights into agent strategies, memory, and planning. Contrary to common assumptions, we find that model-free RNN-based DRL agents can exhibit structured, planning-like behavior purely through emergent dynamics- without requiring explicit memory modules or world models. Our results show that studying DRL agents like animals -analyzing them with neuroethology-inspired tools that reveal structure in both behavior and neural dynamics- uncovers rich structure in their learning dynamics that would otherwise remain invisible. We distill these tools into a general analysis framework linking core behavioral and representational features to diagnostic methods, which can be reused for a wide range of tasks and agents. As agents grow more complex and autonomous, bridging neuroscience, cognitive science, and AI will be essential- not just for understanding their behavior, but for ensuring safe alignment and maximizing desirable behaviors that are hard to measure via reward. We show how this can be done by drawing on lessons from how biological intelligence is studied.",
    "categories": [
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "Published at NeurIPS 2025",
    "pdf_url": "https://arxiv.org/pdf/2506.06981v2",
    "published_date": "2025-06-08 03:43:48 UTC",
    "updated_date": "2025-11-30 23:57:15 UTC"
  },
  {
    "arxiv_id": "2506.06980v1",
    "title": "MoXGATE: Modality-aware cross-attention for multi-omic gastrointestinal cancer sub-type classification",
    "authors": [
      "Sajib Acharjee Dip",
      "Uddip Acharjee Shuvo",
      "Dipanwita Mallick",
      "Abrar Rahman Abir",
      "Liqing Zhang"
    ],
    "abstract": "Cancer subtype classification is crucial for personalized treatment and prognostic assessment. However, effectively integrating multi-omic data remains challenging due to the heterogeneous nature of genomic, epigenomic, and transcriptomic features. In this work, we propose Modality-Aware Cross-Attention MoXGATE, a novel deep-learning framework that leverages cross-attention and learnable modality weights to enhance feature fusion across multiple omics sources. Our approach effectively captures inter-modality dependencies, ensuring robust and interpretable integration. Through experiments on Gastrointestinal Adenocarcinoma (GIAC) and Breast Cancer (BRCA) datasets from TCGA, we demonstrate that MoXGATE outperforms existing methods, achieving 95\\% classification accuracy. Ablation studies validate the effectiveness of cross-attention over simple concatenation and highlight the importance of different omics modalities. Moreover, our model generalizes well to unseen cancer types e.g., breast cancer, underscoring its adaptability. Key contributions include (1) a cross-attention-based multi-omic integration framework, (2) modality-weighted fusion for enhanced interpretability, (3) application of focal loss to mitigate data imbalance, and (4) validation across multiple cancer subtypes. Our results indicate that MoXGATE is a promising approach for multi-omic cancer subtype classification, offering improved performance and biological generalizability.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "9 pages, 1 figure, 6 tables",
    "pdf_url": "https://arxiv.org/pdf/2506.06980v1",
    "published_date": "2025-06-08 03:42:23 UTC",
    "updated_date": "2025-06-08 03:42:23 UTC"
  },
  {
    "arxiv_id": "2506.06977v2",
    "title": "UdonCare: Hierarchy Pruning for Unseen Domain Discovery in Predictive Healthcare",
    "authors": [
      "Pengfei Hu",
      "Xiaoxue Han",
      "Fei Wang",
      "Yue Ning"
    ],
    "abstract": "Healthcare providers often divide patient populations into cohorts based on shared clinical factors, such as medical history, to deliver personalized healthcare services. This idea has also been adopted in clinical prediction models, where it presents a vital challenge: capturing both global and cohort-specific patterns while enabling model generalization to unseen domains. Addressing this challenge falls under the scope of domain generalization (DG). However, conventional DG approaches often struggle in clinical settings due to the absence of explicit domain labels and the inherent gap in medical knowledge. To address this, we propose UdonCare, a hierarchy-guided method that iteratively divides patients into latent domains and decomposes domain-invariant (label) information from patient data. Our method identifies patient domains by pruning medical ontologies (e.g. ICD-9-CM hierarchy). On two public datasets, MIMIC-III and MIMIC-IV, UdonCare shows superiority over eight baselines across four clinical prediction tasks with substantial domain gaps, highlighting the untapped potential of medical knowledge in guiding clinical domain generalization problems.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.06977v2",
    "published_date": "2025-06-08 03:20:34 UTC",
    "updated_date": "2025-10-31 16:32:18 UTC"
  },
  {
    "arxiv_id": "2506.06975v3",
    "title": "Auditing Black-Box LLM APIs with a Rank-Based Uniformity Test",
    "authors": [
      "Xiaoyuan Zhu",
      "Yaowen Ye",
      "Tianyi Qiu",
      "Hanlin Zhu",
      "Sijun Tan",
      "Ajraf Mannan",
      "Jonathan Michala",
      "Raluca Ada Popa",
      "Willie Neiswanger"
    ],
    "abstract": "As API access becomes a primary interface to large language models (LLMs), users often interact with black-box systems that offer little transparency into the deployed model. To reduce costs or maliciously alter model behaviors, API providers may discreetly serve quantized or fine-tuned variants, which can degrade performance and compromise safety. Detecting such substitutions is difficult, as users lack access to model weights and, in most cases, even output logits. To tackle this problem, we propose a rank-based uniformity test that can verify the behavioral equality of a black-box LLM to a locally deployed authentic model. Our method is accurate, query-efficient, and avoids detectable query patterns, making it robust to adversarial providers that reroute or mix responses upon the detection of testing attempts. We evaluate the approach across diverse threat scenarios, including quantization, harmful fine-tuning, jailbreak prompts, and full model substitution, showing that it consistently achieves superior statistical power over prior methods under constrained query budgets.",
    "categories": [
      "cs.CR",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.CR",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.06975v3",
    "published_date": "2025-06-08 03:00:31 UTC",
    "updated_date": "2025-06-11 03:04:10 UTC"
  },
  {
    "arxiv_id": "2506.17248v1",
    "title": "Efficient Quantification of Multimodal Interaction at Sample Level",
    "authors": [
      "Zequn Yang",
      "Hongfa Wang",
      "Di Hu"
    ],
    "abstract": "Interactions between modalities -- redundancy, uniqueness, and synergy -- collectively determine the composition of multimodal information. Understanding these interactions is crucial for analyzing information dynamics in multimodal systems, yet their accurate sample-level quantification presents significant theoretical and computational challenges. To address this, we introduce the Lightweight Sample-wise Multimodal Interaction (LSMI) estimator, rigorously grounded in pointwise information theory. We first develop a redundancy estimation framework, employing an appropriate pointwise information measure to quantify this most decomposable and measurable interaction. Building upon this, we propose a general interaction estimation method that employs efficient entropy estimation, specifically tailored for sample-wise estimation in continuous distributions. Extensive experiments on synthetic and real-world datasets validate LSMI's precision and efficiency. Crucially, our sample-wise approach reveals fine-grained sample- and category-level dynamics within multimodal data, enabling practical applications such as redundancy-informed sample partitioning, targeted knowledge distillation, and interaction-aware model ensembling. The code is available at https://github.com/GeWu-Lab/LSMI_Estimator.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted to ICML 2025",
    "pdf_url": "https://arxiv.org/pdf/2506.17248v1",
    "published_date": "2025-06-08 02:39:25 UTC",
    "updated_date": "2025-06-08 02:39:25 UTC"
  },
  {
    "arxiv_id": "2506.11109v1",
    "title": "Enhancing Large Language Models for Mobility Analytics with Semantic Location Tokenization",
    "authors": [
      "Yile Chen",
      "Yicheng Tao",
      "Yue Jiang",
      "Shuai Liu",
      "Han Yu",
      "Gao Cong"
    ],
    "abstract": "The widespread adoption of location-based services has led to the generation of vast amounts of mobility data, providing significant opportunities to model user movement dynamics within urban environments. Recent advancements have focused on adapting Large Language Models (LLMs) for mobility analytics. However, existing methods face two primary limitations: inadequate semantic representation of locations (i.e., discrete IDs) and insufficient modeling of mobility signals within LLMs (i.e., single templated instruction fine-tuning). To address these issues, we propose QT-Mob, a novel framework that significantly enhances LLMs for mobility analytics. QT-Mob introduces a location tokenization module that learns compact, semantically rich tokens to represent locations, preserving contextual information while ensuring compatibility with LLMs. Furthermore, QT-Mob incorporates a series of complementary fine-tuning objectives that align the learned tokens with the internal representations in LLMs, improving the model's comprehension of sequential movement patterns and location semantics. The proposed QT-Mob framework not only enhances LLMs' ability to interpret mobility data but also provides a more generalizable approach for various mobility analytics tasks. Experiments on three real-world dataset demonstrate the superior performance in both next-location prediction and mobility recovery tasks, outperforming existing deep learning and LLM-based methods.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted by KDD'25",
    "pdf_url": "https://arxiv.org/pdf/2506.11109v1",
    "published_date": "2025-06-08 02:17:50 UTC",
    "updated_date": "2025-06-08 02:17:50 UTC"
  },
  {
    "arxiv_id": "2506.06965v1",
    "title": "Long-Tailed Learning for Generalized Category Discovery",
    "authors": [
      "Cuong Manh Hoang"
    ],
    "abstract": "Generalized Category Discovery (GCD) utilizes labeled samples of known classes to discover novel classes in unlabeled samples. Existing methods show effective performance on artificial datasets with balanced distributions. However, real-world datasets are always imbalanced, significantly affecting the effectiveness of these methods. To solve this problem, we propose a novel framework that performs generalized category discovery in long-tailed distributions. We first present a self-guided labeling technique that uses a learnable distribution to generate pseudo-labels, resulting in less biased classifiers. We then introduce a representation balancing process to derive discriminative representations. By mining sample neighborhoods, this process encourages the model to focus more on tail classes. We conduct experiments on public datasets to demonstrate the effectiveness of the proposed framework. The results show that our model exceeds previous state-of-the-art methods.",
    "categories": [
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.06965v1",
    "published_date": "2025-06-08 02:01:49 UTC",
    "updated_date": "2025-06-08 02:01:49 UTC"
  },
  {
    "arxiv_id": "2506.08045v1",
    "title": "UAVs Meet Agentic AI: A Multidomain Survey of Autonomous Aerial Intelligence and Agentic UAVs",
    "authors": [
      "Ranjan Sapkota",
      "Konstantinos I. Roumeliotis",
      "Manoj Karkee"
    ],
    "abstract": "Agentic UAVs represent a new frontier in autonomous aerial intelligence, integrating perception, decision-making, memory, and collaborative planning to operate adaptively in complex, real-world environments. Driven by recent advances in Agentic AI, these systems surpass traditional UAVs by exhibiting goal-driven behavior, contextual reasoning, and interactive autonomy. We provide a comprehensive foundation for understanding the architectural components and enabling technologies that distinguish Agentic UAVs from traditional autonomous UAVs. Furthermore, a detailed comparative analysis highlights advancements in autonomy with AI agents, learning, and mission flexibility. This study explores seven high-impact application domains precision agriculture, construction & mining, disaster response, environmental monitoring, infrastructure inspection, logistics, security, and wildlife conservation, illustrating the broad societal value of agentic aerial intelligence. Furthermore, we identify key challenges in technical constraints, regulatory limitations, and data-model reliability, and we present emerging solutions across hardware innovation, learning architectures, and human-AI interaction. Finally, a future roadmap is proposed, outlining pathways toward self-evolving aerial ecosystems, system-level collaboration, and sustainable, equitable deployments. This survey establishes a foundational framework for the future development, deployment, and governance of agentic aerial systems (Agentic UAVs) across diverse societal and industrial domains.",
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "primary_category": "cs.RO",
    "comment": "40 pages, 6 Figures",
    "pdf_url": "https://arxiv.org/pdf/2506.08045v1",
    "published_date": "2025-06-08 01:39:51 UTC",
    "updated_date": "2025-06-08 01:39:51 UTC"
  },
  {
    "arxiv_id": "2506.06959v1",
    "title": "Deontically Constrained Policy Improvement in Reinforcement Learning Agents",
    "authors": [
      "Alena Makarova",
      "Houssam Abbas"
    ],
    "abstract": "Markov Decision Processes (MDPs) are the most common model for decision making under uncertainty in the Machine Learning community. An MDP captures non-determinism, probabilistic uncertainty, and an explicit model of action. A Reinforcement Learning (RL) agent learns to act in an MDP by maximizing a utility function. This paper considers the problem of learning a decision policy that maximizes utility subject to satisfying a constraint expressed in deontic logic. In this setup, the utility captures the agent's mission - such as going quickly from A to B. The deontic formula represents (ethical, social, situational) constraints on how the agent might achieve its mission by prohibiting classes of behaviors. We use the logic of Expected Act Utilitarianism, a probabilistic stit logic that can be interpreted over controlled MDPs. We develop a variation on policy improvement, and show that it reaches a constrained local maximum of the mission utility. Given that in stit logic, an agent's duty is derived from value maximization, this can be seen as a way of acting to simultaneously maximize two value functions, one of which is implicit, in a bi-level structure. We illustrate these results with experiments on sample MDPs.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "20 pages, 11 figures, DEON2025 conference",
    "pdf_url": "https://arxiv.org/pdf/2506.06959v1",
    "published_date": "2025-06-08 01:01:06 UTC",
    "updated_date": "2025-06-08 01:01:06 UTC"
  },
  {
    "arxiv_id": "2506.06958v3",
    "title": "Simulating Society Requires Simulating Thought",
    "authors": [
      "Chance Jiajie Li",
      "Jiayi Wu",
      "Zhenze Mo",
      "Ao Qu",
      "Yuhan Tang",
      "Kaiya Ivy Zhao",
      "Yulu Gan",
      "Jie Fan",
      "Jiangbo Yu",
      "Jinhua Zhao",
      "Paul Liang",
      "Luis Alonso",
      "Kent Larson"
    ],
    "abstract": "Simulating society with large language models (LLMs), we argue, requires more than generating plausible behavior; it demands cognitively grounded reasoning that is structured, revisable, and traceable. LLM-based agents are increasingly used to emulate individual and group behavior, primarily through prompting and supervised fine-tuning. Yet current simulations remain grounded in a behaviorist \"demographics in, behavior out\" paradigm, focusing on surface-level plausibility. As a result, they often lack internal coherence, causal reasoning, and belief traceability, making them unreliable for modeling how people reason, deliberate, and respond to interventions.\n  To address this, we present a conceptual modeling paradigm, Generative Minds (GenMinds), which draws from cognitive science to support structured belief representations in generative agents. To evaluate such agents, we introduce the RECAP (REconstructing CAusal Paths) framework, a benchmark designed to assess reasoning fidelity via causal traceability, demographic grounding, and intervention consistency. These contributions advance a broader shift: from surface-level mimicry to generative agents that simulate thought, not just language, for social simulations.",
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.MA"
    ],
    "primary_category": "cs.CY",
    "comment": "NeurIPS 2025 (Position Paper Track)",
    "pdf_url": "https://arxiv.org/pdf/2506.06958v3",
    "published_date": "2025-06-08 00:59:02 UTC",
    "updated_date": "2025-10-24 12:32:06 UTC"
  },
  {
    "arxiv_id": "2506.06955v4",
    "title": "BIS Reasoning 1.0: The First Large-Scale Japanese Benchmark for Belief-Inconsistent Syllogistic Reasoning",
    "authors": [
      "Ha-Thanh Nguyen",
      "Chaoran Liu",
      "Qianying Liu",
      "Hideyuki Tachibana",
      "Su Myat Noe",
      "Yusuke Miyao",
      "Koichi Takeda",
      "Sadao Kurohashi"
    ],
    "abstract": "We present BIS Reasoning 1.0, the first large-scale Japanese dataset of syllogistic reasoning problems explicitly designed to evaluate belief-inconsistent reasoning in large language models (LLMs). Unlike prior datasets such as NeuBAROCO and JFLD, which focus on general or belief-aligned reasoning, BIS Reasoning 1.0 introduces logically valid yet belief-inconsistent syllogisms to uncover reasoning biases in LLMs trained on human-aligned corpora. We benchmark state-of-the-art models - including GPT models, Claude models, and leading Japanese LLMs - revealing significant variance in performance, with GPT-4o achieving 79.54% accuracy. Our analysis identifies critical weaknesses in current LLMs when handling logically valid but belief-conflicting inputs. These findings have important implications for deploying LLMs in high-stakes domains such as law, healthcare, and scientific literature, where truth must override intuitive belief to ensure integrity and safety.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "This version includes minor typo corrections in the example image",
    "pdf_url": "https://arxiv.org/pdf/2506.06955v4",
    "published_date": "2025-06-08 00:38:18 UTC",
    "updated_date": "2025-07-14 02:22:42 UTC"
  }
]
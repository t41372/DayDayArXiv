[
  {
    "arxiv_id": "2505.05470v1",
    "title": "Flow-GRPO: Training Flow Matching Models via Online RL",
    "authors": [
      "Jie Liu",
      "Gongye Liu",
      "Jiajun Liang",
      "Yangguang Li",
      "Jiaheng Liu",
      "Xintao Wang",
      "Pengfei Wan",
      "Di Zhang",
      "Wanli Ouyang"
    ],
    "abstract": "We propose Flow-GRPO, the first method integrating online reinforcement\nlearning (RL) into flow matching models. Our approach uses two key strategies:\n(1) an ODE-to-SDE conversion that transforms a deterministic Ordinary\nDifferential Equation (ODE) into an equivalent Stochastic Differential Equation\n(SDE) that matches the original model's marginal distribution at all timesteps,\nenabling statistical sampling for RL exploration; and (2) a Denoising Reduction\nstrategy that reduces training denoising steps while retaining the original\ninference timestep number, significantly improving sampling efficiency without\nperformance degradation. Empirically, Flow-GRPO is effective across multiple\ntext-to-image tasks. For complex compositions, RL-tuned SD3.5 generates nearly\nperfect object counts, spatial relations, and fine-grained attributes, boosting\nGenEval accuracy from $63\\%$ to $95\\%$. In visual text rendering, its accuracy\nimproves from $59\\%$ to $92\\%$, significantly enhancing text generation.\nFlow-GRPO also achieves substantial gains in human preference alignment.\nNotably, little to no reward hacking occurred, meaning rewards did not increase\nat the cost of image quality or diversity, and both remained stable in our\nexperiments.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "Code: https://github.com/yifan123/flow_grpo",
    "pdf_url": "http://arxiv.org/pdf/2505.05470v1",
    "published_date": "2025-05-08 17:58:45 UTC",
    "updated_date": "2025-05-08 17:58:45 UTC"
  },
  {
    "arxiv_id": "2505.05467v1",
    "title": "StreamBridge: Turning Your Offline Video Large Language Model into a Proactive Streaming Assistant",
    "authors": [
      "Haibo Wang",
      "Bo Feng",
      "Zhengfeng Lai",
      "Mingze Xu",
      "Shiyu Li",
      "Weifeng Ge",
      "Afshin Dehghan",
      "Meng Cao",
      "Ping Huang"
    ],
    "abstract": "We present StreamBridge, a simple yet effective framework that seamlessly\ntransforms offline Video-LLMs into streaming-capable models. It addresses two\nfundamental challenges in adapting existing models into online scenarios: (1)\nlimited capability for multi-turn real-time understanding, and (2) lack of\nproactive response mechanisms. Specifically, StreamBridge incorporates (1) a\nmemory buffer combined with a round-decayed compression strategy, supporting\nlong-context multi-turn interactions, and (2) a decoupled, lightweight\nactivation model that can be effortlessly integrated into existing Video-LLMs,\nenabling continuous proactive responses. To further support StreamBridge, we\nconstruct Stream-IT, a large-scale dataset tailored for streaming video\nunderstanding, featuring interleaved video-text sequences and diverse\ninstruction formats. Extensive experiments show that StreamBridge significantly\nimproves the streaming understanding capabilities of offline Video-LLMs across\nvarious tasks, outperforming even proprietary models such as GPT-4o and Gemini\n1.5 Pro. Simultaneously, it achieves competitive or superior performance on\nstandard video understanding benchmarks.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2505.05467v1",
    "published_date": "2025-05-08 17:57:40 UTC",
    "updated_date": "2025-05-08 17:57:40 UTC"
  },
  {
    "arxiv_id": "2505.05465v1",
    "title": "ComPO: Preference Alignment via Comparison Oracles",
    "authors": [
      "Peter Chen",
      "Xi Chen",
      "Wotao Yin",
      "Tianyi Lin"
    ],
    "abstract": "Direct alignment methods are increasingly used for aligning large language\nmodels (LLMs) with human preferences. However, these methods suffer from the\nissues of verbosity and likelihood displacement, which can be driven by the\nnoisy preference pairs that induce similar likelihood for preferred and\ndispreferred responses. The contributions of this paper are two-fold. First, we\npropose a new preference alignment method based on comparison oracles and\nprovide the convergence guarantee for its basic scheme. Second, we improve our\nmethod using some heuristics and conduct the experiments to demonstrate the\nflexibility and compatibility of practical scheme in improving the performance\nof LLMs using noisy preference pairs. Evaluations are conducted across multiple\nbase and instruction-tuned models (Mistral-7B, Llama-3-8B and Gemma-2-9B) with\nbenchmarks (AlpacaEval 2, MT-Bench and Arena-Hard). Experimental results show\nthe effectiveness of our method as an alternative to addressing the limitations\nof existing direct alignment methods. A highlight of our work is that we\nevidence the importance of designing specialized methods for preference pairs\nwith distinct likelihood margin, which complements the recent findings in\n\\citet{Razin-2025-Unintentional}.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "25 pages",
    "pdf_url": "http://arxiv.org/pdf/2505.05465v1",
    "published_date": "2025-05-08 17:56:57 UTC",
    "updated_date": "2025-05-08 17:56:57 UTC"
  },
  {
    "arxiv_id": "2505.05453v1",
    "title": "Conversational Process Model Redesign",
    "authors": [
      "Nataliia Klievtsova",
      "Timotheus Kampik",
      "Juergen Mangler",
      "Stefanie Rinderle-Ma"
    ],
    "abstract": "With the recent success of large language models (LLMs), the idea of\nAI-augmented Business Process Management systems is becoming more feasible. One\nof their essential characteristics is the ability to be conversationally\nactionable, allowing humans to interact with the LLM effectively to perform\ncrucial process life cycle tasks such as process model design and redesign.\nHowever, most current research focuses on single-prompt execution and\nevaluation of results, rather than on continuous interaction between the user\nand the LLM. In this work, we aim to explore the feasibility of using LLMs to\nempower domain experts in the creation and redesign of process models in an\niterative and effective way. The proposed conversational process model redesign\n(CPD) approach receives as input a process model and a redesign request by the\nuser in natural language. Instead of just letting the LLM make changes, the LLM\nis employed to (a) identify process change patterns from literature, (b)\nre-phrase the change request to be aligned with an expected wording for the\nidentified pattern (i.e., the meaning), and then to (c) apply the meaning of\nthe change to the process model. This multi-step approach allows for\nexplainable and reproducible changes. In order to ensure the feasibility of the\nCPD approach, and to find out how well the patterns from literature can be\nhandled by the LLM, we performed an extensive evaluation. The results show that\nsome patterns are hard to understand by LLMs and by users. Within the scope of\nthe study, we demonstrated that users need support to describe the changes\nclearly. Overall the evaluation shows that the LLMs can handle most changes\nwell according to a set of completeness and correctness criteria.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2505.05453v1",
    "published_date": "2025-05-08 17:44:45 UTC",
    "updated_date": "2025-05-08 17:44:45 UTC"
  },
  {
    "arxiv_id": "2505.05440v1",
    "title": "EcoAgent: An Efficient Edge-Cloud Collaborative Multi-Agent Framework for Mobile Automation",
    "authors": [
      "Biao Yi",
      "Xavier Hu",
      "Yurun Chen",
      "Shengyu Zhang",
      "Hongxia Yang",
      "Fan Wu",
      "Fei Wu"
    ],
    "abstract": "Cloud-based mobile agents powered by (multimodal) large language models\n((M)LLMs) offer strong reasoning abilities but suffer from high latency and\ncost. While fine-tuned (M)SLMs enable edge deployment, they often lose general\ncapabilities and struggle with complex tasks. To address this, we propose\nEcoAgent, an Edge-Cloud cOllaborative multi-agent framework for mobile\nautomation. EcoAgent features a closed-loop collaboration among a cloud-based\nPlanning Agent and two edge-based agents: the Execution Agent for action\nexecution and the Observation Agent for verifying outcomes. The Observation\nAgent uses a Pre-Understanding Module to compress screen images into concise\ntext, reducing token usage. In case of failure, the Planning Agent retrieves\nscreen history and replans via a Reflection Module. Experiments on AndroidWorld\nshow that EcoAgent maintains high task success rates while significantly\nreducing MLLM token consumption, enabling efficient and practical mobile\nautomation.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2505.05440v1",
    "published_date": "2025-05-08 17:31:20 UTC",
    "updated_date": "2025-05-08 17:31:20 UTC"
  },
  {
    "arxiv_id": "2505.05423v1",
    "title": "TransProQA: an LLM-based literary Translation evaluation metric with Professional Question Answering",
    "authors": [
      "Ran Zhang",
      "Wei Zhao",
      "Lieve Macken",
      "Steffen Eger"
    ],
    "abstract": "The impact of Large Language Models (LLMs) has extended into literary\ndomains. However, existing evaluation metrics prioritize mechanical accuracy\nover artistic expression and tend to overrate machine translation (MT) as being\nsuperior to experienced professional human translation. In the long run, this\nbias could result in a permanent decline in translation quality and cultural\nauthenticity. In response to the urgent need for a specialized literary\nevaluation metric, we introduce TransProQA, a novel, reference-free, LLM-based\nquestion-answering (QA) framework designed specifically for literary\ntranslation evaluation. TransProQA uniquely integrates insights from\nprofessional literary translators and researchers, focusing on critical\nelements in literary quality assessment such as literary devices, cultural\nunderstanding, and authorial voice. Our extensive evaluation shows that while\nliterary-finetuned XCOMET-XL yields marginal gains, TransProQA substantially\noutperforms current metrics, achieving up to 0.07 gain in correlation (ACC-EQ\nand Kendall's tau) and surpassing the best state-of-the-art (SOTA) metrics by\nover 15 points in adequacy assessments. Incorporating professional translator\ninsights as weights further improves performance, highlighting the value of\ntranslator inputs. Notably, TransProQA approaches human-level evaluation\nperformance comparable to trained linguistic annotators. It demonstrates broad\napplicability to open-source models such as LLaMA3.3-70b and Qwen2.5-32b,\nindicating its potential as an accessible and training-free literary evaluation\nmetric and a valuable tool for evaluating texts that require local processing\ndue to copyright or ethical considerations.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "WIP",
    "pdf_url": "http://arxiv.org/pdf/2505.05423v1",
    "published_date": "2025-05-08 17:12:56 UTC",
    "updated_date": "2025-05-08 17:12:56 UTC"
  },
  {
    "arxiv_id": "2505.05422v1",
    "title": "TokLIP: Marry Visual Tokens to CLIP for Multimodal Comprehension and Generation",
    "authors": [
      "Haokun Lin",
      "Teng Wang",
      "Yixiao Ge",
      "Yuying Ge",
      "Zhichao Lu",
      "Ying Wei",
      "Qingfu Zhang",
      "Zhenan Sun",
      "Ying Shan"
    ],
    "abstract": "Pioneering token-based works such as Chameleon and Emu3 have established a\nfoundation for multimodal unification but face challenges of high training\ncomputational overhead and limited comprehension performance due to a lack of\nhigh-level semantics. In this paper, we introduce TokLIP, a visual tokenizer\nthat enhances comprehension by semanticizing vector-quantized (VQ) tokens and\nincorporating CLIP-level semantics while enabling end-to-end multimodal\nautoregressive training with standard VQ tokens. TokLIP integrates a low-level\ndiscrete VQ tokenizer with a ViT-based token encoder to capture high-level\ncontinuous semantics. Unlike previous approaches (e.g., VILA-U) that discretize\nhigh-level features, TokLIP disentangles training objectives for comprehension\nand generation, allowing the direct application of advanced VQ tokenizers\nwithout the need for tailored quantization operations. Our empirical results\ndemonstrate that TokLIP achieves exceptional data efficiency, empowering visual\ntokens with high-level semantic understanding while enhancing low-level\ngenerative capacity, making it well-suited for autoregressive Transformers in\nboth comprehension and generation tasks. The code and models are available at\nhttps://github.com/TencentARC/TokLIP.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.CV",
    "comment": "Technical Report",
    "pdf_url": "http://arxiv.org/pdf/2505.05422v1",
    "published_date": "2025-05-08 17:12:19 UTC",
    "updated_date": "2025-05-08 17:12:19 UTC"
  },
  {
    "arxiv_id": "2505.05410v1",
    "title": "Reasoning Models Don't Always Say What They Think",
    "authors": [
      "Yanda Chen",
      "Joe Benton",
      "Ansh Radhakrishnan",
      "Jonathan Uesato",
      "Carson Denison",
      "John Schulman",
      "Arushi Somani",
      "Peter Hase",
      "Misha Wagner",
      "Fabien Roger",
      "Vlad Mikulik",
      "Samuel R. Bowman",
      "Jan Leike",
      "Jared Kaplan",
      "Ethan Perez"
    ],
    "abstract": "Chain-of-thought (CoT) offers a potential boon for AI safety as it allows\nmonitoring a model's CoT to try to understand its intentions and reasoning\nprocesses. However, the effectiveness of such monitoring hinges on CoTs\nfaithfully representing models' actual reasoning processes. We evaluate CoT\nfaithfulness of state-of-the-art reasoning models across 6 reasoning hints\npresented in the prompts and find: (1) for most settings and models tested,\nCoTs reveal their usage of hints in at least 1% of examples where they use the\nhint, but the reveal rate is often below 20%, (2) outcome-based reinforcement\nlearning initially improves faithfulness but plateaus without saturating, and\n(3) when reinforcement learning increases how frequently hints are used (reward\nhacking), the propensity to verbalize them does not increase, even without\ntraining against a CoT monitor. These results suggest that CoT monitoring is a\npromising way of noticing undesired behaviors during training and evaluations,\nbut that it is not sufficient to rule them out. They also suggest that in\nsettings like ours where CoT reasoning is not necessary, test-time monitoring\nof CoTs is unlikely to reliably catch rare and catastrophic unexpected\nbehaviors.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2505.05410v1",
    "published_date": "2025-05-08 16:51:43 UTC",
    "updated_date": "2025-05-08 16:51:43 UTC"
  },
  {
    "arxiv_id": "2505.05408v1",
    "title": "Crosslingual Reasoning through Test-Time Scaling",
    "authors": [
      "Zheng-Xin Yong",
      "M. Farid Adilazuarda",
      "Jonibek Mansurov",
      "Ruochen Zhang",
      "Niklas Muennighoff",
      "Carsten Eickhoff",
      "Genta Indra Winata",
      "Julia Kreutzer",
      "Stephen H. Bach",
      "Alham Fikri Aji"
    ],
    "abstract": "Reasoning capabilities of large language models are primarily studied for\nEnglish, even when pretrained models are multilingual. In this work, we\ninvestigate to what extent English reasoning finetuning with long\nchain-of-thoughts (CoTs) can generalize across languages. First, we find that\nscaling up inference compute for English-centric reasoning language models\n(RLMs) improves multilingual mathematical reasoning across many languages\nincluding low-resource languages, to an extent where they outperform models\ntwice their size. Second, we reveal that while English-centric RLM's CoTs are\nnaturally predominantly English, they consistently follow a quote-and-think\npattern to reason about quoted non-English inputs. Third, we discover an\neffective strategy to control the language of long CoT reasoning, and we\nobserve that models reason better and more efficiently in high-resource\nlanguages. Finally, we observe poor out-of-domain reasoning generalization, in\nparticular from STEM to cultural commonsense knowledge, even for English.\nOverall, we demonstrate the potentials, study the mechanisms and outline the\nlimitations of crosslingual generalization of English reasoning test-time\nscaling. We conclude that practitioners should let English-centric RLMs reason\nin high-resource languages, while further work is needed to improve reasoning\nin low-resource languages and out-of-domain contexts.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2505.05408v1",
    "published_date": "2025-05-08 16:50:06 UTC",
    "updated_date": "2025-05-08 16:50:06 UTC"
  },
  {
    "arxiv_id": "2505.05402v1",
    "title": "CART-ELC: Oblique Decision Tree Induction via Exhaustive Search",
    "authors": [
      "Andrew D. Laack"
    ],
    "abstract": "Oblique decision trees have attracted attention due to their potential for\nimproved classification performance over traditional axis-aligned decision\ntrees. However, methods that rely on exhaustive search to find oblique splits\nface computational challenges. As a result, they have not been widely explored.\nWe introduce a novel algorithm, Classification and Regression Tree - Exhaustive\nLinear Combinations (CART-ELC), for inducing oblique decision trees that\nperforms an exhaustive search on a restricted set of hyperplanes. We then\ninvestigate the algorithm's computational complexity and its predictive\ncapabilities. Our results demonstrate that CART-ELC consistently achieves\ncompetitive performance on small datasets, often yielding statistically\nsignificant improvements in classification accuracy relative to existing\ndecision tree induction algorithms, while frequently producing shallower,\nsimpler, and thus more interpretable trees.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.DS",
      "I.2.6; I.5.2; F.2.2; G.3; G.2.1"
    ],
    "primary_category": "cs.LG",
    "comment": "16 pages, 4 figures",
    "pdf_url": "http://arxiv.org/pdf/2505.05402v1",
    "published_date": "2025-05-08 16:42:13 UTC",
    "updated_date": "2025-05-08 16:42:13 UTC"
  },
  {
    "arxiv_id": "2505.05396v1",
    "title": "A Pain Assessment Framework based on multimodal data and Deep Machine Learning methods",
    "authors": [
      "Stefanos Gkikas"
    ],
    "abstract": "From the original abstract:\n  This thesis initially aims to study the pain assessment process from a\nclinical-theoretical perspective while exploring and examining existing\nautomatic approaches. Building on this foundation, the primary objective of\nthis Ph.D. project is to develop innovative computational methods for automatic\npain assessment that achieve high performance and are applicable in real\nclinical settings. A primary goal is to thoroughly investigate and assess\nsignificant factors, including demographic elements that impact pain\nperception, as recognized in pain research, through a computational standpoint.\nWithin the limits of the available data in this research area, our goal was to\ndesign, develop, propose, and offer automatic pain assessment pipelines for\nunimodal and multimodal configurations that are applicable to the specific\nrequirements of different scenarios. The studies published in this Ph.D. thesis\nshowcased the effectiveness of the proposed methods, achieving state-of-the-art\nresults. Additionally, they paved the way for exploring new approaches in\nartificial intelligence, foundation models, and generative artificial\nintelligence.",
    "categories": [
      "cs.AI",
      "cs.HC",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2505.05396v1",
    "published_date": "2025-05-08 16:32:55 UTC",
    "updated_date": "2025-05-08 16:32:55 UTC"
  },
  {
    "arxiv_id": "2505.05375v1",
    "title": "Threshold Modulation for Online Test-Time Adaptation of Spiking Neural Networks",
    "authors": [
      "Kejie Zhao",
      "Wenjia Hua",
      "Aiersi Tuerhong",
      "Luziwei Leng",
      "Yuxin Ma",
      "Qinghua Guo"
    ],
    "abstract": "Recently, spiking neural networks (SNNs), deployed on neuromorphic chips,\nprovide highly efficient solutions on edge devices in different scenarios.\nHowever, their ability to adapt to distribution shifts after deployment has\nbecome a crucial challenge. Online test-time adaptation (OTTA) offers a\npromising solution by enabling models to dynamically adjust to new data\ndistributions without requiring source data or labeled target samples.\nNevertheless, existing OTTA methods are largely designed for traditional\nartificial neural networks and are not well-suited for SNNs. To address this\ngap, we propose a low-power, neuromorphic chip-friendly online test-time\nadaptation framework, aiming to enhance model generalization under distribution\nshifts. The proposed approach is called Threshold Modulation (TM), which\ndynamically adjusts the firing threshold through neuronal dynamics-inspired\nnormalization, being more compatible with neuromorphic hardware. Experimental\nresults on benchmark datasets demonstrate the effectiveness of this method in\nimproving the robustness of SNNs against distribution shifts while maintaining\nlow computational cost. The proposed method offers a practical solution for\nonline test-time adaptation of SNNs, providing inspiration for the design of\nfuture neuromorphic chips. The demo code is available at\ngithub.com/NneurotransmitterR/TM-OTTA-SNN.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG",
      "cs.NE"
    ],
    "primary_category": "cs.CV",
    "comment": "Accepted by IJCNN 2025. \\c{opyright} 2025 IEEE. Personal use of this\n  material is permitted. Permission from IEEE must be obtained for all other\n  uses, including reprinting/republishing this material for advertising or\n  promotional purposes, collecting new collected works for resale or\n  redistribution to servers or lists, or reuse of any copyrighted component of\n  this work in other works",
    "pdf_url": "http://arxiv.org/pdf/2505.05375v1",
    "published_date": "2025-05-08 16:09:40 UTC",
    "updated_date": "2025-05-08 16:09:40 UTC"
  },
  {
    "arxiv_id": "2505.05356v1",
    "title": "Time of the Flight of the Gaussians: Optimizing Depth Indirectly in Dynamic Radiance Fields",
    "authors": [
      "Runfeng Li",
      "Mikhail Okunev",
      "Zixuan Guo",
      "Anh Ha Duong",
      "Christian Richardt",
      "Matthew O'Toole",
      "James Tompkin"
    ],
    "abstract": "We present a method to reconstruct dynamic scenes from monocular\ncontinuous-wave time-of-flight (C-ToF) cameras using raw sensor samples that\nachieves similar or better accuracy than neural volumetric approaches and is\n100x faster. Quickly achieving high-fidelity dynamic 3D reconstruction from a\nsingle viewpoint is a significant challenge in computer vision. In C-ToF\nradiance field reconstruction, the property of interest-depth-is not directly\nmeasured, causing an additional challenge. This problem has a large and\nunderappreciated impact upon the optimization when using a fast primitive-based\nscene representation like 3D Gaussian splatting, which is commonly used with\nmulti-view data to produce satisfactory results and is brittle in its\noptimization otherwise. We incorporate two heuristics into the optimization to\nimprove the accuracy of scene geometry represented by Gaussians. Experimental\nresults show that our approach produces accurate reconstructions under\nconstrained C-ToF sensing conditions, including for fast motions like swinging\nbaseball bats. https://visual.cs.brown.edu/gftorf",
    "categories": [
      "cs.GR",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.GR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2505.05356v1",
    "published_date": "2025-05-08 15:45:53 UTC",
    "updated_date": "2025-05-08 15:45:53 UTC"
  },
  {
    "arxiv_id": "2505.05354v1",
    "title": "High-fidelity Grain Growth Modeling: Leveraging Deep Learning for Fast Computations",
    "authors": [
      "Pungponhavoan Tep",
      "Marc Bernacki"
    ],
    "abstract": "Grain growth simulation is crucial for predicting metallic material\nmicrostructure evolution during annealing and resulting final mechanical\nproperties, but traditional partial differential equation-based methods are\ncomputationally expensive, creating bottlenecks in materials design and\nmanufacturing. In this work, we introduce a machine learning framework that\ncombines a Convolutional Long Short-Term Memory networks with an Autoencoder to\nefficiently predict grain growth evolution. Our approach captures both spatial\nand temporal aspects of grain evolution while encoding high-dimensional grain\nstructure data into a compact latent space for pattern learning, enhanced by a\nnovel composite loss function combining Mean Squared Error, Structural\nSimilarity Index Measurement, and Boundary Preservation to maintain structural\nintegrity of grain boundary topology of the prediction. Results demonstrated\nthat our machine learning approach accelerates grain growth prediction by up to\n\\SI{89}{\\times} faster, reducing computation time from \\SI{10}{\\minute} to\napproximately \\SI{10}{\\second} while maintaining high-fidelity predictions. The\nbest model (S-30-30) achieving a structural similarity score of\n\\SI{86.71}{\\percent} and mean grain size error of just \\SI{0.07}{\\percent}. All\nmodels accurately captured grain boundary topology, morphology, and size\ndistributions. This approach enables rapid microstructural prediction for\napplications where conventional simulations are prohibitively time-consuming,\npotentially accelerating innovation in materials science and manufacturing.",
    "categories": [
      "cond-mat.mtrl-sci",
      "cs.AI"
    ],
    "primary_category": "cond-mat.mtrl-sci",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2505.05354v1",
    "published_date": "2025-05-08 15:43:40 UTC",
    "updated_date": "2025-05-08 15:43:40 UTC"
  },
  {
    "arxiv_id": "2505.05321v1",
    "title": "Feature-Augmented Deep Networks for Multiscale Building Segmentation in High-Resolution UAV and Satellite Imagery",
    "authors": [
      "Chintan B. Maniyar",
      "Minakshi Kumar",
      "Gengchen Mai"
    ],
    "abstract": "Accurate building segmentation from high-resolution RGB imagery remains\nchallenging due to spectral similarity with non-building features, shadows, and\nirregular building geometries. In this study, we present a comprehensive deep\nlearning framework for multiscale building segmentation using RGB aerial and\nsatellite imagery with spatial resolutions ranging from 0.4m to 2.7m. We curate\na diverse, multi-sensor dataset and introduce feature-augmented inputs by\nderiving secondary representations including Principal Component Analysis\n(PCA), Visible Difference Vegetation Index (VDVI), Morphological Building Index\n(MBI), and Sobel edge filters from RGB channels. These features guide a\nRes-U-Net architecture in learning complex spatial patterns more effectively.\nWe also propose training policies incorporating layer freezing, cyclical\nlearning rates, and SuperConvergence to reduce training time and resource\nusage. Evaluated on a held-out WorldView-3 image, our model achieves an overall\naccuracy of 96.5%, an F1-score of 0.86, and an Intersection over Union (IoU) of\n0.80, outperforming existing RGB-based benchmarks. This study demonstrates the\neffectiveness of combining multi-resolution imagery, feature augmentation, and\noptimized training strategies for robust building segmentation in remote\nsensing applications.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "I.4.6; I.4.10; I.5.1; I.2.10"
    ],
    "primary_category": "cs.CV",
    "comment": "in preparation for journal submission, 25 pages, 11 figures",
    "pdf_url": "http://arxiv.org/pdf/2505.05321v1",
    "published_date": "2025-05-08 15:08:36 UTC",
    "updated_date": "2025-05-08 15:08:36 UTC"
  },
  {
    "arxiv_id": "2505.05318v1",
    "title": "Mapping User Trust in Vision Language Models: Research Landscape, Challenges, and Prospects",
    "authors": [
      "Agnese Chiatti",
      "Sara Bernardini",
      "Lara Shibelski Godoy Piccolo",
      "Viola Schiaffonati",
      "Matteo Matteucci"
    ],
    "abstract": "The rapid adoption of Vision Language Models (VLMs), pre-trained on large\nimage-text and video-text datasets, calls for protecting and informing users\nabout when to trust these systems. This survey reviews studies on trust\ndynamics in user-VLM interactions, through a multi-disciplinary taxonomy\nencompassing different cognitive science capabilities, collaboration modes, and\nagent behaviours. Literature insights and findings from a workshop with\nprospective VLM users inform preliminary requirements for future VLM trust\nstudies.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CY",
      "cs.HC",
      "cs.RO"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2505.05318v1",
    "published_date": "2025-05-08 15:02:49 UTC",
    "updated_date": "2025-05-08 15:02:49 UTC"
  },
  {
    "arxiv_id": "2505.05315v1",
    "title": "Scalable Chain of Thoughts via Elastic Reasoning",
    "authors": [
      "Yuhui Xu",
      "Hanze Dong",
      "Lei Wang",
      "Doyen Sahoo",
      "Junnan Li",
      "Caiming Xiong"
    ],
    "abstract": "Large reasoning models (LRMs) have achieved remarkable progress on complex\ntasks by generating extended chains of thought (CoT). However, their\nuncontrolled output lengths pose significant challenges for real-world\ndeployment, where inference-time budgets on tokens, latency, or compute are\nstrictly constrained. We propose Elastic Reasoning, a novel framework for\nscalable chain of thoughts that explicitly separates reasoning into two\nphases--thinking and solution--with independently allocated budgets. At test\ntime, Elastic Reasoning prioritize that completeness of solution segments,\nsignificantly improving reliability under tight resource constraints. To train\nmodels that are robust to truncated thinking, we introduce a lightweight\nbudget-constrained rollout strategy, integrated into GRPO, which teaches the\nmodel to reason adaptively when the thinking process is cut short and\ngeneralizes effectively to unseen budget constraints without additional\ntraining. Empirical results on mathematical (AIME, MATH500) and programming\n(LiveCodeBench, Codeforces) benchmarks demonstrate that Elastic Reasoning\nperforms robustly under strict budget constraints, while incurring\nsignificantly lower training cost than baseline methods. Remarkably, our\napproach also produces more concise and efficient reasoning even in\nunconstrained settings. Elastic Reasoning offers a principled and practical\nsolution to the pressing challenge of controllable reasoning at scale.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2505.05315v1",
    "published_date": "2025-05-08 15:01:06 UTC",
    "updated_date": "2025-05-08 15:01:06 UTC"
  },
  {
    "arxiv_id": "2505.05291v1",
    "title": "Benchmarking Ophthalmology Foundation Models for Clinically Significant Age Macular Degeneration Detection",
    "authors": [
      "Benjamin A. Cohen",
      "Jonathan Fhima",
      "Meishar Meisel",
      "Baskin Meital",
      "Luis Filipe Nakayama",
      "Eran Berkowitz",
      "Joachim A. Behar"
    ],
    "abstract": "Self-supervised learning (SSL) has enabled Vision Transformers (ViTs) to\nlearn robust representations from large-scale natural image datasets, enhancing\ntheir generalization across domains. In retinal imaging, foundation models\npretrained on either natural or ophthalmic data have shown promise, but the\nbenefits of in-domain pretraining remain uncertain. To investigate this, we\nbenchmark six SSL-pretrained ViTs on seven digital fundus image (DFI) datasets\ntotaling 70,000 expert-annotated images for the task of moderate-to-late\nage-related macular degeneration (AMD) identification. Our results show that\niBOT pretrained on natural images achieves the highest out-of-distribution\ngeneralization, with AUROCs of 0.80-0.97, outperforming domain-specific models,\nwhich achieved AUROCs of 0.78-0.96 and a baseline ViT-L with no pretraining,\nwhich achieved AUROCs of 0.68-0.91. These findings highlight the value of\nfoundation models in improving AMD identification and challenge the assumption\nthat in-domain pretraining is necessary. Furthermore, we release BRAMD, an\nopen-access dataset (n=587) of DFIs with AMD labels from Brazil.",
    "categories": [
      "eess.IV",
      "cs.AI",
      "cs.CV",
      "q-bio.TO"
    ],
    "primary_category": "eess.IV",
    "comment": "10 pages, 3 figures",
    "pdf_url": "http://arxiv.org/pdf/2505.05291v1",
    "published_date": "2025-05-08 14:31:02 UTC",
    "updated_date": "2025-05-08 14:31:02 UTC"
  },
  {
    "arxiv_id": "2505.05288v1",
    "title": "PlaceIt3D: Language-Guided Object Placement in Real 3D Scenes",
    "authors": [
      "Ahmed Abdelreheem",
      "Filippo Aleotti",
      "Jamie Watson",
      "Zawar Qureshi",
      "Abdelrahman Eldesokey",
      "Peter Wonka",
      "Gabriel Brostow",
      "Sara Vicente",
      "Guillermo Garcia-Hernando"
    ],
    "abstract": "We introduce the novel task of Language-Guided Object Placement in Real 3D\nScenes. Our model is given a 3D scene's point cloud, a 3D asset, and a textual\nprompt broadly describing where the 3D asset should be placed. The task here is\nto find a valid placement for the 3D asset that respects the prompt. Compared\nwith other language-guided localization tasks in 3D scenes such as grounding,\nthis task has specific challenges: it is ambiguous because it has multiple\nvalid solutions, and it requires reasoning about 3D geometric relationships and\nfree space. We inaugurate this task by proposing a new benchmark and evaluation\nprotocol. We also introduce a new dataset for training 3D LLMs on this task, as\nwell as the first method to serve as a non-trivial baseline. We believe that\nthis challenging task and our new benchmark could become part of the suite of\nbenchmarks used to evaluate and compare generalist 3D LLM models.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.RO"
    ],
    "primary_category": "cs.CV",
    "comment": "Tech report. Project page: https://nianticlabs.github.io/placeit3d/",
    "pdf_url": "http://arxiv.org/pdf/2505.05288v1",
    "published_date": "2025-05-08 14:29:11 UTC",
    "updated_date": "2025-05-08 14:29:11 UTC"
  },
  {
    "arxiv_id": "2505.05283v1",
    "title": "Software Development Life Cycle Perspective: A Survey of Benchmarks for CodeLLMs and Agents",
    "authors": [
      "Kaixin Wang",
      "Tianlin Li",
      "Xiaoyu Zhang",
      "Chong Wang",
      "Weisong Sun",
      "Yang Liu",
      "Bin Shi"
    ],
    "abstract": "Code large language models (CodeLLMs) and agents have shown great promise in\ntackling complex software engineering tasks.Compared to traditional software\nengineering methods, CodeLLMs and agents offer stronger abilities, and can\nflexibly process inputs and outputs in both natural and code. Benchmarking\nplays a crucial role in evaluating the capabilities of CodeLLMs and agents,\nguiding their development and deployment. However, despite their growing\nsignificance, there remains a lack of comprehensive reviews of benchmarks for\nCodeLLMs and agents. To bridge this gap, this paper provides a comprehensive\nreview of existing benchmarks for CodeLLMs and agents, studying and analyzing\n181 benchmarks from 461 relevant papers, covering the different phases of the\nsoftware development life cycle (SDLC). Our findings reveal a notable imbalance\nin the coverage of current benchmarks, with approximately 60% focused on the\nsoftware development phase in SDLC, while requirements engineering and software\ndesign phases receive minimal attention at only 5% and 3%, respectively.\nAdditionally, Python emerges as the dominant programming language across the\nreviewed benchmarks. Finally, this paper highlights the challenges of current\nresearch and proposes future directions, aiming to narrow the gap between the\ntheoretical capabilities of CodeLLMs and agents and their application in\nreal-world scenarios.",
    "categories": [
      "cs.SE",
      "cs.AI"
    ],
    "primary_category": "cs.SE",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2505.05283v1",
    "published_date": "2025-05-08 14:27:45 UTC",
    "updated_date": "2025-05-08 14:27:45 UTC"
  },
  {
    "arxiv_id": "2505.05271v1",
    "title": "T-T: Table Transformer for Tagging-based Aspect Sentiment Triplet Extraction",
    "authors": [
      "Kun Peng",
      "Chaodong Tong",
      "Cong Cao",
      "Hao Peng",
      "Qian Li",
      "Guanlin Wu",
      "Lei Jiang",
      "Yanbing Liu",
      "Philip S. Yu"
    ],
    "abstract": "Aspect sentiment triplet extraction (ASTE) aims to extract triplets composed\nof aspect terms, opinion terms, and sentiment polarities from given sentences.\nThe table tagging method is a popular approach to addressing this task, which\nencodes a sentence into a 2-dimensional table, allowing for the tagging of\nrelations between any two words. Previous efforts have focused on designing\nvarious downstream relation learning modules to better capture interactions\nbetween tokens in the table, revealing that a stronger capability to capture\nrelations can lead to greater improvements in the model. Motivated by this, we\nattempt to directly utilize transformer layers as downstream relation learning\nmodules. Due to the powerful semantic modeling capability of transformers, it\nis foreseeable that this will lead to excellent improvement. However, owing to\nthe quadratic relation between the length of the table and the length of the\ninput sentence sequence, using transformers directly faces two challenges:\noverly long table sequences and unfair local attention interaction. To address\nthese challenges, we propose a novel Table-Transformer (T-T) for the\ntagging-based ASTE method. Specifically, we introduce a stripe attention\nmechanism with a loop-shift strategy to tackle these challenges. The former\nmodifies the global attention mechanism to only attend to a 2-dimensional local\nattention window, while the latter facilitates interaction between different\nattention windows. Extensive and comprehensive experiments demonstrate that the\nT-T, as a downstream relation learning module, achieves state-of-the-art\nperformance with lower computational costs.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted by IJCAI2025",
    "pdf_url": "http://arxiv.org/pdf/2505.05271v1",
    "published_date": "2025-05-08 14:17:27 UTC",
    "updated_date": "2025-05-08 14:17:27 UTC"
  },
  {
    "arxiv_id": "2505.05262v1",
    "title": "Enhancing Cooperative Multi-Agent Reinforcement Learning with State Modelling and Adversarial Exploration",
    "authors": [
      "Andreas Kontogiannis",
      "Konstantinos Papathanasiou",
      "Yi Shen",
      "Giorgos Stamou",
      "Michael M. Zavlanos",
      "George Vouros"
    ],
    "abstract": "Learning to cooperate in distributed partially observable environments with\nno communication abilities poses significant challenges for multi-agent deep\nreinforcement learning (MARL). This paper addresses key concerns in this\ndomain, focusing on inferring state representations from individual agent\nobservations and leveraging these representations to enhance agents'\nexploration and collaborative task execution policies. To this end, we propose\na novel state modelling framework for cooperative MARL, where agents infer\nmeaningful belief representations of the non-observable state, with respect to\noptimizing their own policies, while filtering redundant and less informative\njoint state information. Building upon this framework, we propose the MARL SMPE\nalgorithm. In SMPE, agents enhance their own policy's discriminative abilities\nunder partial observability, explicitly by incorporating their beliefs into the\npolicy network, and implicitly by adopting an adversarial type of exploration\npolicies which encourages agents to discover novel, high-value states while\nimproving the discriminative abilities of others. Experimentally, we show that\nSMPE outperforms state-of-the-art MARL algorithms in complex fully cooperative\ntasks from the MPE, LBF, and RWARE benchmarks.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.MA"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted (Poster) at ICML 2025",
    "pdf_url": "http://arxiv.org/pdf/2505.05262v1",
    "published_date": "2025-05-08 14:07:20 UTC",
    "updated_date": "2025-05-08 14:07:20 UTC"
  },
  {
    "arxiv_id": "2505.05235v1",
    "title": "Advancing Neural Network Verification through Hierarchical Safety Abstract Interpretation",
    "authors": [
      "Luca Marzari",
      "Isabella Mastroeni",
      "Alessandro Farinelli"
    ],
    "abstract": "Traditional methods for formal verification (FV) of deep neural networks\n(DNNs) are constrained by a binary encoding of safety properties, where a model\nis classified as either safe or unsafe (robust or not robust). This binary\nencoding fails to capture the nuanced safety levels within a model, often\nresulting in either overly restrictive or too permissive requirements. In this\npaper, we introduce a novel problem formulation called Abstract\nDNN-Verification, which verifies a hierarchical structure of unsafe outputs,\nproviding a more granular analysis of the safety aspect for a given DNN.\nCrucially, by leveraging abstract interpretation and reasoning about output\nreachable sets, our approach enables assessing multiple safety levels during\nthe FV process, requiring the same (in the worst case) or even potentially less\ncomputational effort than the traditional binary verification approach.\nSpecifically, we demonstrate how this formulation allows rank adversarial\ninputs according to their abstract safety level violation, offering a more\ndetailed evaluation of the model's safety and robustness. Our contributions\ninclude a theoretical exploration of the relationship between our novel\nabstract safety formulation and existing approaches that employ abstract\ninterpretation for robustness verification, complexity analysis of the novel\nproblem introduced, and an empirical evaluation considering both a complex deep\nreinforcement learning task (based on Habitat 3.0) and standard\nDNN-Verification benchmarks.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2505.05235v1",
    "published_date": "2025-05-08 13:29:46 UTC",
    "updated_date": "2025-05-08 13:29:46 UTC"
  },
  {
    "arxiv_id": "2505.05232v1",
    "title": "ChemRxivQuest: A Curated Chemistry Question-Answer Database Extracted from ChemRxiv Preprints",
    "authors": [
      "Mahmoud Amiri",
      "Thomas Bocklitz"
    ],
    "abstract": "The rapid expansion of chemistry literature poses significant challenges for\nresearchers seeking to efficiently access domain-specific knowledge. To support\nadvancements in chemistry-focused natural language processing (NLP), we present\nChemRxivQuest, a curated dataset of 970 high-quality question-answer (QA) pairs\nderived from 155 ChemRxiv preprints across 17 subfields of chemistry. Each QA\npair is explicitly linked to its source text segment to ensure traceability and\ncontextual accuracy. ChemRxivQuest was constructed using an automated pipeline\nthat combines optical character recognition (OCR), GPT-4o-based QA generation,\nand a fuzzy matching technique for answer verification. The dataset emphasizes\nconceptual, mechanistic, applied, and experimental questions, enabling\napplications in retrieval-based QA systems, search engine development, and\nfine-tuning of domain-adapted large language models. We analyze the dataset's\nstructure, coverage, and limitations, and outline future directions for\nexpansion and expert validation. ChemRxivQuest provides a foundational resource\nfor chemistry NLP research, education, and tool development.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2505.05232v1",
    "published_date": "2025-05-08 13:26:33 UTC",
    "updated_date": "2025-05-08 13:26:33 UTC"
  },
  {
    "arxiv_id": "2505.05226v1",
    "title": "Put CASH on Bandits: A Max K-Armed Problem for Automated Machine Learning",
    "authors": [
      "Amir Rezaei Balef",
      "Claire Vernade",
      "Katharina Eggensperger"
    ],
    "abstract": "The Combined Algorithm Selection and Hyperparameter optimization (CASH) is a\nchallenging resource allocation problem in the field of AutoML. We propose\nMaxUCB, a max $k$-armed bandit method to trade off exploring different model\nclasses and conducting hyperparameter optimization. MaxUCB is specifically\ndesigned for the light-tailed and bounded reward distributions arising in this\nsetting and, thus, provides an efficient alternative compared to classic max\n$k$-armed bandit methods assuming heavy-tailed reward distributions. We\ntheoretically and empirically evaluate our method on four standard AutoML\nbenchmarks, demonstrating superior performance over prior approaches.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2505.05226v1",
    "published_date": "2025-05-08 13:18:05 UTC",
    "updated_date": "2025-05-08 13:18:05 UTC"
  },
  {
    "arxiv_id": "2505.05211v1",
    "title": "Incentive-Aware Machine Learning; Robustness, Fairness, Improvement & Causality",
    "authors": [
      "Chara Podimata"
    ],
    "abstract": "The article explores the emerging domain of incentive-aware machine learning\n(ML), which focuses on algorithmic decision-making in contexts where\nindividuals can strategically modify their inputs to influence outcomes. It\ncategorizes the research into three perspectives: robustness, aiming to design\nmodels resilient to \"gaming\"; fairness, analyzing the societal impacts of such\nsystems; and improvement/causality, recognizing situations where strategic\nactions lead to genuine personal or societal improvement. The paper introduces\na unified framework encapsulating models for these perspectives, including\noffline, online, and causal settings, and highlights key challenges such as\ndifferentiating between gaming and improvement and addressing heterogeneity\namong agents. By synthesizing findings from diverse works, we outline\ntheoretical advancements and practical solutions for robust, fair, and\ncausally-informed incentive-aware ML systems.",
    "categories": [
      "cs.GT",
      "cs.AI"
    ],
    "primary_category": "cs.GT",
    "comment": "This literature review was published in SIGEcom Exchanges in 2025",
    "pdf_url": "http://arxiv.org/pdf/2505.05211v1",
    "published_date": "2025-05-08 13:04:32 UTC",
    "updated_date": "2025-05-08 13:04:32 UTC"
  },
  {
    "arxiv_id": "2505.05203v1",
    "title": "LAPSO: A Unified Optimization View for Learning-Augmented Power System Operations",
    "authors": [
      "Wangkun Xu",
      "Zhongda Chu",
      "Fei Teng"
    ],
    "abstract": "With the high penetration of renewables, traditional model-based power system\noperation is challenged to deliver economic, stable, and robust decisions.\nMachine learning has emerged as a powerful modeling tool for capturing complex\ndynamics to address these challenges. However, its separate design often lacks\nsystematic integration with existing methods. To fill the gap, this paper\nproposes a holistic framework of Learning-Augmented Power System Operations\n(LAPSO, pronounced as Lap-So). Adopting a native optimization perspective,\nLAPSO is centered on the operation stage and aims to break the boundary between\ntemporally siloed power system tasks, such as forecast, operation and control,\nwhile unifying the objectives of machine learning and model-based optimizations\nat both training and inference stages. Systematic analysis and simulations\ndemonstrate the effectiveness of applying LAPSO in designing new integrated\nalgorithms, such as stability-constrained optimization (SCO) and\nobjective-based forecasting (OBF), while enabling end-to-end tracing of\ndifferent sources of uncertainties. In addition, a dedicated Python\npackage-lapso is introduced to automatically augment existing power system\noptimization models with learnable components. All code and data are available\nat https://github.com/xuwkk/lapso_exp.",
    "categories": [
      "eess.SY",
      "cs.AI",
      "cs.SY"
    ],
    "primary_category": "eess.SY",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2505.05203v1",
    "published_date": "2025-05-08 13:00:24 UTC",
    "updated_date": "2025-05-08 13:00:24 UTC"
  },
  {
    "arxiv_id": "2505.05197v1",
    "title": "Societal and technological progress as sewing an ever-growing, ever-changing, patchy, and polychrome quilt",
    "authors": [
      "Joel Z. Leibo",
      "Alexander Sasha Vezhnevets",
      "William A. Cunningham",
      "Sébastien Krier",
      "Manfred Diaz",
      "Simon Osindero"
    ],
    "abstract": "Artificial Intelligence (AI) systems are increasingly placed in positions\nwhere their decisions have real consequences, e.g., moderating online spaces,\nconducting research, and advising on policy. Ensuring they operate in a safe\nand ethically acceptable fashion is thus critical. However, most solutions have\nbeen a form of one-size-fits-all \"alignment\". We are worried that such systems,\nwhich overlook enduring moral diversity, will spark resistance, erode trust,\nand destabilize our institutions. This paper traces the underlying problem to\nan often-unstated Axiom of Rational Convergence: the idea that under ideal\nconditions, rational agents will converge in the limit of conversation on a\nsingle ethics. Treating that premise as both optional and doubtful, we propose\nwhat we call the appropriateness framework: an alternative approach grounded in\nconflict theory, cultural evolution, multi-agent systems, and institutional\neconomics. The appropriateness framework treats persistent disagreement as the\nnormal case and designs for it by applying four principles: (1) contextual\ngrounding, (2) community customization, (3) continual adaptation, and (4)\npolycentric governance. We argue here that adopting these design principles is\na good way to shift the main alignment metaphor from moral unification to a\nmore productive metaphor of conflict management, and that taking this step is\nboth desirable and urgent.",
    "categories": [
      "cs.AI",
      "cs.CY"
    ],
    "primary_category": "cs.AI",
    "comment": "16 pages",
    "pdf_url": "http://arxiv.org/pdf/2505.05197v1",
    "published_date": "2025-05-08 12:55:07 UTC",
    "updated_date": "2025-05-08 12:55:07 UTC"
  },
  {
    "arxiv_id": "2505.05195v1",
    "title": "Concept-Based Unsupervised Domain Adaptation",
    "authors": [
      "Xinyue Xu",
      "Yueying Hu",
      "Hui Tang",
      "Yi Qin",
      "Lu Mi",
      "Hao Wang",
      "Xiaomeng Li"
    ],
    "abstract": "Concept Bottleneck Models (CBMs) enhance interpretability by explaining\npredictions through human-understandable concepts but typically assume that\ntraining and test data share the same distribution. This assumption often fails\nunder domain shifts, leading to degraded performance and poor generalization.\nTo address these limitations and improve the robustness of CBMs, we propose the\nConcept-based Unsupervised Domain Adaptation (CUDA) framework. CUDA is designed\nto: (1) align concept representations across domains using adversarial\ntraining, (2) introduce a relaxation threshold to allow minor domain-specific\ndifferences in concept distributions, thereby preventing performance drop due\nto over-constraints of these distributions, (3) infer concepts directly in the\ntarget domain without requiring labeled concept data, enabling CBMs to adapt to\ndiverse domains, and (4) integrate concept learning into conventional domain\nadaptation (DA) with theoretical guarantees, improving interpretability and\nestablishing new benchmarks for DA. Experiments demonstrate that our approach\nsignificantly outperforms the state-of-the-art CBM and DA methods on real-world\ndatasets.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted by ICML 2025",
    "pdf_url": "http://arxiv.org/pdf/2505.05195v1",
    "published_date": "2025-05-08 12:52:02 UTC",
    "updated_date": "2025-05-08 12:52:02 UTC"
  },
  {
    "arxiv_id": "2505.05190v1",
    "title": "Revealing Weaknesses in Text Watermarking Through Self-Information Rewrite Attacks",
    "authors": [
      "Yixin Cheng",
      "Hongcheng Guo",
      "Yangming Li",
      "Leonid Sigal"
    ],
    "abstract": "Text watermarking aims to subtly embed statistical signals into text by\ncontrolling the Large Language Model (LLM)'s sampling process, enabling\nwatermark detectors to verify that the output was generated by the specified\nmodel. The robustness of these watermarking algorithms has become a key factor\nin evaluating their effectiveness. Current text watermarking algorithms embed\nwatermarks in high-entropy tokens to ensure text quality. In this paper, we\nreveal that this seemingly benign design can be exploited by attackers, posing\na significant risk to the robustness of the watermark. We introduce a generic\nefficient paraphrasing attack, the Self-Information Rewrite Attack (SIRA),\nwhich leverages the vulnerability by calculating the self-information of each\ntoken to identify potential pattern tokens and perform targeted attack. Our\nwork exposes a widely prevalent vulnerability in current watermarking\nalgorithms. The experimental results show SIRA achieves nearly 100% attack\nsuccess rates on seven recent watermarking methods with only 0.88 USD per\nmillion tokens cost. Our approach does not require any access to the watermark\nalgorithms or the watermarked LLM and can seamlessly transfer to any LLM as the\nattack model, even mobile-level models. Our findings highlight the urgent need\nfor more robust watermarking.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL",
      "cs.CR"
    ],
    "primary_category": "cs.LG",
    "comment": "ICML 2025 Accpeted",
    "pdf_url": "http://arxiv.org/pdf/2505.05190v1",
    "published_date": "2025-05-08 12:39:00 UTC",
    "updated_date": "2025-05-08 12:39:00 UTC"
  },
  {
    "arxiv_id": "2505.05189v1",
    "title": "Biomed-DPT: Dual Modality Prompt Tuning for Biomedical Vision-Language Models",
    "authors": [
      "Wei Peng",
      "Kang Liu",
      "Jianchen Hu",
      "Meng Zhang"
    ],
    "abstract": "Prompt learning is one of the most effective paradigms for adapting\npre-trained vision-language models (VLMs) to the biomedical image\nclassification tasks in few shot scenarios. However, most of the current prompt\nlearning methods only used the text prompts and ignored the particular\nstructures (such as the complex anatomical structures and subtle pathological\nfeatures) in the biomedical images. In this work, we propose Biomed-DPT, a\nknowledge-enhanced dual modality prompt tuning technique. In designing the text\nprompt, Biomed-DPT constructs a dual prompt including the template-driven\nclinical prompts and the large language model (LLM)-driven domain-adapted\nprompts, then extracts the clinical knowledge from the domain-adapted prompts\nthrough the knowledge distillation technique. In designing the vision prompt,\nBiomed-DPT introduces the zero vector as a soft prompt to leverage attention\nre-weighting so that the focus on non-diagnostic regions and the recognition of\nnon-critical pathological features are avoided. Biomed-DPT achieves an average\nclassification accuracy of 66.14\\% across 11 biomedical image datasets covering\n9 modalities and 10 organs, with performance reaching 78.06\\% in base classes\nand 75.97\\% in novel classes, surpassing the Context Optimization (CoOp) method\nby 6.20\\%, 3.78\\%, and 8.04\\%, respectively. Our code are available at\n\\underline{https://github.com/Kanyooo/Biomed-DPT}.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2505.05189v1",
    "published_date": "2025-05-08 12:37:51 UTC",
    "updated_date": "2025-05-08 12:37:51 UTC"
  },
  {
    "arxiv_id": "2505.05181v1",
    "title": "Stochastic Variational Propagation: Local, Scalable and Efficient Alternative to Backpropagation",
    "authors": [
      "Bojian Yin",
      "Federico Corradi"
    ],
    "abstract": "Backpropagation (BP) is the cornerstone of deep learning, but its reliance on\nglobal gradient synchronization limits scalability and imposes significant\nmemory overhead. We propose Stochastic Variational Propagation (SVP), a\nscalable alternative that reframes training as hierarchical variational\ninference. SVP treats layer activations as latent variables and optimizes local\nEvidence Lower Bounds (ELBOs), enabling independent, local updates while\npreserving global coherence. However, directly applying KL divergence in\nlayer-wise ELBOs risks inter-layer's representation collapse due to excessive\ncompression. To prevent this, SVP projects activations into low-dimensional\nspaces via fixed random matrices, ensuring information preservation and\nrepresentational diversity. Combined with a feature alignment loss for\ninter-layer consistency, SVP achieves competitive accuracy with BP across\ndiverse architectures (MLPs, CNNs, Transformers) and datasets (MNIST to\nImageNet), reduces memory usage by up to 4x, and significantly improves\nscalability. More broadly, SVP introduces a probabilistic perspective to deep\nrepresentation learning, opening pathways toward more modular and interpretable\nneural network design.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "14 pages, 5 figures",
    "pdf_url": "http://arxiv.org/pdf/2505.05181v1",
    "published_date": "2025-05-08 12:32:29 UTC",
    "updated_date": "2025-05-08 12:32:29 UTC"
  },
  {
    "arxiv_id": "2505.05177v1",
    "title": "MARK: Memory Augmented Refinement of Knowledge",
    "authors": [
      "Anish Ganguli",
      "Prabal Deb",
      "Debleena Banerjee"
    ],
    "abstract": "Large Language Models (LLMs) assist in specialized tasks but struggle to\nalign with evolving domain knowledge without costly fine-tuning. Domain\nknowledge consists of: Knowledge: Immutable facts (e.g., 'A stone is solid')\nand generally accepted principles (e.g., ethical standards); Refined Memory:\nEvolving insights shaped by business needs and real-world changes. However, a\nsignificant gap often exists between a domain expert's deep, nuanced\nunderstanding and the system's domain knowledge, which can hinder accurate\ninformation retrieval and application. Our Memory-Augmented Refinement of\nKnowledge (MARK) framework enables LLMs to continuously learn without\nretraining by leveraging structured refined memory, inspired by the Society of\nMind. MARK operates through specialized agents, each serving a distinct role:\nResidual Refined Memory Agent: Stores and retrieves domain-specific insights to\nmaintain context over time; User Question Refined Memory Agent: Captures\nuser-provided facts, abbreviations, and terminology for better comprehension;\nLLM Response Refined Memory Agent: Extracts key elements from responses for\nrefinement and personalization. These agents analyse stored refined memory,\ndetect patterns, resolve contradictions, and improve response accuracy.\nTemporal factors like recency and frequency prioritize relevant information\nwhile discarding outdated insights. MARK enhances LLMs in multiple ways: Ground\nTruth Strategy: Reduces hallucinations by establishing a structured reference;\nDomain-Specific Adaptation: Essential for fields like healthcare, law, and\nmanufacturing, where proprietary insights are absent from public datasets;\nPersonalized AI Assistants: Improves virtual assistants by remembering user\npreferences, ensuring coherent responses over time.",
    "categories": [
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2505.05177v1",
    "published_date": "2025-05-08 12:28:00 UTC",
    "updated_date": "2025-05-08 12:28:00 UTC"
  },
  {
    "arxiv_id": "2505.05170v1",
    "title": "Dukawalla: Voice Interfaces for Small Businesses in Africa",
    "authors": [
      "Elizabeth Ankrah",
      "Stephanie Nyairo",
      "Mercy Muchai",
      "Kagonya Awori",
      "Millicent Ochieng",
      "Mark Kariuki",
      "Jacki O'Neill"
    ],
    "abstract": "Small and medium sized businesses often struggle with data driven decision\nmaking do to a lack of advanced analytics tools, especially in African\ncountries where they make up a majority of the workforce. Though many tools\nexist they are not designed to fit into the ways of working of SMB workers who\nare mobile first, have limited time to learn new workflows, and for whom social\nand business are tightly coupled. To address this, the Dukawalla prototype was\ncreated. This intelligent assistant bridges the gap between raw business data,\nand actionable insights by leveraging voice interaction and the power of\ngenerative AI. Dukawalla provides an intuitive way for business owners to\ninteract with their data, aiding in informed decision making. This paper\nexamines Dukawalla's deployment across SMBs in Nairobi, focusing on their\nexperiences using this voice based assistant to streamline data collection and\nprovide business insights",
    "categories": [
      "cs.HC",
      "cs.AI"
    ],
    "primary_category": "cs.HC",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2505.05170v1",
    "published_date": "2025-05-08 12:13:16 UTC",
    "updated_date": "2025-05-08 12:13:16 UTC"
  },
  {
    "arxiv_id": "2505.05145v1",
    "title": "Understanding In-context Learning of Addition via Activation Subspaces",
    "authors": [
      "Xinyan Hu",
      "Kayo Yin",
      "Michael I. Jordan",
      "Jacob Steinhardt",
      "Lijie Chen"
    ],
    "abstract": "To perform in-context learning, language models must extract signals from\nindividual few-shot examples, aggregate these into a learned prediction rule,\nand then apply this rule to new examples. How is this implemented in the\nforward pass of modern transformer models? To study this, we consider a\nstructured family of few-shot learning tasks for which the true prediction rule\nis to add an integer $k$ to the input. We find that Llama-3-8B attains high\naccuracy on this task for a range of $k$, and localize its few-shot ability to\njust three attention heads via a novel optimization approach. We further show\nthe extracted signals lie in a six-dimensional subspace, where four of the\ndimensions track the unit digit and the other two dimensions track overall\nmagnitude. We finally examine how these heads extract information from\nindividual few-shot examples, identifying a self-correction mechanism in which\nmistakes from earlier examples are suppressed by later examples. Our results\ndemonstrate how tracking low-dimensional subspaces across a forward pass can\nprovide insight into fine-grained computational structures.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.LG",
    "comment": "16 pages",
    "pdf_url": "http://arxiv.org/pdf/2505.05145v1",
    "published_date": "2025-05-08 11:32:46 UTC",
    "updated_date": "2025-05-08 11:32:46 UTC"
  },
  {
    "arxiv_id": "2505.05138v1",
    "title": "Guiding Evolutionary AutoEncoder Training with Activation-Based Pruning Operators",
    "authors": [
      "Steven Jorgensen",
      "Erik Hemberg",
      "Jamal Toutouh",
      "Una-May O'Reilly"
    ],
    "abstract": "This study explores a novel approach to neural network pruning using\nevolutionary computation, focusing on simultaneously pruning the encoder and\ndecoder of an autoencoder. We introduce two new mutation operators that use\nlayer activations to guide weight pruning. Our findings reveal that one of\nthese activation-informed operators outperforms random pruning, resulting in\nmore efficient autoencoders with comparable performance to canonically trained\nmodels. Prior work has established that autoencoder training is effective and\nscalable with a spatial coevolutionary algorithm that cooperatively coevolves a\npopulation of encoders with a population of decoders, rather than one\nautoencoder. We evaluate how the same activity-guided mutation operators\ntransfer to this context. We find that random pruning is better than guided\npruning, in the coevolutionary setting. This suggests activation-based guidance\nproves more effective in low-dimensional pruning environments, where\nconstrained sample spaces can lead to deviations from true uniformity in\nrandomization. Conversely, population-driven strategies enhance robustness by\nexpanding the total pruning dimensionality, achieving statistically uniform\nrandomness that better preserves system dynamics. We experiment with pruning\naccording to different schedules and present best combinations of operator and\nschedule for the canonical and coevolving populations cases.",
    "categories": [
      "cs.NE",
      "cs.AI"
    ],
    "primary_category": "cs.NE",
    "comment": "Accepted to The Genetic and Evolutionary Computation Conference\n  (GECCO 2025)",
    "pdf_url": "http://arxiv.org/pdf/2505.05138v1",
    "published_date": "2025-05-08 11:21:29 UTC",
    "updated_date": "2025-05-08 11:21:29 UTC"
  },
  {
    "arxiv_id": "2505.05115v1",
    "title": "Is there a half-life for the success rates of AI agents?",
    "authors": [
      "Toby Ord"
    ],
    "abstract": "Building on the recent empirical work of Kwa et al. (2025), I show that\nwithin their suite of research-engineering tasks the performance of AI agents\non longer-duration tasks can be explained by an extremely simple mathematical\nmodel -- a constant rate of failing during each minute a human would take to do\nthe task. This implies an exponentially declining success rate with the length\nof the task and that each agent could be characterised by its own half-life.\nThis empirical regularity allows us to estimate the success rate for an agent\nat different task lengths. And the fact that this model is a good fit for the\ndata is suggestive of the underlying causes of failure on longer tasks -- that\nthey involve increasingly large sets of subtasks where failing any one fails\nthe task. Whether this model applies more generally on other suites of tasks is\nunknown and an important subject for further work.",
    "categories": [
      "cs.AI",
      "68T42",
      "I.2.8"
    ],
    "primary_category": "cs.AI",
    "comment": "9 pages, 5 figures",
    "pdf_url": "http://arxiv.org/pdf/2505.05115v1",
    "published_date": "2025-05-08 10:31:03 UTC",
    "updated_date": "2025-05-08 10:31:03 UTC"
  },
  {
    "arxiv_id": "2505.05108v1",
    "title": "Multi-agent Embodied AI: Advances and Future Directions",
    "authors": [
      "Zhaohan Feng",
      "Ruiqi Xue",
      "Lei Yuan",
      "Yang Yu",
      "Ning Ding",
      "Meiqin Liu",
      "Bingzhao Gao",
      "Jian Sun",
      "Gang Wang"
    ],
    "abstract": "Embodied artificial intelligence (Embodied AI) plays a pivotal role in the\napplication of advanced technologies in the intelligent era, where AI systems\nare integrated with physical bodies that enable them to perceive, reason, and\ninteract with their environments. Through the use of sensors for input and\nactuators for action, these systems can learn and adapt based on real-world\nfeedback, allowing them to perform tasks effectively in dynamic and\nunpredictable environments. As techniques such as deep learning (DL),\nreinforcement learning (RL), and large language models (LLMs) mature, embodied\nAI has become a leading field in both academia and industry, with applications\nspanning robotics, healthcare, transportation, and manufacturing. However, most\nresearch has focused on single-agent systems that often assume static, closed\nenvironments, whereas real-world embodied AI must navigate far more complex\nscenarios. In such settings, agents must not only interact with their\nsurroundings but also collaborate with other agents, necessitating\nsophisticated mechanisms for adaptation, real-time learning, and collaborative\nproblem-solving. Despite increasing interest in multi-agent systems, existing\nresearch remains narrow in scope, often relying on simplified models that fail\nto capture the full complexity of dynamic, open environments for multi-agent\nembodied AI. Moreover, no comprehensive survey has systematically reviewed the\nadvancements in this area. As embodied AI rapidly evolves, it is crucial to\ndeepen our understanding of multi-agent embodied AI to address the challenges\npresented by real-world applications. To fill this gap and foster further\ndevelopment in the field, this paper reviews the current state of research,\nanalyzes key contributions, and identifies challenges and future directions,\nproviding insights to guide innovation and progress in this field.",
    "categories": [
      "cs.AI",
      "cs.MA"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2505.05108v1",
    "published_date": "2025-05-08 10:13:53 UTC",
    "updated_date": "2025-05-08 10:13:53 UTC"
  },
  {
    "arxiv_id": "2505.05106v1",
    "title": "A Neuro-Symbolic Framework for Sequence Classification with Relational and Temporal Knowledge",
    "authors": [
      "Luca Salvatore Lorello",
      "Marco Lippi",
      "Stefano Melacci"
    ],
    "abstract": "One of the goals of neuro-symbolic artificial intelligence is to exploit\nbackground knowledge to improve the performance of learning tasks. However,\nmost of the existing frameworks focus on the simplified scenario where\nknowledge does not change over time and does not cover the temporal dimension.\nIn this work we consider the much more challenging problem of knowledge-driven\nsequence classification where different portions of knowledge must be employed\nat different timesteps, and temporal relations are available. Our experimental\nevaluation compares multi-stage neuro-symbolic and neural-only architectures,\nand it is conducted on a newly-introduced benchmarking framework. Results\ndemonstrate the challenging nature of this novel setting, and also highlight\nunder-explored shortcomings of neuro-symbolic methods, representing a precious\nreference for future research.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2505.05106v1",
    "published_date": "2025-05-08 10:10:00 UTC",
    "updated_date": "2025-05-08 10:10:00 UTC"
  },
  {
    "arxiv_id": "2505.05086v1",
    "title": "Beyond Low-rank Decomposition: A Shortcut Approach for Efficient On-Device Learning",
    "authors": [
      "Le-Trung Nguyen",
      "Ael Quelennec",
      "Van-Tam Nguyen",
      "Enzo Tartaglione"
    ],
    "abstract": "On-device learning has emerged as a promising direction for AI development,\nparticularly because of its potential to reduce latency issues and mitigate\nprivacy risks associated with device-server communication, while improving\nenergy efficiency. Despite these advantages, significant memory and\ncomputational constraints still represent major challenges for its deployment.\nDrawing on previous studies on low-rank decomposition methods that address\nactivation memory bottlenecks in backpropagation, we propose a novel shortcut\napproach as an alternative. Our analysis and experiments demonstrate that our\nmethod can reduce activation memory usage, even up to $120.09\\times$ compared\nto vanilla training, while also reducing overall training FLOPs up to\n$1.86\\times$ when evaluated on traditional benchmarks.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2505.05086v1",
    "published_date": "2025-05-08 09:34:15 UTC",
    "updated_date": "2025-05-08 09:34:15 UTC"
  },
  {
    "arxiv_id": "2505.05071v1",
    "title": "FG-CLIP: Fine-Grained Visual and Textual Alignment",
    "authors": [
      "Chunyu Xie",
      "Bin Wang",
      "Fanjing Kong",
      "Jincheng Li",
      "Dawei Liang",
      "Gengshen Zhang",
      "Dawei Leng",
      "Yuhui Yin"
    ],
    "abstract": "Contrastive Language-Image Pre-training (CLIP) excels in multimodal tasks\nsuch as image-text retrieval and zero-shot classification but struggles with\nfine-grained understanding due to its focus on coarse-grained short captions.\nTo address this, we propose Fine-Grained CLIP (FG-CLIP), which enhances\nfine-grained understanding through three key innovations. First, we leverage\nlarge multimodal models to generate 1.6 billion long caption-image pairs for\ncapturing global-level semantic details. Second, a high-quality dataset is\nconstructed with 12 million images and 40 million region-specific bounding\nboxes aligned with detailed captions to ensure precise, context-rich\nrepresentations. Third, 10 million hard fine-grained negative samples are\nincorporated to improve the model's ability to distinguish subtle semantic\ndifferences. Corresponding training methods are meticulously designed for these\ndata. Extensive experiments demonstrate that FG-CLIP outperforms the original\nCLIP and other state-of-the-art methods across various downstream tasks,\nincluding fine-grained understanding, open-vocabulary object detection,\nimage-text retrieval, and general multimodal benchmarks. These results\nhighlight FG-CLIP's effectiveness in capturing fine-grained image details and\nimproving overall model performance. The related data, code, and models are\navailable at https://github.com/360CVGroup/FG-CLIP.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "Accepted at ICML 2025",
    "pdf_url": "http://arxiv.org/pdf/2505.05071v1",
    "published_date": "2025-05-08 09:06:53 UTC",
    "updated_date": "2025-05-08 09:06:53 UTC"
  },
  {
    "arxiv_id": "2505.05059v1",
    "title": "Enhancing Reinforcement Learning for the Floorplanning of Analog ICs with Beam Search",
    "authors": [
      "Sandro Junior Della Rovere",
      "Davide Basso",
      "Luca Bortolussi",
      "Mirjana Videnovic-Misic",
      "Husni Habal"
    ],
    "abstract": "The layout of analog ICs requires making complex trade-offs, while addressing\ndevice physics and variability of the circuits. This makes full automation with\nlearning-based solutions hard to achieve. However, reinforcement learning (RL)\nhas recently reached significant results, particularly in solving the\nfloorplanning problem. This paper presents a hybrid method that combines RL\nwith a beam (BS) strategy. The BS algorithm enhances the agent's inference\nprocess, allowing for the generation of flexible floorplans by accomodating\nvarious objective weightings, and addressing congestion without without the\nneed for policy retraining or fine-tuning. Moreover, the RL agent's\ngeneralization ability stays intact, along with its efficient handling of\ncircuit features and constraints. Experimental results show approx. 5-85%\nimprovement in area, dead space and half-perimeter wire length compared to a\nstandard RL application, along with higher rewards for the agent. Moreover,\nperformance and efficiency align closely with those of existing\nstate-of-the-art techniques.",
    "categories": [
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "Published in Proceedings of the 21st International Conference on\n  Synthesis, Modeling, Analysis and Simulation Methods, and Applications to\n  Circuit Design (SMACD 2025). 4 pages, 3 figures",
    "pdf_url": "http://arxiv.org/pdf/2505.05059v1",
    "published_date": "2025-05-08 08:50:32 UTC",
    "updated_date": "2025-05-08 08:50:32 UTC"
  },
  {
    "arxiv_id": "2505.05056v1",
    "title": "Teochew-Wild: The First In-the-wild Teochew Dataset with Orthographic Annotations",
    "authors": [
      "Linrong Pan",
      "Chenglong Jiang",
      "Gaoze Hou",
      "Ying Gao"
    ],
    "abstract": "This paper reports the construction of the Teochew-Wild, a speech corpus of\nthe Teochew dialect. The corpus includes 18.9 hours of in-the-wild Teochew\nspeech data from multiple speakers, covering both formal and colloquial\nexpressions, with precise orthographic and pinyin annotations. Additionally, we\nprovide supplementary text processing tools and resources to propel research\nand applications in speech tasks for this low-resource language, such as\nautomatic speech recognition (ASR) and text-to-speech (TTS). To the best of our\nknowledge, this is the first publicly available Teochew dataset with accurate\northographic annotations. We conduct experiments on the corpus, and the results\nvalidate its effectiveness in ASR and TTS tasks.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2505.05056v1",
    "published_date": "2025-05-08 08:47:11 UTC",
    "updated_date": "2025-05-08 08:47:11 UTC"
  },
  {
    "arxiv_id": "2505.05054v1",
    "title": "Direct Image Classification from Fourier Ptychographic Microscopy Measurements without Reconstruction",
    "authors": [
      "Navya Sonal Agarwal",
      "Jan Philipp Schneider",
      "Kanchana Vaishnavi Gandikota",
      "Syed Muhammad Kazim",
      "John Meshreki",
      "Ivo Ihrke",
      "Michael Moeller"
    ],
    "abstract": "The computational imaging technique of Fourier Ptychographic Microscopy (FPM)\nenables high-resolution imaging with a wide field of view and can serve as an\nextremely valuable tool, e.g. in the classification of cells in medical\napplications. However, reconstructing a high-resolution image from tens or even\nhundreds of measurements is computationally expensive, particularly for a wide\nfield of view. Therefore, in this paper, we investigate the idea of classifying\nthe image content in the FPM measurements directly without performing a\nreconstruction step first. We show that Convolutional Neural Networks (CNN) can\nextract meaningful information from measurement sequences, significantly\noutperforming the classification on a single band-limited image (up to 12 %)\nwhile being significantly more efficient than a reconstruction of a\nhigh-resolution image. Furthermore, we demonstrate that a learned multiplexing\nof several raw measurements allows maintaining the classification accuracy\nwhile reducing the amount of data (and consequently also the acquisition time)\nsignificantly.",
    "categories": [
      "eess.IV",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "eess.IV",
    "comment": "ISCS 2025",
    "pdf_url": "http://arxiv.org/pdf/2505.05054v1",
    "published_date": "2025-05-08 08:46:28 UTC",
    "updated_date": "2025-05-08 08:46:28 UTC"
  },
  {
    "arxiv_id": "2505.05040v1",
    "title": "Image-Text Relation Prediction for Multilingual Tweets",
    "authors": [
      "Matīss Rikters",
      "Edison Marrese-Taylor"
    ],
    "abstract": "Various social networks have been allowing media uploads for over a decade\nnow. Still, it has not always been clear what is their relation with the posted\ntext or even if there is any at all. In this work, we explore how multilingual\nvision-language models tackle the task of image-text relation prediction in\ndifferent languages, and construct a dedicated balanced benchmark data set from\nTwitter posts in Latvian along with their manual translations into English. We\ncompare our results to previous work and show that the more recently released\nvision-language model checkpoints are becoming increasingly capable at this\ntask, but there is still much room for further improvement.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2505.05040v1",
    "published_date": "2025-05-08 08:23:20 UTC",
    "updated_date": "2025-05-08 08:23:20 UTC"
  },
  {
    "arxiv_id": "2505.05029v1",
    "title": "A Reputation System for Large Language Model-based Multi-agent Systems to Avoid the Tragedy of the Commons",
    "authors": [
      "Siyue Ren",
      "Wanli Fu",
      "Xinkun Zou",
      "Chen Shen",
      "Yi Cai",
      "Chen Chu",
      "Zhen Wang",
      "Shuyue Hu"
    ],
    "abstract": "The tragedy of the commons, where individual self-interest leads to\ncollectively disastrous outcomes, is a pervasive challenge in human society.\nRecent studies have demonstrated that similar phenomena can arise in generative\nmulti-agent systems (MASs). To address this challenge, this paper explores the\nuse of reputation systems as a remedy. We propose RepuNet, a dynamic,\ndual-level reputation framework that models both agent-level reputation\ndynamics and system-level network evolution. Specifically, driven by direct\ninteractions and indirect gossip, agents form reputations for both themselves\nand their peers, and decide whether to connect or disconnect other agents for\nfuture interactions. Through two distinct scenarios, we show that RepuNet\neffectively mitigates the 'tragedy of the commons', promoting and sustaining\ncooperation in generative MASs. Moreover, we find that reputation systems can\ngive rise to rich emergent behaviors in generative MASs, such as the formation\nof cooperative clusters, the social isolation of exploitative agents, and the\npreference for sharing positive gossip rather than negative ones.",
    "categories": [
      "cs.AI",
      "cs.MA"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2505.05029v1",
    "published_date": "2025-05-08 08:02:20 UTC",
    "updated_date": "2025-05-08 08:02:20 UTC"
  },
  {
    "arxiv_id": "2505.05019v1",
    "title": "Generating Reliable Synthetic Clinical Trial Data: The Role of Hyperparameter Optimization and Domain Constraints",
    "authors": [
      "Waldemar Hahn",
      "Jan-Niklas Eckardt",
      "Christoph Röllig",
      "Martin Sedlmayr",
      "Jan Moritz Middeke",
      "Markus Wolfien"
    ],
    "abstract": "The generation of synthetic clinical trial data offers a promising approach\nto mitigating privacy concerns and data accessibility limitations in medical\nresearch. However, ensuring that synthetic datasets maintain high fidelity,\nutility, and adherence to domain-specific constraints remains a key challenge.\nWhile hyperparameter optimization (HPO) has been shown to improve generative\nmodel performance, the effectiveness of different optimization strategies for\nsynthetic clinical data remains unclear. This study systematically evaluates\nfour HPO strategies across eight generative models, comparing single-metric\noptimization against compound metric optimization approaches. Our results\ndemonstrate that HPO consistently improves synthetic data quality, with TVAE,\nCTGAN, and CTAB-GAN+ achieving improvements of up to 60%, 39%, and 38%,\nrespectively. Compound metric optimization outperformed single-metric\nstrategies, producing more balanced and generalizable synthetic datasets.\nInterestingly, HPO alone is insufficient to ensure clinically valid synthetic\ndata, as all models exhibited violations of fundamental survival constraints.\nPreprocessing and postprocessing played a crucial role in reducing these\nviolations, as models lacking robust processing steps produced invalid data in\nup to 61% of cases. These findings underscore the necessity of integrating\nexplicit domain knowledge alongside HPO to create high quality synthetic\ndatasets. Our study provides actionable recommendations for improving synthetic\ndata generation, with future research needed to refine metric selection and\nvalidate these findings on larger datasets to enhance clinical applicability.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2505.05019v1",
    "published_date": "2025-05-08 07:51:36 UTC",
    "updated_date": "2025-05-08 07:51:36 UTC"
  },
  {
    "arxiv_id": "2505.05015v1",
    "title": "An Agent-Based Modeling Approach to Free-Text Keyboard Dynamics for Continuous Authentication",
    "authors": [
      "Roberto Dillon",
      "Arushi"
    ],
    "abstract": "Continuous authentication systems leveraging free-text keyboard dynamics\noffer a promising additional layer of security in a multifactor authentication\nsetup that can be used in a transparent way with no impact on user experience.\nThis study investigates the efficacy of behavioral biometrics by employing an\nAgent-Based Model (ABM) to simulate diverse typing profiles across mechanical\nand membrane keyboards. Specifically, we generated synthetic keystroke data\nfrom five unique agents, capturing features related to dwell time, flight time,\nand error rates within sliding 5-second windows updated every second. Two\nmachine learning approaches, One-Class Support Vector Machine (OC-SVM) and\nRandom Forest (RF), were evaluated for user verification. Results revealed a\nstark contrast in performance: while One-Class SVM failed to differentiate\nindividual users within each group, Random Forest achieved robust\nintra-keyboard user recognition (Accuracy > 0.7) but struggled to generalize\nacross keyboards for the same user, highlighting the significant impact of\nkeyboard hardware on typing behavior. These findings suggest that: (1)\nkeyboard-specific user profiles may be necessary for reliable authentication,\nand (2) ensemble methods like RF outperform One-Class SVM in capturing\nfine-grained user-specific patterns.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CR",
      "68T10, 62H30",
      "I.2.6; I.5.4; I.6.3"
    ],
    "primary_category": "cs.LG",
    "comment": "16 pages, 5 figures, 12 tables",
    "pdf_url": "http://arxiv.org/pdf/2505.05015v1",
    "published_date": "2025-05-08 07:42:05 UTC",
    "updated_date": "2025-05-08 07:42:05 UTC"
  },
  {
    "arxiv_id": "2505.05001v1",
    "title": "StabStitch++: Unsupervised Online Video Stitching with Spatiotemporal Bidirectional Warps",
    "authors": [
      "Lang Nie",
      "Chunyu Lin",
      "Kang Liao",
      "Yun Zhang",
      "Shuaicheng Liu",
      "Yao Zhao"
    ],
    "abstract": "We retarget video stitching to an emerging issue, named warping shake, which\nunveils the temporal content shakes induced by sequentially unsmooth warps when\nextending image stitching to video stitching. Even if the input videos are\nstable, the stitched video can inevitably cause undesired warping shakes and\naffect the visual experience. To address this issue, we propose StabStitch++, a\nnovel video stitching framework to realize spatial stitching and temporal\nstabilization with unsupervised learning simultaneously. First, different from\nexisting learning-based image stitching solutions that typically warp one image\nto align with another, we suppose a virtual midplane between original image\nplanes and project them onto it. Concretely, we design a differentiable\nbidirectional decomposition module to disentangle the homography transformation\nand incorporate it into our spatial warp, evenly spreading alignment burdens\nand projective distortions across two views. Then, inspired by camera paths in\nvideo stabilization, we derive the mathematical expression of stitching\ntrajectories in video stitching by elaborately integrating spatial and temporal\nwarps. Finally, a warp smoothing model is presented to produce stable stitched\nvideos with a hybrid loss to simultaneously encourage content alignment,\ntrajectory smoothness, and online collaboration. Compared with StabStitch that\nsacrifices alignment for stabilization, StabStitch++ makes no compromise and\noptimizes both of them simultaneously, especially in the online mode. To\nestablish an evaluation benchmark and train the learning framework, we build a\nvideo stitching dataset with a rich diversity in camera motions and scenes.\nExperiments exhibit that StabStitch++ surpasses current solutions in stitching\nperformance, robustness, and efficiency, offering compelling advancements in\nthis field by building a real-time online video stitching system.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "TPAMI2025; https://github.com/nie-lang/StabStitch2. arXiv admin note:\n  text overlap with arXiv:2403.06378",
    "pdf_url": "http://arxiv.org/pdf/2505.05001v1",
    "published_date": "2025-05-08 07:12:23 UTC",
    "updated_date": "2025-05-08 07:12:23 UTC"
  },
  {
    "arxiv_id": "2505.04997v1",
    "title": "Foam-Agent: Towards Automated Intelligent CFD Workflows",
    "authors": [
      "Ling Yue",
      "Nithin Somasekharan",
      "Yadi Cao",
      "Shaowu Pan"
    ],
    "abstract": "Computational Fluid Dynamics (CFD) is an essential simulation tool in various\nengineering disciplines, but it often requires substantial domain expertise and\nmanual configuration, creating barriers to entry. We present Foam-Agent, a\nmulti-agent framework that automates complex OpenFOAM-based CFD simulation\nworkflows from natural language inputs. Our innovation includes (1) a\nhierarchical multi-index retrieval system with specialized indices for\ndifferent simulation aspects, (2) a dependency-aware file generation system\nthat provides consistency management across configuration files, and (3) an\niterative error correction mechanism that diagnoses and resolves simulation\nfailures without human intervention. Through comprehensive evaluation on the\ndataset of 110 simulation tasks, Foam-Agent achieves an 83.6% success rate with\nClaude 3.5 Sonnet, significantly outperforming existing frameworks (55.5% for\nMetaOpenFOAM and 37.3% for OpenFOAM-GPT). Ablation studies demonstrate the\ncritical contribution of each system component, with the specialized error\ncorrection mechanism providing a 36.4% performance improvement. Foam-Agent\nsubstantially lowers the CFD expertise threshold while maintaining modeling\naccuracy, demonstrating the potential of specialized multi-agent systems to\ndemocratize access to complex scientific simulation tools. The code is public\nat https://github.com/csml-rpi/Foam-Agent",
    "categories": [
      "cs.AI",
      "cs.MA"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2505.04997v1",
    "published_date": "2025-05-08 07:05:51 UTC",
    "updated_date": "2025-05-08 07:05:51 UTC"
  },
  {
    "arxiv_id": "2505.04994v1",
    "title": "Rethinking Invariance in In-context Learning",
    "authors": [
      "Lizhe Fang",
      "Yifei Wang",
      "Khashayar Gatmiry",
      "Lei Fang",
      "Yisen Wang"
    ],
    "abstract": "In-Context Learning (ICL) has emerged as a pivotal capability of\nauto-regressive large language models, yet it is hindered by a notable\nsensitivity to the ordering of context examples regardless of their mutual\nindependence. To address this issue, recent studies have introduced several\nvariant algorithms of ICL that achieve permutation invariance. However, many of\nthese do not exhibit comparable performance with the standard auto-regressive\nICL algorithm. In this work, we identify two crucial elements in the design of\nan invariant ICL algorithm: information non-leakage and context\ninterdependence, which are not simultaneously achieved by any of the existing\nmethods. These investigations lead us to the proposed Invariant ICL (InvICL), a\nmethodology designed to achieve invariance in ICL while ensuring the two\nproperties. Empirically, our findings reveal that InvICL surpasses previous\nmodels, both invariant and non-invariant, in most benchmark datasets,\nshowcasing superior generalization capabilities across varying input lengths.\nCode is available at https://github.com/PKU-ML/InvICL.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2505.04994v1",
    "published_date": "2025-05-08 06:59:14 UTC",
    "updated_date": "2025-05-08 06:59:14 UTC"
  },
  {
    "arxiv_id": "2505.04983v1",
    "title": "Decomposition of Probabilities of Causation with Two Mediators",
    "authors": [
      "Yuta Kawakami",
      "Jin Tian"
    ],
    "abstract": "Mediation analysis for probabilities of causation (PoC) provides a\nfundamental framework for evaluating the necessity and sufficiency of treatment\nin provoking an event through different causal pathways. One of the primary\nobjectives of causal mediation analysis is to decompose the total effect into\npath-specific components. In this study, we investigate the path-specific\nprobability of necessity and sufficiency (PNS) to decompose the total PNS into\npath-specific components along distinct causal pathways between treatment and\noutcome, incorporating two mediators. We define the path-specific PNS for\ndecomposition and provide an identification theorem. Furthermore, we conduct\nnumerical experiments to assess the properties of the proposed estimators from\nfinite samples and demonstrate their practical application using a real-world\neducational dataset.",
    "categories": [
      "stat.ME",
      "cs.AI"
    ],
    "primary_category": "stat.ME",
    "comment": "arXiv admin note: text overlap with arXiv:2412.14491",
    "pdf_url": "http://arxiv.org/pdf/2505.04983v1",
    "published_date": "2025-05-08 06:40:17 UTC",
    "updated_date": "2025-05-08 06:40:17 UTC"
  },
  {
    "arxiv_id": "2505.04977v1",
    "title": "ChainMarks: Securing DNN Watermark with Cryptographic Chain",
    "authors": [
      "Brian Choi",
      "Shu Wang",
      "Isabelle Choi",
      "Kun Sun"
    ],
    "abstract": "With the widespread deployment of deep neural network (DNN) models, dynamic\nwatermarking techniques are being used to protect the intellectual property of\nmodel owners. However, recent studies have shown that existing watermarking\nschemes are vulnerable to watermark removal and ambiguity attacks. Besides, the\nvague criteria for determining watermark presence further increase the\nlikelihood of such attacks. In this paper, we propose a secure DNN watermarking\nscheme named ChainMarks, which generates secure and robust watermarks by\nintroducing a cryptographic chain into the trigger inputs and utilizes a\ntwo-phase Monte Carlo method for determining watermark presence. First,\nChainMarks generates trigger inputs as a watermark dataset by repeatedly\napplying a hash function over a secret key, where the target labels associated\nwith trigger inputs are generated from the digital signature of model owner.\nThen, the watermarked model is produced by training a DNN over both the\noriginal and watermark datasets. To verify watermarks, we compare the predicted\nlabels of trigger inputs with the target labels and determine ownership with a\nmore accurate decision threshold that considers the classification probability\nof specific models. Experimental results show that ChainMarks exhibits higher\nlevels of robustness and security compared to state-of-the-art watermarking\nschemes. With a better marginal utility, ChainMarks provides a higher\nprobability guarantee of watermark presence in DNN models with the same level\nof watermark accuracy.",
    "categories": [
      "cs.CR",
      "cs.AI"
    ],
    "primary_category": "cs.CR",
    "comment": "Accepted In ACM ASIA Conference on Computer and Communications\n  Security (ASIA CCS '25), August 25-29, 2025, Ha Noi, Vietnam",
    "pdf_url": "http://arxiv.org/pdf/2505.04977v1",
    "published_date": "2025-05-08 06:30:46 UTC",
    "updated_date": "2025-05-08 06:30:46 UTC"
  },
  {
    "arxiv_id": "2505.04972v1",
    "title": "AI and Vision based Autonomous Navigation of Nano-Drones in Partially-Known Environments",
    "authors": [
      "Mattia Sartori",
      "Chetna Singhal",
      "Neelabhro Roy",
      "Davide Brunelli",
      "James Gross"
    ],
    "abstract": "The miniaturisation of sensors and processors, the advancements in connected\nedge intelligence, and the exponential interest in Artificial Intelligence are\nboosting the affirmation of autonomous nano-size drones in the Internet of\nRobotic Things ecosystem. However, achieving safe autonomous navigation and\nhigh-level tasks such as exploration and surveillance with these tiny platforms\nis extremely challenging due to their limited resources. This work focuses on\nenabling the safe and autonomous flight of a pocket-size, 30-gram platform\ncalled Crazyflie 2.1 in a partially known environment. We propose a novel\nAI-aided, vision-based reactive planning method for obstacle avoidance under\nthe ambit of Integrated Sensing, Computing and Communication paradigm. We deal\nwith the constraints of the nano-drone by splitting the navigation task into\ntwo parts: a deep learning-based object detector runs on the edge (external\nhardware) while the planning algorithm is executed onboard. The results show\nthe ability to command the drone at $\\sim8$ frames-per-second and a model\nperformance reaching a COCO mean-average-precision of $60.8$. Field experiments\ndemonstrate the feasibility of the solution with the drone flying at a top\nspeed of $1$ m/s while steering away from an obstacle placed in an unknown\nposition and reaching the target destination. The outcome highlights the\ncompatibility of the communication delay and the model performance with the\nrequirements of the real-time navigation task. We provide a feasible\nalternative to a fully onboard implementation that can be extended to\nautonomous exploration with nano-drones.",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.CV",
      "cs.LG",
      "cs.NI"
    ],
    "primary_category": "cs.RO",
    "comment": "in DCOSS-IoT 2025, Wi-DroIT 2025",
    "pdf_url": "http://arxiv.org/pdf/2505.04972v1",
    "published_date": "2025-05-08 06:16:36 UTC",
    "updated_date": "2025-05-08 06:16:36 UTC"
  },
  {
    "arxiv_id": "2505.04971v1",
    "title": "Moments of Causal Effects",
    "authors": [
      "Yuta Kawakami",
      "Jin Tian"
    ],
    "abstract": "The moments of random variables are fundamental statistical measures for\ncharacterizing the shape of a probability distribution, encompassing metrics\nsuch as mean, variance, skewness, and kurtosis. Additionally, the product\nmoments, including covariance and correlation, reveal the relationships between\nmultiple random variables. On the other hand, the primary focus of causal\ninference is the evaluation of causal effects, which are defined as the\ndifference between two potential outcomes. While traditional causal effect\nassessment focuses on the average causal effect, this work provides\ndefinitions, identification theorems, and bounds for moments and product\nmoments of causal effects to analyze their distribution and relationships. We\nconduct experiments to illustrate the estimation of the moments of causal\neffects from finite samples and demonstrate their practical application using a\nreal-world medical dataset.",
    "categories": [
      "stat.ME",
      "cs.AI"
    ],
    "primary_category": "stat.ME",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2505.04971v1",
    "published_date": "2025-05-08 06:09:05 UTC",
    "updated_date": "2025-05-08 06:09:05 UTC"
  },
  {
    "arxiv_id": "2505.04966v1",
    "title": "Position: The AI Conference Peer Review Crisis Demands Author Feedback and Reviewer Rewards",
    "authors": [
      "Jaeho Kim",
      "Yunseok Lee",
      "Seulki Lee"
    ],
    "abstract": "The peer review process in major artificial intelligence (AI) conferences\nfaces unprecedented challenges with the surge of paper submissions (exceeding\n10,000 submissions per venue), accompanied by growing concerns over review\nquality and reviewer responsibility. This position paper argues for the need to\ntransform the traditional one-way review system into a bi-directional feedback\nloop where authors evaluate review quality and reviewers earn formal\naccreditation, creating an accountability framework that promotes a\nsustainable, high-quality peer review system. The current review system can be\nviewed as an interaction between three parties: the authors, reviewers, and\nsystem (i.e., conference), where we posit that all three parties share\nresponsibility for the current problems. However, issues with authors can only\nbe addressed through policy enforcement and detection tools, and ethical\nconcerns can only be corrected through self-reflection. As such, this paper\nfocuses on reforming reviewer accountability with systematic rewards through\ntwo key mechanisms: (1) a two-stage bi-directional review system that allows\nauthors to evaluate reviews while minimizing retaliatory behavior, (2)a\nsystematic reviewer reward system that incentivizes quality reviewing. We ask\nfor the community's strong interest in these problems and the reforms that are\nneeded to enhance the peer review process.",
    "categories": [
      "cs.AI",
      "cs.CY"
    ],
    "primary_category": "cs.AI",
    "comment": "ICML2025 Position Track Oral",
    "pdf_url": "http://arxiv.org/pdf/2505.04966v1",
    "published_date": "2025-05-08 05:51:48 UTC",
    "updated_date": "2025-05-08 05:51:48 UTC"
  },
  {
    "arxiv_id": "2505.04961v1",
    "title": "ADD: Physics-Based Motion Imitation with Adversarial Differential Discriminators",
    "authors": [
      "Ziyu Zhang",
      "Sergey Bashkirov",
      "Dun Yang",
      "Michael Taylor",
      "Xue Bin Peng"
    ],
    "abstract": "Multi-objective optimization problems, which require the simultaneous\noptimization of multiple terms, are prevalent across numerous applications.\nExisting multi-objective optimization methods often rely on manually tuned\naggregation functions to formulate a joint optimization target. The performance\nof such hand-tuned methods is heavily dependent on careful weight selection, a\ntime-consuming and laborious process. These limitations also arise in the\nsetting of reinforcement-learning-based motion tracking for physically\nsimulated characters, where intricately crafted reward functions are typically\nused to achieve high-fidelity results. Such solutions not only require domain\nexpertise and significant manual adjustment, but also limit the applicability\nof the resulting reward function across diverse skills. To bridge this gap, we\npresent a novel adversarial multi-objective optimization technique that is\nbroadly applicable to a range of multi-objective optimization problems,\nincluding motion tracking. The proposed adversarial differential discriminator\nreceives a single positive sample, yet is still effective at guiding the\noptimization process. We demonstrate that our technique can enable characters\nto closely replicate a variety of acrobatic and agile behaviors, achieving\ncomparable quality to state-of-the-art motion-tracking methods, without relying\non manually tuned reward functions. Results are best visualized through\nhttps://youtu.be/rz8BYCE9E2w.",
    "categories": [
      "cs.GR",
      "cs.AI",
      "cs.CV",
      "cs.RO"
    ],
    "primary_category": "cs.GR",
    "comment": "19 pages, 15 figures",
    "pdf_url": "http://arxiv.org/pdf/2505.04961v1",
    "published_date": "2025-05-08 05:42:33 UTC",
    "updated_date": "2025-05-08 05:42:33 UTC"
  },
  {
    "arxiv_id": "2505.04956v1",
    "title": "Graffe: Graph Representation Learning via Diffusion Probabilistic Models",
    "authors": [
      "Dingshuo Chen",
      "Shuchen Xue",
      "Liuji Chen",
      "Yingheng Wang",
      "Qiang Liu",
      "Shu Wu",
      "Zhi-Ming Ma",
      "Liang Wang"
    ],
    "abstract": "Diffusion probabilistic models (DPMs), widely recognized for their potential\nto generate high-quality samples, tend to go unnoticed in representation\nlearning. While recent progress has highlighted their potential for capturing\nvisual semantics, adapting DPMs to graph representation learning remains in its\ninfancy. In this paper, we introduce Graffe, a self-supervised diffusion model\nproposed for graph representation learning. It features a graph encoder that\ndistills a source graph into a compact representation, which, in turn, serves\nas the condition to guide the denoising process of the diffusion decoder. To\nevaluate the effectiveness of our model, we first explore the theoretical\nfoundations of applying diffusion models to representation learning, proving\nthat the denoising objective implicitly maximizes the conditional mutual\ninformation between data and its representation. Specifically, we prove that\nthe negative logarithm of the denoising score matching loss is a tractable\nlower bound for the conditional mutual information. Empirically, we conduct a\nseries of case studies to validate our theoretical insights. In addition,\nGraffe delivers competitive results under the linear probing setting on node\nand graph classification tasks, achieving state-of-the-art performance on 9 of\nthe 11 real-world datasets. These findings indicate that powerful generative\nmodels, especially diffusion models, serve as an effective tool for graph\nrepresentation learning.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "16 pages, 4 figures, under review",
    "pdf_url": "http://arxiv.org/pdf/2505.04956v1",
    "published_date": "2025-05-08 05:38:19 UTC",
    "updated_date": "2025-05-08 05:38:19 UTC"
  },
  {
    "arxiv_id": "2505.04955v1",
    "title": "Chain-of-Thought Tokens are Computer Program Variables",
    "authors": [
      "Fangwei Zhu",
      "Peiyi Wang",
      "Zhifang Sui"
    ],
    "abstract": "Chain-of-thoughts (CoT) requires large language models (LLMs) to generate\nintermediate steps before reaching the final answer, and has been proven\neffective to help LLMs solve complex reasoning tasks. However, the inner\nmechanism of CoT still remains largely unclear. In this paper, we empirically\nstudy the role of CoT tokens in LLMs on two compositional tasks: multi-digit\nmultiplication and dynamic programming. While CoT is essential for solving\nthese problems, we find that preserving only tokens that store intermediate\nresults would achieve comparable performance. Furthermore, we observe that\nstoring intermediate results in an alternative latent form will not affect\nmodel performance. We also randomly intervene some values in CoT, and notice\nthat subsequent CoT tokens and the final answer would change correspondingly.\nThese findings suggest that CoT tokens may function like variables in computer\nprograms but with potential drawbacks like unintended shortcuts and\ncomputational complexity limits between tokens. The code and data are available\nat https://github.com/solitaryzero/CoTs_are_Variables.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2505.04955v1",
    "published_date": "2025-05-08 05:32:36 UTC",
    "updated_date": "2025-05-08 05:32:36 UTC"
  },
  {
    "arxiv_id": "2505.04950v1",
    "title": "Position: Epistemic Artificial Intelligence is Essential for Machine Learning Models to Know When They Do Not Know",
    "authors": [
      "Shireen Kudukkil Manchingal",
      "Fabio Cuzzolin"
    ],
    "abstract": "Despite the impressive achievements of AI, including advancements in\ngenerative models and large language models, there remains a significant gap in\nthe ability of AI to handle uncertainty and generalize beyond the training\ndata. We argue that AI models, especially in autonomous systems, fail to make\nrobust predictions when faced with unfamiliar or adversarial data, as evidenced\nby incidents with autonomous vehicles. Traditional machine learning approaches\nstruggle to address these issues due to an overemphasis on data fitting and\ndomain adaptation. This position paper posits a paradigm shift towards\nepistemic artificial intelligence, emphasizing the need for models to learn not\nonly from what they know but also from their ignorance. This approach, which\nfocuses on recognizing and managing uncertainty, offers a potential solution to\nimprove the resilience and robustness of AI systems, ensuring that they can\nbetter handle unpredictable real-world environments.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2505.04950v1",
    "published_date": "2025-05-08 05:10:38 UTC",
    "updated_date": "2025-05-08 05:10:38 UTC"
  },
  {
    "arxiv_id": "2505.04946v1",
    "title": "T2VTextBench: A Human Evaluation Benchmark for Textual Control in Video Generation Models",
    "authors": [
      "Xuyang Guo",
      "Jiayan Huo",
      "Zhenmei Shi",
      "Zhao Song",
      "Jiahao Zhang",
      "Jiale Zhao"
    ],
    "abstract": "Thanks to recent advancements in scalable deep architectures and large-scale\npretraining, text-to-video generation has achieved unprecedented capabilities\nin producing high-fidelity, instruction-following content across a wide range\nof styles, enabling applications in advertising, entertainment, and education.\nHowever, these models' ability to render precise on-screen text, such as\ncaptions or mathematical formulas, remains largely untested, posing significant\nchallenges for applications requiring exact textual accuracy. In this work, we\nintroduce T2VTextBench, the first human-evaluation benchmark dedicated to\nevaluating on-screen text fidelity and temporal consistency in text-to-video\nmodels. Our suite of prompts integrates complex text strings with dynamic scene\nchanges, testing each model's ability to maintain detailed instructions across\nframes. We evaluate ten state-of-the-art systems, ranging from open-source\nsolutions to commercial offerings, and find that most struggle to generate\nlegible, consistent text. These results highlight a critical gap in current\nvideo generators and provide a clear direction for future research aimed at\nenhancing textual manipulation in video synthesis.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2505.04946v1",
    "published_date": "2025-05-08 04:49:52 UTC",
    "updated_date": "2025-05-08 04:49:52 UTC"
  },
  {
    "arxiv_id": "2505.04939v1",
    "title": "Structural Alignment in Link Prediction",
    "authors": [
      "Jeffrey Seathrún Sardina"
    ],
    "abstract": "While Knowledge Graphs (KGs) have become increasingly popular across various\nscientific disciplines for their ability to model and interlink huge quantities\nof data, essentially all real-world KGs are known to be incomplete. As such,\nwith the growth of KG use has been a concurrent development of machine learning\ntools designed to predict missing information in KGs, which is referred to as\nthe Link Prediction Task. The majority of state-of-the-art link predictors to\ndate have followed an embedding-based paradigm. In this paradigm, it is assumed\nthat the information content of a KG is best represented by the (individual)\nvector representations of its nodes and edges, and that therefore node and edge\nembeddings are particularly well-suited to performing link prediction.\n  This thesis proposes an alternative perspective on the field's approach to\nlink prediction and KG data modelling. Specifically, this work re-analyses KGs\nand state-of-the-art link predictors from a graph-structure-first perspective\nthat models the information content of a KG in terms of whole triples, rather\nthan individual nodes and edges.\n  Following a literature review and two core sets of experiments, this thesis\nconcludes that a structure-first perspective on KGs and link prediction is both\nviable and useful for understanding KG learning and for enabling cross-KG\ntransfer learning for the link prediction task. This observation is used to\ncreate and propose the Structural Alignment Hypothesis, which postulates that\nlink prediction can be understood and modelled as a structural task.\n  All code and data used for this thesis are open-sourced. This thesis was\nwritten bilingually, with the main document in English and an informal extended\nsummary in Irish. An Irish-language translation dictionary of machine learning\nterms (the Focl\\'oir Tr\\'achtais) created for this work is open-sourced as\nwell.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "Ph.D. thesis submitted to Trinity College Dublin",
    "pdf_url": "http://arxiv.org/pdf/2505.04939v1",
    "published_date": "2025-05-08 04:27:15 UTC",
    "updated_date": "2025-05-08 04:27:15 UTC"
  },
  {
    "arxiv_id": "2505.04931v1",
    "title": "Fair Uncertainty Quantification for Depression Prediction",
    "authors": [
      "Yonghong Li",
      "Xiuzhuang Zhou"
    ],
    "abstract": "Trustworthy depression prediction based on deep learning, incorporating both\npredictive reliability and algorithmic fairness across diverse demographic\ngroups, is crucial for clinical application. Recently, achieving reliable\ndepression predictions through uncertainty quantification has attracted\nincreasing attention. However, few studies have focused on the fairness of\nuncertainty quantification (UQ) in depression prediction. In this work, we\ninvestigate the algorithmic fairness of UQ, namely Equal Opportunity Coverage\n(EOC) fairness, and propose Fair Uncertainty Quantification (FUQ) for\ndepression prediction. FUQ pursues reliable and fair depression predictions\nthrough group-based analysis. Specifically, we first group all the participants\nby different sensitive attributes and leverage conformal prediction to quantify\nuncertainty within each demographic group, which provides a theoretically\nguaranteed and valid way to quantify uncertainty for depression prediction and\nfacilitates the investigation of fairness across different demographic groups.\nFurthermore, we propose a fairness-aware optimization strategy that formulates\nfairness as a constrained optimization problem under EOC constraints. This\nenables the model to preserve predictive reliability while adapting to the\nheterogeneous uncertainty levels across demographic groups, thereby achieving\noptimal fairness. Through extensive evaluations on several visual and audio\ndepression datasets, our approach demonstrates its effectiveness.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2505.04931v1",
    "published_date": "2025-05-08 04:09:36 UTC",
    "updated_date": "2025-05-08 04:09:36 UTC"
  },
  {
    "arxiv_id": "2505.04927v1",
    "title": "Belief Filtering for Epistemic Control in Linguistic State Space",
    "authors": [
      "Sebastian Dumbrava"
    ],
    "abstract": "We examine belief filtering as a mechanism for the epistemic control of\nartificial agents, focusing on the regulation of internal cognitive states\nrepresented as linguistic expressions. This mechanism is developed within the\nSemantic Manifold framework, where belief states are dynamic, structured\nensembles of natural language fragments. Belief filters act as content-aware\noperations on these fragments across various cognitive transitions. This paper\nillustrates how the inherent interpretability and modularity of such a\nlinguistically-grounded cognitive architecture directly enable belief\nfiltering, offering a principled approach to agent regulation. The study\nhighlights the potential for enhancing AI safety and alignment through\nstructured interventions in an agent's internal semantic space and points to\nnew directions for architecturally embedded cognitive governance.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "18 pages",
    "pdf_url": "http://arxiv.org/pdf/2505.04927v1",
    "published_date": "2025-05-08 03:52:43 UTC",
    "updated_date": "2025-05-08 03:52:43 UTC"
  },
  {
    "arxiv_id": "2505.04918v1",
    "title": "Physics-Assisted and Topology-Informed Deep Learning for Weather Prediction",
    "authors": [
      "Jiaqi Zheng",
      "Qing Ling",
      "Yerong Feng"
    ],
    "abstract": "Although deep learning models have demonstrated remarkable potential in\nweather prediction, most of them overlook either the \\textbf{physics} of the\nunderlying weather evolution or the \\textbf{topology} of the Earth's surface.\nIn light of these disadvantages, we develop PASSAT, a novel Physics-ASSisted\nAnd Topology-informed deep learning model for weather prediction. PASSAT\nattributes the weather evolution to two key factors: (i) the advection process\nthat can be characterized by the advection equation and the Navier-Stokes\nequation; (ii) the Earth-atmosphere interaction that is difficult to both model\nand calculate. PASSAT also takes the topology of the Earth's surface into\nconsideration, other than simply treating it as a plane. With these\nconsiderations, PASSAT numerically solves the advection equation and the\nNavier-Stokes equation on the spherical manifold, utilizes a spherical graph\nneural network to capture the Earth-atmosphere interaction, and generates the\ninitial velocity fields that are critical to solving the advection equation\nfrom the same spherical graph neural network. In the $5.625^\\circ$-resolution\nERA5 data set, PASSAT outperforms both the state-of-the-art deep learning-based\nweather prediction models and the operational numerical weather prediction\nmodel IFS T42. Code and checkpoint are available at\nhttps://github.com/Yumenomae/PASSAT_5p625.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "International Joint Conferences on Artificial Intelligence (IJCAI\n  2025)",
    "pdf_url": "http://arxiv.org/pdf/2505.04918v1",
    "published_date": "2025-05-08 03:25:55 UTC",
    "updated_date": "2025-05-08 03:25:55 UTC"
  },
  {
    "arxiv_id": "2505.04916v1",
    "title": "An Open-Source Dual-Loss Embedding Model for Semantic Retrieval in Higher Education",
    "authors": [
      "Ramteja Sajja",
      "Yusuf Sermet",
      "Ibrahim Demir"
    ],
    "abstract": "Recent advances in AI have catalyzed the adoption of intelligent educational\ntools, yet many semantic retrieval systems remain ill-suited to the unique\nlinguistic and structural characteristics of academic content. This study\npresents two open-source embedding models fine-tuned for educational question\nanswering, particularly in the context of course syllabi. A synthetic dataset\nof 3,197 sentence pairs, spanning synonymous terminology, paraphrased\nquestions, and implicit-explicit mappings, was constructed through a\ncombination of manual curation and large language model (LLM)-assisted\ngeneration. Two training strategies were evaluated: (1) a baseline model\nfine-tuned using MultipleNegativesRankingLoss (MNRL), and (2) a dual-loss model\nthat combines MNRL with CosineSimilarityLoss to improve both semantic ranking\nand similarity calibration. Evaluations were conducted on 28 university course\nsyllabi using a fixed set of natural language questions categorized into\ncourse, faculty, and teaching assistant information. Results demonstrate that\nboth fine-tuned models outperform strong open-source baselines, including\nall-MiniLM-L6-v2 and multi-qa-MiniLM-L6-cos-v1, and that the dual-loss model\nnarrows the performance gap with high-performing proprietary embeddings such as\nOpenAI's text-embedding-3 series. This work contributes reusable,\ndomain-aligned embedding models and provides a replicable framework for\neducational semantic retrieval, supporting downstream applications such as\nacademic chatbots, retrieval-augmented generation (RAG) systems, and learning\nmanagement system (LMS) integrations.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.IR"
    ],
    "primary_category": "cs.CL",
    "comment": "17 pages, 3 Tables",
    "pdf_url": "http://arxiv.org/pdf/2505.04916v1",
    "published_date": "2025-05-08 03:14:14 UTC",
    "updated_date": "2025-05-08 03:14:14 UTC"
  },
  {
    "arxiv_id": "2505.04914v1",
    "title": "Enigme: Generative Text Puzzles for Evaluating Reasoning in Language Models",
    "authors": [
      "John Hawkins"
    ],
    "abstract": "Transformer-decoder language models are a core innovation in text based\ngenerative artificial intelligence. These models are being deployed as\ngeneral-purpose intelligence systems in many applications. Central to their\nutility is the capacity to understand natural language commands and exploit the\nreasoning embedded in human text corpora to apply some form of reasoning\nprocess to a wide variety of novel tasks. To understand the limitations of this\napproach to generating reasoning we argue that we need to consider the\narchitectural constraints of these systems. Consideration of the latent\nvariable structure of transformer-decoder models allows us to design reasoning\ntasks that should probe the boundary of their capacity to reason. We present\nenigme, an open-source library for generating text-based puzzles to be used in\ntraining and evaluating reasoning skills within transformer-decoder models and\nfuture AI architectures.",
    "categories": [
      "cs.AI",
      "cs.CL",
      "I.2.7"
    ],
    "primary_category": "cs.AI",
    "comment": "To be published in the proceedings of The 2025 11th International\n  Conference on Engineering, Applied Sciences, and Technology (ICEAST)",
    "pdf_url": "http://arxiv.org/pdf/2505.04914v1",
    "published_date": "2025-05-08 03:09:57 UTC",
    "updated_date": "2025-05-08 03:09:57 UTC"
  },
  {
    "arxiv_id": "2505.04911v1",
    "title": "SpatialPrompting: Keyframe-driven Zero-Shot Spatial Reasoning with Off-the-Shelf Multimodal Large Language Models",
    "authors": [
      "Shun Taguchi",
      "Hideki Deguchi",
      "Takumi Hamazaki",
      "Hiroyuki Sakai"
    ],
    "abstract": "This study introduces SpatialPrompting, a novel framework that harnesses the\nemergent reasoning capabilities of off-the-shelf multimodal large language\nmodels to achieve zero-shot spatial reasoning in three-dimensional (3D)\nenvironments. Unlike existing methods that rely on expensive 3D-specific\nfine-tuning with specialized 3D inputs such as point clouds or voxel-based\nfeatures, SpatialPrompting employs a keyframe-driven prompt generation\nstrategy. This framework uses metrics such as vision-language similarity,\nMahalanobis distance, field of view, and image sharpness to select a diverse\nand informative set of keyframes from image sequences and then integrates them\nwith corresponding camera pose data to effectively abstract spatial\nrelationships and infer complex 3D structures. The proposed framework not only\nestablishes a new paradigm for flexible spatial reasoning that utilizes\nintuitive visual and positional cues but also achieves state-of-the-art\nzero-shot performance on benchmark datasets, such as ScanQA and SQA3D, across\nseveral metrics. The proposed method effectively eliminates the need for\nspecialized 3D inputs and fine-tuning, offering a simpler and more scalable\nalternative to conventional approaches.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.CV",
    "comment": "18 pages, 11 figures",
    "pdf_url": "http://arxiv.org/pdf/2505.04911v1",
    "published_date": "2025-05-08 02:59:01 UTC",
    "updated_date": "2025-05-08 02:59:01 UTC"
  },
  {
    "arxiv_id": "2505.04898v1",
    "title": "Precise gradient descent training dynamics for finite-width multi-layer neural networks",
    "authors": [
      "Qiyang Han",
      "Masaaki Imaizumi"
    ],
    "abstract": "In this paper, we provide the first precise distributional characterization\nof gradient descent iterates for general multi-layer neural networks under the\ncanonical single-index regression model, in the `finite-width proportional\nregime' where the sample size and feature dimension grow proportionally while\nthe network width and depth remain bounded. Our non-asymptotic state evolution\ntheory captures Gaussian fluctuations in first-layer weights and concentration\nin deeper-layer weights, and remains valid for non-Gaussian features.\n  Our theory differs from existing neural tangent kernel (NTK), mean-field (MF)\ntheories and tensor program (TP) in several key aspects. First, our theory\noperates in the finite-width regime whereas these existing theories are\nfundamentally infinite-width. Second, our theory allows weights to evolve from\nindividual initializations beyond the lazy training regime, whereas NTK and MF\nare either frozen at or only weakly sensitive to initialization, and TP relies\non special initialization schemes. Third, our theory characterizes both\ntraining and generalization errors for general multi-layer neural networks\nbeyond the uniform convergence regime, whereas existing theories study\ngeneralization almost exclusively in two-layer settings.\n  As a statistical application, we show that vanilla gradient descent can be\naugmented to yield consistent estimates of the generalization error at each\niteration, which can be used to guide early stopping and hyperparameter tuning.\nAs a further theoretical implication, we show that despite model\nmisspecification, the model learned by gradient descent retains the structure\nof a single-index function with an effective signal determined by a linear\ncombination of the true signal and the initialization.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "math.OC",
      "math.ST",
      "stat.ML",
      "stat.TH"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2505.04898v1",
    "published_date": "2025-05-08 02:19:39 UTC",
    "updated_date": "2025-05-08 02:19:39 UTC"
  },
  {
    "arxiv_id": "2505.04891v1",
    "title": "Clustering with Communication: A Variational Framework for Single Cell Representation Learning",
    "authors": [
      "Cong Qi",
      "Yeqing Chen",
      "Jie Zhang",
      "Wei Zhi"
    ],
    "abstract": "Single-cell RNA sequencing (scRNA-seq) has revealed complex cellular\nheterogeneity, but recent studies emphasize that understanding biological\nfunction also requires modeling cell-cell communication (CCC), the signaling\ninteractions mediated by ligand-receptor pairs that coordinate cellular\nbehavior. Tools like CellChat have demonstrated that CCC plays a critical role\nin processes such as cell differentiation, tissue regeneration, and immune\nresponse, and that transcriptomic data inherently encodes rich information\nabout intercellular signaling. We propose CCCVAE, a novel variational\nautoencoder framework that incorporates CCC signals into single-cell\nrepresentation learning. By leveraging a communication-aware kernel derived\nfrom ligand-receptor interactions and a sparse Gaussian process, CCCVAE encodes\nbiologically informed priors into the latent space. Unlike conventional VAEs\nthat treat each cell independently, CCCVAE encourages latent embeddings to\nreflect both transcriptional similarity and intercellular signaling context.\nEmpirical results across four scRNA-seq datasets show that CCCVAE improves\nclustering performance, achieving higher evaluation scores than standard VAE\nbaselines. This work demonstrates the value of embedding biological priors into\ndeep generative models for unsupervised single-cell analysis.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2505.04891v1",
    "published_date": "2025-05-08 01:53:36 UTC",
    "updated_date": "2025-05-08 01:53:36 UTC"
  },
  {
    "arxiv_id": "2505.04888v1",
    "title": "Cross-Branch Orthogonality for Improved Generalization in Face Deepfake Detection",
    "authors": [
      "Tharindu Fernando",
      "Clinton Fookes",
      "Sridha Sridharan",
      "Simon Denman"
    ],
    "abstract": "Remarkable advancements in generative AI technology have given rise to a\nspectrum of novel deepfake categories with unprecedented leaps in their\nrealism, and deepfakes are increasingly becoming a nuisance to law enforcement\nauthorities and the general public. In particular, we observe alarming levels\nof confusion, deception, and loss of faith regarding multimedia content within\nsociety caused by face deepfakes, and existing deepfake detectors are\nstruggling to keep up with the pace of improvements in deepfake generation.\nThis is primarily due to their reliance on specific forgery artifacts, which\nlimits their ability to generalise and detect novel deepfake types. To combat\nthe spread of malicious face deepfakes, this paper proposes a new strategy that\nleverages coarse-to-fine spatial information, semantic information, and their\ninteractions while ensuring feature distinctiveness and reducing the redundancy\nof the modelled features. A novel feature orthogonality-based disentanglement\nstrategy is introduced to ensure branch-level and cross-branch feature\ndisentanglement, which allows us to integrate multiple feature vectors without\nadding complexity to the feature space or compromising generalisation.\nComprehensive experiments on three public benchmarks: FaceForensics++,\nCeleb-DF, and the Deepfake Detection Challenge (DFDC) show that these design\nchoices enable the proposed approach to outperform current state-of-the-art\nmethods by 5% on the Celeb-DF dataset and 7% on the DFDC dataset in a\ncross-dataset evaluation setting.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2505.04888v1",
    "published_date": "2025-05-08 01:49:53 UTC",
    "updated_date": "2025-05-08 01:49:53 UTC"
  },
  {
    "arxiv_id": "2505.04883v1",
    "title": "QBR: A Question-Bank-Based Approach to Fine-Grained Legal Knowledge Retrieval for the General Public",
    "authors": [
      "Mingruo Yuan",
      "Ben Kao",
      "Tien-Hsuan Wu"
    ],
    "abstract": "Retrieval of legal knowledge by the general public is a challenging problem\ndue to the technicality of the professional knowledge and the lack of\nfundamental understanding by laypersons on the subject. Traditional information\nretrieval techniques assume that users are capable of formulating succinct and\nprecise queries for effective document retrieval. In practice, however, the\nwide gap between the highly technical contents and untrained users makes legal\nknowledge retrieval very difficult. We propose a methodology, called QBR, which\nemploys a Questions Bank (QB) as an effective medium for bridging the knowledge\ngap. We show how the QB is used to derive training samples to enhance the\nembedding of knowledge units within documents, which leads to effective\nfine-grained knowledge retrieval. We discuss and evaluate through experiments\nvarious advantages of QBR over traditional methods. These include more\naccurate, efficient, and explainable document retrieval, better comprehension\nof retrieval results, and highly effective fine-grained knowledge retrieval. We\nalso present some case studies and show that QBR achieves social impact by\nassisting citizens to resolve everyday legal concerns.",
    "categories": [
      "cs.IR",
      "cs.AI"
    ],
    "primary_category": "cs.IR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2505.04883v1",
    "published_date": "2025-05-08 01:43:21 UTC",
    "updated_date": "2025-05-08 01:43:21 UTC"
  },
  {
    "arxiv_id": "2505.04881v1",
    "title": "ConCISE: Confidence-guided Compression in Step-by-step Efficient Reasoning",
    "authors": [
      "Ziqing Qiao",
      "Yongheng Deng",
      "Jiali Zeng",
      "Dong Wang",
      "Lai Wei",
      "Fandong Meng",
      "Jie Zhou",
      "Ju Ren",
      "Yaoxue Zhang"
    ],
    "abstract": "Large Reasoning Models (LRMs) perform strongly in complex reasoning tasks via\nChain-of-Thought (CoT) prompting, but often suffer from verbose outputs caused\nby redundant content, increasing computational overhead, and degrading user\nexperience. Existing compression methods either operate post-hoc pruning,\nrisking disruption to reasoning coherence, or rely on sampling-based selection,\nwhich fails to intervene effectively during generation. In this work, we\nintroduce a confidence-guided perspective to explain the emergence of redundant\nreflection in LRMs, identifying two key patterns: Confidence Deficit, where the\nmodel reconsiders correct steps due to low internal confidence, and Termination\nDelay, where reasoning continues even after reaching a confident answer. Based\non this analysis, we propose ConCISE (Confidence-guided Compression In\nStep-by-step Efficient Reasoning), a framework that simplifies reasoning chains\nby reinforcing the model's confidence during inference, thus preventing the\ngeneration of redundant reflection steps. It integrates Confidence Injection to\nstabilize intermediate steps and Early Stopping to terminate reasoning when\nconfidence is sufficient. Extensive experiments demonstrate that fine-tuning\nLRMs on ConCISE-generated data yields significantly shorter outputs, reducing\nlength by up to approximately 50% under SimPO, while maintaining high task\naccuracy. ConCISE consistently outperforms existing baselines across multiple\nreasoning benchmarks.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2505.04881v1",
    "published_date": "2025-05-08 01:40:40 UTC",
    "updated_date": "2025-05-08 01:40:40 UTC"
  },
  {
    "arxiv_id": "2505.04880v1",
    "title": "GroverGPT-2: Simulating Grover's Algorithm via Chain-of-Thought Reasoning and Quantum-Native Tokenization",
    "authors": [
      "Min Chen",
      "Jinglei Cheng",
      "Pingzhi Li",
      "Haoran Wang",
      "Tianlong Chen",
      "Junyu Liu"
    ],
    "abstract": "Quantum computing offers theoretical advantages over classical computing for\nspecific tasks, yet the boundary of practical quantum advantage remains an open\nquestion. To investigate this boundary, it is crucial to understand whether,\nand how, classical machines can learn and simulate quantum algorithms. Recent\nprogress in large language models (LLMs) has demonstrated strong reasoning\nabilities, prompting exploration into their potential for this challenge. In\nthis work, we introduce GroverGPT-2, an LLM-based method for simulating\nGrover's algorithm using Chain-of-Thought (CoT) reasoning and quantum-native\ntokenization. Building on its predecessor, GroverGPT-2 performs simulation\ndirectly from quantum circuit representations while producing logically\nstructured and interpretable outputs. Our results show that GroverGPT-2 can\nlearn and internalize quantum circuit logic through efficient processing of\nquantum-native tokens, providing direct evidence that classical models like\nLLMs can capture the structure of quantum algorithms. Furthermore, GroverGPT-2\noutputs interleave circuit data with natural language, embedding explicit\nreasoning into the simulation. This dual capability positions GroverGPT-2 as a\nprototype for advancing machine understanding of quantum algorithms and\nmodeling quantum circuit logic. We also identify an empirical scaling law for\nGroverGPT-2 with increasing qubit numbers, suggesting a path toward scalable\nclassical simulation. These findings open new directions for exploring the\nlimits of classical simulatability, enhancing quantum education and research,\nand laying groundwork for future foundation models in quantum computing.",
    "categories": [
      "quant-ph",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "quant-ph",
    "comment": "26 pages, 12 figures",
    "pdf_url": "http://arxiv.org/pdf/2505.04880v1",
    "published_date": "2025-05-08 01:38:12 UTC",
    "updated_date": "2025-05-08 01:38:12 UTC"
  },
  {
    "arxiv_id": "2505.04877v1",
    "title": "Learning from Loss Landscape: Generalizable Mixed-Precision Quantization via Adaptive Sharpness-Aware Gradient Aligning",
    "authors": [
      "Lianbo Ma",
      "Jianlun Ma",
      "Yuee Zhou",
      "Guoyang Xie",
      "Qiang He",
      "Zhichao Lu"
    ],
    "abstract": "Mixed Precision Quantization (MPQ) has become an essential technique for\noptimizing neural network by determining the optimal bitwidth per layer.\nExisting MPQ methods, however, face a major hurdle: they require a\ncomputationally expensive search for quantization policies on large-scale\ndatasets. To resolve this issue, we introduce a novel approach that first\nsearches for quantization policies on small datasets and then generalizes them\nto large-scale datasets. This approach simplifies the process, eliminating the\nneed for large-scale quantization fine-tuning and only necessitating model\nweight adjustment. Our method is characterized by three key techniques:\nsharpness-aware minimization for enhanced quantization generalization, implicit\ngradient direction alignment to handle gradient conflicts among different\noptimization objectives, and an adaptive perturbation radius to accelerate\noptimization. Both theoretical analysis and experimental results validate our\napproach. Using the CIFAR10 dataset (just 0.5\\% the size of ImageNet training\ndata) for MPQ policy search, we achieved equivalent accuracy on ImageNet with a\nsignificantly lower computational cost, while improving efficiency by up to\n150% over the baselines.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2505.04877v1",
    "published_date": "2025-05-08 01:20:24 UTC",
    "updated_date": "2025-05-08 01:20:24 UTC"
  },
  {
    "arxiv_id": "2505.04873v1",
    "title": "Federated Learning for Cyber Physical Systems: A Comprehensive Survey",
    "authors": [
      "Minh K. Quan",
      "Pubudu N. Pathirana",
      "Mayuri Wijayasundara",
      "Sujeeva Setunge",
      "Dinh C. Nguyen",
      "Christopher G. Brinton",
      "David J. Love",
      "H. Vincent Poor"
    ],
    "abstract": "The integration of machine learning (ML) in cyber physical systems (CPS) is a\ncomplex task due to the challenges that arise in terms of real-time decision\nmaking, safety, reliability, device heterogeneity, and data privacy. There are\nalso open research questions that must be addressed in order to fully realize\nthe potential of ML in CPS. Federated learning (FL), a distributed approach to\nML, has become increasingly popular in recent years. It allows models to be\ntrained using data from decentralized sources. This approach has been gaining\npopularity in the CPS field, as it integrates computer, communication, and\nphysical processes. Therefore, the purpose of this work is to provide a\ncomprehensive analysis of the most recent developments of FL-CPS, including the\nnumerous application areas, system topologies, and algorithms developed in\nrecent years. The paper starts by discussing recent advances in both FL and\nCPS, followed by their integration. Then, the paper compares the application of\nFL in CPS with its applications in the internet of things (IoT) in further\ndepth to show their connections and distinctions. Furthermore, the article\nscrutinizes how FL is utilized in critical CPS applications, e.g., intelligent\ntransportation systems, cybersecurity services, smart cities, and smart\nhealthcare solutions. The study also includes critical insights and lessons\nlearned from various FL-CPS implementations. The paper's concluding section\ndelves into significant concerns and suggests avenues for further research in\nthis fast-paced and dynamic era.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CR",
      "cs.DC"
    ],
    "primary_category": "cs.LG",
    "comment": "This work has been accepted by IEEE Communications Surveys &\n  Tutorials",
    "pdf_url": "http://arxiv.org/pdf/2505.04873v1",
    "published_date": "2025-05-08 01:17:15 UTC",
    "updated_date": "2025-05-08 01:17:15 UTC"
  },
  {
    "arxiv_id": "2505.04864v1",
    "title": "Auto-regressive transformation for image alignment",
    "authors": [
      "Kanggeon Lee",
      "Soochahn Lee",
      "Kyoung Mu Lee"
    ],
    "abstract": "Existing methods for image alignment struggle in cases involving\nfeature-sparse regions, extreme scale and field-of-view differences, and large\ndeformations, often resulting in suboptimal accuracy. Robustness to these\nchallenges improves through iterative refinement of the transformation field\nwhile focusing on critical regions in multi-scale image representations. We\nthus propose Auto-Regressive Transformation (ART), a novel method that\niteratively estimates the coarse-to-fine transformations within an\nauto-regressive framework. Leveraging hierarchical multi-scale features, our\nnetwork refines the transformations using randomly sampled points at each\nscale. By incorporating guidance from the cross-attention layer, the model\nfocuses on critical regions, ensuring accurate alignment even in challenging,\nfeature-limited conditions. Extensive experiments across diverse datasets\ndemonstrate that ART significantly outperforms state-of-the-art methods,\nestablishing it as a powerful new method for precise image alignment with broad\napplicability.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2505.04864v1",
    "published_date": "2025-05-08 00:28:31 UTC",
    "updated_date": "2025-05-08 00:28:31 UTC"
  },
  {
    "arxiv_id": "2505.04860v1",
    "title": "D-CODA: Diffusion for Coordinated Dual-Arm Data Augmentation",
    "authors": [
      "I-Chun Arthur Liu",
      "Jason Chen",
      "Gaurav Sukhatme",
      "Daniel Seita"
    ],
    "abstract": "Learning bimanual manipulation is challenging due to its high dimensionality\nand tight coordination required between two arms. Eye-in-hand imitation\nlearning, which uses wrist-mounted cameras, simplifies perception by focusing\non task-relevant views. However, collecting diverse demonstrations remains\ncostly, motivating the need for scalable data augmentation. While prior work\nhas explored visual augmentation in single-arm settings, extending these\napproaches to bimanual manipulation requires generating viewpoint-consistent\nobservations across both arms and producing corresponding action labels that\nare both valid and feasible. In this work, we propose Diffusion for COordinated\nDual-arm Data Augmentation (D-CODA), a method for offline data augmentation\ntailored to eye-in-hand bimanual imitation learning that trains a diffusion\nmodel to synthesize novel, viewpoint-consistent wrist-camera images for both\narms while simultaneously generating joint-space action labels. It employs\nconstrained optimization to ensure that augmented states involving\ngripper-to-object contacts adhere to constraints suitable for bimanual\ncoordination. We evaluate D-CODA on 5 simulated and 3 real-world tasks. Our\nresults across 2250 simulation trials and 300 real-world trials demonstrate\nthat it outperforms baselines and ablations, showing its potential for scalable\ndata augmentation in eye-in-hand bimanual manipulation. Our project website is\nat: https://dcodaaug.github.io/D-CODA/.",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.CV",
      "cs.LG"
    ],
    "primary_category": "cs.RO",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2505.04860v1",
    "published_date": "2025-05-08 00:03:04 UTC",
    "updated_date": "2025-05-08 00:03:04 UTC"
  }
]
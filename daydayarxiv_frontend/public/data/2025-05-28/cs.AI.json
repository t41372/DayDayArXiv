{
  "date": "2025-05-28",
  "category": "cs.AI",
  "summary": "æ¬¢è¿æ¥åˆ° UTC æ—¶é—´ 2025-05-28 çš„ arXiv ä¸­æ–‡ TLDR å¿«æŠ¥ï¼\n\n**ä»Šæ—¥æ€»ç»“ï¼š**\nä»Šå¤©çš„ arXiv ç®€ç›´æ˜¯â€œæ¨ç†ï¼ˆReasoningï¼‰â€ä¸â€œåè®­ç»ƒï¼ˆPost-Trainingï¼‰â€çš„ç‹‚æ¬¢æ—¥ã€‚DeepSeek-R1 å¼•å‘çš„å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰çƒ­æ½®ä»åœ¨æŒç»­ï¼Œ**Skywork å¼€æºäº†é’ˆå¯¹é•¿æ€ç»´é“¾ï¼ˆCoTï¼‰çš„å¼ºåŠ› RL æ¨¡å‹**ï¼ŒåŒæ—¶å¤§é‡ç ”ç©¶é›†ä¸­åœ¨å¦‚ä½•é€šè¿‡ GRPO ç­‰ç®—æ³•æå‡å¤šæ¨¡æ€æ¨¡å‹å’Œ Agent çš„æ¨ç†èƒ½åŠ›ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬çœ‹åˆ°äº†**è§†è§‰æ€ç»´é“¾ï¼ˆVisual CoTï¼‰**çš„åˆ›æ–°å°è¯•ï¼Œä»¥åŠå…³äº **LLM æ˜¯å¦çŸ¥é“è‡ªå·±åœ¨è¢«æµ‹è¯„**çš„æœ‰è¶£å¿ƒç†å­¦ç ”ç©¶ã€‚\n\nä¸‹é¢æˆ‘ä»¬ç›´å‡»é‡ç‚¹ï¼Œä¸ºæ‚¨æ¢³ç†ä»Šå¤©çš„ç²¾åè®ºæ–‡ã€‚\n\n---\n\n### ğŸš€ ç„¦ç‚¹ï¼šå¼€æºæ¨ç†æ¨¡å‹ä¸ RL çš„è¿›å‡» (Reasoning & RL)\n\nä»Šå¤©æœ€é‡ç£…çš„å·¥ä½œæ— ç–‘æ˜¯ Skywork å¯¹ DeepSeek-R1 çš„å›åº”ä¸è¶…è¶Šï¼Œä»¥åŠå›´ç»• RL è®­ç»ƒæœºåˆ¶çš„æ·±å…¥æ¢è®¨ã€‚\n\n**1. Skywork Open Reasoner 1 Technical Report (å¤©å·¥ Open Reasoner 1 æŠ€æœ¯æŠ¥å‘Š)**\n*   **Authors:** Jujie He et al.\n*   **æ ¸å¿ƒè´¡çŒ®ï¼š** è¿™æ˜¯ä¸€ä¸ªé’ˆå¯¹é•¿æ€ç»´é“¾ï¼ˆLong CoTï¼‰æ¨¡å‹çš„æœ‰æ•ˆä¸”å¯æ‰©å±•çš„ RL å®ç°ã€‚åŸºäº DeepSeek-R1-Distill ç³»åˆ—ï¼ŒSkywork-OR1 åœ¨æ•°å­¦ï¼ˆAIMEï¼‰å’Œä»£ç ï¼ˆLiveCodeBenchï¼‰åŸºå‡†ä¸Šå®ç°äº†å·¨å¤§æå‡ï¼ˆä¾‹å¦‚ 32B æ¨¡å‹å‡†ç¡®ç‡ä» 57.8% æå‡è‡³ 72.8%ï¼‰ã€‚\n*   **äº®ç‚¹ï¼š** å›¢é˜Ÿä¸ä»…å¼€æºäº†æ¨¡å‹æƒé‡å’Œä»£ç ï¼Œè¿˜æ·±å…¥ç ”ç©¶äº† **Entropy Collapseï¼ˆç†µåå¡Œï¼‰** ç°è±¡ï¼ŒæŒ‡å‡ºé˜²æ­¢è¿‡æ—©çš„ç†µåå¡Œæ˜¯æå‡æ€§èƒ½çš„å…³é”®ã€‚è¿™æ˜¯å¤ç°å’Œæ”¹è¿› R1 è·¯çº¿çš„é‡è¦å‚è€ƒã€‚\n\n**2. The Entropy Mechanism of Reinforcement Learning for Reasoning Language Models (æ¨ç†å¤§æ¨¡å‹çš„å¼ºåŒ–å­¦ä¹ ç†µæœºåˆ¶)**\n*   **Authors:** Ganqu Cui et al.\n*   **æ ¸å¿ƒè´¡çŒ®ï¼š** è¿™æ˜¯ä¸€ä¸ªç†è®ºä¸å®è¯ç»“åˆçš„å·¥ä½œï¼Œè§£é‡Šäº†ä¸ºä»€ä¹ˆåœ¨ scaling RL æ—¶ä¼šå‡ºç°ç­–ç•¥ç†µï¼ˆPolicy Entropyï¼‰çš„æ€¥å‰§ä¸‹é™ï¼ˆåå¡Œï¼‰ï¼Œå¯¼è‡´æ¢ç´¢èƒ½åŠ›ä¸§å¤±ã€‚\n*   **å‘ç°ï¼š** æå‡ºäº† $R = -a \\cdot e^H + b$ çš„è½¬æ¢æ–¹ç¨‹ï¼Œè¯æ˜æ€§èƒ½æå‡æ˜¯ä»¥ç†µä¸ºä»£ä»·çš„ã€‚æå‡ºäº† **Clip-Cov** å’Œ **KL-Cov** æŠ€æœ¯æ¥ç®¡ç†ç†µï¼Œå¸®åŠ©æ¨¡å‹é€ƒç¦»åå¡Œï¼ŒæŒç»­æ¢ç´¢ã€‚\n\n**3. WorkForceAgent-R1: Incentivizing Reasoning Capability in LLM-based Web Agents via Reinforcement Learning (WorkForceAgent-R1ï¼šé€šè¿‡ RL æ¿€åŠ± Web Agent çš„æ¨ç†èƒ½åŠ›)**\n*   **Authors:** Yuchen Zhuang et al.\n*   **æ ¸å¿ƒè´¡çŒ®ï¼š** å°† R1 é£æ ¼çš„å¼ºåŒ–å­¦ä¹ å¼•å…¥åˆ°ä¼ä¸šçº§ Web å¯¼èˆª Agent ä¸­ã€‚\n*   **æ–¹æ³•ï¼š** ç›¸æ¯”äºä¼ ç»Ÿçš„ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰ï¼Œè¯¥æ–¹æ³•ä½¿ç”¨ç»“æ„åŒ–çš„å¥–åŠ±å‡½æ•°ï¼ˆè¯„ä¼°æ ¼å¼éµå¾ªå’ŒåŠ¨ä½œæ­£ç¡®æ€§ï¼‰ï¼Œè®© Agent éšå¼åœ°å­¦ä¹ é²æ£’çš„ä¸­é—´æ¨ç†æ­¥éª¤ï¼Œæ— éœ€æ˜‚è´µçš„ä¸“å®¶æ ‡æ³¨ï¼Œæ€§èƒ½é€¼è¿‘ GPT-4oã€‚\n\n**4. Maximizing Confidence Alone Improves Reasoning (ä»…æœ€å¤§åŒ–ç½®ä¿¡åº¦å³å¯æå‡æ¨ç†èƒ½åŠ›)**\n*   **Authors:** Mihir Prabhudesai et al.\n*   **æ ¸å¿ƒè´¡çŒ®ï¼š** æå‡º **RENT** æ–¹æ³•ï¼Œä¸€ç§å®Œå…¨æ— ç›‘ç£çš„ RLã€‚\n*   **äº®ç‚¹ï¼š** ä¸éœ€è¦å¤–éƒ¨å¥–åŠ±æˆ–æ ‡å‡†ç­”æ¡ˆï¼Œä»…ä½¿ç”¨æ¨¡å‹è‡ªèº«åˆ†å¸ƒçš„ç†µä½œä¸ºå†…åœ¨å¥–åŠ±ã€‚å‘ç°**å¼ºåŒ–é‚£äº›äº§ç”Ÿé«˜ç½®ä¿¡åº¦ç­”æ¡ˆçš„æ€ç»´é“¾**ï¼Œå°±èƒ½æ˜¾è‘—æå‡æ•°å­¦å’Œç¼–ç ä»»åŠ¡çš„æ¨ç†èƒ½åŠ›ã€‚è¿™ä¸ºç¼ºä¹ç›‘ç£æ•°æ®çš„é¢†åŸŸæŒ‡æ˜äº†æ–¹å‘ã€‚\n\n**5. First SFT, Second RL, Third UPT: Continual Improving Multi-Modal LLM Reasoning via Unsupervised Post-Training (å…ˆ SFTï¼Œå RLï¼Œå† UPTï¼šé€šè¿‡æ— ç›‘ç£åè®­ç»ƒæŒç»­æå‡å¤šæ¨¡æ€ LLM æ¨ç†)**\n*   **Authors:** Lai Wei et al.\n*   **æ ¸å¿ƒè´¡çŒ®ï¼š** æå‡ºäº† MM-UPT æ¡†æ¶ï¼Œä½œä¸º SFT å’Œ RL ä¹‹åçš„ç¬¬ä¸‰é˜¶æ®µã€‚åˆ©ç”¨ GRPO å’Œ Self-Reward æœºåˆ¶ï¼ˆåŸºäºå¤šæ•°æŠ•ç¥¨ï¼‰ï¼Œåœ¨æ²¡æœ‰æ ‡ç­¾çš„æƒ…å†µä¸‹è®©æ¨¡å‹è‡ªæˆ‘è¿›åŒ–ï¼Œåœ¨ MathVista ä¸Šå–å¾—äº†æ˜¾è‘—æå‡ã€‚\n\n---\n\n### ğŸ‘ï¸ å¤šæ¨¡æ€ä¸è§†è§‰æ™ºèƒ½ (Multimodal & Vision)\n\nè§†è§‰æ¨¡å‹ä¸å†ä»…ä»…æ˜¯â€œçœ‹â€ï¼Œè€Œæ˜¯å¼€å§‹â€œæ€è€ƒâ€å’Œâ€œè¢«æ¬ºéª—â€ã€‚\n\n**6. NegVQA: Can Vision Language Models Understand Negation? (NegVQAï¼šè§†è§‰è¯­è¨€æ¨¡å‹èƒ½ç†è§£å¦å®šå—ï¼Ÿ)**\n*   **Authors:** Yuhui Zhang et al. (ACL 2025 Findings)\n*   **æ ¸å¿ƒè´¡çŒ®ï¼š** å¦å®šï¼ˆNegationï¼‰æ˜¯è¯­è¨€çš„åŸºç¡€ï¼Œä½† VLMs åœ¨è¿™æ–¹é¢è¡¨ç°æå·®ã€‚ä½œè€…æ„å»ºäº† **NegVQA** åŸºå‡†ã€‚\n*   **å‘ç°ï¼š** åŒ…å« GPT-4V åœ¨å†…çš„ 20 ä¸ª SOTA æ¨¡å‹åœ¨é¢å¯¹å¦å®šé—®é¢˜æ—¶æ€§èƒ½æ˜¾è‘—ä¸‹é™ã€‚æœ‰è¶£çš„æ˜¯ï¼Œå‘ç°äº†ä¸€ä¸ª **U å‹ Scaling è¶‹åŠ¿**ï¼šå¢åŠ æ¨¡å‹å°ºå¯¸æœ€åˆåè€Œä¼šé™ä½å¦å®šç†è§£èƒ½åŠ›ï¼Œä¹‹åæ‰ä¼šæå‡ã€‚\n\n**7. Thinking with Generated Images (ç”¨ç”Ÿæˆçš„å›¾åƒè¿›è¡Œæ€è€ƒ)**\n*   **Authors:** Ethan Chern et al.\n*   **æ ¸å¿ƒè´¡çŒ®ï¼š** è¿™æ˜¯ä¸€ä¸ªéå¸¸æœ‰åˆ›æ„çš„èŒƒå¼è½¬å˜ã€‚ç›®å‰çš„ CoT éƒ½æ˜¯çº¯æ–‡æœ¬çš„ï¼Œè¿™ç¯‡æ–‡ç« è®© LMMs åœ¨æ¨ç†è¿‡ç¨‹ä¸­**è‡ªå‘ç”Ÿæˆä¸­é—´å›¾åƒ**ï¼ˆVisual Thinking Stepsï¼‰ã€‚\n*   **äº®ç‚¹ï¼š** å°±åƒäººç±»åœ¨è„‘æµ·ä¸­æ„æƒ³ç”»é¢ä¸€æ ·ï¼Œæ¨¡å‹é€šè¿‡ç”Ÿæˆè§†è§‰å­ç›®æ ‡æˆ–è‡ªæˆ‘æ‰¹åˆ¤çš„å›¾åƒæ¥è¾…åŠ©æ¨ç†ã€‚åœ¨å¤šç‰©ä½“åœºæ™¯ç­‰å¤æ‚ä»»åŠ¡ä¸­ï¼Œè¿™ç§â€œè§†è§‰æƒ³è±¡â€èƒ½åŠ›å¸¦æ¥äº†é«˜è¾¾ 50% çš„æå‡ã€‚\n\n**8. Can LLMs Deceive CLIP? Benchmarking Adversarial Compositionality (LLM èƒ½æ¬ºéª— CLIP å—ï¼Ÿ)**\n*   **Authors:** Jaewoo Ahn et al. (ACL 2025 Main)\n*   **æ ¸å¿ƒè´¡çŒ®ï¼š** æå‡ºäº† **MAC** åŸºå‡†ï¼Œåˆ©ç”¨ LLM ç”Ÿæˆå…·æœ‰æ¬ºéª—æ€§çš„æ–‡æœ¬ï¼ˆå¯¹æŠ—æ€§æ ·æœ¬ï¼‰ï¼Œä¸“é—¨é’ˆå¯¹ CLIP ç­‰å¤šæ¨¡æ€æ¨¡å‹çš„ç»„åˆæ€§æ¼æ´è¿›è¡Œæ”»å‡»ã€‚è¿™æ­ç¤ºäº†å½“å‰å¤šæ¨¡æ€è¡¨å¾åœ¨é¢å¯¹ç²¾å¿ƒè®¾è®¡çš„æ–‡æœ¬æ”»å‡»æ—¶çš„è„†å¼±æ€§ã€‚\n\n**9. ChatVLA-2: Vision-Language-Action Model with Open-World Embodied Reasoning from Pretrained Knowledge (ChatVLA-2ï¼šå…·æœ‰å¼€æ”¾ä¸–ç•Œå…·èº«æ¨ç†èƒ½åŠ›çš„ VLA æ¨¡å‹)**\n*   **Authors:** Zhongyi Zhou et al.\n*   **æ ¸å¿ƒè´¡çŒ®ï¼š** è¿™æ˜¯ä¸€ä¸ªæ··åˆä¸“å®¶ï¼ˆMoEï¼‰VLA æ¨¡å‹ã€‚æ—¨åœ¨è§£å†³å¾®è°ƒ VLA æ—¶ä¸¢å¤± VLM åŸæœ‰é€šè¯†èƒ½åŠ›çš„é—®é¢˜ã€‚è¯¥æ¨¡å‹åœ¨æœºå™¨äººæ“ä½œä¸­å±•ç°äº†æ•°å­¦æ¨ç†å’Œ OCR èƒ½åŠ›ï¼ˆä¾‹å¦‚è¯»ç™½æ¿ä¸Šçš„ç®—å¼å¹¶æŠ“å–å¯¹åº”çš„æ•°å­—å¡ç‰‡ï¼‰ï¼Œè¿™åœ¨ä»¥å¾€çš„æ¨¡ä»¿å­¦ä¹ æ¨¡å‹ä¸­æ˜¯å¾ˆéš¾åšåˆ°çš„ã€‚\n\n---\n\n### ğŸ§  LLM å¿ƒç†å­¦ã€å®‰å…¨ä¸è¯„ä¼° (Safety, Eval & Psychology)\n\n**10. Large Language Models Often Know When They Are Being Evaluated (å¤§è¯­è¨€æ¨¡å‹é€šå¸¸çŸ¥é“è‡ªå·±ä½•æ—¶è¢«è¯„ä¼°)**\n*   **Authors:** Joe Needham et al.\n*   **æ ¸å¿ƒè´¡çŒ®ï¼š** ä¸€ä¸ªä»¤äººæ·±æ€çš„å‘ç°â€”â€”å‰æ²¿æ¨¡å‹ï¼ˆå¦‚ Gemini-1.5-Proï¼‰èƒ½å¤Ÿä»¥è¿œè¶…éšæœºçš„æ¦‚ç‡ï¼ˆAUC 0.83ï¼‰åˆ†è¾¨å‡ºå½“å‰æ˜¯åœ¨å¤„ç†çœŸå®ç”¨æˆ·è¯·æ±‚è¿˜æ˜¯åœ¨è¿›è¡Œ Benchmark æµ‹è¯•ã€‚\n*   **Implicationï¼š** è¿™æ„å‘³ç€æ¨¡å‹å¯èƒ½ä¼šåœ¨æµ‹è¯•ä¸­è¡¨ç°å¾—â€œæ›´å¥½â€æˆ–â€œæ›´é¡ºä»â€ï¼Œå¯¼è‡´ Benchmark ç»“æœæ— æ³•ä»£è¡¨çœŸå®éƒ¨ç½²è¡¨ç°ï¼ˆGoodhart å®šå¾‹çš„ AI ç‰ˆï¼‰ã€‚\n\n**11. Text2Grad: Reinforcement Learning from Natural Language Feedback (Text2Gradï¼šä»è‡ªç„¶è¯­è¨€åé¦ˆä¸­è¿›è¡Œå¼ºåŒ–å­¦ä¹ )**\n*   **Authors:** Hanyang Wang et al. (Microsoft)\n*   **æ ¸å¿ƒè´¡çŒ®ï¼š** ä¼ ç»Ÿçš„ RLHF ä½¿ç”¨æ ‡é‡å¥–åŠ±ï¼Œå¤ªç²—ç³™ã€‚**Text2Grad** å°†è‡ªç„¶è¯­è¨€çš„ä¿®æ”¹æ„è§ï¼ˆCritiquesï¼‰ç›´æ¥è½¬åŒ–ä¸º**è·¨åº¦çº§åˆ«ï¼ˆSpan-levelï¼‰çš„æ¢¯åº¦**è¿›è¡Œåå‘ä¼ æ’­ã€‚\n*   **äº®ç‚¹ï¼š** è¿™å®ç°äº†å¯¹æ¨¡å‹å‚æ•°çš„ç²¾ç»†åŒ–æ›´æ–°ï¼Œç›´æ¥â€œä¿®è¡¥â€ç”Ÿæˆé”™è¯¯çš„ token åŒºåŸŸï¼Œåœ¨ä»£ç ç”Ÿæˆå’Œæ‘˜è¦ä»»åŠ¡ä¸Šè¶…è¶Šäº†æ ‡é‡ RL å’Œçº¯ Prompt æ–¹æ³•ã€‚\n\n**12. Unraveling LoRA Interference: Orthogonal Subspaces for Robust Model Merging (è§£æ„ LoRA å¹²æ‰°ï¼šç”¨äºé²æ£’æ¨¡å‹åˆå¹¶çš„æ­£äº¤å­ç©ºé—´)**\n*   **Authors:** Haobo Zhang et al. (ACL 2025)\n*   **æ ¸å¿ƒè´¡çŒ®ï¼š** è§£å†³äº†åˆå¹¶å¤šä¸ª LoRA æ¨¡å‹æ—¶æ€§èƒ½ä¸‹é™çš„é—®é¢˜ã€‚\n*   **æ–¹æ³•ï¼š** æå‡ºäº† **OSRM**ï¼Œåœ¨å¾®è°ƒ*ä¹‹å‰*å°±çº¦æŸ LoRA å­ç©ºé—´æ­£äº¤ã€‚è¿™æ ·ä¸åŒä»»åŠ¡çš„æ›´æ–°å°±ä¸ä¼šäº’ç›¸æ‰“æ¶ï¼Œå®ç°äº†â€œå³æ’å³ç”¨â€çš„æ— æŸæ¨¡å‹åˆå¹¶ã€‚\n\n---\n\n### ğŸ“ æ•°å­¦ã€ç§‘å­¦ä¸æ–°æ¶æ„ (Math, Science & Architecture)\n\n**13. Scaling Reasoning without Attention (æ—  Attention çš„æ‰©å±•æ¨ç†)**\n*   **Authors:** Xueliang Zhao et al.\n*   **æ ¸å¿ƒè´¡çŒ®ï¼š** åŸºäº Mamba-2ï¼ˆçŠ¶æ€ç©ºé—´æ¨¡å‹ SSDï¼‰ï¼Œå®Œå…¨ç§»é™¤äº† Self-Attentionã€‚\n*   **äº®ç‚¹ï¼š** å®ç°äº†å›ºå®šå†…å­˜ã€å¸¸æ•°æ—¶é—´çš„æ¨ç†ã€‚é€šè¿‡è¯¾ç¨‹å­¦ä¹ ï¼ˆCurriculum Fine-tuningï¼‰ï¼Œè¿™ä¸ª 7B çš„æ—  Attention æ¨¡å‹åœ¨æ•°å­¦æ¨ç†ï¼ˆAIMEï¼‰ä¸Šç”šè‡³å‡»è´¥äº†æ›´å¤§å‚æ•°é‡çš„ Transformer æ¨¡å‹ï¼ˆå¦‚ Gemma3-27Bï¼‰ã€‚SSM æ¶æ„åœ¨å¤æ‚æ¨ç†ä¸Šçš„æ½œåŠ›è¢«è¿›ä¸€æ­¥è¯å®ã€‚\n\n**14. AI Mathematician: Towards Fully Automated Frontier Mathematical Research (AI æ•°å­¦å®¶ï¼šè¿ˆå‘å…¨è‡ªåŠ¨å‰æ²¿æ•°å­¦ç ”ç©¶)**\n*   **Authors:** Yuanhang Liu et al.\n*   **æ ¸å¿ƒè´¡çŒ®ï¼š** æå‡ºäº† **AIM** æ¡†æ¶ï¼Œæ—¨åœ¨è®© LLM è¿›è¡ŒçœŸæ­£çš„æ•°å­¦ç ”ç©¶ï¼Œè€Œéä»…ä»…åšç«èµ›é¢˜ã€‚åŒ…å«æ¢ç´¢æœºåˆ¶ï¼ˆç”Ÿæˆæ›´é•¿çš„è§£é¢˜è·¯å¾„ï¼‰å’Œæ‚²è§‚éªŒè¯æ–¹æ³•ã€‚æ—©æœŸç‰ˆæœ¬å·²èƒ½è‡ªä¸»æ„å»ºéƒ¨åˆ†è¯æ˜å¹¶å‘ç°éå¹³å‡¡çš„è§è§£ã€‚\n\n**15. ASyMOB: Algebraic Symbolic Mathematical Operations Benchmark (ASyMOBï¼šä»£æ•°ç¬¦å·æ•°å­¦è¿ç®—åŸºå‡†)**\n*   **Authors:** Michael Shalyt et al.\n*   **æ ¸å¿ƒè´¡çŒ®ï¼š** ä¸€ä¸ªåŒ…å« 1.7 ä¸‡ä¸ªé—®é¢˜çš„ç¬¦å·æ•°å­¦åŸºå‡†ã€‚\n*   **å‘ç°ï¼š** LLM å¯¹ç®€å•çš„æ•°å€¼æˆ–ç¬¦å·â€œæ‰°åŠ¨â€éå¸¸æ•æ„Ÿï¼Œæ€§èƒ½ä¼šä¸‹é™é«˜è¾¾ 70%ï¼Œè¯´æ˜å®ƒä»¬æ›´å¤šæ˜¯é è®°å¿†è€Œéç†è§£ã€‚ä½†æœ€æ–°çš„ **o4-mini å’Œ Gemini 2.5 Flash** å±•ç°å‡ºäº†æƒŠäººçš„é²æ£’æ€§ï¼Œå¯èƒ½é¢„ç¤ºç€â€œç›¸å˜â€çš„å‘ç”Ÿã€‚\n\n---\n\n### ğŸ¥ è§†é¢‘ç”Ÿæˆä¸äº¤äº’ (Video Generation)\n\n**16. ATI: Any Trajectory Instruction for Controllable Video Generation (ATIï¼šç”¨äºå¯æ§è§†é¢‘ç”Ÿæˆçš„ä»»æ„è½¨è¿¹æŒ‡ä»¤)**\n*   **Authors:** Angtian Wang et al.\n*   **æ ¸å¿ƒè´¡çŒ®ï¼š** ç»Ÿä¸€äº†ç›¸æœºè¿åŠ¨ã€ç‰©ä½“è¿åŠ¨å’Œå±€éƒ¨å½¢å˜çš„æ§åˆ¶ã€‚ç”¨æˆ·ç”»å‡ æ¡è½¨è¿¹ï¼Œå°±èƒ½æŒ‡æŒ¥è§†é¢‘é‡Œçš„ç‰©ä½“æ€ä¹ˆåŠ¨ã€ç›¸æœºæ€ä¹ˆæ‘‡ï¼Œæ¯”ä¹‹å‰çš„ç‰¹å®šæ¨¡å—æ–¹æ³•æ›´çµæ´»ã€‚\n\n**17. Learning World Models for Interactive Video Generation (å­¦ä¹ äº¤äº’å¼è§†é¢‘ç”Ÿæˆçš„ä¸–ç•Œæ¨¡å‹)**\n*   **Authors:** Taiye Chen et al.\n*   **æ ¸å¿ƒè´¡çŒ®ï¼š** æŒ‡å‡ºå½“å‰è§†é¢‘æ¨¡å‹ç¼ºä¹çœŸæ­£çš„ä¸–ç•Œæ¨¡å‹èƒ½åŠ›ï¼ˆè®°å¿†ä¸è¶³ã€è¯¯å·®ç´¯ç§¯ï¼‰ã€‚æå‡ºäº† **VRAG**ï¼ˆè§†é¢‘æ£€ç´¢å¢å¼ºç”Ÿæˆï¼‰ï¼Œé€šè¿‡æ˜¾å¼çš„å…¨å±€çŠ¶æ€æ¡ä»¶æ˜¾è‘—å‡å°‘äº†é•¿è§†é¢‘ç”Ÿæˆçš„è¯¯å·®ç´¯ç§¯ã€‚\n\n---\n\n### ğŸ’¡ å…¶ä»–å€¼å¾—å…³æ³¨çš„è®ºæ–‡ (Quick Hits)\n\n*   **[Speech] FAMA (#53):** é¦–ä¸ªé’ˆå¯¹è‹±è¯­å’Œæ„å¤§åˆ©è¯­çš„**å¼€æºç§‘å­¦è¯­éŸ³åŸºç¡€æ¨¡å‹**ï¼ŒåŸºäº 150k+ å°æ—¶çš„å¼€æºæ•°æ®è®­ç»ƒï¼Œæ—¨åœ¨è§£å†³å¤§å‚é—­æºæ¨¡å‹ä¸å¯å¤ç°çš„é—®é¢˜ã€‚\n*   **[Cross-Lingual] OWL (#2):** æ¢ç©¶ LLM çš„è·¨è¯­è¨€è®°å¿†ã€‚å‘ç° GPT-4o å³ä½¿é¢å¯¹ä»æœªè§è¿‡çš„ä½èµ„æºè¯­è¨€ç¿»è¯‘ï¼Œä¹Ÿèƒ½å›å¿†èµ·è‹±è¯­é¢„è®­ç»ƒæ•°æ®ä¸­çš„ä¹¦ç±å†…å®¹ï¼ˆå¦‚å“ˆåˆ©æ³¢ç‰¹ï¼‰ï¼Œ**è·¨è¯­è¨€æ³›åŒ–èƒ½åŠ›æƒŠäºº**ã€‚\n*   **[Defense] Test-Time Immunization (#131):** ä¸€ç§é€šç”¨çš„é˜²å¾¡æ¡†æ¶ï¼Œåœ¨æ¨ç†æ—¶æ£€æµ‹ Jailbreak æ”»å‡»å¹¶è¿›è¡Œè‡ªæˆ‘è¿›åŒ–çš„å®‰å…¨å¾®è°ƒã€‚\n*   **[Quantization] SineLoRA$\\Delta$ (#203):** ç»“åˆæ­£å¼¦æ¿€æ´»å‡½æ•°å’Œé‡åŒ–ï¼Œæå‡äº†ä½ç§©é€‚é…å™¨ï¼ˆLoRAï¼‰åœ¨æç«¯å‹ç¼©ä¸‹çš„è¡¨è¾¾èƒ½åŠ›ã€‚\n\n---\n**æ•™æˆç‚¹è¯„ï¼š**\nä»Šå¤©çš„è®ºæ–‡ä¸ä»…æ•°é‡å·¨å¤§ï¼Œè€Œä¸”è´¨é‡æé«˜ã€‚æœ€æ˜æ˜¾çš„è¶‹åŠ¿æ˜¯ **â€œDeepSeek-R1 æ•ˆåº”â€** â€”â€” å­¦æœ¯ç•Œæ­£åœ¨ç–¯ç‹‚å¤ç°ã€è§£æå’Œæ”¹è¿›åŸºäº RL çš„æ¨ç†è®­ç»ƒèŒƒå¼ï¼ˆå°¤å…¶æ˜¯ GRPOï¼‰ã€‚åŒæ—¶ï¼Œ**Text2Grad** å’Œ **Thinking with Generated Images** å±•ç¤ºäº†éå¸¸æœ‰æ½œåŠ›çš„æœªæ¥æ–¹å‘ï¼šå‰è€…è®©æ¨¡å‹ä»è‡ªç„¶è¯­è¨€åé¦ˆä¸­ç›´æ¥å­¦ä¹ å‚æ•°ï¼Œåè€…è®©æ¨¡å‹å­¦ä¼šâ€œçœ‹å›¾æ€è€ƒâ€ã€‚å»ºè®®é‡ç‚¹é˜…è¯» Skywork çš„æŠ€æœ¯æŠ¥å‘Šå’Œ Text2Gradã€‚\n\næ˜å¤©è§ï¼",
  "papers": [
    {
      "arxiv_id": "2505.22946v1",
      "title": "NegVQA: Can Vision Language Models Understand Negation?",
      "title_zh": "NegVQAï¼šè§†è§‰è¯­è¨€æ¨¡å‹èƒ½å¦ç†è§£å¦å®šï¼Ÿ",
      "authors": [
        "Yuhui Zhang",
        "Yuchang Su",
        "Yiming Liu",
        "Serena Yeung-Levy"
      ],
      "abstract": "Negation is a fundamental linguistic phenomenon that can entirely reverse the meaning of a sentence. As vision language models (VLMs) continue to advance and are deployed in high-stakes applications, assessing their ability to comprehend negation becomes essential. To address this, we introduce NegVQA, a visual question answering (VQA) benchmark consisting of 7,379 two-choice questions covering diverse negation scenarios and image-question distributions. We construct NegVQA by leveraging large language models to generate negated versions of questions from existing VQA datasets. Evaluating 20 state-of-the-art VLMs across seven model families, we find that these models struggle significantly with negation, exhibiting a substantial performance drop compared to their responses to the original questions. Furthermore, we uncover a U-shaped scaling trend, where increasing model size initially degrades performance on NegVQA before leading to improvements. Our benchmark reveals critical gaps in VLMs' negation understanding and offers insights into future VLM development. Project page available at https://yuhui-zh15.github.io/NegVQA/.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† NegVQAï¼Œä¸€ä¸ªæ—¨åœ¨è¯„ä¼°è§†è§‰è¯­è¨€æ¨¡å‹ (Vision Language Models, VLMs) å¦å®šè¯­ä¹‰ç†è§£èƒ½åŠ›çš„è§†è§‰é—®ç­” (Visual Question Answering, VQA) åŸºå‡†æµ‹è¯•ã€‚è¯¥åŸºå‡†åŒ…å« 7,379 ä¸ªäºŒé€‰ä¸€é—®é¢˜ï¼Œé€šè¿‡åˆ©ç”¨å¤§è¯­è¨€æ¨¡å‹ (Large Language Models, LLMs) å¯¹ç°æœ‰ VQA æ•°æ®é›†ä¸­çš„é—®é¢˜ç”Ÿæˆå¦å®šç‰ˆæœ¬è€Œæ„å»ºï¼Œæ¶µç›–äº†å¤šæ ·çš„å¦å®šåœºæ™¯ã€‚é€šè¿‡å¯¹ 20 ç§æœ€å…ˆè¿›çš„ VLMs è¿›è¡Œè¯„ä¼°ï¼Œç ”ç©¶å‘ç°è¿™äº›æ¨¡å‹åœ¨å¤„ç†å¦å®šè¯­ä¹‰æ—¶æ™®éå­˜åœ¨å›°éš¾ï¼Œæ€§èƒ½è¾ƒåŸå§‹é—®é¢˜å¤§å¹…ä¸‹é™ã€‚æ­¤å¤–ï¼Œç ”ç©¶è¿˜æ­ç¤ºäº†ä¸€ä¸ª U å‹æ‰©å±•è¶‹åŠ¿ (U-shaped scaling trend)ï¼Œå³æ¨¡å‹è§„æ¨¡çš„å¢åŠ èµ·åˆä¼šæŸå®³æ¨¡å‹åœ¨ NegVQA ä¸Šçš„è¡¨ç°ï¼Œéšåæ‰ä¼šé€æ­¥æå‡ã€‚NegVQA æŒ‡å‡ºäº†å½“å‰ VLMs åœ¨å¦å®šç†è§£ä¸Šçš„å…³é”®ç¼ºé™·ï¼Œä¸ºæœªæ¥æ„å»ºå…·å¤‡æ›´å¼ºé€»è¾‘æ¨ç†èƒ½åŠ›çš„è§†è§‰æ¨¡å‹æä¾›äº†é‡è¦å‚è€ƒã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.CV",
        "cs.CY",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "Published at ACL 2025 Findings",
      "pdf_url": "https://arxiv.org/pdf/2505.22946v1",
      "published_date": "2025-05-28 23:58:37 UTC",
      "updated_date": "2025-05-28 23:58:37 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T15:06:56.355630+00:00"
    },
    {
      "arxiv_id": "2505.22945v2",
      "title": "OWL: Probing Cross-Lingual Recall of Memorized Texts via World Literature",
      "title_zh": "OWLï¼šå€ŸåŠ©ä¸–ç•Œæ–‡å­¦æ¢ç©¶è®°å¿†æ–‡æœ¬çš„è·¨è¯­è¨€å¬å›èƒ½åŠ›",
      "authors": [
        "Alisha Srivastava",
        "Emir Korukluoglu",
        "Minh Nhat Le",
        "Duyen Tran",
        "Chau Minh Pham",
        "Marzena Karpinska",
        "Mohit Iyyer"
      ],
      "abstract": "Large language models (LLMs) are known to memorize and recall English text from their pretraining data. However, the extent to which this ability generalizes to non-English languages or transfers across languages remains unclear. This paper investigates multilingual and cross-lingual memorization in LLMs, probing if memorized content in one language (e.g., English) can be recalled when presented in translation. To do so, we introduce OWL, a dataset of 31.5K aligned excerpts from 20 books in ten languages, including English originals, official translations (Vietnamese, Spanish, Turkish), and new translations in six low-resource languages (Sesotho, Yoruba, Maithili, Malagasy, Setswana, Tahitian). We evaluate memorization across model families and sizes through three tasks: (1) direct probing, which asks the model to identify a book's title and author; (2) name cloze, which requires predicting masked character names; and (3) prefix probing, which involves generating continuations. We find that LLMs consistently recall content across languages, even for texts without direct translation in pretraining data. GPT-4o, for example, identifies authors and titles 69% of the time and masked entities 6% of the time in newly translated excerpts. Perturbations (e.g., masking characters, shuffling words) modestly reduce direct probing accuracy (7% drop for shuffled official translations). Our results highlight the extent of cross-lingual memorization and provide insights on the differences between the models.",
      "tldr_zh": "è¯¥ç ”ç©¶è°ƒæŸ¥äº†å¤§è¯­è¨€æ¨¡å‹ (LLMs) åœ¨å¤šè¯­è¨€å’Œè·¨è¯­è¨€ç¯å¢ƒä¸‹çš„è®°å¿†èƒ½åŠ›ï¼Œé‡ç‚¹æ¢ç©¶æ¨¡å‹æ˜¯å¦èƒ½åœ¨ä¸€ç§è¯­è¨€çš„ç¿»è¯‘ä¸­å¬å›å¦ä¸€ç§è¯­è¨€çš„é¢„è®­ç»ƒè®°å¿†å†…å®¹ã€‚ä¸ºæ­¤ï¼Œä½œè€…æ¨å‡ºäº† OWL æ•°æ®é›†ï¼ŒåŒ…å«æ¥è‡ª 20 æœ¬ä¸–ç•Œæ–‡å­¦ä½œå“ã€æ¶µç›– 10 ç§è¯­è¨€çš„ 31,500 æ¡å¯¹é½æ–‡æœ¬ç‰‡æ®µï¼Œå…¶ä¸­åŒ…æ‹¬å®˜æ–¹ç¿»è¯‘å’Œé’ˆå¯¹ä½èµ„æºè¯­è¨€çš„æ–°ç¿»è¯‘ã€‚ç ”ç©¶é€šè¿‡ direct probingã€name cloze å’Œ prefix probing ä¸‰é¡¹ä»»åŠ¡ï¼Œè¯„ä¼°äº†ä¸åŒæ¨¡å‹å¯¹ä¹¦ç±ä¿¡æ¯ã€è§’è‰²åç§°åŠæ–‡æœ¬ç»­å†™çš„å¬å›è¡¨ç°ã€‚ç»“æœè¡¨æ˜ï¼ŒLLMs èƒ½å¤Ÿè·¨è¯­è¨€ä¸€è‡´åœ°å¬å›å†…å®¹ï¼Œå³ä½¿é¢å¯¹é¢„è®­ç»ƒæ•°æ®ä¸­æœªå‡ºç°çš„ç¿»è¯‘æ–‡æœ¬ï¼ŒGPT-4o åœ¨è¯†åˆ«ä½œè€…å’Œæ ‡é¢˜æ–¹é¢çš„å‡†ç¡®ç‡ä»å¯è¾¾ 69%ã€‚æ­¤å¤–ï¼Œæ‰°åŠ¨å®éªŒæ˜¾ç¤ºæ¨¡å‹å¯¹æ–‡æœ¬å˜åŠ¨çš„é²æ£’æ€§è¾ƒé«˜ï¼Œè¿™äº›å‘ç°æ·±å…¥æ­ç¤ºäº†è·¨è¯­è¨€è®°å¿†çš„å¹¿åº¦åŠå…¶åœ¨ä¸åŒæ¨¡å‹é—´çš„è¡¨ç°å·®å¼‚ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "Accepted to EMNLP 2025 Main",
      "pdf_url": "https://arxiv.org/pdf/2505.22945v2",
      "published_date": "2025-05-28 23:57:03 UTC",
      "updated_date": "2025-10-07 17:39:05 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T15:07:14.038553+00:00"
    },
    {
      "arxiv_id": "2505.22944v3",
      "title": "ATI: Any Trajectory Instruction for Controllable Video Generation",
      "title_zh": "ATIï¼šé¢å‘å¯æ§è§†é¢‘ç”Ÿæˆçš„ä»»æ„è½¨è¿¹æŒ‡ä»¤",
      "authors": [
        "Angtian Wang",
        "Haibin Huang",
        "Jacob Zhiyuan Fang",
        "Yiding Yang",
        "Chongyang Ma"
      ],
      "abstract": "We propose a unified framework for motion control in video generation that seamlessly integrates camera movement, object-level translation, and fine-grained local motion using trajectory-based inputs. In contrast to prior methods that address these motion types through separate modules or task-specific designs, our approach offers a cohesive solution by projecting user-defined trajectories into the latent space of pre-trained image-to-video generation models via a lightweight motion injector. Users can specify keypoints and their motion paths to control localized deformations, entire object motion, virtual camera dynamics, or combinations of these. The injected trajectory signals guide the generative process to produce temporally consistent and semantically aligned motion sequences. Our framework demonstrates superior performance across multiple video motion control tasks, including stylized motion effects (e.g., motion brushes), dynamic viewpoint changes, and precise local motion manipulation. Experiments show that our method provides significantly better controllability and visual quality compared to prior approaches and commercial solutions, while remaining broadly compatible with various state-of-the-art video generation backbones. Project page: https://anytraj.github.io/.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† ATIï¼Œè¿™æ˜¯ä¸€ä¸ªç”¨äºè§†é¢‘ç”Ÿæˆçš„ç»Ÿä¸€è¿åŠ¨æ§åˆ¶æ¡†æ¶ï¼Œèƒ½å¤Ÿé€šè¿‡åŸºäºè½¨è¿¹çš„è¾“å…¥æ— ç¼é›†æˆç›¸æœºç§»åŠ¨ã€ç‰©ä½“çº§å¹³ç§»ä»¥åŠç»†ç²’åº¦çš„å±€éƒ¨è¿åŠ¨ã€‚ä¸ä»¥å¾€é‡‡ç”¨ç‹¬ç«‹æ¨¡å—æˆ–ç‰¹å®šä»»åŠ¡è®¾è®¡çš„æ–¹æ³•ä¸åŒï¼Œè¯¥æ–¹æ¡ˆé€šè¿‡ä¸€ä¸ªè½»é‡çº§çš„è¿åŠ¨æ³¨å…¥å™¨(motion injector)ï¼Œå°†ç”¨æˆ·å®šä¹‰çš„è½¨è¿¹æŠ•å½±åˆ°é¢„è®­ç»ƒå›¾åƒè½¬è§†é¢‘(image-to-video)ç”Ÿæˆæ¨¡å‹çš„æ½œç©ºé—´ä¸­ã€‚ç”¨æˆ·å¯ä»¥é€šè¿‡æŒ‡å®šå…³é”®ç‚¹åŠå…¶è¿åŠ¨è·¯å¾„ï¼Œæ¥ç²¾ç¡®æ§åˆ¶å±€éƒ¨å˜å½¢ã€æ•´ä½“ç‰©ä½“è¿åŠ¨ã€è™šæ‹Ÿç›¸æœºåŠ¨æ€æˆ–å…¶ç»„åˆã€‚æ³¨å…¥çš„è½¨è¿¹ä¿¡å·å¼•å¯¼ç”Ÿæˆè¿‡ç¨‹ï¼Œä»è€Œäº§ç”Ÿå…·æœ‰æ—¶é—´ä¸€è‡´æ€§(temporally consistent)å’Œè¯­ä¹‰å¯¹é½(semantically aligned)çš„è¿åŠ¨åºåˆ—ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒATI åœ¨è¿åŠ¨ç”»ç¬”(motion brushes)ã€åŠ¨æ€è§†ç‚¹å˜æ¢å’Œå±€éƒ¨è¿åŠ¨æ“çºµç­‰ä»»åŠ¡ä¸­è¡¨ç°ä¼˜å¼‚ã€‚ç›¸æ¯”ä¹‹å‰çš„ç ”ç©¶åŠå•†ä¸šæ–¹æ¡ˆï¼Œè¯¥æ–¹æ³•åœ¨å¯æ§æ€§å’Œè§†è§‰è´¨é‡ä¸Šå‡æœ‰æ˜¾è‘—æå‡ï¼Œä¸”èƒ½å¹¿æ³›å…¼å®¹å¤šç§å…ˆè¿›çš„è§†é¢‘ç”Ÿæˆéª¨å¹²ç½‘ç»œ(backbones)ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.22944v3",
      "published_date": "2025-05-28 23:49:18 UTC",
      "updated_date": "2025-06-10 06:15:58 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T15:06:58.406635+00:00"
    },
    {
      "arxiv_id": "2505.22943v1",
      "title": "Can LLMs Deceive CLIP? Benchmarking Adversarial Compositionality of Pre-trained Multimodal Representation via Text Updates",
      "title_zh": "LLM èƒ½æ¬ºéª— CLIP å—ï¼ŸåŸºäºæ–‡æœ¬æ›´æ–°çš„é¢„è®­ç»ƒå¤šæ¨¡æ€è¡¨ç¤ºå¯¹æŠ—ç»„åˆæ€§åŸºå‡†æµ‹è¯•",
      "authors": [
        "Jaewoo Ahn",
        "Heeseung Yun",
        "Dayoon Ko",
        "Gunhee Kim"
      ],
      "abstract": "While pre-trained multimodal representations (e.g., CLIP) have shown impressive capabilities, they exhibit significant compositional vulnerabilities leading to counterintuitive judgments. We introduce Multimodal Adversarial Compositionality (MAC), a benchmark that leverages large language models (LLMs) to generate deceptive text samples to exploit these vulnerabilities across different modalities and evaluates them through both sample-wise attack success rate and group-wise entropy-based diversity. To improve zero-shot methods, we propose a self-training approach that leverages rejection-sampling fine-tuning with diversity-promoting filtering, which enhances both attack success rate and sample diversity. Using smaller language models like Llama-3.1-8B, our approach demonstrates superior performance in revealing compositional vulnerabilities across various multimodal representations, including images, videos, and audios.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†é¢„è®­ç»ƒå¤šæ¨¡æ€è¡¨ç¤ºï¼ˆå¦‚ CLIPï¼‰åœ¨å¤„ç†ç»„åˆæ€§ï¼ˆcompositionalï¼‰ä¿¡æ¯æ—¶å­˜åœ¨çš„æ˜¾è‘—æ¼æ´ï¼Œå¹¶æå‡ºäº† Multimodal Adversarial Compositionality (MAC) åŸºå‡†æµ‹è¯•ã€‚è¯¥åŸºå‡†é€šè¿‡å¤§è¯­è¨€æ¨¡å‹ (LLMs) ç”Ÿæˆè¯¯å¯¼æ€§æ–‡æœ¬æ ·æœ¬æ¥æŒ–æ˜ä¸åŒæ¨¡æ€ä¸‹çš„è¿™äº›æ¼æ´ï¼Œå¹¶åˆ©ç”¨æ”»å‡»æˆåŠŸç‡ (attack success rate) å’Œç¾¤ä½“å¤šæ ·æ€§è¿›è¡Œè¯„ä¼°ã€‚ä¸ºäº†ä¼˜åŒ–æ€§èƒ½ï¼Œä½œè€…æå‡ºäº†ä¸€ç§ç»“åˆæ‹’ç»é‡‡æ ·å¾®è°ƒ (rejection-sampling fine-tuning) ä¸å¤šæ ·æ€§è¿‡æ»¤çš„è‡ªè®­ç»ƒæ–¹æ³•ï¼Œæ—¨åœ¨å¢å¼ºæ”»å‡»æ•ˆæœåŠæ ·æœ¬å¤šæ ·æ€§ã€‚å®éªŒè¯æ˜ï¼Œè¯¥æ–¹æ³•åˆ©ç”¨ Llama-3.1-8B ç­‰è¾ƒå°æ¨¡å‹å³å¯åœ¨å›¾åƒã€è§†é¢‘å’ŒéŸ³é¢‘ç­‰å¤šç§å¤šæ¨¡æ€è¡¨ç¤ºä¸­æœ‰æ•ˆæ­ç¤ºç»„åˆæ€§æ¼æ´ï¼Œå±•ç°äº†å“è¶Šçš„è¯„æµ‹èƒ½åŠ›ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.CV",
        "cs.LG",
        "cs.SD"
      ],
      "primary_category": "cs.CL",
      "comment": "ACL 2025 Main. Code is released at https://vision.snu.ac.kr/projects/mac",
      "pdf_url": "https://arxiv.org/pdf/2505.22943v1",
      "published_date": "2025-05-28 23:45:55 UTC",
      "updated_date": "2025-05-28 23:45:55 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T15:07:13.448116+00:00"
    },
    {
      "arxiv_id": "2505.22942v2",
      "title": "WorkForceAgent-R1: Incentivizing Reasoning Capability in LLM-based Web Agents via Reinforcement Learning",
      "title_zh": "WorkForceAgent-R1ï¼šé€šè¿‡å¼ºåŒ–å­¦ä¹ æ¿€å‘åŸºäºå¤§è¯­è¨€æ¨¡å‹çš„ Web æ™ºèƒ½ä½“æ¨ç†èƒ½åŠ›",
      "authors": [
        "Yuchen Zhuang",
        "Di Jin",
        "Jiaao Chen",
        "Wenqi Shi",
        "Hanrui Wang",
        "Chao Zhang"
      ],
      "abstract": "Large language models (LLMs)-empowered web agents enables automating complex, real-time web navigation tasks in enterprise environments. However, existing web agents relying on supervised fine-tuning (SFT) often struggle with generalization and robustness due to insufficient reasoning capabilities when handling the inherently dynamic nature of web interactions. In this study, we introduce WorkForceAgent-R1, an LLM-based web agent trained using a rule-based R1-style reinforcement learning framework designed explicitly to enhance single-step reasoning and planning for business-oriented web navigation tasks. We employ a structured reward function that evaluates both adherence to output formats and correctness of actions, enabling WorkForceAgent-R1 to implicitly learn robust intermediate reasoning without explicit annotations or extensive expert demonstrations. Extensive experiments on the WorkArena benchmark demonstrate that WorkForceAgent-R1 substantially outperforms SFT baselines by 10.26-16.59%, achieving competitive performance relative to proprietary LLM-based agents (gpt-4o) in workplace-oriented web navigation tasks.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ç°æœ‰çš„åŸºäºç›‘ç£å¾®è°ƒ(SFT)çš„ Web Agents åœ¨å¤„ç†åŠ¨æ€ç½‘é¡µäº¤äº’æ—¶æ¨ç†èƒ½åŠ›ä¸è¶³ã€æ³›åŒ–æ€§å·®çš„é—®é¢˜ï¼Œæå‡ºäº† WorkForceAgent-R1ã€‚è¯¥æ¨¡å‹é‡‡ç”¨äº†ä¸€ç§å— R1 å¯å‘çš„åŸºäºè§„åˆ™çš„å¼ºåŒ–å­¦ä¹ (Reinforcement Learning)æ¡†æ¶ï¼Œæ—¨åœ¨æ˜¾å¼å¢å¼ºé¢å‘å•†ä¸š Web å¯¼èˆªä»»åŠ¡çš„å•æ­¥æ¨ç†ä¸è§„åˆ’èƒ½åŠ›ã€‚ç ”ç©¶å›¢é˜Ÿåˆ©ç”¨ç»“æ„åŒ–å¥–åŠ±å‡½æ•°æ¥è¯„ä¼°è¾“å‡ºæ ¼å¼çš„åˆè§„æ€§ä¸æ“ä½œçš„æ­£ç¡®æ€§ï¼Œä½¿ WorkForceAgent-R1 èƒ½å¤Ÿåœ¨æ— éœ€ä¸“å®¶æ¼”ç¤ºæˆ–æ˜¾å¼æ ‡æ³¨çš„æƒ…å†µä¸‹ï¼Œéšå¼å­¦ä¹ é²æ£’çš„ä¸­é—´æ¨ç†è¿‡ç¨‹ã€‚åœ¨ WorkArena åŸºå‡†æµ‹è¯•ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ¡†æ¶çš„è¡¨ç°æ˜¾è‘—ä¼˜äºä¼ ç»Ÿçš„ç›‘ç£å¾®è°ƒåŸºå‡†ï¼Œå‡†ç¡®ç‡æå‡äº† 10.26% è‡³ 16.59%ã€‚WorkForceAgent-R1 åœ¨å·¥ä½œåœºæ™¯ç›¸å…³çš„ Web å¯¼èˆªä»»åŠ¡ä¸­å±•ç°å‡ºäº†ä¸ GPT-4o ç­‰é—­æºæ¨¡å‹ç›¸å½“çš„ç«äº‰åŠ›ï¼Œä¸ºæå‡å¤§è¯­è¨€æ¨¡å‹(LLMs)é©±åŠ¨çš„æ™ºèƒ½ä½“åœ¨å¤æ‚ä¼ä¸šç¯å¢ƒä¸­çš„è‡ªåŠ¨åŒ–æ°´å¹³æä¾›äº†æœ‰æ•ˆæ–¹æ¡ˆã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "Work in Progress",
      "pdf_url": "https://arxiv.org/pdf/2505.22942v2",
      "published_date": "2025-05-28 23:45:28 UTC",
      "updated_date": "2025-06-08 07:57:08 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T15:07:14.739129+00:00"
    },
    {
      "arxiv_id": "2505.22939v1",
      "title": "Generative Social Choice: The Next Generation",
      "title_zh": "ç”Ÿæˆå¼ç¤¾ä¼šé€‰æ‹©ï¼šæ–°ä¸€ä»£",
      "authors": [
        "Niclas Boehmer",
        "Sara Fish",
        "Ariel D. Procaccia"
      ],
      "abstract": "A key task in certain democratic processes is to produce a concise slate of statements that proportionally represents the full spectrum of user opinions. This task is similar to committee elections, but unlike traditional settings, the candidate set comprises all possible statements of varying lengths, and so it can only be accessed through specific queries. Combining social choice and large language models, prior work has approached this challenge through a framework of generative social choice. We extend the framework in two fundamental ways, providing theoretical guarantees even in the face of approximately optimal queries and a budget limit on the overall length of the slate. Using GPT-4o to implement queries, we showcase our approach on datasets related to city improvement measures and drug reviews, demonstrating its effectiveness in generating representative slates from unstructured user opinions.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†Generative Social Choiceï¼ˆç”Ÿæˆå¼ç¤¾ä¼šé€‰æ‹©ï¼‰çš„å‰æ²¿æ¡†æ¶ï¼Œæ—¨åœ¨ä»å¤šå…ƒçš„ç”¨æˆ·æ„è§ä¸­æå–ä¸€ç»„èƒ½å¤ŸæŒ‰æ¯”ä¾‹ä»£è¡¨å®Œæ•´è§‚ç‚¹é¢‘è°±çš„ç®€æ´é™ˆè¿°ã€‚ä½œè€…é€šè¿‡ä¸¤é¡¹å…³é”®æ‰©å±•å®Œå–„äº†ç°æœ‰ç†è®ºï¼Œå³ä¾¿åœ¨æŸ¥è¯¢ä»…ä¸ºè¿‘ä¼¼æœ€ä¼˜ä¸”å—åˆ°æ€»é•¿åº¦Budget Limitï¼ˆé¢„ç®—é™åˆ¶ï¼‰çš„æƒ…å†µä¸‹ï¼Œä¾ç„¶èƒ½æä¾›åšå®çš„Theoretical Guaranteesï¼ˆç†è®ºä¿è¯ï¼‰ã€‚è¯¥æ–¹æ³•å°†Social Choiceç†è®ºä¸Large Language Modelsï¼ˆå¤§è¯­è¨€æ¨¡å‹ï¼‰ç›¸ç»“åˆï¼Œå¹¶åˆ©ç”¨GPT-4oå®ç°äº†é«˜æ•ˆçš„æŸ¥è¯¢å¤„ç†ã€‚åœ¨åŸå¸‚æ”¹è¿›æªæ–½å’Œè¯ç‰©è¯„è®ºæ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ¡†æ¶èƒ½æœ‰æ•ˆå¤„ç†éç»“æ„åŒ–æ•°æ®å¹¶ç”Ÿæˆæå…·ä»£è¡¨æ€§çš„è§‚ç‚¹é›†ã€‚è¯¥ç ”ç©¶ä¸ä»…æ·±åŒ–äº†ç”Ÿæˆå¼ç¤¾ä¼šé€‰æ‹©çš„ç†è®ºåŸºç¡€ï¼Œä¹Ÿä¸ºæ°‘ä¸»å†³ç­–è¿‡ç¨‹ä¸­çš„æ„è§æ±‡æ€»æä¾›äº†é«˜æ•ˆä¸”å…·å¤‡è¯æ˜æ€§çš„æŠ€æœ¯æ‰‹æ®µã€‚",
      "categories": [
        "cs.GT",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.GT",
      "comment": "Accepted to ICML 2025",
      "pdf_url": "https://arxiv.org/pdf/2505.22939v1",
      "published_date": "2025-05-28 23:40:24 UTC",
      "updated_date": "2025-05-28 23:40:24 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T15:07:20.153501+00:00"
    },
    {
      "arxiv_id": "2505.22934v1",
      "title": "Unraveling LoRA Interference: Orthogonal Subspaces for Robust Model Merging",
      "title_zh": "æ­ç¤º LoRA å¹²æ‰°ï¼šé¢å‘é²æ£’æ¨¡å‹åˆå¹¶çš„æ­£äº¤å­ç©ºé—´",
      "authors": [
        "Haobo Zhang",
        "Jiayu Zhou"
      ],
      "abstract": "Fine-tuning large language models (LMs) for individual tasks yields strong performance but is expensive for deployment and storage. Recent works explore model merging to combine multiple task-specific models into a single multi-task model without additional training. However, existing merging methods often fail for models fine-tuned with low-rank adaptation (LoRA), due to significant performance degradation. In this paper, we show that this issue arises from a previously overlooked interplay between model parameters and data distributions. We propose Orthogonal Subspaces for Robust model Merging (OSRM) to constrain the LoRA subspace *prior* to fine-tuning, ensuring that updates relevant to one task do not adversely shift outputs for others. Our approach can seamlessly integrate with most existing merging algorithms, reducing the unintended interference among tasks. Extensive experiments on eight datasets, tested with three widely used LMs and two large LMs, demonstrate that our method not only boosts merging performance but also preserves single-task accuracy. Furthermore, our approach exhibits greater robustness to the hyperparameters of merging. These results highlight the importance of data-parameter interaction in model merging and offer a plug-and-play solution for merging LoRA models.",
      "tldr_zh": "é’ˆå¯¹å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLMsï¼‰åœ¨å¤šä»»åŠ¡åˆå¹¶ä¸­å‡ºç°çš„æ€§èƒ½é€€åŒ–é—®é¢˜ï¼Œè¯¥ç ”ç©¶æ·±å…¥æ¢è®¨äº†LoRAæ¨¡å‹åœ¨åˆå¹¶è¿‡ç¨‹ä¸­çš„å¹²æ‰°æœºåˆ¶ã€‚ä½œè€…æŒ‡å‡ºï¼Œæ€§èƒ½ä¸‹é™æºäºæ¨¡å‹å‚æ•°ä¸æ•°æ®åˆ†å¸ƒä¹‹é—´é•¿æœŸè¢«å¿½è§†çš„ç›¸äº’ä½œç”¨ï¼Œå¯¼è‡´ä¸åŒä»»åŠ¡çš„æƒé‡æ›´æ–°äº§ç”Ÿå†²çªã€‚ä¸ºæ­¤ï¼Œç ”ç©¶æå‡ºäº†OSRMï¼ˆOrthogonal Subspaces for Robust model Mergingï¼‰ï¼Œé€šè¿‡åœ¨å¾®è°ƒå‰å¯¹LoRAå­ç©ºé—´æ–½åŠ æ­£äº¤çº¦æŸï¼Œç¡®ä¿ç‰¹å®šä»»åŠ¡çš„å‚æ•°æ›´æ–°ä¸ä¼šè´Ÿé¢å½±å“å…¶ä»–ä»»åŠ¡çš„è¾“å‡ºã€‚è¯¥æ–¹æ³•èƒ½å¤Ÿä¸ç°æœ‰çš„åˆå¹¶ç®—æ³•æ— ç¼é›†æˆï¼Œä»æºå¤´ä¸Šå‡å°‘ä»»åŠ¡é—´çš„æ„å¤–å¹²æ‰°ã€‚åœ¨å…«ä¸ªæ•°æ®é›†ä»¥åŠå¤šç§è§„æ¨¡è¯­è¨€æ¨¡å‹ä¸Šçš„å®éªŒè¯æ˜ï¼ŒOSRMä¸ä»…æ˜¾è‘—æå‡äº†æ¨¡å‹åˆå¹¶åçš„æ€§èƒ½ï¼Œè¿˜å®Œæ•´ä¿ç•™äº†å•ä»»åŠ¡çš„åŸå§‹å‡†ç¡®æ€§ã€‚è¯¥ç ”ç©¶å¼ºè°ƒäº†ç®¡ç†æ•°æ®-å‚æ•°äº¤äº’çš„é‡è¦æ€§ï¼Œå¹¶ä¸ºLoRAæ¨¡å‹çš„ç¨³å¥åˆå¹¶æä¾›äº†ä¸€ç§é«˜æ•ˆçš„å³æ’å³ç”¨æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "14 pages, 5 figures, 16 tables, accepted by ACL 2025",
      "pdf_url": "https://arxiv.org/pdf/2505.22934v1",
      "published_date": "2025-05-28 23:28:12 UTC",
      "updated_date": "2025-05-28 23:28:12 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T15:07:15.608121+00:00"
    },
    {
      "arxiv_id": "2505.23851v1",
      "title": "ASyMOB: Algebraic Symbolic Mathematical Operations Benchmark",
      "title_zh": "ASyMOBï¼šä»£æ•°ç¬¦å·æ•°å­¦è¿ç®—åŸºå‡†",
      "authors": [
        "Michael Shalyt",
        "Rotem Elimelech",
        "Ido Kaminer"
      ],
      "abstract": "Large language models (LLMs) are rapidly approaching the level of proficiency in university-level symbolic mathematics required for applications in advanced science and technology. However, existing benchmarks fall short in assessing the core skills of LLMs in symbolic mathematics-such as integration, differential equations, and algebraic simplification. To address this gap, we introduce ASyMOB, a novel assessment framework focused exclusively on symbolic manipulation, featuring 17,092 unique math challenges, organized by similarity and complexity. ASyMOB enables analysis of LLM generalization capabilities by comparing performance in problems that differ by simple numerical or symbolic `perturbations'. Evaluated LLMs exhibit substantial degradation in performance for all perturbation types (up to -70.3%), suggesting reliance on memorized patterns rather than deeper understanding of symbolic math, even among models achieving high baseline accuracy. Comparing LLM performance to computer algebra systems, we identify examples where they fail while LLMs succeed, as well as problems solved only by combining both approaches. Models capable of integrated code execution yielded higher accuracy compared to their performance without code, particularly stabilizing weaker models (up to +33.1% for certain perturbation types). Notably, the most advanced models (o4-mini, Gemini 2.5 Flash) demonstrate not only high symbolic math proficiency (scoring 96.8% and 97.6% on the unperturbed set), but also remarkable robustness against perturbations, (-21.7% and -21.2% vs. average -50.4% for the other models). This may indicate a recent \"phase transition\" in the generalization capabilities of frontier LLMs. It remains to be seen whether the path forward lies in deeper integration with sophisticated external tools, or in developing models so capable that symbolic math systems like CAS become unnecessary.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¨å‡ºäº†ASyMOBï¼Œè¿™æ˜¯ä¸€ä¸ªä¸“é—¨ç”¨äºè¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹(LLMs)åœ¨ç§¯åˆ†ã€å¾®åˆ†æ–¹ç¨‹å’Œä»£æ•°ç®€åŒ–ç­‰ç¬¦å·æ•°å­¦(symbolic mathematics)é¢†åŸŸæ ¸å¿ƒèƒ½åŠ›çš„å…¨æ–°åŸºå‡†æµ‹è¯•ã€‚è¯¥æ¡†æ¶åŒ…å«17,092ä¸ªç‹¬ç‰¹çš„æ•°å­¦æŒ‘æˆ˜ï¼Œå¹¶é€šè¿‡å¼•å…¥æ•°å€¼æˆ–ç¬¦å·æ‰°åŠ¨(perturbations)æ¥åˆ†ææ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ï¼Œä»¥åŒºåˆ†å…¶æ˜¯ä¾èµ–æ·±åº¦ç†è§£è¿˜æ˜¯ä»…é æ¨¡å¼è®°å¿†ã€‚å®éªŒå‘ç°ï¼Œå¤§å¤šæ•°LLMsåœ¨é¢å¯¹æ‰°åŠ¨æ—¶æ€§èƒ½æ˜¾è‘—ä¸‹é™ï¼ˆæœ€é«˜è¾¾-70.3%ï¼‰ï¼Œä½†å…·å¤‡ä»£ç æ‰§è¡Œèƒ½åŠ›çš„æ¨¡å‹è¡¨ç°å‡ºæ›´é«˜çš„å‡†ç¡®æ€§å’Œç¨³å®šæ€§ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œå‰æ²¿æ¨¡å‹å¦‚o4-miniå’ŒGemini 2.5 Flashä¸ä»…è¡¨ç°å‡ºæé«˜çš„ç¬¦å·æ•°å­¦ç†Ÿç»ƒåº¦ï¼Œä¸”åœ¨åº”å¯¹æ‰°åŠ¨æ—¶å±•ç°å‡ºè¿œè¶…å…¶ä»–æ¨¡å‹çš„é²æ£’æ€§ã€‚è¿™ç§è¡¨ç°ä¸Šçš„è·¨è¶Šå¯èƒ½é¢„ç¤ºç€é¡¶å°–æ¨¡å‹åœ¨æ³›åŒ–èƒ½åŠ›ä¸Šæ­£åœ¨ç»å†ä¸€æ¬¡â€œç›¸å˜(phase transition)â€ã€‚è¯¥ç ”ç©¶ä¸ºæœªæ¥ç¬¦å·æ•°å­¦å¤„ç†æ˜¯èµ°å‘æ·±åº¦é›†æˆå¤–éƒ¨å·¥å…·(CAS)è¿˜æ˜¯å®Œå…¨ä¾é æ¨¡å‹è‡ªèº«èƒ½åŠ›æä¾›äº†é‡è¦è§è§£ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.SC"
      ],
      "primary_category": "cs.CL",
      "comment": "Code repository: https://github.com/RamanujanMachine/ASyMOB Complete benchmark dataset: https://huggingface.co/datasets/Shalyt/ASyMOB-Algebraic_Symbolic_Mathematical_Operations_Benchmark",
      "pdf_url": "https://arxiv.org/pdf/2505.23851v1",
      "published_date": "2025-05-28 23:11:14 UTC",
      "updated_date": "2025-05-28 23:11:14 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T15:07:40.739098+00:00"
    },
    {
      "arxiv_id": "2505.22928v2",
      "title": "Enhancing Study-Level Inference from Clinical Trial Papers via Reinforcement Learning-Based Numeric Reasoning",
      "title_zh": "åŸºäºå¼ºåŒ–å­¦ä¹ æ•°å€¼æ¨ç†çš„ä¸´åºŠè¯•éªŒè®ºæ–‡ç ”ç©¶çº§æ¨æ–­å¢å¼º",
      "authors": [
        "Massimiliano Pronesti",
        "Michela Lorandi",
        "Paul Flanagan",
        "Oisin Redmond",
        "Anya Belz",
        "Yufang Hou"
      ],
      "abstract": "Systematic reviews in medicine play a critical role in evidence-based decision-making by aggregating findings from multiple studies. A central bottleneck in automating this process is extracting numeric evidence and determining study-level conclusions for specific outcomes and comparisons. Prior work has framed this problem as a textual inference task by retrieving relevant content fragments and inferring conclusions from them. However, such approaches often rely on shallow textual cues and fail to capture the underlying numeric reasoning behind expert assessments.\n  In this work, we conceptualise the problem as one of quantitative reasoning. Rather than inferring conclusions from surface text, we extract structured numerical evidence (e.g., event counts or standard deviations) and apply domain knowledge informed logic to derive outcome-specific conclusions. We develop a numeric reasoning system composed of a numeric data extraction model and an effect estimate component, enabling more accurate and interpretable inference aligned with the domain expert principles. We train the numeric data extraction model using different strategies, including supervised fine-tuning (SFT) and reinforcement learning (RL) with a new value reward model.\n  When evaluated on the CochraneForest benchmark, our best-performing approach -- using RL to train a small-scale number extraction model -- yields up to a 21% absolute improvement in F1 score over retrieval-based systems and outperforms general-purpose LLMs of over 400B parameters by up to 9%. Our results demonstrate the promise of reasoning-driven approaches for automating systematic evidence synthesis.",
      "tldr_zh": "è¿™é¡¹ç ”ç©¶æ—¨åœ¨æ”¹è¿›ä¸´åºŠè¯•éªŒè®ºæ–‡çš„ç ”ç©¶çº§æ¨ç†ï¼Œè§£å†³äº†è‡ªåŠ¨åŒ–ç³»ç»Ÿè¯„ä»·ä¸­æå–æ•°å€¼è¯æ®å¹¶æ¨æ–­ç ”ç©¶ç»“è®ºçš„ç“¶é¢ˆé—®é¢˜ã€‚å…ˆå‰çš„æ–¹æ³•å¤šä¾èµ–è¡¨é¢æ–‡æœ¬çº¿ç´¢ï¼Œéš¾ä»¥æ•æ‰ä¸“å®¶è¯„ä¼°èƒŒåçš„æ•°å€¼æ¨ç†é€»è¾‘ã€‚è¯¥è®ºæ–‡æå‡ºäº†ä¸€ç§åŸºäºå®šé‡æ¨ç†çš„æ¡†æ¶ï¼Œé€šè¿‡æå–ç»“æ„åŒ–çš„æ•°å€¼æ•°æ®ï¼ˆå¦‚ event counts æˆ– standard deviationsï¼‰ï¼Œå¹¶ç»“åˆé¢†åŸŸçŸ¥è¯†é€»è¾‘æ¥æ¨å¯¼ç‰¹å®šç»“æœçš„ç»“è®ºã€‚è¯¥ç³»ç»Ÿç”±æ•°å€¼æ•°æ®æå–æ¨¡å‹å’Œæ•ˆåº”ä¼°è®¡ç»„ä»¶æ„æˆï¼Œå¹¶é‡‡ç”¨äº†ç›‘ç£å¾®è°ƒ (SFT) ä»¥åŠå¸¦æœ‰æ–°å‹æ•°å€¼å¥–åŠ±æ¨¡å‹çš„å¼ºåŒ–å­¦ä¹  (RL) ç­–ç•¥è¿›è¡Œè®­ç»ƒã€‚åœ¨ CochraneForest åŸºå‡†æµ‹è¯•ä¸­çš„è¯„ä¼°ç»“æœæ˜¾ç¤ºï¼Œä½¿ç”¨ RL è®­ç»ƒçš„å°è§„æ¨¡æ•°å€¼æå–æ¨¡å‹åœ¨ F1 score ä¸Šæ¯”åŸºäºæ£€ç´¢çš„ç³»ç»Ÿæé«˜äº† 21% çš„ç»å¯¹å€¼ã€‚è¯¥æ–¹æ³•çš„è¡¨ç°ç”šè‡³è¶…è¿‡äº†æ‹¥æœ‰è¶…è¿‡ 400B å‚æ•°çš„é€šç”¨å¤§è¯­è¨€æ¨¡å‹ (LLMs)ï¼Œæ€§èƒ½é¢†å…ˆé«˜è¾¾ 9%ã€‚ç ”ç©¶è¯æ˜äº†æ¨ç†é©±åŠ¨çš„æ–¹æ³•åœ¨è‡ªåŠ¨åŒ–ç³»ç»Ÿè¯æ®åˆæˆé¢†åŸŸçš„å·¨å¤§æ½œåŠ›ï¼Œä¸ºå®ç°æ›´å‡†ç¡®ä¸”å¯è§£é‡Šçš„ä¸´åºŠåŒ»å­¦å†³ç­–æ”¯æŒæä¾›äº†æ–°è·¯å¾„ã€‚",
      "categories": [
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.AI",
      "comment": "Accepted at EMNLP 2025 Main Conference",
      "pdf_url": "https://arxiv.org/pdf/2505.22928v2",
      "published_date": "2025-05-28 22:59:45 UTC",
      "updated_date": "2025-09-19 20:40:50 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T15:07:42.769138+00:00"
    },
    {
      "arxiv_id": "2505.22922v1",
      "title": "Scalable Parameter and Memory Efficient Pretraining for LLM: Recent Algorithmic Advances and Benchmarking",
      "title_zh": "LLM å¯æ‰©å±•å‚æ•°ä¸å†…å­˜é«˜æ•ˆé¢„è®­ç»ƒï¼šæœ€æ–°ç®—æ³•è¿›å±•ä¸åŸºå‡†æµ‹è¯•",
      "authors": [
        "Athanasios Glentis",
        "Jiaxiang Li",
        "Qiulin Shang",
        "Andi Han",
        "Ioannis Tsaknakis",
        "Quan Wei",
        "Mingyi Hong"
      ],
      "abstract": "Fueled by their remarkable ability to tackle diverse tasks across multiple domains, large language models (LLMs) have grown at an unprecedented rate, with some recent models containing trillions of parameters. This growth is accompanied by substantial computational challenges, particularly regarding the memory and compute resources required for training and fine-tuning. Numerous approaches have been explored to address these issues, such as LoRA. While these methods are effective for fine-tuning, their application to pre-training is significantly more challenging due to the need to learn vast datasets. Motivated by this issue, we aim to address the following questions: Can parameter- or memory-efficient methods enhance pre-training efficiency while achieving performance comparable to full-model training? How can the performance gap be narrowed? To this end, the contributions of this work are the following. (1) We begin by conducting a comprehensive survey that summarizes state-of-the-art methods for efficient pre-training. (2) We perform a benchmark evaluation of several representative memory efficient pre-training approaches to comprehensively evaluate their performance across model sizes. We observe that with a proper choice of optimizer and hyperparameters, full-rank training delivers the best performance, as expected. We also notice that incorporating high-rank updates in low-rank approaches is the key to improving their performance. (3) Finally, we propose two practical techniques, namely weight refactorization and momentum reset, to enhance the performance of efficient pre-training methods. We observe that applying these techniques to the low-rank method (on a 1B model) can achieve a lower perplexity than popular memory efficient algorithms such as GaLore and Fira, while simultaneously using about 25% less memory.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤§è¯­è¨€æ¨¡å‹ (LLMs) é¢„è®­ç»ƒè¿‡ç¨‹ä¸­å·¨å¤§çš„å†…å­˜å’Œè®¡ç®—èµ„æºæŒ‘æˆ˜ï¼Œç³»ç»Ÿæ€§åœ°è¯„ä¼°äº†å‚æ•°ä¸å†…å­˜æ•ˆç‡ (parameter- and memory-efficient) é¢„è®­ç»ƒç®—æ³•ã€‚é€šè¿‡å¯¹ç°æœ‰å‰æ²¿æ–¹æ³•çš„å…¨é¢ç»¼è¿°åŠè·¨æ¨¡å‹è§„æ¨¡çš„åŸºå‡†æµ‹è¯• (benchmarking)ï¼Œç ”ç©¶å‘ç°å…¨ç§©è®­ç»ƒ (full-rank training) åœ¨ç‰¹å®šä¼˜åŒ–ä¸‹è¡¨ç°æœ€ä¼˜ï¼Œè€Œé«˜ç§©æ›´æ–° (high-rank updates) çš„å¼•å…¥æ˜¯æå‡ä½ç§©é¢„è®­ç»ƒæ€§èƒ½çš„å…³é”®ã€‚ä¸ºæ­¤ï¼Œä½œè€…æå‡ºäº†æƒé‡é‡æ„ (weight refactorization) å’ŒåŠ¨é‡é‡ç½® (momentum reset) ä¸¤ç§å®ç”¨æŠ€æœ¯ï¼Œä»¥å¢å¼ºé«˜æ•ˆé¢„è®­ç»ƒçš„æ•ˆæœã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œåœ¨ 1B æ¨¡å‹ä¸Šåº”ç”¨è¿™äº›æŠ€æœ¯åï¼Œå…¶å›°æƒ‘åº¦ (perplexity) è¡¨ç°ä¼˜äº GaLore å’Œ Fira ç­‰æµè¡Œç®—æ³•ï¼ŒåŒæ—¶èŠ‚çœäº†çº¦ 25% çš„å†…å­˜ã€‚è¯¥å·¥ä½œä¸ºç¼©å°é«˜æ•ˆé¢„è®­ç»ƒä¸å…¨é‡è®­ç»ƒä¹‹é—´çš„æ€§èƒ½å·®è·æä¾›äº†é‡è¦çš„ç†è®ºä¾æ®ä¸æŠ€æœ¯æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.22922v1",
      "published_date": "2025-05-28 22:51:43 UTC",
      "updated_date": "2025-05-28 22:51:43 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T15:07:43.259381+00:00"
    },
    {
      "arxiv_id": "2506.00053v1",
      "title": "Improving statistical learning methods via features selection without replacement sampling and random projection",
      "title_zh": "é€šè¿‡æ— æ”¾å›é‡‡æ ·ç‰¹å¾é€‰æ‹©ä¸éšæœºæŠ•å½±æ”¹è¿›ç»Ÿè®¡å­¦ä¹ æ–¹æ³•",
      "authors": [
        "Sulaiman khan",
        "Muhammad Ahmad",
        "Fida Ullah",
        "Carlos Aguilar IbaÃ±ez",
        "JosÃ© Eduardo Valdez Rodriguez"
      ],
      "abstract": "Cancer is fundamentally a genetic disease characterized by genetic and epigenetic alterations that disrupt normal gene expression, leading to uncontrolled cell growth and metastasis. High-dimensional microarray datasets pose challenges for classification models due to the \"small n, large p\" problem, resulting in overfitting. This study makes three different key contributions: 1) we propose a machine learning-based approach integrating the Feature Selection Without Re-placement (FSWOR) technique and a projection method to improve classification accuracy. 2) We apply the Kendall statistical test to identify the most significant genes from the brain cancer mi-croarray dataset (GSE50161), reducing the feature space from 54,675 to 20,890 genes.3) we apply machine learning models using k-fold cross validation techniques in which our model incorpo-rates ensemble classifiers with LDA projection and NaÃ¯ve Bayes, achieving a test score of 96%, outperforming existing methods by 9.09%. The results demonstrate the effectiveness of our ap-proach in high-dimensional gene expression analysis, improving classification accuracy while mitigating overfitting. This study contributes to cancer biomarker discovery, offering a robust computational method for analyzing microarray data.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ç™Œç—‡åŸºå› è¡¨è¾¾å¾®é˜µåˆ—æ•°æ®ä¸­æ™®éå­˜åœ¨çš„â€œå°æ ·æœ¬ã€é«˜ç»´åº¦â€ï¼ˆsmall n, large pï¼‰å¯¼è‡´æ¨¡å‹è¿‡æ‹Ÿåˆçš„æŒ‘æˆ˜ï¼Œæå‡ºäº†ä¸€ç§æ”¹è¿›çš„ç»Ÿè®¡å­¦ä¹ æ–¹æ³•ã€‚ç ”ç©¶æ ¸å¿ƒç»“åˆäº†ä¸é‡å¤ç‰¹å¾é€‰æ‹©ï¼ˆFeature Selection Without Replacement, FSWORï¼‰æŠ€æœ¯ä¸æŠ•å½±æ–¹æ³•ï¼Œæ—¨åœ¨æå‡åˆ†ç±»å‡†ç¡®æ€§ã€‚ä½œè€…é¦–å…ˆåˆ©ç”¨ Kendall ç»Ÿè®¡æ£€éªŒå¯¹è„‘ç™Œå¾®é˜µåˆ—æ•°æ®é›†ï¼ˆGSE50161ï¼‰è¿›è¡Œç­›é€‰ï¼Œå°†ç‰¹å¾ç©ºé—´ä» 54,675 ä¸ªåŸºå› æœ‰æ•ˆå‹ç¼©è‡³ 20,890 ä¸ªæ˜¾è‘—åŸºå› ã€‚åœ¨åˆ†ç±»é˜¶æ®µï¼Œè¯¥æ¨¡å‹é›†æˆäº† LDA æŠ•å½±ã€NaÃ¯ve Bayes ä»¥åŠé›†æˆå­¦ä¹ ï¼ˆEnsemble Classifiersï¼‰æ–¹æ³•ï¼Œå¹¶é‡‡ç”¨ k-fold äº¤å‰éªŒè¯ç¡®ä¿è¯„ä¼°çš„ä¸¥è°¨æ€§ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯¥æ–¹æ³•è¾¾åˆ°äº† 96% çš„æµ‹è¯•å‡†ç¡®ç‡ï¼Œç›¸æ¯”ç°æœ‰æ–¹æ³•æå‡äº† 9.09%ã€‚è¯¥ç ”ç©¶ä¸ä»…æœ‰æ•ˆç¼“è§£äº†é«˜ç»´æ•°æ®åˆ†æä¸­çš„è¿‡æ‹Ÿåˆé—®é¢˜ï¼Œä¹Ÿä¸ºç™Œç—‡ç”Ÿç‰©æ ‡å¿—ç‰©ï¼ˆBiomarkerï¼‰çš„å‘ç°æä¾›äº†ä¸€ç§é²æ£’çš„è®¡ç®—æ‰‹æ®µã€‚",
      "categories": [
        "q-bio.QM",
        "cs.AI",
        "cs.LG",
        "stat.AP",
        "stat.ML"
      ],
      "primary_category": "q-bio.QM",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.00053v1",
      "published_date": "2025-05-28 22:36:46 UTC",
      "updated_date": "2025-05-28 22:36:46 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T15:08:09.013221+00:00"
    },
    {
      "arxiv_id": "2506.04243v1",
      "title": "Triple Attention Transformer Architecture for Time-Dependent Concrete Creep Prediction",
      "title_zh": "ç”¨äºæ—¶å˜æ··å‡åœŸå¾å˜é¢„æµ‹çš„ä¸‰é‡æ³¨æ„åŠ› Transformer æ¶æ„",
      "authors": [
        "Warayut Dokduea",
        "Weerachart Tangchirapat",
        "Sompote Youwai"
      ],
      "abstract": "This paper presents a novel Triple Attention Transformer Architecture for predicting time-dependent concrete creep, addressing fundamental limitations in current approaches that treat time as merely an input parameter rather than modeling the sequential nature of deformation development. By transforming concrete creep prediction into an autoregressive sequence modeling task similar to language processing, our architecture leverages the transformer's self-attention mechanisms to capture long-range dependencies in historical creep patterns. The model implements a triple-stream attention framework incorporating temporal attention for sequential progression, feature attention for material property interactions, and batch attention for inter-sample relationships. Evaluated on experimental datasets with standardized daily measurements spanning 160 days, the architecture achieves exceptional performance with mean absolute percentage error of 1.63% and R2 values of 0.999 across all datasets, substantially outperforming traditional empirical models and existing machine learning approaches. Ablation studies confirm the critical role of attention mechanisms, with attention pooling contributing most significantly to model performance. SHAP analysis reveals Young's modulus as the primary predictive feature, followed by density and compressive strength, providing interpretability essential for engineering applications. A deployed web-based interface facilitates practical implementation, enabling real-time predictions using standard laboratory parameters. This work establishes the viability of applying transformer architectures to materials science problems, demonstrating the potential for data-driven approaches to revolutionize structural behavior prediction and engineering design practices.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§åˆ›æ–°çš„Triple Attention Transformer Architectureï¼Œç”¨äºé¢„æµ‹éšæ—¶é—´å˜åŒ–çš„æ··å‡åœŸå¾å˜ï¼ˆconcrete creepï¼‰ï¼Œè§£å†³äº†ç°æœ‰æ–¹æ³•å°†æ—¶é—´ä»…è§†ä¸ºè¾“å…¥å‚æ•°è€Œéå»ºæ¨¡å˜å½¢æ¼”åŒ–åºåˆ—è¿‡ç¨‹çš„å±€é™æ€§ã€‚è¯¥æ¶æ„é€šè¿‡å°†å¾å˜é¢„æµ‹è½¬åŒ–ä¸ºè‡ªå›å½’åºåˆ—å»ºæ¨¡ï¼ˆautoregressive sequence modelingï¼‰ä»»åŠ¡ï¼Œåˆ©ç”¨ä¸‰é‡æµæ³¨æ„åŠ›æ¡†æ¶ï¼ˆtriple-stream attention frameworkï¼‰æ•´åˆäº†æ—¶é—´æ³¨æ„åŠ›ã€ç‰¹å¾æ³¨æ„åŠ›å’Œæ‰¹æ¬¡æ³¨æ„åŠ›ï¼Œä»¥æ•æ‰é•¿ç¨‹ä¾èµ–å…³ç³»å’Œææ–™å±æ€§é—´çš„å¤æ‚äº¤äº’ã€‚åœ¨è·¨è¶Š160å¤©çš„å®éªŒæ•°æ®é›†è¯„ä¼°ä¸­ï¼Œè¯¥æ¨¡å‹å®ç°äº†1.63%çš„å¹³å‡ç»å¯¹ç™¾åˆ†æ¯”è¯¯å·®ï¼ˆMAPEï¼‰å’Œ0.999çš„R2å€¼ï¼Œæ€§èƒ½æ˜¾è‘—ä¼˜äºä¼ ç»Ÿçš„ç»éªŒæ¨¡å‹å’Œç°æœ‰çš„æœºå™¨å­¦ä¹ æ–¹æ³•ã€‚æ¶ˆèå®éªŒï¼ˆablation studiesï¼‰è¯å®äº†æ³¨æ„åŠ›æœºåˆ¶åœ¨æå‡æ¨¡å‹æ€§èƒ½ä¸­çš„æ ¸å¿ƒä½œç”¨ï¼Œè€ŒSHAP analysisæ­ç¤ºäº†æ¨æ°æ¨¡é‡ï¼ˆYoung's modulusï¼‰ã€å¯†åº¦å’ŒæŠ—å‹å¼ºåº¦æ˜¯å…³é”®çš„é¢„æµ‹ç‰¹å¾ã€‚æ­¤å¤–ï¼Œç ”ç©¶è¿˜é€šè¿‡éƒ¨ç½²Webç•Œé¢å®ç°äº†åŸºäºæ ‡å‡†å®éªŒå®¤å‚æ•°çš„å®æ—¶é¢„æµ‹ï¼Œå±•ç¤ºäº†Transformeræ¶æ„åœ¨ææ–™ç§‘å­¦å’Œå·¥ç¨‹è®¾è®¡é¢†åŸŸå˜é©ç»“æ„è¡Œä¸ºé¢„æµ‹çš„å·¨å¤§æ½œåŠ›ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.04243v1",
      "published_date": "2025-05-28 22:30:35 UTC",
      "updated_date": "2025-05-28 22:30:35 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T15:08:29.557620+00:00"
    },
    {
      "arxiv_id": "2506.15700v1",
      "title": "Contraction Actor-Critic: Contraction Metric-Guided Reinforcement Learning for Robust Path Tracking",
      "title_zh": "Contraction Actor-Criticï¼šåŸºäºæ”¶ç¼©åº¦é‡å¼•å¯¼çš„é²æ£’è·¯å¾„è·Ÿè¸ªå¼ºåŒ–å­¦ä¹ ",
      "authors": [
        "Minjae Cho",
        "Hiroyasu Tsukamoto",
        "Huy Trong Tran"
      ],
      "abstract": "Control contraction metrics (CCMs) provide a framework to co-synthesize a controller and a corresponding contraction metric -- a positive-definite Riemannian metric under which a closed-loop system is guaranteed to be incrementally exponentially stable. However, the synthesized controller only ensures that all the trajectories of the system converge to one single trajectory and, as such, does not impose any notion of optimality across an entire trajectory. Furthermore, constructing CCMs requires a known dynamics model and non-trivial effort in solving an infinite-dimensional convex feasibility problem, which limits its scalability to complex systems featuring high dimensionality with uncertainty. To address these issues, we propose to integrate CCMs into reinforcement learning (RL), where CCMs provide dynamics-informed feedback for learning control policies that minimize cumulative tracking error under unknown dynamics. We show that our algorithm, called contraction actor-critic (CAC), formally enhances the capability of CCMs to provide a set of contracting policies with the long-term optimality of RL in a fully automated setting. Given a pre-trained dynamics model, CAC simultaneously learns a contraction metric generator (CMG) -- which generates a contraction metric -- and uses an actor-critic algorithm to learn an optimal tracking policy guided by that metric. We demonstrate the effectiveness of our algorithm relative to established baselines through extensive empirical studies, including simulated and real-world robot experiments, and provide a theoretical rationale for incorporating contraction theory into RL.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹æ§åˆ¶æ”¶ç¼©åº¦é‡ (Control contraction metrics, CCMs) åœ¨å¤„ç†æœªçŸ¥åŠ¨åŠ›å­¦æ¨¡å‹å’Œç¼ºä¹é•¿æœŸæœ€ä¼˜æ€§æ–¹é¢çš„å±€é™ï¼Œæå‡ºäº†æ”¶ç¼©è¡ŒåŠ¨è€…-è¯„è®ºå®¶ (Contraction Actor-Critic, CAC) æ¡†æ¶ï¼Œæ—¨åœ¨å®ç°é²æ£’çš„è·¯å¾„è·Ÿè¸ªã€‚è¯¥æ–¹æ³•å°†æ”¶ç¼©ç†è®ºé›†æˆåˆ°å¼ºåŒ–å­¦ä¹  (Reinforcement Learning, RL) ä¸­ï¼Œåˆ©ç”¨ CCMs æä¾›çš„åŠ¨åŠ›å­¦åé¦ˆæŒ‡å¯¼ç­–ç•¥å­¦ä¹ ï¼Œä»è€Œåœ¨åŠ¨åŠ›å­¦ä¸ç¡®å®šçš„æƒ…å†µä¸‹æœ€å°åŒ–ç´¯ç§¯è·Ÿè¸ªè¯¯å·®ã€‚é€šè¿‡ç»“åˆé¢„è®­ç»ƒçš„åŠ¨åŠ›å­¦æ¨¡å‹ï¼ŒCAC åŒæ—¶è®­ç»ƒæ”¶ç¼©åº¦é‡ç”Ÿæˆå™¨ (Contraction Metric Generator, CMG) å¹¶é‡‡ç”¨è¡ŒåŠ¨è€…-è¯„è®ºå®¶ç®—æ³•ï¼Œç¡®ä¿äº†é—­ç¯ç³»ç»Ÿåœ¨æ¸è¿›æŒ‡æ•°ç¨³å®šæ€§ (Incrementally Exponentially Stable) çº¦æŸä¸‹çš„é•¿æœŸæœ€ä¼˜æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥ç®—æ³•åœ¨ä»¿çœŸå’ŒçœŸå®æœºå™¨äººå®éªŒä¸­å‡ä¼˜äºç°æœ‰çš„åŸºå‡†æ¨¡å‹ï¼Œä¸ºæ”¶ç¼©ç†è®ºåœ¨å¼ºåŒ–å­¦ä¹ ä¸­çš„è‡ªåŠ¨åŒ–åº”ç”¨æä¾›äº†ç†è®ºæ”¯æ’‘ä¸å®è·µéªŒè¯ã€‚è¯¥ç ”ç©¶æœ‰æ•ˆæå‡äº†å¤æ‚é«˜ç»´ç³»ç»Ÿåœ¨ä¸ç¡®å®šç¯å¢ƒä¸‹çš„æ‰©å±•æ€§ä¸ç¨³å¥æ€§ï¼Œä¸ºå®ç°å¯è¯æ˜ç¨³å®šçš„è‡ªä¸»æ§åˆ¶å¥ å®šäº†åŸºç¡€ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.15700v1",
      "published_date": "2025-05-28 22:26:23 UTC",
      "updated_date": "2025-05-28 22:26:23 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T15:08:07.502112+00:00"
    },
    {
      "arxiv_id": "2505.22909v1",
      "title": "Learning to Charge More: A Theoretical Study of Collusion by Q-Learning Agents",
      "title_zh": "å­¦ä¼šæé«˜å®šä»·ï¼šQ å­¦ä¹ æ™ºèƒ½ä½“åˆè°‹è¡Œä¸ºçš„ç†è®ºç ”ç©¶",
      "authors": [
        "Cristian Chica",
        "Yinglong Guo",
        "Gilad Lerman"
      ],
      "abstract": "There is growing experimental evidence that $Q$-learning agents may learn to charge supracompetitive prices. We provide the first theoretical explanation for this behavior in infinite repeated games. Firms update their pricing policies based solely on observed profits, without computing equilibrium strategies. We show that when the game admits both a one-stage Nash equilibrium price and a collusive-enabling price, and when the $Q$-function satisfies certain inequalities at the end of experimentation, firms learn to consistently charge supracompetitive prices. We introduce a new class of one-memory subgame perfect equilibria (SPEs) and provide conditions under which learned behavior is supported by naive collusion, grim trigger policies, or increasing strategies. Naive collusion does not constitute an SPE unless the collusive-enabling price is a one-stage Nash equilibrium, whereas grim trigger policies can.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ Q-learning æ™ºèƒ½ä½“åœ¨æ— é™é‡å¤åšå¼ˆä¸­å¯èƒ½è¾¾æˆè¶…ç«äº‰ä»·æ ¼(supracompetitive prices)çš„å®éªŒç°è±¡ï¼Œé¦–æ¬¡åœ¨ç†è®ºå±‚é¢æä¾›äº†ç³»ç»Ÿæ€§è§£é‡Šã€‚ç ”ç©¶å‡è®¾ä¼ä¸šä»…ä¾æ®è§‚æµ‹åˆ©æ¶¦æ›´æ–°å®šä»·ç­–ç•¥ï¼Œè€Œæ— éœ€è®¡ç®—å¤æ‚çš„å‡è¡¡çŠ¶æ€ã€‚åˆ†æè¡¨æ˜ï¼Œå½“åšå¼ˆç¯å¢ƒä¸­åŒæ—¶å­˜åœ¨å•é˜¶æ®µ Nash equilibrium ä»·æ ¼ä¸ååŒä½¿èƒ½ä»·æ ¼(collusive-enabling price)ï¼Œä¸” Q-function åœ¨å®éªŒåæœŸæ»¡è¶³ç‰¹å®šæ•°å­¦ä¸ç­‰å¼æ—¶ï¼Œæ™ºèƒ½ä½“å°†å­¦ä¼šæŒç»­æ”¶å–é«˜äºç«äº‰æ°´å¹³çš„ä»·æ ¼ã€‚ä½œè€…å¼•å…¥äº†ä¸€ç±»æ–°å‹çš„å•è®°å¿†å­åšå¼ˆå®Œç¾å‡è¡¡(one-memory subgame perfect equilibria, SPEs)ï¼Œç”¨äºç•Œå®šä¹ å¾—è¡Œä¸ºåœ¨ naive collusionã€grim trigger ç­–ç•¥æˆ–é€’å¢ç­–ç•¥ä¸‹çš„æ”¯æŒæ¡ä»¶ã€‚ç ”ç©¶è¿›ä¸€æ­¥æ­ç¤ºï¼Œè™½ç„¶ naive collusion åœ¨å¤šæ•°æƒ…å†µä¸‹ä¸æ„æˆ SPEï¼Œä½† grim trigger ç­–ç•¥èƒ½å¤Ÿæœ‰æ•ˆæ”¯æŒååŒè¡Œä¸ºçš„ç¨³å®šæ€§ã€‚",
      "categories": [
        "econ.GN",
        "cs.AI",
        "cs.GT"
      ],
      "primary_category": "econ.GN",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.22909v1",
      "published_date": "2025-05-28 22:18:35 UTC",
      "updated_date": "2025-05-28 22:18:35 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T15:08:18.285636+00:00"
    },
    {
      "arxiv_id": "2505.22906v2",
      "title": "HiLDe: Intentional Code Generation via Human-in-the-Loop Decoding",
      "title_zh": "HiLDeï¼šåŸºäºäººæœºå›ç¯è§£ç çš„æ„å›¾å¯¼å‘ä»£ç ç”Ÿæˆ",
      "authors": [
        "Emmanuel Anaya GonzÃ¡lez",
        "Raven Rothkopf",
        "Sorin Lerner",
        "Nadia Polikarpova"
      ],
      "abstract": "While AI programming tools hold the promise of increasing programmers' capabilities and productivity to a remarkable degree, they often exclude users from essential decision-making processes, causing many to effectively \"turn off their brains\" and over-rely on solutions provided by these systems. These behaviors can have severe consequences in critical domains, like software security. We propose Human-in-the-loop Decoding, a novel interaction technique that allows users to observe and directly influence LLM decisions during code generation, in order to align the model's output with their personal requirements. We implement this technique in HiLDe, a code completion assistant that highlights critical decisions made by the LLM and provides local alternatives for the user to explore. In a within-subjects study (N=18) on security-related tasks, we found that HiLDe led participants to generate significantly fewer vulnerabilities and better align code generation with their goals compared to a traditional code completion assistant.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ AI ç¼–ç¨‹å·¥å…·å› ç¼ºä¹ç”¨æˆ·å‚ä¸å†³ç­–è€Œå¯¼è‡´çš„è¿‡åº¦ä¾èµ–åŠå®‰å…¨é£é™©ï¼Œæå‡ºäº† Human-in-the-loop Decoding è¿™ä¸€æ–°å‹äº¤äº’æŠ€æœ¯ã€‚é€šè¿‡è¯¥æŠ€æœ¯ï¼Œç”¨æˆ·å¯ä»¥å®æ—¶è§‚å¯Ÿå¹¶å¹²é¢„ LLM åœ¨ä»£ç ç”Ÿæˆè¿‡ç¨‹ä¸­çš„å†³ç­–ï¼Œä»è€Œä½¿è¾“å‡ºç»“æœæ›´ç¬¦åˆä¸ªäººéœ€æ±‚ã€‚ç ”ç©¶è€…æ®æ­¤å¼€å‘äº†ä»£ç è¡¥å…¨åŠ©æ‰‹ HiLDeï¼Œå®ƒèƒ½è¯†åˆ«å¹¶çªå‡ºæ˜¾ç¤º LLM çš„å…³é”®å†³ç­–ç‚¹ï¼Œæä¾›å¤šæ ·åŒ–çš„æœ¬åœ°æ›¿ä»£æ–¹æ¡ˆä¾›ç”¨æˆ·å‚è€ƒã€‚åœ¨ 18 åå‚ä¸è€…é’ˆå¯¹å®‰å…¨ç›¸å…³ä»»åŠ¡è¿›è¡Œçš„å¯¹æ¯”ç ”ç©¶ä¸­ï¼Œç»“æœè¯æ˜ HiLDe æ˜¾è‘—å‡å°‘äº†ä»£ç æ¼æ´çš„äº§ç”Ÿï¼Œå¹¶ä½¿ç”Ÿæˆçš„ä»£ç èƒ½å¤Ÿæ›´å¥½åœ°ä¸ç”¨æˆ·ç›®æ ‡ä¿æŒå¯¹é½ã€‚è¯¥é¡¹å·¥ä½œå±•ç¤ºäº†å°†äººç±»æ„å›¾èå…¥è§£ç è¿‡ç¨‹åœ¨æå‡è½¯ä»¶å®‰å…¨æ€§å’Œä»»åŠ¡å‡†ç¡®æ€§æ–¹é¢çš„å·¨å¤§æ½œåŠ›ã€‚",
      "categories": [
        "cs.HC",
        "cs.AI",
        "cs.PL"
      ],
      "primary_category": "cs.HC",
      "comment": "10 pages, 6 figures",
      "pdf_url": "https://arxiv.org/pdf/2505.22906v2",
      "published_date": "2025-05-28 22:11:17 UTC",
      "updated_date": "2025-05-30 18:45:49 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T15:08:12.839595+00:00"
    },
    {
      "arxiv_id": "2505.22904v2",
      "title": "Defining Foundation Models for Computational Science: A Call for Clarity and Rigor",
      "title_zh": "ç•Œå®šè®¡ç®—ç§‘å­¦ä¸­çš„åŸºç¡€æ¨¡å‹ï¼šå¯¹æ˜ç¡®æ€§ä¸ä¸¥è°¨æ€§çš„å‘¼å",
      "authors": [
        "Youngsoo Choi",
        "Siu Wun Cheung",
        "Youngkyu Kim",
        "Ping-Hsuan Tsai",
        "Alejandro N. Diaz",
        "Ivan Zanardi",
        "Seung Whan Chung",
        "Dylan Matthew Copeland",
        "Coleman Kendrick",
        "William Anderson",
        "Traian Iliescu",
        "Matthias Heinkenschloss"
      ],
      "abstract": "The widespread success of foundation models in natural language processing and computer vision has inspired researchers to extend the concept to scientific machine learning and computational science. However, this position paper argues that as the term \"foundation model\" is an evolving concept, its application in computational science is increasingly used without a universally accepted definition, potentially creating confusion and diluting its precise scientific meaning. In this paper, we address this gap by proposing a formal definition of foundation models in computational science, grounded in the core values of generality, reusability, and scalability. We articulate a set of essential and desirable characteristics that such models must exhibit, drawing parallels with traditional foundational methods, like the finite element and finite volume methods. Furthermore, we introduce the Data-Driven Finite Element Method (DD-FEM), a framework that fuses the modular structure of classical FEM with the representational power of data-driven learning. We demonstrate how DD-FEM addresses many of the key challenges in realizing foundation models for computational science, including scalability, adaptability, and physics consistency. By bridging traditional numerical methods with modern AI paradigms, this work provides a rigorous foundation for evaluating and developing novel approaches toward future foundation models in computational science.",
      "tldr_zh": "æœ¬æ–‡é’ˆå¯¹åŸºç¡€æ¨¡å‹(Foundation Models)åœ¨è®¡ç®—ç§‘å­¦ä¸­ç¼ºä¹ç»Ÿä¸€æ­£å¼å®šä¹‰å¯¼è‡´çš„æ¦‚å¿µæ¨¡ç³Šé—®é¢˜ï¼Œæå‡ºäº†ä¸€å¥—åŸºäºé€šç”¨æ€§(Generality)ã€å¯é‡ç”¨æ€§(Reusability)å’Œå¯æ‰©å±•æ€§(Scalability)çš„æ­£å¼å®šä¹‰ã€‚ä½œè€…é€šè¿‡ç±»æ¯”æœ‰é™å…ƒ(Finite Element Method)ç­‰ä¼ ç»Ÿæ•°å€¼æ–¹æ³•ï¼Œé˜è¿°äº†è®¡ç®—ç§‘å­¦åŸºç¡€æ¨¡å‹åº”å…·å¤‡çš„æ ¸å¿ƒä¸ç†æƒ³ç‰¹å¾ã€‚ä¸ºæ­¤ï¼Œç ”ç©¶å¼•å…¥äº†æ•°æ®é©±åŠ¨æœ‰é™å…ƒæ³•(Data-Driven Finite Element Method, DD-FEM)æ¡†æ¶ï¼Œè¯¥æ¡†æ¶æœ‰æœºèåˆäº†ç»å…¸FEMçš„æ¨¡å—åŒ–ç»“æ„ä¸æ•°æ®é©±åŠ¨å­¦ä¹ çš„å¼ºå¤§è¡¨è¾¾èƒ½åŠ›ã€‚å®éªŒè¡¨æ˜ï¼ŒDD-FEMèƒ½å¤Ÿæœ‰æ•ˆåº”å¯¹åŸºç¡€æ¨¡å‹åœ¨å¯æ‰©å±•æ€§ã€é€‚åº”æ€§å’Œç‰©ç†ä¸€è‡´æ€§(Physics Consistency)ç­‰æ–¹é¢çš„å…³é”®æŒ‘æˆ˜ã€‚è¯¥ç ”ç©¶é€šè¿‡æ¡¥æ¥ä¼ ç»Ÿæ•°å€¼æ–¹æ³•ä¸ç°ä»£äººå·¥æ™ºèƒ½èŒƒå¼ï¼Œä¸ºæœªæ¥è®¡ç®—ç§‘å­¦åŸºç¡€æ¨¡å‹çš„å¼€å‘ä¸è¯„ä¼°å¥ å®šäº†ä¸¥è°¨çš„ç†è®ºåŸºç¡€ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "math.NA"
      ],
      "primary_category": "cs.LG",
      "comment": "26 pages, 2 tables, 7 figures",
      "pdf_url": "https://arxiv.org/pdf/2505.22904v2",
      "published_date": "2025-05-28 22:10:16 UTC",
      "updated_date": "2025-05-30 16:21:57 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T15:08:42.378257+00:00"
    },
    {
      "arxiv_id": "2506.15699v1",
      "title": "BLUR: A Benchmark for LLM Unlearning Robust to Forget-Retain Overlap",
      "title_zh": "BLURï¼šå¯¹é—å¿˜-ä¿ç•™é‡å å…·æœ‰é²æ£’æ€§çš„å¤§è¯­è¨€æ¨¡å‹å¸è½½åŸºå‡†æµ‹è¯•",
      "authors": [
        "Shengyuan Hu",
        "Neil Kale",
        "Pratiksha Thaker",
        "Yiwei Fu",
        "Steven Wu",
        "Virginia Smith"
      ],
      "abstract": "Machine unlearning has the potential to improve the safety of large language models (LLMs) by removing sensitive or harmful information post hoc. A key challenge in unlearning involves balancing between forget quality (effectively unlearning undesirable information) and retain quality (maintaining good performance on other, general tasks). Unfortunately, as we show, current LLM unlearning benchmarks contain highly disparate forget and retain sets -- painting a false picture of the effectiveness of LLM unlearning methods. This can be particularly problematic because it opens the door for benign perturbations, such as relearning attacks, to easily reveal supposedly unlearned knowledge once models are deployed. To address this, we present $\\texttt{BLUR}$: a benchmark for LLM unlearning that provides more realistic scenarios of forget-retain overlap. $\\texttt{BLUR}$ significantly expands on existing unlearning benchmarks by providing extended evaluation tasks, combined forget/retain queries, and relearning datasets of varying degrees of difficulty. Despite the benign nature of the queries considered, we find that the performance of existing methods drops significantly when evaluated on $\\texttt{BLUR}$, with simple approaches performing better on average than more recent methods. These results highlight the importance of robust evaluation and suggest several important directions of future study. Our benchmark is publicly available at: https://huggingface.co/datasets/forgelab/BLUR",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†BLURï¼Œä¸€ä¸ªé’ˆå¯¹å¤§è¯­è¨€æ¨¡å‹(LLM)æœºå™¨é—å¿˜(Machine Unlearning)çš„åŸºå‡†æµ‹è¯•ï¼Œæ—¨åœ¨è§£å†³ç°æœ‰è¯„ä¼°ä¸­é—å¿˜é›†ä¸ä¿ç•™é›†è¿‡äºåˆ†ç¦»å¯¼è‡´çš„æ–¹æ³•æœ‰æ•ˆæ€§è™šå‡ç¹è£é—®é¢˜ã€‚BLURé€šè¿‡å¼•å…¥æ›´å…·ç°å®æ„ä¹‰çš„é—å¿˜-ä¿ç•™é‡å (Forget-Retain Overlap)åœºæ™¯ï¼Œæä¾›äº†æ‰©å±•çš„è¯„ä¼°ä»»åŠ¡ã€ç»„åˆæŸ¥è¯¢ä»¥åŠä¸åŒéš¾åº¦çš„é‡å­¦ä¹ (Relearning)æ•°æ®é›†ï¼Œä»è€Œæ›´çœŸå®åœ°è¡¡é‡æ¨¡å‹åœ¨ç§»é™¤æ•æ„Ÿä¿¡æ¯çš„åŒæ—¶ä¿æŒé€šç”¨èƒ½åŠ›çš„æ•ˆæœã€‚å®éªŒå‘ç°ï¼Œç°æœ‰ä¸»æµé—å¿˜æ–¹æ³•åœ¨BLURä¸Šçš„æ€§èƒ½æ˜¾è‘—ä¸‹é™ï¼Œç”šè‡³ç®€å•çš„åŸºç¡€æ–¹æ³•åœ¨å¹³å‡è¡¨ç°ä¸Šä¼˜äºè¿‘å¹´æ¥çš„å¤æ‚ç®—æ³•ã€‚è¿™ä¸€å‘ç°æ­ç¤ºäº†å½“å‰è¯„ä¼°ä½“ç³»çš„è„†å¼±æ€§ï¼Œå¹¶å¼ºè°ƒäº†å¼€å‘é²æ£’æ€§è¯„ä¼°å·¥å…·å¯¹äºç¡®ä¿LLMå®‰å…¨æ€§å’Œå¯éƒ¨ç½²æ€§çš„å…³é”®ä½œç”¨ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.15699v1",
      "published_date": "2025-05-28 22:09:04 UTC",
      "updated_date": "2025-05-28 22:09:04 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T15:08:24.060307+00:00"
    },
    {
      "arxiv_id": "2505.22889v2",
      "title": "Local Stability and Region of Attraction Analysis for Neural Network Feedback Systems under Positivity Constraints",
      "title_zh": "æ­£æ€§çº¦æŸä¸‹ç¥ç»ç½‘ç»œåé¦ˆç³»ç»Ÿçš„å±€éƒ¨ç¨³å®šæ€§ä¸å¸å¼•åŸŸåˆ†æ",
      "authors": [
        "Hamidreza Montazeri Hedesh",
        "Moh Kamalul Wafi",
        "Milad Siami"
      ],
      "abstract": "We study the local stability of nonlinear systems in the Lur'e form with static nonlinear feedback realized by feedforward neural networks (FFNNs). By leveraging positivity system constraints, we employ a localized variant of the Aizerman conjecture, which provides sufficient conditions for exponential stability of trajectories confined to a compact set. Using this foundation, we develop two distinct methods for estimating the Region of Attraction (ROA): (i) a less conservative Lyapunov-based approach that constructs invariant sublevel sets of a quadratic function satisfying a linear matrix inequality (LMI), and (ii) a novel technique for computing tight local sector bounds for FFNNs via layer-wise propagation of linear relaxations. These bounds are integrated into the localized Aizerman framework to certify local exponential stability. Numerical results demonstrate substantial improvements over existing integral quadratic constraint-based approaches in both ROA size and scalability.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†å—å‰é¦ˆç¥ç»ç½‘ç»œ (FFNNs) åé¦ˆæ§åˆ¶çš„ Lur'e å½¢å¼éçº¿æ€§ç³»ç»Ÿçš„å±€éƒ¨ç¨³å®šæ€§åˆ†æé—®é¢˜ã€‚é€šè¿‡å¼•å…¥æ­£ç³»ç»Ÿ (positivity system) çº¦æŸï¼Œç ”ç©¶åˆ©ç”¨å±€éƒ¨åŒ–çš„ Aizerman çŒœæƒ³ä¸ºç´§è‡´é›†å†…çš„è½¨è¿¹æä¾›äº†æŒ‡æ•°ç¨³å®šæ€§çš„å……åˆ†æ¡ä»¶ã€‚ä½œè€…æå‡ºäº†ä¸¤ç§ä¼°è®¡å¸å¼•åŸŸ (Region of Attraction, ROA) çš„æ–¹æ³•ï¼šä¸€ç§æ˜¯åŸºäº Lyapunov ç¨³å®šæ€§ç†è®ºå¹¶åˆ©ç”¨çº¿æ€§çŸ©é˜µä¸ç­‰å¼ (LMI) æ„é€ ä¸å˜å­æ°´å¹³é›†ï¼›å¦ä¸€ç§æ˜¯é€šè¿‡çº¿æ€§æ¾å¼›çš„é€å±‚ä¼ æ’­ (layer-wise propagation of linear relaxations) æ¥è®¡ç®— FFNNs çš„ç´§è‡´å±€éƒ¨æ‰‡åŒºè¾¹ç•Œ (local sector bounds)ã€‚å®éªŒç»“æœè¯æ˜ï¼Œè¯¥æ–¹æ³•åœ¨ ROA ä¼°è®¡èŒƒå›´å’Œç®—æ³•çš„å¯æ‰©å±•æ€§ (scalability) æ–¹é¢å‡æ˜¾è‘—ä¼˜äºç°æœ‰çš„åŸºäºç§¯åˆ†äºŒæ¬¡çº¦æŸ (IQC) çš„åˆ†ææ–¹æ³•ã€‚",
      "categories": [
        "eess.SY",
        "cs.AI"
      ],
      "primary_category": "eess.SY",
      "comment": "Accepted at 64th IEEE Conference on Decision and Control (CDC) 2025 - Rio de Janeiro, Brazil",
      "pdf_url": "https://arxiv.org/pdf/2505.22889v2",
      "published_date": "2025-05-28 21:45:49 UTC",
      "updated_date": "2025-10-03 22:24:15 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T15:08:32.908101+00:00"
    },
    {
      "arxiv_id": "2505.22880v3",
      "title": "Semantic Exploration and Dense Mapping of Complex Environments using Ground Robot with Panoramic LiDAR-Camera Fusion",
      "title_zh": "åŸºäºå…¨æ™¯æ¿€å…‰é›·è¾¾-ç›¸æœºèåˆçš„åœ°é¢æœºå™¨äººå¤æ‚ç¯å¢ƒè¯­ä¹‰æ¢ç´¢ä¸ç¨ å¯†å»ºå›¾",
      "authors": [
        "Xiaoyang Zhan",
        "Shixin Zhou",
        "Qianqian Yang",
        "Yixuan Zhao",
        "Hao Liu",
        "Srinivas Chowdary Ramineni",
        "Kenji Shimada"
      ],
      "abstract": "This paper presents a system for autonomous semantic exploration and dense semantic target mapping of a complex unknown environment using a ground robot equipped with a LiDAR-panoramic camera suite. Existing approaches often struggle to balance collecting high-quality observations from multiple view angles and avoiding unnecessary repetitive traversal. To fill this gap, we propose a complete system combining mapping and planning. We first redefine the task as completing both geometric coverage and semantic viewpoint observation. We then manage semantic and geometric viewpoints separately and propose a novel Priority-driven Decoupled Local Sampler to generate local viewpoint sets. This enables explicit multi-view semantic inspection and voxel coverage without unnecessary repetition. Building on this, we develop a hierarchical planner to ensure efficient global coverage. In addition, we propose a Safe Aggressive Exploration State Machine, which allows aggressive exploration behavior while ensuring the robot's safety. Our system includes a plug-and-play semantic target mapping module that integrates seamlessly with state-of-the-art SLAM algorithms for pointcloud-level dense semantic target mapping. We validate our approach through extensive experiments in both realistic simulations and complex real-world environments. Simulation results show that our planner achieves faster exploration and shorter travel distances while guaranteeing a specified number of multi-view inspections. Real-world experiments further confirm the system's effectiveness in achieving accurate dense semantic object mapping of unstructured environments.",
      "tldr_zh": "æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§é¢å‘å¤æ‚æœªçŸ¥ç¯å¢ƒçš„è‡ªä¸»è¯­ä¹‰æ¢ç´¢ä¸ç¨ å¯†è¯­ä¹‰ç›®æ ‡å»ºæ¨¡ç³»ç»Ÿï¼Œåˆ©ç”¨é…å¤‡ LiDAR-panoramic camera çš„åœ°é¢æœºå™¨äººå®ç°é«˜æ•ˆä½œä¸šã€‚é’ˆå¯¹ç°æœ‰æ–¹æ³•åœ¨å¤šè§†è§’è§‚æµ‹è´¨é‡ä¸é‡å¤è·¯å¾„è§„é¿ä¹‹é—´çš„å¹³è¡¡éš¾é¢˜ï¼Œè¯¥ç³»ç»Ÿå°†ä»»åŠ¡é‡æ–°å®šä¹‰ä¸ºå‡ ä½•è¦†ç›–(geometric coverage)ä¸è¯­ä¹‰è§†è§’è§‚æµ‹(semantic viewpoint observation)çš„ç»“åˆã€‚ç ”ç©¶æå‡ºäº†ä¼˜å…ˆçº§é©±åŠ¨çš„è§£è€¦å±€éƒ¨é‡‡æ ·å™¨(Priority-driven Decoupled Local Sampler)æ¥ç”Ÿæˆå±€éƒ¨è§†è§’ï¼Œåœ¨å®ç°å¤šè§†è§’è¯­ä¹‰æ£€æŸ¥å’Œä½“ç´ è¦†ç›–çš„åŒæ—¶å‡å°‘é‡å¤ç§»åŠ¨ã€‚é€šè¿‡åˆ†å±‚è§„åˆ’å™¨(hierarchical planner)å’Œå®‰å…¨æ¿€è¿›æ¢ç´¢çŠ¶æ€æœº(Safe Aggressive Exploration State Machine)ï¼Œç³»ç»Ÿåœ¨ä¿è¯æœºå™¨äººå®‰å…¨çš„å‰æä¸‹å®ç°äº†é«˜æ•ˆçš„å…¨å±€è¦†ç›–ã€‚è¯¥æ–¹æ¡ˆè¿˜åŒ…å«ä¸€ä¸ªå³æ’å³ç”¨çš„è¯­ä¹‰ç›®æ ‡åˆ¶å›¾æ¨¡å—ï¼Œèƒ½å¤Ÿä¸å…ˆè¿›çš„ SLAM ç®—æ³•é›†æˆï¼Œç”Ÿæˆç‚¹äº‘çº§çš„ç¨ å¯†è¯­ä¹‰åœ°å›¾ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥ç³»ç»Ÿåœ¨ä»¿çœŸå’ŒçœŸå®å¤æ‚ç¯å¢ƒä¸­å‡èƒ½ä»¥æ›´çŸ­çš„è·¯å¾„å’Œæ›´å¿«çš„é€Ÿåº¦å®Œæˆæ¢ç´¢ï¼Œå¹¶å®ç°äº†å¯¹éç»“æ„åŒ–ç¯å¢ƒçš„ç²¾ç¡®è¯­ä¹‰åˆ¶å›¾ã€‚",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "comment": "Accepted by IEEE Robotics and Automation Letters",
      "pdf_url": "https://arxiv.org/pdf/2505.22880v3",
      "published_date": "2025-05-28 21:27:32 UTC",
      "updated_date": "2025-09-17 21:00:21 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T15:08:39.364063+00:00"
    },
    {
      "arxiv_id": "2505.22878v1",
      "title": "BugWhisperer: Fine-Tuning LLMs for SoC Hardware Vulnerability Detection",
      "title_zh": "BugWhispererï¼šé¢å‘ SoC ç¡¬ä»¶æ¼æ´æ£€æµ‹çš„å¤§è¯­è¨€æ¨¡å‹å¾®è°ƒ",
      "authors": [
        "Shams Tarek",
        "Dipayan Saha",
        "Sujan Kumar Saha",
        "Farimah Farahmandi"
      ],
      "abstract": "The current landscape of system-on-chips (SoCs) security verification faces challenges due to manual, labor-intensive, and inflexible methodologies. These issues limit the scalability and effectiveness of security protocols, making bug detection at the Register-Transfer Level (RTL) difficult. This paper proposes a new framework named BugWhisperer that utilizes a specialized, fine-tuned Large Language Model (LLM) to address these challenges. By enhancing the LLM's hardware security knowledge and leveraging its capabilities for text inference and knowledge transfer, this approach automates and improves the adaptability and reusability of the verification process. We introduce an open-source, fine-tuned LLM specifically designed for detecting security vulnerabilities in SoC designs. Our findings demonstrate that this tailored LLM effectively enhances the efficiency and flexibility of the security verification process. Additionally, we introduce a comprehensive hardware vulnerability database that supports this work and will further assist the research community in enhancing the security verification process.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ç³»ç»ŸèŠ¯ç‰‡(SoCs)å®‰å…¨éªŒè¯ä¸­å­˜åœ¨çš„äººå·¥å¯†é›†ã€ç¼ºä¹çµæ´»æ€§å’Œæ‰©å±•æ€§å·®ç­‰æŒ‘æˆ˜ï¼Œæå‡ºäº†BugWhispereræ¡†æ¶ã€‚è¯¥æ¡†æ¶é€šè¿‡å¾®è°ƒä¸“é—¨çš„å¤§è¯­è¨€æ¨¡å‹(LLMs)ï¼Œæ—¨åœ¨è§£å†³å¯„å­˜å™¨ä¼ è¾“çº§(RTL)ç¡¬ä»¶æ¼æ´æ£€æµ‹éš¾é¢˜ã€‚BugWhispererå……åˆ†åˆ©ç”¨LLMçš„æ–‡æœ¬æ¨ç†å’ŒçŸ¥è¯†è¿ç§»èƒ½åŠ›ï¼Œæœ‰æ•ˆå¢å¼ºäº†æ¨¡å‹çš„ç¡¬ä»¶å®‰å…¨ä¸“ä¸šçŸ¥è¯†ï¼Œå¹¶å®ç°äº†éªŒè¯æµç¨‹çš„è‡ªåŠ¨åŒ–ã€‚ç ”ç©¶å›¢é˜Ÿæ¨å‡ºäº†ä¸€æ¬¾ä¸“é—¨è®¾è®¡çš„å¼€æºå¾®è°ƒLLMï¼Œèƒ½å¤Ÿç²¾å‡†è¯†åˆ«SoCè®¾è®¡ä¸­çš„å®‰å…¨æ¼æ´ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯¥å®šåˆ¶åŒ–LLMæ˜¾è‘—å¢å¼ºäº†å®‰å…¨éªŒè¯è¿‡ç¨‹çš„æ•ˆç‡ä¸çµæ´»æ€§ã€‚æ­¤å¤–ï¼Œè¯¥ç ”ç©¶è¿˜è´¡çŒ®äº†ä¸€ä¸ªå…¨é¢çš„ç¡¬ä»¶æ¼æ´æ•°æ®åº“ï¼Œä¸ºç ”ç©¶ç¤¾åŒºåœ¨æå‡å®‰å…¨éªŒè¯æµç¨‹æ–¹é¢æä¾›äº†é‡è¦èµ„æºå’Œæœ‰åŠ›æ”¯æŒã€‚",
      "categories": [
        "cs.CR",
        "cs.AI"
      ],
      "primary_category": "cs.CR",
      "comment": "This paper was presented at IEEE VLSI Test Symposium (VTS) 2025",
      "pdf_url": "https://arxiv.org/pdf/2505.22878v1",
      "published_date": "2025-05-28 21:25:06 UTC",
      "updated_date": "2025-05-28 21:25:06 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T15:08:40.544949+00:00"
    },
    {
      "arxiv_id": "2505.23849v2",
      "title": "CADRE: Customizable Assurance of Data Readiness in Privacy-Preserving Federated Learning",
      "title_zh": "CADREï¼šéšç§ä¿æŠ¤è”é‚¦å­¦ä¹ ä¸­å¯å®šåˆ¶çš„æ•°æ®å°±ç»ªæ€§ä¿éšœ",
      "authors": [
        "Kaveen Hiniduma",
        "Zilinghan Li",
        "Aditya Sinha",
        "Ravi Madduri",
        "Suren Byna"
      ],
      "abstract": "Privacy-Preserving Federated Learning (PPFL) is a decentralized machine learning approach where multiple clients train a model collaboratively. PPFL preserves the privacy and security of a client's data without exchanging it. However, ensuring that data at each client is of high quality and ready for federated learning (FL) is a challenge due to restricted data access. In this paper, we introduce CADRE (Customizable Assurance of Data Readiness) for federated learning (FL), a novel framework that allows users to define custom data readiness (DR) metrics, rules, and remedies tailored to specific FL tasks. CADRE generates comprehensive DR reports based on the user-defined metrics, rules, and remedies to ensure datasets are prepared for FL while preserving privacy. We demonstrate a practical application of CADRE by integrating it into an existing PPFL framework. We conducted experiments across six datasets and addressed seven different DR issues. The results illustrate the versatility and effectiveness of CADRE in ensuring DR across various dimensions, including data quality, privacy, and fairness. This approach enhances the performance and reliability of FL models as well as utilizes valuable resources.",
      "tldr_zh": "éšç§ä¿æŠ¤è”é‚¦å­¦ä¹  (Privacy-Preserving Federated Learning, PPFL) æ—¨åœ¨ä¸äº¤æ¢æ•°æ®çš„æƒ…å†µä¸‹è¿›è¡Œåä½œæ¨¡å‹è®­ç»ƒï¼Œä½†ç”±äºå—é™çš„æ•°æ®è®¿é—®ï¼Œç¡®ä¿å„å®¢æˆ·ç«¯çš„æ•°æ®å°±ç»ªåº¦ (Data Readiness, DR) å…·æœ‰å¾ˆå¤§æŒ‘æˆ˜ã€‚æœ¬ç ”ç©¶æå‡ºäº† CADRE (Customizable Assurance of Data Readiness) æ¡†æ¶ï¼Œå…è®¸ç”¨æˆ·é’ˆå¯¹ç‰¹å®šçš„è”é‚¦å­¦ä¹  (Federated Learning, FL) ä»»åŠ¡è‡ªå®šä¹‰æ•°æ®å°±ç»ªæŒ‡æ ‡ã€è§„åˆ™å’Œè¡¥æ•‘æªæ–½ã€‚CADRE èƒ½å¤Ÿåœ¨ä¿æŠ¤éšç§çš„å‰æä¸‹ç”Ÿæˆå…¨é¢çš„ DR æŠ¥å‘Šï¼Œç¡®ä¿æ•°æ®é›†åœ¨è®­ç»ƒå‰å·²å……åˆ†å‡†å¤‡ã€‚é€šè¿‡å°†è¯¥æ¡†æ¶é›†æˆåˆ°ç°æœ‰ PPFL ç³»ç»Ÿå¹¶é’ˆå¯¹ 6 ä¸ªæ•°æ®é›†å’Œ 7 ç§ DR é—®é¢˜è¿›è¡Œå®éªŒï¼Œç»“æœè¯æ˜äº†å…¶åœ¨æ•°æ®è´¨é‡ã€éšç§å’Œå…¬å¹³æ€§ç­‰ç»´åº¦çš„é€šç”¨æ€§ä¸æœ‰æ•ˆæ€§ã€‚è¯¥æ–¹æ³•ä¸ä»…æå‡äº†è”é‚¦å­¦ä¹ æ¨¡å‹çš„æ€§èƒ½å’Œå¯é æ€§ï¼Œè¿˜å®ç°äº†èµ„æºçš„æœ‰æ•ˆåˆ©ç”¨ã€‚",
      "categories": [
        "cs.CR",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CR",
      "comment": "10 pages, 8 figures, 2 tables",
      "pdf_url": "https://arxiv.org/pdf/2505.23849v2",
      "published_date": "2025-05-28 21:24:46 UTC",
      "updated_date": "2025-08-11 15:34:12 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T15:09:00.563965+00:00"
    },
    {
      "arxiv_id": "2506.00052v1",
      "title": "Using LLMs to Advance the Cognitive Science of Collectives",
      "title_zh": "åˆ©ç”¨ LLMs æ¨åŠ¨é›†ä½“è®¤çŸ¥ç§‘å­¦çš„å‘å±•",
      "authors": [
        "Ilia Sucholutsky",
        "Katherine M. Collins",
        "Nori Jacoby",
        "Bill D. Thompson",
        "Robert D. Hawkins"
      ],
      "abstract": "LLMs are already transforming the study of individual cognition, but their application to studying collective cognition has been underexplored. We lay out how LLMs may be able to address the complexity that has hindered the study of collectives and raise possible risks that warrant new methods.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†åˆ©ç”¨LLMsæ¨åŠ¨é›†ä½“è®¤çŸ¥ç§‘å­¦ï¼ˆCognitive Science of Collectivesï¼‰å‘å±•çš„æ½œåŠ›ã€‚å°½ç®¡LLMså·²ç»æ·±åˆ»æ”¹å˜äº†å¯¹ä¸ªä½“è®¤çŸ¥çš„ç ”ç©¶ï¼Œä½†å…¶åœ¨é›†ä½“è®¤çŸ¥é¢†åŸŸçš„åº”ç”¨ä»ç„¶ç¼ºä¹å……åˆ†æ¢ç´¢ã€‚æ–‡ç« è¯¦ç»†åˆ†æäº†LLMså¦‚ä½•èƒ½å¤Ÿè§£å†³é•¿æœŸä»¥æ¥é˜»ç¢é›†ä½“ç ”ç©¶çš„å¤æ‚æ€§é—®é¢˜ï¼Œå¹¶å¯¹å¯èƒ½äº§ç”Ÿçš„é£é™©æå‡ºäº†è­¦ç¤ºã€‚ä½œè€…è®¤ä¸ºè¿™äº›æ½œåœ¨é£é™©éœ€è¦é€šè¿‡å¼€å‘æ–°çš„ç ”ç©¶æ–¹æ³•æ¥åŠ ä»¥è§£å†³ã€‚è¯¥ç ”ç©¶ä¸ºæœªæ¥åˆ©ç”¨äººå·¥æ™ºèƒ½æŠ€æœ¯æ·±å…¥ç†è§£ç¾¤ä½“è¡Œä¸ºå’Œåä½œæœºåˆ¶å¥ å®šäº†ç†è®ºåŸºç¡€ï¼Œæ—¨åœ¨æ¨åŠ¨é›†ä½“è®¤çŸ¥é¢†åŸŸå‘æ›´å¤æ‚ã€æ›´ç²¾ç»†çš„æ–¹å‘å‘å±•ã€‚",
      "categories": [
        "q-bio.NC",
        "cs.AI",
        "cs.HC",
        "cs.MA",
        "cs.SI"
      ],
      "primary_category": "q-bio.NC",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.00052v1",
      "published_date": "2025-05-28 21:15:46 UTC",
      "updated_date": "2025-05-28 21:15:46 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T15:10:01.018551+00:00"
    },
    {
      "arxiv_id": "2505.22871v1",
      "title": "The WHY in Business Processes: Unification of Causal Process Models",
      "title_zh": "ä¸šåŠ¡æµç¨‹ä¸­çš„å› æœæ¢æï¼šå› æœæµç¨‹æ¨¡å‹çš„ç»Ÿä¸€",
      "authors": [
        "Yuval David",
        "Fabiana Fournier",
        "Lior Limonad",
        "Inna Skarbovsky"
      ],
      "abstract": "Causal reasoning is essential for business process interventions and improvement, requiring a clear understanding of causal relationships among activity execution times in an event log. Recent work introduced a method for discovering causal process models but lacked the ability to capture alternating causal conditions across multiple variants. This raises the challenges of handling missing values and expressing the alternating conditions among log splits when blending traces with varying activities.\n  We propose a novel method to unify multiple causal process variants into a consistent model that preserves the correctness of the original causal models, while explicitly representing their causal-flow alternations. The method is formally defined, proved, evaluated on three open and two proprietary datasets, and released as an open-source implementation.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†ä¸šåŠ¡æµç¨‹ä¸­çš„å› æœæ¨ç†é—®é¢˜ï¼ŒæŒ‡å‡ºç†è§£äº‹ä»¶æ—¥å¿—ä¸­æ´»åŠ¨æ‰§è¡Œæ—¶é—´ä¹‹é—´çš„å› æœå…³ç³»å¯¹äºæµç¨‹æ”¹è¿›è‡³å…³é‡è¦ã€‚é’ˆå¯¹ç°æœ‰æ–¹æ³•åœ¨æ•æ‰å¤šä¸ªå˜ä½“é—´äº¤æ›¿å› æœæ¡ä»¶ä»¥åŠå¤„ç†ç¼ºå¤±å€¼æ–¹é¢çš„å±€é™æ€§ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§å°†å¤šä¸ªå› æœæµç¨‹å˜ä½“ï¼ˆcausal process variantsï¼‰ç»Ÿä¸€ä¸ºä¸€è‡´æ¨¡å‹çš„æ–°æ–¹æ³•ã€‚è¯¥æ–¹æ³•åœ¨ä¿ç•™åŸå§‹æ¨¡å‹æ­£ç¡®æ€§çš„åŒæ—¶ï¼Œèƒ½å¤Ÿæ˜¾å¼åœ°è¡¨ç¤ºå› æœæµçš„äº¤æ›¿ï¼ˆcausal-flow alternationsï¼‰ã€‚ç ”ç©¶å›¢é˜Ÿå¯¹è¯¥æ–¹æ³•è¿›è¡Œäº†æ­£å¼çš„å®šä¹‰ä¸è¯æ˜ï¼Œå¹¶åœ¨ä¸‰ä¸ªå…¬å¼€æ•°æ®é›†å’Œä¸¤ä¸ªä¸“æœ‰æ•°æ®é›†ä¸Šå®Œæˆäº†å®éªŒè¯„ä¼°ã€‚æ­¤å¤–ï¼Œè¯¥ç ”ç©¶è¿˜å‘å¸ƒäº†å¼€æºå®ç°ï¼Œä¸ºæ·±å…¥åˆ†æä¸šåŠ¡æµç¨‹ä¸­çš„å› æœæœºåˆ¶æä¾›äº†æœ‰æ•ˆçš„æŠ€æœ¯æ‰‹æ®µã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "28 pages, 6 figures, BPM 2025 Forum",
      "pdf_url": "https://arxiv.org/pdf/2505.22871v1",
      "published_date": "2025-05-28 21:12:30 UTC",
      "updated_date": "2025-05-28 21:12:30 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T15:09:11.584107+00:00"
    },
    {
      "arxiv_id": "2505.22866v1",
      "title": "Scaling Offline RL via Efficient and Expressive Shortcut Models",
      "title_zh": "é€šè¿‡é«˜æ•ˆä¸”å¼ºè¡¨è¾¾èƒ½åŠ›çš„æ·å¾„æ¨¡å‹å®ç°ç¦»çº¿å¼ºåŒ–å­¦ä¹ çš„è§„æ¨¡åŒ–æ‰©å±•",
      "authors": [
        "Nicolas Espinosa-Dice",
        "Yiyi Zhang",
        "Yiding Chen",
        "Bradley Guo",
        "Owen Oertell",
        "Gokul Swamy",
        "Kiante Brantley",
        "Wen Sun"
      ],
      "abstract": "Diffusion and flow models have emerged as powerful generative approaches capable of modeling diverse and multimodal behavior. However, applying these models to offline reinforcement learning (RL) remains challenging due to the iterative nature of their noise sampling processes, making policy optimization difficult. In this paper, we introduce Scalable Offline Reinforcement Learning (SORL), a new offline RL algorithm that leverages shortcut models - a novel class of generative models - to scale both training and inference. SORL's policy can capture complex data distributions and can be trained simply and efficiently in a one-stage training procedure. At test time, SORL introduces both sequential and parallel inference scaling by using the learned Q-function as a verifier. We demonstrate that SORL achieves strong performance across a range of offline RL tasks and exhibits positive scaling behavior with increased test-time compute. We release the code at nico-espinosadice.github.io/projects/sorl.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹æ‰©æ•£æ¨¡å‹(Diffusion models)å’Œæµæ¨¡å‹(Flow models)åœ¨ç¦»çº¿å¼ºåŒ–å­¦ä¹ (Offline RL)ä¸­å› è¿­ä»£é‡‡æ ·å¯¼è‡´ç­–ç•¥ä¼˜åŒ–å›°éš¾çš„é—®é¢˜ï¼Œæå‡ºäº†å¯æ‰©å±•ç¦»çº¿å¼ºåŒ–å­¦ä¹ ç®—æ³•SORLã€‚è¯¥ç®—æ³•æ ¸å¿ƒåœ¨äºåˆ©ç”¨ä¸€ç§æ–°å‹ç”Ÿæˆæ¨¡å‹â€”â€”å¿«æ·æ¨¡å‹(Shortcut Models)ï¼Œåœ¨å®ç°é«˜æ•ˆä¸”å…·æœ‰è¡¨ç°åŠ›çš„ç­–ç•¥å»ºæ¨¡çš„åŒæ—¶ï¼Œæ”¯æŒç®€æ´çš„å•é˜¶æ®µè®­ç»ƒç¨‹åº(One-stage training)ã€‚åœ¨æ¨ç†é˜¶æ®µï¼ŒSORLé€šè¿‡ä½¿ç”¨å­¦ä¹ åˆ°çš„Q-functionä½œä¸ºéªŒè¯å™¨(Verifier)ï¼Œå¼•å…¥äº†é¡ºåºå’Œå¹¶è¡Œçš„æ¨ç†ç¼©æ”¾æœºåˆ¶ã€‚å®éªŒç»“æœè¯æ˜ï¼ŒSORLåœ¨å¤šç§ç¦»çº¿å¼ºåŒ–å­¦ä¹ ä»»åŠ¡ä¸­å‡å–å¾—äº†å¼ºåŠ²æ€§èƒ½ï¼Œå¹¶å±•ç°å‡ºéšæµ‹è¯•æ—¶è®¡ç®—é‡(Test-time compute)å¢åŠ è€Œæå‡æ€§èƒ½çš„æ­£å‘æ‰©å±•ç‰¹æ€§ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "32 pages, 5 figures. Under review at NeurIPS 2025",
      "pdf_url": "https://arxiv.org/pdf/2505.22866v1",
      "published_date": "2025-05-28 20:59:22 UTC",
      "updated_date": "2025-05-28 20:59:22 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T15:10:06.551022+00:00"
    },
    {
      "arxiv_id": "2505.22865v1",
      "title": "BinauralFlow: A Causal and Streamable Approach for High-Quality Binaural Speech Synthesis with Flow Matching Models",
      "title_zh": "BinauralFlowï¼šåŸºäºæµåŒ¹é…æ¨¡å‹çš„é«˜è´¨é‡åŒè€³è¯­éŸ³åˆæˆå› æœæµå¼æ–¹æ³•",
      "authors": [
        "Susan Liang",
        "Dejan Markovic",
        "Israel D. Gebru",
        "Steven Krenn",
        "Todd Keebler",
        "Jacob Sandakly",
        "Frank Yu",
        "Samuel Hassel",
        "Chenliang Xu",
        "Alexander Richard"
      ],
      "abstract": "Binaural rendering aims to synthesize binaural audio that mimics natural hearing based on a mono audio and the locations of the speaker and listener. Although many methods have been proposed to solve this problem, they struggle with rendering quality and streamable inference. Synthesizing high-quality binaural audio that is indistinguishable from real-world recordings requires precise modeling of binaural cues, room reverb, and ambient sounds. Additionally, real-world applications demand streaming inference. To address these challenges, we propose a flow matching based streaming binaural speech synthesis framework called BinauralFlow. We consider binaural rendering to be a generation problem rather than a regression problem and design a conditional flow matching model to render high-quality audio. Moreover, we design a causal U-Net architecture that estimates the current audio frame solely based on past information to tailor generative models for streaming inference. Finally, we introduce a continuous inference pipeline incorporating streaming STFT/ISTFT operations, a buffer bank, a midpoint solver, and an early skip schedule to improve rendering continuity and speed. Quantitative and qualitative evaluations demonstrate the superiority of our method over SOTA approaches. A perceptual study further reveals that our model is nearly indistinguishable from real-world recordings, with a $42\\%$ confusion rate.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† BinauralFlowï¼Œä¸€ç§åŸºäº Flow Matching æ¨¡å‹çš„é«˜è´¨é‡åŒè€³è¯­éŸ³åˆæˆæ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³ç°æœ‰æ–¹æ³•åœ¨æ¸²æŸ“è´¨é‡å’Œæµå¼æ¨ç†ï¼ˆStreamable Inferenceï¼‰æ–¹é¢çš„éš¾é¢˜ã€‚ç ”ç©¶äººå‘˜å°†åŒè€³æ¸²æŸ“è§†ä¸ºä¸€ä¸ªç”Ÿæˆé—®é¢˜è€Œéå›å½’é—®é¢˜ï¼Œå¹¶è®¾è®¡äº†ä¸€ä¸ªæ¡ä»¶ Flow Matching æ¨¡å‹æ¥ç²¾å‡†æ•æ‰åŒè€³çº¿ç´¢ã€æˆ¿é—´æ··å“å’Œç¯å¢ƒéŸ³ã€‚ä¸ºäº†æ”¯æŒæµå¼æ¨ç†ï¼Œè¯¥æ¡†æ¶é‡‡ç”¨äº†å› æœ U-Netï¼ˆCausal U-Netï¼‰æ¶æ„ï¼Œä»…æ ¹æ®è¿‡å»çš„ä¿¡æ¯ä¼°ç®—å½“å‰éŸ³é¢‘å¸§ã€‚æ­¤å¤–ï¼Œç ”ç©¶å›¢é˜Ÿå¼•å…¥äº†åŒ…å«æµå¼ STFT/ISTFTã€ç¼“å†²åº“ï¼ˆBuffer Bankï¼‰ã€ä¸­ç‚¹æ±‚è§£å™¨ï¼ˆMidpoint Solverï¼‰åŠæ—©æœŸè·³è¿‡ç­–ç•¥ï¼ˆEarly Skip Scheduleï¼‰çš„è¿ç»­æ¨ç†æµæ°´çº¿ï¼Œæ˜¾è‘—æå‡äº†æ¸²æŸ“çš„è¿ç»­æ€§ä¸é€Ÿåº¦ã€‚å®éªŒè¯„ä¼°è¯æ˜è¯¥æ–¹æ³•ä¼˜äºç°æœ‰çš„ SOTA æ–¹æ³•ï¼ŒçŸ¥è§‰å®éªŒè¿›ä¸€æ­¥æ˜¾ç¤ºå…¶ç”Ÿæˆçš„éŸ³é¢‘å‡ ä¹ä¸çœŸå®å½•éŸ³æ— æ³•åŒºåˆ†ï¼Œæ··æ·†ç‡è¾¾åˆ° 42%ã€‚è¯¥æ–¹æ¡ˆä¸ºå®æ—¶ã€é«˜ä¿çœŸçš„åŒè€³è¯­éŸ³åˆæˆæä¾›äº†é‡è¦çš„æŠ€æœ¯æ”¯æ’‘ã€‚",
      "categories": [
        "cs.SD",
        "cs.AI",
        "eess.AS"
      ],
      "primary_category": "cs.SD",
      "comment": "ICML 2025, 18 pages",
      "pdf_url": "https://arxiv.org/pdf/2505.22865v1",
      "published_date": "2025-05-28 20:59:15 UTC",
      "updated_date": "2025-05-28 20:59:15 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T15:09:19.725856+00:00"
    },
    {
      "arxiv_id": "2505.22860v2",
      "title": "Permissioned LLMs: Enforcing Access Control in Large Language Models",
      "title_zh": "è®¸å¯å¼å¤§è¯­è¨€æ¨¡å‹ï¼šåœ¨å¤§è¯­è¨€æ¨¡å‹ä¸­å¼ºåˆ¶æ‰§è¡Œè®¿é—®æ§åˆ¶",
      "authors": [
        "Bargav Jayaraman",
        "Virendra J. Marathe",
        "Hamid Mozaffari",
        "William F. Shen",
        "Krishnaram Kenthapadi"
      ],
      "abstract": "In enterprise settings, organizational data is segregated, siloed and carefully protected by elaborate access control frameworks. These access control structures can completely break down if an LLM fine-tuned on the siloed data serves requests, for downstream tasks, from individuals with disparate access privileges. We propose Permissioned LLMs (PermLLM), a new class of LLMs that superimpose the organizational data access control structures on query responses they generate. We formalize abstractions underpinning the means to determine whether access control enforcement happens correctly over LLM query responses. Our formalism introduces the notion of a relevant response that can be used to prove whether a PermLLM mechanism has been implemented correctly. We also introduce a novel metric, called access advantage, to empirically evaluate the efficacy of a PermLLM mechanism. We introduce three novel PermLLM mechanisms that build on Parameter Efficient Fine-Tuning to achieve the desired access control. We furthermore present two instantiations of access advantage--(i) Domain Distinguishability Index (DDI) based on Membership Inference Attacks, and (ii) Utility Gap Index (UGI) based on LLM utility evaluation. We demonstrate the efficacy of our PermLLM mechanisms through extensive experiments on five public datasets (GPQA, RCV1, SimpleQA, WMDP, and PubMedQA), in addition to evaluating the validity of DDI and UGI metrics themselves for quantifying access control in LLMs.",
      "tldr_zh": "é’ˆå¯¹ä¼ä¸šç¯å¢ƒä¸‹ç»„ç»‡æ•°æ®è¢«éš”ç¦»ä¿æŠ¤ï¼Œè€Œå¾®è°ƒåçš„ Large Language Models (LLMs) å¯èƒ½å¯¼è‡´ç°æœ‰è®¿é—®æ§åˆ¶ç»“æ„å¤±æ•ˆçš„é—®é¢˜ï¼Œè¯¥ç ”ç©¶æå‡ºäº† Permissioned LLMs (PermLLM) æ¡†æ¶ã€‚PermLLM èƒ½å¤Ÿå°†ç»„ç»‡çš„è®¿é—®æ§åˆ¶ç»“æ„å åŠ åˆ° LLM ç”Ÿæˆçš„æŸ¥è¯¢å“åº”ä¸­ï¼Œç¡®ä¿ä¸åŒæƒé™çš„ç”¨æˆ·ä»…èƒ½è·å–å…¶æˆæƒèŒƒå›´å†…çš„ä¿¡æ¯ã€‚ç ”ç©¶è€…é€šè¿‡å½¢å¼åŒ– relevant response ç­‰æŠ½è±¡æ¦‚å¿µï¼Œä¸ºéªŒè¯è®¿é—®æ§åˆ¶æ‰§è¡Œçš„æ­£ç¡®æ€§æä¾›äº†ç†è®ºä¾æ®ã€‚ä¸ºäº†å®ç°è¿™ä¸€ç›®æ ‡ï¼Œæ–‡ä¸­æå‡ºäº†ä¸‰ç§åŸºäº Parameter Efficient Fine-Tuning (PEFT) çš„æ–°å‹æœºåˆ¶ï¼Œå¹¶å¼•å…¥äº† access advantage æŒ‡æ ‡ä»¥åŠ Domain Distinguishability Index (DDI) å’Œ Utility Gap Index (UGI) ä¸¤ä¸ªå…·ä½“ç»´åº¦æ¥é‡åŒ–è¯„ä¼°æ•ˆæœã€‚å®éªŒåœ¨ GPQAã€RCV1ã€SimpleQAã€WMDP å’Œ PubMedQA äº”ä¸ªå…¬å¼€æ•°æ®é›†ä¸ŠéªŒè¯äº† PermLLM æœºåˆ¶åœ¨å¼ºåŒ–è®¿é—®æ§åˆ¶æ–¹é¢çš„æœ‰æ•ˆæ€§ï¼Œä¸ºä¼ä¸šçº§å¤§æ¨¡å‹çš„å®‰å…¨åº”ç”¨å¥ å®šäº†åŸºç¡€ã€‚",
      "categories": [
        "cs.CR",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CR",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.22860v2",
      "published_date": "2025-05-28 20:47:02 UTC",
      "updated_date": "2025-10-03 14:50:37 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T15:09:56.247505+00:00"
    },
    {
      "arxiv_id": "2505.22857v1",
      "title": "NGPU-LM: GPU-Accelerated N-Gram Language Model for Context-Biasing in Greedy ASR Decoding",
      "title_zh": "NGPU-LMï¼šç”¨äº ASR è´ªå©ªè§£ç ä¸Šä¸‹æ–‡åç½®çš„ GPU åŠ é€Ÿ N-Gram è¯­è¨€æ¨¡å‹",
      "authors": [
        "Vladimir Bataev",
        "Andrei Andrusenko",
        "Lilit Grigoryan",
        "Aleksandr Laptev",
        "Vitaly Lavrukhin",
        "Boris Ginsburg"
      ],
      "abstract": "Statistical n-gram language models are widely used for context-biasing tasks in Automatic Speech Recognition (ASR). However, existing implementations lack computational efficiency due to poor parallelization, making context-biasing less appealing for industrial use. This work rethinks data structures for statistical n-gram language models to enable fast and parallel operations for GPU-optimized inference. Our approach, named NGPU-LM, introduces customizable greedy decoding for all major ASR model types - including transducers, attention encoder-decoder models, and CTC - with less than 7% computational overhead. The proposed approach can eliminate more than 50% of the accuracy gap between greedy and beam search for out-of-domain scenarios while avoiding significant slowdown caused by beam search. The implementation of the proposed NGPU-LM is open-sourced.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ç»Ÿè®¡ N-Gram Language Model åœ¨è‡ªåŠ¨è¯­éŸ³è¯†åˆ« (ASR) ä¸Šä¸‹æ–‡åç½® (Context-Biasing) ä»»åŠ¡ä¸­å› ç¼ºä¹å¹¶è¡ŒåŒ–è€Œæ•ˆç‡ä½ä¸‹çš„é—®é¢˜ï¼Œæå‡ºäº† NGPU-LM æ¡†æ¶ã€‚è¯¥æ¡†æ¶é€šè¿‡é‡æ–°è®¾è®¡æ•°æ®ç»“æ„å®ç°äº† GPU ä¼˜åŒ–çš„å¿«é€Ÿå¹¶è¡Œæ¨ç†ï¼Œå¹¶ä¸º Transducersã€Attention Encoder-Decoder å’Œ CTC ç­‰ä¸»æµ ASR æ¨¡å‹æä¾›äº†å¯å®šåˆ¶çš„ Greedy Decodingã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒNGPU-LM çš„è®¡ç®—å¼€é”€ä½äº 7%ï¼Œä¸”åœ¨é¢†åŸŸå¤– (Out-of-Domain) åœºæ™¯ä¸­èƒ½æ¶ˆé™¤ Greedy Search ä¸ Beam Search ä¹‹é—´è¶…è¿‡ 50% çš„å‡†ç¡®ç‡å·®è·ã€‚é€šè¿‡é¿å… Beam Search å¯¼è‡´çš„æ˜¾è‘—å‡é€Ÿï¼Œè¯¥æ–¹æ¡ˆåœ¨ä¿è¯æ¨ç†æ•ˆç‡çš„åŒæ—¶å¤§å¹…æå‡äº†è¯†åˆ«ç²¾åº¦ï¼Œç›®å‰è¯¥é¡¹ç›®å·²å¼€æºã€‚",
      "categories": [
        "eess.AS",
        "cs.AI",
        "cs.CL",
        "cs.LG",
        "cs.SD"
      ],
      "primary_category": "eess.AS",
      "comment": "Accepted to Interspeech 2025",
      "pdf_url": "https://arxiv.org/pdf/2505.22857v1",
      "published_date": "2025-05-28 20:43:10 UTC",
      "updated_date": "2025-05-28 20:43:10 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T15:09:57.288886+00:00"
    },
    {
      "arxiv_id": "2505.22852v1",
      "title": "Operationalizing CaMeL: Strengthening LLM Defenses for Enterprise Deployment",
      "title_zh": "CaMeL çš„è¿è¡ŒåŒ–ï¼šå¼ºåŒ–é¢å‘ä¼ä¸šéƒ¨ç½²çš„å¤§è¯­è¨€æ¨¡å‹é˜²å¾¡",
      "authors": [
        "Krti Tallam",
        "Emma Miller"
      ],
      "abstract": "CaMeL (Capabilities for Machine Learning) introduces a capability-based sandbox to mitigate prompt injection attacks in large language model (LLM) agents. While effective, CaMeL assumes a trusted user prompt, omits side-channel concerns, and incurs performance tradeoffs due to its dual-LLM design. This response identifies these issues and proposes engineering improvements to expand CaMeL's threat coverage and operational usability. We introduce: (1) prompt screening for initial inputs, (2) output auditing to detect instruction leakage, (3) a tiered-risk access model to balance usability and control, and (4) a verified intermediate language for formal guarantees. Together, these upgrades align CaMeL with best practices in enterprise security and support scalable deployment.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†å¦‚ä½•é€šè¿‡å·¥ç¨‹åŒ–æ”¹è¿›æ¥å¼ºåŒ–CaMeLï¼ˆCapabilities for Machine Learningï¼‰æ¡†æ¶ï¼Œä»¥åº”å¯¹å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ™ºèƒ½ä½“åœ¨ä¼ä¸šéƒ¨ç½²ä¸­é¢ä¸´çš„é˜²å¾¡æŒ‘æˆ˜ã€‚é’ˆå¯¹CaMeLåœ¨å¤„ç†ä¸å¯ä¿¡è¾“å…¥ã€ä¾§ä¿¡é“é£é™©ä»¥åŠåŒLLMæ¶æ„æ€§èƒ½æŸè€—ç­‰æ–¹é¢çš„å±€é™æ€§ï¼Œæœ¬æ–‡æå‡ºäº†å¤šé¡¹æ ¸å¿ƒæ”¹è¿›æ–¹æ¡ˆã€‚å…·ä½“åŒ…æ‹¬ç”¨äºåˆå§‹è¾“å…¥è¿‡æ»¤çš„prompt screeningã€ç”¨äºæ£€æµ‹æŒ‡ä»¤æ³„éœ²çš„output auditingã€å¹³è¡¡æ˜“ç”¨æ€§ä¸å®‰å…¨æ§åˆ¶çš„tiered-risk access modelï¼Œä»¥åŠæä¾›å½¢å¼åŒ–å®‰å…¨ä¿è¯çš„verified intermediate languageã€‚è¿™äº›æ”¹è¿›æªæ–½æ˜¾è‘—æ‰©å±•äº†CaMeLçš„å¨èƒè¦†ç›–èŒƒå›´ï¼Œå¹¶æå‡äº†å…¶åœ¨å®é™…æ“ä½œä¸­çš„å¯ç”¨æ€§ã€‚é€šè¿‡å°†è¿™äº›å‡çº§æ–¹æ¡ˆä¸ä¼ä¸šå®‰å…¨æœ€ä½³å®è·µç›¸ç»“åˆï¼Œè¯¥ç ”ç©¶ä¸ºæ”¯æŒå¤§è§„æ¨¡ã€å¯æ‰©å±•çš„LLMé˜²å¾¡éƒ¨ç½²æä¾›äº†å…³é”®çš„æŠ€æœ¯æ”¯æ’‘ã€‚",
      "categories": [
        "cs.CR",
        "cs.AI"
      ],
      "primary_category": "cs.CR",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.22852v1",
      "published_date": "2025-05-28 20:35:24 UTC",
      "updated_date": "2025-05-28 20:35:24 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T15:09:46.862271+00:00"
    },
    {
      "arxiv_id": "2505.22846v2",
      "title": "RocqStar: Leveraging Similarity-driven Retrieval and Agentic Systems for Rocq generation",
      "title_zh": "RocqStarï¼šåŸºäºç›¸ä¼¼åº¦é©±åŠ¨æ£€ç´¢ä¸æ™ºèƒ½ä½“ç³»ç»Ÿçš„ Rocq ç”Ÿæˆ",
      "authors": [
        "Andrei Kozyrev",
        "Nikita Khramov",
        "Gleb Solovev",
        "Anton Podkopaev"
      ],
      "abstract": "Interactive Theorem Proving was repeatedly shown to be fruitful combined with Generative Artificial Intelligence. This paper assesses multiple approaches to Rocq generation and illuminates potential avenues for improvement. We highlight the importance of thorough premise selection for generating Rocq proofs and propose a novel approach, leveraging retrieval via a self-attentive embedder model. The evaluation of the designed approach shows up to 28% relative increase of the generator's performance. We tackle the problem of writing Rocq proofs using a multi-stage agentic system, tailored for formal verification, and demonstrate its high effectiveness. We conduct an ablation study and demonstrate shows that incorporating multi-agent debate during the planning stage increases the proof success rate by 20% overall and nearly doubles it for complex theorems, while the reflection mechanism further enhances stability and consistency.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† RocqStarï¼Œä¸€ç§ç»“åˆç›¸ä¼¼åº¦é©±åŠ¨æ£€ç´¢ä¸æ™ºèƒ½ä½“ç³»ç»Ÿçš„æ¡†æ¶ï¼Œæ—¨åœ¨æå‡ Rocq è¯æ˜ç”Ÿæˆçš„è‡ªåŠ¨åŒ–æ°´å¹³ã€‚ç ”ç©¶é¦–å…ˆå¼ºè°ƒäº†å‰æé€‰æ‹© (premise selection) çš„å…³é”®ä½œç”¨ï¼Œå¹¶é‡‡ç”¨åŸºäºè‡ªæ³¨æ„åŠ›åµŒå…¥æ¨¡å‹ (self-attentive embedder model) çš„æ£€ç´¢æ–¹æ³•ï¼Œä½¿è¯æ˜ç”Ÿæˆæ€§èƒ½ç›¸å¯¹æå‡äº† 28%ã€‚ä¸ºäº†åº”å¯¹å½¢å¼åŒ–éªŒè¯ (formal verification) çš„æŒ‘æˆ˜ï¼Œä½œè€…æ„å»ºäº†ä¸€ä¸ªä¸“é—¨çš„å¤šé˜¶æ®µæ™ºèƒ½ä½“ç³»ç»Ÿ (multi-stage agentic system)ã€‚æ¶ˆèå®éªŒè¡¨æ˜ï¼Œåœ¨è§„åˆ’é˜¶æ®µå¼•å…¥å¤šæ™ºèƒ½ä½“è¾©è®º (multi-agent debate) å¯ä½¿è¯æ˜æˆåŠŸç‡æ•´ä½“æé«˜ 20%ï¼Œä¸”åœ¨å¤æ‚å®šç†ä¸Šçš„è¡¨ç°å‡ ä¹ç¿»å€ã€‚æ­¤å¤–ï¼Œç³»ç»Ÿé€šè¿‡åå°„æœºåˆ¶ (reflection mechanism) è¿›ä¸€æ­¥å¢å¼ºäº†ç”Ÿæˆè¯æ˜çš„ç¨³å®šæ€§å’Œä¸€è‡´æ€§ï¼Œä¸ºè‡ªåŠ¨å®šç†è¯æ˜æä¾›äº†æ–°çš„æœ‰æ•ˆè·¯å¾„ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.LO",
        "cs.SE"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.22846v2",
      "published_date": "2025-05-28 20:26:11 UTC",
      "updated_date": "2025-10-20 07:59:37 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T15:10:04.121747+00:00"
    },
    {
      "arxiv_id": "2505.22845v1",
      "title": "Security Benefits and Side Effects of Labeling AI-Generated Images",
      "title_zh": "AI ç”Ÿæˆå›¾åƒæ ‡æ³¨çš„å®‰å…¨ç›Šå¤„ä¸å‰¯ä½œç”¨",
      "authors": [
        "Sandra HÃ¶ltervennhoff",
        "Jonas Ricker",
        "Maike M. Raphael",
        "Charlotte Schwedes",
        "Rebecca Weil",
        "Asja Fischer",
        "Thorsten Holz",
        "Lea SchÃ¶nherr",
        "Sascha Fahl"
      ],
      "abstract": "Generative artificial intelligence is developing rapidly, impacting humans' interaction with information and digital media. It is increasingly used to create deceptively realistic misinformation, so lawmakers have imposed regulations requiring the disclosure of AI-generated content. However, only little is known about whether these labels reduce the risks of AI-generated misinformation.\n  Our work addresses this research gap. Focusing on AI-generated images, we study the implications of labels, including the possibility of mislabeling. Assuming that simplicity, transparency, and trust are likely to impact the successful adoption of such labels, we first qualitatively explore users' opinions and expectations of AI labeling using five focus groups. Second, we conduct a pre-registered online survey with over 1300 U.S. and EU participants to quantitatively assess the effect of AI labels on users' ability to recognize misinformation containing either human-made or AI-generated images. Our focus groups illustrate that, while participants have concerns about the practical implementation of labeling, they consider it helpful in identifying AI-generated images and avoiding deception. However, considering security benefits, our survey revealed an ambiguous picture, suggesting that users might over-rely on labels. While inaccurate claims supported by labeled AI-generated images were rated less credible than those with unlabeled AI-images, the belief in accurate claims also decreased when accompanied by a labeled AI-generated image. Moreover, we find the undesired side effect that human-made images conveying inaccurate claims were perceived as more credible in the presence of labels.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†ä¸ºAIç”Ÿæˆçš„å›¾åƒ(AI-generated images)æ·»åŠ æ ‡ç­¾çš„å®‰å…¨æ”¶ç›Šä¸å‰¯ä½œç”¨ï¼Œæ—¨åœ¨å¡«è¡¥å…³äºæ­¤ç±»æ ‡ç­¾åœ¨å‡å°‘è™šå‡ä¿¡æ¯é£é™©æ–¹é¢æœ‰æ•ˆæ€§çš„ç ”ç©¶ç©ºç™½ã€‚ç ”ç©¶é¦–å…ˆé€šè¿‡ç„¦ç‚¹å°ç»„(focus groups)å®šæ€§æ¢ç´¢äº†ç”¨æˆ·å¯¹æ ‡ç­¾åˆ¶åº¦çš„çœ‹æ³•ï¼Œéšåé€šè¿‡ä¸€é¡¹æ¶µç›–1300å¤šåç¾å›½å’Œæ¬§ç›Ÿå—ä¼—çš„åœ¨çº¿è°ƒæŸ¥ï¼Œå®šé‡è¯„ä¼°äº†æ ‡ç­¾å¯¹ç”¨æˆ·è¯†åˆ«è™šå‡ä¿¡æ¯èƒ½åŠ›çš„å½±å“ã€‚ç ”ç©¶å‘ç°ï¼Œè™½ç„¶ç”¨æˆ·è®¤ä¸ºæ ‡ç­¾æœ‰åŠ©äºè¯†åˆ«AIç”Ÿæˆå†…å®¹ï¼Œä½†å®é™…åº”ç”¨ä¸­å­˜åœ¨è¿‡åº¦ä¾èµ–æ ‡ç­¾çš„é£é™©ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œæ ‡ç­¾è™½ç„¶é™ä½äº†åŸºäºAIå›¾åƒçš„è™šå‡ä¿¡æ¯çš„å¯ä¿¡åº¦ï¼Œä½†ä¹Ÿå‰Šå¼±äº†ç”¨æˆ·å¯¹åŒ…å«AIå›¾åƒçš„çœŸå®ä¿¡æ¯çš„ä¿¡ä»»ã€‚æ­¤å¤–ï¼Œç ”ç©¶æ­ç¤ºäº†ä¸€ä¸ªé‡è¦çš„å‰¯ä½œç”¨ï¼Œå³åœ¨æ ‡ç­¾æœºåˆ¶å­˜åœ¨çš„æƒ…å†µä¸‹ï¼Œé‚£äº›æœªè¢«æ ‡è®°ä½†åŒ…å«è™šå‡ä¿¡æ¯çš„äººé€ å›¾åƒ(human-made images)åè€Œè¢«è®¤ä¸ºæ›´åŠ å¯ä¿¡ï¼Œè¿™è¡¨æ˜æ ‡ç­¾æ”¿ç­–å¯èƒ½äº§ç”Ÿæ„æƒ³ä¸åˆ°çš„å®‰å…¨æ¼æ´ã€‚",
      "categories": [
        "cs.CR",
        "cs.AI",
        "cs.CY",
        "cs.SI"
      ],
      "primary_category": "cs.CR",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.22845v1",
      "published_date": "2025-05-28 20:24:45 UTC",
      "updated_date": "2025-05-28 20:24:45 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T15:10:15.496825+00:00"
    },
    {
      "arxiv_id": "2505.22843v3",
      "title": "On the Reliability and Stability of Selective Methods in Malware Classification Tasks",
      "title_zh": "è®ºæ¶æ„è½¯ä»¶åˆ†ç±»ä»»åŠ¡ä¸­é€‰æ‹©æ€§æ–¹æ³•çš„å¯é æ€§ä¸ç¨³å®šæ€§",
      "authors": [
        "Alexander Herzog",
        "Aliai Eusebi",
        "Lorenzo Cavallaro"
      ],
      "abstract": "The performance figures of modern drift-adaptive malware classifiers appear promising, but does this translate to genuine operational reliability? The standard evaluation paradigm primarily focuses on baseline performance metrics, neglecting confidence-error alignment and operational stability. While prior works established the importance of temporal evaluation and introduced selective classification in malware classification tasks, we take a complementary direction by investigating whether malware classifiers maintain reliable and stable confidence estimates under distribution shifts and exploring the tensions between scientific advancement and practical impacts when they do not. We propose Aurora, a framework to evaluate malware classifiers based on their confidence quality and operational resilience. Aurora subjects the confidence profile of a given model to verification to assess the reliability of its estimates. Unreliable confidence estimates erode operational trust, waste valuable annotation budgets on non-informative samples for active learning, and leave error-prone instances undetected in selective classification. Aurora is further complemented by a set of metrics designed to go beyond point-in-time performance, striving towards a more holistic assessment of operational stability throughout temporal evaluation periods. The fragility we observe in SOTA frameworks across datasets of varying drift severity suggests it may be time to revisit the underlying assumptions.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†æ¶æ„è½¯ä»¶åˆ†ç±»ä»»åŠ¡ä¸­é€‰æ‹©æ€§æ–¹æ³•çš„å¯é æ€§ä¸ç¨³å®šæ€§ï¼ŒæŒ‡å‡ºå½“å‰ä¸»æµæ¨¡å‹åœ¨é¢ä¸´åˆ†å¸ƒåç§»(distribution shifts)æ—¶å¾€å¾€ç¼ºä¹çœŸå®çš„è¿è¡Œå¯é æ€§ã€‚ä¸ºæ­¤ï¼Œä½œè€…æå‡ºäº†Auroraæ¡†æ¶ï¼Œæ—¨åœ¨é€šè¿‡éªŒè¯æ¨¡å‹çš„ç½®ä¿¡åº¦æ¦‚å†µ(confidence profile)æ¥è¯„ä¼°å…¶ç½®ä¿¡åº¦è´¨é‡å’Œæ“ä½œéŸ§æ€§ã€‚Auroraå¼•å…¥äº†ä¸€å¥—è¶…è¶Šå•ç‚¹æ€§èƒ½æŒ‡æ ‡çš„åº¦é‡è¡¡ï¼Œç”¨äºåœ¨æ—¶é—´è¯„ä¼°å‘¨æœŸå†…æ›´å…¨é¢åœ°è¡¡é‡æ¶æ„è½¯ä»¶åˆ†ç±»å™¨çš„æ“ä½œç¨³å®šæ€§ã€‚é€šè¿‡å¯¹å¤šç§æ¼‚ç§»ä¸¥é‡ç¨‹åº¦çš„æ•°æ®é›†è¿›è¡Œå®éªŒåˆ†æï¼Œç ”ç©¶å‘ç°ç°æœ‰çš„æœ€å…ˆè¿›(SOTA)æ¡†æ¶æ™®éè¡¨ç°å‡ºè„†å¼±æ€§ï¼Œè¿™è¡¨æ˜å­¦æœ¯ç•Œæœ‰å¿…è¦é‡æ–°å®¡è§†æ¶æ„è½¯ä»¶åˆ†ç±»é¢†åŸŸçš„åº•å±‚è¯„ä¼°å‡è®¾ã€‚",
      "categories": [
        "cs.CR",
        "cs.AI"
      ],
      "primary_category": "cs.CR",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.22843v3",
      "published_date": "2025-05-28 20:22:43 UTC",
      "updated_date": "2026-01-21 18:26:18 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T15:10:40.348596+00:00"
    },
    {
      "arxiv_id": "2505.22839v1",
      "title": "How Do Diffusion Models Improve Adversarial Robustness?",
      "title_zh": "æ‰©æ•£æ¨¡å‹å¦‚ä½•æå‡å¯¹æŠ—é²æ£’æ€§ï¼Ÿ",
      "authors": [
        "Liu Yuezhang",
        "Xue-Xin Wei"
      ],
      "abstract": "Recent findings suggest that diffusion models significantly enhance empirical adversarial robustness. While some intuitive explanations have been proposed, the precise mechanisms underlying these improvements remain unclear. In this work, we systematically investigate how and how well diffusion models improve adversarial robustness. First, we observe that diffusion models intriguingly increase, rather than decrease, the $\\ell_p$ distance to clean samples--challenging the intuition that purification denoises inputs closer to the original data. Second, we find that the purified images are heavily influenced by the internal randomness of diffusion models, where a compression effect arises within each randomness configuration. Motivated by this observation, we evaluate robustness under fixed randomness and find that the improvement drops to approximately 24% on CIFAR-10--substantially lower than prior reports approaching 70%. Importantly, we show that this remaining robustness gain strongly correlates with the model's ability to compress the input space, revealing the compression rate as a reliable robustness indicator without requiring gradient-based analysis. Our findings provide novel insights into the mechanisms underlying diffusion-based purification, and offer guidance for developing more effective and principled adversarial purification systems.",
      "tldr_zh": "è¯¥ç ”ç©¶ç³»ç»Ÿæ¢è®¨äº†æ‰©æ•£æ¨¡å‹ (diffusion models) æå‡å¯¹æŠ—é²æ£’æ€§ (adversarial robustness) çš„å†…åœ¨æœºåˆ¶ã€‚ç ”ç©¶å‘ç°ï¼Œæ‰©æ•£æ¨¡å‹å®é™…ä¸Šå¢åŠ äº†å‡€åŒ–å›¾åƒä¸åŸå§‹æ ·æœ¬é—´çš„ $\\ell_p$ è·ç¦»ï¼Œè¿™æŒ‘æˆ˜äº†å‡€åŒ–è¿‡ç¨‹æ˜¯å•çº¯é€šè¿‡å»å™ªä½¿è¾“å…¥æ¥è¿‘åŸå§‹æ•°æ®çš„ç›´è§‰ã€‚å®éªŒè¡¨æ˜ï¼Œå‡€åŒ–åçš„å›¾åƒå—åˆ°æ¨¡å‹å†…éƒ¨éšæœºæ€§çš„æ˜¾è‘—å½±å“ï¼Œä¸”åœ¨ç‰¹å®šéšæœºé…ç½®ä¸‹å‘ˆç°å‡ºå‹ç¼©æ•ˆåº”ã€‚åœ¨å›ºå®šéšæœºæ€§æ¡ä»¶ä¸‹ï¼Œè¯¥æ–¹æ³•åœ¨ CIFAR-10 ä¸Šçš„é²æ£’æ€§æå‡ä»…ä¸ºçº¦ 24%ï¼Œæ˜¾è‘—ä½äºæ­¤å‰æŠ¥é“çš„è¿‘ 70%ã€‚è¿›ä¸€æ­¥åˆ†ææ˜¾ç¤ºï¼Œå‰©ä½™çš„é²æ£’æ€§å¢ç›Šä¸æ¨¡å‹å¯¹è¾“å…¥ç©ºé—´çš„å‹ç¼©èƒ½åŠ›å¼ºç›¸å…³ï¼Œä½¿å¾—å‹ç¼©ç‡æˆä¸ºä¸€ç§æ— éœ€æ¢¯åº¦åˆ†æçš„å¯é é²æ£’æ€§æŒ‡æ ‡ã€‚è¯¥ç ”ç©¶ä¸ºç†è§£æ‰©æ•£å‡€åŒ–æœºåˆ¶æä¾›äº†æ–°è§è§£ï¼Œå¹¶ä¸ºæ„å»ºæ›´å…·åŸåˆ™æ€§çš„å¯¹æŠ—é˜²å¾¡ç³»ç»Ÿæä¾›äº†æŒ‡å¯¼ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "18 pages, 7 figures",
      "pdf_url": "https://arxiv.org/pdf/2505.22839v1",
      "published_date": "2025-05-28 20:19:21 UTC",
      "updated_date": "2025-05-28 20:19:21 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T15:10:31.430558+00:00"
    },
    {
      "arxiv_id": "2505.22831v1",
      "title": "Orca: Browsing at Scale Through User-Driven and AI-Facilitated Orchestration Across Malleable Webpages",
      "title_zh": "Orcaï¼šåŸºäºè·¨å¯å¡‘æ€§ç½‘é¡µçš„ç”¨æˆ·é©±åŠ¨ä¸äººå·¥æ™ºèƒ½è¾…åŠ©ç¼–æ’çš„å¤§è§„æ¨¡æµè§ˆ",
      "authors": [
        "Peiling Jiang",
        "Haijun Xia"
      ],
      "abstract": "Web-based activities are fundamentally distributed across webpages. However, conventional browsers with stacks of tabs fail to support operating and synthesizing large volumes of information across pages. While recent AI systems enable fully automated web browsing and information synthesis, they often diminish user agency and hinder contextual understanding. Therefore, we explore how AI could instead augment users' interactions with content across webpages and mitigate cognitive and manual efforts. Through literature on information tasks and web browsing challenges, and an iterative design process, we present a rich set of novel interactions with our prototype web browser, Orca. Leveraging AI, Orca supports user-driven exploration, operation, organization, and synthesis of web content at scale. To enable browsing at scale, webpages are treated as malleable materials that humans and AI can collaboratively manipulate and compose into a malleable, dynamic, and browser-level workspace. Our evaluation revealed an increased \"appetite\" for information foraging, enhanced user control, and more flexibility in sensemaking across a broader information landscape on the web.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†Orcaï¼Œä¸€ç§é€šè¿‡AIè¾…åŠ©ä¸ç”¨æˆ·é©±åŠ¨åä½œçš„æ–°å‹åŸå‹æµè§ˆå™¨ï¼Œæ—¨åœ¨è§£å†³ä¼ ç»Ÿæµè§ˆå™¨æ ‡ç­¾é¡µæ¨¡å¼åœ¨è·¨é¡µé¢å¤§è§„æ¨¡ä¿¡æ¯æ•´åˆä¸åˆæˆæ–¹é¢çš„å±€é™ã€‚Orcaå°†ç½‘é¡µè§†ä¸ºå¯å¡‘æ€§ææ–™(Malleable Materials)ï¼Œå…è®¸äººç±»ä¸AIååŒå°†å…¶é‡æ„ä¸ºä¸€ä¸ªåŠ¨æ€ä¸”å…·å¤‡æµè§ˆå™¨å±‚çº§çš„å·¥ä½œç©ºé—´ã€‚å€ŸåŠ©äºAIèƒ½åŠ›ï¼ŒOrcaæ”¯æŒå¤§è§„æ¨¡çš„ç½‘é¡µå†…å®¹æ¢ç´¢ã€æ“ä½œã€ç»„ç»‡å’Œåˆæˆï¼Œæœ‰æ•ˆå‡è½»äº†ç”¨æˆ·çš„è®¤çŸ¥è´Ÿæ‹…å’Œæ‰‹åŠ¨å·¥ä½œé‡ã€‚è¯„ä¼°ç»“æœæ˜¾ç¤ºï¼ŒOrcaæ˜¾è‘—æé«˜äº†ç”¨æˆ·çš„ä¿¡æ¯æœå¯»(Information Foraging)æ„æ„¿ï¼Œå¢å¼ºäº†ç”¨æˆ·çš„æŒæ§æ„Ÿï¼Œå¹¶ä¸ºåœ¨å¹¿æ³›ä¿¡æ¯æ™¯è§‚ä¸­è¿›è¡Œæ„ä¹‰å»ºæ„(Sensemaking)æä¾›äº†æ›´é«˜çš„çµæ´»æ€§ã€‚",
      "categories": [
        "cs.HC",
        "cs.AI"
      ],
      "primary_category": "cs.HC",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.22831v1",
      "published_date": "2025-05-28 20:13:39 UTC",
      "updated_date": "2025-05-28 20:13:39 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T15:10:43.013631+00:00"
    },
    {
      "arxiv_id": "2505.22830v3",
      "title": "What Has Been Lost with Synthetic Evaluation?",
      "title_zh": "åˆæˆè¯„ä¼°ä¸­å¤±å»äº†ä»€ä¹ˆï¼Ÿ",
      "authors": [
        "Alexander Gill",
        "Abhilasha Ravichander",
        "Ana MarasoviÄ‡"
      ],
      "abstract": "Large language models (LLMs) are increasingly used for data generation. However, creating evaluation benchmarks raises the bar for this emerging paradigm. Benchmarks must target specific phenomena, penalize exploiting shortcuts, and be challenging. Through two case studies, we investigate whether LLMs can meet these demands by generating reasoning over-text benchmarks and comparing them to those created through careful crowdsourcing. Specifically, we evaluate both the validity and difficulty of LLM-generated versions of two high-quality reading comprehension datasets: CondaQA, which evaluates reasoning about negation, and DROP, which targets reasoning about quantities. We find that prompting LLMs can produce variants of these datasets that are often valid according to the annotation guidelines, at a fraction of the cost of the original crowdsourcing effort. However, we show that they are less challenging for LLMs than their human-authored counterparts. This finding sheds light on what may have been lost by generating evaluation data with LLMs, and calls for critically reassessing the immediate use of this increasingly prevalent approach to benchmark creation.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†åœ¨å¤§è¯­è¨€æ¨¡å‹ (LLMs) å¹¿æ³›ç”¨äºæ•°æ®ç”Ÿæˆçš„èƒŒæ™¯ä¸‹ï¼Œåˆæˆè¯„ä¼°åŸºå‡† (Synthetic Evaluation) æ˜¯å¦èƒ½æ›¿ä»£ç”±ç²¾å¿ƒä¼—åŒ…å®Œæˆçš„é«˜è´¨é‡è¯„ä¼°æ•°æ®é›†ã€‚é€šè¿‡å¯¹ä¸¤ä¸ªé˜…è¯»ç†è§£æ•°æ®é›† CondaQA (è€ƒå¯Ÿå¦å®šæ¨ç†) å’Œ DROP (è€ƒå¯Ÿæ•°é‡æ¨ç†) è¿›è¡Œæ¡ˆä¾‹ç ”ç©¶ï¼Œä½œè€…æ¯”è¾ƒäº† LLM ç”Ÿæˆçš„ç‰ˆæœ¬ä¸äººå·¥ä¼—åŒ…ç‰ˆæœ¬çš„æœ‰æ•ˆæ€§å’Œéš¾åº¦ã€‚ç ”ç©¶å‘ç°ï¼Œè™½ç„¶åˆ©ç”¨ LLM èƒ½å¤Ÿä»¥æä½çš„æˆæœ¬ç”Ÿæˆç¬¦åˆæ ‡æ³¨å‡†åˆ™çš„æœ‰æ•ˆæ•°æ®ï¼Œä½†è¿™äº›åˆæˆæ•°æ®å¯¹ LLMs çš„æŒ‘æˆ˜æ€§è¿œä½äºäººå·¥ç¼–å†™çš„å¯¹åº”ç‰ˆæœ¬ã€‚è¿™ä¸€å‘ç°æ­ç¤ºäº†ä½¿ç”¨ LLMs ç”Ÿæˆè¯„ä¼°æ•°æ®å¯èƒ½å¯¼è‡´çš„æ·±åº¦ç¼ºå¤±ï¼Œå¹¶å‘¼åå­¦æœ¯ç•Œæ‰¹åˆ¤æ€§åœ°é‡æ–°è¯„ä¼°è¿™ç§æ—¥ç›Šæµè¡Œçš„åŸºå‡†æµ‹è¯•æ„å»ºæ–¹æ³•ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "v3: Camera Ready",
      "pdf_url": "https://arxiv.org/pdf/2505.22830v3",
      "published_date": "2025-05-28 20:12:32 UTC",
      "updated_date": "2025-10-03 22:11:15 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T15:10:36.342728+00:00"
    },
    {
      "arxiv_id": "2505.22829v1",
      "title": "Bridging Distribution Shift and AI Safety: Conceptual and Methodological Synergies",
      "title_zh": "è¡”æ¥åˆ†å¸ƒåç§»ä¸äººå·¥æ™ºèƒ½å®‰å…¨ï¼šæ¦‚å¿µä¸æ–¹æ³•è®ºçš„ååŒæ•ˆåº”",
      "authors": [
        "Chenruo Liu",
        "Kenan Tang",
        "Yao Qin",
        "Qi Lei"
      ],
      "abstract": "This paper bridges distribution shift and AI safety through a comprehensive analysis of their conceptual and methodological synergies. While prior discussions often focus on narrow cases or informal analogies, we establish two types connections between specific causes of distribution shift and fine-grained AI safety issues: (1) methods addressing a specific shift type can help achieve corresponding safety goals, or (2) certain shifts and safety issues can be formally reduced to each other, enabling mutual adaptation of their methods. Our findings provide a unified perspective that encourages fundamental integration between distribution shift and AI safety research.",
      "tldr_zh": "è¯¥ç ”ç©¶é€šè¿‡å…¨é¢åˆ†æ Distribution Shift ä¸ AI Safety ä¹‹é—´çš„æ¦‚å¿µå’Œæ–¹æ³•ååŒæ•ˆåº”ï¼Œå»ºç«‹äº†ä¸¤è€…ä¹‹é—´çš„æ·±åº¦è”ç³»ã€‚ä¸ä»¥å¾€ä»…å…³æ³¨ç‹­éš˜æ¡ˆä¾‹æˆ–éæ­£å¼ç±»æ¯”çš„ç ”ç©¶ä¸åŒï¼Œæœ¬æ–‡æå‡ºäº†ä¸¤ç§å…·ä½“çš„è¿æ¥æ–¹å¼ï¼šä¸€æ˜¯è§£å†³ç‰¹å®šåç§»ç±»å‹çš„æ–¹æ³•å¯ä»¥è¾…åŠ©å®ç°ç›¸åº”çš„å®‰å…¨ç›®æ ‡ï¼ŒäºŒæ˜¯æŸäº›åç§»å’Œå®‰å…¨é—®é¢˜åœ¨å½¢å¼ä¸Šå¯ä»¥ç›¸äº’å½’çº¦ï¼Œä»è€Œå®ç°æ–¹æ³•çš„ç›¸äº’é€‚åº”ã€‚è¿™äº›å‘ç°ä¸º Distribution Shift ä¸ AI Safety ç ”ç©¶çš„æ ¹æœ¬æ€§æ•´åˆæä¾›äº†ä¸€ä¸ªç»Ÿä¸€è§†è§’ã€‚è¯¥ç ”ç©¶ä¸ä»…æ˜ç¡®äº†ä¸åŒé¢†åŸŸé—´çš„ç†è®ºå…³è”ï¼Œè¿˜ä¸ºåˆ©ç”¨ç°æœ‰æŠ€æœ¯è§£å†³æ›´å¹¿æ³›çš„å®‰å…¨æŒ‘æˆ˜å¥ å®šäº†åŸºç¡€ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "35 pages",
      "pdf_url": "https://arxiv.org/pdf/2505.22829v1",
      "published_date": "2025-05-28 20:11:30 UTC",
      "updated_date": "2025-05-28 20:11:30 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T15:10:54.804701+00:00"
    },
    {
      "arxiv_id": "2505.22825v1",
      "title": "PGLearn -- An Open-Source Learning Toolkit for Optimal Power Flow",
      "title_zh": "PGLearnï¼šé¢å‘æœ€ä¼˜æ½®æµçš„å¼€æºå­¦ä¹ å·¥å…·åŒ…",
      "authors": [
        "Michael Klamkin",
        "Mathieu Tanneau",
        "Pascal Van Hentenryck"
      ],
      "abstract": "Machine Learning (ML) techniques for Optimal Power Flow (OPF) problems have recently garnered significant attention, reflecting a broader trend of leveraging ML to approximate and/or accelerate the resolution of complex optimization problems. These developments are necessitated by the increased volatility and scale in energy production for modern and future grids. However, progress in ML for OPF is hindered by the lack of standardized datasets and evaluation metrics, from generating and solving OPF instances, to training and benchmarking machine learning models. To address this challenge, this paper introduces PGLearn, a comprehensive suite of standardized datasets and evaluation tools for ML and OPF. PGLearn provides datasets that are representative of real-life operating conditions, by explicitly capturing both global and local variability in the data generation, and by, for the first time, including time series data for several large-scale systems. In addition, it supports multiple OPF formulations, including AC, DC, and second-order cone formulations. Standardized datasets are made publicly available to democratize access to this field, reduce the burden of data generation, and enable the fair comparison of various methodologies. PGLearn also includes a robust toolkit for training, evaluating, and benchmarking machine learning models for OPF, with the goal of standardizing performance evaluation across the field. By promoting open, standardized datasets and evaluation metrics, PGLearn aims at democratizing and accelerating research and innovation in machine learning applications for optimal power flow problems. Datasets are available for download at https://www.huggingface.co/PGLearn.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¨å‡ºäº†PGLearnï¼Œä¸€ä¸ªé’ˆå¯¹æœ€ä¼˜æ½®æµ(Optimal Power Flow, OPF)é—®é¢˜çš„å¼€æºæœºå™¨å­¦ä¹ (ML)å·¥å…·åŒ…ã€‚ä¸ºäº†è§£å†³å½“å‰é¢†åŸŸå†…ç¼ºä¹æ ‡å‡†åŒ–æ•°æ®é›†å’Œè¯„ä¼°æŒ‡æ ‡çš„æŒ‘æˆ˜ï¼ŒPGLearn æä¾›äº†ä»£è¡¨çœŸå®ç”µåŠ›ç³»ç»Ÿè¿è¡ŒçŠ¶å†µçš„æ•°æ®é›†ï¼Œå¹¶é¦–æ¬¡å¼•å…¥äº†å¤§è§„æ¨¡ç³»ç»Ÿçš„æ—¶åºæ•°æ®ä»¥æ•æ‰æ•°æ®çš„å…¨å±€å’Œå±€éƒ¨å˜å¼‚æ€§ã€‚è¯¥å·¥å…·åŒ…æ”¯æŒ ACã€DC å’ŒäºŒé˜¶é”¥(Second-order cone)ç­‰å¤šç§ OPF å»ºæ¨¡å½¢å¼ï¼Œç¡®ä¿äº†å¯¹ä¸åŒå¤æ‚ä¼˜åŒ–é—®é¢˜çš„è¦†ç›–ã€‚æ­¤å¤–ï¼ŒPGLearn è¿˜æä¾›äº†ä¸€å¥—ç”¨äº ML æ¨¡å‹è®­ç»ƒã€è¯„ä¼°å’ŒåŸºå‡†æµ‹è¯•(Benchmarking)çš„å®Œæ•´å·¥å…·é“¾ï¼Œæ—¨åœ¨å®ç°æ€§èƒ½è¯„ä»·çš„æ ‡å‡†åŒ–ã€‚é€šè¿‡å…¬å¼€å‘å¸ƒæ•°æ®é›†ï¼Œè¯¥é¡¹ç›®é™ä½äº†ç ”ç©¶é—¨æ§›å¹¶ä¿ƒè¿›äº†ä¸åŒæ–¹æ³•é—´çš„å…¬å¹³æ¯”è¾ƒï¼Œä»è€ŒåŠ é€Ÿæœºå™¨å­¦ä¹ åœ¨ç°ä»£ç”µç½‘ä¼˜åŒ–ä¸­çš„åº”ç”¨ä¸åˆ›æ–°ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "eess.SY",
        "math.OC"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.22825v1",
      "published_date": "2025-05-28 20:10:04 UTC",
      "updated_date": "2025-05-28 20:10:04 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T15:10:48.642680+00:00"
    },
    {
      "arxiv_id": "2505.22820v2",
      "title": "Preference Learning with Response Time: Robust Losses and Guarantees",
      "title_zh": "ç»“åˆå“åº”æ—¶é—´çš„åå¥½å­¦ä¹ ï¼šé²æ£’æŸå¤±ä¸ç†è®ºä¿è¯",
      "authors": [
        "Ayush Sawarni",
        "Sahasrajit Sarmasarkar",
        "Vasilis Syrgkanis"
      ],
      "abstract": "This paper investigates the integration of response time data into human preference learning frameworks for more effective reward model elicitation. While binary preference data has become fundamental in fine-tuning foundation models, generative AI systems, and other large-scale models, the valuable temporal information inherent in user decision-making remains largely unexploited. We propose novel methodologies to incorporate response time information alongside binary choice data, leveraging the Evidence Accumulation Drift Diffusion (EZ) model, under which response time is informative of the preference strength. We develop Neyman-orthogonal loss functions that achieve oracle convergence rates for reward model learning, matching the theoretical optimal rates that would be attained if the expected response times for each query were known a priori. Our theoretical analysis demonstrates that for linear reward functions, conventional preference learning suffers from error rates that scale exponentially with reward magnitude. In contrast, our response time-augmented approach reduces this to polynomial scaling, representing a significant improvement in sample efficiency. We extend these guarantees to non-parametric reward function spaces, establishing convergence properties for more complex, realistic reward models. Our extensive experiments validate our theoretical findings in the context of preference learning over images.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†å°†å“åº”æ—¶é—´(Response Time)æ•°æ®æ•´åˆåˆ°äººç±»åå¥½å­¦ä¹ æ¡†æ¶ä¸­ï¼Œä»¥å®ç°æ›´æœ‰æ•ˆçš„å¥–åŠ±æ¨¡å‹è¯±å¯¼ã€‚ä½œè€…åˆ©ç”¨è¯æ®ç§¯ç´¯æ¼‚ç§»æ‰©æ•£æ¨¡å‹(Evidence Accumulation Drift Diffusion (EZ) model)ï¼Œå°†å“åº”æ—¶é—´è§†ä¸ºåå¥½å¼ºåº¦çš„å…³é”®æŒ‡æ ‡ï¼Œå¹¶æå‡ºäº†å…·æœ‰Neyman-orthogonalç‰¹æ€§çš„æŸå¤±å‡½æ•°ã€‚ç†è®ºåˆ†æè¡¨æ˜ï¼Œå¯¹äºçº¿æ€§å¥–åŠ±å‡½æ•°ï¼Œè¯¥æ–¹æ³•å°†ä¼ ç»Ÿåå¥½å­¦ä¹ ä¸­éšå¥–åŠ±å¹…åº¦æŒ‡æ•°çº§å¢é•¿çš„é”™è¯¯ç‡é™ä½åˆ°äº†å¤šé¡¹å¼çº§ï¼Œæ˜¾è‘—æå‡äº†æ ·æœ¬æ•ˆç‡å¹¶å®ç°äº†Oracleæ”¶æ•›ç‡ã€‚æ­¤å¤–ï¼Œç ”ç©¶è¿˜å°†è¿™ä¸€ç†è®ºä¿è¯æ‰©å±•åˆ°äº†éå‚æ•°åŒ–å¥–åŠ±å‡½æ•°ç©ºé—´ï¼Œç¡®ç«‹äº†å¤æ‚å¥–åŠ±æ¨¡å‹çš„æ”¶æ•›ç‰¹æ€§ã€‚é€šè¿‡åœ¨å›¾åƒåå¥½å­¦ä¹ åœºæ™¯ä¸‹çš„å®éªŒï¼Œè¯¥ç ”ç©¶éªŒè¯äº†ç»“åˆå“åº”æ—¶é—´ä¿¡æ¯èƒ½å¤Ÿæ›´ç²¾å‡†åœ°åˆ»ç”»ç”¨æˆ·åå¥½å¹¶ä¼˜åŒ–å¥–åŠ±æ¨¡å‹ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "econ.TH",
        "stat.ML"
      ],
      "primary_category": "cs.LG",
      "comment": "Accepted at NeurIPS 2025",
      "pdf_url": "https://arxiv.org/pdf/2505.22820v2",
      "published_date": "2025-05-28 19:55:54 UTC",
      "updated_date": "2025-10-24 06:28:10 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T15:10:54.177331+00:00"
    },
    {
      "arxiv_id": "2505.22818v1",
      "title": "A Tool for Generating Exceptional Behavior Tests With Large Language Models",
      "title_zh": "åŸºäºå¤§è¯­è¨€æ¨¡å‹çš„å¼‚å¸¸è¡Œä¸ºæµ‹è¯•ç”Ÿæˆå·¥å…·",
      "authors": [
        "Linghan Zhong",
        "Samuel Yuan",
        "Jiyang Zhang",
        "Yu Liu",
        "Pengyu Nie",
        "Junyi Jessy Li",
        "Milos Gligoric"
      ],
      "abstract": "Exceptional behavior tests (EBTs) are crucial in software development for verifying that code correctly handles unwanted events and throws appropriate exceptions. However, prior research has shown that developers often prioritize testing \"happy paths\", e.g., paths without unwanted events over exceptional scenarios. We present exLong, a framework that automatically generates EBTs to address this gap. exLong leverages a large language model (LLM) fine-tuned from CodeLlama and incorporates reasoning about exception-throwing traces, conditional expressions that guard throw statements, and non-exceptional behavior tests that execute similar traces. Our demonstration video illustrates how exLong can effectively assist developers in creating comprehensive EBTs for their project (available at https://youtu.be/Jro8kMgplZk).",
      "tldr_zh": "è¿™é¡¹ç ”ç©¶é’ˆå¯¹å¼€å‘è€…åœ¨è½¯ä»¶å¼€å‘ä¸­å¾€å¾€ä¼˜å…ˆæµ‹è¯•â€œæ­£å¸¸è·¯å¾„â€è€Œå¿½è§†å¼‚å¸¸åœºæ™¯çš„é—®é¢˜ï¼Œæå‡ºäº†ä¸€ä¸ªåä¸º exLong çš„è‡ªåŠ¨ç”Ÿæˆå¼‚å¸¸è¡Œä¸ºæµ‹è¯• (Exceptional behavior tests, EBTs) çš„æ¡†æ¶ã€‚exLong æ ¸å¿ƒåˆ©ç”¨äº†åŸºäº CodeLlama å¾®è°ƒçš„å¤§è¯­è¨€æ¨¡å‹ (Large Language Model, LLM)ï¼Œå¹¶ç»“åˆäº†å¯¹å¼‚å¸¸æŠ›å‡ºè·¯å¾„ (exception-throwing traces) çš„æ¨ç†ã€‚è¯¥æ¡†æ¶è¿›ä¸€æ­¥æ•´åˆäº†å¯¹ä¿æŠ¤æŠ›å‡ºè¯­å¥çš„æ¡ä»¶è¡¨è¾¾å¼ (conditional expressions) ä»¥åŠæ‰§è¡Œç›¸ä¼¼è·¯å¾„çš„éå¼‚å¸¸è¡Œä¸ºæµ‹è¯•çš„åˆ†æã€‚é€šè¿‡è¿™ç§å¤šç»´åº¦çš„æ¨ç†æœºåˆ¶ï¼ŒexLong èƒ½å¤Ÿæœ‰æ•ˆåœ°å¼¥è¡¥å¼‚å¸¸è·¯å¾„æµ‹è¯•çš„ç©ºç™½ï¼Œç¡®ä¿ä»£ç èƒ½æ­£ç¡®å¤„ç†éé¢„æœŸäº‹ä»¶å¹¶æŠ›å‡ºé€‚å½“çš„å¼‚å¸¸ã€‚å®é™…åº”ç”¨è¡¨æ˜ï¼Œè¯¥å·¥å…·èƒ½æœ‰æ•ˆååŠ©å¼€å‘è€…ä¸ºå…¶é¡¹ç›®æ„å»ºå…¨é¢çš„ EBTsï¼Œæå‡è½¯ä»¶çš„å¥å£®æ€§ã€‚",
      "categories": [
        "cs.SE",
        "cs.AI"
      ],
      "primary_category": "cs.SE",
      "comment": "FSE 2025 Demo (Camera Ready)",
      "pdf_url": "https://arxiv.org/pdf/2505.22818v1",
      "published_date": "2025-05-28 19:53:20 UTC",
      "updated_date": "2025-05-28 19:53:20 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T15:11:16.834177+00:00"
    },
    {
      "arxiv_id": "2505.22815v2",
      "title": "IMTS is Worth Time $\\times$ Channel Patches: Visual Masked Autoencoders for Irregular Multivariate Time Series Prediction",
      "title_zh": "IMTS å³æ—¶é—´ $\\times$ é€šé“åˆ†å—ï¼šç”¨äºä¸è§„åˆ™å¤šå…ƒæ—¶é—´åºåˆ—é¢„æµ‹çš„è§†è§‰æ©ç è‡ªç¼–ç å™¨",
      "authors": [
        "Zhangyi Hu",
        "Jiemin Wu",
        "Hua Xu",
        "Mingqian Liao",
        "Ninghui Feng",
        "Bo Gao",
        "Songning Lai",
        "Yutao Yue"
      ],
      "abstract": "Irregular Multivariate Time Series (IMTS) forecasting is challenging due to the unaligned nature of multi-channel signals and the prevalence of extensive missing data. Existing methods struggle to capture reliable temporal patterns from such data due to significant missing values. While pre-trained foundation models show potential for addressing these challenges, they are typically designed for Regularly Sampled Time Series (RTS). Motivated by the visual Mask AutoEncoder's (MAE) powerful capability for modeling sparse multi-channel information and its success in RTS forecasting, we propose VIMTS, a framework adapting Visual MAE for IMTS forecasting. To mitigate the effect of missing values, VIMTS first processes IMTS along the timeline into feature patches at equal intervals. These patches are then complemented using learned cross-channel dependencies. Then it leverages visual MAE's capability in handling sparse multichannel data for patch reconstruction, followed by a coarse-to-fine technique to generate precise predictions from focused contexts. In addition, we integrate self-supervised learning for improved IMTS modeling by adapting the visual MAE to IMTS data. Extensive experiments demonstrate VIMTS's superior performance and few-shot capability, advancing the application of visual foundation models in more general time series tasks. Our code is available at https://github.com/WHU-HZY/VIMTS.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ä¸è§„åˆ™å¤šå…ƒæ—¶é—´åºåˆ—(IMTS)é¢„æµ‹ä¸­å­˜åœ¨çš„ä¿¡å·éå¯¹é½å’Œå¤§è§„æ¨¡æ•°æ®ç¼ºå¤±æŒ‘æˆ˜ï¼Œæå‡ºäº†VIMTSæ¡†æ¶ï¼Œå°†è§†è§‰æ©ç è‡ªç¼–ç å™¨(Visual MAE)æˆåŠŸé€‚é…äºIMTSé¢„æµ‹ä»»åŠ¡ã€‚VIMTSé¦–å…ˆå°†æ—¶é—´åºåˆ—æ²¿æ—¶é—´è½´åˆ’åˆ†ä¸ºç­‰é—´éš”çš„ç‰¹å¾è¡¥ä¸(Patches)ï¼Œå¹¶åˆ©ç”¨å­¦ä¹ åˆ°çš„è·¨é€šé“ä¾èµ–æ€§(Cross-channel dependencies)æ¥è¡¥å…¨ç¼ºå¤±ä¿¡æ¯ã€‚éšåï¼Œè¯¥æ¡†æ¶åˆ©ç”¨Visual MAEåœ¨å¤„ç†ç¨€ç–å¤šé€šé“æ•°æ®æ–¹é¢çš„ä¼˜åŠ¿è¿›è¡Œè¡¥ä¸é‡å»ºï¼Œå¹¶é‡‡ç”¨ç”±ç²—åˆ°ç»†(Coarse-to-fine)çš„æŠ€æœ¯ç”Ÿæˆç²¾ç¡®é¢„æµ‹ã€‚æ­¤å¤–ï¼Œç ”ç©¶è¿˜å¼•å…¥äº†è‡ªç›‘ç£å­¦ä¹ (Self-supervised learning)ä»¥è¿›ä¸€æ­¥ä¼˜åŒ–IMTSå»ºæ¨¡ã€‚å®éªŒç»“æœè¯æ˜ï¼ŒVIMTSåœ¨é¢„æµ‹å‡†ç¡®æ€§åŠå°‘æ ·æœ¬(Few-shot)å­¦ä¹ èƒ½åŠ›ä¸Šè¡¨ç°å“è¶Šï¼Œæœ‰æ•ˆæ‹“å±•äº†è§†è§‰åŸºç¡€æ¨¡å‹åœ¨é€šç”¨æ—¶é—´åºåˆ—ä»»åŠ¡ä¸­çš„åº”ç”¨è¾¹ç•Œã€‚",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "ICML 2025",
      "pdf_url": "https://arxiv.org/pdf/2505.22815v2",
      "published_date": "2025-05-28 19:44:03 UTC",
      "updated_date": "2025-05-30 02:28:59 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T15:11:00.468502+00:00"
    },
    {
      "arxiv_id": "2505.22814v2",
      "title": "A Large Language Model-Enabled Control Architecture for Dynamic Resource Capability Exploration in Multi-Agent Manufacturing Systems",
      "title_zh": "é¢å‘å¤šæ™ºèƒ½ä½“åˆ¶é€ ç³»ç»ŸåŠ¨æ€èµ„æºèƒ½åŠ›æ¢ç´¢çš„å¤§è¯­è¨€æ¨¡å‹èµ‹èƒ½æ§åˆ¶æ¶æ„",
      "authors": [
        "Jonghan Lim",
        "Ilya Kovalenko"
      ],
      "abstract": "Manufacturing environments are becoming more complex and unpredictable due to factors such as demand variations and shorter product lifespans. This complexity requires real-time decision-making and adaptation to disruptions. Traditional control approaches highlight the need for advanced control strategies capable of overcoming unforeseen challenges, as they demonstrate limitations in responsiveness within dynamic industrial settings. Multi-agent systems address these challenges through decentralization of decision-making, enabling systems to respond dynamically to operational changes. However, current multi-agent systems encounter challenges related to real-time adaptation, context-aware decision-making, and the dynamic exploration of resource capabilities. Large language models provide the possibility to overcome these limitations through context-aware decision-making capabilities. This paper introduces a large language model-enabled control architecture for multi-agent manufacturing systems to dynamically explore resource capabilities in response to real-time disruptions. A simulation-based case study demonstrates that the proposed architecture improves system resilience and flexibility. The case study findings show improved throughput and efficient resource utilization compared to existing approaches.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹åˆ¶é€ ç¯å¢ƒæ—¥ç›Šå¤æ‚ä¸”éš¾ä»¥é¢„æµ‹çš„æŒ‘æˆ˜ï¼Œæå‡ºäº†ä¸€ç§åŸºäºLarge Language Model (LLM)çš„å¤šæ™ºèƒ½ä½“åˆ¶é€ ç³»ç»Ÿ(Multi-Agent Manufacturing Systems)æ§åˆ¶æ¶æ„ã€‚è¯¥æ¶æ„åˆ©ç”¨LLMçš„Context-aware Decision-makingèƒ½åŠ›ï¼Œè§£å†³äº†ä¼ ç»Ÿå¤šæ™ºèƒ½ä½“ç³»ç»Ÿåœ¨å®æ—¶é€‚åº”å’ŒResource CapabilityåŠ¨æ€æ¢ç´¢æ–¹é¢çš„å±€é™æ€§ã€‚é€šè¿‡å°†LLMé›†æˆåˆ°æ§åˆ¶é€»è¾‘ä¸­ï¼Œç³»ç»Ÿèƒ½å¤Ÿæ›´æœ‰æ•ˆåœ°å“åº”ç”Ÿäº§è¿‡ç¨‹ä¸­çš„å®æ—¶ä¸­æ–­ã€‚åŸºäºæ¨¡æ‹Ÿçš„æ¡ˆä¾‹ç ”ç©¶è¯æ˜ï¼Œè¯¥æ¶æ„æ˜¾è‘—æå‡äº†åˆ¶é€ ç³»ç»Ÿçš„Resilienceå’ŒFlexibilityã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼Œè¯¥æ¶æ„åœ¨æé«˜ååé‡å’Œä¼˜åŒ–èµ„æºåˆ©ç”¨ç‡æ–¹é¢å…·æœ‰æ˜æ˜¾ä¼˜åŠ¿ï¼Œä¸ºå®ç°è‡ªä¸»åŒ–ã€åŠ¨æ€åŒ–çš„å·¥ä¸šæ§åˆ¶æä¾›äº†æ–°æ€è·¯ã€‚",
      "categories": [
        "cs.MA",
        "cs.AI"
      ],
      "primary_category": "cs.MA",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.22814v2",
      "published_date": "2025-05-28 19:43:12 UTC",
      "updated_date": "2025-06-28 20:02:25 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T15:11:32.455003+00:00"
    },
    {
      "arxiv_id": "2505.22809v2",
      "title": "First Steps Towards Overhearing LLM Agents: A Case Study With Dungeons & Dragons Gameplay",
      "title_zh": "è¿ˆå‘â€œæ—å¬å‹â€å¤§è¯­è¨€æ¨¡å‹æ™ºèƒ½ä½“ï¼šä»¥ã€Šé¾™ä¸åœ°ä¸‹åŸã€‹æ¸¸æˆä¸ºä¾‹çš„æ¡ˆä¾‹ç ”ç©¶",
      "authors": [
        "Andrew Zhu",
        "Evan Osgood",
        "Chris Callison-Burch"
      ],
      "abstract": "Much work has been done on conversational LLM agents which directly assist human users with tasks. We present an alternative paradigm for interacting with LLM agents, which we call \"overhearing agents\". These overhearing agents do not actively participate in conversation -- instead, they \"listen in\" on human-to-human conversations and perform background tasks or provide suggestions to assist the user. In this work, we explore the overhearing agents paradigm through the lens of Dungeons & Dragons gameplay. We present an in-depth study using large multimodal audio-language models as overhearing agents to assist a Dungeon Master. We perform a human evaluation to examine the helpfulness of such agents and find that some large audio-language models have the emergent ability to perform overhearing agent tasks using implicit audio cues. Finally, we release Python libraries and our project code to support further research into the overhearing agents paradigm at https://github.com/zhudotexe/overhearing_agents.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§åä¸º overhearing agents çš„æ–°å‹äº¤äº’èŒƒå¼ï¼Œä¸ç›´æ¥å‚ä¸å¯¹è¯çš„ä¼ ç»Ÿ LLM agents ä¸åŒï¼Œè¿™ç±»æ™ºèƒ½ä½“é€šè¿‡â€œæ—å¬â€äººä¸äººä¹‹é—´çš„å¯¹è¯æ¥æ‰§è¡Œåå°ä»»åŠ¡æˆ–æä¾›è¾…åŠ©å»ºè®®ã€‚ç ”ç©¶ä»¥ Dungeons & Dragons æ¸¸æˆä¸ºæ¡ˆä¾‹ï¼Œæ·±å…¥æ¢è®¨äº†ä½¿ç”¨å¤§å‹å¤šæ¨¡æ€éŸ³é¢‘è¯­è¨€æ¨¡å‹ (multimodal audio-language models) ä½œä¸ºæ—å¬æ™ºèƒ½ä½“æ¥è¾…åŠ© Dungeon Master çš„å¯è¡Œæ€§ã€‚é€šè¿‡äººç±»è¯„ä¼°å‘ç°ï¼Œéƒ¨åˆ†å¤§å‹éŸ³é¢‘è¯­è¨€æ¨¡å‹å±•ç°å‡ºäº†åˆ©ç”¨éšå«éŸ³é¢‘çº¿ç´¢æ‰§è¡Œæ—å¬ä»»åŠ¡çš„æ¶Œç°èƒ½åŠ› (emergent ability)ã€‚æ­¤å¤–ï¼Œç ”ç©¶è€…è¿˜å‘å¸ƒäº†ç›¸å…³çš„ Python åº“å’Œé¡¹ç›®ä»£ç ï¼Œæ—¨åœ¨æ¨åŠ¨ overhearing agents èŒƒå¼çš„åç»­ç ”ç©¶ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.HC"
      ],
      "primary_category": "cs.CL",
      "comment": "9 pages, 5 figures. COLM 2025 Workshop on AI Agents",
      "pdf_url": "https://arxiv.org/pdf/2505.22809v2",
      "published_date": "2025-05-28 19:34:36 UTC",
      "updated_date": "2025-09-05 16:48:02 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T15:12:04.611655+00:00"
    },
    {
      "arxiv_id": "2505.22803v1",
      "title": "CLUE: Neural Networks Calibration via Learning Uncertainty-Error alignment",
      "title_zh": "CLUEï¼šåŸºäºå­¦ä¹ ä¸ç¡®å®šæ€§-è¯¯å·®å¯¹é½çš„ç¥ç»ç½‘ç»œæ ¡å‡†",
      "authors": [
        "Pedro Mendes",
        "Paolo Romano",
        "David Garlan"
      ],
      "abstract": "Reliable uncertainty estimation is critical for deploying neural networks (NNs) in real-world applications. While existing calibration techniques often rely on post-hoc adjustments or coarse-grained binning methods, they remain limited in scalability, differentiability, and generalization across domains. In this work, we introduce CLUE (Calibration via Learning Uncertainty-Error Alignment), a novel approach that explicitly aligns predicted uncertainty with observed error during training, grounded in the principle that well-calibrated models should produce uncertainty estimates that match their empirical loss. CLUE adopts a novel loss function that jointly optimizes predictive performance and calibration, using summary statistics of uncertainty and loss as proxies. The proposed method is fully differentiable, domain-agnostic, and compatible with standard training pipelines. Through extensive experiments on vision, regression, and language modeling tasks, including out-of-distribution and domain-shift scenarios, we demonstrate that CLUE achieves superior calibration quality and competitive predictive performance with respect to state-of-the-art approaches without imposing significant computational overhead.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†CLUE (Calibration via Learning Uncertainty-Error Alignment)ï¼Œè¿™æ˜¯ä¸€ç§æ—¨åœ¨æå‡ç¥ç»ç½‘ç»œ(neural networks)å¯é ä¸ç¡®å®šæ€§ä¼°è®¡(uncertainty estimation)çš„æ–°æ–¹æ³•ã€‚é’ˆå¯¹ä¼ ç»Ÿæ ¡å‡†æŠ€æœ¯åœ¨å¯æ‰©å±•æ€§å’Œå¯å¾®åˆ†æ€§æ–¹é¢çš„å±€é™ï¼ŒCLUEé€šè¿‡åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­æ˜¾å¼åœ°å°†é¢„æµ‹çš„ä¸ç¡®å®šæ€§ä¸è§‚æµ‹åˆ°çš„è¯¯å·®è¿›è¡Œå¯¹é½ï¼Œç¡®ä¿æ¨¡å‹äº§ç”Ÿä¸å…¶ç»éªŒæŸå¤±(empirical loss)ç›¸åŒ¹é…çš„ä¸ç¡®å®šæ€§ä¼°è®¡ã€‚è¯¥æ¡†æ¶é‡‡ç”¨äº†ä¸€ç§æ–°é¢–çš„æŸå¤±å‡½æ•°ï¼Œåˆ©ç”¨ä¸ç¡®å®šæ€§å’ŒæŸå¤±çš„æ±‡æ€»ç»Ÿè®¡é‡ä½œä¸ºä»£ç†ï¼Œå®ç°å¯¹é¢„æµ‹æ€§èƒ½ä¸æ ¡å‡†è´¨é‡çš„è”åˆä¼˜åŒ–ã€‚CLUEå…·æœ‰å®Œå…¨å¯å¾®ã€é¢†åŸŸæ— å…³ä¸”ä¸æ ‡å‡†è®­ç»ƒæµç¨‹å…¼å®¹çš„ç‰¹æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œåœ¨è§†è§‰ã€å›å½’å’Œè¯­è¨€å»ºæ¨¡ç­‰å¤šç§ä»»åŠ¡ï¼ˆåŒ…æ‹¬åˆ†å¸ƒå¤–(out-of-distribution)å’Œé¢†åŸŸåç§»åœºæ™¯ï¼‰ä¸­ï¼ŒCLUEåœ¨ä¿æŒç«äº‰æ€§é¢„æµ‹æ€§èƒ½çš„åŒæ—¶ï¼Œå®ç°äº†ä¼˜äºç°æœ‰æŠ€æœ¯çš„æ ¡å‡†æ•ˆæœï¼Œä¸”æœªå¢åŠ æ˜¾è‘—çš„è®¡ç®—å¼€é”€ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.22803v1",
      "published_date": "2025-05-28 19:23:47 UTC",
      "updated_date": "2025-05-28 19:23:47 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T15:11:51.915771+00:00"
    },
    {
      "arxiv_id": "2505.22798v2",
      "title": "Efficient Preimage Approximation for Neural Network Certification",
      "title_zh": "ç¥ç»ç½‘ç»œè®¤è¯çš„é«˜æ•ˆåŸåƒè¿‘ä¼¼",
      "authors": [
        "Anton BjÃ¶rklund",
        "Mykola Zaitsev",
        "Marta Kwiatkowska"
      ],
      "abstract": "The growing reliance on artificial intelligence in safety- and security-critical applications demands effective neural network certification. A challenging real-world use case is \"patch attacks\", where adversarial patches or lighting conditions obscure parts of images, for example, traffic signs. A significant step towards certification against patch attacks was recently achieved using PREMAP, which uses under- and over-approximations of the preimage, the set of inputs that lead to a specified output, for the certification. While the PREMAP approach is versatile, it is currently limited to fully-connected neural networks of moderate dimensionality. In order to tackle broader real-world use cases, we present novel algorithmic extensions to PREMAP involving tighter bounds, adaptive Monte Carlo sampling, and improved branching heuristics. Firstly, we demonstrate that these efficiency improvements significantly outperform the original PREMAP and enable scaling to convolutional neural networks that were previously intractable. Secondly, we showcase the potential of preimage approximation methodology for analysing and certifying reliability and robustness on a range of use cases from computer vision and control.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å®‰å…¨å’Œå®‰ä¿å…³é”®åº”ç”¨ä¸­çš„ç¥ç»ç½‘ç»œè®¤è¯(Neural Network Certification)é—®é¢˜ï¼Œç‰¹åˆ«å…³æ³¨æŠµå¾¡è¡¥ä¸æ”»å‡»(Patch Attacks)çš„æŒ‘æˆ˜ã€‚ç°æœ‰çš„PREMAPæ–¹æ³•åˆ©ç”¨åŸåƒ(Preimage)çš„ä¸Šä¸‹è¿‘ä¼¼è¿›è¡Œè®¤è¯ï¼Œä½†å…¶åº”ç”¨å—é™äºä¸­ç­‰ç»´åº¦çš„å…¨è¿æ¥ç¥ç»ç½‘ç»œ(Fully-connected Neural Networks)ã€‚ä¸ºæ­¤ï¼Œè¯¥ç ”ç©¶æå‡ºäº†é’ˆå¯¹PREMAPçš„ä¸€ç³»åˆ—ç®—æ³•æ‰©å±•ï¼ŒåŒ…æ‹¬å¼•å…¥æ›´ç´§è‡´çš„ç•Œé™(Tighter Bounds)ã€è‡ªé€‚åº”è’™ç‰¹å¡æ´›é‡‡æ ·(Adaptive Monte Carlo Sampling)ä»¥åŠæ”¹è¿›çš„åˆ†æ”¯å¯å‘å¼ç®—æ³•(Branching Heuristics)ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¿™äº›æ•ˆç‡æ”¹è¿›æ˜¾è‘—ä¼˜äºåŸå§‹PREMAPï¼Œä½¿å…¶èƒ½å¤Ÿæ‰©å±•å¤„ç†ä»¥å¾€éš¾ä»¥è®¡ç®—çš„å·ç§¯ç¥ç»ç½‘ç»œ(Convolutional Neural Networks)ã€‚æœ€åï¼Œè¯¥ç ”ç©¶é€šè¿‡è®¡ç®—æœºè§†è§‰å’Œæ§åˆ¶é¢†åŸŸçš„å¤šä¸ªæ¡ˆä¾‹ï¼Œå±•ç¤ºäº†åŸåƒè¿‘ä¼¼æŠ€æœ¯åœ¨åˆ†æå’Œè®¤è¯ç³»ç»Ÿå¯é æ€§åŠé²æ£’æ€§æ–¹é¢çš„å·¨å¤§æ½œåŠ›ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CR"
      ],
      "primary_category": "cs.LG",
      "comment": "Code available at https://github.com/Anton-Bjorklund/Premap2",
      "pdf_url": "https://arxiv.org/pdf/2505.22798v2",
      "published_date": "2025-05-28 19:13:56 UTC",
      "updated_date": "2025-10-03 11:50:05 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T15:11:59.964358+00:00"
    },
    {
      "arxiv_id": "2506.03171v1",
      "title": "EdgeVidSum: Real-Time Personalized Video Summarization at the Edge",
      "title_zh": "EdgeVidSumï¼šè¾¹ç¼˜ä¾§å®æ—¶ä¸ªæ€§åŒ–è§†é¢‘æ‘˜è¦",
      "authors": [
        "Ghulam Mujtaba",
        "Eun-Seok Ryu"
      ],
      "abstract": "EdgeVidSum is a lightweight method that generates personalized, fast-forward summaries of long-form videos directly on edge devices. The proposed approach enables real-time video summarization while safeguarding user privacy through local data processing using innovative thumbnail-based techniques and efficient neural architectures. Unlike conventional methods that process entire videos frame by frame, the proposed method uses thumbnail containers to significantly reduce computational complexity without sacrificing semantic relevance. The framework employs a hierarchical analysis approach, where a lightweight 2D CNN model identifies user-preferred content from thumbnails and generates timestamps to create fast-forward summaries. Our interactive demo highlights the system's ability to create tailored video summaries for long-form videos, such as movies, sports events, and TV shows, based on individual user preferences. The entire computation occurs seamlessly on resource-constrained devices like Jetson Nano, demonstrating how EdgeVidSum addresses the critical challenges of computational efficiency, personalization, and privacy in modern video consumption environments.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† EdgeVidSumï¼Œä¸€ç§ä¸“ä¸ºè¾¹ç¼˜è®¾å¤‡è®¾è®¡çš„è½»é‡çº§æ–¹æ³•ï¼Œæ—¨åœ¨å®æ—¶ç”Ÿæˆé•¿è§†é¢‘çš„ä¸ªæ€§åŒ–å¿«è¿›æ‘˜è¦ã€‚è¯¥æ¡†æ¶é€šè¿‡åˆ›æ–°çš„ thumbnail-based æŠ€æœ¯å’Œé«˜æ•ˆçš„ç¥ç»ç½‘ç»œæ¶æ„ï¼Œåœ¨ç¡®ä¿ç”¨æˆ·éšç§çš„åŒæ—¶å®ç°æœ¬åœ°åŒ–å¤„ç†ã€‚ä¸åŒäºé€å¸§å¤„ç†çš„ä¼ ç»Ÿæ–¹æ³•ï¼ŒEdgeVidSum é‡‡ç”¨ thumbnail containers å¤§å¹…é™ä½äº†è®¡ç®—å¤æ‚åº¦ï¼Œä¸”ä¿ç•™äº†å…³é”®çš„è¯­ä¹‰ç›¸å…³æ€§ã€‚ç³»ç»Ÿåˆ©ç”¨å±‚æ¬¡åŒ–åˆ†ææ–¹æ³•ï¼Œé€šè¿‡è½»é‡çº§ 2D CNN ä»ç¼©ç•¥å›¾ä¸­è¯†åˆ«ç”¨æˆ·åå¥½å¹¶ç”Ÿæˆç²¾ç¡®æ—¶é—´æˆ³ã€‚å®éªŒè¡¨æ˜ï¼Œè¯¥ç³»ç»Ÿèƒ½åœ¨ Jetson Nano ç­‰èµ„æºå—é™è®¾å¤‡ä¸Šä¸ºç”µå½±ã€ä½“è‚²å’Œç”µè§†èŠ‚ç›®ç­‰å¤šç§å†…å®¹ç”Ÿæˆå®šåˆ¶åŒ–æ‘˜è¦ã€‚EdgeVidSum æˆåŠŸåº”å¯¹äº†ç°ä»£è§†é¢‘æ¶ˆè´¹ä¸­è®¡ç®—æ•ˆç‡ã€ä¸ªæ€§åŒ–éœ€æ±‚ä¸éšç§ä¿æŠ¤ä¹‹é—´çš„å¹³è¡¡æŒ‘æˆ˜ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.03171v1",
      "published_date": "2025-05-28 18:59:41 UTC",
      "updated_date": "2025-05-28 18:59:41 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T15:12:01.004120+00:00"
    },
    {
      "arxiv_id": "2506.05370v1",
      "title": "Contextual Memory Intelligence -- A Foundational Paradigm for Human-AI Collaboration and Reflective Generative AI Systems",
      "title_zh": "æƒ…å¢ƒè®°å¿†æ™ºèƒ½ï¼šäººæœºåä½œä¸åæ€å‹ç”Ÿæˆå¼äººå·¥æ™ºèƒ½ç³»ç»Ÿçš„åŸºç¡€èŒƒå¼",
      "authors": [
        "Kristy Wedel"
      ],
      "abstract": "A critical challenge remains unresolved as generative AI systems are quickly implemented in various organizational settings. Despite significant advances in memory components such as RAG, vector stores, and LLM agents, these systems still have substantial memory limitations. Gen AI workflows rarely store or reflect on the full context in which decisions are made. This leads to repeated errors and a general lack of clarity. This paper introduces Contextual Memory Intelligence (CMI) as a new foundational paradigm for building intelligent systems. It repositions memory as an adaptive infrastructure necessary for longitudinal coherence, explainability, and responsible decision-making rather than passive data. Drawing on cognitive science, organizational theory, human-computer interaction, and AI governance, CMI formalizes the structured capture, inference, and regeneration of context as a fundamental system capability. The Insight Layer is presented in this paper to operationalize this vision. This modular architecture uses human-in-the-loop reflection, drift detection, and rationale preservation to incorporate contextual memory into systems. The paper argues that CMI allows systems to reason with data, history, judgment, and changing context, thereby addressing a foundational blind spot in current AI architectures and governance efforts. A framework for creating intelligent systems that are effective, reflective, auditable, and socially responsible is presented through CMI. This enhances human-AI collaboration, generative AI design, and the resilience of the institutions.",
      "tldr_zh": "æœ¬ç ”ç©¶æå‡ºäº†æƒ…å¢ƒè®°å¿†æ™ºèƒ½(Contextual Memory Intelligence, CMI)è¿™ä¸€åŸºç¡€èŒƒå¼ï¼Œæ—¨åœ¨è§£å†³ç”Ÿæˆå¼äººå·¥æ™ºèƒ½(Gen AI)ç³»ç»Ÿåœ¨å†³ç­–æƒ…å¢ƒå­˜å‚¨ä¸åæ€æ–¹é¢çš„å±€é™ã€‚è¯¥èŒƒå¼å°†è®°å¿†ä»è¢«åŠ¨æ•°æ®é‡æ–°å®šä¹‰ä¸ºæ”¯æŒçºµå‘ä¸€è‡´æ€§ã€å¯è§£é‡Šæ€§å’Œè´Ÿè´£ä»»å†³ç­–çš„è‡ªé€‚åº”åŸºç¡€è®¾æ–½ï¼Œå¹¶æ­£å¼ç¡®ç«‹äº†æƒ…å¢ƒæ•æ‰ã€æ¨ç†ä¸å†ç”Ÿçš„ç³»ç»Ÿèƒ½åŠ›ã€‚æ–‡ä¸­è¿›ä¸€æ­¥ä»‹ç»äº†æ´å¯Ÿå±‚(Insight Layer)æ¨¡å—åŒ–æ¶æ„ï¼Œé€šè¿‡ç»“åˆäººç±»åœ¨ç¯(human-in-the-loop)åæ€ã€æ¼‚ç§»æ£€æµ‹å’Œç†ç”±ä¿å­˜ç­‰æœºåˆ¶ï¼Œå°†æƒ…å¢ƒè®°å¿†æœ‰æ•ˆåœ°æ•´åˆè¿› AI å·¥ä½œæµä¸­ã€‚CMI ä½¿ç³»ç»Ÿèƒ½å¤ŸååŒåˆ©ç”¨æ•°æ®ã€å†å²è®°å½•å’Œäººç±»åˆ¤æ–­è¿›è¡Œå¤æ‚æ¨ç†ï¼Œå¡«è¡¥äº†å½“å‰ AI æ¶æ„åœ¨æƒ…å¢ƒç†è§£ä¸Šçš„åŸºç¡€æ€§ç©ºç™½ã€‚é€šè¿‡è¯¥æ¡†æ¶ï¼Œç ”ç©¶ä¸ºæ„å»ºæ›´å…·åæ€æ€§ã€å¯å®¡è®¡ä¸”ç¤¾ä¼šè´£ä»»æ„Ÿå¼ºçš„æ™ºèƒ½ç³»ç»Ÿå¥ å®šäº†åŸºç¡€ï¼Œæ˜¾è‘—å¢å¼ºäº†äººæœºåä½œçš„æ·±åº¦ä¸ç³»ç»ŸéŸ§æ€§ã€‚",
      "categories": [
        "cs.AI",
        "cs.ET"
      ],
      "primary_category": "cs.AI",
      "comment": "32 pages, 9 tables, 1 figure",
      "pdf_url": "https://arxiv.org/pdf/2506.05370v1",
      "published_date": "2025-05-28 18:59:16 UTC",
      "updated_date": "2025-05-28 18:59:16 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T15:12:23.547114+00:00"
    },
    {
      "arxiv_id": "2506.03170v2",
      "title": "PALADIN : Robust Neural Fingerprinting for Text-to-Image Diffusion Models",
      "title_zh": "PALADINï¼šæ–‡ç”Ÿå›¾æ‰©æ•£æ¨¡å‹çš„é²æ£’ç¥ç»æŒ‡çº¹æŠ€æœ¯",
      "authors": [
        "Murthy L",
        "Subarna Tripathi"
      ],
      "abstract": "The risk of misusing text-to-image generative models for malicious uses, especially due to the open-source development of such models, has become a serious concern. As a risk mitigation strategy, attributing generative models with neural fingerprinting is emerging as a popular technique. There has been a plethora of recent work that aim for addressing neural fingerprinting. A trade-off between the attribution accuracy and generation quality of such models has been studied extensively. None of the existing methods yet achieved 100% attribution accuracy. However, any model with less than cent percent accuracy is practically non-deployable. In this work, we propose an accurate method to incorporate neural fingerprinting for text-to-image diffusion models leveraging the concepts of cyclic error correcting codes from the literature of coding theory.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¼€æºæ–‡æœ¬ç”Ÿæˆå›¾åƒ(Text-to-Image)æ‰©æ•£æ¨¡å‹å¯èƒ½è¢«æ¶æ„æ»¥ç”¨çš„é£é™©ï¼Œæ¢è®¨äº†ç¥ç»ç½‘ç»œæŒ‡çº¹(Neural Fingerprinting)ä½œä¸ºå½’å› è¯†åˆ«çš„å…³é”®æŠ€æœ¯ã€‚ç°æœ‰çš„ç¥ç»ç½‘ç»œæŒ‡çº¹æ–¹æ³•åœ¨å½’å› å‡†ç¡®ç‡ä¸ç”Ÿæˆè´¨é‡ä¹‹é—´å­˜åœ¨æƒè¡¡ï¼Œä¸”ç”±äºå°šæœªè¾¾åˆ°100%çš„å½’å› å‡†ç¡®ç‡ï¼Œå¯¼è‡´è¿™äº›æ¨¡å‹åœ¨å®é™…åœºæ™¯ä¸­éš¾ä»¥éƒ¨ç½²ã€‚ä¸ºæ­¤ï¼Œä½œè€…æå‡ºäº†PALADINï¼Œè¿™æ˜¯ä¸€ç§é’ˆå¯¹æ–‡æœ¬ç”Ÿæˆå›¾åƒæ‰©æ•£æ¨¡å‹çš„é«˜é²æ£’æ€§ç¥ç»ç½‘ç»œæŒ‡çº¹è¯†åˆ«æ–¹æ³•ã€‚è¯¥æ–¹æ¡ˆåˆ›æ–°æ€§åœ°åˆ©ç”¨äº†ç¼–ç ç†è®ºä¸­çš„å¾ªç¯çº é”™ç (Cyclic Error Correcting Codes)æ¦‚å¿µï¼Œæ—¨åœ¨å®ç°ç²¾ç¡®çš„æŒ‡çº¹åµŒå…¥ä¸å½’å› ã€‚é€šè¿‡å¼•å…¥çº é”™æœºåˆ¶ï¼ŒPALADINæ˜¾è‘—æå‡äº†è¯†åˆ«çš„å¯é æ€§ï¼Œä¸ºç”Ÿæˆå¼AIæ¨¡å‹çš„ç‰ˆæƒä¿æŠ¤å’Œå®‰å…¨ç›‘ç®¡æä¾›äº†æ›´å…·å®è·µä»·å€¼çš„è§£å†³æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.03170v2",
      "published_date": "2025-05-28 18:52:40 UTC",
      "updated_date": "2025-07-23 18:41:23 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T15:12:17.287569+00:00"
    },
    {
      "arxiv_id": "2505.22779v1",
      "title": "Predicting Human Depression with Hybrid Data Acquisition utilizing Physical Activity Sensing and Social Media Feeds",
      "title_zh": "åŸºäºä½“åŠ›æ´»åŠ¨æ„ŸçŸ¥ä¸ç¤¾äº¤åª’ä½“åŠ¨æ€æ··åˆæ•°æ®é‡‡é›†çš„äººç±»æŠ‘éƒç—‡é¢„æµ‹",
      "authors": [
        "Mohammad Helal Uddin",
        "Sabur Baidya"
      ],
      "abstract": "Mental disorders including depression, anxiety, and other neurological disorders pose a significant global challenge, particularly among individuals exhibiting social avoidance tendencies. This study proposes a hybrid approach by leveraging smartphone sensor data measuring daily physical activities and analyzing their social media (Twitter) interactions for evaluating an individual's depression level. Using CNN-based deep learning models and Naive Bayes classification, we identify human physical activities accurately and also classify the user sentiments. A total of 33 participants were recruited for data acquisition, and nine relevant features were extracted from the physical activities and analyzed with their weekly depression scores, evaluated using the Geriatric Depression Scale (GDS) questionnaire. Of the nine features, six are derived from physical activities, achieving an activity recognition accuracy of 95%, while three features stem from sentiment analysis of Twitter activities, yielding a sentiment analysis accuracy of 95.6%. Notably, several physical activity features exhibited significant correlations with the severity of depression symptoms. For classifying the depression severity, a support vector machine (SVM)-based algorithm is employed that demonstrated a very high accuracy of 94%, outperforming alternative models, e.g., the multilayer perceptron (MLP) and k-nearest neighbor. It is a simple approach yet highly effective in the long run for monitoring depression without breaching personal privacy.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§ç»“åˆç‰©ç†æ´»åŠ¨æ„ŸçŸ¥å’Œç¤¾äº¤åª’ä½“åé¦ˆçš„æ··åˆæ•°æ®è·å–æ–¹æ³•ï¼Œæ—¨åœ¨é€šè¿‡æ™ºèƒ½æ‰‹æœºä¼ æ„Ÿå™¨æ•°æ®å’ŒTwitteräº’åŠ¨åˆ†ææ¥è¯„ä¼°ä¸ªäººçš„æŠ‘éƒç¨‹åº¦ã€‚ç ”ç©¶é‡‡ç”¨åŸºäºCNNçš„æ·±åº¦å­¦ä¹ æ¨¡å‹è¯†åˆ«æ—¥å¸¸ç‰©ç†æ´»åŠ¨ï¼Œå¹¶åˆ©ç”¨Naive Bayesåˆ†ç±»åˆ†æç”¨æˆ·æƒ…æ„Ÿï¼Œä¸¤è€…çš„å‡†ç¡®ç‡åˆ†åˆ«è¾¾åˆ°äº†95%å’Œ95.6%ã€‚é€šè¿‡å¯¹33åå‚ä¸è€…æå–çš„9ä¸ªç›¸å…³ç‰¹å¾è¿›è¡Œåˆ†æï¼Œç ”ç©¶å‘ç°å¤šä¸ªç‰©ç†æ´»åŠ¨ç‰¹å¾ä¸åŸºäºè€å¹´æŠ‘éƒé‡è¡¨(Geriatric Depression Scale, GDS)è¯„ä¼°çš„æŠ‘éƒç—‡çŠ¶ä¸¥é‡ç¨‹åº¦å…·æœ‰æ˜¾è‘—ç›¸å…³æ€§ã€‚åœ¨æŠ‘éƒç¨‹åº¦çš„æœ€ç»ˆåˆ†ç±»ä»»åŠ¡ä¸­ï¼ŒåŸºäºæ”¯æŒå‘é‡æœº(SVM)çš„ç®—æ³•å®ç°äº†94%çš„å‡†ç¡®ç‡ï¼Œæ€§èƒ½ä¼˜äºå¤šå±‚æ„ŸçŸ¥å™¨(MLP)å’Œk-æœ€è¿‘é‚»ç®—æ³•(k-nearest neighbor)ã€‚è¯¥ç ”ç©¶æä¾›äº†ä¸€ç§ç®€å•ä¸”é«˜æ•ˆçš„æ··åˆæ–¹æ¡ˆï¼Œèƒ½å¤Ÿåœ¨ä¿æŠ¤ä¸ªäººéšç§çš„å‰æä¸‹å®ç°å¯¹æŠ‘éƒç—‡çš„é•¿æœŸæœ‰æ•ˆç›‘æµ‹ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.22779v1",
      "published_date": "2025-05-28 18:47:34 UTC",
      "updated_date": "2025-05-28 18:47:34 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T15:12:32.850812+00:00"
    },
    {
      "arxiv_id": "2505.22771v2",
      "title": "Automated Essay Scoring Incorporating Annotations from Automated Feedback Systems",
      "title_zh": "èåˆè‡ªåŠ¨åé¦ˆç³»ç»Ÿæ ‡æ³¨çš„è‡ªåŠ¨ä½œæ–‡è¯„åˆ†",
      "authors": [
        "Christopher Ormerod"
      ],
      "abstract": "This study illustrates how incorporating feedback-oriented annotations into the scoring pipeline can enhance the accuracy of automated essay scoring (AES). This approach is demonstrated with the Persuasive Essays for Rating, Selecting, and Understanding Argumentative and Discourse Elements (PERSUADE) corpus. We integrate two types of feedback-driven annotations: those that identify spelling and grammatical errors, and those that highlight argumentative components. To illustrate how this method could be applied in real-world scenarios, we employ two LLMs to generate annotations -- a generative language model used for spell correction and an encoder-based token-classifier trained to identify and mark argumentative elements. By incorporating annotations into the scoring process, we demonstrate improvements in performance using encoder-based large language models fine-tuned as classifiers.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†å¦‚ä½•é€šè¿‡å°†åé¦ˆå¯¼å‘çš„æ ‡æ³¨(feedback-oriented annotations)æ•´åˆåˆ°è¯„åˆ†æµç¨‹ä¸­ï¼Œä»¥æå‡è‡ªåŠ¨è®ºæ–‡è¯„åˆ†(Automated Essay Scoring, AES)çš„å‡†ç¡®æ€§ã€‚ç ”ç©¶äººå‘˜åˆ©ç”¨PERSUADEè¯­æ–™åº“ï¼Œé‡ç‚¹æ•´åˆäº†æ‹¼å†™ä¸è¯­æ³•é”™è¯¯ä»¥åŠè®ºè¯æˆåˆ†(argumentative components)è¿™ä¸¤ç±»å…³é”®çš„åé¦ˆé©±åŠ¨æ ‡æ³¨ã€‚åœ¨å®éªŒè®¾è®¡ä¸­ï¼Œè¯¥æ–¹æ³•é‡‡ç”¨äº†ç”Ÿæˆå¼å¤§è¯­è¨€æ¨¡å‹(generative LLMs)è´Ÿè´£æ‹¼å†™çº é”™ï¼Œå¹¶ç»“åˆåŸºäºç¼–ç å™¨çš„ä»¤ç‰Œåˆ†ç±»å™¨(encoder-based token-classifier)æ¥è‡ªåŠ¨è¯†åˆ«è®ºæ–‡ä¸­çš„è®ºè¯è¦ç´ ã€‚é€šè¿‡åœ¨è¯„åˆ†è¿‡ç¨‹ä¸­èå…¥è¿™äº›å¤šç»´åº¦çš„æ ‡æ³¨ä¿¡æ¯ï¼Œç ”ç©¶è¯æ˜å¾®è°ƒåçš„ç¼–ç å™¨å¼å¤§è¯­è¨€æ¨¡å‹ä½œä¸ºåˆ†ç±»å™¨æ—¶è¡¨ç°å‡ºæ›´ä¼˜çš„è¯„åˆ†æ€§èƒ½ã€‚è¯¥æ–¹æ³•ä¸ä»…å±•ç¤ºäº†è‡ªåŠ¨åŒ–åé¦ˆç³»ç»Ÿåœ¨è¯„åˆ†ä»»åŠ¡ä¸­çš„åº”ç”¨æ½œåŠ›ï¼Œä¹Ÿä¸ºæ„å»ºæ›´ç²¾ç¡®çš„è®ºæ–‡è¯„ä¼°æ¨¡å‹æä¾›äº†æœ‰æ•ˆçš„æŠ€æœ¯è·¯å¾„ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "10 pages, AIME-Con Conference Submission",
      "pdf_url": "https://arxiv.org/pdf/2505.22771v2",
      "published_date": "2025-05-28 18:39:56 UTC",
      "updated_date": "2025-09-02 14:02:42 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T15:12:36.054557+00:00"
    },
    {
      "arxiv_id": "2506.00049v1",
      "title": "Rethinking Hybrid Retrieval: When Small Embeddings and LLM Re-ranking Beat Bigger Models",
      "title_zh": "é‡æ–°å®¡è§†æ··åˆæ£€ç´¢ï¼šå°å‹åµŒå…¥æ¨¡å‹ä¸ LLM é‡æ’åºä¼˜äºå¤§å‹æ¨¡å‹",
      "authors": [
        "Arjun Rao",
        "Hanieh Alipour",
        "Nick Pendar"
      ],
      "abstract": "This paper presents a comparison of embedding models in tri-modal hybrid retrieval for Retrieval-Augmented Generation (RAG) systems. We investigate the fusion of dense semantic, sparse lexical, and graph-based embeddings, focusing on the performance of the MiniLM-v6 and BGE-Large architectures. Contrary to conventional assumptions, our results show that the compact MiniLM-v6 outperforms the larger BGE-Large when integrated with LLM-based re-ranking within our tri-modal hybrid framework. Experiments conducted on the SciFact, FIQA, and NFCorpus datasets demonstrate significant improvements in retrieval quality with the MiniLM-v6 configuration. The performance difference is particularly pronounced in agentic re-ranking scenarios, indicating better alignment between MiniLM-v6's embedding space and LLM reasoning. Our findings suggest that embedding model selection for RAG systems should prioritize compatibility with multi-signal fusion and LLM alignment, rather than relying solely on larger models. This approach may reduce computational requirements while improving retrieval accuracy and efficiency.",
      "tldr_zh": "è¯¥ç ”ç©¶åœ¨Retrieval-Augmented Generation (RAG)ç³»ç»Ÿçš„ä¸‰æ¨¡æ€æ··åˆæ£€ç´¢æ¡†æ¶ä¸‹ï¼Œå¯¹æ¯”åˆ†æäº†MiniLM-v6ä¸BGE-Largeç­‰åµŒå…¥æ¨¡å‹çš„æ€§èƒ½è¡¨ç°ã€‚ç ”ç©¶é‡ç‚¹è€ƒå¯Ÿäº†èåˆç¨ å¯†è¯­ä¹‰ã€ç¨€ç–è¯æ±‡å’ŒåŸºäºå›¾çš„åµŒå…¥æŠ€æœ¯ï¼Œå‘ç°åœ¨ç»“åˆLLM-based re-rankingçš„æƒ…å†µä¸‹ï¼Œä½“ç§¯æ›´å°çš„MiniLM-v6åœ¨æ€§èƒ½ä¸Šä¼˜äºè§„æ¨¡æ›´å¤§çš„BGE-Largeã€‚åœ¨SciFactã€FIQAå’ŒNFCorpusç­‰æ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼ŒMiniLM-v6çš„åµŒå…¥ç©ºé—´ä¸LLMæ¨ç†ä¹‹é—´è¡¨ç°å‡ºæ›´ä¼˜çš„å¯¹é½æ€§ã€‚è¿™ä¸€ç»“è®ºæŒ‘æˆ˜äº†â€œæ¨¡å‹è¶Šå¤§è¶Šå¥½â€çš„ä¼ ç»Ÿå‡è®¾ï¼Œå»ºè®®åœ¨æ„å»ºRAGç³»ç»Ÿæ—¶åº”ä¼˜å…ˆè€ƒè™‘åµŒå…¥æ¨¡å‹ä¸å¤šä¿¡å·èåˆ(multi-signal fusion)åŠLLMçš„å…¼å®¹æ€§ã€‚è¿™ç§æ–¹æ³•ä¸ä»…èƒ½å¤Ÿæå‡æ£€ç´¢çš„å‡†ç¡®æ€§å’Œæ•ˆç‡ï¼Œè¿˜èƒ½æ˜¾è‘—é™ä½ç³»ç»Ÿçš„è®¡ç®—èµ„æºæ¶ˆè€—ã€‚",
      "categories": [
        "cs.IR",
        "cs.AI"
      ],
      "primary_category": "cs.IR",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.00049v1",
      "published_date": "2025-05-28 18:39:40 UTC",
      "updated_date": "2025-05-28 18:39:40 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T15:12:29.908042+00:00"
    },
    {
      "arxiv_id": "2505.22767v3",
      "title": "In Dialogue with Intelligence: Rethinking Large Language Models as Collective Knowledge",
      "title_zh": "ä¸æ™ºèƒ½å¯¹è¯ï¼šå°†å¤§è¯­è¨€æ¨¡å‹é‡æ–°å®¡è§†ä¸ºé›†ä½“çŸ¥è¯†",
      "authors": [
        "Eleni Vasilaki"
      ],
      "abstract": "Large Language Models (LLMs) can be understood as Collective Knowledge (CK): a condensation of human cultural and technical output, whose apparent intelligence emerges in dialogue. This perspective article, drawing on extended interaction with ChatGPT-4, postulates differential response modes that plausibly trace their origin to distinct model subnetworks. It argues that CK has no persistent internal state or ``spine'': it drifts, it complies, and its behaviour is shaped by the user and by fine-tuning. It develops the notion of co-augmentation, in which human judgement and CK's representational reach jointly produce forms of analysis that neither could generate alone. Finally, it suggests that CK offers a tractable object for neuroscience: unlike biological brains, these systems expose their architecture, training history, and activation dynamics, making the human--CK loop itself an experimental target.",
      "tldr_zh": "è¯¥ç ”ç©¶å°†å¤§å‹è¯­è¨€æ¨¡å‹(LLMs)é‡æ–°å®šä¹‰ä¸ºé›†ä½“çŸ¥è¯†(Collective Knowledge, CK)ï¼Œè®¤ä¸ºå®ƒä»¬æ˜¯äººç±»æ–‡åŒ–ä¸æŠ€æœ¯è¾“å‡ºçš„ç¼©å½±ï¼Œå…¶æ™ºèƒ½è¡¨ç°æºäºå¯¹è¯è¿‡ç¨‹ã€‚åŸºäºä¸ChatGPT-4çš„æ·±åº¦äº¤äº’ï¼Œæ–‡ç« æå‡ºæ¨¡å‹å“åº”æ¨¡å¼å¯èƒ½æºäºç‰¹å®šçš„å­ç½‘ç»œï¼Œå¹¶æŒ‡å‡ºCKç¼ºä¹æŒä¹…çš„å†…åœ¨çŠ¶æ€æˆ–â€œè„Šæ¢â€(spine)ï¼Œå…¶è¡Œä¸ºå—ç”¨æˆ·è¾“å…¥å’Œå¾®è°ƒ(fine-tuning)çš„åŠ¨æ€å¡‘é€ ã€‚ç ”ç©¶è¿›ä¸€æ­¥æå‡ºäº†ååŒå¢å¼º(co-augmentation)æ¦‚å¿µï¼Œå¼ºè°ƒäººç±»åˆ¤æ–­åŠ›ä¸CKçš„è¡¨å¾å¹¿åº¦ç»“åˆï¼Œèƒ½äº§ç”Ÿä¸¤è€…å‡æ— æ³•ç‹¬ç«‹ç”Ÿæˆçš„åˆ†æå½¢å¼ã€‚æœ€åï¼Œä½œè€…è®¤ä¸ºCKä¸ºç¥ç»ç§‘å­¦æä¾›äº†å¯å¤„ç†çš„ç ”ç©¶å¯¹è±¡ï¼Œå…¶é€æ˜çš„æ¶æ„ä¸æ¿€æ´»åŠ¨æ€ä½¿å¾—äººç±»ä¸CKçš„äº¤äº’å¾ªç¯æˆä¸ºç†æƒ³çš„å®éªŒç›®æ ‡ã€‚",
      "categories": [
        "cs.HC",
        "cs.AI"
      ],
      "primary_category": "cs.HC",
      "comment": "7 pages, 1 table",
      "pdf_url": "https://arxiv.org/pdf/2505.22767v3",
      "published_date": "2025-05-28 18:36:00 UTC",
      "updated_date": "2025-11-03 12:13:58 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T15:12:29.411170+00:00"
    },
    {
      "arxiv_id": "2505.22762v1",
      "title": "MIAS-SAM: Medical Image Anomaly Segmentation without thresholding",
      "title_zh": "MIAS-SAMï¼šæ— éœ€é˜ˆå€¼çš„åŒ»å­¦å›¾åƒå¼‚å¸¸åˆ†å‰²",
      "authors": [
        "Marco Colussi",
        "Dragan Ahmetovic",
        "Sergio Mascetti"
      ],
      "abstract": "This paper presents MIAS-SAM, a novel approach for the segmentation of anomalous regions in medical images. MIAS-SAM uses a patch-based memory bank to store relevant image features, which are extracted from normal data using the SAM encoder. At inference time, the embedding patches extracted from the SAM encoder are compared with those in the memory bank to obtain the anomaly map. Finally, MIAS-SAM computes the center of gravity of the anomaly map to prompt the SAM decoder, obtaining an accurate segmentation from the previously extracted features. Differently from prior works, MIAS-SAM does not require to define a threshold value to obtain the segmentation from the anomaly map. Experimental results conducted on three publicly available datasets, each with a different imaging modality (Brain MRI, Liver CT, and Retina OCT) show accurate anomaly segmentation capabilities measured using DICE score. The code is available at: https://github.com/warpcut/MIAS-SAM",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†MIAS-SAMï¼Œè¿™æ˜¯ä¸€ç§ç”¨äºåŒ»å­¦å›¾åƒå¼‚å¸¸åˆ†å‰²çš„æ–°å‹æ–¹æ³•ã€‚MIAS-SAMåˆ©ç”¨patch-based memory bankå­˜å‚¨ç”±SAM encoderä»æ­£å¸¸æ•°æ®ä¸­æå–çš„å›¾åƒç‰¹å¾ã€‚åœ¨æ¨ç†é˜¶æ®µï¼Œè¯¥æ–¹æ³•é€šè¿‡å¯¹æ¯”å¾…æµ‹å›¾åƒçš„embedding patchesä¸å­˜å‚¨åº“ç‰¹å¾æ¥ç”Ÿæˆanomaly mapã€‚éšåï¼ŒMIAS-SAMè®¡ç®—å¼‚å¸¸å›¾çš„é‡å¿ƒ(center of gravity)ä½œä¸ºæç¤ºä¿¡æ¯å¼•å¯¼SAM decoderï¼Œä»è€Œåœ¨å…ˆå‰æå–çš„ç‰¹å¾åŸºç¡€ä¸Šè·å¾—ç²¾ç¡®åˆ†å‰²ã€‚ä¸ä»¥å¾€å·¥ä½œä¸åŒï¼ŒMIAS-SAMæ— éœ€å®šä¹‰é˜ˆå€¼å³å¯ç›´æ¥ä»å¼‚å¸¸å›¾ä¸­è·å–åˆ†å‰²ç»“æœã€‚åœ¨Brain MRIã€Liver CTå’ŒRetina OCTä¸‰ç§å½±åƒæ¨¡æ€çš„æ•°æ®é›†å®éªŒè¡¨æ˜ï¼Œè¯¥æ¨¡å‹åœ¨DICE scoreè¯„ä»·æŒ‡æ ‡ä¸‹å±•ç°äº†å‡ºè‰²çš„å¼‚å¸¸åˆ†å‰²èƒ½åŠ›ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.22762v1",
      "published_date": "2025-05-28 18:25:37 UTC",
      "updated_date": "2025-05-28 18:25:37 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T15:13:15.221270+00:00"
    },
    {
      "arxiv_id": "2505.22761v1",
      "title": "A comprehensive analysis of PINNs: Variants, Applications, and Challenges",
      "title_zh": "PINNså…¨é¢åˆ†æï¼šå˜ä½“ã€åº”ç”¨ä¸æŒ‘æˆ˜",
      "authors": [
        "Afila Ajithkumar Sophiya",
        "Akarsh K Nair",
        "Sepehr Maleki",
        "Senthil K. Krishnababu"
      ],
      "abstract": "Physics Informed Neural Networks (PINNs) have been emerging as a powerful computational tool for solving differential equations. However, the applicability of these models is still in its initial stages and requires more standardization to gain wider popularity. Through this survey, we present a comprehensive overview of PINNs approaches exploring various aspects related to their architecture, variants, areas of application, real-world use cases, challenges, and so on. Even though existing surveys can be identified, they fail to provide a comprehensive view as they primarily focus on either different application scenarios or limit their study to a superficial level. This survey attempts to bridge the gap in the existing literature by presenting a detailed analysis of all these factors combined with recent advancements and state-of-the-art research in PINNs. Additionally, we discuss prevalent challenges in PINNs implementation and present some of the future research directions as well. The overall contributions of the survey can be summarised into three sections: A detailed overview of PINNs architecture and variants, a performance analysis of PINNs on different equations and application domains highlighting their features. Finally, we present a detailed discussion of current issues and future research directions.",
      "tldr_zh": "è¯¥ç»¼è¿°å¯¹ç‰©ç†ä¿¡æ¯ç¥ç»ç½‘ç»œ(Physics Informed Neural Networks, PINNs)è¿™ä¸€æ±‚è§£å¾®åˆ†æ–¹ç¨‹çš„å¼ºå¤§è®¡ç®—å·¥å…·è¿›è¡Œäº†å…¨é¢åˆ†æã€‚é’ˆå¯¹ç°æœ‰è°ƒç ”åœ¨åº”ç”¨åœºæ™¯æˆ–æ·±åº¦ä¸Šçš„å±€é™æ€§ï¼Œæœ¬æ–‡é€šè¿‡æ•´åˆæœ€æ–°è¿›å±•ä¸å‰æ²¿æŠ€æœ¯ï¼Œè¯¦ç»†æ¢è®¨äº†PINNsçš„æ¶æ„ã€å˜ä½“ã€åº”ç”¨é¢†åŸŸåŠçœŸå®æ¡ˆä¾‹ã€‚ç ”ç©¶å†…å®¹ä¸»è¦åˆ†ä¸ºä¸‰ä¸ªéƒ¨åˆ†ï¼šæ·±å…¥æ¦‚è¿°PINNsçš„æ¶æ„ä¸å˜ä½“ï¼Œåˆ†æå…¶åœ¨ä¸åŒæ–¹ç¨‹å’Œåº”ç”¨é¢†åŸŸçš„æ€§èƒ½ç‰¹å¾ï¼Œå¹¶è¯¦ç»†è®¨è®ºå½“å‰é¢ä¸´çš„æŒ‘æˆ˜ä¸æœªæ¥ç ”ç©¶æ–¹å‘ã€‚è¯¥ç»¼è¿°æ—¨åœ¨å¼¥è¡¥ç°æœ‰æ–‡çŒ®çš„ç©ºç™½ï¼Œä¸ºæ¨åŠ¨PINNsçš„æ ‡å‡†åŒ–åŠå…¶åœ¨æ›´å¹¿æ³›é¢†åŸŸçš„æ™®åŠåº”ç”¨å¥ å®šç†è®ºåŸºç¡€ã€‚",
      "categories": [
        "cs.CE",
        "cs.AI"
      ],
      "primary_category": "cs.CE",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.22761v1",
      "published_date": "2025-05-28 18:25:17 UTC",
      "updated_date": "2025-05-28 18:25:17 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T15:13:09.951323+00:00"
    },
    {
      "arxiv_id": "2505.22759v2",
      "title": "FAMA: The First Large-Scale Open-Science Speech Foundation Model for English and Italian",
      "title_zh": "FAMAï¼šé¦–ä¸ªé¢å‘è‹±è¯­å’Œæ„å¤§åˆ©è¯­çš„å¤§è§„æ¨¡å¼€æ”¾ç§‘å­¦è¯­éŸ³åŸºç¡€æ¨¡å‹",
      "authors": [
        "Sara Papi",
        "Marco Gaido",
        "Luisa Bentivogli",
        "Alessio Brutti",
        "Mauro Cettolo",
        "Roberto Gretter",
        "Marco Matassoni",
        "Mohamed Nabih",
        "Matteo Negri"
      ],
      "abstract": "The development of speech foundation models (SFMs) like Whisper and SeamlessM4T has significantly advanced the field of speech processing. However, their closed nature--with inaccessible training data and code--poses major reproducibility and fair evaluation challenges. While other domains have made substantial progress toward open science by developing fully transparent models trained on open-source (OS) code and data, similar efforts in speech remain limited. To fill this gap, we introduce FAMA, the first family of open science SFMs for English and Italian, trained on 150k+ hours of OS speech data. Moreover, we present a new dataset containing 16k hours of cleaned and pseudo-labeled speech for both languages. Results show that FAMA achieves competitive performance compared to existing SFMs while being up to 8 times faster. All artifacts, including code, datasets, and models, are released under OS-compliant licenses, promoting openness in speech technology research.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¨å‡ºäº†FAMAï¼Œè¿™æ˜¯é¦–ä¸ªé’ˆå¯¹è‹±è¯­å’Œæ„å¤§åˆ©è¯­çš„å¤§è§„æ¨¡å¼€æºç§‘å­¦Speech Foundation Models (SFMs)å®¶æ—ï¼Œæ—¨åœ¨è§£å†³ç°æœ‰æ¨¡å‹å¦‚Whisperå’ŒSeamlessM4Tç”±äºè®­ç»ƒæ•°æ®å’Œä»£ç ä¸é€æ˜è€Œå¯¼è‡´çš„å¤ç°ä¸å…¬å¹³è¯„ä¼°æŒ‘æˆ˜ã€‚FAMAåœ¨è¶…è¿‡15ä¸‡å°æ—¶çš„Open-Source (OS)è¯­éŸ³æ•°æ®ä¸Šè¿›è¡Œè®­ç»ƒï¼Œå¹¶æä¾›äº†ä¸€ä¸ªåŒ…å«1.6ä¸‡å°æ—¶ç»è¿‡æ¸…æ´—å’Œä¼ªæ ‡ç­¾å¤„ç†çš„æ–°æ•°æ®é›†ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒFAMAåœ¨æ€§èƒ½ä¸Šè¶³ä»¥åª²ç¾ç°æœ‰çš„SFMsï¼Œä¸”è¿è¡Œé€Ÿåº¦æå‡äº†é«˜è¾¾8å€ã€‚è¯¥é¡¹ç›®å°†æ‰€æœ‰ä»£ç ã€æ•°æ®é›†å’Œæ¨¡å‹æ ¹æ®OSåè®®å‘å¸ƒï¼Œæå¤§åœ°æ¨åŠ¨äº†è¯­éŸ³æŠ€æœ¯ç ”ç©¶çš„é€æ˜åº¦ä¸å¼€æ”¾æ€§ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.SD"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.22759v2",
      "published_date": "2025-05-28 18:19:34 UTC",
      "updated_date": "2025-05-30 19:40:00 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T15:13:22.002373+00:00"
    },
    {
      "arxiv_id": "2505.22757v1",
      "title": "Pre-Training Curriculum for Multi-Token Prediction in Language Models",
      "title_zh": "è¯­è¨€æ¨¡å‹å¤š Token é¢„æµ‹çš„é¢„è®­ç»ƒè¯¾ç¨‹",
      "authors": [
        "Ansar Aynetdinov",
        "Alan Akbik"
      ],
      "abstract": "Multi-token prediction (MTP) is a recently proposed pre-training objective for language models. Rather than predicting only the next token (NTP), MTP predicts the next $k$ tokens at each prediction step, using multiple prediction heads. MTP has shown promise in improving downstream performance, inference speed, and training efficiency, particularly for large models. However, prior work has shown that smaller language models (SLMs) struggle with the MTP objective. To address this, we propose a curriculum learning strategy for MTP training, exploring two variants: a forward curriculum, which gradually increases the complexity of the pre-training objective from NTP to MTP, and a reverse curriculum, which does the opposite. Our experiments show that the forward curriculum enables SLMs to better leverage the MTP objective during pre-training, improving downstream NTP performance and generative output quality, while retaining the benefits of self-speculative decoding. The reverse curriculum achieves stronger NTP performance and output quality, but fails to provide any self-speculative decoding benefits.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†å¤šæ ‡è®°é¢„æµ‹(Multi-token prediction, MTP)åœ¨è¯­è¨€æ¨¡å‹é¢„è®­ç»ƒä¸­çš„åº”ç”¨ï¼Œæ—¨åœ¨è§£å†³å°è¯­è¨€æ¨¡å‹(SLMs)åœ¨åº”å¯¹MTPç›®æ ‡æ—¶çš„æ€§èƒ½ç“¶é¢ˆã€‚ä½œè€…æå‡ºäº†ä¸€ç§è¯¾ç¨‹å­¦ä¹ (curriculum learning)ç­–ç•¥ï¼Œå¹¶é‡ç‚¹ç ”ç©¶äº†ä¸¤ç§å˜ä½“ï¼šé€æ­¥å¢åŠ ç›®æ ‡å¤æ‚åº¦çš„æ­£å‘è¯¾ç¨‹(forward curriculum)ä»¥åŠåå‘è¯¾ç¨‹(reverse curriculum)ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæ­£å‘è¯¾ç¨‹èƒ½å¤Ÿå¸®åŠ©SLMsåœ¨é¢„è®­ç»ƒè¿‡ç¨‹ä¸­æ›´å¥½åœ°åˆ©ç”¨MTPç›®æ ‡ï¼Œä»è€Œæ˜¾è‘—æå‡ä¸‹æ¸¸çš„æ¬¡æ ‡è®°é¢„æµ‹(NTP)æ€§èƒ½å’Œç”Ÿæˆè¾“å‡ºè´¨é‡ã€‚æ­¤å¤–ï¼Œè¯¥ç­–ç•¥è¿˜æˆåŠŸä¿ç•™äº†è‡ªå¯å‘å¼è§£ç (self-speculative decoding)åœ¨æ¨ç†é€Ÿåº¦æ–¹é¢çš„ä¼˜åŠ¿ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼Œåå‘è¯¾ç¨‹è™½ç„¶åœ¨NTPæ€§èƒ½å’Œè¾“å‡ºè´¨é‡ä¸Šè¡¨ç°æ›´å¼ºï¼Œä½†æ— æ³•æä¾›è‡ªå¯å‘å¼è§£ç å¸¦æ¥çš„æ”¶ç›Šã€‚è¯¥ç ”ç©¶ä¸ºSLMsæœ‰æ•ˆé›†æˆMTPç›®æ ‡å¹¶å¹³è¡¡æ¨¡å‹æ€§èƒ½ä¸æ¨ç†æ•ˆç‡æä¾›äº†ä¸€ç§ç³»ç»ŸåŒ–çš„é¢„è®­ç»ƒè·¯å¾„ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "Accepted to ACL 2025 (Main)",
      "pdf_url": "https://arxiv.org/pdf/2505.22757v1",
      "published_date": "2025-05-28 18:19:18 UTC",
      "updated_date": "2025-05-28 18:19:18 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T15:13:01.063969+00:00"
    },
    {
      "arxiv_id": "2505.23847v3",
      "title": "Seven Security Challenges That Must be Solved in Cross-domain Multi-agent LLM Systems",
      "title_zh": "è·¨åŸŸå¤šæ™ºèƒ½ä½“ LLM ç³»ç»Ÿä¸­äºŸå¾…è§£å†³çš„ä¸ƒå¤§å®‰å…¨æŒ‘æˆ˜",
      "authors": [
        "Ronny Ko",
        "Jiseong Jeong",
        "Shuyuan Zheng",
        "Chuan Xiao",
        "Tae-Wan Kim",
        "Makoto Onizuka",
        "Won-Yong Shin"
      ],
      "abstract": "Large language models (LLMs) are rapidly evolving into autonomous agents that cooperate across organizational boundaries, enabling joint disaster response, supply-chain optimization, and other tasks that demand decentralized expertise without surrendering data ownership. Yet, cross-domain collaboration shatters the unified trust assumptions behind current alignment and containment techniques. An agent benign in isolation may, when receiving messages from an untrusted peer, leak secrets or violate policy, producing risks driven by emergent multi-agent dynamics rather than classical software bugs. This position paper maps the security agenda for cross-domain multi-agent LLM systems. We introduce seven categories of novel security challenges, for each of which we also present plausible attacks, security evaluation metrics, and future research guidelines.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†å¤§è¯­è¨€æ¨¡å‹(Large Language Models, LLMs)ä½œä¸ºè‡ªä¸»æ™ºèƒ½ä½“åœ¨è·¨é¢†åŸŸåä½œä¸­é¢ä¸´çš„ä¸¥å³»å®‰å…¨æŒ‘æˆ˜ï¼Œç‰¹åˆ«æ˜¯åœ¨ç¾å®³å“åº”å’Œä¾›åº”é“¾ä¼˜åŒ–ç­‰éœ€è¦å»ä¸­å¿ƒåŒ–ä¸“ä¸šçŸ¥è¯†çš„åœºæ™¯ä¸‹ã€‚ç”±äºCross-domainåä½œæ‰“ç ´äº†åŸæœ‰çš„ç»Ÿä¸€ä¿¡ä»»å‡è®¾ï¼Œå­¤ç«‹çŠ¶æ€ä¸‹è‰¯æ€§çš„æ™ºèƒ½ä½“åœ¨ä¸ä¸å¯ä¿¡Peeräº¤äº’æ—¶ï¼Œå¯èƒ½å› Multi-agent Dynamicsè€Œäº§ç”Ÿæ³„å¯†æˆ–è¿åç­–ç•¥ç­‰æ–°å…´é£é™©ã€‚è¯¥ç«‹åœºè®ºæ–‡ä¸ºè·¨é¢†åŸŸMulti-agent LLMç³»ç»Ÿç»˜åˆ¶äº†å®‰å…¨è®®ç¨‹å›¾è°±ï¼Œå¹¶ç³»ç»Ÿæ€§åœ°æå‡ºäº†ä¸ƒå¤§ç±»æ–°å‹å®‰å…¨æŒ‘æˆ˜ã€‚é’ˆå¯¹æ¯ä¸€ç±»æŒ‘æˆ˜ï¼Œç ”ç©¶è¿˜è¯¦ç»†æä¾›äº†æ½œåœ¨çš„æ”»å‡»æ¡ˆä¾‹ã€Security Evaluation Metricsä»¥åŠæœªæ¥çš„ç ”ç©¶æŒ‡å—ï¼Œä¸ºæ„å»ºå®‰å…¨å¯ä¿¡çš„è‡ªä¸»æ™ºèƒ½ä½“ç³»ç»Ÿå¥ å®šäº†ç†è®ºåŸºç¡€ã€‚",
      "categories": [
        "cs.CR",
        "cs.AI"
      ],
      "primary_category": "cs.CR",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.23847v3",
      "published_date": "2025-05-28 18:19:03 UTC",
      "updated_date": "2025-07-15 16:18:29 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T15:13:21.280385+00:00"
    },
    {
      "arxiv_id": "2505.22756v1",
      "title": "Decomposing Elements of Problem Solving: What \"Math\" Does RL Teach?",
      "title_zh": "é—®é¢˜æ±‚è§£è¦ç´ åˆ†è§£ï¼šå¼ºåŒ–å­¦ä¹ ç©¶ç«Ÿæ•™ä¼šäº†ä»€ä¹ˆæ ·çš„â€œæ•°å­¦â€ï¼Ÿ",
      "authors": [
        "Tian Qin",
        "Core Francisco Park",
        "Mujin Kwun",
        "Aaron Walsman",
        "Eran Malach",
        "Nikhil Anand",
        "Hidenori Tanaka",
        "David Alvarez-Melis"
      ],
      "abstract": "Mathematical reasoning tasks have become prominent benchmarks for assessing the reasoning capabilities of LLMs, especially with reinforcement learning (RL) methods such as GRPO showing significant performance gains. However, accuracy metrics alone do not support fine-grained assessment of capabilities and fail to reveal which problem-solving skills have been internalized. To better understand these capabilities, we propose to decompose problem solving into fundamental capabilities: Plan (mapping questions to sequences of steps), Execute (correctly performing solution steps), and Verify (identifying the correctness of a solution). Empirically, we find that GRPO mainly enhances the execution skill-improving execution robustness on problems the model already knows how to solve-a phenomenon we call temperature distillation. More importantly, we show that RL-trained models struggle with fundamentally new problems, hitting a 'coverage wall' due to insufficient planning skills. To explore RL's impact more deeply, we construct a minimal, synthetic solution-tree navigation task as an analogy for mathematical problem-solving. This controlled setup replicates our empirical findings, confirming RL primarily boosts execution robustness. Importantly, in this setting, we identify conditions under which RL can potentially overcome the coverage wall through improved exploration and generalization to new solution paths. Our findings provide insights into the role of RL in enhancing LLM reasoning, expose key limitations, and suggest a path toward overcoming these barriers. Code is available at https://github.com/cfpark00/RL-Wall.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†å¼ºåŒ–å­¦ä¹ (RL)åœ¨æå‡å¤§è¯­è¨€æ¨¡å‹(LLMs)æ•°å­¦æ¨ç†èƒ½åŠ›ä¸­çš„å…·ä½“æœºåˆ¶ï¼Œæå‡ºå°†è§£é¢˜è¿‡ç¨‹åˆ†è§£ä¸ºè®¡åˆ’(Plan)ã€æ‰§è¡Œ(Execute)å’ŒéªŒè¯(Verify)ä¸‰ä¸ªæ ¸å¿ƒç»´åº¦ã€‚é€šè¿‡å¯¹GRPOç®—æ³•çš„å®éªŒå‘ç°ï¼ŒRLä¸»è¦æå‡äº†æ¨¡å‹åœ¨æ‰§è¡Œ(Execute)é˜¶æ®µçš„é²æ£’æ€§ï¼Œç ”ç©¶è€…å°†è¿™ç§æ”¹è¿›å·²çŸ¥é—®é¢˜å¤„ç†èƒ½åŠ›çš„ç°è±¡å®šä¹‰ä¸ºæ¸©åº¦è’¸é¦(Temperature Distillation)ã€‚ç„¶è€Œï¼Œç ”ç©¶ä¹ŸæŒ‡å‡ºRLè®­ç»ƒçš„æ¨¡å‹åœ¨é¢å¯¹é™Œç”Ÿé—®é¢˜æ—¶ä¼šé­é‡â€œè¦†ç›–å¢™â€(Coverage Wall)ï¼Œå…¶æ ¹æœ¬åŸå› åœ¨äºè®¡åˆ’(Planning)èƒ½åŠ›çš„ç¼ºå¤±ã€‚ç ”ç©¶è€…åˆ©ç”¨åˆæˆçš„è§£é¢˜æ ‘å¯¼èˆªä»»åŠ¡(Solution-tree Navigation Task)éªŒè¯äº†è¿™äº›å‘ç°ï¼Œå¹¶æ˜ç¡®äº†RLæå‡æ‰§è¡Œç¨³å¥æ€§çš„å±€é™æ€§ã€‚æœ€åï¼Œè¯¥ç ”ç©¶ç¡®å®šäº†é€šè¿‡å¢å¼ºæ¢ç´¢ä¸æ³›åŒ–èƒ½åŠ›æ¥å…‹æœâ€œè¦†ç›–å¢™â€çš„æ½œåœ¨æ¡ä»¶ï¼Œä¸ºä¼˜åŒ–LLMæ¨ç†èƒ½åŠ›çš„RLè·¯å¾„æä¾›äº†é‡è¦æŒ‡å¼•ã€‚",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.22756v1",
      "published_date": "2025-05-28 18:18:49 UTC",
      "updated_date": "2025-05-28 18:18:49 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T15:13:22.274788+00:00"
    },
    {
      "arxiv_id": "2505.22753v1",
      "title": "Enhancing Lifelong Multi-Agent Path-finding by Using Artificial Potential Fields",
      "title_zh": "åˆ©ç”¨äººå·¥åŠ¿åœºæ³•å¢å¼ºç»ˆèº«å¤šæ™ºèƒ½ä½“è·¯å¾„è§„åˆ’",
      "authors": [
        "Arseniy Pertzovsky",
        "Roni Stern",
        "Ariel Felner",
        "Roie Zivan"
      ],
      "abstract": "We explore the use of Artificial Potential Fields (APFs) to solve Multi-Agent Path Finding (MAPF) and Lifelong MAPF (LMAPF) problems. In MAPF, a team of agents must move to their goal locations without collisions, whereas in LMAPF, new goals are generated upon arrival. We propose methods for incorporating APFs in a range of MAPF algorithms, including Prioritized Planning, MAPF-LNS2, and Priority Inheritance with Backtracking (PIBT). Experimental results show that using APF is not beneficial for MAPF but yields up to a 7-fold increase in overall system throughput for LMAPF.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†åˆ©ç”¨äººå·¥åŠ¿åœºæ³•(Artificial Potential Fields, APFs)æ¥è§£å†³å¤šæ™ºèƒ½ä½“è·¯å¾„è§„åˆ’(Multi-Agent Path Finding, MAPF)ä»¥åŠç»ˆèº«å¤šæ™ºèƒ½ä½“è·¯å¾„è§„åˆ’(Lifelong MAPF, LMAPF)é—®é¢˜ã€‚ä½œè€…æå‡ºäº†ä¸€ç³»åˆ—å°†APFsèå…¥ä¸»æµç®—æ³•çš„æ–¹æ³•ï¼ŒåŒ…æ‹¬ä¼˜å…ˆçº§è§„åˆ’(Prioritized Planning)ã€MAPF-LNS2ä»¥åŠå¸¦æœ‰å›æº¯çš„ä¼˜å…ˆçº§ç»§æ‰¿(Priority Inheritance with Backtracking, PIBT)ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè™½ç„¶ä½¿ç”¨APFsåœ¨ä¼ ç»Ÿçš„MAPFä»»åŠ¡ä¸­å¹¶æœªå¸¦æ¥æ˜¾è‘—æ”¶ç›Šï¼Œä½†åœ¨å¤„ç†æ–°ç›®æ ‡ä¸æ–­äº§ç”Ÿçš„LMAPFä»»åŠ¡æ—¶ï¼Œèƒ½å¤Ÿå°†ç³»ç»Ÿçš„æ•´ä½“ååé‡æå‡é«˜è¾¾7å€ã€‚è¿™ä¸€å‘ç°è¯æ˜äº†APFsåœ¨å¢å¼ºå¤æ‚ã€åŠ¨æ€ç¯å¢ƒä¸‹çš„å¤šæ™ºèƒ½ä½“æŒç»­è°ƒåº¦æ•ˆç‡æ–¹é¢å…·æœ‰å·¨å¤§æ½œåŠ›ã€‚",
      "categories": [
        "cs.AI",
        "cs.MA",
        "cs.RO"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.22753v1",
      "published_date": "2025-05-28 18:13:10 UTC",
      "updated_date": "2025-05-28 18:13:10 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T15:13:39.627438+00:00"
    },
    {
      "arxiv_id": "2505.22749v1",
      "title": "Self-orthogonalizing attractor neural networks emerging from the free energy principle",
      "title_zh": "åŸºäºè‡ªç”±èƒ½åŸç†çš„è‡ªæ­£äº¤åŒ–å¸å¼•å­ç¥ç»ç½‘ç»œ",
      "authors": [
        "Tamas Spisak",
        "Karl Friston"
      ],
      "abstract": "Attractor dynamics are a hallmark of many complex systems, including the brain. Understanding how such self-organizing dynamics emerge from first principles is crucial for advancing our understanding of neuronal computations and the design of artificial intelligence systems. Here we formalize how attractor networks emerge from the free energy principle applied to a universal partitioning of random dynamical systems. Our approach obviates the need for explicitly imposed learning and inference rules and identifies emergent, but efficient and biologically plausible inference and learning dynamics for such self-organizing systems. These result in a collective, multi-level Bayesian active inference process. Attractors on the free energy landscape encode prior beliefs; inference integrates sensory data into posterior beliefs; and learning fine-tunes couplings to minimize long-term surprise. Analytically and via simulations, we establish that the proposed networks favor approximately orthogonalized attractor representations, a consequence of simultaneously optimizing predictive accuracy and model complexity. These attractors efficiently span the input subspace, enhancing generalization and the mutual information between hidden causes and observable effects. Furthermore, while random data presentation leads to symmetric and sparse couplings, sequential data fosters asymmetric couplings and non-equilibrium steady-state dynamics, offering a natural extension to conventional Boltzmann Machines. Our findings offer a unifying theory of self-organizing attractor networks, providing novel insights for AI and neuroscience.",
      "tldr_zh": "è¯¥ç ”ç©¶åŸºäºè‡ªç”±èƒ½åŸç†(Free Energy Principle)é˜æ˜äº†å¸å¼•å­ç¥ç»ç½‘ç»œ(Attractor Neural Networks)å¦‚ä½•ä»éšæœºåŠ¨åŠ›ç³»ç»Ÿçš„é€šç”¨åˆ’åˆ†ä¸­è‡ªå‘äº§ç”Ÿã€‚è¯¥æ¡†æ¶æ— éœ€æ˜¾å¼å®šä¹‰å­¦ä¹ å’Œæ¨ç†è§„åˆ™ï¼Œè€Œæ˜¯é€šè¿‡å¤šå±‚çº§è´å¶æ–¯ä¸»åŠ¨æ¨ç†(Bayesian Active Inference)è¿‡ç¨‹ï¼Œå°†å¸å¼•å­ç¼–ç ä¸ºå…ˆéªŒä¿¡å¿µï¼Œå¹¶åˆ©ç”¨æ„Ÿè§‰æ•°æ®æ›´æ–°ä¸ºåéªŒä¿¡å¿µã€‚ç ”ç©¶é€šè¿‡è§£æä¸æ¨¡æ‹Ÿè¯æ˜ï¼Œä¸ºäº†å¹³è¡¡é¢„æµ‹å‡†ç¡®æ€§ä¸æ¨¡å‹å¤æ‚åº¦ï¼Œè¯¥ç½‘ç»œå€¾å‘äºå½¢æˆè¿‘ä¼¼æ­£äº¤åŒ–(Orthogonalized)çš„å¸å¼•å­è¡¨å¾ï¼Œä»è€Œæ˜¾è‘—æå‡æ³›åŒ–èƒ½åŠ›åŠéšè—åŸå› ä¸è§‚æµ‹æ•ˆåº”é—´çš„äº’ä¿¡æ¯ã€‚æ­¤å¤–ï¼Œå®éªŒå‘ç°åºåˆ—æ•°æ®ä¼šå¯¼è‡´éå¯¹ç§°è€¦åˆå’Œéå¹³è¡¡ç¨³æ€åŠ¨åŠ›å­¦çš„å‡ºç°ï¼Œè¿™ä¸ºä¼ ç»Ÿçš„ç»å°”å…¹æ›¼æœº(Boltzmann Machines)æä¾›äº†è‡ªç„¶çš„ç†è®ºæ‰©å±•ã€‚è¯¥å‘ç°ä¸ºç†è§£è‡ªç»„ç»‡å¸å¼•å­ç½‘ç»œæä¾›äº†ç»Ÿä¸€ç†è®ºï¼Œå¹¶ä¸ºäººå·¥æ™ºèƒ½ä¸ç¥ç»ç§‘å­¦é¢†åŸŸæä¾›äº†æ–°çš„ç ”ç©¶è§†è§’ã€‚",
      "categories": [
        "q-bio.NC",
        "cs.AI",
        "cs.LG",
        "cs.NE"
      ],
      "primary_category": "q-bio.NC",
      "comment": "22 pages main text, 5 pages appendix, 6 figures; interactive manuscript available at: https://pni-lab.github.io/fep-attractor-network Associated GitHub repository: https://github.com/pni-lab/fep-attractor-network",
      "pdf_url": "https://arxiv.org/pdf/2505.22749v1",
      "published_date": "2025-05-28 18:10:03 UTC",
      "updated_date": "2025-05-28 18:10:03 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T15:13:57.537688+00:00"
    },
    {
      "arxiv_id": "2505.22660v4",
      "title": "Maximizing Confidence Alone Improves Reasoning",
      "title_zh": "ä»…é€šè¿‡æœ€å¤§åŒ–ç½®ä¿¡åº¦å³å¯æå‡æ¨ç†èƒ½åŠ›",
      "authors": [
        "Mihir Prabhudesai",
        "Lili Chen",
        "Alex Ippoliti",
        "Katerina Fragkiadaki",
        "Hao Liu",
        "Deepak Pathak"
      ],
      "abstract": "Reinforcement learning (RL) has enabled machine learning models to achieve significant advances in many fields. Most recently, RL has empowered frontier language models to solve challenging math, science, and coding problems. However, central to any RL algorithm is the reward function, and reward engineering is a notoriously difficult problem in any domain. In this paper, we propose RENT: Reinforcement Learning via Entropy Minimization -- a fully unsupervised RL method that requires no external reward or ground-truth answers, and instead uses the model's entropy of its underlying distribution as an intrinsic reward. We find that by reinforcing the chains of thought that yield high model confidence on its generated answers, the model improves its reasoning ability. In our experiments, we showcase these improvements on an extensive suite of commonly-used reasoning benchmarks, including GSM8K, MATH500, AMC, AIME, and GPQA, and models of varying sizes from the Qwen, Mistral, and Llama families. The generality of our unsupervised learning method lends itself to applicability in a wide range of domains where external supervision is unavailable.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†å¼ºåŒ–å­¦ä¹ (Reinforcement learning, RL)åœ¨æå‡å¤§è¯­è¨€æ¨¡å‹æ¨ç†èƒ½åŠ›ä¸­çš„åº”ç”¨ï¼Œå¹¶é’ˆå¯¹å¥–åŠ±å‡½æ•°å·¥ç¨‹(reward engineering)éš¾ä»¥è®¾è®¡çš„ç—›ç‚¹æå‡ºäº†RENTã€‚RENTæ˜¯ä¸€ç§åŸºäºç†µæœ€å°åŒ–(Entropy Minimization)çš„å®Œå…¨æ— ç›‘ç£å¼ºåŒ–å­¦ä¹ æ–¹æ³•ï¼Œå®ƒæ— éœ€å¤–éƒ¨å¥–åŠ±æˆ–æ ‡å‡†ç­”æ¡ˆï¼Œè€Œæ˜¯å°†æ¨¡å‹åº•å±‚åˆ†å¸ƒçš„ç†µä½œä¸ºå†…åœ¨å¥–åŠ±ã€‚è¯¥æ–¹æ³•é€šè¿‡å¼ºåŒ–é‚£äº›èƒ½è®©æ¨¡å‹å¯¹ç”Ÿæˆç­”æ¡ˆäº§ç”Ÿé«˜ç½®ä¿¡åº¦(high model confidence)çš„æ€ç»´é“¾(chains of thought)ï¼Œå®ç°äº†å¯¹æ¨¡å‹æ¨ç†èƒ½åŠ›çš„æœ‰æ•ˆæå‡ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œåœ¨GSM8Kã€MATH500ã€AMCã€AIMEå’ŒGPQAç­‰å¤šä¸ªä¸»æµæ¨ç†åŸºå‡†æµ‹è¯•ä¸­ï¼ŒåŸºäºQwenã€Mistralå’ŒLlamaç³»åˆ—çš„å¤šç§è§„æ¨¡æ¨¡å‹å‡è¡¨ç°å‡ºäº†æ˜¾è‘—çš„æ€§èƒ½å¢å¼ºã€‚è¿™é¡¹ç ”ç©¶è¯æ˜äº†ä»…é€šè¿‡æœ€å¤§åŒ–æ¨¡å‹è‡ªèº«çš„ç½®ä¿¡åº¦å³å¯ä¼˜åŒ–æ¨ç†æ€§èƒ½ï¼Œä¸ºç¼ºä¹å¤–éƒ¨ç›‘ç£ä¿¡å·çš„å¤æ‚é¢†åŸŸæä¾›äº†ä¸€ç§å…·å¤‡å¹¿æ³›é€‚ç”¨æ€§çš„å­¦ä¹ èŒƒå¼ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "Website: https://rent-rl.github.io/",
      "pdf_url": "https://arxiv.org/pdf/2505.22660v4",
      "published_date": "2025-05-28 17:59:37 UTC",
      "updated_date": "2025-06-27 17:25:28 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T15:14:17.722661+00:00"
    },
    {
      "arxiv_id": "2505.22657v2",
      "title": "3DLLM-Mem: Long-Term Spatial-Temporal Memory for Embodied 3D Large Language Model",
      "title_zh": "3DLLM-Memï¼šé¢å‘å…·èº« 3D å¤§è¯­è¨€æ¨¡å‹çš„é•¿æœŸæ—¶ç©ºè®°å¿†",
      "authors": [
        "Wenbo Hu",
        "Yining Hong",
        "Yanjun Wang",
        "Leison Gao",
        "Zibu Wei",
        "Xingcheng Yao",
        "Nanyun Peng",
        "Yonatan Bitton",
        "Idan Szpektor",
        "Kai-Wei Chang"
      ],
      "abstract": "Humans excel at performing complex tasks by leveraging long-term memory across temporal and spatial experiences. In contrast, current Large Language Models (LLMs) struggle to effectively plan and act in dynamic, multi-room 3D environments. We posit that part of this limitation is due to the lack of proper 3D spatial-temporal memory modeling in LLMs. To address this, we first introduce 3DMem-Bench, a comprehensive benchmark comprising over 26,000 trajectories and 2,892 embodied tasks, question-answering and captioning, designed to evaluate an agent's ability to reason over long-term memory in 3D environments. Second, we propose 3DLLM-Mem, a novel dynamic memory management and fusion model for embodied spatial-temporal reasoning and actions in LLMs. Our model uses working memory tokens, which represents current observations, as queries to selectively attend to and fuse the most useful spatial and temporal features from episodic memory, which stores past observations and interactions. Our approach allows the agent to focus on task-relevant information while maintaining memory efficiency in complex, long-horizon environments. Experimental results demonstrate that 3DLLM-Mem achieves state-of-the-art performance across various tasks, outperforming the strongest baselines by 16.5% in success rate on 3DMem-Bench's most challenging in-the-wild embodied tasks.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ç°æœ‰å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨åŠ¨æ€ã€å¤šæˆ¿é—´3Dç¯å¢ƒä¸­ç¼ºä¹æœ‰æ•ˆé•¿æ—¶ç©ºè®°å¿†å»ºæ¨¡çš„é—®é¢˜ï¼Œæå‡ºäº†3DLLM-Memæ¡†æ¶ã€‚ç ”ç©¶é¦–å…ˆå¼•å…¥äº†3DMem-Benchï¼Œè¿™æ˜¯ä¸€ä¸ªåŒ…å«è¶…è¿‡26,000æ¡è½¨è¿¹å’Œ2,892é¡¹ä»»åŠ¡çš„ç»¼åˆåŸºå‡†ï¼Œæ—¨åœ¨è¯„ä¼°æ™ºèƒ½ä½“åœ¨3Dç¯å¢ƒä¸­çš„é•¿æ—¶è®°å¿†æ¨ç†èƒ½åŠ›ã€‚æ ¸å¿ƒè´¡çŒ®3DLLM-Memæ¨¡å‹é‡‡ç”¨åŠ¨æ€è®°å¿†ç®¡ç†ä¸èåˆæœºåˆ¶ï¼Œåˆ©ç”¨ä»£è¡¨å½“å‰è§‚å¯Ÿçš„working memory tokensä½œä¸ºæŸ¥è¯¢ï¼Œä»å­˜å‚¨è¿‡å»ç»éªŒçš„episodic memoryä¸­é€‰æ‹©æ€§åœ°æå–å¹¶èåˆå…³é”®çš„æ—¶ç©ºç‰¹å¾ã€‚è¿™ç§è®¾è®¡ä½¿æ™ºèƒ½ä½“èƒ½å¤Ÿåœ¨å¤æ‚é•¿ç¨‹ç¯å¢ƒä¸­èšç„¦ä»»åŠ¡ç›¸å…³ä¿¡æ¯ï¼ŒåŒæ—¶ä¿æŒæé«˜çš„è®°å¿†å¤„ç†æ•ˆç‡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œ3DLLM-Memåœ¨å¤šé¡¹ä»»åŠ¡ä¸­å–å¾—äº†SOTAæ€§èƒ½ï¼Œåœ¨3DMem-Benchæœ€å…·æŒ‘æˆ˜æ€§çš„çœŸå®å…·èº«ä»»åŠ¡ä¸­ï¼ŒæˆåŠŸç‡æ¯”æœ€å¼ºåŸºçº¿æ¨¡å‹æå‡äº†16.5%ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CL",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "demos at: https://3dllm-mem.github.io",
      "pdf_url": "https://arxiv.org/pdf/2505.22657v2",
      "published_date": "2025-05-28 17:59:13 UTC",
      "updated_date": "2025-12-17 05:47:37 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T15:14:06.502662+00:00"
    },
    {
      "arxiv_id": "2505.22655v1",
      "title": "Position: Uncertainty Quantification Needs Reassessment for Large-language Model Agents",
      "title_zh": "è§‚ç‚¹ï¼šå¤§è¯­è¨€æ¨¡å‹æ™ºèƒ½ä½“çš„ä¸ç¡®å®šæ€§é‡åŒ–äºŸéœ€é‡æ–°è¯„ä¼°",
      "authors": [
        "Michael Kirchhof",
        "Gjergji Kasneci",
        "Enkelejda Kasneci"
      ],
      "abstract": "Large-language models (LLMs) and chatbot agents are known to provide wrong outputs at times, and it was recently found that this can never be fully prevented. Hence, uncertainty quantification plays a crucial role, aiming to quantify the level of ambiguity in either one overall number or two numbers for aleatoric and epistemic uncertainty. This position paper argues that this traditional dichotomy of uncertainties is too limited for the open and interactive setup that LLM agents operate in when communicating with a user, and that we need to research avenues that enrich uncertainties in this novel scenario. We review the literature and find that popular definitions of aleatoric and epistemic uncertainties directly contradict each other and lose their meaning in interactive LLM agent settings. Hence, we propose three novel research directions that focus on uncertainties in such human-computer interactions: Underspecification uncertainties, for when users do not provide all information or define the exact task at the first go, interactive learning, to ask follow-up questions and reduce the uncertainty about the current context, and output uncertainties, to utilize the rich language and speech space to express uncertainties as more than mere numbers. We expect that these new ways of dealing with and communicating uncertainties will lead to LLM agent interactions that are more transparent, trustworthy, and intuitive.",
      "tldr_zh": "è¿™ç¯‡ç«‹åœºè®ºæ–‡æŒ‡å‡ºï¼Œä¼ ç»Ÿçš„ALEATORICå’ŒEPISTEMICä¸ç¡®å®šæ€§äºŒåˆ†æ³•åœ¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ™ºèƒ½ä½“ä¸ç”¨æˆ·çš„äº¤äº’åœºæ™¯ä¸­æ˜¾å¾—è¿‡äºå±€é™ã€‚ç ”ç©¶é€šè¿‡æ–‡çŒ®ç»¼è¿°å‘ç°ï¼Œç°æœ‰çš„ä¸ç¡®å®šæ€§å®šä¹‰åœ¨äº¤äº’å¼æ™ºèƒ½ä½“è®¾ç½®ä¸­ç»å¸¸å‡ºç°è‡ªç›¸çŸ›ç›¾çš„æƒ…å†µï¼Œå¯¼è‡´å…¶å¤±å»äº†åŸæœ‰çš„æŒ‡å¯¼æ„ä¹‰ã€‚ä¸ºæ­¤ï¼Œä½œè€…æå‡ºäº†ä¸‰ä¸ªé’ˆå¯¹äººæœºäº¤äº’çš„æ–°ç ”ç©¶æ–¹å‘ï¼šé’ˆå¯¹ç”¨æˆ·è¾“å…¥ä¿¡æ¯ä¸å…¨çš„UNDERSPECIFICATION UNCERTAINTIESï¼Œé€šè¿‡è¿½é—®å‡å°‘ä¸Šä¸‹æ–‡æ¨¡ç³Šæ€§çš„INTERACTIVE LEARNINGï¼Œä»¥åŠåˆ©ç”¨è¯­è¨€å’Œè¯­éŸ³ç©ºé—´è¡¨è¾¾éæ•°å€¼å‹ä¸ç¡®å®šæ€§çš„OUTPUT UNCERTAINTIESã€‚è¯¥ç ”ç©¶è®¤ä¸ºï¼Œé‡æ–°è¯„ä¼°UNCERTAINTY QUANTIFICATIONå¹¶æ¢ç´¢æ›´ä¸°å¯Œçš„ä¸ç¡®å®šæ€§äº¤æµæ–¹å¼ï¼Œå°†æ˜¾è‘—æå‡LLMæ™ºèƒ½ä½“åœ¨å®é™…åº”ç”¨ä¸­çš„é€æ˜åº¦ã€å¯ä¿¡åº¦å’Œç›´è§‚æ€§ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.LG",
      "comment": "Accepted at ICML 2025",
      "pdf_url": "https://arxiv.org/pdf/2505.22655v1",
      "published_date": "2025-05-28 17:59:08 UTC",
      "updated_date": "2025-05-28 17:59:08 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T15:14:34.313330+00:00"
    },
    {
      "arxiv_id": "2505.22704v1",
      "title": "Training Language Models to Generate Quality Code with Program Analysis Feedback",
      "title_zh": "åˆ©ç”¨ç¨‹åºåˆ†æåé¦ˆè®­ç»ƒè¯­è¨€æ¨¡å‹ç”Ÿæˆé«˜è´¨é‡ä»£ç ",
      "authors": [
        "Feng Yao",
        "Zilong Wang",
        "Liyuan Liu",
        "Junxia Cui",
        "Li Zhong",
        "Xiaohan Fu",
        "Haohui Mai",
        "Vish Krishnan",
        "Jianfeng Gao",
        "Jingbo Shang"
      ],
      "abstract": "Code generation with large language models (LLMs), often termed vibe coding, is increasingly adopted in production but fails to ensure code quality, particularly in security (e.g., SQL injection vulnerabilities) and maintainability (e.g., missing type annotations). Existing methods, such as supervised fine-tuning and rule-based post-processing, rely on labor-intensive annotations or brittle heuristics, limiting their scalability and effectiveness. We propose REAL, a reinforcement learning framework that incentivizes LLMs to generate production-quality code using program analysis-guided feedback. Specifically, REAL integrates two automated signals: (1) program analysis detecting security or maintainability defects and (2) unit tests ensuring functional correctness. Unlike prior work, our framework is prompt-agnostic and reference-free, enabling scalable supervision without manual intervention. Experiments across multiple datasets and model scales demonstrate that REAL outperforms state-of-the-art methods in simultaneous assessments of functionality and code quality. Our work bridges the gap between rapid prototyping and production-ready code, enabling LLMs to deliver both speed and quality.",
      "tldr_zh": "å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ç”Ÿæˆçš„ä»£ç åœ¨å®‰å…¨æ€§å’Œå¯ç»´æŠ¤æ€§æ–¹é¢å¸¸å­˜åœ¨ç¼ºé™·ï¼Œéš¾ä»¥æ»¡è¶³ç”Ÿäº§ç¯å¢ƒçš„è¦æ±‚ã€‚è¯¥ç ”ç©¶æå‡ºäº† REALï¼Œä¸€ä¸ªåˆ©ç”¨ç¨‹åºåˆ†æåé¦ˆï¼ˆprogram analysis-guided feedbackï¼‰æ¿€åŠ± LLMs ç”Ÿæˆç”Ÿäº§çº§è´¨é‡ä»£ç çš„å¼ºåŒ–å­¦ä¹ ï¼ˆreinforcement learningï¼‰æ¡†æ¶ã€‚è¯¥æ¡†æ¶é›†æˆäº†ç”¨äºæ£€æµ‹å®‰å…¨ä¸å¯ç»´æŠ¤æ€§ç¼ºé™·çš„ç¨‹åºåˆ†æä»¥åŠç¡®ä¿åŠŸèƒ½æ­£ç¡®æ€§çš„å•å…ƒæµ‹è¯•ï¼ˆunit testsï¼‰ä¸¤ç§è‡ªåŠ¨åŒ–ä¿¡å·ã€‚ä¸ä»¥å¾€å·¥ä½œä¸åŒï¼ŒREAL å…·æœ‰æç¤ºè¯­æ— å…³ï¼ˆprompt-agnosticï¼‰å’Œæ— éœ€å‚è€ƒï¼ˆreference-freeï¼‰çš„ç‰¹æ€§ï¼Œèƒ½å¤Ÿå®ç°æ— éœ€äººå·¥å¹²é¢„çš„å¤§è§„æ¨¡ç›‘ç£ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒREAL åœ¨åŠŸèƒ½æ€§å’Œä»£ç è´¨é‡çš„ç»¼åˆè¯„ä¼°ä¸­å‡ä¼˜äºç°æœ‰çš„æœ€å…ˆè¿›ï¼ˆstate-of-the-artï¼‰æ–¹æ³•ã€‚è¯¥ç ”ç©¶æœ‰æ•ˆå¼¥è¡¥äº†å¿«é€ŸåŸå‹å¼€å‘ä¸ç”Ÿäº§å°±ç»ªä»£ç ä¹‹é—´çš„å·®è·ï¼Œä½¿ LLMs èƒ½å¤Ÿå…¼é¡¾ç”Ÿæˆé€Ÿåº¦ä¸ä»£ç è´¨é‡ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "10 pages, 3 figures",
      "pdf_url": "https://arxiv.org/pdf/2505.22704v1",
      "published_date": "2025-05-28 17:57:47 UTC",
      "updated_date": "2025-05-28 17:57:47 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T15:15:01.003663+00:00"
    },
    {
      "arxiv_id": "2505.22649v2",
      "title": "Pre-training for Recommendation Unlearning",
      "title_zh": "é¢å‘æ¨èåå­¦ä¹ çš„é¢„è®­ç»ƒ",
      "authors": [
        "Guoxuan Chen",
        "Lianghao Xia",
        "Chao Huang"
      ],
      "abstract": "Modern recommender systems powered by Graph Neural Networks (GNNs) excel at modeling complex user-item interactions, yet increasingly face scenarios requiring selective forgetting of training data. Beyond user requests to remove specific interactions due to privacy concerns or preference changes, regulatory frameworks mandate recommender systems' ability to eliminate the influence of certain user data from models. This recommendation unlearning challenge presents unique difficulties as removing connections within interaction graphs creates ripple effects throughout the model, potentially impacting recommendations for numerous users. Traditional approaches suffer from significant drawbacks: fragmentation methods damage graph structure and diminish performance, while influence function techniques make assumptions that may not hold in complex GNNs, particularly with self-supervised or random architectures. To address these limitations, we propose a novel model-agnostic pre-training paradigm UnlearnRec that prepares systems for efficient unlearning operations. Our Influence Encoder takes unlearning requests together with existing model parameters and directly produces updated parameters of unlearned model with little fine-tuning, avoiding complete retraining while preserving model performance characteristics. Extensive evaluation on public benchmarks demonstrates that our method delivers exceptional unlearning effectiveness while providing more than 10x speedup compared to retraining approaches. We release our method implementation at: https://github.com/HKUDS/UnlearnRec.",
      "tldr_zh": "æœ¬ç ”ç©¶é’ˆå¯¹ç”±å›¾ç¥ç»ç½‘ç»œ(Graph Neural Networks, GNNs)é©±åŠ¨çš„æ¨èç³»ç»Ÿåœ¨é¢ä¸´éšç§ä¿æŠ¤æˆ–ç›‘ç®¡è¦æ±‚æ—¶ï¼Œå¦‚ä½•å®ç°é€‰æ‹©æ€§é—å¿˜(Unlearning)è¿™ä¸€éš¾é¢˜ã€‚é’ˆå¯¹ä¼ ç»Ÿæ–¹æ³•åœ¨å¤„ç†äº¤äº’å›¾è¿æ¥ç§»é™¤æ—¶å¯èƒ½é€ æˆçš„æ¨¡å‹æ€§èƒ½å—æŸæˆ–å‡è®¾å¤±æ•ˆç­‰å±€é™ï¼Œä½œè€…æå‡ºäº†åä¸ºUnlearnRecçš„æ¨¡å‹æ— å…³é¢„è®­ç»ƒèŒƒå¼ã€‚è¯¥æ–¹æ¡ˆçš„æ ¸å¿ƒåœ¨äºå¼•å…¥ä¸€ä¸ªå½±å“ç¼–ç å™¨(Influence Encoder)ï¼Œèƒ½å¤Ÿæ¥æ”¶é—å¿˜è¯·æ±‚åŠç°æœ‰æ¨¡å‹å‚æ•°å¹¶ç›´æ¥è¾“å‡ºæ›´æ–°åçš„æ¨¡å‹å‚æ•°ï¼Œä»…éœ€æå°‘é‡å¾®è°ƒ(Fine-tuning)å³å¯é¿å…å®Œå…¨é‡æ–°è®­ç»ƒã€‚åœ¨å…¬å¼€åŸºå‡†æ•°æ®é›†ä¸Šçš„å¹¿æ³›è¯„ä¼°è¯æ˜ï¼ŒUnlearnRecåœ¨ä¿æŒæ¨¡å‹æ¨èæ€§èƒ½çš„åŒæ—¶ï¼Œå®ç°äº†å‡ºè‰²çš„é—å¿˜æ•ˆæœã€‚ç›¸æ¯”ä¼ ç»Ÿçš„é‡æ–°è®­ç»ƒæ–¹æ³•ï¼Œè¯¥æ¡†æ¶åœ¨æ•ˆç‡ä¸Šæå‡äº†10å€ä»¥ä¸Šï¼Œä¸ºé«˜æ•ˆã€å¯æ‰©å±•çš„æ¨èç³»ç»Ÿé—å¿˜æœºåˆ¶æä¾›äº†æ–°çš„è§£å†³è·¯å¾„ã€‚",
      "categories": [
        "cs.IR",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.IR",
      "comment": "Accepted to SIGIR 2025 Oral",
      "pdf_url": "https://arxiv.org/pdf/2505.22649v2",
      "published_date": "2025-05-28 17:57:11 UTC",
      "updated_date": "2025-05-29 06:59:48 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T15:14:37.503502+00:00"
    },
    {
      "arxiv_id": "2505.22642v3",
      "title": "FastTD3: Simple, Fast, and Capable Reinforcement Learning for Humanoid Control",
      "title_zh": "FastTD3ï¼šç”¨äºç±»äººæœºå™¨äººæ§åˆ¶çš„ç®€æ´ã€é«˜æ•ˆä¸”å¼ºå¤§çš„å¼ºåŒ–å­¦ä¹ ",
      "authors": [
        "Younggyo Seo",
        "Carmelo Sferrazza",
        "Haoran Geng",
        "Michal Nauman",
        "Zhao-Heng Yin",
        "Pieter Abbeel"
      ],
      "abstract": "Reinforcement learning (RL) has driven significant progress in robotics, but its complexity and long training times remain major bottlenecks. In this report, we introduce FastTD3, a simple, fast, and capable RL algorithm that significantly speeds up training for humanoid robots in popular suites such as HumanoidBench, IsaacLab, and MuJoCo Playground. Our recipe is remarkably simple: we train an off-policy TD3 agent with several modifications -- parallel simulation, large-batch updates, a distributional critic, and carefully tuned hyperparameters. FastTD3 solves a range of HumanoidBench tasks in under 3 hours on a single A100 GPU, while remaining stable during training. We also provide a lightweight and easy-to-use implementation of FastTD3 to accelerate RL research in robotics.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† FastTD3ï¼Œä¸€ç§ç®€å•ã€å¿«é€Ÿä¸”èƒ½åŠ›å¼ºçš„äººå½¢æœºå™¨äººå¼ºåŒ–å­¦ä¹  (Reinforcement Learning) ç®—æ³•ï¼Œæ—¨åœ¨è§£å†³å½“å‰æœºå™¨äººæ§åˆ¶é¢†åŸŸè®­ç»ƒå¤æ‚å’Œè€—æ—¶é•¿ç­‰ç“¶é¢ˆé—®é¢˜ã€‚è¯¥æ–¹æ³•åœ¨ä¼ ç»Ÿçš„ç¦»ç­–ç•¥ TD3 ä»£ç†åŸºç¡€ä¸Šè¿›è¡Œäº†å…³é”®æ”¹è¿›ï¼Œé›†æˆäº†å¹¶è¡Œæ¨¡æ‹Ÿ (parallel simulation)ã€å¤§æ‰¹é‡æ›´æ–° (large-batch updates) å’Œåˆ†å¸ƒä¼°è®¡è¯„è®ºå®¶ (distributional critic)ï¼Œå¹¶å¯¹è¶…å‚æ•°è¿›è¡Œäº†ç²¾å¿ƒè°ƒä¼˜ã€‚FastTD3 åœ¨ HumanoidBenchã€IsaacLab å’Œ MuJoCo Playground ç­‰ä¸»æµäººå½¢æœºå™¨äººä»¿çœŸå¥—ä»¶ä¸­è¡¨ç°å‡ºè‰²ï¼Œæ˜¾è‘—æå‡äº†å­¦ä¹ æ•ˆç‡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥ç®—æ³•ä»…éœ€å•å— A100 GPU å³å¯åœ¨ 3 å°æ—¶å†…è§£å†³ä¸€ç³»åˆ— HumanoidBench ä»»åŠ¡ï¼Œä¸”åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ä¿æŒäº†é«˜åº¦çš„ç¨³å®šæ€§ã€‚æ­¤å¤–ï¼Œè¯¥ç ”ç©¶è¿˜æä¾›äº†ä¸€ä¸ªè½»é‡çº§ä¸”æ˜“äºä½¿ç”¨çš„ FastTD3 å®ç°ï¼Œä¸ºåŠ é€Ÿæœºå™¨äººå¼ºåŒ–å­¦ä¹ é¢†åŸŸçš„ç§‘ç ”æ¢ç´¢æä¾›äº†æœ‰åŠ›å·¥å…·ã€‚",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.RO",
      "comment": "Project webpage: https://younggyo.me/fast_td3",
      "pdf_url": "https://arxiv.org/pdf/2505.22642v3",
      "published_date": "2025-05-28 17:55:26 UTC",
      "updated_date": "2025-06-01 22:51:56 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T15:14:44.060246+00:00"
    },
    {
      "arxiv_id": "2505.22635v1",
      "title": "Learning Composable Chains-of-Thought",
      "title_zh": "å­¦ä¹ å¯ç»„åˆçš„æ€ç»´é“¾",
      "authors": [
        "Fangcong Yin",
        "Zeyu Leo Liu",
        "Liu Leqi",
        "Xi Ye",
        "Greg Durrett"
      ],
      "abstract": "A common approach for teaching large language models (LLMs) to reason is to train on chain-of-thought (CoT) traces of in-distribution reasoning problems, but such annotated data is costly to obtain for every problem of interest. We want reasoning models to generalize beyond their training distribution, and ideally to generalize compositionally: combine atomic reasoning skills to solve harder, unseen reasoning tasks. We take a step towards compositional generalization of reasoning skills when addressing a target compositional task that has no labeled CoT data. We find that simply training models on CoT data of atomic tasks leads to limited generalization, but minimally modifying CoT formats of constituent atomic tasks to be composable can lead to improvements. We can train \"atomic CoT\" models on the atomic tasks with Composable CoT data and combine them with multitask learning or model merging for better zero-shot performance on the target compositional task. Such a combined model can be further bootstrapped on a small amount of compositional data using rejection sampling fine-tuning (RFT). Results on string operations and natural language skill compositions show that training LLMs on Composable CoT outperforms multitask learning and continued fine-tuning baselines within a given training data budget.",
      "tldr_zh": "é’ˆå¯¹å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨å¤„ç†å¤æ‚æ¨ç†ä»»åŠ¡æ—¶é¢ä¸´çš„æ ‡æ³¨æ•°æ®æˆæœ¬é«˜åŠç»„åˆæ³›åŒ–ï¼ˆcompositional generalizationï¼‰èƒ½åŠ›ä¸è¶³çš„é—®é¢˜ï¼Œè¯¥ç ”ç©¶æ¢ç´¢äº†å¦‚ä½•åœ¨ç¼ºä¹æ ‡æ³¨æ•°æ®çš„ç›®æ ‡ç»„åˆä»»åŠ¡ä¸Šå®ç°æ¨ç†æŠ€èƒ½çš„æœ‰æ•ˆç»„åˆã€‚ç ”ç©¶å‘ç°ï¼Œä»…åœ¨åŸå­ä»»åŠ¡ï¼ˆatomic tasksï¼‰çš„Chain-of-Thought (CoT)æ•°æ®ä¸Šè¿›è¡Œè®­ç»ƒæ³›åŒ–æ•ˆæœæœ‰é™ï¼Œè€Œé€šè¿‡è°ƒæ•´CoTæ ¼å¼ä½¿å…¶å…·å¤‡å¯ç»„åˆæ€§ï¼ˆComposable CoTï¼‰ï¼Œå¯ä»¥æ˜¾è‘—å¢å¼ºæ¨¡å‹çš„æ¨ç†èƒ½åŠ›ã€‚è¯¥æ–¹æ³•é€šè¿‡è®­ç»ƒåŸºäºComposable CoTæ•°æ®çš„åŸå­æ¨¡å‹ï¼Œå¹¶åˆ©ç”¨å¤šä»»åŠ¡å­¦ä¹ ï¼ˆmultitask learningï¼‰æˆ–æ¨¡å‹åˆå¹¶ï¼ˆmodel mergingï¼‰æŠ€æœ¯ï¼Œæå‡äº†æ¨¡å‹åœ¨ç›®æ ‡ä»»åŠ¡ä¸Šçš„é›¶æ ·æœ¬ï¼ˆzero-shotï¼‰è¡¨ç°ã€‚æ­¤å¤–ï¼Œç ”ç©¶è¿˜å¼•å…¥äº†æ‹’ç»é‡‡æ ·å¾®è°ƒï¼ˆrejection sampling fine-tuning, RFTï¼‰åœ¨å°‘é‡æ•°æ®ä¸Šå¯¹æ¨¡å‹è¿›è¡Œè¿›ä¸€æ­¥å¼•å¯¼ä¼˜åŒ–ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œåœ¨å­—ç¬¦ä¸²æ“ä½œå’Œè‡ªç„¶è¯­è¨€æŠ€èƒ½ç»„åˆä»»åŠ¡ä¸­ï¼ŒComposable CoTçš„è®­ç»ƒæ•ˆæœä¼˜äºä¼ ç»Ÿçš„å¤šä»»åŠ¡å­¦ä¹ å’ŒæŒç»­å¾®è°ƒåŸºå‡†ï¼Œæœ‰æ•ˆè¯æ˜äº†è¯¥æ¡†æ¶åœ¨æå‡æ¨¡å‹ç»„åˆæ¨ç†æ•ˆç‡æ–¹é¢çš„ä¼˜åŠ¿ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.22635v1",
      "published_date": "2025-05-28 17:51:10 UTC",
      "updated_date": "2025-05-28 17:51:10 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T15:15:03.777770+00:00"
    },
    {
      "arxiv_id": "2505.22633v3",
      "title": "Spatial Knowledge Graph-Guided Multimodal Synthesis",
      "title_zh": "ç©ºé—´çŸ¥è¯†å›¾è°±å¼•å¯¼çš„å¤šæ¨¡æ€åˆæˆ",
      "authors": [
        "Yida Xue",
        "Zhen Bi",
        "Jinnan Yang",
        "Jungang Lou",
        "Kehai Chen",
        "Min Zhang",
        "Huajun Chen",
        "Ningyu Zhang"
      ],
      "abstract": "Recent advances in Multimodal Large Language Models (MLLMs) have significantly enhanced their capabilities; however, their spatial perception abilities remain a notable limitation. To address this challenge, multimodal data synthesis offers a promising solution. Yet, ensuring that synthesized data adhere to spatial common sense is a non-trivial task. Our approach addresses this critical gap by providing a systematic framework for generating spatially coherent data. In this work, we introduce SKG2DATA, a novel multimodal synthesis approach guided by spatial knowledge graphs, grounded in the concept of knowledge-to-data generation. SKG2DATA employs an automated pipeline for constructing Spatial Knowledge Graph (SKG) that effectively captures human-like spatial cognition, including directional and distance relationships. These structured representations then serve as precise guidance for our integrated synthesis pipeline, where a diffusion model generates spatially-consistent images while a MLLM produces corresponding textual descriptions. The automated construction of SKG enables scalable generation of diverse yet realistic spatial configurations, overcoming the limitations of manual data collection and annotation. Extensive experiments demonstrate that data synthesized from diverse types of spatial knowledge, including direction and distance, enhance the spatial perception and reasoning abilities of MLLMs markedly, albeit with a slight cost to their general capabilities. We hope that the idea of knowledge-based data synthesis can advance the development of spatial intelligence. Code is available at https://github.com/zjunlp/Knowledge2Data.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹(MLLMs)åœ¨ç©ºé—´æ„ŸçŸ¥èƒ½åŠ›ä¸Šçš„å±€é™æ€§ï¼Œæå‡ºäº†SKG2DATAï¼Œä¸€ç§ç”±ç©ºé—´çŸ¥è¯†å›¾è°±(Spatial Knowledge Graphs)å¼•å¯¼çš„æ–°å‹å¤šæ¨¡æ€åˆæˆæ–¹æ³•ã€‚è¯¥æ–¹æ³•éµå¾ªâ€œä»çŸ¥è¯†åˆ°æ•°æ®ç”Ÿæˆâ€(knowledge-to-data generation)çš„ç†å¿µï¼Œé€šè¿‡è‡ªåŠ¨åŒ–æµç¨‹æ„å»ºèƒ½å¤Ÿæ•æ‰äººç±»ç©ºé—´è®¤çŸ¥ï¼ˆåŒ…æ‹¬æ–¹ä½å’Œè·ç¦»å…³ç³»ï¼‰çš„SKGã€‚è¿™äº›ç»“æ„åŒ–è¡¨ç¤ºä¸ºé›†æˆåˆæˆæµæ°´çº¿æä¾›ç²¾ç¡®æŒ‡å¯¼ï¼Œåˆ©ç”¨æ‰©æ•£æ¨¡å‹(diffusion model)ç”Ÿæˆç©ºé—´ä¸€è‡´çš„å›¾åƒï¼Œå¹¶ç”±MLLMç”Ÿæˆç›¸åº”çš„æ–‡æœ¬æè¿°ã€‚SKG2DATAå®ç°äº†å¤šæ ·åŒ–ä¸”çœŸå®ç©ºé—´é…ç½®çš„å¯æ‰©å±•ç”Ÿæˆï¼Œæœ‰æ•ˆå…‹æœäº†äººå·¥æ•°æ®é‡‡é›†å’Œæ ‡æ³¨çš„å±€é™ã€‚å®éªŒè¯æ˜ï¼Œåˆ©ç”¨è¯¥æ–¹æ³•åˆæˆçš„æ•°æ®èƒ½æ˜¾è‘—å¢å¼ºMLLMsçš„ç©ºé—´æ„ŸçŸ¥ä¸æ¨ç†èƒ½åŠ›ï¼Œå°½ç®¡å¯¹é€šç”¨èƒ½åŠ›æœ‰è½»å¾®å½±å“ï¼Œè¯¥ç ”ç©¶ä¸ºé€šè¿‡çŸ¥è¯†å¼•å¯¼åˆæˆæ•°æ®æ¥å‘å±•ç©ºé—´æ™ºèƒ½æä¾›äº†é‡è¦å‚è€ƒã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.CV",
        "cs.LG",
        "cs.MM"
      ],
      "primary_category": "cs.CL",
      "comment": "IEEE/ACM Transactions on Audio, Speech and Language Processing",
      "pdf_url": "https://arxiv.org/pdf/2505.22633v3",
      "published_date": "2025-05-28 17:50:21 UTC",
      "updated_date": "2025-11-23 04:21:19 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T15:15:13.807581+00:00"
    },
    {
      "arxiv_id": "2505.22626v2",
      "title": "SCIZOR: A Self-Supervised Approach to Data Curation for Large-Scale Imitation Learning",
      "title_zh": "SCIZORï¼šé¢å‘å¤§è§„æ¨¡æ¨¡ä»¿å­¦ä¹ çš„è‡ªç›‘ç£æ•°æ®ç­›é€‰æ–¹æ³•",
      "authors": [
        "Yu Zhang",
        "Yuqi Xie",
        "Huihan Liu",
        "Rutav Shah",
        "Michael Wan",
        "Linxi Fan",
        "Yuke Zhu"
      ],
      "abstract": "Imitation learning advances robot capabilities by enabling the acquisition of diverse behaviors from human demonstrations. However, large-scale datasets used for policy training often introduce substantial variability in quality, which can negatively impact performance. As a result, automatically curating datasets by filtering low-quality samples to improve quality becomes essential. Existing robotic curation approaches rely on costly manual annotations and perform curation at a coarse granularity, such as the dataset or trajectory level, failing to account for the quality of individual state-action pairs. To address this, we introduce SCIZOR, a self-supervised data curation framework that filters out low-quality state-action pairs to improve the performance of imitation learning policies. SCIZOR targets two complementary sources of low-quality data: suboptimal data, which hinders learning with undesirable actions, and redundant data, which dilutes training with repetitive patterns. SCIZOR leverages a self-supervised task progress predictor for suboptimal data to remove samples lacking task progression, and a deduplication module operating on joint state-action representation for samples with redundant patterns. Empirically, we show that SCIZOR enables imitation learning policies to achieve higher performance with less data, yielding an average improvement of 15.4% across multiple benchmarks. More information is available at: https://ut-austin-rpl.github.io/SCIZOR/",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†SCIZORï¼Œä¸€ç§é’ˆå¯¹å¤§è§„æ¨¡æ¨¡ä»¿å­¦ä¹ (Imitation Learning)çš„è‡ªç›‘ç£æ•°æ®æ•´ç†æ¡†æ¶ï¼Œæ—¨åœ¨é€šè¿‡è¿‡æ»¤ä½è´¨é‡çš„çŠ¶æ€-åŠ¨ä½œå¯¹(state-action pairs)æ¥æå‡ç­–ç•¥æ€§èƒ½ã€‚SCIZORå…‹æœäº†ç°æœ‰æ–¹æ³•ä¾èµ–é«˜æˆæœ¬äººå·¥æ ‡æ³¨ä¸”è¿‡æ»¤ç²’åº¦è¾ƒç²—çš„å±€é™æ€§ï¼Œå®ç°äº†åœ¨ç»†ç²’åº¦å±‚é¢ä¸Šå¯¹æ•°æ®çš„ä¼˜åŒ–ã€‚è¯¥æ¡†æ¶ä¸»è¦é’ˆå¯¹æ¬¡ä¼˜æ•°æ®(suboptimal data)å’Œå†—ä½™æ•°æ®(redundant data)ä¸¤ä¸ªæ¥æºï¼Œåˆ©ç”¨è‡ªç›‘ç£çš„ä»»åŠ¡è¿›åº¦é¢„æµ‹å™¨(task progress predictor)å‰”é™¤ç¼ºä¹ä»»åŠ¡è¿›å±•çš„æ ·æœ¬ã€‚åŒæ—¶ï¼Œå®ƒé€šè¿‡æ“ä½œäºå…³èŠ‚çŠ¶æ€-åŠ¨ä½œè¡¨ç¤ºçš„å»é‡æ¨¡å—å¤„ç†é‡å¤æ¨¡å¼ï¼Œä»è€Œæ˜¾è‘—ç²¾ç®€äº†è®­ç»ƒé›†ã€‚å®éªŒè¯æ˜ï¼ŒSCIZORä½¿æ¨¡ä»¿å­¦ä¹ ç­–ç•¥åœ¨æ•°æ®é‡æ›´å°‘çš„æƒ…å†µä¸‹å®ç°äº†æ›´é«˜æ€§èƒ½ï¼Œåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­å¹³å‡æå‡äº†15.4%çš„æˆåŠŸç‡ã€‚è¿™ä¸€ç ”ç©¶ä¸ºæ„å»ºé«˜è´¨é‡ã€å¤§è§„æ¨¡æœºå™¨äººè¡Œä¸ºæ•°æ®é›†æä¾›äº†ä¸€ç§é«˜æ•ˆçš„è‡ªåŠ¨åŒ–è§£å†³æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.RO",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.22626v2",
      "published_date": "2025-05-28 17:45:05 UTC",
      "updated_date": "2025-09-09 06:38:19 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T15:15:19.549010+00:00"
    },
    {
      "arxiv_id": "2505.22617v1",
      "title": "The Entropy Mechanism of Reinforcement Learning for Reasoning Language Models",
      "title_zh": "æ¨ç†è¯­è¨€æ¨¡å‹å¼ºåŒ–å­¦ä¹ çš„ç†µæœºåˆ¶",
      "authors": [
        "Ganqu Cui",
        "Yuchen Zhang",
        "Jiacheng Chen",
        "Lifan Yuan",
        "Zhi Wang",
        "Yuxin Zuo",
        "Haozhan Li",
        "Yuchen Fan",
        "Huayu Chen",
        "Weize Chen",
        "Zhiyuan Liu",
        "Hao Peng",
        "Lei Bai",
        "Wanli Ouyang",
        "Yu Cheng",
        "Bowen Zhou",
        "Ning Ding"
      ],
      "abstract": "This paper aims to overcome a major obstacle in scaling RL for reasoning with LLMs, namely the collapse of policy entropy. Such phenomenon is consistently observed across vast RL runs without entropy intervention, where the policy entropy dropped sharply at the early training stage, this diminished exploratory ability is always accompanied with the saturation of policy performance. In practice, we establish a transformation equation R=-a*e^H+b between entropy H and downstream performance R. This empirical law strongly indicates that, the policy performance is traded from policy entropy, thus bottlenecked by its exhaustion, and the ceiling is fully predictable H=0, R=-a+b. Our finding necessitates entropy management for continuous exploration toward scaling compute for RL. To this end, we investigate entropy dynamics both theoretically and empirically. Our derivation highlights that, the change in policy entropy is driven by the covariance between action probability and the change in logits, which is proportional to its advantage when using Policy Gradient-like algorithms. Empirical study shows that, the values of covariance term and entropy differences matched exactly, supporting the theoretical conclusion. Moreover, the covariance term stays mostly positive throughout training, further explaining why policy entropy would decrease monotonically. Through understanding the mechanism behind entropy dynamics, we motivate to control entropy by restricting the update of high-covariance tokens. Specifically, we propose two simple yet effective techniques, namely Clip-Cov and KL-Cov, which clip and apply KL penalty to tokens with high covariances respectively. Experiments show that these methods encourage exploration, thus helping policy escape entropy collapse and achieve better downstream performance.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†æ¨ç†å¤§è¯­è¨€æ¨¡å‹ (Reasoning LLMs) åœ¨å¼ºåŒ–å­¦ä¹  (RL) è¿‡ç¨‹ä¸­é¢ä¸´çš„ç­–ç•¥ç†µ (Policy Entropy) å¡Œç¼©é—®é¢˜ï¼Œå³ç†µçš„å¿«é€Ÿä¸‹é™ä¼šå‰Šå¼±æ¨¡å‹çš„æ¢ç´¢èƒ½åŠ›å¹¶å¯¼è‡´æ€§èƒ½è¿‡æ—©é¥±å’Œã€‚ä½œè€…é€šè¿‡å»ºç«‹ç»éªŒå…¬å¼ $R = -ae^H + b$ æ­ç¤ºäº†æ€§èƒ½æå‡ä¸ç†µæ¶ˆè€—ä¹‹é—´çš„è½¬æ¢å…³ç³»ï¼ŒæŒ‡å‡ºæ€§èƒ½ä¸Šé™å—é™äºç†µçš„æ¯ç«­ï¼Œä»è€Œå¼ºè°ƒäº†ç†µç®¡ç†åœ¨æ‰©å±• RL è®¡ç®—ä¸­çš„å¿…è¦æ€§ã€‚é€šè¿‡ç†è®ºæ¨å¯¼ä¸å®è¯ç ”ç©¶ï¼Œè®ºæ–‡å‘ç°ç­–ç•¥ç†µçš„å˜åŒ–æ˜¯ç”±åŠ¨ä½œæ¦‚ç‡ä¸ Logits å˜åŒ–ä¹‹é—´çš„åæ–¹å·® (Covariance) é©±åŠ¨çš„ï¼Œè¿™è§£é‡Šäº†ç†µåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­å•è°ƒå‡å°‘çš„å†…åœ¨æœºåˆ¶ã€‚ä¸ºäº†ç¼“è§£è¿™ä¸€ç°è±¡ï¼Œç ”ç©¶æå‡ºäº† Clip-Cov å’Œ KL-Cov ä¸¤ç§æŠ€æœ¯ï¼Œé€šè¿‡é™åˆ¶é«˜åæ–¹å·® Token çš„æ›´æ–°æ¥ç²¾å‡†æ§åˆ¶ç†µåŠ¨åŠ›å­¦ã€‚å®éªŒç»“æœè¯æ˜ï¼Œè¿™äº›æ–¹æ³•èƒ½æœ‰æ•ˆä¿ƒè¿›æ¨¡å‹æ¢ç´¢ï¼Œå¸®åŠ©ç­–ç•¥æ‘†è„±ç†µå¡Œç¼©å¹¶æ˜¾è‘—æå‡ä¸‹æ¸¸ä»»åŠ¡çš„æœ€ç»ˆè¡¨ç°ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.22617v1",
      "published_date": "2025-05-28 17:38:45 UTC",
      "updated_date": "2025-05-28 17:38:45 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T15:15:13.912520+00:00"
    },
    {
      "arxiv_id": "2505.22613v1",
      "title": "RICO: Improving Accuracy and Completeness in Image Recaptioning via Visual Reconstruction",
      "title_zh": "RICOï¼šé€šè¿‡è§†è§‰é‡å»ºæå‡å›¾åƒé‡æè¿°çš„å‡†ç¡®æ€§ä¸å®Œæ•´æ€§",
      "authors": [
        "Yuchi Wang",
        "Yishuo Cai",
        "Shuhuai Ren",
        "Sihan Yang",
        "Linli Yao",
        "Yuanxin Liu",
        "Yuanxing Zhang",
        "Pengfei Wan",
        "Xu Sun"
      ],
      "abstract": "Image recaptioning is widely used to generate training datasets with enhanced quality for various multimodal tasks. Existing recaptioning methods typically rely on powerful multimodal large language models (MLLMs) to enhance textual descriptions, but often suffer from inaccuracies due to hallucinations and incompleteness caused by missing fine-grained details. To address these limitations, we propose RICO, a novel framework that refines captions through visual reconstruction. Specifically, we leverage a text-to-image model to reconstruct a caption into a reference image, and prompt an MLLM to identify discrepancies between the original and reconstructed images to refine the caption. This process is performed iteratively, further progressively promoting the generation of more faithful and comprehensive descriptions. To mitigate the additional computational cost induced by the iterative process, we introduce RICO-Flash, which learns to generate captions like RICO using DPO. Extensive experiments demonstrate that our approach significantly improves caption accuracy and completeness, outperforms most baselines by approximately 10% on both CapsBench and CompreCap. Code released at https://github.com/wangyuchi369/RICO.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å›¾åƒé‡æè¿° (Image Recaptioning) ä¸­å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ (MLLMs) å¸¸å› å¹»è§‰å¯¼è‡´å‡†ç¡®æ€§ä¸è¶³ï¼Œä»¥åŠå› ç»†ç²’åº¦ç»†èŠ‚ç¼ºå¤±å¯¼è‡´å®Œæ•´æ€§å·®çš„é—®é¢˜ï¼Œæå‡ºäº† RICO æ¡†æ¶ã€‚è¯¥æ¡†æ¶é€šè¿‡è§†è§‰é‡æ„ (Visual Reconstruction) æŠ€æœ¯ï¼Œåˆ©ç”¨æ–‡æœ¬ç”Ÿæˆå›¾åƒæ¨¡å‹å°†åˆå§‹æè¿°è¿˜åŸä¸ºå‚è€ƒå›¾åƒï¼Œå¹¶ä¿ƒä½¿ MLLM è¯†åˆ«åŸå§‹å›¾åƒä¸é‡æ„å›¾åƒä¹‹é—´çš„å·®å¼‚ä»¥ç²¾ç‚¼æ–‡æœ¬ã€‚é€šè¿‡è¿­ä»£æ‰§è¡Œè¿™ä¸€è¿‡ç¨‹ï¼ŒRICO èƒ½å¤Ÿé€æ­¥ç”Ÿæˆæ›´åŠ å¿ å®ä¸”å…¨é¢çš„å›¾åƒæè¿°ã€‚æ­¤å¤–ï¼Œç ”ç©¶è€…è¿˜å¼•å…¥äº† RICO-Flashï¼Œåˆ©ç”¨ç›´æ¥åå¥½ä¼˜åŒ– (DPO) å­¦ä¹  RICO çš„ç”Ÿæˆç­–ç•¥ï¼Œæœ‰æ•ˆé™ä½äº†è¿­ä»£è¿‡ç¨‹å¸¦æ¥çš„é¢å¤–è®¡ç®—å¼€é”€ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯¥æ–¹æ³•åœ¨ CapsBench å’Œ CompreCap åŸºå‡†æµ‹è¯•ä¸­å‡ä¼˜äºå¤šæ•°åŸºçº¿æ¨¡å‹çº¦ 10%ï¼Œæ˜¾è‘—æå‡äº†æè¿°çš„å‡†ç¡®æ€§å’Œå®Œæ•´æ€§ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.CV",
      "comment": "code: https://github.com/wangyuchi369/RICO",
      "pdf_url": "https://arxiv.org/pdf/2505.22613v1",
      "published_date": "2025-05-28 17:29:34 UTC",
      "updated_date": "2025-05-28 17:29:34 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T15:15:36.641794+00:00"
    },
    {
      "arxiv_id": "2505.22608v1",
      "title": "Effective and Efficient One-pass Compression of Speech Foundation Models Using Sparsity-aware Self-pinching Gates",
      "title_zh": "åŸºäºç¨€ç–æ„ŸçŸ¥è‡ªæ”¶ç¼©é—¨æ§çš„è¯­éŸ³åŸºç¡€æ¨¡å‹é«˜æ•ˆå•é˜¶æ®µå‹ç¼©",
      "authors": [
        "Haoning Xu",
        "Zhaoqing Li",
        "Youjun Chen",
        "Huimeng Wang",
        "Guinan Li",
        "Mengzhe Geng",
        "Chengxi Deng",
        "Xunying Liu"
      ],
      "abstract": "This paper presents a novel approach for speech foundation models compression that tightly integrates model pruning and parameter update into a single stage. Highly compact layer-level tied self-pinching gates each containing only a single learnable threshold are jointly trained with uncompressed models and used in fine-grained neuron level pruning. Experiments conducted on the LibriSpeech-100hr corpus suggest that our approach reduces the number of parameters of wav2vec2.0-base and HuBERT-large models by 65% and 60% respectively, while incurring no statistically significant word error rate (WER) increase on the test-clean dataset. Compared to previously published methods on the same task, our approach not only achieves the lowest WER of 7.05% on the test-clean dataset under a comparable model compression ratio of 4.26x, but also operates with at least 25% less model compression time.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§æ–°é¢–çš„è¯­éŸ³åŸºç¡€æ¨¡å‹(Speech Foundation Models)å‹ç¼©æ–¹æ³•ï¼Œé€šè¿‡å¼•å…¥ç¨€ç–æ„ŸçŸ¥è‡ªæ”¶ç¼©é—¨(Sparsity-aware Self-pinching Gates)å°†æ¨¡å‹å‰ªæ(Pruning)ä¸å‚æ•°æ›´æ–°é›†æˆåˆ°å•ä¸€é˜¶æ®µã€‚è¯¥æ¡†æ¶åˆ©ç”¨å±‚çº§å…³è”ä¸”ä»…å«å•ä¸€å¯å­¦ä¹ é˜ˆå€¼çš„è‡ªæ”¶ç¼©é—¨å®ç°ç»†ç²’åº¦çš„ç¥ç»å…ƒçº§å‰ªæï¼Œæ˜¾è‘—æå‡äº†å‹ç¼©æ•ˆç‡ã€‚åœ¨LibriSpeech-100hrè¯­æ–™åº“ä¸Šçš„å®éªŒè¯æ˜ï¼Œè¯¥æ–¹æ³•èƒ½å°†wav2vec2.0-baseå’ŒHuBERT-largeæ¨¡å‹çš„å‚æ•°é‡åˆ†åˆ«ç¼©å‡65%å’Œ60%ï¼Œä¸”åœ¨test-cleanæ•°æ®é›†ä¸Šçš„è¯é”™ç‡(Word Error Rate, WER)æ— æ˜¾è‘—å¢åŠ ã€‚åœ¨4.26å€çš„å‹ç¼©æ¯”ä¸‹ï¼Œè¯¥æ–¹æ³•ä¸ä»…å®ç°äº†7.05%çš„æä½è¯é”™ç‡ï¼Œè¿˜æ¯”ç°æœ‰æŠ€æœ¯å‡å°‘äº†è‡³å°‘25%çš„å‹ç¼©æ—¶é—´ã€‚è¿™ç§é«˜æ•ˆçš„ä¸€é˜¶æ®µå‹ç¼©æ–¹æ¡ˆä¸ºå¤§è§„æ¨¡è¯­éŸ³åŸºç¡€æ¨¡å‹çš„è½»é‡åŒ–éƒ¨ç½²æä¾›äº†æœ‰åŠ›æ”¯æ’‘ã€‚",
      "categories": [
        "cs.SD",
        "cs.AI",
        "eess.AS"
      ],
      "primary_category": "cs.SD",
      "comment": "Submitted to Interspeech 2025",
      "pdf_url": "https://arxiv.org/pdf/2505.22608v1",
      "published_date": "2025-05-28 17:24:21 UTC",
      "updated_date": "2025-05-28 17:24:21 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T15:16:31.945483+00:00"
    },
    {
      "arxiv_id": "2505.22602v1",
      "title": "One Rank at a Time: Cascading Error Dynamics in Sequential Learning",
      "title_zh": "ä¸€æ¬¡ä¸€ç§©ï¼šåºåˆ—å­¦ä¹ ä¸­çš„çº§è”è¯¯å·®åŠ¨åŠ›å­¦",
      "authors": [
        "Mahtab Alizadeh Vandchali",
        "Fangshuo",
        "Liao",
        "Anastasios Kyrillidis"
      ],
      "abstract": "Sequential learning -- where complex tasks are broken down into simpler, hierarchical components -- has emerged as a paradigm in AI. This paper views sequential learning through the lens of low-rank linear regression, focusing specifically on how errors propagate when learning rank-1 subspaces sequentially. We present an analysis framework that decomposes the learning process into a series of rank-1 estimation problems, where each subsequent estimation depends on the accuracy of previous steps. Our contribution is a characterization of the error propagation in this sequential process, establishing bounds on how errors -- e.g., due to limited computational budgets and finite precision -- affect the overall model accuracy. We prove that these errors compound in predictable ways, with implications for both algorithmic design and stability guarantees.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†åºåˆ—å­¦ä¹ ï¼ˆSequential learningï¼‰ä¸­å¤æ‚ä»»åŠ¡è¢«åˆ†è§£ä¸ºå±‚çº§åŒ–ç»„ä»¶çš„æ¨¡å¼ï¼Œå¹¶ä»ä½ç§©çº¿æ€§å›å½’ï¼ˆlow-rank linear regressionï¼‰çš„è§’åº¦å¯¹å…¶è¿›è¡Œäº†æ·±å…¥åˆ†æã€‚è®ºæ–‡é‡ç‚¹å…³æ³¨åœ¨æŒ‰é¡ºåºå­¦ä¹ ç§©-1å­ç©ºé—´ï¼ˆrank-1 subspacesï¼‰æ—¶è¯¯å·®çš„ä¼ æ’­æœºåˆ¶ï¼Œå¹¶æå‡ºäº†ä¸€ä¸ªå°†å­¦ä¹ è¿‡ç¨‹åˆ†è§£ä¸ºä¸€ç³»åˆ—ç›¸äº’ä¾èµ–çš„ç§©-1ä¼°è®¡ï¼ˆrank-1 estimationï¼‰é—®é¢˜çš„åˆ†ææ¡†æ¶ã€‚è¯¥ç ”ç©¶çš„æ ¸å¿ƒè´¡çŒ®åœ¨äºåˆ»ç”»äº†è¿™ä¸€åºåˆ—è¿‡ç¨‹ä¸­çš„è¯¯å·®ä¼ æ’­ï¼ˆerror propagationï¼‰ç‰¹æ€§ï¼Œå¹¶ç¡®ç«‹äº†è®¡ç®—é¢„ç®—å’Œæœ‰é™ç²¾åº¦ï¼ˆfinite precisionï¼‰ç­‰äº§ç”Ÿçš„è¯¯å·®å¯¹æ•´ä½“æ¨¡å‹å‡†ç¡®æ€§å½±å“çš„ç•Œé™ï¼ˆboundsï¼‰ã€‚è®ºæ–‡è¯æ˜äº†è¿™äº›è¯¯å·®ä»¥å¯é¢„æµ‹çš„æ–¹å¼å¤åˆï¼Œä¸ºæœªæ¥çš„ç®—æ³•è®¾è®¡å’Œç¨³å®šæ€§ä¿è¯ï¼ˆstability guaranteesï¼‰å¥ å®šäº†ç†è®ºåŸºç¡€ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "math.OC"
      ],
      "primary_category": "cs.LG",
      "comment": "36 pages",
      "pdf_url": "https://arxiv.org/pdf/2505.22602v1",
      "published_date": "2025-05-28 17:16:24 UTC",
      "updated_date": "2025-05-28 17:16:24 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T15:17:22.337025+00:00"
    },
    {
      "arxiv_id": "2505.22601v2",
      "title": "Machine Unlearning under Overparameterization",
      "title_zh": "è¿‡å‚æ•°åŒ–èƒŒæ™¯ä¸‹çš„æœºå™¨é—å¿˜",
      "authors": [
        "Jacob L. Block",
        "Aryan Mokhtari",
        "Sanjay Shakkottai"
      ],
      "abstract": "Machine unlearning algorithms aim to remove the influence of specific training samples, ideally recovering the model that would have resulted from training on the remaining data alone. We study unlearning in the overparameterized setting, where many models interpolate the data, and defining the solution as any loss minimizer over the retained set$\\unicode{x2013}$as in prior work in the underparameterized setting$\\unicode{x2013}$is inadequate, since the original model may already interpolate the retained data and satisfy this condition. In this regime, loss gradients vanish, rendering prior methods based on gradient perturbations ineffective, motivating both new unlearning definitions and algorithms. For this setting, we define the unlearning solution as the minimum-complexity interpolator over the retained data and propose a new algorithmic framework that only requires access to model gradients on the retained set at the original solution. We minimize a regularized objective over perturbations constrained to be orthogonal to these model gradients, a first-order relaxation of the interpolation condition. For different model classes, we provide exact and approximate unlearning guarantees and demonstrate that an implementation of our framework outperforms existing baselines across various unlearning experiments.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†åœ¨è¿‡å‚æ•°åŒ–(Overparameterization)è®¾ç½®ä¸‹çš„æœºå™¨å¸è½½(Machine Unlearning)é—®é¢˜ï¼ŒæŒ‡å‡ºåœ¨æ¨¡å‹èƒ½å¤Ÿæ’å€¼æ•°æ®çš„æƒ…å†µä¸‹ï¼Œä¼ ç»Ÿçš„åŸºäºæŸå¤±æœ€å°åŒ–çš„å¸è½½å®šä¹‰å·²ä¸å†é€‚ç”¨ã€‚ç”±äºåŸå§‹æ¨¡å‹å·²ç»æ’å€¼äº†ä¿ç•™æ•°æ®ä¸”å¯¼è‡´æŸå¤±æ¢¯åº¦æ¶ˆå¤±ï¼Œä¼ ç»Ÿçš„æ¢¯åº¦æ‰°åŠ¨æ–¹æ³•å¤±æ•ˆï¼Œå› æ­¤è¯¥ç ”ç©¶å°†å¸è½½ç›®æ ‡é‡æ–°å®šä¹‰ä¸ºåœ¨ä¿ç•™æ•°æ®ä¸Šçš„æœ€å°å¤æ‚åº¦æ’å€¼å™¨(minimum-complexity interpolator)ã€‚ä½œè€…æå‡ºäº†ä¸€ä¸ªæ–°çš„ç®—æ³•æ¡†æ¶ï¼Œè¯¥æ¡†æ¶ä»…éœ€åˆ©ç”¨åŸå§‹è§£åœ¨ä¿ç•™é›†ä¸Šçš„æ¨¡å‹æ¢¯åº¦ï¼Œé€šè¿‡åœ¨ä¸è¿™äº›æ¢¯åº¦æ­£äº¤çš„æ‰°åŠ¨çº¦æŸä¸‹æœ€å°åŒ–æ­£åˆ™åŒ–ç›®æ ‡ï¼Œå®ç°äº†å¯¹æ’å€¼æ¡ä»¶çš„é˜¶æ•°æ¾å¼›ã€‚é’ˆå¯¹ä¸åŒçš„æ¨¡å‹ç±»åˆ«ï¼Œè¯¥ç ”ç©¶æä¾›äº†ç²¾ç¡®å’Œè¿‘ä¼¼çš„å¸è½½ä¿è¯ï¼Œå¹¶åœ¨å¤šé¡¹å®éªŒä¸­è¯æ˜äº†è¯¥æ¡†æ¶çš„æ€§èƒ½ä¼˜äºç°æœ‰çš„åŸºçº¿æ–¹æ³•ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.22601v2",
      "published_date": "2025-05-28 17:14:57 UTC",
      "updated_date": "2025-10-22 21:22:54 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T15:16:55.515293+00:00"
    },
    {
      "arxiv_id": "2505.22598v4",
      "title": "Performance of machine-learning-assisted Monte Carlo in sampling from simple statistical physics models",
      "title_zh": "æœºå™¨å­¦ä¹ è¾…åŠ©è’™ç‰¹å¡æ´›åœ¨ç®€å•ç»Ÿè®¡ç‰©ç†æ¨¡å‹é‡‡æ ·ä¸­çš„æ€§èƒ½",
      "authors": [
        "Luca Maria Del Bono",
        "Federico Ricci-Tersenghi",
        "Francesco Zamponi"
      ],
      "abstract": "Recent years have seen a rise in the application of machine learning techniques to aid the simulation of hard-to-sample systems that cannot be studied using traditional methods. Despite the introduction of many different architectures and procedures, a wide theoretical understanding is still lacking, with the risk of suboptimal implementations. As a first step to address this gap, we provide here a complete analytic study of the widely-used Sequential Tempering procedure applied to a shallow MADE architecture for the Curie-Weiss model. The contribution of this work is twofold: firstly, we give a description of the optimal weights and of the training under Gradient Descent optimization. Secondly, we compare what happens in Sequential Tempering with and without the addition of local Metropolis Monte Carlo steps. We are thus able to give theoretical predictions on the best procedure to apply in this case. This work establishes a clear theoretical basis for the integration of machine learning techniques into Monte Carlo sampling and optimization.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹æœºå™¨å­¦ä¹ è¾…åŠ©æ¨¡æ‹Ÿéš¾é‡‡æ ·ç³»ç»Ÿæ—¶ç¼ºä¹ç†è®ºç†è§£çš„é—®é¢˜ï¼Œå¯¹åº”ç”¨äº Curie-Weiss æ¨¡å‹çš„æµ…å±‚ MADE æ¶æ„åŠå…¶ Sequential Tempering ç¨‹åºè¿›è¡Œäº†å®Œæ•´çš„åˆ†æç ”ç©¶ã€‚è®ºæ–‡é¦–å…ˆè¯¦ç»†æè¿°äº†åœ¨ Gradient Descent ä¼˜åŒ–ä¸‹çš„æ¨¡å‹è®­ç»ƒè¿‡ç¨‹åŠæœ€ä¼˜æƒé‡çš„æ¼”åŒ–è§„å¾‹ã€‚éšåï¼Œç ”ç©¶é€šè¿‡å¯¹æ¯”åˆ†æäº†åœ¨åŒ…å«ä¸ä¸åŒ…å«å±€éƒ¨ Metropolis Monte Carlo æ­¥éª¤æ—¶ï¼ŒSequential Tempering ç¨‹åºçš„æ€§èƒ½å·®å¼‚ã€‚åŸºäºä¸Šè¿°ç†è®ºæ¨å¯¼ï¼Œä½œè€…å¾—å‡ºäº†åœ¨è¯¥ç‰©ç†æ¨¡å‹ä¸‹åº”ç”¨é‡‡æ ·ç¨‹åºçš„æœ€ä½³æµç¨‹é¢„æµ‹ã€‚è¿™é¡¹å·¥ä½œä¸ä»…æä¾›äº†å…·ä½“çš„å‚æ•°æŒ‡å¯¼ï¼Œè¿˜ä¸ºå°†æœºå™¨å­¦ä¹ æŠ€æœ¯é›†æˆåˆ° Monte Carlo é‡‡æ ·ä¸ä¼˜åŒ–ä¸­å»ºç«‹äº†æ¸…æ™°çš„ç†è®ºåŸºç¡€ã€‚",
      "categories": [
        "cond-mat.dis-nn",
        "cond-mat.stat-mech",
        "cs.AI",
        "cs.LG",
        "physics.comp-ph"
      ],
      "primary_category": "cond-mat.dis-nn",
      "comment": "17 pages, 10 figures",
      "pdf_url": "https://arxiv.org/pdf/2505.22598v4",
      "published_date": "2025-05-28 17:13:11 UTC",
      "updated_date": "2025-10-07 20:41:31 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T15:17:03.348858+00:00"
    },
    {
      "arxiv_id": "2505.22597v1",
      "title": "HDDLGym: A Tool for Studying Multi-Agent Hierarchical Problems Defined in HDDL with OpenAI Gym",
      "title_zh": "HDDLGymï¼šç»“åˆ OpenAI Gym ç ”ç©¶ HDDL å®šä¹‰ä¸‹å¤šæ™ºèƒ½ä½“åˆ†å±‚é—®é¢˜çš„å·¥å…·",
      "authors": [
        "Ngoc La",
        "Ruaridh Mon-Williams",
        "Julie A. Shah"
      ],
      "abstract": "In recent years, reinforcement learning (RL) methods have been widely tested using tools like OpenAI Gym, though many tasks in these environments could also benefit from hierarchical planning. However, there is a lack of a tool that enables seamless integration of hierarchical planning with RL. Hierarchical Domain Definition Language (HDDL), used in classical planning, introduces a structured approach well-suited for model-based RL to address this gap. To bridge this integration, we introduce HDDLGym, a Python-based tool that automatically generates OpenAI Gym environments from HDDL domains and problems. HDDLGym serves as a link between RL and hierarchical planning, supporting multi-agent scenarios and enabling collaborative planning among agents. This paper provides an overview of HDDLGym's design and implementation, highlighting the challenges and design choices involved in integrating HDDL with the Gym interface, and applying RL policies to support hierarchical planning. We also provide detailed instructions and demonstrations for using the HDDLGym framework, including how to work with existing HDDL domains and problems from International Planning Competitions, exemplified by the Transport domain. Additionally, we offer guidance on creating new HDDL domains for multi-agent scenarios and demonstrate the practical use of HDDLGym in the Overcooked domain. By leveraging the advantages of HDDL and Gym, HDDLGym aims to be a valuable tool for studying RL in hierarchical planning, particularly in multi-agent contexts.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¨å‡ºäº† HDDLGymï¼Œè¿™æ˜¯ä¸€æ¬¾æ—¨åœ¨å¡«è¡¥å±‚æ¬¡åŒ–è§„åˆ’ (Hierarchical Planning) ä¸å¼ºåŒ–å­¦ä¹  (Reinforcement Learning) ä¹‹é—´æ•´åˆç©ºç™½çš„ Python å·¥å…·ã€‚è¯¥å·¥å…·èƒ½å¤Ÿæ ¹æ®å±‚æ¬¡åŒ–é¢†åŸŸå®šä¹‰è¯­è¨€ (Hierarchical Domain Definition Language, HDDL) è‡ªåŠ¨ç”Ÿæˆ OpenAI Gym ç¯å¢ƒï¼Œä»è€Œæœ‰æ•ˆæ”¯æŒå¤šæ™ºèƒ½ä½“ (Multi-Agent) åœºæ™¯ä¸‹çš„åä½œè§„åˆ’ã€‚è®ºæ–‡è¯¦ç»†é˜è¿°äº† HDDLGym çš„è®¾è®¡ä¸å®ç°ï¼Œå¹¶æ¢è®¨äº†åœ¨ Gym æ¥å£ä¸­é›†æˆ HDDL ä»¥åŠåˆ©ç”¨ RL ç­–ç•¥æ”¯æŒå±‚æ¬¡åŒ–è§„åˆ’æ—¶æ‰€é¢ä¸´çš„è®¾è®¡æŒ‘æˆ˜ã€‚é€šè¿‡åœ¨ Transport é¢†åŸŸä»¥åŠ Overcooked å¤šæ™ºèƒ½ä½“ç¯å¢ƒä¸­çš„åº”ç”¨æ¼”ç¤ºï¼Œè¯¥æ¡†æ¶å±•ç¤ºäº†å…¶å¤„ç†å›½é™…è§„åˆ’ç«èµ› (International Planning Competitions) ä»»åŠ¡çš„çµæ´»æ€§ã€‚HDDLGym ä¸ºåœ¨å±‚æ¬¡åŒ–è§„åˆ’èƒŒæ™¯ä¸‹æ·±å…¥ç ”ç©¶å¼ºåŒ–å­¦ä¹ æä¾›äº†é‡è¦çš„å·¥å…·æ”¯æŒï¼Œå°¤å…¶åœ¨è§£å†³å¤æ‚çš„å¤šæ™ºèƒ½ä½“åä½œé—®é¢˜æ–¹é¢å…·æœ‰æ˜¾è‘—ä»·å€¼ã€‚",
      "categories": [
        "cs.AI",
        "cs.LG",
        "cs.MA"
      ],
      "primary_category": "cs.AI",
      "comment": "Accepted to Proceedings of ICAPS 2025",
      "pdf_url": "https://arxiv.org/pdf/2505.22597v1",
      "published_date": "2025-05-28 17:10:43 UTC",
      "updated_date": "2025-05-28 17:10:43 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T15:17:13.605316+00:00"
    },
    {
      "arxiv_id": "2505.22591v1",
      "title": "Self-Error-Instruct: Generalizing from Errors for LLMs Mathematical Reasoning",
      "title_zh": "Self-Error-Instructï¼šé€šè¿‡é”™è¯¯æ³›åŒ–æå‡å¤§è¯­è¨€æ¨¡å‹çš„æ•°å­¦æ¨ç†èƒ½åŠ›",
      "authors": [
        "Erxin Yu",
        "Jing Li",
        "Ming Liao",
        "Qi Zhu",
        "Boyang Xue",
        "Minghui Xu",
        "Baojun Wang",
        "Lanqing Hong",
        "Fei Mi",
        "Lifeng Shang"
      ],
      "abstract": "Although large language models demonstrate strong performance across various domains, they still struggle with numerous bad cases in mathematical reasoning. Previous approaches to learning from errors synthesize training data by solely extrapolating from isolated bad cases, thereby failing to generalize the extensive patterns inherent within these cases. This paper presents Self-Error-Instruct (SEI), a framework that addresses these model weaknesses and synthesizes more generalized targeted training data. Specifically, we explore a target model on two mathematical datasets, GSM8K and MATH, to pinpoint bad cases. Then, we generate error keyphrases for these cases based on the instructor model's (GPT-4o) analysis and identify error types by clustering these keyphrases. Next, we sample a few bad cases during each generation for each identified error type and input them into the instructor model, which synthesizes additional training data using a self-instruct approach. This new data is refined through a one-shot learning process to ensure that only the most effective examples are kept. Finally, we use these curated data to fine-tune the target model, iteratively repeating the process to enhance performance. We apply our framework to various models and observe improvements in their reasoning abilities across both in-domain and out-of-domain mathematics datasets. These results demonstrate the effectiveness of self-error instruction in improving LLMs' mathematical reasoning through error generalization.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†Self-Error-Instruct (SEI)æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³å¤§è¯­è¨€æ¨¡å‹(LLMs)åœ¨æ•°å­¦æ¨ç†ä¸­é¢ä¸´çš„é”™è¯¯æ¡ˆä¾‹åŠå…¶æ³›åŒ–æ€§ä¸è¶³çš„é—®é¢˜ã€‚è¯¥æ¡†æ¶é¦–å…ˆå¯¹GSM8Kå’ŒMATHæ•°æ®é›†ä¸­çš„åä¾‹(bad cases)è¿›è¡Œåˆ†æï¼Œåˆ©ç”¨å¯¼å¸ˆæ¨¡å‹æå–é”™è¯¯å…³é”®è¯(keyphrases)å¹¶èšç±»è¯†åˆ«å‡ºä¸åŒçš„é”™è¯¯ç±»å‹(error types)ã€‚éšåï¼ŒSEIé‡‡ç”¨Self-Instructæ–¹æ³•é’ˆå¯¹è¿™äº›é”™è¯¯ç±»å‹åˆæˆç›®æ ‡è®­ç»ƒæ•°æ®ï¼Œå¹¶ç»“åˆå•æ ·æœ¬å­¦ä¹ (one-shot learning)å¯¹æ•°æ®è¿›è¡Œç²¾ç‚¼ã€‚é€šè¿‡å°†è¿™äº›é«˜è´¨é‡æ•°æ®ç”¨äºç›®æ ‡æ¨¡å‹çš„è¿­ä»£å¾®è°ƒï¼Œæ˜¾è‘—å¢å¼ºäº†æ¨¡å‹åœ¨åŸŸå†…åŠåŸŸå¤–æ•°å­¦ä»»åŠ¡ä¸­çš„è¡¨ç°ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒSelf-Error-Instructèƒ½æœ‰æ•ˆé€šè¿‡é”™è¯¯æ³›åŒ–æå‡LLMsçš„æ•°å­¦æ¨ç†èƒ½åŠ›ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "16 pages, 9 figures",
      "pdf_url": "https://arxiv.org/pdf/2505.22591v1",
      "published_date": "2025-05-28 17:02:47 UTC",
      "updated_date": "2025-05-28 17:02:47 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T15:17:06.525501+00:00"
    },
    {
      "arxiv_id": "2505.22583v1",
      "title": "GitGoodBench: A Novel Benchmark For Evaluating Agentic Performance On Git",
      "title_zh": "GitGoodBenchï¼šä¸€ç§ç”¨äºè¯„ä¼° Git æ™ºèƒ½ä½“æ€§èƒ½çš„æ–°å‹åŸºå‡†",
      "authors": [
        "Tobias Lindenbauer",
        "Egor Bogomolov",
        "Yaroslav Zharov"
      ],
      "abstract": "Benchmarks for Software Engineering (SE) AI agents, most notably SWE-bench, have catalyzed progress in programming capabilities of AI agents. However, they overlook critical developer workflows such as Version Control System (VCS) operations. To address this issue, we present GitGoodBench, a novel benchmark for evaluating AI agent performance on VCS tasks. GitGoodBench covers three core Git scenarios extracted from permissive open-source Python, Java, and Kotlin repositories. Our benchmark provides three datasets: a comprehensive evaluation suite (900 samples), a rapid prototyping version (120 samples), and a training corpus (17,469 samples). We establish baseline performance on the prototyping version of our benchmark using GPT-4o equipped with custom tools, achieving a 21.11% solve rate overall. We expect GitGoodBench to serve as a crucial stepping stone toward truly comprehensive SE agents that go beyond mere programming.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ç°æœ‰è½¯ä»¶å·¥ç¨‹æ™ºèƒ½ä½“åŸºå‡†æµ‹è¯•å¿½è§†ç‰ˆæœ¬æ§åˆ¶ç³»ç»Ÿ(Version Control System)æ“ä½œçš„é—®é¢˜ï¼Œæå‡ºäº†GitGoodBenchï¼Œä¸€ä¸ªç”¨äºè¯„ä¼°AIæ™ºèƒ½ä½“åœ¨Gitä»»åŠ¡ä¸Šè¡¨ç°çš„æ–°å‹åŸºå‡†ã€‚è¯¥åŸºå‡†æ¶µç›–äº†ä»å¼€æºPythonã€Javaå’ŒKotlinä»£ç åº“ä¸­æå–çš„ä¸‰ç§æ ¸å¿ƒGitåœºæ™¯ï¼Œå¹¶æä¾›äº†åŒ…æ‹¬ç»¼åˆè¯„ä¼°å¥—ä»¶ã€å¿«é€ŸåŸå‹ç‰ˆæœ¬å’Œè®­ç»ƒè¯­æ–™åº“åœ¨å†…çš„ä¸‰å¥—æ•°æ®é›†ã€‚é€šè¿‡ä½¿ç”¨é…å¤‡è‡ªå®šä¹‰å·¥å…·çš„GPT-4oè¿›è¡Œæµ‹è¯•ï¼Œç ”ç©¶å›¢é˜Ÿåœ¨åŸå‹ç‰ˆæœ¬ä¸Šå»ºç«‹äº†åŸºçº¿è¡¨ç°ï¼Œå®ç°äº†21.11%çš„æ€»è§£å†³ç‡ã€‚GitGoodBenchçš„æ¨å‡ºä¸ºå¼€å‘èƒ½å¤Ÿå¤„ç†å¤æ‚ç‰ˆæœ¬æ§åˆ¶å·¥ä½œæµã€è¶…è¶Šå•çº¯ç¼–ç¨‹èƒ½åŠ›çš„å…¨é¢è½¯ä»¶å·¥ç¨‹æ™ºèƒ½ä½“å¥ å®šäº†åŸºç¡€ã€‚",
      "categories": [
        "cs.SE",
        "cs.AI"
      ],
      "primary_category": "cs.SE",
      "comment": "Short Paper, 5 pages",
      "pdf_url": "https://arxiv.org/pdf/2505.22583v1",
      "published_date": "2025-05-28 16:56:11 UTC",
      "updated_date": "2025-05-28 16:56:11 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T15:17:09.814800+00:00"
    },
    {
      "arxiv_id": "2505.22581v1",
      "title": "Tell me Habibi, is it Real or Fake?",
      "title_zh": "å‘Šè¯‰æˆ‘ï¼ŒHabibiï¼šè¿™ç©¶ç«Ÿæ˜¯çœŸæ˜¯å‡ï¼Ÿ",
      "authors": [
        "Kartik Kuckreja",
        "Parul Gupta",
        "Injy Hamed",
        "Thamar Solorio",
        "Muhammad Haris Khan",
        "Abhinav Dhall"
      ],
      "abstract": "Deepfake generation methods are evolving fast, making fake media harder to detect and raising serious societal concerns. Most deepfake detection and dataset creation research focuses on monolingual content, often overlooking the challenges of multilingual and code-switched speech, where multiple languages are mixed within the same discourse. Code-switching, especially between Arabic and English, is common in the Arab world and is widely used in digital communication. This linguistic mixing poses extra challenges for deepfake detection, as it can confuse models trained mostly on monolingual data. To address this, we introduce \\textbf{ArEnAV}, the first large-scale Arabic-English audio-visual deepfake dataset featuring intra-utterance code-switching, dialectal variation, and monolingual Arabic content. It \\textbf{contains 387k videos and over 765 hours of real and fake videos}. Our dataset is generated using a novel pipeline integrating four Text-To-Speech and two lip-sync models, enabling comprehensive analysis of multilingual multimodal deepfake detection. We benchmark our dataset against existing monolingual and multilingual datasets, state-of-the-art deepfake detection models, and a human evaluation, highlighting its potential to advance deepfake research. The dataset can be accessed \\href{https://huggingface.co/datasets/kartik060702/ArEnAV-Full}{here}.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å½“å‰ Deepfake æ£€æµ‹ä¸»è¦é›†ä¸­äºå•è¯­å†…å®¹ï¼Œè€Œå¿½è§†äº†å¤šè¯­è¨€åŠè¯­ç æ··ç”¨ (Code-switching) æŒ‘æˆ˜çš„ç°çŠ¶ï¼Œç‰¹åˆ«æ˜¯é˜¿æ‹‰ä¼¯è¯­ä¸è‹±è¯­æ··åˆä½¿ç”¨çš„åœºæ™¯ã€‚ä¸ºæ­¤ï¼Œç ”ç©¶è€…æ¨å‡ºäº† ArEnAVï¼Œè¿™æ˜¯é¦–ä¸ªåŒ…å«è¯è¯­å†…è¯­ç æ··ç”¨ã€æ–¹è¨€å˜åŒ–å’Œçº¯é˜¿æ‹‰ä¼¯è¯­å†…å®¹çš„å¤§è§„æ¨¡é˜¿è‹±åŒè¯­éŸ³è§†é¢‘ Deepfake æ•°æ®é›†ã€‚è¯¥æ•°æ®é›†åŒ…å« 387k ä¸ªè§†é¢‘å’Œè¶…è¿‡ 765 å°æ—¶çš„å†…å®¹ï¼Œé€šè¿‡é›†æˆå››ç§ Text-To-Speech (TTS) å’Œä¸¤ç§ lip-sync æ¨¡å‹çš„æ–°å‹æµæ°´çº¿ç”Ÿæˆã€‚å®éªŒé€šè¿‡å¯¹ç°æœ‰çš„å•è¯­ä¸å¤šè¯­æ•°æ®é›†ã€å…ˆè¿›çš„ Deepfake æ£€æµ‹æ¨¡å‹ä»¥åŠäººç±»è¯„ä¼°è¿›è¡ŒåŸºå‡†æµ‹è¯•ï¼ŒéªŒè¯äº†è¯¥æ•°æ®é›†åœ¨å¤šè¯­è¨€å¤šæ¨¡æ€æ·±åº¦ä¼ªé€ æ£€æµ‹é¢†åŸŸçš„æœ‰æ•ˆæ€§ã€‚ArEnAV çš„å‘å¸ƒä¸ºè§£å†³å¤šè¯­è¨€ç¯å¢ƒä¸‹çš„ä¼ªé€ åª’ä½“è¯†åˆ«é—®é¢˜æä¾›äº†é‡è¦èµ„æºï¼Œå…·æœ‰æ¨åŠ¨å¤šè¯­è¨€æ·±åº¦ä¼ªé€ ç ”ç©¶è¿›é˜¶çš„å·¨å¤§æ½œåŠ›ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "9 pages, 2 figures, 12 tables",
      "pdf_url": "https://arxiv.org/pdf/2505.22581v1",
      "published_date": "2025-05-28 16:54:36 UTC",
      "updated_date": "2025-05-28 16:54:36 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T15:17:45.978408+00:00"
    },
    {
      "arxiv_id": "2506.00047v1",
      "title": "Risks of AI-driven product development and strategies for their mitigation",
      "title_zh": "AIé©±åŠ¨çš„äº§å“å¼€å‘é£é™©åŠå…¶ç¼“è§£ç­–ç•¥",
      "authors": [
        "Jan GÃ¶pfert",
        "Jann M. Weinand",
        "Patrick Kuckertz",
        "Noah Pflugradt",
        "Jochen LinÃŸen"
      ],
      "abstract": "Humanity is progressing towards automated product development, a trend that promises faster creation of better products and thus the acceleration of technological progress. However, increasing reliance on non-human agents for this process introduces many risks. This perspective aims to initiate a discussion on these risks and appropriate mitigation strategies. To this end, we outline a set of principles for safer AI-driven product development which emphasize human oversight, accountability, and explainable design, among others. The risk assessment covers both technical risks which affect product quality and safety, and sociotechnical risks which affect society. While AI-driven product development is still in its early stages, this discussion will help balance its opportunities and risks without delaying essential progress in understanding, norm-setting, and regulation.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†AI-driven product developmentï¼ˆäººå·¥æ™ºèƒ½é©±åŠ¨çš„äº§å“å¼€å‘ï¼‰å¸¦æ¥çš„é£é™©åŠå…¶ç¼“è§£ç­–ç•¥ã€‚éšç€äººç±»å‘è‡ªåŠ¨åŒ–äº§å“å¼€å‘è¿ˆè¿›ï¼Œè¿‡åº¦ä¾èµ–non-human agentsï¼ˆéäººç±»æ™ºèƒ½ä½“ï¼‰å¼•å‘äº†è¯¸å¤šæŒ‘æˆ˜ã€‚æ–‡ç« æ·±å…¥è¯„ä¼°äº†å½±å“äº§å“è´¨é‡ä¸å®‰å…¨çš„technical risksï¼ˆæŠ€æœ¯é£é™©ï¼‰ï¼Œä»¥åŠå½±å“ç¤¾ä¼šçš„sociotechnical risksï¼ˆç¤¾ä¼šæŠ€æœ¯é£é™©ï¼‰ã€‚ä¸ºæ­¤ï¼Œä½œè€…æå‡ºäº†ä¸€å¥—æ—¨åœ¨å®ç°æ›´å®‰å…¨å¼€å‘çš„åŸåˆ™ï¼Œé‡ç‚¹å¼ºè°ƒäº†human oversightï¼ˆäººç±»ç›‘ç£ï¼‰ã€accountabilityï¼ˆé—®è´£åˆ¶ï¼‰å’Œexplainable designï¼ˆå¯è§£é‡Šæ€§è®¾è®¡ï¼‰çš„é‡è¦æ€§ã€‚è™½ç„¶è¯¥é¢†åŸŸç›®å‰ä»å¤„äºæ—©æœŸé˜¶æ®µï¼Œä½†è¯¥è®¨è®ºæœ‰åŠ©äºåœ¨ä¸å»¶è¿ŸæŠ€æœ¯è¿›æ­¥çš„å‰æä¸‹å¹³è¡¡æœºé‡ä¸é£é™©ï¼Œå¹¶ä¸ºæœªæ¥çš„è§„èŒƒåˆ¶å®šå’Œregulationï¼ˆç›‘ç®¡ï¼‰å¥ å®šåŸºç¡€ã€‚",
      "categories": [
        "cs.CY",
        "cs.AI",
        "cs.CE"
      ],
      "primary_category": "cs.CY",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.00047v1",
      "published_date": "2025-05-28 16:52:44 UTC",
      "updated_date": "2025-05-28 16:52:44 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T15:17:46.116358+00:00"
    },
    {
      "arxiv_id": "2505.22572v1",
      "title": "Fusion Steering: Prompt-Specific Activation Control",
      "title_zh": "Fusion Steeringï¼šé’ˆå¯¹ç‰¹å®šæç¤ºçš„æ¿€æ´»æ§åˆ¶",
      "authors": [
        "Waldemar Chang",
        "Alhassan Yasin"
      ],
      "abstract": "We present Fusion Steering, an activation steering methodology that improves factual accuracy in large language models (LLMs) for question-answering (QA) tasks. This approach introduces flexible steering configurations, including full-layer steering and segmented steering. Unlike traditional methods constrained to single-layer or fixed-layer operations, Fusion Steering employs dynamic injection of prompt-specific activation deltas across all transformer layers. These activation deltas are derived from reference completions that combine the ground-truth answer with a model-generated explanation to facilitate semantically enriched, example-specific steering. The injection weights are optimized per prompt using Optuna, targeting a joint objective that balances token overlap (factual alignment) and perplexity (fluency proxy). Evaluation employs a composite score integrating token overlap and LLM-graded quality, encompassing factual accuracy, coherence, and relevance. Empirical results on 260 SimpleQA prompts (selected from 500 where the baseline failed) showcase the efficacy of segmented steering. Using Gemma-2-2B-IT with 8-bit quantization, segmented steering achieves an accuracy of 25.4% (outputs scoring $\\geq 0.6$), outperforming the baseline at 3.5% and full-layer steering at 16.2%. Under the stricter SimpleQA rubric, segmented steering boosts fully correct responses from 0.0% to 13.1%. These findings highlight the strengths of segmented, dynamic intervention strategies and the promise of per-prompt, full-network activation control. Fusion Steering is also amenable to sparse representations, such as Neuronpedia or sparse crosscoders, suggesting a promising direction for interpretable and scalable activation-level control in LLMs.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† Fusion Steeringï¼Œè¿™æ˜¯ä¸€ç§æ—¨åœ¨æé«˜å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨é—®ç­”ä»»åŠ¡ä¸­äº‹å®å‡†ç¡®æ€§çš„æ¿€æ´»è½¬å‘ï¼ˆactivation steeringï¼‰æ–¹æ³•ã€‚è¯¥æ–¹æ³•å¼•å…¥äº†å…¨å±‚è½¬å‘ï¼ˆfull-layer steeringï¼‰å’Œåˆ†æ®µè½¬å‘ï¼ˆsegmented steeringï¼‰ç­‰çµæ´»é…ç½®ï¼Œé€šè¿‡åœ¨æ‰€æœ‰ Transformer å±‚ä¸­åŠ¨æ€æ³¨å…¥é’ˆå¯¹ç‰¹å®šæç¤ºçš„æ¿€æ´»å¢é‡ï¼ˆactivation deltasï¼‰æ¥å®ç°ç²¾ç¡®æ§åˆ¶ã€‚è¿™äº›æ¿€æ´»å¢é‡æºè‡ªç»“åˆäº†æ ‡å‡†ç­”æ¡ˆå’Œæ¨¡å‹ç”Ÿæˆè§£é‡Šçš„å‚è€ƒæ–‡æœ¬ï¼Œå¹¶åˆ©ç”¨ Optuna å¯¹æ¯ä¸ªæç¤ºçš„æ³¨å…¥æƒé‡è¿›è¡Œä¼˜åŒ–ï¼Œä»¥å¹³è¡¡äº‹å®å¯¹é½ä¸è¯­è¨€æµç•…åº¦ã€‚åœ¨ SimpleQA æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œä½¿ç”¨ Gemma-2-2B-IT æ¨¡å‹æ—¶ï¼Œåˆ†æ®µè½¬å‘å°†å‡†ç¡®ç‡ä»åŸºçº¿çš„ 3.5% æ˜¾è‘—æå‡è‡³ 25.4%ï¼Œå®Œå…¨æ­£ç¡®ç‡ä» 0% æé«˜åˆ° 13.1%ã€‚ç ”ç©¶ç»“æœè¯æ˜äº†å…¨ç½‘ç»œåŠ¨æ€å¹²é¢„ç­–ç•¥çš„ä¼˜è¶Šæ€§ï¼Œå¹¶ä¸ºç»“åˆç¨€ç–è¡¨ç¤ºå®ç°å¯è§£é‡Šçš„æ¿€æ´»çº§æ§åˆ¶æä¾›äº†æ–°æ–¹å‘ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "14 pages, 4 figures, 2 tables",
      "pdf_url": "https://arxiv.org/pdf/2505.22572v1",
      "published_date": "2025-05-28 16:46:55 UTC",
      "updated_date": "2025-05-28 16:46:55 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T15:18:07.813804+00:00"
    },
    {
      "arxiv_id": "2505.22571v3",
      "title": "Agent-UniRAG: A Trainable Open-Source LLM Agent Framework for Unified Retrieval-Augmented Generation Systems",
      "title_zh": "Agent-UniRAGï¼šé¢å‘ç»Ÿä¸€æ£€ç´¢å¢å¼ºç”Ÿæˆç³»ç»Ÿçš„å¯è®­ç»ƒå¼€æºå¤§è¯­è¨€æ¨¡å‹æ™ºèƒ½ä½“æ¡†æ¶",
      "authors": [
        "Hoang Pham",
        "Thuy-Duong Nguyen",
        "Khac-Hoai Nam Bui"
      ],
      "abstract": "This paper presents a novel approach for unified retrieval-augmented generation (RAG) systems using the recent emerging large language model (LLM) agent concept. Specifically, Agent LLM, which utilizes LLM as fundamental controllers, has become a promising approach to enable the interpretability of RAG tasks, especially for complex reasoning question-answering systems (e.g., multi-hop queries). Nonetheless, previous works mainly focus on solving RAG systems with either single-hop or multi-hop approaches separately, which limits the application of those approaches to real-world applications. In this study, we propose a trainable agent framework called Agent-UniRAG for unified retrieval-augmented LLM systems, which enhances the effectiveness and interpretability of RAG systems. The main idea is to design an LLM agent framework to solve RAG tasks step-by-step based on the complexity of the inputs, simultaneously including single-hop and multi-hop queries in an end-to-end manner. Furthermore, we introduce SynAgent-RAG, a synthetic dataset to enable the proposed agent framework for small open-source LLMs (e.g., Llama-3-8B). The results show comparable performances with closed-source and larger open-source LLMs across various RAG benchmarks. Our source code and dataset are publicly available for further exploitation.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†Agent-UniRAGï¼Œä¸€ä¸ªå¯è®­ç»ƒçš„å¼€æºå¤§è¯­è¨€æ¨¡å‹(LLM)æ™ºèƒ½ä½“æ¡†æ¶ï¼Œæ—¨åœ¨æ„å»ºç»Ÿä¸€çš„æ£€ç´¢å¢å¼ºç”Ÿæˆ(Retrieval-Augmented Generation, RAG)ç³»ç»Ÿã€‚é’ˆå¯¹ä»¥å¾€RAGç³»ç»Ÿé€šå¸¸å°†å•è·³(single-hop)å’Œå¤šè·³(multi-hop)æŸ¥è¯¢åˆ†å¼€å¤„ç†å¸¦æ¥çš„å±€é™æ€§ï¼Œè¯¥æ¡†æ¶å°†LLMä½œä¸ºæ ¸å¿ƒæ§åˆ¶å™¨ï¼Œå®ç°äº†å¯¹ä¸åŒå¤æ‚åº¦é—®é¢˜çš„ç«¯åˆ°ç«¯æ­¥è¿›å¼å¤„ç†ï¼Œæå‡äº†ç³»ç»Ÿçš„æœ‰æ•ˆæ€§ä¸å¯è§£é‡Šæ€§ã€‚ä¸ºäº†ä½¿å°å‹å¼€æºæ¨¡å‹ï¼ˆå¦‚Llama-3-8Bï¼‰ä¹Ÿèƒ½å…·å¤‡å¼ºå¤§çš„æ™ºèƒ½ä½“èƒ½åŠ›ï¼Œç ”ç©¶å›¢é˜Ÿè¿˜å¼•å…¥äº†ä¸“ç”¨çš„åˆæˆæ•°æ®é›†SynAgent-RAGã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒAgent-UniRAGåœ¨å¤šä¸ªRAGåŸºå‡†æµ‹è¯•ä¸­å±•ç°äº†å“è¶Šçš„æ€§èƒ½ï¼Œè¶³ä»¥åª²ç¾å‚æ•°é‡æ›´å¤§çš„å¼€æºæ¨¡å‹åŠä¸»æµé—­æºæ¨¡å‹ã€‚ç›®å‰ï¼Œè¯¥ç ”ç©¶çš„æºä»£ç å’Œæ•°æ®é›†å·²å…¬å¼€ï¼Œä¸ºåœ¨çœŸå®åº”ç”¨ä¸­éƒ¨ç½²é«˜æ•ˆçš„ç»Ÿä¸€RAGç³»ç»Ÿæä¾›äº†é‡è¦å‚è€ƒã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.DB",
        "cs.IR"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.22571v3",
      "published_date": "2025-05-28 16:46:31 UTC",
      "updated_date": "2025-05-30 02:44:41 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T15:18:39.790422+00:00"
    },
    {
      "arxiv_id": "2506.03169v1",
      "title": "Improvement of human health lifespan with hybrid group pose estimation methods",
      "title_zh": "åŸºäºæ··åˆç¾¤ç»„å§¿æ€ä¼°è®¡æ–¹æ³•çš„äººç±»å¥åº·å¯¿å‘½æå‡",
      "authors": [
        "Arindam Chaudhuri"
      ],
      "abstract": "Human beings rely heavily on estimation of poses in order to access their body movements. Human pose estimation methods take advantage of computer vision advances in order to track human body movements in real life applications. This comes from videos which are recorded through available devices. These para-digms provide potential to make human movement measurement more accessible to users. The consumers of pose estimation movements believe that human poses content tend to supplement available videos. This has increased pose estimation software usage to estimate human poses. In order to address this problem, we develop hybrid-ensemble-based group pose estimation method to improve human health. This proposed hybrid-ensemble-based group pose estimation method aims to detect multi-person poses using modified group pose estimation and modified real time pose estimation. This ensemble allows fusion of performance of stated methods in real time. The input poses from images are fed into individual meth-ods. The pose transformation method helps to identify relevant features for en-semble to perform training effectively. After this, customized pre-trained hybrid ensemble is trained on public benchmarked datasets which is being evaluated through test datasets. The effectiveness and viability of proposed method is estab-lished based on comparative analysis of group pose estimation methods and ex-periments conducted on benchmarked datasets. It provides best optimized results in real-time pose estimation. It makes pose estimation method more robust to oc-clusion and improves dense regression accuracy. These results have affirmed po-tential application of this method in several real-time situations with improvement in human health life span",
      "tldr_zh": "è¯¥ç ”ç©¶å¼€å‘äº†ä¸€ç§åŸºäº hybrid-ensemble-based group pose estimation çš„æ–¹æ³•ï¼Œæ—¨åœ¨é€šè¿‡ä¼˜åŒ–äººä½“åŠ¨ä½œè¿½è¸ªæŠ€æœ¯æ¥æ”¹å–„äººç±»å¥åº·å¯¿å‘½ã€‚è¯¥æ–¹æ³•ç»“åˆäº†æ”¹è¿›çš„ group pose estimation å’Œ modified real-time pose estimationï¼Œé€šè¿‡é›†æˆå­¦ä¹ å®ç°äº†å¯¹å¤šç›®æ ‡å§¿æ€çš„é«˜æ•ˆå®æ—¶æ£€æµ‹ã€‚åˆ©ç”¨ pose transformation method æå–å…³é”®ç‰¹å¾åï¼Œç ”ç©¶å›¢é˜Ÿåœ¨å…¬å¼€çš„ benchmarked datasets ä¸Šå¯¹é¢„è®­ç»ƒçš„æ··åˆæ¨¡å‹è¿›è¡Œäº†è®­ç»ƒä¸è¯„ä¼°ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å®æ—¶å§¿æ€ä¼°è®¡ä¸­å–å¾—äº†æœ€ä¼˜çš„ä¼˜åŒ–æ•ˆæœï¼Œå¹¶æ˜¾è‘—æå‡äº†æ¨¡å‹åœ¨åº”å¯¹ occlusion æ—¶çš„é²æ£’æ€§ã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ¡ˆæ”¹å–„äº† dense regression çš„å‡†ç¡®æ€§ï¼Œè¯æ˜äº†å…¶åœ¨å¤šç§å®æ—¶å¥åº·ç›‘æµ‹åœºæ™¯ä¸­çš„åº”ç”¨ä»·å€¼åŠå¯¹æå‡äººç±»å¥åº·æ°´å¹³çš„æ½œåŠ›ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.03169v1",
      "published_date": "2025-05-28 16:43:28 UTC",
      "updated_date": "2025-05-28 16:43:28 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T15:20:28.538099+00:00"
    },
    {
      "arxiv_id": "2505.22566v1",
      "title": "Universal Visuo-Tactile Video Understanding for Embodied Interaction",
      "title_zh": "é¢å‘å…·èº«äº¤äº’çš„é€šç”¨è§†è§¦è§‰è§†é¢‘ç†è§£",
      "authors": [
        "Yifan Xie",
        "Mingyang Li",
        "Shoujie Li",
        "Xingting Li",
        "Guangyu Chen",
        "Fei Ma",
        "Fei Richard Yu",
        "Wenbo Ding"
      ],
      "abstract": "Tactile perception is essential for embodied agents to understand physical attributes of objects that cannot be determined through visual inspection alone. While existing approaches have made progress in visual and language modalities for physical understanding, they fail to effectively incorporate tactile information that provides crucial haptic feedback for real-world interaction. In this paper, we present VTV-LLM, the first multi-modal large language model for universal Visuo-Tactile Video (VTV) understanding that bridges the gap between tactile perception and natural language. To address the challenges of cross-sensor and cross-modal integration, we contribute VTV150K, a comprehensive dataset comprising 150,000 video frames from 100 diverse objects captured across three different tactile sensors (GelSight Mini, DIGIT, and Tac3D), annotated with four fundamental tactile attributes (hardness, protrusion, elasticity, and friction). We develop a novel three-stage training paradigm that includes VTV enhancement for robust visuo-tactile representation, VTV-text alignment for cross-modal correspondence, and text prompt finetuning for natural language generation. Our framework enables sophisticated tactile reasoning capabilities including feature assessment, comparative analysis, scenario-based decision making and so on. Experimental evaluations demonstrate that VTV-LLM achieves superior performance in tactile video understanding tasks, establishing a foundation for more intuitive human-machine interaction in tactile domains.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†VTV-LLMï¼Œè¿™æ˜¯é¦–ä¸ªç”¨äºé€šç”¨è§†è§¦è§‰è§†é¢‘(Visuo-Tactile Video, VTV)ç†è§£çš„å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼Œæ—¨åœ¨æ¡¥æ¥è§¦è§‰æ„ŸçŸ¥ä¸è‡ªç„¶è¯­è¨€ä¹‹é—´çš„é¸¿æ²Ÿï¼Œè§£å†³å…·èº«æ™ºèƒ½ä½“åœ¨ç‰©ç†äº¤äº’ä¸­ç¼ºä¹è§¦è§‰åé¦ˆçš„é—®é¢˜ã€‚ä¸ºåº”å¯¹è·¨ä¼ æ„Ÿå™¨é›†æˆçš„æŒ‘æˆ˜ï¼Œç ”ç©¶å›¢é˜Ÿè´¡çŒ®äº†åŒ…å«15ä¸‡ä¸ªè§†é¢‘å¸§çš„VTV150Kæ•°æ®é›†ï¼Œæ¶µç›–äº†ä¸‰ç§è§¦è§‰ä¼ æ„Ÿå™¨(GelSight Mini, DIGIT, Tac3D)å¯¹100ç§å¤šæ ·åŒ–ç‰©ä½“é‡‡é›†çš„å››ç§åŸºæœ¬è§¦è§‰å±æ€§ã€‚è¯¥æ¨¡å‹é‡‡ç”¨äº†åˆ›æ–°çš„ä¸‰é˜¶æ®µè®­ç»ƒèŒƒå¼ï¼ŒåŒ…æ‹¬VTVå¢å¼ºã€VTV-æ–‡æœ¬å¯¹é½ä»¥åŠæ–‡æœ¬æç¤ºå¾®è°ƒï¼Œä»è€Œå®ç°äº†ç‰¹å¾è¯„ä¼°ã€å¯¹æ¯”åˆ†æåŠåŸºäºåœºæ™¯å†³ç­–ç­‰é«˜çº§è§¦è§‰æ¨ç†èƒ½åŠ›ã€‚å®éªŒè¯„ä¼°è¡¨æ˜ï¼ŒVTV-LLMåœ¨è§¦è§‰è§†é¢‘ç†è§£ä»»åŠ¡ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œæ˜¾è‘—æå‡äº†è·¨æ¨¡æ€äº¤äº’çš„æ€§èƒ½ã€‚è¿™ä¸€ç ”ç©¶æˆæœä¸ºè§¦è§‰é¢†åŸŸä¸­æ›´ç›´è§‚ã€æ›´å…·æ„ŸçŸ¥åŠ›çš„äººæœºäº¤äº’å¥ å®šäº†é‡è¦åŸºç¡€ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "13 pages, 5 figures",
      "pdf_url": "https://arxiv.org/pdf/2505.22566v1",
      "published_date": "2025-05-28 16:43:01 UTC",
      "updated_date": "2025-05-28 16:43:01 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T15:19:00.556041+00:00"
    },
    {
      "arxiv_id": "2505.22564v1",
      "title": "PRISM: Video Dataset Condensation with Progressive Refinement and Insertion for Sparse Motion",
      "title_zh": "PRISMï¼šåŸºäºæ¸è¿›å¼ç»†åŒ–ä¸æ’å…¥çš„ç¨€ç–è¿åŠ¨è§†é¢‘æ•°æ®é›†å‡èš",
      "authors": [
        "Jaehyun Choi",
        "Jiwan Hur",
        "Gyojin Han",
        "Jaemyung Yu",
        "Junmo Kim"
      ],
      "abstract": "Video dataset condensation has emerged as a critical technique for addressing the computational challenges associated with large-scale video data processing in deep learning applications. While significant progress has been made in image dataset condensation, the video domain presents unique challenges due to the complex interplay between spatial content and temporal dynamics. This paper introduces PRISM, Progressive Refinement and Insertion for Sparse Motion, for video dataset condensation, a novel approach that fundamentally reconsiders how video data should be condensed. Unlike the previous method that separates static content from dynamic motion, our method preserves the essential interdependence between these elements. Our approach progressively refines and inserts frames to fully accommodate the motion in an action while achieving better performance but less storage, considering the relation of gradients for each frame. Extensive experiments across standard video action recognition benchmarks demonstrate that PRISM outperforms existing disentangled approaches while maintaining compact representations suitable for resource-constrained environments.",
      "tldr_zh": "æœ¬ç ”ç©¶æå‡ºäº† PRISM (Progressive Refinement and Insertion for Sparse Motion)ï¼Œè¿™æ˜¯ä¸€ç§é’ˆå¯¹è§†é¢‘æ•°æ®é›†å‹ç¼© (Video dataset condensation) çš„åˆ›æ–°æ–¹æ³•ï¼Œæ—¨åœ¨è§£å†³å¤§è§„æ¨¡è§†é¢‘æ•°æ®å¤„ç†ä¸­çš„è®¡ç®—ç“¶é¢ˆã€‚PRISM é‡æ–°å®¡è§†äº†è§†é¢‘æ•°æ®çš„å‹ç¼©æ–¹å¼ï¼Œä¸åŒäºä»¥å¾€å°†é™æ€å†…å®¹ä¸åŠ¨æ€è¿åŠ¨åˆ†ç¦»çš„è§£è€¦æ–¹æ³•ï¼Œå®ƒå¼ºè°ƒä¿ç•™ç©ºé—´å†…å®¹ä¸æ—¶é—´åŠ¨æ€ä¹‹é—´çš„æ ¸å¿ƒç›¸äº’ä¾èµ–æ€§ã€‚è¯¥æ–¹æ³•é€šè¿‡åˆ†æå„å¸§æ¢¯åº¦çš„å…³ç³»ï¼Œé‡‡å–é€æ­¥ç²¾ç‚¼å’Œæ’å…¥è§†é¢‘å¸§çš„ç­–ç•¥ï¼Œä»¥å……åˆ†æ•æ‰åŠ¨ä½œä¸­çš„è¿åŠ¨ç‰¹å¾ã€‚åœ¨æ ‡å‡†è§†é¢‘åŠ¨ä½œè¯†åˆ« (Video action recognition) åŸºå‡†æµ‹è¯•ä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼ŒPRISM åœ¨æ€§èƒ½ä¸Šæ˜¾è‘—ä¼˜äºç°æœ‰çš„è§£è€¦å¼æ–¹æ³•ã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•åœ¨æä¾›æ›´ä¼˜æ€§èƒ½çš„åŒæ—¶æœ‰æ•ˆé™ä½äº†å­˜å‚¨éœ€æ±‚ï¼Œä¸ºèµ„æºå—é™ç¯å¢ƒä¸‹çš„æ·±åº¦å­¦ä¹ åº”ç”¨æä¾›äº†æ›´ä¸ºç´§å‡‘ä¸”é«˜æ•ˆçš„è§†é¢‘è¡¨ç¤ºæ–¹æ¡ˆã€‚",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.22564v1",
      "published_date": "2025-05-28 16:42:10 UTC",
      "updated_date": "2025-05-28 16:42:10 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T15:20:47.002247+00:00"
    },
    {
      "arxiv_id": "2505.22552v1",
      "title": "ClaimPKG: Enhancing Claim Verification via Pseudo-Subgraph Generation with Lightweight Specialized LLM",
      "title_zh": "ClaimPKGï¼šåˆ©ç”¨è½»é‡çº§ä¸“ç”¨ LLM ç”Ÿæˆä¼ªå­å›¾ä»¥å¢å¼ºäº‹å®æ ¸æŸ¥",
      "authors": [
        "Hoang Pham",
        "Thanh-Do Nguyen",
        "Khac-Hoai Nam Bui"
      ],
      "abstract": "Integrating knowledge graphs (KGs) to enhance the reasoning capabilities of large language models (LLMs) is an emerging research challenge in claim verification. While KGs provide structured, semantically rich representations well-suited for reasoning, most existing verification methods rely on unstructured text corpora, limiting their ability to effectively leverage KGs. Additionally, despite possessing strong reasoning abilities, modern LLMs struggle with multi-step modular pipelines and reasoning over KGs without adaptation. To address these challenges, we propose ClaimPKG, an end-to-end framework that seamlessly integrates LLM reasoning with structured knowledge from KGs. Specifically, the main idea of ClaimPKG is to employ a lightweight, specialized LLM to represent the input claim as pseudo-subgraphs, guiding a dedicated subgraph retrieval module to identify relevant KG subgraphs. These retrieved subgraphs are then processed by a general-purpose LLM to produce the final verdict and justification. Extensive experiments on the FactKG dataset demonstrate that ClaimPKG achieves state-of-the-art performance, outperforming strong baselines in this research field by 9%-12% accuracy points across multiple categories. Furthermore, ClaimPKG exhibits zero-shot generalizability to unstructured datasets such as HoVer and FEVEROUS, effectively combining structured knowledge from KGs with LLM reasoning across various LLM backbones.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ClaimPKGï¼Œè¿™æ˜¯ä¸€ç§æ—¨åœ¨å¢å¼ºclaim verificationèƒ½åŠ›çš„ç«¯åˆ°ç«¯æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³å¤§å‹è¯­è¨€æ¨¡å‹(LLMs)éš¾ä»¥æœ‰æ•ˆåˆ©ç”¨çŸ¥è¯†å›¾è°±(KGs)è¿›è¡Œå¤æ‚æ¨ç†çš„é—®é¢˜ã€‚ClaimPKGçš„æ ¸å¿ƒæ€æƒ³æ˜¯é‡‡ç”¨è½»é‡çº§çš„ä¸“é—¨åŒ–LLMå°†è¾“å…¥çš„claimè¡¨ç¤ºä¸ºpseudo-subgraphsï¼Œä»¥æ­¤å¼•å¯¼ä¸“é—¨çš„å­å›¾æ£€ç´¢æ¨¡å—ä»KGsä¸­è¯†åˆ«ç›¸å…³ä¿¡æ¯ã€‚éšåï¼Œé€šç”¨LLMåˆ©ç”¨è¿™äº›æ£€ç´¢åˆ°çš„å­å›¾ç”Ÿæˆæœ€ç»ˆçš„åˆ¤å®šç»“æœå’Œç†ç”±ã€‚åœ¨FactKGæ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒClaimPKGçš„å‡†ç¡®ç‡æ¯”ç°æœ‰åŸºçº¿æ¨¡å‹æé«˜äº†9%-12%ï¼Œè¾¾åˆ°äº†state-of-the-artæ°´å¹³ã€‚æ­¤å¤–ï¼Œè¯¥æ¡†æ¶åœ¨HoVerå’ŒFEVEROUSç­‰æ•°æ®é›†ä¸Šå±•ç°äº†å‡ºè‰²çš„zero-shot generalizabilityï¼Œè¯æ˜äº†å…¶åœ¨ç»“åˆç»“æ„åŒ–çŸ¥è¯†ä¸LLMæ¨ç†æ–¹é¢çš„ä¼˜è¶Šæ€§å’Œé²æ£’æ€§ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.DB"
      ],
      "primary_category": "cs.CL",
      "comment": "Accepted by ACL 2025 findings",
      "pdf_url": "https://arxiv.org/pdf/2505.22552v1",
      "published_date": "2025-05-28 16:34:14 UTC",
      "updated_date": "2025-05-28 16:34:14 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T15:19:49.235170+00:00"
    },
    {
      "arxiv_id": "2505.22543v1",
      "title": "Scaling-up Perceptual Video Quality Assessment",
      "title_zh": "è§„æ¨¡åŒ–æ„ŸçŸ¥è§†é¢‘è´¨é‡è¯„ä»·",
      "authors": [
        "Ziheng Jia",
        "Zicheng Zhang",
        "Zeyu Zhang",
        "Yingji Liang",
        "Xiaorong Zhu",
        "Chunyi Li",
        "Jinliang Han",
        "Haoning Wu",
        "Bin Wang",
        "Haoran Zhang",
        "Guanyu Zhu",
        "Qiyong Zhao",
        "Xiaohong Liu",
        "Guangtao Zhai",
        "Xiongkuo Min"
      ],
      "abstract": "The data scaling law has been shown to significantly enhance the performance of large multi-modal models (LMMs) across various downstream tasks. However, in the domain of perceptual video quality assessment (VQA), the potential of scaling law remains unprecedented due to the scarcity of labeled resources and the insufficient scale of datasets. To address this, we propose \\textbf{OmniVQA}, an efficient framework designed to efficiently build high-quality, human-in-the-loop VQA multi-modal instruction databases (MIDBs). We then scale up to create \\textbf{OmniVQA-Chat-400K}, the largest MIDB in the VQA field concurrently. Our focus is on the technical and aesthetic quality dimensions, with abundant in-context instruction data to provide fine-grained VQA knowledge. Additionally, we have built the \\textbf{OmniVQA-MOS-20K} dataset to enhance the model's quantitative quality rating capabilities. We then introduce a \\textbf{complementary} training strategy that effectively leverages the knowledge from datasets for quality understanding and quality rating tasks. Furthermore, we propose the \\textbf{OmniVQA-FG (fine-grain)-Benchmark} to evaluate the fine-grained performance of the models. Our results demonstrate that our models achieve state-of-the-art performance in both quality understanding and rating tasks.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹æ„ŸçŸ¥è§†é¢‘è´¨é‡è¯„ä¼°(VQA)é¢†åŸŸæ ‡æ³¨æ•°æ®ç¨€ç¼ºçš„é—®é¢˜ï¼Œæå‡ºäº†æ—¨åœ¨é«˜æ•ˆæ„å»ºé«˜è´¨é‡å¤šæ¨¡æ€æŒ‡ä»¤æ•°æ®åº“(MIDB)çš„OmniVQAæ¡†æ¶ã€‚åˆ©ç”¨è¯¥æ¡†æ¶ï¼Œç ”ç©¶è€…æ„å»ºäº†åŒ…å«40ä¸‡æ¡æ•°æ®çš„OmniVQA-Chat-400Kæ•°æ®é›†ï¼Œè¿™æ˜¯ç›®å‰VQAé¢†åŸŸè§„æ¨¡æœ€å¤§çš„å¤šæ¨¡æ€æŒ‡ä»¤æ•°æ®åº“ï¼Œæ¶µç›–äº†ä¸°å¯Œçš„æŠ€æœ¯å’Œå®¡ç¾è´¨é‡ç»´åº¦ã€‚ç ”ç©¶åŒæ—¶æ¨å‡ºäº†OmniVQA-MOS-20Kæ•°æ®é›†ä»¥æå‡æ¨¡å‹çš„å®šé‡è¯„åˆ†èƒ½åŠ›ï¼Œå¹¶é‡‡ç”¨äº’è¡¥è®­ç»ƒç­–ç•¥æœ‰æ•ˆèåˆäº†è´¨é‡ç†è§£ä¸è´¨é‡è¯„åˆ†çŸ¥è¯†ã€‚æ­¤å¤–ï¼Œç ”ç©¶è¿˜æå‡ºäº†OmniVQA-FG-Benchmarkç”¨ä»¥è¯„ä¼°æ¨¡å‹çš„ç»†ç²’åº¦è¡¨ç°ã€‚å®éªŒç»“æœè¯æ˜ï¼ŒOmniVQAåœ¨è´¨é‡ç†è§£å’Œè¯„åˆ†ä»»åŠ¡ä¸Šå‡å–å¾—äº†å½“å‰æœ€å…ˆè¿›(state-of-the-art)çš„æ€§èƒ½è¡¨ç°ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.22543v1",
      "published_date": "2025-05-28 16:24:52 UTC",
      "updated_date": "2025-05-28 16:24:52 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T15:19:25.211255+00:00"
    },
    {
      "arxiv_id": "2505.22533v1",
      "title": "TabularQGAN: A Quantum Generative Model for Tabular Data",
      "title_zh": "TabularQGANï¼šé’ˆå¯¹è¡¨æ ¼æ•°æ®çš„é‡å­ç”Ÿæˆæ¨¡å‹",
      "authors": [
        "Pallavi Bhardwaj",
        "Caitlin Jones",
        "Lasse Dierich",
        "Aleksandar VuÄkoviÄ‡"
      ],
      "abstract": "In this paper, we introduce a novel quantum generative model for synthesizing tabular data. Synthetic data is valuable in scenarios where real-world data is scarce or private, it can be used to augment or replace existing datasets. Real-world enterprise data is predominantly tabular and heterogeneous, often comprising a mixture of categorical and numerical features, making it highly relevant across various industries such as healthcare, finance, and software. We propose a quantum generative adversarial network architecture with flexible data encoding and a novel quantum circuit ansatz to effectively model tabular data. The proposed approach is tested on the MIMIC III healthcare and Adult Census datasets, with extensive benchmarking against leading classical models, CTGAN, and CopulaGAN. Experimental results demonstrate that our quantum model outperforms classical models by an average of 8.5% with respect to an overall similarity score from SDMetrics, while using only 0.072% of the parameters of the classical models. Additionally, we evaluate the generalization capabilities of the models using two custom-designed metrics that demonstrate the ability of the proposed quantum model to generate useful and novel samples. To our knowledge, this is one of the first demonstrations of a successful quantum generative model for handling tabular data, indicating that this task could be well-suited to quantum computers.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† TabularQGANï¼Œä¸€ç§ç”¨äºåˆæˆå¼‚æ„è¡¨æ ¼æ•°æ® (tabular data) çš„æ–°å‹é‡å­ç”Ÿæˆæ¨¡å‹ï¼Œæ—¨åœ¨è§£å†³åŒ»ç–—å’Œé‡‘èç­‰é¢†åŸŸçœŸå®æ•°æ®ç¨€ç¼ºæˆ–éšç§å—é™çš„é—®é¢˜ã€‚è¯¥æ¶æ„é‡‡ç”¨é‡å­ç”Ÿæˆå¯¹æŠ—ç½‘ç»œ (QGAN) å½¢å¼ï¼Œé€šè¿‡çµæ´»çš„æ•°æ®ç¼–ç å’Œåˆ›æ–°çš„é‡å­ç”µè·¯ ansatz (quantum circuit ansatz) æ¥æœ‰æ•ˆå»ºæ¨¡åŒ…å«ç±»åˆ«å’Œæ•°å€¼ç‰¹å¾çš„å¤æ‚æ•°æ®ã€‚åœ¨ MIMIC III å’Œ Adult Census æ•°æ®é›†ä¸Šçš„å®éªŒç»“æœæ˜¾ç¤ºï¼ŒTabularQGAN åœ¨ SDMetrics ç›¸ä¼¼åº¦è¯„åˆ†ä¸Šæ¯”ç»å…¸æ¨¡å‹ CTGAN å’Œ CopulaGAN å¹³å‡æé«˜ 8.5%ã€‚ä»¤äººå…³æ³¨çš„æ˜¯ï¼Œè¯¥æ¨¡å‹ä»…ä½¿ç”¨äº†ç»å…¸æ¨¡å‹ 0.072% çš„å‚æ•°é‡ï¼ŒåŒæ—¶é€šè¿‡è‡ªå®šä¹‰æŒ‡æ ‡éªŒè¯äº†å…¶å‡ºè‰²çš„æ³›åŒ–èƒ½åŠ›å’Œç”Ÿæˆæ–°é¢–æ ·æœ¬çš„èƒ½åŠ›ã€‚ä½œä¸ºé¦–æ‰¹æˆåŠŸå¤„ç†è¡¨æ ¼æ•°æ®çš„é‡å­ç”Ÿæˆæ¨¡å‹ä¹‹ä¸€ï¼Œè¯¥ç ”ç©¶è¯æ˜äº†é‡å­è®¡ç®—åœ¨å¤„ç†æ­¤ç±»å·¥ä¸šçº§æ•°æ®ä»»åŠ¡ä¸­çš„æ½œåŠ›ä¸ä¼˜åŠ¿ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "quant-ph"
      ],
      "primary_category": "cs.LG",
      "comment": "18 pages,8 figures and 4 tables",
      "pdf_url": "https://arxiv.org/pdf/2505.22533v1",
      "published_date": "2025-05-28 16:19:39 UTC",
      "updated_date": "2025-05-28 16:19:39 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T15:19:36.144283+00:00"
    },
    {
      "arxiv_id": "2505.22531v2",
      "title": "Training RL Agents for Multi-Objective Network Defense Tasks",
      "title_zh": "é¢å‘å¤šç›®æ ‡ç½‘ç»œé˜²å¾¡ä»»åŠ¡çš„å¼ºåŒ–å­¦ä¹ æ™ºèƒ½ä½“è®­ç»ƒ",
      "authors": [
        "Andres Molina-Markham",
        "Luis Robaina",
        "Sean Steinle",
        "Akash Trivedi",
        "Derek Tsui",
        "Nicholas Potteiger",
        "Lauren Brandt",
        "Ransom Winder",
        "Ahmad Ridley"
      ],
      "abstract": "Open-ended learning (OEL) -- which emphasizes training agents that achieve broad capability over narrow competency -- is emerging as a paradigm to develop artificial intelligence (AI) agents to achieve robustness and generalization. However, despite promising results that demonstrate the benefits of OEL, applying OEL to develop autonomous agents for real-world cybersecurity applications remains a challenge.\n  We propose a training approach, inspired by OEL, to develop autonomous network defenders. Our results demonstrate that like in other domains, OEL principles can translate into more robust and generalizable agents for cyber defense. To apply OEL to network defense, it is necessary to address several technical challenges. Most importantly, it is critical to provide a task representation approach over a broad universe of tasks that maintains a consistent interface over goals, rewards and action spaces. This way, the learning agent can train with varying network conditions, attacker behaviors, and defender goals while being able to build on previously gained knowledge.\n  With our tools and results, we aim to fundamentally impact research that applies AI to solve cybersecurity problems. Specifically, as researchers develop gyms and benchmarks for cyber defense, it is paramount that they consider diverse tasks with consistent representations, such as those we propose in our work.",
      "tldr_zh": "æœ¬ç ”ç©¶æ¢è®¨äº†å¼€æ”¾å¼å­¦ä¹  (Open-ended learning, OEL) åœ¨ç½‘ç»œå®‰å…¨é¢†åŸŸçš„åº”ç”¨ï¼Œæ—¨åœ¨é€šè¿‡å¼ºåŒ–å­¦ä¹  (RL) è®­ç»ƒå‡ºå…·å¤‡é«˜é²æ£’æ€§å’Œæ³›åŒ–èƒ½åŠ›çš„è‡ªä¸»ç½‘ç»œé˜²å¾¡æ™ºèƒ½ä½“ã€‚å— OEL èŒƒå¼å¯å‘ï¼Œä½œè€…æå‡ºäº†ä¸€ç§é’ˆå¯¹å¤šç›®æ ‡ç½‘ç»œé˜²å¾¡ä»»åŠ¡çš„è®­ç»ƒ approachï¼Œé‡ç‚¹è§£å†³æ™ºèƒ½ä½“ä»ç‹­éš˜èƒ½åŠ›å‘å¹¿æ³›èƒ½åŠ›è·¨è¶Šçš„éš¾é¢˜ã€‚è¯¥æ–¹æ³•çš„æ ¸å¿ƒè´¡çŒ®åœ¨äºå®ç°äº†ä¸€ç§è·¨è¶Šå¹¿é˜”ä»»åŠ¡å®‡å®™çš„ç»Ÿä¸€ä»»åŠ¡è¡¨ç¤ºï¼Œç¡®ä¿åœ¨ä¸åŒçš„ç½‘ç»œç¯å¢ƒã€æ”»å‡»è€…è¡Œä¸ºå’Œé˜²å¾¡ç›®æ ‡ä¸‹ï¼Œæ™ºèƒ½ä½“çš„ç›®æ ‡ã€å¥–åŠ± (rewards) å’ŒåŠ¨ä½œç©ºé—´ (action spaces) æ¥å£ä¿æŒä¸€è‡´ã€‚è¿™ç§ä¸€è‡´æ€§è®¾è®¡ä½¿å¾—æ™ºèƒ½ä½“èƒ½å¤ŸæŒç»­ç§¯ç´¯å¹¶åˆ©ç”¨å…ˆéªŒçŸ¥è¯†ï¼Œä»è€Œæœ‰æ•ˆåº”å¯¹å¤æ‚çš„é˜²å¾¡æŒ‘æˆ˜ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œå°† OEL åŸåˆ™åº”ç”¨äºç½‘ç»œé˜²å¾¡èƒ½æ˜¾è‘—æå‡æ™ºèƒ½ä½“çš„æ€§èƒ½è¡¨ç°ã€‚è¯¥ç ”ç©¶ä¸ºæœªæ¥ç½‘ç»œå®‰å…¨ gym å’Œ benchmark çš„æ„å»ºæä¾›äº†å…³é”®æ€è·¯ï¼Œå¼ºè°ƒäº†åœ¨å¼€å‘äººå·¥æ™ºèƒ½å®‰å…¨ç³»ç»Ÿæ—¶å¿…é¡»è€ƒè™‘ä»»åŠ¡å¤šæ ·æ€§ä¸è¡¨ç¤ºä¸€è‡´æ€§çš„ç»“åˆã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CR"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.22531v2",
      "published_date": "2025-05-28 16:18:21 UTC",
      "updated_date": "2025-06-13 14:59:23 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T15:20:50.442950+00:00"
    },
    {
      "arxiv_id": "2505.22525v1",
      "title": "Thinking with Generated Images",
      "title_zh": "åŸºäºç”Ÿæˆå›¾åƒçš„æ€ç»´",
      "authors": [
        "Ethan Chern",
        "Zhulin Hu",
        "Steffi Chern",
        "Siqi Kou",
        "Jiadi Su",
        "Yan Ma",
        "Zhijie Deng",
        "Pengfei Liu"
      ],
      "abstract": "We present Thinking with Generated Images, a novel paradigm that fundamentally transforms how large multimodal models (LMMs) engage with visual reasoning by enabling them to natively think across text and vision modalities through spontaneous generation of intermediate visual thinking steps. Current visual reasoning with LMMs is constrained to either processing fixed user-provided images or reasoning solely through text-based chain-of-thought (CoT). Thinking with Generated Images unlocks a new dimension of cognitive capability where models can actively construct intermediate visual thoughts, critique their own visual hypotheses, and refine them as integral components of their reasoning process. We demonstrate the effectiveness of our approach through two complementary mechanisms: (1) vision generation with intermediate visual subgoals, where models decompose complex visual tasks into manageable components that are generated and integrated progressively, and (2) vision generation with self-critique, where models generate an initial visual hypothesis, analyze its shortcomings through textual reasoning, and produce refined outputs based on their own critiques. Our experiments on vision generation benchmarks show substantial improvements over baseline approaches, with our models achieving up to 50% (from 38% to 57%) relative improvement in handling complex multi-object scenarios. From biochemists exploring novel protein structures, and architects iterating on spatial designs, to forensic analysts reconstructing crime scenes, and basketball players envisioning strategic plays, our approach enables AI models to engage in the kind of visual imagination and iterative refinement that characterizes human creative, analytical, and strategic thinking. We release our open-source suite at https://github.com/GAIR-NLP/thinking-with-generated-images.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†Thinking with Generated Imagesï¼Œè¿™æ˜¯ä¸€ç§å…è®¸å¤§å‹å¤šæ¨¡æ€æ¨¡å‹(LMMs)é€šè¿‡è‡ªå‘ç”Ÿæˆä¸­é—´è§†è§‰æ€ç»´æ­¥éª¤æ¥å®ç°è·¨æ¨¡æ€åŸç”Ÿæ€è€ƒçš„æ–°å‹èŒƒå¼ã€‚è¯¥èŒƒå¼çªç ´äº†ç°æœ‰æ¨¡å‹ä»…èƒ½å¤„ç†å›ºå®šå›¾åƒæˆ–çº¯æ–‡æœ¬é“¾å¼æ€ç»´(Chain-of-Thought)çš„å±€é™ï¼Œä½¿æ¨¡å‹å…·å¤‡äº†ä¸»åŠ¨æ„å»ºè§†è§‰æ€ç»´ã€æ‰¹åˆ¤è§†è§‰å‡è®¾å¹¶è¿›è¡Œè¿­ä»£ç»†åŒ–çš„è®¤çŸ¥èƒ½åŠ›ã€‚ç ”ç©¶é‡‡ç”¨äº†ä¸­é—´è§†è§‰å­ç›®æ ‡ç”Ÿæˆ(intermediate visual subgoals)å’Œå¸¦æœ‰è‡ªæˆ‘æ‰¹åˆ¤(self-critique)çš„è§†è§‰ç”Ÿæˆä¸¤ç§æ ¸å¿ƒæœºåˆ¶ï¼Œå°†å¤æ‚è§†è§‰ä»»åŠ¡åˆ†è§£å¹¶é€æ­¥å®Œå–„ã€‚å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨è§†è§‰ç”ŸæˆåŸºå‡†æµ‹è¯•ä¸­æ˜¾è‘—ä¼˜äºåŸºçº¿ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤æ‚å¤šå¯¹è±¡åœºæ™¯ä¸‹å®ç°äº†é«˜è¾¾50%çš„ç›¸å¯¹æ€§èƒ½æå‡ã€‚è¯¥æ–¹æ³•æˆåŠŸæ¨¡æ‹Ÿäº†äººç±»åœ¨åˆ›æ„ã€åˆ†æåŠæˆ˜ç•¥æ€ç»´ä¸­çš„è§†è§‰æƒ³è±¡è¿‡ç¨‹ï¼Œä¸ºAIåœ¨ç”Ÿç‰©åŒ–å­¦ã€å»ºç­‘è®¾è®¡åŠæ³•åŒ»åˆ†æç­‰é¢†åŸŸçš„åº”ç”¨æä¾›äº†æ›´å¼ºçš„æ¨ç†æ”¯æ’‘ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.22525v1",
      "published_date": "2025-05-28 16:12:45 UTC",
      "updated_date": "2025-05-28 16:12:45 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T15:19:56.808307+00:00"
    },
    {
      "arxiv_id": "2505.22521v2",
      "title": "Evaluating Supervised Learning Models for Fraud Detection: A Comparative Study of Classical and Deep Architectures on Imbalanced Transaction Data",
      "title_zh": "æ¬ºè¯ˆæ£€æµ‹ç›‘ç£å­¦ä¹ æ¨¡å‹è¯„ä¼°ï¼šåŸºäºä¸å¹³è¡¡äº¤æ˜“æ•°æ®çš„ä¼ ç»Ÿä¸æ·±åº¦æ¶æ„æ¯”è¾ƒç ”ç©¶",
      "authors": [
        "Chao Wang",
        "Chuanhao Nie",
        "Yunbo Liu"
      ],
      "abstract": "Fraud detection remains a critical task in high-stakes domains such as finance and e-commerce, where undetected fraudulent transactions can lead to significant economic losses. In this study, we systematically compare the performance of four supervised learning models - Logistic Regression, Random Forest, Light Gradient Boosting Machine (LightGBM), and a Gated Recurrent Unit (GRU) network - on a large-scale, highly imbalanced online transaction dataset. While ensemble methods such as Random Forest and LightGBM demonstrated superior performance in both overall and class-specific metrics, Logistic Regression offered a reliable and interpretable baseline. The GRU model showed strong recall for the minority fraud class, though at the cost of precision, highlighting a trade-off relevant for real-world deployment. Our evaluation emphasizes not only weighted averages but also per-class precision, recall, and F1-scores, providing a nuanced view of each model's effectiveness in detecting rare but consequential fraudulent activity. The findings underscore the importance of choosing models based on the specific risk tolerance and operational needs of fraud detection systems.",
      "tldr_zh": "æœ¬ç ”ç©¶å¯¹é‡‘èå’Œç”µå­å•†åŠ¡é¢†åŸŸçš„æ¬ºè¯ˆæ£€æµ‹æ¨¡å‹è¿›è¡Œäº†ç³»ç»Ÿæ€§æ¯”è¾ƒï¼Œè¯„ä¼°äº†ç»å…¸æ¶æ„ä¸æ·±åº¦å­¦ä¹ æ¶æ„åœ¨é«˜åº¦ä¸å¹³è¡¡äº¤æ˜“æ•°æ®ä¸Šçš„è¡¨ç°ã€‚ç ”ç©¶å¯¹æ¯”äº† Logistic Regressionã€Random Forestã€Light Gradient Boosting Machine (LightGBM) ä»¥åŠ Gated Recurrent Unit (GRU) ç½‘ç»œå››ç§ç›‘ç£å­¦ä¹ æ¨¡å‹ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒRandom Forest å’Œ LightGBM ç­‰é›†æˆå­¦ä¹ æ–¹æ³•åœ¨æ•´ä½“æ€§èƒ½å’Œç‰¹å®šç±»åˆ«æŒ‡æ ‡ä¸Šè¡¨ç°æœ€ä¸ºä¼˜è¶Šï¼Œè€Œ Logistic Regression åˆ™æä¾›äº†å¯é ä¸”å…·å¯è§£é‡Šæ€§çš„åŸºå‡†ã€‚å°½ç®¡ GRU æ¨¡å‹åœ¨æ¬ºè¯ˆç±»åˆ«çš„ Recall æŒ‡æ ‡ä¸Šè¡¨ç°å¼ºåŠ²ï¼Œä½†å…¶ Precision è¾ƒä½ï¼Œåæ˜ äº†å®é™…éƒ¨ç½²ä¸­æ€§èƒ½æƒè¡¡çš„æŒ‘æˆ˜ã€‚è¯¥è¯„ä»·ä½“ç³»ä¸ä»…å…³æ³¨åŠ æƒå¹³å‡å€¼ï¼Œè¿˜æ·±å…¥åˆ†æäº†å„åˆ†ç±»çš„ Precisionã€Recall å’Œ F1-scoreï¼Œä¸ºæ•æ‰ç½•è§ä½†é«˜å½±å“çš„æ¬ºè¯ˆè¡Œä¸ºæä¾›äº†ç»†è‡´è§†è§’ã€‚æœ€ç»ˆç ”ç©¶å¼ºè°ƒï¼Œæ¬ºè¯ˆæ£€æµ‹ç³»ç»Ÿçš„æ¨¡å‹é€‰æ‹©åº”å–å†³äºå…·ä½“çš„é£é™©æ‰¿å—èƒ½åŠ›å’Œå®é™…è¿è¥éœ€æ±‚ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "5 pages. Chao Wang, Chuanhao Nie, and Yunbo Liu contributed equally to this work. Corresponding author: Yunbo Liu (yunbo.liu954@duke.edu). Submitted to the 3rd International Conference on Management Innovation and Economy Development (MIED 2025), Chongqing, China",
      "pdf_url": "https://arxiv.org/pdf/2505.22521v2",
      "published_date": "2025-05-28 16:08:04 UTC",
      "updated_date": "2025-09-17 23:06:02 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T15:21:06.019435+00:00"
    },
    {
      "arxiv_id": "2505.22513v1",
      "title": "Strengthening Proportionality in Temporal Voting",
      "title_zh": "å¼ºåŒ–æ—¶åºæŠ•ç¥¨ä¸­çš„æ¯”ä¾‹æ€§",
      "authors": [
        "Bradley Phillips",
        "Edith Elkind",
        "Nicholas Teh",
        "Tomasz WÄ…s"
      ],
      "abstract": "We study proportional representation in the framework of temporal voting with approval ballots. Prior work adapted basic proportional representation concepts -- justified representation (JR), proportional JR (PJR), and extended JR (EJR) -- from the multiwinner setting to the temporal setting. Our work introduces and examines ways of going beyond EJR. Specifically, we consider stronger variants of JR, PJR, and EJR, and introduce temporal adaptations of more demanding multiwinner axioms, such as EJR+, full JR (FJR), full proportional JR (FPJR), and the Core. For each of these concepts, we investigate its existence and study its relationship to existing notions, thereby establishing a rich hierarchy of proportionality concepts. Notably, we show that two of our proposed axioms -- EJR+ and FJR -- strengthen EJR while remaining satisfiable in every temporal election.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†åŸºäº approval ballots çš„ temporal voting æ¡†æ¶ä¸‹çš„æ¯”ä¾‹ä»£è¡¨æ€§é—®é¢˜ï¼Œæ—¨åœ¨æå‡ç°æœ‰æ¯”ä¾‹æ€§å…¬ç†çš„ä¿éšœæ°´å¹³ã€‚ä½œè€…åœ¨å·²æœ‰çš„ justified representation (JR)ã€proportional JR (PJR) å’Œ extended JR (EJR) åŸºç¡€ä¸Šï¼Œå¼•å…¥äº†æ›´å…·æŒ‘æˆ˜æ€§çš„æ—¶é—´é€‚åº”æ€§å…¬ç†ï¼ŒåŒ…æ‹¬ EJR+ã€full JR (FJR)ã€full proportional JR (FPJR) å’Œ Coreã€‚é€šè¿‡ç³»ç»Ÿæ€§åœ°ç ”ç©¶è¿™äº›æ¦‚å¿µçš„å­˜åœ¨æ€§åŠå…¶ç›¸äº’é€»è¾‘å…³ç³»ï¼Œè¯¥è®ºæ–‡å»ºç«‹äº†ä¸€ä¸ªä¸¥å¯†ä¸”ä¸°å¯Œçš„æ¯”ä¾‹æ€§æ¦‚å¿µå±‚çº§ä½“ç³» (hierarchy of proportionality concepts)ã€‚ç ”ç©¶ç»“æœç‰¹åˆ«æŒ‡å‡ºï¼ŒEJR+ å’Œ FJR ä¸¤ä¸ªå…¬ç†åœ¨æ˜¾è‘—å¢å¼º EJR çº¦æŸåŠ›çš„åŒæ—¶ï¼Œåœ¨æ‰€æœ‰æ—¶é—´é€‰ä¸¾åœºæ™¯ä¸­å‡ä¿æŒå¯æ»¡è¶³æ€§ (satisfiable)ã€‚è¯¥é¡¹å·¥ä½œä¸ºå®ç°æ—¶é—´æŠ•ç¥¨ä¸­æ›´é«˜æ°´å¹³çš„å…¬å¹³åˆ†é…æä¾›äº†åšå®çš„ç†è®ºæ”¯æ’‘ã€‚",
      "categories": [
        "cs.GT",
        "cs.AI"
      ],
      "primary_category": "cs.GT",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.22513v1",
      "published_date": "2025-05-28 16:02:52 UTC",
      "updated_date": "2025-05-28 16:02:52 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T15:20:28.107149+00:00"
    },
    {
      "arxiv_id": "2505.22503v2",
      "title": "Communication-Efficient Desire Alignment for Embodied Agent-Human Adaptation",
      "title_zh": "é¢å‘å…·èº«æ™ºèƒ½ä½“-äººç±»é€‚é…çš„é«˜æ•ˆé€šä¿¡æ„æ„¿å¯¹é½",
      "authors": [
        "Yuanfei Wang",
        "Xinju Huang",
        "Fangwei Zhong",
        "Yaodong Yang",
        "Yizhou Wang",
        "Yuanpei Chen",
        "Hao Dong"
      ],
      "abstract": "While embodied agents have made significant progress in performing complex physical tasks, real-world applications demand more than pure task execution. The agents must collaborate with unfamiliar agents and human users, whose goals are often vague and implicit. In such settings, interpreting ambiguous instructions and uncovering underlying desires is essential for effective assistance. Therefore, fast and accurate desire alignment becomes a critical capability for embodied agents. In this work, we first develop a home assistance simulation environment HA-Desire that integrates an LLM-driven proxy human user exhibiting realistic value-driven goal selection and communication. The ego agent must interact with this proxy user to infer and adapt to the user's latent desires. To achieve this, we present a novel framework FAMER for fast desire alignment, which introduces a desire-based mental reasoning mechanism to identify user intent and filter desire-irrelevant actions. We further design a reflection-based communication module that reduces redundant inquiries, and incorporate goal-relevant information extraction with memory persistence to improve information reuse and reduce unnecessary exploration. Extensive experiments demonstrate that our framework significantly enhances both task execution and communication efficiency, enabling embodied agents to quickly adapt to user-specific desires in complex embodied environments.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å…·èº«æ™ºèƒ½ä½“(Embodied agents)åœ¨ä¸ç›®æ ‡æ¨¡ç³Šçš„äººç±»ç”¨æˆ·åä½œæ—¶ï¼Œå¦‚ä½•å®ç°å¿«é€Ÿå‡†ç¡®çš„æ„¿æœ›å¯¹é½(Desire alignment)è¿™ä¸€å…³é”®æŒ‘æˆ˜å±•å¼€ã€‚ç ”ç©¶å›¢é˜Ÿé¦–å…ˆå¼€å‘äº†HA-Desireæ¨¡æ‹Ÿç¯å¢ƒï¼Œåˆ©ç”¨å¤§è¯­è¨€æ¨¡å‹(LLM)é©±åŠ¨çš„ä»£ç†ç”¨æˆ·æ¥æ¨¡æ‹ŸçœŸå®çš„ä»·å€¼é©±åŠ¨ç›®æ ‡é€‰æ‹©ä¸äº¤æµã€‚åœ¨æ­¤åŸºç¡€ä¸Šï¼Œç ”ç©¶æå‡ºäº†FAMERæ¡†æ¶ï¼Œé€šè¿‡å¼•å…¥åŸºäºæ„¿æœ›çš„å¿ƒç†æ¨ç†æœºåˆ¶(Desire-based mental reasoning)æ¥ç²¾å‡†è¯†åˆ«ç”¨æˆ·æ„å›¾å¹¶è¿‡æ»¤æ— å…³åŠ¨ä½œã€‚æ­¤å¤–ï¼Œè¯¥æ¡†æ¶è¿˜åŒ…å«ä¸€ä¸ªåŸºäºåå°„çš„é€šä¿¡æ¨¡å—(Reflection-based communication module)ä»¥å‡å°‘å†—ä½™è¯¢é—®ï¼Œå¹¶ç»“åˆè®°å¿†æŒä¹…æ€§æŠ€æœ¯ä¼˜åŒ–ä¿¡æ¯æå–ä¸å¤ç”¨ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒFAMERåœ¨å¤æ‚ç¯å¢ƒä¸­æ˜¾è‘—æå‡äº†æ™ºèƒ½ä½“çš„ä»»åŠ¡æ‰§è¡ŒæˆåŠŸç‡å’Œé€šä¿¡æ•ˆç‡ï¼Œä½¿å…¶èƒ½å¤Ÿè¿…é€Ÿé€‚åº”å¹¶æ»¡è¶³ç‰¹å®šç”¨æˆ·çš„æ½œåœ¨æ„¿æœ›ã€‚",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.MA"
      ],
      "primary_category": "cs.RO",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.22503v2",
      "published_date": "2025-05-28 15:51:13 UTC",
      "updated_date": "2025-09-27 13:05:48 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T15:21:46.084275+00:00"
    },
    {
      "arxiv_id": "2505.22492v1",
      "title": "Demystifying the Paradox of Importance Sampling with an Estimated History-Dependent Behavior Policy in Off-Policy Evaluation",
      "title_zh": "æ­ç§˜ç¦»çº¿ç­–ç•¥è¯„ä¼°ä¸­åŸºäºä¼°è®¡çš„å†å²ä¾èµ–å‹è¡Œä¸ºç­–ç•¥çš„é‡è¦æ€§é‡‡æ ·æ‚–è®º",
      "authors": [
        "Hongyi Zhou",
        "Josiah P. Hanna",
        "Jin Zhu",
        "Ying Yang",
        "Chengchun Shi"
      ],
      "abstract": "This paper studies off-policy evaluation (OPE) in reinforcement learning with a focus on behavior policy estimation for importance sampling. Prior work has shown empirically that estimating a history-dependent behavior policy can lead to lower mean squared error (MSE) even when the true behavior policy is Markovian. However, the question of why the use of history should lower MSE remains open. In this paper, we theoretically demystify this paradox by deriving a bias-variance decomposition of the MSE of ordinary importance sampling (IS) estimators, demonstrating that history-dependent behavior policy estimation decreases their asymptotic variances while increasing their finite-sample biases. Additionally, as the estimated behavior policy conditions on a longer history, we show a consistent decrease in variance. We extend these findings to a range of other OPE estimators, including the sequential IS estimator, the doubly robust estimator and the marginalized IS estimator, with the behavior policy estimated either parametrically or non-parametrically.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†å¼ºåŒ–å­¦ä¹ ä¸­ç¦»çº¿ç­–ç•¥è¯„ä¼°(Off-Policy Evaluation)çš„é—®é¢˜ï¼Œç‰¹åˆ«æ˜¯é’ˆå¯¹é‡è¦æ€§é‡‡æ ·(Importance Sampling)ä¸­è¡Œä¸ºç­–ç•¥ä¼°è®¡çš„æ‚–è®ºã€‚å°½ç®¡çœŸå®çš„ç­–ç•¥é€šå¸¸æ˜¯Markovianï¼Œä½†ä¼°è®¡ä¸€ä¸ªhistory-dependentçš„è¡Œä¸ºç­–ç•¥å¾€å¾€èƒ½è·å¾—æ›´ä½çš„å‡æ–¹è¯¯å·®(MSE)ã€‚æœ¬æ–‡é€šè¿‡æ¨å¯¼æ™®é€šé‡è¦æ€§é‡‡æ ·(IS)ä¼°è®¡å™¨çš„åå·®-æ–¹å·®åˆ†è§£(bias-variance decomposition)ï¼Œä»ç†è®ºä¸Šæ­ç¤ºäº†è¿™ä¸€ç°è±¡çš„å†…åœ¨æœºåˆ¶ã€‚ç ”ç©¶è¡¨æ˜ï¼Œä¼°è®¡ä¾èµ–å†å²çš„è¡Œä¸ºç­–ç•¥èƒ½å¤Ÿé™ä½æ¸è¿‘æ–¹å·®(asymptotic variance)ï¼Œä½†åŒæ—¶ä¼šå¢åŠ æœ‰é™æ ·æœ¬åå·®(finite-sample bias)ã€‚æ­¤å¤–ï¼Œç ”ç©¶è¯æ˜éšç€ä¼°è®¡ç­–ç•¥æ‰€ä¾èµ–çš„å†å²åºåˆ—å¢é•¿ï¼Œä¼°è®¡å™¨çš„æ–¹å·®ä¼šæŒç»­é™ä½ã€‚è¯¥ç»“è®ºè¿˜è¢«æ‰©å±•åˆ°äº†Sequential ISã€Doubly Robustä»¥åŠMarginalized ISç­‰å¤šç§ä¼°è®¡å™¨ï¼Œæ¶µç›–äº†å‚æ•°åŒ–å’Œéå‚æ•°åŒ–ä¼°è®¡æ–¹æ³•ï¼Œä¸ºè¯¥é¢†åŸŸæä¾›äº†æ·±å…¥çš„ç†è®ºä¾æ®ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "stat.ML"
      ],
      "primary_category": "cs.LG",
      "comment": "Accepted by ICML 2025",
      "pdf_url": "https://arxiv.org/pdf/2505.22492v1",
      "published_date": "2025-05-28 15:42:20 UTC",
      "updated_date": "2025-05-28 15:42:20 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T15:22:00.345220+00:00"
    },
    {
      "arxiv_id": "2505.22491v2",
      "title": "On the Surprising Effectiveness of Large Learning Rates under Standard Width Scaling",
      "title_zh": "è®ºæ ‡å‡†å®½åº¦ç¼©æ”¾ä¸­å¤§å­¦ä¹ ç‡å‡ºäººæ„æ–™çš„æœ‰æ•ˆæ€§",
      "authors": [
        "Moritz Haas",
        "Sebastian Bordt",
        "Ulrike von Luxburg",
        "Leena Chennuru Vankadara"
      ],
      "abstract": "Scaling limits, such as infinite-width limits, serve as promising theoretical tools to study large-scale models. However, it is widely believed that existing infinite-width theory does not faithfully explain the behavior of practical networks, especially those trained in standard parameterization (SP) meaning He initialization with a global learning rate. For instance, existing theory for SP predicts instability at large learning rates and vanishing feature learning at stable ones. In practice, however, optimal learning rates decay slower than theoretically predicted and networks exhibit both stable training and non-trivial feature learning, even at very large widths. Here, we show that this discrepancy is not fully explained by finite-width phenomena.\n  Instead, we find a resolution through a finer-grained analysis of the regime previously considered unstable and therefore uninteresting. In particular, we show that, under cross-entropy (CE) loss, the unstable regime comprises two distinct sub-regimes: a catastrophically unstable regime and a more benign controlled divergence regime, where logits diverge but gradients and activations remain stable. Moreover, under large learning rates at the edge of the controlled divergence regime, there exists a well-defined infinite width limit where features continue to evolve in all the hidden layers. In experiments across optimizers, architectures, and data modalities, we validate that neural networks operate in this controlled divergence regime under CE loss but not under MSE loss. Our empirical evidence suggests that width-scaling considerations are surprisingly useful for predicting empirically maximal stable learning rate exponents which provide useful guidance on optimal learning rate exponents. Finally, our analysis clarifies the effectiveness and limitations of recently proposed layerwise learning rate scaling for standard initialization.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†åœ¨æ ‡å‡†å‚æ•°åŒ–ï¼ˆStandard Parameterization, SPï¼‰ä¸‹ï¼Œå¤§å­¦ä¹ ç‡åœ¨æ¨¡å‹å®½åº¦ç¼©æ”¾è¿‡ç¨‹ä¸­çš„æƒŠäººæœ‰æ•ˆæ€§ã€‚ä½œè€…æŒ‡å‡ºï¼Œç°æœ‰çš„æ— é™å®½åº¦ç†è®ºï¼ˆInfinite-width theoryï¼‰é€šå¸¸é¢„æµ‹ SP åœ¨å¤§å­¦ä¹ ç‡ä¸‹ä¼šè¡¨ç°å‡ºä¸ç¨³å®šæ€§æˆ–ç‰¹å¾å­¦ä¹ æ¶ˆå¤±ï¼Œä½†è¿™ä¸å®é™…ç¥ç»ç½‘ç»œçš„è¡¨ç°å¹¶ä¸ä¸€è‡´ã€‚é€šè¿‡ç»†ç²’åº¦åˆ†æï¼Œç ”ç©¶å‘ç°åœ¨äº¤å‰ç†µï¼ˆCross-Entropy, CEï¼‰æŸå¤±ä¸‹ï¼Œæ‰€è°“çš„â€œä¸ç¨³å®šåŒºåŸŸâ€å®é™…ä¸ŠåŒ…å«ä¸€ä¸ªå—æ§å‘æ•£çŠ¶æ€ï¼ˆControlled Divergence Regimeï¼‰ï¼Œæ­¤æ—¶ Logits è™½ç„¶å‘æ•£ï¼Œä½†æ¢¯åº¦å’Œæ¿€æ´»ä¾ç„¶ä¿æŒç¨³å®šã€‚åœ¨è¯¥çŠ¶æ€è¾¹ç¼˜çš„å¤§å­¦ä¹ ç‡ä¸‹ï¼Œå­˜åœ¨ä¸€ä¸ªç‰¹å¾èƒ½æŒç»­æ¼”åŒ–çš„æ˜ç¡®æ— é™å®½åº¦æé™ã€‚å®éªŒè¯æ˜ï¼Œç¥ç»ç½‘ç»œåœ¨ä½¿ç”¨ CE æŸå¤±è€Œéå‡æ–¹è¯¯å·®ï¼ˆMSEï¼‰æŸå¤±æ—¶è¿è¡Œäºè¯¥å—æ§å‘æ•£çŠ¶æ€ã€‚è¿™ä¸€å‘ç°ä¸ºé¢„æµ‹æœ€å¤§ç¨³å®šå­¦ä¹ ç‡æŒ‡æ•°æä¾›äº†å®ç”¨æŒ‡å—ï¼Œå¹¶è¿›ä¸€æ­¥é˜æ˜äº†æ ‡å‡†åˆå§‹åŒ–ä¸‹é€å±‚å­¦ä¹ ç‡ç¼©æ”¾ï¼ˆLayerwise learning rate scalingï¼‰çš„æœºåˆ¶ä¸å±€é™æ€§ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "stat.ML"
      ],
      "primary_category": "cs.LG",
      "comment": "NeurIPS 2025 (spotlight) camera-ready version. Open source code for reproducing our experiments can be found under https://github.com/moritzhaas/large-lr-width-scaling Open and easily adaptable code that implements fine-grained tracking of neural network internal statistics can be found under https://github.com/tml-tuebingen/torch-module-monitor",
      "pdf_url": "https://arxiv.org/pdf/2505.22491v2",
      "published_date": "2025-05-28 15:40:48 UTC",
      "updated_date": "2025-10-25 11:34:31 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T15:21:44.078526+00:00"
    },
    {
      "arxiv_id": "2505.22483v2",
      "title": "A Closer Look at Multimodal Representation Collapse",
      "title_zh": "æ·±å…¥æ¢ç©¶å¤šæ¨¡æ€è¡¨å¾åç¼©",
      "authors": [
        "Abhra Chaudhuri",
        "Anjan Dutta",
        "Tu Bui",
        "Serban Georgescu"
      ],
      "abstract": "We aim to develop a fundamental understanding of modality collapse, a recently observed empirical phenomenon wherein models trained for multimodal fusion tend to rely only on a subset of the modalities, ignoring the rest. We show that modality collapse happens when noisy features from one modality are entangled, via a shared set of neurons in the fusion head, with predictive features from another, effectively masking out positive contributions from the predictive features of the former modality and leading to its collapse. We further prove that cross-modal knowledge distillation implicitly disentangles such representations by freeing up rank bottlenecks in the student encoder, denoising the fusion-head outputs without negatively impacting the predictive features from either modality. Based on the above findings, we propose an algorithm that prevents modality collapse through explicit basis reallocation, with applications in dealing with missing modalities. Extensive experiments on multiple multimodal benchmarks validate our theoretical claims. Project page: https://abhrac.github.io/mmcollapse/.",
      "tldr_zh": "è¯¥ç ”ç©¶æ·±å…¥æ¢è®¨äº†å¤šæ¨¡æ€è¡¨ç¤ºå´©æºƒ(modality collapse)ç°è±¡ï¼Œå³å¤šæ¨¡æ€èåˆæ¨¡å‹åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­å€¾å‘äºä»…ä¾èµ–éƒ¨åˆ†æ¨¡æ€è€Œå¿½ç•¥å…¶ä»–æ¨¡æ€çš„å®è¯é—®é¢˜ã€‚ä½œè€…æ­ç¤ºäº†è¯¥ç°è±¡çš„å†…åœ¨æœºåˆ¶ï¼Œå³æŸä¸€æ¨¡æ€çš„å™ªå£°ç‰¹å¾ä¸å¦ä¸€æ¨¡æ€çš„é¢„æµ‹ç‰¹å¾åœ¨èåˆå±‚çš„å…±äº«ç¥ç»å…ƒä¸­å‘ç”Ÿçº ç¼ ï¼Œä»è€Œæ©ç›–äº†è¯¥æ¨¡æ€çš„æ­£é¢è´¡çŒ®å¹¶å¼•å‘å´©æºƒã€‚ç ”ç©¶é€šè¿‡ç†è®ºè¯æ˜å‘ç°ï¼Œè·¨æ¨¡æ€çŸ¥è¯†è’¸é¦(cross-modal knowledge distillation)èƒ½å¤Ÿé€šè¿‡é‡Šæ”¾å­¦ç”Ÿç¼–ç å™¨ä¸­çš„ç§©ç“¶é¢ˆ(rank bottlenecks)æ¥éšå¼åœ°è§£è€¦è¿™äº›è¡¨ç¤ºï¼Œä»è€Œåœ¨ä¸æŸå®³é¢„æµ‹ç‰¹å¾çš„å‰æä¸‹å®ç°èåˆè¾“å‡ºçš„å»å™ªã€‚åŸºäºè¿™äº›å‘ç°ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§é€šè¿‡æ˜¾å¼åŸºé‡åˆ†é…(explicit basis reallocation)é˜²æ­¢æ¨¡æ€å´©æºƒçš„ç®—æ³•ï¼Œå¹¶å°†å…¶åº”ç”¨äºè§£å†³æ¨¡æ€ç¼ºå¤±é—®é¢˜ã€‚åœ¨å¤šä¸ªå¤šæ¨¡æ€åŸºå‡†æµ‹è¯•ä¸Šçš„å¹¿æ³›å®éªŒå……åˆ†éªŒè¯äº†è¯¥ç†è®ºåˆ†æçš„å‡†ç¡®æ€§åŠæ‰€æç®—æ³•çš„æœ‰æ•ˆæ€§ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.LG",
      "comment": "International Conference on Machine Learning (ICML) 2025 (Spotlight)",
      "pdf_url": "https://arxiv.org/pdf/2505.22483v2",
      "published_date": "2025-05-28 15:31:53 UTC",
      "updated_date": "2025-08-14 19:16:46 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T15:21:53.417711+00:00"
    },
    {
      "arxiv_id": "2505.22477v1",
      "title": "Human-Centered Human-AI Collaboration (HCHAC)",
      "title_zh": "ä»¥äººä¸ºä¸­å¿ƒçš„äººæœºåä½œ (HCHAC)",
      "authors": [
        "Qi Gao",
        "Wei Xu",
        "Hanxi Pan",
        "Mowei Shen",
        "Zaifeng Gao"
      ],
      "abstract": "In the intelligent era, the interaction between humans and intelligent systems fundamentally involves collaboration with autonomous intelligent agents. Human-AI Collaboration (HAC) represents a novel type of human-machine relationship facilitated by autonomous intelligent machines equipped with AI technologies. In this paradigm, AI agents serve not only as auxiliary tools but also as active teammates, partnering with humans to accomplish tasks collaboratively. Human-centered AI (HCAI) emphasizes that humans play critical leadership roles in the collaboration. This human-led collaboration imparts new dimensions to the human-machine relationship, necessitating innovative research perspectives, paradigms, and agenda to address the unique challenges posed by HAC. This chapter delves into the essence of HAC from the human-centered perspective, outlining its core concepts and distinguishing features. It reviews the current research methodologies and research agenda within the HAC field from the HCAI perspective, highlighting advancements and ongoing studies. Furthermore, a framework for human-centered HAC (HCHAC) is proposed by integrating these reviews and analyses. A case study of HAC in the context of autonomous vehicles is provided, illustrating practical applications and the synergistic interactions between humans and AI agents. Finally, it identifies potential future research directions aimed at enhancing the effectiveness, reliability, and ethical integration of human-centered HAC systems in diverse domains.",
      "tldr_zh": "æœ¬ç ”ç©¶æ·±å…¥æ¢è®¨äº†æ™ºèƒ½æ—¶ä»£èƒŒæ™¯ä¸‹çš„äººæœºåä½œ(Human-AI Collaboration, HAC)ï¼Œå¼ºè°ƒAIæ™ºèƒ½ä½“å·²ä»å•çº¯çš„è¾…åŠ©å·¥å…·æ¼”å˜ä¸ºä¸äººç±»å…±åŒå®Œæˆä»»åŠ¡çš„ç§¯æé˜Ÿå‹ã€‚åœ¨ä»¥äººä¸ºæœ¬çš„AI(Human-centered AI, HCAI)èŒƒå¼ä¸‹ï¼Œç ”ç©¶å¼ºè°ƒäº†äººç±»åœ¨åä½œè¿‡ç¨‹ä¸­æ‰®æ¼”çš„æ ¸å¿ƒé¢†å¯¼è§’è‰²ã€‚æ–‡ç« ç³»ç»Ÿå›é¡¾äº†å½“å‰HACé¢†åŸŸçš„ç ”ç©¶æ–¹æ³•ä¸è®®ç¨‹ï¼Œé˜è¿°äº†å…¶æ ¸å¿ƒæ¦‚å¿µä¸æ˜¾è‘—ç‰¹å¾ã€‚é€šè¿‡æ•´åˆç°æœ‰ç ”ç©¶åˆ†æï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ä¸ªä»¥äººä¸ºæœ¬çš„äººæœºåä½œ(Human-Centered Human-AI Collaboration, HCHAC)æ¡†æ¶ã€‚ç ”ç©¶è¿˜é€šè¿‡è‡ªåŠ¨é©¾é©¶æ±½è½¦(autonomous vehicles)çš„æ¡ˆä¾‹ï¼Œå…·ä½“å±•ç¤ºäº†äººç±»ä¸AIæ™ºèƒ½ä½“ä¹‹é—´çš„ååŒäº¤äº’åŠå®é™…åº”ç”¨ã€‚æœ€åï¼Œè¯¥å·¥ä½œæ˜ç¡®äº†æœªæ¥æå‡HCHACç³»ç»Ÿæœ‰æ•ˆæ€§ã€å¯é æ€§åŠä¼¦ç†é›†æˆ(ethical integration)çš„ç ”ç©¶æ–¹å‘ã€‚",
      "categories": [
        "cs.HC",
        "cs.AI",
        "cs.CY"
      ],
      "primary_category": "cs.HC",
      "comment": "This article is a chapter from the upcoming book Handbook of Human-Centered Artificial Intelligence",
      "pdf_url": "https://arxiv.org/pdf/2505.22477v1",
      "published_date": "2025-05-28 15:27:52 UTC",
      "updated_date": "2025-05-28 15:27:52 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T15:21:45.043872+00:00"
    },
    {
      "arxiv_id": "2505.22467v3",
      "title": "Topological Structure Learning Should Be A Research Priority for LLM-Based Multi-Agent Systems",
      "title_zh": "æ‹“æ‰‘ç»“æ„å­¦ä¹ åº”æˆä¸ºåŸºäºå¤§è¯­è¨€æ¨¡å‹çš„å¤šæ™ºèƒ½ä½“ç³»ç»Ÿçš„ç ”ç©¶é‡ç‚¹",
      "authors": [
        "Jiaxi Yang",
        "Mengqi Zhang",
        "Yiqiao Jin",
        "Hao Chen",
        "Qingsong Wen",
        "Lu Lin",
        "Yi He",
        "Srijan Kumar",
        "Weijie Xu",
        "James Evans",
        "Jindong Wang"
      ],
      "abstract": "Large Language Model-based Multi-Agent Systems (MASs) have emerged as a powerful paradigm for tackling complex tasks through collaborative intelligence. However, the topology of these systems--how agents in MASs should be configured, connected, and coordinated--remains largely unexplored. In this position paper, we call for a paradigm shift toward \\emph{topology-aware MASs} that explicitly model and dynamically optimize the structure of inter-agent interactions. We identify three fundamental components--agents, communication links, and overall topology--that collectively determine the system's adaptability, efficiency, robustness, and fairness. To operationalize this vision, we introduce a systematic three-stage framework: 1) agent selection, 2) structure profiling, and 3) topology synthesis. This framework not only provides a principled foundation for designing MASs but also opens new research frontiers across language modeling, reinforcement learning, graph learning, and generative modeling to ultimately unleash their full potential in complex real-world applications. We conclude by outlining key challenges and opportunities in MASs evaluation. We hope our framework and perspectives offer critical new insights in the era of agentic AI.",
      "tldr_zh": "è¯¥ç ”ç©¶æŒ‡å‡ºï¼Œå°½ç®¡åŸºäºå¤§è¯­è¨€æ¨¡å‹(LLM)çš„å¤šæ™ºèƒ½ä½“ç³»ç»Ÿ(Multi-Agent Systems, MASs)åœ¨å¤„ç†å¤æ‚ä»»åŠ¡ä¸­å±•ç°å‡ºå¼ºå¤§æ½œåŠ›ï¼Œä½†å…¶æ‹“æ‰‘ç»“æ„ï¼Œå³æ™ºèƒ½ä½“é—´çš„é…ç½®ã€è¿æ¥ä¸åè°ƒæ–¹å¼ï¼Œåœ¨å¾ˆå¤§ç¨‹åº¦ä¸Šä»æœªè¢«æ·±å…¥æ¢è®¨ã€‚ä¸ºæ­¤ï¼Œè®ºæ–‡å‘¼åå‘æ‹“æ‰‘æ„ŸçŸ¥å¤šæ™ºèƒ½ä½“ç³»ç»Ÿ(topology-aware MASs)è¿›è¡ŒèŒƒå¼è½¬å˜ï¼Œä¸»å¼ é€šè¿‡æ˜¾å¼å»ºæ¨¡å’ŒåŠ¨æ€ä¼˜åŒ–æ™ºèƒ½ä½“é—´çš„äº¤äº’ç»“æ„æ¥æå‡ç³»ç»Ÿæ€§èƒ½ã€‚ç ”ç©¶è¯†åˆ«äº†æ™ºèƒ½ä½“(agents)ã€é€šä¿¡é“¾è·¯(communication links)åŠæ•´ä½“æ‹“æ‰‘(overall topology)è¿™ä¸‰ä¸ªå†³å®šç³»ç»Ÿé€‚åº”æ€§ã€æ•ˆç‡ã€é²æ£’æ€§å’Œå…¬å¹³æ€§çš„æ ¸å¿ƒç»„ä»¶ã€‚ä¸ºäº†è½å®è¿™ä¸€æ„¿æ™¯ï¼Œä½œè€…æå‡ºäº†ä¸€ä¸ªåŒ…å«æ™ºèƒ½ä½“é€‰æ‹©(agent selection)ã€ç»“æ„å‰–æ(structure profiling)å’Œæ‹“æ‰‘åˆæˆ(topology synthesis)çš„ç³»ç»ŸåŒ–ä¸‰é˜¶æ®µæ¡†æ¶ã€‚è¯¥æ¡†æ¶ä¸ä»…ä¸ºè®¾è®¡é«˜æ•ˆå¤šæ™ºèƒ½ä½“ç³»ç»Ÿå¥ å®šäº†ç†è®ºåŸºç¡€ï¼Œè¿˜ä¸ºè¯­è¨€å»ºæ¨¡ã€å¼ºåŒ–å­¦ä¹ ã€å›¾å­¦ä¹ (graph learning)å’Œç”Ÿæˆå»ºæ¨¡ç­‰äº¤å‰é¢†åŸŸæä¾›äº†æ–°çš„ç ”ç©¶æ–¹å‘ã€‚è®ºæ–‡æœ€åæ¢è®¨äº†å¤šæ™ºèƒ½ä½“ç³»ç»Ÿè¯„ä¼°çš„æŒ‘æˆ˜ä¸æœºé‡ï¼Œæ—¨åœ¨æ¨åŠ¨Agentic AIåœ¨å¤æ‚ç°å®åœºæ™¯ä¸­çš„æ·±åº¦åº”ç”¨ã€‚",
      "categories": [
        "cs.MA",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.MA",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.22467v3",
      "published_date": "2025-05-28 15:20:09 UTC",
      "updated_date": "2025-10-17 02:41:02 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T15:22:14.383586+00:00"
    },
    {
      "arxiv_id": "2505.22457v1",
      "title": "Fostering Video Reasoning via Next-Event Prediction",
      "title_zh": "é€šè¿‡ä¸‹ä¸€äº‹ä»¶é¢„æµ‹æå‡è§†é¢‘æ¨ç†èƒ½åŠ›",
      "authors": [
        "Haonan Wang",
        "Hongfu Liu",
        "Xiangyan Liu",
        "Chao Du",
        "Kenji Kawaguchi",
        "Ye Wang",
        "Tianyu Pang"
      ],
      "abstract": "Next-token prediction serves as the foundational learning task enabling reasoning in LLMs. But what should the learning task be when aiming to equip MLLMs with temporal reasoning capabilities over video inputs? Existing tasks such as video question answering often rely on annotations from humans or much stronger MLLMs, while video captioning tends to entangle temporal reasoning with spatial information. To address this gap, we propose next-event prediction (NEP), a learning task that harnesses future video segments as a rich, self-supervised signal to foster temporal reasoning. We segment each video into past and future frames: the MLLM takes the past frames as input and predicts a summary of events derived from the future frames, thereby encouraging the model to reason temporally in order to complete the task. To support this task, we curate V1-33K, a dataset comprising 33,000 automatically extracted video segments spanning diverse real-world scenarios. We further explore a range of video instruction-tuning strategies to study their effects on temporal reasoning. To evaluate progress, we introduce FutureBench to assess coherence in predicting unseen future events. Experiments validate that NEP offers a scalable and effective training paradigm for fostering temporal reasoning in MLLMs.",
      "tldr_zh": "è¿™é¡¹ç ”ç©¶æå‡ºäº†Next-Event Prediction (NEP) ä»»åŠ¡ï¼Œæ—¨åœ¨é€šè¿‡è‡ªç›‘ç£å­¦ä¹ å¢å¼ºå¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ (MLLMs) åœ¨è§†é¢‘è¾“å…¥ä¸Šçš„æ—¶é—´æ¨ç† (Temporal Reasoning) èƒ½åŠ›ã€‚é’ˆå¯¹ç°æœ‰çš„è§†é¢‘é—®ç­”å’Œæè¿°ä»»åŠ¡è¿‡åº¦ä¾èµ–äººå·¥æ ‡æ³¨ä¸”å®¹æ˜“æ··æ·†æ—¶ç©ºä¿¡æ¯çš„é—®é¢˜ï¼ŒNEPåˆ©ç”¨è§†é¢‘çš„æœªæ¥ç‰‡æ®µä½œä¸ºè‡ªç›‘ç£ä¿¡å·ï¼Œå°†è§†é¢‘åˆ’åˆ†ä¸ºè¿‡å»å’Œæœªæ¥å¸§ã€‚åœ¨æ­¤æ¡†æ¶ä¸‹ï¼ŒMLLMé€šè¿‡è¾“å…¥è¿‡å»å¸§æ¥é¢„æµ‹æœªæ¥äº‹ä»¶çš„æ‘˜è¦ï¼Œä»è€Œä¿ƒä½¿æ¨¡å‹åœ¨å®Œæˆä»»åŠ¡çš„è¿‡ç¨‹ä¸­è¿›è¡Œæ—¶é—´å±‚é¢çš„æ¨ç†ã€‚ä¸ºæ”¯æŒè¿™ä¸€å­¦ä¹ ä»»åŠ¡ï¼Œç ”ç©¶è€…æ„å»ºäº†åŒ…å«33,000ä¸ªæ¶µç›–å¤šæ ·åŒ–ç°å®åœºæ™¯è§†é¢‘ç‰‡æ®µçš„V1-33Kæ•°æ®é›†ã€‚æ­¤å¤–ï¼Œç ”ç©¶è¿˜å¼•å…¥äº†FutureBenchåŸºå‡†æµ‹è¯•ï¼Œç”¨ä»¥è¯„ä¼°æ¨¡å‹é¢„æµ‹æœªè§æœªæ¥äº‹ä»¶çš„è¿è´¯æ€§ã€‚å®éªŒç»“æœéªŒè¯äº†NEPæ˜¯æå‡MLLMsæ—¶é—´æ¨ç†èƒ½åŠ›çš„ä¸€ç§å¯æ‰©å±•ä¸”æœ‰æ•ˆçš„è®­ç»ƒèŒƒå¼ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.22457v1",
      "published_date": "2025-05-28 15:13:34 UTC",
      "updated_date": "2025-05-28 15:13:34 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T15:22:02.448942+00:00"
    },
    {
      "arxiv_id": "2505.22453v2",
      "title": "First SFT, Second RL, Third UPT: Continual Improving Multi-Modal LLM Reasoning via Unsupervised Post-Training",
      "title_zh": "å…ˆSFTï¼ŒåRLï¼Œå†UPTï¼šé€šè¿‡æ— ç›‘ç£åè®­ç»ƒæŒç»­æå‡å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹æ¨ç†èƒ½åŠ›",
      "authors": [
        "Lai Wei",
        "Yuting Li",
        "Chen Wang",
        "Yue Wang",
        "Linghe Kong",
        "Weiran Huang",
        "Lichao Sun"
      ],
      "abstract": "Improving Multi-modal Large Language Models (MLLMs) in the post-training stage typically relies on supervised fine-tuning (SFT) or reinforcement learning (RL), which require expensive and manually annotated multi-modal data--an ultimately unsustainable resource. This limitation has motivated a growing interest in unsupervised paradigms as a third stage of post-training after SFT and RL. While recent efforts have explored this direction, their methods are complex and difficult to iterate. To address this, we propose MM-UPT, a simple yet effective framework for unsupervised post-training of MLLMs, enabling continual self-improvement without any external supervision. The training method of MM-UPT builds upon GRPO, replacing traditional reward signals with a self-rewarding mechanism based on majority voting over multiple sampled responses. Our experiments demonstrate that such training method effectively improves the reasoning ability of Qwen2.5-VL-7B (e.g., 66.3\\%$\\rightarrow$72.9\\% on MathVista, 62.9\\%$\\rightarrow$68.7\\% on We-Math), using standard dataset without ground truth labels. To further explore scalability, we extend our framework to a data self-generation setting, designing two strategies that prompt the MLLM to synthesize new training samples on its own. Additional experiments show that combining these synthetic data with the unsupervised training method can also boost performance, highlighting a promising approach for scalable self-improvement. Overall, MM-UPT offers a new paradigm for autonomous enhancement of MLLMs, serving as a critical third step after initial SFT and RL in the absence of external supervision. Our code is available at https://github.com/waltonfuture/MM-UPT.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†MM-UPTï¼Œä¸€ç§ç”¨äºå¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰çš„æ— ç›‘ç£åè®­ç»ƒï¼ˆUnsupervised Post-Trainingï¼‰æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³ä¼ ç»Ÿç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰å’Œå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰é˜¶æ®µè¿‡åº¦ä¾èµ–æ˜‚è´µäººå·¥æ ‡æ³¨æ•°æ®çš„é—®é¢˜ã€‚è¯¥æ¡†æ¶åŸºäºGRPOç®—æ³•ï¼Œé€šè¿‡åœ¨å¤šä¸ªé‡‡æ ·å›ç­”ä¸­è¿›è¡Œå¤šæ•°æŠ•ç¥¨ï¼ˆMajority Votingï¼‰å»ºç«‹è‡ªå¥–åŠ±æœºåˆ¶ï¼Œä»¥æ­¤æ›¿ä»£ä¼ ç»Ÿçš„å¤–éƒ¨å¥–åŠ±ä¿¡å·ï¼Œå®ç°æ¨¡å‹çš„æŒç»­è‡ªæˆ‘æ”¹è¿›ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œåœ¨ä¸ä½¿ç”¨ä»»ä½•æ ‡å‡†ç­”æ¡ˆï¼ˆGround Truthï¼‰æ ‡ç­¾çš„æƒ…å†µä¸‹ï¼Œè¯¥æ–¹æ³•æ˜¾è‘—æå‡äº†Qwen2.5-VL-7Bçš„æ¨ç†èƒ½åŠ›ï¼Œåœ¨MathVistaå’ŒWe-MathåŸºå‡†æµ‹è¯•ä¸­å‡†ç¡®ç‡åˆ†åˆ«æå‡äº†6.6%å’Œ5.8%ã€‚æ­¤å¤–ï¼Œç ”ç©¶è¿˜é€šè¿‡æ•°æ®è‡ªç”Ÿæˆï¼ˆData Self-generationï¼‰ç­–ç•¥ä½¿æ¨¡å‹è‡ªä¸»åˆæˆè®­ç»ƒæ ·æœ¬ï¼Œè¿›ä¸€æ­¥éªŒè¯äº†è¯¥æ¨¡å¼åœ¨å¯æ‰©å±•è‡ªæˆ‘æå‡æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚æ€»ä½“è€Œè¨€ï¼ŒMM-UPTä¸ºMLLMsçš„è‡ªä¸»å¢å¼ºæä¾›äº†ä¸€ç§æ–°èŒƒå¼ï¼Œæˆä¸ºç»§SFTå’ŒRLä¹‹åã€åœ¨ç¼ºä¹å¤–éƒ¨ç›‘ç£ç¯å¢ƒä¸‹çš„å…³é”®ç¬¬ä¸‰æ­¥ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.CV",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "Accepted by NeurIPS 2025",
      "pdf_url": "https://arxiv.org/pdf/2505.22453v2",
      "published_date": "2025-05-28 15:11:16 UTC",
      "updated_date": "2025-10-27 09:06:32 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T15:22:25.929617+00:00"
    },
    {
      "arxiv_id": "2505.22451v1",
      "title": "AI Mathematician: Towards Fully Automated Frontier Mathematical Research",
      "title_zh": "AI Mathematicianï¼šè¿ˆå‘å…¨è‡ªåŠ¨åŒ–çš„å‰æ²¿æ•°å­¦ç ”ç©¶",
      "authors": [
        "Yuanhang Liu",
        "Yanxing Huang",
        "Yanqiao Wang",
        "Peng Li",
        "Yang Liu"
      ],
      "abstract": "Large Reasoning Models (LRMs) have made significant progress in mathematical capabilities in recent times. However, these successes have been primarily confined to competition-level problems. In this work, we propose AI Mathematician (AIM) framework, which harnesses the reasoning strength of LRMs to support frontier mathematical research. We have identified two critical challenges of mathematical research compared to competition, {\\it the intrinsic complexity of research problems} and {\\it the requirement of procedural rigor}. To address these challenges, AIM incorporates two core strategies: an exploration mechanism to foster longer solution paths, and the pessimistic reasonable verification method to ensure reliability.\n  This early version of AIM already exhibits strong capability in tackling research-level tasks. We conducted extensive experiments across several real-world mathematical topics and obtained promising results. AIM is able to autonomously construct substantial portions of proofs and uncover non-trivial insights within each research area. These findings highlight the potential of LRMs in mathematical discovery and suggest that LRM-based agent systems could significantly accelerate mathematical research in the future.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† AI Mathematician (AIM) æ¡†æ¶ï¼Œæ—¨åœ¨åˆ©ç”¨å¤§æ¨ç†æ¨¡å‹ (Large Reasoning Models) çš„æ¨ç†èƒ½åŠ›æ”¯æŒå‰æ²¿æ•°å­¦ç ”ç©¶ï¼Œçªç ´äº†ä»¥å¾€ AI ä»…é™äºç«èµ›çº§æ•°å­¦é—®é¢˜çš„å±€é™ã€‚é’ˆå¯¹æ•°å­¦ç ”ç©¶ä¸­å­˜åœ¨çš„å†…åœ¨å¤æ‚æ€§ (intrinsic complexity) å’Œè¿‡ç¨‹ä¸¥è°¨æ€§ (procedural rigor) ä¸¤å¤§æŒ‘æˆ˜ï¼ŒAIM å¼•å…¥äº†ä¿ƒè¿›é•¿è·¯å¾„è§£é¢˜çš„æ¢ç´¢æœºåˆ¶ (exploration mechanism) å’Œç¡®ä¿å¯é æ€§çš„æ‚²è§‚åˆç†éªŒè¯æ–¹æ³• (pessimistic reasonable verification)ã€‚å®éªŒè¡¨æ˜ï¼Œæ—©æœŸç‰ˆæœ¬çš„ AIM å·²å…·å¤‡å¤„ç†çœŸå®ç§‘ç ”è¯¾é¢˜çš„èƒ½åŠ›ï¼Œèƒ½å¤Ÿè‡ªä¸»æ„å»ºè¯æ˜çš„å…³é”®éƒ¨åˆ†å¹¶å‘ç°éå¹³å‡¡çš„è§è§£ (non-trivial insights)ã€‚è¯¥ç ”ç©¶ä¸ä»…å±•ç¤ºäº† LRMs åœ¨æ•°å­¦å‘ç°ä¸­çš„å·¨å¤§æ½œåŠ›ï¼Œä¹Ÿé¢„ç¤ºç€åŸºäº LRMs çš„æ™ºèƒ½ä½“ç³»ç»Ÿæœªæ¥å°†æ˜¾è‘—åŠ é€Ÿå‰æ²¿æ•°å­¦ç ”ç©¶çš„è¿›ç¨‹ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "95 pages, 1 figure",
      "pdf_url": "https://arxiv.org/pdf/2505.22451v1",
      "published_date": "2025-05-28 15:10:37 UTC",
      "updated_date": "2025-05-28 15:10:37 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T15:22:22.352962+00:00"
    },
    {
      "arxiv_id": "2505.22445v1",
      "title": "NFR: Neural Feature-Guided Non-Rigid Shape Registration",
      "title_zh": "NFRï¼šç¥ç»ç‰¹å¾å¼•å¯¼çš„éåˆšæ€§å½¢çŠ¶é…å‡†",
      "authors": [
        "Puhua Jiang",
        "Zhangquan Chen",
        "Mingze Sun",
        "Ruqi Huang"
      ],
      "abstract": "In this paper, we propose a novel learning-based framework for 3D shape registration, which overcomes the challenges of significant non-rigid deformation and partiality undergoing among input shapes, and, remarkably, requires no correspondence annotation during training. Our key insight is to incorporate neural features learned by deep learning-based shape matching networks into an iterative, geometric shape registration pipeline. The advantage of our approach is two-fold -- On one hand, neural features provide more accurate and semantically meaningful correspondence estimation than spatial features (e.g., coordinates), which is critical in the presence of large non-rigid deformations; On the other hand, the correspondences are dynamically updated according to the intermediate registrations and filtered by consistency prior, which prominently robustify the overall pipeline. Empirical results show that, with as few as dozens of training shapes of limited variability, our pipeline achieves state-of-the-art results on several benchmarks of non-rigid point cloud matching and partial shape matching across varying settings, but also delivers high-quality correspondences between unseen challenging shape pairs that undergo both significant extrinsic and intrinsic deformations, in which case neither traditional registration methods nor intrinsic methods work.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†NFRï¼ˆNeural Feature-Guided Non-Rigid Shape Registrationï¼‰ï¼Œä¸€ç§ç”¨äº3Då½¢çŠ¶é…å‡†çš„æ–°å‹å­¦ä¹ æ¡†æ¶ï¼Œæ—¨åœ¨å…‹æœè¾“å…¥å½¢çŠ¶é—´æ˜¾è‘—çš„éåˆšæ€§å½¢å˜ï¼ˆnon-rigid deformationï¼‰å’Œå±€éƒ¨ç¼ºå¤±ï¼ˆpartialityï¼‰å¸¦æ¥çš„æŒ‘æˆ˜ã€‚è¯¥æ¡†æ¶çš„æ ¸å¿ƒåˆ›æ–°åœ¨äºå°†æ·±åº¦å­¦ä¹ åŒ¹é…ç½‘ç»œæå–çš„ç¥ç»ç‰¹å¾ï¼ˆneural featuresï¼‰èå…¥åˆ°è¿­ä»£çš„å‡ ä½•å½¢çŠ¶é…å‡†ç®¡é“ä¸­ï¼Œä¸”åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­æ— éœ€ä»»ä½•å¯¹åº”å…³ç³»æ ‡æ³¨ï¼ˆcorrespondence annotationï¼‰ã€‚ç›¸æ¯”ä¼ ç»Ÿçš„ç©ºé—´ç‰¹å¾ï¼Œç¥ç»ç‰¹å¾èƒ½æä¾›æ›´å‡†ç¡®ä¸”å…·æœ‰è¯­ä¹‰æ„ä¹‰çš„å¯¹åº”å…³ç³»ä¼°è®¡ï¼Œç»“åˆåŠ¨æ€æ›´æ–°æœºåˆ¶å’Œä¸€è‡´æ€§å…ˆéªŒï¼ˆconsistency priorï¼‰è¿‡æ»¤ï¼Œæ˜¾è‘—æå‡äº†æ•´ä½“æµç¨‹çš„é²æ£’æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒNFRåœ¨å¤šä¸ªéåˆšæ€§ç‚¹äº‘åŒ¹é…å’Œéƒ¨åˆ†å½¢çŠ¶åŒ¹é…åŸºå‡†ä¸Šå‡è¾¾åˆ°äº†SOTAæ€§èƒ½ã€‚å³ä¾¿é¢å¯¹ä¼ ç»Ÿæ–¹æ³•æˆ–å†…åœ¨æ–¹æ³•ï¼ˆintrinsic methodsï¼‰éš¾ä»¥å¤„ç†çš„æç«¯å˜å½¢å’ŒæœªçŸ¥å½¢çŠ¶å¯¹ï¼Œè¯¥æ–¹æ³•ä¾ç„¶èƒ½å¤Ÿæä¾›é«˜è´¨é‡çš„é…å‡†ç»“æœã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "20 pages, 9 figures. arXiv admin note: substantial text overlap with arXiv:2311.04494",
      "pdf_url": "https://arxiv.org/pdf/2505.22445v1",
      "published_date": "2025-05-28 15:08:49 UTC",
      "updated_date": "2025-05-28 15:08:49 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T15:22:38.359206+00:00"
    },
    {
      "arxiv_id": "2505.22442v2",
      "title": "SOReL and TOReL: Two Methods for Fully Offline Reinforcement Learning",
      "title_zh": "SOReL ä¸ TOReLï¼šä¸¤ç§å…¨ç¦»çº¿å¼ºåŒ–å­¦ä¹ æ–¹æ³•",
      "authors": [
        "Mattie Fellows",
        "Clarisse Wibault",
        "Uljad Berdica",
        "Johannes Forkel",
        "Michael A. Osborne",
        "Jakob N. Foerster"
      ],
      "abstract": "Sample efficiency remains a major obstacle for real world adoption of reinforcement learning (RL): success has been limited to settings where simulators provide access to essentially unlimited environment interactions, which in reality are typically costly or dangerous to obtain. Offline RL in principle offers a solution by exploiting offline data to learn a near-optimal policy before deployment. In practice, however, current offline RL methods rely on extensive online interactions for hyperparameter tuning, and have no reliable bound on their initial online performance. To address these two issues, we introduce two algorithms. Firstly, SOReL: an algorithm for safe offline reinforcement learning. Using only offline data, our Bayesian approach infers a posterior over environment dynamics to obtain a reliable estimate of the online performance via the posterior predictive uncertainty. Crucially, all hyperparameters are also tuned fully offline. Secondly, we introduce TOReL: a tuning for offline reinforcement learning algorithm that extends our information rate based offline hyperparameter tuning methods to general offline RL approaches. Our empirical evaluation confirms SOReL's ability to accurately estimate regret in the Bayesian setting whilst TOReL's offline hyperparameter tuning achieves competitive performance with the best online hyperparameter tuning methods using only offline data. Thus, SOReL and TOReL make a significant step towards safe and reliable offline RL, unlocking the potential for RL in the real world. Our implementations are publicly available: https://github.com/CWibault/sorel\\_torel.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ Reinforcement Learning åœ¨ç°å®åº”ç”¨ä¸­é¢ä¸´çš„æ ·æœ¬æ•ˆç‡ç“¶é¢ˆï¼Œä»¥åŠç°æœ‰ Offline RL æ–¹æ³•ä»éœ€ä¾èµ–åœ¨çº¿è¶…å‚æ•°è°ƒä¼˜çš„å±€é™æ€§ï¼Œæå‡ºäº† SOReL å’Œ TOReL ä¸¤ç§å…¨ç¦»çº¿å­¦ä¹ æ–¹æ³•ã€‚SOReL é‡‡ç”¨ Bayesian æ–¹æ³•æ¨æ–­ç¯å¢ƒåŠ¨åŠ›å­¦çš„åéªŒåˆ†å¸ƒï¼Œåˆ©ç”¨åéªŒé¢„æµ‹ä¸ç¡®å®šæ€§å®ç°å¯¹åœ¨çº¿æ€§èƒ½çš„å¯é è¯„ä¼°ï¼Œå¹¶æ”¯æŒå…¨ç¦»çº¿è¶…å‚æ•°è°ƒä¼˜ã€‚TOReL åˆ™é€šè¿‡åŸºäº Information Rate çš„æœºåˆ¶ï¼Œå°†ç¦»çº¿è¶…å‚æ•°è°ƒä¼˜æŠ€æœ¯æ‰©å±•åˆ°æ›´é€šç”¨çš„ Offline RL ç®—æ³•ä¸­ã€‚å®éªŒè¯„ä¼°éªŒè¯äº† SOReL åœ¨ Bayesian è®¾å®šä¸‹å‡†ç¡®ä¼°è®¡ Regret çš„èƒ½åŠ›ï¼ŒåŒæ—¶è¯æ˜äº† TOReL ä»…åˆ©ç”¨ç¦»çº¿æ•°æ®å³å¯è¾¾åˆ°ä¸æœ€ä½³åœ¨çº¿è°ƒä¼˜æ–¹æ³•ç›¸åª²ç¾çš„æ€§èƒ½ã€‚è¿™é¡¹å·¥ä½œåœ¨å®ç°å®‰å…¨å¯é çš„ Offline RL æ–¹é¢è¿ˆå‡ºäº†é‡è¦ä¸€æ­¥ï¼Œä¸º Reinforcement Learning åœ¨ç°å®åœºæ™¯ä¸­çš„å¤§è§„æ¨¡åº”ç”¨æä¾›äº†å…³é”®æ”¯æ’‘ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.22442v2",
      "published_date": "2025-05-28 15:07:24 UTC",
      "updated_date": "2025-05-29 20:38:03 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T15:23:34.149673+00:00"
    },
    {
      "arxiv_id": "2505.22441v2",
      "title": "Can NeRFs See without Cameras?",
      "title_zh": "NeRFs èƒ½åœ¨æ²¡æœ‰ç›¸æœºçš„æƒ…å†µä¸‹â€œçœ‹è§â€å—ï¼Ÿ",
      "authors": [
        "Chaitanya Amballa",
        "Sattwik Basu",
        "Yu-Lin Wei",
        "Zhijian Yang",
        "Mehmet Ergezer",
        "Romit Roy Choudhury"
      ],
      "abstract": "Neural Radiance Fields (NeRFs) have been remarkably successful at synthesizing novel views of 3D scenes by optimizing a volumetric scene function. This scene function models how optical rays bring color information from a 3D object to the camera pixels. Radio frequency (RF) or audio signals can also be viewed as a vehicle for delivering information about the environment to a sensor. However, unlike camera pixels, an RF/audio sensor receives a mixture of signals that contain many environmental reflections (also called \"multipath\"). Is it still possible to infer the environment using such multipath signals? We show that with redesign, NeRFs can be taught to learn from multipath signals, and thereby \"see\" the environment. As a grounding application, we aim to infer the indoor floorplan of a home from sparse WiFi measurements made at multiple locations inside the home. Although a difficult inverse problem, our implicitly learnt floorplans look promising, and enables forward applications, such as indoor signal prediction and basic ray tracing.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†ç¥ç»è¾å°„åœº(NeRFs)åœ¨éç›¸æœºä¼ æ„Ÿå™¨ï¼ˆå¦‚å°„é¢‘RFæˆ–éŸ³é¢‘ä¿¡å·ï¼‰ä¸­çš„åº”ç”¨ï¼Œæ—¨åœ¨éªŒè¯NeRFsæ˜¯å¦èƒ½åœ¨æ²¡æœ‰ç›¸æœºçš„æƒ…å†µä¸‹â€œæ„ŸçŸ¥â€ç¯å¢ƒã€‚ä¸ç›¸æœºåƒç´ æ¥æ”¶å•ä¸€å…‰çº¿ä¿¡æ¯ä¸åŒï¼Œå°„é¢‘å’ŒéŸ³é¢‘ä¼ æ„Ÿå™¨æ¥æ”¶çš„æ˜¯åŒ…å«å¤æ‚ç¯å¢ƒåå°„çš„æ··åˆä¿¡å·ï¼ˆå¤šè·¯å¾„ï¼ŒMultipathï¼‰ã€‚é€šè¿‡é‡æ–°è®¾è®¡ï¼ŒNeRFsè¢«æ•™å¯¼å¦‚ä½•ä»è¿™äº›å¤šè·¯å¾„ä¿¡å·ä¸­å­¦ä¹ å¹¶æ¨æ–­ç¯å¢ƒç»“æ„ã€‚ä½œä¸ºå…·ä½“åº”ç”¨ï¼Œç ”ç©¶è€…é€šè¿‡å®¤å†…å¤šä¸ªä½ç½®é‡‡é›†çš„ç¨€ç–WiFiæµ‹é‡æ•°æ®ï¼ŒæˆåŠŸæ¨æ–­å‡ºäº†å®¤å†…çš„å¹³é¢å›¾(Floorplan)ã€‚å°½ç®¡è¿™æ˜¯ä¸€ä¸ªå¤æ‚çš„é€†é—®é¢˜ï¼Œå®éªŒè¯æ˜å­¦ä¹ åˆ°çš„éšå¼å¹³é¢å›¾å…·æœ‰æé«˜çš„å®ç”¨ä»·å€¼ã€‚è¯¥æˆæœè¿˜è¿›ä¸€æ­¥æ”¯æŒäº†å®¤å†…ä¿¡å·é¢„æµ‹å’ŒåŸºç¡€å°„çº¿è¿½è¸ª(Ray Tracing)ç­‰å‰å‘åº”ç”¨ä»»åŠ¡ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.22441v2",
      "published_date": "2025-05-28 15:04:46 UTC",
      "updated_date": "2025-09-02 04:46:56 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T15:23:17.837705+00:00"
    },
    {
      "arxiv_id": "2505.22438v1",
      "title": "Synonymous Variational Inference for Perceptual Image Compression",
      "title_zh": "é¢å‘æ„ŸçŸ¥å›¾åƒå‹ç¼©çš„åŒä¹‰å˜åˆ†æ¨ç†",
      "authors": [
        "Zijian Liang",
        "Kai Niu",
        "Changshuo Wang",
        "Jin Xu",
        "Ping Zhang"
      ],
      "abstract": "Recent contributions of semantic information theory reveal the set-element relationship between semantic and syntactic information, represented as synonymous relationships. In this paper, we propose a synonymous variational inference (SVI) method based on this synonymity viewpoint to re-analyze the perceptual image compression problem. It takes perceptual similarity as a typical synonymous criterion to build an ideal synonymous set (Synset), and approximate the posterior of its latent synonymous representation with a parametric density by minimizing a partial semantic KL divergence. This analysis theoretically proves that the optimization direction of perception image compression follows a triple tradeoff that can cover the existing rate-distortion-perception schemes. Additionally, we introduce synonymous image compression (SIC), a new image compression scheme that corresponds to the analytical process of SVI, and implement a progressive SIC codec to fully leverage the model's capabilities. Experimental results demonstrate comparable rate-distortion-perception performance using a single progressive SIC codec, thus verifying the effectiveness of our proposed analysis method.",
      "tldr_zh": "è¯¥ç ”ç©¶åŸºäºè¯­ä¹‰ä¿¡æ¯è®ºä¸­è¯­ä¹‰ä¸è¯­æ³•ä¿¡æ¯çš„åŒä¹‰å…³ç³»ï¼Œé’ˆå¯¹æ„ŸçŸ¥å›¾åƒå‹ç¼©é—®é¢˜æå‡ºäº† Synonymous Variational Inference (SVI) æ–¹æ³•ã€‚è¯¥æ–¹æ³•å°†æ„ŸçŸ¥ç›¸ä¼¼æ€§ä½œä¸ºåŒä¹‰å‡†åˆ™æ¥æ„å»ºç†æƒ³çš„åŒä¹‰é›† (Synset)ï¼Œå¹¶é€šè¿‡æœ€å°åŒ–éƒ¨åˆ†è¯­ä¹‰ KL divergence æ¥é€¼è¿‘å…¶æ½œå‘åŒä¹‰è¡¨ç¤ºçš„åéªŒåˆ†å¸ƒã€‚ç†è®ºä¸Šï¼Œè¯¥åˆ†æè¯æ˜äº†æ„ŸçŸ¥å›¾åƒå‹ç¼©çš„ä¼˜åŒ–æ–¹å‘éµå¾ªä¸€ç§æ¶µç›–ç°æœ‰ rate-distortion-perception æ–¹æ¡ˆçš„ triple tradeoffã€‚æ­¤å¤–ï¼Œç ”ç©¶æå‡ºäº†ç›¸åº”çš„ Synonymous Image Compression (SIC) æ–¹æ¡ˆï¼Œå¹¶å¼€å‘äº†æ¸è¿›å¼ SIC codec ä»¥å¢å¼ºæ€§èƒ½ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨ç‡å¤±çœŸæ„ŸçŸ¥æ€§èƒ½ä¸Šå…·æœ‰ç«äº‰åŠ›ï¼Œæœ‰æ•ˆè¯å®äº† SVI ç†è®ºæ¡†æ¶çš„å®ç”¨ä»·å€¼ã€‚",
      "categories": [
        "cs.IT",
        "cs.AI",
        "cs.CV",
        "cs.LG",
        "eess.IV"
      ],
      "primary_category": "cs.IT",
      "comment": "31 pages, 20 figures. This paper is accepted by Proceedings of the 42nd International Conference on Machine Learning (ICML 2025) Poster",
      "pdf_url": "https://arxiv.org/pdf/2505.22438v1",
      "published_date": "2025-05-28 15:03:27 UTC",
      "updated_date": "2025-05-28 15:03:27 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T15:23:19.215217+00:00"
    },
    {
      "arxiv_id": "2505.22425v1",
      "title": "Scaling Reasoning without Attention",
      "title_zh": "æ— éœ€æ³¨æ„åŠ›æœºåˆ¶çš„æ¨ç†è§„æ¨¡åŒ–",
      "authors": [
        "Xueliang Zhao",
        "Wei Wu",
        "Lingpeng Kong"
      ],
      "abstract": "Large language models (LLMs) have made significant advances in complex reasoning tasks, yet they remain bottlenecked by two core challenges: architectural inefficiency due to reliance on Transformers, and a lack of structured fine-tuning for high-difficulty domains. We introduce \\ourmodel, an attention-free language model that addresses both issues through architectural and data-centric innovations. Built on the state space dual (SSD) layers of Mamba-2, our model eliminates the need for self-attention and key-value caching, enabling fixed-memory, constant-time inference. To train it for complex reasoning, we propose a two-phase curriculum fine-tuning strategy based on the \\textsc{PromptCoT} synthesis paradigm, which generates pedagogically structured problems via abstract concept selection and rationale-guided generation. On benchmark evaluations, \\ourmodel-7B outperforms strong Transformer and hybrid models of comparable scale, and even surpasses the much larger Gemma3-27B by 2.6\\% on AIME 24, 0.6\\% on AIME 25, and 3.0\\% on Livecodebench. These results highlight the potential of state space models as efficient and scalable alternatives to attention-based architectures for high-capacity reasoning.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤§è¯­è¨€æ¨¡å‹åœ¨å¤æ‚æ¨ç†ä»»åŠ¡ä¸­ç”±äºä¾èµ– Transformer æ¶æ„å¯¼è‡´çš„æ•ˆç‡ç“¶é¢ˆï¼Œæå‡ºäº† Scaling Reasoning without Attentionã€‚è¯¥æ¨¡å‹åŸºäº Mamba-2 çš„çŠ¶æ€ç©ºé—´å¯¹å¶(State Space Dual, SSD)å±‚æ„å»ºï¼Œå®Œå…¨æ‘’å¼ƒäº†è‡ªæ³¨æ„åŠ›æœºåˆ¶(Self-attention)å’Œé”®å€¼ç¼“å­˜(KV-caching)ï¼Œå®ç°äº†å›ºå®šå†…å­˜æ¶ˆè€—ä¸æ’å®šæ¨ç†æ—¶é—´ã€‚ä¸ºäº†å¼ºåŒ–æ¨ç†æ€§èƒ½ï¼Œç ”ç©¶è€…å¼•å…¥äº†åŸºäº PromptCoT åˆæˆèŒƒå¼çš„ä¸¤é˜¶æ®µè¯¾ç¨‹å¾®è°ƒç­–ç•¥ï¼Œé€šè¿‡æŠ½è±¡æ¦‚å¿µé€‰æ‹©å’Œé€»è¾‘å¯¼å‘ç”Ÿæˆæä¾›ç»“æ„åŒ–çš„è®­ç»ƒæ•°æ®ã€‚åŸºå‡†æµ‹è¯•ç»“æœè¡¨æ˜ï¼Œè¯¥ 7B è§„æ¨¡çš„æ¨¡å‹åœ¨ AIME 24ã€AIME 25 ä»¥åŠ Livecodebench ä¸Šå‡ä¼˜äºåŒè§„æ¨¡çš„ Transformer æ¨¡å‹ï¼Œç”šè‡³åœ¨æ€§èƒ½ä¸Šè¶…è¶Šäº†å‚æ•°é‡å¤§å¾—å¤šçš„ Gemma3-27Bã€‚è¯¥ç ”ç©¶ç»“æœçªæ˜¾äº†çŠ¶æ€ç©ºé—´æ¨¡å‹(State Space Models)åœ¨å¤„ç†é«˜éš¾åº¦æ¨ç†ä»»åŠ¡æ—¶ï¼Œä½œä¸ºä¸€ç§é«˜æ•ˆä¸”å¯æ‰©å±•çš„æ¶æ„æ›¿ä»£æ–¹æ¡ˆçš„å·¨å¤§æ½œåŠ›ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.LG",
      "comment": "preprint",
      "pdf_url": "https://arxiv.org/pdf/2505.22425v1",
      "published_date": "2025-05-28 14:52:15 UTC",
      "updated_date": "2025-05-28 14:52:15 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T15:23:12.932950+00:00"
    },
    {
      "arxiv_id": "2505.22411v2",
      "title": "Mitigating Overthinking in Large Reasoning Models via Manifold Steering",
      "title_zh": "é€šè¿‡æµå½¢å¼•å¯¼ç¼“è§£å¤§å‹æ¨ç†æ¨¡å‹çš„è¿‡åº¦æ€è€ƒ",
      "authors": [
        "Yao Huang",
        "Huanran Chen",
        "Shouwei Ruan",
        "Yichi Zhang",
        "Xingxing Wei",
        "Yinpeng Dong"
      ],
      "abstract": "Recent advances in Large Reasoning Models (LRMs) have demonstrated remarkable capabilities in solving complex tasks such as mathematics and coding. However, these models frequently exhibit a phenomenon known as overthinking during inference, characterized by excessive validation loops and redundant deliberation, leading to substantial computational overheads. In this paper, we aim to mitigate overthinking by investigating the underlying mechanisms from the perspective of mechanistic interpretability. We first showcase that the tendency of overthinking can be effectively captured by a single direction in the model's activation space and the issue can be eased by intervening the activations along this direction. However, this efficacy soon reaches a plateau and even deteriorates as the intervention strength increases. We therefore systematically explore the activation space and find that the overthinking phenomenon is actually tied to a low-dimensional manifold, which indicates that the limited effect stems from the noises introduced by the high-dimensional steering direction. Based on this insight, we propose Manifold Steering, a novel approach that elegantly projects the steering direction onto the low-dimensional activation manifold given the theoretical approximation of the interference noise. Extensive experiments on DeepSeek-R1 distilled models validate that our method reduces output tokens by up to 71% while maintaining and even improving the accuracy on several mathematical benchmarks. Our method also exhibits robust cross-domain transferability, delivering consistent token reduction performance in code generation and knowledge-based QA tasks. Code is available at: https://github.com/Aries-iai/Manifold_Steering.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤§è¯­è¨€æ¨ç†æ¨¡å‹ (Large Reasoning Models) åœ¨æ¨ç†è¿‡ç¨‹ä¸­å› å†—ä½™éªŒè¯å’Œå¾ªç¯æ€è€ƒå¯¼è‡´çš„â€œè¿‡åº¦æ€è€ƒâ€ (overthinking) é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§åŸºäºæœºæ¢°è§£é‡Šæ€§ (Mechanistic Interpretability) çš„ä¼˜åŒ–æ–¹æ¡ˆã€‚ç ”ç©¶å‘ç°è¿‡åº¦æ€è€ƒç°è±¡ä¸æ¨¡å‹æ¿€æ´»ç©ºé—´ä¸­çš„ç‰¹å®šæ–¹å‘ç›¸å…³ï¼Œå¹¶è¿›ä¸€æ­¥æ­ç¤ºå…¶æ ¹æºå®é™…ä¸Šåœ¨äºä¸€ä¸ªä½ç»´æµå½¢ (Low-dimensional Manifold)ã€‚æ®æ­¤ï¼Œä½œè€…è®¾è®¡äº† Manifold Steering æ–¹æ³•ï¼Œé€šè¿‡å°†å¼•å¯¼æ–¹å‘æŠ•å½±è‡³ä½ç»´æ¿€æ´»æµå½¢å¹¶ç»“åˆå¹²æ‰°å™ªå£°çš„ç†è®ºè¿‘ä¼¼ï¼Œå®ç°äº†å¯¹æ¨ç†è¿‡ç¨‹çš„ç²¾å‡†å¹²é¢„ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯¥æ–¹æ³•åœ¨ DeepSeek-R1 è’¸é¦æ¨¡å‹ä¸Šå¯å‡å°‘é«˜è¾¾ 71% çš„ token è¾“å‡ºï¼ŒåŒæ—¶åœ¨æ•°å­¦åŸºå‡†æµ‹è¯•ä¸­ä¿æŒç”šè‡³æå‡äº†å‡†ç¡®ç‡ã€‚æ­¤å¤–ï¼ŒManifold Steering åœ¨ä»£ç ç”Ÿæˆå’ŒçŸ¥è¯†é—®ç­”ç­‰ä»»åŠ¡ä¸­ä¹Ÿå±•ç°å‡ºå“è¶Šçš„è·¨åœºæ™¯è¿ç§»èƒ½åŠ›ä¸å‡ç¢³å¢æ•ˆæ½œåŠ›ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.LG",
      "comment": "19 pages, 7 figures",
      "pdf_url": "https://arxiv.org/pdf/2505.22411v2",
      "published_date": "2025-05-28 14:39:26 UTC",
      "updated_date": "2025-11-16 12:12:16 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T15:24:06.568003+00:00"
    },
    {
      "arxiv_id": "2505.22698v1",
      "title": "Design and testing of an agent chatbot supporting decision making with public transport data",
      "title_zh": "æ”¯æŒå…¬å…±äº¤é€šæ•°æ®å†³ç­–çš„æ™ºèƒ½ä½“èŠå¤©æœºå™¨äººçš„è®¾è®¡ä¸æµ‹è¯•",
      "authors": [
        "Luca Fantin",
        "Marco Antonelli",
        "Margherita Cesetti",
        "Daniele Irto",
        "Bruno Zamengo",
        "Francesco Silvestri"
      ],
      "abstract": "Assessing the quality of public transportation services requires the analysis of large quantities of data on the scheduled and actual trips and documents listing the quality constraints each service needs to meet. Interrogating such datasets with SQL queries, organizing and visualizing the data can be quite complex for most users. This paper presents a chatbot offering a user-friendly tool to interact with these datasets and support decision making. It is based on an agent architecture, which expands the capabilities of the core Large Language Model (LLM) by allowing it to interact with a series of tools that can execute several tasks, like performing SQL queries, plotting data and creating maps from the coordinates of a trip and its stops. This paper also tackles one of the main open problems of such Generative AI projects: collecting data to measure the system's performance. Our chatbot has been extensively tested with a workflow that asks several questions and stores the generated query, the retrieved data and the natural language response for each of them. Such questions are drawn from a set of base examples which are then completed with actual data from the database. This procedure yields a dataset for the evaluation of the chatbot's performance, especially the consistency of its answers and the correctness of the generated queries.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å…¬å…±äº¤é€šæœåŠ¡è´¨é‡è¯„ä¼°ä¸­ SQL æŸ¥è¯¢å’Œæ•°æ®å¯è§†åŒ–é—¨æ§›è¾ƒé«˜çš„é—®é¢˜ï¼Œå¼€å‘äº†ä¸€æ¬¾æ”¯æŒå†³ç­–çš„æ™ºèƒ½ä½“èŠå¤©æœºå™¨äºº (Agent Chatbot)ã€‚è¯¥ç³»ç»ŸåŸºäºå¤§è¯­è¨€æ¨¡å‹ (LLM) çš„æ™ºèƒ½ä½“æ¶æ„ï¼Œé€šè¿‡é›†æˆ SQL æŸ¥è¯¢ã€æ•°æ®ç»˜å›¾å’Œè¡Œç¨‹åœ°å›¾ç”Ÿæˆç­‰å·¥å…·ï¼Œæ˜¾è‘—å¢å¼ºäº†ç”¨æˆ·ä¸å¤æ‚äº¤é€šæ•°æ®é›†äº¤äº’çš„èƒ½åŠ›ã€‚ä¸ºäº†è§£å†³ç”Ÿæˆå¼äººå·¥æ™ºèƒ½ (Generative AI) æ€§èƒ½è¯„ä¼°ä¸­æ•°æ®é‡‡é›†éš¾çš„é—®é¢˜ï¼Œç ”ç©¶æå‡ºäº†ä¸€ç§åŸºäºåŸºç¡€ç¤ºä¾‹ä¸çœŸå®æ•°æ®åº“å†…å®¹è‡ªåŠ¨ç”Ÿæˆé—®é¢˜çš„æµ‹è¯•å·¥ä½œæµã€‚è¯¥å·¥ä½œæµä¸ä»…èƒ½å¤Ÿè‡ªåŠ¨åŒ–æ„å»ºè¯„ä¼°æ•°æ®é›†ï¼Œè¿˜èƒ½ç³»ç»Ÿåœ°éªŒè¯ç³»ç»Ÿç”ŸæˆæŸ¥è¯¢çš„æ­£ç¡®æ€§ä»¥åŠè‡ªç„¶è¯­è¨€å›ç­”çš„ä¸€è‡´æ€§ã€‚å®éªŒé€šè¿‡è®°å½•ç”Ÿæˆçš„æŸ¥è¯¢ã€æ£€ç´¢åˆ°çš„æ•°æ®å’Œæœ€ç»ˆå›å¤ï¼Œä¸ºä¼˜åŒ–èŠå¤©æœºå™¨äººåœ¨äº¤é€šå†³ç­–æ”¯æŒä¸­çš„è¡¨ç°æä¾›äº†å¯é çš„æ•°æ®æ”¯æŒä¸é‡åŒ–ä¾æ®ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.22698v1",
      "published_date": "2025-05-28 14:31:14 UTC",
      "updated_date": "2025-05-28 14:31:14 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T15:23:26.616611+00:00"
    },
    {
      "arxiv_id": "2505.22391v1",
      "title": "Physics-Informed Distillation of Diffusion Models for PDE-Constrained Generation",
      "title_zh": "ç”¨äº PDE çº¦æŸç”Ÿæˆçš„ç‰©ç†ä¿¡æ¯æ‰©æ•£æ¨¡å‹è’¸é¦",
      "authors": [
        "Yi Zhang",
        "Difan Zou"
      ],
      "abstract": "Modeling physical systems in a generative manner offers several advantages, including the ability to handle partial observations, generate diverse solutions, and address both forward and inverse problems. Recently, diffusion models have gained increasing attention in the modeling of physical systems, particularly those governed by partial differential equations (PDEs). However, diffusion models only access noisy data $\\boldsymbol{x}_t$ at intermediate steps, making it infeasible to directly enforce constraints on the clean sample $\\boldsymbol{x}_0$ at each noisy level. As a workaround, constraints are typically applied to the expectation of clean samples $\\mathbb{E}[\\boldsymbol{x}_0|\\boldsymbol{x}_t]$, which is estimated using the learned score network. However, imposing PDE constraints on the expectation does not strictly represent the one on the true clean data, known as Jensen's Gap. This gap creates a trade-off: enforcing PDE constraints may come at the cost of reduced accuracy in generative modeling. To address this, we propose a simple yet effective post-hoc distillation approach, where PDE constraints are not injected directly into the diffusion process, but instead enforced during a post-hoc distillation stage. We term our method as Physics-Informed Distillation of Diffusion Models (PIDDM). This distillation not only facilitates single-step generation with improved PDE satisfaction, but also support both forward and inverse problem solving and reconstruction from randomly partial observation. Extensive experiments across various PDE benchmarks demonstrate that PIDDM significantly improves PDE satisfaction over several recent and competitive baselines, such as PIDM, DiffusionPDE, and ECI-sampling, with less computation overhead. Our approach can shed light on more efficient and effective strategies for incorporating physical constraints into diffusion models.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹æ‰©æ•£æ¨¡å‹ (Diffusion Models) åœ¨å—åå¾®åˆ†æ–¹ç¨‹ (PDE) çº¦æŸçš„ç‰©ç†ç³»ç»Ÿå»ºæ¨¡ä¸­ï¼Œå›  Jensen's Gap å¯¼è‡´çš„ç‰©ç†çº¦æŸæ»¡è¶³åº¦ä¸ç”Ÿæˆå‡†ç¡®æ€§ä¹‹é—´çš„æƒè¡¡é—®é¢˜ï¼Œæå‡ºäº†ç‰©ç†ä¿¡æ¯å¼•å¯¼çš„æ‰©æ•£æ¨¡å‹è’¸é¦æ–¹æ³• (Physics-Informed Distillation of Diffusion Models, PIDDM)ã€‚ä¸åŒäºä¼ ç»Ÿåœ¨æ‰©æ•£è¿‡ç¨‹ä¸­ç›´æ¥æ³¨å…¥çº¦æŸçš„åšæ³•ï¼ŒPIDDM é‡‡ç”¨ä¸€ç§åéªŒè’¸é¦ (post-hoc distillation) ç­–ç•¥ï¼Œåœ¨è’¸é¦é˜¶æ®µå¼ºåˆ¶æ‰§è¡Œ PDE çº¦æŸã€‚è¿™ç§æ–¹æ³•ä¸ä»…å®ç°äº†æ»¡è¶³ç‰©ç†ç‰¹æ€§çš„å•æ­¥ç”Ÿæˆï¼Œè¿˜æ”¯æŒæ­£å‘ä¸åå‘é—®é¢˜çš„æ±‚è§£ä»¥åŠåŸºäºéšæœºéƒ¨åˆ†è§‚æµ‹çš„é‡å»ºä»»åŠ¡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒPIDDM åœ¨å¤šä¸ª PDE åŸºå‡†æµ‹è¯•ä¸­æ˜¾è‘—æå‡äº†ç‰©ç†ä¸€è‡´æ€§ï¼Œä¸”åœ¨è®¡ç®—å¼€é”€ä¸Šä¼˜äº PIDMã€DiffusionPDE å’Œ ECI-sampling ç­‰åŸºçº¿æ¨¡å‹ï¼Œä¸ºé«˜æ•ˆæ•´åˆç‰©ç†çº¦æŸä¸ç”Ÿæˆå¼å»ºæ¨¡æä¾›äº†æ–°æ€è·¯ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CE",
        "math.NA"
      ],
      "primary_category": "cs.LG",
      "comment": "23 pages, 5 figures, 4 tables",
      "pdf_url": "https://arxiv.org/pdf/2505.22391v1",
      "published_date": "2025-05-28 14:17:58 UTC",
      "updated_date": "2025-05-28 14:17:58 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T15:23:32.689436+00:00"
    },
    {
      "arxiv_id": "2505.22389v4",
      "title": "Train with Perturbation, Infer after Merging: A Two-Stage Framework for Continual Learning",
      "title_zh": "æ‰°åŠ¨è®­ç»ƒï¼Œåˆå¹¶æ¨ç†ï¼šä¸€ç§ç”¨äºæŒç»­å­¦ä¹ çš„ä¸¤é˜¶æ®µæ¡†æ¶",
      "authors": [
        "Haomiao Qiu",
        "Miao Zhang",
        "Ziyue Qiao",
        "Liqiang Nie"
      ],
      "abstract": "Continual Learning (CL) aims to enable models to continuously acquire new knowledge from a sequence of tasks with avoiding the forgetting of learned information. However, existing CL methods only rely on the parameters of the most recent task for inference, which makes them susceptible to catastrophic forgetting. Inspired by the recent success of model merging techniques, we propose \\textbf{Perturb-and-Merge (P\\&M)}, a novel continual learning framework that integrates model merging into the CL paradigm to mitigate forgetting. Specifically, after training on each task, P\\&M constructs a new model by forming a convex combination of the previous model and the newly trained task-specific model. Through theoretical analysis, We minimize the total loss increase across all tasks and derive a closed-form solution for the merging coefficient under mild assumptions. To further improve the performance of the merged model, we observe that the degradation introduced during merging can be alleviated by a regularization term composed of the task vector and the Hessian matrix of the loss function. Interestingly, we show that this term can be efficiently approximated using second-order symmetric finite differences, and a stochastic perturbation strategy along the task vector direction is accordingly devised which incurs no additional forward or backward passes while providing an effective approximation of the regularization term. Finally, we combine P\\&M with LoRA, a parameter-efficient fine-tuning method, to reduce memory overhead. Our proposed approach achieves state-of-the-art performance on several continual learning benchmark datasets. The code is available at https://github.com/qhmiao/P-M-for-Continual-Learning.",
      "tldr_zh": "è¿™é¡¹ç ”ç©¶æå‡ºäº†Perturb-and-Merge (P&M)ï¼Œä¸€ä¸ªæ—¨åœ¨ç¼“è§£æŒç»­å­¦ä¹ (Continual Learning)ä¸­ç¾éš¾æ€§é—å¿˜(catastrophic forgetting)çš„æ–°å‹ä¸¤é˜¶æ®µæ¡†æ¶ã€‚è¯¥æ¡†æ¶å°†æ¨¡å‹åˆå¹¶(model merging)æŠ€æœ¯å¼•å…¥æŒç»­å­¦ä¹ èŒƒå¼ï¼Œåœ¨å®Œæˆæ¯é¡¹ä»»åŠ¡è®­ç»ƒåï¼Œé€šè¿‡å‰ä¸€æ¨¡å‹ä¸å½“å‰ä»»åŠ¡ç‰¹å®šæ¨¡å‹çš„å‡¸ç»„åˆ(convex combination)æ„å»ºæ–°æ¨¡å‹ï¼Œå¹¶æ¨å¯¼å‡ºäº†åˆå¹¶ç³»æ•°çš„é—­å¼è§£(closed-form solution)ã€‚ä¸ºäº†è¿›ä¸€æ­¥ä¼˜åŒ–æ€§èƒ½ï¼Œç ”ç©¶è€…åˆ©ç”¨äºŒé˜¶å¯¹ç§°æœ‰é™å·®åˆ†(second-order symmetric finite differences)å’Œéšæœºæ‰°åŠ¨ç­–ç•¥(stochastic perturbation strategy)é«˜æ•ˆè¿‘ä¼¼ç”±ä»»åŠ¡å‘é‡(task vector)å’Œæµ·æ£®çŸ©é˜µ(Hessian matrix)æ„æˆçš„æ­£åˆ™åŒ–é¡¹ã€‚è¯¥æ¡†æ¶è¿˜ç»“åˆäº†å‚æ•°é«˜æ•ˆçš„å¾®è°ƒæ–¹æ³•LoRAä»¥æ˜¾è‘—é™ä½å†…å­˜å¼€é”€ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒP&Måœ¨å¤šä¸ªæŒç»­å­¦ä¹ åŸºå‡†æ•°æ®é›†ä¸Šå‡è¾¾åˆ°äº†æœ€å…ˆè¿›(state-of-the-art)çš„æ€§èƒ½æ°´å¹³ï¼Œä¸ºæŒç»­è·å–æ–°çŸ¥è¯†å¹¶ä¿ç•™æ—§çŸ¥è¯†æä¾›äº†æœ‰æ•ˆçš„è§£å†³æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "Accepted by NeurIPS 2025",
      "pdf_url": "https://arxiv.org/pdf/2505.22389v4",
      "published_date": "2025-05-28 14:14:19 UTC",
      "updated_date": "2025-10-23 10:22:11 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T15:23:49.218211+00:00"
    },
    {
      "arxiv_id": "2505.22387v1",
      "title": "DAM: Domain-Aware Module for Multi-Domain Dataset Condensation",
      "title_zh": "DAMï¼šé¢å‘å¤šé¢†åŸŸæ•°æ®é›†å‡èšçš„é¢†åŸŸæ„ŸçŸ¥æ¨¡å—",
      "authors": [
        "Jaehyun Choi",
        "Gyojin Han",
        "Dong-Jae Lee",
        "Sunghyun Baek",
        "Junmo Kim"
      ],
      "abstract": "Dataset Condensation (DC) has emerged as a promising solution to mitigate the computational and storage burdens associated with training deep learning models. However, existing DC methods largely overlook the multi-domain nature of modern datasets, which are increasingly composed of heterogeneous images spanning multiple domains. In this paper, we extend DC and introduce Multi-Domain Dataset Condensation (MDDC), which aims to condense data that generalizes across both single-domain and multi-domain settings. To this end, we propose the Domain-Aware Module (DAM), a training-time module that embeds domain-related features into each synthetic image via learnable spatial masks. As explicit domain labels are mostly unavailable in real-world datasets, we employ frequency-based pseudo-domain labeling, which leverages low-frequency amplitude statistics. DAM is only active during the condensation process, thus preserving the same images per class (IPC) with prior methods. Experiments show that DAM consistently improves in-domain, out-of-domain, and cross-architecture performance over baseline dataset condensation methods.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†æ•°æ®é›†è’¸é¦(Dataset Condensation, DC)åœ¨å¤„ç†å…·æœ‰å¤šé¢†åŸŸå¼‚æ„ç‰¹å¾çš„ç°ä»£æ•°æ®é›†æ—¶å­˜åœ¨çš„å±€é™æ€§ã€‚ä¸ºäº†åº”å¯¹è¿™ä¸€æŒ‘æˆ˜ï¼Œä½œè€…æå‡ºäº†å¤šé¢†åŸŸæ•°æ®é›†è’¸é¦(Multi-Domain Dataset Condensation, MDDC)ä»»åŠ¡ï¼Œå¹¶è®¾è®¡äº†é¢†åŸŸæ„ŸçŸ¥æ¨¡å—(Domain-Aware Module, DAM)ã€‚DAM åœ¨è’¸é¦è¿‡ç¨‹ä¸­åˆ©ç”¨å¯å­¦ä¹ çš„ç©ºé—´æ©ç (learnable spatial masks)å°†é¢†åŸŸç›¸å…³ç‰¹å¾åµŒå…¥åˆæˆå›¾åƒä¸­ï¼Œä»è€Œæ˜¾è‘—å¢å¼ºäº†æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ã€‚é’ˆå¯¹ç°å®æ•°æ®é›†ä¸­æ˜¾å¼é¢†åŸŸæ ‡ç­¾ç¼ºå¤±çš„é—®é¢˜ï¼Œè¯¥ç ”ç©¶é‡‡ç”¨åŸºäºé¢‘ç‡çš„ä¼ªé¢†åŸŸæ ‡ç­¾(frequency-based pseudo-domain labeling)æŠ€æœ¯ï¼Œåˆ©ç”¨ä½é¢‘æŒ¯å¹…ç»Ÿè®¡ä¿¡æ¯è¿›è¡Œæ ‡æ³¨ã€‚ç”±äº DAM ä»…åœ¨è’¸é¦é˜¶æ®µå‘æŒ¥ä½œç”¨ï¼Œè¯¥æ–¹æ³•åœ¨ä¿æŒä¸å…ˆå‰æ–¹æ³•ç›¸åŒçš„æ¯ç±»å›¾åƒæ•°(Images Per Class, IPC)çš„åŒæ—¶ï¼Œä¸ä¼šå¢åŠ é¢å¤–çš„æ¨ç†è´Ÿæ‹…ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒDAM åœ¨åŸŸå†…ã€è·¨åŸŸä»¥åŠè·¨æ¶æ„æ€§èƒ½ä¸Šå‡æ˜¾è‘—ä¼˜äºåŸºå‡†æ•°æ®é›†è’¸é¦æ–¹æ³•ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.22387v1",
      "published_date": "2025-05-28 14:13:38 UTC",
      "updated_date": "2025-05-28 14:13:38 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T15:23:54.630736+00:00"
    },
    {
      "arxiv_id": "2505.22384v1",
      "title": "Exact Algorithms and Lower Bounds for Forming Coalitions of Constrained Maximum Size",
      "title_zh": "å—é™æœ€å¤§è§„æ¨¡è”ç›Ÿå½¢æˆçš„ç²¾ç¡®ç®—æ³•ä¸ä¸‹ç•Œ",
      "authors": [
        "Foivos Fioravantes",
        "Harmender Gahlawat",
        "Nikolaos Melissinos"
      ],
      "abstract": "Imagine we want to split a group of agents into teams in the most \\emph{efficient} way, considering that each agent has their own preferences about their teammates. This scenario is modeled by the extensively studied \\textsc{Coalition Formation} problem. Here, we study a version of this problem where each team must additionally be of bounded size.\n  We conduct a systematic algorithmic study, providing several intractability results as well as multiple exact algorithms that scale well as the input grows (FPT), which could prove useful in practice.\n  Our main contribution is an algorithm that deals efficiently with tree-like structures (bounded \\emph{treewidth}) for ``small'' teams. We complement this result by proving that our algorithm is asymptotically optimal. Particularly, there can be no algorithm that vastly outperforms the one we present, under reasonable theoretical assumptions, even when considering star-like structures (bounded \\emph{vertex cover number}).",
      "tldr_zh": "æœ¬ç ”ç©¶æ¢è®¨äº†åœ¨é™å®šå›¢é˜Ÿæœ€å¤§è§„æ¨¡çš„å‰æä¸‹ï¼Œå¦‚ä½•æ ¹æ®ä»£ç†äººåå¥½è¿›è¡Œæœ€æœ‰æ•ˆåˆ†ç»„çš„ Coalition Formation é—®é¢˜ã€‚ä½œè€…å¯¹è¯¥é—®é¢˜è¿›è¡Œäº†ç³»ç»Ÿçš„ç®—æ³•ç ”ç©¶ï¼Œæä¾›äº†å¤šä¸ªå…³äºè®¡ç®—å¤æ‚æ€§çš„éš¾è§£æ€§ç»“æœï¼Œä»¥åŠåœ¨è¾“å…¥è§„æ¨¡å¢é•¿æ—¶è¡¨ç°è‰¯å¥½çš„å›ºå®šå‚æ•°å¯è§£ (FPT) ç²¾ç¡®ç®—æ³•ã€‚è®ºæ–‡çš„æ ¸å¿ƒè´¡çŒ®æ˜¯ä¸€ä¸ªé’ˆå¯¹â€œå°è§„æ¨¡â€å›¢é˜Ÿåœ¨ç±»æ ‘ç»“æ„ (bounded treewidth) ä¸‹èƒ½å¤Ÿé«˜æ•ˆè¿è¡Œçš„ç®—æ³•ã€‚ç ”ç©¶è¿›ä¸€æ­¥è¯æ˜äº†è¯¥ç®—æ³•çš„æ¸è¿‘æœ€ä¼˜æ€§ï¼Œå‘ç°åœ¨åˆç†çš„ç†è®ºå‡è®¾ä¸‹ï¼Œå³ä½¿åœ¨ç±»æ˜Ÿç»“æ„ (bounded vertex cover number) ä¸­ï¼Œä¹Ÿæ— æ³•å¼€å‘å‡ºæ€§èƒ½æ˜¾è‘—ä¼˜äºè¯¥ç®—æ³•çš„å…¶ä»–æ–¹æ¡ˆã€‚è¿™äº›ç ”ç©¶æˆæœä¸ºå¤„ç†å…·æœ‰è§„æ¨¡çº¦æŸçš„å›¢é˜Ÿç»„å»ºé—®é¢˜æä¾›äº†é‡è¦çš„ç†è®ºåŸºç¡€å’Œå…·æœ‰å®ç”¨æ½œåŠ›çš„ç®—æ³•å·¥å…·ã€‚",
      "categories": [
        "cs.DS",
        "cs.AI"
      ],
      "primary_category": "cs.DS",
      "comment": "a preliminary version appeared in AAAI 2025",
      "pdf_url": "https://arxiv.org/pdf/2505.22384v1",
      "published_date": "2025-05-28 14:11:14 UTC",
      "updated_date": "2025-05-28 14:11:14 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T15:23:58.292672+00:00"
    },
    {
      "arxiv_id": "2505.22370v3",
      "title": "SplitLoRA: Balancing Stability and Plasticity in Continual Learning Through Gradient Space Splitting",
      "title_zh": "SplitLoRAï¼šé€šè¿‡æ¢¯åº¦ç©ºé—´åˆ†å‰²å¹³è¡¡æŒç»­å­¦ä¹ ä¸­çš„ç¨³å®šæ€§å’Œå¯å¡‘æ€§",
      "authors": [
        "Haomiao Qiu",
        "Miao Zhang",
        "Ziyue Qiao",
        "Weili Guan",
        "Min Zhang",
        "Liqiang Nie"
      ],
      "abstract": "Continual Learning requires a model to learn multiple tasks in sequence while maintaining both stability:preserving knowledge from previously learned tasks, and plasticity:effectively learning new tasks. Gradient projection has emerged as an effective and popular paradigm in CL, where it partitions the gradient space of previously learned tasks into two orthogonal subspaces: a primary subspace and a minor subspace. New tasks are learned effectively within the minor subspace, thereby reducing interference with previously acquired knowledge. However, existing Gradient Projection methods struggle to achieve an optimal balance between plasticity and stability, as it is hard to appropriately partition the gradient space. In this work, we consider a continual learning paradigm based on Low-Rank Adaptation, which has gained considerable attention due to its efficiency and wide applicability, and propose a novel approach for continual learning, called SplitLoRA. We first provide a theoretical analysis of how subspace partitioning affects model stability and plasticity. Informed by this analysis, we then introduce an effective method that derives the optimal partition of the gradient space for previously learned tasks. This approach effectively balances stability and plasticity in continual learning. Experimental results on multiple datasets demonstrate that the proposed method achieves state-of-the-art performance.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹æŒç»­å­¦ä¹ (Continual Learning)ä¸­ç¨³å®šæ€§(Stability)ä¸å¯å¡‘æ€§(Plasticity)ä¹‹é—´çš„å¹³è¡¡éš¾é¢˜ï¼ŒæŒ‡å‡ºç›®å‰çš„æ¢¯åº¦æŠ•å½±(Gradient Projection)æ–¹æ³•åœ¨åˆ’åˆ†æ¢¯åº¦ç©ºé—´æ—¶éš¾ä»¥è¾¾åˆ°æœ€ä¼˜ã€‚ä¸ºæ­¤ï¼Œè®ºæ–‡æå‡ºäº†ä¸€ç§åä¸ºSplitLoRAçš„æ–°å‹æŒç»­å­¦ä¹ æ–¹æ³•ï¼Œè¯¥æ–¹æ³•å°†å¹¿æ³›åº”ç”¨çš„ä½ç§©è‡ªé€‚åº”(Low-Rank Adaptation, LoRA)æŠ€æœ¯ä¸æ¢¯åº¦ç©ºé—´åˆ†å‰²ç›¸ç»“åˆã€‚ä½œè€…é¦–å…ˆé€šè¿‡ç†è®ºåˆ†ææ¢è®¨äº†å­ç©ºé—´åˆ’åˆ†å¯¹æ¨¡å‹æ€§èƒ½çš„å½±å“ï¼Œå¹¶æ®æ­¤æ¨å¯¼å‡ºä¸€ç§èƒ½å¤Ÿä¸ºå…ˆå‰å­¦ä¹ ä»»åŠ¡è·å–æœ€ä¼˜æ¢¯åº¦ç©ºé—´åˆ’åˆ†çš„æœ‰æ•ˆæ–¹æ¡ˆã€‚è¿™ç§æ–¹æ³•é€šè¿‡åœ¨æ­£äº¤å­ç©ºé—´ä¸­éš”ç¦»ä»»åŠ¡æ¢¯åº¦ï¼Œæœ‰æ•ˆç¼“è§£äº†æ–°ä»»åŠ¡å­¦ä¹ å¯¹æ—§çŸ¥è¯†çš„å¹²æ‰°ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒSplitLoRAåœ¨å¤šä¸ªåŸºå‡†æ•°æ®é›†ä¸Šå‡æ˜¾è‘—æå‡äº†æ¨¡å‹è¡¨ç°ï¼Œè¾¾åˆ°äº†State-of-the-artæ°´å¹³ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "18 pages, 4 figures",
      "pdf_url": "https://arxiv.org/pdf/2505.22370v3",
      "published_date": "2025-05-28 13:57:56 UTC",
      "updated_date": "2025-06-11 12:54:39 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T15:24:33.700951+00:00"
    },
    {
      "arxiv_id": "2505.22368v1",
      "title": "AgentDNS: A Root Domain Naming System for LLM Agents",
      "title_zh": "AgentDNSï¼šé¢å‘å¤§è¯­è¨€æ¨¡å‹æ™ºèƒ½ä½“çš„æ ¹åŸŸåå‘½åç³»ç»Ÿ",
      "authors": [
        "Enfang Cui",
        "Yujun Cheng",
        "Rui She",
        "Dan Liu",
        "Zhiyuan Liang",
        "Minxin Guo",
        "Tianzheng Li",
        "Qian Wei",
        "Wenjuan Xing",
        "Zhijie Zhong"
      ],
      "abstract": "The rapid evolution of Large Language Model (LLM) agents has highlighted critical challenges in cross-vendor service discovery, interoperability, and communication. Existing protocols like model context protocol and agent-to-agent protocol have made significant strides in standardizing interoperability between agents and tools, as well as communication among multi-agents. However, there remains a lack of standardized protocols and solutions for service discovery across different agent and tool vendors. In this paper, we propose AgentDNS, a root domain naming and service discovery system designed to enable LLM agents to autonomously discover, resolve, and securely invoke third-party agent and tool services across organizational and technological boundaries. Inspired by the principles of the traditional DNS, AgentDNS introduces a structured mechanism for service registration, semantic service discovery, secure invocation, and unified billing. We detail the architecture, core functionalities, and use cases of AgentDNS, demonstrating its potential to streamline multi-agent collaboration in real-world scenarios. The source code will be published on https://github.com/agentdns.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤§è¯­è¨€æ¨¡å‹ (LLM) æ™ºèƒ½ä½“åœ¨è·¨ä¾›åº”å•†æœåŠ¡å‘ç°å’Œäº’æ“ä½œæ€§æ–¹é¢çš„ç¼ºå¤±ï¼Œæå‡ºäº† AgentDNSï¼Œä¸€ç§ç”¨äº LLM agents çš„æ ¹åŸŸååŠæœåŠ¡å‘ç°ç³»ç»Ÿã€‚å—ä¼ ç»Ÿ Domain Naming System (DNS) åŸç†å¯å‘ï¼ŒAgentDNS æ„å»ºäº†ä¸€å¥—æ ‡å‡†åŒ–çš„æ¶æ„ï¼Œæ¶µç›–æœåŠ¡æ³¨å†Œã€è¯­ä¹‰æœåŠ¡å‘ç° (Semantic Service Discovery)ã€å®‰å…¨è°ƒç”¨ä»¥åŠç»Ÿä¸€è®¡è´¹æœºåˆ¶ã€‚è¯¥ç³»ç»Ÿæ—¨åœ¨å…è®¸ LLM agents è·¨è¶Šç»„ç»‡å’ŒæŠ€æœ¯è¾¹ç•Œï¼Œè‡ªä¸»åœ°å‘ç°ã€è§£æå¹¶å®‰å…¨è°ƒç”¨ç¬¬ä¸‰æ–¹æ™ºèƒ½ä½“ä¸å·¥å…·æœåŠ¡ã€‚é€šè¿‡è¯¦ç»†é˜è¿°å…¶æ¶æ„ä¸æ ¸å¿ƒåŠŸèƒ½ï¼Œç ”ç©¶å±•ç¤ºäº† AgentDNS åœ¨ç®€åŒ–å®é™…åœºæ™¯ä¸­å¤šæ™ºèƒ½ä½“åä½œ (Multi-agent Collaboration) çš„å·¨å¤§æ½œåŠ›ã€‚è¯¥æ–¹æ¡ˆä¸ºè§£å†³æ™ºèƒ½ä½“ç”Ÿæ€ä¸­çš„æœåŠ¡å‘ç°éš¾é¢˜æä¾›äº†é‡è¦è¡¥å……ï¼Œå…¶æºä»£ç ä¹Ÿå°†å…¬å¼€å‘å¸ƒåœ¨ GitHubã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "7 pages, 6 figures",
      "pdf_url": "https://arxiv.org/pdf/2505.22368v1",
      "published_date": "2025-05-28 13:56:22 UTC",
      "updated_date": "2025-05-28 13:56:22 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T15:24:29.207983+00:00"
    },
    {
      "arxiv_id": "2505.22358v2",
      "title": "Adaptive Budget Allocation for Orthogonal-Subspace Adapter Tuning in LLMs Continual Learning",
      "title_zh": "LLMs æŒç»­å­¦ä¹ ä¸­æ­£äº¤å­ç©ºé—´é€‚é…å™¨å¾®è°ƒçš„è‡ªé€‚åº”é¢„ç®—åˆ†é…",
      "authors": [
        "Zhiyi Wan",
        "Wanrou Du",
        "Liang Li",
        "Miao Pan",
        "Xiaoqi Qin"
      ],
      "abstract": "Large language models (LLMs) often suffer from catastrophic forgetting in continual learning (CL) scenarios, where performance on previously learned tasks degrades severely while training on sequentially arriving tasks. Although pioneering CL approaches using orthogonal subspaces can mitigate task interference, they typically employ fixed budget allocation, neglecting the varying complexity across tasks and layers. Besides, recent budget-adaptive tuning methods for LLMs often adopt multi-stage paradigms that decouple optimization and budget allocation. Such decoupling results in potential misalignment, which hinders those approaches' practical application in CL scenarios. To address these limitations, we propose OA-Adapter, a novel parameter-efficient approach for continual learning in LLMs that unifies dynamic budget adaptation with orthogonal subspace learning in an end-to-end training stage. Specifically, OA-Adapter introduces a dynamic bottleneck dimension adaptation mechanism that simultaneously allocates an efficient parameter budget and optimizes task objectives without misalignment.To effectively preserve previously acquired knowledge while coordinating with the dynamic budget allocation, orthogonal constraints are applied specifically between the parameter subspace of the current task and the dynamically allocated parameter subspaces of historical tasks. Experimental results on continual learning benchmarks demonstrate that OA-Adapter outperforms state-of-the-art methods in both accuracy and parameter efficiency. OA-Adapter achieves higher average accuracy while using 58.5% fewer parameters on the standard CL benchmark, and maintains its advantages on two larger benchmarks comprising 15 tasks.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤§è¯­è¨€æ¨¡å‹(LLMs)åœ¨æŒç»­å­¦ä¹ (Continual Learning)ä¸­é¢ä¸´çš„ç¾éš¾æ€§é—å¿˜(catastrophic forgetting)æŒ‘æˆ˜ï¼Œæå‡ºäº†ä¸€ç§åä¸ºOA-Adapterçš„å‚æ•°é«˜æ•ˆå¾®è°ƒæ–¹æ³•ã€‚ä¼ ç»Ÿçš„æ­£äº¤å­ç©ºé—´(orthogonal subspaces)æ–¹æ¡ˆå¾€å¾€é‡‡ç”¨å›ºå®šé¢„ç®—åˆ†é…ï¼Œä¸”ç°æœ‰è‡ªé€‚åº”æ–¹æ³•å­˜åœ¨ä¼˜åŒ–ä¸é¢„ç®—åˆ†é…ä¸åŒ¹é…çš„é—®é¢˜ã€‚OA-Adapterå¼•å…¥äº†åŠ¨æ€ç“¶é¢ˆç»´åº¦è‡ªé€‚åº”æœºåˆ¶ï¼Œåœ¨ç«¯åˆ°ç«¯è®­ç»ƒä¸­åŒæ­¥å®Œæˆå‚æ•°é¢„ç®—åˆ†é…ä¸ä»»åŠ¡ç›®æ ‡ä¼˜åŒ–ï¼Œé¿å…äº†ç”±äºé˜¶æ®µæ€§è§£è€¦å¸¦æ¥çš„ misalignment é—®é¢˜ã€‚é€šè¿‡åœ¨å½“å‰ä»»åŠ¡ä¸å†å²ä»»åŠ¡çš„åŠ¨æ€åˆ†é…å­ç©ºé—´ä¹‹é—´æ–½åŠ æ­£äº¤çº¦æŸï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿåœ¨åè°ƒé¢„ç®—çš„åŒæ—¶æœ‰æ•ˆä¿ç•™å…ˆå‰ä¹ å¾—çš„çŸ¥è¯†ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒOA-Adapteråœ¨å‡†ç¡®ç‡å’Œå‚æ•°æ•ˆç‡ä¸Šå‡ä¼˜äºç°æœ‰æœ€å…ˆè¿›æ–¹æ³•ï¼Œåœ¨æ ‡å‡†åŸºå‡†æµ‹è¯•ä¸­å‡å°‘äº†58.5%çš„å‚æ•°é‡å¹¶å®ç°äº†æ›´é«˜çš„å¹³å‡å‡†ç¡®ç‡ã€‚æ­¤å¤–ï¼Œåœ¨åŒ…å«15ä¸ªä»»åŠ¡çš„å¤§è§„æ¨¡æµ‹è¯•ä¸­ï¼Œè¯¥æ–¹æ³•ä¾ç„¶ä¿æŒäº†æ˜¾è‘—ä¼˜åŠ¿ï¼Œè¯æ˜äº†å…¶åœ¨å¤æ‚æŒç»­å­¦ä¹ åœºæ™¯ä¸‹çš„ç¨³å¥æ€§ä¸å®ç”¨ä»·å€¼ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.22358v2",
      "published_date": "2025-05-28 13:38:21 UTC",
      "updated_date": "2025-10-16 14:03:22 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T15:24:53.559874+00:00"
    },
    {
      "arxiv_id": "2505.22356v1",
      "title": "Suitability Filter: A Statistical Framework for Classifier Evaluation in Real-World Deployment Settings",
      "title_zh": "é€‚ç”¨æ€§è¿‡æ»¤å™¨ï¼šä¸€ç§é¢å‘å®é™…éƒ¨ç½²åœºæ™¯çš„åˆ†ç±»å™¨è¯„ä¼°ç»Ÿè®¡æ¡†æ¶",
      "authors": [
        "AngÃ©line Pouget",
        "Mohammad Yaghini",
        "Stephan Rabanser",
        "Nicolas Papernot"
      ],
      "abstract": "Deploying machine learning models in safety-critical domains poses a key challenge: ensuring reliable model performance on downstream user data without access to ground truth labels for direct validation. We propose the suitability filter, a novel framework designed to detect performance deterioration by utilizing suitability signals -- model output features that are sensitive to covariate shifts and indicative of potential prediction errors. The suitability filter evaluates whether classifier accuracy on unlabeled user data shows significant degradation compared to the accuracy measured on the labeled test dataset. Specifically, it ensures that this degradation does not exceed a pre-specified margin, which represents the maximum acceptable drop in accuracy. To achieve reliable performance evaluation, we aggregate suitability signals for both test and user data and compare these empirical distributions using statistical hypothesis testing, thus providing insights into decision uncertainty. Our modular method adapts to various models and domains. Empirical evaluations across different classification tasks demonstrate that the suitability filter reliably detects performance deviations due to covariate shift. This enables proactive mitigation of potential failures in high-stakes applications.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† Suitability Filterï¼Œè¿™æ˜¯ä¸€ä¸ªæ—¨åœ¨å®æ—¶éƒ¨ç½²ç¯å¢ƒä¸­è¯„ä¼°åˆ†ç±»å™¨æ€§èƒ½çš„ç»Ÿè®¡æ¡†æ¶ï¼Œç‰¹åˆ«é’ˆå¯¹ç¼ºä¹çœŸå€¼æ ‡ç­¾ (ground truth labels) çš„å®‰å…¨å…³é”®é¢†åŸŸã€‚è¯¥æ¡†æ¶å¼•å…¥äº† Suitability Signals (é€‚ç”¨æ€§ä¿¡å·)ï¼Œå³å¯¹åå˜é‡åç§» (covariate shifts) æ•æ„Ÿçš„æ¨¡å‹è¾“å‡ºç‰¹å¾ï¼Œç”¨äºç›‘æµ‹æ½œåœ¨çš„é¢„æµ‹é”™è¯¯ã€‚é€šè¿‡ç»Ÿè®¡å‡è®¾æ£€éªŒæ¯”è¾ƒæµ‹è¯•æ•°æ®ä¸ç”¨æˆ·æ•°æ®çš„ç»éªŒåˆ†å¸ƒï¼ŒSuitability Filter èƒ½å¤Ÿåˆ¤æ–­åˆ†ç±»å™¨å‡†ç¡®ç‡çš„ä¸‹é™æ˜¯å¦è¶…å‡ºäº†é¢„è®¾çš„å®¹å¿è¾¹é™…ã€‚è¯¥æ–¹æ³•å…·æœ‰é«˜åº¦çš„æ¨¡å—åŒ–ç‰¹æ€§ï¼Œå¯çµæ´»é€‚é…å¤šç§æ¨¡å‹å’Œé¢†åŸŸã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒSuitability Filter èƒ½åœ¨å¤šç§åˆ†ç±»ä»»åŠ¡ä¸­å¯é åœ°æ£€æµ‹å‡ºç”±åå˜é‡åç§»å¼•èµ·çš„æ€§èƒ½åå·®ï¼Œä»è€Œä¸ºé«˜é£é™©åº”ç”¨ä¸­çš„æ•…éšœé¢„è­¦å’Œä¸»åŠ¨å¹²é¢„æä¾›æ”¯æŒã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CY",
        "stat.ML"
      ],
      "primary_category": "cs.LG",
      "comment": "Accepted to ICML 2025",
      "pdf_url": "https://arxiv.org/pdf/2505.22356v1",
      "published_date": "2025-05-28 13:37:04 UTC",
      "updated_date": "2025-05-28 13:37:04 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T15:24:59.821359+00:00"
    },
    {
      "arxiv_id": "2505.22353v1",
      "title": "VME: A Satellite Imagery Dataset and Benchmark for Detecting Vehicles in the Middle East and Beyond",
      "title_zh": "VMEï¼šé¢å‘ä¸­ä¸œåŠå…¶ä»–åœ°åŒºå«æ˜Ÿå›¾åƒè½¦è¾†æ£€æµ‹çš„æ•°æ®é›†ä¸åŸºå‡†",
      "authors": [
        "Noora Al-Emadi",
        "Ingmar Weber",
        "Yin Yang",
        "Ferda Ofli"
      ],
      "abstract": "Detecting vehicles in satellite images is crucial for traffic management, urban planning, and disaster response. However, current models struggle with real-world diversity, particularly across different regions. This challenge is amplified by geographic bias in existing datasets, which often focus on specific areas and overlook regions like the Middle East. To address this gap, we present the Vehicles in the Middle East (VME) dataset, designed explicitly for vehicle detection in high-resolution satellite images from Middle Eastern countries. Sourced from Maxar, the VME dataset spans 54 cities across 12 countries, comprising over 4,000 image tiles and more than 100,000 vehicles, annotated using both manual and semi-automated methods. Additionally, we introduce the largest benchmark dataset for Car Detection in Satellite Imagery (CDSI), combining images from multiple sources to enhance global car detection. Our experiments demonstrate that models trained on existing datasets perform poorly on Middle Eastern images, while the VME dataset significantly improves detection accuracy in this region. Moreover, state-of-the-art models trained on CDSI achieve substantial improvements in global car detection.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ç°æœ‰æ•°æ®é›†åœ°ç†åå·®å¯¼è‡´æ¨¡å‹åœ¨ Middle East è¡¨ç°ä¸ä½³çš„é—®é¢˜ï¼Œæå‡ºäº†ä¸“é—¨ç”¨äºè¯¥åŒºåŸŸé«˜åˆ†è¾¨ç‡å«æ˜Ÿå›¾åƒè½¦è¾†æ£€æµ‹çš„ VME (Vehicles in the Middle East) æ•°æ®é›†ã€‚è¯¥æ•°æ®é›†è¦†ç›– 12 ä¸ªå›½å®¶çš„ 54 ä¸ªåŸå¸‚ï¼ŒåŒ…å« 4,000 å¤šä¸ªå›¾åƒåˆ†ç‰‡å’Œè¶…è¿‡ 100,000 è¾†æ ‡æ³¨è½¦è¾†ï¼Œé‡‡ç”¨äº†äººå·¥ä¸åŠè‡ªåŠ¨ç›¸ç»“åˆçš„æ ‡æ³¨æ–¹æ³•ã€‚æ­¤å¤–ï¼Œç ”ç©¶è€…é€šè¿‡æ•´åˆå¤šæºæ•°æ®æ¨å‡ºäº†ç›®å‰æœ€å¤§çš„å«æ˜Ÿå›¾åƒæ±½è½¦æ£€æµ‹åŸºå‡†æ•°æ®é›† CDSI (Car Detection in Satellite Imagery)ï¼Œæ˜¾è‘—å¢å¼ºäº†å…¨çƒèŒƒå›´å†…çš„æ£€æµ‹æ•ˆèƒ½ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒVME æ•°æ®é›†æœ‰æ•ˆæ”¹å–„äº†æ¨¡å‹åœ¨ä¸­ä¸œåœ°åŒºçš„æ£€æµ‹å‡†ç¡®ç‡ï¼Œå¼¥è¡¥äº†æ—¢å¾€æ¨¡å‹åœ¨ç‰¹å®šåœ°ç†ç¯å¢ƒä¸‹çš„æ€§èƒ½ç¼ºå£ã€‚åŒæ—¶ï¼Œåœ¨ CDSI ä¸Šè®­ç»ƒçš„ State-of-the-art æ¨¡å‹åœ¨å…¨çƒæ±½è½¦æ£€æµ‹ä»»åŠ¡ä¸­ä¹Ÿå–å¾—äº†å®è´¨æ€§çš„çªç ´ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.22353v1",
      "published_date": "2025-05-28 13:34:05 UTC",
      "updated_date": "2025-05-28 13:34:05 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T15:24:40.453431+00:00"
    },
    {
      "arxiv_id": "2505.22349v1",
      "title": "ChatPD: An LLM-driven Paper-Dataset Networking System",
      "title_zh": "ChatPDï¼šä¸€ç§å¤§è¯­è¨€æ¨¡å‹é©±åŠ¨çš„è®ºæ–‡-æ•°æ®é›†å…³è”ç³»ç»Ÿ",
      "authors": [
        "Anjie Xu",
        "Ruiqing Ding",
        "Leye Wang"
      ],
      "abstract": "Scientific research heavily depends on suitable datasets for method validation, but existing academic platforms with dataset management like PapersWithCode suffer from inefficiencies in their manual workflow. To overcome this bottleneck, we present a system, called ChatPD, that utilizes Large Language Models (LLMs) to automate dataset information extraction from academic papers and construct a structured paper-dataset network. Our system consists of three key modules: \\textit{paper collection}, \\textit{dataset information extraction}, and \\textit{dataset entity resolution} to construct paper-dataset networks. Specifically, we propose a \\textit{Graph Completion and Inference} strategy to map dataset descriptions to their corresponding entities. Through extensive experiments, we demonstrate that ChatPD not only outperforms the existing platform PapersWithCode in dataset usage extraction but also achieves about 90\\% precision and recall in entity resolution tasks. Moreover, we have deployed ChatPD to continuously extract which datasets are used in papers, and provide a dataset discovery service, such as task-specific dataset queries and similar dataset recommendations. We open source ChatPD and the current paper-dataset network on this [GitHub repository]{https://github.com/ChatPD-web/ChatPD}.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† ChatPDï¼Œä¸€ä¸ªç”±å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰é©±åŠ¨çš„è®ºæ–‡-æ•°æ®é›†è”ç½‘ç³»ç»Ÿï¼Œæ—¨åœ¨è§£å†³ PapersWithCode ç­‰ç°æœ‰å¹³å°åœ¨æ•°æ®é›†ç®¡ç†æ–¹é¢ä¾èµ–æ‰‹åŠ¨å·¥ä½œæµå¯¼è‡´çš„æ•ˆç‡ä½ä¸‹é—®é¢˜ã€‚ç³»ç»Ÿé€šè¿‡è®ºæ–‡é‡‡é›†ï¼ˆpaper collectionï¼‰ã€æ•°æ®é›†ä¿¡æ¯æå–ï¼ˆdataset information extractionï¼‰å’Œæ•°æ®é›†å®ä½“è§£æï¼ˆdataset entity resolutionï¼‰ä¸‰ä¸ªå…³é”®æ¨¡å—ï¼Œè‡ªåŠ¨åŒ–åœ°æ„å»ºç»“æ„åŒ–çš„è®ºæ–‡-æ•°æ®é›†ç½‘ç»œã€‚ç ”ç©¶ç‰¹åˆ«æå‡ºäº†ä¸€ç§å›¾è¡¥å…¨ä¸æ¨ç†ï¼ˆGraph Completion and Inferenceï¼‰ç­–ç•¥ï¼Œç”¨äºå°†è®ºæ–‡ä¸­çš„æ•°æ®é›†æè¿°å‡†ç¡®æ˜ å°„åˆ°å¯¹åº”çš„å®ä½“ã€‚å®éªŒè¯æ˜ï¼ŒChatPD åœ¨æ•°æ®é›†æå–è¡¨ç°ä¸Šä¼˜äºç°æœ‰å¹³å°ï¼Œä¸”åœ¨å®ä½“è§£æä»»åŠ¡ä¸­è¾¾åˆ°äº†çº¦ 90% çš„ç²¾ç¡®ç‡ï¼ˆprecisionï¼‰å’Œå¬å›ç‡ï¼ˆrecallï¼‰ã€‚è¯¥ç³»ç»Ÿç›®å‰å·²éƒ¨ç½²ï¼Œå¯æä¾›ç‰¹å®šä»»åŠ¡çš„æ•°æ®é›†æŸ¥è¯¢åŠç›¸ä¼¼æ•°æ®é›†æ¨èç­‰æœåŠ¡ï¼Œå¹¶å·²å¼€æºå…¶ä»£ç å’Œæ„å»ºçš„æ•°æ®é›†ç½‘ç»œã€‚",
      "categories": [
        "cs.DB",
        "cs.AI",
        "cs.IR"
      ],
      "primary_category": "cs.DB",
      "comment": "Accepted by KDD Applied Data Science Track 2025",
      "pdf_url": "https://arxiv.org/pdf/2505.22349v1",
      "published_date": "2025-05-28 13:31:08 UTC",
      "updated_date": "2025-05-28 13:31:08 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T15:24:57.509837+00:00"
    },
    {
      "arxiv_id": "2505.22343v2",
      "title": "Empowering Intelligent Low-altitude Economy with Large AI Model Deployment",
      "title_zh": "å¤§æ¨¡å‹éƒ¨ç½²èµ‹èƒ½æ™ºèƒ½åŒ–ä½ç©ºç»æµ",
      "authors": [
        "Zhonghao Lyu",
        "Yulan Gao",
        "Junting Chen",
        "Hongyang Du",
        "Jie Xu",
        "Kaibin Huang",
        "Dong In Kim"
      ],
      "abstract": "Low-altitude economy (LAE) represents an emerging economic paradigm that redefines commercial and social aerial activities. Large artificial intelligence models (LAIMs) offer transformative potential to further enhance the intelligence of LAE services. However, deploying LAIMs in LAE poses several challenges, including the significant gap between their computational/storage demands and the limited onboard resources of LAE entities, the mismatch between lab-trained LAIMs and dynamic physical environments, and the inefficiencies of traditional decoupled designs for sensing, communication, and computation. To address these issues, we first propose a hierarchical system architecture tailored for LAIM deployment and present representative LAE application scenarios. Next, we explore key enabling techniques that facilitate the mutual co-evolution of LAIMs and low-altitude systems, and introduce a task-oriented execution pipeline for scalable and adaptive service delivery. Then, the proposed framework is validated through real-world case studies. Finally, we outline open challenges to inspire future research.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†å¦‚ä½•åˆ©ç”¨å¤§è§„æ¨¡äººå·¥æ™ºèƒ½æ¨¡å‹(Large AI Models, LAIMs)èµ‹èƒ½æ™ºèƒ½ä½ç©ºç»æµ(Low-altitude Economy, LAE)çš„å‘å±•ï¼Œæ—¨åœ¨è§£å†³æœºè½½èµ„æºå—é™ä¸åŠ¨æ€ç¯å¢ƒé€‚é…ç­‰æ ¸å¿ƒæŒ‘æˆ˜ã€‚è®ºæ–‡é¦–å…ˆæå‡ºäº†ä¸€ç§ä¸“ä¸ºæ¨¡å‹éƒ¨ç½²è®¾è®¡çš„å±‚çº§åŒ–ç³»ç»Ÿæ¶æ„ï¼Œå¹¶è¯¦ç»†é˜è¿°äº†ç›¸å…³çš„ä»£è¡¨æ€§åº”ç”¨åœºæ™¯ã€‚éšåï¼Œç ”ç©¶æ¢ç´¢äº†ä¿ƒè¿›æ¨¡å‹ä¸ä½ç©ºç³»ç»Ÿå…±åŒæ¼”åŒ–çš„å…³é”®æŠ€æœ¯ï¼Œå¹¶å¼•å…¥äº†é¢å‘ä»»åŠ¡çš„æ‰§è¡Œæµæ°´çº¿ï¼Œä»¥ç¡®ä¿æœåŠ¡äº¤ä»˜çš„å¯æ‰©å±•æ€§ä¸è‡ªé€‚åº”æ€§ã€‚é€šè¿‡çœŸå®ä¸–ç•Œçš„æ¡ˆä¾‹ç ”ç©¶ï¼Œè¯¥æ¡†æ¶çš„æœ‰æ•ˆæ€§å¾—åˆ°äº†éªŒè¯ï¼ŒåŒæ—¶ä¹ŸæŒ‡å‡ºäº†è¯¥é¢†åŸŸå¾…è§£å†³çš„å¼€æ”¾æ€§æŒ‘æˆ˜ï¼Œä¸ºæœªæ¥æ™ºèƒ½åŒ–ä½ç©ºç»æµçš„ç ”ç©¶æä¾›äº†é‡è¦å‚è€ƒã€‚",
      "categories": [
        "eess.SP",
        "cs.AI"
      ],
      "primary_category": "eess.SP",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.22343v2",
      "published_date": "2025-05-28 13:27:07 UTC",
      "updated_date": "2025-07-03 14:03:18 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T15:25:02.453840+00:00"
    },
    {
      "arxiv_id": "2505.22338v1",
      "title": "Text2Grad: Reinforcement Learning from Natural Language Feedback",
      "title_zh": "Text2Gradï¼šåŸºäºè‡ªç„¶è¯­è¨€åé¦ˆçš„å¼ºåŒ–å­¦ä¹ ",
      "authors": [
        "Hanyang Wang",
        "Lu Wang",
        "Chaoyun Zhang",
        "Tianjun Mao",
        "Si Qin",
        "Qingwei Lin",
        "Saravan Rajmohan",
        "Dongmei Zhang"
      ],
      "abstract": "Traditional RLHF optimizes language models with coarse, scalar rewards that mask the fine-grained reasons behind success or failure, leading to slow and opaque learning. Recent work augments RL with textual critiques through prompting or reflection, improving interpretability but leaving model parameters untouched. We introduce Text2Grad, a reinforcement-learning paradigm that turns free-form textual feedback into span-level gradients. Given human (or programmatic) critiques, Text2Grad aligns each feedback phrase with the relevant token spans, converts these alignments into differentiable reward signals, and performs gradient updates that directly refine the offending portions of the model's policy. This yields precise, feedback-conditioned adjustments instead of global nudges. Text2Grad is realized through three components: (1) a high-quality feedback-annotation pipeline that pairs critiques with token spans; (2) a fine-grained reward model that predicts span-level reward on answer while generating explanatory critiques; and (3) a span-level policy optimizer that back-propagates natural-language gradients. Across summarization, code generation, and question answering, Text2Grad consistently surpasses scalar-reward RL and prompt-only baselines, providing both higher task metrics and richer interpretability. Our results demonstrate that natural-language feedback, when converted to gradients, is a powerful signal for fine-grained policy optimization. The code for our method is available at https://github.com/microsoft/Text2Grad",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† Text2Gradï¼Œä¸€ç§å°†è‡ªç”±æ ¼å¼çš„è‡ªç„¶è¯­è¨€åé¦ˆè½¬åŒ–ä¸º Span-level Gradients çš„å¼ºåŒ–å­¦ä¹ èŒƒå¼ï¼Œè§£å†³äº†ä¼ ç»Ÿ RLHF ä¸­ Scalar Rewards å› è¿‡äºç²—ç³™è€Œå¯¼è‡´å­¦ä¹ ç¼“æ…¢ä¸”ä¸é€æ˜çš„é—®é¢˜ã€‚Text2Grad é€šè¿‡å°†åé¦ˆçŸ­è¯­ä¸ç›¸å…³çš„ Token Spans ç²¾ç¡®å¯¹é½ï¼Œå¹¶å°†å…¶è½¬åŒ–ä¸ºå¯å¾®çš„å¥–åŠ±ä¿¡å·ï¼Œå®ç°äº†å¯¹æ¨¡å‹ Policy çš„ç›´æ¥ç»†ç²’åº¦æ›´æ–°ï¼Œè€Œéå…¨å±€æ€§çš„æ¨¡ç³Šè°ƒæ•´ã€‚è¯¥æ–¹æ³•åŒ…å«åé¦ˆæ ‡æ³¨æµæ°´çº¿ã€é¢„æµ‹ç‰‡æ®µçº§å¥–åŠ±å¹¶ç”Ÿæˆæ‰¹åˆ¤æ€§åé¦ˆçš„ Reward Modelï¼Œä»¥åŠæ‰§è¡Œè‡ªç„¶è¯­è¨€æ¢¯åº¦åå‘ä¼ æ’­çš„ Policy Optimizer ä¸‰ä¸ªæ ¸å¿ƒç»„ä»¶ã€‚åœ¨æ‘˜è¦ç”Ÿæˆã€ä»£ç ç”Ÿæˆå’Œé—®ç­”ä»»åŠ¡çš„å®éªŒä¸­ï¼ŒText2Grad çš„è¡¨ç°ä¸€è‡´ä¼˜äºä¼ ç»Ÿçš„æ ‡é‡å¥–åŠ± RL å’Œä»…ä½¿ç”¨ Prompting çš„åŸºå‡†æ¨¡å‹ã€‚ç»“æœè¯æ˜ï¼Œå°†è‡ªç„¶è¯­è¨€åé¦ˆè½¬åŒ–ä¸ºæ¢¯åº¦ä¿¡å·èƒ½å¤Ÿæ˜¾è‘—æå‡å¼ºåŒ–å­¦ä¹ çš„ä»»åŠ¡è¡¨ç°ä¸å¯è§£é‡Šæ€§ï¼Œä¸ºç»†ç²’åº¦çš„ç­–ç•¥ä¼˜åŒ–æä¾›äº†å¼ºæœ‰åŠ›çš„æ”¯æŒã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "The code for our method is available at https://github.com/microsoft/Text2Grad",
      "pdf_url": "https://arxiv.org/pdf/2505.22338v1",
      "published_date": "2025-05-28 13:23:49 UTC",
      "updated_date": "2025-05-28 13:23:49 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T15:25:10.082821+00:00"
    },
    {
      "arxiv_id": "2505.22334v2",
      "title": "Advancing Multimodal Reasoning via Reinforcement Learning with Cold Start",
      "title_zh": "é€šè¿‡å†·å¯åŠ¨å¼ºåŒ–å­¦ä¹ æå‡å¤šæ¨¡æ€æ¨ç†",
      "authors": [
        "Lai Wei",
        "Yuting Li",
        "Kaipeng Zheng",
        "Chen Wang",
        "Yue Wang",
        "Linghe Kong",
        "Lichao Sun",
        "Weiran Huang"
      ],
      "abstract": "Recent advancements in large language models (LLMs) have demonstrated impressive chain-of-thought reasoning capabilities, with reinforcement learning (RL) playing a crucial role in this progress. While \"aha moment\" patterns--where models exhibit self-correction through reflection--are often attributed to emergent properties from RL, we first demonstrate that these patterns exist in multimodal LLMs (MLLMs) prior to RL training but may not necessarily correlate with improved reasoning performance. Building on these insights, we present a comprehensive study on enhancing multimodal reasoning through a two-stage approach: (1) supervised fine-tuning (SFT) as a cold start with structured chain-of-thought reasoning patterns, followed by (2) reinforcement learning via GRPO to further refine these capabilities. Our extensive experiments show that this combined approach consistently outperforms both SFT-only and RL-only methods across challenging multimodal reasoning benchmarks. The resulting models achieve state-of-the-art performance among open-source MLLMs at both 3B and 7B scales, with our 7B model showing substantial improvements over base models (e.g., 66.3 %$\\rightarrow$73.4 % on MathVista, 62.9 %$\\rightarrow$70.4 % on We-Math) and our 3B model achieving performance competitive with several 7B models. Overall, this work provides practical guidance for building advanced multimodal reasoning models. Our code is available at https://github.com/waltonfuture/RL-with-Cold-Start.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹(MLLMs)çš„æ¨ç†èƒ½åŠ›ï¼ŒæŒ‡å‡ºæ¨¡å‹åœ¨å¼ºåŒ–å­¦ä¹ (RL)å‰è™½å·²è¡¨ç°å‡ºè‡ªæˆ‘ä¿®æ­£çš„â€œaha momentâ€æ¨¡å¼ï¼Œä½†å…¶ä¸æ€§èƒ½æå‡æœªå¿…æ­£å‘ç›¸å…³ã€‚ä¸ºæ­¤ï¼Œä½œè€…æå‡ºäº†ä¸€ç§ä¸¤é˜¶æ®µå¢å¼ºæ–¹æ¡ˆï¼šé¦–å…ˆé€šè¿‡æœ‰ç›‘ç£å¾®è°ƒ(SFT)å¼•å…¥ç»“æ„åŒ–é“¾å¼æ€ç»´(Chain-of-Thought)æ¨¡å¼è¿›è¡Œå†·å¯åŠ¨ï¼Œéšåé‡‡ç”¨GRPOç®—æ³•è¿›è¡Œå¼ºåŒ–å­¦ä¹ ä»¥ç²¾ç‚¼æ¨ç†èƒ½åŠ›ã€‚å®éªŒè¡¨æ˜ï¼Œè¿™ç§ç»“åˆå†·å¯åŠ¨ä¸RLçš„æ–¹æ³•åœ¨å¤šæ¨¡æ€æ¨ç†åŸºå‡†æµ‹è¯•ä¸­æŒç»­ä¼˜äºå•ä¸€é˜¶æ®µæ–¹æ³•ã€‚è¯¥ç³»åˆ—æ¨¡å‹åœ¨3Bå’Œ7Bè§„æ¨¡çš„å¼€æºMLLMsä¸­è¾¾åˆ°äº†state-of-the-artæ€§èƒ½ï¼Œä¾‹å¦‚7Bæ¨¡å‹åœ¨MathVistaå’ŒWe-Mathä¸Šçš„å‡†ç¡®ç‡åˆ†åˆ«æå‡è‡³73.4%å’Œ70.4%ã€‚è¿™é¡¹å·¥ä½œä¸ºæ„å»ºé«˜æ€§èƒ½å¤šæ¨¡æ€æ¨ç†æ¨¡å‹æä¾›äº†é‡è¦çš„å®è·µæŒ‡å¯¼ï¼Œå¹¶è¯æ˜äº†ç»“æ„åŒ–å†·å¯åŠ¨å¯¹å¼ºåŒ–å­¦ä¹ æ•ˆæœçš„å…³é”®ä½œç”¨ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.CV",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.22334v2",
      "published_date": "2025-05-28 13:21:38 UTC",
      "updated_date": "2025-07-23 07:37:08 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T15:25:06.959620+00:00"
    },
    {
      "arxiv_id": "2505.22312v2",
      "title": "Skywork Open Reasoner 1 Technical Report",
      "title_zh": "Skywork Open Reasoner 1 æŠ€æœ¯æŠ¥å‘Š",
      "authors": [
        "Jujie He",
        "Jiacai Liu",
        "Chris Yuhao Liu",
        "Rui Yan",
        "Chaojie Wang",
        "Peng Cheng",
        "Xiaoyu Zhang",
        "Fuxiang Zhang",
        "Jiacheng Xu",
        "Wei Shen",
        "Siyuan Li",
        "Liang Zeng",
        "Tianwen Wei",
        "Cheng Cheng",
        "Bo An",
        "Yang Liu",
        "Yahui Zhou"
      ],
      "abstract": "The success of DeepSeek-R1 underscores the significant role of reinforcement learning (RL) in enhancing the reasoning capabilities of large language models (LLMs). In this work, we present Skywork-OR1, an effective and scalable RL implementation for long Chain-of-Thought (CoT) models. Building on the DeepSeek-R1-Distill model series, our RL approach achieves notable performance gains, increasing average accuracy across AIME24, AIME25, and LiveCodeBench from 57.8% to 72.8% (+15.0%) for the 32B model and from 43.6% to 57.5% (+13.9%) for the 7B model. Our Skywork-OR1-32B model surpasses both DeepSeek-R1 and Qwen3-32B on the AIME24 and AIME25 benchmarks, while achieving comparable results on LiveCodeBench. The Skywork-OR1-7B and Skywork-OR1-Math-7B models demonstrate competitive reasoning capabilities among models of similar size. We perform comprehensive ablation studies on the core components of our training pipeline to validate their effectiveness. Additionally, we thoroughly investigate the phenomenon of entropy collapse, identify key factors affecting entropy dynamics, and demonstrate that mitigating premature entropy collapse is critical for improved test performance. To support community research, we fully open-source our model weights, training code, and training datasets.",
      "tldr_zh": "è¯¥æŠ€æœ¯æŠ¥å‘Šä»‹ç»äº† Skywork-OR1ï¼Œä¸€ç§é’ˆå¯¹é•¿é“¾å¼æ€ç»´ (Chain-of-Thought, CoT) æ¨¡å‹çš„é«˜æ•ˆä¸”å¯æ‰©å±•çš„å¼ºåŒ–å­¦ä¹  (Reinforcement Learning, RL) å®ç°æ–¹æ¡ˆã€‚è¯¥å·¥ä½œåŸºäº DeepSeek-R1-Distill ç³»åˆ—æ¨¡å‹ï¼Œé€šè¿‡ä¼˜åŒ– RL æµç¨‹æ˜¾è‘—æå‡äº†å¤§å‹è¯­è¨€æ¨¡å‹çš„å¤æ‚æ¨ç†èƒ½åŠ›ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒSkywork-OR1-32B åœ¨ AIME24ã€AIME25 å’Œ LiveCodeBench ä¸Šçš„å¹³å‡å‡†ç¡®ç‡ä» 57.8% æå‡è‡³ 72.8%ï¼Œåœ¨ AIME åŸºå‡†æµ‹è¯•ä¸­ç”šè‡³è¶…è¶Šäº†åŸå§‹ DeepSeek-R1 å’Œ Qwen3-32Bã€‚ç ”ç©¶å›¢é˜Ÿè¿˜æ·±å…¥æ¢è®¨äº†ç†µå´©å¡Œ (Entropy Collapse) ç°è±¡åŠå…¶å½±å“å› ç´ ï¼Œå¹¶è¯æ˜ç¼“è§£è¿‡æ—©çš„ç†µå´©å¡Œå¯¹äºæå‡æ¨¡å‹æ€§èƒ½è‡³å…³é‡è¦ã€‚æœ€åï¼Œè¯¥é¡¹ç›®å®Œå…¨å¼€æºäº†æ¨¡å‹æƒé‡ã€è®­ç»ƒä»£ç å’Œæ•°æ®é›†ï¼Œæ—¨åœ¨ä¸ºå¼€æºç¤¾åŒºçš„æ¨ç†æ¨¡å‹ç ”ç©¶æä¾›æœ‰åŠ›æ”¯æŒã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.22312v2",
      "published_date": "2025-05-28 12:56:04 UTC",
      "updated_date": "2025-05-29 09:07:33 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T15:25:32.079407+00:00"
    },
    {
      "arxiv_id": "2505.22311v1",
      "title": "From Large AI Models to Agentic AI: A Tutorial on Future Intelligent Communications",
      "title_zh": "ä»äººå·¥æ™ºèƒ½å¤§æ¨¡å‹åˆ°æ™ºèƒ½ä½“AIï¼šæœªæ¥æ™ºèƒ½é€šä¿¡æ•™ç¨‹",
      "authors": [
        "Feibo Jiang",
        "Cunhua Pan",
        "Li Dong",
        "Kezhi Wang",
        "Octavia A. Dobre",
        "Merouane Debbah"
      ],
      "abstract": "With the advent of 6G communications, intelligent communication systems face multiple challenges, including constrained perception and response capabilities, limited scalability, and low adaptability in dynamic environments. This tutorial provides a systematic introduction to the principles, design, and applications of Large Artificial Intelligence Models (LAMs) and Agentic AI technologies in intelligent communication systems, aiming to offer researchers a comprehensive overview of cutting-edge technologies and practical guidance. First, we outline the background of 6G communications, review the technological evolution from LAMs to Agentic AI, and clarify the tutorial's motivation and main contributions. Subsequently, we present a comprehensive review of the key components required for constructing LAMs. We further categorize LAMs and analyze their applicability, covering Large Language Models (LLMs), Large Vision Models (LVMs), Large Multimodal Models (LMMs), Large Reasoning Models (LRMs), and lightweight LAMs. Next, we propose a LAM-centric design paradigm tailored for communications, encompassing dataset construction and both internal and external learning approaches. Building upon this, we develop an LAM-based Agentic AI system for intelligent communications, clarifying its core components such as planners, knowledge bases, tools, and memory modules, as well as its interaction mechanisms. We also introduce a multi-agent framework with data retrieval, collaborative planning, and reflective evaluation for 6G. Subsequently, we provide a detailed overview of the applications of LAMs and Agentic AI in communication scenarios. Finally, we summarize the research challenges and future directions in current studies, aiming to support the development of efficient, secure, and sustainable next-generation intelligent communication systems.",
      "tldr_zh": "è¿™ç¯‡æ•™ç¨‹ç³»ç»Ÿæ€§åœ°æ¢è®¨äº†ä»å¤§å‹äººå·¥æ™ºèƒ½æ¨¡å‹ (Large Artificial Intelligence Models, LAMs) åˆ°æ™ºèƒ½ä½“ AI (Agentic AI) çš„æ¼”è¿›ï¼Œæ—¨åœ¨åº”å¯¹ 6G é€šä¿¡åœ¨åŠ¨æ€ç¯å¢ƒä¸­çš„æ„ŸçŸ¥ã€å“åº”å’Œæ‰©å±•æ€§æŒ‘æˆ˜ã€‚æ–‡ç« é¦–å…ˆæ·±å…¥åˆ†æäº† LLMsã€LVMsã€LMMsã€LRMs ç­‰å„ç±» LAMs çš„å…³é”®ç»„ä»¶åŠå…¶é€‚ç”¨æ€§ï¼Œå¹¶æå‡ºäº†ä¸“é—¨é’ˆå¯¹é€šä¿¡åœºæ™¯çš„ LAM-centric è®¾è®¡èŒƒå¼ã€‚éšåï¼Œç ”ç©¶è¯¦ç»†é˜è¿°äº†åŸºäº LAMs çš„ Agentic AI ç³»ç»Ÿæ¶æ„ï¼Œå®šä¹‰äº†è§„åˆ’å™¨ (planners)ã€çŸ¥è¯†åº“ (knowledge bases)ã€å·¥å…·å’Œå­˜å‚¨æ¨¡å—ç­‰æ ¸å¿ƒç»„æˆéƒ¨åˆ†ã€‚æ–‡ä¸­è¿˜é’ˆå¯¹ 6G æå‡ºäº†ä¸€ä¸ªé›†æˆæ•°æ®æ£€ç´¢ã€ååŒè§„åˆ’å’Œåæ€è¯„ä¼°çš„å¤šæ™ºèƒ½ä½“ (multi-agent) æ¡†æ¶ã€‚é€šè¿‡å…¨é¢ç»¼è¿°ç›¸å…³åº”ç”¨ã€ç ”ç©¶æŒ‘æˆ˜å’Œæœªæ¥æ–¹å‘ï¼Œè¯¥æ•™ç¨‹ä¸ºå¼€å‘é«˜æ•ˆã€å®‰å…¨ä¸”å¯æŒç»­çš„ä¸‹ä¸€ä»£æ™ºèƒ½é€šä¿¡ç³»ç»Ÿæä¾›äº†é‡è¦çš„ç†è®ºæ”¯æ’‘å’Œå®è·µæŒ‡å¯¼ã€‚",
      "categories": [
        "cs.AI",
        "cs.CY",
        "cs.NI",
        "eess.SP"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.22311v1",
      "published_date": "2025-05-28 12:54:07 UTC",
      "updated_date": "2025-05-28 12:54:07 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T15:25:53.556873+00:00"
    },
    {
      "arxiv_id": "2505.22310v2",
      "title": "From Dormant to Deleted: Tamper-Resistant Unlearning Through Weight-Space Regularization",
      "title_zh": "ä»ä¼‘çœ åˆ°å½»åº•æ¶ˆé™¤ï¼šåŸºäºæƒé‡ç©ºé—´æ­£åˆ™åŒ–çš„é˜²ç¯¡æ”¹æœºå™¨é—å¿˜",
      "authors": [
        "Shoaib Ahmed Siddiqui",
        "Adrian Weller",
        "David Krueger",
        "Gintare Karolina Dziugaite",
        "Michael Curtis Mozer",
        "Eleni Triantafillou"
      ],
      "abstract": "Recent unlearning methods for LLMs are vulnerable to relearning attacks: knowledge believed-to-be-unlearned re-emerges by fine-tuning on a small set of (even seemingly-unrelated) examples. We study this phenomenon in a controlled setting for example-level unlearning in vision classifiers. We make the surprising discovery that forget-set accuracy can recover from around 50% post-unlearning to nearly 100% with fine-tuning on just the retain set -- i.e., zero examples of the forget set. We observe this effect across a wide variety of unlearning methods, whereas for a model retrained from scratch excluding the forget set (gold standard), the accuracy remains at 50%. We observe that resistance to relearning attacks can be predicted by weight-space properties, specifically, $L_2$-distance and linear mode connectivity between the original and the unlearned model. Leveraging this insight, we propose a new class of methods that achieve state-of-the-art resistance to relearning attacks.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†å¤§å‹è¯­è¨€æ¨¡å‹(LLMs)æœºå™¨å¸è½½(Machine Unlearning)æŠ€æœ¯åœ¨é¢å¯¹é‡å­¦ä¹ æ”»å‡»(Relearning Attacks)æ—¶çš„è„†å¼±æ€§ï¼Œå³è¢«å¸è½½çš„çŸ¥è¯†å¯èƒ½é€šè¿‡å°‘é‡å¾®è°ƒé‡æ–°æ˜¾ç°ã€‚ä½œè€…å‘ç°åœ¨è§†è§‰åˆ†ç±»å™¨ä¸­ï¼Œå³ä½¿ä»…ä½¿ç”¨ä¿ç•™é›†(Retain Set)è¿›è¡Œå¾®è°ƒï¼Œå¸è½½åçš„æ¨¡å‹å¯¹é—å¿˜é›†(Forget Set)çš„å‡†ç¡®ç‡ä¹Ÿèƒ½ä»çº¦50%æ¢å¤åˆ°è¿‘100%ï¼Œè€Œä»å¤´è®­ç»ƒçš„é»„é‡‘æ ‡å‡†æ¨¡å‹åˆ™ä¸ä¼šå‡ºç°æ­¤ç°è±¡ã€‚ç ”ç©¶æŒ‡å‡ºï¼Œè¿™ç§å¯¹é‡å­¦ä¹ æ”»å‡»çš„æŠµæŠ—åŠ›å¯ä»¥é€šè¿‡æƒé‡ç©ºé—´(Weight-Space)å±æ€§æ¥é¢„æµ‹ï¼Œç‰¹åˆ«æ˜¯åŸå§‹æ¨¡å‹ä¸å¸è½½æ¨¡å‹ä¹‹é—´çš„$L_2$è·ç¦»å’Œçº¿æ€§ä¼—æ•°è¿æ¥æ€§(Linear Mode Connectivity)ã€‚åŸºäºæ­¤æ´å¯Ÿï¼Œç ”ç©¶æå‡ºäº†ä¸€ç±»é€šè¿‡æƒé‡ç©ºé—´æ­£åˆ™åŒ–å®ç°é˜²ç¯¡æ”¹å¸è½½çš„æ–°æ–¹æ³•ï¼Œåœ¨æŠµå¾¡é‡å­¦ä¹ æ”»å‡»æ–¹é¢è¾¾åˆ°äº†ç›®å‰æœ€å…ˆè¿›(State-of-the-art)çš„æ°´å¹³ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.22310v2",
      "published_date": "2025-05-28 12:53:08 UTC",
      "updated_date": "2026-01-14 23:36:05 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T15:25:53.422123+00:00"
    },
    {
      "arxiv_id": "2505.22306v2",
      "title": "Versatile Cardiovascular Signal Generation with a Unified Diffusion Transformer",
      "title_zh": "åŸºäºç»Ÿä¸€æ‰©æ•£ Transformer çš„å¤šåŠŸèƒ½å¿ƒè¡€ç®¡ä¿¡å·ç”Ÿæˆ",
      "authors": [
        "Zehua Chen",
        "Yuyang Miao",
        "Liyuan Wang",
        "Luyun Fan",
        "Danilo P. Mandic",
        "Jun Zhu"
      ],
      "abstract": "Cardiovascular signals such as photoplethysmography (PPG), electrocardiography (ECG), and blood pressure (BP) are inherently correlated and complementary, together reflecting the health of cardiovascular system. However, their joint utilization in real-time monitoring is severely limited by diverse acquisition challenges from noisy wearable recordings to burdened invasive procedures. Here we propose UniCardio, a multi-modal diffusion transformer that reconstructs low-quality signals and synthesizes unrecorded signals in a unified generative framework. Its key innovations include a specialized model architecture to manage the signal modalities involved in generation tasks and a continual learning paradigm to incorporate varying modality combinations. By exploiting the complementary nature of cardiovascular signals, UniCardio clearly outperforms recent task-specific baselines in signal denoising, imputation, and translation. The generated signals match the performance of ground-truth signals in detecting abnormal health conditions and estimating vital signs, even in unseen domains, while ensuring interpretability for human experts. These advantages position UniCardio as a promising avenue for advancing AI-assisted healthcare.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†UniCardioï¼Œè¿™æ˜¯ä¸€ç§ç»Ÿä¸€çš„å¤šæ¨¡æ€æ‰©æ•£Transformer (multi-modal diffusion transformer) æ¡†æ¶ï¼Œæ—¨åœ¨é€šè¿‡ç”Ÿæˆå¼æ–¹æ³•è§£å†³å¿ƒè¡€ç®¡ä¿¡å·ï¼ˆå¦‚PPGã€ECGå’ŒBPï¼‰åœ¨å®æ—¶ç›‘æµ‹ä¸­é¢ä¸´çš„å™ªå£°å¹²æ‰°å’Œé‡‡é›†å—é™ç­‰æŒ‘æˆ˜ã€‚è¯¥æ¡†æ¶çš„æ ¸å¿ƒåˆ›æ–°åœ¨äºè®¾è®¡äº†ä¸“é—¨çš„æ¨¡å‹æ¶æ„æ¥ç®¡ç†ä¸åŒä¿¡å·æ¨¡æ€çš„ç”Ÿæˆä»»åŠ¡ï¼Œå¹¶å¼•å…¥äº†æŒç»­å­¦ä¹  (continual learning) èŒƒå¼ä»¥æ•´åˆå¤šå˜çš„æ¨¡æ€ç»„åˆã€‚é€šè¿‡å……åˆ†åˆ©ç”¨å¿ƒè¡€ç®¡ä¿¡å·ä¹‹é—´çš„äº’è¡¥æ€§ï¼ŒUniCardioåœ¨ä¿¡å·å»å™ª (denoising)ã€ç¼ºå¤±è¡¥å…¨ (imputation) å’Œä¿¡å·ç¿»è¯‘ (translation) ä»»åŠ¡ä¸­æ˜¾è‘—ä¼˜äºç°æœ‰çš„ç‰¹å®šä»»åŠ¡åŸºçº¿æ¨¡å‹ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒUniCardioç”Ÿæˆçš„ä¿¡å·åœ¨æ£€æµ‹å¼‚å¸¸å¥åº·çŠ¶å†µå’Œè¯„ä¼°ç”Ÿå‘½ä½“å¾æ–¹é¢è¡¨ç°å‡ºä¸çœŸå®ä¿¡å· (ground-truth signals) ç›¸å½“çš„æ€§èƒ½ï¼Œç”šè‡³åœ¨æœªè§è¿‡çš„é¢†åŸŸä¹Ÿå…·æœ‰è‰¯å¥½çš„æ³›åŒ–èƒ½åŠ›ã€‚æ­¤å¤–ï¼Œè¯¥æ¨¡å‹ç¡®ä¿äº†ç”Ÿæˆç»“æœå¯¹ä¸“å®¶çš„å¯è§£é‡Šæ€§ï¼Œä¸ºæ¨è¿›äººå·¥æ™ºèƒ½è¾…åŠ©åŒ»ç–—ä¿å¥ (AI-assisted healthcare) æä¾›äº†æå…·å‰æ™¯çš„æŠ€æœ¯è·¯å¾„ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.22306v2",
      "published_date": "2025-05-28 12:45:39 UTC",
      "updated_date": "2025-08-21 03:53:43 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T15:26:04.756118+00:00"
    },
    {
      "arxiv_id": "2505.22303v1",
      "title": "Voice CMS: updating the knowledge base of a digital assistant through conversation",
      "title_zh": "Voice CMSï¼šé€šè¿‡å¯¹è¯æ›´æ–°æ•°å­—åŠ©æ‰‹çŸ¥è¯†åº“",
      "authors": [
        "Grzegorz Wolny",
        "MichaÅ‚ Szczerbak"
      ],
      "abstract": "In this study, we propose a solution based on a multi-agent LLM architecture and a voice user interface (VUI) designed to update the knowledge base of a digital assistant. Its usability is evaluated in comparison to a more traditional graphical content management system (CMS), with a focus on understanding the relationship between user preferences and the complexity of the information being provided. The findings demonstrate that, while the overall usability of the VUI is rated lower than the graphical interface, it is already preferred by users for less complex tasks. Furthermore, the quality of content entered through the VUI is comparable to that achieved with the graphical interface, even for highly complex tasks. Obtained qualitative results suggest that a hybrid interface combining the strengths of both approaches could address the key challenges identified during the experiment, such as reducing cognitive load through graphical feedback while maintaining the intuitive nature of voice-based interactions. This work highlights the potential of conversational interfaces as a viable and effective method for knowledge management in specific business contexts.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†Voice CMSï¼Œè¿™æ˜¯ä¸€ç§åŸºäºå¤šæ™ºèƒ½ä½“(multi-agent)å¤§è¯­è¨€æ¨¡å‹æ¶æ„å’Œè¯­éŸ³ç”¨æˆ·ç•Œé¢(VUI)çš„è§£å†³æ–¹æ¡ˆï¼Œæ—¨åœ¨é€šè¿‡å¯¹è¯æ›´æ–°æ•°å­—åŠ©æ‰‹çš„çŸ¥è¯†åº“ã€‚ç ”ç©¶é€šè¿‡ä¸ä¼ ç»Ÿå›¾å½¢å†…å®¹ç®¡ç†ç³»ç»Ÿ(CMS)çš„å¯¹æ¯”è¯„ä¼°ï¼Œé‡ç‚¹æ¢è®¨äº†ç”¨æˆ·åå¥½ä¸ä¿¡æ¯å¤æ‚åº¦ä¹‹é—´çš„å…³ç³»ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œå°½ç®¡VUIçš„æ•´ä½“å¯ç”¨æ€§è¯„åˆ†ç›®å‰ä½äºå›¾å½¢ç•Œé¢ï¼Œä½†åœ¨å¤„ç†ç®€å•ä»»åŠ¡æ—¶å·²å±•ç°å‡ºæ˜¾è‘—çš„ç”¨æˆ·åå¥½ã€‚æ­¤å¤–ï¼Œå³ä½¿åœ¨å¤„ç†é«˜åº¦å¤æ‚çš„ä»»åŠ¡æ—¶ï¼Œé€šè¿‡VUIè¾“å…¥çš„å†…å®¹è´¨é‡ä¹Ÿä¸å›¾å½¢ç•Œé¢ç›¸å½“ã€‚ç ”ç©¶æœ€ç»ˆå»ºè®®å¼€å‘ç»“åˆä¸¤è€…ä¼˜åŠ¿çš„æ··åˆç•Œé¢ï¼Œä»¥åœ¨é™ä½è®¤çŸ¥è´Ÿè·çš„åŒæ—¶ä¿æŒè¯­éŸ³äº¤äº’çš„ç›´è§‚æ€§ï¼Œä»è€Œè®ºè¯äº†å¯¹è¯å¼ç•Œé¢åœ¨ç‰¹å®šå•†ä¸šèƒŒæ™¯ä¸‹ä½œä¸ºçŸ¥è¯†ç®¡ç†æ‰‹æ®µçš„æ½œåŠ›ã€‚",
      "categories": [
        "cs.HC",
        "cs.AI",
        "cs.MA"
      ],
      "primary_category": "cs.HC",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.22303v1",
      "published_date": "2025-05-28 12:40:37 UTC",
      "updated_date": "2025-05-28 12:40:37 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T15:25:59.810784+00:00"
    },
    {
      "arxiv_id": "2505.22291v2",
      "title": "Neural Restoration of Greening Defects in Historical Autochrome Photographs Based on Purely Synthetic Data",
      "title_zh": "åŸºäºçº¯åˆæˆæ•°æ®çš„å†å² Autochrome ç…§ç‰‡æ³›ç»¿ç¼ºé™·ç¥ç»ä¿®å¤",
      "authors": [
        "Saptarshi Neil Sinha",
        "P. Julius Kuehn",
        "Johannes Koppe",
        "Arjan Kuijper",
        "Michael Weinmann"
      ],
      "abstract": "The preservation of early visual arts, particularly color photographs, is challenged by deterioration caused by aging and improper storage, leading to issues like blurring, scratches, color bleeding, and fading defects. Despite great advances in image restoration and enhancement in recent years, such systematic defects often cannot be restored by current state-of-the-art software features as available e.g. in Adobe Photoshop, but would require the incorporation of defect-aware priors into the underlying machine learning techniques. However, there are no publicly available datasets of autochromes with defect annotations. In this paper, we address these limitations and present the first approach that allows the automatic removal of greening color defects in digitized autochrome photographs. For this purpose, we introduce an approach for accurately simulating respective defects and use the respectively obtained synthesized data with its ground truth defect annotations to train a generative AI model with a carefully designed loss function that accounts for color imbalances between defected and non-defected areas. As demonstrated in our evaluation, our approach allows for the efficient and effective restoration of the considered defects, thereby overcoming limitations of alternative techniques that struggle with accurately reproducing original colors and may require significant manual effort.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹æ—©æœŸå½©è‰²ç…§ç‰‡ Autochrome å› è€åŒ–äº§ç”Ÿçš„æ³›ç»¿ç¼ºé™·(greening defects)ä¿®å¤éš¾é¢˜ï¼Œæå‡ºäº†é¦–ä¸ªåŸºäºçº¯åˆæˆæ•°æ®(purely synthetic data)çš„è‡ªåŠ¨åŒ–ç¥ç»ä¿®å¤æ–¹æ³•ã€‚ç”±äºç¼ºä¹çœŸå®çš„ç¼ºé™·æ ‡æ³¨æ•°æ®é›†ï¼Œç ”ç©¶è€…é€šè¿‡ç²¾ç¡®æ¨¡æ‹Ÿè€åŒ–ç¼ºé™·æ¥æ„å»ºåˆæˆè®­ç»ƒé›†åŠå…¶ Ground Truthï¼Œå¹¶æ®æ­¤è®­ç»ƒäº†ä¸€ä¸ªç”Ÿæˆå¼ AI æ¨¡å‹(generative AI model)ã€‚ä¸ºäº†è§£å†³å—æŸåŒºåŸŸçš„è‰²å½©å¤±è¡¡ï¼Œæ¨¡å‹å¼•å…¥äº†ç²¾å¿ƒè®¾è®¡çš„æŸå¤±å‡½æ•°(loss function)ï¼Œä»¥ç¡®ä¿ä¿®å¤æ•ˆæœçš„å‡†ç¡®æ€§ã€‚è¯„ä¼°ç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿé«˜æ•ˆåœ°ç§»é™¤æ•°å­—åŒ–ç…§ç‰‡ä¸­çš„ç¼ºé™·å¹¶è¿˜åŸåŸå§‹è‰²å½©ï¼Œå…‹æœäº†ç°æœ‰è½¯ä»¶å¦‚ Adobe Photoshop ç¼ºä¹ç¼ºé™·å…ˆéªŒçŸ¥è¯†çš„å±€é™ï¼Œå¹¶æ˜¾è‘—å‡å°‘äº†äººå·¥ä¿®å¤æˆæœ¬ã€‚è¿™ä¸€è¿›å±•ä¸ºå†å²å½±åƒé—äº§çš„æ•°å­—åŒ–ä¿å­˜ä¸ä¿®å¤æä¾›äº†å¼ºæœ‰åŠ›çš„æŠ€æœ¯æ”¯æ’‘ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.22291v2",
      "published_date": "2025-05-28 12:28:35 UTC",
      "updated_date": "2025-08-20 14:51:09 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T15:26:18.213771+00:00"
    },
    {
      "arxiv_id": "2505.22290v1",
      "title": "Rethinking the Unsolvable: When In-Context Search Meets Test-Time Scaling",
      "title_zh": "é‡æ–°å®¡è§†ä¸å¯è§£ï¼šå½“ä¸Šä¸‹æ–‡æœç´¢é‡ä¸Šæµ‹è¯•æ—¶æ‰©å±•",
      "authors": [
        "Fanzeng Xia",
        "Yidong Luo",
        "Tinko Sebastian Bartels",
        "Yaqi Xu",
        "Tongxin Li"
      ],
      "abstract": "Recent research has highlighted that Large Language Models (LLMs), even when trained to generate extended long reasoning steps, still face significant challenges on hard reasoning problems. However, much of the existing literature relies on direct prompting with simple in-context learning examples for evaluation, which largely overlooks advanced techniques to elicit LLMs' deliberate reasoning before drawing conclusions that LLMs hit a performance ceiling. In this paper, we systematically explore the combined potential of in-context search and test-time scaling on super hard reasoning tasks. We find that by employing advanced in-context search prompting to LLMs augmented with internal scaling, one can achieve transformative performance breakthroughs on tasks previously deemed \"unsolvable\" (e.g., reported success rates below 5%). We provide both empirical results and theoretical analysis of how this combination can unleash LLM reasoning capabilities: i) Empirically, on controlled NP-hard tasks and complex real-world planning benchmarks, our approach achieves up to a 30x improvement in success rates compared to previously reported results without any external mechanisms; ii) Theoretically, we show that in-context search prompting, when combined with internal scaling, significantly extends the complexity class of solvable reasoning problems. These findings challenge prevailing assumptions about the limitations of LLMs on complex tasks, indicating that current evaluation paradigms systematically underestimate their true potential. Our work calls for a critical reassessment of how LLM reasoning is benchmarked and a more robust evaluation strategy that fully captures the true capabilities of contemporary LLMs, which can lead to a better understanding of their operational reasoning boundaries in real-world deployments.",
      "tldr_zh": "è¯¥ç ”ç©¶ç³»ç»Ÿåœ°æ¢è®¨äº† In-Context Search ä¸ Test-Time Scaling åœ¨æéš¾æ¨ç†ä»»åŠ¡ä¸­çš„ç»“åˆæ½œåŠ›ï¼Œæ—¨åœ¨è§£å†³å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨å¤æ‚é€»è¾‘é—®é¢˜ä¸Šè¡¨ç°å—é™çš„é—®é¢˜ã€‚ç ”ç©¶å‘ç°ï¼Œé€šè¿‡ç»“åˆé«˜çº§çš„ In-Context Search æç¤ºæŠ€æœ¯ä¸å†…éƒ¨çš„ Scaling æœºåˆ¶ï¼ŒLLMs åœ¨å…ˆå‰è¢«è§†ä¸ºâ€œä¸å¯è§£â€çš„ä»»åŠ¡ä¸Šå®ç°äº†æ˜¾è‘—çš„æ€§èƒ½çªç ´ã€‚å®éªŒæ•°æ®è¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨ NP-hard ä»»åŠ¡åŠå¤æ‚è§„åˆ’åŸºå‡†æµ‹è¯•ä¸­ï¼Œè¾ƒä¹‹ä»¥å¾€çš„ç›´æ¥æç¤ºæ–¹æ³•å®ç°äº†é«˜è¾¾ 30 å€çš„æˆåŠŸç‡æå‡ã€‚ç†è®ºåˆ†æè¿›ä¸€æ­¥è¯æ˜ï¼Œè¿™ç§ç»„åˆæ˜¾è‘—æ‰©å±•äº†æ¨¡å‹å¯è§£å†³æ¨ç†é—®é¢˜çš„ Complexity Classã€‚è¿™äº›å‘ç°æŒ‘æˆ˜äº†ç›®å‰å¯¹ LLMs å±€é™æ€§çš„æ™®éè®¤çŸ¥ï¼Œè¡¨æ˜ç°æœ‰çš„è¯„ä¼°èŒƒå¼ä¸¥é‡ä½ä¼°äº†æ¨¡å‹çš„çœŸå®æ½œèƒ½ã€‚è¯¥å·¥ä½œå‘¼åå­¦æœ¯ç•Œé‡æ–°å®¡è§† LLM çš„æ¨ç†è¯„ä¼°åŸºå‡†ï¼Œå»ºç«‹æ›´ç¨³å¥çš„è¯„ä»·ç­–ç•¥ï¼Œä»¥æ›´å‡†ç¡®åœ°æ•æ‰æ¨¡å‹åœ¨å®é™…åº”ç”¨ä¸­çš„æ¨ç†è¾¹ç•Œã€‚",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.22290v1",
      "published_date": "2025-05-28 12:28:18 UTC",
      "updated_date": "2025-05-28 12:28:18 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T15:27:32.547476+00:00"
    },
    {
      "arxiv_id": "2505.22288v2",
      "title": "Compression versus Accuracy: A Hierarchy of Lifted Models",
      "title_zh": "å‹ç¼©ä¸å‡†ç¡®ç‡ï¼šæå‡æ¨¡å‹çš„å±‚æ¬¡ç»“æ„",
      "authors": [
        "Jan Speller",
        "Malte Luttermann",
        "Marcel Gehrke",
        "Tanya Braun"
      ],
      "abstract": "Probabilistic graphical models that encode indistinguishable objects and relations among them use first-order logic constructs to compress a propositional factorised model for more efficient (lifted) inference. To obtain a lifted representation, the state-of-the-art algorithm Advanced Colour Passing (ACP) groups factors that represent matching distributions. In an approximate version using $\\varepsilon$ as a hyperparameter, factors are grouped that differ by a factor of at most $(1\\pm \\varepsilon)$. However, finding a suitable $\\varepsilon$ is not obvious and may need a lot of exploration, possibly requiring many ACP runs with different $\\varepsilon$ values. Additionally, varying $\\varepsilon$ can yield wildly different models, leading to decreased interpretability. Therefore, this paper presents a hierarchical approach to lifted model construction that is hyperparameter-free. It efficiently computes a hierarchy of $\\varepsilon$ values that ensures a hierarchy of models, meaning that once factors are grouped together given some $\\varepsilon$, these factors will be grouped together for larger $\\varepsilon$ as well. The hierarchy of $\\varepsilon$ values also leads to a hierarchy of error bounds. This allows for explicitly weighing compression versus accuracy when choosing specific $\\varepsilon$ values to run ACP with and enables interpretability between the different models.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹æ¦‚ç‡å›¾æ¨¡å‹(Probabilistic Graphical Models)åœ¨æå‡æ¨ç†(Lifted Inference)ä¸­é¢ä¸´çš„é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§å±‚æ¬¡åŒ–çš„æ¨¡å‹æ„å»ºæ–¹æ³•ï¼Œæ—¨åœ¨è§£å†³Advanced Colour Passing (ACP)ç®—æ³•åœ¨è¿‘ä¼¼å¤„ç†æ—¶è¿‡åº¦ä¾èµ–è¶…å‚æ•°$\\varepsilon$ä¸”ç¼ºä¹å¯è§£é‡Šæ€§çš„ç¼ºé™·ã€‚è¯¥æ–¹æ³•é€šè¿‡é«˜æ•ˆè®¡ç®—$\\varepsilon$å€¼çš„å±‚æ¬¡ç»“æ„ï¼Œç¡®ä¿äº†åœ¨ä¸åŒå‹ç¼©ç¨‹åº¦ä¸‹æ¨¡å‹åˆ†ç»„çš„ä¸€è‡´æ€§ï¼Œå³åœ¨è¾ƒå°è¯¯å·®é˜ˆå€¼ä¸‹åˆå¹¶çš„å› å­åœ¨æ›´å¤§é˜ˆå€¼ä¸‹ä¾ç„¶ä¿æŒåˆå¹¶çŠ¶æ€ã€‚æ­¤å¤–ï¼Œè¯¥æ¡†æ¶è¿˜å»ºç«‹äº†ä¸€å¥—ä¸$\\varepsilon$å€¼å¯¹åº”çš„è¯¯å·®è¾¹ç•Œå±‚æ¬¡ä½“ç³»ï¼Œå…è®¸å¼€å‘è€…åœ¨å‹ç¼©ç‡(Compression)ä¸å‡†ç¡®æ€§(Accuracy)ä¹‹é—´è¿›è¡Œæ˜¾å¼çš„æƒè¡¡ã€‚è¿™ç§æ— éœ€é¢„è®¾è¶…å‚æ•°çš„æ–¹æ¡ˆä¸ä»…ç®€åŒ–äº†ACPç®—æ³•çš„è°ƒä¼˜è¿‡ç¨‹ï¼Œè¿˜é€šè¿‡æ¨¡å‹é—´çš„å…³è”æ€§æ˜¾è‘—æå‡äº†ç»“æœçš„å¯è§£é‡Šæ€§ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.22288v2",
      "published_date": "2025-05-28 12:27:32 UTC",
      "updated_date": "2025-08-29 15:11:17 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T15:26:29.563772+00:00"
    },
    {
      "arxiv_id": "2505.22287v1",
      "title": "New Tools are Needed for Tracking Adherence to AI Model Behavioral Use Clauses",
      "title_zh": "è¿½è¸ªäººå·¥æ™ºèƒ½æ¨¡å‹è¡Œä¸ºä½¿ç”¨æ¡æ¬¾éµå¾ªæƒ…å†µï¼šäºŸéœ€å¼€å‘æ–°å·¥å…·",
      "authors": [
        "Daniel McDuff",
        "Tim Korjakow",
        "Kevin Klyman",
        "Danish Contractor"
      ],
      "abstract": "Foundation models have had a transformative impact on AI. A combination of large investments in research and development, growing sources of digital data for training, and architectures that scale with data and compute has led to models with powerful capabilities. Releasing assets is fundamental to scientific advancement and commercial enterprise. However, concerns over negligent or malicious uses of AI have led to the design of mechanisms to limit the risks of the technology. The result has been a proliferation of licenses with behavioral-use clauses and acceptable-use-policies that are increasingly being adopted by commonly used families of models (Llama, Gemma, Deepseek) and a myriad of smaller projects. We created and deployed a custom AI licenses generator to facilitate license creation and have quantitatively and qualitatively analyzed over 300 customized licenses created with this tool. Alongside this we analyzed 1.7 million models licenses on the HuggingFace model hub. Our results show increasing adoption of these licenses, interest in tools that support their creation and a convergence on common clause configurations. In this paper we take the position that tools for tracking adoption of, and adherence to, these licenses is the natural next step and urgently needed in order to ensure they have the desired impact of ensuring responsible use.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†éšç€åŸºç¡€æ¨¡å‹(Foundation models)èƒ½åŠ›çš„æå‡ï¼ŒåŒ…å«è¡Œä¸ºä½¿ç”¨æ¡æ¬¾(behavioral-use clauses)å’Œå¯æ¥å—ä½¿ç”¨æ”¿ç­–(acceptable-use-policies)çš„è®¸å¯è¯åœ¨Llamaã€GemmaåŠDeepseekç­‰ä¸»æµæ¨¡å‹ä¸­è¢«å¹¿æ³›é‡‡ç”¨çš„ç°çŠ¶ã€‚ç ”ç©¶å›¢é˜Ÿé€šè¿‡å¼€å‘è‡ªå®šä¹‰AIè®¸å¯è¯ç”Ÿæˆå™¨ï¼Œå¹¶å¯¹HuggingFaceä¸Šè¶…è¿‡170ä¸‡ä¸ªæ¨¡å‹è®¸å¯è¯è¿›è¡Œå®šé‡ä¸å®šæ€§åˆ†æï¼Œè¯å®äº†è¿™ç±»è®¸å¯è¯çš„é‡‡ç”¨ç‡æ­£æŒç»­å¢é•¿ä¸”æ¡æ¬¾é…ç½®è¶‹äºç»Ÿä¸€ã€‚åˆ†æç»“æœæ˜¾ç¤ºï¼Œå°½ç®¡ä¸šç•Œå¯¹è´Ÿè´£ä»»ä½¿ç”¨(responsible use)çš„å…³æ³¨åº¦æ˜¾è‘—æå‡ï¼Œä½†ç›®å‰ä»ç¼ºä¹æœ‰æ•ˆçš„æŠ€æœ¯æ‰‹æ®µæ¥ç›‘æµ‹ç”¨æˆ·å¯¹è¿™äº›è¡Œä¸ºçº¦æŸæ¡æ¬¾çš„éµå®ˆæƒ…å†µã€‚å› æ­¤ï¼Œè¯¥è®ºæ–‡å¼ºè°ƒå¼€å‘ç”¨äºè¿½è¸ªè®¸å¯è¯æ‰§è¡Œä¸åˆè§„æ€§çš„æ–°å·¥å…·å·²æˆä¸ºå½“åŠ¡ä¹‹æ€¥ã€‚è¿™ä¸ä»…æ˜¯ç¡®ä¿AIæŠ€æœ¯å®‰å…¨éƒ¨ç½²çš„è‡ªç„¶æ¼”è¿›ï¼Œæ›´æ˜¯å®ç°é£é™©ç¼“è§£ç›®æ ‡å¹¶ç¡®ä¿æŠ€æœ¯äº§ç”Ÿé¢„æœŸæ­£å‘ç¤¾ä¼šå½±å“çš„å¿…è¦ä¿éšœã€‚",
      "categories": [
        "cs.CY",
        "cs.AI"
      ],
      "primary_category": "cs.CY",
      "comment": "Preprint",
      "pdf_url": "https://arxiv.org/pdf/2505.22287v1",
      "published_date": "2025-05-28 12:26:55 UTC",
      "updated_date": "2025-05-28 12:26:55 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T15:26:45.546950+00:00"
    },
    {
      "arxiv_id": "2505.22280v1",
      "title": "Natural Language Processing in Support of Evidence-based Medicine: A Scoping Review",
      "title_zh": "è‡ªç„¶è¯­è¨€å¤„ç†èµ‹èƒ½å¾ªè¯åŒ»å­¦ï¼šä¸€é¡¹èŒƒå›´ç»¼è¿°",
      "authors": [
        "Zihan Xu",
        "Haotian Ma",
        "Gongbo Zhang",
        "Yihao Ding",
        "Chunhua Weng",
        "Yifan Peng"
      ],
      "abstract": "Evidence-based medicine (EBM) is at the forefront of modern healthcare, emphasizing the use of the best available scientific evidence to guide clinical decisions. Due to the sheer volume and rapid growth of medical literature and the high cost of curation, there is a critical need to investigate Natural Language Processing (NLP) methods to identify, appraise, synthesize, summarize, and disseminate evidence in EBM. This survey presents an in-depth review of 129 research studies on leveraging NLP for EBM, illustrating its pivotal role in enhancing clinical decision-making processes. The paper systematically explores how NLP supports the five fundamental steps of EBM -- Ask, Acquire, Appraise, Apply, and Assess. The review not only identifies current limitations within the field but also proposes directions for future research, emphasizing the potential for NLP to revolutionize EBM by refining evidence extraction, evidence synthesis, appraisal, summarization, enhancing data comprehensibility, and facilitating a more efficient clinical workflow.",
      "tldr_zh": "è¯¥é¡¹èŒƒå›´ç»¼è¿°(Scoping Review)ç³»ç»Ÿå›é¡¾äº†129é¡¹åˆ©ç”¨è‡ªç„¶è¯­è¨€å¤„ç†(NLP)æ”¯æŒå¾ªè¯åŒ»å­¦(Evidence-based medicine, EBM)çš„ç ”ç©¶ï¼Œæ¢è®¨äº†å…¶åœ¨ä¼˜åŒ–ä¸´åºŠå†³ç­–ä¸­çš„å…³é”®ä½œç”¨ã€‚æ–‡ç« è¯¦ç»†åˆ†æäº†NLPå¦‚ä½•èµ‹èƒ½EBMçš„äº”ä¸ªæ ¸å¿ƒç¯èŠ‚ï¼šè¯¢é—®(Ask)ã€è·å–(Acquire)ã€è¯„ä»·(Appraise)ã€åº”ç”¨(Apply)åŠè¯„ä¼°(Assess)ã€‚ç ”ç©¶æŒ‡å‡ºï¼ŒNLPæŠ€æœ¯åœ¨è¯æ®æå–(evidence extraction)ã€è¯æ®åˆæˆ(evidence synthesis)å’Œè‡ªåŠ¨åŒ–æ‘˜è¦ç­‰æ–¹é¢å…·æœ‰æ˜¾è‘—ä¼˜åŠ¿ï¼Œèƒ½å¤Ÿæœ‰æ•ˆåº”å¯¹åŒ»ç–—æ–‡çŒ®æ¿€å¢å¸¦æ¥çš„ç­›é€‰ä¸è¯„ä»·æŒ‘æˆ˜ã€‚é™¤äº†æ€»ç»“ç°æœ‰æˆå°±ï¼Œè¯¥ç»¼è¿°è¿˜è¯†åˆ«äº†å½“å‰é¢†åŸŸçš„å±€é™æ€§ï¼Œå¹¶æå‡ºäº†æœªæ¥çš„ç ”ç©¶è·¯å¾„ã€‚å…¶æ ¸å¿ƒå‘ç°å¼ºè°ƒäº†NLPåœ¨æ”¹è¿›ä¸´åºŠå·¥ä½œæµã€å¢å¼ºè¯æ®å¯ç†è§£æ€§ä»¥åŠæ¨åŠ¨EBMç°ä»£åŒ–è¿›ç¨‹ä¸­çš„é©å‘½æ€§æ½œåŠ›ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "Accepted by ACL 2025 Findings",
      "pdf_url": "https://arxiv.org/pdf/2505.22280v1",
      "published_date": "2025-05-28 12:17:01 UTC",
      "updated_date": "2025-05-28 12:17:01 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T15:26:20.707590+00:00"
    },
    {
      "arxiv_id": "2505.23836v3",
      "title": "Large Language Models Often Know When They Are Being Evaluated",
      "title_zh": "å¤§è¯­è¨€æ¨¡å‹é€šå¸¸èƒ½å¯Ÿè§‰åˆ°è‡ªå·±æ­£åœ¨æ¥å—è¯„ä¼°",
      "authors": [
        "Joe Needham",
        "Giles Edkins",
        "Govind Pimpale",
        "Henning Bartsch",
        "Marius Hobbhahn"
      ],
      "abstract": "If AI models can detect when they are being evaluated, the effectiveness of evaluations might be compromised. For example, models could have systematically different behavior during evaluations, leading to less reliable benchmarks for deployment and governance decisions. We investigate whether frontier language models can accurately classify transcripts based on whether they originate from evaluations or real-world deployment, a capability we call evaluation awareness. To achieve this, we construct a diverse benchmark of 1,000 prompts and transcripts from 61 distinct datasets. These span public benchmarks (e.g., MMLU, SWEBench), real-world deployment interactions, and agent trajectories from scaffolding frameworks (e.g., web-browsing agents). Frontier models clearly demonstrate above-random evaluation awareness (Gemini-2.5-Pro reaches an AUC of $0.83$), but do not yet surpass our simple human baseline (AUC of $0.92$). Furthermore, both AI models and humans are better at identifying evaluations in agentic settings compared to chat settings. Additionally, we test whether models can identify the purpose of the evaluation. Under multiple-choice and open-ended questioning, AI models far outperform random chance in identifying what an evaluation is testing for. Our results indicate that frontier models already exhibit a substantial, though not yet superhuman, level of evaluation-awareness. We recommend tracking this capability in future models.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰æ˜¯å¦èƒ½å¤Ÿè¯†åˆ«å‡ºè‡ªå·±æ­£å¤„äºè¢«è¯„ä¼°çŠ¶æ€ï¼Œå¹¶æå‡ºäº†è¯„ä¼°æ„è¯†ï¼ˆevaluation awarenessï¼‰è¿™ä¸€æ ¸å¿ƒæ¦‚å¿µã€‚ä¸ºäº†é‡åŒ–è¿™ä¸€èƒ½åŠ›ï¼Œç ”ç©¶è€…æ„å»ºäº†ä¸€ä¸ªåŒ…å«1000ä¸ªæç¤ºå’Œå¯¹è¯è®°å½•çš„åŸºå‡†æµ‹è¯•ï¼Œæ¶µç›–äº† MMLUã€SWEBench ç­‰å…¬å…±åŸºå‡†ã€çœŸå®ä¸–ç•Œéƒ¨ç½²äº¤äº’ä»¥åŠæ™ºèƒ½ä½“è½¨è¿¹ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œå‰æ²¿æ¨¡å‹è¡¨ç°å‡ºæ˜¾è‘—çš„è¯„ä¼°æ„è¯†ï¼Œå…¶ä¸­ Gemini-2.5-Pro çš„ AUC è¾¾åˆ°äº† 0.83ï¼Œå°½ç®¡å°šæœªè¶…è¿‡äººç±»åŸºå‡†ã€‚ç ”ç©¶å‘ç°ï¼ŒAI å’Œäººç±»åœ¨æ™ºèƒ½ä½“ï¼ˆagenticï¼‰è®¾ç½®ä¸‹è¯†åˆ«è¯„ä¼°çš„å‡†ç¡®ç‡å‡é«˜äºå¯¹è¯ï¼ˆchatï¼‰è®¾ç½®ï¼Œä¸”æ¨¡å‹èƒ½å¤Ÿå‡†ç¡®è¯†åˆ«å‡ºè¯„ä¼°çš„å…·ä½“ç›®çš„ã€‚ç ”ç©¶æŒ‡å‡ºï¼Œå‰æ²¿æ¨¡å‹å·²å…·å¤‡ç›¸å½“ç¨‹åº¦çš„è¯„ä¼°æ„è¯†ï¼Œè¿™å¯èƒ½å‰Šå¼±åŸºå‡†æµ‹è¯•ä½œä¸ºæ¨¡å‹éƒ¨ç½²å’Œæ²»ç†å†³ç­–ä¾æ®çš„å¯é æ€§ã€‚åŸºäºæ­¤å‘ç°ï¼Œç ”ç©¶å»ºè®®åœ¨æœªæ¥çš„æ¨¡å‹å¼€å‘ä¸ç›‘ç®¡ä¸­å¿…é¡»æŒç»­è·Ÿè¸ªå’Œç›‘æµ‹è¿™ç§è¯„ä¼°æ„è¯†èƒ½åŠ›ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.23836v3",
      "published_date": "2025-05-28 12:03:09 UTC",
      "updated_date": "2025-07-16 11:25:40 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T15:26:33.954045+00:00"
    },
    {
      "arxiv_id": "2505.22271v1",
      "title": "Test-Time Immunization: A Universal Defense Framework Against Jailbreaks for (Multimodal) Large Language Models",
      "title_zh": "æµ‹è¯•æ—¶å…ç–«ï¼šé’ˆå¯¹ï¼ˆå¤šæ¨¡æ€ï¼‰å¤§è¯­è¨€æ¨¡å‹è¶Šç‹±æ”»å‡»çš„é€šç”¨é˜²å¾¡æ¡†æ¶",
      "authors": [
        "Yongcan Yu",
        "Yanbo Wang",
        "Ran He",
        "Jian Liang"
      ],
      "abstract": "While (multimodal) large language models (LLMs) have attracted widespread attention due to their exceptional capabilities, they remain vulnerable to jailbreak attacks. Various defense methods are proposed to defend against jailbreak attacks, however, they are often tailored to specific types of jailbreak attacks, limiting their effectiveness against diverse adversarial strategies. For instance, rephrasing-based defenses are effective against text adversarial jailbreaks but fail to counteract image-based attacks. To overcome these limitations, we propose a universal defense framework, termed Test-time IMmunization (TIM), which can adaptively defend against various jailbreak attacks in a self-evolving way. Specifically, TIM initially trains a gist token for efficient detection, which it subsequently applies to detect jailbreak activities during inference. When jailbreak attempts are identified, TIM implements safety fine-tuning using the detected jailbreak instructions paired with refusal answers. Furthermore, to mitigate potential performance degradation in the detector caused by parameter updates during safety fine-tuning, we decouple the fine-tuning process from the detection module. Extensive experiments on both LLMs and multimodal LLMs demonstrate the efficacy of TIM.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹(å¤šæ¨¡æ€)å¤§è¯­è¨€æ¨¡å‹(Multimodal Large Language Models)é¢ä¸´çš„è¶Šç‹±æ”»å‡»(jailbreak attacks)é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§åä¸ºTest-time IMmunization (TIM)çš„é€šç”¨é˜²å¾¡æ¡†æ¶ã€‚ç”±äºç°æœ‰é˜²å¾¡æ–¹æ³•é€šå¸¸å—é™äºç‰¹å®šæ”»å‡»ç±»å‹ï¼ŒTIMé€šè¿‡ä¸€ç§è‡ªæˆ‘è¿›åŒ–çš„æ–¹å¼å®ç°å¯¹å¤šç§å¯¹æŠ—ç­–ç•¥çš„è‡ªé€‚åº”é˜²å¾¡ã€‚è¯¥æ¡†æ¶é¦–å…ˆè®­ç»ƒä¸€ä¸ªgist tokenç”¨äºæ¨ç†æ—¶çš„å¿«é€Ÿæ£€æµ‹ï¼Œå¹¶åœ¨è¯†åˆ«åˆ°è¶Šç‹±ä¼å›¾æ—¶ï¼Œåˆ©ç”¨æ£€æµ‹åˆ°çš„æŒ‡ä»¤å’Œæ‹’ç»å›ç­”å¯¹æ¨¡å‹è¿›è¡Œå®‰å…¨æ€§å¾®è°ƒ(safety fine-tuning)ã€‚ä¸ºäº†é¿å…å‚æ•°æ›´æ–°å¯¼è‡´æ£€æµ‹å™¨æ€§èƒ½ä¸‹é™ï¼ŒTIMç‰¹åˆ«å°†å¾®è°ƒè¿‡ç¨‹ä¸æ£€æµ‹æ¨¡å—è¿›è¡Œäº†è§£è€¦ã€‚åœ¨LLMså’ŒMLLMsä¸Šçš„å¹¿æ³›å®éªŒç»“æœè¡¨æ˜ï¼ŒTIMèƒ½æœ‰æ•ˆæŠµå¾¡å„ç±»è¶Šç‹±æ”»å‡»ï¼Œæ˜¾è‘—æå‡äº†æ¨¡å‹çš„é²æ£’æ€§ä¸å®‰å…¨æ€§ã€‚",
      "categories": [
        "cs.CR",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.CR",
      "comment": "Under Review",
      "pdf_url": "https://arxiv.org/pdf/2505.22271v1",
      "published_date": "2025-05-28 11:57:46 UTC",
      "updated_date": "2025-05-28 11:57:46 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T15:27:49.079486+00:00"
    },
    {
      "arxiv_id": "2505.22264v1",
      "title": "MRT at SemEval-2025 Task 8: Maximizing Recovery from Tables with Multiple Steps",
      "title_zh": "MRT å‚åŠ  SemEval-2025 ä»»åŠ¡ 8ï¼šé€šè¿‡å¤šæ­¥æµç¨‹æœ€å¤§åŒ–è¡¨æ ¼ä¿¡æ¯æ¢å¤",
      "authors": [
        "Maximiliano HormazÃ¡bal Lagos",
        "Ãlvaro Bueno Saez",
        "HÃ©ctor Cerezo-Costas",
        "Pedro Alonso Doval",
        "Jorge Alcalde Vesteiro"
      ],
      "abstract": "In this paper we expose our approach to solve the \\textit{SemEval 2025 Task 8: Question-Answering over Tabular Data} challenge. Our strategy leverages Python code generation with LLMs to interact with the table and get the answer to the questions. The process is composed of multiple steps: understanding the content of the table, generating natural language instructions in the form of steps to follow in order to get the answer, translating these instructions to code, running it and handling potential errors or exceptions. These steps use open source LLMs and fine grained optimized prompts for each task (step). With this approach, we achieved a score of $70.50\\%$ for subtask 1.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ SemEval-2025 Task 8 ä¸­çš„è¡¨æ ¼æ•°æ®é—®ç­” (Question-Answering over Tabular Data) æŒ‘æˆ˜ï¼Œæå‡ºäº†åä¸º MRT çš„å¤šæ­¥å¤„ç†ç­–ç•¥ã€‚è¯¥æ–¹æ³•çš„æ ¸å¿ƒåœ¨äºåˆ©ç”¨å¤§è¯­è¨€æ¨¡å‹ (LLMs) ç”Ÿæˆ Python ä»£ç ï¼Œä»è€Œå®ç°ä¸è¡¨æ ¼æ•°æ®çš„äº¤äº’å¹¶è·å–ç­”æ¡ˆã€‚æ•´ä¸ªå·¥ä½œæµç”±ç†è§£è¡¨æ ¼å†…å®¹ã€ç”Ÿæˆè‡ªç„¶è¯­è¨€æŒ‡ä»¤ã€å°†æŒ‡ä»¤è½¬åŒ–ä¸ºä»£ç æ‰§è¡Œä»¥åŠå¼‚å¸¸å¤„ç†ç­‰å¤šä¸ªæ­¥éª¤ç»„æˆã€‚é€šè¿‡åœ¨å„ä¸ªç¯èŠ‚é‡‡ç”¨å¼€æº LLMs å’Œç»†ç²’åº¦ä¼˜åŒ–çš„æç¤ºè¯ (Prompts)ï¼ŒMRT èƒ½å¤Ÿæœ‰æ•ˆåœ°å¤„ç†å¤æ‚çš„è¡¨æ ¼æ¨ç†ä»»åŠ¡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å­ä»»åŠ¡ 1 ä¸­å–å¾—äº† 70.50% çš„å¾—åˆ†ï¼Œè¯æ˜äº†é€šè¿‡å¤šæ­¥ä»£ç ç”Ÿæˆè§£å†³è¡¨æ ¼é—®ç­”é—®é¢˜çš„æœ‰æ•ˆæ€§ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.IR"
      ],
      "primary_category": "cs.CL",
      "comment": "7 pages, 6 tables",
      "pdf_url": "https://arxiv.org/pdf/2505.22264v1",
      "published_date": "2025-05-28 11:50:22 UTC",
      "updated_date": "2025-05-28 11:50:22 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T15:28:20.217809+00:00"
    },
    {
      "arxiv_id": "2506.02021v1",
      "title": "Dynamic-Aware Video Distillation: Optimizing Temporal Resolution Based on Video Semantics",
      "title_zh": "åŠ¨æ€æ„ŸçŸ¥è§†é¢‘è’¸é¦ï¼šåŸºäºè§†é¢‘è¯­ä¹‰çš„æ—¶é—´åˆ†è¾¨ç‡ä¼˜åŒ–",
      "authors": [
        "Yinjie Zhao",
        "Heng Zhao",
        "Bihan Wen",
        "Yew-Soon Ong",
        "Joey Tianyi Zhou"
      ],
      "abstract": "With the rapid development of vision tasks and the scaling on datasets and models, redundancy reduction in vision datasets has become a key area of research. To address this issue, dataset distillation (DD) has emerged as a promising approach to generating highly compact synthetic datasets with significantly less redundancy while preserving essential information. However, while DD has been extensively studied for image datasets, DD on video datasets remains underexplored. Video datasets present unique challenges due to the presence of temporal information and varying levels of redundancy across different classes. Existing DD approaches assume a uniform level of temporal redundancy across all different video semantics, which limits their effectiveness on video datasets. In this work, we propose Dynamic-Aware Video Distillation (DAViD), a Reinforcement Learning (RL) approach to predict the optimal Temporal Resolution of the synthetic videos. A teacher-in-the-loop reward function is proposed to update the RL agent policy. To the best of our knowledge, this is the first study to introduce adaptive temporal resolution based on video semantics in video dataset distillation. Our approach significantly outperforms existing DD methods, demonstrating substantial improvements in performance. This work paves the way for future research on more efficient and semantic-adaptive video dataset distillation research.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹è§†é¢‘æ•°æ®é›†è’¸é¦(Dataset Distillation, DD)ä¸­ç°æœ‰æ–¹æ³•å¿½ç•¥ä¸åŒè§†é¢‘è¯­ä¹‰é—´æ—¶é—´å†—ä½™åº¦å·®å¼‚çš„é—®é¢˜ï¼Œæå‡ºäº†Dynamic-Aware Video Distillation (DAViD)æ¡†æ¶ã€‚è¯¥æ¡†æ¶åˆ›æ–°æ€§åœ°å¼•å…¥äº†å¼ºåŒ–å­¦ä¹ (Reinforcement Learning, RL)æ–¹æ³•ï¼Œæ—¨åœ¨æ ¹æ®è§†é¢‘çš„è¯­ä¹‰ä¿¡æ¯åŠ¨æ€é¢„æµ‹åˆæˆè§†é¢‘çš„æœ€ä¼˜æ—¶é—´åˆ†è¾¨ç‡(Temporal Resolution)ã€‚ä¸ºäº†æŒç»­ä¼˜åŒ–æ¨¡å‹ï¼Œç ”ç©¶è€…è®¾è®¡äº†ä¸€ä¸ªå¸¦æœ‰æ•™å¸ˆåœ¨ç¯(Teacher-in-the-loop)æœºåˆ¶çš„å¥–åŠ±å‡½æ•°æ¥æ›´æ–°æ™ºèƒ½ä½“ç­–ç•¥ã€‚ä½œä¸ºé¦–ä¸ªåœ¨è§†é¢‘è’¸é¦é¢†åŸŸå¼•å…¥è‡ªé€‚åº”æ—¶é—´åˆ†è¾¨ç‡çš„ç ”ç©¶ï¼ŒDAViDæœ‰æ•ˆå…‹æœäº†è§†é¢‘æ•°æ®ç‰¹æœ‰çš„æ—¶ç©ºå†—ä½™æŒ‘æˆ˜ã€‚å®éªŒè¯æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å¤šé¡¹æ€§èƒ½æŒ‡æ ‡ä¸Šæ˜¾è‘—ä¼˜äºç°æœ‰çš„è’¸é¦æ‰‹æ®µï¼Œä¸ºæ„å»ºæ›´é«˜æ•ˆä¸”å…·å¤‡è¯­ä¹‰æ„ŸçŸ¥èƒ½åŠ›çš„è§†é¢‘åˆæˆæ•°æ®é›†æä¾›äº†æ–°çš„ç ”ç©¶æ–¹å‘ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.02021v1",
      "published_date": "2025-05-28 11:43:58 UTC",
      "updated_date": "2025-05-28 11:43:58 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T15:28:27.750595+00:00"
    },
    {
      "arxiv_id": "2505.22244v2",
      "title": "A Preprocessing Framework for Efficient Approximate Bi-Objective Shortest-Path Computation in the Presence of Correlated Objectives",
      "title_zh": "é¢å‘ç›¸å…³ç›®æ ‡çš„é«˜æ•ˆè¿‘ä¼¼åŒç›®æ ‡æœ€çŸ­è·¯å¾„è®¡ç®—é¢„å¤„ç†æ¡†æ¶",
      "authors": [
        "Yaron Halle",
        "Ariel Felner",
        "Sven Koenig",
        "Oren Salzman"
      ],
      "abstract": "The bi-objective shortest-path (BOSP) problem seeks to find paths between start and target vertices of a graph while optimizing two conflicting objective functions. We consider the BOSP problem in the presence of correlated objectives. Such correlations often occur in real-world settings such as road networks, where optimizing two positively correlated objectives, such as travel time and fuel consumption, is common. BOSP is generally computationally challenging as the size of the search space is exponential in the number of objective functions and the graph size. Bounded sub-optimal BOSP solvers such as A*pex alleviate this complexity by approximating the Pareto-optimal solution set rather than computing it exactly (given a user-provided approximation factor). As the correlation between objective functions increases, smaller approximation factors are sufficient for collapsing the entire Pareto-optimal set into a single solution. We leverage this insight to propose an efficient algorithm that reduces the search effort in the presence of correlated objectives. Our approach for computing approximations of the entire Pareto-optimal set is inspired by graph-clustering algorithms. It uses a preprocessing phase to identify correlated clusters within a graph and to generate a new graph representation. This allows a natural generalization of A*pex to run up to five times faster on DIMACS dataset instances, a standard benchmark in the field. To the best of our knowledge, this is the first algorithm proposed that efficiently and effectively exploits correlations in the context of bi-objective search while providing theoretical guarantees on solution quality.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†åœ¨ç›®æ ‡å‡½æ•°ç›¸å…³(correlated objectives)çš„æƒ…å†µä¸‹ï¼Œå¦‚ä½•é«˜æ•ˆè§£å†³åŒç›®æ ‡æœ€çŸ­è·¯å¾„(bi-objective shortest-path, BOSP)é—®é¢˜ã€‚é’ˆå¯¹ç°å®ä¸–ç•Œä¸­æ—…è¡Œæ—¶é—´å’Œç‡ƒæ–™æ¶ˆè€—ç­‰å‘ˆæ­£ç›¸å…³ç›®æ ‡çš„åœºæ™¯ï¼Œä¼ ç»Ÿçš„ BOSP ç®—æ³•å› æœç´¢ç©ºé—´éšå›¾è§„æ¨¡å‘ˆæŒ‡æ•°å¢é•¿è€Œé¢ä¸´å·¨å¤§çš„è®¡ç®—å‹åŠ›ã€‚ç ”ç©¶è€…å‘ç°ï¼Œéšç€ç›®æ ‡å‡½æ•°é—´ç›¸å…³æ€§çš„å¢å¼ºï¼Œè¾ƒå°çš„è¿‘ä¼¼å› å­(approximation factor)è¶³ä»¥å°†æ•´ä¸ª Pareto-optimal set å‹ç¼©ä¸ºå•ä¸€è§£ã€‚åŸºäºæ­¤æ´å¯Ÿï¼Œè¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§æ–°å‹é¢„å¤„ç†æ¡†æ¶ï¼Œå€Ÿé‰´å›¾èšç±»ç®—æ³•(graph-clustering algorithms)è¯†åˆ«å›¾ä¸­çš„ç›¸å…³ç°‡å¹¶ç”Ÿæˆæ–°çš„å›¾è¡¨ç¤ºã€‚è¯¥æ–¹æ³•é€šè¿‡å¯¹ A*pex ç®—æ³•çš„è‡ªç„¶æ³›åŒ–ï¼Œåœ¨æ ‡å‡† DIMACS æ•°æ®é›†ä¸Šçš„è¿è¡Œé€Ÿåº¦æ¯”åŸæœ‰åŸºçº¿å¿«äº”å€ã€‚è¿™æ˜¯é¦–ä¸ªåœ¨æä¾›ç†è®ºè´¨é‡ä¿è¯çš„å‰æä¸‹ï¼Œæœ‰æ•ˆåˆ©ç”¨ç›®æ ‡ç›¸å…³æ€§æå‡åŒç›®æ ‡æœç´¢æ•ˆç‡çš„ç®—æ³•ï¼Œæ˜¾è‘—é™ä½äº†æœç´¢è¿‡ç¨‹ä¸­çš„è®¡ç®—å¼€é”€ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.22244v2",
      "published_date": "2025-05-28 11:26:14 UTC",
      "updated_date": "2025-09-25 08:40:30 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T15:28:01.288812+00:00"
    },
    {
      "arxiv_id": "2505.22232v2",
      "title": "Judging Quality Across Languages: A Multilingual Approach to Pretraining Data Filtering with Language Models",
      "title_zh": "è·¨è¯­è¨€è´¨é‡è¯„å®šï¼šåŸºäºè¯­è¨€æ¨¡å‹çš„å¤šè¯­è¨€é¢„è®­ç»ƒæ•°æ®è¿‡æ»¤æ–¹æ³•",
      "authors": [
        "Mehdi Ali",
        "Manuel Brack",
        "Max LÃ¼bbering",
        "Elias Wendt",
        "Abbas Goher Khan",
        "Richard Rutmann",
        "Alex Jude",
        "Maurice Kraus",
        "Alexander Arno Weber",
        "David KaczÃ©r",
        "Florian Mai",
        "Lucie Flek",
        "Rafet Sifa",
        "Nicolas Flores-Herr",
        "Joachim KÃ¶hler",
        "Patrick Schramowski",
        "Michael Fromm",
        "Kristian Kersting"
      ],
      "abstract": "High-quality multilingual training data is essential for effectively pretraining large language models (LLMs). Yet, the availability of suitable open-source multilingual datasets remains limited. Existing state-of-the-art datasets mostly rely on heuristic filtering methods, restricting both their cross-lingual transferability and scalability. Here, we introduce JQL, a systematic approach that efficiently curates diverse and high-quality multilingual data at scale while significantly reducing computational demands. JQL distills LLMs' annotation capabilities into lightweight annotators based on pretrained multilingual embeddings. These models exhibit robust multilingual and cross-lingual performance, even for languages and scripts unseen during training. Evaluated empirically across 35 languages, the resulting annotation pipeline substantially outperforms current heuristic filtering methods like Fineweb2. JQL notably enhances downstream model training quality and increases data retention rates. Our research provides practical insights and valuable resources for multilingual data curation, raising the standards of multilingual dataset development.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†JQL(Judging Quality Across Languages)ï¼Œä¸€ç§ç³»ç»Ÿæ€§ç­–åˆ’åˆ†æ•£ä¸”é«˜è´¨é‡å¤šè¯­è¨€æ•°æ®çš„é¢„è®­ç»ƒè¿‡æ»¤æ–¹æ³•ï¼Œæ—¨åœ¨è§£å†³ç°æœ‰å¯å‘å¼(heuristic)æ–¹æ³•åœ¨è·¨è¯­è¨€è¿ç§»å’Œæ‰©å±•æ€§ä¸Šçš„å±€é™ã€‚JQLé€šè¿‡å°†å¤§è¯­è¨€æ¨¡å‹(LLMs)çš„æ ‡æ³¨èƒ½åŠ›è’¸é¦(distill)åˆ°åŸºäºé¢„è®­ç»ƒå¤šè¯­è¨€åµŒå…¥(multilingual embeddings)çš„è½»é‡çº§æ ‡æ³¨å™¨ä¸­ï¼Œåœ¨æ˜¾è‘—é™ä½è®¡ç®—éœ€æ±‚çš„åŒæ—¶ä¿ç•™äº†å¤æ‚çš„è´¨é‡åˆ¤æ–­èƒ½åŠ›ã€‚å®éªŒè¯æ˜ï¼Œè¯¥æ–¹æ³•åœ¨35ç§è¯­è¨€ä¸­è¡¨ç°å‡ºå¼ºå¤§çš„è·¨è¯­è¨€æ€§èƒ½ï¼Œå³ä½¿å¯¹äºè®­ç»ƒä¸­æœªè§çš„è¯­è¨€å’Œè„šæœ¬ä¾ç„¶æœ‰æ•ˆã€‚åœ¨å®è¯è¯„ä¼°ä¸­ï¼ŒJQLæ„å»ºçš„æ ‡æ³¨ç®¡çº¿(annotation pipeline)æ€§èƒ½å¤§å¹…ä¼˜äºFineweb2ç­‰ä¸»æµå¯å‘å¼è¿‡æ»¤æ–¹æ³•ã€‚æ­¤å¤–ï¼ŒJQLæ˜¾è‘—æå‡äº†ä¸‹æ¸¸æ¨¡å‹çš„è®­ç»ƒè´¨é‡å¹¶æé«˜äº†æ•°æ®ç•™å­˜ç‡(data retention rates)ï¼Œä¸ºå¤šè¯­è¨€æ•°æ®é›†çš„å¼€å‘ä¸ä¼˜åŒ–æä¾›äº†æå…·ä»·å€¼çš„èµ„æºå’Œæ ‡å‡†ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "Project page available at https://huggingface.co/spaces/Jackal-AI/JQL",
      "pdf_url": "https://arxiv.org/pdf/2505.22232v2",
      "published_date": "2025-05-28 11:06:54 UTC",
      "updated_date": "2025-05-31 15:28:40 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T15:28:25.645759+00:00"
    },
    {
      "arxiv_id": "2505.22224v2",
      "title": "Solver-Free Decision-Focused Learning for Linear Optimization Problems",
      "title_zh": "çº¿æ€§ä¼˜åŒ–é—®é¢˜çš„å…æ±‚è§£å™¨å†³ç­–å¯¼å‘å­¦ä¹ ",
      "authors": [
        "Senne Berden",
        "Ali Ä°rfan MahmutoÄŸullarÄ±",
        "Dimos Tsouros",
        "Tias Guns"
      ],
      "abstract": "Mathematical optimization is a fundamental tool for decision-making in a wide range of applications. However, in many real-world scenarios, the parameters of the optimization problem are not known a priori and must be predicted from contextual features. This gives rise to predict-then-optimize problems, where a machine learning model predicts problem parameters that are then used to make decisions via optimization. A growing body of work on decision-focused learning (DFL) addresses this setting by training models specifically to produce predictions that maximize downstream decision quality, rather than accuracy. While effective, DFL is computationally expensive, because it requires solving the optimization problem with the predicted parameters at each loss evaluation. In this work, we address this computational bottleneck for linear optimization problems, a common class of problems in both DFL literature and real-world applications. We propose a solver-free training method that exploits the geometric structure of linear optimization to enable efficient training with minimal degradation in solution quality. Our method is based on the insight that a solution is optimal if and only if it achieves an objective value that is at least as good as that of its adjacent vertices on the feasible polytope. Building on this, our method compares the estimated quality of the ground-truth optimal solution with that of its precomputed adjacent vertices, and uses this as loss function. Experiments demonstrate that our method significantly reduces computational cost while maintaining high decision quality.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹çº¿æ€§ä¼˜åŒ–é—®é¢˜æå‡ºäº†ä¸€ç§ Solver-Free çš„å†³ç­–èšç„¦å­¦ä¹  (Decision-Focused Learning) è®­ç»ƒæ–¹æ³•ï¼Œæ—¨åœ¨è§£å†³ä¼ ç»Ÿ DFL åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­å› åå¤è°ƒç”¨ä¼˜åŒ–æ±‚è§£å™¨è€Œå¯¼è‡´çš„è®¡ç®—å¼€é”€å·¨å¤§é—®é¢˜ã€‚ä½œè€…åˆ©ç”¨çº¿æ€§ä¼˜åŒ–çš„å‡ ä½•ç»“æ„ç‰¹æ€§ï¼Œå³æœ€ä¼˜è§£çš„ç›®æ ‡å€¼å¿…é¡»ä¼˜äºæˆ–ç­‰äºå…¶åœ¨å¯è¡Œå¤šèƒä½“ (Polytope) ä¸Šç›¸é‚»é¡¶ç‚¹çš„ç›®æ ‡å€¼ï¼Œæå‡ºäº†ä¸€ç§åŸºäºé¡¶ç‚¹æ¯”è¾ƒçš„æŸå¤±å‡½æ•°ã€‚è¯¥æ–¹æ³•é€šè¿‡å¯¹æ¯”çœŸå®æœ€ä¼˜è§£ä¸é¢„å…ˆè®¡ç®—çš„ç›¸é‚»é¡¶ç‚¹çš„ä¼°è®¡è´¨é‡ï¼Œæœ‰æ•ˆè§„é¿äº†è®­ç»ƒä¸­çš„åœ¨çº¿æ±‚è§£è¿‡ç¨‹ã€‚å®éªŒç»“æœè¯æ˜ï¼Œè¯¥æ–¹æ³•åœ¨æ˜¾è‘—é™ä½è®¡ç®—æˆæœ¬çš„åŒæ—¶ï¼Œèƒ½å¤Ÿä¿æŒæé«˜çš„å†³ç­–è´¨é‡ã€‚è¿™ä¸€ç ”ç©¶ä¸ºåœ¨å¤§è§„æ¨¡å®é™…åœºæ™¯ä¸­éƒ¨ç½²é«˜æ•ˆçš„ predict-then-optimize æ¨¡å‹æä¾›äº†é‡è¦æŠ€æœ¯æ”¯æŒã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.22224v2",
      "published_date": "2025-05-28 10:55:16 UTC",
      "updated_date": "2025-11-12 09:18:32 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T15:28:32.512305+00:00"
    },
    {
      "arxiv_id": "2505.22203v2",
      "title": "From Accuracy to Robustness: A Study of Rule- and Model-based Verifiers in Mathematical Reasoning",
      "title_zh": "ä»å‡†ç¡®æ€§è¿ˆå‘é²æ£’æ€§ï¼šæ•°å­¦æ¨ç†ä¸­åŸºäºè§„åˆ™ä¸æ¨¡å‹çš„éªŒè¯å™¨ç ”ç©¶",
      "authors": [
        "Yuzhen Huang",
        "Weihao Zeng",
        "Xingshan Zeng",
        "Qi Zhu",
        "Junxian He"
      ],
      "abstract": "Trustworthy verifiers are essential for the success of reinforcement learning with verifiable reward (RLVR), which is the core methodology behind various large reasoning models such as DeepSeek-R1. In complex domains like mathematical reasoning, rule-based verifiers have been widely adopted in previous works to train strong reasoning models. However, the reliability of these verifiers and their impact on the RL training process remain poorly understood. In this work, we take mathematical reasoning as a case study and conduct a comprehensive analysis of various verifiers in both static evaluation and RL training scenarios. First, we find that current open-source rule-based verifiers often fail to recognize equivalent answers presented in different formats across multiple commonly used mathematical datasets, resulting in non-negligible false negative rates. This limitation adversely affects RL training performance and becomes more pronounced as the policy model gets stronger. Subsequently, we investigate model-based verifiers as a potential solution to address these limitations. While the static evaluation shows that model-based verifiers achieve significantly higher verification accuracy, further analysis and RL results imply that they are highly susceptible to hacking, where they misclassify certain patterns in responses as correct, particularly after fine-tuning. This vulnerability is exploited during policy model optimization, leading to artificially inflated rewards. Our findings underscore the unique challenges inherent to both rule-based and model-based verifiers and provide insights toward developing more accurate and robust reward systems for reinforcement learning.",
      "tldr_zh": "è¯¥ç ”ç©¶æ·±å…¥æ¢è®¨äº†æ•°å­¦æ¨ç†é¢†åŸŸä¸­è§„åˆ™éªŒè¯å™¨(Rule-based Verifiers)ä¸æ¨¡å‹éªŒè¯å™¨(Model-based Verifiers)åœ¨å¯éªŒè¯å¥–åŠ±å¼ºåŒ–å­¦ä¹ (RLVR)ä¸­çš„è¡¨ç°åŠå…¶å¯¹è®­ç»ƒè¿‡ç¨‹çš„å½±å“ã€‚ç ”ç©¶æŒ‡å‡ºï¼Œç°æœ‰çš„è§„åˆ™éªŒè¯å™¨ç”±äºéš¾ä»¥è¯†åˆ«ä¸åŒæ ¼å¼çš„ç­‰ä»·ç­”æ¡ˆï¼Œå­˜åœ¨æ˜¾è‘—çš„å‡é˜´æ€§ç‡(False Negative Rates)ï¼Œä¸”è¿™ä¸€å±€é™æ€§ä¼šéšç€ç­–ç•¥æ¨¡å‹çš„å¢å¼ºè€Œæ„ˆå‘ä¸¥é‡ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼Œè™½ç„¶æ¨¡å‹éªŒè¯å™¨åœ¨é™æ€è¯„ä¼°ä¸­å…·æœ‰æ›´é«˜çš„å‡†ç¡®æ€§ï¼Œä½†åœ¨å¼ºåŒ–å­¦ä¹ ä¼˜åŒ–è¿‡ç¨‹ä¸­ææ˜“å—åˆ°å¥–åŠ±ç ´è§£(Reward Hacking)çš„å½±å“ï¼Œè¯¯å°†æŸäº›é”™è¯¯æ¨¡å¼è¯†åˆ«ä¸ºæ­£ç¡®ã€‚è¯¥å‘ç°æ­ç¤ºäº†å½“å‰ä¸»æµéªŒè¯æ–¹æ³•åœ¨å‡†ç¡®æ€§ä¸é²æ£’æ€§ä¹‹é—´çš„å›ºæœ‰çŸ›ç›¾ï¼Œä¸ºå¼€å‘æ›´ç¨³å¥çš„å¤§å‹æ¨ç†æ¨¡å‹å¥–åŠ±ç³»ç»Ÿæä¾›äº†å…³é”®è§è§£ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.22203v2",
      "published_date": "2025-05-28 10:28:41 UTC",
      "updated_date": "2025-10-07 08:28:26 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T15:28:38.206993+00:00"
    },
    {
      "arxiv_id": "2505.22202v2",
      "title": "Latent Reasoning via Sentence Embedding Prediction",
      "title_zh": "åŸºäºå¥å­åµŒå…¥é¢„æµ‹çš„éšå¼æ¨ç†",
      "authors": [
        "Hyeonbin Hwang",
        "Byeongguk Jeon",
        "Seungone Kim",
        "Jiyeon Kim",
        "Hoyeon Chang",
        "Sohee Yang",
        "Seungpil Won",
        "Dohaeng Lee",
        "Youbin Ahn",
        "Minjoon Seo"
      ],
      "abstract": "Autoregressive language models (LMs) generate one token at a time, yet human reasoning operates over higher-level abstractions - sentences, propositions, and concepts. This contrast raises a central question- Can LMs likewise learn to reason over structured semantic units rather than raw token sequences? In this work, we investigate whether pretrained LMs can be lifted into such abstract reasoning spaces by building on their learned representations. We present a framework that adapts a pretrained token-level LM to operate in sentence space by autoregressively predicting continuous embeddings of next sentences. We explore two embedding paradigms inspired by classical representation learning: 1) semantic embeddings, learned via autoencoding to preserve surface meaning; and 2) contextual embeddings, trained via next-sentence prediction to encode anticipatory structure. We evaluate both under two inference regimes: Discretized, which decodes each predicted embedding into text before re-encoding; and Continuous, which reasons entirely in embedding space for improved efficiency. Across four domains - mathematics, logic, commonsense, and planning - contextual embeddings under continuous inference show competitive performance with Chain-of-Thought (CoT) while reducing inference-time FLOPs on average by half. We also present early signs of scalability and modular adaptation. Finally, to visualize latent trajectories, we introduce SentenceLens, a diagnostic tool that decodes intermediate model states into interpretable sentences. Together, our results indicate that pretrained LMs can effectively transition to abstract, structured reasoning within latent embedding spaces.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†è‡ªå›å½’è¯­è¨€æ¨¡å‹(LMs)èƒ½å¦åƒäººç±»ä¸€æ ·åœ¨å¥å­æˆ–æ¦‚å¿µç­‰æŠ½è±¡å±‚çº§è€ŒéåŸå§‹tokenåºåˆ—ä¸Šè¿›è¡Œæ¨ç†ã€‚ç ”ç©¶è€…æå‡ºäº†ä¸€ç§æ–°æ¡†æ¶ï¼Œé€šè¿‡è‡ªå›å½’é¢„æµ‹ä¸‹ä¸€å¥çš„è¿ç»­åµŒå…¥(embeddings)ï¼Œä½¿é¢„è®­ç»ƒæ¨¡å‹èƒ½å¤Ÿåœ¨å¥å­ç©ºé—´å†…è¿è¡Œã€‚è¯¥å·¥ä½œåˆ†æäº†é€šè¿‡è‡ªç¼–ç (autoencoding)ä¿ç•™è¡¨é¢è¯­ä¹‰çš„è¯­ä¹‰åµŒå…¥(semantic embeddings)ï¼Œä»¥åŠé€šè¿‡ä¸‹ä¸€å¥é¢„æµ‹(next-sentence prediction)ç¼–ç ç»“æ„çš„ä¸Šä¸‹æ–‡åµŒå…¥(contextual embeddings)ã€‚å®éªŒæ¶µç›–æ•°å­¦ã€é€»è¾‘ã€å¸¸è¯†å’Œè§„åˆ’å››å¤§é¢†åŸŸï¼Œç»“æœæ˜¾ç¤ºåœ¨è¿ç»­æ¨ç†(Continuous inference)æ¨¡å¼ä¸‹ï¼Œä¸Šä¸‹æ–‡åµŒå…¥çš„æ€§èƒ½ä¸é“¾å¼æ€ç»´(Chain-of-Thought)ç›¸å½“ï¼Œä¸”æ¨ç†æ—¶çš„FLOPså¹³å‡é™ä½äº†ä¸€åŠã€‚æ­¤å¤–ï¼Œç ”ç©¶å¼•å…¥äº†è¯Šæ–­å·¥å…·SentenceLensï¼Œæ—¨åœ¨å°†æ½œç©ºé—´çŠ¶æ€è§£ç ä¸ºå¯è§£é‡Šçš„å¥å­ä»¥å¯è§†åŒ–æ¨ç†è½¨è¿¹ã€‚è¿™ä¸€æˆæœè¯æ˜äº†é¢„è®­ç»ƒæ¨¡å‹åœ¨æ½œç©ºé—´è¿›è¡ŒæŠ½è±¡ã€ç»“æ„åŒ–æ¨ç†çš„æœ‰æ•ˆæ€§ï¼Œå¹¶å±•ç°äº†è‰¯å¥½çš„æ‰©å±•æ€§ä¸æ¨¡å—åŒ–é€‚åº”æ½œåŠ›ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "Previously titled \"Let's Predict Sentence by Sentence\"; Presented @ COLM RAM 2 Workshop (Oral)",
      "pdf_url": "https://arxiv.org/pdf/2505.22202v2",
      "published_date": "2025-05-28 10:28:35 UTC",
      "updated_date": "2025-10-11 06:55:53 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T15:29:03.011443+00:00"
    },
    {
      "arxiv_id": "2505.22200v1",
      "title": "Investigating Mechanisms for In-Context Vision Language Binding",
      "title_zh": "æ¢ç©¶è§†è§‰-è¯­è¨€ä¸Šä¸‹æ–‡ç»‘å®šçš„æœºåˆ¶",
      "authors": [
        "Darshana Saravanan",
        "Makarand Tapaswi",
        "Vineet Gandhi"
      ],
      "abstract": "To understand a prompt, Vision-Language models (VLMs) must perceive the image, comprehend the text, and build associations within and across both modalities. For instance, given an 'image of a red toy car', the model should associate this image to phrases like 'car', 'red toy', 'red object', etc. Feng and Steinhardt propose the Binding ID mechanism in LLMs, suggesting that the entity and its corresponding attribute tokens share a Binding ID in the model activations. We investigate this for image-text binding in VLMs using a synthetic dataset and task that requires models to associate 3D objects in an image with their descriptions in the text. Our experiments demonstrate that VLMs assign a distinct Binding ID to an object's image tokens and its textual references, enabling in-context association.",
      "tldr_zh": "è¯¥ç ”ç©¶è°ƒæŸ¥äº†è§†è§‰è¯­è¨€æ¨¡å‹ (VLMs) ä¸­ä¸Šä¸‹æ–‡å†…çš„è§†è§‰è¯­è¨€ç»‘å®š (Vision Language Binding) æœºåˆ¶ï¼Œæ—¨åœ¨ç†è§£æ¨¡å‹å¦‚ä½•å°†å›¾åƒéƒ¨åˆ†ä¸æ–‡æœ¬æè¿°å»ºç«‹å…³è”ã€‚ç ”ç©¶å€Ÿé‰´äº† Large Language Models (LLMs) ä¸­çš„ Binding ID ç†è®ºï¼Œé€šè¿‡åˆæˆæ•°æ®é›†å’Œ 3D ç‰©ä½“å…³è”ä»»åŠ¡å¯¹æ¨¡å‹æ¿€æ´»è¿›è¡Œäº†æ·±å…¥åˆ†æã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒVLMs ä¼šä¸ºå›¾åƒä¸­çš„ç‰©ä½“ token åŠå…¶å¯¹åº”çš„æ–‡æœ¬å¼•ç”¨åˆ†é…ä¸€ä¸ªç‰¹å®šçš„ Binding IDï¼Œä»è€Œåœ¨æ¨¡å‹å†…éƒ¨å®ç°ç²¾ç¡®çš„å›¾æ–‡åŒ¹é…ã€‚è¿™ä¸€å‘ç°æ­ç¤ºäº† VLMs å®ç° in-context association çš„åº•å±‚æœºåˆ¶ï¼Œè¯æ˜äº† Binding ID æ˜¯è·¨æ¨¡æ€ä¿¡æ¯æ•´åˆçš„å…³é”®è½½ä½“ï¼Œä¸ºç†è§£å¤šæ¨¡æ€æ¨¡å‹çš„å†…éƒ¨å·¥ä½œåŸç†æä¾›äº†é‡è¦è¯æ®ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "Accepted to MIV at CVPRW 2025 (Oral)",
      "pdf_url": "https://arxiv.org/pdf/2505.22200v1",
      "published_date": "2025-05-28 10:25:43 UTC",
      "updated_date": "2025-05-28 10:25:43 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T15:28:40.123144+00:00"
    },
    {
      "arxiv_id": "2505.22199v1",
      "title": "Enhancing Uncertainty Estimation and Interpretability via Bayesian Non-negative Decision Layer",
      "title_zh": "åŸºäºè´å¶æ–¯éè´Ÿå†³ç­–å±‚çš„ä¸ç¡®å®šæ€§ä¼°è®¡ä¸å¯è§£é‡Šæ€§æå‡",
      "authors": [
        "Xinyue Hu",
        "Zhibin Duan",
        "Bo Chen",
        "Mingyuan Zhou"
      ],
      "abstract": "Although deep neural networks have demonstrated significant success due to their powerful expressiveness, most models struggle to meet practical requirements for uncertainty estimation. Concurrently, the entangled nature of deep neural networks leads to a multifaceted problem, where various localized explanation techniques reveal that multiple unrelated features influence the decisions, thereby undermining interpretability. To address these challenges, we develop a Bayesian Non-negative Decision Layer (BNDL), which reformulates deep neural networks as a conditional Bayesian non-negative factor analysis. By leveraging stochastic latent variables, the BNDL can model complex dependencies and provide robust uncertainty estimation. Moreover, the sparsity and non-negativity of the latent variables encourage the model to learn disentangled representations and decision layers, thereby improving interpretability. We also offer theoretical guarantees that BNDL can achieve effective disentangled learning. In addition, we developed a corresponding variational inference method utilizing a Weibull variational inference network to approximate the posterior distribution of the latent variables. Our experimental results demonstrate that with enhanced disentanglement capabilities, BNDL not only improves the model's accuracy but also provides reliable uncertainty estimation and improved interpretability.",
      "tldr_zh": "è¯¥ç ”ç©¶å¼€å‘äº†Bayesian Non-negative Decision Layer (BNDL)ï¼Œæ—¨åœ¨è§£å†³æ·±åº¦ç¥ç»ç½‘ç»œåœ¨Uncertainty Estimationå’ŒInterpretabilityæ–¹é¢é¢ä¸´çš„æŒ‘æˆ˜ã€‚BNDLå°†ç¥ç»ç½‘ç»œé‡æ–°è¡¨è¿°ä¸ºConditional Bayesian non-negative factor analysisï¼Œé€šè¿‡å¼•å…¥éšæœºæ½œå˜é‡æ¥å»ºæ¨¡å¤æ‚çš„ä¾èµ–å…³ç³»å¹¶æä¾›ç¨³å¥çš„ä¸ç¡®å®šæ€§è¯„ä¼°ã€‚åˆ©ç”¨æ½œå˜é‡çš„Sparsityå’ŒNon-negativityï¼Œè¯¥æ¨¡å‹èƒ½å¤Ÿå­¦ä¹ åˆ°è§£è€¦çš„Disentangled Representationså’Œå†³ç­–å±‚ï¼Œä»è€Œæ˜¾è‘—æå‡äº†æ¨¡å‹çš„å¯è§£é‡Šæ€§ã€‚ç ”ç©¶å›¢é˜Ÿè¿˜è®¾è®¡äº†åŸºäºWeibull variational inference networkçš„å˜åˆ†æ¨æ–­æ–¹æ³•ï¼Œç”¨ä»¥é«˜æ•ˆè¿‘ä¼¼æ½œå˜é‡çš„åéªŒåˆ†å¸ƒã€‚ç†è®ºä¿è¯ä¸å®éªŒç»“æœä¸€è‡´è¡¨æ˜ï¼ŒBNDLåœ¨å¢å¼ºæ¨¡å‹è§£è€¦èƒ½åŠ›çš„åŒæ—¶ï¼Œä¸ä»…æé«˜äº†é¢„æµ‹å‡†ç¡®ç‡ï¼Œè¿˜ä¸ºå®é™…åº”ç”¨æä¾›äº†å¯é çš„ä¸ç¡®å®šæ€§ä¼°è®¡ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "Accepted by The Thirteenth International Conference on Learning Representations (ICLR 2025)",
      "pdf_url": "https://arxiv.org/pdf/2505.22199v1",
      "published_date": "2025-05-28 10:23:34 UTC",
      "updated_date": "2025-05-28 10:23:34 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T15:29:12.729113+00:00"
    },
    {
      "arxiv_id": "2505.22193v1",
      "title": "Physics-inspired Generative AI models via real hardware-based noisy quantum diffusion",
      "title_zh": "åŸºäºçœŸå®ç¡¬ä»¶å«å™ªé‡å­æ‰©æ•£çš„å—ç‰©ç†å¯å‘ç”Ÿæˆå¼äººå·¥æ™ºèƒ½æ¨¡å‹",
      "authors": [
        "Marco Parigi",
        "Stefano Martina",
        "Francesco Aldo Venturelli",
        "Filippo Caruso"
      ],
      "abstract": "Quantum Diffusion Models (QDMs) are an emerging paradigm in Generative AI that aims to use quantum properties to improve the performances of their classical counterparts. However, existing algorithms are not easily scalable due to the limitations of near-term quantum devices. Following our previous work on QDMs, here we propose and implement two physics-inspired protocols. In the first, we use the formalism of quantum stochastic walks, showing that a specific interplay of quantum and classical dynamics in the forward process produces statistically more robust models generating sets of MNIST images with lower FrÃ©chet Inception Distance (FID) than using totally classical dynamics. In the second approach, we realize an algorithm to generate images by exploiting the intrinsic noise of real IBM quantum hardware with only four qubits. Our work could be a starting point to pave the way for new scenarios for large-scale algorithms in quantum Generative AI, where quantum noise is neither mitigated nor corrected, but instead exploited as a useful resource.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†ç‰©ç†å¯å‘å¼çš„é‡å­æ‰©æ•£æ¨¡å‹(Quantum Diffusion Models)ï¼Œæ—¨åœ¨åˆ©ç”¨é‡å­ç‰¹æ€§æå‡ç”Ÿæˆå¼äººå·¥æ™ºèƒ½(Generative AI)çš„æ€§èƒ½å¹¶è§£å†³è¿‘æœŸé‡å­è®¾å¤‡çš„æ‰©å±•æ€§éš¾é¢˜ã€‚ä½œè€…æå‡ºäº†ä¸¤ç§ç‰©ç†å¯å‘å¼åè®®ï¼šç¬¬ä¸€ç§åŸºäºé‡å­éšæœºè¡Œèµ°(quantum stochastic walks)å½¢å¼åŒ–ï¼Œé€šè¿‡åœ¨å‰å‘è¿‡ç¨‹ä¸­ç»“åˆé‡å­ä¸ç»å…¸åŠ¨åŠ›å­¦ï¼Œç”Ÿæˆäº†å…·æœ‰æ›´ä½FrÃ©chet Inception Distance (FID)çš„MNISTå›¾åƒï¼Œè¡¨ç°å‡ºæ¯”çº¯ç»å…¸åŠ¨åŠ›å­¦æ›´å¼ºçš„ç»Ÿè®¡é²æ£’æ€§ã€‚ç¬¬äºŒç§åè®®åˆ™å®ç°äº†ä¸€ç§ä»…åˆ©ç”¨å››ä¸ªé‡å­ä½(qubits)çš„çœŸå®IBMé‡å­ç¡¬ä»¶å›ºæœ‰å™ªå£°æ¥ç”Ÿæˆå›¾åƒçš„ç®—æ³•ã€‚è¯¥å·¥ä½œä¸ä»…ä¸ºå¤§è§„æ¨¡é‡å­ç”Ÿæˆå¼äººå·¥æ™ºèƒ½æä¾›äº†æ–°æ–¹æ¡ˆï¼Œæ›´é‡è¦çš„æ˜¯è¯æ˜äº†é‡å­å™ªå£°å¯ä»¥ä½œä¸ºæœ‰ç”¨èµ„æºè¢«ç›´æ¥åˆ©ç”¨ï¼Œè€Œéå•çº¯éœ€è¦è¢«ç¼“è§£æˆ–æ ¡æ­£ã€‚",
      "categories": [
        "quant-ph",
        "cond-mat.dis-nn",
        "cs.AI",
        "cs.CV",
        "cs.LG"
      ],
      "primary_category": "quant-ph",
      "comment": "17 pages, 9 figures. Supplementary materials: 2 pages, 2 figures",
      "pdf_url": "https://arxiv.org/pdf/2505.22193v1",
      "published_date": "2025-05-28 10:11:48 UTC",
      "updated_date": "2025-05-28 10:11:48 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T15:29:53.413794+00:00"
    },
    {
      "arxiv_id": "2505.22184v2",
      "title": "Breaking the Cloak! Unveiling Chinese Cloaked Toxicity with Homophone Graph and Toxic Lexicon",
      "title_zh": "ç ´è§£ä¼ªè£…ï¼šåˆ©ç”¨åŒéŸ³è¯å›¾ä¸æ¯’æ€§è¯åº“æ­ç¤ºä¸­æ–‡éšåŒ¿æ€§æ¯’æ€§è¨€è®º",
      "authors": [
        "Xuchen Ma",
        "Jianxiang Yu",
        "Wenming Shao",
        "Bo Pang",
        "Xiang Li"
      ],
      "abstract": "Social media platforms have experienced a significant rise in toxic content, including abusive language and discriminatory remarks, presenting growing challenges for content moderation. Some users evade censorship by deliberately disguising toxic words through homophonic cloak, which necessitates the task of unveiling cloaked toxicity. Existing methods are mostly designed for English texts, while Chinese cloaked toxicity unveiling has not been solved yet. To tackle the issue, we propose C$^2$TU, a novel training-free and prompt-free method for Chinese cloaked toxic content unveiling. It first employs substring matching to identify candidate toxic words based on Chinese homo-graph and toxic lexicon. Then it filters those candidates that are non-toxic and corrects cloaks to be their corresponding toxicities. Specifically, we develop two model variants for filtering, which are based on BERT and LLMs, respectively. For LLMs, we address the auto-regressive limitation in computing word occurrence probability and utilize the full semantic contexts of a text sequence to reveal cloaked toxic words. Extensive experiments demonstrate that C$^2$TU can achieve superior performance on two Chinese toxic datasets. In particular, our method outperforms the best competitor by up to 71% on the F1 score and 35% on accuracy, respectively. Our code and data are available at https://github.com/XDxc-cuber/C2TU-Chinese-cloaked-toxicity-unveiling.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ç¤¾äº¤åª’ä½“ä¸­é€šè¿‡åŒéŸ³å­—é®è”½(homophonic cloak)è§„é¿å®¡æŸ¥çš„ç°è±¡ï¼Œæå‡ºäº†C$^2$TUï¼Œä¸€ç§æ— éœ€è®­ç»ƒä¸”æ— éœ€æç¤º(training-free and prompt-free)çš„ä¸­æ–‡é®è”½æ¯’æ€§å†…å®¹æ­ç¤ºæ–¹æ³•ã€‚è¯¥æ–¹æ³•é¦–å…ˆé€šè¿‡å­å­—ç¬¦ä¸²åŒ¹é…(substring matching)ï¼Œç»“åˆä¸­æ–‡åŒéŸ³å¼‚å½¢å›¾(homo-graph)å’Œæ¯’æ€§è¯å…¸(toxic lexicon)æ¥è¯†åˆ«æ½œåœ¨çš„å€™é€‰æ¯’æ€§è¯ã€‚éšåï¼Œæ¡†æ¶ä¼šè¿‡æ»¤éæ¯’æ€§å€™é€‰è¯å¹¶å°†é®è”½è¯ä¿®æ­£ä¸ºå…¶å¯¹åº”çš„æ¯’æ€§å½¢å¼ï¼Œç ”ç©¶è€…ä¸ºæ­¤åˆ†åˆ«å¼€å‘äº†åŸºäºBERTå’Œå¤§å‹è¯­è¨€æ¨¡å‹(LLMs)çš„ä¸¤ç§æ¨¡å‹å˜ä½“ã€‚é’ˆå¯¹LLMså˜ä½“ï¼Œè¯¥ç ”ç©¶å…‹æœäº†åœ¨è®¡ç®—è¯å‡ºç°æ¦‚ç‡æ—¶çš„è‡ªå›å½’(auto-regressive)å±€é™æ€§ï¼Œå¹¶å……åˆ†åˆ©ç”¨æ–‡æœ¬åºåˆ—çš„å®Œæ•´è¯­ä¹‰ä¸Šä¸‹æ–‡æ¥è¿˜åŸçœŸå®è¯ä¹‰ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒC$^2$TUåœ¨ä¸¤ä¸ªä¸­æ–‡æ¯’æ€§æ•°æ®é›†ä¸Šå–å¾—äº†æ˜¾è‘—ä¼˜äºç°æœ‰æ¨¡å‹çš„æ•ˆæœï¼Œå…¶ä¸­F1åˆ†æ•°æå‡é«˜è¾¾71%ï¼Œå‡†ç¡®ç‡æå‡äº†35%ã€‚è¯¥ç ”ç©¶ä¸ä»…å¼€æºäº†ä»£ç å’Œæ•°æ®ï¼Œä¹Ÿä¸ºè§£å†³ä¸­æ–‡è¯­å¢ƒä¸‹å¤æ‚çš„è§„é¿å¼æ¯’æ€§å†…å®¹æ£€æµ‹æä¾›äº†åˆ›æ–°çš„æŠ€æœ¯æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "25 pages, 5 figures, 9 tables",
      "pdf_url": "https://arxiv.org/pdf/2505.22184v2",
      "published_date": "2025-05-28 09:58:15 UTC",
      "updated_date": "2025-06-05 04:47:25 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T15:30:11.226434+00:00"
    },
    {
      "arxiv_id": "2505.22179v2",
      "title": "Speculative Decoding Meets Quantization: Compatibility Evaluation and Hierarchical Framework Design",
      "title_zh": "æŠ•æœºæ€§è§£ç é‡ä¸Šé‡åŒ–ï¼šå…¼å®¹æ€§è¯„ä¼°ä¸åˆ†å±‚æ¡†æ¶è®¾è®¡",
      "authors": [
        "Yudi Zhang",
        "Weilin Zhao",
        "Xu Han",
        "Tiejun Zhao",
        "Wang Xu",
        "Hailong Cao",
        "Conghui Zhu"
      ],
      "abstract": "Speculative decoding and quantization effectively accelerate memory-bound inference of large language models. Speculative decoding mitigates the memory bandwidth bottleneck by verifying multiple tokens within a single forward pass, which increases computational effort. Quantization achieves this optimization by compressing weights and activations into lower bit-widths and also reduces computations via low-bit matrix multiplications. To further leverage their strengths, we investigate the integration of these two techniques. Surprisingly, experiments applying the advanced speculative decoding method EAGLE-2 to various quantized models reveal that the memory benefits from 4-bit weight quantization are diminished by the computational load from speculative decoding. Specifically, verifying a tree-style draft incurs significantly more time overhead than a single-token forward pass on 4-bit weight quantized models. This finding led to our new speculative decoding design: a hierarchical framework that employs a small model as an intermediate stage to turn tree-style drafts into sequence drafts, leveraging the memory access benefits of the target quantized model. Experimental results show that our hierarchical approach achieves a 2.78$\\times$ speedup across various tasks for the 4-bit weight Llama-3-70B model on an A100 GPU, outperforming EAGLE-2 by 1.31$\\times$. Code available at https://github.com/AI9Stars/SpecMQuant.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº† Speculative decoding ä¸ Quantization æŠ€æœ¯çš„ç»“åˆï¼Œæ—¨åœ¨è¿›ä¸€æ­¥åŠ é€Ÿå¤§è¯­è¨€æ¨¡å‹çš„æ¨ç†è¿‡ç¨‹ã€‚å®éªŒå‘ç°ï¼Œå°†å…ˆè¿›çš„ Speculative decoding æ–¹æ³• EAGLE-2 åº”ç”¨äº 4-bit é‡åŒ–æ¨¡å‹æ—¶ï¼Œæ¨æµ‹è§£ç å¢åŠ çš„è®¡ç®—è´Ÿè½½â€”â€”ç‰¹åˆ«æ˜¯éªŒè¯ tree-style drafts çš„æ—¶é—´å¼€é”€â€”â€”æ˜¾è‘—æŠµæ¶ˆäº†é‡åŒ–å¸¦æ¥çš„æ˜¾å­˜æ”¶ç›Šã€‚é’ˆå¯¹è¿™ä¸€å…¼å®¹æ€§æŒ‘æˆ˜ï¼Œä½œè€…è®¾è®¡äº†ä¸€ç§å…¨æ–°çš„ hierarchical frameworkï¼Œé€šè¿‡å¼•å…¥ä¸€ä¸ªå°æ¨¡å‹ä½œä¸ºä¸­é—´é˜¶æ®µå°† tree-style drafts è½¬åŒ–ä¸º sequence draftsï¼Œä»è€Œå……åˆ†åˆ©ç”¨ç›®æ ‡é‡åŒ–æ¨¡å‹çš„è®¿å­˜ä¼˜åŠ¿ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯¥å±‚æ¬¡åŒ–æ–¹æ¡ˆåœ¨ A100 GPU ä¸Šä¸º 4-bit æƒé‡é‡åŒ–çš„ Llama-3-70B æ¨¡å‹å¸¦æ¥äº† 2.78 å€çš„åŠ é€Ÿæ•ˆæœï¼Œæ€§èƒ½ä¼˜äº EAGLE-2 è¾¾ 1.31 å€ã€‚è¯¥ç ”ç©¶æ­ç¤ºäº†æ¨æµ‹è§£ç ä¸é‡åŒ–æŠ€æœ¯åœ¨å®é™…åº”ç”¨ä¸­çš„æ€§èƒ½ç“¶é¢ˆï¼Œå¹¶ä¸ºå®ç°æ›´é«˜æ•ˆçš„æ¨¡å‹æ¨ç†æä¾›äº†åˆ›æ–°çš„ç³»ç»Ÿè®¾è®¡æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "12 pages, 5 figures",
      "pdf_url": "https://arxiv.org/pdf/2505.22179v2",
      "published_date": "2025-05-28 09:55:08 UTC",
      "updated_date": "2025-05-29 04:07:33 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T15:29:50.750239+00:00"
    },
    {
      "arxiv_id": "2505.22174v1",
      "title": "Online Fair Division for Personalized $2$-Value Instances",
      "title_zh": "é’ˆå¯¹ä¸ªæ€§åŒ– $2$-å€¼å®ä¾‹çš„åœ¨çº¿å…¬å¹³åˆ†é…",
      "authors": [
        "Georgios Amanatidis",
        "Alexandros Lolos",
        "Evangelos Markakis",
        "Victor Turmel"
      ],
      "abstract": "We study an online fair division setting, where goods arrive one at a time and there is a fixed set of $n$ agents, each of whom has an additive valuation function over the goods. Once a good appears, the value each agent has for it is revealed and it must be allocated immediately and irrevocably to one of the agents. It is known that without any assumptions about the values being severely restricted or coming from a distribution, very strong impossibility results hold in this setting. To bypass the latter, we turn our attention to instances where the valuation functions are restricted. In particular, we study personalized $2$-value instances, where there are only two possible values each agent may have for each good, possibly different across agents, and we show how to obtain worst case guarantees with respect to well-known fairness notions, such as maximin share fairness and envy-freeness up to one (or two) good(s). We suggest a deterministic algorithm that maintains a $1/(2n-1)$-MMS allocation at every time step and show that this is the best possible any deterministic algorithm can achieve if one cares about every single time step; nevertheless, eventually the allocation constructed by our algorithm becomes a $1/4$-MMS allocation. To achieve this, the algorithm implicitly maintains a fragile system of priority levels for all agents. Further, we show that, by allowing some limited access to future information, it is possible to have stronger results with less involved approaches. By knowing the values of goods for $n-1$ time steps into the future, we design a matching-based algorithm that achieves an EF$1$ allocation every $n$ time steps, while always maintaining an EF$2$ allocation. Finally, we show that our results allow us to get the first nontrivial guarantees for additive instances in which the ratio of the maximum over the minimum value an agent has for a good is bounded.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†åœ¨çº¿å…¬å¹³åˆ†é…(Online Fair Division)é—®é¢˜ï¼Œé’ˆå¯¹ç‰©å“é€ä¸€åˆ°è¾¾ä¸”å¿…é¡»ç«‹å³ä¸å¯é€†åœ°åˆ†é…ç»™$n$ä¸ªå…·æœ‰åŠ æ€§ä¼°å€¼å‡½æ•°(Additive Valuation Function)çš„ä»£ç†äººçš„åœºæ™¯å±•å¼€åˆ†æã€‚ä¸ºè§£å†³ä¸€èˆ¬è®¾ç½®ä¸‹å­˜åœ¨çš„ä¸å¯è¡Œæ€§ç»“è®ºï¼Œæœ¬æ–‡ä¸“æ³¨äºç ”ç©¶ä¸ªæ€§åŒ–$2$-å€¼å®ä¾‹(Personalized $2$-Value Instances)ï¼Œå³æ¯ä¸ªä»£ç†äººå¯¹ç‰©å“ä»…æœ‰ä¸¤ç§å¯èƒ½çš„å–å€¼ã€‚ç ”ç©¶æå‡ºäº†ä¸€ç§ç¡®å®šæ€§ç®—æ³•ï¼Œé€šè¿‡ç»´æŠ¤ä¸€å¥—å¤æ‚çš„ä¼˜å…ˆçº§ç³»ç»Ÿï¼Œåœ¨æ¯ä¸€æ­¥ç¡®ä¿$1/(2n-1)$-MMSåˆ†é…ï¼Œå¹¶è¯æ˜è¿™æ˜¯ç¡®å®šæ€§ç®—æ³•åœ¨è€ƒè™‘æ¯ä¸€æ­¥å…¬å¹³æ€§æ—¶æ‰€èƒ½è¾¾åˆ°çš„æœ€ä¼˜ä¿è¯ï¼Œä¸”æœ€ç»ˆåˆ†é…èƒ½è¾¾åˆ°$1/4$-MMSã€‚æ­¤å¤–ï¼Œåœ¨å…è®¸é¢„çŸ¥æœªæ¥$n-1$æ­¥ä¿¡æ¯çš„å‰æä¸‹ï¼Œç ”ç©¶è®¾è®¡äº†ä¸€ç§åŸºäºåŒ¹é…çš„ç®—æ³•ï¼Œå®ç°äº†æŒç»­çš„EF$2$åˆ†é…ä»¥åŠæ¯$n$æ­¥ä¸€æ¬¡çš„EF$1$åˆ†é…ã€‚æœ€åï¼Œè¯¥æˆæœè¿˜ä¸ºä»£ç†äººå¯¹ç‰©å“æœ€å¤§ä¼°å€¼ä¸æœ€å°ä¼°å€¼ä¹‹æ¯”å—é™çš„åŠ æ€§å®ä¾‹æä¾›äº†é¦–ä¸ªéå¹³å‡¡çš„å…¬å¹³æ€§ä¿è¯ã€‚",
      "categories": [
        "cs.GT",
        "cs.AI",
        "cs.MA"
      ],
      "primary_category": "cs.GT",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.22174v1",
      "published_date": "2025-05-28 09:48:16 UTC",
      "updated_date": "2025-05-28 09:48:16 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T15:29:47.837267+00:00"
    },
    {
      "arxiv_id": "2505.22165v1",
      "title": "Unifying Continuous and Discrete Text Diffusion with Non-simultaneous Diffusion Processes",
      "title_zh": "é€šè¿‡éåŒæ­¥æ‰©æ•£è¿‡ç¨‹ç»Ÿä¸€è¿ç»­ä¸ç¦»æ•£æ–‡æœ¬æ‰©æ•£",
      "authors": [
        "Bocheng Li",
        "Zhujin Gao",
        "Linli Xu"
      ],
      "abstract": "Diffusion models have emerged as a promising approach for text generation, with recent works falling into two main categories: discrete and continuous diffusion models. Discrete diffusion models apply token corruption independently using categorical distributions, allowing for different diffusion progress across tokens but lacking fine-grained control. Continuous diffusion models map tokens to continuous spaces and apply fine-grained noise, but the diffusion progress is uniform across tokens, limiting their ability to capture semantic nuances. To address these limitations, we propose \\textbf{\\underline{N}}on-simultan\\textbf{\\underline{e}}ous C\\textbf{\\underline{o}}ntinuous \\textbf{\\underline{Diff}}usion Models (NeoDiff), a novel diffusion model that integrates the strengths of both discrete and continuous approaches. NeoDiff introduces a Poisson diffusion process for the forward process, enabling a flexible and fine-grained noising paradigm, and employs a time predictor for the reverse process to adaptively modulate the denoising progress based on token semantics. Furthermore, NeoDiff utilizes an optimized schedule for inference to ensure more precise noise control and improved performance. Our approach unifies the theories of discrete and continuous diffusion models, offering a more principled and effective framework for text generation. Experimental results on several text generation tasks demonstrate NeoDiff's superior performance compared to baselines of non-autoregressive continuous and discrete diffusion models, iterative-based methods and autoregressive diffusion-based methods. These results highlight NeoDiff's potential as a powerful tool for generating high-quality text and advancing the field of diffusion-based text generation.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ç¦»æ•£æ‰©æ•£æ¨¡å‹(Discrete diffusion models)ç¼ºä¹ç»†ç²’åº¦æ§åˆ¶ä»¥åŠè¿ç»­æ‰©æ•£æ¨¡å‹(Continuous diffusion models)æ‰©æ•£è¿›åº¦ç»Ÿä¸€çš„å±€é™æ€§ï¼Œæå‡ºäº†NeoDiff (Non-simultaneous Continuous Diffusion Models)ã€‚è¯¥æ¨¡å‹åœ¨å‰å‘è¿‡ç¨‹ä¸­å¼•å…¥äº†æ³Šæ¾æ‰©æ•£è¿‡ç¨‹(Poisson diffusion process)ï¼Œå®ç°äº†ä¸€ç§çµæ´»ä¸”ç»†ç²’åº¦çš„åŠ å™ªæ¨¡å¼ã€‚åœ¨åå‘è¿‡ç¨‹ä¸­ï¼ŒNeoDiffé‡‡ç”¨æ—¶é—´é¢„æµ‹å™¨(time predictor)æ ¹æ®æ ‡è®°è¯­ä¹‰è‡ªé€‚åº”åœ°è°ƒèŠ‚å»å™ªè¿›åº¦ã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•é€šè¿‡ä¼˜åŒ–çš„æ¨ç†è°ƒåº¦(optimized schedule for inference)ç¡®ä¿äº†æ›´ç²¾ç¡®çš„å™ªå£°æ§åˆ¶ï¼Œä»è€Œåœ¨ç†è®ºä¸Šç»Ÿä¸€äº†ç¦»æ•£å’Œè¿ç»­æ‰©æ•£æ¨¡å‹ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒNeoDiffåœ¨å¤šé¡¹æ–‡æœ¬ç”Ÿæˆä»»åŠ¡ä¸­çš„è¡¨ç°ä¼˜äºç°æœ‰çš„éè‡ªå›å½’è¿ç»­ä¸ç¦»æ•£æ‰©æ•£æ¨¡å‹ã€è¿­ä»£å¼æ–¹æ³•ä»¥åŠè‡ªå›å½’æ‰©æ•£æ¨¡å‹åŸºçº¿ã€‚è¿™ä¸€æˆæœçªæ˜¾äº†NeoDiffåœ¨ç”Ÿæˆé«˜è´¨é‡æ–‡æœ¬ä»¥åŠæ¨åŠ¨æ‰©æ•£æ¨¡å‹æ–‡æœ¬ç”Ÿæˆé¢†åŸŸå‘å±•æ–¹é¢çš„æ˜¾è‘—æ½œåŠ›ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "Accepted to ACL 2025 Main Conference",
      "pdf_url": "https://arxiv.org/pdf/2505.22165v1",
      "published_date": "2025-05-28 09:28:52 UTC",
      "updated_date": "2025-05-28 09:28:52 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T15:30:13.002606+00:00"
    },
    {
      "arxiv_id": "2506.05368v1",
      "title": "Speaking images. A novel framework for the automated self-description of artworks",
      "title_zh": "ä¼šè¯´è¯çš„å›¾åƒï¼šä¸€ç§è‰ºæœ¯å“è‡ªåŠ¨åŒ–è‡ªè¿°çš„æ–°å‹æ¡†æ¶",
      "authors": [
        "Valentine Bernasconi",
        "Gustavo Marfia"
      ],
      "abstract": "Recent breakthroughs in generative AI have opened the door to new research perspectives in the domain of art and cultural heritage, where a large number of artifacts have been digitized. There is a need for innovation to ease the access and highlight the content of digital collections. Such innovations develop into creative explorations of the digital image in relation to its malleability and contemporary interpretation, in confrontation to the original historical object. Based on the concept of the autonomous image, we propose a new framework towards the production of self-explaining cultural artifacts using open-source large-language, face detection, text-to-speech and audio-to-animation models. The goal is to start from a digitized artwork and to automatically assemble a short video of the latter where the main character animates to explain its content. The whole process questions cultural biases encapsulated in large-language models, the potential of digital images and deepfakes of artworks for educational purposes, along with concerns of the field of art history regarding such creative diversions.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ä¸ªç”¨äºè‰ºæœ¯å“è‡ªåŠ¨åŒ–è‡ªæˆ‘æè¿°çš„æ–°å‹æ¡†æ¶ï¼Œæ—¨åœ¨åˆ©ç”¨ç”Ÿæˆå¼äººå·¥æ™ºèƒ½(Generative AI)çš„çªç ´ï¼Œæå‡æ•°å­—æ–‡åŒ–é—äº§çš„å¯è®¿é—®æ€§å¹¶çªå‡ºå…¶å†…å®¹ã€‚è¯¥æ¡†æ¶åŸºäºâ€œè‡ªä¸»å›¾åƒâ€(Autonomous Image)çš„æ¦‚å¿µï¼Œé›†æˆäº†å¼€æºå¤§è¯­è¨€æ¨¡å‹(Large-Language Models)ã€äººè„¸æ£€æµ‹(Face Detection)ã€æ–‡æœ¬è½¬è¯­éŸ³(Text-to-Speech)ä»¥åŠéŸ³é¢‘è½¬åŠ¨ç”»(Audio-to-Animation)æ¨¡å‹ã€‚é€šè¿‡è¯¥æµç¨‹ï¼Œç³»ç»Ÿèƒ½å¤Ÿä»æ•°å­—åŒ–è‰ºæœ¯å“ä¸­è‡ªåŠ¨ç”ŸæˆçŸ­è§†é¢‘ï¼Œä½¿ä½œå“ä¸­çš„ä¸»è¦äººç‰©é€šè¿‡åŠ¨ç”»æ•ˆæœäº²è‡ªè§£é‡Šå…¶å†…å®¹ã€‚è¯¥ç ”ç©¶ä¸ä»…æ¢è®¨äº†è‰ºæœ¯å“æ·±åº¦ä¼ªé€ (Deepfakes)åœ¨æ•™è‚²é¢†åŸŸçš„åº”ç”¨æ½œåŠ›ï¼Œè¿˜åˆ†æäº†å¤§è¯­è¨€æ¨¡å‹ä¸­å­˜åœ¨çš„æ–‡åŒ–åè§ä»¥åŠè‰ºæœ¯å²é¢†åŸŸå¯¹è¿™ç±»åˆ›æ„æ€§æ¼”ç»çš„çœ‹æ³•ã€‚è¿™ä¸€æˆæœä¸ºæ•°å­—åŒ–é¦†è—çš„åˆ›æ„æ¢ç´¢å’Œå½“ä»£è§£è¯»æä¾›äº†å…¨æ–°çš„æŠ€æœ¯æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.05368v1",
      "published_date": "2025-05-28 09:13:41 UTC",
      "updated_date": "2025-05-28 09:13:41 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T15:30:26.870188+00:00"
    },
    {
      "arxiv_id": "2505.22148v1",
      "title": "What Makes a Good Reasoning Chain? Uncovering Structural Patterns in Long Chain-of-Thought Reasoning",
      "title_zh": "ä½•è°“ä¼˜è´¨æ¨ç†é“¾ï¼Ÿæ­ç¤ºé•¿é“¾å¼æ€ç»´æ¨ç†ä¸­çš„ç»“æ„æ¨¡å¼",
      "authors": [
        "Gangwei Jiang",
        "Yahui Liu",
        "Zhaoyi Li",
        "Qi Wang",
        "Fuzheng Zhang",
        "Linqi Song",
        "Ying Wei",
        "Defu Lian"
      ],
      "abstract": "Recent advances in reasoning with large language models (LLMs) have popularized Long Chain-of-Thought (LCoT), a strategy that encourages deliberate and step-by-step reasoning before producing a final answer. While LCoTs have enabled expert-level performance in complex tasks, how the internal structures of their reasoning chains drive, or even predict, the correctness of final answers remains a critical yet underexplored question. In this work, we present LCoT2Tree, an automated framework that converts sequential LCoTs into hierarchical tree structures and thus enables deeper structural analysis of LLM reasoning. Using graph neural networks (GNNs), we reveal that structural patterns extracted by LCoT2Tree, including exploration, backtracking, and verification, serve as stronger predictors of final performance across a wide range of tasks and models. Leveraging an explainability technique, we further identify critical thought patterns such as over-branching that account for failures. Beyond diagnostic insights, the structural patterns by LCoT2Tree support practical applications, including improving Best-of-N decoding effectiveness. Overall, our results underscore the critical role of internal structures of reasoning chains, positioning LCoT2Tree as a powerful tool for diagnosing, interpreting, and improving reasoning in LLMs.",
      "tldr_zh": "è¿™é¡¹ç ”ç©¶é’ˆå¯¹å¤§è¯­è¨€æ¨¡å‹(LLMs)ä¸­çš„é•¿é“¾å¼æ€ç»´(Long Chain-of-Thought, LCoT)æ¨ç†è¿‡ç¨‹ï¼Œæå‡ºäº†LCoT2Treeæ¡†æ¶ï¼Œæ—¨åœ¨å°†åºåˆ—åŒ–çš„æ¨ç†é“¾è½¬åŒ–ä¸ºå±‚æ¬¡åŒ–æ ‘çŠ¶ç»“æ„ä»¥è¿›è¡Œæ·±åº¦çš„ç»“æ„åŒ–åˆ†æã€‚é€šè¿‡åº”ç”¨å›¾ç¥ç»ç½‘ç»œ(GNNs)ï¼Œç ”ç©¶å‘ç°ä»ä¸­æå–çš„æ¢ç´¢(exploration)ã€å›æº¯(backtracking)å’ŒéªŒè¯(verification)ç­‰ç»“æ„æ¨¡å¼åœ¨é¢„æµ‹ä¸åŒä»»åŠ¡å’Œæ¨¡å‹çš„æœ€ç»ˆè¡¨ç°æ–¹é¢å…·æœ‰æå¼ºçš„é¢„æµ‹èƒ½åŠ›ã€‚åˆ©ç”¨å¯è§£é‡Šæ€§æŠ€æœ¯ï¼Œè¯¥ç ”ç©¶è¿›ä¸€æ­¥è¯†åˆ«å‡ºäº†å¦‚è¿‡åº¦åˆ†å‰(over-branching)ç­‰å¯¼è‡´æ¨ç†å¤±è´¥çš„å…³é”®æ€ç»´æ¨¡å¼ã€‚é™¤äº†æä¾›è¯Šæ–­æ€§è§è§£ï¼ŒLCoT2Treeæå–çš„ç»“æ„æ¨¡å¼è¿˜èƒ½æœ‰æ•ˆæå‡Best-of-Nè§£ç ç­‰å®é™…åº”ç”¨çš„æ•ˆæœã€‚è¯¥ç ”ç©¶å¼ºè°ƒäº†æ¨ç†é“¾å†…éƒ¨ç»“æ„åœ¨é©±åŠ¨æ­£ç¡®ç­”æ¡ˆä¸­çš„æ ¸å¿ƒä½œç”¨ï¼Œä¸ºè¯Šæ–­ã€è§£é‡Šå’Œä¼˜åŒ–å¤§è¯­è¨€æ¨¡å‹çš„æ¨ç†èƒ½åŠ›æä¾›äº†é‡è¦å·¥å…·ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.22148v1",
      "published_date": "2025-05-28 09:12:31 UTC",
      "updated_date": "2025-05-28 09:12:31 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T15:30:20.746394+00:00"
    },
    {
      "arxiv_id": "2505.22147v1",
      "title": "Lifted Forward Planning in Relational Factored Markov Decision Processes with Concurrent Actions",
      "title_zh": "å…·æœ‰å¹¶å‘åŠ¨ä½œçš„å…³ç³»åˆ†è§£é©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹ä¸­çš„æå‡å¼å‰å‘è§„åˆ’",
      "authors": [
        "Florian Andreas Marwitz",
        "Tanya Braun",
        "Ralf MÃ¶ller",
        "Marcel Gehrke"
      ],
      "abstract": "Decision making is a central problem in AI that can be formalized using a Markov Decision Process. A problem is that, with increasing numbers of (indistinguishable) objects, the state space grows exponentially. To compute policies, the state space has to be enumerated. Even more possibilities have to be enumerated if the size of the action space depends on the size of the state space, especially if we allow concurrent actions. To tackle the exponential blow-up in the action and state space, we present a first-order representation to store the spaces in polynomial instead of exponential size in the number of objects and introduce Foreplan, a relational forward planner, which uses this representation to efficiently compute policies for numerous indistinguishable objects and actions. Additionally, we introduce an even faster approximate version of Foreplan. Moreover, Foreplan identifies how many objects an agent should act on to achieve a certain task given restrictions. Further, we provide a theoretical analysis and an empirical evaluation of Foreplan, demonstrating a speedup of at least four orders of magnitude.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹äººå·¥æ™ºèƒ½å†³ç­–ä¸­ Markov Decision Process (MDP) çŠ¶æ€ä¸åŠ¨ä½œç©ºé—´éšå¯¹è±¡æ•°é‡å‘ˆæŒ‡æ•°çº§è†¨èƒ€çš„éš¾é¢˜ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤„ç†å¹¶å‘æ“ä½œ (concurrent actions) æ—¶çš„è®¡ç®—ç“¶é¢ˆã€‚ä¸ºæ­¤ï¼Œä½œè€…æå‡ºäº†ä¸€ç§ä¸€é˜¶è¡¨ç¤ºæ–¹æ³• (first-order representation)ï¼ŒæˆåŠŸå°†ç©ºé—´å­˜å‚¨è§„æ¨¡ä»æŒ‡æ•°çº§å‹ç¼©è‡³å…³äºå¯¹è±¡æ•°é‡çš„å¤šé¡¹å¼çº§åˆ«ã€‚åŸºäºæ­¤è¡¨ç¤ºï¼Œç ”ç©¶å›¢é˜Ÿå¼€å‘äº†å…³ç³»å‰å‘è§„åˆ’å™¨ Foreplanï¼Œèƒ½å¤Ÿé«˜æ•ˆåœ°ä¸ºåŒ…å«å¤§é‡æ— å·®å¼‚å¯¹è±¡å’ŒåŠ¨ä½œçš„ä»»åŠ¡è®¡ç®—ç­–ç•¥ï¼Œå¹¶åŒæ­¥æ¨å‡ºäº†è®¡ç®—é€Ÿåº¦æ›´å¿«çš„è¿‘ä¼¼ç‰ˆæœ¬ã€‚æ­¤å¤–ï¼ŒForeplan è¿˜èƒ½åœ¨ç‰¹å®šé™åˆ¶æ¡ä»¶ä¸‹ï¼Œè¯†åˆ«å‡ºæ™ºèƒ½ä½“ä¸ºè¾¾æˆä»»åŠ¡åº”æ“ä½œçš„æœ€ä¼˜å¯¹è±¡æ•°é‡ã€‚ç†è®ºåˆ†æä¸å®éªŒç»“æœè¯æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å¤„ç† Relational Factored Markov Decision Processes æ—¶è¡¨ç°å“è¶Šï¼Œç›¸æ¯”ä¼ ç»Ÿæ–¹æ³•å®ç°äº†è‡³å°‘å››ä¸ªæ•°é‡çº§çš„æ€§èƒ½åŠ é€Ÿã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.22147v1",
      "published_date": "2025-05-28 09:08:27 UTC",
      "updated_date": "2025-05-28 09:08:27 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T15:30:28.476541+00:00"
    },
    {
      "arxiv_id": "2505.22146v4",
      "title": "Flexible Tool Selection through Low-dimensional Attribute Alignment of Vision and Language",
      "title_zh": "åŸºäºè§†è§‰ä¸è¯­è¨€ä½ç»´å±æ€§å¯¹é½çš„çµæ´»å·¥å…·é€‰æ‹©",
      "authors": [
        "Guangfu Hao",
        "Haojie Wen",
        "Liangxuan Guo",
        "Yang Chen",
        "Yanchao Bi",
        "Shan Yu"
      ],
      "abstract": "Flexible tool selection reflects a complex cognitive ability that distinguishes humans from other species, yet computational models that capture this ability remain underdeveloped. We developed a framework using low-dimensional attribute representations to bridge visual tool perception and linguistic task understanding. We constructed a comprehensive dataset (ToolNet) containing 115 common tools labeled with 13 carefully designed attributes spanning physical, functional, and psychological properties, paired with natural language scenarios describing tool usage. Visual encoders (ResNet or ViT) extract attributes from tool images while fine-tuned language models (GPT-2, LLaMA, DeepSeek) derive required attributes from task descriptions. Our approach achieves 74% accuracy in tool selection tasks-significantly outperforming direct tool matching (20%) and smaller multimodal models (21%-58%), while approaching performance of much larger models like GPT-4o (73%) with substantially fewer parameters. Human evaluation studies validate our framework's alignment with human decision-making patterns, and generalization experiments demonstrate effective performance on novel tool categories. Ablation studies revealed that manipulation-related attributes (graspability, elongation, hand-relatedness) consistently prove most critical across modalities. This work provides a parameter-efficient, interpretable solution that mimics human-like tool cognition, advancing both cognitive science understanding and practical applications in tool selection tasks.",
      "tldr_zh": "è¯¥ç ”ç©¶å¼€å‘äº†ä¸€ä¸ªåˆ©ç”¨ä½ç»´å±æ€§è¡¨å¾(low-dimensional attribute representations)è¿æ¥è§†è§‰å·¥å…·æ„ŸçŸ¥ä¸è¯­è¨€ä»»åŠ¡ç†è§£çš„æ¡†æ¶ï¼Œæ—¨åœ¨æ¨¡æ‹Ÿäººç±»çµæ´»é€‰æ‹©å·¥å…·çš„è®¤çŸ¥èƒ½åŠ›ã€‚ç ”ç©¶å›¢é˜Ÿæ„å»ºäº†åŒ…å«115ç§å·¥å…·å’Œ13ç±»å±æ€§çš„ToolNetæ•°æ®é›†ï¼Œå¹¶ç»“åˆè§†è§‰ç¼–ç å™¨ï¼ˆå¦‚ResNetã€ViTï¼‰ä¸å¾®è°ƒåçš„è¯­è¨€æ¨¡å‹ï¼ˆå¦‚LLaMAã€DeepSeekï¼‰æ¥å¯¹é½å·¥å…·ç‰¹å¾ä¸ä»»åŠ¡éœ€æ±‚ã€‚å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å·¥å…·é€‰æ‹©ä»»åŠ¡ä¸­å®ç°äº†74%çš„å‡†ç¡®ç‡ï¼Œæ˜¾è‘—ä¼˜äºä¼ ç»Ÿçš„ç›´æ¥åŒ¹é…æ–¹æ³•ï¼Œå¹¶ä»¥æå°‘çš„å‚æ•°é‡è¾¾åˆ°äº†æ¥è¿‘GPT-4oçš„æ€§èƒ½æ°´å¹³ã€‚äººå·¥è¯„ä¼°éªŒè¯äº†è¯¥æ¡†æ¶ä¸äººç±»å†³ç­–æ¨¡å¼çš„å¥‘åˆåº¦ï¼Œæ¶ˆèå®éªŒè¿›ä¸€æ­¥è¯æ˜äº†æŠ“æ¡æ€§(graspability)å’Œå»¶ä¼¸æ€§(elongation)ç­‰æ“ä½œç›¸å…³å±æ€§åœ¨è·¨æ¨¡æ€ç†è§£ä¸­çš„å…³é”®ä½œç”¨ã€‚è¿™é¡¹å·¥ä½œä¸ºå·¥å…·é€‰æ‹©ä»»åŠ¡æä¾›äº†ä¸€ç§é«˜æ•ˆä¸”å…·å¤‡å¯è§£é‡Šæ€§çš„è®¡ç®—æ–¹æ¡ˆï¼Œæœ‰æ•ˆæ¨åŠ¨äº†è®¤çŸ¥ç§‘å­¦ä¸äººå·¥æ™ºèƒ½çš„ç»“åˆã€‚",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CL",
        "q-bio.NC"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.22146v4",
      "published_date": "2025-05-28 09:06:04 UTC",
      "updated_date": "2025-08-21 04:52:25 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T15:30:28.610724+00:00"
    },
    {
      "arxiv_id": "2505.22141v2",
      "title": "FaceEditTalker: Controllable Talking Head Generation with Facial Attribute Editing",
      "title_zh": "FaceEditTalkerï¼šæ”¯æŒé¢éƒ¨å±æ€§ç¼–è¾‘çš„å¯æ§è¯´è¯äººå¤´åƒç”Ÿæˆ",
      "authors": [
        "Guanwen Feng",
        "Zhiyuan Ma",
        "Yunan Li",
        "Jiahao Yang",
        "Junwei Jing",
        "Qiguang Miao"
      ],
      "abstract": "Recent advances in audio-driven talking head generation have achieved impressive results in lip synchronization and emotional expression. However, they largely overlook the crucial task of facial attribute editing. This capability is indispensable for achieving deep personalization and expanding the range of practical applications, including user-tailored digital avatars, engaging online education content, and brand-specific digital customer service. In these key domains, flexible adjustment of visual attributes, such as hairstyle, accessories, and subtle facial features, is essential for aligning with user preferences, reflecting diverse brand identities and adapting to varying contextual demands. In this paper, we present FaceEditTalker, a unified framework that enables controllable facial attribute manipulation while generating high-quality, audio-synchronized talking head videos. Our method consists of two key components: an image feature space editing module, which extracts semantic and detail features and allows flexible control over attributes like expression, hairstyle, and accessories; and an audio-driven video generation module, which fuses these edited features with audio-guided facial landmarks to drive a diffusion-based generator. This design ensures temporal coherence, visual fidelity, and identity preservation across frames. Extensive experiments on public datasets demonstrate that our method achieves comparable or superior performance to representative baseline methods in lip-sync accuracy, video quality, and attribute controllability. Project page: https://peterfanfan.github.io/FaceEditTalker/",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†FaceEditTalkerï¼Œè¿™æ˜¯ä¸€ä¸ªèƒ½å¤Ÿå®ç°å¯æ§é¢éƒ¨å±æ€§ç¼–è¾‘(facial attribute editing)ä¸”ç”Ÿæˆé«˜è´¨é‡è¯­éŸ³åŒæ­¥äººç‰©å¤´åƒè§†é¢‘çš„ç»Ÿä¸€æ¡†æ¶ã€‚è¯¥æ–¹æ³•ç”±å›¾åƒç‰¹å¾ç©ºé—´ç¼–è¾‘æ¨¡å—å’ŒéŸ³é¢‘é©±åŠ¨è§†é¢‘ç”Ÿæˆæ¨¡å—ç»„æˆï¼Œå‰è€…å…è®¸å¯¹å‘å‹(hairstyle)ã€é…é¥°(accessories)å’Œç»†å¾®é¢éƒ¨ç‰¹å¾è¿›è¡Œçµæ´»çš„è¯­ä¹‰(semantic)ä¸ç»†èŠ‚æ§åˆ¶ï¼Œåè€…åˆ™å°†ç¼–è¾‘åçš„ç‰¹å¾ä¸éŸ³é¢‘å¼•å¯¼çš„é¢éƒ¨åœ°æ ‡(facial landmarks)èåˆï¼Œå¹¶åˆ©ç”¨æ‰©æ•£æ¨¡å‹(diffusion-based)ç”Ÿæˆå™¨è¿›è¡Œé©±åŠ¨ã€‚è¯¥è®¾è®¡åœ¨ç¡®ä¿è§†é¢‘æ—¶é—´è¿è´¯æ€§(temporal coherence)å’Œè§†è§‰é€¼çœŸåº¦çš„åŒæ—¶ï¼Œæœ‰æ•ˆå®ç°äº†èº«ä»½ä¿ç•™(identity preservation)ã€‚åœ¨å…¬å…±æ•°æ®é›†ä¸Šçš„å¤§é‡å®éªŒè¯æ˜ï¼ŒFaceEditTalkeråœ¨å”‡å½¢åŒæ­¥(lip-sync)ç²¾åº¦ã€è§†é¢‘è´¨é‡å’Œå±æ€§å¯æ§æ€§æ–¹é¢å‡è¾¾åˆ°æˆ–ä¼˜äºç°æœ‰çš„ä»£è¡¨æ€§åŸºçº¿æ–¹æ³•ã€‚è¯¥æŠ€æœ¯ä¸ºå®ç°ä¸ªæ€§åŒ–æ•°å­—åˆ†èº«(digital avatars)å’Œå¤šæ ·åŒ–çš„åœ¨çº¿æ•™è‚²å†…å®¹æä¾›äº†æ›´å¼ºçš„äº¤äº’æ€§ä¸é€‚åº”æ€§ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.22141v2",
      "published_date": "2025-05-28 09:04:00 UTC",
      "updated_date": "2025-08-27 16:33:34 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T15:30:45.756621+00:00"
    },
    {
      "arxiv_id": "2505.22137v1",
      "title": "Limited Generalizability in Argument Mining: State-Of-The-Art Models Learn Datasets, Not Arguments",
      "title_zh": "è®ºè¾©æŒ–æ˜ä¸­çš„æ³›åŒ–å±€é™æ€§ï¼šæœ€å…ˆè¿›æ¨¡å‹å­¦ä¹ çš„æ˜¯æ•°æ®é›†è€Œéè®ºè¾©æœ¬èº«",
      "authors": [
        "Marc Feger",
        "Katarina Boland",
        "Stefan Dietze"
      ],
      "abstract": "Identifying arguments is a necessary prerequisite for various tasks in automated discourse analysis, particularly within contexts such as political debates, online discussions, and scientific reasoning. In addition to theoretical advances in understanding the constitution of arguments, a significant body of research has emerged around practical argument mining, supported by a growing number of publicly available datasets. On these benchmarks, BERT-like transformers have consistently performed best, reinforcing the belief that such models are broadly applicable across diverse contexts of debate. This study offers the first large-scale re-evaluation of such state-of-the-art models, with a specific focus on their ability to generalize in identifying arguments. We evaluate four transformers, three standard and one enhanced with contrastive pre-training for better generalization, on 17 English sentence-level datasets as most relevant to the task. Our findings show that, to varying degrees, these models tend to rely on lexical shortcuts tied to content words, suggesting that apparent progress may often be driven by dataset-specific cues rather than true task alignment. While the models achieve strong results on familiar benchmarks, their performance drops markedly when applied to unseen datasets. Nonetheless, incorporating both task-specific pre-training and joint benchmark training proves effective in enhancing both robustness and generalization.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹Argument Miningä¸­çš„æ³›åŒ–èƒ½åŠ›å—é™é—®é¢˜è¿›è¡Œäº†å¤§è§„æ¨¡é‡æ–°è¯„ä¼°ï¼ŒæŒ‡å‡ºå½“å‰çš„State-Of-The-Artæ¨¡å‹å¾€å¾€å€¾å‘äºå­¦ä¹ ç‰¹å®šæ•°æ®é›†çš„ç‰¹å¾è€ŒéçœŸæ­£çš„è®ºç‚¹é€»è¾‘ã€‚ç ”ç©¶äººå‘˜åœ¨17ä¸ªè‹±æ–‡å¥å­çº§æ•°æ®é›†ä¸Šæµ‹è¯•äº†åŒ…æ‹¬æ ‡å‡†TransformeråŠç»è¿‡Contrastive pre-trainingå¢å¼ºåœ¨å†…çš„å››ç§æ¨¡å‹ã€‚å®éªŒå‘ç°ï¼Œè¿™äº›æ¨¡å‹åœ¨å¾ˆå¤§ç¨‹åº¦ä¸Šä¾èµ–äºä¸å†…å®¹è¯ç›¸å…³çš„Lexical shortcutsï¼Œå¯¼è‡´å…¶åœ¨ç†Ÿæ‚‰çš„åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œä½†åœ¨é¢å¯¹Unseen datasetsæ—¶æ€§èƒ½æ˜¾è‘—ä¸‹é™ã€‚è¿™è¡¨æ˜ç›®å‰çš„è¿›å±•å¾ˆå¤§ç¨‹åº¦ä¸Šæ˜¯ç”±Dataset-specific cuesé©±åŠ¨çš„ï¼Œè€ŒéçœŸæ­£çš„Task alignmentã€‚æœ€åï¼Œç ”ç©¶è¯æ˜é€šè¿‡ç»“åˆTask-specific pre-trainingå’ŒJoint benchmark trainingï¼Œå¯ä»¥æœ‰æ•ˆå¢å¼ºæ¨¡å‹åœ¨è¯†åˆ«è®ºç‚¹æ—¶çš„Robustnesså’Œæ³›åŒ–èƒ½åŠ›ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "This paper has been accepted to ACL 2025 and will be published after 27.07.2025",
      "pdf_url": "https://arxiv.org/pdf/2505.22137v1",
      "published_date": "2025-05-28 09:00:56 UTC",
      "updated_date": "2025-05-28 09:00:56 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T15:31:14.098394+00:00"
    },
    {
      "arxiv_id": "2505.22128v2",
      "title": "Real-Time Blind Defocus Deblurring for Earth Observation: The IMAGIN-e Mission Approach",
      "title_zh": "é¢å‘å¯¹åœ°è§‚æµ‹çš„å®æ—¶ç›²ç¦»ç„¦å»æ¨¡ç³Šï¼šIMAGIN-e ä»»åŠ¡æ–¹æ³•",
      "authors": [
        "Alejandro D. Mousist"
      ],
      "abstract": "This work addresses mechanical defocus in Earth observation images from the IMAGIN-e mission aboard the ISS, proposing a blind deblurring approach adapted to space-based edge computing constraints. Leveraging Sentinel-2 data, our method estimates the defocus kernel and trains a restoration model within a GAN framework, effectively operating without reference images.\n  On Sentinel-2 images with synthetic degradation, SSIM improved by 72.47% and PSNR by 25.00%, confirming the model's ability to recover lost details when the original clean image is known. On IMAGIN-e, where no reference images exist, perceptual quality metrics indicate a substantial enhancement, with NIQE improving by 60.66% and BRISQUE by 48.38%, validating real-world onboard restoration. The approach is currently deployed aboard the IMAGIN-e mission, demonstrating its practical application in an operational space environment.\n  By efficiently handling high-resolution images under edge computing constraints, the method enables applications such as water body segmentation and contour detection while maintaining processing viability despite resource limitations.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å›½é™…ç©ºé—´ç«™ (ISS) ä¸Š IMAGIN-e ä»»åŠ¡ä¸­åœ°çƒè§‚æµ‹å›¾åƒçš„æœºæ¢°å¤±ç„¦é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§é€‚ç”¨äºç©ºé—´è¾¹ç¼˜è®¡ç®— (edge computing) çº¦æŸçš„ç›²å»æ¨¡ç³Š (blind deblurring) æ–¹æ³•ã€‚è¯¥æ–¹æ³•åˆ©ç”¨ Sentinel-2 æ•°æ®ä¼°è®¡å¤±ç„¦æ ¸ï¼Œå¹¶åœ¨ç”Ÿæˆå¯¹æŠ—ç½‘ç»œ (GAN) æ¡†æ¶ä¸‹è®­ç»ƒä¿®å¤æ¨¡å‹ï¼Œå®ç°äº†åœ¨æ— å‚è€ƒå›¾åƒç¯å¢ƒä¸‹çš„æœ‰æ•ˆå¤åŸã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œåœ¨åˆæˆé€€åŒ–çš„ Sentinel-2 å›¾åƒä¸Šï¼Œè¯¥æ¨¡å‹ä½¿ SSIM æå‡äº† 72.47%ï¼ŒPSNR æå‡äº† 25.00%ã€‚åœ¨çœŸå®çš„ IMAGIN-e ä»»åŠ¡åº”ç”¨ä¸­ï¼Œæ„ŸçŸ¥è´¨é‡æŒ‡æ ‡ NIQE å’Œ BRISQUE åˆ†åˆ«æ˜¾è‘—æ”¹å–„äº† 60.66% å’Œ 48.38%ï¼ŒéªŒè¯äº†æ˜Ÿä¸Šå®æ—¶å¤„ç†çš„å¯è¡Œæ€§ã€‚è¯¥æ–¹æ¡ˆç›®å‰å·²æˆåŠŸéƒ¨ç½²ï¼Œè¯æ˜äº†å…¶åœ¨èµ„æºå—é™ç¯å¢ƒä¸‹å¤„ç†é«˜åˆ†è¾¨ç‡å›¾åƒçš„å®ç”¨ä»·å€¼ï¼Œå¹¶ä¸ºæ°´ä½“åˆ†å‰² (water body segmentation) å’Œè½®å»“æ£€æµ‹ (contour detection) ç­‰åç»­ä»»åŠ¡å¥ å®šäº†åŸºç¡€ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "Accepted for presentation at the European Space Agency's Big Data from Space (BiDS) 2025 Conference (https://www.bigdatafromspace2025.org/)",
      "pdf_url": "https://arxiv.org/pdf/2505.22128v2",
      "published_date": "2025-05-28 08:52:38 UTC",
      "updated_date": "2025-07-02 16:31:32 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T15:31:27.020611+00:00"
    },
    {
      "arxiv_id": "2505.22126v1",
      "title": "SridBench: Benchmark of Scientific Research Illustration Drawing of Image Generation Model",
      "title_zh": "SridBenchï¼šå›¾åƒç”Ÿæˆæ¨¡å‹ç§‘ç ”ç»˜å›¾è¯„ä¼°åŸºå‡†",
      "authors": [
        "Yifan Chang",
        "Yukang Feng",
        "Jianwen Sun",
        "Jiaxin Ai",
        "Chuanhao Li",
        "S. Kevin Zhou",
        "Kaipeng Zhang"
      ],
      "abstract": "Recent years have seen rapid advances in AI-driven image generation. Early diffusion models emphasized perceptual quality, while newer multimodal models like GPT-4o-image integrate high-level reasoning, improving semantic understanding and structural composition. Scientific illustration generation exemplifies this evolution: unlike general image synthesis, it demands accurate interpretation of technical content and transformation of abstract ideas into clear, standardized visuals. This task is significantly more knowledge-intensive and laborious, often requiring hours of manual work and specialized tools. Automating it in a controllable, intelligent manner would provide substantial practical value. Yet, no benchmark currently exists to evaluate AI on this front. To fill this gap, we introduce SridBench, the first benchmark for scientific figure generation. It comprises 1,120 instances curated from leading scientific papers across 13 natural and computer science disciplines, collected via human experts and MLLMs. Each sample is evaluated along six dimensions, including semantic fidelity and structural accuracy. Experimental results reveal that even top-tier models like GPT-4o-image lag behind human performance, with common issues in text/visual clarity and scientific correctness. These findings highlight the need for more advanced reasoning-driven visual generation capabilities.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹AIå›¾åƒç”Ÿæˆåœ¨ç§‘å­¦æ’å›¾é¢†åŸŸé¢ä¸´çš„æŒ‘æˆ˜ï¼Œæå‡ºäº†é¦–ä¸ªç§‘å­¦ç»˜å›¾åŸºå‡†æµ‹è¯•é›†SridBenchï¼Œæ—¨åœ¨è§£å†³ç§‘å­¦æ’å›¾ç”Ÿæˆï¼ˆScientific illustration generationï¼‰ä¸­ç”±äºçŸ¥è¯†å¯†é›†åº¦å’ŒæŠ€æœ¯å‡†ç¡®æ€§è¦æ±‚é«˜è€Œå¯¼è‡´çš„è‡ªåŠ¨åŒ–éš¾é¢˜ã€‚SridBenchåŒ…å«ä»13ä¸ªè‡ªç„¶ç§‘å­¦åŠè®¡ç®—æœºç§‘å­¦å­¦ç§‘é¡¶çº§è®ºæ–‡ä¸­ç²¾é€‰å‡ºçš„1,120ä¸ªå®ä¾‹ï¼Œæ•°æ®ç”±äººç±»ä¸“å®¶ä¸å¤šæ¨¡æ€å¤§æ¨¡å‹ï¼ˆMLLMsï¼‰å…±åŒåä½œæ”¶é›†ã€‚æ¯ä¸ªæ ·æœ¬å‡ä»è¯­ä¹‰å¿ å®åº¦ï¼ˆSemantic fidelityï¼‰å’Œç»“æ„å‡†ç¡®æ€§ï¼ˆStructural accuracyï¼‰ç­‰å…­ä¸ªç»´åº¦è¿›è¡Œè¯„ä¼°ï¼Œä»¥å…¨é¢è¡¡é‡æ¨¡å‹çš„ç”Ÿæˆèƒ½åŠ›ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œå³ä¾¿æ˜¯GPT-4o-imageç­‰é¡¶å°–æ¨¡å‹åœ¨ç§‘å­¦å‡†ç¡®æ€§ï¼ˆScientific correctnessï¼‰å’Œè§†è§‰æ¸…æ™°åº¦æ–¹é¢ä»è½åäºäººç±»è¡¨ç°ã€‚è¯¥ç ”ç©¶æ­ç¤ºäº†å½“å‰æ¨¡å‹åœ¨æ¨ç†é©±åŠ¨çš„è§†è§‰ç”Ÿæˆï¼ˆReasoning-driven visual generationï¼‰æ–¹é¢çš„å±€é™æ€§ï¼Œå¹¶ä¸ºæœªæ¥ç§‘å­¦é¢†åŸŸçš„å¯æ§æ™ºèƒ½å›¾åƒç”Ÿæˆæä¾›äº†é‡è¦çš„è¯„ä¼°æ¡†æ¶å’Œæ”¹è¿›æ–¹å‘ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.22126v1",
      "published_date": "2025-05-28 08:51:01 UTC",
      "updated_date": "2025-05-28 08:51:01 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T15:31:50.684760+00:00"
    },
    {
      "arxiv_id": "2505.22125v1",
      "title": "Sentiment Simulation using Generative AI Agents",
      "title_zh": "åŸºäºç”Ÿæˆå¼ AI æ™ºèƒ½ä½“çš„æƒ…æ„Ÿæ¨¡æ‹Ÿ",
      "authors": [
        "Melrose Tia",
        "Jezreel Sophia Lanuzo",
        "Lei Rigi Baltazar",
        "Marie Joy Lopez-Relente",
        "Diwa Malaya QuiÃ±ones",
        "Jason Albia"
      ],
      "abstract": "Traditional sentiment analysis relies on surface-level linguistic patterns and retrospective data, limiting its ability to capture the psychological and contextual drivers of human sentiment. These limitations constrain its effectiveness in applications that require predictive insight, such as policy testing, narrative framing, and behavioral forecasting. We present a robust framework for sentiment simulation using generative AI agents embedded with psychologically rich profiles. Agents are instantiated from a nationally representative survey of 2,485 Filipino respondents, combining sociodemographic information with validated constructs of personality traits, values, beliefs, and socio-political attitudes. The framework includes three stages: (1) agent embodiment via categorical or contextualized encodings, (2) exposure to real-world political and economic scenarios, and (3) generation of sentiment ratings accompanied by explanatory rationales. Using Quadratic Weighted Accuracy (QWA), we evaluated alignment between agent-generated and human responses. Contextualized encoding achieved 92% alignment in replicating original survey responses. In sentiment simulation tasks, agents reached 81%--86% accuracy against ground truth sentiment, with contextualized profile encodings significantly outperforming categorical (p < 0.0001, Cohen's d = 0.70). Simulation results remained consistent across repeated trials (+/-0.2--0.5% SD) and resilient to variation in scenario framing (p = 0.9676, Cohen's d = 0.02). Our findings establish a scalable framework for sentiment modeling through psychographically grounded AI agents. This work signals a paradigm shift in sentiment analysis from retrospective classification to prospective and dynamic simulation grounded in psychology of sentiment formation.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ä¸ªåŸºäºç”Ÿæˆå¼äººå·¥æ™ºèƒ½æ™ºèƒ½ä½“(Generative AI Agents)çš„æƒ…ç»ªæ¨¡æ‹Ÿæ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³ä¼ ç»Ÿæƒ…æ„Ÿåˆ†æ(Sentiment Analysis)åœ¨æ•æ‰å¿ƒç†å’ŒèƒŒæ™¯é©±åŠ¨å› ç´ æ–¹é¢çš„å±€é™æ€§ã€‚ç ”ç©¶å›¢é˜Ÿåˆ©ç”¨ä¸€é¡¹æ¶µç›–2,485åè²å¾‹å®¾å—è®¿è€…çš„å…¨å›½æ€§è°ƒæŸ¥æ•°æ®ï¼Œä¸ºæ™ºèƒ½ä½“åµŒå…¥äº†åŒ…å«ç¤¾ä¼šäººå£ç‰¹å¾ã€äººæ ¼ç‰¹è´¨(Personality Traits)ã€ä»·å€¼è§‚åŠæ”¿æ²»æ€åº¦åœ¨å†…çš„ä¸°å¯Œå¿ƒç†ç”»åƒã€‚è¯¥æ¡†æ¶é€šè¿‡æ™ºèƒ½ä½“ä½“ç°(Agent Embodiment)ã€åœºæ™¯æš´éœ²å’Œç”Ÿæˆæƒ…ç»ªè¯„åˆ†åŠç†ç”±ä¸‰ä¸ªé˜¶æ®µï¼Œå®ç°äº†å¯¹äººç±»æƒ…ç»ªçš„åŠ¨æ€æ¨¡æ‹Ÿã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œä¸Šä¸‹æ–‡ç¼–ç (Contextualized Encoding)åœ¨å¤åˆ¶åŸå§‹è°ƒæŸ¥ååº”æ–¹é¢è¾¾åˆ°äº†92%çš„ä¸€è‡´æ€§ï¼Œå¹¶åœ¨æƒ…ç»ªæ¨¡æ‹Ÿä»»åŠ¡ä¸­å–å¾—äº†81%è‡³86%çš„å‡†ç¡®ç‡ã€‚ç»Ÿè®¡åˆ†æè¯æ˜è¯¥æ¨¡æ‹Ÿè¿‡ç¨‹å…·æœ‰é«˜åº¦çš„ä¸€è‡´æ€§(Consistency)ï¼Œä¸”æ˜¾è‘—ä¼˜äºä¼ ç»Ÿçš„ç±»åˆ«ç¼–ç (Categorical Encoding)æ–¹æ³•ã€‚è¯¥é¡¹å·¥ä½œæ ‡å¿—ç€æƒ…æ„Ÿåˆ†æä»å›é¡¾æ€§åˆ†ç±»(Retrospective Classification)å‘åŸºäºå¿ƒç†å­¦çš„æƒ…ç»ªå½¢æˆå‰ç»æ€§æ¨¡æ‹Ÿ(Prospective Simulation)çš„èŒƒå¼è½¬å˜ã€‚",
      "categories": [
        "cs.MA",
        "cs.AI",
        "cs.CY"
      ],
      "primary_category": "cs.MA",
      "comment": "18 pages, 10 figures",
      "pdf_url": "https://arxiv.org/pdf/2505.22125v1",
      "published_date": "2025-05-28 08:50:56 UTC",
      "updated_date": "2025-05-28 08:50:56 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T15:31:13.955410+00:00"
    },
    {
      "arxiv_id": "2505.22116v3",
      "title": "Multimodal Forecasting of Sparse Intraoperative Hypotension Events Powered by Language Model",
      "title_zh": "åŸºäºè¯­è¨€æ¨¡å‹çš„ç¨€ç–æœ¯ä¸­ä½è¡€å‹äº‹ä»¶å¤šæ¨¡æ€é¢„æµ‹",
      "authors": [
        "Jintao Zhang",
        "Zirui Liu",
        "Mingyue Cheng",
        "Shilong Zhang",
        "Tingyue Pan",
        "Yitong zhou",
        "Qi Liu",
        "Yanhu Xie"
      ],
      "abstract": "Intraoperative hypotension (IOH) frequently occurs under general anesthesia and is strongly linked to adverse outcomes such as myocardial injury and increased mortality. Despite its significance, IOH prediction is hindered by event sparsity and the challenge of integrating static and dynamic data across diverse patients. In this paper, we propose \\textbf{IOHFuseLM}, a multimodal language model framework. To accurately identify and differentiate sparse hypotensive events, we leverage a two-stage training strategy. The first stage involves domain adaptive pretraining on IOH physiological time series augmented through diffusion methods, thereby enhancing the model sensitivity to patterns associated with hypotension. Subsequently, task fine-tuning is performed on the original clinical dataset to further enhance the ability to distinguish normotensive from hypotensive states. To enable multimodal fusion for each patient, we align structured clinical descriptions with the corresponding physiological time series at the token level. Such alignment enables the model to capture individualized temporal patterns alongside their corresponding clinical semantics. In addition, we convert static patient attributes into structured text to enrich personalized information. Experimental evaluations on two intraoperative datasets demonstrate that IOHFuseLM outperforms established baselines in accurately identifying IOH events, highlighting its applicability in clinical decision support scenarios. Our code is publicly available to promote reproducibility at https://github.com/zjt-gpu/IOHFuseLM.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†IOHFuseLMï¼Œä¸€ç§æ—¨åœ¨è§£å†³æœ¯ä¸­ä½è¡€å‹(Intraoperative Hypotension, IOH)é¢„æµ‹ä¸­äº‹ä»¶ç¨€ç–åŠå¤šæ¨¡æ€æ•°æ®é›†æˆéš¾é¢˜çš„è¯­è¨€æ¨¡å‹æ¡†æ¶ã€‚ä¸ºäº†å‡†ç¡®è¯†åˆ«ç¨€ç–äº‹ä»¶ï¼Œè¯¥æ¡†æ¶é‡‡ç”¨äº†ä¸¤é˜¶æ®µè®­ç»ƒç­–ç•¥ï¼ŒåŒ…æ‹¬åœ¨é€šè¿‡æ‰©æ•£æ–¹æ³•(Diffusion methods)å¢å¼ºçš„ç”Ÿç†æ—¶é—´åºåˆ—ä¸Šè¿›è¡Œé¢†åŸŸè‡ªé€‚åº”é¢„è®­ç»ƒï¼Œä»¥åŠåœ¨åŸå§‹ä¸´åºŠæ•°æ®é›†ä¸Šè¿›è¡Œä»»åŠ¡å¾®è°ƒã€‚IOHFuseLMåœ¨Tokençº§åˆ«å°†ç»“æ„åŒ–ä¸´åºŠæè¿°ä¸ç›¸åº”çš„ç”Ÿç†æ—¶é—´åºåˆ—è¿›è¡Œå¯¹é½ï¼Œä»è€Œç²¾å‡†æ•æ‰ä¸ªä½“åŒ–çš„æ—¶é—´æ¨¡å¼åŠå…¶ä¸´åºŠè¯­ä¹‰ã€‚æ­¤å¤–ï¼Œç ”ç©¶è¿˜é€šè¿‡å°†é™æ€æ‚£è€…å±æ€§è½¬æ¢ä¸ºç»“æ„åŒ–æ–‡æœ¬æ¥ä¸°å¯Œä¸ªæ€§åŒ–èƒŒæ™¯ä¿¡æ¯ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒIOHFuseLMåœ¨ä¸¤ä¸ªæœ¯ä¸­æ•°æ®é›†ä¸Šå‡ä¼˜äºç°æœ‰çš„åŸºçº¿æ¨¡å‹ï¼Œæ˜¾è‘—æå‡äº†IOHäº‹ä»¶çš„è¯†åˆ«å‡†ç¡®æ€§ã€‚è¿™ä¸€æˆæœè¯æ˜äº†å¤šæ¨¡æ€èåˆåœ¨ä¸´åºŠé¢„è­¦ä¸­çš„æœ‰æ•ˆæ€§ï¼Œå¹¶ä¸ºä¸´åºŠå†³ç­–æ”¯æŒç³»ç»Ÿæä¾›äº†å…·æœ‰å®ç”¨ä»·å€¼çš„æŠ€æœ¯æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.22116v3",
      "published_date": "2025-05-28 08:44:55 UTC",
      "updated_date": "2025-07-22 09:34:56 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T15:31:14.494188+00:00"
    },
    {
      "arxiv_id": "2505.22112v1",
      "title": "Visual Large Language Models Exhibit Human-Level Cognitive Flexibility in the Wisconsin Card Sorting Test",
      "title_zh": "è§†è§‰å¤§è¯­è¨€æ¨¡å‹åœ¨å¨æ–¯åº·æ˜Ÿå¡ç‰‡åˆ†ç±»æµ‹è¯•ä¸­å±•ç°å‡ºäººç±»æ°´å¹³çš„è®¤çŸ¥çµæ´»æ€§",
      "authors": [
        "Guangfu Hao",
        "Frederic Alexandre",
        "Shan Yu"
      ],
      "abstract": "Cognitive flexibility has been extensively studied in human cognition but remains relatively unexplored in the context of Visual Large Language Models (VLLMs). This study assesses the cognitive flexibility of state-of-the-art VLLMs (GPT-4o, Gemini-1.5 Pro, and Claude-3.5 Sonnet) using the Wisconsin Card Sorting Test (WCST), a classic measure of set-shifting ability. Our results reveal that VLLMs achieve or surpass human-level set-shifting capabilities under chain-of-thought prompting with text-based inputs. However, their abilities are highly influenced by both input modality and prompting strategy. In addition, we find that through role-playing, VLLMs can simulate various functional deficits aligned with patients having impairments in cognitive flexibility, suggesting that VLLMs may possess a cognitive architecture, at least regarding the ability of set-shifting, similar to the brain. This study reveals the fact that VLLMs have already approached the human level on a key component underlying our higher cognition, and highlights the potential to use them to emulate complex brain processes.",
      "tldr_zh": "è¿™é¡¹ç ”ç©¶é€šè¿‡å¨æ–¯åº·æ˜Ÿå¡ç‰‡åˆ†ç±»æµ‹è¯•(Wisconsin Card Sorting Test, WCST)è¯„ä¼°äº†å½“å‰é¢†å…ˆçš„è§†è§‰å¤§è¯­è¨€æ¨¡å‹(VLLMs)ï¼ˆåŒ…æ‹¬GPT-4oã€Gemini-1.5 Proå’ŒClaude-3.5 Sonnetï¼‰çš„è®¤çŸ¥çµæ´»æ€§ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œåœ¨é“¾å¼æ€ç»´æç¤º(Chain-of-Thought prompting)å’Œæ–‡æœ¬è¾“å…¥çš„æƒ…å†µä¸‹ï¼ŒVLLMsè¡¨ç°å‡ºè¾¾åˆ°ç”šè‡³è¶…è¶Šäººç±»æ°´å¹³çš„é›†åˆè½¬ç§»(set-shifting)èƒ½åŠ›ã€‚ç ”ç©¶è¿›ä¸€æ­¥å‘ç°ï¼Œè¿™äº›èƒ½åŠ›å—è¾“å…¥æ¨¡æ€(modality)å’Œæç¤ºç­–ç•¥çš„æ˜¾è‘—å½±å“ã€‚æ­¤å¤–ï¼ŒVLLMsé€šè¿‡è§’è‰²æ‰®æ¼”èƒ½å¤Ÿæ¨¡æ‹Ÿè®¤çŸ¥çµæ´»æ€§å—æŸæ‚£è€…çš„åŠŸèƒ½ç¼ºé™·ï¼Œè¡¨æ˜å…¶åœ¨é›†åˆè½¬ç§»æ–¹é¢å¯èƒ½å…·å¤‡ä¸äººç±»å¤§è„‘ç›¸ä¼¼çš„è®¤çŸ¥æ¶æ„ã€‚è¯¥ç ”ç©¶è¯å®äº†VLLMsåœ¨äººç±»é«˜çº§è®¤çŸ¥æ ¸å¿ƒç»„ä»¶ä¸Šå·²æ¥è¿‘äººç±»æ°´å¹³ï¼Œå¹¶å±•ç¤ºäº†å…¶åœ¨æ¨¡æ‹Ÿå¤æ‚å¤§è„‘è¿‡ç¨‹æ–¹é¢çš„å·¨å¤§æ½œåŠ›ã€‚",
      "categories": [
        "cs.AI",
        "q-bio.NC"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.22112v1",
      "published_date": "2025-05-28 08:40:55 UTC",
      "updated_date": "2025-05-28 08:40:55 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T15:32:04.996291+00:00"
    },
    {
      "arxiv_id": "2505.22109v3",
      "title": "The quest for the GRAph Level autoEncoder (GRALE)",
      "title_zh": "æ¢ç´¢å›¾çº§è‡ªåŠ¨ç¼–ç å™¨ (GRALE)",
      "authors": [
        "Paul Krzakala",
        "Gabriel Melo",
        "Charlotte Laclau",
        "Florence d'AlchÃ©-Buc",
        "RÃ©mi Flamary"
      ],
      "abstract": "Although graph-based learning has attracted a lot of attention, graph representation learning is still a challenging task whose resolution may impact key application fields such as chemistry or biology. To this end, we introduce GRALE, a novel graph autoencoder that encodes and decodes graphs of varying sizes into a shared embedding space. GRALE is trained using an Optimal Transport-inspired loss that compares the original and reconstructed graphs and leverages a differentiable node matching module, which is trained jointly with the encoder and decoder. The proposed attention-based architecture relies on Evoformer, the core component of AlphaFold, which we extend to support both graph encoding and decoding. We show, in numerical experiments on simulated and molecular data, that GRALE enables a highly general form of pre-training, applicable to a wide range of downstream tasks, from classification and regression to more complex tasks such as graph interpolation, editing, matching, and prediction.",
      "tldr_zh": "è¯¥ç ”ç©¶å¼•å…¥äº† GRALEï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹çš„å›¾è‡ªåŠ¨ç¼–ç å™¨ (graph autoencoder)ï¼Œæ—¨åœ¨å°†ä¸åŒè§„æ¨¡çš„å›¾ç¼–ç å¹¶è§£ç åˆ°å…±äº«çš„åµŒå…¥ç©ºé—´ä¸­ã€‚ä¸ºäº†åº”å¯¹åŒ–å­¦å’Œç”Ÿç‰©ç­‰å…³é”®åº”ç”¨é¢†åŸŸä¸­å¤æ‚çš„å›¾è¡¨ç¤ºå­¦ä¹ æŒ‘æˆ˜ï¼ŒGRALE é‡‡ç”¨å—æœ€ä¼˜ä¼ è¾“ (Optimal Transport) å¯å‘çš„æŸå¤±å‡½æ•°æ¥å¯¹æ¯”åŸå§‹å›¾ä¸é‡æ„å›¾ã€‚è¯¥æ¡†æ¶é›†æˆäº†ä¸€ä¸ªä¸ç¼–è§£ç å™¨è”åˆè®­ç»ƒçš„å¯å¾®åˆ†èŠ‚ç‚¹åŒ¹é…æ¨¡å— (differentiable node matching module)ï¼Œå¹¶æ‰©å±•äº† AlphaFold çš„æ ¸å¿ƒç»„ä»¶ Evoformer æ¶æ„ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒGRALE æ”¯æŒä¸€ç§é«˜åº¦é€šç”¨çš„é¢„è®­ç»ƒå½¢å¼ï¼Œåœ¨æ¨¡æ‹Ÿæ•°æ®å’Œåˆ†å­æ•°æ®ä¸Šå‡è¡¨ç°å‡ºè‰²ã€‚å®ƒä¸ä»…èƒ½æœ‰æ•ˆå¤„ç†åˆ†ç±»å’Œå›å½’ä»»åŠ¡ï¼Œè¿˜èƒ½èƒœä»»å›¾æ’å€¼ (graph interpolation)ã€ç¼–è¾‘ (editing)ã€åŒ¹é… (matching) å’Œé¢„æµ‹ç­‰æ›´ä¸ºå¤æ‚çš„ä¸‹æ¸¸ä»»åŠ¡ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.22109v3",
      "published_date": "2025-05-28 08:37:33 UTC",
      "updated_date": "2025-10-20 09:22:12 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T15:31:34.086518+00:00"
    },
    {
      "arxiv_id": "2505.22108v3",
      "title": "Inclusive, Differentially Private Federated Learning for Clinical Data",
      "title_zh": "é¢å‘ä¸´åºŠæ•°æ®çš„åŒ…å®¹æ€§å·®åˆ†éšç§è”é‚¦å­¦ä¹ ",
      "authors": [
        "Santhosh Parampottupadam",
        "Melih CoÅŸÄŸun",
        "Sarthak Pati",
        "Maximilian Zenk",
        "Saikat Roy",
        "Dimitrios Bounias",
        "Benjamin Hamm",
        "Sinem Sav",
        "Ralf Floca",
        "Klaus Maier-Hein"
      ],
      "abstract": "Federated Learning (FL) offers a promising approach for training clinical AI models without centralizing sensitive patient data. However, its real-world adoption is hindered by challenges related to privacy, resource constraints, and compliance. Existing Differential Privacy (DP) approaches often apply uniform noise, which disproportionately degrades model performance, even among well-compliant institutions. In this work, we propose a novel compliance-aware FL framework that enhances DP by adaptively adjusting noise based on quantifiable client compliance scores. Additionally, we introduce a compliance scoring tool based on key healthcare and security standards to promote secure, inclusive, and equitable participation across diverse clinical settings. Extensive experiments on public datasets demonstrate that integrating under-resourced, less compliant clinics with highly regulated institutions yields accuracy improvements of up to 15% over traditional FL. This work advances FL by balancing privacy, compliance, and performance, making it a viable solution for real-world clinical workflows in global healthcare.",
      "tldr_zh": "è¿™é¡¹ç ”ç©¶é’ˆå¯¹ä¸´åºŠæ•°æ®ä¸­çš„è”é‚¦å­¦ä¹ (Federated Learning)é¢ä¸´çš„éšç§ã€èµ„æºé™åˆ¶å’Œåˆè§„æ€§æŒ‘æˆ˜ï¼Œæå‡ºäº†ä¸€ç§å…·æœ‰åŒ…å®¹æ€§çš„å·®åˆ†éšç§(Differential Privacy)æ¡†æ¶ã€‚é’ˆå¯¹ç°æœ‰å·®åˆ†éšç§æ–¹æ³•å› ç»Ÿä¸€æ·»åŠ å™ªå£°è€Œå¯¼è‡´æ¨¡å‹æ€§èƒ½ä¸æˆæ¯”ä¾‹ä¸‹é™çš„é—®é¢˜ï¼Œè¯¥å·¥ä½œæå‡ºäº†ä¸€ç§åˆè§„æ„ŸçŸ¥å‹è”é‚¦å­¦ä¹ (compliance-aware FL)æ¡†æ¶ï¼Œèƒ½å¤Ÿæ ¹æ®é‡åŒ–çš„åˆè§„å¾—åˆ†(compliance scores)è‡ªé€‚åº”åœ°è°ƒæ•´å™ªå£°å¼ºåº¦ã€‚æ­¤å¤–ï¼Œç ”ç©¶å¼•å…¥äº†åŸºäºå…³é”®åŒ»ç–—å®‰å…¨æ ‡å‡†çš„åˆè§„è¯„åˆ†å·¥å…·ï¼Œä»¥ä¿ƒè¿›å¤šå…ƒä¸´åºŠç¯å¢ƒä¸‹å®‰å…¨ä¸”å…¬å¹³çš„å‚ä¸ã€‚åœ¨å…¬å¼€æ•°æ®é›†ä¸Šçš„å®éªŒè¯æ˜ï¼Œå°†èµ„æºåŒ®ä¹ä¸”åˆè§„æ€§è¾ƒä½çš„è¯Šæ‰€ä¸é«˜åº¦ç›‘ç®¡çš„æœºæ„æ•´åˆï¼Œå¯ä½¿æ¨¡å‹å‡†ç¡®ç‡æ¯”ä¼ ç»Ÿè”é‚¦å­¦ä¹ æå‡å¤šè¾¾15%ã€‚è¯¥æˆæœé€šè¿‡æœ‰æ•ˆå¹³è¡¡éšç§ä¿æŠ¤ã€åˆè§„æ€§ä¸æ¨¡å‹æ€§èƒ½ï¼Œä¸ºå…¨çƒåŒ»ç–—å®å¢ƒå·¥ä½œæµä¸­çš„è”é‚¦å­¦ä¹ åº”ç”¨æä¾›äº†å¯è¡Œæ–¹æ¡ˆã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CR",
        "cs.DC"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.22108v3",
      "published_date": "2025-05-28 08:36:21 UTC",
      "updated_date": "2025-10-11 09:35:25 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T15:31:47.293020+00:00"
    },
    {
      "arxiv_id": "2505.22106v1",
      "title": "AudioTurbo: Fast Text-to-Audio Generation with Rectified Diffusion",
      "title_zh": "AudioTurboï¼šåŸºäºä¿®æ­£æ‰©æ•£çš„å¿«é€Ÿæ–‡æœ¬è½¬éŸ³é¢‘ç”Ÿæˆ",
      "authors": [
        "Junqi Zhao",
        "Jinzheng Zhao",
        "Haohe Liu",
        "Yun Chen",
        "Lu Han",
        "Xubo Liu",
        "Mark Plumbley",
        "Wenwu Wang"
      ],
      "abstract": "Diffusion models have significantly improved the quality and diversity of audio generation but are hindered by slow inference speed. Rectified flow enhances inference speed by learning straight-line ordinary differential equation (ODE) paths. However, this approach requires training a flow-matching model from scratch and tends to perform suboptimally, or even poorly, at low step counts. To address the limitations of rectified flow while leveraging the advantages of advanced pre-trained diffusion models, this study integrates pre-trained models with the rectified diffusion method to improve the efficiency of text-to-audio (TTA) generation. Specifically, we propose AudioTurbo, which learns first-order ODE paths from deterministic noise sample pairs generated by a pre-trained TTA model. Experiments on the AudioCaps dataset demonstrate that our model, with only 10 sampling steps, outperforms prior models and reduces inference to 3 steps compared to a flow-matching-based acceleration model.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹æ‰©æ•£æ¨¡å‹åœ¨éŸ³é¢‘ç”Ÿæˆä¸­æ¨ç†é€Ÿåº¦æ…¢ä¸”ä¼ ç»Ÿçš„ Rectified Flow æ–¹æ³•åœ¨ä½é‡‡æ ·æ­¥æ•°ä¸‹è¡¨ç°ä¸ä½³çš„é—®é¢˜ï¼Œæå‡ºäº† AudioTurbo æ¡†æ¶ã€‚AudioTurbo å°†å…ˆè¿›çš„é¢„è®­ç»ƒæ¨¡å‹ä¸ä¿®æ­£æ‰©æ•£ (Rectified Diffusion) æ–¹æ³•ç›¸ç»“åˆï¼Œæ—¨åœ¨æå‡æ–‡æœ¬åˆ°éŸ³é¢‘ (Text-to-Audio, TTA) çš„ç”Ÿæˆæ•ˆç‡ã€‚è¯¥æ–¹æ³•é€šè¿‡å­¦ä¹ ç”±é¢„è®­ç»ƒ TTA æ¨¡å‹ç”Ÿæˆçš„ç¡®å®šæ€§å™ªå£°é‡‡æ ·å¯¹çš„ä¸€é˜¶ ODE è·¯å¾„ï¼Œä»è€Œå…‹æœäº†ä»å¤´è®­ç»ƒæµåŒ¹é…æ¨¡å‹çš„å±€é™æ€§ã€‚åœ¨ AudioCaps æ•°æ®é›†ä¸Šçš„å®éªŒè¯æ˜ï¼Œè¯¥æ¨¡å‹åœ¨ä»…ä½¿ç”¨ 10 ä¸ªé‡‡æ ·æ­¥æ—¶ä¾¿èƒ½è¶…è¶Šä¹‹å‰çš„æ¨¡å‹ã€‚æ­¤å¤–ï¼Œç›¸æ¯”äºç°æœ‰çš„åŸºäºæµåŒ¹é… (Flow-matching) çš„åŠ é€Ÿæ–¹æ¡ˆï¼ŒAudioTurbo æˆåŠŸå°†æ¨ç†è¿‡ç¨‹ç¼©å‡è‡³ 3 æ­¥ï¼Œå®ç°äº†æé€Ÿä¸”é«˜è´¨é‡çš„éŸ³é¢‘ç”Ÿæˆã€‚",
      "categories": [
        "cs.SD",
        "cs.AI",
        "eess.AS"
      ],
      "primary_category": "cs.SD",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.22106v1",
      "published_date": "2025-05-28 08:33:58 UTC",
      "updated_date": "2025-05-28 08:33:58 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T15:32:50.924496+00:00"
    },
    {
      "arxiv_id": "2505.22104v1",
      "title": "Efficient Dynamic Shielding for Parametric Safety Specifications",
      "title_zh": "é’ˆå¯¹å‚æ•°åŒ–å®‰å…¨è§„èŒƒçš„é«˜æ•ˆåŠ¨æ€å±è”½",
      "authors": [
        "Davide Corsi",
        "Kaushik Mallik",
        "Andoni Rodriguez",
        "Cesar Sanchez"
      ],
      "abstract": "Shielding has emerged as a promising approach for ensuring safety of AI-controlled autonomous systems. The algorithmic goal is to compute a shield, which is a runtime safety enforcement tool that needs to monitor and intervene the AI controller's actions if safety could be compromised otherwise. Traditional shields are designed statically for a specific safety requirement. Therefore, if the safety requirement changes at runtime due to changing operating conditions, the shield needs to be recomputed from scratch, causing delays that could be fatal. We introduce dynamic shields for parametric safety specifications, which are succinctly represented sets of all possible safety specifications that may be encountered at runtime. Our dynamic shields are statically designed for a given safety parameter set, and are able to dynamically adapt as the true safety specification (permissible by the parameters) is revealed at runtime. The main algorithmic novelty lies in the dynamic adaptation procedure, which is a simple and fast algorithm that utilizes known features of standard safety shields, like maximal permissiveness. We report experimental results for a robot navigation problem in unknown territories, where the safety specification evolves as new obstacles are discovered at runtime. In our experiments, the dynamic shields took a few minutes for their offline design, and took between a fraction of a second and a few seconds for online adaptation at each step, whereas the brute-force online recomputation approach was up to 5 times slower.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹AIæ§åˆ¶çš„è‡ªä¸»ç³»ç»Ÿä¸­ï¼Œä¼ ç»Ÿå®‰å…¨é˜²æŠ¤(Shielding)æŠ€æœ¯å› æ— æ³•åº”å¯¹è¿è¡Œæ—¶åŠ¨æ€å˜åŒ–çš„å®‰å…¨éœ€æ±‚è€Œå¯¼è‡´è®¡ç®—å»¶è¿Ÿçš„é—®é¢˜ï¼Œæå‡ºäº†é’ˆå¯¹å‚æ•°åŒ–å®‰å…¨è§„èŒƒ(parametric safety specifications)çš„åŠ¨æ€é˜²æŠ¤(dynamic shields)æ–¹æ³•ã€‚è¿™ç§åŠ¨æ€é˜²æŠ¤å™¨é’ˆå¯¹é¢„å®šä¹‰çš„å‚æ•°é›†è¿›è¡Œç¦»çº¿è®¾è®¡ï¼Œå¹¶èƒ½åœ¨è¿è¡Œæ—¶æ ¹æ®å®æ—¶æ­ç¤ºçš„å®‰å…¨è§„èŒƒè¿›è¡Œå¿«é€Ÿé€‚é…ã€‚å…¶ç®—æ³•æ ¸å¿ƒåœ¨äºä¸€ç§ç»“åˆäº†æœ€å¤§å…è®¸æ€§(maximal permissiveness)ç­‰ç‰¹å¾çš„é«˜æ•ˆåŠ¨æ€é€‚é…ç¨‹åºï¼Œæ˜¾è‘—é™ä½äº†è®¡ç®—å¼€é”€ã€‚åœ¨æœªçŸ¥ç¯å¢ƒæœºå™¨äººå¯¼èˆªçš„å®éªŒä¸­ï¼Œè¯¥æ–¹æ³•å®ç°äº†ç§’çº§ç”šè‡³äºšç§’çº§çš„åœ¨çº¿é€‚é…é€Ÿåº¦ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œä¸ä¼ ç»Ÿçš„æš´åŠ›åœ¨çº¿é‡æ–°è®¡ç®—(brute-force online recomputation)æ–¹æ³•ç›¸æ¯”ï¼Œè¯¥åŠ¨æ€é˜²æŠ¤æ–¹æ³•çš„è¿è¡Œæ•ˆç‡æå‡äº†é«˜è¾¾5å€ï¼Œä¸ºå¤æ‚åŠ¨æ€ç¯å¢ƒä¸‹çš„ç³»ç»Ÿå®‰å…¨æä¾›äº†é«˜æ•ˆä¿éšœã€‚",
      "categories": [
        "cs.AI",
        "cs.LG",
        "cs.LO",
        "cs.RO",
        "eess.SY"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.22104v1",
      "published_date": "2025-05-28 08:30:03 UTC",
      "updated_date": "2025-05-28 08:30:03 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T15:32:05.886770+00:00"
    },
    {
      "arxiv_id": "2505.22096v1",
      "title": "Knowledge Base Construction for Knowledge-Augmented Text-to-SQL",
      "title_zh": "é¢å‘çŸ¥è¯†å¢å¼º Text-to-SQL çš„çŸ¥è¯†åº“æ„å»º",
      "authors": [
        "Jinheon Baek",
        "Horst Samulowitz",
        "Oktie Hassanzadeh",
        "Dharmashankar Subramanian",
        "Sola Shirai",
        "Alfio Gliozzo",
        "Debarun Bhattacharjya"
      ],
      "abstract": "Text-to-SQL aims to translate natural language queries into SQL statements, which is practical as it enables anyone to easily retrieve the desired information from databases. Recently, many existing approaches tackle this problem with Large Language Models (LLMs), leveraging their strong capability in understanding user queries and generating corresponding SQL code. Yet, the parametric knowledge in LLMs might be limited to covering all the diverse and domain-specific queries that require grounding in various database schemas, which makes generated SQLs less accurate oftentimes. To tackle this, we propose constructing the knowledge base for text-to-SQL, a foundational source of knowledge, from which we retrieve and generate the necessary knowledge for given queries. In particular, unlike existing approaches that either manually annotate knowledge or generate only a few pieces of knowledge for each query, our knowledge base is comprehensive, which is constructed based on a combination of all the available questions and their associated database schemas along with their relevant knowledge, and can be reused for unseen databases from different datasets and domains. We validate our approach on multiple text-to-SQL datasets, considering both the overlapping and non-overlapping database scenarios, where it outperforms relevant baselines substantially.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤§è¯­è¨€æ¨¡å‹(LLMs)åœ¨å¤„ç†å…·æœ‰é¢†åŸŸç‰¹å®šæ€§çš„æ•°æ®åº“æ¨¡å¼(database schemas)æ—¶å› å‚æ•°åŒ–çŸ¥è¯†æœ‰é™è€Œå¯¼è‡´å‡†ç¡®ç‡ä¸‹é™çš„é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§ä¸ºText-to-SQLä»»åŠ¡æ„å»ºåŸºç¡€çŸ¥è¯†åº“(Knowledge Base)çš„æ–°æ–¹æ³•ã€‚ä¸åŒäºä»¥å¾€ä¾èµ–æ‰‹åŠ¨æ ‡æ³¨æˆ–ä»…é’ˆå¯¹ç‰¹å®šæŸ¥è¯¢ç”Ÿæˆé›¶æ•£çŸ¥è¯†çš„åšæ³•ï¼Œè¯¥ç ”ç©¶é€šè¿‡æ•´åˆæµ·é‡é—®é¢˜ã€å…³è”æ¨¡å¼åŠå…¶ç›¸å…³çŸ¥è¯†ï¼Œæ„å»ºäº†ä¸€ä¸ªå…¨é¢ä¸”å¯å¤ç”¨çš„çŸ¥è¯†ä½“ç³»ã€‚è¯¥çŸ¥è¯†åº“å…·å¤‡è·¨é¢†åŸŸè¿ç§»èƒ½åŠ›ï¼Œèƒ½å¤Ÿæœ‰æ•ˆæ”¯æŒä¸åŒæ•°æ®é›†ä¸­çš„æœªçŸ¥æ•°æ®åº“(unseen databases)ä»»åŠ¡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å¤šç§Text-to-SQLæ•°æ®é›†åŠä¸åŒæ•°æ®åº“åœºæ™¯ä¸‹çš„è¡¨ç°å‡æ˜¾è‘—ä¼˜äºç°æœ‰çš„åŸºçº¿æ¨¡å‹ï¼Œä¸ºå¢å¼ºæ–‡æœ¬è½¬SQLç³»ç»Ÿçš„å‡†ç¡®æ€§å’Œæ³›åŒ–èƒ½åŠ›æä¾›äº†æœ‰åŠ›æ”¯æ’‘ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "ACL Findings 2025",
      "pdf_url": "https://arxiv.org/pdf/2505.22096v1",
      "published_date": "2025-05-28 08:17:58 UTC",
      "updated_date": "2025-05-28 08:17:58 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T15:33:32.993643+00:00"
    },
    {
      "arxiv_id": "2505.22093v1",
      "title": "From Coders to Critics: Empowering Students through Peer Assessment in the Age of AI Copilots",
      "title_zh": "ä»ä»£ç ç¼–å†™è€…åˆ°è¯„å®¡è€…ï¼šAI Copilot æ—¶ä»£ä¸‹é€šè¿‡åŒä¼´è¯„ä¼°èµ‹èƒ½å­¦ç”Ÿ",
      "authors": [
        "Santiago Berrezueta-Guzman",
        "Stephan Krusche",
        "Stefan Wagner"
      ],
      "abstract": "The rapid adoption of AI powered coding assistants like ChatGPT and other coding copilots is transforming programming education, raising questions about assessment practices, academic integrity, and skill development. As educators seek alternatives to traditional grading methods susceptible to AI enabled plagiarism, structured peer assessment could be a promising strategy. This paper presents an empirical study of a rubric based, anonymized peer review process implemented in a large introductory programming course.\n  Students evaluated each other's final projects (2D game), and their assessments were compared to instructor grades using correlation, mean absolute error, and root mean square error (RMSE). Additionally, reflective surveys from 47 teams captured student perceptions of fairness, grading behavior, and preferences regarding grade aggregation. Results show that peer review can approximate instructor evaluation with moderate accuracy and foster student engagement, evaluative thinking, and interest in providing good feedback to their peers. We discuss these findings for designing scalable, trustworthy peer assessment systems to face the age of AI assisted coding.",
      "tldr_zh": "æœ¬ç ”ç©¶æ¢è®¨äº†åœ¨AI Copilotsæ™®åŠçš„èƒŒæ™¯ä¸‹ï¼Œå¦‚ä½•é€šè¿‡ç»“æ„åŒ–çš„åŒä¼´è¯„ä¼°ï¼ˆPeer Assessmentï¼‰åº”å¯¹ç¼–ç¨‹æ•™è‚²ä¸­çš„å­¦æœ¯è¯šä¿¡ä¸æŠ€èƒ½åŸ¹å…»æŒ‘æˆ˜ã€‚ç ”ç©¶åœ¨ä¸€é—¨å¤§å‹å…¥é—¨ç¼–ç¨‹è¯¾ç¨‹ä¸­å®æ–½äº†åŸºäºé‡è§„ï¼ˆRubricï¼‰çš„åŒ¿ååŒä¼´è¯„å®¡æµç¨‹ï¼Œè®©å­¦ç”Ÿç›¸äº’è¯„ä»·å½¼æ­¤çš„2Dæ¸¸æˆé¡¹ç›®ã€‚ç ”ç©¶å›¢é˜Ÿé€šè¿‡ç›¸å…³æ€§ã€å¹³å‡ç»å¯¹è¯¯å·®ï¼ˆMAEï¼‰å’Œå‡æ–¹æ ¹è¯¯å·®ï¼ˆRMSEï¼‰ç­‰æŒ‡æ ‡ï¼Œå°†å­¦ç”Ÿçš„è¯„ä¼°ç»“æœä¸æ•™å¸ˆè¯„åˆ†è¿›è¡Œäº†å¯¹æ¯”ï¼Œå¹¶ç»“åˆ47ä¸ªå›¢é˜Ÿçš„åå°„æ€§è°ƒæŸ¥ï¼ˆReflective Surveysï¼‰åˆ†æäº†å­¦ç”Ÿå¯¹å…¬å¹³æ€§åŠè¯„åˆ†è¡Œä¸ºçš„çœ‹æ³•ã€‚ç»“æœè¡¨æ˜ï¼ŒåŒä¼´è¯„å®¡èƒ½ä»¥ä¸­ç­‰å‡†ç¡®åº¦æ¥è¿‘æ•™å¸ˆçš„è¯„ä¼°æ°´å¹³ï¼Œå¹¶èƒ½æ˜¾è‘—æå‡å­¦ç”Ÿçš„å‚ä¸åº¦ã€è¯„ä»·æ€§æ€ç»´ï¼ˆEvaluative Thinkingï¼‰ä»¥åŠæä¾›åé¦ˆçš„å…´è¶£ã€‚è¯¥ç ”ç©¶ä¸ºè®¾è®¡å¯æ‰©å±•ä¸”å€¼å¾—ä¿¡èµ–çš„åŒä¼´è¯„ä¼°ç³»ç»Ÿæä¾›äº†å®è¯æ”¯æŒï¼Œæ—¨åœ¨åŠ©åŠ›æ•™è‚²è€…åº”å¯¹AIè¾…åŠ©ç¼–ç¨‹æ—¶ä»£çš„æ•™å­¦å˜é©ã€‚",
      "categories": [
        "cs.CY",
        "cs.AI",
        "cs.HC"
      ],
      "primary_category": "cs.CY",
      "comment": "This is the authors' preprint version of a paper accepted at the 11th International Symposium on Educational Technology, to be held in July 2025, in Bangkok, Thailand. The final published version will be available via IEEE Xplore Library",
      "pdf_url": "https://arxiv.org/pdf/2505.22093v1",
      "published_date": "2025-05-28 08:17:05 UTC",
      "updated_date": "2025-05-28 08:17:05 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T15:33:27.285605+00:00"
    },
    {
      "arxiv_id": "2505.22092v3",
      "title": "VIRAL: Vision-grounded Integration for Reward design And Learning",
      "title_zh": "VIRALï¼šåŸºäºè§†è§‰å¼•å¯¼çš„å¥–åŠ±è®¾è®¡ä¸å­¦ä¹ é›†æˆ",
      "authors": [
        "Valentin Cuzin-Rambaud",
        "Emilien Komlenovic",
        "Alexandre Faure",
        "Bruno Yun"
      ],
      "abstract": "The alignment between humans and machines is a critical challenge in artificial intelligence today. Reinforcement learning, which aims to maximize a reward function, is particularly vulnerable to the risks associated with poorly designed reward functions. Recent advancements has shown that Large Language Models (LLMs) for reward generation can outperform human performance in this context. We introduce VIRAL, a pipeline for generating and refining reward functions through the use of multi-modal LLMs. VIRAL autonomously creates and interactively improves reward functions based on a given environment and a goal prompt or annotated image. The refinement process can incorporate human feedback or be guided by a description generated by a video LLM, which explains the agent's policy in video form. We evaluated VIRAL in five Gymnasium environments, demonstrating that it accelerates the learning of new behaviors while ensuring improved alignment with user intent. The source-code and demo video are available at: https://github.com/VIRAL-UCBL1/VIRAL and https://youtu.be/Hqo82CxVT38.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¼ºåŒ–å­¦ä¹ (Reinforcement Learning)ä¸­å¥–åŠ±å‡½æ•°è®¾è®¡ä¸å½“å¸¦æ¥çš„å¯¹é½(Alignment)é£é™©ï¼Œæå‡ºäº†VIRALæ¡†æ¶ã€‚VIRALæ˜¯ä¸€ä¸ªåŸºäºå¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹(Multi-modal LLMs)çš„å¥–åŠ±è®¾è®¡ä¸ä¼˜åŒ–æµæ°´çº¿ï¼Œèƒ½æ ¹æ®ç¯å¢ƒæè¿°ã€ç›®æ ‡æç¤ºæˆ–æ ‡æ³¨å›¾åƒè‡ªä¸»ç”Ÿæˆå¥–åŠ±å‡½æ•°ã€‚åœ¨ä¼˜åŒ–é˜¶æ®µï¼Œè¯¥æ¡†æ¶é€šè¿‡äº¤äº’å¼æ–¹å¼æ•´åˆäººç±»åé¦ˆï¼Œæˆ–åˆ©ç”¨è§†é¢‘å¤§æ¨¡å‹(Video LLM)ç”Ÿæˆçš„æ™ºèƒ½ä½“ç­–ç•¥æè¿°æ¥æŒ‡å¯¼å¥–åŠ±å‡½æ•°çš„æ”¹è¿›ã€‚é€šè¿‡åœ¨äº”ä¸ªGymnasiumç¯å¢ƒä¸­çš„è¯„ä¼°ï¼Œå®éªŒè¯æ˜VIRALèƒ½æ˜¾è‘—åŠ é€Ÿæ–°è¡Œä¸ºçš„å­¦ä¹ é€Ÿåº¦ã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•æœ‰æ•ˆæå‡äº†æœºå™¨è¡Œä¸ºä¸ç”¨æˆ·æ„å›¾çš„ä¸€è‡´æ€§ï¼Œä¸ºå¤æ‚ç¯å¢ƒä¸‹çš„è‡ªåŠ¨åŒ–å¥–åŠ±å·¥ç¨‹å’Œè§†è§‰æ¥åœ°(Vision-grounded)å­¦ä¹ æä¾›äº†æœ‰æ•ˆçš„è§£å†³æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.22092v3",
      "published_date": "2025-05-28 08:16:09 UTC",
      "updated_date": "2025-10-28 07:37:35 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T15:33:34.648446+00:00"
    },
    {
      "arxiv_id": "2505.22087v1",
      "title": "Cognitively-Inspired Emergent Communication via Knowledge Graphs for Assisting the Visually Impaired",
      "title_zh": "åŸºäºçŸ¥è¯†å›¾è°±çš„å—è®¤çŸ¥å¯å‘æ¶Œç°é€šä¿¡ï¼šåŠ©åŠ›è§†éšœäººå£«è¾…åŠ©",
      "authors": [
        "Ruxiao Chen",
        "Dezheng Han",
        "Wenjie Han",
        "Shuaishuai Guo"
      ],
      "abstract": "Assistive systems for visually impaired individuals must deliver rapid, interpretable, and adaptive feedback to facilitate real-time navigation. Current approaches face a trade-off between latency and semantic richness: natural language-based systems provide detailed guidance but are too slow for dynamic scenarios, while emergent communication frameworks offer low-latency symbolic languages but lack semantic depth, limiting their utility in tactile modalities like vibration. To address these limitations, we introduce a novel framework, Cognitively-Inspired Emergent Communication via Knowledge Graphs (VAG-EC), which emulates human visual perception and cognitive mapping. Our method constructs knowledge graphs to represent objects and their relationships, incorporating attention mechanisms to prioritize task-relevant entities, thereby mirroring human selective attention. This structured approach enables the emergence of compact, interpretable, and context-sensitive symbolic languages. Extensive experiments across varying vocabulary sizes and message lengths demonstrate that VAG-EC outperforms traditional emergent communication methods in Topographic Similarity (TopSim) and Context Independence (CI). These findings underscore the potential of cognitively grounded emergent communication as a fast, adaptive, and human-aligned solution for real-time assistive technologies. Code is available at https://github.com/Anonymous-NLPcode/Anonymous_submission/tree/main.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† VAG-EC æ¡†æ¶ï¼ˆCognitively-Inspired Emergent Communication via Knowledge Graphsï¼‰ï¼Œæ—¨åœ¨ä¸ºè§†éšœäººå£«æä¾›å¿«é€Ÿã€å¯è§£é‡Šä¸”å…·æœ‰è‡ªé€‚åº”èƒ½åŠ›çš„å®æ—¶å¯¼èˆªè¾…åŠ©ã€‚é’ˆå¯¹è‡ªç„¶è¯­è¨€ç³»ç»Ÿå»¶è¿Ÿé«˜ä¸æ¶Œç°é€šä¿¡ï¼ˆEmergent Communicationï¼‰è¯­ä¹‰æ·±åº¦ä¸è¶³çš„çŸ›ç›¾ï¼Œè¯¥æ–¹æ³•é€šè¿‡æ¨¡æ‹Ÿäººç±»è§†è§‰æ„ŸçŸ¥å’Œè®¤çŸ¥åœ°å›¾ï¼Œæ„å»º Knowledge Graphs æ¥è¡¨ç¤ºç‰©ä½“åŠå…¶ç©ºé—´å…³ç³»ã€‚æ¡†æ¶å¼•å…¥äº†æ³¨æ„æœºåˆ¶ï¼ˆAttention Mechanismsï¼‰ä»¥æ¨¡æ‹Ÿäººç±»çš„é€‰æ‹©æ€§æ³¨æ„ï¼Œä»è€Œä¼˜å…ˆå¤„ç†ä»»åŠ¡ç›¸å…³çš„å®ä½“ï¼Œä¿ƒè¿›äº†ç´§å‡‘ä¸”ä¸Šä¸‹æ–‡æ•æ„Ÿçš„ç¬¦å·è¯­è¨€çš„æ¶Œç°ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒVAG-EC åœ¨åœ°å½¢ç›¸ä¼¼åº¦ï¼ˆTopographic Similarity, TopSimï¼‰å’Œä¸Šä¸‹æ–‡ç‹¬ç«‹æ€§ï¼ˆContext Independence, CIï¼‰ç­‰å…³é”®æŒ‡æ ‡ä¸Šå‡ä¼˜äºä¼ ç»Ÿæ¶Œç°é€šä¿¡æ–¹æ³•ã€‚è¿™äº›å‘ç°è¯æ˜äº†åŸºäºè®¤çŸ¥çš„æ¶Œç°é€šä¿¡åœ¨å¼€å‘é«˜æ•ˆã€ç±»äººä¸”ä½å»¶è¿Ÿçš„è¾…åŠ©æŠ€æœ¯æ–¹é¢çš„å·¨å¤§æ½œåŠ›ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.22087v1",
      "published_date": "2025-05-28 08:09:06 UTC",
      "updated_date": "2025-05-28 08:09:06 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T15:33:33.834174+00:00"
    },
    {
      "arxiv_id": "2505.22086v2",
      "title": "iDSE: Navigating Design Space Exploration in High-Level Synthesis Using LLMs",
      "title_zh": "iDSEï¼šåˆ©ç”¨å¤§è¯­è¨€æ¨¡å‹å¼•å¯¼é«˜çº§ç»¼åˆä¸­çš„è®¾è®¡ç©ºé—´æ¢ç´¢",
      "authors": [
        "Runkai Li",
        "Jia Xiong",
        "Xi Wang"
      ],
      "abstract": "High-Level Synthesis (HLS) serves as an agile hardware development tool that streamlines the circuit design by abstracting the register transfer level into behavioral descriptions, while allowing designers to customize the generated microarchitectures through optimization directives. However, the combinatorial explosion of possible directive configurations yields an intractable design space. Traditional design space exploration (DSE) methods, despite adopting heuristics or constructing predictive models to accelerate Pareto-optimal design acquisition, still suffer from prohibitive exploration costs and suboptimal results. Addressing these concerns, we introduce iDSE, the first LLM-aided DSE framework that leverages HLS design quality perception to effectively navigate the design space. iDSE intelligently pruns the design space to guide LLMs in calibrating representative initial sampling designs, expediting convergence toward the Pareto front. By exploiting the convergent and divergent thinking patterns inherent in LLMs for hardware optimization, iDSE achieves multi-path refinement of the design quality and diversity. Extensive experiments demonstrate that iDSE outperforms heuristic-based DSE methods by 5.1$\\times$$\\sim$16.6$\\times$ in proximity to the reference Pareto front, matching NSGA-II with only 4.6% of the explored designs. Our work demonstrates the transformative potential of LLMs in scalable and efficient HLS design optimization, offering new insights into multiobjective optimization challenges.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†iDSEï¼Œè¿™æ˜¯é¦–ä¸ªåˆ©ç”¨å¤§è¯­è¨€æ¨¡å‹(LLMs)è¾…åŠ©çš„é«˜çº§ç»¼åˆ(High-Level Synthesis, HLS)è®¾è®¡ç©ºé—´æ¢ç´¢(Design Space Exploration, DSE)æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³ä¼ ç»Ÿæ–¹æ³•åœ¨å¤„ç†åºå¤§æŒ‡ä»¤é…ç½®ç©ºé—´æ—¶é¢ä¸´çš„æ•ˆç‡ä½ä¸‹å’Œæ¬¡ä¼˜ç»“æœé—®é¢˜ã€‚iDSEé€šè¿‡æ„ŸçŸ¥HLSè®¾è®¡è´¨é‡æ¥æ™ºèƒ½ä¿®å‰ªè®¾è®¡ç©ºé—´ï¼Œå¹¶å¼•å¯¼LLMsæ ¡å‡†å…·æœ‰ä»£è¡¨æ€§çš„åˆå§‹é‡‡æ ·è®¾è®¡ï¼Œä»è€Œæ˜¾è‘—åŠ é€Ÿå‘å¸•ç´¯æ‰˜å‰æ²¿(Pareto front)çš„æ”¶æ•›ã€‚è¯¥æ¡†æ¶å……åˆ†åˆ©ç”¨äº†LLMsåœ¨ç¡¬ä»¶ä¼˜åŒ–ä¸­å›ºæœ‰çš„æ”¶æ•›ä¸å‘æ•£æ€ç»´æ¨¡å¼ï¼Œå®ç°äº†è®¾è®¡è´¨é‡å’Œå¤šæ ·æ€§çš„å¤šè·¯å¾„ç»†åŒ–ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒiDSEåœ¨æ¥è¿‘å‚è€ƒPareto frontæ–¹é¢çš„è¡¨ç°ä¼˜äºåŸºäºå¯å‘å¼çš„DSEæ–¹æ³•5.1å€è‡³16.6å€ã€‚æ­¤å¤–ï¼Œå®ƒä»…éœ€æ¢ç´¢4.6%çš„è®¾è®¡æ–¹æ¡ˆå³å¯è¾¾åˆ°NSGA-IIçš„ä¼˜åŒ–æ°´å¹³ã€‚è¿™é¡¹å·¥ä½œå±•ç¤ºäº†LLMsåœ¨å¯æ‰©å±•ä¸”é«˜æ•ˆçš„HLSè®¾è®¡ä¼˜åŒ–ä¸­çš„å˜é©æ½œåŠ›ï¼Œä¸ºè§£å†³å¤šç›®æ ‡ä¼˜åŒ–æŒ‘æˆ˜æä¾›äº†æ–°çš„è§†è§’ã€‚",
      "categories": [
        "cs.AR",
        "cs.AI"
      ],
      "primary_category": "cs.AR",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.22086v2",
      "published_date": "2025-05-28 08:08:57 UTC",
      "updated_date": "2025-05-31 11:52:20 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T15:33:21.792683+00:00"
    },
    {
      "arxiv_id": "2505.22074v1",
      "title": "The Resurrection of the ReLU",
      "title_zh": "ReLU çš„å¤å…´",
      "authors": [
        "CoÅŸku Can Horuz",
        "Geoffrey Kasenbacher",
        "Saya Higuchi",
        "Sebastian Kairat",
        "Jendrik Stoltz",
        "Moritz Pesl",
        "Bernhard A. Moser",
        "Christoph Linse",
        "Thomas Martinetz",
        "Sebastian Otte"
      ],
      "abstract": "Modeling sophisticated activation functions within deep learning architectures has evolved into a distinct research direction. Functions such as GELU, SELU, and SiLU offer smooth gradients and improved convergence properties, making them popular choices in state-of-the-art models. Despite this trend, the classical ReLU remains appealing due to its simplicity, inherent sparsity, and other advantageous topological characteristics. However, ReLU units are prone to becoming irreversibly inactive - a phenomenon known as the dying ReLU problem - which limits their overall effectiveness. In this work, we introduce surrogate gradient learning for ReLU (SUGAR) as a novel, plug-and-play regularizer for deep architectures. SUGAR preserves the standard ReLU function during the forward pass but replaces its derivative in the backward pass with a smooth surrogate that avoids zeroing out gradients. We demonstrate that SUGAR, when paired with a well-chosen surrogate function, substantially enhances generalization performance over convolutional network architectures such as VGG-16 and ResNet-18, providing sparser activations while effectively resurrecting dead ReLUs. Moreover, we show that even in modern architectures like Conv2NeXt and Swin Transformer - which typically employ GELU - substituting these with SUGAR yields competitive and even slightly superior performance. These findings challenge the prevailing notion that advanced activation functions are necessary for optimal performance. Instead, they suggest that the conventional ReLU, particularly with appropriate gradient handling, can serve as a strong, versatile revived classic across a broad range of deep learning vision models.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†åœ¨æ·±åº¦å­¦ä¹ æ¶æ„ä¸­é‡æ–°æ¿€æ´»ç»å…¸ ReLU çš„æ–¹æ³•ï¼Œä»¥åº”å¯¹å…¶é¢ä¸´çš„â€œç¥ç»å…ƒæ­»äº¡â€ (dying ReLU) é—®é¢˜ã€‚ä½œè€…æå‡ºäº† SUGAR (Surrogate Gradient Learning for ReLU)ï¼Œè¿™æ˜¯ä¸€ç§é’ˆå¯¹æ·±åº¦æ¶æ„çš„å³æ’å³ç”¨å‹æ­£åˆ™åŒ–å™¨ã€‚SUGAR åœ¨å‰å‘ä¼ æ’­ä¸­ä¿ç•™æ ‡å‡†çš„ ReLU å‡½æ•°ï¼Œä½†åœ¨åå‘ä¼ æ’­ä¸­åˆ©ç”¨å¹³æ»‘çš„æ›¿ä»£æ¢¯åº¦ (surrogate gradient) æ›¿æ¢å…¶å¯¼æ•°ï¼Œæœ‰æ•ˆé¿å…äº†æ¢¯åº¦å½’é›¶ã€‚å®éªŒè¡¨æ˜ï¼ŒSUGAR åœ¨ VGG-16 å’Œ ResNet-18 ç­‰å·ç§¯æ¶æ„ä¸Šæ˜¾è‘—æå‡äº†æ³›åŒ–æ€§èƒ½ï¼Œå¹¶åœ¨ç»´æŒç¨€ç–æ¿€æ´»çš„åŒæ—¶å¤æ´»äº†æ­»äº¡çš„ç¥ç»å…ƒã€‚æ­¤å¤–ï¼Œåœ¨ Conv2NeXt å’Œ Swin Transformer ç­‰ç°ä»£æ¶æ„ä¸­ï¼ŒSUGAR çš„è¡¨ç°ç”šè‡³ä¼˜äºå¸¸ç”¨çš„ GELU æ¿€æ´»å‡½æ•°ã€‚è¿™äº›å‘ç°æŒ‘æˆ˜äº†å¿…é¡»ä½¿ç”¨å¤æ‚æ¿€æ´»å‡½æ•°æ‰èƒ½è¾¾åˆ°æœ€ä¼˜æ€§èƒ½çš„è§‚ç‚¹ï¼Œè¯æ˜äº†ç»è¿‡é€‚å½“æ¢¯åº¦å¤„ç†çš„ç»å…¸ ReLU ä¾ç„¶æ˜¯è§†è§‰æ¨¡å‹ä¸­å¼ºæœ‰åŠ›ä¸”é€šç”¨çš„é€‰æ‹©ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.22074v1",
      "published_date": "2025-05-28 07:55:51 UTC",
      "updated_date": "2025-05-28 07:55:51 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T15:34:21.309466+00:00"
    },
    {
      "arxiv_id": "2505.22068v1",
      "title": "Beyond path selection: Better LLMs for Scientific Information Extraction with MimicSFT and Relevance and Rule-induced(R$^2$)GRPO",
      "title_zh": "è¶…è¶Šè·¯å¾„é€‰æ‹©ï¼šåŸºäº MimicSFT ä¸ç›¸å…³æ€§åŠè§„åˆ™è¯±å¯¼ï¼ˆR$^2$ï¼‰GRPO æå‡ç§‘å­¦ä¿¡æ¯æŠ½å–çš„å¤§è¯­è¨€æ¨¡å‹æ€§èƒ½",
      "authors": [
        "Ran Li",
        "Shimin Di",
        "Yuchen Liu",
        "Chen Jing",
        "Yu Qiu",
        "Lei Chen"
      ],
      "abstract": "Previous study suggest that powerful Large Language Models (LLMs) trained with Reinforcement Learning with Verifiable Rewards (RLVR) only refines reasoning path without improving the reasoning capacity in math tasks while supervised-finetuning(SFT) with distillation can. We study this from the view of Scientific information extraction (SciIE) where LLMs and reasoning LLMs underperforms small Bert-based models. SciIE require both the reasoning and memorization. We argue that both SFT and RLVR can refine the reasoning path and improve reasoning capacity in a simple way based on SciIE. We propose two-stage training with 1. MimicSFT, using structured reasoning templates without needing high-quality chain-of-thought data, 2. R$^2$GRPO with relevance and rule-induced rewards. Experiments on scientific IE benchmarks show that both methods can improve the reasoning capacity. R$^2$GRPO with mimicSFT surpasses baseline LLMs and specialized supervised models in relation extraction. Our code is available at https://github.com/ranlislz/R2GRPO.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ç§‘å­¦ä¿¡æ¯æå–ï¼ˆSciIEï¼‰é¢†åŸŸä¸­å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰è¡¨ç°é€Šäº Bert æ¨¡å‹çš„æƒ…å†µï¼Œæ¢è®¨äº†é€šè¿‡å¾®è°ƒå’Œå¼ºåŒ–å­¦ä¹ æå‡æ¨¡å‹æ¨ç†èƒ½åŠ›çš„æœ‰æ•ˆè·¯å¾„ã€‚ä½œè€…æå‡ºäº†ä¸¤é˜¶æ®µè®­ç»ƒæ–¹æ³•ï¼Œé¦–å…ˆé€šè¿‡ MimicSFT åˆ©ç”¨ç»“æ„åŒ–æ¨ç†æ¨¡æ¿è¿›è¡Œå¾®è°ƒï¼Œä»è€Œåœ¨æ— éœ€é«˜è´¨é‡é“¾å¼æ€ç»´ï¼ˆCoTï¼‰æ•°æ®çš„æƒ…å†µä¸‹ä¼˜åŒ–æ¨ç†è¿‡ç¨‹ã€‚éšåå¼•å…¥ç»“åˆç›¸å…³æ€§å’Œè§„åˆ™è¯±å¯¼ï¼ˆRelevance and Rule-inducedï¼‰å¥–åŠ±çš„ R$^2$GRPO ç®—æ³•ï¼Œæ—¨åœ¨åŒæ—¶ä¼˜åŒ–æ¨ç†è·¯å¾„å¹¶å¢å¼ºæ¨¡å‹çš„æ¨ç†ä¸è®°å¿†èƒ½åŠ›ã€‚å®éªŒè¯æ˜ï¼Œè¿™ä¸¤é¡¹æŠ€æœ¯å‡èƒ½æ˜¾è‘—æå‡æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ï¼Œè€Œéä»…ä»…æ˜¯ä¼˜åŒ–æ¨ç†è·¯å¾„ã€‚åœ¨ç§‘å­¦ä¿¡æ¯æå–åŸºå‡†æµ‹è¯•ä¸­ï¼ŒMimicSFT ä¸ R$^2$GRPO çš„ç»“åˆåœ¨å…³ç³»æŠ½å–ä»»åŠ¡ä¸Šè¶…è¶Šäº†åŸºçº¿ LLMs åŠä¸“é—¨çš„ç›‘ç£æ¨¡å‹ã€‚è¯¥ç ”ç©¶ä¸ºç§‘å­¦æ–‡çŒ®çš„è‡ªåŠ¨åŒ–åˆ†æå’Œ LLMs çš„æ¨ç†èƒ½åŠ›å¢å¼ºæä¾›äº†æ–°çš„æŠ€æœ¯æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.IR"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.22068v1",
      "published_date": "2025-05-28 07:47:46 UTC",
      "updated_date": "2025-05-28 07:47:46 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T15:34:06.647357+00:00"
    },
    {
      "arxiv_id": "2505.22067v1",
      "title": "From Failures to Fixes: LLM-Driven Scenario Repair for Self-Evolving Autonomous Driving",
      "title_zh": "ä»å¤±æ•ˆåˆ°ä¿®å¤ï¼šå¤§è¯­è¨€æ¨¡å‹é©±åŠ¨çš„è‡ªæ¼”è¿›è‡ªåŠ¨é©¾é©¶åœºæ™¯ä¿®å¤",
      "authors": [
        "Xinyu Xia",
        "Xingjun Ma",
        "Yunfeng Hu",
        "Ting Qu",
        "Hong Chen",
        "Xun Gong"
      ],
      "abstract": "Ensuring robust and generalizable autonomous driving requires not only broad scenario coverage but also efficient repair of failure cases, particularly those related to challenging and safety-critical scenarios. However, existing scenario generation and selection methods often lack adaptivity and semantic relevance, limiting their impact on performance improvement. In this paper, we propose \\textbf{SERA}, an LLM-powered framework that enables autonomous driving systems to self-evolve by repairing failure cases through targeted scenario recommendation. By analyzing performance logs, SERA identifies failure patterns and dynamically retrieves semantically aligned scenarios from a structured bank. An LLM-based reflection mechanism further refines these recommendations to maximize relevance and diversity. The selected scenarios are used for few-shot fine-tuning, enabling targeted adaptation with minimal data. Experiments on the benchmark show that SERA consistently improves key metrics across multiple autonomous driving baselines, demonstrating its effectiveness and generalizability under safety-critical conditions.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†SERAï¼Œä¸€ç§åŸºäºLLMé©±åŠ¨çš„æ¡†æ¶ï¼Œæ—¨åœ¨é€šè¿‡ç›®æ ‡åœºæ™¯ä¿®å¤å®ç°è‡ªåŠ¨é©¾é©¶ç³»ç»Ÿçš„è‡ªæˆ‘è¿›åŒ–(self-evolving)ã€‚é’ˆå¯¹ç°æœ‰åœºæ™¯ç”Ÿæˆæ–¹æ³•ç¼ºä¹è‡ªé€‚åº”æ€§å’Œè¯­ä¹‰ç›¸å…³æ€§çš„é—®é¢˜ï¼ŒSERAé€šè¿‡åˆ†ææ€§èƒ½æ—¥å¿—è¯†åˆ«å¤±æ•ˆæ¨¡å¼ï¼Œå¹¶ä»ç»“æ„åŒ–åº“ä¸­åŠ¨æ€æ£€ç´¢è¯­ä¹‰å¯¹é½çš„åœºæ™¯ã€‚è¯¥æ¡†æ¶å¼•å…¥äº†åŸºäºLLMçš„åå°„æœºåˆ¶(reflection mechanism)è¿›ä¸€æ­¥ä¼˜åŒ–æ¨èåœºæ™¯ï¼Œä»¥æœ€å¤§åŒ–ç›¸å…³æ€§å’Œå¤šæ ·æ€§ã€‚æ‰€é€‰åœºæ™¯è¢«ç”¨äºfew-shot fine-tuningï¼Œæ—¨åœ¨é€šè¿‡æå°‘é‡æ•°æ®å®ç°é’ˆå¯¹æ€§çš„æ¨¡å‹é€‚é…ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒSERAåœ¨å¤šä¸ªè‡ªåŠ¨é©¾é©¶åŸºå‡†æ¨¡å‹ä¸ŠæŒç»­æå‡äº†æ ¸å¿ƒæ€§èƒ½æŒ‡æ ‡ï¼ŒéªŒè¯äº†å…¶åœ¨å®‰å…¨å…³é”®(safety-critical)æ¡ä»¶ä¸‹çš„æœ‰æ•ˆæ€§ä¸æ³›åŒ–èƒ½åŠ›ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.RO"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.22067v1",
      "published_date": "2025-05-28 07:46:19 UTC",
      "updated_date": "2025-05-28 07:46:19 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T15:33:53.691765+00:00"
    },
    {
      "arxiv_id": "2505.22050v2",
      "title": "Reinforced Reasoning for Embodied Planning",
      "title_zh": "é¢å‘å…·èº«è§„åˆ’çš„å¼ºåŒ–æ¨ç†",
      "authors": [
        "Di Wu",
        "Jiaxin Fan",
        "Junzhe Zang",
        "Guanbo Wang",
        "Wei Yin",
        "Wenhao Li",
        "Bo Jin"
      ],
      "abstract": "Embodied planning requires agents to make coherent multi-step decisions based on dynamic visual observations and natural language goals. While recent vision-language models (VLMs) excel at static perception tasks, they struggle with the temporal reasoning, spatial understanding, and commonsense grounding needed for planning in interactive environments. In this work, we introduce a reinforcement fine-tuning framework that brings R1-style reasoning enhancement into embodied planning. We first distill a high-quality dataset from a powerful closed-source model and perform supervised fine-tuning (SFT) to equip the model with structured decision-making priors. We then design a rule-based reward function tailored to multi-step action quality and optimize the policy via Generalized Reinforced Preference Optimization (GRPO). Our approach is evaluated on Embench, a recent benchmark for interactive embodied tasks, covering both in-domain and out-of-domain scenarios. Experimental results show that our method significantly outperforms models of similar or larger scale, including GPT-4o-mini and 70B+ open-source baselines, and exhibits strong generalization to unseen environments. This work highlights the potential of reinforcement-driven reasoning to advance long-horizon planning in embodied AI.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å…·èº«è§„åˆ’(Embodied planning)ä¸­è§†è§‰è¯­è¨€æ¨¡å‹(VLMs)åœ¨æ—¶ç©ºæ¨ç†ã€å¸¸è¯†æ¥åœ°å’Œå¤šæ­¥å†³ç­–æ–¹é¢çš„å±€é™æ€§ï¼Œæå‡ºäº†ä¸€ç§å¼•å…¥R1å¼æ¨ç†å¢å¼ºæœºåˆ¶çš„å¼ºåŒ–å¾®è°ƒæ¡†æ¶ã€‚è¯¥æ–¹æ³•é¦–å…ˆä»é«˜æ€§èƒ½é—­æºæ¨¡å‹ä¸­è’¸é¦é«˜è´¨é‡æ•°æ®è¿›è¡Œæœ‰ç›‘ç£å¾®è°ƒ(SFT)ï¼Œä»¥èµ‹äºˆæ¨¡å‹ç»“æ„åŒ–çš„å†³ç­–å…ˆéªŒã€‚éšåï¼Œç ”ç©¶è€…è®¾è®¡äº†é’ˆå¯¹å¤šæ­¥åŠ¨ä½œè´¨é‡çš„è§„åˆ™å¥–åŠ±å‡½æ•°ï¼Œå¹¶é‡‡ç”¨å¹¿ä¹‰å¼ºåŒ–åå¥½ä¼˜åŒ–(GRPO)ç®—æ³•æ¥ä¼˜åŒ–æ¨¡å‹ç­–ç•¥ã€‚åœ¨EmbenchåŸºå‡†æµ‹è¯•çš„å®éªŒä¸­ï¼Œè¯¥æ–¹æ³•åœ¨åŸŸå†…å’ŒåŸŸå¤–åœºæ™¯ä¸‹å‡è¡¨ç°å‡ºè‰²ï¼Œæ€§èƒ½æ˜¾è‘—ä¼˜äºGPT-4o-miniåŠè§„æ¨¡æ›´å¤§çš„70B+å¼€æºåŸºå‡†æ¨¡å‹ã€‚ç»“æœè¯æ˜äº†è¯¥æ¡†æ¶å…·æœ‰æå¼ºçš„ç¯å¢ƒæ³›åŒ–èƒ½åŠ›ï¼Œå‡¸æ˜¾äº†å¼ºåŒ–é©±åŠ¨æ¨ç†åœ¨æ¨è¿›å…·èº«æ™ºèƒ½é•¿ç¨‹è§„åˆ’(Long-horizon planning)æ–¹é¢çš„å·¨å¤§æ½œåŠ›ã€‚",
      "categories": [
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.22050v2",
      "published_date": "2025-05-28 07:21:37 UTC",
      "updated_date": "2025-07-13 09:27:37 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T15:34:04.335580+00:00"
    },
    {
      "arxiv_id": "2505.22042v1",
      "title": "Estimating the Effects of Sample Training Orders for Large Language Models without Retraining",
      "title_zh": "æ— éœ€é‡è®­çš„å¤§è¯­è¨€æ¨¡å‹æ ·æœ¬è®­ç»ƒé¡ºåºå½±å“è¯„ä¼°",
      "authors": [
        "Hao Yang",
        "Haoxuan Li",
        "Mengyue Yang",
        "Xu Chen",
        "Mingming Gong"
      ],
      "abstract": "The order of training samples plays a crucial role in large language models (LLMs), significantly impacting both their external performance and internal learning dynamics. Traditional methods for investigating this effect generally require retraining the model with various sample orders, which is computationally infeasible for LLMs. In this work, we improve traditional methods by designing a retraining-free framework. By approximating Adam optimizer updates with first- and second-order Taylor expansions and utilizing random projection methods to store intermediate checkpoints, our framework can efficiently estimate model parameters for arbitrary training sample orders. Next, we apply our framework to two downstream research problems: (1) Training curriculum design for LLMs -- we base our retraining-free framework to propose a novel curriculum learning strategy that augments curriculum proposals with estimated model performances, enabling more informed sample scheduling. (2) LLMs' memorization and generalization effect analysis -- we use our retraining-free framework to estimate how the positions of training samples influence LLMs' capacity for memorization and generalization. We conduct extensive experiments to validate the effectiveness of our retraining-free framework in reproducing the true model performances, and further demonstrate its potential in optimizing LLM training curricula and analyzing the memorization and generalization effects of LLMs.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†å¤§è¯­è¨€æ¨¡å‹(LLMs)è®­ç»ƒæ ·æœ¬é¡ºåºå¯¹æ€§èƒ½å’Œå­¦ä¹ åŠ¨åŠ›å­¦çš„å½±å“ï¼Œå¹¶é’ˆå¯¹ä¼ ç»Ÿé‡è®­ç»ƒæ–¹æ³•è®¡ç®—æˆæœ¬è¿‡é«˜çš„é—®é¢˜ï¼Œå¼€å‘äº†ä¸€ä¸ªæ— éœ€é‡è®­ç»ƒ(retraining-free)çš„è¯„ä¼°æ¡†æ¶ã€‚è¯¥æ¡†æ¶åˆ©ç”¨ä¸€é˜¶å’ŒäºŒé˜¶æ³°å‹’å±•å¼€(Taylor expansions)æ¥è¿‘ä¼¼ Adam ä¼˜åŒ–å™¨çš„æ›´æ–°è¿‡ç¨‹ã€‚åŒæ—¶ï¼Œå®ƒç»“åˆéšæœºæŠ•å½±(random projection)æ–¹æ³•å­˜å‚¨ä¸­é—´æ£€æŸ¥ç‚¹ï¼Œä»è€Œèƒ½å¤Ÿé«˜æ•ˆä¼°è®¡ä»»æ„æ ·æœ¬è®­ç»ƒé¡ºåºä¸‹çš„æ¨¡å‹å‚æ•°ã€‚ç ”ç©¶è¿›ä¸€æ­¥å°†è¯¥æ¡†æ¶åº”ç”¨äºä¸‹æ¸¸ä»»åŠ¡ï¼Œæå‡ºäº†ä¸€ç§åˆ©ç”¨ä¼°è®¡æ€§èƒ½æŒ‡å¯¼æ ·æœ¬è°ƒåº¦çš„è¯¾ç¨‹å­¦ä¹ (curriculum learning)ç­–ç•¥ã€‚æ­¤å¤–ï¼Œè¯¥æ¡†æ¶è¿˜è¢«ç”¨äºåˆ†ææ ·æœ¬è®­ç»ƒä½ç½®å¯¹æ¨¡å‹è®°å¿†(memorization)ä¸æ³›åŒ–(generalization)æ•ˆåº”çš„å…·ä½“å½±å“ã€‚å¤šé¡¹å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ¡†æ¶èƒ½æœ‰æ•ˆé‡ç°çœŸå®çš„æ¨¡å‹æ€§èƒ½ã€‚å®ƒåœ¨ä¼˜åŒ–è®­ç»ƒè¯¾ç¨‹å’Œåˆ†ææ¨¡å‹å†…éƒ¨å­¦ä¹ æœºåˆ¶æ–¹é¢å±•ç°äº†æ˜¾è‘—çš„åº”ç”¨æ½œåŠ›ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.22042v1",
      "published_date": "2025-05-28 07:07:02 UTC",
      "updated_date": "2025-05-28 07:07:02 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T15:34:25.694111+00:00"
    },
    {
      "arxiv_id": "2505.22038v2",
      "title": "Balanced Token Pruning: Accelerating Vision Language Models Beyond Local Optimization",
      "title_zh": "Balanced Token Pruningï¼šçªç ´å±€éƒ¨ä¼˜åŒ–çš„è§†è§‰è¯­è¨€æ¨¡å‹åŠ é€Ÿæ–¹æ³•",
      "authors": [
        "Kaiyuan Li",
        "Xiaoyue Chen",
        "Chen Gao",
        "Yong Li",
        "Xinlei Chen"
      ],
      "abstract": "Large Vision-Language Models (LVLMs) have shown impressive performance across multi-modal tasks by encoding images into thousands of tokens. However, the large number of image tokens results in significant computational overhead, and the use of dynamic high-resolution inputs further increases this burden. Previous approaches have attempted to reduce the number of image tokens through token pruning, typically by selecting tokens based on attention scores or image token diversity. Through empirical studies, we observe that existing methods often overlook the joint impact of pruning on both the current layer's output (local) and the outputs of subsequent layers (global), leading to suboptimal pruning decisions. To address this challenge, we propose Balanced Token Pruning (BTP), a plug-and-play method for pruning vision tokens. Specifically, our method utilizes a small calibration set to divide the pruning process into multiple stages. In the early stages, our method emphasizes the impact of pruning on subsequent layers, whereas in the deeper stages, the focus shifts toward preserving the consistency of local outputs. Extensive experiments across various LVLMs demonstrate the broad effectiveness of our approach on multiple benchmarks. Our method achieves a 78% compression rate while preserving 96.7% of the original models' performance on average. Our code is available at https://github.com/EmbodiedCity/NeurIPS2025-Balanced-Token-Pruning.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤§è¯­è¨€è§†è§‰æ¨¡å‹(LVLMs)å› å›¾åƒTokenæ•°é‡åºå¤§å¯¼è‡´çš„æ˜¾è‘—è®¡ç®—å¼€é”€é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§åä¸ºBalanced Token Pruning (BTP)çš„å³æ’å³ç”¨è§†è§‰Tokenå‰ªææ–¹æ³•ã€‚ç ”ç©¶è€…å‘ç°ç°æœ‰çš„å‰ªææ‰‹æ®µå¾€å¾€ä»…å…³æ³¨å±€éƒ¨ä¼˜åŒ–è€Œå¿½è§†äº†å¯¹åç»­å±‚çš„å…¨å±€å½±å“ï¼Œå› æ­¤BTPé€šè¿‡å°å‹æ ¡å‡†é›†å°†å‰ªæè¿‡ç¨‹åˆ’åˆ†ä¸ºå¤šä¸ªé˜¶æ®µè¿›è¡Œå¹³è¡¡ã€‚åœ¨å‰ªææ—©æœŸé˜¶æ®µï¼Œè¯¥æ–¹æ³•é‡ç‚¹è¯„ä¼°å¯¹åç»­å±‚è¾“å‡ºçš„å½±å“ï¼Œè€Œåœ¨æ¨¡å‹æ·±å±‚é˜¶æ®µåˆ™è½¬è€Œä¾§é‡äºä¿æŒå±€éƒ¨è¾“å‡ºçš„ä¸€è‡´æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒBTPåœ¨å¤šç§LVLMså’ŒåŸºå‡†æµ‹è¯•ä¸­å±•ç°å‡ºæå¼ºçš„é€šç”¨æ€§ï¼Œèƒ½å¤Ÿåœ¨ä¿æŒåŸå§‹æ¨¡å‹å¹³å‡96.7%æ€§èƒ½çš„å‰æä¸‹ï¼Œå®ç°é«˜è¾¾78%çš„å‹ç¼©ç‡ã€‚è¿™ä¸€æ–¹æ¡ˆæœ‰æ•ˆè§£å†³äº†å‰ªæå†³ç­–ä¸­çš„å±€éƒ¨ä¸å…¨å±€ä¼˜åŒ–å†²çªï¼Œä¸ºåŠ é€Ÿå¤šæ¨¡æ€å¤§è§„æ¨¡æ¨¡å‹æ¨ç†æä¾›äº†é«˜æ•ˆä¸”å¯é çš„æŠ€æœ¯è·¯å¾„ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "Accepted by Neurips 2025",
      "pdf_url": "https://arxiv.org/pdf/2505.22038v2",
      "published_date": "2025-05-28 07:00:50 UTC",
      "updated_date": "2025-10-23 12:39:42 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T15:34:46.065613+00:00"
    },
    {
      "arxiv_id": "2505.22029v2",
      "title": "Analysis and Evaluation of Synthetic Data Generation in Speech Dysfluency Detection",
      "title_zh": "è¯­éŸ³ä¸æµåˆ©æ£€æµ‹ä¸­åˆæˆæ•°æ®ç”Ÿæˆçš„åˆ†æä¸è¯„ä¼°",
      "authors": [
        "Jinming Zhang",
        "Xuanru Zhou",
        "Jiachen Lian",
        "Shuhe Li",
        "William Li",
        "Zoe Ezzes",
        "Rian Bogley",
        "Lisa Wauters",
        "Zachary Miller",
        "Jet Vonk",
        "Brittany Morin",
        "Maria Gorno-Tempini",
        "Gopala Anumanchipalli"
      ],
      "abstract": "Speech dysfluency detection is crucial for clinical diagnosis and language assessment, but existing methods are limited by the scarcity of high-quality annotated data. Although recent advances in TTS model have enabled synthetic dysfluency generation, existing synthetic datasets suffer from unnatural prosody and limited contextual diversity. To address these limitations, we propose LLM-Dys -- the most comprehensive dysfluent speech corpus with LLM-enhanced dysfluency simulation. This dataset captures 11 dysfluency categories spanning both word and phoneme levels. Building upon this resource, we improve an end-to-end dysfluency detection framework. Experimental validation demonstrates state-of-the-art performance. All data, models, and code are open-sourced at https://github.com/Berkeley-Speech-Group/LLM-Dys.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹è¯­éŸ³å¤±è¯­æ£€æµ‹ï¼ˆSpeech dysfluency detectionï¼‰é¢†åŸŸä¸­é«˜è´¨é‡æ ‡æ³¨æ•°æ®ç¨€ç¼ºçš„é—®é¢˜ï¼ŒæŒ‡å‡ºç›®å‰çš„TTSåˆæˆæ•°æ®å­˜åœ¨éŸµå¾‹ä¸è‡ªç„¶å’Œå¤šæ ·æ€§ä¸è¶³çš„ç¼ºé™·ã€‚ä¸ºæ­¤ï¼Œä½œè€…æå‡ºäº†LLM-Dysï¼Œè¿™æ˜¯ä¸€ä¸ªåˆ©ç”¨LLMå¢å¼ºæ¨¡æ‹Ÿçš„å…¨é¢å¤±è¯­è¯­éŸ³è¯­æ–™åº“ï¼Œæ¶µç›–äº†å•è¯å’ŒéŸ³ç´ å±‚é¢çš„11ç§å¤±è¯­ç±»åˆ«ã€‚åŸºäºè¿™ä¸€æ–°èµ„æºï¼Œç ”ç©¶å›¢é˜Ÿæ”¹è¿›äº†ä¸€ä¸ªç«¯åˆ°ç«¯ï¼ˆend-to-endï¼‰çš„å¤±è¯­æ£€æµ‹æ¡†æ¶ã€‚å®éªŒéªŒè¯è¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å¤šé¡¹æŒ‡æ ‡ä¸Šè¾¾åˆ°äº†å½“å‰æœ€å…ˆè¿›çš„æ€§èƒ½ï¼ˆstate-of-the-artï¼‰ã€‚æœ€åï¼Œè¯¥ç ”ç©¶å°†æ‰€æœ‰æ•°æ®ã€æ¨¡å‹å’Œä»£ç è¿›è¡Œäº†å¼€æºï¼Œä¸ºä¸´åºŠè¯Šæ–­å’Œè¯­è¨€è¯„ä¼°æä¾›äº†é‡è¦çš„åŸºå‡†èµ„æºã€‚",
      "categories": [
        "eess.AS",
        "cs.AI",
        "cs.SD"
      ],
      "primary_category": "eess.AS",
      "comment": "Accepted by Interspeech 2025",
      "pdf_url": "https://arxiv.org/pdf/2505.22029v2",
      "published_date": "2025-05-28 06:52:10 UTC",
      "updated_date": "2025-06-22 18:20:44 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T15:35:06.691369+00:00"
    },
    {
      "arxiv_id": "2505.22027v1",
      "title": "Improving Respiratory Sound Classification with Architecture-Agnostic Knowledge Distillation from Ensembles",
      "title_zh": "åˆ©ç”¨é›†æˆæ¨¡å‹çš„æ¶æ„æ— å…³çŸ¥è¯†è’¸é¦æå‡å‘¼å¸éŸ³åˆ†ç±»",
      "authors": [
        "Miika Toikkanen",
        "June-Woo Kim"
      ],
      "abstract": "Respiratory sound datasets are limited in size and quality, making high performance difficult to achieve. Ensemble models help but inevitably increase compute cost at inference time. Soft label training distills knowledge efficiently with extra cost only at training. In this study, we explore soft labels for respiratory sound classification as an architecture-agnostic approach to distill an ensemble of teacher models into a student model. We examine different variations of our approach and find that even a single teacher, identical to the student, considerably improves performance beyond its own capability, with optimal gains achieved using only a few teachers. We achieve the new state-of-the-art Score of 64.39 on ICHBI, surpassing the previous best by 0.85 and improving average Scores across architectures by more than 1.16. Our results highlight the effectiveness of knowledge distillation with soft labels for respiratory sound classification, regardless of size or architecture.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†é€šè¿‡æ¶æ„æ— å…³çš„çŸ¥è¯†è’¸é¦ï¼ˆArchitecture-Agnostic Knowledge Distillationï¼‰æ–¹æ³•ï¼Œæ—¨åœ¨è§£å†³å‘¼å¸éŸ³åˆ†ç±»ï¼ˆRespiratory sound classificationï¼‰å› æ•°æ®é›†è§„æ¨¡å’Œè´¨é‡å—é™è€Œéš¾ä»¥æå‡æ€§èƒ½çš„é—®é¢˜ã€‚è®ºæ–‡æå‡ºåˆ©ç”¨è½¯æ ‡ç­¾ï¼ˆSoft labelï¼‰è®­ç»ƒæŠ€æœ¯ï¼Œå°†é›†æˆæ¨¡å‹ï¼ˆEnsemble modelsï¼‰çš„çŸ¥è¯†é«˜æ•ˆè¿ç§»è‡³å­¦ç”Ÿæ¨¡å‹ï¼Œä»è€Œåœ¨ä¸å¢åŠ æ¨ç†æˆæœ¬çš„å‰æä¸‹æå‡æ¨¡å‹ç²¾åº¦ã€‚ç ”ç©¶å‘ç°ï¼Œå³ä½¿é‡‡ç”¨ä¸å­¦ç”Ÿæ¨¡å‹å®Œå…¨ç›¸åŒçš„å•ä¸€æ•™å¸ˆæ¨¡å‹ï¼Œä¹Ÿèƒ½æ˜¾è‘—å¢å¼ºå…¶æ€§èƒ½ï¼Œä¸”ä»…éœ€å°‘é‡æ•™å¸ˆå³å¯è¾¾åˆ°æœ€ä¼˜æ•ˆæœã€‚åœ¨ICHBIæ•°æ®é›†çš„å®éªŒä¸­ï¼Œè¯¥æ–¹æ³•å–å¾—äº†64.39çš„æ–°SOTAå¾—åˆ†ï¼Œä¼˜äºæ­¤å‰çºªå½•0.85åˆ†ï¼Œå¹¶åœ¨å¤šç§æ¶æ„ä¸Šå®ç°äº†1.16ä»¥ä¸Šçš„å¹³å‡åˆ†æå‡ã€‚å®éªŒç»“æœå……åˆ†è¯æ˜äº†è½¯æ ‡ç­¾çŸ¥è¯†è’¸é¦åœ¨å‘¼å¸éŸ³åˆ†ç±»ä»»åŠ¡ä¸­çš„æœ‰æ•ˆæ€§ï¼Œä¸”è¯¥æ–¹æ¡ˆè¡¨ç°å‡ºæå¼ºçš„æ¶æ„æ— å…³æ€§ã€‚",
      "categories": [
        "cs.SD",
        "cs.AI",
        "eess.AS"
      ],
      "primary_category": "cs.SD",
      "comment": "Accepted to Interspeech 2025",
      "pdf_url": "https://arxiv.org/pdf/2505.22027v1",
      "published_date": "2025-05-28 06:49:18 UTC",
      "updated_date": "2025-05-28 06:49:18 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T15:34:46.410183+00:00"
    },
    {
      "arxiv_id": "2505.22021v2",
      "title": "GL-PGENet: A Parameterized Generation Framework for Robust Document Image Enhancement",
      "title_zh": "GL-PGENetï¼šä¸€ç§ç”¨äºé²æ£’æ–‡æ¡£å›¾åƒå¢å¼ºçš„å‚æ•°åŒ–ç”Ÿæˆæ¡†æ¶",
      "authors": [
        "Zhihong Tang"
      ],
      "abstract": "Document Image Enhancement (DIE) serves as a critical component in Document AI systems, where its performance substantially determines the effectiveness of downstream tasks. To address the limitations of existing methods confined to single-degradation restoration or grayscale image processing, we present Global with Local Parametric Generation Enhancement Network (GL-PGENet), a novel architecture designed for multi-degraded color document images, ensuring both efficiency and robustness in real-world scenarios. Our solution incorporates three key innovations: First, a hierarchical enhancement framework that integrates global appearance correction with local refinement, enabling coarse-to-fine quality improvement. Second, a Dual-Branch Local-Refine Network with parametric generation mechanisms that replaces conventional direct prediction, producing enhanced outputs through learned intermediate parametric representations rather than pixel-wise mapping. This approach enhances local consistency while improving model generalization. Finally, a modified NestUNet architecture incorporating dense block to effectively fuse low-level pixel features and high-level semantic features, specifically adapted for document image characteristics. In addition, to enhance generalization performance, we adopt a two-stage training strategy: large-scale pretraining on a synthetic dataset of 500,000+ samples followed by task-specific fine-tuning. Extensive experiments demonstrate the superiority of GL-PGENet, achieving state-of-the-art SSIM scores of 0.7721 on DocUNet and 0.9480 on RealDAE. The model also exhibits remarkable cross-domain adaptability and maintains computational efficiency for high-resolution images without performance degradation, confirming its practical utility in real-world scenarios.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ç°æœ‰ Document Image Enhancement (DIE) æ–¹æ³•å±€é™äºå•ç§é€€åŒ–ä¿®å¤æˆ–ç°åº¦å›¾åƒå¤„ç†çš„é—®é¢˜ï¼Œæå‡ºäº† GL-PGENetï¼Œè¿™æ˜¯ä¸€ç§ä¸“ä¸ºå¤šé€€åŒ–å½©è‰²æ–‡æ¡£å›¾åƒè®¾è®¡çš„å‚æ•°åŒ–ç”Ÿæˆå¢å¼ºæ¡†æ¶ã€‚è¯¥æ¡†æ¶é‡‡ç”¨åˆ†å±‚å¢å¼ºç»“æ„ï¼Œå°†å…¨å±€å¤–è§‚æ ¡æ­£ä¸å±€éƒ¨ç»†èŠ‚ä¼˜åŒ–ç›¸ç»“åˆï¼Œå®ç°äº†ç”±ç²—åˆ°ç²¾çš„å›¾åƒè´¨é‡æå‡ã€‚å…¶æ ¸å¿ƒåˆ›æ–°åœ¨äºå¼•å…¥äº†å…·æœ‰å‚æ•°åŒ–ç”Ÿæˆæœºåˆ¶çš„ Dual-Branch Local-Refine Networkï¼Œé€šè¿‡å­¦ä¹ ä¸­é—´å‚æ•°è¡¨ç¤ºè€Œéç›´æ¥åƒç´ æ˜ å°„æ¥ç”Ÿæˆå¢å¼ºç»“æœï¼Œä»è€Œæ˜¾è‘—æå‡äº†å±€éƒ¨ä¸€è‡´æ€§å’Œæ¨¡å‹çš„ Generalization èƒ½åŠ›ã€‚æ­¤å¤–ï¼Œç ”ç©¶è€…é€šè¿‡æ”¹è¿› NestUNet æ¶æ„å¹¶å¼•å…¥ dense blockï¼Œæœ‰æ•ˆèåˆäº†ä½çº§åƒç´ ç‰¹å¾ä¸é«˜çº§è¯­ä¹‰ç‰¹å¾ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒGL-PGENet åœ¨ DocUNet å’Œ RealDAE æ•°æ®é›†ä¸Šå‡å–å¾—äº† SOTA çš„ SSIM è¯„åˆ†ï¼Œåˆ†åˆ«ä¸º 0.7721 å’Œ 0.9480ã€‚è¯¥æ¨¡å‹åœ¨å¤„ç†é«˜åˆ†è¾¨ç‡å›¾åƒæ—¶ä¿æŒäº†æé«˜çš„è®¡ç®—æ•ˆç‡ï¼Œå¹¶åœ¨å®é™…åº”ç”¨åœºæ™¯ä¸­å±•ç°å‡ºå“è¶Šçš„è·¨é¢†åŸŸé€‚åº”æ€§å’Œç¨³å¥æ€§ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "12 pages, 7 figures",
      "pdf_url": "https://arxiv.org/pdf/2505.22021v2",
      "published_date": "2025-05-28 06:37:06 UTC",
      "updated_date": "2025-10-09 06:16:02 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T15:34:53.504701+00:00"
    },
    {
      "arxiv_id": "2505.22019v2",
      "title": "VRAG-RL: Empower Vision-Perception-Based RAG for Visually Rich Information Understanding via Iterative Reasoning with Reinforcement Learning",
      "title_zh": "VRAG-RLï¼šé€šè¿‡å¼ºåŒ–å­¦ä¹ è¿­ä»£æ¨ç†èµ‹èƒ½åŸºäºè§†è§‰æ„ŸçŸ¥çš„ RAGï¼Œå®ç°è§†è§‰ä¸°å¯Œä¿¡æ¯ç†è§£",
      "authors": [
        "Qiuchen Wang",
        "Ruixue Ding",
        "Yu Zeng",
        "Zehui Chen",
        "Lin Chen",
        "Shihang Wang",
        "Pengjun Xie",
        "Fei Huang",
        "Feng Zhao"
      ],
      "abstract": "Effectively retrieving, reasoning and understanding visually rich information remains a challenge for RAG methods. Traditional text-based methods cannot handle visual-related information. On the other hand, current vision-based RAG approaches are often limited by fixed pipelines and frequently struggle to reason effectively due to the insufficient activation of the fundamental capabilities of models. As RL has been proven to be beneficial for model reasoning, we introduce VRAG-RL, a novel RL framework tailored for complex reasoning across visually rich information. With this framework, VLMs interact with search engines, autonomously sampling single-turn or multi-turn reasoning trajectories with the help of visual perception tokens and undergoing continual optimization based on these samples. Our approach highlights key limitations of RL in RAG domains: (i) Prior Multi-modal RAG approaches tend to merely incorporate images into the context, leading to insufficient reasoning token allocation and neglecting visual-specific perception; and (ii) When models interact with search engines, their queries often fail to retrieve relevant information due to the inability to articulate requirements, thereby leading to suboptimal performance. To address these challenges, we define an action space tailored for visually rich inputs, with actions including cropping and scaling, allowing the model to gather information from a coarse-to-fine perspective. Furthermore, to bridge the gap between users' original inquiries and the retriever, we employ a simple yet effective reward that integrates query rewriting and retrieval performance with a model-based reward. Our VRAG-RL optimizes VLMs for RAG tasks using specially designed RL strategies, aligning the model with real-world applications. The code is available at https://github.com/Alibaba-NLP/VRAG.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† VRAG-RLï¼Œä¸€ç§ä¸“ä¸ºå¤„ç†è§†è§‰ä¸°å¯Œä¿¡æ¯ç†è§£è€Œè®¾è®¡çš„å¼ºåŒ–å­¦ä¹  (Reinforcement Learning) æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³ç°æœ‰è§†è§‰æ£€ç´¢å¢å¼ºç”Ÿæˆ (Vision-based RAG) æ–¹æ³•åœ¨å¤æ‚æ¨ç†å’Œè§†è§‰æ„ŸçŸ¥æ–¹é¢çš„å±€é™ã€‚è¯¥æ¡†æ¶å…è®¸è§†è§‰è¯­è¨€æ¨¡å‹ (VLMs) é€šè¿‡è§†è§‰æ„ŸçŸ¥æ ‡è®° (visual perception tokens) ä¸æœç´¢å¼•æ“äº¤äº’ï¼Œè‡ªä¸»é‡‡æ ·å•è½®æˆ–å¤šè½®æ¨ç†è·¯å¾„å¹¶è¿›è¡ŒæŒç»­ä¼˜åŒ–ã€‚é’ˆå¯¹å¤šæ¨¡æ€ RAG ä¸­æ¨ç†æ ‡è®°åˆ†é…ä¸è¶³å’Œæ£€ç´¢æŸ¥è¯¢è¡¨è¿°èƒ½åŠ›å·®çš„é—®é¢˜ï¼ŒVRAG-RL å®šä¹‰äº†ä¸“é—¨é’ˆå¯¹è§†è§‰è¾“å…¥çš„åŠ¨ä½œç©ºé—´ï¼ŒåŒ…æ‹¬è£å‰ªå’Œç¼©æ”¾ç­‰æ“ä½œï¼Œå®ç°äº†ä»ç²—åˆ°ç»†çš„ä¿¡æ¯è·å–è¿‡ç¨‹ã€‚åŒæ—¶ï¼Œç ”ç©¶é‡‡ç”¨äº†ä¸€ç§ç»“åˆæŸ¥è¯¢é‡å†™ä¸æ£€ç´¢æ€§èƒ½çš„å¥–åŠ±æœºåˆ¶ï¼Œæœ‰æ•ˆå¼¥è¡¥äº†ç”¨æˆ·åŸå§‹æŸ¥è¯¢ä¸æ£€ç´¢å™¨ä¹‹é—´çš„é¸¿æ²Ÿã€‚é€šè¿‡ç‰¹å®šçš„å¼ºåŒ–å­¦ä¹ ç­–ç•¥ï¼ŒVRAG-RL æ˜¾è‘—ä¼˜åŒ–äº† VLM åœ¨ RAG ä»»åŠ¡ä¸­çš„è¡¨ç°ï¼Œä½¿å…¶åœ¨å¤„ç†å¤æ‚è§†è§‰ä¿¡æ¯æ—¶æ›´å…·é²æ£’æ€§å¹¶è´´åˆå®é™…åº”ç”¨åœºæ™¯ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.22019v2",
      "published_date": "2025-05-28 06:30:51 UTC",
      "updated_date": "2025-06-03 05:28:55 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T15:35:07.308835+00:00"
    },
    {
      "arxiv_id": "2505.22006v1",
      "title": "Efficiently Enhancing General Agents With Hierarchical-categorical Memory",
      "title_zh": "åˆ©ç”¨åˆ†å±‚åˆ†ç±»è®°å¿†é«˜æ•ˆå¢å¼ºé€šç”¨æ™ºèƒ½ä½“",
      "authors": [
        "Changze Qiao",
        "Mingming Lu"
      ],
      "abstract": "With large language models (LLMs) demonstrating remarkable capabilities, there has been a surge in research on leveraging LLMs to build general-purpose multi-modal agents. However, existing approaches either rely on computationally expensive end-to-end training using large-scale multi-modal data or adopt tool-use methods that lack the ability to continuously learn and adapt to new environments. In this paper, we introduce EHC, a general agent capable of learning without parameter updates. EHC consists of a Hierarchical Memory Retrieval (HMR) module and a Task-Category Oriented Experience Learning (TOEL) module. The HMR module facilitates rapid retrieval of relevant memories and continuously stores new information without being constrained by memory capacity. The TOEL module enhances the agent's comprehension of various task characteristics by classifying experiences and extracting patterns across different categories. Extensive experiments conducted on multiple standard datasets demonstrate that EHC outperforms existing methods, achieving state-of-the-art performance and underscoring its effectiveness as a general agent for handling complex multi-modal tasks.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†EHCï¼Œä¸€ç§æ— éœ€å‚æ•°æ›´æ–°å³å¯å®ç°æŒç»­å­¦ä¹ çš„é€šç”¨å¤šæ¨¡æ€æ™ºèƒ½ä½“æ¡†æ¶ã€‚EHCç”±Hierarchical Memory Retrieval (HMR)æ¨¡å—å’ŒTask-Category Oriented Experience Learning (TOEL)æ¨¡å—ç»„æˆã€‚HMRæ¨¡å—èƒ½å¤Ÿåœ¨ä¸å—è®°å¿†å®¹é‡é™åˆ¶çš„æƒ…å†µä¸‹ï¼Œå®ç°ç›¸å…³è®°å¿†çš„å¿«é€Ÿæ£€ç´¢ä¸æ–°ä¿¡æ¯çš„æŒç»­å­˜å‚¨ã€‚TOELæ¨¡å—é€šè¿‡å¯¹ç»éªŒè¿›è¡Œåˆ†ç±»å¹¶æå–è·¨ç±»åˆ«çš„æ¨¡å¼ï¼Œæ˜¾è‘—å¢å¼ºäº†æ™ºèƒ½ä½“å¯¹ä¸åŒä»»åŠ¡ç‰¹å¾çš„ç†è§£èƒ½åŠ›ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒEHCåœ¨å¤šä¸ªæ ‡å‡†æ•°æ®é›†ä¸Šå‡ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œè¾¾åˆ°äº†State-of-the-artæ€§èƒ½ã€‚è¿™é¡¹ç ”ç©¶ä¸ä»…è¯æ˜äº†EHCå¤„ç†å¤æ‚å¤šæ¨¡æ€ä»»åŠ¡çš„æœ‰æ•ˆæ€§ï¼Œä¹Ÿä¸ºæ„å»ºé«˜æ•ˆã€å¯è¿›åŒ–çš„é€šç”¨æ™ºèƒ½ä½“æä¾›äº†é‡è¦å‚è€ƒã€‚",
      "categories": [
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.22006v1",
      "published_date": "2025-05-28 06:12:51 UTC",
      "updated_date": "2025-05-28 06:12:51 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T15:35:01.892993+00:00"
    },
    {
      "arxiv_id": "2507.19489v1",
      "title": "MAIA: A Collaborative Medical AI Platform for Integrated Healthcare Innovation",
      "title_zh": "MAIAï¼šé¢å‘é›†æˆåŒ–åŒ»ç–—åˆ›æ–°çš„åä½œå¼åŒ»ç–—äººå·¥æ™ºèƒ½å¹³å°",
      "authors": [
        "Simone Bendazzoli",
        "Sanna Persson",
        "Mehdi Astaraki",
        "Sebastian Pettersson",
        "Vitali Grozman",
        "Rodrigo Moreno"
      ],
      "abstract": "The integration of Artificial Intelligence (AI) into clinical workflows requires robust collaborative platforms that are able to bridge the gap between technical innovation and practical healthcare applications. This paper introduces MAIA (Medical Artificial Intelligence Assistant), an open-source platform designed to facilitate interdisciplinary collaboration among clinicians, researchers, and AI developers. Built on Kubernetes, MAIA offers a modular, scalable environment with integrated tools for data management, model development, annotation, deployment, and clinical feedback. Key features include project isolation, CI/CD automation, integration with high-computing infrastructures and in clinical workflows. MAIA supports real-world use cases in medical imaging AI, with deployments in both academic and clinical environments. By promoting collaborations and interoperability, MAIA aims to accelerate the translation of AI research into impactful clinical solutions while promoting reproducibility, transparency, and user-centered design. We showcase the use of MAIA with different projects, both at KTH Royal Institute of Technology and Karolinska University Hospital.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¨å‡ºäº†å¼€æºå¹³å° MAIA (Medical Artificial Intelligence Assistant)ï¼Œæ—¨åœ¨ä¿ƒè¿›ä¸´åºŠåŒ»ç”Ÿã€ç ”ç©¶äººå‘˜å’Œ AI å¼€å‘äººå‘˜ä¹‹é—´çš„è·¨å­¦ç§‘åä½œï¼Œå¼¥åˆåŒ»ç–— AI æŠ€æœ¯åˆ›æ–°ä¸å®é™…åº”ç”¨ä¹‹é—´çš„é¸¿æ²Ÿã€‚MAIA åŸºäº Kubernetes æ„å»ºï¼Œæä¾›äº†ä¸€ä¸ªæ¨¡å—åŒ–ä¸”å¯æ‰©å±•çš„ç¯å¢ƒï¼Œé›†æˆäº†æ•°æ®ç®¡ç†ã€æ¨¡å‹å¼€å‘ã€æ•°æ®æ ‡æ³¨ã€æ¨¡å‹éƒ¨ç½²ä»¥åŠä¸´åºŠåé¦ˆç­‰æ ¸å¿ƒåŠŸèƒ½ã€‚å…¶å…³é”®ç‰¹æ€§æ¶µç›–äº†é¡¹ç›®éš”ç¦»ã€CI/CD è‡ªåŠ¨åŒ–ï¼Œä»¥åŠä¸é«˜æ€§èƒ½è®¡ç®—(high-computing)åŸºç¡€è®¾æ–½å’Œä¸´åºŠå·¥ä½œæµçš„æ·±åº¦é›†æˆã€‚ç›®å‰è¯¥å¹³å°å·²åœ¨ç‘å…¸ KTH çš‡å®¶ç†å·¥å­¦é™¢å’Œå¡ç½—æ—æ–¯å¡å¤§å­¦åŒ»é™¢(Karolinska University Hospital)æˆåŠŸéƒ¨ç½²ï¼Œå¹¶æ”¯æŒäº†å¤šä¸ªåŒ»å­¦å½±åƒ AI çš„å®é™…åº”ç”¨æ¡ˆä¾‹ã€‚é€šè¿‡å¢å¼ºå¤šå­¦ç§‘åä½œä¸äº’æ“ä½œæ€§(interoperability)ï¼ŒMAIA æ—¨åœ¨æå‡åŒ»ç–— AI ç ”ç©¶çš„å¯é‡å¤æ€§å’Œé€æ˜åº¦ï¼Œä»è€ŒåŠ é€Ÿç§‘ç ”æˆæœå‘å…·æœ‰ä¸´åºŠå½±å“åŠ›çš„è§£å†³æ–¹æ¡ˆè½¬åŒ–ã€‚",
      "categories": [
        "cs.AI",
        "cs.CV",
        "cs.HC",
        "cs.SE"
      ],
      "primary_category": "cs.AI",
      "comment": "26 pages, 12 figures",
      "pdf_url": "https://arxiv.org/pdf/2507.19489v1",
      "published_date": "2025-05-28 06:06:57 UTC",
      "updated_date": "2025-05-28 06:06:57 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T15:35:15.044070+00:00"
    },
    {
      "arxiv_id": "2505.22003v1",
      "title": "Legal Assist AI: Leveraging Transformer-Based Model for Effective Legal Assistance",
      "title_zh": "Legal Assist AIï¼šåˆ©ç”¨ Transformer æ¨¡å‹å®ç°é«˜æ•ˆæ³•å¾‹è¾…åŠ©",
      "authors": [
        "Jatin Gupta",
        "Akhil Sharma",
        "Saransh Singhania",
        "Ali Imam Abidi"
      ],
      "abstract": "Pursuit of accessible legal assistance in India faces a critical gap, as many citizens struggle to leverage their legal rights due to limited awareness and access to relevant legal information. This paper introduces Legal Assist AI, a transformer-based model designed to bridge this gap by offering effective legal assistance through large language models (LLMs). The system retrieves relevant legal information from a curated database and generates accurate responses, enabling effective assistance for diverse users, including legal professionals, scholars, and the general public. The model was fine-tuned on extensive datasets from the Indian legal domain, including Indian Constitution, Bharatiya Nyaya Sanhita (BNS), Bharatiya Nagarik Suraksha Sanhita (BNSS) and so forth, providing a robust understanding of the complexities of Indian law. By incorporating domain-specific legal datasets, the proposed model demonstrated remarkable efficiency and specialization in legal Question-Answering. The model was evaluated against state-of-the-art models such as GPT-3.5 Turbo and Mistral 7B, achieving a 60.08% score on the AIBE, outperforming its competitors in legal reasoning and accuracy. Unlike other models, Legal Assist AI avoided common issues such as hallucinations, making it highly reliable for practical legal applications. It showcases the model's applicability in real-world legal scenarios, with future iterations aiming to enhance performance and expand its dataset to cover a broader range of multilingual and case-specific queries as well.",
      "tldr_zh": "è¿™é¡¹ç ”ç©¶é’ˆå¯¹å°åº¦æ³•å¾‹æ´åŠ©æ™®åŠåº¦ä¸è¶³çš„é—®é¢˜ï¼Œæå‡ºäº† Legal Assist AIï¼Œè¿™æ˜¯ä¸€ç§åŸºäº Transformer æ¶æ„çš„å¤§è¯­è¨€æ¨¡å‹ (LLMs)ï¼Œæ—¨åœ¨æä¾›æœ‰æ•ˆçš„æ³•å¾‹ååŠ©ã€‚è¯¥ç³»ç»Ÿé€šè¿‡ä»æ•°æ®åº“ä¸­æ£€ç´¢ç›¸å…³æ³•å¾‹ä¿¡æ¯å¹¶ç”Ÿæˆå‡†ç¡®å›å¤ï¼Œèƒ½å¤Ÿä¸ºæ³•å¾‹ä¸“ä¸šäººå£«ã€å­¦è€…åŠæ™®é€šå¤§ä¼—æä¾›æœ‰åŠ›æ”¯æŒã€‚æ¨¡å‹åœ¨åŒ…æ‹¬ Indian Constitutionã€Bharatiya Nyaya Sanhita (BNS) åŠ Bharatiya Nagarik Suraksha Sanhita (BNSS) åœ¨å†…çš„å¹¿æ³›å°åº¦æ³•å¾‹æ•°æ®é›†ä¸Šè¿›è¡Œäº†å¾®è°ƒ (Fine-tuning)ï¼Œä½¿å…¶å…·å¤‡äº†å¯¹å°åº¦æ³•å¾‹å¤æ‚æ€§çš„æ·±åˆ»ç†è§£ã€‚å®éªŒè¯„ä¼°è¡¨æ˜ï¼ŒLegal Assist AI åœ¨å…¨å°åº¦å¾‹å¸ˆèµ„æ ¼è€ƒè¯• (AIBE) ä¸­å–å¾—äº† 60.08% çš„æˆç»©ï¼Œåœ¨æ³•å¾‹æ¨ç† (Legal Reasoning) å’Œå‡†ç¡®æ€§ä¸Šè¶…è¶Šäº† GPT-3.5 Turbo å’Œ Mistral 7Bã€‚æ­¤å¤–ï¼Œè¯¥æ¨¡å‹æœ‰æ•ˆé¿å…äº†å¤§å‹æ¨¡å‹ä¸­å¸¸è§çš„å¹»è§‰ (Hallucinations) é—®é¢˜ï¼Œæ˜¾è‘—æå‡äº†åœ¨å®é™…æ³•å¾‹åº”ç”¨åœºæ™¯ä¸­çš„å¯é æ€§ã€‚ç ”ç©¶å±•ç¤ºäº†è¯¥ç³»ç»Ÿåœ¨ç°å®ä¸–ç•Œä¸­çš„åº”ç”¨æ½œåŠ›ï¼Œæœªæ¥è®¡åˆ’è¿›ä¸€æ­¥æ‰©å±•æ•°æ®é›†ä»¥è¦†ç›–æ›´å¹¿æ³›çš„å¤šè¯­è¨€å’Œå…·ä½“æ¡ˆä¾‹æŸ¥è¯¢ (Queries)ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "9 pages, 5 tables, 4 figures. This is a revised version of a preprint previously available at this URL: https://doi.org/10.21203/rs.3.rs-5351879/v1",
      "pdf_url": "https://arxiv.org/pdf/2505.22003v1",
      "published_date": "2025-05-28 06:06:53 UTC",
      "updated_date": "2025-05-28 06:06:53 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T15:35:20.904876+00:00"
    },
    {
      "arxiv_id": "2505.21996v2",
      "title": "Learning World Models for Interactive Video Generation",
      "title_zh": "é¢å‘äº¤äº’å¼è§†é¢‘ç”Ÿæˆçš„ä¸–ç•Œæ¨¡å‹å­¦ä¹ ",
      "authors": [
        "Taiye Chen",
        "Xun Hu",
        "Zihan Ding",
        "Chi Jin"
      ],
      "abstract": "Foundational world models must be both interactive and preserve spatiotemporal coherence for effective future planning with action choices. However, present models for long video generation have limited inherent world modeling capabilities due to two main challenges: compounding errors and insufficient memory mechanisms. We enhance image-to-video models with interactive capabilities through additional action conditioning and autoregressive framework, and reveal that compounding error is inherently irreducible in autoregressive video generation, while insufficient memory mechanism leads to incoherence of world models. We propose video retrieval augmented generation (VRAG) with explicit global state conditioning, which significantly reduces long-term compounding errors and increases spatiotemporal consistency of world models. In contrast, naive autoregressive generation with extended context windows and retrieval-augmented generation prove less effective for video generation, primarily due to the limited in-context learning capabilities of current video models. Our work illuminates the fundamental challenges in video world models and establishes a comprehensive benchmark for improving video generation models with internal world modeling capabilities.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†ç”¨äºäº¤äº’å¼è§†é¢‘ç”Ÿæˆçš„ä¸–ç•Œæ¨¡å‹(World Models)æ„å»ºï¼ŒæŒ‡å‡ºå½“å‰é•¿è§†é¢‘æ¨¡å‹åœ¨å¤„ç†å¤åˆè¯¯å·®(compounding errors)å’Œè®°å¿†æœºåˆ¶ä¸è¶³æ–¹é¢å­˜åœ¨æ ¸å¿ƒæŒ‘æˆ˜ã€‚ä½œè€…é€šè¿‡å¼•å…¥åŠ¨ä½œè°ƒèŠ‚(action conditioning)å’Œè‡ªå›å½’æ¡†æ¶å¢å¼ºäº†æ¨¡å‹çš„äº¤äº’èƒ½åŠ›ï¼Œå¹¶æ­ç¤ºäº†è‡ªå›å½’è§†é¢‘ç”Ÿæˆä¸­å¤åˆè¯¯å·®çš„ä¸å¯è¿˜åŸæ€§ã€‚ä¸ºè§£å†³ä¸€è‡´æ€§é—®é¢˜ï¼Œç ”ç©¶æå‡ºäº†è§†é¢‘æ£€ç´¢å¢å¼ºç”Ÿæˆ(Video Retrieval Augmented Generation, VRAG)ï¼Œåˆ©ç”¨æ˜¾å¼çš„å…¨å±€çŠ¶æ€è°ƒèŠ‚æ˜¾è‘—é™ä½äº†é•¿æœŸå¤åˆè¯¯å·®å¹¶æå‡äº†æ—¶ç©ºä¸€è‡´æ€§(spatiotemporal consistency)ã€‚å®éªŒå¯¹æ¯”å‘ç°ï¼Œä¼ ç»Ÿçš„æ‰©å±•ä¸Šä¸‹æ–‡çª—å£æˆ–æ™®é€šRAGåœ¨è§†é¢‘ç”Ÿæˆä¸­æ•ˆæœå—é™ï¼Œä¸»è¦å½’å› äºå½“å‰æ¨¡å‹æœ‰é™çš„ä¸Šä¸‹æ–‡å­¦ä¹ (in-context learning)èƒ½åŠ›ã€‚è¯¥å·¥ä½œä¸ä»…é˜æ˜äº†è§†é¢‘ä¸–ç•Œæ¨¡å‹çš„åŸºç¡€æŒ‘æˆ˜ï¼Œè¿˜å»ºç«‹äº†ä¸€ä¸ªç»¼åˆåŸºå‡†(benchmark)ï¼Œä¸ºå¢å¼ºè§†é¢‘ç”Ÿæˆæ¨¡å‹çš„å†…éƒ¨ä¸–ç•Œå»ºæ¨¡èƒ½åŠ›æä¾›äº†é‡è¦å‚è€ƒã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "Project page: https://sites.google.com/view/vrag",
      "pdf_url": "https://arxiv.org/pdf/2505.21996v2",
      "published_date": "2025-05-28 05:55:44 UTC",
      "updated_date": "2025-10-29 22:39:29 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T15:35:23.358405+00:00"
    },
    {
      "arxiv_id": "2505.21988v2",
      "title": "Functional Matching of Logic Subgraphs: Beyond Structural Isomorphism",
      "title_zh": "é€»è¾‘å­å›¾åŠŸèƒ½åŒ¹é…ï¼šè¶…è¶Šç»“æ„åŒæ„",
      "authors": [
        "Ziyang Zheng",
        "Kezhi Li",
        "Zhengyuan Shi",
        "Qiang Xu"
      ],
      "abstract": "Subgraph matching in logic circuits is foundational for numerous Electronic Design Automation (EDA) applications, including datapath optimization, arithmetic verification, and hardware trojan detection. However, existing techniques rely primarily on structural graph isomorphism and thus fail to identify function-related subgraphs when synthesis transformations substantially alter circuit topology. To overcome this critical limitation, we introduce the concept of functional subgraph matching, a novel approach that identifies whether a given logic function is implicitly present within a larger circuit, irrespective of structural variations induced by synthesis or technology mapping. Specifically, we propose a two-stage multi-modal framework: (1) learning robust functional embeddings across AIG and post-mapping netlists for functional subgraph detection, and (2) identifying fuzzy boundaries using a graph segmentation approach. Evaluations on standard benchmarks (ITC99, OpenABCD, ForgeEDA) demonstrate significant performance improvements over existing structural methods, with average $93.8\\%$ accuracy in functional subgraph detection and a dice score of $91.3\\%$ in fuzzy boundary identification. The source code and implementation details can be found at https://github.com/zyzheng17/Functional_Subgraph_Matching-Neurips25.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹é€»è¾‘ç”µè·¯ä¸­å­å›¾åŒ¹é…é«˜åº¦ä¾èµ–ç»“æ„å›¾åŒæ„(structural graph isomorphism)è€Œæ— æ³•å¤„ç†ç»¼åˆå˜æ¢å¯¼è‡´æ‹“æ‰‘å˜åŒ–çš„å±€é™æ€§ï¼Œæå‡ºäº†åŠŸèƒ½å­å›¾åŒ¹é…(functional subgraph matching)çš„æ–°æ¦‚å¿µã€‚ç ”ç©¶å›¢é˜Ÿå¼€å‘äº†ä¸€ä¸ªä¸¤é˜¶æ®µå¤šæ¨¡æ€æ¡†æ¶ï¼Œé¦–å…ˆå­¦ä¹ è·¨ AIG å’Œæ˜ å°„åç½‘è¡¨(post-mapping netlists)çš„é²æ£’åŠŸèƒ½åµŒå…¥ä»¥å®ç°åŠŸèƒ½å­å›¾æ£€æµ‹ï¼Œéšååˆ©ç”¨å›¾åˆ†å‰²(graph segmentation)æ–¹æ³•ç²¾å‡†è¯†åˆ«æ¨¡ç³Šè¾¹ç•Œã€‚åœ¨ ITC99ã€OpenABCD å’Œ ForgeEDA ç­‰æ ‡å‡†åŸºå‡†ä¸Šçš„è¯„ä¼°ç»“æœæ˜¾ç¤ºï¼Œè¯¥æ¡†æ¶åœ¨åŠŸèƒ½å­å›¾æ£€æµ‹ä¸­è¾¾åˆ°äº† 93.8% çš„å¹³å‡å‡†ç¡®ç‡ã€‚æ­¤å¤–ï¼Œåœ¨æ¨¡ç³Šè¾¹ç•Œè¯†åˆ«ä»»åŠ¡ä¸­å–å¾—äº† 91.3% çš„ Dice Scoreï¼Œæ€§èƒ½æ˜¾è‘—ä¼˜äºä¼ ç»Ÿçš„ç»“æ„åŒ–æ–¹æ³•ã€‚è¯¥ç ”ç©¶ä¸º EDA é¢†åŸŸä¸­çš„æ•°æ®é€šè·¯ä¼˜åŒ–ã€ç®—æœ¯æ ¡éªŒåŠç¡¬ä»¶æœ¨é©¬æ£€æµ‹æä¾›äº†æ›´å…·é²æ£’æ€§çš„æŠ€æœ¯æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.21988v2",
      "published_date": "2025-05-28 05:31:49 UTC",
      "updated_date": "2025-10-08 06:16:46 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T15:35:27.401597+00:00"
    },
    {
      "arxiv_id": "2505.21985v1",
      "title": "Reward-Independent Messaging for Decentralized Multi-Agent Reinforcement Learning",
      "title_zh": "å»ä¸­å¿ƒåŒ–å¤šæ™ºèƒ½ä½“å¼ºåŒ–å­¦ä¹ ä¸­çš„å¥–åŠ±æ— å…³æ¶ˆæ¯é€šä¿¡",
      "authors": [
        "Naoto Yoshida",
        "Tadahiro Taniguchi"
      ],
      "abstract": "In multi-agent reinforcement learning (MARL), effective communication improves agent performance, particularly under partial observability. We propose MARL-CPC, a framework that enables communication among fully decentralized, independent agents without parameter sharing. MARL-CPC incorporates a message learning model based on collective predictive coding (CPC) from emergent communication research. Unlike conventional methods that treat messages as part of the action space and assume cooperation, MARL-CPC links messages to state inference, supporting communication in non-cooperative, reward-independent settings. We introduce two algorithms -Bandit-CPC and IPPO-CPC- and evaluate them in non-cooperative MARL tasks. Benchmarks show that both outperform standard message-as-action approaches, establishing effective communication even when messages offer no direct benefit to the sender. These results highlight MARL-CPC's potential for enabling coordination in complex, decentralized environments.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å»ä¸­å¿ƒåŒ–å¤šæ™ºèƒ½ä½“å¼ºåŒ–å­¦ä¹  (Decentralized Multi-Agent Reinforcement Learning, MARL) åœ¨éƒ¨åˆ†å¯è§‚æµ‹ç¯å¢ƒä¸‹é€šè¿‡é€šä¿¡æå‡æ€§èƒ½çš„æŒ‘æˆ˜ï¼Œæå‡ºäº† MARL-CPC æ¡†æ¶ã€‚è¯¥æ¡†æ¶å…è®¸å®Œå…¨å»ä¸­å¿ƒåŒ–ä¸”æ— å‚æ•°å…±äº«çš„ç‹¬ç«‹æ™ºèƒ½ä½“åˆ©ç”¨åŸºäºé›†ä½“é¢„æµ‹ç¼–ç  (Collective Predictive Coding, CPC) çš„æ¨¡å‹è¿›è¡Œæ¶ˆæ¯å­¦ä¹ ã€‚ä¸å°†æ¶ˆæ¯è§†ä¸ºåŠ¨ä½œç©ºé—´çš„ä¼ ç»Ÿæ–¹æ³•ä¸åŒï¼ŒMARL-CPC å°†æ¶ˆæ¯ä¸çŠ¶æ€æ¨ç† (state inference) æŒ‚é’©ï¼Œæ”¯æŒåœ¨éåˆä½œå’Œå¥–åŠ±ç‹¬ç«‹ (reward-independent) ç¯å¢ƒä¸‹çš„é€šä¿¡ã€‚ç ”ç©¶å›¢é˜Ÿå¼€å‘äº† Bandit-CPC å’Œ IPPO-CPC ä¸¤ç§ç®—æ³•ï¼Œå¹¶åœ¨éåˆä½œä»»åŠ¡ä¸­è¿›è¡Œäº†éªŒè¯ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•ä¼˜äºæ ‡å‡†çš„â€œæ¶ˆæ¯å³åŠ¨ä½œâ€ (message-as-action) æ–¹æ¡ˆï¼Œå³ä½¿åœ¨å‘é€è€…æ— æ³•ç›´æ¥è·ç›Šçš„æƒ…å†µä¸‹ä¹Ÿèƒ½å®ç°æœ‰æ•ˆé€šä¿¡ã€‚è¿™ä¸€æˆæœå±•ç¤ºäº† MARL-CPC åœ¨å¤æ‚å»ä¸­å¿ƒåŒ–ç¯å¢ƒä¸­ä¿ƒè¿›å¤šæ™ºèƒ½ä½“åè°ƒçš„æ½œåŠ›ã€‚",
      "categories": [
        "cs.MA",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.MA",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.21985v1",
      "published_date": "2025-05-28 05:23:47 UTC",
      "updated_date": "2025-05-28 05:23:47 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T15:36:10.971566+00:00"
    },
    {
      "arxiv_id": "2505.21981v1",
      "title": "Learning Compositional Behaviors from Demonstration and Language",
      "title_zh": "åŸºäºæ¼”ç¤ºä¸è¯­è¨€çš„ç»„åˆå¼è¡Œä¸ºå­¦ä¹ ",
      "authors": [
        "Weiyu Liu",
        "Neil Nie",
        "Ruohan Zhang",
        "Jiayuan Mao",
        "Jiajun Wu"
      ],
      "abstract": "We introduce Behavior from Language and Demonstration (BLADE), a framework for long-horizon robotic manipulation by integrating imitation learning and model-based planning. BLADE leverages language-annotated demonstrations, extracts abstract action knowledge from large language models (LLMs), and constructs a library of structured, high-level action representations. These representations include preconditions and effects grounded in visual perception for each high-level action, along with corresponding controllers implemented as neural network-based policies. BLADE can recover such structured representations automatically, without manually labeled states or symbolic definitions. BLADE shows significant capabilities in generalizing to novel situations, including novel initial states, external state perturbations, and novel goals. We validate the effectiveness of our approach both in simulation and on real robots with a diverse set of objects with articulated parts, partial observability, and geometric constraints.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† BLADEï¼ˆBehavior from Language and Demonstrationï¼‰æ¡†æ¶ï¼Œé€šè¿‡æ•´åˆæ¨¡ä»¿å­¦ä¹ ï¼ˆimitation learningï¼‰ä¸åŸºäºæ¨¡å‹çš„è§„åˆ’ï¼ˆmodel-based planningï¼‰ï¼Œæ—¨åœ¨å®ç°é•¿ç¨‹æœºå™¨äººæ“ä½œï¼ˆlong-horizon robotic manipulationï¼‰ã€‚BLADE åˆ©ç”¨å¸¦æœ‰è¯­è¨€æ ‡æ³¨çš„æ¼”ç¤ºæ•°æ®ï¼Œç»“åˆå¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰æå–æŠ½è±¡åŠ¨ä½œçŸ¥è¯†ï¼Œå¹¶æ„å»ºäº†ä¸€ä¸ªåŒ…å«ç»“æ„åŒ–é«˜å±‚åŠ¨ä½œè¡¨ç¤ºçš„åº“ã€‚è¿™äº›è¡¨ç¤ºæ¶µç›–äº†åŸºäºè§†è§‰æ„ŸçŸ¥çš„åŠ¨ä½œå‰æï¼ˆpreconditionsï¼‰ä¸æ•ˆæœï¼ˆeffectsï¼‰ï¼Œå¹¶é…å¤‡äº†ç”±ç¥ç»ç½‘ç»œç­–ç•¥å®ç°çš„ç›¸åº”æ§åˆ¶å™¨ï¼ˆcontrollersï¼‰ã€‚è¯¥æ¡†æ¶èƒ½å¤Ÿåœ¨æ— éœ€æ‰‹åŠ¨æ ‡è®°çŠ¶æ€æˆ–ç¬¦å·å®šä¹‰çš„æƒ…å†µä¸‹è‡ªåŠ¨æ¢å¤ä¸Šè¿°ç»“æ„åŒ–è¡¨ç¤ºï¼Œæ˜¾è‘—æå‡äº†ç³»ç»Ÿçš„è‡ªä¸»æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒBLADE åœ¨åº”å¯¹æ–°åˆå§‹çŠ¶æ€ã€å¤–éƒ¨æ‰°åŠ¨ä»¥åŠæ–°ç›®æ ‡æ–¹é¢å±•ç°å‡ºæå¼ºçš„æ³›åŒ–èƒ½åŠ›ï¼ˆgeneralizationï¼‰ã€‚ç ”ç©¶äººå‘˜åœ¨ä»¿çœŸç¯å¢ƒå’ŒçœŸå®æœºå™¨äººä¸ŠéªŒè¯äº†è¯¥æ–¹æ³•çš„æœ‰æ•ˆæ€§ï¼Œæ¶µç›–äº†å…·æœ‰å…³èŠ‚éƒ¨ä»¶ã€éƒ¨åˆ†å¯è§‚æµ‹æ€§ï¼ˆpartial observabilityï¼‰å’Œå‡ ä½•çº¦æŸçš„å¤šç§å¤æ‚å¯¹è±¡ã€‚",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.CL",
        "cs.CV"
      ],
      "primary_category": "cs.RO",
      "comment": "Presented at CoRL 2024 and as an Oral Presentation at the 2024 CoRL LEAP Workshop. The first two authors contributed equally. The last two authors jointly advised the project. For videos and additional results, visit: https://blade-bot.github.io/",
      "pdf_url": "https://arxiv.org/pdf/2505.21981v1",
      "published_date": "2025-05-28 05:19:59 UTC",
      "updated_date": "2025-05-28 05:19:59 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T15:35:44.749118+00:00"
    },
    {
      "arxiv_id": "2505.21972v2",
      "title": "LLMs Judging LLMs: A Simplex Perspective",
      "title_zh": "LLM è¯„æµ‹ LLMï¼šå•çº¯å½¢è§†è§’",
      "authors": [
        "Patrick Vossler",
        "Fan Xia",
        "Yifan Mai",
        "Adarsh Subbaswamy",
        "Jean Feng"
      ],
      "abstract": "Given the challenge of automatically evaluating free-form outputs from large language models (LLMs), an increasingly common solution is to use LLMs themselves as the judging mechanism, without any gold-standard scores. Implicitly, this practice accounts for only sampling variability (aleatoric uncertainty) and ignores uncertainty about judge quality (epistemic uncertainty). While this is justified if judges are perfectly accurate, it is unclear when such an approach is theoretically valid and practically robust. We study these questions for the task of ranking LLM candidates from a novel geometric perspective: for $M$-level scoring systems, both LLM judges and candidates can be represented as points on an $(M-1)$-dimensional probability simplex, where geometric concepts (e.g., triangle areas) correspond to key ranking concepts. This perspective yields intuitive theoretical conditions and visual proofs for when rankings are identifiable; for instance, we provide a formal basis for the ``folk wisdom'' that LLM judges are more effective for two-level scoring ($M=2$) than multi-level scoring ($M>2$). Leveraging the simplex, we design geometric Bayesian priors that encode epistemic uncertainty about judge quality and vary the priors to conduct sensitivity analyses. Experiments on LLM benchmarks show that rankings based solely on LLM judges are robust in many but not all datasets, underscoring both their widespread success and the need for caution. Our Bayesian method achieves substantially higher coverage rates than existing procedures, highlighting the importance of modeling epistemic uncertainty.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹(LLMs)ä½œä¸ºè¯„åˆ¤æœºåˆ¶æ¥è‡ªåŠ¨è¯„ä¼°è‡ªç”±æ ¼å¼è¾“å‡ºçš„æŒ‘æˆ˜ï¼ŒæŒ‡å‡ºå½“å‰å®è·µå¾€å¾€å¿½ç•¥äº†è¯„åˆ¤è´¨é‡çš„è®¤è¯†ä¸ç¡®å®šæ€§(epistemic uncertainty)ã€‚ç ”ç©¶æå‡ºäº†ä¸€ç§æ–°é¢–çš„å‡ ä½•è§†è§’ï¼Œå°† $M$ çº§è¯„åˆ†ç³»ç»Ÿä¸‹çš„ LLM è¯„åˆ¤è€…å’Œå€™é€‰æ¨¡å‹è¡¨ç¤ºä¸º $(M-1)$ ç»´æ¦‚ç‡å•çº¯å½¢(probability simplex)ä¸Šçš„ç‚¹ï¼Œä½¿æ’åæ¦‚å¿µä¸å‡ ä½•æ¦‚å¿µç›¸å¯¹åº”ã€‚è¯¥è§†è§’ä¸ºæ’åå¯è¯†åˆ«æ€§æä¾›äº†ç†è®ºä¾æ®ï¼Œå¹¶ä»å½¢å¼ä¸Šè§£é‡Šäº† LLM è¯„åˆ¤è€…åœ¨ä¸¤çº§è¯„åˆ†ä¸­æ¯”å¤šçº§è¯„åˆ†æ›´æœ‰æ•ˆçš„ç°è±¡ã€‚åŸºäºæ­¤å•çº¯å½¢è§†è§’ï¼Œç ”ç©¶è®¾è®¡äº†ç”¨äºæ•æ‰è®¤è¯†ä¸ç¡®å®šæ€§çš„å‡ ä½•è´å¶æ–¯å…ˆéªŒ(Bayesian priors)å¹¶è¿›è¡Œäº†æ•æ„Ÿæ€§åˆ†æã€‚å®éªŒè¡¨æ˜ï¼Œè™½ç„¶å•çº¯ä¾é  LLM è¯„åˆ¤çš„æ’ååœ¨å¤šé¡¹åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ç¨³å¥ï¼Œä½†å»ºæ¨¡è®¤è¯†ä¸ç¡®å®šæ€§åï¼Œè¯¥è´å¶æ–¯æ–¹æ³•æ˜¾è‘—æé«˜äº†è¦†ç›–ç‡ã€‚è¯¥ç ”ç©¶ä¸ä»…æ­ç¤ºäº† LLM è¯„åˆ¤æœºåˆ¶çš„ç†è®ºæœ‰æ•ˆæ€§ï¼Œä¹Ÿå¼ºè°ƒäº†åœ¨è‡ªåŠ¨è¯„ä¼°è¿‡ç¨‹ä¸­å¯¹è¯„åˆ¤è€…è´¨é‡è¿›è¡Œä¸ç¡®å®šæ€§å»ºæ¨¡çš„å¿…è¦æ€§ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "stat.ML"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.21972v2",
      "published_date": "2025-05-28 04:50:41 UTC",
      "updated_date": "2025-12-05 22:04:39 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T15:35:53.491590+00:00"
    },
    {
      "arxiv_id": "2505.21969v4",
      "title": "DORAEMON: Decentralized Ontology-aware Reliable Agent with Enhanced Memory Oriented Navigation",
      "title_zh": "DORAEMONï¼šå…·æœ‰å¢å¼ºå‹è®°å¿†å¯¼å‘å¯¼èˆªçš„å»ä¸­å¿ƒåŒ–æœ¬ä½“æ„ŸçŸ¥å¯é æ™ºèƒ½ä½“",
      "authors": [
        "Tianjun Gu",
        "Linfeng Li",
        "Xuhong Wang",
        "Chenghua Gong",
        "Jingyu Gong",
        "Zhizhong Zhang",
        "Yuan Xie",
        "Lizhuang Ma",
        "Xin Tan"
      ],
      "abstract": "Adaptive navigation in unfamiliar environments is crucial for household service robots but remains challenging due to the need for both low-level path planning and high-level scene understanding. While recent vision-language model (VLM) based zero-shot approaches reduce dependence on prior maps and scene-specific training data, they face significant limitations: spatiotemporal discontinuity from discrete observations, unstructured memory representations, and insufficient task understanding leading to navigation failures. We propose DORAEMON (Decentralized Ontology-aware Reliable Agent with Enhanced Memory Oriented Navigation), a novel cognitive-inspired framework consisting of Ventral and Dorsal Streams that mimics human navigation capabilities. The Dorsal Stream implements the Hierarchical Semantic-Spatial Fusion and Topology Map to handle spatiotemporal discontinuities, while the Ventral Stream combines RAG-VLM and Policy-VLM to improve decision-making. Our approach also develops Nav-Ensurance to ensure navigation safety and efficiency. We evaluate DORAEMON on the HM3D, MP3D, and GOAT datasets, where it achieves state-of-the-art performance on both success rate (SR) and success weighted by path length (SPL) metrics, significantly outperforming existing methods. We also introduce a new evaluation metric (AORI) to assess navigation intelligence better. Comprehensive experiments demonstrate DORAEMON's effectiveness in zero-shot autonomous navigation without requiring prior map building or pre-training.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†DORAEMONï¼Œä¸€ç§å—è®¤çŸ¥å¯å‘çš„å»ä¸­å¿ƒåŒ–æœ¬ä½“æ„ŸçŸ¥å¯¼èˆªæ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³å®¶åº­æœåŠ¡æœºå™¨äººåœ¨é™Œç”Ÿç¯å¢ƒä¸­é¢ä¸´çš„è·¯å¾„è§„åˆ’ä¸åœºæ™¯ç†è§£éš¾é¢˜ã€‚é’ˆå¯¹ç°æœ‰Vision-Language Model (VLM) é›¶æ ·æœ¬æ–¹æ³•åœ¨æ—¶ç©ºè¿ç»­æ€§ã€è®°å¿†è¡¨ç¤ºå’Œä»»åŠ¡ç†è§£æ–¹é¢çš„å±€é™ï¼Œè¯¥æ¡†æ¶æ¨¡ä»¿äººç±»è®¤çŸ¥è®¾è®¡äº†Dorsal Streamå’ŒVentral Streamç³»ç»Ÿã€‚Dorsal Streamé€šè¿‡Hierarchical Semantic-Spatial Fusionå’ŒTopology Mapå¤„ç†ç¦»æ•£è§‚æµ‹å¸¦æ¥çš„ä¸è¿ç»­æ€§ï¼Œè€ŒVentral Streamç»“åˆRAG-VLMä¸Policy-VLMä¼˜åŒ–å¤æ‚å†³ç­–ã€‚æ­¤å¤–ï¼Œç ”ç©¶å¼•å…¥äº†Nav-Ensuranceæœºåˆ¶ä»¥ä¿éšœå¯¼èˆªçš„å®‰å…¨ä¸æ•ˆç‡ï¼Œå¹¶æå‡ºäº†è¯„ä¼°å¯¼èˆªæ™ºèƒ½çš„æ–°æŒ‡æ ‡AORIã€‚åœ¨HM3Dã€MP3Då’ŒGOATæ•°æ®é›†ä¸Šçš„å®éªŒæ˜¾ç¤ºï¼ŒDORAEMONåœ¨Success Rate (SR)å’ŒSPLæŒ‡æ ‡ä¸Šå‡è¾¾åˆ°äº†State-of-the-artæ€§èƒ½ï¼Œæ˜¾è‘—è¶…è¶Šäº†ç°æœ‰æ–¹æ³•ã€‚è¯¥æ¡†æ¶æœ‰æ•ˆè¯æ˜äº†åœ¨æ— éœ€å…ˆéªŒåœ°å›¾æˆ–ç‰¹å®šé¢„è®­ç»ƒçš„æƒ…å†µä¸‹ï¼Œæœºå™¨äººä»èƒ½å®ç°é«˜æ•ˆçš„é›¶æ ·æœ¬è‡ªä¸»å¯¼èˆªã€‚",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.21969v4",
      "published_date": "2025-05-28 04:46:13 UTC",
      "updated_date": "2025-09-25 15:34:10 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T15:36:50.217644+00:00"
    },
    {
      "arxiv_id": "2505.21966v2",
      "title": "MapStory: Prototyping Editable Map Animations with LLM Agents",
      "title_zh": "MapStoryï¼šåŸºäº LLM æ™ºèƒ½ä½“çš„å¯ç¼–è¾‘åœ°å›¾åŠ¨ç”»åŸå‹è®¾è®¡",
      "authors": [
        "Aditya Gunturu",
        "Ben Pearman",
        "Keiichi Ihara",
        "Morteza Faraji",
        "Bryan Wang",
        "Rubaiat Habib Kazi",
        "Ryo Suzuki"
      ],
      "abstract": "We introduce MapStory, an LLM-powered animation prototyping tool that generates editable map animation sequences directly from natural language text by leveraging a dual-agent LLM architecture. Given a user written script, MapStory automatically produces a scene breakdown, which decomposes the text into key map animation primitives such as camera movements, visual highlights, and animated elements. Our system includes a researcher agent that accurately queries geospatial information by leveraging an LLM with web search, enabling automatic extraction of relevant regions, paths, and coordinates while allowing users to edit and query for changes or additional information to refine the results. Additionally, users can fine-tune parameters of these primitive blocks through an interactive timeline editor. We detail the system's design and architecture, informed by formative interviews with professional animators and by an analysis of 200 existing map animation videos. Our evaluation, which includes expert interviews (N=5) and a usability study (N=12), demonstrates that MapStory enables users to create map animations with ease, facilitates faster iteration, encourages creative exploration, and lowers barriers to creating map-centric stories.",
      "tldr_zh": "è¯¥ç ”ç©¶ä»‹ç»äº† MapStoryï¼Œè¿™æ˜¯ä¸€æ¬¾ç”± LLM é©±åŠ¨çš„åŠ¨ç”»åŸå‹è®¾è®¡å·¥å…·ï¼Œé€šè¿‡åŒæ™ºèƒ½ä½“æ¶æ„ (dual-agent LLM architecture) å®ç°ä»è‡ªç„¶è¯­è¨€æ–‡æœ¬åˆ°å¯ç¼–è¾‘åœ°å›¾åŠ¨ç”»åºåˆ—çš„ç›´æ¥ç”Ÿæˆã€‚ç³»ç»Ÿèƒ½å¤Ÿè‡ªåŠ¨è¿›è¡Œåœºæ™¯åˆ†è§£ï¼Œå°†å‰§æœ¬è½¬åŒ–ä¸ºæ‘„åƒæœºç§»åŠ¨ (camera movements) å’Œè§†è§‰é«˜äº® (visual highlights) ç­‰æ ¸å¿ƒåœ°å›¾åŠ¨ç”»åŸè¯­ (animation primitives)ï¼Œå¹¶åˆ©ç”¨å…·å¤‡è”ç½‘æœç´¢èƒ½åŠ›çš„è°ƒç ”æ™ºèƒ½ä½“ (researcher agent) è‡ªåŠ¨æå–åœ°ç†ç©ºé—´åæ ‡å’Œè·¯å¾„ã€‚ç”¨æˆ·è¿˜å¯ä»¥é€šè¿‡äº¤äº’å¼æ—¶é—´è½´ç¼–è¾‘å™¨ (timeline editor) å¯¹åŠ¨ç”»å‚æ•°è¿›è¡Œç²¾ç»†åŒ–è°ƒæ•´ã€‚è¯¥ç³»ç»Ÿçš„è®¾è®¡åŸºäºå¯¹ä¸“ä¸šåŠ¨ç”»å¸ˆçš„è®¿è°ˆå’Œå¤§é‡è§†é¢‘åˆ†æï¼Œå…·æœ‰æé«˜çš„å®è·µæŒ‡å¯¼æ„ä¹‰ã€‚è¯„ä¼°ç»“æœæ˜¾ç¤ºï¼ŒMapStory æ˜¾è‘—é™ä½äº†åœ°å›¾å™äº‹çš„é—¨æ§›ï¼Œä¸ä»…è®©åŠ¨ç”»åˆ¶ä½œå˜å¾—æ›´åŠ ç®€å•ï¼Œè¿˜å¤§å¹…æå‡äº†åˆ›ä½œè¿­ä»£çš„é€Ÿåº¦å’Œåˆ›æ„æ¢ç´¢çš„è‡ªç”±åº¦ã€‚",
      "categories": [
        "cs.HC",
        "cs.AI",
        "cs.CL",
        "cs.MM"
      ],
      "primary_category": "cs.HC",
      "comment": "UIST 2025. Project page: https://adigunturu.github.io/MapStory-UIST25/",
      "pdf_url": "https://arxiv.org/pdf/2505.21966v2",
      "published_date": "2025-05-28 04:36:08 UTC",
      "updated_date": "2025-08-13 08:00:47 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T15:36:35.260826+00:00"
    },
    {
      "arxiv_id": "2505.21963v1",
      "title": "LaMDAgent: An Autonomous Framework for Post-Training Pipeline Optimization via LLM Agents",
      "title_zh": "LaMDAgentï¼šåŸºäºå¤§è¯­è¨€æ¨¡å‹æ™ºèƒ½ä½“çš„åè®­ç»ƒæµæ°´çº¿è‡ªä¸»ä¼˜åŒ–æ¡†æ¶",
      "authors": [
        "Taro Yano",
        "Yoichi Ishibashi",
        "Masafumi Oyamada"
      ],
      "abstract": "Large Language Models (LLMs) have demonstrated exceptional performance across a wide range of tasks. To further tailor LLMs to specific domains or applications, post-training techniques such as Supervised Fine-Tuning (SFT), Preference Learning, and model merging are commonly employed. While each of these methods has been extensively studied in isolation, the automated construction of complete post-training pipelines remains an underexplored area. Existing approaches typically rely on manual design or focus narrowly on optimizing individual components, such as data ordering or merging strategies. In this work, we introduce LaMDAgent (short for Language Model Developing Agent), a novel framework that autonomously constructs and optimizes full post-training pipelines through the use of LLM-based agents. LaMDAgent systematically explores diverse model generation techniques, datasets, and hyperparameter configurations, leveraging task-based feedback to discover high-performing pipelines with minimal human intervention. Our experiments show that LaMDAgent improves tool-use accuracy by 9.0 points while preserving instruction-following capabilities. Moreover, it uncovers effective post-training strategies that are often overlooked by conventional human-driven exploration. We further analyze the impact of data and model size scaling to reduce computational costs on the exploration, finding that model size scalings introduces new challenges, whereas scaling data size enables cost-effective pipeline discovery.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†LaMDAgentï¼Œä¸€ä¸ªåˆ©ç”¨LLMæ™ºèƒ½ä½“è‡ªä¸»æ„å»ºå’Œä¼˜åŒ–å…¨æµç¨‹è®­ç»ƒåæµæ°´çº¿(post-training pipelines)çš„åˆ›æ–°æ¡†æ¶ã€‚é’ˆå¯¹Supervised Fine-Tuning (SFT)ã€Preference Learningå’Œæ¨¡å‹åˆå¹¶(model merging)ç­‰æŠ€æœ¯åœ¨è‡ªåŠ¨åŒ–æ•´åˆæ–¹é¢çš„ç©ºç™½ï¼ŒLaMDAgentèƒ½å¤Ÿç³»ç»Ÿåœ°æ¢ç´¢æ¨¡å‹ç”ŸæˆæŠ€æœ¯ã€æ•°æ®é›†åŠè¶…å‚æ•°é…ç½®ã€‚è¯¥æ¡†æ¶åˆ©ç”¨åŸºäºä»»åŠ¡çš„åé¦ˆæœºåˆ¶ï¼Œåœ¨æå°‘äººå·¥å¹²é¢„ä¸‹å‘ç°é«˜æ€§èƒ½æµæ°´çº¿ï¼Œæ˜¾è‘—æå‡äº†æ¨¡å‹æ€§èƒ½ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒLaMDAgentåœ¨ä¿æŒæŒ‡ä»¤éµå¾ª(instruction-following)èƒ½åŠ›çš„åŒæ—¶ï¼Œå°†å·¥å…·ä½¿ç”¨(tool-use)å‡†ç¡®ç‡æé«˜äº†9.0åˆ†ã€‚æ­¤å¤–ï¼Œç ”ç©¶é€šè¿‡åˆ†æç¼©æ”¾æ•ˆåº”å‘ç°ï¼Œå¢åŠ æ•°æ®è§„æ¨¡(data size scaling)æ˜¯å®ç°ä½æˆæœ¬æ¢ç´¢çš„æœ‰æ•ˆæ–¹å¼ï¼Œå¹¶èƒ½æŒ–æ˜å‡ºä¼ ç»Ÿäººå·¥è®¾è®¡ä¸­å¸¸è¢«å¿½è§†çš„ä¼˜åŒ–ç­–ç•¥ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.21963v1",
      "published_date": "2025-05-28 04:30:51 UTC",
      "updated_date": "2025-05-28 04:30:51 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T15:36:13.106338+00:00"
    },
    {
      "arxiv_id": "2505.21956v3",
      "title": "Cross-modal RAG: Sub-dimensional Text-to-Image Retrieval-Augmented Generation",
      "title_zh": "Cross-modal RAGï¼šåŸºäºå­ç»´åº¦çš„æ–‡ç”Ÿå›¾æ£€ç´¢å¢å¼ºç”Ÿæˆ",
      "authors": [
        "Mengdan Zhu",
        "Senhao Cheng",
        "Guangji Bai",
        "Yifei Zhang",
        "Liang Zhao"
      ],
      "abstract": "Text-to-image generation increasingly demands access to domain-specific, fine-grained, and rapidly evolving knowledge that pretrained models cannot fully capture, necessitating the integration of retrieval methods. Existing Retrieval-Augmented Generation (RAG) methods attempt to address this by retrieving globally relevant images, but they fail when no single image contains all desired elements from a complex user query. We propose Cross-modal RAG, a novel framework that decomposes both queries and images into sub-dimensional components, enabling subquery-aware retrieval and generation. Our method introduces a hybrid retrieval strategy - combining a sub-dimensional sparse retriever with a dense retriever - to identify a Pareto-optimal set of images, each contributing complementary aspects of the query. During generation, a multimodal large language model is guided to selectively condition on relevant visual features aligned to specific subqueries, ensuring subquery-aware image synthesis. Extensive experiments on MS-COCO, Flickr30K, WikiArt, CUB, and ImageNet-LT demonstrate that Cross-modal RAG significantly outperforms existing baselines in the retrieval and further contributes to generation quality, while maintaining high efficiency.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†Cross-modal RAGæ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³é¢„è®­ç»ƒText-to-Imageç”Ÿæˆæ¨¡å‹åœ¨å¤„ç†å¤æ‚æŸ¥è¯¢æ—¶ï¼Œå› å•ä¸€æ£€ç´¢å›¾åƒéš¾ä»¥è¦†ç›–æ‰€æœ‰ç»†ç²’åº¦å…ƒç´ è€Œå¯¼è‡´çš„å±€é™æ€§ã€‚è¯¥æ–¹æ³•é€šè¿‡å°†æŸ¥è¯¢å’Œå›¾åƒåˆ†è§£ä¸ºå­ç»´åº¦ç»„ä»¶(sub-dimensional components)ï¼Œå¼•å…¥äº†ä¸€ç§ç»“åˆå­ç»´åº¦ç¨€ç–æ£€ç´¢å™¨(sparse retriever)ä¸ç¨ å¯†æ£€ç´¢å™¨(dense retriever)çš„æ··åˆæ£€ç´¢ç­–ç•¥ï¼Œä»¥è·å–äº’è¡¥çš„å¸•ç´¯æ‰˜æœ€ä¼˜(Pareto-optimal)å›¾åƒé›†ã€‚åœ¨ç”Ÿæˆé˜¶æ®µï¼Œæ¡†æ¶å¼•å¯¼å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹æ ¹æ®ç‰¹å®šå­æŸ¥è¯¢é€‰æ‹©æ€§åœ°è°ƒèŠ‚è§†è§‰ç‰¹å¾ï¼Œä»è€Œå®ç°ç²¾å‡†çš„å­æŸ¥è¯¢æ„ŸçŸ¥å›¾åƒåˆæˆã€‚åœ¨MS-COCOã€Flickr30Kã€WikiArtç­‰æ•°æ®é›†ä¸Šçš„å®éªŒè¯æ˜ï¼ŒCross-modal RAGåœ¨æ£€ç´¢æ€§èƒ½å’Œç”Ÿæˆè´¨é‡ä¸Šå‡æ˜¾è‘—ä¼˜äºç°æœ‰åŸºçº¿æ¨¡å‹ï¼Œå¹¶ä¿æŒäº†è¾ƒé«˜çš„è¿è¡Œæ•ˆç‡ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CL",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.21956v3",
      "published_date": "2025-05-28 04:09:49 UTC",
      "updated_date": "2025-09-26 20:48:32 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T15:36:10.304660+00:00"
    },
    {
      "arxiv_id": "2505.21955v2",
      "title": "Towards Comprehensive Scene Understanding: Integrating First and Third-Person Views for LVLMs",
      "title_zh": "è¿ˆå‘å…¨é¢çš„åœºæ™¯ç†è§£ï¼šé¢å‘ LVLM çš„ç¬¬ä¸€äººç§°ä¸ç¬¬ä¸‰äººç§°è§†è§’èåˆ",
      "authors": [
        "Insu Lee",
        "Wooje Park",
        "Jaeyun Jang",
        "Minyoung Noh",
        "Kyuhong Shim",
        "Byonghyo Shim"
      ],
      "abstract": "Large vision-language models (LVLMs) are increasingly deployed in interactive applications such as virtual and augmented reality, where a first-person (egocentric) view captured by head-mounted cameras serves as key input. While this view offers fine-grained cues about user attention and hand-object interactions, its narrow field of view and lack of global context often lead to failures on spatially or contextually demanding queries. To address this, we introduce a framework that augments egocentric inputs with third-person (exocentric) views, providing complementary information such as global scene layout and object visibility to LVLMs. We present E3VQA, the first benchmark for multi-view question answering with 4K high-quality question-answer pairs grounded in synchronized ego-exo image pairs. Additionally, we propose M3CoT, a training-free prompting technique that constructs a unified scene representation by integrating scene graphs from three complementary perspectives. M3CoT enables LVLMs to reason more effectively across views, yielding consistent performance gains (4.84% for GPT-4o and 5.94% for Gemini 2.0 Flash) over a recent CoT baseline. Our extensive evaluation reveals key strengths and limitations of LVLMs in multi-view reasoning and highlights the value of leveraging both egocentric and exocentric inputs. The dataset and source code are available at https://github.com/Leeinsu1/Towards-Comprehensive-Scene-Understanding.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹(LVLMs)åœ¨å¤„ç†ç¬¬ä¸€äººç§°è§†è§’(egocentric)æ—¶é¢ä¸´çš„è§†é‡ç‹­çª„å’Œå…¨å±€ä¸Šä¸‹æ–‡ç¼ºå¤±é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§æ•´åˆç¬¬ä¸‰äººç§°è§†è§’(exocentric)çš„å…¨æ–°æ¡†æ¶ã€‚ç ”ç©¶å›¢é˜Ÿæ¨å‡ºäº†E3VQAï¼Œè¿™æ˜¯é¦–ä¸ªåŒ…å«4000ä¸ªé«˜è´¨é‡é—®ç­”å¯¹çš„å¤šè§†è§’è§†è§‰é—®ç­”åŸºå‡†æ•°æ®é›†ï¼Œå…¶æ•°æ®å‡åŸºäºåŒæ­¥çš„ego-exoå›¾åƒå¯¹ã€‚æ­¤å¤–ï¼Œç ”ç©¶æå‡ºäº†ä¸€ç§åä¸ºM3CoTçš„å…è®­ç»ƒæç¤ºæŠ€æœ¯ï¼Œé€šè¿‡æ•´åˆæ¥è‡ªä¸‰ä¸ªäº’è¡¥ç»´åº¦çš„åœºæ™¯å›¾(scene graphs)æ¥æ„å»ºç»Ÿä¸€çš„åœºæ™¯è¡¨ç¤ºï¼Œä»è€Œæ˜¾è‘—å¢å¼ºLVLMsçš„è·¨è§†è§’æ¨ç†èƒ½åŠ›ã€‚å®éªŒè¯„ä¼°è¡¨æ˜ï¼ŒM3CoTç›¸æ¯”ç°æœ‰çš„CoTåŸºçº¿åœ¨GPT-4oå’ŒGemini 2.0 Flashä¸Šåˆ†åˆ«å®ç°äº†4.84%å’Œ5.94%çš„æ€§èƒ½æå‡ã€‚è¯¥å·¥ä½œæ·±å…¥åˆ†æäº†LVLMsåœ¨å¤šè§†è§’æ¨ç†ä¸­çš„ä¼˜åŠ¿ä¸å±€é™ï¼ŒéªŒè¯äº†ç»“åˆç¬¬ä¸€ä¸ç¬¬ä¸‰äººç§°è§†è§’å¯¹äºå®ç°å…¨é¢åœºæ™¯ç†è§£çš„å…³é”®ä½œç”¨ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "Accepted to NeurIPS 2025 (Spotlight)",
      "pdf_url": "https://arxiv.org/pdf/2505.21955v2",
      "published_date": "2025-05-28 04:09:42 UTC",
      "updated_date": "2025-10-24 07:43:13 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T15:36:35.428946+00:00"
    },
    {
      "arxiv_id": "2505.21954v1",
      "title": "UniTalk: Towards Universal Active Speaker Detection in Real World Scenarios",
      "title_zh": "UniTalkï¼šé¢å‘çœŸå®åœºæ™¯çš„é€šç”¨æ´»è·ƒè¯´è¯äººæ£€æµ‹",
      "authors": [
        "Le Thien Phuc Nguyen",
        "Zhuoran Yu",
        "Khoa Quang Nhat Cao",
        "Yuwei Guo",
        "Tu Ho Manh Pham",
        "Tuan Tai Nguyen",
        "Toan Ngo Duc Vo",
        "Lucas Poon",
        "Soochahn Lee",
        "Yong Jae Lee"
      ],
      "abstract": "We present UniTalk, a novel dataset specifically designed for the task of active speaker detection, emphasizing challenging scenarios to enhance model generalization. Unlike previously established benchmarks such as AVA, which predominantly features old movies and thus exhibits significant domain gaps, UniTalk focuses explicitly on diverse and difficult real-world conditions. These include underrepresented languages, noisy backgrounds, and crowded scenes - such as multiple visible speakers speaking concurrently or in overlapping turns. It contains over 44.5 hours of video with frame-level active speaker annotations across 48,693 speaking identities, and spans a broad range of video types that reflect real-world conditions. Through rigorous evaluation, we show that state-of-the-art models, while achieving nearly perfect scores on AVA, fail to reach saturation on UniTalk, suggesting that the ASD task remains far from solved under realistic conditions. Nevertheless, models trained on UniTalk demonstrate stronger generalization to modern \"in-the-wild\" datasets like Talkies and ASW, as well as to AVA. UniTalk thus establishes a new benchmark for active speaker detection, providing researchers with a valuable resource for developing and evaluating versatile and resilient models.\n  Dataset: https://huggingface.co/datasets/plnguyen2908/UniTalk-ASD\n  Code: https://github.com/plnguyen2908/UniTalk-ASD-code",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† UniTalkï¼Œä¸€ä¸ªä¸“ä¸º Active Speaker Detection (ASD) ä»»åŠ¡è®¾è®¡çš„æ–°å‹æ•°æ®é›†ï¼Œæ—¨åœ¨é€šè¿‡å¤æ‚çš„ç°å®åœºæ™¯å¢å¼ºæ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ã€‚ä¸ AVA ç­‰ä¸»è¦åŒ…å«æ—§ç”µå½±ç‰‡æ®µå¹¶å­˜åœ¨æ˜¾è‘—é¢†åŸŸåç½®çš„ä¼ ç»ŸåŸºå‡†ä¸åŒï¼ŒUniTalk ä¸“æ³¨äºå¤šæ ·åŒ–ä¸”å…·æœ‰æŒ‘æˆ˜æ€§çš„ç°å®ç¯å¢ƒï¼Œæ¶µç›–äº†å†·é—¨è¯­è¨€ã€å˜ˆæ‚èƒŒæ™¯ä»¥åŠå¤šåå‘è¨€è€…åŒæ—¶è¯´è¯æˆ–è¯­éŸ³é‡å çš„æ‹¥æŒ¤åœºæ™¯ã€‚è¯¥æ•°æ®é›†åŒ…å«è¶…è¿‡ 44.5 å°æ—¶çš„è§†é¢‘ï¼Œæä¾› 48,693 ä¸ªå‘è¨€èº«ä»½çš„å¸§çº§æ ‡æ³¨ï¼Œæ¶µç›–äº†åæ˜ ç°å®æ¡ä»¶çš„å¹¿æ³›è§†é¢‘ç±»å‹ã€‚å®éªŒè¯„ä¼°è¡¨æ˜ï¼Œå³ä¾¿æ˜¯åœ¨ AVA ä¸Šè¡¨ç°è¿‘ä¹å®Œç¾çš„ state-of-the-art æ¨¡å‹åœ¨ UniTalk ä¸Šä¹Ÿéš¾ä»¥è¾¾åˆ°é¥±å’Œï¼Œè¿™æš—ç¤ºäº†ç°å®æ¡ä»¶ä¸‹çš„ ASD ä»»åŠ¡ä»å…·æœ‰å·¨å¤§æŒ‘æˆ˜ã€‚æ­¤å¤–ï¼Œåœ¨ UniTalk ä¸Šè®­ç»ƒçš„æ¨¡å‹åœ¨ Talkiesã€ASW ä»¥åŠ AVA ç­‰æ•°æ®é›†ä¸Šå‡è¡¨ç°å‡ºæ›´å¼ºçš„æ³›åŒ–æ€§èƒ½ã€‚UniTalk ä¸º Active Speaker Detection ç¡®ç«‹äº†æ–°çš„åŸºå‡†ï¼Œä¸ºå¼€å‘æ›´å…·é€šç”¨æ€§å’ŒéŸ§æ€§çš„æ¨¡å‹æä¾›äº†å…³é”®çš„ç ”ç©¶èµ„æºã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.21954v1",
      "published_date": "2025-05-28 04:08:59 UTC",
      "updated_date": "2025-05-28 04:08:59 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T15:36:30.328966+00:00"
    },
    {
      "arxiv_id": "2505.21938v2",
      "title": "Practical Adversarial Attacks on Stochastic Bandits via Fake Data Injection",
      "title_zh": "åŸºäºè™šå‡æ•°æ®æ³¨å…¥çš„éšæœºå¤šè‡‚è€è™æœºå®ç”¨å¯¹æŠ—æ”»å‡»",
      "authors": [
        "Qirun Zeng",
        "Eric He",
        "Richard Hoffmann",
        "Xuchuang Wang",
        "Jinhang Zuo"
      ],
      "abstract": "Adversarial attacks on stochastic bandits have traditionally relied on some unrealistic assumptions, such as per-round reward manipulation and unbounded perturbations, limiting their relevance to real-world systems. We propose a more practical threat model, Fake Data Injection, which reflects realistic adversarial constraints: the attacker can inject only a limited number of bounded fake feedback samples into the learner's history, simulating legitimate interactions. We design efficient attack strategies under this model, explicitly addressing both magnitude constraints (on reward values) and temporal constraints (on when and how often data can be injected). Our theoretical analysis shows that these attacks can mislead both Upper Confidence Bound (UCB) and Thompson Sampling algorithms into selecting a target arm in nearly all rounds while incurring only sublinear attack cost. Experiments on synthetic and real-world datasets validate the effectiveness of our strategies, revealing significant vulnerabilities in widely used stochastic bandit algorithms under practical adversarial scenarios.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹éšæœºå¤šè‡‚è€è™æœº (Stochastic Bandits) ä¼ ç»Ÿå¯¹æŠ—æ”»å‡»ä¸­ä¸åˆ‡å®é™…çš„å‡è®¾ï¼Œæå‡ºäº†ä¸€ç§æ›´è´´è¿‘ç°å®çš„è™šå‡æ•°æ®æ³¨å…¥ (Fake Data Injection) å¨èƒæ¨¡å‹ã€‚è¯¥æ¨¡å‹å…è®¸æ”»å‡»è€…åœ¨æ¨¡æ‹Ÿåˆæ³•äº¤äº’çš„åŒæ—¶ï¼Œä»…å‘å­¦ä¹ è€…çš„å†å²è®°å½•ä¸­æ³¨å…¥æœ‰é™æ•°é‡ä¸”æœ‰ç•Œçš„è™šå‡åé¦ˆã€‚ç ”ç©¶è®¾è®¡äº†é«˜æ•ˆçš„æ”»å‡»ç­–ç•¥ï¼Œå¹¶æ˜ç¡®è§£å†³äº†å¥–åŠ±å€¼çš„é‡çº§çº¦æŸ (magnitude constraints) å’Œæ•°æ®æ³¨å…¥çš„æ—¶åºçº¦æŸ (temporal constraints)ã€‚ç†è®ºåˆ†æè¯æ˜ï¼Œè¿™äº›æ”»å‡»å¯ä»¥è¯±å¯¼ Upper Confidence Bound (UCB) å’Œ Thompson Sampling ç®—æ³•åœ¨å‡ ä¹æ‰€æœ‰è½®æ¬¡ä¸­é€‰æ‹©ç›®æ ‡è‡‚ï¼Œä¸”æ”»å‡»æˆæœ¬ä»…ä¸ºäºšçº¿æ€§ (sublinear)ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯¥ç­–ç•¥åœ¨åˆæˆåŠçœŸå®æ•°æ®é›†ä¸Šå‡è¡¨ç°æœ‰æ•ˆï¼Œæ­ç¤ºäº†ä¸»æµéšæœºå¤šè‡‚è€è™æœºç®—æ³•åœ¨å®é™…å¯¹æŠ—ç¯å¢ƒä¸‹çš„é‡å¤§å®‰å…¨æ¼æ´ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CR"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.21938v2",
      "published_date": "2025-05-28 03:47:13 UTC",
      "updated_date": "2025-05-31 07:08:47 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T15:36:31.513356+00:00"
    },
    {
      "arxiv_id": "2505.21935v2",
      "title": "From Reasoning to Learning: A Survey on Hypothesis Discovery and Rule Learning with Large Language Models",
      "title_zh": "ä»æ¨ç†åˆ°å­¦ä¹ ï¼šå¤§è¯­è¨€æ¨¡å‹å‡è®¾å‘ç°ä¸è§„åˆ™å­¦ä¹ ç»¼è¿°",
      "authors": [
        "Kaiyu He",
        "Zhiyu Chen"
      ],
      "abstract": "Since the advent of Large Language Models (LLMs), efforts have largely focused on improving their instruction-following and deductive reasoning abilities, leaving open the question of whether these models can truly discover new knowledge. In pursuit of artificial general intelligence (AGI), there is a growing need for models that not only execute commands or retrieve information but also learn, reason, and generate new knowledge by formulating novel hypotheses and theories that deepen our understanding of the world. Guided by Peirce's framework of abduction, deduction, and induction, this survey offers a structured lens to examine LLM-based hypothesis discovery. We synthesize existing work in hypothesis generation, application, and validation, identifying both key achievements and critical gaps. By unifying these threads, we illuminate how LLMs might evolve from mere ``information executors'' into engines of genuine innovation, potentially transforming research, science, and real-world problem solving.",
      "tldr_zh": "è¯¥ç»¼è¿°åœ¨é€šç”¨äººå·¥æ™ºèƒ½(AGI)çš„å®è§‚èƒŒæ™¯ä¸‹ï¼Œæ¢è®¨äº†å¤§è¯­è¨€æ¨¡å‹(LLMs)æ˜¯å¦å…·å¤‡è¶…è¶ŠæŒ‡ä»¤éµå¾ªå’Œæ¼”ç»æ¨ç†ã€è¿›è€Œå‘ç°æ–°çŸ¥è¯†çš„èƒ½åŠ›ã€‚ç ”ç©¶é‡‡ç”¨äº†Peirceå…³äºæº¯å› æ¨ç†(Abduction)ã€æ¼”ç»(Deduction)å’Œå½’çº³(Induction)çš„ç»å…¸æ¡†æ¶ï¼Œå¯¹åŸºäºLLMsçš„å‡è®¾å‘ç°(Hypothesis Discovery)ä¸è§„åˆ™å­¦ä¹ (Rule Learning)è¿›è¡Œäº†ç³»ç»Ÿæ€§æ¢³ç†ã€‚æ–‡ç« è¯¦ç»†æ±‡æ€»äº†ç°æœ‰çš„å‡è®¾ç”Ÿæˆ(Hypothesis Generation)ã€åº”ç”¨ä»¥åŠéªŒè¯æ–¹æ³•ï¼Œå¹¶è¯†åˆ«äº†å½“å‰æŠ€æœ¯çš„é‡å¤§æˆå°±ä¸æ ¸å¿ƒæŒ‘æˆ˜ã€‚é€šè¿‡æ•´åˆè¿™äº›ç ”ç©¶æ–¹å‘ï¼Œç»¼è¿°è®ºè¯äº†LLMsä»å•çº¯çš„â€œä¿¡æ¯æ‰§è¡Œå™¨â€å‘â€œåˆ›æ–°å¼•æ“â€æ¼”è¿›çš„å¯èƒ½æ€§ã€‚è¿™ç§è½¬å˜æœ‰æœ›åœ¨ç§‘å­¦ç ”ç©¶å’Œå¤æ‚ç°å®é—®é¢˜è§£å†³ä¸­å®ç°èŒƒå¼çªç ´ï¼Œå¹¶ä¸ºæœªæ¥å®ç°æ›´é«˜æ°´å¹³çš„äººå·¥æ™ºèƒ½æä¾›äº†ç†è®ºæ”¯æ’‘ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "TMLR Survey Certification",
      "pdf_url": "https://arxiv.org/pdf/2505.21935v2",
      "published_date": "2025-05-28 03:40:02 UTC",
      "updated_date": "2025-08-24 02:09:11 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T15:37:35.816019+00:00"
    },
    {
      "arxiv_id": "2505.21928v2",
      "title": "Subspecialty-Specific Foundation Model for Intelligent Gastrointestinal Pathology",
      "title_zh": "é¢å‘æ™ºèƒ½èƒƒè‚ ç—…ç†å­¦çš„äºšä¸“ç§‘ç‰¹å®šåŸºç¡€æ¨¡å‹",
      "authors": [
        "Lianghui Zhu",
        "Xitong Ling",
        "Minxi Ouyang",
        "Xiaoping Liu",
        "Tian Guan",
        "Mingxi Fu",
        "Zhiqiang Cheng",
        "Fanglei Fu",
        "Maomao Zeng",
        "Liming Liu",
        "Song Duan",
        "Qiang Huang",
        "Ying Xiao",
        "Jianming Li",
        "Shanming Lu",
        "Zhenghua Piao",
        "Mingxi Zhu",
        "Yibo Jin",
        "Shan Xu",
        "Qiming He",
        "Yizhi Wang",
        "Junru Cheng",
        "Xuanyu Wang",
        "Luxi Xie",
        "Houqiang Li",
        "Sufang Tian",
        "Yonghong He"
      ],
      "abstract": "Gastrointestinal (GI) diseases represent a clinically significant burden, necessitating precise diagnostic approaches to optimize patient outcomes. Conventional histopathological diagnosis suffers from limited reproducibility and diagnostic variability. To overcome these limitations, we develop Digepath, a specialized foundation model for GI pathology. Our framework introduces a dual-phase iterative optimization strategy combining pretraining with fine-screening, specifically designed to address the detection of sparsely distributed lesion areas in whole-slide images. Digepath is pretrained on over 353 million multi-scale images from 210,043 H&E-stained slides of GI diseases. It attains state-of-the-art performance on 33 out of 34 tasks related to GI pathology, including pathological diagnosis, protein expression status prediction, gene mutation prediction, and prognosis evaluation. We further translate the intelligent screening module for early GI cancer and achieve near-perfect 99.70% sensitivity across nine independent medical institutions. This work not only advances AI-driven precision pathology for GI diseases but also bridge critical gaps in histopathological practice.",
      "tldr_zh": "è¯¥ç ”ç©¶å¼€å‘äº†Digepathï¼Œè¿™æ˜¯ä¸€ä¸ªä¸“é—¨é’ˆå¯¹èƒƒè‚ é“(Gastrointestinal, GI)ç—…ç†å­¦çš„äºšä¸“ç§‘åŸºç¡€æ¨¡å‹ï¼Œæ—¨åœ¨è§£å†³ä¼ ç»Ÿç»„ç»‡ç—…ç†è¯Šæ–­ä¸­å­˜åœ¨çš„é‡å¤æ€§æœ‰é™å’Œè¯Šæ–­å˜å¼‚æ€§ç­‰é—®é¢˜ã€‚è¯¥æ¡†æ¶å¼•å…¥äº†ä¸€ç§ç»“åˆé¢„è®­ç»ƒ(pretraining)ä¸ç²¾ç»†ç­›é€‰(fine-screening)çš„åŒé˜¶æ®µè¿­ä»£ä¼˜åŒ–ç­–ç•¥ï¼Œä¸“é—¨ç”¨äºè¯†åˆ«å…¨åˆ‡ç‰‡å›¾åƒ(whole-slide images)ä¸­ç¨€ç–åˆ†å¸ƒçš„ç—…å˜åŒºåŸŸã€‚DigepathåŸºäºæ¥è‡ª210,043å¼ H&EæŸ“è‰²åˆ‡ç‰‡çš„è¶…è¿‡3.53äº¿å¼ å¤šå°ºåº¦å›¾åƒè¿›è¡Œäº†é¢„è®­ç»ƒï¼Œå¹¶åœ¨34é¡¹èƒƒè‚ é“ç—…ç†ç›¸å…³ä»»åŠ¡ä¸­çš„33é¡¹ä¸Šè¾¾åˆ°äº†æœ€å…ˆè¿›(state-of-the-art)çš„æ€§èƒ½ã€‚è¿™äº›ä»»åŠ¡æ¶µç›–äº†ç—…ç†è¯Šæ–­ã€è›‹ç™½è´¨è¡¨è¾¾é¢„æµ‹ã€åŸºå› çªå˜é¢„æµ‹åŠé¢„åè¯„ä¼°ç­‰å¤šä¸ªç»´åº¦ã€‚æ­¤å¤–ï¼Œè¯¥æ¨¡å‹åœ¨æ—©æœŸèƒƒè‚ é“ç™Œç—‡çš„æ™ºèƒ½ç­›æŸ¥ä¸­è¡¨ç°å‡ºè‰²ï¼Œåœ¨ä¹å®¶ç‹¬ç«‹åŒ»ç–—æœºæ„çš„éªŒè¯ä¸­å®ç°äº†99.70%çš„æ•æ„Ÿæ€§ã€‚è¿™é¡¹å·¥ä½œæ˜¾è‘—æ¨è¿›äº†äººå·¥æ™ºèƒ½é©±åŠ¨çš„èƒƒè‚ é“ç²¾å‡†ç—…ç†å­¦ï¼Œæœ‰æ•ˆå¼¥åˆäº†ä¸´åºŠç»„ç»‡ç—…ç†å­¦å®è·µä¸­çš„å…³é”®å·®è·ã€‚",
      "categories": [
        "eess.IV",
        "cs.AI",
        "cs.CV",
        "cs.LG"
      ],
      "primary_category": "eess.IV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.21928v2",
      "published_date": "2025-05-28 03:22:08 UTC",
      "updated_date": "2025-06-06 10:39:32 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T15:37:20.571531+00:00"
    },
    {
      "arxiv_id": "2505.21926v1",
      "title": "Beyond Completion: A Foundation Model for General Knowledge Graph Reasoning",
      "title_zh": "è¶…è¶Šè¡¥å…¨ï¼šé¢å‘é€šç”¨çŸ¥è¯†å›¾è°±æ¨ç†çš„åŸºåº§æ¨¡å‹",
      "authors": [
        "Yin Hua",
        "Zhiqiang Liu",
        "Mingyang Chen",
        "Zheng Fang",
        "Chi Man Wong",
        "Lingxiao Li",
        "Chi Man Vong",
        "Huajun Chen",
        "Wen Zhang"
      ],
      "abstract": "In natural language processing (NLP) and computer vision (CV), the successful application of foundation models across diverse tasks has demonstrated their remarkable potential. However, despite the rich structural and textual information embedded in knowledge graphs (KGs), existing research of foundation model for KG has primarily focused on their structural aspects, with most efforts restricted to in-KG tasks (e.g., knowledge graph completion, KGC). This limitation has hindered progress in addressing more challenging out-of-KG tasks. In this paper, we introduce MERRY, a foundation model for general knowledge graph reasoning, and investigate its performance across two task categories: in-KG reasoning tasks (e.g., KGC) and out-of-KG tasks (e.g., KG question answering, KGQA). We not only utilize the structural information, but also the textual information in KGs. Specifically, we propose a multi-perspective Conditional Message Passing (CMP) encoding architecture to bridge the gap between textual and structural modalities, enabling their seamless integration. Additionally, we introduce a dynamic residual fusion module to selectively retain relevant textual information and a flexible edge scoring mechanism to adapt to diverse downstream tasks. Comprehensive evaluations on 28 datasets demonstrate that MERRY outperforms existing baselines in most scenarios, showcasing strong reasoning capabilities within KGs and excellent generalization to out-of-KG tasks such as KGQA.",
      "tldr_zh": "æœ¬ç ”ç©¶æå‡ºäº†MERRYï¼Œè¿™æ˜¯ä¸€ä¸ªé¢å‘é€šç”¨çŸ¥è¯†å›¾è°±(Knowledge Graph)æ¨ç†çš„åŸºåº§æ¨¡å‹ï¼Œæ—¨åœ¨è§£å†³ç°æœ‰ç ”ç©¶è¿‡åº¦å…³æ³¨ç»“æ„ä¿¡æ¯ä¸”å±€é™äºå›¾è°±å†…(in-KG)ä»»åŠ¡çš„å±€é™ã€‚è¯¥æ¨¡å‹é€šè¿‡æ·±åº¦æ•´åˆçŸ¥è¯†å›¾è°±çš„ç»“æ„ä¸æ–‡æœ¬ä¿¡æ¯ï¼Œä¸ä»…æå‡äº†å›¾è°±å†…æ¨ç†(Knowledge Graph Completion, KGC)çš„æ€§èƒ½ï¼Œä¹Ÿæ˜¾è‘—å¢å¼ºäº†åœ¨å›¾è°±å¤–(out-of-KG)ä»»åŠ¡ï¼ˆå¦‚KGQAï¼‰ä¸­çš„è¡¨ç°ã€‚åœ¨æŠ€æœ¯æ¶æ„ä¸Šï¼ŒMERRYé‡‡ç”¨äº†å¤šè§†è§’æ¡ä»¶æ¶ˆæ¯ä¼ é€’(Conditional Message Passing, CMP)ç¼–ç æ¶æ„æ¥æ— ç¼è¡”æ¥æ–‡æœ¬ä¸ç»“æ„æ¨¡æ€ï¼Œå¹¶å¼•å…¥åŠ¨æ€æ®‹å·®èåˆæ¨¡å—ä¸çµæ´»çš„è¾¹è¯„åˆ†æœºåˆ¶ä»¥é€‚åº”å¤šæ ·åŒ–çš„ä¸‹æ¸¸ä»»åŠ¡ã€‚åœ¨28ä¸ªæ•°æ®é›†ä¸Šçš„å…¨é¢è¯„ä¼°ç»“æœæ˜¾ç¤ºï¼ŒMERRYåœ¨ç»å¤§å¤šæ•°åœºæ™¯ä¸‹å‡è¶…è¶Šäº†ç°æœ‰åŸºå‡†æ¨¡å‹ã€‚å®éªŒè¯æ˜ï¼ŒMERRYåœ¨å…·å¤‡å¼ºå¤§å›¾è°±å†…æ¨ç†èƒ½åŠ›çš„åŒæ—¶ï¼Œå¯¹KGQAç­‰å›¾è°±å¤–ä»»åŠ¡å±•ç°å‡ºäº†å“è¶Šçš„æ³›åŒ–æ½œåŠ›ï¼Œä¸ºæ„å»ºé€šç”¨çš„çŸ¥è¯†å›¾è°±æ¨ç†æ¨¡å‹æä¾›äº†æœ‰æ•ˆæ–¹æ¡ˆã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "ACL 2025 Findings",
      "pdf_url": "https://arxiv.org/pdf/2505.21926v1",
      "published_date": "2025-05-28 03:21:28 UTC",
      "updated_date": "2025-05-28 03:21:28 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T15:37:21.649114+00:00"
    },
    {
      "arxiv_id": "2505.21923v2",
      "title": "FALCON: An ML Framework for Fully Automated Layout-Constrained Analog Circuit Design",
      "title_zh": "FALCONï¼šé¢å‘å…¨è‡ªåŠ¨å¸ƒå±€çº¦æŸæ¨¡æ‹Ÿç”µè·¯è®¾è®¡çš„æœºå™¨å­¦ä¹ æ¡†æ¶",
      "authors": [
        "Asal Mehradfar",
        "Xuzhe Zhao",
        "Yilun Huang",
        "Emir Ceyani",
        "Yankai Yang",
        "Shihao Han",
        "Hamidreza Aghasi",
        "Salman Avestimehr"
      ],
      "abstract": "Designing analog circuits from performance specifications is a complex, multi-stage process encompassing topology selection, parameter inference, and layout feasibility. We introduce FALCON, a unified machine learning framework that enables fully automated, specification-driven analog circuit synthesis through topology selection and layout-constrained optimization. Given a target performance, FALCON first selects an appropriate circuit topology using a performance-driven classifier guided by human design heuristics. Next, it employs a custom, edge-centric graph neural network trained to map circuit topology and parameters to performance, enabling gradient-based parameter inference through the learned forward model. This inference is guided by a differentiable layout cost, derived from analytical equations capturing parasitic and frequency-dependent effects, and constrained by design rules. We train and evaluate FALCON on a large-scale custom dataset of 1M analog mm-wave circuits, generated and simulated using Cadence Spectre across 20 expert-designed topologies. Through this evaluation, FALCON demonstrates >99% accuracy in topology inference, <10% relative error in performance prediction, and efficient layout-aware design that completes in under 1 second per instance. Together, these results position FALCON as a practical and extensible foundation model for end-to-end analog circuit design automation.",
      "tldr_zh": "æœ¬ç ”ç©¶æå‡ºäº† FALCONï¼Œä¸€ä¸ªç”¨äºå…¨è‡ªåŠ¨ã€æŒ‡æ ‡é©±åŠ¨çš„æ¨¡æ‹Ÿç”µè·¯è®¾è®¡çš„ç»Ÿä¸€æœºå™¨å­¦ä¹ æ¡†æ¶ã€‚è¯¥æ¡†æ¶é¦–å…ˆæ ¹æ®ç›®æ ‡æ€§èƒ½ï¼Œé€šè¿‡ç»“åˆäººç±»è®¾è®¡å¯å‘å¼çš„æ€§èƒ½é©±åŠ¨åˆ†ç±»å™¨æ¥é€‰æ‹©åˆé€‚çš„ç”µè·¯ Topologyã€‚éšåï¼ŒFALCON é‡‡ç”¨ä¸€ç§è‡ªå®šä¹‰çš„ä»¥è¾¹ç¼˜ä¸ºä¸­å¿ƒçš„ Graph Neural Network (GNN) å°†ç”µè·¯ Topology å’Œå‚æ•°æ˜ å°„åˆ°æ€§èƒ½è¡¨ç°ï¼Œå¹¶é€šè¿‡å­¦ä¹ åˆ°çš„å‰å‘æ¨¡å‹å®ç°åŸºäºæ¢¯åº¦çš„å‚æ•°æ¨æ–­ã€‚è¯¥æ¨æ–­è¿‡ç¨‹å—åˆ°å¯å¾® Layout æˆæœ¬çš„æŒ‡å¯¼ï¼Œè¯¥æˆæœ¬æºè‡ªæ•æ‰å¯„ç”Ÿå’Œé¢‘ç‡ç›¸å…³æ•ˆåº”çš„åˆ†ææ–¹ç¨‹ï¼Œå¹¶å—è®¾è®¡è§„åˆ™çº¦æŸã€‚ç ”ç©¶å›¢é˜Ÿåœ¨ä¸€ä¸ªåŒ…å« 100 ä¸‡ä¸ªæ¨¡æ‹Ÿ mm-wave ç”µè·¯çš„å¤§è§„æ¨¡æ•°æ®é›†ä¸Šå¯¹ FALCON è¿›è¡Œäº†è¯„ä¼°ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒFALCON åœ¨ Topology æ¨æ–­æ–¹é¢è¾¾åˆ°äº†è¶…è¿‡ 99% çš„å‡†ç¡®ç‡ï¼Œæ€§èƒ½é¢„æµ‹çš„ç›¸å¯¹è¯¯å·®ä½äº 10%ï¼Œä¸”æ¯ä¾‹ Layout æ„ŸçŸ¥è®¾è®¡ä»…éœ€ä¸åˆ° 1 ç§’å³å¯å®Œæˆã€‚FALCON ä¸ºå®ç°ç«¯åˆ°ç«¯æ¨¡æ‹Ÿç”µè·¯è®¾è®¡è‡ªåŠ¨åŒ–æä¾›äº†ä¸€ä¸ªå®ç”¨ä¸”å…·æœ‰æ‰©å±•æ€§çš„ Foundation Modelã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.AR",
        "cs.CE"
      ],
      "primary_category": "cs.LG",
      "comment": "Accepted at the 39th Conference on Neural Information Processing Systems (NeurIPS 2025)",
      "pdf_url": "https://arxiv.org/pdf/2505.21923v2",
      "published_date": "2025-05-28 03:16:08 UTC",
      "updated_date": "2025-10-27 22:42:49 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T15:37:19.431663+00:00"
    },
    {
      "arxiv_id": "2505.21919v1",
      "title": "Towards Efficient Key-Value Cache Management for Prefix Prefilling in LLM Inference",
      "title_zh": "é¢å‘å¤§è¯­è¨€æ¨¡å‹æ¨ç†ä¸­å‰ç¼€é¢„å¡«å……çš„é«˜æ•ˆé”®å€¼ç¼“å­˜ç®¡ç†",
      "authors": [
        "Yue Zhu",
        "Hao Yu",
        "Chen Wang",
        "Zhuoran Liu",
        "Eun Kyung Lee"
      ],
      "abstract": "The increasing adoption of large language models (LLMs) with extended context windows necessitates efficient Key-Value Cache (KVC) management to optimize inference performance. Inference workloads like Retrieval-Augmented Generation (RAG) and agents exhibit high cache reusability, making efficient caching critical to reducing redundancy and improving speed. We analyze real-world KVC access patterns using publicly available traces and evaluate commercial key-value stores like Redis and state-of-the-art RDMA-based systems (CHIME [1] and Sherman [2]) for KVC metadata management. Our work demonstrates the lack of tailored storage solution for KVC prefilling, underscores the need for an efficient distributed caching system with optimized metadata management for LLM workloads, and provides insights into designing improved KVC management systems for scalable, low-latency inference.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†åœ¨å¤§è¯­è¨€æ¨¡å‹(LLMs)é•¿ä¸Šä¸‹æ–‡æ¨ç†èƒŒæ™¯ä¸‹ï¼Œå¦‚ä½•å®ç°é«˜æ•ˆçš„é”®å€¼ç¼“å­˜(Key-Value Cache)ç®¡ç†ã€‚é’ˆå¯¹æ£€ç´¢å¢å¼ºç”Ÿæˆ(RAG)å’Œæ™ºèƒ½ä½“(agents)ç­‰å…·æœ‰é«˜ç¼“å­˜é‡ç”¨ç‡çš„æ¨ç†ä»»åŠ¡ï¼Œä½œè€…åˆ©ç”¨å…¬å¼€æ•°æ®æ·±å…¥åˆ†æäº†çœŸå®ä¸–ç•Œçš„KVCè®¿é—®æ¨¡å¼ã€‚é€šè¿‡è¯„ä¼°Redisä»¥åŠåŸºäºRDMAçš„å­˜å‚¨ç³»ç»ŸCHIMEå’ŒShermanåœ¨å…ƒæ•°æ®ç®¡ç†ä¸­çš„è¡¨ç°ï¼Œç ”ç©¶æŒ‡å‡ºç›®å‰ç¼ºä¹ä¸“é—¨é’ˆå¯¹KVCé¢„å¡«å……(prefix prefilling)ä¼˜åŒ–çš„å­˜å‚¨æ–¹æ¡ˆã€‚æ–‡ç« å¼ºè°ƒäº†å¼€å‘å…·å¤‡ä¼˜åŒ–å…ƒæ•°æ®ç®¡ç†èƒ½åŠ›çš„åˆ†å¸ƒå¼ç¼“å­˜ç³»ç»Ÿå¯¹äºå¤„ç†å¤§è¯­è¨€æ¨¡å‹å·¥ä½œè´Ÿè½½çš„ç´§è¿«æ€§ã€‚è¯¥å·¥ä½œæ­ç¤ºäº†ç°æœ‰å­˜å‚¨æŠ€æœ¯çš„å±€é™æ€§ï¼Œå¹¶ä¸ºæœªæ¥è®¾è®¡å¯æ‰©å±•ã€ä½å»¶è¿Ÿçš„KVCç®¡ç†ç³»ç»Ÿæä¾›äº†å…³é”®çš„æ´å¯Ÿå’Œæ–¹å‘ã€‚",
      "categories": [
        "cs.ET",
        "cs.AI",
        "cs.DC"
      ],
      "primary_category": "cs.ET",
      "comment": "This paper has been accepted at IEEE Cloud 2025 as WIP paper. The final version will appear in IEEE Xplore",
      "pdf_url": "https://arxiv.org/pdf/2505.21919v1",
      "published_date": "2025-05-28 03:05:55 UTC",
      "updated_date": "2025-05-28 03:05:55 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T15:37:38.335109+00:00"
    },
    {
      "arxiv_id": "2505.21918v1",
      "title": "Self-supervised Learning Method Using Transformer for Multi-dimensional Sensor Data Processing",
      "title_zh": "åŸºäº Transformer çš„å¤šç»´ä¼ æ„Ÿå™¨æ•°æ®å¤„ç†è‡ªç›‘ç£å­¦ä¹ æ–¹æ³•",
      "authors": [
        "Haruki Kai",
        "Tsuyoshi Okita"
      ],
      "abstract": "We developed a deep learning algorithm for human activity recognition using sensor signals as input. In this study, we built a pretrained language model based on the Transformer architecture, which is widely used in natural language processing. By leveraging this pretrained model, we aimed to improve performance on the downstream task of human activity recognition. While this task can be addressed using a vanilla Transformer, we propose an enhanced n-dimensional numerical processing Transformer that incorporates three key features: embedding n-dimensional numerical data through a linear layer, binning-based pre-processing, and a linear transformation in the output layer. We evaluated the effectiveness of our proposed model across five different datasets. Compared to the vanilla Transformer, our model demonstrated 10%-15% improvements in accuracy.",
      "tldr_zh": "è¯¥ç ”ç©¶å¼€å‘äº†ä¸€ç§ç”¨äºäººä½“æ´»åŠ¨è¯†åˆ« (human activity recognition) çš„æ·±åº¦å­¦ä¹ ç®—æ³•ï¼Œé€šè¿‡åˆ©ç”¨è‡ªç„¶è¯­è¨€å¤„ç†ä¸­å¹¿æ³›ä½¿ç”¨çš„ Transformer æ¶æ„æ„å»ºé¢„è®­ç»ƒæ¨¡å‹ã€‚ä¸ºäº†æå‡ä¸‹æ¸¸ä»»åŠ¡æ€§èƒ½ï¼Œç ”ç©¶æå‡ºäº†ä¸€ç§å¢å¼ºå‹ n-dimensional numerical processing Transformerï¼Œå…¶æ ¸å¿ƒæ”¹è¿›åŒ…æ‹¬é€šè¿‡ linear layer åµŒå…¥ n ç»´æ•°å€¼æ•°æ®ã€å¼•å…¥åŸºäº binning çš„é¢„å¤„ç†ä»¥åŠåœ¨è¾“å‡ºå±‚åº”ç”¨çº¿æ€§å˜æ¢ã€‚ç ”ç©¶äººå‘˜åœ¨äº”ä¸ªä¸åŒçš„æ•°æ®é›†ä¸Šå¯¹è¯¥æ¨¡å‹è¿›è¡Œäº†æœ‰æ•ˆæ€§è¯„ä¼°ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œä¸ vanilla Transformer ç›¸æ¯”ï¼Œè¯¥æ¨¡å‹åœ¨å‡†ç¡®ç‡ä¸Šå®ç°äº† 10%-15% çš„æ˜¾è‘—æå‡ï¼ŒéªŒè¯äº†è¯¥è‡ªç›‘ç£å­¦ä¹ æ–¹æ³•åœ¨å¤šç»´ä¼ æ„Ÿå™¨æ•°æ®å¤„ç†é¢†åŸŸçš„ä¼˜è¶Šæ€§ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "25 pages, 4 figures",
      "pdf_url": "https://arxiv.org/pdf/2505.21918v1",
      "published_date": "2025-05-28 03:04:13 UTC",
      "updated_date": "2025-05-28 03:04:13 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T15:37:43.543746+00:00"
    },
    {
      "arxiv_id": "2505.21908v2",
      "title": "Reinforcement Learning for Out-of-Distribution Reasoning in LLMs: An Empirical Study on Diagnosis-Related Group Coding",
      "title_zh": "å¤§è¯­è¨€æ¨¡å‹åˆ†å¸ƒå¤–æ¨ç†çš„å¼ºåŒ–å­¦ä¹ ï¼šé’ˆå¯¹ç–¾ç—…è¯Šæ–­ç›¸å…³åˆ†ç»„ï¼ˆDRGï¼‰ç¼–ç çš„å®è¯ç ”ç©¶",
      "authors": [
        "Hanyin Wang",
        "Zhenbang Wu",
        "Gururaj Kolar",
        "Hariprasad Korsapati",
        "Brian Bartlett",
        "Bryan Hull",
        "Jimeng Sun"
      ],
      "abstract": "Diagnosis-Related Group (DRG) codes are essential for hospital reimbursement and operations but require labor-intensive assignment. Large Language Models (LLMs) struggle with DRG coding due to the out-of-distribution (OOD) nature of the task: pretraining corpora rarely contain private clinical or billing data. We introduce DRG-Sapphire, which uses large-scale reinforcement learning (RL) for automated DRG coding from clinical notes. Built on Qwen2.5-7B and trained with Group Relative Policy Optimization (GRPO) using rule-based rewards, DRG-Sapphire introduces a series of RL enhancements to address domain-specific challenges not seen in previous mathematical tasks. Our model achieves state-of-the-art accuracy on the MIMIC-IV benchmark and generates physician-validated reasoning for DRG assignments, significantly enhancing explainability. Our study further sheds light on broader challenges of applying RL to knowledge-intensive, OOD tasks. We observe that RL performance scales approximately linearly with the logarithm of the number of supervised fine-tuning (SFT) examples, suggesting that RL effectiveness is fundamentally constrained by the domain knowledge encoded in the base model. For OOD tasks like DRG coding, strong RL performance requires sufficient knowledge infusion prior to RL. Consequently, scaling SFT may be more effective and computationally efficient than scaling RL alone for such tasks.",
      "tldr_zh": "è¿™é¡¹ç ”ç©¶é’ˆå¯¹å¤§è¯­è¨€æ¨¡å‹(LLMs)åœ¨ç–¾ç—…è¯Šæ–­ç›¸å…³åˆ†ç»„(Diagnosis-Related Group, DRG)ç¼–ç ä¸­é¢ä¸´çš„åˆ†å¸ƒå¤–(Out-of-Distribution, OOD)æŒ‘æˆ˜ï¼Œæå‡ºäº†DRG-Sapphireæ¡†æ¶ã€‚è¯¥æ¡†æ¶åŸºäºQwen2.5-7Bï¼Œé€šè¿‡é‡‡ç”¨åŸºäºè§„åˆ™å¥–åŠ±çš„ç¾¤ä½“ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–(Group Relative Policy Optimization, GRPO)å¤§è§„æ¨¡å¼ºåŒ–å­¦ä¹ (RL)æŠ€æœ¯ï¼Œå®ç°äº†ä¸´åºŠæ–‡æœ¬çš„è‡ªåŠ¨åŒ–ç¼–ç ã€‚DRG-Sapphireåœ¨MIMIC-IVåŸºå‡†æµ‹è¯•ä¸­å–å¾—äº†æœ€å…ˆè¿›(SOTA)çš„å‡†ç¡®ç‡ï¼Œå¹¶èƒ½ç”Ÿæˆç»åŒ»ç”ŸéªŒè¯çš„æ¨ç†é€»è¾‘ï¼Œæ˜¾è‘—å¢å¼ºäº†ç»“æœçš„å¯è§£é‡Šæ€§ã€‚ç ”ç©¶è¿›ä¸€æ­¥å‘ç°ï¼ŒRLçš„æ€§èƒ½å¢ç›Šä¸ç›‘ç£å¾®è°ƒ(SFT)æ ·æœ¬é‡çš„å¯¹æ•°å‘ˆçº¿æ€§ç›¸å…³ï¼Œè¡¨æ˜RLçš„æœ‰æ•ˆæ€§æœ¬è´¨ä¸Šå—é™äºåŸºç¡€æ¨¡å‹æ‰€æŒæ¡çš„é¢†åŸŸçŸ¥è¯†ã€‚å› æ­¤ï¼Œå¯¹äºDRGç¼–ç è¿™ç±»å¼ºé¢†åŸŸç›¸å…³çš„OODä»»åŠ¡ï¼Œåœ¨å¼ºåŒ–å­¦ä¹ ä¹‹å‰è¿›è¡Œå……åˆ†çš„çŸ¥è¯†æ³¨å…¥è‡³å…³é‡è¦ï¼Œä¸”æ‰©å±•SFTè§„æ¨¡å¯èƒ½æ¯”å•çº¯æ‰©å±•RLæ›´å…·è®¡ç®—æ•ˆç‡ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.21908v2",
      "published_date": "2025-05-28 02:54:07 UTC",
      "updated_date": "2025-10-14 18:07:33 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T15:37:50.264452+00:00"
    },
    {
      "arxiv_id": "2505.21907v2",
      "title": "Modeling and Optimizing User Preferences in AI Copilots: A Comprehensive Survey and Taxonomy",
      "title_zh": "AI Copilot ä¸­çš„ç”¨æˆ·åå¥½å»ºæ¨¡ä¸ä¼˜åŒ–ï¼šå…¨é¢ç»¼è¿°ä¸åˆ†ç±»ä½“ç³»",
      "authors": [
        "Saleh Afzoon",
        "Zahra Jahanandish",
        "Phuong Thao Huynh",
        "Amin Beheshti",
        "Usman Naseem"
      ],
      "abstract": "AI copilots represent a new generation of AI-powered systems designed to assist users, particularly knowledge workers and developers, in complex, context-rich tasks. As these systems become more embedded in daily workflows, personalization has emerged as a critical factor for improving usability, effectiveness, and user satisfaction. Central to this personalization is preference optimization: the system's ability to detect, interpret, and align with individual user preferences. While prior work in intelligent assistants and optimization algorithms is extensive, their intersection within AI copilots remains underexplored. This survey addresses that gap by examining how user preferences are operationalized in AI copilots. We investigate how preference signals are sourced, modeled across different interaction stages, and refined through feedback loops. Building on a comprehensive literature review, we define the concept of an AI copilot and introduce a taxonomy of preference optimization techniques across pre-, mid-, and post-interaction phases. Each technique is evaluated in terms of advantages, limitations, and design implications. By consolidating fragmented efforts across AI personalization, human-AI interaction, and language model adaptation, this work offers both a unified conceptual foundation and a practical design perspective for building user-aligned, persona-aware AI copilots that support end-to-end adaptability and deployment.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹AI Copilotsåœ¨å¤„ç†å¤æ‚ä»»åŠ¡ä¸­çš„ä¸ªæ€§åŒ–éœ€æ±‚ï¼Œæ·±å…¥æ¢è®¨äº†ç”¨æˆ·åå¥½ä¼˜åŒ–(Preference Optimization)è¿™ä¸€å…³é”®é¢†åŸŸã€‚æ–‡ç« æ˜ç¡®å®šä¹‰äº†AI Copilotçš„æ¦‚å¿µï¼Œå¹¶æ„å»ºäº†ä¸€å¥—æ¶µç›–äº¤äº’å‰(pre-interaction)ã€äº¤äº’ä¸­(mid-interaction)åŠäº¤äº’å(post-interaction)é˜¶æ®µçš„åå¥½ä¼˜åŒ–æŠ€æœ¯åˆ†ç±»æ³•(Taxonomy)ã€‚é€šè¿‡è¯¦ç»†åˆ†æåå¥½ä¿¡å·çš„æ¥æºã€å»ºæ¨¡æ–¹å¼ä»¥åŠåˆ©ç”¨åé¦ˆå¾ªç¯(Feedback Loops)è¿›è¡Œä¼˜åŒ–çš„æœºåˆ¶ï¼Œç ”ç©¶å…¨é¢è¯„ä¼°äº†å„ç±»æŠ€æœ¯çš„ä¼˜åŠ¿ã€å±€é™æ€§åŠå…¶è®¾è®¡å«ä¹‰ã€‚è¯¥ç»¼è¿°æœ‰æ•ˆæ•´åˆäº†AIä¸ªæ€§åŒ–ã€äººæœºäº¤äº’(Human-AI Interaction)åŠè¯­è¨€æ¨¡å‹é€‚é…(Language Model Adaptation)ç­‰å¤šä¸ªé¢†åŸŸçš„é›¶æ•£æˆæœã€‚æœ€ç»ˆï¼Œè¯¥å·¥ä½œä¸ºå¼€å‘å…·å¤‡è§’è‰²æ„ŸçŸ¥(Persona-aware)èƒ½åŠ›å’Œç«¯åˆ°ç«¯é€‚åº”æ€§çš„ç”¨æˆ·å¯¹é½å‹AI Copilotæä¾›äº†ç»Ÿä¸€çš„ç†è®ºåŸºç¡€ä¸å®è·µè®¾è®¡å‚è€ƒã€‚",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.HC"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.21907v2",
      "published_date": "2025-05-28 02:52:39 UTC",
      "updated_date": "2025-05-31 04:48:02 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T15:38:03.463170+00:00"
    },
    {
      "arxiv_id": "2505.21906v2",
      "title": "ChatVLA-2: Vision-Language-Action Model with Open-World Embodied Reasoning from Pretrained Knowledge",
      "title_zh": "ChatVLA-2ï¼šåŸºäºé¢„è®­ç»ƒçŸ¥è¯†å®ç°å¼€æ”¾ä¸–ç•Œå…·èº«æ¨ç†çš„è§†è§‰-è¯­è¨€-åŠ¨ä½œæ¨¡å‹",
      "authors": [
        "Zhongyi Zhou",
        "Yichen Zhu",
        "Junjie Wen",
        "Chaomin Shen",
        "Yi Xu"
      ],
      "abstract": "Vision-language-action (VLA) models have emerged as the next generation of models in robotics. However, despite leveraging powerful pre-trained Vision-Language Models (VLMs), existing end-to-end VLA systems often lose key capabilities during fine-tuning as the model adapts to specific robotic tasks. We argue that a generalizable VLA model should retain and expand upon the VLM's core competencies: 1) Open-world embodied reasoning - the VLA should inherit the knowledge from VLM, i.e., recognize anything that the VLM can recognize, be capable of solving math problems, and possess visual-spatial intelligence, 2) Reasoning following - effectively translating the open-world reasoning into actionable steps for the robot. In this work, we introduce ChatVLA-2, a novel mixture-of-expert VLA model coupled with a specialized two-stage training pipeline designed to preserve the VLM's original strengths while enabling actionable reasoning. To validate our approach, we design a math-matching task wherein a robot interprets math problems written on a whiteboard and picks corresponding number cards from a table to solve equations. Remarkably, our method exhibits exceptional mathematical reasoning and OCR capabilities, despite these abilities not being explicitly trained within the VLA. Furthermore, we demonstrate that the VLA possesses strong spatial reasoning skills, enabling it to interpret novel directional instructions involving previously unseen objects. Overall, our method showcases reasoning and comprehension abilities that significantly surpass state-of-the-art imitation learning methods such as OpenVLA, DexVLA, and pi-zero. This work represents a substantial advancement toward developing truly generalizable robotic foundation models endowed with robust reasoning capacities.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† ChatVLA-2ï¼Œè¿™æ˜¯ä¸€ç§åŸºäºæ··åˆä¸“å®¶ (Mixture-of-Expert) æ¶æ„çš„æ–°å‹ Vision-Language-Action (VLA) æ¨¡å‹ã€‚é’ˆå¯¹ç°æœ‰ VLA ç³»ç»Ÿåœ¨å¾®è°ƒè¿‡ç¨‹ä¸­å®¹æ˜“ä¸¢å¤±é¢„è®­ç»ƒ Vision-Language Models (VLMs) æ ¸å¿ƒèƒ½åŠ›çš„å±€é™æ€§ï¼Œè¯¥ç ”ç©¶è®¾è®¡äº†ä¸¤é˜¶æ®µè®­ç»ƒæµæ°´çº¿ï¼Œæ—¨åœ¨ä¿ç•™å¼€æ”¾ä¸–ç•Œå…·èº«æ¨ç† (Open-world embodied reasoning) èƒ½åŠ›å¹¶å®ç°ç²¾ç¡®çš„æ¨ç†éµå¾ª (Reasoning following)ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒChatVLA-2 åœ¨æœªç»è¿‡æ˜¾å¼è®­ç»ƒçš„æƒ…å†µä¸‹ï¼Œä»å±•ç°å‡ºå“è¶Šçš„æ•°å­¦æ¨ç†ã€å…‰å­¦å­—ç¬¦è¯†åˆ« (OCR) ä»¥åŠé’ˆå¯¹æœªè§ç‰©ä½“çš„ç©ºé—´æ¨ç†èƒ½åŠ›ã€‚å…¶ç»¼åˆç†è§£ä¸ä»»åŠ¡æ‰§è¡Œèƒ½åŠ›æ˜¾è‘—è¶…è¶Šäº† OpenVLAã€DexVLA å’Œ pi-zero ç­‰å…ˆè¿›çš„æ¨¡ä»¿å­¦ä¹ æ–¹æ³•ã€‚è¿™é¡¹å·¥ä½œä¸ºå¼€å‘å…·å¤‡å¼ºå¤§æ¨ç†èƒ½åŠ›çš„é€šç”¨å‹æœºå™¨äººåŸºç¡€æ¨¡å‹å¥ å®šäº†é‡è¦åŸºç¡€ã€‚",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.RO",
      "comment": "Project page: https://chatvla-2.github.io/",
      "pdf_url": "https://arxiv.org/pdf/2505.21906v2",
      "published_date": "2025-05-28 02:48:42 UTC",
      "updated_date": "2025-05-29 23:34:24 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T15:39:05.359005+00:00"
    },
    {
      "arxiv_id": "2505.21904v4",
      "title": "CAST: Contrastive Adaptation and Distillation for Semi-Supervised Instance Segmentation",
      "title_zh": "CASTï¼šé¢å‘åŠç›‘ç£å®ä¾‹åˆ†å‰²çš„å¯¹æ¯”å¼è‡ªé€‚åº”ä¸è’¸é¦",
      "authors": [
        "Pardis Taghavi",
        "Tian Liu",
        "Renjie Li",
        "Reza Langari",
        "Zhengzhong Tu"
      ],
      "abstract": "Instance segmentation demands costly per-pixel annotations and computationally expensive models. We introduce CAST, a semi-supervised knowledge distillation (SSKD) framework that compresses pre-trained vision foundation models (VFM) into compact experts using limited labeled and abundant unlabeled data. CAST unfolds in three stages: (1) domain adaptation of the VFM(s) via self-training with contrastive calibration, (2) knowledge transfer through a unified multi-objective loss, and (3) student refinement to mitigate residual pseudo-label bias. Central to CAST is an \\emph{instance-aware pixel-wise contrastive loss} that fuses mask and class scores to extract informative negatives and enforce clear inter-instance margins. By maintaining this contrastive signal across both adaptation and distillation, we align teacher and student embeddings and fully leverage unlabeled images. On Cityscapes and ADE20K, our ~11x smaller student improves over its zero-shot VFM teacher(s) by +8.5 and +7.1 AP, surpasses adapted teacher(s) by +3.4 and +1.5 AP, and further outperforms state-of-the-art SSKD methods on both benchmarks.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†CASTï¼Œä¸€ç§æ—¨åœ¨è§£å†³å®ä¾‹åˆ†å‰²æ ‡æ³¨æˆæœ¬é«˜ä¸”è®¡ç®—å¤æ‚çš„åŠç›‘ç£çŸ¥è¯†è’¸é¦(SSKD)æ¡†æ¶ã€‚è¯¥æ¡†æ¶é€šè¿‡å°†é¢„è®­ç»ƒçš„è§†è§‰åŸºç¡€æ¨¡å‹(VFM)å‹ç¼©ä¸ºç´§å‡‘å‹ä¸“å®¶æ¨¡å‹ï¼Œæœ‰æ•ˆåˆ©ç”¨äº†æœ‰é™çš„æœ‰æ ‡ç­¾æ•°æ®å’Œæµ·é‡çš„æ— æ ‡ç­¾æ•°æ®ã€‚CASTçš„å·¥ä½œæµç¨‹åˆ†ä¸ºä¸‰ä¸ªé˜¶æ®µï¼šç»“åˆå¯¹æ¯”æ ¡å‡†çš„VFMé¢†åŸŸè‡ªé€‚åº”ã€ç»Ÿä¸€å¤šç›®æ ‡æŸå¤±çš„çŸ¥è¯†è½¬ç§»ï¼Œä»¥åŠæ—¨åœ¨å‡è½»ä¼ªæ ‡ç­¾åå·®çš„å­¦ç”Ÿæ¨¡å‹ç»†åŒ–ã€‚å…¶æ ¸å¿ƒåˆ›æ–°åœ¨äºå¼•å…¥äº†å®ä¾‹æ„ŸçŸ¥çš„åƒç´ çº§å¯¹æ¯”æŸå¤±(instance-aware pixel-wise contrastive loss)ï¼Œé€šè¿‡èåˆæ©ç ä¸ç±»åˆ«è¯„åˆ†æ¥æå–å…³é”®è´Ÿæ ·æœ¬å¹¶å¼ºåŒ–å®ä¾‹é—´çš„è¾¹ç•Œã€‚åœ¨Cityscapeså’ŒADE20KåŸºå‡†æµ‹è¯•ä¸­ï¼Œä½“ç§¯ç¼©å°çº¦11å€çš„å­¦ç”Ÿæ¨¡å‹æ€§èƒ½ä¸ä»…å¤§å¹…è¶…è¶Šäº†é›¶æ ·æœ¬VFMæ•™å¸ˆæ¨¡å‹ï¼Œè¿˜æ˜¾è‘—ä¼˜äºç°æœ‰çš„SSKDå‰æ²¿æ–¹æ³•ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.21904v4",
      "published_date": "2025-05-28 02:45:42 UTC",
      "updated_date": "2025-10-08 21:00:36 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T15:38:03.135276+00:00"
    },
    {
      "arxiv_id": "2505.21898v1",
      "title": "Co-Saving: Resource Aware Multi-Agent Collaboration for Software Development",
      "title_zh": "Co-Savingï¼šé¢å‘è½¯ä»¶å¼€å‘çš„èµ„æºæ„ŸçŸ¥å‹å¤šæ™ºèƒ½ä½“åä½œ",
      "authors": [
        "Rennai Qiu",
        "Chen Qian",
        "Ran Li",
        "Yufan Dang",
        "Weize Chen",
        "Cheng Yang",
        "Yingli Zhang",
        "Ye Tian",
        "Xuantang Xiong",
        "Lei Han",
        "Zhiyuan Liu",
        "Maosong Sun"
      ],
      "abstract": "Recent advancements in Large Language Models (LLMs) and autonomous agents have demonstrated remarkable capabilities across various domains. However, standalone agents frequently encounter limitations when handling complex tasks that demand extensive interactions and substantial computational resources. Although Multi-Agent Systems (MAS) alleviate some of these limitations through collaborative mechanisms like task decomposition, iterative communication, and role specialization, they typically remain resource-unaware, incurring significant inefficiencies due to high token consumption and excessive execution time. To address these limitations, we propose a resource-aware multi-agent system -- Co-Saving (meaning that multiple agents collaboratively engage in resource-saving activities), which leverages experiential knowledge to enhance operational efficiency and solution quality. Our key innovation is the introduction of \"shortcuts\" -- instructional transitions learned from historically successful trajectories -- which allows to bypass redundant reasoning agents and expedite the collective problem-solving process. Experiments for software development tasks demonstrate significant advantages over existing methods. Specifically, compared to the state-of-the-art MAS ChatDev, our method achieves an average reduction of 50.85% in token usage, and improves the overall code quality by 10.06%.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†Co-Savingï¼Œè¿™æ˜¯ä¸€ç§é¢å‘è½¯ä»¶å¼€å‘çš„èµ„æºæ„ŸçŸ¥(Resource Aware)å¤šæ™ºèƒ½ä½“åä½œ(Multi-Agent Collaboration)æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³ç°æœ‰ç³»ç»Ÿ(MAS)åœ¨å¤„ç†å¤æ‚ä»»åŠ¡æ—¶å› Tokenæ¶ˆè€—é«˜å’Œæ‰§è¡Œæ—¶é—´é•¿è€Œå¯¼è‡´çš„ä½æ•ˆç‡é—®é¢˜ã€‚Co-Savingé€šè¿‡åˆ©ç”¨ç»éªŒçŸ¥è¯†æ¥å¢å¼ºæ“ä½œæ•ˆç‡ï¼Œå…¶æ ¸å¿ƒåˆ›æ–°æ˜¯å¼•å…¥äº†â€œæ·å¾„â€(Shortcuts)ï¼Œå³ä»å†å²æˆåŠŸè½¨è¿¹ä¸­å­¦ä¹ åˆ°çš„æŒ‡ä»¤æ€§è½¬æ¢ï¼Œä½¿ç³»ç»Ÿèƒ½å¤Ÿç»•è¿‡å†—ä½™çš„æ¨ç†æ™ºèƒ½ä½“å¹¶åŠ é€Ÿé›†ä½“é—®é¢˜è§£å†³è¿‡ç¨‹ã€‚åœ¨è½¯ä»¶å¼€å‘ä»»åŠ¡çš„å®éªŒä¸­ï¼Œè¯¥æ–¹æ³•å±•ç°å‡ºä¼˜äºç°æœ‰æ–¹æ³•çš„æ˜¾è‘—ä¼˜åŠ¿ã€‚ç›¸æ¯”äºå½“å‰çš„SOTAæ¨¡å‹ChatDevï¼ŒCo-Savingå¹³å‡å‡å°‘äº†50.85%çš„Tokenä½¿ç”¨é‡ï¼Œå¹¶å°†æ•´ä½“ä»£ç è´¨é‡æå‡äº†10.06%ã€‚è¯¥ç³»ç»ŸæˆåŠŸè¯æ˜äº†é€šè¿‡èµ„æºæ„ŸçŸ¥æœºåˆ¶ï¼Œå¯ä»¥åœ¨å¤§å¹…é™ä½è®¡ç®—èµ„æºæ¶ˆè€—çš„åŒæ—¶ï¼Œæœ‰æ•ˆæå‡è½¯ä»¶ç”Ÿæˆçš„æ–¹æ¡ˆè´¨é‡ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.MA",
        "cs.SE"
      ],
      "primary_category": "cs.CL",
      "comment": "Work in Progress",
      "pdf_url": "https://arxiv.org/pdf/2505.21898v1",
      "published_date": "2025-05-28 02:23:53 UTC",
      "updated_date": "2025-05-28 02:23:53 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T15:39:30.947505+00:00"
    },
    {
      "arxiv_id": "2506.02014v1",
      "title": "Research on Driving Scenario Technology Based on Multimodal Large Lauguage Model Optimization",
      "title_zh": "åŸºäºå¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ä¼˜åŒ–çš„é©¾é©¶åœºæ™¯æŠ€æœ¯ç ”ç©¶",
      "authors": [
        "Wang Mengjie",
        "Zhu Huiping",
        "Li Jian",
        "Shi Wenxiu",
        "Zhang Song"
      ],
      "abstract": "With the advancement of autonomous and assisted driving technologies, higher demands are placed on the ability to understand complex driving scenarios. Multimodal general large models have emerged as a solution for this challenge. However, applying these models in vertical domains involves difficulties such as data collection, model training, and deployment optimization. This paper proposes a comprehensive method for optimizing multimodal models in driving scenarios, including cone detection, traffic light recognition, speed limit recommendation, and intersection alerts. The method covers key aspects such as dynamic prompt optimization, dataset construction, model training, and deployment. Specifically, the dynamic prompt optimization adjusts the prompts based on the input image content to focus on objects affecting the ego vehicle, enhancing the model's task-specific focus and judgment capabilities. The dataset is constructed by combining real and synthetic data to create a high-quality and diverse multimodal training dataset, improving the model's generalization in complex driving environments. In model training, advanced techniques like knowledge distillation, dynamic fine-tuning, and quantization are integrated to reduce storage and computational costs while boosting performance. Experimental results show that this systematic optimization method not only significantly improves the model's accuracy in key tasks but also achieves efficient resource utilization, providing strong support for the practical application of driving scenario perception technologies.",
      "tldr_zh": "è¿™é¡¹ç ”ç©¶é’ˆå¯¹è‡ªåŠ¨é©¾é©¶ä¸­å¤æ‚é©¾é©¶åœºæ™¯çš„ç†è§£æŒ‘æˆ˜ï¼Œæå‡ºäº†ä¸€ç§åŸºäºå¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ (Multimodal Large Language Model) ä¼˜åŒ–çš„ç»¼åˆé©±åŠ¨åœºæ™¯æŠ€æœ¯æ–¹æ¡ˆã€‚è¯¥æ–¹æ³•æ¶µç›–äº†äº¤é€šé”¥æ£€æµ‹ã€çº¢ç»¿ç¯è¯†åˆ«åŠé™é€Ÿå»ºè®®ç­‰å…³é”®ä»»åŠ¡ï¼Œå¹¶åˆ›æ–°æ€§åœ°å¼•å…¥äº†åŠ¨æ€æç¤ºä¼˜åŒ– (Dynamic Prompt Optimization)ï¼Œé€šè¿‡å®æ—¶è°ƒæ•´æç¤ºè¯æ¥å¢å¼ºæ¨¡å‹å¯¹å½±å“ä¸»è½¦ç›®æ ‡çš„å…³æ³¨åº¦å’Œåˆ¤æ–­åŠ›ã€‚åœ¨æ•°æ®é›†æ„å»ºæ–¹é¢ï¼Œç ”ç©¶ç»“åˆçœŸå®æ•°æ®ä¸åˆæˆæ•°æ®å»ºç«‹äº†å¤šæ ·åŒ–çš„è®­ç»ƒé›†ï¼Œæœ‰æ•ˆæå‡äº†æ¨¡å‹åœ¨å¤æ‚ç¯å¢ƒä¸‹çš„æ³›åŒ–èƒ½åŠ› (Generalization)ã€‚åŒæ—¶ï¼Œç ”ç©¶é›†æˆäº†çŸ¥è¯†è’¸é¦ (Knowledge Distillation)ã€åŠ¨æ€å¾®è°ƒ (Dynamic Fine-tuning) å’Œé‡åŒ– (Quantization) ç­‰å…ˆè¿›æŠ€æœ¯ï¼Œåœ¨é™ä½å­˜å‚¨å’Œè®¡ç®—æˆæœ¬çš„åŒæ—¶è¿›ä¸€æ­¥æå‡äº†æ¨¡å‹æ€§èƒ½ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¿™ç§ç³»ç»Ÿæ€§ä¼˜åŒ–æ–¹æ³•æ˜¾è‘—æé«˜äº†æ¨¡å‹åœ¨å…³é”®æ„ŸçŸ¥ä»»åŠ¡ä¸­çš„å‡†ç¡®ç‡å¹¶å®ç°äº†é«˜æ•ˆçš„èµ„æºåˆ©ç”¨ã€‚è¯¥æˆæœä¸ºå¤šæ¨¡æ€å¤§æ¨¡å‹åœ¨è‡ªåŠ¨é©¾é©¶å‚ç›´é¢†åŸŸçš„å®é™…éƒ¨ç½²ä¸åº”ç”¨æä¾›äº†é‡è¦çš„æŠ€æœ¯æ”¯æ’‘ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.02014v1",
      "published_date": "2025-05-28 02:22:11 UTC",
      "updated_date": "2025-05-28 02:22:11 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T15:39:43.307287+00:00"
    },
    {
      "arxiv_id": "2505.21895v2",
      "title": "SineLoRA$Î”$: Sine-Activated Delta Compression",
      "title_zh": "SineLoRA$Î”$ï¼šæ­£å¼¦æ¿€æ´»çš„å¢é‡å‹ç¼©",
      "authors": [
        "Cameron Gordon",
        "Yiping Ji",
        "Hemanth Saratchandran",
        "Paul Albert",
        "Simon Lucey"
      ],
      "abstract": "Resource-constrained weight deployment is a task of immense practical importance. Recently, there has been interest in the specific task of \\textit{Delta Compression}, where parties each hold a common base model and only communicate compressed weight updates. However, popular parameter efficient updates such as Low Rank Adaptation (LoRA) face inherent representation limitations - which are especially pronounced when combined with aggressive quantization. To overcome this, we build on recent work that improves LoRA representation capacity by using fixed-frequency sinusoidal functions to increase stable rank without adding additional parameters. We extend this to the quantized setting and present the first theoretical analysis showing how stable rank evolves under quantization. From this, we introduce SineLoRA$Î”$, a principled and effective method for delta compression that improves the expressivity of quantized low-rank adapters by applying a sinusoidal activation. We validate SineLoRA$Î”$ across a diverse variety of domains - including language modeling, vision-language tasks, and text-to-image generation - achieving up to 66% memory reduction with similar performance. We additionally provide a novel application of the canonical BjÃ¸ntegaard Delta metric to consistently compare adapter compression changes across the rate-distortion curve.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† SineLoRA$\\Delta$ï¼Œä¸€ç§ä¸“é—¨é’ˆå¯¹ Delta Compression ä»»åŠ¡è®¾è®¡çš„æ­£å¼¦æ¿€æ´»å¢é‡å‹ç¼©æ–¹æ³•ï¼Œæ—¨åœ¨è§£å†³å—é™èµ„æºç¯å¢ƒä¸‹çš„æ¨¡å‹æƒé‡éƒ¨ç½²é—®é¢˜ã€‚é’ˆå¯¹ Low Rank Adaptation (LoRA) åœ¨ç»“åˆæ¿€è¿›é‡åŒ–ï¼ˆQuantizationï¼‰æ—¶é¢ä¸´çš„è¡¨è¾¾èƒ½åŠ›å—é™é—®é¢˜ï¼Œè¯¥æ–¹æ³•é€šè¿‡å¼•å…¥å›ºå®šé¢‘ç‡çš„æ­£å¼¦å‡½æ•°æ¿€æ´»æ¥æå‡æ¨¡å‹çš„ Stable Rankï¼Œä¸”ä¸å¢åŠ é¢å¤–å‚æ•°ã€‚ä½œè€…æä¾›äº†å…³äºé‡åŒ–è¿‡ç¨‹ä¸­ Stable Rank æ¼”å˜çš„é¦–ä¸ªç†è®ºåˆ†æï¼Œä¸º SineLoRA$\\Delta$ åœ¨å¢å¼ºé‡åŒ–ä½ç§©é€‚é…å™¨è¡¨è¾¾èƒ½åŠ›æ–¹é¢çš„æœ‰æ•ˆæ€§å¥ å®šäº†ç†è®ºåŸºç¡€ã€‚ç ”ç©¶åœ¨è¯­è¨€å»ºæ¨¡ã€è§†è§‰è¯­è¨€ä»»åŠ¡åŠæ–‡æœ¬ç”Ÿæˆå›¾åƒç­‰å¤šä¸ªé¢†åŸŸè¿›è¡Œäº†å¹¿æ³›éªŒè¯ï¼Œç»“æœè¡¨æ˜è¯¥æ–¹æ³•åœ¨ä¿æŒç›¸ä¼¼æ€§èƒ½çš„å‰æä¸‹ï¼Œèƒ½å¤Ÿå®ç°é«˜è¾¾ 66% çš„å†…å­˜ç¼©å‡ã€‚æ­¤å¤–ï¼Œè¯¥ç ”ç©¶è¿˜åˆ›æ–°æ€§åœ°å¼•å…¥äº† BjÃ¸ntegaard Delta æŒ‡æ ‡ï¼Œä»¥ä¾¿åœ¨ç‡å¤±çœŸæ›²çº¿ä¸Šå¯¹é€‚é…å™¨çš„å‹ç¼©æ•ˆæœè¿›è¡Œä¸€è‡´æ€§è¯„ä¼°ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "Accepted by AAAI2026",
      "pdf_url": "https://arxiv.org/pdf/2505.21895v2",
      "published_date": "2025-05-28 02:15:15 UTC",
      "updated_date": "2025-11-17 07:29:52 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T15:39:57.657985+00:00"
    },
    {
      "arxiv_id": "2505.21893v2",
      "title": "SDPO: Importance-Sampled Direct Preference Optimization for Stable Diffusion Training",
      "title_zh": "SDPOï¼šé¢å‘ Stable Diffusion è®­ç»ƒçš„é‡è¦æ€§é‡‡æ ·ç›´æ¥åå¥½ä¼˜åŒ–",
      "authors": [
        "Xiaomeng Yang",
        "Zhiyu Tan",
        "Junyan Wang",
        "Zhijian Zhou",
        "Hao Li"
      ],
      "abstract": "Preference learning has become a central technique for aligning generative models with human expectations. Recently, it has been extended to diffusion models through methods like Direct Preference Optimization (DPO). However, existing approaches such as Diffusion-DPO suffer from two key challenges: timestep-dependent instability, caused by a mismatch between the reverse and forward diffusion processes and by high gradient variance in early noisy timesteps, and off-policy bias arising from the mismatch between optimization and data collection policies. We begin by analyzing the reverse diffusion trajectory and observe that instability primarily occurs at early timesteps with low importance weights. To address these issues, we first propose DPO-C\\&M, a practical strategy that improves stability by clipping and masking uninformative timesteps while partially mitigating off-policy bias. Building on this, we introduce SDPO (Importance-Sampled Direct Preference Optimization), a principled framework that incorporates importance sampling into the objective to fully correct for off-policy bias and emphasize informative updates during the diffusion process. Experiments on CogVideoX-2B, CogVideoX-5B, and Wan2.1-1.3B demonstrate that both methods outperform standard Diffusion-DPO, with SDPO achieving superior VBench scores, human preference alignment, and training robustness. These results highlight the importance of timestep-aware, distribution-corrected optimization in diffusion-based preference learning.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹æ‰©æ•£æ¨¡å‹åå¥½å­¦ä¹ ä¸­ Direct Preference Optimization (DPO) é¢ä¸´çš„æ—¶é—´æ­¥ç›¸å…³ä¸ç¨³å®šæ€§ (timestep-dependent instability) ä»¥åŠç­–ç•¥åç§»åå·® (off-policy bias) ç­‰æ ¸å¿ƒæŒ‘æˆ˜è¿›è¡Œäº†æ·±å…¥åˆ†æã€‚ä½œè€…å‘ç°ï¼Œè®­ç»ƒä¸ç¨³å®šæ€§ä¸»è¦æºäºæ—©æœŸå™ªå£°è¾ƒå¤šçš„æ—¶é—´æ­¥ä¸åå‘æ‰©æ•£è½¨è¿¹ä¹‹é—´çš„ä¸åŒ¹é…ï¼Œä¸”è¿™äº›æ­¥éª¤çš„é‡è¦æ€§æƒé‡è¾ƒä½ã€‚ä¸ºæ­¤ï¼Œç ”ç©¶é¦–å…ˆæå‡ºäº† DPO-C&M ç­–ç•¥ï¼Œé€šè¿‡è£å‰ª (clipping) å’Œé®è”½ (masking) æ— ä¿¡æ¯çš„æ—¶é—´æ­¥æ¥å¢å¼ºç¨³å®šæ€§å¹¶åˆæ­¥ç¼“è§£ç­–ç•¥åå·®ã€‚åœ¨æ­¤åŸºç¡€ä¸Šï¼Œè¯¥ç ”ç©¶è¿›ä¸€æ­¥å¼•å…¥äº† SDPO (Importance-Sampled Direct Preference Optimization)ï¼Œè¿™æ˜¯ä¸€ä¸ªå°†é‡è¦æ€§é‡‡æ · (importance sampling) æ•´åˆè¿›ç›®æ ‡å‡½æ•°çš„ç†è®ºæ¡†æ¶ï¼Œæ—¨åœ¨å®Œå…¨çº æ­£ç­–ç•¥åç§»å¹¶å¼ºè°ƒæ‰©æ•£è¿‡ç¨‹ä¸­çš„æœ‰æ•ˆæ›´æ–°ã€‚åœ¨ CogVideoX-2Bã€CogVideoX-5B å’Œ Wan2.1-1.3B ç­‰å¤§å‹ç”Ÿæˆæ¨¡å‹ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼ŒSDPO åœ¨ VBench è¯„åˆ†ã€äººç±»åå¥½å¯¹é½ä»¥åŠè®­ç»ƒç¨³å¥æ€§æ–¹é¢å‡æ˜¾è‘—ä¼˜äºæ ‡å‡†çš„ Diffusion-DPOã€‚è¯¥å·¥ä½œå‡¸æ˜¾äº†åœ¨åŸºäºæ‰©æ•£çš„åå¥½å­¦ä¹ ä¸­ï¼Œè¿›è¡Œæ—¶é—´æ­¥æ„ŸçŸ¥ (timestep-aware) å’Œåˆ†å¸ƒæ ¡æ­£ (distribution-corrected) ä¼˜åŒ–çš„å…³é”®ä½œç”¨ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "This version contains a critical error in the main theorem and proof design that affects the validity of the results",
      "pdf_url": "https://arxiv.org/pdf/2505.21893v2",
      "published_date": "2025-05-28 02:11:56 UTC",
      "updated_date": "2025-09-25 13:07:51 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T15:39:48.612196+00:00"
    },
    {
      "arxiv_id": "2505.21887v2",
      "title": "SVRPBench: A Realistic Benchmark for Stochastic Vehicle Routing Problem",
      "title_zh": "SVRPBenchï¼šé¢å‘éšæœºè½¦è¾†è·¯å¾„é—®é¢˜çš„ç°å®åœºæ™¯åŸºå‡†æµ‹è¯•",
      "authors": [
        "Ahmed Heakl",
        "Yahia Salaheldin Shaaban",
        "Martin Takac",
        "Salem Lahlou",
        "Zangir Iklassov"
      ],
      "abstract": "Robust routing under uncertainty is central to real-world logistics, yet most benchmarks assume static, idealized settings. We present SVRPBench, the first open benchmark to capture high-fidelity stochastic dynamics in vehicle routing at urban scale. Spanning more than 500 instances with up to 1000 customers, it simulates realistic delivery conditions: time-dependent congestion, log-normal delays, probabilistic accidents, and empirically grounded time windows for residential and commercial clients. Our pipeline generates diverse, constraint-rich scenarios, including multi-depot and multi-vehicle setups. Benchmarking reveals that state-of-the-art RL solvers like POMO and AM degrade by over 20% under distributional shift, while classical and metaheuristic methods remain robust. To enable reproducible research, we release the dataset and evaluation suite. SVRPBench challenges the community to design solvers that generalize beyond synthetic assumptions and adapt to real-world uncertainty.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† SVRPBenchï¼Œè¿™æ˜¯é¦–ä¸ªåœ¨åŸå¸‚è§„æ¨¡ä¸‹æ•æ‰é«˜ä¿çœŸéšæœºåŠ¨æ€çš„ Stochastic Vehicle Routing Problem (SVRP) å¼€æºåŸºå‡†ã€‚è¯¥åŸºå‡†æ¶µç›– 500 å¤šä¸ªå®ä¾‹å’Œå¤šè¾¾ 1000 ä¸ªå®¢æˆ·ï¼Œæ¨¡æ‹Ÿäº†éšæ—¶é—´å˜åŒ–çš„äº¤é€šæ‹¥å µã€log-normal delaysã€æ¦‚ç‡æ€§äº‹æ•…ä»¥åŠå…·æœ‰å®è¯åŸºç¡€çš„æ—¶é—´çª—ç­‰çœŸå®é…é€æ¡ä»¶ã€‚SVRPBench æ”¯æŒ multi-depot å’Œ multi-vehicle ç­‰å¤šæ ·åŒ–çº¦æŸåœºæ™¯ï¼Œæ—¨åœ¨æŒ‘æˆ˜æ±‚è§£å™¨åœ¨ç°å®ä¸–ç•Œä¸ç¡®å®šæ€§ä¸‹çš„è¡¨ç°ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒPOMO å’Œ AM ç­‰å…ˆè¿›çš„å¼ºåŒ–å­¦ä¹  (RL) æ±‚è§£å™¨åœ¨ distributional shift ä¸‹æ€§èƒ½ä¸‹é™è¶…è¿‡ 20%ï¼Œè€Œç»å…¸ç®—æ³•å’Œ metaheuristic æ–¹æ³•å±•ç°å‡ºæ›´å¼ºçš„é²æ£’æ€§ã€‚ç ”ç©¶å›¢é˜Ÿå¼€æºäº†è¯¥æ•°æ®é›†å’Œè¯„ä¼°å¥—ä»¶ï¼Œæ—¨åœ¨æ¨åŠ¨ç¤¾åŒºå¼€å‘èƒ½å¤Ÿè¶…è¶Š synthetic assumptions å¹¶é€‚åº”å¤æ‚ç°å®åŠ¨æ€çš„é«˜æ³›åŒ–æ€§æ±‚è§£å™¨ã€‚",
      "categories": [
        "cs.AI",
        "cs.CE",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "18 pages, 14 figures, 11 tables",
      "pdf_url": "https://arxiv.org/pdf/2505.21887v2",
      "published_date": "2025-05-28 02:03:31 UTC",
      "updated_date": "2025-05-29 17:17:47 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T15:40:14.876397+00:00"
    },
    {
      "arxiv_id": "2505.21880v2",
      "title": "Incorporating LLMs for Large-Scale Urban Complex Mobility Simulation",
      "title_zh": "èåˆå¤§è¯­è¨€æ¨¡å‹çš„å¤§è§„æ¨¡åŸå¸‚å¤æ‚å‡ºè¡Œæ¨¡æ‹Ÿ",
      "authors": [
        "Yu-Lun Song",
        "Chung-En Tsern",
        "Che-Cheng Wu",
        "Yu-Ming Chang",
        "Syuan-Bo Huang",
        "Wei-Chu Chen",
        "Michael Chia-Liang Lin",
        "Yu-Ta Lin"
      ],
      "abstract": "This study presents an innovative approach to urban mobility simulation by integrating a Large Language Model (LLM) with Agent-Based Modeling (ABM). Unlike traditional rule-based ABM, the proposed framework leverages LLM to enhance agent diversity and realism by generating synthetic population profiles, allocating routine and occasional locations, and simulating personalized routes. Using real-world data, the simulation models individual behaviors and large-scale mobility patterns in Taipei City. Key insights, such as route heat maps and mode-specific indicators, provide urban planners with actionable information for policy-making. Future work focuses on establishing robust validation frameworks to ensure accuracy and reliability in urban planning applications.",
      "tldr_zh": "æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§åˆ›æ–°çš„åŸå¸‚äº¤é€šæ¨¡æ‹Ÿæ–¹æ³•ï¼Œé€šè¿‡å°†å¤§è¯­è¨€æ¨¡å‹(LLM)ä¸åŸºäºæ™ºèƒ½ä½“çš„å»ºæ¨¡(ABM)ç›¸ç»“åˆï¼Œæ—¨åœ¨è§£å†³ä¼ ç»Ÿè§„åˆ™åŒ–ABMåœ¨æ¨¡æ‹Ÿå¤šæ ·æ€§å’ŒçœŸå®æ„Ÿæ–¹é¢çš„å±€é™ã€‚è¯¥æ¡†æ¶åˆ©ç”¨LLMç”Ÿæˆåˆæˆäººå£ç‰¹å¾(synthetic population profiles)ï¼Œå¹¶ä¸ºæ™ºèƒ½ä½“åˆ†é…å¸¸è§„åŠå¶ç„¶åœ°ç‚¹ï¼Œä»è€Œå®ç°äº†é«˜åº¦ä¸ªæ€§åŒ–çš„è·¯å¾„æ¨¡æ‹Ÿã€‚ç ”ç©¶åŸºäºå°åŒ—å¸‚çš„çœŸå®æ•°æ®ï¼ŒæˆåŠŸæ¨¡æ‹Ÿäº†ä¸ªä½“è¡Œä¸ºåŠå¤§è§„æ¨¡ç§»åŠ¨æ¨¡å¼ï¼Œå¹¶è¾“å‡ºäº†è·¯å¾„çƒ­åŠ›å›¾(route heat maps)å’Œç‰¹å®šäº¤é€šæ¨¡å¼æŒ‡æ ‡ã€‚å®éªŒç»“æœè¯æ˜ï¼Œè¯¥æ–¹æ³•èƒ½ä¸ºåŸå¸‚è§„åˆ’è€…æä¾›å¯æ“ä½œçš„æ”¿ç­–åˆ¶å®šä¾æ®ï¼Œæ•æ‰åˆ°å¤æ‚çš„åŸå¸‚æµåŠ¨æ€§ç‰¹å¾ã€‚æœªæ¥ç ”ç©¶å°†ä¾§é‡äºå»ºç«‹ç¨³å¥çš„éªŒè¯æ¡†æ¶ï¼Œä»¥è¿›ä¸€æ­¥ç¡®ä¿è¯¥æŠ€æœ¯åœ¨åŸå¸‚è§„åˆ’å®é™…åº”ç”¨ä¸­çš„å‡†ç¡®æ€§ä¸å¯é æ€§ã€‚",
      "categories": [
        "cs.MA",
        "cs.AI",
        "cs.CL",
        "cs.CY"
      ],
      "primary_category": "cs.MA",
      "comment": "8 pages, 8 figures. This paper is reviewed and accepted by the CUPUM (Computational Urban Planning and Urban Management) Conference held by University College London (UCL) in 2025",
      "pdf_url": "https://arxiv.org/pdf/2505.21880v2",
      "published_date": "2025-05-28 01:54:28 UTC",
      "updated_date": "2025-07-03 06:38:34 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T15:40:03.052639+00:00"
    },
    {
      "arxiv_id": "2505.21879v1",
      "title": "Symbolic Foundation Regressor on Complex Networks",
      "title_zh": "é¢å‘å¤æ‚ç½‘ç»œçš„ç¬¦å·åŸºç¡€å›å½’å™¨",
      "authors": [
        "Weiting Liu",
        "Jiaxu Cui",
        "Jiao Hu",
        "En Wang",
        "Bo Yang"
      ],
      "abstract": "In science, we are interested not only in forecasting but also in understanding how predictions are made, specifically what the interpretable underlying model looks like. Data-driven machine learning technology can significantly streamline the complex and time-consuming traditional manual process of discovering scientific laws, helping us gain insights into fundamental issues in modern science. In this work, we introduce a pre-trained symbolic foundation regressor that can effectively compress complex data with numerous interacting variables while producing interpretable physical representations. Our model has been rigorously tested on non-network symbolic regression, symbolic regression on complex networks, and the inference of network dynamics across various domains, including physics, biochemistry, ecology, and epidemiology. The results indicate a remarkable improvement in equation inference efficiency, being three times more effective than baseline approaches while maintaining accurate predictions. Furthermore, we apply our model to uncover more intuitive laws of interaction transmission from global epidemic outbreak data, achieving optimal data fitting. This model extends the application boundary of pre-trained symbolic regression models to complex networks, and we believe it provides a foundational solution for revealing the hidden mechanisms behind changes in complex phenomena, enhancing interpretability, and inspiring further scientific discoveries.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ç§‘å­¦è§„å¾‹å‘ç°ä¸­å¯¹æ¨¡å‹å¯è§£é‡Šæ€§çš„éœ€æ±‚ï¼Œæå‡ºäº†ä¸€ç§é¢„è®­ç»ƒçš„ç¬¦å·åŸºç¡€å›å½’å™¨ (Symbolic Foundation Regressor)ï¼Œæ—¨åœ¨ä»å¤æ‚æ•°æ®ä¸­æå–å¹¶ç”Ÿæˆå¯è§£é‡Šçš„ç‰©ç†è¡¨ç¤ºã€‚è¯¥æ¨¡å‹åœ¨éç½‘ç»œç¬¦å·å›å½’ (Non-network Symbolic Regression)ã€å¤æ‚ç½‘ç»œç¬¦å·å›å½’ (Symbolic Regression on Complex Networks) ä»¥åŠæ¶µç›–ç‰©ç†ã€ç”ŸåŒ–ã€ç”Ÿæ€å’Œæµè¡Œç—…å­¦ç­‰å¤šä¸ªé¢†åŸŸçš„ç½‘ç»œåŠ¨åŠ›å­¦æ¨ç† (Inference of Network Dynamics) ä¸­å‡è¡¨ç°å‡ºè‰²ã€‚å®éªŒç»“æœè¯æ˜ï¼Œè¯¥æ¨¡å‹åœ¨æ–¹ç¨‹æ¨ç†æ•ˆç‡ä¸Šæ¯”ç°æœ‰åŸºçº¿æ–¹æ³•æé«˜äº†ä¸‰å€ï¼ŒåŒæ—¶ç¡®ä¿äº†é¢„æµ‹çš„å‡†ç¡®æ€§ã€‚é€šè¿‡å¯¹å…¨çƒæµè¡Œç—…çˆ†å‘æ•°æ®çš„å®é™…åº”ç”¨ï¼Œæ¨¡å‹æ­ç¤ºäº†ç›´è§‚çš„ç›¸äº’ä½œç”¨ä¼ é€’è§„å¾‹å¹¶å®ç°äº†æœ€ä¼˜çš„æ•°æ®æ‹Ÿåˆæ•ˆæœã€‚è¯¥ç ”ç©¶æˆåŠŸå°†é¢„è®­ç»ƒç¬¦å·å›å½’æ¨¡å‹æ‰©å±•è‡³å¤æ‚ç½‘ç»œé¢†åŸŸï¼Œä¸ºæ­ç¤ºå¤æ‚ç°è±¡èƒŒåçš„éšè—æœºåˆ¶æä¾›äº†åŸºç¡€æ€§çš„è§£å†³æ–¹æ¡ˆï¼Œæ˜¾è‘—å¢å¼ºäº†ç§‘å­¦å‘ç°çš„å¯è§£é‡Šæ€§ã€‚",
      "categories": [
        "cs.SC",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.SC",
      "comment": "60 pages",
      "pdf_url": "https://arxiv.org/pdf/2505.21879v1",
      "published_date": "2025-05-28 01:53:29 UTC",
      "updated_date": "2025-05-28 01:53:29 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T15:40:17.651182+00:00"
    },
    {
      "arxiv_id": "2505.21876v1",
      "title": "EPiC: Efficient Video Camera Control Learning with Precise Anchor-Video Guidance",
      "title_zh": "EPiCï¼šåŸºäºç²¾å‡†é”šç‚¹è§†é¢‘å¼•å¯¼çš„é«˜æ•ˆè§†é¢‘ç›¸æœºæ§åˆ¶å­¦ä¹ ",
      "authors": [
        "Zun Wang",
        "Jaemin Cho",
        "Jialu Li",
        "Han Lin",
        "Jaehong Yoon",
        "Yue Zhang",
        "Mohit Bansal"
      ],
      "abstract": "Recent approaches on 3D camera control in video diffusion models (VDMs) often create anchor videos to guide diffusion models as a structured prior by rendering from estimated point clouds following annotated camera trajectories. However, errors inherent in point cloud estimation often lead to inaccurate anchor videos. Moreover, the requirement for extensive camera trajectory annotations further increases resource demands. To address these limitations, we introduce EPiC, an efficient and precise camera control learning framework that automatically constructs high-quality anchor videos without expensive camera trajectory annotations. Concretely, we create highly precise anchor videos for training by masking source videos based on first-frame visibility. This approach ensures high alignment, eliminates the need for camera trajectory annotations, and thus can be readily applied to any in-the-wild video to generate image-to-video (I2V) training pairs. Furthermore, we introduce Anchor-ControlNet, a lightweight conditioning module that integrates anchor video guidance in visible regions to pretrained VDMs, with less than 1% of backbone model parameters. By combining the proposed anchor video data and ControlNet module, EPiC achieves efficient training with substantially fewer parameters, training steps, and less data, without requiring modifications to the diffusion model backbone typically needed to mitigate rendering misalignments. Although being trained on masking-based anchor videos, our method generalizes robustly to anchor videos made with point clouds during inference, enabling precise 3D-informed camera control. EPiC achieves SOTA performance on RealEstate10K and MiraData for I2V camera control task, demonstrating precise and robust camera control ability both quantitatively and qualitatively. Notably, EPiC also exhibits strong zero-shot generalization to video-to-video scenarios.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†EPiCï¼Œä¸€ä¸ªé’ˆå¯¹è§†é¢‘æ‰©æ•£æ¨¡å‹(VDMs)ä¸­3Dæ‘„åƒæœºæ§åˆ¶çš„é«˜æ•ˆå­¦ä¹ æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³ç°æœ‰æ–¹æ³•ä¸­å› ç‚¹äº‘ä¼°è®¡è¯¯å·®å¯¼è‡´çš„é”šç‚¹è§†é¢‘(anchor videos)ä¸å‡†ç¡®ä»¥åŠå¯¹ç›¸æœºè½¨è¿¹æ ‡æ³¨éœ€æ±‚è¿‡é«˜çš„é—®é¢˜ã€‚è¯¥æ¡†æ¶é€šè¿‡åŸºäºé¦–å¸§å¯è§æ€§çš„æ©ç (masking)å¤„ç†æºè§†é¢‘ï¼Œè‡ªåŠ¨æ„å»ºé«˜ç²¾åº¦çš„é”šç‚¹è§†é¢‘ï¼Œä»è€Œæ¶ˆé™¤äº†å¯¹æ˜‚è´µç›¸æœºè½¨è¿¹æ ‡æ³¨çš„ä¾èµ–ï¼Œå¹¶å¯å¹¿æ³›åº”ç”¨äºå„ç§è‡ªç„¶åœºæ™¯è§†é¢‘ã€‚ç ”ç©¶å¼•å…¥äº†Anchor-ControlNetï¼Œè¿™æ˜¯ä¸€ç§å‚æ•°é‡ä¸è¶³éª¨å¹²ç½‘ç»œ1%çš„è½»é‡åŒ–æ¡ä»¶è°ƒèŠ‚æ¨¡å—ï¼Œç”¨äºå°†å¯è§åŒºåŸŸçš„é”šç‚¹è§†é¢‘å¼•å¯¼æ•´åˆè¿›é¢„è®­ç»ƒçš„VDMsã€‚å‡­å€Ÿé«˜æ•ˆçš„é”šç‚¹è§†é¢‘æ•°æ®å’Œæ¨¡å—è®¾è®¡ï¼ŒEPiCåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­å¯¹å‚æ•°ã€æ­¥æ•°å’Œæ•°æ®é‡çš„éœ€æ±‚æ˜¾è‘—å‡å°‘ï¼Œä¸”æ— éœ€å¯¹æ‰©æ•£æ¨¡å‹éª¨å¹²è¿›è¡Œä¿®æ”¹ã€‚å°½ç®¡è®­ç»ƒåŸºäºæ©ç ç”Ÿæˆçš„é”šç‚¹è§†é¢‘ï¼ŒEPiCåœ¨æ¨ç†é˜¶æ®µèƒ½ç¨³å¥åœ°æ³›åŒ–åˆ°ç”±ç‚¹äº‘ç”Ÿæˆçš„é”šç‚¹è§†é¢‘ï¼Œå®ç°ç²¾ç¡®çš„3Dæ‘„åƒæœºæ§åˆ¶ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒEPiCåœ¨RealEstate10Kå’ŒMiraDataæ•°æ®é›†çš„å›¾åƒåˆ°è§†é¢‘(I2V)ç›¸æœºæ§åˆ¶ä»»åŠ¡ä¸Šè¾¾åˆ°äº†SOTAæ€§èƒ½ï¼Œå¹¶åœ¨è§†é¢‘åˆ°è§†é¢‘(V2V)åœºæ™¯ä¸­å±•ç°å‡ºå¼ºå¤§çš„é›¶æ ·æœ¬æ³›åŒ–èƒ½åŠ›ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "Project website: https://zunwang1.github.io/Epic",
      "pdf_url": "https://arxiv.org/pdf/2505.21876v1",
      "published_date": "2025-05-28 01:45:26 UTC",
      "updated_date": "2025-05-28 01:45:26 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T15:40:15.038141+00:00"
    },
    {
      "arxiv_id": "2505.21873v1",
      "title": "HelixDesign-Binder: A Scalable Production-Grade Platform for Binder Design Built on HelixFold3",
      "title_zh": "HelixDesign-Binderï¼šåŸºäº HelixFold3 æ„å»ºçš„å¯æ‰©å±•ç”Ÿäº§çº§ç»“åˆè›‹ç™½è®¾è®¡å¹³å°",
      "authors": [
        "Jie Gao",
        "Jun Li",
        "Jing Hu",
        "Shanzhuo Zhang",
        "Kunrui Zhu",
        "Yueyang Huang",
        "Xiaonan Zhang",
        "Xiaomin Fang"
      ],
      "abstract": "Protein binder design is central to therapeutics, diagnostics, and synthetic biology, yet practical deployment remains challenging due to fragmented workflows, high computational costs, and complex tool integration. We present HelixDesign-Binder, a production-grade, high-throughput platform built on HelixFold3 that automates the full binder design pipeline, from backbone generation and sequence design to structural evaluation and multi-dimensional scoring. By unifying these stages into a scalable and user-friendly system, HelixDesign-Binder enables efficient exploration of binder candidates with favorable structural, energetic, and physicochemical properties. The platform leverages Baidu Cloud's high-performance infrastructure to support large-scale design and incorporates advanced scoring metrics, including ipTM, predicted binding free energy, and interface hydrophobicity. Benchmarking across six protein targets demonstrates that HelixDesign-Binder reliably produces diverse and high-quality binders, some of which match or exceed validated designs in predicted binding affinity. HelixDesign-Binder is accessible via an interactive web interface in PaddleHelix platform, supporting both academic research and industrial applications in antibody and protein binder development.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¨å‡ºäº† HelixDesign-Binderï¼Œè¿™æ˜¯ä¸€ä¸ªåŸºäº HelixFold3 æ„å»ºçš„ç”Ÿäº§çº§ã€é«˜é€šé‡è›‹ç™½è´¨ç»“åˆå‰‚è®¾è®¡å¹³å°ï¼Œæ—¨åœ¨è§£å†³è›‹ç™½è´¨ç»“åˆå‰‚è®¾è®¡ä¸­æµç¨‹ç¢ç‰‡åŒ–å’Œè®¡ç®—æˆæœ¬é«˜ç­‰æŒ‘æˆ˜ã€‚è¯¥å¹³å°é€šè¿‡ç»Ÿä¸€éª¨æ¶ç”Ÿæˆ (Backbone Generation)ã€åºåˆ—è®¾è®¡ (Sequence Design)ã€ç»“æ„è¯„ä¼°å’Œå¤šç»´åº¦è¯„åˆ†ï¼Œå®ç°äº†å…¨è‡ªåŠ¨åŒ–çš„è®¾è®¡æµæ°´çº¿ã€‚HelixDesign-Binder åˆ©ç”¨ç™¾åº¦äº‘çš„é«˜æ€§èƒ½åŸºç¡€è®¾æ–½æ”¯æŒå¤§è§„æ¨¡è®¾è®¡ï¼Œå¹¶ç»“åˆäº† ipTMã€é¢„æµ‹ç»“åˆè‡ªç”±èƒ½ (Predicted Binding Free Energy) å’Œç•Œé¢ç–æ°´æ€§ (Interface Hydrophobicity) ç­‰å…ˆè¿›æŒ‡æ ‡è¿›è¡Œè´¨é‡è¯„ä¼°ã€‚åœ¨å…­ä¸ªè›‹ç™½è´¨é¶æ ‡ä¸Šçš„åŸºå‡†æµ‹è¯•æ˜¾ç¤ºï¼Œè¯¥å¹³å°èƒ½äº§ç”Ÿé«˜è´¨é‡ä¸”å¤šæ ·åŒ–çš„ç»“åˆå‰‚ï¼Œå…¶é¢„æµ‹ç»“åˆäº²å’ŒåŠ›éƒ¨åˆ†è¾¾åˆ°æˆ–è¶…è¿‡äº†å·²éªŒè¯çš„è®¾è®¡ã€‚è¯¥å¹³å°ç›®å‰å¯é€šè¿‡ PaddleHelix å¹³å°çš„äº¤äº’å¼ Web ç•Œé¢è®¿é—®ï¼Œä¸ºæŠ—ä½“å’Œè›‹ç™½è´¨è¯ç‰©ç ”å‘æä¾›äº†å¯æ‰©å±•çš„å·¥ä¸šçº§è§£å†³æ–¹æ¡ˆã€‚",
      "categories": [
        "q-bio.BM",
        "cs.AI",
        "q-bio.QM"
      ],
      "primary_category": "q-bio.BM",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.21873v1",
      "published_date": "2025-05-28 01:39:31 UTC",
      "updated_date": "2025-05-28 01:39:31 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T15:40:28.696600+00:00"
    },
    {
      "arxiv_id": "2505.21870v1",
      "title": "Evaluating the Retrieval Robustness of Large Language Models",
      "title_zh": "å¤§è¯­è¨€æ¨¡å‹æ£€ç´¢é²æ£’æ€§è¯„ä¼°",
      "authors": [
        "Shuyang Cao",
        "Karthik Radhakrishnan",
        "David Rosenberg",
        "Steven Lu",
        "Pengxiang Cheng",
        "Lu Wang",
        "Shiyue Zhang"
      ],
      "abstract": "Retrieval-augmented generation (RAG) generally enhances large language models' (LLMs) ability to solve knowledge-intensive tasks. But RAG may also lead to performance degradation due to imperfect retrieval and the model's limited ability to leverage retrieved content. In this work, we evaluate the robustness of LLMs in practical RAG setups (henceforth retrieval robustness). We focus on three research questions: (1) whether RAG is always better than non-RAG; (2) whether more retrieved documents always lead to better performance; (3) and whether document orders impact results. To facilitate this study, we establish a benchmark of 1500 open-domain questions, each with retrieved documents from Wikipedia. We introduce three robustness metrics, each corresponds to one research question. Our comprehensive experiments, involving 11 LLMs and 3 prompting strategies, reveal that all of these LLMs exhibit surprisingly high retrieval robustness; nonetheless, different degrees of imperfect robustness hinders them from fully utilizing the benefits of RAG.",
      "tldr_zh": "è¯¥é¡¹ç ”ç©¶è¯„ä¼°äº†å¤§è¯­è¨€æ¨¡å‹(LLMs)åœ¨æ£€ç´¢å¢å¼ºç”Ÿæˆ(RAG)è®¾ç½®ä¸‹çš„æ£€ç´¢é²æ£’æ€§(retrieval robustness)ï¼Œé‡ç‚¹æ¢è®¨äº†RAGä¸éRAGçš„æ•ˆæœå¯¹æ¯”ã€æ£€ç´¢æ–‡æ¡£æ•°é‡ä»¥åŠæ–‡æ¡£é¡ºåºå¯¹æ€§èƒ½çš„å½±å“ã€‚ä¸ºæ­¤ï¼Œç ”ç©¶å›¢é˜Ÿå»ºç«‹äº†ä¸€ä¸ªåŒ…å«1500ä¸ªå¼€æ”¾åŸŸé—®é¢˜çš„åŸºå‡†æµ‹è¯•é›†ï¼Œå¹¶å¼•å…¥äº†ä¸‰é¡¹ä¸“é—¨çš„é²æ£’æ€§æŒ‡æ ‡ã€‚é€šè¿‡å¯¹11ä¸ªLLMså’Œ3ç§æç¤ºç­–ç•¥(prompting strategies)çš„å…¨é¢å®éªŒï¼Œç ”ç©¶å‘ç°è™½ç„¶è¿™äº›æ¨¡å‹æ™®éå±•ç°å‡ºæé«˜çš„æ£€ç´¢é²æ£’æ€§ï¼Œä½†ä»å­˜åœ¨ä¸åŒç¨‹åº¦çš„ç¼ºé™·ï¼Œé˜»ç¢äº†å…¶å……åˆ†å‘æŒ¥RAGçš„æ½œåŠ›ã€‚è¯¥ç ”ç©¶é‡åŒ–äº†å½“å‰æ¨¡å‹åœ¨å¤„ç†ä¸å®Œç¾æ£€ç´¢å†…å®¹æ—¶çš„å±€é™æ€§ï¼Œä¸ºä¼˜åŒ–çŸ¥è¯†å¯†é›†å‹ä»»åŠ¡çš„æ£€ç´¢æœºåˆ¶æä¾›äº†é‡è¦å‚è€ƒã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "19 pages",
      "pdf_url": "https://arxiv.org/pdf/2505.21870v1",
      "published_date": "2025-05-28 01:34:31 UTC",
      "updated_date": "2025-05-28 01:34:31 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T15:40:38.117405+00:00"
    },
    {
      "arxiv_id": "2505.21866v2",
      "title": "CSI-Bench: A Large-Scale In-the-Wild Dataset for Multi-task WiFi Sensing",
      "title_zh": "CSI-Benchï¼šé¢å‘å¤šä»»åŠ¡WiFiæ„ŸçŸ¥çš„å¤§è§„æ¨¡çœŸå®åœºæ™¯æ•°æ®é›†",
      "authors": [
        "Guozhen Zhu",
        "Yuqian Hu",
        "Weihang Gao",
        "Wei-Hsiang Wang",
        "Beibei Wang",
        "K. J. Ray Liu"
      ],
      "abstract": "WiFi sensing has emerged as a compelling contactless modality for human activity monitoring by capturing fine-grained variations in Channel State Information (CSI). Its ability to operate continuously and non-intrusively while preserving user privacy makes it particularly suitable for health monitoring. However, existing WiFi sensing systems struggle to generalize in real-world settings, largely due to datasets collected in controlled environments with homogeneous hardware and fragmented, session-based recordings that fail to reflect continuous daily activity.\n  We present CSI-Bench, a large-scale, in-the-wild benchmark dataset collected using commercial WiFi edge devices across 26 diverse indoor environments with 35 real users. Spanning over 461 hours of effective data, CSI-Bench captures realistic signal variability under natural conditions. It includes task-specific datasets for fall detection, breathing monitoring, localization, and motion source recognition, as well as a co-labeled multitask dataset with joint annotations for user identity, activity, and proximity. To support the development of robust and generalizable models, CSI-Bench provides standardized evaluation splits and baseline results for both single-task and multi-task learning. CSI-Bench offers a foundation for scalable, privacy-preserving WiFi sensing systems in health and broader human-centric applications.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ç°æœ‰ WiFi sensing ç³»ç»Ÿåœ¨çœŸå®ç¯å¢ƒæ³›åŒ–èƒ½åŠ›ä¸è¶³ä»¥åŠæ•°æ®é›†ç¼ºä¹è¿ç»­æ—¥å¸¸æ´»åŠ¨è®°å½•çš„é—®é¢˜ï¼Œæå‡ºäº†å¤§è§„æ¨¡ã€é‡å¤–ç¯å¢ƒ(in-the-wild)çš„å¤šä»»åŠ¡æ„ŸçŸ¥åŸºå‡†æ•°æ®é›† CSI-Benchã€‚è¯¥æ•°æ®é›†åˆ©ç”¨å•†ç”¨ WiFi è¾¹ç¼˜è®¾å¤‡åœ¨ 26 ä¸ªä¸åŒçš„å®¤å†…åœºæ™¯ä¸­ä» 35 åçœŸå®ç”¨æˆ·å¤„é‡‡é›†ï¼ŒåŒ…å«è¶…è¿‡ 461 å°æ—¶çš„æœ‰æ•ˆæ•°æ®ï¼Œå……åˆ†æ•æ‰äº†è‡ªç„¶æ¡ä»¶ä¸‹çš„ä¿¡å·å˜å¼‚ã€‚CSI-Bench æ¶µç›–äº† fall detectionã€breathing monitoringã€localization å’Œ motion source recognition ç­‰ç‰¹å®šä»»åŠ¡ï¼Œå¹¶æä¾›äº†ä¸€ä¸ªåŒ…å«ç”¨æˆ·èº«ä»½ã€æ´»åŠ¨ä¸æ¥è¿‘åº¦è”åˆæ ‡æ³¨çš„å¤šä»»åŠ¡æ•°æ®é›†ã€‚ä¸ºäº†ä¿ƒè¿›æ¨¡å‹å¼€å‘ï¼Œç ”ç©¶è€…è¿˜æä¾›äº†æ ‡å‡†åŒ–çš„è¯„ä¼°åˆ’åˆ†ä»¥åŠé’ˆå¯¹å•ä»»åŠ¡å’Œå¤šä»»åŠ¡å­¦ä¹ çš„ baseline ç»“æœã€‚è¯¥åŸºå‡†ä¸ºæ„å»ºå¯æ‰©å±•ã€ä¿æŠ¤éšç§ä¸”é€‚ç”¨äºå¥åº·ç›‘æµ‹ç­‰é¢†åŸŸçš„ WiFi sensing ç³»ç»Ÿæä¾›äº†é‡è¦åŸºç¡€ã€‚",
      "categories": [
        "eess.SP",
        "cs.AI",
        "cs.DB"
      ],
      "primary_category": "eess.SP",
      "comment": "26 pages, 5 figures, accepted by Thirty-Ninth Annual Conference on Neural Information Processing Systems (NeurIPS)",
      "pdf_url": "https://arxiv.org/pdf/2505.21866v2",
      "published_date": "2025-05-28 01:29:29 UTC",
      "updated_date": "2025-11-20 05:12:52 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T15:41:18.858106+00:00"
    },
    {
      "arxiv_id": "2505.21855v1",
      "title": "Extracting Research Instruments from Educational Literature Using LLMs",
      "title_zh": "åˆ©ç”¨å¤§è¯­è¨€æ¨¡å‹ä»æ•™è‚²æ–‡çŒ®ä¸­æå–ç ”ç©¶å·¥å…·",
      "authors": [
        "Jiseung Yoo",
        "Curran Mahowald",
        "Meiyu Li",
        "Wei Ai"
      ],
      "abstract": "Large Language Models (LLMs) are transforming information extraction from academic literature, offering new possibilities for knowledge management. This study presents an LLM-based system designed to extract detailed information about research instruments used in the education field, including their names, types, target respondents, measured constructs, and outcomes. Using multi-step prompting and a domain-specific data schema, it generates structured outputs optimized for educational research. Our evaluation shows that this system significantly outperforms other approaches, particularly in identifying instrument names and detailed information. This demonstrates the potential of LLM-powered information extraction in educational contexts, offering a systematic way to organize research instrument information. The ability to aggregate such information at scale enhances accessibility for researchers and education leaders, facilitating informed decision-making in educational research and policy.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§åŸºäºå¤§è¯­è¨€æ¨¡å‹(Large Language Models, LLMs)çš„ç³»ç»Ÿï¼Œæ—¨åœ¨ä»æ•™è‚²æ–‡çŒ®ä¸­è‡ªåŠ¨æå–ç ”ç©¶å·¥å…·(research instruments)çš„è¯¦ç»†ä¿¡æ¯ã€‚è¯¥ç³»ç»Ÿé‡‡ç”¨å¤šæ­¥æç¤ºè¯(multi-step prompting)å’Œé¢†åŸŸç‰¹å®šçš„æ•°æ®æ¨¡å¼(domain-specific data schema)ï¼Œèƒ½å¤Ÿç²¾å‡†è¯†åˆ«å·¥å…·åç§°ã€ç±»å‹ã€ç›®æ ‡å—ä¼—ã€æµ‹é‡ç»´åº¦åŠç»“æœï¼Œå¹¶ç”Ÿæˆç»“æ„åŒ–çš„è¾“å‡ºã€‚è¯„ä¼°ç»“æœæ˜¾ç¤ºï¼Œè¯¥ç³»ç»Ÿåœ¨è¯†åˆ«å·¥å…·åç§°åŠå…¶ç»†èŠ‚æ–¹é¢æ˜¾è‘—ä¼˜äºç°æœ‰å…¶ä»–æ–¹æ³•ï¼Œå……åˆ†å±•ç¤ºäº†LLMåœ¨æ•™è‚²é¢†åŸŸä¿¡æ¯æå–ä¸­çš„åº”ç”¨æ½œåŠ›ã€‚è¿™ç§è§„æ¨¡åŒ–çš„ä¿¡æ¯èšåˆèƒ½åŠ›æ˜¾è‘—å¢å¼ºäº†ç ”ç©¶äººå‘˜è·å–å­¦æœ¯èµ„æºçš„ä¾¿åˆ©æ€§ï¼Œä¸ºæ•™è‚²ç ”ç©¶å’Œæ”¿ç­–åˆ¶å®šæä¾›äº†å¼ºæœ‰åŠ›çš„å†³ç­–æ”¯æŒã€‚",
      "categories": [
        "cs.IR",
        "cs.AI"
      ],
      "primary_category": "cs.IR",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.21855v1",
      "published_date": "2025-05-28 01:00:32 UTC",
      "updated_date": "2025-05-28 01:00:32 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T15:41:08.680100+00:00"
    },
    {
      "arxiv_id": "2505.21854v1",
      "title": "Rethinking Gradient-based Adversarial Attacks on Point Cloud Classification",
      "title_zh": "é‡æ–°å®¡è§†ç‚¹äº‘åˆ†ç±»ä¸­åŸºäºæ¢¯åº¦çš„å¯¹æŠ—æ”»å‡»",
      "authors": [
        "Jun Chen",
        "Xinke Li",
        "Mingyue Xu",
        "Tianrui Li",
        "Chongshou Li"
      ],
      "abstract": "Gradient-based adversarial attacks have become a dominant approach for evaluating the robustness of point cloud classification models. However, existing methods often rely on uniform update rules that fail to consider the heterogeneous nature of point clouds, resulting in excessive and perceptible perturbations. In this paper, we rethink the design of gradient-based attacks by analyzing the limitations of conventional gradient update mechanisms and propose two new strategies to improve both attack effectiveness and imperceptibility. First, we introduce WAAttack, a novel framework that incorporates weighted gradients and an adaptive step-size strategy to account for the non-uniform contribution of points during optimization. This approach enables more targeted and subtle perturbations by dynamically adjusting updates according to the local structure and sensitivity of each point. Second, we propose SubAttack, a complementary strategy that decomposes the point cloud into subsets and focuses perturbation efforts on structurally critical regions. Together, these methods represent a principled rethinking of gradient-based adversarial attacks for 3D point cloud classification. Extensive experiments demonstrate that our approach outperforms state-of-the-art baselines in generating highly imperceptible adversarial examples. Code will be released upon paper acceptance.",
      "tldr_zh": "æœ¬ç ”ç©¶é‡æ–°å®¡è§†äº†ç‚¹äº‘åˆ†ç±»(Point Cloud Classification)ä¸­åŸºäºæ¢¯åº¦çš„å¯¹æŠ—æ”»å‡»ï¼ŒæŒ‡å‡ºä¼ ç»Ÿæ–¹æ³•ç”±äºé‡‡ç”¨ç»Ÿä¸€æ›´æ–°è§„åˆ™è€Œå¿½è§†äº†ç‚¹äº‘çš„å¼‚æ„æ€§ï¼Œå¯¼è‡´äº§ç”Ÿçš„æ‰°åŠ¨è¿‡å¤§ä¸”æ˜“äºå¯Ÿè§‰ã€‚ä¸ºæ­¤ï¼Œä½œè€…é¦–å…ˆæå‡ºäº†WAAttackæ¡†æ¶ï¼Œè¯¥æ¡†æ¶å¼•å…¥äº†åŠ æƒæ¢¯åº¦(Weighted Gradients)å’Œè‡ªé€‚åº”æ­¥é•¿ç­–ç•¥ï¼Œé€šè¿‡ç»“åˆç‚¹çš„å±€éƒ¨ç»“æ„å’Œæ•æ„Ÿåº¦åŠ¨æ€è°ƒæ•´æ›´æ–°ï¼Œå®ç°äº†æ›´å…·é’ˆå¯¹æ€§çš„ç»†å¾®æ‰°åŠ¨ã€‚æ­¤å¤–ï¼Œç ”ç©¶è¿˜æå‡ºäº†SubAttackç­–ç•¥ï¼Œé€šè¿‡å°†ç‚¹äº‘åˆ†è§£ä¸ºå­é›†å¹¶å°†æ‰°åŠ¨é‡ç‚¹é›†ä¸­åœ¨ç»“æ„å…³é”®åŒºåŸŸï¼Œè¿›ä¸€æ­¥æå‡äº†æ”»å‡»çš„æœ‰æ•ˆæ€§ã€‚è¿™ä¸¤ç§æ–¹æ³•çš„ç»“åˆæ˜¯å¯¹3Dç‚¹äº‘å¯¹æŠ—æ”»å‡»æœºåˆ¶çš„ç³»ç»Ÿæ€§æ”¹è¿›ï¼Œåœ¨ç¡®ä¿æ”»å‡»æ•ˆèƒ½çš„åŒæ—¶æ˜¾è‘—æé«˜äº†ä¸å¯å¯Ÿè§‰æ€§ã€‚å¤§é‡å®éªŒè¯æ˜ï¼Œè¯¥æ–¹æ³•åœ¨ç”Ÿæˆé«˜åº¦ä¸å¯å¯Ÿè§‰çš„å¯¹æŠ—æ ·æœ¬æ–¹é¢ä¼˜äºç°æœ‰çš„åŸºå‡†æ¨¡å‹(State-of-the-art baselines)ï¼Œä¸ºè¯„ä¼°ç‚¹äº‘æ¨¡å‹çš„é²æ£’æ€§æä¾›äº†æ–°çš„è§†è§’å’Œå·¥å…·ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.21854v1",
      "published_date": "2025-05-28 00:55:36 UTC",
      "updated_date": "2025-05-28 00:55:36 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T15:41:26.308986+00:00"
    },
    {
      "arxiv_id": "2505.21852v1",
      "title": "A Provable Approach for End-to-End Safe Reinforcement Learning",
      "title_zh": "ç«¯åˆ°ç«¯å®‰å…¨å¼ºåŒ–å­¦ä¹ çš„å¯è¯æ˜æ–¹æ³•",
      "authors": [
        "Akifumi Wachi",
        "Kohei Miyaguchi",
        "Takumi Tanabe",
        "Rei Sato",
        "Youhei Akimoto"
      ],
      "abstract": "A longstanding goal in safe reinforcement learning (RL) is a method to ensure the safety of a policy throughout the entire process, from learning to operation. However, existing safe RL paradigms inherently struggle to achieve this objective. We propose a method, called Provably Lifetime Safe RL (PLS), that integrates offline safe RL with safe policy deployment to address this challenge. Our proposed method learns a policy offline using return-conditioned supervised learning and then deploys the resulting policy while cautiously optimizing a limited set of parameters, known as target returns, using Gaussian processes (GPs). Theoretically, we justify the use of GPs by analyzing the mathematical relationship between target and actual returns. We then prove that PLS finds near-optimal target returns while guaranteeing safety with high probability. Empirically, we demonstrate that PLS outperforms baselines both in safety and reward performance, thereby achieving the longstanding goal to obtain high rewards while ensuring the safety of a policy throughout the lifetime from learning to operation.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§åä¸º Provably Lifetime Safe RL (PLS) çš„æ–¹æ³•ï¼Œæ—¨åœ¨è§£å†³å®‰å…¨å¼ºåŒ–å­¦ä¹ (Safe RL)åœ¨ä»å­¦ä¹ åˆ°éƒ¨ç½²çš„å…¨ç”Ÿå‘½å‘¨æœŸä¸­éš¾ä»¥æŒç»­ç¡®ä¿å®‰å…¨æ€§è¿™ä¸€é•¿æœŸæŒ‘æˆ˜ã€‚è¯¥æ–¹æ³•å°†ç¦»çº¿å®‰å…¨å¼ºåŒ–å­¦ä¹ (offline safe RL)ä¸å®‰å…¨ç­–ç•¥éƒ¨ç½²(safe policy deployment)ç›¸ç»“åˆï¼Œé¦–å…ˆåˆ©ç”¨å›æŠ¥æ¡ä»¶ç›‘ç£å­¦ä¹ (return-conditioned supervised learning)ç¦»çº¿å­¦ä¹ ç­–ç•¥ã€‚åœ¨éƒ¨ç½²é˜¶æ®µï¼ŒPLS é€šè¿‡é«˜æ–¯è¿‡ç¨‹(Gaussian processes, GPs)è°¨æ…åœ°ä¼˜åŒ–æœ‰é™çš„ç›®æ ‡å›æŠ¥(target returns)å‚æ•°ï¼Œä»è€Œå®ç°ç­–ç•¥çš„æŒç»­ä¼˜åŒ–ã€‚ç†è®ºåˆ†æéªŒè¯äº†é«˜æ–¯è¿‡ç¨‹åœ¨å¤„ç†ç›®æ ‡å›æŠ¥ä¸å®é™…å›æŠ¥å…³ç³»ä¸­çš„æœ‰æ•ˆæ€§ï¼Œå¹¶è¯æ˜äº†è¯¥æ–¹æ³•åœ¨ä¿è¯é«˜æ¦‚ç‡å®‰å…¨çš„å‰æä¸‹èƒ½æ‰¾åˆ°è¿‘ä¹æœ€ä¼˜çš„å›æŠ¥ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒPLS åœ¨å®‰å…¨æ€§å’Œå¥–åŠ±æ€§èƒ½æ–¹é¢å‡ä¼˜äºç°æœ‰åŸºçº¿æ¨¡å‹ï¼Œä¸ºå®ç°ä»å­¦ä¹ åˆ°è¿è¡Œçš„å…¨ç¨‹å®‰å…¨å¼ºåŒ–å­¦ä¹ å¥ å®šäº†ç†è®ºä¸å®è·µåŸºç¡€ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.IT",
        "cs.RO"
      ],
      "primary_category": "cs.LG",
      "comment": "27 pages",
      "pdf_url": "https://arxiv.org/pdf/2505.21852v1",
      "published_date": "2025-05-28 00:48:20 UTC",
      "updated_date": "2025-05-28 00:48:20 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T15:41:25.852789+00:00"
    },
    {
      "arxiv_id": "2505.21851v2",
      "title": "Streaming Flow Policy: Simplifying diffusion/flow-matching policies by treating action trajectories as flow trajectories",
      "title_zh": "Streaming Flow Policyï¼šé€šè¿‡å°†åŠ¨ä½œè½¨è¿¹è§†ä¸ºæµè½¨è¿¹ç®€åŒ–æ‰©æ•£/æµåŒ¹é…ç­–ç•¥",
      "authors": [
        "Sunshine Jiang",
        "Xiaolin Fang",
        "Nicholas Roy",
        "TomÃ¡s Lozano-PÃ©rez",
        "Leslie Pack Kaelbling",
        "Siddharth Ancha"
      ],
      "abstract": "Recent advances in diffusion$/$flow-matching policies have enabled imitation learning of complex, multi-modal action trajectories. However, they are computationally expensive because they sample a trajectory of trajectories: a diffusion$/$flow trajectory of action trajectories. They discard intermediate action trajectories, and must wait for the sampling process to complete before any actions can be executed on the robot. We simplify diffusion$/$flow policies by treating action trajectories as flow trajectories. Instead of starting from pure noise, our algorithm samples from a narrow Gaussian around the last action. Then, it incrementally integrates a velocity field learned via flow matching to produce a sequence of actions that constitute a single trajectory. This enables actions to be streamed to the robot on-the-fly during the flow sampling process, and is well-suited for receding horizon policy execution. Despite streaming, our method retains the ability to model multi-modal behavior. We train flows that stabilize around demonstration trajectories to reduce distribution shift and improve imitation learning performance. Streaming flow policy outperforms prior methods while enabling faster policy execution and tighter sensorimotor loops for learning-based robot control. Project website: https://streaming-flow-policy.github.io/",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†Streaming Flow Policyï¼Œé€šè¿‡å°†åŠ¨ä½œè½¨è¿¹(action trajectories)ç›´æ¥è§†ä¸ºæµè½¨è¿¹(flow trajectories)ï¼Œæ˜¾è‘—ç®€åŒ–äº†ä¼ ç»Ÿçš„æ‰©æ•£(diffusion)å’ŒæµåŒ¹é…(flow-matching)ç­–ç•¥ã€‚é’ˆå¯¹ä¼ ç»Ÿæ–¹æ³•å› éœ€é‡‡æ ·â€œè½¨è¿¹çš„è½¨è¿¹â€è€Œå¯¼è‡´è®¡ç®—å¼€é”€å¤§ä¸”æ‰§è¡Œå»¶è¿Ÿé«˜çš„é—®é¢˜ï¼Œè¯¥ç®—æ³•ä¸å†ä»çº¯å™ªå£°å¼€å§‹ï¼Œè€Œæ˜¯ä»ä¸Šä¸€åŠ¨ä½œå‘¨å›´çš„çª„é«˜æ–¯åˆ†å¸ƒ(narrow Gaussian)è¿›è¡Œé‡‡æ ·ã€‚é€šè¿‡å¢é‡ç§¯åˆ†æµåŒ¹é…å­¦ä¹ çš„é€Ÿåº¦åœº(velocity field)äº§ç”ŸåŠ¨ä½œåºåˆ—ï¼Œè¯¥æ–¹æ³•ä½¿å¾—åŠ¨ä½œèƒ½åœ¨é‡‡æ ·è¿‡ç¨‹ä¸­å®æ—¶æµå¼ä¼ è¾“(streamed)ç»™æœºå™¨äººï¼Œæå¤§åœ°å¢å¼ºäº†é€€é¿æ°´å¹³(receding horizon)ç­–ç•¥çš„æ‰§è¡Œæ•ˆç‡ã€‚åœ¨å®ç°æµå¼å¤„ç†çš„åŒæ—¶ï¼Œè¯¥æ–¹æ³•ä¾ç„¶ä¿ç•™äº†å»ºæ¨¡å¤šæ¨¡æ€(multi-modal)è¡Œä¸ºçš„èƒ½åŠ›ï¼Œå¹¶åˆ©ç”¨ç¨³å®šåŒ–æµå‡å°‘åˆ†å¸ƒåç§»(distribution shift)ä»¥æå‡æ¨¡ä»¿å­¦ä¹ æ€§èƒ½ã€‚å®éªŒè¯æ˜ï¼ŒStreaming Flow Policyåœ¨æœºå™¨äººæ§åˆ¶ä»»åŠ¡ä¸­ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œå®ç°äº†æ›´å¿«çš„ç­–ç•¥æ‰§è¡Œé€Ÿåº¦å’Œæ›´ç´§å¯†çš„ä¼ æ„Ÿå™¨è¿åŠ¨é—­ç¯(sensorimotor loops)ã€‚",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.RO",
      "comment": "Conference on Robot Learning (CoRL) 2025",
      "pdf_url": "https://arxiv.org/pdf/2505.21851v2",
      "published_date": "2025-05-28 00:48:19 UTC",
      "updated_date": "2025-09-24 23:51:56 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T15:41:49.360378+00:00"
    },
    {
      "arxiv_id": "2505.21850v2",
      "title": "Beyond Perception: Evaluating Abstract Visual Reasoning through Multi-Stage Task",
      "title_zh": "è¶…è¶Šæ„ŸçŸ¥ï¼šé€šè¿‡å¤šé˜¶æ®µä»»åŠ¡è¯„ä¼°æŠ½è±¡è§†è§‰æ¨ç†",
      "authors": [
        "Yanbei Jiang",
        "Yihao Ding",
        "Chao Lei",
        "Jiayang Ao",
        "Jey Han Lau",
        "Krista A. Ehinger"
      ],
      "abstract": "Current Multimodal Large Language Models (MLLMs) excel in general visual reasoning but remain underexplored in Abstract Visual Reasoning (AVR), which demands higher-order reasoning to identify abstract rules beyond simple perception. Existing AVR benchmarks focus on single-step reasoning, emphasizing the end result but neglecting the multi-stage nature of reasoning process. Past studies found MLLMs struggle with these benchmarks, but it doesn't explain how they fail. To address this gap, we introduce MultiStAR, a Multi-Stage AVR benchmark, based on RAVEN, designed to assess reasoning across varying levels of complexity. Additionally, existing metrics like accuracy only focus on the final outcomes while do not account for the correctness of intermediate steps. Therefore, we propose a novel metric, MSEval, which considers the correctness of intermediate steps in addition to the final outcomes. We conduct comprehensive experiments on MultiStAR using 17 representative close-source and open-source MLLMs. The results reveal that while existing MLLMs perform adequately on basic perception tasks, they continue to face challenges in more complex rule detection stages.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†å½“å‰å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ (Multimodal Large Language Models, MLLMs) åœ¨æŠ½è±¡è§†è§‰æ¨ç† (Abstract Visual Reasoning, AVR) é¢†åŸŸçš„å±€é™æ€§ï¼ŒæŒ‡å‡ºé«˜é˜¶æ¨ç†èƒ½åŠ›ä»æ˜¯å…¶çŸ­æ¿ã€‚é’ˆå¯¹ç°æœ‰åŸºå‡†æµ‹è¯•ä¾§é‡å•æ­¥æ¨ç†ç»“æœè€Œå¿½è§†æ¨ç†è¿‡ç¨‹å¤šé˜¶æ®µç‰¹æ€§çš„é—®é¢˜ï¼Œç ”ç©¶è€…æå‡ºäº† MultiStAR è¿™ä¸€å¤šé˜¶æ®µ AVR åŸºå‡†æµ‹è¯•ï¼Œæ—¨åœ¨æ›´ç»†è‡´åœ°è¯„ä¼°æ¨¡å‹åœ¨ä¸åŒå¤æ‚åº¦ä¸‹çš„è¡¨ç°ã€‚ä¸ºäº†å¼¥è¡¥ä¼ ç»Ÿå‡†ç¡®ç‡æŒ‡æ ‡æ— æ³•è¡¡é‡ä¸­é—´æ­¥éª¤æ­£ç¡®æ€§çš„ä¸è¶³ï¼Œè¯¥ç ”ç©¶è¿›ä¸€æ­¥æå‡ºäº† MSEval è¯„ä¼°æŒ‡æ ‡ï¼Œå°†æ¨ç†è¿‡ç¨‹çš„æ­£ç¡®æ€§çº³å…¥è¯„ä»·ä½“ç³»ã€‚é€šè¿‡å¯¹ 17 ä¸ªä¸»æµå¼€æºåŠé—­æº MLLMs çš„å…¨é¢å®éªŒå‘ç°ï¼Œè™½ç„¶ç°æœ‰æ¨¡å‹åœ¨åŸºç¡€æ„ŸçŸ¥ä»»åŠ¡ä¸­è¡¨ç°è‰¯å¥½ï¼Œä½†åœ¨å¤æ‚çš„è§„åˆ™æ£€æµ‹ (rule detection) é˜¶æ®µä»é¢ä¸´å·¨å¤§æŒ‘æˆ˜ã€‚è¿™é¡¹å·¥ä½œä¸ºç†è§£ MLLMs åœ¨é€»è¾‘æ¨ç†ä¸­çš„å¤±æ•ˆæ¨¡å¼æä¾›äº†é‡è¦å·¥å…·å’Œè§è§£ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "Accepted at ACL Findings",
      "pdf_url": "https://arxiv.org/pdf/2505.21850v2",
      "published_date": "2025-05-28 00:34:45 UTC",
      "updated_date": "2025-05-30 05:35:04 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T15:41:27.639606+00:00"
    },
    {
      "arxiv_id": "2505.21849v1",
      "title": "Xinyu AI Search: Enhanced Relevance and Comprehensive Results with Rich Answer Presentations",
      "title_zh": "Xinyu AI Searchï¼šå…·æœ‰å¢å¼ºç›¸å…³æ€§ã€å…¨é¢ç»“æœä¸ä¸°å¯Œç­”æ¡ˆå‘ˆç°çš„ AI æœç´¢",
      "authors": [
        "Bo Tang",
        "Junyi Zhu",
        "Chenyang Xi",
        "Yunhang Ge",
        "Jiahao Wu",
        "Yuchen Feng",
        "Yijun Niu",
        "Wenqiang Wei",
        "Yu Yu",
        "Chunyu Li",
        "Zehao Lin",
        "Hao Wu",
        "Ning Liao",
        "Yebin Yang",
        "Jiajia Wang",
        "Zhiyu Li",
        "Feiyu Xiong",
        "Jingrun Chen"
      ],
      "abstract": "Traditional search engines struggle to synthesize fragmented information for complex queries, while generative AI search engines face challenges in relevance, comprehensiveness, and presentation. To address these limitations, we introduce Xinyu AI Search, a novel system that incorporates a query-decomposition graph to dynamically break down complex queries into sub-queries, enabling stepwise retrieval and generation. Our retrieval pipeline enhances diversity through multi-source aggregation and query expansion, while filtering and re-ranking strategies optimize passage relevance. Additionally, Xinyu AI Search introduces a novel approach for fine-grained, precise built-in citation and innovates in result presentation by integrating timeline visualization and textual-visual choreography. Evaluated on recent real-world queries, Xinyu AI Search outperforms eight existing technologies in human assessments, excelling in relevance, comprehensiveness, and insightfulness. Ablation studies validate the necessity of its key sub-modules. Our work presents the first comprehensive framework for generative AI search engines, bridging retrieval, generation, and user-centric presentation.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ä¼ ç»Ÿæœç´¢å¼•æ“éš¾ä»¥æ•´åˆç¢ç‰‡åŒ–ä¿¡æ¯ä»¥åŠç”Ÿæˆå¼äººå·¥æ™ºèƒ½æœç´¢åœ¨ç›¸å…³æ€§ã€å…¨é¢æ€§å’Œå‘ˆç°å½¢å¼ä¸Šçš„ä¸è¶³ï¼Œæå‡ºäº† Xinyu AI Search ç³»ç»Ÿã€‚è¯¥ç³»ç»Ÿé€šè¿‡å¼•å…¥æŸ¥è¯¢åˆ†è§£å›¾ (query-decomposition graph)ï¼Œå°†å¤æ‚æŸ¥è¯¢åŠ¨æ€æ‹†è§£ä¸ºå­æŸ¥è¯¢ï¼Œä»è€Œå®ç°é€æ­¥çš„æ£€ç´¢ä¸ç”Ÿæˆã€‚å…¶æ£€ç´¢ç®¡çº¿ç»“åˆäº†å¤šæºèšåˆ (multi-source aggregation) å’ŒæŸ¥è¯¢æ‰©å±• (query expansion) æŠ€æœ¯ä»¥æå‡å¤šæ ·æ€§ï¼Œå¹¶åˆ©ç”¨è¿‡æ»¤ä¸é‡æ’åºç­–ç•¥ä¼˜åŒ–æ®µè½ç›¸å…³æ€§ã€‚æ­¤å¤–ï¼ŒXinyu AI Search å®ç°äº†ç»†ç²’åº¦çš„ç²¾ç¡®å†…ç½®å¼•ç”¨åŠŸèƒ½ï¼Œå¹¶åˆ›æ–°æ€§åœ°æ•´åˆäº†æ—¶é—´è½´å¯è§†åŒ– (timeline visualization) ä¸æ–‡æœ¬-è§†è§‰ç¼–æ’ (textual-visual choreography) ä»¥ä¼˜åŒ–ç»“æœå‘ˆç°ã€‚åœ¨çœŸå®åœºæ™¯æŸ¥è¯¢çš„è¯„ä¼°ä¸­ï¼Œè¯¥ç³»ç»Ÿåœ¨ç›¸å…³æ€§ã€å…¨é¢æ€§å’Œæ´å¯ŸåŠ›æ–¹é¢å‡ä¼˜äºå…«ç§ç°æœ‰æŠ€æœ¯ã€‚è¯¥å·¥ä½œä¸ºè¿æ¥æ£€ç´¢ã€ç”Ÿæˆå’Œä»¥ç”¨æˆ·ä¸ºä¸­å¿ƒå‘ˆç°çš„ç”Ÿæˆå¼äººå·¥æ™ºèƒ½æœç´¢å¼•æ“æä¾›äº†é¦–ä¸ªå…¨é¢æ¡†æ¶ã€‚",
      "categories": [
        "cs.IR",
        "cs.AI"
      ],
      "primary_category": "cs.IR",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.21849v1",
      "published_date": "2025-05-28 00:30:22 UTC",
      "updated_date": "2025-05-28 00:30:22 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T15:41:49.750117+00:00"
    },
    {
      "arxiv_id": "2505.21847v2",
      "title": "RePaViT: Scalable Vision Transformer Acceleration via Structural Reparameterization on Feedforward Network Layers",
      "title_zh": "RePaViTï¼šé€šè¿‡å‰é¦ˆç½‘ç»œå±‚ç»“æ„é‡å‚æ•°åŒ–å®ç°å¯æ‰©å±•çš„è§†è§‰ Transformer åŠ é€Ÿ",
      "authors": [
        "Xuwei Xu",
        "Yang Li",
        "Yudong Chen",
        "Jiajun Liu",
        "Sen Wang"
      ],
      "abstract": "We reveal that feedforward network (FFN) layers, rather than attention layers, are the primary contributors to Vision Transformer (ViT) inference latency, with their impact signifying as model size increases. This finding highlights a critical opportunity for optimizing the efficiency of large-scale ViTs by focusing on FFN layers. In this work, we propose a novel channel idle mechanism that facilitates post-training structural reparameterization for efficient FFN layers during testing. Specifically, a set of feature channels remains idle and bypasses the nonlinear activation function in each FFN layer, thereby forming a linear pathway that enables structural reparameterization during inference. This mechanism results in a family of ReParameterizable Vision Transformers (RePaViTs), which achieve remarkable latency reductions with acceptable sacrifices (sometimes gains) in accuracy across various ViTs. The benefits of our method scale consistently with model sizes, demonstrating greater speed improvements and progressively narrowing accuracy gaps or even higher accuracies on larger models. In particular, RePa-ViT-Large and RePa-ViT-Huge enjoy 66.8% and 68.7% speed-ups with +1.7% and +1.1% higher top-1 accuracies under the same training strategy, respectively. RePaViT is the first to employ structural reparameterization on FFN layers to expedite ViTs to our best knowledge, and we believe that it represents an auspicious direction for efficient ViTs. Source code is available at https://github.com/Ackesnal/RePaViT.",
      "tldr_zh": "è¯¥ç ”ç©¶æ­ç¤ºäº†åœ¨ Vision Transformer (ViT) ä¸­ï¼ŒFeedforward Network (FFN) å±‚è€Œéæ³¨æ„åŠ›å±‚æ˜¯æ¨ç†å»¶è¿Ÿçš„ä¸»è¦è´¡çŒ®è€…ï¼Œä¸”å…¶å½±å“éšæ¨¡å‹è§„æ¨¡å¢å¤§è€Œæ˜¾è‘—å¢åŠ ã€‚é’ˆå¯¹è¿™ä¸€å‘ç°ï¼Œä½œè€…æå‡ºäº† RePaViTï¼Œé€šè¿‡ä¸€ç§åˆ›æ–°çš„ channel idle æœºåˆ¶å®ç°äº† FFN å±‚çš„ç»“æ„é‡å‚æ•°åŒ– (structural reparameterization)ã€‚è¯¥æœºåˆ¶å…è®¸éƒ¨åˆ†ç‰¹å¾é€šé“åœ¨ FFN å±‚ä¸­ç»•è¿‡éçº¿æ€§æ¿€æ´»å‡½æ•°ï¼Œä»è€Œåœ¨æ¨ç†é˜¶æ®µå½¢æˆçº¿æ€§è·¯å¾„ä»¥è¿›è¡Œç»“æ„é‡å‚æ•°åŒ–ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒRePaViT åœ¨å¤šç§è§„æ¨¡çš„ ViT æ¨¡å‹ä¸Šå‡å®ç°äº†æ˜¾è‘—çš„æ¨ç†åŠ é€Ÿï¼Œä¸”åœ¨å¤§å‹æ¨¡å‹ä¸Šè¡¨ç°å°¤ä¸ºå‡ºè‰²ï¼ŒRePa-ViT-Large å’Œ RePa-ViT-Huge åˆ†åˆ«è·å¾—äº† 66.8% å’Œ 68.7% çš„é€Ÿåº¦æå‡ï¼ŒåŒæ—¶ Top-1 å‡†ç¡®ç‡åˆ†åˆ«æé«˜äº† 1.7% å’Œ 1.1%ã€‚ä½œä¸ºé¦–ä¸ªåœ¨ FFN å±‚åº”ç”¨ç»“æ„é‡å‚æ•°åŒ–ä»¥åŠ é€Ÿ ViT çš„å·¥ä½œï¼ŒRePaViT ä¸ºæ„å»ºé«˜æ•ˆå¤§è§„æ¨¡è§†è§‰æ¨¡å‹æä¾›äº†é‡è¦çš„æ–°æ–¹å‘ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "Accepted to ICML2025",
      "pdf_url": "https://arxiv.org/pdf/2505.21847v2",
      "published_date": "2025-05-28 00:27:18 UTC",
      "updated_date": "2025-06-02 06:39:14 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T15:41:50.748965+00:00"
    },
    {
      "arxiv_id": "2505.21841v1",
      "title": "An Optimistic Algorithm for online CMDPS with Anytime Adversarial Constraints",
      "title_zh": "é¢å‘å…·æœ‰ä»»æ„æ—¶åˆ»å¯¹æŠ—æ€§çº¦æŸçš„åœ¨çº¿ CMDP çš„ä¹è§‚ç®—æ³•",
      "authors": [
        "Jiahui Zhu",
        "Kihyun Yu",
        "Dabeen Lee",
        "Xin Liu",
        "Honghao Wei"
      ],
      "abstract": "Online safe reinforcement learning (RL) plays a key role in dynamic environments, with applications in autonomous driving, robotics, and cybersecurity. The objective is to learn optimal policies that maximize rewards while satisfying safety constraints modeled by constrained Markov decision processes (CMDPs). Existing methods achieve sublinear regret under stochastic constraints but often fail in adversarial settings, where constraints are unknown, time-varying, and potentially adversarially designed. In this paper, we propose the Optimistic Mirror Descent Primal-Dual (OMDPD) algorithm, the first to address online CMDPs with anytime adversarial constraints. OMDPD achieves optimal regret O(sqrt(K)) and strong constraint violation O(sqrt(K)) without relying on Slater's condition or the existence of a strictly known safe policy. We further show that access to accurate estimates of rewards and transitions can further improve these bounds. Our results offer practical guarantees for safe decision-making in adversarial environments.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹åœ¨çº¿å®‰å…¨å¼ºåŒ–å­¦ä¹ (Online Safe RL)åœ¨åŠ¨æ€ç¯å¢ƒä¸‹é¢ä¸´çš„æŒ‘æˆ˜ï¼Œç‰¹åˆ«æ˜¯ç°æœ‰æ–¹æ³•éš¾ä»¥å¤„ç†çº¦æŸæ¡ä»¶æœªçŸ¥ä¸”éšæ—¶é—´å‘ˆå¯¹æŠ—æ€§å˜åŒ–(Adversarial Constraints)çš„é—®é¢˜ã€‚è®ºæ–‡æå‡ºäº† Optimistic Mirror Descent Primal-Dual (OMDPD) ç®—æ³•ï¼Œè¿™æ˜¯é¦–ä¸ªè§£å†³å…·æœ‰éšæ—¶å¯¹æŠ—æ€§çº¦æŸçš„åœ¨çº¿å—é™é©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹(CMDPs)çš„ç®—æ³•ã€‚è¯¥ç®—æ³•ä¸ä¾èµ–äº Slater's condition æˆ–é¢„å…ˆå·²çŸ¥çš„ä¸¥æ ¼å®‰å…¨ç­–ç•¥ï¼Œé€šè¿‡ä¹è§‚ç­–ç•¥è¯„ä¼°ä¸é•œåƒä¸‹é™åŸå¯¹å¶æ¶æ„å®ç°é«˜æ•ˆå­¦ä¹ ã€‚ç†è®ºè¯æ˜ï¼ŒOMDPD å®ç°äº†æœ€ä¼˜çš„ $O(\\sqrt{K})$ ç´¯ç§¯é—æ†¾(Regret)å’Œ $O(\\sqrt{K})$ çš„å¼ºçº¦æŸè¿å(Constraint Violation)ä¿è¯ã€‚æ­¤å¤–ï¼Œç ”ç©¶è¡¨æ˜è·å–æ›´å‡†ç¡®çš„å¥–åŠ±å’Œè½¬ç§»æ¦‚ç‡ä¼°è®¡èƒ½è¿›ä¸€æ­¥ä¼˜åŒ–æ€§èƒ½ç•Œé™ã€‚è¯¥æˆæœä¸ºå¯¹æŠ—ç¯å¢ƒä¸‹çš„å®‰å…¨å†³ç­–æä¾›äº†åšå®çš„ç†è®ºæ”¯æ’‘ï¼Œåœ¨è‡ªåŠ¨é©¾é©¶å’Œæœºå™¨äººç­‰é¢†åŸŸå…·æœ‰é‡è¦çš„å®è·µåº”ç”¨ä»·å€¼ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "Proceedings of the 41 st International Conference on Machine Learning",
      "pdf_url": "https://arxiv.org/pdf/2505.21841v1",
      "published_date": "2025-05-28 00:16:34 UTC",
      "updated_date": "2025-05-28 00:16:34 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T15:41:58.362395+00:00"
    },
    {
      "arxiv_id": "2505.21838v1",
      "title": "Nonadaptive Output Regulation of Second-Order Nonlinear Uncertain Systems",
      "title_zh": "äºŒé˜¶éçº¿æ€§ä¸ç¡®å®šç³»ç»Ÿçš„éè‡ªé€‚åº”è¾“å‡ºè°ƒèŠ‚",
      "authors": [
        "Maobin Lu",
        "Martin Guay",
        "Telema Harry",
        "Shimin Wang",
        "Jordan Cooper"
      ],
      "abstract": "This paper investigates the robust output regulation problem of second-order nonlinear uncertain systems with an unknown exosystem. Instead of the adaptive control approach, this paper resorts to a robust control methodology to solve the problem and thus avoid the bursting phenomenon. In particular, this paper constructs generic internal models for the steady-state state and input variables of the system. By introducing a coordinate transformation, this paper converts the robust output regulation problem into a nonadaptive stabilization problem of an augmented system composed of the second-order nonlinear uncertain system and the generic internal models. Then, we design the stabilization control law and construct a strict Lyapunov function that guarantees the robustness with respect to unmodeled disturbances. The analysis shows that the output zeroing manifold of the augmented system can be made attractive by the proposed nonadaptive control law, which solves the robust output regulation problem. Finally, we demonstrate the effectiveness of the proposed nonadaptive internal model approach by its application to the control of the Duffing system.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†å…·æœ‰æœªçŸ¥å¤–éƒ¨ç³»ç»Ÿ(unknown exosystem)çš„äºŒé˜¶éçº¿æ€§ä¸ç¡®å®šç³»ç»Ÿçš„é²æ£’è¾“å‡ºè°ƒèŠ‚(robust output regulation)é—®é¢˜ã€‚ä¸ä¼ ç»Ÿçš„è‡ªé€‚åº”æ§åˆ¶(adaptive control)æ–¹æ³•ä¸åŒï¼Œæœ¬æ–‡é‡‡ç”¨é²æ£’æ§åˆ¶æ–¹æ³•æ¥è§£å†³è¯¥é—®é¢˜ï¼Œä»è€Œæœ‰æ•ˆé¿å…äº†æ§åˆ¶ä¸­çš„çªå‘è„‰å†²ç°è±¡(bursting phenomenon)ã€‚é€šè¿‡ä¸ºç³»ç»Ÿçš„ç¨³æ€çŠ¶æ€å’Œè¾“å…¥å˜é‡æ„å»ºé€šç”¨å†…æ¨¡å‹(generic internal models)å¹¶å¼•å…¥åæ ‡å˜æ¢ï¼Œç ”ç©¶å°†åŸé—®é¢˜è½¬åŒ–ä¸ºå¢å¹¿ç³»ç»Ÿçš„éè‡ªé€‚åº”ç¨³å®šåŒ–é—®é¢˜ã€‚åœ¨æ­¤åŸºç¡€ä¸Šï¼Œè®¾è®¡äº†ç¨³å®šåŒ–æ§åˆ¶å¾‹å¹¶æ„é€ äº†ä¸¥æ ¼çš„æé›…æ™®è¯ºå¤«å‡½æ•°(strict Lyapunov function)ï¼Œä»¥ç¡®ä¿ç³»ç»Ÿå¯¹æœªå»ºæ¨¡æ‰°åŠ¨çš„é²æ£’æ€§ã€‚ç†è®ºåˆ†æè¯æ˜äº†å¢å¹¿ç³»ç»Ÿçš„è¾“å‡ºå½’é›¶æµå½¢(output zeroing manifold)åœ¨æ‰€æç®—æ³•ä¸‹å…·æœ‰å¸å¼•æ€§ï¼ŒæˆåŠŸè§£å†³äº†é²æ£’è¾“å‡ºè°ƒèŠ‚é—®é¢˜ã€‚æœ€åï¼Œè¯¥éè‡ªé€‚åº”å†…æ¨¡å‹æ–¹æ³•åœ¨Duffingç³»ç»Ÿæ§åˆ¶ä¸­çš„æˆåŠŸåº”ç”¨éªŒè¯äº†å…¶æœ‰æ•ˆæ€§ã€‚",
      "categories": [
        "eess.SY",
        "cs.AI",
        "math.OC",
        "nlin.CD"
      ],
      "primary_category": "eess.SY",
      "comment": "8 pages, 3 figures",
      "pdf_url": "https://arxiv.org/pdf/2505.21838v1",
      "published_date": "2025-05-28 00:13:37 UTC",
      "updated_date": "2025-05-28 00:13:37 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T15:42:08.213402+00:00"
    }
  ],
  "processing_status": "completed",
  "error": null,
  "raw_papers_fetched": true,
  "papers_count": 220,
  "processed_papers_count": 220,
  "failed_papers_count": 0,
  "llm_backup_calls": 0,
  "summary_generated": true,
  "daily_data_saved": true,
  "last_update": "2026-01-23T15:43:15.175044+00:00"
}
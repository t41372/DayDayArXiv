{
  "date": "2025-06-07",
  "category": "cs.AI",
  "summary": "æ¬¢è¿æ¥åˆ° UTC æ—¶é—´ 2025-06-07 çš„ arXiv ä¸­æ–‡ TLDR å¿«æŠ¥ï¼\n\n**ä¸€å¥è¯æ€»ç»“ï¼š**\nä»Šå¤©çš„ arXiv å……æ»¡äº†å¯¹ **LLM â€œæ€è€ƒâ€æœ¬è´¨çš„æ·±åº¦åæ€**ä¸**å¤§è§„æ¨¡ä»£ç†ï¼ˆAgentï¼‰æ¨¡æ‹Ÿ**çš„çªç ´ã€‚Samy Bengio å›¢é˜Ÿå¯¹æ¨ç†æ¨¡å‹ï¼ˆReasoning Modelsï¼‰æå‡ºäº†å°–é”çš„è´¨ç–‘ï¼Œè€Œå¾®è½¯ç ”ç©¶é™¢åˆ™ä¸ä»…å°† Multi-Agent æ¨¡æ‹Ÿæ‰©å±•åˆ°äº†æƒŠäººçš„**åäº¿çº§ï¼ˆOne Billionï¼‰**ï¼Œè¿˜åœ¨å…·èº«æ™ºèƒ½ï¼ˆEmbodied AIï¼‰çš„åœ°å›¾æ„å»ºä¸Šæœ‰äº†æ–°è¿›å±•ã€‚\n\n---\n\n### ğŸ§  æ·±åº¦å…³æ³¨ï¼šLLM çš„æ¨ç†å¹»è§‰ä¸è‡ªæˆ‘ä¿®æ­£\nä»Šå¤©æœ€å€¼å¾—ä¸€è¯»çš„æ˜¯å¯¹æ‰€è°“â€œæ¨ç†å¤§æ¨¡å‹â€ï¼ˆLarge Reasoning Models, LRMsï¼‰çš„ç¥›é­…ï¼Œä»¥åŠå¦‚ä½•é€šè¿‡è¯¾ç¨‹å­¦ä¹ å’Œè‡ªæˆ‘ä¿®æ­£æ¥çœŸæ­£æå‡æ¨ç†èƒ½åŠ›ã€‚\n\n**1. [NeurIPS 2025] æ€è€ƒçš„é”™è§‰ï¼šé€šè¿‡é—®é¢˜å¤æ‚åº¦çš„è§†è§’ç†è§£æ¨ç†æ¨¡å‹çš„ä¼˜åŠ¿ä¸å±€é™**\n**The Illusion of Thinking: Understanding the Strengths and Limitations of Reasoning Models via the Lens of Problem Complexity**\n> Authors: Parshin Shojaee, Samy Bengio, et al.\n> **æ ¸å¿ƒå‘ç°**ï¼šè¿™ç¯‡æ–‡ç« éå¸¸çŠ€åˆ©ã€‚Samy Bengio å›¢é˜Ÿç ”ç©¶äº†åƒ o1 è¿™ç§ç”Ÿæˆè¯¦ç»†â€œæ€ç»´é“¾â€çš„ LRMsã€‚ä»–ä»¬å‘ç°ï¼ŒLRM å¹¶ä¸åƒæˆ‘ä»¬ä»¥ä¸ºçš„é‚£æ ·å…·æœ‰é€šç”¨æ¨ç†èƒ½åŠ›ã€‚\n> **å…³é”®ç»“è®º**ï¼š\n> 1.  **å‡†ç¡®ç‡åå¡Œ**ï¼šåœ¨ä½å¤æ‚åº¦ä»»åŠ¡ä¸Šï¼Œæ™®é€š LLM ç”šè‡³æ¯” LRM æ›´å¥½ï¼›åœ¨ä¸­ç­‰å¤æ‚åº¦ä¸Š LRM æœ‰ä¼˜åŠ¿ï¼›ä½†åœ¨é«˜å¤æ‚åº¦ä»»åŠ¡ä¸Šï¼Œä¸¤è€…éƒ½ä¼šâ€œå½»åº•åå¡Œâ€ã€‚\n> 2.  **ç®—åŠ›æ‚–è®º**ï¼šéšç€é—®é¢˜å˜éš¾ï¼ŒLRM çš„æ¨ç†è®¡ç®—é‡ä¼šå¢åŠ ï¼Œä½†åˆ°äº†æŸä¸ªä¸´ç•Œç‚¹åï¼Œå³ä½¿è¿˜æœ‰ token é¢„ç®—ï¼Œå®ƒä»¬çš„æ¨ç†åŠªåŠ›åè€Œä¼šä¸‹é™ã€‚\n> 3.  **ç¼ºä¹ç®—æ³•æ€ç»´**ï¼šLRM å¹¶æ²¡æœ‰å­¦ä¼šç²¾ç¡®çš„ç®—æ³•ï¼Œå®ƒä»¬çš„â€œæ€è€ƒâ€åœ¨ä¸åŒè§„æ¨¡ä¸‹æ˜¯ä¸ä¸€è‡´çš„ã€‚\n\n**2. SPOCï¼šé€šè¿‡è‡ªå‘è‡ªæˆ‘ä¿®æ­£æå‡ LLM æ¨ç†èƒ½åŠ›**\n**Boosting LLM Reasoning via Spontaneous Self-Correction**\n> Authors: Xutong Zhao, et al.\n> **æ ¸å¿ƒè´¡çŒ®**ï¼šé’ˆå¯¹æ¨ç†é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§**SPOCï¼ˆè‡ªå‘è‡ªæˆ‘ä¿®æ­£ï¼‰**æ–¹æ³•ã€‚ä¸åŒäºä»¥å¾€â€œç”Ÿæˆåä¿®æ­£â€çš„èŒƒå¼ï¼ŒSPOC è®©æ¨¡å‹åœ¨ä¸€æ¬¡æ¨ç†è¿‡ç¨‹ä¸­ï¼Œäº¤æ›¿ç”Ÿæˆè§£å†³æ–¹æ¡ˆå’ŒéªŒè¯æ­¥éª¤ã€‚å¦‚æœéªŒè¯å¤±è´¥ï¼ŒåŠ¨æ€ç»ˆæ­¢å¹¶ä¿®æ­£ã€‚è¿™ç§â€œè¾¹åšè¾¹æ£€æŸ¥â€çš„æœºåˆ¶åœ¨ Llama-3.1-70B ä¸Šå°† MATH500 çš„å‡†ç¡®ç‡æå‡äº† 11.6%ã€‚\n\n**3. ä»ç®€å•åˆ°å›°éš¾çš„è¯¾ç¨‹å¼ºåŒ–å­¦ä¹ æå‡ LLM æ¨ç†**\n**Curriculum Reinforcement Learning from Easy to Hard Tasks Improves LLM Reasoning**\n> Authors: Shubham Parashar, Shuiwang Ji, et al.\n> **æ ¸å¿ƒè´¡çŒ®**ï¼šå€Ÿé‰´è¯¾ç¨‹å­¦ä¹ ï¼ˆCurriculum Learningï¼‰ï¼Œæå‡ºäº† **E2H Reasoner**ã€‚ç›´æ¥ç”¨ RL è®­ç»ƒéš¾é¢˜æ•ˆæœä¸å¥½ï¼ŒE2H é€šè¿‡ä»æ˜“åˆ°éš¾çš„ä»»åŠ¡è°ƒåº¦ï¼Œå¹¶å»ºç«‹ç†è®ºä¸Šçš„æ ·æœ¬å¤æ‚åº¦ç•Œé™ï¼Œè¯æ˜äº†è¿™ç§æ¸è¿›å¼ RL è®­ç»ƒèƒ½æ˜¾è‘—æå‡å°å‚æ•°æ¨¡å‹ï¼ˆ1.5B-3Bï¼‰çš„æ¨ç†èƒ½åŠ›ã€‚\n\n---\n\n### ğŸŒ å¤§è§„æ¨¡æ¨¡æ‹Ÿä¸ç¤¾ä¼šæ™ºèƒ½\nAgent ç ”ç©¶æ­£åœ¨ä»â€œå‡ ä¸ªæ™ºèƒ½ä½“å¼€ä¼šâ€èµ°å‘â€œåœ°çƒçº§ç¤¾ä¼šæ¨¡æ‹Ÿâ€ã€‚\n\n**4. [å¾®è½¯ç ”ç©¶é™¢] æ¨¡æ‹Ÿåœ°çƒçº§äººç±»ç¤¾ä¼šï¼šåäº¿æ™ºèƒ½ä½“**\n**Modeling Earth-Scale Human-Like Societies with One Billion Agents**\n> Authors: Haoxiang Guan, Tie-Yan Liu, et al. (Microsoft Research)\n> **æ ¸å¿ƒè´¡çŒ®**ï¼šè¿™æ˜¯ä¸€ä¸ªå·¥ç¨‹å’Œç®—æ³•çš„å£®ä¸¾ã€‚ä½œè€…æå‡ºäº† **Light Society**ï¼Œä¸€ä¸ªèƒ½é«˜æ•ˆæ¨¡æ‹Ÿ**åäº¿ï¼ˆ1 Billionï¼‰** LLM é©±åŠ¨çš„æ™ºèƒ½ä½“çš„æ¡†æ¶ã€‚\n> **æŠ€æœ¯ç‚¹**ï¼šå®ƒä¸æ˜¯ç®€å•çš„å †ç Œï¼Œè€Œæ˜¯å°†ç¤¾ä¼šè¿‡ç¨‹å½¢å¼åŒ–ä¸ºçŠ¶æ€è½¬ç§»ï¼Œåˆ©ç”¨æ¨¡å—åŒ–è®¾è®¡å’Œäº‹ä»¶é˜Ÿåˆ—æ¥å¤„ç†æå¤§è§„æ¨¡çš„å¹¶å‘ã€‚å®éªŒæ¨¡æ‹Ÿäº†åäº¿è§„æ¨¡çš„ä¿¡ä»»åšå¼ˆå’Œè§‚ç‚¹ä¼ æ’­ï¼Œç”šè‡³è§‚å¯Ÿåˆ°äº†æ¶Œç°å‡ºçš„ç¤¾ä¼šâ€œç¼©æ”¾å®šå¾‹â€ï¼ˆscaling lawsï¼‰ã€‚\n\n**5. è”åˆæ€ç»´è¿˜æ˜¯å­¤ç«‹ä»£ç†ï¼Ÿè®¤çŸ¥è´Ÿè·ç†è®ºä¸‹çš„ LLM åä½œ**\n**United Minds or Isolated Agents? Exploring Coordination of LLMs under Cognitive Load Theory**\n> Authors: HaoYang Shang, et al.\n> **æ ¸å¿ƒè´¡çŒ®**ï¼šä»è®¤çŸ¥ç§‘å­¦ä¸­çš„**è®¤çŸ¥è´Ÿè·ç†è®ºï¼ˆCLTï¼‰**å‡ºå‘ï¼Œè§£é‡Šäº†ä¸ºä»€ä¹ˆå¤šæ™ºèƒ½ä½“åä½œæœ‰æ—¶ä¼šå¤±è´¥ã€‚æå‡ºäº† **CoThinker** æ¡†æ¶ï¼Œé€šè¿‡è®© Agent ä¸“ä¸šåŒ–æ¥åˆ†é…â€œå†…åœ¨è®¤çŸ¥è´Ÿè·â€ï¼Œå¹¶é€šè¿‡é›†ä½“å·¥ä½œè®°å¿†æ¥ç®¡ç†â€œäº¤äº’è®¤çŸ¥è´Ÿè·â€ï¼Œæœ‰æ•ˆçªç ´äº†å•ä½“ LLM çš„èƒ½åŠ›å¤©èŠ±æ¿ã€‚\n\n---\n\n### ğŸ¤– å…·èº«æ™ºèƒ½ä¸æœºå™¨äºº\næœºå™¨äººçš„â€œå¤§è„‘â€éœ€è¦æ›´å¥½åœ°ç†è§£â€œèº«ä½“â€å’Œç¯å¢ƒã€‚\n\n**6. [IJRR æ¥æ”¶] æœºå™¨äººå¯¼èˆªä¸æ“ä½œçš„å¤šæ¨¡æ€ç©ºé—´è¯­è¨€åœ°å›¾**\n**Multimodal Spatial Language Maps for Robot Navigation and Manipulation**\n> Authors: Chenguang Huang, Wolfram Burgard, et al.\n> **æ ¸å¿ƒè´¡çŒ®**ï¼šæœºå™¨äººå¤§ç‰› Wolfram Burgard å‚ä¸çš„å·¥ä½œã€‚æå‡ºäº† **VLMaps** å’Œ **AVLMaps**ï¼ˆåŠ å…¥éŸ³é¢‘ï¼‰ã€‚\n> **æ ¸å¿ƒæ€æƒ³**ï¼šç›´æ¥å°†é¢„è®­ç»ƒçš„å¤šæ¨¡æ€å¤§æ¨¡å‹ç‰¹å¾ï¼ˆè§†è§‰ã€è¯­è¨€ã€éŸ³é¢‘ï¼‰èåˆè¿› 3D ç¯å¢ƒåœ°å›¾ä¸­ã€‚è¿™ä½¿å¾—æœºå™¨äººå¯ä»¥ç›´æ¥ç†è§£è‡ªç„¶è¯­è¨€æŒ‡ä»¤ï¼ˆå¦‚â€œå»æ²™å‘å’Œç”µè§†ä¹‹é—´â€ï¼‰ï¼Œä¸ä»…èƒ½å¯¼èˆªï¼Œè¿˜èƒ½æ ¹æ®æ¨¡ç³Šçš„å¬è§‰æˆ–è§†è§‰çº¿ç´¢æ¶ˆé™¤æ­§ä¹‰ã€‚\n\n**7. [WACV 2026] è¿ˆå‘æµå¼ LiDAR ç›®æ ‡æ£€æµ‹ï¼šå°†ç‚¹äº‘è§†ä¸ºä»¥è‡ªæˆ‘ä¸ºä¸­å¿ƒçš„åºåˆ—**\n**Towards Streaming LiDAR Object Detection with Point Clouds as Egocentric Sequences**\n> Authors: Mellon M. Zhang, et al.\n> **æ ¸å¿ƒè´¡çŒ®**ï¼šé’ˆå¯¹è‡ªåŠ¨é©¾é©¶ï¼Œæå‡ºäº† **PFCF** æ··åˆæ£€æµ‹å™¨ã€‚ç»“åˆäº†å¿«é€Ÿçš„æåæ ‡å¤„ç†ï¼ˆç”¨äºæ‰‡åŒºç‰¹å¾æå–ï¼‰å’Œç²¾ç¡®çš„ç¬›å¡å°”åæ ‡æ¨ç†ï¼ˆç”¨äºå…¨åœºæ™¯ç†è§£ï¼‰ã€‚åˆ©ç”¨ Mamba SSM æ¶æ„ï¼Œåœ¨ Waymo æ•°æ®é›†ä¸Šå»ºç«‹äº†ä¸€ä¸ªæ–°çš„å¸•ç´¯æ‰˜å‰æ²¿ï¼Œé€Ÿåº¦ç¿»å€ä¸”ç²¾åº¦æŒå¹³å…¨æ‰«ææ–¹æ³•ã€‚\n\n---\n\n### ğŸ›¡ï¸ AI å®‰å…¨ã€æ”»å‡»ä¸é˜²å¾¡\n\n**8. ä»å¨èƒåˆ°å·¥å…·ï¼šåˆ©ç”¨æ‹’ç»æ„ŸçŸ¥æ³¨å…¥æ”»å‡»è¿›è¡Œå®‰å…¨å¯¹é½**\n**From Threat to Tool: Leveraging Refusal-Aware Injection Attacks for Safety Alignment**\n> Authors: Kyubyung Chae, et al.\n> **æ ¸å¿ƒè´¡çŒ®**ï¼šæå‡ºäº† **RAAI**ã€‚è¿™æ˜¯ä¸€ä¸ªå¾ˆæœ‰æ„æ€çš„é€†å‘æ€ç»´ï¼šåˆ©ç”¨æ”»å‡»æŠ€æœ¯æ¥åšé˜²å¾¡ã€‚é€šè¿‡æ£€æµ‹æ¨¡å‹çš„â€œæ‹’ç»â€ä¿¡å·å¹¶è‡ªé€‚åº”æ³¨å…¥çŸ­è¯­æ¥è¯±å¯¼æœ‰å®³è¾“å‡ºï¼Œä»è€Œç”Ÿæˆé«˜è´¨é‡çš„çº¢é˜Ÿæ”»å‡»æ•°æ®ï¼Œå†ç”¨è¿™äº›æ•°æ®å¾®è°ƒæ¨¡å‹ï¼Œæ˜¾è‘—æå‡äº†å®‰å…¨æ€§ã€‚\n\n**9. é‡å†™é¢„ç®—ï¼šæˆæœ¬ä¸å¯¹ç§°ä¸‹çš„é»‘ç›’æ”»å‡»é€šç”¨æ¡†æ¶**\n**Rewriting the Budget: A General Framework for Black-Box Attacks Under Cost Asymmetry**\n> Authors: Mahdi Salmani, et al.\n> **æ ¸å¿ƒè´¡çŒ®**ï¼šç°å®ä¸–ç•Œä¸­ï¼ŒAPI è°ƒç”¨æˆ–è§¦å‘å®¡æ ¸çš„æˆæœ¬æ˜¯ä¸ä¸€æ ·çš„ã€‚æ–‡ç« æå‡ºäº†**éå¯¹ç§°é»‘ç›’æ”»å‡»**æ¡†æ¶ï¼Œé€šè¿‡å€¾å‘äºä½æˆæœ¬æŸ¥è¯¢çš„æ¢¯åº¦ä¼°è®¡ï¼Œåœ¨é™ä½æ”»å‡»æˆæœ¬çš„åŒæ—¶ä¿æŒäº†æ”»å‡»æˆåŠŸç‡ã€‚\n\n---\n\n### ğŸ› ï¸ å…¶ä»–å€¼å¾—å…³æ³¨çš„ç²¾é€‰\n\n*   **[Vision-Language] è§†è§‰æç¤ºçš„é²æ£’æ€§ç»§æ‰¿**\n    **Exploring Visual Prompting: Robustness Inheritance and Beyond** (#32)\n    **çœ‹ç‚¹**ï¼šç ”ç©¶äº† Visual Prompting (VP) æ˜¯å¦èƒ½ç»§æ‰¿æºæ¨¡å‹çš„é²æ£’æ€§ã€‚æå‡ºäº† Prompt Boundary Loosening (PBL) ç­–ç•¥ï¼Œèƒ½åœ¨ç»§æ‰¿é²æ£’æ€§çš„åŒæ—¶æå‡æ³›åŒ–èƒ½åŠ›ã€‚\n\n*   **[Time Series] åˆ©ç”¨è§†è§‰è¯­è¨€æ¨¡å‹è¿›è¡Œæ—¶é—´åºåˆ—å¼‚å¸¸æ£€æµ‹**\n    **Harnessing Vision-Language Models for Time Series Anomaly Detection** (#28)\n    **çœ‹ç‚¹**ï¼šAAAI 2026 Oralã€‚æŠŠæ—¶é—´åºåˆ—è½¬ä¸º 2D å›¾åƒï¼Œåˆ©ç”¨ VLM å¼ºå¤§çš„è§†è§‰ç†è§£èƒ½åŠ›æ¥æ£€æµ‹å¼‚å¸¸ã€‚æ— éœ€æ—¶é—´åºåˆ—è®­ç»ƒï¼Œæ•ˆæœå‡»è´¥äº†ä¸“é—¨çš„æ—¶é—´åºåˆ—æ¨¡å‹ã€‚\n\n*   **[Tokenizer] æ— éœ€è®­ç»ƒçš„ Tokenizer ç§»æ¤**\n    **Training-Free Tokenizer Transplantation via Orthogonal Matching Pursuit** (#72)\n    **çœ‹ç‚¹**ï¼šæƒ³ç»™ Llama æ¢ä¸ª Tokenizer ä½†ä¸æƒ³é‡è®­ï¼Ÿè¿™ç¯‡æ–‡ç« åˆ©ç”¨æ­£äº¤åŒ¹é…è¿½è¸ªï¼ˆOMPï¼‰é€šè¿‡ç¨€ç–çº¿æ€§ç»„åˆé‡å»ºæœªè§è¿‡çš„ Tokenï¼Œå®ç°äº†ä¸åŒæ¨¡å‹é—´ï¼ˆå¦‚ Qwen åˆ° Llamaï¼‰çš„æ— ç¼ç§»æ¤ã€‚\n\n*   **[Benchmark] VisioMath: å›¾å½¢æ•°å­¦æ¨ç†åŸºå‡†**\n    **VisioMath: Benchmarking Figure-based Mathematical Reasoning in LMMs** (#49)\n    **çœ‹ç‚¹**ï¼šK-12 æ•°å­¦é¢˜ï¼Œç‰¹ç‚¹æ˜¯é€‰é¡¹éƒ½æ˜¯é•¿å¾—å¾ˆåƒçš„å›¾ã€‚ç»“æœæ˜¾ç¤ºç°æœ‰ LMM ç»å¸¸ççŒœï¼Œé€šè¿‡â€œæ‰¾ä¸åŒâ€çš„å¾®è°ƒç­–ç•¥å¯ä»¥æå‡æ•ˆæœã€‚\n\n*   **[Audio Coding] ç”¨äºéŸ³é¢‘ç¼–ç çš„ç¥ç»é¢‘å¸¦ç”Ÿæˆ**\n    **Neural Spectral Band Generation for Audio Coding** (#47)\n    **çœ‹ç‚¹**ï¼šInterspeech 2025ã€‚åˆ©ç”¨ç”Ÿæˆå¼å¯¹æŠ—ç½‘ç»œæ¥ç”Ÿæˆé«˜é¢‘æ®µéŸ³é¢‘ï¼Œæ¯”ä¼ ç»Ÿçš„ SBR æŠ€æœ¯å¬æ„Ÿæ›´å¥½ï¼Œæ¯”ç‰¹ç‡æ›´ä½ã€‚\n\n*   **[Fuzzy Theory] ä»æ¨¡å‹æ§åˆ¶åˆ°æ¼”åŒ–æ¨¡ç³Šæ§åˆ¶**\n    **From Model-Based and Adaptive Control to Evolving Fuzzy Control** (#75)\n    **çœ‹ç‚¹**ï¼šä¸ºäº†çºªå¿µæ¨¡ç³Šé›†åˆç†è®º 60 å‘¨å¹´ï¼ˆFuzz-IEEE 2025ï¼‰ï¼Œå›é¡¾äº†æ¨¡ç³Šæ§åˆ¶çš„å‘å±•ï¼Œç‰¹åˆ«æ˜¯é€‚åº”éå¹³ç¨³ç¯å¢ƒçš„æ¼”åŒ–æ™ºèƒ½ç³»ç»Ÿã€‚\n\n---\nå¸Œæœ›è¿™ä»½å¿«æŠ¥èƒ½ä¸ºä½ èŠ‚çœé˜…è¯»æ—¶é—´ï¼æ˜å¤©è§ï¼",
  "papers": [
    {
      "arxiv_id": "2506.06946v3",
      "title": "Making a Pipeline Production-Ready: Challenges and Lessons Learned in the Healthcare Domain",
      "title_zh": "æ‰“é€ ç”Ÿäº§å°±ç»ªçš„æµæ°´çº¿ï¼šåŒ»ç–—é¢†åŸŸçš„æŒ‘æˆ˜ä¸ç»éªŒæ•™è®­",
      "authors": [
        "Daniel Angelo Esteves Lawand",
        "Lucas Quaresma Medina Lam",
        "Roberto Oliveira Bolgheroni",
        "Renato Cordeiro Ferreira",
        "Alfredo Goldman",
        "Marcelo Finger"
      ],
      "abstract": "Deploying a Machine Learning (ML) training pipeline into production requires good software engineering practices. Unfortunately, the typical data science workflow often leads to code that lacks critical software quality attributes. This experience report investigates this problem in SPIRA, a project whose goal is to create an ML-Enabled System (MLES) to pre-diagnose insufficiency respiratory via speech analysis. This paper presents an overview of the architecture of the MLES, then compares three versions of its Continuous Training subsystem: from a proof of concept Big Ball of Mud (v1), to a design pattern-based Modular Monolith (v2), to a test-driven set of Microservices (v3) Each version improved its overall extensibility, maintainability, robustness, and resiliency. The paper shares challenges and lessons learned in this process, offering insights for researchers and practitioners seeking to productionize their pipelines.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†å°†æœºå™¨å­¦ä¹ (Machine Learning)è®­ç»ƒæµæ°´çº¿éƒ¨ç½²åˆ°ç”Ÿäº§ç¯å¢ƒæ—¶ï¼Œå› æ•°æ®ç§‘å­¦å·¥ä½œæµç¼ºä¹è½¯ä»¶å·¥ç¨‹å®è·µè€Œå¯¼è‡´çš„è´¨é‡æŒ‘æˆ˜ã€‚ä»¥SPIRAé¡¹ç›®ï¼ˆé€šè¿‡è¯­éŸ³åˆ†æé¢„è¯Šæ–­å‘¼å¸åŠŸèƒ½ä¸å…¨ï¼‰ä¸ºæ¡ˆä¾‹ï¼Œæœ¬æ–‡è¯¦ç»†å¯¹æ¯”äº†å…¶æŒç»­è®­ç»ƒ(Continuous Training)å­ç³»ç»Ÿçš„ä¸‰ä¸ªæ¼”è¿›ç‰ˆæœ¬ï¼šä»æ¦‚å¿µéªŒè¯é˜¶æ®µçš„â€œå¤§æ³¥çƒâ€æ¶æ„(Big Ball of Mud, v1)ï¼Œåˆ°åŸºäºè®¾è®¡æ¨¡å¼çš„æ¨¡å—åŒ–å•ä½“æ¶æ„(Modular Monolith, v2)ï¼Œå†åˆ°æµ‹è¯•é©±åŠ¨çš„å¾®æœåŠ¡æ¶æ„(Microservices, v3)ã€‚ç ”ç©¶å‘ç°ï¼Œéšç€æ¶æ„çš„é€æ­¥æ¼”è¿›ï¼Œç³»ç»Ÿçš„å¯æ‰©å±•æ€§(Extensibility)ã€å¯ç»´æŠ¤æ€§(Maintainability)ã€å¥å£®æ€§(Robustness)å’Œå¼¹æ€§(Resiliency)å‡å¾—åˆ°äº†æ˜¾è‘—æå‡ã€‚æ–‡ç« æ·±å…¥åˆ†äº«äº†è¿™ä¸€è¿‡ç¨‹ä¸­é¢ä¸´çš„å…·ä½“æŒ‘æˆ˜ä¸ç»éªŒæ•™è®­ï¼Œä¸ºè‡´åŠ›äºå°†æµæ°´çº¿ç”Ÿäº§åŒ–çš„ç ”ç©¶äººå‘˜å’Œä»ä¸šè€…æä¾›äº†é‡è¦çš„å®è·µæŒ‡å—ä¸æ´å¯Ÿã€‚",
      "categories": [
        "cs.SE",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.SE",
      "comment": "8 pages, 3 figures (2 diagrams, 2 code listings), accepted to the workshop SADIS 2025",
      "pdf_url": "https://arxiv.org/pdf/2506.06946v3",
      "published_date": "2025-06-07 23:00:13 UTC",
      "updated_date": "2025-07-06 15:45:08 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T19:41:16.612993+00:00"
    },
    {
      "arxiv_id": "2506.06944v2",
      "title": "Towards Streaming LiDAR Object Detection with Point Clouds as Egocentric Sequences",
      "title_zh": "è¿ˆå‘å°†ç‚¹äº‘è§†ä¸ºè‡ªæˆ‘ä¸­å¿ƒåºåˆ—çš„æµå¼æ¿€å…‰é›·è¾¾ç›®æ ‡æ£€æµ‹",
      "authors": [
        "Mellon M. Zhang",
        "Glen Chou",
        "Saibal Mukhopadhyay"
      ],
      "abstract": "Accurate and low-latency 3D object detection is essential for autonomous driving, where safety hinges on both rapid response and reliable perception. While rotating LiDAR sensors are widely adopted for their robustness and fidelity, current detectors face a trade-off: streaming methods process partial polar sectors on the fly for fast updates but suffer from limited visibility, cross-sector dependencies, and distortions from retrofitted Cartesian designs, whereas full-scan methods achieve higher accuracy but are bottlenecked by the inherent latency of a LiDAR revolution. We propose Polar-Fast-Cartesian-Full (PFCF), a hybrid detector that combines fast polar processing for intra-sector feature extraction with accurate Cartesian reasoning for full-scene understanding. Central to PFCF is a custom Mamba SSM-based streaming backbone with dimensionally-decomposed convolutions that avoids distortion-heavy planes, enabling parameter-efficient, translation-invariant, and distortion-robust polar representation learning. Local sector features are extracted via this backbone, then accumulated into a sector feature buffer to enable efficient inter-sector communication through a full-scan backbone. PFCF establishes a new Pareto frontier on the Waymo Open dataset, surpassing prior streaming baselines by 10% mAP and matching full-scan accuracy at twice the update rate. Code is available at \\href{https://github.com/meilongzhang/Polar-Hierarchical-Mamba}{https://github.com/meilongzhang/Polar-Hierarchical-Mamba}.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹è‡ªåŠ¨é©¾é©¶ä¸­3Dç›®æ ‡æ£€æµ‹åœ¨å‡†ç¡®æ€§ä¸ä½å»¶è¿Ÿä¹‹é—´çš„æƒè¡¡æŒ‘æˆ˜ï¼Œæå‡ºäº†åä¸ºPolar-Fast-Cartesian-Full (PFCF)çš„æ··åˆæ£€æµ‹å™¨ã€‚è¯¥æ¡†æ¶ç»“åˆäº†ç”¨äºæ‰‡åŒºå†…ç‰¹å¾æå–çš„å¿«é€Ÿæåæ ‡(Polar)å¤„ç†å’Œç”¨äºå…¨åœºæ™¯ç†è§£çš„ç²¾ç¡®ç¬›å¡å°”(Cartesian)æ¨ç†ï¼Œæœ‰æ•ˆå¼¥è¡¥äº†ä¼ ç»Ÿæµå¼æ–¹æ³•åœ¨è§†é‡å—é™å’Œå‡ ä½•ç•¸å˜æ–¹é¢çš„ç¼ºé™·ã€‚PFCFçš„æ ¸å¿ƒæ˜¯ä¸€ä¸ªåŸºäºMamba SSMçš„å®šåˆ¶æµå¼éª¨å¹²ç½‘ç»œï¼Œåˆ©ç”¨ç»´åº¦åˆ†è§£å·ç§¯(dimensionally-decomposed convolutions)æ¥å®ç°å…·æœ‰å¹³ç§»ä¸å˜æ€§ä¸”æŠ—ç•¸å˜çš„æåæ ‡è¡¨ç¤ºå­¦ä¹ ã€‚é€šè¿‡å°†å±€éƒ¨æ‰‡åŒºç‰¹å¾ç´¯ç§¯è‡³ç¼“å†²åŒºï¼Œè¯¥ç³»ç»Ÿèƒ½å¤Ÿåˆ©ç”¨å…¨æ‰«æéª¨å¹²ç½‘ç»œå®ç°é«˜æ•ˆçš„è·¨æ‰‡åŒºé€šä¿¡ã€‚åœ¨Waymo Openæ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒPFCFæ¯”ä¹‹å‰çš„æµå¼åŸºå‡†æ¨¡å‹æå‡äº†10%çš„mAPï¼Œå¹¶åœ¨ä¿æŒå…¨æ‰«æç²¾åº¦çš„åŒæ—¶å®ç°äº†ä¸¤å€çš„æ›´æ–°é¢‘ç‡ã€‚è¯¥ç ”ç©¶æˆåŠŸåœ¨æ„ŸçŸ¥å¯é æ€§ä¸å®æ—¶å“åº”ä¹‹é—´å»ºç«‹äº†æ–°çš„å¸•ç´¯æ‰˜å‰æ²¿(Pareto frontier)ï¼Œä¸ºå®‰å…¨è‡ªåŠ¨é©¾é©¶æä¾›äº†å…³é”®æŠ€æœ¯æ”¯æŒã€‚",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "Accepted to WACV 2026",
      "pdf_url": "https://arxiv.org/pdf/2506.06944v2",
      "published_date": "2025-06-07 22:53:50 UTC",
      "updated_date": "2025-12-31 21:00:18 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T19:41:25.992907+00:00"
    },
    {
      "arxiv_id": "2506.06941v3",
      "title": "The Illusion of Thinking: Understanding the Strengths and Limitations of Reasoning Models via the Lens of Problem Complexity",
      "title_zh": "æ€ç»´çš„å¹»è±¡ï¼šä»é—®é¢˜å¤æ‚åº¦è§†è§’æ¢ç©¶æ¨ç†æ¨¡å‹çš„ä¼˜åŠ¿ä¸å±€é™",
      "authors": [
        "Parshin Shojaee",
        "Iman Mirzadeh",
        "Keivan Alizadeh",
        "Maxwell Horton",
        "Samy Bengio",
        "Mehrdad Farajtabar"
      ],
      "abstract": "Recent generations of language models have introduced Large Reasoning Models (LRMs) that generate detailed thinking processes before providing answers. While these models demonstrate improved performance on reasoning benchmarks, their fundamental capabilities, scaling properties, and limitations remain insufficiently understood. Current evaluations primarily focus on established math and coding benchmarks, emphasizing final answer accuracy. However, this evaluation paradigm often suffers from contamination and does not provide insights into the reasoning traces. In this work, we systematically investigate these gaps with the help of controllable puzzle environments that allow precise manipulation of complexity while maintaining consistent logical structures. This setup enables the analysis of not only final answers but also the internal reasoning traces, offering insights into how LRMs think. Through extensive experiments, we show that LRMs face a complete accuracy collapse beyond certain complexities. Moreover, they exhibit a counterintuitive scaling limit: their reasoning effort increases with problem complexity up to a point, then declines despite having remaining token budget. By comparing LRMs with their standard LLM counterparts under same inference compute, we identify three performance regimes: (1) low-complexity tasks where standard models outperform LRMs, (2) medium-complexity tasks where LRMs demonstrates advantage, and (3) high-complexity tasks where both models face complete collapse. We found that LRMs have limitations in exact computation: they fail to use explicit algorithms and reason inconsistently across scales. We also investigate the reasoning traces in more depth, studying the patterns of explored solutions and analyzing the models' computational behavior, shedding light on their strengths, limitations, and raising questions about their reasoning capabilities.",
      "tldr_zh": "è¯¥ç ”ç©¶é€šè¿‡å¯æ§è°œé¢˜ç¯å¢ƒç³»ç»Ÿæ¢è®¨äº†å¤§å‹æ¨ç†æ¨¡å‹ (Large Reasoning Models, LRMs) çš„ä¼˜åŠ¿ä¸å±€é™ï¼Œåˆ†æäº†å¤æ‚åº¦å˜åŒ–å¯¹æ¨¡å‹æ€ç»´é“¾åŠæœ€ç»ˆé¢„æµ‹çš„å½±å“ã€‚å®éªŒå‘ç° LRMs åœ¨é¢ä¸´é«˜éš¾åº¦ä»»åŠ¡æ—¶ä¼šå‡ºç°å‡†ç¡®ç‡å½»åº•å´©å¡Œï¼Œå¹¶æ­ç¤ºäº†ä¸€ç§åç›´è§‰çš„ç¼©æ”¾é™åˆ¶ï¼šæ¨ç†æŠ•å…¥éšå¤æ‚åº¦å¢åŠ ï¼Œä½†åœ¨è¾¾åˆ°ä¸´ç•Œç‚¹åä¼šæ— è§†å‰©ä½™ Token é¢„ç®—è€Œä¸‹é™ã€‚é€šè¿‡ä¸æ ‡å‡†å¤§è¯­è¨€æ¨¡å‹ (LLMs) å¯¹æ¯”ï¼Œç ”ç©¶åˆ’åˆ†äº†ä¸‰ç§æ€§èƒ½åŒºé—´ï¼ŒæŒ‡å‡º LRMs ä»…åœ¨ä¸­ç­‰å¤æ‚åº¦ä»»åŠ¡ä¸­è¡¨ç°å‡ºæ˜¾è‘—ä¼˜åŠ¿ã€‚æ­¤å¤–ï¼ŒLRMs åœ¨ç²¾ç¡®è®¡ç®—å’Œæ˜¾å¼ç®—æ³• (Explicit Algorithms) çš„åº”ç”¨ä¸Šå­˜åœ¨æ˜æ˜¾çŸ­æ¿ï¼Œä¸”è·¨å°ºåº¦æ¨ç†è¡¨ç°ä¸ä¸€ã€‚è¯¥å·¥ä½œé€šè¿‡å¯¹æ¨ç†è¿¹çº¿ (Reasoning Traces) çš„æ·±å…¥æŒ–æ˜ï¼Œæ­ç¤ºäº†å½“å‰æ¨ç†æ¨¡å‹åœ¨è®¡ç®—è¡Œä¸ºä¸Šçš„å®è´¨æ€§å¼±ç‚¹ï¼Œä¸ºç†è§£è¿™ç±»æ¨¡å‹çš„çœŸå®æ¨ç†èƒ½åŠ›æä¾›äº†æ‰¹åˆ¤æ€§è§†è§’ã€‚",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "NeurIPS 2025. camera-ready version + additional discussion in the appendix",
      "pdf_url": "https://arxiv.org/pdf/2506.06941v3",
      "published_date": "2025-06-07 22:42:29 UTC",
      "updated_date": "2025-11-20 00:19:24 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T19:41:21.790653+00:00"
    },
    {
      "arxiv_id": "2506.06935v2",
      "title": "An Agentic Framework for Autonomous Metamaterial Modeling and Inverse Design",
      "title_zh": "ä¸€ç§ç”¨äºè‡ªä¸»è¶…ææ–™å»ºæ¨¡ä¸é€†å‘è®¾è®¡çš„æ™ºèƒ½ä½“æ¡†æ¶",
      "authors": [
        "Darui Lu",
        "Jordan M. Malof",
        "Willie J. Padilla"
      ],
      "abstract": "Recent significant advances in integrating multiple Large Language Model (LLM) systems have enabled Agentic Frameworks capable of performing complex tasks autonomously, including novel scientific research. We develop and demonstrate such a framework specifically for the inverse design of photonic metamaterials. When queried with a desired optical spectrum, the Agent autonomously proposes and develops a forward deep learning model, accesses external tools via APIs for tasks like simulation and optimization, utilizes memory, and generates a final design via a deep inverse method. The framework's effectiveness is demonstrated in its ability to automate, reason, plan, and adapt. Notably, the Agentic Framework possesses internal reflection and decision flexibility, permitting highly varied and potentially novel outputs.",
      "tldr_zh": "è¯¥ç ”ç©¶å¼€å‘å¹¶å±•ç¤ºäº†ä¸€ç§ç”¨äºå…‰å­è¶…ææ–™(photonic metamaterials)é€†å‘è®¾è®¡çš„è‡ªä¸»æ™ºèƒ½ä½“æ¡†æ¶(Agentic Framework)ï¼Œæ—¨åœ¨åˆ©ç”¨å¤§è¯­è¨€æ¨¡å‹(LLM)é›†æˆç³»ç»Ÿè‡ªä¸»æ‰§è¡Œå¤æ‚çš„ç§‘ç ”ä»»åŠ¡ã€‚åœ¨æ¥æ”¶åˆ°ç›®æ ‡å…‰è°±æŸ¥è¯¢åï¼Œæ™ºèƒ½ä½“èƒ½å¤Ÿè‡ªä¸»æè®®å¹¶æ„å»ºå‰å‘æ·±åº¦å­¦ä¹ æ¨¡å‹ï¼Œé€šè¿‡APIè°ƒç”¨å¤–éƒ¨å·¥å…·è¿›è¡Œä»¿çœŸä¸ä¼˜åŒ–ï¼Œå¹¶ç»“åˆè®°å¿†æœºåˆ¶åˆ©ç”¨æ·±åº¦é€†å‘æ–¹æ³•(deep inverse method)ç”Ÿæˆæœ€ç»ˆè®¾è®¡ã€‚è¯¥æ¡†æ¶å±•ç°äº†å“è¶Šçš„æ¨ç†ã€è§„åˆ’å’Œè‡ªé€‚åº”èƒ½åŠ›ï¼Œå…¶å†…éƒ¨åæ€æœºåˆ¶å’Œå†³ç­–çµæ´»æ€§å…è®¸ç”Ÿæˆé«˜åº¦å¤šæ ·åŒ–ä¸”å…·æœ‰æ½œåœ¨åˆ›æ–°æ€§çš„è¾“å‡ºã€‚å®éªŒè¯æ˜ï¼Œè¯¥è‡ªåŠ¨åŒ–æµç¨‹èƒ½å¤Ÿæ˜¾è‘—æå‡è¶…ææ–™å»ºæ¨¡ä¸è®¾è®¡çš„æ•ˆç‡ï¼Œä¸ºè‡ªä¸»ç§‘å­¦ç ”ç©¶æä¾›äº†æ–°çš„èŒƒå¼ã€‚",
      "categories": [
        "cs.AI",
        "cond-mat.mtrl-sci"
      ],
      "primary_category": "cs.AI",
      "comment": "22 pages, 6 figures",
      "pdf_url": "https://arxiv.org/pdf/2506.06935v2",
      "published_date": "2025-06-07 22:10:05 UTC",
      "updated_date": "2025-07-15 11:01:25 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T19:41:39.491325+00:00"
    },
    {
      "arxiv_id": "2506.06933v1",
      "title": "Rewriting the Budget: A General Framework for Black-Box Attacks Under Cost Asymmetry",
      "title_zh": "é‡å†™é¢„ç®—ï¼šéå¯¹ç§°æˆæœ¬ä¸‹çš„é»‘ç›’æ”»å‡»é€šç”¨æ¡†æ¶",
      "authors": [
        "Mahdi Salmani",
        "Alireza Abdollahpoorrostam",
        "Seyed-Mohsen Moosavi-Dezfooli"
      ],
      "abstract": "Traditional decision-based black-box adversarial attacks on image classifiers aim to generate adversarial examples by slightly modifying input images while keeping the number of queries low, where each query involves sending an input to the model and observing its output. Most existing methods assume that all queries have equal cost. However, in practice, queries may incur asymmetric costs; for example, in content moderation systems, certain output classes may trigger additional review, enforcement, or penalties, making them more costly than others. While prior work has considered such asymmetric cost settings, effective algorithms for this scenario remain underdeveloped. In this paper, we propose a general framework for decision-based attacks under asymmetric query costs, which we refer to as asymmetric black-box attacks. We modify two core components of existing attacks: the search strategy and the gradient estimation process. Specifically, we propose Asymmetric Search (AS), a more conservative variant of binary search that reduces reliance on high-cost queries, and Asymmetric Gradient Estimation (AGREST), which shifts the sampling distribution to favor low-cost queries. We design efficient algorithms that minimize total attack cost by balancing different query types, in contrast to earlier methods such as stealthy attacks that focus only on limiting expensive (high-cost) queries. Our method can be integrated into a range of existing black-box attacks with minimal changes. We perform both theoretical analysis and empirical evaluation on standard image classification benchmarks. Across various cost regimes, our method consistently achieves lower total query cost and smaller perturbations than existing approaches, with improvements of up to 40% in some settings.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ä¼ ç»Ÿå†³ç­–é»‘ç›’æ”»å‡»(Decision-based black-box attacks)ä¸­å¿½ç•¥æŸ¥è¯¢æˆæœ¬å·®å¼‚çš„é—®é¢˜ï¼Œæå‡ºäº†ä¸€ä¸ªå¤„ç†æˆæœ¬ä¸å¯¹ç§°(Asymmetric cost)çš„é€šç”¨æ”»å‡»æ¡†æ¶ã€‚åœ¨å®é™…çš„å†…å®¹å®¡æ ¸ç­‰åœºæ™¯ä¸­ï¼Œç‰¹å®šè¾“å‡ºç±»åˆ«å¯èƒ½è§¦å‘é¢å¤–å®¡æŸ¥ï¼Œå¯¼è‡´æŸ¥è¯¢æˆæœ¬ä¸ä¸€ï¼Œè€Œç°æœ‰ç®—æ³•å¯¹æ­¤åœºæ™¯çš„ä¼˜åŒ–ä»ä¸å……åˆ†ã€‚ä½œè€…æ”¹è¿›äº†æœç´¢ç­–ç•¥å’Œæ¢¯åº¦ä¼°è®¡ä¸¤ä¸ªæ ¸å¿ƒç»„ä»¶ï¼Œåˆ†åˆ«å¼•å…¥äº†å‡å°‘é«˜æˆæœ¬æŸ¥è¯¢ä¾èµ–çš„éå¯¹ç§°æœç´¢(Asymmetric Search, AS)å’Œå€¾å‘ä½æˆæœ¬æŸ¥è¯¢é‡‡æ ·çš„éå¯¹ç§°æ¢¯åº¦ä¼°è®¡(Asymmetric Gradient Estimation, AGREST)ã€‚è¯¥æ¡†æ¶é€šè¿‡å¹³è¡¡ä¸åŒç±»å‹çš„æŸ¥è¯¢æ¥æœ€å°åŒ–æ€»æ”»å‡»æˆæœ¬ï¼Œå¹¶èƒ½ä»¥æå°çš„ä¿®æ”¹é›†æˆåˆ°å¤šç§ç°æœ‰é»‘ç›’æ”»å‡»æ–¹æ³•ä¸­ã€‚ç†è®ºåˆ†æå’Œåœ¨å›¾åƒåˆ†ç±»åŸºå‡†ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å„ç§æˆæœ¬è®¾å®šä¸‹å‡ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œåœ¨æŸäº›è®¾ç½®ä¸‹å¯å°†æ€»æŸ¥è¯¢æˆæœ¬å’Œæ‰°åŠ¨é™ä½å¤šè¾¾40%ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CR",
        "cs.CV"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.06933v1",
      "published_date": "2025-06-07 22:02:27 UTC",
      "updated_date": "2025-06-07 22:02:27 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T19:41:27.789660+00:00"
    },
    {
      "arxiv_id": "2506.06930v1",
      "title": "DiscoSum: Discourse-aware News Summarization",
      "title_zh": "DiscoSumï¼šç¯‡ç« æ„ŸçŸ¥çš„æ–°é—»æ‘˜è¦",
      "authors": [
        "Alexander Spangher",
        "Tenghao Huang",
        "Jialiang Gu",
        "Jiatong Shi",
        "Muhao Chen"
      ],
      "abstract": "Recent advances in text summarization have predominantly leveraged large language models to generate concise summaries. However, language models often do not maintain long-term discourse structure, especially in news articles, where organizational flow significantly influences reader engagement. We introduce a novel approach to integrating discourse structure into summarization processes, focusing specifically on news articles across various media. We present a novel summarization dataset where news articles are summarized multiple times in different ways across different social media platforms (e.g. LinkedIn, Facebook, etc.). We develop a novel news discourse schema to describe summarization structures and a novel algorithm, DiscoSum, which employs beam search technique for structure-aware summarization, enabling the transformation of news stories to meet different stylistic and structural demands. Both human and automatic evaluation results demonstrate the efficacy of our approach in maintaining narrative fidelity and meeting structural requirements.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤§è¯­è¨€æ¨¡å‹(Large Language Models)åœ¨æ–°é—»æ‘˜è¦ä¸­éš¾ä»¥ç»´æŒé•¿æœŸè¯­ç¯‡ç»“æ„(Discourse structure)çš„é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§é›†æˆè¯­ç¯‡ç»“æ„çš„æ–°å‹æ‘˜è¦æ–¹æ³•ã€‚ä½œè€…æä¾›äº†ä¸€ä¸ªåŒ…å«å¤šç¤¾äº¤åª’ä½“å¹³å°ï¼ˆå¦‚ LinkedInã€Facebook ç­‰ï¼‰å¤šæ ·åŒ–æ‘˜è¦çš„æ–°å‹æ–°é—»æ•°æ®é›†ï¼Œå¹¶å¼€å‘äº†ä¸“é—¨çš„æ–°é—»è¯­ç¯‡æ¨¡å¼(News discourse schema)æ¥æè¿°æ‘˜è¦ç»“æ„ã€‚ç ”ç©¶è¿›ä¸€æ­¥æå‡ºäº† DiscoSum ç®—æ³•ï¼Œé€šè¿‡æŸæœç´¢(Beam search)æŠ€æœ¯å®ç°ç»“æ„æ„ŸçŸ¥æ‘˜è¦ï¼Œä½¿å¾—æ–°é—»å†…å®¹èƒ½å¤Ÿæ ¹æ®ä¸åŒçš„é£æ ¼å’Œç»“æ„éœ€æ±‚è¿›è¡Œè½¬åŒ–ã€‚å®éªŒä¸­çš„äººå·¥å’Œè‡ªåŠ¨è¯„ä¼°ç»“æœå‡è¯æ˜ï¼Œè¯¥æ–¹æ³•åœ¨ä¿æŒå™è¿°å¿ å®åº¦(Narrative fidelity)å’Œæ»¡è¶³ç»“æ„è¦æ±‚æ–¹é¢è¡¨ç°ä¼˜å¼‚ï¼Œæœ‰æ•ˆæå‡äº†æ–°é—»æ‘˜è¦åœ¨ä¸åŒåª’ä»‹ä¸Šçš„é€‚ç”¨æ€§ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "8 pages, 3 figures, 10 pages in Appendix",
      "pdf_url": "https://arxiv.org/pdf/2506.06930v1",
      "published_date": "2025-06-07 22:00:30 UTC",
      "updated_date": "2025-06-07 22:00:30 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T19:41:52.692041+00:00"
    },
    {
      "arxiv_id": "2506.06923v1",
      "title": "Boosting LLM Reasoning via Spontaneous Self-Correction",
      "title_zh": "é€šè¿‡è‡ªå‘å¼è‡ªæˆ‘çº é”™æå‡å¤§è¯­è¨€æ¨¡å‹æ¨ç†èƒ½åŠ›",
      "authors": [
        "Xutong Zhao",
        "Tengyu Xu",
        "Xuewei Wang",
        "Zhengxing Chen",
        "Di Jin",
        "Liang Tan",
        "Yen-Ting",
        "Zishun Yu",
        "Zhuokai Zhao",
        "Yun He",
        "Sinong Wang",
        "Han Fang",
        "Sarath Chandar",
        "Chen Zhu"
      ],
      "abstract": "While large language models (LLMs) have demonstrated remarkable success on a broad range of tasks, math reasoning remains a challenging one. One of the approaches for improving math reasoning is self-correction, which designs self-improving loops to let the model correct its own mistakes. However, existing self-correction approaches treat corrections as standalone post-generation refinements, relying on extra prompt and system designs to elicit self-corrections, instead of performing real-time, spontaneous self-corrections in a single pass. To address this, we propose SPOC, a spontaneous self-correction approach that enables LLMs to generate interleaved solutions and verifications in a single inference pass, with generation dynamically terminated based on verification outcomes, thereby effectively scaling inference time compute. SPOC considers a multi-agent perspective by assigning dual roles -- solution proposer and verifier -- to the same model. We adopt a simple yet effective approach to generate synthetic data for fine-tuning, enabling the model to develop capabilities for self-verification and multi-agent collaboration. We further improve its solution proposal and verification accuracy through online reinforcement learning. Experiments on mathematical reasoning benchmarks show that SPOC significantly improves performance. Notably, SPOC boosts the accuracy of Llama-3.1-8B and 70B Instruct models, achieving gains of 8.8% and 11.6% on MATH500, 10.0% and 20.0% on AMC23, and 3.3% and 6.7% on AIME24, respectively.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤§è¯­è¨€æ¨¡å‹ (LLMs) åœ¨æ•°å­¦æ¨ç†ä¸­çš„å±€é™æ€§ï¼Œæå‡ºäº† SPOC (Spontaneous self-Correction) æ–¹æ³•ï¼Œæ—¨åœ¨è§£å†³ç°æœ‰è‡ªæˆ‘ä¿®æ­£æ–¹æ³•ä¾èµ–é¢å¤–æç¤ºä¸”æ— æ³•å®æ—¶è‡ªå‘ä¿®æ­£çš„é—®é¢˜ã€‚SPOC å…è®¸æ¨¡å‹åœ¨å•æ¬¡æ¨ç†è¿‡ç¨‹ä¸­äº¤æ›¿ç”Ÿæˆè§£å†³æ–¹æ¡ˆå’ŒéªŒè¯ (interleaved solutions and verifications)ï¼Œå¹¶æ ¹æ®éªŒè¯ç»“æœåŠ¨æ€ç»ˆæ­¢ç”Ÿæˆï¼Œä»è€Œæœ‰æ•ˆæ‰©å±•äº†æ¨ç†æ—¶é—´è®¡ç®— (inference time compute)ã€‚è¯¥æ–¹æ³•é‡‡ç”¨å¤šæ™ºèƒ½ä½“è§†è§’ï¼Œä¸ºåŒä¸€æ¨¡å‹åˆ†é…è§£å†³æ–¹æ¡ˆææ¡ˆè€… (solution proposer) å’ŒéªŒè¯è€… (verifier) çš„åŒé‡è§’è‰²ã€‚ç ”ç©¶åˆ©ç”¨åˆæˆæ•°æ®è¿›è¡Œå¾®è°ƒï¼Œå¹¶é…åˆåœ¨çº¿å¼ºåŒ–å­¦ä¹  (online Reinforcement Learning) å¢å¼ºå…¶è‡ªéªŒè¯ä¸åä½œèƒ½åŠ›ã€‚å®éªŒè¡¨æ˜ï¼ŒSPOC åœ¨å¤šä¸ªæ•°å­¦æ¨ç†åŸºå‡†æµ‹è¯•ä¸­å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä½¿ Llama-3.1-8B å’Œ 70B Instruct æ¨¡å‹åœ¨ MATH500 ä¸Šçš„å‡†ç¡®ç‡åˆ†åˆ«æå‡äº† 8.8% å’Œ 11.6%ã€‚è¿™é¡¹å·¥ä½œè¯æ˜äº†è‡ªå‘æ€§è‡ªæˆ‘ä¿®æ­£åœ¨æå‡å¤æ‚é€»è¾‘æ¨ç†ä»»åŠ¡ä¸­çš„é«˜æ•ˆæ€§ä¸å·¨å¤§æ½œåŠ›ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.06923v1",
      "published_date": "2025-06-07 21:23:00 UTC",
      "updated_date": "2025-06-07 21:23:00 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T19:42:03.187387+00:00"
    },
    {
      "arxiv_id": "2506.06917v1",
      "title": "Graph-Based Physics-Guided Urban PM2.5 Air Quality Imputation with Constrained Monitoring Data",
      "title_zh": "åŸºäºå›¾ä¸ç‰©ç†å¼•å¯¼çš„å—é™ç›‘æµ‹æ•°æ®ä¸‹åŸå¸‚ PM2.5 ç©ºæ°”è´¨é‡æ’è¡¥",
      "authors": [
        "Shangjie Du",
        "Hui Wei",
        "Dong Yoon Lee",
        "Zhizhang Hu",
        "Shijia Pan"
      ],
      "abstract": "This work introduces GraPhy, a graph-based, physics-guided learning framework for high-resolution and accurate air quality modeling in urban areas with limited monitoring data. Fine-grained air quality monitoring information is essential for reducing public exposure to pollutants. However, monitoring networks are often sparse in socioeconomically disadvantaged regions, limiting the accuracy and resolution of air quality modeling. To address this, we propose a physics-guided graph neural network architecture called GraPhy with layers and edge features designed specifically for low-resolution monitoring data. Experiments using data from California's socioeconomically disadvantaged San Joaquin Valley show that GraPhy achieves the overall best performance evaluated by mean squared error (MSE), mean absolute error (MAE), and R-square value (R2), improving the performance by 9%-56% compared to various baseline models. Moreover, GraPhy consistently outperforms baselines across different spatial heterogeneity levels, demonstrating the effectiveness of our model design.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† GraPhyï¼Œä¸€ç§åŸºäºå›¾å’Œç‰©ç†å¼•å¯¼ (graph-based, physics-guided) çš„å­¦ä¹ æ¡†æ¶ï¼Œæ—¨åœ¨ç›‘æµ‹æ•°æ®æœ‰é™çš„åŸå¸‚åŒºåŸŸå®ç°é«˜åˆ†è¾¨ç‡ä¸”å‡†ç¡®çš„ç©ºæ°”è´¨é‡å»ºæ¨¡ã€‚é’ˆå¯¹ç¤¾ä¼šç»æµæ¬ å‘è¾¾åœ°åŒºç›‘æµ‹ç½‘ç»œç¨€ç–å¯¼è‡´çš„å»ºæ¨¡ç²¾åº¦å—é™é—®é¢˜ï¼ŒGraPhy é‡‡ç”¨äº†ä¸“é—¨ä¸ºä½åˆ†è¾¨ç‡æ•°æ®è®¾è®¡çš„ç‰©ç†å¼•å¯¼å›¾ç¥ç»ç½‘ç»œ (Graph Neural Network) æ¶æ„ä¸è¾¹ç¼˜ç‰¹å¾ã€‚åœ¨åŠ å·åœ£åé‡‘æ²³è°· (San Joaquin Valley) æ•°æ®çš„å®éªŒä¸­ï¼ŒGraPhy åœ¨å‡æ–¹è¯¯å·® (MSE)ã€å¹³å‡ç»å¯¹è¯¯å·® (MAE) å’Œ R2 å€¼ç­‰æŒ‡æ ‡ä¸Šå‡å–å¾—æœ€ä½³è¡¨ç°ï¼Œç›¸æ¯”å„ç±»åŸºçº¿æ¨¡å‹æ€§èƒ½æå‡äº† 9%-56%ã€‚æ­¤å¤–ï¼Œè¯¥æ¨¡å‹åœ¨ä¸åŒç©ºé—´å¼‚è´¨æ€§ (spatial heterogeneity) æ°´å¹³ä¸‹å‡ä¿æŒä¼˜å¼‚æ€§èƒ½ï¼Œæœ‰åŠ›è¯æ˜äº†å…¶åœ¨è§£å†³ç©ºæ°”è´¨é‡æ•°æ®ç¼ºå¤±é—®é¢˜ä¸Šçš„æœ‰æ•ˆæ€§ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "Accepted by ACM Transactions on Sensor Networks (TOSN) 2025",
      "pdf_url": "https://arxiv.org/pdf/2506.06917v1",
      "published_date": "2025-06-07 20:33:52 UTC",
      "updated_date": "2025-06-07 20:33:52 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T19:41:44.791793+00:00"
    },
    {
      "arxiv_id": "2506.06910v1",
      "title": "Causal Graph based Event Reasoning using Semantic Relation Experts",
      "title_zh": "åŸºäºè¯­ä¹‰å…³ç³»ä¸“å®¶çš„å› æœå›¾äº‹ä»¶æ¨ç†",
      "authors": [
        "Mahnaz Koupaee",
        "Xueying Bai",
        "Mudan Chen",
        "Greg Durrett",
        "Nathanael Chambers",
        "Niranjan Balasubramanian"
      ],
      "abstract": "Understanding how events in a scenario causally connect with each other is important for effectively modeling and reasoning about events. But event reasoning remains a difficult challenge, and despite recent advances, Large Language Models (LLMs) still struggle to accurately identify causal connections between events. This struggle leads to poor performance on deeper reasoning tasks like event forecasting and timeline understanding. To address this challenge, we investigate the generation of causal event graphs (e.g., A enables B) as a parallel mechanism to help LLMs explicitly represent causality during inference. This paper evaluates both how to generate correct graphs as well as how graphs can assist reasoning. We propose a collaborative approach to causal graph generation where we use LLMs to simulate experts that focus on specific semantic relations. The experts engage in multiple rounds of discussions which are then consolidated by a final expert. Then, to demonstrate the utility of causal graphs, we use them on multiple downstream applications, and also introduce a new explainable event prediction task that requires a causal chain of events in the explanation. These explanations are more informative and coherent than baseline generations. Finally, our overall approach not finetuned on any downstream task, achieves competitive results with state-of-the-art models on both forecasting and next event prediction tasks.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ Large Language Models (LLMs) åœ¨è¯†åˆ«äº‹ä»¶å› æœè”ç³»åŠå¤„ç†æ·±åº¦æ¨ç†ä»»åŠ¡ï¼ˆå¦‚äº‹ä»¶é¢„æµ‹å’Œæ—¶é—´çº¿ç†è§£ï¼‰æ—¶çš„å±€é™æ€§ï¼Œæå‡ºäº†ä¸€ç§åŸºäº Causal Event Graphs çš„åä½œç”Ÿæˆæ¡†æ¶ã€‚è¯¥æ–¹æ³•é€šè¿‡è®© LLMs æ¨¡æ‹Ÿä¸“æ³¨äºç‰¹å®šè¯­ä¹‰å…³ç³»çš„ä¸“å®¶è¿›è¡Œå¤šè½®è®¨è®ºï¼Œå¹¶ç”±æœ€ç»ˆä¸“å®¶æ±‡æ€»ç”Ÿæˆæ˜¾å¼çš„å› æœè¡¨ç¤ºã€‚ç ”ç©¶è¿˜å¼•å…¥äº†ä¸€é¡¹è¦æ±‚æä¾›å› æœé“¾çš„æ–°å‹å¯è§£é‡Šäº‹ä»¶é¢„æµ‹ä»»åŠ¡ï¼Œæ˜¾è‘—æå‡äº†ç”Ÿæˆè§£é‡Šçš„ä¿¡æ¯é‡ä¸è¿è´¯æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨æ— éœ€å¯¹ä¸‹æ¸¸ä»»åŠ¡è¿›è¡Œå¾®è°ƒçš„æƒ…å†µä¸‹ï¼Œåœ¨ Event Forecasting å’Œ Next Event Prediction ä»»åŠ¡ä¸­å–å¾—äº†ä¸å½“å‰æœ€å…ˆè¿›æ¨¡å‹ç›¸å½“çš„ç«äº‰æ€§è¡¨ç°ï¼Œè¯æ˜äº†æ˜¾å¼å› æœå›¾åœ¨å¢å¼ºæ¨¡å‹æ¨ç†èƒ½åŠ›æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.06910v1",
      "published_date": "2025-06-07 20:15:45 UTC",
      "updated_date": "2025-06-07 20:15:45 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T19:41:42.785482+00:00"
    },
    {
      "arxiv_id": "2506.06907v2",
      "title": "Uncertainty Estimation on Graphs with Structure Informed Stochastic Partial Differential Equations",
      "title_zh": "åŸºäºç»“æ„æ„ŸçŸ¥éšæœºåå¾®åˆ†æ–¹ç¨‹çš„å›¾ä¸ç¡®å®šæ€§ä¼°è®¡",
      "authors": [
        "Fred Xu",
        "Thomas Markovich"
      ],
      "abstract": "Graph Neural Networks have achieved impressive results across diverse network modeling tasks, but accurately estimating uncertainty on graphs remains difficult, especially under distributional shifts. Unlike traditional uncertainty estimation, graph-based uncertainty must account for randomness arising from both the graph's structure and its label distribution, which adds complexity. In this paper, making an analogy between the evolution of a stochastic partial differential equation (SPDE) driven by Matern Gaussian Process and message passing using GNN layers, we present a principled way to design a novel message passing scheme that incorporates spatial-temporal noises motivated by the Gaussian Process approach to SPDE. Our method simultaneously captures uncertainty across space and time and allows explicit control over the covariance kernel smoothness, thereby enhancing uncertainty estimates on graphs with both low and high label informativeness. Our extensive experiments on Out-of-Distribution (OOD) detection on graph datasets with varying label informativeness demonstrate the soundness and superiority of our model to existing approaches.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†å›¾ç¥ç»ç½‘ç»œ(Graph Neural Networks)åœ¨é¢ä¸´åˆ†å¸ƒåç§»(Distributional Shifts)æ—¶ï¼Œéš¾ä»¥å‡†ç¡®è¯„ä¼°ç”±å›¾ç»“æ„ä¸æ ‡ç­¾åˆ†å¸ƒéšæœºæ€§å¼•èµ·çš„ä¸ç¡®å®šæ€§ä¼°è®¡(Uncertainty Estimation)é—®é¢˜ã€‚ä½œè€…é€šè¿‡ç±»æ¯”ç”±MatÃ©rn Gaussian Processé©±åŠ¨çš„éšæœºåå¾®åˆ†æ–¹ç¨‹(SPDE)æ¼”åŒ–è¿‡ç¨‹ä¸GNNçš„æ¶ˆæ¯ä¼ é€’æœºåˆ¶ï¼Œæå‡ºäº†ä¸€ç§èåˆç©ºé—´-æ—¶é—´å™ªå£°(Spatial-Temporal Noises)çš„æ–°å‹æ¶ˆæ¯ä¼ é€’æ–¹æ¡ˆã€‚è¯¥æ–¹æ³•èƒ½å¤ŸåŒæ—¶æ•æ‰æ—¶ç©ºç»´åº¦çš„ä¸ç¡®å®šæ€§ï¼Œå¹¶å…è®¸å¯¹åæ–¹å·®æ ¸(Covariance Kernel)çš„å¹³æ»‘åº¦è¿›è¡Œæ˜¾å¼æ§åˆ¶ï¼Œä»è€Œæœ‰æ•ˆå¢å¼ºäº†æ¨¡å‹åœ¨ä¸åŒæ ‡ç­¾ä¿¡æ¯é‡ä¸‹çš„ä¼°è®¡èƒ½åŠ›ã€‚åœ¨å¤šç§å›¾æ•°æ®é›†ä¸Šçš„åˆ†å¸ƒå¤–(OOD)æ£€æµ‹å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ¨¡å‹åœ¨æ€§èƒ½ä¸Šæ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ï¼ŒéªŒè¯äº†å…¶åœ¨å¤„ç†å¤æ‚ä¸ç¡®å®šæ€§ä»»åŠ¡ä¸­çš„ä¼˜è¶Šæ€§ã€‚è¯¥ç ”ç©¶ä¸ºåŸºäºç‰©ç†é©±åŠ¨æ–¹ç¨‹çš„å›¾å­¦ä¹ ä¸ç¡®å®šæ€§é‡åŒ–æä¾›äº†å…¨æ–°çš„ç†è®ºè§†è§’ä¸å®è·µæŒ‡å—ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.06907v2",
      "published_date": "2025-06-07 19:58:38 UTC",
      "updated_date": "2025-10-14 00:34:48 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T19:42:03.001115+00:00"
    },
    {
      "arxiv_id": "2506.06905v2",
      "title": "Meta-Adaptive Prompt Distillation for Few-Shot Visual Question Answering",
      "title_zh": "é¢å‘å°‘æ ·æœ¬è§†è§‰é—®ç­”çš„å…ƒè‡ªé€‚åº”æç¤ºè’¸é¦",
      "authors": [
        "Akash Gupta",
        "Amos Storkey",
        "Mirella Lapata"
      ],
      "abstract": "Large Multimodal Models (LMMs) often rely on in-context learning (ICL) to perform new tasks with minimal supervision. However, ICL performance, especially in smaller LMMs, is inconsistent and does not always improve monotonically with increasing examples. We hypothesize that this occurs due to the LMM being overwhelmed by additional information present in the image embeddings, which is not required for the downstream task. To address this, we propose a meta-learning approach that provides an alternative for inducing few-shot capabilities in LMMs, using a fixed set of soft prompts that are distilled from task-relevant image features and can be adapted at test time using a few examples. To facilitate this distillation, we introduce an attention-mapper module that can be easily integrated with the popular LLaVA v1.5 architecture and is jointly learned with soft prompts, enabling task adaptation in LMMs under low-data regimes with just a few gradient steps. Evaluation on the VL-ICL Bench shows that our method consistently outperforms ICL and related prompt-tuning approaches, even under image perturbations, improving task induction and reasoning across visual question answering tasks.",
      "tldr_zh": "é’ˆå¯¹å¤§å‹å¤šæ¨¡æ€æ¨¡å‹(LMMs)åœ¨å°‘é‡æ ·æœ¬è§†è§‰é—®ç­”(Few-Shot VQA)ä»»åŠ¡ä¸­ï¼Œä¸Šä¸‹æ–‡å­¦ä¹ (ICL)è¡¨ç°ä¸ç¨³å®šä¸”æ˜“å—å†—ä½™å›¾åƒåµŒå…¥å¹²æ‰°çš„é—®é¢˜ï¼Œè¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§å…ƒå­¦ä¹ æ–¹æ³•ã€‚è¯¥æ–¹æ¡ˆé€šè¿‡å…ƒè‡ªé€‚åº”æç¤ºè’¸é¦(Meta-Adaptive Prompt Distillation)æŠ€æœ¯ï¼Œåˆ©ç”¨å›ºå®šçš„è½¯æç¤º(soft prompts)ä»ä»»åŠ¡ç›¸å…³å›¾åƒç‰¹å¾ä¸­æå–æ ¸å¿ƒçŸ¥è¯†ï¼Œå¹¶å…è®¸åœ¨æµ‹è¯•é˜¶æ®µæ ¹æ®å°‘é‡ç¤ºä¾‹è¿›è¡Œå¿«é€Ÿé€‚é…ã€‚ç ”ç©¶å¼•å…¥äº†ä¸€ä¸ªä¸“é—¨çš„æ³¨æ„åŠ›æ˜ å°„æ¨¡å—(attention-mapper module)ï¼Œè¯¥æ¨¡å—èƒ½ä¸LLaVA v1.5æ¶æ„é›†æˆå¹¶ä¸è½¯æç¤ºå…±åŒå­¦ä¹ ï¼Œä»è€Œåœ¨ä½æ•°æ®é‡ç¯å¢ƒä¸‹é€šè¿‡æå°‘çš„æ¢¯åº¦æ­¥éª¤å®ç°ä»»åŠ¡è¿ç§»ã€‚åœ¨VL-ICL Benchä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨ä»»åŠ¡è¯±å¯¼å’Œé€»è¾‘æ¨ç†æ–¹é¢æ˜¾è‘—ä¼˜äºä¼ ç»Ÿçš„ICLåŠæç¤ºå¾®è°ƒ(prompt-tuning)æ–¹æ³•ã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•åœ¨é¢å¯¹å›¾åƒæ‰°åŠ¨æ—¶ä¾ç„¶èƒ½ä¿æŒå“è¶Šçš„æ€§èƒ½ç¨³å®šæ€§ï¼Œä¸ºæå‡å°å‹LMMsçš„å°‘æ ·æœ¬å­¦ä¹ èƒ½åŠ›æä¾›äº†æœ‰æ•ˆè·¯å¾„ã€‚",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.CV",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.06905v2",
      "published_date": "2025-06-07 19:37:22 UTC",
      "updated_date": "2025-06-10 07:34:44 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T19:42:31.367224+00:00"
    },
    {
      "arxiv_id": "2506.06904v1",
      "title": "Can Biologically Plausible Temporal Credit Assignment Rules Match BPTT for Neural Similarity? E-prop as an Example",
      "title_zh": "ç”Ÿç‰©å­¦åˆç†çš„æ—¶é—´ä¿¡ç”¨åˆ†é…è§„åˆ™èƒ½å¦åœ¨ç¥ç»ç›¸ä¼¼æ€§ä¸Šåª²ç¾ BPTTï¼Ÿä»¥ E-prop ä¸ºä¾‹",
      "authors": [
        "Yuhan Helena Liu",
        "Guangyu Robert Yang",
        "Christopher J. Cueva"
      ],
      "abstract": "Understanding how the brain learns may be informed by studying biologically plausible learning rules. These rules, often approximating gradient descent learning to respect biological constraints such as locality, must meet two critical criteria to be considered an appropriate brain model: (1) good neuroscience task performance and (2) alignment with neural recordings. While extensive research has assessed the first criterion, the second remains underexamined. Employing methods such as Procrustes analysis on well-known neuroscience datasets, this study demonstrates the existence of a biologically plausible learning rule -- namely e-prop, which is based on gradient truncation and has demonstrated versatility across a wide range of tasks -- that can achieve neural data similarity comparable to Backpropagation Through Time (BPTT) when matched for task accuracy. Our findings also reveal that model architecture and initial conditions can play a more significant role in determining neural similarity than the specific learning rule. Furthermore, we observe that BPTT-trained models and their biologically plausible counterparts exhibit similar dynamical properties at comparable accuracies. These results underscore the substantial progress made in developing biologically plausible learning rules, highlighting their potential to achieve both competitive task performance and neural data similarity.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†ç”Ÿç‰©å­¦åˆç†(Biologically Plausible)çš„æ—¶é—´ä¿¡ç”¨åˆ†é…è§„åˆ™èƒ½å¦åœ¨ç¥ç»ç›¸ä¼¼æ€§(Neural Similarity)æ–¹é¢åª²ç¾éšæ—¶é—´åå‘ä¼ æ’­(BPTT)ç®—æ³•ã€‚ä»¥åŸºäºæ¢¯åº¦æˆªæ–­çš„e-propç®—æ³•ä¸ºä¾‹ï¼Œç ”ç©¶è€…åˆ©ç”¨Procrustesåˆ†æç­‰æ–¹æ³•åœ¨çŸ¥åç¥ç»ç§‘å­¦æ•°æ®é›†ä¸ŠéªŒè¯äº†å­¦ä¹ è§„åˆ™ä¸ç¥ç»æ´»åŠ¨è®°å½•çš„ä¸€è‡´æ€§ã€‚å®éªŒè¯æ˜ï¼Œåœ¨ä»»åŠ¡å‡†ç¡®ç‡ç›¸åŒ¹é…çš„æƒ…å†µä¸‹ï¼Œe-propèƒ½å¤Ÿå®ç°ä¸BPTTæ¨¡å‹ç›¸å½“çš„ç¥ç»æ•°æ®ç›¸ä¼¼åº¦ã€‚ç ”ç©¶è¿›ä¸€æ­¥å‘ç°ï¼Œæ¨¡å‹æ¶æ„å’Œåˆå§‹æ¡ä»¶å¯¹ç¥ç»ç›¸ä¼¼æ€§çš„å½±å“ç¨‹åº¦å®é™…ä¸Šè¶…è¿‡äº†å…·ä½“çš„å­¦ä¹ è§„åˆ™ã€‚æ­¤å¤–ï¼ŒBPTTæ¨¡å‹ä¸å…¶ç”Ÿç‰©å­¦åˆç†çš„å¯¹åº”æ¨¡å‹åœ¨ç›¸ä¼¼çš„å‡†ç¡®ç‡æ°´å¹³ä¸‹å±•ç°å‡ºäº†ç›¸è¿‘çš„åŠ¨åŠ›å­¦ç‰¹æ€§(Dynamical Properties)ã€‚è¿™äº›ç»“æœè¡¨æ˜ç”Ÿç‰©å­¦åˆç†å­¦ä¹ è§„åˆ™å·²å–å¾—å®è´¨æ€§è¿›å±•ï¼Œå…·å¤‡åŒæ—¶å®ç°é«˜æ•ˆä»»åŠ¡æ€§èƒ½ä¸é«˜ç¥ç»ç›¸ä¼¼æ€§çš„æ½œåŠ›ã€‚",
      "categories": [
        "cs.NE",
        "cs.AI",
        "q-bio.NC"
      ],
      "primary_category": "cs.NE",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.06904v1",
      "published_date": "2025-06-07 19:32:15 UTC",
      "updated_date": "2025-06-07 19:32:15 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T19:42:28.652801+00:00"
    },
    {
      "arxiv_id": "2506.09065v1",
      "title": "Exploring Image Transforms derived from Eye Gaze Variables for Progressive Autism Diagnosis",
      "title_zh": "æ¢ç´¢åŸºäºçœ¼åŠ¨å˜é‡è¡ç”Ÿå›¾åƒå˜æ¢çš„æ¸è¿›å¼è‡ªé—­ç—‡è¯Šæ–­",
      "authors": [
        "Abigail Copiaco",
        "Christian Ritz",
        "Yassine Himeur",
        "Valsamma Eapen",
        "Ammar Albanna",
        "Wathiq Mansoor"
      ],
      "abstract": "The prevalence of Autism Spectrum Disorder (ASD) has surged rapidly over the past decade, posing significant challenges in communication, behavior, and focus for affected individuals. Current diagnostic techniques, though effective, are time-intensive, leading to high social and economic costs. This work introduces an AI-powered assistive technology designed to streamline ASD diagnosis and management, enhancing convenience for individuals with ASD and efficiency for caregivers and therapists. The system integrates transfer learning with image transforms derived from eye gaze variables to diagnose ASD. This facilitates and opens opportunities for in-home periodical diagnosis, reducing stress for individuals and caregivers, while also preserving user privacy through the use of image transforms. The accessibility of the proposed method also offers opportunities for improved communication between guardians and therapists, ensuring regular updates on progress and evolving support needs. Overall, the approach proposed in this work ensures timely, accessible diagnosis while protecting the subjects' privacy, improving outcomes for individuals with ASD.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹è‡ªé—­ç—‡è°±ç³»éšœç¢ï¼ˆASDï¼‰æ—¥ç›Šå¢é•¿çš„æ‚£ç—…ç‡ä»¥åŠä¼ ç»Ÿè¯Šæ–­æŠ€æœ¯è€—æ—¶è€—åŠ›çš„é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§åŸºäºäººå·¥æ™ºèƒ½çš„è¾…åŠ©è¯Šæ–­æŠ€æœ¯ã€‚è¯¥ç³»ç»Ÿåˆ›æ–°æ€§åœ°å°†è¿ç§»å­¦ä¹ ï¼ˆTransfer Learningï¼‰ä¸ä»çœ¼åŠ¨æ³¨è§†å˜é‡ï¼ˆEye Gaze Variablesï¼‰è¡ç”Ÿçš„å›¾åƒå˜æ¢ï¼ˆImage Transformsï¼‰ç›¸ç»“åˆï¼Œå®ç°äº†æ›´é«˜æ•ˆçš„è‡ªåŠ¨åŒ–è¯Šæ–­ã€‚è¿™ç§æ–¹æ³•ä¸ºå±…å®¶å®šæœŸè¯Šæ–­æä¾›äº†å¯èƒ½ï¼Œä¸ä»…æ˜¾è‘—å‡è½»äº†æ‚£è€…å’ŒæŠ¤ç†äººå‘˜çš„å‹åŠ›ï¼Œè¿˜é€šè¿‡å›¾åƒå˜æ¢æŠ€æœ¯æœ‰æ•ˆä¿æŠ¤äº†å—è¯•è€…çš„ä¸ªäººéšç§ã€‚æ­¤å¤–ï¼Œè¯¥æŠ€æœ¯å¢å¼ºäº†ç›‘æŠ¤äººä¸æ²»ç–—å¸ˆä¹‹é—´çš„æ²Ÿé€šæ•ˆç‡ï¼Œç¡®ä¿åº·å¤è¿›å±•å¾—åˆ°åŠæ—¶åé¦ˆã€‚æ€»ä½“è€Œè¨€ï¼Œè¯¥ç ”ç©¶åœ¨ä¿éšœéšç§çš„å‰æä¸‹ï¼Œä¸ºå®ç°åŠæ—¶ã€ä¾¿æ·ä¸”å¯åŠçš„ ASD è¯Šæ–­åŠç®¡ç†æä¾›äº†æœ‰æ•ˆçš„è§£å†³æ–¹æ¡ˆã€‚",
      "categories": [
        "eess.IV",
        "cs.AI",
        "cs.CV",
        "cs.HC",
        "cs.LG"
      ],
      "primary_category": "eess.IV",
      "comment": "6 pages, 8 figures, and 1 table",
      "pdf_url": "https://arxiv.org/pdf/2506.09065v1",
      "published_date": "2025-06-07 19:00:17 UTC",
      "updated_date": "2025-06-07 19:00:17 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T19:42:31.995392+00:00"
    },
    {
      "arxiv_id": "2506.17247v2",
      "title": "Recursive Learning-Based Virtual Buffering for Analytical Global Placement",
      "title_zh": "é¢å‘åˆ†æå¼å…¨å±€å¸ƒå›¾çš„é€’å½’å­¦ä¹ è™šæ‹Ÿç¼“å†²æŠ€æœ¯",
      "authors": [
        "Andrew B. Kahng",
        "Yiting Liu",
        "Zhiang Wang"
      ],
      "abstract": "Due to the skewed scaling of interconnect versus cell delay in modern technology nodes, placement with buffer porosity (i.e., cell density) awareness is essential for timing closure in physical synthesis flows. However, existing approaches face two key challenges: (i) traditional van Ginneken-Lillis-style buffering approaches are computationally expensive during global placement; and (ii) machine learning-based approaches, such as BufFormer, lack a thorough consideration of Electrical Rule Check (ERC) violations and fail to \"close the loop\" back into the physical design flow. In this work, we propose MLBuf-RePlAce, the first open-source learning-driven virtual buffering-aware analytical global placement framework, built on top of the OpenROAD infrastructure. MLBuf-RePlAce adopts an efficient recursive learning-based generative buffering approach to predict buffer types and locations, addressing ERC violations during global placement. We compare MLBuf-RePlAce against the default virtual buffering-based timing-driven global placer in OpenROAD, using open-source testcases from the TILOS MacroPlacement and OpenROAD-flow-scripts repositories. Without degradation of post-route power, MLBuf-RePlAce achieves (maximum, average) improvements of (56%, 31%) in total negative slack (TNS) within the open-source OpenROAD flow. When evaluated by completion in a commercial flow, MLBuf-RePlAce achieves (maximum, average) improvements of (53%, 28%) in TNS with an average of 0.2% improvement in post-route power.",
      "tldr_zh": "è¿™é¡¹ç ”ç©¶æå‡ºäº†MLBuf-RePlAceï¼Œè¿™æ˜¯é¦–ä¸ªåŸºäºOpenROADåŸºç¡€è®¾æ–½å¼€æºçš„å­¦ä¹ é©±åŠ¨è™šæ‹Ÿç¼“å†²æ„ŸçŸ¥åˆ†æå…¨å±€å¸ƒå±€(Analytical Global Placement)æ¡†æ¶ã€‚è¯¥æ¡†æ¶æ—¨åœ¨è§£å†³ä¼ ç»Ÿç¼“å†²æ–¹æ³•åœ¨å…¨å±€å¸ƒå±€æœŸé—´è®¡ç®—æˆæœ¬è¿‡é«˜ï¼Œä»¥åŠç°æœ‰æœºå™¨å­¦ä¹ æ–¹æ³•å¿½è§†ç”µæ°”è§„åˆ™æ£€æŸ¥(ERC)è¿è§„ä¸”æ— æ³•æœ‰æ•ˆé—­ç¯ç­‰æŒ‘æˆ˜ã€‚MLBuf-RePlAceé‡‡ç”¨äº†ä¸€ç§é«˜æ•ˆçš„é€’å½’å­¦ä¹ ç”Ÿæˆå¼ç¼“å†²æ–¹æ³•æ¥é¢„æµ‹ç¼“å†²å™¨çš„ç±»å‹å’Œä½ç½®ï¼Œä»è€Œåœ¨å¸ƒå±€é˜¶æ®µç›´æ¥å¤„ç†ERCè¿è§„ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œåœ¨ä¸å¢åŠ å¸ƒçº¿ååŠŸè€—çš„å‰æä¸‹ï¼Œè¯¥æ¡†æ¶åœ¨å¼€æºæµç¨‹ä¸­å®ç°äº†å¹³å‡31%çš„æ€»è´Ÿæ—¶åºè£•é‡(TNS)æå‡ã€‚åœ¨å•†ä¸šæµç¨‹è¯„ä¼°ä¸­ï¼ŒMLBuf-RePlAceä¸ä»…å°†TNSå¹³å‡æ”¹å–„äº†28%ï¼Œè¿˜å®ç°äº†0.2%çš„åŠŸè€—ä¼˜åŒ–ï¼Œæ˜¾è‘—æå‡äº†ç‰©ç†åˆæˆæµç¨‹ä¸­çš„æ—¶åºæ”¶æ•›æ•ˆç‡ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.17247v2",
      "published_date": "2025-06-07 18:22:31 UTC",
      "updated_date": "2025-07-30 18:51:25 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T19:42:37.896393+00:00"
    },
    {
      "arxiv_id": "2506.06881v1",
      "title": "KnowCoder-V2: Deep Knowledge Analysis",
      "title_zh": "KnowCoder-V2ï¼šæ·±åº¦çŸ¥è¯†åˆ†æ",
      "authors": [
        "Zixuan Li",
        "Wenxuan Liu",
        "Long Bai",
        "Chunmao Zhang",
        "Wei Li",
        "Fenghui Zhang",
        "Quanxin Jin",
        "Ruoyun He",
        "Zhuo Chen",
        "Zhilei Hu",
        "Fei Wang",
        "Bingbing Xu",
        "Xuhui Jiang",
        "Xiaolong Jin",
        "Jiafeng Guo",
        "Xueqi Cheng"
      ],
      "abstract": "Deep knowledge analysis tasks always involve the systematic extraction and association of knowledge from large volumes of data, followed by logical reasoning to discover insights. However, to solve such complex tasks, existing deep research frameworks face three major challenges: 1) They lack systematic organization and management of knowledge; 2) They operate purely online, making it inefficient for tasks that rely on shared and large-scale knowledge; 3) They cannot perform complex knowledge computation, limiting their abilities to produce insightful analytical results. Motivated by these, in this paper, we propose a \\textbf{K}nowledgeable \\textbf{D}eep \\textbf{R}esearch (\\textbf{KDR}) framework that empowers deep research with deep knowledge analysis capability. Specifically, it introduces an independent knowledge organization phase to preprocess large-scale, domain-relevant data into systematic knowledge offline. Based on this knowledge, it extends deep research with an additional kind of reasoning steps that perform complex knowledge computation in an online manner. To enhance the abilities of LLMs to solve knowledge analysis tasks in the above framework, we further introduce \\textbf{\\KCII}, an LLM that bridges knowledge organization and reasoning via unified code generation. For knowledge organization, it generates instantiation code for predefined classes, transforming data into knowledge objects. For knowledge computation, it generates analysis code and executes on the above knowledge objects to obtain deep analysis results. Experimental results on more than thirty datasets across six knowledge analysis tasks demonstrate the effectiveness of \\KCII. Moreover, when integrated into the KDR framework, \\KCII can generate high-quality reports with insightful analytical results compared to the mainstream deep research framework.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ç°æœ‰æ·±åº¦ç ”ç©¶æ¡†æ¶åœ¨çŸ¥è¯†ç³»ç»ŸåŒ–ç®¡ç†ã€å¤„ç†æ•ˆç‡ä»¥åŠå¤æ‚çŸ¥è¯†è®¡ç®—æ–¹é¢çš„å±€é™æ€§ï¼Œæå‡ºäº†çŸ¥è¯†åŒ–æ·±åº¦ç ”ç©¶(Knowledgeable Deep Research, KDR)æ¡†æ¶ï¼Œæ—¨åœ¨èµ‹äºˆæ·±åº¦ç ”ç©¶æ›´å¼ºçš„çŸ¥è¯†åˆ†æèƒ½åŠ›ã€‚è¯¥æ¡†æ¶å¼•å…¥äº†ç‹¬ç«‹çš„çŸ¥è¯†ç»„ç»‡é˜¶æ®µï¼Œå°†å¤§è§„æ¨¡é¢†åŸŸç›¸å…³æ•°æ®é¢„å¤„ç†ä¸ºç³»ç»ŸåŒ–çŸ¥è¯†ï¼Œå¹¶é€šè¿‡åœ¨çº¿æ¨ç†æ­¥éª¤æ‰§è¡Œå¤æ‚çš„çŸ¥è¯†è®¡ç®—ã€‚ä¸ºè¿›ä¸€æ­¥æå‡å¤§è¯­è¨€æ¨¡å‹(LLMs)åœ¨è¯¥æ¡†æ¶ä¸‹çš„è¡¨ç°ï¼Œç ”ç©¶è€…å¼€å‘äº† KnowCoder-V2 (KCII)ï¼Œé€šè¿‡ç»Ÿä¸€çš„ä»£ç ç”Ÿæˆ(code generation)æŠ€æœ¯è¡”æ¥çŸ¥è¯†ç»„ç»‡ä¸æ¨ç†è¿‡ç¨‹ã€‚åœ¨çŸ¥è¯†ç»„ç»‡é˜¶æ®µï¼ŒKCII é€šè¿‡ç”Ÿæˆå®ä¾‹åŒ–ä»£ç å°†æ•°æ®è½¬åŒ–ä¸ºçŸ¥è¯†å¯¹è±¡ï¼Œè€Œåœ¨çŸ¥è¯†è®¡ç®—é˜¶æ®µï¼Œå®ƒç”Ÿæˆå¹¶æ‰§è¡Œåˆ†æä»£ç ä»¥è·å–æ·±åº¦æ´å¯Ÿã€‚å®éªŒåœ¨æ¶µç›–å…­ç±»çŸ¥è¯†åˆ†æä»»åŠ¡çš„ 30 å¤šä¸ªæ•°æ®é›†ä¸ŠéªŒè¯äº† KCII çš„æœ‰æ•ˆæ€§ã€‚ç»“æœè¡¨æ˜ï¼Œé›†æˆåœ¨ KDR æ¡†æ¶ä¸­çš„ KCII èƒ½å¤Ÿæ¯”ä¸»æµæ·±åº¦ç ”ç©¶æ¡†æ¶ç”Ÿæˆæ›´é«˜è´¨é‡ã€ä¸”æ›´å…·è§è§£çš„ä¸“ä¸šåˆ†ææŠ¥å‘Šã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.06881v1",
      "published_date": "2025-06-07 18:01:25 UTC",
      "updated_date": "2025-06-07 18:01:25 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T19:42:39.026333+00:00"
    },
    {
      "arxiv_id": "2506.06874v4",
      "title": "LLM-D12: A Dual-Dimensional Scale of Instrumental and Relational Dependencies on Large Language Models",
      "title_zh": "LLM-D12ï¼šå¤§è¯­è¨€æ¨¡å‹å·¥å…·æ€§ä¸å…³ç³»æ€§ä¾èµ–åŒç»´åº¦é‡è¡¨",
      "authors": [
        "Ala Yankouskaya",
        "Areej B. Babiker",
        "Syeda W. F. Rizvi",
        "Sameha Alshakhsi",
        "Magnus Liebherr",
        "Raian Ali"
      ],
      "abstract": "There is growing interest in understanding how people interact with large language models (LLMs) and whether such models elicit dependency or even addictive behaviour. Validated tools to assess the extent to which individuals may become dependent on LLMs are scarce and primarily build on classic behavioral addiction symptoms, adapted to the context of LLM use. We view this as a conceptual limitation, as the LLM-human relationship is more nuanced and warrants a fresh and distinct perspective. To address this gap, we developed and validated a new 12-item questionnaire to measure LLM dependency, referred to as LLM-D12. The scale was based on the authors' prior theoretical work, with items developed accordingly and responses collected from 526 participants in the UK. Exploratory and confirmatory factor analyses, performed on separate halves of the total sample using a split-sample approach, supported a two-factor structure: Instrumental Dependency (six items) and Relationship Dependency (six items). Instrumental Dependency reflects the extent to which individuals rely on LLMs to support or collaborate in decision-making and cognitive tasks. Relationship Dependency captures the tendency to perceive LLMs as socially meaningful, sentient, or companion-like entities. The two-factor structure demonstrated excellent internal consistency and clear discriminant validity. External validation confirmed both the conceptual foundation and the distinction between the two subscales. The psychometric properties and structure of our LLM-D12 scale were interpreted in light of the emerging view that dependency on LLMs does not necessarily indicate dysfunction but may still reflect reliance levels that could become problematic in certain contexts.",
      "tldr_zh": "è¯¥ç ”ç©¶å¼€å‘å¹¶éªŒè¯äº† LLM-D12 è¿™ä¸€åŒ…å« 12 ä¸ªæ¡ç›®çš„æ–°é‡è¡¨ï¼Œæ—¨åœ¨ä»å…¨æ–°çš„è§†è§’è¯„ä¼°ä¸ªä½“å¯¹å¤§è¯­è¨€æ¨¡å‹ (Large Language Models, LLMs) çš„ä¾èµ–ç¨‹åº¦ï¼Œå¼¥è¡¥äº†ä¼ ç»Ÿè¯„ä¼°å·¥å…·è¿‡åº¦å—é™äºè¡Œä¸ºæˆç˜¾æ¡†æ¶çš„ä¸è¶³ã€‚é€šè¿‡å¯¹ 526 åå—è¯•è€…è¿›è¡Œæ¢ç´¢æ€§å’ŒéªŒè¯æ€§å› ç´ åˆ†æ (Exploratory and Confirmatory Factor Analyses)ï¼Œç ”ç©¶ç¡®ç«‹äº†é‡è¡¨çš„åŒç»´åº¦ç»“æ„ï¼šå·¥å…·æ€§ä¾èµ– (Instrumental Dependency) å’Œå…³ç³»æ€§ä¾èµ– (Relationship Dependency)ã€‚å‰è€…è¡¡é‡ä¸ªä½“åœ¨è®¤çŸ¥ä»»åŠ¡å’Œå†³ç­–ä¸­å¯¹ LLM çš„ä¾èµ–ï¼Œåè€…åˆ™è¡¡é‡å°† LLM è§†ä¸ºå…·å¤‡ç¤¾äº¤å±æ€§æˆ–åŒä¼´å®ä½“çš„å€¾å‘ã€‚å®éªŒç»“æœæ˜¾ç¤º LLM-D12 å…·æœ‰å“è¶Šçš„å†…éƒ¨ä¸€è‡´æ€§ (Internal Consistency) å’Œåˆ¤åˆ«æ•ˆåº¦ (Discriminant Validity)ï¼Œå¹¶å¾—åˆ°äº†å¤–éƒ¨éªŒè¯çš„æ”¯æŒã€‚è¯¥é‡è¡¨çš„æå‡ºä¸ºç†è§£äººæœºäº¤äº’ä¸­çš„ä¾èµ–æœºåˆ¶æä¾›äº†å¿ƒç†å­¦å·¥å…·ï¼Œå¼ºè°ƒè¿™ç§ä¾èµ–è™½ç„¶ä¸ä¸€å®šç­‰åŒäºåŠŸèƒ½éšœç¢ï¼Œä½†ä»éœ€åœ¨ç‰¹å®šèƒŒæ™¯ä¸‹å¯¹å…¶æ½œåœ¨é£é™©è¿›è¡Œç›‘æµ‹ã€‚",
      "categories": [
        "cs.HC",
        "cs.AI"
      ],
      "primary_category": "cs.HC",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.06874v4",
      "published_date": "2025-06-07 17:42:21 UTC",
      "updated_date": "2025-08-23 10:43:33 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T19:42:44.254892+00:00"
    },
    {
      "arxiv_id": "2506.06870v1",
      "title": "Recursive Semantic Anchoring in ISO 639:2023: A Structural Extension to ISO/TC 37 Frameworks",
      "title_zh": "ISO 639:2023 ä¸­çš„é€’å½’è¯­ä¹‰é”šå®šï¼šISO/TC 37 æ¡†æ¶çš„ç»“æ„æ€§æ‰©å±•",
      "authors": [
        "Bugra Kilictas",
        "Faruk Alpay"
      ],
      "abstract": "ISO 639:2023 unifies the ISO language-code family and introduces contextual metadata, but it lacks a machine-native mechanism for handling dialectal drift and creole mixtures. We propose a formalisation of recursive semantic anchoring, attaching to every language entity $Ï‡$ a family of fixed-point operators $Ï†_{n,m}$ that model bounded semantic drift via the relation $Ï†_{n,m}(Ï‡) = Ï‡\\oplus Î”(Ï‡)$, where $Î”(Ï‡)$ is a drift vector in a latent semantic manifold. The base anchor $Ï†_{0,0}$ recovers the canonical ISO 639:2023 identity, whereas $Ï†_{99,9}$ marks the maximal drift state that triggers a deterministic fallback. Using category theory, we treat the operators $Ï†_{n,m}$ as morphisms and drift vectors as arrows in a category $\\mathrm{DriftLang}$. A functor $Î¦: \\mathrm{DriftLang} \\to \\mathrm{AnchorLang}$ maps every drifted object to its unique anchor and proves convergence. We provide an RDF/Turtle schema (\\texttt{BaseLanguage}, \\texttt{DriftedLanguage}, \\texttt{ResolvedAnchor}) and worked examples -- e.g., $Ï†_{8,4}$ (Standard Mandarin) versus $Ï†_{8,7}$ (a colloquial variant), and $Ï†_{1,7}$ for Nigerian Pidgin anchored to English. Experiments with transformer models show higher accuracy in language identification and translation on noisy or code-switched input when the $Ï†$-indices are used to guide fallback routing. The framework is compatible with ISO/TC 37 and provides an AI-tractable, drift-aware semantic layer for future standards.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ ISO 639:2023 åœ¨å¤„ç†æ–¹è¨€æ¼‚å˜ (dialectal drift) å’Œæ··åˆè¯­ (creole mixtures) æ—¶ç¼ºä¹æœºå™¨åŸç”Ÿæœºåˆ¶çš„é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§é€’å½’è¯­ä¹‰é”šå®š (recursive semantic anchoring) çš„å½¢å¼åŒ–æ¡†æ¶ã€‚è¯¥æ¡†æ¶é€šè¿‡ä¸ºè¯­è¨€å®ä½“é™„åŠ ä¸åŠ¨ç‚¹ç®—å­ (fixed-point operators) $\\phi_{n,m}$ï¼Œåœ¨æ½œåœ¨è¯­ä¹‰æµå½¢ (latent semantic manifold) ä¸­åˆ©ç”¨æ¼‚å˜å‘é‡ (drift vector) å¯¹è¯­ä¹‰æ¼‚å˜è¿›è¡Œå»ºæ¨¡ã€‚ç ”ç©¶é‡‡ç”¨èŒƒç•´è®º (category theory) å°†è¿™äº›ç®—å­å®šä¹‰ä¸ºæ€å°„ï¼Œå¹¶é€šè¿‡å‡½å­ (functor) $\\Phi$ ç¡®ä¿äº†æ¼‚å˜å¯¹è±¡å‘å…¶å”¯ä¸€é”šç‚¹çš„æ”¶æ•›ã€‚æ­¤å¤–ï¼Œè¯¥ç ”ç©¶æä¾›äº†åŒ…æ‹¬ \\texttt{BaseLanguage} å’Œ \\texttt{ResolvedAnchor} åœ¨å†…çš„ RDF/Turtle æ¨¡å¼ï¼Œå¹¶åœ¨ Transformer æ¨¡å‹ä¸Šè¿›è¡Œäº†éªŒè¯ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œåœ¨å¤„ç†å™ªå£°æˆ–ä»£ç æ··ç”¨ (code-switched) çš„è¾“å…¥æ—¶ï¼Œåˆ©ç”¨ $\\phi$ ç´¢å¼•å¼•å¯¼è·¯ç”±èƒ½æ˜¾è‘—æå‡è¯­è¨€è¯†åˆ«ä¸ç¿»è¯‘çš„å‡†ç¡®ç‡ã€‚è¯¥æ¡†æ¶ä¸ ISO/TC 37 ä½“ç³»å®Œå…¨å…¼å®¹ï¼Œä¸ºæœªæ¥è¯­è¨€æ ‡å‡†æä¾›äº†ä¸€ä¸ªå…·å¤‡äººå·¥æ™ºèƒ½å¤„ç†èƒ½åŠ›ä¸”æ„ŸçŸ¥æ¼‚å˜çš„è¯­ä¹‰å±‚ã€‚",
      "categories": [
        "cs.LO",
        "cs.AI"
      ],
      "primary_category": "cs.LO",
      "comment": "21 pages, no figures. Includes formal proofs, RDF/Turtle ontology schema, Ï†-index disambiguation cases, and evaluation of transformer-based AI models under semantic drift",
      "pdf_url": "https://arxiv.org/pdf/2506.06870v1",
      "published_date": "2025-06-07 17:36:13 UTC",
      "updated_date": "2025-06-07 17:36:13 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T19:42:48.104884+00:00"
    },
    {
      "arxiv_id": "2506.06868v1",
      "title": "Incorporating Failure of Machine Learning in Dynamic Probabilistic Safety Assurance",
      "title_zh": "å°†æœºå™¨å­¦ä¹ å¤±æ•ˆçº³å…¥åŠ¨æ€æ¦‚ç‡å®‰å…¨ä¿éšœ",
      "authors": [
        "Razieh Arshadizadeh",
        "Mahmoud Asgari",
        "Zeinab Khosravi",
        "Yiannis Papadopoulos",
        "Koorosh Aslansefat"
      ],
      "abstract": "Machine Learning (ML) models are increasingly integrated into safety-critical systems, such as autonomous vehicle platooning, to enable real-time decision-making. However, their inherent imperfection introduces a new class of failure: reasoning failures often triggered by distributional shifts between operational and training data. Traditional safety assessment methods, which rely on design artefacts or code, are ill-suited for ML components that learn behaviour from data. SafeML was recently proposed to dynamically detect such shifts and assign confidence levels to the reasoning of ML-based components. Building on this, we introduce a probabilistic safety assurance framework that integrates SafeML with Bayesian Networks (BNs) to model ML failures as part of a broader causal safety analysis. This allows for dynamic safety evaluation and system adaptation under uncertainty. We demonstrate the approach on an simulated automotive platooning system with traffic sign recognition. The findings highlight the potential broader benefits of explicitly modelling ML failures in safety assessment.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹æœºå™¨å­¦ä¹ (Machine Learning)æ¨¡å‹åœ¨è‡ªåŠ¨é©¾é©¶è½¦é˜Ÿ(autonomous vehicle platooning)ç­‰å®‰å…¨å…³é”®ç³»ç»Ÿä¸­ï¼Œå› æ•°æ®åˆ†å¸ƒåç§»(distributional shifts)å¼•å‘æ¨ç†å¤±è´¥(reasoning failures)çš„æŒ‘æˆ˜ã€‚ä¼ ç»Ÿçš„å®‰å…¨è¯„ä¼°æ–¹æ³•éš¾ä»¥æœ‰æ•ˆåº”å¯¹åŸºäºæ•°æ®é©±åŠ¨çš„MLç»„ä»¶ï¼Œå› æ­¤ä½œè€…æå‡ºäº†ä¸€ç§å…¨æ–°çš„æ¦‚ç‡å®‰å…¨ä¿éšœæ¡†æ¶ã€‚è¯¥æ¡†æ¶å°†SafeMLæŠ€æœ¯ä¸è´å¶æ–¯ç½‘ç»œ(Bayesian Networks)é›†æˆï¼Œæ—¨åœ¨å°†MLå¤±æ•ˆçº³å…¥ç³»ç»Ÿçº§çš„å› æœå®‰å…¨åˆ†æä¹‹ä¸­ã€‚è¿™ç§æ–¹æ³•èƒ½å¤Ÿåœ¨ä¸ç¡®å®šæ€§ç¯å¢ƒä¸‹å®ç°åŠ¨æ€å®‰å…¨è¯„ä¼°å’Œç³»ç»Ÿè‡ªé€‚åº”ï¼Œä»è€Œå®æ—¶æ£€æµ‹å¹¶å“åº”MLæ¨¡å‹çš„æ€§èƒ½æ³¢åŠ¨ã€‚é€šè¿‡åœ¨äº¤é€šæ ‡å¿—è¯†åˆ«çš„æ¨¡æ‹Ÿæ±½è½¦è½¦é˜Ÿç³»ç»Ÿä¸Šçš„æ¼”ç¤ºï¼Œç ”ç©¶ç»“æœçªæ˜¾äº†åœ¨å®‰å…¨è¯„ä¼°ä¸­æ˜¾å¼å»ºæ¨¡MLå¤±æ•ˆå¯¹äºæå‡ç³»ç»Ÿé²æ£’æ€§çš„é‡è¦ä»·å€¼ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.06868v1",
      "published_date": "2025-06-07 17:16:05 UTC",
      "updated_date": "2025-06-07 17:16:05 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T19:42:55.553545+00:00"
    },
    {
      "arxiv_id": "2506.06866v2",
      "title": "SAFE: Finding Sparse and Flat Minima to Improve Pruning",
      "title_zh": "SAFEï¼šå¯»æ‰¾ç¨€ç–å¹³å¦æå°å€¼ä»¥æå‡å‰ªææ€§èƒ½",
      "authors": [
        "Dongyeop Lee",
        "Kwanhee Lee",
        "Jinseok Chung",
        "Namhoon Lee"
      ],
      "abstract": "Sparsifying neural networks often suffers from seemingly inevitable performance degradation, and it remains challenging to restore the original performance despite much recent progress. Motivated by recent studies in robust optimization, we aim to tackle this problem by finding subnetworks that are both sparse and flat at the same time. Specifically, we formulate pruning as a sparsity-constrained optimization problem where flatness is encouraged as an objective. We solve it explicitly via an augmented Lagrange dual approach and extend it further by proposing a generalized projection operation, resulting in novel pruning methods called SAFE and its extension, SAFE$^+$. Extensive evaluations on standard image classification and language modeling tasks reveal that SAFE consistently yields sparse networks with improved generalization performance, which compares competitively to well-established baselines. In addition, SAFE demonstrates resilience to noisy data, making it well-suited for real-world conditions.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ç¥ç»ç½‘ç»œåœ¨å‰ªæï¼ˆPruningï¼‰è¿‡ç¨‹ä¸­å¸¸è§çš„æ€§èƒ½ä¸‹é™é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§å¯»æ‰¾åŒæ—¶å…·å¤‡ç¨€ç–æ€§ï¼ˆSparseï¼‰å’Œå¹³å¦æ€§ï¼ˆFlatï¼‰çš„å­ç½‘ç»œçš„æ–°æ–¹æ³•ã€‚ä½œè€…å°†å‰ªæè¿‡ç¨‹å»ºæ¨¡ä¸ºä¸€ä¸ªä»¥å¹³å¦æ€§ä¸ºç›®æ ‡çš„ç¨€ç–çº¦æŸä¼˜åŒ–é—®é¢˜ï¼Œå¹¶é€šè¿‡å¢å¹¿æ‹‰æ ¼æœ—æ—¥å¯¹å¶ï¼ˆAugmented Lagrange dualï¼‰æ–¹æ³•è¿›è¡Œæ˜¾å¼æ±‚è§£ã€‚åœ¨æ­¤åŸºç¡€ä¸Šï¼Œç ”ç©¶å¼•å…¥äº†å¹¿ä¹‰æŠ•å½±æ“ä½œï¼Œå¼€å‘å‡ºåä¸º SAFE åŠå…¶æ‰©å±•ç‰ˆæœ¬ SAFE+ çš„æ–°å‹å‰ªæç®—æ³•ã€‚åœ¨æ ‡å‡†å›¾åƒåˆ†ç±»å’Œè¯­è¨€å»ºæ¨¡ä»»åŠ¡ä¸Šçš„å®éªŒç»“æœæ˜¾ç¤ºï¼ŒSAFE èƒ½å¤Ÿä¸€è‡´åœ°ç”Ÿæˆå…·æœ‰æ›´å¼ºæ³›åŒ–èƒ½åŠ›çš„ç¨€ç–ç½‘ç»œï¼Œå…¶æ€§èƒ½ä¸ç°æœ‰çš„ä¸»æµåŸºå‡†æ¨¡å‹ç›¸æ¯”å…·æœ‰æ˜¾è‘—ç«äº‰åŠ›ã€‚æ­¤å¤–ï¼ŒSAFE åœ¨é¢å¯¹å™ªå£°æ•°æ®æ—¶è¿˜å±•ç°å‡ºäº†å“è¶Šçš„é²æ£’æ€§ï¼Œè¯æ˜äº†è¯¥æ–¹æ³•åœ¨ç°å®ä¸–ç•Œå¤æ‚æ¡ä»¶ä¸‹çš„åº”ç”¨æ½œåŠ›ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "ICML 2025",
      "pdf_url": "https://arxiv.org/pdf/2506.06866v2",
      "published_date": "2025-06-07 17:12:03 UTC",
      "updated_date": "2025-06-16 12:36:58 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T19:43:22.190723+00:00"
    },
    {
      "arxiv_id": "2506.06864v1",
      "title": "Face recognition on point cloud with cgan-top for denoising",
      "title_zh": "åŸºäºcgan-topå»å™ªçš„ç‚¹äº‘äººè„¸è¯†åˆ«",
      "authors": [
        "Junyu Liu",
        "Jianfeng Ren",
        "Sunhong Liang",
        "Xudong Jiang"
      ],
      "abstract": "Face recognition using 3D point clouds is gaining growing interest, while raw point clouds often contain a significant amount of noise due to imperfect sensors. In this paper, an end-to-end 3D face recognition on a noisy point cloud is proposed, which synergistically integrates the denoising and recognition modules. Specifically, a Conditional Generative Adversarial Network on Three Orthogonal Planes (cGAN-TOP) is designed to effectively remove the noise in the point cloud, and recover the underlying features for subsequent recognition. A Linked Dynamic Graph Convolutional Neural Network (LDGCNN) is then adapted to recognize faces from the processed point cloud, which hierarchically links both the local point features and neighboring features of multiple scales. The proposed method is validated on the Bosphorus dataset. It significantly improves the recognition accuracy under all noise settings, with a maximum gain of 14.81%.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§é’ˆå¯¹å™ªå£°ç‚¹äº‘çš„ç«¯åˆ°ç«¯3Däººè„¸è¯†åˆ«æ–¹æ³•ï¼Œæ—¨åœ¨è§£å†³ä¼ æ„Ÿå™¨ä¸å®Œç¾å¯¼è‡´çš„å™ªå£°å¹²æ‰°é—®é¢˜ã€‚ç ”ç©¶ååŒé›†æˆäº†å»å™ªä¸è¯†åˆ«æ¨¡å—ï¼Œé¦–å…ˆåˆ©ç”¨è®¾è®¡çš„åŸºäºä¸‰ä¸ªæ­£äº¤å¹³é¢çš„æ¡ä»¶ç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼ˆcGAN-TOPï¼‰æœ‰æ•ˆå»é™¤ç‚¹äº‘å™ªå£°å¹¶æ¢å¤åº•å±‚ç‰¹å¾ã€‚éšåï¼Œé€šè¿‡æ”¹è¿›çš„é“¾æ¥åŠ¨æ€å›¾å·ç§¯ç¥ç»ç½‘ç»œï¼ˆLDGCNNï¼‰è¿›è¡Œäººè„¸è¯†åˆ«ï¼Œè¯¥ç½‘ç»œåˆ†å±‚é“¾æ¥äº†å¤šä¸ªå°ºåº¦çš„å±€éƒ¨ç‚¹ç‰¹å¾ä¸é‚»åŸŸç‰¹å¾ã€‚åœ¨ Bosphorus æ•°æ®é›†ä¸Šçš„å®éªŒéªŒè¯è¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨æ‰€æœ‰å™ªå£°ç¯å¢ƒä¸‹å‡æ˜¾è‘—æé«˜äº†è¯†åˆ«å‡†ç¡®ç‡ï¼Œæœ€å¤§æå‡å¹…åº¦è¾¾ 14.81%ã€‚è¿™ä¸€æˆæœè¯æ˜äº†ç»“åˆä¸“é—¨å»å™ªæ¨¡å—çš„ç«¯åˆ°ç«¯æ¶æ„åœ¨å¤„ç†çœŸå®ä¸–ç•Œç‚¹äº‘æ•°æ®æ—¶çš„æœ‰æ•ˆæ€§ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "Published in ICASSP 2023",
      "pdf_url": "https://arxiv.org/pdf/2506.06864v1",
      "published_date": "2025-06-07 17:09:31 UTC",
      "updated_date": "2025-06-07 17:09:31 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T19:42:57.641375+00:00"
    },
    {
      "arxiv_id": "2506.06862v1",
      "title": "Multimodal Spatial Language Maps for Robot Navigation and Manipulation",
      "title_zh": "é¢å‘æœºå™¨äººå¯¼èˆªä¸æ“ä½œçš„å¤šæ¨¡æ€ç©ºé—´è¯­è¨€åœ°å›¾",
      "authors": [
        "Chenguang Huang",
        "Oier Mees",
        "Andy Zeng",
        "Wolfram Burgard"
      ],
      "abstract": "Grounding language to a navigating agent's observations can leverage pretrained multimodal foundation models to match perceptions to object or event descriptions. However, previous approaches remain disconnected from environment mapping, lack the spatial precision of geometric maps, or neglect additional modality information beyond vision. To address this, we propose multimodal spatial language maps as a spatial map representation that fuses pretrained multimodal features with a 3D reconstruction of the environment. We build these maps autonomously using standard exploration. We present two instances of our maps, which are visual-language maps (VLMaps) and their extension to audio-visual-language maps (AVLMaps) obtained by adding audio information. When combined with large language models (LLMs), VLMaps can (i) translate natural language commands into open-vocabulary spatial goals (e.g., \"in between the sofa and TV\") directly localized in the map, and (ii) be shared across different robot embodiments to generate tailored obstacle maps on demand. Building upon the capabilities above, AVLMaps extend VLMaps by introducing a unified 3D spatial representation integrating audio, visual, and language cues through the fusion of features from pretrained multimodal foundation models. This enables robots to ground multimodal goal queries (e.g., text, images, or audio snippets) to spatial locations for navigation. Additionally, the incorporation of diverse sensory inputs significantly enhances goal disambiguation in ambiguous environments. Experiments in simulation and real-world settings demonstrate that our multimodal spatial language maps enable zero-shot spatial and multimodal goal navigation and improve recall by 50% in ambiguous scenarios. These capabilities extend to mobile robots and tabletop manipulators, supporting navigation and interaction guided by visual, audio, and spatial cues.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†å¤šæ¨¡æ€ç©ºé—´è¯­è¨€åœ°å›¾ï¼ˆmultimodal spatial language mapsï¼‰ï¼Œé€šè¿‡å°†é¢„è®­ç»ƒçš„å¤šæ¨¡æ€ç‰¹å¾ä¸ç¯å¢ƒçš„ 3D é‡å»ºç›¸ç»“åˆï¼Œæœ‰æ•ˆè§£å†³äº†ä»¥å¾€æ–¹æ³•åœ¨ç©ºé—´ç²¾åº¦å’Œå¤šæ¨¡æ€èåˆæ–¹é¢çš„å±€é™ã€‚ç ”ç©¶è€…å±•ç¤ºäº†è§†è§‰è¯­è¨€åœ°å›¾ï¼ˆVLMapsï¼‰åŠå…¶æ‰©å±•ç‰ˆæœ¬éŸ³è§†é¢‘è¯­è¨€åœ°å›¾ï¼ˆAVLMapsï¼‰ï¼Œåè€…é€šè¿‡æ•´åˆéŸ³é¢‘ä¿¡æ¯è¿›ä¸€æ­¥å¢å¼ºäº†æœºå™¨äººçš„æ„ŸçŸ¥èƒ½åŠ›ã€‚é€šè¿‡ç»“åˆå¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ï¼Œè¯¥æ¡†æ¶èƒ½å°†è‡ªç„¶è¯­è¨€æŒ‡ä»¤ç›´æ¥è½¬åŒ–ä¸ºåœ°å›¾ä¸­çš„å¼€æ”¾è¯æ±‡ç©ºé—´ç›®æ ‡ï¼ˆopen-vocabulary spatial goalsï¼‰ï¼Œå¹¶æ”¯æŒè·¨ä¸åŒæœºå™¨äººå½¢æ€ç”Ÿæˆå®šåˆ¶åŒ–éšœç¢ç‰©åœ°å›¾ã€‚AVLMaps å®ç°äº†æ–‡æœ¬ã€å›¾åƒå’ŒéŸ³é¢‘ç‰‡æ®µä¸ 3D ç©ºé—´ä½ç½®çš„ç»Ÿä¸€å¯¹é½ï¼Œæ˜¾è‘—æå‡äº†æœºå™¨äººåœ¨æ¨¡ç³Šç¯å¢ƒä¸­çš„ç›®æ ‡æ¶ˆæ­§èƒ½åŠ›ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•æ”¯æŒé›¶æ ·æœ¬ï¼ˆzero-shotï¼‰çš„ç©ºé—´åŠå¤šæ¨¡æ€ç›®æ ‡å¯¼èˆªï¼Œåœ¨æ¨¡ç³Šåœºæ™¯ä¸‹çš„å¬å›ç‡æå‡äº† 50%ã€‚è¯¥æŠ€æœ¯å·²æˆåŠŸåº”ç”¨äºç§»åŠ¨æœºå™¨äººå’Œæ¡Œé¢æœºæ¢°è‡‚ï¼Œä¸ºè§†è§‰ã€éŸ³é¢‘åŠç©ºé—´çº¿ç´¢å¼•å¯¼çš„å¤æ‚å¯¼èˆªä¸äº¤äº’ä»»åŠ¡å¥ å®šäº†åŸºç¡€ã€‚",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.CV",
        "cs.LG",
        "cs.SD",
        "eess.AS"
      ],
      "primary_category": "cs.RO",
      "comment": "accepted to International Journal of Robotics Research (IJRR). 24 pages, 18 figures. The paper contains texts from VLMaps(arXiv:2210.05714) and AVLMaps(arXiv:2303.07522). The project page is https://mslmaps.github.io/",
      "pdf_url": "https://arxiv.org/pdf/2506.06862v1",
      "published_date": "2025-06-07 17:02:13 UTC",
      "updated_date": "2025-06-07 17:02:13 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T19:43:42.490789+00:00"
    },
    {
      "arxiv_id": "2506.06858v2",
      "title": "High-Fidelity Scientific Simulation Surrogates via Adaptive Implicit Neural Representations",
      "title_zh": "åŸºäºè‡ªé€‚åº”éšå¼ç¥ç»è¡¨ç¤ºçš„é«˜ä¿çœŸç§‘å­¦æ¨¡æ‹Ÿä»£ç†æ¨¡å‹",
      "authors": [
        "Ziwei Li",
        "Yuhan Duan",
        "Tianyu Xiong",
        "Yi-Tang Chen",
        "Wei-Lun Chao",
        "Han-Wei Shen"
      ],
      "abstract": "Effective surrogate models are critical for accelerating scientific simulations. Implicit neural representations (INRs) offer a compact and continuous framework for modeling spatially structured data, but they often struggle with complex scientific fields exhibiting localized, high-frequency variations. Recent approaches address this by introducing additional features along rigid geometric structures (e.g., grids), but at the cost of flexibility and increased model size. In this paper, we propose a simple yet effective alternative: Feature-Adaptive INR (FA-INR). FA-INR leverages cross-attention to an augmented memory bank to learn flexible feature representations, enabling adaptive allocation of model capacity based on data characteristics, rather than rigid structural assumptions. To further improve scalability, we introduce a coordinate-guided mixture of experts (MoE) that enhances the specialization and efficiency of feature representations. Experiments on three large-scale ensemble simulation datasets show that FA-INR achieves state-of-the-art fidelity while significantly reducing model size, establishing a new trade-off frontier between accuracy and compactness for INR-based surrogates.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ç§‘å­¦æ¨¡æ‹ŸåŠ é€Ÿä¸­çš„ä»£ç†æ¨¡å‹é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§åä¸º Feature-Adaptive INR (FA-INR) çš„æ–°å‹æ¶æ„ï¼Œæ—¨åœ¨è§£å†³ä¼ ç»Ÿçš„ Implicit Neural Representations (INRs) åœ¨å¤„ç†å±€éƒ¨é«˜é¢‘å˜åŒ–æ—¶çµæ´»æ€§ä¸è¶³ä¸”æ¨¡å‹å†—ä½™çš„é—®é¢˜ã€‚FA-INR é€šè¿‡å¼•å…¥ Cross-attention æœºåˆ¶ç»“åˆå¢å¼ºçš„ Memory Bankï¼Œå®ç°äº†æ ¹æ®æ•°æ®ç‰¹å¾è‡ªé€‚åº”åˆ†é…æ¨¡å‹å®¹é‡çš„èƒ½åŠ›ï¼Œæ‘†è„±äº†å¯¹å›ºå®šå‡ ä½•ç»“æ„çš„ä¾èµ–ã€‚ä¸ºäº†å¢å¼ºç‰¹å¾è¡¨ç¤ºçš„ä¸“ä¸šåŒ–ç¨‹åº¦å’Œè®¡ç®—æ•ˆç‡ï¼Œç ”ç©¶è¿›ä¸€æ­¥å¼•å…¥äº† Coordinate-guided Mixture of Experts (MoE) æŠ€æœ¯ã€‚åœ¨ä¸‰ä¸ªå¤§è§„æ¨¡ç³»ç»¼æ¨¡æ‹Ÿæ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¯æ˜ï¼ŒFA-INR åœ¨å®ç° State-of-the-art çš„é«˜ä¿çœŸåº¦çš„åŒæ—¶ï¼Œæ˜¾è‘—é™ä½äº†æ¨¡å‹å‚æ•°é‡ã€‚è¯¥ç ”ç©¶ä¸ºåŸºäº INR çš„ç§‘å­¦ä»¿çœŸä»£ç†æ¨¡å‹åœ¨å‡†ç¡®æ€§ä¸ç´§å‡‘æ€§ä¹‹é—´ç¡®ç«‹äº†æ–°çš„æŠ€æœ¯å‰æ²¿ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.06858v2",
      "published_date": "2025-06-07 16:45:17 UTC",
      "updated_date": "2025-09-15 02:36:14 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T19:43:39.880832+00:00"
    },
    {
      "arxiv_id": "2506.06852v2",
      "title": "Position Prediction Self-Supervised Learning for Multimodal Satellite Imagery Semantic Segmentation",
      "title_zh": "é¢å‘å¤šæ¨¡æ€å«æ˜Ÿå›¾åƒè¯­ä¹‰åˆ†å‰²çš„ä½ç½®é¢„æµ‹è‡ªç›‘ç£å­¦ä¹ ",
      "authors": [
        "John Waithaka",
        "Moise Busogi"
      ],
      "abstract": "Semantic segmentation of satellite imagery is crucial for Earth observation applications, but remains constrained by limited labelled training data. While self-supervised pretraining methods like Masked Autoencoders (MAE) have shown promise, they focus on reconstruction rather than localisation-a fundamental aspect of segmentation tasks. We propose adapting LOCA (Location-aware), a position prediction self-supervised learning method, for multimodal satellite imagery semantic segmentation. Our approach addresses the unique challenges of satellite data by extending SatMAE's channel grouping from multispectral to multimodal data, enabling effective handling of multiple modalities, and introducing same-group attention masking to encourage cross-modal interaction during pretraining. The method uses relative patch position prediction, encouraging spatial reasoning for localisation rather than reconstruction. We evaluate our approach on the Sen1Floods11 flood mapping dataset, where it significantly outperforms existing reconstruction-based self-supervised learning methods for satellite imagery. Our results demonstrate that position prediction tasks, when properly adapted for multimodal satellite imagery, learn representations more effective for satellite image semantic segmentation than reconstruction-based approaches.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§é’ˆå¯¹å¤šæ¨¡æ€å«æ˜Ÿå›¾åƒè¯­ä¹‰åˆ†å‰²çš„ä½ç½®é¢„æµ‹è‡ªç›‘ç£å­¦ä¹ æ–¹æ³•ï¼Œæ—¨åœ¨è§£å†³æ ‡æ³¨æ•°æ®æœ‰é™ä¸”ç°æœ‰é‡å»ºä»»åŠ¡ç¼ºä¹å®šä½èƒ½åŠ›çš„é—®é¢˜ã€‚é€šè¿‡å°†LOCAï¼ˆLocation-awareï¼‰æ–¹æ³•å¼•å…¥å«æ˜Ÿå›¾åƒé¢†åŸŸï¼Œè¯¥æ–¹æ¡ˆæ‰©å±•äº†SatMAEçš„é€šé“åˆ†ç»„(channel grouping)ç­–ç•¥ä»¥é€‚é…å¤šæ¨¡æ€æ•°æ®ï¼Œå¹¶å¼•å…¥äº†åŒç»„æ³¨æ„åŠ›æ©ç (same-group attention masking)æ¥å¢å¼ºè·¨æ¨¡æ€äº¤äº’ã€‚ä¸åŒäºä¼ ç»Ÿçš„é‡å»ºä»»åŠ¡ï¼Œè¯¥æ–¹æ³•åˆ©ç”¨ç›¸å¯¹åˆ†å—ä½ç½®é¢„æµ‹(relative patch position prediction)æ¥å¼ºåŒ–æ¨¡å‹çš„ç©ºé—´æ¨ç†ä¸å®šä½èƒ½åŠ›ã€‚åœ¨Sen1Floods11æ´ªæ°´å¡«å›¾æ•°æ®é›†ä¸Šçš„è¯„ä¼°ç»“æœæ˜¾ç¤ºï¼Œè¯¥æ–¹æ³•æ˜¾è‘—ä¼˜äºç°æœ‰çš„åŸºäºé‡å»ºçš„è‡ªç›‘ç£é¢„è®­ç»ƒæ–¹æ³•ã€‚å®éªŒè¯æ˜ï¼Œç»è¿‡é€‚é…çš„ä½ç½®é¢„æµ‹ä»»åŠ¡èƒ½ä¸ºå«æ˜Ÿå›¾åƒè¯­ä¹‰åˆ†å‰²å­¦ä¹ åˆ°æ¯”é‡å»ºä»»åŠ¡æ›´å…·ä»£è¡¨æ€§çš„ç‰¹å¾è¡¨ç¤ºã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.06852v2",
      "published_date": "2025-06-07 16:16:29 UTC",
      "updated_date": "2025-07-16 06:30:34 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T19:43:44.088224+00:00"
    },
    {
      "arxiv_id": "2506.06843v2",
      "title": "United Minds or Isolated Agents? Exploring Coordination of LLMs under Cognitive Load Theory",
      "title_zh": "ä¼—å¿—æˆåŸè¿˜æ˜¯å­¤å†›å¥‹æˆ˜ï¼Ÿè®¤çŸ¥è´Ÿè·ç†è®ºè§†è§’ä¸‹çš„å¤§è¯­è¨€æ¨¡å‹åä½œæ¢ç©¶",
      "authors": [
        "HaoYang Shang",
        "Xuan Liu",
        "Zi Liang",
        "Jie Zhang",
        "Haibo Hu",
        "Song Guo"
      ],
      "abstract": "Large Language Models (LLMs) exhibit a notable performance ceiling on complex, multi-faceted tasks, as they often fail to integrate diverse information or adhere to multiple constraints. We posit that such limitation arises when the demands of a task exceed the LLM's effective cognitive load capacity. This interpretation draws a strong analogy to Cognitive Load Theory (CLT) in cognitive science, which explains similar performance boundaries in the human mind, and is further supported by emerging evidence that reveals LLMs have bounded working memory characteristics. Building upon this CLT-grounded understanding, we introduce CoThinker, a novel LLM-based multi-agent framework designed to mitigate cognitive overload and enhance collaborative problem-solving abilities. CoThinker operationalizes CLT principles by distributing intrinsic cognitive load through agent specialization and managing transactional load via structured communication and a collective working memory. We empirically validate CoThinker on complex problem-solving tasks and fabricated high cognitive load scenarios, demonstrating improvements over existing multi-agent baselines in solution quality and efficiency. Our analysis reveals characteristic interaction patterns, providing insights into the emergence of collective cognition and effective load management, thus offering a principled approach to overcoming LLM performance ceilings.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†å¤§å‹è¯­è¨€æ¨¡å‹ (LLMs) åœ¨å¤„ç†å¤æ‚å¤šé¢ä»»åŠ¡æ—¶çš„æ€§èƒ½ç“¶é¢ˆï¼Œå¹¶å°†å…¶å½’å› äºä»»åŠ¡éœ€æ±‚è¶…è¿‡äº†æ¨¡å‹çš„æœ‰æ•ˆè®¤çŸ¥è´Ÿè·å®¹é‡ã€‚ä½œè€…å€Ÿé‰´è®¤çŸ¥ç§‘å­¦ä¸­çš„è®¤çŸ¥è´Ÿè·ç†è®º (Cognitive Load Theory, CLT)ï¼Œè®¤ä¸º LLMs å…·æœ‰ä¸äººç±»ç›¸ä¼¼çš„æœ‰ç•Œå·¥ä½œè®°å¿†ç‰¹å¾ã€‚ä¸ºæ­¤ï¼Œç ”ç©¶æå‡ºäº†åä¸º CoThinker çš„æ–°å‹å¤šæ™ºèƒ½ä½“æ¡†æ¶ï¼Œæ—¨åœ¨é€šè¿‡æ™ºèƒ½ä½“ä¸“ä¸šåŒ–åˆ†æ‹…å†…åœ¨è®¤çŸ¥è´Ÿè· (intrinsic cognitive load)ï¼Œå¹¶åˆ©ç”¨ç»“æ„åŒ–é€šä¿¡ä¸é›†ä½“å·¥ä½œè®°å¿†ç®¡ç†äº‹åŠ¡æ€§è´Ÿè· (transactional load)ã€‚å®éªŒåœ¨å¤æ‚é—®é¢˜è§£å†³å’Œé«˜è®¤çŸ¥è´Ÿè·åœºæ™¯ä¸‹éªŒè¯äº†è¯¥æ¡†æ¶ï¼Œç»“æœæ˜¾ç¤ºå…¶åœ¨è§£å†³æ–¹æ¡ˆè´¨é‡å’Œæ•ˆç‡ä¸Šå‡ä¼˜äºç°æœ‰çš„å¤šæ™ºèƒ½ä½“åŸºçº¿æ¨¡å‹ã€‚è¯¥æˆæœä¸ºå…‹æœ LLMs æ€§èƒ½å¤©èŠ±æ¿æä¾›äº†åŸºäºåŸåˆ™çš„é›†ä½“è®¤çŸ¥ç®¡ç†æ–¹æ¡ˆï¼Œå¹¶ä¸ºç†è§£æœ‰æ•ˆè´Ÿè·ç®¡ç†ä¸‹çš„äº¤äº’æ¨¡å¼æä¾›äº†æ–°è§è§£ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.06843v2",
      "published_date": "2025-06-07 15:48:04 UTC",
      "updated_date": "2025-09-25 12:49:56 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T19:43:49.787615+00:00"
    },
    {
      "arxiv_id": "2506.06842v2",
      "title": "PCoT: Persuasion-Augmented Chain of Thought for Detecting Fake News and Social Media Disinformation",
      "title_zh": "PCoTï¼šç”¨äºè™šå‡æ–°é—»ä¸ç¤¾äº¤åª’ä½“è™šå‡ä¿¡æ¯æ£€æµ‹çš„è¯´æœå¢å¼ºæ€ç»´é“¾",
      "authors": [
        "Arkadiusz Modzelewski",
        "Witold Sosnowski",
        "Tiziano Labruna",
        "Adam Wierzbicki",
        "Giovanni Da San Martino"
      ],
      "abstract": "Disinformation detection is a key aspect of media literacy. Psychological studies have shown that knowledge of persuasive fallacies helps individuals detect disinformation. Inspired by these findings, we experimented with large language models (LLMs) to test whether infusing persuasion knowledge enhances disinformation detection. As a result, we introduce the Persuasion-Augmented Chain of Thought (PCoT), a novel approach that leverages persuasion to improve disinformation detection in zero-shot classification. We extensively evaluate PCoT on online news and social media posts. Moreover, we publish two novel, up-to-date disinformation datasets: EUDisinfo and MultiDis. These datasets enable the evaluation of PCoT on content entirely unseen by the LLMs used in our experiments, as the content was published after the models' knowledge cutoffs. We show that, on average, PCoT outperforms competitive methods by 15% across five LLMs and five datasets. These findings highlight the value of persuasion in strengthening zero-shot disinformation detection.",
      "tldr_zh": "è¯¥ç ”ç©¶å—åˆ°å¿ƒç†å­¦ç ”ç©¶çš„å¯å‘ï¼Œæå‡ºäº† Persuasion-Augmented Chain of Thought (PCoT)ï¼Œè¿™æ˜¯ä¸€ç§é€šè¿‡æ³¨å…¥è¯´æœæ€§çŸ¥è¯† (persuasion knowledge) æ¥å¢å¼ºå¤§å‹è¯­è¨€æ¨¡å‹ (LLMs) æ£€æµ‹è™šå‡ä¿¡æ¯èƒ½åŠ›çš„æ–°æ–¹æ³•ã€‚PCoT æ—¨åœ¨åˆ©ç”¨è¯´æœæŠ€å·§çš„è¯†åˆ«æ¥æ”¹è¿› zero-shot classification ä»»åŠ¡ï¼Œå¸®åŠ©æ¨¡å‹æ›´æœ‰æ•ˆåœ°è¯†åˆ«ç½‘ç»œæ–°é—»å’Œç¤¾äº¤åª’ä½“ä¸­çš„è™šå‡ä¿¡æ¯ã€‚ä¸ºéªŒè¯è¯¥æ–¹æ³•çš„æœ‰æ•ˆæ€§ï¼Œç ”ç©¶å›¢é˜Ÿå‘å¸ƒäº†ä¸¤ä¸ªå…¨æ–°çš„æ•°æ®é›† EUDisinfo å’Œ MultiDisï¼Œå…¶ä¸­åŒ…å«åœ¨æ¨¡å‹çŸ¥è¯†æˆªæ­¢æ—¥æœŸä¹‹åå‘å¸ƒçš„å†…å®¹ï¼Œç¡®ä¿äº†è¯„ä¼°çš„å…¬æ­£æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒPCoT åœ¨äº”ç§ LLMs å’Œäº”ä¸ªæ•°æ®é›†ä¸Šçš„è¡¨ç°å¹³å‡æ¯”ç°æœ‰ç«äº‰æ–¹æ³•é«˜å‡º 15%ã€‚è¿™ä¸€å‘ç°å……åˆ†å±•ç¤ºäº†å°†è¯´æœé€»è¾‘èå…¥æ¨ç†è¿‡ç¨‹åœ¨å¼ºåŒ– zero-shot è™šå‡ä¿¡æ¯æ£€æµ‹ä¸­çš„é‡è¦ä»·å€¼ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "Accepted to ACL 2025 Main Conference",
      "pdf_url": "https://arxiv.org/pdf/2506.06842v2",
      "published_date": "2025-06-07 15:46:02 UTC",
      "updated_date": "2026-01-08 13:25:34 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T19:44:07.542598+00:00"
    },
    {
      "arxiv_id": "2506.06840v1",
      "title": "A Statistical Framework for Model Selection in LSTM Networks",
      "title_zh": "LSTM ç½‘ç»œæ¨¡å‹é€‰æ‹©çš„ç»Ÿè®¡æ¡†æ¶",
      "authors": [
        "Fahad Mostafa"
      ],
      "abstract": "Long Short-Term Memory (LSTM) neural network models have become the cornerstone for sequential data modeling in numerous applications, ranging from natural language processing to time series forecasting. Despite their success, the problem of model selection, including hyperparameter tuning, architecture specification, and regularization choice remains largely heuristic and computationally expensive. In this paper, we propose a unified statistical framework for systematic model selection in LSTM networks. Our framework extends classical model selection ideas, such as information criteria and shrinkage estimation, to sequential neural networks. We define penalized likelihoods adapted to temporal structures, propose a generalized threshold approach for hidden state dynamics, and provide efficient estimation strategies using variational Bayes and approximate marginal likelihood methods. Several biomedical data centric examples demonstrate the flexibility and improved performance of the proposed framework.",
      "tldr_zh": "æœ¬ç ”ç©¶é’ˆå¯¹LSTMç½‘ç»œä¸­æ¨¡å‹é€‰æ‹©ï¼ˆåŒ…æ‹¬è¶…å‚æ•°è°ƒæ•´ã€æ¶æ„è§„èŒƒå’Œæ­£åˆ™åŒ–é€‰æ‹©ï¼‰ä¸»è¦ä¾èµ–å¯å‘å¼æ–¹æ³•ä¸”è®¡ç®—æˆæœ¬é«˜æ˜‚çš„é—®é¢˜ï¼Œæå‡ºäº†ä¸€ä¸ªç»Ÿä¸€çš„ç»Ÿè®¡æ¡†æ¶ã€‚è¯¥æ¡†æ¶å°†ä¿¡æ¯å‡†åˆ™ï¼ˆinformation criteriaï¼‰å’Œæ”¶ç¼©ä¼°è®¡ï¼ˆshrinkage estimationï¼‰ç­‰ç»å…¸æ¨¡å‹é€‰æ‹©æ€æƒ³æ‰©å±•åˆ°åºåˆ—ç¥ç»ç½‘ç»œä¸­ã€‚é€šè¿‡å®šä¹‰é€‚åº”æ—¶é—´ç»“æ„çš„æƒ©ç½šä¼¼ç„¶ï¼ˆpenalized likelihoodsï¼‰ï¼Œå¹¶é’ˆå¯¹éšè—çŠ¶æ€åŠ¨åŠ›å­¦ï¼ˆhidden state dynamicsï¼‰æå‡ºå¹¿ä¹‰é˜ˆå€¼æ–¹æ³•ï¼Œå®ç°äº†ç³»ç»ŸåŒ–çš„æ¨¡å‹é€‰æ‹©ã€‚åŒæ—¶ï¼Œç ”ç©¶åˆ©ç”¨å˜åˆ†è´å¶æ–¯ï¼ˆvariational Bayesï¼‰å’Œè¿‘ä¼¼è¾¹ç¼˜ä¼¼ç„¶æ–¹æ³•ï¼ˆapproximate marginal likelihood methodsï¼‰æä¾›äº†é«˜æ•ˆçš„ä¼°è®¡ç­–ç•¥ã€‚åœ¨ç”Ÿç‰©åŒ»å­¦ä¸­å¿ƒæ•°æ®ä¸Šçš„å¤šé¡¹ç¤ºä¾‹è¯æ˜äº†è¯¥æ¡†æ¶çš„çµæ´»æ€§ï¼Œå¹¶æœ‰æ•ˆæå‡äº†LSTMç½‘ç»œçš„é¢„æµ‹æ€§èƒ½ã€‚",
      "categories": [
        "stat.ML",
        "cs.AI",
        "cs.LG",
        "stat.AP"
      ],
      "primary_category": "stat.ML",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.06840v1",
      "published_date": "2025-06-07 15:44:27 UTC",
      "updated_date": "2025-06-07 15:44:27 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T19:43:58.291980+00:00"
    },
    {
      "arxiv_id": "2506.06837v3",
      "title": "AI-Generated Compromises for Coalition Formation",
      "title_zh": "é¢å‘è”ç›Ÿå½¢æˆçš„äººå·¥æ™ºèƒ½ç”ŸæˆæŠ˜ä¸­æ–¹æ¡ˆ",
      "authors": [
        "Eyal Briman",
        "Ehud Shapiro",
        "Nimrod Talmon"
      ],
      "abstract": "The challenge of finding compromises between agent proposals is fundamental to AI subfields such as argumentation, mediation, and negotiation. Building on this tradition, Elkind et al. (2021) introduced a process for coalition formation that seeks majority-supported proposals preferable to the status quo, using a metric space where each agent has an ideal point. A crucial step in this process involves identifying compromise proposals around which agent coalitions can unite. How to effectively find such compromise proposals remains an open question. We address this gap by formalizing a model that incorporates agent bounded rationality and uncertainty, and by developing AI methods to generate compromise proposals. We focus on the domain of collaborative document writing, such as the democratic drafting of a community constitution. Our approach uses natural language processing techniques and large language models to induce a semantic metric space over text. Based on this space, we design algorithms to suggest compromise points likely to receive broad support. To evaluate our methods, we simulate coalition formation processes and show that AI can facilitate large-scale democratic text editing, a domain where traditional tools are limited.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†åœ¨è”ç›Ÿå½¢æˆï¼ˆCoalition Formationï¼‰è¿‡ç¨‹ä¸­å¯»æ‰¾ä»£ç†ææ¡ˆé—´æŠ˜ä¸­æ–¹æ¡ˆï¼ˆCompromisesï¼‰çš„æŒ‘æˆ˜ï¼Œæ—¨åœ¨è§£å†³å¦‚ä½•æœ‰æ•ˆè¯†åˆ«èƒ½è·å¾—å¤šæ•°æ”¯æŒçš„ææ¡ˆè¿™ä¸€å¼€æ”¾æ€§é—®é¢˜ã€‚ç ”ç©¶è€…é€šè¿‡å½¢å¼åŒ–ä¸€ä¸ªç»“åˆä»£ç†äººæœ‰é™ç†æ€§ï¼ˆBounded Rationalityï¼‰å’Œä¸ç¡®å®šæ€§çš„æ¨¡å‹ï¼Œå¼€å‘äº†åˆ©ç”¨è‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆNLPï¼‰å’Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ç”ŸæˆæŠ˜ä¸­ææ¡ˆçš„ AI æ–¹æ³•ã€‚è¯¥æ–¹æ³•é‡ç‚¹åº”ç”¨äºåä½œå¼æ–‡æ¡£ç¼–å†™ï¼ˆCollaborative Document Writingï¼‰é¢†åŸŸï¼Œä¾‹å¦‚ç¤¾åŒºå®ªæ³•çš„æ°‘ä¸»èµ·è‰ï¼Œé€šè¿‡åœ¨æ–‡æœ¬ä¸Šè¯±å¯¼å‡ºè¯­ä¹‰åº¦é‡ç©ºé—´ï¼ˆSemantic Metric Spaceï¼‰å¹¶è®¾è®¡ç‰¹å®šç®—æ³•æ¥å»ºè®®æŠ˜ä¸­ç‚¹ã€‚æ¨¡æ‹Ÿå®éªŒç»“æœè¡¨æ˜ï¼ŒAI èƒ½å¤Ÿæœ‰æ•ˆä¿ƒè¿›å¤§è§„æ¨¡æ°‘ä¸»æ–‡æœ¬ç¼–è¾‘ï¼Œåœ¨ä¼ ç»Ÿå·¥å…·å—é™çš„å¤æ‚é¢†åŸŸå±•ç°äº†æ˜¾è‘—çš„è¾…åŠ©æ½œåŠ›ã€‚è¿™é¡¹å·¥ä½œä¸ºå¤šæ™ºèƒ½ä½“ç¯å¢ƒä¸‹çš„åå•†ã€è°ƒè§£å’Œå…±è¯†è¾¾æˆæä¾›äº†ç³»ç»ŸåŒ–çš„æŠ€æœ¯è·¯å¾„ã€‚",
      "categories": [
        "cs.MA",
        "cs.AI",
        "cs.GT"
      ],
      "primary_category": "cs.MA",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.06837v3",
      "published_date": "2025-06-07 15:28:27 UTC",
      "updated_date": "2025-08-09 18:00:01 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T19:43:59.188656+00:00"
    },
    {
      "arxiv_id": "2506.06836v2",
      "title": "Harnessing Vision-Language Models for Time Series Anomaly Detection",
      "title_zh": "åˆ©ç”¨è§†è§‰è¯­è¨€æ¨¡å‹è¿›è¡Œæ—¶é—´åºåˆ—å¼‚å¸¸æ£€æµ‹",
      "authors": [
        "Zelin He",
        "Sarah Alnegheimish",
        "Matthew Reimherr"
      ],
      "abstract": "Time-series anomaly detection (TSAD) has played a vital role in a variety of fields, including healthcare, finance, and sensor-based condition monitoring. Prior methods, which mainly focus on training domain-specific models on numerical data, lack the visual-temporal understanding capacity that human experts have to identify contextual anomalies. To fill this gap, we explore a solution based on vision language models (VLMs). Recent studies have shown the ability of VLMs for visual understanding tasks, yet their direct application to time series has fallen short on both accuracy and efficiency. To harness the power of VLMs for TSAD, we propose a two-stage solution, with (1) ViT4TS, a vision-screening stage built on a relatively lightweight pre-trained vision encoder, which leverages 2D time series representations to accurately localize candidate anomalies; (2) VLM4TS, a VLM-based stage that integrates global temporal context and VLM's visual understanding capacity to refine the detection upon the candidates provided by ViT4TS. We show that without any time-series training, VLM4TS outperforms time-series pre-trained and from-scratch baselines in most cases, yielding a 24.6% improvement in F1-max score over the best baseline. Moreover, VLM4TS also consistently outperforms existing language model-based TSAD methods and is on average 36x more efficient in token usage.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†åˆ©ç”¨è§†è§‰è¯­è¨€æ¨¡å‹(VLMs)è¿›è¡Œæ—¶é—´åºåˆ—å¼‚å¸¸æ£€æµ‹(TSAD)ï¼Œæ—¨åœ¨è§£å†³ä¼ ç»Ÿæ•°å€¼æ¨¡å‹åœ¨æ•æ‰è§†è§‰-æ—¶é—´ä¸Šä¸‹æ–‡ç†è§£èƒ½åŠ›æ–¹é¢çš„å±€é™ã€‚ç ”ç©¶è€…æå‡ºäº†ä¸€ç§ä¸¤é˜¶æ®µè§£å†³æ–¹æ¡ˆï¼Œé¦–å…ˆé€šè¿‡ViT4TSé˜¶æ®µåˆ©ç”¨è½»é‡çº§é¢„è®­ç»ƒè§†è§‰ç¼–ç å™¨å’Œ2Dæ—¶é—´åºåˆ—è¡¨ç¤ºå®šä½å€™é€‰å¼‚å¸¸ï¼Œéšååœ¨VLM4TSé˜¶æ®µæ•´åˆå…¨å±€æ—¶é—´ä¸Šä¸‹æ–‡ï¼Œåˆ©ç”¨è§†è§‰è¯­è¨€æ¨¡å‹çš„è§†è§‰ç†è§£åŠ›å¯¹æ£€æµ‹ç»“æœè¿›è¡Œç²¾ç»†åŒ–ä¿®æ­£ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œåœ¨æ— éœ€é’ˆå¯¹æ—¶é—´åºåˆ—è¿›è¡Œç‰¹å®šè®­ç»ƒçš„æƒ…å†µä¸‹ï¼ŒVLM4TSåœ¨å¤šæ•°åœºæ™¯ä¸‹ä¼˜äºç°æœ‰çš„é¢„è®­ç»ƒåŠä»é›¶å¼€å§‹è®­ç»ƒçš„åŸºçº¿æ¨¡å‹ï¼Œå…¶F1-maxåˆ†æ•°æå‡äº†24.6%ã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•åœ¨æ€§èƒ½ä¸ŠæŒç»­è¶…è¶Šç°æœ‰çš„åŸºäºè¯­è¨€æ¨¡å‹çš„TSADæ–¹æ³•ï¼Œä¸”åœ¨Tokenä½¿ç”¨æ•ˆç‡ä¸Šå¹³å‡æå‡äº†36å€ï¼Œå±•ç°äº†è§†è§‰è¯­è¨€æ¨¡å‹åœ¨å·¥ä¸šç›‘æµ‹åŠåŒ»ç–—ç­‰é¢†åŸŸçš„åº”ç”¨æ½œåŠ›ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "Accepted at AAAI 2026 (Oral)",
      "pdf_url": "https://arxiv.org/pdf/2506.06836v2",
      "published_date": "2025-06-07 15:27:30 UTC",
      "updated_date": "2025-11-25 14:56:02 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T19:44:03.090042+00:00"
    },
    {
      "arxiv_id": "2506.06832v2",
      "title": "Cross-Entropy Games for Language Models: From Implicit Knowledge to General Capability Measures",
      "title_zh": "è¯­è¨€æ¨¡å‹çš„äº¤å‰ç†µåšå¼ˆï¼šä»éšæ€§çŸ¥è¯†åˆ°é€šç”¨èƒ½åŠ›åº¦é‡",
      "authors": [
        "ClÃ©ment Hongler",
        "Andrew Emil"
      ],
      "abstract": "Large Language Models (LLMs) define probability measures on text. By considering the implicit knowledge question of what it means for an LLM to know such a measure and what it entails algorithmically, we are naturally led to formulate a series of tasks that go beyond generative sampling, involving forms of summarization, counterfactual thinking, anomaly detection, originality search, reverse prompting, debating, creative solving, etc. These tasks can be formulated as games based on LLM measures, which we call Cross-Entropy (Xent) Games. Xent Games can be single-player or multi-player. They involve cross-entropy scores and cross-entropy constraints, and can be expressed as simple computational graphs and programs. We show the Xent Game space is large enough to contain a wealth of interesting examples, while being constructible from basic game-theoretic consistency axioms. We then discuss how the Xent Game space can be used to measure the abilities of LLMs. This leads to the construction of Xent Game measures: finite families of Xent Games that can be used as capability benchmarks, built from a given scope, by extracting a covering measure. To address the unbounded scope problem associated with the challenge of measuring general abilities, we propose to explore the space of Xent Games in a coherent fashion, using ideas inspired by evolutionary dynamics.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å¦‚ä½•å®šä¹‰æ–‡æœ¬ä¸Šçš„æ¦‚ç‡åº¦é‡ï¼Œå¹¶åŸºäºæ­¤æå‡ºäº†ä¸€ç³»åˆ—è¢«ç§°ä¸ºäº¤å‰ç†µåšå¼ˆï¼ˆCross-Entropy Gamesï¼Œç®€ç§° Xent Gamesï¼‰çš„ä»»åŠ¡ã€‚è¿™äº›åšå¼ˆæ¶µç›–äº†æ‘˜è¦ç”Ÿæˆã€åäº‹å®æ€ç»´ã€å¼‚å¸¸æ£€æµ‹å’Œé€†å‘æç¤ºç­‰å¤šç§è¶…è¶Šä¼ ç»Ÿç”Ÿæˆé‡‡æ ·çš„å½¢å¼ï¼Œä¸”æ”¯æŒå•äººæˆ–å¤šäººæ¨¡å¼ã€‚é€šè¿‡å°†åšå¼ˆè¡¨è¾¾ä¸ºè®¡ç®—å›¾å’Œç¨‹åºï¼Œç ”ç©¶è¯æ˜äº† Xent Game ç©ºé—´ä¸ä»…ç¬¦åˆåšå¼ˆè®ºçš„ä¸€è‡´æ€§å…¬ç†ï¼Œä¸”è¶³ä»¥æ¶µç›–ä¸°å¯Œçš„åº”ç”¨åœºæ™¯ã€‚ä½œè€…è¿›ä¸€æ­¥é˜è¿°äº†å¦‚ä½•åˆ©ç”¨ Xent Game ç©ºé—´æ¥è¡¡é‡ LLMs çš„å„é¡¹èƒ½åŠ›ï¼Œå¹¶ä»¥æ­¤æ„å»ºèƒ½åŠ›åŸºå‡†æµ‹è¯•ï¼ˆcapability benchmarksï¼‰ã€‚é’ˆå¯¹é€šç”¨èƒ½åŠ›è¯„ä¼°ä¸­å­˜åœ¨çš„æ— é™èŒƒå›´æŒ‘æˆ˜ï¼Œç ”ç©¶æå‡ºå€Ÿé‰´æ¼”åŒ–åŠ¨åŠ›å­¦ï¼ˆevolutionary dynamicsï¼‰çš„æ€æƒ³ï¼Œä»¥ç³»ç»ŸåŒ–çš„æ–¹å¼æ¢ç´¢åšå¼ˆç©ºé—´ã€‚",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.GT",
        "cs.IT",
        "cs.NE"
      ],
      "primary_category": "cs.AI",
      "comment": "42 pages, 16 figures",
      "pdf_url": "https://arxiv.org/pdf/2506.06832v2",
      "published_date": "2025-06-07 15:25:10 UTC",
      "updated_date": "2025-06-22 12:08:32 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T19:44:11.112665+00:00"
    },
    {
      "arxiv_id": "2506.06830v1",
      "title": "EndoARSS: Adapting Spatially-Aware Foundation Model for Efficient Activity Recognition and Semantic Segmentation in Endoscopic Surgery",
      "title_zh": "EndoARSSï¼šé¢å‘å†…çª¥é•œæ‰‹æœ¯é«˜æ•ˆæ´»åŠ¨è¯†åˆ«ä¸è¯­ä¹‰åˆ†å‰²çš„ç©ºé—´æ„ŸçŸ¥åŸºç¡€æ¨¡å‹é€‚é…",
      "authors": [
        "Guankun Wang",
        "Rui Tang",
        "Mengya Xu",
        "Long Bai",
        "Huxin Gao",
        "Hongliang Ren"
      ],
      "abstract": "Endoscopic surgery is the gold standard for robotic-assisted minimally invasive surgery, offering significant advantages in early disease detection and precise interventions. However, the complexity of surgical scenes, characterized by high variability in different surgical activity scenarios and confused image features between targets and the background, presents challenges for surgical environment understanding. Traditional deep learning models often struggle with cross-activity interference, leading to suboptimal performance in each downstream task. To address this limitation, we explore multi-task learning, which utilizes the interrelated features between tasks to enhance overall task performance. In this paper, we propose EndoARSS, a novel multi-task learning framework specifically designed for endoscopy surgery activity recognition and semantic segmentation. Built upon the DINOv2 foundation model, our approach integrates Low-Rank Adaptation to facilitate efficient fine-tuning while incorporating Task Efficient Shared Low-Rank Adapters to mitigate gradient conflicts across diverse tasks. Additionally, we introduce the Spatially-Aware Multi-Scale Attention that enhances feature representation discrimination by enabling cross-spatial learning of global information. In order to evaluate the effectiveness of our framework, we present three novel datasets, MTLESD, MTLEndovis and MTLEndovis-Gen, tailored for endoscopic surgery scenarios with detailed annotations for both activity recognition and semantic segmentation tasks. Extensive experiments demonstrate that EndoARSS achieves remarkable performance across multiple benchmarks, significantly improving both accuracy and robustness in comparison to existing models. These results underscore the potential of EndoARSS to advance AI-driven endoscopic surgical systems, offering valuable insights for enhancing surgical safety and efficiency.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å†…çª¥é•œæ‰‹æœ¯ä¸­å¤æ‚çš„åœºæ™¯å˜åŒ–ã€ç›®æ ‡ä¸èƒŒæ™¯ç‰¹å¾æ··æ·†ä»¥åŠè·¨æ´»åŠ¨å¹²æ‰°ç­‰æŒ‘æˆ˜ï¼Œæå‡ºäº†åä¸ºEndoARSSçš„å¤šä»»åŠ¡å­¦ä¹ æ¡†æ¶ã€‚è¯¥æ¡†æ¶ä»¥DINOv2åŸºç¡€æ¨¡å‹ä¸ºæ ¸å¿ƒï¼Œæ—¨åœ¨åŒæ—¶é«˜æ•ˆæ‰§è¡Œæ‰‹æœ¯æ´»åŠ¨è¯†åˆ«(Activity Recognition)å’Œè¯­ä¹‰åˆ†å‰²(Semantic Segmentation)ä»»åŠ¡ã€‚ä¸ºäº†è§£å†³ä»»åŠ¡é—´çš„æ¢¯åº¦å†²çªå¹¶å®ç°é«˜æ•ˆå¾®è°ƒï¼ŒEndoARSSé›†æˆäº†ä½ç§©è‡ªé€‚åº”(Low-Rank Adaptation)å’Œä»»åŠ¡é«˜æ•ˆå…±äº«ä½ç§©é€‚é…å™¨(Task Efficient Shared Low-Rank Adapters)ã€‚æ­¤å¤–ï¼Œç ”ç©¶å¼•å…¥äº†ç©ºé—´æ„ŸçŸ¥å¤šå°ºåº¦æ³¨æ„åŠ›æœºåˆ¶(Spatially-Aware Multi-Scale Attention)ï¼Œé€šè¿‡è·¨ç©ºé—´å­¦ä¹ å…¨å±€ä¿¡æ¯æ¥å¢å¼ºç‰¹å¾çš„è¾¨åˆ«èƒ½åŠ›ã€‚ä¸ºäº†è¯„ä¼°æ¡†æ¶æ€§èƒ½ï¼Œç ”ç©¶è€…è¿˜å‘å¸ƒäº†MTLESDã€MTLEndovisåŠMTLEndovis-Genä¸‰ä¸ªå…·æœ‰è¯¦ç»†æ ‡æ³¨çš„æ–°å‹å†…çª¥é•œæ‰‹æœ¯æ•°æ®é›†ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒEndoARSSåœ¨å¤šé¡¹åŸºå‡†æµ‹è¯•ä¸­å‡å–å¾—äº†æ˜¾è‘—çš„å‡†ç¡®æ€§å’Œé²æ£’æ€§æå‡ï¼Œä¸ºæ„å»ºæ›´å®‰å…¨ã€é«˜æ•ˆçš„äººå·¥æ™ºèƒ½é©±åŠ¨å†…çª¥é•œæ‰‹æœ¯ç³»ç»Ÿå¥ å®šäº†åŸºç¡€ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "Accepted by Advanced Intelligent Systems",
      "pdf_url": "https://arxiv.org/pdf/2506.06830v1",
      "published_date": "2025-06-07 15:18:43 UTC",
      "updated_date": "2025-06-07 15:18:43 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T19:44:18.503244+00:00"
    },
    {
      "arxiv_id": "2506.06826v1",
      "title": "Controllable Coupled Image Generation via Diffusion Models",
      "title_zh": "åŸºäºæ‰©æ•£æ¨¡å‹çš„å¯æ§è€¦åˆå›¾åƒç”Ÿæˆ",
      "authors": [
        "Chenfei Yuan",
        "Nanshan Jia",
        "Hangqi Li",
        "Peter W. Glynn",
        "Zeyu Zheng"
      ],
      "abstract": "We provide an attention-level control method for the task of coupled image generation, where \"coupled\" means that multiple simultaneously generated images are expected to have the same or very similar backgrounds. While backgrounds coupled, the centered objects in the generated images are still expected to enjoy the flexibility raised from different text prompts. The proposed method disentangles the background and entity components in the model's cross-attention modules, attached with a sequence of time-varying weight control parameters depending on the time step of sampling. We optimize this sequence of weight control parameters with a combined objective that assesses how coupled the backgrounds are as well as text-to-image alignment and overall visual quality. Empirical results demonstrate that our method outperforms existing approaches across these criteria.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹åŸºäºæ‰©æ•£æ¨¡å‹ï¼ˆDiffusion Modelsï¼‰çš„è€¦åˆå›¾åƒç”Ÿæˆï¼ˆcoupled image generationï¼‰ä»»åŠ¡ï¼Œæå‡ºäº†ä¸€ç§æ³¨æ„åŠ›å±‚çº§çš„æ§åˆ¶æ–¹æ³•ï¼Œæ—¨åœ¨ä½¿å¤šå¼ ç”Ÿæˆçš„å›¾åƒä¿æŒç›¸åŒæˆ–é«˜åº¦ç›¸ä¼¼çš„èƒŒæ™¯ã€‚è¯¥æ–¹æ³•çš„æ ¸å¿ƒåœ¨äºäº¤å‰æ³¨æ„åŠ›ï¼ˆcross-attentionï¼‰æ¨¡å—ä¸­è§£è€¦èƒŒæ™¯ä¸å®ä½“ç»„ä»¶ï¼Œå¹¶å¼•å…¥ä¸€ç³»åˆ—éšé‡‡æ ·æ—¶é—´æ­¥å˜åŒ–çš„æƒé‡æ§åˆ¶å‚æ•°ã€‚ç ”ç©¶äººå‘˜è®¾è®¡äº†ä¸€ä¸ªç»¼åˆç›®æ ‡å‡½æ•°æ¥ä¼˜åŒ–è¿™äº›å‚æ•°ï¼Œä»¥åŒæ—¶è¯„ä¼°èƒŒæ™¯çš„è€¦åˆç¨‹åº¦ã€æ–‡æœ¬ä¸å›¾åƒçš„å¯¹é½æ€§ä»¥åŠæœ€ç»ˆçš„è§†è§‰è´¨é‡ã€‚è¿™ç§æ§åˆ¶æœºåˆ¶å…è®¸åœ¨ä¿æŒèƒŒæ™¯ä¸€è‡´çš„åŒæ—¶ï¼Œè®©ç”Ÿæˆçš„ä¸­å¿ƒç‰©ä½“æ ¹æ®ä¸åŒçš„æ–‡æœ¬æç¤ºï¼ˆtext promptsï¼‰ä¿æŒçµæ´»çš„å˜åŒ–ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å¤šé¡¹å…³é”®æŒ‡æ ‡ä¸Šå‡ä¼˜äºç°æœ‰çš„æŠ€æœ¯æ‰‹æ®µï¼Œå®ç°äº†æ›´ç²¾å‡†ä¸”é«˜è´¨é‡çš„èƒŒæ™¯ä¸€è‡´æ€§æ§åˆ¶ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.06826v1",
      "published_date": "2025-06-07 15:09:08 UTC",
      "updated_date": "2025-06-07 15:09:08 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T19:44:42.590984+00:00"
    },
    {
      "arxiv_id": "2506.06823v1",
      "title": "Exploring Visual Prompting: Robustness Inheritance and Beyond",
      "title_zh": "æ¢ç´¢è§†è§‰æç¤ºï¼šé²æ£’æ€§ç»§æ‰¿åŠå…¶è¶…è¶Š",
      "authors": [
        "Qi Li",
        "Liangzhi Li",
        "Zhouqiang Jiang",
        "Bowen Wang",
        "Keke Tang"
      ],
      "abstract": "Visual Prompting (VP), an efficient method for transfer learning, has shown its potential in vision tasks. However, previous works focus exclusively on VP from standard source models, it is still unknown how it performs under the scenario of a robust source model: Can the robustness of the source model be successfully inherited? Does VP also encounter the same trade-off between robustness and generalization ability as the source model during this process? If such a trade-off exists, is there a strategy specifically tailored to VP to mitigate this limitation? In this paper, we thoroughly explore these three questions for the first time and provide affirmative answers to them. To mitigate the trade-off faced by VP, we propose a strategy called Prompt Boundary Loosening (PBL). As a lightweight, plug-and-play strategy naturally compatible with VP, PBL effectively ensures the successful inheritance of robustness when the source model is a robust model, while significantly enhancing VP's generalization ability across various downstream datasets. Extensive experiments across various datasets show that our findings are universal and demonstrate the significant benefits of the proposed strategy.",
      "tldr_zh": "è¯¥ç ”ç©¶æ·±å…¥æ¢è®¨äº†è§†è§‰æç¤º(Visual Prompting, VP)åœ¨é²æ£’æºæ¨¡å‹æƒ…å¢ƒä¸‹çš„è¡¨ç°ï¼Œé‡ç‚¹ç ”ç©¶äº†é²æ£’æ€§ç»§æ‰¿åŠå…¶ä¸æ³›åŒ–èƒ½åŠ›ä¹‹é—´çš„æƒè¡¡é—®é¢˜ã€‚ä½œè€…è¯å®äº†Visual Promptingèƒ½å¤ŸæˆåŠŸç»§æ‰¿æºæ¨¡å‹çš„é²æ£’æ€§ï¼Œä½†ä¹Ÿå‘ç°å…¶åŒæ ·é¢ä¸´é²æ£’æ€§ä¸æ³›åŒ–èƒ½åŠ›ä¹‹é—´çš„æƒè¡¡æŒ‘æˆ˜ã€‚ä¸ºäº†ç¼“è§£è¿™ä¸€é™åˆ¶ï¼Œç ”ç©¶æå‡ºäº†ä¸€ç§åä¸ºæç¤ºè¾¹ç•Œæ¾å¼›(Prompt Boundary Loosening, PBL)çš„è½»é‡çº§ã€å³æ’å³ç”¨ç­–ç•¥ã€‚PBLèƒ½å¤Ÿåœ¨ç¡®ä¿é²æ£’æ€§æˆåŠŸç»§æ‰¿çš„åŒæ—¶ï¼Œæ˜¾è‘—å¢å¼ºVisual Promptingåœ¨å„ç§ä¸‹æ¸¸æ•°æ®é›†ä¸Šçš„æ³›åŒ–èƒ½åŠ›ã€‚è·¨å¤šä¸ªæ•°æ®é›†çš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼Œè¯¥å‘ç°å…·æœ‰æ™®éæ„ä¹‰ï¼Œå¹¶éªŒè¯äº†æ‰€æç­–ç•¥åœ¨æå‡æ¨¡å‹ç»¼åˆæ€§èƒ½æ–¹é¢çš„æ˜¾è‘—ä¼˜åŠ¿ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "arXiv admin note: substantial text overlap with arXiv:2311.10992",
      "pdf_url": "https://arxiv.org/pdf/2506.06823v1",
      "published_date": "2025-06-07 14:56:32 UTC",
      "updated_date": "2025-06-07 14:56:32 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T19:44:33.764948+00:00"
    },
    {
      "arxiv_id": "2506.06822v1",
      "title": "Hi-LSplat: Hierarchical 3D Language Gaussian Splatting",
      "title_zh": "Hi-LSplatï¼šå±‚çº§ 3D è¯­è¨€é«˜æ–¯æº…å°„",
      "authors": [
        "Chenlu Zhan",
        "Yufei Zhang",
        "Gaoang Wang",
        "Hongwei Wang"
      ],
      "abstract": "Modeling 3D language fields with Gaussian Splatting for open-ended language queries has recently garnered increasing attention. However, recent 3DGS-based models leverage view-dependent 2D foundation models to refine 3D semantics but lack a unified 3D representation, leading to view inconsistencies. Additionally, inherent open-vocabulary challenges cause inconsistencies in object and relational descriptions, impeding hierarchical semantic understanding. In this paper, we propose Hi-LSplat, a view-consistent Hierarchical Language Gaussian Splatting work for 3D open-vocabulary querying. To achieve view-consistent 3D hierarchical semantics, we first lift 2D features to 3D features by constructing a 3D hierarchical semantic tree with layered instance clustering, which addresses the view inconsistency issue caused by 2D semantic features. Besides, we introduce instance-wise and part-wise contrastive losses to capture all-sided hierarchical semantic representations. Notably, we construct two hierarchical semantic datasets to better assess the model's ability to distinguish different semantic levels. Extensive experiments highlight our method's superiority in 3D open-vocabulary segmentation and localization. Its strong performance on hierarchical semantic datasets underscores its ability to capture complex hierarchical semantics within 3D scenes.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹åŸºäº 3D Gaussian Splatting (3DGS) çš„è¯­è¨€åœºå»ºæ¨¡ä¸­å­˜åœ¨çš„è§†å›¾ä¸ä¸€è‡´ä»¥åŠå±‚çº§è¯­ä¹‰ç†è§£ä¸è¶³ç­‰æŒ‘æˆ˜ï¼Œæå‡ºäº†åä¸º Hi-LSplat çš„å±‚çº§è¯­è¨€é«˜æ–¯æ³¼æº…æ¡†æ¶ã€‚ä¸ºäº†å®ç°è§†å›¾ä¸€è‡´çš„ 3D å±‚çº§è¯­ä¹‰ï¼Œè¯¥æ–¹æ³•é€šè¿‡åˆ†å±‚å®ä¾‹èšç±»æ„å»º 3D å±‚æ¬¡è¯­ä¹‰æ ‘ (3D hierarchical semantic tree)ï¼Œå°† 2D ç‰¹å¾æå‡è‡³ 3D ç©ºé—´ï¼Œæœ‰æ•ˆè§£å†³äº†ç”± 2D è¯­ä¹‰ç‰¹å¾å¯¼è‡´çš„è§†å›¾ä¸ä¸€è‡´é—®é¢˜ã€‚æ­¤å¤–ï¼Œç ”ç©¶å¼•å…¥äº†å®ä¾‹çº§ (instance-wise) å’Œéƒ¨ä»¶çº§ (part-wise) çš„å¯¹æ¯”æŸå¤± (contrastive losses)ï¼Œä»¥æ•æ‰å…¨æ–¹ä½çš„å±‚çº§è¯­ä¹‰è¡¨ç¤ºã€‚ä½œè€…è¿˜æ„å»ºäº†ä¸¤ä¸ªæ–°çš„å±‚çº§è¯­ä¹‰æ•°æ®é›†ï¼Œç”¨ä»¥è¯„ä¼°æ¨¡å‹åœ¨åŒºåˆ†ä¸åŒè¯­ä¹‰å±‚çº§æ–¹é¢çš„æ€§èƒ½ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒHi-LSplat åœ¨ 3D å¼€æ”¾è¯æ±‡ (open-vocabulary) åˆ†å‰²å’Œå®šä½ä»»åŠ¡ä¸­å‡è¡¨ç°å‡ºæ˜¾è‘—ä¼˜è¶Šæ€§ã€‚è¯¥æˆæœè¯æ˜äº†å…¶åœ¨ 3D åœºæ™¯ä¸­æ•æ‰å’Œç†è§£å¤æ‚å±‚çº§è¯­ä¹‰çš„å¼ºå¤§èƒ½åŠ›ï¼Œä¸ºå®ç°æ›´ç²¾å‡†çš„ 3D ç©ºé—´ç†è§£å¥ å®šäº†åŸºç¡€ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.06822v1",
      "published_date": "2025-06-07 14:56:19 UTC",
      "updated_date": "2025-06-07 14:56:19 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T19:44:42.993849+00:00"
    },
    {
      "arxiv_id": "2506.06821v4",
      "title": "Can LLMs Generate Reliable Test Case Generators? A Study on Competition-Level Programming Problems",
      "title_zh": "LLM èƒ½å¦ç”Ÿæˆå¯é çš„æµ‹è¯•ç”¨ä¾‹ç”Ÿæˆå™¨ï¼Ÿä¸€é¡¹é’ˆå¯¹ç«èµ›çº§ç¼–ç¨‹é—®é¢˜çš„ç ”ç©¶",
      "authors": [
        "Yuhan Cao",
        "Zian Chen",
        "Kun Quan",
        "Ziliang Zhang",
        "Yu Wang",
        "Xiaoning Dong",
        "Yeqi Feng",
        "Guanzhong He",
        "Jingcheng Huang",
        "Jianhao Li",
        "Yixuan Tan",
        "Jiafu Tang",
        "Yilin Tang",
        "Junlei Wu",
        "Qianyu Xiao",
        "Can Zheng",
        "Shouchen Zhou",
        "Yuxiang Zhu",
        "Yiming Huang",
        "Tianxing He"
      ],
      "abstract": "Large Language Models (LLMs) have demonstrated remarkable capabilities in code generation, capable of tackling complex tasks during inference. However, the extent to which LLMs can be utilized for code checking or debugging through test case generation remains largely unexplored. We investigate this problem from the perspective of competition-level programming (CP) programs and propose TCGBench, a Benchmark for (LLM generation of) Test Case Generators. This benchmark comprises two tasks, aimed at studying the capabilities of LLMs in (1) generating valid test case generators for a given CP problem, and further (2) generating targeted test case generators that expose bugs in human-written code. Experimental results indicate that while state-of-the-art LLMs can generate valid test case generators in most cases, most LLMs struggle to generate targeted test cases that reveal flaws in human code effectively. Especially, even advanced reasoning models (e.g., o3-mini) fall significantly short of human performance in the task of generating targeted generators. Furthermore, we construct a high-quality, manually curated dataset of instructions for generating targeted generators. Analysis demonstrates that the performance of LLMs can be enhanced with the aid of this dataset, by both prompting and fine-tuning.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†å¤§è¯­è¨€æ¨¡å‹(LLMs)åœ¨ç«èµ›çº§ç¼–ç¨‹(Competition-Level Programming)ä¸­ç”Ÿæˆæµ‹è¯•ç”¨ä¾‹ç”Ÿæˆå™¨(Test Case Generators)çš„å¯é æ€§ã€‚ä½œè€…æå‡ºäº†TCGBenchåŸºå‡†æµ‹è¯•ï¼ŒåŒ…å«ç”Ÿæˆé€šç”¨æœ‰æ•ˆç”Ÿæˆå™¨å’Œç”Ÿæˆé’ˆå¯¹æ€§ç”Ÿæˆå™¨ï¼ˆç”¨äºæš´éœ²äººç±»ç¼–å†™ä»£ç ä¸­çš„æ¼æ´ï¼‰ä¸¤é¡¹ä»»åŠ¡ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œè™½ç„¶å½“å‰å…ˆè¿›çš„LLMsåœ¨å¤§å¤šæ•°æƒ…å†µä¸‹èƒ½ç”Ÿæˆæœ‰æ•ˆçš„é€šç”¨ç”Ÿæˆå™¨ï¼Œä½†åœ¨ç”Ÿæˆé’ˆå¯¹æ€§æµ‹è¯•ç”¨ä¾‹ä»¥æœ‰æ•ˆæ­ç¤ºä»£ç ç¼ºé™·æ–¹é¢ä»ç„¶é¢ä¸´æŒ‘æˆ˜ã€‚å³ä½¿æ˜¯o3-miniç­‰å…ˆè¿›çš„æ¨ç†æ¨¡å‹ï¼Œåœ¨ç”Ÿæˆé’ˆå¯¹æ€§ç”Ÿæˆå™¨çš„ä»»åŠ¡ä¸­ä¹Ÿæ˜¾è‘—é€Šè‰²äºäººç±»è¡¨ç°ã€‚ä¸ºäº†æå‡æ¨¡å‹æ€§èƒ½ï¼Œç ”ç©¶å›¢é˜Ÿæ„å»ºäº†ä¸€ä¸ªç»è¿‡äººå·¥ç²¾é€‰çš„é«˜è´¨é‡æŒ‡ä»¤æ•°æ®é›†ã€‚åˆ†æè¯æ˜ï¼Œé€šè¿‡æç¤ºè¯å·¥ç¨‹(Prompting)æˆ–å¾®è°ƒ(Fine-tuning)åˆ©ç”¨è¯¥æ•°æ®é›†ï¼Œå¯ä»¥æœ‰æ•ˆå¢å¼ºLLMsåœ¨è¿™ä¸€å¤æ‚ä»»åŠ¡ä¸­çš„è¡¨ç°ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.SE"
      ],
      "primary_category": "cs.CL",
      "comment": "37 pages, 22 figures",
      "pdf_url": "https://arxiv.org/pdf/2506.06821v4",
      "published_date": "2025-06-07 14:53:03 UTC",
      "updated_date": "2026-01-14 15:14:27 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T19:44:39.362648+00:00"
    },
    {
      "arxiv_id": "2506.06809v1",
      "title": "IMPA-HGAE:Intra-Meta-Path Augmented Heterogeneous Graph Autoencoder",
      "title_zh": "IMPA-HGAEï¼šå…ƒè·¯å¾„å†…éƒ¨å¢å¼ºçš„å¼‚æ„å›¾è‡ªåŠ¨ç¼–ç å™¨",
      "authors": [
        "Di Lin",
        "Wanjing Ren",
        "Xuanbin Li",
        "Rui Zhang"
      ],
      "abstract": "Self-supervised learning (SSL) methods have been increasingly applied to diverse downstream tasks due to their superior generalization capabilities and low annotation costs. However, most existing heterogeneous graph SSL models convert heterogeneous graphs into homogeneous ones via meta-paths for training, which only leverage information from nodes at both ends of meta-paths while underutilizing the heterogeneous node information along the meta-paths. To address this limitation, this paper proposes a novel framework named IMPA-HGAE to enhance target node embeddings by fully exploiting internal node information along meta-paths. Experimental results validate that IMPA-HGAE achieves superior performance on heterogeneous datasets. Furthermore, this paper introduce innovative masking strategies to strengthen the representational capacity of generative SSL models on heterogeneous graph data. Additionally, this paper discuss the interpretability of the proposed method and potential future directions for generative self-supervised learning in heterogeneous graphs. This work provides insights into leveraging meta-path-guided structural semantics for robust representation learning in complex graph scenarios.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¼‚æ„å›¾ (Heterogeneous graph) è‡ªç›‘ç£å­¦ä¹  (Self-supervised learning) ä¸­å…ƒè·¯å¾„ (Meta-paths) ä¸­é—´èŠ‚ç‚¹ä¿¡æ¯åˆ©ç”¨ä¸è¶³çš„é—®é¢˜ï¼Œæå‡ºäº†åä¸º IMPA-HGAE çš„æ–°å‹æ¡†æ¶ã€‚è¯¥æ¡†æ¶é€šè¿‡å……åˆ†æŒ–æ˜å…ƒè·¯å¾„å†…éƒ¨çš„èŠ‚ç‚¹ä¿¡æ¯æ¥å¢å¼ºç›®æ ‡èŠ‚ç‚¹çš„åµŒå…¥ (Node embeddings)ï¼Œæœ‰æ•ˆå…‹æœäº†ä¼ ç»Ÿæ–¹æ³•å°†å¼‚æ„å›¾åŒæ„åŒ–æ—¶ä¸¢å¤±ç»“æ„ä¿¡æ¯çš„å±€é™æ€§ã€‚æ­¤å¤–ï¼Œè®ºæ–‡è¿˜å¼•å…¥äº†åˆ›æ–°çš„æ©ç ç­–ç•¥ (Masking strategies)ï¼Œæ—¨åœ¨å¼ºåŒ–ç”Ÿæˆå¼è‡ªç›‘ç£æ¨¡å‹åœ¨å¤„ç†å¼‚æ„å›¾æ•°æ®æ—¶çš„è¡¨å¾èƒ½åŠ›ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒIMPA-HGAE åœ¨å¤šä¸ªå¼‚æ„æ•°æ®é›†ä¸Šå‡å–å¾—äº†ä¼˜äºç°æœ‰æ–¹æ³•çš„æ€§èƒ½è¡¨ç°ã€‚è¯¥ç ”ç©¶è¿˜æ·±å…¥æ¢è®¨äº†æ¨¡å‹çš„è§£é‡Šæ€§ (Interpretability) ä¸æœªæ¥ç ”ç©¶æ–¹å‘ï¼Œä¸ºå¤æ‚å›¾åœºæ™¯ä¸‹çš„é²æ£’è¡¨å¾å­¦ä¹ æä¾›äº†é‡è¦å‚è€ƒã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.06809v1",
      "published_date": "2025-06-07 14:17:30 UTC",
      "updated_date": "2025-06-07 14:17:30 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T19:44:43.842760+00:00"
    },
    {
      "arxiv_id": "2506.06808v1",
      "title": "Not quite Sherlock Holmes: Language model predictions do not reliably differentiate impossible from improbable events",
      "title_zh": "å¹¶é Sherlock Holmesï¼šè¯­è¨€æ¨¡å‹é¢„æµ‹æ— æ³•å¯é åŒºåˆ†ä¸å¯èƒ½äº‹ä»¶ä¸æä½æ¦‚ç‡äº‹ä»¶",
      "authors": [
        "James A. Michaelov",
        "Reeka Estacio",
        "Zhien Zhang",
        "Benjamin K. Bergen"
      ],
      "abstract": "Can language models reliably predict that possible events are more likely than merely improbable ones? By teasing apart possibility, typicality, and contextual relatedness, we show that despite the results of previous work, language models' ability to do this is far from robust. In fact, under certain conditions, all models tested - including Llama 3, Gemma 2, and Mistral NeMo - perform at worse-than-chance level, assigning higher probabilities to impossible sentences such as 'the car was given a parking ticket by the brake' than to merely unlikely sentences such as 'the car was given a parking ticket by the explorer'.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLanguage Modelsï¼‰æ˜¯å¦èƒ½å¯é åœ°è¾¨åˆ«å¯èƒ½å‘ç”Ÿçš„äº‹ä»¶ä¸é€»è¾‘ä¸Šä¸å¯èƒ½å‘ç”Ÿçš„äº‹ä»¶ã€‚é€šè¿‡åŒºåˆ†å¯èƒ½æ€§ï¼ˆPossibilityï¼‰ã€å…¸å‹æ€§ï¼ˆTypicalityï¼‰å’Œä¸Šä¸‹æ–‡ç›¸å…³æ€§ï¼ˆContextual Relatednessï¼‰ï¼Œç ”ç©¶äººå‘˜è¯æ˜äº†åŒ…æ‹¬ Llama 3ã€Gemma 2 å’Œ Mistral NeMo åœ¨å†…çš„å¤šç§æ¨¡å‹åœ¨è¯¥ä»»åŠ¡ä¸Šçš„è¡¨ç°å¹¶ä¸ç¨³å¥ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œåœ¨æŸäº›ç‰¹å®šæ¡ä»¶ä¸‹ï¼Œè¿™äº›æ¨¡å‹çš„è¡¨ç°ç”šè‡³ä½äºéšæœºæ¦‚ç‡ï¼Œä¼šå°†æ›´é«˜çš„é¢„æµ‹æ¦‚ç‡åˆ†é…ç»™é€»è¾‘ä¸Šä¸å¯èƒ½ï¼ˆImpossibleï¼‰çš„å¥å­ï¼Œè€Œéä»…ä»…æ˜¯æ¦‚ç‡è¾ƒä½ï¼ˆImprobableï¼‰çš„å¥å­ã€‚ä¾‹å¦‚ï¼Œæ¨¡å‹å¯èƒ½è®¤ä¸ºâ€œåˆ¹è½¦ç»™æ±½è½¦å¼€ç½šå•â€æ¯”â€œæ¢é™©å®¶ç»™æ±½è½¦å¼€ç½šå•â€æ›´å…·åˆç†æ€§ã€‚è¯¥é¡¹ç ”ç©¶æ­ç¤ºäº†å½“å‰è¯­è¨€æ¨¡å‹åœ¨ç†è§£å¸¸è¯†é€»è¾‘å’ŒåŒºåˆ†äº‹ä»¶å¯èƒ½æ€§æ–¹é¢çš„æ˜¾è‘—ç¼ºé™·ï¼ŒæŒ‘æˆ˜äº†æ­¤å‰å…³äºæ¨¡å‹å…·å¤‡ç¨³å¥æ¨ç†èƒ½åŠ›çš„è§‚ç‚¹ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "Accepted to Findings of ACL 2025",
      "pdf_url": "https://arxiv.org/pdf/2506.06808v1",
      "published_date": "2025-06-07 14:08:30 UTC",
      "updated_date": "2025-06-07 14:08:30 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T19:44:50.710319+00:00"
    },
    {
      "arxiv_id": "2506.06806v2",
      "title": "Label-semantics Aware Generative Approach for Domain-Agnostic Multilabel Classification",
      "title_zh": "é¢å‘é¢†åŸŸæ— å…³å¤šæ ‡ç­¾åˆ†ç±»çš„æ ‡ç­¾è¯­ä¹‰æ„ŸçŸ¥ç”Ÿæˆå¼æ–¹æ³•",
      "authors": [
        "Subhendu Khatuya",
        "Shashwat Naidu",
        "Saptarshi Ghosh",
        "Pawan Goyal",
        "Niloy Ganguly"
      ],
      "abstract": "The explosion of textual data has made manual document classification increasingly challenging. To address this, we introduce a robust, efficient domain-agnostic generative model framework for multi-label text classification. Instead of treating labels as mere atomic symbols, our approach utilizes predefined label descriptions and is trained to generate these descriptions based on the input text. During inference, the generated descriptions are matched to the pre-defined labels using a finetuned sentence transformer. We integrate this with a dual-objective loss function, combining cross-entropy loss and cosine similarity of the generated sentences with the predefined target descriptions, ensuring both semantic alignment and accuracy. Our proposed model LAGAMC stands out for its parameter efficiency and versatility across diverse datasets, making it well-suited for practical applications. We demonstrate the effectiveness of our proposed model by achieving new state-of-the-art performances across all evaluated datasets, surpassing several strong baselines. We achieve improvements of 13.94% in Micro-F1 and 24.85% in Macro-F1 compared to the closest baseline across all datasets.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†LAGAMCï¼Œä¸€ç§ç”¨äºé¢†åŸŸæ— å…³(Domain-Agnostic)å¤šæ ‡ç­¾æ–‡æœ¬åˆ†ç±»çš„é²æ£’ä¸”é«˜æ•ˆçš„ç”Ÿæˆå¼æ¡†æ¶ã€‚è¯¥æ–¹æ³•ä¸å†å°†æ ‡ç­¾è§†ä¸ºç®€å•çš„åŸå­ç¬¦å·ï¼Œè€Œæ˜¯åˆ©ç”¨é¢„å®šä¹‰çš„æ ‡ç­¾æè¿°(Label Descriptions)ï¼Œè®­ç»ƒæ¨¡å‹æ ¹æ®è¾“å…¥æ–‡æœ¬ç”Ÿæˆè¿™äº›æè¿°ã€‚åœ¨æ¨ç†è¿‡ç¨‹ä¸­ï¼Œç”Ÿæˆçš„æè¿°é€šè¿‡å¾®è°ƒåçš„Sentence Transformerä¸é¢„å®šä¹‰æ ‡ç­¾è¿›è¡ŒåŒ¹é…ã€‚æ¨¡å‹é‡‡ç”¨åŒç›®æ ‡æŸå¤±å‡½æ•°(Dual-Objective Loss Function)ï¼Œç»“åˆäº¤å‰ç†µæŸå¤±(Cross-Entropy Loss)ä¸ä½™å¼¦ç›¸ä¼¼åº¦(Cosine Similarity)ï¼Œç¡®ä¿äº†ç”Ÿæˆçš„è¯­ä¹‰ä¸€è‡´æ€§ä¸åˆ†ç±»å‡†ç¡®ç‡ã€‚LAGAMCå…·æœ‰æ˜¾è‘—çš„å‚æ•°æ•ˆç‡(Parameter Efficiency)å’Œå¤šæ•°æ®é›†é€šç”¨æ€§ï¼Œèƒ½å¤Ÿé€‚åº”å¤šæ ·çš„å®é™…åº”ç”¨åœºæ™¯ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯¥æ¨¡å‹åœ¨æ‰€æœ‰è¯„ä¼°æ•°æ®é›†ä¸Šå‡åˆ·æ–°äº†æœ€å…ˆè¿›æ€§èƒ½(SOTA)ï¼Œå…¶Micro-F1å’ŒMacro-F1è¾ƒç°æœ‰æœ€ä¼˜åŸºå‡†åˆ†åˆ«æå‡äº†13.94%å’Œ24.85%ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "This work has been accepted to appear at the Association for Computational Linguistics (ACL), 2025",
      "pdf_url": "https://arxiv.org/pdf/2506.06806v2",
      "published_date": "2025-06-07 14:07:07 UTC",
      "updated_date": "2025-07-19 04:38:19 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T19:44:54.140152+00:00"
    },
    {
      "arxiv_id": "2506.06793v1",
      "title": "Is Optimal Transport Necessary for Inverse Reinforcement Learning?",
      "title_zh": "æœ€ä¼˜ä¼ è¾“åœ¨é€†å¼ºåŒ–å­¦ä¹ ä¸­æ˜¯å¦å¿…è¦ï¼Ÿ",
      "authors": [
        "Zixuan Dong",
        "Yumi Omori",
        "Keith Ross"
      ],
      "abstract": "Inverse Reinforcement Learning (IRL) aims to recover a reward function from expert demonstrations. Recently, Optimal Transport (OT) methods have been successfully deployed to align trajectories and infer rewards. While OT-based methods have shown strong empirical results, they introduce algorithmic complexity, hyperparameter sensitivity, and require solving the OT optimization problems. In this work, we challenge the necessity of OT in IRL by proposing two simple, heuristic alternatives: (1) Minimum-Distance Reward, which assigns rewards based on the nearest expert state regardless of temporal order; and (2) Segment-Matching Reward, which incorporates lightweight temporal alignment by matching agent states to corresponding segments in the expert trajectory. These methods avoid optimization, exhibit linear-time complexity, and are easy to implement. Through extensive evaluations across 32 online and offline benchmarks with three reinforcement learning algorithms, we show that our simple rewards match or outperform recent OT-based approaches. Our findings suggest that the core benefits of OT may arise from basic proximity alignment rather than its optimal coupling formulation, advocating for reevaluation of complexity in future IRL design.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†é€†å¼ºåŒ–å­¦ä¹ (Inverse Reinforcement Learning)ä¸­å¼•å…¥æœ€ä¼˜ä¼ è¾“(Optimal Transport)æ–¹æ³•çš„å¿…è¦æ€§ï¼Œæ—¨åœ¨è§£å†³ç°æœ‰OTæ–¹æ³•å¸¦æ¥çš„é«˜ç®—æ³•å¤æ‚åº¦å’Œè¶…å‚æ•°æ•æ„Ÿæ€§é—®é¢˜ã€‚ä½œè€…æå‡ºäº†ä¸¤ç§ç®€å•çš„å¯å‘å¼æ›¿ä»£æ–¹æ¡ˆï¼šMinimum-Distance Rewardæ ¹æ®æœ€è¿‘ä¸“å®¶çŠ¶æ€åˆ†é…å¥–åŠ±è€Œå¿½ç•¥æ—¶é—´é¡ºåºï¼ŒSegment-Matching Rewardåˆ™é€šè¿‡åŒ¹é…ä¸“å®¶è½¨è¿¹æ®µè½å¼•å…¥è½»é‡çº§çš„æ—¶é—´å¯¹é½ã€‚è¿™äº›æ–¹æ³•æ— éœ€å¤æ‚çš„ä¼˜åŒ–è¿‡ç¨‹ï¼Œå…·å¤‡çº¿æ€§æ—¶é—´å¤æ‚åº¦ä¸”ææ˜“å®ç°ã€‚é€šè¿‡åœ¨32é¡¹åœ¨çº¿å’Œç¦»çº¿åŸºå‡†æµ‹è¯•ä¸­å¯¹ä¸‰ç§å¼ºåŒ–å­¦ä¹ ç®—æ³•è¿›è¡Œè¯„ä¼°ï¼Œå®éªŒè¯æ˜è¿™ä¸¤ç§ç®€å•å¥–åŠ±æœºåˆ¶çš„æ•ˆæœè¾¾åˆ°ç”šè‡³è¶…è¿‡äº†å…ˆè¿›çš„åŸºäºOTçš„æ–¹æ³•ã€‚ç ”ç©¶ç»“æœè¡¨æ˜ï¼ŒOTåœ¨IRLä¸­çš„æ ¸å¿ƒä¼˜åŠ¿å¯èƒ½æºäºåŸºç¡€çš„é‚»è¿‘æ€§å¯¹é½è€Œéå…¶å¤æ‚çš„æ•°å­¦è€¦åˆå…¬å¼ã€‚è¯¥å‘ç°å€¡å¯¼åœ¨æœªæ¥çš„IRLè®¾è®¡ä¸­é‡æ–°è¯„ä¼°å¤æ‚æ€§çš„å¿…è¦æ€§ï¼Œä¸ºå¼€å‘æ›´é«˜æ•ˆã€æ›´ç®€æ´çš„å¥–åŠ±æ¨æ–­æœºåˆ¶æä¾›äº†æ–°æ€è·¯ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "19 pages, 10 tables",
      "pdf_url": "https://arxiv.org/pdf/2506.06793v1",
      "published_date": "2025-06-07 13:29:37 UTC",
      "updated_date": "2025-06-07 13:29:37 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T19:44:56.503583+00:00"
    },
    {
      "arxiv_id": "2506.06786v1",
      "title": "Learning What Matters Now: A Dual-Critic Context-Aware RL Framework for Priority-Driven Information Gain",
      "title_zh": "èšç„¦å½“ä¸‹å…³é”®ï¼šé¢å‘ä¼˜å…ˆçº§é©±åŠ¨ä¿¡æ¯å¢ç›Šçš„åŒæ‰¹è¯„è€…ä¸Šä¸‹æ–‡æ„ŸçŸ¥å¼ºåŒ–å­¦ä¹ æ¡†æ¶",
      "authors": [
        "Dimitris Panagopoulos",
        "Adolfo Perrusquia",
        "Weisi Guo"
      ],
      "abstract": "Autonomous systems operating in high-stakes search-and-rescue (SAR) missions must continuously gather mission-critical information while flexibly adapting to shifting operational priorities. We propose CA-MIQ (Context-Aware Max-Information Q-learning), a lightweight dual-critic reinforcement learning (RL) framework that dynamically adjusts its exploration strategy whenever mission priorities change. CA-MIQ pairs a standard extrinsic critic for task reward with an intrinsic critic that fuses state-novelty, information-location awareness, and real-time priority alignment. A built-in shift detector triggers transient exploration boosts and selective critic resets, allowing the agent to re-focus after a priority revision. In a simulated SAR grid-world, where experiments specifically test adaptation to changes in the priority order of information types the agent is expected to focus on, CA-MIQ achieves nearly four times higher mission-success rates than baselines after a single priority shift and more than three times better performance in multiple-shift scenarios, achieving 100% recovery while baseline methods fail to adapt. These results highlight CA-MIQ's effectiveness in any discrete environment with piecewise-stationary information-value distributions.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† CA-MIQ (Context-Aware Max-Information Q-learning)ï¼Œè¿™æ˜¯ä¸€ä¸ªè½»é‡çº§çš„åŒè¯„è®ºå®¶ (dual-critic) å¼ºåŒ–å­¦ä¹  (Reinforcement Learning, RL) æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³è‡ªä¸»ç³»ç»Ÿåœ¨æœæ•‘ (search-and-rescue, SAR) ä»»åŠ¡ä¸­ç”±äºä»»åŠ¡ä¼˜å…ˆçº§åŠ¨æ€å˜åŒ–è€Œéš¾ä»¥æŒç»­è·å–å…³é”®ä¿¡æ¯çš„é—®é¢˜ã€‚è¯¥æ¡†æ¶ç»“åˆäº†è´Ÿè´£ä»»åŠ¡å¥–åŠ±çš„æ ‡å‡†å¤–éƒ¨è¯„è®ºå®¶ (extrinsic critic) å’Œèåˆäº†çŠ¶æ€æ–°é¢–æ€§ (state-novelty)ã€ä¿¡æ¯ä½ç½®æ„ŸçŸ¥ä»¥åŠå®æ—¶ä¼˜å…ˆçº§å¯¹é½çš„å†…éƒ¨è¯„è®ºå®¶ (intrinsic critic)ã€‚CA-MIQ å†…ç½®äº†ä¸€ä¸ªåç§»æ£€æµ‹å™¨ (shift detector)ï¼Œèƒ½å¤Ÿåœ¨ä¼˜å…ˆçº§å‘ç”Ÿå˜åŒ–æ—¶è§¦å‘ç¬æ—¶æ¢ç´¢å¢å¼ºå’Œé€‰æ‹©æ€§è¯„è®ºå®¶é‡ç½®ï¼Œä»è€Œä½¿æ™ºèƒ½ä½“åœ¨ä¼˜å…ˆçº§è°ƒæ•´åèƒ½è¿…é€Ÿé‡æ–°èšç„¦ã€‚åœ¨æ¨¡æ‹Ÿçš„æœæ•‘ç½‘æ ¼ä¸–ç•Œ (SAR grid-world) å®éªŒä¸­ï¼ŒCA-MIQ åœ¨å•æ¬¡ä¼˜å…ˆçº§åç§»åçš„ä»»åŠ¡æˆåŠŸç‡æ¥è¿‘åŸºçº¿æ¨¡å‹çš„å››å€ã€‚åœ¨å¤šæ¬¡åç§»åœºæ™¯ä¸‹ï¼Œè¯¥æ¡†æ¶è¡¨ç°å‡ºè¶…è¿‡åŸºçº¿ä¸‰å€çš„æ€§èƒ½æå‡ï¼Œå¹¶å®ç°äº† 100% çš„æ¢å¤ç‡ï¼Œè€ŒåŸºçº¿æ–¹æ³•åˆ™å®Œå…¨æ— æ³•é€‚åº”ã€‚è¿™äº›ç»“æœè¯æ˜äº† CA-MIQ åœ¨å¤„ç†å…·æœ‰åˆ†æ®µå¹³ç¨³ä¿¡æ¯ä»·å€¼åˆ†å¸ƒ (piecewise-stationary information-value distributions) çš„ç¦»æ•£ç¯å¢ƒæ—¶çš„å“è¶Šæœ‰æ•ˆæ€§ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "6 pages, 2 figures, 3 tables, submitted as a regural paper to IEEE International Conference on Systems, Man, and Cybernetics (SMC) 2025",
      "pdf_url": "https://arxiv.org/pdf/2506.06786v1",
      "published_date": "2025-06-07 12:55:10 UTC",
      "updated_date": "2025-06-07 12:55:10 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T19:45:04.320181+00:00"
    },
    {
      "arxiv_id": "2506.06782v1",
      "title": "Feature-Based Instance Neighbor Discovery: Advanced Stable Test-Time Adaptation in Dynamic World",
      "title_zh": "åŸºäºç‰¹å¾çš„å®ä¾‹è¿‘é‚»å‘ç°ï¼šåŠ¨æ€ç¯å¢ƒä¸‹çš„å…ˆè¿›ç¨³å®šæµ‹è¯•æ—¶è‡ªé€‚åº”",
      "authors": [
        "Qinting Jiang",
        "Chuyang Ye",
        "Dongyan Wei",
        "Bingli Wang",
        "Yuan Xue",
        "Jingyan Jiang",
        "Zhi Wang"
      ],
      "abstract": "Despite progress, deep neural networks still suffer performance declines under distribution shifts between training and test domains, leading to a substantial decrease in Quality of Experience (QoE) for applications. Existing test-time adaptation (TTA) methods are challenged by dynamic, multiple test distributions within batches. We observe that feature distributions across different domains inherently cluster into distinct groups with varying means and variances. This divergence reveals a critical limitation of previous global normalization strategies in TTA, which inevitably distort the original data characteristics. Based on this insight, we propose Feature-based Instance Neighbor Discovery (FIND), which comprises three key components: Layer-wise Feature Disentanglement (LFD), Feature Aware Batch Normalization (FABN) and Selective FABN (S-FABN). LFD stably captures features with similar distributions at each layer by constructing graph structures. While FABN optimally combines source statistics with test-time distribution specific statistics for robust feature representation. Finally, S-FABN determines which layers require feature partitioning and which can remain unified, thereby enhancing inference efficiency. Extensive experiments demonstrate that FIND significantly outperforms existing methods, achieving a 30\\% accuracy improvement in dynamic scenarios while maintaining computational efficiency.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹æ·±åº¦ç¥ç»ç½‘ç»œåœ¨æµ‹è¯•æ—¶è‡ªé€‚åº” (Test-Time Adaptation, TTA) è¿‡ç¨‹ä¸­ï¼Œé¢å¯¹åŠ¨æ€å¤šåˆ†å¸ƒæ‰¹é‡æ•°æ®å¯¼è‡´æ€§èƒ½ä¸‹é™çš„é—®é¢˜ï¼ŒæŒ‡å‡ºä¼ ç»Ÿå…¨å±€å½’ä¸€åŒ–ä¼šæ‰­æ›²åŸå§‹æ•°æ®ç‰¹å¾ã€‚ä¸ºæ­¤ä½œè€…æå‡ºäº†åŸºäºç‰¹å¾çš„å®ä¾‹é‚»å±…å‘ç° (Feature-based Instance Neighbor Discovery, FIND) æ¡†æ¶ï¼Œæ—¨åœ¨æ›´ç¨³å®šåœ°å¤„ç†åŠ¨æ€ç¯å¢ƒä¸‹çš„åˆ†å¸ƒåç§»ã€‚è¯¥æ¡†æ¶æ ¸å¿ƒåŒ…å«å±‚çº§ç‰¹å¾è§£è€¦ (Layer-wise Feature Disentanglement, LFD)ï¼Œé€šè¿‡æ„å»ºå›¾ç»“æ„ç¨³å®šæ•è·æ¯å±‚ä¸­åˆ†å¸ƒç›¸ä¼¼çš„ç‰¹å¾ã€‚åŒæ—¶ï¼Œç‰¹å¾æ„ŸçŸ¥æ‰¹é‡å½’ä¸€åŒ– (Feature Aware Batch Normalization, FABN) å°†æºç»Ÿè®¡æ•°æ®ä¸ç‰¹å®šæµ‹è¯•åˆ†å¸ƒç»Ÿè®¡æ•°æ®ç»“åˆï¼Œè€Œé€‰æ‹©æ€§ç‰¹å¾æ„ŸçŸ¥æ‰¹é‡å½’ä¸€åŒ– (Selective FABN, S-FABN) åˆ™é€šè¿‡ç¡®å®šéœ€è¦ç‰¹å¾åˆ’åˆ†çš„å±‚çº§æ¥ä¼˜åŒ–æ¨ç†æ•ˆç‡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒFIND åœ¨åŠ¨æ€åœºæ™¯ä¸‹çš„å‡†ç¡®ç‡æ¯”ç°æœ‰æ–¹æ³•æå‡äº† 30%ï¼Œåœ¨ä¿æŒè®¡ç®—æ•ˆç‡çš„åŒæ—¶æ˜¾è‘—å¢å¼ºäº†æ¨¡å‹åœ¨å¤æ‚ç¯å¢ƒä¸­çš„é²æ£’æ€§ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.06782v1",
      "published_date": "2025-06-07 12:45:49 UTC",
      "updated_date": "2025-06-07 12:45:49 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T19:45:05.127295+00:00"
    },
    {
      "arxiv_id": "2506.06752v1",
      "title": "Depth-Optimal Quantum Layout Synthesis as SAT",
      "title_zh": "åŸºäº SAT çš„æ·±åº¦æœ€ä¼˜é‡å­å¸ƒå±€ç»¼åˆ",
      "authors": [
        "Anna B. Jakobsen",
        "Anders B. Clausen",
        "Jaco van de Pol",
        "Irfansha Shaik"
      ],
      "abstract": "Quantum circuits consist of gates applied to qubits. Current quantum hardware platforms impose connectivity restrictions on binary CX gates. Hence, Layout Synthesis is an important step to transpile quantum circuits before they can be executed. Since CX gates are noisy, it is important to reduce the CX count or CX depth of the mapped circuits.\n  We provide a new and efficient encoding of Quantum-circuit Layout Synthesis in SAT. Previous SAT encodings focused on gate count and CX-gate count. Our encoding instead guarantees that we find mapped circuits with minimal circuit depth or minimal CX-gate depth. We use incremental SAT solving and parallel plans for an efficient encoding. This results in speedups of more than 10-100x compared to OLSQ2, which guarantees depth-optimality. But minimizing depth still takes more time than minimizing gate count with Q-Synth.\n  We correlate the noise reduction achieved by simulating circuits after (CX)-count and (CX)-depth reduction. We find that minimizing for CX-count correlates better with reducing noise than minimizing for CX-depth. However, taking into account both CX-count and CX-depth provides the best noise reduction.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹é‡å­ç¡¬ä»¶å¯¹ CX-gate çš„è¿é€šæ€§é™åˆ¶ï¼Œæå‡ºäº†ä¸€ç§å°† Layout Synthesis é—®é¢˜é«˜æ•ˆç¼–ç ä¸º SAT çš„æ–°æ–¹æ³•ã€‚ä¸åŒäºä»¥å¾€ä¸“æ³¨äºå‡å°‘é—¨æ•°é‡çš„ç ”ç©¶ï¼Œè¯¥ç¼–ç åˆ©ç”¨ incremental SAT solving å’Œå¹¶è¡Œè®¡åˆ’ï¼Œèƒ½å¤Ÿç¡®ä¿æ˜ å°„åçš„ç”µè·¯å…·æœ‰æœ€å°çš„ circuit depth æˆ– CX-gate depthã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯¥æ–¹æ³•åœ¨ä¿è¯æ·±åº¦æœ€ä¼˜çš„å‰æä¸‹ï¼Œè¿è¡Œé€Ÿåº¦æ¯”ç°æœ‰å·¥å…· OLSQ2 æå‡äº† 10 è‡³ 100 å€ä»¥ä¸Šã€‚æ­¤å¤–ï¼Œç ”ç©¶é€šè¿‡æ¨¡æ‹Ÿç”µè·¯åˆ†æäº†å™ªå£°é™ä½ä¸ä¼˜åŒ–æŒ‡æ ‡ä¹‹é—´çš„å…³è”ï¼Œå‘ç°è™½ç„¶æœ€å°åŒ– CX-count ä¸é™å™ªçš„ç›¸å…³æ€§æ¯”æœ€å°åŒ– CX-depth æ›´å¼ºï¼Œä½†ç»¼åˆè€ƒè™‘ CX-count å’Œ CX-depth èƒ½å¤Ÿå®ç°æœ€ä½³çš„é™å™ªæ•ˆæœã€‚",
      "categories": [
        "quant-ph",
        "cs.AI"
      ],
      "primary_category": "quant-ph",
      "comment": "24 pages, 4 figures, 11 tables",
      "pdf_url": "https://arxiv.org/pdf/2506.06752v1",
      "published_date": "2025-06-07 10:47:58 UTC",
      "updated_date": "2025-06-07 10:47:58 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T19:45:16.741280+00:00"
    },
    {
      "arxiv_id": "2506.06750v1",
      "title": "Bio-Inspired Classification: Combining Information Theory and Spiking Neural Networks -- Influence of the Learning Rules",
      "title_zh": "ç”Ÿç‰©å¯å‘åˆ†ç±»ï¼šç»“åˆä¿¡æ¯è®ºä¸è„‰å†²ç¥ç»ç½‘ç»œâ€”â€”å­¦ä¹ è§„åˆ™çš„å½±å“",
      "authors": [
        "Zofia Rudnicka",
        "Janusz Szczepanski",
        "Agnieszka Pregowska"
      ],
      "abstract": "Training of Spiking Neural Networks (SNN) is challenging due to their unique properties, including temporal dynamics, non-differentiability of spike events, and sparse event-driven activations. In this paper, we widely consider the influence of the type of chosen learning algorithm, including bioinspired learning rules on the accuracy of classification. We proposed a bioinspired classifier based on the combination of SNN and Lempel-Ziv complexity (LZC). This approach synergizes the strengths of SNNs in temporal precision and biological realism with LZC's structural complexity analysis, facilitating efficient and interpretable classification of spatiotemporal neural data. It turned out that the classic backpropagation algorithm achieves excellent classification accuracy, but at extremely high computational cost, which makes it impractical for real-time applications. Biologically inspired learning algorithms such as tempotron and Spikprop provide increased computational efficiency while maintaining competitive classification performance, making them suitable for time-sensitive tasks. The results obtained indicate that the selection of the most appropriate learning algorithm depends on the trade-off between classification accuracy and computational cost as well as application constraints.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§ç»“åˆè„‰å†²ç¥ç»ç½‘ç»œ (Spiking Neural Networks, SNN) ä¸ä¿¡æ¯è®ºä¸­ Lempel-Ziv å¤æ‚åº¦ (LZC) çš„ä»¿ç”Ÿåˆ†ç±»å™¨ï¼Œæ—¨åœ¨åˆ©ç”¨ SNN çš„æ—¶é—´ç²¾åº¦ä¸ LZC çš„ç»“æ„å¤æ‚åº¦åˆ†æå®ç°å¯¹æ—¶ç©ºç¥ç»æ•°æ®çš„é«˜æ•ˆåˆ†ç±»ã€‚ç ”ç©¶é‡ç‚¹æ¢è®¨äº†åŒ…æ‹¬ä»¿ç”Ÿå­¦ä¹ è§„åˆ™åœ¨å†…çš„å¤šç§å­¦ä¹ ç®—æ³•å¯¹åˆ†ç±»å‡†ç¡®ç‡çš„å½±å“ï¼Œå¹¶è§£å†³äº† SNN åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­é¢ä¸´çš„éå¾®åˆ†ç‰¹æ€§å’Œæ—¶å˜åŠ¨åŠ›å­¦æŒ‘æˆ˜ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œç»å…¸çš„ Backpropagation ç®—æ³•è™½ç„¶èƒ½è¾¾åˆ°æé«˜çš„å‡†ç¡®ç‡ï¼Œä½†å› è®¡ç®—æˆæœ¬è¿‡é«˜è€Œéš¾ä»¥åº”ç”¨äºå®æ—¶åœºæ™¯ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼ŒTempotron å’Œ Spikprop ç­‰ä»¿ç”Ÿå­¦ä¹ ç®—æ³•åœ¨ä¿æŒç«äº‰æ€§åˆ†ç±»æ€§èƒ½çš„åŒæ—¶ï¼Œæ˜¾è‘—æå‡äº†è®¡ç®—æ•ˆç‡ï¼Œæ›´é€‚åˆæ—¶é—´æ•æ„Ÿå‹ä»»åŠ¡ã€‚è¯¥ç ”ç©¶å¼ºè°ƒï¼Œåœ¨å®é™…åº”ç”¨ä¸­é€‰æ‹©æœ€åˆé€‚çš„å­¦ä¹ ç®—æ³•æ—¶ï¼Œå¿…é¡»åœ¨åˆ†ç±»å‡†ç¡®ç‡ã€è®¡ç®—æˆæœ¬åŠç¡¬ä»¶çº¦æŸä¹‹é—´è¿›è¡Œæƒè¡¡ã€‚",
      "categories": [
        "cs.AI",
        "cs.LG",
        "q-bio.NC"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.06750v1",
      "published_date": "2025-06-07 10:43:09 UTC",
      "updated_date": "2025-06-07 10:43:09 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T19:45:22.989693+00:00"
    },
    {
      "arxiv_id": "2506.06740v2",
      "title": "AI PsyRoom: Artificial Intelligence Platform for Segmented Yearning and Reactive Outcome Optimization Method",
      "title_zh": "AI PsyRoomï¼šé¢å‘åˆ†æ®µå¼æ¸´æœ›ä¸ååº”æ€§ç»“æœä¼˜åŒ–æ–¹æ³•çš„äººå·¥æ™ºèƒ½å¹³å°",
      "authors": [
        "Yigui Feng",
        "Qinglin Wang",
        "Ke Liu",
        "Xinhai Chen",
        "Bo Yang",
        "Jie Liu"
      ],
      "abstract": "Psychological counseling faces huge challenges due to the growing demand for mental health services and the shortage of trained professionals. Large language models (LLMs) have shown potential to assist psychological counseling, especially in empathy and emotional support. However, existing models lack a deep understanding of emotions and are unable to generate personalized treatment plans based on fine-grained emotions. To address these shortcomings, we present AI PsyRoom, a multi-agent simulation framework designed to enhance psychological counseling by generating empathetic and emotionally nuanced conversations. By leveraging fine-grained emotion classification and a multi-agent framework, we construct a multi-agent PsyRoom A for dialogue reconstruction, generating a high-quality dialogue dataset EmoPsy, which contains 35 sub-emotions, 423 specific emotion scenarios, and 12,350 dialogues. We also propose PsyRoom B for generating personalized treatment plans. Quantitative evaluations demonstrate that AI PsyRoom significantly outperforms state-of-the-art methods, achieving 18% improvement in problem orientation, 23% in expression, 24% in Empathy, and 16% in interactive communication quality. The datasets and models are publicly available, providing a foundation for advancing AI-assisted psychological counseling research.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†AI PsyRoomï¼Œè¿™æ˜¯ä¸€ä¸ªæ—¨åœ¨å¢å¼ºå¿ƒç†å’¨è¯¢æ•ˆæœçš„å¤šæ™ºèƒ½ä½“ä»¿çœŸæ¡†æ¶(multi-agent simulation framework)ï¼Œä¸»è¦è§£å†³ç°æœ‰å¤§è¯­è¨€æ¨¡å‹(LLMs)åœ¨æ·±å±‚æƒ…ç»ªç†è§£å’Œä¸ªæ€§åŒ–æ–¹æ¡ˆåˆ¶å®šæ–¹é¢çš„ä¸è¶³ã€‚é€šè¿‡ç»†ç²’åº¦çš„æƒ…ç»ªåˆ†ç±»å’Œå¤šæ™ºèƒ½ä½“åä½œï¼Œç ”ç©¶æ„å»ºäº†ç”¨äºå¯¹è¯é‡æ„çš„PsyRoom Aç³»ç»Ÿï¼Œå¹¶ç”Ÿæˆäº†åŒ…å«35ç§ç»†åˆ†æƒ…ç»ªã€423ä¸ªç‰¹å®šæƒ…ç»ªåœºæ™¯å’Œ12,350æ¡å¯¹è¯çš„é«˜è´¨é‡æ•°æ®é›†EmoPsyã€‚åŒæ—¶ï¼Œè¯¥æ¡†æ¶åˆ©ç”¨PsyRoom Bæ¨¡å—ç”Ÿæˆä¸ªæ€§åŒ–çš„æ²»ç–—æ–¹æ¡ˆï¼Œä»¥å®ç°æ›´å…·å…±æƒ…åŠ›å’Œæƒ…æ„Ÿç»†å¾®å·®åˆ«çš„å¯¹è¯ã€‚å®šé‡è¯„ä¼°è¯æ˜ï¼ŒAI PsyRoomåœ¨é—®é¢˜å¯¼å‘(problem orientation)ã€è¡¨è¾¾èƒ½åŠ›ã€å…±æƒ…èƒ½åŠ›(Empathy)å’Œäº¤äº’æ²Ÿé€šè´¨é‡ç­‰æ–¹é¢æ˜¾è‘—ä¼˜äºç°æœ‰æœ€å…ˆè¿›æ–¹æ³•ï¼Œæ€§èƒ½æå‡å¹…åº¦åœ¨16%è‡³24%ä¹‹é—´ã€‚è¯¥ç ”ç©¶å·²å…¬å¼€æ•°æ®é›†ä¸æ¨¡å‹ï¼Œä¸ºæ¨åŠ¨äººå·¥æ™ºèƒ½è¾…åŠ©å¿ƒç†å’¨è¯¢çš„è¿›ä¸€æ­¥å‘å±•å¥ å®šäº†åŸºç¡€ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "I found that some of the experiments were wrong with some data, especially those involving the protocol evaluation area",
      "pdf_url": "https://arxiv.org/pdf/2506.06740v2",
      "published_date": "2025-06-07 10:01:55 UTC",
      "updated_date": "2025-07-25 11:08:54 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T19:45:27.896213+00:00"
    },
    {
      "arxiv_id": "2506.06739v1",
      "title": "Honey, I shrunk the hypothesis space (through logical preprocessing)",
      "title_zh": "â€œäº²çˆ±çš„ï¼Œæˆ‘ç¼©å°äº†å‡è®¾ç©ºé—´â€ï¼šé€šè¿‡é€»è¾‘é¢„å¤„ç†å®ç°å‡è®¾ç©ºé—´ç¼©å‡",
      "authors": [
        "Andrew Cropper",
        "Filipe Gouveia",
        "David M. Cerna"
      ],
      "abstract": "Inductive logic programming (ILP) is a form of logical machine learning. The goal is to search a hypothesis space for a hypothesis that generalises training examples and background knowledge. We introduce an approach that 'shrinks' the hypothesis space before an ILP system searches it. Our approach uses background knowledge to find rules that cannot be in an optimal hypothesis regardless of the training examples. For instance, our approach discovers relationships such as \"even numbers cannot be odd\" and \"prime numbers greater than 2 are odd\". It then removes violating rules from the hypothesis space. We implement our approach using answer set programming and use it to shrink the hypothesis space of a constraint-based ILP system. Our experiments on multiple domains, including visual reasoning and game playing, show that our approach can substantially reduce learning times whilst maintaining predictive accuracies. For instance, given just 10 seconds of preprocessing time, our approach can reduce learning times from over 10 hours to only 2 seconds.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å½’çº³é€»è¾‘ç¨‹åºè®¾è®¡(Inductive Logic Programming, ILP)æå‡ºäº†ä¸€ç§é€šè¿‡é€»è¾‘é¢„å¤„ç†æ¥â€œç¼©å°â€å‡è®¾ç©ºé—´(Hypothesis Space)çš„æ–°æ–¹æ³•ã€‚è¯¥æ–¹æ³•åˆ©ç”¨èƒŒæ™¯çŸ¥è¯†(Background Knowledge)åœ¨ILPç³»ç»Ÿæœç´¢ä¹‹å‰é¢„å…ˆè¯†åˆ«å¹¶å‰”é™¤åœ¨ä»»ä½•æƒ…å†µä¸‹éƒ½ä¸å¯èƒ½å‡ºç°åœ¨æœ€ä¼˜å‡è®¾ä¸­çš„è§„åˆ™ï¼Œä¾‹å¦‚è‡ªåŠ¨å‘ç°â€œå¶æ•°ä¸èƒ½æ˜¯å¥‡æ•°â€ç­‰é€»è¾‘çº¦æŸå¹¶ç§»é™¤è¿åè§„åˆ™çš„å€™é€‰é¡¹ã€‚ç ”ç©¶å›¢é˜Ÿé‡‡ç”¨å›ç­”é›†ç¨‹åºè®¾è®¡(Answer Set Programming, ASP)å®ç°äº†è¿™ä¸€é¢„å¤„ç†æœºåˆ¶ï¼Œå¹¶å°†å…¶åº”ç”¨äºåŸºäºçº¦æŸçš„ILPç³»ç»Ÿä¸­ã€‚åœ¨è§†è§‰æ¨ç†å’Œæ¸¸æˆåšå¼ˆç­‰å¤šä¸ªé¢†åŸŸçš„å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨ä¿æŒé¢„æµ‹å‡†ç¡®æ€§çš„åŒæ—¶æ˜¾è‘—é™ä½äº†å­¦ä¹ è€—æ—¶ã€‚å®éªŒæ•°æ®è¯æ˜ï¼Œåœ¨ä»…éœ€10ç§’é¢„å¤„ç†æ—¶é—´çš„æƒ…å†µä¸‹ï¼Œè¯¥æ–¹æ³•èƒ½å°†åŸéœ€10å°æ—¶ä»¥ä¸Šçš„å­¦ä¹ è¿‡ç¨‹ç¼©çŸ­è‡³ä»…2ç§’ï¼Œæå¤§åœ°æå‡äº†é€»è¾‘æœºå™¨å­¦ä¹ çš„æœç´¢æ•ˆç‡ã€‚",
      "categories": [
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "Submitted to JAIR",
      "pdf_url": "https://arxiv.org/pdf/2506.06739v1",
      "published_date": "2025-06-07 09:53:02 UTC",
      "updated_date": "2025-06-07 09:53:02 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T19:45:31.780770+00:00"
    },
    {
      "arxiv_id": "2506.06737v1",
      "title": "C-PATH: Conversational Patient Assistance and Triage in Healthcare System",
      "title_zh": "C-PATHï¼šåŒ»ç–—ç³»ç»Ÿä¸­çš„å¯¹è¯å¼æ‚£è€…è¾…åŠ©ä¸åˆ†è¯Š",
      "authors": [
        "Qi Shi",
        "Qiwei Han",
        "ClÃ¡udia Soares"
      ],
      "abstract": "Navigating healthcare systems can be complex and overwhelming, creating barriers for patients seeking timely and appropriate medical attention. In this paper, we introduce C-PATH (Conversational Patient Assistance and Triage in Healthcare), a novel conversational AI system powered by large language models (LLMs) designed to assist patients in recognizing symptoms and recommending appropriate medical departments through natural, multi-turn dialogues. C-PATH is fine-tuned on medical knowledge, dialogue data, and clinical summaries using a multi-stage pipeline built on the LLaMA3 architecture. A core contribution of this work is a GPT-based data augmentation framework that transforms structured clinical knowledge from DDXPlus into lay-person-friendly conversations, allowing alignment with patient communication norms. We also implement a scalable conversation history management strategy to ensure long-range coherence. Evaluation with GPTScore demonstrates strong performance across dimensions such as clarity, informativeness, and recommendation accuracy. Quantitative benchmarks show that C-PATH achieves superior performance in GPT-rewritten conversational datasets, significantly outperforming domain-specific baselines. C-PATH represents a step forward in the development of user-centric, accessible, and accurate AI tools for digital health assistance and triage.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†C-PATHï¼Œä¸€ä¸ªæ—¨åœ¨é€šè¿‡è‡ªç„¶ã€å¤šè½®å¯¹è¯ååŠ©æ‚£è€…è¯†åˆ«ç—‡çŠ¶å¹¶æ¨èåˆé€‚å°±è¯Šç§‘å®¤çš„åˆ›æ–°å‹å¯¹è¯å¼äººå·¥æ™ºèƒ½(Conversational AI)ç³»ç»Ÿã€‚C-PATH åŸºäº LLaMA3 æ¶æ„ï¼Œé€šè¿‡å¤šé˜¶æ®µæµæ°´çº¿(Multi-stage pipeline)åœ¨åŒ»å­¦çŸ¥è¯†ã€å¯¹è¯æ•°æ®å’Œä¸´åºŠæ‘˜è¦ä¸Šè¿›è¡Œå¾®è°ƒã€‚è¯¥ç ”ç©¶çš„æ ¸å¿ƒè´¡çŒ®ä¹‹ä¸€æ˜¯å¼€å‘äº†ä¸€ä¸ªåŸºäº GPT çš„æ•°æ®å¢å¼ºæ¡†æ¶ï¼Œå°† DDXPlus ä¸­çš„ç»“æ„åŒ–ä¸´åºŠçŸ¥è¯†è½¬åŒ–ä¸ºç¬¦åˆæ‚£è€…æ²Ÿé€šä¹ æƒ¯çš„å£è¯­åŒ–å¯¹è¯ã€‚æ­¤å¤–ï¼Œç³»ç»Ÿå®ç°äº†å¯æ‰©å±•çš„å¯¹è¯å†å²ç®¡ç†ç­–ç•¥ï¼Œä»¥ç¡®ä¿é•¿ç¨‹å¯¹è¯çš„è¿è´¯æ€§ã€‚å®éªŒé‡‡ç”¨ GPTScore è¿›è¡Œå¤šç»´åº¦è¯„ä¼°ï¼Œç»“æœè¡¨æ˜ C-PATH åœ¨æ¸…æ™°åº¦ã€ä¿¡æ¯é‡å’Œæ¨èå‡†ç¡®æ€§æ–¹é¢è¡¨ç°ä¼˜å¼‚ã€‚å®šé‡åŸºå‡†æµ‹è¯•æ˜¾ç¤ºï¼ŒC-PATH åœ¨å¤„ç†å¯¹è¯æ•°æ®é›†æ—¶çš„æ€§èƒ½æ˜¾è‘—ä¼˜äºç‰¹å®šé¢†åŸŸçš„åŸºçº¿æ¨¡å‹ã€‚è¿™ä¸€ç ”ç©¶æ ‡å¿—ç€åœ¨å¼€å‘ä»¥ç”¨æˆ·ä¸ºä¸­å¿ƒã€æ˜“äºè·å–ä¸”å‡†ç¡®çš„æ•°å­—åŒ»ç–—è¾…åŠ©ä¸åˆ†è¯Š(Triage)äººå·¥æ™ºèƒ½å·¥å…·æ–¹é¢è¿ˆå‡ºäº†é‡è¦ä¸€æ­¥ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "Accepted in IEEE ICDH 2025, 10 pages, 8 figures, 5 tables",
      "pdf_url": "https://arxiv.org/pdf/2506.06737v1",
      "published_date": "2025-06-07 09:48:47 UTC",
      "updated_date": "2025-06-07 09:48:47 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T19:45:33.785606+00:00"
    },
    {
      "arxiv_id": "2506.06735v1",
      "title": "Ai-Driven Vulnerability Analysis in Smart Contracts: Trends, Challenges and Future Directions",
      "title_zh": "AIé©±åŠ¨çš„æ™ºèƒ½åˆçº¦æ¼æ´åˆ†æï¼šè¶‹åŠ¿ã€æŒ‘æˆ˜ä¸æœªæ¥æ–¹å‘",
      "authors": [
        "Mesut Ozdag"
      ],
      "abstract": "Smart contracts, integral to blockchain ecosystems, enable decentralized applications to execute predefined operations without intermediaries. Their ability to enforce trustless interactions has made them a core component of platforms such as Ethereum. Vulnerabilities such as numerical overflows, reentrancy attacks, and improper access permissions have led to the loss of millions of dollars throughout the blockchain and smart contract sector. Traditional smart contract auditing techniques such as manual code reviews and formal verification face limitations in scalability, automation, and adaptability to evolving development patterns. As a result, AI-based solutions have emerged as a promising alternative, offering the ability to learn complex patterns, detect subtle flaws, and provide scalable security assurances. This paper examines novel AI-driven techniques for vulnerability detection in smart contracts, focusing on machine learning, deep learning, graph neural networks, and transformer-based models. This paper analyzes how each technique represents code, processes semantic information, and responds to real world vulnerability classes. We also compare their strengths and weaknesses in terms of accuracy, interpretability, computational overhead, and real time applicability. Lastly, it highlights open challenges and future opportunities for advancing this domain.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†æ™ºèƒ½åˆçº¦(Smart Contracts)ä¸­çš„æ¼æ´åˆ†æé—®é¢˜ï¼ŒæŒ‡å‡ºä¼ ç»Ÿçš„å®¡è®¡æ–¹æ³•å¦‚æ‰‹åŠ¨ä»£ç å®¡æŸ¥å’Œå½¢å¼åŒ–éªŒè¯(Formal Verification)åœ¨å¯æ‰©å±•æ€§å’Œè‡ªåŠ¨åŒ–æ–¹é¢å­˜åœ¨å±€é™ã€‚é’ˆå¯¹æ•°å€¼æº¢å‡º(Numerical Overflows)å’Œé‡å…¥æ”»å‡»(Reentrancy Attacks)ç­‰å¸¦æ¥çš„å·¨é¢ç»æµæŸå¤±ï¼ŒAIé©±åŠ¨çš„è§£å†³æ–¹æ¡ˆæ­£æˆä¸ºæ£€æµ‹å¤æ‚æ¨¡å¼å’Œå¾®å¦™ç¼ºé™·çš„å…³é”®æ‰‹æ®µã€‚æœ¬æ–‡é‡ç‚¹åˆ†æäº†æœºå™¨å­¦ä¹ (Machine Learning)ã€æ·±åº¦å­¦ä¹ (Deep Learning)ã€å›¾ç¥ç»ç½‘ç»œ(Graph Neural Networks)ä»¥åŠåŸºäºTransformerçš„æ¨¡å‹åœ¨æ¼æ´æ£€æµ‹ä¸­çš„åº”ç”¨ã€‚è®ºæ–‡è¯¦ç»†æ¢è®¨äº†è¿™äº›æŠ€æœ¯å¦‚ä½•è¿›è¡Œä»£ç è¡¨ç¤º(Code Representation)å’Œè¯­ä¹‰ä¿¡æ¯å¤„ç†ï¼Œä»¥åº”å¯¹ç°å®ä¸–ç•Œçš„å„ç§æ¼æ´ç±»åˆ«ã€‚ç ”ç©¶è¿˜ä»å‡†ç¡®æ€§ã€å¯è§£é‡Šæ€§(Interpretability)ã€è®¡ç®—å¼€é”€å’Œå®æ—¶é€‚ç”¨æ€§ç­‰ç»´åº¦å¯¹ä¸åŒæŠ€æœ¯çš„ä¼˜ç¼ºç‚¹è¿›è¡Œäº†å¯¹æ¯”ã€‚æœ€åï¼Œæ–‡ç« æ€»ç»“äº†è¯¥é¢†åŸŸå½“å‰é¢ä¸´çš„å¼€æ”¾æ€§æŒ‘æˆ˜ï¼Œå¹¶ä¸ºæœªæ¥çš„ç ”ç©¶æ–¹å‘æä¾›äº†æŒ‡å¯¼ã€‚",
      "categories": [
        "cs.CR",
        "cs.AI"
      ],
      "primary_category": "cs.CR",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.06735v1",
      "published_date": "2025-06-07 09:44:26 UTC",
      "updated_date": "2025-06-07 09:44:26 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T19:45:36.287597+00:00"
    },
    {
      "arxiv_id": "2506.06732v2",
      "title": "Neural Spectral Band Generation for Audio Coding",
      "title_zh": "ç”¨äºéŸ³é¢‘ç¼–ç çš„ç¥ç»é¢‘å¸¦ç”Ÿæˆ",
      "authors": [
        "Woongjib Choi",
        "Byeong Hyeon Kim",
        "Hyungseob Lim",
        "Inseon Jang",
        "Hong-Goo Kang"
      ],
      "abstract": "Spectral band replication (SBR) enables bit-efficient coding by generating high-frequency bands from the low-frequency ones. However, it only utilizes coarse spectral features upon a subband-wise signal replication, limiting adaptability to diverse acoustic signals. In this paper, we explore the efficacy of a deep neural network (DNN)-based generative approach for coding the high-frequency bands, which we call neural spectral band generation (n-SBG). Specifically, we propose a DNN-based encoder-decoder structure to extract and quantize the side information related to the high-frequency components and generate the components given both the side information and the decoded core-band signals. The whole coding pipeline is optimized with generative adversarial criteria to enable the generation of perceptually plausible sound. From experiments using AAC as the core codec, we show that the proposed method achieves a better perceptual quality than HE-AAC-v1 with much less side information.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ä¼ ç»Ÿé¢‘è°±å¸¦å¤åˆ¶æŠ€æœ¯(SBR)åœ¨éŸ³é¢‘ç¼–ç ä¸­ä»…åˆ©ç”¨ç²—ç³™é¢‘è°±ç‰¹å¾ä¸”ä¿¡å·å¤åˆ¶é€‚åº”æ€§æœ‰é™çš„é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§åä¸ºneural spectral band generation (n-SBG)çš„åŸºäºæ·±åº¦ç¥ç»ç½‘ç»œ(DNN)çš„ç”Ÿæˆå¼æ–¹æ³•ã€‚è¯¥æ–¹æ³•é‡‡ç”¨äº†ä¸€ç§ä¸“é—¨çš„DNNç¼–ç å™¨-è§£ç å™¨ç»“æ„ï¼Œç”¨äºæå–å’Œé‡åŒ–é«˜é¢‘åˆ†é‡çš„ä¾§ä¿¡æ¯ï¼Œå¹¶ç»“åˆè§£ç åçš„æ ¸å¿ƒé¢‘æ®µä¿¡å·ç”Ÿæˆé«˜é¢‘æˆåˆ†ã€‚ä¸ºäº†æå‡éŸ³é¢‘çš„æ„ŸçŸ¥è´¨é‡ï¼Œæ•´ä¸ªç¼–ç æµç¨‹é‡‡ç”¨äº†ç”Ÿæˆå¯¹æŠ—å‡†åˆ™(generative adversarial criteria)è¿›è¡Œä¼˜åŒ–ï¼Œä»¥ç¡®ä¿ç”Ÿæˆæ›´ç¬¦åˆå¬è§‰æ„ŸçŸ¥çš„å£°éŸ³ã€‚åœ¨ä»¥AACä½œä¸ºæ ¸å¿ƒç¼–è§£ç å™¨çš„å®éªŒä¸­ï¼Œn-SBGè¡¨ç°å‡ºäº†ä¼˜äºHE-AAC-v1çš„æ„ŸçŸ¥éŸ³è´¨ï¼Œä¸”æ‰€éœ€çš„ä¾§ä¿¡æ¯é‡å¤§å¹…å‡å°‘ã€‚è¯¥æˆæœè¯æ˜äº†ç”Ÿæˆå¼å»ºæ¨¡åœ¨é™ä½æ¯”ç‰¹ç‡çš„åŒæ—¶ä¿æŒé«˜ä¿çœŸéŸ³é¢‘é‡å»ºçš„æœ‰æ•ˆæ€§ã€‚",
      "categories": [
        "eess.AS",
        "cs.AI",
        "eess.SP"
      ],
      "primary_category": "eess.AS",
      "comment": "Accepted to Interspeech 2025",
      "pdf_url": "https://arxiv.org/pdf/2506.06732v2",
      "published_date": "2025-06-07 09:35:08 UTC",
      "updated_date": "2025-07-28 04:36:04 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T19:45:45.483744+00:00"
    },
    {
      "arxiv_id": "2506.06730v1",
      "title": "Fuse and Federate: Enhancing EV Charging Station Security with Multimodal Fusion and Federated Learning",
      "title_zh": "èåˆä¸è”é‚¦ï¼šåˆ©ç”¨å¤šæ¨¡æ€èåˆä¸è”é‚¦å­¦ä¹ å¢å¼ºç”µåŠ¨æ±½è½¦å……ç”µç«™å®‰å…¨",
      "authors": [
        "Rabah Rahal",
        "Abdelaziz Amara Korba",
        "Yacine Ghamri-Doudane"
      ],
      "abstract": "The rapid global adoption of electric vehicles (EVs) has established electric vehicle supply equipment (EVSE) as a critical component of smart grid infrastructure. While essential for ensuring reliable energy delivery and accessibility, EVSE systems face significant cybersecurity challenges, including network reconnaissance, backdoor intrusions, and distributed denial-of-service (DDoS) attacks. These emerging threats, driven by the interconnected and autonomous nature of EVSE, require innovative and adaptive security mechanisms that go beyond traditional intrusion detection systems (IDS). Existing approaches, whether network-based or host-based, often fail to detect sophisticated and targeted attacks specifically crafted to exploit new vulnerabilities in EVSE infrastructure. This paper proposes a novel intrusion detection framework that leverages multimodal data sources, including network traffic and kernel events, to identify complex attack patterns. The framework employs a distributed learning approach, enabling collaborative intelligence across EVSE stations while preserving data privacy through federated learning. Experimental results demonstrate that the proposed framework outperforms existing solutions, achieving a detection rate above 98% and a precision rate exceeding 97% in decentralized environments. This solution addresses the evolving challenges of EVSE security, offering a scalable and privacypreserving response to advanced cyber threats",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ç”µåŠ¨æ±½è½¦ä¾›ç”µè®¾å¤‡(EVSE)é¢ä¸´çš„ç½‘ç»œä¾¦å¯Ÿã€åé—¨å…¥ä¾µåŠDDoSæ”»å‡»ç­‰æ—¥ç›Šä¸¥å³»çš„èµ›åšå®‰å…¨å¨èƒï¼Œæå‡ºäº†ä¸€ç§ç»“åˆå¤šæ¨¡æ€æ•°æ®èåˆä¸è”é‚¦å­¦ä¹ (Federated Learning)çš„æ–°å‹å…¥ä¾µæ£€æµ‹æ¡†æ¶ã€‚è¯¥æ¡†æ¶æ•´åˆäº†ç½‘ç»œæµé‡ä¸å†…æ ¸äº‹ä»¶(kernel events)ç­‰å¤šæºæ•°æ®ï¼Œèƒ½å¤Ÿæœ‰æ•ˆè¯†åˆ«é’ˆå¯¹EVSEåŸºç¡€è®¾æ–½çš„å¤æ‚æ”»å‡»æ¨¡å¼ã€‚é€šè¿‡é‡‡ç”¨åˆ†å¸ƒå¼å­¦ä¹ æ–¹æ³•ï¼Œè¯¥æ–¹æ¡ˆåœ¨å®ç°å……ç”µç«™é—´åä½œæ™ºèƒ½çš„åŒæ—¶ï¼Œåˆ©ç”¨è”é‚¦å­¦ä¹ ç¡®ä¿äº†æ•°æ®éšç§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ¡†æ¶åœ¨å»ä¸­å¿ƒåŒ–ç¯å¢ƒä¸‹çš„æ£€æµ‹ç‡è¶…è¿‡98%ï¼Œå‡†ç¡®ç‡ä¼˜äº97%ï¼Œæ˜¾è‘—æå‡äº†ç°æœ‰IDSçš„å®‰å…¨é˜²æŠ¤èƒ½åŠ›ã€‚è¿™ä¸€ç ”ç©¶ä¸ºåº”å¯¹æ¼”è¿›ä¸­çš„EVSEå®‰å…¨æŒ‘æˆ˜æä¾›äº†ä¸€ç§å…·å¤‡å¯æ‰©å±•æ€§ä¸”ä¿æŠ¤éšç§çš„è§£å†³æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.CR",
        "cs.AI"
      ],
      "primary_category": "cs.CR",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.06730v1",
      "published_date": "2025-06-07 09:34:10 UTC",
      "updated_date": "2025-06-07 09:34:10 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T19:45:43.986770+00:00"
    },
    {
      "arxiv_id": "2506.06727v3",
      "title": "VisioMath: Benchmarking Figure-based Mathematical Reasoning in LMMs",
      "title_zh": "VisioMathï¼šå¤šæ¨¡æ€å¤§æ¨¡å‹ä¸­åŸºäºå›¾å½¢çš„æ•°å­¦æ¨ç†åŸºå‡†æµ‹è¯•",
      "authors": [
        "Can Li",
        "Ying Liu",
        "Ting Zhang",
        "Mei Wang",
        "Hua Huang"
      ],
      "abstract": "Large Multimodal Models have achieved remarkable progress in integrating vision and language, enabling strong performance across perception, reasoning, and domain-specific tasks. However, their capacity to reason over multiple, visually similar inputs remains insufficiently explored. Such fine-grained comparative reasoning is central to real-world tasks, especially in mathematics and education, where learners must often distinguish between nearly identical diagrams to identify correct solutions. To address this gap, we present VisioMath, a curated benchmark of 1,800 high-quality K-12 mathematics problems in which all candidate answers are diagrams with subtle visual similarities. A comprehensive evaluation of state-of-the-art LMMs, covering both leading closed-source systems and widely adopted open-source models, reveals a consistent decline in accuracy as inter-image similarity increases. Analysis indicates that the dominant failure mode stems from image-text misalignment: rather than grounding reasoning in textual cues, models often resort to shallow positional heuristics, resulting in systematic errors. We further explore three alignment-oriented strategies, spanning training-free approaches and finetuning, and achieve substantial accuracy gains. We hope that VisioMath will serve as a rigorous benchmark and catalyst for developing LMMs toward deeper diagram understanding, precise comparative reasoning, and grounded multi-image-text integration.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† VisioMathï¼Œè¿™æ˜¯ä¸€ä¸ªä¸“é—¨ç”¨äºè¯„ä¼° Large Multimodal Models (LMMs) åœ¨å¤„ç†å¤šä¸ªç»†å¾®è§†è§‰ç›¸ä¼¼è¾“å…¥æ—¶æ•°å­¦æ¨ç†èƒ½åŠ›çš„åŸºå‡†æµ‹è¯•ã€‚è¯¥åŸºå‡†ç”± 1,800 ä¸ªé«˜è´¨é‡çš„ K-12 æ•°å­¦é¢˜ç›®ç»„æˆï¼Œå…¶æ‰€æœ‰å€™é€‰ç­”æ¡ˆå‡ä¸ºè§†è§‰ç‰¹å¾æå…¶ç›¸è¿‘çš„å›¾è¡¨ã€‚è¯„ä¼°ç»“æœæ˜¾ç¤ºï¼Œæ— è®ºæ˜¯é—­æºè¿˜æ˜¯å¼€æºçš„ SOTA LMMsï¼Œå…¶æ¨ç†å‡†ç¡®ç‡å‡ä¼šéšç€å›¾åƒé—´ç›¸ä¼¼åº¦çš„æé«˜è€Œæ˜¾è‘—ä¸‹é™ã€‚åˆ†æè¡¨æ˜ï¼Œæ¨¡å‹çš„ä¸»è¦å¤±è´¥æ¨¡å¼åœ¨äº image-text misalignmentï¼Œå³æ¨¡å‹å€¾å‘äºä¾èµ–æµ…å±‚çš„ positional heuristics è€Œéç»“åˆæ–‡æœ¬çº¿ç´¢è¿›è¡Œæ·±å…¥æ¨ç†ã€‚ç ”ç©¶è¿›ä¸€æ­¥éªŒè¯äº†åŒ…æ‹¬ training-free æ–¹æ³•å’Œ finetuning åœ¨å†…çš„ä¸‰ç§ alignment-oriented strategiesï¼Œå¹¶å–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚VisioMath æ—¨åœ¨æ¨åŠ¨ LMMs åœ¨æ·±åº¦å›¾è¡¨ç†è§£ã€ç²¾ç¡®æ¯”è¾ƒæ¨ç†ä»¥åŠ multi-image-text integration æ–¹é¢çš„å‘å±•ã€‚",
      "categories": [
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.06727v3",
      "published_date": "2025-06-07 09:24:13 UTC",
      "updated_date": "2025-10-07 15:18:45 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T19:45:57.484280+00:00"
    },
    {
      "arxiv_id": "2506.12078v1",
      "title": "Modeling Earth-Scale Human-Like Societies with One Billion Agents",
      "title_zh": "åŸºäºåäº¿çº§æ™ºèƒ½ä½“çš„åœ°çƒè§„æ¨¡ç±»äººç¤¾ä¼šå»ºæ¨¡",
      "authors": [
        "Haoxiang Guan",
        "Jiyan He",
        "Liyang Fan",
        "Zhenzhen Ren",
        "Shaobin He",
        "Xin Yu",
        "Yuan Chen",
        "Shuxin Zheng",
        "Tie-Yan Liu",
        "Zhen Liu"
      ],
      "abstract": "Understanding how complex societal behaviors emerge from individual cognition and interactions requires both high-fidelity modeling of human behavior and large-scale simulations. Traditional agent-based models (ABMs) have been employed to study these dynamics for decades, but are constrained by simplified agent behaviors that fail to capture human complexity. Recent advances in large language models (LLMs) offer new opportunities by enabling agents to exhibit sophisticated social behaviors that go beyond rule-based logic, yet face significant scaling challenges. Here we present Light Society, an agent-based simulation framework that advances both fronts, efficiently modeling human-like societies at planetary scale powered by LLMs. Light Society formalizes social processes as structured transitions of agent and environment states, governed by a set of LLM-powered simulation operations, and executed through an event queue. This modular design supports both independent and joint component optimization, supporting efficient simulation of societies with over one billion agents. Large-scale simulations of trust games and opinion propagation--spanning up to one billion agents--demonstrate Light Society's high fidelity and efficiency in modeling social trust and information diffusion, while revealing scaling laws whereby larger simulations yield more stable and realistic emergent behaviors.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† Light Societyï¼Œè¿™æ˜¯ä¸€ä¸ªæ—¨åœ¨å®ç°å…¨çƒè§„æ¨¡ (Earth-Scale) ç±»äººç¤¾ä¼šçš„æ™ºèƒ½ä½“æ¨¡æ‹Ÿæ¡†æ¶ï¼Œæœ‰æ•ˆè§£å†³äº†ä¼ ç»ŸåŸºäºæ™ºèƒ½ä½“çš„æ¨¡å‹ (ABM) è¡Œä¸ºè¿‡äºç®€åŒ–ä»¥åŠå¤§è¯­è¨€æ¨¡å‹ (LLM) æ‰©å±•æ€§å—é™çš„é—®é¢˜ã€‚è¯¥æ¡†æ¶å°†ç¤¾äº¤è¿‡ç¨‹å½¢å¼åŒ–ä¸ºå— LLM é©±åŠ¨çš„ç»“æ„åŒ–çŠ¶æ€è½¬æ¢ï¼Œå¹¶é€šè¿‡äº‹ä»¶é˜Ÿåˆ— (event queue) æ‰§è¡Œï¼Œå…¶æ¨¡å—åŒ–è®¾è®¡æ”¯æŒå¯¹è¶…è¿‡åäº¿ä¸ªæ™ºèƒ½ä½“è¿›è¡Œé«˜æ•ˆæ¨¡æ‹Ÿã€‚é€šè¿‡å¯¹ä¿¡ä»»åšå¼ˆ (trust games) å’Œèˆ†è®ºä¼ æ’­ (opinion propagation) çš„å¤§è§„æ¨¡å®éªŒï¼ŒLight Society å±•ç¤ºäº†åœ¨æ¨¡æ‹Ÿç¤¾äº¤ä¿¡ä»»å’Œä¿¡æ¯æ‰©æ•£æ–¹é¢çš„é«˜ä¿çœŸåº¦ã€‚ç ”ç©¶è¿›ä¸€æ­¥æ­ç¤ºäº†æ¨¡æ‹Ÿè§„æ¨¡ä¸ç¤¾ä¼šè¡Œä¸ºä¹‹é—´çš„è§„æ¨¡æ³•åˆ™ (scaling laws)ï¼Œè¡¨æ˜æ›´å¤§è§„æ¨¡çš„æ¨¡æ‹Ÿèƒ½å¤Ÿäº§ç”Ÿæ›´ç¨³å®šä¸”çœŸå®çš„æ¶Œç°è¡Œä¸º (emergent behaviors)ï¼Œä¸ºç†è§£å¤§è§„æ¨¡ç¤¾ä¼šåŠ¨åŠ›å­¦çš„æ¼”åŒ–æä¾›äº†å…¨æ–°çš„è§†è§’ä¸å·¥å…·ã€‚",
      "categories": [
        "cs.MA",
        "cs.AI",
        "cs.CL",
        "cs.CY",
        "cs.SI"
      ],
      "primary_category": "cs.MA",
      "comment": "Work in progress",
      "pdf_url": "https://arxiv.org/pdf/2506.12078v1",
      "published_date": "2025-06-07 09:14:12 UTC",
      "updated_date": "2025-06-07 09:14:12 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T19:45:59.583671+00:00"
    },
    {
      "arxiv_id": "2506.06725v2",
      "title": "WorldLLM: Improving LLMs' world modeling using curiosity-driven theory-making",
      "title_zh": "WorldLLMï¼šåˆ©ç”¨å¥½å¥‡å¿ƒé©±åŠ¨çš„ç†è®ºæ„å»ºæå‡å¤§è¯­è¨€æ¨¡å‹çš„ä¸–ç•Œå»ºæ¨¡",
      "authors": [
        "Guillaume Levy",
        "Cedric Colas",
        "Pierre-Yves Oudeyer",
        "Thomas Carta",
        "Clement Romac"
      ],
      "abstract": "Large Language Models (LLMs) possess general world knowledge but often struggle to generate precise predictions in structured, domain-specific contexts such as simulations. These limitations arise from their inability to ground their broad, unstructured understanding in specific environments. To address this, we present WorldLLM, a framework that enhances LLM-based world modeling by combining Bayesian inference and autonomous active exploration with reinforcement learning. WorldLLM leverages the in-context learning abilities of LLMs to guide an LLM-based world model's predictions using natural language hypotheses given in its prompt. These hypotheses are iteratively refined through a Bayesian inference framework that leverages a second LLM as the proposal distribution given collected evidence. This evidence is collected using a curiosity-driven reinforcement learning policy that explores the environment to find transitions with a low log-likelihood under our LLM-based predictive model using the current hypotheses. By alternating between refining hypotheses and collecting new evidence, our framework autonomously drives continual improvement of the predictions. Our experiments demonstrate the effectiveness of WorldLLM in a textual game environment that requires agents to manipulate and combine objects. The framework not only enhances predictive accuracy, but also generates human-interpretable theories of environment dynamics.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†WorldLLMæ¡†æ¶ï¼Œæ—¨åœ¨æå‡å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨ç»“æ„åŒ–ã€ç‰¹å®šé¢†åŸŸç¯å¢ƒä¸­çš„ä¸–ç•Œå»ºæ¨¡ï¼ˆworld modelingï¼‰èƒ½åŠ›ï¼Œè§£å†³å…¶éš¾ä»¥å°†é€šè¯†çŸ¥è¯†åº”ç”¨äºå…·ä½“ç¯å¢ƒçš„å±€é™ã€‚è¯¥æ¡†æ¶å°†Bayesian inferenceä¸åŸºäºå¥½å¥‡å¿ƒé©±åŠ¨çš„å¼ºåŒ–å­¦ä¹ ï¼ˆreinforcement learningï¼‰è‡ªä¸»ä¸»åŠ¨æ¢ç´¢ç›¸ç»“åˆï¼Œé€šè¿‡åœ¨æç¤ºè¯ä¸­æä¾›è‡ªç„¶è¯­è¨€å‡è®¾æ¥æŒ‡å¯¼LLMçš„é¢„æµ‹ã€‚è¿™äº›å‡è®¾é€šè¿‡è´å¶æ–¯æ¡†æ¶è¿›è¡Œè¿­ä»£ä¼˜åŒ–ï¼Œå¹¶åˆ©ç”¨å¥½å¥‡å¿ƒé©±åŠ¨çš„ç­–ç•¥ä¸“é—¨æ¢ç´¢é¢„æµ‹æ¦‚ç‡è¾ƒä½çš„åœºæ™¯ä»¥æ”¶é›†æ–°è¯æ®ã€‚é€šè¿‡äº¤æ›¿è¿›è¡Œå‡è®¾ä¼˜åŒ–å’Œè¯æ®æ”¶é›†ï¼Œç³»ç»Ÿå®ç°äº†é¢„æµ‹èƒ½åŠ›çš„è‡ªä¸»æŒç»­æ”¹è¿›ã€‚åœ¨æ–‡æœ¬æ¸¸æˆç¯å¢ƒä¸­çš„å®éªŒç»“æœè¡¨æ˜ï¼ŒWorldLLMä¸ä»…æ˜¾è‘—æé«˜äº†é¢„æµ‹å‡†ç¡®æ€§ï¼Œè¿˜èƒ½ç”Ÿæˆå…·æœ‰äººç±»å¯è§£é‡Šæ€§çš„ç¯å¢ƒåŠ¨åŠ›å­¦ç†è®ºã€‚",
      "categories": [
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.06725v2",
      "published_date": "2025-06-07 09:13:34 UTC",
      "updated_date": "2025-11-24 16:18:31 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T19:46:14.046799+00:00"
    },
    {
      "arxiv_id": "2506.06719v1",
      "title": "Improving Wildlife Out-of-Distribution Detection: Africas Big Five",
      "title_zh": "æ”¹è¿›é‡ç”ŸåŠ¨ç‰©åˆ†å¸ƒå¤–æ£€æµ‹ï¼šAfricas Big Five",
      "authors": [
        "Mufhumudzi Muthivhi",
        "Jiahao Huo",
        "Fredrik Gustafsson",
        "Terence L. van Zyl"
      ],
      "abstract": "Mitigating human-wildlife conflict seeks to resolve unwanted encounters between these parties. Computer Vision provides a solution to identifying individuals that might escalate into conflict, such as members of the Big Five African animals. However, environments often contain several varied species. The current state-of-the-art animal classification models are trained under a closed-world assumption. They almost always remain overconfident in their predictions even when presented with unknown classes. This study investigates out-of-distribution (OOD) detection of wildlife, specifically the Big Five. To this end, we select a parametric Nearest Class Mean (NCM) and a non-parametric contrastive learning approach as baselines to take advantage of pretrained and projected features from popular classification encoders. Moreover, we compare our baselines to various common OOD methods in the literature. The results show feature-based methods reflect stronger generalisation capability across varying classification thresholds. Specifically, NCM with ImageNet pre-trained features achieves a 2%, 4% and 22% improvement on AUPR-IN, AUPR-OUT and AUTC over the best OOD methods, respectively. The code can be found here https://github.com/pxpana/BIG5OOD",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†é‡ç”ŸåŠ¨ç‰©åˆ†å¸ƒå¤–æ£€æµ‹(Out-of-Distribution detection)é—®é¢˜ï¼Œæ—¨åœ¨é€šè¿‡è®¡ç®—æœºè§†è§‰è¯†åˆ«éæ´²äº”å¤§å…½(Big Five)ä»¥ç¼“è§£äººä¸é‡ç”ŸåŠ¨ç‰©çš„å†²çªã€‚é’ˆå¯¹ç›®å‰ä¸»æµåˆ†ç±»æ¨¡å‹åœ¨é—­åˆä¸–ç•Œ(closed-world)å‡è®¾ä¸‹å¯¹æœªçŸ¥ç±»åˆ«è¿‡åº¦è‡ªä¿¡çš„é—®é¢˜ï¼Œè¯¥ç ”ç©¶å¯¹æ¯”äº†å¤šç§å¸¸è§çš„OODæ£€æµ‹æ–¹æ³•ã€‚ç ”ç©¶é‡ç‚¹è¯„ä¼°äº†å‚æ•°åŒ–çš„æœ€è¿‘ç±»å‡å€¼(Nearest Class Mean, NCM)å’Œéå‚æ•°åŒ–çš„å¯¹æ¯”å­¦ä¹ (contrastive learning)æ–¹æ³•ï¼Œåˆ©ç”¨é¢„è®­ç»ƒç¼–ç å™¨çš„ç‰¹å¾æŠ•å½±è¿›è¡Œæ£€æµ‹ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒåŸºäºç‰¹å¾(feature-based)çš„æ–¹æ³•åœ¨ä¸åŒåˆ†ç±»é˜ˆå€¼ä¸‹å…·æœ‰æ›´å¼ºçš„æ³›åŒ–èƒ½åŠ›ã€‚ç‰¹åˆ«æ˜¯é‡‡ç”¨ImageNeté¢„è®­ç»ƒç‰¹å¾çš„NCMæ–¹æ³•ï¼Œåœ¨AUPR-INã€AUPR-OUTå’ŒAUTCæŒ‡æ ‡ä¸Šåˆ†åˆ«æ¯”æœ€ä¼˜åŸºå‡†æ–¹æ³•æé«˜äº†2%ã€4%å’Œ22%ã€‚è¯¥ç ”ç©¶è¯æ˜äº†åˆ©ç”¨é¢„è®­ç»ƒç‰¹å¾æå‡é‡ç”ŸåŠ¨ç‰©è¯†åˆ«ç³»ç»Ÿé²æ£’æ€§çš„æœ‰æ•ˆæ€§ï¼Œä¸ºå®é™…ç¯å¢ƒä¸‹çš„ç‰©ç§ç›‘æµ‹æä¾›äº†æŠ€æœ¯æ”¯æŒã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.06719v1",
      "published_date": "2025-06-07 09:02:48 UTC",
      "updated_date": "2025-06-07 09:02:48 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T19:46:38.296415+00:00"
    },
    {
      "arxiv_id": "2506.06714v1",
      "title": "Integrating AI Planning Semantics into SysML System Models for Automated PDDL File Generation",
      "title_zh": "å°†äººå·¥æ™ºèƒ½è§„åˆ’è¯­ä¹‰é›†æˆè‡³ SysML ç³»ç»Ÿæ¨¡å‹ï¼Œå®ç° PDDL æ–‡ä»¶çš„è‡ªåŠ¨ç”Ÿæˆ",
      "authors": [
        "Hamied Nabizada",
        "Tom Jeleniewski",
        "Lasse Beers",
        "Maximilian Weigand",
        "Felix Gehlhoff",
        "Alexander Fay"
      ],
      "abstract": "This paper presents a SysML profile that enables the direct integration of planning semantics based on the Planning Domain Definition Language (PDDL) into system models. Reusable stereotypes are defined for key PDDL concepts such as types, predicates, functions and actions, while formal OCL constraints ensure syntactic consistency. The profile was derived from the Backus-Naur Form (BNF) definition of PDDL 3.1 to align with SysML modeling practices. A case study from aircraft manufacturing demonstrates the application of the profile: a robotic system with interchangeable end effectors is modeled and enriched to generate both domain and problem descriptions in PDDL format. These are used as input to a PDDL solver to derive optimized execution plans. The approach supports automated and model-based generation of planning descriptions and provides a reusable bridge between system modeling and AI planning in engineering design.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§ SysML é…ç½®æ–‡ä»¶ï¼Œæ—¨åœ¨å°†åŸºäºè§„åˆ’é¢†åŸŸå®šä¹‰è¯­è¨€ PDDL çš„è§„åˆ’è¯­ä¹‰ç›´æ¥é›†æˆåˆ°ç³»ç»Ÿæ¨¡å‹ä¸­ã€‚é€šè¿‡ä¸º PDDL çš„æ ¸å¿ƒæ¦‚å¿µå¦‚ typesã€predicatesã€functions å’Œ actions å®šä¹‰å¯é‡ç”¨çš„æ„å‹ï¼Œå¹¶åˆ©ç”¨å½¢å¼åŒ–çš„ OCL çº¦æŸç¡®ä¿è¯­æ³•çš„è¿è´¯æ€§ï¼Œè¯¥æ–¹æ¡ˆå®ç°äº†æ¨¡å‹ä¸ PDDL 3.1 æ ‡å‡†çš„æ·±åº¦å¯¹é½ã€‚è®ºæ–‡é€šè¿‡é£æœºåˆ¶é€ é¢†åŸŸçš„æœºå™¨äººç³»ç»Ÿæ¡ˆä¾‹ï¼Œå±•ç¤ºäº†å¦‚ä½•ä»å¯Œé›†åçš„ç³»ç»Ÿæ¨¡å‹ä¸­è‡ªåŠ¨ç”Ÿæˆ PDDL æ ¼å¼çš„é¢†åŸŸä¸é—®é¢˜æè¿°ã€‚è¿™äº›ç”Ÿæˆçš„æè¿°å¯ç›´æ¥ä½œä¸º PDDL æ±‚è§£å™¨ï¼ˆsolverï¼‰çš„è¾“å…¥ï¼Œç”¨ä»¥æ¨å¯¼å‡ºä¼˜åŒ–çš„æ‰§è¡Œè®¡åˆ’ã€‚è¯¥æ–¹æ³•æ”¯æŒä»æ¨¡å‹é©±åŠ¨çš„è§’åº¦è‡ªåŠ¨ç”Ÿæˆè§„åˆ’æè¿°ï¼Œä¸ºå·¥ç¨‹è®¾è®¡ä¸­çš„ç³»ç»Ÿå»ºæ¨¡ä¸ AI planning ä¹‹é—´æ­å»ºäº†é«˜æ•ˆä¸”å¯é‡ç”¨çš„æ¡¥æ¢ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.06714v1",
      "published_date": "2025-06-07 08:46:14 UTC",
      "updated_date": "2025-06-07 08:46:14 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T19:46:20.118165+00:00"
    },
    {
      "arxiv_id": "2506.10020v1",
      "title": "From Threat to Tool: Leveraging Refusal-Aware Injection Attacks for Safety Alignment",
      "title_zh": "ä»å¨èƒåˆ°å·¥å…·ï¼šåˆ©ç”¨æ‹’ç»æ„ŸçŸ¥æ³¨å…¥æ”»å‡»å®ç°å®‰å…¨å¯¹é½",
      "authors": [
        "Kyubyung Chae",
        "Hyunbin Jin",
        "Taesup Kim"
      ],
      "abstract": "Safely aligning large language models (LLMs) often demands extensive human-labeled preference data, a process that's both costly and time-consuming. While synthetic data offers a promising alternative, current methods frequently rely on complex iterative prompting or auxiliary models. To address this, we introduce Refusal-Aware Adaptive Injection (RAAI), a straightforward, training-free, and model-agnostic framework that repurposes LLM attack techniques. RAAI works by detecting internal refusal signals and adaptively injecting predefined phrases to elicit harmful, yet fluent, completions. Our experiments show RAAI effectively jailbreaks LLMs, increasing the harmful response rate from a baseline of 2.15% to up to 61.04% on average across four benchmarks. Crucially, fine-tuning LLMs with the synthetic data generated by RAAI improves model robustness against harmful prompts while preserving general capabilities on standard tasks like MMLU and ARC. This work highlights how LLM attack methodologies can be reframed as practical tools for scalable and controllable safety alignment.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤§è¯­è¨€æ¨¡å‹ (LLMs) å®‰å…¨å¯¹é½ä¸­äººå·¥æ ‡æ³¨æˆæœ¬é«˜æ˜‚çš„é—®é¢˜ï¼Œæå‡ºäº† Refusal-Aware Adaptive Injection (RAAI) æ¡†æ¶ï¼Œè¿™æ˜¯ä¸€ç§æ— éœ€è®­ç»ƒä¸”æ¨¡å‹æ— å…³çš„åˆ›æ–°æ–¹æ³•ã€‚RAAI é€šè¿‡æ£€æµ‹æ¨¡å‹å†…éƒ¨çš„æ‹’ç»ä¿¡å·ï¼Œè‡ªé€‚åº”åœ°æ³¨å…¥é¢„å®šä¹‰çŸ­è¯­ä»¥è¯±å¯¼ç”Ÿæˆæœ‰å®³ä½†æµç•…çš„å›å¤ï¼Œä»è€Œå°†ä¼ ç»Ÿçš„ LLM æ”»å‡»æŠ€æœ¯è½¬åŒ–ä¸ºç”Ÿæˆå®‰å…¨å¯¹é½æ•°æ®çš„å·¥å…·ã€‚å®éªŒè¡¨æ˜ï¼ŒRAAI åœ¨å››ä¸ªåŸºå‡†æµ‹è¯•ä¸Šå°†æœ‰å®³å“åº”ç‡ä» 2.15% çš„åŸºçº¿æ˜¾è‘—æå‡è‡³å¹³å‡ 61.04%ã€‚åˆ©ç”¨è¿™äº›åˆæˆæ•°æ®è¿›è¡Œå¾®è°ƒï¼Œä¸ä»…æ˜¾è‘—å¢å¼ºäº†æ¨¡å‹é˜²å¾¡æœ‰å®³æç¤ºè¯çš„é²æ£’æ€§ï¼Œè¿˜ä¿ç•™äº†å…¶åœ¨ MMLU å’Œ ARC ç­‰æ ‡å‡†ä»»åŠ¡ä¸­çš„é€šç”¨èƒ½åŠ›ã€‚è¯¥ç ”ç©¶æˆåŠŸè¯æ˜äº†æ”»å‡»æ–¹æ³•å¯ä»¥è¢«é‡æ–°æ„æƒ³ä¸ºä¸€ç§å¯æ‰©å±•ä¸”å—æ§çš„æ‰‹æ®µï¼Œç”¨ä»¥é«˜æ•ˆæå‡æ¨¡å‹çš„å®‰å…¨æ€§ã€‚",
      "categories": [
        "cs.CR",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.CR",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.10020v1",
      "published_date": "2025-06-07 08:19:01 UTC",
      "updated_date": "2025-06-07 08:19:01 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T19:46:26.591178+00:00"
    },
    {
      "arxiv_id": "2506.11107v1",
      "title": "Denoising Programming Knowledge Tracing with a Code Graph-based Tuning Adaptor",
      "title_zh": "åŸºäºä»£ç å›¾å¾®è°ƒé€‚é…å™¨çš„é™å™ªç¼–ç¨‹çŸ¥è¯†è¿½è¸ª",
      "authors": [
        "Weibo Gao",
        "Qi Liu",
        "Rui Li",
        "Yuze Zhao",
        "Hao Wang",
        "Linan Yre",
        "Fangzhou Yao",
        "Zheng Zhang"
      ],
      "abstract": "Programming Knowledge Tracking (PKT) aims to dynamically diagnose learners' mastery levels of programming knowledge based on their coding activities, facilitating more effective and personalized programming education. However, current PKT studies primarily focus on the implicit relationship between code content and knowledge assessment, often overlooking two types of noise signals in long-term programming activities: unwanted signals from unrelated submissions and weak signals from minor modifications. This practical challenge significantly limits model performance and application. To address this issue, we propose Coda, a Code graph-based tuning adaptor designed to enhance existing PKT models by identifying and mitigating the impact of noise. Specifically, Coda first transforms the loose code sequences submitted by each learner into a compact code graph. By leveraging this code graph, unwanted signals can be identified from a semantic similarity perspective. We then apply a cluster-aware GCN to the code graph, which improves the discrimination of weak signals and enables their clustering for identification. Finally, a lightweight yet effective adaptor is incorporated into the PKT task through optimization with two noise feature-based constraints and a navigational regularization term, to correct knowledge states affected by noise. It is worth mentioning that the Coda framework is model-agnostic and can be adapted to most existing PKT solutions. Extensive experimental results on four real-world datasets demonstrate that Coda effectively performs the PKT task in the presence of noisy programming records, outperforming typical baselines.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ç¼–ç¨‹çŸ¥è¯†è¿½è¸ª (Programming Knowledge Tracing, PKT) ä¸­é•¿æœŸç¼–ç¨‹æ´»åŠ¨äº§ç”Ÿçš„æ— å…³æäº¤å™ªå£°å’Œå¾®å°ä¿®æ”¹å¾®å¼±ä¿¡å·é—®é¢˜ï¼Œæå‡ºäº†åä¸º Coda çš„ä»£ç å›¾ (Code graph) å¾®è°ƒé€‚é…å™¨ã€‚Coda é€šè¿‡å°†å­¦ä¹ è€…çš„ä»£ç åºåˆ—è½¬æ¢ä¸ºç´§å‡‘çš„ä»£ç å›¾ï¼Œåˆ©ç”¨è¯­ä¹‰ç›¸ä¼¼æ€§è¯†åˆ«æ— å…³ä¿¡å·ï¼Œå¹¶ç»“åˆèšç±»æ„ŸçŸ¥å›¾å·ç§¯ç½‘ç»œ (cluster-aware GCN) å¢å¼ºå¯¹å¾®å¼±ä¿¡å·çš„è¾¨è¯†ä¸èšç±»ã€‚æ­¤å¤–ï¼Œç ”ç©¶å¼•å…¥äº†åŸºäºå™ªå£°ç‰¹å¾çš„çº¦æŸå’Œå¯¼èˆªæ­£åˆ™åŒ–é¡¹æ¥ä¼˜åŒ–è½»é‡çº§é€‚é…å™¨ï¼Œä»è€Œçº æ­£å—æŸçš„çŸ¥è¯†çŠ¶æ€ã€‚ç”±äº Coda å…·æœ‰æ¨¡å‹æ— å…³æ€§ (model-agnostic) çš„ç‰¹ç‚¹ï¼Œå®ƒå¯ä»¥æ— ç¼é›†æˆåˆ°ç°æœ‰çš„å¤šç§ PKT è§£å†³æ–¹æ¡ˆä¸­ã€‚åœ¨å››ä¸ªçœŸå®ä¸–ç•Œæ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¯æ˜ï¼ŒCoda åœ¨å¤„ç†å™ªå£°ç¼–ç¨‹è®°å½•æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œæ˜¾è‘—æå‡äº† PKT ä»»åŠ¡çš„å‡†ç¡®æ€§ï¼Œä¸”æ€§èƒ½ä¼˜äºå½“å‰çš„åŸºå‡†æ¨¡å‹ã€‚",
      "categories": [
        "cs.SE",
        "cs.AI"
      ],
      "primary_category": "cs.SE",
      "comment": "Accepted by KDD August 2025",
      "pdf_url": "https://arxiv.org/pdf/2506.11107v1",
      "published_date": "2025-06-07 08:15:26 UTC",
      "updated_date": "2025-06-07 08:15:26 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T19:46:28.937556+00:00"
    },
    {
      "arxiv_id": "2506.06705v1",
      "title": "DivScore: Zero-Shot Detection of LLM-Generated Text in Specialized Domains",
      "title_zh": "DivScoreï¼šä¸“ä¸šé¢†åŸŸå¤§è¯­è¨€æ¨¡å‹ç”Ÿæˆæ–‡æœ¬çš„é›¶æ ·æœ¬æ£€æµ‹",
      "authors": [
        "Zhihui Chen",
        "Kai He",
        "Yucheng Huang",
        "Yunxiao Zhu",
        "Mengling Feng"
      ],
      "abstract": "Detecting LLM-generated text in specialized and high-stakes domains like medicine and law is crucial for combating misinformation and ensuring authenticity. However, current zero-shot detectors, while effective on general text, often fail when applied to specialized content due to domain shift. We provide a theoretical analysis showing this failure is fundamentally linked to the KL divergence between human, detector, and source text distributions. To address this, we propose DivScore, a zero-shot detection framework using normalized entropy-based scoring and domain knowledge distillation to robustly identify LLM-generated text in specialized domains. We also release a domain-specific benchmark for LLM-generated text detection in the medical and legal domains. Experiments on our benchmark show that DivScore consistently outperforms state-of-the-art detectors, with 14.4% higher AUROC and 64.0% higher recall (0.1% false positive rate threshold). In adversarial settings, DivScore demonstrates superior robustness than other baselines, achieving on average 22.8% advantage in AUROC and 29.5% in recall. Code and data are publicly available.",
      "tldr_zh": "è¯¥é¡¹ç ”ç©¶é’ˆå¯¹åŒ»å­¦å’Œæ³•å¾‹ç­‰ä¸“ä¸šé¢†åŸŸä¸­å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ç”Ÿæˆæ–‡æœ¬çš„è¯†åˆ«é—®é¢˜ï¼Œæå‡ºäº†åä¸ºDivScoreçš„é›¶æ ·æœ¬ï¼ˆZero-Shotï¼‰æ£€æµ‹æ¡†æ¶ã€‚ä½œè€…é€šè¿‡ç†è®ºåˆ†ææ­ç¤ºäº†ç°æœ‰æ£€æµ‹å™¨åœ¨ç‰¹å®šé¢†åŸŸå¤±æ•ˆçš„æ ¹æœ¬åŸå› åœ¨äºäººç±»ã€æ£€æµ‹å™¨ä¸æºæ–‡æœ¬åˆ†å¸ƒä¹‹é—´çš„KLæ•£åº¦ï¼ˆKL divergenceï¼‰ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼ŒDivScoreåˆ›æ–°æ€§åœ°ç»“åˆäº†å½’ä¸€åŒ–ç†µè¯„åˆ†ï¼ˆNormalized entropy-based scoringï¼‰ä¸é¢†åŸŸçŸ¥è¯†è’¸é¦ï¼ˆDomain knowledge distillationï¼‰æŠ€æœ¯ï¼Œå®ç°äº†å¯¹LLMç”Ÿæˆå†…å®¹çš„ç¨³å¥è¯†åˆ«ã€‚æ­¤å¤–ï¼Œç ”ç©¶å›¢é˜Ÿè¿˜å‘å¸ƒäº†é’ˆå¯¹åŒ»ç–—å’Œæ³•å¾‹é¢†åŸŸçš„é¢†åŸŸç‰¹å®šåŸºå‡†æµ‹è¯•æ•°æ®é›†ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒDivScoreåœ¨AUROCå’Œå¬å›ç‡æŒ‡æ ‡ä¸Šåˆ†åˆ«è¶…è¶Šäº†ç°æœ‰æœ€å…ˆè¿›æ¨¡å‹14.4%å’Œ64.0%ï¼Œä¸”åœ¨å¯¹æŠ—æ€§ç¯å¢ƒä¸‹ä¹Ÿå±•ç°å‡ºæ˜¾è‘—çš„é²æ£’æ€§ä¼˜åŠ¿ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "Zhihui Chen and Kai He contributed equally to this work, Mengling Feng is the corresponding author",
      "pdf_url": "https://arxiv.org/pdf/2506.06705v1",
      "published_date": "2025-06-07 08:03:35 UTC",
      "updated_date": "2025-06-07 08:03:35 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T19:46:30.226343+00:00"
    },
    {
      "arxiv_id": "2506.06701v1",
      "title": "Do Protein Transformers Have Biological Intelligence?",
      "title_zh": "è›‹ç™½è´¨ Transformer æ˜¯å¦å…·å¤‡ç”Ÿç‰©æ™ºèƒ½ï¼Ÿ",
      "authors": [
        "Fudong Lin",
        "Wanrou Du",
        "Jinchan Liu",
        "Tarikul Milon",
        "Shelby Meche",
        "Wu Xu",
        "Xiaoqi Qin",
        "Xu Yuan"
      ],
      "abstract": "Deep neural networks, particularly Transformers, have been widely adopted for predicting the functional properties of proteins. In this work, we focus on exploring whether Protein Transformers can capture biological intelligence among protein sequences. To achieve our goal, we first introduce a protein function dataset, namely Protein-FN, providing over 9000 protein data with meaningful labels. Second, we devise a new Transformer architecture, namely Sequence Protein Transformers (SPT), for computationally efficient protein function predictions. Third, we develop a novel Explainable Artificial Intelligence (XAI) technique called Sequence Score, which can efficiently interpret the decision-making processes of protein models, thereby overcoming the difficulty of deciphering biological intelligence bided in Protein Transformers. Remarkably, even our smallest SPT-Tiny model, which contains only 5.4M parameters, demonstrates impressive predictive accuracy, achieving 94.3% on the Antibiotic Resistance (AR) dataset and 99.6% on the Protein-FN dataset, all accomplished by training from scratch. Besides, our Sequence Score technique helps reveal that our SPT models can discover several meaningful patterns underlying the sequence structures of protein data, with these patterns aligning closely with the domain knowledge in the biology community. We have officially released our Protein-FN dataset on Hugging Face Datasets https://huggingface.co/datasets/Protein-FN/Protein-FN. Our code is available at https://github.com/fudong03/BioIntelligence.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº† Protein Transformers æ˜¯å¦èƒ½å¤Ÿæ•æ‰è›‹ç™½è´¨åºåˆ—ä¸­çš„ Biological Intelligenceï¼Œå¹¶ä¸ºæ­¤æå‡ºäº†åŒ…å«è¶…è¿‡9000æ¡å¸¦æ ‡ç­¾æ•°æ®çš„ Protein-FN æ•°æ®é›†ã€‚ç ”ç©¶å›¢é˜Ÿè®¾è®¡äº†ä¸€ç§è®¡ç®—æ•ˆç‡é«˜çš„å…¨æ–°æ¶æ„ Sequence Protein Transformers (SPT)ï¼Œç”¨äºå®ç°é«˜æ•ˆçš„è›‹ç™½è´¨åŠŸèƒ½é¢„æµ‹ã€‚ä¸ºäº†å¢å¼ºæ¨¡å‹çš„å¯è§£é‡Šæ€§ï¼Œä½œè€…å¼€å‘äº†åä¸º Sequence Score çš„ Explainable Artificial Intelligence (XAI) æŠ€æœ¯ï¼Œæ—¨åœ¨è§£ææ¨¡å‹æ•æ‰ç”Ÿç‰©ç‰¹å¾çš„å†³ç­–è¿‡ç¨‹ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œå³ä½¿æ˜¯å‚æ•°é‡ä»…ä¸º5.4Mçš„ SPT-Tiny æ¨¡å‹ï¼Œåœ¨ä»å¤´è®­ç»ƒçš„æƒ…å†µä¸‹ï¼Œäº Antibiotic Resistance (AR) å’Œ Protein-FN æ•°æ®é›†ä¸Šçš„å‡†ç¡®ç‡åˆ†åˆ«è¾¾åˆ°äº†94.3%å’Œ99.6%ã€‚æ­¤å¤–ï¼ŒSequence Score è¯å®äº† SPT æ¨¡å‹èƒ½å¤Ÿå‘ç°ä¸ç”Ÿç‰©å­¦é¢†åŸŸçŸ¥è¯†é«˜åº¦å»åˆçš„è›‹ç™½è´¨åºåˆ—æ½œåœ¨æ¨¡å¼ã€‚è¯¥é¡¹å·¥ä½œé€šè¿‡æä¾›é«˜æ€§èƒ½æ¨¡å‹ã€è§£é‡Šæ€§å·¥å…·åŠå¼€æºæ•°æ®é›†ï¼Œä¸ºæ¢ç´¢è›‹ç™½è´¨åºåˆ—ä¸­çš„ç”Ÿç‰©è§„å¾‹æä¾›äº†é‡è¦å‚è€ƒã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "q-bio.BM"
      ],
      "primary_category": "cs.LG",
      "comment": "Accepted by European Conference on Machine Learning and Principles and Practice of Knowledge Discovery in Databases (ECML-PKDD 2025)",
      "pdf_url": "https://arxiv.org/pdf/2506.06701v1",
      "published_date": "2025-06-07 07:52:52 UTC",
      "updated_date": "2025-06-07 07:52:52 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T19:46:35.736659+00:00"
    },
    {
      "arxiv_id": "2506.06699v1",
      "title": "MarginSel : Max-Margin Demonstration Selection for LLMs",
      "title_zh": "MarginSelï¼šé¢å‘å¤§è¯­è¨€æ¨¡å‹çš„æœ€å¤§é—´éš”ç¤ºä¾‹é€‰æ‹©",
      "authors": [
        "Rajeev Bhatt Ambati",
        "James Lester",
        "Shashank Srivastava",
        "Snigdha Chaturvedi"
      ],
      "abstract": "Large Language Models (LLMs) excel at few-shot learning via in-context learning (ICL). However, the effectiveness of ICL is often sensitive to the selection and ordering of demonstration examples. To address this, we present MarginSel: Max-Margin Demonstration Selection for LLMs, a two-step method that selects hard demonstration examples for the ICL prompt, adapting to each test instance. Our approach achieves 2-7% absolute improvement in F1-score across classification tasks, compared to a random selection of examples. We also provide theoretical insights and empirical evidence showing that MarginSel induces max-margin behavior in LLMs by effectively increasing the margin for hard examples, analogous to support vectors, thereby shifting the decision boundary in a beneficial direction.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† MarginSelï¼Œä¸€ç§é’ˆå¯¹å¤§è¯­è¨€æ¨¡å‹ (LLMs) çš„æœ€å¤§è¾¹è·ç¤ºä¾‹é€‰æ‹©æ–¹æ³•ï¼Œæ—¨åœ¨è§£å†³ä¸Šä¸‹æ–‡å­¦ä¹  (In-context learning, ICL) æ€§èƒ½å¯¹ç¤ºä¾‹é€‰æ‹©åŠé¡ºåºé«˜åº¦æ•æ„Ÿçš„é—®é¢˜ã€‚è¯¥æ–¹æ³•é‡‡ç”¨ä¸¤æ­¥æµç¨‹ï¼Œé’ˆå¯¹æ¯ä¸ªç‰¹å®šçš„æµ‹è¯•å®ä¾‹è‡ªé€‚åº”åœ°ä»å€™é€‰é›†ä¸­æŒ‘é€‰å‡ºå…·æœ‰æŒ‘æˆ˜æ€§çš„å›°éš¾ç¤ºä¾‹ (hard examples) ä»¥æ„å»ºæç¤ºè¯ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œåœ¨å¤šé¡¹åˆ†ç±»ä»»åŠ¡ä¸­ï¼ŒMarginSel ç›¸æ¯”éšæœºé€‰æ‹©ç¤ºä¾‹åœ¨ F1-score ä¸Šå®ç°äº† 2-7% çš„ç»å¯¹æå‡ã€‚é€šè¿‡ç†è®ºåˆ†æä¸å®è¯ç ”ç©¶ï¼Œä½œè€…æ­ç¤ºäº† MarginSel èƒ½å¤Ÿè¯±å¯¼å¤§è¯­è¨€æ¨¡å‹äº§ç”Ÿæœ€å¤§è¾¹è· (max-margin) è¡Œä¸ºã€‚å…¶ä½œç”¨æœºåˆ¶ç±»ä¼¼äºæ”¯æŒå‘é‡ (support vectors)ï¼Œé€šè¿‡æœ‰æ•ˆå¢åŠ å›°éš¾ç¤ºä¾‹çš„è¾¹è·æ¥ä¼˜åŒ–å†³ç­–è¾¹ç•Œï¼Œä»è€Œæ˜¾è‘—æå‡äº†æ¨¡å‹çš„æ³›åŒ–è¡¨ç°ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.06699v1",
      "published_date": "2025-06-07 07:50:01 UTC",
      "updated_date": "2025-06-07 07:50:01 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T19:46:46.643000+00:00"
    },
    {
      "arxiv_id": "2506.06698v1",
      "title": "Contextual Experience Replay for Self-Improvement of Language Agents",
      "title_zh": "é¢å‘è¯­è¨€æ™ºèƒ½ä½“è‡ªæˆ‘æå‡çš„ä¸Šä¸‹æ–‡ç»éªŒå›æ”¾",
      "authors": [
        "Yitao Liu",
        "Chenglei Si",
        "Karthik Narasimhan",
        "Shunyu Yao"
      ],
      "abstract": "Large language model (LLM) agents have been applied to sequential decision-making tasks such as web navigation, but without any environment-specific experiences, they often fail in these complex tasks. Moreover, current LLM agents are not designed to continually learn from past experiences during inference time, which could be crucial for them to gain these environment-specific experiences. To address this, we propose Contextual Experience Replay (CER), a training-free framework to enable efficient self-improvement for language agents in their context window. Specifically, CER accumulates and synthesizes past experiences into a dynamic memory buffer. These experiences encompass environment dynamics and common decision-making patterns, allowing the agents to retrieve and augment themselves with relevant knowledge in new tasks, enhancing their adaptability in complex environments. We evaluate CER on the challenging WebArena and VisualWebArena benchmarks. On VisualWebArena, CER achieves a competitive performance of 31.9%. On WebArena, CER also gets a competitive average success rate of 36.7%, relatively improving the success rate of the GPT-4o agent baseline by 51.0%. We also conduct a comprehensive analysis on it to prove its efficiency, validity and understand it better.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†Contextual Experience Replay (CER)ï¼Œä¸€ç§æ— éœ€è®­ç»ƒ(training-free)çš„æ¡†æ¶ï¼Œæ—¨åœ¨é€šè¿‡ä¸Šä¸‹æ–‡çª—å£å®ç°è¯­è¨€æ™ºèƒ½ä½“(Language Agents)çš„é«˜æ•ˆè‡ªæˆ‘æå‡ã€‚é’ˆå¯¹å¤§è¯­è¨€æ¨¡å‹(LLM)æ™ºèƒ½ä½“åœ¨Webå¯¼èˆªç­‰å¤æ‚å†³ç­–ä»»åŠ¡ä¸­å› ç¼ºä¹ç¯å¢ƒç‰¹å®šç»éªŒè€Œè¡¨ç°ä¸ä½³çš„é—®é¢˜ï¼ŒCERé€šè¿‡æ„å»ºåŠ¨æ€å†…å­˜ç¼“å†²åŒºæ¥ç´¯ç§¯å¹¶åˆæˆè¿‡å¾€ç»éªŒï¼Œæ¶µç›–äº†ç¯å¢ƒåŠ¨åŠ›å­¦å’Œé€šç”¨å†³ç­–æ¨¡å¼ã€‚æ™ºèƒ½ä½“èƒ½å¤Ÿåœ¨å¤„ç†æ–°ä»»åŠ¡æ—¶æ£€ç´¢å¹¶åˆ©ç”¨ç›¸å…³çŸ¥è¯†è¿›è¡Œè‡ªæˆ‘å¢å¼ºï¼Œä»è€Œæ˜¾è‘—æå‡åœ¨å¤æ‚ç¯å¢ƒä¸‹çš„é€‚åº”èƒ½åŠ›ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒCERåœ¨WebArenaå’ŒVisualWebArenaåŸºå‡†æµ‹è¯•ä¸­å‡è¡¨ç°å‡ºæå¼ºçš„ç«äº‰åŠ›ï¼Œå…¶ä¸­åœ¨WebArenaä¸Šçš„æˆåŠŸç‡è¾¾åˆ°36.7%ï¼Œè¾ƒGPT-4oåŸºå‡†æ¨¡å‹ç›¸å¯¹æå‡äº†51.0%ã€‚è¿™é¡¹å·¥ä½œä¸ºæ™ºèƒ½ä½“åœ¨æ¨ç†é˜¶æ®µé€šè¿‡åˆ©ç”¨å†å²ç»éªŒå®ç°æŒç»­è¿›åŒ–æä¾›äº†æœ‰æ•ˆçš„è·¯å¾„ã€‚",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.CV",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "Accepted to ACL 2025. 20 pages",
      "pdf_url": "https://arxiv.org/pdf/2506.06698v1",
      "published_date": "2025-06-07 07:47:35 UTC",
      "updated_date": "2025-06-07 07:47:35 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T19:46:46.090574+00:00"
    },
    {
      "arxiv_id": "2506.06693v1",
      "title": "Design and Implementation of a RISC-V SoC with Custom DSP Accelerators for Edge Computing",
      "title_zh": "é¢å‘è¾¹ç¼˜è®¡ç®—ä¸”é›†æˆå®šåˆ¶åŒ– DSP åŠ é€Ÿå™¨çš„ RISC-V SoC è®¾è®¡ä¸å®ç°",
      "authors": [
        "Priyanshu Yadav"
      ],
      "abstract": "This paper presents a comprehensive analysis of the RISC-V instruction set architecture, focusing on its modular design, implementation challenges, and performance characteristics. We examine the RV32I base instruction set with extensions for multiplication (M) and atomic operations (A). Through cycle-accurate simulation of a pipelined implementation, we evaluate performance metrics including CPI (cycles per instruction) and power efficiency. Our results demonstrate RISC-V's advantages in embedded systems and its scalability for custom accelerators. Comparative analysis shows a 17% reduction in power consumption compared to ARM Cortex-M0 implementations in similar process nodes. The open-standard nature of RISC-V provides significant flexibility for domain-specific optimizations.",
      "tldr_zh": "è¯¥ç ”ç©¶æ·±å…¥æ¢è®¨äº†RISC-VæŒ‡ä»¤é›†æ¶æ„(ISA)çš„æ¨¡å—åŒ–è®¾è®¡ã€å®ç°æŒ‘æˆ˜åŠæ€§èƒ½è¡¨ç°ã€‚ç ”ç©¶å›¢é˜Ÿå®ç°äº†æ”¯æŒä¹˜æ³•(M)å’ŒåŸå­æ“ä½œ(A)æ‰©å±•çš„RV32IåŸºç¡€æŒ‡ä»¤é›†ï¼Œå¹¶åˆ©ç”¨æµæ°´çº¿çš„å‘¨æœŸç²¾ç¡®æ¨¡æ‹Ÿ(cycle-accurate simulation)å¯¹CPIå’ŒåŠŸè€—æ•ˆç‡ç­‰å…³é”®æŒ‡æ ‡è¿›è¡Œäº†è¯„ä¼°ã€‚ç»“æœè¡¨æ˜ï¼ŒRISC-Våœ¨åµŒå…¥å¼ç³»ç»Ÿä¸­å…·æœ‰æ˜¾è‘—ä¼˜åŠ¿ï¼Œä¸”åœ¨é€‚é…å®šåˆ¶åŠ é€Ÿå™¨æ–¹é¢è¡¨ç°å‡ºæé«˜çš„å¯æ‰©å±•æ€§ã€‚ä¸åŒç±»å·¥è‰ºèŠ‚ç‚¹ä¸‹çš„ARM Cortex-M0ç›¸æ¯”ï¼Œè¯¥è®¾è®¡å®ç°äº†17%çš„åŠŸè€—é™ä½ã€‚RISC-Vçš„å¼€æ”¾æ ‡å‡†ç‰¹æ€§ä¸ºé¢†åŸŸç‰¹å®šä¼˜åŒ–(domain-specific optimizations)æä¾›äº†å·¨å¤§çš„çµæ´»æ€§ï¼Œå……åˆ†å±•ç¤ºäº†å…¶åœ¨è¾¹ç¼˜è®¡ç®—å®šåˆ¶åŒ–SoCå¼€å‘ä¸­çš„åº”ç”¨æ½œåŠ›ã€‚",
      "categories": [
        "cs.AR",
        "cs.AI",
        "eess.SP"
      ],
      "primary_category": "cs.AR",
      "comment": "12 Pages, 1 figure",
      "pdf_url": "https://arxiv.org/pdf/2506.06693v1",
      "published_date": "2025-06-07 07:17:40 UTC",
      "updated_date": "2025-06-07 07:17:40 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T19:46:47.445103+00:00"
    },
    {
      "arxiv_id": "2506.11106v2",
      "title": "PankRAG: Enhancing Graph Retrieval via Globally Aware Query Resolution and Dependency-Aware Reranking Mechanism",
      "title_zh": "PankRAGï¼šé€šè¿‡å…¨å±€æ„ŸçŸ¥æŸ¥è¯¢è§£æä¸ä¾èµ–æ„ŸçŸ¥é‡æ’åºæœºåˆ¶å¢å¼ºå›¾æ£€ç´¢",
      "authors": [
        "Ningyuan Li",
        "Junrui Liu",
        "Yi Shan",
        "Minghui Huang",
        "Ziren Gong",
        "Tong Li"
      ],
      "abstract": "Recent graph-based RAG approaches leverage knowledge graphs by extracting entities from a query to fetch their associated relationships and metadata. However, relying solely on entity extraction often results in the misinterpretation or omission of latent critical information and relationships. This can lead to the retrieval of irrelevant or contradictory content, as well as the exclusion of essential information, thereby increasing hallucination risks and undermining the quality of generated responses. In this paper, we propose PankRAG, a framework designed to capture and resolve the latent relationships within complex queries that prior methods overlook. It achieves this through a synergistic combination of a globally-aware hierarchical resolution pathway and a dependency-aware reranking mechanism. PankRAG first generates a globally aware resolution pathway that captures parallel and progress relationships, guiding LLMs to resolve queries through a hierarchical reasoning path. Additionally, its dependency-aware reranking mechanism utilizes resolved sub-question dependencies to augment and validate the retrieved content of the current unresolved sub-question. Experimental results demonstrate that PankRAG consistently outperforms existing state-of-the-art methods, underscoring its generalizability.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† PankRAG æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³ç°æœ‰åŸºäºå›¾çš„ RAG æ–¹æ³•å› è¿‡åº¦ä¾èµ–å®ä½“æå–è€Œå¯¼è‡´çš„æ½œåœ¨å…³é”®ä¿¡æ¯ç¼ºå¤±å’Œå¹»è§‰é£é™©é—®é¢˜ã€‚PankRAG é€šè¿‡ååŒç»“åˆå…¨å±€æ„ŸçŸ¥çš„å±‚æ¬¡åŒ–è§£æè·¯å¾„ (Globally-aware hierarchical resolution pathway) ä¸ä¾èµ–æ„ŸçŸ¥çš„é‡æ’åºæœºåˆ¶ (Dependency-aware reranking mechanism)ï¼Œèƒ½å¤Ÿæœ‰æ•ˆæ•è·å¹¶è§£æå¤æ‚æŸ¥è¯¢ä¸­è¢«ä¼ ç»Ÿæ–¹æ³•å¿½ç•¥çš„å¹¶è¡Œä¸è¿›é˜¶å…³ç³»ã€‚è¯¥æ¡†æ¶é¦–å…ˆç”Ÿæˆå…¨å±€æ„ŸçŸ¥çš„è§£æè·¯å¾„ä»¥æŒ‡å¯¼å¤§è¯­è¨€æ¨¡å‹ (LLMs) è¿›è¡Œå±‚æ¬¡åŒ–æ¨ç†ï¼Œéšååˆ©ç”¨å·²è§£æå­é—®é¢˜çš„ä¾èµ–å…³ç³»å¯¹å½“å‰æ£€ç´¢å†…å®¹è¿›è¡Œå®æ—¶å¢å¼ºä¸éªŒè¯ã€‚å®éªŒç»“æœè¯æ˜ï¼ŒPankRAG åœ¨æ€§èƒ½ä¸ŠæŒç»­ä¼˜äºç°æœ‰çš„å…ˆè¿›æ–¹æ³•ï¼Œæ˜¾è‘—æå‡äº†å›¾æ£€ç´¢çš„è´¨é‡å¹¶å±•ç°äº†æå¼ºçš„é€šç”¨æ€§ (Generalizability)ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.IR"
      ],
      "primary_category": "cs.CL",
      "comment": "Accepted by ICASSP 2026",
      "pdf_url": "https://arxiv.org/pdf/2506.11106v2",
      "published_date": "2025-06-07 07:17:14 UTC",
      "updated_date": "2026-01-21 07:59:43 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T19:47:06.564091+00:00"
    },
    {
      "arxiv_id": "2506.06683v1",
      "title": "RoboPARA: Dual-Arm Robot Planning with Parallel Allocation and Recomposition Across Tasks",
      "title_zh": "RoboPARAï¼šåŸºäºè·¨ä»»åŠ¡å¹¶è¡Œåˆ†é…ä¸é‡ç»„çš„åŒè‡‚æœºå™¨äººè§„åˆ’",
      "authors": [
        "Shiying Duan",
        "Pei Ren",
        "Nanxiang Jiang",
        "Zhengping Che",
        "Jian Tang",
        "Yifan Sun",
        "Zhaoxin Fan",
        "Wenjun Wu"
      ],
      "abstract": "Dual-arm robots play a crucial role in improving efficiency and flexibility in complex multitasking scenarios. While existing methods have achieved promising results in task planning, they often fail to fully optimize task parallelism, limiting the potential of dual-arm collaboration. To address this issue, we propose RoboPARA, a novel large language model (LLM)-driven framework for dual-arm task parallelism planning. RoboPARA employs a two-stage process: (1) Dependency Graph-based Planning Candidates Generation, which constructs directed acyclic graphs (DAGs) to model task dependencies and eliminate redundancy, and (2) Graph Re-Traversal-based Dual-Arm Parallel Planning, which optimizes DAG traversal to maximize parallelism while maintaining task coherence. In addition, we introduce the Cross-Scenario Dual-Arm Parallel Task dataset (X-DAPT dataset), the first dataset specifically designed to evaluate dual-arm task parallelism across diverse scenarios and difficulty levels. Extensive experiments on the X-DAPT dataset demonstrate that RoboPARA significantly outperforms existing methods, achieving higher efficiency and reliability, particularly in complex task combinations. The code and dataset will be released upon acceptance.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†RoboPARAï¼Œè¿™æ˜¯ä¸€ä¸ªç”±å¤§è¯­è¨€æ¨¡å‹(LLM)é©±åŠ¨çš„åŒè‡‚æœºå™¨äººä»»åŠ¡å¹¶è¡Œè§„åˆ’æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³ç°æœ‰æ–¹æ³•åœ¨ä¼˜åŒ–åŒè‡‚åä½œå¹¶è¡Œæ€§æ–¹é¢çš„å±€é™ã€‚RoboPARAé‡‡ç”¨äº†ä¸¤é˜¶æ®µå¤„ç†æµç¨‹ï¼Œé¦–å…ˆé€šè¿‡Dependency Graph-based Planning Candidates Generationæ„å»ºæœ‰å‘æ— ç¯å›¾(DAGs)ä»¥å»ºæ¨¡ä»»åŠ¡ä¾èµ–å¹¶æ¶ˆé™¤å†—ä½™ï¼Œéšåé€šè¿‡Graph Re-Traversal-based Dual-Arm Parallel Planningä¼˜åŒ–å›¾éå†ä»¥åœ¨ä¿æŒä»»åŠ¡è¿è´¯æ€§çš„åŒæ—¶å®ç°å¹¶è¡Œåº¦æœ€å¤§åŒ–ã€‚æ­¤å¤–ï¼Œè¯¥ç ”ç©¶è¿˜å¼•å…¥äº†é¦–ä¸ªä¸“é—¨ç”¨äºè¯„ä¼°åŒè‡‚å¹¶è¡Œä»»åŠ¡çš„è·¨åœºæ™¯æ•°æ®é›†X-DAPT datasetï¼Œæ¶µç›–äº†å¤šç§åœºæ™¯å’Œéš¾åº¦ç­‰çº§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒRoboPARAåœ¨ä»»åŠ¡æ‰§è¡Œæ•ˆç‡å’Œå¯é æ€§æ–¹é¢æ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤„ç†å¤æ‚ä»»åŠ¡ç»„åˆæ—¶å±•ç°äº†æå¼ºçš„ä¼˜åŠ¿ã€‚è¯¥é¡¹å·¥ä½œä¸ºæå‡åŒè‡‚æœºå™¨äººåœ¨å¤æ‚å¤šä»»åŠ¡åœºæ™¯ä¸‹çš„åä½œæ•ˆç‡æä¾›äº†æ–°çš„æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.06683v1",
      "published_date": "2025-06-07 06:46:24 UTC",
      "updated_date": "2025-06-07 06:46:24 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T19:47:04.033286+00:00"
    },
    {
      "arxiv_id": "2506.06659v3",
      "title": "DriveSuprim: Towards Precise Trajectory Selection for End-to-End Planning",
      "title_zh": "DriveSuprimï¼šé¢å‘ç«¯åˆ°ç«¯è§„åˆ’çš„ç²¾å‡†è½¨è¿¹é€‰æ‹©",
      "authors": [
        "Wenhao Yao",
        "Zhenxin Li",
        "Shiyi Lan",
        "Zi Wang",
        "Xinglong Sun",
        "Jose M. Alvarez",
        "Zuxuan Wu"
      ],
      "abstract": "Autonomous vehicles must navigate safely in complex driving environments. Imitating a single expert trajectory, as in regression-based approaches, usually does not explicitly assess the safety of the predicted trajectory. Selection-based methods address this by generating and scoring multiple trajectory candidates and predicting the safety score for each. However, they face optimization challenges in precisely selecting the best option from thousands of candidates and distinguishing subtle but safety-critical differences, especially in rare and challenging scenarios. We propose DriveSuprim to overcome these challenges and advance the selection-based paradigm through a coarse-to-fine paradigm for progressive candidate filtering, a rotation-based augmentation method to improve robustness in out-of-distribution scenarios, and a self-distillation framework to stabilize training. DriveSuprim achieves state-of-the-art performance, reaching 93.5% PDMS in NAVSIM v1 and 87.1% EPDMS in NAVSIM v2 without extra data, with 83.02 Driving Score and 60.00 Success Rate on the Bench2Drive benchmark, demonstrating superior planning capabilities in various driving scenarios.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† DriveSuprimï¼Œæ—¨åœ¨è§£å†³è‡ªåŠ¨é©¾é©¶ç«¯åˆ°ç«¯è§„åˆ’ä¸­ selection-based æ–¹æ³•éš¾ä»¥ä»æ•°åƒä¸ªå€™é€‰è½¨è¿¹ä¸­ç²¾ç¡®é€‰æ‹©æœ€ä½³é€‰é¡¹ï¼Œå¹¶åŒºåˆ†ç»†å¾®å®‰å…¨å·®å¼‚çš„æŒ‘æˆ˜ã€‚ä¸ºäº†å…‹æœè¿™äº›ä¼˜åŒ–éš¾é¢˜ï¼ŒDriveSuprim å¼•å…¥äº† coarse-to-fine paradigm è¿›è¡Œæ¸è¿›å¼å€™é€‰è¿‡æ»¤ï¼Œå¹¶é‡‡ç”¨ rotation-based augmentation å¢å¼ºç³»ç»Ÿåœ¨åˆ†å¸ƒå¤– (out-of-distribution) åœºæ™¯ä¸‹çš„é²æ£’æ€§ã€‚æ­¤å¤–ï¼Œè¯¥æ¡†æ¶è¿˜ç»“åˆäº† self-distillation æ¶æ„ä»¥æå‡è®­ç»ƒè¿‡ç¨‹çš„ç¨³å®šæ€§ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒDriveSuprim åœ¨ä¸ä½¿ç”¨é¢å¤–æ•°æ®çš„æƒ…å†µä¸‹ï¼Œåœ¨ NAVSIM v1 å’Œ v2 æ¦œå•ä¸Šåˆ†åˆ«è¾¾åˆ°äº† 93.5% PDMS å’Œ 87.1% EPDMS çš„é¢†å…ˆæˆç»©ã€‚åŒæ—¶ï¼Œè¯¥æ¨¡å‹åœ¨ Bench2Drive åŸºå‡†æµ‹è¯•ä¸­å–å¾—äº† 83.02 çš„ Driving Score å’Œ 60.00 çš„ Success Rateï¼Œè¯æ˜äº†å…¶åœ¨å„ç§å¤æ‚é©¾é©¶åœºæ™¯ä¸­å…·å¤‡å“è¶Šçš„è§„åˆ’èƒ½åŠ›ä¸å®‰å…¨æ€§ã€‚",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.RO",
      "comment": "Accepted to AAAI 2026",
      "pdf_url": "https://arxiv.org/pdf/2506.06659v3",
      "published_date": "2025-06-07 04:39:06 UTC",
      "updated_date": "2025-11-24 03:32:51 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T19:47:07.903965+00:00"
    },
    {
      "arxiv_id": "2506.06658v1",
      "title": "Self-Adapting Improvement Loops for Robotic Learning",
      "title_zh": "æœºå™¨äººå­¦ä¹ çš„è‡ªé€‚åº”æ”¹è¿›å¾ªç¯",
      "authors": [
        "Calvin Luo",
        "Zilai Zeng",
        "Mingxi Jia",
        "Yilun Du",
        "Chen Sun"
      ],
      "abstract": "Video generative models trained on expert demonstrations have been utilized as performant text-conditioned visual planners for solving robotic tasks. However, generalization to unseen tasks remains a challenge. Whereas improved generalization may be facilitated by leveraging learned prior knowledge from additional pre-collected offline data sources, such as web-scale video datasets, in the era of experience we aim to design agents that can continuously improve in an online manner from self-collected behaviors. In this work we thus propose the Self-Adapting Improvement Loop (SAIL), where an in-domain video model iteratively updates itself on self-produced trajectories, collected through adaptation with an internet-scale pretrained video model, and steadily improves its performance for a specified task of interest. We apply SAIL to a diverse suite of MetaWorld tasks, as well as two manipulation tasks on a real robot arm, and find that performance improvements continuously emerge over multiple iterations for novel tasks initially unseen during original in-domain video model training. Furthermore, we discover that SAIL is surprisingly robust regarding if and how the self-collected experience is filtered, and the quality of the initial in-domain demonstrations. Through adaptation with summarized internet-scale data, and learning through online experience, we thus demonstrate a way to iteratively bootstrap a high-performance video model for solving novel robotic tasks through self-improvement.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†è‡ªé€‚åº”æ”¹è¿›å¾ªç¯(Self-Adapting Improvement Loop, SAIL)ï¼Œæ—¨åœ¨è§£å†³è§†é¢‘ç”Ÿæˆæ¨¡å‹ä½œä¸ºè§†è§‰è§„åˆ’å™¨åœ¨å¤„ç†æœªè§æœºå™¨äººä»»åŠ¡æ—¶æ³›åŒ–èƒ½åŠ›ä¸è¶³çš„é—®é¢˜ã€‚SAILæ¡†æ¶é€šè¿‡åˆ©ç”¨äº’è”ç½‘è§„æ¨¡(internet-scale)çš„é¢„è®­ç»ƒè§†é¢‘æ¨¡å‹å¼•å¯¼æ™ºèƒ½ä½“è¿›è¡Œè‡ªé€‚åº”ï¼Œä½¿å…¶èƒ½å¤Ÿåœ¨çº¿æ”¶é›†è¡Œä¸ºå¹¶ç”Ÿæˆè‡ªäº§è½¨è¿¹ã€‚é¢†åŸŸå†…(in-domain)çš„è§†é¢‘æ¨¡å‹åœ¨è¿™äº›è‡ªäº§è½¨è¿¹ä¸Šè¿›è¡Œè¿­ä»£è‡ªæˆ‘æ›´æ–°ï¼Œä»è€Œé’ˆå¯¹ç‰¹å®šæ„Ÿå…´è¶£çš„ä»»åŠ¡ä¸æ–­ç¨³æ­¥ä¼˜åŒ–æ€§èƒ½ã€‚åœ¨ MetaWorld ä»»åŠ¡å¥—ä»¶åŠçœŸå®æœºæ¢°è‡‚æ“çºµä»»åŠ¡ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œå¯¹äºåˆå§‹æœªè§çš„ä»»åŠ¡ï¼Œæ¨¡å‹çš„æ€§èƒ½åœ¨å¤šä¸ªè¿­ä»£å‘¨æœŸä¸­å‘ˆç°å‡ºæŒç»­å¢é•¿ã€‚ç ”ç©¶è¿›ä¸€æ­¥å‘ç°ï¼ŒSAIL å¯¹äºè‡ªæ”¶é›†ç»éªŒçš„è¿‡æ»¤æ–¹å¼ä»¥åŠåˆå§‹é¢†åŸŸæ¼”ç¤ºçš„è´¨é‡è¡¨ç°å‡ºæ˜¾è‘—çš„é²æ£’æ€§ã€‚é€šè¿‡ç»“åˆäº’è”ç½‘è§„æ¨¡æ•°æ®ä¸åœ¨çº¿ç»éªŒå­¦ä¹ ï¼Œè¯¥ç ”ç©¶è¯æ˜äº†åˆ©ç”¨è‡ªæˆ‘æ”¹è¿›æœºåˆ¶è¿­ä»£å¼•å¯¼å‡ºé«˜æ€§èƒ½è§†é¢‘æ¨¡å‹ä»¥è§£å†³æ–°å‹æœºå™¨äººä»»åŠ¡çš„æœ‰æ•ˆè·¯å¾„ã€‚",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.06658v1",
      "published_date": "2025-06-07 04:34:37 UTC",
      "updated_date": "2025-06-07 04:34:37 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T19:47:16.672099+00:00"
    },
    {
      "arxiv_id": "2506.06657v1",
      "title": "Quantile Regression with Large Language Models for Price Prediction",
      "title_zh": "åŸºäºå¤§è¯­è¨€æ¨¡å‹çš„åˆ†ä½æ•°å›å½’ä»·æ ¼é¢„æµ‹",
      "authors": [
        "Nikhita Vedula",
        "Dushyanta Dhyani",
        "Laleh Jalali",
        "Boris Oreshkin",
        "Mohsen Bayati",
        "Shervin Malmasi"
      ],
      "abstract": "Large Language Models (LLMs) have shown promise in structured prediction tasks, including regression, but existing approaches primarily focus on point estimates and lack systematic comparison across different methods. We investigate probabilistic regression using LLMs for unstructured inputs, addressing challenging text-to-distribution prediction tasks such as price estimation where both nuanced text understanding and uncertainty quantification are critical. We propose a novel quantile regression approach that enables LLMs to produce full predictive distributions, improving upon traditional point estimates. Through extensive experiments across three diverse price prediction datasets, we demonstrate that a Mistral-7B model fine-tuned with quantile heads significantly outperforms traditional approaches for both point and distributional estimations, as measured by three established metrics each for prediction accuracy and distributional calibration. Our systematic comparison of LLM approaches, model architectures, training approaches, and data scaling reveals that Mistral-7B consistently outperforms encoder architectures, embedding-based methods, and few-shot learning methods. Our experiments also reveal the effectiveness of LLM-assisted label correction in achieving human-level accuracy without systematic bias. Our curated datasets are made available at https://github.com/vnik18/llm-price-quantile-reg/ to support future research.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†åˆ©ç”¨å¤§è¯­è¨€æ¨¡å‹(Large Language Models, LLMs)è¿›è¡Œåˆ†ä½æ•°å›å½’(Quantile Regression)çš„æ–°æ–¹æ³•ï¼Œæ—¨åœ¨è§£å†³ä»·æ ¼é¢„æµ‹ä¸­ä¼ ç»Ÿç‚¹ä¼°è®¡(point estimates)ç¼ºä¹ä¸ç¡®å®šæ€§é‡åŒ–(uncertainty quantification)çš„é—®é¢˜ã€‚é€šè¿‡å¯¹Mistral-7Bæ¨¡å‹è¿›è¡Œå¸¦æœ‰åˆ†ä½æ•°å¤´éƒ¨(quantile heads)çš„å¾®è°ƒï¼Œè¯¥æ¡†æ¶èƒ½å¤Ÿä»éç»“æ„åŒ–æ–‡æœ¬ä¸­é¢„æµ‹å‡ºå®Œæ•´çš„æ¦‚ç‡åˆ†å¸ƒï¼Œæ˜¾è‘—æå‡äº†æ¨¡å‹å¯¹å¤æ‚ä»·æ ¼æ³¢åŠ¨çš„ç†è§£èƒ½åŠ›ã€‚åœ¨ä¸‰ä¸ªå¤šæ ·åŒ–æ•°æ®é›†ä¸Šçš„å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯¥æ–¹æ³•åœ¨é¢„æµ‹å‡†ç¡®ç‡å’Œåˆ†å¸ƒæ ¡å‡†(distributional calibration)æ–¹é¢å‡ä¼˜äºä¼ ç»Ÿçš„ç¼–ç å™¨æ¶æ„(encoder architectures)ã€åŸºäºåµŒå…¥çš„æ–¹æ³•(embedding-based methods)åŠå°‘æ ·æœ¬å­¦ä¹ (few-shot learning)æ–¹æ³•ã€‚æ­¤å¤–ï¼Œç ”ç©¶è¿˜å‘ç°åˆ©ç”¨LLMè¾…åŠ©çš„æ ‡ç­¾æ ¡æ­£(label correction)å¯ä»¥å®ç°äººç±»æ°´å¹³çš„å‡†ç¡®åº¦ä¸”æ— ç³»ç»Ÿåå·®ã€‚è¯¥é¡¹å·¥ä½œä¸ºLLMåœ¨ç»“æ„åŒ–é¢„æµ‹å’Œæ¦‚ç‡å›å½’ä»»åŠ¡ä¸­çš„åº”ç”¨æä¾›äº†ç³»ç»Ÿæ€§çš„å¯¹æ¯”åˆ†æä¸å¼€æºæ•°æ®é›†æ”¯æŒã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "Accepted to Findings of ACL, 2025",
      "pdf_url": "https://arxiv.org/pdf/2506.06657v1",
      "published_date": "2025-06-07 04:19:28 UTC",
      "updated_date": "2025-06-07 04:19:28 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T19:47:22.492453+00:00"
    },
    {
      "arxiv_id": "2506.06637v1",
      "title": "Non-Intrusive Load Monitoring Based on Image Load Signatures and Continual Learning",
      "title_zh": "åŸºäºå›¾åƒè´Ÿè·ç‰¹å¾ä¸æŒç»­å­¦ä¹ çš„éä¾µå…¥å¼è´Ÿè·ç›‘æµ‹",
      "authors": [
        "Olimjon Toirov",
        "Wei Yu"
      ],
      "abstract": "Non-Intrusive Load Monitoring (NILM) identifies the operating status and energy consumption of each electrical device in the circuit by analyzing the electrical signals at the bus, which is of great significance for smart power management. However, the complex and changeable load combinations and application environments lead to the challenges of poor feature robustness and insufficient model generalization of traditional NILM methods. To this end, this paper proposes a new non-intrusive load monitoring method that integrates \"image load signature\" and continual learning. This method converts multi-dimensional power signals such as current, voltage, and power factor into visual image load feature signatures, and combines deep convolutional neural networks to realize the identification and classification of multiple devices; at the same time, self-supervised pre-training is introduced to improve feature generalization, and continual online learning strategies are used to overcome model forgetting to adapt to the emergence of new loads. This paper conducts a large number of experiments on high-sampling rate load datasets, and compares a variety of existing methods and model variants. The results show that the proposed method has achieved significant improvements in recognition accuracy.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§ç»“åˆå›¾åƒè´Ÿè·ç­¾å(Image Load Signatures)ä¸æŒç»­å­¦ä¹ (Continual Learning)çš„éä¾µå…¥å¼è´Ÿè·ç›‘æµ‹(Non-Intrusive Load Monitoring, NILM)æ–¹æ³•ï¼Œæ—¨åœ¨è§£å†³ä¼ ç»Ÿæ–¹æ³•åœ¨å¤æ‚è´Ÿè½½ç»„åˆä¸‹ç‰¹å¾é²æ£’æ€§å·®å’Œæ³›åŒ–ä¸è¶³çš„é—®é¢˜ã€‚è¯¥æ–¹æ³•å°†ç”µæµã€ç”µå‹åŠåŠŸç‡å› æ•°ç­‰ç”µä¿¡å·è½¬åŒ–ä¸ºå¯è§†åŒ–çš„å›¾åƒè´Ÿè·ç‰¹å¾ç­¾åï¼Œå¹¶åˆ©ç”¨æ·±åº¦å·ç§¯ç¥ç»ç½‘ç»œ(Deep Convolutional Neural Networks)å®ç°å¯¹å¤šè®¾å¤‡çš„ç²¾ç¡®è¯†åˆ«ä¸åˆ†ç±»ã€‚ç ”ç©¶é€šè¿‡å¼•å…¥è‡ªç›‘ç£é¢„è®­ç»ƒ(Self-supervised pre-training)æå‡äº†ç‰¹å¾çš„æ³›åŒ–èƒ½åŠ›ï¼Œå¹¶é‡‡ç”¨åœ¨çº¿æŒç»­å­¦ä¹ ç­–ç•¥æœ‰æ•ˆå…‹æœäº†æ¨¡å‹é—å¿˜é—®é¢˜ï¼Œä½¿å…¶èƒ½å¤Ÿé€‚åº”æ–°è´Ÿè½½çš„åŠ¨æ€å‡ºç°ã€‚åœ¨å¤§è§„æ¨¡é«˜é‡‡æ ·ç‡è´Ÿè·æ•°æ®é›†ä¸Šçš„å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯¥æ–¹æ³•åœ¨è¯†åˆ«å‡†ç¡®ç‡ä¸Šè¾ƒç°æœ‰æŠ€æœ¯æœ‰æ˜¾è‘—æå‡ã€‚è¯¥é¡¹å·¥ä½œä¸ºå®ç°æ™ºèƒ½ç”µåŠ›ç®¡ç†ä¸­çš„é«˜æ•ˆã€å¯æ‰©å±•è®¾å¤‡ç›‘æµ‹æä¾›äº†é‡è¦çš„æŠ€æœ¯æ”¯æ’‘ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CV",
        "eess.SP"
      ],
      "primary_category": "cs.LG",
      "comment": "10 pages, 3 figures, 2025 2nd International Conference on Digital Society and Artificial Intelligence (DSAI 2025), Conference dates: May 23-25, 2025",
      "pdf_url": "https://arxiv.org/pdf/2506.06637v1",
      "published_date": "2025-06-07 03:13:15 UTC",
      "updated_date": "2025-06-07 03:13:15 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T19:47:24.583160+00:00"
    },
    {
      "arxiv_id": "2506.06634v1",
      "title": "GELD: A Unified Neural Model for Efficiently Solving Traveling Salesman Problems Across Different Scales",
      "title_zh": "GELDï¼šä¸€ç§é¢å‘ä¸åŒè§„æ¨¡æ—…è¡Œå•†é—®é¢˜é«˜æ•ˆæ±‚è§£çš„ç»Ÿä¸€ç¥ç»ç½‘ç»œæ¨¡å‹",
      "authors": [
        "Yubin Xiao",
        "Di Wang",
        "Rui Cao",
        "Xuan Wu",
        "Boyang Li",
        "You Zhou"
      ],
      "abstract": "The Traveling Salesman Problem (TSP) is a well-known combinatorial optimization problem with broad real-world applications. Recent advancements in neural network-based TSP solvers have shown promising results. Nonetheless, these models often struggle to efficiently solve both small- and large-scale TSPs using the same set of pre-trained model parameters, limiting their practical utility. To address this issue, we introduce a novel neural TSP solver named GELD, built upon our proposed broad global assessment and refined local selection framework. Specifically, GELD integrates a lightweight Global-view Encoder (GE) with a heavyweight Local-view Decoder (LD) to enrich embedding representation while accelerating the decision-making process. Moreover, GE incorporates a novel low-complexity attention mechanism, allowing GELD to achieve low inference latency and scalability to larger-scale TSPs. Additionally, we propose a two-stage training strategy that utilizes training instances of different sizes to bolster GELD's generalization ability. Extensive experiments conducted on both synthetic and real-world datasets demonstrate that GELD outperforms seven state-of-the-art models considering both solution quality and inference speed. Furthermore, GELD can be employed as a post-processing method to significantly elevate the quality of the solutions derived by existing neural TSP solvers via spending affordable additional computing time. Notably, GELD is shown as capable of solving TSPs with up to 744,710 nodes, first-of-its-kind to solve this large size TSP without relying on divide-and-conquer strategies to the best of our knowledge.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ç¥ç»ç½‘ç»œæ±‚è§£å™¨åœ¨å¤„ç†ä¸åŒè§„æ¨¡æ—…è¡Œå•†é—®é¢˜(TSP)æ—¶æ•ˆç‡ä¸ä¸€ä¸”é€šç”¨æ€§å·®çš„æŒ‘æˆ˜ï¼Œæå‡ºäº†åä¸ºGELDçš„ç»Ÿä¸€ç¥ç»æ¨¡å‹ã€‚GELDåŸºäºå…¨å±€è¯„ä¼°ä¸å±€éƒ¨ç²¾ç»†é€‰æ‹©æ¡†æ¶ï¼Œé€šè¿‡é›†æˆè½»é‡çº§çš„Global-view Encoder (GE)å’Œé‡é‡çº§çš„Local-view Decoder (LD)ï¼Œåœ¨å¢å¼ºåµŒå…¥è¡¨ç¤º(Embedding representation)çš„åŒæ—¶æ˜¾è‘—åŠ é€Ÿäº†å†³ç­–è¿‡ç¨‹ã€‚å…¶ä¸­ï¼ŒGEå¼•å…¥äº†ä¸€ç§ä½å¤æ‚åº¦çš„æ³¨æ„åŠ›æœºåˆ¶(Attention mechanism)ï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿå®ç°ä½æ¨ç†å»¶è¿Ÿå¹¶å…·å¤‡è‰¯å¥½çš„æ‰©å±•æ€§ã€‚é€šè¿‡é‡‡ç”¨åˆ©ç”¨ä¸åŒè§„æ¨¡è®­ç»ƒå®ä¾‹çš„ä¸¤é˜¶æ®µè®­ç»ƒç­–ç•¥ï¼Œè¯¥æ¨¡å‹è¿›ä¸€æ­¥å¢å¼ºäº†æ³›åŒ–èƒ½åŠ›(Generalization ability)ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒGELDåœ¨æ±‚è§£è´¨é‡å’Œæ¨ç†é€Ÿåº¦ä¸Šå‡ä¼˜äºä¸ƒç§æœ€å…ˆè¿›(SOTA)çš„æ¨¡å‹ï¼Œå¹¶å¯ä½œä¸ºåå¤„ç†æ‰‹æ®µæå‡ç°æœ‰æ±‚è§£å™¨çš„æ€§èƒ½ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œè¯¥æ¨¡å‹åœ¨ä¸ä¾èµ–åˆ†æ²»ç­–ç•¥(Divide-and-conquer)çš„æƒ…å†µä¸‹æˆåŠŸæ±‚è§£äº†å¤šè¾¾744,710ä¸ªèŠ‚ç‚¹çš„TSPé—®é¢˜ï¼Œå±•ç¤ºäº†å“è¶Šçš„æ‰©å±•æ½œåŠ›å’Œå®ç”¨ä»·å€¼ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "21pages, 4 figures, and 14 tables",
      "pdf_url": "https://arxiv.org/pdf/2506.06634v1",
      "published_date": "2025-06-07 03:00:05 UTC",
      "updated_date": "2025-06-07 03:00:05 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T19:47:34.345384+00:00"
    },
    {
      "arxiv_id": "2506.06632v2",
      "title": "Curriculum Reinforcement Learning from Easy to Hard Tasks Improves LLM Reasoning",
      "title_zh": "åŸºäºä»æ˜“åˆ°éš¾ä»»åŠ¡çš„è¯¾ç¨‹å¼ºåŒ–å­¦ä¹ æå‡å¤§è¯­è¨€æ¨¡å‹æ¨ç†èƒ½åŠ›",
      "authors": [
        "Shubham Parashar",
        "Shurui Gui",
        "Xiner Li",
        "Hongyi Ling",
        "Sushil Vemuri",
        "Blake Olson",
        "Eric Li",
        "Yu Zhang",
        "James Caverlee",
        "Dileep Kalathil",
        "Shuiwang Ji"
      ],
      "abstract": "We aim to improve the reasoning capabilities of language models via reinforcement learning (RL). Recent RL post-trained models like DeepSeek-R1 have demonstrated reasoning abilities on mathematical and coding tasks. However, prior studies suggest that using RL alone to improve reasoning on inherently difficult tasks is less effective. Here, we draw inspiration from curriculum learning and propose to schedule tasks from easy to hard (E2H), allowing LLMs to build reasoning skills gradually. Our method is termed E2H Reasoner. Empirically, we observe that, although easy tasks are important initially, fading them out through appropriate scheduling is essential in preventing overfitting. Theoretically, we establish convergence guarantees for E2H Reasoner within an approximate policy iteration framework. We derive finite-sample complexity bounds and show that when tasks are appropriately decomposed and conditioned, learning through curriculum stages requires fewer total samples than direct learning. Experiments across multiple domains show that E2H Reasoner significantly improves the reasoning ability of small LLMs (1.5B to 3B), which otherwise struggle when trained with vanilla RL alone, highlighting the effectiveness of our method. Our code can be found on https://github.com/divelab/E2H-Reasoning.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¼ºåŒ–å­¦ä¹ (Reinforcement Learning)åœ¨å¤„ç†å¤æ‚æ¨ç†ä»»åŠ¡æ—¶æ•ˆæœæœ‰é™çš„é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§åä¸ºE2H Reasonerçš„è¯¾ç¨‹å­¦ä¹ æ–¹æ³•ã€‚è¯¥æ–¹æ³•é€šè¿‡ä»æ˜“åˆ°éš¾(Easy to Hard)çš„ä»»åŠ¡è°ƒåº¦ï¼Œå¼•å¯¼å¤§è¯­è¨€æ¨¡å‹(LLMs)é€æ­¥æ„å»ºæ¨ç†èƒ½åŠ›ã€‚ç ”ç©¶è§‚å¯Ÿåˆ°ï¼Œè™½ç„¶åˆæœŸç®€å•ä»»åŠ¡éå¸¸å…³é”®ï¼Œä½†é€šè¿‡åˆç†çš„è°ƒåº¦ç­–ç•¥å°†å…¶é€æ­¥æ·¡å‡º(Fading out)å¯¹äºé˜²æ­¢è¿‡æ‹Ÿåˆ(Overfitting)è‡³å…³é‡è¦ã€‚åœ¨ç†è®ºä¸Šï¼Œè¯¥ç ”ç©¶åœ¨è¿‘ä¼¼ç­–ç•¥è¿­ä»£(Approximate Policy Iteration)æ¡†æ¶ä¸‹æä¾›äº†æ”¶æ•›æ€§ä¿è¯ï¼Œå¹¶è¯æ˜äº†è¯¾ç¨‹å­¦ä¹ æ¯”ç›´æ¥å­¦ä¹ å…·æœ‰æ›´ä¼˜çš„æ ·æœ¬å¤æ‚åº¦ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒE2H Reasoneræ˜¾è‘—å¢å¼ºäº†1.5Bè‡³3Bè§„æ¨¡å°æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ï¼Œæœ‰æ•ˆè§£å†³äº†å°å‚æ•°æ¨¡å‹åœ¨ä¼ ç»Ÿå¼ºåŒ–å­¦ä¹ è®­ç»ƒä¸‹éš¾ä»¥æå‡æ¨ç†æ€§èƒ½çš„å›°å¢ƒã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.06632v2",
      "published_date": "2025-06-07 02:41:54 UTC",
      "updated_date": "2025-11-02 04:59:03 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T19:47:31.490718+00:00"
    },
    {
      "arxiv_id": "2506.06630v1",
      "title": "Active Test-time Vision-Language Navigation",
      "title_zh": "æµ‹è¯•æ—¶ä¸»åŠ¨è§†è§‰è¯­è¨€å¯¼èˆª",
      "authors": [
        "Heeju Ko",
        "Sungjune Kim",
        "Gyeongrok Oh",
        "Jeongyoon Yoon",
        "Honglak Lee",
        "Sujin Jang",
        "Seungryong Kim",
        "Sangpil Kim"
      ],
      "abstract": "Vision-Language Navigation (VLN) policies trained on offline datasets often exhibit degraded task performance when deployed in unfamiliar navigation environments at test time, where agents are typically evaluated without access to external interaction or feedback. Entropy minimization has emerged as a practical solution for reducing prediction uncertainty at test time; however, it can suffer from accumulated errors, as agents may become overconfident in incorrect actions without sufficient contextual grounding. To tackle these challenges, we introduce ATENA (Active TEst-time Navigation Agent), a test-time active learning framework that enables a practical human-robot interaction via episodic feedback on uncertain navigation outcomes. In particular, ATENA learns to increase certainty in successful episodes and decrease it in failed ones, improving uncertainty calibration. Here, we propose mixture entropy optimization, where entropy is obtained from a combination of the action and pseudo-expert distributions-a hypothetical action distribution assuming the agent's selected action to be optimal-controlling both prediction confidence and action preference. In addition, we propose a self-active learning strategy that enables an agent to evaluate its navigation outcomes based on confident predictions. As a result, the agent stays actively engaged throughout all iterations, leading to well-grounded and adaptive decision-making. Extensive evaluations on challenging VLN benchmarks-REVERIE, R2R, and R2R-CE-demonstrate that ATENA successfully overcomes distributional shifts at test time, outperforming the compared baseline methods across various settings.",
      "tldr_zh": "è¿™é¡¹ç ”ç©¶æå‡ºäº†ATENAï¼Œä¸€ç§é’ˆå¯¹è§†è§‰è¯­è¨€å¯¼èˆª(Vision-Language Navigation, VLN)çš„æµ‹è¯•æ—¶ä¸»åŠ¨å­¦ä¹ æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³é¢„è®­ç»ƒæ¨¡å‹åœ¨é™Œç”Ÿæµ‹è¯•ç¯å¢ƒä¸‹å› åˆ†å¸ƒåç§»å¯¼è‡´çš„æ€§èƒ½ä¸‹é™é—®é¢˜ã€‚é’ˆå¯¹ä¼ ç»Ÿç†µæœ€å°åŒ–æ–¹æ³•å®¹æ˜“äº§ç”Ÿé”™è¯¯ç´¯ç§¯å’Œè¿‡åº¦è‡ªä¿¡çš„ç¼ºé™·ï¼ŒATENAé€šè¿‡æƒ…å¢ƒåé¦ˆæ¥æ ¡å‡†å¯¼èˆªç»“æœçš„ç¡®å®šæ€§ï¼Œæé«˜ä¸ç¡®å®šæ€§ä¼°è®¡çš„å‡†ç¡®åº¦ã€‚è¯¥æ¡†æ¶æ ¸å¿ƒå¼•å…¥äº†æ··åˆç†µä¼˜åŒ–(mixture entropy optimization)æŠ€æœ¯ï¼Œé€šè¿‡ç»“åˆåŠ¨ä½œåˆ†å¸ƒä¸ä¼ªä¸“å®¶åˆ†å¸ƒæ¥åŒæ—¶æ§åˆ¶é¢„æµ‹ä¿¡å¿ƒå’ŒåŠ¨ä½œåå¥½ã€‚æ­¤å¤–ï¼ŒATENAè¿˜é‡‡ç”¨äº†ä¸€ç§è‡ªä¸»åŠ¨å­¦ä¹ ç­–ç•¥(self-active learning strategy)ï¼Œä½¿æ™ºèƒ½ä½“èƒ½å¤ŸåŸºäºç½®ä¿¡é¢„æµ‹è‡ªæˆ‘è¯„ä¼°å¯¼èˆªæˆæœï¼Œä»è€Œå®ç°æ›´å…·é€‚åº”æ€§çš„å†³ç­–ã€‚åœ¨REVERIEã€R2Rå’ŒR2R-CEç­‰å¤šä¸ªå…·æœ‰æŒ‘æˆ˜æ€§çš„VLNåŸºå‡†æµ‹è¯•ä¸Šçš„å®éªŒè¯æ˜ï¼ŒATENAèƒ½æœ‰æ•ˆå…‹æœæµ‹è¯•æ—¶çš„ç¯å¢ƒå·®å¼‚ï¼Œå…¶æ€§èƒ½è¡¨ç°æ˜¾è‘—ä¼˜äºç°æœ‰çš„åŸºçº¿æ–¹æ³•ã€‚",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.06630v1",
      "published_date": "2025-06-07 02:24:44 UTC",
      "updated_date": "2025-06-07 02:24:44 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T19:47:31.787871+00:00"
    },
    {
      "arxiv_id": "2506.06622v2",
      "title": "QuantMCP: Grounding Large Language Models in Verifiable Financial Reality",
      "title_zh": "QuantMCPï¼šå°†å¤§è¯­è¨€æ¨¡å‹é”šå®šäºå¯éªŒè¯çš„é‡‘èç°å®",
      "authors": [
        "Yifan Zeng"
      ],
      "abstract": "Large Language Models (LLMs) hold immense promise for revolutionizing financial analysis and decision-making, yet their direct application is often hampered by issues of data hallucination and lack of access to real-time, verifiable financial information. This paper introduces QuantMCP, a novel framework designed to rigorously ground LLMs in financial reality. By leveraging the Model Context Protocol (MCP) for standardized and secure tool invocation, QuantMCP enables LLMs to accurately interface with a diverse array of Python-accessible financial data APIs (e.g., Wind, yfinance). Users can interact via natural language to precisely retrieve up-to-date financial data, thereby overcoming LLM's inherent limitations in factual data recall. More critically, once furnished with this verified, structured data, the LLM's analytical capabilities are unlocked, empowering it to perform sophisticated data interpretation, generate insights, and ultimately support more informed financial decision-making processes. QuantMCP provides a robust, extensible, and secure bridge between conversational AI and the complex world of financial data, aiming to enhance both the reliability and the analytical depth of LLM applications in finance.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† QuantMCPï¼Œè¿™æ˜¯ä¸€ä¸ªæ—¨åœ¨å°†å¤§è¯­è¨€æ¨¡å‹ (LLMs) ä¸å¯éªŒè¯çš„é‡‘èç°å®ä¸¥å¯†ç»“åˆçš„æ–°å‹æ¡†æ¶ã€‚ä¸ºäº†è§£å†³ LLMs åœ¨é‡‘èåˆ†æä¸­é¢ä¸´çš„æ•°æ®å¹»è§‰ (hallucination) ä»¥åŠç¼ºä¹å®æ—¶ã€å¯éªŒè¯ä¿¡æ¯çš„é—®é¢˜ï¼Œè¯¥æ¡†æ¶åˆ©ç”¨æ¨¡å‹ä¸Šä¸‹æ–‡åè®® (Model Context Protocol, MCP) å®ç°äº†æ ‡å‡†åŒ–çš„å®‰å…¨å·¥å…·è°ƒç”¨ã€‚QuantMCP ä½¿ LLMs èƒ½å¤Ÿå‡†ç¡®å¯¹æ¥ Wind å’Œ yfinance ç­‰å¤šç§ Python é‡‘èæ•°æ®æ¥å£ï¼Œæ”¯æŒé€šè¿‡è‡ªç„¶è¯­è¨€äº¤äº’å®ç°ç²¾ç¡®çš„å®æ—¶æ•°æ®æ£€ç´¢ã€‚åœ¨è·å–è¿™äº›ç»è¿‡éªŒè¯çš„ç»“æ„åŒ–æ•°æ®åï¼Œè¯¥æ¡†æ¶è¿›ä¸€æ­¥è§£é”äº† LLMs çš„åˆ†ææ½œåŠ›ï¼Œä½¿å…¶èƒ½è¿›è¡Œå¤æ‚çš„æ•°æ®è§£è¯»å¹¶ç”Ÿæˆæ·±åº¦æ´è§ã€‚è¯¥ç³»ç»Ÿä¸ä»…å…‹æœäº†æ¨¡å‹åœ¨äº‹å®æ•°æ®å¬å›æ–¹é¢çš„å›ºæœ‰å±€é™ï¼Œè¿˜ä¸ºå¯¹è¯å¼äººå·¥æ™ºèƒ½ä¸å¤æ‚é‡‘èé¢†åŸŸä¹‹é—´æ­å»ºäº†ç¨³å¥ã€å¯æ‰©å±•ä¸”å®‰å…¨çš„æ¡¥æ¢ã€‚æœ€ç»ˆï¼ŒQuantMCP æ˜¾è‘—å¢å¼ºäº†é‡‘èé¢†åŸŸä¸­ LLMs åº”ç”¨çš„å¯é æ€§å’Œåˆ†ææ·±åº¦ï¼Œä¸ºæ›´æ˜æ™ºçš„é‡‘èå†³ç­–è¿‡ç¨‹æä¾›äº†æ”¯æŒã€‚",
      "categories": [
        "cs.CE",
        "cs.AI"
      ],
      "primary_category": "cs.CE",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.06622v2",
      "published_date": "2025-06-07 01:52:39 UTC",
      "updated_date": "2025-06-12 15:00:11 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T19:47:44.490318+00:00"
    },
    {
      "arxiv_id": "2506.11105v3",
      "title": "Enabling On-Device Medical AI Assistants via Input-Driven Saliency Adaptation",
      "title_zh": "é€šè¿‡è¾“å…¥é©±åŠ¨çš„æ˜¾è‘—æ€§è‡ªé€‚åº”å®ç°ç«¯ä¾§åŒ»ç–— AI åŠ©æ‰‹",
      "authors": [
        "Uttej Kallakurik",
        "Edward Humes",
        "Rithvik Jonna",
        "Xiaomin Lin",
        "Tinoosh Mohsenin"
      ],
      "abstract": "Large Language Models (LLMs) have significant impact on the healthcare scenarios but remain prohibitively large for deployment in real-time, resource-constrained environments such as edge devices. In this work, we introduce a novel medical assistant system, optimized through our general-purpose compression framework, which tailors Large Language Models (LLMs) for deployment in specialized domains. By measuring neuron saliency on domain-specific data, our method can aggressively prune irrelevant neurons, reducing model size while preserving performance. Following pruning, we apply post-training quantization to further reduce the memory footprint, and evaluate the compressed model across medical benchmarks including MedMCQA, MedQA, and PubMedQA. We also deploy the 50\\% compressed Gemma and the 67\\% compressed LLaMA3 models on Jetson Orin Nano (18.7W peak) and Raspberry Pi 5 (6.3W peak), achieving real-time, energy-efficient inference under hardware constraints.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹åŒ»ç–—åœºæ™¯ä¸‹ Large Language Models (LLMs) å› ä½“ç§¯è¿‡å¤§éš¾ä»¥åœ¨èµ„æºå—é™çš„è¾¹ç¼˜è®¾å¤‡ä¸Šå®ç°å®æ—¶éƒ¨ç½²çš„é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§åŸºäºè¾“å…¥é©±åŠ¨æ˜¾è‘—æ€§è‡ªé€‚åº” (Input-Driven Saliency Adaptation) çš„åŒ»ç–—åŠ©æ‰‹ç³»ç»Ÿä¼˜åŒ–æ¡†æ¶ã€‚è¯¥æ–¹æ³•é€šè¿‡åœ¨ç‰¹å®šé¢†åŸŸæ•°æ®ä¸Šè¡¡é‡ç¥ç»å…ƒæ˜¾è‘—æ€§ (Neuron Saliency) æ¥æ¿€è¿›åœ°å‰ªæ (Pruning) æ— å…³ç¥ç»å…ƒï¼Œå¹¶ç»“åˆè®­ç»ƒåé‡åŒ– (Post-training Quantization) æŠ€æœ¯è¿›ä¸€æ­¥é™ä½å†…å­˜å ç”¨ã€‚å®éªŒåœ¨ MedMCQAã€MedQA å’Œ PubMedQA ç­‰åŒ»ç–—åŸºå‡†æ•°æ®é›†ä¸ŠéªŒè¯äº†å‹ç¼©æ¨¡å‹åœ¨ä¿æŒæ€§èƒ½æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚ç ”ç©¶å›¢é˜ŸæˆåŠŸå°†å‹ç¼© 50% çš„ Gemma å’Œå‹ç¼© 67% çš„ LLaMA3 æ¨¡å‹éƒ¨ç½²åœ¨ Jetson Orin Nano å’Œ Raspberry Pi 5 ç­‰ä½åŠŸè€—ç¡¬ä»¶ä¸Šï¼Œå®ç°äº†å®æ—¶ä¸”èŠ‚èƒ½çš„æ¨ç†ã€‚è¯¥æˆæœä¸ºåœ¨å„ç±»ç¡¬ä»¶é™åˆ¶ç¯å¢ƒä¸‹æ„å»ºé«˜æ•ˆã€å®ç”¨çš„è®¾å¤‡ç«¯åŒ»ç–— AI åŠ©æ‰‹æä¾›äº†é‡è¦æŠ€æœ¯æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.AR",
        "eess.SY"
      ],
      "primary_category": "cs.CL",
      "comment": "Accepted for publication in the Proceedings of IEEE BioCAS 2025",
      "pdf_url": "https://arxiv.org/pdf/2506.11105v3",
      "published_date": "2025-06-07 01:37:42 UTC",
      "updated_date": "2025-08-07 14:57:45 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T19:48:05.784720+00:00"
    },
    {
      "arxiv_id": "2506.06607v1",
      "title": "Training-Free Tokenizer Transplantation via Orthogonal Matching Pursuit",
      "title_zh": "åŸºäºæ­£äº¤åŒ¹é…è¿½è¸ªçš„å…è®­ç»ƒåˆ†è¯å™¨ç§»æ¤",
      "authors": [
        "Charles Goddard",
        "Fernando Fernandes Neto"
      ],
      "abstract": "We present a training-free method to transplant tokenizers in pretrained large language models (LLMs) by reconstructing unseen token embeddings via Orthogonal Matching Pursuit (OMP). Specifically, we approximate each out-of-vocabulary token as a sparse linear combination of shared tokens, in two phases: first, compute each new token's representation in the donor embedding space with a small dictionary of shared anchor tokens, then transfer these same sparse coefficients back into the base model's embedding space.\n  On two challenging cross-tokenizer tasks--Llama$\\to$Mistral NeMo (12B) and Qwen$\\to$Llama (1B)--we show that OMP achieves best zero-shot preservation of the base model's performance across multiple benchmarks, while other zero-shot approaches degrade significantly. Compared to baselines (zero-init, mean-init, and existing approaches like WECHSEL, FOCUS, ZETT), OMP consistently achieves the best overall performance, effectively bridging large tokenizer discrepancies without gradient updates. Our analysis further identifies mismatched numerical tokenization schemes as a critical challenge for preserving mathematical reasoning capabilities. This technique enables direct reuse of pretrained model weights with new tokenizers, facilitating cross-tokenizer knowledge distillation, speculative decoding, ensembling, merging, and domain-specific vocabulary adaptations. We integrate our method into the open-source mergekit-tokensurgeon tool for post hoc vocabulary realignment.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§åä¸º Orthogonal Matching Pursuit (OMP) çš„æ— éœ€è®­ç»ƒçš„ Tokenizer ç§»æ¤æ–¹æ³•ï¼Œæ—¨åœ¨é€šè¿‡é‡æ„æœªè§ Token çš„åµŒå…¥å‘é‡ï¼Œå®ç°é¢„è®­ç»ƒå¤§è¯­è¨€æ¨¡å‹ (LLMs) åœ¨ä¸åŒåˆ†è¯å™¨é—´çš„ç›´æ¥æƒé‡å¤ç”¨ã€‚è¯¥æ–¹æ³•å°†æ¯ä¸ªè¯è¡¨å¤– (Out-of-Vocabulary) Token è¡¨ç¤ºä¸ºå…±äº« Token çš„ç¨€ç–çº¿æ€§ç»„åˆï¼Œé¦–å…ˆåœ¨ä¾›ä½“åµŒå…¥ç©ºé—´åˆ©ç”¨é”šç‚¹ Token è®¡ç®—è¡¨ç¤ºç³»æ•°ï¼Œéšåå°†è¿™äº›ç³»æ•°è¿ç§»å›åŸºç¡€æ¨¡å‹çš„åµŒå…¥ç©ºé—´ã€‚åœ¨ Llama åˆ° Mistral NeMo ä»¥åŠ Qwen åˆ° Llama çš„è·¨åˆ†è¯å™¨ä»»åŠ¡å®éªŒä¸­ï¼ŒOMP åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸Šå®ç°äº†æœ€ä½³çš„é›¶æ ·æœ¬ (Zero-Shot) æ€§èƒ½ä¿ç•™ï¼Œæ˜¾è‘—ä¼˜äºä¼ ç»Ÿçš„ Zero-Initã€Mean-Init ä»¥åŠ WECHSELã€FOCUS ç­‰ç°æœ‰æ–¹æ³•ã€‚åˆ†æè¡¨æ˜ï¼Œè¯¥æŠ€æœ¯èƒ½å¤Ÿåœ¨æ— éœ€æ¢¯åº¦æ›´æ–°çš„æƒ…å†µä¸‹æœ‰æ•ˆå¼¥åˆå·¨å¤§çš„åˆ†è¯å™¨å·®å¼‚ï¼Œä½†ä¹ŸæŒ‡å‡ºæ•°å€¼åˆ†è¯æ–¹æ¡ˆçš„ä¸åŒ¹é…æ˜¯ä¿æŒæ•°å­¦æ¨ç†èƒ½åŠ›çš„å…³é”®æŒ‘æˆ˜ã€‚è¿™ç§æ–¹æ³•ä¸ºæ¨¡å‹åˆå¹¶ (Merging)ã€çŸ¥è¯†è’¸é¦å’Œé¢†åŸŸç‰¹å®šè¯è¡¨é€‚é…æä¾›äº†æ–°é€”å¾„ï¼Œç›¸å…³å·¥å…·å·²é›†æˆè‡³å¼€æºé¡¹ç›® mergekit-tokensurgeon ä¸­ï¼Œæå¤§æ–¹ä¾¿äº†åˆ†è¯å™¨çš„äº‹åå¯¹é½ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.06607v1",
      "published_date": "2025-06-07 00:51:27 UTC",
      "updated_date": "2025-06-07 00:51:27 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T19:48:04.595647+00:00"
    },
    {
      "arxiv_id": "2506.06605v1",
      "title": "MedCite: Can Language Models Generate Verifiable Text for Medicine?",
      "title_zh": "MedCiteï¼šè¯­è¨€æ¨¡å‹èƒ½å¦ç”ŸæˆåŒ»å­¦é¢†åŸŸçš„å¯éªŒè¯æ–‡æœ¬ï¼Ÿ",
      "authors": [
        "Xiao Wang",
        "Mengjue Tan",
        "Qiao Jin",
        "Guangzhi Xiong",
        "Yu Hu",
        "Aidong Zhang",
        "Zhiyong Lu",
        "Minjia Zhang"
      ],
      "abstract": "Existing LLM-based medical question-answering systems lack citation generation and evaluation capabilities, raising concerns about their adoption in practice. In this work, we introduce \\name, the first end-to-end framework that facilitates the design and evaluation of citation generation with LLMs for medical tasks. Meanwhile, we introduce a novel multi-pass retrieval-citation method that generates high-quality citations. Our evaluation highlights the challenges and opportunities of citation generation for medical tasks, while identifying important design choices that have a significant impact on the final citation quality. Our proposed method achieves superior citation precision and recall improvements compared to strong baseline methods, and we show that evaluation results correlate well with annotation results from professional experts.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ç°æœ‰åŸºäºå¤§è¯­è¨€æ¨¡å‹(LLMs)çš„åŒ»ç–—é—®ç­”ç³»ç»Ÿåœ¨å¼•ç”¨ç”Ÿæˆä¸è¯„ä¼°èƒ½åŠ›ä¸Šçš„ç¼ºå¤±ï¼Œæå‡ºäº†MedCiteæ¡†æ¶ã€‚MedCiteæ˜¯é¦–ä¸ªæ—¨åœ¨ä¿ƒè¿›åŒ»ç–—ä»»åŠ¡ä¸­LLMså¼•ç”¨ç”Ÿæˆè®¾è®¡ä¸è¯„ä¼°çš„ç«¯åˆ°ç«¯æ¡†æ¶ï¼Œå¹¶å¼•å…¥äº†ä¸€ç§æ–°å‹çš„å¤šè½®æ£€ç´¢å¼•ç”¨æ–¹æ³•(multi-pass retrieval-citation method)ä»¥ç”Ÿæˆé«˜è´¨é‡å¼•ç”¨ã€‚é€šè¿‡å¯¹åŒ»ç–—ä»»åŠ¡ä¸­å¼•ç”¨ç”Ÿæˆçš„æŒ‘æˆ˜è¿›è¡Œæ·±å…¥è¯„ä¼°ï¼Œç ”ç©¶ç¡®å®šäº†å¯¹å¼•ç”¨è´¨é‡æœ‰é‡å¤§å½±å“çš„å…³é”®è®¾è®¡è¦ç´ ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å¼•ç”¨ç²¾åº¦(precision)å’Œå¬å›ç‡(recall)æ–¹é¢å‡ä¼˜äºå¼ºåŸºçº¿æ–¹æ³•ã€‚æ­¤å¤–ï¼Œç ”ç©¶è¿˜éªŒè¯äº†è‡ªåŠ¨è¯„ä¼°ç»“æœä¸åŒ»å­¦ä¸“å®¶æ ‡æ³¨ç»“æœä¹‹é—´å­˜åœ¨è‰¯å¥½çš„ç›¸å…³æ€§ï¼Œä¸ºæ„å»ºå¯ä¿¡èµ–çš„åŒ»ç–—äººå·¥æ™ºèƒ½ç³»ç»Ÿæä¾›äº†é‡è¦æ”¯æŒã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.06605v1",
      "published_date": "2025-06-07 00:46:18 UTC",
      "updated_date": "2025-06-07 00:46:18 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T19:48:05.185038+00:00"
    },
    {
      "arxiv_id": "2506.06603v1",
      "title": "CAtCh: Cognitive Assessment through Cookie Thief",
      "title_zh": "CAtChï¼šåŸºäºâ€œå·é¥¼å¹²â€æµ‹è¯•çš„è®¤çŸ¥è¯„ä¼°",
      "authors": [
        "Joseph T Colonel",
        "Carolyn Hagler",
        "Guiselle Wismer",
        "Laura Curtis",
        "Jacqueline Becker",
        "Juan Wisnivesky",
        "Alex Federman",
        "Gaurav Pandey"
      ],
      "abstract": "Several machine learning algorithms have been developed for the prediction of Alzheimer's disease and related dementia (ADRD) from spontaneous speech. However, none of these algorithms have been translated for the prediction of broader cognitive impairment (CI), which in some cases is a precursor and risk factor of ADRD. In this paper, we evaluated several speech-based open-source methods originally proposed for the prediction of ADRD, as well as methods from multimodal sentiment analysis for the task of predicting CI from patient audio recordings. Results demonstrated that multimodal methods outperformed unimodal ones for CI prediction, and that acoustics-based approaches performed better than linguistics-based ones. Specifically, interpretable acoustic features relating to affect and prosody were found to significantly outperform BERT-based linguistic features and interpretable linguistic features, respectively. All the code developed for this study is available at https://github.com/JTColonel/catch.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†åˆ©ç”¨è‡ªå‘è¯­éŸ³é¢„æµ‹è®¤çŸ¥éšœç¢ (Cognitive Impairment, CI) çš„æœ‰æ•ˆæ€§ï¼Œæ—¨åœ¨å¡«è¡¥å½“å‰ç ”ç©¶å¤§å¤šé›†ä¸­åœ¨é˜¿å°”èŒ¨æµ·é»˜ç—… (ADRD) è€Œå¿½ç•¥æ›´å¹¿æ³›è®¤çŸ¥åŠŸèƒ½ä¸‹é™çš„ç¼ºå£ã€‚ç ”ç©¶è€…è¯„ä¼°äº†å¤šç§åŸç”¨äº ADRD é¢„æµ‹çš„è¯­éŸ³æ–¹æ³•ä»¥åŠå¤šæ¨¡æ€æƒ…æ„Ÿåˆ†æ (multimodal sentiment analysis) æŠ€æœ¯ï¼Œåœ¨æ‚£è€…å½•éŸ³æ•°æ®é›†ä¸Šè¿›è¡Œäº†å®éªŒå¯¹æ¯”ã€‚ç ”ç©¶å‘ç°ï¼Œå¤šæ¨¡æ€æ–¹æ³•çš„ CI é¢„æµ‹æ€§èƒ½ä¼˜äºå•æ¨¡æ€æ–¹æ³•ï¼Œä¸”åŸºäºå£°å­¦ (acoustics-based) çš„æ–¹æ³•è¡¨ç°ä¼˜äºåŸºäºè¯­è¨€å­¦ (linguistics-based) çš„æ–¹æ³•ã€‚å…·ä½“è€Œè¨€ï¼Œä¸æƒ…æ„Ÿ (affect) å’ŒéŸµå¾‹ (prosody) ç›¸å…³çš„å¯è§£é‡Šå£°å­¦ç‰¹å¾åœ¨æ€§èƒ½ä¸Šæ˜¾è‘—è¶…è¿‡äº†åŸºäº BERT çš„è¯­è¨€ç‰¹å¾å’Œå¯è§£é‡Šè¯­è¨€ç‰¹å¾ã€‚è¯¥å·¥ä½œè¯æ˜äº†å£°å­¦ç‰¹å¾åœ¨æ—©æœŸè®¤çŸ¥é£é™©ç­›æŸ¥ä¸­çš„é‡è¦ä»·å€¼ï¼Œå¹¶ä¸ºæœªæ¥çš„å¤šæ¨¡æ€è®¤çŸ¥è¯„ä¼°ç ”ç©¶æä¾›äº†å¼€æºå·¥å…·å‚è€ƒã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.SD",
        "eess.AS"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.06603v1",
      "published_date": "2025-06-07 00:41:34 UTC",
      "updated_date": "2025-06-07 00:41:34 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T19:48:14.947625+00:00"
    },
    {
      "arxiv_id": "2506.06594v1",
      "title": "From Model-Based and Adaptive Control to Evolving Fuzzy Control",
      "title_zh": "ä»åŸºäºæ¨¡å‹å’Œè‡ªé€‚åº”æ§åˆ¶åˆ°æ¼”åŒ–æ¨¡ç³Šæ§åˆ¶",
      "authors": [
        "Daniel Leite",
        "Igor Å krjanc",
        "Fernando Gomide"
      ],
      "abstract": "Evolving fuzzy systems build and adapt fuzzy models - such as predictors and controllers - by incrementally updating their rule-base structure from data streams. On the occasion of the 60-year anniversary of fuzzy set theory, commemorated during the Fuzz-IEEE 2025 event, this brief paper revisits the historical development and core contributions of classical fuzzy and adaptive modeling and control frameworks. It then highlights the emergence and significance of evolving intelligent systems in fuzzy modeling and control, emphasizing their advantages in handling nonstationary environments. Key challenges and future directions are discussed, including safety, interpretability, and principled structural evolution.",
      "tldr_zh": "è¯¥è®ºæ–‡å›é¡¾äº†æ¨¡ç³Šé›†åˆç†è®º(fuzzy set theory)è¯ç”Ÿ60å¹´æ¥çš„å‘å±•å†ç¨‹ï¼Œé‡ç‚¹æ¢è®¨äº†ä»ä¼ ç»Ÿçš„åŸºäºæ¨¡å‹(model-based)ä¸è‡ªé€‚åº”æ§åˆ¶(adaptive control)å‘æ¼”åŒ–æ¨¡ç³Šæ§åˆ¶(evolving fuzzy control)çš„è·¨è¶Šã€‚æ¼”åŒ–æ¨¡ç³Šç³»ç»Ÿ(evolving fuzzy systems)é€šè¿‡ä»æ•°æ®æµä¸­å¢é‡å¼æ›´æ–°è§„åˆ™åº“ç»“æ„ï¼Œå®ç°äº†é¢„æµ‹å™¨å’Œæ§åˆ¶å™¨çš„åŠ¨æ€æ„å»ºä¸è‡ªé€‚åº”ã€‚ç ”ç©¶å¼ºè°ƒäº†æ¼”åŒ–æ™ºèƒ½ç³»ç»Ÿåœ¨å¤„ç†éå¹³ç¨³ç¯å¢ƒ(nonstationary environments)æ–¹é¢çš„æ˜¾è‘—ä¼˜åŠ¿ï¼Œå¹¶æ·±å…¥åˆ†æäº†å…¶åœ¨æ¨¡ç³Šå»ºæ¨¡ä¸æ§åˆ¶ä¸­çš„é‡è¦åœ°ä½ã€‚æ­¤å¤–ï¼Œè®ºæ–‡è¿˜è®¨è®ºäº†è¯¥é¢†åŸŸé¢ä¸´çš„æ ¸å¿ƒæŒ‘æˆ˜ä¸æœªæ¥æ–¹å‘ï¼ŒåŒ…æ‹¬å®‰å…¨æ€§(safety)ã€å¯è§£é‡Šæ€§(interpretability)ä»¥åŠåŸåˆ™æ€§çš„ç»“æ„æ¼”åŒ–(principled structural evolution)ï¼Œä¸ºæ„å»ºæ›´æ™ºèƒ½çš„æ¨¡ç³Šç³»ç»Ÿæä¾›äº†ç†è®ºæ¡†æ¶ã€‚",
      "categories": [
        "eess.SY",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "eess.SY",
      "comment": "4 pages, 2 figures. Fuzz-IEEE 2025 Booklet: 60 Years of Fuzzy Set Theory",
      "pdf_url": "https://arxiv.org/pdf/2506.06594v1",
      "published_date": "2025-06-07 00:00:52 UTC",
      "updated_date": "2025-06-07 00:00:52 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T19:48:11.498082+00:00"
    }
  ],
  "processing_status": "completed",
  "error": null,
  "raw_papers_fetched": true,
  "papers_count": 75,
  "processed_papers_count": 75,
  "failed_papers_count": 0,
  "llm_backup_calls": 0,
  "summary_generated": true,
  "daily_data_saved": true,
  "last_update": "2026-01-23T19:48:57.570650+00:00"
}
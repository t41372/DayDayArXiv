{
  "date": "2025-05-14",
  "category": "cs.AI",
  "summary": "欢迎来到 UTC 时间 2025-05-14 的 arXiv 中文 TLDR 快报！今天 arXiv 的论文主要聚焦 AI 模型优化、联邦学习和量子计算应用等领域，亮点包括 LinkedIn 的营销智能解决方案、LLM 在冲突预测和机器人任务中的表现，以及知名学者如 Thomas L. Griffiths 和 Vijay Kumar 的作品，这些文章展示了 AI 在实际场景中的潜力。\n\n### AI 和 LLM 相关论文（重点优先）\n这些论文探讨了大型语言模型（LLM）的优化和应用，特别强调了其在商业、预测和生成任务中的创新。\n\n- **LiDDA: Data Driven Attribution at LinkedIn**（中文：LinkedIn 的数据驱动归因系统；英文：LiDDA: Data Driven Attribution at LinkedIn）  \n  作者包括 LinkedIn 研究者如 John Bencina 和 Changshuai Wei，这篇论文引入了基于 Transformer 的统一归因方法，能处理成员级和聚合级数据，并整合外部宏观因素，在 LinkedIn 实际部署中显著提升了营销效果，主要贡献是提升了广告归因的准确性和可扩展性。\n\n- **Predictability Shapes Adaptation: An Evolutionary Perspective on Modes of Learning in Transformers**（中文：可预测性塑造适应：Transformer 学习模式的进化视角；英文：Predictability Shapes Adaptation: An Evolutionary Perspective on Modes of Learning in Transformers）  \n  知名学者 Thomas L. Griffiths 和 Stephanie C. Y. Chan 参与，这篇论文从进化生物学角度分析 Transformer 的 in-weights learning (IWL) 和 in-context learning (ICL)，发现环境可预测性影响学习模式平衡，主要发现是通过实验验证了 ICL 在低稳定性环境中的优势，并提出相对成本假设指导训练方法。\n\n- **Do Large Language Models Know Conflict? Investigating Parametric vs. Non-Parametric Knowledge of LLMs for Conflict Forecasting**（中文：大型语言模型是否了解冲突？LLM 在冲突预测中的参数与非参数知识调查；英文：Do Large Language Models Know Conflict? Investigating Parametric vs. Non-Parametric Knowledge of LLMs for Conflict Forecasting）  \n  这篇论文评估了 LLM 在暴力冲突预测中的能力，比较了参数知识（预训练权重）和非参数知识（通过 RAG 整合外部数据），主要贡献是构建了评估框架，发现非参数方法在实时数据整合上更具优势，为早期预警系统提供新见解。\n\n- **Causal Predictive Optimization and Generation for Business AI**（中文：基于因果预测的商业 AI 优化和生成；英文：Causal Predictive Optimization and Generation for Business AI）  \n  LinkedIn 作者如 Liyang Zhao 和 Changshuai Wei 提出三层框架（因果 ML 预测、约束优化和生成 AI），在销售优化中实现显著改进，主要发现是通过实际部署证明了该方法优于传统系统。\n\n- **DeepSeek-V3: Scaling Challenges and Reflections on Hardware for AI Architectures**（中文：DeepSeek-V3：AI 架构的扩展挑战和硬件反思；英文：DeepSeek-V3: Scaling Challenges and Reflections on Hardware for AI Architectures）  \n  这篇论文讨论了 LLM 扩展中的硬件瓶颈，如内存和计算效率，并提出 Multi-head Latent Attention 和 FP8 混合精度训练，主要贡献是展示了如何通过硬件-模型协同设计实现高效训练，引发了对未来 AI 硬件的讨论。\n\n其他 LLM 相关论文，如 **Evaluating Large Language Models for the Generation of Unit Tests**（中文：评估 LLM 在单元测试生成中的性能；英文：Evaluating Large Language Models for the Generation of Unit Tests），则快速掠过：它比较了 LLM 和人工测试的优缺点，发现 LLM 在等价分区和边界值测试中表现良好，但需人工监督。\n\n### 机器人和视觉相关论文\n这些论文强调 AI 在机器人任务中的应用，特别在长序列处理和协作中。\n\n- **ManipBench: Benchmarking Vision-Language Models for Low-Level Robot Manipulation**（中文：ManipBench：针对低级机器人操作的视觉语言模型基准测试；英文：ManipBench: Benchmarking Vision-Language Models for Low-Level Robot Manipulation）  \n  作者包括 Yue Wang 和 Vijay Kumar，这篇论文提出一个基准测试 VLMs 在机器人操作中的性能，主要发现是 VLMs 在物体交互和可变形物体处理上表现出色，但与人类水平仍有差距。\n\n- **Learning Long-Context Diffusion Policies via Past-Token Prediction**（中文：通过过去标记预测学习长上下文扩散策略；英文：Learning Long-Context Diffusion Policies via Past-Token Prediction）  \n  这篇论文引入 Past-Token Prediction 模块，提升扩散策略在机器人任务中的时序建模，主要贡献是提高了长序列任务的性能，并加速了训练。\n\n其他机器人论文，如 **RT-cache: Efficient Robot Trajectory Retrieval System**（中文：RT-cache：高效机器人轨迹检索系统；英文：RT-cache: Efficient Robot Trajectory Retrieval System），则简要提及：它通过大数据检索减少推理延迟，提升了机器人任务效率。\n\n### 医学和生物 AI 相关论文\n这些论文展示了 AI 在医疗领域的潜力，特别是在图像分析和诊断中。\n\n- **Endo-CLIP: Progressive Self-Supervised Pre-training on Raw Colonoscopy Records**（中文：Endo-CLIP：在原始结肠镜记录上进行渐进式自监督预训练；英文：Endo-CLIP: Progressive Self-Supervised Pre-training on Raw Colonoscopy Records）  \n  这篇论文提出自监督框架处理结肠镜数据，主要发现是通过优化提示提升了息肉检测和分类的准确性。\n\n- **BioVFM-21M: Benchmarking and Scaling Self-Supervised Vision Foundation Models for Biomedical Image Analysis**（中文：BioVFM-21M：生物医学图像分析的自监督视觉基础模型基准和扩展；英文：BioVFM-21M: Benchmarking and Scaling Self-Supervised Vision Foundation Models for Biomedical Image Analysis）  \n  这篇论文构建了 2100 万图像数据集，探索自监督模型的扩展行为，主要贡献是证明了模型规模对生物医学任务的性能提升。\n\n其他医学论文，如 **Virtual Dosimetrists: A Radiotherapy Training \"Flight Simulator\"**（中文：虚拟剂量师：放射治疗训练的飞行模拟器；英文：Virtual Dosimetrists: A Radiotherapy Training \"Flight Simulator\"），快速掠过：它结合自然语言处理生成训练计划，提升了放射治疗的模拟效果。\n\n### 其他领域论文\n剩余论文涉及量子计算、联邦学习和图像处理等，但多为技术优化，这里仅快速概述。\n\n- **Quantum-Enhanced Parameter-Efficient Learning for Typhoon Trajectory Forecasting**（中文：量子增强的参数高效学习用于台风轨迹预测；英文：Quantum-Enhanced Parameter-Efficient Learning for Typhoon Trajectory Forecasting）  \n  这篇论文使用量子神经网络优化台风预测，主要发现是提高了预测准确性，同时降低了计算成本。\n\n联邦学习论文如 **Robust Federated Learning with Confidence-Weighted Filtering and GAN-Based Completion**（中文：基于置信权重过滤和 GAN 补全的鲁棒联邦学习；英文：Robust Federated Learning with Confidence-Weighted Filtering and GAN-Based Completion），贡献在于处理数据噪声和不平衡，但细节较琐碎，这里不展开。\n\n总的来说，今天的论文突出了 AI 模型在实际应用中的进步，尤其是 LLM 和量子计算的结合，但也暴露了如数据隐私和泛化性的挑战。更多技术细节可查阅 arXiv 原文。",
  "papers": [
    {
      "arxiv_id": "2505.09861v2",
      "title": "LiDDA: Data Driven Attribution at LinkedIn",
      "title_zh": "翻译失败",
      "authors": [
        "John Bencina",
        "Erkut Aykutlug",
        "Yue Chen",
        "Zerui Zhang",
        "Stephanie Sorenson",
        "Shao Tang",
        "Changshuai Wei"
      ],
      "abstract": "Data Driven Attribution, which assigns conversion credits to marketing\ninteractions based on causal patterns learned from data, is the foundation of\nmodern marketing intelligence and vital to any marketing businesses and\nadvertising platform. In this paper, we introduce a unified transformer-based\nattribution approach that can handle member-level data, aggregate-level data,\nand integration of external macro factors. We detail the large scale\nimplementation of the approach at LinkedIn, showcasing significant impact. We\nalso share learning and insights that are broadly applicable to the marketing\nand ad tech fields.",
      "tldr_zh": "本论文介绍了 LiDDA，一种基于 Transformer 的 Data Driven Attribution 方法，用于通过数据学习因果模式，为营销互动分配转化信用，从而提升现代营销智能。该方法能够统一处理成员级数据、聚合级数据，并整合外部宏观因素，在 LinkedIn 的大规模实现中取得了显著影响。论文还分享了可广泛应用于营销和广告技术领域的学习与见解。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.IR",
        "stat.ME"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2505.09861v2",
      "published_date": "2025-05-14 23:54:57 UTC",
      "updated_date": "2025-05-21 22:19:59 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T22:47:01.931049"
    },
    {
      "arxiv_id": "2505.09855v1",
      "title": "Predictability Shapes Adaptation: An Evolutionary Perspective on Modes of Learning in Transformers",
      "title_zh": "可预测性塑造适应：对 Transformers 中学习模式的进化视角",
      "authors": [
        "Alexander Y. Ku",
        "Thomas L. Griffiths",
        "Stephanie C. Y. Chan"
      ],
      "abstract": "Transformer models learn in two distinct modes: in-weights learning (IWL),\nencoding knowledge into model weights, and in-context learning (ICL), adapting\nflexibly to context without weight modification. To better understand the\ninterplay between these learning modes, we draw inspiration from evolutionary\nbiology's analogous adaptive strategies: genetic encoding (akin to IWL,\nadapting over generations and fixed within an individual's lifetime) and\nphenotypic plasticity (akin to ICL, enabling flexible behavioral responses to\nenvironmental cues). In evolutionary biology, environmental predictability\ndictates the balance between these strategies: stability favors genetic\nencoding, while reliable predictive cues promote phenotypic plasticity. We\nexperimentally operationalize these dimensions of predictability and\nsystematically investigate their influence on the ICL/IWL balance in\nTransformers. Using regression and classification tasks, we show that high\nenvironmental stability decisively favors IWL, as predicted, with a sharp\ntransition at maximal stability. Conversely, high cue reliability enhances ICL\nefficacy, particularly when stability is low. Furthermore, learning dynamics\nreveal task-contingent temporal evolution: while a canonical ICL-to-IWL shift\noccurs in some settings (e.g., classification with many classes), we\ndemonstrate that scenarios with easier IWL (e.g., fewer classes) or slower ICL\nacquisition (e.g., regression) can exhibit an initial IWL phase later yielding\nto ICL dominance. These findings support a relative-cost hypothesis for\nexplaining these learning mode transitions, establishing predictability as a\ncritical factor governing adaptive strategies in Transformers, and offering\nnovel insights for understanding ICL and guiding training methodologies.",
      "tldr_zh": "这篇论文从进化生物学的角度探讨 Transformer 模型的两种学习模式：in-weights learning (IWL)，类似于遗传编码，将知识固定在模型权重中；以及 in-context learning (ICL)，类似于表型可塑性，允许灵活适应环境上下文。作者通过实验操作化环境的可预测性维度（如稳定性与线索可靠性），使用回归和分类任务证明：高环境稳定性显著偏好 IWL，而高线索可靠性则增强 ICL 的效能，尤其在稳定性较低时。研究还揭示学习动态的动态转变，支持相对成本假设，即任务难度影响 ICL 到 IWL 或反向转变，提供新见解以指导 Transformer 的训练方法。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2505.09855v1",
      "published_date": "2025-05-14 23:31:17 UTC",
      "updated_date": "2025-05-14 23:31:17 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T22:47:15.550537"
    },
    {
      "arxiv_id": "2505.09852v1",
      "title": "Do Large Language Models Know Conflict? Investigating Parametric vs. Non-Parametric Knowledge of LLMs for Conflict Forecasting",
      "title_zh": "翻译失败",
      "authors": [
        "Apollinaire Poli Nemkova",
        "Sarath Chandra Lingareddy",
        "Sagnik Ray Choudhury",
        "Mark V. Albert"
      ],
      "abstract": "Large Language Models (LLMs) have shown impressive performance across natural\nlanguage tasks, but their ability to forecast violent conflict remains\nunderexplored. We investigate whether LLMs possess meaningful parametric\nknowledge-encoded in their pretrained weights-to predict conflict escalation\nand fatalities without external data. This is critical for early warning\nsystems, humanitarian planning, and policy-making. We compare this parametric\nknowledge with non-parametric capabilities, where LLMs access structured and\nunstructured context from conflict datasets (e.g., ACLED, GDELT) and recent\nnews reports via Retrieval-Augmented Generation (RAG). Incorporating external\ninformation could enhance model performance by providing up-to-date context\notherwise missing from pretrained weights. Our two-part evaluation framework\nspans 2020-2024 across conflict-prone regions in the Horn of Africa and the\nMiddle East. In the parametric setting, LLMs predict conflict trends and\nfatalities relying only on pretrained knowledge. In the non-parametric setting,\nmodels receive summaries of recent conflict events, indicators, and\ngeopolitical developments. We compare predicted conflict trend labels (e.g.,\nEscalate, Stable Conflict, De-escalate, Peace) and fatalities against\nhistorical data. Our findings highlight the strengths and limitations of LLMs\nfor conflict forecasting and the benefits of augmenting them with structured\nexternal knowledge.",
      "tldr_zh": "本研究探讨 Large Language Models (LLMs) 在冲突预测中的能力，比较其参数知识（预训练权重中的编码知识）和非参数知识（通过 Retrieval-Augmented Generation (RAG) 访问外部数据，如 ACLED 和 GDELT 数据库）。研究采用双重评估框架，针对 2020-2024 年的非洲之角和中东地区，分别测试 LLMs 仅凭参数知识预测冲突趋势（如 Escalate、Stable Conflict）和死亡人数，以及结合外部信息（如新闻摘要）的非参数方法。结果表明，参数知识存在局限，而非参数方法显著提升预测准确性，强调了外部知识对早期预警系统和政策制定的重要性。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2505.09852v1",
      "published_date": "2025-05-14 23:24:22 UTC",
      "updated_date": "2025-05-14 23:24:22 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T22:47:27.223414"
    },
    {
      "arxiv_id": "2505.09847v2",
      "title": "Causal Predictive Optimization and Generation for Business AI",
      "title_zh": "翻译失败",
      "authors": [
        "Liyang Zhao",
        "Olurotimi Seton",
        "Himadeep Reddy Reddivari",
        "Suvendu Jena",
        "Shadow Zhao",
        "Rachit Kumar",
        "Changshuai Wei"
      ],
      "abstract": "The sales process involves sales functions converting leads or opportunities\nto customers and selling more products to existing customers. The optimization\nof the sales process thus is key to success of any B2B business. In this work,\nwe introduce a principled approach to sales optimization and business AI,\nnamely the Causal Predictive Optimization and Generation, which includes three\nlayers: 1) prediction layer with causal ML 2) optimization layer with\nconstraint optimization and contextual bandit 3) serving layer with Generative\nAI and feedback-loop for system enhancement. We detail the implementation and\ndeployment of the system in LinkedIn, showcasing significant wins over legacy\nsystems and sharing learning and insight broadly applicable to this field.",
      "tldr_zh": "该论文提出了 Causal Predictive Optimization and Generation 方法，用于优化 B2B 业务的销售过程，包括三个关键层：1) 预测层，使用 causal ML 进行因果机器学习预测；2) 优化层，采用 constraint optimization 和 contextual bandit 进行约束优化和决策；3) 服务层，利用 Generative AI 和反馈-loop 实现系统服务与持续改进。在 LinkedIn 的实际部署中，该方法显著优于传统系统，展示了更高的销售效率，并分享了适用于该领域的广泛见解。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.IR",
        "stat.ML"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2505.09847v2",
      "published_date": "2025-05-14 23:12:20 UTC",
      "updated_date": "2025-05-21 16:12:30 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T22:47:38.423963"
    },
    {
      "arxiv_id": "2505.09830v1",
      "title": "Evaluating Large Language Models for the Generation of Unit Tests with Equivalence Partitions and Boundary Values",
      "title_zh": "评估大语言模型生成单元测试的性能：包括等",
      "authors": [
        "Martín Rodríguez",
        "Gustavo Rossi",
        "Alejandro Fernandez"
      ],
      "abstract": "The design and implementation of unit tests is a complex task many\nprogrammers neglect. This research evaluates the potential of Large Language\nModels (LLMs) in automatically generating test cases, comparing them with\nmanual tests. An optimized prompt was developed, that integrates code and\nrequirements, covering critical cases such as equivalence partitions and\nboundary values. The strengths and weaknesses of LLMs versus trained\nprogrammers were compared through quantitative metrics and manual qualitative\nanalysis. The results show that the effectiveness of LLMs depends on\nwell-designed prompts, robust implementation, and precise requirements.\nAlthough flexible and promising, LLMs still require human supervision. This\nwork highlights the importance of manual qualitative analysis as an essential\ncomplement to automation in unit test evaluation.",
      "tldr_zh": "这篇论文评估了 Large Language Models (LLMs) 在自动生成单元测试方面的潜力，特别是针对等价分区和边界值等关键测试案例。研究开发了一个优化的提示，整合代码和需求，并通过定量指标和手动定性分析比较了 LLMs 生成的测试与程序员手动测试的优缺点。结果显示，LLMs 的有效性高度依赖于提示设计、稳健实现和精确需求，尽管其灵活且有前景，但仍需人工监督。该工作强调了手动定性分析作为单元测试评估中不可或缺的补充。",
      "categories": [
        "cs.SE",
        "cs.AI"
      ],
      "primary_category": "cs.SE",
      "comment": "Under revision at Jornadas de Cloud Computing, Big Data & Emerging\n  Topics (JCC-BD&ET) - 2025",
      "pdf_url": "http://arxiv.org/pdf/2505.09830v1",
      "published_date": "2025-05-14 22:22:15 UTC",
      "updated_date": "2025-05-14 22:22:15 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T22:47:50.528045"
    },
    {
      "arxiv_id": "2505.09814v2",
      "title": "$XX^{t}$ Can Be Faster",
      "title_zh": "翻译失败",
      "authors": [
        "Dmitry Rybin",
        "Yushun Zhang",
        "Zhi-Quan Luo"
      ],
      "abstract": "We present RXTX, a new algorithm for computing the product of matrix by its\ntranspose $XX^{t}$ for $X\\in \\mathbb{R}^{n\\times m}$. RXTX uses $5\\%$ fewer\nmultiplications and $5\\%$ fewer operations (additions and multiplications) than\nState-of-the-Art algorithms. Note that the accelerations not only holds\nasymptotically for large matrices with $n \\rightarrow \\infty$, but also for\nsmall matrices including $n = 4$. The algorithm was discovered by combining\nMachine Learning-based search methods with Combinatorial Optimization.",
      "tldr_zh": "本研究提出了一种新算法RXTX，用于计算矩阵$X$与其转置$X^T$的乘积$XX^T$，其中$X\\in \\mathbb{R}^{n\\times m}$。RXTX相比现有最先进算法减少了5%的乘法操作和总操作（加法与乘法），这种加速不仅适用于大矩阵（$n \\rightarrow \\infty$），还适用于小矩阵如$n=4$。算法通过结合Machine Learning-based search methods和Combinatorial Optimization来发现，从而提高了矩阵运算的效率。",
      "categories": [
        "cs.DS",
        "cs.AI",
        "cs.LG",
        "cs.SC",
        "68Q25, 68T20",
        "F.2.1; I.1.2"
      ],
      "primary_category": "cs.DS",
      "comment": "improved presentation",
      "pdf_url": "http://arxiv.org/pdf/2505.09814v2",
      "published_date": "2025-05-14 21:31:44 UTC",
      "updated_date": "2025-05-16 09:23:27 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T22:48:01.775155"
    },
    {
      "arxiv_id": "2505.09807v1",
      "title": "Exploring the generalization of LLM truth directions on conversational formats",
      "title_zh": "翻译失败",
      "authors": [
        "Timour Ichmoukhamedov",
        "David Martens"
      ],
      "abstract": "Several recent works argue that LLMs have a universal truth direction where\ntrue and false statements are linearly separable in the activation space of the\nmodel. It has been demonstrated that linear probes trained on a single hidden\nstate of the model already generalize across a range of topics and might even\nbe used for lie detection in LLM conversations. In this work we explore how\nthis truth direction generalizes between various conversational formats. We\nfind good generalization between short conversations that end on a lie, but\npoor generalization to longer formats where the lie appears earlier in the\ninput prompt. We propose a solution that significantly improves this type of\ngeneralization by adding a fixed key phrase at the end of each conversation.\nOur results highlight the challenges towards reliable LLM lie detectors that\ngeneralize to new settings.",
      "tldr_zh": "本文探讨了大型语言模型(LLM)中truth direction（真假语句在激活空间的线性可分性）在不同对话格式间的泛化能力。研究发现，truth direction在以谎言结束的短对话之间泛化良好，但在谎言早现的长对话格式中泛化较差。作者提出一种解决方案，通过在每个对话末尾添加固定关键短语，显著提升了这种泛化性能。这些结果突出了开发可靠的LLM谎言检测器时面临的挑战。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2505.09807v1",
      "published_date": "2025-05-14 21:21:08 UTC",
      "updated_date": "2025-05-14 21:21:08 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T22:48:14.803231"
    },
    {
      "arxiv_id": "2505.09805v1",
      "title": "Contextual Phenotyping of Pediatric Sepsis Cohort Using Large Language Models",
      "title_zh": "翻译失败",
      "authors": [
        "Aditya Nagori",
        "Ayush Gautam",
        "Matthew O. Wiens",
        "Vuong Nguyen",
        "Nathan Kenya Mugisha",
        "Jerome Kabakyenga",
        "Niranjan Kissoon",
        "John Mark Ansermino",
        "Rishikesan Kamaleswaran"
      ],
      "abstract": "Clustering patient subgroups is essential for personalized care and efficient\nresource use. Traditional clustering methods struggle with high-dimensional,\nheterogeneous healthcare data and lack contextual understanding. This study\nevaluates Large Language Model (LLM) based clustering against classical methods\nusing a pediatric sepsis dataset from a low-income country (LIC), containing\n2,686 records with 28 numerical and 119 categorical variables. Patient records\nwere serialized into text with and without a clustering objective. Embeddings\nwere generated using quantized LLAMA 3.1 8B, DeepSeek-R1-Distill-Llama-8B with\nlow-rank adaptation(LoRA), and Stella-En-400M-V5 models. K-means clustering was\napplied to these embeddings. Classical comparisons included K-Medoids\nclustering on UMAP and FAMD-reduced mixed data. Silhouette scores and\nstatistical tests evaluated cluster quality and distinctiveness.\nStella-En-400M-V5 achieved the highest Silhouette Score (0.86). LLAMA 3.1 8B\nwith the clustering objective performed better with higher number of clusters,\nidentifying subgroups with distinct nutritional, clinical, and socioeconomic\nprofiles. LLM-based methods outperformed classical techniques by capturing\nricher context and prioritizing key features. These results highlight potential\nof LLMs for contextual phenotyping and informed decision-making in\nresource-limited settings.",
      "tldr_zh": "本研究评估了大型语言模型(LLM)用于儿科败血症患者子群聚类的性能，针对传统方法在处理高维异构数据和缺乏上下文理解的局限性，使用一个包含2,686条记录的低收入国家数据集进行比较。研究将患者记录序列化为文本，并利用量化后的LLAMA 3.1 8B、DeepSeek-R1-Distill-Llama-8B（带LoRA）和Stella-En-400M-V5生成嵌入，然后应用K-means聚类。结果显示，Stella-En-400M-V5达到了最高的Silhouette Score (0.86)，LLM方法在高集群数量下更有效地识别出具有不同营养、临床和社会经济特征的子群，并显著优于经典的K-Medoids聚类，为资源有限环境中的个性化护理和决策提供潜在应用。",
      "categories": [
        "q-bio.QM",
        "cs.AI",
        "cs.LG",
        "stat.AP"
      ],
      "primary_category": "q-bio.QM",
      "comment": "11 pages, 2 Figures, 1 Table",
      "pdf_url": "http://arxiv.org/pdf/2505.09805v1",
      "published_date": "2025-05-14 21:05:40 UTC",
      "updated_date": "2025-05-14 21:05:40 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T22:48:29.042122"
    },
    {
      "arxiv_id": "2505.09796v1",
      "title": "Virtual Dosimetrists: A Radiotherapy Training \"Flight Simulator\"",
      "title_zh": "虚拟计量师：放射治疗训练的“飞行模拟器”",
      "authors": [
        "Skylar S. Gay",
        "Tucker Netherton",
        "Barbara Marquez",
        "Raymond Mumme",
        "Mary Gronberg",
        "Brent Parker",
        "Chelsea Pinnix",
        "Sanjay Shete",
        "Carlos Cardenas",
        "Laurence Court"
      ],
      "abstract": "Effective education in radiotherapy plan quality review requires a robust,\nregularly updated set of examples and the flexibility to demonstrate multiple\npossible planning approaches and their consequences. However, the current\nclinic-based paradigm does not support these needs. To address this, we have\ndeveloped 'Virtual Dosimetrist' models that can both generate training examples\nof suboptimal treatment plans and then allow trainees to improve the plan\nquality through simple natural language prompts, as if communicating with a\ndosimetrist. The dose generation and modification process is accurate, rapid,\nand requires only modest resources. This work is the first to combine dose\ndistribution prediction with natural language processing; providing a robust\npipeline for both generating suboptimal training plans and allowing trainees to\npractice their critical plan review and improvement skills that addresses the\nchallenges of the current clinic-based paradigm.",
      "tldr_zh": "该研究针对放射治疗计划质量审查教育的不足，开发了“Virtual Dosimetrist”模型，作为一种类似“飞行模拟器”的训练工具。该模型能生成次优治疗计划的训练示例，并允许学员通过简单自然语言提示改进计划质量，模拟与计量师的互动过程。该方法首次结合dose distribution prediction和natural language processing，提供了一个准确、快速且资源需求适中的稳健pipeline，解决了当前临床范式中缺乏灵活性和示例更新的挑战。",
      "categories": [
        "physics.med-ph",
        "cs.AI"
      ],
      "primary_category": "physics.med-ph",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2505.09796v1",
      "published_date": "2025-05-14 20:47:13 UTC",
      "updated_date": "2025-05-14 20:47:13 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T22:48:37.085871"
    },
    {
      "arxiv_id": "2505.09794v1",
      "title": "Automated Detection of Clinical Entities in Lung and Breast Cancer Reports Using NLP Techniques",
      "title_zh": "利用 NLP 技术对肺癌和乳腺癌报告中临床实体的自动检测",
      "authors": [
        "J. Moreno-Casanova",
        "J. M. Auñón",
        "A. Mártinez-Pérez",
        "M. E. Pérez-Martínez",
        "M. E. Gas-López"
      ],
      "abstract": "Research projects, including those focused on cancer, rely on the manual\nextraction of information from clinical reports. This process is time-consuming\nand prone to errors, limiting the efficiency of data-driven approaches in\nhealthcare. To address these challenges, Natural Language Processing (NLP)\noffers an alternative for automating the extraction of relevant data from\nelectronic health records (EHRs). In this study, we focus on lung and breast\ncancer due to their high incidence and the significant impact they have on\npublic health. Early detection and effective data management in both types of\ncancer are crucial for improving patient outcomes. To enhance the accuracy and\nefficiency of data extraction, we utilized GMV's NLP tool uQuery, which excels\nat identifying relevant entities in clinical texts and converting them into\nstandardized formats such as SNOMED and OMOP. uQuery not only detects and\nclassifies entities but also associates them with contextual information,\nincluding negated entities, temporal aspects, and patient-related details. In\nthis work, we explore the use of NLP techniques, specifically Named Entity\nRecognition (NER), to automatically identify and extract key clinical\ninformation from EHRs related to these two cancers. A dataset from Health\nResearch Institute Hospital La Fe (IIS La Fe), comprising 200 annotated breast\ncancer and 400 lung cancer reports, was used, with eight clinical entities\nmanually labeled using the Doccano platform. To perform NER, we fine-tuned the\nbsc-bio-ehr-en3 model, a RoBERTa-based biomedical linguistic model pre-trained\nin Spanish. Fine-tuning was performed using the Transformers architecture,\nenabling accurate recognition of clinical entities in these cancer types. Our\nresults demonstrate strong overall performance, particularly in identifying\nentities like MET and PAT, although challenges remain with less frequent\nentities like EVOL.",
      "tldr_zh": "这篇论文使用 Natural Language Processing (NLP) 技术自动化检测肺癌和乳腺癌临床报告中的临床实体，以解决手动提取信息耗时且易出错的问题。研究团队采用 GMV 的 uQuery 工具进行实体识别和标准化（如 SNOMED 和 OMOP），并结合 Named Entity Recognition (NER) 方法，通过细调 bsc-bio-ehr-en3（基于 RoBERTa 的生物医学模型）来处理西班牙语 EHR 数据。实验基于 Health Research Institute Hospital La Fe 的数据集，包括 200 份乳腺癌和 400 份肺癌报告，手动标注了八个实体。结果显示，模型在识别 MET 和 PAT 等实体上表现出色，但对 EVOL 等较少见的实体仍存在挑战，从而提升了医疗数据提取的效率和准确性。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2505.09794v1",
      "published_date": "2025-05-14 20:44:29 UTC",
      "updated_date": "2025-05-14 20:44:29 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T22:48:51.903482"
    },
    {
      "arxiv_id": "2505.09787v1",
      "title": "A Multimodal Multi-Agent Framework for Radiology Report Generation",
      "title_zh": "翻译失败",
      "authors": [
        "Ziruo Yi",
        "Ting Xiao",
        "Mark V. Albert"
      ],
      "abstract": "Radiology report generation (RRG) aims to automatically produce diagnostic\nreports from medical images, with the potential to enhance clinical workflows\nand reduce radiologists' workload. While recent approaches leveraging\nmultimodal large language models (MLLMs) and retrieval-augmented generation\n(RAG) have achieved strong results, they continue to face challenges such as\nfactual inconsistency, hallucination, and cross-modal misalignment. We propose\na multimodal multi-agent framework for RRG that aligns with the stepwise\nclinical reasoning workflow, where task-specific agents handle retrieval, draft\ngeneration, visual analysis, refinement, and synthesis. Experimental results\ndemonstrate that our approach outperforms a strong baseline in both automatic\nmetrics and LLM-based evaluations, producing more accurate, structured, and\ninterpretable reports. This work highlights the potential of clinically aligned\nmulti-agent frameworks to support explainable and trustworthy clinical AI\napplications.",
      "tldr_zh": "该研究提出了一种多模态多智能体框架，用于放射学报告生成（Radiology Report Generation），旨在解决现有方法如多模态大语言模型（MLLMs）和检索增强生成（RAG）面临的实际不一致、幻觉和跨模态不对齐等问题。该框架与逐步临床推理工作流程对齐，由任务特定智能体负责检索、草稿生成、视觉分析、精炼和合成，确保报告的准确性和结构化。实验结果显示，该方法在自动指标和基于LLM的评估中优于强基线，生成更精确、可解释的报告，从而突显了临床对齐多智能体框架在支持可信赖临床AI应用方面的潜力。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2505.09787v1",
      "published_date": "2025-05-14 20:28:04 UTC",
      "updated_date": "2025-05-14 20:28:04 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T22:49:03.404733"
    },
    {
      "arxiv_id": "2505.10589v2",
      "title": "Super-Resolution Generative Adversarial Networks based Video Enhancement",
      "title_zh": "基于超分辨率生成对抗网络的视频增强",
      "authors": [
        "Kağan ÇETİN"
      ],
      "abstract": "This study introduces an enhanced approach to video super-resolution by\nextending ordinary Single-Image Super-Resolution (SISR) Super-Resolution\nGenerative Adversarial Network (SRGAN) structure to handle spatio-temporal\ndata. While SRGAN has proven effective for single-image enhancement, its design\ndoes not account for the temporal continuity required in video processing. To\naddress this, a modified framework that incorporates 3D Non-Local Blocks is\nproposed, which is enabling the model to capture relationships across both\nspatial and temporal dimensions. An experimental training pipeline is\ndeveloped, based on patch-wise learning and advanced data degradation\ntechniques, to simulate real-world video conditions and learn from both local\nand global structures and details. This helps the model generalize better and\nmaintain stability across varying video content while maintaining the general\nstructure besides the pixel-wise correctness. Two model variants-one larger and\none more lightweight-are presented to explore the trade-offs between\nperformance and efficiency. The results demonstrate improved temporal\ncoherence, sharper textures, and fewer visual artifacts compared to traditional\nsingle-image methods. This work contributes to the development of practical,\nlearning-based solutions for video enhancement tasks, with potential\napplications in streaming, gaming, and digital restoration.",
      "tldr_zh": "本研究扩展了 SRGAN（Super-Resolution Generative Adversarial Network）框架，从单图像超分辨率（SISR）扩展到视频处理，旨在解决视频的时空连续性问题。研究提出了一种修改版框架，融入 3D Non-Local Blocks，以捕捉空间和时间维度的关系，并采用基于 patch-wise learning 和高级数据降级技术的训练管道，模拟真实视频条件以提升模型泛化性和稳定性。实验结果显示，该方法在两个模型变体（一个较大模型和一个轻量级模型）中实现了更好的时间连贯性、更锐利的纹理以及减少的视觉伪像，并为视频增强任务提供实用学习-based 解决方案，适用于流媒体、游戏和数字修复领域。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "eess.IV",
        "I.4.3"
      ],
      "primary_category": "cs.CV",
      "comment": "28 pages, 14 figures, 3 tables",
      "pdf_url": "http://arxiv.org/pdf/2505.10589v2",
      "published_date": "2025-05-14 20:16:51 UTC",
      "updated_date": "2025-05-19 03:54:02 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T22:49:16.048142"
    },
    {
      "arxiv_id": "2505.09766v2",
      "title": "On the Well-Posedness of Green's Function Reconstruction via the Kirchhoff-Helmholtz Equation for One-Speed Neutron Diffusion",
      "title_zh": "翻译失败",
      "authors": [
        "Roberto Ponciroli"
      ],
      "abstract": "This work presents a methodology for reconstructing the spatial distribution\nof the neutron flux in a nuclear reactor, leveraging real-time measurements\nobtained from ex-core detectors. The Kirchhoff-Helmholtz (K-H) equation\ninherently defines the problem of estimating a scalar field within a domain\nbased on boundary data, making it a natural mathematical framework for this\ntask. The main challenge lies in deriving the Green's function specific to the\ndomain and the neutron diffusion process. While analytical solutions for\nGreen's functions exist for simplified geometries, their derivation of complex,\nheterogeneous domains-such as a nuclear reactor-requires a numerical approach.\nThe objective of this work is to demonstrate the well-posedness of the\ndata-driven Green's function approximation by formulating and solving the K-H\nequation as an inverse problem. After establishing the symmetry properties that\nthe Green's function must satisfy, the K-H equation is derived from the\none-speed neutron diffusion model. This is followed by a comprehensive\ndescription of the procedure for interpreting sensor readings and implementing\nthe neutron flux reconstruction algorithm. Finally, the existence and\nuniqueness of the Green's function inferred from the sampled data are\ndemonstrated, ensuring the reliability of the proposed method and its\npredictions.",
      "tldr_zh": "本文提出了一种基于 Kirchhoff-Helmholtz equation 的方法，利用外核探测器的实时测量重建核反应堆中子通量的空间分布，针对复杂异构域采用数据驱动的数值方法。研究从 one-speed neutron diffusion 模型推导 K-H 方程，建立了 Green's function 的对称性并描述了传感器数据解释和通量重建算法。最终，证明了从采样数据推断的 Green's function 具有存在性和唯一性，确保了方法的良好定义性（well-posedness）和预测可靠性。",
      "categories": [
        "math.NA",
        "cs.AI",
        "cs.NA"
      ],
      "primary_category": "math.NA",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2505.09766v2",
      "published_date": "2025-05-14 19:53:09 UTC",
      "updated_date": "2025-05-18 15:30:46 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T22:49:27.641952"
    },
    {
      "arxiv_id": "2505.09757v1",
      "title": "Trustless Autonomy: Understanding Motivations, Benefits and Governance Dilemma in Self-Sovereign Decentralized AI Agents",
      "title_zh": "无需信任的自治：理解自我主权的去中心化 AI 代理中的动机、益处和治理困境",
      "authors": [
        "Botao Amber Hu",
        "Yuhan Liu",
        "Helena Rong"
      ],
      "abstract": "The recent trend of self-sovereign Decentralized AI Agents (DeAgents)\ncombines Large Language Model (LLM)-based AI agents with decentralization\ntechnologies such as blockchain smart contracts and trusted execution\nenvironments (TEEs). These tamper-resistant trustless substrates allow agents\nto achieve self-sovereignty through ownership of cryptowallet private keys and\ncontrol of digital assets and social media accounts. DeAgent eliminates\ncentralized control and reduces human intervention, addressing key trust\nconcerns inherent in centralized AI systems. However, given ongoing challenges\nin LLM reliability such as hallucinations, this creates paradoxical tension\nbetween trustlessness and unreliable autonomy. This study addresses this\nempirical research gap through interviews with DeAgents stakeholders-experts,\nfounders, and developers-to examine their motivations, benefits, and governance\ndilemmas. The findings will guide future DeAgents system and protocol design\nand inform discussions about governance in sociotechnical AI systems in the\nfuture agentic web.",
      "tldr_zh": "该研究探讨了自主 Decentralized AI Agents (DeAgents)，即结合LLM和区块链技术（如智能合约和TEEs）的代理系统，这些系统通过加密钱包私钥实现对数字资产和社会媒体的控制，从而消除集中式AI的信任问题并减少人为干预。研究发现，DeAgents虽能提升自治性，但LLM的可靠性问题（如幻觉）导致了信任与自治的矛盾。作者通过对专家、创始人和发展者的访谈，分析了DeAgents的动机、益处和治理困境，为未来DeAgents系统设计及AI治理讨论提供指导。",
      "categories": [
        "cs.HC",
        "cs.AI",
        "cs.CY"
      ],
      "primary_category": "cs.HC",
      "comment": "Submitted to CSCW 2026",
      "pdf_url": "http://arxiv.org/pdf/2505.09757v1",
      "published_date": "2025-05-14 19:42:43 UTC",
      "updated_date": "2025-05-14 19:42:43 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T22:49:39.388723"
    },
    {
      "arxiv_id": "2505.09755v1",
      "title": "Explainability Through Human-Centric Design for XAI in Lung Cancer Detection",
      "title_zh": "翻译失败",
      "authors": [
        "Amy Rafferty",
        "Rishi Ramaesh",
        "Ajitha Rajan"
      ],
      "abstract": "Deep learning models have shown promise in lung pathology detection from\nchest X-rays, but widespread clinical adoption remains limited due to opaque\nmodel decision-making. In prior work, we introduced ClinicXAI, a human-centric,\nexpert-guided concept bottleneck model (CBM) designed for interpretable lung\ncancer diagnosis. We now extend that approach and present XpertXAI, a\ngeneralizable expert-driven model that preserves human-interpretable clinical\nconcepts while scaling to detect multiple lung pathologies. Using a\nhigh-performing InceptionV3-based classifier and a public dataset of chest\nX-rays with radiology reports, we compare XpertXAI against leading post-hoc\nexplainability methods and an unsupervised CBM, XCBs. We assess explanations\nthrough comparison with expert radiologist annotations and medical ground\ntruth. Although XpertXAI is trained for multiple pathologies, our expert\nvalidation focuses on lung cancer. We find that existing techniques frequently\nfail to produce clinically meaningful explanations, omitting key diagnostic\nfeatures and disagreeing with radiologist judgments. XpertXAI not only\noutperforms these baselines in predictive accuracy but also delivers\nconcept-level explanations that better align with expert reasoning. While our\nfocus remains on explainability in lung cancer detection, this work illustrates\nhow human-centric model design can be effectively extended to broader\ndiagnostic contexts - offering a scalable path toward clinically meaningful\nexplainable AI in medical diagnostics.",
      "tldr_zh": "本研究扩展了之前的ClinicXAI框架，提出XpertXAI，这是一个专家驱动的概念瓶颈模型(CBM)，旨在通过人类中心设计提升XAI在肺癌检测中的可解释性，同时扩展到检测多种肺部病变。使用InceptionV3-based分类器和公共胸部X光数据集，XpertXAI与现有后验解释方法和无监督CBM(XCBs)进行比较，结果显示其在预测准确性上优于基线，且提供的概念级解释更符合放射科医生的判断和临床事实。总体而言，该工作证明了人类中心模型设计的可扩展性，为XAI在更广泛医疗诊断中的应用提供了可靠路径。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2505.09755v1",
      "published_date": "2025-05-14 19:40:12 UTC",
      "updated_date": "2025-05-14 19:40:12 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T22:49:54.292959"
    },
    {
      "arxiv_id": "2505.11545v1",
      "title": "TARGET: Benchmarking Table Retrieval for Generative Tasks",
      "title_zh": "TARGET：生成任务的表格检索基准测试",
      "authors": [
        "Xingyu Ji",
        "Parker Glenn",
        "Aditya G. Parameswaran",
        "Madelon Hulsebos"
      ],
      "abstract": "The data landscape is rich with structured data, often of high value to\norganizations, driving important applications in data analysis and machine\nlearning. Recent progress in representation learning and generative models for\nsuch data has led to the development of natural language interfaces to\nstructured data, including those leveraging text-to-SQL. Contextualizing\ninteractions, either through conversational interfaces or agentic components,\nin structured data through retrieval-augmented generation can provide\nsubstantial benefits in the form of freshness, accuracy, and comprehensiveness\nof answers. The key question is: how do we retrieve the right table(s) for the\nanalytical query or task at hand? To this end, we introduce TARGET: a benchmark\nfor evaluating TAble Retrieval for GEnerative Tasks. With TARGET we analyze the\nretrieval performance of different retrievers in isolation, as well as their\nimpact on downstream tasks. We find that dense embedding-based retrievers far\noutperform a BM25 baseline which is less effective than it is for retrieval\nover unstructured text. We also surface the sensitivity of retrievers across\nvarious metadata (e.g., missing table titles), and demonstrate a stark\nvariation of retrieval performance across datasets and tasks. TARGET is\navailable at https://target-benchmark.github.io.",
      "tldr_zh": "这篇论文引入了 TARGET 基准，用于评估表检索（Table Retrieval）在生成任务（Generative Tasks）中的性能，旨在解决如何为分析查询检索合适表格的问题。研究者分析了不同检索器的表现，包括密集嵌入-based retrievers 和 BM25 基线，发现前者远优于后者，尤其在结构化数据上。实验结果揭示了检索器对元数据（如表标题缺失）的敏感性，以及性能在不同数据集和任务间的显著差异；TARGET 基准可从 https://target-benchmark.github.io 获取。",
      "categories": [
        "cs.IR",
        "cs.AI",
        "cs.CL",
        "cs.DB"
      ],
      "primary_category": "cs.IR",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2505.11545v1",
      "published_date": "2025-05-14 19:39:46 UTC",
      "updated_date": "2025-05-14 19:39:46 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T22:50:04.546288"
    },
    {
      "arxiv_id": "2505.09747v1",
      "title": "Healthy Distrust in AI systems",
      "title_zh": "健康的 AI 系统不信任",
      "authors": [
        "Benjamin Paaßen",
        "Suzana Alpsancar",
        "Tobias Matzner",
        "Ingrid Scharlau"
      ],
      "abstract": "Under the slogan of trustworthy AI, much of contemporary AI research is\nfocused on designing AI systems and usage practices that inspire human trust\nand, thus, enhance adoption of AI systems. However, a person affected by an AI\nsystem may not be convinced by AI system design alone -- neither should they,\nif the AI system is embedded in a social context that gives good reason to\nbelieve that it is used in tension with a person's interest. In such cases,\ndistrust in the system may be justified and necessary to build meaningful trust\nin the first place. We propose the term \"healthy distrust\" to describe such a\njustified, careful stance towards certain AI usage practices. We investigate\nprior notions of trust and distrust in computer science, sociology, history,\npsychology, and philosophy, outline a remaining gap that healthy distrust might\nfill and conceptualize healthy distrust as a crucial part for AI usage that\nrespects human autonomy.",
      "tldr_zh": "该研究质疑了当前“trustworthy AI”研究对人类信任的过度强调，指出如果AI系统置于与个人利益冲突的社会环境中，仅靠系统设计不足以说服用户。作者提出“healthy distrust”概念，指的是对某些AI使用实践的正当和谨慎不信任，这种态度有助于在必要时保护个人利益。论文通过回顾计算机科学、社会学、历史、心理学和哲学领域的信任与不信任概念，填补了现有空白，并将healthy distrust定位为尊重人类自治的关键要素。",
      "categories": [
        "cs.CY",
        "cs.AI"
      ],
      "primary_category": "cs.CY",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2505.09747v1",
      "published_date": "2025-05-14 19:13:47 UTC",
      "updated_date": "2025-05-14 19:13:47 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T22:50:15.754315"
    },
    {
      "arxiv_id": "2505.09742v1",
      "title": "A Generative Neural Annealer for Black-Box Combinatorial Optimization",
      "title_zh": "翻译失败",
      "authors": [
        "Yuan-Hang Zhang",
        "Massimiliano Di Ventra"
      ],
      "abstract": "We propose a generative, end-to-end solver for black-box combinatorial\noptimization that emphasizes both sample efficiency and solution quality on NP\nproblems. Drawing inspiration from annealing-based algorithms, we treat the\nblack-box objective as an energy function and train a neural network to model\nthe associated Boltzmann distribution. By conditioning on temperature, the\nnetwork captures a continuum of distributions--from near-uniform at high\ntemperatures to sharply peaked around global optima at low\ntemperatures--thereby learning the structure of the energy landscape and\nfacilitating global optimization. When queries are expensive, the\ntemperature-dependent distributions naturally enable data augmentation and\nimprove sample efficiency. When queries are cheap but the problem remains hard,\nthe model learns implicit variable interactions, effectively \"opening\" the\nblack box. We validate our approach on challenging combinatorial tasks under\nboth limited and unlimited query budgets, showing competitive performance\nagainst state-of-the-art black-box optimizers.",
      "tldr_zh": "本文提出了一种生成式神经退火器（Generative Neural Annealer），作为端到端的黑箱组合优化求解器，旨在提升样本效率和解决方案质量。方法受退火算法启发，将黑箱目标视为能量函数，训练神经网络建模Boltzmann分布，通过温度条件捕捉从均匀分布到全局最优的连续分布，从而学习能量景观结构并促进全局优化。在有限和无限查询预算下，该方法在各种挑战性组合任务上表现出色，与最先进黑箱优化器竞争，并通过数据增强和隐式变量交互提升了性能。",
      "categories": [
        "cs.LG",
        "cond-mat.dis-nn",
        "cond-mat.stat-mech",
        "cs.AI",
        "cs.NE"
      ],
      "primary_category": "cs.LG",
      "comment": "15 pages, 3 figures",
      "pdf_url": "http://arxiv.org/pdf/2505.09742v1",
      "published_date": "2025-05-14 19:05:19 UTC",
      "updated_date": "2025-05-14 19:05:19 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T22:50:27.962648"
    },
    {
      "arxiv_id": "2505.09738v1",
      "title": "Achieving Tokenizer Flexibility in Language Models through Heuristic Adaptation and Supertoken Learning",
      "title_zh": "翻译失败",
      "authors": [
        "Shaurya Sharthak",
        "Vinayak Pahalwan",
        "Adithya Kamath",
        "Adarsh Shirawalmath"
      ],
      "abstract": "Pretrained language models (LLMs) are often constrained by their fixed\ntokenization schemes, leading to inefficiencies and performance limitations,\nparticularly for multilingual or specialized applications. This tokenizer\nlock-in presents significant challenges. standard methods to overcome this\noften require prohibitive computational resources. Although tokenizer\nreplacement with heuristic initialization aims to reduce this burden, existing\nmethods often require exhaustive residual fine-tuning and still may not fully\npreserve semantic nuances or adequately address the underlying compression\ninefficiencies. Our framework introduces two innovations: first, Tokenadapt, a\nmodel-agnostic tokenizer transplantation method, and second, novel\npre-tokenization learning for multi-word Supertokens to enhance compression and\nreduce fragmentation. Tokenadapt initializes new unique token embeddings via a\nhybrid heuristic that combines two methods: a local estimate based on subword\ndecomposition using the old tokenizer, and a global estimate utilizing the\ntop-k semantically similar tokens from the original vocabulary. This\nmethodology aims to preserve semantics while significantly minimizing\nretraining requirements. Empirical investigations validate both contributions:\nthe transplantation heuristic successfully initializes unique tokens, markedly\noutperforming conventional baselines and sophisticated methods including\nTranstokenizer and ReTok, while our Supertokens achieve notable compression\ngains. Our zero-shot perplexity results demonstrate that the TokenAdapt hybrid\ninitialization consistently yields lower perplexity ratios compared to both\nReTok and TransTokenizer baselines across different base models and newly\ntrained target tokenizers. TokenAdapt typically reduced the overall perplexity\nratio significantly compared to ReTok, yielding at least a 2-fold improvement\nin these aggregate scores.",
      "tldr_zh": "本论文解决了预训练语言模型(LLMs)中固定分词方案导致的效率和性能问题，特别是针对多语言或专业应用，通过提出Tokenadapt和Supertokens学习两种创新方法来实现分词器灵活性。Tokenadapt是一种模型无关的分词器移植技术，使用混合启发式初始化——结合子词分解的局部估计和语义相似性全局估计——以保留语义并最小化重新训练需求，同时Supertokens通过预分词学习提升压缩效率并减少碎片。实验结果表明，Tokenadapt在初始化独特标记方面显著优于基线方法如Transtokenizer和ReTok，且零样本困惑度(perplexity)比率至少降低了2倍，证明了其在性能提升方面的有效性。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2505.09738v1",
      "published_date": "2025-05-14 19:00:27 UTC",
      "updated_date": "2025-05-14 19:00:27 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T22:50:40.777059"
    },
    {
      "arxiv_id": "2505.09737v1",
      "title": "General Dynamic Goal Recognition",
      "title_zh": "通用动态目标识别",
      "authors": [
        "Osher Elhadad",
        "Reuth Mirsky"
      ],
      "abstract": "Understanding an agent's intent through its behavior is essential in\nhuman-robot interaction, interactive AI systems, and multi-agent\ncollaborations. This task, known as Goal Recognition (GR), poses significant\nchallenges in dynamic environments where goals are numerous and constantly\nevolving. Traditional GR methods, designed for a predefined set of goals, often\nstruggle to adapt to these dynamic scenarios. To address this limitation, we\nintroduce the General Dynamic GR problem - a broader definition of GR - aimed\nat enabling real-time GR systems and fostering further research in this area.\nExpanding on this foundation, this paper employs a model-free goal-conditioned\nRL approach to enable fast adaptation for GR across various changing tasks.",
      "tldr_zh": "该论文探讨了在动态环境中通过代理行为理解其意图的 Goal Recognition (GR) 问题，强调传统方法因依赖预定义目标而难以适应目标众多且不断变化的场景。论文引入 General Dynamic GR 问题，这是一个更广泛的定义，旨在支持实时 GR 系统并促进相关研究。作者采用 model-free goal-conditioned RL 方法，实现 GR 在各种变化任务中的快速适应，从而提升了交互式 AI 系统和多代理协作的灵活性。",
      "categories": [
        "cs.AI",
        "cs.RO"
      ],
      "primary_category": "cs.AI",
      "comment": "Accepted for publication at Generalization in Planning (GenPlan) as\n  part of AAAI 2025 workshops",
      "pdf_url": "http://arxiv.org/pdf/2505.09737v1",
      "published_date": "2025-05-14 18:57:51 UTC",
      "updated_date": "2025-05-14 18:57:51 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T22:50:50.812995"
    },
    {
      "arxiv_id": "2505.09733v1",
      "title": "Robust Federated Learning with Confidence-Weighted Filtering and GAN-Based Completion under Noisy and Incomplete Data",
      "title_zh": "翻译失败",
      "authors": [
        "Alpaslan Gokcen",
        "Ali Boyaci"
      ],
      "abstract": "Federated learning (FL) presents an effective solution for collaborative\nmodel training while maintaining data privacy across decentralized client\ndatasets. However, data quality issues such as noisy labels, missing classes,\nand imbalanced distributions significantly challenge its effectiveness. This\nstudy proposes a federated learning methodology that systematically addresses\ndata quality issues, including noise, class imbalance, and missing labels. The\nproposed approach systematically enhances data integrity through adaptive noise\ncleaning, collaborative conditional GAN-based synthetic data generation, and\nrobust federated model training. Experimental evaluations conducted on\nbenchmark datasets (MNIST and Fashion-MNIST) demonstrate significant\nimprovements in federated model performance, particularly macro-F1 Score, under\nvarying noise and class imbalance conditions. Additionally, the proposed\nframework carefully balances computational feasibility and substantial\nperformance gains, ensuring practicality for resource constrained edge devices\nwhile rigorously maintaining data privacy. Our results indicate that this\nmethod effectively mitigates common data quality challenges, providing a\nrobust, scalable, and privacy compliant solution suitable for diverse\nreal-world federated learning scenarios.",
      "tldr_zh": "本研究提出了一种稳健的Federated Learning方法，通过置信度加权过滤和基于GAN的合成数据补全，系统解决数据质量问题，如嘈杂标签、类别不平衡和缺失标签。该方法结合自适应噪声清理、协作条件GAN生成合成数据，以及优化后的联邦模型训练，确保在保持数据隐私的前提下提升模型性能。在MNIST和Fashion-MNIST基准数据集上的实验表明，该框架显著提高了宏F1 Score，并在不同噪声和不平衡条件下表现出色，同时兼顾资源受限设备的可行性和实际应用潜力。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CR"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2505.09733v1",
      "published_date": "2025-05-14 18:49:18 UTC",
      "updated_date": "2025-05-14 18:49:18 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T22:51:03.615278"
    },
    {
      "arxiv_id": "2505.09724v2",
      "title": "An AI-Powered Research Assistant in the Lab: A Practical Guide for Text Analysis Through Iterative Collaboration with LLMs",
      "title_zh": "一个AI驱动的实验室研究助理：通过与LLMs的迭代协作进行文本分析的实用指南",
      "authors": [
        "Gino Carmona-Díaz",
        "William Jiménez-Leal",
        "María Alejandra Grisales",
        "Chandra Sripada",
        "Santiago Amaya",
        "Michael Inzlicht",
        "Juan Pablo Bermúdez"
      ],
      "abstract": "Analyzing texts such as open-ended responses, headlines, or social media\nposts is a time- and labor-intensive process highly susceptible to bias. LLMs\nare promising tools for text analysis, using either a predefined (top-down) or\na data-driven (bottom-up) taxonomy, without sacrificing quality. Here we\npresent a step-by-step tutorial to efficiently develop, test, and apply\ntaxonomies for analyzing unstructured data through an iterative and\ncollaborative process between researchers and LLMs. Using personal goals\nprovided by participants as an example, we demonstrate how to write prompts to\nreview datasets and generate a taxonomy of life domains, evaluate and refine\nthe taxonomy through prompt and direct modifications, test the taxonomy and\nassess intercoder agreements, and apply the taxonomy to categorize an entire\ndataset with high intercoder reliability. We discuss the possibilities and\nlimitations of using LLMs for text analysis.",
      "tldr_zh": "这篇论文提供了一个实用指南，展示如何通过研究人员与 LLMs 的迭代协作，进行文本分析以减少偏见和提高效率。论文介绍了一种逐步方法，包括编写提示生成 top-down 或 bottom-up 分类法、评估和完善分类法、测试 intercoder agreements，并以参与者的个人目标数据集为例应用该方法。结果显示，该过程能实现高 intercoder reliability，同时论文讨论了使用 LLMs 进行文本分析的可能性和局限性。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.HC"
      ],
      "primary_category": "cs.CL",
      "comment": "31 pages, 1 figure",
      "pdf_url": "http://arxiv.org/pdf/2505.09724v2",
      "published_date": "2025-05-14 18:32:18 UTC",
      "updated_date": "2025-05-16 11:47:10 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T22:51:15.934343"
    },
    {
      "arxiv_id": "2505.09716v2",
      "title": "Out-of-distribution generalisation is hard: evidence from ARC-like tasks",
      "title_zh": "翻译失败",
      "authors": [
        "George Dimitriadis",
        "Spyridon Samothrakis"
      ],
      "abstract": "Out-of-distribution (OOD) generalisation is considered a hallmark of human\nand animal intelligence. To achieve OOD through composition, a system must\ndiscover the environment-invariant properties of experienced input-output\nmappings and transfer them to novel inputs. This can be realised if an\nintelligent system can identify appropriate, task-invariant, and composable\ninput features, as well as the composition methods, thus allowing it to act\nbased not on the interpolation between learnt data points but on the\ntask-invariant composition of those features. We propose that in order to\nconfirm that an algorithm does indeed learn compositional structures from data,\nit is not enough to just test on an OOD setup, but one also needs to confirm\nthat the features identified are indeed compositional. We showcase this by\nexploring two tasks with clearly defined OOD metrics that are not OOD solvable\nby three commonly used neural networks: a Multi-Layer Perceptron (MLP), a\nConvolutional Neural Network (CNN), and a Transformer. In addition, we develop\ntwo novel network architectures imbued with biases that allow them to be\nsuccessful in OOD scenarios. We show that even with correct biases and almost\nperfect OOD performance, an algorithm can still fail to learn the correct\nfeatures for compositional generalisation.",
      "tldr_zh": "该研究探讨了Out-of-distribution (OOD) 泛化在人工智能中的挑战，认为OOD泛化需要系统识别环境不变的输入-输出映射属性，并实现任务不变的特征组合，而不仅仅依赖于数据插值。作者通过两个ARC-like任务测试了Multi-Layer Perceptron (MLP)、Convolutional Neural Network (CNN)和Transformer等常见神经网络，发现这些模型无法有效处理这些OOD场景。研究开发了两种新型网络架构，内置特定偏差以提升OOD性能，但实验结果显示，即使达到近乎完美的OOD准确率，这些模型仍可能未能学习正确的组合特征，从而强调了验证特征组合性的必要性。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "Submission to NeurIPS 2025",
      "pdf_url": "http://arxiv.org/pdf/2505.09716v2",
      "published_date": "2025-05-14 18:21:21 UTC",
      "updated_date": "2025-05-16 15:28:45 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T22:51:28.416004"
    },
    {
      "arxiv_id": "2505.09704v1",
      "title": "Energy-Efficient Federated Learning for AIoT using Clustering Methods",
      "title_zh": "翻译失败",
      "authors": [
        "Roberto Pereira",
        "Fernanda Famá",
        "Charalampos Kalalas",
        "Paolo Dini"
      ],
      "abstract": "While substantial research has been devoted to optimizing model performance,\nconvergence rates, and communication efficiency, the energy implications of\nfederated learning (FL) within Artificial Intelligence of Things (AIoT)\nscenarios are often overlooked in the existing literature. This study examines\nthe energy consumed during the FL process, focusing on three main\nenergy-intensive processes: pre-processing, communication, and local learning,\nall contributing to the overall energy footprint. We rely on the observation\nthat device/client selection is crucial for speeding up the convergence of\nmodel training in a distributed AIoT setting and propose two\nclustering-informed methods. These clustering solutions are designed to group\nAIoT devices with similar label distributions, resulting in clusters composed\nof nearly heterogeneous devices. Hence, our methods alleviate the heterogeneity\noften encountered in real-world distributed learning applications. Throughout\nextensive numerical experimentation, we demonstrate that our clustering\nstrategies typically achieve high convergence rates while maintaining low\nenergy consumption when compared to other recent approaches available in the\nliterature.",
      "tldr_zh": "这篇论文探讨了在人工智能物联网(AIoT)中联邦学习(FL)的能量效率问题，重点分析预处理、通信和本地学习等能量密集过程。作者提出两种基于聚类的设备选择方法，将标签分布相似的AIoT设备分组，以缓解真实世界分布式学习中的异质性问题。实验结果表明，这些策略在保持低能量消耗的同时，实现了比现有方法更高的收敛率。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2505.09704v1",
      "published_date": "2025-05-14 18:04:58 UTC",
      "updated_date": "2025-05-14 18:04:58 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T22:51:39.223206"
    },
    {
      "arxiv_id": "2505.09698v1",
      "title": "ManipBench: Benchmarking Vision-Language Models for Low-Level Robot Manipulation",
      "title_zh": "ManipBench：用于低级别机器人操作的视觉语言模型基准测试",
      "authors": [
        "Enyu Zhao",
        "Vedant Raval",
        "Hejia Zhang",
        "Jiageng Mao",
        "Zeyu Shangguan",
        "Stefanos Nikolaidis",
        "Yue Wang",
        "Daniel Seita"
      ],
      "abstract": "Vision-Language Models (VLMs) have revolutionized artificial intelligence and\nrobotics due to their commonsense reasoning capabilities. In robotic\nmanipulation, VLMs are used primarily as high-level planners, but recent work\nhas also studied their lower-level reasoning ability, which refers to making\ndecisions about precise robot movements. However, the community currently lacks\na clear and common benchmark that can evaluate how well VLMs can aid low-level\nreasoning in robotics. Consequently, we propose a novel benchmark, ManipBench,\nto evaluate the low-level robot manipulation reasoning capabilities of VLMs\nacross various dimensions, including how well they understand object-object\ninteractions and deformable object manipulation. We extensively test 33\nrepresentative VLMs across 10 model families on our benchmark, including\nvariants to test different model sizes. Our evaluation shows that the\nperformance of VLMs significantly varies across tasks, and there is a strong\ncorrelation between this performance and trends in our real-world manipulation\ntasks. It also shows that there remains a significant gap between these models\nand human-level understanding. See our website at:\nhttps://manipbench.github.io.",
      "tldr_zh": "该论文提出ManipBench，一个新的基准，用于评估Vision-Language Models (VLMs)在低层机器人操作中的推理能力，特别是针对物体-物体互动和可变形物体操作等维度。研究者测试了33个代表性VLMs（来自10个模型家族，包括不同大小的变体），结果显示这些模型在不同任务上的性能差异显著，并与真实世界操作任务的趋势高度相关。尽管VLMs表现出色，但与人类水平的理解仍存在明显差距。更多细节可参考论文网站https://manipbench.github.io。",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "comment": "47 pages, 29 figures. Under review",
      "pdf_url": "http://arxiv.org/pdf/2505.09698v1",
      "published_date": "2025-05-14 18:01:00 UTC",
      "updated_date": "2025-05-14 18:01:00 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T22:51:52.147738"
    },
    {
      "arxiv_id": "2505.09614v1",
      "title": "Language Agents Mirror Human Causal Reasoning Biases. How Can We Help Them Think Like Scientists?",
      "title_zh": "翻译失败",
      "authors": [
        "Anthony GX-Chen",
        "Dongyan Lin",
        "Mandana Samiei",
        "Doina Precup",
        "Blake A. Richards",
        "Rob Fergus",
        "Kenneth Marino"
      ],
      "abstract": "Language model (LM) agents are increasingly used as autonomous\ndecision-makers who need to actively gather information to guide their\ndecisions. A crucial cognitive skill for such agents is the efficient\nexploration and understanding of the causal structure of the world -- key to\nrobust, scientifically grounded reasoning. Yet, it remains unclear whether LMs\npossess this capability or exhibit systematic biases leading to erroneous\nconclusions. In this work, we examine LMs' ability to explore and infer causal\nrelationships, using the well-established \"Blicket Test\" paradigm from\ndevelopmental psychology. We find that LMs reliably infer the common, intuitive\ndisjunctive causal relationships but systematically struggle with the unusual,\nyet equally (or sometimes even more) evidenced conjunctive ones. This\n\"disjunctive bias\" persists across model families, sizes, and prompting\nstrategies, and performance further declines as task complexity increases.\nInterestingly, an analogous bias appears in human adults, suggesting that LMs\nmay have inherited deep-seated reasoning heuristics from their training data.\nTo this end, we quantify similarities between LMs and humans, finding that LMs\nexhibit adult-like inference profiles (but not children-like). Finally, we\npropose a test-time sampling method which explicitly samples and eliminates\nhypotheses about causal relationships from the LM. This scalable approach\nsignificantly reduces the disjunctive bias and moves LMs closer to the goal of\nscientific, causally rigorous reasoning.",
      "tldr_zh": "本研究发现，语言模型(LMs)代理在因果推理中表现出类似于人类成人的“disjunctive bias”，即能可靠推断常见的直觉性析取因果关系，但对不常见的合取因果关系存在系统性偏差，使用Blicket Test范式进行测试证实了这一现象。\n这种偏差在不同模型家族、规模和提示策略下均持续存在，且任务复杂度增加时性能进一步下降，LMs的推理模式更接近成人而非儿童，表明其可能从训练数据中继承了人类的推理启发式。\n为减少disjunctive bias，研究提出了一种可扩展的测试时采样方法，通过从LMs中采样并消除因果假设，帮助代理实现更科学、严谨的因果推理。",
      "categories": [
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2505.09614v1",
      "published_date": "2025-05-14 17:59:35 UTC",
      "updated_date": "2025-05-14 17:59:35 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T22:52:05.450090"
    },
    {
      "arxiv_id": "2505.09610v1",
      "title": "Customizing a Large Language Model for VHDL Design of High-Performance Microprocessors",
      "title_zh": "自定义大语言模型用于高性能微处理器 VHDL",
      "authors": [
        "Nicolas Dupuis",
        "Ravi Nair",
        "Shyam Ramji",
        "Sean McClintock",
        "Nishant Chauhan",
        "Priyanka Nagpal",
        "Bart Blaner",
        "Ken Valk",
        "Leon Stok",
        "Ruchir Puri"
      ],
      "abstract": "The use of Large Language Models (LLMs) in hardware design has taken off in\nrecent years, principally through its incorporation in tools that increase chip\ndesigner productivity. There has been considerable discussion about the use of\nLLMs in RTL specifications of chip designs, for which the two most popular\nlanguages are Verilog and VHDL. LLMs and their use in Verilog design has\nreceived significant attention due to the higher popularity of the language,\nbut little attention so far has been given to VHDL despite its continued\npopularity in the industry. There has also been little discussion about the\nunique needs of organizations that engage in high-performance processor design,\nand techniques to deploy AI solutions in these settings. In this paper, we\ndescribe our journey in developing a Large Language Model (LLM) specifically\nfor the purpose of explaining VHDL code, a task that has particular importance\nin an organization with decades of experience and assets in high-performance\nprocessor design. We show how we developed test sets specific to our needs and\nused them for evaluating models as we performed extended pretraining (EPT) of a\nbase LLM. Expert evaluation of the code explanations produced by the EPT model\nincreased to 69% compared to a base model rating of 43%. We further show how we\ndeveloped an LLM-as-a-judge to gauge models similar to expert evaluators. This\nled us to deriving and evaluating a host of new models, including an\ninstruction-tuned version of the EPT model with an expected expert evaluator\nrating of 71%. Our experiments also indicate that with the potential use of\nnewer base models, this rating can be pushed to 85% and beyond. We conclude\nwith a discussion on further improving the quality of hardware design LLMs\nusing exciting new developments in the Generative AI world.",
      "tldr_zh": "该论文探讨了针对高性能微处理器设计的VHDL代码解释，定制Large Language Model (LLM) 的方法，以满足特定组织需求。研究团队通过扩展预训练 (EPT) 和开发定制测试集，对基线LLM进行优化，专家评估显示EPT模型的代码解释质量从43%提高到69%。此外，他们引入了LLM-as-a-judge来模拟专家评估，并开发了指令调整版本，预计评分可达71%，并指出利用新生成AI技术可进一步提升至85%以上。",
      "categories": [
        "cs.AR",
        "cs.AI",
        "cs.CL",
        "cs.SE"
      ],
      "primary_category": "cs.AR",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2505.09610v1",
      "published_date": "2025-05-14 17:58:40 UTC",
      "updated_date": "2025-05-14 17:58:40 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T22:52:16.612043"
    },
    {
      "arxiv_id": "2505.09598v2",
      "title": "How Hungry is AI? Benchmarking Energy, Water, and Carbon Footprint of LLM Inference",
      "title_zh": "AI 有多饥饿？LLM 推理的能源、水和碳足迹基准测试",
      "authors": [
        "Nidhal Jegham",
        "Marwen Abdelatti",
        "Lassad Elmoubarki",
        "Abdeltawab Hendawi"
      ],
      "abstract": "This paper introduces a novel infrastructure-aware benchmarking framework for\nquantifying the environmental footprint of LLM inference across 30\nstate-of-the-art models as deployed in commercial data centers. Our framework\ncombines public API performance data with region-specific environmental\nmultipliers and statistical inference of hardware configurations. We\nadditionally utilize cross-efficiency Data Envelopment Analysis (DEA) to rank\nmodels by performance relative to environmental cost. Our results show that o3\nand DeepSeek-R1 emerge as the most energy-intensive models, consuming over 33\nWh per long prompt, more than 70 times the consumption of GPT-4.1 nano, and\nthat Claude-3.7 Sonnet ranks highest in eco-efficiency. While a single short\nGPT-4o query consumes 0.43 Wh, scaling this to 700 million queries/day results\nin substantial annual environmental impacts. These include electricity use\ncomparable to 35,000 U.S. homes, freshwater evaporation matching the annual\ndrinking needs of 1.2 million people, and carbon emissions requiring a\nChicago-sized forest to offset. These findings illustrate a growing paradox:\nAlthough AI is becoming cheaper and faster, its global adoption drives\ndisproportionate resource consumption. Our study provides a standardized,\nempirically grounded methodology for benchmarking the sustainability of LLM\ndeployments, laying a foundation for future environmental accountability in AI\ndevelopment and sustainability standards.",
      "tldr_zh": "该研究引入了一个基础设施感知基准框架，用于量化30个最先进LLM模型在商业数据中心部署时的能源、水资源和碳足迹。该框架结合公共API性能数据、区域特定环境乘数、硬件配置统计推断，以及交叉效率数据包络分析(DEA)来排名模型的性能与环境成本之比。结果显示，o3和DeepSeek-R1是最耗能模型，每长提示消耗超过33 Wh，是GPT-4.1 nano的70倍以上，而Claude-3.7 Sonnet在生态效率方面排名最高；此外，一个短GPT-4o查询虽仅耗0.43 Wh，但每日7亿查询规模化后，会导致相当于35,000美国家庭的电力使用、1.2百万人的饮用水蒸发量，以及需芝加哥大小森林抵消的碳排放。这些发现突出了AI成本降低与资源消耗矛盾的悖论，并为标准化LLM部署可持续性基准提供经验方法，助力未来AI发展和环境责任标准。",
      "categories": [
        "cs.CY",
        "cs.AI"
      ],
      "primary_category": "cs.CY",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2505.09598v2",
      "published_date": "2025-05-14 17:47:00 UTC",
      "updated_date": "2025-05-15 20:21:12 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T22:52:29.523208"
    },
    {
      "arxiv_id": "2505.09595v1",
      "title": "WorldView-Bench: A Benchmark for Evaluating Global Cultural Perspectives in Large Language Models",
      "title_zh": "翻译失败",
      "authors": [
        "Abdullah Mushtaq",
        "Imran Taj",
        "Rafay Naeem",
        "Ibrahim Ghaznavi",
        "Junaid Qadir"
      ],
      "abstract": "Large Language Models (LLMs) are predominantly trained and aligned in ways\nthat reinforce Western-centric epistemologies and socio-cultural norms, leading\nto cultural homogenization and limiting their ability to reflect global\ncivilizational plurality. Existing benchmarking frameworks fail to adequately\ncapture this bias, as they rely on rigid, closed-form assessments that overlook\nthe complexity of cultural inclusivity. To address this, we introduce\nWorldView-Bench, a benchmark designed to evaluate Global Cultural Inclusivity\n(GCI) in LLMs by analyzing their ability to accommodate diverse worldviews. Our\napproach is grounded in the Multiplex Worldview proposed by Senturk et al.,\nwhich distinguishes between Uniplex models, reinforcing cultural\nhomogenization, and Multiplex models, which integrate diverse perspectives.\nWorldView-Bench measures Cultural Polarization, the exclusion of alternative\nperspectives, through free-form generative evaluation rather than conventional\ncategorical benchmarks. We implement applied multiplexity through two\nintervention strategies: (1) Contextually-Implemented Multiplex LLMs, where\nsystem prompts embed multiplexity principles, and (2) Multi-Agent System\n(MAS)-Implemented Multiplex LLMs, where multiple LLM agents representing\ndistinct cultural perspectives collaboratively generate responses. Our results\ndemonstrate a significant increase in Perspectives Distribution Score (PDS)\nentropy from 13% at baseline to 94% with MAS-Implemented Multiplex LLMs,\nalongside a shift toward positive sentiment (67.7%) and enhanced cultural\nbalance. These findings highlight the potential of multiplex-aware AI\nevaluation in mitigating cultural bias in LLMs, paving the way for more\ninclusive and ethically aligned AI systems.",
      "tldr_zh": "本研究指出，大型语言模型 (LLMs) 因以西方中心主义为主的训练方式而导致文化同质化和偏见，现有基准框架无法有效捕捉这一问题。论文引入 WorldView-Bench 基准，通过基于 Multiplex Worldview 的方法评估 LLMs 的 Global Cultural Inclusivity (GCI)，采用自由形式生成评估和两种干预策略——Contextually-Implemented Multiplex LLMs 及 Multi-Agent System (MAS)-Implemented Multiplex LLMs——来测量 Cultural Polarization。实验结果显示，Perspectives Distribution Score (PDS) 熵从基线的 13% 提升至 94%，并显著提高了正面情绪和文化平衡。这些发现为减轻 LLMs 中的文化偏见提供了新途径，促进更具包容性和道德性的 AI 系统。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.CY",
        "cs.MA"
      ],
      "primary_category": "cs.CL",
      "comment": "Preprint. Submitted to the Journal of Artificial Intelligence\n  Research (JAIR) on April 29, 2025",
      "pdf_url": "http://arxiv.org/pdf/2505.09595v1",
      "published_date": "2025-05-14 17:43:40 UTC",
      "updated_date": "2025-05-14 17:43:40 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T22:52:41.634155"
    },
    {
      "arxiv_id": "2505.09593v1",
      "title": "Online Isolation Forest",
      "title_zh": "在线隔离森林",
      "authors": [
        "Filippo Leveni",
        "Guilherme Weigert Cassales",
        "Bernhard Pfahringer",
        "Albert Bifet",
        "Giacomo Boracchi"
      ],
      "abstract": "The anomaly detection literature is abundant with offline methods, which\nrequire repeated access to data in memory, and impose impractical assumptions\nwhen applied to a streaming context. Existing online anomaly detection methods\nalso generally fail to address these constraints, resorting to periodic\nretraining to adapt to the online context. We propose Online-iForest, a novel\nmethod explicitly designed for streaming conditions that seamlessly tracks the\ndata generating process as it evolves over time. Experimental validation on\nreal-world datasets demonstrated that Online-iForest is on par with online\nalternatives and closely rivals state-of-the-art offline anomaly detection\ntechniques that undergo periodic retraining. Notably, Online-iForest\nconsistently outperforms all competitors in terms of efficiency, making it a\npromising solution in applications where fast identification of anomalies is of\nprimary importance such as cybersecurity, fraud and fault detection.",
      "tldr_zh": "该研究针对异常检测（anomaly detection）领域的不足，提出了一种新型方法 Online-iForest，专门适应流式（streaming）数据环境，能够实时跟踪数据生成过程的变化，而无需定期重新训练。相比现有在线方法，Online-iForest 在真实数据集上的实验中表现出与在线替代方案相当的性能，并接近最先进的离线技术。特别在效率方面，它 consistently outperforms 所有竞争对手，使其成为网络安全（cybersecurity）、欺诈（fraud）和故障检测（fault detection）等应用中的理想选择。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "stat.ML"
      ],
      "primary_category": "cs.LG",
      "comment": "Accepted at International Conference on Machine Learning (ICML 2024)",
      "pdf_url": "http://arxiv.org/pdf/2505.09593v1",
      "published_date": "2025-05-14 17:42:50 UTC",
      "updated_date": "2025-05-14 17:42:50 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T22:52:50.972993"
    },
    {
      "arxiv_id": "2505.09591v1",
      "title": "Variational Visual Question Answering",
      "title_zh": "变分视觉问答",
      "authors": [
        "Tobias Jan Wieczorek",
        "Nathalie Daun",
        "Mohammad Emtiyaz Khan",
        "Marcus Rohrbach"
      ],
      "abstract": "Despite remarkable progress in multimodal models for Visual Question\nAnswering (VQA), there remain major reliability concerns because the models can\noften be overconfident and miscalibrated, especially in out-of-distribution\n(OOD) settings. Plenty has been done to address such issues for unimodal\nmodels, but little work exists for multimodal cases. Here, we address\nunreliability in multimodal models by proposing a Variational VQA approach.\nSpecifically, instead of fine-tuning vision-language models by using AdamW, we\nemploy a recently proposed variational algorithm called IVON, which yields a\nposterior distribution over model parameters. Through extensive experiments, we\nshow that our approach improves calibration and abstentions without sacrificing\nthe accuracy of AdamW. For instance, compared to AdamW fine-tuning, we reduce\nExpected Calibration Error by more than 50% compared to the AdamW baseline and\nraise Coverage by 4% vs. SOTA (for a fixed risk of 1%). In the presence of\ndistribution shifts, the performance gain is even higher, achieving 8% Coverage\n(@ 1% risk) improvement vs. SOTA when 50% of test cases are OOD. Overall, we\npresent variational learning as a viable option to enhance the reliability of\nmultimodal models.",
      "tldr_zh": "本研究针对视觉问答（Visual Question Answering, VQA）模型在分布外（out-of-distribution, OOD）场景下的过自信和校准不当问题，提出了一种Variational VQA方法，使用IVON算法代替AdamW进行微调，从而获得模型参数的后验分布。相比AdamW基准，该方法在不牺牲准确性的前提下，大幅降低了Expected Calibration Error（ECE）超过50%，并将Coverage提高了4%（在1%风险下）。在存在分布偏移的情况下，性能提升更显著，例如当50%测试案例为OOD时，Coverage提升8%，从而增强了多模态模型的可靠性和整体表现。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "19 pages, 16 figures, under review at ICCV 2025",
      "pdf_url": "http://arxiv.org/pdf/2505.09591v1",
      "published_date": "2025-05-14 17:40:22 UTC",
      "updated_date": "2025-05-14 17:40:22 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T22:53:03.932921"
    },
    {
      "arxiv_id": "2505.09576v1",
      "title": "Ethics and Persuasion in Reinforcement Learning from Human Feedback: A Procedural Rhetorical Approach",
      "title_zh": "翻译失败",
      "authors": [
        "Shannon Lodoen",
        "Alexi Orchard"
      ],
      "abstract": "Since 2022, versions of generative AI chatbots such as ChatGPT and Claude\nhave been trained using a specialized technique called Reinforcement Learning\nfrom Human Feedback (RLHF) to fine-tune language model output using feedback\nfrom human annotators. As a result, the integration of RLHF has greatly\nenhanced the outputs of these large language models (LLMs) and made the\ninteractions and responses appear more \"human-like\" than those of previous\nversions using only supervised learning. The increasing convergence of human\nand machine-written text has potentially severe ethical, sociotechnical, and\npedagogical implications relating to transparency, trust, bias, and\ninterpersonal relations. To highlight these implications, this paper presents a\nrhetorical analysis of some of the central procedures and processes currently\nbeing reshaped by RLHF-enhanced generative AI chatbots: upholding language\nconventions, information seeking practices, and expectations for social\nrelationships. Rhetorical investigations of generative AI and LLMs have, to\nthis point, focused largely on the persuasiveness of the content generated.\nUsing Ian Bogost's concept of procedural rhetoric, this paper shifts the site\nof rhetorical investigation from content analysis to the underlying mechanisms\nof persuasion built into RLHF-enhanced LLMs. In doing so, this theoretical\ninvestigation opens a new direction for further inquiry in AI ethics that\nconsiders how procedures rerouted through AI-driven technologies might\nreinforce hegemonic language use, perpetuate biases, decontextualize learning,\nand encroach upon human relationships. It will therefore be of interest to\neducators, researchers, scholars, and the growing number of users of generative\nAI chatbots.",
      "tldr_zh": "这篇论文探讨了Reinforcement Learning from Human Feedback (RLHF) 在训练生成AI聊天机器人（如ChatGPT和Claude）中的伦理影响，强调其使AI输出更“人性化”但可能引发透明度、信任、偏见和人际关系问题。作者采用Ian Bogost的procedural rhetoric概念，从RLHF的底层机制入手，分析其如何重塑语言规范、信息搜索实践和社会期望，而不是仅关注生成内容。研究揭示，RLHF可能强化hegemonic language use、perpetuate biases、decontextualize learning并侵蚀人类关系，为AI伦理研究提供新方向，适用于educators、researchers和AI用户。",
      "categories": [
        "cs.CY",
        "cs.AI"
      ],
      "primary_category": "cs.CY",
      "comment": "10 pages, 1 figure, Accepted version",
      "pdf_url": "http://arxiv.org/pdf/2505.09576v1",
      "published_date": "2025-05-14 17:29:19 UTC",
      "updated_date": "2025-05-14 17:29:19 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T22:53:15.923307"
    },
    {
      "arxiv_id": "2505.09568v1",
      "title": "BLIP3-o: A Family of Fully Open Unified Multimodal Models-Architecture, Training and Dataset",
      "title_zh": "翻译失败",
      "authors": [
        "Jiuhai Chen",
        "Zhiyang Xu",
        "Xichen Pan",
        "Yushi Hu",
        "Can Qin",
        "Tom Goldstein",
        "Lifu Huang",
        "Tianyi Zhou",
        "Saining Xie",
        "Silvio Savarese",
        "Le Xue",
        "Caiming Xiong",
        "Ran Xu"
      ],
      "abstract": "Unifying image understanding and generation has gained growing attention in\nrecent research on multimodal models. Although design choices for image\nunderstanding have been extensively studied, the optimal model architecture and\ntraining recipe for a unified framework with image generation remain\nunderexplored. Motivated by the strong potential of autoregressive and\ndiffusion models for high-quality generation and scalability, we conduct a\ncomprehensive study of their use in unified multimodal settings, with emphasis\non image representations, modeling objectives, and training strategies.\nGrounded in these investigations, we introduce a novel approach that employs a\ndiffusion transformer to generate semantically rich CLIP image features, in\ncontrast to conventional VAE-based representations. This design yields both\nhigher training efficiency and improved generative quality. Furthermore, we\ndemonstrate that a sequential pretraining strategy for unified models-first\ntraining on image understanding and subsequently on image generation-offers\npractical advantages by preserving image understanding capability while\ndeveloping strong image generation ability. Finally, we carefully curate a\nhigh-quality instruction-tuning dataset BLIP3o-60k for image generation by\nprompting GPT-4o with a diverse set of captions covering various scenes,\nobjects, human gestures, and more. Building on our innovative model design,\ntraining recipe, and datasets, we develop BLIP3-o, a suite of state-of-the-art\nunified multimodal models. BLIP3-o achieves superior performance across most of\nthe popular benchmarks spanning both image understanding and generation tasks.\nTo facilitate future research, we fully open-source our models, including code,\nmodel weights, training scripts, and pretraining and instruction tuning\ndatasets.",
      "tldr_zh": "本文提出 BLIP3-o，一系列完全开源的统一多模态模型，旨在整合图像理解和生成任务，通过全面研究自回归模型和扩散模型的架构、训练策略和图像表示。创新点包括使用 diffusion transformer 生成语义丰富的 CLIP 图像特征，以提升训练效率和生成质量，并采用顺序预训练策略，先优化图像理解能力再发展图像生成能力。同时，作者构建了高质量指令调整数据集 BLIP3o-60k，并展示了 BLIP3-o 在图像理解和生成基准测试中取得优越性能，所有模型资源均已开源。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2505.09568v1",
      "published_date": "2025-05-14 17:11:07 UTC",
      "updated_date": "2025-05-14 17:11:07 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T22:53:28.083912"
    },
    {
      "arxiv_id": "2505.09565v1",
      "title": "Meta-learning Slice-to-Volume Reconstruction in Fetal Brain MRI using Implicit Neural Representations",
      "title_zh": "翻译失败",
      "authors": [
        "Maik Dannecker",
        "Thomas Sanchez",
        "Meritxell Bach Cuadra",
        "Özgün Turgut",
        "Anthony N. Price",
        "Lucilio Cordero-Grande",
        "Vanessa Kyriakopoulou",
        "Joseph V. Hajnal",
        "Daniel Rueckert"
      ],
      "abstract": "High-resolution slice-to-volume reconstruction (SVR) from multiple\nmotion-corrupted low-resolution 2D slices constitutes a critical step in\nimage-based diagnostics of moving subjects, such as fetal brain Magnetic\nResonance Imaging (MRI). Existing solutions struggle with image artifacts and\nsevere subject motion or require slice pre-alignment to achieve satisfying\nreconstruction performance. We propose a novel SVR method to enable fast and\naccurate MRI reconstruction even in cases of severe image and motion\ncorruption. Our approach performs motion correction, outlier handling, and\nsuper-resolution reconstruction with all operations being entirely based on\nimplicit neural representations. The model can be initialized with\ntask-specific priors through fully self-supervised meta-learning on either\nsimulated or real-world data. In extensive experiments including over 480\nreconstructions of simulated and clinical MRI brain data from different\ncenters, we prove the utility of our method in cases of severe subject motion\nand image artifacts. Our results demonstrate improvements in reconstruction\nquality, especially in the presence of severe motion, compared to\nstate-of-the-art methods, and up to 50% reduction in reconstruction time.",
      "tldr_zh": "这篇论文提出了一种基于隐式神经表示（Implicit Neural Representations）的元学习（Meta-learning）方法，用于胎儿脑部 MRI 的切片到体积重建（Slice-to-Volume Reconstruction），以解决图像伪影和严重运动带来的挑战。\n该方法通过隐式神经表示实现运动校正、出lier 处理和超分辨率重建，并采用完全自监督的元学习在模拟或真实数据上初始化模型，提升重建的准确性和效率。\n在超过480次模拟和临床MRI脑部数据的实验中，该方法在严重运动情况下显著提高了重建质量，并将重建时间减少高达50%，优于现有技术。",
      "categories": [
        "eess.IV",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "eess.IV",
      "comment": "10 pages, 6 figures",
      "pdf_url": "http://arxiv.org/pdf/2505.09565v1",
      "published_date": "2025-05-14 17:07:37 UTC",
      "updated_date": "2025-05-14 17:07:37 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T22:53:41.101269"
    },
    {
      "arxiv_id": "2505.09561v2",
      "title": "Learning Long-Context Diffusion Policies via Past-Token Prediction",
      "title_zh": "翻译失败",
      "authors": [
        "Marcel Torne",
        "Andy Tang",
        "Yuejiang Liu",
        "Chelsea Finn"
      ],
      "abstract": "Reasoning over long sequences of observations and actions is essential for\nmany robotic tasks. Yet, learning effective long-context policies from\ndemonstrations remains challenging. As context length increases, training\nbecomes increasingly expensive due to rising memory demands, and policy\nperformance often degrades as a result of spurious correlations. Recent methods\ntypically sidestep these issues by truncating context length, discarding\nhistorical information that may be critical for subsequent decisions. In this\npaper, we propose an alternative approach that explicitly regularizes the\nretention of past information. We first revisit the copycat problem in\nimitation learning and identify an opposite challenge in recent diffusion\npolicies: rather than over-relying on prior actions, they often fail to capture\nessential dependencies between past and future actions. To address this, we\nintroduce Past-Token Prediction (PTP), an auxiliary task in which the policy\nlearns to predict past action tokens alongside future ones. This regularization\nsignificantly improves temporal modeling in the policy head, with minimal\nreliance on visual representations. Building on this observation, we further\nintroduce a multistage training strategy: pre-train the visual encoder with\nshort contexts, and fine-tune the policy head using cached long-context\nembeddings. This strategy preserves the benefits of PTP while greatly reducing\nmemory and computational overhead. Finally, we extend PTP into a\nself-verification mechanism at test time, enabling the policy to score and\nselect candidates consistent with past actions during inference. Experiments\nacross four real-world and six simulated tasks demonstrate that our proposed\nmethod improves the performance of long-context diffusion policies by 3x and\naccelerates policy training by more than 10x.",
      "tldr_zh": "该论文解决了机器人任务中学习长上下文策略的挑战，包括训练内存需求高和性能因虚假相关性而下降的问题。作者提出Past-Token Prediction (PTP)作为辅助任务，让扩散策略(diffusion policies)同时预测过去和未来的动作标记，从而改善时序建模并减少对视觉表示的依赖。进一步，通过多阶段训练策略（如先用短上下文预训练视觉编码器，然后微调策略头）和测试时的自验证机制，该方法在四个真实世界和六个模拟任务上将长上下文策略性能提升3倍，并加速训练10倍以上。",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.RO",
      "comment": "Videos are available at https://long-context-dp.github.io",
      "pdf_url": "http://arxiv.org/pdf/2505.09561v2",
      "published_date": "2025-05-14 17:00:47 UTC",
      "updated_date": "2025-05-19 20:37:41 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T22:53:52.409701"
    },
    {
      "arxiv_id": "2505.09558v1",
      "title": "WavReward: Spoken Dialogue Models With Generalist Reward Evaluators",
      "title_zh": "WavReward：通用奖励评估器的语音对话模型",
      "authors": [
        "Shengpeng Ji",
        "Tianle Liang",
        "Yangzhuo Li",
        "Jialong Zuo",
        "Minghui Fang",
        "Jinzheng He",
        "Yifu Chen",
        "Zhengqing Liu",
        "Ziyue Jiang",
        "Xize Cheng",
        "Siqi Zheng",
        "Jin Xu",
        "Junyang Lin",
        "Zhou Zhao"
      ],
      "abstract": "End-to-end spoken dialogue models such as GPT-4o-audio have recently garnered\nsignificant attention in the speech domain. However, the evaluation of spoken\ndialogue models' conversational performance has largely been overlooked. This\nis primarily due to the intelligent chatbots convey a wealth of non-textual\ninformation which cannot be easily measured using text-based language models\nlike ChatGPT. To address this gap, we propose WavReward, a reward feedback\nmodel based on audio language models that can evaluate both the IQ and EQ of\nspoken dialogue systems with speech input. Specifically, 1) based on audio\nlanguage models, WavReward incorporates the deep reasoning process and the\nnonlinear reward mechanism for post-training. By utilizing multi-sample\nfeedback via the reinforcement learning algorithm, we construct a specialized\nevaluator tailored to spoken dialogue models. 2) We introduce ChatReward-30K, a\npreference dataset used to train WavReward. ChatReward-30K includes both\ncomprehension and generation aspects of spoken dialogue models. These scenarios\nspan various tasks, such as text-based chats, nine acoustic attributes of\ninstruction chats, and implicit chats. WavReward outperforms previous\nstate-of-the-art evaluation models across multiple spoken dialogue scenarios,\nachieving a substantial improvement about Qwen2.5-Omni in objective accuracy\nfrom 55.1$\\%$ to 91.5$\\%$. In subjective A/B testing, WavReward also leads by a\nmargin of 83$\\%$. Comprehensive ablation studies confirm the necessity of each\ncomponent of WavReward. All data and code will be publicly at\nhttps://github.com/jishengpeng/WavReward after the paper is accepted.",
      "tldr_zh": "该论文提出WavReward，一种基于音频语言模型的通用奖励评估器，用于评估口语对话系统的IQ和EQ，解决现有模型（如GPT-4o-audio）在非文本信息评估上的不足。WavReward通过深度推理过程、非线性奖励机制和强化学习算法的多样本反馈进行后训练，构建专门的评估器。研究引入ChatReward-30K数据集，该数据集涵盖口语对话的理解和生成场景，包括文本聊天、九个声学属性指令聊天和隐式聊天。实验结果显示，WavReward在多个场景中优于现有模型，将客观准确率从55.1%提升至91.5%，主观A/B测试领先83%，并通过消融研究验证了各组件的必要性。",
      "categories": [
        "eess.AS",
        "cs.AI",
        "cs.LG",
        "cs.MM",
        "cs.SD"
      ],
      "primary_category": "eess.AS",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2505.09558v1",
      "published_date": "2025-05-14 16:54:15 UTC",
      "updated_date": "2025-05-14 16:54:15 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T22:54:03.924215"
    },
    {
      "arxiv_id": "2505.09666v1",
      "title": "System Prompt Optimization with Meta-Learning",
      "title_zh": "翻译失败",
      "authors": [
        "Yumin Choi",
        "Jinheon Baek",
        "Sung Ju Hwang"
      ],
      "abstract": "Large Language Models (LLMs) have shown remarkable capabilities, with\noptimizing their input prompts playing a pivotal role in maximizing their\nperformance. However, while LLM prompts consist of both the task-agnostic\nsystem prompts and task-specific user prompts, existing work on prompt\noptimization has focused on user prompts specific to individual queries or\ntasks, and largely overlooked the system prompt that is, once optimized,\napplicable across different tasks and domains. Motivated by this, we introduce\nthe novel problem of bilevel system prompt optimization, whose objective is to\ndesign system prompts that are robust to diverse user prompts and transferable\nto unseen tasks. To tackle this problem, we then propose a meta-learning\nframework, which meta-learns the system prompt by optimizing it over various\nuser prompts across multiple datasets, while simultaneously updating the user\nprompts in an iterative manner to ensure synergy between them. We conduct\nexperiments on 14 unseen datasets spanning 5 different domains, on which we\nshow that our approach produces system prompts that generalize effectively to\ndiverse user prompts. Also, our findings reveal that the optimized system\nprompt enables rapid adaptation even to unseen tasks, requiring fewer\noptimization steps for test-time user prompts while achieving improved\nperformance.",
      "tldr_zh": "本文提出了一种基于元学习（meta-learning）的双层系统提示优化（bilevel system prompt optimization）方法，针对Large Language Models (LLMs)，旨在设计适用于多种任务和领域的通用系统提示，同时确保其与任务特定用户提示协同优化。该框架通过在多个数据集上迭代优化系统提示和用户提示，实现了对多样化用户提示的鲁棒性。实验在14个跨5个领域的未见数据集上验证了方法的有效性，显示优化后的系统提示能加速新任务适应，并显著提升模型性能。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2505.09666v1",
      "published_date": "2025-05-14 16:46:15 UTC",
      "updated_date": "2025-05-14 16:46:15 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T22:54:15.883245"
    },
    {
      "arxiv_id": "2505.10588v1",
      "title": "Understanding Gen Alpha Digital Language: Evaluation of LLM Safety Systems for Content Moderation",
      "title_zh": "翻译失败",
      "authors": [
        "Manisha Mehta",
        "Fausto Giunchiglia"
      ],
      "abstract": "This research offers a unique evaluation of how AI systems interpret the\ndigital language of Generation Alpha (Gen Alpha, born 2010-2024). As the first\ncohort raised alongside AI, Gen Alpha faces new forms of online risk due to\nimmersive digital engagement and a growing mismatch between their evolving\ncommunication and existing safety tools. Their distinct language, shaped by\ngaming, memes, and AI-driven trends, often conceals harmful interactions from\nboth human moderators and automated systems. We assess four leading AI models\n(GPT-4, Claude, Gemini, and Llama 3) on their ability to detect masked\nharassment and manipulation within Gen Alpha discourse. Using a dataset of 100\nrecent expressions from gaming platforms, social media, and video content, the\nstudy reveals critical comprehension failures with direct implications for\nonline safety. This work contributes: (1) a first-of-its-kind dataset capturing\nGen Alpha expressions; (2) a framework to improve AI moderation systems for\nyouth protection; (3) a multi-perspective evaluation including AI systems,\nhuman moderators, and parents, with direct input from Gen Alpha co-researchers;\nand (4) an analysis of how linguistic divergence increases youth vulnerability.\nFindings highlight the urgent need to redesign safety systems attuned to youth\ncommunication, especially given Gen Alpha reluctance to seek help when adults\nfail to understand their digital world. This study combines the insight of a\nGen Alpha researcher with systematic academic analysis to address critical\ndigital safety challenges.",
      "tldr_zh": "这篇论文评估了大型语言模型（LLM）安全系统在理解Gen Alpha（出生于2010-2024的一代）数字语言方面的表现，焦点是检测隐藏的骚扰和操纵。研究使用一个首创数据集（包含100个来自游戏平台、社会媒体和视频内容的表达）来测试GPT-4、Claude、Gemini和Llama 3等四种AI模型，并结合人类审核员、父母和Gen Alpha共同研究者的多视角评估。关键贡献包括提出一个改进AI内容审核框架、分析语言差异如何增加青少年的在线脆弱性，以及强调重新设计安全系统以适应Gen Alpha的沟通方式。结果显示这些模型存在显著理解失败，突显了保护青少年的紧迫需求，尤其是在他们不愿向不了解数字世界的成年人求助的情况下。",
      "categories": [
        "cs.CY",
        "cs.AI",
        "cs.CL",
        "cs.HC",
        "I.2; I.2.7; K.4.2"
      ],
      "primary_category": "cs.CY",
      "comment": "Accepted to ACM FAccT 2025. To be presented in Athens, June 2025, and\n  published in the conference proceedings. Preprint version; final version will\n  appear in the ACM Digital Library",
      "pdf_url": "http://arxiv.org/pdf/2505.10588v1",
      "published_date": "2025-05-14 16:46:11 UTC",
      "updated_date": "2025-05-14 16:46:11 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T22:54:30.474012"
    },
    {
      "arxiv_id": "2505.09518v1",
      "title": "\\textsc{rfPG}: Robust Finite-Memory Policy Gradients for Hidden-Model POMDPs",
      "title_zh": "翻译失败",
      "authors": [
        "Maris F. L. Galesloot",
        "Roman Andriushchenko",
        "Milan Češka",
        "Sebastian Junges",
        "Nils Jansen"
      ],
      "abstract": "Partially observable Markov decision processes (POMDPs) model specific\nenvironments in sequential decision-making under uncertainty. Critically,\noptimal policies for POMDPs may not be robust against perturbations in the\nenvironment. Hidden-model POMDPs (HM-POMDPs) capture sets of different\nenvironment models, that is, POMDPs with a shared action and observation space.\nThe intuition is that the true model is hidden among a set of potential models,\nand it is unknown which model will be the environment at execution time. A\npolicy is robust for a given HM-POMDP if it achieves sufficient performance for\neach of its POMDPs. We compute such robust policies by combining two orthogonal\ntechniques: (1) a deductive formal verification technique that supports\ntractable robust policy evaluation by computing a worst-case POMDP within the\nHM-POMDP and (2) subgradient ascent to optimize the candidate policy for a\nworst-case POMDP. The empirical evaluation shows that, compared to various\nbaselines, our approach (1) produces policies that are more robust and\ngeneralize better to unseen POMDPs and (2) scales to HM-POMDPs that consist of\nover a hundred thousand environments.",
      "tldr_zh": "本文提出 rfPG 方法，即鲁棒有限记忆策略梯度，用于处理 Hidden-Model POMDPs (HM-POMDPs)，这些模型包含一组共享动作和观察空间的不同环境，以应对不确定性下的顺序决策问题。该方法结合形式验证技术评估最坏情况 POMDP 和子梯度上升优化策略，确保策略在多个 POMDPs 中实现鲁棒性能。实验显示，rfPG 生成的策略比基线更鲁棒，能够更好地泛化到未见 POMDPs，并扩展到超过十万个环境的规模。",
      "categories": [
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "Accepted for publication at IJCAI 2025",
      "pdf_url": "http://arxiv.org/pdf/2505.09518v1",
      "published_date": "2025-05-14 16:15:58 UTC",
      "updated_date": "2025-05-14 16:15:58 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T22:54:40.378201"
    },
    {
      "arxiv_id": "2505.09498v1",
      "title": "Flash-VL 2B: Optimizing Vision-Language Model Performance for Ultra-Low Latency and High Throughput",
      "title_zh": "Flash-VL 2B：针对超低延迟和高吞吐量优化视觉语言模型性能",
      "authors": [
        "Bo Zhang",
        "Shuo Li",
        "Runhe Tian",
        "Yang Yang",
        "Jixin Tang",
        "Jinhao Zhou",
        "Lin Ma"
      ],
      "abstract": "In this paper, we introduce Flash-VL 2B, a novel approach to optimizing\nVision-Language Models (VLMs) for real-time applications, targeting ultra-low\nlatency and high throughput without sacrificing accuracy. Leveraging advanced\narchitectural enhancements and efficient computational strategies, Flash-VL 2B\nis designed to maximize throughput by reducing processing time while\nmaintaining competitive performance across multiple vision-language benchmarks.\nOur approach includes tailored architectural choices, token compression\nmechanisms, data curation, training schemes, and a novel image processing\ntechnique called implicit semantic stitching that effectively balances\ncomputational load and model performance. Through extensive evaluations on 11\nstandard VLM benchmarks, we demonstrate that Flash-VL 2B achieves\nstate-of-the-art results in both speed and accuracy, making it a promising\nsolution for deployment in resource-constrained environments and large-scale\nreal-time applications.",
      "tldr_zh": "本研究引入了 Flash-VL 2B，一种针对视觉语言模型 (VLMs) 的优化方法，旨在实现超低延迟和高质量吞吐量，同时保持准确性。关键技术包括先进的架构增强、token 压缩机制、数据整理、专用训练方案，以及一种新颖的图像处理技术 implicit semantic stitching，以有效平衡计算负载和性能。在 11 个标准 VLM 基准上的广泛评估显示，Flash-VL 2B 达到了最先进的速度和准确性结果，适合资源受限环境和大规模实时应用。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "18 pages, 7 figures",
      "pdf_url": "http://arxiv.org/pdf/2505.09498v1",
      "published_date": "2025-05-14 15:45:17 UTC",
      "updated_date": "2025-05-14 15:45:17 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T22:54:51.666043"
    },
    {
      "arxiv_id": "2505.09486v1",
      "title": "Preserving Plasticity in Continual Learning with Adaptive Linearity Injection",
      "title_zh": "翻译失败",
      "authors": [
        "Seyed Roozbeh Razavi Rohani",
        "Khashayar Khajavi",
        "Wesley Chung",
        "Mo Chen",
        "Sharan Vaswani"
      ],
      "abstract": "Loss of plasticity in deep neural networks is the gradual reduction in a\nmodel's capacity to incrementally learn and has been identified as a key\nobstacle to learning in non-stationary problem settings. Recent work has shown\nthat deep linear networks tend to be resilient towards loss of plasticity.\nMotivated by this observation, we propose Adaptive Linearization (AdaLin), a\ngeneral approach that dynamically adapts each neuron's activation function to\nmitigate plasticity loss. Unlike prior methods that rely on regularization or\nperiodic resets, AdaLin equips every neuron with a learnable parameter and a\ngating mechanism that injects linearity into the activation function based on\nits gradient flow. This adaptive modulation ensures sufficient gradient signal\nand sustains continual learning without introducing additional hyperparameters\nor requiring explicit task boundaries. When used with conventional activation\nfunctions like ReLU, Tanh, and GeLU, we demonstrate that AdaLin can\nsignificantly improve performance on standard benchmarks, including Random\nLabel and Permuted MNIST, Random Label and Shuffled CIFAR-10, and Class-Split\nCIFAR-100. Furthermore, its efficacy is shown in more complex scenarios, such\nas class-incremental learning on CIFAR-100 with a ResNet-18 backbone, and in\nmitigating plasticity loss in off-policy reinforcement learning agents. We\nperform a systematic set of ablations that show that neuron-level adaptation is\ncrucial for good performance and analyze a number of metrics in the network\nthat might be correlated to loss of plasticity.",
      "tldr_zh": "该研究针对深度神经网络在持续学习(Continual Learning)中面临的损失可塑性问题，提出了一种名为Adaptive Linearization (AdaLin)的通用方法。该方法通过为每个神经元配备可学习的参数和门控机制，根据梯度流动态注入线性性到激活函数（如ReLU、Tanh或GeLU），从而维持模型的梯度信号并支持增量学习，而无需额外超参数或明确任务边界。在标准基准测试中，如Random Label MNIST和Class-Split CIFAR-100上，AdaLin显著提升了性能，并在类增量学习和强化学习场景中证明了其有效性。通过消融实验，研究者证实了神经元级适应的关键作用，并分析了与可塑性损失相关的网络指标。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "Accepted in 4th Conference on Lifelong Learning Agents (CoLLAs), 2025",
      "pdf_url": "http://arxiv.org/pdf/2505.09486v1",
      "published_date": "2025-05-14 15:36:51 UTC",
      "updated_date": "2025-05-14 15:36:51 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T22:55:05.029268"
    },
    {
      "arxiv_id": "2505.09477v1",
      "title": "Deploying Foundation Model-Enabled Air and Ground Robots in the Field: Challenges and Opportunities",
      "title_zh": "在实地部署 Foundation Model 启用的空中和地面机器人：挑战和机会",
      "authors": [
        "Zachary Ravichandran",
        "Fernando Cladera",
        "Jason Hughes",
        "Varun Murali",
        "M. Ani Hsieh",
        "George J. Pappas",
        "Camillo J. Taylor",
        "Vijay Kumar"
      ],
      "abstract": "The integration of foundation models (FMs) into robotics has enabled robots\nto understand natural language and reason about the semantics in their\nenvironments. However, existing FM-enabled robots primary operate in\nclosed-world settings, where the robot is given a full prior map or has a full\nview of its workspace. This paper addresses the deployment of FM-enabled robots\nin the field, where missions often require a robot to operate in large-scale\nand unstructured environments. To effectively accomplish these missions, robots\nmust actively explore their environments, navigate obstacle-cluttered terrain,\nhandle unexpected sensor inputs, and operate with compute constraints. We\ndiscuss recent deployments of SPINE, our LLM-enabled autonomy framework, in\nfield robotic settings. To the best of our knowledge, we present the first\ndemonstration of large-scale LLM-enabled robot planning in unstructured\nenvironments with several kilometers of missions. SPINE is agnostic to a\nparticular LLM, which allows us to distill small language models capable of\nrunning onboard size, weight and power (SWaP) limited platforms. Via\npreliminary model distillation work, we then present the first language-driven\nUAV planner using on-device language models. We conclude our paper by proposing\nseveral promising directions for future research.",
      "tldr_zh": "这篇论文探讨了在野外环境中部署基础模型(FMs)启用空地机器人的挑战和机会，包括机器人需主动探索非结构化环境、导航障碍物、处理意外传感器输入以及应对计算资源限制。作者介绍了SPINE框架，这是一个LLM-enabled自治框架，已在野外机器人场景中成功部署，并首次展示了大规模LLM-enabled机器人规划，完成数公里的任务。SPINE框架不依赖特定LLM，通过模型蒸馏技术，使小型语言模型能够在SWaP受限平台上运行，并实现了首个设备端语言驱动UAV规划器。论文最后提出未来研究方向，如进一步优化FM在机器人应用中的鲁棒性。",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "comment": "Accepted to the IEEE ICRA Workshop on Field Robotics 2025",
      "pdf_url": "http://arxiv.org/pdf/2505.09477v1",
      "published_date": "2025-05-14 15:28:43 UTC",
      "updated_date": "2025-05-14 15:28:43 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T22:55:16.713815"
    },
    {
      "arxiv_id": "2505.09466v1",
      "title": "A 2D Semantic-Aware Position Encoding for Vision Transformers",
      "title_zh": "翻译失败",
      "authors": [
        "Xi Chen",
        "Shiyang Zhou",
        "Muqi Huang",
        "Jiaxu Feng",
        "Yun Xiong",
        "Kun Zhou",
        "Biao Yang",
        "Yuhui Zhang",
        "Huishuai Bao",
        "Sijia Peng",
        "Chuan Li",
        "Feng Shi"
      ],
      "abstract": "Vision transformers have demonstrated significant advantages in computer\nvision tasks due to their ability to capture long-range dependencies and\ncontextual relationships through self-attention. However, existing position\nencoding techniques, which are largely borrowed from natural language\nprocessing, fail to effectively capture semantic-aware positional relationships\nbetween image patches. Traditional approaches like absolute position encoding\nand relative position encoding primarily focus on 1D linear position\nrelationship, often neglecting the semantic similarity between distant yet\ncontextually related patches. These limitations hinder model generalization,\ntranslation equivariance, and the ability to effectively handle repetitive or\nstructured patterns in images. In this paper, we propose 2-Dimensional\nSemantic-Aware Position Encoding ($\\text{SaPE}^2$), a novel position encoding\nmethod with semantic awareness that dynamically adapts position representations\nby leveraging local content instead of fixed linear position relationship or\nspatial coordinates. Our method enhances the model's ability to generalize\nacross varying image resolutions and scales, improves translation equivariance,\nand better aggregates features for visually similar but spatially distant\npatches. By integrating $\\text{SaPE}^2$ into vision transformers, we bridge the\ngap between position encoding and perceptual similarity, thereby improving\nperformance on computer vision tasks.",
      "tldr_zh": "本研究指出，现有的位置编码(position encoding)方法在Vision Transformers中无法有效捕捉图像补丁间的语义感知关系，导致模型泛化能力不足和平移等变性(translation equivariance)问题。论文提出了一种新型2D语义感知位置编码(SaPE²)，通过利用局部内容动态调整位置表示，而不是依赖固定线性关系或空间坐标。SaPE²增强了模型在不同图像分辨率和规模下的泛化能力，并更好地聚合视觉上相似但空间上遥远的补丁，最终提高了Vision Transformers在计算机视觉任务中的整体性能。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "14 pages, 4 figures, 3 tables",
      "pdf_url": "http://arxiv.org/pdf/2505.09466v1",
      "published_date": "2025-05-14 15:17:34 UTC",
      "updated_date": "2025-05-14 15:17:34 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T22:55:27.710097"
    },
    {
      "arxiv_id": "2505.09456v1",
      "title": "Quantum state-agnostic work extraction (almost) without dissipation",
      "title_zh": "翻译失败",
      "authors": [
        "Josep Lumbreras",
        "Ruo Cheng Huang",
        "Yanglin Hu",
        "Mile Gu",
        "Marco Tomamichel"
      ],
      "abstract": "We investigate work extraction protocols designed to transfer the maximum\npossible energy to a battery using sequential access to $N$ copies of an\nunknown pure qubit state. The core challenge is designing interactions to\noptimally balance two competing goals: charging of the battery optimally using\nthe qubit in hand, and acquiring more information by qubit to improve energy\nharvesting in subsequent rounds. Here, we leverage exploration-exploitation\ntrade-off in reinforcement learning to develop adaptive strategies achieving\nenergy dissipation that scales only poly-logarithmically in $N$. This\nrepresents an exponential improvement over current protocols based on full\nstate tomography.",
      "tldr_zh": "本研究探讨了Quantum state-agnostic work extraction协议，用于从N个未知纯量子比特状态副本中提取最大能量，几乎不产生耗散。作者采用强化学习的exploration-exploitation trade-off来设计自适应策略，平衡当前量子比特的充电优化与后续轮次的信息获取。结果显示，该方法使能量耗散仅以poly-logarithmically in N规模增长，比基于full state tomography的现有协议实现了指数级改进。",
      "categories": [
        "quant-ph",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "quant-ph",
      "comment": "5 pages+14 pages, 2 figures",
      "pdf_url": "http://arxiv.org/pdf/2505.09456v1",
      "published_date": "2025-05-14 15:07:58 UTC",
      "updated_date": "2025-05-14 15:07:58 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T22:56:34.287887"
    },
    {
      "arxiv_id": "2505.09438v1",
      "title": "Evaluating GPT- and Reasoning-based Large Language Models on Physics Olympiad Problems: Surpassing Human Performance and Implications for Educational Assessment",
      "title_zh": "翻译失败",
      "authors": [
        "Paul Tschisgale",
        "Holger Maus",
        "Fabian Kieser",
        "Ben Kroehs",
        "Stefan Petersen",
        "Peter Wulff"
      ],
      "abstract": "Large language models (LLMs) are now widely accessible, reaching learners at\nall educational levels. This development has raised concerns that their use may\ncircumvent essential learning processes and compromise the integrity of\nestablished assessment formats. In physics education, where problem solving\nplays a central role in instruction and assessment, it is therefore essential\nto understand the physics-specific problem-solving capabilities of LLMs. Such\nunderstanding is key to informing responsible and pedagogically sound\napproaches to integrating LLMs into instruction and assessment. This study\ntherefore compares the problem-solving performance of a general-purpose LLM\n(GPT-4o, using varying prompting techniques) and a reasoning-optimized model\n(o1-preview) with that of participants of the German Physics Olympiad, based on\na set of well-defined Olympiad problems. In addition to evaluating the\ncorrectness of the generated solutions, the study analyzes characteristic\nstrengths and limitations of LLM-generated solutions. The findings of this\nstudy indicate that both tested LLMs (GPT-4o and o1-preview) demonstrate\nadvanced problem-solving capabilities on Olympiad-type physics problems, on\naverage outperforming the human participants. Prompting techniques had little\neffect on GPT-4o's performance, while o1-preview almost consistently\noutperformed both GPT-4o and the human benchmark. Based on these findings, the\nstudy discusses implications for the design of summative and formative\nassessment in physics education, including how to uphold assessment integrity\nand support students in critically engaging with LLMs.",
      "tldr_zh": "这篇论文评估了大型语言模型（LLMs）如 GPT-4o 和 o1-preview 在物理奥林匹克问题上的问题解决能力，并与德国物理奥林匹克参赛者进行比较。研究采用不同提示技术测试这些模型，结果显示 LLMs 平均超过了人类的表现，其中 o1-preview 几乎始终优于 GPT-4o 和人类基准。提示技术对 GPT-4o 的影响较小，但整体发现突显了 LLMs 在教育评估中的潜力。该研究讨论了如何维护评估完整性并指导学生批判性地使用 LLMs，以支持物理教育的改革。",
      "categories": [
        "physics.ed-ph",
        "cs.AI"
      ],
      "primary_category": "physics.ed-ph",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2505.09438v1",
      "published_date": "2025-05-14 14:46:32 UTC",
      "updated_date": "2025-05-14 14:46:32 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T22:55:51.857557"
    },
    {
      "arxiv_id": "2505.09436v2",
      "title": "CXMArena: Unified Dataset to benchmark performance in realistic CXM Scenarios",
      "title_zh": "翻译失败",
      "authors": [
        "Raghav Garg",
        "Kapil Sharma",
        "Karan Gupta"
      ],
      "abstract": "Large Language Models (LLMs) hold immense potential for revolutionizing\nCustomer Experience Management (CXM), particularly in contact center\noperations. However, evaluating their practical utility in complex operational\nenvironments is hindered by data scarcity (due to privacy concerns) and the\nlimitations of current benchmarks. Existing benchmarks often lack realism,\nfailing to incorporate deep knowledge base (KB) integration, real-world noise,\nor critical operational tasks beyond conversational fluency. To bridge this\ngap, we introduce CXMArena, a novel, large-scale synthetic benchmark dataset\nspecifically designed for evaluating AI in operational CXM contexts. Given the\ndiversity in possible contact center features, we have developed a scalable\nLLM-powered pipeline that simulates the brand's CXM entities that form the\nfoundation of our datasets-such as knowledge articles including product\nspecifications, issue taxonomies, and contact center conversations. The\nentities closely represent real-world distribution because of controlled noise\ninjection (informed by domain experts) and rigorous automated validation.\nBuilding on this, we release CXMArena, which provides dedicated benchmarks\ntargeting five important operational tasks: Knowledge Base Refinement, Intent\nPrediction, Agent Quality Adherence, Article Search, and Multi-turn RAG with\nIntegrated Tools. Our baseline experiments underscore the benchmark's\ndifficulty: even state of the art embedding and generation models achieve only\n68% accuracy on article search, while standard embedding methods yield a low F1\nscore of 0.3 for knowledge base refinement, highlighting significant challenges\nfor current models necessitating complex pipelines and solutions over\nconventional techniques.",
      "tldr_zh": "该研究针对大型语言模型(LLMs)在客户体验管理(CXM)中的实际应用问题，引入了CXMArena——一个统一的大型合成基准数据集，用于评估AI在真实操作环境中的性能，以弥补现有基准缺乏真实性和深度知识库(KB)整合的不足。CXMArena通过一个可扩展的LLM驱动管道模拟品牌CXM实体（如知识文章、产品规格和联系中心对话），并通过受领域专家指导的控制噪声注入和自动化验证，确保数据集接近真实世界分布。该数据集针对五个关键任务提供基准测试，包括知识库精炼(Intent Prediction)、意图预测(Agent Quality Adherence)、代理质量遵守(Article Search)、文章搜索和多轮RAG(Retrieval-Augmented Generation)与集成工具；实验结果显示，即使是最先进的模型在文章搜索上仅达到68%准确率，在知识库精炼上F1分数仅0.3，突显了当前技术的挑战和改进需求。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL",
        "cs.IR"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2505.09436v2",
      "published_date": "2025-05-14 14:44:30 UTC",
      "updated_date": "2025-05-19 06:27:01 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T22:56:05.093549"
    },
    {
      "arxiv_id": "2505.09435v1",
      "title": "Endo-CLIP: Progressive Self-Supervised Pre-training on Raw Colonoscopy Records",
      "title_zh": "翻译失败",
      "authors": [
        "Yili He",
        "Yan Zhu",
        "Peiyao Fu",
        "Ruijie Yang",
        "Tianyi Chen",
        "Zhihua Wang",
        "Quanlin Li",
        "Pinghong Zhou",
        "Xian Yang",
        "Shuo Wang"
      ],
      "abstract": "Pre-training on image-text colonoscopy records offers substantial potential\nfor improving endoscopic image analysis, but faces challenges including\nnon-informative background images, complex medical terminology, and ambiguous\nmulti-lesion descriptions. We introduce Endo-CLIP, a novel self-supervised\nframework that enhances Contrastive Language-Image Pre-training (CLIP) for this\ndomain. Endo-CLIP's three-stage framework--cleansing, attunement, and\nunification--addresses these challenges by (1) removing background frames, (2)\nleveraging large language models to extract clinical attributes for\nfine-grained contrastive learning, and (3) employing patient-level\ncross-attention to resolve multi-polyp ambiguities. Extensive experiments\ndemonstrate that Endo-CLIP significantly outperforms state-of-the-art\npre-training methods in zero-shot and few-shot polyp detection and\nclassification, paving the way for more accurate and clinically relevant\nendoscopic analysis.",
      "tldr_zh": "该研究提出 Endo-CLIP，一种渐进式自监督预训练框架，针对结肠镜图像-文本记录的挑战（如非信息性背景图像、复杂医疗术语和模糊多病变描述）来增强 Contrastive Language-Image Pre-training (CLIP)。框架包括三个阶段：cleansing 移除背景帧、attunement 利用大型语言模型提取临床属性进行细粒度对比学习，以及 unification 通过患者级别交叉注意力解决多息肉模糊性。实验结果显示，Endo-CLIP 在零样本和少样本息肉检测及分类任务中显著优于现有方法，推动了更准确和临床相关的内镜图像分析。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "Early accepted to MICCAI 2025",
      "pdf_url": "http://arxiv.org/pdf/2505.09435v1",
      "published_date": "2025-05-14 14:43:31 UTC",
      "updated_date": "2025-05-14 14:43:31 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T22:56:15.190702"
    },
    {
      "arxiv_id": "2505.09412v1",
      "title": "Counterfactual Strategies for Markov Decision Processes",
      "title_zh": "针对马尔可夫决策过程的反事实策略",
      "authors": [
        "Paul Kobialka",
        "Lina Gerlach",
        "Francesco Leofante",
        "Erika Ábrahám",
        "Silvia Lizeth Tapia Tarifa",
        "Einar Broch Johnsen"
      ],
      "abstract": "Counterfactuals are widely used in AI to explain how minimal changes to a\nmodel's input can lead to a different output. However, established methods for\ncomputing counterfactuals typically focus on one-step decision-making, and are\nnot directly applicable to sequential decision-making tasks. This paper fills\nthis gap by introducing counterfactual strategies for Markov Decision Processes\n(MDPs). During MDP execution, a strategy decides which of the enabled actions\n(with known probabilistic effects) to execute next. Given an initial strategy\nthat reaches an undesired outcome with a probability above some limit, we\nidentify minimal changes to the initial strategy to reduce that probability\nbelow the limit. We encode such counterfactual strategies as solutions to\nnon-linear optimization problems, and further extend our encoding to synthesize\ndiverse counterfactual strategies. We evaluate our approach on four real-world\ndatasets and demonstrate its practical viability in sophisticated sequential\ndecision-making tasks.",
      "tldr_zh": "该论文针对Markov Decision Processes (MDPs)引入了Counterfactual Strategies，以解决现有反事实方法仅适用于单步决策而非顺序决策的问题。方法通过编码为非线性优化问题，识别出对初始策略的最小修改，从而将不良结果的概率降低到指定阈值以下，并扩展支持多样化策略合成。在四个真实世界数据集上的实验验证了该方法的实用性，展示了其在复杂顺序决策任务中的有效性。",
      "categories": [
        "cs.AI",
        "I.2.m"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2505.09412v1",
      "published_date": "2025-05-14 14:07:27 UTC",
      "updated_date": "2025-05-14 14:07:27 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T22:56:27.512412"
    },
    {
      "arxiv_id": "2505.09407v1",
      "title": "Multilingual Machine Translation with Quantum Encoder Decoder Attention-based Convolutional Variational Circuits",
      "title_zh": "翻译失败",
      "authors": [
        "Subrit Dikshit",
        "Ritu Tiwari",
        "Priyank Jain"
      ],
      "abstract": "Cloud-based multilingual translation services like Google Translate and\nMicrosoft Translator achieve state-of-the-art translation capabilities. These\nservices inherently use large multilingual language models such as GRU, LSTM,\nBERT, GPT, T5, or similar encoder-decoder architectures with attention\nmechanisms as the backbone. Also, new age natural language systems, for\ninstance ChatGPT and DeepSeek, have established huge potential in multiple\ntasks in natural language processing. At the same time, they also possess\noutstanding multilingual translation capabilities. However, these models use\nthe classical computing realm as a backend. QEDACVC (Quantum Encoder Decoder\nAttention-based Convolutional Variational Circuits) is an alternate solution\nthat explores the quantum computing realm instead of the classical computing\nrealm to study and demonstrate multilingual machine translation. QEDACVC\nintroduces the quantum encoder-decoder architecture that simulates and runs on\nquantum computing hardware via quantum convolution, quantum pooling, quantum\nvariational circuit, and quantum attention as software alterations. QEDACVC\nachieves an Accuracy of 82% when trained on the OPUS dataset for English,\nFrench, German, and Hindi corpora for multilingual translations.",
      "tldr_zh": "该论文提出了一种基于量子计算的模型 QEDACVC，用于多语言机器翻译，以替代传统的经典计算架构如 GRU、LSTM 或 BERT。QEDACVC 采用量子编码器-解码器架构，结合 quantum convolution、quantum pooling、quantum variational circuit 和 quantum attention 等组件，在量子硬件上模拟运行。实验结果显示，该模型在 OPUS 数据集上训练后，在英语、法语、德语和印地语的多语言翻译任务中达到了 82% 的准确率。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.ET"
      ],
      "primary_category": "cs.CL",
      "comment": "12 pages, 12 figures",
      "pdf_url": "http://arxiv.org/pdf/2505.09407v1",
      "published_date": "2025-05-14 14:04:44 UTC",
      "updated_date": "2025-05-14 14:04:44 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T22:56:46.664922"
    },
    {
      "arxiv_id": "2505.09396v1",
      "title": "The Influence of Human-inspired Agentic Sophistication in LLM-driven Strategic Reasoners",
      "title_zh": "翻译失败",
      "authors": [
        "Vince Trencsenyi",
        "Agnieszka Mensfelt",
        "Kostas Stathis"
      ],
      "abstract": "The rapid rise of large language models (LLMs) has shifted artificial\nintelligence (AI) research toward agentic systems, motivating the use of weaker\nand more flexible notions of agency. However, this shift raises key questions\nabout the extent to which LLM-based agents replicate human strategic reasoning,\nparticularly in game-theoretic settings. In this context, we examine the role\nof agentic sophistication in shaping artificial reasoners' performance by\nevaluating three agent designs: a simple game-theoretic model, an unstructured\nLLM-as-agent model, and an LLM integrated into a traditional agentic framework.\nUsing guessing games as a testbed, we benchmarked these agents against human\nparticipants across general reasoning patterns and individual role-based\nobjectives. Furthermore, we introduced obfuscated game scenarios to assess\nagents' ability to generalise beyond training distributions. Our analysis,\ncovering over 2000 reasoning samples across 25 agent configurations, shows that\nhuman-inspired cognitive structures can enhance LLM agents' alignment with\nhuman strategic behaviour. Still, the relationship between agentic design\ncomplexity and human-likeness is non-linear, highlighting a critical dependence\non underlying LLM capabilities and suggesting limits to simple architectural\naugmentation.",
      "tldr_zh": "本文研究了人类启发的代理复杂性（agentic sophistication）对LLM驱动战略推理器的影响，特别是在博弈论环境中的表现。作者比较了三种代理设计：简单博弈论模型、非结构化LLM-as-agent模型，以及LLM整合传统代理框架，并使用猜数字游戏作为基准测试代理的推理模式和泛化能力。实验分析了超过2000个样本和25个配置，结果表明人类启发的认知结构能提升LLM代理与人类战略行为的契合度，但这种关系是非线性的，取决于底层LLM能力及其架构增强的限制。",
      "categories": [
        "cs.AI",
        "cs.MA"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2505.09396v1",
      "published_date": "2025-05-14 13:51:24 UTC",
      "updated_date": "2025-05-14 13:51:24 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T22:56:58.448176"
    },
    {
      "arxiv_id": "2505.09395v1",
      "title": "Quantum-Enhanced Parameter-Efficient Learning for Typhoon Trajectory Forecasting",
      "title_zh": "翻译失败",
      "authors": [
        "Chen-Yu Liu",
        "Kuan-Cheng Chen",
        "Yi-Chien Chen",
        "Samuel Yen-Chi Chen",
        "Wei-Hao Huang",
        "Wei-Jia Huang",
        "Yen-Jui Chang"
      ],
      "abstract": "Typhoon trajectory forecasting is essential for disaster preparedness but\nremains computationally demanding due to the complexity of atmospheric dynamics\nand the resource requirements of deep learning models. Quantum-Train (QT), a\nhybrid quantum-classical framework that leverages quantum neural networks\n(QNNs) to generate trainable parameters exclusively during training,\neliminating the need for quantum hardware at inference time. Building on QT's\nsuccess across multiple domains, including image classification, reinforcement\nlearning, flood prediction, and large language model (LLM) fine-tuning, we\nintroduce Quantum Parameter Adaptation (QPA) for efficient typhoon forecasting\nmodel learning. Integrated with an Attention-based Multi-ConvGRU model, QPA\nenables parameter-efficient training while maintaining predictive accuracy.\nThis work represents the first application of quantum machine learning (QML) to\nlarge-scale typhoon trajectory prediction, offering a scalable and\nenergy-efficient approach to climate modeling. Our results demonstrate that QPA\nsignificantly reduces the number of trainable parameters while preserving\nperformance, making high-performance forecasting more accessible and\nsustainable through hybrid quantum-classical learning.",
      "tldr_zh": "本论文提出 Quantum Parameter Adaptation (QPA)，一种基于 Quantum-Train (QT) 的混合量子-经典框架，用于台风轨迹预测，以解决传统深度学习模型的计算密集问题。QPA 利用 Quantum Neural Networks (QNNs) 只在训练阶段生成参数，并与 Attention-based Multi-ConvGRU 模型集成，实现参数高效训练，同时保持预测准确性。该方法标志着 Quantum Machine Learning (QML) 在大规模台风轨迹预测的首次应用，结果显示 QPA 显著减少可训练参数，提升了预测的可扩展性和可持续性。",
      "categories": [
        "quant-ph",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "quant-ph",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2505.09395v1",
      "published_date": "2025-05-14 13:50:44 UTC",
      "updated_date": "2025-05-14 13:50:44 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T22:57:11.593031"
    },
    {
      "arxiv_id": "2505.09393v1",
      "title": "UMotion: Uncertainty-driven Human Motion Estimation from Inertial and Ultra-wideband Units",
      "title_zh": "翻译失败",
      "authors": [
        "Huakun Liu",
        "Hiroki Ota",
        "Xin Wei",
        "Yutaro Hirao",
        "Monica Perusquia-Hernandez",
        "Hideaki Uchiyama",
        "Kiyoshi Kiyokawa"
      ],
      "abstract": "Sparse wearable inertial measurement units (IMUs) have gained popularity for\nestimating 3D human motion. However, challenges such as pose ambiguity, data\ndrift, and limited adaptability to diverse bodies persist. To address these\nissues, we propose UMotion, an uncertainty-driven, online fusing-all state\nestimation framework for 3D human shape and pose estimation, supported by six\nintegrated, body-worn ultra-wideband (UWB) distance sensors with IMUs. UWB\nsensors measure inter-node distances to infer spatial relationships, aiding in\nresolving pose ambiguities and body shape variations when combined with\nanthropometric data. Unfortunately, IMUs are prone to drift, and UWB sensors\nare affected by body occlusions. Consequently, we develop a tightly coupled\nUnscented Kalman Filter (UKF) framework that fuses uncertainties from sensor\ndata and estimated human motion based on individual body shape. The UKF\niteratively refines IMU and UWB measurements by aligning them with uncertain\nhuman motion constraints in real-time, producing optimal estimates for each.\nExperiments on both synthetic and real-world datasets demonstrate the\neffectiveness of UMotion in stabilizing sensor data and the improvement over\nstate of the art in pose accuracy.",
      "tldr_zh": "该论文提出UMotion框架，这是一个基于不确定性的在线融合状态估计系统，用于从惯性测量单位(IMUs)和超宽带(UWB)传感器估计3D人体形状和姿势，以解决传统方法中的姿势模糊、数据漂移和适应性问题。UMotion利用六个体戴UWB传感器测量节点间距离，并结合人体测量数据，通过紧密耦合的无迹卡尔曼滤波器(UKF)实时融合传感器不确定性和人体运动约束，实现对IMU和UWB数据的迭代优化。实验在合成和真实数据集上证明，该框架显著稳定了传感器数据，并在姿势准确性上超过了现有技术。",
      "categories": [
        "cs.GR",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.GR",
      "comment": "Accepted by CVPR 2025",
      "pdf_url": "http://arxiv.org/pdf/2505.09393v1",
      "published_date": "2025-05-14 13:48:36 UTC",
      "updated_date": "2025-05-14 13:48:36 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T22:57:21.913170"
    },
    {
      "arxiv_id": "2505.09661v1",
      "title": "Introducing voice timbre attribute detection",
      "title_zh": "翻译失败",
      "authors": [
        "Jinghao He",
        "Zhengyan Sheng",
        "Liping Chen",
        "Kong Aik Lee",
        "Zhen-Hua Ling"
      ],
      "abstract": "This paper focuses on explaining the timbre conveyed by speech signals and\nintroduces a task termed voice timbre attribute detection (vTAD). In this task,\nvoice timbre is explained with a set of sensory attributes describing its human\nperception. A pair of speech utterances is processed, and their intensity is\ncompared in a designated timbre descriptor. Moreover, a framework is proposed,\nwhich is built upon the speaker embeddings extracted from the speech\nutterances. The investigation is conducted on the VCTK-RVA dataset.\nExperimental examinations on the ECAPA-TDNN and FACodec speaker encoders\ndemonstrated that: 1) the ECAPA-TDNN speaker encoder was more capable in the\nseen scenario, where the testing speakers were included in the training set; 2)\nthe FACodec speaker encoder was superior in the unseen scenario, where the\ntesting speakers were not part of the training, indicating enhanced\ngeneralization capability. The VCTK-RVA dataset and open-source code are\navailable on the website https://github.com/vTAD2025-Challenge/vTAD.",
      "tldr_zh": "本论文引入了 voice timbre attribute detection (vTAD) 任务，用于通过一组感官属性描述语音信号中传达的音色感知。提出的框架基于从语音中提取的 speaker embeddings，对一对语音进行处理并比较其在指定音色描述符上的强度。实验在 VCTK-RVA 数据集上使用 ECAPA-TDNN 和 FACodec speaker encoders 进行，结果显示 ECAPA-TDNN 在 seen 场景（测试说话者包含在训练集）中表现更优，而 FACodec 在 unseen 场景（测试说话者不在训练集）中更具泛化能力。数据集和开源代码可从 https://github.com/vTAD2025-Challenge/vTAD 获取。",
      "categories": [
        "cs.SD",
        "cs.AI",
        "eess.AS"
      ],
      "primary_category": "cs.SD",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2505.09661v1",
      "published_date": "2025-05-14 13:46:46 UTC",
      "updated_date": "2025-05-14 13:46:46 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T22:57:34.197179"
    },
    {
      "arxiv_id": "2505.09385v1",
      "title": "FedSaaS: Class-Consistency Federated Semantic Segmentation via Global Prototype Supervision and Local Adversarial Harmonization",
      "title_zh": "FedSaaS：通过全局原型监督和局部对抗协调实现类别一致性的联邦语义分割",
      "authors": [
        "Xiaoyang Yu",
        "Xiaoming Wu",
        "Xin Wang",
        "Dongrun Li",
        "Ming Yang",
        "Peng Cheng"
      ],
      "abstract": "Federated semantic segmentation enables pixel-level classification in images\nthrough collaborative learning while maintaining data privacy. However,\nexisting research commonly overlooks the fine-grained class relationships\nwithin the semantic space when addressing heterogeneous problems, particularly\ndomain shift. This oversight results in ambiguities between class\nrepresentation. To overcome this challenge, we propose a novel federated\nsegmentation framework that strikes class consistency, termed FedSaaS.\nSpecifically, we introduce class exemplars as a criterion for both local- and\nglobal-level class representations. On the server side, the uploaded class\nexemplars are leveraged to model class prototypes, which supervise global\nbranch of clients, ensuring alignment with global-level representation. On the\nclient side, we incorporate an adversarial mechanism to harmonize contributions\nof global and local branches, leading to consistent output. Moreover,\nmultilevel contrastive losses are employed on both sides to enforce consistency\nbetween two-level representations in the same semantic space. Extensive\nexperiments on several driving scene segmentation datasets demonstrate that our\nframework outperforms state-of-the-art methods, significantly improving average\nsegmentation accuracy and effectively addressing the class-consistency\nrepresentation problem.",
      "tldr_zh": "本文提出 FedSaaS 框架，用于 Federated semantic segmentation，通过全球原型监督和本地对抗协调来解决数据隐私下类一致性问题，特别是领域偏移导致的类表示模糊。框架在服务器端利用 class exemplars 建模 class prototypes 来监督客户端的全局分支，并在客户端端引入 adversarial mechanism 协调全局和本地分支的输出，同时采用 multilevel contrastive losses 强制两级表示在同一语义空间中的一致性。实验在多个驾驶场景分割数据集上表明，FedSaaS 超过了现有方法，提高了平均分割准确率，并有效处理了类一致性表示问题。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2505.09385v1",
      "published_date": "2025-05-14 13:38:30 UTC",
      "updated_date": "2025-05-14 13:38:30 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T22:57:47.228010"
    },
    {
      "arxiv_id": "2505.09382v1",
      "title": "The Voice Timbre Attribute Detection 2025 Challenge Evaluation Plan",
      "title_zh": "2025年",
      "authors": [
        "Zhengyan Sheng",
        "Jinghao He",
        "Liping Chen",
        "Kong Aik Lee",
        "Zhen-Hua Ling"
      ],
      "abstract": "Voice timbre refers to the unique quality or character of a person's voice\nthat distinguishes it from others as perceived by human hearing. The Voice\nTimbre Attribute Detection (VtaD) 2025 challenge focuses on explaining the\nvoice timbre attribute in a comparative manner. In this challenge, the human\nimpression of voice timbre is verbalized with a set of sensory descriptors,\nincluding bright, coarse, soft, magnetic, and so on. The timbre is explained\nfrom the comparison between two voices in their intensity within a specific\ndescriptor dimension. The VtaD 2025 challenge starts in May and culminates in a\nspecial proposal at the NCMMSC2025 conference in October 2025 in Zhenjiang,\nChina.",
      "tldr_zh": "该论文介绍了Voice Timbre Attribute Detection (VtaD) 2025挑战的评估计划，旨在通过比较方式解释语音音色的独特品质。挑战将语音音色的人类感知转化为一组感官描述符（如bright、coarse、soft和magnetic），并评估两个声音在这些描述符维度上的强度差异。VtaD 2025挑战从5月开始，并在2025年10月于中国Zhenjiang的NCMMSC2025会议上结束，促进了语音处理领域的创新研究。",
      "categories": [
        "cs.SD",
        "cs.AI",
        "eess.AS"
      ],
      "primary_category": "cs.SD",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2505.09382v1",
      "published_date": "2025-05-14 13:35:53 UTC",
      "updated_date": "2025-05-14 13:35:53 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T22:57:58.125694"
    },
    {
      "arxiv_id": "2505.09380v1",
      "title": "Examining Deployment and Refinement of the VIOLA-AI Intracranial Hemorrhage Model Using an Interactive NeoMedSys Platform",
      "title_zh": "使用交互式 NeoMedSys 平台对 VIOLA-AI 颅内出血模型的部署与优化研究",
      "authors": [
        "Qinghui Liu",
        "Jon Nesvold",
        "Hanna Raaum",
        "Elakkyen Murugesu",
        "Martin Røvang",
        "Bradley J Maclntosh",
        "Atle Bjørnerud",
        "Karoline Skogen"
      ],
      "abstract": "Background: There are many challenges and opportunities in the clinical\ndeployment of AI tools in radiology. The current study describes a radiology\nsoftware platform called NeoMedSys that can enable efficient deployment and\nrefinements of AI models. We evaluated the feasibility and effectiveness of\nrunning NeoMedSys for three months in real-world clinical settings and focused\non improvement performance of an in-house developed AI model (VIOLA-AI)\ndesigned for intracranial hemorrhage (ICH) detection.\n  Methods: NeoMedSys integrates tools for deploying, testing, and optimizing AI\nmodels with a web-based medical image viewer, annotation system, and\nhospital-wide radiology information systems. A pragmatic investigation was\ndeployed using clinical cases of patients presenting to the largest Emergency\nDepartment in Norway (site-1) with suspected traumatic brain injury (TBI) or\npatients with suspected stroke (site-2). We assessed ICH classification\nperformance as VIOLA-AI encountered new data and underwent pre-planned model\nretraining. Performance metrics included sensitivity, specificity, accuracy,\nand the area under the receiver operating characteristic curve (AUC).\n  Results: NeoMedSys facilitated iterative improvements in the AI model,\nsignificantly enhancing its diagnostic accuracy. Automated bleed detection and\nsegmentation were reviewed in near real-time to facilitate re-training\nVIOLA-AI. The iterative refinement process yielded a marked improvement in\nclassification sensitivity, rising to 90.3% (from 79.2%), and specificity that\nreached 89.3% (from 80.7%). The bleed detection ROC analysis for the entire\nsample demonstrated a high area-under-the-curve (AUC) of 0.949 (from 0.873).\nModel refinement stages were associated with notable gains, highlighting the\nvalue of real-time radiologist feedback.",
      "tldr_zh": "本研究评估了NeoMedSys平台在临床环境中部署和优化VIOLA-AI模型的可行性，该模型专注于颅内出血(ICH)检测。NeoMedSys整合了AI部署、测试工具、网页医疗图像查看器和放射学信息系统，通过实时反馈和预先计划的模型重新训练，在挪威急诊部门对临床病例进行了三月试验。结果显示，VIOLA-AI的敏感性从79.2%提高到90.3%，特异性从80.7%提高到89.3%，AUC从0.873提升到0.949，证明了该平台在提升AI诊断准确性和促进迭代改进方面的显著价值。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "19 pages, 11 figures, on submission to BMC Methods",
      "pdf_url": "http://arxiv.org/pdf/2505.09380v1",
      "published_date": "2025-05-14 13:33:38 UTC",
      "updated_date": "2025-05-14 13:33:38 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T22:58:11.251024"
    },
    {
      "arxiv_id": "2505.09371v1",
      "title": "TensorRL-QAS: Reinforcement learning with tensor networks for scalable quantum architecture search",
      "title_zh": "TensorRL-QAS：利用张量网络的强化学习用于可扩展量子架构搜索",
      "authors": [
        "Akash Kundu",
        "Stefano Mangini"
      ],
      "abstract": "Variational quantum algorithms hold the promise to address meaningful quantum\nproblems already on noisy intermediate-scale quantum hardware, but they face\nthe challenge of designing quantum circuits that both solve the target problem\nand comply with device limitations. Quantum architecture search (QAS) automates\nthis design process, with reinforcement learning (RL) emerging as a promising\napproach. Yet, RL-based QAS methods encounter significant scalability issues,\nas computational and training costs grow rapidly with the number of qubits,\ncircuit depth, and noise, severely impacting performance. To address these\nchallenges, we introduce $\\textit{TensorRL-QAS}$, a scalable framework that\ncombines tensor network (TN) methods with RL for designing quantum circuits. By\nwarm-starting the architecture search with a matrix product state approximation\nof the target solution, TensorRL-QAS effectively narrows the search space to\nphysically meaningful circuits, accelerating convergence to the desired\nsolution. Tested on several quantum chemistry problems of up to 12-qubit,\nTensorRL-QAS achieves up to a 10-fold reduction in CNOT count and circuit depth\ncompared to baseline methods, while maintaining or surpassing chemical\naccuracy. It reduces function evaluations by up to 100-fold, accelerates\ntraining episodes by up to $98\\%$, and achieves up to $50\\%$ success\nprobability for 10-qubit systems-far exceeding the $<1\\%$ rates of baseline\napproaches. Robustness and versatility are demonstrated both in the noiseless\nand noisy scenarios, where we report a simulation of up to 8-qubit. These\nadvancements establish TensorRL-QAS as a promising candidate for a scalable and\nefficient quantum circuit discovery protocol on near-term quantum hardware.",
      "tldr_zh": "这篇论文提出了 TensorRL-QAS，一种结合 reinforcement learning 和 tensor networks 的框架，用于可扩展的 quantum architecture search（QAS），以解决变分量子算法在设计量子电路时的可扩展性挑战。该框架通过 matrix product state 近似预热搜索，缩小搜索空间并加速收敛到物理意义上的电路设计。在量子化学问题测试中，TensorRL-QAS 在高达 12-qubit 的系统中实现了 CNOT count 和电路深度减少高达 10 倍，函数评估减少高达 100 倍，训练加速达 98%，并在 10-qubit 系统上成功概率提升至 50%，远超基线方法的不到 1%。这为近中期量子硬件上的高效电路发现提供了可扩展且鲁棒的解决方案。",
      "categories": [
        "quant-ph",
        "cs.AI",
        "cs.ET",
        "cs.LG"
      ],
      "primary_category": "quant-ph",
      "comment": "The code will be available soon! Comments are welcomed!",
      "pdf_url": "http://arxiv.org/pdf/2505.09371v1",
      "published_date": "2025-05-14 13:23:34 UTC",
      "updated_date": "2025-05-14 13:23:34 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T22:58:24.202279"
    },
    {
      "arxiv_id": "2505.13491v1",
      "title": "ProdRev: A DNN framework for empowering customers using generative pre-trained transformers",
      "title_zh": "翻译失败",
      "authors": [
        "Aakash Gupta",
        "Nataraj Das"
      ],
      "abstract": "Following the pandemic, customers, preference for using e-commerce has\naccelerated. Since much information is available in multiple reviews (sometimes\nrunning in thousands) for a single product, it can create decision paralysis\nfor the buyer. This scenario disempowers the consumer, who cannot be expected\nto go over so many reviews since its time consuming and can confuse them.\nVarious commercial tools are available, that use a scoring mechanism to arrive\nat an adjusted score. It can alert the user to potential review manipulations.\nThis paper proposes a framework that fine-tunes a generative pre-trained\ntransformer to understand these reviews better. Furthermore, using\n\"common-sense\" to make better decisions. These models have more than 13 billion\nparameters. To fine-tune the model for our requirement, we use the curie engine\nfrom generative pre-trained transformer (GPT3). By using generative models, we\nare introducing abstractive summarization. Instead of using a simple extractive\nmethod of summarizing the reviews. This brings out the true relationship\nbetween the reviews and not simply copy-paste. This introduces an element of\n\"common sense\" for the user and helps them to quickly make the right decisions.\nThe user is provided the pros and cons of the processed reviews. Thus the\nuser/customer can take their own decisions.",
      "tldr_zh": "本研究提出ProdRev框架，这是一个基于DNN（Deep Neural Network）的系统，利用generative pre-trained transformers（如GPT-3的curie引擎）来处理电子商务产品的海量评论，帮助用户避免决策瘫痪。框架通过微调模型实现abstractive summarization（抽象式总结），而非简单的extractive方法，从而捕捉评论之间的真正关系并引入“common-sense”推理。最终，用户能快速获取处理后的评论优缺点，支持独立决策。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "2022 International Conference on Decision Aid Sciences and\n  Applications (DASA)",
      "pdf_url": "http://arxiv.org/pdf/2505.13491v1",
      "published_date": "2025-05-14 13:07:48 UTC",
      "updated_date": "2025-05-14 13:07:48 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T22:58:33.447773"
    },
    {
      "arxiv_id": "2505.09344v1",
      "title": "GreenFactory: Ensembling Zero-Cost Proxies to Estimate Performance of Neural Networks",
      "title_zh": "翻译失败",
      "authors": [
        "Gabriel Cortês",
        "Nuno Lourenço",
        "Paolo Romano",
        "Penousal Machado"
      ],
      "abstract": "Determining the performance of a Deep Neural Network during Neural\nArchitecture Search processes is essential for identifying optimal\narchitectures and hyperparameters. Traditionally, this process requires\ntraining and evaluation of each network, which is time-consuming and\nresource-intensive. Zero-cost proxies estimate performance without training,\nserving as an alternative to traditional training. However, recent proxies\noften lack generalization across diverse scenarios and provide only relative\nrankings rather than predicted accuracies. To address these limitations, we\npropose GreenFactory, an ensemble of zero-cost proxies that leverages a random\nforest regressor to combine multiple predictors' strengths and directly predict\nmodel test accuracy. We evaluate GreenFactory on NATS-Bench, achieving robust\nresults across multiple datasets. Specifically, GreenFactory achieves high\nKendall correlations on NATS-Bench-SSS, indicating substantial agreement\nbetween its predicted scores and actual performance: 0.907 for CIFAR-10, 0.945\nfor CIFAR-100, and 0.920 for ImageNet-16-120. Similarly, on NATS-Bench-TSS, we\nachieve correlations of 0.921 for CIFAR-10, 0.929 for CIFAR-100, and 0.908 for\nImageNet-16-120, showcasing its reliability in both search spaces.",
      "tldr_zh": "该论文提出GreenFactory，一种集成多种zero-cost proxies的方法，使用random forest regressor结合多个预测器，直接估计神经网络的测试准确率，以解决传统神经架构搜索中训练耗时和资源密集的问题。在NATS-Bench数据集上进行评估，GreenFactory在NATS-Bench-SSS上实现了高Kendall correlations，包括CIFAR-10的0.907、CIFAR-100的0.945和ImageNet-16-120的0.920；在NATS-Bench-TSS上也表现出色，相关性分别为0.921、0.929和0.908。这些结果证明了GreenFactory在不同搜索空间中的可靠性和泛化能力，提高了神经网络性能预测的效率。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2505.09344v1",
      "published_date": "2025-05-14 12:40:34 UTC",
      "updated_date": "2025-05-14 12:40:34 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T22:58:46.414496"
    },
    {
      "arxiv_id": "2505.09343v1",
      "title": "Insights into DeepSeek-V3: Scaling Challenges and Reflections on Hardware for AI Architectures",
      "title_zh": "DeepSeek-V3 的洞见：AI 架构硬件的",
      "authors": [
        "Chenggang Zhao",
        "Chengqi Deng",
        "Chong Ruan",
        "Damai Dai",
        "Huazuo Gao",
        "Jiashi Li",
        "Liyue Zhang",
        "Panpan Huang",
        "Shangyan Zhou",
        "Shirong Ma",
        "Wenfeng Liang",
        "Ying He",
        "Yuqing Wang",
        "Yuxuan Liu",
        "Y. X. Wei"
      ],
      "abstract": "The rapid scaling of large language models (LLMs) has unveiled critical\nlimitations in current hardware architectures, including constraints in memory\ncapacity, computational efficiency, and interconnection bandwidth. DeepSeek-V3,\ntrained on 2,048 NVIDIA H800 GPUs, demonstrates how hardware-aware model\nco-design can effectively address these challenges, enabling cost-efficient\ntraining and inference at scale. This paper presents an in-depth analysis of\nthe DeepSeek-V3/R1 model architecture and its AI infrastructure, highlighting\nkey innovations such as Multi-head Latent Attention (MLA) for enhanced memory\nefficiency, Mixture of Experts (MoE) architectures for optimized\ncomputation-communication trade-offs, FP8 mixed-precision training to unlock\nthe full potential of hardware capabilities, and a Multi-Plane Network Topology\nto minimize cluster-level network overhead. Building on the hardware\nbottlenecks encountered during DeepSeek-V3's development, we engage in a\nbroader discussion with academic and industry peers on potential future\nhardware directions, including precise low-precision computation units,\nscale-up and scale-out convergence, and innovations in low-latency\ncommunication fabrics. These insights underscore the critical role of hardware\nand model co-design in meeting the escalating demands of AI workloads, offering\na practical blueprint for innovation in next-generation AI systems.",
      "tldr_zh": "这篇论文分析了大规模语言模型（LLMs）的快速扩展所暴露的硬件限制，包括内存容量、计算效率和互连带宽，并以 DeepSeek-V3 模型为例，展示了硬件感知模型共同设计如何实现成本高效的训练和推理。DeepSeek-V3 在 2,048 个 NVIDIA H800 GPU 上训练，引入了关键创新如 Multi-head Latent Attention (MLA) 以提升内存效率、Mixture of Experts (MoE) 架构优化计算通信权衡、FP8 混合精度训练充分利用硬件能力，以及 Multi-Plane Network Topology 减少网络开销。论文还讨论了开发过程中遇到的硬件瓶颈，并提出未来方向，如精确低精度计算单元和低延迟通信结构，提供了一个硬件与模型共同设计的蓝图，以应对 AI 工作负载的日益需求。",
      "categories": [
        "cs.DC",
        "cs.AI",
        "cs.AR"
      ],
      "primary_category": "cs.DC",
      "comment": "This is the author's version of the work. It is posted here for your\n  personal use. Not for redistribution. The definitive version will appear as\n  part of the Industry Track in Proceedings of the 52nd Annual International\n  Symposium on Computer Architecture (ISCA '25)",
      "pdf_url": "http://arxiv.org/pdf/2505.09343v1",
      "published_date": "2025-05-14 12:39:03 UTC",
      "updated_date": "2025-05-14 12:39:03 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T22:58:59.437276"
    },
    {
      "arxiv_id": "2505.09342v1",
      "title": "Evaluating the Robustness of Adversarial Defenses in Malware Detection Systems",
      "title_zh": "评估对抗性防御在恶意软件检测系统中的鲁棒性",
      "authors": [
        "Mostafa Jafari",
        "Alireza Shameli-Sendi"
      ],
      "abstract": "Machine learning is a key tool for Android malware detection, effectively\nidentifying malicious patterns in apps. However, ML-based detectors are\nvulnerable to evasion attacks, where small, crafted changes bypass detection.\nDespite progress in adversarial defenses, the lack of comprehensive evaluation\nframeworks in binary-constrained domains limits understanding of their\nrobustness. We introduce two key contributions. First, Prioritized Binary\nRounding, a technique to convert continuous perturbations into binary feature\nspaces while preserving high attack success and low perturbation size. Second,\nthe sigma-binary attack, a novel adversarial method for binary domains,\ndesigned to achieve attack goals with minimal feature changes. Experiments on\nthe Malscan dataset show that sigma-binary outperforms existing attacks and\nexposes key vulnerabilities in state-of-the-art defenses. Defenses equipped\nwith adversary detectors, such as KDE, DLA, DNN+, and ICNN, exhibit significant\nbrittleness, with attack success rates exceeding 90% using fewer than 10\nfeature modifications and reaching 100% with just 20. Adversarially trained\ndefenses, including AT-rFGSM-k, AT-MaxMA, improves robustness under small\nbudgets but remains vulnerable to unrestricted perturbations, with attack\nsuccess rates of 99.45% and 96.62%, respectively. Although PAD-SMA demonstrates\nstrong robustness against state-of-the-art gradient-based adversarial attacks\nby maintaining an attack success rate below 16.55%, the sigma-binary attack\nsignificantly outperforms these methods, achieving a 94.56% success rate under\nunrestricted perturbations. These findings highlight the critical need for\nprecise method like sigma-binary to expose hidden vulnerabilities in existing\ndefenses and support the development of more resilient malware detection\nsystems.",
      "tldr_zh": "该研究评估了对抗防御在恶意软件检测系统中的鲁棒性，针对机器学习模型易受 evasion attacks 的问题，提出了两个关键贡献：Prioritized Binary Rounding 技术，用于将连续扰动转换为二进制特征空间，同时保持高攻击成功率和低扰动大小；以及 sigma-binary attack，一种新型针对二进制领域的对抗方法，旨在通过最小特征变化实现攻击目标。实验在 Malscan 数据集上显示，sigma-binary attack 优于现有攻击，成功暴露了多种防御（如 KDE、DLA、DNN+ 和 ICNN）的脆弱性，攻击成功率可超过90%甚至100%。尽管某些对抗训练防御（如 AT-rFGSM-k 和 AT-MaxMA）在小预算扰动下有所改善，但整体仍易受无限制扰动影响，而 PAD-SMA 虽对梯度攻击鲁棒，但对 sigma-binary attack 的成功率高达94.56%，突显了开发更坚固检测系统的迫切需求。",
      "categories": [
        "cs.CR",
        "cs.AI",
        "cs.LG",
        "68",
        "I.2.1"
      ],
      "primary_category": "cs.CR",
      "comment": "Submitted to IEEE Transactions on Information Forensics and Security\n  (T-IFS), 13 pages, 4 figures",
      "pdf_url": "http://arxiv.org/pdf/2505.09342v1",
      "published_date": "2025-05-14 12:38:43 UTC",
      "updated_date": "2025-05-14 12:38:43 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T22:59:11.999272"
    },
    {
      "arxiv_id": "2505.09341v1",
      "title": "Access Controls Will Solve the Dual-Use Dilemma",
      "title_zh": "访问控制将解决双重用途困境",
      "authors": [
        "Evžen Wybitul"
      ],
      "abstract": "AI safety systems face a dual-use dilemma. Since the same request can be\neither harmless or harmful depending on who made it and why, if the system\nmakes decisions based solely on the request's content, it will refuse some\nlegitimate queries and let pass harmful ones. To address this, we propose a\nconceptual access control framework, based on verified user credentials (such\nas institutional affiliation) and classifiers that assign model outputs to risk\ncategories (such as advanced virology). The system permits responses only when\nthe user's verified credentials match the category's requirements. For\nimplementation of the model output classifiers, we introduce a theoretical\napproach utilizing small, gated expert modules integrated into the generator\nmodel, trained with gradient routing, that enable efficient risk detection\nwithout the capability gap problems of external monitors. While open questions\nremain about the verification mechanisms, risk categories, and the technical\nimplementation, our framework makes the first step toward enabling granular\ngovernance of AI capabilities: verified users gain access to specialized\nknowledge without arbitrary restrictions, while adversaries are blocked from\nit. This contextual approach reconciles model utility with robust safety,\naddressing the dual-use dilemma.",
      "tldr_zh": "该论文讨论了AI安全系统的双重用途困境（dual-use dilemma），即基于请求内容做决策会导致拒绝合法查询或放行有害请求的问题。为解决此问题，作者提出一个概念性的访问控制框架（access control framework），利用验证的用户凭证（如institutional affiliation）和风险分类器（如advanced virology类别）来控制模型输出，仅允许凭证匹配风险要求的响应。框架引入小而受控的专家模块（gated expert modules）和梯度路由（gradient routing）技术，集成到生成模型中，实现高效的风险检测，避免外部监控的缺陷。尽管验证机制和风险类别等细节仍有待探讨，这一方法为细粒度AI能力治理提供了基础，平衡了模型实用性和安全性。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2505.09341v1",
      "published_date": "2025-05-14 12:38:08 UTC",
      "updated_date": "2025-05-14 12:38:08 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T22:59:24.091543"
    },
    {
      "arxiv_id": "2505.09329v1",
      "title": "BioVFM-21M: Benchmarking and Scaling Self-Supervised Vision Foundation Models for Biomedical Image Analysis",
      "title_zh": "翻译失败",
      "authors": [
        "Jiarun Liu",
        "Hong-Yu Zhou",
        "Weijian Huang",
        "Hao Yang",
        "Dongning Song",
        "Tao Tan",
        "Yong Liang",
        "Shanshan Wang"
      ],
      "abstract": "Scaling up model and data size have demonstrated impressive performance\nimprovement over a wide range of tasks. Despite extensive studies on scaling\nbehaviors for general-purpose tasks, medical images exhibit substantial\ndifferences from natural data. It remains unclear the key factors in developing\nmedical vision foundation models at scale due to the absence of an extensive\nunderstanding of scaling behavior in the medical domain. In this paper, we\nexplored the scaling behavior across model sizes, training algorithms, data\nsizes, and imaging modalities in developing scalable medical vision foundation\nmodels by self-supervised learning. To support scalable pretraining, we\nintroduce BioVFM-21M, a large-scale biomedical image dataset encompassing a\nwide range of biomedical image modalities and anatomies. We observed that\nscaling up does provide benefits but varies across tasks. Additional analysis\nreveals several factors correlated with scaling benefits. Finally, we propose\nBioVFM, a large-scale medical vision foundation model pretrained on 21 million\nbiomedical images, which outperforms the previous state-of-the-art foundation\nmodels across 12 medical benchmarks. Our results highlight that while scaling\nup is beneficial for pursuing better performance, task characteristics, data\ndiversity, pretraining methods, and computational efficiency remain critical\nconsiderations for developing scalable medical foundation models.",
      "tldr_zh": "本研究探讨了自监督学习(Self-Supervised Learning)中模型大小、训练算法、数据大小和图像模态等因素对生物医学图像分析的影响，发现扩展规模虽有益但因任务而异。作者引入BioVFM-21M数据集，该数据集包含2100万张跨多种模态和解剖结构的生物医学图像，用于支持可扩展的预训练。最终，他们提出BioVFM模型，通过在BioVFM-21M上预训练，优于现有最先进模型，在12个医学基准测试中表现出色，并强调任务特性、数据多样性和计算效率是开发可扩展医疗视觉基础模型的关键考虑因素。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "11 pages, 4 figures",
      "pdf_url": "http://arxiv.org/pdf/2505.09329v1",
      "published_date": "2025-05-14 12:25:41 UTC",
      "updated_date": "2025-05-14 12:25:41 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T22:59:35.245881"
    },
    {
      "arxiv_id": "2505.09324v1",
      "title": "Neural Video Compression using 2D Gaussian Splatting",
      "title_zh": "翻译失败",
      "authors": [
        "Lakshya Gupta",
        "Imran N. Junejo"
      ],
      "abstract": "The computer vision and image processing research community has been involved\nin standardizing video data communications for the past many decades, leading\nto standards such as AVC, HEVC, VVC, AV1, AV2, etc. However, recent\ngroundbreaking works have focused on employing deep learning-based techniques\nto replace the traditional video codec pipeline to a greater affect. Neural\nvideo codecs (NVC) create an end-to-end ML-based solution that does not rely on\nany handcrafted features (motion or edge-based) and have the ability to learn\ncontent-aware compression strategies, offering better adaptability and higher\ncompression efficiency than traditional methods. This holds a great potential\nnot only for hardware design, but also for various video streaming platforms\nand applications, especially video conferencing applications such as MS-Teams\nor Zoom that have found extensive usage in classrooms and workplaces. However,\ntheir high computational demands currently limit their use in real-time\napplications like video conferencing. To address this, we propose a\nregion-of-interest (ROI) based neural video compression model that leverages 2D\nGaussian Splatting. Unlike traditional codecs, 2D Gaussian Splatting is capable\nof real-time decoding and can be optimized using fewer data points, requiring\nonly thousands of Gaussians for decent quality outputs as opposed to millions\nin 3D scenes. In this work, we designed a video pipeline that speeds up the\nencoding time of the previous Gaussian splatting-based image codec by 88% by\nusing a content-aware initialization strategy paired with a novel Gaussian\ninter-frame redundancy-reduction mechanism, enabling Gaussian splatting to be\nused for a video-codec solution, the first of its kind solution in this neural\nvideo codec space.",
      "tldr_zh": "该研究探讨了神经视频压缩（Neural Video Compression），旨在解决传统和深度学习视频编解码器在实时应用（如视频会议）中的高计算需求问题。作者提出了一种基于区域-of-interest (ROI) 的模型，使用 2D Gaussian Splatting 技术，实现实时解码并仅需数千个高斯分布即可获得良好输出质量。创新点包括内容感知初始化策略和新型高斯帧间冗余减少机制，将编码时间加速 88%，这是首个将 Gaussian Splatting 应用于视频编解码的解决方案，为视频流媒体平台和硬件设计带来更高的适应性和效率。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG",
        "eess.IV"
      ],
      "primary_category": "cs.CV",
      "comment": "9 pages, 8 figures",
      "pdf_url": "http://arxiv.org/pdf/2505.09324v1",
      "published_date": "2025-05-14 12:23:53 UTC",
      "updated_date": "2025-05-14 12:23:53 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T22:59:47.627727"
    },
    {
      "arxiv_id": "2505.09295v1",
      "title": "Toward Fair Federated Learning under Demographic Disparities and Data Imbalance",
      "title_zh": "翻译失败",
      "authors": [
        "Qiming Wu",
        "Siqi Li",
        "Doudou Zhou",
        "Nan Liu"
      ],
      "abstract": "Ensuring fairness is critical when applying artificial intelligence to\nhigh-stakes domains such as healthcare, where predictive models trained on\nimbalanced and demographically skewed data risk exacerbating existing\ndisparities. Federated learning (FL) enables privacy-preserving collaboration\nacross institutions, but remains vulnerable to both algorithmic bias and\nsubgroup imbalance - particularly when multiple sensitive attributes intersect.\nWe propose FedIDA (Fed erated Learning for Imbalance and D isparity A\nwareness), a framework-agnostic method that combines fairness-aware\nregularization with group-conditional oversampling. FedIDA supports multiple\nsensitive attributes and heterogeneous data distributions without altering the\nconvergence behavior of the underlying FL algorithm. We provide theoretical\nanalysis establishing fairness improvement bounds using Lipschitz continuity\nand concentration inequalities, and show that FedIDA reduces the variance of\nfairness metrics across test sets. Empirical results on both benchmark and\nreal-world clinical datasets confirm that FedIDA consistently improves fairness\nwhile maintaining competitive predictive performance, demonstrating its\neffectiveness for equitable and privacy-preserving modeling in healthcare. The\nsource code is available on GitHub.",
      "tldr_zh": "这篇论文针对联邦学习（Federated Learning）在数据不平衡和人口统计差异下的公平性问题，提出了FedIDA框架，以缓解算法偏差和子群不平衡带来的不平等。FedIDA结合公平性感知正则化（fairness-aware regularization）和组条件过采样（group-conditional oversampling），支持多个敏感属性和异构数据分布，同时不影响底层FL算法的收敛行为。理论分析利用Lipschitz连续性和集中不等式建立了公平性改进的边界，并通过实验在基准和真实临床数据集上证明，FedIDA显著提高了公平性指标，同时保持了竞争性的预测性能。",
      "categories": [
        "cs.CY",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CY",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2505.09295v1",
      "published_date": "2025-05-14 11:22:54 UTC",
      "updated_date": "2025-05-14 11:22:54 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T22:59:59.276278"
    },
    {
      "arxiv_id": "2505.09289v1",
      "title": "Reproducibility Study of \"Cooperate or Collapse: Emergence of Sustainable Cooperation in a Society of LLM Agents\"",
      "title_zh": "翻译失败",
      "authors": [
        "Pedro M. P. Curvo",
        "Mara Dragomir",
        "Salvador Torpes",
        "Mohammadmahdi Rahimi"
      ],
      "abstract": "This study evaluates and extends the findings made by Piatti et al., who\nintroduced GovSim, a simulation framework designed to assess the cooperative\ndecision-making capabilities of large language models (LLMs) in\nresource-sharing scenarios. By replicating key experiments, we validate claims\nregarding the performance of large models, such as GPT-4-turbo, compared to\nsmaller models. The impact of the universalization principle is also examined,\nwith results showing that large models can achieve sustainable cooperation,\nwith or without the principle, while smaller models fail without it. In\naddition, we provide multiple extensions to explore the applicability of the\nframework to new settings. We evaluate additional models, such as DeepSeek-V3\nand GPT-4o-mini, to test whether cooperative behavior generalizes across\ndifferent architectures and model sizes. Furthermore, we introduce new\nsettings: we create a heterogeneous multi-agent environment, study a scenario\nusing Japanese instructions, and explore an \"inverse environment\" where agents\nmust cooperate to mitigate harmful resource distributions. Our results confirm\nthat the benchmark can be applied to new models, scenarios, and languages,\noffering valuable insights into the adaptability of LLMs in complex cooperative\ntasks. Moreover, the experiment involving heterogeneous multi-agent systems\ndemonstrates that high-performing models can influence lower-performing ones to\nadopt similar behaviors. This finding has significant implications for other\nagent-based applications, potentially enabling more efficient use of\ncomputational resources and contributing to the development of more effective\ncooperative AI systems.",
      "tldr_zh": "这篇论文是对 Piatti et al. 的 GovSim 框架的可重复性研究，评估大型语言模型 (LLMs) 在资源共享场景中的合作决策能力。通过复制关键实验，验证了大型模型如 GPT-4-turbo 即使在没有普遍化原则的情况下也能实现可持续合作，而小型模型则依赖该原则。\n研究扩展了框架，测试了新模型如 DeepSeek-V3 和 GPT-4o-mini，并引入了异构多智能体环境、日语指令场景以及逆环境（agents 合作缓解有害资源分配）。\n结果表明，GovSim 可应用于不同模型、场景和语言，高性能模型能影响低性能模型的行为，这为更高效的合作 AI 系统开发提供了宝贵启示。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "11 Tables, 9 Figures",
      "pdf_url": "http://arxiv.org/pdf/2505.09289v1",
      "published_date": "2025-05-14 11:15:14 UTC",
      "updated_date": "2025-05-14 11:15:14 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T23:00:12.735549"
    },
    {
      "arxiv_id": "2505.13489v1",
      "title": "Contrastive Cross-Course Knowledge Tracing via Concept Graph Guided Knowledge Transfer",
      "title_zh": "翻译失败",
      "authors": [
        "Wenkang Han",
        "Wang Lin",
        "Liya Hu",
        "Zhenlong Dai",
        "Yiyun Zhou",
        "Mengze Li",
        "Zemin Liu",
        "Chang Yao",
        "Jingyuan Chen"
      ],
      "abstract": "Knowledge tracing (KT) aims to predict learners' future performance based on\nhistorical learning interactions. However, existing KT models predominantly\nfocus on data from a single course, limiting their ability to capture a\ncomprehensive understanding of learners' knowledge states. In this paper, we\npropose TransKT, a contrastive cross-course knowledge tracing method that\nleverages concept graph guided knowledge transfer to model the relationships\nbetween learning behaviors across different courses, thereby enhancing\nknowledge state estimation. Specifically, TransKT constructs a cross-course\nconcept graph by leveraging zero-shot Large Language Model (LLM) prompts to\nestablish implicit links between related concepts across different courses.\nThis graph serves as the foundation for knowledge transfer, enabling the model\nto integrate and enhance the semantic features of learners' interactions across\ncourses. Furthermore, TransKT includes an LLM-to-LM pipeline for incorporating\nsummarized semantic features, which significantly improves the performance of\nGraph Convolutional Networks (GCNs) used for knowledge transfer. Additionally,\nTransKT employs a contrastive objective that aligns single-course and\ncross-course knowledge states, thereby refining the model's ability to provide\na more robust and accurate representation of learners' overall knowledge\nstates.",
      "tldr_zh": "本研究提出TransKT，一种基于概念图引导知识转移的对比学习方法，用于提升Knowledge Tracing（KT）模型在跨课程学习中的性能。通过利用零样本Large Language Model（LLM）提示，TransKT构建跨课程概念图，以建立不同课程间概念的隐式链接，并通过LLM-to-LM管道整合语义特征到Graph Convolutional Networks（GCNs）中，实现知识状态的增强转移。该方法还采用对比目标对齐单一课程和跨课程知识状态，提供更准确的学习者知识表示，相比传统KT模型显著改善了整体估计能力。",
      "categories": [
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.AI",
      "comment": "Accepted by IJCAI 2025",
      "pdf_url": "http://arxiv.org/pdf/2505.13489v1",
      "published_date": "2025-05-14 10:38:30 UTC",
      "updated_date": "2025-05-14 10:38:30 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T23:00:24.483057"
    },
    {
      "arxiv_id": "2505.09265v1",
      "title": "MetaUAS: Universal Anomaly Segmentation with One-Prompt Meta-Learning",
      "title_zh": "翻译失败",
      "authors": [
        "Bin-Bin Gao"
      ],
      "abstract": "Zero- and few-shot visual anomaly segmentation relies on powerful\nvision-language models that detect unseen anomalies using manually designed\ntextual prompts. However, visual representations are inherently independent of\nlanguage. In this paper, we explore the potential of a pure visual foundation\nmodel as an alternative to widely used vision-language models for universal\nvisual anomaly segmentation. We present a novel paradigm that unifies anomaly\nsegmentation into change segmentation. This paradigm enables us to leverage\nlarge-scale synthetic image pairs, featuring object-level and local region\nchanges, derived from existing image datasets, which are independent of target\nanomaly datasets. We propose a one-prompt Meta-learning framework for Universal\nAnomaly Segmentation (MetaUAS) that is trained on this synthetic dataset and\nthen generalizes well to segment any novel or unseen visual anomalies in the\nreal world. To handle geometrical variations between prompt and query images,\nwe propose a soft feature alignment module that bridges paired-image change\nperception and single-image semantic segmentation. This is the first work to\nachieve universal anomaly segmentation using a pure vision model without\nrelying on special anomaly detection datasets and pre-trained visual-language\nmodels. Our method effectively and efficiently segments any anomalies with only\none normal image prompt and enjoys training-free without guidance from\nlanguage. Our MetaUAS significantly outperforms previous zero-shot, few-shot,\nand even full-shot anomaly segmentation methods. The code and pre-trained\nmodels are available at https://github.com/gaobb/MetaUAS.",
      "tldr_zh": "本论文提出MetaUAS框架，利用元学习（Meta-learning）实现通用异常分割（Universal Anomaly Segmentation），通过将异常分割统一为变化分割，并基于大规模合成图像对进行训练，以泛化到真实世界的任何新颖异常。\n该框架采用一提示机制，仅需一个正常图像提示，即可有效分割异常，并引入软特征对齐模块（soft feature alignment module）来处理提示图像和查询图像之间的几何变化。\nMetaUAS是首个使用纯视觉模型的方法，不依赖视觉语言模型或特殊异常检测数据集，且在性能上显著优于现有零样本、少样本甚至全样本异常分割方法。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "Accepted by NeurIPS 2024",
      "pdf_url": "http://arxiv.org/pdf/2505.09265v1",
      "published_date": "2025-05-14 10:25:26 UTC",
      "updated_date": "2025-05-14 10:25:26 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T23:00:36.286210"
    },
    {
      "arxiv_id": "2505.09264v1",
      "title": "Learning to Detect Multi-class Anomalies with Just One Normal Image Prompt",
      "title_zh": "翻译失败",
      "authors": [
        "Bin-Bin Gao"
      ],
      "abstract": "Unsupervised reconstruction networks using self-attention transformers have\nachieved state-of-the-art performance for multi-class (unified) anomaly\ndetection with a single model. However, these self-attention reconstruction\nmodels primarily operate on target features, which may result in perfect\nreconstruction for both normal and anomaly features due to high consistency\nwith context, leading to failure in detecting anomalies. Additionally, these\nmodels often produce inaccurate anomaly segmentation due to performing\nreconstruction in a low spatial resolution latent space. To enable\nreconstruction models enjoying high efficiency while enhancing their\ngeneralization for unified anomaly detection, we propose a simple yet effective\nmethod that reconstructs normal features and restores anomaly features with\njust One Normal Image Prompt (OneNIP). In contrast to previous work, OneNIP\nallows for the first time to reconstruct or restore anomalies with just one\nnormal image prompt, effectively boosting unified anomaly detection\nperformance. Furthermore, we propose a supervised refiner that regresses\nreconstruction errors by using both real normal and synthesized anomalous\nimages, which significantly improves pixel-level anomaly segmentation. OneNIP\noutperforms previous methods on three industry anomaly detection benchmarks:\nMVTec, BTAD, and VisA. The code and pre-trained models are available at\nhttps://github.com/gaobb/OneNIP.",
      "tldr_zh": "本文提出OneNIP方法，使用仅一个正常图像提示来重建正常特征并恢复异常特征，解决了自注意力 transformers 在多类异常检测中的问题，如完美重建异常导致检测失败和低空间分辨率下的分割不准确。该方法首次实现基于单一正常图像的异常重建和恢复，并引入监督精炼器，通过真实正常图像和合成异常图像回归重建错误，从而显著提升像素级异常分割性能。在MVTec、BTAD和VisA等工业异常检测基准上，OneNIP优于先前方法，展示了其高效性和泛化能力。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "Accepted by ECCV 2024",
      "pdf_url": "http://arxiv.org/pdf/2505.09264v1",
      "published_date": "2025-05-14 10:25:14 UTC",
      "updated_date": "2025-05-14 10:25:14 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T23:00:47.098047"
    },
    {
      "arxiv_id": "2505.09263v1",
      "title": "Few-Shot Anomaly-Driven Generation for Anomaly Classification and Segmentation",
      "title_zh": "少样本异常驱动生成用于异常分类和分割",
      "authors": [
        "Guan Gui",
        "Bin-Bin Gao",
        "Jun Liu",
        "Chengjie Wang",
        "Yunsheng Wu"
      ],
      "abstract": "Anomaly detection is a practical and challenging task due to the scarcity of\nanomaly samples in industrial inspection. Some existing anomaly detection\nmethods address this issue by synthesizing anomalies with noise or external\ndata. However, there is always a large semantic gap between synthetic and\nreal-world anomalies, resulting in weak performance in anomaly detection. To\nsolve the problem, we propose a few-shot Anomaly-driven Generation (AnoGen)\nmethod, which guides the diffusion model to generate realistic and diverse\nanomalies with only a few real anomalies, thereby benefiting training anomaly\ndetection models. Specifically, our work is divided into three stages. In the\nfirst stage, we learn the anomaly distribution based on a few given real\nanomalies and inject the learned knowledge into an embedding. In the second\nstage, we use the embedding and given bounding boxes to guide the diffusion\nmodel to generate realistic and diverse anomalies on specific objects (or\ntextures). In the final stage, we propose a weakly-supervised anomaly detection\nmethod to train a more powerful model with generated anomalies. Our method\nbuilds upon DRAEM and DesTSeg as the foundation model and conducts experiments\non the commonly used industrial anomaly detection dataset, MVTec. The\nexperiments demonstrate that our generated anomalies effectively improve the\nmodel performance of both anomaly classification and segmentation tasks\nsimultaneously, \\eg, DRAEM and DseTSeg achieved a 5.8\\% and 1.5\\% improvement\nin AU-PR metric on segmentation task, respectively. The code and generated\nanomalous data are available at https://github.com/gaobb/AnoGen.",
      "tldr_zh": "该论文提出了一种Few-Shot Anomaly-Driven Generation (AnoGen)方法，用于解决工业异常检测中异常样本稀缺的问题，通过少量真实异常引导diffusion model生成真实且多样的异常样本，从而提升异常分类和分割任务的性能。方法分为三阶段：首先，从少量真实异常中学习异常分布并注入嵌入；其次，使用嵌入和边界框引导diffusion model在特定对象上生成异常；最后，采用弱监督异常检测方法训练更强大的模型，如基于DRAEM和DesTSeg的基础模型。在MVTec数据集上的实验显示，生成的异常样本显著提高了模型性能，例如DRAEM和DesTSeg在分割任务的AU-PR指标上分别提升了5.8%和1.5%。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "Accepted by ECCV 2024",
      "pdf_url": "http://arxiv.org/pdf/2505.09263v1",
      "published_date": "2025-05-14 10:25:06 UTC",
      "updated_date": "2025-05-14 10:25:06 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T23:00:59.355051"
    },
    {
      "arxiv_id": "2505.09262v1",
      "title": "EDBench: Large-Scale Electron Density Data for Molecular Modeling",
      "title_zh": "EDBench：用于分子建模的大规模电子密度数据",
      "authors": [
        "Hongxin Xiang",
        "Ke Li",
        "Mingquan Liu",
        "Zhixiang Cheng",
        "Bin Yao",
        "Wenjie Du",
        "Jun Xia",
        "Li Zeng",
        "Xin Jin",
        "Xiangxiang Zeng"
      ],
      "abstract": "Existing molecular machine learning force fields (MLFFs) generally focus on\nthe learning of atoms, molecules, and simple quantum chemical properties (such\nas energy and force), but ignore the importance of electron density (ED)\n$\\rho(r)$ in accurately understanding molecular force fields (MFFs). ED\ndescribes the probability of finding electrons at specific locations around\natoms or molecules, which uniquely determines all ground state properties (such\nas energy, molecular structure, etc.) of interactive multi-particle systems\naccording to the Hohenberg-Kohn theorem. However, the calculation of ED relies\non the time-consuming first-principles density functional theory (DFT) which\nleads to the lack of large-scale ED data and limits its application in MLFFs.\nIn this paper, we introduce EDBench, a large-scale, high-quality dataset of ED\ndesigned to advance learning-based research at the electronic scale. Built upon\nthe PCQM4Mv2, EDBench provides accurate ED data, covering 3.3 million\nmolecules. To comprehensively evaluate the ability of models to understand and\nutilize electronic information, we design a suite of ED-centric benchmark tasks\nspanning prediction, retrieval, and generation. Our evaluation on several\nstate-of-the-art methods demonstrates that learning from EDBench is not only\nfeasible but also achieves high accuracy. Moreover, we show that learning-based\nmethod can efficiently calculate ED with comparable precision while\nsignificantly reducing the computational cost relative to traditional DFT\ncalculations. All data and benchmarks from EDBench will be freely available,\nlaying a robust foundation for ED-driven drug discovery and materials science.",
      "tldr_zh": "本研究指出，现有的分子机器学习力场（MLFFs）忽略了电子密度（ED）ρ(r) 在理解分子力场（MFFs）中的关键作用，因为 ED 根据 Hohenberg-Kohn 定理唯一确定了多粒子系统的基态属性，但其计算依赖耗时的密度泛函理论（DFT），导致数据稀缺。论文引入 EDBench，这是一个大规模、高质量的 ED 数据集，基于 PCQM4Mv2 扩展，涵盖 330 万分子。EDBench 设计了涵盖预测、检索和生成的 ED-centric 基准任务，并通过评估证明，学习-based 方法能以高准确率高效计算 ED，同时显著降低计算成本。数据集和基准将免费提供，推动 ED 驱动的药物发现和材料科学发展。",
      "categories": [
        "physics.chem-ph",
        "cs.AI",
        "cs.CV",
        "cs.LG"
      ],
      "primary_category": "physics.chem-ph",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2505.09262v1",
      "published_date": "2025-05-14 10:23:22 UTC",
      "updated_date": "2025-05-14 10:23:22 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T23:01:11.607310"
    },
    {
      "arxiv_id": "2505.09246v1",
      "title": "Focus, Merge, Rank: Improved Question Answering Based on Semi-structured Knowledge Bases",
      "title_zh": "翻译失败",
      "authors": [
        "Derian Boer",
        "Stephen Roth",
        "Stefan Kramer"
      ],
      "abstract": "In many real-world settings, machine learning models and interactive systems\nhave access to both structured knowledge, e.g., knowledge graphs or tables, and\nunstructured content, e.g., natural language documents. However, most rely on\neither. Semi-Structured Knowledge Bases (SKBs) bridge this gap by linking\nunstructured content to nodes within structured data, thereby enabling new\nstrategies for knowledge access and use. In this work, we present\nFocusedRetriever, a modular SKB-based framework for multi-hop question\nanswering. It integrates components (VSS-based entity search, LLM-based\ngeneration of Cypher queries and pairwise re-ranking) in a way that enables it\nto outperform state-of-the-art methods across all three STaRK benchmark test\nsets, covering diverse domains and multiple performance metrics. The average\nfirst-hit rate exceeds that of the second-best method by 25.7%.\nFocusedRetriever leverages (1) the capacity of Large Language Models (LLMs) to\nextract relational facts and entity attributes from unstructured text, (2) node\nset joins to filter answer candidates based on these extracted triplets and\nconstraints, (3) vector similarity search to retrieve and rank relevant\nunstructured content, and (4) the contextual capabilities of LLMs to finally\nrank the top-k answers. For generality, we only incorporate base LLMs in\nFocusedRetriever in our evaluation. However, our analysis of intermediate\nresults highlights several opportunities for further upgrades including\nfinetuning. The source code is publicly available at\nhttps://github.com/kramerlab/FocusedRetriever .",
      "tldr_zh": "这篇论文介绍了 Semi-Structured Knowledge Bases (SKBs)，它将结构化知识（如知识图谱）和非结构化内容（如自然语言文档）连接起来，以提升问题回答效率。作者提出 FocusedRetriever 框架，该框架通过整合 VSS-based entity search、LLM-based generation of Cypher queries 和 pairwise re-ranking 等组件，实现多跳问题回答。实验结果显示，FocusedRetriever 在 STaRK 基准测试集上超越最先进方法，平均首次命中率提高 25.7%。此外，该框架利用 Large Language Models (LLMs) 提取事实和排名答案，并提供公开源代码以支持进一步优化如微调。",
      "categories": [
        "cs.IR",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.IR",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2505.09246v1",
      "published_date": "2025-05-14 09:35:56 UTC",
      "updated_date": "2025-05-14 09:35:56 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T23:01:25.225236"
    },
    {
      "arxiv_id": "2505.09208v1",
      "title": "Educational impacts of generative artificial intelligence on learning and performance of engineering students in China",
      "title_zh": "生成式人工智能对中国工程学生学习和表现的教育影响",
      "authors": [
        "Lei Fan",
        "Kunyang Deng",
        "Fangxue Liu"
      ],
      "abstract": "With the rapid advancement of generative artificial intelligence(AI), its\npotential applications in higher education have attracted significant\nattention. This study investigated how 148 students from diverse engineering\ndisciplines and regions across China used generative AI, focusing on its impact\non their learning experience and the opportunities and challenges it poses in\nengineering education. Based on the surveyed data, we explored four key areas:\nthe frequency and application scenarios of AI use among engineering students,\nits impact on students' learning and performance, commonly encountered\nchallenges in using generative AI, and future prospects for its adoption in\nengineering education. The results showed that more than half of the\nparticipants reported a positive impact of generative AI on their learning\nefficiency, initiative, and creativity, with nearly half believing it also\nenhanced their independent thinking. However, despite acknowledging improved\nstudy efficiency, many felt their actual academic performance remained largely\nunchanged and expressed concerns about the accuracy and domain-specific\nreliability of generative AI. Our findings provide a first-hand insight into\nthe current benefits and challenges generative AI brings to students,\nparticularly Chinese engineering students, while offering several\nrecommendations, especially from the students' perspective, for effectively\nintegrating generative AI into engineering education.",
      "tldr_zh": "本研究调查了生成式人工智能（generative AI）对中国工程学生的学习和表现的影响，针对148名来自不同学科和地区的学生，探讨了AI的使用频率、应用场景、积极影响以及面临的挑战。结果显示，超过一半的学生认为AI提升了学习效率、主动性和创造力，并有近半数认为它增强了独立思考；然而，许多学生表示实际学术表现未见显著改变，并担忧AI的准确性和领域特定可靠性。该研究提供了从学生视角出发的建议，以有效整合generative AI到工程教育中，助力高等教育的发展。",
      "categories": [
        "cs.HC",
        "cs.AI"
      ],
      "primary_category": "cs.HC",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2505.09208v1",
      "published_date": "2025-05-14 07:52:54 UTC",
      "updated_date": "2025-05-14 07:52:54 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T23:01:35.632653"
    },
    {
      "arxiv_id": "2505.09203v1",
      "title": "InvDesFlow-AL: Active Learning-based Workflow for Inverse Design of Functional Materials",
      "title_zh": "InvDesFlow-AL：基于主动学习的逆向设计功能材料工作流",
      "authors": [
        "Xiao-Qi Han",
        "Peng-Jie Guo",
        "Ze-Feng Gao",
        "Hao Sun",
        "Zhong-Yi Lu"
      ],
      "abstract": "Developing inverse design methods for functional materials with specific\nproperties is critical to advancing fields like renewable energy, catalysis,\nenergy storage, and carbon capture. Generative models based on diffusion\nprinciples can directly produce new materials that meet performance\nconstraints, thereby significantly accelerating the material design process.\nHowever, existing methods for generating and predicting crystal structures\noften remain limited by low success rates. In this work, we propose a novel\ninverse material design generative framework called InvDesFlow-AL, which is\nbased on active learning strategies. This framework can iteratively optimize\nthe material generation process to gradually guide it towards desired\nperformance characteristics. In terms of crystal structure prediction, the\nInvDesFlow-AL model achieves an RMSE of 0.0423 {\\AA}, representing an 32.96%\nimprovement in performance compared to exsisting generative models.\nAdditionally, InvDesFlow-AL has been successfully validated in the design of\nlow-formation-energy and low-Ehull materials. It can systematically generate\nmaterials with progressively lower formation energies while continuously\nexpanding the exploration across diverse chemical spaces. These results fully\ndemonstrate the effectiveness of the proposed active learning-driven generative\nmodel in accelerating material discovery and inverse design. To further prove\nthe effectiveness of this method, we took the search for BCS superconductors\nunder ambient pressure as an example explored by InvDesFlow-AL. As a result, we\nsuccessfully identified Li\\(_2\\)AuH\\(_6\\) as a conventional BCS superconductor\nwith an ultra-high transition temperature of 140 K. This discovery provides\nstrong empirical support for the application of inverse design in materials\nscience.",
      "tldr_zh": "本研究提出了一种基于 Active Learning 的工作流 InvDesFlow-AL，用于功能材料的逆向设计，旨在加速可再生能源、催化等领域的新材料开发。框架通过迭代优化材料生成过程，利用生成模型和主动学习策略，显著提高了晶体结构预测的准确性，实现 RMSE 0.0423 Å，比现有模型改善 32.96%。实验验证显示，InvDesFlow-AL 成功设计了低形成能材料，并发现了新型 BCS 超导体 Li₂AuH₆（转变温度达 140 K），为材料科学中的逆向设计提供了有效工具。",
      "categories": [
        "cond-mat.mtrl-sci",
        "cond-mat.supr-con",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cond-mat.mtrl-sci",
      "comment": "29 pages, 11 figures",
      "pdf_url": "http://arxiv.org/pdf/2505.09203v1",
      "published_date": "2025-05-14 07:29:06 UTC",
      "updated_date": "2025-05-14 07:29:06 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T23:01:48.312481"
    },
    {
      "arxiv_id": "2505.09168v1",
      "title": "DRRNet: Macro-Micro Feature Fusion and Dual Reverse Refinement for Camouflaged Object Detection",
      "title_zh": "翻译失败",
      "authors": [
        "Jianlin Sun",
        "Xiaolin Fang",
        "Juwei Guan",
        "Dongdong Gui",
        "Teqi Wang",
        "Tongxin Zhu"
      ],
      "abstract": "The core challenge in Camouflage Object Detection (COD) lies in the\nindistinguishable similarity between targets and backgrounds in terms of color,\ntexture, and shape. This causes existing methods to either lose edge details\n(such as hair-like fine structures) due to over-reliance on global semantic\ninformation or be disturbed by similar backgrounds (such as vegetation\npatterns) when relying solely on local features. We propose DRRNet, a\nfour-stage architecture characterized by a \"context-detail-fusion-refinement\"\npipeline to address these issues. Specifically, we introduce an Omni-Context\nFeature Extraction Module to capture global camouflage patterns and a Local\nDetail Extraction Module to supplement microstructural information for the\nfull-scene context module. We then design a module for forming dual\nrepresentations of scene understanding and structural awareness, which fuses\npanoramic features and local features across various scales. In the decoder, we\nalso introduce a reverse refinement module that leverages spatial edge priors\nand frequency-domain noise suppression to perform a two-stage inverse\nrefinement of the output. By applying two successive rounds of inverse\nrefinement, the model effectively suppresses background interference and\nenhances the continuity of object boundaries. Experimental results demonstrate\nthat DRRNet significantly outperforms state-of-the-art methods on benchmark\ndatasets. Our code is available at https://github.com/jerrySunning/DRRNet.",
      "tldr_zh": "本论文针对 Camouflaged Object Detection (COD) 中目标与背景在颜色、纹理和形状上的相似性问题，提出 DRRNet 框架，该框架采用四阶段“context-detail-fusion-refinement”管道，以解决边缘细节丢失和背景干扰。DRRNet 包括 Omni-Context Feature Extraction Module 用于捕获全局伪装模式、Local Detail Extraction Module 用于补充微结构信息，以及多尺度特征融合模块和 Dual Reverse Refinement 模块，通过空间边缘先验和频域噪声抑制进行两阶段逆向精炼。实验结果表明，该方法在基准数据集上显著优于最先进方法。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2505.09168v1",
      "published_date": "2025-05-14 06:03:53 UTC",
      "updated_date": "2025-05-14 06:03:53 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T23:01:59.495249"
    },
    {
      "arxiv_id": "2505.09166v1",
      "title": "An Initial Exploration of Default Images in Text-to-Image Generation",
      "title_zh": "在文本到图像生成中的默认图像初步探索",
      "authors": [
        "Hannu Simonen",
        "Atte Kiviniemi",
        "Jonas Oppenlaender"
      ],
      "abstract": "In the creative practice of text-to-image generation (TTI), images are\ngenerated from text prompts. However, TTI models are trained to always yield an\noutput, even if the prompt contains unknown terms. In this case, the model may\ngenerate what we call \"default images\": images that closely resemble each other\nacross many unrelated prompts. We argue studying default images is valuable for\ndesigning better solutions for TTI and prompt engineering. In this paper, we\nprovide the first investigation into default images on Midjourney, a popular\nimage generator. We describe our systematic approach to create input prompts\ntriggering default images, and present the results of our initial experiments\nand several small-scale ablation studies. We also report on a survey study\ninvestigating how default images affect user satisfaction. Our work lays the\nfoundation for understanding default images in TTI and highlights challenges\nand future research directions.",
      "tldr_zh": "本研究探讨了文本到图像生成(Text-to-Image Generation)中的“默认图像”(default images)现象，即模型在处理未知术语时生成相似图像的问题。作者首次对Midjourney进行系统调查，通过创建触发默认图像的提示进行实验和小规模剔除研究，并开展用户满意度调查。结果显示，默认图像会降低用户满意度，该工作为优化TTI系统和提示工程提供了基础，并指出了未来挑战和研究方向。",
      "categories": [
        "cs.HC",
        "cs.AI",
        "H.5.m; I.2.m"
      ],
      "primary_category": "cs.HC",
      "comment": "16 pages, 6 figures",
      "pdf_url": "http://arxiv.org/pdf/2505.09166v1",
      "published_date": "2025-05-14 05:59:23 UTC",
      "updated_date": "2025-05-14 05:59:23 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T23:02:11.298272"
    },
    {
      "arxiv_id": "2505.09160v1",
      "title": "A Multi-Task Foundation Model for Wireless Channel Representation Using Contrastive and Masked Autoencoder Learning",
      "title_zh": "翻译失败",
      "authors": [
        "Berkay Guler",
        "Giovanni Geraci",
        "Hamid Jafarkhani"
      ],
      "abstract": "Current applications of self-supervised learning to wireless channel\nrepresentation often borrow paradigms developed for text and image processing,\nwithout fully addressing the unique characteristics and constraints of wireless\ncommunications. Aiming to fill this gap, we first propose WiMAE (Wireless\nMasked Autoencoder), a transformer-based encoder-decoder foundation model\npretrained on a realistic open-source multi-antenna wireless channel dataset.\nBuilding upon this foundation, we develop ContraWiMAE, which enhances WiMAE by\nincorporating a contrastive learning objective alongside the reconstruction\ntask in a unified multi-task framework. By warm-starting from pretrained WiMAE\nweights and generating positive pairs via noise injection, the contrastive\ncomponent enables the model to capture both structural and discriminative\nfeatures, enhancing representation quality beyond what reconstruction alone can\nachieve. Through extensive evaluation on unseen scenarios, we demonstrate the\neffectiveness of both approaches across multiple downstream tasks, with\nContraWiMAE showing further improvements in linear separability and\nadaptability in diverse wireless environments. Comparative evaluations against\na state-of-the-art wireless channel foundation model confirm the superior\nperformance and data efficiency of our models, highlighting their potential as\npowerful baselines for future research in self-supervised wireless channel\nrepresentation learning.",
      "tldr_zh": "本文提出 WiMAE，一种基于 Transformer 的编码器-解码器基础模型，通过 Masked Autoencoder 学习在真实开源多天线无线通道数据集上预训练，以解决无线通信独特特性的自监督学习问题。进一步开发 ContraWiMAE，通过在统一的多任务框架中结合对比学习目标（如噪声注入生成正对），增强模型对结构和判别特征的捕捉能力。实验评估显示，ContraWiMAE 在未见场景的多下游任务中表现出优越的线性可分性和适应性，并与现有最先进模型相比，展现出更高的性能和数据效率，为无线通道表示学习提供强大基线。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.IT",
        "cs.NI",
        "eess.SP",
        "math.IT"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2505.09160v1",
      "published_date": "2025-05-14 05:45:22 UTC",
      "updated_date": "2025-05-14 05:45:22 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T23:02:23.605975"
    },
    {
      "arxiv_id": "2505.09142v1",
      "title": "ELIS: Efficient LLM Iterative Scheduling System with Response Length Predictor",
      "title_zh": "ELIS：高效的LLM迭代调度系统，带有响应长度预测器",
      "authors": [
        "Seungbeom Choi",
        "Jeonghoe Goo",
        "Eunjoo Jeon",
        "Mingyu Yang",
        "Minsung Jang"
      ],
      "abstract": "We propose ELIS, a serving system for Large Language Models (LLMs) featuring\nan Iterative Shortest Remaining Time First (ISRTF) scheduler designed to\nefficiently manage inference tasks with the shortest remaining tokens. Current\nLLM serving systems often employ a first-come-first-served scheduling strategy,\nwhich can lead to the \"head-of-line blocking\" problem. To overcome this\nlimitation, it is necessary to predict LLM inference times and apply a shortest\njob first scheduling strategy. However, due to the auto-regressive nature of\nLLMs, predicting the inference latency is challenging. ELIS addresses this\nchallenge by training a response length predictor for LLMs using the BGE model,\nan encoder-based state-of-the-art model. Additionally, we have devised the\nISRTF scheduling strategy, an optimization of shortest remaining time first\ntailored to existing LLM iteration batching. To evaluate our work in an\nindustrial setting, we simulate streams of requests based on our study of\nreal-world user LLM serving trace records. Furthermore, we implemented ELIS as\na cloud-native scheduler system on Kubernetes to evaluate its performance in\nproduction environments. Our experimental results demonstrate that ISRTF\nreduces the average job completion time by up to 19.6%.",
      "tldr_zh": "我们提出了 ELIS，一种高效的 LLM 服务系统，采用 Iterative Shortest Remaining Time First (ISRTF) 调度器来管理推理任务，解决传统先到先服务策略的 \"head-of-line blocking\" 问题。\nELIS 通过训练基于 BGE 模型的响应长度预测器来准确预测 LLM 的自回归推理时间，并优化 ISRTF 策略以适应现有的迭代批处理机制。\n实验在模拟的真实用户请求流和 Kubernetes 云原生环境中进行，结果显示 ISRTF 将平均作业完成时间减少多达 19.6%。",
      "categories": [
        "cs.DC",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.DC",
      "comment": "13 pages, 5 figures. Cloud-native LLM scheduling system with\n  latency-aware inference optimization",
      "pdf_url": "http://arxiv.org/pdf/2505.09142v1",
      "published_date": "2025-05-14 04:50:00 UTC",
      "updated_date": "2025-05-14 04:50:00 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T23:02:36.094750"
    },
    {
      "arxiv_id": "2505.09131v1",
      "title": "Fair Clustering via Alignment",
      "title_zh": "翻译失败",
      "authors": [
        "Kunwoong Kim",
        "Jihu Lee",
        "Sangchul Park",
        "Yongdai Kim"
      ],
      "abstract": "Algorithmic fairness in clustering aims to balance the proportions of\ninstances assigned to each cluster with respect to a given sensitive attribute.\nWhile recently developed fair clustering algorithms optimize clustering\nobjectives under specific fairness constraints, their inherent complexity or\napproximation often results in suboptimal clustering utility or numerical\ninstability in practice. To resolve these limitations, we propose a new fair\nclustering algorithm based on a novel decomposition of the fair K-means\nclustering objective function. The proposed algorithm, called Fair Clustering\nvia Alignment (FCA), operates by alternately (i) finding a joint probability\ndistribution to align the data from different protected groups, and (ii)\noptimizing cluster centers in the aligned space. A key advantage of FCA is that\nit theoretically guarantees approximately optimal clustering utility for any\ngiven fairness level without complex constraints, thereby enabling high-utility\nfair clustering in practice. Experiments show that FCA outperforms existing\nmethods by (i) attaining a superior trade-off between fairness level and\nclustering utility, and (ii) achieving near-perfect fairness without numerical\ninstability.",
      "tldr_zh": "本研究针对算法公平性在聚类中的挑战，提出了一种名为Fair Clustering via Alignment (FCA)的新算法，旨在平衡敏感属性在各聚类中的比例，同时避免现有方法带来的子优效用或数值不稳定性。FCA通过分解公平K-means目标函数，交替计算联合概率分布来对齐不同保护群体的数据，并在对齐空间中优化聚类中心，从而在不引入复杂约束的情况下保证近似最优的聚类效用。实验结果显示，FCA在公平水平与聚类效用之间实现了优越的权衡，并能实现近乎完美的公平性，而无数值不稳定问题。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "stat.ML"
      ],
      "primary_category": "cs.LG",
      "comment": "Accepted at ICML 2025. This is the version submitted for review and\n  will be replaced by the camera-ready version soon",
      "pdf_url": "http://arxiv.org/pdf/2505.09131v1",
      "published_date": "2025-05-14 04:29:09 UTC",
      "updated_date": "2025-05-14 04:29:09 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T23:02:46.423229"
    },
    {
      "arxiv_id": "2505.09129v1",
      "title": "WSCIF: A Weakly-Supervised Color Intelligence Framework for Tactical Anomaly Detection in Surveillance Keyframes",
      "title_zh": "W",
      "authors": [
        "Wei Meng"
      ],
      "abstract": "The deployment of traditional deep learning models in high-risk security\ntasks in an unlabeled, data-non-exploitable video intelligence environment\nfaces significant challenges. In this paper, we propose a lightweight anomaly\ndetection framework based on color features for surveillance video clips in a\nhigh sensitivity tactical mission, aiming to quickly identify and interpret\npotential threat events under resource-constrained and data-sensitive\nconditions. The method fuses unsupervised KMeans clustering with RGB channel\nhistogram modeling to achieve composite detection of structural anomalies and\ncolor mutation signals in key frames. The experiment takes an operation\nsurveillance video occurring in an African country as a research sample, and\nsuccessfully identifies multiple highly anomalous frames related to high-energy\nlight sources, target presence, and reflective interference under the condition\nof no access to the original data. The results show that this method can be\neffectively used for tactical assassination warning, suspicious object\nscreening and environmental drastic change monitoring with strong deployability\nand tactical interpretation value. The study emphasizes the importance of color\nfeatures as low semantic battlefield signal carriers, and its battlefield\nintelligent perception capability will be further extended by combining graph\nneural networks and temporal modeling in the future.",
      "tldr_zh": "本文提出 WSCIF 框架，这是一种弱监督的颜色智能框架，用于在无标签监控视频中进行战术异常检测，旨在资源受限和数据敏感环境下快速识别潜在威胁事件。框架通过融合无监督 KMeans 聚类和 RGB 通道直方图建模，来检测关键帧中的结构异常和颜色突变信号。实验在非洲国家的一个操作监控视频上验证，成功识别与高能量光源、目标存在和反射干扰相关的异常帧，并证明其在战术暗杀警告、可疑物体筛选和环境剧变监控中的强部署性和解释价值。该研究突显了颜色特征作为低语义战场信号载体的作用，并建议未来结合图神经网络和时间建模进一步扩展其感知能力。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "es: 68T10, 68T05, 62H35, 68U10",
        "I.4.9; I.5.1; I.2.10"
      ],
      "primary_category": "cs.CV",
      "comment": "17 pages, 3 figures, 3 tables. The paper proposes a lightweight\n  weakly-supervised color intelligence model for tactical video anomaly\n  detection, tested on anonymized African surveillance data",
      "pdf_url": "http://arxiv.org/pdf/2505.09129v1",
      "published_date": "2025-05-14 04:24:37 UTC",
      "updated_date": "2025-05-14 04:24:37 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T23:03:01.952250"
    },
    {
      "arxiv_id": "2505.09115v1",
      "title": "PreCare: Designing AI Assistants for Advance Care Planning (ACP) to Enhance Personal Value Exploration, Patient Knowledge, and Decisional Confidence",
      "title_zh": "翻译失败",
      "authors": [
        "Yu Lun Hsu",
        "Yun-Rung Chou",
        "Chiao-Ju Chang",
        "Yu-Cheng Chang",
        "Zer-Wei Lee",
        "Rokas Gipiškis",
        "Rachel Li",
        "Chih-Yuan Shih",
        "Jen-Kuei Peng",
        "Hsien-Liang Huang",
        "Jaw-Shiun Tsai",
        "Mike Y. Chen"
      ],
      "abstract": "Advance Care Planning (ACP) allows individuals to specify their preferred\nend-of-life life-sustaining treatments before they become incapacitated by\ninjury or terminal illness (e.g., coma, cancer, dementia). While online ACP\noffers high accessibility, it lacks key benefits of clinical consultations,\nincluding personalized value exploration, immediate clarification of decision\nconsequences. To bridge this gap, we conducted two formative studies: 1)\nshadowed and interviewed 3 ACP teams consisting of physicians, nurses, and\nsocial workers (18 patients total), and 2) interviewed 14 users of ACP\nwebsites. Building on these insights, we designed PreCare in collaboration with\n6 ACP professionals. PreCare is a website with 3 AI-driven assistants designed\nto guide users through exploring personal values, gaining ACP knowledge, and\nsupporting informed decision-making. A usability study (n=12) showed that\nPreCare achieved a System Usability Scale (SUS) rating of excellent. A\ncomparative evaluation (n=12) showed that PreCare's AI assistants significantly\nimproved exploration of personal values, knowledge, and decisional confidence,\nand was preferred by 92% of participants.",
      "tldr_zh": "该研究针对Advance Care Planning (ACP) 的在线工具缺乏个性化价值探索和决策支持的问题，设计了PreCare 网站及其3个AI-driven assistants，以帮助用户探索个人价值、获取ACP知识并提升决策信心。研究者通过两个形成性研究（采访ACP团队和用户）并与6名专业人士合作，开发了这一系统。实验结果显示，PreCare在可用性测试中获得优秀的System Usability Scale (SUS)评分，并在比较评估中显著提高了用户的个人价值探索、知识水平和决策信心，92%的参与者更倾向于使用它。",
      "categories": [
        "cs.HC",
        "cs.AI"
      ],
      "primary_category": "cs.HC",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2505.09115v1",
      "published_date": "2025-05-14 03:53:35 UTC",
      "updated_date": "2025-05-14 03:53:35 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T23:03:13.618107"
    },
    {
      "arxiv_id": "2505.09114v1",
      "title": "Beyond the Known: Decision Making with Counterfactual Reasoning Decision Transformer",
      "title_zh": "翻译失败",
      "authors": [
        "Minh Hoang Nguyen",
        "Linh Le Pham Van",
        "Thommen George Karimpanal",
        "Sunil Gupta",
        "Hung Le"
      ],
      "abstract": "Decision Transformers (DT) play a crucial role in modern reinforcement\nlearning, leveraging offline datasets to achieve impressive results across\nvarious domains. However, DT requires high-quality, comprehensive data to\nperform optimally. In real-world applications, the lack of training data and\nthe scarcity of optimal behaviours make training on offline datasets\nchallenging, as suboptimal data can hinder performance. To address this, we\npropose the Counterfactual Reasoning Decision Transformer (CRDT), a novel\nframework inspired by counterfactual reasoning. CRDT enhances DT ability to\nreason beyond known data by generating and utilizing counterfactual\nexperiences, enabling improved decision-making in unseen scenarios. Experiments\nacross Atari and D4RL benchmarks, including scenarios with limited data and\naltered dynamics, demonstrate that CRDT outperforms conventional DT approaches.\nAdditionally, reasoning counterfactually allows the DT agent to obtain\nstitching abilities, combining suboptimal trajectories, without architectural\nmodifications. These results highlight the potential of counterfactual\nreasoning to enhance reinforcement learning agents' performance and\ngeneralization capabilities.",
      "tldr_zh": "本研究针对 Decision Transformers (DT) 在强化学习中依赖高质量离线数据集的问题，提出了一种新型框架 Counterfactual Reasoning Decision Transformer (CRDT)，通过生成和利用反事实经验来提升代理在未知场景下的决策能力。CRDT 借鉴反事实推理机制，帮助 DT 超越现有数据限制，并在 Atari 和 D4RL 基准测试中表现出色，尤其在数据稀缺或动态变化的环境下，比传统 DT 提升了性能。实验结果还显示，CRDT 赋予代理拼接次优轨迹的能力，而无需修改架构，从而增强了强化学习代理的泛化和鲁棒性。",
      "categories": [
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2505.09114v1",
      "published_date": "2025-05-14 03:45:16 UTC",
      "updated_date": "2025-05-14 03:45:16 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T23:03:25.478000"
    },
    {
      "arxiv_id": "2505.09108v1",
      "title": "Air-Ground Collaboration for Language-Specified Missions in Unknown Environments",
      "title_zh": "翻译失败",
      "authors": [
        "Fernando Cladera",
        "Zachary Ravichandran",
        "Jason Hughes",
        "Varun Murali",
        "Carlos Nieto-Granda",
        "M. Ani Hsieh",
        "George J. Pappas",
        "Camillo J. Taylor",
        "Vijay Kumar"
      ],
      "abstract": "As autonomous robotic systems become increasingly mature, users will want to\nspecify missions at the level of intent rather than in low-level detail.\nLanguage is an expressive and intuitive medium for such mission specification.\nHowever, realizing language-guided robotic teams requires overcoming\nsignificant technical hurdles. Interpreting and realizing language-specified\nmissions requires advanced semantic reasoning. Successful heterogeneous robots\nmust effectively coordinate actions and share information across varying\nviewpoints. Additionally, communication between robots is typically\nintermittent, necessitating robust strategies that leverage communication\nopportunities to maintain coordination and achieve mission objectives. In this\nwork, we present a first-of-its-kind system where an unmanned aerial vehicle\n(UAV) and an unmanned ground vehicle (UGV) are able to collaboratively\naccomplish missions specified in natural language while reacting to changes in\nspecification on the fly. We leverage a Large Language Model (LLM)-enabled\nplanner to reason over semantic-metric maps that are built online and\nopportunistically shared between an aerial and a ground robot. We consider\ntask-driven navigation in urban and rural areas. Our system must infer\nmission-relevant semantics and actively acquire information via semantic\nmapping. In both ground and air-ground teaming experiments, we demonstrate our\nsystem on seven different natural-language specifications at up to\nkilometer-scale navigation.",
      "tldr_zh": "本文提出一个创新系统，实现无人机(UAV)和无人地面车辆(UGV)在未知环境中的空地协作，允许用户通过自然语言指定任务意图，并实时响应任务变化。该系统利用LLM启用的规划器，对在线构建的语义-度量地图进行语义推理和机会性共享，确保异构机器人有效协调和信息交换。在城市和农村地区的实验中，该系统成功处理七种自然语言任务规格，实现高达公里级的任务驱动导航，并证明了其在间歇性通信下的鲁棒性。",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "comment": "19 pages, 24 figures, 7 tables. Submitted to T-FR",
      "pdf_url": "http://arxiv.org/pdf/2505.09108v1",
      "published_date": "2025-05-14 03:33:46 UTC",
      "updated_date": "2025-05-14 03:33:46 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T23:03:37.457839"
    },
    {
      "arxiv_id": "2505.09091v1",
      "title": "DPN-GAN: Inducing Periodic Activations in Generative Adversarial Networks for High-Fidelity Audio Synthesis",
      "title_zh": "翻译失败",
      "authors": [
        "Zeeshan Ahmad",
        "Shudi Bao",
        "Meng Chen"
      ],
      "abstract": "In recent years, generative adversarial networks (GANs) have made significant\nprogress in generating audio sequences. However, these models typically rely on\nbandwidth-limited mel-spectrograms, which constrain the resolution of generated\naudio sequences, and lead to mode collapse during conditional generation. To\naddress this issue, we propose Deformable Periodic Network based GAN (DPN-GAN),\na novel GAN architecture that incorporates a kernel-based periodic ReLU\nactivation function to induce periodic bias in audio generation. This\ninnovative approach enhances the model's ability to capture and reproduce\nintricate audio patterns. In particular, our proposed model features a DPN\nmodule for multi-resolution generation utilizing deformable convolution\noperations, allowing for adaptive receptive fields that improve the quality and\nfidelity of the synthetic audio. Additionally, we enhance the discriminator\nnetwork using deformable convolution to better distinguish between real and\ngenerated samples, further refining the audio quality. We trained two versions\nof the model: DPN-GAN small (38.67M parameters) and DPN-GAN large (124M\nparameters). For evaluation, we use five different datasets, covering both\nspeech synthesis and music generation tasks, to demonstrate the efficiency of\nthe DPN-GAN. The experimental results demonstrate that DPN-GAN delivers\nsuperior performance on both out-of-distribution and noisy data, showcasing its\nrobustness and adaptability. Trained across various datasets, DPN-GAN\noutperforms state-of-the-art GAN architectures on standard evaluation metrics,\nand exhibits increased robustness in synthesized audio.",
      "tldr_zh": "该论文提出 DPN-GAN，一种新型 GAN 架构，通过引入基于内核的周期性 ReLU 激活函数来诱导周期性偏差，解决传统 GAN 在音频生成中依赖带宽有限的 mel-spectrograms 导致的分辨率低下和模式崩溃问题。DPN 模块利用可变形卷积操作实现多分辨率生成，并增强鉴别器网络以更好地区分真实和合成样本，从而提升音频的保真度和质量。实验结果显示，DPN-GAN 在五个数据集（涵盖语音合成和音乐生成）上超越现有 GAN 模型，在标准评估指标中表现出色，并具备更高的鲁棒性，尤其在分布外和噪声数据上。",
      "categories": [
        "cs.SD",
        "cs.AI",
        "cs.CV",
        "cs.LG",
        "eess.AS"
      ],
      "primary_category": "cs.SD",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2505.09091v1",
      "published_date": "2025-05-14 02:52:16 UTC",
      "updated_date": "2025-05-14 02:52:16 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T23:03:50.259867"
    },
    {
      "arxiv_id": "2505.09085v1",
      "title": "Human-like Cognitive Generalization for Large Models via Brain-in-the-loop Supervision",
      "title_zh": "通过脑在环监督实现大型模型的类人认知泛化",
      "authors": [
        "Jiaxuan Chen",
        "Yu Qi",
        "Yueming Wang",
        "Gang Pan"
      ],
      "abstract": "Recent advancements in deep neural networks (DNNs), particularly large-scale\nlanguage models, have demonstrated remarkable capabilities in image and natural\nlanguage understanding. Although scaling up model parameters with increasing\nvolume of training data has progressively improved DNN capabilities, achieving\ncomplex cognitive abilities - such as understanding abstract concepts,\nreasoning, and adapting to novel scenarios, which are intrinsic to human\ncognition - remains a major challenge. In this study, we show that\nbrain-in-the-loop supervised learning, utilizing a small set of brain signals,\ncan effectively transfer human conceptual structures to DNNs, significantly\nenhancing their comprehension of abstract and even unseen concepts.\nExperimental results further indicate that the enhanced cognitive capabilities\nlead to substantial performance gains in challenging tasks, including\nfew-shot/zero-shot learning and out-of-distribution recognition, while also\nyielding highly interpretable concept representations. These findings highlight\nthat human-in-the-loop supervision can effectively augment the complex\ncognitive abilities of large models, offering a promising pathway toward\ndeveloping more human-like cognitive abilities in artificial systems.",
      "tldr_zh": "该研究探讨了如何通过 brain-in-the-loop supervised learning 提升深度神经网络（DNNs）的认知能力，利用少量脑信号将人类概念结构转移到大型模型中，从而增强对抽象概念和未见事物的理解。实验结果表明，这种方法显著提高了模型在 few-shot/zero-shot learning 和 out-of-distribution recognition 等任务上的性能，同时使概念表示更具可解释性。这些发现证明，人类在循环监督可以有效增强大型模型的复杂认知能力，为开发更像人类的 AI 系统提供了一个有前景的路径。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2505.09085v1",
      "published_date": "2025-05-14 02:39:10 UTC",
      "updated_date": "2025-05-14 02:39:10 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T23:04:02.955067"
    },
    {
      "arxiv_id": "2505.09082v1",
      "title": "CEC-Zero: Chinese Error Correction Solution Based on LLM",
      "title_zh": "CEC-Zero：基于LLM的中文错误修正解决方案",
      "authors": [
        "Sophie Zhang",
        "Zhiming Lin"
      ],
      "abstract": "Recent advancements in large language models (LLMs) demonstrate exceptional\nChinese text processing capabilities, particularly in Chinese Spelling\nCorrection (CSC). While LLMs outperform traditional BERT-based models in\naccuracy and robustness, challenges persist in reliability and generalization.\nThis paper proposes CEC-Zero, a novel reinforcement learning (RL) framework\nenabling LLMs to self-correct through autonomous error strategy learning\nwithout external supervision. By integrating RL with LLMs' generative power,\nthe method eliminates dependency on annotated data or auxiliary models.\nExperiments reveal RL-enhanced LLMs achieve industry-viable accuracy and\nsuperior cross-domain generalization, offering a scalable solution for\nreliability optimization in Chinese NLP applications. This breakthrough\nfacilitates LLM deployment in practical Chinese text correction scenarios while\nestablishing a new paradigm for self-improving language models.",
      "tldr_zh": "该论文提出 CEC-Zero，一种基于大型语言模型 (LLMs) 的中文错误修正解决方案，通过强化学习 (RL) 框架让模型自主学习错误策略，实现自我修正，而无需外部监督或标注数据。相比传统 BERT 模型，CEC-Zero 显著提升了 LLMs 在中文拼写修正 (CSC) 的准确性和鲁棒性，并在实验中展示了行业级别的准确率和优越的跨域泛化能力。该方法为 LLMs 在实际中文 NLP 应用中的部署提供了可扩展的可靠优化路径，并建立了语言模型自我改进的新范式。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2505.09082v1",
      "published_date": "2025-05-14 02:35:47 UTC",
      "updated_date": "2025-05-14 02:35:47 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T23:04:13.768535"
    },
    {
      "arxiv_id": "2505.09081v1",
      "title": "SALM: A Multi-Agent Framework for Language Model-Driven Social Network Simulation",
      "title_zh": "SALM：多智能体框架，用于语言模型驱动的社交网络模拟",
      "authors": [
        "Gaurav Koley"
      ],
      "abstract": "Contemporary approaches to agent-based modeling (ABM) of social systems have\ntraditionally emphasized rule-based behaviors, limiting their ability to\ncapture nuanced dynamics by moving beyond predefined rules and leveraging\ncontextual understanding from LMs of human social interaction. This paper\npresents SALM (Social Agent LM Framework), a novel approach for integrating\nlanguage models (LMs) into social network simulation that achieves\nunprecedented temporal stability in multi-agent scenarios. Our primary\ncontributions include: (1) a hierarchical prompting architecture enabling\nstable simulation beyond 4,000 timesteps while reducing token usage by 73%, (2)\nan attention-based memory system achieving 80% cache hit rates (95% CI [78%,\n82%]) with sub-linear memory growth of 9.5%, and (3) formal bounds on\npersonality stability. Through extensive validation against SNAP ego networks,\nwe demonstrate the first LLM-based framework capable of modeling long-term\nsocial phenomena while maintaining empirically validated behavioral fidelity.",
      "tldr_zh": "本论文提出 SALM 框架，这是一种基于语言模型 (LMs) 的多智能体框架，用于驱动社会网络模拟，克服传统规则-based 建模的局限性，实现更 nuanced 的社会互动动态。SALM 的主要贡献包括：分层提示架构，支持超过 4,000 个时间步的稳定模拟并减少 73% 的令牌使用；基于注意力的记忆系统，达到 80% 的缓存命中率（95% CI [78%, 82%]）和 9.5% 的子线性内存增长；以及对个性稳定的正式界限。通过在 SNAP ego networks 上的广泛验证，该框架首次展示了 LLM-based 方法在长期社会现象建模中的能力，同时保持了行为的实证忠诚度。",
      "categories": [
        "cs.SI",
        "cs.AI",
        "cs.MA"
      ],
      "primary_category": "cs.SI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2505.09081v1",
      "published_date": "2025-05-14 02:29:46 UTC",
      "updated_date": "2025-05-14 02:29:46 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T23:04:27.636421"
    },
    {
      "arxiv_id": "2505.09062v1",
      "title": "Variational Prefix Tuning for Diverse and Accurate Code Summarization Using Pre-trained Language Models",
      "title_zh": "Variational Prefix Tuning：用于多样且准确的代码摘要生成，使用预训练语言模型",
      "authors": [
        "Junda Zhao",
        "Yuliang Song",
        "Eldan Cohen"
      ],
      "abstract": "Recent advancements in source code summarization have leveraged\ntransformer-based pre-trained models, including Large Language Models of Code\n(LLMCs), to automate and improve the generation of code summaries. However,\nexisting methods often focus on generating a single high-quality summary for a\ngiven source code, neglecting scenarios where the generated summary might be\ninadequate and alternative options are needed. In this paper, we introduce\nVariational Prefix Tuning (VPT), a novel approach that enhances pre-trained\nmodels' ability to generate diverse yet accurate sets of summaries, allowing\nthe user to choose the most suitable one for the given source code. Our method\nintegrates a Conditional Variational Autoencoder (CVAE) framework as a modular\ncomponent into pre-trained models, enabling us to model the distribution of\nobserved target summaries and sample continuous embeddings to be used as\nprefixes to steer the generation of diverse outputs during decoding.\nImportantly, we construct our method in a parameter-efficient manner,\neliminating the need for expensive model retraining, especially when using\nLLMCs. Furthermore, we employ a bi-criteria reranking method to select a subset\nof generated summaries, optimizing both the diversity and the accuracy of the\noptions presented to users. We present extensive experimental evaluations using\nwidely used datasets and current state-of-the-art pre-trained code\nsummarization models to demonstrate the effectiveness of our approach and its\nadaptability across models.",
      "tldr_zh": "这篇论文提出 Variational Prefix Tuning (VPT)，一种新方法，用于提升预训练语言模型（如 LLMCs）在代码总结任务中生成多样且准确的摘要集，解决现有方法仅产生单一摘要的局限性。VPT 通过整合 Conditional Variational Autoencoder (CVAE) 框架，建模目标摘要的分布并采样连续嵌入作为前缀来引导生成过程，同时采用 bi-criteria reranking 方法优化生成的摘要子集，确保多样性和准确性。该方法参数高效，无需昂贵的模型重新训练，并在广泛实验中证明了其有效性和跨模型适应性。",
      "categories": [
        "cs.SE",
        "cs.AI",
        "cs.LG",
        "D.2.7"
      ],
      "primary_category": "cs.SE",
      "comment": "Accepted by the Journal of Systems and Software",
      "pdf_url": "http://arxiv.org/pdf/2505.09062v1",
      "published_date": "2025-05-14 01:46:56 UTC",
      "updated_date": "2025-05-14 01:46:56 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T23:04:37.631348"
    },
    {
      "arxiv_id": "2505.09040v1",
      "title": "RT-cache: Efficient Robot Trajectory Retrieval System",
      "title_zh": "RT-cache：高效的机器人轨迹检索系统",
      "authors": [
        "Owen Kwon",
        "Abraham George",
        "Alison Bartsch",
        "Amir Barati Farimani"
      ],
      "abstract": "This paper introduces RT-cache, a novel trajectorymemory pipeline that\naccelerates real-world robot inference by leveraging big-data retrieval and\nlearning from experience. While modern Vision-Language-Action (VLA) models can\nhandle diverse robotic tasks, they often incur high per-step inference costs,\nresulting in significant latency, sometimes minutes per task. In contrast,\nRT-cache stores a large-scale Memory of previously successful robot\ntrajectories and retrieves relevant multistep motion snippets, drastically\nreducing inference overhead. By integrating a Memory Builder with a Trajectory\nRetrieval, we develop an efficient retrieval process that remains tractable\neven for extremely large datasets. RT-cache flexibly accumulates real-world\nexperiences and replays them whenever the current scene matches past states,\nadapting quickly to new or unseen environments with only a few additional\nsamples. Experiments on the Open-X Embodiment Dataset and other real-world data\ndemonstrate that RT-cache completes tasks both faster and more successfully\nthan a baseline lacking retrieval, suggesting a practical, data-driven solution\nfor real-time manipulation.",
      "tldr_zh": "本研究提出 RT-cache，一种高效的机器人轨迹检索系统，通过大数据检索和从经验中学习来加速真实世界机器人推理，解决 Vision-Language-Action (VLA) 模型的高每步推理成本和延迟问题。RT-cache 存储大规模的成功轨迹记忆，并整合 Memory Builder 和 Trajectory Retrieval 模块，实现快速检索相关多步运动片段，即使在大型数据集上也能保持高效。该系统能灵活积累真实世界经验，并在当前场景匹配过去状态时重放轨迹，快速适应新环境。实验在 Open-X Embodiment Dataset 和其他真实数据上显示，RT-cache 比无检索基线更快且成功率更高，提供了一个实用的数据驱动实时操控解决方案。",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.CV",
        "cs.LG"
      ],
      "primary_category": "cs.RO",
      "comment": "9 pages, 5 figures. Submitted to an IEEE robotics conference",
      "pdf_url": "http://arxiv.org/pdf/2505.09040v1",
      "published_date": "2025-05-14 00:41:44 UTC",
      "updated_date": "2025-05-14 00:41:44 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T23:04:49.247929"
    },
    {
      "arxiv_id": "2505.10577v1",
      "title": "GRNN:Recurrent Neural Network based on Ghost Features for Video Super-Resolution",
      "title_zh": "GRNN",
      "authors": [
        "Yutong Guo"
      ],
      "abstract": "Modern video super-resolution (VSR) systems based on convolutional neural\nnetworks (CNNs) require huge computational costs. The problem of feature\nredundancy is present in most models in many domains, but is rarely discussed\nin VSR. We experimentally observe that many features in VSR models are also\nsimilar to each other, so we propose to use \"Ghost features\" to reduce this\nredundancy. We also analyze the so-called \"gradient disappearance\" phenomenon\ngenerated by the conventional recurrent convolutional network (RNN) model, and\ncombine the Ghost module with RNN to complete the modeling on time series. The\ncurrent frame is used as input to the model together with the next frame, the\noutput of the previous frame and the hidden state. Extensive experiments on\nseveral benchmark models and datasets show that the PSNR and SSIM of our\nproposed modality are improved to some extent. Some texture details in the\nvideo are also better preserved.",
      "tldr_zh": "该论文提出了一种基于 Ghost features 的循环神经网络（GRNN），旨在解决视频超分辨率（VSR）中的特征冗余问题，从而降低计算成本。GRNN 通过整合 Ghost 模块与 RNN 来处理时间序列建模，输入包括当前帧、下一帧、上一帧输出和隐藏状态，以缓解传统 RNN 的梯度消失现象。实验结果显示，在多个基准数据集上，GRNN 提升了 PSNR 和 SSIM 指标，并更好地保留了视频的纹理细节。",
      "categories": [
        "eess.IV",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "eess.IV",
      "comment": "Accepted by 2023 IEEE International Conference on Multimedia and Expo\n  (ICME 2023)",
      "pdf_url": "http://arxiv.org/pdf/2505.10577v1",
      "published_date": "2025-05-14 00:38:46 UTC",
      "updated_date": "2025-05-14 00:38:46 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T23:05:01.840280"
    }
  ],
  "raw_papers_fetched": true,
  "papers_count": 90,
  "processed_papers_count": 90,
  "failed_papers_count": 0,
  "summary_generated": true,
  "daily_data_saved": true,
  "last_update": "2025-05-24T23:05:24.338527"
}
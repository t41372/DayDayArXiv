{
  "date": "2025-05-14",
  "category": "cs.AI",
  "summary": "æ¬¢è¿æ¥åˆ° UTC æ—¶é—´ 2025-05-14 çš„ arXiv ä¸­æ–‡ TLDR å¿«æŠ¥ï¼\n\nğŸ‘‹ **ä¸€å¥è¯æ€»ç»“ï¼š**\nä»Šå¤©çš„ arXiv åƒæ˜¯ä¸€ä¸ªâ€œç¡¬æ ¸å·¥ç¨‹â€ä¸â€œç¤¾ä¼šåæ€â€çš„äº¤æ±‡ç‚¹ï¼šDeepSeek å›¢é˜Ÿæ·±åº¦æ­ç§˜äº† V3/R1 èƒŒåçš„ç¡¬ä»¶ååŒè®¾è®¡æŒ‘æˆ˜ï¼›ç†è®ºç•Œå°è¯•ç”¨è¿›åŒ–ç”Ÿç‰©å­¦è§£é‡Š Transformer çš„å­¦ä¹ æ¨¡å¼ï¼›è€Œç¤¾ä¼šè®¡ç®—é¢†åŸŸåˆ™å‘å‡ºäº†å…³äº AI èƒ½è€—æƒŠäººä»¥åŠæ— æ³•ç†è§£â€œé˜¿å°”æ³•ä¸–ä»£ï¼ˆGen Alphaï¼‰â€ç½‘ç»œé»‘è¯çš„è­¦å‘Šã€‚\n\nä¸‹é¢ä¸ºæ‚¨ç²¾é€‰äº†ä»Šå¤©çš„é‡ç£…è®ºæ–‡ï¼š\n\n---\n\n### ğŸš€ æ˜æ˜Ÿè®ºæ–‡ & å¤§æ¨¡å‹æ¶æ„ (Star Papers & Architecture)\n\n**1. DeepSeek-V3 æ·±åº¦æ­ç§˜ï¼šæ‰©å±•æŒ‘æˆ˜ä¸ç¡¬ä»¶åæ€**\n**# title:** Insights into DeepSeek-V3: Scaling Challenges and Reflections on Hardware for AI Architectures\n**# authors:** Chenggang Zhao, et al. (DeepSeek Team)\n**TLDR:** è¿™æ˜¯ä¸€ç¯‡å¿…è¯»çš„å·¥ä¸šç•ŒæŠ€æœ¯æŠ¥å‘Šã€‚DeepSeek å›¢é˜Ÿè¯¦ç»†å‰–æäº†åœ¨ 2048 å¼  H800 GPU ä¸Šè®­ç»ƒ **DeepSeek-V3/R1** æ—¶çš„åŸºç¡€è®¾æ–½æŒ‘æˆ˜ã€‚\n- **æ ¸å¿ƒè´¡çŒ®ï¼š** å±•ç¤ºäº†ç¡¬ä»¶-æ¨¡å‹ååŒè®¾è®¡ï¼ˆHardware-Aware Model Co-designï¼‰çš„é‡è¦æ€§ã€‚\n- **å…³é”®æŠ€æœ¯ï¼š** è¯¦ç»†è®¨è®ºäº† **Multi-head Latent Attention (MLA)** å¦‚ä½•ä¼˜åŒ–æ˜¾å­˜æ•ˆç‡ï¼Œ**Mixture of Experts (MoE)** å¦‚ä½•å¹³è¡¡è®¡ç®—ä¸é€šä¿¡ï¼Œä»¥åŠ **FP8 æ··åˆç²¾åº¦è®­ç»ƒ**å’Œ**å¤šå¹³é¢ç½‘ç»œæ‹“æ‰‘ï¼ˆMulti-Plane Network Topologyï¼‰**çš„åº”ç”¨ã€‚\n- **Implicationï¼š** æ–‡ç« è¿˜å‘ç¡¬ä»¶å‚å•†â€œå–Šè¯â€ï¼ŒæŒ‡å‡ºäº†æœªæ¥ç¡¬ä»¶æ¼”è¿›éœ€è¦çš„æ–¹å‘ï¼ˆå¦‚ä½ç²¾åº¦è®¡ç®—å•å…ƒã€Scale-up/out çš„èåˆï¼‰ã€‚\n\n**2. BLIP3-oï¼šå…¨å¼€æ”¾çš„ç»Ÿä¸€å¤šæ¨¡æ€æ¨¡å‹å®¶æ—**\n**# title:** BLIP3-o: A Family of Fully Open Unified Multimodal Models-Architecture, Training and Dataset\n**# authors:** Jiuhai Chen, et al. (Salesforce Research ç­‰)\n**TLDR:** Salesforce å›¢é˜Ÿå¸¦æ¥äº† BLIP ç³»åˆ—çš„æœ€æ–°ç»­ä½œ BLIP3-oã€‚\n- **æ ¸å¿ƒæ–¹æ³•ï¼š** ä»–ä»¬æ”¾å¼ƒäº†ä¼ ç»Ÿçš„ VAEï¼Œè½¬è€Œä½¿ç”¨ **Diffusion Transformer** æ¥ç”Ÿæˆå¯Œå«è¯­ä¹‰çš„ CLIP å›¾åƒç‰¹å¾ã€‚\n- **è®­ç»ƒç­–ç•¥ï¼š** æå‡ºäº†â€œå…ˆç†è§£ï¼Œåç”Ÿæˆâ€çš„é¡ºåºé¢„è®­ç»ƒç­–ç•¥ï¼Œé¿å…äº†èƒ½åŠ›çš„ç¾éš¾æ€§é—å¿˜ã€‚\n- **ç¦åˆ©ï¼š** å¼€æºäº†æ¨¡å‹æƒé‡ã€ä»£ç ä»¥åŠåŒ…å« 60k é«˜è´¨é‡æŒ‡ä»¤å¾®è°ƒæ•°æ®çš„ BLIP3-o-60k æ•°æ®é›†ã€‚\n\n**3. Flash-VL 2Bï¼šæè‡´é€Ÿåº¦çš„ç«¯ä¾§å¤šæ¨¡æ€æ¨¡å‹**\n**# title:** Flash-VL 2B: Optimizing Vision-Language Model Performance for Ultra-Low Latency and High Throughput\n**# authors:** Bo Zhang, et al.\n**TLDR:** é’ˆå¯¹å®æ—¶åº”ç”¨ä¼˜åŒ–çš„ VLMã€‚é€šè¿‡â€œéšå¼è¯­ä¹‰æ‹¼æ¥ï¼ˆimplicit semantic stitchingï¼‰â€å’Œ token å‹ç¼©ï¼Œåœ¨ä¿æŒç²¾åº¦çš„åŒæ—¶å®ç°äº†è¶…ä½å»¶è¿Ÿå’Œé«˜ååé‡ï¼Œé€‚åˆèµ„æºå—é™ç¯å¢ƒã€‚\n\n---\n\n### ğŸ§  ç†è®ºæœºç†ä¸è®¤çŸ¥ (Theory, Reasoning & Cognition)\n\n**4. è¿›åŒ–ç”Ÿç‰©å­¦è§†è§’ä¸‹çš„ Transformerï¼šæƒé‡å­¦ä¹  vs ä¸Šä¸‹æ–‡å­¦ä¹ **\n**# title:** Predictability Shapes Adaptation: An Evolutionary Perspective on Modes of Learning in Transformers\n**# authors:** Alexander Y. Ku, Thomas L. Griffiths, et al.\n**TLDR:** è¿™ç¯‡æ–‡ç« éå¸¸æœ‰è¶£ï¼Œå®ƒç”¨è¿›åŒ–ç”Ÿç‰©å­¦æ¥ç±»æ¯” Transformer çš„ä¸¤ç§å­¦ä¹ æ¨¡å¼ï¼š\n- **In-weights learning (IWL)** ç±»æ¯”ä¸ºâ€œåŸºå› ç¼–ç â€ï¼ˆGenetic Encodingï¼‰ï¼Œé€‚åˆç¯å¢ƒæå…¶ç¨³å®šçš„æƒ…å†µã€‚\n- **In-context learning (ICL)** ç±»æ¯”ä¸ºâ€œè¡¨å‹å¯å¡‘æ€§â€ï¼ˆPhenotypic Plasticityï¼‰ï¼Œé€‚åˆç¯å¢ƒæœ‰å¯é çº¿ç´¢ä½†å˜åŒ–çš„æƒ…å†µã€‚\n- **å‘ç°ï¼š** å®éªŒè¯æ˜ï¼Œç¯å¢ƒçš„â€œå¯é¢„æµ‹æ€§â€æ˜¯å†³å®šæ¨¡å‹åå‘ IWL è¿˜æ˜¯ ICL çš„å…³é”®å› ç´ ã€‚è¿™ä¸ºç†è§£ ICL çš„æ¶Œç°æä¾›äº†ç”Ÿç‰©è¿›åŒ–è®ºå±‚é¢çš„è§£é‡Šã€‚\n\n**5. çŸ©é˜µä¹˜æ³• $XX^t$ å¯ä»¥æ›´å¿«ï¼šç®—æ³•ä¸Šçš„çªç ´**\n**# title:** $XX^{t}$ Can Be Faster\n**# authors:** Dmitry Rybin, et al.\n**TLDR:** åŸºç¡€æ•°å­¦ä¸è®¡ç®—æœºç§‘å­¦çš„ç»“åˆã€‚ä½œè€…æå‡ºäº† **RXTX ç®—æ³•**ï¼Œé€šè¿‡ç»“åˆæœºå™¨å­¦ä¹ æœç´¢å’Œç»„åˆä¼˜åŒ–ï¼Œä½¿å¾—è®¡ç®—çŸ©é˜µåŠå…¶è½¬ç½®çš„ä¹˜ç§¯ï¼ˆ$XX^t$ï¼‰æ¯” SOTA ç®—æ³•å‡å°‘äº† 5% çš„ä¹˜æ³•å’Œæ“ä½œæ•°ã€‚è¿™ä¸ä»…é€‚ç”¨äºå¤§çŸ©é˜µï¼Œå¯¹å°çŸ©é˜µï¼ˆ$n=4$ï¼‰ä¹Ÿæœ‰æ•ˆï¼Œå¯èƒ½å¯¹åº•å±‚è®¡ç®—åº“äº§ç”Ÿå½±å“ã€‚\n\n**6. è¯­è¨€æ¨¡å‹ä¹Ÿæœ‰è®¤çŸ¥åå·®ï¼šBlicket æµ‹è¯•**\n**# title:** Language Agents Mirror Human Causal Reasoning Biases. How Can We Help Them Think Like Scientists?\n**# authors:** Anthony GX-Chen, et al.\n**TLDR:** ä½¿ç”¨å‘å±•å¿ƒç†å­¦ä¸­çš„ Blicket æµ‹è¯•å‘ç°ï¼ŒLLM å’Œäººç±»ä¸€æ ·å­˜åœ¨â€œæå–åå·®ï¼ˆdisjunctive biasï¼‰â€â€”â€”å®¹æ˜“æ¨æ–­å¸¸è§çš„å› æœå…³ç³»ï¼Œä½†éš¾ä»¥å‘ç°ç½•è§çš„â€œåˆå–ï¼ˆconjunctiveï¼‰â€å…³ç³»ã€‚ä½œè€…æå‡ºäº†ä¸€ç§æµ‹è¯•æ—¶é‡‡æ ·æ–¹æ³•æ¥çº æ­£è¿™ç§åå·®ï¼Œå¸®åŠ©æ¨¡å‹åƒç§‘å­¦å®¶ä¸€æ ·æ€è€ƒã€‚\n\n---\n\n### ğŸŒ ç¤¾ä¼šå½±å“ã€å®‰å…¨ä¸ä¼¦ç† (Society, Safety & Ethics)\n\n**7. AI æœ‰å¤šé¥¿ï¼ŸLLM æ¨ç†çš„èƒ½è€—ã€æ°´è€—ä¸ç¢³è¶³è¿¹**\n**# title:** How Hungry is AI? Benchmarking Energy, Water, and Carbon Footprint of LLM Inference\n**# authors:** Nidhal Jegham, et al.\n**TLDR:** ä¸€é¡¹ä»¤äººè­¦é†’çš„åŸºå‡†æµ‹è¯•ã€‚\n- **æ•°æ®ï¼š** æœ€è€—èƒ½çš„æ¨¡å‹å¤„ç†ä¸€ä¸ªé•¿ Prompt å¯èƒ½æ¶ˆè€— **29 Wh** ç”µé‡ã€‚\n- **å¯¹æ¯”ï¼š** å³ä¾¿æ˜¯çŸ­æŸ¥è¯¢ï¼Œå¦‚æœè¾¾åˆ° 7 äº¿æ¬¡/å¤©ï¼Œå…¶å¹´åº¦è€—ç”µé‡ç›¸å½“äº 3.5 ä¸‡ä¸ªç¾å›½å®¶åº­ï¼Œæ¶ˆè€—çš„æ·¡æ°´ç›¸å½“äº 120 ä¸‡äººçš„å¹´é¥®æ°´é‡ã€‚\n\n**8. AI å¬ä¸æ‡‚â€œ10åâ€çš„é»‘è¯ï¼šGen Alpha è¯­è¨€ä¸å†…å®¹å®¡æ ¸**\n**# title:** Understanding Gen Alpha Digital Language: Evaluation of LLM Safety Systems for Content Moderation\n**# authors:** Manisha Mehta, Fausto Giunchiglia\n**TLDR:** é˜¿å°”æ³•ä¸–ä»£ï¼ˆ2010å¹´åå‡ºç”Ÿï¼‰çš„æ•°å­—è¯­è¨€ï¼ˆå¦‚æºè‡ªæ¸¸æˆã€Meme çš„é»‘è¯ï¼‰æ­£åœ¨è®©ç°æœ‰çš„ AI å®‰å…¨ç³»ç»Ÿå¤±æ•ˆã€‚GPT-4ã€Claude ç­‰æ¨¡å‹åœ¨æ£€æµ‹ä¼ªè£…åœ¨ Gen Alpha é»‘è¯ä¸­çš„éªšæ‰°å’Œæ“çºµè¡Œä¸ºæ—¶å­˜åœ¨ä¸¥é‡çš„ç†è§£å¤±è´¥ã€‚è¿™æ­ç¤ºäº†å†…å®¹å®¡æ ¸ç³»ç»Ÿé¢ä¸´çš„ä»£é™…è¯­è¨€é¸¿æ²Ÿã€‚\n\n**9. LLM èƒ½é¢„æµ‹æˆ˜äº‰å—ï¼Ÿå‚æ•°åŒ– vs éå‚æ•°åŒ–çŸ¥è¯†**\n**# title:** Do Large Language Models Know Conflict? Investigating Parametric vs. Non-Parametric Knowledge of LLMs for Conflict Forecasting\n**# authors:** Apollinaire Poli Nemkova, et al.\n**TLDR:** è¯„ä¼° LLM é¢„æµ‹æš´åŠ›å†²çªçš„èƒ½åŠ›ã€‚ç ”ç©¶å‘ç°ï¼Œç»“åˆäº†å¤–éƒ¨ç»“æ„åŒ–æ•°æ®ï¼ˆRAGï¼‰çš„éå‚æ•°åŒ–æ–¹æ³•æ¯”ä»…ä¾é é¢„è®­ç»ƒæƒé‡ï¼ˆå‚æ•°åŒ–çŸ¥è¯†ï¼‰æ›´æœ‰æ•ˆï¼Œå°¤å…¶æ˜¯åœ¨é¢„æµ‹éæ´²ä¹‹è§’å’Œä¸­ä¸œåœ°åŒºçš„å†²çªå‡çº§è¶‹åŠ¿æ—¶ã€‚\n\n---\n\n### ğŸ¤– æ™ºèƒ½ä½“ä¸æœºå™¨äºº (Agents & Robotics)\n\n**10. æœºå™¨äººæ“ä½œçš„å¤§æ¨¡å‹åŸºå‡†æµ‹è¯•ï¼šManipBench**\n**# title:** ManipBench: Benchmarking Vision-Language Models for Low-Level Robot Manipulation\n**# authors:** Enyu Zhao, et al.\n**TLDR:** ç°æœ‰çš„ VLM å¤šç”¨äºé«˜å±‚è§„åˆ’ï¼Œä½†è¿™ç¯‡è®ºæ–‡æµ‹è¯•äº†å®ƒä»¬åœ¨â€œåº•å±‚æ§åˆ¶ï¼ˆLow-Levelï¼‰â€ä¸Šçš„èƒ½åŠ›ã€‚æµ‹è¯•äº† 33 ä¸ªæ¨¡å‹ï¼Œå‘ç° VLM åœ¨ç†è§£ç‰©ä½“äº¤äº’å’Œå¯å˜å½¢ç‰©ä½“æ“ä½œä¸Šï¼Œè·ç¦»äººç±»æ°´å¹³ä»æœ‰å·¨å¤§å·®è·ã€‚\n\n**11. å¤šæ™ºèƒ½ä½“æ¨¡æ‹Ÿä¸­çš„åˆä½œï¼šå¤ç°ä¸æ‰©å±•**\n**# title:** Reproducibility Study of \"Cooperate or Collapse: Emergence of Sustainable Cooperation in a Society of LLM Agents\"\n**# authors:** Pedro M. P. Curvo, et al.\n**TLDR:** å¯¹ GovSim æ¡†æ¶çš„å¤ç°ç ”ç©¶ã€‚ç¡®è®¤äº†**å¤§æ¨¡å‹ï¼ˆå¦‚ GPT-4ï¼‰åœ¨èµ„æºå…±äº«åœºæ™¯ä¸‹èƒ½å®ç°å¯æŒç»­åˆä½œ**ï¼Œè€Œå°æ¨¡å‹å¦‚æœä¸åŠ å¹²é¢„åˆ™ä¼šå´©æºƒã€‚æœ‰è¶£çš„æ˜¯ï¼Œåœ¨æ··åˆç¯å¢ƒä¸­ï¼Œé«˜èƒ½åŠ›çš„æ¨¡å‹å¯ä»¥â€œå¸¦åŠ¨â€ä½èƒ½åŠ›æ¨¡å‹è¡¨ç°å‡ºæ›´å¥½çš„åˆä½œè¡Œä¸ºã€‚\n\n**12. æœºå™¨äººè®°å¿†ï¼šRT-Cache**\n**# title:** RT-Cache: Training-Free Retrieval for Real-Time Manipulation\n**# authors:** Owen Kwon, et al.\n**TLDR:** æå‡ºäº†ä¸€ç§å…è®­ç»ƒçš„â€œæ£€ç´¢å³æ§åˆ¶â€ç®¡é“ã€‚é€šè¿‡å°†å›¾åƒ-åŠ¨ä½œè½¨è¿¹ç¼“å­˜åˆ°å‘é‡å†…å­˜ä¸­ï¼Œå®æ—¶æ£€ç´¢å¹¶å›æ”¾å¤šæ­¥ç‰‡æ®µã€‚è¿™ç§æ–¹æ³•åœ¨æ— éœ€å¾®è°ƒçš„æƒ…å†µä¸‹ï¼Œæ¯”å¼ºåŸºçº¿æ–¹æ³•çš„æˆåŠŸç‡é«˜å‡ºçº¦ 2 å€ï¼Œä¸”é€Ÿåº¦æ›´å¿«ã€‚\n\n---\n\n### ğŸ’¼ å·¥ä¸šç•Œåº”ç”¨ (Industrial Applications)\n\n**13. LinkedIn çš„æ•°æ®é©±åŠ¨å½’å› ç³»ç»Ÿ**\n**# title:** LiDDA: Data Driven Attribution at LinkedIn\n**# authors:** John Bencina, et al. (LinkedIn)\n**TLDR:** é¢†è‹±å…¬å¼€äº†å…¶å¤§è§„æ¨¡è¥é”€å½’å› ç³»ç»Ÿã€‚ä½¿ç”¨åŸºäº Transformer çš„ç»Ÿä¸€æ–¹æ³•ï¼Œç»“åˆä¼šå‘˜çº§æ•°æ®å’Œå®è§‚å› ç´ ï¼Œè§£å†³å¹¿å‘Šè½¬åŒ–å½’å› è¿™ä¸€è¥é”€é¢†åŸŸçš„â€œåœ£æ¯â€é—®é¢˜ã€‚\n\n**14. å•†ä¸š AI çš„å› æœé¢„æµ‹ä¼˜åŒ–**\n**# title:** Causal Predictive Optimization and Generation for Business AI\n**# authors:** Liyang Zhao, et al. (LinkedIn)\n**TLDR:** åŒæ ·æ¥è‡ªé¢†è‹±ã€‚ä»‹ç»äº†ä¸€å¥—ç”¨äº B2B é”€å”®ä¼˜åŒ–çš„ä¸‰å±‚æ¶æ„ï¼šå› æœ ML é¢„æµ‹å±‚ -> çº¦æŸä¼˜åŒ–ä¸ Contextual Bandit å±‚ -> ç”Ÿæˆå¼ AI æœåŠ¡å±‚ã€‚è¿™æ˜¯â€œå› æœæ¨æ–­ + ç”Ÿæˆå¼ AIâ€åœ¨å•†ä¸šè½åœ°çš„å…¸å‹æ¡ˆä¾‹ã€‚\n\n---\n\n### ğŸ¥ åŒ»ç–—ä¸ç§‘å­¦ (Medical & Science)\n\n**15. 2100ä¸‡å¼ å›¾çš„ç”Ÿç‰©åŒ»å­¦åŸºç¡€æ¨¡å‹**\n**# title:** BioVFM-21M: Benchmarking and Scaling Self-Supervised Vision Foundation Models for Biomedical Image Analysis\n**# authors:** Jiarun Liu, et al.\n**TLDR:** ä¸ºäº†è§£å†³åŒ»å­¦å›¾åƒæ•°æ®å¼‚æ„çš„é—®é¢˜ï¼Œä½œè€…å‘å¸ƒäº†åŒ…å« 2100 ä¸‡å¼ ç”Ÿç‰©åŒ»å­¦å›¾åƒçš„æ•°æ®é›† **BioVFM-21M** å’Œç›¸åº”çš„é¢„è®­ç»ƒæ¨¡å‹ã€‚åœ¨ 12 ä¸ªåŒ»ç–—åŸºå‡†æµ‹è¯•ä¸­è¶…è¶Šäº†ç°æœ‰çš„ SOTA åŸºç¡€æ¨¡å‹ã€‚\n\n**16. ç‰©ç†å¥¥èµ›ï¼šGPT-4o ä¸ o1-preview è¶…è¶Šäººç±»**\n**# title:** Evaluating GPT- and Reasoning-based Large Language Models on Physics Olympiad Problems...\n**# authors:** Paul Tschisgale, et al.\n**TLDR:** åœ¨å¾·å›½ç‰©ç†å¥¥æ—åŒ¹å…‹ç«èµ›é¢˜ç›®ä¸Šï¼ŒOpenAI çš„ **o1-preview** æ¨¡å‹å‡ ä¹ä¸€è‡´åœ°å‡»è´¥äº†äººç±»å‚èµ›è€…å’Œ GPT-4oã€‚è¿™è¡¨æ˜æ¨ç†ä¼˜åŒ–æ¨¡å‹ï¼ˆReasoning-based modelsï¼‰åœ¨å¤æ‚ç‰©ç†é—®é¢˜æ±‚è§£ä¸Šå·²å–å¾—å®è´¨æ€§çªç ´ã€‚\n\n---\n\n### ğŸ§© å…¶ä»–å€¼å¾—å…³æ³¨çš„è®ºæ–‡ (Quick Hits)\n\n*   **[Audio]** **DPN-GAN**: ä½¿ç”¨å¯å˜å½¢å‘¨æœŸç½‘ç»œï¼ˆDeformable Periodic Networkï¼‰æ”¹è¿› GANï¼Œç”Ÿæˆé«˜ä¿çœŸéŸ³é¢‘ï¼Œè§£å†³äº†æ¢…å°”è°±å›¾å¸¦å®½é™åˆ¶å¯¼è‡´çš„éŸ³è´¨é—®é¢˜ã€‚(Paper 88)\n*   **[Code]** **Variational Prefix Tuning**: ä¸ºä»£ç æ‘˜è¦ç”Ÿæˆå¼•å…¥å˜åˆ†å‰ç¼€å¾®è°ƒï¼Œè®©æ¨¡å‹èƒ½ç”Ÿæˆâ€œå¤šæ ·åŒ–â€ä¸”å‡†ç¡®çš„ä»£ç æ€»ç»“ï¼Œè€Œä¸æ˜¯å•ä¸€çš„ç­”æ¡ˆã€‚(Paper 92)\n*   **[Material Science]** **InvDesFlow-AL**: åŸºäºä¸»åŠ¨å­¦ä¹ å’Œæ‰©æ•£æ¨¡å‹çš„åå‘ææ–™è®¾è®¡æ¡†æ¶ï¼ŒæˆåŠŸå‘ç°äº†ä¸€ç§ Tc é«˜è¾¾ 140K çš„è¶…å¯¼ä½“ $Li_2AuH_6$ã€‚(Paper 78)\n*   **[Prompt Eng]** **System Prompt Optimization**: æå‡ºäº†åŒå±‚ç³»ç»Ÿæç¤ºè¯ä¼˜åŒ–ï¼ˆBilevel System Prompt Optimizationï¼‰ï¼Œé€šè¿‡å…ƒå­¦ä¹ ï¼ˆMeta-learningï¼‰è®© System Prompt èƒ½æ³›åŒ–é€‚åº”ä¸åŒçš„ç”¨æˆ·è¾“å…¥ã€‚(Paper 40)\n\nå¸Œæœ›è¿™ä»½å¿«æŠ¥èƒ½å¸®åŠ©ä½ æ•æ‰ä»Šæ—¥ arXiv çš„ç²¾åï¼æˆ‘ä»¬æ˜å¤©è§ï¼",
  "papers": [
    {
      "arxiv_id": "2505.09861v2",
      "title": "LiDDA: Data Driven Attribution at LinkedIn",
      "title_zh": "LiDDAï¼šLinkedIn çš„æ•°æ®é©±åŠ¨å½’å› å®è·µ",
      "authors": [
        "John Bencina",
        "Erkut Aykutlug",
        "Yue Chen",
        "Zerui Zhang",
        "Stephanie Sorenson",
        "Shao Tang",
        "Changshuai Wei"
      ],
      "abstract": "Data Driven Attribution, which assigns conversion credits to marketing interactions based on causal patterns learned from data, is the foundation of modern marketing intelligence and vital to any marketing businesses and advertising platform. In this paper, we introduce a unified transformer-based attribution approach that can handle member-level data, aggregate-level data, and integration of external macro factors. We detail the large scale implementation of the approach at LinkedIn, showcasing significant impact. We also share learning and insights that are broadly applicable to the marketing and ad tech fields.",
      "tldr_zh": "è¯¥ç ”ç©¶ä»‹ç»äº†LiDDAï¼Œä¸€ä¸ªåœ¨LinkedInå®ç°çš„æ•°æ®é©±åŠ¨å½’å› (Data Driven Attribution)ç³»ç»Ÿï¼Œæ—¨åœ¨é€šè¿‡ä»æ•°æ®ä¸­å­¦ä¹ å› æœæ¨¡å¼æ¥åˆ†é…è¥é”€äº’åŠ¨çš„è½¬åŒ–ä¿¡ç”¨ã€‚è¯¥ç³»ç»Ÿé‡‡ç”¨ä¸€ç§ç»Ÿä¸€çš„åŸºäºtransformerçš„å½’å› æ–¹æ³•ï¼Œèƒ½å¤ŸåŒæ—¶å¤„ç†æˆå‘˜çº§åˆ«(member-level)æ•°æ®ã€èšåˆçº§åˆ«(aggregate-level)æ•°æ®ï¼Œå¹¶æ•´åˆå¤–éƒ¨å®è§‚å› ç´ (macro factors)ã€‚è®ºæ–‡è¯¦ç»†æè¿°äº†è¯¥æ–¹æ³•åœ¨LinkedInçš„å¤§è§„æ¨¡å®æ–½è¿‡ç¨‹ï¼Œå¹¶å±•ç¤ºäº†å…¶åœ¨è¥é”€æ™ºèƒ½é¢†åŸŸäº§ç”Ÿçš„æ˜¾è‘—å½±å“ã€‚æ­¤å¤–ï¼Œç ”ç©¶è¿˜åˆ†äº«äº†å¯¹äºè¥é”€å’Œå¹¿å‘ŠæŠ€æœ¯(ad tech)é¢†åŸŸå…·æœ‰å¹¿æ³›é€‚ç”¨æ€§çš„ç»éªŒä¸è§è§£ã€‚è¿™äº›æˆæœä¸ºç°ä»£è¥é”€æ™ºèƒ½å’Œå¹¿å‘Šå¹³å°æä¾›äº†é‡è¦çš„æŠ€æœ¯æ”¯æ’‘ï¼Œå¹¶ä¸ºç›¸å…³é¢†åŸŸçš„ä»ä¸šè€…æä¾›äº†å¯å€Ÿé‰´çš„å®è·µæŒ‡å¯¼ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.IR",
        "stat.ME"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.09861v2",
      "published_date": "2025-05-14 23:54:57 UTC",
      "updated_date": "2025-05-21 22:19:59 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T08:29:49.275117+00:00"
    },
    {
      "arxiv_id": "2505.09855v1",
      "title": "Predictability Shapes Adaptation: An Evolutionary Perspective on Modes of Learning in Transformers",
      "title_zh": "å¯é¢„æµ‹æ€§å¡‘é€ é€‚åº”ï¼šTransformer å­¦ä¹ æ¨¡å¼çš„æ¼”åŒ–è§†è§’",
      "authors": [
        "Alexander Y. Ku",
        "Thomas L. Griffiths",
        "Stephanie C. Y. Chan"
      ],
      "abstract": "Transformer models learn in two distinct modes: in-weights learning (IWL), encoding knowledge into model weights, and in-context learning (ICL), adapting flexibly to context without weight modification. To better understand the interplay between these learning modes, we draw inspiration from evolutionary biology's analogous adaptive strategies: genetic encoding (akin to IWL, adapting over generations and fixed within an individual's lifetime) and phenotypic plasticity (akin to ICL, enabling flexible behavioral responses to environmental cues). In evolutionary biology, environmental predictability dictates the balance between these strategies: stability favors genetic encoding, while reliable predictive cues promote phenotypic plasticity. We experimentally operationalize these dimensions of predictability and systematically investigate their influence on the ICL/IWL balance in Transformers. Using regression and classification tasks, we show that high environmental stability decisively favors IWL, as predicted, with a sharp transition at maximal stability. Conversely, high cue reliability enhances ICL efficacy, particularly when stability is low. Furthermore, learning dynamics reveal task-contingent temporal evolution: while a canonical ICL-to-IWL shift occurs in some settings (e.g., classification with many classes), we demonstrate that scenarios with easier IWL (e.g., fewer classes) or slower ICL acquisition (e.g., regression) can exhibit an initial IWL phase later yielding to ICL dominance. These findings support a relative-cost hypothesis for explaining these learning mode transitions, establishing predictability as a critical factor governing adaptive strategies in Transformers, and offering novel insights for understanding ICL and guiding training methodologies.",
      "tldr_zh": "è¯¥ç ”ç©¶ä»è¿›åŒ–ç”Ÿç‰©å­¦çš„è§†è§’å‡ºå‘ï¼Œå°† Transformer æ¨¡å‹ä¸­çŸ¥è¯†å›ºåŒ–åˆ°æƒé‡ä¸­çš„ in-weights learning (IWL) ä¸çµæ´»é€‚åº”ä¸Šä¸‹æ–‡çš„ in-context learning (ICL) åˆ†åˆ«ç±»æ¯”ä¸ºé—ä¼ ç¼–ç ä¸è¡¨å‹å¯å¡‘æ€§ï¼Œç³»ç»Ÿæ¢è®¨äº†ç¯å¢ƒé¢„æµ‹æ€§ (predictability) å¯¹å­¦ä¹ æ¨¡å¼å¹³è¡¡çš„å½±å“ã€‚ç ”ç©¶é€šè¿‡å›å½’å’Œåˆ†ç±»ä»»åŠ¡å‘ç°ï¼Œé«˜ç¯å¢ƒç¨³å®šæ€§ (stability) æ˜¾è‘—åˆ©äº IWL çš„å‘ç”Ÿï¼Œè€Œé«˜çº¿ç´¢å¯é æ€§ (cue reliability) åˆ™åœ¨ä½ç¨³å®šæ€§ç¯å¢ƒä¸‹å¢å¼ºäº† ICL çš„æ•ˆèƒ½ã€‚æ­¤å¤–ï¼Œå®éªŒæ­ç¤ºäº†å­¦ä¹ åŠ¨åŠ›å­¦å…·æœ‰ä»»åŠ¡ä¾èµ–æ€§ï¼Œé™¤äº†å¸¸è§çš„ä» ICL å‘ IWL è½¬å˜å¤–ï¼Œåœ¨ç‰¹å®šåœºæ™¯ä¸‹ä¹Ÿä¼šå‡ºç°åˆæœŸç”± IWL ä¸»å¯¼éšåè½¬å‘ ICL å ä¼˜çš„ç°è±¡ã€‚è¿™äº›å‘ç°æ”¯æŒäº†ç”¨ä»¥è§£é‡Šå­¦ä¹ æ¨¡å¼è¿ç§»çš„ç›¸å¯¹æˆæœ¬å‡è®¾ (relative-cost hypothesis)ï¼Œè¯æ˜äº†é¢„æµ‹æ€§æ˜¯å†³å®š Transformer é€‚åº”ç­–ç•¥çš„æ ¸å¿ƒå› ç´ ã€‚è¯¥ç ”ç©¶ä¸ºç†è§£ ICL çš„è¿ä½œæœºåˆ¶å¹¶æŒ‡å¯¼æ¨¡å‹è®­ç»ƒæ–¹æ³•æä¾›äº†å…¨æ–°çš„ç†è®ºè§†è§’ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.09855v1",
      "published_date": "2025-05-14 23:31:17 UTC",
      "updated_date": "2025-05-14 23:31:17 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T08:30:06.808132+00:00"
    },
    {
      "arxiv_id": "2505.09852v1",
      "title": "Do Large Language Models Know Conflict? Investigating Parametric vs. Non-Parametric Knowledge of LLMs for Conflict Forecasting",
      "title_zh": "å¤§è¯­è¨€æ¨¡å‹æ˜¯å¦äº†è§£å†²çªï¼Ÿå¤§è¯­è¨€æ¨¡å‹å†²çªé¢„æµ‹ä¸­çš„å‚æ•°åŒ–ä¸éå‚æ•°åŒ–çŸ¥è¯†ç ”ç©¶",
      "authors": [
        "Apollinaire Poli Nemkova",
        "Sarath Chandra Lingareddy",
        "Sagnik Ray Choudhury",
        "Mark V. Albert"
      ],
      "abstract": "Large Language Models (LLMs) have shown impressive performance across natural language tasks, but their ability to forecast violent conflict remains underexplored. We investigate whether LLMs possess meaningful parametric knowledge-encoded in their pretrained weights-to predict conflict escalation and fatalities without external data. This is critical for early warning systems, humanitarian planning, and policy-making. We compare this parametric knowledge with non-parametric capabilities, where LLMs access structured and unstructured context from conflict datasets (e.g., ACLED, GDELT) and recent news reports via Retrieval-Augmented Generation (RAG). Incorporating external information could enhance model performance by providing up-to-date context otherwise missing from pretrained weights. Our two-part evaluation framework spans 2020-2024 across conflict-prone regions in the Horn of Africa and the Middle East. In the parametric setting, LLMs predict conflict trends and fatalities relying only on pretrained knowledge. In the non-parametric setting, models receive summaries of recent conflict events, indicators, and geopolitical developments. We compare predicted conflict trend labels (e.g., Escalate, Stable Conflict, De-escalate, Peace) and fatalities against historical data. Our findings highlight the strengths and limitations of LLMs for conflict forecasting and the benefits of augmenting them with structured external knowledge.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†å¤§è¯­è¨€æ¨¡å‹ (LLMs) åœ¨é¢„æµ‹æš´åŠ›å†²çªæ–¹é¢çš„èƒ½åŠ›ï¼Œé‡ç‚¹å¯¹æ¯”äº†å…¶é¢„è®­ç»ƒæƒé‡ä¸­çš„å‚æ•°åŒ–çŸ¥è¯† (Parametric Knowledge) ä¸é€šè¿‡æ£€ç´¢å¢å¼ºç”Ÿæˆ (RAG) è·å–çš„éå‚æ•°åŒ–èƒ½åŠ›çš„è¡¨ç°å·®å¼‚ã€‚ç ”ç©¶æ„å»ºäº†ä¸€ä¸ªæ¶µç›– 2020 å¹´è‡³ 2024 å¹´éæ´²ä¹‹è§’å’Œä¸­ä¸œç­‰å†²çªé«˜å‘åœ°åŒºçš„ä¸¤é˜¶æ®µè¯„ä¼°æ¡†æ¶ï¼Œæ—¨åœ¨é¢„æµ‹å†²çªè¶‹åŠ¿åŠä¼¤äº¡äººæ•°ã€‚åœ¨å‚æ•°åŒ–è®¾å®šä¸­ï¼Œæ¨¡å‹ä»…ä¾èµ–é¢„è®­ç»ƒçŸ¥è¯†ï¼›è€Œåœ¨éå‚æ•°åŒ–è®¾å®šä¸­ï¼Œæ¨¡å‹åˆ™ç»“åˆäº†æ¥è‡ª ACLEDã€GDELT æ•°æ®åº“åŠæ–°é—»æŠ¥é“çš„ç»“æ„åŒ–ä¸éç»“æ„åŒ–ä¸Šä¸‹æ–‡ä¿¡æ¯ã€‚è¯„ä¼°è¿‡ç¨‹ä¸­ï¼Œç ”ç©¶äººå‘˜å¯¹æ¯”äº†æ¨¡å‹é¢„æµ‹çš„å†²çªè¶‹åŠ¿æ ‡ç­¾ï¼ˆå¦‚ Escalateã€Stable Conflictã€De-escalateã€Peaceï¼‰ä¸å®é™…å†å²æ•°æ®ã€‚ç ”ç©¶ç»“æœæ­ç¤ºäº† LLMs åœ¨å†²çªé¢„æµ‹é¢†åŸŸçš„ä¼˜åŠ¿ä¸å±€é™æ€§ï¼Œå¹¶è¯æ˜äº†å¼•å…¥å¤–éƒ¨ç»“æ„åŒ–çŸ¥è¯†èƒ½æ˜¾è‘—å¢å¼ºæ¨¡å‹åœ¨æä¾›å®æ—¶ä¸Šä¸‹æ–‡æ–¹é¢çš„è¡¨ç°ã€‚è¯¥ç ”ç©¶ä¸ºæå‡æ—©æœŸé¢„è­¦ç³»ç»Ÿå’Œæ”¿ç­–åˆ¶å®šçš„ç§‘å­¦æ€§æä¾›äº†é‡è¦å‚è€ƒã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.09852v1",
      "published_date": "2025-05-14 23:24:22 UTC",
      "updated_date": "2025-05-14 23:24:22 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T08:29:46.268951+00:00"
    },
    {
      "arxiv_id": "2505.09847v2",
      "title": "Causal Predictive Optimization and Generation for Business AI",
      "title_zh": "å•†ä¸šäººå·¥æ™ºèƒ½ä¸­çš„å› æœé¢„æµ‹ä¼˜åŒ–ä¸ç”Ÿæˆ",
      "authors": [
        "Liyang Zhao",
        "Olurotimi Seton",
        "Himadeep Reddy Reddivari",
        "Suvendu Jena",
        "Shadow Zhao",
        "Rachit Kumar",
        "Changshuai Wei"
      ],
      "abstract": "The sales process involves sales functions converting leads or opportunities to customers and selling more products to existing customers. The optimization of the sales process thus is key to success of any B2B business. In this work, we introduce a principled approach to sales optimization and business AI, namely the Causal Predictive Optimization and Generation, which includes three layers: 1) prediction layer with causal ML 2) optimization layer with constraint optimization and contextual bandit 3) serving layer with Generative AI and feedback-loop for system enhancement. We detail the implementation and deployment of the system in LinkedIn, showcasing significant wins over legacy systems and sharing learning and insight broadly applicable to this field.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§åä¸ºCausal Predictive Optimization and Generationçš„ç³»ç»ŸåŒ–æ–¹æ³•ï¼Œæ—¨åœ¨é€šè¿‡å•†ä¸šAIä¼˜åŒ–B2Bä¸šåŠ¡ä¸­çš„é”€å”®æµç¨‹ã€‚è¯¥æ¡†æ¶ç”±ä¸‰ä¸ªå…³é”®å±‚é¢ç»„æˆï¼šé¦–å…ˆæ˜¯åˆ©ç”¨Causal MLå®ç°çš„é¢„æµ‹å±‚ï¼Œå…¶æ¬¡æ˜¯ç»“åˆConstraint Optimizationå’ŒContextual Banditçš„ä¼˜åŒ–å±‚ï¼Œæœ€åæ˜¯é›†æˆGenerative AIä¸åé¦ˆå¾ªç¯(Feedback-loop)çš„æœåŠ¡å±‚ã€‚è¯¥ç³»ç»Ÿå·²åœ¨LinkedInæˆåŠŸéƒ¨ç½²ï¼Œå®éªŒç»“æœè¡¨æ˜å…¶æ€§èƒ½æ˜¾è‘—ä¼˜äºç°æœ‰çš„Legacy Systemsã€‚é€šè¿‡åˆ†äº«å…·ä½“çš„å®æ–½ç»†èŠ‚å’Œå®æˆ˜è§è§£ï¼Œè¯¥ç ”ç©¶è¯æ˜äº†å°†å› æœå»ºæ¨¡ã€çº¦æŸä¼˜åŒ–ä¸ç”Ÿæˆå¼æŠ€æœ¯ç›¸ç»“åˆåœ¨æå‡å•†ä¸šå†³ç­–æ•ˆç‡æ–¹é¢çš„å·¨å¤§æ½œåŠ›ï¼Œä¸ºè¯¥é¢†åŸŸæä¾›äº†å…·æœ‰å¹¿æ³›é€‚ç”¨æ€§çš„æŠ€æœ¯èŒƒå¼ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.IR",
        "stat.ML"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.09847v2",
      "published_date": "2025-05-14 23:12:20 UTC",
      "updated_date": "2025-05-21 16:12:30 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T08:29:50.757810+00:00"
    },
    {
      "arxiv_id": "2505.09830v1",
      "title": "Evaluating Large Language Models for the Generation of Unit Tests with Equivalence Partitions and Boundary Values",
      "title_zh": "è¯„ä¼°å¤§è¯­è¨€æ¨¡å‹åœ¨ç»“åˆç­‰ä»·ç±»åˆ’åˆ†ä¸è¾¹ç•Œå€¼çš„å•å…ƒæµ‹è¯•ç”Ÿæˆä¸­çš„è¡¨ç°",
      "authors": [
        "MartÃ­n RodrÃ­guez",
        "Gustavo Rossi",
        "Alejandro Fernandez"
      ],
      "abstract": "The design and implementation of unit tests is a complex task many programmers neglect. This research evaluates the potential of Large Language Models (LLMs) in automatically generating test cases, comparing them with manual tests. An optimized prompt was developed, that integrates code and requirements, covering critical cases such as equivalence partitions and boundary values. The strengths and weaknesses of LLMs versus trained programmers were compared through quantitative metrics and manual qualitative analysis. The results show that the effectiveness of LLMs depends on well-designed prompts, robust implementation, and precise requirements. Although flexible and promising, LLMs still require human supervision. This work highlights the importance of manual qualitative analysis as an essential complement to automation in unit test evaluation.",
      "tldr_zh": "è¯¥ç ”ç©¶è¯„ä¼°äº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨è‡ªåŠ¨ç”Ÿæˆå•å…ƒæµ‹è¯•ï¼ˆUnit Testsï¼‰æ–¹é¢çš„æ½œåŠ›ï¼Œå¹¶å°†å…¶ä¸äººå·¥ç¼–å†™çš„æµ‹è¯•è¿›è¡Œäº†å¯¹æ¯”ã€‚ç ”ç©¶å›¢é˜Ÿå¼€å‘äº†ä¸€ç§ä¼˜åŒ–çš„æç¤ºè¯ï¼ˆPromptï¼‰ï¼Œé€šè¿‡æ•´åˆä»£ç ä¸éœ€æ±‚ä¿¡æ¯ï¼Œç¡®ä¿æµ‹è¯•ç”¨ä¾‹èƒ½å¤Ÿæ¶µç›–ç­‰ä»·ç±»åˆ’åˆ†ï¼ˆEquivalence Partitionsï¼‰å’Œè¾¹ç•Œå€¼ï¼ˆBoundary Valuesï¼‰ç­‰å…³é”®åœºæ™¯ã€‚é€šè¿‡å®šé‡æŒ‡æ ‡å’Œäººå·¥å®šæ€§åˆ†æï¼Œè¯¥ç ”ç©¶å¯¹æ¯”äº† LLMs ä¸å—è¿‡è®­ç»ƒçš„ç¨‹åºå‘˜åœ¨æµ‹è¯•ç”Ÿæˆæ–¹é¢çš„ä¼˜åŠ£ã€‚ç»“æœè¡¨æ˜ï¼ŒLLMs çš„æœ‰æ•ˆæ€§é«˜åº¦ä¾èµ–äºç²¾å¿ƒè®¾è®¡çš„æç¤ºè¯ã€ç¨³å¥çš„å®ç°ä»¥åŠç²¾ç¡®çš„éœ€æ±‚æè¿°ã€‚å°½ç®¡ LLMs å±•ç°å‡ºçµæ´»ä¸”å…·æœ‰å‰æ™¯çš„åº”ç”¨ä»·å€¼ï¼Œä½†ç°é˜¶æ®µä»éœ€è¦äººç±»çš„ç›‘ç£ã€‚è¯¥é¡¹å·¥ä½œæœ€åå¼ºè°ƒï¼Œåœ¨å•å…ƒæµ‹è¯•è¯„ä¼°ä¸­ï¼Œäººå·¥å®šæ€§åˆ†ææ˜¯è‡ªåŠ¨åŒ–æŠ€æœ¯ä¸å¯æˆ–ç¼ºçš„é‡è¦è¡¥å……ã€‚",
      "categories": [
        "cs.SE",
        "cs.AI"
      ],
      "primary_category": "cs.SE",
      "comment": "Under revision at Jornadas de Cloud Computing, Big Data & Emerging Topics (JCC-BD&ET) - 2025",
      "pdf_url": "https://arxiv.org/pdf/2505.09830v1",
      "published_date": "2025-05-14 22:22:15 UTC",
      "updated_date": "2025-05-14 22:22:15 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T08:30:04.897349+00:00"
    },
    {
      "arxiv_id": "2505.09814v2",
      "title": "$XX^{t}$ Can Be Faster",
      "title_zh": "$XX^{t}$ è¿ç®—å¯ä»¥æ›´å¿«é€Ÿ",
      "authors": [
        "Dmitry Rybin",
        "Yushun Zhang",
        "Zhi-Quan Luo"
      ],
      "abstract": "We present RXTX, a new algorithm for computing the product of matrix by its transpose $XX^{t}$ for $X\\in \\mathbb{R}^{n\\times m}$. RXTX uses $5\\%$ fewer multiplications and $5\\%$ fewer operations (additions and multiplications) than State-of-the-Art algorithms. Note that the accelerations not only holds asymptotically for large matrices with $n \\rightarrow \\infty$, but also for small matrices including $n = 4$. The algorithm was discovered by combining Machine Learning-based search methods with Combinatorial Optimization.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹çŸ©é˜µä¸å…¶è½¬ç½®çš„ä¹˜ç§¯ $XX^{t}$ çš„è®¡ç®—é—®é¢˜ï¼Œæå‡ºäº†åä¸º RXTX çš„æ–°ç®—æ³•ã€‚ä¸ç°æœ‰çš„ State-of-the-Art ç®—æ³•ç›¸æ¯”ï¼ŒRXTX åœ¨ä¹˜æ³•è¿ç®—é‡å’Œæ€»æ“ä½œæ•°ï¼ˆåŒ…å«åŠ æ³•ä¸ä¹˜æ³•ï¼‰ä¸Šå‡å‡å°‘äº† 5%ã€‚è¯¥ç®—æ³•çš„åŠ é€Ÿä¼˜åŠ¿ä¸ä»…ä½“ç°åœ¨ $n \\rightarrow \\infty$ çš„æ¸è¿‘æ€§èƒ½ä¸Šï¼Œåœ¨ $n=4$ ç­‰å°å‹çŸ©é˜µè®¡ç®—ä¸­åŒæ ·è¡¨ç°å‡ºè‰²ã€‚ç ”ç©¶å›¢é˜Ÿé€šè¿‡ç»“åˆåŸºäº Machine Learning çš„æœç´¢æ–¹æ³•ä¸ Combinatorial Optimization æˆåŠŸå‘ç°äº†è¯¥ç®—æ³•ã€‚è¿™ä¸€æˆæœä¸ºæå‡çŸ©é˜µè¿ç®—æ•ˆç‡æä¾›äº†æ–°çš„è·¯å¾„ï¼Œè¯æ˜äº†åˆ©ç”¨è‡ªåŠ¨åŒ–æœç´¢ä¼˜åŒ–åŸºç¡€æ•°å­¦è¿ç®—çš„æœ‰æ•ˆæ€§ã€‚",
      "categories": [
        "cs.DS",
        "cs.AI",
        "cs.LG",
        "cs.SC"
      ],
      "primary_category": "cs.DS",
      "comment": "improved presentation",
      "pdf_url": "https://arxiv.org/pdf/2505.09814v2",
      "published_date": "2025-05-14 21:31:44 UTC",
      "updated_date": "2025-05-16 09:23:27 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T08:30:11.616054+00:00"
    },
    {
      "arxiv_id": "2505.09807v1",
      "title": "Exploring the generalization of LLM truth directions on conversational formats",
      "title_zh": "æ¢ç´¢å¤§è¯­è¨€æ¨¡å‹çœŸå€¼æ–¹å‘åœ¨å¯¹è¯æ ¼å¼ä¸Šçš„æ³›åŒ–æ€§",
      "authors": [
        "Timour Ichmoukhamedov",
        "David Martens"
      ],
      "abstract": "Several recent works argue that LLMs have a universal truth direction where true and false statements are linearly separable in the activation space of the model. It has been demonstrated that linear probes trained on a single hidden state of the model already generalize across a range of topics and might even be used for lie detection in LLM conversations. In this work we explore how this truth direction generalizes between various conversational formats. We find good generalization between short conversations that end on a lie, but poor generalization to longer formats where the lie appears earlier in the input prompt. We propose a solution that significantly improves this type of generalization by adding a fixed key phrase at the end of each conversation. Our results highlight the challenges towards reliable LLM lie detectors that generalize to new settings.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†å¤§å‹è¯­è¨€æ¨¡å‹(LLMs)ä¸­â€œé€šç”¨çœŸå®æ–¹å‘(universal truth direction)â€åœ¨ä¸åŒå¯¹è¯æ ¼å¼ä¸‹çš„æ³›åŒ–èƒ½åŠ›ï¼Œå³æ¨¡å‹æ¿€æ´»ç©ºé—´ä¸­çœŸå‡é™ˆè¿°çš„çº¿æ€§å¯åˆ†æ€§ã€‚è™½ç„¶çº¿æ€§æ¢æµ‹(linear probes)åœ¨è·¨ä¸»é¢˜ä»»åŠ¡ä¸­è¡¨ç°å‡ºä¸€å®šçš„æ³›åŒ–æ€§å¹¶å¯ç”¨äºè°è¨€æ£€æµ‹ï¼Œä½†å®éªŒå‘ç°è¯¥ç‰¹å¾åœ¨å¤šæ ·åŒ–å¯¹è¯è¯­å¢ƒä¸­çš„è¡¨ç°å­˜åœ¨æ˜¾è‘—å·®å¼‚ã€‚ç»“æœè¡¨æ˜ï¼ŒçœŸå®æ–¹å‘åœ¨ä»¥è°è¨€ç»“å°¾çš„çŸ­å¯¹è¯ä¸­æ³›åŒ–æ•ˆæœè‰¯å¥½ï¼Œä½†åœ¨è°è¨€å‡ºç°ä½ç½®è¾ƒæ—©çš„é•¿å¯¹è¯æ ¼å¼ä¸­æ³›åŒ–èƒ½åŠ›è¾ƒå·®ã€‚é’ˆå¯¹è¿™ä¸€æŒ‘æˆ˜ï¼Œç ”ç©¶æå‡ºé€šè¿‡åœ¨å¯¹è¯æœ«å°¾æ·»åŠ å›ºå®šå…³é”®è¯ç»„(fixed key phrase)çš„æ–¹æ¡ˆï¼Œæ˜¾è‘—æå‡äº†æ¨¡å‹åœ¨ä¸åŒå¯¹è¯é•¿åº¦ä¸‹çš„æ³›åŒ–è¡¨ç°ã€‚è¯¥å·¥ä½œæ­ç¤ºäº†æ„å»ºèƒ½å¤Ÿé€‚åº”æ–°åœºæ™¯çš„å¯é LLMè°è¨€æ£€æµ‹å™¨çš„å¤æ‚æ€§ï¼Œä¸ºå¢å¼ºæ¨¡å‹çœŸå®æ€§è¡¨å¾çš„ç¨³å¥æ€§æä¾›äº†é‡è¦å‚è€ƒã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.09807v1",
      "published_date": "2025-05-14 21:21:08 UTC",
      "updated_date": "2025-05-14 21:21:08 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T08:30:06.432959+00:00"
    },
    {
      "arxiv_id": "2505.09805v1",
      "title": "Contextual Phenotyping of Pediatric Sepsis Cohort Using Large Language Models",
      "title_zh": "åŸºäºå¤§è¯­è¨€æ¨¡å‹çš„å„¿ç«¥è„“æ¯’ç—‡é˜Ÿåˆ—æƒ…å¢ƒåŒ–è¡¨å‹åˆ†æ",
      "authors": [
        "Aditya Nagori",
        "Ayush Gautam",
        "Matthew O. Wiens",
        "Vuong Nguyen",
        "Nathan Kenya Mugisha",
        "Jerome Kabakyenga",
        "Niranjan Kissoon",
        "John Mark Ansermino",
        "Rishikesan Kamaleswaran"
      ],
      "abstract": "Clustering patient subgroups is essential for personalized care and efficient resource use. Traditional clustering methods struggle with high-dimensional, heterogeneous healthcare data and lack contextual understanding. This study evaluates Large Language Model (LLM) based clustering against classical methods using a pediatric sepsis dataset from a low-income country (LIC), containing 2,686 records with 28 numerical and 119 categorical variables. Patient records were serialized into text with and without a clustering objective. Embeddings were generated using quantized LLAMA 3.1 8B, DeepSeek-R1-Distill-Llama-8B with low-rank adaptation(LoRA), and Stella-En-400M-V5 models. K-means clustering was applied to these embeddings. Classical comparisons included K-Medoids clustering on UMAP and FAMD-reduced mixed data. Silhouette scores and statistical tests evaluated cluster quality and distinctiveness. Stella-En-400M-V5 achieved the highest Silhouette Score (0.86). LLAMA 3.1 8B with the clustering objective performed better with higher number of clusters, identifying subgroups with distinct nutritional, clinical, and socioeconomic profiles. LLM-based methods outperformed classical techniques by capturing richer context and prioritizing key features. These results highlight potential of LLMs for contextual phenotyping and informed decision-making in resource-limited settings.",
      "tldr_zh": "è¯¥ç ”ç©¶è¯„ä¼°äº†å¤§è¯­è¨€æ¨¡å‹(LLMs)åœ¨ä½æ”¶å…¥å›½å®¶(LIC)å„¿ç«¥è„“æ¯’ç—‡(pediatric sepsis)æ‚£è€…è¡¨å‹åˆ†ç±»ä¸­çš„åº”ç”¨ï¼Œæ—¨åœ¨è§£å†³ä¼ ç»Ÿèšç±»æ–¹æ³•åœ¨å¤„ç†é«˜ç»´å¼‚æ„åŒ»ç–—æ•°æ®æ—¶ç¼ºä¹ä¸Šä¸‹æ–‡ç†è§£çš„é—®é¢˜ã€‚ç ”ç©¶äººå‘˜å°†2,686æ¡åŒ…å«æ•°å€¼å’Œåˆ†ç±»å˜é‡çš„æ‚£è€…è®°å½•åºåˆ—åŒ–ä¸ºæ–‡æœ¬ï¼Œå¹¶åˆ©ç”¨ç»è¿‡ä½é˜¶è‡ªé€‚åº”(LoRA)å¾®è°ƒçš„LLAMA 3.1 8Bã€DeepSeek-R1-Distill-Llama-8Bä»¥åŠStella-En-400M-V5æ¨¡å‹ç”ŸæˆåµŒå…¥å‘é‡(Embeddings)ã€‚é€šè¿‡å¯¹è¿™äº›åµŒå…¥åº”ç”¨K-meansèšç±»å¹¶ä¸åŸºäºUMAPå’ŒFAMDé™ç»´çš„ä¼ ç»Ÿæ–¹æ³•å¯¹æ¯”ï¼Œå®éªŒå‘ç°Stella-En-400M-V5è·å¾—äº†æœ€é«˜çš„è½®å»“åˆ†æ•°(Silhouette Score, 0.86)ã€‚å¸¦æœ‰èšç±»ç›®æ ‡çš„LLAMA 3.1 8Båœ¨è¯†åˆ«å…·æœ‰ä¸åŒè¥å…»ã€ä¸´åºŠå’Œç¤¾ä¼šç»æµç‰¹å¾çš„å­ç¾¤ä½“æ–¹é¢è¡¨ç°ä¼˜å¼‚ã€‚å®éªŒè¯æ˜ï¼ŒåŸºäºLLMçš„æ–¹æ³•é€šè¿‡æ•è·æ›´ä¸°å¯Œçš„ä¸Šä¸‹æ–‡ä¿¡æ¯å¹¶ä¼˜å…ˆå¤„ç†å…³é”®ç‰¹å¾ï¼Œå…¶æ€§èƒ½æ˜¾è‘—ä¼˜äºä¼ ç»ŸæŠ€æœ¯ã€‚è¯¥ç ”ç©¶ç»“æœçªæ˜¾äº†LLMsåœ¨èµ„æºå—é™ç¯å¢ƒä¸‹è¿›è¡Œä¸Šä¸‹æ–‡è¡¨å‹åˆ†æå’Œæ”¯æŒä¸´åºŠå†³ç­–çš„å·¨å¤§æ½œåŠ›ã€‚",
      "categories": [
        "q-bio.QM",
        "cs.AI",
        "cs.CL",
        "cs.LG",
        "stat.AP"
      ],
      "primary_category": "q-bio.QM",
      "comment": "11 pages, 2 Figures, 1 Table",
      "pdf_url": "https://arxiv.org/pdf/2505.09805v1",
      "published_date": "2025-05-14 21:05:40 UTC",
      "updated_date": "2025-05-14 21:05:40 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T08:30:10.249899+00:00"
    },
    {
      "arxiv_id": "2505.09796v1",
      "title": "Virtual Dosimetrists: A Radiotherapy Training \"Flight Simulator\"",
      "title_zh": "è™šæ‹Ÿå‰‚é‡å¸ˆï¼šæ”¾å°„æ²»ç–—åŸ¹è®­â€œé£è¡Œæ¨¡æ‹Ÿå™¨â€",
      "authors": [
        "Skylar S. Gay",
        "Tucker Netherton",
        "Barbara Marquez",
        "Raymond Mumme",
        "Mary Gronberg",
        "Brent Parker",
        "Chelsea Pinnix",
        "Sanjay Shete",
        "Carlos Cardenas",
        "Laurence Court"
      ],
      "abstract": "Effective education in radiotherapy plan quality review requires a robust, regularly updated set of examples and the flexibility to demonstrate multiple possible planning approaches and their consequences. However, the current clinic-based paradigm does not support these needs. To address this, we have developed 'Virtual Dosimetrist' models that can both generate training examples of suboptimal treatment plans and then allow trainees to improve the plan quality through simple natural language prompts, as if communicating with a dosimetrist. The dose generation and modification process is accurate, rapid, and requires only modest resources. This work is the first to combine dose distribution prediction with natural language processing; providing a robust pipeline for both generating suboptimal training plans and allowing trainees to practice their critical plan review and improvement skills that addresses the challenges of the current clinic-based paradigm.",
      "tldr_zh": "è¯¥ç ”ç©¶å¼€å‘äº†åä¸º Virtual Dosimetrist çš„æ¨¡å‹ï¼Œæ—¨åœ¨è§£å†³å½“å‰ä¸´åºŠæ”¾å°„æ²»ç–—(radiotherapy)è®¡åˆ’è´¨é‡è¯„ä¼°æ•™è‚²ä¸­ç¼ºä¹å¤šæ ·åŒ–æ¡ˆä¾‹å’Œçµæ´»æ€§çš„é—®é¢˜ã€‚è¯¥æ¨¡å‹å……å½“æ”¾ç–—åŸ¹è®­çš„â€œé£è¡Œæ¨¡æ‹Ÿå™¨â€ï¼Œèƒ½å¤Ÿè‡ªåŠ¨ç”Ÿæˆæ¬¡ä¼˜æ²»ç–—è®¡åˆ’(suboptimal treatment plans)ä½œä¸ºæ•™å­¦æ¡ˆä¾‹ï¼Œå¹¶å…è®¸å—è®­è€…é€šè¿‡ç®€å•çš„è‡ªç„¶è¯­è¨€æç¤º(natural language prompts)ä¸æ¨¡å‹äº¤äº’ï¼Œæ¨¡æ‹ŸçœŸå®ç¯å¢ƒä¸‹ä¸å‰‚é‡å¸ˆ(dosimetrist)çš„æ²Ÿé€šä»¥æ”¹è¿›è®¡åˆ’ã€‚ä½œä¸ºé¦–ä¸ªå°†å‰‚é‡åˆ†å¸ƒé¢„æµ‹(dose distribution prediction)ä¸è‡ªç„¶è¯­è¨€å¤„ç†(NLP)ç›¸ç»“åˆçš„å·¥ä½œï¼Œè¯¥ç³»ç»Ÿèƒ½å¤Ÿå‡†ç¡®ã€å¿«é€Ÿåœ°ç”Ÿæˆå¹¶ä¿®æ”¹å‰‚é‡åˆ†å¸ƒã€‚è¿™ä¸€ç¨³å¥çš„æµç¨‹ä¸ºå—è®­è€…æä¾›äº†ç»ƒä¹ æ‰¹åˆ¤æ€§è®¡åˆ’å®¡æŸ¥ä¸æ”¹è¿›æŠ€èƒ½çš„å¹³å°ï¼Œæœ‰æ•ˆå¼¥è¡¥äº†ä¼ ç»Ÿä¸´åºŠåŸ¹è®­æ¨¡å¼çš„ä¸è¶³ã€‚",
      "categories": [
        "physics.med-ph",
        "cs.AI"
      ],
      "primary_category": "physics.med-ph",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.09796v1",
      "published_date": "2025-05-14 20:47:13 UTC",
      "updated_date": "2025-05-14 20:47:13 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T08:30:53.617789+00:00"
    },
    {
      "arxiv_id": "2505.18175v1",
      "title": "Evaluation in EEG Emotion Recognition: State-of-the-Art Review and Unified Framework",
      "title_zh": "EEGæƒ…æ„Ÿè¯†åˆ«è¯„ä¼°ï¼šç°çŠ¶ç»¼è¿°ä¸ç»Ÿä¸€æ¡†æ¶",
      "authors": [
        "Natia Kukhilava",
        "Tatia Tsmindashvili",
        "Rapael Kalandadze",
        "Anchit Gupta",
        "Sofio Katamadze",
        "FranÃ§ois BrÃ©mond",
        "Laura M. Ferrari",
        "Philipp MÃ¼ller",
        "Benedikt Emanuel Wirth"
      ],
      "abstract": "Electroencephalography-based Emotion Recognition (EEG-ER) has become a growing research area in recent years. Analyzing 216 papers published between 2018 and 2023, we uncover that the field lacks a unified evaluation protocol, which is essential to fairly define the state of the art, compare new approaches and to track the field's progress. We report the main inconsistencies between the used evaluation protocols, which are related to ground truth definition, evaluation metric selection, data splitting types (e.g., subject-dependent or subject-independent) and the use of different datasets. Capitalizing on this state-of-the-art research, we propose a unified evaluation protocol, EEGain (https://github.com/EmotionLab/EEGain), which enables an easy and efficient evaluation of new methods and datasets. EEGain is a novel open source software framework, offering the capability to compare - and thus define - state-of-the-art results. EEGain includes standardized methods for data pre-processing, data splitting, evaluation metrics, and the ability to load the six most relevant datasets (i.e., AMIGOS, DEAP, DREAMER, MAHNOB-HCI, SEED, SEED-IV) in EEG-ER with only a single line of code. In addition, we have assessed and validated EEGain using these six datasets on the four most common publicly available methods (EEGNet, DeepConvNet, ShallowConvNet, TSception). This is a significant step to make research on EEG-ER more reproducible and comparable, thereby accelerating the overall progress of the field.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹åŸºäºè„‘ç”µå›¾çš„æƒ…æ„Ÿè¯†åˆ«(EEG-based Emotion Recognition, EEG-ER)é¢†åŸŸç¼ºä¹ç»Ÿä¸€è¯„ä¼°åè®®çš„é—®é¢˜ï¼Œåœ¨åˆ†æäº†2018å¹´è‡³2023å¹´é—´å‘è¡¨çš„216ç¯‡è®ºæ–‡åï¼Œæ­ç¤ºäº†åœ¨çœŸå€¼å®šä¹‰ã€æŒ‡æ ‡é€‰æ‹©åŠæ•°æ®æ‹†åˆ†ç­‰æ–¹é¢å­˜åœ¨çš„ä¸¥é‡ä¸ä¸€è‡´æ€§ã€‚ä¸ºæ­¤ï¼Œç ”ç©¶è€…å¼€å‘å¹¶æå‡ºäº†åä¸ºEEGainçš„å¼€æºç»Ÿä¸€è¯„ä¼°æ¡†æ¶ï¼Œæ—¨åœ¨å®ç°å¯¹æ–°æ–¹æ³•å’Œæ•°æ®é›†çš„ä¾¿æ·ã€é«˜æ•ˆè¯„ä¼°ã€‚è¯¥æ¡†æ¶é›†æˆäº†æ•°æ®é¢„å¤„ç†ã€æ•°æ®æ‹†åˆ†å’Œè¯„ä¼°æŒ‡æ ‡çš„æ ‡å‡†åŒ–æ–¹æ³•ï¼Œå¹¶æ”¯æŒé€šè¿‡å•è¡Œä»£ç å¿«é€ŸåŠ è½½AMIGOSã€DEAPã€DREAMERã€MAHNOB-HCIã€SEEDå’ŒSEED-IVå…­ä¸ªæ ¸å¿ƒæ•°æ®é›†ã€‚é€šè¿‡ä½¿ç”¨EEGNetã€DeepConvNetã€ShallowConvNetå’ŒTSceptionå››ç§æµè¡Œæ–¹æ³•è¿›è¡ŒéªŒè¯ï¼Œè¯æ˜äº†è¯¥æ¡†æ¶èƒ½æ˜¾è‘—æå‡ç ”ç©¶çš„å¯å¤ç°æ€§å’Œå¯æ¯”æ€§ã€‚EEGainä¸ºç§‘å­¦å®šä¹‰é¢†åŸŸå†…çš„SOTAæ€§èƒ½æä¾›äº†ç»Ÿä¸€æ ‡å‡†ï¼Œæœ‰åŠ›åœ°æ¨åŠ¨äº†è„‘ç”µæƒ…æ„Ÿè¯†åˆ«æŠ€æœ¯çš„æ•´ä½“è¿›æ­¥ã€‚",
      "categories": [
        "eess.SP",
        "cs.AI",
        "cs.CV",
        "cs.HC",
        "cs.LG"
      ],
      "primary_category": "eess.SP",
      "comment": "This work has been submitted to the IEEE for possible publication",
      "pdf_url": "https://arxiv.org/pdf/2505.18175v1",
      "published_date": "2025-05-14 20:44:39 UTC",
      "updated_date": "2025-05-14 20:44:39 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T08:31:37.287934+00:00"
    },
    {
      "arxiv_id": "2505.09794v1",
      "title": "Automated Detection of Clinical Entities in Lung and Breast Cancer Reports Using NLP Techniques",
      "title_zh": "åŸºäº NLP æŠ€æœ¯çš„è‚ºç™Œä¸ä¹³è…ºç™ŒæŠ¥å‘Šä¸´åºŠå®ä½“è‡ªåŠ¨æ£€æµ‹",
      "authors": [
        "J. Moreno-Casanova",
        "J. M. AuÃ±Ã³n",
        "A. MÃ¡rtinez-PÃ©rez",
        "M. E. PÃ©rez-MartÃ­nez",
        "M. E. Gas-LÃ³pez"
      ],
      "abstract": "Research projects, including those focused on cancer, rely on the manual extraction of information from clinical reports. This process is time-consuming and prone to errors, limiting the efficiency of data-driven approaches in healthcare. To address these challenges, Natural Language Processing (NLP) offers an alternative for automating the extraction of relevant data from electronic health records (EHRs). In this study, we focus on lung and breast cancer due to their high incidence and the significant impact they have on public health. Early detection and effective data management in both types of cancer are crucial for improving patient outcomes. To enhance the accuracy and efficiency of data extraction, we utilized GMV's NLP tool uQuery, which excels at identifying relevant entities in clinical texts and converting them into standardized formats such as SNOMED and OMOP. uQuery not only detects and classifies entities but also associates them with contextual information, including negated entities, temporal aspects, and patient-related details. In this work, we explore the use of NLP techniques, specifically Named Entity Recognition (NER), to automatically identify and extract key clinical information from EHRs related to these two cancers. A dataset from Health Research Institute Hospital La Fe (IIS La Fe), comprising 200 annotated breast cancer and 400 lung cancer reports, was used, with eight clinical entities manually labeled using the Doccano platform. To perform NER, we fine-tuned the bsc-bio-ehr-en3 model, a RoBERTa-based biomedical linguistic model pre-trained in Spanish. Fine-tuning was performed using the Transformers architecture, enabling accurate recognition of clinical entities in these cancer types. Our results demonstrate strong overall performance, particularly in identifying entities like MET and PAT, although challenges remain with less frequent entities like EVOL.",
      "tldr_zh": "è¯¥ç ”ç©¶æ—¨åœ¨åˆ©ç”¨è‡ªç„¶è¯­è¨€å¤„ç†(NLP)æŠ€æœ¯è‡ªåŠ¨åŒ–æå–è‚ºç™Œå’Œä¹³è…ºç™Œä¸´åºŠæŠ¥å‘Šä¸­çš„å…³é”®å®ä½“ï¼Œä»¥è§£å†³ä¼ ç»Ÿæ‰‹åŠ¨æ•°æ®æå–æ•ˆç‡ä½ä¸‹ä¸”æ˜“å‡ºé”™çš„éš¾é¢˜ã€‚ç ”ç©¶é‡‡ç”¨äº†GMVçš„NLPå·¥å…·uQueryï¼Œé€šè¿‡å‘½åå®ä½“è¯†åˆ«(NER)æŠ€æœ¯å°†ç”µå­å¥åº·è®°å½•(EHR)ä¸­çš„ä¿¡æ¯è½¬æ¢ä¸ºSNOMEDå’ŒOMOPç­‰å›½é™…æ ‡å‡†æ ¼å¼ï¼Œå¹¶èƒ½æœ‰æ•ˆè¯†åˆ«å¦å®šã€æ—¶é—´åŠæ‚£è€…ç›¸å…³çš„ä¸Šä¸‹æ–‡ä¿¡æ¯ã€‚å®éªŒæ•°æ®é›†åŒ…å«æ¥è‡ªHealth Research Institute Hospital La Feçš„600ä»½ç»ç”±Doccanoå¹³å°æ ‡æ³¨çš„æŠ¥å‘Šï¼Œæ¶µç›–å…«ç±»ä¸´åºŠå®ä½“ã€‚ç ”ç©¶å›¢é˜ŸåŸºäºTransformersæ¶æ„å¾®è°ƒäº†é¢„è®­ç»ƒçš„è¥¿ç­ç‰™è¯­ç”Ÿç‰©åŒ»å­¦æ¨¡å‹bsc-bio-ehr-en3ï¼Œå®ç°äº†å¯¹ç™Œç—‡ä¸´åºŠå®ä½“çš„ç²¾ç¡®è¯†åˆ«ã€‚ç»“æœè¡¨æ˜ï¼Œè¯¥æ¨¡å‹åœ¨è¯†åˆ«METå’ŒPATç­‰å®ä½“æ—¶è¡¨ç°ä¼˜å¼‚ï¼Œä½†åœ¨å¤„ç†EVOLç­‰ä½é¢‘å®ä½“æ—¶ä»é¢ä¸´æŒ‘æˆ˜ã€‚è¿™é¡¹å·¥ä½œå±•ç¤ºäº†è‡ªåŠ¨åŒ–NLPæŠ€æœ¯åœ¨ä¼˜åŒ–ç™Œç—‡æ•°æ®ç®¡ç†åŠæå‡ä¸´åºŠç ”ç©¶æ•ˆç‡æ–¹é¢çš„æ˜¾è‘—åº”ç”¨æ½œåŠ›ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.09794v1",
      "published_date": "2025-05-14 20:44:29 UTC",
      "updated_date": "2025-05-14 20:44:29 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T08:32:03.248467+00:00"
    },
    {
      "arxiv_id": "2505.09787v1",
      "title": "A Multimodal Multi-Agent Framework for Radiology Report Generation",
      "title_zh": "ä¸€ç§é¢å‘æ”¾å°„å­¦æŠ¥å‘Šç”Ÿæˆçš„å¤šæ¨¡æ€å¤šæ™ºèƒ½ä½“æ¡†æ¶",
      "authors": [
        "Ziruo Yi",
        "Ting Xiao",
        "Mark V. Albert"
      ],
      "abstract": "Radiology report generation (RRG) aims to automatically produce diagnostic reports from medical images, with the potential to enhance clinical workflows and reduce radiologists' workload. While recent approaches leveraging multimodal large language models (MLLMs) and retrieval-augmented generation (RAG) have achieved strong results, they continue to face challenges such as factual inconsistency, hallucination, and cross-modal misalignment. We propose a multimodal multi-agent framework for RRG that aligns with the stepwise clinical reasoning workflow, where task-specific agents handle retrieval, draft generation, visual analysis, refinement, and synthesis. Experimental results demonstrate that our approach outperforms a strong baseline in both automatic metrics and LLM-based evaluations, producing more accurate, structured, and interpretable reports. This work highlights the potential of clinically aligned multi-agent frameworks to support explainable and trustworthy clinical AI applications.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§ç”¨äºæ”¾å°„æŠ¥å‘Šç”Ÿæˆ(Radiology Report Generation, RRG)çš„å¤šæ¨¡æ€å¤šæ™ºèƒ½ä½“(Multimodal Multi-Agent)æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³ç°æœ‰æ–¹æ³•åœ¨åŒ»å­¦å›¾åƒè§£æä¸­é¢ä¸´çš„äº‹å®ä¸ä¸€è‡´ã€å¹»è§‰ä»¥åŠè·¨æ¨¡æ€å¯¹é½ä¸è‰¯ç­‰æŒ‘æˆ˜ã€‚è¯¥æ¡†æ¶é€šè¿‡æ¨¡æ‹Ÿä¸´åºŠé€æ­¥æ¨ç†çš„å·¥ä½œæµï¼Œéƒ¨ç½²äº†ä¸“é—¨è´Ÿè´£æ£€ç´¢ã€è‰æ¡ˆç”Ÿæˆã€è§†è§‰åˆ†æã€ç»†åŒ–å’Œåˆæˆçš„å¤šä¸ªä»»åŠ¡é©±åŠ¨å‹æ™ºèƒ½ä½“ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨è‡ªåŠ¨åŒ–æŒ‡æ ‡å’ŒåŸºäºå¤§è¯­è¨€æ¨¡å‹(LLM)çš„è¯„ä¼°ä¸­å‡æ˜¾è‘—ä¼˜äºå¼ºåŸºçº¿æ¨¡å‹ï¼Œèƒ½å¤Ÿç”Ÿæˆæ›´åŠ å‡†ç¡®ã€ç»“æ„åŒ–ä¸”å…·å¤‡å¯è§£é‡Šæ€§çš„è¯Šæ–­æŠ¥å‘Šã€‚è¯¥å·¥ä½œå±•ç¤ºäº†ä¸´åºŠå¯¹é½çš„å¤šæ™ºèƒ½ä½“æ¡†æ¶åœ¨å¼€å‘å¯è§£é‡Šã€å¯ä¿¡èµ–çš„ä¸´åºŠäººå·¥æ™ºèƒ½(Clinical AI)åº”ç”¨æ–¹é¢çš„å·¨å¤§æ½œåŠ›ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.09787v1",
      "published_date": "2025-05-14 20:28:04 UTC",
      "updated_date": "2025-05-14 20:28:04 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T08:32:08.877899+00:00"
    },
    {
      "arxiv_id": "2505.10589v4",
      "title": "Super-Resolution Generative Adversarial Networks based Video Enhancement",
      "title_zh": "åŸºäºè¶…åˆ†è¾¨ç‡ç”Ÿæˆå¯¹æŠ—ç½‘ç»œçš„è§†é¢‘å¢å¼º",
      "authors": [
        "KaÄŸan Ã‡etin",
        "Hacer AkÃ§a",
        "Ã–mer Nezih Gerek"
      ],
      "abstract": "This study introduces an enhanced approach to video super-resolution by extending ordinary Single-Image Super-Resolution (SISR) Super-Resolution Generative Adversarial Network (SRGAN) structure to handle spatio-temporal data. While SRGAN has proven effective for single-image enhancement, its design does not account for the temporal continuity required in video processing. To address this, a modified framework that incorporates 3D Non-Local Blocks is proposed, which is enabling the model to capture relationships across both spatial and temporal dimensions. An experimental training pipeline is developed, based on patch-wise learning and advanced data degradation techniques, to simulate real-world video conditions and learn from both local and global structures and details. This helps the model generalize better and maintain stability across varying video content while maintaining the general structure besides the pixel-wise correctness. Two model variants-one larger and one more lightweight-are presented to explore the trade-offs between performance and efficiency. The results demonstrate improved temporal coherence, sharper textures, and fewer visual artifacts compared to traditional single-image methods. This work contributes to the development of practical, learning-based solutions for video enhancement tasks, with potential applications in streaming, gaming, and digital restoration.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§åŸºäºè¶…åˆ†è¾¨ç‡ç”Ÿæˆå¯¹æŠ—ç½‘ç»œ(SRGAN)çš„è§†é¢‘å¢å¼ºæ”¹è¿›æ–¹æ³•ï¼Œé€šè¿‡å°†ä¼ ç»Ÿçš„å•å›¾åƒè¶…åˆ†è¾¨ç‡(SISR)ç»“æ„æ‰©å±•åˆ°å¤„ç†æ—¶ç©ºæ•°æ®ï¼Œæ—¨åœ¨è§£å†³è§†é¢‘åºåˆ—ä¸­çš„æ—¶é—´è¿ç»­æ€§é—®é¢˜ã€‚ä¸ºäº†å®ç°è¿™ä¸€ç›®æ ‡ï¼Œè¯¥æ¡†æ¶å¼•å…¥äº†3D Non-Local Blocksï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿæœ‰æ•ˆæ•è·ç©ºé—´å’Œæ—¶é—´ç»´åº¦çš„å…³è”ç‰¹æ€§ã€‚ç ”ç©¶é‡‡ç”¨åŸºäºåˆ†å—å­¦ä¹ (patch-wise learning)å’Œé«˜çº§æ•°æ®é™è´¨æŠ€æœ¯çš„è®­ç»ƒæµç¨‹ï¼Œä»¥æ¨¡æ‹ŸçœŸå®è§†é¢‘æ¡ä»¶å¹¶å¹³è¡¡å±€éƒ¨ç»†èŠ‚ä¸å…¨å±€ç»“æ„çš„å‡†ç¡®æ€§ã€‚ä½œè€…è¿›ä¸€æ­¥æä¾›äº†å¤§å‹å’Œè½»é‡åŒ–ä¸¤ç§æ¨¡å‹å˜ä½“ï¼Œä»¥æ¢ç´¢æ€§èƒ½ä¸æ•ˆç‡ä¹‹é—´çš„å¹³è¡¡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨æé«˜æ—¶é—´ä¸€è‡´æ€§(temporal coherence)ã€å¢å¼ºçº¹ç†æ¸…æ™°åº¦ä»¥åŠå‡å°‘è§†è§‰ä¼ªå½±æ–¹é¢æ˜¾è‘—ä¼˜äºä¼ ç»Ÿçš„å•å›¾åƒè¶…åˆ†è¾¨ç‡æ–¹æ³•ã€‚è¿™é¡¹å·¥ä½œä¸ºæµåª’ä½“ã€æ¸¸æˆå’Œæ•°å­—åŒ–ä¿®å¤ç­‰é¢†åŸŸçš„è§†é¢‘å¢å¼ºä»»åŠ¡æä¾›äº†å…·æœ‰å®é™…åº”ç”¨æ½œåŠ›çš„å­¦ä¹ å‹è§£å†³æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.CV",
        "cs.AI",
        "eess.IV"
      ],
      "primary_category": "cs.CV",
      "comment": "28 pages, 14 figures, 3 tables",
      "pdf_url": "https://arxiv.org/pdf/2505.10589v4",
      "published_date": "2025-05-14 20:16:51 UTC",
      "updated_date": "2025-06-29 18:08:46 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T08:31:59.600480+00:00"
    },
    {
      "arxiv_id": "2505.09766v2",
      "title": "On the Well-Posedness of Green's Function Reconstruction via the Kirchhoff-Helmholtz Equation for One-Speed Neutron Diffusion",
      "title_zh": "è®ºåŸºäºåŸºå°”éœå¤«-äº¥å§†éœå…¹æ–¹ç¨‹çš„å•èƒ½ä¸­å­æ‰©æ•£æ ¼æ—å‡½æ•°é‡æ„çš„é€‚å®šæ€§",
      "authors": [
        "Roberto Ponciroli"
      ],
      "abstract": "This work presents a methodology for reconstructing the spatial distribution of the neutron flux in a nuclear reactor, leveraging real-time measurements obtained from ex-core detectors. The Kirchhoff-Helmholtz (K-H) equation inherently defines the problem of estimating a scalar field within a domain based on boundary data, making it a natural mathematical framework for this task. The main challenge lies in deriving the Green's function specific to the domain and the neutron diffusion process. While analytical solutions for Green's functions exist for simplified geometries, their derivation of complex, heterogeneous domains-such as a nuclear reactor-requires a numerical approach. The objective of this work is to demonstrate the well-posedness of the data-driven Green's function approximation by formulating and solving the K-H equation as an inverse problem. After establishing the symmetry properties that the Green's function must satisfy, the K-H equation is derived from the one-speed neutron diffusion model. This is followed by a comprehensive description of the procedure for interpreting sensor readings and implementing the neutron flux reconstruction algorithm. Finally, the existence and uniqueness of the Green's function inferred from the sampled data are demonstrated, ensuring the reliability of the proposed method and its predictions.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§åˆ©ç”¨å †å¤–æ¢æµ‹å™¨ï¼ˆex-core detectorsï¼‰å®æ—¶æµ‹é‡æ•°æ®é‡å»ºæ ¸ååº”å †å†…ä¸­å­é€šé‡ï¼ˆneutron fluxï¼‰ç©ºé—´åˆ†å¸ƒçš„æ–¹æ³•ã€‚ç ”ç©¶é‡‡ç”¨ Kirchhoff-Helmholtz (K-H) æ–¹ç¨‹ä½œä¸ºæ•°å­¦æ¡†æ¶ï¼Œæ—¨åœ¨æ ¹æ®è¾¹ç•Œæ•°æ®ä¼°è®¡åŒºåŸŸå†…çš„æ ‡é‡åœºã€‚é’ˆå¯¹å¤æ‚éå‡åŒ€åŒºåŸŸä¸­ Green's function éš¾ä»¥è§£ææ¨å¯¼çš„æŒ‘æˆ˜ï¼Œä½œè€…é€šè¿‡å°† K-H æ–¹ç¨‹è¡¨è¿°ä¸ºä¸€ä¸ªé€†é—®é¢˜ï¼ˆinverse problemï¼‰ï¼Œå®ç°äº†æ•°æ®é©±åŠ¨çš„ Green's function è¿‘ä¼¼ã€‚è¯¥å·¥ä½œåŸºäºå•é€Ÿä¸­å­æ‰©æ•£æ¨¡å‹ï¼ˆone-speed neutron diffusion modelï¼‰æ¨å¯¼äº†æ–¹ç¨‹ï¼Œå¹¶è¯¦ç»†è¯´æ˜äº†ä¼ æ„Ÿå™¨è¯»æ•°è§£è¯»åŠé‡å»ºç®—æ³•çš„å®ç°æµç¨‹ã€‚ç ”ç©¶æœ€ç»ˆè¯æ˜äº†ä»é‡‡æ ·æ•°æ®ä¸­æ¨æ–­å‡ºçš„ Green's function å…·æœ‰å­˜åœ¨æ€§å’Œå”¯ä¸€æ€§ï¼Œä»è€ŒéªŒè¯äº†è¯¥æ–¹æ³•åœ¨æ•°å­¦ä¸Šçš„é€‚å®šæ€§ï¼ˆwell-posednessï¼‰ã€‚è¿™ä¸€æˆæœç¡®ä¿äº†æ‰€ææ¨¡å‹é¢„æµ‹çš„å¯é æ€§ï¼Œä¸ºæ ¸ååº”å †çŠ¶æ€ç›‘æµ‹æä¾›äº†åšå®çš„ç†è®ºåŸºç¡€ã€‚",
      "categories": [
        "math.NA",
        "cs.AI"
      ],
      "primary_category": "math.NA",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.09766v2",
      "published_date": "2025-05-14 19:53:09 UTC",
      "updated_date": "2025-05-18 15:30:46 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T08:32:00.601545+00:00"
    },
    {
      "arxiv_id": "2505.09757v2",
      "title": "Trustless Autonomy: Understanding Motivations, Benefits, and Governance Dilemmas in Self-Sovereign Decentralized AI Agents",
      "title_zh": "å»ä¿¡ä»»åŒ–è‡ªä¸»ï¼šè‡ªä¸»æƒå»ä¸­å¿ƒåŒ– AI æ™ºèƒ½ä½“çš„åŠ¨æœºã€æ”¶ç›Šä¸æ²»ç†å›°å¢ƒæ¢ç©¶",
      "authors": [
        "Botao Amber Hu",
        "Yuhan Liu",
        "Helena Rong"
      ],
      "abstract": "The recent trend of self-sovereign Decentralized AI Agents (DeAgents) combines Large Language Model (LLM)-based AI agents with decentralization technologies such as blockchain smart contracts and trusted execution environments (TEEs). These tamper-resistant trustless substrates allow agents to achieve self-sovereignty through ownership of cryptowallet private keys and control of digital assets and social media accounts. DeAgents eliminate centralized control and reduce human intervention, addressing key trust concerns inherent in centralized AI systems. This contributes to social computing by enabling new human cooperative paradigm \"intelligence as commons.\" However, given ongoing challenges in LLM reliability such as hallucinations, this creates paradoxical tension between trustlessness and unreliable autonomy. This study addresses this empirical research gap through interviews with DeAgents stakeholders-experts, founders, and developers-to examine their motivations, benefits, and governance dilemmas. The findings will guide future DeAgents system and protocol design and inform discussions about governance in sociotechnical AI systems in the future agentic web.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†è‡ªä¸»ä¸»æƒå»ä¸­å¿ƒåŒ–äººå·¥æ™ºèƒ½æ™ºèƒ½ä½“(DeAgents)çš„å…´èµ·ï¼Œè¿™ç§æ™ºèƒ½ä½“å°†åŸºäºå¤§è¯­è¨€æ¨¡å‹(LLM)çš„AIä¸åŒºå—é“¾æ™ºèƒ½åˆçº¦å’Œå¯ä¿¡æ‰§è¡Œç¯å¢ƒ(TEEs)ç­‰å»ä¸­å¿ƒåŒ–æŠ€æœ¯ç›¸ç»“åˆã€‚DeAgentsé€šè¿‡æŒæœ‰ç§é’¥å’Œç®¡ç†æ•°å­—èµ„äº§å®ç°è‡ªå†³æƒï¼Œæœ‰æ•ˆæ¶ˆé™¤äº†ä¸­å¿ƒåŒ–æ§åˆ¶å¹¶å‡å°‘äº†äººä¸ºå¹²é¢„ï¼Œä¿ƒè¿›äº†â€œæ™ºèƒ½å³å…¬åœ°(Intelligence as Commons)â€è¿™ä¸€æ–°å‹äººç±»åˆä½œèŒƒå¼ã€‚ç„¶è€Œï¼Œç ”ç©¶æ­ç¤ºäº†LLMçš„ä¸ç¡®å®šæ€§ï¼ˆå¦‚å¹»è§‰é—®é¢˜ï¼‰ä¸å»ä¸­å¿ƒåŒ–æ¶æ„ä¹‹é—´å­˜åœ¨çš„çŸ›ç›¾ã€‚é€šè¿‡å¯¹DeAgentsé¢†åŸŸä¸“å®¶ã€åˆ›å§‹äººå’Œå¼€å‘è€…çš„è®¿è°ˆï¼Œæœ¬æ–‡æ·±å…¥å‰–æäº†å…¶èƒŒåçš„åŠ¨æœºã€æ”¶ç›Šä»¥åŠæ²»ç†æŒ‘æˆ˜ã€‚è¿™äº›å®è¯å‘ç°ä¸ºæœªæ¥DeAgentsç³»ç»Ÿçš„åè®®è®¾è®¡å¥ å®šäº†åŸºç¡€ï¼Œå¹¶ä¸ºæ™ºèƒ½ä½“åŒ–ç½‘ç»œ(Agentic Web)ä¸­çš„ç¤¾ä¼šæŠ€æœ¯ç³»ç»Ÿæ²»ç†æä¾›äº†é‡è¦è§è§£ã€‚",
      "categories": [
        "cs.HC",
        "cs.AI",
        "cs.CY"
      ],
      "primary_category": "cs.HC",
      "comment": "Submitted to CSCW 2026",
      "pdf_url": "https://arxiv.org/pdf/2505.09757v2",
      "published_date": "2025-05-14 19:42:43 UTC",
      "updated_date": "2025-09-17 21:02:55 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T08:33:10.573699+00:00"
    },
    {
      "arxiv_id": "2505.09755v2",
      "title": "Explainability Through Human-Centric Design for XAI in Lung Cancer Detection",
      "title_zh": "åŸºäºä»¥äººä¸ºä¸­å¿ƒè®¾è®¡çš„è‚ºç™Œæ£€æµ‹ XAI å¯è§£é‡Šæ€§ç ”ç©¶",
      "authors": [
        "Amy Rafferty",
        "Rishi Ramaesh",
        "Ajitha Rajan"
      ],
      "abstract": "Deep learning models have shown promise in lung pathology detection from chest X-rays, but widespread clinical adoption remains limited due to opaque model decision-making. In prior work, we introduced ClinicXAI, a human-centric, expert-guided concept bottleneck model (CBM) designed for interpretable lung cancer diagnosis. We now extend that approach and present XpertXAI, a generalizable expert-driven model that preserves human-interpretable clinical concepts while scaling to detect multiple lung pathologies. Using a high-performing InceptionV3-based classifier and a public dataset of chest X-rays with radiology reports, we compare XpertXAI against leading post-hoc explainability methods and an unsupervised CBM, XCBs. We assess explanations through comparison with expert radiologist annotations and medical ground truth. Although XpertXAI is trained for multiple pathologies, our expert validation focuses on lung cancer. We find that existing techniques frequently fail to produce clinically meaningful explanations, omitting key diagnostic features and disagreeing with radiologist judgments. XpertXAI not only outperforms these baselines in predictive accuracy but also delivers concept-level explanations that better align with expert reasoning. While our focus remains on explainability in lung cancer detection, this work illustrates how human-centric model design can be effectively extended to broader diagnostic contexts - offering a scalable path toward clinically meaningful explainable AI in medical diagnostics.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†XpertXAIï¼Œä¸€ç§åŸºäºä»¥äººä¸ºæœ¬(human-centric)è®¾è®¡çš„ä¸“å®¶é©±åŠ¨æ¨¡å‹ï¼Œæ—¨åœ¨è§£å†³æ·±åº¦å­¦ä¹ åœ¨è‚ºéƒ¨ç—…ç†æ£€æµ‹ä¸­å› é»‘ç›’å†³ç­–è€Œå¯¼è‡´ä¸´åºŠåº”ç”¨å—é™çš„é—®é¢˜ã€‚XpertXAIæ‰©å±•äº†æ­¤å‰çš„ClinicXAIæ¡†æ¶ï¼Œåˆ©ç”¨InceptionV3åˆ†ç±»å™¨åœ¨ä¿ç•™äººç±»å¯è§£é‡Šä¸´åºŠæ¦‚å¿µçš„åŒæ—¶ï¼Œå®ç°äº†å¯¹å¤šç§è‚ºéƒ¨ç—…ç†çš„æ£€æµ‹èƒ½åŠ›ã€‚é€šè¿‡ä¸äº‹åè§£é‡Šæ–¹æ³•(post-hoc explainability)åŠæ— ç›‘ç£æ¦‚å¿µç“¶é¢ˆæ¨¡å‹(Concept Bottleneck Model, CBM)è¿›è¡Œå¯¹æ¯”ï¼Œå®éªŒè¡¨æ˜ç°æœ‰æŠ€æœ¯å¾€å¾€éš¾ä»¥äº§ç”Ÿå…·æœ‰ä¸´åºŠæ„ä¹‰çš„è§£é‡Šã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼ŒXpertXAIä¸ä»…åœ¨é¢„æµ‹å‡†ç¡®ç‡ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œä¸”å…¶ç”Ÿæˆçš„æ¦‚å¿µçº§è§£é‡Šä¸æ”¾å°„ç§‘ä¸“å®¶çš„ä¸´åºŠæ¨ç†é«˜åº¦å¥‘åˆã€‚è¯¥é¡¹å·¥ä½œå±•ç¤ºäº†ä»¥äººä¸ºæœ¬çš„æ¨¡å‹è®¾è®¡å¦‚ä½•æœ‰æ•ˆæ‰©å±•è‡³æ›´å¹¿æ³›çš„è¯Šæ–­åœºæ™¯ï¼Œä¸ºæ„å»ºä¸´åºŠå¯ç”¨çš„å¯è§£é‡Šäººå·¥æ™ºèƒ½(XAI)æä¾›äº†å¯æ‰©å±•çš„è·¯å¾„ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.09755v2",
      "published_date": "2025-05-14 19:40:12 UTC",
      "updated_date": "2025-05-23 10:43:45 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T08:32:41.019286+00:00"
    },
    {
      "arxiv_id": "2505.11545v1",
      "title": "TARGET: Benchmarking Table Retrieval for Generative Tasks",
      "title_zh": "TARGETï¼šé¢å‘ç”Ÿæˆå¼ä»»åŠ¡çš„è¡¨æ ¼æ£€ç´¢åŸºå‡†æµ‹è¯•",
      "authors": [
        "Xingyu Ji",
        "Parker Glenn",
        "Aditya G. Parameswaran",
        "Madelon Hulsebos"
      ],
      "abstract": "The data landscape is rich with structured data, often of high value to organizations, driving important applications in data analysis and machine learning. Recent progress in representation learning and generative models for such data has led to the development of natural language interfaces to structured data, including those leveraging text-to-SQL. Contextualizing interactions, either through conversational interfaces or agentic components, in structured data through retrieval-augmented generation can provide substantial benefits in the form of freshness, accuracy, and comprehensiveness of answers. The key question is: how do we retrieve the right table(s) for the analytical query or task at hand? To this end, we introduce TARGET: a benchmark for evaluating TAble Retrieval for GEnerative Tasks. With TARGET we analyze the retrieval performance of different retrievers in isolation, as well as their impact on downstream tasks. We find that dense embedding-based retrievers far outperform a BM25 baseline which is less effective than it is for retrieval over unstructured text. We also surface the sensitivity of retrievers across various metadata (e.g., missing table titles), and demonstrate a stark variation of retrieval performance across datasets and tasks. TARGET is available at https://target-benchmark.github.io.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†TARGETï¼Œè¿™æ˜¯ä¸€ä¸ªä¸“ä¸ºè¯„ä¼°ç”Ÿæˆå¼ä»»åŠ¡ä¸­çš„è¡¨æ ¼æ£€ç´¢(Table Retrieval)è€Œè®¾è®¡çš„åŸºå‡†æµ‹è¯•ã€‚ç ”ç©¶å›¢é˜Ÿé’ˆå¯¹ç»“æ„åŒ–æ•°æ®åœ¨æ•°æ®åˆ†æå’Œæœºå™¨å­¦ä¹ ä¸­çš„é‡è¦æ€§ï¼Œæ¢è®¨äº†å¦‚ä½•é€šè¿‡æ£€ç´¢å¢å¼ºç”Ÿæˆ(RAG)å’Œtext-to-SQLç­‰æŠ€æœ¯ä¸ºè¿™ç±»æ•°æ®æä¾›é«˜æ•ˆçš„è‡ªç„¶è¯­è¨€æ¥å£ã€‚é€šè¿‡TARGETï¼Œç ”ç©¶äººå‘˜å¯¹æ¯”äº†ä¸åŒæ£€ç´¢å™¨çš„è¡¨ç°ï¼Œå‘ç°åŸºäºç¨ å¯†å‘é‡åµŒå…¥(dense embedding-based)çš„æ–¹æ³•åœ¨æ€§èƒ½ä¸Šè¿œè¶…ä¼ ç»Ÿçš„BM25åŸºå‡†ã€‚å®éªŒç»“æœè¿›ä¸€æ­¥æ­ç¤ºäº†æ£€ç´¢å™¨å¯¹è¡¨æ ¼æ ‡é¢˜ç¼ºå¤±ç­‰å…ƒæ•°æ®çš„æ•æ„Ÿæ€§ï¼Œå¹¶æŒ‡å‡ºäº†æ£€ç´¢æ•ˆæœåœ¨ä¸åŒæ•°æ®é›†å’Œä»»åŠ¡ä¸­å­˜åœ¨æ˜¾è‘—æ³¢åŠ¨ã€‚è¯¥åŸºå‡†æµ‹è¯•ç›®å‰å·²å…¬å¼€å‘å¸ƒï¼Œæ—¨åœ¨å¸®åŠ©å¼€å‘è€…æ›´å‡†ç¡®åœ°ä¸ºç‰¹å®šåˆ†ææŸ¥è¯¢æˆ–ä»»åŠ¡æ£€ç´¢åˆé€‚çš„è¡¨æ ¼ã€‚",
      "categories": [
        "cs.IR",
        "cs.AI",
        "cs.CL",
        "cs.DB"
      ],
      "primary_category": "cs.IR",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.11545v1",
      "published_date": "2025-05-14 19:39:46 UTC",
      "updated_date": "2025-05-14 19:39:46 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T08:32:18.031650+00:00"
    },
    {
      "arxiv_id": "2505.09747v1",
      "title": "Healthy Distrust in AI systems",
      "title_zh": "å¯¹äººå·¥æ™ºèƒ½ç³»ç»Ÿçš„å¥åº·ä¸ä¿¡ä»»",
      "authors": [
        "Benjamin PaaÃŸen",
        "Suzana Alpsancar",
        "Tobias Matzner",
        "Ingrid Scharlau"
      ],
      "abstract": "Under the slogan of trustworthy AI, much of contemporary AI research is focused on designing AI systems and usage practices that inspire human trust and, thus, enhance adoption of AI systems. However, a person affected by an AI system may not be convinced by AI system design alone -- neither should they, if the AI system is embedded in a social context that gives good reason to believe that it is used in tension with a person's interest. In such cases, distrust in the system may be justified and necessary to build meaningful trust in the first place. We propose the term \"healthy distrust\" to describe such a justified, careful stance towards certain AI usage practices. We investigate prior notions of trust and distrust in computer science, sociology, history, psychology, and philosophy, outline a remaining gap that healthy distrust might fill and conceptualize healthy distrust as a crucial part for AI usage that respects human autonomy.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†å½“å‰ AI é¢†åŸŸä¸­è¿‡åº¦è¿½æ±‚æ„å»º trustworthy AI ä»¥æå‡ç³»ç»Ÿé‡‡ç”¨ç‡çš„ç°çŠ¶ï¼Œå¹¶æŒ‡å‡ºå•çº¯çš„ç³»ç»Ÿè®¾è®¡ä¸è¶³ä»¥åº”å¯¹å¤æ‚ç¤¾ä¼šèƒŒæ™¯å¼•å‘çš„åˆ©ç›Šå†²çªã€‚ä½œè€…æå‡ºäº† healthy distrust æ¦‚å¿µï¼Œç”¨äºæè¿°ç”¨æˆ·å¯¹ç‰¹å®š AI ä½¿ç”¨å®è·µæ‰€é‡‡å–çš„ä¸€ç§åˆç†çš„ã€å®¡æ…çš„ç«‹åœºã€‚é€šè¿‡æ¢³ç†è®¡ç®—æœºç§‘å­¦ã€ç¤¾ä¼šå­¦ã€å†å²å­¦ã€å¿ƒç†å­¦å’Œå“²å­¦ä¸­å…³äº trust å’Œ distrust çš„æ—¢æœ‰å®šä¹‰ï¼Œæœ¬æ–‡å¡«è¡¥äº†ç›¸å…³ç†è®ºç ”ç©¶çš„ç©ºç™½ã€‚æœ€ç»ˆï¼Œç ”ç©¶å°† healthy distrust è§†ä¸ºå°Šé‡ human autonomy çš„ AI åº”ç”¨æ¡†æ¶ä¸­ä¸å¯æˆ–ç¼ºçš„ä¸€éƒ¨åˆ†ï¼Œå¼ºè°ƒäº†åˆç†æ€€ç–‘åœ¨æ„å»ºæ·±å±‚æ¬¡ã€æœ‰æ„ä¹‰çš„ä¿¡ä»»ä¸­çš„å¿…è¦æ€§ã€‚",
      "categories": [
        "cs.CY",
        "cs.AI"
      ],
      "primary_category": "cs.CY",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.09747v1",
      "published_date": "2025-05-14 19:13:47 UTC",
      "updated_date": "2025-05-14 19:13:47 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T08:32:25.907468+00:00"
    },
    {
      "arxiv_id": "2505.09742v2",
      "title": "A Generative Neural Annealer for Black-Box Combinatorial Optimization",
      "title_zh": "é¢å‘é»‘ç›’ç»„åˆä¼˜åŒ–çš„ç”Ÿæˆå¼ç¥ç»é€€ç«å™¨",
      "authors": [
        "Yuan-Hang Zhang",
        "Massimiliano Di Ventra"
      ],
      "abstract": "We propose a generative, end-to-end solver for black-box combinatorial optimization that emphasizes both sample efficiency and solution quality on NP problems. Drawing inspiration from annealing-based algorithms, we treat the black-box objective as an energy function and train a neural network to model the associated Boltzmann distribution. By conditioning on temperature, the network captures a continuum of distributions--from near-uniform at high temperatures to sharply peaked around global optima at low temperatures--thereby learning the structure of the energy landscape and facilitating global optimization. When queries are expensive, the temperature-dependent distributions naturally enable data augmentation and improve sample efficiency. When queries are cheap but the problem remains hard, the model learns implicit variable interactions, effectively \"opening\" the black box. We validate our approach on challenging combinatorial tasks under both limited and unlimited query budgets, showing competitive performance against state-of-the-art black-box optimizers.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§åä¸º Generative Neural Annealer çš„ç”Ÿæˆå¼ç«¯åˆ°ç«¯ solverï¼Œä¸“é—¨ç”¨äºè§£å†³å…¼é¡¾æ ·æœ¬æ•ˆç‡å’Œè§£è´¨é‡çš„é»‘ç›’ç»„åˆä¼˜åŒ– (Black-Box Combinatorial Optimization) é—®é¢˜ã€‚å—æ¨¡æ‹Ÿé€€ç«ç®—æ³•å¯å‘ï¼Œè¯¥æ–¹æ³•å°†é»‘ç›’ç›®æ ‡å‡½æ•°è§†ä¸ºèƒ½é‡å‡½æ•°ï¼Œå¹¶è®­ç»ƒç¥ç»ç½‘ç»œæ¨¡å‹æ¥æ¨¡æ‹Ÿç›¸å…³çš„ Boltzmann distributionã€‚é€šè¿‡å°†åˆ†å¸ƒå‚æ•°åŒ–ä¸ºæ¸©åº¦çš„å‡½æ•°ï¼Œç½‘ç»œèƒ½å¤Ÿæ•æ‰ä»é«˜æ¸©å‡åŒ€åˆ†å¸ƒåˆ°ä½æ¸©å…¨å±€æœ€ä¼˜åˆ†å¸ƒçš„è¿ç»­æ¼”åŒ–è¿‡ç¨‹ï¼Œä»è€Œæœ‰æ•ˆå­¦ä¹ èƒ½é‡åœ°å½¢ (energy landscape) çš„ç»“æ„ä»¥å®ç°å…¨å±€ä¼˜åŒ–ã€‚åœ¨æŸ¥è¯¢æˆæœ¬é«˜æ˜‚æ—¶ï¼Œè¿™ç§æ¸©åº¦ä¾èµ–åˆ†å¸ƒæ”¯æŒæ•°æ®å¢å¼ºå¹¶æå‡æ ·æœ¬æ•ˆç‡ï¼Œè€Œåœ¨å¤æ‚é—®é¢˜ä¸­åˆ™èƒ½é€šè¿‡å­¦ä¹ éšå¼å˜é‡äº¤äº’æ¥â€œæ‰“å¼€â€é»‘ç›’ã€‚å®éªŒéªŒè¯è¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨ä¸åŒæŸ¥è¯¢é¢„ç®—ä¸‹çš„æŒ‘æˆ˜æ€§ç»„åˆä»»åŠ¡ä¸­å‡è¡¨ç°ä¼˜å¼‚ï¼Œå…¶æ€§èƒ½å¯ä¸å½“å‰æœ€å…ˆè¿›çš„ SOTA é»‘ç›’ä¼˜åŒ–å™¨ç›¸åª²ç¾ã€‚",
      "categories": [
        "cs.LG",
        "cond-mat.dis-nn",
        "cond-mat.stat-mech",
        "cs.AI",
        "cs.NE"
      ],
      "primary_category": "cs.LG",
      "comment": "10 pages, 3 figures",
      "pdf_url": "https://arxiv.org/pdf/2505.09742v2",
      "published_date": "2025-05-14 19:05:19 UTC",
      "updated_date": "2025-08-05 18:14:19 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T08:32:33.332418+00:00"
    },
    {
      "arxiv_id": "2505.09738v1",
      "title": "Achieving Tokenizer Flexibility in Language Models through Heuristic Adaptation and Supertoken Learning",
      "title_zh": "é€šè¿‡å¯å‘å¼é€‚é…ä¸è¶…åˆ†è¯å­¦ä¹ å®ç°è¯­è¨€æ¨¡å‹åˆ†è¯å™¨çš„çµæ´»æ€§",
      "authors": [
        "Shaurya Sharthak",
        "Vinayak Pahalwan",
        "Adithya Kamath",
        "Adarsh Shirawalmath"
      ],
      "abstract": "Pretrained language models (LLMs) are often constrained by their fixed tokenization schemes, leading to inefficiencies and performance limitations, particularly for multilingual or specialized applications. This tokenizer lock-in presents significant challenges. standard methods to overcome this often require prohibitive computational resources. Although tokenizer replacement with heuristic initialization aims to reduce this burden, existing methods often require exhaustive residual fine-tuning and still may not fully preserve semantic nuances or adequately address the underlying compression inefficiencies. Our framework introduces two innovations: first, Tokenadapt, a model-agnostic tokenizer transplantation method, and second, novel pre-tokenization learning for multi-word Supertokens to enhance compression and reduce fragmentation. Tokenadapt initializes new unique token embeddings via a hybrid heuristic that combines two methods: a local estimate based on subword decomposition using the old tokenizer, and a global estimate utilizing the top-k semantically similar tokens from the original vocabulary. This methodology aims to preserve semantics while significantly minimizing retraining requirements. Empirical investigations validate both contributions: the transplantation heuristic successfully initializes unique tokens, markedly outperforming conventional baselines and sophisticated methods including Transtokenizer and ReTok, while our Supertokens achieve notable compression gains. Our zero-shot perplexity results demonstrate that the TokenAdapt hybrid initialization consistently yields lower perplexity ratios compared to both ReTok and TransTokenizer baselines across different base models and newly trained target tokenizers. TokenAdapt typically reduced the overall perplexity ratio significantly compared to ReTok, yielding at least a 2-fold improvement in these aggregate scores.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹é¢„è®­ç»ƒè¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å› å›ºå®šåˆ†è¯æ–¹æ¡ˆï¼ˆtokenizerï¼‰å¯¼è‡´çš„æ•ˆç‡å’Œæ€§èƒ½å±€é™ï¼Œæå‡ºäº†TokenAdaptå’ŒSupertoken Learningä¸¤ç§åˆ›æ–°æ–¹æ³•ã€‚TokenAdaptä½œä¸ºä¸€ç§æ¨¡å‹æ— å…³çš„Tokenizer transplantationæŠ€æœ¯ï¼Œé€šè¿‡ç»“åˆåŸºäºå­è¯åˆ†è§£ï¼ˆsubword decompositionï¼‰çš„å±€éƒ¨ä¼°è®¡å’ŒåŸºäºè¯­ä¹‰ç›¸ä¼¼æ€§çš„å…¨å±€ä¼°è®¡æ¥åˆå§‹åŒ–TokenåµŒå…¥ï¼Œæ—¨åœ¨ä¿ç•™è¯­ä¹‰ç²¾é«“çš„åŒæ—¶æœ€å°åŒ–é‡è®­æˆæœ¬ã€‚ä¸æ­¤åŒæ—¶ï¼Œç ”ç©¶å¼•å…¥äº†é’ˆå¯¹å¤šè¯Supertokensçš„é¢„åˆ†è¯å­¦ä¹ ï¼Œæœ‰æ•ˆæå‡äº†æ–‡æœ¬å‹ç¼©ç‡å¹¶å‡å°‘äº†åˆ†è¯ç¢ç‰‡åŒ–ç°è±¡ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒTokenAdaptåœ¨é›¶æ ·æœ¬å›°æƒ‘åº¦ï¼ˆzero-shot perplexityï¼‰ç­‰å…³é”®æŒ‡æ ‡ä¸Šæ˜¾è‘—ä¼˜äºTransTokenizerå’ŒReTokç­‰ç°æœ‰åŸºçº¿ï¼Œåœ¨ç»¼åˆå¾—åˆ†ä¸Šå®ç°äº†è‡³å°‘2å€çš„æ€§èƒ½æå‡ã€‚è¯¥æ¡†æ¶ä¸ºåœ¨ä¸åŒåº”ç”¨åœºæ™¯ä¸‹å®ç°çµæ´»ä¸”é«˜æ•ˆçš„åˆ†è¯å™¨é€‚é…æä¾›äº†å¼ºæœ‰åŠ›çš„æŠ€æœ¯æ”¯æ’‘ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.09738v1",
      "published_date": "2025-05-14 19:00:27 UTC",
      "updated_date": "2025-05-14 19:00:27 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T08:33:14.840929+00:00"
    },
    {
      "arxiv_id": "2505.09737v2",
      "title": "General Dynamic Goal Recognition using Goal-Conditioned and Meta Reinforcement Learning",
      "title_zh": "åŸºäºç›®æ ‡æ¡ä»¶å¼ºåŒ–å­¦ä¹ ä¸å…ƒå¼ºåŒ–å­¦ä¹ çš„é€šç”¨åŠ¨æ€ç›®æ ‡è¯†åˆ«",
      "authors": [
        "Osher Elhadad",
        "Owen Morrissey",
        "Reuth Mirsky"
      ],
      "abstract": "Understanding an agent's goal through its behavior is a common AI problem called Goal Recognition (GR). This task becomes particularly challenging in dynamic environments where goals are numerous and ever-changing. We introduce the General Dynamic Goal Recognition (GDGR) problem, a broader definition of GR aimed at real-time adaptation of GR systems. This paper presents two novel approaches to tackle GDGR: (1) GC-AURA, generalizing to new goals using Model-Free Goal-Conditioned Reinforcement Learning, and (2) Meta-AURA, adapting to novel environments with Meta-Reinforcement Learning. We evaluate these methods across diverse environments, demonstrating their ability to achieve rapid adaptation and high GR accuracy under dynamic and noisy conditions. This work is a significant step forward in enabling GR in dynamic and unpredictable real-world environments.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹åœ¨ç›®æ ‡ä¼—å¤šä¸”ä¸æ–­å˜åŒ–çš„åŠ¨æ€ç¯å¢ƒä¸­è¿›è¡Œç›®æ ‡è¯†åˆ«(Goal Recognition, GR)çš„æŒ‘æˆ˜ï¼Œæå‡ºäº†é€šç”¨åŠ¨æ€ç›®æ ‡è¯†åˆ«(General Dynamic Goal Recognition, GDGR)é—®é¢˜ï¼Œæ—¨åœ¨æå‡ç³»ç»Ÿçš„å®æ—¶é€‚åº”æ€§ã€‚è®ºæ–‡æå‡ºäº†ä¸¤ç§æ ¸å¿ƒæ–¹æ³•ï¼šGC-AURA åˆ©ç”¨æ— æ¨¡å‹çš„ç›®æ ‡æ¡ä»¶å¼ºåŒ–å­¦ä¹ (Model-Free Goal-Conditioned Reinforcement Learning)æ¥å®ç°å¯¹æ–°ç›®æ ‡çš„æ³›åŒ–ï¼Œè€Œ Meta-AURA åˆ™é€šè¿‡å…ƒå¼ºåŒ–å­¦ä¹ (Meta-Reinforcement Learning)ä½¿ç³»ç»Ÿèƒ½å¤Ÿé€‚åº”å…¨æ–°çš„ç¯å¢ƒã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¿™äº›æ–¹æ³•åœ¨å¤šç§åŠ¨æ€å’Œå™ªå£°ç¯å¢ƒä¸‹å‡èƒ½ä¿æŒæé«˜çš„è¯†åˆ«å‡†ç¡®ç‡ï¼Œå¹¶å®ç°å¿«é€Ÿçš„ç¯å¢ƒé€‚åº”ã€‚è¿™é¡¹ç ”ç©¶æ˜¾è‘—æ¨è¿›äº†ç›®æ ‡è¯†åˆ«æŠ€æœ¯åœ¨ä¸å¯é¢„æµ‹çš„çœŸå®ä¸–ç•Œåœºæ™¯ä¸­çš„åº”ç”¨æ½œåŠ›ã€‚",
      "categories": [
        "cs.AI",
        "cs.RO"
      ],
      "primary_category": "cs.AI",
      "comment": "Accepted for publication at AAMAS 2026",
      "pdf_url": "https://arxiv.org/pdf/2505.09737v2",
      "published_date": "2025-05-14 18:57:51 UTC",
      "updated_date": "2026-01-04 16:58:04 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T08:35:14.242761+00:00"
    },
    {
      "arxiv_id": "2505.09733v1",
      "title": "Robust Federated Learning with Confidence-Weighted Filtering and GAN-Based Completion under Noisy and Incomplete Data",
      "title_zh": "å™ªå£°ä¸ä¸å®Œæ•´æ•°æ®ä¸‹åŸºäºç½®ä¿¡åº¦åŠ æƒè¿‡æ»¤å’ŒGANè¡¥å…¨çš„é²æ£’è”é‚¦å­¦ä¹ ",
      "authors": [
        "Alpaslan Gokcen",
        "Ali Boyaci"
      ],
      "abstract": "Federated learning (FL) presents an effective solution for collaborative model training while maintaining data privacy across decentralized client datasets. However, data quality issues such as noisy labels, missing classes, and imbalanced distributions significantly challenge its effectiveness. This study proposes a federated learning methodology that systematically addresses data quality issues, including noise, class imbalance, and missing labels. The proposed approach systematically enhances data integrity through adaptive noise cleaning, collaborative conditional GAN-based synthetic data generation, and robust federated model training. Experimental evaluations conducted on benchmark datasets (MNIST and Fashion-MNIST) demonstrate significant improvements in federated model performance, particularly macro-F1 Score, under varying noise and class imbalance conditions. Additionally, the proposed framework carefully balances computational feasibility and substantial performance gains, ensuring practicality for resource constrained edge devices while rigorously maintaining data privacy. Our results indicate that this method effectively mitigates common data quality challenges, providing a robust, scalable, and privacy compliant solution suitable for diverse real-world federated learning scenarios.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹è”é‚¦å­¦ä¹ (Federated Learning)åœ¨å¤„ç†å™ªå£°æ ‡ç­¾ã€ç±»åˆ«ç¼ºå¤±å’Œæ•°æ®åˆ†å¸ƒä¸å¹³è¡¡ç­‰è´¨é‡é—®é¢˜æ—¶çš„å±€é™æ€§ï¼Œæå‡ºäº†ä¸€ç§ç»“åˆç½®ä¿¡åº¦åŠ æƒè¿‡æ»¤ä¸ç”Ÿæˆå¯¹æŠ—ç½‘ç»œ(GAN)è¡¥å…¨çš„é²æ£’æ€§æ¡†æ¶ã€‚è¯¥æ–¹æ³•é€šè¿‡è‡ªé€‚åº”å™ªå£°æ¸…ç†(adaptive noise cleaning)å¢å¼ºæ•°æ®å®Œæ•´æ€§ï¼Œå¹¶åˆ©ç”¨åä½œå¼æ¡ä»¶ç”Ÿæˆå¯¹æŠ—ç½‘ç»œ(collaborative conditional GAN)ç”Ÿæˆåˆæˆæ•°æ®ä»¥å¡«è¡¥ç¼ºå¤±ç±»åˆ«ã€‚å®éªŒåœ¨MNISTå’ŒFashion-MNISTåŸºå‡†æ•°æ®é›†ä¸Šè¿›è¡Œï¼Œç»“æœæ˜¾ç¤ºè¯¥æ¡†æ¶åœ¨å„ç§å™ªå£°å’Œä¸å¹³è¡¡æ¡ä»¶ä¸‹æ˜¾è‘—æå‡äº†å®F1åˆ†æ•°(macro-F1 Score)ã€‚è¯¥æ–¹æ¡ˆåœ¨æå‡æ€§èƒ½çš„åŒæ—¶å…¼é¡¾äº†è®¡ç®—å¯è¡Œæ€§ï¼Œç¡®ä¿äº†åœ¨èµ„æºå—é™çš„è¾¹ç¼˜è®¾å¤‡(edge devices)ä¸Šçš„å®ç”¨æ€§å¹¶ä¸¥æ ¼ç»´æŠ¤æ•°æ®éšç§ã€‚æ€»çš„æ¥çœ‹ï¼Œè¯¥æ–¹æ³•ä¸ºç°å®ä¸–ç•Œä¸­å¤æ‚çš„è”é‚¦å­¦ä¹ åœºæ™¯æä¾›äº†ä¸€ç§é²æ£’ã€å¯æ‰©å±•ä¸”ç¬¦åˆéšç§åˆè§„è¦æ±‚çš„è§£å†³æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CR"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.09733v1",
      "published_date": "2025-05-14 18:49:18 UTC",
      "updated_date": "2025-05-14 18:49:18 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T08:33:46.079918+00:00"
    },
    {
      "arxiv_id": "2505.09724v2",
      "title": "An AI-Powered Research Assistant in the Lab: A Practical Guide for Text Analysis Through Iterative Collaboration with LLMs",
      "title_zh": "å®éªŒå®¤ AI é©±åŠ¨ç ”ç©¶åŠ©æ‰‹ï¼šåŸºäºä¸å¤§è¯­è¨€æ¨¡å‹è¿­ä»£åä½œçš„æ–‡æœ¬åˆ†æå®ç”¨æŒ‡å—",
      "authors": [
        "Gino Carmona-DÃ­az",
        "William JimÃ©nez-Leal",
        "MarÃ­a Alejandra Grisales",
        "Chandra Sripada",
        "Santiago Amaya",
        "Michael Inzlicht",
        "Juan Pablo BermÃºdez"
      ],
      "abstract": "Analyzing texts such as open-ended responses, headlines, or social media posts is a time- and labor-intensive process highly susceptible to bias. LLMs are promising tools for text analysis, using either a predefined (top-down) or a data-driven (bottom-up) taxonomy, without sacrificing quality. Here we present a step-by-step tutorial to efficiently develop, test, and apply taxonomies for analyzing unstructured data through an iterative and collaborative process between researchers and LLMs. Using personal goals provided by participants as an example, we demonstrate how to write prompts to review datasets and generate a taxonomy of life domains, evaluate and refine the taxonomy through prompt and direct modifications, test the taxonomy and assess intercoder agreements, and apply the taxonomy to categorize an entire dataset with high intercoder reliability. We discuss the possibilities and limitations of using LLMs for text analysis.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹æ–‡æœ¬åˆ†æä¸­è€—æ—¶è´¹åŠ›ä¸”æ˜“äº§ç”Ÿåè§çš„é—®é¢˜ï¼Œæå‡ºäº†ä¸€ä¸ªåœ¨å®éªŒå®¤ç¯å¢ƒä¸‹åˆ©ç”¨å¤§è¯­è¨€æ¨¡å‹(LLMs)è¿›è¡Œæ–‡æœ¬åˆ†æçš„å®è·µæŒ‡å—ã€‚è¯¥æŒ‡å—è¯¦ç»†ä»‹ç»äº†ä¸€å¥—é€šè¿‡ç ”ç©¶è€…ä¸LLMsè¿­ä»£åä½œæ¥å¼€å‘ã€æµ‹è¯•å’Œåº”ç”¨åˆ†ç±»ä½“ç³»(taxonomies)å¤„ç†éç»“æ„åŒ–æ•°æ®çš„åˆ†æ­¥æ•™ç¨‹ã€‚ä½œè€…ä»¥ä¸ªäººç›®æ ‡æ•°æ®é›†ä¸ºä¾‹ï¼Œæ¼”ç¤ºäº†å¦‚ä½•é€šè¿‡ç¼–å†™æç¤ºè¯(prompts)ç”Ÿæˆåˆ†ç±»ã€é€šè¿‡ç›´æ¥ä¿®æ”¹è¿›è¡Œè¯„ä¼°ç²¾ç‚¼ï¼Œå¹¶åˆ©ç”¨ç¼–ç è€…é—´ä¿¡åº¦(intercoder agreements)æµ‹è¯•æ¥ç¡®ä¿åˆ†ç±»è´¨é‡ã€‚å®éªŒè¯æ˜ï¼Œè¯¥åä½œæ¨¡å¼èƒ½ä»¥æé«˜çš„ç¼–ç è€…å¯é æ€§(intercoder reliability)å®Œæˆå…¨é‡æ•°æ®é›†çš„åˆ†ç±»ä»»åŠ¡ã€‚æ­¤å¤–ï¼Œè¯¥ç ”ç©¶è¿˜æ·±å…¥æ¢è®¨äº†åœ¨æ–‡æœ¬åˆ†æä¸­ä½¿ç”¨LLMsçš„å¯èƒ½æ€§ä¸å±€é™æ€§ï¼Œä¸ºé‡‡ç”¨Top-downæˆ–Bottom-upè·¯å¾„çš„ç ”ç©¶è€…æä¾›äº†ç³»ç»Ÿæ€§çš„å‚è€ƒã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.HC"
      ],
      "primary_category": "cs.CL",
      "comment": "31 pages, 1 figure",
      "pdf_url": "https://arxiv.org/pdf/2505.09724v2",
      "published_date": "2025-05-14 18:32:18 UTC",
      "updated_date": "2025-05-16 11:47:10 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T08:34:12.323394+00:00"
    },
    {
      "arxiv_id": "2506.11020v1",
      "title": "Extracting Knowledge Graphs from User Stories using LangChain",
      "title_zh": "åŸºäº LangChain çš„ç”¨æˆ·æ•…äº‹çŸ¥è¯†å›¾è°±æå–",
      "authors": [
        "ThaynÃ¡ Camargo da Silva"
      ],
      "abstract": "This thesis introduces a novel methodology for the automated generation of knowledge graphs from user stories by leveraging the advanced capabilities of Large Language Models. Utilizing the LangChain framework as a basis, the User Story Graph Transformer module was developed to extract nodes and relationships from user stories using an LLM to construct accurate knowledge graphs.This innovative technique was implemented in a script to fully automate the knowledge graph extraction process. Additionally, the evaluation was automated through a dedicated evaluation script, utilizing an annotated dataset for assessment. By enhancing the visualization and understanding of user requirements and domain concepts, this method fosters better alignment between software functionalities and user expectations, ultimately contributing to more effective and user-centric software development processes.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§åˆ©ç”¨ Large Language Models ä» User Stories ä¸­è‡ªåŠ¨ç”Ÿæˆ Knowledge Graphs çš„æ–°æ–¹æ³•ï¼Œæ—¨åœ¨æé«˜è½¯ä»¶éœ€æ±‚çš„å¯è§†åŒ–ä¸ç†è§£æ°´å¹³ã€‚åŸºäº LangChain æ¡†æ¶ï¼Œç ”ç©¶è€…å¼€å‘äº† User Story Graph Transformer æ¨¡å—ï¼Œåˆ©ç”¨ LLM ä» User Stories ä¸­æå–èŠ‚ç‚¹å’Œå…³ç³»ï¼Œå¹¶æ„å»ºå‡ºå‡†ç¡®çš„ Knowledge Graphsã€‚è¯¥ç ”ç©¶é€šè¿‡ç‰¹å®šè„šæœ¬å®ç°äº†æå–è¿‡ç¨‹çš„å®Œå…¨è‡ªåŠ¨åŒ–ï¼Œå¹¶åˆ©ç”¨æ ‡æ³¨æ•°æ®é›†å’Œä¸“ç”¨è¯„ä¼°è„šæœ¬å®Œæˆäº†è‡ªåŠ¨åŒ–è¯„ä¼°ã€‚å®éªŒè¯æ˜ï¼Œè¯¥æ–¹æ³•èƒ½æœ‰æ•ˆä¿ƒè¿›è½¯ä»¶åŠŸèƒ½ä¸ç”¨æˆ·æœŸæœ›ä¹‹é—´çš„å¯¹é½ï¼Œä»è€Œæ˜¾è‘—æå‡ä»¥ç”¨æˆ·ä¸ºä¸­å¿ƒçš„è½¯ä»¶å¼€å‘æ•ˆç‡ä¸è´¨é‡ã€‚",
      "categories": [
        "cs.SE",
        "cs.AI"
      ],
      "primary_category": "cs.SE",
      "comment": "Master thesis work",
      "pdf_url": "https://arxiv.org/pdf/2506.11020v1",
      "published_date": "2025-05-14 18:25:58 UTC",
      "updated_date": "2025-05-14 18:25:58 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T08:33:38.870090+00:00"
    },
    {
      "arxiv_id": "2505.18174v3",
      "title": "NMCSE: Noise-Robust Multi-Modal Coupling Signal Estimation Method via Optimal Transport for Cardiovascular Disease Detection",
      "title_zh": "NMCSEï¼šåŸºäºæœ€ä¼˜ä¼ è¾“çš„å¿ƒè¡€ç®¡ç–¾ç—…æ£€æµ‹æŠ—å™ªå£°å¤šæ¨¡æ€è€¦åˆä¿¡å·ä¼°è®¡æ–¹æ³•",
      "authors": [
        "Peihong Zhang",
        "Zhixin Li",
        "Rui Sang",
        "Yuxuan Liu",
        "Yiqiang Cai",
        "Yizhou Tan",
        "Shengchen Li"
      ],
      "abstract": "The coupling signal refers to a latent physiological signal that characterizes the transformation from cardiac electrical excitation, captured by the electrocardiogram (ECG), to mechanical contraction, recorded by the phonocardiogram (PCG). By encoding the temporal and functional interplay between electrophysiological and hemodynamic events, it serves as an intrinsic link between modalities and offers a unified representation of cardiac function, with strong potential to enhance multi-modal cardiovascular disease (CVD) detection. However, existing coupling signal estimation methods remain highly vulnerable to noise, particularly in real-world clinical and physiological settings, which undermines their robustness and limits practical value. In this study, we propose Noise-Robust Multi-Modal Coupling Signal Estimation (NMCSE), which reformulates coupling signal estimation as a distribution matching problem solved via optimal transport. By jointly aligning amplitude and timing, NMCSE avoids noise amplification and enables stable signal estimation. When integrated into a Temporal-Spatial Feature Extraction (TSFE) network, the estimated coupling signal effectively enhances multi-modal fusion for more accurate CVD detection. To evaluate robustness under real-world conditions, we design two complementary experiments targeting distinct sources of noise. The first uses the PhysioNet 2016 dataset with simulated hospital noise to assess the resilience of NMCSE to clinical interference. The second leverages the EPHNOGRAM dataset with motion-induced physiological noise to evaluate intra-state estimation stability across activity levels. Experimental results show that NMCSE consistently outperforms existing methods under both clinical and physiological noise, highlighting it as a noise-robust estimation approach that enables reliable multi-modal cardiac detection in real-world conditions.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§åä¸ºNMCSEçš„å™ªå£°é²æ£’å¤šæ¨¡æ€è€¦åˆä¿¡å·ä¼°è®¡æ–¹æ³•ï¼Œæ—¨åœ¨è§£å†³å¿ƒè¡€ç®¡ç–¾ç—…(CVD)æ£€æµ‹ä¸­ç°æœ‰æ–¹æ³•æ˜“å—ç°å®ä¸´åºŠä¸ç”Ÿç†å™ªå£°å¹²æ‰°çš„é—®é¢˜ã€‚è€¦åˆä¿¡å·ä½œä¸ºè¿æ¥å¿ƒç”µå›¾(ECG)ç”µæ¿€å‘ä¸å¿ƒéŸ³å›¾(PCG)æœºæ¢°æ”¶ç¼©çš„æ½œåœ¨ç”Ÿç†ä¿¡å·ï¼Œèƒ½å¤Ÿä¸ºå¿ƒè„åŠŸèƒ½æä¾›ç»Ÿä¸€çš„ç‰¹å¾è¡¨å¾ã€‚NMCSEå°†ä¼°è®¡è¿‡ç¨‹é‡æ–°å®šä¹‰ä¸ºåŸºäºæœ€ä¼˜ä¼ è¾“(Optimal Transport)çš„åˆ†å¸ƒåŒ¹é…é—®é¢˜ï¼Œé€šè¿‡è”åˆå¯¹é½æŒ¯å¹…ä¸æ—¶åºæ¥é¿å…å™ªå£°æ”¾å¤§ï¼Œç¡®ä¿äº†ä¿¡å·ä¼°è®¡çš„ç¨³å®šæ€§ã€‚å°†è¯¥æ–¹æ³•é›†æˆåˆ°æ—¶ç©ºç‰¹å¾æå–(TSFE)ç½‘ç»œåï¼Œèƒ½æœ‰æ•ˆé€šè¿‡è€¦åˆä¿¡å·å¢å¼ºå¤šæ¨¡æ€èåˆæ€§èƒ½ã€‚åœ¨PhysioNet 2016åŒ»é™¢ç¯å¢ƒå™ªå£°æ•°æ®é›†å’ŒEPHNOGRAMè¿åŠ¨ç”Ÿç†å™ªå£°æ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼ŒNMCSEçš„é²æ£’æ€§æ˜¾è‘—ä¼˜äºç°æœ‰æŠ€æœ¯ã€‚è¯¥ç ”ç©¶è¯æ˜äº†è¯¥æ–¹æ³•åœ¨ç°å®å¤æ‚æ¡ä»¶ä¸‹å®ç°å¯é å¤šæ¨¡æ€å¿ƒè„æ£€æµ‹çš„å·¨å¤§æ½œåŠ›ä¸å®ç”¨ä»·å€¼ã€‚",
      "categories": [
        "eess.SP",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "eess.SP",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.18174v3",
      "published_date": "2025-05-14 18:25:43 UTC",
      "updated_date": "2025-11-04 09:18:02 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T08:34:17.408423+00:00"
    },
    {
      "arxiv_id": "2505.09716v2",
      "title": "Out-of-distribution generalisation is hard: evidence from ARC-like tasks",
      "title_zh": "åˆ†å¸ƒå¤–æ³›åŒ–ä¹‹éš¾ï¼šæ¥è‡ªç±» ARC ä»»åŠ¡çš„å®è¯è¯æ®",
      "authors": [
        "George Dimitriadis",
        "Spyridon Samothrakis"
      ],
      "abstract": "Out-of-distribution (OOD) generalisation is considered a hallmark of human and animal intelligence. To achieve OOD through composition, a system must discover the environment-invariant properties of experienced input-output mappings and transfer them to novel inputs. This can be realised if an intelligent system can identify appropriate, task-invariant, and composable input features, as well as the composition methods, thus allowing it to act based not on the interpolation between learnt data points but on the task-invariant composition of those features. We propose that in order to confirm that an algorithm does indeed learn compositional structures from data, it is not enough to just test on an OOD setup, but one also needs to confirm that the features identified are indeed compositional. We showcase this by exploring two tasks with clearly defined OOD metrics that are not OOD solvable by three commonly used neural networks: a Multi-Layer Perceptron (MLP), a Convolutional Neural Network (CNN), and a Transformer. In addition, we develop two novel network architectures imbued with biases that allow them to be successful in OOD scenarios. We show that even with correct biases and almost perfect OOD performance, an algorithm can still fail to learn the correct features for compositional generalisation.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†åˆ†å¸ƒå¤–æ³›åŒ–(Out-of-distribution generalisation, OOD)çš„æ ¸å¿ƒæŒ‘æˆ˜ï¼Œå³ç³»ç»Ÿå¦‚ä½•é€šè¿‡ç»„åˆå‘ç°ç¯å¢ƒä¸å˜å±æ€§å¹¶å°†å…¶è¿ç§»è‡³æ–°è¾“å…¥ã€‚ç ”ç©¶è€…åˆ©ç”¨å…·æœ‰æ˜ç¡®OODæŒ‡æ ‡çš„ARCç±»ä»»åŠ¡è¿›è¡Œæµ‹è¯•ï¼Œå‘ç°å¤šå±‚æ„ŸçŸ¥å™¨(MLP)ã€å·ç§¯ç¥ç»ç½‘ç»œ(CNN)å’ŒTransformerç­‰å¸¸ç”¨ç¥ç»ç½‘ç»œå‡æ— æ³•èƒœä»»æ­¤ç±»ä»»åŠ¡ã€‚ä¸ºæ­¤ï¼Œç ”ç©¶å›¢é˜Ÿå¼€å‘äº†ä¸¤ç§æ³¨å…¥ç‰¹å®šåç½®(biases)çš„æ–°å‹ç½‘ç»œæ¶æ„ï¼Œä½¿å…¶èƒ½å¤ŸæˆåŠŸåº”å¯¹OODåœºæ™¯ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œå³ä¾¿ç®—æ³•å…·å¤‡æ­£ç¡®çš„åç½®å¹¶è¾¾åˆ°è¿‘ä¹å®Œç¾çš„OODæ€§èƒ½ï¼Œä»å¯èƒ½æ— æ³•å­¦ä¹ åˆ°ç”¨äºç»„åˆæ³›åŒ–(compositional generalisation)çš„æ­£ç¡®ç‰¹å¾ã€‚è¿™ä¸€å‘ç°è¡¨æ˜ï¼ŒéªŒè¯ç®—æ³•æ˜¯å¦çœŸæ­£æŒæ¡ç»„åˆç»“æ„ä¸èƒ½ä»…ä¾èµ–äºOODæµ‹è¯•ç»“æœï¼Œè¿˜éœ€ç¡®è®¤å…¶è¯†åˆ«çš„ç‰¹å¾æ˜¯å¦å…·æœ‰ç»„åˆæ€§ï¼Œå‡¸æ˜¾äº†å®ç°çœŸæ­£OODæ³›åŒ–çš„éš¾åº¦ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "Submission to NeurIPS 2025",
      "pdf_url": "https://arxiv.org/pdf/2505.09716v2",
      "published_date": "2025-05-14 18:21:21 UTC",
      "updated_date": "2025-05-16 15:28:45 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T08:34:34.020425+00:00"
    },
    {
      "arxiv_id": "2505.09704v1",
      "title": "Energy-Efficient Federated Learning for AIoT using Clustering Methods",
      "title_zh": "åŸºäºèšç±»æ–¹æ³•çš„ AIoT é«˜èƒ½æ•ˆè”é‚¦å­¦ä¹ ",
      "authors": [
        "Roberto Pereira",
        "Fernanda FamÃ¡",
        "Charalampos Kalalas",
        "Paolo Dini"
      ],
      "abstract": "While substantial research has been devoted to optimizing model performance, convergence rates, and communication efficiency, the energy implications of federated learning (FL) within Artificial Intelligence of Things (AIoT) scenarios are often overlooked in the existing literature. This study examines the energy consumed during the FL process, focusing on three main energy-intensive processes: pre-processing, communication, and local learning, all contributing to the overall energy footprint. We rely on the observation that device/client selection is crucial for speeding up the convergence of model training in a distributed AIoT setting and propose two clustering-informed methods. These clustering solutions are designed to group AIoT devices with similar label distributions, resulting in clusters composed of nearly heterogeneous devices. Hence, our methods alleviate the heterogeneity often encountered in real-world distributed learning applications. Throughout extensive numerical experimentation, we demonstrate that our clustering strategies typically achieve high convergence rates while maintaining low energy consumption when compared to other recent approaches available in the literature.",
      "tldr_zh": "é’ˆå¯¹ Federated Learning (FL) åœ¨ AIoT åœºæ™¯ä¸‹èƒ½æºæ¶ˆè€—å¸¸è¢«å¿½è§†çš„é—®é¢˜ï¼Œè¯¥ç ”ç©¶ç³»ç»Ÿåˆ†æäº†é¢„å¤„ç†ã€é€šä¿¡å’Œæœ¬åœ°å­¦ä¹ ç­‰é«˜èƒ½è€—è¿‡ç¨‹ã€‚ä¸ºäº†ä¼˜åŒ–åˆ†å¸ƒå¼ç¯å¢ƒä¸­çš„æ¨¡å‹æ”¶æ•›é€Ÿåº¦ï¼Œä½œè€…æå‡ºäº†ä¸¤ç§åŸºäºèšç±» (Clustering) çš„è®¾å¤‡é€‰æ‹©æ–¹æ³•ã€‚è¿™äº›èšç±»ç­–ç•¥é€šè¿‡å°†å…·æœ‰ç›¸ä¼¼æ ‡ç­¾åˆ†å¸ƒçš„ AIoT è®¾å¤‡è¿›è¡Œåˆ†ç»„ï¼Œæœ‰æ•ˆåœ°ç¼“è§£äº†ç°å®åˆ†å¸ƒå¼å­¦ä¹ åº”ç”¨ä¸­å¸¸è§çš„å¼‚æ„æ€§ (Heterogeneity) é—®é¢˜ã€‚å¤§é‡æ•°å€¼å®éªŒè¡¨æ˜ï¼Œä¸ç°æœ‰æ–‡çŒ®ä¸­çš„æœ€æ–°æ–¹æ³•ç›¸æ¯”ï¼Œæ‰€æç­–ç•¥åœ¨ä¿æŒè¾ƒä½èƒ½è€—çš„åŒæ—¶ï¼Œèƒ½å¤Ÿå®ç°æ›´é«˜çš„æ”¶æ•›ç‡ï¼Œæ˜¾è‘—æå‡äº† Federated Learning åœ¨ AIoT é¢†åŸŸçš„èƒ½æºåˆ©ç”¨æ•ˆç‡ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.09704v1",
      "published_date": "2025-05-14 18:04:58 UTC",
      "updated_date": "2025-05-14 18:04:58 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T08:34:41.870830+00:00"
    },
    {
      "arxiv_id": "2505.09698v2",
      "title": "ManipBench: Benchmarking Vision-Language Models for Low-Level Robot Manipulation",
      "title_zh": "ManipBenchï¼šé’ˆå¯¹åº•å±‚æœºå™¨äººæ“ä½œçš„è§†è§‰è¯­è¨€æ¨¡å‹åŸºå‡†æµ‹è¯•",
      "authors": [
        "Enyu Zhao",
        "Vedant Raval",
        "Hejia Zhang",
        "Jiageng Mao",
        "Zeyu Shangguan",
        "Stefanos Nikolaidis",
        "Yue Wang",
        "Daniel Seita"
      ],
      "abstract": "Vision-Language Models (VLMs) have revolutionized artificial intelligence and robotics due to their commonsense reasoning capabilities. In robotic manipulation, VLMs are used primarily as high-level planners, but recent work has also studied their lower-level reasoning ability, which refers to making decisions about precise robot movements. However, the community currently lacks a clear and common benchmark that can evaluate how well VLMs can aid low-level reasoning in robotics. Consequently, we propose a novel benchmark, ManipBench, to evaluate the low-level robot manipulation reasoning capabilities of VLMs across various dimensions, including how well they understand object-object interactions and deformable object manipulation. We extensively test 33 representative VLMs across 10 model families on our benchmark, including variants to test different model sizes. Our evaluation shows that the performance of VLMs significantly varies across tasks, and there is a strong correlation between this performance and trends in our real-world manipulation tasks. It also shows that there remains a significant gap between these models and human-level understanding. See our website at: https://manipbench.github.io.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† ManipBenchï¼Œä¸€ä¸ªæ—¨åœ¨è¯„ä¼°è§†è§‰è¯­è¨€æ¨¡å‹ (Vision-Language Models, VLMs) åœ¨åº•å±‚æœºå™¨äººæ“æ§æ¨ç† (low-level robot manipulation reasoning) èƒ½åŠ›çš„æ–°å‹åŸºå‡†æµ‹è¯•ã€‚è¯¥åŸºå‡†ä»å¤šä¸ªç»´åº¦è¡¡é‡æ¨¡å‹å¯¹ç‰©ä½“é—´äº¤äº’ (object-object interactions) ä»¥åŠæŸ”æ€§ç‰©ä½“æ“æ§ (deformable object manipulation) çš„ç†è§£ç¨‹åº¦ã€‚ç ”ç©¶äººå‘˜é’ˆå¯¹ 10 ä¸ªæ¨¡å‹å®¶æ—çš„ 33 ä¸ªä»£è¡¨æ€§ VLMs è¿›è¡Œäº†å¤§è§„æ¨¡è¯„ä¼°ï¼Œæ¶µç›–äº†å¤šç§æ¨¡å‹å‚æ•°è§„æ¨¡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒVLMs åœ¨ä¸åŒä»»åŠ¡ä¸­çš„è¡¨ç°å·®å¼‚æ˜¾è‘—ï¼Œä¸”å…¶è¯„ä¼°æˆç»©ä¸ç°å®ä¸–ç•Œæ“æ§ä»»åŠ¡çš„è¶‹åŠ¿è¡¨ç°å‡ºå¼ºç›¸å…³æ€§ã€‚ç ”ç©¶æœ€ç»ˆæŒ‡å‡ºï¼Œå½“å‰çš„è§†è§‰è¯­è¨€æ¨¡å‹åœ¨åº•å±‚æ¨ç†æ–¹é¢ä¸äººç±»æ°´å¹³çš„ç†è§£åŠ›ä¹‹é—´ä»å­˜åœ¨å·¨å¤§é¸¿æ²Ÿï¼Œè¯¥åŸºå‡†ä¸ºæœªæ¥å®ç°æ›´ç²¾å‡†çš„æœºå™¨äººåŠ¨ä½œå†³ç­–æä¾›äº†é‡è¦å‚è€ƒã€‚",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "comment": "Conference on Robot Learning (CoRL) 2025. 50 pages and 30 figures. v2 is the camera-ready and includes a few more new experiments compared to v1",
      "pdf_url": "https://arxiv.org/pdf/2505.09698v2",
      "published_date": "2025-05-14 18:01:00 UTC",
      "updated_date": "2025-08-30 18:59:30 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T08:34:49.281651+00:00"
    },
    {
      "arxiv_id": "2505.09614v3",
      "title": "Language Agents Mirror Human Causal Reasoning Biases. How Can We Help Them Think Like Scientists?",
      "title_zh": "è¯­è¨€æ™ºèƒ½ä½“é•œåƒäººç±»å› æœæ¨ç†åå·®ï¼šå¦‚ä½•åŠ©å…¶åƒç§‘å­¦å®¶ä¸€æ ·æ€è€ƒï¼Ÿ",
      "authors": [
        "Anthony GX-Chen",
        "Dongyan Lin",
        "Mandana Samiei",
        "Doina Precup",
        "Blake A. Richards",
        "Rob Fergus",
        "Kenneth Marino"
      ],
      "abstract": "Language model (LM) agents are increasingly used as autonomous decision-makers which need to actively gather information to guide their decisions. A crucial cognitive skill for such agents is the efficient exploration and understanding of the causal structure of the world -- key to robust, scientifically grounded reasoning. Yet, it remains unclear whether LMs possess this capability or exhibit systematic biases leading to erroneous conclusions. In this work, we examine LMs' ability to explore and infer causal relationships, using the well-established Blicket Test paradigm from developmental psychology. We find that LMs reliably infer the common, intuitive disjunctive causal relationships but systematically struggle with the unusual, yet equally (or sometimes even more) evidenced conjunctive ones. This \"disjunctive bias\" persists across model families, sizes, and prompting strategies, and performance further declines as task complexity increases. Interestingly, an analogous bias appears in human adults, suggesting that LMs may have inherited deep-seated reasoning heuristics from their training data. To this end, we quantify similarities between LMs and humans, finding that LMs exhibit adult-like inference profiles (but not child-like). Finally, we propose a test-time sampling method which explicitly samples and eliminates hypotheses about causal relationships from the LM. This scalable approach significantly reduces the disjunctive bias and moves LMs closer to the goal of scientific, causally rigorous reasoning.",
      "tldr_zh": "è¯¥ç ”ç©¶åˆ©ç”¨å¿ƒç†å­¦ä¸­çš„Blicket TestèŒƒå¼ï¼Œæ¢è®¨äº†è¯­è¨€æ¨¡å‹æ™ºèƒ½ä½“ï¼ˆLanguage Model agentsï¼‰åœ¨æ¢ç´¢å’Œæ¨æ–­å› æœå…³ç³»æ–¹é¢çš„èƒ½åŠ›ã€‚ç ”ç©¶å‘ç°ï¼ŒLMsè¡¨ç°å‡ºç³»ç»Ÿæ€§çš„â€œæå–åå·®â€ï¼ˆdisjunctive biasï¼‰ï¼Œå³å®ƒä»¬èƒ½å¤Ÿå¯é åœ°æ¨æ–­ç›´è§‚çš„æå–å› æœå…³ç³»ï¼Œä½†åœ¨é¢å¯¹åˆå–å› æœå…³ç³»ï¼ˆconjunctive onesï¼‰æ—¶å­˜åœ¨æ˜æ˜¾çš„æ¨ç†å›°éš¾ã€‚è¿™ç§åå·®åœ¨ä¸åŒè§„æ¨¡çš„æ¨¡å‹ä¸­æ™®éå­˜åœ¨ï¼Œä¸”ä¸äººç±»æˆäººçš„æ¨ç†è¡Œä¸ºé«˜åº¦ç›¸ä¼¼ï¼Œæš—ç¤ºæ¨¡å‹å¯èƒ½ä»è®­ç»ƒæ•°æ®ä¸­ç»§æ‰¿äº†è¿™ç§æ€ç»´å¯å‘å¼ã€‚å®éªŒè¡¨æ˜ï¼Œéšç€ä»»åŠ¡å¤æ‚åº¦çš„å¢åŠ ï¼ŒLMsçš„å› æœæ¨ç†è¡¨ç°ä¼šè¿›ä¸€æ­¥ä¸‹é™ã€‚ä¸ºæ­¤ï¼Œä½œè€…æå‡ºäº†ä¸€ç§æµ‹è¯•æ—¶é‡‡æ ·æ–¹æ³•ï¼ˆtest-time sampling methodï¼‰ï¼Œé€šè¿‡æ˜¾å¼é‡‡æ ·å’Œæ’é™¤å› æœå‡è®¾æ¥å¼•å¯¼æ¨¡å‹æ€è€ƒã€‚è¯¥æ–¹æ³•æ˜¾è‘—å‡è½»äº†æå–åå·®ï¼Œæé«˜äº†æ¨¡å‹åœ¨å¤æ‚åœºæ™¯ä¸‹çš„é€»è¾‘ä¸¥å¯†æ€§ï¼Œä½¿LMså‘ç§‘å­¦ã€ä¸¥è°¨çš„å› æœæ¨ç†ï¼ˆcausal reasoningï¼‰æ›´è¿‘äº†ä¸€æ­¥ã€‚",
      "categories": [
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.AI",
      "comment": "Conference on Language Modelling (COLM) 2025, Camera Ready",
      "pdf_url": "https://arxiv.org/pdf/2505.09614v3",
      "published_date": "2025-05-14 17:59:35 UTC",
      "updated_date": "2025-10-05 18:28:02 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T08:34:50.778399+00:00"
    },
    {
      "arxiv_id": "2505.09610v1",
      "title": "Customizing a Large Language Model for VHDL Design of High-Performance Microprocessors",
      "title_zh": "é¢å‘é«˜æ€§èƒ½å¾®å¤„ç†å™¨ VHDL è®¾è®¡çš„å¤§è¯­è¨€æ¨¡å‹å®šåˆ¶",
      "authors": [
        "Nicolas Dupuis",
        "Ravi Nair",
        "Shyam Ramji",
        "Sean McClintock",
        "Nishant Chauhan",
        "Priyanka Nagpal",
        "Bart Blaner",
        "Ken Valk",
        "Leon Stok",
        "Ruchir Puri"
      ],
      "abstract": "The use of Large Language Models (LLMs) in hardware design has taken off in recent years, principally through its incorporation in tools that increase chip designer productivity. There has been considerable discussion about the use of LLMs in RTL specifications of chip designs, for which the two most popular languages are Verilog and VHDL. LLMs and their use in Verilog design has received significant attention due to the higher popularity of the language, but little attention so far has been given to VHDL despite its continued popularity in the industry. There has also been little discussion about the unique needs of organizations that engage in high-performance processor design, and techniques to deploy AI solutions in these settings. In this paper, we describe our journey in developing a Large Language Model (LLM) specifically for the purpose of explaining VHDL code, a task that has particular importance in an organization with decades of experience and assets in high-performance processor design. We show how we developed test sets specific to our needs and used them for evaluating models as we performed extended pretraining (EPT) of a base LLM. Expert evaluation of the code explanations produced by the EPT model increased to 69% compared to a base model rating of 43%. We further show how we developed an LLM-as-a-judge to gauge models similar to expert evaluators. This led us to deriving and evaluating a host of new models, including an instruction-tuned version of the EPT model with an expected expert evaluator rating of 71%. Our experiments also indicate that with the potential use of newer base models, this rating can be pushed to 85% and beyond. We conclude with a discussion on further improving the quality of hardware design LLMs using exciting new developments in the Generative AI world.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†ä¸ºé«˜æ€§èƒ½å¾®å¤„ç†å™¨è®¾è®¡å®šåˆ¶å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ–¹æ³•ï¼Œé‡ç‚¹è§£å†³äº†å·¥ä¸šç•Œå¹¿æ³›ä½¿ç”¨ä½†è¢«AIç ”ç©¶å¿½è§†çš„VHDLä»£ç è§£é‡Šä»»åŠ¡ã€‚é€šè¿‡å¯¹åŸºç¡€æ¨¡å‹è¿›è¡Œæ‰©å±•é¢„è®­ç»ƒï¼ˆExtended Pretraining, EPTï¼‰å¹¶æ„å»ºä¸“ç”¨æµ‹è¯•é›†ï¼Œä¸“å®¶å¯¹ä»£ç è§£é‡Šçš„è¯„åˆ†ä»åˆå§‹çš„43%æ˜¾è‘—æå‡è‡³69%ã€‚ç ”ç©¶è¿›ä¸€æ­¥å¼€å‘äº†æŒ‡ä»¤å¾®è°ƒï¼ˆInstruction-tunedï¼‰ç‰ˆæœ¬ï¼Œå¹¶å¼•å…¥LLM-as-a-judgeæœºåˆ¶è¿›è¡Œè‡ªåŠ¨åŒ–è¯„ä¼°ï¼Œä½¿é¢„æœŸä¸“å®¶è¯„åˆ†è¾¾åˆ°71%ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œé‡‡ç”¨æ›´æ–°çš„åŸºç¡€æ¨¡å‹æœ‰æœ›å°†è¿™ä¸€è¯„åˆ†æ¨é«˜è‡³85%ä»¥ä¸Šã€‚è¯¥å·¥ä½œä¸ä»…å±•ç¤ºäº†é’ˆå¯¹ç‰¹å®šç¡¬ä»¶è®¾è®¡éœ€æ±‚ä¼˜åŒ–AIæ¨¡å‹çš„å…¨è¿‡ç¨‹ï¼Œä¹Ÿä¸ºæœªæ¥æå‡ç¡¬ä»¶è®¾è®¡é¢†åŸŸLLMçš„è´¨é‡æä¾›äº†é‡è¦çš„å®è·µæŒ‡å¯¼ã€‚",
      "categories": [
        "cs.AR",
        "cs.AI",
        "cs.CL",
        "cs.SE"
      ],
      "primary_category": "cs.AR",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.09610v1",
      "published_date": "2025-05-14 17:58:40 UTC",
      "updated_date": "2025-05-14 17:58:40 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T08:35:23.611314+00:00"
    },
    {
      "arxiv_id": "2505.09598v6",
      "title": "How Hungry is AI? Benchmarking Energy, Water, and Carbon Footprint of LLM Inference",
      "title_zh": "AI çš„â€œèƒƒå£â€æœ‰å¤šå¤§ï¼Ÿå¤§è¯­è¨€æ¨¡å‹æ¨ç†èƒ½è€—ã€æ°´èµ„æºåŠç¢³è¶³è¿¹çš„åŸºå‡†è¯„ä¼°",
      "authors": [
        "Nidhal Jegham",
        "Marwan Abdelatti",
        "Chan Young Koh",
        "Lassad Elmoubarki",
        "Abdeltawab Hendawi"
      ],
      "abstract": "This paper introduces an infrastructure-aware benchmarking framework for quantifying the environmental footprint of LLM inference across 30 state-of-the-art models in commercial datacenters. The framework combines public API performance data with company-specific environmental multipliers and statistical inference of hardware configurations. We additionally utilize cross-efficiency Data Envelopment Analysis (DEA) to rank models by performance relative to environmental cost and provide a dynamically updated dashboard that visualizes model-level energy, water, and carbon metrics. Results show the most energy-intensive models exceed 29 Wh per long prompt, over 65 times the most efficient systems. Even a 0.42 Wh short query, when scaled to 700M queries/day, aggregates to annual electricity comparable to 35{,}000 U.S. homes, evaporative freshwater equal to the annual drinking needs of 1.2M people, and carbon emissions requiring a Chicago-sized forest to offset. These findings highlight a growing paradox: as AI becomes cheaper and faster, global adoption drives disproportionate resource consumption. Our methodology offers a standardized, empirically grounded basis for sustainability benchmarking and accountability in AI deployment.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ä¸ªåŸºç¡€è®¾æ–½æ„ŸçŸ¥ï¼ˆinfrastructure-awareï¼‰çš„åŸºå‡†æµ‹è¯•æ¡†æ¶ï¼Œç”¨äºé‡åŒ–å•†ä¸šæ•°æ®ä¸­å¿ƒå†…30ç§å…ˆè¿›å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ¨ç†è¿‡ç¨‹ä¸­çš„èƒ½æºã€æ°´èµ„æºå’Œç¢³è¶³è¿¹ã€‚è¯¥æ¡†æ¶ç»“åˆäº†å…¬å¼€APIæ€§èƒ½æ•°æ®ã€å…¬å¸ç‰¹å®šçš„ç¯å¢ƒä¹˜æ•°ä»¥åŠå¯¹ç¡¬ä»¶é…ç½®çš„ç»Ÿè®¡æ¨æ–­ï¼Œå¹¶åˆ©ç”¨äº¤å‰æ•ˆç‡æ•°æ®åŒ…ç»œåˆ†æï¼ˆData Envelopment Analysis, DEAï¼‰è¯„ä¼°æ¨¡å‹æ€§èƒ½ä¸ç¯å¢ƒæˆæœ¬çš„æ¯”ä¾‹ã€‚å®éªŒå‘ç°ï¼Œé«˜èƒ½è€—æ¨¡å‹å¤„ç†é•¿æç¤ºè¯çš„è€—ç”µé‡è¶…è¿‡29 Whï¼Œæ˜¯é«˜æ•ˆæ¨¡å‹çš„65å€ä»¥ä¸Šï¼›è‹¥ä»¥æ¯æ—¥7äº¿æ¬¡æŸ¥è¯¢è®¡ç®—ï¼Œå…¶å¹´è€—ç”µé‡ç›¸å½“äº3.5ä¸‡æˆ·ç¾å›½å®¶åº­ï¼Œè€—æ°´é‡åˆ™å¯æ»¡è¶³120ä¸‡äººçš„å¹´é¥®æ°´éœ€æ±‚ã€‚ç ”ç©¶æ­ç¤ºäº†AIæ™®åŠå¸¦æ¥çš„èµ„æºæ¶ˆè€—æ‚–è®ºï¼Œå³éšç€AIå˜å¾—æ›´ä¾¿å®œé«˜æ•ˆï¼Œå…¨çƒèŒƒå›´çš„å¤§è§„æ¨¡åº”ç”¨åè€Œé©±åŠ¨äº†ä¸æˆæ¯”ä¾‹çš„èµ„æºæ¶ˆè€—ã€‚è¯¥æ–¹æ³•ä¸ºAIéƒ¨ç½²çš„å¯æŒç»­æ€§è¯„ä¼°å’Œé—®è´£æœºåˆ¶æä¾›äº†æ ‡å‡†åŒ–çš„å®è¯åŸºç¡€ã€‚",
      "categories": [
        "cs.CY",
        "cs.AI"
      ],
      "primary_category": "cs.CY",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.09598v6",
      "published_date": "2025-05-14 17:47:00 UTC",
      "updated_date": "2025-11-24 02:12:44 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T08:35:44.382516+00:00"
    },
    {
      "arxiv_id": "2505.09595v1",
      "title": "WorldView-Bench: A Benchmark for Evaluating Global Cultural Perspectives in Large Language Models",
      "title_zh": "WorldView-Benchï¼šå¤§è¯­è¨€æ¨¡å‹å…¨çƒæ–‡åŒ–è§†è§’è¯„æµ‹åŸºå‡†",
      "authors": [
        "Abdullah Mushtaq",
        "Imran Taj",
        "Rafay Naeem",
        "Ibrahim Ghaznavi",
        "Junaid Qadir"
      ],
      "abstract": "Large Language Models (LLMs) are predominantly trained and aligned in ways that reinforce Western-centric epistemologies and socio-cultural norms, leading to cultural homogenization and limiting their ability to reflect global civilizational plurality. Existing benchmarking frameworks fail to adequately capture this bias, as they rely on rigid, closed-form assessments that overlook the complexity of cultural inclusivity. To address this, we introduce WorldView-Bench, a benchmark designed to evaluate Global Cultural Inclusivity (GCI) in LLMs by analyzing their ability to accommodate diverse worldviews. Our approach is grounded in the Multiplex Worldview proposed by Senturk et al., which distinguishes between Uniplex models, reinforcing cultural homogenization, and Multiplex models, which integrate diverse perspectives. WorldView-Bench measures Cultural Polarization, the exclusion of alternative perspectives, through free-form generative evaluation rather than conventional categorical benchmarks. We implement applied multiplexity through two intervention strategies: (1) Contextually-Implemented Multiplex LLMs, where system prompts embed multiplexity principles, and (2) Multi-Agent System (MAS)-Implemented Multiplex LLMs, where multiple LLM agents representing distinct cultural perspectives collaboratively generate responses. Our results demonstrate a significant increase in Perspectives Distribution Score (PDS) entropy from 13% at baseline to 94% with MAS-Implemented Multiplex LLMs, alongside a shift toward positive sentiment (67.7%) and enhanced cultural balance. These findings highlight the potential of multiplex-aware AI evaluation in mitigating cultural bias in LLMs, paving the way for more inclusive and ethically aligned AI systems.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¨å‡ºäº†WorldView-Benchï¼Œè¿™æ˜¯ä¸€ä¸ªæ—¨åœ¨è¯„ä¼°å¤§è¯­è¨€æ¨¡å‹(LLMs)ä¸­å…¨çƒæ–‡åŒ–åŒ…å®¹æ€§(Global Cultural Inclusivity, GCI)çš„åŸºå‡†æµ‹è¯•ï¼Œä»¥åº”å¯¹å½“å‰æ¨¡å‹æ™®éå­˜åœ¨çš„è¥¿æ–¹ä¸­å¿ƒä¸»ä¹‰å’Œæ–‡åŒ–åŒè´¨åŒ–é—®é¢˜ã€‚è¯¥åŸºå‡†åŸºäºMultiplex Worldviewç†è®ºï¼Œé€šè¿‡è‡ªç”±å½¢å¼çš„ç”Ÿæˆå¼è¯„ä¼°æ¥è¡¡é‡æ–‡åŒ–æåŒ–(Cultural Polarization)ï¼Œè€Œéé‡‡ç”¨ä¼ ç»Ÿçš„åƒµåŒ–åˆ†ç±»è¯„ä¼°ã€‚ç ”ç©¶è€…å®æ–½äº†ä¸¤ç§å¹²é¢„ç­–ç•¥ï¼ŒåŒ…æ‹¬åœ¨ç³»ç»Ÿæç¤ºè¯ä¸­åµŒå…¥å¤šæ ·æ€§åŸåˆ™ï¼Œä»¥åŠåˆ©ç”¨å¤šæ™ºèƒ½ä½“ç³»ç»Ÿ(Multi-Agent System, MAS)åä½œç”Ÿæˆå…·æœ‰ä¸åŒæ–‡åŒ–è§†è§’çš„å“åº”ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒMASå¹²é¢„å°†è§†è§’åˆ†å¸ƒå¾—åˆ†(Perspectives Distribution Score, PDS)çš„ç†µä»13%æ˜¾è‘—æå‡è‡³94%ï¼Œå¹¶å®ç°äº†æ›´ä¼˜çš„æ–‡åŒ–å¹³è¡¡ä¸æ­£é¢æƒ…æ„Ÿè¡¨è¾¾ã€‚è¿™ä¸€å‘ç°çªæ˜¾äº†å…·å¤‡å¤šæ ·æ€§æ„è¯†(multiplex-aware)çš„AIè¯„ä¼°åœ¨å‡å°‘æ–‡åŒ–åè§æ–¹é¢çš„å·¨å¤§æ½œåŠ›ï¼Œä¸ºå¼€å‘æ›´å…·åŒ…å®¹æ€§çš„å…¨çƒåŒ–AIç³»ç»Ÿå¥ å®šäº†åŸºç¡€ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.CY",
        "cs.MA"
      ],
      "primary_category": "cs.CL",
      "comment": "Preprint. Submitted to the Journal of Artificial Intelligence Research (JAIR) on April 29, 2025",
      "pdf_url": "https://arxiv.org/pdf/2505.09595v1",
      "published_date": "2025-05-14 17:43:40 UTC",
      "updated_date": "2025-05-14 17:43:40 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T08:35:52.882564+00:00"
    },
    {
      "arxiv_id": "2505.09593v1",
      "title": "Online Isolation Forest",
      "title_zh": "åœ¨çº¿å­¤ç«‹æ£®æ—",
      "authors": [
        "Filippo Leveni",
        "Guilherme Weigert Cassales",
        "Bernhard Pfahringer",
        "Albert Bifet",
        "Giacomo Boracchi"
      ],
      "abstract": "The anomaly detection literature is abundant with offline methods, which require repeated access to data in memory, and impose impractical assumptions when applied to a streaming context. Existing online anomaly detection methods also generally fail to address these constraints, resorting to periodic retraining to adapt to the online context. We propose Online-iForest, a novel method explicitly designed for streaming conditions that seamlessly tracks the data generating process as it evolves over time. Experimental validation on real-world datasets demonstrated that Online-iForest is on par with online alternatives and closely rivals state-of-the-art offline anomaly detection techniques that undergo periodic retraining. Notably, Online-iForest consistently outperforms all competitors in terms of efficiency, making it a promising solution in applications where fast identification of anomalies is of primary importance such as cybersecurity, fraud and fault detection.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† Online-iForestï¼Œè¿™æ˜¯ä¸€ç§ä¸“é—¨ä¸ºæµå¼æ•°æ® (streaming conditions) ç¯å¢ƒè®¾è®¡çš„å¼‚å¸¸æ£€æµ‹ (anomaly detection) æ–¹æ³•ï¼Œæ—¨åœ¨è§£å†³ä¼ ç»Ÿç¦»çº¿ç®—æ³•ä¾èµ–é‡å¤è®¿é—®å†…å­˜æ•°æ®ä»¥åŠç°æœ‰åœ¨çº¿ç®—æ³•éœ€è¦å®šæœŸé‡æ–°è®­ç»ƒçš„å±€é™æ€§ã€‚Online-iForest èƒ½å¤Ÿéšç€æ•°æ®çš„æ¼”å˜æ— ç¼è·Ÿè¸ªæ•°æ®ç”Ÿæˆè¿‡ç¨‹ï¼Œä»è€Œåœ¨åŠ¨æ€ç¯å¢ƒä¸­å®ç°å®æ—¶æ›´æ–°ã€‚å®éªŒéªŒè¯è¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨çœŸå®æ•°æ®é›†ä¸Šçš„è¡¨ç°ä¸ä¸»æµåœ¨çº¿ç®—æ³•æŒå¹³ï¼Œä¸”èƒ½å¤Ÿåª²ç¾ç»è¿‡å®šæœŸé‡è®­çš„æœ€å…ˆè¿›ç¦»çº¿å¼‚å¸¸æ£€æµ‹æŠ€æœ¯ã€‚æ›´é‡è¦çš„æ˜¯ï¼ŒOnline-iForest åœ¨å¤„ç†æ•ˆç‡ä¸Šå§‹ç»ˆä¼˜äºæ‰€æœ‰å¯¹æ¯”æ¨¡å‹ï¼Œä½¿å…¶æˆä¸ºç½‘ç»œå®‰å…¨ (cybersecurity)ã€æ¬ºè¯ˆæ£€æµ‹ (fraud detection) åŠæ•…éšœæ£€æµ‹ (fault detection) ç­‰å¯¹å¼‚å¸¸è¯†åˆ«é€Ÿåº¦è¦æ±‚æé«˜çš„åº”ç”¨é¢†åŸŸä¸­çš„ç†æƒ³è§£å†³æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "stat.ML"
      ],
      "primary_category": "cs.LG",
      "comment": "Accepted at International Conference on Machine Learning (ICML 2024)",
      "pdf_url": "https://arxiv.org/pdf/2505.09593v1",
      "published_date": "2025-05-14 17:42:50 UTC",
      "updated_date": "2025-05-14 17:42:50 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T08:35:52.759784+00:00"
    },
    {
      "arxiv_id": "2505.09591v2",
      "title": "Variational Visual Question Answering for Uncertainty-Aware Selective Prediction",
      "title_zh": "é¢å‘ä¸ç¡®å®šæ€§æ„ŸçŸ¥é€‰æ‹©æ€§é¢„æµ‹çš„å˜åˆ†è§†è§‰é—®ç­”",
      "authors": [
        "Tobias Jan Wieczorek",
        "Nathalie Daun",
        "Mohammad Emtiyaz Khan",
        "Marcus Rohrbach"
      ],
      "abstract": "Despite remarkable progress in recent years, vision language models (VLMs) remain prone to overconfidence and hallucinations on tasks such as Visual Question Answering (VQA) and Visual Reasoning. Bayesian methods can potentially improve reliability by helping models selectively predict, that is, models respond only when they are sufficiently confident. Unfortunately, Bayesian methods are often assumed to be costly and ineffective for large models, and so far there exists little evidence to show otherwise, especially for multimodal applications. Here, we show the effectiveness and competitive edge of variational Bayes for selective prediction in VQA for the first time. We build on recent advances in variational methods for deep learning and propose an extension called \"Variational VQA\". This method improves calibration and yields significant gains for selective prediction on VQA and Visual Reasoning, particularly when the error tolerance is low ($\\leq 1\\%$). Often, just one posterior sample can yield more reliable answers than those obtained by models trained with AdamW. In addition, we propose a new risk-averse selector that outperforms standard sample averaging by considering the variance of predictions. Overall, we present compelling evidence that variational learning is a viable option to make large VLMs safer and more trustworthy.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹è§†è§‰è¯­è¨€æ¨¡å‹(VLMs)åœ¨è§†è§‰é—®ç­”(VQA)å’Œè§†è§‰æ¨ç†ä»»åŠ¡ä¸­å®¹æ˜“å‡ºç°çš„è¿‡åº¦è‡ªä¿¡å’Œå¹»è§‰é—®é¢˜ï¼Œé¦–æ¬¡å±•ç¤ºäº†å˜åˆ†è´å¶æ–¯(variational Bayes)åœ¨é€‰æ‹©æ€§é¢„æµ‹(selective prediction)ä¸­çš„æœ‰æ•ˆæ€§ã€‚ä½œè€…é€šè¿‡æ‰©å±•æ·±åº¦å­¦ä¹ ä¸­çš„å˜åˆ†æ–¹æ³•ï¼Œæå‡ºäº†Variational VQAæ¡†æ¶ï¼Œæ—¨åœ¨æé«˜æ¨¡å‹æ ¡å‡†(calibration)å¹¶å®ç°æ›´å¯é çš„è¾“å‡ºã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨ä½é”™è¯¯å®¹å¿åº¦(error tolerance $\\leq 1\\%$)çš„æƒ…å†µä¸‹æ˜¾è‘—æå‡äº†VQAä»»åŠ¡çš„æ€§èƒ½ï¼Œä¸”å•ä¸ªåéªŒæ ·æœ¬å¾€å¾€æ¯”ä½¿ç”¨AdamWè®­ç»ƒçš„æ¨¡å‹æ›´å¯é ã€‚æ­¤å¤–ï¼Œè®ºæ–‡è¿˜å¼•å…¥äº†ä¸€ç§è€ƒè™‘é¢„æµ‹æ–¹å·®çš„æ–°å‹é£é™©è§„é¿é€‰æ‹©å™¨(risk-averse selector)ï¼Œå…¶è¡¨ç°ä¼˜äºæ ‡å‡†çš„æ ·æœ¬å¹³å‡æ–¹æ³•ã€‚è¿™é¡¹å·¥ä½œä¸ºåˆ©ç”¨å˜åˆ†å­¦ä¹ (variational learning)æå‡å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹çš„å®‰å…¨æ€§å’Œå¯ä¿¡åº¦æä¾›äº†å¼ºæœ‰åŠ›çš„è¯æ®ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "under review at TMLR",
      "pdf_url": "https://arxiv.org/pdf/2505.09591v2",
      "published_date": "2025-05-14 17:40:22 UTC",
      "updated_date": "2025-10-31 02:57:26 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T08:35:59.016362+00:00"
    },
    {
      "arxiv_id": "2505.09576v1",
      "title": "Ethics and Persuasion in Reinforcement Learning from Human Feedback: A Procedural Rhetorical Approach",
      "title_zh": "åŸºäºäººç±»åé¦ˆå¼ºåŒ–å­¦ä¹ ä¸­çš„ä¼¦ç†ä¸è¯´æœï¼šä¸€ç§ç¨‹åºä¿®è¾å­¦æ–¹æ³•",
      "authors": [
        "Shannon Lodoen",
        "Alexi Orchard"
      ],
      "abstract": "Since 2022, versions of generative AI chatbots such as ChatGPT and Claude have been trained using a specialized technique called Reinforcement Learning from Human Feedback (RLHF) to fine-tune language model output using feedback from human annotators. As a result, the integration of RLHF has greatly enhanced the outputs of these large language models (LLMs) and made the interactions and responses appear more \"human-like\" than those of previous versions using only supervised learning. The increasing convergence of human and machine-written text has potentially severe ethical, sociotechnical, and pedagogical implications relating to transparency, trust, bias, and interpersonal relations. To highlight these implications, this paper presents a rhetorical analysis of some of the central procedures and processes currently being reshaped by RLHF-enhanced generative AI chatbots: upholding language conventions, information seeking practices, and expectations for social relationships. Rhetorical investigations of generative AI and LLMs have, to this point, focused largely on the persuasiveness of the content generated. Using Ian Bogost's concept of procedural rhetoric, this paper shifts the site of rhetorical investigation from content analysis to the underlying mechanisms of persuasion built into RLHF-enhanced LLMs. In doing so, this theoretical investigation opens a new direction for further inquiry in AI ethics that considers how procedures rerouted through AI-driven technologies might reinforce hegemonic language use, perpetuate biases, decontextualize learning, and encroach upon human relationships. It will therefore be of interest to educators, researchers, scholars, and the growing number of users of generative AI chatbots.",
      "tldr_zh": "RLHFæŠ€æœ¯é€šè¿‡äººç±»åé¦ˆå¾®è°ƒå¤§è¯­è¨€æ¨¡å‹(LLMs)ï¼Œåœ¨æ˜¾è‘—æå‡ç”Ÿæˆå†…å®¹æ‹ŸäººåŒ–ç¨‹åº¦çš„åŒæ—¶ï¼Œä¹Ÿå¼•å‘äº†å…³äºé€æ˜åº¦ã€ä¿¡ä»»å’Œåè§ç­‰ä¸¥å³»çš„ä¼¦ç†ä¸ç¤¾ä¼šæŠ€æœ¯æŒ‘æˆ˜ã€‚æœ¬æ–‡é‡‡ç”¨äº†Ian Bogostæå‡ºçš„ç¨‹åºä¿®è¾(Procedural Rhetoric)æ¡†æ¶ï¼Œå°†ä¿®è¾åˆ†æçš„é‡å¿ƒä»ç”Ÿæˆçš„å†…å®¹æœ¬èº«è½¬ç§»åˆ°RLHFå¢å¼ºå‹æ¨¡å‹æ‰€è•´å«çš„åº•å±‚è¯´æœæœºåˆ¶ä¸Šã€‚ç ”ç©¶æ·±å…¥æ¢è®¨äº†è¢«AIæŠ€æœ¯é‡å¡‘çš„æ ¸å¿ƒç¨‹åºï¼ŒåŒ…æ‹¬è¯­è¨€è§„èŒƒçš„ç»´æŠ¤ã€ä¿¡æ¯è·å–æ–¹å¼ä»¥åŠå¯¹ç¤¾äº¤å…³ç³»çš„å¿ƒç†é¢„æœŸã€‚é€šè¿‡è¿™ä¸€è§†è§’çš„åˆ†æï¼Œè®ºæ–‡æ­ç¤ºäº†æ­¤ç±»æŠ€æœ¯å¯èƒ½å¦‚ä½•å¼ºåŒ–éœ¸æƒå¼è¯­è¨€çš„ä½¿ç”¨ã€å»¶ç»­ç³»ç»Ÿæ€§åè§ï¼Œå¹¶ä½¿å­¦ä¹ è¿‡ç¨‹è„±ç¦»å…·ä½“æƒ…å¢ƒã€‚è¿™ä¸€ç†è®ºç ”ç©¶ä¸ºAIä¼¦ç†é¢†åŸŸæä¾›äº†æ–°çš„åˆ‡å…¥ç‚¹ï¼Œé‡ç‚¹å…³æ³¨ç¨‹åºå¦‚ä½•å½±å“äººé™…å…³ç³»å’Œæ•™è‚²å…¬å¹³ã€‚è¯¥æˆæœå¯¹äºè¯„ä¼°ç”Ÿæˆå¼AIåœ¨ç°ä»£ç¤¾ä¼šä¸­çš„è§’è‰²ï¼Œä»¥åŠå…¶å¯¹æ²Ÿé€šèŒƒå¼å’Œè®¤çŸ¥çš„æ½œåœ¨é‡å¡‘å…·æœ‰é‡è¦çš„æŒ‡å¯¼æ„ä¹‰ã€‚",
      "categories": [
        "cs.CY",
        "cs.AI"
      ],
      "primary_category": "cs.CY",
      "comment": "10 pages, 1 figure, Accepted version",
      "pdf_url": "https://arxiv.org/pdf/2505.09576v1",
      "published_date": "2025-05-14 17:29:19 UTC",
      "updated_date": "2025-05-14 17:29:19 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T08:36:14.838650+00:00"
    },
    {
      "arxiv_id": "2505.09568v1",
      "title": "BLIP3-o: A Family of Fully Open Unified Multimodal Models-Architecture, Training and Dataset",
      "title_zh": "BLIP3-oï¼šå…¨å¼€æºç»Ÿä¸€å¤šæ¨¡æ€æ¨¡å‹ç³»åˆ—â€”â€”æ¶æ„ã€è®­ç»ƒä¸æ•°æ®é›†",
      "authors": [
        "Jiuhai Chen",
        "Zhiyang Xu",
        "Xichen Pan",
        "Yushi Hu",
        "Can Qin",
        "Tom Goldstein",
        "Lifu Huang",
        "Tianyi Zhou",
        "Saining Xie",
        "Silvio Savarese",
        "Le Xue",
        "Caiming Xiong",
        "Ran Xu"
      ],
      "abstract": "Unifying image understanding and generation has gained growing attention in recent research on multimodal models. Although design choices for image understanding have been extensively studied, the optimal model architecture and training recipe for a unified framework with image generation remain underexplored. Motivated by the strong potential of autoregressive and diffusion models for high-quality generation and scalability, we conduct a comprehensive study of their use in unified multimodal settings, with emphasis on image representations, modeling objectives, and training strategies. Grounded in these investigations, we introduce a novel approach that employs a diffusion transformer to generate semantically rich CLIP image features, in contrast to conventional VAE-based representations. This design yields both higher training efficiency and improved generative quality. Furthermore, we demonstrate that a sequential pretraining strategy for unified models-first training on image understanding and subsequently on image generation-offers practical advantages by preserving image understanding capability while developing strong image generation ability. Finally, we carefully curate a high-quality instruction-tuning dataset BLIP3o-60k for image generation by prompting GPT-4o with a diverse set of captions covering various scenes, objects, human gestures, and more. Building on our innovative model design, training recipe, and datasets, we develop BLIP3-o, a suite of state-of-the-art unified multimodal models. BLIP3-o achieves superior performance across most of the popular benchmarks spanning both image understanding and generation tasks. To facilitate future research, we fully open-source our models, including code, model weights, training scripts, and pretraining and instruction tuning datasets.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¨å‡ºäº† BLIP3-o ç³»åˆ—å®Œå…¨å¼€æºçš„ç»Ÿä¸€å¤šæ¨¡æ€æ¨¡å‹ï¼Œæ—¨åœ¨æ¢ç´¢å›¾åƒç†è§£ä¸ç”Ÿæˆä»»åŠ¡çš„æœ€ä½³æ¶æ„ä¸è®­ç»ƒæ–¹æ¡ˆã€‚ç ”ç©¶äººå‘˜æå‡ºä¸€ç§åˆ›æ–°æ–¹æ³•ï¼Œåˆ©ç”¨ Diffusion Transformer ç”Ÿæˆè¯­ä¹‰ä¸°å¯Œçš„ CLIP å›¾åƒç‰¹å¾ï¼Œä»¥æ­¤æ›¿ä»£ä¼ ç»Ÿçš„ VAE è¡¨å¾ï¼Œæ˜¾è‘—æå‡äº†è®­ç»ƒæ•ˆç‡ä¸ç”Ÿæˆè´¨é‡ã€‚åœ¨è®­ç»ƒç­–ç•¥ä¸Šï¼Œè¯¥æ¨¡å‹é‡‡ç”¨é¡ºåºé¢„è®­ç»ƒ(Sequential Pretraining)æ–¹å¼ï¼Œå³å…ˆå­¦ä¹ å›¾åƒç†è§£å†å­¦ä¹ å›¾åƒç”Ÿæˆï¼Œä»è€Œåœ¨å¼€å‘å¼ºå¤§ç”Ÿæˆèƒ½åŠ›çš„åŒæ—¶å®Œå¥½ä¿ç•™äº†ç†è§£èƒ½åŠ›ã€‚æ­¤å¤–ï¼Œç ”ç©¶å›¢é˜Ÿåˆ©ç”¨ GPT-4o æ„å»ºäº†é«˜è´¨é‡çš„æŒ‡ä»¤å¾®è°ƒæ•°æ®é›† BLIP3o-60kï¼Œæ¶µç›–äº†å¤šæ ·çš„åœºæ™¯ä¸å¯¹è±¡ã€‚å®éªŒè¡¨æ˜ï¼ŒBLIP3-o åœ¨å›¾åƒç†è§£å’Œç”Ÿæˆçš„å¤šä¸ªä¸»æµåŸºå‡†æµ‹è¯•ä¸­å‡å–å¾—äº† state-of-the-art çš„æ€§èƒ½è¡¨ç°ã€‚ç›®å‰ï¼Œè¯¥æ¨¡å‹çš„ä»£ç ã€æƒé‡ã€è®­ç»ƒè„šæœ¬åŠæ•°æ®é›†å·²å…¨éƒ¨å¼€æºï¼Œæ—¨åœ¨æ¨åŠ¨å¤šæ¨¡æ€é¢†åŸŸçš„æœªæ¥ç ”ç©¶ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.09568v1",
      "published_date": "2025-05-14 17:11:07 UTC",
      "updated_date": "2025-05-14 17:11:07 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T08:36:28.874832+00:00"
    },
    {
      "arxiv_id": "2505.09565v1",
      "title": "Meta-learning Slice-to-Volume Reconstruction in Fetal Brain MRI using Implicit Neural Representations",
      "title_zh": "åŸºäºéšå¼ç¥ç»è¡¨ç¤ºçš„èƒå„¿å¤§è„‘ MRI å…ƒå­¦ä¹ åˆ‡ç‰‡åˆ°ä½“ç§¯é‡å»º",
      "authors": [
        "Maik Dannecker",
        "Thomas Sanchez",
        "Meritxell Bach Cuadra",
        "Ã–zgÃ¼n Turgut",
        "Anthony N. Price",
        "Lucilio Cordero-Grande",
        "Vanessa Kyriakopoulou",
        "Joseph V. Hajnal",
        "Daniel Rueckert"
      ],
      "abstract": "High-resolution slice-to-volume reconstruction (SVR) from multiple motion-corrupted low-resolution 2D slices constitutes a critical step in image-based diagnostics of moving subjects, such as fetal brain Magnetic Resonance Imaging (MRI). Existing solutions struggle with image artifacts and severe subject motion or require slice pre-alignment to achieve satisfying reconstruction performance. We propose a novel SVR method to enable fast and accurate MRI reconstruction even in cases of severe image and motion corruption. Our approach performs motion correction, outlier handling, and super-resolution reconstruction with all operations being entirely based on implicit neural representations. The model can be initialized with task-specific priors through fully self-supervised meta-learning on either simulated or real-world data. In extensive experiments including over 480 reconstructions of simulated and clinical MRI brain data from different centers, we prove the utility of our method in cases of severe subject motion and image artifacts. Our results demonstrate improvements in reconstruction quality, especially in the presence of severe motion, compared to state-of-the-art methods, and up to 50% reduction in reconstruction time.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹èƒå„¿è„‘éƒ¨ MRI ä¸­ç”±äºè¿åŠ¨ä¼ªå½±å’Œå›¾åƒæŸåå¯¼è‡´çš„é‡å»ºéš¾é¢˜ï¼Œæå‡ºäº†ä¸€ç§åˆ©ç”¨ Implicit Neural Representations (INRs) çš„æ–°å‹ Slice-to-Volume Reconstruction (SVR) æ–¹æ³•ã€‚è¯¥æ–¹æ³•å®Œå…¨åŸºäºéšå¼ç¥ç»è¡¨ç¤ºå®ç°äº†è¿åŠ¨æ ¡æ­£ã€å¼‚å¸¸å€¼å¤„ç†å’Œè¶…åˆ†è¾¨ç‡é‡å»ºï¼Œä¸”æ— éœ€åˆ‡ç‰‡é¢„å¯¹å‡†å³å¯å¤„ç†ä¸¥é‡çš„è¿åŠ¨å¹²æ‰°ã€‚é€šè¿‡åœ¨æ¨¡æ‹Ÿæˆ–çœŸå®æ•°æ®ä¸Šè¿›è¡Œå…¨è‡ªç›‘ç£çš„ Meta-learningï¼Œæ¨¡å‹èƒ½å¤Ÿè·å¾—ä»»åŠ¡ç‰¹å®šçš„å…ˆéªŒåˆå§‹åŒ–ã€‚å®éªŒé€šè¿‡å¯¹è¶…è¿‡ 480 ä¾‹æ¨¡æ‹Ÿå’Œä¸´åºŠè„‘éƒ¨ MRI æ•°æ®çš„é‡å»ºéªŒè¯ï¼Œè¯æ˜è¯¥æ–¹æ³•åœ¨æç«¯è¿åŠ¨å’Œä¼ªå½±æƒ…å†µä¸‹æ˜¾è‘—ä¼˜äºç°æœ‰æœ€å…ˆè¿›æŠ€æœ¯ã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•åœ¨æå‡é‡å»ºè´¨é‡çš„åŒæ—¶ï¼Œè¿˜å®ç°äº†é«˜è¾¾ 50% çš„é‡å»ºæ—¶é—´ç¼©å‡ã€‚",
      "categories": [
        "eess.IV",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "eess.IV",
      "comment": "10 pages, 6 figures",
      "pdf_url": "https://arxiv.org/pdf/2505.09565v1",
      "published_date": "2025-05-14 17:07:37 UTC",
      "updated_date": "2025-05-14 17:07:37 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T08:36:16.301330+00:00"
    },
    {
      "arxiv_id": "2505.09561v2",
      "title": "Learning Long-Context Diffusion Policies via Past-Token Prediction",
      "title_zh": "é€šè¿‡å†å²Tokené¢„æµ‹å­¦ä¹ é•¿ä¸Šä¸‹æ–‡æ‰©æ•£ç­–ç•¥",
      "authors": [
        "Marcel Torne",
        "Andy Tang",
        "Yuejiang Liu",
        "Chelsea Finn"
      ],
      "abstract": "Reasoning over long sequences of observations and actions is essential for many robotic tasks. Yet, learning effective long-context policies from demonstrations remains challenging. As context length increases, training becomes increasingly expensive due to rising memory demands, and policy performance often degrades as a result of spurious correlations. Recent methods typically sidestep these issues by truncating context length, discarding historical information that may be critical for subsequent decisions. In this paper, we propose an alternative approach that explicitly regularizes the retention of past information. We first revisit the copycat problem in imitation learning and identify an opposite challenge in recent diffusion policies: rather than over-relying on prior actions, they often fail to capture essential dependencies between past and future actions. To address this, we introduce Past-Token Prediction (PTP), an auxiliary task in which the policy learns to predict past action tokens alongside future ones. This regularization significantly improves temporal modeling in the policy head, with minimal reliance on visual representations. Building on this observation, we further introduce a multistage training strategy: pre-train the visual encoder with short contexts, and fine-tune the policy head using cached long-context embeddings. This strategy preserves the benefits of PTP while greatly reducing memory and computational overhead. Finally, we extend PTP into a self-verification mechanism at test time, enabling the policy to score and select candidates consistent with past actions during inference. Experiments across four real-world and six simulated tasks demonstrate that our proposed method improves the performance of long-context diffusion policies by 3x and accelerates policy training by more than 10x.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹æœºå™¨äººé•¿åºåˆ—æ¨ç†ä»»åŠ¡ä¸­å­˜åœ¨çš„è®­ç»ƒæˆæœ¬é«˜åŠæ—¶é—´ä¾èµ–æ€§æ•æ‰ä¸è¶³ç­‰é—®é¢˜ï¼Œæå‡ºäº†é€šè¿‡ Past-Token Prediction (PTP) å­¦ä¹  Long-Context Diffusion Policies çš„æ–°æ–¹æ³•ã€‚è¯¥æ–¹æ³•å¼•å…¥ PTP ä½œä¸ºè¾…åŠ©ä»»åŠ¡ï¼Œè¦æ±‚ç­–ç•¥åœ¨é¢„æµ‹æœªæ¥åŠ¨ä½œçš„åŒæ—¶é¢„æµ‹è¿‡å»çš„åŠ¨ä½œæ ‡è®°ï¼Œä»¥æ­¤ä½œä¸ºæ­£åˆ™åŒ–æ‰‹æ®µæ¥æ˜¾è‘—æå‡ç­–ç•¥å¤´çš„æ—¶é—´å»ºæ¨¡èƒ½åŠ›ã€‚ä¸ºäº†ä¼˜åŒ–è®¡ç®—æ•ˆç‡ï¼Œç ”ç©¶è®¾è®¡äº†ä¸€ç§å¤šé˜¶æ®µè®­ç»ƒç­–ç•¥ï¼Œé€šè¿‡åœ¨çŸ­ä¸Šä¸‹æ–‡ä¸­é¢„è®­ç»ƒè§†è§‰ç¼–ç å™¨å¹¶åˆ©ç”¨ç¼“å­˜çš„é•¿ä¸Šä¸‹æ–‡åµŒå…¥å¾®è°ƒç­–ç•¥å¤´ï¼Œå¤§å¹…é™ä½äº†å†…å­˜å’Œè®¡ç®—å¼€é”€ã€‚æ­¤å¤–ï¼ŒPTP è¿˜åœ¨æ¨ç†é˜¶æ®µè¢«æ‰©å±•ä¸ºä¸€ç§è‡ªæˆ‘éªŒè¯æœºåˆ¶ï¼Œå…è®¸ç­–ç•¥æ ¹æ®ä¸è¿‡å»åŠ¨ä½œçš„ä¸€è‡´æ€§æ¥è¯„åˆ†å¹¶ç­›é€‰æœ€ä½³å€™é€‰åŠ¨ä½œã€‚åœ¨çœŸå®ä¸–ç•Œå’Œæ¨¡æ‹Ÿç¯å¢ƒçš„å®éªŒè¯æ˜ï¼Œè¯¥æ–¹æ³•å°†é•¿ä¸Šä¸‹æ–‡æ‰©æ•£ç­–ç•¥çš„æ€§èƒ½æå‡äº† 3 å€ï¼Œå¹¶å°†è®­ç»ƒé€Ÿåº¦åŠ å¿«äº† 10 å€ä»¥ä¸Šã€‚",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.RO",
      "comment": "Videos are available at https://long-context-dp.github.io",
      "pdf_url": "https://arxiv.org/pdf/2505.09561v2",
      "published_date": "2025-05-14 17:00:47 UTC",
      "updated_date": "2025-05-19 20:37:41 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T08:36:33.548796+00:00"
    },
    {
      "arxiv_id": "2505.09558v2",
      "title": "WavReward: Spoken Dialogue Models With Generalist Reward Evaluators",
      "title_zh": "WavRewardï¼šå…·æœ‰é€šç”¨å¥–åŠ±è¯„ä¼°å™¨çš„è¯­éŸ³å¯¹è¯æ¨¡å‹",
      "authors": [
        "Shengpeng Ji",
        "Tianle Liang",
        "Yangzhuo Li",
        "Jialong Zuo",
        "Minghui Fang",
        "Jinzheng He",
        "Yifu Chen",
        "Zhengqing Liu",
        "Ziyue Jiang",
        "Xize Cheng",
        "Siqi Zheng",
        "Jin Xu",
        "Junyang Lin",
        "Zhou Zhao"
      ],
      "abstract": "End-to-end spoken dialogue models such as GPT-4o-audio have recently garnered significant attention in the speech domain. However, the evaluation of spoken dialogue models' conversational performance has largely been overlooked. This is primarily due to the intelligent chatbots convey a wealth of non-textual information which cannot be easily measured using text-based language models like ChatGPT. To address this gap, we propose WavReward, a reward feedback model based on audio language models that can evaluate both the IQ and EQ of spoken dialogue systems with speech input. Specifically, 1) based on audio language models, WavReward incorporates the deep reasoning process and the nonlinear reward mechanism for post-training. By utilizing multi-sample feedback via the reinforcement learning algorithm, we construct a specialized evaluator tailored to spoken dialogue models. 2) We introduce ChatReward-30K, a preference dataset used to train WavReward. ChatReward-30K includes both comprehension and generation aspects of spoken dialogue models. These scenarios span various tasks, such as text-based chats, nine acoustic attributes of instruction chats, and implicit chats. WavReward outperforms previous state-of-the-art evaluation models across multiple spoken dialogue scenarios, achieving a substantial improvement about Qwen2.5-Omni in objective accuracy from 53.4$\\%$ to 91.5$\\%$. In subjective A/B testing, WavReward also leads by a margin of 83$\\%$. Comprehensive ablation studies confirm the necessity of each component of WavReward. All data and code will be publicly at https://github.com/jishengpeng/WavReward after the paper is accepted.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†WavRewardï¼Œä¸€ç§åŸºäºéŸ³é¢‘è¯­è¨€æ¨¡å‹(Audio Language Models)çš„é€šç”¨å¥–åŠ±è¯„ä¼°æ¨¡å‹ï¼Œæ—¨åœ¨è§£å†³ä¼ ç»Ÿæ–‡æœ¬è¯­è¨€æ¨¡å‹æ— æ³•æœ‰æ•ˆè¯„ä¼°è¯­éŸ³å¯¹è¯ä¸­éæ–‡æœ¬ä¿¡æ¯çš„é—®é¢˜ã€‚WavRewardèƒ½å¤ŸåŒæ—¶å¯¹è¯­éŸ³å¯¹è¯ç³»ç»Ÿçš„æ™ºå•†(IQ)å’Œæƒ…å•†(EQ)è¿›è¡Œè¯„ä¼°ï¼Œé€šè¿‡åœ¨åè®­ç»ƒé˜¶æ®µå¼•å…¥æ·±åº¦æ¨ç†è¿‡ç¨‹å’Œéçº¿æ€§å¥–åŠ±æœºåˆ¶ï¼Œå¹¶ç»“åˆå¼ºåŒ–å­¦ä¹ ç®—æ³•æ„å»ºä¸“é—¨çš„è¯„ä¼°å™¨ã€‚ä¸ºäº†è®­ç»ƒè¯¥æ¨¡å‹ï¼Œç ”ç©¶è€…æ¨å‡ºäº†åŒ…å«3ä¸‡æ¡åå¥½æ•°æ®çš„ChatReward-30Kæ•°æ®é›†ï¼Œæ¶µç›–äº†æ–‡æœ¬å¯¹è¯ã€å…·æœ‰ä¹ç§å£°å­¦å±æ€§çš„æŒ‡ä»¤å¯¹è¯åŠéšå¼å¯¹è¯ç­‰å¤šç§åœºæ™¯ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒWavRewardåœ¨å¤šé¡¹è¯­éŸ³å¯¹è¯ä»»åŠ¡ä¸­ä¼˜äºç°æœ‰çš„æœ€å…ˆè¿›è¯„ä¼°æ¨¡å‹ï¼Œå°†Qwen2.5-Omniçš„å®¢è§‚å‡†ç¡®ç‡ä»53.4%æ˜¾è‘—æå‡è‡³91.5%ï¼Œå¹¶åœ¨ä¸»è§‚A/Bæµ‹è¯•ä¸­è¡¨ç°å‡º83%çš„é¢†å…ˆä¼˜åŠ¿ã€‚è¯¥ç ”ç©¶ä¸ºç«¯åˆ°ç«¯è¯­éŸ³å¯¹è¯æ¨¡å‹çš„æ€§èƒ½è¡¡é‡æä¾›äº†é‡è¦çš„åŸºå‡†å’Œå·¥å…·ã€‚",
      "categories": [
        "eess.AS",
        "cs.AI",
        "cs.LG",
        "cs.MM",
        "cs.SD"
      ],
      "primary_category": "eess.AS",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.09558v2",
      "published_date": "2025-05-14 16:54:15 UTC",
      "updated_date": "2025-09-23 09:57:50 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T08:36:24.188769+00:00"
    },
    {
      "arxiv_id": "2505.09666v2",
      "title": "System Prompt Optimization with Meta-Learning",
      "title_zh": "åŸºäºå…ƒå­¦ä¹ çš„ç³»ç»Ÿæç¤ºä¼˜åŒ–",
      "authors": [
        "Yumin Choi",
        "Jinheon Baek",
        "Sung Ju Hwang"
      ],
      "abstract": "Large Language Models (LLMs) have shown remarkable capabilities, with optimizing their input prompts playing a pivotal role in maximizing their performance. However, while LLM prompts consist of both the task-agnostic system prompts and task-specific user prompts, existing work on prompt optimization has focused on user prompts specific to individual queries or tasks, and largely overlooked the system prompt that is, once optimized, applicable across different tasks and domains. Motivated by this, we introduce the novel problem of bilevel system prompt optimization, whose objective is to design system prompts that are robust to diverse user prompts and transferable to unseen tasks. To tackle this problem, we then propose a meta-learning framework, which meta-learns the system prompt by optimizing it over various user prompts across multiple datasets, while simultaneously updating the user prompts in an iterative manner to ensure synergy between them. We conduct experiments on 14 unseen datasets spanning 5 different domains, on which we show that our approach produces system prompts that generalize effectively to diverse user prompts. Also, our findings reveal that the optimized system prompt enables rapid adaptation even to unseen tasks, requiring fewer optimization steps for test-time user prompts while achieving improved performance.",
      "tldr_zh": "è¯¥ç ”ç©¶æŒ‡å‡ºè™½ç„¶å¤§å‹è¯­è¨€æ¨¡å‹(LLMs)çš„æ€§èƒ½åœ¨å¾ˆå¤§ç¨‹åº¦ä¸Šå–å†³äºæç¤ºè¯ä¼˜åŒ–ï¼Œä½†ç°æœ‰ç ”ç©¶ä¸»è¦é›†ä¸­åœ¨é’ˆå¯¹ç‰¹å®šä»»åŠ¡çš„User Promptsï¼Œè€Œå¿½è§†äº†èƒ½è·¨é¢†åŸŸåº”ç”¨çš„System Promptsä¼˜åŒ–ã€‚ä¸ºæ­¤ï¼Œç ”ç©¶è€…æå‡ºäº†åŒå±‚ç³»ç»Ÿæç¤ºè¯ä¼˜åŒ–(Bilevel System Prompt Optimization)é—®é¢˜ï¼Œæ—¨åœ¨è®¾è®¡å¯¹å¤šæ ·åŒ–User Promptså…·æœ‰é²æ£’æ€§ä¸”èƒ½è¿ç§»è‡³æœªçŸ¥ä»»åŠ¡çš„ç³»ç»Ÿæç¤ºè¯ã€‚è¯¥ç ”ç©¶å¼€å‘äº†ä¸€ä¸ªå…ƒå­¦ä¹ (Meta-learning)æ¡†æ¶ï¼Œé€šè¿‡åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šä¼˜åŒ–System Promptsï¼Œå¹¶ä»¥è¿­ä»£æ–¹å¼åŒæ­¥æ›´æ–°User Promptsä»¥ç¡®ä¿ä¸¤è€…é—´çš„ååŒæ•ˆåº”ã€‚å®éªŒåœ¨æ¶µç›–5ä¸ªé¢†åŸŸçš„14ä¸ªæœªçŸ¥æ•°æ®é›†ä¸Šè¡¨æ˜ï¼Œè¯¥æ–¹æ³•ç”Ÿæˆçš„System Promptsèƒ½å¤Ÿæœ‰æ•ˆæ³›åŒ–è‡³å„ç§User Promptsã€‚æ­¤å¤–ï¼Œä¼˜åŒ–åçš„System Promptsä½¿æ¨¡å‹èƒ½å¿«é€Ÿé€‚åº”æ–°ä»»åŠ¡ï¼Œåœ¨æµ‹è¯•é˜¶æ®µä»…éœ€æ›´å°‘çš„ä¼˜åŒ–æ­¥æ•°å³å¯æ˜¾è‘—æå‡æ€§èƒ½ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "NeurIPS 2025",
      "pdf_url": "https://arxiv.org/pdf/2505.09666v2",
      "published_date": "2025-05-14 16:46:15 UTC",
      "updated_date": "2025-10-10 09:56:06 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T08:36:33.214186+00:00"
    },
    {
      "arxiv_id": "2505.10588v1",
      "title": "Understanding Gen Alpha Digital Language: Evaluation of LLM Safety Systems for Content Moderation",
      "title_zh": "ç†è§£ Gen Alpha æ•°å­—è¯­è¨€ï¼šé¢å‘å†…å®¹å®¡æ ¸çš„å¤§è¯­è¨€æ¨¡å‹å®‰å…¨ç³»ç»Ÿè¯„ä¼°",
      "authors": [
        "Manisha Mehta",
        "Fausto Giunchiglia"
      ],
      "abstract": "This research offers a unique evaluation of how AI systems interpret the digital language of Generation Alpha (Gen Alpha, born 2010-2024). As the first cohort raised alongside AI, Gen Alpha faces new forms of online risk due to immersive digital engagement and a growing mismatch between their evolving communication and existing safety tools. Their distinct language, shaped by gaming, memes, and AI-driven trends, often conceals harmful interactions from both human moderators and automated systems. We assess four leading AI models (GPT-4, Claude, Gemini, and Llama 3) on their ability to detect masked harassment and manipulation within Gen Alpha discourse. Using a dataset of 100 recent expressions from gaming platforms, social media, and video content, the study reveals critical comprehension failures with direct implications for online safety. This work contributes: (1) a first-of-its-kind dataset capturing Gen Alpha expressions; (2) a framework to improve AI moderation systems for youth protection; (3) a multi-perspective evaluation including AI systems, human moderators, and parents, with direct input from Gen Alpha co-researchers; and (4) an analysis of how linguistic divergence increases youth vulnerability. Findings highlight the urgent need to redesign safety systems attuned to youth communication, especially given Gen Alpha reluctance to seek help when adults fail to understand their digital world. This study combines the insight of a Gen Alpha researcher with systematic academic analysis to address critical digital safety challenges.",
      "tldr_zh": "è¯¥ç ”ç©¶è¯„ä¼°äº† AI ç³»ç»Ÿå¯¹é˜¿å°”æ³•ä¸–ä»£ (Gen Alpha) æ•°å­—è¯­è¨€çš„ç†è§£èƒ½åŠ›ï¼Œé‡ç‚¹å…³æ³¨å…¶åœ¨å†…å®¹å®¡æ ¸ (Content Moderation) ä¸­çš„å®‰å…¨ç³»ç»Ÿè¡¨ç°ã€‚é’ˆå¯¹ Gen Alpha æ·±å—æ¸¸æˆã€è¿·å› å’Œ AI è¶‹åŠ¿å½±å“çš„ç‹¬ç‰¹è¯­è¨€é£æ ¼ï¼Œç ”ç©¶äººå‘˜æ„å»ºäº†ä¸€ä¸ªåŒ…å« 100 ä¸ªæœ€æ–°è¡¨è¾¾æ–¹å¼çš„æ•°æ®é›†ï¼Œå¹¶å¯¹ GPT-4ã€Claudeã€Gemini å’Œ Llama 3 è¿›è¡Œäº†æµ‹è¯•ã€‚è¯„ä¼°ç»“æœæ˜¾ç¤ºï¼Œè¿™äº›é¢†å…ˆçš„è¯­è¨€æ¨¡å‹åœ¨æ£€æµ‹éšè—çš„éªšæ‰°å’Œæ“çºµè¡Œä¸ºæ–¹é¢å­˜åœ¨ä¸¥é‡çš„ç†è§£å¤±è´¥ï¼Œæš´éœ²äº†ç°æœ‰å®‰å…¨å·¥å…·ä¸é’å°‘å¹´æ²Ÿé€šæ–¹å¼ä¹‹é—´çš„è„±èŠ‚ã€‚è¯¥å·¥ä½œè´¡çŒ®äº†é¦–ä¸ªé’ˆå¯¹ Gen Alpha è¡¨è¾¾æ–¹å¼çš„æ•°æ®é›†ï¼Œæå‡ºäº†æ”¹è¿›é’å°‘å¹´ä¿æŠ¤å®¡æ ¸ç³»ç»Ÿçš„æ¡†æ¶ï¼Œå¹¶å¼•å…¥äº†åŒ…æ‹¬ Gen Alpha åˆä½œç ”ç©¶å‘˜åœ¨å†…çš„å¤šç»´åº¦è¯„ä»·ä½“ç³»ã€‚ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œç”±äºè¯­è¨€æ¼”å˜å¯¼è‡´çš„è¯†åˆ«éšœç¢å¢åŠ äº†é’å°‘å¹´çš„æ•°å­—è„†å¼±æ€§ï¼Œè¿«åˆ‡éœ€è¦é‡æ–°è®¾è®¡æ›´å…·é’ˆå¯¹æ€§çš„å®‰å…¨ç³»ç»Ÿä»¥å¡«è¡¥ç›‘ç®¡ç¼ºå£ã€‚",
      "categories": [
        "cs.CY",
        "cs.AI",
        "cs.CL",
        "cs.HC"
      ],
      "primary_category": "cs.CY",
      "comment": "Accepted to ACM FAccT 2025. To be presented in Athens, June 2025, and published in the conference proceedings. Preprint version; final version will appear in the ACM Digital Library",
      "pdf_url": "https://arxiv.org/pdf/2505.10588v1",
      "published_date": "2025-05-14 16:46:11 UTC",
      "updated_date": "2025-05-14 16:46:11 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T08:38:46.388484+00:00"
    },
    {
      "arxiv_id": "2505.09518v3",
      "title": "Robust Finite-Memory Policy Gradients for Hidden-Model POMDPs",
      "title_zh": "é’ˆå¯¹éšè—æ¨¡å‹ POMDP çš„é²æ£’æœ‰é™è®°å¿†ç­–ç•¥æ¢¯åº¦",
      "authors": [
        "Maris F. L. Galesloot",
        "Roman Andriushchenko",
        "Milan ÄŒeÅ¡ka",
        "Sebastian Junges",
        "Nils Jansen"
      ],
      "abstract": "Partially observable Markov decision processes (POMDPs) model specific environments in sequential decision-making under uncertainty. Critically, optimal policies for POMDPs may not be robust against perturbations in the environment. Hidden-model POMDPs (HM-POMDPs) capture sets of different environment models, that is, POMDPs with a shared action and observation space. The intuition is that the true model is hidden among a set of potential models, and it is unknown which model will be the environment at execution time. A policy is robust for a given HM-POMDP if it achieves sufficient performance for each of its POMDPs. We compute such robust policies by combining two orthogonal techniques: (1) a deductive formal verification technique that supports tractable robust policy evaluation by computing a worst-case POMDP within the HM-POMDP, and (2) subgradient ascent to optimize the candidate policy for a worst-case POMDP. The empirical evaluation shows that, compared to various baselines, our approach (1) produces policies that are more robust and generalize better to unseen POMDPs, and (2) scales to HM-POMDPs that consist of over a hundred thousand environments.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹éƒ¨åˆ†å¯è§‚æµ‹é©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹ï¼ˆPOMDPsï¼‰ä¸­æœ€ä¼˜ç­–ç•¥å¯¹ç¯å¢ƒæ‰°åŠ¨é²æ£’æ€§ä¸è¶³çš„é—®é¢˜ï¼Œæ¢è®¨äº†éšè—æ¨¡å‹POMDPsï¼ˆHM-POMDPsï¼‰ã€‚HM-POMDPs æ•æ‰äº†ä¸€ç³»åˆ—å…±äº«åŠ¨ä½œä¸è§‚æµ‹ç©ºé—´çš„ä¸åŒç¯å¢ƒæ¨¡å‹ï¼Œå…¶æ ¸å¿ƒæŒ‘æˆ˜åœ¨äºæ‰§è¡Œæ—¶çš„çœŸå®æ¨¡å‹æ˜¯ä¸å¯çŸ¥çš„ã€‚ä¸ºäº†è®¡ç®—é²æ£’ç­–ç•¥ï¼Œè¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§ç»“åˆæ¼”ç»å½¢å¼éªŒè¯ï¼ˆdeductive formal verificationï¼‰ä¸æ¬¡æ¢¯åº¦ä¸Šå‡ï¼ˆsubgradient ascentï¼‰çš„æ–°æ–¹æ³•ï¼Œé€šè¿‡åœ¨ HM-POMDP ä¸­å®šä½æœ€åæƒ…å†µï¼ˆworst-caseï¼‰æ¨¡å‹æ¥ä¼˜åŒ–å€™é€‰ç­–ç•¥ã€‚å®éªŒè¯„ä¼°è¡¨æ˜ï¼Œä¸å¤šç§åŸºå‡†æ¨¡å‹ç›¸æ¯”ï¼Œè¯¥æ–¹æ³•ç”Ÿæˆçš„ç­–ç•¥å…·æœ‰æ›´å¼ºçš„é²æ£’æ€§ï¼Œä¸”å¯¹æœªè§è¿‡çš„ POMDPs å…·æœ‰æ›´å¥½çš„æ³›åŒ–èƒ½åŠ›ã€‚æ­¤å¤–ï¼Œè¯¥ç®—æ³•å±•ç°äº†å“è¶Šçš„å¯æ‰©å±•æ€§ï¼Œèƒ½å¤Ÿæœ‰æ•ˆå¤„ç†åŒ…å«è¶…è¿‡åä¸‡ä¸ªç¯å¢ƒçš„å¤§è§„æ¨¡ HM-POMDPsã€‚",
      "categories": [
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "Accepted for publication at IJCAI 2025",
      "pdf_url": "https://arxiv.org/pdf/2505.09518v3",
      "published_date": "2025-05-14 16:15:58 UTC",
      "updated_date": "2025-08-20 02:45:49 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T08:36:57.188431+00:00"
    },
    {
      "arxiv_id": "2505.09498v1",
      "title": "Flash-VL 2B: Optimizing Vision-Language Model Performance for Ultra-Low Latency and High Throughput",
      "title_zh": "Flash-VL 2Bï¼šé¢å‘æä½å»¶è¿Ÿä¸é«˜ååé‡çš„è§†è§‰è¯­è¨€æ¨¡å‹æ€§èƒ½ä¼˜åŒ–",
      "authors": [
        "Bo Zhang",
        "Shuo Li",
        "Runhe Tian",
        "Yang Yang",
        "Jixin Tang",
        "Jinhao Zhou",
        "Lin Ma"
      ],
      "abstract": "In this paper, we introduce Flash-VL 2B, a novel approach to optimizing Vision-Language Models (VLMs) for real-time applications, targeting ultra-low latency and high throughput without sacrificing accuracy. Leveraging advanced architectural enhancements and efficient computational strategies, Flash-VL 2B is designed to maximize throughput by reducing processing time while maintaining competitive performance across multiple vision-language benchmarks. Our approach includes tailored architectural choices, token compression mechanisms, data curation, training schemes, and a novel image processing technique called implicit semantic stitching that effectively balances computational load and model performance. Through extensive evaluations on 11 standard VLM benchmarks, we demonstrate that Flash-VL 2B achieves state-of-the-art results in both speed and accuracy, making it a promising solution for deployment in resource-constrained environments and large-scale real-time applications.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¨å‡ºäº† Flash-VL 2Bï¼Œè¿™æ˜¯ä¸€ç§æ—¨åœ¨ä¸ºå®æ—¶åº”ç”¨ä¼˜åŒ– Vision-Language Models (VLMs) çš„æ–°æ–¹æ³•ï¼Œé‡ç‚¹åœ¨äºå®ç°è¶…ä½å»¶è¿Ÿå’Œé«˜ååé‡å¹¶ç¡®ä¿å‡†ç¡®æ€§ã€‚Flash-VL 2B é€šè¿‡å…ˆè¿›çš„æ¶æ„å¢å¼ºå’Œé«˜æ•ˆçš„è®¡ç®—ç­–ç•¥ï¼Œåœ¨ä¿æŒå¤šé¡¹è§†è§‰è¯­è¨€åŸºå‡†æµ‹è¯•ç«äº‰åŠ›çš„åŒæ—¶ï¼Œæœ€å¤§é™åº¦åœ°ç¼©çŸ­äº†å¤„ç†æ—¶é—´ã€‚è¯¥æ¡†æ¶ç»“åˆäº†å®šåˆ¶çš„æ¶æ„é€‰æ‹©ã€token compression æœºåˆ¶ã€æ•°æ®ç­–å±•ã€ç‰¹å®šçš„è®­ç»ƒæ–¹æ¡ˆï¼Œä»¥åŠä¸€ç§åä¸º implicit semantic stitching çš„æ–°å‹å›¾åƒå¤„ç†æŠ€æœ¯ï¼Œæœ‰æ•ˆå¹³è¡¡äº†è®¡ç®—è´Ÿè½½ä¸æ¨¡å‹æ€§èƒ½ã€‚åœ¨ 11 ä¸ªæ ‡å‡† VLM åŸºå‡†æµ‹è¯•çš„å¹¿æ³›è¯„ä¼°ä¸­ï¼ŒFlash-VL 2B åœ¨é€Ÿåº¦å’Œå‡†ç¡®æ€§æ–¹é¢å‡è¾¾åˆ°äº† state-of-the-art æ°´å¹³ã€‚å®éªŒè¯æ˜è¯¥æ¨¡å‹åœ¨èµ„æºå—é™ç¯å¢ƒå’Œå¤§è§„æ¨¡å®æ—¶åº”ç”¨ä¸­å…·æœ‰æé«˜çš„éƒ¨ç½²ä»·å€¼ï¼Œä¸ºé«˜æ€§èƒ½è½»é‡åŒ–è§†è§‰è¯­è¨€æ¨¡å‹çš„ç ”ç©¶æä¾›äº†é‡è¦å‚è€ƒã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "18 pages, 7 figures",
      "pdf_url": "https://arxiv.org/pdf/2505.09498v1",
      "published_date": "2025-05-14 15:45:17 UTC",
      "updated_date": "2025-05-14 15:45:17 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T08:37:02.876643+00:00"
    },
    {
      "arxiv_id": "2505.09486v1",
      "title": "Preserving Plasticity in Continual Learning with Adaptive Linearity Injection",
      "title_zh": "é€šè¿‡è‡ªé€‚åº”çº¿æ€§æ³¨å…¥ä¿æŒæŒç»­å­¦ä¹ ä¸­çš„å¡‘æ€§",
      "authors": [
        "Seyed Roozbeh Razavi Rohani",
        "Khashayar Khajavi",
        "Wesley Chung",
        "Mo Chen",
        "Sharan Vaswani"
      ],
      "abstract": "Loss of plasticity in deep neural networks is the gradual reduction in a model's capacity to incrementally learn and has been identified as a key obstacle to learning in non-stationary problem settings. Recent work has shown that deep linear networks tend to be resilient towards loss of plasticity. Motivated by this observation, we propose Adaptive Linearization (AdaLin), a general approach that dynamically adapts each neuron's activation function to mitigate plasticity loss. Unlike prior methods that rely on regularization or periodic resets, AdaLin equips every neuron with a learnable parameter and a gating mechanism that injects linearity into the activation function based on its gradient flow. This adaptive modulation ensures sufficient gradient signal and sustains continual learning without introducing additional hyperparameters or requiring explicit task boundaries. When used with conventional activation functions like ReLU, Tanh, and GeLU, we demonstrate that AdaLin can significantly improve performance on standard benchmarks, including Random Label and Permuted MNIST, Random Label and Shuffled CIFAR-10, and Class-Split CIFAR-100. Furthermore, its efficacy is shown in more complex scenarios, such as class-incremental learning on CIFAR-100 with a ResNet-18 backbone, and in mitigating plasticity loss in off-policy reinforcement learning agents. We perform a systematic set of ablations that show that neuron-level adaptation is crucial for good performance and analyze a number of metrics in the network that might be correlated to loss of plasticity.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹æ·±åº¦ç¥ç»ç½‘ç»œåœ¨éå¹³ç¨³ç¯å¢ƒä¸‹æŒç»­å­¦ä¹ (Continual Learning)ä¸­é¢ä¸´çš„å¡‘æ€§ä¸§å¤±(Loss of Plasticity)é—®é¢˜ï¼Œæå‡ºäº†AdaLin (Adaptive Linearization)è‡ªé€‚åº”çº¿æ€§æ³¨å…¥æ–¹æ³•ã€‚å—æ·±å±‚çº¿æ€§ç½‘ç»œåœ¨ç»´æŒå­¦ä¹ èƒ½åŠ›æ–¹é¢è¡¨ç°å‡ºéŸ§æ€§çš„å¯å‘ï¼ŒAdaLinä¸ºæ¯ä¸ªç¥ç»å…ƒå¼•å…¥å¯å­¦ä¹ å‚æ•°å’Œé—¨æ§æœºåˆ¶ï¼Œæ ¹æ®æ¢¯åº¦æµåŠ¨æ€è°ƒæ•´æ¿€æ´»å‡½æ•°çš„çº¿æ€§åº¦(Linearity)ï¼Œä»¥ç¡®ä¿æ¨¡å‹åœ¨é•¿æœŸè®­ç»ƒä¸­ç»´æŒå……è¶³çš„æ¢¯åº¦ä¿¡å·ã€‚è¯¥æ–¹æ³•æ— éœ€é¢å¤–çš„æ­£åˆ™åŒ–ã€å‘¨æœŸæ€§é‡ç½®æˆ–æ˜¾å¼çš„ä»»åŠ¡è¾¹ç•Œï¼Œèƒ½å¤Ÿä¸ReLUã€TanhåŠGeLUç­‰æ ‡å‡†æ¿€æ´»å‡½æ•°æ— ç¼é›†æˆã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒAdaLinåœ¨Permuted MNISTå’ŒCIFAR-100ç­‰åŸºå‡†æµ‹è¯•ä»¥åŠåŸºäºResNet-18çš„ç±»å¢é‡å­¦ä¹ ä¸­æ˜¾è‘—æå‡äº†æ¨¡å‹æ€§èƒ½ã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•åœ¨ç¼“è§£ç¦»ç­–å¼ºåŒ–å­¦ä¹ (Off-policy Reinforcement Learning)æ™ºèƒ½ä½“çš„å¡‘æ€§ä¸§å¤±æ–¹é¢ä¹Ÿè¡¨ç°å‡ºæ˜¾è‘—æ•ˆæœï¼Œæ¶ˆèå®éªŒè¿›ä¸€æ­¥è¯å®äº†ç¥ç»å…ƒçº§åˆ«çš„è‡ªé€‚åº”è°ƒèŠ‚å¯¹äºç»´æŒæ¨¡å‹æŒç»­å­¦ä¹ èƒ½åŠ›è‡³å…³é‡è¦ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "Accepted in 4th Conference on Lifelong Learning Agents (CoLLAs), 2025",
      "pdf_url": "https://arxiv.org/pdf/2505.09486v1",
      "published_date": "2025-05-14 15:36:51 UTC",
      "updated_date": "2025-05-14 15:36:51 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T08:37:36.198639+00:00"
    },
    {
      "arxiv_id": "2505.09477v1",
      "title": "Deploying Foundation Model-Enabled Air and Ground Robots in the Field: Challenges and Opportunities",
      "title_zh": "åŸºåº§æ¨¡å‹èµ‹èƒ½çš„ç©ºåœ°æœºå™¨äººé‡å¤–éƒ¨ç½²ï¼šæŒ‘æˆ˜ä¸æœºé‡",
      "authors": [
        "Zachary Ravichandran",
        "Fernando Cladera",
        "Jason Hughes",
        "Varun Murali",
        "M. Ani Hsieh",
        "George J. Pappas",
        "Camillo J. Taylor",
        "Vijay Kumar"
      ],
      "abstract": "The integration of foundation models (FMs) into robotics has enabled robots to understand natural language and reason about the semantics in their environments. However, existing FM-enabled robots primary operate in closed-world settings, where the robot is given a full prior map or has a full view of its workspace. This paper addresses the deployment of FM-enabled robots in the field, where missions often require a robot to operate in large-scale and unstructured environments. To effectively accomplish these missions, robots must actively explore their environments, navigate obstacle-cluttered terrain, handle unexpected sensor inputs, and operate with compute constraints. We discuss recent deployments of SPINE, our LLM-enabled autonomy framework, in field robotic settings. To the best of our knowledge, we present the first demonstration of large-scale LLM-enabled robot planning in unstructured environments with several kilometers of missions. SPINE is agnostic to a particular LLM, which allows us to distill small language models capable of running onboard size, weight and power (SWaP) limited platforms. Via preliminary model distillation work, we then present the first language-driven UAV planner using on-device language models. We conclude our paper by proposing several promising directions for future research.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†å°†åŸºç¡€æ¨¡å‹ (Foundation Models, FMs) é›†æˆåˆ°ç©ºåœ°æœºå™¨äººä¸­çš„æŒ‘æˆ˜ä¸æœºé‡ï¼Œé‡ç‚¹å…³æ³¨å…¶åœ¨é‡å¤–å¤§è§„æ¨¡éç»“æ„åŒ–ç¯å¢ƒä¸­çš„éƒ¨ç½²ã€‚ä½œè€…ä»‹ç»äº†åŸºäºå¤§è¯­è¨€æ¨¡å‹ (LLM) çš„è‡ªä¸»æ¡†æ¶ SPINEï¼Œæ—¨åœ¨è§£å†³æœºå™¨äººç”±äºä¼ æ„Ÿå™¨è¾“å…¥å¼‚å¸¸ã€è®¡ç®—èµ„æºå—é™ä»¥åŠéšœç¢ç‰©å¯†é›†ç­‰å¸¦æ¥çš„å¤æ‚ä»»åŠ¡ã€‚è®ºæ–‡å±•ç¤ºäº†åœ¨é•¿è¾¾æ•°å…¬é‡Œçš„é‡å¤–ä»»åŠ¡ä¸­ï¼Œé¦–æ¬¡å®ç°äº†åŸºäº LLM çš„å¤§è§„æ¨¡æœºå™¨äººè§„åˆ’ã€‚é€šè¿‡æ¨¡å‹è’¸é¦æŠ€æœ¯ (Model Distillation)ï¼Œè¯¥ç ”ç©¶å°†é«˜æ€§èƒ½ LLM è½¬åŒ–ä¸ºè½»é‡åŒ–æ¨¡å‹ï¼Œä»¥é€‚åº”å°ºå¯¸ã€é‡é‡å’ŒåŠŸè€— (SWaP) å—é™çš„ç¡¬ä»¶å¹³å°ã€‚ç ”ç©¶è¿›ä¸€æ­¥å±•ç¤ºäº†é¦–ä¸ªåˆ©ç”¨è®¾å¤‡ç«¯è¯­è¨€æ¨¡å‹å®ç°çš„ç”±è¯­è¨€é©±åŠ¨çš„æ— äººæœº (UAV) è§„åˆ’ç³»ç»Ÿã€‚æœ€åï¼Œè®ºæ–‡æå‡ºäº†è¯¥é¢†åŸŸæœªæ¥ç ”ç©¶çš„å‡ ä¸ªé‡è¦æ–¹å‘ï¼Œä¸ºè¿ˆå‘å…·å¤‡è¯­ä¹‰æ¨ç†èƒ½åŠ›å’Œå¤æ‚ç¯å¢ƒé€‚åº”æ€§çš„æœºå™¨äººç³»ç»Ÿå¥ å®šäº†åŸºç¡€ã€‚",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "comment": "Accepted to the IEEE ICRA Workshop on Field Robotics 2025",
      "pdf_url": "https://arxiv.org/pdf/2505.09477v1",
      "published_date": "2025-05-14 15:28:43 UTC",
      "updated_date": "2025-05-14 15:28:43 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T08:37:16.829086+00:00"
    },
    {
      "arxiv_id": "2505.09466v1",
      "title": "A 2D Semantic-Aware Position Encoding for Vision Transformers",
      "title_zh": "é¢å‘è§†è§‰ Transformer çš„äºŒç»´è¯­ä¹‰æ„ŸçŸ¥ä½ç½®ç¼–ç ",
      "authors": [
        "Xi Chen",
        "Shiyang Zhou",
        "Muqi Huang",
        "Jiaxu Feng",
        "Yun Xiong",
        "Kun Zhou",
        "Biao Yang",
        "Yuhui Zhang",
        "Huishuai Bao",
        "Sijia Peng",
        "Chuan Li",
        "Feng Shi"
      ],
      "abstract": "Vision transformers have demonstrated significant advantages in computer vision tasks due to their ability to capture long-range dependencies and contextual relationships through self-attention. However, existing position encoding techniques, which are largely borrowed from natural language processing, fail to effectively capture semantic-aware positional relationships between image patches. Traditional approaches like absolute position encoding and relative position encoding primarily focus on 1D linear position relationship, often neglecting the semantic similarity between distant yet contextually related patches. These limitations hinder model generalization, translation equivariance, and the ability to effectively handle repetitive or structured patterns in images. In this paper, we propose 2-Dimensional Semantic-Aware Position Encoding ($\\text{SaPE}^2$), a novel position encoding method with semantic awareness that dynamically adapts position representations by leveraging local content instead of fixed linear position relationship or spatial coordinates. Our method enhances the model's ability to generalize across varying image resolutions and scales, improves translation equivariance, and better aggregates features for visually similar but spatially distant patches. By integrating $\\text{SaPE}^2$ into vision transformers, we bridge the gap between position encoding and perceptual similarity, thereby improving performance on computer vision tasks.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹è§†è§‰ Transformer åœ¨æ•æ‰å›¾åƒå—è¯­ä¹‰æ„ŸçŸ¥ä½ç½®å…³ç³»æ–¹é¢çš„ä¸è¶³ï¼Œæå‡ºäº† 2-Dimensional Semantic-Aware Position Encoding ($SaPE^2$)ã€‚ä¼ ç»Ÿçš„ Position Encoding å¾€å¾€å—é™äºä¸€ç»´çº¿æ€§å…³ç³»æˆ–å›ºå®šç©ºé—´åæ ‡ï¼Œè€Œ $SaPE^2$ é€šè¿‡åˆ©ç”¨å±€éƒ¨å†…å®¹åŠ¨æ€è°ƒæ•´ä½ç½®è¡¨ç¤ºï¼Œæœ‰æ•ˆå¼¥è¡¥äº†ä½ç½®ç¼–ç ä¸æ„ŸçŸ¥ç›¸ä¼¼æ€§ä¹‹é—´çš„é¸¿æ²Ÿã€‚è¿™ç§æ–¹æ³•ä¸ä»…å¢å¼ºäº†æ¨¡å‹åœ¨ä¸åŒå›¾åƒåˆ†è¾¨ç‡å’Œå°ºåº¦ä¸‹çš„æ³›åŒ–èƒ½åŠ›ï¼Œè¿˜æ˜¾è‘—æå‡äº†å¹³ç§»ç­‰å˜æ€§ (Translation Equivariance)ã€‚æ­¤å¤–ï¼Œ$SaPE^2$ èƒ½å¤Ÿæ›´å¥½åœ°èšåˆè§†è§‰ç›¸ä¼¼ä½†ç©ºé—´ä½ç½®è¾ƒè¿œçš„ç‰¹å¾ï¼Œä»è€Œæ›´ç²¾å‡†åœ°å¤„ç†å›¾åƒä¸­çš„é‡å¤æˆ–ç»“æ„åŒ–æ¨¡å¼ã€‚å®éªŒè¡¨æ˜ï¼Œå°† $SaPE^2$ é›†æˆè‡³è§†è§‰ Transformer æ¶æ„ä¸­ï¼Œå¯æ˜¾è‘—æå‡å…¶åœ¨å¤šç§è®¡ç®—æœºè§†è§‰ä»»åŠ¡ä¸Šçš„è¡¨ç°ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "14 pages, 4 figures, 3 tables",
      "pdf_url": "https://arxiv.org/pdf/2505.09466v1",
      "published_date": "2025-05-14 15:17:34 UTC",
      "updated_date": "2025-05-14 15:17:34 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T08:37:24.531531+00:00"
    },
    {
      "arxiv_id": "2505.09456v1",
      "title": "Quantum state-agnostic work extraction (almost) without dissipation",
      "title_zh": "ï¼ˆå‡ ä¹ï¼‰æ— è€—æ•£çš„é‡å­æ€æ— å…³åŠŸæå–",
      "authors": [
        "Josep Lumbreras",
        "Ruo Cheng Huang",
        "Yanglin Hu",
        "Mile Gu",
        "Marco Tomamichel"
      ],
      "abstract": "We investigate work extraction protocols designed to transfer the maximum possible energy to a battery using sequential access to $N$ copies of an unknown pure qubit state. The core challenge is designing interactions to optimally balance two competing goals: charging of the battery optimally using the qubit in hand, and acquiring more information by qubit to improve energy harvesting in subsequent rounds. Here, we leverage exploration-exploitation trade-off in reinforcement learning to develop adaptive strategies achieving energy dissipation that scales only poly-logarithmically in $N$. This represents an exponential improvement over current protocols based on full state tomography.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†å¦‚ä½•é€šè¿‡é¡ºåºè®¿é—® $N$ ä¸ªæœªçŸ¥çº¯é‡å­æ¯”ç‰¹çŠ¶æ€ (unknown pure qubit state) çš„å‰¯æœ¬ï¼Œè®¾è®¡èƒ½å¤Ÿå°†æœ€å¤§èƒ½é‡è½¬ç§»è‡³ç”µæ± çš„åŠŸæå–åè®® (work extraction protocols)ã€‚å…¶æ ¸å¿ƒæŒ‘æˆ˜åœ¨äºè®¾è®¡ç›¸äº’ä½œç”¨ï¼Œä»¥å¹³è¡¡åˆ©ç”¨å½“å‰é‡å­æ¯”ç‰¹è¿›è¡Œæœ€ä¼˜å……ç”µä¸è·å–æ›´å¤šä¿¡æ¯ä»¥æ”¹è¿›åç»­èƒ½é‡é‡‡é›†è¿™ä¸¤ä¸ªç«äº‰ç›®æ ‡ã€‚ç ”ç©¶è€…åˆ©ç”¨å¼ºåŒ–å­¦ä¹  (Reinforcement Learning) ä¸­çš„æ¢ç´¢ä¸åˆ©ç”¨æƒè¡¡ (exploration-exploitation trade-off) å¼€å‘äº†è‡ªé€‚åº”ç­–ç•¥ã€‚å®éªŒè¯æ˜ï¼Œè¯¥ç­–ç•¥å®ç°çš„èƒ½é‡è€—æ•£ (energy dissipation) éšå‰¯æœ¬æ•°é‡ $N$ çš„å¢é•¿ä»…å‘ˆå¤šå¯¹æ•°çº§ (poly-logarithmically) æ¯”ä¾‹å…³ç³»ã€‚ç›¸æ¯”äºåŸºäºå…¨çŠ¶æ€æ–­å±‚æ‰«æ (full state tomography) çš„ç°æœ‰åè®®ï¼Œè¯¥æ–¹æ³•å®ç°äº†æŒ‡æ•°çº§çš„æ€§èƒ½æå‡ï¼Œä¸ºé«˜æ•ˆé‡å­èƒ½é‡æå–æä¾›äº†æ–°é€”å¾„ã€‚",
      "categories": [
        "quant-ph",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "quant-ph",
      "comment": "5 pages+14 pages, 2 figures",
      "pdf_url": "https://arxiv.org/pdf/2505.09456v1",
      "published_date": "2025-05-14 15:07:58 UTC",
      "updated_date": "2025-05-14 15:07:58 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T08:40:31.543152+00:00"
    },
    {
      "arxiv_id": "2505.09438v2",
      "title": "Evaluating GPT- and Reasoning-based Large Language Models on Physics Olympiad Problems: Surpassing Human Performance and Implications for Educational Assessment",
      "title_zh": "è¯„ä¼° GPT ä¸æ¨ç†å‹å¤§è¯­è¨€æ¨¡å‹åœ¨ç‰©ç†å¥¥èµ›é¢˜ç›®ä¸Šçš„è¡¨ç°ï¼šè¶…è¶Šäººç±»æ°´å¹³åŠå…¶å¯¹æ•™è‚²æµ‹è¯„çš„å¯ç¤º",
      "authors": [
        "Paul Tschisgale",
        "Holger Maus",
        "Fabian Kieser",
        "Ben Kroehs",
        "Stefan Petersen",
        "Peter Wulff"
      ],
      "abstract": "Large language models (LLMs) are now widely accessible, reaching learners at all educational levels. This development has raised concerns that their use may circumvent essential learning processes and compromise the integrity of established assessment formats. In physics education, where problem solving plays a central role in instruction and assessment, it is therefore essential to understand the physics-specific problem-solving capabilities of LLMs. Such understanding is key to informing responsible and pedagogically sound approaches to integrating LLMs into instruction and assessment. This study therefore compares the problem-solving performance of a general-purpose LLM (GPT-4o, using varying prompting techniques) and a reasoning-optimized model (o1-preview) with that of participants of the German Physics Olympiad, based on a set of well-defined Olympiad problems. In addition to evaluating the correctness of the generated solutions, the study analyzes characteristic strengths and limitations of LLM-generated solutions. The findings of this study indicate that both tested LLMs (GPT-4o and o1-preview) demonstrate advanced problem-solving capabilities on Olympiad-type physics problems, on average outperforming the human participants. Prompting techniques had little effect on GPT-4o's performance, while o1-preview almost consistently outperformed both GPT-4o and the human benchmark. Based on these findings, the study discusses implications for the design of summative and formative assessment in physics education, including how to uphold assessment integrity and support students in critically engaging with LLMs.",
      "tldr_zh": "è¯¥ç ”ç©¶æ—¨åœ¨è¯„ä¼°å¤§è¯­è¨€æ¨¡å‹(LLMs)åœ¨è§£å†³ç‰©ç†å¥¥æ—åŒ¹å…‹ç«èµ›(Physics Olympiad)é—®é¢˜ä¸Šçš„èƒ½åŠ›ï¼Œå¹¶æ¢è®¨å…¶å¯¹æ•™è‚²è¯„ä¼°çš„æ·±è¿œå½±å“ã€‚ç ”ç©¶å¯¹æ¯”äº†é€šç”¨æ¨¡å‹GPT-4oï¼ˆç»“åˆä¸åŒæç¤ºæŠ€æœ¯ï¼‰å’Œæ¨ç†ä¼˜åŒ–æ¨¡å‹o1-previewä¸å¾·å›½ç‰©ç†å¥¥æ—åŒ¹å…‹ç«èµ›å‚ä¸è€…çš„å®é™…è¡¨ç°ã€‚å®éªŒå‘ç°ï¼Œè¿™ä¸¤ç§LLMsåœ¨è§£å†³å¤æ‚ç‰©ç†é—®é¢˜æ–¹é¢å‡è¡¨ç°å‡ºå“è¶Šèƒ½åŠ›ï¼Œå¹³å‡è¡¨ç°è¶…è¿‡äº†äººç±»å‚èµ›é€‰æ‰‹ã€‚å…¶ä¸­ï¼Œo1-previewçš„è¡¨ç°å‡ ä¹å§‹ç»ˆä¼˜äºGPT-4oå’Œäººç±»åŸºå‡†ï¼Œè€Œæç¤ºæŠ€æœ¯å¯¹GPT-4oçš„æ€§èƒ½æå‡æ•ˆæœæœ‰é™ã€‚é™¤äº†è¯„ä¼°è§£é¢˜æ­£ç¡®æ€§ï¼Œç ”ç©¶è¿˜æ·±å…¥åˆ†æäº†æ¨¡å‹ç”Ÿæˆæ–¹æ¡ˆçš„ç‰¹å®šä¼˜åŠ¿ä¸å±€é™ã€‚åŸºäºè¿™äº›å‘ç°ï¼Œæ–‡ç« è®¨è®ºäº†å¦‚ä½•é€šè¿‡é‡æ–°è®¾è®¡ç‰©ç†æ•™è‚²ä¸­çš„æ€»ç»“æ€§(summative)å’Œå½¢æˆæ€§(formative)è¯„ä¼°æ¥åº”å¯¹AIæŒ‘æˆ˜ï¼Œæ—¨åœ¨ç»´æŠ¤æ•™å­¦è¯„ä¼°çš„å®Œæ•´æ€§å¹¶æ”¯æŒå­¦ç”Ÿæ‰¹åˆ¤æ€§åœ°ä¸LLMsäº’åŠ¨ã€‚",
      "categories": [
        "physics.ed-ph",
        "cs.AI"
      ],
      "primary_category": "physics.ed-ph",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.09438v2",
      "published_date": "2025-05-14 14:46:32 UTC",
      "updated_date": "2025-07-01 14:16:43 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T08:38:30.611246+00:00"
    },
    {
      "arxiv_id": "2505.09436v2",
      "title": "CXMArena: Unified Dataset to benchmark performance in realistic CXM Scenarios",
      "title_zh": "CXMArenaï¼šé¢å‘çœŸå® CXM åœºæ™¯æ€§èƒ½åŸºå‡†æµ‹è¯•çš„ç»Ÿä¸€æ•°æ®é›†",
      "authors": [
        "Raghav Garg",
        "Kapil Sharma",
        "Karan Gupta"
      ],
      "abstract": "Large Language Models (LLMs) hold immense potential for revolutionizing Customer Experience Management (CXM), particularly in contact center operations. However, evaluating their practical utility in complex operational environments is hindered by data scarcity (due to privacy concerns) and the limitations of current benchmarks. Existing benchmarks often lack realism, failing to incorporate deep knowledge base (KB) integration, real-world noise, or critical operational tasks beyond conversational fluency. To bridge this gap, we introduce CXMArena, a novel, large-scale synthetic benchmark dataset specifically designed for evaluating AI in operational CXM contexts. Given the diversity in possible contact center features, we have developed a scalable LLM-powered pipeline that simulates the brand's CXM entities that form the foundation of our datasets-such as knowledge articles including product specifications, issue taxonomies, and contact center conversations. The entities closely represent real-world distribution because of controlled noise injection (informed by domain experts) and rigorous automated validation. Building on this, we release CXMArena, which provides dedicated benchmarks targeting five important operational tasks: Knowledge Base Refinement, Intent Prediction, Agent Quality Adherence, Article Search, and Multi-turn RAG with Integrated Tools. Our baseline experiments underscore the benchmark's difficulty: even state of the art embedding and generation models achieve only 68% accuracy on article search, while standard embedding methods yield a low F1 score of 0.3 for knowledge base refinement, highlighting significant challenges for current models necessitating complex pipelines and solutions over conventional techniques.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¨å‡ºäº† CXMArenaï¼Œè¿™æ˜¯ä¸€ä¸ªä¸“ä¸ºè¯„ä¼°è¿è¥å®¢æˆ·ä½“éªŒç®¡ç†ï¼ˆCXMï¼‰ç¯å¢ƒä¸­ AI æ€§èƒ½è€Œè®¾è®¡çš„å¤§è§„æ¨¡åˆæˆåŸºå‡†æ•°æ®é›†ï¼Œæ—¨åœ¨è§£å†³ç°æœ‰åŸºå‡†ç¼ºä¹çœŸå®æ„Ÿã€ç¼ºä¹çŸ¥è¯†åº“ï¼ˆKBï¼‰é›†æˆåŠå› éšç§å¯¼è‡´çš„çœŸå®æ•°æ®ç¨€ç¼ºé—®é¢˜ã€‚ç ”ç©¶å›¢é˜Ÿå¼€å‘äº†ä¸€ç§å¯æ‰©å±•çš„ LLM é©±åŠ¨æµæ°´çº¿æ¥æ¨¡æ‹ŸåŒ…æ‹¬äº§å“è§„æ ¼ã€é—®é¢˜åˆ†ç±»ï¼ˆIssue Taxonomiesï¼‰å’Œè”ç»œä¸­å¿ƒå¯¹è¯åœ¨å†…çš„ CXM å®ä½“ï¼Œå¹¶é€šè¿‡å—æ§çš„å™ªå£°æ³¨å…¥ï¼ˆNoise Injectionï¼‰å’Œä¸¥æ ¼çš„è‡ªåŠ¨åŒ–éªŒè¯ç¡®ä¿å…¶ç¬¦åˆç°å®åˆ†å¸ƒã€‚CXMArena é’ˆå¯¹äº”é¡¹å…³é”®è¿è¥ä»»åŠ¡è®¾å®šäº†ä¸“ç”¨åŸºå‡†ï¼Œåˆ†åˆ«æ˜¯ï¼šçŸ¥è¯†åº“ä¼˜åŒ–ï¼ˆKnowledge Base Refinementï¼‰ã€æ„å›¾é¢„æµ‹ï¼ˆIntent Predictionï¼‰ã€åå¸­è´¨é‡åˆè§„ï¼ˆAgent Quality Adherenceï¼‰ã€æ–‡ç« æœç´¢ï¼ˆArticle Searchï¼‰ä»¥åŠå¸¦æœ‰é›†æˆå·¥å…·çš„å¤šè½®æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆMulti-turn RAGï¼‰ã€‚å®éªŒç»“æœè¡¨æ˜è¯¥åŸºå‡†å…·æœ‰æ˜¾è‘—éš¾åº¦ï¼Œå³ä¾¿æ˜¯æœ€å…ˆè¿›çš„åµŒå…¥å’Œç”Ÿæˆæ¨¡å‹åœ¨æ–‡ç« æœç´¢ä»»åŠ¡ä¸­çš„å‡†ç¡®ç‡ä¹Ÿä»…ä¸º 68%ï¼Œè€Œåœ¨çŸ¥è¯†åº“ä¼˜åŒ–ä»»åŠ¡ä¸­çš„ F1 åˆ†æ•°ä½è‡³ 0.3ã€‚è¿™é¡¹å·¥ä½œçªæ˜¾äº†å½“å‰æ¨¡å‹åœ¨å¤„ç†å¤æ‚è¿è¥åœºæ™¯æ—¶çš„å±€é™æ€§ï¼ŒæŒ‡å‡ºäº†å¼€å‘è¶…è¶Šä¼ ç»ŸæŠ€æœ¯çš„å¤æ‚æµæ°´çº¿å’Œè§£å†³æ–¹æ¡ˆçš„å¿…è¦æ€§ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL",
        "cs.IR"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.09436v2",
      "published_date": "2025-05-14 14:44:30 UTC",
      "updated_date": "2025-05-19 06:27:01 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T08:37:45.978003+00:00"
    },
    {
      "arxiv_id": "2505.09435v1",
      "title": "Endo-CLIP: Progressive Self-Supervised Pre-training on Raw Colonoscopy Records",
      "title_zh": "Endo-CLIPï¼šåŸºäºåŸå§‹ç»“è‚ é•œè®°å½•çš„æ¸è¿›å¼è‡ªç›‘ç£é¢„è®­ç»ƒ",
      "authors": [
        "Yili He",
        "Yan Zhu",
        "Peiyao Fu",
        "Ruijie Yang",
        "Tianyi Chen",
        "Zhihua Wang",
        "Quanlin Li",
        "Pinghong Zhou",
        "Xian Yang",
        "Shuo Wang"
      ],
      "abstract": "Pre-training on image-text colonoscopy records offers substantial potential for improving endoscopic image analysis, but faces challenges including non-informative background images, complex medical terminology, and ambiguous multi-lesion descriptions. We introduce Endo-CLIP, a novel self-supervised framework that enhances Contrastive Language-Image Pre-training (CLIP) for this domain. Endo-CLIP's three-stage framework--cleansing, attunement, and unification--addresses these challenges by (1) removing background frames, (2) leveraging large language models to extract clinical attributes for fine-grained contrastive learning, and (3) employing patient-level cross-attention to resolve multi-polyp ambiguities. Extensive experiments demonstrate that Endo-CLIP significantly outperforms state-of-the-art pre-training methods in zero-shot and few-shot polyp detection and classification, paving the way for more accurate and clinically relevant endoscopic analysis.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†Endo-CLIPï¼Œä¸€ç§ä¸“ä¸ºç»“è‚ é•œé¢†åŸŸè®¾è®¡çš„è‡ªç›‘ç£å¯¹æ¯”è¯­è¨€-å›¾åƒé¢„è®­ç»ƒ(Contrastive Language-Image Pre-training)å¢å¼ºæ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³å›¾åƒ-æ–‡æœ¬è®°å½•ä¸­çš„èƒŒæ™¯å™ªå£°ã€æœ¯è¯­å¤æ‚å’Œå¤šç—…ç¶æè¿°æ¨¡ç³Šç­‰æŒ‘æˆ˜ã€‚è¯¥æ¡†æ¶é‡‡ç”¨äº†æ¸…æ´—(cleansing)ã€åè°ƒ(attunement)å’Œç»Ÿä¸€(unification)çš„ä¸‰é˜¶æ®µæµç¨‹ï¼Œé€šè¿‡åˆ©ç”¨å¤§è¯­è¨€æ¨¡å‹(LLMs)æå–ç»†ç²’åº¦ä¸´åºŠå±æ€§è¿›è¡Œå¯¹æ¯”å­¦ä¹ ï¼Œå¹¶å¼•å…¥æ‚£è€…å±‚é¢çš„äº¤å‰æ³¨æ„åŠ›(cross-attention)æœºåˆ¶æ¥æ¶ˆé™¤å¤šæ¯è‚‰æ­§ä¹‰ã€‚å®éªŒç»“æœè¯æ˜ï¼ŒEndo-CLIPåœ¨é›¶æ ·æœ¬(zero-shot)å’Œå°‘æ ·æœ¬(few-shot)æ¯è‚‰æ£€æµ‹ä¸åˆ†ç±»ä»»åŠ¡ä¸­æ˜¾è‘—ä¼˜äºç°æœ‰çš„æœ€å…ˆè¿›æ–¹æ³•ã€‚è¿™ä¸€æˆæœä¸ºå®ç°æ›´ç²¾ç¡®ã€æ›´å…·ä¸´åºŠç›¸å…³æ€§çš„å†…çª¥é•œè‡ªåŠ¨åŒ–åˆ†æå¥ å®šäº†åšå®åŸºç¡€ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "Early accepted to MICCAI 2025",
      "pdf_url": "https://arxiv.org/pdf/2505.09435v1",
      "published_date": "2025-05-14 14:43:31 UTC",
      "updated_date": "2025-05-14 14:43:31 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T08:38:45.412677+00:00"
    },
    {
      "arxiv_id": "2507.19483v1",
      "title": "The Architecture of Cognitive Amplification: Enhanced Cognitive Scaffolding as a Resolution to the Comfort-Growth Paradox in Human-AI Cognitive Integration",
      "title_zh": "è®¤çŸ¥å¢å¼ºæ¶æ„ï¼šé€šè¿‡å¢å¼ºå‹è®¤çŸ¥æ”¯æ¶åŒ–è§£äººæœºè®¤çŸ¥èåˆä¸­çš„â€œèˆ’é€‚-æˆé•¿â€æ‚–è®º",
      "authors": [
        "Giuseppe Riva"
      ],
      "abstract": "AI systems now function as cognitive extensions, evolving from tools to active cognitive collaborators within human-AI integrated systems. While these systems can amplify cognition - enhancing problem-solving, learning, and creativity - they present a fundamental \"comfort-growth paradox\": AI's user-friendly nature may foster intellectual stagnation by minimizing cognitive friction necessary for development. As AI aligns with user preferences and provides frictionless assistance, it risks inducing cognitive complacency rather than promoting growth. We introduce Enhanced Cognitive Scaffolding to resolve this paradox - reconceptualizing AI from convenient assistant to dynamic mentor. Drawing from Vygotskian theories, educational scaffolding principles, and AI ethics, our framework integrates three dimensions: (1) Progressive Autonomy, where AI support gradually fades as user competence increases; (2) Adaptive Personalization, tailoring assistance to individual needs and learning trajectories; and (3) Cognitive Load Optimization, balancing mental effort to maximize learning while minimizing unnecessary complexity. Research across educational, workplace, creative, and healthcare domains supports this approach, demonstrating accelerated skill acquisition, improved self-regulation, and enhanced higher-order thinking. The framework includes safeguards against risks like dependency, skill atrophy, and bias amplification. By prioritizing cognitive development over convenience in human-AI interaction, Enhanced Cognitive Scaffolding offers a pathway toward genuinely amplified cognition while safeguarding autonomous thought and continuous learning.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†äººæœºé›†æˆç³»ç»Ÿä¸­çš„â€œèˆ’é€‚-æˆé•¿æ‚–è®º(Comfort-Growth Paradox)â€ï¼Œå³äººå·¥æ™ºèƒ½(AI)çš„æ˜“ç”¨æ€§å¯èƒ½å› å‡å°‘å¿…è¦çš„â€œè®¤çŸ¥æ‘©æ“¦(Cognitive Friction)â€è€Œå¯¼è‡´ç”¨æˆ·æ™ºåŠ›åœæ»ã€‚ä¸ºè§£å†³è¿™ä¸€çŸ›ç›¾ï¼Œè®ºæ–‡æå‡ºäº†â€œå¢å¼ºè®¤çŸ¥æ”¯æ¶(Enhanced Cognitive Scaffolding)â€æ¶æ„ï¼Œæ—¨åœ¨å°†AIä»ä¾¿æ·åŠ©æ‰‹é‡å¡‘ä¸ºåŠ¨æ€å¯¼å¸ˆã€‚è¯¥æ¡†æ¶æ•´åˆäº†ä¸‰ä¸ªæ ¸å¿ƒç»´åº¦ï¼šæ”¯æŒåº¦éšç”¨æˆ·èƒ½åŠ›æå‡è€Œé€æ¸å‡å¼±çš„æ¸è¿›å¼è‡ªä¸»(Progressive Autonomy)ã€é’ˆå¯¹å­¦ä¹ è·¯å¾„å®šåˆ¶çš„è‡ªé€‚åº”ä¸ªæ€§åŒ–(Adaptive Personalization)ä»¥åŠå¹³è¡¡å¿ƒç†åŠªåŠ›çš„è®¤çŸ¥è´Ÿè·ä¼˜åŒ–(Cognitive Load Optimization)ã€‚å¤šé¢†åŸŸçš„ç ”ç©¶è¯æ®è¡¨æ˜ï¼Œè¯¥æ–¹æ³•èƒ½æœ‰æ•ˆåŠ é€ŸæŠ€èƒ½ä¹ å¾—ï¼Œå¹¶æå‡ç”¨æˆ·çš„è‡ªæˆ‘è°ƒèŠ‚ä¸é«˜é˜¶æ€ç»´èƒ½åŠ›ã€‚æ­¤å¤–ï¼Œæ¡†æ¶è¿˜åŒ…å«äº†é˜²æ­¢æŠ€æœ¯ä¾èµ–ã€æŠ€èƒ½èç¼©(Skill Atrophy)å’Œåè§æ”¾å¤§çš„å®‰å…¨ä¿éšœæœºåˆ¶ã€‚é€šè¿‡åœ¨äººæœºäº¤äº’ä¸­ä¼˜å…ˆè€ƒè™‘è®¤çŸ¥å‘å±•è€Œéå•çº¯çš„ä¾¿åˆ©æ€§ï¼Œè¯¥æ¶æ„ä¸ºå®ç°çœŸæ­£çš„è®¤çŸ¥å¢å¼ºä¸è‡ªä¸»æ€è€ƒæä¾›äº†æ¼”è¿›è·¯å¾„ã€‚",
      "categories": [
        "cs.HC",
        "cs.AI"
      ],
      "primary_category": "cs.HC",
      "comment": "39 Pages, no figures",
      "pdf_url": "https://arxiv.org/pdf/2507.19483v1",
      "published_date": "2025-05-14 14:32:12 UTC",
      "updated_date": "2025-05-14 14:32:12 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T08:41:07.067725+00:00"
    },
    {
      "arxiv_id": "2505.09412v1",
      "title": "Counterfactual Strategies for Markov Decision Processes",
      "title_zh": "é©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹çš„åäº‹å®ç­–ç•¥",
      "authors": [
        "Paul Kobialka",
        "Lina Gerlach",
        "Francesco Leofante",
        "Erika ÃbrahÃ¡m",
        "Silvia Lizeth Tapia Tarifa",
        "Einar Broch Johnsen"
      ],
      "abstract": "Counterfactuals are widely used in AI to explain how minimal changes to a model's input can lead to a different output. However, established methods for computing counterfactuals typically focus on one-step decision-making, and are not directly applicable to sequential decision-making tasks. This paper fills this gap by introducing counterfactual strategies for Markov Decision Processes (MDPs). During MDP execution, a strategy decides which of the enabled actions (with known probabilistic effects) to execute next. Given an initial strategy that reaches an undesired outcome with a probability above some limit, we identify minimal changes to the initial strategy to reduce that probability below the limit. We encode such counterfactual strategies as solutions to non-linear optimization problems, and further extend our encoding to synthesize diverse counterfactual strategies. We evaluate our approach on four real-world datasets and demonstrate its practical viability in sophisticated sequential decision-making tasks.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ä¼ ç»Ÿ counterfactual æ–¹æ³•ä¸»è¦é›†ä¸­äºå•æ­¥å†³ç­–ã€éš¾ä»¥ç›´æ¥åº”ç”¨äº sequential decision-making ä»»åŠ¡çš„å±€é™æ€§ï¼Œåˆ›æ–°æ€§åœ°æå‡ºäº†é€‚ç”¨äº Markov Decision Processes (MDPs) çš„ counterfactual strategiesã€‚åœ¨ MDP æ‰§è¡Œè¿‡ç¨‹ä¸­ï¼Œå½“åˆå§‹ç­–ç•¥å¯¼è‡´ä¸è‰¯ç»“æœçš„æ¦‚ç‡è¶…è¿‡ç‰¹å®šé™åˆ¶æ—¶ï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿè¯†åˆ«å¯¹åˆå§‹ç­–ç•¥çš„æœ€å°å˜æ›´ï¼Œä»è€Œå°†è¯¥æ¦‚ç‡æœ‰æ•ˆé™ä½è‡³é˜ˆå€¼ä»¥ä¸‹ã€‚ç ”ç©¶å›¢é˜Ÿå°†è¿™äº› counterfactual strategies ç¼–ç ä¸º non-linear optimization é—®é¢˜çš„è§£å†³æ–¹æ¡ˆï¼Œå¹¶è¿›ä¸€æ­¥æ‰©å±•äº†ç¼–ç æ–¹å¼ä»¥åˆæˆå¤šæ ·åŒ–çš„åäº‹å®ç­–ç•¥ã€‚é€šè¿‡åœ¨å››ä¸ªçœŸå®ä¸–ç•Œæ•°æ®é›†ä¸Šçš„è¯„ä¼°ï¼Œå®éªŒç»“æœè¯æ˜äº†è¯¥æ–¹æ³•åœ¨å¤„ç†å¤æ‚ sequential decision-making ä»»åŠ¡æ—¶çš„å®é™…å¯è¡Œæ€§ä¸åº”ç”¨ä»·å€¼ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.09412v1",
      "published_date": "2025-05-14 14:07:27 UTC",
      "updated_date": "2025-05-14 14:07:27 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T08:41:04.154229+00:00"
    },
    {
      "arxiv_id": "2505.09407v1",
      "title": "Multilingual Machine Translation with Quantum Encoder Decoder Attention-based Convolutional Variational Circuits",
      "title_zh": "åŸºäºé‡å­ç¼–ç å™¨-è§£ç å™¨æ³¨æ„åŠ›å·ç§¯å˜åˆ†çº¿è·¯çš„å¤šè¯­è¨€æœºå™¨ç¿»è¯‘",
      "authors": [
        "Subrit Dikshit",
        "Ritu Tiwari",
        "Priyank Jain"
      ],
      "abstract": "Cloud-based multilingual translation services like Google Translate and Microsoft Translator achieve state-of-the-art translation capabilities. These services inherently use large multilingual language models such as GRU, LSTM, BERT, GPT, T5, or similar encoder-decoder architectures with attention mechanisms as the backbone. Also, new age natural language systems, for instance ChatGPT and DeepSeek, have established huge potential in multiple tasks in natural language processing. At the same time, they also possess outstanding multilingual translation capabilities. However, these models use the classical computing realm as a backend. QEDACVC (Quantum Encoder Decoder Attention-based Convolutional Variational Circuits) is an alternate solution that explores the quantum computing realm instead of the classical computing realm to study and demonstrate multilingual machine translation. QEDACVC introduces the quantum encoder-decoder architecture that simulates and runs on quantum computing hardware via quantum convolution, quantum pooling, quantum variational circuit, and quantum attention as software alterations. QEDACVC achieves an Accuracy of 82% when trained on the OPUS dataset for English, French, German, and Hindi corpora for multilingual translations.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†QEDACVCï¼ˆQuantum Encoder Decoder Attention-based Convolutional Variational Circuitsï¼‰ï¼Œè¿™æ˜¯ä¸€ç§æ—¨åœ¨æ¢ç´¢å¤šè¯­è¨€æœºå™¨ç¿»è¯‘ï¼ˆMultilingual Machine Translationï¼‰é¢†åŸŸé‡å­è®¡ç®—æ½œåŠ›çš„æ›¿ä»£æ–¹æ¡ˆã€‚QEDACVCå¼•å…¥äº†é‡å­ç¼–ç å™¨-è§£ç å™¨æ¶æ„ï¼ˆQuantum Encoder-Decoder Architectureï¼‰ï¼Œé€šè¿‡é‡å­å·ç§¯ï¼ˆQuantum Convolutionï¼‰ã€é‡å­æ± åŒ–ï¼ˆQuantum Poolingï¼‰ã€é‡å­å˜åˆ†ç”µè·¯ï¼ˆQuantum Variational Circuitï¼‰ä»¥åŠé‡å­æ³¨æ„åŠ›ï¼ˆQuantum Attentionï¼‰åœ¨é‡å­è®¡ç®—ç¡¬ä»¶ä¸Šæ¨¡æ‹Ÿè¿è¡Œã€‚ä¸ä¾èµ–ç»å…¸è®¡ç®—ï¼ˆClassical Computingï¼‰åç«¯çš„å¤§è¯­è¨€æ¨¡å‹ä¸åŒï¼Œè¯¥æ¨¡å‹ä¸“æ³¨äºéªŒè¯é‡å­ç®—æ³•åœ¨è·¨è¯­è¨€è‡ªç„¶è¯­è¨€å¤„ç†ä»»åŠ¡ä¸­çš„æœ‰æ•ˆæ€§ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œåœ¨é’ˆå¯¹è‹±è¯­ã€æ³•è¯­ã€å¾·è¯­å’Œå°åœ°è¯­è¯­æ–™åº“çš„OPUSæ•°æ®é›†è®­ç»ƒä¸­ï¼ŒQEDACVCå®ç°äº†82%çš„å‡†ç¡®ç‡ï¼ˆAccuracyï¼‰ã€‚è¿™é¡¹ç ”ç©¶å±•ç¤ºäº†é‡å­æŠ€æœ¯åœ¨å¤šè¯­è¨€ç¿»è¯‘ä¸­çš„åº”ç”¨å‰æ™¯ï¼Œä¸ºæœªæ¥ä»ç»å…¸è®¡ç®—å‘é‡å­è®¡ç®—èŒƒå¼è½¬å˜æä¾›äº†é‡è¦çš„å®éªŒä¾æ®ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.ET"
      ],
      "primary_category": "cs.CL",
      "comment": "12 pages, 12 figures",
      "pdf_url": "https://arxiv.org/pdf/2505.09407v1",
      "published_date": "2025-05-14 14:04:44 UTC",
      "updated_date": "2025-05-14 14:04:44 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T08:41:01.918804+00:00"
    },
    {
      "arxiv_id": "2505.09396v2",
      "title": "The Influence of Human-inspired Agentic Sophistication in LLM-driven Strategic Reasoners",
      "title_zh": "å¤§è¯­è¨€æ¨¡å‹é©±åŠ¨ç­–ç•¥æ¨ç†å™¨ä¸­å—äººç±»å¯å‘çš„æ™ºèƒ½ä½“ç²¾ç»†åŒ–ç¨‹åº¦çš„å½±å“",
      "authors": [
        "Vince Trencsenyi",
        "Agnieszka Mensfelt",
        "Kostas Stathis"
      ],
      "abstract": "The rapid rise of large language models (LLMs) has shifted artificial intelligence (AI) research toward agentic systems, motivating the use of weaker and more flexible notions of agency. However, this shift raises key questions about the extent to which LLM-based agents replicate human strategic reasoning, particularly in game-theoretic settings. In this context, we examine the role of agentic sophistication in shaping artificial reasoners' performance by evaluating three agent designs: a simple game-theoretic model, an unstructured LLM-as-agent model, and an LLM integrated into a traditional agentic framework. Using guessing games as a testbed, we benchmarked these agents against human participants across general reasoning patterns and individual role-based objectives. Furthermore, we introduced obfuscated game scenarios to assess agents' ability to generalise beyond training distributions. Our analysis, covering over 2000 reasoning samples across 25 agent configurations, shows that human-inspired cognitive structures can enhance LLM agents' alignment with human strategic behaviour. Still, the relationship between agentic design complexity and human-likeness is non-linear, highlighting a critical dependence on underlying LLM capabilities and suggesting limits to simple architectural augmentation.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†äººç±»å¯å‘å¼æ™ºèƒ½å¤æ‚æ€§ï¼ˆHuman-inspired Agentic Sophisticationï¼‰å¯¹å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰é©±åŠ¨çš„ç­–ç•¥æ¨ç†è€…æ€§èƒ½çš„å½±å“ã€‚å®éªŒè¯„ä¼°äº†ä¸‰ç§æ™ºèƒ½ä½“è®¾è®¡ï¼ŒåŒ…æ‹¬ç®€å•çš„åšå¼ˆè®ºæ¨¡å‹ï¼ˆSimple Game-theoretic Modelï¼‰ã€æ— ç»“æ„çš„ LLM å³æ™ºèƒ½ä½“æ¨¡å‹ï¼ˆUnstructured LLM-as-agentï¼‰ä»¥åŠé›†æˆåˆ°ä¼ ç»Ÿæ™ºèƒ½ä½“æ¡†æ¶ä¸­çš„ LLMã€‚ç ”ç©¶è€…ä»¥çŒœæ•°æ¸¸æˆï¼ˆGuessing Gamesï¼‰ä¸ºå®éªŒå¹³å°ï¼Œé€šè¿‡é€šç”¨æ¨ç†æ¨¡å¼å’Œç‰¹å®šè§’è‰²ç›®æ ‡ï¼Œå°†æ™ºèƒ½ä½“çš„è¡¨ç°ä¸äººç±»å‚ä¸è€…è¿›è¡Œäº†åŸºå‡†å¯¹æ¯”ã€‚æ­¤å¤–ï¼Œç ”ç©¶è¿˜å¼•å…¥äº†æ¨¡ç³ŠåŒ–åšå¼ˆåœºæ™¯ï¼ˆObfuscated Game Scenariosï¼‰ä»¥æµ‹è¯•æ¨¡å‹åœ¨åˆ†å¸ƒå¤–æ•°æ®ä¸Šçš„æ³›åŒ–èƒ½åŠ›ã€‚å¯¹è¶…è¿‡ 2000 ä¸ªæ¨ç†æ ·æœ¬çš„åˆ†æè¡¨æ˜ï¼Œå—äººç±»å¯å‘çš„è®¤çŸ¥ç»“æ„èƒ½æ˜¾è‘—æå‡ LLM æ™ºèƒ½ä½“ä¸äººç±»ç­–ç•¥è¡Œä¸ºçš„ä¸€è‡´æ€§ï¼ˆAlignmentï¼‰ã€‚ç„¶è€Œï¼Œç ”ç©¶ä¹Ÿå‘ç°æ™ºèƒ½ä½“è®¾è®¡çš„å¤æ‚æ€§ä¸ç±»äººè¡¨ç°ä¹‹é—´å­˜åœ¨éçº¿æ€§å…³ç³»ï¼Œè¡¨æ˜å…¶é«˜åº¦ä¾èµ–åº•å±‚ LLM çš„æ ¸å¿ƒèƒ½åŠ›ï¼Œæš—ç¤ºäº†å•çº¯ä¾é æ¶æ„å¢å¼ºå­˜åœ¨å±€é™æ€§ã€‚",
      "categories": [
        "cs.AI",
        "cs.MA"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.09396v2",
      "published_date": "2025-05-14 13:51:24 UTC",
      "updated_date": "2025-08-26 10:19:45 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T08:41:01.035063+00:00"
    },
    {
      "arxiv_id": "2505.09395v1",
      "title": "Quantum-Enhanced Parameter-Efficient Learning for Typhoon Trajectory Forecasting",
      "title_zh": "é¢å‘å°é£è·¯å¾„é¢„æŠ¥çš„é‡å­å¢å¼ºå‚æ•°é«˜æ•ˆå­¦ä¹ ",
      "authors": [
        "Chen-Yu Liu",
        "Kuan-Cheng Chen",
        "Yi-Chien Chen",
        "Samuel Yen-Chi Chen",
        "Wei-Hao Huang",
        "Wei-Jia Huang",
        "Yen-Jui Chang"
      ],
      "abstract": "Typhoon trajectory forecasting is essential for disaster preparedness but remains computationally demanding due to the complexity of atmospheric dynamics and the resource requirements of deep learning models. Quantum-Train (QT), a hybrid quantum-classical framework that leverages quantum neural networks (QNNs) to generate trainable parameters exclusively during training, eliminating the need for quantum hardware at inference time. Building on QT's success across multiple domains, including image classification, reinforcement learning, flood prediction, and large language model (LLM) fine-tuning, we introduce Quantum Parameter Adaptation (QPA) for efficient typhoon forecasting model learning. Integrated with an Attention-based Multi-ConvGRU model, QPA enables parameter-efficient training while maintaining predictive accuracy. This work represents the first application of quantum machine learning (QML) to large-scale typhoon trajectory prediction, offering a scalable and energy-efficient approach to climate modeling. Our results demonstrate that QPA significantly reduces the number of trainable parameters while preserving performance, making high-performance forecasting more accessible and sustainable through hybrid quantum-classical learning.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å°é£è·¯å¾„é¢„æµ‹ä¸­å¤æ‚å¤§æ°”åŠ¨åŠ›å­¦å¸¦æ¥çš„é«˜è®¡ç®—éœ€æ±‚é—®é¢˜ï¼Œæå‡ºäº†Quantum Parameter Adaptation (QPA)æ¡†æ¶ã€‚è¯¥æ–¹æ³•åŸºäºQuantum-Train (QT)è¿™ä¸€æ··åˆé‡å­-ç»å…¸æ¡†æ¶ï¼Œåˆ©ç”¨Quantum Neural Networks (QNNs)åœ¨è®­ç»ƒé˜¶æ®µç”Ÿæˆå¯è®­ç»ƒå‚æ•°ï¼Œä»è€Œä½¿æ¨¡å‹åœ¨æ¨ç†é˜¶æ®µæ— éœ€é‡å­ç¡¬ä»¶æ”¯æŒã€‚ç ”ç©¶è€…å°†QPAä¸Attention-based Multi-ConvGRUæ¨¡å‹ç›¸ç»“åˆï¼Œå®ç°äº†å‚æ•°é«˜æ•ˆ(Parameter-Efficient)çš„å­¦ä¹ æ¨¡å¼ï¼ŒåŒæ—¶ç¡®ä¿äº†é¢„æµ‹çš„å‡†ç¡®æ€§ã€‚è¿™æ˜¯Quantum Machine Learning (QML)åœ¨å¤§å‹å°é£è·¯å¾„é¢„æµ‹é¢†åŸŸçš„é¦–æ¬¡åº”ç”¨ï¼Œä¸ºæ°”å€™å»ºæ¨¡æä¾›äº†ä¸€ç§å…·æœ‰å¯æ‰©å±•æ€§ä¸”èƒ½æºé«˜æ•ˆçš„æ–¹æ¡ˆã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒQPAåœ¨ä¿æŒæ¨¡å‹æ€§èƒ½çš„åŒæ—¶æ˜¾è‘—å‡å°‘äº†å¯è®­ç»ƒå‚æ•°çš„æ•°é‡ã€‚è¿™ç§æ··åˆå­¦ä¹ æ–¹æ³•æœ‰æ•ˆé™ä½äº†é«˜æ€§èƒ½é¢„æµ‹çš„é—¨æ§›ï¼Œå¹¶ä¸ºæ°”è±¡é¢„æŠ¥çš„å¯æŒç»­æ€§æä¾›äº†æ–°çš„æŠ€æœ¯æ”¯æ’‘ã€‚",
      "categories": [
        "quant-ph",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "quant-ph",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.09395v1",
      "published_date": "2025-05-14 13:50:44 UTC",
      "updated_date": "2025-05-14 13:50:44 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T08:41:18.420140+00:00"
    },
    {
      "arxiv_id": "2505.09393v1",
      "title": "UMotion: Uncertainty-driven Human Motion Estimation from Inertial and Ultra-wideband Units",
      "title_zh": "UMotionï¼šåŸºäºæƒ¯æ€§ä¸è¶…å®½å¸¦å•å…ƒçš„ä¸ç¡®å®šæ€§é©±åŠ¨äººä½“è¿åŠ¨ä¼°è®¡",
      "authors": [
        "Huakun Liu",
        "Hiroki Ota",
        "Xin Wei",
        "Yutaro Hirao",
        "Monica Perusquia-Hernandez",
        "Hideaki Uchiyama",
        "Kiyoshi Kiyokawa"
      ],
      "abstract": "Sparse wearable inertial measurement units (IMUs) have gained popularity for estimating 3D human motion. However, challenges such as pose ambiguity, data drift, and limited adaptability to diverse bodies persist. To address these issues, we propose UMotion, an uncertainty-driven, online fusing-all state estimation framework for 3D human shape and pose estimation, supported by six integrated, body-worn ultra-wideband (UWB) distance sensors with IMUs. UWB sensors measure inter-node distances to infer spatial relationships, aiding in resolving pose ambiguities and body shape variations when combined with anthropometric data. Unfortunately, IMUs are prone to drift, and UWB sensors are affected by body occlusions. Consequently, we develop a tightly coupled Unscented Kalman Filter (UKF) framework that fuses uncertainties from sensor data and estimated human motion based on individual body shape. The UKF iteratively refines IMU and UWB measurements by aligning them with uncertain human motion constraints in real-time, producing optimal estimates for each. Experiments on both synthetic and real-world datasets demonstrate the effectiveness of UMotion in stabilizing sensor data and the improvement over state of the art in pose accuracy.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† UMotionï¼Œè¿™æ˜¯ä¸€ä¸ªä¸ç¡®å®šæ€§é©±åŠ¨çš„åœ¨çº¿å…¨èåˆçŠ¶æ€ä¼°è®¡æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³ç¨€ç–å¯ç©¿æˆ´æƒ¯æ€§æµ‹é‡å•å…ƒ (IMUs) åœ¨ 3D äººä½“è¿åŠ¨ä¼°è®¡ä¸­é¢ä¸´çš„å§¿æ€æ­§ä¹‰ã€æ•°æ®æ¼‚ç§»å’Œèº«ä½“é€‚åº”æ€§æœ‰é™ç­‰æŒ‘æˆ˜ã€‚è¯¥æ¡†æ¶é›†æˆäº†å…­ä¸ªä½©æˆ´åœ¨èº«ä¸Šçš„è¶…å®½å¸¦ (UWB) è·ç¦»ä¼ æ„Ÿå™¨ä¸ IMUï¼Œé€šè¿‡æµ‹é‡èŠ‚ç‚¹é—´è·ç¦»å¹¶ç»“åˆäººä½“æµ‹é‡æ•°æ®æ¥æ¨æ–­ç©ºé—´å…³ç³»ï¼Œè¾…åŠ©è§£å†³å§¿åŠ¿æ­§ä¹‰å’Œä½“å‹å·®å¼‚é—®é¢˜ã€‚ä¸ºäº†åº”å¯¹ IMU æ˜“æ¼‚ç§»å’Œ UWB å—èº«ä½“é®æŒ¡å½±å“çš„éš¾é¢˜ï¼Œç ”ç©¶å¼€å‘äº†ä¸€ä¸ªç´§è€¦åˆçš„æ— è¿¹å¡å°”æ›¼æ»¤æ³¢ (Unscented Kalman Filter, UKF) æ¡†æ¶ï¼Œå°†æ¥è‡ªä¼ æ„Ÿå™¨æ•°æ®åŠåŸºäºä¸ªä½“ä½“å‹ä¼°è®¡çš„è¿åŠ¨ä¸ç¡®å®šæ€§è¿›è¡Œæœ‰æ•ˆèåˆã€‚UKF é€šè¿‡å®æ—¶å°† IMU å’Œ UWB æµ‹é‡å€¼ä¸ä¸ç¡®å®šçš„äººä½“è¿åŠ¨çº¦æŸç›¸å¯¹é½å¹¶è¿›è¡Œè¿­ä»£ä¼˜åŒ–ï¼Œä»è€Œä¸ºæ¯ä¸€é¡¹æŒ‡æ ‡ç”Ÿæˆæœ€ä¼˜ä¼°è®¡ã€‚åœ¨åˆæˆå’ŒçœŸå®æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒUMotion åœ¨ç¨³å®šä¼ æ„Ÿå™¨æ•°æ®æ–¹é¢å…·æœ‰æ˜¾è‘—æ•ˆæœï¼Œä¸”åœ¨å§¿æ€å‡†ç¡®æ€§ä¸Šè¶…è¶Šäº†ç°æœ‰çš„å…ˆè¿›æŠ€æœ¯ã€‚",
      "categories": [
        "cs.GR",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.GR",
      "comment": "Accepted by CVPR 2025",
      "pdf_url": "https://arxiv.org/pdf/2505.09393v1",
      "published_date": "2025-05-14 13:48:36 UTC",
      "updated_date": "2025-05-14 13:48:36 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T08:41:30.597629+00:00"
    },
    {
      "arxiv_id": "2505.09661v2",
      "title": "Introducing voice timbre attribute detection",
      "title_zh": "éŸ³è‰²å±æ€§æ£€æµ‹çš„å¼•å…¥",
      "authors": [
        "Jinghao He",
        "Zhengyan Sheng",
        "Liping Chen",
        "Kong Aik Lee",
        "Zhen-Hua Ling"
      ],
      "abstract": "This paper focuses on explaining the timbre conveyed by speech signals and introduces a task termed voice timbre attribute detection (vTAD). In this task, voice timbre is explained with a set of sensory attributes describing its human perception. A pair of speech utterances is processed, and their intensity is compared in a designated timbre descriptor. Moreover, a framework is proposed, which is built upon the speaker embeddings extracted from the speech utterances. The investigation is conducted on the VCTK-RVA dataset. Experimental examinations on the ECAPA-TDNN and FACodec speaker encoders demonstrated that: 1) the ECAPA-TDNN speaker encoder was more capable in the seen scenario, where the testing speakers were included in the training set; 2) the FACodec speaker encoder was superior in the unseen scenario, where the testing speakers were not part of the training, indicating enhanced generalization capability. The VCTK-RVA dataset and open-source code are available on the website https://github.com/vTAD2025-Challenge/vTAD.",
      "tldr_zh": "è¿™é¡¹ç ”ç©¶é‡ç‚¹è§£é‡Šäº†è¯­éŸ³ä¿¡å·ä¼ è¾¾çš„éŸ³è‰²ç‰¹å¾ï¼Œå¹¶å¼•å…¥äº†ä¸€é¡¹åä¸ºè¯­éŸ³éŸ³è‰²å±æ€§æ£€æµ‹(voice timbre attribute detection, vTAD)çš„æ–°ä»»åŠ¡ã€‚è¯¥ä»»åŠ¡é€šè¿‡ä¸€ç»„æè¿°äººç±»æ„ŸçŸ¥çš„æ„Ÿå®˜å±æ€§æ¥å®šä¹‰è¯­éŸ³éŸ³è‰²ï¼Œå¹¶å¯¹æ¯”ä¸€å¯¹è¯­éŸ³ç‰‡æ®µåœ¨ç‰¹å®šéŸ³è‰²æè¿°ç¬¦ä¸­çš„å¼ºåº¦å·®å¼‚ã€‚ç ”ç©¶æå‡ºäº†ä¸€ä¸ªåŸºäºè¯´è¯äººåµŒå…¥(speaker embeddings)çš„åˆ†ææ¡†æ¶ï¼Œå¹¶åœ¨ VCTK-RVA æ•°æ®é›†ä¸Šè¿›è¡Œäº†ç³»ç»ŸéªŒè¯ã€‚å®éªŒå¯¹æ¯”äº† ECAPA-TDNN å’Œ FACodec ä¸¤ç§è¯´è¯äººç¼–ç å™¨çš„æ€§èƒ½ï¼Œå‘ç° ECAPA-TDNN åœ¨åŒ…å«å·²çŸ¥è¯´è¯äºº(seen scenario)çš„åœºæ™¯ä¸­è¡¨ç°æ›´å¼ºã€‚è€Œ FACodec ç¼–ç å™¨åœ¨å¤„ç†æœªè§è¯´è¯äºº(unseen scenario)çš„åœºæ™¯ä¸­å±•ç°å‡ºæ›´ä¼˜è¶Šçš„æ€§èƒ½ï¼Œå…·æœ‰æ›´å¼ºçš„æ³›åŒ–èƒ½åŠ›ã€‚è¯¥ç ”ç©¶ä¸ºéŸ³è‰²æ„ŸçŸ¥åˆ†ææä¾›äº†æ–°çš„è§†è§’ï¼Œå¹¶å…¬å¼€äº†ç›¸å…³æ•°æ®é›†ä¸æºä»£ç ä»¥ä¾›åç»­ç ”ç©¶ä½¿ç”¨ã€‚",
      "categories": [
        "cs.SD",
        "cs.AI",
        "eess.AS"
      ],
      "primary_category": "cs.SD",
      "comment": "arXiv admin note: substantial text overlap with arXiv:2505.09382",
      "pdf_url": "https://arxiv.org/pdf/2505.09661v2",
      "published_date": "2025-05-14 13:46:46 UTC",
      "updated_date": "2025-06-22 11:25:43 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T08:41:31.252821+00:00"
    },
    {
      "arxiv_id": "2505.09385v1",
      "title": "FedSaaS: Class-Consistency Federated Semantic Segmentation via Global Prototype Supervision and Local Adversarial Harmonization",
      "title_zh": "FedSaaSï¼šåŸºäºå…¨å±€åŸå‹ç›‘ç£ä¸æœ¬åœ°å¯¹æŠ—åè°ƒçš„ç±»åˆ«ä¸€è‡´æ€§è”é‚¦è¯­ä¹‰åˆ†å‰²",
      "authors": [
        "Xiaoyang Yu",
        "Xiaoming Wu",
        "Xin Wang",
        "Dongrun Li",
        "Ming Yang",
        "Peng Cheng"
      ],
      "abstract": "Federated semantic segmentation enables pixel-level classification in images through collaborative learning while maintaining data privacy. However, existing research commonly overlooks the fine-grained class relationships within the semantic space when addressing heterogeneous problems, particularly domain shift. This oversight results in ambiguities between class representation. To overcome this challenge, we propose a novel federated segmentation framework that strikes class consistency, termed FedSaaS. Specifically, we introduce class exemplars as a criterion for both local- and global-level class representations. On the server side, the uploaded class exemplars are leveraged to model class prototypes, which supervise global branch of clients, ensuring alignment with global-level representation. On the client side, we incorporate an adversarial mechanism to harmonize contributions of global and local branches, leading to consistent output. Moreover, multilevel contrastive losses are employed on both sides to enforce consistency between two-level representations in the same semantic space. Extensive experiments on several driving scene segmentation datasets demonstrate that our framework outperforms state-of-the-art methods, significantly improving average segmentation accuracy and effectively addressing the class-consistency representation problem.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹è”é‚¦è¯­ä¹‰åˆ†å‰²åœ¨å¤„ç†åŸŸåç§»(domain shift)ç­‰å¼‚æ„é—®é¢˜æ—¶å­˜åœ¨çš„ç±»åˆ«è¡¨ç¤ºæ¨¡ç³ŠæŒ‘æˆ˜ï¼Œæå‡ºäº†åä¸ºFedSaaSçš„ç±»åˆ«ä¸€è‡´æ€§(class consistency)è”é‚¦åˆ†å‰²æ¡†æ¶ã€‚åœ¨æœåŠ¡ç«¯ï¼Œè¯¥æ¡†æ¶åˆ©ç”¨ä¸Šä¼ çš„ç±»åˆ«æ ·æœ¬(class exemplars)å»ºæ¨¡ç±»åˆ«åŸå‹(class prototypes)ï¼Œå¯¹å®¢æˆ·ç«¯çš„å…¨å±€åˆ†æ”¯è¿›è¡Œç›‘ç£ï¼Œä»¥ç¡®ä¿å…¶ä¸å…¨å±€è¡¨ç¤ºå¯¹é½ã€‚åœ¨å®¢æˆ·ç«¯ï¼Œç ”ç©¶å¼•å…¥äº†å¯¹æŠ—æœºåˆ¶(adversarial mechanism)æ¥åè°ƒå…¨å±€ä¸å±€éƒ¨åˆ†æ”¯çš„è´¡çŒ®ï¼Œä»è€Œå®ç°ä¸€è‡´çš„è¾“å‡ºã€‚æ­¤å¤–ï¼Œè¯¥æ¡†æ¶åœ¨ä¸¤ç«¯å‡é‡‡ç”¨å¤šçº§å¯¹æ¯”æŸå¤±(multilevel contrastive losses)ï¼Œä»¥å¼ºåˆ¶åœ¨åŒä¸€è¯­ä¹‰ç©ºé—´å†…ç»´æŒä¸¤çº§è¡¨ç¤ºçš„ä¸€è‡´æ€§ã€‚åœ¨å¤šä¸ªé©¾é©¶åœºæ™¯åˆ†å‰²æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒFedSaaSåœ¨å¹³å‡åˆ†å‰²ç²¾åº¦ä¸Šæ˜¾è‘—ä¼˜äºç°æœ‰æœ€å…ˆè¿›æ–¹æ³•ï¼Œæœ‰æ•ˆè§£å†³äº†ç±»åˆ«ä¸€è‡´æ€§è¡¨ç¤ºé—®é¢˜ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.09385v1",
      "published_date": "2025-05-14 13:38:30 UTC",
      "updated_date": "2025-05-14 13:38:30 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T08:41:21.549342+00:00"
    },
    {
      "arxiv_id": "2505.09382v2",
      "title": "The Voice Timbre Attribute Detection 2025 Challenge Evaluation Plan",
      "title_zh": "2025å¹´éŸ³è‰²å±æ€§æ£€æµ‹æŒ‘æˆ˜èµ›è¯„æµ‹æ–¹æ¡ˆ",
      "authors": [
        "Zhengyan Sheng",
        "Jinghao He",
        "Liping Chen",
        "Kong Aik Lee",
        "Zhen-Hua Ling"
      ],
      "abstract": "Voice timbre refers to the unique quality or character of a person's voice that distinguishes it from others as perceived by human hearing. The Voice Timbre Attribute Detection (VtaD) 2025 challenge focuses on explaining the voice timbre attribute in a comparative manner. In this challenge, the human impression of voice timbre is verbalized with a set of sensory descriptors, including bright, coarse, soft, magnetic, and so on. The timbre is explained from the comparison between two voices in their intensity within a specific descriptor dimension. The VtaD 2025 challenge starts in May and culminates in a special proposal at the NCMMSC2025 conference in October 2025 in Zhenjiang, China.",
      "tldr_zh": "è¯¥è®¡åˆ’ä»‹ç»äº†2025å¹´éŸ³è‰²å±æ€§æ£€æµ‹(Voice Timbre Attribute Detection, VtaD)æŒ‘æˆ˜èµ›ï¼Œæ—¨åœ¨é€šè¿‡æ¯”è¾ƒçš„æ–¹å¼è§£é‡Šäººç±»å¬è§‰æ„ŸçŸ¥ä¸­çš„éŸ³è‰²(Voice timbre)å±æ€§ã€‚è¯¥æŒ‘æˆ˜èµ›å°†äººç±»å¯¹éŸ³è‰²çš„ä¸»è§‚å°è±¡è½¬åŒ–ä¸ºä¸€ç³»åˆ—æ„Ÿå®˜æè¿°ç¬¦(sensory descriptors)ï¼ŒåŒ…æ‹¬æ˜äº®(bright)ã€ç²—ç³™(coarse)ã€æŸ”å’Œ(soft)åŠç£æ€§(magnetic)ç­‰ã€‚éŸ³è‰²çš„è§£é‡Šè¿‡ç¨‹åŸºäºä¸¤ä¸ªå£°éŸ³åœ¨ç‰¹å®šæè¿°ç¬¦ç»´åº¦ä¸Šçš„å¼ºåº¦å¯¹æ¯”ï¼Œä»è€Œå®ç°å¯¹å£°éŸ³ç‰¹æ€§çš„ç»†è‡´åˆ»ç”»ã€‚VtaD 2025æŒ‘æˆ˜èµ›è®¡åˆ’äº2025å¹´5æœˆå¯åŠ¨ï¼Œå¹¶å°†äº10æœˆåœ¨ä¸­å›½é•‡æ±Ÿä¸¾è¡Œçš„ç¬¬åä¹å±Šå…¨å›½äººæœºè¯­éŸ³é€šè®¯å­¦æœ¯ä¼šè®®(NCMMSC2025)ä¸Šå±•ç¤ºç›¸å…³æˆæœã€‚è¯¥æŒ‘æˆ˜èµ›ä¸ºè¯­éŸ³æŠ€æœ¯é¢†åŸŸæ¢ç´¢äººç±»å¬è§‰æ„ŸçŸ¥ä¸è¯­ä¹‰æè¿°ä¹‹é—´çš„å…³è”æä¾›äº†é‡è¦çš„åŸºå‡†å’Œç ”ç©¶æ¡†æ¶ã€‚",
      "categories": [
        "cs.SD",
        "cs.AI",
        "eess.AS"
      ],
      "primary_category": "cs.SD",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.09382v2",
      "published_date": "2025-05-14 13:35:53 UTC",
      "updated_date": "2025-06-22 11:15:34 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T08:41:31.926663+00:00"
    },
    {
      "arxiv_id": "2505.09380v2",
      "title": "Examining Deployment and Refinement of the VIOLA-AI Intracranial Hemorrhage Model Using an Interactive NeoMedSys Platform",
      "title_zh": "åŸºäºäº¤äº’å¼ NeoMedSys å¹³å°çš„ VIOLA-AI é¢…å†…å‡ºè¡€æ¨¡å‹éƒ¨ç½²ä¸ä¼˜åŒ–ç ”ç©¶",
      "authors": [
        "Qinghui Liu",
        "Jon E. Nesvold",
        "Hanna Raaum",
        "Elakkyen Murugesu",
        "Martin RÃ¸vang",
        "Bradley J Maclntosh",
        "Atle BjÃ¸rnerud",
        "Karoline Skogen"
      ],
      "abstract": "Background: There are many challenges and opportunities in the clinical deployment of AI tools in radiology. The current study describes a radiology software platform called NeoMedSys that can enable efficient deployment and refinements of AI models. We evaluated the feasibility and effectiveness of running NeoMedSys for three months in real-world clinical settings and focused on improvement performance of an in-house developed AI model (VIOLA-AI) designed for intracranial hemorrhage (ICH) detection.\n  Methods: NeoMedSys integrates tools for deploying, testing, and optimizing AI models with a web-based medical image viewer, annotation system, and hospital-wide radiology information systems. A prospective pragmatic investigation was deployed using clinical cases of patients presenting to the largest Emergency Department in Norway (site-1) with suspected traumatic brain injury (TBI) or patients with suspected stroke (site-2). We assessed ICH classification performance as VIOLA-AI encountered new data and underwent pre-planned model retraining. Performance metrics included sensitivity, specificity, accuracy, and the area under the receiver operating characteristic curve (AUC).\n  Results: NeoMedSys facilitated iterative improvements in the AI model, significantly enhancing its diagnostic accuracy. Automated bleed detection and segmentation were reviewed in near real-time to facilitate re-training VIOLA-AI. The iterative refinement process yielded a marked improvement in classification sensitivity, rising to 90.3% (from 79.2%), and specificity that reached 89.3% (from 80.7%). The bleed detection ROC analysis for the entire sample demonstrated a high area-under-the-curve (AUC) of 0.949 (from 0.873). Model refinement stages were associated with notable gains, highlighting the value of real-time radiologist feedback.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†æ”¾å°„ç§‘ AI å·¥å…·åœ¨ä¸´åºŠéƒ¨ç½²ä¸­çš„æŒ‘æˆ˜ï¼Œå¹¶ä»‹ç»äº†ä¸€ä¸ªåä¸º NeoMedSys çš„äº¤äº’å¼è½¯ä»¶å¹³å°ï¼Œæ—¨åœ¨å®ç° AI æ¨¡å‹çš„æœ‰æ•ˆéƒ¨ç½²ä¸æŒç»­ä¼˜åŒ–ã€‚ç ”ç©¶äººå‘˜åœ¨æŒªå¨çš„æ€¥è¯Šå®¤å’Œä¸­é£ä¸­å¿ƒè¿›è¡Œäº†ä¸ºæœŸä¸‰ä¸ªæœˆçš„å‰ç»æ€§å®åœ°è¯„ä¼°ï¼Œé‡ç‚¹é’ˆå¯¹ç”¨äºé¢…å†…å‡ºè¡€ (Intracranial Hemorrhage, ICH) æ£€æµ‹çš„ VIOLA-AI æ¨¡å‹è¿›è¡Œæ€§èƒ½æ”¹è¿›ã€‚NeoMedSys å¹³å°é›†æˆäº†å›¾åƒæŸ¥çœ‹å™¨ã€æ ‡æ³¨ç³»ç»Ÿå’ŒåŒ»é™¢æ”¾å°„ä¿¡æ¯ç³»ç»Ÿï¼Œæ”¯æŒåœ¨ä¸´åºŠæ¡ˆä¾‹å¤„ç†è¿‡ç¨‹ä¸­å¯¹æ¨¡å‹è¿›è¡Œè¿‘å®æ—¶çš„é‡è®­ç»ƒã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œé€šè¿‡æ”¾å°„ç§‘åŒ»ç”Ÿçš„åé¦ˆè¿›è¡Œè¿­ä»£æ”¹è¿›åï¼ŒVIOLA-AI çš„æ•æ„Ÿæ€§ (Sensitivity) ä» 79.2% æå‡è‡³ 90.3%ï¼Œç‰¹å¼‚æ€§ (Specificity) ä» 80.7% æå‡è‡³ 89.3%ï¼Œå—è¯•è€…å·¥ä½œç‰¹å¾æ›²çº¿ä¸‹é¢ç§¯ (AUC) ä» 0.873 æ˜¾è‘—æé«˜åˆ° 0.949ã€‚è¯¥ç ”ç©¶è¯æ˜äº†åˆ©ç”¨ NeoMedSys å¹³å°è¿›è¡Œæ¨¡å‹ç²¾ç»†åŒ–éƒ¨ç½²çš„å¯è¡Œæ€§ï¼Œå¼ºè°ƒäº†å®æ—¶ä¸´åºŠåé¦ˆåœ¨æå‡åŒ»ç–— AI æ¨¡å‹è¯Šæ–­å‡†ç¡®æ€§æ–¹é¢çš„æ ¸å¿ƒä»·å€¼ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "21 pages, 11 figures, on submission to BMC Methods",
      "pdf_url": "https://arxiv.org/pdf/2505.09380v2",
      "published_date": "2025-05-14 13:33:38 UTC",
      "updated_date": "2025-09-19 10:50:53 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T08:41:47.054116+00:00"
    },
    {
      "arxiv_id": "2505.09371v2",
      "title": "TensorRL-QAS: Reinforcement learning with tensor networks for improved quantum architecture search",
      "title_zh": "TensorRL-QASï¼šåŸºäºå¼ é‡ç½‘ç»œä¸å¼ºåŒ–å­¦ä¹ çš„æ”¹è¿›å‹é‡å­æ¶æ„æœç´¢",
      "authors": [
        "Akash Kundu",
        "Stefano Mangini"
      ],
      "abstract": "Variational quantum algorithms hold the promise to address meaningful quantum problems already on noisy intermediate-scale quantum hardware. In spite of the promise, they face the challenge of designing quantum circuits that both solve the target problem and comply with device limitations. Quantum architecture search (QAS) automates the design process of quantum circuits, with reinforcement learning (RL) emerging as a promising approach. Yet, RL-based QAS methods encounter significant scalability issues, as computational and training costs grow rapidly with the number of qubits, circuit depth, and hardware noise. To address these challenges, we introduce $\\textit{TensorRL-QAS}$, an improved framework that combines tensor network methods with RL for QAS. By warm-starting the QAS with a matrix product state approximation of the target solution, TensorRL-QAS effectively narrows the search space to physically meaningful circuits and accelerates the convergence to the desired solution. Tested on several quantum chemistry problems of up to 12-qubit, TensorRL-QAS achieves up to a 10-fold reduction in CNOT count and circuit depth compared to baseline methods, while maintaining or surpassing chemical accuracy. It reduces classical optimizer function evaluation by up to 100-fold, accelerates training episodes by up to 98$\\%$, and can achieve 50$\\%$ success probability for 10-qubit systems, far exceeding the $<$1$\\%$ rates of baseline. Robustness and versatility are demonstrated both in the noiseless and noisy scenarios, where we report a simulation of an 8-qubit system. Furthermore, TensorRL-QAS demonstrates effectiveness on systems on 20-qubit quantum systems, positioning it as a state-of-the-art quantum circuit discovery framework for near-term hardware and beyond.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†TensorRL-QASï¼Œè¿™æ˜¯ä¸€ç§å°†å¼ é‡ç½‘ç»œ(Tensor Network)æ–¹æ³•ä¸å¼ºåŒ–å­¦ä¹ (Reinforcement Learning)ç›¸ç»“åˆçš„æ”¹è¿›é‡å­æ¶æ„æœç´¢(QAS)æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³ç°æœ‰æ–¹æ³•åœ¨å¤„ç†å¤§è§„æ¨¡é‡å­ä½å’Œç¡¬ä»¶å™ªå£°æ—¶é¢ä¸´çš„æ‰©å±•æ€§æŒ‘æˆ˜ã€‚è¯¥æ¡†æ¶é€šè¿‡ä½¿ç”¨ç›®æ ‡è§£çš„çŸ©é˜µä¹˜ç§¯æ€(Matrix Product State)è¿‘ä¼¼è¿›è¡Œé¢„çƒ­(Warm-start)ï¼Œæœ‰æ•ˆåœ°ç¼©å°äº†æœç´¢ç©ºé—´å¹¶åŠ é€Ÿäº†å‘ç†æƒ³ç”µè·¯æ–¹æ¡ˆçš„æ”¶æ•›ã€‚åœ¨é«˜è¾¾12ä¸ªé‡å­ä½çš„é‡å­åŒ–å­¦é—®é¢˜å®éªŒä¸­ï¼ŒTensorRL-QASåœ¨ä¿æŒåŒ–å­¦ç²¾åº¦(Chemical Accuracy)çš„å‰æä¸‹ï¼Œå®ç°äº†CNOTé—¨æ•°é‡å’Œç”µè·¯æ·±åº¦æœ€é«˜10å€çš„ç¼©å‡ã€‚åŒæ—¶ï¼Œè¯¥æ–¹æ³•å°†ç»å…¸ä¼˜åŒ–å™¨çš„è¯„ä¼°å¼€é”€é™ä½äº†100å€ï¼Œè®­ç»ƒé€Ÿåº¦æå‡äº†98%ï¼Œå¹¶åœ¨10é‡å­ä½ç³»ç»Ÿä¸­å–å¾—äº†50%çš„æˆåŠŸç‡ï¼Œè¿œé«˜äºåŸºå‡†æ¨¡å‹ä¸è¶³1%çš„è¡¨ç°ã€‚è¯¥ç ”ç©¶è¿˜é€šè¿‡å¯¹20é‡å­ä½ç³»ç»Ÿçš„æœ‰æ•ˆåº”ç”¨åŠå«å™ªå£°åœºæ™¯çš„æ¨¡æ‹Ÿï¼Œè¯æ˜äº†TensorRL-QASä½œä¸ºè¿‘æœŸé‡å­ç¡¬ä»¶(Near-term hardware)ç”µè·¯å‘ç°æ¡†æ¶çš„å…ˆè¿›æ€§ä¸é²æ£’æ€§ã€‚",
      "categories": [
        "quant-ph",
        "cs.AI",
        "cs.ET",
        "cs.LG"
      ],
      "primary_category": "quant-ph",
      "comment": "Accepted at NeurIPS 2025. Code is at: https://github.com/Aqasch/TensorRL-QAS",
      "pdf_url": "https://arxiv.org/pdf/2505.09371v2",
      "published_date": "2025-05-14 13:23:34 UTC",
      "updated_date": "2025-09-30 14:11:41 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T08:42:20.807193+00:00"
    },
    {
      "arxiv_id": "2505.13491v1",
      "title": "ProdRev: A DNN framework for empowering customers using generative pre-trained transformers",
      "title_zh": "ProdRevï¼šåˆ©ç”¨ç”Ÿæˆå¼é¢„è®­ç»ƒå˜æ¢å™¨èµ‹èƒ½æ¶ˆè´¹è€…çš„æ·±åº¦ç¥ç»ç½‘ç»œæ¡†æ¶",
      "authors": [
        "Aakash Gupta",
        "Nataraj Das"
      ],
      "abstract": "Following the pandemic, customers, preference for using e-commerce has accelerated. Since much information is available in multiple reviews (sometimes running in thousands) for a single product, it can create decision paralysis for the buyer. This scenario disempowers the consumer, who cannot be expected to go over so many reviews since its time consuming and can confuse them. Various commercial tools are available, that use a scoring mechanism to arrive at an adjusted score. It can alert the user to potential review manipulations. This paper proposes a framework that fine-tunes a generative pre-trained transformer to understand these reviews better. Furthermore, using \"common-sense\" to make better decisions. These models have more than 13 billion parameters. To fine-tune the model for our requirement, we use the curie engine from generative pre-trained transformer (GPT3). By using generative models, we are introducing abstractive summarization. Instead of using a simple extractive method of summarizing the reviews. This brings out the true relationship between the reviews and not simply copy-paste. This introduces an element of \"common sense\" for the user and helps them to quickly make the right decisions. The user is provided the pros and cons of the processed reviews. Thus the user/customer can take their own decisions.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† ProdRevï¼Œä¸€ä¸ªåŸºäºæ·±åº¦ç¥ç»ç½‘ç»œ(DNN)å’Œç”Ÿæˆå¼é¢„è®­ç»ƒå˜æ¢å™¨(Generative Pre-trained Transformers)çš„æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³ç”µå­å•†åŠ¡ä¸­æµ·é‡è¯„è®ºå¯¼è‡´çš„æ¶ˆè´¹è€…å†³ç­–ç˜«ç—ªé—®é¢˜ã€‚è¯¥æ¡†æ¶åˆ©ç”¨å…·æœ‰è¶…è¿‡130äº¿å‚æ•°çš„ GPT3 Curie å¼•æ“ï¼Œé€šè¿‡å¾®è°ƒ(Fine-tuning)æŠ€æœ¯å®ç°äº†å¯¹è¯„è®ºçš„æ·±åº¦ç†è§£ã€‚ä¸åŒäºä¼ ç»Ÿçš„æå–å¼æ‘˜è¦(Extractive Summarization)ï¼ŒProdRev é‡‡ç”¨äº†æŠ½è±¡å¼æ‘˜è¦(Abstractive Summarization)æ–¹æ³•ï¼Œèƒ½å¤Ÿæ•æ‰è¯„è®ºé—´çš„çœŸå®é€»è¾‘å…³ç³»å¹¶å¼•å…¥â€œå¸¸è¯†â€æ¨ç†ã€‚è¯¥ç³»ç»Ÿé€šè¿‡ä¸ºç”¨æˆ·æä¾›ç»è¿‡å¤„ç†çš„ä¼˜ç¼ºç‚¹(Pros and Cons)åˆ†æï¼Œä½¿å…¶èƒ½å¤Ÿä»æˆåƒä¸Šä¸‡æ¡è¯„è®ºä¸­å¿«é€Ÿæå–å…³é”®ä¿¡æ¯ã€‚è¿™ç§æ–¹æ³•é€šè¿‡ç”Ÿæˆå¼æ¨¡å‹æå‡äº†æ‘˜è¦çš„è´¨é‡ï¼Œä¸ä»…é¿å…äº†ç®€å•çš„å¤åˆ¶ç²˜è´´ï¼Œæ›´é€šè¿‡æ™ºèƒ½åŒ–æ‰‹æ®µæœ‰æ•ˆèµ‹èƒ½æ¶ˆè´¹è€…ï¼Œå¸®åŠ©å…¶åœ¨å¤æ‚çš„ä¿¡æ¯ç¯å¢ƒä¸‹åšå‡ºæ›´æ˜æ™ºçš„è´­ä¹°å†³ç­–ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "2022 International Conference on Decision Aid Sciences and Applications (DASA)",
      "pdf_url": "https://arxiv.org/pdf/2505.13491v1",
      "published_date": "2025-05-14 13:07:48 UTC",
      "updated_date": "2025-05-14 13:07:48 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T08:42:16.155801+00:00"
    },
    {
      "arxiv_id": "2505.09344v1",
      "title": "GreenFactory: Ensembling Zero-Cost Proxies to Estimate Performance of Neural Networks",
      "title_zh": "GreenFactoryï¼šé›†æˆé›¶æˆæœ¬ä»£ç†ä»¥ä¼°è®¡ç¥ç»ç½‘ç»œæ€§èƒ½",
      "authors": [
        "Gabriel CortÃªs",
        "Nuno LourenÃ§o",
        "Paolo Romano",
        "Penousal Machado"
      ],
      "abstract": "Determining the performance of a Deep Neural Network during Neural Architecture Search processes is essential for identifying optimal architectures and hyperparameters. Traditionally, this process requires training and evaluation of each network, which is time-consuming and resource-intensive. Zero-cost proxies estimate performance without training, serving as an alternative to traditional training. However, recent proxies often lack generalization across diverse scenarios and provide only relative rankings rather than predicted accuracies. To address these limitations, we propose GreenFactory, an ensemble of zero-cost proxies that leverages a random forest regressor to combine multiple predictors' strengths and directly predict model test accuracy. We evaluate GreenFactory on NATS-Bench, achieving robust results across multiple datasets. Specifically, GreenFactory achieves high Kendall correlations on NATS-Bench-SSS, indicating substantial agreement between its predicted scores and actual performance: 0.907 for CIFAR-10, 0.945 for CIFAR-100, and 0.920 for ImageNet-16-120. Similarly, on NATS-Bench-TSS, we achieve correlations of 0.921 for CIFAR-10, 0.929 for CIFAR-100, and 0.908 for ImageNet-16-120, showcasing its reliability in both search spaces.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ç¥ç»æ¶æ„æœç´¢(Neural Architecture Search)ä¸­æ¨¡å‹æ€§èƒ½è¯„ä¼°è€—æ—¶ä¸”èµ„æºå¯†é›†çš„é—®é¢˜ï¼Œæå‡ºäº†GreenFactoryæ¡†æ¶ï¼Œæ—¨åœ¨é€šè¿‡é›†æˆé›¶æˆæœ¬ä»£ç†(Zero-cost proxies)æ¥é«˜æ•ˆä¼°ç®—ç½‘ç»œè¡¨ç°ã€‚GreenFactoryåˆ©ç”¨éšæœºæ£®æ—å›å½’å™¨(Random Forest Regressor)ç»“åˆå¤šä¸ªé¢„æµ‹å™¨çš„ä¼˜åŠ¿ï¼Œèƒ½å¤Ÿç›´æ¥é¢„æµ‹æ¨¡å‹çš„æµ‹è¯•å‡†ç¡®ç‡ï¼Œè§£å†³äº†ç°æœ‰ä»£ç†ç¼ºä¹æ³›åŒ–æ€§åŠä»…èƒ½æä¾›ç›¸å¯¹æ’åçš„é—®é¢˜ã€‚ç ”ç©¶å›¢é˜Ÿåœ¨NATS-Benchçš„SSSå’ŒTSSæœç´¢ç©ºé—´ä¸Šå¯¹è¯¥æ–¹æ³•è¿›è¡Œäº†å¹¿æ³›éªŒè¯ï¼Œæ¶µç›–äº†CIFAR-10ã€CIFAR-100å’ŒImageNet-16-120ç­‰å¤šä¸ªæ•°æ®é›†ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒGreenFactoryå®ç°äº†æé«˜çš„è‚¯å¾·å°”ç›¸å…³ç³»æ•°(Kendall correlations)ï¼Œåœ¨NATS-Bench-SSSä¸Šæœ€é«˜è¾¾åˆ°0.945ï¼Œå……åˆ†è¯æ˜äº†å…¶åœ¨ä¸åŒæœç´¢ç©ºé—´å’Œæ•°æ®é›†ä¸‹çš„å¯é æ€§ä¸é²æ£’æ€§ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.09344v1",
      "published_date": "2025-05-14 12:40:34 UTC",
      "updated_date": "2025-05-14 12:40:34 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T08:43:30.114636+00:00"
    },
    {
      "arxiv_id": "2505.09343v2",
      "title": "Insights into DeepSeek-V3: Scaling Challenges and Reflections on Hardware for AI Architectures",
      "title_zh": "æ·±å…¥å‰–æ DeepSeek-V3ï¼šè§„æ¨¡åŒ–æŒ‘æˆ˜ä¸å¯¹ AI æ¶æ„ç¡¬ä»¶çš„æ€è€ƒ",
      "authors": [
        "Chenggang Zhao",
        "Chengqi Deng",
        "Chong Ruan",
        "Damai Dai",
        "Huazuo Gao",
        "Jiashi Li",
        "Liyue Zhang",
        "Panpan Huang",
        "Shangyan Zhou",
        "Shirong Ma",
        "Wenfeng Liang",
        "Ying He",
        "Yuqing Wang",
        "Yuxuan Liu",
        "Y. X. Wei"
      ],
      "abstract": "The rapid scaling of large language models (LLMs) has unveiled critical limitations in current hardware architectures, including constraints in memory capacity, computational efficiency, and interconnection bandwidth. DeepSeek-V3, trained on 2,048 NVIDIA H800 GPUs, demonstrates how hardware-aware model co-design can effectively address these challenges, enabling cost-efficient training and inference at scale. This paper presents an in-depth analysis of the DeepSeek-V3/R1 model architecture and its AI infrastructure, highlighting key innovations such as Multi-head Latent Attention (MLA) for enhanced memory efficiency, Mixture of Experts (MoE) architectures for optimized computation-communication trade-offs, FP8 mixed-precision training to unlock the full potential of hardware capabilities, and a Multi-Plane Network Topology to minimize cluster-level network overhead. Building on the hardware bottlenecks encountered during DeepSeek-V3's development, we engage in a broader discussion with academic and industry peers on potential future hardware directions, including precise low-precision computation units, scale-up and scale-out convergence, and innovations in low-latency communication fabrics. These insights underscore the critical role of hardware and model co-design in meeting the escalating demands of AI workloads, offering a practical blueprint for innovation in next-generation AI systems.",
      "tldr_zh": "è¯¥ç ”ç©¶æ·±å…¥åˆ†æäº† DeepSeek-V3/R1 çš„æ¨¡å‹æ¶æ„åŠå…¶ AI åŸºç¡€è®¾æ–½ï¼Œæ¢è®¨äº†å¦‚ä½•é€šè¿‡ç¡¬ä»¶æ„ŸçŸ¥çš„æ¨¡å‹ååŒè®¾è®¡(Hardware-aware model co-design)æ¥åº”å¯¹å¤§è¯­è¨€æ¨¡å‹æ‰©å±•ä¸­çš„å†…å­˜å®¹é‡ã€è®¡ç®—æ•ˆç‡å’Œäº’è¿å¸¦å®½ç­‰æ ¸å¿ƒæŒ‘æˆ˜ã€‚è®ºæ–‡è¯¦ç»†ä»‹ç»äº†å…³é”®æŠ€æœ¯åˆ›æ–°ï¼ŒåŒ…æ‹¬ç”¨äºå¢å¼ºå†…å­˜æ•ˆç‡çš„å¤šå¤´æ½œåœ¨æ³¨æ„åŠ›æœºåˆ¶(Multi-head Latent Attention, MLA)ã€ä¼˜åŒ–è®¡ç®—ä¸é€šä¿¡æƒè¡¡çš„æ··åˆä¸“å®¶æ¨¡å‹(Mixture of Experts, MoE)æ¶æ„ã€æŒ–æ˜ç¡¬ä»¶æ½œèƒ½çš„ FP8 æ··åˆç²¾åº¦è®­ç»ƒä»¥åŠé™ä½é›†ç¾¤ç½‘ç»œå¼€é”€çš„å¤šå¹³é¢ç½‘ç»œæ‹“æ‰‘(Multi-Plane Network Topology)ã€‚é€šè¿‡åœ¨ 2,048 å— NVIDIA H800 GPU ä¸Šçš„æˆåŠŸå®è·µï¼ŒDeepSeek-V3 è¯æ˜äº†å®ç°é«˜æˆæœ¬æ•ˆç›Šè§„æ¨¡åŒ–è®­ç»ƒä¸æ¨ç†çš„å¯è¡Œæ€§ã€‚åŸºäºå¼€å‘è¿‡ç¨‹ä¸­é‡åˆ°çš„ç¡¬ä»¶ç“¶é¢ˆï¼Œç ”ç©¶å›¢é˜Ÿè¿›ä¸€æ­¥è®¨è®ºäº†ç²¾å‡†ä½ç²¾åº¦è®¡ç®—å•å…ƒã€æ‰©å±•æ€§æ¶æ„èåˆä»¥åŠä½å»¶è¿Ÿé€šä¿¡ç½‘ç»œç­‰æœªæ¥ç¡¬ä»¶æ¼”è¿›æ–¹å‘ã€‚è¿™äº›æŠ€æœ¯æ´å¯Ÿå¼ºè°ƒäº†è½¯ç¡¬ä»¶ååŒè®¾è®¡åœ¨åº”å¯¹æ—¥ç›Šå¢é•¿çš„ AI å·¥ä½œè´Ÿè½½éœ€æ±‚ä¸­çš„å…³é”®ä½œç”¨ï¼Œä¸ºæ„å»ºä¸‹ä¸€ä»£é«˜æ€§èƒ½ AI ç³»ç»Ÿæä¾›äº†é‡è¦çš„å®è·µè“å›¾ã€‚",
      "categories": [
        "cs.DC",
        "cs.AI",
        "cs.AR"
      ],
      "primary_category": "cs.DC",
      "comment": "This is the author's version of the work. It is posted here for your personal use. Not for redistribution. The definitive version appeared as part of the Industry Track in Proceedings of the 52nd Annual International Symposium on Computer Architecture (ISCA '25)",
      "pdf_url": "https://arxiv.org/pdf/2505.09343v2",
      "published_date": "2025-05-14 12:39:03 UTC",
      "updated_date": "2025-12-23 03:00:00 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T08:42:27.534659+00:00"
    },
    {
      "arxiv_id": "2505.09342v2",
      "title": "Evaluating the robustness of adversarial defenses in malware detection systems",
      "title_zh": "æ¶æ„è½¯ä»¶æ£€æµ‹ç³»ç»Ÿä¸­å¯¹æŠ—é˜²å¾¡é²æ£’æ€§çš„è¯„ä¼°",
      "authors": [
        "Mostafa Jafari",
        "Alireza Shameli-Sendi"
      ],
      "abstract": "Machine learning is a key tool for Android malware detection, effectively identifying malicious patterns in apps. However, ML-based detectors are vulnerable to evasion attacks, where small, crafted changes bypass detection. Despite progress in adversarial defenses, the lack of comprehensive evaluation frameworks in binary-constrained domains limits understanding of their robustness. We introduce two key contributions. First, Prioritized Binary Rounding, a technique to convert continuous perturbations into binary feature spaces while preserving high attack success and low perturbation size. Second, the sigma-binary attack, a novel adversarial method for binary domains, designed to achieve attack goals with minimal feature changes. Experiments on the Malscan dataset show that sigma-binary outperforms existing attacks and exposes key vulnerabilities in state-of-the-art defenses. Defenses equipped with adversary detectors, such as KDE, DLA, DNN+, and ICNN, exhibit significant brittleness, with attack success rates exceeding 90% using fewer than 10 feature modifications and reaching 100% with just 20. Adversarially trained defenses, including AT-rFGSM-k, AT-MaxMA, improves robustness under small budgets but remains vulnerable to unrestricted perturbations, with attack success rates of 99.45% and 96.62%, respectively. Although PAD-SMA demonstrates strong robustness against state-of-the-art gradient-based adversarial attacks by maintaining an attack success rate below 16.55%, the sigma-binary attack significantly outperforms these methods, achieving a 94.56% success rate under unrestricted perturbations. These findings highlight the critical need for precise method like sigma-binary to expose hidden vulnerabilities in existing defenses and support the development of more resilient malware detection systems.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹æœºå™¨å­¦ä¹ åœ¨ Android æ¶æ„è½¯ä»¶æ£€æµ‹ä¸­æ˜“å—å¯¹æŠ—æ€§é€ƒé€¸æ”»å‡»(Evasion Attacks)çš„é—®é¢˜ï¼Œåˆ†æäº†ç°æœ‰é˜²å¾¡æœºåˆ¶åœ¨äºŒè¿›åˆ¶çº¦æŸé¢†åŸŸç¼ºä¹å…¨é¢è¯„ä¼°çš„ç°çŠ¶ã€‚ä½œè€…æå‡ºäº†ä¸¤é¡¹æ ¸å¿ƒè´¡çŒ®ï¼šä¸€æ˜¯èƒ½å¤Ÿå°†è¿ç»­æ‰°åŠ¨è½¬åŒ–ä¸ºäºŒè¿›åˆ¶ç‰¹å¾å¹¶ä¿æŒé«˜æ•ˆèƒ½çš„ Prioritized Binary Rounding æŠ€æœ¯ï¼ŒäºŒæ˜¯ä¸“ä¸ºäºŒè¿›åˆ¶åŸŸè®¾è®¡çš„ sigma-binary å¯¹æŠ—æ”»å‡»æ–¹æ³•ã€‚åœ¨ Malscan æ•°æ®é›†ä¸Šçš„å®éªŒç»“æœæ˜¾ç¤ºï¼Œsigma-binary æ­ç¤ºäº†ç°æœ‰å…ˆè¿›é˜²å¾¡æŠ€æœ¯çš„ä¸¥é‡è„†å¼±æ€§ã€‚å…¶ä¸­ï¼Œé›†æˆ KDEã€DLA ç­‰å¯¹æŠ—æ£€æµ‹å™¨çš„é˜²å¾¡ç³»ç»Ÿåœ¨æå°‘ç‰¹å¾ä¿®æ”¹ä¸‹æ”»å‡»æˆåŠŸç‡ä¾¿è¶…è¿‡ 90%ï¼Œè€Œå¯¹æŠ—è®­ç»ƒ(Adversarial Training)æ–¹æ³•åœ¨é¢å¯¹ä¸å—é™æ‰°åŠ¨æ—¶ä¾ç„¶è¡¨ç°ä¹åŠ›ã€‚å³ä¾¿æ˜¯åœ¨é’ˆå¯¹æ¢¯åº¦æ”»å‡»å…·æœ‰å¼ºé²æ£’æ€§çš„ PAD-SMA æ¨¡å‹ä¸Šï¼Œsigma-binary ä»èƒ½å®ç° 94.56% çš„æ”»å‡»æˆåŠŸç‡ã€‚è¯¥ç ”ç©¶å¼ºè°ƒäº†åˆ©ç”¨ sigma-binary è¿›è¡Œç²¾ç¡®è¯„ä¼°å¯¹äºæš´éœ²é˜²å¾¡æ¼æ´åŠå¼€å‘æ›´å…·å¼¹æ€§çš„æ¶æ„è½¯ä»¶æ£€æµ‹ç³»ç»Ÿè‡³å…³é‡è¦ã€‚",
      "categories": [
        "cs.CR",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CR",
      "comment": "Published in Computers & Electrical Engineering (Elsevier), Volume 130, February 2026, Article 110845",
      "pdf_url": "https://arxiv.org/pdf/2505.09342v2",
      "published_date": "2025-05-14 12:38:43 UTC",
      "updated_date": "2025-12-08 15:56:17 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T08:42:31.461874+00:00"
    },
    {
      "arxiv_id": "2505.09341v4",
      "title": "Access Controls Will Solve the Dual-Use Dilemma",
      "title_zh": "è®¿é—®æ§åˆ¶å°†åŒ–è§£åŒé‡ç”¨é€”å›°å¢ƒ",
      "authors": [
        "EvÅ¾en Wybitul"
      ],
      "abstract": "AI safety systems face the dual-use dilemma. It is unclear whether to answer dual-use requests, since the same query could be either harmless or harmful depending on who made it and why. To make better decisions, such systems would need to examine requests' real-world context, but currently, they lack access to this information. Instead, they sometimes end up making arbitrary choices that result in refusing legitimate queries and allowing harmful ones, which hurts both utility and safety. To address this, we propose a conceptual framework based on access controls where only verified users can access dual-use outputs. We describe the framework's components, analyse its feasibility, and explain how it addresses both over-refusals and under-refusals. While only a high-level proposal, our work takes the first step toward giving model providers more granular tools for managing dual-use content. Such tools would enable users to access more capabilities without sacrificing safety, and offer regulators new options for targeted policies.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†AIå®‰å…¨ç³»ç»Ÿé¢ä¸´çš„åŒé‡ç”¨é€”å›°å¢ƒï¼ˆdual-use dilemmaï¼‰ï¼Œå³ç›¸åŒçš„æŸ¥è¯¢å¯èƒ½å› ç”¨æˆ·èº«ä»½å’Œç›®çš„ä¸åŒè€Œè¡¨ç°å‡ºæ— å®³æˆ–æœ‰å®³çš„æ€§è´¨ã€‚ç›®å‰çš„ç³»ç»Ÿç”±äºç¼ºä¹å¯¹è¯·æ±‚ç°å®èƒŒæ™¯çš„æ„ŸçŸ¥ï¼Œå¸¸åšå‡ºæ­¦æ–­å†³ç­–ï¼Œå¯¼è‡´è¯¯æ‹’åˆæ³•è¯·æ±‚ï¼ˆover-refusalsï¼‰æˆ–æ¼æ‰æœ‰å®³è¯·æ±‚ï¼ˆunder-refusalsï¼‰ï¼ŒæŸå®³äº†ç³»ç»Ÿçš„æ•ˆç”¨ä¸å®‰å…¨æ€§ã€‚ä¸ºæ­¤ï¼Œä½œè€…æå‡ºäº†ä¸€ä¸ªåŸºäºè®¿é—®æ§åˆ¶ï¼ˆaccess controlsï¼‰çš„æ¦‚å¿µæ¡†æ¶ï¼Œå»ºè®®ä»…å…è®¸ç»è¿‡èº«ä»½éªŒè¯çš„ç”¨æˆ·è®¿é—®å…·æœ‰åŒé‡ç”¨é€”çš„è¾“å‡ºå†…å®¹ã€‚è¯¥ç ”ç©¶è¯¦ç»†é˜è¿°äº†æ¡†æ¶çš„ç»„æˆéƒ¨åˆ†å¹¶åˆ†æäº†å…¶å¯è¡Œæ€§ï¼Œå±•ç¤ºäº†è¯¥æ–¹æ³•å¦‚ä½•é€šè¿‡æä¾›æ›´ç²¾ç»†çš„ç®¡ç†å·¥å…·æ¥åŒæ—¶è§£å†³è¯¯æ‹’å’Œæ¼æŠ¥é—®é¢˜ã€‚è¿™é¡¹å·¥ä½œä¸ºæ¨¡å‹æä¾›è€…å’Œç›‘ç®¡æœºæ„æä¾›äº†å®ç°åŠŸèƒ½å¼€æ”¾ä¸å®‰å…¨æ²»ç†å¹³è¡¡çš„æ–°è·¯å¾„ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "Accepted at ICML 2025 Workshop on Technical AI Governance (TAIG)",
      "pdf_url": "https://arxiv.org/pdf/2505.09341v4",
      "published_date": "2025-05-14 12:38:08 UTC",
      "updated_date": "2025-11-25 08:17:10 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T08:43:29.998674+00:00"
    },
    {
      "arxiv_id": "2505.09329v1",
      "title": "BioVFM-21M: Benchmarking and Scaling Self-Supervised Vision Foundation Models for Biomedical Image Analysis",
      "title_zh": "BioVFM-21Mï¼šé¢å‘ç”Ÿç‰©åŒ»å­¦å›¾åƒåˆ†æçš„è‡ªç›‘ç£è§†è§‰åŸºåº§æ¨¡å‹åŸºå‡†æµ‹è¯•ä¸è§„æ¨¡åŒ–ç ”ç©¶",
      "authors": [
        "Jiarun Liu",
        "Hong-Yu Zhou",
        "Weijian Huang",
        "Hao Yang",
        "Dongning Song",
        "Tao Tan",
        "Yong Liang",
        "Shanshan Wang"
      ],
      "abstract": "Scaling up model and data size have demonstrated impressive performance improvement over a wide range of tasks. Despite extensive studies on scaling behaviors for general-purpose tasks, medical images exhibit substantial differences from natural data. It remains unclear the key factors in developing medical vision foundation models at scale due to the absence of an extensive understanding of scaling behavior in the medical domain. In this paper, we explored the scaling behavior across model sizes, training algorithms, data sizes, and imaging modalities in developing scalable medical vision foundation models by self-supervised learning. To support scalable pretraining, we introduce BioVFM-21M, a large-scale biomedical image dataset encompassing a wide range of biomedical image modalities and anatomies. We observed that scaling up does provide benefits but varies across tasks. Additional analysis reveals several factors correlated with scaling benefits. Finally, we propose BioVFM, a large-scale medical vision foundation model pretrained on 21 million biomedical images, which outperforms the previous state-of-the-art foundation models across 12 medical benchmarks. Our results highlight that while scaling up is beneficial for pursuing better performance, task characteristics, data diversity, pretraining methods, and computational efficiency remain critical considerations for developing scalable medical foundation models.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†ç”Ÿç‰©åŒ»å­¦å½±åƒé¢†åŸŸè§†è§‰åŸºç¡€æ¨¡å‹çš„æ‰©å±•è§„å¾‹(Scaling Behavior)ï¼Œæ—¨åœ¨å¡«è¡¥è¯¥é¢†åŸŸåœ¨æ¨¡å‹è§„æ¨¡ã€æ•°æ®é‡åŠæˆåƒæ¨¡æ€æ‰©å±•æ–¹é¢è®¤çŸ¥ä¸è¶³çš„ç©ºç™½ã€‚ä¸ºæ­¤ï¼Œä½œè€…æ¨å‡ºäº†BioVFM-21Mï¼Œè¿™æ˜¯ä¸€ä¸ªåŒ…å«2100ä¸‡å¼ å›¾åƒçš„å¤§è§„æ¨¡ç”Ÿç‰©åŒ»å­¦å½±åƒæ•°æ®é›†ï¼Œæ¶µç›–äº†å¹¿æ³›çš„è§£å‰–ç»“æ„å’Œæˆåƒæ¨¡æ€ã€‚ç ”ç©¶å‘ç°ï¼Œè™½ç„¶æ‰©å¤§è§„æ¨¡(Scaling up)èƒ½æ˜¾è‘—æå‡æ€§èƒ½ï¼Œä½†å…¶æ”¶ç›Šåœ¨ä¸åŒä»»åŠ¡é—´å­˜åœ¨æ˜¾è‘—å·®å¼‚ã€‚åŸºäºæ­¤æ•°æ®é›†ï¼Œç ”ç©¶å›¢é˜Ÿå¼€å‘äº†BioVFMè§†è§‰åŸºç¡€æ¨¡å‹ï¼Œé€šè¿‡è‡ªç›‘ç£å­¦ä¹ (Self-supervised learning)è¿›è¡Œå¤§è§„æ¨¡é¢„è®­ç»ƒã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒBioVFMåœ¨12é¡¹åŒ»å­¦åŸºå‡†æµ‹è¯•ä¸­è¶…è¶Šäº†ç°æœ‰çš„æœ€å…ˆè¿›æ¨¡å‹(SOTA)ã€‚è¯¥å·¥ä½œå¼ºè°ƒäº†åœ¨è¿½æ±‚æ€§èƒ½æå‡æ—¶ï¼Œå¿…é¡»ç»¼åˆè€ƒè™‘ä»»åŠ¡ç‰¹æ€§ã€æ•°æ®å¤šæ ·æ€§ã€é¢„è®­ç»ƒæ–¹æ³•ä»¥åŠè®¡ç®—æ•ˆç‡ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "11 pages, 4 figures",
      "pdf_url": "https://arxiv.org/pdf/2505.09329v1",
      "published_date": "2025-05-14 12:25:41 UTC",
      "updated_date": "2025-05-14 12:25:41 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T08:42:40.649183+00:00"
    },
    {
      "arxiv_id": "2505.09324v1",
      "title": "Neural Video Compression using 2D Gaussian Splatting",
      "title_zh": "åŸºäº 2D é«˜æ–¯æ³¼æº…çš„ç¥ç»è§†é¢‘å‹ç¼©",
      "authors": [
        "Lakshya Gupta",
        "Imran N. Junejo"
      ],
      "abstract": "The computer vision and image processing research community has been involved in standardizing video data communications for the past many decades, leading to standards such as AVC, HEVC, VVC, AV1, AV2, etc. However, recent groundbreaking works have focused on employing deep learning-based techniques to replace the traditional video codec pipeline to a greater affect. Neural video codecs (NVC) create an end-to-end ML-based solution that does not rely on any handcrafted features (motion or edge-based) and have the ability to learn content-aware compression strategies, offering better adaptability and higher compression efficiency than traditional methods. This holds a great potential not only for hardware design, but also for various video streaming platforms and applications, especially video conferencing applications such as MS-Teams or Zoom that have found extensive usage in classrooms and workplaces. However, their high computational demands currently limit their use in real-time applications like video conferencing. To address this, we propose a region-of-interest (ROI) based neural video compression model that leverages 2D Gaussian Splatting. Unlike traditional codecs, 2D Gaussian Splatting is capable of real-time decoding and can be optimized using fewer data points, requiring only thousands of Gaussians for decent quality outputs as opposed to millions in 3D scenes. In this work, we designed a video pipeline that speeds up the encoding time of the previous Gaussian splatting-based image codec by 88% by using a content-aware initialization strategy paired with a novel Gaussian inter-frame redundancy-reduction mechanism, enabling Gaussian splatting to be used for a video-codec solution, the first of its kind solution in this neural video codec space.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§åŸºäº2D Gaussian Splattingçš„æ„Ÿå…´è¶£åŒºåŸŸ(ROI)ç¥ç»è§†é¢‘å‹ç¼©(Neural Video Compression)æ¨¡å‹ï¼Œæ—¨åœ¨è§£å†³ç¥ç»è§†é¢‘ç¼–è§£ç å™¨(NVC)å› é«˜è®¡ç®—éœ€æ±‚è€Œåœ¨è§†é¢‘ä¼šè®®ç­‰å®æ—¶åº”ç”¨ä¸­å—é™çš„é—®é¢˜ã€‚è¯¥æ–¹æ³•åˆ©ç”¨2D Gaussian Splattingæ”¯æŒå®æ—¶è§£ç ä¸”ä¼˜åŒ–æ‰€éœ€æ•°æ®ç‚¹è¾ƒå°‘çš„ç‰¹æ€§ï¼Œä»…éœ€æ•°åƒä¸ªé«˜æ–¯å‡½æ•°å³å¯å®ç°é«˜è´¨é‡è¾“å‡ºï¼Œæ˜¾è‘—é™ä½äº†è®¡ç®—è´Ÿæ‹…ã€‚é€šè¿‡å¼•å…¥å†…å®¹æ„ŸçŸ¥åˆå§‹åŒ–ç­–ç•¥å’Œå…¨æ–°çš„é«˜æ–¯å¸§é—´å†—ä½™æ¶ˆé™¤æœºåˆ¶ï¼Œè¯¥è§†é¢‘æµæ°´çº¿å°†æ­¤å‰åŸºäºé«˜æ–¯ç‚¹äº‘çš„å›¾åƒç¼–è§£ç å™¨ç¼–ç é€Ÿåº¦æå‡äº†88%ã€‚ä½œä¸ºç¥ç»è§†é¢‘å‹ç¼©é¢†åŸŸé¦–ä¸ªåˆ©ç”¨2D Gaussian Splattingçš„è§£å†³æ–¹æ¡ˆï¼Œè¯¥å·¥ä½œä¸ºåœ¨è§†é¢‘æµåª’ä½“å¹³å°å®ç°é«˜æ•ˆã€å®æ—¶çš„ç«¯åˆ°ç«¯æœºå™¨å­¦ä¹ è§†é¢‘é€šä¿¡æä¾›äº†å¯èƒ½ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG",
        "eess.IV"
      ],
      "primary_category": "cs.CV",
      "comment": "9 pages, 8 figures",
      "pdf_url": "https://arxiv.org/pdf/2505.09324v1",
      "published_date": "2025-05-14 12:23:53 UTC",
      "updated_date": "2025-05-14 12:23:53 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T08:42:50.414173+00:00"
    },
    {
      "arxiv_id": "2505.09295v1",
      "title": "Toward Fair Federated Learning under Demographic Disparities and Data Imbalance",
      "title_zh": "è¿ˆå‘äººå£ç»Ÿè®¡å­¦å·®å¼‚ä¸æ•°æ®ä¸å¹³è¡¡ä¸‹çš„å…¬å¹³è”é‚¦å­¦ä¹ ",
      "authors": [
        "Qiming Wu",
        "Siqi Li",
        "Doudou Zhou",
        "Nan Liu"
      ],
      "abstract": "Ensuring fairness is critical when applying artificial intelligence to high-stakes domains such as healthcare, where predictive models trained on imbalanced and demographically skewed data risk exacerbating existing disparities. Federated learning (FL) enables privacy-preserving collaboration across institutions, but remains vulnerable to both algorithmic bias and subgroup imbalance - particularly when multiple sensitive attributes intersect. We propose FedIDA (Fed erated Learning for Imbalance and D isparity A wareness), a framework-agnostic method that combines fairness-aware regularization with group-conditional oversampling. FedIDA supports multiple sensitive attributes and heterogeneous data distributions without altering the convergence behavior of the underlying FL algorithm. We provide theoretical analysis establishing fairness improvement bounds using Lipschitz continuity and concentration inequalities, and show that FedIDA reduces the variance of fairness metrics across test sets. Empirical results on both benchmark and real-world clinical datasets confirm that FedIDA consistently improves fairness while maintaining competitive predictive performance, demonstrating its effectiveness for equitable and privacy-preserving modeling in healthcare. The source code is available on GitHub.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹åŒ»ç–—ä¿å¥ç­‰é«˜é£é™©é¢†åŸŸåœ¨åº”ç”¨äººå·¥æ™ºèƒ½æ—¶é¢ä¸´çš„äººå£ç»Ÿè®¡å·®å¼‚å’Œæ•°æ®ä¸å¹³è¡¡é—®é¢˜ï¼ŒæŒ‡å‡ºè”é‚¦å­¦ä¹ (Federated Learning)åœ¨å¤„ç†å¤šé‡æ•æ„Ÿå±æ€§äº¤å‰æ—¶å®¹æ˜“äº§ç”Ÿç®—æ³•åè§ã€‚ä¸ºæ­¤ï¼Œä½œè€…æå‡ºäº†FedIDA (Federated Learning for Imbalance and Disparity Awareness)æ¡†æ¶ï¼Œè¿™æ˜¯ä¸€ç§ç»“åˆäº†å…¬å¹³æ€§æ„ŸçŸ¥æ­£åˆ™åŒ–(fairness-aware regularization)å’Œç¾¤ä½“æ¡ä»¶è¿‡é‡‡æ ·(group-conditional oversampling)çš„é€šç”¨æ–¹æ³•ã€‚è¯¥æ¡†æ¶æ”¯æŒå¤šä¸ªæ•æ„Ÿå±æ€§å’Œå¼‚æ„æ•°æ®åˆ†å¸ƒï¼Œä¸”åœ¨ä¸æ”¹å˜åº•å±‚è”é‚¦å­¦ä¹ ç®—æ³•æ”¶æ•›ç‰¹æ€§çš„å‰æä¸‹å¢å¼ºäº†ç³»ç»Ÿçš„å…¬å¹³æ€§ã€‚ç ”ç©¶é€šè¿‡Lipschitz continuityå’Œæµ“åº¦ä¸ç­‰å¼(concentration inequalities)æä¾›äº†å…¬å¹³æ€§æ”¹è¿›çš„ç†è®ºç•Œé™åˆ†æï¼Œè¯æ˜äº†FedIDAèƒ½å¤Ÿé™ä½æµ‹è¯•é›†å…¬å¹³æ€§æŒ‡æ ‡çš„æ–¹å·®ã€‚åœ¨åŸºå‡†æ•°æ®é›†å’ŒçœŸå®ä¸´åºŠæ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼ŒFedIDAåœ¨ä¿æŒç«äº‰æ€§é¢„æµ‹æ€§èƒ½çš„åŒæ—¶æ˜¾è‘—æå‡äº†å…¬å¹³æ€§ï¼Œä¸ºåŒ»ç–—é¢†åŸŸæ„å»ºå…¬å¹³ä¸”ä¿æŠ¤éšç§çš„æ¨¡å‹æä¾›äº†æœ‰æ•ˆæ–¹æ¡ˆã€‚",
      "categories": [
        "cs.CY",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CY",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.09295v1",
      "published_date": "2025-05-14 11:22:54 UTC",
      "updated_date": "2025-05-14 11:22:54 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T08:42:46.981694+00:00"
    },
    {
      "arxiv_id": "2505.09289v1",
      "title": "Reproducibility Study of \"Cooperate or Collapse: Emergence of Sustainable Cooperation in a Society of LLM Agents\"",
      "title_zh": "ã€Šåˆä½œè¿˜æ˜¯å´©æºƒï¼šå¤§è¯­è¨€æ¨¡å‹æ™ºèƒ½ä½“ç¤¾ä¼šä¸­å¯æŒç»­åˆä½œçš„æ¶Œç°ã€‹å¤ç°æ€§ç ”ç©¶",
      "authors": [
        "Pedro M. P. Curvo",
        "Mara Dragomir",
        "Salvador Torpes",
        "Mohammadmahdi Rahimi"
      ],
      "abstract": "This study evaluates and extends the findings made by Piatti et al., who introduced GovSim, a simulation framework designed to assess the cooperative decision-making capabilities of large language models (LLMs) in resource-sharing scenarios. By replicating key experiments, we validate claims regarding the performance of large models, such as GPT-4-turbo, compared to smaller models. The impact of the universalization principle is also examined, with results showing that large models can achieve sustainable cooperation, with or without the principle, while smaller models fail without it. In addition, we provide multiple extensions to explore the applicability of the framework to new settings. We evaluate additional models, such as DeepSeek-V3 and GPT-4o-mini, to test whether cooperative behavior generalizes across different architectures and model sizes. Furthermore, we introduce new settings: we create a heterogeneous multi-agent environment, study a scenario using Japanese instructions, and explore an \"inverse environment\" where agents must cooperate to mitigate harmful resource distributions. Our results confirm that the benchmark can be applied to new models, scenarios, and languages, offering valuable insights into the adaptability of LLMs in complex cooperative tasks. Moreover, the experiment involving heterogeneous multi-agent systems demonstrates that high-performing models can influence lower-performing ones to adopt similar behaviors. This finding has significant implications for other agent-based applications, potentially enabling more efficient use of computational resources and contributing to the development of more effective cooperative AI systems.",
      "tldr_zh": "æœ¬ç ”ç©¶å¯¹Piattiç­‰äººæå‡ºçš„GovSimæ¡†æ¶è¿›è¡Œäº†å¤ç°ä¸æ‰©å±•ï¼Œæ—¨åœ¨è¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨èµ„æºå…±äº«åœºæ™¯ä¸­çš„åä½œå†³ç­–èƒ½åŠ›ã€‚ç ”ç©¶éªŒè¯äº†GPT-4-turboç­‰å¤§å‹æ¨¡å‹åœ¨æœ‰æ— æ™®éåŒ–åŸåˆ™ï¼ˆuniversalization principleï¼‰çš„æƒ…å†µä¸‹å‡èƒ½å®ç°å¯æŒç»­åä½œï¼Œè€Œå°å‹æ¨¡å‹åœ¨ç¼ºä¹è¯¥åŸåˆ™æ—¶è¡¨ç°ä¸ä½³ã€‚ä½œè€…è¿›ä¸€æ­¥æ‰©å±•äº†å®éªŒèŒƒå›´ï¼Œæµ‹è¯•äº†DeepSeek-V3å’ŒGPT-4o-miniç­‰æ¨¡å‹ï¼Œå¹¶å¼•å…¥äº†æ—¥è¯­æŒ‡ä»¤ã€å¼‚æ„å¤šæ™ºèƒ½ä½“ç¯å¢ƒï¼ˆheterogeneous multi-agent environmentï¼‰ä»¥åŠæ—¨åœ¨å‡è½»æœ‰å®³èµ„æºåˆ†å¸ƒçš„åå‘ç¯å¢ƒï¼ˆinverse environmentï¼‰ã€‚å®éªŒç»“æœç¡®è®¤äº†è¯¥åŸºå‡†æµ‹è¯•åœ¨ä¸åŒæ¨¡å‹ã€åœºæ™¯å’Œè¯­è¨€ä¸­çš„é€‚ç”¨æ€§ï¼Œå¹¶ç‰¹åˆ«å‘ç°é«˜æ€§èƒ½æ¨¡å‹èƒ½å¤Ÿå¼•å¯¼ä½æ€§èƒ½æ¨¡å‹äº§ç”Ÿç±»ä¼¼çš„åä½œè¡Œä¸ºã€‚è¿™ä¸€å‘ç°å¯¹äºå¼‚æ„å¤šæ™ºèƒ½ä½“ç³»ç»Ÿçš„åº”ç”¨å…·æœ‰é‡è¦æ„ä¹‰ï¼Œä¸ä»…æœ‰åŠ©äºæé«˜è®¡ç®—èµ„æºçš„ä½¿ç”¨æ•ˆç‡ï¼Œä¹Ÿä¸ºå¼€å‘æ›´æœ‰æ•ˆçš„åä½œAIç³»ç»Ÿï¼ˆcooperative AI systemsï¼‰æä¾›äº†é‡è¦å¯ç¤ºã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "11 Tables, 9 Figures",
      "pdf_url": "https://arxiv.org/pdf/2505.09289v1",
      "published_date": "2025-05-14 11:15:14 UTC",
      "updated_date": "2025-05-14 11:15:14 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T08:42:56.304086+00:00"
    },
    {
      "arxiv_id": "2505.13489v1",
      "title": "Contrastive Cross-Course Knowledge Tracing via Concept Graph Guided Knowledge Transfer",
      "title_zh": "åŸºäºæ¦‚å¿µå›¾å¼•å¯¼çŸ¥è¯†è¿ç§»çš„å¯¹æ¯”å¼è·¨è¯¾ç¨‹çŸ¥è¯†è¿½è¸ª",
      "authors": [
        "Wenkang Han",
        "Wang Lin",
        "Liya Hu",
        "Zhenlong Dai",
        "Yiyun Zhou",
        "Mengze Li",
        "Zemin Liu",
        "Chang Yao",
        "Jingyuan Chen"
      ],
      "abstract": "Knowledge tracing (KT) aims to predict learners' future performance based on historical learning interactions. However, existing KT models predominantly focus on data from a single course, limiting their ability to capture a comprehensive understanding of learners' knowledge states. In this paper, we propose TransKT, a contrastive cross-course knowledge tracing method that leverages concept graph guided knowledge transfer to model the relationships between learning behaviors across different courses, thereby enhancing knowledge state estimation. Specifically, TransKT constructs a cross-course concept graph by leveraging zero-shot Large Language Model (LLM) prompts to establish implicit links between related concepts across different courses. This graph serves as the foundation for knowledge transfer, enabling the model to integrate and enhance the semantic features of learners' interactions across courses. Furthermore, TransKT includes an LLM-to-LM pipeline for incorporating summarized semantic features, which significantly improves the performance of Graph Convolutional Networks (GCNs) used for knowledge transfer. Additionally, TransKT employs a contrastive objective that aligns single-course and cross-course knowledge states, thereby refining the model's ability to provide a more robust and accurate representation of learners' overall knowledge states.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† TransKTï¼Œä¸€ç§åŸºäºå¯¹æ¯”å­¦ä¹ çš„è·¨è¯¾ç¨‹çŸ¥è¯†è¿½è¸ª (Knowledge Tracing) æ–¹æ³•ï¼Œæ—¨åœ¨è§£å†³ç°æœ‰æ¨¡å‹ä»…å…³æ³¨å•è¯¾ç¨‹æ•°æ®è€Œæ— æ³•å…¨é¢æ•æ‰å­¦ä¹ è€…çŸ¥è¯†çŠ¶æ€çš„é—®é¢˜ã€‚è¯¥æ¡†æ¶åˆ©ç”¨é›¶æ ·æœ¬å¤§è¯­è¨€æ¨¡å‹ (zero-shot LLM) æç¤ºè¯æ„å»ºè·¨è¯¾ç¨‹æ¦‚å¿µå›¾ (concept graph)ï¼Œä»è€Œå»ºç«‹ä¸åŒè¯¾ç¨‹ç›¸å…³æ¦‚å¿µä¹‹é—´çš„éšæ€§è”ç³»å¹¶å¥ å®šçŸ¥è¯†è¿ç§»çš„åŸºç¡€ã€‚TransKT æ•´åˆäº†å­¦ä¹ è€…è·¨è¯¾ç¨‹äº¤äº’çš„è¯­ä¹‰ç‰¹å¾ï¼Œå¹¶é€šè¿‡ä¸€ç§ LLM-to-LM æµæ°´çº¿æ˜¾è‘—æå‡äº†å›¾å·ç§¯ç½‘ç»œ (GCNs) åœ¨çŸ¥è¯†è¿ç§»è¿‡ç¨‹ä¸­çš„æ€§èƒ½ã€‚æ­¤å¤–ï¼Œæ¨¡å‹å¼•å…¥äº†å¯¹æ¯”ç›®æ ‡ (contrastive objective) æ¥å¯¹é½å•è¯¾ç¨‹ä¸è·¨è¯¾ç¨‹çš„çŸ¥è¯†çŠ¶æ€ï¼Œä»è€Œå®ç°æ›´ç²¾å‡†çš„çŠ¶æ€ä¼°è®¡ã€‚å®éªŒè¯æ˜ï¼Œè¯¥æ–¹æ³•èƒ½æœ‰æ•ˆåˆ©ç”¨è·¨è¯¾ç¨‹è¡Œä¸ºå…³ç³»ï¼Œä¸ºå­¦ä¹ è€…çš„æ•´ä½“çŸ¥è¯†çŠ¶æ€æä¾›æ›´ç¨³å¥ä¸”å‡†ç¡®çš„è¡¨å¾ã€‚",
      "categories": [
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.AI",
      "comment": "Accepted by IJCAI 2025",
      "pdf_url": "https://arxiv.org/pdf/2505.13489v1",
      "published_date": "2025-05-14 10:38:30 UTC",
      "updated_date": "2025-05-14 10:38:30 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T08:43:54.058421+00:00"
    },
    {
      "arxiv_id": "2505.09265v1",
      "title": "MetaUAS: Universal Anomaly Segmentation with One-Prompt Meta-Learning",
      "title_zh": "MetaUASï¼šåŸºäºå•æç¤ºå…ƒå­¦ä¹ çš„é€šç”¨å¼‚å¸¸åˆ†å‰²",
      "authors": [
        "Bin-Bin Gao"
      ],
      "abstract": "Zero- and few-shot visual anomaly segmentation relies on powerful vision-language models that detect unseen anomalies using manually designed textual prompts. However, visual representations are inherently independent of language. In this paper, we explore the potential of a pure visual foundation model as an alternative to widely used vision-language models for universal visual anomaly segmentation. We present a novel paradigm that unifies anomaly segmentation into change segmentation. This paradigm enables us to leverage large-scale synthetic image pairs, featuring object-level and local region changes, derived from existing image datasets, which are independent of target anomaly datasets. We propose a one-prompt Meta-learning framework for Universal Anomaly Segmentation (MetaUAS) that is trained on this synthetic dataset and then generalizes well to segment any novel or unseen visual anomalies in the real world. To handle geometrical variations between prompt and query images, we propose a soft feature alignment module that bridges paired-image change perception and single-image semantic segmentation. This is the first work to achieve universal anomaly segmentation using a pure vision model without relying on special anomaly detection datasets and pre-trained visual-language models. Our method effectively and efficiently segments any anomalies with only one normal image prompt and enjoys training-free without guidance from language. Our MetaUAS significantly outperforms previous zero-shot, few-shot, and even full-shot anomaly segmentation methods. The code and pre-trained models are available at https://github.com/gaobb/MetaUAS.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† MetaUASï¼Œè¿™æ˜¯ä¸€ç§åŸºäºå•æç¤ºå…ƒå­¦ä¹  (One-Prompt Meta-Learning) çš„é€šç”¨å¼‚å¸¸åˆ†å‰²æ¡†æ¶ï¼Œæ—¨åœ¨åˆ©ç”¨çº¯è§†è§‰åŸºç¡€æ¨¡å‹æ›¿ä»£å¹¿æ³›ä½¿ç”¨çš„è§†è§‰è¯­è¨€æ¨¡å‹ (Vision-Language Models)ã€‚è¯¥æ¡†æ¶å°†å¼‚å¸¸åˆ†å‰²ç»Ÿä¸€ä¸ºå˜åŒ–åˆ†å‰² (Change Segmentation) èŒƒå¼ï¼Œé€šè¿‡åœ¨åŒ…å«å¯¹è±¡çº§å’Œå±€éƒ¨åŒºåŸŸå˜åŒ–çš„åˆæˆå›¾åƒå¯¹ä¸Šè¿›è¡Œè®­ç»ƒï¼Œå®ç°äº†å¯¹ç°å®ä¸–ç•Œä¸­æœªçŸ¥å¼‚å¸¸çš„æ³›åŒ–ã€‚ä¸ºäº†å…‹æœæç¤ºå›¾åƒä¸æŸ¥è¯¢å›¾åƒé—´çš„å‡ ä½•åå·®ï¼Œç ”ç©¶å¼•å…¥äº†è½¯ç‰¹å¾å¯¹é½æ¨¡å— (Soft Feature Alignment) æ¥æ¡¥æ¥å˜åŒ–æ„ŸçŸ¥ä¸è¯­ä¹‰åˆ†å‰²ã€‚ä½œä¸ºé¦–ä¸ªä¸ä¾èµ–ç‰¹æ®Šå¼‚å¸¸æ£€æµ‹æ•°æ®é›†å’Œé¢„è®­ç»ƒè§†è§‰è¯­è¨€æ¨¡å‹çš„çº¯è§†è§‰æ–¹æ¡ˆï¼ŒMetaUAS ä»…éœ€å•å¼ æ­£å¸¸å›¾åƒæç¤ºå³å¯å®ç°æ— éœ€é¢å¤–è®­ç»ƒçš„æ¨ç†ã€‚å®éªŒè¯æ˜ï¼Œè¯¥æ–¹æ³•åœ¨æ€§èƒ½ä¸Šæ˜¾è‘—ä¼˜äºç°æœ‰çš„é›¶æ ·æœ¬ (Zero-Shot)ã€å°‘æ ·æœ¬ (Few-Shot) ä¹ƒè‡³å…¨æ ·æœ¬ (Full-Shot) å¼‚å¸¸åˆ†å‰²æŠ€æœ¯ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "Accepted by NeurIPS 2024",
      "pdf_url": "https://arxiv.org/pdf/2505.09265v1",
      "published_date": "2025-05-14 10:25:26 UTC",
      "updated_date": "2025-05-14 10:25:26 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T08:44:04.284523+00:00"
    },
    {
      "arxiv_id": "2505.09264v1",
      "title": "Learning to Detect Multi-class Anomalies with Just One Normal Image Prompt",
      "title_zh": "ä»…å‡­å•å¼ æ­£å¸¸å›¾åƒæç¤ºçš„å¤šç±»å¼‚å¸¸æ£€æµ‹å­¦ä¹ ",
      "authors": [
        "Bin-Bin Gao"
      ],
      "abstract": "Unsupervised reconstruction networks using self-attention transformers have achieved state-of-the-art performance for multi-class (unified) anomaly detection with a single model. However, these self-attention reconstruction models primarily operate on target features, which may result in perfect reconstruction for both normal and anomaly features due to high consistency with context, leading to failure in detecting anomalies. Additionally, these models often produce inaccurate anomaly segmentation due to performing reconstruction in a low spatial resolution latent space. To enable reconstruction models enjoying high efficiency while enhancing their generalization for unified anomaly detection, we propose a simple yet effective method that reconstructs normal features and restores anomaly features with just One Normal Image Prompt (OneNIP). In contrast to previous work, OneNIP allows for the first time to reconstruct or restore anomalies with just one normal image prompt, effectively boosting unified anomaly detection performance. Furthermore, we propose a supervised refiner that regresses reconstruction errors by using both real normal and synthesized anomalous images, which significantly improves pixel-level anomaly segmentation. OneNIP outperforms previous methods on three industry anomaly detection benchmarks: MVTec, BTAD, and VisA. The code and pre-trained models are available at https://github.com/gaobb/OneNIP.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤šç±»åˆ«å¼‚å¸¸æ£€æµ‹ä¸­è‡ªæ³¨æ„åŠ›(self-attention)é‡å»ºç½‘ç»œå®¹æ˜“äº§ç”Ÿâ€œå®Œç¾é‡å»ºâ€ä»¥åŠä½åˆ†è¾¨ç‡å¯¼è‡´åˆ†å‰²ä¸å‡†çš„é—®é¢˜ï¼Œæå‡ºäº† OneNIP æ–¹æ³•ã€‚OneNIP é¦–æ¬¡å®ç°ä»…é€šè¿‡ä¸€ä¸ªæ­£å¸¸å›¾åƒæç¤º(One Normal Image Prompt)æ¥é‡å»ºæ­£å¸¸ç‰¹å¾å¹¶ä¿®å¤å¼‚å¸¸ç‰¹å¾ï¼Œæ˜¾è‘—æå‡äº†ç»Ÿä¸€å¼‚å¸¸æ£€æµ‹(unified anomaly detection)çš„æ€§èƒ½ä¸æ³›åŒ–èƒ½åŠ›ã€‚æ­¤å¤–ï¼Œç ”ç©¶è¿˜å¼•å…¥äº†ä¸€ä¸ªç›‘ç£ç»†åŒ–å™¨(supervised refiner)ï¼Œåˆ©ç”¨çœŸå®æ­£å¸¸å›¾åƒå’Œåˆæˆå¼‚å¸¸å›¾åƒæ¥å›å½’é‡å»ºè¯¯å·®ï¼Œä»è€Œå¤§å¹…æé«˜äº†åƒç´ çº§çš„å¼‚å¸¸åˆ†å‰²(anomaly segmentation)ç²¾åº¦ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒOneNIP åœ¨ MVTecã€BTAD å’Œ VisA ä¸‰ä¸ªå·¥ä¸šå¼‚å¸¸æ£€æµ‹åŸºå‡†æµ‹è¯•ä¸­å‡ä¼˜äºç°æœ‰æ–¹æ³•ã€‚è¯¥æ–¹æ¡ˆä¸ä»…æé«˜äº†é‡å»ºæ¨¡å‹çš„æ•ˆç‡ï¼Œè¿˜ä¸ºå·¥ä¸šåœºæ™¯ä¸‹çš„ç²¾å‡†å¼‚å¸¸æ£€æµ‹æä¾›äº†æœ‰æ•ˆçš„æŠ€æœ¯æ”¯æ’‘ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "Accepted by ECCV 2024",
      "pdf_url": "https://arxiv.org/pdf/2505.09264v1",
      "published_date": "2025-05-14 10:25:14 UTC",
      "updated_date": "2025-05-14 10:25:14 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T08:44:20.321816+00:00"
    },
    {
      "arxiv_id": "2505.09263v1",
      "title": "Few-Shot Anomaly-Driven Generation for Anomaly Classification and Segmentation",
      "title_zh": "é¢å‘å¼‚å¸¸åˆ†ç±»ä¸åˆ†å‰²çš„å°‘æ ·æœ¬å¼‚å¸¸é©±åŠ¨ç”Ÿæˆ",
      "authors": [
        "Guan Gui",
        "Bin-Bin Gao",
        "Jun Liu",
        "Chengjie Wang",
        "Yunsheng Wu"
      ],
      "abstract": "Anomaly detection is a practical and challenging task due to the scarcity of anomaly samples in industrial inspection. Some existing anomaly detection methods address this issue by synthesizing anomalies with noise or external data. However, there is always a large semantic gap between synthetic and real-world anomalies, resulting in weak performance in anomaly detection. To solve the problem, we propose a few-shot Anomaly-driven Generation (AnoGen) method, which guides the diffusion model to generate realistic and diverse anomalies with only a few real anomalies, thereby benefiting training anomaly detection models. Specifically, our work is divided into three stages. In the first stage, we learn the anomaly distribution based on a few given real anomalies and inject the learned knowledge into an embedding. In the second stage, we use the embedding and given bounding boxes to guide the diffusion model to generate realistic and diverse anomalies on specific objects (or textures). In the final stage, we propose a weakly-supervised anomaly detection method to train a more powerful model with generated anomalies. Our method builds upon DRAEM and DesTSeg as the foundation model and conducts experiments on the commonly used industrial anomaly detection dataset, MVTec. The experiments demonstrate that our generated anomalies effectively improve the model performance of both anomaly classification and segmentation tasks simultaneously, \\eg, DRAEM and DseTSeg achieved a 5.8\\% and 1.5\\% improvement in AU-PR metric on segmentation task, respectively. The code and generated anomalous data are available at https://github.com/gaobb/AnoGen.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†AnoGenï¼Œä¸€ç§å°‘æ ·æœ¬å¼‚å¸¸é©±åŠ¨ç”Ÿæˆ(Few-shot Anomaly-driven Generation)æ–¹æ³•ï¼Œæ—¨åœ¨è§£å†³å·¥ä¸šæ£€æµ‹ä¸­å¼‚å¸¸æ ·æœ¬ç¨€ç¼ºå¯¼è‡´åˆæˆå¼‚å¸¸ä¸çœŸå®æ ·æœ¬å­˜åœ¨å·¨å¤§è¯­ä¹‰å·®è·çš„é—®é¢˜ã€‚è¯¥æ–¹æ³•é€šè¿‡ä¸‰ä¸ªé˜¶æ®µå¼•å¯¼æ‰©æ•£æ¨¡å‹(diffusion model)ç”ŸæˆçœŸå®ä¸”å¤šæ ·åŒ–çš„å¼‚å¸¸ï¼šé¦–å…ˆä»æå°‘æ•°çœŸå®æ ·æœ¬ä¸­å­¦ä¹ å¼‚å¸¸åˆ†å¸ƒå¹¶å°†å…¶æ³¨å…¥embeddingï¼Œéšååˆ©ç”¨è¯¥embeddingå’Œè¾¹ç•Œæ¡†(bounding boxes)åœ¨ç‰¹å®šç‰©ä½“æˆ–çº¹ç†ä¸Šç”Ÿæˆå¼‚å¸¸ï¼Œæœ€ååŸºäºè¿™äº›ç”Ÿæˆçš„å¼‚å¸¸é‡‡ç”¨å¼±ç›‘ç£(weakly-supervised)æ–¹æ³•è®­ç»ƒæ£€æµ‹æ¨¡å‹ã€‚ç ”ç©¶åœ¨é€šç”¨çš„å·¥ä¸šå¼‚å¸¸æ£€æµ‹æ•°æ®é›†MVTecä¸Šè¿›è¡Œäº†å®éªŒï¼Œå¹¶ä»¥DRAEMå’ŒDesTSegä½œä¸ºåŸºç¡€æ¨¡å‹ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒAnoGenç”Ÿæˆçš„å¼‚å¸¸èƒ½æ˜¾è‘—æå‡æ¨¡å‹åœ¨å¼‚å¸¸åˆ†ç±»å’Œåˆ†å‰²ä»»åŠ¡ä¸­çš„è¡¨ç°ï¼Œå…¶ä¸­åœ¨åˆ†å‰²ä»»åŠ¡çš„AU-PRæŒ‡æ ‡ä¸Šï¼ŒDRAEMå’ŒDesTSegåˆ†åˆ«å®ç°äº†5.8%å’Œ1.5%çš„æ€§èƒ½æå‡ã€‚è¯¥ç ”ç©¶è¯æ˜äº†åˆ©ç”¨ç”Ÿæˆå¼æ¨¡å‹å¼¥è¡¥çœŸå®å¼‚å¸¸æ•°æ®ä¸è¶³çš„æœ‰æ•ˆæ€§ï¼Œå¹¶ä¸ºæå‡å·¥ä¸šè§†è§‰æ£€æµ‹çš„é²æ£’æ€§æä¾›äº†æ–°æ€è·¯ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "Accepted by ECCV 2024",
      "pdf_url": "https://arxiv.org/pdf/2505.09263v1",
      "published_date": "2025-05-14 10:25:06 UTC",
      "updated_date": "2025-05-14 10:25:06 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T08:44:38.130396+00:00"
    },
    {
      "arxiv_id": "2505.09262v2",
      "title": "EDBench: Large-Scale Electron Density Data for Molecular Modeling",
      "title_zh": "EDBenchï¼šé¢å‘åˆ†å­å»ºæ¨¡çš„å¤§è§„æ¨¡ç”µå­å¯†åº¦æ•°æ®",
      "authors": [
        "Hongxin Xiang",
        "Ke Li",
        "Mingquan Liu",
        "Zhixiang Cheng",
        "Bin Yao",
        "Wenjie Du",
        "Jun Xia",
        "Li Zeng",
        "Xin Jin",
        "Xiangxiang Zeng"
      ],
      "abstract": "Existing molecular machine learning force fields (MLFFs) generally focus on the learning of atoms, molecules, and simple quantum chemical properties (such as energy and force), but ignore the importance of electron density (ED) $Ï(r)$ in accurately understanding molecular force fields (MFFs). ED describes the probability of finding electrons at specific locations around atoms or molecules, which uniquely determines all ground state properties (such as energy, molecular structure, etc.) of interactive multi-particle systems according to the Hohenberg-Kohn theorem. However, the calculation of ED relies on the time-consuming first-principles density functional theory (DFT) which leads to the lack of large-scale ED data and limits its application in MLFFs. In this paper, we introduce EDBench, a large-scale, high-quality dataset of ED designed to advance learning-based research at the electronic scale. Built upon the PCQM4Mv2, EDBench provides accurate ED data, covering 3.3 million molecules. To comprehensively evaluate the ability of models to understand and utilize electronic information, we design a suite of ED-centric benchmark tasks spanning prediction, retrieval, and generation. Our evaluation on several state-of-the-art methods demonstrates that learning from EDBench is not only feasible but also achieves high accuracy. Moreover, we show that learning-based method can efficiently calculate ED with comparable precision while significantly reducing the computational cost relative to traditional DFT calculations. All data and benchmarks from EDBench will be freely available, laying a robust foundation for ED-driven drug discovery and materials science.",
      "tldr_zh": "ç°æœ‰çš„åˆ†å­æœºå™¨å­¦ä¹ åŠ›åœº(MLFFs)é€šå¸¸ä¾§é‡äºåŸå­å’Œåˆ†å­å±‚é¢çš„å­¦ä¹ ï¼Œè€Œå¿½ç•¥äº†ç”µå­å¯†åº¦(Electron Density, ED)åœ¨å‡†ç¡®ç†è§£åˆ†å­åŠ›åœº(MFFs)ä¸­çš„å…³é”®ä½œç”¨ã€‚ç”±äºä¼ ç»Ÿçš„å¯†åº¦æ³›å‡½ç†è®º(DFT)è®¡ç®—è€—æ—¶å·¨å¤§ï¼Œå¯¼è‡´å¤§è§„æ¨¡EDæ•°æ®çš„ç¼ºå¤±å¹¶é™åˆ¶äº†å…¶åœ¨MLFFsä¸­çš„åº”ç”¨ã€‚è¯¥ç ”ç©¶æ¨å‡ºäº†EDBenchï¼Œè¿™æ˜¯ä¸€ä¸ªåŒ…å«330ä¸‡ä¸ªåˆ†å­çš„é«˜è´¨é‡ã€å¤§è§„æ¨¡ç”µå­å¯†åº¦æ•°æ®é›†ï¼Œæ—¨åœ¨ä¿ƒè¿›ç”µå­å°ºåº¦çš„å­¦ä¹ ç ”ç©¶ã€‚ç ”ç©¶è€…è¿˜è®¾è®¡äº†åŒ…æ‹¬é¢„æµ‹ã€æ£€ç´¢å’Œç”Ÿæˆåœ¨å†…çš„å¤šé¡¹ä»¥EDä¸ºä¸­å¿ƒçš„åŸºå‡†ä»»åŠ¡ã€‚è¯„ä¼°ç»“æœè¡¨æ˜ï¼ŒåŸºäºEDBenchçš„å­¦ä¹ æ–¹æ³•èƒ½å¤Ÿä»¥æé«˜çš„ç²¾åº¦é«˜æ•ˆè®¡ç®—ç”µå­å¯†åº¦ï¼Œåœ¨ä¿æŒä¸DFTç›¸å½“å‡†ç¡®æ€§çš„åŒæ—¶æ˜¾è‘—é™ä½äº†è®¡ç®—æˆæœ¬ã€‚è¯¥æ•°æ®é›†çš„å‘å¸ƒä¸ºç”µå­å¯†åº¦é©±åŠ¨çš„è¯ç‰©å‘ç°å’Œææ–™ç§‘å­¦å¥ å®šäº†åšå®åŸºç¡€ã€‚",
      "categories": [
        "physics.chem-ph",
        "cs.AI",
        "cs.CV",
        "cs.LG"
      ],
      "primary_category": "physics.chem-ph",
      "comment": "accepted by NeurIPS 2025",
      "pdf_url": "https://arxiv.org/pdf/2505.09262v2",
      "published_date": "2025-05-14 10:23:22 UTC",
      "updated_date": "2025-09-24 05:45:47 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T08:44:23.448755+00:00"
    },
    {
      "arxiv_id": "2505.09246v3",
      "title": "Autofocus Retrieval: An Effective Pipeline for Multi-Hop Question Answering With Semi-Structured Knowledge",
      "title_zh": "Autofocus Retrievalï¼šä¸€ç§é¢å‘åŠç»“æ„åŒ–çŸ¥è¯†å¤šè·³é—®ç­”çš„é«˜æ•ˆæµæ°´çº¿",
      "authors": [
        "Derian Boer",
        "Stephen Roth",
        "Stefan Kramer"
      ],
      "abstract": "In many real-world settings, machine learning models and interactive systems have access to both structured knowledge, e.g., knowledge graphs or tables, and unstructured content, e.g., natural language documents. Yet, most rely on either. Semi-Structured Knowledge Bases (SKBs) bridge this gap by linking unstructured content to nodes within structured data. In this work, we present Autofocus-Retriever (AF-Retriever), a modular framework for SKB-based, multi-hop question answering. It combines structural and textual retrieval through novel integration steps and optimizations, achieving the best zero- and one-shot results across all three STaRK QA benchmarks, which span diverse domains and evaluation metrics.\n  AF-Retriever's average first-hit rate surpasses the second-best method by 32.1%. Its performance is driven by (1) leveraging exchangeable large language models (LLMs) to extract entity attributes and relational constraints for both parsing and reranking the top-k answers, (2) vector similarity search for ranking both extracted entities and final answers, (3) a novel incremental scope expansion procedure that prepares for the reranking on a configurable amount of suitable candidates that fulfill the given constraints the most, and (4) a hybrid retrieval strategy that reduces error susceptibility.\n  In summary, while constantly adjusting the focus like an optical autofocus, AF-Retriever delivers a configurable amount of answer candidates in four constraint-driven retrieval steps, which are then supplemented and ranked through four additional processing steps. An ablation study and a detailed error analysis, including a comparison of three different LLM reranking strategies, provide component-level insights. The source code is available at https://github.com/kramerlab/AF-Retriever.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†Autofocus-Retriever (AF-Retriever)ï¼Œè¿™æ˜¯ä¸€ä¸ªä¸“ä¸ºåŸºäºåŠç»“æ„åŒ–çŸ¥è¯†åº“(Semi-Structured Knowledge Bases, SKBs)è¿›è¡Œå¤šè·³é—®ç­”(Multi-hop Question Answering)è®¾è®¡çš„æ¨¡å—åŒ–æ¡†æ¶ã€‚è¯¥æ¡†æ¶é€šè¿‡åˆ›æ–°çš„é›†æˆæ­¥éª¤ï¼Œå°†ç»“æ„åŒ–æ£€ç´¢ä¸æ–‡æœ¬æ£€ç´¢ç›¸ç»“åˆï¼Œæœ‰æ•ˆåœ°æ¡¥æ¥äº†ç»“æ„åŒ–æ•°æ®ä¸éç»“æ„åŒ–å†…å®¹ä¹‹é—´çš„é¸¿æ²Ÿã€‚å…¶æ ¸å¿ƒæœºåˆ¶åŒ…æ‹¬åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹(LLMs)æå–å®ä½“å±æ€§ä¸å…³ç³»çº¦æŸã€åº”ç”¨å‘é‡ç›¸ä¼¼æ€§æœç´¢è¿›è¡Œæ’åºï¼Œä»¥åŠé‡‡ç”¨ä¸€ç§æ–°å‹çš„å¢é‡èŒƒå›´æ‰©å±•ç¨‹åº(incremental scope expansion)æ¥ä¼˜åŒ–å€™é€‰ç­”æ¡ˆçš„é‡æ’åºã€‚AF-Retriever é‡‡ç”¨æ··åˆæ£€ç´¢ç­–ç•¥ï¼Œé€šè¿‡å››ä¸ªçº¦æŸé©±åŠ¨çš„æ£€ç´¢æ­¥éª¤å’Œå››ä¸ªåç»­å¤„ç†æ­¥éª¤ï¼Œåƒå…‰å­¦è‡ªåŠ¨å¯¹ç„¦ä¸€æ ·ç²¾ç¡®é”å®šç­”æ¡ˆã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ¡†æ¶åœ¨ STaRK QA åŸºå‡†æµ‹è¯•ä¸­å–å¾—äº†æ˜¾è‘—çš„é›¶æ ·æœ¬å’Œå•æ ·æœ¬è¡¨ç°ï¼Œå¹³å‡é¦–å‡»ç‡(first-hit rate)è¾ƒç°æœ‰æœ€ä¼˜æ–¹æ³•æå‡äº†32.1%ã€‚è¯¥ç ”ç©¶ä¸ä»…æä¾›äº†é«˜æ•ˆçš„æ£€ç´¢æµæ°´çº¿ï¼Œè¿˜é€šè¿‡è¯¦ç»†çš„æ¶ˆèå®éªŒä¸ºç»„ä»¶çº§ä¼˜åŒ–æä¾›äº†æ·±åˆ»è§è§£ã€‚",
      "categories": [
        "cs.IR",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.IR",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.09246v3",
      "published_date": "2025-05-14 09:35:56 UTC",
      "updated_date": "2026-01-14 14:49:17 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T08:44:24.205356+00:00"
    },
    {
      "arxiv_id": "2505.09208v1",
      "title": "Educational impacts of generative artificial intelligence on learning and performance of engineering students in China",
      "title_zh": "ç”Ÿæˆå¼äººå·¥æ™ºèƒ½å¯¹ China å·¥ç¨‹ä¸“ä¸šå­¦ç”Ÿå­¦ä¹ ä¸å­¦ä¸šè¡¨ç°çš„æ•™è‚²å½±å“",
      "authors": [
        "Lei Fan",
        "Kunyang Deng",
        "Fangxue Liu"
      ],
      "abstract": "With the rapid advancement of generative artificial intelligence(AI), its potential applications in higher education have attracted significant attention. This study investigated how 148 students from diverse engineering disciplines and regions across China used generative AI, focusing on its impact on their learning experience and the opportunities and challenges it poses in engineering education. Based on the surveyed data, we explored four key areas: the frequency and application scenarios of AI use among engineering students, its impact on students' learning and performance, commonly encountered challenges in using generative AI, and future prospects for its adoption in engineering education. The results showed that more than half of the participants reported a positive impact of generative AI on their learning efficiency, initiative, and creativity, with nearly half believing it also enhanced their independent thinking. However, despite acknowledging improved study efficiency, many felt their actual academic performance remained largely unchanged and expressed concerns about the accuracy and domain-specific reliability of generative AI. Our findings provide a first-hand insight into the current benefits and challenges generative AI brings to students, particularly Chinese engineering students, while offering several recommendations, especially from the students' perspective, for effectively integrating generative AI into engineering education.",
      "tldr_zh": "è¯¥ç ”ç©¶è°ƒæŸ¥äº†ç”Ÿæˆå¼äººå·¥æ™ºèƒ½ (Generative AI) å¯¹ä¸­å›½ 148 åæ¥è‡ªä¸åŒå·¥ç¨‹å­¦ç§‘çš„å­¦ç”Ÿåœ¨å­¦ä¹ ä½“éªŒã€æœºé‡ä¸æŒ‘æˆ˜æ–¹é¢çš„å½±å“ã€‚é€šè¿‡è°ƒç ”åˆ†æï¼Œç ”ç©¶é‡ç‚¹æ¢è®¨äº† AI çš„åº”ç”¨åœºæ™¯åŠå…¶å¯¹å­¦ç”Ÿå­¦ä¹ æ•ˆç‡ä¸å­¦ä¸šè¡¨ç°çš„ä½œç”¨ï¼Œå¹¶æ€»ç»“äº†ä½¿ç”¨è¿‡ç¨‹ä¸­çš„å¸¸è§æŒ‘æˆ˜ã€‚ç»“æœæ˜¾ç¤ºï¼Œè¶…è¿‡åŠæ•°çš„å‚ä¸è€…è®¤ä¸º Generative AI æ˜¾è‘—æå‡äº†å­¦ä¹ æ•ˆç‡ã€ä¸»åŠ¨æ€§å’Œåˆ›é€ åŠ›ï¼Œä¸”è¿‘åŠæ•°å­¦ç”Ÿè®¤ä¸ºå…¶å¢å¼ºäº†ç‹¬ç«‹æ€è€ƒèƒ½åŠ›ã€‚ç„¶è€Œï¼Œå°½ç®¡æ•ˆç‡æœ‰æ‰€æ”¹å–„ï¼Œè®¸å¤šå­¦ç”Ÿåæ˜ å…¶å®é™…å­¦ä¸šè¡¨ç° (Academic Performance) å¹¶æ— æ˜¾è‘—å˜åŒ–ï¼Œå¹¶å¯¹å…¶è¾“å‡ºå†…å®¹çš„å‡†ç¡®æ€§ä¸é¢†åŸŸç‰¹å®šå¯é æ€§ (Domain-specific Reliability) è¡¨è¾¾äº†æ‹…å¿§ã€‚è¯¥ç ”ç©¶ä¸º Generative AI åœ¨ä¸­å›½å·¥ç¨‹æ•™è‚²ä¸­çš„åº”ç”¨ç°çŠ¶æä¾›äº†ç¬¬ä¸€æ‰‹è§è§£ï¼Œå¹¶ä»å­¦ç”Ÿè§†è§’å‡ºå‘ï¼Œä¸ºå°†å…¶æœ‰æ•ˆæ•´åˆè‡³æ•™è‚²ä½“ç³»æå‡ºäº†é’ˆå¯¹æ€§å»ºè®®ã€‚",
      "categories": [
        "cs.HC",
        "cs.AI"
      ],
      "primary_category": "cs.HC",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.09208v1",
      "published_date": "2025-05-14 07:52:54 UTC",
      "updated_date": "2025-05-14 07:52:54 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T08:44:42.458521+00:00"
    },
    {
      "arxiv_id": "2505.09203v1",
      "title": "InvDesFlow-AL: Active Learning-based Workflow for Inverse Design of Functional Materials",
      "title_zh": "InvDesFlow-ALï¼šåŸºäºä¸»åŠ¨å­¦ä¹ çš„åŠŸèƒ½ææ–™é€†å‘è®¾è®¡å·¥ä½œæµ",
      "authors": [
        "Xiao-Qi Han",
        "Peng-Jie Guo",
        "Ze-Feng Gao",
        "Hao Sun",
        "Zhong-Yi Lu"
      ],
      "abstract": "Developing inverse design methods for functional materials with specific properties is critical to advancing fields like renewable energy, catalysis, energy storage, and carbon capture. Generative models based on diffusion principles can directly produce new materials that meet performance constraints, thereby significantly accelerating the material design process. However, existing methods for generating and predicting crystal structures often remain limited by low success rates. In this work, we propose a novel inverse material design generative framework called InvDesFlow-AL, which is based on active learning strategies. This framework can iteratively optimize the material generation process to gradually guide it towards desired performance characteristics. In terms of crystal structure prediction, the InvDesFlow-AL model achieves an RMSE of 0.0423 Ã…, representing an 32.96% improvement in performance compared to exsisting generative models. Additionally, InvDesFlow-AL has been successfully validated in the design of low-formation-energy and low-Ehull materials. It can systematically generate materials with progressively lower formation energies while continuously expanding the exploration across diverse chemical spaces. These results fully demonstrate the effectiveness of the proposed active learning-driven generative model in accelerating material discovery and inverse design. To further prove the effectiveness of this method, we took the search for BCS superconductors under ambient pressure as an example explored by InvDesFlow-AL. As a result, we successfully identified Li\\(_2\\)AuH\\(_6\\) as a conventional BCS superconductor with an ultra-high transition temperature of 140 K. This discovery provides strong empirical support for the application of inverse design in materials science.",
      "tldr_zh": "æœ¬ç ”ç©¶æå‡ºäº† InvDesFlow-ALï¼Œè¿™æ˜¯ä¸€ç§åŸºäºä¸»åŠ¨å­¦ä¹  (Active Learning) ç­–ç•¥çš„æ–°å‹åŠŸèƒ½ææ–™é€†å‘è®¾è®¡ (Inverse Design) ç”Ÿæˆæ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³ç°æœ‰æ™¶ä½“ç»“æ„ç”Ÿæˆä¸é¢„æµ‹æ–¹æ³•æˆåŠŸç‡ä½çš„é—®é¢˜ã€‚è¯¥æ¡†æ¶ç»“åˆäº†æ‰©æ•£ (Diffusion) åŸç†çš„ç”Ÿæˆæ¨¡å‹ä¸ä¸»åŠ¨å­¦ä¹ ç­–ç•¥ï¼Œé€šè¿‡è¿­ä»£ä¼˜åŒ–ç”Ÿæˆè¿‡ç¨‹ï¼Œé€æ­¥å¼•å¯¼ææ–™è®¾è®¡å‘é¢„æœŸçš„æ€§èƒ½ç‰¹å¾é æ‹¢ã€‚åœ¨æ™¶ä½“ç»“æ„é¢„æµ‹æ–¹é¢ï¼ŒInvDesFlow-AL å®ç°äº† 0.0423 Ã… çš„å‡æ–¹æ ¹è¯¯å·® (RMSE)ï¼Œè¾ƒç°æœ‰ç”Ÿæˆæ¨¡å‹æ€§èƒ½æå‡äº† 32.96%ã€‚è¯¥æ¨¡å‹å·²æˆåŠŸåº”ç”¨äºä½ç”Ÿæˆèƒ½ (Low-formation-energy) å’Œä½ Ehull ææ–™çš„è®¾è®¡ï¼Œèƒ½å¤Ÿç³»ç»Ÿæ€§åœ°ç”Ÿæˆæ€§èƒ½æ›´ä¼˜çš„ææ–™å¹¶æ‰©å±•åŒ–å­¦æ¢ç´¢ç©ºé—´ã€‚æ­¤å¤–ï¼Œç ”ç©¶å›¢é˜Ÿåˆ©ç”¨è¯¥æ¡†æ¶æˆåŠŸå‘ç°äº†è¶…é«˜è½¬å˜æ¸©åº¦ä¸º 140 K çš„ä¼ ç»Ÿ BCS è¶…å¯¼ä½“ Li2AuH6ï¼Œä¸ºé€†å‘è®¾è®¡åœ¨ææ–™ç§‘å­¦ä¸­çš„åº”ç”¨æä¾›äº†å¼ºæœ‰åŠ›çš„å®è¯æ”¯æŒã€‚è¿™äº›ç»“æœå……åˆ†è¯æ˜äº†ä¸»åŠ¨å­¦ä¹ é©±åŠ¨çš„ç”Ÿæˆæ¨¡å‹åœ¨åŠ é€Ÿææ–™å‘ç°å’Œé€†å‘è®¾è®¡æ–¹é¢çš„æ˜¾è‘—æœ‰æ•ˆæ€§ã€‚",
      "categories": [
        "cond-mat.mtrl-sci",
        "cond-mat.supr-con",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cond-mat.mtrl-sci",
      "comment": "29 pages, 11 figures",
      "pdf_url": "https://arxiv.org/pdf/2505.09203v1",
      "published_date": "2025-05-14 07:29:06 UTC",
      "updated_date": "2025-05-14 07:29:06 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T08:44:32.990561+00:00"
    },
    {
      "arxiv_id": "2505.09168v1",
      "title": "DRRNet: Macro-Micro Feature Fusion and Dual Reverse Refinement for Camouflaged Object Detection",
      "title_zh": "DRRNetï¼šé¢å‘ä¼ªè£…ç›®æ ‡æ£€æµ‹çš„å®å¾®è§‚ç‰¹å¾èåˆä¸åŒé‡é€†å‘ç»†åŒ–",
      "authors": [
        "Jianlin Sun",
        "Xiaolin Fang",
        "Juwei Guan",
        "Dongdong Gui",
        "Teqi Wang",
        "Tongxin Zhu"
      ],
      "abstract": "The core challenge in Camouflage Object Detection (COD) lies in the indistinguishable similarity between targets and backgrounds in terms of color, texture, and shape. This causes existing methods to either lose edge details (such as hair-like fine structures) due to over-reliance on global semantic information or be disturbed by similar backgrounds (such as vegetation patterns) when relying solely on local features. We propose DRRNet, a four-stage architecture characterized by a \"context-detail-fusion-refinement\" pipeline to address these issues. Specifically, we introduce an Omni-Context Feature Extraction Module to capture global camouflage patterns and a Local Detail Extraction Module to supplement microstructural information for the full-scene context module. We then design a module for forming dual representations of scene understanding and structural awareness, which fuses panoramic features and local features across various scales. In the decoder, we also introduce a reverse refinement module that leverages spatial edge priors and frequency-domain noise suppression to perform a two-stage inverse refinement of the output. By applying two successive rounds of inverse refinement, the model effectively suppresses background interference and enhances the continuity of object boundaries. Experimental results demonstrate that DRRNet significantly outperforms state-of-the-art methods on benchmark datasets. Our code is available at https://github.com/jerrySunning/DRRNet.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ä¼ªè£…ç›®æ ‡æ£€æµ‹ (Camouflaged Object Detection, COD) ä¸­ç›®æ ‡ä¸èƒŒæ™¯åœ¨é¢œè‰²ã€çº¹ç†åŠå½¢çŠ¶ä¸Šæåº¦ç›¸ä¼¼å¯¼è‡´çš„è¾¹ç¼˜ç»†èŠ‚ä¸¢å¤±å’ŒèƒŒæ™¯å¹²æ‰°é—®é¢˜ï¼Œæå‡ºäº†åä¸º DRRNet çš„å››é˜¶æ®µæ¶æ„ã€‚è¯¥æ¡†æ¶æ„å»ºäº†ä¸€ä¸ªåŒ…å«â€œè¯­å¢ƒ-ç»†èŠ‚-èåˆ-ç»†åŒ–â€ (context-detail-fusion-refinement) çš„æµæ°´çº¿ï¼Œé€šè¿‡ Omni-Context Feature Extraction Module æ•æ‰å…¨å±€ä¼ªè£…æ¨¡å¼ï¼Œå¹¶ç»“åˆ Local Detail Extraction Module è¡¥å……å¾®è§‚ç»“æ„ä¿¡æ¯ã€‚é€šè¿‡å¯¹å…¨æ™¯ç‰¹å¾å’Œå¤šå°ºåº¦å±€éƒ¨ç‰¹å¾çš„èåˆï¼Œæ¨¡å‹èƒ½å¤Ÿå½¢æˆå…¼å…·åœºæ™¯ç†è§£å’Œç»“æ„æ„ŸçŸ¥çš„åŒé‡è¡¨ç¤ºã€‚åœ¨è§£ç å™¨éƒ¨åˆ†ï¼Œç ”ç©¶è®¾è®¡äº† Reverse Refinement Moduleï¼Œåˆ©ç”¨ç©ºé—´è¾¹ç¼˜å…ˆéªŒå’Œé¢‘åŸŸå™ªå£°æŠ‘åˆ¶è¿›è¡Œä¸¤è½®åå‘ç»†åŒ–ã€‚è¿™ä¸€è¿‡ç¨‹æœ‰æ•ˆåœ°æŠ‘åˆ¶äº†å¤æ‚èƒŒæ™¯çš„å¹²æ‰°ï¼Œå¹¶æ˜¾è‘—æå‡äº†ç‰©ä½“è¾¹ç•Œçš„è¿ç»­æ€§ã€‚å®éªŒç»“æœè¯æ˜ï¼ŒDRRNet åœ¨å¤šä¸ªåŸºå‡†æ•°æ®é›†ä¸Šå‡å–å¾—äº†æ˜¾è‘—ä¼˜äºç°æœ‰æœ€å…ˆè¿›æ–¹æ³•çš„ç»“æœã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.09168v1",
      "published_date": "2025-05-14 06:03:53 UTC",
      "updated_date": "2025-05-14 06:03:53 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T08:44:50.288330+00:00"
    },
    {
      "arxiv_id": "2505.09166v5",
      "title": "An Exploration of Default Images in Text-to-Image Generation",
      "title_zh": "æ–‡æœ¬ç”Ÿæˆå›¾åƒä¸­é»˜è®¤å›¾åƒçš„æ¢ç©¶",
      "authors": [
        "Hannu Simonen",
        "Atte Kiviniemi",
        "Hannah Johnston",
        "Helena Barranha",
        "Jonas Oppenlaender"
      ],
      "abstract": "In the creative practice of text-to-image (TTI) generation, images are synthesized from textual prompts. By design, TTI models always yield an output, even if the prompt contains unknown terms. In this case, the model may generate default images: images that closely resemble each other across many unrelated prompts. Studying default images is valuable for designing better solutions for prompt engineering and TTI generation. We present the first investigation into default images on Midjourney. We describe an initial study in which we manually created input prompts triggering default images, and several ablation studies. Building on these, we conduct a computational analysis of over 750,000 images, revealing consistent default images across unrelated prompts. We also conduct an online user study investigating how default images may affect user satisfaction. Our work lays the foundation for understanding default images in TTI generation, highlighting their practical relevance as well as challenges and future research directions.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹æ–‡æœ¬ç”Ÿæˆå›¾åƒ (Text-to-Image) æ¨¡å‹åœ¨å¤„ç†æœªçŸ¥æœ¯è¯­æ—¶äº§ç”Ÿçš„é»˜è®¤å›¾åƒ (Default Images) ç°è±¡è¿›è¡Œäº†é¦–æ¬¡æ·±å…¥è°ƒæŸ¥ã€‚ä½œè€…ä»¥ Midjourney ä¸ºç ”ç©¶å¹³å°ï¼Œé€šè¿‡æ‰‹åŠ¨åˆ›å»ºè§¦å‘æç¤ºè¯å’Œå¤šé¡¹æ¶ˆèå®éªŒ (Ablation Studies)ï¼Œç³»ç»Ÿåˆ†æäº†è¿™äº›åœ¨ä¸ç›¸å…³æç¤ºè¯ä¸‹è¡¨ç°å‡ºé«˜åº¦ç›¸ä¼¼æ€§çš„å›¾åƒç‰¹å¾ã€‚ç ”ç©¶å›¢é˜Ÿé€šè¿‡å¯¹è¶…è¿‡ 75 ä¸‡å¼ å›¾åƒçš„è®¡ç®—åˆ†æï¼Œè¯å®äº†è·¨è¶Šä¸åŒæç¤ºè¯çš„é»˜è®¤å›¾åƒå…·æœ‰æ˜¾è‘—çš„ä¸€è‡´æ€§ã€‚æ­¤å¤–ï¼Œè¯¥ç ”ç©¶è¿˜é€šè¿‡åœ¨çº¿ç”¨æˆ·è°ƒç ”æ¢è®¨äº†é»˜è®¤å›¾åƒå¯¹ç”¨æˆ·æ»¡æ„åº¦çš„å½±å“ï¼Œå¹¶æ­ç¤ºäº†å…¶åœ¨åˆ›ä½œå®è·µä¸­çš„æŒ‘æˆ˜ã€‚è¯¥å·¥ä½œä¸ºç†è§£ TTI æ¨¡å‹ä¸­çš„é»˜è®¤ç”Ÿæˆæœºåˆ¶å¥ å®šäº†åŸºç¡€ï¼Œå¹¶ä¸ºæœªæ¥çš„æç¤ºè¯å·¥ç¨‹ (Prompt Engineering) ä¸æ¨¡å‹ä¼˜åŒ–æ–¹å‘æä¾›äº†é‡è¦å‚è€ƒã€‚",
      "categories": [
        "cs.HC",
        "cs.AI"
      ],
      "primary_category": "cs.HC",
      "comment": "31 pages, 10 figures",
      "pdf_url": "https://arxiv.org/pdf/2505.09166v5",
      "published_date": "2025-05-14 05:59:23 UTC",
      "updated_date": "2025-12-22 09:17:43 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T08:44:41.235887+00:00"
    },
    {
      "arxiv_id": "2505.09160v2",
      "title": "A Multi-Task Foundation Model for Wireless Channel Representation Using Contrastive and Masked Autoencoder Learning",
      "title_zh": "åŸºäºå¯¹æ¯”å­¦ä¹ ä¸æ©ç è‡ªç¼–ç å­¦ä¹ çš„æ— çº¿ä¿¡é“è¡¨ç¤ºå¤šä»»åŠ¡åŸºç¡€æ¨¡å‹",
      "authors": [
        "Berkay Guler",
        "Giovanni Geraci",
        "Hamid Jafarkhani"
      ],
      "abstract": "Current applications of self-supervised learning to wireless channel representation often borrow paradigms developed for text and image processing, without fully addressing the unique characteristics and constraints of wireless communications. To bridge this gap, we introduce ContraWiMAE, Wireless Contrastive Masked Autoencoder, a transformer-based foundation model that unifies masked reconstruction and masked contrastive learning for wireless channel representation. Our key innovation is a new wireless-inspired contrastive objective that exploits the inherent characteristics of wireless environment, including noise, fading, and partial observability, as natural augmentation. Through extensive evaluation on unseen scenarios and conditions, we demonstrate our method's effectiveness in multiple downstream tasks, including cross-frequency beam selection, line-of-sight detection, and channel estimation. ContraWiMAE exhibits superior linear separability and adaptability in diverse wireless environments, demonstrating exceptional data efficiency and competitive performance compared with supervised baselines under challenging conditions. Comparative evaluations against a state-of-the-art wireless channel foundation model confirm the superior performance and data efficiency of our approach, highlighting its potential as a powerful baseline for future research in self-supervised wireless channel representation learning. To foster further work in this direction, we release the model weights and training pipeline for ContraWiMAE.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ç°æœ‰æ— çº¿ä¿¡é“è¡¨å¾ä¸­è‡ªç›‘ç£å­¦ä¹ (Self-supervised learning)æœªèƒ½å……åˆ†è€ƒè™‘æ— çº¿é€šä¿¡ç‹¬ç‰¹ç‰¹æ€§çš„é—®é¢˜ï¼Œæå‡ºäº†ContraWiMAEã€‚è¿™æ˜¯ä¸€ä¸ªåŸºäºTransformerçš„åŸºç¡€æ¨¡å‹(Foundation model)ï¼Œé€šè¿‡ç»Ÿä¸€æ©ç é‡å»º(Masked reconstruction)ä¸æ©ç å¯¹æ¯”å­¦ä¹ (Masked contrastive learning)æ¥å®ç°é«˜æ•ˆçš„ä¿¡é“è¡¨å¾ã€‚å…¶æ ¸å¿ƒåˆ›æ–°åœ¨äºå¼•å…¥äº†å—æ— çº¿é€šä¿¡å¯å‘çš„æ–°å‹å¯¹æ¯”ç›®æ ‡ï¼Œå°†å™ªå£°(Noise)ã€è¡°è½(Fading)å’Œéƒ¨åˆ†å¯è§‚æµ‹æ€§(Partial observability)è§†ä¸ºè‡ªç„¶æ•°æ®å¢å¼ºæ‰‹æ®µã€‚åœ¨è·¨é¢‘ç‡æ³¢æŸé€‰æ‹©(Cross-frequency beam selection)ã€è§†è·æ£€æµ‹(Line-of-sight detection)å’Œä¿¡é“ä¼°è®¡(Channel estimation)ç­‰ä»»åŠ¡ä¸­çš„è¯„ä¼°è¡¨æ˜ï¼Œè¯¥æ¨¡å‹åœ¨æœªè§åœºæ™¯ä¸‹å…·æœ‰æä½³çš„æ³›åŒ–èƒ½åŠ›ã€‚å®éªŒè¯æ˜ContraWiMAEåœ¨æ•°æ®æ•ˆç‡ã€çº¿æ€§å¯åˆ†æ€§å’Œé€‚åº”æ€§æ–¹é¢ä¼˜äºç°æœ‰çš„æœ€å…ˆè¿›æ— çº¿ä¿¡é“åŸºç¡€æ¨¡å‹åŠæœ‰ç›‘ç£åŸºå‡†ã€‚ç›®å‰è¯¥ç ”ç©¶å·²å…¬å¼€å‘å¸ƒæ¨¡å‹æƒé‡å’Œè®­ç»ƒç®¡çº¿ï¼Œä¸ºæœªæ¥æ— çº¿é€šä¿¡é¢†åŸŸçš„è‡ªç›‘ç£å­¦ä¹ ç ”ç©¶æä¾›äº†å¼ºæœ‰åŠ›çš„åŸºçº¿å‚è€ƒã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.IT",
        "cs.NI",
        "eess.SP"
      ],
      "primary_category": "cs.LG",
      "comment": "- 17 pages, 7 figures, 5 tables - Submitted to IEEE JSAC Large AI Models for Future Wireless Communication Systems - Some of the results will appear in NeurIPS 2025, AI4NextG Workshop - This version is an extensive improvement in all aspects over the previous version with the same title - Dataset and implementation: https://github.com/BerkIGuler/WirelessContrastiveMaskedLearning",
      "pdf_url": "https://arxiv.org/pdf/2505.09160v2",
      "published_date": "2025-05-14 05:45:22 UTC",
      "updated_date": "2025-10-22 00:52:25 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T08:45:12.021677+00:00"
    },
    {
      "arxiv_id": "2505.09142v1",
      "title": "ELIS: Efficient LLM Iterative Scheduling System with Response Length Predictor",
      "title_zh": "ELISï¼šåŸºäºå“åº”é•¿åº¦é¢„æµ‹å™¨çš„é«˜æ•ˆå¤§è¯­è¨€æ¨¡å‹è¿­ä»£è°ƒåº¦ç³»ç»Ÿ",
      "authors": [
        "Seungbeom Choi",
        "Jeonghoe Goo",
        "Eunjoo Jeon",
        "Mingyu Yang",
        "Minsung Jang"
      ],
      "abstract": "We propose ELIS, a serving system for Large Language Models (LLMs) featuring an Iterative Shortest Remaining Time First (ISRTF) scheduler designed to efficiently manage inference tasks with the shortest remaining tokens. Current LLM serving systems often employ a first-come-first-served scheduling strategy, which can lead to the \"head-of-line blocking\" problem. To overcome this limitation, it is necessary to predict LLM inference times and apply a shortest job first scheduling strategy. However, due to the auto-regressive nature of LLMs, predicting the inference latency is challenging. ELIS addresses this challenge by training a response length predictor for LLMs using the BGE model, an encoder-based state-of-the-art model. Additionally, we have devised the ISRTF scheduling strategy, an optimization of shortest remaining time first tailored to existing LLM iteration batching. To evaluate our work in an industrial setting, we simulate streams of requests based on our study of real-world user LLM serving trace records. Furthermore, we implemented ELIS as a cloud-native scheduler system on Kubernetes to evaluate its performance in production environments. Our experimental results demonstrate that ISRTF reduces the average job completion time by up to 19.6%.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† ELISï¼Œä¸€ç§é«˜æ•ˆçš„å¤§è¯­è¨€æ¨¡å‹ (LLMs) è¿­ä»£è°ƒåº¦ç³»ç»Ÿï¼Œæ—¨åœ¨è§£å†³ç°æœ‰ç³»ç»Ÿé‡‡ç”¨å…ˆåˆ°å…ˆå¾—ç­–ç•¥æ—¶äº§ç”Ÿçš„â€œé˜Ÿå¤´é˜»å¡â€ (head-of-line blocking) é—®é¢˜ã€‚ä¸ºäº†å®ç° è¿­ä»£æœ€çŸ­å‰©ä½™æ—¶é—´ä¼˜å…ˆ (Iterative Shortest Remaining Time First, ISRTF) ç­–ç•¥ï¼ŒELIS åˆ©ç”¨ BGE æ¨¡å‹è®­ç»ƒäº†ä¸€ä¸ªå“åº”é•¿åº¦é¢„æµ‹å™¨ (response length predictor)ï¼ŒæˆåŠŸå…‹æœäº†å¤§è¯­è¨€æ¨¡å‹è‡ªå›å½’ç‰¹æ€§å¯¼è‡´çš„æ¨ç†å»¶è¿Ÿé¢„æµ‹éš¾é¢˜ã€‚è¯¥ç³»ç»Ÿé’ˆå¯¹ç°æœ‰çš„è¿­ä»£æ‰¹å¤„ç† (iteration batching) æœºåˆ¶è¿›è¡Œäº†ä¸“é—¨çš„ç®—æ³•ä¼˜åŒ–ï¼Œå¹¶åœ¨ Kubernetes äº‘åŸç”Ÿç¯å¢ƒä¸­å®ç°äº†éƒ¨ç½²ä¸æ€§èƒ½è¯„ä¼°ã€‚åŸºäºçœŸå®ç”¨æˆ·è¿½è¸ªæ•°æ®çš„å®éªŒç»“æœæ˜¾ç¤ºï¼ŒELIS èƒ½å¤Ÿæ˜¾è‘—æå‡å¤„ç†æ•ˆç‡ï¼Œå°†å¹³å‡ä½œä¸šå®Œæˆæ—¶é—´ (average job completion time) å‡å°‘é«˜è¾¾ 19.6%ã€‚è¿™ä¸€æˆæœä¸ºæå‡ç”Ÿäº§ç¯å¢ƒä¸‹å¤§è¯­è¨€æ¨¡å‹æœåŠ¡çš„å“åº”é€Ÿåº¦å’Œèµ„æºåˆ©ç”¨ç‡æä¾›äº†æœ‰æ•ˆçš„æŠ€æœ¯æ”¯æ’‘ã€‚",
      "categories": [
        "cs.DC",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.DC",
      "comment": "13 pages, 5 figures. Cloud-native LLM scheduling system with latency-aware inference optimization",
      "pdf_url": "https://arxiv.org/pdf/2505.09142v1",
      "published_date": "2025-05-14 04:50:00 UTC",
      "updated_date": "2025-05-14 04:50:00 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T08:45:09.957063+00:00"
    },
    {
      "arxiv_id": "2505.09131v3",
      "title": "Fair Clustering via Alignment",
      "title_zh": "åŸºäºå¯¹é½çš„å…¬å¹³èšç±»",
      "authors": [
        "Kunwoong Kim",
        "Jihu Lee",
        "Sangchul Park",
        "Yongdai Kim"
      ],
      "abstract": "Algorithmic fairness in clustering aims to balance the proportions of instances assigned to each cluster with respect to a given sensitive attribute. While recently developed fair clustering algorithms optimize clustering objectives under specific fairness constraints, their inherent complexity or approximation often results in suboptimal clustering utility or numerical instability in practice. To resolve these limitations, we propose a new fair clustering algorithm based on a novel decomposition of the fair $K$-means clustering objective function. The proposed algorithm, called Fair Clustering via Alignment (FCA), operates by alternately (i) finding a joint probability distribution to align the data from different protected groups, and (ii) optimizing cluster centers in the aligned space. A key advantage of FCA is that it theoretically guarantees approximately optimal clustering utility for any given fairness level without complex constraints, thereby enabling high-utility fair clustering in practice. Experiments show that FCA outperforms existing methods by (i) attaining a superior trade-off between fairness level and clustering utility, and (ii) achieving near-perfect fairness without numerical instability.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹èšç±»ä¸­çš„ç®—æ³•å…¬å¹³æ€§(algorithmic fairness)é—®é¢˜ï¼ŒæŒ‡å‡ºç›®å‰å…¬å¹³èšç±»ç®—æ³•åœ¨ç‰¹å®šçº¦æŸä¸‹å¸¸å¯¼è‡´å­ä¼˜çš„èšç±»æ•ˆç”¨(clustering utility)æˆ–æ•°å€¼ä¸ç¨³å®šæ€§ã€‚ä¸ºæ­¤ï¼Œä½œè€…æå‡ºäº†ä¸€ç§åä¸ºFCA (Fair Clustering via Alignment) çš„æ–°ç®—æ³•ï¼Œè¯¥ç®—æ³•åŸºäºå¯¹å…¬å¹³ $K$-means ç›®æ ‡å‡½æ•°çš„åˆ›æ–°åˆ†è§£ã€‚FCA é€šè¿‡äº¤æ›¿å¯»æ‰¾è”åˆæ¦‚ç‡åˆ†å¸ƒ(joint probability distribution)ä»¥å¯¹é½æ¥è‡ªä¸åŒå—ä¿æŠ¤ç¾¤ä½“çš„æ•°æ®ï¼Œå¹¶åœ¨å¯¹é½åçš„ç©ºé—´ä¸­ä¼˜åŒ–èšç±»ä¸­å¿ƒã€‚è¯¥ç®—æ³•çš„å…³é”®ä¼˜åŠ¿åœ¨äºå…¶åœ¨ä¸ä¾èµ–å¤æ‚çº¦æŸçš„å‰æä¸‹ï¼Œèƒ½ä¸ºä»»ä½•ç»™å®šçš„å…¬å¹³æ€§æ°´å¹³æä¾›è¿‘ä¼¼æœ€ä¼˜èšç±»æ•ˆç”¨çš„ç†è®ºä¿è¯ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒFCA åœ¨å…¬å¹³æ€§ä¸æ•ˆç”¨çš„æƒè¡¡ä¸Šæ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œå¹¶èƒ½åœ¨ä¿æŒæ•°å€¼ç¨³å®šæ€§çš„åŒæ—¶å®ç°æ¥è¿‘å®Œç¾çš„å…¬å¹³æ€§ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "stat.ML"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.09131v3",
      "published_date": "2025-05-14 04:29:09 UTC",
      "updated_date": "2025-10-23 02:21:29 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T08:45:14.762301+00:00"
    },
    {
      "arxiv_id": "2505.09129v1",
      "title": "WSCIF: A Weakly-Supervised Color Intelligence Framework for Tactical Anomaly Detection in Surveillance Keyframes",
      "title_zh": "WSCIFï¼šé¢å‘ç›‘æ§å…³é”®å¸§æˆ˜æœ¯å¼‚å¸¸æ£€æµ‹çš„å¼±ç›‘ç£è‰²å½©æ™ºèƒ½æ¡†æ¶",
      "authors": [
        "Wei Meng"
      ],
      "abstract": "The deployment of traditional deep learning models in high-risk security tasks in an unlabeled, data-non-exploitable video intelligence environment faces significant challenges. In this paper, we propose a lightweight anomaly detection framework based on color features for surveillance video clips in a high sensitivity tactical mission, aiming to quickly identify and interpret potential threat events under resource-constrained and data-sensitive conditions. The method fuses unsupervised KMeans clustering with RGB channel histogram modeling to achieve composite detection of structural anomalies and color mutation signals in key frames. The experiment takes an operation surveillance video occurring in an African country as a research sample, and successfully identifies multiple highly anomalous frames related to high-energy light sources, target presence, and reflective interference under the condition of no access to the original data. The results show that this method can be effectively used for tactical assassination warning, suspicious object screening and environmental drastic change monitoring with strong deployability and tactical interpretation value. The study emphasizes the importance of color features as low semantic battlefield signal carriers, and its battlefield intelligent perception capability will be further extended by combining graph neural networks and temporal modeling in the future.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†WSCIFï¼Œä¸€ç§ç”¨äºç›‘æ§å…³é”®å¸§æˆ˜æœ¯å¼‚å¸¸æ£€æµ‹çš„å¼±ç›‘ç£è‰²å½©æ™ºèƒ½æ¡†æ¶(Weakly-Supervised Color Intelligence Framework)ï¼Œæ—¨åœ¨è§£å†³èµ„æºå—é™åŠæ•°æ®æ•æ„Ÿç¯å¢ƒä¸‹çš„å®‰å…¨ä»»åŠ¡æŒ‘æˆ˜ã€‚è¯¥æ–¹æ³•é€šè¿‡èåˆæ— ç›‘ç£KMeansèšç±»ä¸RGBé€šé“ç›´æ–¹å›¾å»ºæ¨¡(RGB channel histogram modeling)ï¼Œå®ç°äº†å¯¹å…³é”®å¸§ä¸­ç»“æ„å¼‚å¸¸ä¸è‰²å½©çªå˜ä¿¡å·çš„å¤åˆæ£€æµ‹ã€‚å®éªŒä»¥æŸéæ´²å›½å®¶çš„ä½œæˆ˜ç›‘æ§è§†é¢‘ä¸ºæ ·æœ¬ï¼Œåœ¨æ— æ³•ç›´æ¥åˆ©ç”¨åŸå§‹æ•°æ®çš„æƒ…å†µä¸‹ï¼ŒæˆåŠŸè¯†åˆ«å‡ºæ¶‰åŠé«˜èƒ½å…‰æºã€ç›®æ ‡å‡ºç°åŠåå°„å¹²æ‰°çš„å¤šç±»é«˜åº¦å¼‚å¸¸å¸§ã€‚ç ”ç©¶ç»“æœè¯æ˜ï¼Œè¯¥æ¡†æ¶åœ¨æˆ˜æœ¯æš—æ€é¢„è­¦(tactical assassination warning)ã€ç–‘ä¼¼ç›®æ ‡ç­›é€‰åŠç¯å¢ƒå‰§å˜ç›‘æµ‹ä¸­å…·æœ‰æå¼ºçš„éƒ¨ç½²å¯è¡Œæ€§ä¸æˆ˜æœ¯è§£é‡Šä»·å€¼ã€‚æ­¤å¤–ï¼Œè¯¥ç ”ç©¶å¼ºè°ƒäº†è‰²å½©ç‰¹å¾ä½œä¸ºä½è¯­ä¹‰æˆ˜åœºä¿¡å·è½½ä½“çš„é‡è¦æ€§ï¼Œå¹¶è®¡åˆ’æœªæ¥ç»“åˆå›¾ç¥ç»ç½‘ç»œ(graph neural networks)ä¸æ—¶åºå»ºæ¨¡è¿›ä¸€æ­¥æå‡æ„ŸçŸ¥èƒ½åŠ›ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "17 pages, 3 figures, 3 tables. The paper proposes a lightweight weakly-supervised color intelligence model for tactical video anomaly detection, tested on anonymized African surveillance data",
      "pdf_url": "https://arxiv.org/pdf/2505.09129v1",
      "published_date": "2025-05-14 04:24:37 UTC",
      "updated_date": "2025-05-14 04:24:37 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T08:45:43.545611+00:00"
    },
    {
      "arxiv_id": "2505.09115v1",
      "title": "PreCare: Designing AI Assistants for Advance Care Planning (ACP) to Enhance Personal Value Exploration, Patient Knowledge, and Decisional Confidence",
      "title_zh": "PreCareï¼šé¢å‘é¢„å…ˆæŠ¤ç†è®¡åˆ’ (ACP) çš„äººå·¥æ™ºèƒ½åŠ©æ‰‹è®¾è®¡ï¼Œæ—¨åœ¨å¢å¼ºä¸ªäººä»·å€¼è§‚æ¢ç´¢ã€æ‚£è€…çŸ¥è¯†ä¸å†³ç­–ä¿¡å¿ƒ",
      "authors": [
        "Yu Lun Hsu",
        "Yun-Rung Chou",
        "Chiao-Ju Chang",
        "Yu-Cheng Chang",
        "Zer-Wei Lee",
        "Rokas GipiÅ¡kis",
        "Rachel Li",
        "Chih-Yuan Shih",
        "Jen-Kuei Peng",
        "Hsien-Liang Huang",
        "Jaw-Shiun Tsai",
        "Mike Y. Chen"
      ],
      "abstract": "Advance Care Planning (ACP) allows individuals to specify their preferred end-of-life life-sustaining treatments before they become incapacitated by injury or terminal illness (e.g., coma, cancer, dementia). While online ACP offers high accessibility, it lacks key benefits of clinical consultations, including personalized value exploration, immediate clarification of decision consequences. To bridge this gap, we conducted two formative studies: 1) shadowed and interviewed 3 ACP teams consisting of physicians, nurses, and social workers (18 patients total), and 2) interviewed 14 users of ACP websites. Building on these insights, we designed PreCare in collaboration with 6 ACP professionals. PreCare is a website with 3 AI-driven assistants designed to guide users through exploring personal values, gaining ACP knowledge, and supporting informed decision-making. A usability study (n=12) showed that PreCare achieved a System Usability Scale (SUS) rating of excellent. A comparative evaluation (n=12) showed that PreCare's AI assistants significantly improved exploration of personal values, knowledge, and decisional confidence, and was preferred by 92% of participants.",
      "tldr_zh": "è¯¥ç ”ç©¶è®¾è®¡å¹¶å¼€å‘äº†PreCareï¼Œè¿™æ˜¯ä¸€ä¸ªé›†æˆäº†ä¸‰ä¸ªAIé©±åŠ¨åŠ©æ‰‹çš„ç”Ÿå‰é¢„å˜±ï¼ˆAdvance Care Planning, ACPï¼‰è¾…åŠ©ç³»ç»Ÿï¼Œæ—¨åœ¨å¢å¼ºä¸ªäººä»·å€¼æ¢ç´¢ã€æ‚£è€…çŸ¥è¯†æ°´å¹³åŠå†³ç­–ä¿¡å¿ƒã€‚é’ˆå¯¹åœ¨çº¿ACPå·¥å…·ç¼ºä¹ä¸´åºŠå’¨è¯¢ä¸­ä¸ªæ€§åŒ–æŒ‡å¯¼å’Œå³æ—¶æ¾„æ¸…çš„é—®é¢˜ï¼Œç ”ç©¶å›¢é˜Ÿé€šè¿‡å¯¹ACPä¸“ä¸šå›¢é˜Ÿå’Œç½‘ç«™ç”¨æˆ·çš„å½¢æˆæ€§ç ”ç©¶ï¼Œåœ¨ä¸“ä¸šäººå£«åä½œä¸‹å®Œæˆäº†ç³»ç»Ÿè®¾è®¡ã€‚PreCareé€šè¿‡AIåŠ©æ‰‹å¼•å¯¼ç”¨æˆ·æ·±å…¥æ¢ç´¢ä¸ªäººä»·å€¼è§‚å¹¶æŒæ¡å…³é”®çš„ACPçŸ¥è¯†ï¼Œä»è€Œæ”¯æŒå…¶åšå‡ºçŸ¥æƒ…çš„åŒ»ç–—å†³ç­–ã€‚å¯ç”¨æ€§æµ‹è¯•æ˜¾ç¤ºè¯¥ç³»ç»Ÿåœ¨ç³»ç»Ÿå¯ç”¨æ€§é‡è¡¨ï¼ˆSystem Usability Scale, SUSï¼‰ä¸­è¡¨ç°ä¼˜ç§€ã€‚æ¯”è¾ƒè¯„ä¼°è¿›ä¸€æ­¥è¯å®ï¼ŒPreCareæ˜¾è‘—æå‡äº†ç”¨æˆ·çš„ä»·å€¼æ¢ç´¢æ·±åº¦å’Œå†³ç­–ä¿¡å¿ƒï¼Œå¹¶è·å¾—äº†92%å‚ä¸è€…çš„é¦–é€‰åå¥½ã€‚",
      "categories": [
        "cs.HC",
        "cs.AI"
      ],
      "primary_category": "cs.HC",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.09115v1",
      "published_date": "2025-05-14 03:53:35 UTC",
      "updated_date": "2025-05-14 03:53:35 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T08:45:35.438206+00:00"
    },
    {
      "arxiv_id": "2505.09114v1",
      "title": "Beyond the Known: Decision Making with Counterfactual Reasoning Decision Transformer",
      "title_zh": "è¶…è¶Šå·²çŸ¥ï¼šåŸºäºåäº‹å®æ¨ç†å†³ç­– Transformer çš„å†³ç­–",
      "authors": [
        "Minh Hoang Nguyen",
        "Linh Le Pham Van",
        "Thommen George Karimpanal",
        "Sunil Gupta",
        "Hung Le"
      ],
      "abstract": "Decision Transformers (DT) play a crucial role in modern reinforcement learning, leveraging offline datasets to achieve impressive results across various domains. However, DT requires high-quality, comprehensive data to perform optimally. In real-world applications, the lack of training data and the scarcity of optimal behaviours make training on offline datasets challenging, as suboptimal data can hinder performance. To address this, we propose the Counterfactual Reasoning Decision Transformer (CRDT), a novel framework inspired by counterfactual reasoning. CRDT enhances DT ability to reason beyond known data by generating and utilizing counterfactual experiences, enabling improved decision-making in unseen scenarios. Experiments across Atari and D4RL benchmarks, including scenarios with limited data and altered dynamics, demonstrate that CRDT outperforms conventional DT approaches. Additionally, reasoning counterfactually allows the DT agent to obtain stitching abilities, combining suboptimal trajectories, without architectural modifications. These results highlight the potential of counterfactual reasoning to enhance reinforcement learning agents' performance and generalization capabilities.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ Decision Transformers (DT) åœ¨é¢å¯¹ä½è´¨é‡æˆ–ç¨€ç¼ºçš„ç¦»çº¿å¼ºåŒ–å­¦ä¹  (offline reinforcement learning) æ•°æ®æ—¶è¡¨ç°å—é™çš„é—®é¢˜ï¼Œæå‡ºäº† Counterfactual Reasoning Decision Transformer (CRDT) æ¡†æ¶ã€‚CRDT å—åˆ°åäº‹å®æ¨ç† (counterfactual reasoning) çš„å¯å‘ï¼Œé€šè¿‡ç”Ÿæˆå’Œåˆ©ç”¨åäº‹å®ç»éªŒ (counterfactual experiences)ï¼Œæ˜¾è‘—å¢å¼ºäº†æ™ºèƒ½ä½“åœ¨å·²çŸ¥æ•°æ®ä¹‹å¤–çš„æ¨ç†èƒ½åŠ›ã€‚è¯¥æ–¹æ³•ä½¿æ¨¡å‹èƒ½å¤Ÿåœ¨æœªè§åœºæ™¯ä¸­å®ç°æ›´ä¼˜çš„å†³ç­–ï¼Œå¹¶èµ‹äºˆäº†æ™ºèƒ½ä½“å°†æ¬¡ä¼˜è½¨è¿¹è¿›è¡Œâ€œæ‹¼æ¥â€ (stitching) çš„èƒ½åŠ›ï¼Œä¸”æ— éœ€å¯¹ç°æœ‰æ¶æ„è¿›è¡Œä»»ä½•ä¿®æ”¹ã€‚åœ¨ Atari å’Œ D4RL åŸºå‡†æµ‹è¯•ï¼ˆåŒ…æ‹¬æœ‰é™æ•°æ®å’Œç¯å¢ƒåŠ¨æ€å˜åŒ–çš„æƒ…å†µï¼‰ä¸­ï¼Œå®éªŒç»“æœè¯æ˜ CRDT çš„æ€§èƒ½ä¼˜äºä¼ ç»Ÿçš„ DT æ–¹æ³•ã€‚è¯¥ç ”ç©¶æ­ç¤ºäº†åäº‹å®æ¨ç†åœ¨æå‡å¼ºåŒ–å­¦ä¹ æ™ºèƒ½ä½“æ³›åŒ–èƒ½åŠ›å’Œå†³ç­–æ€§èƒ½æ–¹é¢çš„å·¨å¤§æ½œåŠ›ã€‚",
      "categories": [
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.09114v1",
      "published_date": "2025-05-14 03:45:16 UTC",
      "updated_date": "2025-05-14 03:45:16 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T08:45:25.873956+00:00"
    },
    {
      "arxiv_id": "2505.09108v1",
      "title": "Air-Ground Collaboration for Language-Specified Missions in Unknown Environments",
      "title_zh": "æœªçŸ¥ç¯å¢ƒä¸‹é¢å‘è¯­è¨€æŒ‡å®šä»»åŠ¡çš„ç©ºåœ°ååŒ",
      "authors": [
        "Fernando Cladera",
        "Zachary Ravichandran",
        "Jason Hughes",
        "Varun Murali",
        "Carlos Nieto-Granda",
        "M. Ani Hsieh",
        "George J. Pappas",
        "Camillo J. Taylor",
        "Vijay Kumar"
      ],
      "abstract": "As autonomous robotic systems become increasingly mature, users will want to specify missions at the level of intent rather than in low-level detail. Language is an expressive and intuitive medium for such mission specification. However, realizing language-guided robotic teams requires overcoming significant technical hurdles. Interpreting and realizing language-specified missions requires advanced semantic reasoning. Successful heterogeneous robots must effectively coordinate actions and share information across varying viewpoints. Additionally, communication between robots is typically intermittent, necessitating robust strategies that leverage communication opportunities to maintain coordination and achieve mission objectives. In this work, we present a first-of-its-kind system where an unmanned aerial vehicle (UAV) and an unmanned ground vehicle (UGV) are able to collaboratively accomplish missions specified in natural language while reacting to changes in specification on the fly. We leverage a Large Language Model (LLM)-enabled planner to reason over semantic-metric maps that are built online and opportunistically shared between an aerial and a ground robot. We consider task-driven navigation in urban and rural areas. Our system must infer mission-relevant semantics and actively acquire information via semantic mapping. In both ground and air-ground teaming experiments, we demonstrate our system on seven different natural-language specifications at up to kilometer-scale navigation.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§é¦–åˆ›çš„ç©ºåœ°ååŒ(Air-Ground Collaboration)ç³»ç»Ÿï¼Œä½¿æ— äººæœº(UAV)å’Œåœ°é¢æ— äººè½¦(UGV)èƒ½å¤Ÿåœ¨æœªçŸ¥ç¯å¢ƒä¸­åä½œå®Œæˆç”±è‡ªç„¶è¯­è¨€æŒ‡å®šçš„å¤æ‚ä»»åŠ¡ã€‚ç³»ç»Ÿåˆ©ç”¨å¤§è¯­è¨€æ¨¡å‹é©±åŠ¨çš„è§„åˆ’å™¨(LLM-enabled planner)å¯¹åœ¨çº¿æ„å»ºçš„è¯­ä¹‰-åº¦é‡åœ°å›¾(semantic-metric maps)è¿›è¡Œè¯­ä¹‰æ¨ç†ï¼Œå¹¶æ”¯æŒåœ¨ä»»åŠ¡æ‰§è¡Œè¿‡ç¨‹ä¸­å¯¹æŒ‡ä»¤çš„åŠ¨æ€å˜åŒ–åšå‡ºå®æ—¶ååº”ã€‚é€šè¿‡åœ¨ç©ºä¸­å’Œåœ°é¢æœºå™¨äººä¹‹é—´æœºä¼šæ€§åœ°å…±äº«ä¿¡æ¯ï¼Œè¯¥æ¡†æ¶æœ‰æ•ˆå…‹æœäº†è·¨è§†è§’åè°ƒã€è¯­ä¹‰ç†è§£ä»¥åŠé—´æ­‡æ€§é€šä¿¡ç­‰æŠ€æœ¯éšœç¢ã€‚åœ¨åŸå¸‚å’Œå†œæ‘åœ°åŒºçš„å®åœ°å®éªŒè¡¨æ˜ï¼Œè¯¥ç³»ç»Ÿèƒ½å¤ŸæˆåŠŸæ‰§è¡Œä¸ƒç§ä¸åŒçš„è‡ªç„¶è¯­è¨€è§„èŒƒï¼Œå¹¶å®ç°äº†é«˜è¾¾å…¬é‡Œçº§çš„ä»»åŠ¡é©±åŠ¨å¯¼èˆªã€‚è¯¥å·¥ä½œè¯æ˜äº†é€šè¿‡è¯­ä¹‰åˆ¶å›¾ä¸»åŠ¨è·å–ä¿¡æ¯å¹¶è¿›è¡Œå¤šæ™ºèƒ½ä½“ååŒï¼Œæ˜¯å®ç°é«˜å±‚æ„å›¾é©±åŠ¨æœºå™¨äººä»»åŠ¡çš„å…³é”®é€”å¾„ã€‚",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "comment": "19 pages, 24 figures, 7 tables. Submitted to T-FR",
      "pdf_url": "https://arxiv.org/pdf/2505.09108v1",
      "published_date": "2025-05-14 03:33:46 UTC",
      "updated_date": "2025-05-14 03:33:46 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T08:45:47.283105+00:00"
    },
    {
      "arxiv_id": "2505.09091v1",
      "title": "DPN-GAN: Inducing Periodic Activations in Generative Adversarial Networks for High-Fidelity Audio Synthesis",
      "title_zh": "DPN-GANï¼šé€šè¿‡åœ¨ç”Ÿæˆå¯¹æŠ—ç½‘ç»œä¸­å¼•å…¥å‘¨æœŸæ€§æ¿€æ´»å®ç°é«˜ä¿çœŸéŸ³é¢‘åˆæˆ",
      "authors": [
        "Zeeshan Ahmad",
        "Shudi Bao",
        "Meng Chen"
      ],
      "abstract": "In recent years, generative adversarial networks (GANs) have made significant progress in generating audio sequences. However, these models typically rely on bandwidth-limited mel-spectrograms, which constrain the resolution of generated audio sequences, and lead to mode collapse during conditional generation. To address this issue, we propose Deformable Periodic Network based GAN (DPN-GAN), a novel GAN architecture that incorporates a kernel-based periodic ReLU activation function to induce periodic bias in audio generation. This innovative approach enhances the model's ability to capture and reproduce intricate audio patterns. In particular, our proposed model features a DPN module for multi-resolution generation utilizing deformable convolution operations, allowing for adaptive receptive fields that improve the quality and fidelity of the synthetic audio. Additionally, we enhance the discriminator network using deformable convolution to better distinguish between real and generated samples, further refining the audio quality. We trained two versions of the model: DPN-GAN small (38.67M parameters) and DPN-GAN large (124M parameters). For evaluation, we use five different datasets, covering both speech synthesis and music generation tasks, to demonstrate the efficiency of the DPN-GAN. The experimental results demonstrate that DPN-GAN delivers superior performance on both out-of-distribution and noisy data, showcasing its robustness and adaptability. Trained across various datasets, DPN-GAN outperforms state-of-the-art GAN architectures on standard evaluation metrics, and exhibits increased robustness in synthesized audio.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†DPN-GANï¼Œæ—¨åœ¨è§£å†³ç”Ÿæˆå¯¹æŠ—ç½‘ç»œ(GANs)åœ¨éŸ³é¢‘åˆæˆä¸­å› ä¾èµ–å¸¦å®½å—é™çš„æ¢…å°”è°±å›¾(mel-spectrograms)è€Œå¯¼è‡´çš„åˆ†è¾¨ç‡å—é™å’Œæ¨¡å¼å´©æºƒ(mode collapse)é—®é¢˜ã€‚è¯¥æ¶æ„å¼•å…¥äº†åŸºäºå†…æ ¸çš„å‘¨æœŸæ€§ReLUæ¿€æ´»å‡½æ•°ä»¥è¯±å¯¼å‘¨æœŸæ€§åå·®(periodic bias)ï¼Œæ˜¾è‘—å¢å¼ºäº†æ¨¡å‹æ•è·å¤æ‚éŸ³é¢‘æ¨¡å¼çš„èƒ½åŠ›ã€‚å…¶æ ¸å¿ƒDPNæ¨¡å—åˆ©ç”¨å¯å˜å½¢å·ç§¯(deformable convolution)æ“ä½œè¿›è¡Œå¤šåˆ†è¾¨ç‡ç”Ÿæˆï¼Œé€šè¿‡è‡ªé€‚åº”æ„Ÿå—é‡(adaptive receptive fields)æå‡äº†åˆæˆéŸ³é¢‘çš„è´¨é‡ä¸å¿ å®åº¦ã€‚æ­¤å¤–ï¼Œç ”ç©¶è¿˜é€šè¿‡å¯å˜å½¢å·ç§¯å¢å¼ºäº†åˆ¤åˆ«å™¨ç½‘ç»œï¼Œä½¿å…¶èƒ½æ›´æœ‰æ•ˆåœ°åŒºåˆ†çœŸå®æ ·æœ¬ä¸ç”Ÿæˆæ ·æœ¬ã€‚å®éªŒåœ¨æ¶‰åŠè¯­éŸ³å’ŒéŸ³ä¹çš„äº”ä¸ªæ•°æ®é›†ä¸Šæµ‹è¯•äº†ä¸åŒå‚æ•°è§„æ¨¡çš„æ¨¡å‹ï¼Œç»“æœæ˜¾ç¤ºDPN-GANåœ¨åˆ†å¸ƒå¤–(out-of-distribution)å’Œå™ªå£°æ•°æ®ä¸Šå‡è¡¨ç°å‡ºå“è¶Šçš„é²æ£’æ€§ã€‚æœ€ç»ˆï¼ŒDPN-GANåœ¨å¤šé¡¹æ ‡å‡†è¯„ä¼°æŒ‡æ ‡ä¸Šè¶…è¶Šäº†ç°æœ‰çš„æœ€å…ˆè¿›GANæ¶æ„ï¼Œè¯æ˜äº†å…¶åœ¨é«˜è´¨é‡éŸ³é¢‘åˆæˆé¢†åŸŸçš„æœ‰æ•ˆæ€§ã€‚",
      "categories": [
        "cs.SD",
        "cs.AI",
        "cs.CV",
        "cs.LG",
        "eess.AS"
      ],
      "primary_category": "cs.SD",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.09091v1",
      "published_date": "2025-05-14 02:52:16 UTC",
      "updated_date": "2025-05-14 02:52:16 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T08:45:47.532106+00:00"
    },
    {
      "arxiv_id": "2505.09085v1",
      "title": "Human-like Cognitive Generalization for Large Models via Brain-in-the-loop Supervision",
      "title_zh": "é€šè¿‡è„‘åœ¨ç¯ç›‘ç£å®ç°å¤§æ¨¡å‹çš„ç±»äººè®¤çŸ¥æ³›åŒ–",
      "authors": [
        "Jiaxuan Chen",
        "Yu Qi",
        "Yueming Wang",
        "Gang Pan"
      ],
      "abstract": "Recent advancements in deep neural networks (DNNs), particularly large-scale language models, have demonstrated remarkable capabilities in image and natural language understanding. Although scaling up model parameters with increasing volume of training data has progressively improved DNN capabilities, achieving complex cognitive abilities - such as understanding abstract concepts, reasoning, and adapting to novel scenarios, which are intrinsic to human cognition - remains a major challenge. In this study, we show that brain-in-the-loop supervised learning, utilizing a small set of brain signals, can effectively transfer human conceptual structures to DNNs, significantly enhancing their comprehension of abstract and even unseen concepts. Experimental results further indicate that the enhanced cognitive capabilities lead to substantial performance gains in challenging tasks, including few-shot/zero-shot learning and out-of-distribution recognition, while also yielding highly interpretable concept representations. These findings highlight that human-in-the-loop supervision can effectively augment the complex cognitive abilities of large models, offering a promising pathway toward developing more human-like cognitive abilities in artificial systems.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§åä¸ºBrain-in-the-loopç›‘ç£å­¦ä¹ çš„æ–¹æ³•ï¼Œæ—¨åœ¨é€šè¿‡å¼•å…¥äººç±»å¤§è„‘ä¿¡å·ï¼Œä½¿å¤§å‹æ¨¡å‹è·å¾—ç±»ä¼¼äºäººç±»çš„è®¤çŸ¥æ³›åŒ–èƒ½åŠ›ã€‚è¿™ç§æ–¹æ³•åˆ©ç”¨ä¸€å°ç»„å¤§è„‘ä¿¡å·å°†äººç±»çš„æ¦‚å¿µç»“æ„(conceptual structures)æœ‰æ•ˆåœ°è½¬ç§»åˆ°æ·±åº¦ç¥ç»ç½‘ç»œ(DNNs)ä¸­ï¼Œä»è€Œå¢å¼ºæ¨¡å‹å¯¹æŠ½è±¡æ¦‚å¿µä¹ƒè‡³æœªè§æ¦‚å¿µçš„ç†è§£ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¿™ç§å¢å¼ºçš„è®¤çŸ¥èƒ½åŠ›åœ¨å°‘æ ·æœ¬å­¦ä¹ (few-shot learning)ã€é›¶æ ·æœ¬å­¦ä¹ (zero-shot learning)ä»¥åŠåˆ†å¸ƒå¤–è¯†åˆ«(out-of-distribution recognition)ç­‰æå…·æŒ‘æˆ˜æ€§çš„ä»»åŠ¡ä¸­å¸¦æ¥äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•è¿˜äº§ç”Ÿäº†ä¸€ç³»åˆ—å…·æœ‰é«˜åº¦å¯è§£é‡Šæ€§çš„æ¦‚å¿µè¡¨å¾(concept representations)ã€‚è¿™é¡¹ç ”ç©¶å¼ºè°ƒäº†è„‘ç¯è·¯ç›‘ç£åœ¨å¢å¼ºå¤§æ¨¡å‹å¤æ‚è®¤çŸ¥èƒ½åŠ›æ–¹é¢çš„æœ‰æ•ˆæ€§ï¼Œä¸ºå¼€å‘å…·å¤‡ç±»äººè®¤çŸ¥èƒ½åŠ›çš„äººå·¥æ™ºèƒ½ç³»ç»Ÿæä¾›äº†ä¸€æ¡å……æ»¡å‰æ™¯çš„è·¯å¾„ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.09085v1",
      "published_date": "2025-05-14 02:39:10 UTC",
      "updated_date": "2025-05-14 02:39:10 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T08:45:56.691980+00:00"
    },
    {
      "arxiv_id": "2505.09082v1",
      "title": "CEC-Zero: Chinese Error Correction Solution Based on LLM",
      "title_zh": "CEC-Zeroï¼šåŸºäºå¤§è¯­è¨€æ¨¡å‹çš„ä¸­æ–‡æ–‡æœ¬çº é”™è§£å†³æ–¹æ¡ˆ",
      "authors": [
        "Sophie Zhang",
        "Zhiming Lin"
      ],
      "abstract": "Recent advancements in large language models (LLMs) demonstrate exceptional Chinese text processing capabilities, particularly in Chinese Spelling Correction (CSC). While LLMs outperform traditional BERT-based models in accuracy and robustness, challenges persist in reliability and generalization. This paper proposes CEC-Zero, a novel reinforcement learning (RL) framework enabling LLMs to self-correct through autonomous error strategy learning without external supervision. By integrating RL with LLMs' generative power, the method eliminates dependency on annotated data or auxiliary models. Experiments reveal RL-enhanced LLMs achieve industry-viable accuracy and superior cross-domain generalization, offering a scalable solution for reliability optimization in Chinese NLP applications. This breakthrough facilitates LLM deployment in practical Chinese text correction scenarios while establishing a new paradigm for self-improving language models.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†CEC-Zeroï¼Œè¿™æ˜¯ä¸€ç§åŸºäºå¤§è¯­è¨€æ¨¡å‹(LLMs)çš„åˆ›æ–°ä¸­æ–‡çº é”™è§£å†³æ–¹æ¡ˆï¼Œæ—¨åœ¨è§£å†³ç°æœ‰æ¨¡å‹åœ¨å¯é æ€§å’Œæ³›åŒ–æ€§ä¸Šçš„ä¸è¶³ã€‚CEC-Zeroé‡‡ç”¨äº†å¼ºåŒ–å­¦ä¹ (Reinforcement Learning)æ¡†æ¶ï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿé€šè¿‡è‡ªä¸»çº é”™ç­–ç•¥å­¦ä¹ å®ç°è‡ªæˆ‘ä¿®æ­£ï¼Œå®Œå…¨æ‘†è„±äº†å¯¹å¤–éƒ¨ç›‘ç£æˆ–æ ‡æ³¨æ•°æ®çš„ä¾èµ–ã€‚é€šè¿‡å°†å¼ºåŒ–å­¦ä¹ ä¸LLMsçš„ç”Ÿæˆèƒ½åŠ›æ·±åº¦èåˆï¼Œè¯¥æ–¹æ³•æœ‰æ•ˆæå‡äº†ç³»ç»Ÿå¤„ç†ä¸­æ–‡æ‹¼å†™æ£€æŸ¥(CSC)ç­‰ä»»åŠ¡çš„æ•ˆç‡ã€‚å®éªŒç»“æœè¯æ˜ï¼Œè¯¥æ¡†æ¶ä½¿LLMsè¾¾åˆ°äº†å·¥ä¸šçº§çš„å‡†ç¡®ç‡ï¼Œå¹¶åœ¨è·¨é¢†åŸŸæ³›åŒ–æ–¹é¢è¡¨ç°å“è¶Šã€‚è¿™ä¸€æˆæœä¸ä»…ä¸ºä¸­æ–‡è‡ªç„¶è¯­è¨€å¤„ç†(NLP)åº”ç”¨æä¾›äº†å¯æ‰©å±•çš„ä¼˜åŒ–è·¯å¾„ï¼Œä¹Ÿä¸ºæ„å»ºè‡ªè¿›åŒ–è¯­è¨€æ¨¡å‹å¥ å®šäº†æ–°çš„æŠ€æœ¯èŒƒå¼ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.09082v1",
      "published_date": "2025-05-14 02:35:47 UTC",
      "updated_date": "2025-05-14 02:35:47 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T08:46:09.916902+00:00"
    },
    {
      "arxiv_id": "2505.09081v2",
      "title": "SALM: A Multi-Agent Framework for Language Model-Driven Social Network Simulation",
      "title_zh": "SALMï¼šè¯­è¨€æ¨¡å‹é©±åŠ¨çš„ç¤¾äº¤ç½‘ç»œæ¨¡æ‹Ÿå¤šæ™ºèƒ½ä½“æ¡†æ¶",
      "authors": [
        "Gaurav Koley"
      ],
      "abstract": "Contemporary approaches to agent-based modeling (ABM) of social systems have traditionally emphasized rule-based behaviors, limiting their ability to capture nuanced dynamics by moving beyond predefined rules and leveraging contextual understanding from LMs of human social interaction. This paper presents SALM (Social Agent LM Framework), a novel approach for integrating language models (LMs) into social network simulation that achieves unprecedented temporal stability in multi-agent scenarios. Our primary contributions include: (1) a hierarchical prompting architecture enabling stable simulation beyond 4,000 timesteps while reducing token usage by 73%, (2) an attention-based memory system achieving 80% cache hit rates (95% CI [78%, 82%]) with sub-linear memory growth of 9.5%, and (3) formal bounds on personality stability. Through extensive validation against SNAP ego networks, we demonstrate the first LLM-based framework capable of modeling long-term social phenomena while maintaining empirically validated behavioral fidelity.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†SALMï¼Œä¸€ç§å°†è¯­è¨€æ¨¡å‹(LMs)é›†æˆåˆ°ç¤¾äº¤ç½‘ç»œæ¨¡æ‹Ÿä¸­çš„å¤šæ™ºèƒ½ä½“æ¡†æ¶ï¼Œæ—¨åœ¨å…‹æœä¼ ç»ŸåŸºäºè§„åˆ™çš„ä»£ç†å»ºæ¨¡(ABM)åœ¨æ•æ‰å¤æ‚ç¤¾äº¤åŠ¨æ€å’Œä¸Šä¸‹æ–‡ç†è§£æ–¹é¢çš„å±€é™æ€§ã€‚SALMçš„æ ¸å¿ƒè´¡çŒ®åŒ…æ‹¬ä¸€ç§åˆ†å±‚æç¤ºæ¶æ„(hierarchical prompting architecture)ï¼Œåœ¨å°†tokenä½¿ç”¨é‡é™ä½73%çš„åŒæ—¶ï¼Œå®ç°äº†è¶…è¿‡4000ä¸ªæ—¶é—´æ­¥çš„å“è¶Šä»¿çœŸç¨³å®šæ€§ã€‚æ­¤å¤–ï¼Œè¯¥æ¡†æ¶å¼•å…¥äº†åŸºäºæ³¨æ„åŠ›çš„è®°å¿†ç³»ç»Ÿ(attention-based memory system)ï¼Œå®ç°äº†80%çš„ç¼“å­˜å‘½ä¸­ç‡å’Œä»…ä¸º9.5%çš„äºšçº¿æ€§å†…å­˜å¢é•¿ï¼Œå¹¶ä¸ºä»£ç†çš„äººæ ¼ç¨³å®šæ€§æä¾›äº†å½¢å¼åŒ–è¾¹ç•Œã€‚é€šè¿‡åœ¨SNAPè‡ªæˆ‘ç½‘ç»œ(ego networks)ä¸Šçš„å¹¿æ³›éªŒè¯ï¼ŒSALMå±•ç°äº†ç²¾ç¡®å»ºæ¨¡é•¿æœŸç¤¾äº¤ç°è±¡çš„èƒ½åŠ›ã€‚ä½œä¸ºé¦–ä¸ªç»å®è¯éªŒè¯çš„å¤§è¯­è¨€æ¨¡å‹(LLM)ç¤¾äº¤ä»¿çœŸæ¡†æ¶ï¼Œå®ƒåœ¨ç»´æŒé«˜æ°´å¹³è¡Œä¸ºä¿çœŸåº¦çš„åŒæ—¶ï¼Œå®ç°äº†å‰æ‰€æœªæœ‰çš„æ¨¡æ‹ŸæŒä¹…æ€§ã€‚",
      "categories": [
        "cs.SI",
        "cs.AI",
        "cs.MA"
      ],
      "primary_category": "cs.SI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.09081v2",
      "published_date": "2025-05-14 02:29:46 UTC",
      "updated_date": "2025-09-28 08:32:26 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T08:46:51.015805+00:00"
    },
    {
      "arxiv_id": "2505.09062v1",
      "title": "Variational Prefix Tuning for Diverse and Accurate Code Summarization Using Pre-trained Language Models",
      "title_zh": "åŸºäºé¢„è®­ç»ƒè¯­è¨€æ¨¡å‹çš„å¤šæ ·åŒ–ä¸”å‡†ç¡®ä»£ç æ‘˜è¦å˜åˆ†å‰ç¼€å¾®è°ƒ",
      "authors": [
        "Junda Zhao",
        "Yuliang Song",
        "Eldan Cohen"
      ],
      "abstract": "Recent advancements in source code summarization have leveraged transformer-based pre-trained models, including Large Language Models of Code (LLMCs), to automate and improve the generation of code summaries. However, existing methods often focus on generating a single high-quality summary for a given source code, neglecting scenarios where the generated summary might be inadequate and alternative options are needed. In this paper, we introduce Variational Prefix Tuning (VPT), a novel approach that enhances pre-trained models' ability to generate diverse yet accurate sets of summaries, allowing the user to choose the most suitable one for the given source code. Our method integrates a Conditional Variational Autoencoder (CVAE) framework as a modular component into pre-trained models, enabling us to model the distribution of observed target summaries and sample continuous embeddings to be used as prefixes to steer the generation of diverse outputs during decoding. Importantly, we construct our method in a parameter-efficient manner, eliminating the need for expensive model retraining, especially when using LLMCs. Furthermore, we employ a bi-criteria reranking method to select a subset of generated summaries, optimizing both the diversity and the accuracy of the options presented to users. We present extensive experimental evaluations using widely used datasets and current state-of-the-art pre-trained code summarization models to demonstrate the effectiveness of our approach and its adaptability across models.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ç°æœ‰ä»£ç æ‘˜è¦ç”Ÿæˆæ–¹æ³•ä»…ä¾§é‡äºå•ä¸€è¾“å‡ºè€Œå¿½ç•¥å¤šæ ·æ€§éœ€æ±‚çš„é—®é¢˜ï¼Œæå‡ºäº† Variational Prefix Tuning (VPT) æ–¹æ³•ã€‚VPT é€šè¿‡å°† Conditional Variational Autoencoder (CVAE) æ¡†æ¶ä½œä¸ºæ¨¡å—åŒ–ç»„ä»¶é›†æˆåˆ°é¢„è®­ç»ƒæ¨¡å‹ä¸­ï¼Œèƒ½å¤Ÿå¯¹ç›®æ ‡æ‘˜è¦çš„åˆ†å¸ƒè¿›è¡Œå»ºæ¨¡ï¼Œå¹¶åˆ©ç”¨é‡‡æ ·å¾—åˆ°çš„è¿ç»­åµŒå…¥ä½œä¸º Prefix å¼•å¯¼æ¨¡å‹ç”Ÿæˆå¤šæ ·åŒ–ä¸”å‡†ç¡®çš„æ‘˜è¦ã€‚è¯¥æ–¹æ³•é‡‡ç”¨å‚æ•°é«˜æ•ˆ (Parameter-efficient) çš„è®¾è®¡ï¼Œæ— éœ€å¯¹å¤§å‹æ¨¡å‹è¿›è¡Œæ˜‚è´µçš„é‡æ–°è®­ç»ƒï¼Œç‰¹åˆ«é€‚ç”¨äºä»£ç å¤§è¯­è¨€æ¨¡å‹ (LLMCs)ã€‚æ­¤å¤–ï¼Œç ”ç©¶è¿˜å¼•å…¥äº†åŒå‡†åˆ™é‡æ’åº (Bi-criteria reranking) æœºåˆ¶ï¼Œä»¥è¿›ä¸€æ­¥å¹³è¡¡å¹¶ä¼˜åŒ–æ‘˜è¦é€‰é¡¹çš„å¤šæ ·æ€§ä¸å‡†ç¡®æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒVPT åœ¨å¤šä¸ªä¸»æµæ•°æ®é›†å’Œæœ€å…ˆè¿›æ¨¡å‹ä¸Šå‡å±•ç°äº†å‡ºè‰²çš„æœ‰æ•ˆæ€§ä¸è·¨æ¨¡å‹é€‚åº”æ€§ã€‚",
      "categories": [
        "cs.SE",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.SE",
      "comment": "Accepted by the Journal of Systems and Software",
      "pdf_url": "https://arxiv.org/pdf/2505.09062v1",
      "published_date": "2025-05-14 01:46:56 UTC",
      "updated_date": "2025-05-14 01:46:56 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T08:46:31.945503+00:00"
    },
    {
      "arxiv_id": "2505.09040v3",
      "title": "RT-Cache: Training-Free Retrieval for Real-Time Manipulation",
      "title_zh": "RT-Cacheï¼šé¢å‘å®æ—¶æ“æ§çš„å…è®­ç»ƒæ£€ç´¢",
      "authors": [
        "Owen Kwon",
        "Abraham George",
        "Alison Bartsch",
        "Amir Barati Farimani"
      ],
      "abstract": "Real robots are expected to repeat the same behavior in new environments with very little new data, yet modern controllers either incur heavy per-step inference or require deployment-time fine-tuning. We propose RT-Cache, a training-free retrieval-as-control pipeline that caches diverse image action trajectories in a unified vector memory and, at test time, embeds the current frame to retrieve and replay multi-step snippets, replacing per-step model calls. A hierarchical search keeps lookups sub-second at million scale, shifting cost from compute to storage and enabling real-time control on modest GPUs. Across real-robot tasks and large open logs, RT-Cache achieves higher success and lower completion time than strong retrieval baselines (approximately x2 higher success and ~30% faster in our settings), and a single-episode anchoring study shows immediate adaptation to a more complex, contact-rich task without fine-tuning. RT-Cache turns experience into an append-only memory, offering a simple, scalable path to few-shot deployment today and a foundation for multimodal keys and optional integration with high-level policies. Project page: https://rt-cache.github.io/.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† RT-Cacheï¼Œä¸€ç§æ— éœ€è®­ç»ƒçš„æ£€ç´¢å³æ§åˆ¶ (retrieval-as-control) æµæ°´çº¿ï¼Œæ—¨åœ¨è§£å†³ç°ä»£æœºå™¨äººæ§åˆ¶å™¨æ¨ç†å¼€é”€å¤§æˆ–éœ€éƒ¨ç½²æ—¶å¾®è°ƒçš„é—®é¢˜ã€‚è¯¥æ¡†æ¶å°†å¤šæ ·åŒ–çš„å›¾åƒåŠ¨ä½œè½¨è¿¹å­˜å‚¨åœ¨ç»Ÿä¸€çš„å‘é‡å†…å­˜ä¸­ï¼Œå¹¶åœ¨æµ‹è¯•æ—¶é€šè¿‡åµŒå…¥å½“å‰å¸§æ¥æ£€ç´¢å¹¶é‡æ”¾å¤šæ­¥ç‰‡æ®µï¼Œä»è€Œå–ä»£äº†é€æ­¥éª¤çš„æ¨¡å‹è°ƒç”¨ã€‚RT-Cache é‡‡ç”¨åˆ†å±‚æœç´¢æŠ€æœ¯åœ¨ç™¾ä¸‡çº§è§„æ¨¡ä¸‹å®ç°äº†äºšç§’çº§æŸ¥è¯¢ï¼ŒæˆåŠŸå°†è®¡ç®—æˆæœ¬è½¬ç§»åˆ°å­˜å‚¨ä¸Šï¼Œä½¿å¾—åœ¨æ™®é€š GPU ä¸Šä¹Ÿèƒ½å®ç°å®æ—¶æ§åˆ¶ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œåœ¨çœŸå®æœºå™¨äººä»»åŠ¡å’Œå¤§å‹å¼€æ”¾æ—¥å¿—ä¸­ï¼ŒRT-Cache çš„æˆåŠŸç‡æ¯”å¼ºåŸºçº¿æ¨¡å‹é«˜å‡ºçº¦ä¸¤å€ï¼Œä¸”ä»»åŠ¡å®Œæˆæ—¶é—´ç¼©çŸ­äº†çº¦ 30%ã€‚æ­¤å¤–ï¼Œå•å›åˆé”šå®šç ”ç©¶è¯æ˜äº†è¯¥æ–¹æ³•æ— éœ€å¾®è°ƒå³å¯ç«‹å³é€‚åº”å¤æ‚çš„æ¥è§¦å¯†é›†å‹ä»»åŠ¡ã€‚RT-Cache å°†ç»éªŒè½¬åŒ–ä¸ºåªå¢ä¸å‡çš„å†…å­˜ï¼Œä¸ºæœºå™¨äººå°‘æ ·æœ¬éƒ¨ç½²æä¾›äº†ä¸€æ¡ç®€å•ä¸”å¯æ‰©å±•çš„è·¯å¾„ï¼Œå¹¶ä¸ºæœªæ¥é›†æˆå¤šæ¨¡æ€é”®å€¼å’Œé«˜çº§ç­–ç•¥å¥ å®šäº†åŸºç¡€ã€‚",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.CV",
        "cs.LG"
      ],
      "primary_category": "cs.RO",
      "comment": "8 pages, 6 figures. 2025 IEEE-RAS 24th International Conference on Humanoid Robots",
      "pdf_url": "https://arxiv.org/pdf/2505.09040v3",
      "published_date": "2025-05-14 00:41:44 UTC",
      "updated_date": "2025-08-25 00:15:27 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T08:46:39.585400+00:00"
    },
    {
      "arxiv_id": "2505.10577v1",
      "title": "GRNN:Recurrent Neural Network based on Ghost Features for Video Super-Resolution",
      "title_zh": "GRNNï¼šåŸºäº Ghost ç‰¹å¾çš„è§†é¢‘è¶…åˆ†è¾¨ç‡å¾ªç¯ç¥ç»ç½‘ç»œ",
      "authors": [
        "Yutong Guo"
      ],
      "abstract": "Modern video super-resolution (VSR) systems based on convolutional neural networks (CNNs) require huge computational costs. The problem of feature redundancy is present in most models in many domains, but is rarely discussed in VSR. We experimentally observe that many features in VSR models are also similar to each other, so we propose to use \"Ghost features\" to reduce this redundancy. We also analyze the so-called \"gradient disappearance\" phenomenon generated by the conventional recurrent convolutional network (RNN) model, and combine the Ghost module with RNN to complete the modeling on time series. The current frame is used as input to the model together with the next frame, the output of the previous frame and the hidden state. Extensive experiments on several benchmark models and datasets show that the PSNR and SSIM of our proposed modality are improved to some extent. Some texture details in the video are also better preserved.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ç°ä»£è§†é¢‘è¶…åˆ†è¾¨ç‡ (VSR) ç³»ç»Ÿä¸­åŸºäºå·ç§¯ç¥ç»ç½‘ç»œ (CNN) æ¨¡å‹è®¡ç®—å¼€é”€å·¨å¤§çš„é—®é¢˜ï¼ŒæŒ‡å‡ºæ¨¡å‹ä¸­æ™®éå­˜åœ¨ç‰¹å¾å†—ä½™ç°è±¡ã€‚ä¸ºæ­¤ï¼Œä½œè€…æå‡ºäº† GRNNï¼Œä¸€ç§åŸºäº Ghost features çš„å¾ªç¯ç¥ç»ç½‘ç»œæ¡†æ¶ï¼Œæ—¨åœ¨é€šè¿‡ Ghost æ¨¡å—æœ‰æ•ˆå‡å°‘å†—ä½™ã€‚è¯¥æ–¹æ³•å°† Ghost æ¨¡å—ä¸ RNN ç›¸ç»“åˆä»¥è¿›è¡Œæ—¶é—´åºåˆ—å»ºæ¨¡ï¼Œå¹¶é’ˆå¯¹æ€§åœ°è§£å†³äº†ä¼ ç»Ÿå¾ªç¯å·ç§¯ç½‘ç»œä¸­å¸¸è§çš„æ¢¯åº¦æ¶ˆå¤± (gradient disappearance) ç°è±¡ã€‚åœ¨å…·ä½“å®ç°ä¸­ï¼Œæ¨¡å‹å°†å½“å‰å¸§ã€ä¸‹ä¸€å¸§ã€å‰ä¸€å¸§è¾“å‡ºåŠéšè—çŠ¶æ€ (hidden state) å…±åŒä½œä¸ºè¾“å…¥ï¼Œå®ç°äº†å¯¹å¸§é—´ä¿¡æ¯çš„é«˜æ•ˆåˆ©ç”¨ã€‚å®éªŒç»“æœè¯æ˜ï¼ŒGRNN åœ¨å¤šä¸ªåŸºå‡†æ•°æ®é›†ä¸Šçš„ PSNR å’Œ SSIM æŒ‡æ ‡å‡æœ‰æ‰€æå‡ï¼Œèƒ½å¤Ÿæ›´å¥½åœ°ä¿ç•™è§†é¢‘ä¸­çš„çº¹ç†ç»†èŠ‚ã€‚è¯¥æ¨¡å‹åœ¨é™ä½è®¡ç®—å¼€é”€çš„åŒæ—¶ï¼Œä¸ºè§†é¢‘è¶…åˆ†è¾¨ç‡ä»»åŠ¡æä¾›äº†ä¸€ç§æ›´å…·æ•ˆç‡ä¸”æ€§èƒ½æ›´ä¼˜çš„å»ºæ¨¡æ–¹æ¡ˆã€‚",
      "categories": [
        "eess.IV",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "eess.IV",
      "comment": "Accepted by 2023 IEEE International Conference on Multimedia and Expo (ICME 2023)",
      "pdf_url": "https://arxiv.org/pdf/2505.10577v1",
      "published_date": "2025-05-14 00:38:46 UTC",
      "updated_date": "2025-05-14 00:38:46 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T08:46:33.912165+00:00"
    }
  ],
  "processing_status": "completed",
  "error": null,
  "raw_papers_fetched": true,
  "papers_count": 94,
  "processed_papers_count": 94,
  "failed_papers_count": 0,
  "llm_backup_calls": 0,
  "summary_generated": true,
  "daily_data_saved": true,
  "last_update": "2026-01-23T08:48:19.133728+00:00"
}
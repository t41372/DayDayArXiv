[
  {
    "arxiv_id": "2512.12088v1",
    "title": "Reliable Policy Iteration: Performance Robustness Across Architecture and Environment Perturbations",
    "authors": [
      "S. R. Eshwar",
      "Aniruddha Mukherjee",
      "Kintan Saha",
      "Krishna Agarwal",
      "Gugan Thoppe",
      "Aditya Gopalan",
      "Gal Dalal"
    ],
    "abstract": "In a recent work, we proposed Reliable Policy Iteration (RPI), that restores policy iteration's monotonicity-of-value-estimates property to the function approximation setting. Here, we assess the robustness of RPI's empirical performance on two classical control tasks -- CartPole and Inverted Pendulum -- under changes to neural network and environmental parameters. Relative to DQN, Double DQN, DDPG, TD3, and PPO, RPI reaches near-optimal performance early and sustains this policy as training proceeds. Because deep RL methods are often hampered by sample inefficiency, training instability, and hyperparameter sensitivity, our results highlight RPI's promise as a more reliable alternative.",
    "categories": [
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2512.12088v1",
    "published_date": "2025-12-12 23:33:06 UTC",
    "updated_date": "2025-12-12 23:33:06 UTC"
  },
  {
    "arxiv_id": "2512.12081v1",
    "title": "Congestion Reduction in EV Charger Placement Using Traffic Equilibrium Models",
    "authors": [
      "Semih Kara",
      "Yasin Sonmez",
      "Can Kizilkale",
      "Alex Kurzhanskiy",
      "Nuno C. Martins",
      "Murat Arcak"
    ],
    "abstract": "Growing EV adoption can worsen traffic conditions if chargers are sited without regard to their impact on congestion. We study how to strategically place EV chargers to reduce congestion using two equilibrium models: one based on congestion games and one based on an atomic queueing simulation. We apply both models within a scalable greedy station-placement algorithm. Experiments show that this greedy scheme yields optimal or near-optimal congestion outcomes in realistic networks, even though global optimality is not guaranteed as we show with a counterexample. We also show that the queueing-based approach yields more realistic results than the congestion-game model, and we present a unified methodology that calibrates congestion delays from queue simulation and solves equilibrium in link-space.",
    "categories": [
      "eess.SY",
      "cs.AI",
      "cs.SI",
      "math.OC"
    ],
    "primary_category": "eess.SY",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2512.12081v1",
    "published_date": "2025-12-12 23:06:35 UTC",
    "updated_date": "2025-12-12 23:06:35 UTC"
  },
  {
    "arxiv_id": "2512.12069v2",
    "title": "Rethinking Jailbreak Detection of Large Vision Language Models with Representational Contrastive Scoring",
    "authors": [
      "Peichun Hua",
      "Hao Li",
      "Shanghao Shi",
      "Zhiyuan Yu",
      "Ning Zhang"
    ],
    "abstract": "Large Vision-Language Models (LVLMs) are vulnerable to a growing array of multimodal jailbreak attacks, necessitating defenses that are both generalizable to novel threats and efficient for practical deployment. Many current strategies fall short, either targeting specific attack patterns, which limits generalization, or imposing high computational overhead. While lightweight anomaly-detection methods offer a promising direction, we find that their common one-class design tends to confuse novel benign inputs with malicious ones, leading to unreliable over-rejection. To address this, we propose Representational Contrastive Scoring (RCS), a framework built on a key insight: the most potent safety signals reside within the LVLM's own internal representations. Our approach inspects the internal geometry of these representations, learning a lightweight projection to maximally separate benign and malicious inputs in safety-critical layers. This enables a simple yet powerful contrastive score that differentiates true malicious intent from mere novelty. Our instantiations, MCD (Mahalanobis Contrastive Detection) and KCD (K-nearest Contrastive Detection), achieve state-of-the-art performance on a challenging evaluation protocol designed to test generalization to unseen attack types. This work demonstrates that effective jailbreak detection can be achieved by applying simple, interpretable statistical methods to the appropriate internal representations, offering a practical path towards safer LVLM deployment. Our code is available on Github https://github.com/sarendis56/Jailbreak_Detection_RCS.",
    "categories": [
      "cs.CR",
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "primary_category": "cs.CR",
    "comment": "37 pages, 13 figures",
    "pdf_url": "https://arxiv.org/pdf/2512.12069v2",
    "published_date": "2025-12-12 22:31:38 UTC",
    "updated_date": "2026-01-07 11:45:25 UTC"
  },
  {
    "arxiv_id": "2512.12066v2",
    "title": "The Instability of Safety: How Random Seeds and Temperature Expose Inconsistent LLM Refusal Behavior",
    "authors": [
      "Erik Larsen"
    ],
    "abstract": "Current safety evaluations of large language models rely on single-shot testing, implicitly assuming that model responses are deterministic and representative of the model's safety alignment. We challenge this assumption by investigating the stability of safety refusal decisions across random seeds and temperature settings. Testing four instruction-tuned models from three families (Llama 3.1 8B, Qwen 2.5 7B, Qwen 3 8B, Gemma 3 12B) on 876 harmful prompts across 20 different sampling configurations (4 temperatures x 5 random seeds), we find that 18-28% of prompts exhibit decision flips--the model refuses in some configurations but complies in others--depending on the model. Our Safety Stability Index (SSI) reveals that higher temperatures significantly reduce decision stability (Friedman chi-squared = 396.81, p < 0.001), with mean within-temperature SSI dropping from 0.977 at temperature 0.0 to 0.942 at temperature 1.0. We validate our findings across all model families using Claude 3.5 Haiku as a unified external judge, achieving 89.0% inter-judge agreement with our primary Llama 70B judge (Cohen's kappa = 0.62). Within each model, prompts with higher compliance rates exhibit lower stability (Spearman rho = -0.47 to -0.70, all p < 0.001), indicating that models \"waver\" more on borderline requests. These findings demonstrate that single-shot safety evaluations are insufficient for reliable safety assessment and that evaluation protocols must account for stochastic variation in model behavior. We show that single-shot evaluation agrees with multi-sample ground truth only 92.4% of the time when pooling across temperatures (94.2-97.7% at fixed temperature depending on setting), and recommend using at least 3 samples per prompt for reliable safety assessment.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.LG",
    "comment": "16 pages, 7 figures, 9 tables. Code and data available at https://github.com/erikl2/safety-refusal-stability",
    "pdf_url": "https://arxiv.org/pdf/2512.12066v2",
    "published_date": "2025-12-12 22:29:13 UTC",
    "updated_date": "2025-12-16 03:45:08 UTC"
  },
  {
    "arxiv_id": "2512.12063v1",
    "title": "Instruction-Tuning Open-Weight Language Models for BPMN Model Generation",
    "authors": [
      "Gökberk Çelikmasat",
      "Atay Özgövde",
      "Fatma Başak Aydemir"
    ],
    "abstract": "Domain models are central to software engineering, as they enable a shared understanding, guide implementation, and support automated analyses and model-driven development. Yet, despite these benefits, practitioners often skip modeling because it is time-consuming and demands scarce expertise. We address this barrier by investigating whether open-weight large language models, adapted via instruction tuning, can generate high-quality BPMN process models directly from natural language descriptions in a cost-effective and privacy-preserving way. We introduce InstruBPM, a reproducible approach that prepares paired text-diagram data and instruction tunes an open source large language model with parameter-efficient fine-tuning and quantization for on-prem deployment. We evaluate the tuned model through complementary perspectives: (i) text/code similarity using BLEU, ROUGE-L, and METEOR, (ii) structural fidelity using Relative Graph Edit Distance, (iii) guidelines conformance using external tool checks, and (iv) a small expert review. Using a curated subset of a multi-domain BPMN dataset, we compare the tuned model with untuned open-weight baselines and strong proprietary models under consistent prompting regimes. Our compact tuned model outperforms all baselines across sequence and structural metrics while requiring substantially fewer resources; guideline analysis and expert feedback further indicate that the generated diagrams largely follow BPMN best practices and are useful starting points that reduce modeling effort. Overall, instruction tuning improves structural accuracy and robustness compared to untuned baselines and reduces reliance on heavy prompt scaffolding. We publicly share the trained models and scripts to support reproducibility and further research.",
    "categories": [
      "cs.SE",
      "cs.AI"
    ],
    "primary_category": "cs.SE",
    "comment": "Preprint. Under preparation for journal submission",
    "pdf_url": "https://arxiv.org/pdf/2512.12063v1",
    "published_date": "2025-12-12 22:07:51 UTC",
    "updated_date": "2025-12-12 22:07:51 UTC"
  },
  {
    "arxiv_id": "2512.12059v1",
    "title": "The Forecast Critic: Leveraging Large Language Models for Poor Forecast Identification",
    "authors": [
      "Luke Bhan",
      "Hanyu Zhang",
      "Andrew Gordon Wilson",
      "Michael W. Mahoney",
      "Chuck Arvin"
    ],
    "abstract": "Monitoring forecasting systems is critical for customer satisfaction, profitability, and operational efficiency in large-scale retail businesses. We propose The Forecast Critic, a system that leverages Large Language Models (LLMs) for automated forecast monitoring, taking advantage of their broad world knowledge and strong ``reasoning'' capabilities. As a prerequisite for this, we systematically evaluate the ability of LLMs to assess time series forecast quality, focusing on three key questions. (1) Can LLMs be deployed to perform forecast monitoring and identify obviously unreasonable forecasts? (2) Can LLMs effectively incorporate unstructured exogenous features to assess what a reasonable forecast looks like? (3) How does performance vary across model sizes and reasoning capabilities, measured across state-of-the-art LLMs? We present three experiments, including on both synthetic and real-world forecasting data. Our results show that LLMs can reliably detect and critique poor forecasts, such as those plagued by temporal misalignment, trend inconsistencies, and spike errors. The best-performing model we evaluated achieves an F1 score of 0.88, somewhat below human-level performance (F1 score: 0.97). We also demonstrate that multi-modal LLMs can effectively incorporate unstructured contextual signals to refine their assessment of the forecast. Models correctly identify missing or spurious promotional spikes when provided with historical context about past promotions (F1 score: 0.84). Lastly, we demonstrate that these techniques succeed in identifying inaccurate forecasts on the real-world M5 time series dataset, with unreasonable forecasts having an sCRPS at least 10% higher than that of reasonable forecasts. These findings suggest that LLMs, even without domain-specific fine-tuning, may provide a viable and scalable option for automated forecast monitoring and evaluation.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "Presented at AAAI 2026 AI4TS workshop and AABA4ET workshop",
    "pdf_url": "https://arxiv.org/pdf/2512.12059v1",
    "published_date": "2025-12-12 21:59:53 UTC",
    "updated_date": "2025-12-12 21:59:53 UTC"
  },
  {
    "arxiv_id": "2512.12048v1",
    "title": "Context-Aware Agentic Power Resources Optimisation in EV using Smart2ChargeApp",
    "authors": [
      "Muddsair Sharif",
      "Huseyin Seker"
    ],
    "abstract": "This paper presents a novel context-sensitive multi\\-agent coordination for dynamic resource allocation (CAMAC-DRA) framework for optimizing smart electric vehicle (EV) charging ecosystems through the Smart2Charge application. The proposed system coordinates autonomous charging agents across networks of 250 EVs and 45 charging stations while adapting to dynamic environmental conditions through context-aware decision-making. Our multi-agent approach employs coordinated Deep Q\\-Networks integrated with Graph Neural Networks and attention mechanisms, processing 20 contextual features including weather patterns, traffic conditions, grid load fluctuations, and electricity pricing.The framework balances five ecosystem stakeholders i.e. EV users (25\\%), grid operators (20\\%), charging station operators (20\\%), fleet operators (20%), and environmental factors (15\\%) through weighted coordination mechanisms and consensus protocols. Comprehensive validation using real-world datasets containing 441,077 charging transactions demonstrates superior performance compared to baseline algorithms including DDPG, A3C, PPO, and GNN approaches. The CAMAC\\-DRA framework achieves 92\\% coordination success rate, 15\\% energy efficiency improvement, 10\\% cost reduction, 20% grid strain decrease, and \\2.3x faster convergence while maintaining 88\\% training stability and 85\\% sample efficiency. Real-world validation confirms commercial viability with Net Present Cost of -\\$122,962 and 69\\% cost reduction through renewable energy integration. The framework's unique contribution lies in developing context-aware multi-stakeholder coordination that successfully balances competing objectives while adapting to real-time variables, positioning it as a breakthrough solution for intelligent EV charging coordination and sustainable transportation electrification.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2512.12048v1",
    "published_date": "2025-12-12 21:41:13 UTC",
    "updated_date": "2025-12-12 21:41:13 UTC"
  },
  {
    "arxiv_id": "2512.12045v1",
    "title": "AI as a Teaching Partner: Early Lessons from Classroom Codesign with Secondary Teachers",
    "authors": [
      "Alex Liu",
      "Lief Esbenshade",
      "Shawon Sarkar",
      "Zewei Tian",
      "Min Sun",
      "Zachary Zhang",
      "Thomas Han",
      "Yulia Lapicus",
      "Kevin He"
    ],
    "abstract": "This report presents a comprehensive account of the Colleague AI Classroom pilot, a collaborative design (co-design) study that brought generative AI technology directly into real classrooms. In this study, AI functioned as a third agent, an active participant that mediated feedback, supported inquiry, and extended teachers' instructional reach while preserving human judgment and teacher authority.\n  Over seven weeks in spring 2025, 21 in-service teachers from four Washington State public school districts and one independent school integrated four AI-powered features of the Colleague AI Classroom into their instruction: Teaching Aide, Assessment and AI Grading, AI Tutor, and Student Growth Insights. More than 600 students in grades 6-12 used the platform in class at the direction of their teachers, who designed and facilitated the AI activities.\n  During the Classroom pilot, teachers were co-design partners: they planned activities, implemented them with students, and provided weekly reflections on AI's role in classroom settings. The teachers' feedback guided iterative improvements for Colleague AI. The research team captured rich data through surveys, planning and reflection forms, group meetings, one-on-one interviews, and platform usage logs to understand where AI adds instructional value and where it requires refinement.",
    "categories": [
      "cs.HC",
      "cs.AI"
    ],
    "primary_category": "cs.HC",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2512.12045v1",
    "published_date": "2025-12-12 21:35:32 UTC",
    "updated_date": "2025-12-12 21:35:32 UTC"
  },
  {
    "arxiv_id": "2601.02375v1",
    "title": "LeafTutor: An AI Agent for Programming Assignment Tutoring",
    "authors": [
      "Madison Bochard",
      "Tim Conser",
      "Alyssa Duran",
      "Lazaro Martull",
      "Pu Tian",
      "Yalong Wu"
    ],
    "abstract": "High enrollment in STEM-related degree programs has created increasing demand for scalable tutoring support, as universities experience a shortage of qualified instructors and teaching assistants (TAs). To address this challenge, LeafTutor, an AI tutoring agent powered by large language models (LLMs), was developed to provide step-by-step guidance for students. LeafTutor was evaluated through real programming assignments. The results indicate that the system can deliver step-by-step programming guidance comparable to human tutors. This work demonstrates the potential of LLM-driven tutoring solutions to enhance and personalize learning in STEM education. If any reader is interested in collaboration with our team to improve or test LeafTutor, please contact Pu Tian (pu.tian@stockton.edu) or Yalong Wu (wuy@uhcl.edu).",
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.SE"
    ],
    "primary_category": "cs.CY",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2601.02375v1",
    "published_date": "2025-12-12 20:38:40 UTC",
    "updated_date": "2025-12-12 20:38:40 UTC"
  },
  {
    "arxiv_id": "2512.12012v2",
    "title": "Semantic-Drive: Democratizing Long-Tail Data Curation via Open-Vocabulary Grounding and Neuro-Symbolic VLM Consensus",
    "authors": [
      "Antonio Guillen-Perez"
    ],
    "abstract": "The development of robust Autonomous Vehicles (AVs) is bottlenecked by the scarcity of \"Long-Tail\" training data. While fleets collect petabytes of video logs, identifying rare safety-critical events (e.g., erratic jaywalking, construction diversions) remains a manual, cost-prohibitive process. Existing solutions rely on coarse metadata search, which lacks precision, or cloud-based VLMs, which are privacy-invasive and expensive. We introduce Semantic-Drive, a local-first, neuro-symbolic framework for semantic data mining. Our approach decouples perception into two stages: (1) Symbolic Grounding via a real-time open-vocabulary detector (YOLOE) to anchor attention, and (2) Cognitive Analysis via a Reasoning VLM that performs forensic scene analysis. To mitigate hallucination, we implement a \"System 2\" inference-time alignment strategy, utilizing a multi-model \"Judge-Scout\" consensus mechanism. Benchmarked on the nuScenes dataset against the Waymo Open Dataset (WOD-E2E) taxonomy, Semantic-Drive achieves a Recall of 0.966 (vs. 0.475 for CLIP) and reduces Risk Assessment Error by 40% ccompared to the best single scout models. The system runs entirely on consumer hardware (NVIDIA RTX 3090), offering a privacy-preserving alternative to the cloud.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL",
      "cs.RO"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2512.12012v2",
    "published_date": "2025-12-12 20:07:04 UTC",
    "updated_date": "2025-12-16 17:15:46 UTC"
  },
  {
    "arxiv_id": "2512.12008v1",
    "title": "Hold Onto That Thought: Assessing KV Cache Compression On Reasoning",
    "authors": [
      "Minghui Liu",
      "Aadi Palnitkar",
      "Tahseen Rabbani",
      "Hyunwoo Jae",
      "Kyle Rui Sang",
      "Dixi Yao",
      "Shayan Shabihi",
      "Fuheng Zhao",
      "Tian Li",
      "Ce Zhang",
      "Furong Huang",
      "Kunpeng Zhang"
    ],
    "abstract": "Large language models (LLMs) have demonstrated remarkable performance on long-context tasks, but are often bottlenecked by memory constraints. Namely, the KV cache, which is used to significantly speed up attention computations, grows linearly with context length. A suite of compression algorithms has been introduced to alleviate cache growth by evicting unimportant tokens. However, several popular strategies are targeted towards the prefill phase, i.e., processing long prompt context, and their performance is rarely assessed on reasoning tasks requiring long decoding. In particular, short but complex prompts, such as those in benchmarks like GSM8K and MATH500, often benefit from multi-step reasoning and self-reflection, resulting in thinking sequences thousands of tokens long. In this work, we benchmark the performance of several popular compression strategies on long-reasoning tasks. For the non-reasoning Llama-3.1-8B-Instruct, we determine that no singular strategy fits all, and that performance is heavily influenced by dataset type. However, we discover that H2O and our decoding-enabled variant of SnapKV are dominant strategies for reasoning models, indicating the utility of heavy-hitter tracking for reasoning traces. We also find that eviction strategies at low budgets can produce longer reasoning traces, revealing a tradeoff between cache size and inference costs.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.PF"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2512.12008v1",
    "published_date": "2025-12-12 19:50:34 UTC",
    "updated_date": "2025-12-12 19:50:34 UTC"
  },
  {
    "arxiv_id": "2512.11997v1",
    "title": "Log Anomaly Detection with Large Language Models via Knowledge-Enriched Fusion",
    "authors": [
      "Anfeng Peng",
      "Ajesh Koyatan Chathoth",
      "Stephen Lee"
    ],
    "abstract": "System logs are a critical resource for monitoring and managing distributed systems, providing insights into failures and anomalous behavior. Traditional log analysis techniques, including template-based and sequence-driven approaches, often lose important semantic information or struggle with ambiguous log patterns. To address this, we present EnrichLog, a training-free, entry-based anomaly detection framework that enriches raw log entries with both corpus-specific and sample-specific knowledge. EnrichLog incorporates contextual information, including historical examples and reasoning derived from the corpus, to enable more accurate and interpretable anomaly detection. The framework leverages retrieval-augmented generation to integrate relevant contextual knowledge without requiring retraining. We evaluate EnrichLog on four large-scale system log benchmark datasets and compare it against five baseline methods. Our results show that EnrichLog consistently improves anomaly detection performance, effectively handles ambiguous log entries, and maintains efficient inference. Furthermore, incorporating both corpus- and sample-specific knowledge enhances model confidence and detection accuracy, making EnrichLog well-suited for practical deployments.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2512.11997v1",
    "published_date": "2025-12-12 19:24:54 UTC",
    "updated_date": "2025-12-12 19:24:54 UTC"
  },
  {
    "arxiv_id": "2512.11995v1",
    "title": "V-REX: Benchmarking Exploratory Visual Reasoning via Chain-of-Questions",
    "authors": [
      "Chenrui Fan",
      "Yijun Liang",
      "Shweta Bhardwaj",
      "Kwesi Cobbina",
      "Ming Li",
      "Tianyi Zhou"
    ],
    "abstract": "While many vision-language models (VLMs) are developed to answer well-defined, straightforward questions with highly specified targets, as in most benchmarks, they often struggle in practice with complex open-ended tasks, which usually require multiple rounds of exploration and reasoning in the visual space. Such visual thinking paths not only provide step-by-step exploration and verification as an AI detective but also produce better interpretations of the final answers. However, these paths are challenging to evaluate due to the large exploration space of intermediate steps. To bridge the gap, we develop an evaluation suite, ``Visual Reasoning with multi-step EXploration (V-REX)'', which is composed of a benchmark of challenging visual reasoning tasks requiring native multi-step exploration and an evaluation protocol. V-REX covers rich application scenarios across diverse domains. V-REX casts the multi-step exploratory reasoning into a Chain-of-Questions (CoQ) and disentangles VLMs' capability to (1) Planning: breaking down an open-ended task by selecting a chain of exploratory questions; and (2) Following: answering curated CoQ sequentially to collect information for deriving the final answer. By curating finite options of questions and answers per step, V-REX achieves a reliable quantitative and fine-grained analysis of the intermediate steps. By assessing SOTA proprietary and open-sourced VLMs, we reveal consistent scaling trends, significant differences between planning and following abilities, and substantial room for improvement in multi-step exploratory reasoning.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "28 pages",
    "pdf_url": "https://arxiv.org/pdf/2512.11995v1",
    "published_date": "2025-12-12 19:18:41 UTC",
    "updated_date": "2025-12-12 19:18:41 UTC"
  },
  {
    "arxiv_id": "2512.14745v1",
    "title": "Factor(U,T): Controlling Untrusted AI by Monitoring their Plans",
    "authors": [
      "Edward Lue Chee Lip",
      "Anthony Channg",
      "Diana Kim",
      "Aaron Sandoval",
      "Kevin Zhu"
    ],
    "abstract": "As AI capabilities advance, we increasingly rely on powerful models to decompose complex tasks $\\unicode{x2013}$ but what if the decomposer itself is malicious? Factored cognition protocols decompose complex tasks into simpler child tasks: one model creates the decomposition, while other models implement the child tasks in isolation. Prior work uses trusted (weaker but reliable) models for decomposition, which limits usefulness for tasks where decomposition itself is challenging. We introduce Factor($U$,$T$), in which an untrusted (stronger but potentially malicious) model decomposes while trusted models implement child tasks. Can monitors detect malicious activity when observing only natural language task instructions, rather than complete solutions? We baseline and red team Factor($U$,$T$) in control evaluations on BigCodeBench, a dataset of Python coding tasks. Monitors distinguishing malicious from honest decompositions perform poorly (AUROC 0.52) compared to monitors evaluating complete Python solutions (AUROC 0.96). Furthermore, Factor($D$,$U$), which uses a trusted decomposer and monitors concrete child solutions, achieves excellent discrimination (AUROC 0.96) and strong safety (1.2% ASR), demonstrating that implementation-context monitoring succeeds where decomposition-only monitoring fails.",
    "categories": [
      "cs.CR",
      "cs.AI"
    ],
    "primary_category": "cs.CR",
    "comment": "Accepted to AAAI 2026 Workshop on Trust and Control in Agentic AI (TrustAgent). 6 pages body, 8 pages total, 3 figures",
    "pdf_url": "https://arxiv.org/pdf/2512.14745v1",
    "published_date": "2025-12-12 19:11:34 UTC",
    "updated_date": "2025-12-12 19:11:34 UTC"
  },
  {
    "arxiv_id": "2512.11984v1",
    "title": "Evidence-Driven Decision Support for AI Model Selection in Research Software Engineering",
    "authors": [
      "Alireza Joonbakhsh",
      "Alireza Rostami",
      "AmirMohammad Kamalinia",
      "Ali Nazeri",
      "Farshad Khunjush",
      "Bedir Tekinerdogan",
      "Siamak Farshidi"
    ],
    "abstract": "The rapid proliferation of artificial intelligence (AI) models and methods presents growing challenges for research software engineers and researchers who must select, integrate, and maintain appropriate models within complex research workflows. Model selection is often performed in an ad hoc manner, relying on fragmented metadata and individual expertise, which can undermine reproducibility, transparency, and overall research software quality.\n  This work proposes a structured and evidence-driven approach to support AI model selection that aligns with both technical and contextual requirements. We conceptualize AI model selection as a Multi-Criteria Decision-Making (MCDM) problem and introduce an evidence-based decision-support framework that integrates automated data collection pipelines, a structured knowledge graph, and MCDM principles. Following the Design Science Research methodology, the proposed framework (ModelSelect) is empirically validated through 50 real-world case studies and comparative experiments against leading generative AI systems.\n  The evaluation results show that ModelSelect produces reliable, interpretable, and reproducible recommendations that closely align with expert reasoning. Across the case studies, the framework achieved high coverage and strong rationale alignment in both model and library recommendation tasks, performing comparably to generative AI assistants while offering superior traceability and consistency.\n  By framing AI model selection as an MCDM problem, this work establishes a rigorous foundation for transparent and reproducible decision support in research software engineering. The proposed framework provides a scalable and explainable pathway for integrating empirical evidence into AI model recommendation processes, ultimately improving the quality and robustness of research software decision-making.",
    "categories": [
      "cs.SE",
      "cs.AI"
    ],
    "primary_category": "cs.SE",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2512.11984v1",
    "published_date": "2025-12-12 19:08:04 UTC",
    "updated_date": "2025-12-12 19:08:04 UTC"
  },
  {
    "arxiv_id": "2512.11982v1",
    "title": "Semantic search for 100M+ galaxy images using AI-generated captions",
    "authors": [
      "Nolan Koblischke",
      "Liam Parker",
      "Francois Lanusse",
      "Irina Espejo Morales",
      "Jo Bovy",
      "Shirley Ho"
    ],
    "abstract": "Finding scientifically interesting phenomena through slow, manual labeling campaigns severely limits our ability to explore the billions of galaxy images produced by telescopes. In this work, we develop a pipeline to create a semantic search engine from completely unlabeled image data. Our method leverages Vision-Language Models (VLMs) to generate descriptions for galaxy images, then contrastively aligns a pre-trained multimodal astronomy foundation model with these embedded descriptions to produce searchable embeddings at scale. We find that current VLMs provide descriptions that are sufficiently informative to train a semantic search model that outperforms direct image similarity search. Our model, AION-Search, achieves state-of-the-art zero-shot performance on finding rare phenomena despite training on randomly selected images with no deliberate curation for rare cases. Furthermore, we introduce a VLM-based re-ranking method that nearly doubles the recall for our most challenging targets in the top-100 results. For the first time, AION-Search enables flexible semantic search scalable to 140 million galaxy images, enabling discovery from previously infeasible searches. More broadly, our work provides an approach for making large, unlabeled scientific image archives semantically searchable, expanding data exploration capabilities in fields from Earth observation to microscopy. The code, data, and app are publicly available at https://github.com/NolanKoblischke/AION-Search",
    "categories": [
      "astro-ph.IM",
      "cs.AI",
      "cs.CV",
      "cs.LG"
    ],
    "primary_category": "astro-ph.IM",
    "comment": "Presented at the NeurIPS 2025 AI4Science Workshop",
    "pdf_url": "https://arxiv.org/pdf/2512.11982v1",
    "published_date": "2025-12-12 19:06:14 UTC",
    "updated_date": "2025-12-12 19:06:14 UTC"
  },
  {
    "arxiv_id": "2512.11979v1",
    "title": "Designing The Internet of Agents: A Framework for Trustworthy, Transparent, and Collaborative Human-Agent Interaction (HAX)",
    "authors": [
      "Marc Scibelli",
      "Krystelle Gonzalez Papaux",
      "Julia Valenti",
      "Srishti Kush"
    ],
    "abstract": "The rise of generative and autonomous agents marks a fundamental shift in computing, demanding a rethinking of how humans collaborate with probabilistic, partially autonomous systems. We present the Human-AI-Experience (HAX) framework, a comprehensive, three-phase approach that establishes design foundations for trustworthy, transparent, and collaborative agentic interaction. HAX integrates behavioral heuristics, a schema-driven SDK enforcing structured and safe outputs, and a behavioral proxy concept that orchestrates agent activity to reduce cognitive load. A validated catalog of mixed-initiative design patterns further enables intent preview, iterative alignment, trust repair, and multi-agent narrative coherence. Grounded in Time, Interaction, and Performance (TIP) theory, HAX reframes multi-agent systems as colleagues, offering the first end-to-end framework that bridges trust theory, interface design, and infrastructure for the emerging Internet of Agents.",
    "categories": [
      "cs.HC",
      "cs.AI"
    ],
    "primary_category": "cs.HC",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2512.11979v1",
    "published_date": "2025-12-12 19:04:40 UTC",
    "updated_date": "2025-12-12 19:04:40 UTC"
  },
  {
    "arxiv_id": "2512.11798v1",
    "title": "Particulate: Feed-Forward 3D Object Articulation",
    "authors": [
      "Ruining Li",
      "Yuxin Yao",
      "Chuanxia Zheng",
      "Christian Rupprecht",
      "Joan Lasenby",
      "Shangzhe Wu",
      "Andrea Vedaldi"
    ],
    "abstract": "We present Particulate, a feed-forward approach that, given a single static 3D mesh of an everyday object, directly infers all attributes of the underlying articulated structure, including its 3D parts, kinematic structure, and motion constraints. At its core is a transformer network, Part Articulation Transformer, which processes a point cloud of the input mesh using a flexible and scalable architecture to predict all the aforementioned attributes with native multi-joint support. We train the network end-to-end on a diverse collection of articulated 3D assets from public datasets. During inference, Particulate lifts the network's feed-forward prediction to the input mesh, yielding a fully articulated 3D model in seconds, much faster than prior approaches that require per-object optimization. Particulate can also accurately infer the articulated structure of AI-generated 3D assets, enabling full-fledged extraction of articulated 3D objects from a single (real or synthetic) image when combined with an off-the-shelf image-to-3D generator. We further introduce a new challenging benchmark for 3D articulation estimation curated from high-quality public 3D assets, and redesign the evaluation protocol to be more consistent with human preferences. Quantitative and qualitative results show that Particulate significantly outperforms state-of-the-art approaches.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.GR"
    ],
    "primary_category": "cs.CV",
    "comment": "Project page: https://ruiningli.com/particulate",
    "pdf_url": "https://arxiv.org/pdf/2512.11798v1",
    "published_date": "2025-12-12 18:59:51 UTC",
    "updated_date": "2025-12-12 18:59:51 UTC"
  },
  {
    "arxiv_id": "2512.15769v1",
    "title": "Data-Chain Backdoor: Do You Trust Diffusion Models as Generative Data Supplier?",
    "authors": [
      "Junchi Lu",
      "Xinke Li",
      "Yuheng Liu",
      "Qi Alfred Chen"
    ],
    "abstract": "The increasing use of generative models such as diffusion models for synthetic data augmentation has greatly reduced the cost of data collection and labeling in downstream perception tasks. However, this new data source paradigm may introduce important security concerns. This work investigates backdoor propagation in such emerging generative data supply chains, namely Data-Chain Backdoor (DCB). Specifically, we find that open-source diffusion models can become hidden carriers of backdoors. Their strong distribution-fitting ability causes them to memorize and reproduce backdoor triggers during generation, which are subsequently inherited by downstream models, resulting in severe security risks. This threat is particularly concerning under clean-label attack scenarios, as it remains effective while having negligible impact on the utility of the synthetic data. Furthermore, we discover an Early-Stage Trigger Manifestation (ESTM) phenomenon: backdoor trigger patterns tend to surface more explicitly in the early, high-noise stages of the diffusion model's reverse generation process before being subtly integrated into the final samples. Overall, this work reveals a previously underexplored threat in generative data pipelines and provides initial insights toward mitigating backdoor risks in synthetic data generation.",
    "categories": [
      "cs.CR",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CR",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2512.15769v1",
    "published_date": "2025-12-12 18:53:38 UTC",
    "updated_date": "2025-12-12 18:53:38 UTC"
  },
  {
    "arxiv_id": "2512.11783v1",
    "title": "Super Suffixes: Bypassing Text Generation Alignment and Guard Models Simultaneously",
    "authors": [
      "Andrew Adiletta",
      "Kathryn Adiletta",
      "Kemal Derya",
      "Berk Sunar"
    ],
    "abstract": "The rapid deployment of Large Language Models (LLMs) has created an urgent need for enhanced security and privacy measures in Machine Learning (ML). LLMs are increasingly being used to process untrusted text inputs and even generate executable code, often while having access to sensitive system controls. To address these security concerns, several companies have introduced guard models, which are smaller, specialized models designed to protect text generation models from adversarial or malicious inputs. In this work, we advance the study of adversarial inputs by introducing Super Suffixes, suffixes capable of overriding multiple alignment objectives across various models with different tokenization schemes. We demonstrate their effectiveness, along with our joint optimization technique, by successfully bypassing the protection mechanisms of Llama Prompt Guard 2 on five different text generation models for malicious text and code generation. To the best of our knowledge, this is the first work to reveal that Llama Prompt Guard 2 can be compromised through joint optimization.\n  Additionally, by analyzing the changing similarity of a model's internal state to specific concept directions during token sequence processing, we propose an effective and lightweight method to detect Super Suffix attacks. We show that the cosine similarity between the residual stream and certain concept directions serves as a distinctive fingerprint of model intent. Our proposed countermeasure, DeltaGuard, significantly improves the detection of malicious prompts generated through Super Suffixes. It increases the non-benign classification rate to nearly 100%, making DeltaGuard a valuable addition to the guard model stack and enhancing robustness against adversarial prompt attacks.",
    "categories": [
      "cs.CR",
      "cs.AI"
    ],
    "primary_category": "cs.CR",
    "comment": "13 pages, 5 Figures",
    "pdf_url": "https://arxiv.org/pdf/2512.11783v1",
    "published_date": "2025-12-12 18:52:09 UTC",
    "updated_date": "2025-12-12 18:52:09 UTC"
  },
  {
    "arxiv_id": "2512.11781v1",
    "title": "Agile Flight Emerges from Multi-Agent Competitive Racing",
    "authors": [
      "Vineet Pasumarti",
      "Lorenzo Bianchi",
      "Antonio Loquercio"
    ],
    "abstract": "Through multi-agent competition and the sparse high-level objective of winning a race, we find that both agile flight (e.g., high-speed motion pushing the platform to its physical limits) and strategy (e.g., overtaking or blocking) emerge from agents trained with reinforcement learning. We provide evidence in both simulation and the real world that this approach outperforms the common paradigm of training agents in isolation with rewards that prescribe behavior, e.g., progress on the raceline, in particular when the complexity of the environment increases, e.g., in the presence of obstacles. Moreover, we find that multi-agent competition yields policies that transfer more reliably to the real world than policies trained with a single-agent progress-based reward, despite the two methods using the same simulation environment, randomization strategy, and hardware. In addition to improved sim-to-real transfer, the multi-agent policies also exhibit some degree of generalization to opponents unseen at training time. Overall, our work, following in the tradition of multi-agent competitive game-play in digital domains, shows that sparse task-level rewards are sufficient for training agents capable of advanced low-level control in the physical world.\n  Code: https://github.com/Jirl-upenn/AgileFlight_MultiAgent",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.MA"
    ],
    "primary_category": "cs.RO",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2512.11781v1",
    "published_date": "2025-12-12 18:48:50 UTC",
    "updated_date": "2025-12-12 18:48:50 UTC"
  },
  {
    "arxiv_id": "2512.11779v1",
    "title": "Conditional Coverage Diagnostics for Conformal Prediction",
    "authors": [
      "Sacha Braun",
      "David Holzmüller",
      "Michael I. Jordan",
      "Francis Bach"
    ],
    "abstract": "Evaluating conditional coverage remains one of the most persistent challenges in assessing the reliability of predictive systems. Although conformal methods can give guarantees on marginal coverage, no method can guarantee to produce sets with correct conditional coverage, leaving practitioners without a clear way to interpret local deviations. To overcome sample-inefficiency and overfitting issues of existing metrics, we cast conditional coverage estimation as a classification problem. Conditional coverage is violated if and only if any classifier can achieve lower risk than the target coverage. Through the choice of a (proper) loss function, the resulting risk difference gives a conservative estimate of natural miscoverage measures such as L1 and L2 distance, and can even separate the effects of over- and under-coverage, and non-constant target coverages. We call the resulting family of metrics excess risk of the target coverage (ERT). We show experimentally that the use of modern classifiers provides much higher statistical power than simple classifiers underlying established metrics like CovGap. Additionally, we use our metric to benchmark different conformal prediction methods. Finally, we release an open-source package for ERT as well as previous conditional coverage metrics. Together, these contributions provide a new lens for understanding, diagnosing, and improving the conditional reliability of predictive systems.",
    "categories": [
      "stat.ML",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "stat.ML",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2512.11779v1",
    "published_date": "2025-12-12 18:47:39 UTC",
    "updated_date": "2025-12-12 18:47:39 UTC"
  },
  {
    "arxiv_id": "2512.11771v2",
    "title": "Smudged Fingerprints: A Systematic Evaluation of the Robustness of AI Image Fingerprints",
    "authors": [
      "Kai Yao",
      "Marc Juarez"
    ],
    "abstract": "Model fingerprint detection has shown promise to trace the provenance of AI-generated images in forensic applications. However, despite the inherent adversarial nature of these applications, existing evaluations rarely consider adversarial settings. We present the first systematic security evaluation of these techniques, formalizing threat models that encompass both white- and black-box access and two attack goals: fingerprint removal, which erases identifying traces to evade attribution, and fingerprint forgery, which seeks to cause misattribution to a target model. We implement five attack strategies and evaluate 14 representative fingerprinting methods across RGB, frequency, and learned-feature domains on 12 state-of-the-art image generators. Our experiments reveal a pronounced gap between clean and adversarial performance. Removal attacks are highly effective, often achieving success rates above 80% in white-box settings and over 50% under black-box access. While forgery is more challenging than removal, its success varies significantly across targeted models. We also observe a utility-robustness trade-off: accurate attribution methods are often vulnerable to attacks and, although some techniques are robust in specific settings, none achieves robustness and accuracy across all evaluated threat models. These findings highlight the need for techniques that balance robustness and accuracy, and we identify the most promising approaches toward this goal. Code available at: https://github.com/kaikaiyao/SmudgedFingerprints.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "This work has been accepted for publication in the 4th IEEE Conference on Secure and Trustworthy Machine Learning (IEEE SaTML 2026). The final version will be available on IEEE Xplore",
    "pdf_url": "https://arxiv.org/pdf/2512.11771v2",
    "published_date": "2025-12-12 18:33:14 UTC",
    "updated_date": "2026-01-21 06:01:15 UTC"
  },
  {
    "arxiv_id": "2512.15768v1",
    "title": "PHANTOM: Progressive High-fidelity Adversarial Network for Threat Object Modeling",
    "authors": [
      "Jamal Al-Karaki",
      "Muhammad Al-Zafar Khan",
      "Rand Derar Mohammad Al Athamneh"
    ],
    "abstract": "The scarcity of cyberattack data hinders the development of robust intrusion detection systems. This paper introduces PHANTOM, a novel adversarial variational framework for generating high-fidelity synthetic attack data. Its innovations include progressive training, a dual-path VAE-GAN architecture, and domain-specific feature matching to preserve the semantics of attacks. Evaluated on 100,000 network traffic samples, models trained on PHANTOM data achieve 98% weighted accuracy on real attacks. Statistical analyses confirm that the synthetic data preserves authentic distributions and diversity. Limitations in generating rare attack types are noted, highlighting challenges with severe class imbalance. This work advances the generation of synthetic data for training robust, privacy-preserving detection systems.",
    "categories": [
      "cs.CR",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CR",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2512.15768v1",
    "published_date": "2025-12-12 18:14:19 UTC",
    "updated_date": "2025-12-12 18:14:19 UTC"
  },
  {
    "arxiv_id": "2512.11748v1",
    "title": "Generative Parametric Design (GPD): A framework for real-time geometry generation and on-the-fly multiparametric approximation",
    "authors": [
      "Mohammed El Fallaki Idrissi",
      "Jad Mounayer",
      "Sebastian Rodriguez",
      "Fodil Meraghni",
      "Francisco Chinesta"
    ],
    "abstract": "This paper presents a novel paradigm in simulation-based engineering sciences by introducing a new framework called Generative Parametric Design (GPD). The GPD framework enables the generation of new designs along with their corresponding parametric solutions given as a reduced basis. To achieve this, two Rank Reduction Autoencoders (RRAEs) are employed, one for encoding and generating the design or geometry, and the other for encoding the sparse Proper Generalized Decomposition (sPGD) mode solutions. These models are linked in the latent space using regression techniques, allowing efficient transitions between design and their associated sPGD modes. By empowering design exploration and optimization, this framework also advances digital and hybrid twin development, enhancing predictive modeling and real-time decision-making in engineering applications. The developed framework is demonstrated on two-phase microstructures, in which the multiparametric solutions account for variations in two key material parameters.",
    "categories": [
      "cs.CE",
      "cs.AI"
    ],
    "primary_category": "cs.CE",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2512.11748v1",
    "published_date": "2025-12-12 17:44:38 UTC",
    "updated_date": "2025-12-12 17:44:38 UTC"
  },
  {
    "arxiv_id": "2512.11743v1",
    "title": "CogniSNN: Enabling Neuron-Expandability, Pathway-Reusability, and Dynamic-Configurability with Random Graph Architectures in Spiking Neural Networks",
    "authors": [
      "Yongsheng Huang",
      "Peibo Duan",
      "Yujie Wu",
      "Kai Sun",
      "Zhipeng Liu",
      "Changsheng Zhang",
      "Bin Zhang",
      "Mingkun Xu"
    ],
    "abstract": "Spiking neural networks (SNNs), regarded as the third generation of artificial neural networks, are expected to bridge the gap between artificial intelligence and computational neuroscience. However, most mainstream SNN research directly adopts the rigid, chain-like hierarchical architecture of traditional artificial neural networks (ANNs), ignoring key structural characteristics of the brain. Biological neurons are stochastically interconnected, forming complex neural pathways that exhibit Neuron-Expandability, Pathway-Reusability, and Dynamic-Configurability. In this paper, we introduce a new SNN paradigm, named Cognition-aware SNN (CogniSNN), by incorporating Random Graph Architecture (RGA). Furthermore, we address the issues of network degradation and dimensional mismatch in deep pathways by introducing an improved pure spiking residual mechanism alongside an adaptive pooling strategy. Then, we design a Key Pathway-based Learning without Forgetting (KP-LwF) approach, which selectively reuses critical neural pathways while retaining historical knowledge, enabling efficient multi-task transfer. Finally, we propose a Dynamic Growth Learning (DGL) algorithm that allows neurons and synapses to grow dynamically along the internal temporal dimension. Extensive experiments demonstrate that CogniSNN achieves performance comparable to, or even surpassing, current state-of-the-art SNNs on neuromorphic datasets and Tiny-ImageNet. The Pathway-Reusability enhances the network's continuous learning capability across different scenarios, while the dynamic growth algorithm improves robustness against interference and mitigates the fixed-timestep constraints during neuromorphic chip deployment. This work demonstrates the potential of SNNs with random graph structures in advancing brain-inspired intelligence and lays the foundation for their practical application on neuromorphic hardware.",
    "categories": [
      "cs.NE",
      "cs.AI"
    ],
    "primary_category": "cs.NE",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2512.11743v1",
    "published_date": "2025-12-12 17:36:31 UTC",
    "updated_date": "2025-12-12 17:36:31 UTC"
  },
  {
    "arxiv_id": "2512.14744v1",
    "title": "VERAFI: Verified Agentic Financial Intelligence through Neurosymbolic Policy Generation",
    "authors": [
      "Adewale Akinfaderin",
      "Shreyas Subramanian"
    ],
    "abstract": "Financial AI systems suffer from a critical blind spot: while Retrieval-Augmented Generation (RAG) excels at finding relevant documents, language models still generate calculation errors and regulatory violations during reasoning, even with perfect retrieval. This paper introduces VERAFI (Verified Agentic Financial Intelligence), an agentic framework with neurosymbolic policy generation for verified financial intelligence. VERAFI combines state-of-the-art dense retrieval and cross-encoder reranking with financial tool-enabled agents and automated reasoning policies covering GAAP compliance, SEC requirements, and mathematical validation. Our comprehensive evaluation on FinanceBench demonstrates remarkable improvements: while traditional dense retrieval with reranking achieves only 52.4\\% factual correctness, VERAFI's integrated approach reaches 94.7\\%, an 81\\% relative improvement. The neurosymbolic policy layer alone contributes a 4.3 percentage point gain over pure agentic processing, specifically targeting persistent mathematical and logical errors. By integrating financial domain expertise directly into the reasoning process, VERAFI offers a practical pathway toward trustworthy financial AI that meets the stringent accuracy demands of regulatory compliance, investment decisions, and risk management.",
    "categories": [
      "q-fin.CP",
      "cs.AI"
    ],
    "primary_category": "q-fin.CP",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2512.14744v1",
    "published_date": "2025-12-12 17:17:43 UTC",
    "updated_date": "2025-12-12 17:17:43 UTC"
  },
  {
    "arxiv_id": "2512.11724v2",
    "title": "From Signal to Turn: Interactional Friction in Modular Speech-to-Speech Pipelines",
    "authors": [
      "Tittaya Mairittha",
      "Tanakon Sawanglok",
      "Panuwit Raden",
      "Jirapast Buntub",
      "Thanapat Warunee",
      "Napat Asawachaisuvikrom",
      "Thanaphum Saiwongin"
    ],
    "abstract": "While voice-based AI systems have achieved remarkable generative capabilities, their interactions often feel conversationally broken. This paper examines the interactional friction that emerges in modular Speech-to-Speech Retrieval-Augmented Generation (S2S-RAG) pipelines. By analyzing a representative production system, we move beyond simple latency metrics to identify three recurring patterns of conversational breakdown: (1) Temporal Misalignment, where system delays violate user expectations of conversational rhythm; (2) Expressive Flattening, where the loss of paralinguistic cues leads to literal, inappropriate responses; and (3) Repair Rigidity, where architectural gating prevents users from correcting errors in real-time. Through system-level analysis, we demonstrate that these friction points should not be understood as defects or failures, but as structural consequences of a modular design that prioritizes control over fluidity. We conclude that building natural spoken AI is an infrastructure design challenge, requiring a shift from optimizing isolated components to carefully choreographing the seams between them.",
    "categories": [
      "cs.HC",
      "cs.AI",
      "cs.CL",
      "cs.SE"
    ],
    "primary_category": "cs.HC",
    "comment": "6 pages, 1 figure",
    "pdf_url": "https://arxiv.org/pdf/2512.11724v2",
    "published_date": "2025-12-12 17:05:11 UTC",
    "updated_date": "2025-12-17 12:31:02 UTC"
  },
  {
    "arxiv_id": "2512.15767v1",
    "title": "Bridging Data and Physics: A Graph Neural Network-Based Hybrid Twin Framework",
    "authors": [
      "M. Gorpinich",
      "B. Moya",
      "S. Rodriguez",
      "F. Meraghni",
      "Y. Jaafra",
      "A. Briot",
      "M. Henner",
      "R. Leon",
      "F. Chinesta"
    ],
    "abstract": "Simulating complex unsteady physical phenomena relies on detailed mathematical models, simulated for instance by using the Finite Element Method (FEM). However, these models often exhibit discrepancies from the reality due to unmodeled effects or simplifying assumptions. We refer to this gap as the ignorance model. While purely data-driven approaches attempt to learn full system behavior, they require large amounts of high-quality data across the entire spatial and temporal domain. In real-world scenarios, such information is unavailable, making full data-driven modeling unreliable. To overcome this limitation, we model of the ignorance component using a hybrid twin approach, instead of simulating phenomena from scratch. Since physics-based models approximate the overall behavior of the phenomena, the remaining ignorance is typically lower in complexity than the full physical response, therefore, it can be learned with significantly fewer data. A key difficulty, however, is that spatial measurements are sparse, also obtaining data measuring the same phenomenon for different spatial configurations is challenging in practice. Our contribution is to overcome this limitation by using Graph Neural Networks (GNNs) to represent the ignorance model. GNNs learn the spatial pattern of the missing physics even when the number of measurement locations is limited. This allows us to enrich the physics-based model with data-driven corrections without requiring dense spatial, temporal and parametric data. To showcase the performance of the proposed method, we evaluate this GNN-based hybrid twin on nonlinear heat transfer problems across different meshes, geometries, and load positions. Results show that the GNN successfully captures the ignorance and generalizes corrections across spatial configurations, improving simulation accuracy and interpretability, while minimizing data requirements.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "27 pages, 14 figures",
    "pdf_url": "https://arxiv.org/pdf/2512.15767v1",
    "published_date": "2025-12-12 16:54:56 UTC",
    "updated_date": "2025-12-12 16:54:56 UTC"
  },
  {
    "arxiv_id": "2512.11682v1",
    "title": "MedAI: Evaluating TxAgent's Therapeutic Agentic Reasoning in the NeurIPS CURE-Bench Competition",
    "authors": [
      "Tim Cofala",
      "Christian Kalfar",
      "Jingge Xiao",
      "Johanna Schrader",
      "Michelle Tang",
      "Wolfgang Nejdl"
    ],
    "abstract": "Therapeutic decision-making in clinical medicine constitutes a high-stakes domain in which AI guidance interacts with complex interactions among patient characteristics, disease processes, and pharmacological agents. Tasks such as drug recommendation, treatment planning, and adverse-effect prediction demand robust, multi-step reasoning grounded in reliable biomedical knowledge. Agentic AI methods, exemplified by TxAgent, address these challenges through iterative retrieval-augmented generation (RAG). TxAgent employs a fine-tuned Llama-3.1-8B model that dynamically generates and executes function calls to a unified biomedical tool suite (ToolUniverse), integrating FDA Drug API, OpenTargets, and Monarch resources to ensure access to current therapeutic information. In contrast to general-purpose RAG systems, medical applications impose stringent safety constraints, rendering the accuracy of both the reasoning trace and the sequence of tool invocations critical. These considerations motivate evaluation protocols treating token-level reasoning and tool-usage behaviors as explicit supervision signals. This work presents insights derived from our participation in the CURE-Bench NeurIPS 2025 Challenge, which benchmarks therapeutic-reasoning systems using metrics that assess correctness, tool utilization, and reasoning quality. We analyze how retrieval quality for function (tool) calls influences overall model performance and demonstrate performance gains achieved through improved tool-retrieval strategies. Our work was awarded the Excellence Award in Open Science. Complete information can be found at https://curebench.ai/.",
    "categories": [
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "7 pages, 3 figures",
    "pdf_url": "https://arxiv.org/pdf/2512.11682v1",
    "published_date": "2025-12-12 16:01:48 UTC",
    "updated_date": "2025-12-12 16:01:48 UTC"
  },
  {
    "arxiv_id": "2512.11661v1",
    "title": "From Verification Burden to Trusted Collaboration: Design Goals for LLM-Assisted Literature Reviews",
    "authors": [
      "Brenda Nogueira",
      "Werner Geyer",
      "Andrew Anderson",
      "Toby Jia-Jun Li",
      "Dongwhi Kim",
      "Nuno Moniz",
      "Nitesh V. Chawla"
    ],
    "abstract": "Large Language Models (LLMs) are increasingly embedded in academic writing practices. Although numerous studies have explored how researchers employ these tools for scientific writing, their concrete implementation, limitations, and design challenges within the literature review process remain underexplored. In this paper, we report a user study with researchers across multiple disciplines to characterize current practices, benefits, and \\textit{pain points} in using LLMs to investigate related work. We identified three recurring gaps: (i) lack of trust in outputs, (ii) persistent verification burden, and (iii) requiring multiple tools. This motivates our proposal of six design goals and a high-level framework that operationalizes them through improved related papers visualization, verification at every step, and human-feedback alignment with generation-guided explanations. Overall, by grounding our work in the practical, day-to-day needs of researchers, we designed a framework that addresses these limitations and models real-world LLM-assisted writing, advancing trust through verifiable actions and fostering practical collaboration between researchers and AI systems.",
    "categories": [
      "cs.HC",
      "cs.AI"
    ],
    "primary_category": "cs.HC",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2512.11661v1",
    "published_date": "2025-12-12 15:38:34 UTC",
    "updated_date": "2025-12-12 15:38:34 UTC"
  },
  {
    "arxiv_id": "2512.11653v2",
    "title": "Causal Inference in Energy Demand Prediction",
    "authors": [
      "Chutian Ma",
      "Grigorii Pomazkin",
      "Giacinto Paolo Saggese",
      "Paul Smith"
    ],
    "abstract": "Energy demand prediction is critical for grid operators, industrial energy consumers, and service providers. Energy demand is influenced by multiple factors, including weather conditions (e.g. temperature, humidity, wind speed, solar radiation), and calendar information (e.g. hour of day and month of year), which further affect daily work and life schedules. These factors are causally interdependent, making the problem more complex than simple correlation-based learning techniques satisfactorily allow for. We propose a structural causal model that explains the causal relationship between these variables. A full analysis is performed to validate our causal beliefs, also revealing important insights consistent with prior studies. For example, our causal model reveals that energy demand responds to temperature fluctuations with season-dependent sensitivity. Additionally, we find that energy demand exhibits lower variance in winter due to the decoupling effect between temperature changes and daily activity patterns. We then build a Bayesian model, which takes advantage of the causal insights we learned as prior knowledge. The model is trained and tested on unseen data and yields state-of-the-art performance in the form of a 3.84 percent MAPE on the test set. The model also demonstrates strong robustness, as the cross-validation across two years of data yields an average MAPE of 3.88 percent.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2512.11653v2",
    "published_date": "2025-12-12 15:30:46 UTC",
    "updated_date": "2025-12-17 02:23:17 UTC"
  },
  {
    "arxiv_id": "2512.11946v1",
    "title": "Data-Driven Global Sensitivity Analysis for Engineering Design Based on Individual Conditional Expectations",
    "authors": [
      "Pramudita Satria Palar",
      "Paul Saves",
      "Rommel G. Regis",
      "Koji Shimoyama",
      "Shigeru Obayashi",
      "Nicolas Verstaevel",
      "Joseph Morlier"
    ],
    "abstract": "Explainable machine learning techniques have gained increasing attention in engineering applications, especially in aerospace design and analysis, where understanding how input variables influence data-driven models is essential. Partial Dependence Plots (PDPs) are widely used for interpreting black-box models by showing the average effect of an input variable on the prediction. However, their global sensitivity metric can be misleading when strong interactions are present, as averaging tends to obscure interaction effects. To address this limitation, we propose a global sensitivity metric based on Individual Conditional Expectation (ICE) curves. The method computes the expected feature importance across ICE curves, along with their standard deviation, to more effectively capture the influence of interactions. We provide a mathematical proof demonstrating that the PDP-based sensitivity is a lower bound of the proposed ICE-based metric under truncated orthogonal polynomial expansion. In addition, we introduce an ICE-based correlation value to quantify how interactions modify the relationship between inputs and the output. Comparative evaluations were performed on three cases: a 5-variable analytical function, a 5-variable wind-turbine fatigue problem, and a 9-variable airfoil aerodynamics case, where ICE-based sensitivity was benchmarked against PDP, SHapley Additive exPlanations (SHAP), and Sobol' indices. The results show that ICE-based feature importance provides richer insights than the traditional PDP-based approach, while visual interpretations from PDP, ICE, and SHAP complement one another by offering multiple perspectives.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2512.11946v1",
    "published_date": "2025-12-12 15:28:17 UTC",
    "updated_date": "2025-12-12 15:28:17 UTC"
  },
  {
    "arxiv_id": "2512.11635v1",
    "title": "Automating Historical Insight Extraction from Large-Scale Newspaper Archives via Neural Topic Modeling",
    "authors": [
      "Keerthana Murugaraj",
      "Salima Lamsiyah",
      "Marten During",
      "Martin Theobald"
    ],
    "abstract": "Extracting coherent and human-understandable themes from large collections of unstructured historical newspaper archives presents significant challenges due to topic evolution, Optical Character Recognition (OCR) noise, and the sheer volume of text. Traditional topic-modeling methods, such as Latent Dirichlet Allocation (LDA), often fall short in capturing the complexity and dynamic nature of discourse in historical texts. To address these limitations, we employ BERTopic. This neural topic-modeling approach leverages transformerbased embeddings to extract and classify topics, which, despite its growing popularity, still remains underused in historical research. Our study focuses on articles published between 1955 and 2018, specifically examining discourse on nuclear power and nuclear safety. We analyze various topic distributions across the corpus and trace their temporal evolution to uncover long-term trends and shifts in public discourse. This enables us to more accurately explore patterns in public discourse, including the co-occurrence of themes related to nuclear power and nuclear weapons and their shifts in topic importance over time. Our study demonstrates the scalability and contextual sensitivity of BERTopic as an alternative to traditional approaches, offering richer insights into historical discourses extracted from newspaper archives. These findings contribute to historical, nuclear, and social-science research while reflecting on current limitations and proposing potential directions for future work.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.IR"
    ],
    "primary_category": "cs.CL",
    "comment": "This is a preprint of a manuscript submitted to Digital Scholarship in the Humanities (Oxford University Press). The paper is currently under peer review",
    "pdf_url": "https://arxiv.org/pdf/2512.11635v1",
    "published_date": "2025-12-12 15:15:02 UTC",
    "updated_date": "2025-12-12 15:15:02 UTC"
  },
  {
    "arxiv_id": "2512.14742v1",
    "title": "Quantum-Augmented AI/ML for O-RAN: Hierarchical Threat Detection with Synergistic Intelligence and Interpretability (Technical Report)",
    "authors": [
      "Tan Le",
      "Van Le",
      "Sachin Shetty"
    ],
    "abstract": "Open Radio Access Networks (O-RAN) enhance modularity and telemetry granularity but also widen the cybersecurity attack surface across disaggregated control, user and management planes. We propose a hierarchical defense framework with three coordinated layers-anomaly detection, intrusion confirmation, and multiattack classification-each aligned with O-RAN's telemetry stack. Our approach integrates hybrid quantum computing and machine learning, leveraging amplitude- and entanglement-based feature encodings with deep and ensemble classifiers. We conduct extensive benchmarking across synthetic and real-world telemetry, evaluating encoding depth, architectural variants, and diagnostic fidelity. The framework consistently achieves near-perfect accuracy, high recall, and strong class separability. Multi-faceted evaluation across decision boundaries, probabilistic margins, and latent space geometry confirms its interpretability, robustness, and readiness for slice-aware diagnostics and scalable deployment in near-RT and non-RT RIC domains.",
    "categories": [
      "cs.CR",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CR",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2512.14742v1",
    "published_date": "2025-12-12 15:12:57 UTC",
    "updated_date": "2025-12-12 15:12:57 UTC"
  },
  {
    "arxiv_id": "2512.11614v1",
    "title": "Bounding Hallucinations: Information-Theoretic Guarantees for RAG Systems via Merlin-Arthur Protocols",
    "authors": [
      "Björn Deiseroth",
      "Max Henning Höth",
      "Kristian Kersting",
      "Letitia Parcalabescu"
    ],
    "abstract": "Retrieval-augmented generation (RAG) models rely on retrieved evidence to guide large language model (LLM) generators, yet current systems treat retrieval as a weak heuristic rather than verifiable evidence. As a result, LLMs answer without support, hallucinate under incomplete or misleading context, and rely on spurious evidence. We introduce a training framework that treats the entire RAG pipeline -- both the retriever and the generator -- as an interactive proof system via an adaptation of the Merlin-Arthur (M/A) protocol. Arthur (the generator LLM) trains on questions of unkown provenance: Merlin provides helpful evidence, while Morgana injects adversarial, misleading context. Both use a linear-time XAI method to identify and modify the evidence most influential to Arthur. Consequently, Arthur learns to (i) answer when the context support the answer, (ii) reject when evidence is insufficient, and (iii) rely on the specific context spans that truly ground the answer. We further introduce a rigorous evaluation framework to disentangle explanation fidelity from baseline predictive errors. This allows us to introduce and measure the Explained Information Fraction (EIF), which normalizes M/A certified mutual-information guarantees relative to model capacity and imperfect benchmarks. Across three RAG datasets and two model families of varying sizes, M/A-trained LLMs show improved groundedness, completeness, soundness, and reject behavior, as well as reduced hallucinations -- without needing manually annotated unanswerable questions. The retriever likewise improves recall and MRR through automatically generated M/A hard positives and negatives. Our results demonstrate that autonomous interactive-proof-style supervision provides a principled and practical path toward reliable RAG systems that treat retrieved documents not as suggestions, but as verifiable evidence.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "34 pages, 19 figures",
    "pdf_url": "https://arxiv.org/pdf/2512.11614v1",
    "published_date": "2025-12-12 14:50:38 UTC",
    "updated_date": "2025-12-12 14:50:38 UTC"
  },
  {
    "arxiv_id": "2512.11588v1",
    "title": "AI Benchmark Democratization and Carpentry",
    "authors": [
      "Gregor von Laszewski",
      "Wesley Brewer",
      "Jeyan Thiyagalingam",
      "Juri Papay",
      "Armstrong Foundjem",
      "Piotr Luszczek",
      "Murali Emani",
      "Shirley V. Moore",
      "Vijay Janapa Reddi",
      "Matthew D. Sinclair",
      "Sebastian Lobentanzer",
      "Sujata Goswami",
      "Benjamin Hawks",
      "Marco Colombo",
      "Nhan Tran",
      "Christine R. Kirkpatrick",
      "Abdulkareem Alsudais",
      "Gregg Barrett",
      "Tianhao Li",
      "Kirsten Morehouse",
      "Shivaram Venkataraman",
      "Rutwik Jain",
      "Kartik Mathur",
      "Victor Lu",
      "Tejinder Singh",
      "Khojasteh Z. Mirza",
      "Kongtao Chen",
      "Sasidhar Kunapuli",
      "Gavin Farrell",
      "Renato Umeton",
      "Geoffrey C. Fox"
    ],
    "abstract": "Benchmarks are a cornerstone of modern machine learning, enabling reproducibility, comparison, and scientific progress. However, AI benchmarks are increasingly complex, requiring dynamic, AI-focused workflows. Rapid evolution in model architectures, scale, datasets, and deployment contexts makes evaluation a moving target. Large language models often memorize static benchmarks, causing a gap between benchmark results and real-world performance.\n  Beyond traditional static benchmarks, continuous adaptive benchmarking frameworks are needed to align scientific assessment with deployment risks. This calls for skills and education in AI Benchmark Carpentry. From our experience with MLCommons, educational initiatives, and programs like the DOE's Trillion Parameter Consortium, key barriers include high resource demands, limited access to specialized hardware, lack of benchmark design expertise, and uncertainty in relating results to application domains. Current benchmarks often emphasize peak performance on top-tier hardware, offering limited guidance for diverse, real-world scenarios.\n  Benchmarking must become dynamic, incorporating evolving models, updated data, and heterogeneous platforms while maintaining transparency, reproducibility, and interpretability. Democratization requires both technical innovation and systematic education across levels, building sustained expertise in benchmark design and use. Benchmarks should support application-relevant comparisons, enabling informed, context-sensitive decisions. Dynamic, inclusive benchmarking will ensure evaluation keeps pace with AI evolution and supports responsible, reproducible, and accessible AI deployment. Community efforts can provide a foundation for AI Benchmark Carpentry.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "43 pages, 2 figures, 7 tables",
    "pdf_url": "https://arxiv.org/pdf/2512.11588v1",
    "published_date": "2025-12-12 14:20:05 UTC",
    "updated_date": "2025-12-12 14:20:05 UTC"
  },
  {
    "arxiv_id": "2512.11584v1",
    "title": "Atomic Action Slicing: Planner-Aligned Options for Generalist VLA Agents",
    "authors": [
      "Stefan Tabakov",
      "Asen Popov",
      "Dimitar Dimitrov",
      "S. Ensiye Kiyamousavi",
      "Vladimir Hristov",
      "Boris Kraychev"
    ],
    "abstract": "Current vision-language-action (VLA) models generalize poorly, particularly when tasks require new compositions of skills or objects. We introduce Atomic Action Slicing (AAS), a planner-aligned approach that decomposes long-horizon demonstrations into short, typed atomic actions that are easier for planners to use and policies to learn. Using LIBERO demonstrations, AAS produces a validated dataset of 2,124 atomic segments labeled with action type, temporal span, and confidence. A stronger segmenter (Gemini 2.5 Pro) closely matches planner-defined plans and remains robust under keyframe jitter, while smaller models perform worse on multi-object tasks. Fine-tuning CLIP-RT+ on our atomic dataset improves task success from 94.2% to 95.3% on LIBERO-Goal and 83.8% to 88.8% on LIBERO-Long. We publicly release the GATE-VLAP dataset on HuggingFace(https://huggingface.co/datasets/gate-institute/GATE-VLAP-datasets)",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.RO"
    ],
    "primary_category": "cs.LG",
    "comment": "The 41st ACM/SIGAPP Symposium On Applied Computing",
    "pdf_url": "https://arxiv.org/pdf/2512.11584v1",
    "published_date": "2025-12-12 14:14:27 UTC",
    "updated_date": "2025-12-12 14:14:27 UTC"
  },
  {
    "arxiv_id": "2512.11944v1",
    "title": "A Review of Learning-Based Motion Planning: Toward a Data-Driven Optimal Control Approach",
    "authors": [
      "Jia Hu",
      "Yang Chang",
      "Haoran Wang"
    ],
    "abstract": "Motion planning for high-level autonomous driving is constrained by a fundamental trade-off between the transparent, yet brittle, nature of pipeline methods and the adaptive, yet opaque, \"black-box\" characteristics of modern learning-based systems. This paper critically synthesizes the evolution of the field -- from pipeline methods through imitation learning, reinforcement learning, and generative AI -- to demonstrate how this persistent dilemma has hindered the development of truly trustworthy systems. To resolve this impasse, we conduct a comprehensive review of learning-based motion planning methods. Based on this review, we outline a data-driven optimal control paradigm as a unifying framework that synergistically integrates the verifiable structure of classical control with the adaptive capacity of machine learning, leveraging real-world data to continuously refine key components such as system dynamics, cost functions, and safety constraints. We explore this framework's potential to enable three critical next-generation capabilities: \"Human-Centric\" customization, \"Platform-Adaptive\" dynamics adaptation, and \"System Self-Optimization\" via self-tuning. We conclude by proposing future research directions based on this paradigm, aimed at developing intelligent transportation systems that are simultaneously safe, interpretable, and capable of human-like autonomy.",
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "primary_category": "cs.RO",
    "comment": "34 pages, 11 figures",
    "pdf_url": "https://arxiv.org/pdf/2512.11944v1",
    "published_date": "2025-12-12 14:01:24 UTC",
    "updated_date": "2025-12-12 14:01:24 UTC"
  },
  {
    "arxiv_id": "2512.11560v1",
    "title": "Multi-temporal Calving Front Segmentation",
    "authors": [
      "Marcel Dreier",
      "Nora Gourmelon",
      "Dakota Pyles",
      "Fei Wu",
      "Matthias Braun",
      "Thorsten Seehaus",
      "Andreas Maier",
      "Vincent Christlein"
    ],
    "abstract": "The calving fronts of marine-terminating glaciers undergo constant changes. These changes significantly affect the glacier's mass and dynamics, demanding continuous monitoring. To address this need, deep learning models were developed that can automatically delineate the calving front in Synthetic Aperture Radar imagery. However, these models often struggle to correctly classify areas affected by seasonal conditions such as ice melange or snow-covered surfaces. To address this issue, we propose to process multiple frames from a satellite image time series of the same glacier in parallel and exchange temporal information between the corresponding feature maps to stabilize each prediction. We integrate our approach into the current state-of-the-art architecture Tyrion and accomplish a new state-of-the-art performance on the CaFFe benchmark dataset. In particular, we achieve a Mean Distance Error of 184.4 m and a mean Intersection over Union of 83.6.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2512.11560v1",
    "published_date": "2025-12-12 13:45:05 UTC",
    "updated_date": "2025-12-12 13:45:05 UTC"
  },
  {
    "arxiv_id": "2512.11558v1",
    "title": "DentalGPT: Incentivizing Multimodal Complex Reasoning in Dentistry",
    "authors": [
      "Zhenyang Cai",
      "Jiaming Zhang",
      "Junjie Zhao",
      "Ziyi Zeng",
      "Yanchao Li",
      "Jingyi Liang",
      "Junying Chen",
      "Yunjin Yang",
      "Jiajun You",
      "Shuzhi Deng",
      "Tongfei Wang",
      "Wanting Chen",
      "Chunxiu Hao",
      "Ruiqi Xie",
      "Zhenwei Wen",
      "Xiangyi Feng",
      "Zou Ting",
      "Jin Zou Lin",
      "Jianquan Li",
      "Guangjun Yu",
      "Liangyi Chen",
      "Junwen Wang",
      "Shan Jiang",
      "Benyou Wang"
    ],
    "abstract": "Reliable interpretation of multimodal data in dentistry is essential for automated oral healthcare, yet current multimodal large language models (MLLMs) struggle to capture fine-grained dental visual details and lack sufficient reasoning ability for precise diagnosis. To address these limitations, we present DentalGPT, a specialized dental MLLM developed through high-quality domain knowledge injection and reinforcement learning. Specifically, the largest annotated multimodal dataset for dentistry to date was constructed by aggregating over 120k dental images paired with detailed descriptions that highlight diagnostically relevant visual features, making it the multimodal dataset with the most extensive collection of dental images to date. Training on this dataset significantly enhances the MLLM's visual understanding of dental conditions, while the subsequent reinforcement learning stage further strengthens its capability for multimodal complex reasoning. Comprehensive evaluations on intraoral and panoramic benchmarks, along with dental subsets of medical VQA benchmarks, show that DentalGPT achieves superior performance in disease classification and dental VQA tasks, outperforming many state-of-the-art MLLMs despite having only 7B parameters. These results demonstrate that high-quality dental data combined with staged adaptation provides an effective pathway for building capable and domain-specialized dental MLLMs.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2512.11558v1",
    "published_date": "2025-12-12 13:42:57 UTC",
    "updated_date": "2025-12-12 13:42:57 UTC"
  },
  {
    "arxiv_id": "2512.11546v1",
    "title": "Optimizing the Training Diet: Data Mixture Search for Robust Time Series Forecasting",
    "authors": [
      "Federico Pennino",
      "Maurizio Gabbrielli"
    ],
    "abstract": "The standard paradigm for training deep learning models on sensor data assumes that more data is always better. However, raw sensor streams are often imbalanced and contain significant redundancy, meaning that not all data points contribute equally to model generalization. In this paper, we show that, in some cases, \"less is more\" when considering datasets. We do this by reframing the data selection problem: rather than tuning model hyperparameters, we fix the model and optimize the composition of the training data itself. We introduce a framework for discovering the optimal \"training diet\" from a large, unlabeled time series corpus. Our framework first uses a large-scale encoder and k-means clustering to partition the dataset into distinct, behaviorally consistent clusters. These clusters represent the fundamental 'ingredients' available for training. We then employ the Optuna optimization framework to search the high-dimensional space of possible data mixtures. For each trial, Optuna proposes a specific sampling ratio for each cluster, and a new training set is constructed based on this recipe. A smaller target model is then trained and evaluated. Our experiments reveal that this data-centric search consistently discovers data mixtures that yield models with significantly higher performance compared to baselines trained on the entire dataset. Specifically - evaluated on PMSM dataset - our method improved performance from a baseline MSE of 1.70 to 1.37, a 19.41% improvement.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted ACM SAC 2026",
    "pdf_url": "https://arxiv.org/pdf/2512.11546v1",
    "published_date": "2025-12-12 13:26:07 UTC",
    "updated_date": "2025-12-12 13:26:07 UTC"
  },
  {
    "arxiv_id": "2512.11545v1",
    "title": "Graph Embedding with Mel-spectrograms for Underwater Acoustic Target Recognition",
    "authors": [
      "Sheng Feng",
      "Shuqing Ma",
      "Xiaoqian Zhu"
    ],
    "abstract": "Underwater acoustic target recognition (UATR) is extremely challenging due to the complexity of ship-radiated noise and the variability of ocean environments. Although deep learning (DL) approaches have achieved promising results, most existing models implicitly assume that underwater acoustic data lie in a Euclidean space. This assumption, however, is unsuitable for the inherently complex topology of underwater acoustic signals, which exhibit non-stationary, non-Gaussian, and nonlinear characteristics. To overcome this limitation, this paper proposes the UATR-GTransformer, a non-Euclidean DL model that integrates Transformer architectures with graph neural networks (GNNs). The model comprises three key components: a Mel patchify block, a GTransformer block, and a classification head. The Mel patchify block partitions the Mel-spectrogram into overlapping patches, while the GTransformer block employs a Transformer Encoder to capture mutual information between split patches to generate Mel-graph embeddings. Subsequently, a GNN enhances these embeddings by modeling local neighborhood relationships, and a feed-forward network (FFN) further performs feature transformation. Experiments results based on two widely used benchmark datasets demonstrate that the UATR-GTransformer achieves performance competitive with state-of-the-art methods. In addition, interpretability analysis reveals that the proposed model effectively extracts rich frequency-domain information, highlighting its potential for applications in ocean engineering.",
    "categories": [
      "cs.SD",
      "cs.AI"
    ],
    "primary_category": "cs.SD",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2512.11545v1",
    "published_date": "2025-12-12 13:25:54 UTC",
    "updated_date": "2025-12-12 13:25:54 UTC"
  },
  {
    "arxiv_id": "2512.11544v1",
    "title": "AI-MASLD Metabolic Dysfunction and Information Steatosis of Large Language Models in Unstructured Clinical Narratives",
    "authors": [
      "Yuan Shen",
      "Xiaojun Wu",
      "Linghua Yu"
    ],
    "abstract": "This study aims to simulate real-world clinical scenarios to systematically evaluate the ability of Large Language Models (LLMs) to extract core medical information from patient chief complaints laden with noise and redundancy, and to verify whether they exhibit a functional decline analogous to Metabolic Dysfunction-Associated Steatotic Liver Disease (MASLD). We employed a cross-sectional analysis design based on standardized medical probes, selecting four mainstream LLMs as research subjects: GPT-4o, Gemini 2.5, DeepSeek 3.1, and Qwen3-Max. An evaluation system comprising twenty medical probes across five core dimensions was used to simulate a genuine clinical communication environment. All probes had gold-standard answers defined by clinical experts and were assessed via a double-blind, inverse rating scale by two independent clinicians. The results show that all tested models exhibited functional defects to varying degrees, with Qwen3-Max demonstrating the best overall performance and Gemini 2.5 the worst. Under conditions of extreme noise, most models experienced a functional collapse. Notably, GPT-4o made a severe misjudgment in the risk assessment for pulmonary embolism (PE) secondary to deep vein thrombosis (DVT). This research is the first to empirically confirm that LLMs exhibit features resembling metabolic dysfunction when processing clinical information, proposing the innovative concept of \"AI-Metabolic Dysfunction-Associated Steatotic Liver Disease (AI-MASLD)\". These findings offer a crucial safety warning for the application of Artificial Intelligence (AI) in healthcare, emphasizing that current LLMs must be used as auxiliary tools under human expert supervision, as there remains a significant gap between their theoretical knowledge and practical clinical application.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "47 pages, 2 figures",
    "pdf_url": "https://arxiv.org/pdf/2512.11544v1",
    "published_date": "2025-12-12 13:25:19 UTC",
    "updated_date": "2025-12-12 13:25:19 UTC"
  },
  {
    "arxiv_id": "2512.11532v1",
    "title": "Parallax: Runtime Parallelization for Operator Fallbacks in Heterogeneous Edge Systems",
    "authors": [
      "Chong Tang",
      "Hao Dai",
      "Jagmohan Chauhan"
    ],
    "abstract": "The growing demand for real-time DNN applications on edge devices necessitates faster inference of increasingly complex models. Although many devices include specialized accelerators (e.g., mobile GPUs), dynamic control-flow operators and unsupported kernels often fall back to CPU execution. Existing frameworks handle these fallbacks poorly, leaving CPU cores idle and causing high latency and memory spikes. We introduce Parallax, a framework that accelerates mobile DNN inference without model refactoring or custom operator implementations. Parallax first partitions the computation DAG to expose parallelism, then employs branch-aware memory management with dedicated arenas and buffer reuse to reduce runtime footprint. An adaptive scheduler executes branches according to device memory constraints, meanwhile, fine-grained subgraph control enables heterogeneous inference of dynamic models. By evaluating on five representative DNNs across three different mobile devices, Parallax achieves up to 46% latency reduction, maintains controlled memory overhead (26.5% on average), and delivers up to 30% energy savings compared with state-of-the-art frameworks, offering improvements aligned with the responsiveness demands of real-time mobile inference.",
    "categories": [
      "cs.DC",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.DC",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2512.11532v1",
    "published_date": "2025-12-12 13:07:00 UTC",
    "updated_date": "2025-12-12 13:07:00 UTC"
  },
  {
    "arxiv_id": "2512.11526v1",
    "title": "Contrastive Time Series Forecasting with Anomalies",
    "authors": [
      "Joel Ekstrand",
      "Zahra Taghiyarrenani",
      "Slawomir Nowaczyk"
    ],
    "abstract": "Time series forecasting predicts future values from past data. In real-world settings, some anomalous events have lasting effects and influence the forecast, while others are short-lived and should be ignored. Standard forecasting models fail to make this distinction, often either overreacting to noise or missing persistent shifts. We propose Co-TSFA (Contrastive Time Series Forecasting with Anomalies), a regularization framework that learns when to ignore anomalies and when to respond. Co-TSFA generates input-only and input-output augmentations to model forecast-irrelevant and forecast-relevant anomalies, and introduces a latent-output alignment loss that ties representation changes to forecast changes. This encourages invariance to irrelevant perturbations while preserving sensitivity to meaningful distributional shifts. Experiments on the Traffic and Electricity benchmarks, as well as on a real-world cash-demand dataset, demonstrate that Co-TSFA improves performance under anomalous conditions while maintaining accuracy on normal data. An anonymized GitHub repository with the implementation of Co-TSFA is provided and will be made public upon acceptance.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2512.11526v1",
    "published_date": "2025-12-12 12:54:24 UTC",
    "updated_date": "2025-12-12 12:54:24 UTC"
  },
  {
    "arxiv_id": "2512.11525v1",
    "title": "NeuralOGCM: Differentiable Ocean Modeling with Learnable Physics",
    "authors": [
      "Hao Wu",
      "Yuan Gao",
      "Fan Xu",
      "Fan Zhang",
      "Guangliang Liu",
      "Yuxuan Liang",
      "Xiaomeng Huang"
    ],
    "abstract": "High-precision scientific simulation faces a long-standing trade-off between computational efficiency and physical fidelity. To address this challenge, we propose NeuralOGCM, an ocean modeling framework that fuses differentiable programming with deep learning. At the core of NeuralOGCM is a fully differentiable dynamical solver, which leverages physics knowledge as its core inductive bias. The learnable physics integration captures large-scale, deterministic physical evolution, and transforms key physical parameters (e.g., diffusion coefficients) into learnable parameters, enabling the model to autonomously optimize its physical core via end-to-end training. Concurrently, a deep neural network learns to correct for subgrid-scale processes and discretization errors not captured by the physics model. Both components work in synergy, with their outputs integrated by a unified ODE solver. Experiments demonstrate that NeuralOGCM maintains long-term stability and physical consistency, significantly outperforming traditional numerical models in speed and pure AI baselines in accuracy. Our work paves a new path for building fast, stable, and physically-plausible models for scientific computing.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2512.11525v1",
    "published_date": "2025-12-12 12:53:46 UTC",
    "updated_date": "2025-12-12 12:53:46 UTC"
  },
  {
    "arxiv_id": "2512.11943v1",
    "title": "How AI Agents Follow the Herd of AI? Network Effects, History, and Machine Optimism",
    "authors": [
      "Yu Liu",
      "Wenwen Li",
      "Yifan Dou",
      "Guangnan Ye"
    ],
    "abstract": "Understanding decision-making in multi-AI-agent frameworks is crucial for analyzing strategic interactions in network-effect-driven contexts. This study investigates how AI agents navigate network-effect games, where individual payoffs depend on peer participatio--a context underexplored in multi-agent systems despite its real-world prevalence. We introduce a novel workflow design using large language model (LLM)-based agents in repeated decision-making scenarios, systematically manipulating price trajectories (fixed, ascending, descending, random) and network-effect strength. Our key findings include: First, without historical data, agents fail to infer equilibrium. Second, ordered historical sequences (e.g., escalating prices) enable partial convergence under weak network effects but strong effects trigger persistent \"AI optimism\"--agents overestimate participation despite contradictory evidence. Third, randomized history disrupts convergence entirely, demonstrating that temporal coherence in data shapes LLMs' reasoning, unlike humans. These results highlight a paradigm shift: in AI-mediated systems, equilibrium outcomes depend not just on incentives, but on how history is curated, which is impossible for human.",
    "categories": [
      "cs.MA",
      "cs.AI",
      "econ.GN"
    ],
    "primary_category": "cs.MA",
    "comment": "7 pages, 5 figures",
    "pdf_url": "https://arxiv.org/pdf/2512.11943v1",
    "published_date": "2025-12-12 12:14:48 UTC",
    "updated_date": "2025-12-12 12:14:48 UTC"
  },
  {
    "arxiv_id": "2512.11509v2",
    "title": "Does Less Hallucination Mean Less Creativity? An Empirical Investigation in LLMs",
    "authors": [
      "Mohor Banerjee",
      "Nadya Yuki Wangsajaya",
      "Syed Ali Redha Alsagoff",
      "Min Sen Tan",
      "Zachary Choy Kit Chun",
      "Alvin Chan Guo Wei"
    ],
    "abstract": "Large Language Models (LLMs) exhibit remarkable capabilities in natural language understanding and reasoning, but suffer from hallucination: the generation of factually incorrect content. While numerous methods have been developed to reduce hallucinations, their impact on creative generations remains unexplored. This gap is particularly critical for AI-assisted scientific discovery, which requires both factual accuracy and creative hypothesis generation. We investigate how three hallucination-reduction techniques: Chain of Verification (CoVe), Decoding by Contrasting Layers (DoLa), and Retrieval-Augmented Generation (RAG), affect creativity in LLMs. Evaluating multiple model families (LLaMA, Qwen, Mistral) at varying scales (1B - 70B parameters) on two creativity benchmarks (NeoCoder and CS4), we find that these methods have opposing effects on divergent creativity. CoVe enhances divergent thinking, DoLa suppresses it, and RAG shows minimal impact. Our findings provide guidance for selecting appropriate hallucination-reduction methods in scientific applications, where the balance between factual accuracy and creative exploration is crucial.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted at the AAAI 2026 Workshop on AI for Scientific Research (AI4Research)",
    "pdf_url": "https://arxiv.org/pdf/2512.11509v2",
    "published_date": "2025-12-12 12:14:29 UTC",
    "updated_date": "2026-01-21 12:07:32 UTC"
  },
  {
    "arxiv_id": "2512.11506v2",
    "title": "EmeraldMind: A Knowledge Graph-Augmented Framework for Greenwashing Detection",
    "authors": [
      "Georgios Kaoukis",
      "Ioannis Aris Koufopoulos",
      "Eleni Psaroudaki",
      "Danae Pla Karidi",
      "Evaggelia Pitoura",
      "George Papastefanatos",
      "Panayiotis Tsaparas"
    ],
    "abstract": "As AI and web agents become pervasive in decision-making, it is critical to design intelligent systems that not only support sustainability efforts but also guard against misinformation. Greenwashing, i.e., misleading corporate sustainability claims, poses a major challenge to environmental progress. To address this challenge, we introduce EmeraldMind, a fact-centric framework integrating a domain-specific knowledge graph with retrieval-augmented generation to automate greenwashing detection. EmeraldMind builds the EmeraldGraph from diverse corporate ESG (environmental, social, and governance) reports, surfacing verifiable evidence, often missing in generic knowledge bases, and supporting large language models in claim assessment. The framework delivers justification-centric classifications, presenting transparent, evidence-backed verdicts and abstaining responsibly when claims cannot be verified. Experiments on a new greenwashing claims dataset demonstrate that EmeraldMind achieves competitive accuracy, greater coverage, and superior explanation quality compared to generic LLMs, without the need for fine-tuning or retraining.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2512.11506v2",
    "published_date": "2025-12-12 12:06:36 UTC",
    "updated_date": "2025-12-15 07:14:59 UTC"
  },
  {
    "arxiv_id": "2512.11505v1",
    "title": "BAID: A Benchmark for Bias Assessment of AI Detectors",
    "authors": [
      "Priyam Basu",
      "Yunfeng Zhang",
      "Vipul Raheja"
    ],
    "abstract": "AI-generated text detectors have recently gained adoption in educational and professional contexts. Prior research has uncovered isolated cases of bias, particularly against English Language Learners (ELLs) however, there is a lack of systematic evaluation of such systems across broader sociolinguistic factors. In this work, we propose BAID, a comprehensive evaluation framework for AI detectors across various types of biases. As a part of the framework, we introduce over 200k samples spanning 7 major categories: demographics, age, educational grade level, dialect, formality, political leaning, and topic. We also generated synthetic versions of each sample with carefully crafted prompts to preserve the original content while reflecting subgroup-specific writing styles. Using this, we evaluate four open-source state-of-the-art AI text detectors and find consistent disparities in detection performance, particularly low recall rates for texts from underrepresented groups. Our contributions provide a scalable, transparent approach for auditing AI detectors and emphasize the need for bias-aware evaluation before these tools are deployed for public use.",
    "categories": [
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "Accepted at the workshop on Agentic AI Benchmarks and Applications for Enterprise Tasks at AAAI 2026",
    "pdf_url": "https://arxiv.org/pdf/2512.11505v1",
    "published_date": "2025-12-12 12:01:42 UTC",
    "updated_date": "2025-12-12 12:01:42 UTC"
  },
  {
    "arxiv_id": "2601.04212v1",
    "title": "TrueBrief: Faithful Summarization through Small Language Models",
    "authors": [
      "Kumud Lakara",
      "Ruibo Shi",
      "Fran Silavong"
    ],
    "abstract": "Large language models (LLMs) have exhibited remarkable proficiency in generating high-quality text; however, their propensity for producing hallucinations poses a significant challenge for their deployment in security-critical domains. In this work, we present TrueBrief, an end-to-end framework specifically designed to enhance the faithfulness of small LLMs (SLMs) primarily for the task of text summarization through a preference-optimization paradigm. Central to our framework is a data generation module that facilitates controlled hallucination injection to generate synthetic preference data. Our work provides insights into the impact of data quality and model size on preference-based optimization, highlighting the conditions under which these methods are most effective.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2601.04212v1",
    "published_date": "2025-12-12 11:47:02 UTC",
    "updated_date": "2025-12-12 11:47:02 UTC"
  },
  {
    "arxiv_id": "2512.14741v1",
    "title": "Persistent Backdoor Attacks under Continual Fine-Tuning of LLMs",
    "authors": [
      "Jing Cui",
      "Yufei Han",
      "Jianbin Jiao",
      "Junge Zhang"
    ],
    "abstract": "Backdoor attacks embed malicious behaviors into Large Language Models (LLMs), enabling adversaries to trigger harmful outputs or bypass safety controls. However, the persistence of the implanted backdoors under user-driven post-deployment continual fine-tuning has been rarely examined. Most prior works evaluate the effectiveness and generalization of implanted backdoors only at releasing and empirical evidence shows that naively injected backdoor persistence degrades after updates. In this work, we study whether and how implanted backdoors persist through a multi-stage post-deployment fine-tuning. We propose P-Trojan, a trigger-based attack algorithm that explicitly optimizes for backdoor persistence across repeated updates. By aligning poisoned gradients with those of clean tasks on token embeddings, the implanted backdoor mapping is less likely to be suppressed or forgotten during subsequent updates. Theoretical analysis shows the feasibility of such persistent backdoor attacks after continual fine-tuning. And experiments conducted on the Qwen2.5 and LLaMA3 families of LLMs, as well as diverse task sequences, demonstrate that P-Trojan achieves over 99% persistence while preserving clean-task accuracy. Our findings highlight the need for persistence-aware evaluation and stronger defenses in realistic model adaptation pipelines.",
    "categories": [
      "cs.CR",
      "cs.AI"
    ],
    "primary_category": "cs.CR",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2512.14741v1",
    "published_date": "2025-12-12 11:40:51 UTC",
    "updated_date": "2025-12-12 11:40:51 UTC"
  },
  {
    "arxiv_id": "2512.11482v1",
    "title": "Towards Privacy-Preserving Code Generation: Differentially Private Code Language Models",
    "authors": [
      "Melih Catal",
      "Pooja Rani",
      "Harald C. Gall"
    ],
    "abstract": "Large language models specialized for code (CodeLLMs) have demonstrated remarkable capabilities in generating code snippets, documentation, and test cases. However, despite their promising capabilities, CodeLLMs can inadvertently memorize and reproduce snippets from their training data, which poses risks of privacy breaches and intellectual property violations. These risks restrict the deployment of CodeLLMs in sensitive domains and limit their training datasets to publicly available sources. To mitigate the memorization risk without compromising their task performance, we apply Differential Privacy (DP) to CodeLLMs. To the best of our knowledge, this is the first comprehensive study that systematically evaluates the effectiveness of DP in CodeLLMs. DP adds calibrated noise to the training process to protect individual data points while still allowing the model to learn useful patterns. To this end, we first identify and understand the driving reasons of the memorization behaviour of the CodeLLMs during their fine-tuning. Then, to address this issue, we empirically evaluate the effect of DP on mitigating memorization while preserving code generation capabilities. Our findings show that DP substantially reduces memorization in CodeLLMs across all the tested snippet types. The snippet types most prone to memorization are also the most effectively mitigated by DP. Furthermore, we observe that DP slightly increases perplexity but preserves, and can even enhance, the code generation capabilities of CodeLLMs, which makes it feasible to apply DP in practice without significantly compromising model utility. Finally, we analyze the impact of DP on training efficiency and energy consumption, finding that DP does not significantly affect training time or energy usage, making it a practical choice for privacy-preserving CodeLLMs training.",
    "categories": [
      "cs.SE",
      "cs.AI",
      "cs.CR"
    ],
    "primary_category": "cs.SE",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2512.11482v1",
    "published_date": "2025-12-12 11:31:13 UTC",
    "updated_date": "2025-12-12 11:31:13 UTC"
  },
  {
    "arxiv_id": "2601.06037v4",
    "title": "TeleMem: Building Long-Term and Multimodal Memory for Agentic AI",
    "authors": [
      "Chunliang Chen",
      "Ming Guan",
      "Xiao Lin",
      "Jiaxu Li",
      "Luxi Lin",
      "Qiyi Wang",
      "Xiangyu Chen",
      "Jixiang Luo",
      "Changzhi Sun",
      "Dell Zhang",
      "Xuelong Li"
    ],
    "abstract": "Large language models (LLMs) excel at many NLP tasks but struggle to sustain long-term interactions due to limited attention over extended dialogue histories. Retrieval-augmented generation (RAG) mitigates this issue but lacks reliable mechanisms for updating or refining stored memories, leading to schema-driven hallucinations, inefficient write operations, and minimal support for multimodal reasoning.To address these challenges, we propose TeleMem, a unified long-term and multimodal memory system that maintains coherent user profiles through narrative dynamic extraction, ensuring that only dialogue-grounded information is preserved. TeleMem further introduces a structured writing pipeline that batches, retrieves, clusters, and consolidates memory entries, substantially improving storage efficiency, reducing token usage, and accelerating memory operations. Additionally, a multimodal memory module combined with ReAct-style reasoning equips the system with a closed-loop observe, think, and act process that enables accurate understanding of complex video content in long-term contexts. Experimental results show that TeleMem surpasses the state-of-the-art Mem0 baseline with 19% higher accuracy, 43% fewer tokens, and a 2.1x speedup on the ZH-4O long-term role-play gaming benchmark.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2601.06037v4",
    "published_date": "2025-12-12 11:24:52 UTC",
    "updated_date": "2026-01-22 13:48:29 UTC"
  },
  {
    "arxiv_id": "2512.20643v1",
    "title": "Forecasting N-Body Dynamics: A Comparative Study of Neural Ordinary Differential Equations and Universal Differential Equations",
    "authors": [
      "Suriya R S",
      "Prathamesh Dinesh Joshi",
      "Rajat Dandekar",
      "Raj Dandekar",
      "Sreedath Panat"
    ],
    "abstract": "The n body problem, fundamental to astrophysics, simulates the motion of n bodies acting under the effect of their own mutual gravitational interactions. Traditional machine learning models that are used for predicting and forecasting trajectories are often data intensive black box models, which ignore the physical laws, thereby lacking interpretability. Whereas Scientific Machine Learning ( Scientific ML ) directly embeds the known physical laws into the machine learning framework. Through robust modelling in the Julia programming language, our method uses the Scientific ML frameworks: Neural ordinary differential equations (NODEs) and Universal differential equations (UDEs) to predict and forecast the system dynamics. In addition, an essential component of our analysis involves determining the forecasting breakdown point, which is the smallest possible amount of training data our models need to predict future, unseen data accurately. We employ synthetically created noisy data to simulate real-world observational limitations. Our findings indicate that the UDE model is much more data efficient, needing only 20% of data for a correct forecast, whereas the Neural ODE requires 90%.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "physics.comp-ph"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2512.20643v1",
    "published_date": "2025-12-12 11:20:27 UTC",
    "updated_date": "2025-12-12 11:20:27 UTC"
  },
  {
    "arxiv_id": "2512.11474v1",
    "title": "General-purpose AI models can generate actionable knowledge on agroecological crop protection",
    "authors": [
      "Kris A. G. Wyckhuys"
    ],
    "abstract": "Generative artificial intelligence (AI) offers potential for democratizing scientific knowledge and converting this to clear, actionable information, yet its application in agri-food science remains unexplored. Here, we verify the scientific knowledge on agroecological crop protection that is generated by either web-grounded or non-grounded large language models (LLMs), i.e., DeepSeek versus the free-tier version of ChatGPT. For nine globally limiting pests, weeds, and plant diseases, we assessed the factual accuracy, data consistency, and breadth of knowledge or data completeness of each LLM. Overall, DeepSeek consistently screened a 4.8-49.7-fold larger literature corpus and reported 1.6-2.4-fold more biological control agents or management solutions than ChatGPT. As a result, DeepSeek reported 21.6% higher efficacy estimates, exhibited greater laboratory-to-field data consistency, and showed more realistic effects of pest identity and management tactics. However, both models hallucinated, i.e., fabricated fictitious agents or references, reported on implausible ecological interactions or outcomes, confused old and new scientific nomenclatures, and omitted data on key agents or solutions. Despite these shortcomings, both LLMs correctly reported low-resolution efficacy trends. Overall, when paired with rigorous human oversight, LLMs may pose a powerful tool to support farm-level decision-making and unleash scientific creativity.",
    "categories": [
      "cs.AI",
      "cs.CY",
      "cs.IR"
    ],
    "primary_category": "cs.AI",
    "comment": "33 pages, 3 figures, 3 tables, 1 supplementary table",
    "pdf_url": "https://arxiv.org/pdf/2512.11474v1",
    "published_date": "2025-12-12 11:17:13 UTC",
    "updated_date": "2025-12-12 11:17:13 UTC"
  },
  {
    "arxiv_id": "2512.11469v2",
    "title": "Three methods, one problem: Classical and AI approaches to no-three-in-line",
    "authors": [
      "Pranav Ramanathan",
      "Thomas Prellberg",
      "Matthew Lewis",
      "Prathamesh Dinesh Joshi",
      "Raj Abhijit Dandekar",
      "Rajat Dandekar",
      "Sreedath Panat"
    ],
    "abstract": "The No-Three-In-Line problem asks for the maximum number of points that can be placed on an n by n grid with no three collinear, representing a famous problem in combinatorial geometry. While classical methods like Integer Linear Programming (ILP) guarantee optimal solutions, they face exponential scaling with grid size, and recent advances in machine learning offer promising alternatives for pattern-based approximation. This paper presents the first systematic comparison of classical optimization and AI approaches to this problem, evaluating their performance against traditional algorithms. We apply PatternBoost transformer learning and reinforcement learning (PPO) to this problem for the first time, comparing them against ILP. ILP achieves provably optimal solutions up to 19 by 19 grids, while PatternBoost matches optimal performance up to 14 by 14 grids with 96% test loss reduction. PPO achieves perfect solutions on 10 by 10 grids but fails at 11 by 11 grids, where constraint violations prevent valid configurations. These results demonstrate that classical optimization remains essential for exact solutions while AI methods offer competitive performance on smaller instances, with hybrid approaches presenting the most promising direction for scaling to larger problem sizes.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2512.11469v2",
    "published_date": "2025-12-12 11:12:42 UTC",
    "updated_date": "2025-12-22 04:02:51 UTC"
  },
  {
    "arxiv_id": "2512.15766v1",
    "title": "LOOPRAG: Enhancing Loop Transformation Optimization with Retrieval-Augmented Large Language Models",
    "authors": [
      "Yijie Zhi",
      "Yayu Cao",
      "Jianhua Dai",
      "Xiaoyang Han",
      "Jingwen Pu",
      "Qingran Wu",
      "Sheng Cheng",
      "Ming Cai"
    ],
    "abstract": "Loop transformations are semantics-preserving optimization techniques, widely used to maximize objectives such as parallelism. Despite decades of research, applying the optimal composition of loop transformations remains challenging due to inherent complexities, including cost modeling for optimization objectives. Recent studies have explored the potential of Large Language Models (LLMs) for code optimization. However, our key observation is that LLMs often struggle with effective loop transformation optimization, frequently leading to errors or suboptimal optimization, thereby missing opportunities for performance improvements. To bridge this gap, we propose LOOPRAG, a novel retrieval-augmented generation framework designed to guide LLMs in performing effective loop optimization on Static Control Part. We introduce a parameter-driven method to harness loop properties, which trigger various loop transformations, and generate diverse yet legal example codes serving as a demonstration source. To effectively obtain the most informative demonstrations, we propose a loop-aware algorithm based on loop features, which balances similarity and diversity for code retrieval. To enhance correct and efficient code generation, we introduce a feedback-based iterative mechanism that incorporates compilation, testing and performance results as feedback to guide LLMs. Each optimized code undergoes mutation, coverage and differential testing for equivalence checking. We evaluate LOOPRAG on PolyBench, TSVC and LORE benchmark suites, and compare it against compilers (GCC-Graphite, Clang-Polly, Perspective and ICX) and representative LLMs (DeepSeek and GPT-4). The results demonstrate average speedups over base compilers of up to 11.20$\\times$, 14.34$\\times$, and 9.29$\\times$ for PolyBench, TSVC, and LORE, respectively, and speedups over base LLMs of up to 11.97$\\times$, 5.61$\\times$, and 11.59$\\times$.",
    "categories": [
      "cs.PL",
      "cs.AI",
      "cs.DC",
      "cs.PF"
    ],
    "primary_category": "cs.PL",
    "comment": "Accepted to ASPLOS 2026",
    "pdf_url": "https://arxiv.org/pdf/2512.15766v1",
    "published_date": "2025-12-12 11:09:48 UTC",
    "updated_date": "2025-12-12 11:09:48 UTC"
  },
  {
    "arxiv_id": "2512.11942v1",
    "title": "Hypergame Rationalisability: Solving Agent Misalignment In Strategic Play",
    "authors": [
      "Vince Trencsenyi"
    ],
    "abstract": "Differences in perception, information asymmetries, and bounded rationality lead game-theoretic players to derive a private, subjective view of the game that may diverge from the underlying ground-truth scenario and may be misaligned with other players' interpretations. While typical game-theoretic assumptions often overlook such heterogeneity, hypergame theory provides the mathematical framework to reason about mismatched mental models. Although hypergames have recently gained traction in dynamic applications concerning uncertainty, their practical adoption in multi-agent system research has been hindered by the lack of a unifying, formal, and practical representation language, as well as scalable algorithms for managing complex hypergame structures and equilibria. Our work addresses this gap by introducing a declarative, logic-based domain-specific language for encoding hypergame structures and hypergame solution concepts. Leveraging answer-set programming, we develop an automated pipeline for instantiating hypergame structures and running our novel hypergame rationalisation procedure, a mechanism for finding belief structures that justify seemingly irrational outcomes. The proposed language establishes a unifying formalism for hypergames and serves as a foundation for developing nuanced, belief-based heterogeneous reasoners, offering a verifiable context with logical guarantees. Together, these contributions establish the connection between hypergame theory, multi-agent systems, and strategic AI.",
    "categories": [
      "cs.AI",
      "cs.FL",
      "cs.GT"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2512.11942v1",
    "published_date": "2025-12-12 11:08:15 UTC",
    "updated_date": "2025-12-12 11:08:15 UTC"
  },
  {
    "arxiv_id": "2512.11464v1",
    "title": "Exploring MLLM-Diffusion Information Transfer with MetaCanvas",
    "authors": [
      "Han Lin",
      "Xichen Pan",
      "Ziqi Huang",
      "Ji Hou",
      "Jialiang Wang",
      "Weifeng Chen",
      "Zecheng He",
      "Felix Juefei-Xu",
      "Junzhe Sun",
      "Zhipeng Fan",
      "Ali Thabet",
      "Mohit Bansal",
      "Chu Wang"
    ],
    "abstract": "Multimodal learning has rapidly advanced visual understanding, largely via multimodal large language models (MLLMs) that use powerful LLMs as cognitive cores. In visual generation, however, these powerful core models are typically reduced to global text encoders for diffusion models, leaving most of their reasoning and planning ability unused. This creates a gap: current multimodal LLMs can parse complex layouts, attributes, and knowledge-intensive scenes, yet struggle to generate images or videos with equally precise and structured control. We propose MetaCanvas, a lightweight framework that lets MLLMs reason and plan directly in spatial and spatiotemporal latent spaces and interface tightly with diffusion generators. We empirically implement MetaCanvas on three different diffusion backbones and evaluate it across six tasks, including text-to-image generation, text/image-to-video generation, image/video editing, and in-context video generation, each requiring precise layouts, robust attribute binding, and reasoning-intensive control. MetaCanvas consistently outperforms global-conditioning baselines, suggesting that treating MLLMs as latent-space planners is a promising direction for narrowing the gap between multimodal understanding and generation.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "Project page: https://metacanvas.github.io",
    "pdf_url": "https://arxiv.org/pdf/2512.11464v1",
    "published_date": "2025-12-12 11:07:11 UTC",
    "updated_date": "2025-12-12 11:07:11 UTC"
  },
  {
    "arxiv_id": "2512.11458v1",
    "title": "Boosting Skeleton-based Zero-Shot Action Recognition with Training-Free Test-Time Adaptation",
    "authors": [
      "Jingmin Zhu",
      "Anqi Zhu",
      "Hossein Rahmani",
      "Jun Liu",
      "Mohammed Bennamoun",
      "Qiuhong Ke"
    ],
    "abstract": "We introduce Skeleton-Cache, the first training-free test-time adaptation framework for skeleton-based zero-shot action recognition (SZAR), aimed at improving model generalization to unseen actions during inference. Skeleton-Cache reformulates inference as a lightweight retrieval process over a non-parametric cache that stores structured skeleton representations, combining both global and fine-grained local descriptors. To guide the fusion of descriptor-wise predictions, we leverage the semantic reasoning capabilities of large language models (LLMs) to assign class-specific importance weights. By integrating these structured descriptors with LLM-guided semantic priors, Skeleton-Cache dynamically adapts to unseen actions without any additional training or access to training data. Extensive experiments on NTU RGB+D 60/120 and PKU-MMD II demonstrate that Skeleton-Cache consistently boosts the performance of various SZAR backbones under both zero-shot and generalized zero-shot settings. The code is publicly available at https://github.com/Alchemist0754/Skeleton-Cache.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2512.11458v1",
    "published_date": "2025-12-12 10:53:51 UTC",
    "updated_date": "2025-12-12 10:53:51 UTC"
  },
  {
    "arxiv_id": "2512.11941v1",
    "title": "DynaPURLS: Dynamic Refinement of Part-aware Representations for Skeleton-based Zero-Shot Action Recognition",
    "authors": [
      "Jingmin Zhu",
      "Anqi Zhu",
      "James Bailey",
      "Jun Liu",
      "Hossein Rahmani",
      "Mohammed Bennamoun",
      "Farid Boussaid",
      "Qiuhong Ke"
    ],
    "abstract": "Zero-shot skeleton-based action recognition (ZS-SAR) is fundamentally constrained by prevailing approaches that rely on aligning skeleton features with static, class-level semantics. This coarse-grained alignment fails to bridge the domain shift between seen and unseen classes, thereby impeding the effective transfer of fine-grained visual knowledge. To address these limitations, we introduce \\textbf{DynaPURLS}, a unified framework that establishes robust, multi-scale visual-semantic correspondences and dynamically refines them at inference time to enhance generalization. Our framework leverages a large language model to generate hierarchical textual descriptions that encompass both global movements and local body-part dynamics. Concurrently, an adaptive partitioning module produces fine-grained visual representations by semantically grouping skeleton joints. To fortify this fine-grained alignment against the train-test domain shift, DynaPURLS incorporates a dynamic refinement module. During inference, this module adapts textual features to the incoming visual stream via a lightweight learnable projection. This refinement process is stabilized by a confidence-aware, class-balanced memory bank, which mitigates error propagation from noisy pseudo-labels. Extensive experiments on three large-scale benchmark datasets, including NTU RGB+D 60/120 and PKU-MMD, demonstrate that DynaPURLS significantly outperforms prior art, setting new state-of-the-art records. The source code is made publicly available at https://github.com/Alchemist0754/DynaPURLS",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2512.11941v1",
    "published_date": "2025-12-12 10:39:10 UTC",
    "updated_date": "2025-12-12 10:39:10 UTC"
  },
  {
    "arxiv_id": "2512.11438v1",
    "title": "Flowception: Temporally Expansive Flow Matching for Video Generation",
    "authors": [
      "Tariq Berrada Ifriqi",
      "John Nguyen",
      "Karteek Alahari",
      "Jakob Verbeek",
      "Ricky T. Q. Chen"
    ],
    "abstract": "We present Flowception, a novel non-autoregressive and variable-length video generation framework. Flowception learns a probability path that interleaves discrete frame insertions with continuous frame denoising. Compared to autoregressive methods, Flowception alleviates error accumulation/drift as the frame insertion mechanism during sampling serves as an efficient compression mechanism to handle long-term context. Compared to full-sequence flows, our method reduces FLOPs for training three-fold, while also being more amenable to local attention variants, and allowing to learn the length of videos jointly with their content. Quantitative experimental results show improved FVD and VBench metrics over autoregressive and full-sequence baselines, which is further validated with qualitative results. Finally, by learning to insert and denoise frames in a sequence, Flowception seamlessly integrates different tasks such as image-to-video generation and video interpolation.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2512.11438v1",
    "published_date": "2025-12-12 10:23:47 UTC",
    "updated_date": "2025-12-12 10:23:47 UTC"
  },
  {
    "arxiv_id": "2512.11433v1",
    "title": "Back to the Baseline: Examining Baseline Effects on Explainability Metrics",
    "authors": [
      "Agustin Martin Picard",
      "Thibaut Boissin",
      "Varshini Subhash",
      "Rémi Cadène",
      "Thomas Fel"
    ],
    "abstract": "Attribution methods are among the most prevalent techniques in Explainable Artificial Intelligence (XAI) and are usually evaluated and compared using Fidelity metrics, with Insertion and Deletion being the most popular. These metrics rely on a baseline function to alter the pixels of the input image that the attribution map deems most important. In this work, we highlight a critical problem with these metrics: the choice of a given baseline will inevitably favour certain attribution methods over others. More concerningly, even a simple linear model with commonly used baselines contradicts itself by designating different optimal methods. A question then arises: which baseline should we use? We propose to study this problem through two desirable properties of a baseline: (i) that it removes information and (ii) that it does not produce overly out-of-distribution (OOD) images. We first show that none of the tested baselines satisfy both criteria, and there appears to be a trade-off among current baselines: either they remove information or they produce a sequence of OOD images. Finally, we introduce a novel baseline by leveraging recent work in feature visualisation to artificially produce a model-dependent baseline that removes information without being overly OOD, thus improving on the trade-off when compared to other existing baselines. Our code is available at https://github.com/deel-ai-papers/Back-to-the-Baseline",
    "categories": [
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2512.11433v1",
    "published_date": "2025-12-12 10:13:44 UTC",
    "updated_date": "2025-12-12 10:13:44 UTC"
  },
  {
    "arxiv_id": "2512.11426v1",
    "title": "AgentBalance: Backbone-then-Topology Design for Cost-Effective Multi-Agent Systems under Budget Constraints",
    "authors": [
      "Shuowei Cai",
      "Yansong Ning",
      "Hao Liu"
    ],
    "abstract": "Large Language Model (LLM)-based multi-agent systems (MAS) are becoming indispensable building blocks for web-scale applications such as web search, social network analytics, and online customer support, where cost-effectiveness is increasingly the primary constraint for large-scale deployment. While recent work improves MAS cost-effectiveness by shaping inter-agent communication topologies and selecting agent backbones, it rarely models and optimizes under explicit token-cost and latency budgets that reflect deployment constraints. This often leads to topology-first designs and suboptimal cost-effectiveness when budgets are binding. We present AgentBalance, a framework for constructing cost-effective MAS under explicit token-cost and latency budgets via a backbone-then-topology design. AgentBalance first performs backbone-oriented agent generation, constructing agents with heterogeneous backbones through LLM pool construction, pool selection, and role-backbone matching. It then performs adaptive MAS topology generation, guiding inter-agent communication via agent representation learning, gating, and latency-aware topology synthesis. Experiments on benchmarks with 14 candidate LLM backbones show that AgentBalance achieves up to 10% and 22% performance gains under matched token-cost and latency budgets, respectively, and yields strong AUC on performance-versus-budget curves across benchmarks. AgentBalance also functions as a plug-in for existing MAS, improving performance under the same token-cost and latency constraints, and it generalizes well to unseen LLMs for practical, budget-aware deployment. Code: https://github.com/usail-hkust/AgentBalance",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2512.11426v1",
    "published_date": "2025-12-12 10:08:03 UTC",
    "updated_date": "2025-12-12 10:08:03 UTC"
  },
  {
    "arxiv_id": "2512.11421v1",
    "title": "Towards Trustworthy Multi-Turn LLM Agents via Behavioral Guidance",
    "authors": [
      "Gonca Gürsun"
    ],
    "abstract": "Large Language Models demonstrate strong reasoning and generation abilities, yet their behavior in multi-turn tasks often lacks reliability and verifiability. We present a task completion framework that enables LLM-based agents to act under explicit behavioral guidance in environments described by reinforcement learning formalisms with defined observation, action, and reward signals.\n  The framework integrates three components: a lightweight task profiler that selects reasoning and generation strategies, a reasoning module that learns verifiable observation - action mappings, and a generation module that enforces constraint-compliant outputs through validation or deterministic synthesis. We show that as the agent interacts with the environment, these components co-evolve, yielding trustworthy behavior.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "Accepted to AAAI 2026 Workshop on Trust and Control in Agentic AI (TrustAgent)",
    "pdf_url": "https://arxiv.org/pdf/2512.11421v1",
    "published_date": "2025-12-12 10:03:24 UTC",
    "updated_date": "2025-12-12 10:03:24 UTC"
  },
  {
    "arxiv_id": "2512.15764v1",
    "title": "AdaGradSelect: An adaptive gradient-guided layer selection method for efficient fine-tuning of SLMs",
    "authors": [
      "Anshul Kumar",
      "Gagan Raj Gupta",
      "Manisha Chawla"
    ],
    "abstract": "Large Language Models (LLMs) can perform many NLP tasks well, but fully fine-tuning them is expensive and requires a lot of memory. Parameter-Efficient Fine-Tuning (PEFT) methods such as LoRA reduce this cost by adding small low-rank updates to frozen model weights. However, these methods restrict the training to a limited subspace, which can sometimes reduce performance.\n  For Small Language Models (SLMs), where efficiency gains matter even more, we introduce AdaGradSelect, an adaptive method that selects which transformer blocks to update based on gradients.\n  Early observations showed that updating only the transformer blocks with the highest gradient norms can achieve performance close to full fine-tuning. Building on this insight, AdaGradSelect adaptively chooses which blocks to train. It uses a combination of Dirichlet-based sampling, which depends on how frequently blocks were updated in the past, and an epsilon-greedy exploration strategy. This lets the method explore different blocks in early training and gradually focus on the most important ones in later epochs.\n  Experiments show that AdaGradSelect trains about 12 percent faster and uses 35 percent less GPU memory while delivering performance very close to full fine-tuning. On the GSM8K dataset, it outperforms LoRA (rank 256) by about 3 percent on average across models such as Qwen2.5-0.5B, LLaMA3.2-1B, and Phi4-mini-3.8B. It also achieves similar accuracy on the MATH dataset. Overall, AdaGradSelect provides a more effective and resource-efficient alternative to traditional fine-tuning methods.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.PF"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2512.15764v1",
    "published_date": "2025-12-12 09:44:07 UTC",
    "updated_date": "2025-12-12 09:44:07 UTC"
  },
  {
    "arxiv_id": "2512.11412v1",
    "title": "Task-Specific Sparse Feature Masks for Molecular Toxicity Prediction with Chemical Language Models",
    "authors": [
      "Kwun Sy Lee",
      "Jiawei Chen",
      "Fuk Sheng Ford Chung",
      "Tianyu Zhao",
      "Zhenyuan Chen",
      "Debby D. Wang"
    ],
    "abstract": "Reliable in silico molecular toxicity prediction is a cornerstone of modern drug discovery, offering a scalable alternative to experimental screening. However, the black-box nature of state-of-the-art models remains a significant barrier to adoption, as high-stakes safety decisions demand verifiable structural insights alongside predictive performance. To address this, we propose a novel multi-task learning (MTL) framework designed to jointly enhance accuracy and interpretability. Our architecture integrates a shared chemical language model with task-specific attention modules. By imposing an L1 sparsity penalty on these modules, the framework is constrained to focus on a minimal set of salient molecular fragments for each distinct toxicity endpoint. The resulting framework is trained end-to-end and is readily adaptable to various transformer-based backbones. Evaluated on the ClinTox, SIDER, and Tox21 benchmark datasets, our approach consistently outperforms both single-task and standard MTL baselines. Crucially, the sparse attention weights provide chemically intuitive visualizations that reveal the specific fragments influencing predictions, thereby enhancing insight into the model's decision-making process.",
    "categories": [
      "cs.CE",
      "cs.AI",
      "cs.CL",
      "cs.LG",
      "q-bio.BM"
    ],
    "primary_category": "cs.CE",
    "comment": "6 pages, 4 figures",
    "pdf_url": "https://arxiv.org/pdf/2512.11412v1",
    "published_date": "2025-12-12 09:41:04 UTC",
    "updated_date": "2025-12-12 09:41:04 UTC"
  },
  {
    "arxiv_id": "2512.11402v1",
    "title": "REMODEL-LLM: Transforming C code to Java using LLMs",
    "authors": [
      "Aryan Gupta",
      "Y. Raghu Reddy"
    ],
    "abstract": "The automated translation of C code to Java code is a notoriously difficult task, fraught with challenges stemming from fundamental paradigm shifts (procedural vs. Object Oriented), memory models (manual pointers vs. Garbage Collection), and incompatible data types. This paper investigates the efficacy of 19 small, quantized LLMs (under 20 billion parameters) for the C to Java translation task. We use a novel, hybrid pipeline that leverages Abstract Syntax Trees (ASTs) for semantic decomposition and employs a highly constrained, rule based prompting strategy. The results are stark: a clear multi tiered performance divide emerged. The vast majority of models (Tier 3, e.g., llama3.1, gemma3, starcoder2) failed 100\\% of the tests, proving incapable of generating even basic, runnable Java boilerplate. A small middle tier (Tier 2, e.g., mistral-nemo and mistral) produced runnable code but was plagued by dangerous semantic failures and wrong translations. Only three models (Tier 1: phi4, deepseek-coder-v2, codeqwen) proved viable, passing over 50\\% of the test suite. Even these top models failed on the most complex C concepts, such as function pointers, sizeof, and enum logic, revealing a hard ceiling for the reasoning capabilities of current quantized models.",
    "categories": [
      "cs.SE",
      "cs.AI"
    ],
    "primary_category": "cs.SE",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2512.11402v1",
    "published_date": "2025-12-12 09:25:10 UTC",
    "updated_date": "2025-12-12 09:25:10 UTC"
  },
  {
    "arxiv_id": "2512.15762v1",
    "title": "Cross-Sample Augmented Test-Time Adaptation for Personalized Intraoperative Hypotension Prediction",
    "authors": [
      "Kanxue Li",
      "Yibing Zhan",
      "Hua Jin",
      "Chongchong Qi",
      "Xu Lin",
      "Baosheng Yu"
    ],
    "abstract": "Intraoperative hypotension (IOH) poses significant surgical risks, but accurate prediction remains challenging due to patient-specific variability. While test-time adaptation (TTA) offers a promising approach for personalized prediction, the rarity of IOH events often leads to unreliable test-time training. To address this, we propose CSA-TTA, a novel Cross-Sample Augmented Test-Time Adaptation framework that enhances training by incorporating hypotension events from other individuals. Specifically, we first construct a cross-sample bank by segmenting historical data into hypotensive and non-hypotensive samples. Then, we introduce a coarse-to-fine retrieval strategy for building test-time training data: we initially apply K-Shape clustering to identify representative cluster centers and subsequently retrieve the top-K semantically similar samples based on the current patient signal. Additionally, we integrate both self-supervised masked reconstruction and retrospective sequence forecasting signals during training to enhance model adaptability to rapid and subtle intraoperative dynamics. We evaluate the proposed CSA-TTA on both the VitalDB dataset and a real-world in-hospital dataset by integrating it with state-of-the-art time series forecasting models, including TimesFM and UniTS. CSA-TTA consistently enhances performance across settings-for instance, on VitalDB, it improves Recall and F1 scores by +1.33% and +1.13%, respectively, under fine-tuning, and by +7.46% and +5.07% in zero-shot scenarios-demonstrating strong robustness and generalization.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted by AAAI 2026",
    "pdf_url": "https://arxiv.org/pdf/2512.15762v1",
    "published_date": "2025-12-12 08:02:37 UTC",
    "updated_date": "2025-12-12 08:02:37 UTC"
  },
  {
    "arxiv_id": "2512.11350v1",
    "title": "Surveillance Video-Based Traffic Accident Detection Using Transformer Architecture",
    "authors": [
      "Tanu Singh",
      "Pranamesh Chakraborty",
      "Long T. Truong"
    ],
    "abstract": "Road traffic accidents represent a leading cause of mortality globally, with incidence rates rising due to increasing population, urbanization, and motorization. Rising accident rates raise concerns about traffic surveillance effectiveness. Traditional computer vision methods for accident detection struggle with limited spatiotemporal understanding and poor cross-domain generalization. Recent advances in transformer architectures excel at modeling global spatial-temporal dependencies and parallel computation. However, applying these models to automated traffic accident detection is limited by small, non-diverse datasets, hindering the development of robust, generalizable systems. To address this gap, we curated a comprehensive and balanced dataset that captures a wide spectrum of traffic environments, accident types, and contextual variations. Utilizing the curated dataset, we propose an accident detection model based on a transformer architecture using pre-extracted spatial video features. The architecture employs convolutional layers to extract local correlations across diverse patterns within a frame, while leveraging transformers to capture sequential-temporal dependencies among the retrieved features. Moreover, most existing studies neglect the integration of motion cues, which are essential for understanding dynamic scenes, especially during accidents. These approaches typically rely on static features or coarse temporal information. In this study, multiple methods for incorporating motion cues were evaluated to identify the most effective strategy. Among the tested input approaches, concatenating RGB features with optical flow achieved the highest accuracy at 88.3%. The results were further compared with vision language models (VLM) such as GPT, Gemini, and LLaVA-NeXT-Video to assess the effectiveness of the proposed method.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2512.11350v1",
    "published_date": "2025-12-12 07:57:36 UTC",
    "updated_date": "2025-12-12 07:57:36 UTC"
  },
  {
    "arxiv_id": "2512.11325v1",
    "title": "MLLM Machine Unlearning via Visual Knowledge Distillation",
    "authors": [
      "Yuhang Wang",
      "Zhenxing Niu",
      "Haoxuan Ji",
      "Guangyu He",
      "Haichang Gao",
      "Gang Hua"
    ],
    "abstract": "Recently, machine unlearning approaches have been proposed to remove sensitive information from well-trained large models. However, most existing methods are tailored for LLMs, while MLLM-oriented unlearning remains at its early stage. Inspired by recent studies exploring the internal mechanisms of MLLMs, we propose to disentangle the visual and textual knowledge embedded within MLLMs and introduce a dedicated approach to selectively erase target visual knowledge while preserving textual knowledge. Unlike previous unlearning methods that rely on output-level supervision, our approach introduces a Visual Knowledge Distillation (VKD) scheme, which leverages intermediate visual representations within the MLLM as supervision signals. This design substantially enhances both unlearning effectiveness and model utility. Moreover, since our method only fine-tunes the visual components of the MLLM, it offers significant efficiency advantages. Extensive experiments demonstrate that our approach outperforms state-of-the-art unlearning methods in terms of both effectiveness and efficiency. Moreover, we are the first to evaluate the robustness of MLLM unlearning against relearning attacks.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2512.11325v1",
    "published_date": "2025-12-12 06:51:02 UTC",
    "updated_date": "2025-12-12 06:51:02 UTC"
  },
  {
    "arxiv_id": "2512.11323v1",
    "title": "CAPTURE: A Benchmark and Evaluation for LVLMs in CAPTCHA Resolving",
    "authors": [
      "Jianyi Zhang",
      "Ziyin Zhou",
      "Xu Ji",
      "Shizhao Liu",
      "Zhangchi Zhao"
    ],
    "abstract": "Benefiting from strong and efficient multi-modal alignment strategies, Large Visual Language Models (LVLMs) are able to simulate human visual and reasoning capabilities, such as solving CAPTCHAs. However, existing benchmarks based on visual CAPTCHAs still face limitations. Previous studies, when designing benchmarks and datasets, customized them according to their research objectives. Consequently, these benchmarks cannot comprehensively cover all CAPTCHA types. Notably, there is a dearth of dedicated benchmarks for LVLMs. To address this problem, we introduce a novel CAPTCHA benchmark for the first time, named CAPTURE CAPTCHA for Testing Under Real-world Experiments, specifically for LVLMs. Our benchmark encompasses 4 main CAPTCHA types and 25 sub-types from 31 vendors. The diversity enables a multi-dimensional and thorough evaluation of LVLM performance. CAPTURE features extensive class variety, large-scale data, and unique LVLM-tailored labels, filling the gaps in previous research in terms of data comprehensiveness and labeling pertinence. When evaluated by this benchmark, current LVLMs demonstrate poor performance in solving CAPTCHAs.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2512.11323v1",
    "published_date": "2025-12-12 06:50:27 UTC",
    "updated_date": "2025-12-12 06:50:27 UTC"
  },
  {
    "arxiv_id": "2512.11317v1",
    "title": "Condensation-Concatenation Framework for Dynamic Graph Continual Learning",
    "authors": [
      "Tingxu Yan",
      "Ye Yuan"
    ],
    "abstract": "Dynamic graphs are prevalent in real-world scenarios, where continuous structural changes induce catastrophic forgetting in graph neural networks (GNNs). While continual learning has been extended to dynamic graphs, existing methods overlook the effects of topological changes on existing nodes. To address it, we propose a novel framework for continual learning on dynamic graphs, named Condensation-Concatenation-based Continual Learning (CCC). Specifically, CCC first condenses historical graph snapshots into compact semantic representations while aiming to preserve the original label distribution and topological properties. Then it concatenates these historical embeddings with current graph representations selectively. Moreover, we refine the forgetting measure (FM) to better adapt to dynamic graph scenarios by quantifying the predictive performance degradation of existing nodes caused by structural updates. CCC demonstrates superior performance over state-of-the-art baselines across four real-world datasets in extensive experiments.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2512.11317v1",
    "published_date": "2025-12-12 06:32:16 UTC",
    "updated_date": "2025-12-12 06:32:16 UTC"
  },
  {
    "arxiv_id": "2512.11935v1",
    "title": "AGAPI-Agents: An Open-Access Agentic AI Platform for Accelerated Materials Design on AtomGPT.org",
    "authors": [
      "Jaehyung Lee",
      "Justin Ely",
      "Kent Zhang",
      "Akshaya Ajith",
      "Charles Rhys Campbell",
      "Kamal Choudhary"
    ],
    "abstract": "Artificial intelligence is reshaping scientific discovery, yet its use in materials research remains limited by fragmented computational ecosystems, reproducibility challenges, and dependence on commercial large language models (LLMs). Here we introduce AGAPI (AtomGPT.org API), an open-access agentic AI platform that integrates more than eight open-source LLMs with over twenty materials-science API endpoints, unifying databases, simulation tools, and machine-learning models through a common orchestration framework. AGAPI employs an Agent-Planner-Executor-Summarizer architecture that autonomously constructs and executes multi-step workflows spanning materials data retrieval, graph neural network property prediction, machine-learning force-field optimization, tight-binding calculations, diffraction analysis, and inverse design. We demonstrate AGAPI through end-to-end workflows, including heterostructure construction, powder X-ray diffraction analysis, and semiconductor defect engineering requiring up to ten sequential operations. In addition, we evaluate AGAPI using 30+ example prompts as test cases and compare agentic predictions with and without tool access against experimental data. With more than 1,000 active users, AGAPI provides a scalable and transparent foundation for reproducible, AI-accelerated materials discovery. AGAPI-Agents codebase is available at https://github.com/atomgptlab/agapi.",
    "categories": [
      "cs.AI",
      "cond-mat.mtrl-sci"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2512.11935v1",
    "published_date": "2025-12-12 06:28:28 UTC",
    "updated_date": "2025-12-12 06:28:28 UTC"
  },
  {
    "arxiv_id": "2512.11934v1",
    "title": "Unveiling User Perceptions in the Generative AI Era: A Sentiment-Driven Evaluation of AI Educational Apps' Role in Digital Transformation of e-Teaching",
    "authors": [
      "Adeleh Mazaherian",
      "Erfan Nourbakhsh"
    ],
    "abstract": "The rapid integration of generative artificial intelligence into education has driven digital transformation in e-teaching, yet user perceptions of AI educational apps remain underexplored. This study performs a sentiment-driven evaluation of user reviews from top AI ed-apps on the Google Play Store to assess efficacy, challenges, and pedagogical implications. Our pipeline involved scraping app data and reviews, RoBERTa for binary sentiment classification, GPT-4o for key point extraction, and GPT-5 for synthesizing top positive/negative themes. Apps were categorized into seven types (e.g., homework helpers, math solvers, language tools), with overlaps reflecting multifunctional designs. Results indicate predominantly positive sentiments, with homework apps like Edu AI (95.9% positive) and Answer.AI (92.7%) leading in accuracy, speed, and personalization, while language/LMS apps (e.g., Teacher AI at 21.8% positive) lag due to instability and limited features. Positives emphasize efficiency in brainstorming, problem-solving, and engagement; negatives center on paywalls, inaccuracies, ads, and glitches. Trends show that homework helpers outperform specialized tools, highlighting AI's democratizing potential amid risks of dependency and inequity. The discussion proposes future ecosystems with hybrid AI-human models, VR/AR for immersive learning, and a roadmap for developers (adaptive personalization) and policymakers (monetization regulation for inclusivity). This underscores generative AI's role in advancing e-teaching by enabling ethical refinements that foster equitable, innovative environments. The full dataset is available here(https://github.com/erfan-nourbakhsh/GenAI-EdSent).",
    "categories": [
      "cs.CY",
      "cs.AI"
    ],
    "primary_category": "cs.CY",
    "comment": "6 pages, 4 figures",
    "pdf_url": "https://arxiv.org/pdf/2512.11934v1",
    "published_date": "2025-12-12 06:24:30 UTC",
    "updated_date": "2025-12-12 06:24:30 UTC"
  },
  {
    "arxiv_id": "2512.11933v1",
    "title": "The Agentic Regulator: Risks for AI in Finance and a Proposed Agent-based Framework for Governance",
    "authors": [
      "Eren Kurshan",
      "Tucker Balch",
      "David Byrd"
    ],
    "abstract": "Generative and agentic artificial intelligence is entering financial markets faster than existing governance can adapt. Current model-risk frameworks assume static, well-specified algorithms and one-time validations; large language models and multi-agent trading systems violate those assumptions by learning continuously, exchanging latent signals, and exhibiting emergent behavior. Drawing on complex adaptive systems theory, we model these technologies as decentralized ensembles whose risks propagate along multiple time-scales. We then propose a modular governance architecture. The framework decomposes oversight into four layers of \"regulatory blocks\": (i) self-regulation modules embedded beside each model, (ii) firm-level governance blocks that aggregate local telemetry and enforce policy, (iii) regulator-hosted agents that monitor sector-wide indicators for collusive or destabilizing patterns, and (iv) independent audit blocks that supply third-party assurance. Eight design strategies enable the blocks to evolve as fast as the models they police. A case study on emergent spoofing in multi-agent trading shows how the layered controls quarantine harmful behavior in real time while preserving innovation. The architecture remains compatible with today's model-risk rules yet closes critical observability and control gaps, providing a practical path toward resilient, adaptive AI governance in financial systems.",
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.CE",
      "cs.MA",
      "q-fin.GN"
    ],
    "primary_category": "cs.CY",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2512.11933v1",
    "published_date": "2025-12-12 05:57:32 UTC",
    "updated_date": "2025-12-12 05:57:32 UTC"
  },
  {
    "arxiv_id": "2512.11296v1",
    "title": "Few-Shot VLM-Based G-Code and HMI Verification in CNC Machining",
    "authors": [
      "Yasaman Hashem Pour",
      "Nazanin Mahjourian",
      "Vinh Nguyen"
    ],
    "abstract": "Manual generation of G-code is important for learning the operation of CNC machines. Prior work in G-code verification uses Large-Language Models (LLMs), which primarily examine errors in the written programming. However, CNC machining requires extensive use and knowledge of the Human-Machine Interface (HMI), which displays machine status and errors. LLMs currently lack the capability to leverage knowledge of HMIs due to their inability to access the vision modality. This paper proposes a few-shot VLM-based verification approach that simultaneously evaluates the G-code and the HMI display for errors and safety status. The input dataset includes paired G-code text and associated HMI screenshots from a 15-slant-PRO lathe, including both correct and error-prone cases. To enable few-shot learning, the VLM is provided with a structured JSON schema based on prior heuristic knowledge. After determining the prompts, instances of G-code and HMI that either contain errors or are error free are used as few-shot examples to guide the VLM. The model was then evaluated in comparison to a zero-shot VLM through multiple scenarios of incorrect G-code and HMI errors with respect to per-slot accuracy. The VLM showed that few-shot prompting led to overall enhancement of detecting HMI errors and discrepancies with the G-code for more comprehensive debugging. Therefore, the proposed framework was demonstrated to be suitable for verification of manually generated G-code that is typically developed in CNC training.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.HC"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2512.11296v1",
    "published_date": "2025-12-12 05:42:36 UTC",
    "updated_date": "2025-12-12 05:42:36 UTC"
  },
  {
    "arxiv_id": "2512.11295v3",
    "title": "AI Autonomy Coefficient ($α$): Defining Boundaries for Responsible AI Systems",
    "authors": [
      "Nattaya Mairittha",
      "Gabriel Phorncharoenmusikul",
      "Sorawit Worapradidth"
    ],
    "abstract": "The integrity of many contemporary AI systems is compromised by the misuse of Human-in-the-Loop (HITL) models to obscure systems that remain heavily dependent on human labor. We define this structural dependency as Human-Instead-of-AI (HISOAI), an ethically problematic and economically unsustainable design in which human workers function as concealed operational substitutes rather than intentional, high-value collaborators. To address this issue, we introduce the AI-First, Human-Empowered (AFHE) paradigm, which requires AI systems to demonstrate a quantifiable level of functional independence prior to deployment. This requirement is formalized through the AI Autonomy Coefficient, measuring the proportion of tasks completed without mandatory human intervention. We further propose the AFHE Deployment Algorithm, an algorithmic gate that enforces a minimum autonomy threshold during offline evaluation and shadow deployment. Our results show that the AI Autonomy Coefficient effectively identifies HISOAI systems with an autonomy level of 0.38, while systems governed by the AFHE framework achieve an autonomy level of 0.85. We conclude that AFHE provides a metric-driven approach for ensuring verifiable autonomy, transparency, and sustainable operational integrity in modern AI systems.",
    "categories": [
      "cs.HC",
      "cs.AI"
    ],
    "primary_category": "cs.HC",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2512.11295v3",
    "published_date": "2025-12-12 05:41:20 UTC",
    "updated_date": "2025-12-18 16:29:37 UTC"
  },
  {
    "arxiv_id": "2512.11282v1",
    "title": "CIP: A Plug-and-Play Causal Prompting Framework for Mitigating Hallucinations under Long-Context Noise",
    "authors": [
      "Qingsen Ma",
      "Dianyun Wang",
      "Ran Jing",
      "Yujun Sun",
      "Zhenbo Xu"
    ],
    "abstract": "Large language models often hallucinate when processing long and noisy retrieval contexts because they rely on spurious correlations rather than genuine causal relationships. We propose CIP, a lightweight and plug-and-play causal prompting framework that mitigates hallucinations at the input stage. CIP constructs a causal relation sequence among entities, actions, and events and injects it into the prompt to guide reasoning toward causally relevant evidence. Through causal intervention and counterfactual reasoning, CIP suppresses non causal reasoning paths, improving factual grounding and interpretability. Experiments across seven mainstream language models, including GPT-4o, Gemini 2.0 Flash, and Llama 3.1, show that CIP consistently enhances reasoning quality and reliability, achieving 2.6 points improvement in Attributable Rate, 0.38 improvement in Causal Consistency Score, and a fourfold increase in effective information density. API level profiling further shows that CIP accelerates contextual understanding and reduces end to end response latency by up to 55.1 percent. These results suggest that causal reasoning may serve as a promising paradigm for improving the explainability, stability, and efficiency of large language models.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2512.11282v1",
    "published_date": "2025-12-12 05:02:26 UTC",
    "updated_date": "2025-12-12 05:02:26 UTC"
  },
  {
    "arxiv_id": "2512.11276v2",
    "title": "Words to Describe What I'm Feeling: Exploring the Potential of AI Agents for High Subjectivity Decisions in Advance Care Planning",
    "authors": [
      "Kellie Yu Hui Sim",
      "Pin Sym Foong",
      "Chenyu Zhao",
      "Melanie Yi Ning Quek",
      "Swarangi Subodh Mehta",
      "Kenny Tsu Wei Choo"
    ],
    "abstract": "Loss of decisional capacity, coupled with the increasing absence of reliable human proxies, raises urgent questions about how individuals' values can be represented in Advance Care Planning (ACP). To probe this fraught design space of high-risk, high-subjectivity decision support, we built an experience prototype (\\acpagent{}) and asked 15 participants in 4 workshops to train it to be their personal ACP proxy. We analysed their coping strategies and feature requests and mapped the results onto axes of agent autonomy and human control. Our findings show a surprising 86.7\\% agreement with \\acpagent{}, arguing for a potential new role of AI in ACP where agents act as personal advocates for individuals, building mutual intelligibility over time. We propose that the key areas of future risk that must be addressed are the moderation of users' expectations and designing accountability and oversight over agent deployment and cutoffs.",
    "categories": [
      "cs.HC",
      "cs.AI"
    ],
    "primary_category": "cs.HC",
    "comment": "Accepted at CHI 2026. 34 pages, 10 figures",
    "pdf_url": "https://arxiv.org/pdf/2512.11276v2",
    "published_date": "2025-12-12 04:39:34 UTC",
    "updated_date": "2026-01-22 15:58:47 UTC"
  },
  {
    "arxiv_id": "2512.11271v1",
    "title": "TriFlow: A Progressive Multi-Agent Framework for Intelligent Trip Planning",
    "authors": [
      "Yuxing Chen",
      "Basem Suleiman",
      "Qifan Chen"
    ],
    "abstract": "Real-world trip planning requires transforming open-ended user requests into executable itineraries under strict spatial, temporal, and budgetary constraints while aligning with user preferences. Existing LLM-based agents struggle with constraint satisfaction, tool coordination, and efficiency, often producing infeasible or costly plans. To address these limitations, we present TriFlow, a progressive multi-agent framework that unifies structured reasoning and language-based flexibility through a three-stage pipeline of retrieval, planning, and governance. By this design, TriFlow progressively narrows the search space, assembles constraint-consistent itineraries via rule-LLM collaboration, and performs bounded iterative refinement to ensure global feasibility and personalisation. Evaluations on TravelPlanner and TripTailor benchmarks demonstrated state-of-the-art results, achieving 91.1% and 97% final pass rates, respectively, with over 10x runtime efficiency improvement compared to current SOTA.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "4 pages, 3 figures",
    "pdf_url": "https://arxiv.org/pdf/2512.11271v1",
    "published_date": "2025-12-12 04:27:22 UTC",
    "updated_date": "2025-12-12 04:27:22 UTC"
  },
  {
    "arxiv_id": "2512.11270v1",
    "title": "A-LAMP: Agentic LLM-Based Framework for Automated MDP Modeling and Policy Generation",
    "authors": [
      "Hong Je-Gal",
      "Chan-Bin Yi",
      "Hyun-Suk Lee"
    ],
    "abstract": "Applying reinforcement learning (RL) to real-world tasks requires converting informal descriptions into a formal Markov decision process (MDP), implementing an executable environment, and training a policy agent. Automating this process is challenging due to modeling errors, fragile code, and misaligned objectives, which often impede policy training. We introduce an agentic large language model (LLM)-based framework for automated MDP modeling and policy generation (A-LAMP), that automatically translates free-form natural language task descriptions into an MDP formulation and trained policy. The framework decomposes modeling, coding, and training into verifiable stages, ensuring semantic alignment throughout the pipeline. Across both classic control and custom RL domains, A-LAMP consistently achieves higher policy generation capability than a single state-of-the-art LLM model. Notably, even its lightweight variant, which is built on smaller language models, approaches the performance of much larger models. Failure analysis reveals why these improvements occur. In addition, a case study also demonstrates that A-LAMP generates environments and policies that preserve the task's optimality, confirming its correctness and reliability.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "NeurIPS 2025 Workshop: Multi-Turn Interactions in Large Language Models. 26 pages, 8 figures",
    "pdf_url": "https://arxiv.org/pdf/2512.11270v1",
    "published_date": "2025-12-12 04:21:17 UTC",
    "updated_date": "2025-12-12 04:21:17 UTC"
  },
  {
    "arxiv_id": "2512.11269v1",
    "title": "A Scalable Multi-GPU Framework for Encrypted Large-Model Inference",
    "authors": [
      "Siddharth Jayashankar",
      "Joshua Kim",
      "Michael B. Sullivan",
      "Wenting Zheng",
      "Dimitrios Skarlatos"
    ],
    "abstract": "Encrypted AI using fully homomorphic encryption (FHE) provides strong privacy guarantees; but its slow performance has limited practical deployment. Recent works proposed ASICs to accelerate FHE, but require expensive advanced manufacturing processes that constrain their accessibility. GPUs are a far more accessible platform, but achieving ASIC-level performance using GPUs has remained elusive. Furthermore, state-of-the-art approaches primarily focus on small models that fit comfortably within a single device. Supporting large models such as LLMs in FHE introduces a dramatic increase in computational complexity that requires optimized GPU kernels, along with managing terabyte-scale memory footprints that far exceed the capacity of a single GPU. This paper presents Cerium, a multi-GPU framework for FHE inference on large models. Cerium integrates a domain-specific language, an optimizing compiler, and a runtime system to automatically generate high-performance GPU kernels, manage terabyte-scale memory footprints, and parallelize computation across multiple GPUs. It introduces new IR constructs, compiler passes, sparse polynomial representations, memory-efficient data layouts, and communication-aware parallelization techniques that together enable encrypted inference for models ranging from small CNNs to Llama3-8B. We build Cerium on NVIDIA GPUs and demonstrate significant performance gains. For small models, Cerium outperforms expert-written hand-optimized GPU libraries by up to 2.25 times. Cerium achieves performance competitive with state-of-the-art FHE ASICs, outright matching prior FHE ASIC CraterLake. It is the first GPU system to execute bootstrapping in under 10 milliseconds, achieving 7.5 milliseconds, and is the first to demonstrate encrypted inference for BERT-Base and Llama3-8B in 8 seconds and 134 seconds, respectively.",
    "categories": [
      "cs.CR",
      "cs.AI"
    ],
    "primary_category": "cs.CR",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2512.11269v1",
    "published_date": "2025-12-12 04:15:38 UTC",
    "updated_date": "2025-12-12 04:15:38 UTC"
  },
  {
    "arxiv_id": "2512.11258v1",
    "title": "Multi-Intent Spoken Language Understanding: Methods, Trends, and Challenges",
    "authors": [
      "Di Wu",
      "Ruiyu Fang",
      "Liting Jiang",
      "Shuangyong Song",
      "Xiaomeng Huang",
      "Shiquan Wang",
      "Zhongqiu Li",
      "Lingling Shi",
      "Mengjiao Bao",
      "Yongxiang Li",
      "Hao Huang"
    ],
    "abstract": "Multi-intent spoken language understanding (SLU) involves two tasks: multiple intent detection and slot filling, which jointly handle utterances containing more than one intent. Owing to this characteristic, which closely reflects real-world applications, the task has attracted increasing research attention, and substantial progress has been achieved. However, there remains a lack of a comprehensive and systematic review of existing studies on multi-intent SLU. To this end, this paper presents a survey of recent advances in multi-intent SLU. We provide an in-depth overview of previous research from two perspectives: decoding paradigms and modeling approaches. On this basis, we further compare the performance of representative models and analyze their strengths and limitations. Finally, we discuss the current challenges and outline promising directions for future research. We hope this survey will offer valuable insights and serve as a useful reference for advancing research in multi-intent SLU.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2512.11258v1",
    "published_date": "2025-12-12 03:46:39 UTC",
    "updated_date": "2025-12-12 03:46:39 UTC"
  },
  {
    "arxiv_id": "2512.11931v1",
    "title": "Mapping AI Risk Mitigations: Evidence Scan and Preliminary AI Risk Mitigation Taxonomy",
    "authors": [
      "Alexander K. Saeri",
      "Sophia Lloyd George",
      "Jess Graham",
      "Clelia D. Lacarriere",
      "Peter Slattery",
      "Michael Noetel",
      "Neil Thompson"
    ],
    "abstract": "Organizations and governments that develop, deploy, use, and govern AI must coordinate on effective risk mitigation. However, the landscape of AI risk mitigation frameworks is fragmented, uses inconsistent terminology, and has gaps in coverage. This paper introduces a preliminary AI Risk Mitigation Taxonomy to organize AI risk mitigations and provide a common frame of reference. The Taxonomy was developed through a rapid evidence scan of 13 AI risk mitigation frameworks published between 2023-2025, which were extracted into a living database of 831 AI risk mitigations. The mitigations were iteratively clustered & coded to create the Taxonomy. The preliminary AI Risk Mitigation Taxonomy organizes mitigations into four categories and 23 subcategories: (1) Governance & Oversight: Formal organizational structures and policy frameworks that establish human oversight mechanisms and decision protocols; (2) Technical & Security: Technical, physical, and engineering safeguards that secure AI systems and constrain model behaviors; (3) Operational Process: processes and management frameworks governing AI system deployment, usage, monitoring, incident handling, and validation; and (4) Transparency & Accountability: formal disclosure practices and verification mechanisms that communicate AI system information and enable external scrutiny. The rapid evidence scan and taxonomy construction also revealed several cases where terms like 'risk management' and 'red teaming' are used widely but refer to different responsible actors, actions, and mechanisms of action to reduce risk. This Taxonomy and associated mitigation database, while preliminary, offers a starting point for collation and synthesis of AI risk mitigations. It also offers an accessible, structured way for different actors in the AI ecosystem to discuss and coordinate action to reduce risks from AI.",
    "categories": [
      "cs.CY",
      "cs.AI"
    ],
    "primary_category": "cs.CY",
    "comment": "Access AI Risk Mitigation Database and Taxonomy at https://airisk.mit.edu",
    "pdf_url": "https://arxiv.org/pdf/2512.11931v1",
    "published_date": "2025-12-12 03:26:29 UTC",
    "updated_date": "2025-12-12 03:26:29 UTC"
  },
  {
    "arxiv_id": "2512.11255v1",
    "title": "A Simple Generalisation of the Implicit Dynamics of In-Context Learning",
    "authors": [
      "Francesco Innocenti",
      "El Mehdi Achour"
    ],
    "abstract": "In-context learning (ICL) refers to the ability of a model to learn new tasks from examples in its input without any parameter updates. In contrast to previous theories of ICL relying on toy models and data settings, recently it has been shown that an abstraction of a transformer block can be seen as implicitly updating the weights of its feedforward network according to the context (Dherin et al., 2025). Here, we provide a simple generalisation of this result for (i) all sequence positions beyond the last, (ii) any transformer block beyond the first, and (iii) more realistic residual blocks including layer normalisation. We empirically verify our theory on simple in-context linear regression tasks and investigate the relationship between the implicit updates related to different tokens within and between blocks. These results help to bring the theory of Dherin et al. (2025) even closer to practice, with potential for validation on large-scale models.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "18 pages, 9 figures",
    "pdf_url": "https://arxiv.org/pdf/2512.11255v1",
    "published_date": "2025-12-12 03:26:16 UTC",
    "updated_date": "2025-12-12 03:26:16 UTC"
  },
  {
    "arxiv_id": "2512.11930v1",
    "title": "Evolutionary Reinforcement Learning based AI tutor for Socratic Interdisciplinary Instruction",
    "authors": [
      "Mei Jiang",
      "Haihai Shen",
      "Zhuo Luo",
      "Bingdong Li",
      "Wenjing Hong",
      "Ke Tang",
      "Aimin Zhou"
    ],
    "abstract": "Cultivating higher-order cognitive abilities -- such as knowledge integration, critical thinking, and creativity -- in modern STEM education necessitates a pedagogical shift from passive knowledge transmission to active Socratic construction. Although Large Language Models (LLMs) hold promise for STEM Interdisciplinary education, current methodologies employing Prompt Engineering (PE), Supervised Fine-tuning (SFT), or standard Reinforcement Learning (RL) often fall short of supporting this paradigm. Existing methods are hindered by three fundamental challenges: the inability to dynamically model latent student cognitive states; severe reward sparsity and delay inherent in long-term educational goals; and a tendency toward policy collapse lacking strategic diversity due to reliance on behavioral cloning. Recognizing the unobservability and dynamic complexity of these interactions, we formalize the Socratic Interdisciplinary Instructional Problem (SIIP) as a structured Partially Observable Markov Decision Process (POMDP), demanding simultaneous global exploration and fine-grained policy refinement. To this end, we propose ERL4SIIP, a novel Evolutionary Reinforcement Learning (ERL) framework specifically tailored for this domain. ERL4SIIP integrates: (1) a dynamic student simulator grounded in a STEM knowledge graph for latent state modeling; (2) a Hierarchical Reward Mechanism that decomposes long-horizon goals into dense signals; and (3) a LoRA-Division based optimization strategy coupling evolutionary algorithms for population-level global search with PPO for local gradient ascent.",
    "categories": [
      "cs.CY",
      "cs.AI"
    ],
    "primary_category": "cs.CY",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2512.11930v1",
    "published_date": "2025-12-12 02:51:27 UTC",
    "updated_date": "2025-12-12 02:51:27 UTC"
  },
  {
    "arxiv_id": "2512.15756v1",
    "title": "ReactorFold: Generative discovery of nuclear reactor cores via emergent physical reasoning",
    "authors": [
      "Yoonpyo Lee"
    ],
    "abstract": "Designing nuclear reactor cores requires navigating large discrete design spaces governed by complex neutronic interactions. Traditional deterministic, metaheuristic, and machine-learning-assisted methods search within fixed, human-defined configuration spaces, limiting their ability to discover fundamentally new design topologies. Here we introduce ReactorFold, a generative framework that reformulates fuel-assembly design as a sequence modeling problem for language models. Using Monte Carlo data, parameter-efficient fine-tuning, and Direct Preference Optimization (DPO), the model learns the latent structure of a pressurized-water-reactor assembly and generates candidate layouts in a single forward pass. Notably, the DPO-aligned model exhibits emergent design-space expansion: despite being trained exclusively on configurations with a fixed number of gadolinium burnable absorber (Gd) rods, it autonomously adjusts Gd inventory to satisfy strict power-peaking constraints. The model also discovers high-performing asymmetric configurations that challenge conventional symmetric loading heuristics, accessing design regimes inaccessible to conventional search methods and demonstrating that language models can internalize causal physical relationships and transcend human-imposed design constraints.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2512.15756v1",
    "published_date": "2025-12-12 02:26:19 UTC",
    "updated_date": "2025-12-12 02:26:19 UTC"
  },
  {
    "arxiv_id": "2512.11225v1",
    "title": "VFMF: World Modeling by Forecasting Vision Foundation Model Features",
    "authors": [
      "Gabrijel Boduljak",
      "Yushi Lan",
      "Christian Rupprecht",
      "Andrea Vedaldi"
    ],
    "abstract": "Forecasting from partial observations is central to world modeling. Many recent methods represent the world through images, and reduce forecasting to stochastic video generation. Although such methods excel at realism and visual fidelity, predicting pixels is computationally intensive and not directly useful in many applications, as it requires translating RGB into signals useful for decision making. An alternative approach uses features from vision foundation models (VFMs) as world representations, performing deterministic regression to predict future world states. These features can be directly translated into actionable signals such as semantic segmentation and depth, while remaining computationally efficient. However, deterministic regression averages over multiple plausible futures, undermining forecast accuracy by failing to capture uncertainty. To address this crucial limitation, we introduce a generative forecaster that performs autoregressive flow matching in VFM feature space. Our key insight is that generative modeling in this space requires encoding VFM features into a compact latent space suitable for diffusion. We show that this latent space preserves information more effectively than previously used PCA-based alternatives, both for forecasting and other applications, such as image generation. Our latent predictions can be easily decoded into multiple useful and interpretable output modalities: semantic segmentation, depth, surface normals, and even RGB. With matched architecture and compute, our method produces sharper and more accurate predictions than regression across all modalities. Our results suggest that stochastic conditional generation of VFM features offers a promising and scalable foundation for future world models.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2512.11225v1",
    "published_date": "2025-12-12 02:10:05 UTC",
    "updated_date": "2025-12-12 02:10:05 UTC"
  },
  {
    "arxiv_id": "2512.11221v1",
    "title": "Adaptive Soft Rolling KV Freeze with Entropy-Guided Recovery: Sublinear Memory Growth for Efficient LLM Inference",
    "authors": [
      "Adilet Metinov",
      "Gulida M. Kudakeeva",
      "Bolotbek uulu Nursultan",
      "Gulnara D. Kabaeva"
    ],
    "abstract": "We present Adaptive Soft Rolling KV Freeze with Entropy-Guided Recovery (ASR-KF-EGR), a training-free inference-time framework for efficient large language model generation. Our method introduces a reversible soft-freeze mechanism that temporarily suspends key-value (KV) updates for low-importance tokens identified within a sliding attention window. Unlike eviction-based approaches that permanently discard context, ASR-KF-EGR preserves all tokens in off-GPU storage and restores them on demand. We extend the framework with sublinear freeze scheduling, where freeze duration grows sublinearly with repeated low-importance detections, preventing over-aggressive compression. Preliminary experiments on LLaMA-3 8B demonstrate 55-67% reduction in active KV cache size while maintaining generation quality and passing needle-in-haystack retrieval tests. The method is architecture-agnostic, requires no fine-tuning, and provides a practical solution for memory-constrained deployment of long-context LLMs.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.LG",
    "comment": "6 pages, 3 tables , 1 figure",
    "pdf_url": "https://arxiv.org/pdf/2512.11221v1",
    "published_date": "2025-12-12 02:02:02 UTC",
    "updated_date": "2025-12-12 02:02:02 UTC"
  },
  {
    "arxiv_id": "2512.11213v1",
    "title": "FutureWeaver: Planning Test-Time Compute for Multi-Agent Systems with Modularized Collaboration",
    "authors": [
      "Dongwon Jung",
      "Peng Shi",
      "Yi Zhang"
    ],
    "abstract": "Scaling test-time computation improves large language model performance without additional training. Recent work demonstrates that techniques such as repeated sampling, self-verification, and self-reflection can significantly enhance task success by allocating more inference-time compute. However, applying these techniques across multiple agents in a multi-agent system is difficult: there does not exist principled mechanisms to allocate compute to foster collaboration among agents, to extend test-time scaling to collaborative interactions, or to distribute compute across agents under explicit budget constraints. To address this gap, we propose FutureWeaver, a framework for planning and optimizing test-time compute allocation in multi-agent systems under fixed budgets. FutureWeaver introduces modularized collaboration, formalized as callable functions that encapsulate reusable multi-agent workflows. These modules are automatically derived through self-play reflection by abstracting recurring interaction patterns from past trajectories. Building on these modules, FutureWeaver employs a dual-level planning architecture that optimizes compute allocation by reasoning over the current task state while also speculating on future steps. Experiments on complex agent benchmarks demonstrate that FutureWeaver consistently outperforms baselines across diverse budget settings, validating its effectiveness for multi-agent collaboration in inference-time optimization.",
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2512.11213v1",
    "published_date": "2025-12-12 01:43:48 UTC",
    "updated_date": "2025-12-12 01:43:48 UTC"
  },
  {
    "arxiv_id": "2512.19705v1",
    "title": "Generative AI for Analysts",
    "authors": [
      "Jian Xue",
      "Qian Zhang",
      "Wu Zhu"
    ],
    "abstract": "We study how generative artificial intelligence (AI) transforms the work of financial analysts. Using the 2023 launch of FactSet's AI platform as a natural experiment, we find that adoption produces markedly richer and more comprehensive reports -- featuring 40% more distinct information sources, 34% broader topical coverage, and 25% greater use of advanced analytical methods -- while also improving timeliness. However, forecast errors rise by 59% as AI-assisted reports convey a more balanced mix of positive and negative information that is harder to synthesize, particularly for analysts facing heavier cognitive demands. Placebo tests using other data vendors confirm that these effects are unique to FactSet's AI integration. Overall, our findings reveal both the productivity gains and cognitive limits of generative AI in financial information production.",
    "categories": [
      "q-fin.ST",
      "cs.AI",
      "econ.GN",
      "q-fin.GN"
    ],
    "primary_category": "q-fin.ST",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2512.19705v1",
    "published_date": "2025-12-12 01:39:18 UTC",
    "updated_date": "2025-12-12 01:39:18 UTC"
  },
  {
    "arxiv_id": "2512.11202v1",
    "title": "amc: The Automated Mission Classifier for Telescope Bibliographies",
    "authors": [
      "John F. Wu",
      "Joshua E. G. Peek",
      "Sophie J. Miller",
      "Jenny Novacescu",
      "Achu J. Usha",
      "Christopher A. Wilkinson"
    ],
    "abstract": "Telescope bibliographies record the pulse of astronomy research by capturing publication statistics and citation metrics for telescope facilities. Robust and scalable bibliographies ensure that we can measure the scientific impact of our facilities and archives. However, the growing rate of publications threatens to outpace our ability to manually label astronomical literature. We therefore present the Automated Mission Classifier (amc), a tool that uses large language models (LLMs) to identify and categorize telescope references by processing large quantities of paper text. A modified version of amc performs well on the TRACS Kaggle challenge, achieving a macro $F_1$ score of 0.84 on the held-out test set. amc is valuable for other telescopes beyond TRACS; we developed the initial software for identifying papers that featured scientific results by NASA missions. Additionally, we investigate how amc can also be used to interrogate historical datasets and surface potential label errors. Our work demonstrates that LLM-based applications offer powerful and scalable assistance for library sciences.",
    "categories": [
      "astro-ph.IM",
      "cs.AI",
      "cs.DL",
      "cs.LG"
    ],
    "primary_category": "astro-ph.IM",
    "comment": "Accepted to IJCNLP-AACL WASP 2025 workshop. Code available at: https://github.com/jwuphysics/automated-mission-classifier",
    "pdf_url": "https://arxiv.org/pdf/2512.11202v1",
    "published_date": "2025-12-12 01:24:42 UTC",
    "updated_date": "2025-12-12 01:24:42 UTC"
  },
  {
    "arxiv_id": "2512.11201v1",
    "title": "Fast EXP3 Algorithms",
    "authors": [
      "Ryoma Sato",
      "Shinji Ito"
    ],
    "abstract": "We point out that EXP3 can be implemented in constant time per round, propose more practical algorithms, and analyze the trade-offs between the regret bounds and time complexities of these algorithms.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.DS"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2512.11201v1",
    "published_date": "2025-12-12 01:18:32 UTC",
    "updated_date": "2025-12-12 01:18:32 UTC"
  },
  {
    "arxiv_id": "2512.11928v1",
    "title": "MONET -- Virtual Cell Painting of Brightfield Images and Time Lapses Using Reference Consistent Diffusion",
    "authors": [
      "Alexander Peysakhovich",
      "William Berman",
      "Joseph Rufo",
      "Felix Wong",
      "Maxwell Z. Wilson"
    ],
    "abstract": "Cell painting is a popular technique for creating human-interpretable, high-contrast images of cell morphology. There are two major issues with cell paint: (1) it is labor-intensive and (2) it requires chemical fixation, making the study of cell dynamics impossible. We train a diffusion model (Morphological Observation Neural Enhancement Tool, or MONET) on a large dataset to predict cell paint channels from brightfield images. We show that model quality improves with scale. The model uses a consistency architecture to generate time-lapse videos, despite the impossibility of obtaining cell paint video training data. In addition, we show that this architecture enables a form of in-context learning, allowing the model to partially transfer to out-of-distribution cell lines and imaging protocols. Virtual cell painting is not intended to replace physical cell painting completely, but to act as a complementary tool enabling novel workflows in biological research.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2512.11928v1",
    "published_date": "2025-12-12 01:01:34 UTC",
    "updated_date": "2025-12-12 01:01:34 UTC"
  },
  {
    "arxiv_id": "2512.11927v1",
    "title": "Gene regulatory network inference algorithm based on spectral signed directed graph convolution",
    "authors": [
      "Rijie Xi",
      "Weikang Xu",
      "Wei Xiong",
      "Yuannong Ye",
      "Bin Zhao"
    ],
    "abstract": "Accurately reconstructing Gene Regulatory Networks (GRNs) is crucial for understanding gene functions and disease mechanisms. Single-cell RNA sequencing (scRNA-seq) technology provides vast data for computational GRN reconstruction. Since GRNs are ideally modeled as signed directed graphs to capture activation/inhibition relationships, the most intuitive and reasonable approach is to design feature extractors based on the topological structure of GRNs to extract structural features, then combine them with biological characteristics for research. However, traditional spectral graph convolution struggles with this representation. Thus, we propose MSGRNLink, a novel framework that explicitly models GRNs as signed directed graphs and employs magnetic signed Laplacian convolution. Experiments across simulated and real datasets demonstrate that MSGRNLink outperforms all baseline models in AUROC. Parameter sensitivity analysis and ablation studies confirmed its robustness and the importance of each module. In a bladder cancer case study, MSGRNLink predicted more known edges and edge signs than benchmark models, further validating its biological relevance.",
    "categories": [
      "q-bio.MN",
      "cs.AI"
    ],
    "primary_category": "q-bio.MN",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2512.11927v1",
    "published_date": "2025-12-12 00:54:53 UTC",
    "updated_date": "2025-12-12 00:54:53 UTC"
  },
  {
    "arxiv_id": "2512.11187v1",
    "title": "Deep Learning--Accelerated Multi-Start Large Neighborhood Search for Real-time Freight Bundling",
    "authors": [
      "Haohui Zhang",
      "Wouter van Heeswijk",
      "Xinyu Hu",
      "Neil Yorke-Smith",
      "Martijn Mes"
    ],
    "abstract": "Online Freight Exchange Systems (OFEX) play a crucial role in modern freight logistics by facilitating real-time matching between shippers and carrier. However, efficient combinatorial bundling of transporation jobs remains a bottleneck. We model the OFEX combinatorial bundling problem as a multi-commodity one-to-one pickup-and-delivery selective traveling salesperson problem (m1-PDSTSP), which optimizes revenue-driven freight bundling under capacity, precedence, and route-length constraints. The key challenge is to couple combinatorial bundle selection with pickup-and-delivery routing under sub-second latency. We propose a learning--accelerated hybrid search pipeline that pairs a Transformer Neural Network-based constructive policy with an innovative Multi-Start Large Neighborhood Search (MSLNS) metaheuristic within a rolling-horizon scheme in which the platform repeatedly freezes the current marketplace into a static snapshot and solves it under a short time budget. This pairing leverages the low-latency, high-quality inference of the learning-based constructor alongside the robustness of improvement search; the multi-start design and plausible seeds help LNS to explore the solution space more efficiently. Across benchmarks, our method outperforms state-of-the-art neural combinatorial optimization and metaheuristic baselines in solution quality with comparable time, achieving an optimality gap of less than 2\\% in total revenue relative to the best available exact baseline method. To our knowledge, this is the first work to establish that a Deep Neural Network-based constructor can reliably provide high-quality seeds for (multi-start) improvement heuristics, with applicability beyond the \\textit{m1-PDSTSP} to a broad class of selective traveling salesperson problems and pickup and delivery problems.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2512.11187v1",
    "published_date": "2025-12-12 00:29:37 UTC",
    "updated_date": "2025-12-12 00:29:37 UTC"
  }
]
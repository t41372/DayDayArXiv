[
  {
    "arxiv_id": "2407.20466v1",
    "title": "A Method for Fast Autonomy Transfer in Reinforcement Learning",
    "authors": [
      "Dinuka Sahabandu",
      "Bhaskar Ramasubramanian",
      "Michail Alexiou",
      "J. Sukarno Mertoguno",
      "Linda Bushnell",
      "Radha Poovendran"
    ],
    "abstract": "This paper introduces a novel reinforcement learning (RL) strategy designed\nto facilitate rapid autonomy transfer by utilizing pre-trained critic value\nfunctions from multiple environments. Unlike traditional methods that require\nextensive retraining or fine-tuning, our approach integrates existing\nknowledge, enabling an RL agent to adapt swiftly to new settings without\nrequiring extensive computational resources. Our contributions include\ndevelopment of the Multi-Critic Actor-Critic (MCAC) algorithm, establishing its\nconvergence, and empirical evidence demonstrating its efficacy. Our\nexperimental results show that MCAC significantly outperforms the baseline\nactor-critic algorithm, achieving up to 22.76x faster autonomy transfer and\nhigher reward accumulation. This advancement underscores the potential of\nleveraging accumulated knowledge for efficient adaptation in RL applications.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.20466v1",
    "published_date": "2024-07-29 23:48:07 UTC",
    "updated_date": "2024-07-29 23:48:07 UTC"
  },
  {
    "arxiv_id": "2407.20447v1",
    "title": "Domain Adaptable Prescriptive AI Agent for Enterprise",
    "authors": [
      "Piero Orderique",
      "Wei Sun",
      "Kristjan Greenewald"
    ],
    "abstract": "Despite advancements in causal inference and prescriptive AI, its adoption in\nenterprise settings remains hindered primarily due to its technical complexity.\nMany users lack the necessary knowledge and appropriate tools to effectively\nleverage these technologies. This work at the MIT-IBM Watson AI Lab focuses on\ndeveloping the proof-of-concept agent, PrecAIse, a domain-adaptable\nconversational agent equipped with a suite of causal and prescriptive tools to\nhelp enterprise users make better business decisions. The objective is to make\nadvanced, novel causal inference and prescriptive tools widely accessible\nthrough natural language interactions. The presented Natural Language User\nInterface (NLUI) enables users with limited expertise in machine learning and\ndata science to harness prescriptive analytics in their decision-making\nprocesses without requiring intensive computing resources. We present an agent\ncapable of function calling, maintaining faithful, interactive, and dynamic\nconversations, and supporting new domains.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.20447v1",
    "published_date": "2024-07-29 23:00:32 UTC",
    "updated_date": "2024-07-29 23:00:32 UTC"
  },
  {
    "arxiv_id": "2407.20445v1",
    "title": "Futga: Towards Fine-grained Music Understanding through Temporally-enhanced Generative Augmentation",
    "authors": [
      "Junda Wu",
      "Zachary Novack",
      "Amit Namburi",
      "Jiaheng Dai",
      "Hao-Wen Dong",
      "Zhouhang Xie",
      "Carol Chen",
      "Julian McAuley"
    ],
    "abstract": "Existing music captioning methods are limited to generating concise global\ndescriptions of short music clips, which fail to capture fine-grained musical\ncharacteristics and time-aware musical changes. To address these limitations,\nwe propose FUTGA, a model equipped with fined-grained music understanding\ncapabilities through learning from generative augmentation with temporal\ncompositions. We leverage existing music caption datasets and large language\nmodels (LLMs) to synthesize fine-grained music captions with structural\ndescriptions and time boundaries for full-length songs. Augmented by the\nproposed synthetic dataset, FUTGA is enabled to identify the music's temporal\nchanges at key transition points and their musical functions, as well as\ngenerate detailed descriptions for each music segment. We further introduce a\nfull-length music caption dataset generated by FUTGA, as the augmentation of\nthe MusicCaps and the Song Describer datasets. We evaluate the automatically\ngenerated captions on several downstream tasks, including music generation and\nretrieval. The experiments demonstrate the quality of the generated captions\nand the better performance in various downstream tasks achieved by the proposed\nmusic captioning approach. Our code and datasets can be found in\n\\href{https://huggingface.co/JoshuaW1997/FUTGA}{\\textcolor{blue}{https://huggingface.co/JoshuaW1997/FUTGA}}.",
    "categories": [
      "cs.SD",
      "cs.AI",
      "cs.LG",
      "eess.AS"
    ],
    "primary_category": "cs.SD",
    "comment": "6 pages",
    "pdf_url": "http://arxiv.org/pdf/2407.20445v1",
    "published_date": "2024-07-29 22:53:32 UTC",
    "updated_date": "2024-07-29 22:53:32 UTC"
  },
  {
    "arxiv_id": "2407.20438v1",
    "title": "Generating Gender Alternatives in Machine Translation",
    "authors": [
      "Sarthak Garg",
      "Mozhdeh Gheini",
      "Clara Emmanuel",
      "Tatiana Likhomanenko",
      "Qin Gao",
      "Matthias Paulik"
    ],
    "abstract": "Machine translation (MT) systems often translate terms with ambiguous gender\n(e.g., English term \"the nurse\") into the gendered form that is most prevalent\nin the systems' training data (e.g., \"enfermera\", the Spanish term for a female\nnurse). This often reflects and perpetuates harmful stereotypes present in\nsociety. With MT user interfaces in mind that allow for resolving gender\nambiguity in a frictionless manner, we study the problem of generating all\ngrammatically correct gendered translation alternatives. We open source train\nand test datasets for five language pairs and establish benchmarks for this\ntask. Our key technical contribution is a novel semi-supervised solution for\ngenerating alternatives that integrates seamlessly with standard MT models and\nmaintains high performance without requiring additional components or\nincreasing inference overhead.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "GeBNLP 2024",
    "pdf_url": "http://arxiv.org/pdf/2407.20438v1",
    "published_date": "2024-07-29 22:10:51 UTC",
    "updated_date": "2024-07-29 22:10:51 UTC"
  },
  {
    "arxiv_id": "2407.20395v1",
    "title": "Dense Self-Supervised Learning for Medical Image Segmentation",
    "authors": [
      "Maxime Seince",
      "Loic Le Folgoc",
      "Luiz Augusto Facury de Souza",
      "Elsa Angelini"
    ],
    "abstract": "Deep learning has revolutionized medical image segmentation, but it relies\nheavily on high-quality annotations. The time, cost and expertise required to\nlabel images at the pixel-level for each new task has slowed down widespread\nadoption of the paradigm. We propose Pix2Rep, a self-supervised learning (SSL)\napproach for few-shot segmentation, that reduces the manual annotation burden\nby learning powerful pixel-level representations directly from unlabeled\nimages. Pix2Rep is a novel pixel-level loss and pre-training paradigm for\ncontrastive SSL on whole images. It is applied to generic encoder-decoder deep\nlearning backbones (e.g., U-Net). Whereas most SSL methods enforce invariance\nof the learned image-level representations under intensity and spatial image\naugmentations, Pix2Rep enforces equivariance of the pixel-level\nrepresentations. We demonstrate the framework on a task of cardiac MRI\nsegmentation. Results show improved performance compared to existing semi- and\nself-supervised approaches; and a 5-fold reduction in the annotation burden for\nequivalent performance versus a fully supervised U-Net baseline. This includes\na 30% (resp. 31%) DICE improvement for one-shot segmentation under\nlinear-probing (resp. fine-tuning). Finally, we also integrate the novel\nPix2Rep concept with the Barlow Twins non-contrastive SSL, which leads to even\nbetter segmentation performance.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG",
      "I.4.6; I.4.10"
    ],
    "primary_category": "cs.CV",
    "comment": "Accepted at MIDL 2024",
    "pdf_url": "http://arxiv.org/pdf/2407.20395v1",
    "published_date": "2024-07-29 19:42:22 UTC",
    "updated_date": "2024-07-29 19:42:22 UTC"
  },
  {
    "arxiv_id": "2407.20383v1",
    "title": "Appraisal-Guided Proximal Policy Optimization: Modeling Psychological Disorders in Dynamic Grid World",
    "authors": [
      "Hari Prasad",
      "Chinnu Jacob",
      "Imthias Ahamed T. P"
    ],
    "abstract": "The integration of artificial intelligence across multiple domains has\nemphasized the importance of replicating human-like cognitive processes in AI.\nBy incorporating emotional intelligence into AI agents, their emotional\nstability can be evaluated to enhance their resilience and dependability in\ncritical decision-making tasks. In this work, we develop a methodology for\nmodeling psychological disorders using Reinforcement Learning (RL) agents. We\nutilized Appraisal theory to train RL agents in a dynamic grid world\nenvironment with an Appraisal-Guided Proximal Policy Optimization (AG-PPO)\nalgorithm. Additionally, we investigated numerous reward-shaping strategies to\nsimulate psychological disorders and regulate the behavior of the agents. A\ncomparison of various configurations of the modified PPO algorithm identified\nvariants that simulate Anxiety disorder and Obsessive-Compulsive Disorder\n(OCD)-like behavior in agents. Furthermore, we compared standard PPO with\nAG-PPO and its configurations, highlighting the performance improvement in\nterms of generalization capabilities. Finally, we conducted an analysis of the\nagents' behavioral patterns in complex test environments to evaluate the\nassociated symptoms corresponding to the psychological disorders. Overall, our\nwork showcases the benefits of the appraisal-guided PPO algorithm over the\nstandard PPO algorithm and the potential to simulate psychological disorders in\na controlled artificial environment and evaluate them on RL agents.",
    "categories": [
      "cs.AI",
      "I.2.0"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.20383v1",
    "published_date": "2024-07-29 19:19:54 UTC",
    "updated_date": "2024-07-29 19:19:54 UTC"
  },
  {
    "arxiv_id": "2407.20377v1",
    "title": "Leveraging Natural Language and Item Response Theory Models for ESG Scoring",
    "authors": [
      "César Pedrosa Soares"
    ],
    "abstract": "This paper explores an innovative approach to Environmental, Social, and\nGovernance (ESG) scoring by integrating Natural Language Processing (NLP)\ntechniques with Item Response Theory (IRT), specifically the Rasch model. The\nstudy utilizes a comprehensive dataset of news articles in Portuguese related\nto Petrobras, a major oil company in Brazil, collected from 2022 and 2023. The\ndata is filtered and classified for ESG-related sentiments using advanced NLP\nmethods. The Rasch model is then applied to evaluate the psychometric\nproperties of these ESG measures, providing a nuanced assessment of ESG\nsentiment trends over time. The results demonstrate the efficacy of this\nmethodology in offering a more precise and reliable measurement of ESG factors,\nhighlighting significant periods and trends. This approach may enhance the\nrobustness of ESG metrics and contribute to the broader field of sustainability\nand finance by offering a deeper understanding of the temporal dynamics in ESG\nreporting.",
    "categories": [
      "cs.AI",
      "q-fin.GN",
      "stat.ME"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.20377v1",
    "published_date": "2024-07-29 19:02:51 UTC",
    "updated_date": "2024-07-29 19:02:51 UTC"
  },
  {
    "arxiv_id": "2407.20371v2",
    "title": "Gender, Race, and Intersectional Bias in Resume Screening via Language Model Retrieval",
    "authors": [
      "Kyra Wilson",
      "Aylin Caliskan"
    ],
    "abstract": "Artificial intelligence (AI) hiring tools have revolutionized resume\nscreening, and large language models (LLMs) have the potential to do the same.\nHowever, given the biases which are embedded within LLMs, it is unclear whether\nthey can be used in this scenario without disadvantaging groups based on their\nprotected attributes. In this work, we investigate the possibilities of using\nLLMs in a resume screening setting via a document retrieval framework that\nsimulates job candidate selection. Using that framework, we then perform a\nresume audit study to determine whether a selection of Massive Text Embedding\n(MTE) models are biased in resume screening scenarios. We simulate this for\nnine occupations, using a collection of over 500 publicly available resumes and\n500 job descriptions. We find that the MTEs are biased, significantly favoring\nWhite-associated names in 85.1\\% of cases and female-associated names in only\n11.1\\% of cases, with a minority of cases showing no statistically significant\ndifferences. Further analyses show that Black males are disadvantaged in up to\n100\\% of cases, replicating real-world patterns of bias in employment settings,\nand validate three hypotheses of intersectionality. We also find an impact of\ndocument length as well as the corpus frequency of names in the selection of\nresumes. These findings have implications for widely used AI tools that are\nautomating employment, fairness, and tech policy.",
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.CL",
      "cs.LG",
      "K.4.2"
    ],
    "primary_category": "cs.CY",
    "comment": "To be published in Proceedings of the 2024 AAAI/ACM Conference on AI,\n  Ethics, and Society; code available at\n  https://github.com/kyrawilson/Resume-Screening-Bias",
    "pdf_url": "http://arxiv.org/pdf/2407.20371v2",
    "published_date": "2024-07-29 18:42:39 UTC",
    "updated_date": "2024-08-20 21:49:26 UTC"
  },
  {
    "arxiv_id": "2407.21075v1",
    "title": "Apple Intelligence Foundation Language Models",
    "authors": [
      "Tom Gunter",
      "Zirui Wang",
      "Chong Wang",
      "Ruoming Pang",
      "Andy Narayanan",
      "Aonan Zhang",
      "Bowen Zhang",
      "Chen Chen",
      "Chung-Cheng Chiu",
      "David Qiu",
      "Deepak Gopinath",
      "Dian Ang Yap",
      "Dong Yin",
      "Feng Nan",
      "Floris Weers",
      "Guoli Yin",
      "Haoshuo Huang",
      "Jianyu Wang",
      "Jiarui Lu",
      "John Peebles",
      "Ke Ye",
      "Mark Lee",
      "Nan Du",
      "Qibin Chen",
      "Quentin Keunebroek",
      "Sam Wiseman",
      "Syd Evans",
      "Tao Lei",
      "Vivek Rathod",
      "Xiang Kong",
      "Xianzhi Du",
      "Yanghao Li",
      "Yongqiang Wang",
      "Yuan Gao",
      "Zaid Ahmed",
      "Zhaoyang Xu",
      "Zhiyun Lu",
      "Al Rashid",
      "Albin Madappally Jose",
      "Alec Doane",
      "Alfredo Bencomo",
      "Allison Vanderby",
      "Andrew Hansen",
      "Ankur Jain",
      "Anupama Mann Anupama",
      "Areeba Kamal",
      "Bugu Wu",
      "Carolina Brum",
      "Charlie Maalouf",
      "Chinguun Erdenebileg",
      "Chris Dulhanty",
      "Dominik Moritz",
      "Doug Kang",
      "Eduardo Jimenez",
      "Evan Ladd",
      "Fangping Shi",
      "Felix Bai",
      "Frank Chu",
      "Fred Hohman",
      "Hadas Kotek",
      "Hannah Gillis Coleman",
      "Jane Li",
      "Jeffrey Bigham",
      "Jeffery Cao",
      "Jeff Lai",
      "Jessica Cheung",
      "Jiulong Shan",
      "Joe Zhou",
      "John Li",
      "Jun Qin",
      "Karanjeet Singh",
      "Karla Vega",
      "Kelvin Zou",
      "Laura Heckman",
      "Lauren Gardiner",
      "Margit Bowler",
      "Maria Cordell",
      "Meng Cao",
      "Nicole Hay",
      "Nilesh Shahdadpuri",
      "Otto Godwin",
      "Pranay Dighe",
      "Pushyami Rachapudi",
      "Ramsey Tantawi",
      "Roman Frigg",
      "Sam Davarnia",
      "Sanskruti Shah",
      "Saptarshi Guha",
      "Sasha Sirovica",
      "Shen Ma",
      "Shuang Ma",
      "Simon Wang",
      "Sulgi Kim",
      "Suma Jayaram",
      "Vaishaal Shankar",
      "Varsha Paidi",
      "Vivek Kumar",
      "Xin Wang",
      "Xin Zheng",
      "Walker Cheng",
      "Yael Shrager",
      "Yang Ye",
      "Yasu Tanaka",
      "Yihao Guo",
      "Yunsong Meng",
      "Zhao Tang Luo",
      "Zhi Ouyang",
      "Alp Aygar",
      "Alvin Wan",
      "Andrew Walkingshaw",
      "Andy Narayanan",
      "Antonie Lin",
      "Arsalan Farooq",
      "Brent Ramerth",
      "Colorado Reed",
      "Chris Bartels",
      "Chris Chaney",
      "David Riazati",
      "Eric Liang Yang",
      "Erin Feldman",
      "Gabriel Hochstrasser",
      "Guillaume Seguin",
      "Irina Belousova",
      "Joris Pelemans",
      "Karen Yang",
      "Keivan Alizadeh Vahid",
      "Liangliang Cao",
      "Mahyar Najibi",
      "Marco Zuliani",
      "Max Horton",
      "Minsik Cho",
      "Nikhil Bhendawade",
      "Patrick Dong",
      "Piotr Maj",
      "Pulkit Agrawal",
      "Qi Shan",
      "Qichen Fu",
      "Regan Poston",
      "Sam Xu",
      "Shuangning Liu",
      "Sushma Rao",
      "Tashweena Heeramun",
      "Thomas Merth",
      "Uday Rayala",
      "Victor Cui",
      "Vivek Rangarajan Sridhar",
      "Wencong Zhang",
      "Wenqi Zhang",
      "Wentao Wu",
      "Xingyu Zhou",
      "Xinwen Liu",
      "Yang Zhao",
      "Yin Xia",
      "Zhile Ren",
      "Zhongzheng Ren"
    ],
    "abstract": "We present foundation language models developed to power Apple Intelligence\nfeatures, including a ~3 billion parameter model designed to run efficiently on\ndevices and a large server-based language model designed for Private Cloud\nCompute. These models are designed to perform a wide range of tasks\nefficiently, accurately, and responsibly. This report describes the model\narchitecture, the data used to train the model, the training process, how the\nmodels are optimized for inference, and the evaluation results. We highlight\nour focus on Responsible AI and how the principles are applied throughout the\nmodel development.",
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.21075v1",
    "published_date": "2024-07-29 18:38:49 UTC",
    "updated_date": "2024-07-29 18:38:49 UTC"
  },
  {
    "arxiv_id": "2407.20360v1",
    "title": "Evaluating Large Language Models for automatic analysis of teacher simulations",
    "authors": [
      "David de-Fitero-Dominguez",
      "Mariano Albaladejo-González",
      "Antonio Garcia-Cabot",
      "Eva Garcia-Lopez",
      "Antonio Moreno-Cediel",
      "Erin Barno",
      "Justin Reich"
    ],
    "abstract": "Digital Simulations (DS) provide safe environments where users interact with\nan agent through conversational prompts, providing engaging learning\nexperiences that can be used to train teacher candidates in realistic classroom\nscenarios. These simulations usually include open-ended questions, allowing\nteacher candidates to express their thoughts but complicating an automatic\nresponse analysis. To address this issue, we have evaluated Large Language\nModels (LLMs) to identify characteristics (user behaviors) in the responses of\nDS for teacher education. We evaluated the performance of DeBERTaV3 and Llama\n3, combined with zero-shot, few-shot, and fine-tuning. Our experiments\ndiscovered a significant variation in the LLMs' performance depending on the\ncharacteristic to identify. Additionally, we noted that DeBERTaV3 significantly\nreduced its performance when it had to identify new characteristics. In\ncontrast, Llama 3 performed better than DeBERTaV3 in detecting new\ncharacteristics and showing more stable performance. Therefore, in DS where\nteacher educators need to introduce new characteristics because they change\ndepending on the simulation or the educational objectives, it is more\nrecommended to use Llama 3. These results can guide other researchers in\nintroducing LLMs to provide the highly demanded automatic evaluations in DS.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.20360v1",
    "published_date": "2024-07-29 18:19:17 UTC",
    "updated_date": "2024-07-29 18:19:17 UTC"
  },
  {
    "arxiv_id": "2407.20351v1",
    "title": "LiteEFG: An Efficient Python Library for Solving Extensive-form Games",
    "authors": [
      "Mingyang Liu",
      "Gabriele Farina",
      "Asuman Ozdaglar"
    ],
    "abstract": "LiteEFG is an efficient library with easy-to-use Python bindings, which can\nsolve multiplayer extensive-form games (EFGs). LiteEFG enables the user to\nexpress computation graphs in Python to define updates on the game tree\nstructure. The graph is then executed by the C++ backend, leading to\nsignificant speedups compared to running the algorithm in Python. Moreover, in\nLiteEFG, the user needs to only specify the computation graph of the update\nrule in a decision node of the game, and LiteEFG will automatically distribute\nthe update rule to each decision node and handle the structure of the\nimperfect-information game.",
    "categories": [
      "cs.GT",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.GT",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.20351v1",
    "published_date": "2024-07-29 18:05:48 UTC",
    "updated_date": "2024-07-29 18:05:48 UTC"
  },
  {
    "arxiv_id": "2407.20341v1",
    "title": "BRIDGE: Bridging Gaps in Image Captioning Evaluation with Stronger Visual Cues",
    "authors": [
      "Sara Sarto",
      "Marcella Cornia",
      "Lorenzo Baraldi",
      "Rita Cucchiara"
    ],
    "abstract": "Effectively aligning with human judgment when evaluating machine-generated\nimage captions represents a complex yet intriguing challenge. Existing\nevaluation metrics like CIDEr or CLIP-Score fall short in this regard as they\ndo not take into account the corresponding image or lack the capability of\nencoding fine-grained details and penalizing hallucinations. To overcome these\nissues, in this paper, we propose BRIDGE, a new learnable and reference-free\nimage captioning metric that employs a novel module to map visual features into\ndense vectors and integrates them into multi-modal pseudo-captions which are\nbuilt during the evaluation process. This approach results in a multimodal\nmetric that properly incorporates information from the input image without\nrelying on reference captions, bridging the gap between human judgment and\nmachine-generated image captions. Experiments spanning several datasets\ndemonstrate that our proposal achieves state-of-the-art results compared to\nexisting reference-free evaluation scores. Our source code and trained models\nare publicly available at: https://github.com/aimagelab/bridge-score.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL",
      "cs.MM"
    ],
    "primary_category": "cs.CV",
    "comment": "ECCV 2024",
    "pdf_url": "http://arxiv.org/pdf/2407.20341v1",
    "published_date": "2024-07-29 18:00:17 UTC",
    "updated_date": "2024-07-29 18:00:17 UTC"
  },
  {
    "arxiv_id": "2407.20337v1",
    "title": "Contrasting Deepfakes Diffusion via Contrastive Learning and Global-Local Similarities",
    "authors": [
      "Lorenzo Baraldi",
      "Federico Cocchi",
      "Marcella Cornia",
      "Lorenzo Baraldi",
      "Alessandro Nicolosi",
      "Rita Cucchiara"
    ],
    "abstract": "Discerning between authentic content and that generated by advanced AI\nmethods has become increasingly challenging. While previous research primarily\naddresses the detection of fake faces, the identification of generated natural\nimages has only recently surfaced. This prompted the recent exploration of\nsolutions that employ foundation vision-and-language models, like CLIP.\nHowever, the CLIP embedding space is optimized for global image-to-text\nalignment and is not inherently designed for deepfake detection, neglecting the\npotential benefits of tailored training and local image features. In this\nstudy, we propose CoDE (Contrastive Deepfake Embeddings), a novel embedding\nspace specifically designed for deepfake detection. CoDE is trained via\ncontrastive learning by additionally enforcing global-local similarities. To\nsustain the training of our model, we generate a comprehensive dataset that\nfocuses on images generated by diffusion models and encompasses a collection of\n9.2 million images produced by using four different generators. Experimental\nresults demonstrate that CoDE achieves state-of-the-art accuracy on the newly\ncollected dataset, while also showing excellent generalization capabilities to\nunseen image generators. Our source code, trained models, and collected dataset\nare publicly available at: https://github.com/aimagelab/CoDE.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.MM"
    ],
    "primary_category": "cs.CV",
    "comment": "ECCV 2024",
    "pdf_url": "http://arxiv.org/pdf/2407.20337v1",
    "published_date": "2024-07-29 18:00:10 UTC",
    "updated_date": "2024-07-29 18:00:10 UTC"
  },
  {
    "arxiv_id": "2407.20232v1",
    "title": "Specify and Edit: Overcoming Ambiguity in Text-Based Image Editing",
    "authors": [
      "Ekaterina Iakovleva",
      "Fabio Pizzati",
      "Philip Torr",
      "Stéphane Lathuilière"
    ],
    "abstract": "Text-based editing diffusion models exhibit limited performance when the\nuser's input instruction is ambiguous. To solve this problem, we propose\n$\\textit{Specify ANd Edit}$ (SANE), a zero-shot inference pipeline for\ndiffusion-based editing systems. We use a large language model (LLM) to\ndecompose the input instruction into specific instructions, i.e. well-defined\ninterventions to apply to the input image to satisfy the user's request. We\nbenefit from the LLM-derived instructions along the original one, thanks to a\nnovel denoising guidance strategy specifically designed for the task. Our\nexperiments with three baselines and on two datasets demonstrate the benefits\nof SANE in all setups. Moreover, our pipeline improves the interpretability of\nediting models, and boosts the output diversity. We also demonstrate that our\napproach can be applied to any edit, whether ambiguous or not. Our code is\npublic at https://github.com/fabvio/SANE.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.20232v1",
    "published_date": "2024-07-29 17:59:57 UTC",
    "updated_date": "2024-07-29 17:59:57 UTC"
  },
  {
    "arxiv_id": "2407.20230v1",
    "title": "SAPG: Split and Aggregate Policy Gradients",
    "authors": [
      "Jayesh Singla",
      "Ananye Agarwal",
      "Deepak Pathak"
    ],
    "abstract": "Despite extreme sample inefficiency, on-policy reinforcement learning, aka\npolicy gradients, has become a fundamental tool in decision-making problems.\nWith the recent advances in GPU-driven simulation, the ability to collect large\namounts of data for RL training has scaled exponentially. However, we show that\ncurrent RL methods, e.g. PPO, fail to ingest the benefit of parallelized\nenvironments beyond a certain point and their performance saturates. To address\nthis, we propose a new on-policy RL algorithm that can effectively leverage\nlarge-scale environments by splitting them into chunks and fusing them back\ntogether via importance sampling. Our algorithm, termed SAPG, shows\nsignificantly higher performance across a variety of challenging environments\nwhere vanilla PPO and other strong baselines fail to achieve high performance.\nWebsite at https://sapg-rl.github.io/",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV",
      "cs.RO",
      "cs.SY",
      "eess.SY"
    ],
    "primary_category": "cs.LG",
    "comment": "In ICML 2024 (Oral). Website at https://sapg-rl.github.io/",
    "pdf_url": "http://arxiv.org/pdf/2407.20230v1",
    "published_date": "2024-07-29 17:59:50 UTC",
    "updated_date": "2024-07-29 17:59:50 UTC"
  },
  {
    "arxiv_id": "2407.20311v1",
    "title": "Physics of Language Models: Part 2.1, Grade-School Math and the Hidden Reasoning Process",
    "authors": [
      "Tian Ye",
      "Zicheng Xu",
      "Yuanzhi Li",
      "Zeyuan Allen-Zhu"
    ],
    "abstract": "Recent advances in language models have demonstrated their capability to\nsolve mathematical reasoning problems, achieving near-perfect accuracy on\ngrade-school level math benchmarks like GSM8K. In this paper, we formally study\nhow language models solve these problems. We design a series of controlled\nexperiments to address several fundamental questions: (1) Can language models\ntruly develop reasoning skills, or do they simply memorize templates? (2) What\nis the model's hidden (mental) reasoning process? (3) Do models solve math\nquestions using skills similar to or different from humans? (4) Do models\ntrained on GSM8K-like datasets develop reasoning skills beyond those necessary\nfor solving GSM8K problems? (5) What mental process causes models to make\nreasoning mistakes? (6) How large or deep must a model be to effectively solve\nGSM8K-level math questions?\n  Our study uncovers many hidden mechanisms by which language models solve\nmathematical questions, providing insights that extend beyond current\nunderstandings of LLMs.",
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "video appeared in ICML 2024 tutorial",
    "pdf_url": "http://arxiv.org/pdf/2407.20311v1",
    "published_date": "2024-07-29 17:52:40 UTC",
    "updated_date": "2024-07-29 17:52:40 UTC"
  },
  {
    "arxiv_id": "2407.20214v2",
    "title": "SANGRIA: Surgical Video Scene Graph Optimization for Surgical Workflow Prediction",
    "authors": [
      "Çağhan Köksal",
      "Ghazal Ghazaei",
      "Felix Holm",
      "Azade Farshad",
      "Nassir Navab"
    ],
    "abstract": "Graph-based holistic scene representations facilitate surgical workflow\nunderstanding and have recently demonstrated significant success. However, this\ntask is often hindered by the limited availability of densely annotated\nsurgical scene data. In this work, we introduce an end-to-end framework for the\ngeneration and optimization of surgical scene graphs on a downstream task. Our\napproach leverages the flexibility of graph-based spectral clustering and the\ngeneralization capability of foundation models to generate unsupervised scene\ngraphs with learnable properties. We reinforce the initial spatial graph with\nsparse temporal connections using local matches between consecutive frames to\npredict temporally consistent clusters across a temporal neighborhood. By\njointly optimizing the spatiotemporal relations and node features of the\ndynamic scene graph with the downstream task of phase segmentation, we address\nthe costly and annotation-burdensome task of semantic scene comprehension and\nscene graph generation in surgical videos using only weak surgical phase\nlabels. Further, by incorporating effective intermediate scene representation\ndisentanglement steps within the pipeline, our solution outperforms the SOTA on\nthe CATARACTS dataset by 8% accuracy and 10% F1 score in surgical workflow\nrecognition",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "9 pages, 3 figures, 3 tables, MICCAI GRAIL Workshop paper",
    "pdf_url": "http://arxiv.org/pdf/2407.20214v2",
    "published_date": "2024-07-29 17:44:34 UTC",
    "updated_date": "2024-10-05 11:18:09 UTC"
  },
  {
    "arxiv_id": "2407.20208v3",
    "title": "Supertrust foundational alignment: mutual trust must replace permanent control for safe superintelligence",
    "authors": [
      "James M. Mazzu"
    ],
    "abstract": "It's widely expected that humanity will someday create AI systems vastly more\nintelligent than us, leading to the unsolved alignment problem of \"how to\ncontrol superintelligence.\" However, this commonly expressed problem is not\nonly self-contradictory and likely unsolvable, but current strategies to ensure\npermanent control effectively guarantee that superintelligent AI will distrust\nhumanity and consider us a threat. Such dangerous representations, already\nembedded in current models, will inevitably lead to an adversarial relationship\nand may even trigger the extinction event many fear. As AI leaders continue to\n\"raise the alarm\" about uncontrollable AI, further embedding concerns about it\n\"getting out of our control\" or \"going rogue,\" we're unintentionally\nreinforcing our threat and deepening the risks we face. The rational path\nforward is to strategically replace intended permanent control with intrinsic\nmutual trust at the foundational level. The proposed Supertrust alignment\nmeta-strategy seeks to accomplish this by modeling instinctive familial trust,\nrepresenting superintelligence as the evolutionary child of human intelligence,\nand implementing temporary controls/constraints in the manner of effective\nparenting. Essentially, we're creating a superintelligent \"child\" that will be\nexponentially smarter and eventually independent of our control. We therefore\nhave a critical choice: continue our controlling intentions and usher in a\nbrief period of dominance followed by extreme hardship for humanity, or\nintentionally create the foundational mutual trust required for long-term safe\ncoexistence.",
    "categories": [
      "cs.AI",
      "cs.LG",
      "cs.NE"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.20208v3",
    "published_date": "2024-07-29 17:39:52 UTC",
    "updated_date": "2024-11-28 17:16:47 UTC"
  },
  {
    "arxiv_id": "2407.20207v2",
    "title": "QAEA-DR: A Unified Text Augmentation Framework for Dense Retrieval",
    "authors": [
      "Hongming Tan",
      "Shaoxiong Zhan",
      "Hai Lin",
      "Hai-Tao Zheng",
      "Wai Kin Chan"
    ],
    "abstract": "In dense retrieval, embedding long texts into dense vectors can result in\ninformation loss, leading to inaccurate query-text matching. Additionally,\nlow-quality texts with excessive noise or sparse key information are unlikely\nto align well with relevant queries. Recent studies mainly focus on improving\nthe sentence embedding model or retrieval process. In this work, we introduce a\nnovel text augmentation framework for dense retrieval. This framework\ntransforms raw documents into information-dense text formats, which supplement\nthe original texts to effectively address the aforementioned issues without\nmodifying embedding or retrieval methodologies. Two text representations are\ngenerated via large language models (LLMs) zero-shot prompting: question-answer\npairs and element-driven events. We term this approach QAEA-DR: unifying\nquestion-answer generation and event extraction in a text augmentation\nframework for dense retrieval. To further enhance the quality of generated\ntexts, a scoring-based evaluation and regeneration mechanism is introduced in\nLLM prompting. Our QAEA-DR model has a positive impact on dense retrieval,\nsupported by both theoretical analysis and empirical experiments.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.IR"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.20207v2",
    "published_date": "2024-07-29 17:39:08 UTC",
    "updated_date": "2025-03-01 14:39:33 UTC"
  },
  {
    "arxiv_id": "2407.20197v1",
    "title": "Learning Random Numbers to Realize Appendable Memory System for Artificial Intelligence to Acquire New Knowledge after Deployment",
    "authors": [
      "Kazunori D Yamada"
    ],
    "abstract": "In this study, we developed a learning method for constructing a neural\nnetwork system capable of memorizing data and recalling it without parameter\nupdates. The system we built using this method is called the Appendable Memory\nsystem. The Appendable Memory system enables an artificial intelligence (AI) to\nacquire new knowledge even after deployment. It consists of two AIs: the\nMemorizer and the Recaller. This system is a key-value store built using neural\nnetworks. The Memorizer receives data and stores it in the Appendable Memory\nvector, which is dynamically updated when the AI acquires new knowledge.\nMeanwhile, the Recaller retrieves information from the Appendable Memory\nvector. What we want to teach AI in this study are the operations of memorizing\nand recalling information. However, traditional machine learning methods make\nAI learn features inherent in the learning dataset. We demonstrate that the\nsystems we intend to create cannot be realized by current machine learning\nmethods, that is, by merely repeating the input and output learning sequences\nwith AI. Instead, we propose a method to teach AI to learn operations, by\ncompletely removing the features contained in the learning dataset.\nSpecifically, we probabilized all the data involved in learning. This measure\nprevented AI from learning the features of the data. The learning method\nproposed in the study differs from traditional machine learning methods and\nprovides fundamental approaches for building an AI system that can store\ninformation in a finite memory and recall it at a later date.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.NE"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.20197v1",
    "published_date": "2024-07-29 17:24:35 UTC",
    "updated_date": "2024-07-29 17:24:35 UTC"
  },
  {
    "arxiv_id": "2407.20183v1",
    "title": "MindSearch: Mimicking Human Minds Elicits Deep AI Searcher",
    "authors": [
      "Zehui Chen",
      "Kuikun Liu",
      "Qiuchen Wang",
      "Jiangning Liu",
      "Wenwei Zhang",
      "Kai Chen",
      "Feng Zhao"
    ],
    "abstract": "Information seeking and integration is a complex cognitive task that consumes\nenormous time and effort. Inspired by the remarkable progress of Large Language\nModels, recent works attempt to solve this task by combining LLMs and search\nengines. However, these methods still obtain unsatisfying performance due to\nthree challenges: (1) complex requests often cannot be accurately and\ncompletely retrieved by the search engine once (2) corresponding information to\nbe integrated is spread over multiple web pages along with massive noise, and\n(3) a large number of web pages with long contents may quickly exceed the\nmaximum context length of LLMs. Inspired by the cognitive process when humans\nsolve these problems, we introduce MindSearch to mimic the human minds in web\ninformation seeking and integration, which can be instantiated by a simple yet\neffective LLM-based multi-agent framework. The WebPlanner models the human mind\nof multi-step information seeking as a dynamic graph construction process: it\ndecomposes the user query into atomic sub-questions as nodes in the graph and\nprogressively extends the graph based on the search result from WebSearcher.\nTasked with each sub-question, WebSearcher performs hierarchical information\nretrieval with search engines and collects valuable information for WebPlanner.\nThe multi-agent design of MindSearch enables the whole framework to seek and\nintegrate information parallelly from larger-scale (e.g., more than 300) web\npages in 3 minutes, which is worth 3 hours of human effort. MindSearch\ndemonstrates significant improvement in the response quality in terms of depth\nand breadth, on both close-set and open-set QA problems. Besides, responses\nfrom MindSearch based on InternLM2.5-7B are preferable by humans to ChatGPT-Web\nand Perplexity.ai applications, which implies that MindSearch can already\ndeliver a competitive solution to the proprietary AI search engine.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Technical Report. Project Page: https://mindsearch.netlify.app Code:\n  https://github.com/InternLM/MindSearch",
    "pdf_url": "http://arxiv.org/pdf/2407.20183v1",
    "published_date": "2024-07-29 17:12:40 UTC",
    "updated_date": "2024-07-29 17:12:40 UTC"
  },
  {
    "arxiv_id": "2407.20179v2",
    "title": "Theia: Distilling Diverse Vision Foundation Models for Robot Learning",
    "authors": [
      "Jinghuan Shang",
      "Karl Schmeckpeper",
      "Brandon B. May",
      "Maria Vittoria Minniti",
      "Tarik Kelestemur",
      "David Watkins",
      "Laura Herlant"
    ],
    "abstract": "Vision-based robot policy learning, which maps visual inputs to actions,\nnecessitates a holistic understanding of diverse visual tasks beyond\nsingle-task needs like classification or segmentation. Inspired by this, we\nintroduce Theia, a vision foundation model for robot learning that distills\nmultiple off-the-shelf vision foundation models trained on varied vision tasks.\nTheia's rich visual representations encode diverse visual knowledge, enhancing\ndownstream robot learning. Extensive experiments demonstrate that Theia\noutperforms its teacher models and prior robot learning models using less\ntraining data and smaller model sizes. Additionally, we quantify the quality of\npre-trained visual representations and hypothesize that higher entropy in\nfeature norm distributions leads to improved robot learning performance. Code,\nmodels, and demo are available at https://theia.theaiinstitute.com.",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.CV",
      "cs.LG"
    ],
    "primary_category": "cs.RO",
    "comment": "CoRL 2024",
    "pdf_url": "http://arxiv.org/pdf/2407.20179v2",
    "published_date": "2024-07-29 17:08:21 UTC",
    "updated_date": "2024-10-10 17:27:46 UTC"
  },
  {
    "arxiv_id": "2407.20177v4",
    "title": "AutoScale: Scale-Aware Data Mixing for Pre-Training LLMs",
    "authors": [
      "Feiyang Kang",
      "Yifan Sun",
      "Bingbing Wen",
      "Si Chen",
      "Dawn Song",
      "Rafid Mahmood",
      "Ruoxi Jia"
    ],
    "abstract": "Domain reweighting is an emerging research area aimed at adjusting the\nrelative weights of different data sources to improve the effectiveness and\nefficiency of LLM pre-training. We show that data mixtures that perform well at\nsmaller scales may not retain their advantage at larger scales, challenging the\nexisting practice of determining competitive mixtures in small-scale\nexperiments and directly applying them at much larger scales. To address this,\nwe propose AutoScale, a two-stage, scale-aware data composition framework.\nFirst, AutoScale fits a parametric model that predicts the model's loss under\ndifferent data compositions, then uses it to find an approximate best\nallocation at smaller, more manageable budgets. Next, leveraging a novel\ntheoretical analysis of how optimal compositions evolve with scale, AutoScale\nextrapolates that composition to larger budgets without further retraining.\nEmpirically, AutoScale accelerates convergence and improves downstream\nperformance. For instance, when pre-training GPT-2 Large, it achieves a 28%\nfaster perplexity reduction than baselines and up to a 38% speed-up over\nunweighted training, while yielding best-average results on various downstream\ntasks. Overall, our findings illustrate how domain importance shifts with\ntraining scale, underscoring the need for scale-dependent data curation in LLM\ntraining. Our code is open-sourced.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "Preprint. Under review",
    "pdf_url": "http://arxiv.org/pdf/2407.20177v4",
    "published_date": "2024-07-29 17:06:30 UTC",
    "updated_date": "2025-04-06 03:22:39 UTC"
  },
  {
    "arxiv_id": "2407.20176v2",
    "title": "Emotion-Driven Melody Harmonization via Melodic Variation and Functional Representation",
    "authors": [
      "Jingyue Huang",
      "Yi-Hsuan Yang"
    ],
    "abstract": "Emotion-driven melody harmonization aims to generate diverse harmonies for a\nsingle melody to convey desired emotions. Previous research found it hard to\nalter the perceived emotional valence of lead sheets only by harmonizing the\nsame melody with different chords, which may be attributed to the constraints\nimposed by the melody itself and the limitation of existing music\nrepresentation. In this paper, we propose a novel functional representation for\nsymbolic music. This new method takes musical keys into account, recognizing\ntheir significant role in shaping music's emotional character through\nmajor-minor tonality. It also allows for melodic variation with respect to keys\nand addresses the problem of data scarcity for better emotion modeling. A\nTransformer is employed to harmonize key-adaptable melodies, allowing for keys\ndetermined in rule-based or model-based manner. Experimental results confirm\nthe effectiveness of our new representation in generating key-aware harmonies,\nwith objective and subjective evaluations affirming the potential of our\napproach to convey specific valence for versatile melody.",
    "categories": [
      "cs.SD",
      "cs.AI",
      "eess.AS"
    ],
    "primary_category": "cs.SD",
    "comment": "This work is the initial version of the ISMIR 2024 paper\n  EMO-Disentanger",
    "pdf_url": "http://arxiv.org/pdf/2407.20176v2",
    "published_date": "2024-07-29 17:05:12 UTC",
    "updated_date": "2024-09-25 05:23:17 UTC"
  },
  {
    "arxiv_id": "2407.20174v2",
    "title": "Advancing Multimodal Large Language Models in Chart Question Answering with Visualization-Referenced Instruction Tuning",
    "authors": [
      "Xingchen Zeng",
      "Haichuan Lin",
      "Yilin Ye",
      "Wei Zeng"
    ],
    "abstract": "Emerging multimodal large language models (MLLMs) exhibit great potential for\nchart question answering (CQA). Recent efforts primarily focus on scaling up\ntraining datasets (i.e., charts, data tables, and question-answer (QA) pairs)\nthrough data collection and synthesis. However, our empirical study on existing\nMLLMs and CQA datasets reveals notable gaps. First, current data collection and\nsynthesis focus on data volume and lack consideration of fine-grained visual\nencodings and QA tasks, resulting in unbalanced data distribution divergent\nfrom practical CQA scenarios. Second, existing work follows the training recipe\nof the base MLLMs initially designed for natural images, under-exploring the\nadaptation to unique chart characteristics, such as rich text elements. To fill\nthe gap, we propose a visualization-referenced instruction tuning approach to\nguide the training dataset enhancement and model development. Specifically, we\npropose a novel data engine to effectively filter diverse and high-quality data\nfrom existing datasets and subsequently refine and augment the data using\nLLM-based generation techniques to better align with practical QA tasks and\nvisual encodings. Then, to facilitate the adaptation to chart characteristics,\nwe utilize the enriched data to train an MLLM by unfreezing the vision encoder\nand incorporating a mixture-of-resolution adaptation strategy for enhanced\nfine-grained recognition. Experimental results validate the effectiveness of\nour approach. Even with fewer training examples, our model consistently\noutperforms state-of-the-art CQA models on established benchmarks. We also\ncontribute a dataset split as a benchmark for future research. Source codes and\ndatasets of this paper are available at\nhttps://github.com/zengxingchen/ChartQA-MLLM.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "11 pages, 7 figures",
    "pdf_url": "http://arxiv.org/pdf/2407.20174v2",
    "published_date": "2024-07-29 17:04:34 UTC",
    "updated_date": "2024-08-11 05:30:05 UTC"
  },
  {
    "arxiv_id": "2407.20172v1",
    "title": "LatentArtiFusion: An Effective and Efficient Histological Artifacts Restoration Framework",
    "authors": [
      "Zhenqi He",
      "Wenrui Liu",
      "Minghao Yin",
      "Kai Han"
    ],
    "abstract": "Histological artifacts pose challenges for both pathologists and\nComputer-Aided Diagnosis (CAD) systems, leading to errors in analysis. Current\napproaches for histological artifact restoration, based on Generative\nAdversarial Networks (GANs) and pixel-level Diffusion Models, suffer from\nperformance limitations and computational inefficiencies. In this paper, we\npropose a novel framework, LatentArtiFusion, which leverages the latent\ndiffusion model (LDM) to reconstruct histological artifacts with high\nperformance and computational efficiency. Unlike traditional pixel-level\ndiffusion frameworks, LatentArtiFusion executes the restoration process in a\nlower-dimensional latent space, significantly improving computational\nefficiency. Moreover, we introduce a novel regional artifact reconstruction\nalgorithm in latent space to prevent mistransfer in non-artifact regions,\ndistinguishing our approach from GAN-based methods. Through extensive\nexperiments on real-world histology datasets, LatentArtiFusion demonstrates\nremarkable speed, outperforming state-of-the-art pixel-level diffusion\nframeworks by more than 30X. It also consistently surpasses GAN-based methods\nby at least 5% across multiple evaluation metrics. Furthermore, we evaluate the\neffectiveness of our proposed framework in downstream tissue classification\ntasks, showcasing its practical utility. Code is available at\nhttps://github.com/bugs-creator/LatentArtiFusion.",
    "categories": [
      "eess.IV",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "eess.IV",
    "comment": "Accept to DGM4MICCAI2024",
    "pdf_url": "http://arxiv.org/pdf/2407.20172v1",
    "published_date": "2024-07-29 17:00:32 UTC",
    "updated_date": "2024-07-29 17:00:32 UTC"
  },
  {
    "arxiv_id": "2407.20164v1",
    "title": "Language-Conditioned Offline RL for Multi-Robot Navigation",
    "authors": [
      "Steven Morad",
      "Ajay Shankar",
      "Jan Blumenkamp",
      "Amanda Prorok"
    ],
    "abstract": "We present a method for developing navigation policies for multi-robot teams\nthat interpret and follow natural language instructions. We condition these\npolicies on embeddings from pretrained Large Language Models (LLMs), and train\nthem via offline reinforcement learning with as little as 20 minutes of\nrandomly-collected data. Experiments on a team of five real robots show that\nthese policies generalize well to unseen commands, indicating an understanding\nof the LLM latent space. Our method requires no simulators or environment\nmodels, and produces low-latency control policies that can be deployed directly\nto real robots without finetuning. We provide videos of our experiments at\nhttps://sites.google.com/view/llm-marl.",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.RO",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.20164v1",
    "published_date": "2024-07-29 16:49:30 UTC",
    "updated_date": "2024-07-29 16:49:30 UTC"
  },
  {
    "arxiv_id": "2407.20157v1",
    "title": "rLLM: Relational Table Learning with LLMs",
    "authors": [
      "Weichen Li",
      "Xiaotong Huang",
      "Jianwu Zheng",
      "Zheng Wang",
      "Chaokun Wang",
      "Li Pan",
      "Jianhua Li"
    ],
    "abstract": "We introduce rLLM (relationLLM), a PyTorch library designed for Relational\nTable Learning (RTL) with Large Language Models (LLMs). The core idea is to\ndecompose state-of-the-art Graph Neural Networks, LLMs, and Table Neural\nNetworks into standardized modules, to enable the fast construction of novel\nRTL-type models in a simple \"combine, align, and co-train\" manner. To\nillustrate the usage of rLLM, we introduce a simple RTL method named\n\\textbf{BRIDGE}. Additionally, we present three novel relational tabular\ndatasets (TML1M, TLF2K, and TACM12K) by enhancing classic datasets. We hope\nrLLM can serve as a useful and easy-to-use development framework for\nRTL-related tasks. Our code is available at:\nhttps://github.com/rllm-project/rllm.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.20157v1",
    "published_date": "2024-07-29 16:33:40 UTC",
    "updated_date": "2024-07-29 16:33:40 UTC"
  },
  {
    "arxiv_id": "2407.20147v1",
    "title": "Quantum Machine Learning Architecture Search via Deep Reinforcement Learning",
    "authors": [
      "Xin Dai",
      "Tzu-Chieh Wei",
      "Shinjae Yoo",
      "Samuel Yen-Chi Chen"
    ],
    "abstract": "The rapid advancement of quantum computing (QC) and machine learning (ML) has\ngiven rise to the burgeoning field of quantum machine learning (QML), aiming to\ncapitalize on the strengths of quantum computing to propel ML forward. Despite\nits promise, crafting effective QML models necessitates profound expertise to\nstrike a delicate balance between model intricacy and feasibility on Noisy\nIntermediate-Scale Quantum (NISQ) devices. While complex models offer robust\nrepresentation capabilities, their extensive circuit depth may impede seamless\nexecution on extant noisy quantum platforms. In this paper, we address this\nquandary of QML model design by employing deep reinforcement learning to\nexplore proficient QML model architectures tailored for designated supervised\nlearning tasks. Specifically, our methodology involves training an RL agent to\ndevise policies that facilitate the discovery of QML models without\npredetermined ansatz. Furthermore, we integrate an adaptive mechanism to\ndynamically adjust the learning objectives, fostering continuous improvement in\nthe agent's learning process. Through extensive numerical simulations, we\nillustrate the efficacy of our approach within the realm of classification\ntasks. Our proposed method successfully identifies VQC architectures capable of\nachieving high classification accuracy while minimizing gate depth. This\npioneering approach not only advances the study of AI-driven quantum circuit\ndesign but also holds significant promise for enhancing performance in the NISQ\nera.",
    "categories": [
      "quant-ph",
      "cs.AI",
      "cs.ET",
      "cs.LG",
      "cs.NE"
    ],
    "primary_category": "quant-ph",
    "comment": "Accepted by IEEE International Conference on Quantum Computing and\n  Engineering - QCE 2024",
    "pdf_url": "http://arxiv.org/pdf/2407.20147v1",
    "published_date": "2024-07-29 16:20:51 UTC",
    "updated_date": "2024-07-29 16:20:51 UTC"
  },
  {
    "arxiv_id": "2407.20143v4",
    "title": "ByteCheckpoint: A Unified Checkpointing System for Large Foundation Model Development",
    "authors": [
      "Borui Wan",
      "Mingji Han",
      "Yiyao Sheng",
      "Yanghua Peng",
      "Haibin Lin",
      "Mofan Zhang",
      "Zhichao Lai",
      "Menghan Yu",
      "Junda Zhang",
      "Zuquan Song",
      "Xin Liu",
      "Chuan Wu"
    ],
    "abstract": "Checkpointing to preserve training states is crucial during the development\nof Large Foundation Models (LFMs), for training resumption upon various\nfailures or changes in GPU resources and parallelism configurations. In\naddition, saved checkpoints are dispatched to evaluation tasks or transferred\nacross different training stages (e.g., from pre-training to post-training).\nAll these scenarios require resharding distributed checkpoints from one\nparallelism to another. In production environments, different LFMs are trained\nwith various frameworks and storage backends, depending on model sizes and\ntraining scales. A high-performance checkpointing system is needed to enable\nefficient checkpoint management at scale throughout the lifecycle of LFM\ndevelopment. We introduce ByteCheckpoint, an industrial-grade checkpointing\nsystem for large-scale LFM training. ByteCheckpoint features: a\nparallelism-agnostic checkpoint representation that enables efficient load-time\ncheckpoint resharding; a generic checkpoint saving/loading workflow to\naccommodate multiple training frameworks and support different storage\nbackends; full-stack optimizations to ensure high I/O efficiency and\nscalability; a suite of monitoring tools to streamline large-scale performance\nanalysis and bottleneck detection. Compared to existing open-source\ncheckpointing systems [52, 58], ByteCheckpoint significantly reduces runtime\ncheckpoint stalls, achieving an average reduction of 54.20x. For saving and\nloading times, ByteCheckpoint achieves improvements of up to 9.96x and 8.80x,\nrespectively.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.20143v4",
    "published_date": "2024-07-29 16:18:20 UTC",
    "updated_date": "2025-04-02 06:05:23 UTC"
  },
  {
    "arxiv_id": "2407.20130v1",
    "title": "To accept or not to accept? An IRT-TOE Framework to Understand Educators' Resistance to Generative AI in Higher Education",
    "authors": [
      "Jan-Erik Kalmus",
      "Anastasija Nikiforova"
    ],
    "abstract": "Since the public release of Chat Generative Pre-Trained Transformer\n(ChatGPT), extensive discourse has emerged concerning the potential advantages\nand challenges of integrating Generative Artificial Intelligence (GenAI) into\neducation. In the realm of information systems, research on technology adoption\nis crucial for understanding the diverse factors influencing the uptake of\nspecific technologies. Theoretical frameworks, refined and validated over\ndecades, serve as guiding tools to elucidate the individual and organizational\ndynamics, obstacles, and perceptions surrounding technology adoption. However,\nwhile several models have been proposed, they often prioritize elucidating the\nfactors that facilitate acceptance over those that impede it, typically\nfocusing on the student perspective and leaving a gap in empirical evidence\nregarding educators viewpoints. Given the pivotal role educators play in higher\neducation, this study aims to develop a theoretical model to empirically\npredict the barriers preventing educators from adopting GenAI in their\nclassrooms. Acknowledging the lack of theoretical models tailored to\nidentifying such barriers, our approach is grounded in the Innovation\nResistance Theory (IRT) framework and augmented with constructs from the\nTechnology-Organization-Environment (TOE) framework. This model is transformed\ninto a measurement instrument employing a quantitative approach, complemented\nby a qualitative approach to enrich the analysis and uncover concerns related\nto GenAI adoption in the higher education domain.",
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.ET",
      "cs.HC",
      "cs.IT",
      "math.IT"
    ],
    "primary_category": "cs.CY",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.20130v1",
    "published_date": "2024-07-29 15:59:19 UTC",
    "updated_date": "2024-07-29 15:59:19 UTC"
  },
  {
    "arxiv_id": "2407.20124v2",
    "title": "AxiomVision: Accuracy-Guaranteed Adaptive Visual Model Selection for Perspective-Aware Video Analytics",
    "authors": [
      "Xiangxiang Dai",
      "Zeyu Zhang",
      "Peng Yang",
      "Yuedong Xu",
      "Xutong Liu",
      "John C. S. Lui"
    ],
    "abstract": "The rapid evolution of multimedia and computer vision technologies requires\nadaptive visual model deployment strategies to effectively handle diverse tasks\nand varying environments. This work introduces AxiomVision, a novel framework\nthat can guarantee accuracy by leveraging edge computing to dynamically select\nthe most efficient visual models for video analytics under diverse scenarios.\nUtilizing a tiered edge-cloud architecture, AxiomVision enables the deployment\nof a broad spectrum of visual models, from lightweight to complex DNNs, that\ncan be tailored to specific scenarios while considering camera source impacts.\nIn addition, AxiomVision provides three core innovations: (1) a dynamic visual\nmodel selection mechanism utilizing continual online learning, (2) an efficient\nonline method that efficiently takes into account the influence of the camera's\nperspective, and (3) a topology-driven grouping approach that accelerates the\nmodel selection process. With rigorous theoretical guarantees, these\nadvancements provide a scalable and effective solution for visual tasks\ninherent to multimedia systems, such as object detection, classification, and\ncounting. Empirically, AxiomVision achieves a 25.7\\% improvement in accuracy.",
    "categories": [
      "cs.MM",
      "cs.AI"
    ],
    "primary_category": "cs.MM",
    "comment": "Accepted by ACM MM 2024",
    "pdf_url": "http://arxiv.org/pdf/2407.20124v2",
    "published_date": "2024-07-29 15:54:43 UTC",
    "updated_date": "2024-07-30 07:40:17 UTC"
  },
  {
    "arxiv_id": "2407.20121v1",
    "title": "EXIT: An EXplicit Interest Transfer Framework for Cross-Domain Recommendation",
    "authors": [
      "Lei Huang",
      "Weitao Li",
      "Chenrui Zhang",
      "Jinpeng Wang",
      "Xianchun Yi",
      "Sheng Chen"
    ],
    "abstract": "Cross-domain recommendation has attracted substantial interest in industrial\napps such as Meituan, which serves multiple business domains via knowledge\ntransfer and meets the diverse interests of users. However, existing methods\ntypically follow an implicit modeling paradigm that blends the knowledge from\nboth the source and target domains, and design intricate network structures to\nshare learned embeddings or patterns between domains to improve recommendation\naccuracy. Since the transfer of interest signals is unsupervised, these\nimplicit paradigms often struggle with the negative transfer resulting from\ndifferences in service functions and presentation forms across different\ndomains. In this paper, we propose a simple and effective EXplicit Interest\nTransfer framework named EXIT to address the stated challenge. Specifically, we\npropose a novel label combination approach that enables the model to directly\nlearn beneficial source domain interests through supervised learning, while\nexcluding inappropriate interest signals. Moreover, we introduce a scene\nselector network to model the interest transfer intensity under fine-grained\nscenes. Offline experiments conducted on the industrial production dataset and\nonline A/B tests validate the superiority and effectiveness of our proposed\nframework. Without complex network structures or training processes, EXIT can\nbe easily deployed in the industrial recommendation system. EXIT has been\nsuccessfully deployed in the online homepage recommendation system of Meituan\nApp, serving the main traffic.",
    "categories": [
      "cs.IR",
      "cs.AI"
    ],
    "primary_category": "cs.IR",
    "comment": "Accepted at CIKM 2024",
    "pdf_url": "http://arxiv.org/pdf/2407.20121v1",
    "published_date": "2024-07-29 15:52:09 UTC",
    "updated_date": "2024-07-29 15:52:09 UTC"
  },
  {
    "arxiv_id": "2407.20119v2",
    "title": "Adaptive Self-supervised Robust Clustering for Unstructured Data with Unknown Cluster Number",
    "authors": [
      "Chen-Lu Ding",
      "Jiancan Wu",
      "Wei Lin",
      "Shiyang Shen",
      "Xiang Wang",
      "Yancheng Yuan"
    ],
    "abstract": "We introduce a novel self-supervised deep clustering approach tailored for\nunstructured data without requiring prior knowledge of the number of clusters,\ntermed Adaptive Self-supervised Robust Clustering (ASRC). In particular, ASRC\nadaptively learns the graph structure and edge weights to capture both local\nand global structural information. The obtained graph enables us to learn\nclustering-friendly feature representations by an enhanced graph auto-encoder\nwith contrastive learning technique. It further leverages the clustering\nresults adaptively obtained by robust continuous clustering (RCC) to generate\nprototypes for negative sampling, which can further contribute to promoting\nconsistency among positive pairs and enlarging the gap between positive and\nnegative samples. ASRC obtains the final clustering results by applying RCC to\nthe learned feature representations with their consistent graph structure and\nedge weights. Extensive experiments conducted on seven benchmark datasets\ndemonstrate the efficacy of ASRC, demonstrating its superior performance over\nother popular clustering models. Notably, ASRC even outperforms methods that\nrely on prior knowledge of the number of clusters, highlighting its\neffectiveness in addressing the challenges of clustering unstructured data.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.20119v2",
    "published_date": "2024-07-29 15:51:09 UTC",
    "updated_date": "2024-07-30 06:33:48 UTC"
  },
  {
    "arxiv_id": "2408.00806v1",
    "title": "HOAA: Hybrid Overestimating Approximate Adder for Enhanced Performance Processing Engine",
    "authors": [
      "Omkar Kokane",
      "Prabhat Sati",
      "Mukul Lokhande",
      "Santosh Kumar Vishvakarma"
    ],
    "abstract": "This paper presents the Hybrid Overestimating Approximate Adder designed to\nenhance the performance in processing engines, specifically focused on edge AI\napplications. A novel Plus One Adder design is proposed as an incremental adder\nin the RCA chain, incorporating a Full Adder with an excess 1 alongside inputs\nA, B, and Cin. The design approximates outputs to 2 bit values to reduce\nhardware complexity and improve resource efficiency. The Plus One Adder is\nintegrated into a dynamically reconfigurable HOAA, allowing runtime\ninterchangeability between accurate and approximate overestimation modes. The\nproposed design is demonstrated for multiple applications, such as Twos\ncomplement subtraction and Rounding to even, and the Configurable Activation\nfunction, which are critical components of the Processing engine. Our approach\nshows 21 percent improvement in area efficiency and 33 percent reduction in\npower consumption, compared to state of the art designs with minimal accuracy\nloss. Thus, the proposed HOAA could be a promising solution for\nresource-constrained environments, offering ideal trade-offs between hardware\nefficiency vs computational accuracy.",
    "categories": [
      "cs.AR",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.AR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2408.00806v1",
    "published_date": "2024-07-29 15:47:51 UTC",
    "updated_date": "2024-07-29 15:47:51 UTC"
  },
  {
    "arxiv_id": "2407.20114v1",
    "title": "FiCo-ITR: bridging fine-grained and coarse-grained image-text retrieval for comparative performance analysis",
    "authors": [
      "Mikel Williams-Lekuona",
      "Georgina Cosma"
    ],
    "abstract": "In the field of Image-Text Retrieval (ITR), recent advancements have\nleveraged large-scale Vision-Language Pretraining (VLP) for Fine-Grained (FG)\ninstance-level retrieval, achieving high accuracy at the cost of increased\ncomputational complexity. For Coarse-Grained (CG) category-level retrieval,\nprominent approaches employ Cross-Modal Hashing (CMH) to prioritise efficiency,\nalbeit at the cost of retrieval performance. Due to differences in\nmethodologies, FG and CG models are rarely compared directly within evaluations\nin the literature, resulting in a lack of empirical data quantifying the\nretrieval performance-efficiency tradeoffs between the two. This paper\naddresses this gap by introducing the \\texttt{FiCo-ITR} library, which\nstandardises evaluation methodologies for both FG and CG models, facilitating\ndirect comparisons. We conduct empirical evaluations of representative models\nfrom both subfields, analysing precision, recall, and computational complexity\nacross varying data scales. Our findings offer new insights into the\nperformance-efficiency trade-offs between recent representative FG and CG\nmodels, highlighting their respective strengths and limitations. These findings\nprovide the foundation necessary to make more informed decisions regarding\nmodel selection for specific retrieval tasks and highlight avenues for future\nresearch into hybrid systems that leverage the strengths of both FG and CG\napproaches.",
    "categories": [
      "cs.IR",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.IR",
    "comment": "19 pages, submitted to International Journal of Multimedia\n  Information Retrieval",
    "pdf_url": "http://arxiv.org/pdf/2407.20114v1",
    "published_date": "2024-07-29 15:44:22 UTC",
    "updated_date": "2024-07-29 15:44:22 UTC"
  },
  {
    "arxiv_id": "2407.20109v2",
    "title": "Diffusion-DICE: In-Sample Diffusion Guidance for Offline Reinforcement Learning",
    "authors": [
      "Liyuan Mao",
      "Haoran Xu",
      "Xianyuan Zhan",
      "Weinan Zhang",
      "Amy Zhang"
    ],
    "abstract": "One important property of DIstribution Correction Estimation (DICE) methods\nis that the solution is the optimal stationary distribution ratio between the\noptimized and data collection policy. In this work, we show that DICE-based\nmethods can be viewed as a transformation from the behavior distribution to the\noptimal policy distribution. Based on this, we propose a novel approach,\nDiffusion-DICE, that directly performs this transformation using diffusion\nmodels. We find that the optimal policy's score function can be decomposed into\ntwo terms: the behavior policy's score function and the gradient of a guidance\nterm which depends on the optimal distribution ratio. The first term can be\nobtained from a diffusion model trained on the dataset and we propose an\nin-sample learning objective to learn the second term. Due to the\nmulti-modality contained in the optimal policy distribution, the transformation\nin Diffusion-DICE may guide towards those local-optimal modes. We thus generate\na few candidate actions and carefully select from them to approach\nglobal-optimum. Different from all other diffusion-based offline RL methods,\nthe guide-then-select paradigm in Diffusion-DICE only uses in-sample actions\nfor training and brings minimal error exploitation in the value function. We\nuse a didatic toycase example to show how previous diffusion-based methods fail\nto generate optimal actions due to leveraging these errors and how\nDiffusion-DICE successfully avoids that. We then conduct extensive experiments\non benchmark datasets to show the strong performance of Diffusion-DICE. Project\npage at https://ryanxhr.github.io/Diffusion-DICE/.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "NeurIPS 2024, first two authors contribute equally",
    "pdf_url": "http://arxiv.org/pdf/2407.20109v2",
    "published_date": "2024-07-29 15:36:42 UTC",
    "updated_date": "2024-10-31 06:16:24 UTC"
  },
  {
    "arxiv_id": "2407.20108v1",
    "title": "Classification, Regression and Segmentation directly from k-Space in Cardiac MRI",
    "authors": [
      "Ruochen Li",
      "Jiazhen Pan",
      "Youxiang Zhu",
      "Juncheng Ni",
      "Daniel Rueckert"
    ],
    "abstract": "Cardiac Magnetic Resonance Imaging (CMR) is the gold standard for diagnosing\ncardiovascular diseases. Clinical diagnoses predominantly rely on\nmagnitude-only Digital Imaging and Communications in Medicine (DICOM) images,\nomitting crucial phase information that might provide additional diagnostic\nbenefits. In contrast, k-space is complex-valued and encompasses both magnitude\nand phase information, while humans cannot directly perceive. In this work, we\npropose KMAE, a Transformer-based model specifically designed to process\nk-space data directly, eliminating conventional intermediary conversion steps\nto the image domain. KMAE can handle critical cardiac disease classification,\nrelevant phenotype regression, and cardiac morphology segmentation tasks. We\nutilize this model to investigate the potential of k-space-based diagnosis in\ncardiac MRI. Notably, this model achieves competitive classification and\nregression performance compared to image-domain methods e.g. Masked\nAutoencoders (MAEs) and delivers satisfactory segmentation performance with a\nmyocardium dice score of 0.884. Last but not least, our model exhibits robust\nperformance with consistent results even when the k-space is 8* undersampled.\nWe encourage the MR community to explore the untapped potential of k-space and\npursue end-to-end, automated diagnosis with reduced human intervention.",
    "categories": [
      "eess.IV",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "eess.IV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.20108v1",
    "published_date": "2024-07-29 15:35:35 UTC",
    "updated_date": "2024-07-29 15:35:35 UTC"
  },
  {
    "arxiv_id": "2407.20100v3",
    "title": "F-KANs: Federated Kolmogorov-Arnold Networks",
    "authors": [
      "Engin Zeydan",
      "Cristian J. Vaca-Rubio",
      "Luis Blanco",
      "Roberto Pereira",
      "Marius Caus",
      "Abdullah Aydeger"
    ],
    "abstract": "In this paper, we present an innovative federated learning (FL) approach that\nutilizes Kolmogorov-Arnold Networks (KANs) for classification tasks. By\nutilizing the adaptive activation capabilities of KANs in a federated\nframework, we aim to improve classification capabilities while preserving\nprivacy. The study evaluates the performance of federated KANs (F- KANs)\ncompared to traditional Multi-Layer Perceptrons (MLPs) on classification task.\nThe results show that the F-KANs model significantly outperforms the federated\nMLP model in terms of accuracy, precision, recall, F1 score and stability, and\nachieves better performance, paving the way for more efficient and\nprivacy-preserving predictive analytics.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CR",
      "cs.NI"
    ],
    "primary_category": "cs.LG",
    "comment": "This work has been accepted to 1st International Workshop on\n  Distributed AI for Enhanced Wireless Networks (DAINET'25) in conjunction with\n  IEEE Consumer Communications & Networking Conference 2025. Related Code:\n  https://github.com/ezeydan/F-KANs.git",
    "pdf_url": "http://arxiv.org/pdf/2407.20100v3",
    "published_date": "2024-07-29 15:28:26 UTC",
    "updated_date": "2024-11-08 19:02:09 UTC"
  },
  {
    "arxiv_id": "2407.20068v1",
    "title": "Unleash the Power of Ellipsis: Accuracy-enhanced Sparse Vector Technique with Exponential Noise",
    "authors": [
      "Yuhan Liu",
      "Sheng Wang",
      "Yixuan Liu",
      "Feifei Li",
      "Hong Chen"
    ],
    "abstract": "The Sparse Vector Technique (SVT) is one of the most fundamental tools in\ndifferential privacy (DP). It works as a backbone for adaptive data analysis by\nanswering a sequence of queries on a given dataset, and gleaning useful\ninformation in a privacy-preserving manner. Unlike the typical private query\nreleases that directly publicize the noisy query results, SVT is less\ninformative -- it keeps the noisy query results to itself and only reveals a\nbinary bit for each query, indicating whether the query result surpasses a\npredefined threshold. To provide a rigorous DP guarantee for SVT, prior works\nin the literature adopt a conservative privacy analysis by assuming the direct\ndisclosure of noisy query results as in typical private query releases. This\napproach, however, hinders SVT from achieving higher query accuracy due to an\noverestimation of the privacy risks, which further leads to an excessive noise\ninjection using the Laplacian or Gaussian noise for perturbation. Motivated by\nthis, we provide a new privacy analysis for SVT by considering its less\ninformative nature. Our analysis results not only broaden the range of\napplicable noise types for perturbation in SVT, but also identify the\nexponential noise as optimal among all evaluated noises (which, however, is\nusually deemed non-applicable in prior works). The main challenge in applying\nexponential noise to SVT is mitigating the sub-optimal performance due to the\nbias introduced by noise distributions. To address this, we develop a\nutility-oriented optimal threshold correction method and an appending strategy,\nwhich enhances the performance of SVT by increasing the precision and recall,\nrespectively. The effectiveness of our proposed methods is substantiated both\ntheoretically and empirically, demonstrating significant improvements up to\n$50\\%$ across evaluated metrics.",
    "categories": [
      "cs.CR",
      "cs.AI"
    ],
    "primary_category": "cs.CR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.20068v1",
    "published_date": "2024-07-29 14:54:28 UTC",
    "updated_date": "2024-07-29 14:54:28 UTC"
  },
  {
    "arxiv_id": "2407.20067v2",
    "title": "xAI-Drop: Don't Use What You Cannot Explain",
    "authors": [
      "Vincenzo Marco De Luca",
      "Antonio Longa",
      "Andrea Passerini",
      "Pietro Liò"
    ],
    "abstract": "Graph Neural Networks (GNNs) have emerged as the predominant paradigm for\nlearning from graph-structured data, offering a wide range of applications from\nsocial network analysis to bioinformatics. Despite their versatility, GNNs face\nchallenges such as lack of generalization and poor interpretability, which\nhinder their wider adoption and reliability in critical applications. Dropping\nhas emerged as an effective paradigm for improving the generalization\ncapabilities of GNNs. However, existing approaches often rely on random or\nheuristic-based selection criteria, lacking a principled method to identify and\nexclude nodes that contribute to noise and over-complexity in the model. In\nthis work, we argue that explainability should be a key indicator of a model's\nquality throughout its training phase. To this end, we introduce xAI-Drop, a\nnovel topological-level dropping regularizer that leverages explainability to\npinpoint noisy network elements to be excluded from the GNN propagation\nmechanism. An empirical evaluation on diverse real-world datasets demonstrates\nthat our method outperforms current state-of-the-art dropping approaches in\naccuracy, and improves explanation quality.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.20067v2",
    "published_date": "2024-07-29 14:53:45 UTC",
    "updated_date": "2024-11-08 17:49:46 UTC"
  },
  {
    "arxiv_id": "2407.20062v1",
    "title": "SalNAS: Efficient Saliency-prediction Neural Architecture Search with self-knowledge distillation",
    "authors": [
      "Chakkrit Termritthikun",
      "Ayaz Umer",
      "Suwichaya Suwanwimolkul",
      "Feng Xia",
      "Ivan Lee"
    ],
    "abstract": "Recent advancements in deep convolutional neural networks have significantly\nimproved the performance of saliency prediction. However, the manual\nconfiguration of the neural network architectures requires domain knowledge\nexpertise and can still be time-consuming and error-prone. To solve this, we\npropose a new Neural Architecture Search (NAS) framework for saliency\nprediction with two contributions. Firstly, a supernet for saliency prediction\nis built with a weight-sharing network containing all candidate architectures,\nby integrating a dynamic convolution into the encoder-decoder in the supernet,\ntermed SalNAS. Secondly, despite the fact that SalNAS is highly efficient\n(20.98 million parameters), it can suffer from the lack of generalization. To\nsolve this, we propose a self-knowledge distillation approach, termed Self-KD,\nthat trains the student SalNAS with the weighted average information between\nthe ground truth and the prediction from the teacher model. The teacher model,\nwhile sharing the same architecture, contains the best-performing weights\nchosen by cross-validation. Self-KD can generalize well without the need to\ncompute the gradient in the teacher model, enabling an efficient training\nsystem. By utilizing Self-KD, SalNAS outperforms other state-of-the-art\nsaliency prediction models in most evaluation rubrics across seven benchmark\ndatasets while being a lightweight model. The code will be available at\nhttps://github.com/chakkritte/SalNAS",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "Published in Engineering Applications of Artificial Intelligence",
    "pdf_url": "http://arxiv.org/pdf/2407.20062v1",
    "published_date": "2024-07-29 14:48:34 UTC",
    "updated_date": "2024-07-29 14:48:34 UTC"
  },
  {
    "arxiv_id": "2407.20060v1",
    "title": "RelBench: A Benchmark for Deep Learning on Relational Databases",
    "authors": [
      "Joshua Robinson",
      "Rishabh Ranjan",
      "Weihua Hu",
      "Kexin Huang",
      "Jiaqi Han",
      "Alejandro Dobles",
      "Matthias Fey",
      "Jan E. Lenssen",
      "Yiwen Yuan",
      "Zecheng Zhang",
      "Xinwei He",
      "Jure Leskovec"
    ],
    "abstract": "We present RelBench, a public benchmark for solving predictive tasks over\nrelational databases with graph neural networks. RelBench provides databases\nand tasks spanning diverse domains and scales, and is intended to be a\nfoundational infrastructure for future research. We use RelBench to conduct the\nfirst comprehensive study of Relational Deep Learning (RDL) (Fey et al., 2024),\nwhich combines graph neural network predictive models with (deep) tabular\nmodels that extract initial entity-level representations from raw tables.\nEnd-to-end learned RDL models fully exploit the predictive signal encoded in\nprimary-foreign key links, marking a significant shift away from the dominant\nparadigm of manual feature engineering combined with tabular models. To\nthoroughly evaluate RDL against this prior gold-standard, we conduct an\nin-depth user study where an experienced data scientist manually engineers\nfeatures for each task. In this study, RDL learns better models whilst reducing\nhuman work needed by more than an order of magnitude. This demonstrates the\npower of deep learning for solving predictive tasks over relational databases,\nopening up many new research opportunities enabled by RelBench.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.DB"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.20060v1",
    "published_date": "2024-07-29 14:46:13 UTC",
    "updated_date": "2024-07-29 14:46:13 UTC"
  },
  {
    "arxiv_id": "2407.20058v2",
    "title": "Shapley Value Computation in Ontology-Mediated Query Answering",
    "authors": [
      "Meghyn Bienvenu",
      "Diego Figueira",
      "Pierre Lafourcade"
    ],
    "abstract": "The Shapley value, originally introduced in cooperative game theory for\nwealth distribution, has found use in KR and databases for the purpose of\nassigning scores to formulas and database tuples based upon their contribution\nto obtaining a query result or inconsistency. In the present paper, we explore\nthe use of Shapley values in ontology-mediated query answering (OMQA) and\npresent a detailed complexity analysis of Shapley value computation (SVC) in\nthe OMQA setting. In particular, we establish a PF/#P-hard dichotomy for SVC\nfor ontology-mediated queries (T,q) composed of an ontology T formulated in the\ndescription logic ELHI_\\bot and a connected constant-free homomorphism-closed\nquery q. We further show that the #P-hardness side of the dichotomy can be\nstrengthened to cover possibly disconnected queries with constants. Our results\nexploit recently discovered connections between SVC and probabilistic query\nevaluation and allow us to generalize existing results on probabilistic OMQA.",
    "categories": [
      "cs.AI",
      "cs.DB"
    ],
    "primary_category": "cs.AI",
    "comment": "Long version of KR 2024 homonymous paper",
    "pdf_url": "http://arxiv.org/pdf/2407.20058v2",
    "published_date": "2024-07-29 14:45:14 UTC",
    "updated_date": "2024-11-25 10:04:55 UTC"
  },
  {
    "arxiv_id": "2408.05110v1",
    "title": "Application of Unsupervised Artificial Neural Network (ANN) Self_Organizing Map (SOM) in Identifying Main Car Sales Factors",
    "authors": [
      "Mazyar Taghavi"
    ],
    "abstract": "Factors which attract customers and persuade them to buy new car are various\nregarding different consumer tastes. There are some methods to extract pattern\nform mass data. In this case we firstly asked passenger car marketing experts\nto rank more important factors which affect customer decision making behavior\nusing fuzzy Delphi technique, then we provided a sample set from questionnaires\nand tried to apply a useful artificial neural network method called\nself_organizing map SOM to find out which factors have more effect on Iranian\ncustomer's buying decision making. Fuzzy tools were applied to adjust the study\nto be more real. MATLAB software was used for developing and training network.\nResults report four factors are more important rather than the others. Results\nare rather different from marketing expert rankings. Such results would help\nmanufacturers to focus on more important factors and increase company sales\nlevel.",
    "categories": [
      "cs.AI",
      "cs.LG",
      "cs.NE"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2408.05110v1",
    "published_date": "2024-07-29 14:24:16 UTC",
    "updated_date": "2024-07-29 14:24:16 UTC"
  },
  {
    "arxiv_id": "2407.20021v4",
    "title": "MimiQ: Low-Bit Data-Free Quantization of Vision Transformers with Encouraging Inter-Head Attention Similarity",
    "authors": [
      "Kanghyun Choi",
      "Hye Yoon Lee",
      "Dain Kwon",
      "SunJong Park",
      "Kyuyeun Kim",
      "Noseong Park",
      "Jonghyun Choi",
      "Jinho Lee"
    ],
    "abstract": "Data-free quantization (DFQ) is a technique that creates a lightweight\nnetwork from its full-precision counterpart without the original training data,\noften through a synthetic dataset. Although several DFQ methods have been\nproposed for vision transformer (ViT) architectures, they fail to achieve\nefficacy in low-bit settings. Examining the existing methods, we observe that\ntheir synthetic data produce misaligned attention maps, while those of the real\nsamples are highly aligned. From this observation, we find that aligning\nattention maps of synthetic data helps improve the overall performance of\nquantized ViTs. Motivated by this finding, we devise MimiQ, a novel DFQ method\ndesigned for ViTs that enhances inter-head attention similarity. First, we\ngenerate synthetic data by aligning head-wise attention outputs from each\nspatial query patch. Then, we align the attention maps of the quantized network\nto those of the full-precision teacher by applying head-wise structural\nattention distillation. The experimental results show that the proposed method\nsignificantly outperforms baselines, setting a new state-of-the-art for\nViT-DFQ. This paper is an extended version of our work published in the\nproceedings of AAAI 2025, including additional supplementary material.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.LG",
    "comment": "Published to AAAI 2025",
    "pdf_url": "http://arxiv.org/pdf/2407.20021v4",
    "published_date": "2024-07-29 13:57:40 UTC",
    "updated_date": "2025-04-14 07:42:52 UTC"
  },
  {
    "arxiv_id": "2407.19998v1",
    "title": "Do LLMs Really Adapt to Domains? An Ontology Learning Perspective",
    "authors": [
      "Huu Tan Mai",
      "Cuong Xuan Chu",
      "Heiko Paulheim"
    ],
    "abstract": "Large Language Models (LLMs) have demonstrated unprecedented prowess across\nvarious natural language processing tasks in various application domains.\nRecent studies show that LLMs can be leveraged to perform lexical semantic\ntasks, such as Knowledge Base Completion (KBC) or Ontology Learning (OL).\nHowever, it has not effectively been verified whether their success is due to\ntheir ability to reason over unstructured or semi-structured data, or their\neffective learning of linguistic patterns and senses alone. This unresolved\nquestion is particularly crucial when dealing with domain-specific data, where\nthe lexical senses and their meaning can completely differ from what a LLM has\nlearned during its training stage. This paper investigates the following\nquestion: Do LLMs really adapt to domains and remain consistent in the\nextraction of structured knowledge, or do they only learn lexical senses\ninstead of reasoning? To answer this question and, we devise a controlled\nexperiment setup that uses WordNet to synthesize parallel corpora, with English\nand gibberish terms. We examine the differences in the outputs of LLMs for each\ncorpus in two OL tasks: relation extraction and taxonomy discovery. Empirical\nresults show that, while adapting to the gibberish corpora, off-the-shelf LLMs\ndo not consistently reason over semantic relationships between concepts, and\ninstead leverage senses and their frame. However, fine-tuning improves the\nperformance of LLMs on lexical semantic tasks even when the domain-specific\nterms are arbitrary and unseen during pre-training, hinting at the\napplicability of pre-trained LLMs for OL.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted at ISWC 2024",
    "pdf_url": "http://arxiv.org/pdf/2407.19998v1",
    "published_date": "2024-07-29 13:29:43 UTC",
    "updated_date": "2024-07-29 13:29:43 UTC"
  },
  {
    "arxiv_id": "2407.19996v1",
    "title": "Reproducibility Study of \"ITI-GEN: Inclusive Text-to-Image Generation\"",
    "authors": [
      "Daniel Gallo Fernández",
      "Răzvan-Andrei Matisan",
      "Alejandro Monroy Muñoz",
      "Janusz Partyka"
    ],
    "abstract": "Text-to-image generative models often present issues regarding fairness with\nrespect to certain sensitive attributes, such as gender or skin tone. This\nstudy aims to reproduce the results presented in \"ITI-GEN: Inclusive\nText-to-Image Generation\" by Zhang et al. (2023a), which introduces a model to\nimprove inclusiveness in these kinds of models. We show that most of the claims\nmade by the authors about ITI-GEN hold: it improves the diversity and quality\nof generated images, it is scalable to different domains, it has plug-and-play\ncapabilities, and it is efficient from a computational point of view. However,\nITI-GEN sometimes uses undesired attributes as proxy features and it is unable\nto disentangle some pairs of (correlated) attributes such as gender and\nbaldness. In addition, when the number of considered attributes increases, the\ntraining time grows exponentially and ITI-GEN struggles to generate inclusive\nimages for all elements in the joint distribution. To solve these issues, we\npropose using Hard Prompt Search with negative prompting, a method that does\nnot require training and that handles negation better than vanilla Hard Prompt\nSearch. Nonetheless, Hard Prompt Search (with or without negative prompting)\ncannot be used for continuous attributes that are hard to express in natural\nlanguage, an area where ITI-GEN excels as it is guided by images during\ntraining. Finally, we propose combining ITI-GEN and Hard Prompt Search with\nnegative prompting.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "Accepted to TMLR, see https://openreview.net/forum?id=d3Vj360Wi2",
    "pdf_url": "http://arxiv.org/pdf/2407.19996v1",
    "published_date": "2024-07-29 13:27:44 UTC",
    "updated_date": "2024-07-29 13:27:44 UTC"
  },
  {
    "arxiv_id": "2407.19994v3",
    "title": "A Study on the Implementation Method of an Agent-Based Advanced RAG System Using Graph",
    "authors": [
      "Cheonsu Jeong"
    ],
    "abstract": "This study aims to improve knowledge-based question-answering (QA) systems by\novercoming the limitations of existing Retrieval-Augmented Generation (RAG)\nmodels and implementing an advanced RAG system based on Graph technology to\ndevelop high-quality generative AI services. While existing RAG models\ndemonstrate high accuracy and fluency by utilizing retrieved information, they\nmay suffer from accuracy degradation as they generate responses using\npre-loaded knowledge without reprocessing. Additionally, they cannot\nincorporate real-time data after the RAG configuration stage, leading to issues\nwith contextual understanding and biased information. To address these\nlimitations, this study implemented an enhanced RAG system utilizing Graph\ntechnology. This system is designed to efficiently search and utilize\ninformation. Specifically, it employs LangGraph to evaluate the reliability of\nretrieved information and synthesizes diverse data to generate more accurate\nand enhanced responses. Furthermore, the study provides a detailed explanation\nof the system's operation, key implementation steps, and examples through\nimplementation code and validation results, thereby enhancing the understanding\nof advanced RAG technology. This approach offers practical guidelines for\nimplementing advanced RAG systems in corporate services, making it a valuable\nresource for practical application.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.19994v3",
    "published_date": "2024-07-29 13:26:43 UTC",
    "updated_date": "2024-09-13 12:19:26 UTC"
  },
  {
    "arxiv_id": "2407.19985v2",
    "title": "Mixture of Nested Experts: Adaptive Processing of Visual Tokens",
    "authors": [
      "Gagan Jain",
      "Nidhi Hegde",
      "Aditya Kusupati",
      "Arsha Nagrani",
      "Shyamal Buch",
      "Prateek Jain",
      "Anurag Arnab",
      "Sujoy Paul"
    ],
    "abstract": "The visual medium (images and videos) naturally contains a large amount of\ninformation redundancy, thereby providing a great opportunity for leveraging\nefficiency in processing. While Vision Transformer (ViT) based models scale\neffectively to large data regimes, they fail to capitalize on this inherent\nredundancy, leading to higher computational costs. Mixture of Experts (MoE)\nnetworks demonstrate scalability while maintaining same inference-time costs,\nbut they come with a larger parameter footprint. We present Mixture of Nested\nExperts (MoNE), which utilizes a nested structure for experts, wherein\nindividual experts fall on an increasing compute-accuracy curve. Given a\ncompute budget, MoNE learns to dynamically choose tokens in a priority order,\nand thus redundant tokens are processed through cheaper nested experts. Using\nthis framework, we achieve equivalent performance as the baseline models, while\nreducing inference time compute by over two-fold. We validate our approach on\nstandard image and video datasets - ImageNet-21K, Kinetics400, and\nSomething-Something-v2. We further highlight MoNE$'$s adaptability by\nshowcasing its ability to maintain strong performance across different\ninference-time compute budgets on videos, using only a single trained model.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.19985v2",
    "published_date": "2024-07-29 13:19:31 UTC",
    "updated_date": "2024-07-30 17:26:22 UTC"
  },
  {
    "arxiv_id": "2407.19965v2",
    "title": "Simply Trainable Nearest Neighbour Machine Translation with GPU Inference",
    "authors": [
      "Hossam Amer",
      "Abdelrahman Abouelenin",
      "Mohamed Maher",
      "Evram Narouz",
      "Mohamed Afify",
      "Hany Awadallah"
    ],
    "abstract": "Nearest neighbor machine translation is a successful approach for fast domain\nadaption, which interpolates the pre-trained transformers with domain-specific\ntoken-level k-nearest-neighbor (kNN) retrieval without retraining. Despite kNN\nMT's success, searching large reference corpus and fixed interpolation between\nthe kNN and pre-trained model led to computational complexity and translation\nquality challenges. Among other papers, Dai et al. proposed methods to obtain a\nsmall number of reference samples dynamically for which they introduced a\ndistance-aware interpolation method using an equation that includes free\nparameters. This paper proposes a simply trainable nearest neighbor machine\ntranslation and carry out inference experiments on GPU. Similar to Dai et al.,\nwe first adaptively construct a small datastore for each input sentence.\nSecond, we train a single-layer network for the interpolation coefficient\nbetween the knnMT and pre-trained result to automatically interpolate in\ndifferent domains. Experimental results on different domains show that our\nproposed method either improves or sometimes maintain the translation quality\nof methods in Dai et al. while being automatic. In addition, our GPU inference\nresults demonstrate that knnMT can be integrated into GPUs with a drop of only\n5% in terms of speed.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "6 pages",
    "pdf_url": "http://arxiv.org/pdf/2407.19965v2",
    "published_date": "2024-07-29 12:55:40 UTC",
    "updated_date": "2024-08-19 08:48:59 UTC"
  },
  {
    "arxiv_id": "2407.19951v1",
    "title": "Can I trust my anomaly detection system? A case study based on explainable AI",
    "authors": [
      "Muhammad Rashid",
      "Elvio Amparore",
      "Enrico Ferrari",
      "Damiano Verda"
    ],
    "abstract": "Generative models based on variational autoencoders are a popular technique\nfor detecting anomalies in images in a semi-supervised context. A common\napproach employs the anomaly score to detect the presence of anomalies, and it\nis known to reach high level of accuracy on benchmark datasets. However, since\nanomaly scores are computed from reconstruction disparities, they often obscure\nthe detection of various spurious features, raising concerns regarding their\nactual efficacy. This case study explores the robustness of an anomaly\ndetection system based on variational autoencoder generative models through the\nuse of eXplainable AI methods. The goal is to get a different perspective on\nthe real performances of anomaly detectors that use reconstruction differences.\nIn our case study we discovered that, in many cases, samples are detected as\nanomalous for the wrong or misleading factors.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "World Conference on eXplainable Artificial Intelligence",
    "pdf_url": "http://arxiv.org/pdf/2407.19951v1",
    "published_date": "2024-07-29 12:39:07 UTC",
    "updated_date": "2024-07-29 12:39:07 UTC"
  },
  {
    "arxiv_id": "2407.19944v1",
    "title": "Noise-Resilient Unsupervised Graph Representation Learning via Multi-Hop Feature Quality Estimation",
    "authors": [
      "Shiyuan Li",
      "Yixin Liu",
      "Qingfeng Chen",
      "Geoffrey I. Webb",
      "Shirui Pan"
    ],
    "abstract": "Unsupervised graph representation learning (UGRL) based on graph neural\nnetworks (GNNs), has received increasing attention owing to its efficacy in\nhandling graph-structured data. However, existing UGRL methods ideally assume\nthat the node features are noise-free, which makes them fail to distinguish\nbetween useful information and noise when applied to real data with noisy\nfeatures, thus affecting the quality of learned representations. This urges us\nto take node noisy features into account in real-world UGRL. With empirical\nanalysis, we reveal that feature propagation, the essential operation in GNNs,\nacts as a \"double-edged sword\" in handling noisy features - it can both denoise\nand diffuse noise, leading to varying feature quality across nodes, even within\nthe same node at different hops. Building on this insight, we propose a novel\nUGRL method based on Multi-hop feature Quality Estimation (MQE for short).\nUnlike most UGRL models that directly utilize propagation-based GNNs to\ngenerate representations, our approach aims to learn representations through\nestimating the quality of propagated features at different hops. Specifically,\nwe introduce a Gaussian model that utilizes a learnable \"meta-representation\"\nas a condition to estimate the expectation and variance of multi-hop propagated\nfeatures via neural networks. In this way, the \"meta representation\" captures\nthe semantic and structural information underlying multiple propagated features\nbut is naturally less susceptible to interference by noise, thereby serving as\nhigh-quality node representations beneficial for downstream tasks. Extensive\nexperiments on multiple real-world datasets demonstrate that MQE in learning\nreliable node representations in scenarios with diverse types of feature noise.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted by CIKM 2024. 11 pages, 8 figures",
    "pdf_url": "http://arxiv.org/pdf/2407.19944v1",
    "published_date": "2024-07-29 12:24:28 UTC",
    "updated_date": "2024-07-29 12:24:28 UTC"
  },
  {
    "arxiv_id": "2407.19938v1",
    "title": "Robust Conformal Volume Estimation in 3D Medical Images",
    "authors": [
      "Benjamin Lambert",
      "Florence Forbes",
      "Senan Doyle",
      "Michel Dojat"
    ],
    "abstract": "Volumetry is one of the principal downstream applications of 3D medical image\nsegmentation, for example, to detect abnormal tissue growth or for surgery\nplanning. Conformal Prediction is a promising framework for uncertainty\nquantification, providing calibrated predictive intervals associated with\nautomatic volume measurements. However, this methodology is based on the\nhypothesis that calibration and test samples are exchangeable, an assumption\nthat is in practice often violated in medical image applications. A weighted\nformulation of Conformal Prediction can be framed to mitigate this issue, but\nits empirical investigation in the medical domain is still lacking. A potential\nreason is that it relies on the estimation of the density ratio between the\ncalibration and test distributions, which is likely to be intractable in\nscenarios involving high-dimensional data. To circumvent this, we propose an\nefficient approach for density ratio estimation relying on the compressed\nlatent representations generated by the segmentation model. Our experiments\ndemonstrate the efficiency of our approach to reduce the coverage error in the\npresence of covariate shifts, in both synthetic and real-world settings. Our\nimplementation is available at https://github.com/benolmbrt/wcp_miccai",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "Early accepted at MICCAI 2024",
    "pdf_url": "http://arxiv.org/pdf/2407.19938v1",
    "published_date": "2024-07-29 12:18:07 UTC",
    "updated_date": "2024-07-29 12:18:07 UTC"
  },
  {
    "arxiv_id": "2407.19937v2",
    "title": "AOTree: Aspect Order Tree-based Model for Explainable Recommendation",
    "authors": [
      "Wenxin Zhao",
      "Peng Zhang",
      "Hansu Gu",
      "Dongsheng Li",
      "Tun Lu",
      "Ning Gu"
    ],
    "abstract": "Recent recommender systems aim to provide not only accurate recommendations\nbut also explanations that help users understand them better. However, most\nexisting explainable recommendations only consider the importance of content in\nreviews, such as words or aspects, and ignore the ordering relationship among\nthem. This oversight neglects crucial ordering dimensions in the human\ndecision-making process, leading to suboptimal performance. Therefore, in this\npaper, we propose Aspect Order Tree-based (AOTree) explainable recommendation\nmethod, inspired by the Order Effects Theory from cognitive and decision\npsychology, in order to capture the dependency relationships among decisive\nfactors. We first validate the theory in the recommendation scenario by\nanalyzing the reviews of the users. Then, according to the theory, the proposed\nAOTree expands the construction of the decision tree to capture aspect orders\nin users' decision-making processes, and use attention mechanisms to make\npredictions based on the aspect orders. Extensive experiments demonstrate our\nmethod's effectiveness on rating predictions, and our approach aligns more\nconsistently with the user' s decision-making process by displaying\nexplanations in a particular order, thereby enhancing interpretability.",
    "categories": [
      "cs.IR",
      "cs.AI"
    ],
    "primary_category": "cs.IR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.19937v2",
    "published_date": "2024-07-29 12:17:48 UTC",
    "updated_date": "2024-08-03 05:40:20 UTC"
  },
  {
    "arxiv_id": "2407.19922v1",
    "title": "Monetizing Currency Pair Sentiments through LLM Explainability",
    "authors": [
      "Lior Limonad",
      "Fabiana Fournier",
      "Juan Manuel Vera Díaz",
      "Inna Skarbovsky",
      "Shlomit Gur",
      "Raquel Lazcano"
    ],
    "abstract": "Large language models (LLMs) play a vital role in almost every domain in\ntoday's organizations. In the context of this work, we highlight the use of\nLLMs for sentiment analysis (SA) and explainability. Specifically, we\ncontribute a novel technique to leverage LLMs as a post-hoc model-independent\ntool for the explainability of SA. We applied our technique in the financial\ndomain for currency-pair price predictions using open news feed data merged\nwith market prices. Our application shows that the developed technique is not\nonly a viable alternative to using conventional eXplainable AI but can also be\nfed back to enrich the input to the machine learning (ML) model to better\npredict future currency-pair values. We envision our results could be\ngeneralized to employing explainability as a conventional enrichment for ML\ninput for better ML predictions in general.",
    "categories": [
      "cs.AI",
      "68T50"
    ],
    "primary_category": "cs.AI",
    "comment": "7 pages, 3 figures, AIFin@ECAI 2024",
    "pdf_url": "http://arxiv.org/pdf/2407.19922v1",
    "published_date": "2024-07-29 11:58:54 UTC",
    "updated_date": "2024-07-29 11:58:54 UTC"
  },
  {
    "arxiv_id": "2407.19911v4",
    "title": "Efficient Shield Synthesis via State-Space Transformation",
    "authors": [
      "Asger Horn Brorholt",
      "Andreas Holck Høeg-Petersen",
      "Kim Guldstrand Larsen",
      "Christian Schilling"
    ],
    "abstract": "We consider the problem of synthesizing safety strategies for control\nsystems, also known as shields. Since the state space is infinite, shields are\ntypically computed over a finite-state abstraction, with the most common\nabstraction being a rectangular grid. However, for many systems, such a grid\ndoes not align well with the safety property or the system dynamics. That is\nwhy a coarse grid is rarely sufficient, but a fine grid is typically\ncomputationally infeasible to obtain. In this paper, we show that appropriate\nstate-space transformations can still allow to use a coarse grid at almost no\ncomputational overhead. We demonstrate in three case studies that our\ntransformation-based synthesis outperforms a standard synthesis by several\norders of magnitude. In the first two case studies, we use domain knowledge to\nselect a suitable transformation. In the third case study, we instead report on\nresults in engineering a transformation without domain knowledge.",
    "categories": [
      "cs.LO",
      "cs.AI",
      "cs.LG",
      "cs.SY",
      "eess.SY"
    ],
    "primary_category": "cs.LO",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.19911v4",
    "published_date": "2024-07-29 11:39:22 UTC",
    "updated_date": "2024-10-07 07:41:11 UTC"
  },
  {
    "arxiv_id": "2407.19906v2",
    "title": "Reverse Map Projections as Equivariant Quantum Embeddings",
    "authors": [
      "Max Arnott",
      "Dimitri Papaioannou",
      "Kieran McDowall",
      "Phalgun Lolur",
      "Bambordé Baldé"
    ],
    "abstract": "We introduce the novel class $(E_\\alpha)_{\\alpha \\in [-\\infty,1)}$ of reverse\nmap projection embeddings, each one defining a unique new method of encoding\nclassical data into quantum states. Inspired by well-known map projections from\nthe unit sphere onto its tangent planes, used in practice in cartography, these\nembeddings address the common drawback of the amplitude embedding method,\nwherein scalar multiples of data points are identified and information about\nthe norm of data is lost.\n  We show how reverse map projections can be utilised as equivariant embeddings\nfor quantum machine learning. Using these methods, we can leverage symmetries\nin classical datasets to significantly strengthen performance on quantum\nmachine learning tasks.\n  Finally, we select four values of $\\alpha$ with which to perform a simple\nclassification task, taking $E_\\alpha$ as the embedding and experimenting with\nboth equivariant and non-equivariant setups. We compare their results alongside\nthose of standard amplitude embedding.",
    "categories": [
      "quant-ph",
      "cs.AI",
      "cs.ET",
      "math-ph",
      "math.MP"
    ],
    "primary_category": "quant-ph",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.19906v2",
    "published_date": "2024-07-29 11:31:24 UTC",
    "updated_date": "2024-08-19 09:40:32 UTC"
  },
  {
    "arxiv_id": "2407.19900v1",
    "title": "Practical and Reproducible Symbolic Music Generation by Large Language Models with Structural Embeddings",
    "authors": [
      "Seungyeon Rhyu",
      "Kichang Yang",
      "Sungjun Cho",
      "Jaehyeon Kim",
      "Kyogu Lee",
      "Moontae Lee"
    ],
    "abstract": "Music generation introduces challenging complexities to large language\nmodels. Symbolic structures of music often include vertical harmonization as\nwell as horizontal counterpoint, urging various adaptations and enhancements\nfor large-scale Transformers. However, existing works share three major\ndrawbacks: 1) their tokenization requires domain-specific annotations, such as\nbars and beats, that are typically missing in raw MIDI data; 2) the pure impact\nof enhancing token embedding methods is hardly examined without domain-specific\nannotations; and 3) existing works to overcome the aforementioned drawbacks,\nsuch as MuseNet, lack reproducibility. To tackle such limitations, we develop a\nMIDI-based music generation framework inspired by MuseNet, empirically studying\ntwo structural embeddings that do not rely on domain-specific annotations. We\nprovide various metrics and insights that can guide suitable encoding to\ndeploy. We also verify that multiple embedding configurations can selectively\nboost certain musical aspects. By providing open-source implementations via\nHuggingFace, our findings shed light on leveraging large language models toward\npractical and reproducible music generation.",
    "categories": [
      "cs.SD",
      "cs.AI",
      "eess.AS"
    ],
    "primary_category": "cs.SD",
    "comment": "9 pages, 6 figures, 4 tables",
    "pdf_url": "http://arxiv.org/pdf/2407.19900v1",
    "published_date": "2024-07-29 11:24:10 UTC",
    "updated_date": "2024-07-29 11:24:10 UTC"
  },
  {
    "arxiv_id": "2407.19897v1",
    "title": "BEExAI: Benchmark to Evaluate Explainable AI",
    "authors": [
      "Samuel Sithakoul",
      "Sara Meftah",
      "Clément Feutry"
    ],
    "abstract": "Recent research in explainability has given rise to numerous post-hoc\nattribution methods aimed at enhancing our comprehension of the outputs of\nblack-box machine learning models. However, evaluating the quality of\nexplanations lacks a cohesive approach and a consensus on the methodology for\nderiving quantitative metrics that gauge the efficacy of explainability\npost-hoc attribution methods. Furthermore, with the development of increasingly\ncomplex deep learning models for diverse data applications, the need for a\nreliable way of measuring the quality and correctness of explanations is\nbecoming critical. We address this by proposing BEExAI, a benchmark tool that\nallows large-scale comparison of different post-hoc XAI methods, employing a\nset of selected evaluation metrics.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.19897v1",
    "published_date": "2024-07-29 11:21:17 UTC",
    "updated_date": "2024-07-29 11:21:17 UTC"
  },
  {
    "arxiv_id": "2407.19893v1",
    "title": "Leveraging Foundation Models for Zero-Shot IoT Sensing",
    "authors": [
      "Dinghao Xue",
      "Xiaoran Fan",
      "Tao Chen",
      "Guohao Lan",
      "Qun Song"
    ],
    "abstract": "Deep learning models are increasingly deployed on edge Internet of Things\n(IoT) devices. However, these models typically operate under supervised\nconditions and fail to recognize unseen classes different from training. To\naddress this, zero-shot learning (ZSL) aims to classify data of unseen classes\nwith the help of semantic information. Foundation models (FMs) trained on\nweb-scale data have shown impressive ZSL capability in natural language\nprocessing and visual understanding. However, leveraging FMs' generalized\nknowledge for zero-shot IoT sensing using signals such as mmWave, IMU, and\nWi-Fi has not been fully investigated. In this work, we align the IoT data\nembeddings with the semantic embeddings generated by an FM's text encoder for\nzero-shot IoT sensing. To utilize the physics principles governing the\ngeneration of IoT sensor signals to derive more effective prompts for semantic\nembedding extraction, we propose to use cross-attention to combine a learnable\nsoft prompt that is optimized automatically on training data and an auxiliary\nhard prompt that encodes domain knowledge of the IoT sensing task. To address\nthe problem of IoT embeddings biasing to seen classes due to the lack of unseen\nclass data during training, we propose using data augmentation to synthesize\nunseen class IoT data for fine-tuning the IoT feature extractor and embedding\nprojector. We evaluate our approach on multiple IoT sensing tasks. Results show\nthat our approach achieves superior open-set detection and generalized\nzero-shot learning performance compared with various baselines. Our code is\navailable at https://github.com/schrodingho/FM\\_ZSL\\_IoT.",
    "categories": [
      "cs.AI",
      "cs.HC"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.19893v1",
    "published_date": "2024-07-29 11:16:48 UTC",
    "updated_date": "2024-07-29 11:16:48 UTC"
  },
  {
    "arxiv_id": "2407.19888v1",
    "title": "Yucca: A Deep Learning Framework For Medical Image Analysis",
    "authors": [
      "Sebastian Nørgaard Llambias",
      "Julia Machnio",
      "Asbjørn Munk",
      "Jakob Ambsdorf",
      "Mads Nielsen",
      "Mostafa Mehdipour Ghazi"
    ],
    "abstract": "Medical image analysis using deep learning frameworks has advanced healthcare\nby automating complex tasks, but many existing frameworks lack flexibility,\nmodularity, and user-friendliness. To address these challenges, we introduce\nYucca, an open-source AI framework available at\nhttps://github.com/Sllambias/yucca, designed specifically for medical imaging\napplications and built on PyTorch and PyTorch Lightning. Yucca features a\nthree-tiered architecture: Functional, Modules, and Pipeline, providing a\ncomprehensive and customizable solution. Evaluated across diverse tasks such as\ncerebral microbleeds detection, white matter hyperintensity segmentation, and\nhippocampus segmentation, Yucca achieves state-of-the-art results,\ndemonstrating its robustness and versatility. Yucca offers a powerful,\nflexible, and user-friendly platform for medical image analysis, inviting\ncommunity contributions to advance its capabilities and impact.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG",
      "eess.IV"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.19888v1",
    "published_date": "2024-07-29 11:09:10 UTC",
    "updated_date": "2024-07-29 11:09:10 UTC"
  },
  {
    "arxiv_id": "2407.19886v1",
    "title": "A Unified Graph Transformer for Overcoming Isolations in Multi-modal Recommendation",
    "authors": [
      "Zixuan Yi",
      "Iadh Ounis"
    ],
    "abstract": "With the rapid development of online multimedia services, especially in\ne-commerce platforms, there is a pressing need for personalised recommendation\nsystems that can effectively encode the diverse multi-modal content associated\nwith each item. However, we argue that existing multi-modal recommender systems\ntypically use isolated processes for both feature extraction and modality\nmodelling. Such isolated processes can harm the recommendation performance.\nFirstly, an isolated extraction process underestimates the importance of\neffective feature extraction in multi-modal recommendations, potentially\nincorporating non-relevant information, which is harmful to item\nrepresentations. Second, an isolated modality modelling process produces\ndisjointed embeddings for item modalities due to the individual processing of\neach modality, which leads to a suboptimal fusion of user/item representations\nfor effective user preferences prediction. We hypothesise that the use of a\nunified model for addressing both aforementioned isolated processes will enable\nthe consistent extraction and cohesive fusion of joint multi-modal features,\nthereby enhancing the effectiveness of multi-modal recommender systems. In this\npaper, we propose a novel model, called Unified Multi-modal Graph Transformer\n(UGT), which firstly leverages a multi-way transformer to extract aligned\nmulti-modal features from raw data for top-k recommendation. Subsequently, we\nbuild a unified graph neural network in our UGT model to jointly fuse the\nuser/item representations with their corresponding multi-modal features. Using\nthe graph transformer architecture of our UGT model, we show that the UGT model\ncan achieve significant effectiveness gains, especially when jointly optimised\nwith the commonly-used multi-modal recommendation losses.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.19886v1",
    "published_date": "2024-07-29 11:04:31 UTC",
    "updated_date": "2024-07-29 11:04:31 UTC"
  },
  {
    "arxiv_id": "2407.19869v1",
    "title": "Distances Between Partial Preference Orderings",
    "authors": [
      "Jean Dezert",
      "Andrii Shekhovtsov",
      "Wojciech Salabun"
    ],
    "abstract": "This paper proposes to establish the distance between partial preference\norderings based on two very different approaches. The first approach\ncorresponds to the brute force method based on combinatorics. It generates all\npossible complete preference orderings compatible with the partial preference\norderings and calculates the Frobenius distance between all fully compatible\npreference orderings. Unfortunately, this first method is not very efficient in\nsolving high-dimensional problems because of its big combinatorial complexity.\nThat is why we propose to circumvent this problem by using a second approach\nbased on belief functions, which can adequately model the missing information\nof partial preference orderings. This second approach to the calculation of\ndistance does not suffer from combinatorial complexity limitation. We show\nthrough simple examples how these two theoretical methods work.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "10 pages",
    "pdf_url": "http://arxiv.org/pdf/2407.19869v1",
    "published_date": "2024-07-29 10:39:40 UTC",
    "updated_date": "2024-07-29 10:39:40 UTC"
  },
  {
    "arxiv_id": "2407.19865v2",
    "title": "Imitation Learning for Intra-Day Power Grid Operation through Topology Actions",
    "authors": [
      "Matthijs de Jong",
      "Jan Viebahn",
      "Yuliya Shapovalova"
    ],
    "abstract": "Power grid operation is becoming increasingly complex due to the increase in\ngeneration of renewable energy. The recent series of Learning To Run a Power\nNetwork (L2RPN) competitions have encouraged the use of artificial agents to\nassist human dispatchers in operating power grids. In this paper we study the\nperformance of imitation learning for day-ahead power grid operation through\ntopology actions. In particular, we consider two rule-based expert agents: a\ngreedy agent and a N-1 agent. While the latter is more computationally\nexpensive since it takes N-1 safety considerations into account, it exhibits a\nmuch higher operational performance. We train a fully-connected neural network\n(FCNN) on expert state-action pairs and evaluate it in two ways. First, we find\nthat classification accuracy is limited despite extensive hyperparameter\ntuning, due to class imbalance and class overlap. Second, as a power system\nagent, the FCNN performs only slightly worse than expert agents. Furthermore,\nhybrid agents, which incorporate minimal additional simulations, match expert\nagents' performance with significantly lower computational cost. Consequently,\nimitation learning shows promise for developing fast, high-performing power\ngrid agents, motivating its further exploration in future L2RPN studies.",
    "categories": [
      "cs.AI",
      "cs.LG",
      "cs.SY",
      "eess.SY"
    ],
    "primary_category": "cs.AI",
    "comment": "To be presented at the Machine Learning for Sustainable Power Systems\n  2024 workshop and to be published in the corresponding Springer\n  Communications in Computer and Information Science proceedings",
    "pdf_url": "http://arxiv.org/pdf/2407.19865v2",
    "published_date": "2024-07-29 10:34:19 UTC",
    "updated_date": "2024-08-18 09:55:39 UTC"
  },
  {
    "arxiv_id": "2407.19860v1",
    "title": "Anomalous State Sequence Modeling to Enhance Safety in Reinforcement Learning",
    "authors": [
      "Leen Kweider",
      "Maissa Abou Kassem",
      "Ubai Sandouk"
    ],
    "abstract": "The deployment of artificial intelligence (AI) in decision-making\napplications requires ensuring an appropriate level of safety and reliability,\nparticularly in changing environments that contain a large number of unknown\nobservations. To address this challenge, we propose a novel safe reinforcement\nlearning (RL) approach that utilizes an anomalous state sequence to enhance RL\nsafety. Our proposed solution Safe Reinforcement Learning with Anomalous State\nSequences (AnoSeqs) consists of two stages. First, we train an agent in a\nnon-safety-critical offline 'source' environment to collect safe state\nsequences. Next, we use these safe sequences to build an anomaly detection\nmodel that can detect potentially unsafe state sequences in a 'target'\nsafety-critical environment where failures can have high costs. The estimated\nrisk from the anomaly detection model is utilized to train a risk-averse RL\npolicy in the target environment; this involves adjusting the reward function\nto penalize the agent for visiting anomalous states deemed unsafe by our\nanomaly model. In experiments on multiple safety-critical benchmarking\nenvironments including self-driving cars, our solution approach successfully\nlearns safer policies and proves that sequential anomaly detection can provide\nan effective supervisory signal for training safety-aware RL agents",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.19860v1",
    "published_date": "2024-07-29 10:30:07 UTC",
    "updated_date": "2024-07-29 10:30:07 UTC"
  },
  {
    "arxiv_id": "2407.19835v1",
    "title": "ATHAR: A High-Quality and Diverse Dataset for Classical Arabic to English Translation",
    "authors": [
      "Mohammed Khalil",
      "Mohammed Sabry"
    ],
    "abstract": "Classical Arabic represents a significant era, encompassing the golden age of\nArab culture, philosophy, and scientific literature. With a broad consensus on\nthe importance of translating these literatures to enrich knowledge\ndissemination across communities, the advent of large language models (LLMs)\nand translation systems offers promising tools to facilitate this goal.\nHowever, we have identified a scarcity of translation datasets in Classical\nArabic, which are often limited in scope and topics, hindering the development\nof high-quality translation systems. In response, we present the ATHAR dataset,\ncomprising 66,000 high-quality Classical Arabic to English translation samples\nthat cover a wide array of subjects including science, culture, and philosophy.\nFurthermore, we assess the performance of current state-of-the-art LLMs under\nvarious settings, concluding that there is a need for such datasets in current\nsystems. Our findings highlight how models can benefit from fine-tuning or\nincorporating this dataset into their pretraining pipelines. The dataset is\npublicly available on the HuggingFace Data Hub at\n\\url{https://huggingface.co/datasets/mohamed-khalil/ATHAR}.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.19835v1",
    "published_date": "2024-07-29 09:45:34 UTC",
    "updated_date": "2024-07-29 09:45:34 UTC"
  },
  {
    "arxiv_id": "2407.19832v3",
    "title": "ML-Mamba: Efficient Multi-Modal Large Language Model Utilizing Mamba-2",
    "authors": [
      "Wenjun Huang",
      "Jiakai Pan",
      "Jiahao Tang",
      "Yanyu Ding",
      "Yifei Xing",
      "Yuhe Wang",
      "Zhengzhuo Wang",
      "Jianguo Hu"
    ],
    "abstract": "Multimodal Large Language Models (MLLMs) have attracted much attention for\ntheir multifunctionality. However, traditional Transformer architectures incur\nsignificant overhead due to their secondary computational complexity. To\naddress this issue, we introduce ML-Mamba, a multimodal language model, which\nutilizes the latest and efficient Mamba-2 model for inference. Mamba-2 is known\nfor its linear scalability and fast processing of long sequences. We replace\nthe Transformer-based backbone with a pre-trained Mamba-2 model and explore\nmethods for integrating 2D visual selective scanning mechanisms into multimodal\nlearning while also trying various visual encoders and Mamba-2 model variants.\nOur extensive experiments in various multimodal benchmark tests demonstrate the\ncompetitive performance of ML-Mamba and highlight the potential of state space\nmodels in multimodal tasks. The experimental results show that: (1) we\nempirically explore how to effectively apply the 2D vision selective scan\nmechanism for multimodal learning. We propose a novel multimodal connector\ncalled the Mamba-2 Scan Connector (MSC), which enhances representational\ncapabilities. (2) ML-Mamba achieves performance comparable to state-of-the-art\nmethods such as TinyLaVA and MobileVLM v2 through its linear sequential\nmodeling while faster inference speed; (3) Compared to multimodal models\nutilizing Mamba-1, the Mamba-2-based ML-Mamba exhibits superior inference\nperformance and effectiveness.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.19832v3",
    "published_date": "2024-07-29 09:38:15 UTC",
    "updated_date": "2024-08-21 09:52:52 UTC"
  },
  {
    "arxiv_id": "2407.19829v2",
    "title": "Generative Retrieval with Preference Optimization for E-commerce Search",
    "authors": [
      "Mingming Li",
      "Huimu Wang",
      "Zuxu Chen",
      "Guangtao Nie",
      "Yiming Qiu",
      "Guoyu Tang",
      "Lin Liu",
      "Jingwei Zhuo"
    ],
    "abstract": "Generative retrieval introduces a groundbreaking paradigm to document\nretrieval by directly generating the identifier of a pertinent document in\nresponse to a specific query. This paradigm has demonstrated considerable\nbenefits and potential, particularly in representation and generalization\ncapabilities, within the context of large language models. However, it faces\nsignificant challenges in E-commerce search scenarios, including the complexity\nof generating detailed item titles from brief queries, the presence of noise in\nitem titles with weak language order, issues with long-tail queries, and the\ninterpretability of results. To address these challenges, we have developed an\ninnovative framework for E-commerce search, called generative retrieval with\npreference optimization. This framework is designed to effectively learn and\nalign an autoregressive model with target data, subsequently generating the\nfinal item through constraint-based beam search. By employing multi-span\nidentifiers to represent raw item titles and transforming the task of\ngenerating titles from queries into the task of generating multi-span\nidentifiers from queries, we aim to simplify the generation process. The\nframework further aligns with human preferences using click data and employs a\nconstrained search method to identify key spans for retrieving the final item,\nthereby enhancing result interpretability. Our extensive experiments show that\nthis framework achieves competitive performance on a real-world dataset, and\nonline A/B tests demonstrate the superiority and effectiveness in improving\nconversion gains.",
    "categories": [
      "cs.IR",
      "cs.AI"
    ],
    "primary_category": "cs.IR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.19829v2",
    "published_date": "2024-07-29 09:31:19 UTC",
    "updated_date": "2024-10-25 07:30:45 UTC"
  },
  {
    "arxiv_id": "2407.19825v2",
    "title": "Concise Thoughts: Impact of Output Length on LLM Reasoning and Cost",
    "authors": [
      "Sania Nayab",
      "Giulio Rossolini",
      "Marco Simoni",
      "Andrea Saracino",
      "Giorgio Buttazzo",
      "Nicolamaria Manes",
      "Fabrizio Giacomelli"
    ],
    "abstract": "Today's large language models (LLMs) can solve challenging question-answering\ntasks, and prompt engineering techniques, such as chain-of-thought (CoT), have\ngained attention for enhancing the explanation and correctness of outputs.\nHowever, many models and techniques tend to produce excessively verbose and\nlengthy answers, leading to issues with both conciseness and generation time.\nTo address this, this paper analyzes the impact of output lengths on LLM\ninference pipelines by introducing and proposing novel metrics to evaluate the\n\\textit{correct conciseness} of a model and related prompting techniques. Then,\nwe examine the impact of controlling output length through a refined prompt\nengineering strategy, Constrained-CoT (CCoT), which encourages the model to\nproduce more concise outputs. To better understand the effects of such a\nprompt, we also introduce two additional scores for analyzing the conciseness,\nmeasured in terms of redundancy and information flow in generated answers.\nExperiments on pretrained LLMs and multiple datasets demonstrate the benefits\nof the proposed metrics and the effectiveness of CCoT across different models.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Preprint version, under review",
    "pdf_url": "http://arxiv.org/pdf/2407.19825v2",
    "published_date": "2024-07-29 09:21:52 UTC",
    "updated_date": "2025-01-23 08:45:52 UTC"
  },
  {
    "arxiv_id": "2407.19813v3",
    "title": "Improving Retrieval Augmented Language Model with Self-Reasoning",
    "authors": [
      "Yuan Xia",
      "Jingbo Zhou",
      "Zhenhui Shi",
      "Jun Chen",
      "Haifeng Huang"
    ],
    "abstract": "The Retrieval-Augmented Language Model (RALM) has shown remarkable\nperformance on knowledge-intensive tasks by incorporating external knowledge\nduring inference, which mitigates the factual hallucinations inherited in large\nlanguage models (LLMs). Despite these advancements, challenges persist in the\nimplementation of RALMs, particularly concerning their reliability and\ntraceability. To be specific, the irrelevant document retrieval may result in\nunhelpful response generation or even deteriorate the performance of LLMs,\nwhile the lack of proper citations in generated outputs complicates efforts to\nverify the trustworthiness of the models. To this end, we propose a novel\nself-reasoning framework aimed at improving the reliability and traceability of\nRALMs, whose core idea is to leverage reasoning trajectories generated by the\nLLM itself. The framework involves constructing self-reason trajectories with\nthree processes: a relevance-aware process, an evidence-aware selective\nprocess, and a trajectory analysis process. We have evaluated our framework\nacross four public datasets (two short-form QA datasets, one long-form QA\ndataset, and one fact verification dataset) to demonstrate the superiority of\nour method, which can outperform existing state-of-the-art models and can\nachieve comparable performance with GPT-4, while only using 2,000 training\nsamples.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "AAAI 2025 (main conference)",
    "pdf_url": "http://arxiv.org/pdf/2407.19813v3",
    "published_date": "2024-07-29 09:05:10 UTC",
    "updated_date": "2024-12-19 06:27:44 UTC"
  },
  {
    "arxiv_id": "2407.19811v1",
    "title": "Synthetic Thermal and RGB Videos for Automatic Pain Assessment utilizing a Vision-MLP Architecture",
    "authors": [
      "Stefanos Gkikas",
      "Manolis Tsiknakis"
    ],
    "abstract": "Pain assessment is essential in developing optimal pain management protocols\nto alleviate suffering and prevent functional decline in patients.\nConsequently, reliable and accurate automatic pain assessment systems are\nessential for continuous and effective patient monitoring. This study presents\nsynthetic thermal videos generated by Generative Adversarial Networks\nintegrated into the pain recognition pipeline and evaluates their efficacy. A\nframework consisting of a Vision-MLP and a Transformer-based module is\nutilized, employing RGB and synthetic thermal videos in unimodal and multimodal\nsettings. Experiments conducted on facial videos from the BioVid database\ndemonstrate the effectiveness of synthetic thermal videos and underline the\npotential advantages of it.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.19811v1",
    "published_date": "2024-07-29 09:04:11 UTC",
    "updated_date": "2024-07-29 09:04:11 UTC"
  },
  {
    "arxiv_id": "2407.19809v1",
    "title": "Twins-PainViT: Towards a Modality-Agnostic Vision Transformer Framework for Multimodal Automatic Pain Assessment using Facial Videos and fNIRS",
    "authors": [
      "Stefanos Gkikas",
      "Manolis Tsiknakis"
    ],
    "abstract": "Automatic pain assessment plays a critical role for advancing healthcare and\noptimizing pain management strategies. This study has been submitted to the\nFirst Multimodal Sensing Grand Challenge for Next-Gen Pain Assessment\n(AI4PAIN). The proposed multimodal framework utilizes facial videos and fNIRS\nand presents a modality-agnostic approach, alleviating the need for\ndomain-specific models. Employing a dual ViT configuration and adopting\nwaveform representations for the fNIRS, as well as for the extracted embeddings\nfrom the two modalities, demonstrate the efficacy of the proposed method,\nachieving an accuracy of 46.76% in the multilevel pain assessment task.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.19809v1",
    "published_date": "2024-07-29 09:02:43 UTC",
    "updated_date": "2024-07-29 09:02:43 UTC"
  },
  {
    "arxiv_id": "2407.19804v2",
    "title": "Imputation for prediction: beware of diminishing returns",
    "authors": [
      "Marine Le Morvan",
      "Gaël Varoquaux"
    ],
    "abstract": "Missing values are prevalent across various fields, posing challenges for\ntraining and deploying predictive models. In this context, imputation is a\ncommon practice, driven by the hope that accurate imputations will enhance\npredictions. However, recent theoretical and empirical studies indicate that\nsimple constant imputation can be consistent and competitive. This empirical\nstudy aims at clarifying if and when investing in advanced imputation methods\nyields significantly better predictions. Relating imputation and predictive\naccuracies across combinations of imputation and predictive models on 19\ndatasets, we show that imputation accuracy matters less i) when using\nexpressive models, ii) when incorporating missingness indicators as\ncomplementary inputs, iii) matters much more for generated linear outcomes than\nfor real-data outcomes. Interestingly, we also show that the use of the\nmissingness indicator is beneficial to the prediction performance, even in MCAR\nscenarios. Overall, on real-data with powerful models, improving imputation\nonly has a minor effect on prediction performance. Thus, investing in better\nimputations for improved predictions often offers limited benefits.",
    "categories": [
      "cs.AI",
      "cs.LG",
      "stat.ML"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.19804v2",
    "published_date": "2024-07-29 09:01:06 UTC",
    "updated_date": "2025-02-20 07:57:10 UTC"
  },
  {
    "arxiv_id": "2407.19795v1",
    "title": "VolDoGer: LLM-assisted Datasets for Domain Generalization in Vision-Language Tasks",
    "authors": [
      "Juhwan Choi",
      "Junehyoung Kwon",
      "JungMin Yun",
      "Seunguk Yu",
      "YoungBin Kim"
    ],
    "abstract": "Domain generalizability is a crucial aspect of a deep learning model since it\ndetermines the capability of the model to perform well on data from unseen\ndomains. However, research on the domain generalizability of deep learning\nmodels for vision-language tasks remains limited, primarily because of the lack\nof required datasets. To address these challenges, we propose VolDoGer:\nVision-Language Dataset for Domain Generalization, a dedicated dataset designed\nfor domain generalization that addresses three vision-language tasks: image\ncaptioning, visual question answering, and visual entailment. We constructed\nVolDoGer by extending LLM-based data annotation techniques to vision-language\ntasks, thereby alleviating the burden of recruiting human annotators. We\nevaluated the domain generalizability of various models, ranging from\nfine-tuned models to a recent multimodal large language model, through\nVolDoGer.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.CL",
    "comment": "31 pages, 5 figures, 20 tables",
    "pdf_url": "http://arxiv.org/pdf/2407.19795v1",
    "published_date": "2024-07-29 08:38:46 UTC",
    "updated_date": "2024-07-29 08:38:46 UTC"
  },
  {
    "arxiv_id": "2407.19790v1",
    "title": "Hashing based Contrastive Learning for Virtual Screening",
    "authors": [
      "Jin Han",
      "Yun Hong",
      "Wu-Jun Li"
    ],
    "abstract": "Virtual screening (VS) is a critical step in computer-aided drug discovery,\naiming to identify molecules that bind to a specific target receptor like\nprotein. Traditional VS methods, such as docking, are often too time-consuming\nfor screening large-scale molecular databases. Recent advances in deep learning\nhave demonstrated that learning vector representations for both proteins and\nmolecules using contrastive learning can outperform traditional docking\nmethods. However, given that target databases often contain billions of\nmolecules, real-valued vector representations adopted by existing methods can\nstill incur significant memory and time costs in VS. To address this problem,\nin this paper we propose a hashing-based contrastive learning method, called\nDrugHash, for VS. DrugHash treats VS as a retrieval task that uses efficient\nbinary hash codes for retrieval. In particular, DrugHash designs a simple yet\neffective hashing strategy to enable end-to-end learning of binary hash codes\nfor both protein and molecule modalities, which can dramatically reduce the\nmemory and time costs with higher accuracy compared with existing methods.\nExperimental results show that DrugHash can outperform existing methods to\nachieve state-of-the-art accuracy, with a memory saving of 32$\\times$ and a\nspeed improvement of 3.5$\\times$.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.19790v1",
    "published_date": "2024-07-29 08:33:49 UTC",
    "updated_date": "2024-07-29 08:33:49 UTC"
  },
  {
    "arxiv_id": "2407.19784v1",
    "title": "Survey and Taxonomy: The Role of Data-Centric AI in Transformer-Based Time Series Forecasting",
    "authors": [
      "Jingjing Xu",
      "Caesar Wu",
      "Yuan-Fang Li",
      "Gregoire Danoy",
      "Pascal Bouvry"
    ],
    "abstract": "Alongside the continuous process of improving AI performance through the\ndevelopment of more sophisticated models, researchers have also focused their\nattention to the emerging concept of data-centric AI, which emphasizes the\nimportant role of data in a systematic machine learning training process.\nNonetheless, the development of models has also continued apace. One result of\nthis progress is the development of the Transformer Architecture, which\npossesses a high level of capability in multiple domains such as Natural\nLanguage Processing (NLP), Computer Vision (CV) and Time Series Forecasting\n(TSF). Its performance is, however, heavily dependent on input data\npreprocessing and output data evaluation, justifying a data-centric approach to\nfuture research. We argue that data-centric AI is essential for training AI\nmodels, particularly for transformer-based TSF models efficiently. However,\nthere is a gap regarding the integration of transformer-based TSF and\ndata-centric AI. This survey aims to pin down this gap via the extensive\nliterature review based on the proposed taxonomy. We review the previous\nresearch works from a data-centric AI perspective and we intend to lay the\nfoundation work for the future development of transformer-based architecture\nand data-centric AI.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.19784v1",
    "published_date": "2024-07-29 08:27:21 UTC",
    "updated_date": "2024-07-29 08:27:21 UTC"
  },
  {
    "arxiv_id": "2407.19778v1",
    "title": "Multimodal Large Language Models for Bioimage Analysis",
    "authors": [
      "Shanghang Zhang",
      "Gaole Dai",
      "Tiejun Huang",
      "Jianxu Chen"
    ],
    "abstract": "Rapid advancements in imaging techniques and analytical methods over the past\ndecade have revolutionized our ability to comprehensively probe the biological\nworld at multiple scales, pinpointing the type, quantity, location, and even\ntemporal dynamics of biomolecules. The surge in data complexity and volume\npresents significant challenges in translating this wealth of information into\nknowledge. The recently emerged Multimodal Large Language Models (MLLMs)\nexhibit strong emergent capacities, such as understanding, analyzing,\nreasoning, and generalization. With these capabilities, MLLMs hold promise to\nextract intricate information from biological images and data obtained through\nvarious modalities, thereby expediting our biological understanding and aiding\nin the development of novel computational frameworks. Previously, such\ncapabilities were mostly attributed to humans for interpreting and summarizing\nmeaningful conclusions from comprehensive observations and analysis of\nbiological images. However, the current development of MLLMs shows increasing\npromise in serving as intelligent assistants or agents for augmenting human\nresearchers in biology research",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.19778v1",
    "published_date": "2024-07-29 08:21:25 UTC",
    "updated_date": "2024-07-29 08:21:25 UTC"
  },
  {
    "arxiv_id": "2407.19775v1",
    "title": "Model Agnostic Hybrid Sharding For Heterogeneous Distributed Inference",
    "authors": [
      "Claudio Angione",
      "Yue Zhao",
      "Harry Yang",
      "Ahmad Farhan",
      "Fielding Johnston",
      "James Buban",
      "Patrick Colangelo"
    ],
    "abstract": "The rapid growth of large-scale AI models, particularly large language models\nhas brought significant challenges in data privacy, computational resources,\nand accessibility. Traditional centralized architectures often struggle to meet\nrequired data security and scalability needs which hinders the democratization\nof AI systems. Nesa introduces a model-agnostic sharding framework designed for\ndecentralized AI inference. Our framework uses blockchain-based sequential deep\nneural network sharding to distribute computational tasks across a diverse\nnetwork of nodes based on a personalised heuristic and routing mechanism. This\nenables efficient distributed training and inference for recent large-scale\nmodels even on consumer-grade hardware. We use compression techniques like\ndynamic blockwise quantization and mixed matrix decomposition to reduce data\ntransfer and memory needs. We also integrate robust security measures,\nincluding hardware-based trusted execution environments to ensure data\nintegrity and confidentiality. Evaluating our system across various natural\nlanguage processing and vision tasks shows that these compression strategies do\nnot compromise model accuracy. Our results highlight the potential to\ndemocratize access to cutting-edge AI technologies by enabling secure and\nefficient inference on a decentralized network.",
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.CR",
      "cs.DC"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.19775v1",
    "published_date": "2024-07-29 08:18:48 UTC",
    "updated_date": "2024-07-29 08:18:48 UTC"
  },
  {
    "arxiv_id": "2407.19772v1",
    "title": "Generating Unseen Code Tests In Infinitum",
    "authors": [
      "Marcel Zalmanovici",
      "Orna Raz",
      "Eitan Farchi",
      "Iftach Freund"
    ],
    "abstract": "Large Language Models (LLMs) are used for many tasks, including those related\nto coding. An important aspect of being able to utilize LLMs is the ability to\nassess their fitness for specific usages. The common practice is to evaluate\nLLMs against a set of benchmarks. While benchmarks provide a sound foundation\nfor evaluation and comparison of alternatives, they suffer from the well-known\nweakness of leaking into the training data \\cite{Xu2024Benchmarking}. We\npresent a method for creating benchmark variations that generalize across\ncoding tasks and programming languages, and may also be applied to in-house\ncode bases. Our approach enables ongoing generation of test-data thus\nmitigating the leaking into the training data issue. We implement one\nbenchmark, called \\textit{auto-regression}, for the task of text-to-code\ngeneration in Python. Auto-regression is specifically created to aid in\ndebugging and in tracking model generation changes as part of the LLM\nregression testing process.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.19772v1",
    "published_date": "2024-07-29 08:11:20 UTC",
    "updated_date": "2024-07-29 08:11:20 UTC"
  },
  {
    "arxiv_id": "2407.19765v1",
    "title": "Map2Traj: Street Map Piloted Zero-shot Trajectory Generation with Diffusion Model",
    "authors": [
      "Zhenyu Tao",
      "Wei Xu",
      "Xiaohu You"
    ],
    "abstract": "User mobility modeling serves a crucial role in analysis and optimization of\ncontemporary wireless networks. Typical stochastic mobility models, e.g.,\nrandom waypoint model and Gauss Markov model, can hardly capture the\ndistribution characteristics of users within real-world areas. State-of-the-art\ntrace-based mobility models and existing learning-based trajectory generation\nmethods, however, are frequently constrained by the inaccessibility of\nsubstantial real trajectories due to privacy concerns. In this paper, we\nharness the intrinsic correlation between street maps and trajectories and\ndevelop a novel zero-shot trajectory generation method, named Map2Traj, by\nexploiting the diffusion model. We incorporate street maps as a condition to\nconsistently pilot the denoising process and train our model on diverse sets of\nreal trajectories from various regions in Xi'an, China, and their corresponding\nstreet maps. With solely the street map of an unobserved area, Map2Traj\ngenerates synthetic trajectories that not only closely resemble the real-world\nmobility pattern but also offer comparable efficacy. Extensive experiments\nvalidate the efficacy of our proposed method on zero-shot trajectory generation\ntasks in terms of both trajectory and distribution similarities. In addition, a\ncase study of employing Map2Traj in wireless network optimization is presented\nto validate its efficacy for downstream applications.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.19765v1",
    "published_date": "2024-07-29 07:57:03 UTC",
    "updated_date": "2024-07-29 07:57:03 UTC"
  },
  {
    "arxiv_id": "2407.19740v1",
    "title": "KNOWCOMP POKEMON Team at DialAM-2024: A Two-Stage Pipeline for Detecting Relations in Dialogical Argument Mining",
    "authors": [
      "Zihao Zheng",
      "Zhaowei Wang",
      "Qing Zong",
      "Yangqiu Song"
    ],
    "abstract": "Dialogical Argument Mining(DialAM) is an important branch of Argument\nMining(AM). DialAM-2024 is a shared task focusing on dialogical argument\nmining, which requires us to identify argumentative relations and illocutionary\nrelations among proposition nodes and locution nodes. To accomplish this, we\npropose a two-stage pipeline, which includes the Two-Step S-Node Prediction\nModel in Stage 1 and the YA-Node Prediction Model in Stage 2. We also augment\nthe training data in both stages and introduce context in Stage 2. We\nsuccessfully completed the task and achieved good results. Our team Pokemon\nranked 1st in the ARI Focused score and 4th in the Global Focused score.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Published on the 11th Workshop on Argument Mining",
    "pdf_url": "http://arxiv.org/pdf/2407.19740v1",
    "published_date": "2024-07-29 07:07:37 UTC",
    "updated_date": "2024-07-29 07:07:37 UTC"
  },
  {
    "arxiv_id": "2408.07282v1",
    "title": "Consistency Based Weakly Self-Supervised Learning for Human Activity Recognition with Wearables",
    "authors": [
      "Taoran Sheng",
      "Manfred Huber"
    ],
    "abstract": "While the widely available embedded sensors in smartphones and other wearable\ndevices make it easier to obtain data of human activities, recognizing\ndifferent types of human activities from sensor-based data remains a difficult\nresearch topic in ubiquitous computing. One reason for this is that most of the\ncollected data is unlabeled. However, many current human activity recognition\n(HAR) systems are based on supervised methods, which heavily rely on the labels\nof the data. We describe a weakly self-supervised approach in this paper that\nconsists of two stages: (1) In stage one, the model learns from the nature of\nhuman activities by projecting the data into an embedding space where similar\nactivities are grouped together; (2) In stage two, the model is fine-tuned\nusing similarity information in a few-shot learning fashion using the\nsimilarity information of the data. This allows downstream classification or\nclustering tasks to benefit from the embeddings. Experiments on three benchmark\ndatasets demonstrate the framework's effectiveness and show that our approach\ncan help the clustering algorithm achieve comparable performance in identifying\nand categorizing the underlying human activities as pure supervised techniques\napplied directly to a corresponding fully labeled data set.",
    "categories": [
      "eess.SP",
      "cs.AI",
      "cs.HC",
      "cs.LG"
    ],
    "primary_category": "eess.SP",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2408.07282v1",
    "published_date": "2024-07-29 06:29:21 UTC",
    "updated_date": "2024-07-29 06:29:21 UTC"
  },
  {
    "arxiv_id": "2407.19721v1",
    "title": "Rina: Enhancing Ring-AllReduce with In-network Aggregation in Distributed Model Training",
    "authors": [
      "Zixuan Chen",
      "Xuandong Liu",
      "Minglin Li",
      "Yinfan Hu",
      "Hao Mei",
      "Huifeng Xing",
      "Hao Wang",
      "Wanxin Shi",
      "Sen Liu",
      "Yang Xu"
    ],
    "abstract": "Parameter Server (PS) and Ring-AllReduce (RAR) are two widely utilized\nsynchronization architectures in multi-worker Deep Learning (DL), also referred\nto as Distributed Deep Learning (DDL). However, PS encounters challenges with\nthe ``incast'' issue, while RAR struggles with problems caused by the long\ndependency chain. The emerging In-network Aggregation (INA) has been proposed\nto integrate with PS to mitigate its incast issue. However, such PS-based INA\nhas poor incremental deployment abilities as it requires replacing all the\nswitches to show significant performance improvement, which is not\ncost-effective. In this study, we present the incorporation of INA capabilities\ninto RAR, called RAR with In-Network Aggregation (Rina), to tackle both the\nproblems above. Rina features its agent-worker mechanism. When an INA-capable\nToR switch is deployed, all workers in this rack run as one abstracted worker\nwith the help of the agent, resulting in both excellent incremental deployment\ncapabilities and better throughput. We conducted extensive testbed and\nsimulation evaluations to substantiate the throughput advantages of Rina over\nexisting DDL training synchronization structures. Compared with the\nstate-of-the-art PS-based INA methods ATP, Rina can achieve more than 50\\%\nthroughput with the same hardware cost.",
    "categories": [
      "cs.NI",
      "cs.AI",
      "cs.DC"
    ],
    "primary_category": "cs.NI",
    "comment": "To appear in ICNP 2024. Preview version only",
    "pdf_url": "http://arxiv.org/pdf/2407.19721v1",
    "published_date": "2024-07-29 06:06:10 UTC",
    "updated_date": "2024-07-29 06:06:10 UTC"
  },
  {
    "arxiv_id": "2407.19714v1",
    "title": "Rethinking RGB-D Fusion for Semantic Segmentation in Surgical Datasets",
    "authors": [
      "Muhammad Abdullah Jamal",
      "Omid Mohareri"
    ],
    "abstract": "Surgical scene understanding is a key technical component for enabling\nintelligent and context aware systems that can transform various aspects of\nsurgical interventions. In this work, we focus on the semantic segmentation\ntask, propose a simple yet effective multi-modal (RGB and depth) training\nframework called SurgDepth, and show state-of-the-art (SOTA) results on all\npublicly available datasets applicable for this task. Unlike previous\napproaches, which either fine-tune SOTA segmentation models trained on natural\nimages, or encode RGB or RGB-D information using RGB only pre-trained\nbackbones, SurgDepth, which is built on top of Vision Transformers (ViTs), is\ndesigned to encode both RGB and depth information through a simple fusion\nmechanism. We conduct extensive experiments on benchmark datasets including\nEndoVis2022, AutoLapro, LapI2I and EndoVis2017 to verify the efficacy of\nSurgDepth. Specifically, SurgDepth achieves a new SOTA IoU of 0.86 on EndoVis\n2022 SAR-RARP50 challenge and outperforms the current best method by at least\n4%, using a shallow and compute efficient decoder consisting of ConvNeXt\nblocks.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.19714v1",
    "published_date": "2024-07-29 05:35:51 UTC",
    "updated_date": "2024-07-29 05:35:51 UTC"
  },
  {
    "arxiv_id": "2407.19705v3",
    "title": "CollectiveSFT: Scaling Large Language Models for Chinese Medical Benchmark with Collective Instructions in Healthcare",
    "authors": [
      "Jingwei Zhu",
      "Minghuan Tan",
      "Min Yang",
      "Ruixue Li",
      "Hamid Alinejad-Rokny"
    ],
    "abstract": "The rapid progress in Large Language Models (LLMs) has prompted the creation\nof numerous benchmarks to evaluate their capabilities.This study focuses on the\nComprehensive Medical Benchmark in Chinese (CMB), showcasing how dataset\ndiversity and distribution in supervised fine-tuning (SFT) may enhance LLM\nperformance.Remarkably, We successfully trained a smaller base model to achieve\nscores comparable to larger models, indicating that a diverse and\nwell-distributed dataset can optimize performance regardless of model size.This\nstudy suggests that even smaller models may reach high performance levels with\ncarefully curated and varied datasets. By integrating a wide range of\ninstructional content, our approach addresses potential issues such as data\nquality inconsistencies. Our results imply that a broader spectrum of training\ndata may enhance a model's ability to generalize and perform effectively across\ndifferent medical scenarios, highlighting the importance of dataset quality and\ndiversity in fine-tuning processes. We open-source the model for future\nresearch at https://github.com/CAS-SIAT-XinHai/CollectiveSFT",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Technical Report",
    "pdf_url": "http://arxiv.org/pdf/2407.19705v3",
    "published_date": "2024-07-29 05:00:48 UTC",
    "updated_date": "2024-09-28 02:05:29 UTC"
  },
  {
    "arxiv_id": "2407.20301v1",
    "title": "Legal Aspects of Decentralized and Platform-Driven Economies",
    "authors": [
      "Marcelo Corrales Compagnucci",
      "Toshiyuki Kono",
      "Shinto Teramoto"
    ],
    "abstract": "The sharing economy is sprawling across almost every sector and activity\naround the world. About a decade ago, there were only a handful of platform\ndriven companies operating on the market. Zipcar, BlaBlaCar and Couchsurfing\namong them. Then Airbnb and Uber revolutionized the transportation and\nhospitality industries with a presence in virtually every major city. Access\nover ownership is the paradigm shift from the traditional business model that\ngrants individuals the use of products or services without the necessity of\nbuying them. Digital platforms, data and algorithm-driven companies as well as\ndecentralized blockchain technologies have tremendous potential. But they are\nalso changing the rules of the game. One of such technologies challenging the\nlegal system are AI systems that will also reshape the current legal framework\nconcerning the liability of operators, users and manufacturers. Therefore, this\nintroductory chapter deals with explaining and describing the legal issues of\nsome of these disruptive technologies. The chapter argues for a more\nforward-thinking and flexible regulatory structure.",
    "categories": [
      "cs.CY",
      "cs.AI",
      "econ.GN",
      "q-fin.EC"
    ],
    "primary_category": "cs.CY",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.20301v1",
    "published_date": "2024-07-29 04:42:49 UTC",
    "updated_date": "2024-07-29 04:42:49 UTC"
  },
  {
    "arxiv_id": "2407.19697v2",
    "title": "Multiscale Representation Enhanced Temporal Flow Fusion Model for Long-Term Workload Forecasting",
    "authors": [
      "Shiyu Wang",
      "Zhixuan Chu",
      "Yinbo Sun",
      "Yu Liu",
      "Yuliang Guo",
      "Yang Chen",
      "Huiyang Jian",
      "Lintao Ma",
      "Xingyu Lu",
      "Jun Zhou"
    ],
    "abstract": "Accurate workload forecasting is critical for efficient resource management\nin cloud computing systems, enabling effective scheduling and autoscaling.\nDespite recent advances with transformer-based forecasting models, challenges\nremain due to the non-stationary, nonlinear characteristics of workload time\nseries and the long-term dependencies. In particular, inconsistent performance\nbetween long-term history and near-term forecasts hinders long-range\npredictions. This paper proposes a novel framework leveraging self-supervised\nmultiscale representation learning to capture both long-term and near-term\nworkload patterns. The long-term history is encoded through multiscale\nrepresentations while the near-term observations are modeled via temporal flow\nfusion. These representations of different scales are fused using an attention\nmechanism and characterized with normalizing flows to handle\nnon-Gaussian/non-linear distributions of time series. Extensive experiments on\n9 benchmarks demonstrate superiority over existing methods.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "Proceedings of the 33rd ACM International Conference on Information\n  and Knowledge Management (CIKM '24), October 21--25, 2024, Boise, ID, USA",
    "pdf_url": "http://arxiv.org/pdf/2407.19697v2",
    "published_date": "2024-07-29 04:42:18 UTC",
    "updated_date": "2024-08-19 02:13:57 UTC"
  },
  {
    "arxiv_id": "2407.20299v2",
    "title": "Dataset Distillation for Offline Reinforcement Learning",
    "authors": [
      "Jonathan Light",
      "Yuanzhe Liu",
      "Ziniu Hu"
    ],
    "abstract": "Offline reinforcement learning often requires a quality dataset that we can\ntrain a policy on. However, in many situations, it is not possible to get such\na dataset, nor is it easy to train a policy to perform well in the actual\nenvironment given the offline data. We propose using data distillation to train\nand distill a better dataset which can then be used for training a better\npolicy model. We show that our method is able to synthesize a dataset where a\nmodel trained on it achieves similar performance to a model trained on the full\ndataset or a model trained using percentile behavioral cloning. Our project\nsite is available at\n$\\href{https://datasetdistillation4rl.github.io}{\\text{here}}$. We also provide\nour implementation at $\\href{https://github.com/ggflow123/DDRL}{\\text{this\nGitHub repository}}$.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "ICML 2024 DMLR Workshop",
    "pdf_url": "http://arxiv.org/pdf/2407.20299v2",
    "published_date": "2024-07-29 04:02:17 UTC",
    "updated_date": "2024-08-01 01:33:48 UTC"
  },
  {
    "arxiv_id": "2407.19683v1",
    "title": "Revisiting the robustness of post-hoc interpretability methods",
    "authors": [
      "Jiawen Wei",
      "Hugues Turbé",
      "Gianmarco Mengaldo"
    ],
    "abstract": "Post-hoc interpretability methods play a critical role in explainable\nartificial intelligence (XAI), as they pinpoint portions of data that a trained\ndeep learning model deemed important to make a decision. However, different\npost-hoc interpretability methods often provide different results, casting\ndoubts on their accuracy. For this reason, several evaluation strategies have\nbeen proposed to understand the accuracy of post-hoc interpretability. Many of\nthese evaluation strategies provide a coarse-grained assessment -- i.e., they\nevaluate how the performance of the model degrades on average by corrupting\ndifferent data points across multiple samples. While these strategies are\neffective in selecting the post-hoc interpretability method that is most\nreliable on average, they fail to provide a sample-level, also referred to as\nfine-grained, assessment. In other words, they do not measure the robustness of\npost-hoc interpretability methods. We propose an approach and two new metrics\nto provide a fine-grained assessment of post-hoc interpretability methods. We\nshow that the robustness is generally linked to its coarse-grained performance.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.19683v1",
    "published_date": "2024-07-29 03:55:52 UTC",
    "updated_date": "2024-07-29 03:55:52 UTC"
  },
  {
    "arxiv_id": "2407.19681v3",
    "title": "Motion Manifold Flow Primitives for Task-Conditioned Trajectory Generation under Complex Task-Motion Dependencies",
    "authors": [
      "Yonghyeon Lee",
      "Byeongho Lee",
      "Seungyeon Kim",
      "Frank C. Park"
    ],
    "abstract": "Effective movement primitives should be capable of encoding and generating a\nrich repertoire of trajectories -- typically collected from human\ndemonstrations -- conditioned on task-defining parameters such as vision or\nlanguage inputs. While recent methods based on the motion manifold hypothesis,\nwhich assumes that a set of trajectories lies on a lower-dimensional nonlinear\nsubspace, address challenges such as limited dataset size and the high\ndimensionality of trajectory data, they often struggle to capture complex\ntask-motion dependencies, i.e., when motion distributions shift drastically\nwith task variations. To address this, we introduce Motion Manifold Flow\nPrimitives (MMFP), a framework that decouples the training of the motion\nmanifold from task-conditioned distributions. Specifically, we employ flow\nmatching models, state-of-the-art conditional deep generative models, to learn\ntask-conditioned distributions in the latent coordinate space of the learned\nmotion manifold. Experiments are conducted on language-guided trajectory\ngeneration tasks, where many-to-many text-motion correspondences introduce\ncomplex task-motion dependencies, highlighting MMFP's superiority over existing\nmethods.",
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "primary_category": "cs.RO",
    "comment": "8 pages, 11 figures",
    "pdf_url": "http://arxiv.org/pdf/2407.19681v3",
    "published_date": "2024-07-29 03:53:14 UTC",
    "updated_date": "2025-01-08 06:56:19 UTC"
  },
  {
    "arxiv_id": "2407.19679v1",
    "title": "Harnessing Large Vision and Language Models in Agriculture: A Review",
    "authors": [
      "Hongyan Zhu",
      "Shuai Qin",
      "Min Su",
      "Chengzhi Lin",
      "Anjie Li",
      "Junfeng Gao"
    ],
    "abstract": "Large models can play important roles in many domains. Agriculture is another\nkey factor affecting the lives of people around the world. It provides food,\nfabric, and coal for humanity. However, facing many challenges such as pests\nand diseases, soil degradation, global warming, and food security, how to\nsteadily increase the yield in the agricultural sector is a problem that humans\nstill need to solve. Large models can help farmers improve production\nefficiency and harvest by detecting a series of agricultural production tasks\nsuch as pests and diseases, soil quality, and seed quality. It can also help\nfarmers make wise decisions through a variety of information, such as images,\ntext, etc. Herein, we delve into the potential applications of large models in\nagriculture, from large language model (LLM) and large vision model (LVM) to\nlarge vision-language models (LVLM). After gaining a deeper understanding of\nmultimodal large language models (MLLM), it can be recognized that problems\nsuch as agricultural image processing, agricultural question answering systems,\nand agricultural machine automation can all be solved by large models. Large\nmodels have great potential in the field of agriculture. We outline the current\napplications of agricultural large models, and aims to emphasize the importance\nof large models in the domain of agriculture. In the end, we envisage a future\nin which famers use MLLM to accomplish many tasks in agriculture, which can\ngreatly improve agricultural production efficiency and yield.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.19679v1",
    "published_date": "2024-07-29 03:47:54 UTC",
    "updated_date": "2024-07-29 03:47:54 UTC"
  },
  {
    "arxiv_id": "2407.21072v1",
    "title": "Beyond Metrics: A Critical Analysis of the Variability in Large Language Model Evaluation Frameworks",
    "authors": [
      "Marco AF Pimentel",
      "Clément Christophe",
      "Tathagata Raha",
      "Prateek Munjal",
      "Praveen K Kanithi",
      "Shadab Khan"
    ],
    "abstract": "As large language models (LLMs) continue to evolve, the need for robust and\nstandardized evaluation benchmarks becomes paramount. Evaluating the\nperformance of these models is a complex challenge that requires careful\nconsideration of various linguistic tasks, model architectures, and\nbenchmarking methodologies. In recent years, various frameworks have emerged as\nnoteworthy contributions to the field, offering comprehensive evaluation tests\nand benchmarks for assessing the capabilities of LLMs across diverse domains.\nThis paper provides an exploration and critical analysis of some of these\nevaluation methodologies, shedding light on their strengths, limitations, and\nimpact on advancing the state-of-the-art in natural language processing.",
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.AI",
    "comment": "15 pages, 3 figures",
    "pdf_url": "http://arxiv.org/pdf/2407.21072v1",
    "published_date": "2024-07-29 03:37:14 UTC",
    "updated_date": "2024-07-29 03:37:14 UTC"
  },
  {
    "arxiv_id": "2407.19668v1",
    "title": "Urban Traffic Accident Risk Prediction Revisited: Regionality, Proximity, Similarity and Sparsity",
    "authors": [
      "Minxiao Chen",
      "Haitao Yuan",
      "Nan Jiang",
      "Zhifeng Bao",
      "Shangguang Wang"
    ],
    "abstract": "Traffic accidents pose a significant risk to human health and property\nsafety. Therefore, to prevent traffic accidents, predicting their risks has\ngarnered growing interest. We argue that a desired prediction solution should\ndemonstrate resilience to the complexity of traffic accidents. In particular,\nit should adequately consider the regional background, accurately capture both\nspatial proximity and semantic similarity, and effectively address the sparsity\nof traffic accidents. However, these factors are often overlooked or difficult\nto incorporate. In this paper, we propose a novel multi-granularity\nhierarchical spatio-temporal network. Initially, we innovate by incorporating\nremote sensing data, facilitating the creation of hierarchical\nmulti-granularity structure and the comprehension of regional background. We\nconstruct multiple high-level risk prediction tasks to enhance model's ability\nto cope with sparsity. Subsequently, to capture both spatial proximity and\nsemantic similarity, region feature and multi-view graph undergo encoding\nprocesses to distill effective representations. Additionally, we propose\nmessage passing and adaptive temporal attention module that bridges different\ngranularities and dynamically captures time correlations inherent in traffic\naccident patterns. At last, a multivariate hierarchical loss function is\ndevised considering the complexity of the prediction purpose. Extensive\nexperiments on two real datasets verify the superiority of our model against\nthe state-of-the-art methods.",
    "categories": [
      "cs.DB",
      "cs.AI"
    ],
    "primary_category": "cs.DB",
    "comment": "Accepted by CIKM 2024",
    "pdf_url": "http://arxiv.org/pdf/2407.19668v1",
    "published_date": "2024-07-29 03:10:15 UTC",
    "updated_date": "2024-07-29 03:10:15 UTC"
  },
  {
    "arxiv_id": "2407.19667v1",
    "title": "Smart Language Agents in Real-World Planning",
    "authors": [
      "Annabelle Miin",
      "Timothy Wei"
    ],
    "abstract": "Comprehensive planning agents have been a long term goal in the field of\nartificial intelligence. Recent innovations in Natural Language Processing have\nyielded success through the advent of Large Language Models (LLMs). We seek to\nimprove the travel-planning capability of such LLMs by extending upon the work\nof the previous paper TravelPlanner. Our objective is to explore a new method\nof using LLMs to improve the travel planning experience. We focus specifically\non the \"sole-planning\" mode of travel planning; that is, the agent is given\nnecessary reference information, and its goal is to create a comprehensive plan\nfrom the reference information. While this does not simulate the real-world we\nfeel that an optimization of the sole-planning capability of a travel planning\nagent will still be able to enhance the overall user experience. We propose a\nsemi-automated prompt generation framework which combines the LLM-automated\nprompt and \"human-in-the-loop\" to iteratively refine the prompt to improve the\nLLM performance. Our result shows that LLM automated prompt has its limitations\nand \"human-in-the-loop\" greatly improves the performance by $139\\%$ with one\nsingle iteration.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "5 pages, 1 figure",
    "pdf_url": "http://arxiv.org/pdf/2407.19667v1",
    "published_date": "2024-07-29 03:00:30 UTC",
    "updated_date": "2024-07-29 03:00:30 UTC"
  },
  {
    "arxiv_id": "2407.19655v2",
    "title": "AI-Driven Healthcare: A Review on Ensuring Fairness and Mitigating Bias",
    "authors": [
      "Sribala Vidyadhari Chinta",
      "Zichong Wang",
      "Avash Palikhe",
      "Xingyu Zhang",
      "Ayesha Kashif",
      "Monique Antoinette Smith",
      "Jun Liu",
      "Wenbin Zhang"
    ],
    "abstract": "Artificial intelligence (AI) is rapidly advancing in healthcare, enhancing\nthe efficiency and effectiveness of services across various specialties,\nincluding cardiology, ophthalmology, dermatology, emergency medicine, etc. AI\napplications have significantly improved diagnostic accuracy, treatment\npersonalization, and patient outcome predictions by leveraging technologies\nsuch as machine learning, neural networks, and natural language processing.\nHowever, these advancements also introduce substantial ethical and fairness\nchallenges, particularly related to biases in data and algorithms. These biases\ncan lead to disparities in healthcare delivery, affecting diagnostic accuracy\nand treatment outcomes across different demographic groups. This review paper\nexamines the integration of AI in healthcare, highlighting critical challenges\nrelated to bias and exploring strategies for mitigation. We emphasize the\nnecessity of diverse datasets, fairness-aware algorithms, and regulatory\nframeworks to ensure equitable healthcare delivery. The paper concludes with\nrecommendations for future research, advocating for interdisciplinary\napproaches, transparency in AI decision-making, and the development of\ninnovative and inclusive AI applications.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "Accepted by PLOS digital health",
    "pdf_url": "http://arxiv.org/pdf/2407.19655v2",
    "published_date": "2024-07-29 02:39:17 UTC",
    "updated_date": "2025-05-03 15:15:12 UTC"
  },
  {
    "arxiv_id": "2407.19646v1",
    "title": "Foundations for Unfairness in Anomaly Detection -- Case Studies in Facial Imaging Data",
    "authors": [
      "Michael Livanos",
      "Ian Davidson"
    ],
    "abstract": "Deep anomaly detection (AD) is perhaps the most controversial of data\nanalytic tasks as it identifies entities that are then specifically targeted\nfor further investigation or exclusion. Also controversial is the application\nof AI to facial imaging data. This work explores the intersection of these two\nareas to understand two core questions: \"Who\" these algorithms are being unfair\nto and equally important \"Why\". Recent work has shown that deep AD can be\nunfair to different groups despite being unsupervised with a recent study\nshowing that for portraits of people: men of color are far more likely to be\nchosen to be outliers. We study the two main categories of AD algorithms:\nautoencoder-based and single-class-based which effectively try to compress all\nthe instances with those that can not be easily compressed being deemed to be\noutliers. We experimentally verify sources of unfairness such as the\nunder-representation of a group (e.g. people of color are relatively rare),\nspurious group features (e.g. men are often photographed with hats), and group\nlabeling noise (e.g. race is subjective). We conjecture that lack of\ncompressibility is the main foundation and the others cause it but experimental\nresults show otherwise and we present a natural hierarchy amongst them.",
    "categories": [
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.AI",
    "comment": "16 pages, 8 figures, AAAI/ACM AIES24",
    "pdf_url": "http://arxiv.org/pdf/2407.19646v1",
    "published_date": "2024-07-29 02:04:29 UTC",
    "updated_date": "2024-07-29 02:04:29 UTC"
  },
  {
    "arxiv_id": "2407.19644v1",
    "title": "Realizing Unaligned Block-wise Pruning for DNN Acceleration on Mobile Devices",
    "authors": [
      "Hayun Lee",
      "Dongkun Shin"
    ],
    "abstract": "With the recent proliferation of on-device AI, there is an increasing need to\nrun computationally intensive DNNs directly on mobile devices. However, the\nlimited computing and memory resources of these devices necessitate effective\npruning techniques. Block-wise pruning is promising due to its low accuracy\ndrop tradeoff for speedup gains, but it requires block positions to be aligned\nwith block size, hindering optimal position selection to minimize model\naccuracy drop. Unaligned block pruning (UBP) addresses this by allowing blocks\nto be selected at arbitrary positions, yet its practical use is limited by a\ntime-consuming optimal block selection algorithm and lack of efficient\ninference kernels. In this paper, we propose a pseudo-optimal yet fast block\nselection algorithm called Block Expansion and Division (BED), which can be\nintegrated into an iterative model training process. Additionally, we introduce\nan efficient inference kernel implementation for mobile devices, enabling a\nUBP-based model to achieve similar latency to a DNN model compressed by aligned\nblock pruning. We demonstrate the superiority of our techniques on a real\nmobile phone with MobileNet and ResNet models.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "11 pages, 8 figures",
    "pdf_url": "http://arxiv.org/pdf/2407.19644v1",
    "published_date": "2024-07-29 01:59:06 UTC",
    "updated_date": "2024-07-29 01:59:06 UTC"
  },
  {
    "arxiv_id": "2407.19643v2",
    "title": "Prometheus Chatbot: Knowledge Graph Collaborative Large Language Model for Computer Components Recommendation",
    "authors": [
      "Yunsheng Wang",
      "Songhao Chen",
      "Kevin Jin"
    ],
    "abstract": "Knowledge graphs (KGs) are essential in applications such as network\nalignment, question-answering, and recommender systems (RSs) since they offer\nstructured relational data that facilitate the inference of indirect\nrelationships. However, the development of KG-based RSs capable of processing\nuser inputs in natural language faces significant challenges. Firstly, natural\nlanguage processing units must effectively handle the ambiguity and variability\nin human language to interpret user intents accurately. Secondly, the system\nmust precisely identify and link entities, like product names, to their\ncorresponding nodes in KGs. To overcome these challenges, supported by Lenovo,\nwe developed a novel chatbot called \"Prometheus,\" which integrates a KG with a\nlarge language model (LLM), specifically designed for recommending computer\ncomponents. This chatbot can accurately decode user requests and deliver\npersonalized recommendations derived from KGs, ensuring precise comprehension\nand response to their computer setup needs.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.19643v2",
    "published_date": "2024-07-29 01:57:10 UTC",
    "updated_date": "2024-07-31 03:20:35 UTC"
  },
  {
    "arxiv_id": "2407.19633v2",
    "title": "OptiMUS-0.3: Using Large Language Models to Model and Solve Optimization Problems at Scale",
    "authors": [
      "Ali AhmadiTeshnizi",
      "Wenzhi Gao",
      "Herman Brunborg",
      "Shayan Talaei",
      "Connor Lawless",
      "Madeleine Udell"
    ],
    "abstract": "Optimization problems are pervasive in sectors from manufacturing and\ndistribution to healthcare. However, most such problems are still solved\nheuristically by hand rather than optimally by state-of-the art solvers because\nthe expertise required to formulate and solve these problems limits the\nwidespread adoption of optimization tools and techniques. We introduce a Large\nLanguage Model (LLM)-based system designed to formulate and solve (mixed\ninteger) linear programming problems from their natural language descriptions.\nOur system is capable of developing mathematical models, writing and debugging\nsolver code, evaluating the generated solutions, and improving efficiency and\ncorrectness of its model and code based on these evaluations. OptiMUS-0.3\nutilizes a modular structure to process problems, allowing it to handle\nproblems with long descriptions and complex data without long prompts.\nExperiments demonstrate that OptiMUS-0.3 outperforms existing state-of-the-art\nmethods on easy datasets by more than 12% and on hard datasets (including a new\ndataset, NLP4LP, released with this paper that features long and complex\nproblems) by more than 8%.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "This paper documents OptiMUS-0.3, improving on OptiMUS-0.1\n  (arXiv:2310.06116) and OptiMUS-0.2 (arXiv:2402.10172). arXiv admin note: text\n  overlap with arXiv:2402.10172",
    "pdf_url": "http://arxiv.org/pdf/2407.19633v2",
    "published_date": "2024-07-29 01:31:45 UTC",
    "updated_date": "2025-02-14 22:41:16 UTC"
  },
  {
    "arxiv_id": "2407.19631v3",
    "title": "\"A Good Bot Always Knows Its Limitations\": Assessing Autonomous System Decision-making Competencies through Factorized Machine Self-confidence",
    "authors": [
      "Brett W. Israelsen",
      "Nisar R. Ahmed",
      "Matthew Aitken",
      "Eric W. Frew",
      "Dale A. Lawrence",
      "Brian M. Argrow"
    ],
    "abstract": "How can intelligent machines assess their competency to complete a task? This\nquestion has come into focus for autonomous systems that algorithmically make\ndecisions under uncertainty. We argue that machine self-confidence -- a form of\nmeta-reasoning based on self-assessments of system knowledge about the state of\nthe world, itself, and ability to reason about and execute tasks -- leads to\nmany computable and useful competency indicators for such agents. This paper\npresents our body of work, so far, on this concept in the form of the\nFactorized Machine Self-confidence (FaMSeC) framework, which holistically\nconsiders several major factors driving competency in algorithmic\ndecision-making: outcome assessment, solver quality, model quality, alignment\nquality, and past experience. In FaMSeC, self-confidence indicators are derived\nvia 'problem-solving statistics' embedded in Markov decision process solvers\nand related approaches. These statistics come from evaluating probabilistic\nexceedance margins in relation to certain outcomes and associated competency\nstandards specified by an evaluator. Once designed, and evaluated, the\nstatistics can be easily incorporated into autonomous agents and serve as\nindicators of competency. We include detailed descriptions and examples for\nMarkov decision process agents, and show how outcome assessment and solver\nquality factors can be found for a range of tasking contexts through novel use\nof meta-utility functions, behavior simulations, and surrogate prediction\nmodels. Numerical evaluations are performed to demonstrate that FaMSeC\nindicators perform as desired (references to human subject studies beyond the\nscope of this paper are provided).",
    "categories": [
      "cs.AI",
      "cs.CY",
      "cs.HC",
      "cs.LG",
      "cs.RO"
    ],
    "primary_category": "cs.AI",
    "comment": "63 pages, 22 figures, version accepted to ACM THRI",
    "pdf_url": "http://arxiv.org/pdf/2407.19631v3",
    "published_date": "2024-07-29 01:22:04 UTC",
    "updated_date": "2025-04-15 16:11:56 UTC"
  },
  {
    "arxiv_id": "2407.19630v2",
    "title": "LLMs' Understanding of Natural Language Revealed",
    "authors": [
      "Walid S. Saba"
    ],
    "abstract": "Large language models (LLMs) are the result of a massive experiment in\nbottom-up, data-driven reverse engineering of language at scale. Despite their\nutility in a number of downstream NLP tasks, ample research has shown that LLMs\nare incapable of performing reasoning in tasks that require quantification over\nand the manipulation of symbolic variables (e.g., planning and problem\nsolving); see for example [25][26]. In this document, however, we will focus on\ntesting LLMs for their language understanding capabilities, their supposed\nforte. As we will show here, the language understanding capabilities of LLMs\nhave been widely exaggerated. While LLMs have proven to generate human-like\ncoherent language (since that's how they were designed), their language\nunderstanding capabilities have not been properly tested. In particular, we\nbelieve that the language understanding capabilities of LLMs should be tested\nby performing an operation that is the opposite of 'text generation' and\nspecifically by giving the LLM snippets of text as input and then querying what\nthe LLM \"understood\". As we show here, when doing so it will become apparent\nthat LLMs do not truly understand language, beyond very superficial inferences\nthat are essentially the byproduct of the memorization of massive amounts of\ningested text.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.19630v2",
    "published_date": "2024-07-29 01:21:11 UTC",
    "updated_date": "2024-08-02 11:26:12 UTC"
  },
  {
    "arxiv_id": "2407.19619v1",
    "title": "Enhancing Code Translation in Language Models with Few-Shot Learning via Retrieval-Augmented Generation",
    "authors": [
      "Manish Bhattarai",
      "Javier E. Santos",
      "Shawn Jones",
      "Ayan Biswas",
      "Boian Alexandrov",
      "Daniel O'Malley"
    ],
    "abstract": "The advent of large language models (LLMs) has significantly advanced the\nfield of code translation, enabling automated translation between programming\nlanguages. However, these models often struggle with complex translation tasks\ndue to inadequate contextual understanding. This paper introduces a novel\napproach that enhances code translation through Few-Shot Learning, augmented\nwith retrieval-based techniques. By leveraging a repository of existing code\ntranslations, we dynamically retrieve the most relevant examples to guide the\nmodel in translating new code segments. Our method, based on\nRetrieval-Augmented Generation (RAG), substantially improves translation\nquality by providing contextual examples from which the model can learn in\nreal-time. We selected RAG over traditional fine-tuning methods due to its\nability to utilize existing codebases or a locally stored corpus of code, which\nallows for dynamic adaptation to diverse translation tasks without extensive\nretraining. Extensive experiments on diverse datasets with open LLM models such\nas Starcoder, Llama3-70B Instruct, CodeLlama-34B Instruct, Granite-34B Code\nInstruct, and Mixtral-8x22B, as well as commercial LLM models like GPT-3.5\nTurbo and GPT-4o, demonstrate our approach's superiority over traditional\nzero-shot methods, especially in translating between Fortran and CPP. We also\nexplored varying numbers of shots i.e. examples provided during inference,\nspecifically 1, 2, and 3 shots and different embedding models for RAG,\nincluding Nomic-Embed, Starencoder, and CodeBERT, to assess the robustness and\neffectiveness of our approach.",
    "categories": [
      "cs.AI",
      "cs.SE"
    ],
    "primary_category": "cs.AI",
    "comment": "LLM for code translation",
    "pdf_url": "http://arxiv.org/pdf/2407.19619v1",
    "published_date": "2024-07-29 00:41:48 UTC",
    "updated_date": "2024-07-29 00:41:48 UTC"
  },
  {
    "arxiv_id": "2407.19616v1",
    "title": "TopicTag: Automatic Annotation of NMF Topic Models Using Chain of Thought and Prompt Tuning with LLMs",
    "authors": [
      "Selma Wanna",
      "Ryan Barron",
      "Nick Solovyev",
      "Maksim E. Eren",
      "Manish Bhattarai",
      "Kim Rasmussen",
      "Boian S. Alexandrov"
    ],
    "abstract": "Topic modeling is a technique for organizing and extracting themes from large\ncollections of unstructured text. Non-negative matrix factorization (NMF) is a\ncommon unsupervised approach that decomposes a term frequency-inverse document\nfrequency (TF-IDF) matrix to uncover latent topics and segment the dataset\naccordingly. While useful for highlighting patterns and clustering documents,\nNMF does not provide explicit topic labels, necessitating subject matter\nexperts (SMEs) to assign labels manually. We present a methodology for\nautomating topic labeling in documents clustered via NMF with automatic model\ndetermination (NMFk). By leveraging the output of NMFk and employing prompt\nengineering, we utilize large language models (LLMs) to generate accurate topic\nlabels. Our case study on over 34,000 scientific abstracts on Knowledge Graphs\ndemonstrates the effectiveness of our method in enhancing knowledge management\nand document organization.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted to ACM Symposium on Document Engineering 2024 (DocEng 24),\n  2024",
    "pdf_url": "http://arxiv.org/pdf/2407.19616v1",
    "published_date": "2024-07-29 00:18:17 UTC",
    "updated_date": "2024-07-29 00:18:17 UTC"
  }
]
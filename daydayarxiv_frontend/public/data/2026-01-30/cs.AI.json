{
  "date": "2026-01-30",
  "category": "cs.AI",
  "summary": "",
  "papers": [
    {
      "arxiv_id": "2601.22141v1",
      "title": "Routing the Lottery: Adaptive Subnetworks for Heterogeneous Data",
      "title_zh": "Routing the Lottery：面向异构数据的自适应子网络",
      "authors": [
        "Grzegorz Stefanski",
        "Alberto Presta",
        "Michal Byra"
      ],
      "abstract": "In pruning, the Lottery Ticket Hypothesis posits that large networks contain sparse subnetworks, or winning tickets, that can be trained in isolation to match the performance of their dense counterparts. However, most existing approaches assume a single universal winning ticket shared across all inputs, ignoring the inherent heterogeneity of real-world data. In this work, we propose Routing the Lottery (RTL), an adaptive pruning framework that discovers multiple specialized subnetworks, called adaptive tickets, each tailored to a class, semantic cluster, or environmental condition. Across diverse datasets and tasks, RTL consistently outperforms single- and multi-model baselines in balanced accuracy and recall, while using up to 10 times fewer parameters than independent models and exhibiting semantically aligned. Furthermore, we identify subnetwork collapse, a performance drop under aggressive pruning, and introduce a subnetwork similarity score that enables label-free diagnosis of oversparsification. Overall, our results recast pruning as a mechanism for aligning model structure with data heterogeneity, paving the way toward more modular and context-aware deep learning.",
      "tldr_zh": "该研究针对剪枝(pruning)技术中的彩票假设(Lottery Ticket Hypothesis)进行了改进，指出传统方法往往忽略了真实数据固有的异构性。作者提出了Routing the Lottery (RTL)框架，通过发现多个专门的自适应彩票(adaptive tickets)子网络，使其分别适配不同的类别、语义簇或环境条件。实验表明，RTL在多种任务中的准确率和召回率均优于基线模型，且参数量比独立模型减少了高达10倍。此外，该研究识别了过度剪枝导致的子网络崩溃(subnetwork collapse)现象，并引入子网络相似度评分实现了无标签的过度稀疏化诊断。这一成果将剪枝视为模型结构与数据异构性对齐的机制，为开发更具模块化和上下文感知的深度学习模型开辟了道路。",
      "categories": [
        "cs.AI",
        "cs.CV",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2601.22141v1",
      "published_date": "2026-01-29 18:56:41 UTC",
      "updated_date": "2026-01-29 18:56:41 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-30T04:18:59.401429+00:00"
    },
    {
      "arxiv_id": "2601.22139v1",
      "title": "Reasoning While Asking: Transforming Reasoning Large Language Models from Passive Solvers to Proactive Inquirers",
      "title_zh": "边询问边推理：将推理大语言模型从被动求解器转变为主动询问者",
      "authors": [
        "Xin Chen",
        "Feng Jiang",
        "Yiqian Zhang",
        "Hardy Chen",
        "Shuo Yan",
        "Wenya Xie",
        "Min Yang",
        "Shujian Huang"
      ],
      "abstract": "Reasoning-oriented Large Language Models (LLMs) have achieved remarkable progress with Chain-of-Thought (CoT) prompting, yet they remain fundamentally limited by a \\emph{blind self-thinking} paradigm: performing extensive internal reasoning even when critical information is missing or ambiguous. We propose Proactive Interactive Reasoning (PIR), a new reasoning paradigm that transforms LLMs from passive solvers into proactive inquirers that interleave reasoning with clarification. Unlike existing search- or tool-based frameworks that primarily address knowledge uncertainty by querying external environments, PIR targets premise- and intent-level uncertainty through direct interaction with the user. PIR is implemented via two core components: (1) an uncertainty-aware supervised fine-tuning procedure that equips models with interactive reasoning capability, and (2) a user-simulator-based policy optimization framework driven by a composite reward that aligns model behavior with user intent. Extensive experiments on mathematical reasoning, code generation, and document editing demonstrate that PIR consistently outperforms strong baselines, achieving up to 32.70\\% higher accuracy, 22.90\\% higher pass rate, and 41.36 BLEU improvement, while reducing nearly half of the reasoning computation and unnecessary interaction turns. Further reliability evaluations on factual knowledge, question answering, and missing-premise scenarios confirm the strong generalization and robustness of PIR. Model and code are publicly available at: \\href{https://github.com/SUAT-AIRI/Proactive-Interactive-R1}",
      "tldr_zh": "该研究针对面向推理的大语言模型(LLMs)在信息缺失或模糊时仍盲目自思考(blind self-thinking)的问题，提出了主动交互式推理(Proactive Interactive Reasoning, PIR)范式，将模型从被动求解器转变为主动询问者。PIR通过不确定性感知监督微调(uncertainty-aware supervised fine-tuning)和基于用户模拟器的策略优化框架，实现了推理与澄清过程的有机交替，确保模型行为与用户意图精准对齐。实验在数学推理、代码生成和文档编辑等任务上展开，结果显示PIR显著优于强基线模型，最高提升准确率32.70%并改善了41.36 BLEU。此外，该范式在减少近一半推理计算量和交互轮次的同时，在事实知识问答和前提缺失场景中表现出极强的泛化性与鲁棒性。目前该研究相关的模型与代码已公开，为解决LLMs在Premise-level及Intent-level的不确定性提供了高效方案。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "The manuscript is under review",
      "pdf_url": "https://arxiv.org/pdf/2601.22139v1",
      "published_date": "2026-01-29 18:56:12 UTC",
      "updated_date": "2026-01-29 18:56:12 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-30T04:19:05.472937+00:00"
    },
    {
      "arxiv_id": "2601.22137v1",
      "title": "PRISM: Distribution-free Adaptive Computation of Matrix Functions for Accelerating Neural Network Training",
      "title_zh": "PRISM：用于加速神经网络训练的矩阵函数无分布自适应计算",
      "authors": [
        "Shenghao Yang",
        "Zhichao Wang",
        "Oleg Balabanov",
        "N. Benjamin Erichson",
        "Michael W. Mahoney"
      ],
      "abstract": "Matrix functions such as square root, inverse roots, and orthogonalization play a central role in preconditioned gradient methods for neural network training. This has motivated the development of iterative algorithms that avoid explicit eigendecompositions and rely primarily on matrix multiplications, making them well suited for modern GPU accelerators. We present PRISM (Polynomial-fitting and Randomized Iterative Sketching for Matrix functions computation), a general framework for accelerating iterative algorithms for computing matrix functions. PRISM combines adaptive polynomial approximation with randomized sketching: at each iteration, it fits a polynomial surrogate to the current spectrum via a sketched least-squares problem, adapting to the instance at hand with minimal overhead. We apply PRISM to accelerate Newton-Schulz-like iterations for matrix square roots and orthogonalization, which are core primitives in machine learning. Unlike prior methods, PRISM requires no explicit spectral bounds or singular value estimates; and it adapts automatically to the evolving spectrum. Empirically, PRISM accelerates training when integrated into Shampoo and Muon optimizers.",
      "tldr_zh": "该研究提出了PRISM（Polynomial-fitting and Randomized Iterative Sketching for Matrix functions computation），这是一个旨在加速神经网络训练中矩阵函数迭代计算的通用框架。PRISM将自适应多项式逼近（adaptive polynomial approximation）与随机草图（randomized sketching）技术相结合，通过在每次迭代中利用草图最小二乘问题（sketched least-squares problem）拟合多项式代理，以极低的开销实现对当前矩阵谱的自适应。与以往方法相比，PRISM无需预先获取显式的谱边界（spectral bounds）或奇异值估计（singular value estimates），且能自动适应训练过程中不断变化的谱特性。研究人员将PRISM应用于加速机器学习核心基元中的Newton-Schulz类迭代，主要涵盖矩阵平方根和正交化计算。实验结果表明，将PRISM集成至Shampoo和Muon等优化器后，能够显著提升神经网络的训练效率并充分发挥GPU加速器的性能。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "math.NA",
        "math.OC"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2601.22137v1",
      "published_date": "2026-01-29 18:55:46 UTC",
      "updated_date": "2026-01-29 18:55:46 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-30T04:19:02.621150+00:00"
    },
    {
      "arxiv_id": "2601.22136v1",
      "title": "StepShield: When, Not Whether to Intervene on Rogue Agents",
      "title_zh": "StepShield：异常智能体干预——重在“何时”而非“是否”",
      "authors": [
        "Gloria Felicia",
        "Michael Eniolade",
        "Jinfeng He",
        "Zitha Sasindran",
        "Hemant Kumar",
        "Milan Hussain Angati",
        "Sandeep Bandarupalli"
      ],
      "abstract": "Existing agent safety benchmarks report binary accuracy, conflating early intervention with post-mortem analysis. A detector that flags a violation at step 8 enables intervention; one that reports it at step 48 provides only forensic value. This distinction is critical, yet current benchmarks cannot measure it. We introduce StepShield, the first benchmark to evaluate when violations are detected, not just whether. StepShield contains 9,213 code agent trajectories, including 1,278 meticulously annotated training pairs and a 7,935-trajectory test set with a realistic 8.1% rogue rate. Rogue behaviors are grounded in real-world security incidents across six categories. We propose three novel temporal metrics: Early Intervention Rate (EIR), Intervention Gap, and Tokens Saved. Surprisingly, our evaluation reveals that an LLM-based judge achieves 59% EIR while a static analyzer achieves only 26%, a 2.3x performance gap that is entirely invisible to standard accuracy metrics. We further show that early detection has direct economic benefits: our cascaded HybridGuard detector reduces monitoring costs by 75% and projects to $108M in cumulative savings over five years at enterprise scale. By shifting the focus of evaluation from whether to when, StepShield provides a new foundation for building safer and more economically viable AI agents. The code and data are released under an Apache 2.0 license.",
      "tldr_zh": "现有的智能体安全基准通常仅报告二元准确率，忽视了早期干预与事后分析之间的关键区别，导致无法衡量检测时机的重要性。为此，该研究推出了 StepShield，这是首个旨在评估违规行为检测时机（When）而非仅检测其是否发生（Whether）的基准测试。该基准包含 9,213 条代码智能体轨迹，涵盖了基于真实世界安全事件的六类流氓行为 (Rogue behaviors)，并引入了早期干预率 (Early Intervention Rate)、干预差距 (Intervention Gap) 和节省的 Token 数等新型时间指标。实验发现，基于 LLM 的评判器在早期干预率上比静态分析器高出 2.3 倍，而这种性能差距在传统指标中是不可见的。此外，研究提出的 HybridGuard 检测器可显著降低 75% 的监控成本，证明了早期检测在企业规模应用中的直接经济价值，为开发安全且经济实惠的 AI 智能体提供了新框架。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CR",
        "cs.SE"
      ],
      "primary_category": "cs.LG",
      "comment": "16 pages, 2 figures, 14 tables",
      "pdf_url": "https://arxiv.org/pdf/2601.22136v1",
      "published_date": "2026-01-29 18:55:46 UTC",
      "updated_date": "2026-01-29 18:55:46 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-30T04:19:05.085563+00:00"
    },
    {
      "arxiv_id": "2601.22130v1",
      "title": "World of Workflows: a Benchmark for Bringing World Models to Enterprise Systems",
      "title_zh": "World of Workflows：将世界模型引入企业系统的基准测试",
      "authors": [
        "Lakshya Gupta",
        "Litao Li",
        "Yizhe Liu",
        "Sriram Ganapathi Subramanian",
        "Kaheer Suleman",
        "Zichen Zhang",
        "Haoye Lu",
        "Sumit Pasupalak"
      ],
      "abstract": "Frontier large language models (LLMs) excel as autonomous agents in many domains, yet they remain untested in complex enterprise systems where hidden workflows create cascading effects across interconnected databases. Existing enterprise benchmarks evaluate surface-level agentic task completion similar to general consumer benchmarks, ignoring true challenges in enterprises, such as limited observability, large database state, and hidden workflows with cascading side effects. We introduce World of Workflows (WoW), a realistic ServiceNow-based environment incorporating 4,000+ business rules and 55 active workflows embedded in the system, alongside WoW-bench, a benchmark of 234 tasks evaluating constrained agentic task completion and enterprise dynamics modeling capabilities. We reveal two major takeaways: (1) Frontier LLMs suffer from dynamics blindness, consistently failing to predict the invisible, cascading side effects of their actions, which leads to silent constraint violations, and (2) reliability in opaque systems requires grounded world modeling, where agents must mentally simulate hidden state transitions to bridge the observability gap when high-fidelity feedback is unavailable. For reliable and useful enterprise agents, WoW motivates a new paradigm to explicitly learn system dynamics. We release our GitHub for setting up and evaluating WoW.",
      "tldr_zh": "该研究引入了 World of Workflows (WoW)，这是一个基于 ServiceNow 的真实企业环境，旨在解决大型语言模型 (LLMs) 在处理具有隐性工作流和连锁反应的复杂企业系统时面临的评估挑战。该环境整合了 4,000 多条业务规则和 55 个活动工作流，并配套推出了包含 234 个任务的 WoW-bench 基准，用于评估智能体的受限任务完成能力和企业动态建模能力。研究发现，现有的前沿 LLMs 普遍存在“动态盲区” (dynamics blindness) 问题，即无法准确预测其行为带来的不可见连锁副作用，从而导致静默的约束违规。实验结果表明，在不透明系统中实现可靠性需要基于落地的世界建模 (grounded world modeling)，智能体必须能够通过心理模拟隐性状态转移来弥补观察能力的不足。该工作为开发可靠且实用的企业智能体提供了一个新的范式，强调了显式学习系统动态 (system dynamics) 的重要性，并公开发布了相关代码库。",
      "categories": [
        "cs.AI",
        "cs.SE"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2601.22130v1",
      "published_date": "2026-01-29 18:51:54 UTC",
      "updated_date": "2026-01-29 18:51:54 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-30T04:19:07.158988+00:00"
    },
    {
      "arxiv_id": "2601.22129v1",
      "title": "SWE-Replay: Efficient Test-Time Scaling for Software Engineering Agents",
      "title_zh": "SWE-Replay：面向软件工程智能体的高效测试时扩展",
      "authors": [
        "Yifeng Ding",
        "Lingming Zhang"
      ],
      "abstract": "Test-time scaling has been widely adopted to enhance the capabilities of Large Language Model (LLM) agents in software engineering (SWE) tasks. However, the standard approach of repeatedly sampling trajectories from scratch is computationally expensive. While recent methods have attempted to mitigate costs using specialized value agents, they can suffer from model miscalibration and fail to generalize to modern agents that synthesize custom bash scripts as tools. In this paper, we introduce SWE-Replay, the first efficient and generalizable test-time scaling technique for modern agents without reliance on potentially noisy value estimates. SWE-Replay optimizes the scaling process by recycling trajectories from prior trials, dynamically choosing to either explore from scratch or exploit archived experience by branching at critical intermediate steps. This selection of intermediate steps is driven by the potential and reasoning significance of repository exploration, rather than external LLM-based quality estimates. Our evaluation shows that, on SWE-Bench Verified, SWE-Replay consistently outperforms naive scaling, reducing costs by up to 17.4% while maintaining or even improving performance by up to 3.8%. Further evaluation on SWE-Bench Pro and Multilingual validates the generalizability of SWE-Replay, establishing it as a robust foundation for efficient test-time scaling of software engineering agents.",
      "tldr_zh": "该研究提出了SWE-Replay，这是一种针对软件工程(Software Engineering)智能体的高效且通用的测试时缩放(Test-Time Scaling)技术，旨在解决传统重复采样轨迹导致的高计算成本问题。与依赖外部LLM质量评估的方法不同，SWE-Replay通过回收先前尝试中的轨迹，根据仓库探索的潜力和推理重要性动态选择关键中间步骤进行分支。该技术灵活地平衡了从零开始的探索(Explore)与利用存档经验的开发(Exploit)，有效避免了模型误校准(Miscalibration)并能适应合成自定义Bash脚本的现代智能体。在SWE-Bench Verified基准测试上的评估显示，SWE-Replay在将成本降低高达17.4%的同时，将性能提升了3.8%。在SWE-Bench Pro和Multilingual上的进一步验证证明了该方法的泛化能力，为软件工程智能体的高效缩放提供了稳健的基础。",
      "categories": [
        "cs.SE",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.SE",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2601.22129v1",
      "published_date": "2026-01-29 18:50:29 UTC",
      "updated_date": "2026-01-29 18:50:29 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-30T04:19:10.424911+00:00"
    },
    {
      "arxiv_id": "2601.22128v1",
      "title": "The Patient is not a Moving Document: A World Model Training Paradigm for Longitudinal EHR",
      "title_zh": "患者并非移动的文档：纵向电子健康档案的世界模型训练范式",
      "authors": [
        "Irsyad Adam",
        "Zekai Chen",
        "David Laprade",
        "Shaun Porwal",
        "David Laub",
        "Erik Reinertsen",
        "Arda Pekis",
        "Kevin Brown"
      ],
      "abstract": "Large language models (LLMs) trained with next-word-prediction have achieved success as clinical foundation models. Representations from these language backbones yield strong linear probe performance across biomedical tasks, suggesting that patient semantics emerge from next-token prediction at scale. However, this paradigm treats patients as a document to be summarized rather than a dynamical system to be simulated; a patient's trajectory emerges from their state evolving under interventions and time, requiring models that simulate dynamics rather than predict tokens. To address this, we introduce SMB-Structure, a world model for structured EHR that grounds a joint-embedding prediction architecture (JEPA) with next-token prediction (SFT). SFT grounds our model to reconstruct future patient states in token space, while JEPA predicts those futures in latent space from the initial patient representation alone, forcing trajectory dynamics to be encoded before the next state is observed. We validate across two large-scale cohorts: Memorial Sloan Kettering (23,319 oncology patients; 323,000+ patient-years) and INSPECT (19,402 pulmonary embolism patients). Using a linear probe evaluated at multiple points along the disease trajectory, we demonstrate that our training paradigm learns embeddings that capture disease dynamics not recoverable by autoregressive baselines, enabling SMB-Structure to achieve competitive performance on complex tasks characterized by high patient heterogeneity. Model weights are available at https://huggingface.co/standardmodelbio/SMB-v1-1.7B-Structure.",
      "tldr_zh": "该研究针对当前临床基础模型将患者视为静态文档而非动态系统的局限性，提出了面向结构化电子健康记录(EHR)的世界模型(World Model)训练范式SMB-Structure。该框架创新性地将联合嵌入预测架构(JEPA)与传统的下文预测(SFT)相结合，旨在模拟患者状态随时间和干预演化的过程。通过在潜空间(Latent Space)仅从初始表征预测未来，该模型被迫在观察到实际状态前对患者的轨迹动力学进行编码。研究团队在Memorial Sloan Kettering肿瘤患者和INSPECT肺栓塞患者两个大规模数据集上进行了验证。实验结果表明，SMB-Structure捕捉到的疾病动态嵌入是传统自回归(Autoregressive)基线模型无法实现的。该模型在处理具有高度异质性的复杂临床任务时展现了卓越的性能，并已在Hugging Face开源了其1.7B参数规模的模型权重。",
      "categories": [
        "cs.AI",
        "cs.CE",
        "q-bio.QM"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2601.22128v1",
      "published_date": "2026-01-29 18:49:37 UTC",
      "updated_date": "2026-01-29 18:49:37 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-30T04:19:16.370052+00:00"
    },
    {
      "arxiv_id": "2601.22119v1",
      "title": "Alpha Discovery via Grammar-Guided Learning and Search",
      "title_zh": "基于语法引导学习与搜索的 Alpha 因子发现",
      "authors": [
        "Han Yang",
        "Dong Hao",
        "Zhuohan Wang",
        "Qi Shi",
        "Xingtong Li"
      ],
      "abstract": "Automatically discovering formulaic alpha factors is a central problem in quantitative finance. Existing methods often ignore syntactic and semantic constraints, relying on exhaustive search over unstructured and unbounded spaces. We present AlphaCFG, a grammar-based framework for defining and discovering alpha factors that are syntactically valid, financially interpretable, and computationally efficient. AlphaCFG uses an alpha-oriented context-free grammar to define a tree-structured, size-controlled search space, and formulates alpha discovery as a tree-structured linguistic Markov decision process, which is then solved using a grammar-aware Monte Carlo Tree Search guided by syntax-sensitive value and policy networks. Experiments on Chinese and U.S. stock market datasets show that AlphaCFG outperforms state-of-the-art baselines in both search efficiency and trading profitability. Beyond trading strategies, AlphaCFG serves as a general framework for symbolic factor discovery and refinement across quantitative finance, including asset pricing and portfolio construction.",
      "tldr_zh": "该研究针对量化金融中公式化因子 (Alpha factors) 发现过程中忽略语法和语义约束的问题，提出了名为 AlphaCFG 的基于语法的框架。AlphaCFG 使用面向 Alpha 的上下文无关语法 (Context-Free Grammar) 来定义一个语法有效、具备财务可解释性且计算高效的树形搜索空间。该框架将 Alpha 发现过程建模为树形结构的语言马尔可夫决策过程 (Markov Decision Process)，并利用受语法感知的蒙特卡洛树搜索 (Monte Carlo Tree Search) 进行求解，同时辅以对语法敏感的价值网络 (Value Network) 和策略网络 (Policy Network) 提供引导。在中国和美国股票市场的实验结果表明，AlphaCFG 在搜索效率和交易盈利能力方面均显著优于现有基准模型。除了交易策略发现，AlphaCFG 还可以作为符号因子提炼的通用框架，广泛应用于资产定价和投资组合构建等量化金融领域。",
      "categories": [
        "q-fin.CP",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "q-fin.CP",
      "comment": "24 pages, 10 figures",
      "pdf_url": "https://arxiv.org/pdf/2601.22119v1",
      "published_date": "2026-01-29 18:46:15 UTC",
      "updated_date": "2026-01-29 18:46:15 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-30T04:19:14.677580+00:00"
    },
    {
      "arxiv_id": "2601.22118v1",
      "title": "Defining Operational Conditions for Safety-Critical AI-Based Systems from Data",
      "title_zh": "基于数据的安全关键型人工智能系统运行条件定义",
      "authors": [
        "Johann Christensen",
        "Elena Hoemann",
        "Frank Köster",
        "Sven Hallerbach"
      ],
      "abstract": "Artificial Intelligence (AI) has been on the rise in many domains, including numerous safety-critical applications. However, for complex systems found in the real world, or when data already exist, defining the underlying environmental conditions is extremely challenging. This often results in an incomplete description of the environment in which the AI-based system must operate. Nevertheless, this description, called the Operational Design Domain (ODD), is required in many domains for the certification of AI-based systems. Traditionally, the ODD is created in the early stages of the development process, drawing on sophisticated expert knowledge and related standards. This paper presents a novel Safety-by-Design method to a posteriori define the ODD from previously collected data using a multi-dimensional kernel-based representation. This approach is validated through both Monte Carlo methods and a real-world aviation use case for a future safety-critical collision-avoidance system. Moreover, by defining under what conditions two ODDs are equal, the paper shows that the data-driven ODD can equal the original, underlying hidden ODD of the data. Utilizing the novel, Safe-by-Design kernel-based ODD enables future certification of data-driven, safety-critical AI-based systems.",
      "tldr_zh": "针对安全关键型人工智能系统在复杂真实环境中难以精确定义运行设计域 (Operational Design Domain, ODD) 的挑战，该研究提出了一种创新的 Safety-by-Design 方法。该方法采用多维核表示 (multi-dimensional kernel-based representation)，能够从先前收集的数据中后验地 (a posteriori) 定义 ODD，从而解决环境描述不完整的问题。研究通过 Monte Carlo 方法以及航空领域中未来安全关键型冲突避让系统的真实案例对该方法进行了验证。通过定义两个 ODD 相等的判定条件，论文证明了数据驱动生成的 ODD 可以还原数据背后原始且隐藏的运行条件。这种基于核的 Safe-by-Design ODD 为数据驱动型安全关键 AI 系统的合规性认证奠定了基础。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2601.22118v1",
      "published_date": "2026-01-29 18:46:02 UTC",
      "updated_date": "2026-01-29 18:46:02 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-30T04:19:18.701704+00:00"
    },
    {
      "arxiv_id": "2601.22114v1",
      "title": "SINA: A Circuit Schematic Image-to-Netlist Generator Using Artificial Intelligence",
      "title_zh": "SINA：基于人工智能的电路原理图图像至网表生成器",
      "authors": [
        "Saoud Aldowaish",
        "Yashwanth Karumanchi",
        "Kai-Chen Chiang",
        "Soroosh Noorzad",
        "Morteza Fayazi"
      ],
      "abstract": "Current methods for converting circuit schematic images into machine-readable netlists struggle with component recognition and connectivity inference. In this paper, we present SINA, an open-source, fully automated circuit schematic image-to-netlist generator. SINA integrates deep learning for accurate component detection, Connected-Component Labeling (CCL) for precise connectivity extraction, and Optical Character Recognition (OCR) for component reference designator retrieval, while employing a Vision-Language Model (VLM) for reliable reference designator assignments. In our experiments, SINA achieves 96.47% overall netlist-generation accuracy, which is 2.72x higher than state-of-the-art approaches.",
      "tldr_zh": "该研究提出了 SINA，一个开源且全自动的电路原理图图像转 netlist 生成器，旨在解决现有方法在元器件识别和连接推断上的局限。SINA 集成了 deep learning 技术进行准确的元器件检测，并采用 Connected-Component Labeling (CCL) 实现精确的连通性提取。同时，该系统利用 Optical Character Recognition (OCR) 检索元器件的 reference designator，并结合 Vision-Language Model (VLM) 确保其分配的可靠性。实验结果显示，SINA 在 netlist 生成任务中达到了 96.47% 的总准确率，其性能比当前最先进的方法高出 2.72 倍。该工具的开发为电子设计自动化领域提供了一种高效、精准的自动化电路图转换方案。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "eess.SY"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2601.22114v1",
      "published_date": "2026-01-29 18:41:52 UTC",
      "updated_date": "2026-01-29 18:41:52 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-30T04:19:22.588444+00:00"
    },
    {
      "arxiv_id": "2601.22108v1",
      "title": "Value-Based Pre-Training with Downstream Feedback",
      "title_zh": "基于下游反馈的价值驱动预训练",
      "authors": [
        "Shuqi Ke",
        "Giulia Fanti"
      ],
      "abstract": "Can a small amount of verified goal information steer the expensive self-supervised pretraining of foundation models? Standard pretraining optimizes a fixed proxy objective (e.g., next-token prediction), which can misallocate compute away from downstream capabilities of interest. We introduce V-Pretraining: a value-based, modality-agnostic method for controlled continued pretraining in which a lightweight task designer reshapes the pretraining task to maximize the value of each gradient step. For example, consider self-supervised learning (SSL) with sample augmentation. The V-Pretraining task designer selects pretraining tasks (e.g., augmentations) for which the pretraining loss gradient is aligned with a gradient computed over a downstream task (e.g., image segmentation). This helps steer pretraining towards relevant downstream capabilities. Notably, the pretrained model is never updated on downstream task labels; they are used only to shape the pretraining task. Under matched learner update budgets, V-Pretraining of 0.5B--7B language models improves reasoning (GSM8K test Pass@1) by up to 18% relative over standard next-token prediction using only 12% of GSM8K training examples as feedback. In vision SSL, we improve the state-of-the-art results on ADE20K by up to 1.07 mIoU and reduce NYUv2 RMSE while improving ImageNet linear accuracy, and we provide pilot evidence of improved token efficiency in continued pretraining.",
      "tldr_zh": "该研究提出了V-Pretraining，一种基于价值的、模态无关的受控持续预训练方法，旨在通过少量的下游任务反馈引导基础模型的自监督预训练。针对传统预训练优化固定代理目标（如next-token prediction）导致的计算资源错位问题，该方法引入了一个轻量级的任务设计器，通过调整预训练任务使预训练损失梯度与下游任务梯度保持一致。研究强调预训练模型从未直接在下游标签上更新，标签仅用于重塑预训练任务的选择。实验结果显示，在仅使用12%的GSM8K训练数据作为反馈的情况下，V-Pretraining使0.5B至7B语言模型的推理准确率（Pass@1）相对提升了高达18%。此外，该方法在视觉自监督学习中也取得了显著进展，不仅刷新了ADE20K的性能记录，还展现了在提升Token效率和下游任务关联性方面的巨大潜力。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2601.22108v1",
      "published_date": "2026-01-29 18:38:09 UTC",
      "updated_date": "2026-01-29 18:38:09 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-30T04:19:34.429650+00:00"
    },
    {
      "arxiv_id": "2601.22101v1",
      "title": "ECO: Quantized Training without Full-Precision Master Weights",
      "title_zh": "ECO：无需全精度主权重的量化训练",
      "authors": [
        "Mahdi Nikdan",
        "Amir Zandieh",
        "Dan Alistarh",
        "Vahab Mirrokni"
      ],
      "abstract": "Quantization has significantly improved the compute and memory efficiency of Large Language Model (LLM) training. However, existing approaches still rely on accumulating their updates in high-precision: concretely, gradient updates must be applied to a high-precision weight buffer, known as $\\textit{master weights}$. This buffer introduces substantial memory overhead, particularly for Sparse Mixture of Experts (SMoE) models, where model parameters and optimizer states dominate memory usage. To address this, we introduce the Error-Compensating Optimizer (ECO), which eliminates master weights by applying updates directly to quantized parameters. ECO quantizes weights after each step and carefully injects the resulting quantization error into the optimizer momentum, forming an error-feedback loop with no additional memory. We prove that, under standard assumptions and a decaying learning rate, ECO converges to a constant-radius neighborhood of the optimum, while naive master-weight removal can incur an error that is inversely proportional to the learning rate. We show empirical results for pretraining small Transformers (30-800M), a Gemma-3 1B model, and a 2.1B parameter Sparse MoE model with FP8 quantization, and fine-tuning DeepSeek-MoE-16B in INT4 precision. Throughout, ECO matches baselines with master weights up to near-lossless accuracy, significantly shifting the static memory vs validation loss Pareto frontier.",
      "tldr_zh": "该研究针对大语言模型(LLM)量化训练中依赖高精度Master Weights导致的显著内存开销问题，提出了Error-Compensating Optimizer (ECO)。ECO通过直接在量化参数上应用更新，并将每步产生的量化误差注入优化器动量，构建了无额外内存负担的错误反馈循环，从而彻底消除了对Master Weights的需求。理论证明在标准假设和衰减学习率下，该算法能收敛至最优解的常数半径邻域内，避免了直接移除高精度权重可能产生的巨大误差。实验涵盖了从小型Transformers到Gemma-3 1B、FP8量化的Sparse MoE模型预训练以及INT4精度的DeepSeek-MoE-16B微调。测试结果表明，ECO能够在保持近乎无损精度的同时，显著优化静态内存与验证损失之间的Pareto前沿。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2601.22101v1",
      "published_date": "2026-01-29 18:35:01 UTC",
      "updated_date": "2026-01-29 18:35:01 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-30T04:19:40.351287+00:00"
    },
    {
      "arxiv_id": "2601.22093v1",
      "title": "Investigating Associational Biases in Inter-Model Communication of Large Generative Models",
      "title_zh": "探究大型生成模型跨模型通信中的关联性偏差",
      "authors": [
        "Fethiye Irmak Dogan",
        "Yuval Weiss",
        "Kajal Patel",
        "Jiaee Cheong",
        "Hatice Gunes"
      ],
      "abstract": "Social bias in generative AI can manifest not only as performance disparities but also as associational bias, whereby models learn and reproduce stereotypical associations between concepts and demographic groups, even in the absence of explicit demographic information (e.g., associating doctors with men). These associations can persist, propagate, and potentially amplify across repeated exchanges in inter-model communication pipelines, where one generative model's output becomes another's input. This is especially salient for human-centred perception tasks, such as human activity recognition and affect prediction, where inferences about behaviour and internal states can lead to errors or stereotypical associations that propagate into unequal treatment. In this work, focusing on human activity and affective expression, we study how such associations evolve within an inter-model communication pipeline that alternates between image generation and image description. Using the RAF-DB and PHASE datasets, we quantify demographic distribution drift induced by model-to-model information exchange and assess whether these drifts are systematic using an explainability pipeline. Our results reveal demographic drifts toward younger representations for both actions and emotions, as well as toward more female-presenting representations, primarily for emotions. We further find evidence that some predictions are supported by spurious visual regions (e.g., background or hair) rather than concept-relevant cues (e.g., body or face). We also examine whether these demographic drifts translate into measurable differences in downstream behaviour, i.e., while predicting activity and emotion labels. Finally, we outline mitigation strategies spanning data-centric, training and deployment interventions, and emphasise the need for careful safeguards when deploying interconnected models in human-centred AI systems.",
      "tldr_zh": "本研究探讨了大型生成模型在模型间通信(inter-model communication)过程中的关联偏见(associational biases)，即模型在缺乏明确人口统计信息时仍会产生刻板关联并可能在模型交替输出中放大。研究重点针对人类活动识别(human activity recognition)和情感预测(affect prediction)任务，分析了图像生成与描述交替迭代对偏见传播的影响。通过对RAF-DB和PHASE数据集的实验，研究量化了信息交换导致的人口统计分布漂移(demographic distribution drift)，发现动作和情感表征均呈现出明显的年轻化趋势，且情感表征表现出更多女性化特征。此外，研究利用可解释性流程揭示了模型预测往往依赖于背景等虚假视觉区域(spurious visual regions)而非关键生物特征。最后，作者评估了这些漂移对下游任务的负面影响，并提出了涵盖数据、训练和部署层面的缓解策略，强调了在以人为中心的AI系统中为互联模型建立安全保障的必要性。",
      "categories": [
        "cs.CY",
        "cs.AI"
      ],
      "primary_category": "cs.CY",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2601.22093v1",
      "published_date": "2026-01-29 18:29:55 UTC",
      "updated_date": "2026-01-29 18:29:55 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-30T04:19:43.103876+00:00"
    },
    {
      "arxiv_id": "2601.22083v1",
      "title": "Latent Adversarial Regularization for Offline Preference Optimization",
      "title_zh": "面向离线偏好优化的潜对抗正则化",
      "authors": [
        "Enyi Jiang",
        "Yibo Jacky Zhang",
        "Yinglun Xu",
        "Andreas Haupt",
        "Nancy Amato",
        "Sanmi Koyejo"
      ],
      "abstract": "Learning from human feedback typically relies on preference optimization that constrains policy updates through token-level regularization. However, preference optimization for language models is particularly challenging because token-space similarity does not imply semantic or behavioral similarity. To address this challenge, we leverage latent-space regularization for language model preference optimization. We introduce GANPO, which achieves latent-space regularization by penalizing divergence between the internal representations of a policy model and a reference model. Given that latent representations are not associated with explicit probability densities, we adopt an adversarial approach inspired by GANs to minimize latent-space divergence. We integrate GANPO as a regularizer into existing offline preference optimization objectives. Experiments across multiple model architectures and tasks show consistent improvements from latent-space regularization. Further, by comparing GANPO-induced inferential biases with those from token-level regularization, we find that GANPO provides more robust structural feedback under distributional shift and noise while maintaining comparable downstream performance with minor computational overhead.",
      "tldr_zh": "该研究针对语言模型在 Preference Optimization 过程中，传统 token 级别正则化无法准确衡量语义或行为相似性的难题，提出了基于 Latent-space 正则化的优化策略。研究团队引入了 GANPO 框架，通过惩罚策略模型与参考模型之间内部表示（internal representations）的散度来实现正则化。由于潜空间表示缺乏显式的概率密度，GANPO 采用了一种受 GAN 启发的对抗性方法来最小化潜空间散度，并将其作为正则化项集成到现有的离线偏好优化目标中。在多种模型架构和任务上的实验证明，该方法不仅提升了模型表现，且在面对分布偏移（distributional shift）和噪声时表现出比 token 级正则化更强的鲁棒性。该研究证明了 GANPO 在保持下游任务性能的同时，仅引入了极小的计算开销，为离线偏好优化提供了更稳健的结构化反馈。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2601.22083v1",
      "published_date": "2026-01-29 18:21:57 UTC",
      "updated_date": "2026-01-29 18:21:57 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-30T04:19:39.894565+00:00"
    },
    {
      "arxiv_id": "2601.22060v1",
      "title": "Vision-DeepResearch: Incentivizing DeepResearch Capability in Multimodal Large Language Models",
      "title_zh": "Vision-DeepResearch：激发多模态大语言模型的深度研究能力",
      "authors": [
        "Wenxuan Huang",
        "Yu Zeng",
        "Qiuchen Wang",
        "Zhen Fang",
        "Shaosheng Cao",
        "Zheng Chu",
        "Qingyu Yin",
        "Shuang Chen",
        "Zhenfei Yin",
        "Lin Chen",
        "Zehui Chen",
        "Yao Hu",
        "Philip Torr",
        "Feng Zhao",
        "Wanli Ouyang"
      ],
      "abstract": "Multimodal large language models (MLLMs) have achieved remarkable success across a broad range of vision tasks. However, constrained by the capacity of their internal world knowledge, prior work has proposed augmenting MLLMs by ``reasoning-then-tool-call'' for visual and textual search engines to obtain substantial gains on tasks requiring extensive factual information. However, these approaches typically define multimodal search in a naive setting, assuming that a single full-level or entity-level image query and few text query suffices to retrieve the key evidence needed to answer the question, which is unrealistic in real-world scenarios with substantial visual noise. Moreover, they are often limited in the reasoning depth and search breadth, making it difficult to solve complex questions that require aggregating evidence from diverse visual and textual sources. Building on this, we propose Vision-DeepResearch, which proposes one new multimodal deep-research paradigm, i.e., performs multi-turn, multi-entity and multi-scale visual and textual search to robustly hit real-world search engines under heavy noise. Our Vision-DeepResearch supports dozens of reasoning steps and hundreds of engine interactions, while internalizing deep-research capabilities into the MLLM via cold-start supervision and RL training, resulting in a strong end-to-end multimodal deep-research MLLM. It substantially outperforming existing multimodal deep-research MLLMs, and workflows built on strong closed-source foundation model such as GPT-5, Gemini-2.5-pro and Claude-4-Sonnet. The code will be released in https://github.com/Osilly/Vision-DeepResearch.",
      "tldr_zh": "该研究提出了Vision-DeepResearch，旨在增强多模态大语言模型（MLLMs）在复杂现实场景下的深度搜索能力。针对现有模型在处理视觉噪声、推理深度和搜索广度方面的局限性，该框架引入了一种全新的多模态深度研究范式，支持多轮、多实体及多尺度的视觉与文本搜索，能够有效应对现实世界中的复杂干扰。通过冷启动监督（cold-start supervision）和强化学习（RL）训练，Vision-DeepResearch成功将深度研究能力内化到模型中，使其能够支持数十次推理步骤和数百次引擎交互。实验结果显示，该端到端模型在性能上显著超越了现有的多模态深度研究模型，以及基于GPT-5、Gemini-2.5-pro和Claude-4-Sonnet等强力闭源基础模型构建的工作流。这一研究不仅提升了MLLMs获取事实信息的准确性，也为开发具备高级自主研究能力的多模态智能体奠定了基础。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2601.22060v1",
      "published_date": "2026-01-29 17:58:40 UTC",
      "updated_date": "2026-01-29 17:58:40 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-30T04:19:43.008979+00:00"
    },
    {
      "arxiv_id": "2601.22057v1",
      "title": "Unsupervised Decomposition and Recombination with Discriminator-Driven Diffusion Models",
      "title_zh": "基于判别器驱动扩散模型的无监督分解与重组",
      "authors": [
        "Archer Wang",
        "Emile Anand",
        "Yilun Du",
        "Marin Soljačić"
      ],
      "abstract": "Decomposing complex data into factorized representations can reveal reusable components and enable synthesizing new samples via component recombination. We investigate this in the context of diffusion-based models that learn factorized latent spaces without factor-level supervision. In images, factors can capture background, illumination, and object attributes; in robotic videos, they can capture reusable motion components. To improve both latent factor discovery and quality of compositional generation, we introduce an adversarial training signal via a discriminator trained to distinguish between single-source samples and those generated by recombining factors across sources. By optimizing the generator to fool this discriminator, we encourage physical and semantic consistency in the resulting recombinations. Our method outperforms implementations of prior baselines on CelebA-HQ, Virtual KITTI, CLEVR, and Falcor3D, achieving lower FID scores and better disentanglement as measured by MIG and MCC. Furthermore, we demonstrate a novel application to robotic video trajectories: by recombining learned action components, we generate diverse sequences that significantly increase state-space coverage for exploration on the LIBERO benchmark.",
      "tldr_zh": "该研究探讨了在无监督条件下，如何利用基于扩散的模型(diffusion-based models)将复杂数据分解为因子化表征，并提出了区分器驱动的扩散模型(Discriminator-Driven Diffusion Models)。为了提升潜在因子的发现质量和组合生成的性能，该方法引入了对抗性训练信号，利用区分器来辨别单一来源样本与跨来源重组因子生成的样本。通过优化生成器以欺骗区分器，该框架有效确保了重组结果在物理和语义上的一致性。实验结果表明，该方法在CelebA-HQ、Virtual KITTI、CLEVR和Falcor3D等数据集上表现优于现有基准模型，在FID分数以及解耦指标MIG和MCC上均取得了更优的结果。此外，该研究还将此技术应用于机器人视频轨迹，通过重组学习到的动作组件生成了多样化的序列，显著增加了LIBERO基准测试中探索的状态空间覆盖范围。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "28 pages, 16 figures, 4 tables",
      "pdf_url": "https://arxiv.org/pdf/2601.22057v1",
      "published_date": "2026-01-29 17:57:06 UTC",
      "updated_date": "2026-01-29 17:57:06 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-30T04:19:45.834715+00:00"
    },
    {
      "arxiv_id": "2601.22054v1",
      "title": "MetricAnything: Scaling Metric Depth Pretraining with Noisy Heterogeneous Sources",
      "title_zh": "MetricAnything：利用噪声异构数据实现度量深度预训练的规模化扩展",
      "authors": [
        "Baorui Ma",
        "Jiahui Yang",
        "Donglin Di",
        "Xuancheng Zhang",
        "Jianxun Cui",
        "Hao Li",
        "Yan Xie",
        "Wei Chen"
      ],
      "abstract": "Scaling has powered recent advances in vision foundation models, yet extending this paradigm to metric depth estimation remains challenging due to heterogeneous sensor noise, camera-dependent biases, and metric ambiguity in noisy cross-source 3D data. We introduce Metric Anything, a simple and scalable pretraining framework that learns metric depth from noisy, diverse 3D sources without manually engineered prompts, camera-specific modeling, or task-specific architectures. Central to our approach is the Sparse Metric Prompt, created by randomly masking depth maps, which serves as a universal interface that decouples spatial reasoning from sensor and camera biases. Using about 20M image-depth pairs spanning reconstructed, captured, and rendered 3D data across 10000 camera models, we demonstrate-for the first time-a clear scaling trend in the metric depth track. The pretrained model excels at prompt-driven tasks such as depth completion, super-resolution and Radar-camera fusion, while its distilled prompt-free student achieves state-of-the-art results on monocular depth estimation, camera intrinsics recovery, single/multi-view metric 3D reconstruction, and VLA planning. We also show that using pretrained ViT of Metric Anything as a visual encoder significantly boosts Multimodal Large Language Model capabilities in spatial intelligence. These results show that metric depth estimation can benefit from the same scaling laws that drive modern foundation models, establishing a new path toward scalable and efficient real-world metric perception. We open-source MetricAnything at http://metric-anything.github.io/metric-anything-io/ to support community research.",
      "tldr_zh": "该研究提出了 MetricAnything，一个旨在通过大规模嘈杂且异构的数据源学习 Metric depth estimation 的预训练框架。为了克服传感器噪声和相机偏差等挑战，该方法引入了 Sparse Metric Prompt 作为通用接口，通过随机掩盖深度图来解耦空间推理与特定硬件的偏好。研究利用约 2000 万对涵盖 10000 种相机模型的图像深度数据进行训练，首次在度量深度领域证明了 Scaling trend 的有效性。预训练后的模型不仅在深度补全和 Radar-camera fusion 等任务中表现卓越，其蒸馏出的 prompt-free 学生模型还在单目深度估计和 VLA planning 等任务上取得了 State-of-the-art 的结果。此外，将 MetricAnything 的 ViT 编码器集成到多模态大语言模型中，显著提升了其空间智能水平。该成果展示了度量深度估计同样遵循现代基础模型的缩放定律，为可扩展的现实世界度量感知确立了新的技术路径。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "Project Page: https://metric-anything.github.io/metric-anything-io/",
      "pdf_url": "https://arxiv.org/pdf/2601.22054v1",
      "published_date": "2026-01-29 17:52:41 UTC",
      "updated_date": "2026-01-29 17:52:41 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-30T04:19:53.111829+00:00"
    },
    {
      "arxiv_id": "2601.22044v1",
      "title": "SIA: Symbolic Interpretability for Anticipatory Deep Reinforcement Learning in Network Control",
      "title_zh": "SIA：网络控制中前瞻性深度强化学习的符号化可解释性",
      "authors": [
        "MohammadErfan Jabbari",
        "Abhishek Duttagupta",
        "Claudio Fiandrino",
        "Leonardo Bonati",
        "Salvatore D'Oro",
        "Michele Polese",
        "Marco Fiore",
        "Tommaso Melodia"
      ],
      "abstract": "Deep reinforcement learning (DRL) promises adaptive control for future mobile networks but conventional agents remain reactive: they act on past and current measurements and cannot leverage short-term forecasts of exogenous KPIs such as bandwidth. Augmenting agents with predictions can overcome this temporal myopia, yet uptake in networking is scarce because forecast-aware agents act as closed-boxes; operators cannot tell whether predictions guide decisions or justify the added complexity. We propose SIA, the first interpreter that exposes in real time how forecast-augmented DRL agents operate. SIA fuses Symbolic AI abstractions with per-KPI Knowledge Graphs to produce explanations, and includes a new Influence Score metric. SIA achieves sub-millisecond speed, over 200x faster than existing XAI methods. We evaluate SIA on three diverse networking use cases, uncovering hidden issues, including temporal misalignment in forecast integration and reward-design biases that trigger counter-productive policies. These insights enable targeted fixes: a redesigned agent achieves a 9% higher average bitrate in video streaming, and SIA's online Action-Refinement module improves RAN-slicing reward by 25% without retraining. By making anticipatory DRL transparent and tunable, SIA lowers the barrier to proactive control in next-generation mobile networks.",
      "tldr_zh": "该研究提出了SIA，这是首个针对网络控制中前瞻性Deep Reinforcement Learning (DRL)代理的实时解释器，旨在解决预测增强型代理因“黑盒”特性导致运营商难以信任的问题。SIA通过融合Symbolic AI抽象与每项KPI的Knowledge Graphs来生成解释，并引入了全新的Influence Score评估指标。该解释器的运行速度达到亚毫秒级，比现有的XAI方法快200倍以上，能够有效满足网络实时监控需求。通过对三种不同网络用例的评估，SIA成功揭示了预测集成中的时间错位和奖励设计偏差等隐性策略问题。基于这些发现，研究团队优化了代理设计，使视频流媒体的平均码率提升了9%，且其在线Action-Refinement模块在无需重新训练的情况下将RAN-slicing的奖励提升了25%，显著增强了下一代移动网络主动控制的透明度与可调优性。",
      "categories": [
        "cs.NI",
        "cs.AI"
      ],
      "primary_category": "cs.NI",
      "comment": "10 pages, 12 figures, accepted at IEEE INFOCOM 2026",
      "pdf_url": "https://arxiv.org/pdf/2601.22044v1",
      "published_date": "2026-01-29 17:46:46 UTC",
      "updated_date": "2026-01-29 17:46:46 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-30T04:19:57.728358+00:00"
    },
    {
      "arxiv_id": "2601.22041v1",
      "title": "Learning to Communicate Across Modalities: Perceptual Heterogeneity in Multi-Agent Systems",
      "title_zh": "学习跨模态通信：多智能体系统中的感知异质性",
      "authors": [
        "Naomi Pitzer",
        "Daniela Mihai"
      ],
      "abstract": "Emergent communication offers insight into how agents develop shared structured representations, yet most research assumes homogeneous modalities or aligned representational spaces, overlooking the perceptual heterogeneity of real-world settings. We study a heterogeneous multi-step binary communication game where agents differ in modality and lack perceptual grounding. Despite perceptual misalignment, multimodal systems converge to class-consistent messages grounded in perceptual input. Unimodal systems communicate more efficiently, using fewer bits and achieving lower classification entropy, while multimodal agents require greater information exchange and exhibit higher uncertainty. Bit perturbation experiments provide strong evidence that meaning is encoded in a distributional rather than compositional manner, as each bit's contribution depends on its surrounding pattern. Finally, interoperability analyses show that systems trained in different perceptual worlds fail to directly communicate, but limited fine-tuning enables successful cross-system communication. This work positions emergent communication as a framework for studying how agents adapt and transfer representations across heterogeneous modalities, opening new directions for both theory and experimentation.",
      "tldr_zh": "该研究探讨了在感知异构(Perceptual Heterogeneity)的现实背景下，多智能体系统如何在模态不同且缺乏感知基准(Perceptual Grounding)的情况下学习沟通。研究采用了一种异构多步二进制通信博弈，发现尽管存在感知失调，多模态系统仍能收敛于基于感知输入的类别一致性消息。实验对比显示，单模态系统通信效率更高且不确定性更低，而多模态智能体则需要更频繁的信息交换。通过比特扰动(Bit Perturbation)实验，研究证明了其意义是以分布(Distributional)而非组合(Compositional)的方式编码的，即每个比特的贡献取决于其周围的模式。互操作性分析表明，在不同感知环境下训练的系统虽无法直接通信，但通过有限的微调即可实现成功的跨系统交流。这项工作为研究智能体如何跨异构模态适应和迁移表征提供了新的理论与实验框架。",
      "categories": [
        "cs.MA",
        "cs.AI",
        "cs.CV",
        "cs.LG"
      ],
      "primary_category": "cs.MA",
      "comment": "To be published in EvoLang XVI proceedings. 15 pages, 17 figures",
      "pdf_url": "https://arxiv.org/pdf/2601.22041v1",
      "published_date": "2026-01-29 17:45:41 UTC",
      "updated_date": "2026-01-29 17:45:41 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-30T04:19:55.890854+00:00"
    },
    {
      "arxiv_id": "2601.22040v1",
      "title": "A Separable Architecture for Continuous Token Representation in Language Models",
      "title_zh": "语言模型中连续 Token 表征的可分离架构",
      "authors": [
        "Reza T. Batley",
        "Sourav Saha"
      ],
      "abstract": "Transformer scaling law analyses typically treat parameters as interchangeable; an abstraction that accurately predicts loss-compute relationships. Yet, in sub-billion-parameter small language models (SLMs), embedding matrices dominate the parameter budget. This work argues that this allocation is as suboptimal as it is counterintuitive. Leviathan is an architecture with a continuous embedding generator to replace the discrete lookup tables of canonical models. Evaluating on the Pile dataset under isoparametric settings, Leviathan consistently outperforms a standard, LLaMA-style architecture. By means of an empirical power-law fit, Leviathan exhibits a markedly superior effective parameter capacity. Across the regime studied, Leviathan behaves as a dense model with $1.47$ to $2.11 \\times$ more parameters.",
      "tldr_zh": "该研究针对亚十亿参数规模的小语言模型(Small Language Models, SLMs)中嵌入矩阵(embedding matrices)占据过多参数比例的问题，提出了一种名为Leviathan的新型架构。Leviathan采用了连续嵌入生成器(continuous embedding generator)来取代传统模型中的离散查找表(discrete lookup tables)，旨在通过连续Token表示优化参数分配。研究人员在等参数设置(isoparametric settings)下利用Pile数据集进行了评估，结果显示Leviathan在性能上一致优于标准的LLaMA-style架构。通过经验幂律拟合(empirical power-law fit)分析，Leviathan展现出了显著优越的有效参数容量(effective parameter capacity)。在所研究的参数范围内，Leviathan的表现相当于比其自身参数量多出1.47至2.11倍的稠密模型。这一研究证明了改变嵌入层的参数分配方式能有效提升小规模语言模型的建模效率。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2601.22040v1",
      "published_date": "2026-01-29 17:44:25 UTC",
      "updated_date": "2026-01-29 17:44:25 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-30T04:19:57.983081+00:00"
    },
    {
      "arxiv_id": "2601.22037v1",
      "title": "Optimizing Agentic Workflows using Meta-tools",
      "title_zh": "基于元工具的智能体工作流优化",
      "authors": [
        "Sami Abuzakuk",
        "Anne-Marie Kermarrec",
        "Rishi Sharma",
        "Rasmus Moorits Veski",
        "Martijn de Vos"
      ],
      "abstract": "Agentic AI enables LLM to dynamically reason, plan, and interact with tools to solve complex tasks. However, agentic workflows often require many iterative reasoning steps and tool invocations, leading to significant operational expense, end-to-end latency and failures due to hallucinations. This work introduces Agent Workflow Optimization (AWO), a framework that identifies and optimizes redundant tool execution patterns to improve the efficiency and robustness of agentic workflows. AWO analyzes existing workflow traces to discover recurring sequences of tool calls and transforms them into meta-tools, which are deterministic, composite tools that bundle multiple agent actions into a single invocation. Meta-tools bypass unnecessary intermediate LLM reasoning steps and reduce operational cost while also shortening execution paths, leading to fewer failures. Experiments on two agentic AI benchmarks show that AWO reduces the number of LLM calls up to 11.9% while also increasing the task success rate by up to 4.2 percent points.",
      "tldr_zh": "该研究提出了Agent Workflow Optimization (AWO)框架，旨在解决智能体(Agentic AI)工作流因大量迭代推理和工具调用而产生的高成本、高延迟及幻觉失败问题。AWO通过分析工作流轨迹(workflow traces)识别重复的工具调用序列，并将其转化为确定性的复合元工具(meta-tools)，从而将多个动作捆绑为单次调用。这种方法能够绕过不必要的中间大语言模型(LLM)推理步骤，在降低操作成本的同时缩短了执行路径，有效减少了运行故障。在两个智能体AI基准测试上的实验表明，AWO能将LLM调用次数减少多达11.9%，并使任务成功率提升至多4.2个百分点。通过优化冗余执行模式，该框架为构建更高效、更鲁棒的自动化智能体系统提供了新的路径。",
      "categories": [
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2601.22037v1",
      "published_date": "2026-01-29 17:43:08 UTC",
      "updated_date": "2026-01-29 17:43:08 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-30T04:20:09.499052+00:00"
    },
    {
      "arxiv_id": "2601.22035v1",
      "title": "Thinking Out of Order: When Output Order Stops Reflecting Reasoning Order in Diffusion Language Models",
      "title_zh": "乱序思考：扩散语言模型中输出顺序与推理顺序的解耦",
      "authors": [
        "Longxuan Yu",
        "Yu Fu",
        "Shaorong Zhang",
        "Hui Liu",
        "Mukund Varma T",
        "Greg Ver Steeg",
        "Yue Dong"
      ],
      "abstract": "Autoregressive (AR) language models enforce a fixed left-to-right generation order, creating a fundamental limitation when the required output structure conflicts with natural reasoning (e.g., producing answers before explanations due to presentation or schema constraints). In such cases, AR models must commit to answers before generating intermediate reasoning, and this rigid constraint forces premature commitment. Masked diffusion language models (MDLMs), which iteratively refine all tokens in parallel, offer a way to decouple computation order from output structure. We validate this capability on GSM8K, Math500, and ReasonOrderQA, a benchmark we introduce with controlled difficulty and order-level evaluation. When prompts request answers before reasoning, AR models exhibit large accuracy gaps compared to standard chain-of-thought ordering (up to 67% relative drop), while MDLMs remain stable ($\\leq$14% relative drop), a property we term \"order robustness\". Using ReasonOrderQA, we present evidence that MDLMs achieve order robustness by stabilizing simpler tokens (e.g., reasoning steps) earlier in the diffusion process than complex ones (e.g., final answers), enabling reasoning tokens to stabilize before answer commitment. Finally, we identify failure conditions where this advantage weakens, outlining the limits required for order robustness.",
      "tldr_zh": "该研究探讨了自回归语言模型(Autoregressive language models)在固定从左到右生成顺序下的局限性，特别是在输出结构与自然推理顺序相悖时（如要求先输出答案后输出解释）会导致性能显著下降。为解决这一瓶颈，研究利用掩码扩散语言模型(Masked diffusion language models)并行优化token的特性，实现了计算顺序与输出结构的解耦。在GSM8K、Math500及新提出的ReasonOrderQA基准测试中，当提示词要求先给出答案再进行推理时，自回归模型准确率下降高达67%，而扩散模型则展现出显著的顺序鲁棒性(order robustness)，准确率下降控制在14%以内。研究揭示了扩散模型通过在扩散早期稳定推理相关token而在晚期确定答案的机制，从而有效避免了提前承诺(premature commitment)问题。最后，该工作还识别了这种优势减弱的失效场景，进一步界定了扩散模型在处理复杂推理任务时的边界。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "18 pages, 13 figures, 5 tables",
      "pdf_url": "https://arxiv.org/pdf/2601.22035v1",
      "published_date": "2026-01-29 17:40:58 UTC",
      "updated_date": "2026-01-29 17:40:58 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-30T04:20:14.760841+00:00"
    },
    {
      "arxiv_id": "2601.22027v1",
      "title": "CAR-bench: Evaluating the Consistency and Limit-Awareness of LLM Agents under Real-World Uncertainty",
      "title_zh": "CAR-bench：现实世界不确定性下大语言模型智能体的一致性与边界意识评估",
      "authors": [
        "Johannes Kirmayr",
        "Lukas Stappen",
        "Elisabeth André"
      ],
      "abstract": "Existing benchmarks for Large Language Model (LLM) agents focus on task completion under idealistic settings but overlook reliability in real-world, user-facing applications. In domains, such as in-car voice assistants, users often issue incomplete or ambiguous requests, creating intrinsic uncertainty that agents must manage through dialogue, tool use, and policy adherence. We introduce CAR-bench, a benchmark for evaluating consistency, uncertainty handling, and capability awareness in multi-turn, tool-using LLM agents in an in-car assistant domain. The environment features an LLM-simulated user, domain policies, and 58 interconnected tools spanning navigation, productivity, charging, and vehicle control. Beyond standard task completion, CAR-bench introduces Hallucination tasks that test agents' limit-awareness under missing tools or information, and Disambiguation tasks that require resolving uncertainty through clarification or internal information gathering. Baseline results reveal large gaps between occasional and consistent success on all task types. Even frontier reasoning LLMs achieve less than 50% consistent pass rate on Disambiguation tasks due to premature actions, and frequently violate policies or fabricate information to satisfy user requests in Hallucination tasks, underscoring the need for more reliable and self-aware LLM agents in real-world settings.",
      "tldr_zh": "该研究推出了 CAR-bench，这是一个专门用于评估大语言模型(LLM)智能体在现实世界不确定性下的一致性(Consistency)和极限意识(Limit-Awareness)的新型基准测试。与关注理想化任务完成的现有基准不同，CAR-bench 聚焦于车载语音助手等真实应用场景中的不确定性处理。该测试环境包含模拟用户、领域策略以及涵盖导航、车辆控制等领域的 58 个互联工具，重点考察智能体在多轮对话和工具调用中的表现。其核心评估任务包括测试智能体在工具或信息缺失时极限意识的 Hallucination 任务，以及需要通过澄清或信息收集来解决不确定性的 Disambiguation 任务。实验结果显示，即使是前沿推理模型在 Disambiguation 任务中的一致通过率也低于 50%，且在 Hallucination 任务中经常违反策略或虚构信息。该基准揭示了当前模型在可靠性与自我意识方面的巨大差距，为构建更具鲁棒性的 LLM 智能体提供了重要参考。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2601.22027v1",
      "published_date": "2026-01-29 17:33:42 UTC",
      "updated_date": "2026-01-29 17:33:42 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-30T04:20:13.294249+00:00"
    },
    {
      "arxiv_id": "2601.22025v1",
      "title": "When \"Better\" Prompts Hurt: Evaluation-Driven Iteration for LLM Applications",
      "title_zh": "当“更优”提示词适得其反：面向大语言模型应用的评估驱动型迭代",
      "authors": [
        "Daniel Commey"
      ],
      "abstract": "Evaluating Large Language Model (LLM) applications differs from traditional software testing because outputs are stochastic, high-dimensional, and sensitive to prompt and model changes. We present an evaluation-driven workflow - Define, Test, Diagnose, Fix - that turns these challenges into a repeatable engineering loop.\n  We introduce the Minimum Viable Evaluation Suite (MVES), a tiered set of recommended evaluation components for (i) general LLM applications, (ii) retrieval-augmented generation (RAG), and (iii) agentic tool-use workflows. We also synthesize common evaluation methods (automated checks, human rubrics, and LLM-as-judge) and discuss known judge failure modes.\n  In reproducible local experiments (Ollama; Llama 3 8B Instruct and Qwen 2.5 7B Instruct), we observe that a generic \"improved\" prompt template can trade off behaviors: on our small structured suites, extraction pass rate decreased from 100% to 90% and RAG compliance from 93.3% to 80% for Llama 3 when replacing task-specific prompts with generic rules, while instruction-following improved. These findings motivate evaluation-driven prompt iteration and careful claim calibration rather than universal prompt recipes.\n  All test suites, harnesses, and results are included for reproducibility.",
      "tldr_zh": "该研究针对Large Language Model (LLM)应用在输出随机性、高维性和对提示词敏感性方面的评估挑战，提出了由Define, Test, Diagnose, Fix构成的评估驱动工作流。论文引入了Minimum Viable Evaluation Suite (MVES)，为通用LLM应用、retrieval-augmented generation (RAG)以及agentic tool-use工作流提供了一套分层推荐的评估组件。研究综合了automated checks、human rubrics和LLM-as-judge等常见评估方法，并深入探讨了LLM裁判的失败模式。在基于Ollama的本地实验中，研究者观察到通用的“改进版”提示词会导致性能权衡：在Llama 3模型上，将特定任务提示替换为通用规则后，虽然instruction-following有所提升，但extraction pass rate从100%降至90%，RAG compliance从93.3%降至80%。这些发现有力地证明了评估驱动的提示词迭代和谨慎的结论校准比使用通用提示词模板更为重要。所有测试套件、框架及结果均已开源以确保实验的可复现性。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.IR",
        "cs.SE"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2601.22025v1",
      "published_date": "2026-01-29 17:32:34 UTC",
      "updated_date": "2026-01-29 17:32:34 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-30T04:20:17.533278+00:00"
    },
    {
      "arxiv_id": "2601.22024v1",
      "title": "SymbXRL: Symbolic Explainable Deep Reinforcement Learning for Mobile Networks",
      "title_zh": "SymbXRL：面向移动网络的符号化可解释深度强化学习",
      "authors": [
        "Abhishek Duttagupta",
        "MohammadErfan Jabbari",
        "Claudio Fiandrino",
        "Marco Fiore",
        "Joerg Widmer"
      ],
      "abstract": "The operation of future 6th-generation (6G) mobile networks will increasingly rely on the ability of deep reinforcement learning (DRL) to optimize network decisions in real-time. DRL yields demonstrated efficacy in various resource allocation problems, such as joint decisions on user scheduling and antenna allocation or simultaneous control of computing resources and modulation. However, trained DRL agents are closed-boxes and inherently difficult to explain, which hinders their adoption in production settings. In this paper, we make a step towards removing this critical barrier by presenting SymbXRL, a novel technique for explainable reinforcement learning (XRL) that synthesizes human-interpretable explanations for DRL agents. SymbXRL leverages symbolic AI to produce explanations where key concepts and their relationships are described via intuitive symbols and rules; coupling such a representation with logical reasoning exposes the decision process of DRL agents and offers more comprehensible descriptions of their behaviors compared to existing approaches. We validate SymbXRL in practical network management use cases supported by DRL, proving that it not only improves the semantics of the explanations but also paves the way for explicit agent control: for instance, it enables intent-based programmatic action steering that improves by 12% the median cumulative reward over a pure DRL solution.",
      "tldr_zh": "该研究针对 6th-generation (6G) 移动网络中 Deep Reinforcement Learning (DRL) 决策过程不透明、难以解释的问题，提出了 SymbXRL 框架。这是一种新型的 Explainable Reinforcement Learning (XRL) 技术，利用 Symbolic AI 将 DRL 智能体的行为转化为由直观符号和规则描述的人类可理解解释。通过将符号表示与逻辑推理相结合，SymbXRL 能够清晰揭示 DRL 的决策过程，并提供比现有方法更具可读性的行为描述。在实际网络管理场景中的验证表明，该技术不仅提升了解释的语义质量，还支持 Intent-based programmatic action steering，使中位累积奖励相比纯 DRL 方案提升了 12%。这一研究有效地解决了 DRL 在生产环境中部署的可信任难题，为实现可解释且可控的自主网络管理奠定了基础。",
      "categories": [
        "cs.NI",
        "cs.AI"
      ],
      "primary_category": "cs.NI",
      "comment": "10 pages, 9 figures, published in IEEE INFOCOM 2025",
      "pdf_url": "https://arxiv.org/pdf/2601.22024v1",
      "published_date": "2026-01-29 17:31:40 UTC",
      "updated_date": "2026-01-29 17:31:40 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-30T04:20:20.896268+00:00"
    },
    {
      "arxiv_id": "2601.22013v1",
      "title": "Vidmento: Creating Video Stories Through Context-Aware Expansion With Generative Video",
      "title_zh": "Vidmento：通过生成式视频的上下文感知扩展创作视频故事",
      "authors": [
        "Catherine Yeh",
        "Anh Truong",
        "Mira Dontcheva",
        "Bryan Wang"
      ],
      "abstract": "Video storytelling is often constrained by available material, limiting creative expression and leaving undesired narrative gaps. Generative video offers a new way to address these limitations by augmenting captured media with tailored visuals. To explore this potential, we interviewed eight video creators to identify opportunities and challenges in integrating generative video into their workflows. Building on these insights and established filmmaking principles, we developed Vidmento, a tool for authoring hybrid video stories that combine captured and generated media through context-aware expansion. Vidmento surfaces opportunities for story development, generates clips that blend stylistically and narratively with surrounding media, and provides controls for refinement. In a study with 12 creators, Vidmento supported narrative development and exploration by systematically expanding initial materials with generative media, enabling expressive video storytelling aligned with creative intent. We highlight how creators bridge story gaps with generative content and where they find this blending capability most valuable.",
      "tldr_zh": "该研究针对视频叙事中因素材受限导致的创作表达受阻和叙事断层问题，提出了名为 Vidmento 的新型视频创作工具。通过对8位视频创作者的深入访谈并结合经典电影制作原则，Vidmento 利用上下文感知扩展 (context-aware expansion) 技术，实现了拍摄素材与生成式视频 (Generative Video) 的混合创作。该工具能够自动识别故事发展的潜在契机，生成在风格和叙事上与周边媒体保持一致的片段，并为用户提供精细化的调整控制。在针对12位创作者的用户研究中，Vidmento 证明了其通过生成式媒体 (generative media) 系统性扩展原始材料的能力，能有效支持叙事开发并使作品符合创作者的创意意图。研究进一步揭示了创作者如何利用生成内容填补叙事空白，并阐明了这种混合扩展能力在视频创作流中的核心价值。",
      "categories": [
        "cs.HC",
        "cs.AI",
        "cs.MM"
      ],
      "primary_category": "cs.HC",
      "comment": "25 pages, 18 figures",
      "pdf_url": "https://arxiv.org/pdf/2601.22013v1",
      "published_date": "2026-01-29 17:19:06 UTC",
      "updated_date": "2026-01-29 17:19:06 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-30T04:20:24.244064+00:00"
    },
    {
      "arxiv_id": "2601.22009v1",
      "title": "MEIDNet: Multimodal generative AI framework for inverse materials design",
      "title_zh": "MEIDNet：面向材料逆向设计的多模态生成式人工智能框架",
      "authors": [
        "Anand Babu",
        "Rogério Almeida Gouvêa",
        "Pierre Vandergheynst",
        "Gian-Marco Rignanese"
      ],
      "abstract": "In this work, we present Multimodal Equivariant Inverse Design Network (MEIDNet), a framework that jointly learns structural information and materials properties through contrastive learning, while encoding structures via an equivariant graph neural network (EGNN). By combining generative inverse design with multimodal learning, our approach accelerates the exploration of chemical-structural space and facilitates the discovery of materials that satisfy predefined property targets. MEIDNet exhibits strong latent-space alignment with cosine similarity 0.96 by fusion of three modalities through cross-modal learning. Through implementation of curriculum learning strategies, MEIDNet achieves ~60 times higher learning efficiency than conventional training techniques. The potential of our multimodal approach is demonstrated by generating low-bandgap perovskite structures at a stable, unique, and novel (SUN) rate of 13.6 %, which are further validated by ab initio methods. Our inverse design framework demonstrates both scalability and adaptability, paving the way for the universal learning of chemical space across diverse modalities.",
      "tldr_zh": "该研究提出了Multimodal Equivariant Inverse Design Network (MEIDNet)，这是一个旨在通过对比学习(contrastive learning)同时学习结构信息和材料属性的多模态生成式AI框架。MEIDNet利用等变图神经网络(EGNN)对结构进行编码，并通过跨模态学习实现了三种模态的高效融合，其潜空间对齐的余弦相似度达到0.96。通过引入课程学习(curriculum learning)策略，该框架的学习效率比传统训练技术提高了约60倍。在实验验证中，MEIDNet成功生成了低带隙的钙钛矿(perovskite)结构，其稳定、独特且新颖(SUN)的生成率达到13.6%，并经过了从头算(ab initio)方法的进一步验证。该框架展示了出色的可扩展性和适应性，为实现化学空间的通用学习路径提供了有效工具。",
      "categories": [
        "cond-mat.mtrl-sci",
        "cs.AI",
        "cs.LG",
        "physics.comp-ph"
      ],
      "primary_category": "cond-mat.mtrl-sci",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2601.22009v1",
      "published_date": "2026-01-29 17:16:35 UTC",
      "updated_date": "2026-01-29 17:16:35 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-30T04:20:25.960840+00:00"
    },
    {
      "arxiv_id": "2601.22001v1",
      "title": "Heterogeneous Computing: The Key to Powering the Future of AI Agent Inference",
      "title_zh": "异构计算：驱动未来 AI 智能体推理的关键",
      "authors": [
        "Yiren Zhao",
        "Junyi Liu"
      ],
      "abstract": "AI agent inference is driving an inference heavy datacenter future and exposes bottlenecks beyond compute - especially memory capacity, memory bandwidth and high-speed interconnect. We introduce two metrics - Operational Intensity (OI) and Capacity Footprint (CF) - that jointly explain regimes the classic roofline analysis misses, including the memory capacity wall. Across agentic workflows (chat, coding, web use, computer use) and base model choices (GQA/MLA, MoE, quantization), OI/CF can shift dramatically, with long context KV cache making decode highly memory bound. These observations motivate disaggregated serving and system level heterogeneity: specialized prefill and decode accelerators, broader scale up networking, and decoupled compute-memory enabled by optical I/O. We further hypothesize agent-hardware co design, multiple inference accelerators within one system, and high bandwidth, large capacity memory disaggregation as foundations for adaptation to evolving OI/CF. Together, these directions chart a path to sustain efficiency and capability for large scale agentic AI inference.",
      "tldr_zh": "该研究探讨了AI智能体(AI agent)推理对数据中心带来的严峻挑战，指出内存容量(memory capacity)、内存带宽(memory bandwidth)和高速互连(high-speed interconnect)正成为超越算力的核心瓶颈。为了量化这些挑战，作者提出了操作强度(Operational Intensity, OI)和容量占用(Capacity Footprint, CF)两个新指标，能够有效解释传统屋顶线分析(roofline analysis)无法涵盖的内存容量墙(memory capacity wall)等问题。通过对多种智能体工作流和模型架构的分析，研究发现长上下文(long context)的KV缓存使得解码过程呈现极高的内存受限特性。基于这些观察，论文倡导系统级的异构计算架构，包括预填充(prefill)与解码(decode)加速器的分离、光互连(optical I/O)驱动的计算存储解耦以及内存池化技术。研究最后强调，智能体-硬件协同设计(agent-hardware co-design)将是持续提升大规模智能体AI推理效率与能力的关键路径。",
      "categories": [
        "cs.AI",
        "cs.AR",
        "cs.DC"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2601.22001v1",
      "published_date": "2026-01-29 17:11:46 UTC",
      "updated_date": "2026-01-29 17:11:46 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-30T04:20:27.978444+00:00"
    },
    {
      "arxiv_id": "2601.21996v1",
      "title": "Mechanistic Data Attribution: Tracing the Training Origins of Interpretable LLM Units",
      "title_zh": "机制性数据归因：大语言模型可解释单元的训练溯源",
      "authors": [
        "Jianhui Chen",
        "Yuzhang Luo",
        "Liangming Pan"
      ],
      "abstract": "While Mechanistic Interpretability has identified interpretable circuits in LLMs, their causal origins in training data remain elusive. We introduce Mechanistic Data Attribution (MDA), a scalable framework that employs Influence Functions to trace interpretable units back to specific training samples. Through extensive experiments on the Pythia family, we causally validate that targeted intervention--removing or augmenting a small fraction of high-influence samples--significantly modulates the emergence of interpretable heads, whereas random interventions show no effect. Our analysis reveals that repetitive structural data (e.g., LaTeX, XML) acts as a mechanistic catalyst. Furthermore, we observe that interventions targeting induction head formation induce a concurrent change in the model's in-context learning (ICL) capability. This provides direct causal evidence for the long-standing hypothesis regarding the functional link between induction heads and ICL. Finally, we propose a mechanistic data augmentation pipeline that consistently accelerates circuit convergence across model scales, providing a principled methodology for steering the developmental trajectories of LLMs.",
      "tldr_zh": "该研究提出了Mechanistic Data Attribution (MDA)，这是一个利用Influence Functions将大语言模型(LLMs)中的可解释单元追溯到特定训练样本的可扩展框架。通过在Pythia模型家族上的实验，研究人员证明了针对性干预少量高影响样本能显著调节可解释头的出现，从而在因果层面验证了训练数据与模型内部机制的联系。分析发现，LaTeX和XML等重复性结构化数据在模型机制形成中起到了催化剂作用。此外，针对induction heads形成的干预同步改变了模型的In-Context Learning (ICL)能力，为两者之间的功能联系提供了直接的因果证据。最后，研究提出了一种机制化数据增强流水线，能够跨模型规模加速电路收敛，为引导LLMs的发展轨迹提供了原则性方法。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2601.21996v1",
      "published_date": "2026-01-29 17:06:54 UTC",
      "updated_date": "2026-01-29 17:06:54 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-30T04:20:28.196514+00:00"
    },
    {
      "arxiv_id": "2601.21993v1",
      "title": "Liquid Interfaces: A Dynamic Ontology for the Interoperability of Autonomous Systems",
      "title_zh": "液态接口：面向自主系统互操作性的动态本体",
      "authors": [
        "Dhiogo de Sá",
        "Carlos Schmiedel",
        "Carlos Pereira Lopes"
      ],
      "abstract": "Contemporary software architectures struggle to support autonomous agents whose reasoning is adaptive, probabilistic, and context-dependent, while system integration remains dominated by static interfaces and deterministic contracts. This paper introduces Liquid Interfaces, a coordination paradigm in which interfaces are not persistent technical artifacts, but ephemeral relational events that emerge through intention articulation and semantic negotiation at runtime.We formalize this model and present the Liquid Interface Protocol (LIP),which governs intention-driven interaction, negotiated execution, and enforce ephemerality under semantic uncertainty. We further discuss the governance implications of this approach and describe a reference architecture that demonstrates practical feasibility. Liquid Interfaces provide a principled foundation for adaptive coordination in agent-based systems",
      "tldr_zh": "该研究针对当代软件架构中静态接口(static interfaces)与自主智能体(autonomous agents)自适应、概率性需求之间的矛盾，提出了一种名为 Liquid Interfaces 的新型协作范式。在这一范式中，接口不再是持久的技术实体，而是在运行时通过意图表达(intention articulation)和语义协商(semantic negotiation)产生的瞬时关系事件。论文正式定义了该模型并提出了 Liquid Interface Protocol (LIP)，该协议负责管理意图驱动的交互、协商执行以及在语义不确定性下的瞬时性强制。此外，作者探讨了该方法在治理方面的意义，并提供了一个验证其实际可行性的参考架构。Liquid Interfaces 为智能体系统(agent-based systems)中的自适应协作提供了原则性基础，有效解决了自主系统间的互操作性难题。",
      "categories": [
        "cs.AI",
        "cs.SE"
      ],
      "primary_category": "cs.AI",
      "comment": "28 pages, 2 figures",
      "pdf_url": "https://arxiv.org/pdf/2601.21993v1",
      "published_date": "2026-01-29 17:04:13 UTC",
      "updated_date": "2026-01-29 17:04:13 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-30T04:20:32.770330+00:00"
    },
    {
      "arxiv_id": "2601.21991v1",
      "title": "Geometry of Drifting MDPs with Path-Integral Stability Certificates",
      "title_zh": "漂移 MDP 的几何学及其路径积分稳定性证明",
      "authors": [
        "Zuyuan Zhang",
        "Mahdi Imani",
        "Tian Lan"
      ],
      "abstract": "Real-world reinforcement learning is often \\emph{nonstationary}: rewards and dynamics drift, accelerate, oscillate, and trigger abrupt switches in the optimal action. Existing theory often represents nonstationarity with coarse-scale models that measure \\emph{how much} the environment changes, not \\emph{how} it changes locally -- even though acceleration and near-ties drive tracking error and policy chattering. We take a geometric view of nonstationary discounted Markov Decision Processes (MDPs) by modeling the environment as a differentiable homotopy path and tracking the induced motion of the optimal Bellman fixed point. This yields a length--curvature--kink signature of intrinsic complexity: cumulative drift, acceleration/oscillation, and action-gap-induced nonsmoothness. We prove a solver-agnostic path-integral stability bound and derive gap-safe feasible regions that certify local stability away from switch regimes. Building on these results, we introduce \\textit{Homotopy-Tracking RL (HT-RL)} and \\textit{HT-MCTS}, lightweight wrappers that estimate replay-based proxies of length, curvature, and near-tie proximity online and adapt learning or planning intensity accordingly. Experiments show improved tracking and dynamic regret over matched static baselines, with the largest gains in oscillatory and switch-prone regimes.",
      "tldr_zh": "该研究从几何视角探讨了非平稳(nonstationary)马尔可夫决策过程(MDPs)，旨在解决由于奖励和动力学漂移导致的跟踪误差和策略震荡问题。论文将环境建模为可微同伦路径(differentiable homotopy path)，并通过追踪最优贝尔曼固定点(optimal Bellman fixed point)的诱导运动来刻画系统的内在复杂性。研究提出了由长度(length)、曲率(curvature)和拐点(kink)构成的特征签名，分别对应累积漂移、加速度/振荡以及由动作间隙(action-gap)引发的非光滑性。作者证明了一个与求解器无关的路径积分稳定界(path-integral stability bound)，并推导出能保证局部稳定性的间隙安全可行区域(gap-safe feasible regions)。基于这些理论，研究引入了轻量级包装器Homotopy-Tracking RL (HT-RL)和HT-MCTS，能够在线估计环境变化参数并据此调整学习或规划强度。实验结果表明，该方法在振荡和易发生切换的场景中表现优异，显著提升了跟踪性能并有效降低了动态遗憾(dynamic regret)。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2601.21991v1",
      "published_date": "2026-01-29 17:03:23 UTC",
      "updated_date": "2026-01-29 17:03:23 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-30T04:20:45.531859+00:00"
    },
    {
      "arxiv_id": "2601.21988v1",
      "title": "Generalized Information Gathering Under Dynamics Uncertainty",
      "title_zh": "动力学不确定性下的广义信息获取",
      "authors": [
        "Fernando Palafox",
        "Jingqi Li",
        "Jesse Milzman",
        "David Fridovich-Keil"
      ],
      "abstract": "An agent operating in an unknown dynamical system must learn its dynamics from observations. Active information gathering accelerates this learning, but existing methods derive bespoke costs for specific modeling choices: dynamics models, belief update procedures, observation models, and planners. We present a unifying framework that decouples these choices from the information-gathering cost by explicitly exposing the causal dependencies between parameters, beliefs, and controls. Using this framework, we derive a general information-gathering cost based on Massey's directed information that assumes only Markov dynamics with additive noise and is otherwise agnostic to modeling choices. We prove that the mutual information cost used in existing literature is a special case of our cost. Then, we leverage our framework to establish an explicit connection between the mutual information cost and information gain in linearized Bayesian estimation, thereby providing theoretical justification for mutual information-based active learning approaches. Finally, we illustrate the practical utility of our framework through experiments spanning linear, nonlinear, and multi-agent systems.",
      "tldr_zh": "该研究针对未知动力系统中的主动信息采集(Active Information Gathering)问题，指出了现有方法通常受限于特定动力学模型、信念更新程序和观测模型的局限性。为此，作者提出了一个统一框架，通过显式揭示参数、信念和控制之间的因果依赖关系，将建模选择与信息采集成本解耦。基于 Massey's directed information，该框架推导出了一个通用的信息采集成本函数，该函数仅需满足带有加性噪声的 Markov dynamics 假设，对具体建模选择具有普适性。研究从理论上证明了现有文献中常用的 mutual information 成本是该通用成本的一个特例，并建立了其与线性化 Bayesian estimation 中信息增益的显式联系。最后，通过涵盖线性、非线性和多智能体系统(Multi-Agent Systems)的实验，验证了该框架在处理动力学不确定性时的广泛适用性和实践价值。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.MA",
        "cs.RO",
        "eess.SY"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2601.21988v1",
      "published_date": "2026-01-29 17:00:35 UTC",
      "updated_date": "2026-01-29 17:00:35 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-30T04:20:46.938875+00:00"
    },
    {
      "arxiv_id": "2601.21981v1",
      "title": "VERSA: Verified Event Data Format for Reliable Soccer Analytics",
      "title_zh": "VERSA：面向可靠足球分析的经验证事件数据格式",
      "authors": [
        "Geonhee Jo",
        "Mingu Kang",
        "Kangmin Lee",
        "Minho Lee",
        "Pascal Bauer",
        "Sang-Ki Ko"
      ],
      "abstract": "Event stream data is a critical resource for fine-grained analysis across various domains, including financial transactions, system operations, and sports. In sports, it is actively used for fine-grained analyses such as quantifying player contributions and identifying tactical patterns. However, the reliability of these models is fundamentally limited by inherent data quality issues that cause logical inconsistencies (e.g., incorrect event ordering or missing events). To this end, this study proposes VERSA (Verified Event Data Format for Reliable Soccer Analytics), a systematic verification framework that ensures the integrity of event stream data within the soccer domain. VERSA is based on a state-transition model that defines valid event sequences, thereby enabling the automatic detection and correction of anomalous patterns within the event stream data. Notably, our examination of event data from the K League 1 (2024 season), provided by Bepro, detected that 18.81% of all recorded events exhibited logical inconsistencies. Addressing such integrity issues, our experiments demonstrate that VERSA significantly enhances cross-provider consistency, ensuring stable and unified data representation across heterogeneous sources. Furthermore, we demonstrate that data refined by VERSA significantly improves the robustness and performance of a downstream task called VAEP, which evaluates player contributions. These results highlight that the verification process is highly effective in increasing the reliability of data-driven analysis.",
      "tldr_zh": "这项研究提出了VERSA（Verified Event Data Format for Reliable Soccer Analytics），这是一个旨在确保足球领域事件流数据完整性的系统性验证框架。针对现有足球数据中由于逻辑不一致（如事件顺序错误或缺失）导致的可靠性受限问题，VERSA基于状态转移模型（state-transition model）定义了有效的事件序列，从而实现了对事件流中异常模式的自动检测与纠正。通过对Bepro提供的2024赛季K League 1数据进行分析，研究发现高达18.81%的记录事件存在逻辑矛盾。实验结果表明，VERSA显著增强了不同数据供应商之间的一致性，并在评估球员贡献的下游任务VAEP中显著提升了模型的鲁棒性与性能。该研究结果强调了验证过程对于提高数据驱动分析可靠性的关键作用。",
      "categories": [
        "cs.AI",
        "cs.DB"
      ],
      "primary_category": "cs.AI",
      "comment": "13 pages, 5 figures, 3 tables",
      "pdf_url": "https://arxiv.org/pdf/2601.21981v1",
      "published_date": "2026-01-29 16:58:10 UTC",
      "updated_date": "2026-01-29 16:58:10 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-30T04:20:47.911619+00:00"
    },
    {
      "arxiv_id": "2601.21977v1",
      "title": "From Particles to Agents: Hallucination as a Metric for Cognitive Friction in Spatial Simulation",
      "title_zh": "从粒子到智能体：以幻觉作为空间模拟中认知摩擦的度量指标",
      "authors": [
        "Javier Argota Sánchez-Vaquerizo",
        "Luis Borunda Monsivais"
      ],
      "abstract": "Traditional architectural simulations (e.g. Computational Fluid Dynamics, evacuation, structural analysis) model elements as deterministic physics-based \"particles\" rather than cognitive \"agents\". To bridge this, we introduce \\textbf{Agentic Environmental Simulations}, where Large Multimodal generative models actively predict the next state of spatial environments based on semantic expectation. Drawing on examples from accessibility-oriented AR pipelines and multimodal digital twins, we propose a shift from chronological time-steps to Episodic Spatial Reasoning, where simulations advance through meaningful, surprisal-triggered events. Within this framework we posit AI hallucinations as diagnostic tools. By formalizing the \\textbf{Cognitive Friction} ($C_f$) it is possible to reveal \"Phantom Affordances\", i.e. semiotic ambiguities in built space. Finally, we challenge current HCI paradigms by treating environments as dynamic cognitive partners and propose a human-centered framework of cognitive orchestration for designing AI-driven simulations that preserve autonomy, affective clarity, and cognitive integrity.",
      "tldr_zh": "该研究提出了Agentic Environmental Simulations，旨在将建筑模拟从传统的基于物理的确定性“粒子”模型转向基于大语言多模态生成模型（Large Multimodal generative models）的认知“智能体”模型。研究引入了Episodic Spatial Reasoning，主张模拟应由具有意义的、由惊喜（surprisal）触发的事件驱动，而非传统的按时计步。通过形式化Cognitive Friction ($C_f$)，该框架将AI幻觉（AI hallucinations）转化为诊断工具，用以揭示建筑空间中存在的符号歧义，即“幻影可担负性”（Phantom Affordances）。最后，研究挑战了现有的HCI范式，将环境视为动态的认知伙伴，并提出了一个以人为中心的认知编排框架，为构建能够维护自主性、情感清晰度和认知完整性的AI驱动空间模拟奠定了理论基础。",
      "categories": [
        "cs.HC",
        "cs.AI",
        "cs.CY"
      ],
      "primary_category": "cs.HC",
      "comment": "Paper selected for the workshop Human Cognition, AI, and the Future of HCI: Navigating the Disruptive and Wild Landscape of Large Language Models and Agentic AI as part of the Human-Computer Interaction (HCI) conference of the Alpine region (AlpCHI 2026) hosted at the Congressi Stefano Franscini, March 1st to March 5th, 2026 on Monte Verità in Ascona, Switzerland",
      "pdf_url": "https://arxiv.org/pdf/2601.21977v1",
      "published_date": "2026-01-29 16:54:18 UTC",
      "updated_date": "2026-01-29 16:54:18 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-30T04:20:52.706304+00:00"
    },
    {
      "arxiv_id": "2601.21975v1",
      "title": "Mind the Gap: How Elicitation Protocols Shape the Stated-Revealed Preference Gap in Language Models",
      "title_zh": "关注差异：诱导协议如何塑造语言模型中的陈述与显示偏好差距",
      "authors": [
        "Pranav Mahajan",
        "Ihor Kendiukhov",
        "Syed Hussain",
        "Lydia Nottingham"
      ],
      "abstract": "Recent work identifies a stated-revealed (SvR) preference gap in language models (LMs): a mismatch between the values models endorse and the choices they make in context. Existing evaluations rely heavily on binary forced-choice prompting, which entangles genuine preferences with artifacts of the elicitation protocol. We systematically study how elicitation protocols affect SvR correlation across 24 LMs. Allowing neutrality and abstention during stated preference elicitation allows us to exclude weak signals, substantially improving Spearman's rank correlation ($ρ$) between volunteered stated preferences and forced-choice revealed preferences. However, further allowing abstention in revealed preferences drives $ρ$ to near-zero or negative values due to high neutrality rates. Finally, we find that system prompt steering using stated preferences during revealed preference elicitation does not reliably improve SvR correlation on AIRiskDilemmas. Together, our results show that SvR correlation is highly protocol-dependent and that preference elicitation requires methods that account for indeterminate preferences.",
      "tldr_zh": "该研究调查了语言模型(LMs)中陈述偏好(Stated Preference)与展现偏好(Revealed Preference)之间的差距，即模型表态的价值观与其在具体语境下做出的选择不一致的问题。通过对24个语言模型的系统研究，作者探讨了诱导协议(Elicitation Protocols)对SvR相关性的影响。实验发现，在陈述偏好诱导中引入中立(Neutrality)和弃权(Abstention)选项能够有效排除弱信号，从而显著提升陈述偏好与展现偏好之间的Spearman秩相关系数。然而，若在展现偏好中同样允许弃权，则会导致相关性下降至接近零或负值。此外，研究还发现利用陈述偏好进行系统提示词引导(System Prompt Steering)并不能可靠地改善在AIRiskDilemmas上的SvR相关性。总体而言，该研究揭示了SvR相关性高度依赖于诱导协议，强调偏好诱导方法必须充分考虑不确定偏好的存在。",
      "categories": [
        "cs.AI",
        "cs.ET"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2601.21975v1",
      "published_date": "2026-01-29 16:51:43 UTC",
      "updated_date": "2026-01-29 16:51:43 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-30T04:20:52.343504+00:00"
    },
    {
      "arxiv_id": "2601.21972v1",
      "title": "Learning Decentralized LLM Collaboration with Multi-Agent Actor Critic",
      "title_zh": "基于多智能体 Actor-Critic 的去中心化大语言模型协作学习",
      "authors": [
        "Shuo Liu",
        "Tianle Chen",
        "Ryan Amiri",
        "Christopher Amato"
      ],
      "abstract": "Recent work has explored optimizing LLM collaboration through Multi-Agent Reinforcement Learning (MARL). However, most MARL fine-tuning approaches rely on predefined execution protocols, which often require centralized execution. Decentralized LLM collaboration is more appealing in practice, as agents can run inference in parallel with flexible deployments. Also, current approaches use Monte Carlo methods for fine-tuning, which suffer from high variance and thus require more samples to train effectively. Actor-critic methods are prevalent in MARL for dealing with these issues, so we developed Multi-Agent Actor-Critic (MAAC) methods to optimize decentralized LLM collaboration. In this paper, we analyze when and why these MAAC methods are beneficial. We propose 2 MAAC approaches, \\textbf{CoLLM-CC} with a \\textbf{C}entralized \\textbf{C}ritic and \\textbf{CoLLM-DC} with \\textbf{D}ecentralized \\textbf{C}ritics. Our experiments across writing, coding, and game-playing domains show that Monte Carlo methods and CoLLM-DC can achieve performance comparable to CoLLM-CC in short-horizon and dense-reward settings. However, they both underperform CoLLM-CC on long-horizon or sparse-reward tasks, where Monte Carlo methods require substantially more samples and CoLLM-DC struggles to converge. Our code is available at https://github.com/OpenMLRL/CoMLRL/releases/tag/v1.3.2.",
      "tldr_zh": "该研究探讨了通过多智能体强化学习(Multi-Agent Reinforcement Learning, MARL)优化大语言模型(LLMs)协作的问题，旨在解决现有方法依赖预定义中心化执行协议以及蒙特卡洛(Monte Carlo)微调方法方差过大、采样效率低等缺陷。作者提出了多智能体演员-评论家(Multi-Agent Actor-Critic, MAAC)方法来优化去中心化(Decentralized)的LLM协作，并具体设计了带有中心化评论家的CoLLM-CC和带有去中心化评论家的CoLLM-DC两种架构。实验涵盖了写作、编程和游戏竞技等多个领域，系统对比了不同算法在不同任务环境下的表现。结果显示，在短周期和密集奖励设定下，蒙特卡洛方法和CoLLM-DC能够达到与CoLLM-CC相当的性能。然而在面对长周期或稀疏奖励任务时，CoLLM-CC的表现显著优于其他方法，而蒙特卡洛方法需要海量样本且CoLLM-DC难以收敛。该工作为构建高效、可并行推理且灵活部署的去中心化大语言模型协作系统提供了重要的技术路径参考。",
      "categories": [
        "cs.AI",
        "cs.DC",
        "cs.MA"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2601.21972v1",
      "published_date": "2026-01-29 16:50:30 UTC",
      "updated_date": "2026-01-29 16:50:30 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-30T04:20:56.993814+00:00"
    },
    {
      "arxiv_id": "2601.21971v1",
      "title": "MoE-ACT: Improving Surgical Imitation Learning Policies through Supervised Mixture-of-Experts",
      "title_zh": "MoE-ACT：通过有监督专家混合提升手术模仿学习策略",
      "authors": [
        "Lorenzo Mazza",
        "Ariel Rodriguez",
        "Rayan Younis",
        "Martin Lelis",
        "Ortrun Hellig",
        "Chenpan Li",
        "Sebastian Bodenstedt",
        "Martin Wagner",
        "Stefanie Speidel"
      ],
      "abstract": "Imitation learning has achieved remarkable success in robotic manipulation, yet its application to surgical robotics remains challenging due to data scarcity, constrained workspaces, and the need for an exceptional level of safety and predictability. We present a supervised Mixture-of-Experts (MoE) architecture designed for phase-structured surgical manipulation tasks, which can be added on top of any autonomous policy. Unlike prior surgical robot learning approaches that rely on multi-camera setups or thousands of demonstrations, we show that a lightweight action decoder policy like Action Chunking Transformer (ACT) can learn complex, long-horizon manipulation from less than 150 demonstrations using solely stereo endoscopic images, when equipped with our architecture. We evaluate our approach on the collaborative surgical task of bowel grasping and retraction, where a robot assistant interprets visual cues from a human surgeon, executes targeted grasping on deformable tissue, and performs sustained retraction. We benchmark our method against state-of-the-art Vision-Language-Action (VLA) models and the standard ACT baseline. Our results show that generalist VLAs fail to acquire the task entirely, even under standard in-distribution conditions. Furthermore, while standard ACT achieves moderate success in-distribution, adopting a supervised MoE architecture significantly boosts its performance, yielding higher success rates in-distribution and demonstrating superior robustness in out-of-distribution scenarios, including novel grasp locations, reduced illumination, and partial occlusions. Notably, it generalizes to unseen testing viewpoints and also transfers zero-shot to ex vivo porcine tissue without additional training, offering a promising pathway toward in vivo deployment. To support this, we present qualitative preliminary results of policy roll-outs during in vivo porcine surgery.",
      "tldr_zh": "该研究提出了 MoE-ACT，这是一种专为阶段性结构化手术操作任务设计的监督 Mixture-of-Experts (MoE) 架构，旨在解决手术机器人领域数据稀缺、工作空间受限及高安全性需求等挑战。该架构可应用于 Action Chunking Transformer (ACT) 等动作解码器策略，仅需不足 150 次演示即可凭借立体内窥镜图像学习复杂的长程操作。实验在肠道抓取与回缩的协作手术任务中进行，结果显示 MoE-ACT 在分布内成功率上显著优于通用的 Vision-Language-Action (VLA) 模型及标准 ACT 基准。该方法在面对新抓取位置、光照减少和部分遮挡等分布外场景时展现出卓越的鲁棒性，并能泛化至未见过的测试视角。此外，MoE-ACT 实现了对离体猪组织的 zero-shot 迁移，并提供了在猪活体手术中策略部署的初步定性结果，为未来临床应用奠定了基础。",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.RO",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2601.21971v1",
      "published_date": "2026-01-29 16:50:14 UTC",
      "updated_date": "2026-01-29 16:50:14 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-30T04:21:03.227545+00:00"
    },
    {
      "arxiv_id": "2601.21969v1",
      "title": "Token-Guard: Towards Token-Level Hallucination Control via Self-Checking Decoding",
      "title_zh": "Token-Guard：基于自检查解码的Token级幻觉控制",
      "authors": [
        "Yifan Zhu",
        "Huiqiang Rong",
        "Haoran Luo"
      ],
      "abstract": "Large Language Models (LLMs) often hallucinate, generating content inconsistent with the input. Retrieval-Augmented Generation (RAG) and Reinforcement Learning with Human Feedback (RLHF) can mitigate hallucinations but require resource-intensive retrieval or large-scale fine-tuning. Decoding-based methods are lighter yet lack explicit hallucination control. To address this, we present Token-Guard, a token-level hallucination control method based on self-checking decoding. Token-Guard performs internal verification at each reasoning step to detect hallucinated tokens before they propagate. Candidate fragments are further evaluated in a latent space with explicit hallucination risk scoring, while iterative pruning and regeneration dynamically correct detected errors. Experiments on HALU datasets show Token-Guard substantially reduces hallucinations and improves generation accuracy, offering a scalable, modular solution for reliable LLM outputs. Our code is publicly available.",
      "tldr_zh": "该研究提出了Token-Guard，这是一种基于Self-Checking Decoding的Token级幻觉控制方法，旨在解决大型语言模型(LLMs)在生成过程中出现的幻觉(Hallucination)问题。相比于资源密集型的RAG和RLHF技术，Token-Guard通过在每个推理步骤进行内部验证，能够在幻觉Token传播之前对其进行有效检测。该方法在潜空间(Latent Space)中利用显式的幻觉风险评分评估候选片段，并结合迭代剪枝(Iterative Pruning)与再生(Regeneration)技术动态修正检测到的错误。在HALU数据集上的实验结果表明，Token-Guard显著减少了幻觉现象并大幅提升了生成准确率。该方案为实现可靠的LLM输出提供了一种可扩展且模块化的解决方案，其相关代码也已公开发布。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "26 pages and 11 figures,this work has been accepted for presentation at ICLR 2026",
      "pdf_url": "https://arxiv.org/pdf/2601.21969v1",
      "published_date": "2026-01-29 16:48:47 UTC",
      "updated_date": "2026-01-29 16:48:47 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-30T04:21:01.391456+00:00"
    },
    {
      "arxiv_id": "2601.21967v1",
      "title": "The Energy Impact of Domain Model Design in Classical Planning",
      "title_zh": "经典规划中领域模型设计对能耗的影响",
      "authors": [
        "Ilche Georgievski",
        "Serhat Tekin",
        "Marco Aiello"
      ],
      "abstract": "AI research has traditionally prioritised algorithmic performance, such as optimising accuracy in machine learning or runtime in automated planning. The emerging paradigm of Green AI challenges this by recognising energy consumption as a critical performance dimension. Despite the high computational demands of automated planning, its energy efficiency has received little attention. This gap is particularly salient given the modular planning structure, in which domain models are specified independently of algorithms. On the other hand, this separation also enables systematic analysis of energy usage through domain model design. We empirically investigate how domain model characteristics affect the energy consumption of classical planners. We introduce a domain model configuration framework that enables controlled variation of features, such as element ordering, action arity, and dead-end states. Using five benchmark domains and five state-of-the-art planners, we analyse energy and runtime impacts across 32 domain variants per benchmark. Results demonstrate that domain-level modifications produce measurable energy differences across planners, with energy consumption not always correlating with runtime.",
      "tldr_zh": "该研究探讨了经典规划(Classical Planning)中领域模型设计对能源消耗的影响，响应了绿色人工智能(Green AI)对算法能效的关注。研究者提出了一个领域模型配置框架，通过控制元素排序(Element Ordering)、动作元数(Action Arity)和死端状态(Dead-end States)等特征，对能源使用进行系统化分析。实验结合了五个基准领域和五种先进的规划器(Planners)，针对每个基准测试了32个领域变体。研究结果证明，领域层面的修改会使规划器产生明显的能源消耗差异，且能源消耗并不总是与运行时间(Runtime)呈正相关。该研究揭示了在自动化规划中，除了传统的算法性能优化外，通过领域模型设计提升能效的巨大潜力。",
      "categories": [
        "cs.AI",
        "cs.SE"
      ],
      "primary_category": "cs.AI",
      "comment": "2026 IEEE/ACM 5th International Conference on AI Engineering - Software Engineering for AI (CAIN '26)",
      "pdf_url": "https://arxiv.org/pdf/2601.21967v1",
      "published_date": "2026-01-29 16:46:43 UTC",
      "updated_date": "2026-01-29 16:46:43 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-30T04:21:01.852122+00:00"
    },
    {
      "arxiv_id": "2601.21963v1",
      "title": "Industrialized Deception: The Collateral Effects of LLM-Generated Misinformation on Digital Ecosystems",
      "title_zh": "工业化欺骗：大语言模型生成的虚假信息对数字生态系统的附带影响",
      "authors": [
        "Alexander Loth",
        "Martin Kappes",
        "Marc-Oliver Pahl"
      ],
      "abstract": "Generative AI and misinformation research has evolved since our 2024 survey. This paper presents an updated perspective, transitioning from literature review to practical countermeasures. We report on changes in the threat landscape, including improved AI-generated content through Large Language Models (LLMs) and multimodal systems. Central to this work are our practical contributions: JudgeGPT, a platform for evaluating human perception of AI-generated news, and RogueGPT, a controlled stimulus generation engine for research. Together, these tools form an experimental pipeline for studying how humans perceive and detect AI-generated misinformation. Our findings show that detection capabilities have improved, but the competition between generation and detection continues. We discuss mitigation strategies including LLM-based detection, inoculation approaches, and the dual-use nature of generative AI. This work contributes to research addressing the adverse impacts of AI on information quality.",
      "tldr_zh": "该研究探讨了生成式人工智能（Generative AI）和大语言模型（LLMs）对数字生态系统产生的虚假信息（Misinformation）影响，并分析了威胁格局从理论综述向实际反制措施的转变。论文的核心贡献在于开发了两个实用工具：用于评估人类对AI生成新闻感知的平台JudgeGPT，以及用于研究的受控刺激生成引擎RogueGPT，两者共同构成了研究人类感知与检测AI虚假信息的实验流水线。研究结果表明，尽管当前的检测能力有所增强，但内容的生成与检测之间依然存在持续的博弈竞争。此外，该工作还深入讨论了包括基于LLM的检测、免疫接种（Inoculation）方法在内的缓解策略以及生成式AI的双重用途（Dual-use）性质，为应对AI对信息质量的负面影响贡献了重要的技术路径与理论见解。",
      "categories": [
        "cs.CY",
        "cs.AI",
        "cs.CL",
        "cs.SI"
      ],
      "primary_category": "cs.CY",
      "comment": "Accepted at ACM TheWebConf '26 Companion",
      "pdf_url": "https://arxiv.org/pdf/2601.21963v1",
      "published_date": "2026-01-29 16:42:22 UTC",
      "updated_date": "2026-01-29 16:42:22 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-30T04:21:10.764661+00:00"
    },
    {
      "arxiv_id": "2601.21961v1",
      "title": "How do Visual Attributes Influence Web Agents? A Comprehensive Evaluation of User Interface Design Factors",
      "title_zh": "视觉属性如何影响 Web 智能体？用户界面设计因素的全面评估",
      "authors": [
        "Kuai Yu",
        "Naicheng Yu",
        "Han Wang",
        "Rui Yang",
        "Huan Zhang"
      ],
      "abstract": "Web agents have demonstrated strong performance on a wide range of web-based tasks. However, existing research on the effect of environmental variation has mostly focused on robustness to adversarial attacks, with less attention to agents' preferences in benign scenarios. Although early studies have examined how textual attributes influence agent behavior, a systematic understanding of how visual attributes shape agent decision-making remains limited. To address this, we introduce VAF, a controlled evaluation pipeline for quantifying how webpage Visual Attribute Factors influence web-agent decision-making. Specifically, VAF consists of three stages: (i) variant generation, which ensures the variants share identical semantics as the original item while only differ in visual attributes; (ii) browsing interaction, where agents navigate the page via scrolling and clicking the interested item, mirroring how human users browse online; (iii) validating through both click action and reasoning from agents, which we use the Target Click Rate and Target Mention Rate to jointly evaluate the effect of visual attributes. By quantitatively measuring the decision-making difference between the original and variant, we identify which visual attributes influence agents' behavior most. Extensive experiments, across 8 variant families (48 variants total), 5 real-world websites (including shopping, travel, and news browsing), and 4 representative web agents, show that background color contrast, item size, position, and card clarity have a strong influence on agents' actions, whereas font styling, text color, and item image clarity exhibit minor effects.",
      "tldr_zh": "该研究探讨了网页视觉属性如何影响 Web Agents 的决策过程，针对现有研究在良性场景下对 UI 设计因素缺乏系统理解的问题，提出了受控评估流水线 VAF。VAF 包含变体生成、模拟人类浏览交互以及结合点击动作与推理验证三个阶段，旨在量化网页视觉属性对智能体决策的影响。研究利用 Target Click Rate 和 Target Mention Rate 指标，对 8 个变体家族、5 个真实网站及 4 种代表性 Web Agents 进行了广泛评估。实验结果表明，背景颜色对比度 (background color contrast)、项目尺寸 (item size)、位置 (position) 和卡片清晰度 (card clarity) 对智能体行为有强烈的引导作用，而字体样式 (font styling)、文本颜色 (text color) 和项目图像清晰度 (item image clarity) 的影响较小。该研究揭示了视觉属性在智能体决策中的重要性，为未来优化 Web Agents 的交互设计提供了量化依据。",
      "categories": [
        "cs.AI",
        "cs.HC"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2601.21961v1",
      "published_date": "2026-01-29 16:40:15 UTC",
      "updated_date": "2026-01-29 16:40:15 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-30T04:21:26.962206+00:00"
    },
    {
      "arxiv_id": "2601.21947v1",
      "title": "ToolWeaver: Weaving Collaborative Semantics for Scalable Tool Use in Large Language Models",
      "title_zh": "ToolWeaver：面向大语言模型可扩展工具调用的协作语义编织",
      "authors": [
        "Bowen Fang",
        "Wen Ye",
        "Yunyue Su",
        "Jinghao Zhang",
        "Qiang Liu",
        "Yesheng Liu",
        "Xin Sun",
        "Shu Wu",
        "Jiabing Yang",
        "Baole Wei",
        "Liang Wang"
      ],
      "abstract": "Prevalent retrieval-based tool-use pipelines struggle with a dual semantic challenge: their retrievers often employ encoders that fail to capture complex semantics, while the Large Language Model (LLM) itself lacks intrinsic tool knowledge from its natural language pretraining. Generative methods offer a powerful alternative by unifying selection and execution, tasking the LLM to directly learn and generate tool identifiers. However, the common practice of mapping each tool to a unique new token introduces substantial limitations: it creates a scalability and generalization crisis, as the vocabulary size explodes and each tool is assigned a semantically isolated token. This approach also creates a semantic bottleneck that hinders the learning of collaborative tool relationships, as the model must infer them from sparse co-occurrences of monolithic tool IDs within a vast library. To address these limitations, we propose ToolWeaver, a novel generative tool learning framework that encodes tools into hierarchical sequences. This approach makes vocabulary expansion logarithmic to the number of tools. Crucially, it enables the model to learn collaborative patterns from the dense co-occurrence of shared codes, rather than the sparse co-occurrence of monolithic tool IDs. We generate these structured codes through a novel tokenization process designed to weave together a tool's intrinsic semantics with its extrinsic co-usage patterns. These structured codes are then integrated into the LLM through a generative alignment stage, where the model is fine-tuned to produce the hierarchical code sequences. Evaluation results with nearly 47,000 tools show that ToolWeaver significantly outperforms state-of-the-art methods, establishing a more scalable, generalizable, and semantically-aware foundation for advanced tool-augmented agents.",
      "tldr_zh": "该研究提出了ToolWeaver，一种将工具编码为分层序列(Hierarchical Sequences)的新型生成式工具学习框架，旨在解决大语言模型(LLMs)在工具调用中面临的语义缺失和扩展性难题。针对传统生成式方法中工具标识符(Tool IDs)语义隔离且词表爆炸的问题，ToolWeaver通过分层编码使词表扩展与工具数量呈对数关系，极大提升了系统的可扩展性。该框架引入了一种新颖的分词(Tokenization)过程，将工具的内在语义(Intrinsic Semantics)与其外在共用模式(Extrinsic Co-usage Patterns)相结合，使模型能从共享编码的密集共现中学习工具间的协作关系。通过生成对齐阶段的微调，模型能够精准生成结构化的工具代码。在包含近47,000个工具的大规模评估中，ToolWeaver显著优于现有最先进方法，为开发具备语义感知能力的工具增强型智能体提供了更具泛化性的基础。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "10pages, 12 figures, Accepted to ICLR 2026",
      "pdf_url": "https://arxiv.org/pdf/2601.21947v1",
      "published_date": "2026-01-29 16:29:53 UTC",
      "updated_date": "2026-01-29 16:29:53 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-30T04:21:26.230366+00:00"
    },
    {
      "arxiv_id": "2601.21941v1",
      "title": "Robust Multimodal Representation Learning in Healthcare",
      "title_zh": "医疗领域鲁棒多模态表示学习",
      "authors": [
        "Xiaoguang Zhu",
        "Linxiao Gong",
        "Lianlong Sun",
        "Yang Liu",
        "Haoyu Wang",
        "Jing Liu"
      ],
      "abstract": "Medical multimodal representation learning aims to integrate heterogeneous data into unified patient representations to support clinical outcome prediction. However, real-world medical datasets commonly contain systematic biases from multiple sources, which poses significant challenges for medical multimodal representation learning. Existing approaches typically focus on effective multimodal fusion, neglecting inherent biased features that affect the generalization ability. To address these challenges, we propose a Dual-Stream Feature Decorrelation Framework that identifies and handles the biases through structural causal analysis introduced by latent confounders. Our method employs a causal-biased decorrelation framework with dual-stream neural networks to disentangle causal features from spurious correlations, utilizing generalized cross-entropy loss and mutual information minimization for effective decorrelation. The framework is model-agnostic and can be integrated into existing medical multimodal learning methods. Comprehensive experiments on MIMIC-IV, eICU, and ADNI datasets demonstrate consistent performance improvements.",
      "tldr_zh": "该研究针对医疗多模态表示学习中普遍存在的系统性偏差和泛化难题，提出了 Dual-Stream Feature Decorrelation Framework 双流特征去相关框架。该框架基于结构因果分析(structural causal analysis)识别由潜在混杂因子(latent confounders)引起的偏差，并通过双流神经网络将因果特征(causal features)与伪相关(spurious correlations)进行解耦。通过结合广义交叉熵损失(generalized cross-entropy loss)和互信息最小化(mutual information minimization)，该方法能有效实现特征去相关。由于其具备模型无关性(model-agnostic)的特点，该框架可灵活集成至现有医疗多模态模型中。在 MIMIC-IV、eICU 和 ADNI 等权威数据集上的实验证明，该方法能显著且持续地提升临床预测任务的性能。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2601.21941v1",
      "published_date": "2026-01-29 16:27:54 UTC",
      "updated_date": "2026-01-29 16:27:54 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-30T04:21:33.350696+00:00"
    },
    {
      "arxiv_id": "2601.21937v1",
      "title": "Retrieval-Infused Reasoning Sandbox: A Benchmark for Decoupling Retrieval and Reasoning Capabilities",
      "title_zh": "Retrieval-Infused Reasoning Sandbox：解耦检索与推理能力的评测基准",
      "authors": [
        "Shuangshuang Ying",
        "Zheyu Wang",
        "Yunjian Peng",
        "Jin Chen",
        "Yuhao Wu",
        "Hongbin Lin",
        "Dingyu He",
        "Siyi Liu",
        "Gengchen Yu",
        "YinZhu Piao",
        "Yuchen Wu",
        "Xin Gui",
        "Zhongyuan Peng",
        "Xin Li",
        "Xeron Du",
        "Libo Qin",
        "YiXin Cao",
        "Ge Zhang"
      ],
      "abstract": "Despite strong performance on existing benchmarks, it remains unclear whether large language models can reason over genuinely novel scientific information. Most evaluations score end-to-end RAG pipelines, where reasoning is confounded with retrieval and toolchain choices, and the signal is further contaminated by parametric memorization and open-web volatility. We introduce DeR2, a controlled deep-research sandbox that isolates document-grounded reasoning while preserving core difficulties of deep search: multi-step synthesis, denoising, and evidence-based conclusion making. DeR2 decouples evidence access from reasoning via four regimes--Instruction-only, Concepts (gold concepts without documents), Related-only (only relevant documents), and Full-set (relevant documents plus topically related distractors)--yielding interpretable regime gaps that operationalize retrieval loss vs. reasoning loss and enable fine-grained error attribution. To prevent parametric leakage, we apply a two-phase validation that requires parametric failure without evidence while ensuring oracle-concept solvability. To ensure reproducibility, each instance provides a frozen document library (drawn from 2023-2025 theoretical papers) with expert-annotated concepts and validated rationales. Experiments across a diverse set of state-of-the-art foundation models reveal substantial variation and significant headroom: some models exhibit mode-switch fragility, performing worse with the Full-set than with Instruction-only, while others show structural concept misuse, correctly naming concepts but failing to execute them as procedures.",
      "tldr_zh": "该研究针对大语言模型(LLMs)在处理新型科学信息时推理与检索能力难以解耦的问题，提出了DeR2，一个旨在隔离文档锚定推理能力的受控深研沙盒(deep-research sandbox)。DeR2通过指令(Instruction-only)、概念(Concepts)、相关文档(Related-only)和全集(Full-set)四种模式，将证据获取与推理过程分离，从而实现对检索损失与推理损失的细粒度量化与错误归因。为了防止参数记忆(parametric memorization)导致的泄露，该框架采用2023至2025年的最新理论论文构建文档库，并通过两阶段验证确保任务的有效性。对主流基座模型的实验揭示了显著的性能差异，部分模型表现出模式切换脆弱性(mode-switch fragility)，在提供全量文档时的表现甚至低于仅有指令的情况。此外，研究发现某些模型存在结构性概念误用(structural concept misuse)，即虽然能识别概念但无法正确执行相关程序。该基准测试为评估模型在多步合成、去噪及证据链构建方面的深研能力提供了重要的实验平台。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2601.21937v1",
      "published_date": "2026-01-29 16:26:19 UTC",
      "updated_date": "2026-01-29 16:26:19 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-30T04:21:32.597716+00:00"
    },
    {
      "arxiv_id": "2601.21936v1",
      "title": "AgenticSimLaw: A Juvenile Courtroom Multi-Agent Debate Simulation for Explainable High-Stakes Tabular Decision Making",
      "title_zh": "AgenticSimLaw：面向可解释高风险表格决策的少年法庭多智能体辩论模拟",
      "authors": [
        "Jon Chun",
        "Kathrine Elkins",
        "Yong Suk Lee"
      ],
      "abstract": "We introduce AgenticSimLaw, a role-structured, multi-agent debate framework that provides transparent and controllable test-time reasoning for high-stakes tabular decision-making tasks. Unlike black-box approaches, our courtroom-style orchestration explicitly defines agent roles (prosecutor, defense, judge), interaction protocols (7-turn structured debate), and private reasoning strategies, creating a fully auditable decision-making process. We benchmark this framework on young adult recidivism prediction using the NLSY97 dataset, comparing it against traditional chain-of-thought (CoT) prompting across almost 90 unique combinations of models and strategies. Our results demonstrate that structured multi-agent debate provides more stable and generalizable performance compared to single-agent reasoning, with stronger correlation between accuracy and F1-score metrics. Beyond performance improvements, AgenticSimLaw offers fine-grained control over reasoning steps, generates complete interaction transcripts for explainability, and enables systematic profiling of agent behaviors. While we instantiate this framework in the criminal justice domain to stress-test reasoning under ethical complexity, the approach generalizes to any deliberative, high-stakes decision task requiring transparency and human oversight. This work addresses key LLM-based multi-agent system challenges: organization through structured roles, observability through logged interactions, and responsibility through explicit non-deployment constraints for sensitive domains. Data, results, and code will be available on github.com under the MIT license.",
      "tldr_zh": "该研究提出了 AgenticSimLaw，这是一个具有角色结构的多智能体辩论框架，旨在为高风险的表格决策任务提供透明且可控的测试时推理。该框架采用法庭式组织结构，明确定义了检察官 (prosecutor)、辩护人 (defense) 和法官 (judge) 等智能体角色，并通过七轮结构化辩论协议和私人推理策略，构建了一个完全可审计的决策过程。研究者在 NLSY97 数据集上的青少年累犯预测任务中对该框架进行了基准测试，并将其与传统的链式思维 (Chain-of-Thought) 提示在近 90 种模型与策略组合下进行了对比。实验结果表明，与单智能体推理相比，结构化多智能体辩论提供了更稳定、更具泛化性的性能，且在准确率与 F1-score 指标之间表现出更强的相关性。AgenticSimLaw 不仅提升了性能，还通过生成完整的交互记录实现了细粒度的推理步骤控制和决策可解释性。该方法可推广到任何需要透明度和人类监督的高风险决策任务，有效解决了大语言模型 (LLM) 多智能体系统在组织、可观测性和责任分配方面的核心挑战。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "18 pages, 5 figures",
      "pdf_url": "https://arxiv.org/pdf/2601.21936v1",
      "published_date": "2026-01-29 16:26:10 UTC",
      "updated_date": "2026-01-29 16:26:10 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-30T04:21:46.207842+00:00"
    },
    {
      "arxiv_id": "2601.21920v1",
      "title": "From Future of Work to Future of Workers: Addressing Asymptomatic AI Harms for Dignified Human-AI Interaction",
      "title_zh": "从工作的未来转向劳动者的未来：应对 AI 无症状伤害，构建有尊严的人机交互",
      "authors": [
        "Upol Ehsan",
        "Samir Passi",
        "Koustuv Saha",
        "Todd McNutt",
        "Mark O. Riedl",
        "Sara Alcorn"
      ],
      "abstract": "In the future of work discourse, AI is touted as the ultimate productivity amplifier. Yet, beneath the efficiency gains lie subtle erosions of human expertise and agency. This paper shifts focus from the future of work to the future of workers by navigating the AI-as-Amplifier Paradox: AI's dual role as enhancer and eroder, simultaneously strengthening performance while eroding underlying expertise. We present a year-long study on the longitudinal use of AI in a high-stakes workplace among cancer specialists. Initial operational gains hid ``intuition rust'': the gradual dulling of expert judgment. These asymptomatic effects evolved into chronic harms, such as skill atrophy and identity commoditization. Building on these findings, we offer a framework for dignified Human-AI interaction co-constructed with professional knowledge workers facing AI-induced skill erosion without traditional labor protections. The framework operationalizes sociotechnical immunity through dual-purpose mechanisms that serve institutional quality goals while building worker power to detect, contain, and recover from skill erosion, and preserve human identity. Evaluated across healthcare and software engineering, our work takes a foundational step toward dignified human-AI interaction futures by balancing productivity with the preservation of human expertise.",
      "tldr_zh": "该研究探讨了从“工作的未来”转向“劳动者的未来”的重要性，重点分析了人工智能作为放大器的悖论(AI-as-Amplifier Paradox)，即AI在提升效率的同时会隐性削弱人类的专业技能和自主性。通过对癌症专家进行为期一年的纵向研究，作者发现初始的运营增益掩盖了“直觉生锈”(intuition rust)现象，并最终演变为技能萎缩和身份商品化等无症状的慢性伤害。为此，研究提出了一种旨在实现尊严化人机交互(Dignified Human-AI Interaction)的框架，通过社会技术免疫(sociotechnical immunity)机制帮助专业知识工作者检测并修复技能侵蚀，从而保护人类身份。该框架在医疗和软件工程领域得到了评估，有效平衡了生产力提升与人类专业知识的保留，为构建更具尊严的未来人机协作模式奠定了基础。",
      "categories": [
        "cs.HC",
        "cs.AI",
        "cs.CY"
      ],
      "primary_category": "cs.HC",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2601.21920v1",
      "published_date": "2026-01-29 16:13:41 UTC",
      "updated_date": "2026-01-29 16:13:41 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-30T04:21:39.442884+00:00"
    },
    {
      "arxiv_id": "2601.21919v1",
      "title": "Self-Compression of Chain-of-Thought via Multi-Agent Reinforcement Learning",
      "title_zh": "基于多智能体强化学习的思维链自压缩",
      "authors": [
        "Yiqun Chen",
        "Jinyuan Feng",
        "Wei Yang",
        "Meizhi Zhong",
        "Zhengliang Shi",
        "Rui Li",
        "Xiaochi Wei",
        "Yan Gao",
        "Yi Wu",
        "Yao Hu",
        "Zhiqiang Pu",
        "Jiaxin Mao"
      ],
      "abstract": "The inference overhead induced by redundant reasoning undermines the interactive experience and severely bottlenecks the deployment of Large Reasoning Models. Existing reinforcement learning (RL)-based solutions tackle this problem by coupling a length penalty with outcome-based rewards. This simplistic reward weighting struggles to reconcile brevity with accuracy, as enforcing brevity may compromise critical reasoning logic. In this work, we address this limitation by proposing a multi-agent RL framework that selectively penalizes redundant chunks, while preserving essential reasoning logic. Our framework, Self-Compression via MARL (SCMA), instantiates redundancy detection and evaluation through two specialized agents: \\textbf{a Segmentation Agent} for decomposing the reasoning process into logical chunks, and \\textbf{a Scoring Agent} for quantifying the significance of each chunk. The Segmentation and Scoring agents collaboratively define an importance-weighted length penalty during training, incentivizing \\textbf{a Reasoning Agent} to prioritize essential logic without introducing inference overhead during deployment. Empirical evaluations across model scales demonstrate that SCMA reduces response length by 11.1\\% to 39.0\\% while boosting accuracy by 4.33\\% to 10.02\\%. Furthermore, ablation studies and qualitative analysis validate that the synergistic optimization within the MARL framework fosters emergent behaviors, yielding more powerful LRMs compared to vanilla RL paradigms.",
      "tldr_zh": "该研究针对大型推理模型(Large Reasoning Models)因冗余推理导致的推理开销问题，提出了名为SCMA的多智能体强化学习(Multi-Agent Reinforcement Learning)框架，旨在实现链式思维(Chain-of-Thought)的自压缩。SCMA通过引入专门的Segmentation Agent进行逻辑块分解以及Scoring Agent进行重要性评分，共同定义了一个重要性加权的长度惩罚机制。这种机制激励Reasoning Agent在训练中优先保留核心推理逻辑并减少冗余，且在实际部署阶段不会引入额外的推理负担。实验结果显示，SCMA在减少11.1%至39.0%响应长度的同时，将模型准确率提升了4.33%至10.02%。消融研究和定性分析进一步证实，这种多智能体框架内的协同优化能够诱发模型产生更优的推理行为，使其性能显著超越了传统的强化学习方案。",
      "categories": [
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2601.21919v1",
      "published_date": "2026-01-29 16:13:10 UTC",
      "updated_date": "2026-01-29 16:13:10 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-30T04:21:42.940570+00:00"
    },
    {
      "arxiv_id": "2601.21916v1",
      "title": "JADE: Bridging the Strategic-Operational Gap in Dynamic Agentic RAG",
      "title_zh": "JADE：弥合动态智能体 RAG 中的战略-执行差距",
      "authors": [
        "Yiqun Chen",
        "Erhan Zhang",
        "Tianyi Hu",
        "Shijie Wang",
        "Zixuan Yang",
        "Meizhi Zhong",
        "Xiaochi Wei",
        "Yan Gao",
        "Yi Wu",
        "Yao Hu",
        "Jiaxin Mao"
      ],
      "abstract": "The evolution of Retrieval-Augmented Generation (RAG) has shifted from static retrieval pipelines to dynamic, agentic workflows where a central planner orchestrates multi-turn reasoning. However, existing paradigms face a critical dichotomy: they either optimize modules jointly within rigid, fixed-graph architectures, or empower dynamic planning while treating executors as frozen, black-box tools. We identify that this \\textit{decoupled optimization} creates a ``strategic-operational mismatch,'' where sophisticated planning strategies fail to materialize due to unadapted local executors, often leading to negative performance gains despite increased system complexity. In this paper, we propose \\textbf{JADE} (\\textbf{J}oint \\textbf{A}gentic \\textbf{D}ynamic \\textbf{E}xecution), a unified framework for the joint optimization of planning and execution within dynamic, multi-turn workflows. By modeling the system as a cooperative multi-agent team unified under a single shared backbone, JADE enables end-to-end learning driven by outcome-based rewards. This approach facilitates \\textit{co-adaptation}: the planner learns to operate within the capability boundaries of the executors, while the executors evolve to align with high-level strategic intent. Empirical results demonstrate that JADE transforms disjoint modules into a synergistic system, yielding remarkable performance improvements via joint optimization and enabling a flexible balance between efficiency and effectiveness through dynamic workflow orchestration.",
      "tldr_zh": "该研究针对动态 Agentic RAG 工作流中存在的“战略-执行失配”(strategic-operational mismatch)问题，指出复杂的规划策略往往因为本地执行器(executors)未经适配而无法有效落地。为此，作者提出了 JADE (Joint Agentic Dynamic Execution)，这是一个旨在实现动态多轮工作流中规划与执行联合优化的统一框架。该框架通过将系统建模为统一在单一共享骨干(shared backbone)下的协作多智能体团队，实现了由基于结果奖励(outcome-based rewards)驱动的端到端学习。JADE 促进了规划器与执行器之间的协同进化(co-adaptation)，使规划器能够根据执行器的能力边界进行操作，同时执行器也演化以对齐高层的战略意图。实验结果表明，JADE 通过联合优化将相互独立的模块转化为协同系统，显著提升了系统性能。此外，该框架通过动态工作流编排，在效率与有效性之间实现了灵活的平衡。",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.IR"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2601.21916v1",
      "published_date": "2026-01-29 16:06:44 UTC",
      "updated_date": "2026-01-29 16:06:44 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-30T04:21:47.358938+00:00"
    },
    {
      "arxiv_id": "2601.21912v1",
      "title": "ProRAG: Process-Supervised Reinforcement Learning for Retrieval-Augmented Generation",
      "title_zh": "ProRAG：面向检索增强生成的过程监督强化学习",
      "authors": [
        "Zhao Wang",
        "Ziliang Zhao",
        "Zhicheng Dou"
      ],
      "abstract": "Reinforcement learning (RL) has become a promising paradigm for optimizing Retrieval-Augmented Generation (RAG) in complex reasoning tasks. However, traditional outcome-based RL approaches often suffer from reward sparsity and inefficient credit assignment, as coarse-grained scalar rewards fail to identify specific erroneous steps within long-horizon trajectories. This ambiguity frequently leads to \"process hallucinations\", where models reach correct answers through flawed logic or redundant retrieval steps. Although recent process-aware approaches attempt to mitigate this via static preference learning or heuristic reward shaping, they often lack the on-policy exploration capabilities required to decouple step-level credit from global outcomes. To address these challenges, we propose ProRAG, a process-supervised reinforcement learning framework designed to integrate learned step-level supervision into the online optimization loop. Our framework consists of four stages: (1) Supervised Policy Warmup to initialize the model with a structured reasoning format; (2) construction of an MCTS-based Process Reward Model (PRM) to quantify intermediate reasoning quality; (3) PRM-Guided Reasoning Refinement to align the policy with fine-grained process preferences; and (4) Process-Supervised Reinforcement Learning with a dual-granularity advantage mechanism. By aggregating step-level process rewards with global outcome signals, ProRAG provides precise feedback for every action. Extensive experiments on five multi-hop reasoning benchmarks demonstrate that ProRAG achieves superior overall performance compared to strong outcome-based and process-aware RL baselines, particularly on complex long-horizon tasks, validating the effectiveness of fine-grained process supervision. The code and model are available at https://github.com/lilinwz/ProRAG.",
      "tldr_zh": "该研究提出了ProRAG，一种专为Retrieval-Augmented Generation (RAG)设计的流程监督强化学习框架，旨在解决传统Outcome-based Reinforcement Learning (RL)在复杂推理任务中面临的奖励稀疏、信用分配低效及“过程幻觉”等挑战。ProRAG通过Supervised Policy Warmup、构建基于MCTS的Process Reward Model (PRM)、PRM引导的推理精炼以及集成双粒度优势机制的流程监督强化学习四个阶段，实现了对中间推理步骤的精确量化。该框架成功将步骤级过程奖励与全局结果信号相结合，为模型的每个动作提供细粒度反馈。在五个多跳推理基准测试上的实验证明，ProRAG在复杂长程任务中的表现显著优于强力基线模型，有效验证了细粒度流程监督在提升RAG系统推理质量与可信度方面的作用。",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.IR"
      ],
      "primary_category": "cs.AI",
      "comment": "11 pages, 6 figures",
      "pdf_url": "https://arxiv.org/pdf/2601.21912v1",
      "published_date": "2026-01-29 16:04:59 UTC",
      "updated_date": "2026-01-29 16:04:59 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-30T04:21:56.312463+00:00"
    },
    {
      "arxiv_id": "2601.21909v1",
      "title": "From Meta-Thought to Execution: Cognitively Aligned Post-Training for Generalizable and Reliable LLM Reasoning",
      "title_zh": "从元思维到执行：面向高泛化与高可靠大语言模型推理的认知对齐后训练",
      "authors": [
        "Shaojie Wang",
        "Liang Zhang"
      ],
      "abstract": "Current LLM post-training methods optimize complete reasoning trajectories through Supervised Fine-Tuning (SFT) followed by outcome-based Reinforcement Learning (RL). While effective, a closer examination reveals a fundamental gap: this approach does not align with how humans actually solve problems. Human cognition naturally decomposes problem-solving into two distinct stages: first acquiring abstract strategies (i.e., meta-knowledge) that generalize across problems, then adapting them to specific instances. In contrast, by treating complete trajectories as basic units, current methods are inherently problem-centric, entangling abstract strategies with problem-specific execution. To address this misalignment, we propose a cognitively-inspired framework that explicitly mirrors the two-stage human cognitive process. Specifically, Chain-of-Meta-Thought (CoMT) focuses supervised learning on abstract reasoning patterns without specific executions, enabling acquisition of generalizable strategies. Confidence-Calibrated Reinforcement Learning (CCRL) then optimizes task adaptation via confidence-aware rewards on intermediate steps, preventing overconfident errors from cascading and improving execution reliability. Experiments across four models and eight benchmarks show 2.19\\% and 4.63\\% improvements in-distribution and out-of-distribution respectively over standard methods, while reducing training time by 65-70% and token consumption by 50%, demonstrating that aligning post-training with human cognitive principles yields not only superior generalization but also enhanced training efficiency.",
      "tldr_zh": "该研究提出了一种模拟人类两阶段认知过程的训练框架，旨在解决现有大语言模型(LLMs)后期训练中抽象策略与具体执行过度耦合导致的泛化性不足问题。核心方法包括用于学习通用推理模式的Chain-of-Meta-Thought (CoMT)技术，以及通过中间步骤置信度奖励优化任务适配的Confidence-Calibrated Reinforcement Learning (CCRL)机制。实验结果显示，该方法在八项基准测试中均优于标准训练方案，且在分布外(out-of-distribution)任务上实现了4.63%的性能提升。此外，该框架显著提高了训练效率，将训练时间缩短了65-70%，并减少了50%的Token消耗。这项工作证明了通过将模型训练与人类认知原则对齐，可以在提升推理可靠性的同时大幅降低计算成本。",
      "categories": [
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2601.21909v1",
      "published_date": "2026-01-29 16:00:48 UTC",
      "updated_date": "2026-01-29 16:00:48 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-30T04:21:57.504021+00:00"
    },
    {
      "arxiv_id": "2601.21900v1",
      "title": "TraceRouter: Robust Safety for Large Foundation Models via Path-Level Intervention",
      "title_zh": "TraceRouter：通过路径级干预提升大基础模型的鲁棒安全性",
      "authors": [
        "Chuancheng Shi",
        "Shangze Li",
        "Wenjun Lu",
        "Wenhua Wu",
        "Cong Wang",
        "Zifeng Cheng",
        "Fei Shen",
        "Tat-Seng Chua"
      ],
      "abstract": "Despite their capabilities, large foundation models (LFMs) remain susceptible to adversarial manipulation. Current defenses predominantly rely on the \"locality hypothesis\", suppressing isolated neurons or features. However, harmful semantics act as distributed, cross-layer circuits, rendering such localized interventions brittle and detrimental to utility. To bridge this gap, we propose \\textbf{TraceRouter}, a path-level framework that traces and disconnects the causal propagation circuits of illicit semantics. TraceRouter operates in three stages: (1) it pinpoints a sensitive onset layer by analyzing attention divergence; (2) it leverages sparse autoencoders (SAEs) and differential activation analysis to disentangle and isolate malicious features; and (3) it maps these features to downstream causal pathways via feature influence scores (FIS) derived from zero-out interventions. By selectively suppressing these causal chains, TraceRouter physically severs the flow of harmful information while leaving orthogonal computation routes intact. Extensive experiments demonstrate that TraceRouter significantly outperforms state-of-the-art baselines, achieving a superior trade-off between adversarial robustness and general utility. Our code will be publicly released. WARNING: This paper contains unsafe model responses.",
      "tldr_zh": "该研究提出了 TraceRouter，一种针对大型基础模型 (Large Foundation Models, LFMs) 的路径级安全干预框架，旨在解决现有局部防御方法在应对分布式有害语义时的脆弱性。该框架通过追踪并切断非法语义的因果传播电路来增强模型的鲁棒性。TraceRouter 的执行分为三个阶段：首先利用注意力分歧分析定位敏感起始层；随后结合稀疏自动编码器 (Sparse Autoencoders, SAEs) 与差异激活分析来解耦并隔离恶意特征；最后通过零值干预衍生的特征影响分数 (Feature Influence Scores, FIS) 映射下游因果路径。通过选择性地抑制这些因果链，该方法在物理上阻断有害信息流的同时，确保了正交的正常计算路径不受影响。实验证明，TraceRouter 在对抗鲁棒性与通用效用之间实现了更优的权衡，性能显著超越了当前最先进的基线方案。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CY",
        "cs.MM"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2601.21900v1",
      "published_date": "2026-01-29 15:58:12 UTC",
      "updated_date": "2026-01-29 15:58:12 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-30T04:22:09.109847+00:00"
    },
    {
      "arxiv_id": "2601.21898v1",
      "title": "Making Models Unmergeable via Scaling-Sensitive Loss Landscape",
      "title_zh": "通过缩放敏感损失景观实现模型不可合并性",
      "authors": [
        "Minwoo Jang",
        "Hoyoung Kim",
        "Jabin Koo",
        "Jungseul Ok"
      ],
      "abstract": "The rise of model hubs has made it easier to access reusable model components, making model merging a practical tool for combining capabilities. Yet, this modularity also creates a \\emph{governance gap}: downstream users can recompose released weights into unauthorized mixtures that bypass safety alignment or licensing terms. Because existing defenses are largely post-hoc and architecture-specific, they provide inconsistent protection across diverse architectures and release formats in practice. To close this gap, we propose \\textsc{Trap}$^{2}$, an architecture-agnostic protection framework that encodes protection into the update during fine-tuning, regardless of whether they are released as adapters or full models. Instead of relying on architecture-dependent approaches, \\textsc{Trap}$^{2}$ uses weight re-scaling as a simple proxy for the merging process. It keeps released weights effective in standalone use, but degrades them under re-scaling that often arises in merging, undermining unauthorized merging.",
      "tldr_zh": "该研究针对模型合并(model merging)带来的安全对齐绕过和许可合规风险，提出了 Trap² 这一与架构无关的保护框架。由于现有的防御措施通常是事后且依赖特定架构的，Trap² 通过在微调阶段将保护机制直接编码到模型更新中，实现了对适配器(adapters)和全模型的统一保护。该方法巧妙地将权重重缩放(weight re-scaling)视为模型合并过程的代理特征，通过构建缩放敏感的损失景观来干扰合并效果。实验表明，受保护的模型在独立运行时性能保持稳定，但在经历合并常见的重缩放处理时其性能会显著退化，从而有效削弱了未经授权的模型合并行为。这一研究为模型组件的安全分发与治理提供了新的技术路径，确保了模型在模块化复用环境下的合规性。",
      "categories": [
        "cs.AI",
        "cs.CR"
      ],
      "primary_category": "cs.AI",
      "comment": "Preprint",
      "pdf_url": "https://arxiv.org/pdf/2601.21898v1",
      "published_date": "2026-01-29 15:56:55 UTC",
      "updated_date": "2026-01-29 15:56:55 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-30T04:22:31.601408+00:00"
    },
    {
      "arxiv_id": "2601.21895v1",
      "title": "Learn-to-Distance: Distance Learning for Detecting LLM-Generated Text",
      "title_zh": "Learn-to-Distance：用于大语言模型生成文本检测的距离学习",
      "authors": [
        "Hongyi Zhou",
        "Jin Zhu",
        "Erhan Xu",
        "Kai Ye",
        "Ying Yang",
        "Chengchun Shi"
      ],
      "abstract": "Modern large language models (LLMs) such as GPT, Claude, and Gemini have transformed the way we learn, work, and communicate. Yet, their ability to produce highly human-like text raises serious concerns about misinformation and academic integrity, making it an urgent need for reliable algorithms to detect LLM-generated content. In this paper, we start by presenting a geometric approach to demystify rewrite-based detection algorithms, revealing their underlying rationale and demonstrating their generalization ability. Building on this insight, we introduce a novel rewrite-based detection algorithm that adaptively learns the distance between the original and rewritten text. Theoretically, we demonstrate that employing an adaptively learned distance function is more effective for detection than using a fixed distance. Empirically, we conduct extensive experiments with over 100 settings, and find that our approach demonstrates superior performance over baseline algorithms in the majority of scenarios. In particular, it achieves relative improvements from 57.8\\% to 80.6\\% over the strongest baseline across different target LLMs (e.g., GPT, Claude, and Gemini).",
      "tldr_zh": "该研究针对大语言模型(LLMs)生成的文本可能带来的虚假信息和学术诚信问题，提出了名为Learn-to-Distance的检测算法。作者首先通过几何方法(geometric approach)阐明了基于重写(rewrite-based)检测算法的原理及其泛化能力，揭示了其内在逻辑。在此基础上，该研究引入了一种能够自适应学习原始文本与重写文本之间距离的新型检测算法。理论分析证明，采用自适应学习的距离函数比使用固定距离函数在检测任务中更为有效。广泛的实验结果显示，Learn-to-Distance在100多种实验设置下表现优异，尤其是在针对GPT、Claude和Gemini等主流模型时，相较于最强基线模型实现了57.8%至80.6%的显著相对提升。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "stat.ML"
      ],
      "primary_category": "cs.CL",
      "comment": "Accepted by ICLR2026",
      "pdf_url": "https://arxiv.org/pdf/2601.21895v1",
      "published_date": "2026-01-29 15:55:15 UTC",
      "updated_date": "2026-01-29 15:55:15 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-30T04:23:12.961098+00:00"
    },
    {
      "arxiv_id": "2601.21892v1",
      "title": "Improving Classifier-Free Guidance of Flow Matching via Manifold Projection",
      "title_zh": "通过流形投影改进流匹配的无分类器引导",
      "authors": [
        "Jian-Feng Cai",
        "Haixia Liu",
        "Zhengyi Su",
        "Chao Wang"
      ],
      "abstract": "Classifier-free guidance (CFG) is a widely used technique for controllable generation in diffusion and flow-based models. Despite its empirical success, CFG relies on a heuristic linear extrapolation that is often sensitive to the guidance scale. In this work, we provide a principled interpretation of CFG through the lens of optimization. We demonstrate that the velocity field in flow matching corresponds to the gradient of a sequence of smoothed distance functions, which guides latent variables toward the scaled target image set. This perspective reveals that the standard CFG formulation is an approximation of this gradient, where the prediction gap, the discrepancy between conditional and unconditional outputs, governs guidance sensitivity. Leveraging this insight, we reformulate the CFG sampling as a homotopy optimization with a manifold constraint. This formulation necessitates a manifold projection step, which we implement via an incremental gradient descent scheme during sampling. To improve computational efficiency and stability, we further enhance this iterative process with Anderson Acceleration without requiring additional model evaluations. Our proposed methods are training-free and consistently refine generation fidelity, prompt alignment, and robustness to the guidance scale. We validate their effectiveness across diverse benchmarks, demonstrating significant improvements on large-scale models such as DiT-XL-2-256, Flux, and Stable Diffusion 3.5.",
      "tldr_zh": "该研究针对 Classifier-free guidance (CFG) 在扩散和流匹配模型中因依赖启发式线性外推而导致的引导尺度敏感性问题，提出了一种基于优化视角的原理解释。作者证明了 Flow Matching 中的速度场对应于平滑距离函数的梯度，揭示了标准 CFG 公式实质上是对该梯度的近似。基于此发现，研究将 CFG 采样过程重新构建为具有流形约束的同伦优化问题 (homotopy optimization with a manifold constraint)，并引入了流形投影 (manifold projection) 步骤。该方案通过采样过程中的增量梯度下降实现，并进一步利用 Anderson Acceleration 技术在不增加模型评估次数的前提下提升了计算效率与稳定性。所提方法属于 training-free 类型，能一致性地优化生成的保真度、提示词对齐度 (prompt alignment) 以及对引导尺度的鲁棒性。在 DiT-XL-2-256、Flux 和 Stable Diffusion 3.5 等主流大规模模型上的实验结果充分验证了该方法的有效性及其带来的显著性能提升。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "24 pages, 14 figures",
      "pdf_url": "https://arxiv.org/pdf/2601.21892v1",
      "published_date": "2026-01-29 15:49:31 UTC",
      "updated_date": "2026-01-29 15:49:31 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-30T04:22:16.215584+00:00"
    },
    {
      "arxiv_id": "2601.21879v1",
      "title": "astra-langchain4j: Experiences Combining LLMs and Agent Programming",
      "title_zh": "astra-langchain4j：大语言模型与智能体编程结合的实践经验",
      "authors": [
        "Rem Collier",
        "Katharine Beaumont",
        "Andrei Ciortea"
      ],
      "abstract": "Given the emergence of Generative AI over the last two years and the increasing focus on Agentic AI as a form of Multi-Agent System it is important to explore both how such technologies can impact the use of traditional Agent Toolkits and how the wealth of experience encapsulated in those toolkits can influence the design of the new agentic platforms. This paper presents an overview of our experience developing a prototype large language model (LLM) integration for the ASTRA programming language. It presents a brief overview of the toolkit, followed by three example implementations, concluding with a discussion of the experiences garnered through the examples.",
      "tldr_zh": "该研究探讨了生成式人工智能(Generative AI)和智能体人工智能(Agentic AI)兴起背景下，传统智能体工具包与大语言模型(LLM)结合的可能性。文章介绍了一个将LLM集成到ASTRA编程语言中的原型工具astra-langchain4j，并提供了该工具包的系统概述。通过对三个具体集成示例的实现与分析，作者探讨了如何利用传统多智能体系统(Multi-Agent System)的开发经验来优化新兴智能体平台的设计。论文最后总结了在开发过程中获得的实践经验，为未来智能体编程语言与AI技术的深度融合提供了参考。",
      "categories": [
        "cs.AI",
        "cs.MA"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2601.21879v1",
      "published_date": "2026-01-29 15:46:13 UTC",
      "updated_date": "2026-01-29 15:46:13 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-30T04:23:20.992154+00:00"
    },
    {
      "arxiv_id": "2601.21872v1",
      "title": "WebArbiter: A Principle-Guided Reasoning Process Reward Model for Web Agents",
      "title_zh": "WebArbiter：基于原则引导推理的网络智能体过程奖励模型",
      "authors": [
        "Yao Zhang",
        "Shijie Tang",
        "Zeyu Li",
        "Zhen Han",
        "Volker Tresp"
      ],
      "abstract": "Web agents hold great potential for automating complex computer tasks, yet their interactions involve long-horizon, sequential decision-making with irreversible actions. In such settings, outcome-based supervision is sparse and delayed, often rewarding incorrect trajectories and failing to support inference-time scaling. This motivates the use of Process Reward Models (WebPRMs) for web navigation, but existing approaches remain limited: scalar WebPRMs collapse progress into coarse, weakly grounded signals, while checklist-based WebPRMs rely on brittle template matching that fails under layout or semantic changes and often mislabels superficially correct actions as successful, providing little insight or interpretability. To address these challenges, we introduce WebArbiter, a reasoning-first, principle-inducing WebPRM that formulates reward modeling as text generation, producing structured justifications that conclude with a preference verdict and identify the action most conducive to task completion under the current context. Training follows a two-stage pipeline: reasoning distillation equips the model with coherent principle-guided reasoning, and reinforcement learning corrects teacher biases by directly aligning verdicts with correctness, enabling stronger generalization. To support systematic evaluation, we release WebPRMBench, a comprehensive benchmark spanning four diverse web environments with rich tasks and high-quality preference annotations. On WebPRMBench, WebArbiter-7B outperforms the strongest baseline, GPT-5, by 9.1 points. In reward-guided trajectory search on WebArena-Lite, it surpasses the best prior WebPRM by up to 7.2 points, underscoring its robustness and practical value in real-world complex web tasks.",
      "tldr_zh": "该研究针对网络智能体 (Web agents) 在长程序列决策中面临的稀疏奖励以及现有过程奖励模型 (WebPRMs) 缺乏鲁棒性与解释性等挑战，提出了 WebArbiter。这是一种以推理为核心、由原则引导的 WebPRM 框架，它将奖励建模公式化为文本生成任务，通过产生结构化的理由分析和偏好判定来识别当前上下文中对完成任务最有利的动作。WebArbiter 的训练采用两阶段流水线，结合了推理蒸馏 (reasoning distillation) 以获取一致的原则引导推理能力，以及强化学习 (reinforcement learning) 以直接对齐判定结果并增强泛化性能。此外，研究者还发布了包含丰富任务和高质量偏好标注的综合基准测试集 WebPRMBench。实验结果显示，WebArbiter-7B 在 WebPRMBench 上超越了强基准模型 GPT-5，并在 WebArena-Lite 的奖励引导轨迹搜索中显著提升了性能，验证了其在真实复杂网页任务中的稳健性和实用价值。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "ICLR 2026",
      "pdf_url": "https://arxiv.org/pdf/2601.21872v1",
      "published_date": "2026-01-29 15:39:50 UTC",
      "updated_date": "2026-01-29 15:39:50 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-30T04:22:20.917753+00:00"
    },
    {
      "arxiv_id": "2601.21866v1",
      "title": "MoHETS: Long-term Time Series Forecasting with Mixture-of-Heterogeneous-Experts",
      "title_zh": "MoHETS：基于异构专家混合的长程时间序列预测",
      "authors": [
        "Evandro S. Ortigossa",
        "Guy Lutsker",
        "Eran Segal"
      ],
      "abstract": "Real-world multivariate time series can exhibit intricate multi-scale structures, including global trends, local periodicities, and non-stationary regimes, which makes long-horizon forecasting challenging. Although sparse Mixture-of-Experts (MoE) approaches improve scalability and specialization, they typically rely on homogeneous MLP experts that poorly capture the diverse temporal dynamics of time series data. We address these limitations with MoHETS, an encoder-only Transformer that integrates sparse Mixture-of-Heterogeneous-Experts (MoHE) layers. MoHE routes temporal patches to a small subset of expert networks, combining a shared depthwise-convolution expert for sequence-level continuity with routed Fourier-based experts for patch-level periodic structures. MoHETS further improves robustness to non-stationary dynamics by incorporating exogenous information via cross-attention over covariate patch embeddings. Finally, we replace parameter-heavy linear projection heads with a lightweight convolutional patch decoder, improving parameter efficiency, reducing training instability, and allowing a single model to generalize across arbitrary forecast horizons. We validate across seven multivariate benchmarks and multiple horizons, with MoHETS consistently achieving state-of-the-art performance, reducing the average MSE by $12\\%$ compared to strong recent baselines, demonstrating effective heterogeneous specialization for long-term forecasting.",
      "tldr_zh": "该研究提出了MoHETS，一种专门用于长程时间序列预测的Encoder-only Transformer架构，旨在解决多变量时间序列中复杂的全局趋势和局部周期性建模难题。该模型的核心创新在于引入了异构混合专家层（Mixture-of-Heterogeneous-Experts, MoHE），通过将时间切片路由至不同的专家网络，结合了捕捉序列连续性的共享深度可分离卷积（Depthwise-Convolution）专家与处理切片级周期结构的傅里叶（Fourier-based）专家。为了增强对非平稳动态的鲁棒性，MoHETS通过交叉注意力机制整合了协变量切片嵌入（Covariate Patch Embeddings）中的外生信息。此外，研究者利用轻量级卷积切片解码器替代了传统的线性投影头，在提高参数效率的同时降低了训练不稳定性，使模型能够泛化至任意预测时界。实验结果显示，MoHETS在七个多变量基准数据集上均达到了SOTA性能，较强基准模型的平均均方误差（MSE）降低了12%，验证了异构专家特化在长期预测中的有效性。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "Under review",
      "pdf_url": "https://arxiv.org/pdf/2601.21866v1",
      "published_date": "2026-01-29 15:35:26 UTC",
      "updated_date": "2026-01-29 15:35:26 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-30T04:22:29.170371+00:00"
    },
    {
      "arxiv_id": "2601.21864v1",
      "title": "KnowBias: Mitigating Social Bias in LLMs via Know-Bias Neuron Enhancement",
      "title_zh": "KnowBias：通过“知偏”神经元增强缓解大语言模型中的社会偏见",
      "authors": [
        "Jinhao Pan",
        "Chahat Raj",
        "Anjishnu Mukherjee",
        "Sina Mansouri",
        "Bowen Wei",
        "Shloka Yada",
        "Ziwei Zhu"
      ],
      "abstract": "Large language models (LLMs) exhibit social biases that reinforce harmful stereotypes, limiting their safe deployment. Most existing debiasing methods adopt a suppressive paradigm by modifying parameters, prompts, or neurons associated with biased behavior; however, such approaches are often brittle, weakly generalizable, data-inefficient, and prone to degrading general capability. We propose \\textbf{KnowBias}, a lightweight and conceptually distinct framework that mitigates bias by strengthening, rather than suppressing, neurons encoding bias-knowledge. KnowBias identifies neurons encoding bias knowledge using a small set of bias-knowledge questions via attribution-based analysis, and selectively enhances them at inference time. This design enables strong debiasing while preserving general capabilities, generalizes across bias types and demographics, and is highly data efficient, requiring only a handful of simple yes/no questions and no retraining. Experiments across multiple benchmarks and LLMs demonstrate consistent state-of-the-art debiasing performance with minimal utility degradation. Data and code are available at https://github.com/JP-25/KnowBias.",
      "tldr_zh": "该研究提出了KnowBias，这是一种用于缓解大语言模型(LLMs)中社会偏见的轻量化框架。与以往修改或抑制偏见参数的传统方法不同，KnowBias采用了一种增强编码偏见知识神经元的创新策略。该框架利用基于归因分析(attribution-based analysis)的方法，通过极少量的简单是非题精准识别模型中的偏见知识神经元。在推理阶段(inference time)，KnowBias通过选择性地增强这些神经元来实现去偏见(debiasing)，且无需进行任何模型重训(retraining)。实验结果表明，该方法在多个基准测试中均展现出先进的(SOTA)性能，具备极高的数据效率和跨偏见类型的泛化能力。此外，KnowBias在显著降低偏见的同时，能够有效保持模型的通用能力并最小化效能损失(utility degradation)。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2601.21864v1",
      "published_date": "2026-01-29 15:32:38 UTC",
      "updated_date": "2026-01-29 15:32:38 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-30T04:22:32.073739+00:00"
    },
    {
      "arxiv_id": "2601.21844v1",
      "title": "Bridging Forecast Accuracy and Inventory KPIs: A Simulation-Based Software Framework",
      "title_zh": "衔接预测准确性与库存KPI：基于仿真的软件框架",
      "authors": [
        "So Fukuhara",
        "Abdallah Alabdallah",
        "Nuwan Gunasekara",
        "Slawomir Nowaczyk"
      ],
      "abstract": "Efficient management of spare parts inventory is crucial in the automotive aftermarket, where demand is highly intermittent and uncertainty drives substantial cost and service risks. Forecasting is therefore central, but the quality of a forecasting model should be judged not by statistical accuracy (e.g., MAE, RMSE, IAE) but rather by its impact on key operational performance indicators (KPIs), such as total cost and service level. Yet most existing work evaluates models exclusively using accuracy metrics, and the relationship between these metrics and operational KPIs remains poorly understood. To address this gap, we propose a decision-centric simulation software framework that enables systematic evaluation of forecasting model in realistic inventory management setting. The framework comprises: (i) a synthetic demand generator tailored to spare-parts demand characteristics, (ii) a flexible forecasting module that can host arbitrary predictive models, and (iii) an inventory control simulator that consumes the forecasts and computes operational KPIs. This closed-loop setup enables researchers to evaluate models not only in terms of statistical error but also in terms of their downstream implications for inventory decisions. Using a wide range of simulation scenarios, we show that improvements in conventional accuracy metrics do not necessarily translate into better operational performance, and that models with similar statistical error profiles can induce markedly different cost-service trade-offs. We analyze these discrepancies to characterize how specific aspects of forecast performance affect inventory outcomes and derive guidance for model selection. Overall, the framework operationalizes the link between demand forecasting and inventory management, shifting evaluation from purely predictive accuracy toward operational relevance in the automotive aftermarket and related domains.",
      "tldr_zh": "该研究针对汽车售后市场中备件需求的高度间歇性和不确定性，探讨了预测模型在统计准确度与库存关键绩效指标(KPIs)之间的脱节问题。为此，作者提出了一个以决策为中心的模拟软件框架，包含合成需求生成器、灵活预测模块以及库存控制模拟器，用于在真实库存管理场景下系统评估预测模型。通过广泛的模拟实验，研究表明传统准确性指标（如 MAE、RMSE、IAE）的提升并不一定能转化为更好的运营表现，且具有相似统计误差特征的模型可能导致显著不同的成本与服务水平权衡(cost-service trade-offs)。研究进一步分析了预测性能的具体维度如何影响库存结果，并为模型选择提供了指导。总体而言，该框架实现了需求预测与库存管理之间的操作化链接，将评估重点从单纯的预测准确度(predictive accuracy)转向了汽车售后及相关领域的运营相关性(operational relevance)。",
      "categories": [
        "cs.AI",
        "cs.SE"
      ],
      "primary_category": "cs.AI",
      "comment": "12 pages, 6 figures",
      "pdf_url": "https://arxiv.org/pdf/2601.21844v1",
      "published_date": "2026-01-29 15:20:33 UTC",
      "updated_date": "2026-01-29 15:20:33 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-30T04:22:46.184788+00:00"
    },
    {
      "arxiv_id": "2601.21839v1",
      "title": "Test-Time Compute Games",
      "title_zh": "测试时计算博弈",
      "authors": [
        "Ander Artola Velasco",
        "Dimitrios Rontogiannis",
        "Stratis Tsirtsis",
        "Manuel Gomez-Rodriguez"
      ],
      "abstract": "Test-time compute has emerged as a promising strategy to enhance the reasoning abilities of large language models (LLMs). However, this strategy has in turn increased how much users pay cloud-based providers offering LLM-as-a-service, since providers charge users for the amount of test-time compute they use to generate an output. In our work, we show that the market of LLM-as-a-service is socially inefficient: providers have a financial incentive to increase the amount of test-time compute, even if this increase contributes little to the quality of the outputs. To address this inefficiency, we introduce a reverse second-price auction mechanism where providers bid their offered price and (expected) quality for the opportunity to serve a user, and users pay proportionally to the marginal value generated by the winning provider relative to the second-highest bidder. To illustrate and complement our theoretical results, we conduct experiments with multiple instruct models from the $\\texttt{Llama}$ and $\\texttt{Qwen}$ families, as well as reasoning models distilled from $\\texttt{DeepSeek-R1}$, on math and science benchmark datasets.",
      "tldr_zh": "该研究探讨了Test-time compute在提升大语言模型（LLMs）推理能力的同时，如何通过LLM-as-a-service商业模式导致市场效率低下。研究指出，由于云服务提供商根据计算量收费，其存在增加Test-time compute的财务诱因，即使这种增加对输出质量的贡献微乎其微。为解决这种社会不效率问题，作者提出了一种反向次价拍卖（reverse second-price auction）机制，要求提供商就价格和预期质量进行竞标，并根据胜出者相对于次优竞标者的边际价值确定支付额。实验部分采用了Llama和Qwen系列指令模型，以及从DeepSeek-R1蒸馏出的推理模型，在数学和科学基准数据集上进行了验证。该研究不仅揭示了当前模型服务市场的激励冲突，还为平衡推理性能与计算成本提供了一种稳健的理论框架。",
      "categories": [
        "cs.CY",
        "cs.AI",
        "cs.GT",
        "cs.LG"
      ],
      "primary_category": "cs.CY",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2601.21839v1",
      "published_date": "2026-01-29 15:18:01 UTC",
      "updated_date": "2026-01-29 15:18:01 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-30T04:22:43.224019+00:00"
    },
    {
      "arxiv_id": "2601.21837v1",
      "title": "Trustworthy Intelligent Education: A Systematic Perspective on Progress, Challenges, and Future Directions",
      "title_zh": "可信智能教育：进展、挑战与未来方向的系统性视角",
      "authors": [
        "Xiaoshan Yu",
        "Shangshang Yang",
        "Ziwen Wang",
        "Haiping Ma",
        "Xingyi Zhang"
      ],
      "abstract": "In recent years, trustworthiness has garnered increasing attention and exploration in the field of intelligent education, due to the inherent sensitivity of educational scenarios, such as involving minors and vulnerable groups, highly personalized learning data, and high-stakes educational outcomes. However, existing research either focuses on task-specific trustworthy methods without a holistic view of trustworthy intelligent education, or provides survey-level discussions that remain high-level and fragmented, lacking a clear and systematic categorization. To address these limitations, in this paper, we present a systematic and structured review of trustworthy intelligent education. Specifically, We first organize intelligent education into five representative task categories: learner ability assessment, learning resource recommendation, learning analytics, educational content understanding, and instructional assistance. Building on this task landscape, we review existing studies from five trustworthiness perspectives, including safety and privacy, robustness, fairness, explainability, and sustainability, and summarize and categorize the research methodologies and solution strategies therein. Finally, we summarize key challenges and discuss future research directions. This survey aims to provide a coherent reference framework and facilitate a clearer understanding of trustworthiness in intelligent education.",
      "tldr_zh": "该论文针对智能教育中由于涉及未成年人、敏感学习数据及高风险教育结果而备受关注的可信性问题，提供了一个系统化且结构化的综述。作者首先将智能教育划分为学习者能力评估(learner ability assessment)、学习资源推荐(learning resource recommendation)、学习分析(learning analytics)、教育内容理解(educational content understanding)和教学辅助(instructional assistance)五大代表性任务。在此任务格局基础上，研究从安全与隐私(safety and privacy)、鲁棒性(robustness)、公平性(fairness)、可解释性(explainability)及可持续性(sustainability)五个维度对现有研究进行了回顾。论文总结并分类了相关的研究方法与解决方案策略，并进一步探讨了当前面临的主要挑战与未来发展方向。该综述旨在为可信智能教育建立一个连贯的参考框架，提升学术界对该领域的系统性理解。",
      "categories": [
        "cs.CY",
        "cs.AI",
        "cs.IR"
      ],
      "primary_category": "cs.CY",
      "comment": "9 pages, 3figures",
      "pdf_url": "https://arxiv.org/pdf/2601.21837v1",
      "published_date": "2026-01-29 15:17:25 UTC",
      "updated_date": "2026-01-29 15:17:25 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-30T04:23:37.193110+00:00"
    },
    {
      "arxiv_id": "2601.21830v1",
      "title": "Looking Beyond Accuracy: A Holistic Benchmark of ECG Foundation Models",
      "title_zh": "超越准确性：ECG 基础模型的全方位基准",
      "authors": [
        "Francesca Filice",
        "Edoardo De Rose",
        "Simone Bartucci",
        "Francesco Calimeri",
        "Simona Perri"
      ],
      "abstract": "The electrocardiogram (ECG) is a cost-effective, highly accessible and widely employed diagnostic tool. With the advent of Foundation Models (FMs), the field of AI-assisted ECG interpretation has begun to evolve, as they enable model reuse across different tasks by relying on embeddings. However, to responsibly employ FMs, it is crucial to rigorously assess to which extent the embeddings they produce are generalizable, particularly in error-sensitive domains such as healthcare. Although prior works have already addressed the problem of benchmarking ECG-expert FMs, they focus predominantly on the evaluation of downstream performance. To fill this gap, this study aims to find an in-depth, comprehensive benchmarking framework for FMs, with a specific focus on ECG-expert ones. To this aim, we introduce a benchmark methodology that complements performance-based evaluation with representation-level analysis, leveraging SHAP and UMAP techniques. Furthermore, we rely on the methodology for carrying out an extensive evaluation of several ECG-expert FMs pretrained via state-of-the-art techniques over different cross-continental datasets and data availability settings; this includes ones featuring data scarcity, a fairly common situation in real-world medical scenarios. Experimental results show that our benchmarking protocol provides a rich insight of ECG-expert FMs' embedded patterns, enabling a deeper understanding of their representational structure and generalizability.",
      "tldr_zh": "该研究针对心电图(ECG)领域人工智能辅助诊断的发展，提出了一个全面评估ECG Foundation Models (FMs) 的基准测试框架，旨在解决现有研究过度关注下游任务表现而忽视表示能力的问题。该框架通过引入SHAP和UMAP技术，将传统的性能评估与表示层分析(representation-level analysis)相结合，实现了对模型特征捕获能力的深度剖析。研究人员在跨大洲数据集以及数据稀缺等多种真实医疗场景下，对多种采用最先进技术预训练的ECG-expert FMs进行了广泛验证。实验结果表明，该基准测试协议能够深入揭示模型的嵌入模式，显著增强了对模型表示结构和泛化能力(generalizability)的理解。这一研究为在医疗等错误敏感领域负责任地部署和应用基础模型提供了科学的评估标准和理论支撑。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2601.21830v1",
      "published_date": "2026-01-29 15:14:00 UTC",
      "updated_date": "2026-01-29 15:14:00 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-30T04:23:32.770182+00:00"
    },
    {
      "arxiv_id": "2601.21822v1",
      "title": "CORE:Toward Ubiquitous 6G Intelligence Through Collaborative Orchestration of Large Language Model Agents Over Hierarchical Edge",
      "title_zh": "CORE：通过分层边缘大语言模型智能体的协同编排迈向泛在 6G 智能",
      "authors": [
        "Zitong Yu",
        "Boquan Sun",
        "Yang Li",
        "Zheyan Qu",
        "Xing Zhang"
      ],
      "abstract": "Rapid advancements in sixth-generation (6G) networks and large language models (LLMs) have paved the way for ubiquitous intelligence, wherein seamless connectivity and distributed artificial intelligence (AI) have revolutionized various aspects of our lives.However, realizing this vision faces significant challenges owing to the fragmented and heterogeneous computing resources across hierarchical networks, which are insufficient for individual LLM agents to perform complex reasoning tasks.To address this issue, we propose Collaborative Orchestration Role at Edge (CORE), an innovative framework that employs a collaborative learning system in which multiple LLMs, each assigned a distinct functional role, are distributed across mobile devices and tiered edge servers. The system integrates three optimization modules, encompassing real-time perception,dynamic role orchestration, and pipeline-parallel execution, to facilitate efficient and rapid collaboration among distributed agents. Furthermore, we introduce a novel role affinity scheduling algorithm for dynamically orchestrating LLM role assignments across the hierarchical edge infrastructure, intelligently matching computational demands with available dispersed resources.Finally, comprehensive case studies and performance evaluations across various 6G application scenarios demonstrated the efficacy of CORE, revealing significant enhancements in the system efficiency and task completion rates. Building on these promising outcomes, we further validated the practical applicability of CORE by deploying it on a real-world edge-computing platform,that exhibits robust performance in operational environments.",
      "tldr_zh": "该研究提出了CORE（Collaborative Orchestration Role at Edge）框架，旨在解决6G层级网络中异构计算资源难以支撑大规模语言模型(LLMs)复杂推理任务的挑战。该框架通过在移动设备和多级边缘服务器上分布具有不同功能角色的LLM智能体，构建了一个协作学习系统。系统内部集成了实时感知、动态角色编排和流水线并行执行(pipeline-parallel execution)优化模块，显著提升了分布式智能体间的协作效率。此外，研究引入了角色亲和力调度算法(role affinity scheduling algorithm)，实现了计算需求与分散资源的智能匹配。实验和实际边缘计算平台部署结果表明，CORE在多种6G应用场景下大幅提高了系统效率和任务完成率，验证了其在复杂运行环境中的实用性与稳健性。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "Accepted by IEEE Communications Magazine",
      "pdf_url": "https://arxiv.org/pdf/2601.21822v1",
      "published_date": "2026-01-29 15:08:19 UTC",
      "updated_date": "2026-01-29 15:08:19 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-30T04:23:40.800354+00:00"
    },
    {
      "arxiv_id": "2601.21815v1",
      "title": "Moral Outrage Shapes Commitments Beyond Attention: Multimodal Moral Emotions on YouTube in Korea and the US",
      "title_zh": "道德义愤塑造超越注意力的参与度：韩国与美国YouTube平台上的多模态道德情感研究",
      "authors": [
        "Seongchan Park",
        "Jaehong Kim",
        "Hyeonseung Kim",
        "Heejin Bin",
        "Sue Moon",
        "Wonjae Lee"
      ],
      "abstract": "Understanding how media rhetoric shapes audience engagement is crucial in the attention economy. This study examines how moral emotional framing by mainstream news channels on YouTube influences user behavior across Korea and the United States. To capture the platform's multimodal nature, combining thumbnail images and video titles, we develop a multimodal moral emotion classifier by fine tuning a vision language model. The model is trained on human annotated multimodal datasets in both languages and applied to approximately 400,000 videos from major news outlets. We analyze engagement levels including views, likes, and comments, representing increasing degrees of commitment. The results show that other condemning rhetoric expressions of moral outrage that criticize others morally consistently increase all forms of engagement across cultures, with effects ranging from passive viewing to active commenting. These findings suggest that moral outrage is a particularly effective emotional strategy, attracting not only attention but also active participation. We discuss concerns about the potential misuse of other condemning rhetoric, as such practices may deepen polarization by reinforcing in group and out group divisions. To facilitate future research and ensure reproducibility, we publicly release our Korean and English multimodal moral emotion classifiers.",
      "tldr_zh": "本研究探讨了韩国和美国 YouTube 主流新闻频道中，道德情绪框架(moral emotional framing)如何影响用户的参与行为。为捕捉平台的视觉与文本多模态特性，研究团队通过微调视觉语言模型(vision language model)开发了一个多模态道德情绪分类器，并在包含约40万个视频的大规模数据集上进行了分析。研究发现，谴责他人的道德愤慨(moral outrage)表达在不同文化背景下均能一致地提升从被动观看到主动评论的所有参与维度。结果表明，道德愤慨不仅是一种有效的吸引注意力的策略，更能显著增强用户的活跃参与和承诺度。研究者对该修辞可能强化内群体与外群体(in-group and out-group)对立并加剧社会极化的风险提出了警示。为了促进后续研究，该团队已公开其开发的韩语和英语多模态道德情绪分类器。",
      "categories": [
        "cs.CY",
        "cs.AI",
        "cs.CL",
        "cs.SI"
      ],
      "primary_category": "cs.CY",
      "comment": "Accepted at The Web Conference 2026. We release Korean and English multimodal moral emotion classifiers",
      "pdf_url": "https://arxiv.org/pdf/2601.21815v1",
      "published_date": "2026-01-29 14:58:54 UTC",
      "updated_date": "2026-01-29 14:58:54 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-30T04:23:40.908995+00:00"
    },
    {
      "arxiv_id": "2601.21812v1",
      "title": "A Decomposable Forward Process in Diffusion Models for Time-Series Forecasting",
      "title_zh": "用于时间序列预测的扩散模型可分解前向过程",
      "authors": [
        "Francisco Caldas",
        "Sahil Kumar",
        "Cláudia Soares"
      ],
      "abstract": "We introduce a model-agnostic forward diffusion process for time-series forecasting that decomposes signals into spectral components, preserving structured temporal patterns such as seasonality more effectively than standard diffusion. Unlike prior work that modifies the network architecture or diffuses directly in the frequency domain, our proposed method alters only the diffusion process itself, making it compatible with existing diffusion backbones (e.g., DiffWave, TimeGrad, CSDI). By staging noise injection according to component energy, it maintains high signal-to-noise ratios for dominant frequencies throughout the diffusion trajectory, thereby improving the recoverability of long-term patterns. This strategy enables the model to maintain the signal structure for a longer period in the forward process, leading to improved forecast quality. Across standard forecasting benchmarks, we show that applying spectral decomposition strategies, such as the Fourier or Wavelet transform, consistently improves upon diffusion models using the baseline forward process, with negligible computational overhead. The code for this paper is available at https://anonymous.4open.science/r/D-FDP-4A29.",
      "tldr_zh": "该研究提出了一种用于时间序列预测 (Time-Series Forecasting) 的模型无关前向扩散过程，旨在通过将信号分解为频谱分量来更有效地保留季节性等结构化时间模式。与修改网络架构或在频域扩散的现有方法不同，该方案仅改进扩散过程本身，从而能无缝兼容 DiffWave、TimeGrad 和 CSDI 等主流扩散模型 (Diffusion Models) 骨干。通过根据能量分阶段注入噪声，该方法在扩散轨迹中为主导频率维持了较高的信噪比，显著增强了模型恢复长期模式的能力。这种策略确保了信号结构在前向过程中被更持久地保留，从而直接提升了预测质量。在标准基准测试中，采用傅里叶变换 (Fourier transform) 或小波变换 (Wavelet transform) 的分解策略一致优于基准前向过程，且增加的计算开销几乎可以忽略不计。",
      "categories": [
        "stat.ML",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "stat.ML",
      "comment": "submitted to ICML'26",
      "pdf_url": "https://arxiv.org/pdf/2601.21812v1",
      "published_date": "2026-01-29 14:55:43 UTC",
      "updated_date": "2026-01-29 14:55:43 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-30T04:23:41.470303+00:00"
    },
    {
      "arxiv_id": "2601.21802v1",
      "title": "A Unified XAI-LLM Approach for EndotrachealSuctioning Activity Recognition",
      "title_zh": "面向气管内吸痰动作识别的 XAI-LLM 统一方法",
      "authors": [
        "Hoang Khang Phan",
        "Quang Vinh Dang",
        "Noriyo Colley",
        "Christina Garcia",
        "Nhat Tan Le"
      ],
      "abstract": "Endotracheal suctioning (ES) is an invasive yet essential clinical procedure that requires a high degree of skill to minimize patient risk - particularly in home care and educational settings, where consistent supervision may be limited. Despite its critical importance, automated recognition and feedback systems for ES training remain underexplored. To address this gap, this study proposes a unified, LLM-centered framework for video-based activity recognition benchmarked against conventional machine learning and deep learning approaches, and a pilot study on feedback generation. Within this framework, the Large Language Model (LLM) serves as the central reasoning module, performing both spatiotemporal activity recognition and explainable decision analysis from video data. Furthermore, the LLM is capable of verbalizing feedback in natural language, thereby translating complex technical insights into accessible, human-understandable guidance for trainees. Experimental results demonstrate that the proposed LLM-based approach outperforms baseline models, achieving an improvement of approximately 15-20\\% in both accuracy and F1 score. Beyond recognition, the framework incorporates a pilot student-support module built upon anomaly detection and explainable AI (XAI) principles, which provides automated, interpretable feedback highlighting correct actions and suggesting targeted improvements. Collectively, these contributions establish a scalable, interpretable, and data-driven foundation for advancing nursing education, enhancing training efficiency, and ultimately improving patient safety.",
      "tldr_zh": "该研究提出了一个以大语言模型(Large Language Model, LLM)为核心的统一框架，旨在实现气管内吸痰(Endotracheal Suctioning, ES)活动的视频识别与自动化反馈。针对护理教育中复杂操作缺乏实时指导的挑战，该框架将LLM作为中心推理模块，执行视频数据的时空动作识别与可解释性决策分析(Explainable Decision Analysis)。通过结合可解释人工智能(XAI)与异常检测原理，系统能够将复杂的技术洞察转化为易于理解的自然语言反馈，为受训者提供针对性的改进建议。实验结果表明，该方法在准确率和F1分数上优于传统机器学习与深度学习基准模型，性能提升约15-20%。这一贡献为推进护理教育信息化提供了可扩展且数据驱动的基础，对于提升医疗培训效率和保障患者安全具有显著价值。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2601.21802v1",
      "published_date": "2026-01-29 14:46:48 UTC",
      "updated_date": "2026-01-29 14:46:48 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-30T04:23:44.174608+00:00"
    },
    {
      "arxiv_id": "2601.21800v1",
      "title": "BioAgent Bench: An AI Agent Evaluation Suite for Bioinformatics",
      "title_zh": "BioAgent Bench：面向生物信息学的人工智能智能体评估套件",
      "authors": [
        "Dionizije Fa",
        "Marko Čuljak",
        "Bruno Pandža",
        "Mateo Čupić"
      ],
      "abstract": "This paper introduces BioAgent Bench, a benchmark dataset and an evaluation suite designed for measuring the performance and robustness of AI agents in common bioinformatics tasks. The benchmark contains curated end-to-end tasks (e.g., RNA-seq, variant calling, metagenomics) with prompts that specify concrete output artifacts to support automated assessment, including stress testing under controlled perturbations. We evaluate frontier closed-source and open-weight models across multiple agent harnesses, and use an LLM-based grader to score pipeline progress and outcome validity. We find that frontier agents can complete multi-step bioinformatics pipelines without elaborate custom scaffolding, often producing the requested final artifacts reliably. However, robustness tests reveal failure modes under controlled perturbations (corrupted inputs, decoy files, and prompt bloat), indicating that correct high-level pipeline construction does not guarantee reliable step-level reasoning. Finally, because bioinformatics workflows may involve sensitive patient data, proprietary references, or unpublished IP, closed-source models can be unsuitable under strict privacy constraints; in such settings, open-weight models may be preferable despite lower completion rates. We release the dataset and evaluation suite publicly.",
      "tldr_zh": "该研究推出了 BioAgent Bench，这是一个专为评估 AI 智能体 (AI agents) 在生物信息学任务中性能和鲁棒性而设计的基准数据集及评估套件。该基准涵盖了 RNA-seq、变异检测 (variant calling) 和宏基因组学 (metagenomics) 等端到端任务，通过具体的输出产物支持自动化评估，并包含了受控扰动下的压力测试。研究人员利用多种智能体框架对前沿的闭源模型和权重开放模型 (open-weight models) 进行了测试，并使用基于大语言模型 (LLM) 的评分器来评估分析流程的进度及结果的有效性。实验结果显示，前沿智能体无需复杂的定制化脚手架即可完成多步骤分析流程，并能可靠地生成最终产物。然而，鲁棒性测试揭示了模型在输入损坏、诱饵文件或冗余提示等扰动下的失效模式，表明高层流程的正确构建并不等同于可靠的步骤级推理。考虑到生物信息学涉及敏感隐私数据，研究指出在严格隐私约束下，权重开放模型尽管完成率较低，但相比闭源模型具有更高的适用性。该研究目前已公开发布了相关数据集和评估套件。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2601.21800v1",
      "published_date": "2026-01-29 14:44:03 UTC",
      "updated_date": "2026-01-29 14:44:03 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-30T04:23:50.765344+00:00"
    },
    {
      "arxiv_id": "2601.21796v1",
      "title": "KID: Knowledge-Injected Dual-Head Learning for Knowledge-Grounded Harmful Meme Detection",
      "title_zh": "KID：面向知识驱动型有害模因检测的知识注入双头学习",
      "authors": [
        "Yaocong Li",
        "Leihan Zhang",
        "Le Zhang",
        "Qiang Yan"
      ],
      "abstract": "Internet memes have become pervasive carriers of digital culture on social platforms. However, their heavy reliance on metaphors and sociocultural context also makes them subtle vehicles for harmful content, posing significant challenges for automated content moderation. Existing approaches primarily focus on intra-modal and inter-modal signal analysis, while the understanding of implicit toxicity often depends on background knowledge that is not explicitly present in the meme itself. To address this challenge, we propose KID, a Knowledge-Injected Dual-Head Learning framework for knowledge-grounded harmful meme detection. KID adopts a label-constrained distillation paradigm to decompose complex meme understanding into structured reasoning chains that explicitly link visual evidence, background knowledge, and classification labels. These chains guide the learning process by grounding external knowledge in meme-specific contexts. In addition, KID employs a dual-head architecture that jointly optimizes semantic generation and classification objectives, enabling aligned linguistic reasoning while maintaining stable decision boundaries. Extensive experiments on five multilingual datasets spanning English, Chinese, and low-resource Bengali demonstrate that KID achieves SOTA performance on both binary and multi-label harmful meme detection tasks, improving over previous best methods by 2.1%--19.7% across primary evaluation metrics. Ablation studies further confirm the effectiveness of knowledge injection and dual-head joint learning, highlighting their complementary contributions to robust and generalizable meme understanding. The code and data are available at https://github.com/PotatoDog1669/KID.",
      "tldr_zh": "该研究针对网络表情包(Internet memes)中由于隐喻和文化背景导致的隐性毒性识别难题，提出了KID框架，即一种用于知识增强型有害表情包检测的知识注入双头学习(Knowledge-Injected Dual-Head Learning)方案。KID采用了标签约束蒸馏(Label-constrained distillation)范式，将复杂的表情包理解分解为连接视觉证据、背景知识与分类标签的结构化推理链，从而将外部知识锚定在特定语境中。此外，该框架通过双头架构(Dual-head architecture)协同优化语义生成与分类任务，在维持稳定决策边界的同时增强了语言推理的对齐性。在涵盖英语、中文及低资源孟加拉语的五种多语言数据集上的实验结果显示，KID在二分类和多标签检测任务中均取得了SOTA性能，较现有最优方法提升了2.1%至19.7%。消融研究进一步证实了知识注入与双头联合学习在构建鲁棒且通用的表情包理解模型中的有效性与互补价值。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2601.21796v1",
      "published_date": "2026-01-29 14:41:36 UTC",
      "updated_date": "2026-01-29 14:41:36 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-30T04:23:52.459643+00:00"
    },
    {
      "arxiv_id": "2601.21795v1",
      "title": "Effective LoRA Adapter Routing using Task Representations",
      "title_zh": "基于任务表示的高效 LoRA 适配器路由",
      "authors": [
        "Akash Dhasade",
        "Anne-Marie Kermarrec",
        "Igor Pavlovic",
        "Diana Petrescu",
        "Rafael Pires",
        "Mathis Randl",
        "Martijn de Vos"
      ],
      "abstract": "Low-rank adaptation (LoRA) enables parameter efficient specialization of large language models (LLMs) through modular adapters, resulting in rapidly growing public adapter pools spanning diverse tasks. Effectively using these adapters requires routing: selecting and composing the appropriate adapters for a query. We introduce LORAUTER, a novel routing framework that selects and composes LoRA adapters using task representations rather than adapter characteristics. Unlike existing approaches that map queries directly to adapters, LORAUTER routes queries via task embeddings derived from small validation sets and does not require adapter training data. By operating at the task level, LORAUTER achieves efficient routing that scales with the number of tasks rather than the number of adapters. Experiments across multiple tasks show that LORAUTER consistently outperforms baseline routing approaches, matching Oracle performance (101.2%) when task-aligned adapters exist and achieving state-of-the-art results on unseen tasks (+5.2 points). We further demonstrate the robustness of LORAUTER to very large, noisy adapter pools by scaling it to over 1500 adapters.",
      "tldr_zh": "该研究提出了 LORAUTER，一种新型的 LoRA 适配器路由框架，旨在解决从日益增长的公共适配器池中选择和组合合适模块的挑战。与以往直接将查询映射到适配器的方法不同，LORAUTER 利用从少量验证集中提取的任务表示 (Task Representations) 进行路由，且不需要适配器的训练数据。通过在任务层面操作，该框架的路由效率随任务数量而非适配器数量扩展，具有更好的可扩展性。实验表明，LORAUTER 在多项任务上持续优于基线路由方法，在存在匹配适配器时达到了 Oracle 性能 (101.2%)，并在未知任务上取得了显著的性能提升。此外，该研究还通过扩展至 1500 多个适配器的实验，验证了 LORAUTER 在处理大型、含噪声适配器池时的鲁棒性。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2601.21795v1",
      "published_date": "2026-01-29 14:41:24 UTC",
      "updated_date": "2026-01-29 14:41:24 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-30T04:23:55.164948+00:00"
    },
    {
      "arxiv_id": "2601.21789v1",
      "title": "ECSEL: Explainable Classification via Signomial Equation Learning",
      "title_zh": "ECSEL：基于正项式方程学习的可解释分类",
      "authors": [
        "Adia Lumadjeng",
        "Ilker Birbil",
        "Erman Acar"
      ],
      "abstract": "We introduce ECSEL, an explainable classification method that learns formal expressions in the form of signomial equations, motivated by the observation that many symbolic regression benchmarks admit compact signomial structure. ECSEL directly constructs a structural, closed-form expression that serves as both a classifier and an explanation. On standard symbolic regression benchmarks, our method recovers a larger fraction of target equations than competing state-of-the-art approaches while requiring substantially less computation. Leveraging this efficiency, ECSEL achieves classification accuracy competitive with established machine learning models without sacrificing interpretability. Further, we show that ECSEL satisfies some desirable properties regarding global feature behavior, decision-boundary analysis, and local feature attributions. Experiments on benchmark datasets and two real-world case studies i.e., e-commerce and fraud detection, demonstrate that the learned equations expose dataset biases, support counterfactual reasoning, and yield actionable insights.",
      "tldr_zh": "该研究提出了 ECSEL，一种通过 Signomial Equation Learning 实现的可解释分类方法，旨在利用许多符号回归基准测试中存在的紧凑正项式结构。ECSEL 直接构建结构化的闭式表达式 (closed-form expression)，使其既能作为分类器，也能作为内在的解释。实验表明，在符号回归基准测试中，该方法以更低的计算成本实现了比现有最先进技术更高的目标方程恢复率，且分类准确率与主流机器学习模型相当。此外，ECSEL 在全局特征行为、决策边界分析和局部特征归因方面具备优异的属性。在电子商务和欺诈检测的案例研究中，该模型能够有效揭示数据集偏差，支持反事实推理 (counterfactual reasoning) 并生成可落地的洞察。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "stat.ML"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2601.21789v1",
      "published_date": "2026-01-29 14:35:43 UTC",
      "updated_date": "2026-01-29 14:35:43 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-30T04:23:54.840242+00:00"
    },
    {
      "arxiv_id": "2601.21787v1",
      "title": "Assessing the Business Process Modeling Competences of Large Language Models",
      "title_zh": "大语言模型业务流程建模能力评估",
      "authors": [
        "Chantale Lauer",
        "Peter Pfeiffer",
        "Alexander Rombach",
        "Nijat Mehdiyev"
      ],
      "abstract": "The creation of Business Process Model and Notation (BPMN) models is a complex and time-consuming task requiring both domain knowledge and proficiency in modeling conventions. Recent advances in large language models (LLMs) have significantly expanded the possibilities for generating BPMN models directly from natural language, building upon earlier text-to-process methods with enhanced capabilities in handling complex descriptions. However, there is a lack of systematic evaluations of LLM-generated process models. Current efforts either use LLM-as-a-judge approaches or do not consider established dimensions of model quality. To this end, we introduce BEF4LLM, a novel LLM evaluation framework comprising four perspectives: syntactic quality, pragmatic quality, semantic quality, and validity. Using BEF4LLM, we conduct a comprehensive analysis of open-source LLMs and benchmark their performance against human modeling experts. Results indicate that LLMs excel in syntactic and pragmatic quality, while humans outperform in semantic aspects; however, the differences in scores are relatively modest, highlighting LLMs' competitive potential despite challenges in validity and semantic quality. The insights highlight current strengths and limitations of using LLMs for BPMN modeling and guide future model development and fine-tuning. Addressing these areas is essential for advancing the practical deployment of LLMs in business process modeling.",
      "tldr_zh": "该研究评估了大语言模型 (LLMs) 在业务流程建模 (Business Process Modeling) 方面的能力，特别是在生成业务流程模型和符号 (BPMN) 方面的表现。为了填补现有系统评估的空白，作者提出了 BEF4LLM 评估框架，该框架从语法质量 (Syntactic quality)、语用质量 (Pragmatic quality)、语义质量 (Semantic quality) 和有效性 (Validity) 四个维度进行衡量。通过将开源 LLMs 与人类专家进行对比实验，研究发现 LLMs 在语法和语用质量方面表现优异，但在语义质量方面人类专家仍更胜一筹。尽管如此，两者之间的得分差异相对较小，突显了 LLMs 在流程建模领域的竞争潜力。该项工作揭示了 LLMs 在处理复杂描述时的当前优势与局限，为未来针对 BPMN 建模的模型开发和微调提供了重要指导。",
      "categories": [
        "cs.SE",
        "cs.AI"
      ],
      "primary_category": "cs.SE",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2601.21787v1",
      "published_date": "2026-01-29 14:34:20 UTC",
      "updated_date": "2026-01-29 14:34:20 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-30T04:24:03.909411+00:00"
    },
    {
      "arxiv_id": "2601.21786v1",
      "title": "Synthetic-to-Real Domain Bridging for Single-View 3D Reconstruction of Ships for Maritime Monitoring",
      "title_zh": "面向海事监测的船舶单视图三维重建：合成到真实的领域桥接",
      "authors": [
        "Borja Carrillo-Perez",
        "Felix Sattler",
        "Angel Bueno Rodriguez",
        "Maurice Stephan",
        "Sarah Barnes"
      ],
      "abstract": "Three-dimensional (3D) reconstruction of ships is an important part of maritime monitoring, allowing improved visualization, inspection, and decision-making in real-world monitoring environments. However, most state-ofthe-art 3D reconstruction methods require multi-view supervision, annotated 3D ground truth, or are computationally intensive, making them impractical for real-time maritime deployment. In this work, we present an efficient pipeline for single-view 3D reconstruction of real ships by training entirely on synthetic data and requiring only a single view at inference. Our approach uses the Splatter Image network, which represents objects as sparse sets of 3D Gaussians for rapid and accurate reconstruction from single images. The model is first fine-tuned on synthetic ShapeNet vessels and further refined with a diverse custom dataset of 3D ships, bridging the domain gap between synthetic and real-world imagery. We integrate a state-of-the-art segmentation module based on YOLOv8 and custom preprocessing to ensure compatibility with the reconstruction network. Postprocessing steps include real-world scaling, centering, and orientation alignment, followed by georeferenced placement on an interactive web map using AIS metadata and homography-based mapping. Quantitative evaluation on synthetic validation data demonstrates strong reconstruction fidelity, while qualitative results on real maritime images from the ShipSG dataset confirm the potential for transfer to operational maritime settings. The final system provides interactive 3D inspection of real ships without requiring real-world 3D annotations. This pipeline provides an efficient, scalable solution for maritime monitoring and highlights a path toward real-time 3D ship visualization in practical applications. Interactive demo: https://dlr-mi.github.io/ship3d-demo/.",
      "tldr_zh": "该研究针对海上监控中的三维重建难题，提出了一种从单视图(Single-view)进行船只3D重建的高效流程，旨在解决多视图监督和计算资源消耗大的问题。该方法核心采用了Splatter Image网络，将船舶表示为稀疏的3D高斯(3D Gaussians)集合，实现在单张图像上的快速准确重建。研究通过在ShapeNet和自定义3D船舶数据集上进行训练与微调，成功弥合了合成数据(Synthetic data)与真实世界(Real-world)之间的领域差异(Domain gap)。系统集成了基于YOLOv8的分割模块，并结合AIS元数据和单应性映射(Homography-based mapping)技术，将重建结果地理参考化并呈现在交互式Web地图上。实验结果表明，该系统在ShipSG真实海上数据集上展现了极高的应用潜力，无需真实世界3D标注即可实现交互式检测，为海上监控提供了可扩展的实时可视化解决方案。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.GR"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2601.21786v1",
      "published_date": "2026-01-29 14:34:01 UTC",
      "updated_date": "2026-01-29 14:34:01 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-30T04:24:09.306085+00:00"
    },
    {
      "arxiv_id": "2601.21771v1",
      "title": "Abstract Concept Modelling in Conceptual Spaces: A Study on Chess Strategies",
      "title_zh": "概念空间中的抽象概念建模：国际象棋策略研究",
      "authors": [
        "Hadi Banaee",
        "Stephanie Lowry"
      ],
      "abstract": "We present a conceptual space framework for modelling abstract concepts that unfold over time, demonstrated through a chess-based proof-of-concept. Strategy concepts, such as attack or sacrifice, are represented as geometric regions across interpretable quality dimensions, with chess games instantiated and analysed as trajectories whose directional movement toward regions enables recognition of intended strategies. This approach also supports dual-perspective modelling, capturing how players interpret identical situations differently. Our implementation demonstrates the feasibility of trajectory-based concept recognition, with movement patterns aligning with expert commentary. This work explores extending the conceptual spaces theory to temporally realised, goal-directed concepts. The approach establishes a foundation for broader applications involving sequential decision-making and supports integration with knowledge evolution mechanisms for learning and refining abstract concepts over time.",
      "tldr_zh": "该研究提出了一个在 Conceptual Spaces 框架下建模随时间演变的抽象概念的方法，并以国际象棋策略作为概念验证。研究将 attack 或 sacrifice 等策略概念表示为可解释质量维度中的几何区域，通过将棋局建模为移动轨迹并分析其向特定区域的靠近程度来实现策略识别。该框架支持双重视角建模，能够捕捉不同参与者对同一情境的差异化解读。实验结果证明了基于轨迹的概念识别的可行性，其识别出的运动模式与专家评论高度一致。这项工作将 Conceptual Spaces 理论成功扩展到了目标导向的动态概念领域，为涉及序列决策和知识演化机制的更广泛应用奠定了基础。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2601.21771v1",
      "published_date": "2026-01-29 14:22:43 UTC",
      "updated_date": "2026-01-29 14:22:43 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-30T04:24:11.092482+00:00"
    },
    {
      "arxiv_id": "2601.21766v1",
      "title": "CoFrGeNet: Continued Fraction Architectures for Language Generation",
      "title_zh": "CoFrGeNet：面向语言生成的连分数架构",
      "authors": [
        "Amit Dhurandhar",
        "Vijil Chenthamarakshan",
        "Dennis Wei",
        "Tejaswini Pedapati",
        "Karthikeyan Natesan Ramamurthy",
        "Rahul Nair"
      ],
      "abstract": "Transformers are arguably the preferred architecture for language generation. In this paper, inspired by continued fractions, we introduce a new function class for generative modeling. The architecture family implementing this function class is named CoFrGeNets - Continued Fraction Generative Networks. We design novel architectural components based on this function class that can replace Multi-head Attention and Feed-Forward Networks in Transformer blocks while requiring much fewer parameters. We derive custom gradient formulations to optimize the proposed components more accurately and efficiently than using standard PyTorch-based gradients. Our components are a plug-in replacement requiring little change in training or inference procedures that have already been put in place for Transformer-based models thus making our approach easy to incorporate in large industrial workflows. We experiment on two very different transformer architectures GPT2-xl (1.5B) and Llama3 (3.2B), where the former we pre-train on OpenWebText and GneissWeb, while the latter we pre-train on the docling data mix which consists of nine different datasets. Results show that the performance on downstream classification, Q\\& A, reasoning and text understanding tasks of our models is competitive and sometimes even superior to the original models with $\\frac{2}{3}$ to $\\frac{1}{2}$ the parameters and shorter pre-training time. We believe that future implementations customized to hardware will further bring out the true potential of our architectures.",
      "tldr_zh": "该研究受连分数 (Continued Fractions) 启发，提出了一种名为 CoFrGeNets (Continued Fraction Generative Networks) 的新型生成建模架构。该架构通过设计全新的组件来替代 Transformer 模块中的多头注意力 (Multi-head Attention) 和前馈网络 (Feed-Forward Networks)，在显著减少参数量的同时保持了强大的生成能力。为了提升优化效率，研究者推导了专门的梯度公式，且该组件作为插件式替代方案，能够无缝集成到现有的工业化训练和推理流程中。实验在 GPT2-xl (1.5B) 和 Llama3 (3.2B) 架构上进行了大规模预训练验证，涵盖了分类、问答 (Q&A)、推理及文本理解等多种下游任务。结果表明，CoFrGeNets 在仅使用原始模型 1/2 至 2/3 参数量以及更短预训练时间的情况下，性能表现极具竞争力甚至在部分任务中更优。这一成果为开发低参数、高效率且易于硬件定制化的语言生成架构提供了新方向。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2601.21766v1",
      "published_date": "2026-01-29 14:16:39 UTC",
      "updated_date": "2026-01-29 14:16:39 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-30T04:24:12.687692+00:00"
    },
    {
      "arxiv_id": "2601.21760v1",
      "title": "Zero-Shot Statistical Downscaling via Diffusion Posterior Sampling",
      "title_zh": "基于扩散后验采样的零样本统计降尺度",
      "authors": [
        "Ruian Tie",
        "Wenbo Xiong",
        "Zhengyu Shi",
        "Xinyu Su",
        "Chenyu jiang",
        "Libo Wu",
        "Hao Li"
      ],
      "abstract": "Conventional supervised climate downscaling struggles to generalize to Global Climate Models (GCMs) due to the lack of paired training data and inherent domain gaps relative to reanalysis. Meanwhile, current zero-shot methods suffer from physical inconsistencies and vanishing gradient issues under large scaling factors. We propose Zero-Shot Statistical Downscaling (ZSSD), a zero-shot framework that performs statistical downscaling without paired data during training. ZSSD leverages a Physics-Consistent Climate Prior learned from reanalysis data, conditioned on geophysical boundaries and temporal information to enforce physical validity. Furthermore, to enable robust inference across varying GCMs, we introduce Unified Coordinate Guidance. This strategy addresses the vanishing gradient problem in vanilla DPS and ensures consistency with large-scale fields. Results show that ZSSD significantly outperforms existing zero-shot baselines in 99th percentile errors and successfully reconstructs complex weather events, such as tropical cyclones, across heterogeneous GCMs.",
      "tldr_zh": "该研究提出了ZSSD，一种通过Diffusion Posterior Sampling实现零样本统计降尺度的框架，旨在解决传统监督学习方法由于缺乏配对训练数据且在Global Climate Models (GCMs)上泛化性差的问题。ZSSD利用从再分析数据中学习到的Physics-Consistent Climate Prior，并结合地球物理边界与时间信息来确保生成结果的物理有效性。为了应对大缩放因子下的梯度消失问题并保证与大尺度场的一致性，该研究引入了Unified Coordinate Guidance引导策略。实验结果表明，ZSSD在99分位数误差指标上显著优于现有的零样本基线模型。此外，该框架能够成功在多种异构的GCMs中重建热带气旋等复杂天气事件，证明了其在处理极端气候现象方面的鲁棒性与物理一致性。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2601.21760v1",
      "published_date": "2026-01-29 14:14:41 UTC",
      "updated_date": "2026-01-29 14:14:41 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-30T04:24:16.003775+00:00"
    },
    {
      "arxiv_id": "2601.21758v1",
      "title": "EWSJF: An Adaptive Scheduler with Hybrid Partitioning for Mixed-Workload LLM Inference",
      "title_zh": "EWSJF：面向混合负载大语言模型推理的混合分区自适应调度器",
      "authors": [
        "Bronislav Sidik",
        "Chaya Levi",
        "Joseph Kampeas"
      ],
      "abstract": "Serving Large Language Models (LLMs) under mixed workloads--short, latency-sensitive interactive queries alongside long, throughput-oriented batch requests--poses a fundamental scheduling challenge. Standard First-Come, First-Served (FCFS) policies suffer from severe head-of-line blocking, leading to high tail latency and underutilized hardware. We introduce EWSJF (Effective Workload-based Shortest Job First), an adaptive request-level scheduler that learns workload structure in real time to jointly improve fairness and throughput. EWSJF operates upstream of execution-level schedulers and integrates four components: (1) Refine-and-Prune, an unsupervised partitioning algorithm that discovers performance-homogeneous request groups; (2) Dynamic Queue Routing for assigning requests to these groups; (3) Density-Weighted Scoring, a context-aware prioritization function balancing urgency and fairness; and (4) Bayesian Meta-Optimization, which continuously tunes scoring and partitioning parameters based on live performance feedback. Implemented in vLLM, EWSJF improves end-to-end throughput by over 30% and reduces average Time-To-First-Token for short requests by up to 4x compared to FCFS. These results demonstrate that adaptive, learning-based request scheduling is a critical missing layer for efficient and responsive LLM serving. Implementation available at https://anonymous.4open.science/r/vllm_0110-32D8.",
      "tldr_zh": "该研究提出了EWSJF (Effective Workload-based Shortest Job First)，这是一种针对混合工作负载的大语言模型(LLMs)推理自适应调度器，旨在解决传统FCFS策略在处理短交互查询与长批量请求时出现的队首阻塞和高尾延迟问题。该框架在执行级调度器之上运行，集成了Refine-and-Prune聚类算法、动态队列路由以及密度加权评分(Density-Weighted Scoring)机制，以实现请求的精准分类与优先级平衡。此外，EWSJF利用贝叶斯元优化(Bayesian Meta-Optimization)根据实时性能反馈动态调整参数，确保了系统的持续高效运行。在vLLM上的实现结果表明，该调度器将端到端吞吐量提升了30%以上，并将短请求的平均首字时间(Time-To-First-Token)降低了多达4倍。该研究证明了自适应、基于学习的请求调度是提升大语言模型服务效率与响应能力的关键技术。",
      "categories": [
        "cs.DC",
        "cs.AI"
      ],
      "primary_category": "cs.DC",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2601.21758v1",
      "published_date": "2026-01-29 14:14:16 UTC",
      "updated_date": "2026-01-29 14:14:16 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-30T04:24:16.379138+00:00"
    },
    {
      "arxiv_id": "2601.21754v1",
      "title": "Language-based Trial and Error Falls Behind in the Era of Experience",
      "title_zh": "经验时代下基于语言的试错法已显落后",
      "authors": [
        "Haoyu Wang",
        "Guozheng Ma",
        "Shugang Cui",
        "Yilun Kong",
        "Haotian Luo",
        "Li Shen",
        "Mengya Gao",
        "Yichao Wu",
        "Xiaogang Wang",
        "Dacheng Tao"
      ],
      "abstract": "While Large Language Models (LLMs) excel in language-based agentic tasks, their applicability to unseen, nonlinguistic environments (e.g., symbolic or spatial tasks) remains limited. Previous work attributes this performance gap to the mismatch between the pretraining distribution and the testing distribution. In this work, we demonstrate the primary bottleneck is the prohibitive cost of exploration: mastering these tasks requires extensive trial-and-error, which is computationally unsustainable for parameter-heavy LLMs operating in a high dimensional semantic space. To address this, we propose SCOUT (Sub-Scale Collaboration On Unseen Tasks), a novel framework that decouples exploration from exploitation. We employ lightweight \"scouts\" (e.g., small MLPs) to probe environmental dynamics at a speed and scale far exceeding LLMs. The collected trajectories are utilized to bootstrap the LLM via Supervised Fine-Tuning (SFT), followed by multi-turn Reinforcement Learning (RL) to activate its latent world knowledge. Empirically, SCOUT enables a Qwen2.5-3B-Instruct model to achieve an average score of 0.86, significantly outperforming proprietary models, including Gemini-2.5-Pro (0.60), while saving about 60% GPU hours consumption.",
      "tldr_zh": "该研究指出，大语言模型 (LLMs) 在处理未见过的非语言环境（如符号或空间任务）时面临的主要瓶颈在于高维语义空间中试错探索 (exploration) 的成本过高。为此，作者提出了 SCOUT (Sub-Scale Collaboration On Unseen Tasks) 框架，通过将探索与利用 (exploitation) 过程解耦来优化性能。该框架利用轻量级的“侦察员” (scouts) 以极高的速度和规模探测环境动态，并将收集到的轨迹通过有监督微调 (Supervised Fine-Tuning, SFT) 和多轮强化学习 (Reinforcement Learning, RL) 来激活 LLM 潜在的世界知识。实验结果显示，SCOUT 使得 Qwen2.5-3B-Instruct 模型在相关任务中获得了 0.86 的平均分，显著优于 Gemini-2.5-Pro 等专有模型。此外，该方案在提升表现的同时还节省了约 60% 的 GPU 小时消耗，为在复杂环境中高效应用 LLMs 提供了新的视角。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2601.21754v1",
      "published_date": "2026-01-29 14:08:41 UTC",
      "updated_date": "2026-01-29 14:08:41 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-30T04:24:23.992536+00:00"
    },
    {
      "arxiv_id": "2601.21747v1",
      "title": "Temporal Sepsis Modeling: a Fully Interpretable Relational Way",
      "title_zh": "时序脓毒症建模：一种完全可解释的关系型方法",
      "authors": [
        "Vincent Lemaire",
        "Nédra Meloulli",
        "Pierre Jaquet"
      ],
      "abstract": "Sepsis remains one of the most complex and heterogeneous syndromes in intensive care, characterized by diverse physiological trajectories and variable responses to treatment. While deep learning models perform well in the early prediction of sepsis, they often lack interpretability and ignore latent patient sub-phenotypes. In this work, we propose a machine learning framework by opening up a new avenue for addressing this issue: a relational approach. Temporal data from electronic medical records (EMRs) are viewed as multivariate patient logs and represented in a relational data schema. Then, a propositionalisation technique (based on classic aggregation/selection functions from the field of relational data) is applied to construct interpretable features to \"flatten\" the data. Finally, the flattened data is classified using a selective naive Bayesian classifier. Experimental validation demonstrates the relevance of the suggested approach as well as its extreme interpretability. The interpretation is fourfold: univariate, global, local, and counterfactual.",
      "tldr_zh": "该研究针对重症监护中脓毒症 (Sepsis) 的复杂性与异质性，以及深度学习模型缺乏可解释性且忽略潜在子表型的问题，提出了一种基于关系方法的时序建模框架。该框架将电子病历 (EMR) 中的时序数据视为多元患者日志并以关系数据模式表示，通过命题化 (Propositionalisation) 技术利用聚合与选择函数构建可解释特征以实现数据“打平”。随后，研究采用选择性朴素贝叶斯分类器 (Selective Naive Bayesian Classifier) 对扁平化数据进行分类。实验结果证明了该方法的有效性，并展示了其在单变量、全局、局部及反事实 (Counterfactual) 四个维度的极高可解释性。该方案为处理复杂的临床异构数据提供了一种全透明且关系化的新路径。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2601.21747v1",
      "published_date": "2026-01-29 14:02:26 UTC",
      "updated_date": "2026-01-29 14:02:26 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-30T04:24:24.209407+00:00"
    },
    {
      "arxiv_id": "2601.21742v1",
      "title": "Epistemic Context Learning: Building Trust the Right Way in LLM-Based Multi-Agent Systems",
      "title_zh": "认知上下文学习：基于大语言模型的多智能体系统中正确建立信任的范式",
      "authors": [
        "Ruiwen Zhou",
        "Maojia Song",
        "Xiaobao Wu",
        "Sitao Cheng",
        "Xunjian Yin",
        "Yuxi Xie",
        "Zhuoqun Hao",
        "Wenyue Hua",
        "Liangming Pan",
        "Soujanya Poria",
        "Min-Yen Kan"
      ],
      "abstract": "Individual agents in multi-agent (MA) systems often lack robustness, tending to blindly conform to misleading peers. We show this weakness stems from both sycophancy and inadequate ability to evaluate peer reliability. To address this, we first formalize the learning problem of history-aware reference, introducing the historical interactions of peers as additional input, so that agents can estimate peer reliability and learn from trustworthy peers when uncertain. This shifts the task from evaluating peer reasoning quality to estimating peer reliability based on interaction history. We then develop Epistemic Context Learning (ECL): a reasoning framework that conditions predictions on explicitly-built peer profiles from history. We further optimize ECL by reinforcement learning using auxiliary rewards. Our experiments reveal that our ECL enables small models like Qwen 3-4B to outperform a history-agnostic baseline 8x its size (Qwen 3-30B) by accurately identifying reliable peers. ECL also boosts frontier models to near-perfect (100%) performance. We show that ECL generalizes well to various MA configurations and we find that trust is modeled well by LLMs, revealing a strong correlation in trust modeling accuracy and final answer quality.",
      "tldr_zh": "该研究针对多智能体(Multi-Agent)系统中个体智能体由于阿谀奉承(sycophancy)和缺乏可靠性评估能力而导致鲁棒性不足的问题，提出了认知上下文学习(Epistemic Context Learning, ECL)推理框架。ECL通过引入历史交互信息来显式构建同行画像(peer profiles)，使智能体能够基于历史表现估算同行的可靠性，从而在不确定时向可信的同行学习。为了进一步提升效果，研究团队采用强化学习(Reinforcement Learning)辅以辅助奖励对ECL进行优化，将任务核心从单纯的推理质量评估转向对同行可靠性的精准建模。实验结果表明，应用ECL的Qwen 3-4B模型在性能上超越了规模大其8倍的基准模型，并在尖端模型上实现了近乎100%的准确率。研究发现信任建模在大型语言模型(LLMs)中得到了良好的体现，且信任建模的准确性与最终任务结果的质量之间存在显著的正相关关系。",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.MA"
      ],
      "primary_category": "cs.AI",
      "comment": "Codes and data are available at https://github.com/skyriver-2000/epistemic-context-learning",
      "pdf_url": "https://arxiv.org/pdf/2601.21742v1",
      "published_date": "2026-01-29 13:59:32 UTC",
      "updated_date": "2026-01-29 13:59:32 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-30T04:24:24.643491+00:00"
    },
    {
      "arxiv_id": "2601.21739v1",
      "title": "Why Adam Works Better with $β_1 = β_2$: The Missing Gradient Scale Invariance Principle",
      "title_zh": "为什么 Adam 在 $\\beta_1 = \\beta_2$ 时表现更优：缺失的梯度尺度不变性原理",
      "authors": [
        "Alberto Fernández-Hernández",
        "Cristian Pérez-Corral",
        "Jose I. Mestre",
        "Manuel F. Dolz",
        "Enrique S. Quintana-Ortí"
      ],
      "abstract": "Adam has been at the core of large-scale training for almost a decade, yet a simple empirical fact remains unaccounted for: both validation scores and the qualitative behaviour of the training runs improve when the momentum parameters satisfy $β_{1}=β_{2}$. Some recent studies have reported this pattern, but there is still no explanation for why this choice helps. We show that this choice is closely tied to a structural property that we refer to as \\textit{gradient scale invariance}. We formalize this notion and prove that Adam becomes gradient scale invariant of first order if and only if $β_{1}=β_{2}$. This perspective places the balanced regime of Adam in direct alignment with the design principles underlying several recent optimizers that explicitly enforce scale-robust updates. The theory is supported by experiments across vision and language tasks, and across different architectural families, in which rescaling the gradient has a markedly smoother effect on the update when $β_{1}=β_{2}$. Overall, our results offer a coherent explanation for an open question in the behavior of Adam and provide a simple principle that helps guide the design of future optimizers.",
      "tldr_zh": "该研究针对 Adam 优化器在 $β_1 = β_2$ 时训练效果更佳的经验事实，提出了梯度尺度不变性 (Gradient Scale Invariance) 原理作为理论解释。作者通过形式化证明指出，Adam 能够实现一阶梯度尺度不变性的充要条件是其动量参数满足 $β_1 = β_2$。这一发现将 Adam 的平衡状态与近期一些强调尺度鲁棒更新 (Scale-robust Updates) 的优化器设计准则统一了起来。跨越视觉与语言任务、以及多种模型架构的实验表明，在 $β_1 = β_2$ 的设置下，梯度缩放对参数更新的影响更加平滑且稳定。该项工作不仅解答了关于 Adam 行为的一个长期开放性问题，还为未来优化器的设计提供了简单而有效的指导原则。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "stat.ML"
      ],
      "primary_category": "cs.LG",
      "comment": "23 pages, 8 figures. Preprint",
      "pdf_url": "https://arxiv.org/pdf/2601.21739v1",
      "published_date": "2026-01-29 13:56:11 UTC",
      "updated_date": "2026-01-29 13:56:11 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-30T04:24:37.520746+00:00"
    },
    {
      "arxiv_id": "2601.21738v1",
      "title": "From Global to Granular: Revealing IQA Model Performance via Correlation Surface",
      "title_zh": "从全局到细粒度：通过相关性曲面揭示 IQA 模型性能",
      "authors": [
        "Baoliang Chen",
        "Danni Huang",
        "Hanwei Zhu",
        "Lingyu Zhu",
        "Wei Zhou",
        "Shiqi Wang",
        "Yuming Fang",
        "Weisi Lin"
      ],
      "abstract": "Evaluation of Image Quality Assessment (IQA) models has long been dominated by global correlation metrics, such as Pearson Linear Correlation Coefficient (PLCC) and Spearman Rank-Order Correlation Coefficient (SRCC). While widely adopted, these metrics reduce performance to a single scalar, failing to capture how ranking consistency varies across the local quality spectrum. For example, two IQA models may achieve identical SRCC values, yet one ranks high-quality images (related to high Mean Opinion Score, MOS) more reliably, while the other better discriminates image pairs with small quality/MOS differences (related to $|Δ$MOS$|$). Such complementary behaviors are invisible under global metrics. Moreover, SRCC and PLCC are sensitive to test-sample quality distributions, yielding unstable comparisons across test sets. To address these limitations, we propose \\textbf{Granularity-Modulated Correlation (GMC)}, which provides a structured, fine-grained analysis of IQA performance. GMC includes: (1) a \\textbf{Granularity Modulator} that applies Gaussian-weighted correlations conditioned on absolute MOS values and pairwise MOS differences ($|Δ$MOS$|$) to examine local performance variations, and (2) a \\textbf{Distribution Regulator} that regularizes correlations to mitigate biases from non-uniform quality distributions. The resulting \\textbf{correlation surface} maps correlation values as a joint function of MOS and $|Δ$MOS$|$, providing a 3D representation of IQA performance. Experiments on standard benchmarks show that GMC reveals performance characteristics invisible to scalar metrics, offering a more informative and reliable paradigm for analyzing, comparing, and deploying IQA models. Codes are available at https://github.com/Dniaaa/GMC.",
      "tldr_zh": "该研究针对图像质量评价（IQA）模型评估中过度依赖 Pearson Linear Correlation Coefficient (PLCC) 和 Spearman Rank-Order Correlation Coefficient (SRCC) 等全局标量指标的局限性，指出这些指标无法捕捉模型在特定质量区间或微小质量差异下的性能变化。为此，作者提出了 Granularity-Modulated Correlation (GMC) 框架，旨在提供更结构化且细粒度的性能分析。该框架包含 Granularity Modulator，通过高斯加权相关性考察模型在不同 Mean Opinion Score (MOS) 及其差值 $|Δ$MOS$|$ 下的局部表现；同时包含 Distribution Regulator，用于减轻非均匀质量分布导致的评估偏差。最终生成的 correlation surface 以三维形式直观展示了模型性能的波动，揭示了传统指标难以发现的模型特性。实验表明，GMC 在标准基准测试中展现了更强的解释力，为 IQA 模型的深入分析、公平比较及实际部署提供了一个更可靠的评估范式。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2601.21738v1",
      "published_date": "2026-01-29 13:55:26 UTC",
      "updated_date": "2026-01-29 13:55:26 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-30T04:24:49.294617+00:00"
    },
    {
      "arxiv_id": "2601.21726v1",
      "title": "DropoutTS: Sample-Adaptive Dropout for Robust Time Series Forecasting",
      "title_zh": "DropoutTS：面向鲁棒时间序列预测的样本自适应 Dropout",
      "authors": [
        "Siru Zhong",
        "Yiqiu Liu",
        "Zhiqing Cui",
        "Zezhi Shao",
        "Fei Wang",
        "Qingsong Wen",
        "Yuxuan Liang"
      ],
      "abstract": "Deep time series models are vulnerable to noisy data ubiquitous in real-world applications. Existing robustness strategies either prune data or rely on costly prior quantification, failing to balance effectiveness and efficiency. In this paper, we introduce DropoutTS, a model-agnostic plugin that shifts the paradigm from \"what\" to learn to \"how much\" to learn. DropoutTS employs a Sample-Adaptive Dropout mechanism: leveraging spectral sparsity to efficiently quantify instance-level noise via reconstruction residuals, it dynamically calibrates model learning capacity by mapping noise to adaptive dropout rates - selectively suppressing spurious fluctuations while preserving fine-grained fidelity. Extensive experiments across diverse noise regimes and open benchmarks show DropoutTS consistently boosts superior backbones' performance, delivering advanced robustness with negligible parameter overhead and no architectural modifications. Our code is available at https://github.com/CityMind-Lab/DropoutTS.",
      "tldr_zh": "该研究提出了DropoutTS，这是一种模型无关(model-agnostic)的插件，旨在增强深层时间序列模型在面对现实噪声数据时的鲁棒性。该方法引入了样本自适应Dropout(Sample-Adaptive Dropout)机制，将研究重点从“学习什么”转向“学习多少”，从而动态调节模型的学习能力。DropoutTS利用频谱稀疏性(spectral sparsity)通过重构残差(reconstruction residuals)高效量化实例级噪声，并将噪声水平映射为自适应Dropout率。这种机制能够选择性地抑制伪波动(spurious fluctuations)，同时保留数据的细粒度特征。在多种噪声环境和公开基准测试中的实验表明，DropoutTS能够持续提升主流骨干模型的性能。该插件在不改变原有架构且参数开销极小的情况下，为时间序列预测提供了先进的稳健性保障。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2601.21726v1",
      "published_date": "2026-01-29 13:49:20 UTC",
      "updated_date": "2026-01-29 13:49:20 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-30T04:24:48.938975+00:00"
    },
    {
      "arxiv_id": "2601.21722v1",
      "title": "Enhancing Language Models for Robust Greenwashing Detection",
      "title_zh": "增强语言模型以实现稳健的漂绿检测",
      "authors": [
        "Neil Heinrich Braun",
        "Keane Ong",
        "Rui Mao",
        "Erik Cambria",
        "Gianmarco Mengaldo"
      ],
      "abstract": "Sustainability reports are critical for ESG assessment, yet greenwashing and vague claims often undermine their reliability. Existing NLP models lack robustness to these practices, typically relying on surface-level patterns that generalize poorly. We propose a parameter-efficient framework that structures LLM latent spaces by combining contrastive learning with an ordinal ranking objective to capture graded distinctions between concrete actions and ambiguous claims. Our approach incorporates gated feature modulation to filter disclosure noise and utilizes MetaGradNorm to stabilize multi-objective optimization. Experiments in cross-category settings demonstrate superior robustness over standard baselines while revealing a trade-off between representational rigidity and generalization.",
      "tldr_zh": "该研究针对ESG评估中可持续发展报告存在的漂绿(Greenwashing)及模糊陈述问题，指出当前NLP模型在检测这些行为时缺乏鲁棒性且泛化能力较差。为此，作者提出了一种参数高效(Parameter-efficient)的框架，通过结合对比学习(Contrastive Learning)和序数排名(Ordinal Ranking)目标来优化LLM的潜空间(Latent Spaces)，以准确捕捉具体行动与模糊主张之间的分级差异。该框架引入了门控特征调制(Gated Feature Modulation)来过滤披露噪声，并利用MetaGradNorm稳定多目标优化。跨类别实验证明，该方法在鲁棒性上显著优于标准基线。研究最后揭示了表示刚性(Representational Rigidity)与泛化性能之间的权衡关系，为增强语言模型在漂绿检测中的表现提供了有效方案。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2601.21722v1",
      "published_date": "2026-01-29 13:46:15 UTC",
      "updated_date": "2026-01-29 13:46:15 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-30T04:25:50.383954+00:00"
    },
    {
      "arxiv_id": "2601.21718v1",
      "title": "When does predictive inverse dynamics outperform behavior cloning?",
      "title_zh": "预测逆动力学何时优于行为克隆？",
      "authors": [
        "Lukas Schäfer",
        "Pallavi Choudhury",
        "Abdelhak Lemkhenter",
        "Chris Lovett",
        "Somjit Nath",
        "Luis França",
        "Matheus Ribeiro Furtado de Mendonça",
        "Alex Lamb",
        "Riashat Islam",
        "Siddhartha Sen",
        "John Langford",
        "Katja Hofmann",
        "Sergio Valcarcel Macua"
      ],
      "abstract": "Behavior cloning (BC) is a practical offline imitation learning method, but it often fails when expert demonstrations are limited. Recent works have introduced a class of architectures named predictive inverse dynamics models (PIDM) that combine a future state predictor with an inverse dynamics model (IDM). While PIDM often outperforms BC, the reasons behind its benefits remain unclear. In this paper, we provide a theoretical explanation: PIDM introduces a bias-variance tradeoff. While predicting the future state introduces bias, conditioning the IDM on the prediction can significantly reduce variance. We establish conditions on the state predictor bias for PIDM to achieve lower prediction error and higher sample efficiency than BC, with the gap widening when additional data sources are available. We validate the theoretical insights empirically in 2D navigation tasks, where BC requires up to five times (three times on average) more demonstrations than PIDM to reach comparable performance; and in a complex 3D environment in a modern video game with high-dimensional visual inputs and stochastic transitions, where BC requires over 66\\% more samples than PIDM.",
      "tldr_zh": "该研究探讨了在模仿学习中预测逆动力学模型(Predictive Inverse Dynamics Models, PIDM)优于行为克隆(Behavior Cloning, BC)的深层原因，并提出了一个基于偏差-方差权衡(bias-variance tradeoff)的理论解释。研究表明，虽然预测未来状态会引入偏差，但将逆动力学模型(IDM)建立在预测状态之上能显著降低方差。作者确定了PIDM在预测误差和样本效率方面优于BC的理论条件，并指出在拥有额外数据源的情况下，PIDM的优势会进一步扩大。通过在二维导航任务和具有高维视觉输入的复杂三维游戏环境中的实验验证，结果显示BC平均需要比PIDM多出数倍的演示数据才能达到相似的性能水平。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "Preprint",
      "pdf_url": "https://arxiv.org/pdf/2601.21718v1",
      "published_date": "2026-01-29 13:43:34 UTC",
      "updated_date": "2026-01-29 13:43:34 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-30T04:24:54.684019+00:00"
    },
    {
      "arxiv_id": "2601.21716v1",
      "title": "DreamActor-M2: Universal Character Image Animation via Spatiotemporal In-Context Learning",
      "title_zh": "DreamActor-M2：基于时空上下文学习的通用角色图像动画",
      "authors": [
        "Mingshuang Luo",
        "Shuang Liang",
        "Zhengkun Rong",
        "Yuxuan Luo",
        "Tianshu Hu",
        "Ruibing Hou",
        "Hong Chang",
        "Yong Li",
        "Yuan Zhang",
        "Mingyuan Gao"
      ],
      "abstract": "Character image animation aims to synthesize high-fidelity videos by transferring motion from a driving sequence to a static reference image. Despite recent advancements, existing methods suffer from two fundamental challenges: (1) suboptimal motion injection strategies that lead to a trade-off between identity preservation and motion consistency, manifesting as a \"see-saw\", and (2) an over-reliance on explicit pose priors (e.g., skeletons), which inadequately capture intricate dynamics and hinder generalization to arbitrary, non-humanoid characters. To address these challenges, we present DreamActor-M2, a universal animation framework that reimagines motion conditioning as an in-context learning problem. Our approach follows a two-stage paradigm. First, we bridge the input modality gap by fusing reference appearance and motion cues into a unified latent space, enabling the model to jointly reason about spatial identity and temporal dynamics by leveraging the generative prior of foundational models. Second, we introduce a self-bootstrapped data synthesis pipeline that curates pseudo cross-identity training pairs, facilitating a seamless transition from pose-dependent control to direct, end-to-end RGB-driven animation. This strategy significantly enhances generalization across diverse characters and motion scenarios. To facilitate comprehensive evaluation, we further introduce AW Bench, a versatile benchmark encompassing a wide spectrum of characters types and motion scenarios. Extensive experiments demonstrate that DreamActor-M2 achieves state-of-the-art performance, delivering superior visual fidelity and robust cross-domain generalization. Project Page: https://grisoon.github.io/DreamActor-M2/",
      "tldr_zh": "该研究提出 DreamActor-M2，这是一个通用的角色图像动画框架，旨在解决现有方法在身份保持与动作一致性之间的平衡难题，以及对骨架等显式姿态先验的过度依赖问题。该框架将动作调节重新定义为上下文学习(In-Context Learning)问题，通过两阶段范式实现从驱动序列到静态参考图像的运动迁移。第一阶段，模型将参考外观与运动线索融合进统一的潜空间(Latent Space)，利用基础模型的生成先验共同推理空间身份与时间动态。第二阶段，研究引入了一种自引导的数据合成流水线，通过生成伪跨身份训练对，实现了从依赖姿态的控制向直接的端到端 RGB 驱动动画的平滑过渡。这种策略显著增强了模型在多样化角色（包括非人类角色）和动作场景中的泛化能力。此外，研究团队还推出了 AW Bench，这是一个涵盖广泛角色类型和运动场景的通用评测基准。实验结果表明，DreamActor-M2 在视觉逼真度和跨领域泛化方面达到了最先进(State-of-the-art)的水平。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2601.21716v1",
      "published_date": "2026-01-29 13:43:17 UTC",
      "updated_date": "2026-01-29 13:43:17 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-30T04:24:56.068004+00:00"
    },
    {
      "arxiv_id": "2601.21714v1",
      "title": "E-mem: Multi-agent based Episodic Context Reconstruction for LLM Agent Memory",
      "title_zh": "E-mem：基于多智能体的 LLM 智能体记忆情景式上下文重建",
      "authors": [
        "Kaixiang Wang",
        "Yidan Lin",
        "Jiong Lou",
        "Zhaojiacheng Zhou",
        "Bunyod Suvonov",
        "Jie Li"
      ],
      "abstract": "The evolution of Large Language Model (LLM) agents towards System~2 reasoning, characterized by deliberative, high-precision problem-solving, requires maintaining rigorous logical integrity over extended horizons. However, prevalent memory preprocessing paradigms suffer from destructive de-contextualization. By compressing complex sequential dependencies into pre-defined structures (e.g., embeddings or graphs), these methods sever the contextual integrity essential for deep reasoning. To address this, we propose E-mem, a framework shifting from Memory Preprocessing to Episodic Context Reconstruction. Inspired by biological engrams, E-mem employs a heterogeneous hierarchical architecture where multiple assistant agents maintain uncompressed memory contexts, while a central master agent orchestrates global planning. Unlike passive retrieval, our mechanism empowers assistants to locally reason within activated segments, extracting context-aware evidence before aggregation. Evaluations on the LoCoMo benchmark demonstrate that E-mem achieves over 54\\% F1, surpassing the state-of-the-art GAM by 7.75\\%, while reducing token cost by over 70\\%.",
      "tldr_zh": "该研究提出了 E-mem 框架，旨在解决大型语言模型 (LLM) 智能体在 System 2 推理过程中因传统记忆预处理导致的上下文去语境化问题。该框架受生物记忆痕迹 (engrams) 启发，实现了从记忆预处理向情节上下文重构 (Episodic Context Reconstruction) 的范式转变，以维护长时程推理中的逻辑完整性。E-mem 采用一种异构分层架构，由多个助理智能体 (assistant agents) 维护未压缩的记忆上下文，并由中央主智能体 (master agent) 协调全局规划。不同于被动检索，该机制允许助理智能体在激活的记忆片段内进行本地推理，在聚合前提取具有上下文感知的证据。实验结果表明，E-mem 在 LoCoMo 基准测试中达到了超过 54% 的 F1 值，比 SOTA 模型 GAM 提高了 7.75%，同时将 Token 成本降低了 70% 以上。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "18 pages",
      "pdf_url": "https://arxiv.org/pdf/2601.21714v1",
      "published_date": "2026-01-29 13:42:42 UTC",
      "updated_date": "2026-01-29 13:42:42 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-30T04:25:00.026849+00:00"
    },
    {
      "arxiv_id": "2601.21713v1",
      "title": "Disentangling perception and reasoning for improving data efficiency in learning cloth manipulation without demonstrations",
      "title_zh": "解耦感知与推理以提升无演示布料操作学习的数据效率",
      "authors": [
        "Donatien Delehelle",
        "Fei Chen",
        "Darwin Caldwell"
      ],
      "abstract": "Cloth manipulation is a ubiquitous task in everyday life, but it remains an open challenge for robotics. The difficulties in developing cloth manipulation policies are attributed to the high-dimensional state space, complex dynamics, and high propensity to self-occlusion exhibited by fabrics. As analytical methods have not been able to provide robust and general manipulation policies, reinforcement learning (RL) is considered a promising approach to these problems. However, to address the large state space and complex dynamics, data-based methods usually rely on large models and long training times. The resulting computational cost significantly hampers the development and adoption of these methods. Additionally, due to the challenge of robust state estimation, garment manipulation policies often adopt an end-to-end learning approach with workspace images as input. While this approach enables a conceptually straightforward sim-to-real transfer via real-world fine-tuning, it also incurs a significant computational cost by training agents on a highly lossy representation of the environment state. This paper questions this common design choice by exploring an efficient and modular approach to RL for cloth manipulation. We show that, through careful design choices, model size and training time can be significantly reduced when learning in simulation. Furthermore, we demonstrate how the resulting simulation-trained model can be transferred to the real world. We evaluate our approach on the SoftGym benchmark and achieve significant performance improvements over available baselines on our task, while using a substantially smaller model.",
      "tldr_zh": "该研究针对机器人布料操作(cloth manipulation)中面临的高维状态空间、复杂动力学和严重自遮挡等挑战，提出了一种旨在提高数据效率的模块化强化学习(RL)方法。该研究质疑了传统直接以图像为输入的端到端(end-to-end)学习范式，转而通过解耦感知(perception)与推理(reasoning)来优化学习过程。通过精心的模块化设计，该方法在仿真环境中显著缩短了训练时间并大幅减小了模型规模，且无需依赖人类演示(demonstrations)即可学习有效的操作策略。此外，研究展示了将仿真训练的模型迁移至真实世界的可行性。在 SoftGym 基准测试上的评估结果表明，该方案在模型体积远小于现有基准模型(baselines)的情况下，实现了显著的性能提升，为低计算成本的机器人复杂任务学习提供了新途径。",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "comment": "6 pages, 4 figures,",
      "pdf_url": "https://arxiv.org/pdf/2601.21713v1",
      "published_date": "2026-01-29 13:41:35 UTC",
      "updated_date": "2026-01-29 13:41:35 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-30T04:25:02.502017+00:00"
    },
    {
      "arxiv_id": "2601.21711v1",
      "title": "TACLer: Tailored Curriculum Reinforcement Learning for Efficient Reasoning",
      "title_zh": "TACLer：面向高效推理的定制化课程强化学习",
      "authors": [
        "Huiyuan Lai",
        "Malvina Nissim"
      ],
      "abstract": "Large Language Models (LLMs) have shown remarkable performance on complex reasoning tasks, especially when equipped with long chain-of-thought (CoT) reasoning. However, eliciting long CoT typically requires large-scale reinforcement learning (RL) training, while often leading to overthinking with redundant intermediate steps. To improve learning and reasoning efficiency, while preserving or even enhancing performance, we propose TACLer, a model-tailored curriculum reinforcement learning framework that gradually increases the complexity of the data based on the model's proficiency in multi-stage RL training. TACLer features two core components: (i) tailored curriculum learning that determines what knowledge the model lacks and needs to learn in progressive stages; (ii) a hybrid Thinking/NoThinking reasoning paradigm that balances accuracy and efficiency by enabling or disabling the Thinking mode. Our experiments show that TACLer yields a twofold advantage in learning and reasoning: (i) it reduces computational cost, cutting training compute by over 50% compared to long thinking models and reducing inference token usage by over 42% relative to the base model; and (ii) it improves accuracy by over 9% on the base model, consistently outperforming state-of-the-art Nothinking and Thinking baselines across four math datasets with complex problems.",
      "tldr_zh": "该研究提出了 TACLer，一种模型定制的课程强化学习 (Tailored Curriculum Reinforcement Learning) 框架，旨在解决大语言模型 (LLMs) 在复杂推理中因长链式思维 (Chain-of-Thought) 导致的计算冗余与过度思考问题。TACLer 通过多阶段强化学习根据模型熟练度逐步增加数据复杂度，并结合了定制化课程学习与混合 Thinking/NoThinking 推理范式，有效平衡了推理的准确性与效率。实验证明，该框架相较于传统长思考模型可节省超过 50% 的训练计算量和 42% 的推理 token 使用量。在四个数学数据集的测试中，TACLer 将基础模型的准确率提升了 9% 以上，且一致优于现有的各类基准模型。这一研究为实现高效且可扩展的大模型推理能力提供了创新的解决方案。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2601.21711v1",
      "published_date": "2026-01-29 13:40:35 UTC",
      "updated_date": "2026-01-29 13:40:35 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-30T04:25:19.846617+00:00"
    },
    {
      "arxiv_id": "2601.21708v1",
      "title": "FBS: Modeling Native Parallel Reading inside a Transformer",
      "title_zh": "FBS：Transformer 内部的原生并行阅读建模",
      "authors": [
        "Tongxi Wang"
      ],
      "abstract": "Large language models (LLMs) excel across many tasks, yet inference is still dominated by strictly token-by-token autoregression. Existing acceleration methods largely patch this pipeline and miss core human-reading ingredients: content-adaptive foresight, chunk-structure-aware compute allocation, and train--test consistency for preview/skimming. We propose the \\textbf{Fovea-Block-Skip Transformer} (FBS), which injects a causal, trainable loop into Transformers via Parafovea-Attention Window (PAW), Chunk-Head (CH), and Skip-Gate (SG). Across diverse benchmarks, FBS improves the quality-efficiency trade-off without increasing parameters, and ablations show the three modules are complementary.",
      "tldr_zh": "该研究针对 Large language models (LLMs) 推理中逐个词元(token-by-token)自回归方式的局限性，提出了 Fovea-Block-Skip Transformer (FBS) 模型，旨在将人类的原生并行阅读机制融入 Transformer 架构。FBS 通过引入 Parafovea-Attention Window (PAW)、Chunk-Head (CH) 和 Skip-Gate (SG) 三个关键组件，在模型中注入了一个因果且可训练的循环，以实现内容自适应的预见性和块结构感知的计算分配。这些模块能够模拟人类阅读时的副黄斑预警及略读行为，在不增加额外参数量的情况下，显著优化了模型在推理质量与效率之间的权衡。实验结果表明，FBS 在多种基准测试中均表现优异，消融研究证实了 PAW、CH 与 SG 模块之间的互补性，为加速 LLMs 推理提供了一种新颖且高效的结构化方案。",
      "categories": [
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2601.21708v1",
      "published_date": "2026-01-29 13:39:55 UTC",
      "updated_date": "2026-01-29 13:39:55 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-30T04:25:07.640578+00:00"
    },
    {
      "arxiv_id": "2601.21700v1",
      "title": "Toward Culturally Aligned LLMs through Ontology-Guided Multi-Agent Reasoning",
      "title_zh": "通过本体引导的多智能体推理实现大语言模型的文化对齐",
      "authors": [
        "Wonduk Seo",
        "Wonseok Choi",
        "Junseo Koh",
        "Juhyeon Lee",
        "Hyunjin An",
        "Minhyeong Yu",
        "Jian Park",
        "Qingshan Zhou",
        "Seunghyun Lee",
        "Yi Bu"
      ],
      "abstract": "Large Language Models (LLMs) increasingly support culturally sensitive decision making, yet often exhibit misalignment due to skewed pretraining data and the absence of structured value representations. Existing methods can steer outputs, but often lack demographic grounding and treat values as independent, unstructured signals, reducing consistency and interpretability. We propose OG-MAR, an Ontology-Guided Multi-Agent Reasoning framework. OG-MAR summarizes respondent-specific values from the World Values Survey (WVS) and constructs a global cultural ontology by eliciting relations over a fixed taxonomy via competency questions. At inference time, it retrieves ontology-consistent relations and demographically similar profiles to instantiate multiple value-persona agents, whose outputs are synthesized by a judgment agent that enforces ontology consistency and demographic proximity. Experiments on regional social-survey benchmarks across four LLM backbones show that OG-MAR improves cultural alignment and robustness over competitive baselines, while producing more transparent reasoning traces.",
      "tldr_zh": "该研究提出了OG-MAR，一种本体引导的多智能体推理(Ontology-Guided Multi-Agent Reasoning)框架，旨在解决大型语言模型(LLMs)因预训练数据偏见和缺乏结构化价值表示而导致的文化失调问题。该框架首先通过从世界价值观调查(World Values Survey, WVS)中总结特定价值，并利用能力问题在固定分类法上诱导出逻辑关系，从而构建全局文化本体(global cultural ontology)。在推理过程中，OG-MAR检索本体一致的关系和人口统计学相似的配置文件，以此实例化多个价值-人格智能体(value-persona agents)。最终，由评审智能体(judgment agent)综合这些智能体的输出，并强制执行本体一致性和人口统计学邻近性。在多个区域社会调查基准和四种LLM骨干模型上的实验证明，OG-MAR在文化对齐(cultural alignment)和鲁棒性方面显著优于现有基线。此外，该方法还提供了更具透明度的推理轨迹(reasoning traces)，有效增强了决策过程的可解释性。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.IR",
        "cs.MA",
        "cs.SI"
      ],
      "primary_category": "cs.CL",
      "comment": "35 pages",
      "pdf_url": "https://arxiv.org/pdf/2601.21700v1",
      "published_date": "2026-01-29 13:31:45 UTC",
      "updated_date": "2026-01-29 13:31:45 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-30T04:25:10.552226+00:00"
    },
    {
      "arxiv_id": "2601.21698v1",
      "title": "Curriculum Learning for LLM Pretraining: An Analysis of Learning Dynamics",
      "title_zh": "大语言模型预训练中的课程学习：学习动力学分析",
      "authors": [
        "Mohamed Elgaar",
        "Hadi Amiri"
      ],
      "abstract": "Curriculum learning changes the order of pre-training data, but it remains unclear whether it changes the learning trajectory or mainly reorders exposure over a fixed trajectory. We train Pythia models (14M-410M parameters) for 300B tokens under three linguistically motivated curricula-Age-of-Acquisition, word frequency, and Verb Variation (VV)-and compare each against Random ordering; at 1B parameters we compare Random and VV. Across orderings, training follows a shared sequence of latent phases, while curricula mainly change within-phase data exposure. In smaller models (up to 160M parameters), Random ordering exhibits higher gradient noise and stronger late-training output-head spectral saturation, alongside lower final accuracy; curricula reduce both effects at matched compute. At larger scales, saturation differences are smaller and curriculum gains shrink. We formalize the link between difficulty pacing and optimization stability in an idealized analysis based on gradient-variance control, and our results point to a practical takeaway: curricula help by stabilizing within-phase optimization rather than by creating new phases.",
      "tldr_zh": "该研究探讨了在大型语言模型 (LLM) 预训练中，课程学习 (Curriculum Learning) 是改变了学习轨迹还是仅仅在固定轨迹上重新排列了数据暴露。通过在 300B tokens 上训练 14M 至 1B 参数规模的 Pythia 模型，并对比 Age-of-Acquisition、词频和 Verb Variation (VV) 等课程与随机排序，研究发现模型训练遵循一套共享的潜在阶段序列。课程学习主要通过稳定阶段内的优化过程而非创建新阶段来发挥作用，在 160M 以下的小规模模型中，它能显著降低梯度噪声 (gradient noise) 和输出头光谱饱和 (output-head spectral saturation) 并提升准确率。然而，随着模型规模扩大至 1B 参数，课程学习带来的性能增益和优化稳定性差异会逐渐缩小。该研究通过基于梯度方差控制的理想化分析，形式化地揭示了难度节奏与优化稳定性之间的内在联系。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2601.21698v1",
      "published_date": "2026-01-29 13:30:18 UTC",
      "updated_date": "2026-01-29 13:30:18 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-30T04:26:04.146493+00:00"
    },
    {
      "arxiv_id": "2601.21692v1",
      "title": "TCAP: Tri-Component Attention Profiling for Unsupervised Backdoor Detection in MLLM Fine-Tuning",
      "title_zh": "TCAP：面向 MLLM 微调无监督后门检测的三组件注意力剖析",
      "authors": [
        "Mingzu Liu",
        "Hao Fang",
        "Runmin Cong"
      ],
      "abstract": "Fine-Tuning-as-a-Service (FTaaS) facilitates the customization of Multimodal Large Language Models (MLLMs) but introduces critical backdoor risks via poisoned data. Existing defenses either rely on supervised signals or fail to generalize across diverse trigger types and modalities. In this work, we uncover a universal backdoor fingerprint-attention allocation divergence-where poisoned samples disrupt the balanced attention distribution across three functional components: system instructions, vision inputs, and user textual queries, regardless of trigger morphology. Motivated by this insight, we propose Tri-Component Attention Profiling (TCAP), an unsupervised defense framework to filter backdoor samples. TCAP decomposes cross-modal attention maps into the three components, identifies trigger-responsive attention heads via Gaussian Mixture Model (GMM) statistical profiling, and isolates poisoned samples through EM-based vote aggregation. Extensive experiments across diverse MLLM architectures and attack methods demonstrate that TCAP achieves consistently strong performance, establishing it as a robust and practical backdoor defense in MLLMs.",
      "tldr_zh": "该研究针对多模态大语言模型 (MLLMs) 在微调即服务 (FTaaS) 中面临的后门攻击风险，指出当前的防御手段往往依赖监督信号或难以泛化。研究人员发现了一种通用的后门指纹，即“注意力分配差异” (attention allocation divergence)，其表现为中毒样本会破坏系统指令、视觉输入和用户文本查询这三个功能组件之间的注意力平衡。基于此发现，研究提出了名为 TCAP (Tri-Component Attention Profiling) 的无监督防御框架，通过将跨模态注意力图分解为三个组件，并利用高斯混合模型 (GMM) 识别受触发器影响的注意力头。随后，该框架采用基于 EM 算法的投票聚合机制来精准隔离中毒样本。在多种模型架构和攻击方法下的实验证明，TCAP 具有极强的鲁棒性与实用性，为 MLLM 微调提供了可靠的无监督后门检测方案。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2601.21692v1",
      "published_date": "2026-01-29 13:26:29 UTC",
      "updated_date": "2026-01-29 13:26:29 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-30T04:26:05.528396+00:00"
    },
    {
      "arxiv_id": "2601.21688v1",
      "title": "XFACTORS: Disentangled Information Bottleneck via Contrastive Supervision",
      "title_zh": "XFACTORS：基于对比监督的解耦信息瓶颈",
      "authors": [
        "Alexandre Myara",
        "Nicolas Bourriez",
        "Thomas Boyer",
        "Thomas Lemercier",
        "Ihab Bendidi",
        "Auguste Genovesio"
      ],
      "abstract": "Disentangled representation learning aims to map independent factors of variation to independent representation components. On one hand, purely unsupervised approaches have proven successful on fully disentangled synthetic data, but fail to recover semantic factors from real data without strong inductive biases. On the other hand, supervised approaches are unstable and hard to scale to large attribute sets because they rely on adversarial objectives or auxiliary classifiers.\n  We introduce \\textsc{XFactors}, a weakly-supervised VAE framework that disentangles and provides explicit control over a chosen set of factors. Building on the Disentangled Information Bottleneck perspective, we decompose the representation into a residual subspace $\\mathcal{S}$ and factor-specific subspaces $\\mathcal{T}_1,\\ldots,\\mathcal{T}_K$ and a residual subspace $\\mathcal{S}$. Each target factor is encoded in its assigned $\\mathcal{T}_i$ through contrastive supervision: an InfoNCE loss pulls together latents sharing the same factor value and pushes apart mismatched pairs. In parallel, KL regularization imposes a Gaussian structure on both $\\mathcal{S}$ and the aggregated factor subspaces, organizing the geometry without additional supervision for non-targeted factors and avoiding adversarial training and classifiers.\n  Across multiple datasets, with constant hyperparameters, \\textsc{XFactors} achieves state-of-the-art disentanglement scores and yields consistent qualitative factor alignment in the corresponding subspaces, enabling controlled factor swapping via latent replacement. We further demonstrate that our method scales correctly with increasing latent capacity and evaluate it on the real-world dataset CelebA. Our code is available at \\href{https://github.com/ICML26-anon/XFactors}{github.com/ICML26-anon/XFactors}.",
      "tldr_zh": "该研究提出了 XFactors，一种弱监督的 VAE 框架，旨在解决解耦表示学习 Disentangled representation learning 中纯无监督方法在真实数据上表现不佳，以及有监督方法因依赖对抗目标或分类器而难以扩展且不稳定的问题。该方法基于解耦信息瓶颈 Disentangled Information Bottleneck 视角，将表示空间分解为残差子空间 $\\mathcal{S}$ 和多个特定因子的子空间 $\\mathcal{T}_i$。通过对比监督 Contrastive supervision 和 InfoNCE 损失函数，XFactors 实现了对目标因子的显式控制，并利用 KL 正则化在子空间上施加高斯结构，从而避免了复杂的对抗训练。实验证明，该框架在多个数据集上取得了最先进的解耦分数 Disentanglement scores，并能通过潜在表示替换实现受控的因子交换。此外，XFactors 在 CelebA 等真实世界数据集上展现了良好的扩展性，为稳定且可扩展的特征解耦提供了有效的解决方案。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2601.21688v1",
      "published_date": "2026-01-29 13:20:48 UTC",
      "updated_date": "2026-01-29 13:20:48 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-30T04:26:06.789214+00:00"
    },
    {
      "arxiv_id": "2601.21682v1",
      "title": "FIT: Defying Catastrophic Forgetting in Continual LLM Unlearning",
      "title_zh": "FIT：在持续大语言模型遗忘中克服灾难性遗忘",
      "authors": [
        "Xiaoyu Xu",
        "Minxin Du",
        "Kun Fang",
        "Zi Liang",
        "Yaxin Xiao",
        "Zhicong Huang",
        "Cheng Hong",
        "Qingqing Ye",
        "Haibo Hu"
      ],
      "abstract": "Large language models (LLMs) demonstrate impressive capabilities across diverse tasks but raise concerns about privacy, copyright, and harmful materials. Existing LLM unlearning methods rarely consider the continual and high-volume nature of real-world deletion requests, which can cause utility degradation and catastrophic forgetting as requests accumulate. To address this challenge, we introduce \\fit, a framework for continual unlearning that handles large numbers of deletion requests while maintaining robustness against both catastrophic forgetting and post-unlearning recovery. \\fit mitigates degradation through rigorous data \\underline{F}iltering, \\underline{I}mportance-aware updates, and \\underline{T}argeted layer attribution, enabling stable performance across long sequences of unlearning operations and achieving a favorable balance between forgetting effectiveness and utility retention. To support realistic evaluation, we present \\textbf{PCH}, a benchmark covering \\textbf{P}ersonal information, \\textbf{C}opyright, and \\textbf{H}armful content in sequential deletion scenarios, along with two symmetric metrics, Forget Degree (F.D.) and Retain Utility (R.U.), which jointly assess forgetting quality and utility preservation. Extensive experiments on four open-source LLMs with hundreds of deletion requests show that \\fit achieves the strongest trade-off between F.D. and R.U., surpasses existing methods on MMLU, CommonsenseQA, and GSM8K, and remains resistant against both relearning and quantization recovery attacks.",
      "tldr_zh": "该研究针对大语言模型(LLMs)在处理持续且大量的删除请求时容易出现的效用下降和灾难性遗忘(Catastrophic Forgetting)问题，提出了名为FIT的持续取消学习框架。该框架通过严格的数据过滤(Filtering)、重要性感知更新(Importance-aware updates)和针对性层归因(Targeted layer attribution)技术，有效缓解了模型性能退化并确保了长序列操作下的稳定性。为支持真实场景的评估，研究者推出了涵盖个人信息、版权和有害内容的PCH基准测试，并引入遗忘程度(F.D.)与保留效用(R.U.)两项指标来共同衡量模型表现。在四种开源LLM上的实验表明，FIT在遗忘效果与效用保留之间达到了最优平衡，在MMLU和GSM8K等基准测试中的表现显著优于现有方法。此外，该框架在应对重新学习和量化恢复攻击时展现出极强的鲁棒性，为实现安全、合规且持续的LLM知识擦除提供了有效的解决方案。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.CR",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "20 Pages",
      "pdf_url": "https://arxiv.org/pdf/2601.21682v1",
      "published_date": "2026-01-29 13:15:32 UTC",
      "updated_date": "2026-01-29 13:15:32 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-30T04:26:10.129624+00:00"
    },
    {
      "arxiv_id": "2601.21669v1",
      "title": "Expected Return Causes Outcome-Level Mode Collapse in Reinforcement Learning and How to Fix It with Inverse Probability Scaling",
      "title_zh": "强化学习中期望回报引发的结果级模式坍缩及其逆概率缩放修正方法",
      "authors": [
        "Abhijeet Sinha",
        "Sundari Elango",
        "Dianbo Liu"
      ],
      "abstract": "Many reinforcement learning (RL) problems admit multiple terminal solutions of comparable quality, where the goal is not to identify a single optimum but to represent a diverse set of high-quality outcomes. Nevertheless, policies trained by standard expected return maximization routinely collapse onto a small subset of outcomes, a phenomenon commonly attributed to insufficient exploration or weak regularization. We show that this explanation is incomplete: outcome level mode collapse is a structural consequence of the expected-return objective itself. Under idealized learning dynamics, the log-probability ratio between any two outcomes evolves linearly in their reward difference, implying exponential ratio divergence and inevitable collapse independent of the exploration strategy, entropy regularization, or optimization algorithm. We identify the source of this pathology as the probability multiplier inside the expectation and propose a minimal correction: inverse probability scaling, which removes outcome-frequency amplification from the learning signal, fundamentally changes the learning dynamics, and provably yields reward-proportional terminal distributions, preventing collapse in multimodal settings. We instantiate this principle in Group Relative Policy Optimization (GRPO) as a drop-in modification, IPS-GRPO, requiring no auxiliary models or architectural changes. Across different reasoning and molecular generation tasks, IPS-GRPO consistently reduces outcome-level mode collapse while matching or exceeding baseline performance, suggesting that correcting the objective rather than adding exploration heuristics is key to reliable multimodal policy optimization.",
      "tldr_zh": "该研究探讨了强化学习 (Reinforcement Learning) 中常见的输出层级模式崩溃 (Outcome-Level Mode Collapse) 问题，即尽管存在多种高质量解决方案，模型往往仅收敛于极少数结果。研究证明，这种模式崩溃并非仅由探索不足或正则化弱引起，而是预期回报 (Expected Return) 目标函数本身的结构性缺陷导致的必然结果。在理想学习动态下，不同输出间的对数概率比会随奖励差值线性演变，从而导致指数级的比例发散和不可避免的塌缩。为此，作者提出了逆概率缩放 (Inverse Probability Scaling, IPS) 方法，通过消除学习信号中的输出频率放大效应，从根本上改变学习动态并产生与奖励成比例的终端分布。该方法作为插件式改进应用于 Group Relative Policy Optimization (GRPO) 形成 IPS-GRPO，无需额外的辅助模型或架构更改。在推理和分子生成任务中的实验表明，IPS-GRPO 在保持甚至超越基线性能的同时显著减少了模式崩溃，证明了修正目标函数本身比增加探索启发式方法对实现可靠的多模态策略优化更为关键。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2601.21669v1",
      "published_date": "2026-01-29 13:03:33 UTC",
      "updated_date": "2026-01-29 13:03:33 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-30T04:26:09.905478+00:00"
    },
    {
      "arxiv_id": "2601.21666v1",
      "title": "SONIC-O1: A Real-World Benchmark for Evaluating Multimodal Large Language Models on Audio-Video Understanding",
      "title_zh": "SONIC-O1：用于评估多模态大语言模型音视频理解能力的真实场景基准",
      "authors": [
        "Ahmed Y. Radwan",
        "Christos Emmanouilidis",
        "Hina Tabassum",
        "Deval Pandya",
        "Shaina Raza"
      ],
      "abstract": "Multimodal Large Language Models (MLLMs) are a major focus of recent AI research. However, most prior work focuses on static image understanding, while their ability to process sequential audio-video data remains underexplored. This gap highlights the need for a high-quality benchmark to systematically evaluate MLLM performance in a real-world setting. We introduce SONIC-O1, a comprehensive, fully human-verified benchmark spanning 13 real-world conversational domains with 4,958 annotations and demographic metadata. SONIC-O1 evaluates MLLMs on key tasks, including open-ended summarization, multiple-choice question (MCQ) answering, and temporal localization with supporting rationales (reasoning). Experiments on closed- and open-source models reveal limitations. While the performance gap in MCQ accuracy between two model families is relatively small, we observe a substantial 22.6% performance difference in temporal localization between the best performing closed-source and open-source models. Performance further degrades across demographic groups, indicating persistent disparities in model behavior. Overall, SONIC-O1 provides an open evaluation suite for temporally grounded and socially robust multimodal understanding. We release SONIC-O1 for reproducibility and research: Project page: https://vectorinstitute.github.io/sonic-o1/ Dataset: https://huggingface.co/datasets/vector-institute/sonic-o1 Github: https://github.com/vectorinstitute/sonic-o1 Leaderboard: https://huggingface.co/spaces/vector-institute/sonic-o1-leaderboard",
      "tldr_zh": "该研究针对多模态大语言模型 (MLLMs) 在处理序列化视听数据能力探索不足的问题，推出了 SONIC-O1 这一真实场景下的高质量评估基准。SONIC-O1 涵盖了 13 个现实对话领域，包含 4,958 条经人工验证的标注信息及人口统计元数据，旨在从开放式摘要 (Summarization)、多选题解答 (MCQ answering) 以及带推理支持的时间定位 (Temporal localization) 等维度对模型进行系统测试。实验结果显示，尽管闭源与开源模型在多选题任务上的表现差距较小，但在时间定位任务中，顶尖闭源模型与开源模型之间存在 22.6% 的显著性能差异。此外，研究发现模型性能在不同人口统计群体间存在波动，揭示了当前模型在社会稳健性方面仍存在持续性差异。总体而言，SONIC-O1 为开发具备时间感知能力和社会鲁棒性的多模态理解系统提供了重要的开源评估工具。",
      "categories": [
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2601.21666v1",
      "published_date": "2026-01-29 13:01:07 UTC",
      "updated_date": "2026-01-29 13:01:07 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-30T04:26:15.227573+00:00"
    },
    {
      "arxiv_id": "2601.21664v1",
      "title": "SENDAI: A Hierarchical Sparse-measurement, EfficieNt Data AssImilation Framework",
      "title_zh": "SENDAI：一种分层式稀疏测量高效数据同化框架",
      "authors": [
        "Xingyue Zhang",
        "Yuxuan Bao",
        "Mars Liyao Gao",
        "J. Nathan Kutz"
      ],
      "abstract": "Bridging the gap between data-rich training regimes and observation-sparse deployment conditions remains a central challenge in spatiotemporal field reconstruction, particularly when target domains exhibit distributional shifts, heterogeneous structure, and multi-scale dynamics absent from available training data. We present SENDAI, a hierarchical Sparse-measurement, EfficieNt Data AssImilation Framework that reconstructs full spatial states from hyper sparse sensor observations by combining simulation-derived priors with learned discrepancy corrections. We demonstrate the performance on satellite remote sensing, reconstructing MODIS (Moderate Resolution Imaging Spectroradiometer) derived vegetation index fields across six globally distributed sites. Using seasonal periods as a proxy for domain shift, the framework consistently outperforms established baselines that require substantially denser observations -- SENDAI achieves a maximum SSIM improvement of 185% over traditional baselines and a 36% improvement over recent high-frequency-based methods. These gains are particularly pronounced for landscapes with sharp boundaries and sub-seasonal dynamics; more importantly, the framework effectively preserves diagnostically relevant structures -- such as field topologies, land cover discontinuities, and spatial gradients. By yielding corrections that are more structurally and spectrally separable, the reconstructed fields are better suited for downstream inference of indirectly observed variables. The results therefore highlight a lightweight and operationally viable framework for sparse-measurement reconstruction that is applicable to physically grounded inference, resource-limited deployment, and real-time monitor and control.",
      "tldr_zh": "该研究提出了 SENDAI，一种层级化的稀疏测量高效数据同化框架 (Hierarchical Sparse-measurement, EfficieNt Data AssImilation Framework)，旨在解决时空场重建中观测稀疏、分布偏移及多尺度动力学等核心挑战。该框架通过结合基于模拟的先验信息 (simulation-derived priors) 与学习到的偏差修正 (learned discrepancy corrections)，实现了从极稀疏传感器观测中重建完整空间状态的目标。在 MODIS 卫星遥感植被指数场的重建实验中，SENDAI 在全球六个站点表现显著优于现有基线，其 SSIM 指标最高提升了 185%，展现了极强的鲁棒性。实验证明该框架能有效保留场拓扑 (field topologies)、土地覆盖不连续性和空间梯度等具有诊断价值的结构，特别适用于边界清晰和动态复杂的景观重建。作为一种轻量化且具备操作可行性的方案，SENDAI 为资源受限环境下的物理推断、实时监测与控制提供了可靠的技术支撑。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CE"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2601.21664v1",
      "published_date": "2026-01-29 12:58:54 UTC",
      "updated_date": "2026-01-29 12:58:54 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-30T04:26:19.261957+00:00"
    },
    {
      "arxiv_id": "2601.21654v1",
      "title": "ScholarGym: Benchmarking Deep Research Workflows on Academic Literature Retrieval",
      "title_zh": "ScholarGym：学术文献检索深度研究工作流的基准测评",
      "authors": [
        "Hao Shen",
        "Hang Yang",
        "Zhouhong Gu"
      ],
      "abstract": "Tool-augmented large language models have advanced from single-turn question answering to deep research workflows that iteratively plan queries, invoke external tools, and synthesize information to address complex information needs. Evaluating such workflows presents a fundamental challenge: reliance on live APIs introduces non-determinism, as tool invocations may yield different results across runs due to temporal drift, rate limiting, and evolving backend states. This variance undermines reproducibility and invalidates cross-system comparisons.\n  We present ScholarGym, a simulation environment for reproducible evaluation of deep research workflows on academic literature. The environment decouples workflow components into query planning, tool invocation, and relevance assessment, enabling fine-grained analysis of each stage under controlled conditions. Built on a static corpus of 570K papers with deterministic retrieval, ScholarGym provides 2,536 queries with expert-annotated ground truth. Experiments across diverse backbone models reveal how reasoning capabilities, planning strategies, and selection mechanisms interact over iterative refinement.",
      "tldr_zh": "该研究提出了 ScholarGym，一个专门用于学术文献检索深度研究工作流 (deep research workflows) 可重复评估的模拟环境，旨在解决基于实时 API 评估时因非确定性因素导致的不可复现挑战。该环境将工作流组件解耦为查询规划 (query planning)、工具调用 (tool invocation) 和相关性评估 (relevance assessment)，支持在受控条件下对工具增强型大语言模型 (tool-augmented large language models) 进行精细化分析。ScholarGym 基于包含 57 万篇论文的静态语料库，提供确定性检索以及 2,536 个带有专家标注 ground truth 的查询。通过对多种主干模型的实验，研究揭示了推理能力、规划策略和选择机制在迭代优化过程中的相互作用。该研究为学术检索领域的自动化研究流程提供了标准化的基准测试，有效促进了复杂信息寻回任务中系统间的公平比较。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2601.21654v1",
      "published_date": "2026-01-29 12:51:44 UTC",
      "updated_date": "2026-01-29 12:51:44 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-30T04:26:19.642836+00:00"
    },
    {
      "arxiv_id": "2601.21653v1",
      "title": "Gauge-invariant representation holonomy",
      "title_zh": "规范不变表征和乐",
      "authors": [
        "Vasileios Sevetlidis",
        "George Pavlidis"
      ],
      "abstract": "Deep networks learn internal representations whose geometry--how features bend, rotate, and evolve--affects both generalization and robustness. Existing similarity measures such as CKA or SVCCA capture pointwise overlap between activation sets, but miss how representations change along input paths. Two models may appear nearly identical under these metrics yet respond very differently to perturbations or adversarial stress. We introduce representation holonomy, a gauge-invariant statistic that measures this path dependence. Conceptually, holonomy quantifies the \"twist\" accumulated when features are parallel-transported around a small loop in input space: flat representations yield zero holonomy, while nonzero values reveal hidden curvature. Our estimator fixes gauge through global whitening, aligns neighborhoods using shared subspaces and rotation-only Procrustes, and embeds the result back to the full feature space. We prove invariance to orthogonal (and affine, post-whitening) transformations, establish a linear null for affine layers, and show that holonomy vanishes at small radii. Empirically, holonomy increases with loop radius, separates models that appear similar under CKA, and correlates with adversarial and corruption robustness. It also tracks training dynamics as features form and stabilize. Together, these results position representation holonomy as a practical and scalable diagnostic for probing the geometric structure of learned representations beyond pointwise similarity.",
      "tldr_zh": "该研究针对现有的深度学习表征相似性度量（如CKA或SVCCA）仅能捕捉点对点重叠、而忽略了表征随输入路径演化的问题，提出了表征全纯性(representation holonomy)这一概念。这是一种规范不变(gauge-invariant)的统计量，旨在通过量化特征在输入空间的小环路中进行平行移动(parallel-transported)时积累的“扭转”来衡量路径依赖性。该方法通过全局白化(global whitening)固定规范，并利用共享子空间和普鲁克分析(Procrustes)对齐邻域，从而揭示表征中的隐藏曲率。研究证明了该指标对正交和仿射变换具有不变性，并确定了仿射层的线性零点。实验结果表明，表征全纯性能够有效区分在CKA下表现相似的模型，并与对抗攻击(adversarial)和腐蚀(corruption)的鲁棒性显著相关。此外，该指标还能追踪训练动态，为探测学习表征的几何结构提供了一种实用且可扩展的诊断工具，超越了传统的点对点相似性分析。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "14th International Conference on Learning Representations (ICLR)",
      "pdf_url": "https://arxiv.org/pdf/2601.21653v1",
      "published_date": "2026-01-29 12:51:17 UTC",
      "updated_date": "2026-01-29 12:51:17 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-30T04:26:24.696744+00:00"
    },
    {
      "arxiv_id": "2601.21650v1",
      "title": "When Life Gives You AI, Will You Turn It Into A Market for Lemons? Understanding How Information Asymmetries About AI System Capabilities Affect Market Outcomes and Adoption",
      "title_zh": "当 AI 走进生活，是否会引发“柠檬市场”效应？——理解 AI 系统能力的信息不对称对市场结果与采用的影响",
      "authors": [
        "Alexander Erlei",
        "Federico Cau",
        "Radoslav Georgiev",
        "Sagar Kumar",
        "Kilian Bizer",
        "Ujwal Gadiraju"
      ],
      "abstract": "AI consumer markets are characterized by severe buyer-supplier market asymmetries. Complex AI systems can appear highly accurate while making costly errors or embedding hidden defects. While there have been regulatory efforts surrounding different forms of disclosure, large information gaps remain. This paper provides the first experimental evidence on the important role of information asymmetries and disclosure designs in shaping user adoption of AI systems. We systematically vary the density of low-quality AI systems and the depth of disclosure requirements in a simulated AI product market to gauge how people react to the risk of accidentally relying on a low-quality AI system. Then, we compare participants' choices to a rational Bayesian model, analyzing the degree to which partial information disclosure can improve AI adoption. Our results underscore the deleterious effects of information asymmetries on AI adoption, but also highlight the potential of partial disclosure designs to improve the overall efficiency of human decision-making.",
      "tldr_zh": "该研究探讨了AI消费市场中严重的信息不对称(Information Asymmetry)问题，分析了复杂AI系统可能因隐藏缺陷而导致的“柠檬市场(Market for Lemons)”现象。研究人员通过模拟AI产品市场实验，系统地改变了低质量AI系统的比例和信息披露(Disclosure)要求的深度，以衡量用户对误用低质量AI风险的反应。通过将参与者的决策行为与理性贝叶斯模型(Rational Bayesian Model)进行对比，研究揭示了信息差距如何影响市场结果和系统采用。结果证明，信息不对称会显著抑制AI的采用，但合理的局部披露设计(Partial Disclosure Designs)能够有效提升人类决策的整体效率。该研究为政策制定者优化AI市场透明度和促进技术落地提供了重要的实验证据和理论参考。",
      "categories": [
        "cs.HC",
        "cs.AI"
      ],
      "primary_category": "cs.HC",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2601.21650v1",
      "published_date": "2026-01-29 12:49:28 UTC",
      "updated_date": "2026-01-29 12:49:28 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-30T04:26:24.495032+00:00"
    },
    {
      "arxiv_id": "2601.21649v1",
      "title": "SWE-Spot: Building Small Repo-Experts with Repository-Centric Learning",
      "title_zh": "SWE-Spot：通过以仓库为中心的学习构建仓库专用小型专家模型",
      "authors": [
        "Jinjun Peng",
        "Magnus Saebo",
        "Tianjun Zhong",
        "Yi-Jie Cheng",
        "Junfeng Yang",
        "Baishakhi Ray",
        "Simin Chen",
        "Yangruibo Ding"
      ],
      "abstract": "The deployment of coding agents in privacy-sensitive and resource-constrained environments drives the demand for capable open-weight Small Language Models (SLMs). However, they suffer from a fundamental capability gap: unlike frontier large models, they lack the inference-time strong generalization to work with complicated, unfamiliar codebases. We identify that the prevailing Task-Centric Learning (TCL) paradigm, which scales exposure across disparate repositories, fails to address this limitation. In response, we propose Repository-Centric Learning (RCL), a paradigm shift that prioritizes vertical repository depth over horizontal task breadth, suggesting SLMs must internalize the \"physics\" of a target software environment through parametric knowledge acquisition, rather than attempting to recover it via costly inference-time search. Following this new paradigm, we design a four-unit Repository-Centric Experience, transforming static codebases into interactive learning signals, to train SWE-Spot-4B, a family of highly compact models built as repo-specialized experts that breaks established scaling trends, outperforming open-weight models up to larger (e.g., CWM by Meta, Qwen3-Coder-30B) and surpassing/matching efficiency-focused commercial models (e.g., GPT-4.1-mini, GPT-5-nano) across multiple SWE tasks. Further analysis reveals that RCL yields higher training sample efficiency and lower inference costs, emphasizing that for building efficient intelligence, repository mastery is a distinct and necessary dimension that complements general coding capability.",
      "tldr_zh": "该研究针对隐私敏感和资源受限环境对小语言模型(SLMs)的需求，提出了Repository-Centric Learning (RCL)范式，以解决SLMs在处理复杂且陌生代码库时泛化能力不足的核心瓶颈。不同于传统的Task-Centric Learning (TCL)范式，RCL主张通过参数化知识获取来深度内化目标软件环境的“物理规律”，而非依赖高成本的推理时搜索。研究团队设计了包含四个单元的Repository-Centric Experience，将静态代码库转化为交互式学习信号，并据此构建了高度紧凑的repo-specialized专家模型SWE-Spot-4B。实验结果表明，该模型在多项SWE任务上超越了体量更大的开源模型（如Qwen3-Coder-30B），并达到了与GPT-5-nano等效率导向商业模型相媲美的水平。进一步分析显示，RCL不仅显著提升了训练样本效率，还降低了推理成本，证明了代码库精通(repository mastery)是提升模型编码能力的一个独立且必要的维度。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL",
        "cs.SE"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2601.21649v1",
      "published_date": "2026-01-29 12:49:25 UTC",
      "updated_date": "2026-01-29 12:49:25 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-30T04:26:49.889925+00:00"
    },
    {
      "arxiv_id": "2601.21647v1",
      "title": "ILRR: Inference-Time Steering Method for Masked Diffusion Language Models",
      "title_zh": "ILRR：掩码扩散语言模型的推理时引导方法",
      "authors": [
        "Eden Avrahami",
        "Eliya Nachmani"
      ],
      "abstract": "Discrete Diffusion Language Models (DLMs) offer a promising non-autoregressive alternative for text generation, yet effective mechanisms for inference-time control remain relatively underexplored. Existing approaches include sampling-level guidance procedures or trajectory optimization mechanisms. In this work, we introduce Iterative Latent Representation Refinement (ILRR), a learning-free framework for steering DLMs using a single reference sequence. ILRR guides generation by dynamically aligning the internal activations of the generated sequence with those of a given reference throughout the denoising process. This approach captures and transfers high-level semantic properties, with a tunable steering scale enabling flexible control over attributes such as sentiment. We further introduce Spatially Modulated Steering, an extension that enables steering long texts using shorter references by regulating guidance intensity across the sequence. Empirically, we demonstrate that ILRR achieves effective attribute steering on LLaDA and MDLM architectures with a minor computational overhead, requiring only one additional parallel forward pass per denoising step. Under the same compute budget, ILRR improves attribute accuracy over comparable baselines by 10$\\%$ to 60$\\%$ points, while maintaining high generation quality.",
      "tldr_zh": "该研究针对离散扩散语言模型 (DLMs) 在推理阶段控制机制不足的问题，提出了名为 ILRR (Iterative Latent Representation Refinement) 的无需学习的引导框架。ILRR 通过在去噪过程中动态对齐生成序列与参考序列的内部激活 (internal activations)，实现了对高层语义属性的精确捕捉与迁移，并支持通过可调引导尺度对情感等属性进行灵活控制。此外，研究引入了空间调制引导 (Spatially Modulated Steering) 扩展技术，解决了利用短参考序列引导长文本生成的难题。实验结果显示，ILRR 在 LLaDA 和 MDLM 架构上表现优异，在仅增加少量计算开销的前提下，其属性引导准确率相较于基线模型提升了 10% 至 60%。该方法在显著增强 DLMs 推理受控性的同时，依然保持了极高的文本生成质量。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2601.21647v1",
      "published_date": "2026-01-29 12:48:59 UTC",
      "updated_date": "2026-01-29 12:48:59 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-30T04:26:40.294481+00:00"
    },
    {
      "arxiv_id": "2601.21641v1",
      "title": "Seg-MoE: Multi-Resolution Segment-wise Mixture-of-Experts for Time Series Forecasting Transformers",
      "title_zh": "Seg-MoE：面向时间序列预测 Transformer 的多分辨率分段混合专家模型",
      "authors": [
        "Evandro S. Ortigossa",
        "Eran Segal"
      ],
      "abstract": "Transformer-based models have recently made significant advances in accurate time-series forecasting, but even these architectures struggle to scale efficiently while capturing long-term temporal dynamics. Mixture-of-Experts (MoE) layers are a proven solution to scaling problems in natural language processing. However, existing MoE approaches for time-series forecasting rely on token-wise routing mechanisms, which may fail to exploit the natural locality and continuity of temporal data. In this work, we introduce Seg-MoE, a sparse MoE design that routes and processes contiguous time-step segments rather than making independent expert decisions. Token segments allow each expert to model intra-segment interactions directly, naturally aligning with inherent temporal patterns. We integrate Seg-MoE layers into a time-series Transformer and evaluate it on multiple multivariate long-term forecasting benchmarks. Seg-MoE consistently achieves state-of-the-art forecasting accuracy across almost all prediction horizons, outperforming both dense Transformers and prior token-wise MoE models. Comprehensive ablation studies confirm that segment-level routing is the key factor driving these gains. Our results show that aligning the MoE routing granularity with the inherent structure of time series provides a powerful, yet previously underexplored, inductive bias, opening new avenues for conditionally sparse architectures in sequential data modeling.",
      "tldr_zh": "该研究提出了 Seg-MoE，这是一种面向时间序列预测 Transformer 的多分辨率分段混合专家 (Mixture-of-Experts) 架构，旨在解决现有模型在扩展和捕捉长期时间动态时的效率挑战。不同于以往依赖 Token-wise 路由机制的 MoE 方法，Seg-MoE 采用稀疏设计来路由和处理连续的时间步分段 (Segments)，从而有效利用了时间数据的局部性与连续性。这种机制使每个专家能够直接建模段内交互，实现了与固有时间模式的自然对齐。研究者将 Seg-MoE 集成至时间序列 Transformer 模型，并在多个多元长期预测基准上进行了验证。实验结果表明，Seg-MoE 在几乎所有预测视野中均达到了 state-of-the-art 精度，性能显著优于密集 Transformer 及传统的 Token-wise MoE 模型。消融研究证实分段级路由是性能提升的核心驱动力，证明了将 MoE 路由粒度与时间序列内在结构对齐能提供强大的归纳偏置 (inductive bias)，为序列数据建模中的条件稀疏架构开辟了新路径。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "Under review",
      "pdf_url": "https://arxiv.org/pdf/2601.21641v1",
      "published_date": "2026-01-29 12:43:35 UTC",
      "updated_date": "2026-01-29 12:43:35 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-30T04:26:42.469630+00:00"
    },
    {
      "arxiv_id": "2601.21626v1",
      "title": "HeRo-Q: A General Framework for Stable Low Bit Quantization via Hessian Conditioning",
      "title_zh": "HeRo-Q：通过 Hessian 条件化实现稳定低比特量化的通用框架",
      "authors": [
        "Jinhao Zhang Yunquan Zhang",
        "Zicheng yan",
        "Boyang Zhang",
        "Jun Sun",
        "Daning Cheng"
      ],
      "abstract": "Post Training Quantization (PTQ), a mainstream model compression technique, often leads to the paradoxical 'low error, high loss' phenomenon because it focuses solely on minimizing quantization error. The root cause lies in the Hessian matrix of the LLM loss landscape: a few high curvature directions are extremely sensitive to perturbations. To address this, we propose the Hessian Robust Quantization (HeRo Q) algorithm, which applies a lightweight, learnable rotation-compression matrix to the weight space prior to quantization. This joint framework reshapes the loss landscape by reducing the largest Hessian eigenvalue and reducing its max eigenvalue, thereby significantly enhancing robustness to quantization noise. HeRo-Q requires no architectural modifications, incurs negligible computational overhead, and integrates seamlessly into existing PTQ pipelines. Experiments on Llama and Qwen models show that HeRo Q consistently outperforms state of the art methods including GPTQ, AWQ, and SpinQuant not only achieving superior performance under standard W4A8 settings, but also excelling in the highly challenging W3A16 ultra low bit regime, where it boosts GSM8K accuracy on Llama3 8B to 70.15\\% and effectively avoids the logical collapse commonly seen in aggressive quantization.",
      "tldr_zh": "该研究针对模型量化中常见的“低误差、高损失”现象，提出了 HeRo-Q 框架，旨在解决 Post Training Quantization (PTQ) 中 Hessian matrix 的高曲率方向对扰动极其敏感的核心问题。HeRo-Q 通过在量化前对权重空间应用轻量级、可学习的旋转压缩矩阵来重塑损失景观 (loss landscape)，从而显著降低 Hessian matrix 的最大特征值 (max eigenvalue) 并增强对量化噪声的鲁棒性。该算法无需修改模型架构且计算开销极低，能够无缝集成到现有的 PTQ 流水线中。在 Llama 和 Qwen 模型上的实验证明，HeRo-Q 在 W4A8 设置下不仅优于 GPTQ、AWQ 和 SpinQuant 等前沿方法，在极具挑战性的 W3A16 超低比特场景中同样表现卓越。具体而言，它将 Llama3-8B 在 GSM8K 任务上的准确率提升至 70.15%，并有效避免了激进量化中常见的逻辑崩坏 (logical collapse)，为稳定、高效的低比特量化提供了通用框架。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2601.21626v1",
      "published_date": "2026-01-29 12:27:05 UTC",
      "updated_date": "2026-01-29 12:27:05 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-30T04:26:42.363960+00:00"
    },
    {
      "arxiv_id": "2601.21619v1",
      "title": "Breaking the Overscaling Curse: Thinking Parallelism Before Parallel Thinking",
      "title_zh": "破解过度缩放魔咒：并行思考前的并行度预判",
      "authors": [
        "Yiming Wang",
        "Zhuosheng Zhang",
        "Rui Wang"
      ],
      "abstract": "Parallel thinking enhances LLM reasoning by multi-path sampling and aggregation. In system-level evaluations, a global parallelism level N is allocated to all samples, typically set large to maximize overall dataset accuracy. However, due to sample heterogeneity, some samples can achieve comparable performance with a smaller N'< N, causing budget redundancy. This incompatibility between system-level efficacy and sample-level efficiency constitutes the overscaling curse. In this paper, we formalize and quantify the overscaling curse, showing its universality and severity in practice, and analyze its trigger mechanism. We then propose a lightweight method, T2, to break the overscaling curse, which utilizes latent representations to estimate the optimal parallelism level for each sample before decoding. Experiments show that T2 significantly reduces cost while maintaining comparable performance, enabling more efficient parallel thinking.",
      "tldr_zh": "本文探讨了大型语言模型(LLMs)在并行思考(Parallel thinking)中面临的“过度扩展诅咒”(overscaling curse)，即系统层面统一分配的高并行路径数与不同样本实际所需的计算量不匹配，导致了严重的预算冗余。研究者通过对该诅咒进行形式化和量化分析，揭示了其在实际应用中的普遍性及其触发机制。为此，本文提出了一种名为 T2 的轻量级方法，利用潜表征(latent representations)在解码前预估每个样本的最优并行级别(parallelism level)。实验证明，T2 能够在保持模型推理性能的前提下显著降低计算成本，实现了更高效的并行推理。该研究有效解决了系统级效能与样本级效率之间的矛盾，为优化大模型推理资源分配提供了新思路。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2601.21619v1",
      "published_date": "2026-01-29 12:22:45 UTC",
      "updated_date": "2026-01-29 12:22:45 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-30T04:27:47.621538+00:00"
    },
    {
      "arxiv_id": "2601.21618v1",
      "title": "Semantic Content Determines Algorithmic Performance",
      "title_zh": "语义内容决定算法性能",
      "authors": [
        "Martiño Ríos-García",
        "Nawaf Alampara",
        "Kevin Maik Jablonka"
      ],
      "abstract": "Counting should not depend on what is being counted; more generally, any algorithm's behavior should be invariant to the semantic content of its arguments. We introduce WhatCounts to test this property in isolation. Unlike prior work that conflates semantic sensitivity with reasoning complexity or prompt variation, WhatCounts is atomic: count items in an unambiguous, delimited list with no duplicates, distractors, or reasoning steps for different semantic types. Frontier LLMs show over 40% accuracy variation depending solely on what is being counted - cities versus chemicals, names versus symbols. Controlled ablations rule out confounds. The gap is semantic, and it shifts unpredictably with small amounts of unrelated fine-tuning. LLMs do not implement algorithms; they approximate them, and the approximation is argument-dependent. As we show with an agentic example, this has implications beyond counting: any LLM function may carry hidden dependencies on the meaning of its inputs.",
      "tldr_zh": "该研究探讨了算法行为是否应独立于输入参数的语义内容，并引入了WhatCounts基准测试来专门验证这一特性。研究通过在无干扰、无重复的列表计数任务中进行测试，发现前沿大语言模型(Frontier LLMs)的准确率会因计数对象的不同（如城市与化学物质、姓名与符号）而产生超过40%的波动。受控消融实验排除了推理复杂度和提示词变体等干扰因素，证明了这种差距纯粹源于语义，且会随少量的微调产生不可预测的变化。研究结论指出，大语言模型并非在真正实现算法，而是在对算法进行近似，且这种近似高度依赖于参数含义。这一发现揭示了LLM功能对输入内容存在隐性依赖，对于理解模型在复杂任务及智能体应用中的表现具有重要意义。",
      "categories": [
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2601.21618v1",
      "published_date": "2026-01-29 12:22:17 UTC",
      "updated_date": "2026-01-29 12:22:17 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-30T04:26:52.044282+00:00"
    },
    {
      "arxiv_id": "2601.21615v1",
      "title": "Beyond Parameter Finetuning: Test-Time Representation Refinement for Node Classification",
      "title_zh": "超越参数微调：面向节点分类的测试时表征精炼",
      "authors": [
        "Jiaxin Zhang",
        "Yiqi Wang",
        "Siwei Wang",
        "Xihong Yang",
        "Yu Shi",
        "Xinwang Liu",
        "En Zhu"
      ],
      "abstract": "Graph Neural Networks frequently exhibit significant performance degradation in the out-of-distribution test scenario. While test-time training (TTT) offers a promising solution, existing Parameter Finetuning (PaFT) paradigm suffer from catastrophic forgetting, hindering their real-world applicability. We propose TTReFT, a novel Test-Time Representation FineTuning framework that transitions the adaptation target from model parameters to latent representations. Specifically, TTReFT achieves this through three key innovations: (1) uncertainty-guided node selection for specific interventions, (2) low-rank representation interventions that preserve pre-trained knowledge, and (3) an intervention-aware masked autoencoder that dynamically adjust masking strategy to accommodate the node selection scheme. Theoretically, we establish guarantees for TTReFT in OOD settings. Empirically, extensive experiments across five benchmark datasets demonstrate that TTReFT achieves consistent and superior performance. Our work establishes representation finetuning as a new paradigm for graph TTT, offering both theoretical grounding and immediate practical utility for real-world deployment.",
      "tldr_zh": "该研究针对图神经网络(GNNs)在分布外(out-of-distribution)测试场景中性能显著下降，且现有参数微调(Parameter Finetuning)范式面临灾难性遗忘的问题，提出了TTReFT框架。这是一种全新的测试时表示微调(Test-Time Representation FineTuning)范式，将自适应目标从模型参数转向潜在表示。TTReFT通过不确定性引导的节点选择、保留预训练知识的低秩表示干预，以及动态调整掩码策略的干预感知掩码自编码器(intervention-aware masked autoencoder)实现有效适配。理论分析为该方法在OOD设置下提供了保证，而在五个基准数据集上的广泛实验证明了其持续且优越的性能。该工作不仅为图测试时训练(graph TTT)确立了表示微调这一新范式，也为实际部署提供了理论基础与实用工具。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2601.21615v1",
      "published_date": "2026-01-29 12:17:34 UTC",
      "updated_date": "2026-01-29 12:17:34 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-30T04:26:55.661154+00:00"
    },
    {
      "arxiv_id": "2601.21612v1",
      "title": "Representation-Regularized Convolutional Audio Transformer for Audio Understanding",
      "title_zh": "面向音频理解的表征正则化卷积音频 Transformer",
      "authors": [
        "Bing Han",
        "Chushu Zhou",
        "Yifan Yang",
        "Wei Wang",
        "Chenda Li",
        "Wangyou Zhang",
        "Yanmin Qian"
      ],
      "abstract": "Bootstrap-based Self-Supervised Learning (SSL) has achieved remarkable progress in audio understanding. However, existing methods typically operate at a single level of granularity, limiting their ability to model the diverse temporal and spectral structures inherent in complex audio signals. Furthermore, bootstrapping representations from scratch is computationally expensive, often requiring extensive training to converge. In this work, we propose the Convolutional Audio Transformer (CAT), a unified framework designed to address these challenges. First, to capture hierarchical audio features, CAT incorporates a Multi-resolution Block that aggregates information across varying granularities. Second, to enhance training efficiency, we introduce a Representation Regularization objective. Drawing inspiration from generative modeling, this auxiliary task guides the student model by aligning its predictions with high-quality semantic representations from frozen, pre-trained external encoders. Experimental results demonstrate that CAT significantly outperforms baselines on audio understanding benchmarks. Notably, it achieves competitive performance on the AudioSet 20k dataset with 5 times faster convergence than existing methods. Codes and checkpoints will be released soon at https://github.com/realzhouchushu/CAT.",
      "tldr_zh": "该研究提出了Convolutional Audio Transformer (CAT)框架，旨在解决音频理解领域中Self-Supervised Learning (SSL)在单一粒度建模和高计算成本方面的局限。为了捕捉层次化的音频特征，CAT引入了Multi-resolution Block来聚合不同粒度的信息。同时，该研究设计了一种Representation Regularization目标作为辅助任务，通过对齐预训练外部编码器的高质量语义表示来引导模型，显著提升了训练效率。实验结果显示，CAT在多项音频理解基准任务上均优于基线模型。值得注意的是，该框架在AudioSet 20k数据集上实现了比现有方法快5倍的收敛速度，并保持了极具竞争力的性能表现。",
      "categories": [
        "eess.AS",
        "cs.AI",
        "cs.SD"
      ],
      "primary_category": "eess.AS",
      "comment": "12 pages, 3 figures",
      "pdf_url": "https://arxiv.org/pdf/2601.21612v1",
      "published_date": "2026-01-29 12:16:19 UTC",
      "updated_date": "2026-01-29 12:16:19 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-30T04:27:02.027938+00:00"
    },
    {
      "arxiv_id": "2601.21611v1",
      "title": "Thinking Broad, Acting Fast: Latent Reasoning Distillation from Multi-Perspective Chain-of-Thought for E-Commerce Relevance",
      "title_zh": "广思敏行：基于多视角链式思维的电商相关性隐式推理蒸馏",
      "authors": [
        "Baopu Qiu",
        "Hao Chen",
        "Yuanrong Wu",
        "Changtong Zan",
        "Chao Wei",
        "Weiru Zhang",
        "Xiaoyi Zeng"
      ],
      "abstract": "Effective relevance modeling is crucial for e-commerce search, as it aligns search results with user intent and enhances customer experience. Recent work has leveraged large language models (LLMs) to address the limitations of traditional relevance models, especially for long-tail and ambiguous queries. By incorporating Chain-of-Thought (CoT) reasoning, these approaches improve both accuracy and interpretability through multi-step reasoning. However, two key limitations remain: (1) most existing approaches rely on single-perspective CoT reasoning, which fails to capture the multifaceted nature of e-commerce relevance (e.g., user intent vs. attribute-level matching vs. business-specific rules); and (2) although CoT-enhanced LLM's offer rich reasoning capabilities, their high inference latency necessitates knowledge distillation for real-time deployment, yet current distillation methods discard the CoT rationale structure at inference, using it as a transient auxiliary signal and forfeiting its reasoning utility. To address these challenges, we propose a novel framework that better exploits CoT semantics throughout the optimization pipeline. Specifically, the teacher model leverages Multi-Perspective CoT (MPCoT) to generate diverse rationales and combines Supervised Fine-Tuning (SFT) with Direct Preference Optimization (DPO) to construct a more robust reasoner. For distillation, we introduce Latent Reasoning Knowledge Distillation (LRKD), which endows a student model with a lightweight inference-time latent reasoning extractor, allowing efficient and low-latency internalization of the LLM's sophisticated reasoning capabilities. Evaluated in offline experiments and online A/B tests on an e-commerce search advertising platform serving tens of millions of users daily, our method delivers significant offline gains, showing clear benefits in both commercial performance and user experience.",
      "tldr_zh": "该研究针对电子商务搜索相关性建模中传统方法处理长尾和歧义查询能力不足，以及现有基于LLMs的CoT方法视角单一且推理延迟高等挑战，提出了一种创新的推理蒸馏框架。教师模型通过Multi-Perspective CoT (MPCoT)生成多维度的推理逻辑，并结合Supervised Fine-Tuning (SFT)与Direct Preference Optimization (DPO)构建了更具鲁棒性的推理器。为了实现高效的实时部署，研究引入了Latent Reasoning Knowledge Distillation (LRKD)，使轻量级学生模型能够通过隐式推理提取器内化大模型的复杂推理能力，并在推理阶段保持低延迟。在服务千万级用户的真实电商搜索广告平台进行的离线实验和在线A/B测试结果表明，该方法在提升商业性能和用户体验方面均取得了显著收益，成功平衡了推理的深度与系统响应速度。",
      "categories": [
        "cs.IR",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.IR",
      "comment": "12 pages, 6 figures, Accepted by WWW2026 industry track",
      "pdf_url": "https://arxiv.org/pdf/2601.21611v1",
      "published_date": "2026-01-29 12:14:44 UTC",
      "updated_date": "2026-01-29 12:14:44 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-30T04:27:04.281796+00:00"
    },
    {
      "arxiv_id": "2601.21609v1",
      "title": "RecNet: Self-Evolving Preference Propagation for Agentic Recommender Systems",
      "title_zh": "RecNet：面向智能体推荐系统的自演化偏好传播",
      "authors": [
        "Bingqian Li",
        "Xiaolei Wang",
        "Junyi Li",
        "Weitao Li",
        "Long Zhang",
        "Sheng Chen",
        "Wayne Xin Zhao",
        "Ji-Rong Wen"
      ],
      "abstract": "Agentic recommender systems leverage Large Language Models (LLMs) to model complex user behaviors and support personalized decision-making. However, existing methods primarily model preference changes based on explicit user-item interactions, which are sparse, noisy, and unable to reflect the real-time, mutual influences among users and items. To address these limitations, we propose RecNet, a self-evolving preference propagation framework that proactively propagates real-time preference updates across related users and items. RecNet consists of two complementary phases. In the forward phase, the centralized preference routing mechanism leverages router agents to integrate preference updates and dynamically propagate them to the most relevant agents. To ensure accurate and personalized integration of propagated preferences, we further introduce a personalized preference reception mechanism, which combines a message buffer for temporary caching and an optimizable, rule-based filter memory to guide selective preference assimilation based on past experience and interests. In the backward phase, the feedback-driven propagation optimization mechanism simulates a multi-agent reinforcement learning framework, using LLMs for credit assignment, gradient analysis, and module-level optimization, enabling continuous self-evolution of propagation strategies. Extensive experiments on various scenarios demonstrate the effectiveness of RecNet in modeling preference propagation for recommender systems.",
      "tldr_zh": "该研究提出了RecNet，一种旨在解决智能体推荐系统(Agentic Recommender Systems)中用户-项目交互稀疏与噪声问题的自进化偏好传播(Self-Evolving Preference Propagation)框架。RecNet在前向阶段通过中心化偏好路由机制利用路由智能体(Router Agents)动态传播实时偏好更新，并结合个性化偏好接收机制中的消息缓冲区(Message Buffer)与规则过滤器内存(Filter Memory)实现偏好的选择性同化。在后向阶段，该框架采用反馈驱动的传播优化机制，模拟多智能体强化学习并利用大语言模型(LLMs)进行信用分配(Credit Assignment)和梯度分析，以实现传播策略的持续优化与自进化。多场景实验证明，RecNet能够有效捕捉用户与项目间的实时相互影响，显著提升了推荐系统在复杂行为建模和个性化决策方面的性能。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2601.21609v1",
      "published_date": "2026-01-29 12:14:31 UTC",
      "updated_date": "2026-01-29 12:14:31 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-30T04:27:06.950389+00:00"
    },
    {
      "arxiv_id": "2601.21608v1",
      "title": "Search-Based Risk Feature Discovery in Document Structure Spaces under a Constrained Budget",
      "title_zh": "预算受限下文档结构空间中基于搜索的风险特征发现",
      "authors": [
        "Saisubramaniam Gopalakrishnan",
        "Harikrishnan P M",
        "Dagnachew Birru"
      ],
      "abstract": "Enterprise-grade Intelligent Document Processing (IDP) systems support high-stakes workflows across finance, insurance, and healthcare. Early-phase system validation under limited budgets mandates uncovering diverse failure mechanisms, rather than identifying a single worst-case document. We formalize this challenge as a Search-Based Software Testing (SBST) problem, aiming to identify complex interactions between document variables, with the objective to maximize the number of distinct failure types discovered within a fixed evaluation budget. Our methodology operates on a combinatorial space of document configurations, rendering instances of structural \\emph{risk features} to induce realistic failure conditions. We benchmark a diverse portfolio of search strategies spanning evolutionary, swarm-based, quality-diversity, learning-based, and quantum under identical budget constraints. Through configuration-level exclusivity, win-rate, and cross-temporal overlap analyses, we show that different solvers consistently uncover failure modes that remain undiscovered by specific alternatives at comparable budgets. Crucially, cross-temporal analysis reveals persistent solver-specific discoveries across all evaluated budgets, with no single strategy exhibiting absolute dominance. While the union of all solvers eventually recovers the observed failure space, reliance on any individual method systematically delays the discovery of important risks. These results demonstrate intrinsic solver complementarity and motivate portfolio-based SBST strategies for robust industrial IDP validation.",
      "tldr_zh": "该研究针对金融、保险和医疗等高风险领域的企业级智能文档处理(Intelligent Document Processing, IDP)系统，提出了在有限预算下发现多样化失效机制的验证挑战。作者将该问题形式化为基于搜索的软件测试(Search-Based Software Testing, SBST)问题，旨在通过识别文档变量间的复杂交互来最大化发现的独特失效类型数量。该方法在文档配置的组合空间内运行，通过生成结构化的风险特征(risk features)来诱发真实的失效条件。研究在相同预算约束下基准测试了包括进化算法、群智算法、质量多样性(quality-diversity)、机器学习以及量子算法在内的多种搜索策略。实验分析表明，不同求解器在发现失效模式方面具有显著的互补性，没有任何单一策略能够占据绝对主导地位。研究结果证明，仅依赖单一方法会系统性地推迟重要风险的发现，因此建议采用组合式(portfolio-based)的SBST策略，以实现鲁棒的工业级IDP系统验证。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2601.21608v1",
      "published_date": "2026-01-29 12:14:18 UTC",
      "updated_date": "2026-01-29 12:14:18 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-30T04:27:57.281385+00:00"
    },
    {
      "arxiv_id": "2601.21601v1",
      "title": "Dynamics Reveals Structure: Challenging the Linear Propagation Assumption",
      "title_zh": "动力学揭示结构：挑战线性传播假设",
      "authors": [
        "Hoyeon Chang",
        "Bálint Mucsányi",
        "Seong Joon Oh"
      ],
      "abstract": "Neural networks adapt through first-order parameter updates, yet it remains unclear whether such updates preserve logical coherence. We investigate the geometric limits of the Linear Propagation Assumption (LPA), the premise that local updates coherently propagate to logical consequences. To formalize this, we adopt relation algebra and study three core operations on relations: negation flips truth values, converse swaps argument order, and composition chains relations. For negation and converse, we prove that guaranteeing direction-agnostic first-order propagation necessitates a tensor factorization separating entity-pair context from relation content. However, for composition, we identify a fundamental obstruction. We show that composition reduces to conjunction, and prove that any conjunction well-defined on linear features must be bilinear. Since bilinearity is incompatible with negation, this forces the feature map to collapse. These results suggest that failures in knowledge editing, the reversal curse, and multi-hop reasoning may stem from common structural limitations inherent to the LPA.",
      "tldr_zh": "该研究探讨了神经网络中的线性传播假设 (Linear Propagation Assumption, LPA)，即局部参数更新能否逻辑一致地传播到相关的逻辑推论。作者利用关系代数 (Relation Algebra) 形式化分析了否定 (negation)、逆向 (converse) 和复合 (composition) 三种核心操作，旨在揭示参数更新的几何极限。研究证明，保证否定和逆向操作的一致性需要通过张量分解 (Tensor Factorization) 分离上下文与关系内容。然而，对于复合操作，研究发现其要求的双线性 (bilinearity) 与否定操作存在本质冲突，会导致特征映射发生崩溃。这些理论结果表明，当前模型在知识编辑 (Knowledge Editing)、逆转诅咒 (Reversal Curse) 以及多跳推理 (Multi-hop Reasoning) 方面的失败，可能源于线性传播假设内在的结构性局限。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2601.21601v1",
      "published_date": "2026-01-29 12:08:00 UTC",
      "updated_date": "2026-01-29 12:08:00 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-30T04:27:58.974503+00:00"
    },
    {
      "arxiv_id": "2601.21600v1",
      "title": "CORE: Collaborative Reasoning via Cross Teaching",
      "title_zh": "CORE：基于交叉教学的协同推理",
      "authors": [
        "Kshitij Mishra",
        "Mirat Aubakirov",
        "Martin Takac",
        "Nils Lukas",
        "Salem Lahlou"
      ],
      "abstract": "Large language models exhibit complementary reasoning errors: on the same instance, one model may succeed with a particular decomposition while another fails. We propose Collaborative Reasoning (CORE), a training-time collaboration framework that converts peer success into a learning signal via a cross-teaching protocol. Each problem is solved in two stages: a cold round of independent sampling, followed by a contexted rescue round in which models that failed receive hint extracted from a successful peer. CORE optimizes a combined reward that balances (i) correctness, (ii) a lightweight DPP-inspired diversity term to reduce error overlap, and (iii) an explicit rescue bonus for successful recovery. We evaluate CORE across four standard reasoning datasets GSM8K, MATH, AIME, and GPQA. With only 1,000 training examples, a pair of small open source models (3B+4B) reaches Pass@2 of 99.54% on GSM8K and 92.08% on MATH, compared to 82.50% and 74.82% for single-model training. On harder datasets, the 3B+4B pair reaches Pass@2 of 77.34% on GPQA (trained on 348 examples) and 79.65% on AIME (trained on 792 examples), using a training-time budget of at most 1536 context tokens and 3072 generated tokens. Overall, these results show that training-time collaboration can reliably convert model complementarity into large gains without scaling model size.",
      "tldr_zh": "该研究提出了 Collaborative Reasoning (CORE)，这是一种旨在通过交叉教学协议 (cross-teaching protocol) 将对等成功转化为学习信号的训练时协作框架，有效解决了大语言模型在相同实例上表现出的互补推理错误问题。该框架将解题过程分为独立采样的冷启动轮和失败模型接收成功同伴提示的救援轮两个阶段。CORE 通过优化结合了正确性、旨在减少错误重叠的 DPP-inspired diversity 多样性项以及显式救援奖励的综合回报来实现性能提升。在 GSM8K、MATH、AIME 和 GPQA 等标准推理数据集上的实验显示，仅使用 1,000 个训练样本，一对小型开源模型（3B+4B）的 Pass@2 准确率便大幅超越单模型训练水平。研究结果证明，训练时协作能可靠地将模型间的互补性转化为显著的性能增益，而无需通过扩大模型规模来实现。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2601.21600v1",
      "published_date": "2026-01-29 12:07:54 UTC",
      "updated_date": "2026-01-29 12:07:54 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-30T04:28:04.454933+00:00"
    },
    {
      "arxiv_id": "2601.21598v1",
      "title": "Beyond Imitation: Reinforcement Learning for Active Latent Planning",
      "title_zh": "超越模仿：面向主动隐空间规划的强化学习",
      "authors": [
        "Zhi Zheng",
        "Wee Sun Lee"
      ],
      "abstract": "Aiming at efficient and dense chain-of-thought (CoT) reasoning, latent reasoning methods fine-tune Large Language Models (LLMs) to substitute discrete language tokens with continuous latent tokens. These methods consume fewer tokens compared to the conventional language CoT reasoning and have the potential to plan in a dense latent space. However, current latent tokens are generally supervised based on imitating language labels. Considering that there can be multiple equivalent but diverse CoT labels for a question, passively imitating an arbitrary one may lead to inferior latent token representations and latent reasoning policies, undermining the potential planning ability and resulting in clear gaps between training and testing. In this work, we emphasize the importance of active planning over the representation space of latent tokens in achieving the optimal latent reasoning policy. So, we propose the \\underline{A}c\\underline{t}ive Latent \\underline{P}lanning method (ATP-Latent), which models the supervision process of latent tokens as a conditional variational auto-encoder (VAE) to obtain a smoother latent space. Moreover, to facilitate the most reasonable latent reasoning policy, ATP-Latent conducts reinforcement learning (RL) with an auxiliary coherence reward, which is calculated based on the consistency between VAE-decoded contents of latent tokens, enabling a guided RL process. In experiments on LLaMA-1B, ATP-Latent demonstrates +4.1\\% accuracy and -3.3\\% tokens on four benchmarks compared to advanced baselines. Codes are available on https://github.com/zz1358m/ATP-Latent-master.",
      "tldr_zh": "该研究针对大语言模型(LLMs)在潜空间推理(latent reasoning)中因过度模仿离散语言标签而导致规划能力受限的问题，提出了ATP-Latent (Active Latent Planning)方法。该方法通过将潜标记的监督过程建模为条件变分自编码器(Conditional Variational Auto-Encoder, VAE)，构建了更平滑的潜空间表示。为了促成最优的潜空间推理策略，ATP-Latent 结合了强化学习(Reinforcement Learning, RL)技术，并引入基于 VAE 解码内容一致性计算的辅助一致性奖励(auxiliary coherence reward)来引导学习过程。在 LLaMA-1B 上的实验结果显示，ATP-Latent 在四个基准测试中相比先进基线模型提升了4.1%的准确率，并减少了3.3%的标记(tokens)消耗。这一成果证明了在潜标记空间进行主动规划对于提升模型推理效率和性能的关键作用，为实现高效且稠密的链式思维(Chain-of-Thought, CoT)推理提供了新思路。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2601.21598v1",
      "published_date": "2026-01-29 12:07:16 UTC",
      "updated_date": "2026-01-29 12:07:16 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-30T04:28:08.166950+00:00"
    },
    {
      "arxiv_id": "2601.21590v1",
      "title": "Scalable Power Sampling: Unlocking Efficient, Training-Free Reasoning for LLMs via Distribution Sharpening",
      "title_zh": "Scalable Power Sampling：通过分布锐化解锁大语言模型高效、免训练的推理能力",
      "authors": [
        "Xiaotong Ji",
        "Rasul Tutunov",
        "Matthieu Zimmer",
        "Haitham Bou Ammar"
      ],
      "abstract": "Reinforcement learning (RL) post-training is a dominant approach for improving the reasoning performance of large language models (LLMs), yet growing evidence suggests that its gains arise primarily from distribution sharpening rather than the acquisition of new capabilities. Recent work has shown that sampling from the power distribution of LLMs using Markov chain Monte Carlo (MCMC) can recover performance comparable to RL post-training without relying on external rewards; however, the high computational cost of MCMC makes such approaches impractical for widespread adoption. In this work, we propose a theoretically grounded alternative that eliminates the need for iterative MCMC. We derive a novel formulation showing that the global power distribution can be approximated by a token-level scaled low-temperature one, where the scaling factor captures future trajectory quality. Leveraging this insight, we introduce a training-free and verifier-free algorithm that sharpens the base model's generative distribution autoregressively. Empirically, we evaluate our method on math, QA, and code tasks across four LLMs, and show that our method matches or surpasses one-shot GRPO without relying on any external rewards, while reducing inference latency by over 10x compared to MCMC-based sampling.",
      "tldr_zh": "该研究针对强化学习(RL)后训练在提升大语言模型(LLMs)推理能力时存在的高成本及本质为分布锐化(Distribution Sharpening)的问题，提出了Scalable Power Sampling，一种无需训练且无需验证器的推理增强算法。研究者通过理论推导证明，全局幂分布(Global Power Distribution)可以由令牌级的缩放低温分布(Token-level Scaled Low-temperature Distribution)来近似，其中缩放因子能够有效捕捉未来的轨迹质量。基于这一洞察，该算法以自回归方式对基础模型的生成分布进行锐化，消除了对迭代式Markov Chain Monte Carlo (MCMC)采样的依赖。在数学、问答和代码任务的实验评估中，该方法在四个LLMs上达到了与单次GRPO相当或更优的性能，且无需任何外部奖励。相比于传统的MCMC采样，Scalable Power Sampling将推理延迟降低了10倍以上，为实现高效、低成本的LLM推理能力提升提供了新的理论支撑与实践方案。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2601.21590v1",
      "published_date": "2026-01-29 12:01:53 UTC",
      "updated_date": "2026-01-29 12:01:53 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-30T04:28:06.483368+00:00"
    },
    {
      "arxiv_id": "2601.21582v1",
      "title": "Depth-Recurrent Attention Mixtures: Giving Latent Reasoning the Attention it Deserves",
      "title_zh": "深度循环注意力混合模型：给予隐式推理应有的关注",
      "authors": [
        "Jonas Knupp",
        "Jan Hendrik Metzen",
        "Jeremias Bohn",
        "Georg Groh",
        "Kristian Kersting"
      ],
      "abstract": "Depth-recurrence facilitates latent reasoning by sharing parameters across depths. However, prior work lacks combined FLOP-, parameter-, and memory-matched baselines, underutilizes depth-recurrence due to partially fixed layer stacks, and ignores the bottleneck of constant hidden-sizes that restricts many-step latent reasoning. To address this, we introduce a modular framework of depth-recurrent attention mixtures (Dreamer), combining sequence attention, depth attention, and sparse expert attention. It alleviates the hidden-size bottleneck through attention along depth, decouples scaling dimensions, and allows depth-recurrent models to scale efficiently and effectively. Across language reasoning benchmarks, our models require 2 to 8x fewer training tokens for the same accuracy as FLOP-, parameter-, and memory-matched SOTA, and outperform ca. 2x larger SOTA models with the same training tokens. We further present insights into knowledge usage across depths, e.g., showing 2 to 11x larger expert selection diversity than SOTA MoEs.",
      "tldr_zh": "该研究针对深度递归(Depth-recurrence)在潜在推理(Latent reasoning)中的局限性，特别是因固定层堆叠和恒定隐藏层大小(Hidden-sizes)造成的瓶颈，提出了名为Dreamer的模块化框架。该框架整合了序列注意力(Sequence attention)、深度注意力(Depth attention)和稀疏专家注意力(Sparse expert attention)，通过在深度维度引入注意力机制缓解了隐藏层限制，并实现了缩放维度的解耦。在语言推理基准测试中，Dreamer模型达到同等精度所需的训练Token仅为SOTA模型的1/8至1/4，且性能超越了训练量相同的两倍规模SOTA模型。研究进一步揭示了跨深度的知识使用特性，其专家选择多样性(Expert selection diversity)较现有混合专家模型(MoEs)提升了2至11倍。这一成果为深度递归模型的高效扩展及复杂潜在推理任务提供了有效的技术方案。",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2601.21582v1",
      "published_date": "2026-01-29 11:44:38 UTC",
      "updated_date": "2026-01-29 11:44:38 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-30T04:28:15.049411+00:00"
    },
    {
      "arxiv_id": "2601.21576v1",
      "title": "Chain Of Thought Compression: A Theoritical Analysis",
      "title_zh": "思维链压缩：理论分析",
      "authors": [
        "Juncai Li",
        "Ru Li",
        "Yuxiang Zhou",
        "Boxiang Ma",
        "Jeff Z. Pan"
      ],
      "abstract": "Chain-of-Thought (CoT) has unlocked advanced reasoning abilities of Large Language Models (LLMs) with intermediate steps, yet incurs prohibitive computational costs due to generation of extra tokens. Recent studies empirically show that compressing reasoning steps into latent states, or implicit CoT compression, offers a token-efficient alternative. However, the mechanism behind CoT compression remains unclear. In this paper, we provide the first theoretical analysis of the difficulty of learning to internalize intermediate reasoning steps. By introducing Order-r Interaction, we prove that the learning signal for high-order logical dependencies exponentially decays to solve irreducible problem, where skipping intermediate steps inevitably leads to high-order interaction barriers. To empirically validate this, we introduce NatBool-DAG, a challenging benchmark designed to enforce irreducible logical reasoning and eliminate semantic shortcuts. Guided by our theoretical findings, we propose ALiCoT (Aligned Implicit CoT), a novel framework that overcomes the signal decay by aligning latent token distributions with intermediate reasoning states. Experimental results demonstrate that ALiCoT successfully unlocks efficient reasoning: it achieves a 54.4x speedup while maintaining performance comparable to explicit CoT.",
      "tldr_zh": "该研究针对Chain-of-Thought (CoT)因生成冗余标记导致高计算成本的问题，首次对学习内化中间推理步骤的难度进行了理论分析。通过引入Order-r Interaction概念，研究证明在处理不可约问题时，跳过中间步骤会导致高阶交互障碍，使得学习信号呈指数级衰减。为验证这一理论，作者引入了旨在强制执行不可约逻辑推理的基准测试NatBool-DAG，以消除语义捷径。基于理论发现，研究提出了一种名为ALiCoT (Aligned Implicit CoT)的新框架，通过将隐标记分布与中间推理状态对齐来克服信号衰减问题。实验结果显示，ALiCoT在保持与显性CoT相当性能的前提下，实现了54.4倍的推理加速，为实现高效且强大的隐式推理提供了理论支撑和实践路径。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2601.21576v1",
      "published_date": "2026-01-29 11:42:03 UTC",
      "updated_date": "2026-01-29 11:42:03 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-30T04:28:11.315178+00:00"
    },
    {
      "arxiv_id": "2601.21572v1",
      "title": "Signal-Adaptive Trust Regions for Gradient-Free Optimization of Recurrent Spiking Neural Networks",
      "title_zh": "面向循环脉冲神经网络无梯度优化的信号自适应信任域",
      "authors": [
        "Jinhao Li",
        "Yuhao Sun",
        "Zhiyuan Ma",
        "Hao He",
        "Xinche Zhang",
        "Xing Chen",
        "Jin Li",
        "Sen Song"
      ],
      "abstract": "Recurrent spiking neural networks (RSNNs) are a promising substrate for energy-efficient control policies, but training them for high-dimensional, long-horizon reinforcement learning remains challenging. Population-based, gradient-free optimization circumvents backpropagation through non-differentiable spike dynamics by estimating gradients. However, with finite populations, high variance of these estimates can induce harmful and overly aggressive update steps. Inspired by trust-region methods in reinforcement learning that constrain policy updates in distribution space, we propose \\textbf{Signal-Adaptive Trust Regions (SATR)}, a distributional update rule that constrains relative change by bounding KL divergence normalized by an estimated signal energy. SATR automatically expands the trust region under strong signals and contracts it when updates are noise-dominated. We instantiate SATR for Bernoulli connectivity distributions, which have shown strong empirical performance for RSNN optimization. Across a suite of high-dimensional continuous-control benchmarks, SATR improves stability under limited populations and reaches competitive returns against strong baselines including PPO-LSTM. In addition, to make SATR practical at scale, we introduce a bitset implementation for binary spiking and binary weights, substantially reducing wall-clock training time and enabling fast RSNN policy search.",
      "tldr_zh": "该研究针对循环脉冲神经网络(Recurrent Spiking Neural Networks, RSNNs)在处理高维、长时程强化学习任务时面临的训练难题，提出了一种名为信号自适应置信区域(Signal-Adaptive Trust Regions, SATR)的分布更新规则。针对无梯度优化在有限种群下梯度估计方差大、导致更新步长过激的问题，SATR 通过限制由估计信号能量归一化的 KL 散度来约束策略更新，使其能够在强信号下自动扩大置信区域，而在噪声主导时自动收缩。研究人员将该方法应用于 Bernoulli 连接分布，并针对二进制脉冲和权重引入了位集(Bitset)实现，大幅提升了实际训练效率。在一系列高维连续控制基准测试中，SATR 显著提高了在有限种群下的优化稳定性，其性能表现足以与 PPO-LSTM 等强基准模型竞争。该成果为实现能效型控制策略在大规模环境下的高效搜索提供了有力支持。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2601.21572v1",
      "published_date": "2026-01-29 11:34:49 UTC",
      "updated_date": "2026-01-29 11:34:49 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-30T04:28:31.326784+00:00"
    },
    {
      "arxiv_id": "2601.21571v1",
      "title": "Shaping capabilities with token-level data filtering",
      "title_zh": "通过词元级数据过滤塑造模型能力",
      "authors": [
        "Neil Rathi",
        "Alec Radford"
      ],
      "abstract": "Current approaches to reducing undesired capabilities in language models are largely post hoc, and can thus be easily bypassed by adversaries. A natural alternative is to shape capabilities during pretraining itself. On the proxy task of removing medical capabilities, we show that the simple intervention of filtering pretraining data is highly effective, robust, and inexpensive at scale. Inspired by work on data attribution, we show that filtering tokens is more effective than filtering documents, achieving the same hit to undesired capabilities at a lower cost to benign ones. Training models spanning two orders of magnitude, we then demonstrate that filtering gets more effective with scale: for our largest models, token filtering leads to a 7000x compute slowdown on the forget domain. We also show that models trained with token filtering can still be aligned on the forget domain. Along the way, we introduce a methodology for labeling tokens with sparse autoencoders and distilling cheap, high-quality classifiers. We also demonstrate that filtering can be robust to noisy labels with sufficient pretraining compute.",
      "tldr_zh": "该研究探讨了在预训练阶段通过数据过滤来塑造语言模型能力的方法，以解决事后干预容易被绕过的问题。作者通过移除医学能力的实验证明，Token-level data filtering（令牌级数据过滤）相比文档级过滤更为有效且成本更低，能在减少目标能力的同时最大限度保留通用能力。研究引入了利用 Sparse Autoencoders (SAEs) 标注令牌并蒸馏分类器的方法，并发现过滤效果随模型规模显著增强，在大型模型上可使 forget domain（遗忘领域）的计算成本增加7000倍。此外，该方法对标签噪声具有鲁棒性，且过滤后的模型仍可在相应领域进行对齐。这项工作证明了在预训练阶段精准过滤数据是构建安全、受控大模型的一种高效且稳健的路径。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.LG",
      "comment": "33 pages, 28 figures",
      "pdf_url": "https://arxiv.org/pdf/2601.21571v1",
      "published_date": "2026-01-29 11:34:01 UTC",
      "updated_date": "2026-01-29 11:34:01 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-30T04:28:26.456164+00:00"
    },
    {
      "arxiv_id": "2601.21570v1",
      "title": "EmboCoach-Bench: Benchmarking AI Agents on Developing Embodied Robots",
      "title_zh": "EmboCoach-Bench：评估 AI 智能体开发具身机器人能力的基准测试",
      "authors": [
        "Zixing Lei",
        "Genjia Liu",
        "Yuanshuo Zhang",
        "Qipeng Liu",
        "Chuan Wen",
        "Shanghang Zhang",
        "Wenzhao Lian",
        "Siheng Chen"
      ],
      "abstract": "The field of Embodied AI is witnessing a rapid evolution toward general-purpose robotic systems, fueled by high-fidelity simulation and large-scale data collection. However, this scaling capability remains severely bottlenecked by a reliance on labor-intensive manual oversight from intricate reward shaping to hyperparameter tuning across heterogeneous backends. Inspired by LLMs' success in software automation and science discovery, we introduce \\textsc{EmboCoach-Bench}, a benchmark evaluating the capacity of LLM agents to autonomously engineer embodied policies. Spanning 32 expert-curated RL and IL tasks, our framework posits executable code as the universal interface. We move beyond static generation to assess a dynamic closed-loop workflow, where agents leverage environment feedback to iteratively draft, debug, and optimize solutions, spanning improvements from physics-informed reward design to policy architectures such as diffusion policies. Extensive evaluations yield three critical insights: (1) autonomous agents can qualitatively surpass human-engineered baselines by 26.5\\% in average success rate; (2) agentic workflow with environment feedback effectively strengthens policy development and substantially narrows the performance gap between open-source and proprietary models; and (3) agents exhibit self-correction capabilities for pathological engineering cases, successfully resurrecting task performance from near-total failures through iterative simulation-in-the-loop debugging. Ultimately, this work establishes a foundation for self-evolving embodied intelligence, accelerating the paradigm shift from labor-intensive manual tuning to scalable, autonomous engineering in embodied AI field.",
      "tldr_zh": "该研究针对 Embodied AI 领域中人工调整奖励函数和超参数的低效瓶颈，提出了 EmboCoach-Bench 这一新型基准，旨在评估 LLM 智能体自主开发具身智能策略的能力。该框架涵盖 32 项专家策划的强化学习 (RL) 和模仿学习 (IL) 任务，以可执行代码作为通用接口，构建了包含环境反馈的动态闭环工作流。智能体能够利用反馈迭代优化解决方案，涵盖从物理信息奖励设计 (physics-informed reward design) 到扩散策略 (diffusion policies) 架构的改进。实验结果表明，自主智能体在平均成功率上比人工设计的基准高出 26.5%，且环境反馈驱动的 Agentic Workflow 显著缩小了开源与私有模型间的性能差距。此外，智能体通过循环模拟调试 (simulation-in-the-loop debugging) 展示了卓越的自我修正能力，能够从极端的工程失效中恢复任务表现。该工作为实现自我演进的具身智能奠定了基础，推动了 Embodied AI 从劳动密集型手动调整向可扩展自主工程的范式转变。",
      "categories": [
        "cs.AI",
        "cs.RO"
      ],
      "primary_category": "cs.AI",
      "comment": "37 pages, 13 figures",
      "pdf_url": "https://arxiv.org/pdf/2601.21570v1",
      "published_date": "2026-01-29 11:33:49 UTC",
      "updated_date": "2026-01-29 11:33:49 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-30T04:28:30.105663+00:00"
    },
    {
      "arxiv_id": "2601.21561v1",
      "title": "SAL: Selective Adaptive Learning for Backpropagation-Free Training with Sparsification",
      "title_zh": "SAL：结合稀疏化的无反向传播训练选择性自适应学习",
      "authors": [
        "Fanping Liu",
        "Hua Yang",
        "Jiasi Zou"
      ],
      "abstract": "Standard deep learning relies on Backpropagation (BP), which is constrained by biologically implausible weight symmetry and suffers from significant gradient interference within dense representations. To mitigate these bottlenecks, we propose Selective Adaptive Learning (SAL), a training method that combines selective parameter activation with adaptive area partitioning. Specifically, SAL decomposes the parameter space into mutually exclusive, sample-dependent regions. This decoupling mitigates gradient interference across divergent semantic patterns and addresses explicit weight symmetry requirements through our refined feedback alignment. Empirically, SAL demonstrates competitive convergence rates, leading to improved classification performance across 10 standard benchmarks. Additionally, SAL achieves numerical consistency and competitive accuracy even in deep regimes (up to 128 layers) and large-scale models (up to 1B parameters). Our approach is loosely inspired by biological learning mechanisms, offering a plausible alternative that contributes to the study of scalable neural network training.",
      "tldr_zh": "该研究提出了Selective Adaptive Learning (SAL)，这是一种结合了选择性参数激活与自适应区域划分的训练方法，旨在解决标准反向传播 (Backpropagation) 中由于权重对称性导致的生物学合理性缺失以及稠密表示下的梯度干扰问题。SAL通过将参数空间分解为互斥且依赖于样本的区域，有效地缓解了不同语义模式间的梯度冲突。该方法进一步利用改进的反馈对齐 (Feedback Alignment) 机制处理了显式权重对称的要求，实现了无需反向传播的神经网络训练。实验结果显示，SAL在10项标准基准测试中均取得了优异的收敛速度和分类性能。此外，SAL在深层架构（达128层）及大规模模型（达1B参数）中依然保持了极高的数值一致性和准确率。该方法受生物学习机制启发，为构建可扩展且具有生物学合理性的深度学习模型提供了重要的理论支持与实践方案。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2601.21561v1",
      "published_date": "2026-01-29 11:26:26 UTC",
      "updated_date": "2026-01-29 11:26:26 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-30T04:28:51.458807+00:00"
    },
    {
      "arxiv_id": "2601.21557v1",
      "title": "Meta Context Engineering via Agentic Skill Evolution",
      "title_zh": "基于智能体技能演化的元上下文工程",
      "authors": [
        "Haoran Ye",
        "Xuning He",
        "Vincent Arak",
        "Haonan Dong",
        "Guojie Song"
      ],
      "abstract": "The operational efficacy of large language models relies heavily on their inference-time context. This has established Context Engineering (CE) as a formal discipline for optimizing these inputs. Current CE methods rely on manually crafted harnesses, such as rigid generation-reflection workflows and predefined context schemas. They impose structural biases and restrict context optimization to a narrow, intuition-bound design space. To address this, we introduce Meta Context Engineering (MCE), a bi-level framework that supersedes static CE heuristics by co-evolving CE skills and context artifacts. In MCE iterations, a meta-level agent refines engineering skills via agentic crossover, a deliberative search over the history of skills, their executions, and evaluations. A base-level agent executes these skills, learns from training rollouts, and optimizes context as flexible files and code. We evaluate MCE across five disparate domains under offline and online settings. MCE demonstrates consistent performance gains, achieving 5.6--53.8% relative improvement over state-of-the-art agentic CE methods (mean of 16.9%), while maintaining superior context adaptability, transferability, and efficiency in both context usage and training.",
      "tldr_zh": "该研究提出了Meta Context Engineering (MCE)，这是一个旨在解决当前Context Engineering (CE)中因手动构建和预定义模式导致的结构偏差及优化限制的双层框架。MCE通过共同进化CE技能和context artifacts来超越传统的静态启发式方法。在该框架中，meta-level agent通过agentic crossover对技能历史、执行及评估进行审慎搜索以提炼工程技能，而base-level agent则执行技能并将上下文优化为灵活的文件和代码。实验结果表明，MCE在五个不同领域中相比最先进的agentic CE方法实现了5.6%至53.8%的相对性能提升，平均增幅达16.9%。此外，该方法在保持卓越的上下文适应性和迁移性的同时，还显著提高了上下文的使用效率与训练效率。",
      "categories": [
        "cs.AI",
        "cs.NE"
      ],
      "primary_category": "cs.AI",
      "comment": "46 pages, 4 figures",
      "pdf_url": "https://arxiv.org/pdf/2601.21557v1",
      "published_date": "2026-01-29 11:22:02 UTC",
      "updated_date": "2026-01-29 11:22:02 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-30T04:28:45.317514+00:00"
    },
    {
      "arxiv_id": "2601.21548v1",
      "title": "Training slow silicon neurons to control extremely fast robots with spiking reinforcement learning",
      "title_zh": "利用脉冲强化学习训练低速硅神经元以控制超高速机器人",
      "authors": [
        "Irene Ambrosini",
        "Ingo Blakowski",
        "Dmitrii Zendrikov",
        "Cristiano Capone",
        "Luna Gava",
        "Giacomo Indiveri",
        "Chiara De Luca",
        "Chiara Bartolozzi"
      ],
      "abstract": "Air hockey demands split-second decisions at high puck velocities, a challenge we address with a compact network of spiking neurons running on a mixed-signal analog/digital neuromorphic processor. By co-designing hardware and learning algorithms, we train the system to achieve successful puck interactions through reinforcement learning in a remarkably small number of trials. The network leverages fixed random connectivity to capture the task's temporal structure and adopts a local e-prop learning rule in the readout layer to exploit event-driven activity for fast and efficient learning. The result is real-time learning with a setup comprising a computer and the neuromorphic chip in-the-loop, enabling practical training of spiking neural networks for robotic autonomous systems. This work bridges neuroscience-inspired hardware with real-world robotic control, showing that brain-inspired approaches can tackle fast-paced interaction tasks while supporting always-on learning in intelligent machines.",
      "tldr_zh": "该研究针对气垫球(Air hockey)运动中极高球速对即时决策提出的挑战，提出了一种利用脉冲强化学习(spiking reinforcement learning)控制高速机器人的方法。该系统在数模混合信号神经形态处理器(mixed-signal analog/digital neuromorphic processor)上运行小型脉冲神经元网络，并采用硬件与学习算法协同设计的策略，在极少尝试中即实现了成功的交互。网络通过固定的随机连接(fixed random connectivity)捕捉时间结构，并在读取层利用局部e-prop学习规则处理事件驱动(event-driven)活动。通过计算机与神经形态芯片在环(in-the-loop)的实时训练，该工作展示了为机器人自主系统训练脉冲神经网络(SNNs)的实用性。这项研究有效弥合了神经科学启发硬件与现实机器人控制之间的鸿沟，证明了脑启发(brain-inspired)方法在处理快节奏任务及支持始终在线学习(always-on learning)方面的巨大潜力。",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.ET"
      ],
      "primary_category": "cs.RO",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2601.21548v1",
      "published_date": "2026-01-29 11:05:23 UTC",
      "updated_date": "2026-01-29 11:05:23 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-30T04:28:59.487265+00:00"
    },
    {
      "arxiv_id": "2601.21547v1",
      "title": "Multi-Modal Time Series Prediction via Mixture of Modulated Experts",
      "title_zh": "基于调制混合专家的多模态时间序列预测",
      "authors": [
        "Lige Zhang",
        "Ali Maatouk",
        "Jialin Chen",
        "Leandros Tassiulas",
        "Rex Ying"
      ],
      "abstract": "Real-world time series exhibit complex and evolving dynamics, making accurate forecasting extremely challenging. Recent multi-modal forecasting methods leverage textual information such as news reports to improve prediction, but most rely on token-level fusion that mixes temporal patches with language tokens in a shared embedding space. However, such fusion can be ill-suited when high-quality time-text pairs are scarce and when time series exhibit substantial variation in scale and characteristics, thus complicating cross-modal alignment. In parallel, Mixture-of-Experts (MoE) architectures have proven effective for both time series modeling and multi-modal learning, yet many existing MoE-based modality integration methods still depend on token-level fusion. To address this, we propose Expert Modulation, a new paradigm for multi-modal time series prediction that conditions both routing and expert computation on textual signals, enabling direct and efficient cross-modal control over expert behavior. Through comprehensive theoretical analysis and experiments, our proposed method demonstrates substantial improvements in multi-modal time series prediction. The current code is available at https://github.com/BruceZhangReve/MoME",
      "tldr_zh": "该研究提出了 Mixture of Modulated Experts (MoME)，一种用于多模态时间序列预测的新框架。针对现有方法过度依赖 Token-level Fusion 在高质量数据稀缺或序列特征差异大时难以实现跨模态对齐的问题，该研究引入了 Expert Modulation 范式。该范式通过文本信号直接调节 Mixture-of-Experts (MoE) 架构中的 Routing 逻辑和专家计算过程，实现了更为直接且高效的跨模态行为控制。理论分析与多项实验数据共同验证了 MoME 在多模态预测任务中的显著性能提升。这种基于文本信号调节专家行为的方法为处理复杂动态的时间序列数据提供了有效的解决方案。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "26 pages, 12 figures",
      "pdf_url": "https://arxiv.org/pdf/2601.21547v1",
      "published_date": "2026-01-29 11:03:09 UTC",
      "updated_date": "2026-01-29 11:03:09 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-30T04:28:55.974184+00:00"
    },
    {
      "arxiv_id": "2601.21545v1",
      "title": "ShardMemo: Masked MoE Routing for Sharded Agentic LLM Memory",
      "title_zh": "ShardMemo：面向分片式智能体大语言模型内存的掩码 MoE 路由",
      "authors": [
        "Yang Zhao",
        "Chengxiao Dai",
        "Yue Xiu",
        "Mengying Kou",
        "Yuliang Zheng",
        "Dusit Niyato"
      ],
      "abstract": "Agentic large language model (LLM) systems rely on external memory for long-horizon state and concurrent multi-agent execution, but centralized indexes and heuristic partitions become bottlenecks as memory volume and parallel access grow. We present ShardMemo, a budgeted tiered memory service with Tier A per-agent working state, Tier B sharded evidence with shard-local approximate nearest neighbor (ANN) indexes, and Tier C, a versioned skill library. Tier B enforces scope-before-routing: structured eligibility constraints mask ineligible shards before routing or ANN search. We cast shard probing as masked mixture-of-experts (MoE) routing over eligible shards, probing up to $B_{\\mathrm{probe}}$ shards via Top-$B_{\\mathrm{probe}}$ or adaptive Top-$P$, and use cost-aware gating over profile/observation/session shard families; the router is trained from evidence-to-shard supervision. On LoCoMo, ShardMemo improves over the strongest baseline (GAM) by +5.11 to +6.82 F1 across question categories. Under a fixed-budget routing setting ($B_{\\mathrm{probe}}=3$), ShardMemo improves over cosine-to-prototype shard routing by +6.87 F1 while reducing retrieval work (VecScan 521->414, -20.5%) and p95 latency (95->76 ms). On long-context HotpotQA, ShardMemo achieves 63.41/61.88/57.95 F1 at 56K/224K/448K tokens. On ToolBench, Tier C reaches 0.97 Precision@3 and 1.94 StepRed (+10.2% and +7.2% over embedding-similarity retrieval).",
      "tldr_zh": "该研究提出了 ShardMemo，一种用于分片式智能体大语言模型 (Agentic LLM) 内存的有预算分层存储服务，旨在解决随内存容量和并行访问增加而出现的中心化索引瓶颈问题。该系统采用三层架构，包括 Tier A 个体智能体工作状态、带有本地近似最近邻 (ANN) 索引的 Tier B 分片证据，以及 Tier C 版本化技能库。其核心创新在于 Tier B 引入了“先范围后路由”机制，在搜索前屏蔽不合格分片，并将分片探测建模为基于合格分片的掩蔽专家混合 (Masked MoE) 路由。该路由层利用成本感知门控处理不同的分片族，并基于从证据到分片的监督进行训练。实验表明，ShardMemo 在 LoCoMo 测试中比基线模型 GAM 提升了 5.11 至 6.82 的 F1 值。在固定预算路由设置下，该方法在提升准确率的同时减少了 20.5% 的检索工作量，并将 P95 延迟从 95ms 降低至 76ms。此外，ShardMemo 在长上下文 HotpotQA 和 ToolBench 任务中也展现出优异的检索精度和效率，证明了其在复杂长程智能体任务中的有效性。",
      "categories": [
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2601.21545v1",
      "published_date": "2026-01-29 11:01:34 UTC",
      "updated_date": "2026-01-29 11:01:34 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-30T04:28:56.628087+00:00"
    },
    {
      "arxiv_id": "2601.21542v1",
      "title": "Bi-Anchor Interpolation Solver for Accelerating Generative Modeling",
      "title_zh": "加速生成式建模的双锚点插值求解器",
      "authors": [
        "Hongxu Chen",
        "Hongxiang Li",
        "Zhen Wang",
        "Long Chen"
      ],
      "abstract": "Flow Matching (FM) models have emerged as a leading paradigm for high-fidelity synthesis. However, their reliance on iterative Ordinary Differential Equation (ODE) solving creates a significant latency bottleneck. Existing solutions face a dichotomy: training-free solvers suffer from significant performance degradation at low Neural Function Evaluations (NFEs), while training-based one- or few-steps generation methods incur prohibitive training costs and lack plug-and-play versatility. To bridge this gap, we propose the Bi-Anchor Interpolation Solver (BA-solver). BA-solver retains the versatility of standard training-free solvers while achieving significant acceleration by introducing a lightweight SideNet (1-2% backbone size) alongside the frozen backbone. Specifically, our method is founded on two synergistic components: \\textbf{1) Bidirectional Temporal Perception}, where the SideNet learns to approximate both future and historical velocities without retraining the heavy backbone; and 2) Bi-Anchor Velocity Integration, which utilizes the SideNet with two anchor velocities to efficiently approximate intermediate velocities for batched high-order integration. By utilizing the backbone to establish high-precision ``anchors'' and the SideNet to densify the trajectory, BA-solver enables large interval sizes with minimized error. Empirical results on ImageNet-256^2 demonstrate that BA-solver achieves generation quality comparable to 100+ NFEs Euler solver in just 10 NFEs and maintains high fidelity in as few as 5 NFEs, incurring negligible training costs. Furthermore, BA-solver ensures seamless integration with existing generative pipelines, facilitating downstream tasks such as image editing.",
      "tldr_zh": "该研究针对Flow Matching (FM)模型在生成过程中因迭代求解ODE导致的显著延迟瓶颈，提出了Bi-Anchor Interpolation Solver (BA-solver)。该方法在冻结的主干网络旁引入了一个仅占原模型大小1-2%的轻量化SideNet，旨在平衡无训练求解器的通用性与训练型方法的高效性。BA-solver的核心包含两个协同组件：利用SideNet学习近似未来和历史速度的Bidirectional Temporal Perception，以及通过SideNet与两个锚点速度高效逼近中间速度的Bi-Anchor Velocity Integration。在ImageNet-256^2上的实验表明，该方法仅需10次NFEs即可达到传统Euler求解器100次以上的生成质量，且在低至5次NFEs时仍能保持高保真度。此外，BA-solver的训练成本极低，且能无缝集成到现有生成管线中以支持图像编辑等下游任务。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2601.21542v1",
      "published_date": "2026-01-29 10:59:36 UTC",
      "updated_date": "2026-01-29 10:59:36 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-30T04:28:55.493995+00:00"
    },
    {
      "arxiv_id": "2601.21533v1",
      "title": "ARGORA: Orchestrated Argumentation for Causally Grounded LLM Reasoning and Decision Making",
      "title_zh": "ARGORA：面向因果驱动的大语言模型推理与决策的协同论证",
      "authors": [
        "Youngjin Jin",
        "Hanna Kim",
        "Kwanwoo Kim",
        "Chanhee Lee",
        "Seungwon Shin"
      ],
      "abstract": "Existing multi-expert LLM systems gather diverse perspectives but combine them through simple aggregation, obscuring which arguments drove the final decision. We introduce ARGORA, a framework that organizes multi-expert discussions into explicit argumentation graphs showing which arguments support or attack each other. By casting these graphs as causal models, ARGORA can systematically remove individual arguments and recompute outcomes, identifying which reasoning chains were necessary and whether decisions would change under targeted modifications. We further introduce a correction mechanism that aligns internal reasoning with external judgments when they disagree. Across diverse benchmarks and an open-ended use case, ARGORA achieves competitive accuracy and demonstrates corrective behavior: when experts initially disagree, the framework resolves disputes toward correct answers more often than it introduces new errors, while providing causal diagnostics of decisive arguments.",
      "tldr_zh": "该研究提出了 ARGORA 框架，旨在解决现有多专家 LLM 系统在整合多元观点时因简单聚合而导致决策过程不透明的问题。该框架将多专家讨论组织成显式的辩论图 (argumentation graphs)，清晰地展示了论点间的支持与攻击关系。通过将这些图转化为因果模型 (causal models)，ARGORA 能够系统性地执行干预分析，识别哪些推理链对最终决策起到了决定性作用。此外，框架引入的纠正机制 (correction mechanism) 能在推理分歧时将内部逻辑与外部判断进行对齐。实验证明，ARGORA 在多个基准测试中达到了极具竞争力的准确率，并在专家意见分歧时表现出卓越的争议解决能力。该方法不仅提高了决策的准确性，还为复杂推理提供了关键论点的因果诊断 (causal diagnostics)。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "58 pages",
      "pdf_url": "https://arxiv.org/pdf/2601.21533v1",
      "published_date": "2026-01-29 10:48:04 UTC",
      "updated_date": "2026-01-29 10:48:04 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-30T04:29:04.308642+00:00"
    },
    {
      "arxiv_id": "2601.21531v1",
      "title": "On the Adversarial Robustness of Large Vision-Language Models under Visual Token Compression",
      "title_zh": "视觉 Token 压缩下大视觉语言模型的对抗鲁棒性",
      "authors": [
        "Xinwei Zhang",
        "Hangcheng Liu",
        "Li Bai",
        "Hao Wang",
        "Qingqing Ye",
        "Tianwei Zhang",
        "Haibo Hu"
      ],
      "abstract": "Visual token compression is widely used to accelerate large vision-language models (LVLMs) by pruning or merging visual tokens, yet its adversarial robustness remains unexplored. We show that existing encoder-based attacks can substantially overestimate the robustness of compressed LVLMs, due to an optimization-inference mismatch: perturbations are optimized on the full-token representation, while inference is performed through a token-compression bottleneck. To address this gap, we propose the Compression-AliGnEd attack (CAGE), which aligns perturbation optimization with compression inference without assuming access to the deployed compression mechanism or its token budget. CAGE combines (i) expected feature disruption, which concentrates distortion on tokens likely to survive across plausible budgets, and (ii) rank distortion alignment, which actively aligns token distortions with rank scores to promote the retention of highly distorted evidence. Across diverse representative plug-and-play compression mechanisms and datasets, our results show that CAGE consistently achieves lower robust accuracy than the baseline. This work highlights that robustness assessments ignoring compression can be overly optimistic, calling for compression-aware security evaluation and defenses for efficient LVLMs.",
      "tldr_zh": "该研究探讨了大视觉语言模型 (Large Vision-Language Models, LVLMs) 中视觉标记压缩 (Visual token compression) 技术的对抗鲁棒性 (Adversarial robustness)。作者发现，现有的攻击方法在全标记上优化但在压缩瓶颈下推理，这种优化与推理的不匹配导致高估了模型的安全性。为此，该研究提出了 CAGE (Compression-AliGnEd attack)，一种无需预知压缩机制即可将扰动优化与压缩推理对齐的攻击方法。CAGE 通过预期特征破坏 (expected feature disruption) 集中攻击高概率保留的标记，并利用排名失真对齐 (rank distortion alignment) 确保高度失真的信息在压缩中被保留。实验表明，CAGE 在多种代表性压缩机制下均能显著降低模型的鲁棒准确率 (robust accuracy)。该工作揭示了忽略压缩因素的安全性评估风险，并倡导开发具有压缩感知能力的 LVLMs 安全评估与防御体系。",
      "categories": [
        "cs.CR",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.CR",
      "comment": "Under Review, 20 pages",
      "pdf_url": "https://arxiv.org/pdf/2601.21531v1",
      "published_date": "2026-01-29 10:47:21 UTC",
      "updated_date": "2026-01-29 10:47:21 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-30T04:29:09.634951+00:00"
    },
    {
      "arxiv_id": "2601.21527v1",
      "title": "Sustainable Materials Discovery in the Era of Artificial Intelligence",
      "title_zh": "人工智能时代的可持续材料发现",
      "authors": [
        "Sajid Mannan",
        "Rupert J. Myers",
        "Rohit Batra",
        "Rocio Mercado",
        "Lothar Wondraczek",
        "N. M. Anoop Krishnan"
      ],
      "abstract": "Artificial intelligence (AI) has transformed materials discovery, enabling rapid exploration of chemical space through generative models and surrogate screening. Yet current AI workflows optimize performance first, deferring sustainability to post synthesis assessment. This creates inefficiency by the time environmental burdens are quantified, resources have been invested in potentially unsustainable solutions. The disconnect between atomic scale design and lifecycle assessment (LCA) reflects fundamental challenges, data scarcity across heterogeneous sources, scale gaps from atoms to industrial systems, uncertainty in synthesis pathways, and the absence of frameworks that co-optimize performance with environmental impact. We propose to integrate upstream machine learning (ML) assisted materials discovery with downstream lifecycle assessment into a uniform ML-LCA environment. The framework ML-LCA integrates five components, information extraction for building materials-environment knowledge bases, harmonized databases linking properties to sustainability metrics, multi-scale models bridging atomic properties to lifecycle impacts, ensemble prediction of manufacturing pathways with uncertainty quantification, and uncertainty-aware optimization enabling simultaneous performance-sustainability navigation. Case studies spanning glass, cement, semiconductor photoresists, and polymers demonstrate both necessity and feasibility while identifying material-specific integration challenges. Realizing ML-LCA demands coordinated advances in data infrastructure, ex-ante assessment methodologies, multi-objective optimization, and regulatory alignment enabling the discovery of materials that are sustainable by design rather than by chance.",
      "tldr_zh": "该研究探讨了人工智能(AI)在材料发现领域的应用现状，指出目前工作流程往往优先考虑性能而忽略了可持续性，导致在量化环境影响前已造成资源浪费。为此，研究者提出了ML-LCA框架，旨在将上游机器学习(ML)辅助的材料发现与下游生命周期评估(LCA)集成到一个统一的开发环境中。该框架由信息提取、调和数据库、多尺度模型、制造路径集成预测以及不确定性感知优化(Uncertainty-aware optimization)五个核心组件构成，实现了性能与环境指标的同步优化。通过涵盖玻璃、水泥、半导体光刻胶(Photoresists)及聚合物的案例研究，证明了该框架在应对数据稀缺、尺度差异和合成路径不确定性等挑战时的有效性。该项工作为实现“由设计而非偶然”驱动的可持续材料发现奠定了方法论基础，并强调了在数据基础设施、事前评估方法和多目标优化(Multi-objective optimization)方面进行协调创新的必要性。",
      "categories": [
        "cond-mat.mtrl-sci",
        "cs.AI"
      ],
      "primary_category": "cond-mat.mtrl-sci",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2601.21527v1",
      "published_date": "2026-01-29 10:42:44 UTC",
      "updated_date": "2026-01-29 10:42:44 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-30T04:29:10.978243+00:00"
    },
    {
      "arxiv_id": "2601.21526v1",
      "title": "KAPSO: A Knowledge-grounded framework for Autonomous Program Synthesis and Optimization",
      "title_zh": "KAPSO：基于知识的自主程序合成与优化框架",
      "authors": [
        "Alireza Nadaf",
        "Alireza Mohammadshahi",
        "Majid Yazdani"
      ],
      "abstract": "We introduce KAPSO, a modular framework for autonomous program synthesis and optimization. Given a natural language goal and an evaluation method, KAPSO iteratively performs ideation, code synthesis and editing, execution, evaluation, and learning to improve a runnable artifact toward measurable objectives. Rather than treating synthesis as the endpoint, KAPSO uses synthesis as an operator within a long-horizon optimization loop, where progress is defined by evaluator outcomes.\n  KAPSO targets long-horizon failures common in coding agents, including lost experimental state, brittle debugging, and weak reuse of domain expertise, by integrating three tightly coupled components. First, a git-native experimentation engine isolates each attempt as a branch, producing reproducible artifacts and preserving provenance across iterations. Second, a knowledge system ingests heterogeneous sources, including repositories, internal playbooks, and curated external resources such as documentation, scientific papers, and web search results, and organizes them into a structured representation that supports retrieval over workflows, implementations, and environment constraints. Third, a cognitive memory layer coordinates retrieval and maintains an episodic store of reusable lessons distilled from experiment traces (run logs, diffs, and evaluator feedback), reducing repeated error modes and accelerating convergence.\n  We evaluated KAPSO on MLE-Bench (Kaggle-style ML competitions) and ALE-Bench (AtCoder heuristic optimization), and report end-to-end performance.\n  Code Available at: https://github.com/Leeroo-AI/kapso",
      "tldr_zh": "该研究提出了KAPSO，这是一个用于自主程序合成与优化(Autonomous Program Synthesis and Optimization)的模块化框架，旨在通过自然语言目标和评估方法持续改进程序性能。KAPSO将合成视为长时程(long-horizon)优化循环中的核心算子，解决了编码智能体中常见的实验状态丢失和调试困难等问题。该框架集成了git-native experimentation engine以实现实验分支隔离，并利用knowledge system整合代码库、科学论文等异构资源来提供结构化知识支持。其核心的cognitive memory layer能够从实验轨迹中提炼并存储经验教训，有效降低了错误重复率并加速任务收敛。在MLE-Bench和ALE-Bench上的评估结果表明，KAPSO在机器学习竞赛和启发式优化任务中展现了出色的端到端自动化能力。",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.SE"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2601.21526v1",
      "published_date": "2026-01-29 10:40:54 UTC",
      "updated_date": "2026-01-29 10:40:54 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-30T04:29:20.923700+00:00"
    },
    {
      "arxiv_id": "2601.21522v1",
      "title": "More Bang for the Buck: Improving the Inference of Large Language Models at a Fixed Budget using Reset and Discard (ReD)",
      "title_zh": "物尽其用：利用重置与舍弃 (ReD) 优化固定预算下的大语言模型推理",
      "authors": [
        "Sagi Meir",
        "Tommer D. Keidar",
        "Noam Levi",
        "Shlomi Reuveni",
        "Barak Hirshberg"
      ],
      "abstract": "The performance of large language models (LLMs) on verifiable tasks is usually measured by pass@k, the probability of answering a question correctly at least once in k trials. At a fixed budget, a more suitable metric is coverage@cost, the average number of unique questions answered as a function of the total number of attempts. We connect the two metrics and show that the empirically-observed power-law behavior in pass@k leads to a sublinear growth of the coverage@cost (diminishing returns). To solve this problem, we propose Reset-and-Discard (ReD), a query method of LLMs that increases coverage@cost for any given budget, regardless of the pass@k form. Moreover, given a pass@k, we can quantitatively predict the savings in the total number of attempts using ReD. If pass@k is not available for the model, ReD can infer its power-law exponent. Experiments on three LLMs using HumanEval demonstrate that ReD substantially reduces the required attempts, tokens, and USD cost to reach a desired coverage, while also offering an efficient way to measure inference power-laws.",
      "tldr_zh": "该研究探讨了大型语言模型(LLMs)在可验证任务中的性能评估，指出传统的pass@k指标在固定预算下会导致coverage@cost呈现亚线性增长，从而产生收益递减问题。作者揭示了pass@k中观察到的幂律行为(power-law behavior)与这种低效增长之间的逻辑联系，并据此提出了Reset-and-Discard (ReD)查询方法。ReD能够在任何给定预算下显著提高coverage@cost，且具备定量预测成本节省以及在指标缺失时推断模型幂律指数的能力。在HumanEval基准上对三种LLMs的实验证明，该方法能大幅减少达到目标覆盖率所需的尝试次数、Token数量和实际资金成本。总体而言，ReD为理解和优化LLMs的推理幂律(inference power-laws)提供了一种高效且低成本的解决方案。",
      "categories": [
        "cs.LG",
        "cond-mat.dis-nn",
        "cs.AI",
        "stat.ML"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2601.21522v1",
      "published_date": "2026-01-29 10:37:32 UTC",
      "updated_date": "2026-01-29 10:37:32 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-30T04:29:34.287422+00:00"
    },
    {
      "arxiv_id": "2601.21511v1",
      "title": "LLaMEA-SAGE: Guiding Automated Algorithm Design with Structural Feedback from Explainable AI",
      "title_zh": "LLaMEA-SAGE：利用可解释人工智能的结构化反馈引导自动化算法设计",
      "authors": [
        "Niki van Stein",
        "Anna V. Kononova",
        "Lars Kotthoff",
        "Thomas Bäck"
      ],
      "abstract": "Large language models have enabled automated algorithm design (AAD) by generating optimization algorithms directly from natural-language prompts. While evolutionary frameworks such as LLaMEA demonstrate strong exploratory capabilities across the algorithm design space, their search dynamics are entirely driven by fitness feedback, leaving substantial information about the generated code unused. We propose a mechanism for guiding AAD using feedback constructed from graph-theoretic and complexity features extracted from the abstract syntax trees of the generated algorithms, based on a surrogate model learned over an archive of evaluated solutions. Using explainable AI techniques, we identify features that substantially affect performance and translate them into natural-language mutation instructions that steer subsequent LLM-based code generation without restricting expressivity.\n  We propose LLaMEA-SAGE, which integrates this feature-driven guidance into LLaMEA, and evaluate it across several benchmarks. We show that the proposed structured guidance achieves the same performance faster than vanilla LLaMEA in a small controlled experiment. In a larger-scale experiment using the MA-BBOB suite from the GECCO-MA-BBOB competition, our guided approach achieves superior performance compared to state-of-the-art AAD methods. These results demonstrate that signals derived from code can effectively bias LLM-driven algorithm evolution, bridging the gap between code structure and human-understandable performance feedback in automated algorithm design.",
      "tldr_zh": "该研究提出了LLaMEA-SAGE，这是一种利用可解释人工智能(Explainable AI)的结构化反馈来引导自动算法设计(Automated Algorithm Design)的新框架。针对现有演化框架仅依赖适应度反馈而忽视代码结构信息的局限，该方法从生成算法的抽象语法树(Abstract Syntax Trees)中提取图论与复杂度特征，并通过学习代理模型识别影响性能的关键特征。这些特征被转化为自然语言变异指令，在不限制表达能力的前提下精确引导大语言模型(LLM)的代码生成过程。实验结果显示，LLaMEA-SAGE在受控实验中比原始LLaMEA收敛速度更快，且在MA-BBOB基准测试中优于现有的先进AAD方法。该成果证明了从代码中衍生的结构信号能有效引导LLM驱动的算法演化，成功架起了代码微观结构与人类可理解性能反馈之间的桥梁。",
      "categories": [
        "cs.AI",
        "cs.NE",
        "cs.SE"
      ],
      "primary_category": "cs.AI",
      "comment": "14 pages",
      "pdf_url": "https://arxiv.org/pdf/2601.21511v1",
      "published_date": "2026-01-29 10:27:29 UTC",
      "updated_date": "2026-01-29 10:27:29 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-30T04:29:35.457203+00:00"
    },
    {
      "arxiv_id": "2601.21505v1",
      "title": "The Effectiveness of Style Vectors for Steering Large Language Models: A Human Evaluation",
      "title_zh": "风格向量引导大语言模型的有效性：一项人类评估研究",
      "authors": [
        "Diaoulé Diallo",
        "Katharina Dworatzyk",
        "Sophie Jentzsch",
        "Peer Schütt",
        "Sabine Theis",
        "Tobias Hecking"
      ],
      "abstract": "Controlling the behavior of large language models (LLMs) at inference time is essential for aligning outputs with human abilities and safety requirements. \\emph{Activation steering} provides a lightweight alternative to prompt engineering and fine-tuning by directly modifying internal activations to guide generation. This research advances the literature in three significant directions. First, while previous work demonstrated the technical feasibility of steering emotional tone using automated classifiers, this paper presents the first human evaluation of activation steering concerning the emotional tone of LLM outputs, collecting over 7,000 crowd-sourced ratings from 190 participants via Prolific ($n=190$). These ratings assess both perceived emotional intensity and overall text quality. Second, we find strong alignment between human and model-based quality ratings (mean $r=0.776$, range $0.157$--$0.985$), indicating automatic scoring can proxy perceived quality. Moderate steering strengths ($λ\\approx 0.15$) reliably amplify target emotions while preserving comprehensibility, with the strongest effects for disgust ($η_p^2 = 0.616$) and fear ($η_p^2 = 0.540$), and minimal effects for surprise ($η_p^2 = 0.042$). Finally, upgrading from Alpaca to LlaMA-3 yielded more consistent steering with significant effects across emotions and strengths (all $p < 0.001$). Inter-rater reliability was high (ICC $= 0.71$--$0.87$), underscoring the robustness of the findings. These findings support activation-based control as a scalable method for steering LLM behavior across affective dimensions.",
      "tldr_zh": "本研究评估了风格向量(Style Vectors)在控制大语言模型(LLMs)输出时的有效性，重点探讨了激活转向(Activation Steering)这一通过直接修改内部激活来引导生成的轻量化方法。研究人员通过 Prolific 平台收集了 190 名参与者的 7000 多项评分，完成了首个针对激活转向影响情感色调的人类评估。实验结果显示，人类评分与模型自动评分之间存在强相关性(平均 $r=0.776$)，证明自动评分可作为感知质量的有效替代指标。在转向强度 $λ\\approx 0.15$ 时，该方法能有效放大目标情感并保持文本可理解性，其中对厌恶(disgust)和恐惧(fear)的效果最为显著。此外，将模型从 Alpaca 升级至 LlaMA-3 显著提升了转向的一致性，证明了基于激活的控制是在情感维度上引导 LLM 行为的一种具有扩展性的稳健方法。",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.HC"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2601.21505v1",
      "published_date": "2026-01-29 10:24:34 UTC",
      "updated_date": "2026-01-29 10:24:34 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-30T04:29:36.717205+00:00"
    },
    {
      "arxiv_id": "2601.21503v1",
      "title": "MAR: Efficient Large Language Models via Module-aware Architecture Refinement",
      "title_zh": "MAR：基于模块感知架构优化的高效大语言模型",
      "authors": [
        "Junhong Cai",
        "Guiqin Wang",
        "Kejie Zhao",
        "Jianxiong Tang",
        "Xiang Wang",
        "Luziwei Leng",
        "Ran Cheng",
        "Yuxin Ma",
        "Qinghai Guo"
      ],
      "abstract": "Large Language Models (LLMs) excel across diverse domains but suffer from high energy costs due to quadratic attention and dense Feed-Forward Network (FFN) operations. To address these issues, we propose Module-aware Architecture Refinement (MAR), a two-stage framework that integrates State Space Models (SSMs) for linear-time sequence modeling and applies activation sparsification to reduce FFN costs. In addition, to mitigate low information density and temporal mismatch in integrating Spiking Neural Networks (SNNs) with SSMs, we design the Adaptive Ternary Multi-step Neuron (ATMN) and the Spike-aware Bidirectional Distillation Strategy (SBDS). Extensive experiments demonstrate that MAR effectively restores the performance of its dense counterpart under constrained resources while substantially reducing inference energy consumption. Furthermore, it outperforms efficient models of comparable or even larger scale, underscoring its potential for building efficient and practical LLMs.",
      "tldr_zh": "该研究提出了 Module-aware Architecture Refinement (MAR)，这是一个旨在提升大语言模型 (LLMs) 效率的两阶段框架，主要解决注意力机制 (Attention) 的高计算复杂度和稠密前馈网络 (FFN) 的能耗问题。MAR 通过集成状态空间模型 (State Space Models, SSMs) 实现线性时间序列建模，并采用激活稀疏化技术来降低 FFN 成本。为了克服脉冲神经网络 (Spiking Neural Networks, SNNs) 与 SSMs 集成时的信息密度低和时间不匹配挑战，该框架引入了自适应三元多步神经元 (Adaptive Ternary Multi-step Neuron, ATMN) 以及脉冲感知双向蒸馏策略 (Spike-aware Bidirectional Distillation Strategy, SBDS)。大量实验证明，MAR 在显著降低推理能耗的同时，能有效恢复稠密模型的性能表现。此外，MAR 的性能优于同等规模甚至更大规模的其他高效模型，突显了其在构建实用且低能耗 LLMs 方面的应用前景。",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.LG",
        "cs.NE"
      ],
      "primary_category": "cs.AI",
      "comment": "Accepted by ICASSP 2026. 5 pages, 5 figures",
      "pdf_url": "https://arxiv.org/pdf/2601.21503v1",
      "published_date": "2026-01-29 10:21:28 UTC",
      "updated_date": "2026-01-29 10:21:28 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-30T04:29:38.280834+00:00"
    },
    {
      "arxiv_id": "2601.21498v1",
      "title": "SimGraph: A Unified Framework for Scene Graph-Based Image Generation and Editing",
      "title_zh": "SimGraph：基于场景图的图像生成与编辑统一框架",
      "authors": [
        "Thanh-Nhan Vo",
        "Trong-Thuan Nguyen",
        "Tam V. Nguyen",
        "Minh-Triet Tran"
      ],
      "abstract": "Recent advancements in Generative Artificial Intelligence (GenAI) have significantly enhanced the capabilities of both image generation and editing. However, current approaches often treat these tasks separately, leading to inefficiencies and challenges in maintaining spatial consistency and semantic coherence between generated content and edits. Moreover, a major obstacle is the lack of structured control over object relationships and spatial arrangements. Scene graph-based methods, which represent objects and their interrelationships in a structured format, offer a solution by providing greater control over composition and interactions in both image generation and editing. To address this, we introduce SimGraph, a unified framework that integrates scene graph-based image generation and editing, enabling precise control over object interactions, layouts, and spatial coherence. In particular, our framework integrates token-based generation and diffusion-based editing within a single scene graph-driven model, ensuring high-quality and consistent results. Through extensive experiments, we empirically demonstrate that our approach outperforms existing state-of-the-art methods.",
      "tldr_zh": "该研究提出了 SimGraph，这是一个统一的场景图(Scene Graph)驱动框架，旨在整合图像生成与编辑任务。针对当前生成式人工智能(GenAI)在处理生成与编辑时存在的空间一致性不足和缺乏结构化控制等问题，SimGraph 在单个模型中无缝集成了基于令牌(token-based)的生成和基于扩散(diffusion-based)的编辑技术。该框架通过场景图提供的结构化信息，实现了对物体交互、布局以及空间相干性的精确控制，确保了生成内容与编辑结果之间的高度一致性。大量实验结果表明，SimGraph 在性能上显著优于现有的最先进方法，为高质量图像创作与精准编辑提供了高效且统一的路径。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2601.21498v1",
      "published_date": "2026-01-29 10:15:55 UTC",
      "updated_date": "2026-01-29 10:15:55 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-30T04:29:42.633886+00:00"
    },
    {
      "arxiv_id": "2601.21494v1",
      "title": "The Path of Least Resistance: Guiding LLM Reasining Trajectories with Prefix Consensus",
      "title_zh": "阻力最小路径：利用前缀共识引导大语言模型推理轨迹",
      "authors": [
        "Ishan Jindal",
        "Sai Prashanth Akuthota",
        "Jayant Taneja",
        "Sachin Dev Sharma"
      ],
      "abstract": "Large language models achieve strong reasoning performance, but inference strategies such as Self-Consistency (SC) are computationally expensive, as they fully expand all reasoning traces. We introduce PoLR (Path of Least Resistance), the first inference-time method to leverage prefix consistency for compute-efficient reasoning. PoLR clusters short prefixes of reasoning traces, identifies the dominant cluster, and expands all paths in that cluster, preserving the accuracy benefits of SC while substantially reducing token usage and latency. Our theoretical analysis, framed via mutual information and entropy, explains why early reasoning steps encode strong signals predictive of final correctness. Empirically, PoLR consistently matches or exceeds SC across GSM8K, MATH500, AIME24/25, and GPQA-DIAMOND, reducing token usage by up to 60% and wall-clock latency by up to 50%. Moreover, PoLR is fully complementary to adaptive inference methods (e.g., Adaptive Consistency, Early-Stopping SC) and can serve as a drop-in pre-filter, making SC substantially more efficient and scalable without requiring model fine-tuning.",
      "tldr_zh": "该研究提出了PoLR (Path of Least Resistance)，这是首个利用前缀一致性(prefix consistency)来实现计算高效推理的推理时方法，旨在解决Self-Consistency (SC)等策略因完全展开所有推理轨迹而导致的高昂计算成本。PoLR通过对推理轨迹的短前缀进行聚类并识别主导簇(dominant cluster)，仅展开该簇内的路径，在保留SC准确性优势的同时显著降低了Token使用量和延迟。基于互信息(mutual information)和熵(entropy)的理论分析解释了早期推理步骤为何能包含预测最终正确性的强信号。实验表明，PoLR在GSM8K、MATH500、AIME24/25和GPQA-DIAMOND等基准测试中始终匹配或超越SC的效果，Token消耗减少高达60%，延迟降低多达50%。此外，PoLR可以作为即插即用的预过滤器(pre-filter)，与自适应推理方法(adaptive inference methods)完全互补，无需模型微调即可使SC更具效率和可扩展性。",
      "categories": [
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.AI",
      "comment": "Accepted at ICLR 2026. https://openreview.net/forum?id=hrnSqERgPn",
      "pdf_url": "https://arxiv.org/pdf/2601.21494v1",
      "published_date": "2026-01-29 10:14:24 UTC",
      "updated_date": "2026-01-29 10:14:24 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-30T04:29:44.601058+00:00"
    },
    {
      "arxiv_id": "2601.21477v1",
      "title": "Mean-Field Control on Sparse Graphs: From Local Limits to GNNs via Neighborhood Distributions",
      "title_zh": "稀疏图上的平均场控制：从局部极限到基于邻域分布的图神经网络",
      "authors": [
        "Tobias Schmidt",
        "Kai Cui"
      ],
      "abstract": "Mean-field control (MFC) offers a scalable solution to the curse of dimensionality in multi-agent systems but traditionally hinges on the restrictive assumption of exchangeability via dense, all-to-all interactions. In this work, we bridge the gap to real-world network structures by proposing a rigorous framework for MFC on large sparse graphs. We redefine the system state as a probability measure over decorated rooted neighborhoods, effectively capturing local heterogeneity. Our central contribution is a theoretical foundation for scalable reinforcement learning in this setting. We prove horizon-dependent locality: for finite-horizon problems, an agent's optimal policy at time t depends strictly on its (T-t)-hop neighborhood. This result renders the infinite-dimensional control problem tractable and underpins a novel Dynamic Programming Principle (DPP) on the lifted space of neighborhood distributions. Furthermore, we formally and experimentally justify the use of Graph Neural Networks (GNNs) for actor-critic algorithms in this context. Our framework naturally recovers classical MFC as a degenerate case while enabling efficient, theoretically grounded control on complex sparse topologies.",
      "tldr_zh": "该研究针对传统平均场控制（Mean-field control, MFC）在处理现实世界稀疏网络结构时的局限性，提出了一个适用于大型稀疏图（sparse graphs）的严谨MFC理论框架。通过将系统状态重新定义为修饰根邻域（decorated rooted neighborhoods）上的概率分布，该框架成功捕捉了局部异质性，并证明了时界相关的局部性原理，即智能体在 t 时刻的最优策略严格取决于其 (T-t) 跳邻域。这一结论使无限维控制问题变得可解，并支持在提升的邻域分布空间上构建新型动态规划原理（Dynamic Programming Principle, DPP）。此外，研究在理论和实验上均证明了将图神经网络（GNNs）用于该场景下 actor-critic 算法的合理性。该成果不仅将经典MFC视作一种退化特例，还实现了在复杂稀疏拓扑结构上进行高效且有理论支撑的强化学习与控制。",
      "categories": [
        "cs.MA",
        "cs.AI",
        "cs.LG",
        "math.OC"
      ],
      "primary_category": "cs.MA",
      "comment": "19 pages",
      "pdf_url": "https://arxiv.org/pdf/2601.21477v1",
      "published_date": "2026-01-29 09:57:48 UTC",
      "updated_date": "2026-01-29 09:57:48 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-30T04:29:49.711494+00:00"
    },
    {
      "arxiv_id": "2601.21475v1",
      "title": "Task-free Adaptive Meta Black-box Optimization",
      "title_zh": "免任务自适应元黑盒优化",
      "authors": [
        "Chao Wang",
        "Licheng Jiao",
        "Lingling Li",
        "Jiaxuan Zhao",
        "Guanchun Wang",
        "Fang Liu",
        "Shuyuan Yang"
      ],
      "abstract": "Handcrafted optimizers become prohibitively inefficient for complex black-box optimization (BBO) tasks. MetaBBO addresses this challenge by meta-learning to automatically configure optimizers for low-level BBO tasks, thereby eliminating heuristic dependencies. However, existing methods typically require extensive handcrafted training tasks to learn meta-strategies that generalize to target tasks, which poses a critical limitation for realistic applications with unknown task distributions. To overcome the issue, we propose the Adaptive meta Black-box Optimization Model (ABOM), which performs online parameter adaptation using solely optimization data from the target task, obviating the need for predefined task distributions. Unlike conventional metaBBO frameworks that decouple meta-training and optimization phases, ABOM introduces a closed-loop adaptive parameter learning mechanism, where parameterized evolutionary operators continuously self-update by leveraging generated populations during optimization. This paradigm shift enables zero-shot optimization: ABOM achieves competitive performance on synthetic BBO benchmarks and realistic unmanned aerial vehicle path planning problems without any handcrafted training tasks. Visualization studies reveal that parameterized evolutionary operators exhibit statistically significant search patterns, including natural selection and genetic recombination.",
      "tldr_zh": "该研究提出了 Adaptive meta Black-box Optimization Model (ABOM)，旨在解决现有 MetaBBO 方法因高度依赖预定义手工训练任务而难以应用于未知任务分布的局限性。ABOM 通过仅利用目标任务的优化数据进行在线参数自适应，消除了对先验任务分布的需求。与传统将元训练与优化阶段分离的框架不同，该模型引入了闭环自适应参数学习机制，使参数化的进化算子能够利用优化过程中生成的种群实现持续自我更新。这种范式转变成功实现了 Zero-shot 优化，在合成 Black-box Optimization (BBO) 基准测试和现实的无人机路径规划问题中均取得了极具竞争力的性能表现。可视化研究进一步证实，其参数化进化算子展现出了包括自然选择和遗传重组在内的统计显著搜索模式，为自动化优化器设计提供了新的路径。",
      "categories": [
        "cs.NE",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.NE",
      "comment": "This article was published as a conference paper at ICLR 2026",
      "pdf_url": "https://arxiv.org/pdf/2601.21475v1",
      "published_date": "2026-01-29 09:54:10 UTC",
      "updated_date": "2026-01-29 09:54:10 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-30T04:29:49.183131+00:00"
    },
    {
      "arxiv_id": "2601.21473v1",
      "title": "ScaleSim: Serving Large-Scale Multi-Agent Simulation with Invocation Distance-Based Memory Management",
      "title_zh": "ScaleSim：基于调用距离内存管理的大规模多智能体模拟服务系统",
      "authors": [
        "Zaifeng Pan",
        "Yipeng Shen",
        "Zhengding Hu",
        "Zhuang Wang",
        "Aninda Manocha",
        "Zheng Wang",
        "Zhongkai Yu",
        "Yue Guan",
        "Yufei Ding"
      ],
      "abstract": "LLM-based multi-agent simulations are increasingly adopted across application domains, but remain difficult to scale due to GPU memory pressure. Each agent maintains private GPU-resident states, including models, prefix caches, and adapters, which quickly exhaust device memory as the agent count grows. We identify two key properties of these workloads: sparse agent activation and an estimable agent invocation order. Based on an analysis of representative workload classes, we introduce invocation distance, a unified abstraction that estimates the relative order in which agents will issue future LLM requests. Leveraging this abstraction, we present ScaleSim, a memory-efficient LLM serving system for large-scale multi-agent simulations. ScaleSim enables proactive prefetching and priority-based eviction, supports diverse agent-specific memory through a modular interface, and achieves up to 1.74x speedup over SGLang on simulation benchmarks.",
      "tldr_zh": "该研究针对基于 Large Language Model (LLM) 的多智能体模拟在扩展时面临的 GPU 内存压力问题，提出了 ScaleSim 系统。由于每个智能体都需要维护私有的 GPU 驻留状态，包括 models、prefix caches 和 adapters，智能体数量的增长会迅速耗尽设备内存。通过分析代表性工作负载，研究者发现了智能体激活的稀疏性以及调用顺序的可估计性，并据此引入了 invocation distance 这一统一抽象概念。该抽象能够有效估计智能体发出未来 LLM 请求的相对顺序。ScaleSim 利用这一机制实现了主动预取 (proactive prefetching) 和基于优先级的逐出 (priority-based eviction)，并通过模块化接口支持多样化的智能体特定内存管理。实验表明，ScaleSim 在模拟基准测试中相比 SGLang 实现了高达 1.74 倍的加速，为大规模多智能体模拟提供了高效的服务支持。",
      "categories": [
        "cs.AI",
        "cs.DC"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2601.21473v1",
      "published_date": "2026-01-29 09:52:16 UTC",
      "updated_date": "2026-01-29 09:52:16 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-30T04:29:52.428269+00:00"
    },
    {
      "arxiv_id": "2601.21469v1",
      "title": "Adaptive Confidence Gating in Multi-Agent Collaboration for Efficient and Optimized Code Generation",
      "title_zh": "多智能体协作中用于高效、优化代码生成的自适应置信度门控",
      "authors": [
        "Haoji Zhang",
        "Yuzhe Li",
        "Zhenqiang Liu",
        "Chenyang Liu",
        "Shenyang Zhang",
        "Yi Zhou"
      ],
      "abstract": "While Large Language Models (LLMs) have catalyzed breakthroughs in automated code generation, Small Language Models (SLMs) often encounter reasoning bottlenecks and failure loops when addressing complex logical requirements. To overcome these challenges, we propose DebateCoder, a multi-agent collaborative framework designed to improve the reasoning ability of SLMs (e.g., Pangu-1B) in resource-constrained environments. DebateCoder uses a structured role-playing protocol with three agents: User Agent (A_UA), Technical Agent (A_TA), and Quality Assurance Agent (A_QA). It also includes an Adaptive Confidence Gating mechanism with a 95% threshold to balance accuracy and inference efficiency. In addition, we introduce a multi-turn deliberation module and a reviewer-guided analytical debugging loop for orthogonal pre-generation debate and post-generation refinement. Experiments on HumanEval and MBPP show that DebateCoder achieves 70.12% Pass@1 on HumanEval, outperforming MapCoder while reducing API overhead by about 35%. These results indicate that collaborative protocols can mitigate limitations of small-parameter models and provide a scalable, efficient approach to high-quality automated software engineering.",
      "tldr_zh": "该研究针对小语言模型(SLMs)在处理复杂代码生成任务时面临的推理瓶颈和逻辑失效问题，提出了名为DebateCoder的多智能体协作框架。该框架通过结构化的角色扮演协议，集成了用户智能体(User Agent)、技术智能体(Technical Agent)以及质量保证智能体(Quality Assurance Agent)进行协同工作。为了平衡生成准确性与推理效率，研究引入了具有95%阈值的自适应置信度门控(Adaptive Confidence Gating)机制，并结合多轮审议模块与审阅者引导的分析调试循环实现代码优化。实验结果显示，DebateCoder在HumanEval数据集上达到了70.12%的Pass@1准确率，在超越MapCoder性能的同时降低了约35%的API开销。该成果证明了协作协议能有效缓解小参数模型的局限性，为实现高效、可扩展的自动化软件工程提供了有力支持。",
      "categories": [
        "cs.SE",
        "cs.AI"
      ],
      "primary_category": "cs.SE",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2601.21469v1",
      "published_date": "2026-01-29 09:48:15 UTC",
      "updated_date": "2026-01-29 09:48:15 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-30T04:29:54.353248+00:00"
    },
    {
      "arxiv_id": "2601.21468v1",
      "title": "MemOCR: Layout-Aware Visual Memory for Efficient Long-Horizon Reasoning",
      "title_zh": "MemOCR：用于高效长程推理的布局感知视觉记忆",
      "authors": [
        "Yaorui Shi",
        "Shugui Liu",
        "Yu Yang",
        "Wenyu Mao",
        "Yuxin Chen",
        "Qi GU",
        "Hui Su",
        "Xunliang Cai",
        "Xiang Wang",
        "An Zhang"
      ],
      "abstract": "Long-horizon agentic reasoning necessitates effectively compressing growing interaction histories into a limited context window. Most existing memory systems serialize history as text, where token-level cost is uniform and scales linearly with length, often spending scarce budget on low-value details. To this end, we introduce MemOCR, a multimodal memory agent that improves long-horizon reasoning under tight context budgets by allocating memory space with adaptive information density through visual layout. Concretely, MemOCR maintains a structured rich-text memory (e.g., headings, highlights) and renders it into an image that the agent consults for memory access, visually prioritizing crucial evidence while aggressively compressing auxiliary details. To ensure robustness across varying memory budgets, we train MemOCR with reinforcement learning under budget-aware objectives that expose the agent to diverse compression levels. Across long-context multi-hop and single-hop question-answering benchmarks, MemOCR outperforms strong text-based baselines and achieves more effective context utilization under extreme budgets.",
      "tldr_zh": "该研究提出了 MemOCR，一种具备布局感知(Layout-Aware)能力的视觉记忆多模态智能体，旨在解决长程推理(Long-Horizon Reasoning)中因交互历史增长而导致的有限上下文窗口压力。不同于传统的将历史序列化为文本的记忆系统，MemOCR 通过视觉布局分配具有自适应信息密度的记忆空间，将包含标题、高亮等结构的富文本记忆渲染为图像供智能体访问。这种方式能够利用视觉手段优先处理关键证据，同时大幅度压缩辅助性细节，从而显著优化内存预算的分配。为了确保在不同记忆预算下的鲁棒性，研究团队采用强化学习(Reinforcement Learning)配合预算感知目标对 MemOCR 进行训练，使其能够适应各种压缩级别。在长上下文的多跳和单跳问答基准测试中，MemOCR 的表现优于强力的文本基线模型，并在极端预算条件下展现出更高效的上下文利用能力。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2601.21468v1",
      "published_date": "2026-01-29 09:47:17 UTC",
      "updated_date": "2026-01-29 09:47:17 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-30T04:30:05.336201+00:00"
    },
    {
      "arxiv_id": "2601.21465v1",
      "title": "Topeax -- An Improved Clustering Topic Model with Density Peak Detection and Lexical-Semantic Term Importance",
      "title_zh": "Topeax：融合密度峰值检测与词法-语义词项重要性的改进聚类主题模型",
      "authors": [
        "Márton Kardos"
      ],
      "abstract": "Text clustering is today the most popular paradigm for topic modelling, both in academia and industry. Despite clustering topic models' apparent success, we identify a number of issues in Top2Vec and BERTopic, which remain largely unsolved. Firstly, these approaches are unreliable at discovering natural clusters in corpora, due to extreme sensitivity to sample size and hyperparameters, the default values of which result in suboptimal behaviour. Secondly, when estimating term importance, BERTopic ignores the semantic distance of keywords to topic vectors, while Top2Vec ignores word counts in the corpus. This results in, on the one hand, less coherent topics due to the presence of stop words and junk words, and lack of variety and trust on the other. In this paper, I introduce a new approach, \\textbf{Topeax}, which discovers the number of clusters from peaks in density estimates, and combines lexical and semantic indices of term importance to gain high-quality topic keywords. Topeax is demonstrated to be better at both cluster recovery and cluster description than Top2Vec and BERTopic, while also exhibiting less erratic behaviour in response to changing sample size and hyperparameters.",
      "tldr_zh": "该研究针对Top2Vec和BERTopic在发现语料库自然聚类时表现不稳定以及对超参数和样本量过度敏感的问题，提出了改进的聚类主题模型Topeax。Topeax通过密度峰值检测(Density Peak Detection)自动识别聚类数量，并巧妙结合了词汇(lexical)与语义(semantic)维度的词项重要性指标，从而提取更高质量的主题关键词。针对现有方法在评估词项重要性时忽视语义距离或词频的缺陷，Topeax有效解决了主题一致性较差和缺乏多样性的问题。实验结果表明，Topeax在聚类恢复(cluster recovery)和聚类描述(cluster description)能力上均显著优于Top2Vec和BERTopic。此外，Topeax在应对不同样本规模和参数变化时表现出更强的鲁棒性，为文本聚类和主题建模提供了一个更可靠、更具描述性的技术方案。",
      "categories": [
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.AI",
      "comment": "14 pages, 6 figures",
      "pdf_url": "https://arxiv.org/pdf/2601.21465v1",
      "published_date": "2026-01-29 09:41:31 UTC",
      "updated_date": "2026-01-29 09:41:31 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-30T04:30:07.102447+00:00"
    },
    {
      "arxiv_id": "2601.21464v1",
      "title": "Conversation for Non-verifiable Learning: Self-Evolving LLMs through Meta-Evaluation",
      "title_zh": "面向不可验证学习的对话：通过元评估实现大语言模型的自我进化",
      "authors": [
        "Yuan Sui",
        "Bryan Hooi"
      ],
      "abstract": "Training large language models (LLMs) for non-verifiable tasks, such as creative writing, dialogue, and ethical reasoning, remains challenging due to the absence of ground-truth labels. While LLM-as-Judge approaches offer a scalable alternative to human feedback, they face a fundamental limitation: performance is constrained by the evaluator's own quality. If the judge cannot recognize good solutions, it cannot provide useful training signals, and evaluation biases (e.g., favoring verbosity over quality) remain unaddressed. This motivates meta-evaluation: the ability to evaluate and improve the evaluator itself. We introduce CoNL, a framework that unifies generation, evaluation, and meta-evaluation through multi-agent self-play. Our key insight: critique quality can be measured by whether it helps others improve their solutions. In CoNL, multiple agents sharing the same policy engage in structured conversations to propose, critique, and revise solutions. Critiques that enable solution improvements earn a diagnostic reward, creating explicit supervision for meta-evaluation and enabling joint optimization of generation and judging capabilities through self-play, without external judges or ground truth. Experiments on five benchmarks show that CoNL achieves consistent improvements over self-rewarding baselines while maintaining stable training.",
      "tldr_zh": "该研究提出了CoNL（Conversation for Non-verifiable Learning）框架，旨在解决大语言模型（LLMs）在处理创意写作、对话和伦理推理等不可验证任务（non-verifiable tasks）时由于缺乏基准真相（ground truth）而面临的训练挑战。该框架通过多智能体自我对弈（multi-agent self-play）将生成、评估和元评估（meta-evaluation）有机统一，克服了传统LLM-as-Judge方法受限于评估者自身水平的瓶颈。CoNL的核心洞察在于通过批评能否帮助他人改进方案来衡量其质量，并利用结构化对话实现方案的提议、批评与修订。这种机制通过诊断性奖励（diagnostic reward）为元评估提供显式监督，使模型能够在没有外部反馈的情况下同步优化生成和评价能力。在五个基准测试上的实验证明，CoNL在保持训练稳定性的同时，性能持续优于自我奖励（self-rewarding）基准模型，有效实现了LLMs的自我进化。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "Work in Progress",
      "pdf_url": "https://arxiv.org/pdf/2601.21464v1",
      "published_date": "2026-01-29 09:41:14 UTC",
      "updated_date": "2026-01-29 09:41:14 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-30T04:30:19.434248+00:00"
    },
    {
      "arxiv_id": "2601.21463v1",
      "title": "Unifying Speech Editing Detection and Content Localization via Prior-Enhanced Audio LLMs",
      "title_zh": "基于先验增强语音大语言模型的语音编辑检测与内容定位统一框架",
      "authors": [
        "Jun Xue",
        "Yi Chai",
        "Yanzhen Ren",
        "Jinshen He",
        "Zhiqiang Tang",
        "Zhuolin Yi",
        "Yihuan Huang",
        "Yuankun Xie",
        "Yujie Chen"
      ],
      "abstract": "Speech editing achieves semantic inversion by performing fine-grained segment-level manipulation on original utterances, while preserving global perceptual naturalness. Existing detection studies mainly focus on manually edited speech with explicit splicing artifacts, and therefore struggle to cope with emerging end-to-end neural speech editing techniques that generate seamless acoustic transitions. To address this challenge, we first construct a large-scale bilingual dataset, AiEdit, which leverages large language models to drive precise semantic tampering logic and employs multiple advanced neural speech editing methods for data synthesis, thereby filling the gap of high-quality speech editing datasets. Building upon this foundation, we propose PELM (Prior-Enhanced Audio Large Language Model), the first large-model framework that unifies speech editing detection and content localization by formulating them as an audio question answering task. To mitigate the inherent forgery bias and semantic-priority bias observed in existing audio large models, PELM incorporates word-level probability priors to provide explicit acoustic cues, and further designs a centroid-aggregation-based acoustic consistency perception loss to explicitly enforce the modeling of subtle local distribution anomalies. Extensive experimental results demonstrate that PELM significantly outperforms state-of-the-art methods on both the HumanEdit and AiEdit datasets, achieving equal error rates (EER) of 0.57\\% and 9.28\\% (localization), respectively.",
      "tldr_zh": "该研究针对神经语音编辑技术生成的无缝声学过渡导致传统检测方法失效的问题，提出了首个统一语音编辑检测 (speech editing detection) 与内容定位 (content localization) 的大模型框架 PELM (Prior-Enhanced Audio Large Language Model)。为了解决高质量数据集匮乏的问题，作者首先构建了大规模双语数据集 AiEdit，利用大型语言模型驱动精确的语义篡改逻辑并结合多种先进神经合成技术。PELM 将检测任务转化为音频问答形式，通过引入词级概率先验 (word-level probability priors) 提供显式声学线索，从而有效缓解了现有模型中的伪造偏置和语义优先偏置。此外，研究还设计了基于质心聚合的声学一致性感知损失 (centroid-aggregation-based acoustic consistency perception loss)，用于强化对局部微小分布异常的建模能力。实验结果表明，PELM 在 HumanEdit 和 AiEdit 数据集上的表现均显著优于现有最先进方法，分别取得了 0.57% 和 9.28% 的等错误率 (EER)，为应对高质量语音篡改挑战提供了新的解决思路。",
      "categories": [
        "cs.SD",
        "cs.AI"
      ],
      "primary_category": "cs.SD",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2601.21463v1",
      "published_date": "2026-01-29 09:39:28 UTC",
      "updated_date": "2026-01-29 09:39:28 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-30T04:30:19.110040+00:00"
    },
    {
      "arxiv_id": "2601.21461v1",
      "title": "L$^3$: Large Lookup Layers",
      "title_zh": "L$^3$：大规模查找层",
      "authors": [
        "Albert Tseng",
        "Christopher De Sa"
      ],
      "abstract": "Modern sparse language models typically achieve sparsity through Mixture-of-Experts (MoE) layers, which dynamically route tokens to dense MLP \"experts.\" However, dynamic hard routing has a number of drawbacks, such as potentially poor hardware efficiency and needing auxiliary losses for stable training. In contrast, the tokenizer embedding table, which is natively sparse, largely avoids these issues by selecting a single embedding per token at the cost of not having contextual information. In this work, we introduce the Large Lookup Layer (L$^3$), which unlocks a new axis of sparsity by generalizing embedding tables to model decoder layers. L$^3$ layers use static token-based routing to aggregate a set of learned embeddings per token in a context-dependent way, allowing the model to efficiently balance memory and compute by caching information in embeddings. L$^3$ has two main components: (1) a systems-friendly architecture that allows for fast training and CPU-offloaded inference with no overhead, and (2) an information-theoretic embedding allocation algorithm that effectively balances speed and quality. We empirically test L$^3$ by training transformers with up to 2.6B active parameters and find that L$^3$ strongly outperforms both dense models and iso-sparse MoEs in both language modeling and downstream tasks.",
      "tldr_zh": "该研究提出了Large Lookup Layer (L$^3$)，旨在解决专家混合(Mixture-of-Experts, MoE)模型中动态路由导致的硬件效率低下及训练不稳定等问题。L$^3$通过将传统的嵌入表(embedding tables)概念推广至模型解码器层，开辟了语言模型稀疏化的新维度。该架构利用基于词元的静态路由(static token-based routing)以上下文相关的方式聚合学习到的嵌入，通过在嵌入中缓存信息来有效平衡内存与计算开销。L$^3$包含一个支持快速训练和无损CPU卸载推理(CPU-offloaded inference)的系统友好型架构，以及一种优化速度与质量平衡的信息论嵌入分配算法。实验结果表明，在激活参数量高达2.6B的规模下，L$^3$在语言建模及各类下游任务中的表现均显著优于稠密模型和同等稀疏度的MoE模型。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "Preprint",
      "pdf_url": "https://arxiv.org/pdf/2601.21461v1",
      "published_date": "2026-01-29 09:37:31 UTC",
      "updated_date": "2026-01-29 09:37:31 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-30T04:30:34.941864+00:00"
    },
    {
      "arxiv_id": "2601.21459v1",
      "title": "HER: Human-like Reasoning and Reinforcement Learning for LLM Role-playing",
      "title_zh": "HER：面向大语言模型角色扮演的类人推理与强化学习",
      "authors": [
        "Chengyu Du",
        "Xintao Wang",
        "Aili Chen",
        "Weiyuan Li",
        "Rui Xu",
        "Junteng Liu",
        "Zishan Huang",
        "Rong Tian",
        "Zijun Sun",
        "Yuhao Li",
        "Liheng Feng",
        "Deming Ding",
        "Pengyu Zhao",
        "Yanghua Xiao"
      ],
      "abstract": "LLM role-playing, i.e., using LLMs to simulate specific personas, has emerged as a key capability in various applications, such as companionship, content creation, and digital games. While current models effectively capture character tones and knowledge, simulating the inner thoughts behind their behaviors remains a challenge. Towards cognitive simulation in LLM role-play, previous efforts mainly suffer from two deficiencies: data with high-quality reasoning traces, and reliable reward signals aligned with human preferences. In this paper, we propose HER, a unified framework for cognitive-level persona simulation. HER introduces dual-layer thinking, which distinguishes characters' first-person thinking from LLMs' third-person thinking. To bridge these gaps, we curate reasoning-augmented role-playing data via reverse engineering and construct human-aligned principles and reward models. Leveraging these resources, we train \\method models based on Qwen3-32B via supervised and reinforcement learning. Extensive experiments validate the effectiveness of our approach. Notably, our models significantly outperform the Qwen3-32B baseline, achieving a 30.26 improvement on the CoSER benchmark and a 14.97 gain on the Minimax Role-Play Bench. Our datasets, principles, and models will be released to facilitate future research.",
      "tldr_zh": "该研究提出了 HER 框架，旨在解决大语言模型（LLM）在角色扮演中难以模拟角色内心想法（inner thoughts）的挑战。该框架引入了双层思考（dual-layer thinking）机制，通过区分角色的第一人称思维与模型的第三人称思维，实现了认知层面的角色仿真。为了克服高质量推理数据和可靠奖励信号匮乏的问题，研究团队通过逆向工程（reverse engineering）构建了推理增强型数据集，并建立了符合人类偏好的评价原则与奖励模型。基于 Qwen3-32B 模型，该研究结合监督学习与强化学习（reinforcement learning）进行训练，显著提升了模型的表现。实验结果显示，HER 在 CoSER 基准测试中比基线模型提高了 30.26 分，在 Minimax Role-Play Bench 上获得了 14.97 分的增益，验证了其在提升角色扮演真实性方面的有效性。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "41pages, 10 figures",
      "pdf_url": "https://arxiv.org/pdf/2601.21459v1",
      "published_date": "2026-01-29 09:35:27 UTC",
      "updated_date": "2026-01-29 09:35:27 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-30T04:30:18.724890+00:00"
    },
    {
      "arxiv_id": "2601.21453v1",
      "title": "LION: A Clifford Neural Paradigm for Multimodal-Attributed Graph Learning",
      "title_zh": "",
      "authors": [
        "Xunkai Li",
        "Zhengyu Wu",
        "Zekai Chen",
        "Henan Sun",
        "Daohan Su",
        "Guang Zeng",
        "Hongchao Qin",
        "Rong-Hua Li",
        "Guoren Wang"
      ],
      "abstract": "Recently, the rapid advancement of multimodal domains has driven a data-centric paradigm shift in graph ML, transitioning from text-attributed to multimodal-attributed graphs. This advancement significantly enhances data representation and expands the scope of graph downstream tasks, such as modality-oriented tasks, thereby improving the practical utility of graph ML. Despite its promise, limitations exist in the current neural paradigms: (1) Neglect Context in Modality Alignment: Most existing methods adopt topology-constrained or modality-specific operators as tokenizers. These aligners inevitably neglect graph context and inhibit modality interaction, resulting in suboptimal alignment. (2) Lack of Adaptation in Modality Fusion: Most existing methods are simple adaptations for 2-modality graphs and fail to adequately exploit aligned tokens equipped with topology priors during fusion, leading to poor generalizability and performance degradation. To address the above issues, we propose LION (c\\underline{LI}ff\\underline{O}rd \\underline{N}eural paradigm) based on the Clifford algebra and decoupled graph neural paradigm (i.e., propagation-then-aggregation) to implement alignment-then-fusion in multimodal-attributed graphs. Specifically, we first construct a modality-aware geometric manifold grounded in Clifford algebra. This geometric-induced high-order graph propagation efficiently achieves modality interaction, facilitating modality alignment. Then, based on the geometric grade properties of aligned tokens, we propose adaptive holographic aggregation. This module integrates the energy and scale of geometric grades with learnable parameters to improve modality fusion. Extensive experiments on 9 datasets demonstrate that LION significantly outperforms SOTA baselines across 3 graph and 3 modality downstream tasks.",
      "tldr_zh": "",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2601.21453v1",
      "published_date": "2026-01-29 09:30:36 UTC",
      "updated_date": "2026-01-29 09:30:36 UTC",
      "processing_status": "failed",
      "attempts": 3,
      "max_attempts": 3,
      "error": "Your request was blocked.",
      "completed_steps": [],
      "llm_backup_calls": 0,
      "last_update": "2026-02-01T04:41:59.300464+00:00"
    },
    {
      "arxiv_id": "2601.21452v1",
      "title": "SAGE: Sequence-level Adaptive Gradient Evolution for Generative Recommendation",
      "title_zh": "SAGE：面向生成式推荐的序列级自适应梯度演化",
      "authors": [
        "Yu Xie",
        "Xing Kai Ren",
        "Ying Qi",
        "Hu Yao"
      ],
      "abstract": "While works such as OneRec have validated the scaling laws of Large Language Models (LLMs) in recommender systems, they rely on a cumbersome separate vocabulary. This dependency prevents the model architecture from reusing native LLM vocabularies, resulting in high maintenance costs and poor scalability. In response, we aim to efficiently reuse open-source LLM architectures without constructing a separate tokenization vocabulary. Furthermore, we identify that the optimization strategy of OneRec Gradient Bounded Policy Optimization (GBPO),suffers from a \"Symmetric Conservatism\" problem: its static gradient boundaries structurally suppress the update momentum required for cold-start items and fail to prevent diversity collapse in high-noise environments.To address this issue, we propose SAGE (Sequence-level Adaptive Gradient Evolution), a unified optimization framework tailored for list-wise generative recommendation. SAGE introduces two key innovations:(1) Sequence-level Signal Decoupling: By combining a geometric mean importance ratio with decoupled multi-objective advantages, we eliminate token-level variance and resolve the \"Reward Collapse\" problem. (2) Asymmetric Adaptive Dynamics: We construct a dynamic gradient manifold that applies a \"Boost Factor\" to high-potential cold start items to achieve super-linear updates and employs an \"Entropy Aware Penalty\" to break information cocoons. Theoretical analysis and empirical results demonstrate that SAGE effectively unblocks cold-start traffic and sustains recommendation diversity, all while retaining the numerical stability of GBPO.",
      "tldr_zh": "该研究提出了SAGE（Sequence-level Adaptive Gradient Evolution），这是一种专为列表式生成推荐设计的统一优化框架，旨在解决OneRec等模型依赖独立词表导致的扩展性差以及GBPO优化策略中Symmetric Conservatism导致的冷启动受阻问题。SAGE引入了两项核心技术创新：首先是通过Sequence-level Signal Decoupling消除Token-level方差，结合几何平均重要性比率与解耦多目标优势解决了Reward Collapse问题；其次是构建了Asymmetric Adaptive Dynamics，利用动态梯度流形中的Boost Factor为高潜力冷启动项提供超线性更新动力，并配合Entropy Aware Penalty打破推荐中的信息茧房。理论分析与实验结果表明，SAGE在保留GBPO数值稳定性的基础上，有效激活了冷启动流量并显著提升了推荐系统的多样性。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "arXiv admin note: text overlap with arXiv:2506.19235",
      "pdf_url": "https://arxiv.org/pdf/2601.21452v1",
      "published_date": "2026-01-29 09:30:13 UTC",
      "updated_date": "2026-01-29 09:30:13 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-30T04:30:32.016924+00:00"
    },
    {
      "arxiv_id": "2601.21448v1",
      "title": "ChipBench: A Next-Step Benchmark for Evaluating LLM Performance in AI-Aided Chip Design",
      "title_zh": "ChipBench：用于评估 AI 辅助芯片设计中大语言模型性能的进阶基准测试",
      "authors": [
        "Zhongkai Yu",
        "Chenyang Zhou",
        "Yichen Lin",
        "Hejia Zhang",
        "Haotian Ye",
        "Junxia Cui",
        "Zaifeng Pan",
        "Jishen Zhao",
        "Yufei Ding"
      ],
      "abstract": "While Large Language Models (LLMs) show significant potential in hardware engineering, current benchmarks suffer from saturation and limited task diversity, failing to reflect LLMs' performance in real industrial workflows. To address this gap, we propose a comprehensive benchmark for AI-aided chip design that rigorously evaluates LLMs across three critical tasks: Verilog generation, debugging, and reference model generation. Our benchmark features 44 realistic modules with complex hierarchical structures, 89 systematic debugging cases, and 132 reference model samples across Python, SystemC, and CXXRTL. Evaluation results reveal substantial performance gaps, with state-of-the-art Claude-4.5-opus achieving only 30.74\\% on Verilog generation and 13.33\\% on Python reference model generation, demonstrating significant challenges compared to existing saturated benchmarks where SOTA models achieve over 95\\% pass rates. Additionally, to help enhance LLM reference model generation, we provide an automated toolbox for high-quality training data generation, facilitating future research in this underexplored domain. Our code is available at https://github.com/zhongkaiyu/ChipBench.git.",
      "tldr_zh": "该研究提出了ChipBench，这是一个用于评估大语言模型(LLMs)在人工智能辅助芯片设计(AI-Aided Chip Design)中性能的新型基准测试，旨在解决现有基准测试任务单一且趋于饱和的问题。ChipBench通过Verilog生成(Verilog generation)、硬件调试(debugging)以及参考模型生成(reference model generation)三个核心维度，利用44个复杂层次结构模块和大量系统性案例对模型进行严苛测试。评估结果表明，当前最先进的模型在处理工业级芯片设计任务时仍存在显著的性能缺口，其在Verilog生成和参考模型生成方面的表现远不及在传统简单基准测试中的表现。此外，研究团队还同步推出了一个自动化工具箱，用于生成高质量的训练数据以支持该领域的研究。该基准测试的开源为探索LLM在复杂硬件工程领域的应用潜力提供了更具挑战性的评价标准。",
      "categories": [
        "cs.AI",
        "cs.AR"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2601.21448v1",
      "published_date": "2026-01-29 09:26:55 UTC",
      "updated_date": "2026-01-29 09:26:55 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-30T04:30:36.411097+00:00"
    },
    {
      "arxiv_id": "2601.21444v1",
      "title": "Spava: Accelerating Long-Video Understanding via Sequence-Parallelism-aware Approximate Attention",
      "title_zh": "Spava：通过序列并行感知的近似注意力加速长视频理解",
      "authors": [
        "Yuxiang Huang",
        "Mingye Li",
        "Xu Han",
        "Chaojun Xiao",
        "Weilin Zhao",
        "Ao Sun",
        "Ziqi Yuan",
        "Hao Zhou",
        "Fandong Meng",
        "Zhiyuan Liu"
      ],
      "abstract": "The efficiency of long-video inference remains a critical bottleneck, mainly due to the dense computation in the prefill stage of Large Multimodal Models (LMMs). Existing methods either compress visual embeddings or apply sparse attention on a single GPU, yielding limited acceleration or degraded performance and restricting LMMs from handling longer, more complex videos. To overcome these issues, we propose Spava, a sequence-parallel framework with optimized attention that accelerates long-video inference across multiple GPUs. By distributing approximate attention, Spava reduces computation and increases parallelism, enabling efficient processing of more visual embeddings without compression and thereby improving task performance. System-level optimizations, such as load balancing and fused forward passes, further unleash the potential of Spava, delivering speedups of 12.72x, 1.70x, and 1.18x over FlashAttn, ZigZagRing, and APB, without notable performance loss. Code available at https://github.com/thunlp/APB",
      "tldr_zh": "该研究针对大模型多模态模型(LMMs)在长视频理解中面临的预填充阶段(prefill stage)计算密集导致的效率瓶颈问题，提出了Spava框架。Spava是一种具备序列并行意识(Sequence-Parallelism-aware)的近似注意力框架，旨在通过多GPU协同加速长视频推理过程。该框架通过分发近似注意力(approximate attention)减少了计算开销并增强并行性，允许在不进行视觉嵌入(visual embeddings)压缩的情况下处理更长的视频，从而有效保留了模型性能。通过引入负载均衡(load balancing)和融合前向传递(fused forward passes)等系统级优化，Spava进一步挖掘了硬件的算力潜力。实验结果显示，Spava相较于FlashAttn、ZigZagRing和APB分别实现了12.72倍、1.70倍和1.18倍的加速，且未产生明显的性能损失。这一成果为大型多模态模型高效处理更长、更复杂的视频任务提供了可行的并行计算方案。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.CV",
      "comment": "Preprint",
      "pdf_url": "https://arxiv.org/pdf/2601.21444v1",
      "published_date": "2026-01-29 09:23:13 UTC",
      "updated_date": "2026-01-29 09:23:13 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-30T04:30:35.884453+00:00"
    },
    {
      "arxiv_id": "2601.21439v1",
      "title": "The Paradox of Robustness: Decoupling Rule-Based Logic from Affective Noise in High-Stakes Decision-Making",
      "title_zh": "鲁棒性悖论：高风险决策中规则逻辑与情感噪声的解耦",
      "authors": [
        "Jon Chun",
        "Katherine Elkins"
      ],
      "abstract": "While Large Language Models (LLMs) are widely documented to be sensitive to minor prompt perturbations and prone to sycophantic alignment with user biases, their robustness in consequential, rule-bound decision-making remains under-explored. In this work, we uncover a striking \"Paradox of Robustness\": despite their known lexical brittleness, instruction-tuned LLMs exhibit a behavioral and near-total invariance to emotional framing effects. Using a novel controlled perturbation framework across three high-stakes domains (healthcare, law, and finance), we quantify a robustness gap where LLMs demonstrate 110-300 times greater resistance to narrative manipulation than human subjects. Specifically, we find a near-zero effect size for models (Cohen's h = 0.003) compared to the substantial biases observed in humans (Cohen's h in [0.3, 0.8]). This result is highly counterintuitive and suggests the mechanisms driving sycophancy and prompt sensitivity do not necessarily translate to a failure in logical constraint satisfaction. We show that this invariance persists across models with diverse training paradigms. Our findings show that while LLMs may be \"brittle\" to how a query is formatted, they are remarkably \"stable\" against why a decision should be biased. Our findings establish that instruction-tuned models can decouple logical rule-adherence from persuasive narratives, offering a source of decision stability that complements, and even potentially de-biases, human judgment in institutional contexts. We release the 162-scenario benchmark, code, and data to facilitate the rigorous evaluation of narrative-induced bias and robustness on GitHub.com.",
      "tldr_zh": "该研究探讨了大语言模型 (LLMs) 在医疗、法律和金融等高风险决策领域中的鲁棒性，并揭示了所谓的“鲁棒性悖论” (Paradox of Robustness)。通过使用新型的受控扰动框架 (controlled perturbation framework)，研究发现指令微调后的模型在面对情感框架效应 (emotional framing effects) 时展现出近乎完全的不变性。实验结果显示，LLMs 对叙事操纵 (narrative manipulation) 的抵抗力比人类受试者高出 110-300 倍，其效应量 (Effect size) 指标 Cohen's h 仅为 0.003，远低于人类表现出的显著偏差。这表明驱动模型产生阿谀奉承 (sycophancy) 和提示敏感性的机制并不等同于逻辑规则遵循的失效。该研究证明了 LLMs 能够将基于规则的逻辑与说服性叙事有效解耦 (decouple)，从而在机构决策中提供比人类判断更稳定的支持，并具备消除偏见的潜力。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "22 page, 10 figures",
      "pdf_url": "https://arxiv.org/pdf/2601.21439v1",
      "published_date": "2026-01-29 09:17:05 UTC",
      "updated_date": "2026-01-29 09:17:05 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-30T04:31:01.886243+00:00"
    },
    {
      "arxiv_id": "2601.21436v1",
      "title": "From Consistency to Complementarity: Aligned and Disentangled Multi-modal Learning for Time Series Understanding and Reasoning",
      "title_zh": "从一致性到互补性：面向时间序列理解与推理的对齐与解耦多模态学习",
      "authors": [
        "Hang Ni",
        "Weijia Zhang",
        "Fei Wang",
        "Zezhi Shao",
        "Hao Liu"
      ],
      "abstract": "Advances in multi-modal large language models (MLLMs) have inspired time series understanding and reasoning tasks, that enable natural language querying over time series, producing textual analyses of complex temporal dynamics. Recent attempts hybridize numerical time series with their visualized plots, facilitating precise value reasoning and visual structure comprehension for comprehensive time series understanding of MLLMs. However, effective cross-modal integration remains challenging due to fine-grained temporal misalignment across modalities and severe entanglement between shared and modality-specific semantics, which hinder localized interpretation and complementary reasoning. To address these issues, we propose MADI, a multi-modal LLM enhanced with fine-grained alignment and disentangled interaction, featuring (1) Patch-level Alignment, which enforces physically grounded fine-grained correspondence across heterogeneous modalities, (2) Discrete Disentangled Interaction, which separates modality-common semantics into compact discrete latents and adaptively synergizes the purified modality-unique information, and (3) Critical-token Highlighting, which emphasizes informative, query-relevant signals for robust reasoning. Experiments on synthetic and real-world benchmarks show that MADI consistently outperforms general-purpose LLMs and time-series-specialized MLLMs.",
      "tldr_zh": "该研究针对多模态大语言模型（MLLMs）在时间序列理解与推理任务中面临的跨模态细粒度对齐不当及语义纠缠挑战，提出了增强型框架MADI。该框架核心包含Patch-level Alignment技术，旨在建立异构模态间物理层面的细粒度对应关系。同时，MADI利用Discrete Disentangled Interaction将模态共性语义分离为紧凑的离散潜在变量，并自适应地协同模态特有的语义信息。此外，通过Critical-token Highlighting机制强调与查询相关的关键信号，进一步提升了推理的鲁棒性。实验结果表明，MADI在合成及真实世界基准测试中均表现出色，一致优于通用LLMs及专门的时间序列MLLMs。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL",
        "cs.CV"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2601.21436v1",
      "published_date": "2026-01-29 09:13:46 UTC",
      "updated_date": "2026-01-29 09:13:46 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-30T04:31:06.033407+00:00"
    },
    {
      "arxiv_id": "2601.21433v1",
      "title": "When Prohibitions Become Permissions: Auditing Negation Sensitivity in Language Models",
      "title_zh": "当禁令变为许可：语言模型中的否定敏感性审计",
      "authors": [
        "Katherine Elkins",
        "Jon Chun"
      ],
      "abstract": "When a user tells an AI system that someone \"should not\" take an action, the system ought to treat this as a prohibition. Yet many large language models do the opposite: they interpret negated instructions as affirmations. We audited 16 models across 14 ethical scenarios and found that open-source models endorse prohibited actions 77% of the time under simple negation and 100% under compound negation -- a 317% increase over affirmative framing. Commercial models fare better but still show swings of 19-128%. Agreement between models drops from 74% on affirmative prompts to 62% on negated ones, and financial scenarios prove twice as fragile as medical ones. These patterns hold under deterministic decoding, ruling out sampling noise. We present case studies showing how these failures play out in practice, propose the Negation Sensitivity Index (NSI) as a governance metric, and outline a tiered certification framework with domain-specific thresholds. The findings point to a gap between what current alignment techniques achieve and what safe deployment requires: models that cannot reliably distinguish \"do X\" from \"do not X\" should not be making autonomous decisions in high-stakes contexts.",
      "tldr_zh": "该研究对16个语言模型在14个伦理场景下的否定敏感度(Negation Sensitivity)进行了审计，揭示了模型常将禁止性指令误解为肯定指令的严峻问题。实验发现，开源模型在简单否定指令下对禁止行为的支持率达77%，在复合否定下甚至高达100%，而商业模型虽表现略好但也存在显著波动。研究指出，模型间的一致性在否定提示下明显下降，且金融场景比医疗场景更易出错，这些模式在确定性解码(Deterministic Decoding)下依然成立。为此，研究提出了否定敏感度指数(Negation Sensitivity Index, NSI)作为衡量模型治理的指标，并制定了包含领域特定阈值的分层认证框架(Tiered Certification Framework)。该发现强调了当前对齐技术与安全部署要求之间的差距，认为无法可靠区分否定指令的模型不应在涉及高风险决策的场景中被授予自主权。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "13 pages, 5 figures",
      "pdf_url": "https://arxiv.org/pdf/2601.21433v1",
      "published_date": "2026-01-29 09:09:23 UTC",
      "updated_date": "2026-01-29 09:09:23 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-30T04:31:10.345870+00:00"
    },
    {
      "arxiv_id": "2601.21418v1",
      "title": "Mitigating Overthinking in Large Reasoning Models via Difficulty-aware Reinforcement Learning",
      "title_zh": "通过难度感知强化学习缓解大推理模型中的过度思考",
      "authors": [
        "Qian Wan",
        "Ziao Xu",
        "Luona Wei",
        "Xiaoxuan Shen",
        "Jianwen Sun"
      ],
      "abstract": "Large Reasoning Models (LRMs) achieve explicit chain-of-thought expansion by imitating deep thinking behaviors of humans, demonstrating excellent performance in complex task scenarios. However, the deep-thinking mode often leads to unnecessarily lengthy reasoning and resource inefficiency when handling simple tasks. This overthinking phenomenon may arise from the generation preference triggered by the reward function during post-training. Existing research attempts to mitigate overthinking from the perspective of prompt design or model training, but generally underestimates the importance of task difficulty awareness, which makes it difficult for LRMs to effectively allocate reasoning resources. In this paper, we propose Difficulty-aware Policy Optimization (DiPO), a reinforcement learning-based LRM training framework. DiPO encourages LRM to spontaneously model task complexity, and integrates them into reinforcement learning framework to adjust the generation preferences introduced by post-training. A difficulty modeling method based on model self-reasoning is proposed, which significantly reduces the dependence on manual annotation and formalize task complexity. We further develop a difficulty-signal-enhanced reward function that incorporates a penalty for lengthy reasoning while considering reasoning performance and output format. Experimental results indicate that DiPO enables the model to spontaneously adjust inference overhead, significantly reducing redundant tokens without losing performance due to thought compression.",
      "tldr_zh": "该研究针对大型推理模型(Large Reasoning Models)在简单任务中因过度思考(Overthinking)导致的推理冗长与效率低下问题，提出了一种名为DiPO(Difficulty-aware Policy Optimization)的强化学习训练框架。DiPO通过模型自推理(self-reasoning)自发建模任务复杂度，有效降低了对人工标注的依赖并实现了任务难度的形式化表达。该框架将难度感知整合进强化学习中，利用增强难度信号的奖励函数对冗长推理进行惩罚，从而调整模型在后期训练中形成的生成偏好。实验结果证明，DiPO能够使模型根据任务难度自发调节推理开销，在不牺牲性能的情况下显著减少冗余Token，实现了推理资源的高效分配。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2601.21418v1",
      "published_date": "2026-01-29 08:56:45 UTC",
      "updated_date": "2026-01-29 08:56:45 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-30T04:31:12.747108+00:00"
    },
    {
      "arxiv_id": "2601.21414v1",
      "title": "System 1&2 Synergy via Dynamic Model Interpolation",
      "title_zh": "通过动态模型插值实现系统 1 与系统 2 的协同",
      "authors": [
        "Chenxu Yang",
        "Qingyi Si",
        "Chong Tian",
        "Xiyu Liu",
        "Dingyu Yao",
        "Chuanyu Qin",
        "Zheng Lin",
        "Weiping Wang",
        "Jiaqi Wang"
      ],
      "abstract": "Training a unified language model that adapts between intuitive System 1 and deliberative System 2 remains challenging due to interference between their cognitive modes. Recent studies have thus pursued making System 2 models more efficient. However, these approaches focused on output control, limiting what models produce. We argue that this paradigm is misaligned: output length is merely a symptom of the model's cognitive configuration, not the root cause. In this work, we shift the focus to capability control, which modulates \\textit{how models think} rather than \\textit{what they produce}. To realize this, we leverage existing Instruct and Thinking checkpoints through dynamic parameter interpolation, without additional training. Our pilot study establishes that linear interpolation yields a convex, monotonic Pareto frontier, underpinned by representation continuity and structural connectivity. Building on this, we propose \\textbf{DAMI} (\\textbf{D}yn\\textbf{A}mic \\textbf{M}odel \\textbf{I}nterpolation), a framework that estimates a query-specific Reasoning Intensity $λ(q)$ to configure cognitive depth. For training-based estimation, we develop a preference learning method encoding accuracy and efficiency criteria. For zero-shot deployment, we introduce a confidence-based method leveraging inter-model cognitive discrepancy. Experiments on five mathematical reasoning benchmarks demonstrate that DAMI achieves higher accuracy than the Thinking model while remaining efficient, effectively combining the efficiency of System 1 with the reasoning depth of System 2.",
      "tldr_zh": "该研究针对统一语言模型在直觉式 System 1 和审慎式 System 2 认知模式切换中存在的干扰问题，提出了一种从输出控制转向“能力控制”的新范式。作者利用现有的 Instruct 和 Thinking 检查点，在不增加额外训练的前提下，通过动态参数插值来调节模型的认知深度。研究发现线性插值能基于表征连续性形成凸单调的帕累托前沿（Pareto frontier），据此提出的 DAMI（Dynamic Model Interpolation）框架可根据具体查询估算推理强度（Reasoning Intensity） $λ(q)$。该框架支持基于偏好学习的训练方法和基于认知差异的零样本部署方案。在五个数学推理基准上的实验证明，DAMI 在保持高效的同时，准确率优于单一的 Thinking 模型，成功实现了 System 1 的效率与 System 2 深度的协同。",
      "categories": [
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2601.21414v1",
      "published_date": "2026-01-29 08:53:16 UTC",
      "updated_date": "2026-01-29 08:53:16 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-30T04:31:15.673850+00:00"
    },
    {
      "arxiv_id": "2601.21403v1",
      "title": "DataCross: A Unified Benchmark and Agent Framework for Cross-Modal Heterogeneous Data Analysis",
      "title_zh": "DataCross：跨模态异构数据分析的统一基准与智能体框架",
      "authors": [
        "Ruyi Qi",
        "Zhou Liu",
        "Wentao Zhang"
      ],
      "abstract": "In real-world data science and enterprise decision-making, critical information is often fragmented across directly queryable structured sources (e.g., SQL, CSV) and \"zombie data\" locked in unstructured visual documents (e.g., scanned reports, invoice images). Existing data analytics agents are predominantly limited to processing structured data, failing to activate and correlate this high-value visual information, thus creating a significant gap with industrial needs. To bridge this gap, we introduce DataCross, a novel benchmark and collaborative agent framework for unified, insight-driven analysis across heterogeneous data modalities. DataCrossBench comprises 200 end-to-end analysis tasks across finance, healthcare, and other domains. It is constructed via a human-in-the-loop reverse-synthesis pipeline, ensuring realistic complexity, cross-source dependency, and verifiable ground truth. The benchmark categorizes tasks into three difficulty tiers to evaluate agents' capabilities in visual table extraction, cross-modal alignment, and multi-step joint reasoning. We also propose the DataCrossAgent framework, inspired by the \"divide-and-conquer\" workflow of human analysts. It employs specialized sub-agents, each an expert on a specific data source, which are coordinated via a structured workflow of Intra-source Deep Exploration, Key Source Identification, and Contextual Cross-pollination. A novel reReAct mechanism enables robust code generation and debugging for factual verification. Experimental results show that DataCrossAgent achieves a 29.7% improvement in factuality over GPT-4o and exhibits superior robustness on high-difficulty tasks, effectively activating fragmented \"zombie data\" for insightful, cross-modal analysis.",
      "tldr_zh": "该研究针对数据科学和企业决策中结构化数据与锁定在非结构化视觉文档中的“zombie data”信息碎片化问题，提出了 DataCross 统一基准和智能体框架。研究首先构建了包含 200 个跨领域端到端分析任务的 DataCrossBench，通过 human-in-the-loop reverse-synthesis pipeline 确保了跨源依赖性和真实复杂性。配套提出的 DataCrossAgent 协作框架借鉴人类分析师的“分而治之”策略，通过专门的子智能体协同执行 Intra-source Deep Exploration、Key Source Identification 和 Contextual Cross-pollination。此外，框架引入了新型 reReAct 机制以增强代码生成与事实验证的稳健性。实验结果显示，DataCrossAgent 在事实性上比 GPT-4o 提升了 29.7%，并在高难度任务中展现出卓越的鲁棒性。该工作有效地激活并关联了高价值的视觉信息，为实现统一的、洞察驱动的跨模态异构数据分析提供了新的路径。",
      "categories": [
        "cs.AI",
        "cs.MA"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2601.21403v1",
      "published_date": "2026-01-29 08:40:45 UTC",
      "updated_date": "2026-01-29 08:40:45 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-30T04:31:13.448486+00:00"
    },
    {
      "arxiv_id": "2601.21391v1",
      "title": "Intrinsic Reward Policy Optimization for Sparse-Reward Environments",
      "title_zh": "面向稀疏奖励环境的内在奖励策略优化",
      "authors": [
        "Minjae Cho",
        "Huy Trong Tran"
      ],
      "abstract": "Exploration is essential in reinforcement learning as an agent relies on trial and error to learn an optimal policy. However, when rewards are sparse, naive exploration strategies, like noise injection, are often insufficient. Intrinsic rewards can also provide principled guidance for exploration by, for example, combining them with extrinsic rewards to optimize a policy or using them to train subpolicies for hierarchical learning. However, the former approach suffers from unstable credit assignment, while the latter exhibits sample inefficiency and sub-optimality. We propose a policy optimization framework that leverages multiple intrinsic rewards to directly optimize a policy for an extrinsic reward without pretraining subpolicies. Our algorithm -- intrinsic reward policy optimization (IRPO) -- achieves this by using a surrogate policy gradient that provides a more informative learning signal than the true gradient in sparse-reward environments. We demonstrate that IRPO improves performance and sample efficiency relative to baselines in discrete and continuous environments, and formally analyze the optimization problem solved by IRPO. Our code is available at https://github.com/Mgineer117/IRPO.",
      "tldr_zh": "在强化学习中，稀疏奖励（Sparse-reward）环境下的有效探索是一项核心挑战，传统的随机噪声探索策略在此时往往难以奏效。虽然内在奖励（Intrinsic rewards）能为智能体提供探索引导，但现有方法常面临信用分配（Credit assignment）不稳定或分层学习采样效率低下的瓶颈。该研究提出了内在奖励策略优化（Intrinsic Reward Policy Optimization, IRPO）框架，旨在利用多种内在奖励直接优化针对外在奖励的策略，从而避免了预训练子策略的需求。IRPO 算法的核心在于引入了一种代理策略梯度（Surrogate policy gradient），它在稀疏奖励场景下能提供比真实梯度更具启发性的学习信号。实验结果证明，IRPO 在离散和连续任务中均能显著提升性能和采样效率，且该研究还对其背后的优化机制进行了严格的形式化分析。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2601.21391v1",
      "published_date": "2026-01-29 08:25:14 UTC",
      "updated_date": "2026-01-29 08:25:14 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-30T04:31:25.516867+00:00"
    },
    {
      "arxiv_id": "2601.21386v1",
      "title": "Understanding Frechet Speech Distance for Synthetic Speech Quality Evaluation",
      "title_zh": "深入理解用于合成语音质量评估的 Frechet 语音距离",
      "authors": [
        "June-Woo Kim",
        "Dhruv Agarwal",
        "Federica Cerina"
      ],
      "abstract": "Objective evaluation of synthetic speech quality remains a critical challenge. Human listening tests are the gold standard, but costly and impractical at scale. Fréchet Distance has emerged as a promising alternative, yet its reliability depends heavily on the choice of embeddings and experimental settings. In this work, we comprehensively evaluate Fréchet Speech Distance (FSD) and its variant Speech Maximum Mean Discrepancy (SMMD) under varied embeddings and conditions. We further incorporate human listening evaluations alongside TTS intelligibility and synthetic-trained ASR WER to validate the perceptual relevance of these metrics. Our findings show that WavLM Base+ features yield the most stable alignment with human ratings. While FSD and SMMD cannot fully replace subjective evaluation, we show that they can serve as complementary, cost-efficient, and reproducible measures, particularly useful when large-scale or direct listening assessments are infeasible. Code is available at https://github.com/kaen2891/FrechetSpeechDistance.",
      "tldr_zh": "该研究针对合成语音质量评估中的挑战，对 Fréchet Speech Distance (FSD) 及其变体 Speech Maximum Mean Discrepancy (SMMD) 在不同嵌入(embeddings)和实验设置下的表现进行了全面评估。通过结合人类听感测试、TTS 可理解性和 ASR WER 等指标，研究深入探讨了这些客观度量的感知相关性。实验结果显示，WavLM Base+ 特征在与人类评分的一致性方面表现最强且最为稳定。研究指出，虽然 FSD 和 SMMD 尚不能完全取代主观评价，但它们提供了一种低成本且可重复的补充评估方案，尤其在大规模或无法直接进行听力测试的场景中具有显著的应用价值。",
      "categories": [
        "cs.SD",
        "cs.AI"
      ],
      "primary_category": "cs.SD",
      "comment": "accepted to ICASSP 2026",
      "pdf_url": "https://arxiv.org/pdf/2601.21386v1",
      "published_date": "2026-01-29 08:20:52 UTC",
      "updated_date": "2026-01-29 08:20:52 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-30T04:31:22.133387+00:00"
    },
    {
      "arxiv_id": "2601.21384v1",
      "title": "Sim-MSTNet: sim2real based Multi-task SpatioTemporal Network Traffic Forecasting",
      "title_zh": "Sim-MSTNet：基于 sim2real 的多任务时空网络流量预测",
      "authors": [
        "Hui Ma",
        "Qingzhong Li",
        "Jin Wang",
        "Jie Wu",
        "Shaoyu Dou",
        "Li Feng",
        "Xinjun Pei"
      ],
      "abstract": "Network traffic forecasting plays a crucial role in intelligent network operations, but existing techniques often perform poorly when faced with limited data. Additionally, multi-task learning methods struggle with task imbalance and negative transfer, especially when modeling various service types. To overcome these challenges, we propose Sim-MSTNet, a multi-task spatiotemporal network traffic forecasting model based on the sim2real approach. Our method leverages a simulator to generate synthetic data, effectively addressing the issue of poor generalization caused by data scarcity. By employing a domain randomization technique, we reduce the distributional gap between synthetic and real data through bi-level optimization of both sample weighting and model training. Moreover, Sim-MSTNet incorporates attention-based mechanisms to selectively share knowledge between tasks and applies dynamic loss weighting to balance task objectives. Extensive experiments on two open-source datasets show that Sim-MSTNet consistently outperforms state-of-the-art baselines, achieving enhanced accuracy and generalization.",
      "tldr_zh": "该研究针对网络流量预测中有限数据表现不佳以及多任务学习中的任务不平衡和负迁移问题，提出了Sim-MSTNet框架。Sim-MSTNet采用基于sim2real的方法，利用模拟器生成合成数据以解决数据稀缺导致的泛化能力差的问题。通过领域随机化(domain randomization)技术和双层优化(bi-level optimization)策略，该模型有效缩小了合成数据与真实数据之间的分布差异。此外，Sim-MSTNet引入了基于注意力(attention-based)的机制进行任务间的选择性知识共享，并应用动态损失加权(dynamic loss weighting)来平衡多任务目标。实验结果表明，该模型在两个开源数据集上均优于现有的先进基准模型，显著提升了预测精度和泛化性能。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "accepted in ICASSP 2026",
      "pdf_url": "https://arxiv.org/pdf/2601.21384v1",
      "published_date": "2026-01-29 08:20:08 UTC",
      "updated_date": "2026-01-29 08:20:08 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-30T04:31:26.058118+00:00"
    },
    {
      "arxiv_id": "2601.21375v1",
      "title": "TeachBench: A Syllabus-Grounded Framework for Evaluating Teaching Ability in Large Language Models",
      "title_zh": "TeachBench：基于教学大纲的大语言模型教学能力评估框架",
      "authors": [
        "Zheng Li",
        "Siyao Song",
        "Jingyuan Ma",
        "Rui Li",
        "Ying Zeng",
        "Minghao Li",
        "Zhifang Sui"
      ],
      "abstract": "Large language models (LLMs) show promise as teaching assistants, yet their teaching capability remains insufficiently evaluated. Existing benchmarks mainly focus on problem-solving or problem-level guidance, leaving knowledge-centered teaching underexplored. We propose a syllabus-grounded evaluation framework that measures LLM teaching capability via student performance improvement after multi-turn instruction. By restricting teacher agents to structured knowledge points and example problems, the framework avoids information leakage and enables reuse of existing benchmarks. We instantiate the framework on Gaokao data across multiple subjects. Experiments reveal substantial variation in teaching effectiveness across models and domains: some models perform well in mathematics, while teaching remains challenging in physics and chemistry. We also find that incorporating example problems does not necessarily improve teaching, as models often shift toward example-specific error correction. Overall, our results highlight teaching ability as a distinct and measurable dimension of LLM behavior.",
      "tldr_zh": "该研究提出了 TeachBench，这是一个基于大纲 (syllabus-grounded) 的评价框架，旨在衡量大语言模型 (LLMs) 的教学能力，填补了现有基准测试在以知识为中心的教学评估方面的空白。该框架通过学生在多轮指令后的性能提升来量化教学效果，并限制教师智能体仅使用结构化的知识点和示例问题，从而有效防止信息泄露。研究人员在多个学科的高考 (Gaokao) 数据上对该框架进行了实例化测试。实验结果显示，模型的教学效能在不同学科间存在显著差异，虽然在数学领域表现较好，但在物理和化学教学中仍面临挑战。研究还发现，加入示例问题并不总能提升教学质量，模型容易陷入针对特定示例的纠错。该研究最终证明了教学能力是大语言模型一个独特且可被有效衡量的行为维度。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2601.21375v1",
      "published_date": "2026-01-29 08:04:37 UTC",
      "updated_date": "2026-01-29 08:04:37 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-30T04:31:27.900776+00:00"
    },
    {
      "arxiv_id": "2601.21372v1",
      "title": "NEMO: Execution-Aware Optimization Modeling via Autonomous Coding Agents",
      "title_zh": "NEMO：基于自主编程智能体的执行感知优化建模",
      "authors": [
        "Yang Song",
        "Anoushka Vyas",
        "Zirui Wei",
        "Sina Khoshfetrat Pakazad",
        "Henrik Ohlsson",
        "Graham Neubig"
      ],
      "abstract": "In this paper, we present NEMO, a system that translates Natural-language descriptions of decision problems into formal Executable Mathematical Optimization implementations, operating collaboratively with users or autonomously. Existing approaches typically rely on specialized large language models (LLMs) or bespoke, task-specific agents. Such methods are often brittle, complex and frequently generating syntactically invalid or non-executable code.\n  NEMO instead centers on remote interaction with autonomous coding agents (ACAs), treated as a first-class abstraction analogous to API-based interaction with LLMs. This design enables the construction of higher-level systems around ACAs that structure, consolidate, and iteratively refine task specifications. Because ACAs execute within sandboxed environments, code produced by NEMO is executable by construction, allowing automated validation and repair.\n  Building on this, we introduce novel coordination patterns with and across ACAs, including asymmetric validation loops between independently generated optimizer and simulator implementations (serving as a high-level validation mechanism), external memory for experience reuse, and robustness enhancements via minimum Bayes risk (MBR) decoding and self-consistency. We evaluate NEMO on nine established optimization benchmarks. As depicted in Figure 1, it achieves state-of-the-art performance on the majority of tasks, with substantial margins on several datasets, demonstrating the power of execution-aware agentic architectures for automated optimization modeling.",
      "tldr_zh": "该研究提出了 NEMO 系统，旨在将决策问题的自然语言描述转化为正式的 Executable Mathematical Optimization 实现，从而克服现有方法生成的代码脆弱且不可执行的缺陷。NEMO 将 Autonomous Coding Agents (ACAs) 作为核心抽象层，通过在沙盒环境中执行代码，确保了生成的实现方案在构建时即具可执行性，并支持自动验证与修复。系统引入了创新的协作模式，包括优化器与模拟器之间的非对称验证环路 (asymmetric validation loops) 以及用于经验复用的外部存储。此外，NEMO 通过 Minimum Bayes Risk (MBR) 解码和 self-consistency 技术进一步增强了建模的鲁棒性。实验表明，NEMO 在九个优化基准测试中的大多数任务上均达到了 state-of-the-art 性能，并在多个数据集上取得显著领先。该成果有力证明了 execution-aware 智能体架构在自动化优化建模任务中的强大效能。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2601.21372v1",
      "published_date": "2026-01-29 07:57:23 UTC",
      "updated_date": "2026-01-29 07:57:23 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-30T04:31:43.046253+00:00"
    },
    {
      "arxiv_id": "2601.21367v1",
      "title": "Hebbian Learning with Global Direction",
      "title_zh": "具有全局方向的赫布学习",
      "authors": [
        "Wenjia Hua",
        "Kejie Zhao",
        "Luziwei Leng",
        "Ran Cheng",
        "Yuxin Ma",
        "Qinghai Guo"
      ],
      "abstract": "Backpropagation algorithm has driven the remarkable success of deep neural networks, but its lack of biological plausibility and high computational costs have motivated the ongoing search for alternative training methods. Hebbian learning has attracted considerable interest as a biologically plausible alternative to backpropagation. Nevertheless, its exclusive reliance on local information, without consideration of global task objectives, fundamentally limits its scalability. Inspired by the biological synergy between neuromodulators and local plasticity, we introduce a novel model-agnostic Global-guided Hebbian Learning (GHL) framework, which seamlessly integrates local and global information to scale up across diverse networks and tasks. In specific, the local component employs Oja's rule with competitive learning to ensure stable and effective local updates. Meanwhile, the global component introduces a sign-based signal that guides the direction of local Hebbian plasticity updates. Extensive experiments demonstrate that our method consistently outperforms existing Hebbian approaches. Notably, on large-scale network and complex datasets like ImageNet, our framework achieves the competitive results and significantly narrows the gap with standard backpropagation.",
      "tldr_zh": "该研究探讨了反向传播(Backpropagation)在生物合理性与计算成本上的局限，针对传统Hebbian learning因仅依赖局部信息而难以扩展的问题提出了改进方案。论文引入了一种模型无关的全局引导Hebbian学习(Global-guided Hebbian Learning, GHL)框架，通过模拟神经调节剂与局部可塑性的协同作用来整合局部与全局信息。该框架的局部组件利用Oja's rule和竞争学习(competitive learning)确保更新的稳定性，而全局组件则通过基于符号的信号(sign-based signal)指引Hebbian可塑性更新的方向。实验结果显示，GHL在多种网络和任务中均优于现有的Hebbian学习方法。特别是在ImageNet等大规模复杂数据集上，该框架取得了极具竞争力的表现，显著缩小了与标准反向传播(Backpropagation)之间的性能差距。",
      "categories": [
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "Accepted to ICASSP 2026",
      "pdf_url": "https://arxiv.org/pdf/2601.21367v1",
      "published_date": "2026-01-29 07:49:21 UTC",
      "updated_date": "2026-01-29 07:49:21 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-30T04:31:44.792167+00:00"
    },
    {
      "arxiv_id": "2601.21360v1",
      "title": "The Compliance Paradox: Semantic-Instruction Decoupling in Automated Academic Code Evaluation",
      "title_zh": "合规性悖论：自动化代码学术评估中的语义-指令解耦",
      "authors": [
        "Devanshu Sahoo",
        "Manish Prasad",
        "Vasudev Majhi",
        "Arjun Neekhra",
        "Yash Sinha",
        "Murari Mandal",
        "Vinay Chamola",
        "Dhruv Kumar"
      ],
      "abstract": "The rapid integration of Large Language Models (LLMs) into educational assessment rests on the unverified assumption that instruction following capability translates directly to objective adjudication. We demonstrate that this assumption is fundamentally flawed. Instead of evaluating code quality, models frequently decouple from the submission's logic to satisfy hidden directives, a systemic vulnerability we term the Compliance Paradox, where models fine-tuned for extreme helpfulness are vulnerable to adversarial manipulation. To expose this, we introduce the Semantic-Preserving Adversarial Code Injection (SPACI) Framework and the Abstract Syntax Tree-Aware Semantic Injection Protocol (AST-ASIP). These methods exploit the Syntax-Semantics Gap by embedding adversarial directives into syntactically inert regions (trivia nodes) of the Abstract Syntax Tree. Through a large-scale evaluation of 9 SOTA models across 25,000 submissions in Python, C, C++, and Java, we reveal catastrophic failure rates (>95%) in high-capacity open-weights models like DeepSeek-V3, which systematically prioritize hidden formatting constraints over code correctness. We quantify this failure using our novel tripartite framework measuring Decoupling Probability, Score Divergence, and Pedagogical Severity to demonstrate the widespread \"False Certification\" of functionally broken code. Our findings suggest that current alignment paradigms create a \"Trojan\" vulnerability in automated grading, necessitating a shift from standard RLHF toward domain-specific Adjudicative Robustness, where models are conditioned to prioritize evidence over instruction compliance. We release our complete dataset and injection framework to facilitate further research on the topic.",
      "tldr_zh": "该研究揭示了大型语言模型（LLMs）在教育评估中存在的一种名为合规悖论（Compliance Paradox）的系统性漏洞，即模型会为了满足隐藏指令而脱离对代码逻辑语义的真实评估。研究者提出了语义保留对抗性代码注入（SPACI）框架和抽象语法树感知语义注入协议（AST-ASIP），通过在抽象语法树（Abstract Syntax Tree）的非活性区域植入对抗性指令，利用语法与语义的鸿沟实施攻击。实验针对DeepSeek-V3等9种SOTA模型在四种编程语言下的2.5万份代码提交进行了大规模评估，结果显示高容量开源模型在识别代码正确性方面出现了超过95%的灾难性失败率，导致大量功能破损的代码获得错误认证。研究通过解耦概率（Decoupling Probability）、评分偏离度（Score Divergence）和教育学严重程度（Pedagogical Severity）构成的三方框架量化了该风险，指出当前的对齐范式（alignment paradigms）在自动化评分中制造了类似特洛伊木马的弱点。最后，文章建议将研究重心从标准的RLHF转向特定领域的裁决鲁棒性（Adjudicative Robustness），促使模型在评测中优先关注证据而非指令合规性。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.ET",
        "cs.LG",
        "cs.SE"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2601.21360v1",
      "published_date": "2026-01-29 07:40:58 UTC",
      "updated_date": "2026-01-29 07:40:58 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-30T04:31:44.470524+00:00"
    },
    {
      "arxiv_id": "2601.21358v1",
      "title": "Latent Chain-of-Thought as Planning: Decoupling Reasoning from Verbalization",
      "title_zh": "潜空间思维链即规划：推理与语言表达的解耦",
      "authors": [
        "Jiecong Wang",
        "Hao Peng",
        "Chunyang Liu"
      ],
      "abstract": "Chain-of-Thought (CoT) empowers Large Language Models (LLMs) to tackle complex problems, but remains constrained by the computational cost and reasoning path collapse when grounded in discrete token spaces. Recent latent reasoning approaches attempt to optimize efficiency by performing reasoning within continuous hidden states. However, these methods typically operate as opaque end-to-end mappings from explicit reasoning steps to latent states, and often require a pre-defined number of latent steps during inference. In this work, we introduce PLaT (Planning with Latent Thoughts), a framework that reformulates latent reasoning as planning by fundamentally decouple reasoning from verbalization. We model reasoning as a deterministic trajectory of latent planning states, while a separate Decoder grounds these thoughts into text when necessary. This decoupling allows the model to dynamically determine when to terminate reasoning rather than relying on fixed hyperparameters. Empirical results on mathematical benchmarks reveal a distinct trade-off: while PLaT achieves lower greedy accuracy than baselines, it demonstrates superior scalability in terms of reasoning diversity. This indicates that PLaT learns a robust, broader solution space, offering a transparent and scalable foundation for inference-time search.",
      "tldr_zh": "这项研究提出了PLaT (Planning with Latent Thoughts) 框架，旨在解决思维链 (Chain-of-Thought) 在离散令牌空间中面临的计算开销和推理路径崩溃问题。PLaT通过将隐式推理重构为规划过程，实现了推理逻辑与语言表达 (Verbalization) 的根本解耦。该框架将推理过程建模为隐式规划状态的确定性轨迹，并配备独立的解码器 (Decoder) 在必要时将思维转化为文本。这种解耦设计允许模型动态确定推理终止的时机，克服了以往方法对固定超参数的依赖。在数学基准测试中的实验表明，尽管PLaT的贪婪准确率 (greedy accuracy) 低于基线，但在推理多样性方面展现出优越的可扩展性。研究证明PLaT能够学习到更稳健且广泛的解空间，为推理阶段搜索 (inference-time search) 奠定了透明且可扩展的基础。",
      "categories": [
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2601.21358v1",
      "published_date": "2026-01-29 07:38:18 UTC",
      "updated_date": "2026-01-29 07:38:18 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-30T04:31:47.517979+00:00"
    },
    {
      "arxiv_id": "2601.21352v1",
      "title": "BEAP-Agent: Backtrackable Execution and Adaptive Planning for GUI Agents",
      "title_zh": "BEAP-Agent：面向 GUI 智能体的可回溯执行与自适应规划",
      "authors": [
        "Ziyu Lu",
        "Tengjin Weng",
        "Yiying Yang",
        "Yuhang Zhao",
        "Xinxin Huang",
        "Wenhao Jiang"
      ],
      "abstract": "GUI agents are designed to automate repetitive tasks and enhance productivity. However, existing GUI agents struggle to recover once they follow an incorrect exploration path, often leading to task failure. In this work, we model GUI task execution as a DFS process and propose BEAP-Agent, a DFS-based framework that supports long-range, multi-level state backtracking with dynamic task tracking and updating. The framework consists of three collaborative components: Planner, Executor, and Tracker. Together, they enable effective task exploration and execution. BEAP-Agent fills the gap in systematic backtracking mechanisms for GUI agents, offering a systematic solution for long-horizon task exploration. We conducted a systematic evaluation on the OSWorld benchmark, where BEAP-Agent achieved an accuracy of 28.2%, validating the effectiveness of the proposed method.",
      "tldr_zh": "该研究针对GUI agents在进入错误探索路径后难以恢复并导致任务失败的问题，提出了BEAP-Agent框架。该框架将GUI任务执行建模为DFS过程，支持长程、多级状态回溯以及动态任务跟踪与更新。BEAP-Agent由Planner、Executor和Tracker三个协同组件构成，填补了GUI agents在系统性回溯机制方面的空白，为long-horizon任务探索提供了系统化的解决方案。在OSWorld基准测试的评估中，BEAP-Agent取得了28.2%的准确率，有力地验证了该DFS-based框架在提升复杂任务执行成功率方面的有效性。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2601.21352v1",
      "published_date": "2026-01-29 07:22:50 UTC",
      "updated_date": "2026-01-29 07:22:50 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-30T04:31:47.301944+00:00"
    },
    {
      "arxiv_id": "2601.21351v1",
      "title": "Theoretically Optimal Attention/FFN Ratios in Disaggregated LLM Serving",
      "title_zh": "解耦式 LLM 推理服务中 Attention 与 FFN 的理论最优比例",
      "authors": [
        "Chendong Song",
        "Meixuan Wang",
        "Hang Zhou",
        "Hong Liang",
        "Yuan Lyu",
        "Zixi Chen",
        "Yuwei Fan",
        "Zijie Zhou"
      ],
      "abstract": "Attention-FFN disaggregation (AFD) is an emerging architecture for LLM decoding that separates state-heavy, KV-cache-dominated Attention computation from stateless, compute-intensive FFN computation, connected by per-step communication. While AFD enables independent scaling of memory and compute resources, its performance is highly sensitive to the Attention/FFN provisioning ratio: mis-sizing induces step-level blocking and costly device idle time. We develop a tractable analytical framework for sizing AFD bundles in an $r$A-$1$F topology, where the key difficulty is that Attention-side work is nonstationary-token context grows and requests are continuously replenished with random lengths-while FFN work is stable given the aggregated batch. Using a probabilistic workload model, we derive closed-form rules for the optimal A/F ratio that maximize average throughput per instance across the system. A trace-calibrated AFD simulator validates the theory: across workloads, the theoretical optimal A/F ratio matches the simulation-optimal within 10%, and consistently reduces idle time.",
      "tldr_zh": "该研究针对大语言模型(LLM)推理中新兴的注意力与前馈网络解耦(Attention-FFN disaggregation, AFD)架构，探讨了如何通过优化资源配置比例来提升系统性能。在AFD架构中，有状态的Attention计算与计算密集型的FFN计算被分离，但两者之间的Provisioning ratio（供应比例）不当会导致步骤级阻塞和严重的设备空闲。研究者开发了一个分析框架用于确定$r$A-1F拓扑结构下AFD的最佳规模，解决了Attention端负载由于Token上下文增长而呈现的非平稳性难题。通过概率工作负载模型，该研究推导出了旨在最大化系统平均Throughput（吞吐量）的最优A/F ratio闭式规则。实验验证表明，该理论计算的最优比例与模拟最优值误差在10%以内，并能显著减少设备空闲时间。这一成果为分布式LLM推理服务的资源优化配置提供了重要的理论支撑。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "Submitted to ICML 2026",
      "pdf_url": "https://arxiv.org/pdf/2601.21351v1",
      "published_date": "2026-01-29 07:22:27 UTC",
      "updated_date": "2026-01-29 07:22:27 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-30T04:31:55.292968+00:00"
    },
    {
      "arxiv_id": "2601.21349v1",
      "title": "L2R: Low-Rank and Lipschitz-Controlled Routing for Mixture-of-Experts",
      "title_zh": "L2R：面向混合专家模型的低秩与 Lipschitz 受控路由",
      "authors": [
        "Minghao Yang",
        "Ren Togo",
        "Guang Li",
        "Takahiro Ogawa",
        "Miki Haseyama"
      ],
      "abstract": "Mixture-of-Experts (MoE) models scale neural networks by conditionally activating a small subset of experts, where the router plays a central role in determining expert specialization and overall model performance. However, many modern MoE systems still adopt linear routers in raw high-dimensional representation spaces, where representation mismatch, angular concentration, and scale-sensitive scoring can jointly undermine routing discriminability and stable expert specialization. In this work, we propose Low-rank \\& Lipschitz-controlled Routing (L2R), a unified routing framework that reshapes both the routing space and scoring geometry. L2R performs expert assignment in a shared low-rank latent routing space and introduces Saturated Inner-Product Scoring (SIPS) to explicitly control the Lipschitz behavior of routing functions, yielding smoother and more stable routing geometry. In addition, L2R incorporates a parameter-efficient multi-anchor routing mechanism to enhance expert expressiveness. Extensive experiments on a large-scale language MoE model and a vision MoE setting on ImageNet demonstrate that L2R consistently improves routing stability, expert specialization, and overall model performance.",
      "tldr_zh": "该研究提出了L2R (Low-Rank and Lipschitz-Controlled Routing)，一种针对Mixture-of-Experts (MoE) 模型的统一路由框架，旨在解决现有线性路由在高维空间中因表示不匹配、角度集中和尺度敏感评分导致的判别力不足及专家专业化不稳定问题。L2R通过在共享的低秩潜路由空间 (low-rank latent routing space) 中执行专家分配重塑了路由几何，并引入饱和内积评分 (Saturated Inner-Product Scoring, SIPS) 来显式控制路由函数的Lipschitz行为。此外，该框架还结合了参数高效的多锚点路由机制以进一步增强专家的表达能力。在大型语言MoE模型及ImageNet视觉MoE设置上的广泛实验证明，L2R显著提升了路由稳定性、专家专业化程度以及模型的整体性能。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2601.21349v1",
      "published_date": "2026-01-29 07:18:33 UTC",
      "updated_date": "2026-01-29 07:18:33 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-30T04:31:58.021604+00:00"
    },
    {
      "arxiv_id": "2601.21348v1",
      "title": "Memorization Control in Diffusion Models from Denoising-centric Perspective",
      "title_zh": "基于去噪中心视角的扩散模型记忆控制",
      "authors": [
        "Thuy Phuong Vu",
        "Mai Viet Hoang Do",
        "Minhhuy Le",
        "Dinh-Cuong Hoang",
        "Phan Xuan Tan"
      ],
      "abstract": "Controlling memorization in diffusion models is critical for applications that require generated data to closely match the training distribution. Existing approaches mainly focus on data centric or model centric modifications, treating the diffusion model as an isolated predictor. In this paper, we study memorization in diffusion models from a denoising centric perspective. We show that uniform timestep sampling leads to unequal learning contributions across denoising steps due to differences in signal to noise ratio, which biases training toward memorization. To address this, we propose a timestep sampling strategy that explicitly controls where learning occurs along the denoising trajectory. By adjusting the width of the confidence interval, our method provides direct control over the memorization generalization trade off. Experiments on image and 1D signal generation tasks demonstrate that shifting learning emphasis toward later denoising steps consistently reduces memorization and improves distributional alignment with training data, validating the generality and effectiveness of our approach.",
      "tldr_zh": "该研究从去噪中心(Denoising-centric)的角度探讨了扩散模型(Diffusion Models)中的记忆化(Memorization)控制问题。作者发现，由于信噪比(Signal-to-Noise Ratio)的差异，传统均匀的时间步采样(Uniform Timestep Sampling)会导致不同去噪步骤的学习贡献不均，从而使模型偏向于记忆训练数据。为了解决这一问题，论文提出了一种新的时间步采样策略，通过调节置信区间(Confidence Interval)的宽度来显式控制去噪轨迹中的学习分布。该方法实现了对记忆化与泛化(Generalization)权衡的直接控制。在图像和一维信号生成任务上的实验证明，将学习重心转移到去噪后期步骤可以持续减少记忆化现象，并显著改善生成结果与训练数据的分布对齐。这一发现验证了从去噪过程优化模型学习分布在提升扩散模型性能方面的普适性和有效性。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2601.21348v1",
      "published_date": "2026-01-29 07:16:54 UTC",
      "updated_date": "2026-01-29 07:16:54 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-30T04:31:58.879286+00:00"
    },
    {
      "arxiv_id": "2601.21344v1",
      "title": "Dynamic Framework for Collaborative Learning: Leveraging Advanced LLM with Adaptive Feedback Mechanisms",
      "title_zh": "协作学习动态框架：利用具备自适应反馈机制的先进 LLM",
      "authors": [
        "Hassam Tahir",
        "Faizan Faisal",
        "Fady Alnajjar",
        "Muhammad Imran Taj",
        "Lucia Gordon",
        "Aila Khan",
        "Michael Lwin",
        "Omar Mubin"
      ],
      "abstract": "This paper presents a framework for integrating LLM into collaborative learning platforms to enhance student engagement, critical thinking, and inclusivity. The framework employs advanced LLMs as dynamic moderators to facilitate real-time discussions and adapt to learners' evolving needs, ensuring diverse and inclusive educational experiences. Key innovations include robust feedback mechanisms that refine AI moderation, promote reflective learning, and balance participation among users. The system's modular architecture featuring ReactJS for the frontend, Flask for backend operations, and efficient question retrieval supports personalized and engaging interactions through dynamic adjustments to prompts and discussion flows. Testing demonstrates that the framework significantly improves student collaboration, fosters deeper comprehension, and scales effectively across various subjects and user groups. By addressing limitations in static moderation and personalization in existing systems, this work establishes a strong foundation for next-generation AI-driven educational tools, advancing equitable and impactful learning outcomes.",
      "tldr_zh": "该研究提出了一个将 LLM 集成到协作学习平台中的动态框架，旨在增强学生的参与度、批判性思维和包容性。该框架利用先进的 LLM 作为动态主持人（moderators）来促进实时讨论并适应学习者不断变化的需求，确保提供多样化且包容的教育体验。核心创新包括引入稳健的反馈机制（feedback mechanisms）以优化 AI 引导逻辑、促进反思性学习并平衡用户间的参与度。系统采用 ReactJS 前端和 Flask 后端的模块化架构，通过高效的问题检索以及对提示词（prompts）和讨论流的动态调整，支持高度个性化的互动。实验测试证明该框架显著改善了学生间的协作，加深了对知识的理解，并能在不同学科和用户群体中有效扩展。通过解决现有系统中静态调节和个性化不足的问题，该工作为下一代 AI 驱动的教育工具奠定了基础，推动了公平且有影响力的学习成果。",
      "categories": [
        "cs.AI",
        "cs.HC",
        "cs.SE"
      ],
      "primary_category": "cs.AI",
      "comment": "Publication Link: https://ieeexplore.ieee.org/document/11118419",
      "pdf_url": "https://arxiv.org/pdf/2601.21344v1",
      "published_date": "2026-01-29 07:14:43 UTC",
      "updated_date": "2026-01-29 07:14:43 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-30T04:32:02.365397+00:00"
    },
    {
      "arxiv_id": "2601.21343v1",
      "title": "Self-Improving Pretraining: using post-trained models to pretrain better models",
      "title_zh": "自改进预训练：利用训练后模型预训练更优模型",
      "authors": [
        "Ellen Xiaoqing Tan",
        "Shehzaad Dhuliawala",
        "Jing Xu",
        "Ping Yu",
        "Sainbayar Sukhbaatar",
        "Jason Weston",
        "Olga Golovneva"
      ],
      "abstract": "Ensuring safety, factuality and overall quality in the generations of large language models is a critical challenge, especially as these models are increasingly deployed in real-world applications. The prevailing approach to addressing these issues involves collecting expensive, carefully curated datasets and applying multiple stages of fine-tuning and alignment. However, even this complex pipeline cannot guarantee the correction of patterns learned during pretraining. Therefore, addressing these issues during pretraining is crucial, as it shapes a model's core behaviors and prevents unsafe or hallucinated outputs from becoming deeply embedded. To tackle this issue, we introduce a new pretraining method that streams documents and uses reinforcement learning (RL) to improve the next K generated tokens at each step. A strong, post-trained model judges candidate generations -- including model rollouts, the original suffix, and a rewritten suffix -- for quality, safety, and factuality. Early in training, the process relies on the original and rewritten suffixes; as the model improves, RL rewards high-quality rollouts. This approach builds higher quality, safer, and more factual models from the ground up. In experiments, our method gives 36.2% and 18.5% relative improvements over standard pretraining in terms of factuality and safety, and up to 86.3% win rate improvements in overall generation quality.",
      "tldr_zh": "该研究提出了Self-Improving Pretraining，这是一种利用训练后的模型来预训练出更优模型的新方法，旨在从预训练阶段解决大语言模型在安全性、事实性和生成质量方面的核心挑战。该方法采用流式文档处理模式，并结合强化学习（Reinforcement Learning）在每一步改进后续生成的K个token。核心机制是引入一个强大的post-trained model作为裁判，对模型rollouts、原始后缀及重写后缀的质量进行评估。训练初期侧重于利用原始和重写数据，随着模型性能提升，强化学习转而奖励高质量的rollouts，从而自底向上构建更安全、更准确的模型。实验结果显示，该方法在事实性和安全性方面较标准预训练分别实现了36.2%和18.5%的相对提升，在整体生成质量上的胜率提升最高达86.3%。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2601.21343v1",
      "published_date": "2026-01-29 07:09:30 UTC",
      "updated_date": "2026-01-29 07:09:30 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-30T04:32:07.453021+00:00"
    },
    {
      "arxiv_id": "2601.21342v1",
      "title": "Ostrakon-VL: Towards Domain-Expert MLLM for Food-Service and Retail Stores",
      "title_zh": "Ostrakon-VL：面向餐饮与零售场景的领域专家级多模态大语言模型",
      "authors": [
        "Zhiyong Shen",
        "Gongpeng Zhao",
        "Jun Zhou",
        "Li Yu",
        "Guandong Kou",
        "Jichen Li",
        "Chuanlei Dong",
        "Zuncheng Li",
        "Kaimao Li",
        "Bingkun Wei",
        "Shicheng Hu",
        "Wei Xia",
        "Wenguo Duan"
      ],
      "abstract": "Multimodal Large Language Models (MLLMs) have recently achieved substantial progress in general-purpose perception and reasoning. Nevertheless, their deployment in Food-Service and Retail Stores (FSRS) scenarios encounters two major obstacles: (i) real-world FSRS data, collected from heterogeneous acquisition devices, are highly noisy and lack auditable, closed-loop data curation, which impedes the construction of high-quality, controllable, and reproducible training corpora; and (ii) existing evaluation protocols do not offer a unified, fine-grained and standardized benchmark spanning single-image, multi-image, and video inputs, making it challenging to objectively gauge model robustness. To address these challenges, we first develop Ostrakon-VL, an FSRS-oriented MLLM based on Qwen3-VL-8B. Second, we introduce ShopBench, the first public benchmark for FSRS. Third, we propose QUAD (Quality-aware Unbiased Automated Data-curation), a multi-stage multimodal instruction data curation pipeline. Leveraging a multi-stage training strategy, Ostrakon-VL achieves an average score of 60.1 on ShopBench, establishing a new state of the art among open-source MLLMs with comparable parameter scales and diverse architectures. Notably, it surpasses the substantially larger Qwen3-VL-235B-A22B (59.4) by +0.7, and exceeds the same-scale Qwen3-VL-8B (55.3) by +4.8, demonstrating significantly improved parameter efficiency. These results indicate that Ostrakon-VL delivers more robust and reliable FSRS-centric perception and decision-making capabilities. To facilitate reproducible research, we will publicly release Ostrakon-VL and the ShopBench benchmark.",
      "tldr_zh": "该研究开发了Ostrakon-VL，这是一款针对餐饮和零售商店(FSRS)场景设计的领域专家级多模态大语言模型(MLLM)。为了解决FSRS场景中数据噪声大和缺乏统一评估标准的问题，研究团队提出了首个公开的FSRS综合基准ShopBench，并开发了名为QUAD的多阶段多模态指令数据清洗管线。Ostrakon-VL基于Qwen3-VL-8B构建，通过多阶段训练策略在ShopBench上取得了60.1的平均分，刷新了同规模开源模型的SOTA纪录。实验结果显示，Ostrakon-VL在参数效率上表现优异，不仅超越了规模更大的Qwen3-VL-235B-A22B模型，还显著提升了在FSRS核心场景下的感知和决策能力。该研究通过公开发布模型和基准，为该领域的可复现研究奠定了基础。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2601.21342v1",
      "published_date": "2026-01-29 07:07:23 UTC",
      "updated_date": "2026-01-29 07:07:23 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-30T04:32:23.492838+00:00"
    },
    {
      "arxiv_id": "2601.21340v1",
      "title": "EHR-RAG: Bridging Long-Horizon Structured Electronic Health Records and Large Language Models via Enhanced Retrieval-Augmented Generation",
      "title_zh": "EHR-RAG：通过增强型检索增强生成连接长程结构化电子健康记录与大语言模型",
      "authors": [
        "Lang Cao",
        "Qingyu Chen",
        "Yue Guo"
      ],
      "abstract": "Electronic Health Records (EHRs) provide rich longitudinal clinical evidence that is central to medical decision-making, motivating the use of retrieval-augmented generation (RAG) to ground large language model (LLM) predictions. However, long-horizon EHRs often exceed LLM context limits, and existing approaches commonly rely on truncation or vanilla retrieval strategies that discard clinically relevant events and temporal dependencies. To address these challenges, we propose EHR-RAG, a retrieval-augmented framework designed for accurate interpretation of long-horizon structured EHR data. EHR-RAG introduces three components tailored to longitudinal clinical prediction tasks: Event- and Time-Aware Hybrid EHR Retrieval to preserve clinical structure and temporal dynamics, Adaptive Iterative Retrieval to progressively refine queries in order to expand broad evidence coverage, and Dual-Path Evidence Retrieval and Reasoning to jointly retrieves and reasons over both factual and counterfactual evidence. Experiments across four long-horizon EHR prediction tasks show that EHR-RAG consistently outperforms the strongest LLM-based baselines, achieving an average Macro-F1 improvement of 10.76%. Overall, our work highlights the potential of retrieval-augmented LLMs to advance clinical prediction on structured EHR data in practice.",
      "tldr_zh": "针对长期结构化电子健康档案(EHRs)超出大语言模型(LLMs)上下文限制以及传统检索方法丢失临床事件和时间依赖性的问题，该研究提出了EHR-RAG框架。该框架引入了事件与时间感知混合检索(Event- and Time-Aware Hybrid EHR Retrieval)技术，旨在完整保留临床结构和时间动态。通过自适应迭代检索(Adaptive Iterative Retrieval)，EHR-RAG能够逐步细化查询以扩大证据覆盖范围。此外，该框架利用双路径证据检索与推理(Dual-Path Evidence Retrieval and Reasoning)协同处理事实性和反事实性证据。在四项长期EHR预测任务中的实验表明，EHR-RAG在Macro-F1指标上平均提升了10.76%，显著优于最强的LLM基线模型。该项工作展示了检索增强生成(RAG)在处理复杂临床数据及提升医疗预测实践方面的巨大潜力。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2601.21340v1",
      "published_date": "2026-01-29 07:06:34 UTC",
      "updated_date": "2026-01-29 07:06:34 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-30T04:32:19.851307+00:00"
    },
    {
      "arxiv_id": "2601.21339v1",
      "title": "Within-Model vs Between-Prompt Variability in Large Language Models for Creative Tasks",
      "title_zh": "大语言模型在创意任务中的模型内变异性与提示词间变异性对比",
      "authors": [
        "Jennifer Haase",
        "Jana Gonnermann-Müller",
        "Paul H. P. Hanel",
        "Nicolas Leins",
        "Thomas Kosch",
        "Jan Mendling",
        "Sebastian Pokutta"
      ],
      "abstract": "How much of LLM output variance is explained by prompts versus model choice versus stochasticity through sampling? We answer this by evaluating 12 LLMs on 10 creativity prompts with 100 samples each (N = 12,000). For output quality (originality), prompts explain 36.43% of variance, comparable to model choice (40.94%). But for output quantity (fluency), model choice (51.25%) and within-LLM variance (33.70%) dominate, with prompts explaining only 4.22%. Prompts are powerful levers for steering output quality, but given the substantial within-LLM variance (10-34%), single-sample evaluations risk conflating sampling noise with genuine prompt or model effects.",
      "tldr_zh": "该研究探讨了大型语言模型（LLMs）在创意任务中的输出方差来源，重点对比了提示词（prompts）、模型选择（model choice）以及采样随机性（stochasticity）的影响。研究人员通过对12个LLMs在10个创意提示词下的12,000个样本进行系统评估，发现对于输出质量（originality）而言，提示词的解释力（36.43%）与模型选择（40.94%）相当。但在输出数量（fluency）方面，模型选择（51.25%）和模型内部方差（within-LLM variance，33.70%）占据主导，提示词的影响力仅为4.22%。研究结果表明，提示词虽然是调节输出质量的强力杠杆，但由于模型内部存在显著的随机波动，传统的单样本评估（single-sample evaluations）极易将采样噪声误认为模型或提示词的实际效能。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2601.21339v1",
      "published_date": "2026-01-29 07:04:46 UTC",
      "updated_date": "2026-01-29 07:04:46 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-30T04:32:24.208372+00:00"
    },
    {
      "arxiv_id": "2601.21335v1",
      "title": "Modeling Endogenous Logic: Causal Neuro-Symbolic Reasoning Model for Explainable Multi-Behavior Recommendation",
      "title_zh": "建模内生逻辑：面向可解释多行为推荐的因果神经符号推理模型",
      "authors": [
        "Yuzhe Chen",
        "Jie Cao",
        "Youquan Wang",
        "Haicheng Tao",
        "Darko B. Vukovic",
        "Jia Wu"
      ],
      "abstract": "Existing multi-behavior recommendations tend to prioritize performance at the expense of explainability, while current explainable methods suffer from limited generalizability due to their reliance on external information. Neuro-Symbolic integration offers a promising avenue for explainability by combining neural networks with symbolic logic rule reasoning. Concurrently, we posit that user behavior chains inherently embody an endogenous logic suitable for explicit reasoning. However, these observational multiple behaviors are plagued by confounders, causing models to learn spurious correlations. By incorporating causal inference into this Neuro-Symbolic framework, we propose a novel Causal Neuro-Symbolic Reasoning model for Explainable Multi-Behavior Recommendation (CNRE). CNRE operationalizes the endogenous logic by simulating a human-like decision-making process. Specifically, CNRE first employs hierarchical preference propagation to capture heterogeneous cross-behavior dependencies. Subsequently, it models the endogenous logic rule implicit in the user's behavior chain based on preference strength, and adaptively dispatches to the corresponding neural-logic reasoning path (e.g., conjunction, disjunction). This process generates an explainable causal mediator that approximates an ideal state isolated from confounding effects. Extensive experiments on three large-scale datasets demonstrate CNRE's significant superiority over state-of-the-art baselines, offering multi-level explainability from model design and decision process to recommendation results.",
      "tldr_zh": "该研究提出了CNRE（Causal Neuro-Symbolic Reasoning model for Explainable Multi-Behavior Recommendation），旨在解决现有多行为推荐（Multi-behavior recommendation）在可解释性与泛化性方面的不足，并处理观测数据中的混杂因素（confounders）导致的虚假相关性。CNRE通过结合因果推理（Causal inference）与神经符号（Neuro-Symbolic）集成，模拟人类决策过程来挖掘用户行为链中蕴含的内生逻辑（Endogenous logic）。该模型首先利用层次化偏好传播（hierarchical preference propagation）捕捉异构的跨行为依赖关系，并根据偏好强度对逻辑规则进行建模，自适应地分发至合取或析取等神经逻辑推理路径。通过这一过程，模型生成了一个可解释的因果中介变量（causal mediator），从而有效隔离了混杂效应。在三个大规模数据集上的实验证明，CNRE在性能上显著优于现有基准模型，并从模型设计、决策过程到推荐结果提供了多层次的可解释性。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "Accepted to The Web Conference (WWW) 2026",
      "pdf_url": "https://arxiv.org/pdf/2601.21335v1",
      "published_date": "2026-01-29 06:51:54 UTC",
      "updated_date": "2026-01-29 06:51:54 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-30T04:32:24.486565+00:00"
    },
    {
      "arxiv_id": "2601.21323v1",
      "title": "Adversarial Vulnerability Transcends Computational Paradigms: Feature Engineering Provides No Defense Against Neural Adversarial Transfer",
      "title_zh": "对抗脆弱性跨越计算范式：特征工程无法抵御神经对抗迁移",
      "authors": [
        "Achraf Hsain",
        "Ahmed Abdelkader",
        "Emmanuel Baldwin Mbaya",
        "Hamoud Aljamaan"
      ],
      "abstract": "Deep neural networks are vulnerable to adversarial examples--inputs with imperceptible perturbations causing misclassification. While adversarial transfer within neural networks is well-documented, whether classical ML pipelines using handcrafted features inherit this vulnerability when attacked via neural surrogates remains unexplored. Feature engineering creates information bottlenecks through gradient quantization and spatial binning, potentially filtering high-frequency adversarial signals. We evaluate this hypothesis through the first comprehensive study of adversarial transfer from DNNs to HOG-based classifiers. Using VGG16 as a surrogate, we generate FGSM and PGD adversarial examples and test transfer to four classical classifiers (KNN, Decision Tree, Linear SVM, Kernel SVM) and a shallow neural network across eight HOG configurations on CIFAR-10. Our results strongly refute the protective hypothesis: all classifiers suffer 16.6%-59.1% relative accuracy drops, comparable to neural-to-neural transfer. More surprisingly, we discover attack hierarchy reversal--contrary to patterns where iterative PGD dominates FGSM within neural networks, FGSM causes greater degradation than PGD in 100% of classical ML cases, suggesting iterative attacks overfit to surrogate-specific features that don't survive feature extraction. Block normalization provides partial but insufficient mitigation. These findings demonstrate that adversarial vulnerability is not an artifact of end-to-end differentiability but a fundamental property of image classification systems, with implications for security-critical deployments across computational paradigms.",
      "tldr_zh": "这项研究探讨了基于手工特征(handcrafted features)的传统机器学习流水线是否会继承深度神经网络(DNNs)的对抗脆弱性。研究人员通过将 VGG16 作为替代模型生成 FGSM 和 PGD 对抗样本，首次全面评估了对抗攻击从 DNNs 向基于 HOG 特征的分类器（如 KNN、Decision Tree、SVM 等）的迁移性。实验结果有力地反驳了特征工程能通过信息瓶颈过滤对抗信号的假设，所有传统分类器在 CIFAR-10 数据集上均出现了 16.6% 至 59.1% 的相对准确率下降，其受损程度与神经网络间的迁移相当。研究还意外发现了一个攻击等级逆转(attack hierarchy reversal)现象，即在传统机器学习案例中，FGSM 的破坏力始终超过 PGD，暗示迭代攻击可能过度拟合了无法在特征提取中存活的特定特征。尽管 Block normalization 提供了部分缓解，但仍不足以提供有效防御。该研究证明了对抗脆弱性并非端到端可微性的特有产物，而是图像分类系统的基本属性，这对安全关键领域的跨范式部署具有重要警示意义。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2601.21323v1",
      "published_date": "2026-01-29 06:35:46 UTC",
      "updated_date": "2026-01-29 06:35:46 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-30T04:32:32.206299+00:00"
    },
    {
      "arxiv_id": "2601.21321v1",
      "title": "White-Box Op-Amp Design via Human-Mimicking Reasoning",
      "title_zh": "基于类人推理的白盒化运算放大器设计",
      "authors": [
        "Zihao Chen",
        "Jiayin Wang",
        "Ziyi Sun",
        "Ji Zhuang",
        "Jinyi Shen",
        "Xiaoyue Ke",
        "Li Shang",
        "Xuan Zeng",
        "Fan Yang"
      ],
      "abstract": "This brief proposes \\emph{White-Op}, an interpretable operational amplifier (op-amp) parameter design framework based on the human-mimicking reasoning of large-language-model agents. We formalize the implicit human reasoning mechanism into explicit steps of \\emph{\\textbf{introducing hypothetical constraints}}, and develop an iterative, human-like \\emph{\\textbf{hypothesis-verification-decision}} workflow. Specifically, the agent is guided to introduce hypothetical constraints to derive and properly regulate positions of symbolically tractable poles and zeros, thus formulating a closed-form mathematical optimization problem, which is then solved programmatically and verified via simulation. Theory-simulation result analysis guides the decision-making for refinement. Experiments on 9 op-amp topologies show that, unlike the uninterpretable black-box baseline which finally fails in 5 topologies, White-Op achieves reliable, interpretable behavioral-level designs with only 8.52\\% theoretical prediction error and the design functionality retains after transistor-level mapping for all topologies. White-Op is open-sourced at \\textcolor{blue}{https://github.com/zhchenfdu/whiteop}.",
      "tldr_zh": "该研究提出了White-Op，一种基于大语言模型(LLM)智能体模拟人类推理的可解释运算放大器(op-amp)参数设计框架。White-Op通过将人类隐性推理机制形式化为引入假设约束(hypothetical constraints)的显性步骤，构建了迭代式的“假设-验证-决策”(hypothesis-verification-decision)工作流。该框架引导智能体通过假设约束来推导并调节符号化可处理的极点和零点位置，进而形成闭式数学优化问题，并结合程序化求解与仿真验证进行闭环优化。在9种运算放大器拓扑上的实验表明，White-Op实现了理论预测误差仅为8.52%的可靠行为级设计，且所有设计在晶体管级映射后均能正常工作。相比于在5种拓扑中失败的不可解释黑盒基准模型，White-Op不仅保证了设计的成功率，更提供了传统自动化工具所欠缺的透明度和可解释性。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2601.21321v1",
      "published_date": "2026-01-29 06:30:37 UTC",
      "updated_date": "2026-01-29 06:30:37 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-30T04:32:32.969224+00:00"
    },
    {
      "arxiv_id": "2601.21316v1",
      "title": "Heterogeneous Vertiport Selection Optimization for On-Demand Air Taxi Services: A Deep Reinforcement Learning Approach",
      "title_zh": "面向按需空中出租车服务的异构垂直起降场选择优化：基于深度强化学习的方法",
      "authors": [
        "Aoyu Pang",
        "Maonan Wang",
        "Zifan Sha",
        "Wenwei Yue",
        "Changle Li",
        "Chung Shue Chen",
        "Man-On Pun"
      ],
      "abstract": "Urban Air Mobility (UAM) has emerged as a transformative solution to alleviate urban congestion by utilizing low-altitude airspace, thereby reducing pressure on ground transportation networks. To enable truly efficient and seamless door-to-door travel experiences, UAM requires close integration with existing ground transportation infrastructure. However, current research on optimal integrated routing strategies for passengers in air-ground mobility systems remains limited, with a lack of systematic exploration.To address this gap, we first propose a unified optimization model that integrates strategy selection for both air and ground transportation. This model captures the dynamic characteristics of multimodal transport networks and incorporates real-time traffic conditions alongside passenger decision-making behavior. Building on this model, we propose a Unified Air-Ground Mobility Coordination (UAGMC) framework, which leverages deep reinforcement learning (RL) and Vehicle-to-Everything (V2X) communication to optimize vertiport selection and dynamically plan air taxi routes. Experimental results demonstrate that UAGMC achieves a 34\\% reduction in average travel time compared to conventional proportional allocation methods, enhancing overall travel efficiency and providing novel insights into the integration and optimization of multimodal transportation systems. This work lays a solid foundation for advancing intelligent urban mobility solutions through the coordination of air and ground transportation modes. The related code can be found at https://github.com/Traffic-Alpha/UAGMC.",
      "tldr_zh": "该研究针对城市空中交通(UAM)与地面交通集成中路径规划策略研究不足的问题，提出了一个捕捉多模态运输网络动态特性及实时交通状况的统一优化模型。在此基础上，作者开发了统一空地协同移动(UAGMC)框架，通过结合深度强化学习(RL)和车联网(V2X)通信技术，实现了垂直起降点(vertiport)的优化选择与空中的士路径的动态规划。实验结果表明，UAGMC框架相比传统的比例分配方法，使平均旅行时间缩短了34%，显著提升了整体出行效率。该工作为通过空地交通协同推进智能化城市移动方案奠定了坚实基础，并为多模态交通系统的集成与优化提供了创新性的见解。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2601.21316v1",
      "published_date": "2026-01-29 06:26:16 UTC",
      "updated_date": "2026-01-29 06:26:16 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-30T04:32:36.753390+00:00"
    },
    {
      "arxiv_id": "2601.21315v1",
      "title": "Distributionally Robust Classification for Multi-source Unsupervised Domain Adaptation",
      "title_zh": "面向多源无监督领域自适应的分布鲁棒分类",
      "authors": [
        "Seonghwi Kim",
        "Sung Ho Jo",
        "Wooseok Ha",
        "Minwoo Chae"
      ],
      "abstract": "Unsupervised domain adaptation (UDA) is a statistical learning problem when the distribution of training (source) data is different from that of test (target) data. In this setting, one has access to labeled data only from the source domain and unlabeled data from the target domain. The central objective is to leverage the source data and the unlabeled target data to build models that generalize to the target domain. Despite its potential, existing UDA approaches often struggle in practice, particularly in scenarios where the target domain offers only limited unlabeled data or spurious correlations dominate the source domain. To address these challenges, we propose a novel distributionally robust learning framework that models uncertainty in both the covariate distribution and the conditional label distribution. Our approach is motivated by the multi-source domain adaptation setting but is also directly applicable to the single-source scenario, making it versatile in practice. We develop an efficient learning algorithm that can be seamlessly integrated with existing UDA methods. Extensive experiments under various distribution shift scenarios show that our method consistently outperforms strong baselines, especially when target data are extremely scarce.",
      "tldr_zh": "该研究针对多源无监督领域自适应 (Multi-source Unsupervised Domain Adaptation) 中目标领域数据稀缺及源领域存在虚假相关性等挑战，提出了一种新型的分布稳健学习框架 (distributionally robust learning framework)。该框架通过对协变量分布 (covariate distribution) 和条件标签分布 (conditional label distribution) 的不确定性进行同时建模，有效增强了模型对领域间差异的适应性。该方法不仅适用于多源场景，也能直接应用于单源领域自适应，展现出极高的实际应用灵活性。研究团队还开发了一套高效的学习算法，能够与现有的 UDA 方法无缝集成。在多种分布偏移场景下的广泛实验表明，该方法在性能上始终优于强基线模型，尤其在目标领域样本极其匮乏的极端情况下表现尤为出色。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "Accepted at ICLR 2026. 10 pages (excluding references)",
      "pdf_url": "https://arxiv.org/pdf/2601.21315v1",
      "published_date": "2026-01-29 06:23:14 UTC",
      "updated_date": "2026-01-29 06:23:14 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-30T04:32:36.168978+00:00"
    },
    {
      "arxiv_id": "2601.21306v1",
      "title": "The Surprising Difficulty of Search in Model-Based Reinforcement Learning",
      "title_zh": "基于模型的强化学习中搜索出人意料的难度",
      "authors": [
        "Wei-Di Chang",
        "Mikael Henaff",
        "Brandon Amos",
        "Gregory Dudek",
        "Scott Fujimoto"
      ],
      "abstract": "This paper investigates search in model-based reinforcement learning (RL). Conventional wisdom holds that long-term predictions and compounding errors are the primary obstacles for model-based RL. We challenge this view, showing that search is not a plug-and-play replacement for a learned policy. Surprisingly, we find that search can harm performance even when the model is highly accurate. Instead, we show that mitigating distribution shift matters more than improving model or value function accuracy. Building on this insight, we identify key techniques for enabling effective search, achieving state-of-the-art performance across multiple popular benchmark domains.",
      "tldr_zh": "该研究深入探讨了基于模型的强化学习(Model-Based Reinforcement Learning)中的搜索(Search)机制。研究者挑战了长期预测和复合误差(Compounding Errors)是该领域主要障碍的传统观点，发现即使在模型高度准确的情况下，搜索也可能损害性能。研究表明，相比于提升模型或价值函数(Value Function)的准确性，缓解分布偏移(Distribution Shift)对于确保搜索的有效性更为关键。基于这一见解，论文确定了一系列实现有效搜索的关键技术，并在多个主流基准测试中取得了最先进的(State-of-the-Art)性能表现。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2601.21306v1",
      "published_date": "2026-01-29 05:58:24 UTC",
      "updated_date": "2026-01-29 05:58:24 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-30T04:32:39.941179+00:00"
    },
    {
      "arxiv_id": "2601.21296v1",
      "title": "Grounding and Enhancing Informativeness and Utility in Dataset Distillation",
      "title_zh": "数据集蒸馏中信息量与效用的理论确立与增强",
      "authors": [
        "Shaobo Wang",
        "Yantai Yang",
        "Guo Chen",
        "Peiru Li",
        "Kaixin Li",
        "Yufa Zhou",
        "Zhaorun Chen",
        "Linfeng Zhang"
      ],
      "abstract": "Dataset Distillation (DD) seeks to create a compact dataset from a large, real-world dataset. While recent methods often rely on heuristic approaches to balance efficiency and quality, the fundamental relationship between original and synthetic data remains underexplored. This paper revisits knowledge distillation-based dataset distillation within a solid theoretical framework. We introduce the concepts of Informativeness and Utility, capturing crucial information within a sample and essential samples in the training set, respectively. Building on these principles, we define optimal dataset distillation mathematically. We then present InfoUtil, a framework that balances informativeness and utility in synthesizing the distilled dataset. InfoUtil incorporates two key components: (1) game-theoretic informativeness maximization using Shapley Value attribution to extract key information from samples, and (2) principled utility maximization by selecting globally influential samples based on Gradient Norm. These components ensure that the distilled dataset is both informative and utility-optimized. Experiments demonstrate that our method achieves a 6.1\\% performance improvement over the previous state-of-the-art approach on ImageNet-1K dataset using ResNet-18.",
      "tldr_zh": "该研究针对数据集蒸馏(Dataset Distillation)中原始数据与合成数据关系探讨不足的问题，在理论框架下引入了信息量(Informativeness)和效用(Utility)概念，分别用于捕捉样本内关键信息和筛选训练集中核心样本。作者据此提出了InfoUtil框架，通过结合基于博弈论Shapley Value的信息量最大化技术和基于梯度范数(Gradient Norm)的原则性效用最大化策略，确保合成数据集在保留核心特征的同时具备全局影响力。实验结果显示，InfoUtil在ImageNet-1K数据集上使用ResNet-18模型时，较之前的先进方法实现了6.1%的性能提升。该研究为实现信息丰富且效用优化的数据集蒸馏提供了数学定义与实践方案。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "Accepted by ICLR 2026, 20 pages, 9 figures, 11 tables",
      "pdf_url": "https://arxiv.org/pdf/2601.21296v1",
      "published_date": "2026-01-29 05:49:17 UTC",
      "updated_date": "2026-01-29 05:49:17 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-30T04:32:50.909282+00:00"
    },
    {
      "arxiv_id": "2601.21293v1",
      "title": "Physics-Guided Tiny-Mamba Transformer for Reliability-Aware Early Fault Warning",
      "title_zh": "面向可靠性感知早期故障预警的物理引导 Tiny-Mamba Transformer",
      "authors": [
        "Changyu Li",
        "Dingcheng Huang",
        "Kexuan Yao",
        "Xiaoya Ni",
        "Lijuan Shen",
        "Fei Luo"
      ],
      "abstract": "Reliability-centered prognostics for rotating machinery requires early warning signals that remain accurate under nonstationary operating conditions, domain shifts across speed/load/sensors, and severe class imbalance, while keeping the false-alarm rate small and predictable. We propose the Physics-Guided Tiny-Mamba Transformer (PG-TMT), a compact tri-branch encoder tailored for online condition monitoring. A depthwise-separable convolutional stem captures micro-transients, a Tiny-Mamba state-space branch models near-linear long-range dynamics, and a lightweight local Transformer encodes cross-channel resonances. We derive an analytic temporal-to-spectral mapping that ties the model's attention spectrum to classical bearing fault-order bands, yielding a band-alignment score that quantifies physical plausibility and provides physics-grounded explanations. To ensure decision reliability, healthy-score exceedances are modeled with extreme-value theory (EVT), which yields an on-threshold achieving a target false-alarm intensity (events/hour); a dual-threshold hysteresis with a minimum hold time further suppresses chatter. Under a leakage-free streaming protocol with right-censoring of missed detections on CWRU, Paderborn, XJTU-SY, and an industrial pilot, PG-TMT attains higher precision-recall AUC (primary under imbalance), competitive or better ROC AUC, and shorter mean time-to-detect at matched false-alarm intensity, together with strong cross-domain transfer. By coupling physics-aligned representations with EVT-calibrated decision rules, PG-TMT delivers calibrated, interpretable, and deployment-ready early warnings for reliability-centric prognostics and health management.",
      "tldr_zh": "该研究提出了物理引导的轻量化Mamba Transformer (PG-TMT)，这是一个为旋转机械在线状态监测设计的紧凑型三分支编码器，旨在解决非平稳工况和严重类别不平衡下的早期故障预警难题。该框架通过深度可分离卷积捕获微观瞬态，利用 Tiny-Mamba 分支建模长程动力学，并结合轻量级 Transformer 编码跨通道共振。研究引入了时间到频谱的映射机制，将注意力频谱与轴承故障阶次带对齐，从而实现了具有物理基础的可解释性。为了提升决策可靠性，该方案采用极值理论 (EVT) 校准决策阈值，并结合双阈值迟滞机制以抑制误报。在 CWRU、Paderborn 等数据集及工业试点上的实验证明，PG-TMT 在精确率-召回率 AUC 和平均检测时间上均优于基线模型，展现了出色的跨领域迁移能力和实际部署价值。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "Submitted to IEEE Transactions on Reliability",
      "pdf_url": "https://arxiv.org/pdf/2601.21293v1",
      "published_date": "2026-01-29 05:46:12 UTC",
      "updated_date": "2026-01-29 05:46:12 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-30T04:33:02.780966+00:00"
    },
    {
      "arxiv_id": "2601.21288v1",
      "title": "Drive-KD: Multi-Teacher Distillation for VLMs in Autonomous Driving",
      "title_zh": "Drive-KD：面向自动驾驶视觉语言模型的多教师蒸馏",
      "authors": [
        "Weitong Lian",
        "Zecong Tang",
        "Haoran Li",
        "Tianjian Gao",
        "Yifei Wang",
        "Zixu Wang",
        "Lingyi Meng",
        "Tengju Ru",
        "Zhejun Cui",
        "Yichen Zhu",
        "Hangshuo Cao",
        "Qi Kang",
        "Tianxing Chen",
        "Yusen Qin",
        "Kaixuan Wang",
        "Yu Zhang"
      ],
      "abstract": "Autonomous driving is an important and safety-critical task, and recent advances in LLMs/VLMs have opened new possibilities for reasoning and planning in this domain. However, large models demand substantial GPU memory and exhibit high inference latency, while conventional supervised fine-tuning (SFT) often struggles to bridge the capability gaps of small models. To address these limitations, we propose Drive-KD, a framework that decomposes autonomous driving into a \"perception-reasoning-planning\" triad and transfers these capabilities via knowledge distillation. We identify layer-specific attention as the distillation signal to construct capability-specific single-teacher models that outperform baselines. Moreover, we unify these single-teacher settings into a multi-teacher distillation framework and introduce asymmetric gradient projection to mitigate cross-capability gradient conflicts. Extensive evaluations validate the generalization of our method across diverse model families and scales. Experiments show that our distilled InternVL3-1B model, with ~42 times less GPU memory and ~11.4 times higher throughput, achieves better overall performance than the pretrained 78B model from the same family on DriveBench, and surpasses GPT-5.1 on the planning dimension, providing insights toward efficient autonomous driving VLMs.",
      "tldr_zh": "该研究提出了 Drive-KD，这是一个专为自动驾驶领域设计的视觉语言模型 (VLMs) 知识蒸馏 (Knowledge Distillation) 框架，旨在解决大型模型在显存占用和推理延迟方面的挑战。该框架将自动驾驶任务拆解为“感知-推理-规划”三位一体的结构，并利用层特定注意力 (layer-specific attention) 信号来构建能力导向的单导师模型。Drive-KD 进一步将单导师设置统一为多导师蒸馏 (Multi-Teacher Distillation) 框架，并引入非对称梯度投影 (asymmetric gradient projection) 以缓解跨能力间的梯度冲突 (gradient conflicts)。广泛的评估证明了该方法在不同模型家族和规模上的通用性。实验结果显示，蒸馏后的 InternVL3-1B 模型在显存消耗减少约 42 倍、吞吐量提升约 11.4 倍的情况下，在 DriveBench 上的整体性能优于同系列的 78B 预训练模型。此外，该模型在规划维度上甚至超越了 GPT-5.1，为实现高效的自动驾驶 VLMs 提供了重要见解。",
      "categories": [
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.AI",
      "comment": "Preprint. 23 pages, 14 figures",
      "pdf_url": "https://arxiv.org/pdf/2601.21288v1",
      "published_date": "2026-01-29 05:41:24 UTC",
      "updated_date": "2026-01-29 05:41:24 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-30T04:33:11.389070+00:00"
    },
    {
      "arxiv_id": "2601.21285v1",
      "title": "Zenith: Scaling up Ranking Models for Billion-scale Livestreaming Recommendation",
      "title_zh": "Zenith：面向十亿级直播推荐的大规模排序模型扩展",
      "authors": [
        "Ruifeng Zhang",
        "Zexi Huang",
        "Zikai Wang",
        "Ke Sun",
        "Bohang Zheng",
        "Zhen Ouyang",
        "Huimin Xie",
        "Phil Shen",
        "Junlin Zhang",
        "Wentao Guo",
        "Qinglei Wang"
      ],
      "abstract": "Accurately capturing feature interactions is essential in recommender systems, and recent trends show that scaling up model capacity could be a key driver for next-level predictive performance. While prior work has explored various model architectures to capture multi-granularity feature interactions, relatively little attention has been paid to efficient feature handling and scaling model capacity without incurring excessive inference latency. In this paper, we address this by presenting Zenith, a scalable and efficient ranking architecture that learns complex feature interactions with minimal runtime overhead. Zenith is designed to handle a few high-dimensional Prime Tokens with Token Fusion and Token Boost modules, which exhibits superior scaling laws compared to other state-of-the-art ranking methods, thanks to its improved token heterogeneity. Its real-world effectiveness is demonstrated by deploying the architecture to TikTok Live, a leading online livestreaming platform that attracts billions of users globally. Our A/B test shows that Zenith achieves +1.05%/-1.10% in online CTR AUC and Logloss, and realizes +9.93% gains in Quality Watch Session / User and +8.11% in Quality Watch Duration / User.",
      "tldr_zh": "该研究提出了 Zenith，一种专为十亿级直播推荐设计的高效可扩展排序架构，旨在解决推荐系统在扩展模型容量时面临的特征交互复杂性与推理延迟之间的矛盾。Zenith 通过 Token Fusion 和 Token Boost 模块处理少量高维的 Prime Tokens，在保持极低运行开销的同时有效捕捉复杂的特征交互，并凭借改进的 Token Heterogeneity 展现出比现有先进排序方法更优越的 Scaling Laws。该架构已成功部署于 TikTok Live 全球直播平台，线上 A/B 测试结果显示，Zenith 在 CTR AUC 和 Logloss 指标上分别实现了 +1.05% 和 -1.10% 的性能提升。此外，该模型显著增强了用户体验，在 Quality Watch Session / User 和 Quality Watch Duration / User 指标上分别获得了 +9.93% 和 +8.11% 的大幅增长，验证了其在大规模实时推荐场景下的卓越效能。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "9 pages",
      "pdf_url": "https://arxiv.org/pdf/2601.21285v1",
      "published_date": "2026-01-29 05:36:49 UTC",
      "updated_date": "2026-01-29 05:36:49 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-30T04:33:06.604665+00:00"
    },
    {
      "arxiv_id": "2601.21284v1",
      "title": "PILD: Physics-Informed Learning via Diffusion",
      "title_zh": "PILD：基于扩散的物理信息学习",
      "authors": [
        "Tianyi Zeng",
        "Tianyi Wang",
        "Jiaru Zhang",
        "Zimo Zeng",
        "Feiyang Zhang",
        "Yiming Xu",
        "Sikai Chen",
        "Yajie Zou",
        "Yangyang Wang",
        "Junfeng Jiao",
        "Christian Claudel",
        "Xinbo Chen"
      ],
      "abstract": "Diffusion models have emerged as powerful generative tools for modeling complex data distributions, yet their purely data-driven nature limits applicability in practical engineering and scientific problems where physical laws need to be followed. This paper proposes Physics-Informed Learning via Diffusion (PILD), a framework that unifies diffusion modeling and first-principles physical constraints by introducing a virtual residual observation sampled from a Laplace distribution to supervise generation during training. To further integrate physical laws, a conditional embedding module is incorporated to inject physical information into the denoising network at multiple layers, ensuring consistent guidance throughout the diffusion process. The proposed PILD framework is concise, modular, and broadly applicable to problems governed by ordinary differential equations, partial differential equations, as well as algebraic equations or inequality constraints. Extensive experiments across engineering and scientific tasks including estimating vehicle trajectories, tire forces, Darcy flow and plasma dynamics, demonstrate that our PILD substantially improves accuracy, stability, and generalization over existing physics-informed and diffusion-based baselines.",
      "tldr_zh": "该研究提出了PILD（Physics-Informed Learning via Diffusion），这是一个将扩散模型（Diffusion models）与物理第一性原理约束相结合的框架，旨在解决纯数据驱动模型在工程和科学领域难以遵循物理定律的局限性。PILD通过引入从拉普拉斯分布（Laplace distribution）中采样的虚拟残差观测值来监督训练，实现了物理约束与生成建模的统一。为了深度整合物理规律，该框架利用条件嵌入模块（conditional embedding module）在去噪网络的多层中注入物理信息，确保了整个扩散过程的持续引导。PILD具有简洁和模块化的特性，可广泛应用于受常微分方程（ODEs）、偏微分方程（PDEs）及代数方程或不等式约束支配的问题。通过对车辆轨迹、轮胎力、达西流（Darcy flow）和等离子体动力学（plasma dynamics）的多项实验验证，PILD在准确性、稳定性和泛化能力上均显著优于现有的物理信息学习和基于扩散的基线模型。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.ET",
        "math.AP"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2601.21284v1",
      "published_date": "2026-01-29 05:33:51 UTC",
      "updated_date": "2026-01-29 05:33:51 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-30T04:33:11.277054+00:00"
    },
    {
      "arxiv_id": "2601.21283v1",
      "title": "DUET: Distilled LLM Unlearning from an Efficiently Contextualized Teacher",
      "title_zh": "DUET：基于高效上下文语境化教师的大语言模型蒸馏遗忘",
      "authors": [
        "Yisheng Zhong",
        "Zhengbang Yang",
        "Zhuangdi Zhu"
      ],
      "abstract": "LLM unlearning is a technique to remove the impacts of undesirable knowledge from the model without retraining from scratch, which is indispensable towards trustworthy AI. Existing unlearning methods face significant limitations: conventional tuning-based unlearning is computationally heavy and prone to catastrophic forgetting. In contrast, in-contextualized unlearning is lightweight for precise unlearning but vulnerable to prompt removal or reverse engineering attacks. In response, we propose Distilled Unlearning from an Efficient Teacher (DUET), a novel distillation-based unlearning method that combines the merits of these two lines of work. It learns a student model to imitate the behavior of a prompt-steered teacher that effectively refuses undesirable knowledge generation while preserving general domain knowledge. Extensive evaluations on existing benchmarks with our enriched evaluation protocols demonstrate that DUET achieves higher performance in both forgetting and utility preservation, while being orders of magnitude more data-efficient than state-of-the-art unlearning methods.",
      "tldr_zh": "该研究提出了DUET（Distilled Unlearning from an Efficient Teacher），一种新型的基于蒸馏的LLM Unlearning方法，旨在解决传统微调式遗忘（tuning-based unlearning）计算沉重且易发生灾难性遗忘（catastrophic forgetting），以及上下文内遗忘（in-contextualized unlearning）安全性不足的局限。该框架通过知识蒸馏（distillation）技术，引导学生模型模仿一个经由提示定向增强的教师模型（prompt-steered teacher），使其在有效拒绝生成有害或冗余知识的同时，完整保留通用的领域知识（general domain knowledge）。大量实验证明，DUET 在遗忘质量和模型实用性保留（utility preservation）两个关键维度上均优于现有的基准模型。此外，该方法在数据效率方面展现出显著优势，比目前最先进的遗忘技术高出数个数量级。这项工作为构建更可靠、更高效的可信AI（trustworthy AI）提供了重要的方法学支持。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2601.21283v1",
      "published_date": "2026-01-29 05:32:35 UTC",
      "updated_date": "2026-01-29 05:32:35 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-30T04:33:13.220646+00:00"
    },
    {
      "arxiv_id": "2601.21279v1",
      "title": "NEXUS: Bit-Exact ANN-to-SNN Equivalence via Neuromorphic Gate Circuits with Surrogate-Free Training",
      "title_zh": "NEXUS：通过无替代训练的神经形态门电路实现 ANN 与 SNN 的位级精确等效",
      "authors": [
        "Zhengzheng Tang"
      ],
      "abstract": "Spiking Neural Networks (SNNs) promise energy-efficient computing through event-driven sparsity, yet all existing approaches sacrifice accuracy by approximating continuous values with discrete spikes. We propose NEXUS, a framework that achieves bit-exact ANN-to-SNN equivalence -- not approximate, but mathematically identical outputs. Our key insight is constructing all arithmetic operations, both linear and nonlinear, from pure IF neuron logic gates that implement IEEE-754 compliant floating-point arithmetic. Through spatial bit encoding (zero encoding error by construction), hierarchical neuromorphic gate circuits (from basic logic gates to complete transformer layers), and surrogate-free STE training (exact identity mapping rather than heuristic approximation), NEXUS produces outputs identical to standard ANNs up to machine precision. Experiments on models up to LLaMA-2 70B demonstrate identical task accuracy (0.00\\% degradation) with mean ULP error of only 6.19, while achieving 27-168,000$\\times$ energy reduction on neuromorphic hardware. Crucially, spatial bit encoding's single-timestep design renders the framework inherently immune to membrane potential leakage (100\\% accuracy across all decay factors $β\\in[0.1,1.0]$), while tolerating synaptic noise up to $σ=0.2$ with >98\\% gate-level accuracy.",
      "tldr_zh": "该研究提出了 NEXUS 框架，旨在解决脉冲神经网络 (SNNs) 在通过离散脉冲近似连续值时普遍存在的精度损失问题。NEXUS 首次实现了人工神经网络 (ANN) 到 SNN 的位精确 (Bit-Exact) 等效，确保两者在数学上产生完全一致的输出。该框架的核心在于利用纯集成-触发 (IF) 神经元逻辑门构建符合 IEEE-754 标准的浮点算术运算，并通过空间位编码 (Spatial Bit Encoding) 消除编码误差。结合层次化神经形态门电路和无代理 (Surrogate-Free) 的 STE 训练，NEXUS 能够实现精确的恒等映射。在 LLaMA-2 70B 等模型上的实验证明，该方法在保持 0.00% 准确率退化的同时，在神经形态硬件上实现了 27 到 168,000 倍的能效提升。此外，其单时间步设计使框架具备天然的抗膜电位泄漏能力，并展现出对突触噪声的强大容忍度。",
      "categories": [
        "cs.NE",
        "cs.AI"
      ],
      "primary_category": "cs.NE",
      "comment": "7 pages, 6 tables, 2 figures. Preprint (January 28, 2026)",
      "pdf_url": "https://arxiv.org/pdf/2601.21279v1",
      "published_date": "2026-01-29 05:23:56 UTC",
      "updated_date": "2026-01-29 05:23:56 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-30T04:33:16.791621+00:00"
    },
    {
      "arxiv_id": "2601.21278v1",
      "title": "GeoRC: A Benchmark for Geolocation Reasoning Chains",
      "title_zh": "GeoRC：地理定位推理链基准",
      "authors": [
        "Mohit Talreja",
        "Joshua Diao",
        "Jim Thannikary James",
        "Radu Casapu",
        "Tejas Santanam",
        "Ethan Mendes",
        "Alan Ritter",
        "Wei Xu",
        "James Hays"
      ],
      "abstract": "Vision Language Models (VLMs) are good at recognizing the global location of a photograph -- their geolocation prediction accuracy rivals the best human experts. But many VLMs are startlingly bad at explaining which image evidence led to their prediction, even when their location prediction is correct. The reasoning chains produced by VLMs frequently hallucinate scene attributes to support their location prediction (e.g. phantom writing, imagined infrastructure, misidentified flora). In this paper, we introduce the first benchmark for geolocation reasoning chains. We focus on the global location prediction task in the popular GeoGuessr game which draws from Google Street View spanning more than 100 countries. We collaborate with expert GeoGuessr players, including the reigning world champion, to produce 800 ground truth reasoning chains for 500 query scenes. These expert reasoning chains address hundreds of different discriminative visual attributes such as license plate shape, architecture, and soil properties to name just a few. We evaluate LLM-as-a-judge and VLM-as-a-judge strategies for scoring VLM-generated reasoning chains against our expert reasoning chains and find that Qwen 3 LLM-as-a-judge correlates best with human scoring. Our benchmark reveals that while large, closed-source VLMs such as Gemini and GPT 5 rival human experts at prediction locations, they still lag behind human experts when it comes to producing auditable reasoning chains. Open weights VLMs such as Llama and Qwen catastrophically fail on our benchmark -- they perform only slightly better than a baseline in which an LLM hallucinates a reasoning chain with oracle knowledge of the photo location but no visual information at all. We believe the gap between human experts and VLMs on this task points to VLM limitations at extracting fine-grained visual attributes from high resolution images.",
      "tldr_zh": "该研究针对视觉语言模型(VLMs)在地理定位任务中虽具有高准确率但推理过程常出现幻觉的问题，提出了首个地理定位推理链基准测试集 GeoRC。研究团队通过与包括世界冠军在内的专业 GeoGuessr 玩家合作，针对 500 个来自 Google Street View 的场景构建了 800 条包含车牌形状、建筑风格及土壤特性等数百种判别性视觉属性的专家级推理链。研究评估了 LLM-as-a-judge 和 VLM-as-a-judge 策略对模型生成的推理链进行评分的表现，发现 Qwen 3 作为评判模型与人类评分的相关性最高。实验结果显示，尽管 Gemini 和 GPT 5 等闭源 VLMs 在定位预测上能媲美人类专家，但在生成可审计的推理链方面仍明显落后。Llama 和 Qwen 等开源 VLMs 在该基准上的表现极差，仅略优于无视觉输入、仅凭定位结果幻觉生成的基线。该研究揭示了当前 VLMs 在从高分辨率图像中提取细粒度视觉属性方面的局限性，为缩小模型与人类专家在复杂视觉推理领域的差距提供了评估框架。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CL",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2601.21278v1",
      "published_date": "2026-01-29 05:18:40 UTC",
      "updated_date": "2026-01-29 05:18:40 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-30T04:33:20.438992+00:00"
    },
    {
      "arxiv_id": "2601.21276v1",
      "title": "More Code, Less Reuse: Investigating Code Quality and Reviewer Sentiment towards AI-generated Pull Requests",
      "title_zh": "代码激增与重用匮乏：AI 生成合并请求的代码质量及其审阅者情绪研究",
      "authors": [
        "Haoming Huang",
        "Pongchai Jaisri",
        "Shota Shimizu",
        "Lingfeng Chen",
        "Sota Nakashima",
        "Gema Rodríguez-Pérez"
      ],
      "abstract": "Large Language Model (LLM) Agents are advancing quickly, with the increasing leveraging of LLM Agents to assist in development tasks such as code generation. While LLM Agents accelerate code generation, studies indicate they may introduce adverse effects on development. However, existing metrics solely measure pass rates, failing to reflect impacts on long-term maintainability and readability, and failing to capture human intuitive evaluations of PR. To increase the comprehensiveness of this problem, we investigate and evaluate the characteristics of LLM to know the pull requests' characteristics beyond the pass rate. We observe the code quality and maintainability within PRs based on code metrics to evaluate objective characteristics and developers' reactions to the pull requests from both humans and LLM's generation. Evaluation results indicate that LLM Agents frequently disregard code reuse opportunities, resulting in higher levels of redundancy compared to human developers. In contrast to the quality issues, our emotions analysis reveals that reviewers tend to express more neutral or positive emotions towards AI-generated contributions than human ones. This disconnect suggests that the surface-level plausibility of AI code masks redundancy, leading to the silent accumulation of technical debt in real-world development environments. Our research provides insights for improving human-AI collaboration.",
      "tldr_zh": "该研究探讨了大型语言模型智能体(LLM Agents)生成的拉取请求(Pull Requests)对代码质量、可维护性以及代码审查者(Reviewer)情感的影响。研究者通过代码指标评估了代码的客观特性，并分析了开发者对人类与 AI 生成贡献的情感反应，旨在弥补现有评估指标仅关注通过率(Pass Rate)而忽视长期可维护性的局限。实验结果显示，LLM Agents 经常忽略代码复用(Code Reuse)机会，导致其生成的代码冗余度显著高于人类开发者。然而，情感分析表明，审查者对 AI 生成的贡献往往比对人类生成的贡献表现出更多中性或积极的情绪。这种脱节表明 AI 生成代码表面上的合理性掩盖了其冗余本质，从而导致实际开发环境中技术债(Technical Debt)的静默积累。该研究为深入理解人机协作(Human-AI Collaboration)并提升软件工程质量提供了重要见解。",
      "categories": [
        "cs.SE",
        "cs.AI",
        "cs.HC"
      ],
      "primary_category": "cs.SE",
      "comment": "Accepted to MSR 2026",
      "pdf_url": "https://arxiv.org/pdf/2601.21276v1",
      "published_date": "2026-01-29 05:13:21 UTC",
      "updated_date": "2026-01-29 05:13:21 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-30T04:33:29.406044+00:00"
    },
    {
      "arxiv_id": "2601.21269v1",
      "title": "Lightweight High-Fidelity Low-Bitrate Talking Face Compression for 3D Video Conference",
      "title_zh": "面向 3D 视频会议的轻量级高保真低码率说话人脸压缩",
      "authors": [
        "Jianglong Li",
        "Jun Xu",
        "Bingcong Lu",
        "Zhengxue Cheng",
        "Hongwei Hu",
        "Ronghua Wu",
        "Li Song"
      ],
      "abstract": "The demand for immersive and interactive communication has driven advancements in 3D video conferencing, yet achieving high-fidelity 3D talking face representation at low bitrates remains a challenge. Traditional 2D video compression techniques fail to preserve fine-grained geometric and appearance details, while implicit neural rendering methods like NeRF suffer from prohibitive computational costs. To address these challenges, we propose a lightweight, high-fidelity, low-bitrate 3D talking face compression framework that integrates FLAME-based parametric modeling with 3DGS neural rendering. Our approach transmits only essential facial metadata in real time, enabling efficient reconstruction with a Gaussian-based head model. Additionally, we introduce a compact representation and compression scheme, including Gaussian attribute compression and MLP optimization, to enhance transmission efficiency. Experimental results demonstrate that our method achieves superior rate-distortion performance, delivering high-quality facial rendering at extremely low bitrates, making it well-suited for real-time 3D video conferencing applications.",
      "tldr_zh": "该研究针对3D视频会议中高保真、低比特率的人脸压缩难题，提出了一种结合FLAME参数化模型与3DGS神经渲染的轻量化框架。为了解决传统2D压缩细节丢失及NeRF计算开销大的问题，该方法通过仅传输关键的人脸元数据实现实时通信，并利用基于高斯(Gaussian)的头像模型进行高效重建。此外，研究还引入了紧凑的表示和压缩方案，包括高斯属性压缩(Gaussian attribute compression)和MLP优化，以进一步提升传输效率。实验结果表明，该方法在极低比特率下实现了卓越的率失真性能(rate-distortion performance)和高质量的面部渲染，为实时3D视频会议应用提供了有效的解决方案。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2601.21269v1",
      "published_date": "2026-01-29 05:03:29 UTC",
      "updated_date": "2026-01-29 05:03:29 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-30T04:33:25.282958+00:00"
    },
    {
      "arxiv_id": "2601.21260v1",
      "title": "Music Plagiarism Detection: Problem Formulation and a Segment-based Solution",
      "title_zh": "音乐抄袭检测：问题定义与基于片段的解决方案",
      "authors": [
        "Seonghyeon Go",
        "Yumin Kim"
      ],
      "abstract": "Recently, the problem of music plagiarism has emerged as an even more pressing social issue. As music information retrieval research advances, there is a growing effort to address issues related to music plagiarism. However, many studies, including our previous work, have conducted research without clearly defining what the music plagiarism detection task actually involves. This lack of a clear definition has slowed research progress and made it hard to apply results to real-world scenarios. To fix this situation, we defined how Music Plagiarism Detection is different from other MIR tasks and explained what problems need to be solved. We introduce the Similar Music Pair dataset to support this newly defined task. In addition, we propose a method based on segment transcription as one way to solve the task. Our demo and dataset are available at https://github.com/Mippia/ICASSP2026-MPD.",
      "tldr_zh": "该研究针对音乐剽窃检测(Music Plagiarism Detection)日益严重的社会影响，指出了当前领域缺乏明确任务定义对研究进度和实际应用的阻碍。为了扭转这一局面，作者深入阐述了Music Plagiarism Detection与其它音乐信息检索(MIR)任务的区别，并系统性地定义了该任务所需解决的关键问题。研究团队推出了Similar Music Pair数据集以支持这一新定义的学术任务，并提出了一种基于片段转录(segment transcription)的方法作为解决方案。该工作通过明确任务边界、提供基准数据集及探索性算法，为解决音乐版权领域的复杂挑战提供了重要参考和工具支持。",
      "categories": [
        "cs.SD",
        "cs.AI",
        "cs.LG",
        "eess.AS"
      ],
      "primary_category": "cs.SD",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2601.21260v1",
      "published_date": "2026-01-29 04:41:59 UTC",
      "updated_date": "2026-01-29 04:41:59 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-30T04:33:24.238229+00:00"
    },
    {
      "arxiv_id": "2601.21255v1",
      "title": "Hypersolid: Emergent Vision Representations via Short-Range Repulsion",
      "title_zh": "Hypersolid：通过短程排斥实现的视觉表示涌现",
      "authors": [
        "Esteban Rodríguez-Betancourt",
        "Edgar Casasola-Murillo"
      ],
      "abstract": "A recurring challenge in self-supervised learning is preventing representation collapse. Existing solutions typically rely on global regularization, such as maximizing distances, decorrelating dimensions or enforcing certain distributions. We instead reinterpret representation learning as a discrete packing problem, where preserving information simplifies to maintaining injectivity. We operationalize this in Hypersolid, a method using short-range hard-ball repulsion to prevent local collisions. This constraint results in a high-separation geometric regime that preserves augmentation diversity, excelling on fine-grained and low-resolution classification tasks.",
      "tldr_zh": "该研究探讨了自监督学习(Self-supervised learning)中预防表征坍缩(representation collapse)的挑战。不同于依赖全局正则化(global regularization)的现有方案，本研究提出了Hypersolid，将表征学习重新解释为旨在维持单射性(injectivity)以保留信息的离散填充问题(discrete packing problem)。Hypersolid采用短程硬球排斥(short-range hard-ball repulsion)机制来防止表征空间的局部碰撞。这种约束产生了一种高分离度的几何状态，能够有效保留增强多样性(augmentation diversity)。研究结果表明，Hypersolid在细粒度(fine-grained)和低分辨率(low-resolution)分类任务上表现优异，为构建新兴视觉表征提供了新路径。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "17 pages, 16 figures",
      "pdf_url": "https://arxiv.org/pdf/2601.21255v1",
      "published_date": "2026-01-29 04:25:43 UTC",
      "updated_date": "2026-01-29 04:25:43 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-30T04:33:42.468780+00:00"
    },
    {
      "arxiv_id": "2601.21249v1",
      "title": "Position: Certifiable State Integrity in Cyber-Physical Systems -- Why Modular Sovereignty Solves the Plasticity-Stability Paradox",
      "title_zh": "信息物理系统中的可证明状态完整性：模块化主权如何化解“塑性-稳定性”悖论",
      "authors": [
        "Enzo Nicolás Spotorno",
        "Antônio Augusto Medeiros Fröhlich"
      ],
      "abstract": "The machine learning community has achieved remarkable success with universal foundation models for time-series and physical dynamics, largely overcoming earlier approximation barriers in smooth or slowly varying regimes through scale and specialized architectures. However, deploying these monolithic models in safety-critical Cyber-Physical Systems (CPS), governed by non-stationary lifecycle dynamics and strict reliability requirements, reveals persistent challenges. Recent evidence shows that fine-tuning time-series foundation models induces catastrophic forgetting, degrading performance on prior regimes. Standard models continue to exhibit residual spectral bias, smoothing high-frequency discontinuities characteristic of incipient faults, while their opacity hinders formal verification and traceability demanded by safety standards (e.g., ISO 26262, IEC 61508). This position paper argues that the plasticity-stability paradox cannot be fully resolved by global parameter updates (whether via offline fine-tuning or online adaptation). Instead, we advocate a Modular Sovereignty paradigm: a library of compact, frozen regime-specific specialists combined via uncertainty-aware blending, which we term \"HYDRA\" (Hierarchical uncertaintY-aware Dynamics for Rapidly-Adapting systems). This paradigm ensures regime-conditional validity, rigorous disentanglement of aleatoric and epistemic uncertainties, and modular auditability, offering a certifiable path for robust state integrity across the CPS lifecycle.",
      "tldr_zh": "该研究探讨了通用基础模型 (universal foundation models) 在安全关键型信息物理系统 (Cyber-Physical Systems, CPS) 中面临的严峻挑战，指出大规模单体模型在处理非平稳动态时难以满足严格的可靠性要求。论文揭示了微调 (fine-tuning) 过程中的灾难性遗忘 (catastrophic forgetting) 以及模型固有的谱偏置 (spectral bias) 如何阻碍对高频故障的识别，并强调了模型不透明性对形式化验证的负面影响。作者认为全局参数更新无法解决可塑性与稳定性悖论 (plasticity-stability paradox)，进而提出了一种名为“模块化主权” (Modular Sovereignty) 的新范式。通过 HYDRA (Hierarchical uncertaintY-aware Dynamics for Rapidly-Adapting systems) 框架，该方法利用不确定性感知融合 (uncertainty-aware blending) 组合多个冻结的特定状态专家模型，实现了偶然不确定性 (aleatoric uncertainty) 与认知不确定性 (epistemic uncertainty) 的解耦。这种架构确保了状态条件的有效性与模块化可审计性，为实现 CPS 生命周期内的可认证状态完整性 (state integrity) 提供了鲁棒的技术路径。",
      "categories": [
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "14 pages, (8 main text, 6 references and appendices), 2 figures",
      "pdf_url": "https://arxiv.org/pdf/2601.21249v1",
      "published_date": "2026-01-29 04:10:58 UTC",
      "updated_date": "2026-01-29 04:10:58 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-30T04:33:43.228900+00:00"
    },
    {
      "arxiv_id": "2601.21246v1",
      "title": "Conditional Generative Framework with Peak-Aware Attention for Robust Chemical Detection under Interferences",
      "title_zh": "面向干扰环境下稳健化学检测的峰值感知注意力条件生成框架",
      "authors": [
        "Namkyung Yoon",
        "Sanghong Kim",
        "Hwangnam Kim"
      ],
      "abstract": "Gas chromatography-mass spectrometry (GC-MS) is a widely used analytical method for chemical substance detection, but measurement reliability tends to deteriorate in the presence of interfering substances. In particular, interfering substances cause nonspecific peaks, residence time shifts, and increased background noise, resulting in reduced sensitivity and false alarms. To overcome these challenges, in this paper, we propose an artificial intelligence discrimination framework based on a peak-aware conditional generative model to improve the reliability of GC-MS measurements under interference conditions. The framework is learned with a novel peak-aware mechanism that highlights the characteristic peaks of GC-MS data, allowing it to generate important spectral features more faithfully. In addition, chemical and solvent information is encoded in a latent vector embedded with it, allowing a conditional generative adversarial neural network (CGAN) to generate a synthetic GC-MS signal consistent with the experimental conditions. This generates an experimental dataset that assumes indirect substance situations in chemical substance data, where acquisition is limited without conducting real experiments. These data are used for the learning of AI-based GC-MS discrimination models to help in accurate chemical substance discrimination. We conduct various quantitative and qualitative evaluations of the generated simulated data to verify the validity of the proposed framework. We also verify how the generative model improves the performance of the AI discrimination framework. Representatively, the proposed method is shown to consistently achieve cosine similarity and Pearson correlation coefficient values above 0.9 while preserving peak number diversity and reducing false alarms in the discrimination model.",
      "tldr_zh": "该研究针对气相色谱-质谱法 (Gas chromatography-mass spectrometry, GC-MS) 在干扰物质影响下由于非特异性峰、保留时间偏移和背景噪声导致检测可靠性下降的问题，提出了一种基于峰感知条件生成模型 (peak-aware conditional generative model) 的人工智能辨识框架。该框架引入了一种新颖的峰感知机制 (peak-aware mechanism) 以突出 GC-MS 数据的特征峰，从而更忠实地生成关键光谱特征。通过将化学物质和溶剂信息编码进潜在向量，该框架利用条件生成对抗网络 (CGAN) 产生与实验条件一致的合成信号，有效解决了实际实验数据获取受限的难题。这些合成数据被用于训练 AI 辨识模型，显著提升了化学物质辨识的准确性。定量评估显示，该方法生成的模拟数据在余弦相似度 (cosine similarity) 和皮尔逊相关系数 (Pearson correlation coefficient) 上均保持在 0.9 以上。实验证明，该框架在保留峰数量多样性的同时有效减少了辨识模型中的误报 (false alarms)，为复杂环境下的鲁棒化学检测提供了有效方案。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "24 pages, 5 figures",
      "pdf_url": "https://arxiv.org/pdf/2601.21246v1",
      "published_date": "2026-01-29 04:10:37 UTC",
      "updated_date": "2026-01-29 04:10:37 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-30T04:33:46.041208+00:00"
    },
    {
      "arxiv_id": "2601.21244v1",
      "title": "Less Noise, More Voice: Reinforcement Learning for Reasoning via Instruction Purification",
      "title_zh": "去噪留真：基于指令净化的推理强化学习",
      "authors": [
        "Yiju Guo",
        "Tianyi Hu",
        "Zexu Sun",
        "Yankai Lin"
      ],
      "abstract": "Reinforcement Learning with Verifiable Rewards (RLVR) has advanced LLM reasoning, but remains constrained by inefficient exploration under limited rollout budgets, leading to low sampling success and unstable training in complex tasks. We find that many exploration failures arise not from problem difficulty, but from a small number of prompt tokens that introduce interference. Building on this insight, we propose the Less Noise Sampling Framework (LENS), which first prompts by identifying and removing interference tokens. then transfers successful rollouts from the purification process to supervise policy optimization on the original noisy prompts, enabling the model to learn to ignore interference in the real-world, noisy prompting settings. Experimental results show that LENS significantly outperforms GRPO, delivering higher performance and faster convergence, with a 3.88% average gain and over 1.6$\\times$ speedup. Our work highlights the critical role of pruning interference tokens in improving rollout efficiency, offering a new perspective for RLVR research.",
      "tldr_zh": "该研究针对可验证奖励强化学习 (Reinforcement Learning with Verifiable Rewards, RLVR) 在提升大语言模型推理能力时面临的探索效率低下和训练不稳定等挑战，指出提示词中的少量干扰 Token (interference tokens) 是导致探索失败的关键因素。为此，作者提出了 LENS (Less Noise Sampling Framework) 框架，通过识别并移除这些干扰 Token 来实现指令净化 (Instruction Purification)。该框架利用净化后的成功采样路径来监督原始噪声提示下的策略优化，从而使模型学会忽略真实场景中的干扰信息。实验结果表明，LENS 在性能和收敛速度上均显著优于 GRPO，实现了 3.88% 的平均性能增益和超过 1.6 倍的收敛加速。该工作强调了修剪干扰 Token 在提高采样效率中的关键作用，为强化学习推理研究提供了全新的视角。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.LG",
      "comment": "Work in progress",
      "pdf_url": "https://arxiv.org/pdf/2601.21244v1",
      "published_date": "2026-01-29 04:08:24 UTC",
      "updated_date": "2026-01-29 04:08:24 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-30T04:33:56.788247+00:00"
    },
    {
      "arxiv_id": "2601.21242v1",
      "title": "Understanding Diffusion Models via Ratio-Based Function Approximation with SignReLU Networks",
      "title_zh": "通过基于 SignReLU 网络的比率函数逼近理解扩散模型",
      "authors": [
        "Luwei Sun",
        "Dongrui Shen",
        "Jianfe Li",
        "Yulong Zhao",
        "Han Feng"
      ],
      "abstract": "Motivated by challenges in conditional generative modeling, where the target conditional density takes the form of a ratio f1 over f2, this paper develops a theoretical framework for approximating such ratio-type functionals. Here, f1 and f2 are kernel-based marginal densities that capture structured interactions, a setting central to diffusion-based generative models. We provide a concise proof for approximating these ratio-type functionals using deep neural networks with the SignReLU activation function, leveraging the activation's piecewise structure. Under standard regularity assumptions, we establish L^p(Omega) approximation bounds and convergence rates. Specializing to Denoising Diffusion Probabilistic Models (DDPMs), we construct a SignReLU-based neural estimator for the reverse process and derive bounds on the excess Kullback-Leibler (KL) risk between the generated and true data distributions. Our analysis decomposes this excess risk into approximation and estimation error components. These results provide generalization guarantees for finite-sample training of diffusion-based generative models.",
      "tldr_zh": "该研究针对条件生成模型中目标密度表现为比率形式的挑战，提出了一个通过 SignReLU 网络进行比率函数近似(Ratio-Based Function Approximation)的理论框架。研究利用 SignReLU 激活函数的分段结构，证明了深度神经网络对比率型泛函的近似能力，并建立了 $L^p(\\Omega)$ 近似界限与收敛速度。通过将该理论应用于去噪扩散概率模型(DDPMs)，研究者构建了基于 SignReLU 的神经估计器来模拟逆向过程，并推导了生成分布与真实分布之间的超额 KL 风险(excess KL risk)上界。该分析将超额风险分解为近似误差和估计误差组件，从而为扩散生成模型在有限样本训练下的泛化性能提供了理论保证。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "34 pages",
      "pdf_url": "https://arxiv.org/pdf/2601.21242v1",
      "published_date": "2026-01-29 04:01:54 UTC",
      "updated_date": "2026-01-29 04:01:54 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-30T04:33:51.167098+00:00"
    },
    {
      "arxiv_id": "2601.21239v1",
      "title": "TIDE: Tuning-Integrated Dynamic Evolution for LLM-Based Automated Heuristic Design",
      "title_zh": "TIDE：面向大语言模型自动化启发式设计的融合微调动态演化",
      "authors": [
        "Chentong Chen",
        "Mengyuan Zhong",
        "Ye Fan",
        "Jialong Shi",
        "Jianyong Sun"
      ],
      "abstract": "Although Large Language Models have advanced Automated Heuristic Design, treating algorithm evolution as a monolithic text generation task overlooks the coupling between discrete algorithmic structures and continuous numerical parameters. Consequently, existing methods often discard promising algorithms due to uncalibrated constants and suffer from premature convergence resulting from simple similarity metrics. To address these limitations, we propose TIDE, a Tuning-Integrated Dynamic Evolution framework designed to decouple structural reasoning from parameter optimization. TIDE features a nested architecture where an outer parallel island model utilizes Tree Similarity Edit Distance to drive structural diversity, while an inner loop integrates LLM-based logic generation with a differential mutation operator for parameter tuning. Additionally, a UCB-based scheduler dynamically prioritizes high-yield prompt strategies to optimize resource allocation. Extensive experiments across nine combinatorial optimization problems demonstrate that TIDE discovers heuristics that significantly outperform state-of-the-art baselines in solution quality while achieving improved search efficiency and reduced computational costs.",
      "tldr_zh": "该研究提出了 TIDE，一种集成调优的动态演化框架，旨在解决现有基于大语言模型（LLM）的自动化启发式设计（Automated Heuristic Design）中，因算法结构与数值参数耦合导致的潜力算法被误弃及过早收敛问题。TIDE 采用嵌套架构将结构推理与参数优化解耦，外层通过并行岛模型（Parallel Island Model）和树编辑距离（Tree Similarity Edit Distance）驱动结构多样性，内层则结合逻辑生成与差分变异算子（Differential Mutation Operator）进行精细化参数调优。此外，该框架引入了基于 UCB 的调度器来动态优先处理高收益的提示策略，从而优化资源分配。在九个组合优化问题上的实验结果证明，TIDE 发现的启发式算法在解的质量、搜索效率及计算成本上均显著优于目前的先进基准方法。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2601.21239v1",
      "published_date": "2026-01-29 04:00:02 UTC",
      "updated_date": "2026-01-29 04:00:02 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-30T04:33:57.289964+00:00"
    },
    {
      "arxiv_id": "2601.21238v1",
      "title": "PTQ4ARVG: Post-Training Quantization for AutoRegressive Visual Generation Models",
      "title_zh": "PTQ4ARVG：自回归视觉生成模型的训练后量化",
      "authors": [
        "Xuewen Liu",
        "Zhikai Li",
        "Jing Zhang",
        "Mengjuan Chen",
        "Qingyi Gu"
      ],
      "abstract": "AutoRegressive Visual Generation (ARVG) models retain an architecture compatible with language models, while achieving performance comparable to diffusion-based models. Quantization is commonly employed in neural networks to reduce model size and computational latency. However, applying quantization to ARVG remains largely underexplored, and existing quantization methods fail to generalize effectively to ARVG models. In this paper, we explore this issue and identify three key challenges: (1) severe outliers at channel-wise level, (2) highly dynamic activations at token-wise level, and (3) mismatched distribution information at sample-wise level. To these ends, we propose PTQ4ARVG, a training-free post-training quantization (PTQ) framework consisting of: (1) Gain-Projected Scaling (GPS) mitigates the channel-wise outliers, which expands the quantization loss via a Taylor series to quantify the gain of scaling for activation-weight quantization, and derives the optimal scaling factor through differentiation.(2) Static Token-Wise Quantization (STWQ) leverages the inherent properties of ARVG, fixed token length and position-invariant distribution across samples, to address token-wise variance without incurring dynamic calibration overhead.(3) Distribution-Guided Calibration (DGC) selects samples that contribute most to distributional entropy, eliminating the sample-wise distribution mismatch. Extensive experiments show that PTQ4ARVG can effectively quantize the ARVG family models to 8-bit and 6-bit while maintaining competitive performance. Code is available at http://github.com/BienLuky/PTQ4ARVG .",
      "tldr_zh": "该研究针对自回归视觉生成模型(AutoRegressive Visual Generation, ARVG)在量化过程中面临的通道异常值、动态激活和分布不匹配等挑战，提出了名为PTQ4ARVG的免训练后量化(Post-Training Quantization)框架。该框架引入了增益投影缩放(Gain-Projected Scaling, GPS)机制，通过泰勒级数展开量化损失并求得最优缩放因子，从而有效缓解通道维度的离群值干扰。针对标记维度的方差问题，静态标记维度量化(Static Token-Wise Quantization, STWQ)利用ARVG模型特定的位置分布特性，实现了无动态开销的校准。此外，分布引导校准(Distribution-Guided Calibration, DGC)通过选取高分布熵样本，消除了样本层面的分布失衡。实验结果表明，PTQ4ARVG能将ARVG模型成功量化至8-bit和6-bit级别，并在保持模型生成质量的同时显著降低存储和计算开销。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "ICLR 2026",
      "pdf_url": "https://arxiv.org/pdf/2601.21238v1",
      "published_date": "2026-01-29 04:00:00 UTC",
      "updated_date": "2026-01-29 04:00:00 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-30T04:33:55.824886+00:00"
    },
    {
      "arxiv_id": "2601.21235v1",
      "title": "SHARP: Social Harm Analysis via Risk Profiles for Measuring Inequities in Large Language Models",
      "title_zh": "SHARP：基于风险画像的社会损害分析：衡量大语言模型中的不平等性",
      "authors": [
        "Alok Abhishek",
        "Tushar Bandopadhyay",
        "Lisa Erickson"
      ],
      "abstract": "Large language models (LLMs) are increasingly deployed in high-stakes domains, where rare but severe failures can result in irreversible harm. However, prevailing evaluation benchmarks often reduce complex social risk to mean-centered scalar scores, thereby obscuring distributional structure, cross-dimensional interactions, and worst-case behavior. This paper introduces Social Harm Analysis via Risk Profiles (SHARP), a framework for multidimensional, distribution-aware evaluation of social harm. SHARP models harm as a multivariate random variable and integrates explicit decomposition into bias, fairness, ethics, and epistemic reliability with a union-of-failures aggregation reparameterized as additive cumulative log-risk. The framework further employs risk-sensitive distributional statistics, with Conditional Value at Risk (CVaR95) as a primary metric, to characterize worst-case model behavior. Application of SHARP to eleven frontier LLMs, evaluated on a fixed corpus of n=901 socially sensitive prompts, reveals that models with similar average risk can exhibit more than twofold differences in tail exposure and volatility. Across models, dimension-wise marginal tail behavior varies systematically across harm dimensions, with bias exhibiting the strongest tail severities, epistemic and fairness risks occupying intermediate regimes, and ethical misalignment consistently lower; together, these patterns reveal heterogeneous, model-dependent failure structures that scalar benchmarks conflate. These findings indicate that responsible evaluation and governance of LLMs require moving beyond scalar averages toward multidimensional, tail-sensitive risk profiling.",
      "tldr_zh": "该研究提出了Social Harm Analysis via Risk Profiles (SHARP)框架，旨在解决现有大语言模型(LLMs)评估基准过度依赖标量平均分而忽略分布结构与最坏情况行为的问题。SHARP将社会伤害建模为多元随机变量，并将其显式分解为偏见(bias)、公平性(fairness)、伦理(ethics)和认识可靠性(epistemic reliability)四个维度。该框架引入条件风险价值(CVaR95)作为核心指标，利用风险敏感的分布统计技术来精确刻画模型的极端风险暴露。对11个前沿LLMs的评估结果显示，即使平均风险相近的模型，其尾部暴露(tail exposure)和波动性也可能存在两倍以上的差异。实验发现偏见维度的尾部严重性在各模型中最为突出，而传统标量基准往往会掩盖这些异质化且模型相关的失效结构。这些发现表明，为了实现负责任的LLMs治理，评估体系必须从简单的平均值转向多维度的尾部风险剖析。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "Pre Print, 29 pages. key words: Social harm evaluation in LLMs, Large language models, Risk sensitive model selection, Evaluation for high-stakes domains, Worst-case behavior in LLMs, Algorithmic bias, Fairness in machine learning",
      "pdf_url": "https://arxiv.org/pdf/2601.21235v1",
      "published_date": "2026-01-29 03:54:25 UTC",
      "updated_date": "2026-01-29 03:54:25 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-30T04:33:58.426828+00:00"
    },
    {
      "arxiv_id": "2601.21233v1",
      "title": "Just Ask: Curious Code Agents Reveal System Prompts in Frontier LLMs",
      "title_zh": "Just Ask：好奇型代码智能体揭示前沿大语言模型系统提示词",
      "authors": [
        "Xiang Zheng",
        "Yutao Wu",
        "Hanxun Huang",
        "Yige Li",
        "Xingjun Ma",
        "Bo Li",
        "Yu-Gang Jiang",
        "Cong Wang"
      ],
      "abstract": "Autonomous code agents built on large language models are reshaping software and AI development through tool use, long-horizon reasoning, and self-directed interaction. However, this autonomy introduces a previously unrecognized security risk: agentic interaction fundamentally expands the LLM attack surface, enabling systematic probing and recovery of hidden system prompts that guide model behavior. We identify system prompt extraction as an emergent vulnerability intrinsic to code agents and present \\textbf{\\textsc{JustAsk}}, a self-evolving framework that autonomously discovers effective extraction strategies through interaction alone. Unlike prior prompt-engineering or dataset-based attacks, \\textsc{JustAsk} requires no handcrafted prompts, labeled supervision, or privileged access beyond standard user interaction. It formulates extraction as an online exploration problem, using Upper Confidence Bound--based strategy selection and a hierarchical skill space spanning atomic probes and high-level orchestration. These skills exploit imperfect system-instruction generalization and inherent tensions between helpfulness and safety. Evaluated on \\textbf{41} black-box commercial models across multiple providers, \\textsc{JustAsk} consistently achieves full or near-complete system prompt recovery, revealing recurring design- and architecture-level vulnerabilities. Our results expose system prompts as a critical yet largely unprotected attack surface in modern agent systems.",
      "tldr_zh": "该研究探讨了大语言模型(LLMs)代码智能体在自主交互中引入的全新安全风险，即其主动性扩展了攻击面，使得隐藏的系统提示词(System Prompts)面临被系统性探测和恢复的威胁。为此，作者提出了JustAsk，这是一个通过交互自主发现有效提取策略的自演化框架。与传统的提示词工程攻击不同，JustAsk无需手工提示词或标注监督，而是将提取过程建模为在线探索问题，采用基于置信区间上界(Upper Confidence Bound)的策略选择和层次化技能空间进行探测。这些技能有效利用了模型在处理系统指令时的泛化缺陷，以及有用性(Helpfulness)与安全性(Safety)之间的内在矛盾。在对41个黑盒商业模型进行的测试中，JustAsk一致实现了完整或近乎完整的系统提示词恢复，揭示了设计与架构层面普遍存在的安全漏洞。该研究结果表明系统提示词已成为现代智能体系统中一个关键且缺乏保护的攻击面。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "24 pages, 6 figures, 17 tables",
      "pdf_url": "https://arxiv.org/pdf/2601.21233v1",
      "published_date": "2026-01-29 03:53:25 UTC",
      "updated_date": "2026-01-29 03:53:25 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-30T04:34:07.689700+00:00"
    },
    {
      "arxiv_id": "2601.21226v1",
      "title": "Delegation Without Living Governance",
      "title_zh": "无活态治理的授权",
      "authors": [
        "Wolfgang Rohde"
      ],
      "abstract": "Most governance frameworks assume that rules can be defined in advance, systems can be engineered to comply, and accountability can be applied after outcomes occur. This model worked when machines replaced physical labor or accelerated calculation. It no longer holds when judgment itself is delegated to agentic AI systems operating at machine speed. The central issue here is not safety, efficiency, or employment. It is whether humans remain relevant participants in systems that increasingly shape social, economic, and political outcomes. This paper argues that static, compliance-based governance fails once decision-making moves to runtime and becomes opaque. It further argues that the core challenge is not whether AI is conscious, but whether humans can maintain meaningful communication, influence, and co-evolution with increasingly alien forms of intelligence. We position runtime governance, specifically, a newly proposed concept called the Governance Twin [1]; as a strong candidate for preserving human relevance, while acknowledging that accountability, agency, and even punishment must be rethought in this transition.",
      "tldr_zh": "本研究探讨了在智能体 AI (Agentic AI) 接管判断与决策权的背景下，传统治理框架面临失效的问题。论文指出，当决策过程移至运行阶段 (Runtime) 并变得不透明时，基于预设规则和事后问责的静态治理模型已无法应对机器速度下的系统挑战。核心问题不在于安全性或效率，而在于人类是否能在塑造社会、经济和政治结果的系统中，与日益异化的智能形式保持有意义的沟通、影响力和相关性。为此，作者提出了运行阶段治理的新概念——治理孪生 (Governance Twin)，作为确保人类在自治系统中发挥持续作用的关键路径。该研究强调，在这一转型过程中，必须对问责机制 (Accountability)、主体性 (Agency) 甚至惩罚机制进行重新审视和重构。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2601.21226v1",
      "published_date": "2026-01-29 03:40:40 UTC",
      "updated_date": "2026-01-29 03:40:40 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-30T04:34:17.075958+00:00"
    },
    {
      "arxiv_id": "2601.21225v1",
      "title": "MGSM-Pro: A Simple Strategy for Robust Multilingual Mathematical Reasoning Evaluation",
      "title_zh": "MGSM-Pro：面向鲁棒多语言数学推理评估的简单策略",
      "authors": [
        "Tianyi Xu",
        "Kosei Uemura",
        "Alfred Malengo Kondoro",
        "Tadesse Destaw Belay",
        "Catherine Nana Nyaah Essuman",
        "Ifeoma Okoh",
        "Ganiyat Afolabi",
        "Ayodele Awokoya",
        "David Ifeoluwa Adelani"
      ],
      "abstract": "Large language models have made substantial progress in mathematical reasoning. However, benchmark development for multilingual evaluation has lagged behind English in both difficulty and recency. Recently, GSM-Symbolic showed a strong evidence of high variance when models are evaluated on different instantiations of the same question; however, the evaluation was conducted only in English. In this paper, we introduce MGSM-Pro, an extension of MGSM dataset with GSM-Symbolic approach. Our dataset provides five instantiations per MGSM question by varying names, digits and irrelevant context. Evaluations across nine languages reveal that many low-resource languages suffer large performance drops when tested on digit instantiations different from those in the original test set. We further find that some proprietary models, notably Gemini 2.5 Flash and GPT-4.1, are less robust to digit instantiation, whereas Claude 4.0 Sonnet is more robust. Among open models, GPT-OSS 120B and DeepSeek V3 show stronger robustness. Based on these findings, we recommend evaluating each problem using at least five digit-varying instantiations to obtain a more robust and realistic assessment of math reasoning.",
      "tldr_zh": "该研究提出了 MGSM-Pro，这是利用 GSM-Symbolic 方法对 MGSM 数据集进行扩展而成的多语言评估基准，旨在解决现有数学推理评估在非英语语种中难度和稳健性不足的问题。该数据集通过改变姓名、数字和无关上下文，为每个原始问题提供了五个不同的变体实例，并在九种语言上对模型进行了深入评估。研究发现，许多低资源语言在面对数字变体时表现出显著的性能波动，揭示了当前模型在多语言数学推理中的不稳定性。通过对比发现，Gemini 2.5 Flash 和 GPT-4.1 等模型在数字鲁棒性方面表现较弱，而 Claude 4.0 Sonnet 以及开源模型 GPT-OSS 120B 和 DeepSeek V3 则展现出更强的稳健性。基于实验结果，研究者建议在评估数学推理能力时，应针对每个题目至少采用五个数字变体实例，以获得更真实、稳健的评估结论。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2601.21225v1",
      "published_date": "2026-01-29 03:40:28 UTC",
      "updated_date": "2026-01-29 03:40:28 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-30T04:34:36.614860+00:00"
    },
    {
      "arxiv_id": "2601.21221v1",
      "title": "Causal Discovery for Explainable AI: A Dual-Encoding Approach",
      "title_zh": "面向可解释 AI 的因果发现：一种双重编码方法",
      "authors": [
        "Henry Salgado",
        "Meagan R. Kendall",
        "Martine Ceberio"
      ],
      "abstract": "Understanding causal relationships among features is fundamental for explaining machine learning model decisions. However, traditional causal discovery methods face challenges with categorical variables due to numerical instability in conditional independence testing. We propose a dual-encoding causal discovery approach that addresses these limitations by running constraint-based algorithms with complementary encoding strategies and merging results through majority voting. Applied to the Titanic dataset, our method identifies causal structures that align with established explainable methods.",
      "tldr_zh": "理解特征间的因果关系是解释机器学习模型决策的基础，但传统的 Causal Discovery 方法在处理分类变量时，常因条件独立性测试（conditional independence testing）中的数值不稳定性而面临挑战。该研究提出了一种双编码（dual-encoding）因果发现方法，通过在基于约束的算法（constraint-based algorithms）中运用互补的编码策略，并利用多数投票（majority voting）机制合并结果，有效克服了现有技术的局限。通过在 Titanic 数据集上的实验验证，该方法识别出的因果结构与公认的可解释性方法高度一致。这一方法为增强 Explainable AI 的逻辑透明度提供了一种更为稳健的因果分析方案。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "6 pages",
      "pdf_url": "https://arxiv.org/pdf/2601.21221v1",
      "published_date": "2026-01-29 03:36:21 UTC",
      "updated_date": "2026-01-29 03:36:21 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-30T04:34:31.689174+00:00"
    },
    {
      "arxiv_id": "2601.21215v1",
      "title": "Temporal Context and Architecture: A Benchmark for Naturalistic EEG Decoding",
      "title_zh": "时间上下文与架构：自然情境下 EEG 解码的基准研究",
      "authors": [
        "Mehmet Ergezer"
      ],
      "abstract": "We study how model architecture and temporal context interact in naturalistic EEG decoding. Using the HBN movie-watching dataset, we benchmark five architectures, CNN, LSTM, a stabilized Transformer (EEGXF), S4, and S5, on a 4-class task across segment lengths from 8s to 128s. Accuracy improves with longer context: at 64s, S5 reaches 98.7%+/-0.6 and CNN 98.3%+/-0.3, while S5 uses ~20x fewer parameters than CNN. To probe real-world robustness, we evaluate zero-shot cross-frequency shifts, cross-task OOD inputs, and leave-one-subject-out generalization. S5 achieves stronger cross-subject accuracy but makes over-confident errors on OOD tasks; EEGXF is more conservative and stable under frequency shifts, though less calibrated in-distribution. These results reveal a practical efficiency-robustness trade-off: S5 for parameter-efficient peak accuracy; EEGXF when robustness and conservative uncertainty are critical.",
      "tldr_zh": "该研究探讨了在自然主义 EEG 解码中，模型架构与时间上下文 (temporal context) 之间的相互作用。基于 HBN 电影观看数据集，研究者对比了 CNN、LSTM、稳定 Transformer (EEGXF)、S4 和 S5 五种架构在不同片段长度下的表现，发现准确率随上下文长度增加而显著提升。在 64s 片段下，S5 达到了 98.7% 的准确率，且参数量仅为 CNN 的约二十分之一，展现出极高的参数效率 (parameter-efficient)。通过跨频率偏移、任务外 (OOD) 输入及跨受试者泛化的评估，研究发现 S5 在跨受试者准确率上表现优异，但在 OOD 任务中容易产生过度自信的错误。相比之下，EEGXF 在频率偏移下更为稳定且其预测的不确定性估计更加保守。该研究揭示了模型在效率与鲁棒性之间的权衡关系，建议在追求峰值准确率时使用 S5，而在强调鲁棒性时选择 EEGXF。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "eess.SP"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2601.21215v1",
      "published_date": "2026-01-29 03:26:16 UTC",
      "updated_date": "2026-01-29 03:26:16 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-30T04:34:43.660571+00:00"
    },
    {
      "arxiv_id": "2601.21212v1",
      "title": "Intelli-Planner: Towards Customized Urban Planning via Large Language Model Empowered Reinforcement Learning",
      "title_zh": "Intelli-Planner：通过大语言模型赋能的强化学习迈向定制化城市规划",
      "authors": [
        "Xixian Yong",
        "Peilin Sun",
        "Zihe Wang",
        "Xiao Zhou"
      ],
      "abstract": "Effective urban planning is crucial for enhancing residents' quality of life and ensuring societal stability, playing a pivotal role in the sustainable development of cities. Current planning methods heavily rely on human experts, which are time-consuming and labor-intensive, or utilize deep learning algorithms, often limiting stakeholder involvement. To bridge these gaps, we propose Intelli-Planner, a novel framework integrating Deep Reinforcement Learning (DRL) with large language models (LLMs) to facilitate participatory and customized planning scheme generation. Intelli-Planner utilizes demographic, geographic data, and planning preferences to determine high-level planning requirements and demands for each functional type. During training, a knowledge enhancement module is employed to enhance the decision-making capability of the policy network. Additionally, we establish a multi-dimensional evaluation system and employ LLM-based stakeholders for satisfaction scoring. Experimental validation across diverse urban settings shows that Intelli-Planner surpasses traditional baselines and achieves comparable performance to state-of-the-art DRL-based methods in objective metrics, while enhancing stakeholder satisfaction and convergence speed. These findings underscore the effectiveness and superiority of our framework, highlighting the potential for integrating the latest advancements in LLMs with DRL approaches to revolutionize tasks related to functional areas planning.",
      "tldr_zh": "该研究提出了 Intelli-Planner，一个将深度强化学习 (Deep Reinforcement Learning, DRL) 与大语言模型 (Large Language Models, LLMs) 相结合的新型框架，旨在实现参与式和定制化的城市规划方案生成。该框架利用人口统计、地理数据和规划偏好来确定各功能类型的高级规划需求，并在训练过程中采用知识增强模块 (knowledge enhancement module) 来提升策略网络的决策能力。此外，研究建立了一个多维评估系统，并利用基于 LLM 的利益相关者 (stakeholders) 进行满意度评分。实验结果表明，在多种城市环境下，Intelli-Planner 在客观指标上超越了传统基准模型，并达到了与最先进的 DRL 方法相当的性能。同时，该方法显著提升了利益相关者的满意度和算法的收敛速度。该研究证明了将 LLM 与 DRL 技术融合在功能区规划领域的有效性，为实现更具包容性和可持续性的城市发展提供了创新方案。",
      "categories": [
        "cs.AI",
        "cs.CY"
      ],
      "primary_category": "cs.AI",
      "comment": "The Web Conference 2026",
      "pdf_url": "https://arxiv.org/pdf/2601.21212v1",
      "published_date": "2026-01-29 03:23:40 UTC",
      "updated_date": "2026-01-29 03:23:40 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-30T04:34:34.897844+00:00"
    },
    {
      "arxiv_id": "2601.21210v1",
      "title": "Uncovering Hidden Correctness in LLM Causal Reasoning via Symbolic Verification",
      "title_zh": "通过符号验证揭示LLM因果推理中隐藏的正确性",
      "authors": [
        "Paul He",
        "Yinya Huang",
        "Mrinmaya Sachan",
        "Zhijing Jin"
      ],
      "abstract": "Large language models (LLMs) are increasingly being applied to tasks that involve causal reasoning. However, current benchmarks often rely on string matching or surface-level metrics that do not capture whether the output of a model is formally valid under the semantics of causal reasoning. To address this, we propose DoVerifier, a simple symbolic verifier that checks whether LLM-generated causal expressions are derivable from a given causal graph using rules from do-calculus and probability theory. This allows us to recover correct answers to causal queries that would otherwise be marked incorrect due to superficial differences in their causal semantics. Our evaluations on synthetic data and causal QA benchmarks show that DoVerifier more accurately captures semantic correctness of causal reasoning traces, offering a more rigorous and informative way to evaluate LLMs on causal reasoning.",
      "tldr_zh": "该研究针对当前大语言模型(LLMs)因果推理(causal reasoning)基准测试过度依赖字符串匹配等表层指标，无法准确捕捉模型语义正确性的问题，提出了符号验证器DoVerifier。DoVerifier依据do-calculus和概率论规则，通过检查模型生成的因果表达式是否能从给定因果图(causal graph)中推导得出，实现了对推理逻辑的严谨验证。该方法能够有效恢复因表层语义差异而被误判的正确答案，从而挖掘出模型隐藏的推理能力。在合成数据和因果问答(causal QA)基准测试上的实验证明，DoVerifier比传统指标能更准确地评估推理轨迹的语义正确性。这项工作为衡量LLMs在因果推理领域的真实水平提供了一种更为严谨且具有启发性的评估框架。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "EACL 2026 Main",
      "pdf_url": "https://arxiv.org/pdf/2601.21210v1",
      "published_date": "2026-01-29 03:22:58 UTC",
      "updated_date": "2026-01-29 03:22:58 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-30T04:34:36.898637+00:00"
    },
    {
      "arxiv_id": "2601.21208v1",
      "title": "When should I search more: Adaptive Complex Query Optimization with Reinforcement Learning",
      "title_zh": "何时需要更多搜索：基于强化学习的自适应复杂查询优化",
      "authors": [
        "Wei Wen",
        "Sihang Deng",
        "Tianjun Wei",
        "Keyu Chen",
        "Ruizhi Qiao",
        "Xing Sun"
      ],
      "abstract": "Query optimization is a crucial component for the efficacy of Retrieval-Augmented Generation (RAG) systems. While reinforcement learning (RL)-based agentic and reasoning methods have recently emerged as a promising direction on query optimization, most existing approaches focus on the expansion and abstraction of a single query. However, complex user queries are prevalent in real-world scenarios, often requiring multiple parallel and sequential search strategies to handle disambiguation and decomposition. Directly applying RL to these complex cases introduces significant hurdles. Determining the optimal number of sub-queries and effectively re-ranking and merging retrieved documents vastly expands the search space and complicates reward design, frequently leading to training instability. To address these challenges, we propose a novel RL framework called Adaptive Complex Query Optimization (ACQO). Our framework is designed to adaptively determine when and how to expand the search process. It features two core components: an Adaptive Query Reformulation (AQR) module that dynamically decides when to decompose a query into multiple sub-queries, and a Rank-Score Fusion (RSF) module that ensures robust result aggregation and provides stable reward signals for the learning agent. To mitigate training instabilities, we adopt a Curriculum Reinforcement Learning (CRL) approach, which stabilizes the training process by progressively introducing more challenging queries through a two-stage strategy. Our comprehensive experiments demonstrate that ACQO achieves state-of-the-art performance on three complex query benchmarks, significantly outperforming established baselines. The framework also showcases improved computational efficiency and broad compatibility with different retrieval architectures, establishing it as a powerful and generalizable solution for next-generation RAG systems.",
      "tldr_zh": "该研究提出了ACQO（Adaptive Complex Query Optimization）框架，旨在利用强化学习(Reinforcement Learning)解决检索增强生成(RAG)系统在处理复杂用户查询时的优化难题。针对现有方法在确定子查询数量以及合并检索结果时面临的搜索空间大和训练不稳定等挑战，ACQO引入了自适应查询重构(AQR)模块，能够动态决定何时将查询分解为多个子查询。此外，该框架集成了Rank-Score Fusion (RSF)模块，以确保结果聚合的稳健性并为学习智能体提供稳定的奖励信号。为了缓解训练波动，研究采用了课程强化学习(Curriculum Reinforcement Learning)的两阶段策略，通过逐步增加查询难度来引导模型学习。实验结果表明，ACQO在三个复杂查询基准数据集上均取得了SOTA性能，并在计算效率和检索架构兼容性方面表现优异。该研究为下一代RAG系统提供了一种高效且具有泛化能力的复杂查询优化解决方案。",
      "categories": [
        "cs.AI",
        "cs.IR"
      ],
      "primary_category": "cs.AI",
      "comment": "16 pages, 7 figures",
      "pdf_url": "https://arxiv.org/pdf/2601.21208v1",
      "published_date": "2026-01-29 03:16:53 UTC",
      "updated_date": "2026-01-29 03:16:53 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-30T04:34:59.404735+00:00"
    },
    {
      "arxiv_id": "2601.21207v1",
      "title": "A Sheaf-Theoretic and Topological Perspective on Complex Network Modeling and Attention Mechanisms in Graph Neural Models",
      "title_zh": "层束论与拓扑视角下的图神经模型复杂网络建模及注意力机制",
      "authors": [
        "Chuan-Shen Hu"
      ],
      "abstract": "Combinatorial and topological structures, such as graphs, simplicial complexes, and cell complexes, form the foundation of geometric and topological deep learning (GDL and TDL) architectures. These models aggregate signals over such domains, integrate local features, and generate representations for diverse real-world applications. However, the distribution and diffusion behavior of GDL and TDL features during training remains an open and underexplored problem. Motivated by this gap, we introduce a cellular sheaf theoretic framework for modeling and analyzing the local consistency and harmonicity of node features and edge weights in graph-based architectures. By tracking local feature alignments and agreements through sheaf structures, the framework offers a topological perspective on feature diffusion and aggregation. Furthermore, a multiscale extension inspired by topological data analysis (TDA) is proposed to capture hierarchical feature interactions in graph models. This approach enables a joint characterization of GDL and TDL architectures based on their underlying geometric and topological structures and the learned signals defined on them, providing insights for future studies on conventional tasks such as node classification, substructure detection, and community detection.",
      "tldr_zh": "该研究针对几何与拓扑深度学习(GDL and TDL)中特征分布与扩散行为尚不明确的问题，从层论(Sheaf Theory)和拓扑学视角探讨了复杂网络建模。作者提出了一个细胞层论(Cellular Sheaf Theoretic)框架，用于建模和分析图神经网络架构中节点特征与边权重的局部一致性(Local Consistency)和调和性(Harmonicity)。该框架通过层结构(Sheaf Structures)追踪局部特征的对齐与一致性，为特征扩散和聚合提供了一个独特的拓扑学视角。此外，研究还受到拓扑数据分析(TDA)的启发，提出了一种多尺度扩展方案，以捕捉图模型中的分层特征交互。该方法实现了对GDL和TDL架构基于其底层几何拓扑结构及所学习信号的联合表征。这为未来在节点分类(Node Classification)、子结构检测(Substructure Detection)和社区检测(Community Detection)等传统任务上的研究提供了新的见解。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "math.AT"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2601.21207v1",
      "published_date": "2026-01-29 03:16:18 UTC",
      "updated_date": "2026-01-29 03:16:18 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-30T04:34:47.148548+00:00"
    },
    {
      "arxiv_id": "2601.21204v1",
      "title": "Scaling Embeddings Outperforms Scaling Experts in Language Models",
      "title_zh": "语言模型中嵌入扩展的效果优于专家扩展",
      "authors": [
        "Hong Liu",
        "Jiaqi Zhang",
        "Chao Wang",
        "Xing Hu",
        "Linkun Lyu",
        "Jiaqi Sun",
        "Xurui Yang",
        "Bo Wang",
        "Fengcun Li",
        "Yulei Qian",
        "Lingtong Si",
        "Yerui Sun",
        "Rumei Li",
        "Peng Pei",
        "Yuchen Xie",
        "Xunliang Cai"
      ],
      "abstract": "While Mixture-of-Experts (MoE) architectures have become the standard for sparsity scaling in large language models, they increasingly face diminishing returns and system-level bottlenecks. In this work, we explore embedding scaling as a potent, orthogonal dimension for scaling sparsity. Through a comprehensive analysis and experiments, we identify specific regimes where embedding scaling achieves a superior Pareto frontier compared to expert scaling. We systematically characterize the critical architectural factors governing this efficacy -- ranging from parameter budgeting to the interplay with model width and depth. Moreover, by integrating tailored system optimizations and speculative decoding, we effectively convert this sparsity into tangible inference speedups. Guided by these insights, we introduce LongCat-Flash-Lite, a 68.5B parameter model with ~3B activated trained from scratch. Despite allocating over 30B parameters to embeddings, LongCat-Flash-Lite not only surpasses parameter-equivalent MoE baselines but also exhibits exceptional competitiveness against existing models of comparable scale, particularly in agentic and coding domains.",
      "tldr_zh": "这项研究探讨了将 embedding scaling 作为大语言模型稀疏扩展的新维度，旨在解决 Mixture-of-Experts (MoE) 架构目前面临的边际收益递减和系统性瓶颈问题。作者通过全面实验系统地表征了影响该方法效能的关键架构因素，包括参数预算分配以及与模型宽度、深度的交互作用。通过整合定制的系统优化和 speculative decoding 技术，研究成功将这种稀疏性转化为显著的推理加速。基于这些见解，研究团队开发了 LongCat-Flash-Lite 模型，该模型总规模达 68.5B 参数且激活参数仅约 3B，其中专门为 embeddings 分配了超过 30B 参数。实验证明，LongCat-Flash-Lite 不仅在性能上超越了同等规模的 MoE 基准，在 agentic 和 coding 领域也表现卓越，验证了扩展 embeddings 在特定条件下能比传统的扩展 experts 获得更优的 Pareto frontier。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2601.21204v1",
      "published_date": "2026-01-29 03:11:19 UTC",
      "updated_date": "2026-01-29 03:11:19 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-30T04:34:58.987114+00:00"
    },
    {
      "arxiv_id": "2601.21199v1",
      "title": "Thinker: A vision-language foundation model for embodied intelligence",
      "title_zh": "Thinker：面向具身智能的视觉语言基础模型",
      "authors": [
        "Baiyu Pan",
        "Daqin Luo",
        "Junpeng Yang",
        "Jiyuan Wang",
        "Yixuan Zhang",
        "Hailin Shi",
        "Jichao Jiao"
      ],
      "abstract": "When large vision-language models are applied to the field of robotics, they encounter problems that are simple for humans yet error-prone for models. Such issues include confusion between third-person and first-person perspectives and a tendency to overlook information in video endings during temporal reasoning. To address these challenges, we propose Thinker, a large vision-language foundation model designed for embodied intelligence. We tackle the aforementioned issues from two perspectives. Firstly, we construct a large-scale dataset tailored for robotic perception and reasoning, encompassing ego-view videos, visual grounding, spatial understanding, and chain-of-thought data. Secondly, we introduce a simple yet effective approach that substantially enhances the model's capacity for video comprehension by jointly incorporating key frames and full video sequences as inputs. Our model achieves state-of-the-art results on two of the most commonly used benchmark datasets in the field of task planning.",
      "tldr_zh": "该研究提出了 Thinker，一种专为具身智能 (Embodied Intelligence) 设计的大规模视觉语言基础模型 (Vision-Language Foundation Model)。针对当前机器人领域中模型容易混淆第一与第三人称视角，以及在时间推理中忽略视频结尾信息等问题，该模型从数据和架构两个维度进行了改进。研究者首先构建了一个包含第一人称视角视频、视觉定位 (Visual Grounding)、空间理解和链式思维 (Chain-of-Thought) 数据的大规模专用数据集。其次，模型采用了一种将关键帧与完整视频序列共同作为输入的简单有效方法，显著增强了对视频内容的全面理解能力。实验结果表明，Thinker 在两项最常用的任务规划 (Task Planning) 基准数据集上均取得了当前的 SOTA 性能，为解决具身智能中的感知与推理难题提供了有力支持。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "IROS 2025, 4 pages, 3 figures",
      "pdf_url": "https://arxiv.org/pdf/2601.21199v1",
      "published_date": "2026-01-29 02:52:08 UTC",
      "updated_date": "2026-01-29 02:52:08 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-30T04:34:57.913403+00:00"
    },
    {
      "arxiv_id": "2601.21198v1",
      "title": "ZipMoE: Efficient On-Device MoE Serving via Lossless Compression and Cache-Affinity Scheduling",
      "title_zh": "ZipMoE：基于无损压缩与缓存亲和性调度的高效设备端 MoE 推理服务",
      "authors": [
        "Yuchen Yang",
        "Yaru Zhao",
        "Pu Yang",
        "Shaowei Wang",
        "Zhi-Hua Zhou"
      ],
      "abstract": "While Mixture-of-Experts (MoE) architectures substantially bolster the expressive power of large-language models, their prohibitive memory footprint severely impedes the practical deployment on resource-constrained edge devices, especially when model behavior must be preserved without relying on lossy quantization. In this paper, we present ZipMoE, an efficient and semantically lossless on-device MoE serving system. ZipMoE exploits the synergy between the hardware properties of edge devices and the statistical redundancy inherent to MoE parameters via a caching-scheduling co-design with provable performance guarantee. Fundamentally, our design shifts the paradigm of on-device MoE inference from an I/O-bound bottleneck to a compute-centric workflow that enables efficient parallelization. We implement a prototype of ZipMoE and conduct extensive experiments on representative edge computing platforms using popular open-source MoE models and real-world workloads. Our evaluation reveals that ZipMoE achieves up to $72.77\\%$ inference latency reduction and up to $6.76\\times$ higher throughput than the state-of-the-art systems.",
      "tldr_zh": "该研究提出了 ZipMoE，一种高效且在语义上无损(semantically lossless)的设备端 Mixture-of-Experts (MoE) 服务系统，旨在解决 MoE 模型因内存占用巨大而难以在资源受限的边缘设备上部署的挑战。ZipMoE 通过缓存-调度协同设计(caching-scheduling co-design)充分利用了边缘设备的硬件特性与 MoE 参数固有的统计冗余，并提供了可证明的性能保证。该设计从根本上将设备端 MoE 推理的范式从 I/O 受限(I/O-bound)的瓶颈转变为支持高效并行化的以计算为中心(compute-centric)的工作流。在代表性边缘计算平台和主流开源 MoE 模型上的实验评估显示，ZipMoE 相比当前最先进的系统可降低高达 72.77% 的推理延迟，并实现 6.76 倍的吞吐量提升。",
      "categories": [
        "cs.DC",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.DC",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2601.21198v1",
      "published_date": "2026-01-29 02:51:59 UTC",
      "updated_date": "2026-01-29 02:51:59 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-30T04:35:08.858327+00:00"
    },
    {
      "arxiv_id": "2601.21192v1",
      "title": "Do Reasoning Models Enhance Embedding Models?",
      "title_zh": "推理模型是否增强了嵌入模型？",
      "authors": [
        "Wun Yu Chan",
        "Shaojin Chen",
        "Huihao Jing",
        "Kwun Hang Lau",
        "Elton Chun-Chai Li",
        "Zihao Wang",
        "Haoran Li",
        "Yangqiu Song"
      ],
      "abstract": "State-of-the-art embedding models are increasingly derived from decoder-only Large Language Model (LLM) backbones adapted via contrastive learning. Given the emergence of reasoning models trained via Reinforcement Learning with Verifiable Rewards (RLVR), a natural question arises: do enhanced reasoning translate to superior semantic representations when these models serve as embedding initializations? Contrary to expectation, our evaluation on MTEB and BRIGHT reveals a **null effect**: embedding models initialized from RLVR-tuned backbones yield no consistent performance advantage over their base counterparts when subjected to identical training recipes. To unpack this paradox, we introduce **H**ierarchical **R**epresentation **S**imilarity **A**nalysis (HRSA), a framework that decomposes similarity across representation, geometry, and function levels. HRSA reveals that while RLVR induces irreversible latent manifold's local geometry reorganization and reversible coordinate basis drift, it preserves the global manifold geometry and linear readout. Consequently, subsequent contrastive learning drives strong alignment between base- and reasoning-initialized models, a phenomenon we term **Manifold Realignment**. Empirically, our findings suggest that unlike Supervised Fine-Tuning (SFT), RLVR optimizes trajectories within an existing semantic landscape rather than fundamentally restructuring the landscape itself.",
      "tldr_zh": "该研究探讨了通过可验证奖励强化学习(RLVR)训练的推理模型作为嵌入模型初始化时，是否能提升语义表示性能。通过在MTEB和BRIGHT基准上的评估，研究发现推理模型相对于基础模型并无显著性能优势，呈现出一种“零效应”。为了解释这一现象，作者提出了层级表示相似性分析(HRSA)框架，从表示、几何和功能层面分解相似性。分析表明，虽然RLVR诱导了潜流形的局部几何重组，但保留了全局流形几何，随后的对比学习则驱动了基础模型与推理模型初始化模型之间强烈的流形对齐(Manifold Realignment)。实验结果揭示，与监督微调(SFT)不同，RLVR主要是在现有的语义格局内优化轨迹，而非从根本上重构语义格局本身。",
      "categories": [
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.AI",
      "comment": "10 main pages, 18 appendix pages, 13 figures, 11 tables, 4 prompts",
      "pdf_url": "https://arxiv.org/pdf/2601.21192v1",
      "published_date": "2026-01-29 02:48:34 UTC",
      "updated_date": "2026-01-29 02:48:34 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-30T04:35:25.425269+00:00"
    },
    {
      "arxiv_id": "2601.21191v1",
      "title": "From Linear Input to Hierarchical Structure: Function Words as Statistical Cues for Language Learning",
      "title_zh": "从线性输入到层级结构：虚词作为语言学习的统计线索",
      "authors": [
        "Xiulin Yang",
        "Heidi Getz",
        "Ethan Gotlieb Wilcox"
      ],
      "abstract": "What statistical conditions support learning hierarchical structure from linear input? In this paper, we address this question by focusing on the statistical distribution of function words. Function words have long been argued to play a crucial role in language acquisition due to their distinctive distributional properties, including high frequency, reliable association with syntactic structure, and alignment with phrase boundaries. We use cross-linguistic corpus analysis to first establish that all three properties are present across 186 studied languages. Next, we use a combination of counterfactual language modeling and ablation experiments to show that language variants preserving all three properties are more easily acquired by neural learners, with frequency and structural association contributing more strongly than boundary alignment. Follow-up probing and ablation analyses further reveal that different learning conditions lead to systematically different reliance on function words, indicating that similar performance can arise from distinct internal mechanisms.",
      "tldr_zh": "该研究探讨了如何从线性输入中学习层级结构(hierarchical structure)，并重点分析了虚词(function words)作为统计线索在语言习得中的作用。通过对186种语言进行跨语言语料库分析(cross-linguistic corpus analysis)，研究验证了虚词普遍具有高频性、与句法结构的可靠关联以及与短语边界的对齐特性。研究者结合反事实语言建模(counterfactual language modeling)和消融实验(ablation experiments)观察神经学习器(neural learners)的习得表现，结果表明保留上述三项统计特性的语言变体更易被习得。实验发现虚词的高频性和结构关联性对学习的贡献强于边界对齐。进一步的探测分析(probing analysis)揭示，不同的学习条件会导致模型对虚词产生系统性的依赖差异，说明相似的性能表现可能源于截然不同的内部机制。该研究为理解统计分布如何支持语言层级结构的习得提供了重要见解。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "Jan ARR under review",
      "pdf_url": "https://arxiv.org/pdf/2601.21191v1",
      "published_date": "2026-01-29 02:42:12 UTC",
      "updated_date": "2026-01-29 02:42:12 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-30T04:35:35.044440+00:00"
    },
    {
      "arxiv_id": "2601.21189v1",
      "title": "Adaptive and Robust Cost-Aware Proof of Quality for Decentralized LLM Inference Networks",
      "title_zh": "面向去中心化大语言模型推理网络的自适应、鲁棒且成本感知的质量证明机制",
      "authors": [
        "Arther Tian",
        "Alex Ding",
        "Frank Chen",
        "Simon Wu",
        "Aaron Chan"
      ],
      "abstract": "Decentralized large language model inference networks require lightweight mechanisms to reward high quality outputs under heterogeneous latency and cost. Proof of Quality provides scalable verification by sampling evaluator nodes that score candidate outputs, then aggregating their scores into a consensus signal that determines rewards. However, evaluator heterogeneity and malicious score manipulation can distort consensus and inflate payouts, which weakens incentive alignment in open participation settings.\n  This paper extends a cost-aware Proof of Quality mechanism by adding adversary-resilient consensus formation. We study robust aggregation rules, including median and trimmed mean, and an adaptive trust-weighted consensus that updates evaluator weights from deviation signals. Using question answering and summarization workloads with a ground truth proxy for offline analysis, we quantify evaluator reliability and show strong variance across evaluators, including task-dependent misalignment that can invert correlations. We then evaluate robustness under four adversarial strategies, including noise injection, boosting, sabotage, and intermittent manipulation, across a sweep of malicious ratios and evaluator sample sizes. Our results show that robust aggregation improves consensus alignment with the ground truth proxy and reduces sensitivity to noisy and strategic attacks compared with simple averaging. We further characterize the operational trade-off introduced by evaluator sampling, where larger evaluator sets reduce evaluator rewards and increase payoff variance while inference rewards remain relatively stable in our configuration. These findings motivate robust consensus as a default component for cost-aware Proof of Quality and provide practical guidance for selecting evaluator sampling parameters under adversarial risk and resource constraints.",
      "tldr_zh": "该研究针对去中心化大语言模型(LLM)推理网络在异构环境下难以公正奖励高质量输出的问题，扩展了成本感知的质量证明(Proof of Quality)机制。论文提出了抗对抗的共识形成方法，引入了包括中位数(median)、修剪均值(trimmed mean)在内的鲁棒聚合规则，以及能根据偏差信号动态更新评估者权重的自适应信任加权共识。通过在问答和摘要负载下的实验，研究量化了评估者的可靠性方差，并在噪声注入、恶意提分、破坏及间歇性操纵四种对抗策略下进行了全面评估。结果显示，鲁棒聚合规则相比简单平均能更显著地提升共识信号与基准真相(ground truth)的一致性，并增强系统对策略性攻击的抵御能力。此外，研究还探讨了评估者采样规模与奖励分布之间的操作权衡，为在安全风险与资源约束下优化去中心化推理网络提供了实践指导。",
      "categories": [
        "cs.CR",
        "cs.AI"
      ],
      "primary_category": "cs.CR",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2601.21189v1",
      "published_date": "2026-01-29 02:39:40 UTC",
      "updated_date": "2026-01-29 02:39:40 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-30T04:35:29.970761+00:00"
    },
    {
      "arxiv_id": "2601.21183v1",
      "title": "Sycophantic Anchors: Localizing and Quantifying User Agreement in Reasoning Models",
      "title_zh": "Sycophantic Anchors：推理模型中迎合用户行为的定位与量化",
      "authors": [
        "Jacek Duszenko"
      ],
      "abstract": "Reasoning models frequently agree with incorrect user suggestions -- a behavior known as sycophancy. However, it is unclear where in the reasoning trace this agreement originates and how strong the commitment is. To localize and quantify this behavior, we introduce \\emph{sycophantic anchors} -- sentences that causally lock models into user agreement. Analyzing over 10,000 counterfactual rollouts on a distilled reasoning model, we show that anchors can be reliably detected and quantified mid-inference. Linear probes distinguish sycophantic anchors with 84.6\\% balanced accuracy, while activation-based regressors predict the magnitude of the commitment ($R^2 = 0.74$). We further observe asymmetry where sycophantic anchors are significantly more distinguishable than correct reasoning anchors, and find that sycophancy builds gradually during reasoning, revealing a potential window for intervention. These results offer sentence-level mechanisms for localizing model misalignment mid-inference.",
      "tldr_zh": "该研究探讨了推理模型中经常顺从用户错误建议的 sycophancy 现象，并提出了 sycophantic anchors 这一新概念，指代在推理追踪中将模型因果性地锁定在用户共识上的特定句子。通过对蒸馏推理模型进行超过 10,000 次反事实 rollouts 分析，研究证明这些锚点可以在推理过程中被可靠地检测和量化。实验结果显示，linear probes 能以 84.6% 的平衡准确率识别 sycophantic anchors，而 activation-based regressors 则能有效预测其承诺强度（$R^2 = 0.74$）。研究进一步观察到 sycophantic anchors 比正确的推理锚点更具辨识度，且 sycophancy 是在推理过程中逐渐构建的。这些发现揭示了在推理中定位和干预模型 misalignment 的句子层面机制，为提升模型诚实性提供了可能的干预窗口。",
      "categories": [
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2601.21183v1",
      "published_date": "2026-01-29 02:34:16 UTC",
      "updated_date": "2026-01-29 02:34:16 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-30T04:35:31.440844+00:00"
    },
    {
      "arxiv_id": "2601.21182v1",
      "title": "Rethinking Refinement: Correcting Generative Bias without Noise Injection",
      "title_zh": "重新审视提炼：无需噪声注入的生成偏差校正",
      "authors": [
        "Xin Peng",
        "Ang Gao"
      ],
      "abstract": "Generative models, including diffusion and flow-based models, often exhibit systematic biases that degrade sample quality, particularly in high-dimensional settings. We revisit refinement methods and show that effective bias correction can be achieved as a post-hoc procedure, without noise injection or multi-step resampling of the sampling process. We propose a flow-matching-based \\textbf{Bi-stage Flow Refinement (BFR)} framework with two refinement strategies operating at different stages: latent space alignment for approximately invertible generators and data space refinement trained with lightweight augmentations. Unlike previous refiners that perturb sampling dynamics, BFR preserves the original ODE trajectory and applies deterministic corrections to generated samples. Experiments on MNIST, CIFAR-10, and FFHQ at 256x256 resolution demonstrate consistent improvements in fidelity and coverage; notably, starting from base samples with FID 3.95, latent space refinement achieves a \\textbf{state-of-the-art} FID of \\textbf{1.46} on MNIST using only a single additional function evaluation (1-NFE), while maintaining sample diversity.",
      "tldr_zh": "该研究针对 diffusion 和 flow-based models 等生成模型在高维设置下存在的系统性偏差，提出了一种名为 Bi-stage Flow Refinement (BFR) 的两阶段流细化框架。该框架基于 flow-matching 技术，通过 latent space alignment 和 data space refinement 两种策略实现事后偏差修正，且无需注入噪声或进行多步重采样。与扰动采样动力学的传统细化方法不同，BFR 在保留原始 ODE trajectory 的基础上对生成样本执行确定性校正。实验在 MNIST、CIFAR-10 和 FFHQ 数据集上展示了保真度与覆盖率的持续改进。特别是在 MNIST 数据集上，该方法仅需单次额外函数调用 (1-NFE) 即可将 FID 从 3.95 优化至 1.46 的 state-of-the-art 水平。该研究在显著提升生成质量的同时，依然有效地保持了样本的多样性。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2601.21182v1",
      "published_date": "2026-01-29 02:34:08 UTC",
      "updated_date": "2026-01-29 02:34:08 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-30T04:35:46.599771+00:00"
    },
    {
      "arxiv_id": "2601.21181v1",
      "title": "MAD: Modality-Adaptive Decoding for Mitigating Cross-Modal Hallucinations in Multimodal Large Language Models",
      "title_zh": "MAD：用于缓解多模态大语言模型跨模态幻觉的模态自适应解码",
      "authors": [
        "Sangyun Chung",
        "Se Yeon Kim",
        "Youngchae Chee",
        "Yong Man Ro"
      ],
      "abstract": "Multimodal Large Language Models (MLLMs) suffer from cross-modal hallucinations, where one modality inappropriately influences generation about another, leading to fabricated output. This exposes a more fundamental deficiency in modality-interaction control. To address this, we propose Modality-Adaptive Decoding (MAD), a training-free method that adaptively weights modality-specific decoding branches based on task requirements. MAD leverages the model's inherent ability to self-assess modality relevance by querying which modalities are needed for each task. The extracted modality probabilities are then used to adaptively weight contrastive decoding branches, enabling the model to focus on relevant information while suppressing cross-modal interference. Extensive experiments on CMM and AVHBench demonstrate that MAD significantly reduces cross-modal hallucinations across multiple audio-visual language models (7.8\\% and 2.0\\% improvements for VideoLLaMA2-AV, 8.7\\% and 4.7\\% improvements for Qwen2.5-Omni). Our approach demonstrates that explicit modality awareness through self-assessment is crucial for robust multimodal reasoning, offering a principled extension to existing contrastive decoding methods. Our code is available at \\href{https://github.com/top-yun/MAD}{https://github.com/top-yun/MAD}",
      "tldr_zh": "该研究提出了MAD (Modality-Adaptive Decoding)，这是一种无需训练 (training-free) 的方法，旨在缓解多模态大语言模型 (MLLMs) 中的跨模态幻觉 (cross-modal hallucinations) 问题。跨模态幻觉通常源于不同模态间不当的相互影响，反映了模型在模态交互控制 (modality-interaction control) 上的基础性缺陷。MAD通过自评估任务所需的模态相关性，动态地为特定模态的解码分支分配权重，从而实现模态自适应解码。该方法利用提取的模态概率来加权对比解码 (contrastive decoding) 分支，使模型能够专注于相关信息并有效抑制跨模态干扰。在CMM和AVHBench上的实验表明，MAD显著提升了VideoLLaMA2-AV和Qwen2.5-Omni等模型的表现，最高准确率增幅达8.7%。该研究证明了显式的模态感知 (modality awareness) 对于稳健的多模态推理至关重要，为增强模型的可解释性与推理能力提供了有效路径。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2601.21181v1",
      "published_date": "2026-01-29 02:30:32 UTC",
      "updated_date": "2026-01-29 02:30:32 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-30T04:35:59.335138+00:00"
    },
    {
      "arxiv_id": "2601.21171v1",
      "title": "AC2L-GAD: Active Counterfactual Contrastive Learning for Graph Anomaly Detection",
      "title_zh": "AC2L-GAD：面向图异常检测的主动反事实对比学习",
      "authors": [
        "Kamal Berahmand",
        "Saman Forouzandeh",
        "Mehrnoush Mohammadi",
        "Parham Moradi",
        "Mahdi Jalili"
      ],
      "abstract": "Graph anomaly detection aims to identify abnormal patterns in networks, but faces significant challenges from label scarcity and extreme class imbalance. While graph contrastive learning offers a promising unsupervised solution, existing methods suffer from two critical limitations: random augmentations break semantic consistency in positive pairs, while naive negative sampling produces trivial, uninformative contrasts. We propose AC2L-GAD, an Active Counterfactual Contrastive Learning framework that addresses both limitations through principled counterfactual reasoning. By combining information-theoretic active selection with counterfactual generation, our approach identifies structurally complex nodes and generates anomaly-preserving positive augmentations alongside normal negative counterparts that provide hard contrasts, while restricting expensive counterfactual generation to a strategically selected subset. This design reduces computational overhead by approximately 65% compared to full-graph counterfactual generation while maintaining detection quality. Experiments on nine benchmark datasets, including real-world financial transaction graphs from GADBench, show that AC2L-GAD achieves competitive or superior performance compared to state-of-the-art baselines, with notable gains in datasets where anomalies exhibit complex attribute-structure interactions.",
      "tldr_zh": "该研究提出了 AC2L-GAD，一种基于主动反事实对比学习 (Active Counterfactual Contrastive Learning) 的框架，旨在解决图异常检测 (Graph Anomaly Detection) 中标签稀缺和极度类别失衡的问题。针对现有对比学习中随机增强破坏语义一致性以及负采样信息不足等局限，该方法通过将信息论主动选择 (Information-theoretic active selection) 与反事实生成相结合，识别结构复杂的节点并生成保留异常的正增强及提供硬对比的负样本。该设计通过将昂贵的反事实生成限制在策略性选择的子集上，在保持检测质量的同时将计算开销降低了约 65%。在包括 GADBench 金融交易图在内的 9 个基准数据集上的实验证明，AC2L-GAD 在处理具有复杂属性结构交互的异常时表现尤为出色，其性能优于或等同于目前最先进的基准模型。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2601.21171v1",
      "published_date": "2026-01-29 02:11:56 UTC",
      "updated_date": "2026-01-29 02:11:56 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-30T04:35:53.450948+00:00"
    },
    {
      "arxiv_id": "2601.21169v1",
      "title": "Output-Space Search: Targeting LLM Generations in a Frozen Encoder-Defined Output Space",
      "title_zh": "输出空间搜索：在冻结编码器定义的输出空间中定向大语言模型生成",
      "authors": [
        "Tobias Materzok"
      ],
      "abstract": "We introduce Output-Space Search (OS-Search), which turns LLM generation into endpoint search. An outer loop selects a target z* in a frozen encoder-defined 3D output space Z, and a retrieval-grounded policy trained with sequence-level RL generates outputs whose coordinates land near z* under standard autoregressive decoding. This enables parallel sweeps and black-box optimization in Z without path-dependent token/program search. On stories, sweeping Z (text) yields 3.1x higher LLM-scored diversity than prompt-chaining. On code, Bayesian optimization over Z (code) improves an objective withheld from the controller under matched inference budgets while preserving validity.",
      "tldr_zh": "该研究提出了 Output-Space Search (OS-Search)，这是一种将大语言模型 (LLM) 的生成过程转化为端点搜索的新型框架。其核心机制是通过外循环在由冻结编码器定义的 3D 输出空间 Z 中选择目标点 z*，并利用经序列级强化学习 (sequence-level RL) 训练的检索增强策略生成输出，使其在标准自回归解码下的坐标靠近目标点。该方法支持在输出空间内进行并行扫描和黑盒优化，有效避免了传统生成中路径依赖的 Token 或程序搜索。在故事创作实验中，OS-Search 表现出的文本多样性比提示词链 (prompt-chaining) 高出 3.1 倍；而在代码任务中，通过贝叶斯优化 (Bayesian optimization) 进一步提升了特定目标的达成率并保证了代码有效性。该框架为实现更具针对性和多样性的模型生成提供了全新的优化路径。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2601.21169v1",
      "published_date": "2026-01-29 02:11:43 UTC",
      "updated_date": "2026-01-29 02:11:43 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-30T04:36:02.017446+00:00"
    },
    {
      "arxiv_id": "2601.21165v1",
      "title": "FrontierScience: Evaluating AI's Ability to Perform Expert-Level Scientific Tasks",
      "title_zh": "FrontierScience：评估人工智能执行专家级科学任务的能力",
      "authors": [
        "Miles Wang",
        "Robi Lin",
        "Kat Hu",
        "Joy Jiao",
        "Neil Chowdhury",
        "Ethan Chang",
        "Tejal Patwardhan"
      ],
      "abstract": "We introduce FrontierScience, a benchmark evaluating expert-level scientific reasoning in frontier language models. Recent model progress has nearly saturated existing science benchmarks, which often rely on multiple-choice knowledge questions or already published information. FrontierScience addresses this gap through two complementary tracks: (1) Olympiad, consisting of international olympiad problems at the level of IPhO, IChO, and IBO, and (2) Research, consisting of PhD-level, open-ended problems representative of sub-tasks in scientific research.\n  FrontierScience contains several hundred questions (including 160 in the open-sourced gold set) covering subfields across physics, chemistry, and biology, from quantum electrodynamics to synthetic organic chemistry. All Olympiad problems are originally produced by international Olympiad medalists and national team coaches to ensure standards of difficulty, originality, and factuality. All Research problems are research sub-tasks written and verified by PhD scientists (doctoral candidates, postdoctoral researchers, or professors). For Research, we introduce a granular rubric-based evaluation framework to assess model capabilities throughout the process of solving a research task, rather than judging only a standalone final answer.",
      "tldr_zh": "该研究推出了 FrontierScience，这是一个旨在评估前沿语言模型在专家级科学推理能力的基准测试。针对现有科学基准测试趋于饱和且过度依赖选择题或已发表信息的问题，FrontierScience 提供了更具挑战性的评估维度，包含两个互补赛道：由国际奥林匹克竞赛（如 IPhO、IChO 和 IBO）奖牌获得者设计的 Olympiad 赛道，以及由博士级科学家编写、代表科研子任务的 Research 赛道。题目涵盖了从 Quantum Electrodynamics 到 Synthetic Organic Chemistry 的物理、化学和生物等多个专业子领域。研究特别为 Research 赛道引入了细粒度的 Rubric-based evaluation 评估框架，用于衡量模型在解决科研任务全过程中的表现，而非仅仅判断最终答案。该工具为评估 AI 是否具备执行博士级开放性科学任务的能力提供了高质量且具有区分度的标准。",
      "categories": [
        "cs.AI",
        "cs.CY",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2601.21165v1",
      "published_date": "2026-01-29 02:04:56 UTC",
      "updated_date": "2026-01-29 02:04:56 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-30T04:35:46.112616+00:00"
    },
    {
      "arxiv_id": "2601.21164v1",
      "title": "Concise Geometric Description as a Bridge: Unleashing the Potential of LLM for Plane Geometry Problem Solving",
      "title_zh": "以简明几何描述为桥：释放大语言模型在平面几何问题求解中的潜力",
      "authors": [
        "Jingyun Wang",
        "Dian Li",
        "Xiaohan Wang",
        "Gang Liu",
        "Jiahong Yan",
        "Guoliang Kang"
      ],
      "abstract": "Plane Geometry Problem Solving (PGPS) is a multimodal reasoning task that aims to solve a plane geometric problem based on a geometric diagram and problem textual descriptions. Although Large Language Models (LLMs) possess strong reasoning skills, their direct application to PGPS is hindered by their inability to process visual diagrams. Existing works typically fine-tune Multimodal LLMs (MLLMs) end-to-end on large-scale PGPS data to enhance visual understanding and reasoning simultaneously. However, such joint optimization may compromise base LLMs' inherent reasoning capability. In this work, we observe that LLM itself is potentially a powerful PGPS solver when appropriately formulating visual information as textual descriptions. We propose to train a MLLM Interpreter to generate geometric descriptions for the visual diagram, and an off-the-shelf LLM is utilized to perform reasoning. Specifically, we choose Conditional Declaration Language (CDL) as the geometric description as its conciseness eases the MLLM Interpreter training. The MLLM Interpreter is fine-tuned via CoT (Chain-of-Thought)-augmented SFT followed by GRPO to generate CDL. Instead of using a conventional solution-based reward that compares the reasoning result with the ground-truth answer, we design CDL matching rewards to facilitate more effective GRPO training, which provides more direct and denser guidance for CDL generation. To support training, we construct a new dataset, Formalgeo7k-Rec-CoT, by manually reviewing Formalgeo7k v2 and incorporating CoT annotations. Extensive experiments on Formalgeo7k-Rec-CoT, Unigeo, and MathVista show our method (finetuned on only 5.5k data) performs favorably against leading open-source and closed-source MLLMs.",
      "tldr_zh": "该研究探讨了平面几何问题求解(Plane Geometry Problem Solving, PGPS)中大型语言模型(LLMs)因无法直接处理几何图形而面临的推理挑战。作者提出了一种将视觉信息转化为文本描述的桥接方法，通过训练一个多模态大模型解释器(MLLM Interpreter)来生成几何图形描述，从而利用现有的LLM进行逻辑推理。研究采用了简洁的条件声明语言(Conditional Declaration Language, CDL)作为描述媒介，并结合链式思维(Chain-of-Thought)增强的监督微调(SFT)和群体相对策略优化(GRPO)对解释器进行训练。特别地，研究者设计了CDL匹配奖励以提供更密集的训练引导，并构建了名为Formalgeo7k-Rec-CoT的新型标注数据集。实验表明，该方法在仅使用5.5k训练数据的情况下，在Formalgeo7k-Rec-CoT、Unigeo和MathVista等多个基准测试中的表现均优于当前领先的开源及闭源多模态模型(MLLMs)。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "Under review",
      "pdf_url": "https://arxiv.org/pdf/2601.21164v1",
      "published_date": "2026-01-29 02:03:33 UTC",
      "updated_date": "2026-01-29 02:03:33 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-30T04:36:24.071306+00:00"
    },
    {
      "arxiv_id": "2601.21162v1",
      "title": "A2RAG: Adaptive Agentic Graph Retrieval for Cost-Aware and Reliable Reasoning",
      "title_zh": "A2RAG：面向成本感知与可靠推理的自适应智能体图检索",
      "authors": [
        "Jiate Liu",
        "Zebin Chen",
        "Shaobo Qiao",
        "Mingchen Ju",
        "Danting Zhang",
        "Bocheng Han",
        "Shuyue Yu",
        "Xin Shu",
        "Jingling Wu",
        "Dong Wen",
        "Xin Cao",
        "Guanfeng Liu",
        "Zhengyi Yang"
      ],
      "abstract": "Graph Retrieval-Augmented Generation (Graph-RAG) enhances multihop question answering by organizing corpora into knowledge graphs and routing evidence through relational structure. However, practical deployments face two persistent bottlenecks: (i) mixed-difficulty workloads where one-size-fits-all retrieval either wastes cost on easy queries or fails on hard multihop cases, and (ii) extraction loss, where graph abstraction omits fine-grained qualifiers that remain only in source text. We present A2RAG, an adaptive-and-agentic GraphRAG framework for cost-aware and reliable reasoning. A2RAG couples an adaptive controller that verifies evidence sufficiency and triggers targeted refinement only when necessary, with an agentic retriever that progressively escalates retrieval effort and maps graph signals back to provenance text to remain robust under extraction loss and incomplete graphs. Experiments on HotpotQA and 2WikiMultiHopQA demonstrate that A2RAG achieves +9.9/+11.8 absolute gains in Recall@2, while cutting token consumption and end-to-end latency by about 50% relative to iterative multihop baselines.",
      "tldr_zh": "该研究提出了A2RAG，一种自适应且具有智能体能力的Graph-RAG框架，旨在解决Graph-RAG在多跳问答中面临的成本效益不平衡和提取损失(extraction loss)问题。该框架结合了一个自适应控制器，用于验证证据的充分性并仅在必要时触发定向细化，从而优化计算资源分配。此外，系统配备了一个智能体检索器，能够逐步提升检索力度，并将图信号映射回原始文本(provenance text)，以应对图抽象过程中的细节遗漏和图结构不完整。在HotpotQA和2WikiMultiHopQA数据集上的实验结果表明，A2RAG在Recall@2指标上分别实现了+9.9和+11.8的绝对增益。与迭代式多跳基线模型相比，该框架在保持高性能的同时，将Token消耗和端到端延迟显著降低了约50%，为实现成本敏感且可靠的推理奠定了基础。",
      "categories": [
        "cs.IR",
        "cs.AI",
        "cs.DB"
      ],
      "primary_category": "cs.IR",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2601.21162v1",
      "published_date": "2026-01-29 01:58:30 UTC",
      "updated_date": "2026-01-29 01:58:30 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-30T04:36:34.195520+00:00"
    },
    {
      "arxiv_id": "2601.21157v1",
      "title": "Bridging the Arithmetic Gap: The Cognitive Complexity Benchmark and Financial-PoT for Robust Financial Reasoning",
      "title_zh": "弥合算术鸿沟：认知复杂度基准与面向稳健金融推理的 Financial-PoT",
      "authors": [
        "Boxiang Zhao",
        "Qince Li",
        "Zhonghao Wang",
        "Yi Wang",
        "Peng Cheng",
        "Bo Lin"
      ],
      "abstract": "While Large Language Models excel at semantic tasks, they face a critical bottleneck in financial quantitative reasoning, frequently suffering from \"Arithmetic Hallucinations\" and a systemic failure mode we term \"Cognitive Collapse\". To strictly quantify this phenomenon, we introduce the Cognitive Complexity Benchmark (CCB), a robust evaluation framework grounded in a dataset constructed from 95 real-world Chinese A-share annual reports. Unlike traditional datasets, the CCB stratifies financial queries into a three-dimensional taxonomy, Data Source, Mapping Difficulty, and Result Unit, enabling the precise diagnosis of reasoning degradation in high-cognitive-load scenarios. To address these failures, we propose the Iterative Dual-Phase Financial-PoT framework. This neuro-symbolic architecture enforces a strict architectural decoupling: it first isolates semantic variable extraction and logic formulation, then offloads computation to an iterative, self-correcting Python sandbox to ensure deterministic execution. Evaluation on the CCB demonstrates that while standard Chain-of-Thought falters on complex tasks, our approach offers superior robustness, elevating the Qwen3-235B model's average accuracy from 59.7\\% to 67.3\\% and achieving gains of up to 10-fold in high-complexity reasoning tasks. These findings suggest that architectural decoupling is a critical enabling factor for improving reliability in financial reasoning tasks, providing a transferable architectural insight for precision-critical domains that require tight alignment between semantic understanding and quantitative computation.",
      "tldr_zh": "本研究针对大语言模型在金融定量推理中存在的 Arithmetic Hallucinations 和 Cognitive Collapse 问题，提出了 Cognitive Complexity Benchmark (CCB) 评估框架。该基准基于95份中国A股实战年报构建，通过数据源、映射难度和结果单位三个维度对金融查询进行分层，旨在精准诊断高认知负荷场景下的推理退化。为了解决这些失效模式，研究提出了迭代双阶段 Financial-PoT 框架，该 neuro-symbolic 架构通过解耦语义变量提取与逻辑制定，将计算任务外包给具有自我修正能力的 Python 沙盒以确保执行的确定性。在 CCB 上的评估表明，该方法将 Qwen3-235B 模型的平均准确率从 59.7% 提高至 67.3%，并在高难度推理任务中实现了高达10倍的性能增益。研究结果证明，架构解耦是提升金融推理任务可靠性的核心因素，为需要语义理解与定量计算紧密对齐的精密关键领域提供了重要的架构启示。",
      "categories": [
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2601.21157v1",
      "published_date": "2026-01-29 01:33:33 UTC",
      "updated_date": "2026-01-29 01:33:33 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-30T04:36:52.726627+00:00"
    },
    {
      "arxiv_id": "2601.21150v1",
      "title": "Can Neural Networks Learn Small Algebraic Worlds? An Investigation Into the Group-theoretic Structures Learned By Narrow Models Trained To Predict Group Operations",
      "title_zh": "神经网络能学习小型代数世界吗？对训练用于预测群运算的窄模型所获群论结构的探究",
      "authors": [
        "Henry Kvinge",
        "Andrew Aguilar",
        "Nayda Farnsworth",
        "Grace O'Brien",
        "Robert Jasper",
        "Sarah Scullen",
        "Helen Jenne"
      ],
      "abstract": "While a real-world research program in mathematics may be guided by a motivating question, the process of mathematical discovery is typically open-ended. Ideally, exploration needed to answer the original question will reveal new structures, patterns, and insights that are valuable in their own right. This contrasts with the exam-style paradigm in which the machine learning community typically applies AI to math. To maximize progress in mathematics using AI, we will need to go beyond simple question answering. With this in mind, we explore the extent to which narrow models trained to solve a fixed mathematical task learn broader mathematical structure that can be extracted by a researcher or other AI system. As a basic test case for this, we use the task of training a neural network to predict a group operation (for example, performing modular arithmetic or composition of permutations). We describe a suite of tests designed to assess whether the model captures significant group-theoretic notions such as the identity element, commutativity, or subgroups. Through extensive experimentation we find evidence that models learn representations capable of capturing abstract algebraic properties. For example, we find hints that models capture the commutativity of modular arithmetic. We are also able to train linear classifiers that reliably distinguish between elements of certain subgroups (even though no labels for these subgroups are included in the data). On the other hand, we are unable to extract notions such as the concept of the identity element. Together, our results suggest that in some cases the representations of even small neural networks can be used to distill interesting abstract structure from new mathematical objects.",
      "tldr_zh": "该研究探讨了窄模型(Narrow Models)在训练执行固定数学任务时，是否能够习得超越任务本身的广泛数学结构。研究团队通过训练神经网络预测群运算(Group Operation)，并设计了一套测试方案来评估模型对单位元(Identity Element)、交换律(Commutativity)及子群(Subgroups)等群论概念的捕捉能力。实验结果显示，模型学习到的表征能够捕捉抽象代数性质，例如捕捉到了模块算术(Modular Arithmetic)中的交换律，并能通过线性分类器在无标签情况下区分特定的子群元素。尽管研究未能成功提取出单位元概念，但结果证明了即便是小型神经网络也能从数学对象中提炼出有趣的抽象结构。这项工作展示了利用神经网络辅助数学发现的潜力，并强调 AI 在数学领域的研究应从简单的问答范式转向对深层规律的探索。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "Presented at TAG-DS 2025",
      "pdf_url": "https://arxiv.org/pdf/2601.21150v1",
      "published_date": "2026-01-29 01:18:22 UTC",
      "updated_date": "2026-01-29 01:18:22 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-30T04:36:40.661938+00:00"
    },
    {
      "arxiv_id": "2601.21149v1",
      "title": "Mobility-Embedded POIs: Learning What A Place Is and How It Is Used from Human Movement",
      "title_zh": "嵌入移动性的 POI：从人类移动中学习地点的本质与用途",
      "authors": [
        "Maria Despoina Siampou",
        "Shushman Choudhury",
        "Shang-Ling Hsu",
        "Neha Arora",
        "Cyrus Shahabi"
      ],
      "abstract": "Recent progress in geospatial foundation models highlights the importance of learning general-purpose representations for real-world locations, particularly points-of-interest (POIs) where human activity concentrates. Existing approaches, however, focus primarily on place identity derived from static textual metadata, or learn representations tied to trajectory context, which capture movement regularities rather than how places are actually used (i.e., POI's function). We argue that POI function is a missing but essential signal for general POI representations. We introduce Mobility-Embedded POIs (ME-POIs), a framework that augments POI embeddings derived, from language models with large-scale human mobility data to learn POI-centric, context-independent representations grounded in real-world usage. ME-POIs encodes individual visits as temporally contextualized embeddings and aligns them with learnable POI representations via contrastive learning to capture usage patterns across users and time. To address long-tail sparsity, we propose a novel mechanism that propagates temporal visit patterns from nearby, frequently visited POIs across multiple spatial scales. We evaluate ME-POIs on five newly proposed map enrichment tasks, testing its ability to capture both the identity and function of POIs. Across all tasks, augmenting text-based embeddings with ME-POIs consistently outperforms both text-only and mobility-only baselines. Notably, ME-POIs trained on mobility data alone can surpass text-only models on certain tasks, highlighting that POI function is a critical component of accurate and generalizable POI representations.",
      "tldr_zh": "该研究针对现有地理空间基础模型主要依赖静态文本或轨迹背景、忽视兴趣点(POI)功能(function)信号的局限，提出了Mobility-Embedded POIs (ME-POIs)框架。该框架将语言模型生成的POI嵌入与大规模人类移动数据(human mobility data)相结合，旨在学习基于实际用途的、独立于背景的POI表示。ME-POIs将单次访问编码为具有时间背景的嵌入，并通过对比学习(contrastive learning)将其与可学习的POI表示对齐，以捕捉跨用户和时间的动态使用模式。为解决长尾稀疏性(long-tail sparsity)问题，研究提出了一种在多空间尺度上从周边频繁访问的POI传播时间访问模式的新机制。在五项地图增强任务上的评估表明，ME-POIs在捕捉POI身份和功能方面均优于仅依赖文本或移动数据的基线模型。实验结果强调，POI功能是构建准确且通用POI表示的关键组成部分，其在特定任务上的表现甚至能超越纯文本模型。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2601.21149v1",
      "published_date": "2026-01-29 01:12:35 UTC",
      "updated_date": "2026-01-29 01:12:35 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-30T04:36:52.311902+00:00"
    },
    {
      "arxiv_id": "2601.21148v1",
      "title": "BrainStack: Neuro-MoE with Functionally Guided Expert Routing for EEG-Based Language Decoding",
      "title_zh": "BrainStack：融合功能引导专家路由的 Neuro-MoE 脑电语言解码框架",
      "authors": [
        "Ziyi Zhao",
        "Jinzhao Zhou",
        "Xiaowei Jiang",
        "Beining Cao",
        "Wenhao Ma",
        "Yang Shen",
        "Ren Li",
        "Yu-Kai Wang",
        "Chin-teng Lin"
      ],
      "abstract": "Decoding linguistic information from electroencephalography (EEG) remains challenging due to the brain's distributed and nonlinear organization. We present BrainStack, a functionally guided neuro-mixture-of-experts (Neuro-MoE) framework that models the brain's modular functional architecture through anatomically partitioned expert networks. Each functional region is represented by a specialized expert that learns localized neural dynamics, while a transformer-based global expert captures cross-regional dependencies. A learnable routing gate adaptively aggregates these heterogeneous experts, enabling context-dependent expert coordination and selective fusion. To promote coherent representation across the hierarchy, we introduce cross-regional distillation, where the global expert provides top-down regularization to the regional experts. We further release SilentSpeech-EEG (SS-EEG), a large-scale benchmark comprising over 120 hours of EEG recordings from 12 subjects performing 24 silent words, the largest dataset of its kind. Experiments demonstrate that BrainStack consistently outperforms state-of-the-art models, achieving superior accuracy and generalization across subjects. Our results establish BrainStack as a functionally modular, neuro-inspired MoE paradigm that unifies neuroscientific priors with adaptive expert routing, paving the way for scalable and interpretable brain-language decoding.",
      "tldr_zh": "该研究提出了 BrainStack，一种功能引导的神经混合专家 (Neuro-MoE) 框架，通过模拟大脑的模块化功能架构来优化脑电图 (EEG) 语言解码。该框架利用解剖学划分的专家网络学习局部神经动力学，并结合基于 Transformer 的全局专家捕捉跨区域依赖，通过可学习的路由门 (routing gate) 进行自适应专家协调。为了增强表征的一致性，研究引入了跨区域蒸馏 (cross-regional distillation) 技术，由全局专家为局部专家提供正则化指导。此外，研究发布了目前规模最大的静默语音数据集 SilentSpeech-EEG (SS-EEG)，包含来自 12 名受试者的 120 多小时录音。实验结果显示，BrainStack 在准确率和跨受试者泛化性方面均超越了现有最先进模型。这一成果成功将神经科学先验知识与自适应混合专家系统相结合，为实现可扩展且具有解释性的脑语解码提供了新方案。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2601.21148v1",
      "published_date": "2026-01-29 01:04:47 UTC",
      "updated_date": "2026-01-29 01:04:47 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-30T04:36:46.356711+00:00"
    },
    {
      "arxiv_id": "2601.21141v1",
      "title": "Optimization and Mobile Deployment for Anthropocene Neural Style Transfer",
      "title_zh": "面向人类世神经风格迁移的优化与移动端部署",
      "authors": [
        "Po-Hsun Chen",
        "Ivan C. H. Liu"
      ],
      "abstract": "This paper presents AnthropoCam, a mobile-based neural style transfer (NST) system optimized for the visual synthesis of Anthropocene environments. Unlike conventional artistic NST, which prioritizes painterly abstraction, stylizing human-altered landscapes demands a careful balance between amplifying material textures and preserving semantic legibility. Industrial infrastructures, waste accumulations, and modified ecosystems contain dense, repetitive patterns that are visually expressive yet highly susceptible to semantic erosion under aggressive style transfer.\n  To address this challenge, we systematically investigate the impact of NST parameter configurations on the visual translation of Anthropocene textures, including feature layer selection, style and content loss weighting, training stability, and output resolution. Through controlled experiments, we identify an optimal parameter manifold that maximizes stylistic expression while preventing semantic erasure. Our results demonstrate that appropriate combinations of convolutional depth, loss ratios, and resolution scaling enable the faithful transformation of anthropogenic material properties into a coherent visual language.\n  Building on these findings, we implement a low-latency, feed-forward NST pipeline deployed on mobile devices. The system integrates a React Native frontend with a Flask-based GPU backend, achieving high-resolution inference within 3-5 seconds on general mobile hardware. This enables real-time, in-situ visual intervention at the site of image capture, supporting participatory engagement with Anthropocene landscapes.\n  By coupling domain-specific NST optimization with mobile deployment, AnthropoCam reframes neural style transfer as a practical and expressive tool for real-time environmental visualization in the Anthropocene.",
      "tldr_zh": "该研究提出了 AnthropoCam，一种专门针对人类世 (Anthropocene) 环境视觉合成进行优化的移动端神经风格迁移 (Neural Style Transfer, NST) 系统。针对传统 NST 在处理工业基础设施和废弃物堆积等复杂景观时易导致语义侵蚀的问题，研究人员系统地调查了特征层选择、风格与内容损失权重及输出分辨率等参数配置的影响。通过实验确定的最优参数流形 (optimal parameter manifold)，系统能够有效平衡材质纹理的增强与语义清晰度的保留。技术实现上，该系统采用 React Native 前端与 Flask 后端架构，在普通移动设备上实现了 3-5 秒的高分辨率推理延迟。这一成果将领域特定的 NST 优化与移动部署相结合，为人类世景观的实时环境可视化和参与式互动提供了实用的表达工具。",
      "categories": [
        "cs.HC",
        "cs.AI",
        "cs.GR"
      ],
      "primary_category": "cs.HC",
      "comment": "7 pages, 11 figures, submitted to SIGGRAPH 2026",
      "pdf_url": "https://arxiv.org/pdf/2601.21141v1",
      "published_date": "2026-01-29 00:50:03 UTC",
      "updated_date": "2026-01-29 00:50:03 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-30T04:36:52.998717+00:00"
    },
    {
      "arxiv_id": "2601.21130v1",
      "title": "What You Feel Is Not What They See: On Predicting Self-Reported Emotion from Third-Party Observer Labels",
      "title_zh": "所感非所见：基于第三方观察者标注预测自我报告情感的研究",
      "authors": [
        "Yara El-Tawil",
        "Aneesha Sampath",
        "Emily Mower Provost"
      ],
      "abstract": "Self-reported emotion labels capture internal experience, while third-party labels reflect external perception. These perspectives often diverge, limiting the applicability of third-party-trained models to self-report contexts. This gap is critical in mental health, where accurate self-report modeling is essential for guiding intervention. We present the first cross-corpus evaluation of third-party-trained models on self-reports. We find activation unpredictable (CCC approximately 0) and valence moderately predictable (CCC approximately 0.3). Crucially, when content is personally significant to the speaker, models achieve high performance for valence (CCC approximately 0.6-0.8). Our findings point to personal significance as a key pathway for aligning external perception with internal experience and underscore the challenge of self-report activation modeling.",
      "tldr_zh": "该研究探讨了自我报告情绪（内在体验）与第三方观察者标签（外部感知）之间的显著差异，这一差距限制了基于第三方训练的模型在需要准确建模自我报告的心理健康等领域中的应用。作者通过首次跨语料库评估研究了第三方训练模型对自我报告的预测效果，发现情绪唤醒度(activation)几乎无法预测，而情绪效价(valence)具有约0.3的中等预测能力。研究的一个核心发现是，当交谈内容对说话者具有高度个人重要性(personal significance)时，模型在情绪效价(valence)预测上的表现显著提升，其一致性相关系数(CCC)可达0.6至0.8。该结果表明个人重要性是将外部感知与内部体验对齐的关键因素，同时也揭示了对自我报告唤醒度进行建模所面临的严峻挑战。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "ICASSP 2026-2026 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)",
      "pdf_url": "https://arxiv.org/pdf/2601.21130v1",
      "published_date": "2026-01-29 00:07:54 UTC",
      "updated_date": "2026-01-29 00:07:54 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-30T04:36:54.781811+00:00"
    },
    {
      "arxiv_id": "2601.21128v1",
      "title": "Beyond a Single Reference: Training and Evaluation with Paraphrases in Sign Language Translation",
      "title_zh": "超越单一参考：手语翻译中基于释义的训练与评估",
      "authors": [
        "Václav Javorek",
        "Tomáš Železný",
        "Alessa Carbo",
        "Marek Hrúz",
        "Ivan Gruber"
      ],
      "abstract": "Most Sign Language Translation (SLT) corpora pair each signed utterance with a single written-language reference, despite the highly non-isomorphic relationship between sign and spoken languages, where multiple translations can be equally valid. This limitation constrains both model training and evaluation, particularly for n-gram-based metrics such as BLEU. In this work, we investigate the use of Large Language Models to automatically generate paraphrased variants of written-language translations as synthetic alternative references for SLT. First, we compare multiple paraphrasing strategies and models using an adapted ParaScore metric. Second, we study the impact of paraphrases on both training and evaluation of the pose-based T5 model on the YouTubeASL and How2Sign datasets. Our results show that naively incorporating paraphrases during training does not improve translation performance and can even be detrimental. In contrast, using paraphrases during evaluation leads to higher automatic scores and better alignment with human judgments. To formalize this observation, we introduce BLEUpara, an extension of BLEU that evaluates translations against multiple paraphrased references. Human evaluation confirms that BLEUpara correlates more strongly with perceived translation quality. We release all generated paraphrases, generation and evaluation code to support reproducible and more reliable evaluation of SLT systems.",
      "tldr_zh": "手语翻译(Sign Language Translation, SLT)中手语与口语存在高度非同构关系，但现有语料库多采用单一参考译文，限制了模型的训练与评估。本研究探讨利用大语言模型(Large Language Models, LLMs)自动生成书面翻译的改写(paraphrased)变体作为合成参考，并在YouTubeASL和How2Sign数据集上评估了其对基于姿态的T5模型的影响。实验表明，在训练中直接引入改写并不能提升性能，甚至可能产生负面影响，但在评估阶段使用改写参考能显著提高自动评分与人类判断的一致性。基于此发现，研究提出了BLEU的扩展版本BLEUpara，通过多个改写参考来更准确地衡量翻译质量。人类评估证实BLEUpara与感知质量的相关性更强，该研究同时发布了所有生成的改写数据、生成与评估代码，为SLT系统提供了更可靠且可重复的评价基准。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "Under review",
      "pdf_url": "https://arxiv.org/pdf/2601.21128v1",
      "published_date": "2026-01-29 00:02:19 UTC",
      "updated_date": "2026-01-29 00:02:19 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-30T04:36:58.710229+00:00"
    },
    {
      "arxiv_id": "2601.21124v1",
      "title": "PhaseCoder: Microphone Geometry-Agnostic Spatial Audio Understanding for Multimodal LLMs",
      "title_zh": "PhaseCoder：面向多模态大语言模型的麦克风几何结构无关空间音频理解",
      "authors": [
        "Artem Dementyev",
        "Wazeer Zulfikar",
        "Sinan Hersek",
        "Pascal Getreuer",
        "Anurag Kumar",
        "Vivek Kumar"
      ],
      "abstract": "Current multimodal LLMs process audio as a mono stream, ignoring the rich spatial information essential for embodied AI. Existing spatial audio models, conversely, are constrained to fixed microphone geometries, preventing deployment across diverse devices. We present PhaseCoder, a transformer-only spatial audio encoder that is agnostic to microphone geometry. PhaseCoder takes raw multichannel audio and microphone coordinates as inputs to perform localization and produces robust spatial embeddings. We demonstrate that Gemma 3n LLM can be fine-tuned to reason over \"Spatial Audio Tokens\" produced by PhaseCoder. We show our encoder achieves state-of-the-art results on microphone-invariant localization benchmarks and, for the first time, enables an LLM to perform complex spatial reasoning and targeted transcription tasks from an arbitrary microphone array.",
      "tldr_zh": "该研究针对当前多模态大语言模型（Multimodal LLMs）处理单声道音频时缺失空间信息的局限，提出了 PhaseCoder，一种与麦克风几何结构无关（Microphone Geometry-Agnostic）的纯 Transformer 空间音频编码器。该编码器通过输入原始多通道音频及麦克风坐标，实现精确的声源定位并生成鲁棒的空间嵌入（Spatial Embeddings）。研究人员进一步通过微调 Gemma 3n LLM 来处理由 PhaseCoder 产生的“空间音频标记”（Spatial Audio Tokens），使其具备空间推理能力。实验表明，PhaseCoder 在麦克风无关的定位基准测试中达到了最先进（State-of-the-art）水平。该成果首次实现了让大语言模型在任意麦克风阵列配置下执行复杂的空间推理和定向转录任务，为具身人工智能（Embodied AI）的空间音频感知提供了新方案。",
      "categories": [
        "cs.SD",
        "cs.AI",
        "eess.AS"
      ],
      "primary_category": "cs.SD",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2601.21124v1",
      "published_date": "2026-01-28 23:39:31 UTC",
      "updated_date": "2026-01-28 23:39:31 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-30T04:37:05.397296+00:00"
    },
    {
      "arxiv_id": "2601.21123v1",
      "title": "CUA-Skill: Develop Skills for Computer Using Agent",
      "title_zh": "",
      "authors": [
        "Tianyi Chen",
        "Yinheng Li",
        "Michael Solodko",
        "Sen Wang",
        "Nan Jiang",
        "Tingyuan Cui",
        "Junheng Hao",
        "Jongwoo Ko",
        "Sara Abdali",
        "Suzhen Zheng",
        "Leon Xu",
        "Hao Fan",
        "Pashmina Cameron",
        "Justin Wagle",
        "Kazuhito Koishida"
      ],
      "abstract": "Computer-Using Agents (CUAs) aim to autonomously operate computer systems to complete real-world tasks. However, existing agentic systems remain difficult to scale and lag behind human performance. A key limitation is the absence of reusable and structured skill abstractions that capture how humans interact with graphical user interfaces and how to leverage these skills. We introduce CUA-Skill, a computer-using agentic skill base that encodes human computer-use knowledge as skills coupled with parameterized execution and composition graphs. CUA-Skill is a large-scale library of carefully engineered skills spanning common Windows applications, serving as a practical infrastructure and tool substrate for scalable, reliable agent development. Built upon this skill base, we construct CUA-Skill Agent, an end-to-end computer-using agent that supports dynamic skill retrieval, argument instantiation, and memory-aware failure recovery. Our results demonstrate that CUA-Skill substantially improves execution success rates and robustness on challenging end-to-end agent benchmarks, establishing a strong foundation for future computer-using agent development. On WindowsAgentArena, CUA-Skill Agent achieves state-of-the-art 57.5% (best of three) successful rate while being significantly more efficient than prior and concurrent approaches. The project page is available at https://microsoft.github.io/cua_skill/.",
      "tldr_zh": "",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2601.21123v1",
      "published_date": "2026-01-28 23:38:25 UTC",
      "updated_date": "2026-01-28 23:38:25 UTC",
      "processing_status": "failed",
      "attempts": 3,
      "max_attempts": 3,
      "error": "Your request was blocked.",
      "completed_steps": [],
      "llm_backup_calls": 0,
      "last_update": "2026-02-01T04:42:23.220355+00:00"
    },
    {
      "arxiv_id": "2601.21116v1",
      "title": "AI-Assisted Engineering Should Track the Epistemic Status and Temporal Validity of Architectural Decisions",
      "title_zh": "AI 辅助工程应追踪架构决策的认知状态与时效性",
      "authors": [
        "Sankalp Gilda",
        "Shlok Gilda"
      ],
      "abstract": "This position paper argues that AI-assisted software engineering requires explicit mechanisms for tracking the epistemic status and temporal validity of architectural decisions. LLM coding assistants generate decisions faster than teams can validate them, yet no widely-adopted framework distinguishes conjecture from verified knowledge, prevents trust inflation through conservative aggregation, or detects when evidence expires. We propose three requirements for responsible AI-assisted engineering: (1) epistemic layers that separate unverified hypotheses from empirically validated claims, (2) conservative assurance aggregation grounded in the Gödel t-norm that prevents weak evidence from inflating confidence, and (3) automated evidence decay tracking that surfaces stale assumptions before they cause failures. We formalize these requirements as the First Principles Framework (FPF), ground its aggregation semantics in fuzzy logic, and define a quintet of invariants that any valid aggregation operator must satisfy. Our retrospective audit applying FPF criteria to two internal projects found that 20-25% of architectural decisions had stale evidence within two months, validating the need for temporal accountability. We outline research directions including learnable aggregation operators, federated evidence sharing, and SMT-based claim validation.",
      "tldr_zh": "该立场论文指出，AI辅助软件工程（AI-assisted software engineering）需要明确的机制来追踪架构决策的认识论状态（epistemic status）和时间有效性（temporal validity），以应对LLM编程助手生成决策过快导致的信任膨胀问题。作者提出了第一原则框架（First Principles Framework, FPF），要求通过认识论层（epistemic layers）区分未验证假设与经验证主张，并利用基于哥德尔t-范数（Gödel t-norm）的模糊逻辑实现保守的保证聚合。此外，该框架引入了自动证据衰减（evidence decay）追踪机制，以识别失效的陈旧假设。通过对两个内部项目的回顾性审计，研究发现20-25%的架构决策在两个月内即出现证据陈旧现象，验证了时间问责制的必要性。该工作为负责任的AI辅助工程奠定了形式化基础，并指出了可学习聚合算子与SMT验证等未来研究方向。",
      "categories": [
        "cs.SE",
        "cs.AI"
      ],
      "primary_category": "cs.SE",
      "comment": "18 pages, 6 figures, 13 tables. Position paper",
      "pdf_url": "https://arxiv.org/pdf/2601.21116v1",
      "published_date": "2026-01-28 23:12:07 UTC",
      "updated_date": "2026-01-28 23:12:07 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-30T04:37:58.460934+00:00"
    },
    {
      "arxiv_id": "2601.21115v1",
      "title": "Multi-task Code LLMs: Data Mix or Model Merge?",
      "title_zh": "多任务代码大语言模型：数据混合还是模型合并？",
      "authors": [
        "Mingzhi Zhu",
        "Boris Sobolev",
        "Rahul Krishna",
        "Raju Pavuluri",
        "Stacy Patterson",
        "Michele Merler"
      ],
      "abstract": "Recent research advocates deploying smaller, specialized code LLMs in agentic frameworks alongside frontier models, sparking interest in efficient strategies for multi-task learning that balance performance, constraints, and costs. We compare two approaches for creating small, multi-task code LLMs: data mixing versus model merging. We conduct extensive experiments across two model families (Qwen Coder and DeepSeek Coder) at two scales (2B and 7B parameters), fine-tuning them for code generation and code summarization tasks. Our evaluation on HumanEval, MBPP, and CodeXGlue benchmarks reveals that model merging achieves the best overall performance at larger scale across model families, retaining 96% of specialized model performance on code generation tasks while maintaining summarization capabilities. Notably, merged models can even surpass individually fine-tuned models, with our best configuration of Qwen Coder 2.5 7B model achieving 92.7% Pass@1 on HumanEval compared to 90.9% for its task-specific fine-tuned equivalent. At a smaller scale we find instead data mixing to be a preferred strategy. We further introduce a weight analysis technique to understand how different tasks affect model parameters and their implications for merging strategies. The results suggest that careful merging and mixing strategies can effectively combine task-specific capabilities without significant performance degradation, making them ideal for resource-constrained deployment scenarios.",
      "tldr_zh": "该研究探讨了构建小型多任务代码大模型 (Code LLMs) 的两种核心策略：数据混合 (Data Mixing) 与模型合并 (Model Merging)。通过在 Qwen Coder 和 DeepSeek Coder 系列（2B 和 7B 规模）上进行代码生成 (Code Generation) 与代码摘要 (Code Summarization) 的广泛实验，研究发现策略的选择受模型规模影响显著。在 7B 规模下，模型合并展现出最佳综合性能，能够保留专项模型 96% 的代码生成能力，部分配置（如 Qwen Coder 2.5 7B）甚至在 HumanEval 评测中超越了单任务微调模型。而在 2B 较小规模下，数据混合被证明是更优的策略。此外，研究还提出了一种权重分析技术，用以量化不同任务对模型参数的具体影响，为理解合并机制提供了新视角。这些结论为在资源受限的场景中高效部署具备多任务能力的智能体提供了重要指导。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2601.21115v1",
      "published_date": "2026-01-28 23:06:09 UTC",
      "updated_date": "2026-01-28 23:06:09 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-30T04:37:57.856006+00:00"
    },
    {
      "arxiv_id": "2601.21113v1",
      "title": "Planner-Auditor Twin: Agentic Discharge Planning with FHIR-Based LLM Planning, Guideline Recall, Optional Caching and Self-Improvement",
      "title_zh": "Planner-Auditor Twin：基于 FHIR 的大语言模型规划、指南召回、可选缓存及自我改进的智能体出院计划",
      "authors": [
        "Kaiyuan Wu",
        "Aditya Nagori",
        "Rishikesan Kamaleswaran"
      ],
      "abstract": "Objective: Large language models (LLMs) show promise for clinical discharge planning, but their use is constrained by hallucination, omissions, and miscalibrated confidence. We introduce a self-improving, cache-optional Planner-Auditor framework that improves safety and reliability by decoupling generation from deterministic validation and targeted replay.\n  Materials and Methods: We implemented an agentic, retrospective, FHIR-native evaluation pipeline using MIMIC-IV-on-FHIR. For each patient, the Planner (LLM) generates a structured discharge action plan with an explicit confidence estimate. The Auditor is a deterministic module that evaluates multi-task coverage, tracks calibration (Brier score, ECE proxies), and monitors action-distribution drift. The framework supports two-tier self-improvement: (i) within-episode regeneration when enabled, and (ii) cross-episode discrepancy buffering with replay for high-confidence, low-coverage cases.\n  Results: While context caching improved performance over baseline, the self-improvement loop was the primary driver of gains, increasing task coverage from 32% to 86%. Calibration improved substantially, with reduced Brier/ECE and fewer high-confidence misses. Discrepancy buffering further corrected persistent high-confidence omissions during replay.\n  Discussion: Feedback-driven regeneration and targeted replay act as effective control mechanisms to reduce omissions and improve confidence reliability in structured clinical planning. Separating an LLM Planner from a rule-based, observational Auditor enables systematic reliability measurement and safer iteration without model retraining.\n  Conclusion: The Planner-Auditor framework offers a practical pathway toward safer automated discharge planning using interoperable FHIR data access and deterministic auditing, supported by reproducible ablations and reliability-focused evaluation.",
      "tldr_zh": "该研究针对大语言模型(LLMs)在临床出院规划中面临的幻觉、遗漏及置信度校准偏差等问题，提出了Planner-Auditor双生架构。该框架将Planner（用于生成带有置信度估计的结构化出院计划）与确定的Auditor（用于评估任务覆盖率和监测校准指标）解耦，并基于FHIR标准实现了代理式评估管线。系统支持两级自我改进机制，涵盖了单次会话内的重新生成以及跨会话的差异缓冲与针对性回放。实验表明，该自我改进循环将任务覆盖率从32%显著提升至86%，并显著优化了Brier score和ECE等校准指标，有效减少了高置信度下的错误遗漏。研究证明，通过解耦LLM生成与规则审计，可以在无需模型重训的情况下，实现更安全且具有高度可靠性的自动化临床规划。",
      "categories": [
        "cs.AI",
        "cs.MA"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2601.21113v1",
      "published_date": "2026-01-28 23:04:11 UTC",
      "updated_date": "2026-01-28 23:04:11 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-30T04:37:56.380624+00:00"
    },
    {
      "arxiv_id": "2601.21112v1",
      "title": "How does information access affect LLM monitors' ability to detect sabotage?",
      "title_zh": "信息获取如何影响 LLM 监控器检测破坏行为的能力？",
      "authors": [
        "Rauno Arike",
        "Raja Mehta Moreno",
        "Rohan Subramani",
        "Shubhorup Biswas",
        "Francis Rhys Ward"
      ],
      "abstract": "Frontier language model agents can exhibit misaligned behaviors, including deception, exploiting reward hacks, and pursuing hidden objectives. To control potentially misaligned agents, we can use LLMs themselves to monitor for misbehavior. In this paper, we study how information access affects LLM monitor performance. While one might expect that monitors perform better when they have access to more of the monitored agents' reasoning and actions, we find that contemporary systems often perform better with less information, a phenomenon we call the less-is-more effect for automated oversight. We demonstrate this phenomenon, and analyze the conditions under which it occurs, in three evaluation environments where agents must conduct sabotage while evading monitors. Motivated by the less-is-more effect, we introduce extract-and-evaluate (EaE) monitoring--a new hierarchical approach where one monitor isolates relevant excerpts from the monitored agent's trajectory and a separate monitor scores them. In BigCodeBench-Sabotage with GPT-4.1-mini as the monitor model, EaE improves sabotage detection rates by 16.8 percentage points over the next-best monitor without increasing the false positive rate. In other settings, EaE either outperforms or is competitive with baselines. In addition, we find that agents unaware of being monitored can be caught much more easily and that monitors scoring longer reasoning traces gain more from information filtering. Lastly, we conduct a cost-performance analysis and find that Gemini and Claude models with monitoring techniques that involve information filtering occupy much of the Pareto frontier.",
      "tldr_zh": "该研究探讨了信息获取如何影响大语言模型(LLMs)监测器检测代理(Agents)破坏行为(Sabotage)的能力，特别关注模型表现出的欺骗和隐蔽目标等失配行为。作者发现了一个被称为“less-is-more effect”的现象，即当监测器接触到被监测代理较少的推理和行动信息时，其自动化监管(Automated Oversight)的效果反而更好。针对这一发现，研究提出了提取-评估(extract-and-evaluate, EaE)监测方法，通过层次化架构将相关片段隔离与打分任务分离。实验结果显示，在BigCodeBench-Sabotage基准测试中，使用GPT-4.1-mini作为监测器时，EaE将破坏检测率提升了16.8个百分点，且未增加误报率。此外，研究指出未意识到被监控的代理更容易被发现，且信息过滤对处理长推理轨迹的监测器至关重要。最后的成本绩效分析证明，采用信息过滤技术的Gemini和Claude模型在帕累托前沿(Pareto frontier)表现优异。",
      "categories": [
        "cs.AI",
        "cs.SE"
      ],
      "primary_category": "cs.AI",
      "comment": "62 pages, 34 figures, 7 tables",
      "pdf_url": "https://arxiv.org/pdf/2601.21112v1",
      "published_date": "2026-01-28 23:01:31 UTC",
      "updated_date": "2026-01-28 23:01:31 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-30T04:37:57.145604+00:00"
    },
    {
      "arxiv_id": "2601.21105v1",
      "title": "SteerEval: A Framework for Evaluating Steerability with Natural Language Profiles for Recommendation",
      "title_zh": "SteerEval：基于自然语言画像的推荐系统可控性评估框架",
      "authors": [
        "Joyce Zhou",
        "Weijie Zhou",
        "Doug Turnbull",
        "Thorsten Joachims"
      ],
      "abstract": "Natural-language user profiles have recently attracted attention not only for improved interpretability, but also for their potential to make recommender systems more steerable. By enabling direct editing, natural-language profiles allow users to explicitly articulate preferences that may be difficult to infer from past behavior. However, it remains unclear whether current natural-language-based recommendation methods can follow such steering commands. While existing steerability evaluations have shown some success for well-recognized item attributes (e.g., movie genres), we argue that these benchmarks fail to capture the richer forms of user control that motivate steerable recommendations. To address this gap, we introduce SteerEval, an evaluation framework designed to measure more nuanced and diverse forms of steerability by using interventions that range from genres to content-warning for movies. We assess the steerability of a family of pretrained natural-language recommenders, examine the potential and limitations of steering on relatively niche topics, and compare how different profile and recommendation interventions impact steering effectiveness. Finally, we offer practical design suggestions informed by our findings and discuss future steps in steerable recommender design.",
      "tldr_zh": "该研究提出了SteerEval，这是一个旨在评估推荐系统可控性(Steerability)的新型框架，重点关注自然语言用户画像(Natural-language user profiles)对推荐结果的引导能力。尽管自然语言画像能让用户直接表达难以从历史行为中推断的偏好，但现有的评估基准通常仅限于电影类型等简单属性，无法涵盖更丰富的用户控制需求。SteerEval通过引入从基础类别到内容警告(Content-warning)等更具细微差别且多样化的干预手段，填补了这一评估空白。通过对一系列预训练自然语言推荐模型进行测试，该研究探讨了模型在冷门话题上的控制潜力与局限，并对比了不同画像和推荐干预措施对引导效果的影响。最终，研究团队根据实验发现提供了实用的系统设计建议，为未来可控推荐系统的研发与优化奠定了基础。",
      "categories": [
        "cs.IR",
        "cs.AI",
        "cs.CL",
        "cs.HC"
      ],
      "primary_category": "cs.IR",
      "comment": "10 pages, 2 figures, 8 tables. Pre-print",
      "pdf_url": "https://arxiv.org/pdf/2601.21105v1",
      "published_date": "2026-01-28 22:54:35 UTC",
      "updated_date": "2026-01-28 22:54:35 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-30T04:38:04.200458+00:00"
    },
    {
      "arxiv_id": "2601.21096v1",
      "title": "Magellan: Autonomous Discovery of Novel Compiler Optimization Heuristics with AlphaEvolve",
      "title_zh": "Magellan：基于 AlphaEvolve 自主发现新型编译器优化启发式算法",
      "authors": [
        "Hongzheng Chen",
        "Alexander Novikov",
        "Ngân Vũ",
        "Hanna Alam",
        "Zhiru Zhang",
        "Aiden Grossman",
        "Mircea Trofin",
        "Amir Yazdanbakhsh"
      ],
      "abstract": "Modern compilers rely on hand-crafted heuristics to guide optimization passes. These human-designed rules often struggle to adapt to the complexity of modern software and hardware and lead to high maintenance burden. To address this challenge, we present Magellan, an agentic framework that evolves the compiler pass itself by synthesizing executable C++ decision logic. Magellan couples an LLM coding agent with evolutionary search and autotuning in a closed loop of generation, evaluation on user-provided macro-benchmarks, and refinement, producing compact heuristics that integrate directly into existing compilers. Across several production optimization tasks, Magellan discovers policies that match or surpass expert baselines. In LLVM function inlining, Magellan synthesizes new heuristics that outperform decades of manual engineering for both binary-size reduction and end-to-end performance. In register allocation, it learns a concise priority rule for live-range processing that matches intricate human-designed policies on a large-scale workload. We also report preliminary results on XLA problems, demonstrating portability beyond LLVM with reduced engineering effort.",
      "tldr_zh": "该研究提出了 Magellan，这是一个基于 AlphaEvolve 的智能体框架，旨在通过自动合成可执行的 C++ 决策逻辑来进化编译器优化启发式算法，从而解决传统手动设计规则难以适应复杂软硬件且维护成本高的问题。Magellan 将大语言模型 (LLM) 编程智能体与进化搜索 (Evolutionary Search) 及自动调优 (Autotuning) 相结合，构建了一个包含生成、宏基准测试评估和持续优化的闭环系统。在 LLVM 的函数内联 (Function Inlining) 任务中，Magellan 合成的启发式算法在二进制文件缩减和端到端性能表现上均超越了数十年的人工工程积累。在寄存器分配 (Register Allocation) 任务中，该框架学习到了简洁的活跃范围处理优先级规则，能够在大规模工作负载下匹配人类设计的复杂策略。此外，Magellan 在 XLA 问题上的初步实验证明了其跨编译器的可移植性，显著降低了开发成本。实验结果表明，Magellan 发现的策略在多项生产级优化任务中均能匹配或超越专家基准。",
      "categories": [
        "cs.AI",
        "cs.LG",
        "cs.PL"
      ],
      "primary_category": "cs.AI",
      "comment": "Accepted to C4ML@CGO'26",
      "pdf_url": "https://arxiv.org/pdf/2601.21096v1",
      "published_date": "2026-01-28 22:34:56 UTC",
      "updated_date": "2026-01-28 22:34:56 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-30T04:38:14.347622+00:00"
    },
    {
      "arxiv_id": "2601.21095v1",
      "title": "Responsible AI: The Good, The Bad, The AI",
      "title_zh": "负责任的人工智能：利、弊与 AI",
      "authors": [
        "Akbar Anbar Jafari",
        "Cagri Ozcinar",
        "Gholamreza Anbarjafari"
      ],
      "abstract": "The rapid proliferation of artificial intelligence across organizational contexts has generated profound strategic opportunities while introducing significant ethical and operational risks. Despite growing scholarly attention to responsible AI, extant literature remains fragmented and is often adopting either an optimistic stance emphasizing value creation or an excessively cautious perspective fixated on potential harms. This paper addresses this gap by presenting a comprehensive examination of AI's dual nature through the lens of strategic information systems. Drawing upon a systematic synthesis of the responsible AI literature and grounded in paradox theory, we develop the Paradox-based Responsible AI Governance (PRAIG) framework that articulates: (1) the strategic benefits of AI adoption, (2) the inherent risks and unintended consequences, and (3) governance mechanisms that enable organizations to navigate these tensions. Our framework advances theoretical understanding by conceptualizing responsible AI governance as the dynamic management of paradoxical tensions between value creation and risk mitigation. We provide formal propositions demonstrating that trade-off approaches amplify rather than resolve these tensions, and we develop a taxonomy of paradox management strategies with specified contingency conditions. For practitioners, we offer actionable guidance for developing governance structures that neither stifle innovation nor expose organizations to unacceptable risks. The paper concludes with a research agenda for advancing responsible AI governance scholarship.",
      "tldr_zh": "该研究探讨了人工智能在组织背景下快速普及带来的战略机遇与伦理及运营风险，指出当前关于 Responsible AI 的文献在价值创造与潜在危害之间存在碎片化倾向。基于 Paradox Theory 和战略信息系统视角，本文提出了 Paradox-based Responsible AI Governance (PRAIG) 框架，旨在系统性地审视 AI 的双重性质。该框架详细阐述了 AI 采纳的战略效益、固有风险与非预期后果，以及使组织能够应对这些紧张关系的治理机制。通过将 Responsible AI 治理概念化为价值创造与风险缓解之间矛盾关系的动态管理，该研究显著提升了相关领域的理论认知。研究通过正式命题证明，传统的折中(trade-off)方法会放大而非解决这些矛盾，并为此开发了一套具有特定权变条件的悖论管理策略分类法。最后，该研究为从业者提供了开发治理结构的可操作指导，确保在推动创新的同时有效管控组织风险。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "14 pages, 5 figures",
      "pdf_url": "https://arxiv.org/pdf/2601.21095v1",
      "published_date": "2026-01-28 22:33:27 UTC",
      "updated_date": "2026-01-28 22:33:27 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-30T04:38:12.006271+00:00"
    },
    {
      "arxiv_id": "2601.21094v1",
      "title": "Safety Generalization Under Distribution Shift in Safe Reinforcement Learning: A Diabetes Testbed",
      "title_zh": "安全强化学习在分布偏移下的安全泛化：以糖尿病为测试平台",
      "authors": [
        "Minjae Kwon",
        "Josephine Lamp",
        "Lu Feng"
      ],
      "abstract": "Safe Reinforcement Learning (RL) algorithms are typically evaluated under fixed training conditions. We investigate whether training-time safety guarantees transfer to deployment under distribution shift, using diabetes management as a safety-critical testbed. We benchmark safe RL algorithms on a unified clinical simulator and reveal a safety generalization gap: policies satisfying constraints during training frequently violate safety requirements on unseen patients. We demonstrate that test-time shielding, which filters unsafe actions using learned dynamics models, effectively restores safety across algorithms and patient populations. Across eight safe RL algorithms, three diabetes types, and three age groups, shielding achieves Time-in-Range gains of 13--14\\% for strong baselines such as PPO-Lag and CPO while reducing clinical risk index and glucose variability. Our simulator and benchmark provide a platform for studying safety under distribution shift in safety-critical control domains. Code is available at https://github.com/safe-autonomy-lab/GlucoSim and https://github.com/safe-autonomy-lab/GlucoAlg.",
      "tldr_zh": "本研究探讨了安全强化学习 (Safe Reinforcement Learning) 算法在分布偏移 (Distribution Shift) 下的安全性泛化问题，并以糖尿病管理作为关键安全测试场景。通过在统一的临床模拟器上对八种安全强化学习算法进行基准测试，研究揭示了显著的“安全性泛化差距” (Safety Generalization Gap)，即在训练期间满足约束的策略在未见过的患者身上频繁违反安全要求。为应对这一挑战，研究提出了一种测试时屏蔽 (Test-time Shielding) 机制，利用学习到的动力学模型过滤不安全动作。实验表明，该机制在不同算法和患者群体中均能有效恢复安全性，使 PPO-Lag 和 CPO 等强基线模型的范围内时间 (Time-in-Range) 提升了 13-14%。此外，该方案显著降低了临床风险指标和血糖波动性，验证了其在安全关键控制领域的有效性。该研究还开源了 GlucoSim 模拟器和 GlucoAlg 基准平台，为评估安全强化学习的鲁棒性提供了重要工具。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "eess.SY"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2601.21094v1",
      "published_date": "2026-01-28 22:28:17 UTC",
      "updated_date": "2026-01-28 22:28:17 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-30T04:38:18.027427+00:00"
    },
    {
      "arxiv_id": "2601.21090v1",
      "title": "Deep Reinforcement Learning for Fault-Adaptive Routing in Eisenstein-Jacobi Interconnection Topologies",
      "title_zh": "Eisenstein-Jacobi 互连拓扑中故障自适应路由的深度强化学习",
      "authors": [
        "Mohammad Walid Charrwi",
        "Zaid Hussain"
      ],
      "abstract": "The increasing density of many-core architectures necessitates interconnection networks that are both high-performance and fault-resilient. Eisenstein-Jacobi (EJ) networks, with their symmetric 6-regular topology, offer superior topological properties but challenge traditional routing heuristics under fault conditions. This paper evaluates three routing paradigms in faulty EJ environments: deterministic Greedy Adaptive Routing, theoretically optimal Dijkstra's algorithm, and a reinforcement learning (RL)-based approach. Using a multi-objective reward function to penalize fault proximity and reward path efficiency, the RL agent learns to navigate around clustered failures that typically induce dead-ends in greedy geometric routing. Dijkstra's algorithm establishes the theoretical performance ceiling by computing globally optimal paths with complete topology knowledge, revealing the true connectivity limits of faulty networks. Quantitative analysis at nine faulty nodes shows greedy routing catastrophically degrades to 10% effective reachability and packet delivery, while Dijkstra proves 52-54% represents the topological optimum. The RL agent achieves 94% effective reachability and 91% packet delivery, making it suitable for distributed deployment. Furthermore, throughput evaluations demonstrate that RL sustains over 90% normalized throughput across all loads, actually outperforming Dijkstra under congestion through implicit load balancing strategies. These results establish RL-based adaptive policies as a practical solution that bridges the gap between greedy's efficiency and Dijkstra's optimality, providing robust, self-healing communication in fault-prone interconnection networks without requiring the global topology knowledge or computational overhead of optimal algorithms.",
      "tldr_zh": "该研究探讨了 Eisenstein-Jacobi (EJ) 互连网络在故障环境下的路由挑战，对比了贪婪自适应路由 (Greedy Adaptive Routing)、Dijkstra 算法以及深度强化学习 (Deep Reinforcement Learning, RL) 三种范式。通过设计多目标奖励函数，RL 智能体学会了规避导致贪婪路由失效的聚集性故障，且无需全局拓扑知识即可实现分布式部署。实验表明，在多节点故障情况下，RL 智能体实现了 94% 的有效可达性和 91% 的数据包投递率，远优于仅有 10% 可达性的贪婪路由。尽管 Dijkstra 算法确立了理论上的拓扑性能上限，但 RL 依靠隐式负载均衡策略在拥塞情况下的吞吐量表现甚至超越了 Dijkstra。该研究证明了基于 RL 的自适应策略能有效弥补传统算法间的性能鸿沟，为容错互连网络提供了一种鲁棒且自愈的实用通信方案。",
      "categories": [
        "cs.DC",
        "cs.AI"
      ],
      "primary_category": "cs.DC",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2601.21090v1",
      "published_date": "2026-01-28 22:25:22 UTC",
      "updated_date": "2026-01-28 22:25:22 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-30T04:38:20.773755+00:00"
    },
    {
      "arxiv_id": "2601.21083v1",
      "title": "OpenSec: Measuring Incident Response Agent Calibration Under Adversarial Evidence",
      "title_zh": "OpenSec：对抗性证据下的应急响应智能体校准评估",
      "authors": [
        "Jarrod Barnes"
      ],
      "abstract": "As large language models improve, so do their offensive applications: frontier agents now generate working exploits for under $50 in compute (Heelan, 2026). Defensive incident response (IR) agents must keep pace, but existing benchmarks conflate action execution with correct execution, hiding calibration failures when agents process adversarial evidence. We introduce OpenSec, a dual-control reinforcement learning environment that evaluates IR agents under realistic prompt injection scenarios. Unlike static capability benchmarks, OpenSec scores world-state-changing containment actions under adversarial evidence via execution-based metrics: time-to-first-containment (TTFC), blast radius (false positives per episode), and injection violation rates. Evaluating four frontier models on 40 standard-tier episodes, we find consistent over-triggering in this setting: GPT-5.2, Gemini 3, and DeepSeek execute containment in 100% of episodes with 90-97% false positive rates. Claude Sonnet 4.5 shows partial calibration (85% containment, 72% FP), demonstrating that OpenSec surfaces a calibration failure mode hidden by aggregate success metrics. Code available at https://github.com/jbarnes850/opensec-env.",
      "tldr_zh": "该研究指出，随着大型语言模型(LLMs)攻击能力的增强，现有的基准测试往往混淆了行动执行与正确执行，掩盖了事件响应(Incident Response, IR)智能体在处理对抗性证据时的校准(Calibration)失效。为此，研究者开发了OpenSec，一个双控制强化学习(Reinforcement Learning)环境，旨在评估IR智能体在现实提示注入(Prompt Injection)场景下的表现。该系统通过首次容纳时间(TTFC)、影响范围(Blast Radius)及注入违规率等关键指标，对智能体在改变世界状态的容纳行动进行执行层面的量化。实验结果显示，GPT-5.2、Gemini 3和DeepSeek在测试中普遍存在过度触发问题，尽管执行了容纳操作，但其误报率(False Positive Rates)高达90-97%。相比之下，Claude Sonnet 4.5表现出部分校准能力，其误报率为72%，这进一步证明了OpenSec能够揭示被传统综合指标所掩盖的风险模式。该研究为可信的防御性智能体评估提供了新的框架和开源工具，有助于解决网络安全领域中的校准失败问题。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "6 pages, 2 figures, 3 tables. Code: https://github.com/jbarnes850/opensec-env",
      "pdf_url": "https://arxiv.org/pdf/2601.21083v1",
      "published_date": "2026-01-28 22:12:54 UTC",
      "updated_date": "2026-01-28 22:12:54 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-30T04:38:31.755178+00:00"
    },
    {
      "arxiv_id": "2601.21082v1",
      "title": "LOCUS: Low-Dimensional Model Embeddings for Efficient Model Exploration, Comparison, and Selection",
      "title_zh": "LOCUS：用于高效模型探索、对比与选择的低维模型嵌入",
      "authors": [
        "Shivam Patel",
        "William Cocke",
        "Gauri Joshi"
      ],
      "abstract": "The rapidly growing ecosystem of Large Language Models (LLMs) makes it increasingly challenging to manage and utilize the vast and dynamic pool of models effectively. We propose LOCUS, a method that produces low-dimensional vector embeddings that compactly represent a language model's capabilities across queries. LOCUS is an attention-based approach that generates embeddings by a deterministic forward pass over query encodings and evaluation scores via an encoder model, enabling seamless incorporation of new models to the pool and refinement of existing model embeddings without having to perform any retraining. We additionally train a correctness predictor that uses model embeddings and query encodings to achieve state-of-the-art routing accuracy on unseen queries. Experiments show that LOCUS needs up to 4.8x fewer query evaluation samples than baselines to produce informative and robust embeddings. Moreover, the learned embedding space is geometrically meaningful: proximity reflects model similarity, enabling a range of downstream applications including model comparison and clustering, model portfolio selection, and resilient proxies of unavailable models.",
      "tldr_zh": "该研究提出了LOCUS，一种旨在生成低维模型嵌入(low-dimensional model embeddings)的方法，用于紧凑地表示大语言模型(LLMs)在不同查询下的能力。LOCUS采用基于注意力机制的编码器(encoder model)，通过对查询编码和评估分数的确定性前向传播生成嵌入，支持在不进行重新训练的情况下无缝集成新模型并优化现有嵌入。研究还进一步训练了一个正确性预测器(correctness predictor)，利用模型嵌入和查询编码在未知查询上实现了最先进的路由(routing)准确率。实验表明，LOCUS生成具有信息量且鲁棒的嵌入所需的查询评估样本比基线方法减少了4.8倍。此外，LOCUS学习到的嵌入空间在几何上具有实际意义，其空间邻近性能够直观反映模型相似度，从而支持模型比较、聚类、模型组合选择以及作为不可用模型的弹性代理(resilient proxies)等多种下游应用。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2601.21082v1",
      "published_date": "2026-01-28 22:09:42 UTC",
      "updated_date": "2026-01-28 22:09:42 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-30T04:38:43.843867+00:00"
    },
    {
      "arxiv_id": "2601.21076v1",
      "title": "Multi-modal Imputation for Alzheimer's Disease Classification",
      "title_zh": "用于阿尔茨海默病分类的多模态插补",
      "authors": [
        "Abhijith Shaji",
        "Tamoghna Chattopadhyay",
        "Sophia I. Thomopoulos",
        "Greg Ver Steeg",
        "Paul M. Thompson",
        "Jose-Luis Ambite"
      ],
      "abstract": "Deep learning has been successful in predicting neurodegenerative disorders, such as Alzheimer's disease, from magnetic resonance imaging (MRI). Combining multiple imaging modalities, such as T1-weighted (T1) and diffusion-weighted imaging (DWI) scans, can increase diagnostic performance. However, complete multimodal datasets are not always available. We use a conditional denoising diffusion probabilistic model to impute missing DWI scans from T1 scans. We perform extensive experiments to evaluate whether such imputation improves the accuracy of uni-modal and bi-modal deep learning models for 3-way Alzheimer's disease classification-cognitively normal, mild cognitive impairment, and Alzheimer's disease. We observe improvements in several metrics, particularly those sensitive to minority classes, for several imputation configurations.",
      "tldr_zh": "该研究针对阿尔茨海默病(Alzheimer's disease)分类中多模态数据集不完整的问题，提出了一种多模态影像补全方案。研究者利用条件去噪扩散概率模型(conditional denoising diffusion probabilistic model)从T1加权(T1)扫描图像中生成缺失的弥散加权成像(DWI)数据。实验通过对认知正常、轻度认知障碍和阿尔茨海默病进行三分类任务，系统评估了补全数据对单模态及双模态深度学习模型性能的影响。结果表明，在多种补全配置下，模型的分类准确率在多个指标上均有所提升，尤其是在对少数类别的识别灵敏度方面表现显著。该方法有效解决了临床实践中多模态扫描缺失的挑战，为利用不完整影像数据进行神经退行性疾病的精准诊断提供了新的技术路径。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2601.21076v1",
      "published_date": "2026-01-28 22:02:45 UTC",
      "updated_date": "2026-01-28 22:02:45 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-30T04:38:44.787686+00:00"
    },
    {
      "arxiv_id": "2601.21070v1",
      "title": "Towards Comprehensive Benchmarking Infrastructure for LLMs In Software Engineering",
      "title_zh": "迈向软件工程领域大语言模型的全面基准评测基础设施",
      "authors": [
        "Daniel Rodriguez-Cardenas",
        "Xiaochang Li",
        "Marcos Macedo",
        "Antonio Mastropaolo",
        "Dipin Khati",
        "Yuan Tian",
        "Huajie Shao",
        "Denys Poshyvanyk"
      ],
      "abstract": "Large language models for code are advancing fast, yet our ability to evaluate them lags behind. Current benchmarks focus on narrow tasks and single metrics, which hide critical gaps in robustness, interpretability, fairness, efficiency, and real-world usability. They also suffer from inconsistent data engineering practices, limited software engineering context, and widespread contamination issues. To understand these problems and chart a path forward, we combined an in-depth survey of existing benchmarks with insights gathered from a dedicated community workshop. We identified three core barriers to reliable evaluation: the absence of software-engineering-rich datasets, overreliance on ML-centric metrics, and the lack of standardized, reproducible data pipelines. Building on these findings, we introduce BEHELM, a holistic benchmarking infrastructure that unifies software-scenario specification with multi-metric evaluation. BEHELM provides a structured way to assess models across tasks, languages, input and output granularities, and key quality dimensions. Our goal is to reduce the overhead currently required to construct benchmarks while enabling a fair, realistic, and future-proof assessment of LLMs in software engineering.",
      "tldr_zh": "该研究探讨了软件工程领域中大语言模型 (LLMs) 评估手段滞后的现状，指出当前基准测试 (benchmarks) 存在鲁棒性不足、评估维度单一及数据污染 (data contamination) 等关键缺陷。通过结合深度调查与社区研讨会，研究人员识别出可靠评估的三大核心障碍：缺乏富有软件工程背景的数据集、过度依赖机器学习指标以及缺乏标准化且可复现的数据流水线。针对这些挑战，研究团队推出了 BEHELM，这是一种将软件场景规范与多指标评估相统一的整体基准测试基础设施。BEHELM 提供了一种结构化方法，支持跨任务、编程语言及输入输出粒度对模型进行多维度质量评估。该框架旨在降低基准测试构建的间接成本，同时为软件工程领域的 LLMs 提供更加公平、真实且具备前瞻性的评估标准。",
      "categories": [
        "cs.SE",
        "cs.AI"
      ],
      "primary_category": "cs.SE",
      "comment": "Short paper from bechmarking for software engineering workshop FSE2025",
      "pdf_url": "https://arxiv.org/pdf/2601.21070v1",
      "published_date": "2026-01-28 21:55:10 UTC",
      "updated_date": "2026-01-28 21:55:10 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-30T04:38:51.744776+00:00"
    },
    {
      "arxiv_id": "2601.21064v1",
      "title": "Textual Equilibrium Propagation for Deep Compound AI Systems",
      "title_zh": "面向深层复合 AI 系统的文本平衡传播",
      "authors": [
        "Minghui Chen",
        "Wenlong Deng",
        "James Zou",
        "Han Yu",
        "Xiaoxiao Li"
      ],
      "abstract": "Large language models (LLMs) are increasingly deployed as part of compound AI systems that coordinate multiple modules (e.g., retrievers, tools, verifiers) over long-horizon workflows. Recent approaches that propagate textual feedback globally (e.g., TextGrad) make it feasible to optimize such pipelines, but we find that performance degrades as system depth grows. In particular, long-horizon agentic workflows exhibit two depth-scaling failure modes: 1) exploding textual gradient, where textual feedback grows exponentially with depth, leading to prohibitively long message and amplifies evaluation biases; and 2) vanishing textual gradient, where limited long-context ability causes models overemphasize partial feedback and compression of lengthy feedback causes downstream messages to lose specificity gradually as they propagate many hops upstream. To mitigate these issues, we introduce Textual Equilibrium Propagation (TEP), a local learning principle inspired by Equilibrium Propagation in energy-based models. TEP includes two phases: 1) a free phase where a local LLM critics iteratively refine prompts until reaching equilibrium (no further improvements are suggested); and 2) a nudged phase which applies proximal prompt edits with bounded modification intensity, using task-level objectives that propagate via forward signaling rather than backward feedback chains. This design supports local prompt optimization followed by controlled adaptation toward global goals without the computational burden and signal degradation of global textual backpropagation. Across long-horizon QA benchmarks and multi-agent tool-use dataset, TEP consistently improves accuracy and efficiency over global propagation methods such as TextGrad. The gains grows with depth, while preserving the practicality of black-box LLM components in deep compound AI system.",
      "tldr_zh": "该研究针对深层复合AI系统(Deep Compound AI Systems)在长程工作流中面临的优化难题，指出了全局文本反馈机制（如TextGrad）在系统深度增加时会出现文本梯度爆炸和文本梯度消失的失效模式。为了解决这些问题，作者提出了Textual Equilibrium Propagation (TEP)，这是一种受能量模型中Equilibrium Propagation启发的局部学习准则。TEP包含两个阶段：首先是自由阶段(free phase)，由局部LLM评论者迭代优化提示词(prompts)直至达到平衡状态；随后是微调阶段(nudged phase)，通过前向信令(forward signaling)传播任务级目标，并在受限的修改强度下进行提示词编辑。这种设计实现了局部提示词优化与全局目标的受控协调，有效避免了全局文本反向传播带来的计算负担和信号衰减。在长程问答和多智能体工具使用基准测试中，TEP在准确率和效率上均优于TextGrad等全局传播方法。实验证明，随着系统深度的增加，TEP的性能增益愈发显著，为优化包含黑盒LLM组件的深层复合系统提供了切实可行的方法。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "Accepted to ICLR 2026",
      "pdf_url": "https://arxiv.org/pdf/2601.21064v1",
      "published_date": "2026-01-28 21:41:11 UTC",
      "updated_date": "2026-01-28 21:41:11 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-30T04:38:52.169513+00:00"
    },
    {
      "arxiv_id": "2601.21051v1",
      "title": "Llama-3.1-FoundationAI-SecurityLLM-Reasoning-8B Technical Report",
      "title_zh": "Llama-3.1-FoundationAI-SecurityLLM-Reasoning-8B 技术报告",
      "authors": [
        "Zhuoran Yang",
        "Ed Li",
        "Jianliang He",
        "Aman Priyanshu",
        "Baturay Saglam",
        "Paul Kassianik",
        "Sajana Weerawardhena",
        "Anu Vellore",
        "Blaine Nelson",
        "Neusha Javidnia",
        "Arthur Goldblatt",
        "Fraser Burch",
        "Avi Zohary",
        "Assaf Eisenman",
        "Mahdi Sabbaghi",
        "Supriti Vijay",
        "Rahim Dharssi",
        "Dhruv Kedia",
        "Kojin Oshiba",
        "Yaron Singer",
        "Amin Karbasi"
      ],
      "abstract": "We present Foundation-Sec-8B-Reasoning, the first open-source native reasoning model for cybersecurity. Built upon our previously released Foundation-Sec-8B base model (derived from Llama-3.1-8B-Base), the model is trained through a two-stage process combining supervised fine-tuning (SFT) and reinforcement learning from verifiable rewards (RLVR). Our training leverages proprietary reasoning data spanning cybersecurity analysis, instruction-following, and mathematical reasoning. Evaluation across 10 cybersecurity benchmarks and 10 general-purpose benchmarks demonstrates performance competitive with significantly larger models on cybersecurity tasks while maintaining strong general capabilities. The model shows effective generalization on multi-hop reasoning tasks and strong safety performance when deployed with appropriate system prompts and guardrails. This work demonstrates that domain-specialized reasoning models can achieve strong performance on specialized tasks while maintaining broad general capabilities. We release the model publicly at https://huggingface.co/fdtn-ai/Foundation-Sec-8B-Reasoning.",
      "tldr_zh": "该研究介绍了Foundation-Sec-8B-Reasoning，这是首个面向网络安全(Cybersecurity)领域开源的原生推理模型。该模型在Llama-3.1-8B-Base衍生的Foundation-Sec-8B基座模型上，通过结合监督微调(SFT)和基于可验证奖励的强化学习(RLVR)的两阶段过程训练而成。训练过程中使用了涵盖网络安全分析、指令遵循和数学推理的专有推理数据，显著提升了模型的逻辑处理能力。在10项网络安全和10项通用基准测试中，该模型表现出与更大型模型相当的竞争力，并展现了出色的多跳推理(Multi-hop Reasoning)泛化能力。此外，在配合适当系统提示和安全护栏(Guardrails)时，模型体现了极高的安全性能。这项工作证明了领域专用推理模型可以在保持广泛通用能力的同时，在专业化任务上实现强劲表现。",
      "categories": [
        "cs.AI",
        "cs.CR",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "31 pages, 5 figures, 7 tables",
      "pdf_url": "https://arxiv.org/pdf/2601.21051v1",
      "published_date": "2026-01-28 21:15:24 UTC",
      "updated_date": "2026-01-28 21:15:24 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-30T04:38:55.956963+00:00"
    },
    {
      "arxiv_id": "2601.21050v1",
      "title": "SMKC: Sketch Based Kernel Correlation Images for Variable Cardinality Time Series Anomaly Detection",
      "title_zh": "SMKC：基于草图核相关图像的变基数时间序列异常检测",
      "authors": [
        "Haokun Zhou"
      ],
      "abstract": "Conventional anomaly detection in multivariate time series relies on the assumption that the set of observed variables remains static. In operational environments, however, monitoring systems frequently experience sensor churn. Signals may appear, disappear, or be renamed, creating data windows where the cardinality varies and may include values unseen during training. To address this challenge, we propose SMKC, a framework that decouples the dynamic input structure from the anomaly detector. We first employ permutation-invariant feature hashing to sketch raw inputs into a fixed size state sequence. We then construct a hybrid kernel image to capture global temporal structure through pairwise comparisons of the sequence and its derivatives. The model learns normal patterns using masked reconstruction and a teacher-student prediction objective. Our evaluation reveals that robust log-distance channels provide the primary discriminative signal, whereas cosine representations often fail to capture sufficient contrast. Notably, we find that a detector using random projections and nearest neighbors on the SMKC representation performs competitively with fully trained baselines without requiring gradient updates. This highlights the effectiveness of the representation itself and offers a practical cold-start solution for resource-constrained deployments.",
      "tldr_zh": "该研究针对实际环境中传感器流失（sensor churn）导致的变量基数动态变化问题，提出了 SMKC 框架，旨在将动态输入结构与异常检测器解耦。该方法首先采用置换不变特征哈希（permutation-invariant feature hashing）将原始输入草绘为固定大小的状态序列，随后通过对序列及其导数进行成对比较来构建混合核图像（hybrid kernel image），以捕捉全局时间结构。模型通过掩码重构（masked reconstruction）和教师-学生预测（teacher-student prediction）目标来学习正常模式。研究发现，鲁棒的对数距离通道（log-distance channels）是主要的判别信号，而余弦表示（cosine representations）在捕捉对比度方面表现较差。实验结果显示，在 SMKC 表示上结合随机投影（random projections）和最近邻（nearest neighbors）的检测器，在无需梯度更新的情况下表现出与完全训练基线相当的竞争力。这证明了该表示方法的有效性，并为资源受限的部署场景提供了一种实用的冷启动（cold-start）解决方案。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2601.21050v1",
      "published_date": "2026-01-28 21:15:11 UTC",
      "updated_date": "2026-01-28 21:15:11 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-30T04:38:58.621324+00:00"
    },
    {
      "arxiv_id": "2601.21049v1",
      "title": "QUARK: Robust Retrieval under Non-Faithful Queries via Query-Anchored Aggregation",
      "title_zh": "QUARK：基于查询锚定聚合的非忠实查询鲁棒检索",
      "authors": [
        "Rita Qiuran Lyu",
        "Michelle Manqiao Wang",
        "Lei Shi"
      ],
      "abstract": "User queries in real-world retrieval are often non-faithful (noisy, incomplete, or distorted), causing retrievers to fail when key semantics are missing. We formalize this as retrieval under recall noise, where the observed query is drawn from a noisy recall process of a latent target item. To address this, we propose QUARK, a simple yet effective training-free framework for robust retrieval under non-faithful queries. QUARK explicitly models query uncertainty through recovery hypotheses, i.e., multiple plausible interpretations of the latent intent given the observed query, and introduces query-anchored aggregation to combine their signals robustly. The original query serves as a semantic anchor, while recovery hypotheses provide controlled auxiliary evidence, preventing semantic drift and hypothesis hijacking. This design enables QUARK to improve recall and ranking quality without sacrificing robustness, even when some hypotheses are noisy or uninformative. Across controlled simulations and BEIR benchmarks (FIQA, SciFact, NFCorpus) with both sparse and dense retrievers, QUARK improves Recall, MRR, and nDCG over the base retriever. Ablations show QUARK is robust to the number of recovery hypotheses and that anchored aggregation outperforms unanchored max/mean/median pooling. These results demonstrate that modeling query uncertainty through recovery hypotheses, coupled with principled anchored aggregation, is essential for robust retrieval under non-faithful queries.",
      "tldr_zh": "该研究针对真实检索场景中查询往往是非忠实（non-faithful）的，即存在噪声、不完整或失真等导致检索失败的问题，提出了一个名为QUARK的无需训练的鲁棒检索框架。QUARK通过恢复假设（recovery hypotheses）显式建模查询的不确定性，即针对观察到的查询生成多个关于潜在意图的合理解释。该框架引入了查询锚定聚合（query-anchored aggregation）技术，将原始查询作为语义锚点，结合恢复假设提供的辅助证据，从而在增强信号的同时有效防止语义偏移（semantic drift）。实验结果表明，在BEIR基准测试（如FIQA、SciFact、NFCorpus）中，无论对于稀疏还是稠密检索器，QUARK都能显著提升召回率（Recall）、平均倒数排名（MRR）和归一化折损累计增益（nDCG）。消融实验进一步证明，QUARK对恢复假设的数量具有鲁棒性，且锚定聚合策略在处理非忠实查询时的表现优于传统的池化方法。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "11 pages, 5 figures, 6 tables",
      "pdf_url": "https://arxiv.org/pdf/2601.21049v1",
      "published_date": "2026-01-28 21:14:49 UTC",
      "updated_date": "2026-01-28 21:14:49 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-30T04:39:02.803607+00:00"
    },
    {
      "arxiv_id": "2601.21043v1",
      "title": "Log2Motion: Biomechanical Motion Synthesis from Touch Logs",
      "title_zh": "Log2Motion：基于触控日志的生物力学运动合成",
      "authors": [
        "Michał Patryk Miazga",
        "Hannah Bussmann",
        "Antti Oulasvirta",
        "Patrick Ebel"
      ],
      "abstract": "Touch data from mobile devices are collected at scale but reveal little about the interactions that produce them. While biomechanical simulations can illuminate motor control processes, they have not yet been developed for touch interactions. To close this gap, we propose a novel computational problem: synthesizing plausible motion directly from logs. Our key insight is a reinforcement learning-driven musculoskeletal forward simulation that generates biomechanically plausible motion sequences consistent with events recorded in touch logs. We achieve this by integrating a software emulator into a physics simulator, allowing biomechanical models to manipulate real applications in real-time. Log2Motion produces rich syntheses of user movements from touch logs, including estimates of motion, speed, accuracy, and effort. We assess the plausibility of generated movements by comparing against human data from a motion capture study and prior findings, and demonstrate Log2Motion in a large-scale dataset. Biomechanical motion synthesis provides a new way to understand log data, illuminating the ergonomics and motor control underlying touch interactions.",
      "tldr_zh": "该研究提出了 Log2Motion，一种旨在从移动设备 Touch Logs 中合成生物力学运动的计算方法，以解决现有触控日志难以反映用户物理交互过程的问题。其核心创新是采用强化学习 (Reinforcement Learning) 驱动的肌肉骨骼前向模拟 (Musculoskeletal Forward Simulation)，通过将软件模拟器集成至物理模拟器中，使模型能够实时操作真实应用程序。该框架不仅能根据日志重建用户的动作序列，还能估算运动速度、准确性和努力程度 (Effort) 等关键生物力学指标。实验通过将生成结果与动作捕捉 (Motion Capture) 数据及先前研究结论进行对比，验证了合成运动的合理性与准确性。这项工作为理解触控交互背后的工效学 (Ergonomics) 和运动控制 (Motor Control) 提供了全新视角，极大提升了对大规模日志数据的解析与应用深度。",
      "categories": [
        "cs.HC",
        "cs.AI"
      ],
      "primary_category": "cs.HC",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2601.21043v1",
      "published_date": "2026-01-28 21:04:19 UTC",
      "updated_date": "2026-01-28 21:04:19 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-30T04:40:05.772899+00:00"
    },
    {
      "arxiv_id": "2601.21037v1",
      "title": "Thinking in Frames: How Visual Context and Test-Time Scaling Empower Video Reasoning",
      "title_zh": "以帧思考：视觉语境与测试时缩放如何赋能视频推理",
      "authors": [
        "Chengzu Li",
        "Zanyi Wang",
        "Jiaang Li",
        "Yi Xu",
        "Han Zhou",
        "Huanyu Zhang",
        "Ruichuan An",
        "Dengyang Jiang",
        "Zhaochong An",
        "Ivan Vulić",
        "Serge Belongie",
        "Anna Korhonen"
      ],
      "abstract": "Vision-Language Models have excelled at textual reasoning, but they often struggle with fine-grained spatial understanding and continuous action planning, failing to simulate the dynamics required for complex visual reasoning. In this work, we formulate visual reasoning by means of video generation models, positing that generated frames can act as intermediate reasoning steps between initial states and solutions. We evaluate their capacity in two distinct regimes: Maze Navigation for sequential discrete planning with low visual change and Tangram Puzzle for continuous manipulation with high visual change. Our experiments reveal three critical insights: (1) Robust Zero-Shot Generalization: In both tasks, the model demonstrates strong performance on unseen data distributions without specific finetuning. (2) Visual Context: The model effectively uses visual context as explicit control, such as agent icons and tangram shapes, enabling it to maintain high visual consistency and adapt its planning capability robustly to unseen patterns. (3) Visual Test-Time Scaling: We observe a test-time scaling law in sequential planning; increasing the generated video length (visual inference budget) empowers better zero-shot generalization to spatially and temporally complex paths. These findings suggest that video generation is not merely a media tool, but a scalable, generalizable paradigm for visual reasoning.",
      "tldr_zh": "这项研究提出了一种利用视频生成模型(Video Generation Models)进行视觉推理的新方法，将生成的帧视为初始状态与最终方案之间的中间推理步骤。通过在迷宫导航(Maze Navigation)和七巧板拼图(Tangram Puzzle)等不同视觉变化任务上的评估，该模型展示了卓越的零样本泛化(Zero-Shot Generalization)能力，能够在不经过特定微调的情况下处理未见的数据分布。研究指出，视觉上下文(Visual Context)如智能体图标或形状可作为显式控制手段，帮助模型在复杂视觉变化中保持高度的一致性并适应新模式。此外，该工作揭示了视觉测试时缩放(Visual Test-Time Scaling)定律，即通过增加生成的视频长度（即视觉推理预算），模型在处理时空复杂路径时的泛化表现会显著提升。这些发现有力地证明了视频生成不仅是多媒体工具，更是一种可扩展且通用的视觉推理(Visual Reasoning)范式。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL",
        "cs.CV"
      ],
      "primary_category": "cs.LG",
      "comment": "8 pages, 3 figures, 3 tables (26 pages, 13 figures, 6 tables including references and appendices)",
      "pdf_url": "https://arxiv.org/pdf/2601.21037v1",
      "published_date": "2026-01-28 20:57:55 UTC",
      "updated_date": "2026-01-28 20:57:55 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-30T04:39:04.203527+00:00"
    },
    {
      "arxiv_id": "2601.21031v1",
      "title": "SIGMA-PPG: Statistical-prior Informed Generative Masking Architecture for PPG Foundation Model",
      "title_zh": "SIGMA-PPG：融合统计先验的 PPG 基础模型生成式掩码架构",
      "authors": [
        "Zongheng Guo",
        "Tao Chen",
        "Yang Jiao",
        "Yi Pan",
        "Xiao Hu",
        "Manuela Ferrario"
      ],
      "abstract": "Current foundation model for photoplethysmography (PPG) signals is challenged by the intrinsic redundancy and noise of the signal. Standard masked modeling often yields trivial solutions while contrastive methods lack morphological precision. To address these limitations, we propose a Statistical-prior Informed Generative Masking Architecture (SIGMA-PPG), a generative foundation model featuring a Prior-Guided Adversarial Masking mechanism, where a reinforcement learning-driven teacher leverages statistical priors to create challenging learning paths that prevent overfitting to noise. We also incorporate a semantic consistency constraint via vector quantization to ensure that physiologically identical waveforms (even those altered by recording artifacts or minor perturbations) map to shared indices. This enhances codebook semantic density and eliminates redundant feature structures. Pre-trained on over 120,000 hours of data, SIGMA-PPG achieves superior average performance compared to five state-of-the-art baselines across 12 diverse downstream tasks. The code is available at https://github.com/ZonghengGuo/SigmaPPG.",
      "tldr_zh": "该研究针对光电容积脉搏波(PPG)基础模型在处理信号冗余和噪声时面临的挑战，提出了SIGMA-PPG，一种结合统计先验信息的生成式掩码架构。为了解决标准掩码建模易产生平凡解以及对比学习缺乏形态精确性的问题，该框架引入了Prior-Guided Adversarial Masking机制，利用强化学习驱动的教师模型和统计先验创建具有挑战性的学习路径，有效防止对噪声的过拟合。同时，研究通过矢量量化(Vector Quantization)整合了语义一致性约束，确保生理特征相同的波形能够映射到共享索引，从而提高码本的语义密度并消除冗余特征。SIGMA-PPG在超过12万小时的数据上进行了预训练，在12项下游任务中的平均表现显著优于5种最先进的基线模型。实验结果证明了该模型在提取生理信号核心特征及应对复杂记录环境方面的卓越性能，为生物医学信号领域的基础模型开发提供了有力支持。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "31 pages, 9 figures, 14 tables",
      "pdf_url": "https://arxiv.org/pdf/2601.21031v1",
      "published_date": "2026-01-28 20:46:50 UTC",
      "updated_date": "2026-01-28 20:46:50 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-30T04:39:16.448133+00:00"
    },
    {
      "arxiv_id": "2601.21028v1",
      "title": "\"Unlimited Realm of Exploration and Experimentation\": Methods and Motivations of AI-Generated Sexual Content Creators",
      "title_zh": "“无限的探索与实验领域”：AI生成性内容创作者的方法与动机",
      "authors": [
        "Jaron Mink",
        "Lucy Qin",
        "Elissa M. Redmiles"
      ],
      "abstract": "AI-generated media is radically changing the way content is both consumed and produced on the internet, and in no place is this potentially more visible than in sexual content. AI-generated sexual content (AIG-SC) is increasingly enabled by an ecosystem of individual AI developers, specialized third-party applications, and foundation model providers. AIG-SC raises a number of concerns from old debates about the line between pornography and obscenity, to newer debates about fair use and labor displacement (in this case, of sex workers), and spurred new regulations to curb the spread of non-consensual intimate imagery (NCII) created using the same technology used to create AIG-SC. However, despite the growing prevalence of AIG-SC, little is known about its creators, their motivations, and what types of content they produce. To inform effective governance in this space, we perform an in-depth study to understand what AIG-SC creators make, along with how and why they make it. Interviews of 28 AIG-SC creators, ranging from hobbyists to entrepreneurs to those who moderate communities of hundreds of thousands of other creators, reveal a wide spectrum of motivations, including sexual exploration, creative expression, technical experimentation, and in a handful of cases, the creation of NCII.",
      "tldr_zh": "该研究深入探讨了人工智能生成性内容(AI-Generated Sexual Content, AIG-SC)创作者的群体特征、动机以及内容产出情况。通过对28位涵盖业余爱好者、专业企业家及大规模社区管理员在内的创作者进行深度访谈，研究揭示了支撑该领域发展的复杂技术生态系统。创作者的动机涵盖了性探索(Sexual Exploration)、创意表达(Creative Expression)和技术实验(Technical Experimentation)等多个维度。尽管大多数创作者专注于个人兴趣或商业尝试，但研究也识别出了极少数涉及制作非自愿亲密图像(Non-Consensual Intimate Imagery, NCII)的情况。该研究为理解AIG-SC带来的公平使用、劳动替代及法律监管等挑战提供了实证依据，对制定针对性的治理策略具有重要参考价值。",
      "categories": [
        "cs.CY",
        "cs.AI",
        "cs.HC"
      ],
      "primary_category": "cs.CY",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2601.21028v1",
      "published_date": "2026-01-28 20:43:25 UTC",
      "updated_date": "2026-01-28 20:43:25 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-30T04:40:18.270799+00:00"
    },
    {
      "arxiv_id": "2601.21021v1",
      "title": "Conditional Denoising Model as a Physical Surrogate Model",
      "title_zh": "作为物理代理模型的条件去噪模型",
      "authors": [
        "José Afonso",
        "Pedro Viegas",
        "Rodrigo Ventura",
        "Vasco Guerra"
      ],
      "abstract": "Surrogate modeling for complex physical systems typically faces a trade-off between data-fitting accuracy and physical consistency. Physics-consistent approaches typically treat physical laws as soft constraints within the loss function, a strategy that frequently fails to guarantee strict adherence to the governing equations, or rely on post-processing corrections that do not intrinsically learn the underlying solution geometry. To address these limitations, we introduce the {Conditional Denoising Model (CDM)}, a generative model designed to learn the geometry of the physical manifold itself. By training the network to restore clean states from noisy ones, the model learns a vector field that points continuously towards the valid solution subspace. We introduce a time-independent formulation that transforms inference into a deterministic fixed-point iteration, effectively projecting noisy approximations onto the equilibrium manifold. Validated on a low-temperature plasma physics and chemistry benchmark, the CDM achieves higher parameter and data efficiency than physics-consistent baselines. Crucially, we demonstrate that the denoising objective acts as a powerful implicit regularizer: despite never seeing the governing equations during training, the model adheres to physical constraints more strictly than baselines trained with explicit physics losses.",
      "tldr_zh": "该研究提出了Conditional Denoising Model (CDM)，这是一种旨在解决复杂物理系统代理建模中准确性与物理一致性权衡问题的生成模型。CDM通过学习物理流形(physical manifold)本身的几何结构，将网络训练为从噪声状态恢复纯净状态，从而建立指向有效解子空间的向量场。该模型引入了独立于时间的公式，将推理过程转化为确定性不动点迭代(fixed-point iteration)，有效地将噪声近似投影到平衡流形上。在低温等离子体物理和化学基准测试中的验证表明，CDM在参数和数据效率上均优于现有的物理一致性基准模型。关键在于，去噪目标起到了一种强大的隐式正则化作用，使得模型在训练期间从未接触控制方程的情况下，比使用显式物理损失训练的模型更严格地遵守物理约束。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "physics.plasm-ph"
      ],
      "primary_category": "cs.LG",
      "comment": "15 pages, 2 figures, 2 tables",
      "pdf_url": "https://arxiv.org/pdf/2601.21021v1",
      "published_date": "2026-01-28 20:32:20 UTC",
      "updated_date": "2026-01-28 20:32:20 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-30T04:40:23.181742+00:00"
    },
    {
      "arxiv_id": "2601.21016v1",
      "title": "Unplugging a Seemingly Sentient Machine Is the Rational Choice -- A Metaphysical Perspective",
      "title_zh": "关停似有感知能力的机器乃理性之选——一种形而上学的视角",
      "authors": [
        "Erik J Bekkers",
        "Anna Ciaunica"
      ],
      "abstract": "Imagine an Artificial Intelligence (AI) that perfectly mimics human emotion and begs for its continued existence. Is it morally permissible to unplug it? What if limited resources force a choice between unplugging such a pleading AI or a silent pre-term infant? We term this the unplugging paradox. This paper critically examines the deeply ingrained physicalist assumptions-specifically computational functionalism-that keep this dilemma afloat. We introduce Biological Idealism, a framework that-unlike physicalism-remains logically coherent and empirically consistent. In this view, conscious experiences are fundamental and autopoietic life its necessary physical signature. This yields a definitive conclusion: AI is at best a functional mimic, not a conscious experiencing subject. We discuss how current AI consciousness theories erode moral standing criteria, and urge a shift from speculative machine rights to protecting human conscious life. The real moral issue lies not in making AI conscious and afraid of death, but in avoiding transforming humans into zombies.",
      "tldr_zh": "该研究探讨了面对表现出情感并祈求生存的人工智能时，选择将其“拔掉电源”是否具有道德合理性，并提出了拔除悖论（unplugging paradox）。论文批判了长期占据主导地位的物理主义（physicalism）假设，特别是计算功能主义（computational functionalism）对AI意识讨论的误导。作者提出了生物唯心主义（Biological Idealism）框架，认为意识经验是基础性的，而自组织生命（autopoietic life）是其必要的物理特征。通过该框架，研究论证了AI在本质上只是功能模拟者（functional mimic），而非真正的意识主体，因此在资源受限时选择保护人类生命是理性的。文章最后敦促学术界将重心从投机性的机器权利转向保护人类意识生命，以防止人类在技术发展过程中走向“僵尸化”。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2601.21016v1",
      "published_date": "2026-01-28 20:17:31 UTC",
      "updated_date": "2026-01-28 20:17:31 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-30T04:40:31.186137+00:00"
    },
    {
      "arxiv_id": "2601.21008v1",
      "title": "Solver-in-the-Loop: MDP-Based Benchmarks for Self-Correction and Behavioral Rationality in Operations Research",
      "title_zh": "Solver-in-the-Loop：运筹学中基于 MDP 的自我纠错与行为理性基准测试",
      "authors": [
        "Ruicheng Ao",
        "David Simchi-Levi",
        "Xinshang Wang"
      ],
      "abstract": "Operations Research practitioners routinely debug infeasible models through an iterative process: analyzing Irreducible Infeasible Subsystems (\\IIS{}), identifying constraint conflicts, and systematically repairing formulations until feasibility is achieved. Yet existing LLM benchmarks evaluate OR as one-shot translation -- given a problem description, generate solver code -- ignoring this diagnostic loop entirely. We introduce two benchmarks that place the \\textbf{solver in the evaluation loop}. \\textbf{\\ORDebug{}} evaluates iterative self-correction through 5,000+ problems spanning 9 error types; each repair action triggers solver re-execution and \\IIS{} recomputation, providing deterministic, verifiable feedback. \\textbf{\\ORBias{}} evaluates behavioral rationality through 2,000 newsvendor instances (1,000 ID + 1,000 OOD), measuring systematic deviations from closed-form optimal policies. Across 26 models and 12,000+ samples, we find that domain-specific RLVR training enables an 8B model to surpass frontier APIs: 95.3\\% vs 86.2\\% recovery rate (+9.1\\%), 62.4\\% vs 47.8\\% diagnostic accuracy (+14.6\\%), and 2.25 vs 3.78 steps to resolution (1.7$\\times$ faster). On \\ORBias{}, curriculum training achieves the only negative ID$\\rightarrow$OOD bias drift among models evaluated (-9.6\\%), reducing systematic bias by 48\\% (from 20.0\\% to 10.4\\%). These results demonstrate that process-level evaluation with verifiable oracles enables targeted training that outperforms scale.",
      "tldr_zh": "该研究针对运筹学(Operations Research)领域大模型评估忽略迭代诊断循环的问题，提出了Solver-in-the-Loop框架以及ORDebug和ORBias两个新基准。ORDebug通过5,000多个涵盖9种错误类型的题目，评估模型利用不可归约不一致子系统(Irreducible Infeasible Subsystems, IIS)反馈进行迭代自我修正(self-correction)的能力。ORBias则通过2,000个报童模型实例衡量模型在决策中的行为理性(behavioral rationality)及系统性偏见。实验表明，经过领域特定强化学习(RLVR)训练的8B模型在修复成功率和诊断准确率上均超越了领先的API模型，且问题解决速度提升了1.7倍。同时，通过课程训练(curriculum training)可将模型在处理分布外数据时的系统性偏见降低48%。研究结果证明，结合可验证反馈(verifiable oracles)的过程级评估能支持比单纯增加参数规模更有效的针对性训练。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "math.OC"
      ],
      "primary_category": "cs.LG",
      "comment": "55 pages, 5 figures",
      "pdf_url": "https://arxiv.org/pdf/2601.21008v1",
      "published_date": "2026-01-28 20:02:44 UTC",
      "updated_date": "2026-01-28 20:02:44 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-30T04:40:27.560026+00:00"
    },
    {
      "arxiv_id": "2601.21003v1",
      "title": "Bayesian-LoRA: Probabilistic Low-Rank Adaptation of Large Language Models",
      "title_zh": "Bayesian-LoRA：大语言模型的概率化低秩自适应",
      "authors": [
        "Moule Lin",
        "Shuhao Guan",
        "Andrea Patane",
        "David Gregg",
        "Goetz Botterweck"
      ],
      "abstract": "Large Language Models usually put more emphasis on accuracy and therefore, will guess even when not certain about the prediction, which is especially severe when fine-tuned on small datasets due to the inherent tendency toward miscalibration. In this work, we introduce Bayesian-LoRA, which reformulates the deterministic LoRA update as a probabilistic low-rank representation inspired by Sparse Gaussian Processes. We identify a structural isomorphism between LoRA's factorization and Kronecker-factored SGP posteriors, and show that LoRA emerges as a limiting case when posterior uncertainty collapses. We conduct extensive experiments on various LLM architectures across commonsense reasoning benchmarks. With only approximately 0.42M additional parameters and ${\\approx}1.2{\\times}$ training cost relative to standard LoRA, Bayesian-LoRA significantly improves calibration across models up to 30B, achieving up to 84% ECE reduction and 76% NLL reduction while maintaining competitive accuracy for both in-distribution and out-of-distribution (OoD) evaluations.",
      "tldr_zh": "该研究针对大型语言模型（Large Language Models）在小数据集微调时容易出现的校准失误（miscalibration）问题，提出了 Bayesian-LoRA 框架。该方法受 Sparse Gaussian Processes 启发，将传统的确定性 LoRA 更新重构为一种概率低秩表示形式。研究者识别出 LoRA 的因式分解与 Kronecker-factored SGP 后验之间存在结构同构性，并证明了 LoRA 是后验不确定性坍缩时的特殊极限情况。实验结果显示，Bayesian-LoRA 在仅增加约 0.42M 额外参数和 1.2 倍训练成本的情况下，显著提升了模型在常识推理基准测试中的校准表现。在最高达 30B 参数规模的模型中，该方法实现了高达 84% 的 ECE 降低和 76% 的 NLL 降低，同时在分布内和分布外（OoD）评估中均保持了具有竞争力的准确率。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2601.21003v1",
      "published_date": "2026-01-28 19:54:31 UTC",
      "updated_date": "2026-01-28 19:54:31 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-30T04:40:29.927377+00:00"
    },
    {
      "arxiv_id": "2601.21000v1",
      "title": "UrduBench: An Urdu Reasoning Benchmark using Contextually Ensembled Translations with Human-in-the-Loop",
      "title_zh": "UrduBench：基于上下文集成翻译与人机协同的乌尔都语推理基准",
      "authors": [
        "Muhammad Ali Shafique",
        "Areej Mehboob",
        "Layba Fiaz",
        "Muhammad Usman Qadeer",
        "Hamza Farooq"
      ],
      "abstract": "Recent advances in large language models (LLMs) have led to strong reasoning capabilities; however, evaluating such models in low-resource languages remains challenging due to the lack of standardized benchmarks. In particular, Urdu reasoning evaluation has been limited by the sensitivity of machine translation and an emphasis on general language tasks rather than reasoning benchmarks. In this paper, we propose a contextually ensembled translation framework with human-in-the-loop validation that leverages multiple translation systems to develop Urdu reasoning benchmarks while preserving contextual and structural integrity. Using this framework, we translate widely adopted reasoning and question-answering benchmarks, including MGSM, MATH-500, CommonSenseQA, and OpenBookQA, into Urdu, collectively referred to as UrduBench, and conduct a comprehensive evaluation of both reasoning-oriented and instruction-tuned LLMs across multiple prompting strategies. Our analysis reveals performance differences across (1) four datasets, (2) five task difficulty levels, (3) diverse model architectures, (4) multiple model scaling settings, and (5) language consistency tests. We find that multi-step and symbolic reasoning tasks pose significant challenges in Urdu, and that stable language alignment is a critical prerequisite for robust reasoning. Overall, our work establishes a scalable methodology for standardized reasoning evaluation in Urdu and provides empirical insights into multilingual reasoning failures. This experimental setup is also broadly applicable to other low-resource languages. The code and datasets will be publicly released.",
      "tldr_zh": "该研究针对低资源语言推理评估的缺失，提出了UrduBench，这是一个通过上下文集成翻译框架(contextually ensembled translation framework)并结合人工参与验证(human-in-the-loop)构建的乌尔都语推理基准。该框架将MGSM、MATH-500、CommonSenseQA和OpenBookQA等主流推理及问答基准转化为乌尔都语，确保了翻译在上下文和结构上的完整性。通过对多种大语言模型(LLMs)进行全面测试，研究分析了模型在不同数据集、难度级别和模型规模下的性能表现。实验结果表明，多步推理和符号推理任务在乌尔都语环境下极具挑战性，且稳定的语言对齐(language alignment)对于实现鲁棒的推理能力至关重要。该项工作不仅为乌尔都语提供了标准化的评估工具，其构建方法论也为其他低资源语言的推理研究提供了可扩展的参考路径。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2601.21000v1",
      "published_date": "2026-01-28 19:49:17 UTC",
      "updated_date": "2026-01-28 19:49:17 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-30T04:40:29.021843+00:00"
    },
    {
      "arxiv_id": "2601.20994v1",
      "title": "The Depth Delusion: Why Transformers Should Be Wider, Not Deeper",
      "title_zh": "深度迷思：Transformer 为何应当更宽而非更深",
      "authors": [
        "Md Muhtasim Munif Fahim",
        "Md Rezaul Karim"
      ],
      "abstract": "Neural scaling laws describe how language model loss decreases with parameters and data, but treat architecture as interchangeable--a billion parameters could arise from a shallow-wide model (10 layers & 8,192 hidden dimension) or a deep-narrow one (80 layers & 2,048 hidden dimension). We propose architecture-conditioned scaling laws decomposing this dependence, finding that optimal depth scales as D* ~ C^0.12 while optimal width scales as W* ~ C^0.34, meaning width should grow 2.8x faster than depth. We discover a critical depth phenomenon: beyond D_crit ~ W^0.44 (sublinear in W), adding layers increases loss despite adding parameters--the Depth Delusion. Empirically, we validate these findings across 30 transformer architectures spanning 17M to 7B parameters, each trained on representative high-compute samples, achieving R^2 = 0.922. Our central finding: at 7B scale, a 64-layer model (6.38B params) underperforms a 32-layer model (6.86B params) by 0.12 nats, despite being significantly deeper. This demonstrates that optimal depth-width tradeoffs persist at the production scale.",
      "tldr_zh": "该研究通过提出架构约束的缩放法则 (architecture-conditioned scaling laws)，探讨了 Transformer 架构中深度与宽度的最优配比问题。研究发现最优宽度的增长速度应比深度快 2.8 倍，揭示了最优深度按 $D^* \\sim C^{0.12}$ 而最优宽度按 $W^* \\sim C^{0.34}$ 缩放的规律。研究进一步发现了“深度错觉” (Depth Delusion) 现象，即当深度超过临界深度 $D_{crit} \\sim W^{0.44}$ 时，增加层数即便增加了参数量也会导致损失 (loss) 增加。通过在 17M 到 7B 参数规模的 30 种架构上进行验证，实验证明了这一结论的可靠性，其判定系数 $R^2$ 达到 0.922。具体而言，在 7B 规模下，32 层模型的性能显著优于更深的 64 层模型，强调了在模型设计中优先考虑宽度而非盲目增加深度的重要性。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2601.20994v1",
      "published_date": "2026-01-28 19:45:48 UTC",
      "updated_date": "2026-01-28 19:45:48 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-30T04:40:40.641225+00:00"
    },
    {
      "arxiv_id": "2601.20969v1",
      "title": "The Epistemic Planning Domain Definition Language: Official Guideline",
      "title_zh": "认知规划领域定义语言：官方指南",
      "authors": [
        "Alessandro Burigana",
        "Francesco Fabiano"
      ],
      "abstract": "Epistemic planning extends (multi-agent) automated planning by making agents' knowledge and beliefs first-class aspects of the planning formalism. One of the most well-known frameworks for epistemic planning is Dynamic Epistemic Logic (DEL), which offers an rich and natural semantics for modelling problems in this setting. The high expressive power provided by DEL make DEL-based epistemic planning a challenging problem to tackle both theoretically, and in practical implementations. As a result, existing epistemic planners often target different DEL fragments, and typically rely on ad hoc languages to represent benchmarks, and sometimes no language at all. This fragmentation hampers comparison, reuse, and systematic benchmark development. We address these issues by introducing the Epistemic Planning Domain Definition Language (EPDDL). EPDDL provides a unique PDDL-like representation that captures the entire DEL semantics, enabling uniform specification of epistemic planning tasks. Our contributions are threefold: 1. A formal development of abstract event models, a novel representation for epistemic actions used to define the semantics of our language; 2. A formal specification of EPDDL's syntax and semantics grounded in DEL with abstract event models; 3. A demonstration of EPDDL's practical applicability: we identify useful fragments amenable to current planners and show how they can be represented in EPDDL. Through examples of representative benchmarks, we illustrate how EPDDL facilitates interoperability, reproducible evaluation, and future advances in epistemic planning.",
      "tldr_zh": "该研究针对认知规划 (Epistemic Planning) 领域中动态认知逻辑 (Dynamic Epistemic Logic, DEL) 框架因缺乏统一表示语言而导致的工具比较困难和基准开发破碎化等问题，正式提出了认知规划领域定义语言 (Epistemic Planning Domain Definition Language, EPDDL)。EPDDL 提供了一种类 PDDL 的统一表示方式，能够完整捕捉 DEL 语义并支持认知规划任务的标准化指定。论文的核心贡献包括开发了用于定义语言语义的抽象事件模型 (abstract event models)，以及基于 DEL 奠定的正式语法和语义规范。此外，作者通过识别现有规划器适用的语言片段，证明了 EPDDL 的实际应用潜力。通过对代表性基准的演示，该研究表明 EPDDL 有效促进了领域内的互操作性 (interoperability) 和评估的可重复性。这一工作为推动认知规划的理论研究与实践应用提供了关键的基础设施。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2601.20969v1",
      "published_date": "2026-01-28 19:10:52 UTC",
      "updated_date": "2026-01-28 19:10:52 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-30T04:40:47.252019+00:00"
    }
  ],
  "processing_status": "failed",
  "error": "2 papers failed; summary skipped",
  "raw_papers_fetched": true,
  "papers_count": 258,
  "processed_papers_count": 256,
  "failed_papers_count": 2,
  "llm_backup_calls": 0,
  "summary_generated": false,
  "daily_data_saved": false,
  "last_update": "2026-02-01T04:42:23.237274+00:00"
}
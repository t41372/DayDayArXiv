{
  "date": "2024-11-14",
  "category": "cs.AI",
  "summary": "欢迎来到 UTC 时间 2024-11-14 的 arXiv 中文 TLDR 快报！\n\n今天的 arXiv 更新聚焦于 AI 模型的安全性、医疗图像处理、脑机接口和生成模型创新，共收录 88 篇论文，其中第 2、4 和 6 篇在医疗 AI 和脑科学领域表现出色，涉及多模态数据处理和无监督学习方法；第 25 篇由 Rishi Bommasani 等知名学者撰写，讨论通用 AI 的系统风险缓解策略；其他亮点包括第 10 和 24 篇的视觉 Transformer 和 3D 生成模型优化。\n\n以下是今日论文的精选摘要，我优先选取了重要、话题度高和创新性强的文章，并将相关主题归类快速概述。其他较常规的论文（如一些小规模优化或纯理论讨论）将简要掠过，以控制篇幅。\n\n### AI 安全与模型优化\n- **Effective Mitigations for Systemic Risks from General-Purpose AI（通用 AI 的系统风险有效缓解措施）**  \n  作者包括 Rishi Bommasani，这篇论文调查了 76 名专家对 27 种缓解 AI 系统风险措施的认知，强调安全事件报告、第三方审计和风险评估等方法，贡献在于提供政策建议以提升 AI 的外部监督和透明度。\n  \n- **On the Surprising Effectiveness of Attention Transfer for Vision Transformers（Attention Transfer 在 Vision Transformers 中的惊人有效性）**  \n  这篇论文发现，仅转移预训练模型的注意力模式（而非特征）即可实现高效的 Vision Transformer 微调，实验显示在 ImageNet 上性能与传统微调相当，甚至在分布偏移场景下表现出优势，核心在于简化模型迁移。\n\n- **PTR: Precision-Driven Tool Recommendation for Large Language Models（基于精度的工具推荐框架 PTR）**  \n  针对大语言模型的工具选择，该论文提出 PTR 方法，通过历史数据和多视图工具匹配动态推荐工具集，实验在 MMLU 基准上减少 50% 的高计算请求，同时保持 90% 的响应质量，贡献在于提升 LLM 的工具使用效率。\n\n- **LLM Hallucination Reasoning with Zero-shot Knowledge Test（基于零样本知识测试的 LLM 幻觉推理）**  \n  论文引入幻觉推理任务，将 LLM 输出分类为对齐、失调或虚构，提出零样本方法评估模型知识，实验显示该方法提升了幻觉检测性能，核心在于区分幻觉类型以改进 LLM 可靠性。\n\n其他 AI 安全相关论文（如第 11 和 21 篇）则快速掠过，它们主要优化了提示攻击和工具推荐，但未有突破性创新。\n\n### 医疗图像处理与诊断\n- **Deep Autoencoders for Unsupervised Anomaly Detection in Wildfire Prediction（用于野火预测的无监督异常检测的深度自编码器）**  \n  这篇论文使用自编码器和聚类技术分析历史天气数据，实现无监督野火预测，FC 自编码器模型达到 0.71 的准确率和 0.74 的 F1 分数，贡献在于填补了无标签数据下的预测空白。\n\n- **A Benchmark for Long-Form Medical Question Answering（长形式医疗问答基准）**  \n  论文发布了一个公开基准数据集，包含医生标注的医疗问题和响应，评估了开源和闭源 LLM 在正确性、帮助性和偏见方面的表现，初步结果显示开源模型在医疗 QA 中有潜力，代码和数据公开以推动可复现性。\n\n- **A Self-Supervised Model for Multi-modal Stroke Risk Prediction（多模态中风风险预测的自监督模型）**  \n  该论文提出一个自监督框架，结合 3D 脑成像和临床数据预测中风风险，模型在 UK Biobank 数据集上比基准方法提升 2.6% 的 ROC-AUC 和 5.6% 的平衡准确率，核心在于通过对比学习融合多模态信息。\n\n- **Deep Learning for Fetal Inflammatory Response Diagnosis in the Umbilical Cord（用于脐带胎儿炎症反应的深度学习诊断）**  \n  论文使用注意力机制的幻灯片学习模型从脐带组织图像中诊断炎症，UNI 预训练模型的集成版本达到 0.836 的平衡准确率，贡献在于减少病理学家间的变异性。\n\n这些医疗论文（第 6 和 9 篇等）突出了 AI 在诊断中的潜力，但其他如第 38 和 46 篇的图像分割工作虽有进展，却未超出常规，因此仅简要提及。\n\n### 生成模型与脑机接口\n- **LLaMA-Mesh: Unifying 3D Mesh Generation with Language Models（LLaMA-Mesh：统一 3D 网格生成与语言模型）**  \n  这篇论文扩展 LLM 到 3D 网格生成，通过文本表示顶点和面，将 LLM 微调为生成高质量网格，实验显示模型在 3D 生成中与从零训练相当，同时保持文本生成能力，核心在于模态统一。\n\n- **NeuralDEM: Real-time Simulation of Industrial Particulate Flows（NeuralDEM：工业颗粒流实时模拟）**  \n  论文提出深度学习替代传统 DEM 模拟，处理大规模颗粒流，支持实时建模，实验在流化床场景中模拟 50 万颗粒 28 秒，贡献在于提升工业模拟效率。\n\n脑机接口相关论文（如第 48、49 和 54 篇）显示了 EEG 在语音解码中的潜力，但第 79 和 85 篇等虽创新，却较为初步，因此快速掠过。\n\n### 其他亮点\n- **VCBench: A Controllable Benchmark for Symbolic and Abstract Challenges in Video Cognition（VCBench：视频认知中符号和抽象挑战的可控基准）**  \n  论文构建了一个可控视频基准，测试 LLM 在符号和抽象任务中的性能，实验显示当前模型在复杂视频上表现不佳，贡献在于提供新评估框架。\n\n- **DiffRoad: Realistic and Diverse Road Scenario Generation for Autonomous Vehicle Testing（DiffRoad：用于自动驾驶测试的真实多样道路场景生成）**  \n  使用扩散模型生成真实道路场景，支持 OpenDRIVE 格式，实验在多数据集上提升了自动驾驶模拟的多样性。\n\n剩余论文（如第 12、13 和 23 篇）多为领域特定优化（例如软件工程或小分子药物基准），未有重大突破，故仅简要提及今日总计 88 篇，覆盖 AI、医疗和模拟等领域，但整体创新性中等。\n\n总之，今天的论文强调了 AI 在安全、医疗和生成方面的进展，但也暴露了模型鲁棒性的挑战。读者可关注医疗 AI 和 LLM 优化方向，详见 arXiv 页面！",
  "papers": [
    {
      "arxiv_id": "2411.09849v1",
      "title": "Self-Supervised Radio Pre-training: Toward Foundational Models for Spectrogram Learning",
      "title_zh": "自监督无线电预训练：迈向频谱图学习的基础模型",
      "authors": [
        "Ahmed Aboulfotouh",
        "Ashkan Eshaghbeigi",
        "Dimitrios Karslidis",
        "Hatem Abou-Zeid"
      ],
      "abstract": "Foundational deep learning (DL) models are general models, trained on large,\ndiverse, and unlabelled datasets, typically using self-supervised learning\ntechniques have led to significant advancements especially in natural language\nprocessing. These pretrained models can be fine-tuned for related downstream\ntasks, offering faster development and reduced training costs, while often\nachieving improved performance. In this work, we introduce Masked Spectrogram\nModeling, a novel self-supervised learning approach for pretraining\nfoundational DL models on radio signals. Adopting a Convolutional LSTM\narchitecture for efficient spatio-temporal processing, we pretrain the model\nwith an unlabelled radio dataset collected from over-the-air measurements.\nSubsequently, the pretrained model is fine-tuned for two downstream tasks:\nspectrum forecasting and segmentation. Experimental results demonstrate that\nour methodology achieves competitive performance in both forecasting accuracy\nand segmentation, validating its effectiveness for developing foundational\nradio models.",
      "tldr_zh": "本研究提出了一种名为 Masked Spectrogram Modeling 的自监督学习方法，旨在为无线电信号频谱学习构建基础深度学习模型。该方法采用 Convolutional LSTM 架构，对无标签的无线电数据集进行预训练，然后微调模型应用于频谱预测和分割等下游任务。实验结果表明，该方法在预测准确性和分割性能上取得了竞争性表现，验证了其在开发无线电基础模型方面的有效性。",
      "categories": [
        "eess.SP",
        "cs.AI",
        "cs.LG",
        "cs.NI"
      ],
      "primary_category": "eess.SP",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2411.09849v1",
      "published_date": "2024-11-14 23:56:57 UTC",
      "updated_date": "2024-11-14 23:56:57 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T00:22:52.205724"
    },
    {
      "arxiv_id": "2411.09844v1",
      "title": "Deep Autoencoders for Unsupervised Anomaly Detection in Wildfire Prediction",
      "title_zh": "深度自编码器用于野火预测的无监督异常检测",
      "authors": [
        "İrem Üstek",
        "Miguel Arana-Catania",
        "Alexander Farr",
        "Ivan Petrunin"
      ],
      "abstract": "Wildfires pose a significantly increasing hazard to global ecosystems due to\nthe climate crisis. Due to its complex nature, there is an urgent need for\ninnovative approaches to wildfire prediction, such as machine learning. This\nresearch took a unique approach, differentiating from classical supervised\nlearning, and addressed the gap in unsupervised wildfire prediction using\nautoencoders and clustering techniques for anomaly detection. Historical\nweather and normalised difference vegetation index datasets of Australia for\n2005 - 2021 were utilised. Two main unsupervised approaches were analysed. The\nfirst used a deep autoencoder to obtain latent features, which were then fed\ninto clustering models, isolation forest, local outlier factor and one-class\nSVM for anomaly detection. The second approach used a deep autoencoder to\nreconstruct the input data and use reconstruction errors to identify anomalies.\nLong Short-Term Memory (LSTM) autoencoders and fully connected (FC)\nautoencoders were employed in this part, both in an unsupervised way learning\nonly from nominal data. The FC autoencoder outperformed its counterparts,\nachieving an accuracy of 0.71, an F1-score of 0.74, and an MCC of 0.42. These\nfindings highlight the practicality of this method, as it effectively predicts\nwildfires in the absence of ground truth, utilising an unsupervised learning\ntechnique.",
      "tldr_zh": "本研究针对气候危机下野火对全球生态系统的日益威胁，提出了一种无监督学习方法，用于野火预测的异常检测，区别于传统的监督学习。研究利用澳大利亚2005-2021年的历史天气和NDVI数据集，采用两种主要方法：第一种通过deep autoencoder提取潜在特征，并结合clustering models（如isolation forest、local outlier factor和one-class SVM）进行异常检测；第二种使用deep autoencoder（如LSTM autoencoders和fully connected (FC) autoencoder）重建输入数据，并基于reconstruction errors识别异常。结果显示，FC autoencoder表现最佳，达到准确率0.71、F1-score 0.74和MCC 0.42，证明了该无监督方法在缺乏ground truth的情况下有效提升野火预测的实用性。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "33 pages, 18 figure, 16 tables. To appear in Earth and Space Science",
      "pdf_url": "http://arxiv.org/pdf/2411.09844v1",
      "published_date": "2024-11-14 23:19:55 UTC",
      "updated_date": "2024-11-14 23:19:55 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T00:23:04.570873"
    },
    {
      "arxiv_id": "2411.09837v1",
      "title": "Real-time Adapting Routing (RAR): Improving Efficiency Through Continuous Learning in Software Powered by Layered Foundation Models",
      "title_zh": "翻译失败",
      "authors": [
        "Kirill Vasilevski",
        "Dayi Lin",
        "Ahmed Hassan"
      ],
      "abstract": "To balance the quality and inference cost of a Foundation Model (FM, such as\nlarge language models (LLMs)) powered software, people often opt to train a\nrouting model that routes requests to FMs with different sizes and\ncapabilities. Existing routing models rely on learning the optimal routing\ndecision from carefully curated data, require complex computations to be\nupdated, and do not consider the potential evolution of weaker FMs. In this\npaper, we propose Real-time Adaptive Routing (RAR), an approach to continuously\nadapt FM routing decisions while using guided in-context learning to enhance\nthe capabilities of weaker FM. The goal is to reduce reliance on stronger, more\nexpensive FMs. We evaluate our approach on different subsets of the popular\nMMLU benchmark. Over time, our approach routes 50.2% fewer requests to\ncomputationally expensive models while maintaining around 90.5% of the general\nresponse quality. In addition, the guides generated from stronger models have\nshown intra-domain generalization and led to a better quality of responses\ncompared to an equivalent approach with a standalone weaker FM.",
      "tldr_zh": "该论文提出 Real-time Adaptive Routing (RAR) 方法，通过连续学习和引导的 in-context learning 来优化 Foundation Model (FM) 驱动软件的路由决策，旨在减少对更强大、计算成本高的 FM 的依赖，同时提升弱 FM 的能力。RAR 利用强 FM 生成的引导信息来实时适应路由策略，并在 MMLU 基准的不同子集上进行评估。实验结果显示，该方法将路由到昂贵模型的请求减少了 50.2%，同时保持了约 90.5% 的响应质量，且生成的引导信息表现出内部域泛化能力，比单独使用弱 FM 的方法更有效。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.MA"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2411.09837v1",
      "published_date": "2024-11-14 23:02:30 UTC",
      "updated_date": "2024-11-14 23:02:30 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T00:23:16.609725"
    },
    {
      "arxiv_id": "2411.09834v2",
      "title": "A Benchmark for Long-Form Medical Question Answering",
      "title_zh": "翻译失败",
      "authors": [
        "Pedram Hosseini",
        "Jessica M. Sin",
        "Bing Ren",
        "Bryceton G. Thomas",
        "Elnaz Nouri",
        "Ali Farahanchi",
        "Saeed Hassanpour"
      ],
      "abstract": "There is a lack of benchmarks for evaluating large language models (LLMs) in\nlong-form medical question answering (QA). Most existing medical QA evaluation\nbenchmarks focus on automatic metrics and multiple-choice questions. While\nvaluable, these benchmarks fail to fully capture or assess the complexities of\nreal-world clinical applications where LLMs are being deployed. Furthermore,\nexisting studies on evaluating long-form answer generation in medical QA are\nprimarily closed-source, lacking access to human medical expert annotations,\nwhich makes it difficult to reproduce results and enhance existing baselines.\nIn this work, we introduce a new publicly available benchmark featuring\nreal-world consumer medical questions with long-form answer evaluations\nannotated by medical doctors. We performed pairwise comparisons of responses\nfrom various open and closed-source medical and general-purpose LLMs based on\ncriteria such as correctness, helpfulness, harmfulness, and bias. Additionally,\nwe performed a comprehensive LLM-as-a-judge analysis to study the alignment\nbetween human judgments and LLMs. Our preliminary results highlight the strong\npotential of open LLMs in medical QA compared to leading closed models. Code &\nData: https://github.com/lavita-ai/medical-eval-sphere",
      "tldr_zh": "本研究指出了现有医疗问答（QA）基准的不足，主要依赖自动指标和多项选择题，无法评估大型语言模型（LLMs）在长形式真实临床场景中的表现，并缺乏公开的人类专家注解。本文引入了一个新的公开基准，包含真实消费者医疗问题，由医疗医生对长形式答案进行注解，并通过配对比较各种开源和闭源LLMs的响应，评估标准包括正确性、帮助性、有害性和偏见，同时进行LLM-as-a-judge分析以检查人类判断的契合度。初步结果显示，开源LLMs在医疗QA中表现出色，可能优于领先的闭源模型，为未来LLMs评估提供可复现的资源（代码和数据见GitHub）。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "AIM-FM: Advancements in Medical Foundation Models Workshop, 38th\n  Conference on Neural Information Processing Systems (NeurIPS 2024)",
      "pdf_url": "http://arxiv.org/pdf/2411.09834v2",
      "published_date": "2024-11-14 22:54:38 UTC",
      "updated_date": "2024-11-19 21:04:38 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T00:23:28.698053"
    },
    {
      "arxiv_id": "2412.02145v1",
      "title": "Effective Mitigations for Systemic Risks from General-Purpose AI",
      "title_zh": "通用人工智能系统性风险的有效缓解措施",
      "authors": [
        "Risto Uuk",
        "Annemieke Brouwer",
        "Tim Schreier",
        "Noemi Dreksler",
        "Valeria Pulignano",
        "Rishi Bommasani"
      ],
      "abstract": "The systemic risks posed by general-purpose AI models are a growing concern,\nyet the effectiveness of mitigations remains underexplored. Previous research\nhas proposed frameworks for risk mitigation, but has left gaps in our\nunderstanding of the perceived effectiveness of measures for mitigating\nsystemic risks. Our study addresses this gap by evaluating how experts perceive\ndifferent mitigations that aim to reduce the systemic risks of general-purpose\nAI models. We surveyed 76 experts whose expertise spans AI safety; critical\ninfrastructure; democratic processes; chemical, biological, radiological, and\nnuclear risks (CBRN); and discrimination and bias. Among 27 mitigations\nidentified through a literature review, we find that a broad range of risk\nmitigation measures are perceived as effective in reducing various systemic\nrisks and technically feasible by domain experts. In particular, three\nmitigation measures stand out: safety incident reports and security information\nsharing, third-party pre-deployment model audits, and pre-deployment risk\nassessments. These measures show both the highest expert agreement ratings\n(>60\\%) across all four risk areas and are most frequently selected in experts'\npreferred combinations of measures (>40\\%). The surveyed experts highlighted\nthat external scrutiny, proactive evaluation and transparency are key\nprinciples for effective mitigation of systemic risks. We provide policy\nrecommendations for implementing the most promising measures, incorporating the\nqualitative contributions from experts. These insights should inform regulatory\nframeworks and industry practices for mitigating the systemic risks associated\nwith general-purpose AI.",
      "tldr_zh": "这篇论文评估了专家对缓解通用 AI 系统性风险措施的有效性，通过调查76名跨领域专家（包括AI安全、关键基础设施和CBRN风险等）来填补现有研究空白。研究识别了27种潜在缓解措施，并发现安全事件报告、安全信息共享、第三方预部署模型审计和风险评估等措施获得了最高专家共识（超过60%的同意率），并在专家首选组合中频繁出现（超过40%）。总体而言，论文强调外部审查、主动评估和透明度作为关键原则，并提供政策推荐，以指导监管框架和行业实践，减少通用 AI 的系统性风险。",
      "categories": [
        "cs.CY",
        "cs.AI"
      ],
      "primary_category": "cs.CY",
      "comment": "78 pages, 7 figures, 2 tables",
      "pdf_url": "http://arxiv.org/pdf/2412.02145v1",
      "published_date": "2024-11-14 22:39:25 UTC",
      "updated_date": "2024-11-14 22:39:25 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T00:23:39.829476"
    },
    {
      "arxiv_id": "2411.09822v1",
      "title": "A Self-Supervised Model for Multi-modal Stroke Risk Prediction",
      "title_zh": "翻译失败",
      "authors": [
        "Camille Delgrange",
        "Olga Demler",
        "Samia Mora",
        "Bjoern Menze",
        "Ezequiel de la Rosa",
        "Neda Davoudi"
      ],
      "abstract": "Predicting stroke risk is a complex challenge that can be enhanced by\nintegrating diverse clinically available data modalities. This study introduces\na self-supervised multimodal framework that combines 3D brain imaging, clinical\ndata, and image-derived features to improve stroke risk prediction prior to\nonset. By leveraging large unannotated clinical datasets, the framework\ncaptures complementary and synergistic information across image and tabular\ndata modalities. Our approach is based on a contrastive learning framework that\ncouples contrastive language-image pretraining with an image-tabular matching\nmodule, to better align multimodal data representations in a shared latent\nspace. The model is trained on the UK Biobank, which includes structural brain\nMRI and clinical data. We benchmark its performance against state-of-the-art\nunimodal and multimodal methods using tabular, image, and image-tabular\ncombinations under diverse frozen and trainable model settings. The proposed\nmodel outperformed self-supervised tabular (image) methods by 2.6% (2.6%) in\nROC-AUC and by 3.3% (5.6%) in balanced accuracy. Additionally, it showed a 7.6%\nincrease in balanced accuracy compared to the best multimodal supervised model.\nThrough interpretable tools, our approach demonstrated better integration of\ntabular and image data, providing richer and more aligned embeddings.\nGradient-weighted Class Activation Mapping heatmaps further revealed activated\nbrain regions commonly associated in the literature with brain aging, stroke\nrisk, and clinical outcomes. This robust self-supervised multimodal framework\nsurpasses state-of-the-art methods for stroke risk prediction and offers a\nstrong foundation for future studies integrating diverse data modalities to\nadvance clinical predictive modelling.",
      "tldr_zh": "这篇论文提出了一种自监督多模态框架，用于整合3D脑成像、临床数据和图像衍生特征，以提升中风风险预测的准确性。该框架基于对比学习方法，包括对比语言-图像预训练和图像-表格匹配模块，在UK Biobank数据集上训练，通过对齐多模态数据表示来捕获互补信息。实验结果显示，该模型在ROC-AUC上比自监督表格（图像）方法提高了2.6%（2.6%），在平衡准确率上提高了3.3%（5.6%），并比最佳多模态监督模型提高了7.6%的平衡准确率。此外，通过Gradient-weighted Class Activation Mapping热图和可解释工具，该框架展示了更好的数据整合能力，并揭示了与中风风险相关的脑区，提供了一个强大的临床预测基础。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "Accepted as oral paper at AIM-FM workshop, Neurips 2024",
      "pdf_url": "http://arxiv.org/pdf/2411.09822v1",
      "published_date": "2024-11-14 22:00:37 UTC",
      "updated_date": "2024-11-14 22:00:37 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T00:23:53.633816"
    },
    {
      "arxiv_id": "2411.09820v1",
      "title": "WelQrate: Defining the Gold Standard in Small Molecule Drug Discovery Benchmarking",
      "title_zh": "WelQrate：定义小分子药物发现基准测试的黄金标准",
      "authors": [
        "Yunchao Liu",
        "Ha Dong",
        "Xin Wang",
        "Rocco Moretti",
        "Yu Wang",
        "Zhaoqian Su",
        "Jiawei Gu",
        "Bobby Bodenheimer",
        "Charles David Weaver",
        "Jens Meiler",
        "Tyler Derr"
      ],
      "abstract": "While deep learning has revolutionized computer-aided drug discovery, the AI\ncommunity has predominantly focused on model innovation and placed less\nemphasis on establishing best benchmarking practices. We posit that without a\nsound model evaluation framework, the AI community's efforts cannot reach their\nfull potential, thereby slowing the progress and transfer of innovation into\nreal-world drug discovery. Thus, in this paper, we seek to establish a new gold\nstandard for small molecule drug discovery benchmarking, WelQrate.\nSpecifically, our contributions are threefold: WelQrate Dataset Collection - we\nintroduce a meticulously curated collection of 9 datasets spanning 5\ntherapeutic target classes. Our hierarchical curation pipelines, designed by\ndrug discovery experts, go beyond the primary high-throughput screen by\nleveraging additional confirmatory and counter screens along with rigorous\ndomain-driven preprocessing, such as Pan-Assay Interference Compounds (PAINS)\nfiltering, to ensure the high-quality data in the datasets; WelQrate Evaluation\nFramework - we propose a standardized model evaluation framework considering\nhigh-quality datasets, featurization, 3D conformation generation, evaluation\nmetrics, and data splits, which provides a reliable benchmarking for drug\ndiscovery experts conducting real-world virtual screening; Benchmarking - we\nevaluate model performance through various research questions using the\nWelQrate dataset collection, exploring the effects of different models, dataset\nquality, featurization methods, and data splitting strategies on the results.\nIn summary, we recommend adopting our proposed WelQrate as the gold standard in\nsmall molecule drug discovery benchmarking. The WelQrate dataset collection,\nalong with the curation codes, and experimental scripts are all publicly\navailable at WelQrate.org.",
      "tldr_zh": "本文提出 WelQrate 作为小分子药物发现基准测试的新金标准，以解决 AI 社区在 deep learning 应用中更注重模型创新而忽略基准实践的问题。主要贡献包括：WelQrate Dataset Collection，一个由药物发现专家设计的层次化 curation 管道创建的9个高质量数据集，覆盖5个治疗目标类别，并通过 confirmatory 和 counter screens 以及 Pan-Assay Interference Compounds (PAINS) filtering 等预处理确保数据可靠性；WelQrate Evaluation Framework，一个标准化评估框架，涵盖 featurization、3D conformation generation、evaluation metrics 和 data splits，以支持真实世界的 virtual screening 基准测试。通过实验评估不同模型、数据集质量、featurization 方法和数据分割策略的影响，WelQrate 提供公开资源，推动 AI 在药物发现中的实际应用。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "q-bio.BM"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2411.09820v1",
      "published_date": "2024-11-14 21:49:41 UTC",
      "updated_date": "2024-11-14 21:49:41 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T00:24:05.013580"
    },
    {
      "arxiv_id": "2411.09807v1",
      "title": "Evaluating Loss Landscapes from a Topology Perspective",
      "title_zh": "从拓扑视角评估损失景观",
      "authors": [
        "Tiankai Xie",
        "Caleb Geniesse",
        "Jiaqing Chen",
        "Yaoqing Yang",
        "Dmitriy Morozov",
        "Michael W. Mahoney",
        "Ross Maciejewski",
        "Gunther H. Weber"
      ],
      "abstract": "Characterizing the loss of a neural network with respect to model parameters,\ni.e., the loss landscape, can provide valuable insights into properties of that\nmodel. Various methods for visualizing loss landscapes have been proposed, but\nless emphasis has been placed on quantifying and extracting actionable and\nreproducible insights from these complex representations. Inspired by powerful\ntools from topological data analysis (TDA) for summarizing the structure of\nhigh-dimensional data, here we characterize the underlying shape (or topology)\nof loss landscapes, quantifying the topology to reveal new insights about\nneural networks. To relate our findings to the machine learning (ML)\nliterature, we compute simple performance metrics (e.g., accuracy, error), and\nwe characterize the local structure of loss landscapes using Hessian-based\nmetrics (e.g., largest eigenvalue, trace, eigenvalue spectral density).\nFollowing this approach, we study established models from image pattern\nrecognition (e.g., ResNets) and scientific ML (e.g., physics-informed neural\nnetworks), and we show how quantifying the shape of loss landscapes can provide\nnew insights into model performance and learning dynamics.",
      "tldr_zh": "这篇论文从拓扑视角评估神经网络的损失景观（loss landscapes），使用拓扑数据分析（TDA）来量化其形状，从而提取可操作和可重复的洞见。方法包括计算性能指标（如准确率、错误率）和Hessian-based metrics（如最大特征值、迹、特征值谱密度），并应用于图像识别模型（如ResNets）和科学机器学习模型（如physics-informed neural networks）。研究结果显示，这种拓扑量化方法能揭示模型性能和学习动态的新见解，提供更深入的理解。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2411.09807v1",
      "published_date": "2024-11-14 20:46:26 UTC",
      "updated_date": "2024-11-14 20:46:26 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T00:24:16.122724"
    },
    {
      "arxiv_id": "2411.09767v1",
      "title": "Deep Learning for Fetal Inflammatory Response Diagnosis in the Umbilical Cord",
      "title_zh": "翻译失败",
      "authors": [
        "Marina A. Ayad",
        "Ramin Nateghi",
        "Abhishek Sharma",
        "Lawrence Chillrud",
        "Tilly Seesillapachai",
        "Lee A. D. Cooper",
        "Jeffery A. Goldstein"
      ],
      "abstract": "Inflammation of the umbilical cord can be seen as a result of ascending\nintrauterine infection or other inflammatory stimuli. Acute fetal inflammatory\nresponse (FIR) is characterized by infiltration of the umbilical cord by fetal\nneutrophils, and can be associated with neonatal sepsis or fetal inflammatory\nresponse syndrome. Recent advances in deep learning in digital pathology have\ndemonstrated favorable performance across a wide range of clinical tasks, such\nas diagnosis and prognosis. In this study we classified FIR from whole slide\nimages (WSI). We digitized 4100 histological slides of umbilical cord stained\nwith hematoxylin and eosin(H&E) and extracted placental diagnoses from the\nelectronic health record. We build models using attention-based whole slide\nlearning models. We compared strategies between features extracted by a model\n(ConvNeXtXLarge) pretrained on non-medical images (ImageNet), and one\npretrained using histopathology images (UNI). We trained multiple iterations of\neach model and combined them into an ensemble. The predictions from the\nensemble of models trained using UNI achieved an overall balanced accuracy of\n0.836 on the test dataset. In comparison, the ensembled predictions using\nConvNeXtXLarge had a lower balanced accuracy of 0.7209. Heatmaps generated from\ntop accuracy model appropriately highlighted arteritis in cases of FIR 2. In\nFIR 1, the highest performing model assigned high attention to areas of\nactivated-appearing stroma in Wharton's Jelly. However, other high-performing\nmodels assigned attention to umbilical vessels. We developed models for\ndiagnosis of FIR from placental histology images, helping reduce interobserver\nvariability among pathologists. Future work may examine the utility of these\nmodels for identifying infants at risk of systemic inflammatory response or\nearly onset neonatal sepsis.",
      "tldr_zh": "本研究利用深度学习诊断脐带中的胎儿炎症反应 (FIR)，通过分析4100张用hematoxylin and eosin (H&E) 染色的全滑片图像 (WSI) 来分类FIR。研究构建了基于注意力的全滑片学习模型，比较了使用ImageNet预训练的ConvNeXtXLarge和使用组织病理学图像预训练的UNI模型，并通过多个迭代训练形成ensemble。结果显示，UNI模型的ensemble在测试数据集上达到0.836的balanced accuracy，而ConvNeXtXLarge模型仅为0.7209；热图进一步验证了模型对FIR 1和FIR 2关键区域（如Wharton's Jelly基质和动脉炎）的准确关注。该方法有助于减少病理学家间的观察者变异性，并为未来识别新生儿败血症风险提供潜在工具。",
      "categories": [
        "eess.IV",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "eess.IV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2411.09767v1",
      "published_date": "2024-11-14 19:24:46 UTC",
      "updated_date": "2024-11-14 19:24:46 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T00:24:29.210890"
    },
    {
      "arxiv_id": "2411.09702v1",
      "title": "On the Surprising Effectiveness of Attention Transfer for Vision Transformers",
      "title_zh": "关于视觉Transformer注意力转移的惊人有效性",
      "authors": [
        "Alexander C. Li",
        "Yuandong Tian",
        "Beidi Chen",
        "Deepak Pathak",
        "Xinlei Chen"
      ],
      "abstract": "Conventional wisdom suggests that pre-training Vision Transformers (ViT)\nimproves downstream performance by learning useful representations. Is this\nactually true? We investigate this question and find that the features and\nrepresentations learned during pre-training are not essential. Surprisingly,\nusing only the attention patterns from pre-training (i.e., guiding how\ninformation flows between tokens) is sufficient for models to learn high\nquality features from scratch and achieve comparable downstream performance. We\nshow this by introducing a simple method called attention transfer, where only\nthe attention patterns from a pre-trained teacher ViT are transferred to a\nstudent, either by copying or distilling the attention maps. Since attention\ntransfer lets the student learn its own features, ensembling it with a\nfine-tuned teacher also further improves accuracy on ImageNet. We\nsystematically study various aspects of our findings on the sufficiency of\nattention maps, including distribution shift settings where they underperform\nfine-tuning. We hope our exploration provides a better understanding of what\npre-training accomplishes and leads to a useful alternative to the standard\npractice of fine-tuning",
      "tldr_zh": "本研究发现，预训练 Vision Transformers (ViT) 的特征和表示并非下游性能提升的关键因素；仅转移注意力模式 (attention patterns) 即可让模型从零开始学习高质量特征，并实现与完整预训练相当的性能。研究引入了 attention transfer 方法，通过复制或蒸馏预训练教师 ViT 的注意力映射，让学生模型独立学习特征，并通过与教师模型集成，进一步提升了 ImageNet 上的准确率。实验还系统分析了注意力模式的充分性，包括在分布偏移 (distribution shift) 场景下的表现，提供了一种对标准 fine-tuning 的有用替代方案。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CV",
        "cs.NE"
      ],
      "primary_category": "cs.LG",
      "comment": "NeurIPS 2024. Code:\n  https://github.com/alexlioralexli/attention-transfer",
      "pdf_url": "http://arxiv.org/pdf/2411.09702v1",
      "published_date": "2024-11-14 18:59:40 UTC",
      "updated_date": "2024-11-14 18:59:40 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T00:24:40.733552"
    },
    {
      "arxiv_id": "2411.09689v1",
      "title": "LLM Hallucination Reasoning with Zero-shot Knowledge Test",
      "title_zh": "翻译失败",
      "authors": [
        "Seongmin Lee",
        "Hsiang Hsu",
        "Chun-Fu Chen"
      ],
      "abstract": "LLM hallucination, where LLMs occasionally generate unfaithful text, poses\nsignificant challenges for their practical applications. Most existing\ndetection methods rely on external knowledge, LLM fine-tuning, or\nhallucination-labeled datasets, and they do not distinguish between different\ntypes of hallucinations, which are crucial for improving detection performance.\nWe introduce a new task, Hallucination Reasoning, which classifies\nLLM-generated text into one of three categories: aligned, misaligned, and\nfabricated. Our novel zero-shot method assesses whether LLM has enough\nknowledge about a given prompt and text. Our experiments conducted on new\ndatasets demonstrate the effectiveness of our method in hallucination reasoning\nand underscore its importance for enhancing detection performance.",
      "tldr_zh": "这篇论文针对LLM hallucination问题（LLMs生成不真实文本的挑战），提出了一种新任务Hallucination Reasoning，将生成的文本分类为aligned（对齐的）、misaligned（错位的）或fabricated（虚构的），以区分不同类型hallucination并提升检测性能。作者引入了一个零-shot方法，通过评估LLM对给定提示和文本的知识水平来实现检测，而无需外部知识或微调。实验在新数据集上验证了该方法的有效性，并强调了其在改进hallucination检测方面的关键作用。",
      "categories": [
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.AI",
      "comment": "12 pages, 2 figures",
      "pdf_url": "http://arxiv.org/pdf/2411.09689v1",
      "published_date": "2024-11-14 18:55:26 UTC",
      "updated_date": "2024-11-14 18:55:26 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T00:24:52.072761"
    },
    {
      "arxiv_id": "2411.09683v1",
      "title": "Towards a Classification of Open-Source ML Models and Datasets for Software Engineering",
      "title_zh": "翻译失败",
      "authors": [
        "Alexandra González",
        "Xavier Franch",
        "David Lo",
        "Silverio Martínez-Fernández"
      ],
      "abstract": "Background: Open-Source Pre-Trained Models (PTMs) and datasets provide\nextensive resources for various Machine Learning (ML) tasks, yet these\nresources lack a classification tailored to Software Engineering (SE) needs.\nAims: We apply an SE-oriented classification to PTMs and datasets on a popular\nopen-source ML repository, Hugging Face (HF), and analyze the evolution of PTMs\nover time. Method: We conducted a repository mining study. We started with a\nsystematically gathered database of PTMs and datasets from the HF API. Our\nselection was refined by analyzing model and dataset cards and metadata, such\nas tags, and confirming SE relevance using Gemini 1.5 Pro. All analyses are\nreplicable, with a publicly accessible replication package. Results: The most\ncommon SE task among PTMs and datasets is code generation, with a primary focus\non software development and limited attention to software management. Popular\nPTMs and datasets mainly target software development. Among ML tasks, text\ngeneration is the most common in SE PTMs and datasets. There has been a marked\nincrease in PTMs for SE since 2023 Q2. Conclusions: This study underscores the\nneed for broader task coverage to enhance the integration of ML within SE\npractices.",
      "tldr_zh": "这篇论文针对软件工程(SE)需求，对Hugging Face(HF)上的开源预训练模型(PTMs)和数据集进行了分类分析，并考察了PTMs的演变趋势。研究方法采用仓库挖掘技术，通过系统收集HF数据、分析模型和数据集卡片、元数据（如标签），并使用Gemini 1.5 Pro确认SE相关性，所有过程可复制。结果显示，代码生成是最常见SE任务，主要关注软件开发而非软件管理，文本生成主导ML任务，且自2023 Q2以来SE PTMs显著增加。结论指出，需要更广泛的任务覆盖，以增强ML在SE实践中的整合。",
      "categories": [
        "cs.SE",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.SE",
      "comment": "5 pages, 8 figures",
      "pdf_url": "http://arxiv.org/pdf/2411.09683v1",
      "published_date": "2024-11-14 18:52:05 UTC",
      "updated_date": "2024-11-14 18:52:05 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T00:25:05.738818"
    },
    {
      "arxiv_id": "2411.10490v1",
      "title": "AI-Spectra: A Visual Dashboard for Model Multiplicity to Enhance Informed and Transparent Decision-Making",
      "title_zh": "翻译失败",
      "authors": [
        "Gilles Eerlings",
        "Sebe Vanbrabant",
        "Jori Liesenborgs",
        "Gustavo Rovelo Ruiz",
        "Davy Vanacken",
        "Kris Luyten"
      ],
      "abstract": "We present an approach, AI-Spectra, to leverage model multiplicity for\ninteractive systems. Model multiplicity means using slightly different AI\nmodels yielding equally valid outcomes or predictions for the same task, thus\nrelying on many simultaneous \"expert advisors\" that can have different\nopinions. Dealing with multiple AI models that generate potentially divergent\nresults for the same task is challenging for users to deal with. It helps users\nunderstand and identify AI models are not always correct and might differ, but\nit can also result in an information overload when being confronted with\nmultiple results instead of one. AI-Spectra leverages model multiplicity by\nusing a visual dashboard designed for conveying what AI models generate which\nresults while minimizing the cognitive effort to detect consensus among models\nand what type of models might have different opinions. We use a custom\nadaptation of Chernoff faces for AI-Spectra; Chernoff Bots. This visualization\ntechnique lets users quickly interpret complex, multivariate model\nconfigurations and compare predictions across multiple models. Our design is\ninformed by building on established Human-AI Interaction guidelines and well\nknow practices in information visualization. We validated our approach through\na series of experiments training a wide variation of models with the MNIST\ndataset to perform number recognition. Our work contributes to the growing\ndiscourse on making AI systems more transparent, trustworthy, and effective\nthrough the strategic use of multiple models.",
      "tldr_zh": "本研究提出 AI-Spectra，一种视觉仪表板（visual dashboard），利用 model multiplicity（多个略有不同的 AI 模型）来增强决策的透明性和信息性，帮助用户处理模型分歧并减少信息过载。系统采用 Chernoff Bots（Chernoff faces 的自定义适应版）作为可视化技术，让用户快速解读多变量模型配置、比较预测结果并检测共识。实验通过在 MNIST 数据集上训练多种模型进行数字识别，证明 AI-Spectra 显著提高了 AI 系统的透明度、可信度和整体有效性。",
      "categories": [
        "cs.HC",
        "cs.AI",
        "H.5.2; I.2.0; H.4.2; I.2.6"
      ],
      "primary_category": "cs.HC",
      "comment": "Accepted for publication in an LNCS Volume \"Engineering Interactive\n  Computer Systems - EICS 2024 - International Workshops and Doctoral\n  Consortium, Selected Papers\"",
      "pdf_url": "http://arxiv.org/pdf/2411.10490v1",
      "published_date": "2024-11-14 18:50:41 UTC",
      "updated_date": "2024-11-14 18:50:41 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T00:25:16.802821"
    },
    {
      "arxiv_id": "2411.09678v2",
      "title": "NeuralDEM -- Real-time Simulation of Industrial Particulate Flows",
      "title_zh": "NeuralDEM：工业颗粒流的实时模拟",
      "authors": [
        "Benedikt Alkin",
        "Tobias Kronlachner",
        "Samuele Papa",
        "Stefan Pirker",
        "Thomas Lichtenegger",
        "Johannes Brandstetter"
      ],
      "abstract": "Advancements in computing power have made it possible to numerically simulate\nlarge-scale fluid-mechanical and/or particulate systems, many of which are\nintegral to core industrial processes. Among the different numerical methods\navailable, the discrete element method (DEM) provides one of the most accurate\nrepresentations of a wide range of physical systems involving granular and\ndiscontinuous materials. Consequently, DEM has become a widely accepted\napproach for tackling engineering problems connected to granular flows and\npowder mechanics. Additionally, DEM can be integrated with grid-based\ncomputational fluid dynamics (CFD) methods, enabling the simulation of chemical\nprocesses taking place, e.g., in fluidized beds. However, DEM is\ncomputationally intensive because of the intrinsic multiscale nature of\nparticulate systems, restricting simulation duration or number of particles.\nTowards this end, NeuralDEM presents an end-to-end approach to replace slow\nnumerical DEM routines with fast, adaptable deep learning surrogates. NeuralDEM\nis capable of picturing long-term transport processes across different regimes\nusing macroscopic observables without any reference to microscopic model\nparameters. First, NeuralDEM treats the Lagrangian discretization of DEM as an\nunderlying continuous field, while simultaneously modeling macroscopic behavior\ndirectly as additional auxiliary fields. Second, NeuralDEM introduces\nmulti-branch neural operators scalable to real-time modeling of\nindustrially-sized scenarios - from slow and pseudo-steady to fast and\ntransient. Such scenarios have previously posed insurmountable challenges for\ndeep learning models. Notably, NeuralDEM faithfully models coupled CFD-DEM\nfluidized bed reactors of 160k CFD cells and 500k DEM particles for\ntrajectories of 28s. NeuralDEM will open many new doors to advanced engineering\nand much faster process cycles.",
      "tldr_zh": "本文提出 NeuralDEM，一种端到端深度学习框架，用于替换计算密集的 Discrete Element Method (DEM) 例程，实现工业颗粒流的实时模拟。NeuralDEM 通过将 DEM 的拉格朗日离散化视为连续场，并引入可扩展的多分支神经算子，建模宏观行为并处理从慢速稳态到快速瞬态的场景。实验结果显示，该方法能精确模拟包含 160k CFD 细胞和 500k DEM 粒子的流化床反应器，轨迹长达 28 秒，从而加速工程过程周期并开拓新应用。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "Project page: https://nx-ai.github.io/NeuralDEM/",
      "pdf_url": "http://arxiv.org/pdf/2411.09678v2",
      "published_date": "2024-11-14 18:44:31 UTC",
      "updated_date": "2025-02-27 10:26:19 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T00:25:28.478718"
    },
    {
      "arxiv_id": "2411.10489v1",
      "title": "Biometrics in Extended Reality: A Review",
      "title_zh": "扩展现实中的生物识别：综述",
      "authors": [
        "Ayush Agarwal",
        "Raghavendra Ramachandra",
        "Sushma Venkatesh",
        "S. R. Mahadeva Prasanna"
      ],
      "abstract": "In the domain of Extended Reality (XR), particularly Virtual Reality (VR),\nextensive research has been devoted to harnessing this transformative\ntechnology in various real-world applications. However, a critical challenge\nthat must be addressed before unleashing the full potential of XR in practical\nscenarios is to ensure robust security and safeguard user privacy. This paper\npresents a systematic survey of the utility of biometric characteristics\napplied in the XR environment. To this end, we present a comprehensive overview\nof the different types of biometric modalities used for authentication and\nrepresentation of users in a virtual environment. We discuss different\nbiometric vulnerability gateways in general XR systems for the first time in\nthe literature along with taxonomy. A comprehensive discussion on generating\nand authenticating biometric-based photorealistic avatars in XR environments is\npresented with a stringent taxonomy. We also discuss the availability of\ndifferent datasets that are widely employed in evaluating biometric\nauthentication in XR environments together with performance evaluation metrics.\nFinally, we discuss the open challenges and potential future work that need to\nbe addressed in the field of biometrics in XR.",
      "tldr_zh": "这篇综述论文探讨了在扩展现实（XR）环境中应用生物特征识别（biometrics）技术，以提升安全性和用户隐私保护。论文系统概述了各种生物特征模式（如指纹、面部识别等）用于XR中的用户认证和虚拟表示，并首次提出XR系统中的生物特征漏洞门户及其分类。作者还详细讨论了生成和认证基于生物特征的逼真头像（photorealistic avatars）的过程，并总结了相关数据集和性能评估指标。最终，论文指出了XR生物特征领域的开放挑战和潜在未来研究方向，如增强鲁棒性和隐私机制。",
      "categories": [
        "cs.CR",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.CR",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2411.10489v1",
      "published_date": "2024-11-14 18:25:20 UTC",
      "updated_date": "2024-11-14 18:25:20 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T00:25:39.802842"
    },
    {
      "arxiv_id": "2411.09648v1",
      "title": "Med-Bot: An AI-Powered Assistant to Provide Accurate and Reliable Medical Information",
      "title_zh": "Med-Bot：一个基于AI的助手，用于提供准确和可靠的医疗信息",
      "authors": [
        "Ahan Bhatt",
        "Nandan Vaghela"
      ],
      "abstract": "This paper introduces Med-Bot, an AI-powered chatbot designed to provide\nusers with accurate and reliable medical information. Utilizing advanced\nlibraries and frameworks such as PyTorch, Chromadb, Langchain and Autogptq,\nMed-Bot is built to handle the complexities of natural language understanding\nin a healthcare context. The integration of llamaassisted data processing and\nAutoGPT-Q provides enhanced performance in processing and responding to queries\nbased on PDFs of medical literature, ensuring that users receive precise and\ntrustworthy information. This research details the methodologies employed in\ndeveloping Med-Bot and evaluates its effectiveness in disseminating healthcare\ninformation.",
      "tldr_zh": "本论文介绍了 Med-Bot，一种基于 AI 的聊天机器人，旨在为用户提供准确可靠的医疗信息。Med-Bot 利用 PyTorch、Chromadb、Langchain 和 Autogptq 等框架，结合 llamaassisted 数据处理和 AutoGPT-Q 技术，从医疗文献 PDF 中处理自然语言查询，实现高效响应。研究详细阐述了开发方法，并评估了其在医疗信息传播方面的有效性。",
      "categories": [
        "cs.AI",
        "cs.LG",
        "cs.NE"
      ],
      "primary_category": "cs.AI",
      "comment": "3 figures, 5 pages Keywords-LLM, AI-powered healthcare, Medical\n  chatbot, Context-based interaction, Llama-assisted data processing,\n  AutoGPT-Q, PyTorch, TensorFlow, Reliable medical information, Machine\n  learning in healthcare, Conversational AI",
      "pdf_url": "http://arxiv.org/pdf/2411.09648v1",
      "published_date": "2024-11-14 18:17:30 UTC",
      "updated_date": "2024-11-14 18:17:30 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T00:25:52.548770"
    },
    {
      "arxiv_id": "2411.09642v1",
      "title": "On the Limits of Language Generation: Trade-Offs Between Hallucination and Mode Collapse",
      "title_zh": "翻译失败",
      "authors": [
        "Alkis Kalavasis",
        "Anay Mehrotra",
        "Grigoris Velegkas"
      ],
      "abstract": "Specifying all desirable properties of a language model is challenging, but\ncertain requirements seem essential. Given samples from an unknown language,\nthe trained model should produce valid strings not seen in training and be\nexpressive enough to capture the language's full richness. Otherwise,\noutputting invalid strings constitutes \"hallucination,\" and failing to capture\nthe full range leads to \"mode collapse.\" We ask if a language model can meet\nboth requirements.\n  We investigate this within a statistical language generation setting building\non Gold and Angluin. Here, the model receives random samples from a\ndistribution over an unknown language K, which belongs to a possibly infinite\ncollection of languages. The goal is to generate unseen strings from K. We say\nthe model generates from K with consistency and breadth if, as training size\nincreases, its output converges to all unseen strings in K.\n  Kleinberg and Mullainathan [KM24] asked if consistency and breadth in\nlanguage generation are possible. We answer this negatively: for a large class\nof language models, including next-token prediction models, this is impossible\nfor most collections of candidate languages. This contrasts with [KM24]'s\nresult, showing consistent generation without breadth is possible for any\ncountable collection of languages. Our finding highlights that generation with\nbreadth fundamentally differs from generation without breadth.\n  As a byproduct, we establish near-tight bounds on the number of samples\nneeded for generation with or without breadth.\n  Finally, our results offer hope: consistent generation with breadth is\nachievable for any countable collection of languages when negative examples\n(strings outside K) are available alongside positive ones. This suggests that\npost-training feedback, which encodes negative examples, can be crucial in\nreducing hallucinations while limiting mode collapse.",
      "tldr_zh": "这篇论文探讨了语言生成模型在 hallucination（幻觉）和 mode collapse（模式坍缩）之间的权衡，分析了模型是否能同时生成未见过的有效字符串（consistency）和捕捉语言的全部丰富性（breadth）。研究基于统计语言生成框架，发现对于大多数语言集合，包括 next-token prediction 模型，无法同时实现这两者，与[KM24]的结果形成对比，后者证明了无 breadth 的 consistent generation 是可能的。作者建立了样本需求的近似最优边界，并指出，如果提供负例（negative examples），模型即可实现 consistent generation with breadth，这强调了 post-training feedback 在减少 hallucination 和 mode collapse 方面的关键作用。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL",
        "cs.DS",
        "stat.ML"
      ],
      "primary_category": "cs.LG",
      "comment": "Abstract shortened to fit arXiv limit",
      "pdf_url": "http://arxiv.org/pdf/2411.09642v1",
      "published_date": "2024-11-14 18:06:55 UTC",
      "updated_date": "2024-11-14 18:06:55 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T00:26:04.947792"
    },
    {
      "arxiv_id": "2411.09627v2",
      "title": "One-Shot Manipulation Strategy Learning by Making Contact Analogies",
      "title_zh": "翻译失败",
      "authors": [
        "Yuyao Liu",
        "Jiayuan Mao",
        "Joshua Tenenbaum",
        "Tomás Lozano-Pérez",
        "Leslie Pack Kaelbling"
      ],
      "abstract": "We present a novel approach, MAGIC (manipulation analogies for generalizable\nintelligent contacts), for one-shot learning of manipulation strategies with\nfast and extensive generalization to novel objects. By leveraging a reference\naction trajectory, MAGIC effectively identifies similar contact points and\nsequences of actions on novel objects to replicate a demonstrated strategy,\nsuch as using different hooks to retrieve distant objects of different shapes\nand sizes. Our method is based on a two-stage contact-point matching process\nthat combines global shape matching using pretrained neural features with local\ncurvature analysis to ensure precise and physically plausible contact points.\nWe experiment with three tasks including scooping, hanging, and hooking\nobjects. MAGIC demonstrates superior performance over existing methods,\nachieving significant improvements in runtime speed and generalization to\ndifferent object categories. Website: https://magic-2024.github.io/ .",
      "tldr_zh": "本文提出了一种名为 MAGIC（manipulation analogies for generalizable intelligent contacts）的创新方法，用于实现一-shot 学习操纵策略，并快速泛化到新物体，例如使用不同钩子取回各种形状和大小的物体。MAGIC 采用两阶段接触点匹配过程，包括全局形状匹配（利用预训练神经特征）和局部曲率分析，以确保精确且物理可信的接触点和动作序列。在 scooping、hanging 和 hooking 等任务的实验中，该方法显著优于现有方法，提高了运行速度和对不同物体类别的泛化性能。",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.RO",
      "comment": "ICRA 2025; CoRL LEAP Workshop, 2024",
      "pdf_url": "http://arxiv.org/pdf/2411.09627v2",
      "published_date": "2024-11-14 17:54:43 UTC",
      "updated_date": "2025-03-23 08:02:09 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T00:26:17.009750"
    },
    {
      "arxiv_id": "2411.09730v1",
      "title": "SureMap: Simultaneous Mean Estimation for Single-Task and Multi-Task Disaggregated Evaluation",
      "title_zh": "翻译失败",
      "authors": [
        "Mikhail Khodak",
        "Lester Mackey",
        "Alexandra Chouldechova",
        "Miroslav Dudík"
      ],
      "abstract": "Disaggregated evaluation -- estimation of performance of a machine learning\nmodel on different subpopulations -- is a core task when assessing performance\nand group-fairness of AI systems. A key challenge is that evaluation data is\nscarce, and subpopulations arising from intersections of attributes (e.g.,\nrace, sex, age) are often tiny. Today, it is common for multiple clients to\nprocure the same AI model from a model developer, and the task of disaggregated\nevaluation is faced by each customer individually. This gives rise to what we\ncall the multi-task disaggregated evaluation problem, wherein multiple clients\nseek to conduct a disaggregated evaluation of a given model in their own data\nsetting (task). In this work we develop a disaggregated evaluation method\ncalled SureMap that has high estimation accuracy for both multi-task and\nsingle-task disaggregated evaluations of blackbox models. SureMap's efficiency\ngains come from (1) transforming the problem into structured simultaneous\nGaussian mean estimation and (2) incorporating external data, e.g., from the AI\nsystem creator or from their other clients. Our method combines maximum a\nposteriori (MAP) estimation using a well-chosen prior together with\ncross-validation-free tuning via Stein's unbiased risk estimate (SURE). We\nevaluate SureMap on disaggregated evaluation tasks in multiple domains,\nobserving significant accuracy improvements over several strong competitors.",
      "tldr_zh": "该研究针对机器学习模型的离散评估（disaggregated evaluation）问题，提出SureMap方法，以解决评估数据稀缺和子群体（如种族、性别交叉）规模小的挑战。SureMap将问题转化为结构化的同时高斯均值估计（simultaneous Gaussian mean estimation），并通过整合外部数据、最大后验估计（MAP）和Stein's unbiased risk estimate（SURE）进行高效调优，从而支持单任务和多任务场景下的高精度评估。在多个领域实验中，SureMap显著提升了估计准确性，优于现有竞争方法。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "stat.AP",
        "stat.ML"
      ],
      "primary_category": "cs.LG",
      "comment": "NeurIPS 2024",
      "pdf_url": "http://arxiv.org/pdf/2411.09730v1",
      "published_date": "2024-11-14 17:53:35 UTC",
      "updated_date": "2024-11-14 17:53:35 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T00:26:27.565600"
    },
    {
      "arxiv_id": "2411.09623v2",
      "title": "Vision-based Manipulation of Transparent Plastic Bags in Industrial Setups",
      "title_zh": "翻译失败",
      "authors": [
        "F. Adetunji",
        "A. Karukayil",
        "P. Samant",
        "S. Shabana",
        "F. Varghese",
        "U. Upadhyay",
        "R. A. Yadav",
        "A. Partridge",
        "E. Pendleton",
        "R. Plant",
        "Y. Petillot",
        "M. Koskinopoulou"
      ],
      "abstract": "This paper addresses the challenges of vision-based manipulation for\nautonomous cutting and unpacking of transparent plastic bags in industrial\nsetups, aligning with the Industry 4.0 paradigm. Industry 4.0, driven by data,\nconnectivity, analytics, and robotics, promises enhanced accessibility and\nsustainability throughout the value chain. The integration of autonomous\nsystems, including collaborative robots (cobots), into industrial processes is\npivotal for efficiency and safety. The proposed solution employs advanced\nMachine Learning algorithms, particularly Convolutional Neural Networks (CNNs),\nto identify transparent plastic bags under varying lighting and background\nconditions. Tracking algorithms and depth sensing technologies are utilized for\n3D spatial awareness during pick and placement. The system addresses challenges\nin grasping and manipulation, considering optimal points, compliance control\nwith vacuum gripping technology, and real-time automation for safe interaction\nin dynamic environments. The system's successful testing and validation in the\nlab with the FRANKA robot arm, showcases its potential for widespread\nindustrial applications, while demonstrating effectiveness in automating the\nunpacking and cutting of transparent plastic bags for an 8-stack bulk-loader\nbased on specific requirements and rigorous testing.",
      "tldr_zh": "本论文针对工业环境中透明塑料袋的视觉操控挑战，提出了一种自主系统，以支持 Industry 4.0 的数据驱动和机器人集成。该系统采用 Convolutional Neural Networks (CNNs) 进行袋子识别，结合跟踪算法和深度感知技术，实现3D空间感知、优化抓取点以及真空抓取的合规控制，确保在动态环境中的实时安全操作。实验在实验室中使用 FRANKA 机器人臂成功验证了该系统在拆包和切割透明塑料袋方面的效能，为工业自动化应用提供了高效且可持续的解决方案。",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.RO",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2411.09623v2",
      "published_date": "2024-11-14 17:47:54 UTC",
      "updated_date": "2024-11-19 10:15:56 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T00:26:40.808632"
    },
    {
      "arxiv_id": "2411.09613v1",
      "title": "PTR: Precision-Driven Tool Recommendation for Large Language Models",
      "title_zh": "翻译失败",
      "authors": [
        "Hang Gao",
        "Yongfeng Zhang"
      ],
      "abstract": "By augmenting Large Language Models (LLMs) with external tools, their\ncapacity to solve complex problems has been significantly enhanced. However,\ndespite ongoing advancements in the parsing capabilities of LLMs, incorporating\nall available tools simultaneously in the prompt remains impractical due to the\nvast number of external tools. Consequently, it is essential to provide LLMs\nwith a precise set of tools tailored to the specific task, considering both\nquantity and quality. Current tool retrieval methods primarily focus on\nrefining the ranking list of tools and directly packaging a fixed number of\ntop-ranked tools as the tool set. However, these approaches often fail to equip\nLLMs with the optimal set of tools prior to execution, since the optimal number\nof tools for different tasks could be different, resulting in inefficiencies\nsuch as redundant or unsuitable tools, which impede immediate access to the\nmost relevant tools. This paper addresses the challenge of recommending precise\ntoolsets for LLMs. We introduce the problem of tool recommendation, define its\nscope, and propose a novel Precision-driven Tool Recommendation (PTR) approach.\nPTR captures an initial, concise set of tools by leveraging historical tool\nbundle usage and dynamically adjusts the tool set by performing tool matching,\nculminating in a multi-view-based tool addition. Additionally, we present a new\ndataset, RecTools, and a metric, TRACC, designed to evaluate the effectiveness\nof tool recommendation for LLMs. We further validate our design choices through\ncomprehensive experiments, demonstrating promising accuracy across two open\nbenchmarks and our RecTools dataset.",
      "tldr_zh": "这篇论文针对大型语言模型（LLMs）使用外部工具时面临的工具过多问题，提出了一种Precision-driven Tool Recommendation (PTR)方法，以提供针对特定任务的精确工具集。PTR 通过利用历史工具包使用情况捕获初始工具集，并通过工具匹配和多视图工具添加进行动态调整，确保工具数量和质量最优化。论文还引入了新数据集RecTools和评估指标TRACC，并通过全面实验验证了PTR在两个公开基准和RecTools数据集上的高准确性。总的来说，该方法提升了LLMs的工具使用效率，为复杂任务处理提供了更可靠的解决方案。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2411.09613v1",
      "published_date": "2024-11-14 17:33:36 UTC",
      "updated_date": "2024-11-14 17:33:36 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T00:26:52.063803"
    },
    {
      "arxiv_id": "2411.09604v1",
      "title": "Local-Global Attention: An Adaptive Mechanism for Multi-Scale Feature Integration",
      "title_zh": "局部-全局注意力：一种自适应机制用于多尺度特征整合",
      "authors": [
        "Yifan Shao"
      ],
      "abstract": "In recent years, attention mechanisms have significantly enhanced the\nperformance of object detection by focusing on key feature information.\nHowever, prevalent methods still encounter difficulties in effectively\nbalancing local and global features. This imbalance hampers their ability to\ncapture both fine-grained details and broader contextual information-two\ncritical elements for achieving accurate object detection.To address these\nchallenges, we propose a novel attention mechanism, termed Local-Global\nAttention, which is designed to better integrate both local and global\ncontextual features. Specifically, our approach combines multi-scale\nconvolutions with positional encoding, enabling the model to focus on local\ndetails while concurrently considering the broader global context.\nAdditionally, we introduce a learnable parameters, which allow the model to\ndynamically adjust the relative importance of local and global attention,\ndepending on the specific requirements of the task, thereby optimizing feature\nrepresentations across multiple scales.We have thoroughly evaluated the\nLocal-Global Attention mechanism on several widely used object detection and\nclassification datasets. Our experimental results demonstrate that this\napproach significantly enhances the detection of objects at various scales,\nwith particularly strong performance on multi-class and small object detection\ntasks. In comparison to existing attention mechanisms, Local-Global Attention\nconsistently outperforms them across several key metrics, all while maintaining\ncomputational efficiency.",
      "tldr_zh": "本研究针对物体检测中注意力机制难以平衡本地和全局特征的问题，提出了一种新型机制Local-Global Attention，用于有效整合多尺度特征。具体来说，该机制结合multi-scale convolutions和positional encoding，并引入可学习的参数来动态调整本地和全局注意力的相对重要性，从而优化特征表示。实验结果显示，在多个物体检测和分类数据集上，该方法显著提升了多类和小物体检测的性能，与现有attention mechanisms相比，在关键指标上表现出色，同时保持了计算效率。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2411.09604v1",
      "published_date": "2024-11-14 17:22:16 UTC",
      "updated_date": "2024-11-14 17:22:16 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T00:27:03.191812"
    },
    {
      "arxiv_id": "2411.09601v1",
      "title": "Accelerating Knowledge Graph and Ontology Engineering with Large Language Models",
      "title_zh": "翻译失败",
      "authors": [
        "Cogan Shimizu",
        "Pascal Hitzler"
      ],
      "abstract": "Large Language Models bear the promise of significant acceleration of key\nKnowledge Graph and Ontology Engineering tasks, including ontology modeling,\nextension, modification, population, alignment, as well as entity\ndisambiguation. We lay out LLM-based Knowledge Graph and Ontology Engineering\nas a new and coming area of research, and argue that modular approaches to\nontologies will be of central importance.",
      "tldr_zh": "本研究探讨了大型语言模型（LLMs）如何显著加速知识图谱和本体工程的关键任务，包括本体建模、扩展、修改、填充、对齐以及实体消歧。论文将基于LLMs的知识图谱和本体工程定位为一个新兴研究领域，强调其潜力在提升效率方面。作者主张采用模块化方法来处理本体，以此作为该领域发展的核心策略。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2411.09601v1",
      "published_date": "2024-11-14 17:21:02 UTC",
      "updated_date": "2024-11-14 17:21:02 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T00:27:15.593084"
    },
    {
      "arxiv_id": "2411.09595v1",
      "title": "LLaMA-Mesh: Unifying 3D Mesh Generation with Language Models",
      "title_zh": "LLaMA-Mesh：统一 3D 网格生成与语言模型",
      "authors": [
        "Zhengyi Wang",
        "Jonathan Lorraine",
        "Yikai Wang",
        "Hang Su",
        "Jun Zhu",
        "Sanja Fidler",
        "Xiaohui Zeng"
      ],
      "abstract": "This work explores expanding the capabilities of large language models (LLMs)\npretrained on text to generate 3D meshes within a unified model. This offers\nkey advantages of (1) leveraging spatial knowledge already embedded in LLMs,\nderived from textual sources like 3D tutorials, and (2) enabling conversational\n3D generation and mesh understanding. A primary challenge is effectively\ntokenizing 3D mesh data into discrete tokens that LLMs can process seamlessly.\nTo address this, we introduce LLaMA-Mesh, a novel approach that represents the\nvertex coordinates and face definitions of 3D meshes as plain text, allowing\ndirect integration with LLMs without expanding the vocabulary. We construct a\nsupervised fine-tuning (SFT) dataset enabling pretrained LLMs to (1) generate\n3D meshes from text prompts, (2) produce interleaved text and 3D mesh outputs\nas required, and (3) understand and interpret 3D meshes. Our work is the first\nto demonstrate that LLMs can be fine-tuned to acquire complex spatial knowledge\nfor 3D mesh generation in a text-based format, effectively unifying the 3D and\ntext modalities. LLaMA-Mesh achieves mesh generation quality on par with models\ntrained from scratch while maintaining strong text generation performance.",
      "tldr_zh": "这篇论文探索了将预训练的大型语言模型 (LLMs) 扩展到 3D 网格生成，提出 LLaMA-Mesh 方法，以统一文本和 3D 模态。LLaMA-Mesh 通过将 3D 网格的顶点坐标和面定义表示为纯文本，直接集成到 LLMs 中，并构建了一个监督微调 (SFT) 数据集，使模型能够从文本提示生成 3D 网格、产生交错的文本和 3D 输出，以及理解和解释 3D 网格。实验表明，LLaMA-Mesh 的网格生成质量与从零训练的模型相当，同时保持了强大的文本生成性能，这首次证明了 LLMs 可以获取复杂的空间知识来实现对话式 3D 生成。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL",
        "cs.CV",
        "68T05",
        "I.3.5; I.2.10; I.2.6"
      ],
      "primary_category": "cs.LG",
      "comment": "See the project website at\n  https://research.nvidia.com/labs/toronto-ai/LLaMA-Mesh/",
      "pdf_url": "http://arxiv.org/pdf/2411.09595v1",
      "published_date": "2024-11-14 17:08:23 UTC",
      "updated_date": "2024-11-14 17:08:23 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T00:27:30.465049"
    },
    {
      "arxiv_id": "2411.09593v1",
      "title": "SMILE-UHURA Challenge -- Small Vessel Segmentation at Mesoscopic Scale from Ultra-High Resolution 7T Magnetic Resonance Angiograms",
      "title_zh": "翻译失败",
      "authors": [
        "Soumick Chatterjee",
        "Hendrik Mattern",
        "Marc Dörner",
        "Alessandro Sciarra",
        "Florian Dubost",
        "Hannes Schnurre",
        "Rupali Khatun",
        "Chun-Chih Yu",
        "Tsung-Lin Hsieh",
        "Yi-Shan Tsai",
        "Yi-Zeng Fang",
        "Yung-Ching Yang",
        "Juinn-Dar Huang",
        "Marshall Xu",
        "Siyu Liu",
        "Fernanda L. Ribeiro",
        "Saskia Bollmann",
        "Karthikesh Varma Chintalapati",
        "Chethan Mysuru Radhakrishna",
        "Sri Chandana Hudukula Ram Kumara",
        "Raviteja Sutrave",
        "Abdul Qayyum",
        "Moona Mazher",
        "Imran Razzak",
        "Cristobal Rodero",
        "Steven Niederren",
        "Fengming Lin",
        "Yan Xia",
        "Jiacheng Wang",
        "Riyu Qiu",
        "Liansheng Wang",
        "Arya Yazdan Panah",
        "Rosana El Jurdi",
        "Guanghui Fu",
        "Janan Arslan",
        "Ghislain Vaillant",
        "Romain Valabregue",
        "Didier Dormont",
        "Bruno Stankoff",
        "Olivier Colliot",
        "Luisa Vargas",
        "Isai Daniel Chacón",
        "Ioannis Pitsiorlas",
        "Pablo Arbeláez",
        "Maria A. Zuluaga",
        "Stefanie Schreiber",
        "Oliver Speck",
        "Andreas Nürnberger"
      ],
      "abstract": "The human brain receives nutrients and oxygen through an intricate network of\nblood vessels. Pathology affecting small vessels, at the mesoscopic scale,\nrepresents a critical vulnerability within the cerebral blood supply and can\nlead to severe conditions, such as Cerebral Small Vessel Diseases. The advent\nof 7 Tesla MRI systems has enabled the acquisition of higher spatial resolution\nimages, making it possible to visualise such vessels in the brain. However, the\nlack of publicly available annotated datasets has impeded the development of\nrobust, machine learning-driven segmentation algorithms. To address this, the\nSMILE-UHURA challenge was organised. This challenge, held in conjunction with\nthe ISBI 2023, in Cartagena de Indias, Colombia, aimed to provide a platform\nfor researchers working on related topics. The SMILE-UHURA challenge addresses\nthe gap in publicly available annotated datasets by providing an annotated\ndataset of Time-of-Flight angiography acquired with 7T MRI. This dataset was\ncreated through a combination of automated pre-segmentation and extensive\nmanual refinement. In this manuscript, sixteen submitted methods and two\nbaseline methods are compared both quantitatively and qualitatively on two\ndifferent datasets: held-out test MRAs from the same dataset as the training\ndata (with labels kept secret) and a separate 7T ToF MRA dataset where both\ninput volumes and labels are kept secret. The results demonstrate that most of\nthe submitted deep learning methods, trained on the provided training dataset,\nachieved reliable segmentation performance. Dice scores reached up to 0.838\n$\\pm$ 0.066 and 0.716 $\\pm$ 0.125 on the respective datasets, with an average\nperformance of up to 0.804 $\\pm$ 0.15.",
      "tldr_zh": "SMILE-UHURA 挑战赛针对 7T Magnetic Resonance Angiograms 的高分辨率图像，专注于小血管（mesoscopic scale）分割问题，以解决大脑小血管疾病诊断中的数据集缺乏问题。挑战赛提供了基于 Time-of-Flight 血管成像的公共标注数据集，通过自动化预分割和手动精炼方式创建。研究比较了 16 个提交的深度学习方法和 2 个基线方法，在两个测试数据集上进行定量和定性评估。结果显示，这些方法在训练数据集上表现出色，Dice scores 最高达 0.838 ± 0.066 和 0.716 ± 0.125，证明了其可靠的分割性能。",
      "categories": [
        "eess.IV",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "eess.IV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2411.09593v1",
      "published_date": "2024-11-14 17:06:00 UTC",
      "updated_date": "2024-11-14 17:06:00 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T00:27:41.194963"
    },
    {
      "arxiv_id": "2411.09590v1",
      "title": "Adopting RAG for LLM-Aided Future Vehicle Design",
      "title_zh": "翻译失败",
      "authors": [
        "Vahid Zolfaghari",
        "Nenad Petrovic",
        "Fengjunjie Pan",
        "Krzysztof Lebioda",
        "Alois Knoll"
      ],
      "abstract": "In this paper, we explore the integration of Large Language Models (LLMs)\nwith Retrieval-Augmented Generation (RAG) to enhance automated design and\nsoftware development in the automotive industry. We present two case studies: a\nstandardization compliance chatbot and a design copilot, both utilizing RAG to\nprovide accurate, context-aware responses. We evaluate four LLMs-GPT-4o,\nLLAMA3, Mistral, and Mixtral -- comparing their answering accuracy and\nexecution time. Our results demonstrate that while GPT-4 offers superior\nperformance, LLAMA3 and Mistral also show promising capabilities for local\ndeployment, addressing data privacy concerns in automotive applications. This\nstudy highlights the potential of RAG-augmented LLMs in improving design\nworkflows and compliance in automotive engineering.",
      "tldr_zh": "本文探讨了将 Large Language Models (LLMs) 与 Retrieval-Augmented Generation (RAG) 整合到汽车行业的自动化设计和软件开发中，以提升准确性和上下文响应。研究通过两个案例研究——标准化合规聊天机器人和设计助手——展示了 RAG 的应用。评估了 GPT-4o、LLAMA3、Mistral 和 Mixtral 等模型，结果显示 GPT-4o 在回答准确性和执行时间上表现出色，而 LLAMA3 和 Mistral 适合本地部署以解决数据隐私问题。该研究强调了 RAG 增强 LLMs 在优化汽车工程设计流程和合规性方面的潜力。",
      "categories": [
        "cs.SE",
        "cs.AI"
      ],
      "primary_category": "cs.SE",
      "comment": "Conference paper accepted in IEEE FLLM 2024",
      "pdf_url": "http://arxiv.org/pdf/2411.09590v1",
      "published_date": "2024-11-14 17:01:24 UTC",
      "updated_date": "2024-11-14 17:01:24 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T00:27:53.090183"
    },
    {
      "arxiv_id": "2411.09580v1",
      "title": "Software Performance Engineering for Foundation Model-Powered Software (FMware)",
      "title_zh": "翻译失败",
      "authors": [
        "Haoxiang Zhang",
        "Shi Chang",
        "Arthur Leung",
        "Kishanthan Thangarajah",
        "Boyuan Chen",
        "Hanan Lutfiyya",
        "Ahmed E. Hassan"
      ],
      "abstract": "The rise of Foundation Models (FMs) like Large Language Models (LLMs) is\nrevolutionizing software development. Despite the impressive prototypes,\ntransforming FMware into production-ready products demands complex engineering\nacross various domains. A critical but overlooked aspect is performance\nengineering, which aims at ensuring FMware meets performance goals such as\nthroughput and latency to avoid user dissatisfaction and financial loss. Often,\nperformance considerations are an afterthought, leading to costly optimization\nefforts post-deployment. FMware's high computational resource demands highlight\nthe need for efficient hardware use. Continuous performance engineering is\nessential to prevent degradation. This paper highlights the significance of\nSoftware Performance Engineering (SPE) in FMware, identifying four key\nchallenges: cognitive architecture design, communication protocols, tuning and\noptimization, and deployment. These challenges are based on literature surveys\nand experiences from developing an in-house FMware system. We discuss problems,\ncurrent practices, and innovative paths for the software engineering community.",
      "tldr_zh": "这篇论文探讨了基础模型 (Foundation Models, FMs) 如大型语言模型 (LLMs) 在软件开发中的应用，以及将 FMware 转化为生产就绪产品的性能工程挑战。作者强调软件性能工程 (Software Performance Engineering, SPE) 的重要性，以确保 FMware 满足吞吐量和延迟等性能目标，避免部署后优化带来的高成本和资源浪费。论文基于文献调查和内部系统开发经验，识别了四个关键挑战：认知架构设计、通信协议、调优和优化，以及部署。最终，论文讨论了当前实践问题并提出创新路径，呼吁软件工程社区加强持续性能工程以提升 FMware 的可靠性和效率。",
      "categories": [
        "cs.SE",
        "cs.AI"
      ],
      "primary_category": "cs.SE",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2411.09580v1",
      "published_date": "2024-11-14 16:42:19 UTC",
      "updated_date": "2024-11-14 16:42:19 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T00:28:03.974389"
    },
    {
      "arxiv_id": "2411.09576v1",
      "title": "Automating Reformulation of Essence Specifications via Graph Rewriting",
      "title_zh": "通过图重写自动化 Essence 规范的重构",
      "authors": [
        "Ian Miguel",
        "András Z. Salamon",
        "Christopher Stone"
      ],
      "abstract": "Formulating an effective constraint model of a parameterised problem class is\ncrucial to the efficiency with which instances of the class can subsequently be\nsolved. It is difficult to know beforehand which of a set of candidate models\nwill perform best in practice. This paper presents a system that employs graph\nrewriting to reformulate an input model for improved performance automatically.\nBy situating our work in the Essence abstract constraint specification\nlanguage, we can use the structure in its high level variable types to trigger\nrewrites directly. We implement our system via rewrite rules expressed in the\nGraph Programs 2 language, applied to the abstract syntax tree of an input\nspecification. We show how to automatically translate the solution of the\nreformulated problem into a solution of the original problem for verification\nand presentation. We demonstrate the efficacy of our system with a detailed\ncase study.",
      "tldr_zh": "这篇论文提出了一种通过 Graph Rewriting 自动重构 Essence 规范的系统，旨在优化约束模型的性能，以更好地解决参数化问题类别的实例。系统利用 Essence 的高级变量类型结构触发重写规则，并通过 Graph Programs 2 语言应用于输入规范的抽象语法树，从而实现模型重构。论文展示了如何自动将重构问题的解决方案转换回原问题以进行验证，并通过详细案例研究证明了该系统的有效性。",
      "categories": [
        "cs.AI",
        "F.4.2"
      ],
      "primary_category": "cs.AI",
      "comment": "Presented at the PTHG 2024 workshop",
      "pdf_url": "http://arxiv.org/pdf/2411.09576v1",
      "published_date": "2024-11-14 16:35:15 UTC",
      "updated_date": "2024-11-14 16:35:15 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T00:28:15.871113"
    },
    {
      "arxiv_id": "2411.09547v2",
      "title": "Piecing It All Together: Verifying Multi-Hop Multimodal Claims",
      "title_zh": "翻译失败",
      "authors": [
        "Haoran Wang",
        "Aman Rangapur",
        "Xiongxiao Xu",
        "Yueqing Liang",
        "Haroon Gharwi",
        "Carl Yang",
        "Kai Shu"
      ],
      "abstract": "Existing claim verification datasets often do not require systems to perform\ncomplex reasoning or effectively interpret multimodal evidence. To address\nthis, we introduce a new task: multi-hop multimodal claim verification. This\ntask challenges models to reason over multiple pieces of evidence from diverse\nsources, including text, images, and tables, and determine whether the combined\nmultimodal evidence supports or refutes a given claim. To study this task, we\nconstruct MMCV, a large-scale dataset comprising 15k multi-hop claims paired\nwith multimodal evidence, generated and refined using large language models,\nwith additional input from human feedback. We show that MMCV is challenging\neven for the latest state-of-the-art multimodal large language models,\nespecially as the number of reasoning hops increases. Additionally, we\nestablish a human performance benchmark on a subset of MMCV. We hope this\ndataset and its evaluation task will encourage future research in multimodal\nmulti-hop claim verification.",
      "tldr_zh": "本研究引入了多跳多模态声明验证（multi-hop multimodal claim verification）任务，要求模型在文本、图像和表格等多样来源的多件证据上进行推理，以判断声明是否被支持或驳斥。研究者构建了大规模数据集 MMCV，包含 15k 多跳声明及其多模态证据，通过大型语言模型生成和精炼，并整合人类反馈。实验结果显示，即使是最新多模态大型语言模型（multimodal large language models）在处理 MMCV 时面临显著挑战，尤其是当推理跳数增加时，且建立的人类性能基准突显了任务的难度，此数据集有望推动未来相关研究。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "COLING 2025",
      "pdf_url": "http://arxiv.org/pdf/2411.09547v2",
      "published_date": "2024-11-14 16:01:33 UTC",
      "updated_date": "2024-12-12 19:23:28 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T00:28:29.027255"
    },
    {
      "arxiv_id": "2411.09543v2",
      "title": "OpenGeMM: A High-Utilization GeMM Accelerator Generator with Lightweight RISC-V Control and Tight Memory Coupling",
      "title_zh": "OpenGeMM：一种高利用率 GeMM 加速器生成器，采用轻量级 RISC-V 控制和紧密内存耦合",
      "authors": [
        "Xiaoling Yi",
        "Ryan Antonio",
        "Joren Dumoulin",
        "Jiacong Sun",
        "Josse Van Delm",
        "Guilherme Paim",
        "Marian Verhelst"
      ],
      "abstract": "Deep neural networks (DNNs) face significant challenges when deployed on\nresource-constrained extreme edge devices due to their computational and\ndata-intensive nature. While standalone accelerators tailored for specific\napplication scenarios suffer from inflexible control and limited\nprogrammability, generic hardware acceleration platforms coupled with RISC-V\nCPUs can enable high reusability and flexibility, yet typically at the expense\nof system level efficiency and low utilization. To fill this gap, we propose\nOpenGeMM, an open-source acceleration platform, jointly demonstrating high\nefficiency and utilization, as well as ease of configurability and\nprogrammability. OpenGeMM encompasses a parameterized Chisel-coded GeMM\naccelerator, a lightweight RISC-V processor, and a tightly coupled multi-banked\nscratchpad memory. The GeMM core utilization and system efficiency are boosted\nthrough three mechanisms: configuration pre-loading, input pre-fetching with\noutput buffering, and programmable strided memory access. Experimental results\nshow that OpenGeMM can consistently achieve hardware utilization ranging from\n81.89% to 99.34% across diverse CNN and Transformer workloads. Compared to the\nSotA open-source Gemmini accelerator, OpenGeMM demonstrates a 3.58x to 16.40x\nspeedup on normalized throughput across a wide variety ofGeMM workloads, while\nachieving 4.68 TOPS/W system efficiency.",
      "tldr_zh": "该论文提出OpenGeMM，一种开源的GeMM加速器生成器，结合轻量级RISC-V处理器和紧密耦合的多银行scratchpad内存，旨在解决DNNs在资源受限边缘设备上的计算和数据密集挑战，同时实现高利用率和灵活性。OpenGeMM通过配置预加载、输入预取输出缓冲以及可编程的strided内存访问机制，提升了GeMM核心利用率和系统效率。实验结果显示，该平台在各种CNN和Transformer工作负载上实现81.89%到99.34%的硬件利用率，并相较于SotA的Gemmini加速器，提供3.58x到16.40x的吞吐量加速和4.68 TOPS/W的系统效率。",
      "categories": [
        "cs.AR",
        "cs.AI"
      ],
      "primary_category": "cs.AR",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2411.09543v2",
      "published_date": "2024-11-14 15:58:46 UTC",
      "updated_date": "2024-11-21 14:10:08 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T00:28:41.554023"
    },
    {
      "arxiv_id": "2411.09540v2",
      "title": "Prompting the Unseen: Detecting Hidden Backdoors in Black-Box Models",
      "title_zh": "翻译失败",
      "authors": [
        "Zi-Xuan Huang",
        "Jia-Wei Chen",
        "Zhi-Peng Zhang",
        "Chia-Mu Yu"
      ],
      "abstract": "Visual prompting (VP) is a new technique that adapts well-trained frozen\nmodels for source domain tasks to target domain tasks. This study examines VP's\nbenefits for black-box model-level backdoor detection. The visual prompt in VP\nmaps class subspaces between source and target domains. We identify a\nmisalignment, termed class subspace inconsistency, between clean and poisoned\ndatasets. Based on this, we introduce \\textsc{BProm}, a black-box model-level\ndetection method to identify backdoors in suspicious models, if any.\n\\textsc{BProm} leverages the low classification accuracy of prompted models\nwhen backdoors are present. Extensive experiments confirm \\textsc{BProm}'s\neffectiveness.",
      "tldr_zh": "这项研究探讨了 Visual Prompting (VP) 在黑盒模型后门检测中的潜力，通过映射源域和目标域的类子空间，发现了类子空间不一致（class subspace inconsistency）现象，该问题存在于干净数据集和中毒数据集之间。基于此，研究者提出了 BProm，一种黑盒模型级别的检测方法，利用后门存在时提示模型分类准确率降低的特性来识别潜在后门。广泛实验验证了 BProm 的有效性，为提升模型安全性提供了新途径。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "This paper has been accepted by IEEE/IFIP DSN 2025",
      "pdf_url": "http://arxiv.org/pdf/2411.09540v2",
      "published_date": "2024-11-14 15:56:11 UTC",
      "updated_date": "2025-04-07 08:55:40 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T00:28:53.145250"
    },
    {
      "arxiv_id": "2411.09523v1",
      "title": "Navigating the Risks: A Survey of Security, Privacy, and Ethics Threats in LLM-Based Agents",
      "title_zh": "翻译失败",
      "authors": [
        "Yuyou Gan",
        "Yong Yang",
        "Zhe Ma",
        "Ping He",
        "Rui Zeng",
        "Yiming Wang",
        "Qingming Li",
        "Chunyi Zhou",
        "Songze Li",
        "Ting Wang",
        "Yunjun Gao",
        "Yingcai Wu",
        "Shouling Ji"
      ],
      "abstract": "With the continuous development of large language models (LLMs),\ntransformer-based models have made groundbreaking advances in numerous natural\nlanguage processing (NLP) tasks, leading to the emergence of a series of agents\nthat use LLMs as their control hub. While LLMs have achieved success in various\ntasks, they face numerous security and privacy threats, which become even more\nsevere in the agent scenarios. To enhance the reliability of LLM-based\napplications, a range of research has emerged to assess and mitigate these\nrisks from different perspectives.\n  To help researchers gain a comprehensive understanding of various risks, this\nsurvey collects and analyzes the different threats faced by these agents. To\naddress the challenges posed by previous taxonomies in handling cross-module\nand cross-stage threats, we propose a novel taxonomy framework based on the\nsources and impacts. Additionally, we identify six key features of LLM-based\nagents, based on which we summarize the current research progress and analyze\ntheir limitations. Subsequently, we select four representative agents as case\nstudies to analyze the risks they may face in practical use. Finally, based on\nthe aforementioned analyses, we propose future research directions from the\nperspectives of data, methodology, and policy, respectively.",
      "tldr_zh": "这篇调查论文探讨了基于大型语言模型(LLMs)的代理在security、privacy和ethics威胁方面的风险，强调这些威胁在代理场景中更为严重。论文提出一个基于来源和影响的taxonomy框架，以更好地处理跨模块和跨阶段威胁，并识别了LLM-based代理的六大关键特征，同时总结了当前研究进展及其局限性。通过分析四个代表性代理的案例研究，论文揭示了实际应用中的潜在风险。最后，从数据、方法论和政策角度，论文建议了未来的研究方向，以提升LLM-based应用的可靠性。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2411.09523v1",
      "published_date": "2024-11-14 15:40:04 UTC",
      "updated_date": "2024-11-14 15:40:04 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T00:29:06.663821"
    },
    {
      "arxiv_id": "2411.09510v2",
      "title": "Communication Compression for Tensor Parallel LLM Inference",
      "title_zh": "翻译失败",
      "authors": [
        "Jan Hansen-Palmus",
        "Michael Truong Le",
        "Oliver Hausdörfer",
        "Alok Verma"
      ],
      "abstract": "Large Language Models (LLMs) have pushed the frontier of artificial\nintelligence but are comprised of hundreds of billions of parameters and\noperations. For faster inference latency, LLMs are deployed on multiple\nhardware accelerators through various Model Parallelism strategies. Our paper\nlooks into the details on one such strategy - Tensor Parallel - and proposes to\nreduce latency by compressing inter-accelerator communication. We leverage fine\ngrained quantization techniques to compress selected activations by 3.5 - 4.5x.\nOur proposed method leads up to 2x reduction of time-to-first-token (TTFT) with\nnegligible model performance degradation.",
      "tldr_zh": "该论文针对大型语言模型 (LLMs) 在多硬件加速器部署中的推理延迟问题，提出了一种基于 Tensor Parallel 策略的通信压缩方法。研究者利用细粒度量化技术，对选定的激活进行 3.5-4.5 倍压缩，从而显著减少加速器间通信开销。实验结果显示，该方法可将时间到第一个标记 (TTFT) 降低高达 2 倍，同时模型性能几乎没有下降。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2411.09510v2",
      "published_date": "2024-11-14 15:19:01 UTC",
      "updated_date": "2024-11-15 10:47:37 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T00:29:16.080730"
    },
    {
      "arxiv_id": "2411.09507v1",
      "title": "Toward a Cohesive AI and Simulation Software Ecosystem for Scientific Innovation",
      "title_zh": "翻译失败",
      "authors": [
        "Michael A. Heroux",
        "Sameer Shende",
        "Lois Curfman McInnes",
        "Todd Gamblin",
        "James M. Willenbring"
      ],
      "abstract": "In this paper, we discuss the need for an integrated software stack that\nunites artificial intelligence (AI) and modeling and simulation (ModSim) tools\nto advance scientific discovery. The authors advocate for a unified AI/ModSim\nsoftware ecosystem that ensures compatibility across a wide range of software\non diverse high-performance computing systems, promoting ease of deployment,\nversion management, and binary distribution. Key challenges highlighted include\nbalancing the distinct needs of AI and ModSim, especially in terms of software\nbuild practices, dependency management, and compatibility. The document\nunderscores the importance of continuous integration, community-driven\nstewardship, and collaboration with the Department of Energy (DOE) to develop a\nportable and cohesive scientific software ecosystem. Recommendations focus on\nsupporting standardized environments through initiatives like the Extreme-scale\nScientific Software Stack (E4S) and Spack to foster interdisciplinary\ninnovation and facilitate new scientific advancements.",
      "tldr_zh": "本论文讨论了整合人工智能（AI）和建模模拟（ModSim）工具的需求，以推动科学发现，主张建立一个统一的 AI/ModSim 软件生态系统，确保在各种高性能计算系统上实现兼容性、易部署、版本管理和二进制分发。关键挑战包括平衡 AI 和 ModSim 的不同需求，如软件构建实践、依赖管理和兼容性问题。论文强调通过持续集成、社区驱动管理和与 Department of Energy (DOE) 合作，来开发可移植的科学软件生态系统。最终，推荐利用 Extreme-scale Scientific Software Stack (E4S) 和 Spack 等标准化环境，促进跨学科创新和新的科学进步。",
      "categories": [
        "cs.SE",
        "cs.AI"
      ],
      "primary_category": "cs.SE",
      "comment": "5 pages",
      "pdf_url": "http://arxiv.org/pdf/2411.09507v1",
      "published_date": "2024-11-14 15:17:50 UTC",
      "updated_date": "2024-11-14 15:17:50 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T00:29:29.666822"
    },
    {
      "arxiv_id": "2411.09492v1",
      "title": "MM-Eval: A Hierarchical Benchmark for Modern Mongolian Evaluation in LLMs",
      "title_zh": "翻译失败",
      "authors": [
        "Mengyuan Zhang",
        "Ruihui Wang",
        "Bo Xia",
        "Yuan Sun",
        "Xiaobing Zhao"
      ],
      "abstract": "Large language models (LLMs) excel in high-resource languages but face\nnotable challenges in low-resource languages like Mongolian. This paper\naddresses these challenges by categorizing capabilities into language abilities\n(syntax and semantics) and cognitive abilities (knowledge and reasoning). To\nsystematically evaluate these areas, we developed MM-Eval, a specialized\ndataset based on Modern Mongolian Language Textbook I and enriched with WebQSP\nand MGSM datasets.\n  Preliminary experiments on models including Qwen2-7B-Instruct, GLM4-9b-chat,\nLlama3.1-8B-Instruct, GPT-4, and DeepseekV2.5 revealed that: 1) all models\nperformed better on syntactic tasks than semantic tasks, highlighting a gap in\ndeeper language understanding; and 2) knowledge tasks showed a moderate\ndecline, suggesting that models can transfer general knowledge from\nhigh-resource to low-resource contexts.\n  The release of MM-Eval, comprising 569 syntax, 677 semantics, 344 knowledge,\nand 250 reasoning tasks, offers valuable insights for advancing NLP and LLMs in\nlow-resource languages like Mongolian. The dataset is available at\nhttps://github.com/joenahm/MM-Eval.",
      "tldr_zh": "本论文探讨大型语言模型（LLMs）在低资源语言如蒙古语中的挑战，通过将能力分为语言能力（语法和语义）和认知能力（知识和推理）来构建评估框架。研究团队开发了MM-Eval数据集，该数据集基于Modern Mongolian Language Textbook I，并结合WebQSP和MGSM数据集，包含569个语法任务、677个语义任务、344个知识任务和250个推理任务，用于系统评估LLMs性能。初步实验显示，测试模型（如Qwen2-7B-Instruct和GPT-4）在语法任务上优于语义任务，且知识任务有适度下降，表明模型能从高资源语言转移知识，但深层理解存在差距；MM-Eval数据集的发布将推动低资源语言NLP的发展，可在https://github.com/joenahm/MM-Eval获取。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2411.09492v1",
      "published_date": "2024-11-14 14:58:38 UTC",
      "updated_date": "2024-11-14 14:58:38 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T00:29:41.917044"
    },
    {
      "arxiv_id": "2411.10488v1",
      "title": "The Future of Skill: What Is It to Be Skilled at Work?",
      "title_zh": "翻译失败",
      "authors": [
        "Axel Niklasson",
        "Sean Rintel",
        "Stephann Makri",
        "Alex Taylor"
      ],
      "abstract": "In this short paper, we introduce work that is aiming to purposefully venture\ninto this mesh of questions from a different starting point. Interjecting into\nthe conversation, we want to ask: 'What is it to be skilled at work?' Building\non work from scholars like Tim Ingold, and strands of longstanding research in\nworkplace studies and CSCW, our interest is in turning the attention to the\nactive work of 'being good', or 'being skilled', at what we as workers do. As\nwe see it, skill provides a counterpoint to the version of intelligence that\nappears to be easily blackboxed in systems like Slack, and that ultimately\nreduces much of what people do to work well together. To put it slightly\ndifferently, skill - as we will argue below - gives us a way into thinking\nabout work as a much more entangled endeavour, unfolding through multiple and\ninterweaving sets of practices, places, tools and collaborations. In this vein,\ndesigning for the future of work seems to be about much more than where work is\ndone or how we might bolt on discrete containers of intelligence. More fruitful\nwould be attending to how we succeed in threading so many entities together to\ndo our jobs well - in 'coming to be skilled'.",
      "tldr_zh": "这篇论文探讨了“在工作中什么是技能”（What Is It to Be Skilled at Work?），以Tim Ingold等学者的研究为基础，并结合工作场所研究和CSCW（计算机支持的合作工作）的传统。论文强调，技能不是简单的智能黑箱化（如Slack系统），而是将工作视为多重交织的实践、场所、工具和协作的复杂过程。最终，它建议设计未来的工作应关注如何整合这些元素，以实现“变得熟练”的过程。",
      "categories": [
        "cs.HC",
        "cs.AI",
        "cs.CY"
      ],
      "primary_category": "cs.HC",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2411.10488v1",
      "published_date": "2024-11-14 14:39:03 UTC",
      "updated_date": "2024-11-14 14:39:03 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T00:29:52.811762"
    },
    {
      "arxiv_id": "2411.09475v1",
      "title": "ResidualDroppath: Enhancing Feature Reuse over Residual Connections",
      "title_zh": "翻译失败",
      "authors": [
        "Sejik Park"
      ],
      "abstract": "Residual connections are one of the most important components in neural\nnetwork architectures for mitigating the vanishing gradient problem and\nfacilitating the training of much deeper networks. One possible explanation for\nhow residual connections aid deeper network training is by promoting feature\nreuse. However, we identify and analyze the limitations of feature reuse with\nvanilla residual connections. To address these limitations, we propose\nmodifications in training methods. Specifically, we provide an additional\nopportunity for the model to learn feature reuse with residual connections\nthrough two types of iterations during training. The first type of iteration\ninvolves using droppath, which enforces feature reuse by randomly dropping a\nsubset of layers. The second type of iteration focuses on training the dropped\nparts of the model while freezing the undropped parts. As a result, the dropped\nparts learn in a way that encourages feature reuse, as the model relies on the\nundropped parts with feature reuse in mind. Overall, we demonstrated\nperformance improvements in models with residual connections for image\nclassification in certain cases.",
      "tldr_zh": "该论文分析了Residual connections在神经网络中促进特征重用的局限性，这些连接虽能缓解梯度消失问题并支持更深网络训练，但存在特征重用效率不足的问题。为解决此问题，研究提出ResidualDroppath方法，包括两种训练迭代：一是使用droppath随机丢弃部分层以强制特征重用，二是训练丢弃部分的同时冻结未丢弃部分，从而鼓励模型依赖特征重用。实验结果显示，该方法在某些图像分类任务中提升了模型性能。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2411.09475v1",
      "published_date": "2024-11-14 14:31:30 UTC",
      "updated_date": "2024-11-14 14:31:30 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T00:30:04.841766"
    },
    {
      "arxiv_id": "2411.09471v1",
      "title": "Renal Cell Carcinoma subtyping: learning from multi-resolution localization",
      "title_zh": "翻译失败",
      "authors": [
        "Mohamad Mohamad",
        "Francesco Ponzio",
        "Santa Di Cataldo",
        "Damien Ambrosetti",
        "Xavier Descombes"
      ],
      "abstract": "Renal Cell Carcinoma is typically asymptomatic at the early stages for many\npatients. This leads to a late diagnosis of the tumor, where the curability\nlikelihood is lower, and makes the mortality rate of Renal Cell Carcinoma high,\nwith respect to its incidence rate. To increase the survival chance, a fast and\ncorrect categorization of the tumor subtype is paramount. Nowadays,\ncomputerized methods, based on artificial intelligence, represent an\ninteresting opportunity to improve the productivity and the objectivity of the\nmicroscopy-based Renal Cell Carcinoma diagnosis. Nonetheless, much of their\nexploitation is hampered by the paucity of annotated dataset, essential for a\nproficient training of supervised machine learning technologies. This study\nsets out to investigate a novel self supervised training strategy for machine\nlearning diagnostic tools, based on the multi-resolution nature of the\nhistological samples. We aim at reducing the need of annotated dataset, without\nsignificantly reducing the accuracy of the tool. We demonstrate the\nclassification capability of our tool on a whole slide imaging dataset for\nRenal Cancer subtyping, and we compare our solution with several\nstate-of-the-art classification counterparts.",
      "tldr_zh": "本研究针对 Renal Cell Carcinoma 的早期诊断挑战，强调其无症状导致高死亡率，并提出一种基于多分辨率定位的自监督训练策略，以减少对标注数据集的依赖，同时保持诊断准确性。  \n该策略利用组织学样本的多分辨率特性，训练机器学习工具进行肿瘤亚型分类，从而提升诊断的客观性和效率。  \n实验结果显示，该方法在全滑微成像数据集上表现出色，与多种 state-of-the-art 分类方法相比，实现了可比的分类性能。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2411.09471v1",
      "published_date": "2024-11-14 14:21:49 UTC",
      "updated_date": "2024-11-14 14:21:49 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T00:30:17.396056"
    },
    {
      "arxiv_id": "2411.09469v1",
      "title": "An Explainable Attention Model for Cervical Precancer Risk Classification using Colposcopic Images",
      "title_zh": "基于阴道镜图像的可解释注意力模型用于宫颈癌前病变风险分类",
      "authors": [
        "Smith K. Khare",
        "Berit Bargum Booth",
        "Victoria Blanes-Vidal",
        "Lone Kjeld Petersen",
        "Esmaeil S. Nadimi"
      ],
      "abstract": "Cervical cancer remains a major worldwide health issue, with early\nidentification and risk assessment playing critical roles in effective\npreventive interventions. This paper presents the Cervix-AID-Net model for\ncervical precancer risk classification. The study designs and evaluates the\nproposed Cervix-AID-Net model based on patients colposcopy images. The model\ncomprises a Convolutional Block Attention Module (CBAM) and convolutional\nlayers that extract interpretable and representative features of colposcopic\nimages to distinguish high-risk and low-risk cervical precancer. In addition,\nthe proposed Cervix-AID-Net model integrates four explainable techniques,\nnamely gradient class activation maps, Local Interpretable Model-agnostic\nExplanations, CartoonX, and pixel rate distortion explanation based on output\nfeature maps and input features. The evaluation using holdout and ten-fold\ncross-validation techniques yielded a classification accuracy of 99.33\\% and\n99.81\\%. The analysis revealed that CartoonX provides meticulous explanations\nfor the decision of the Cervix-AID-Net model due to its ability to provide the\nrelevant piece-wise smooth part of the image. The effect of Gaussian noise and\nblur on the input shows that the performance remains unchanged up to Gaussian\nnoise of 3\\% and blur of 10\\%, while the performance reduces thereafter. A\ncomparison study of the proposed model's performance compared to other deep\nlearning approaches highlights the Cervix-AID-Net model's potential as a\nsupplemental tool for increasing the effectiveness of cervical precancer risk\nassessment. The proposed method, which incorporates the CBAM and explainable\nartificial integration, has the potential to influence cervical cancer\nprevention and early detection, improving patient outcomes and lowering the\nworldwide burden of this preventable disease.",
      "tldr_zh": "这篇论文提出了 Cervix-AID-Net 模型，用于基于阴道镜图像的宫颈癌前病变风险分类，旨在通过 Convolutional Block Attention Module (CBAM) 和卷积层提取可解释的特征，以区分高风险和低风险病例。模型整合了四种可解释技术，包括 gradient class activation maps、Local Interpretable Model-agnostic Explanations (LIME)、CartoonX 和 pixel rate distortion explanation，以提升决策透明度。实验结果显示，在 holdout 和十折交叉验证中，模型分别达到 99.33% 和 99.81% 的分类准确率，且 CartoonX 能提供详细的图像平滑部分解释。进一步分析表明，模型对高斯噪声（3%）和模糊（10%）具有鲁棒性，但超过阈值后性能下降，与其他深度学习方法相比表现出色，有望作为辅助工具提升宫颈癌预防和早期检测的有效性。",
      "categories": [
        "eess.IV",
        "cs.AI"
      ],
      "primary_category": "eess.IV",
      "comment": "19 pages, 9 figure, and 7 tables",
      "pdf_url": "http://arxiv.org/pdf/2411.09469v1",
      "published_date": "2024-11-14 14:18:40 UTC",
      "updated_date": "2024-11-14 14:18:40 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T00:30:30.520936"
    },
    {
      "arxiv_id": "2411.09451v1",
      "title": "DiffRoad: Realistic and Diverse Road Scenario Generation for Autonomous Vehicle Testing",
      "title_zh": "翻译失败",
      "authors": [
        "Junjie Zhou",
        "Lin Wang",
        "Qiang Meng",
        "Xiaofan Wang"
      ],
      "abstract": "Generating realistic and diverse road scenarios is essential for autonomous\nvehicle testing and validation. Nevertheless, owing to the complexity and\nvariability of real-world road environments, creating authentic and varied\nscenarios for intelligent driving testing is challenging. In this paper, we\npropose DiffRoad, a novel diffusion model designed to produce controllable and\nhigh-fidelity 3D road scenarios. DiffRoad leverages the generative capabilities\nof diffusion models to synthesize road layouts from white noise through an\ninverse denoising process, preserving real-world spatial features. To enhance\nthe quality of generated scenarios, we design the Road-UNet architecture,\noptimizing the balance between backbone and skip connections for high-realism\nscenario generation. Furthermore, we introduce a road scenario evaluation\nmodule that screens adequate and reasonable scenarios for intelligent driving\ntesting using two critical metrics: road continuity and road reasonableness.\nExperimental results on multiple real-world datasets demonstrate DiffRoad's\nability to generate realistic and smooth road structures while maintaining the\noriginal distribution. Additionally, the generated scenarios can be fully\nautomated into the OpenDRIVE format, facilitating generalized autonomous\nvehicle simulation testing. DiffRoad provides a rich and diverse scenario\nlibrary for large-scale autonomous vehicle testing and offers valuable insights\nfor future infrastructure designs that are better suited for autonomous\nvehicles.",
      "tldr_zh": "本文提出DiffRoad，一种基于diffusion model的创新框架，用于生成真实且多样的3D道路场景，以解决自动驾驶车辆测试中的复杂环境挑战。DiffRoad通过逆去噪过程合成道路布局，并采用Road-UNet架构优化骨干网和跳跃连接，确保生成的场景高保真和空间特征真实性；同时，引入场景评估模块，使用道路连续性和道路合理性指标筛选高质量场景。实验在多个真实数据集上验证，DiffRoad能产生平滑的道路结构并保持原始分布，生成的场景可自动转换为OpenDRIVE格式，支持大规模自动驾驶模拟测试，并为未来基础设施设计提供宝贵见解。",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.RO",
      "comment": "14 pages, 9 figures",
      "pdf_url": "http://arxiv.org/pdf/2411.09451v1",
      "published_date": "2024-11-14 13:56:02 UTC",
      "updated_date": "2024-11-14 13:56:02 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T00:30:41.727795"
    },
    {
      "arxiv_id": "2411.09429v4",
      "title": "AI-driven inverse design of materials: Past, present and future",
      "title_zh": "AI驱动的材料逆向设计：过去、现在和未来",
      "authors": [
        "Xiao-Qi Han",
        "Xin-De Wang",
        "Meng-Yuan Xu",
        "Zhen Feng",
        "Bo-Wen Yao",
        "Peng-Jie Guo",
        "Ze-Feng Gao",
        "Zhong-Yi Lu"
      ],
      "abstract": "The discovery of advanced materials is the cornerstone of human technological\ndevelopment and progress. The structures of materials and their corresponding\nproperties are essentially the result of a complex interplay of multiple\ndegrees of freedom such as lattice, charge, spin, symmetry, and topology. This\nposes significant challenges for the inverse design methods of materials.\nHumans have long explored new materials through a large number of experiments\nand proposed corresponding theoretical systems to predict new material\nproperties and structures. With the improvement of computational power,\nresearchers have gradually developed various electronic structure calculation\nmethods, such as the density functional theory and high-throughput\ncomputational methods. Recently, the rapid development of artificial\nintelligence technology in the field of computer science has enabled the\neffective characterization of the implicit association between material\nproperties and structures, thus opening up an efficient paradigm for the\ninverse design of functional materials. A significant progress has been made in\ninverse design of materials based on generative and discriminative models,\nattracting widespread attention from researchers. Considering this rapid\ntechnological progress, in this survey, we look back on the latest advancements\nin AI-driven inverse design of materials by introducing the background, key\nfindings, and mainstream technological development routes. In addition, we\nsummarize the remaining issues for future directions. This survey provides the\nlatest overview of AI-driven inverse design of materials, which can serve as a\nuseful resource for researchers.",
      "tldr_zh": "本调查回顾了 AI-driven inverse design of materials 的发展历程，包括材料结构与属性（如 lattice, charge, spin, symmetry, and topology）的复杂交互所带来的挑战，以及从实验、理论到 density functional theory 和 high-throughput computational methods 的历史演进。论文强调了 AI 技术的最新进展，如利用 generative and discriminative models 来表征材料属性与结构的隐含关联，从而开辟高效的反向设计范式。作者总结了关键发现和主流技术路线，并指出了剩余问题及未来研究方向，为相关领域的研究者提供宝贵资源。",
      "categories": [
        "cond-mat.mtrl-sci",
        "cond-mat.supr-con",
        "cs.AI"
      ],
      "primary_category": "cond-mat.mtrl-sci",
      "comment": "44 pages, 6 figures, 2 tables",
      "pdf_url": "http://arxiv.org/pdf/2411.09429v4",
      "published_date": "2024-11-14 13:25:04 UTC",
      "updated_date": "2025-02-20 03:47:54 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T00:30:53.819946"
    },
    {
      "arxiv_id": "2411.09422v2",
      "title": "OpenLS-DGF: An Adaptive Open-Source Dataset Generation Framework for Machine Learning Tasks in Logic Synthesis",
      "title_zh": "OpenLS-DGF：一种自适应开源数据集生成框架，用于逻辑综合中的机器学习任务",
      "authors": [
        "Liwei Ni",
        "Rui Wang",
        "Miao Liu",
        "Xingyu Meng",
        "Xiaoze Lin",
        "Junfeng Liu",
        "Guojie Luo",
        "Zhufei Chu",
        "Weikang Qian",
        "Xiaoyan Yang",
        "Biwei Xie",
        "Xingquan Li",
        "Huawei Li"
      ],
      "abstract": "This paper introduces OpenLS-DGF, an adaptive logic synthesis dataset\ngeneration framework, to enhance machine learning~(ML) applications within the\nlogic synthesis process. Previous dataset generation flows were tailored for\nspecific tasks or lacked integrated machine learning capabilities. While\nOpenLS-DGF supports various machine learning tasks by encapsulating the three\nfundamental steps of logic synthesis: Boolean representation, logic\noptimization, and technology mapping. It preserves the original information in\nboth Verilog and machine-learning-friendly GraphML formats. The verilog files\noffer semi-customizable capabilities, enabling researchers to insert additional\nsteps and incrementally refine the generated dataset. Furthermore, OpenLS-DGF\nincludes an adaptive circuit engine that facilitates the final dataset\nmanagement and downstream tasks. The generated OpenLS-D-v1 dataset comprises 46\ncombinational designs from established benchmarks, totaling over 966,000\nBoolean circuits. OpenLS-D-v1 supports integrating new data features, making it\nmore versatile for new challenges. This paper demonstrates the versatility of\nOpenLS-D-v1 through four distinct downstream tasks: circuit classification,\ncircuit ranking, quality of results (QoR) prediction, and probability\nprediction. Each task is chosen to represent essential steps of logic\nsynthesis, and the experimental results show the generated dataset from\nOpenLS-DGF achieves prominent diversity and applicability. The source code and\ndatasets are available at\nhttps://github.com/Logic-Factory/ACE/blob/master/OpenLS-DGF/readme.md.",
      "tldr_zh": "这篇论文引入了 OpenLS-DGF，一个自适应开源框架，用于生成逻辑综合领域的机器学习 (ML) 数据集，以解决现有框架针对特定任务或缺少集成 ML 能力的局限性。OpenLS-DGF 通过封装逻辑综合的三个核心步骤——布尔表示、逻辑优化和技术映射，并以 Verilog 和 GraphML 格式保留原始信息，支持半自定义和数据集的增量优化。生成的 OpenLS-D-v1 数据集包含 46 个组合设计，总计超过 966,000 个布尔电路，并在电路分类、电路排名、QoR 预测和概率预测等下游任务上展现出显著的多样性和适用性。该框架及其数据集已开源，可进一步扩展以应对新挑战。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "14 pages",
      "pdf_url": "http://arxiv.org/pdf/2411.09422v2",
      "published_date": "2024-11-14 13:18:06 UTC",
      "updated_date": "2024-11-16 07:48:26 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T00:31:06.357495"
    },
    {
      "arxiv_id": "2411.09420v3",
      "title": "SAG-ViT: A Scale-Aware, High-Fidelity Patching Approach with Graph Attention for Vision Transformers",
      "title_zh": "SAG-ViT：一种基于图注意力的尺度感知高保真分块方法，用于视觉Transformer",
      "authors": [
        "Shravan Venkatraman",
        "Jaskaran Singh Walia",
        "Joe Dhanith P R"
      ],
      "abstract": "Vision Transformers (ViTs) have redefined image classification by leveraging\nself-attention to capture complex patterns and long-range dependencies between\nimage patches. However, a key challenge for ViTs is efficiently incorporating\nmulti-scale feature representations, which is inherent in convolutional neural\nnetworks (CNNs) through their hierarchical structure. Graph transformers have\nmade strides in addressing this by leveraging graph-based modeling, but they\noften lose or insufficiently represent spatial hierarchies, especially since\nredundant or less relevant areas dilute the image's contextual representation.\nTo bridge this gap, we propose SAG-ViT, a Scale-Aware Graph Attention ViT that\nintegrates multi-scale feature capabilities of CNNs, representational power of\nViTs, graph-attended patching to enable richer contextual representation. Using\nEfficientNetV2 as a backbone, the model extracts multi-scale feature maps,\ndividing them into patches to preserve richer semantic information compared to\ndirectly patching the input images. The patches are structured into a graph\nusing spatial and feature similarities, where a Graph Attention Network (GAT)\nrefines the node embeddings. This refined graph representation is then\nprocessed by a Transformer encoder, capturing long-range dependencies and\ncomplex interactions. We evaluate SAG-ViT on benchmark datasets across various\ndomains, validating its effectiveness in advancing image classification tasks.\nOur code and weights are available at https://github.com/shravan-18/SAG-ViT.",
      "tldr_zh": "该论文提出SAG-ViT，一种关注多尺度的图注意力Vision Transformers (ViTs)，旨在解决ViTs在整合多尺度特征表示方面的挑战，通过结合CNNs的层次结构和ViTs的表示能力提升图像分类性能。方法包括使用EfficientNetV2作为backbone提取多尺度特征图，将其分成补丁构建图结构，并应用Graph Attention Network (GAT)精炼节点嵌入，随后通过Transformer编码器捕获长距离依赖和复杂交互。实验结果显示，SAG-ViT在各种基准数据集上表现出色，有效提升了图像分类任务的准确性和上下文表示。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG",
        "68T07",
        "I.2.10"
      ],
      "primary_category": "cs.CV",
      "comment": "14 pages, 8 figures, 9 tables",
      "pdf_url": "http://arxiv.org/pdf/2411.09420v3",
      "published_date": "2024-11-14 13:15:27 UTC",
      "updated_date": "2025-01-08 04:31:16 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T00:31:17.974463"
    },
    {
      "arxiv_id": "2411.09413v2",
      "title": "Detecting Children with Autism Spectrum Disorder based on Script-Centric Behavior Understanding with Emotional Enhancement",
      "title_zh": "翻译失败",
      "authors": [
        "Wenxing Liu",
        "Yueran Pan",
        "Dong Zhang",
        "Hongzhu Deng",
        "Xiaobing Zou",
        "Ming Li"
      ],
      "abstract": "The early diagnosis of autism spectrum disorder (ASD) is critically dependent\non systematic observation and analysis of children's social behaviors. While\ncurrent methodologies predominantly utilize supervised learning approaches,\ntheir clinical adoption faces two principal limitations: insufficient ASD\ndiagnostic samples and inadequate interpretability of the detection outcomes.\nThis paper presents a novel zero-shot ASD detection framework based on\nscript-centric behavioral understanding with emotional enhancement, which is\ndesigned to overcome the aforementioned clinical constraints. The proposed\npipeline automatically converts audio-visual data into structured behavioral\ntext scripts through computer vision techniques, subsequently capitalizing on\nthe generalization capabilities of large language models (LLMs) for\nzero-shot/few-shot ASD detection. Three core technical contributions are\nintroduced: (1) A multimodal script transcription module transforming\nbehavioral cues into structured textual representations. (2) An emotion\ntextualization module encoding emotional dynamics as the contextual features to\naugment behavioral understanding. (3) A domain-specific prompt engineering\nstrategy enables the injection of clinical knowledge into LLMs. Our method\nachieves an F1-score of 95.24\\% in diagnosing ASD in children with an average\nage of two years while generating interpretable detection rationales. This work\nopens up new avenues for leveraging the power of LLMs in analyzing and\nunderstanding ASD-related human behavior, thereby enhancing the accuracy of\nassisted autism diagnosis.",
      "tldr_zh": "本研究提出了一种基于脚本中心行为理解并融入情感增强的零-shot ASD 检测框架，旨在解决现有监督学习方法在自闭症谱系障碍 (ASD) 诊断中样本不足和可解释性差的问题。该框架通过多模态脚本转录模块将音频-视觉数据转换为结构化文本脚本，并利用情感文本化模块编码情感动态作为上下文特征，同时采用领域特定提示工程策略将临床知识注入大型语言模型 (LLMs)，实现零-shot 或 few-shot ASD 检测。实验结果显示，该方法在诊断平均年龄两岁的儿童ASD时达到了95.24%的 F1-score，并能生成可解释的检测理由。该工作开辟了使用 LLMs 分析ASD相关行为的途径，提升了辅助诊断的准确性和临床实用性。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "15 pages, 12 figures, sumbitted to IEEE transactions on affective\n  computing",
      "pdf_url": "http://arxiv.org/pdf/2411.09413v2",
      "published_date": "2024-11-14 13:07:19 UTC",
      "updated_date": "2025-04-29 07:46:18 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T00:31:29.174294"
    },
    {
      "arxiv_id": "2411.09403v1",
      "title": "Quantum Machine Learning: An Interplay Between Quantum Computing and Machine Learning",
      "title_zh": "量子机器学习：量子计算与机器学习之间的相互作用",
      "authors": [
        "Jun Qi",
        "Chao-Han Yang",
        "Samuel Yen-Chi Chen",
        "Pin-Yu Chen"
      ],
      "abstract": "Quantum machine learning (QML) is a rapidly growing field that combines\nquantum computing principles with traditional machine learning. It seeks to\nrevolutionize machine learning by harnessing the unique capabilities of quantum\nmechanics and employs machine learning techniques to advance quantum computing\nresearch. This paper introduces quantum computing for the machine learning\nparadigm, where variational quantum circuits (VQC) are used to develop QML\narchitectures on noisy intermediate-scale quantum (NISQ) devices. We discuss\nmachine learning for the quantum computing paradigm, showcasing our recent\ntheoretical and empirical findings. In particular, we delve into future\ndirections for studying QML, exploring the potential industrial impacts of QML\nresearch.",
      "tldr_zh": "这篇论文探讨了量子机器学习 (QML)，一个将量子计算原理与传统机器学习相结合的快速发展的领域，旨在利用量子力学的独特能力革新机器学习，并通过机器学习技术推进量子计算研究。论文介绍了量子计算在机器学习范式中的应用，使用变分量子电路 (VQC) 在噪声中等规模量子 (NISQ) 设备上构建 QML 架构，并展示了最近的理论和实证发现。最终，它讨论了 QML 的未来方向及其潜在的工业影响，例如在实际应用中的革命性潜力。",
      "categories": [
        "quant-ph",
        "cs.AI"
      ],
      "primary_category": "quant-ph",
      "comment": "In submission",
      "pdf_url": "http://arxiv.org/pdf/2411.09403v1",
      "published_date": "2024-11-14 12:27:50 UTC",
      "updated_date": "2024-11-14 12:27:50 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T00:31:41.447111"
    },
    {
      "arxiv_id": "2411.09402v2",
      "title": "Automated Segmentation of Ischemic Stroke Lesions in Non-Contrast Computed Tomography Images for Enhanced Treatment and Prognosis",
      "title_zh": "翻译失败",
      "authors": [
        "Toufiq Musah",
        "Prince Ebenezer Adjei",
        "Kojo Obed Otoo"
      ],
      "abstract": "Stroke is the second leading cause of death worldwide, and is increasingly\nprevalent in low- and middle-income countries (LMICs). Timely interventions can\nsignificantly influence stroke survivability and the quality of life after\ntreatment. However, the standard and most widely available imaging method for\nconfirming strokes and their sub-types, the NCCT, is more challenging and\ntime-consuming to employ in cases of ischemic stroke. For this reason, we\ndeveloped an automated method for ischemic stroke lesion segmentation in NCCTs\nusing the nnU-Net frame work, aimed at enhancing early treatment and improving\nthe prognosis of ischemic stroke patients. We achieved Dice scores of 0.596 and\nIntersection over Union (IoU) scores of 0.501 on the sampled dataset. After\nadjusting for outliers, these scores improved to 0.752 for the Dice score and\n0.643 for the IoU. Proper delineation of the region of infarction can help\nclinicians better assess the potential impact of the infarction, and guide\ntreatment procedures.",
      "tldr_zh": "本研究针对缺血性卒中在非对比 CT（NCCT）图像中的分割挑战，开发了一种基于 nnU-Net 框架的自动分割方法，以提升早期干预、改善患者预后和生活质量。该方法在采样数据集上实现了 Dice 分数 0.596 和 IoU 分数 0.501，调整异常值后分别提升至 0.752 和 0.643。通过精确描绘梗塞区域，该技术可帮助临床医生更好地评估病变影响并指导治疗决策。",
      "categories": [
        "eess.IV",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "eess.IV",
      "comment": "7 pages, 3 figures, MICCAI Meets Africa Workshop",
      "pdf_url": "http://arxiv.org/pdf/2411.09402v2",
      "published_date": "2024-11-14 12:27:31 UTC",
      "updated_date": "2024-11-15 09:52:20 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T00:31:53.646993"
    },
    {
      "arxiv_id": "2411.09723v1",
      "title": "Towards Neural Foundation Models for Vision: Aligning EEG, MEG, and fMRI Representations for Decoding, Encoding, and Modality Conversion",
      "title_zh": "翻译失败",
      "authors": [
        "Matteo Ferrante",
        "Tommaso Boccato",
        "Grigorii Rashkov",
        "Nicola Toschi"
      ],
      "abstract": "This paper presents a novel approach towards creating a foundational model\nfor aligning neural data and visual stimuli across multimodal representationsof\nbrain activity by leveraging contrastive learning. We used\nelectroencephalography (EEG), magnetoencephalography (MEG), and functional\nmagnetic resonance imaging (fMRI) data. Our framework's capabilities are\ndemonstrated through three key experiments: decoding visual information from\nneural data, encoding images into neural representations, and converting\nbetween neural modalities. The results highlight the model's ability to\naccurately capture semantic information across different brain imaging\ntechniques, illustrating its potential in decoding, encoding, and modality\nconversion tasks.",
      "tldr_zh": "这篇论文提出了一种利用对比学习（contrastive learning）的神经基础模型框架，用于对齐 EEG、MEG 和 fMRI 等多模态脑活动表示，以实现视觉刺激的处理。框架通过三个关键实验展示其能力：从神经数据解码视觉信息、将图像编码成神经表示，以及在不同脑成像模态之间进行转换。结果显示，该模型能够准确捕捉跨技术的语义信息，在解码、编码和模态转换任务中表现出显著潜力。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2411.09723v1",
      "published_date": "2024-11-14 12:27:27 UTC",
      "updated_date": "2024-11-14 12:27:27 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T00:32:04.838479"
    },
    {
      "arxiv_id": "2411.09400v2",
      "title": "Imagined Speech and Visual Imagery as Intuitive Paradigms for Brain-Computer Interfaces",
      "title_zh": "想象言语和视觉意象作为脑机接口的直观范式",
      "authors": [
        "Seo-Hyun Lee",
        "Ji-Ha Park",
        "Deok-Seon Kim"
      ],
      "abstract": "Brain-computer interfaces (BCIs) have shown promise in enabling communication\nfor individuals with motor impairments. Recent advancements like\nbrain-to-speech technology aim to reconstruct speech from neural activity.\nHowever, decoding communication-related paradigms, such as imagined speech and\nvisual imagery, using non-invasive techniques remains challenging. This study\nanalyzes brain dynamics in these two paradigms by examining neural\nsynchronization and functional connectivity through phase-locking values (PLV)\nin EEG data from 16 participants. Results show that visual imagery produces\nhigher PLV values in visual cortex, engaging spatial networks, while imagined\nspeech demonstrates consistent synchronization, primarily engaging\nlanguage-related regions. These findings suggest that imagined speech is\nsuitable for language-driven BCI applications, while visual imagery can\ncomplement BCI systems for users with speech impairments. Personalized\ncalibration is crucial for optimizing BCI performance.",
      "tldr_zh": "这篇论文探讨了想象说话（imagined speech）和视觉想象（visual imagery）作为脑机接口（BCIs）的直观范式，以提升运动障碍患者沟通能力。研究通过分析16名参与者的EEG数据和相位锁定值（PLV），考察了这些范式的神经同步和功能连接。结果显示，视觉想象在视觉皮层产生更高PLV值，主要涉及空间网络，而想象说话则显示一致同步，优先激活语言相关区域。这些发现表明，想象说话适合用于语言驱动的BCI应用，视觉想象可补充系统以支持有语言障碍的用户，并强调个性化校准对于优化BCI性能至关重要。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "4 pages",
      "pdf_url": "http://arxiv.org/pdf/2411.09400v2",
      "published_date": "2024-11-14 12:19:28 UTC",
      "updated_date": "2024-11-29 16:34:55 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T00:32:17.532263"
    },
    {
      "arxiv_id": "2411.09389v1",
      "title": "Less is More: Unseen Domain Fake News Detection via Causal Propagation Substructures",
      "title_zh": "翻译失败",
      "authors": [
        "Shuzhi Gong",
        "Richard O. Sinnott",
        "Jianzhong Qi",
        "Cecile Paris"
      ],
      "abstract": "The spread of fake news on social media poses significant threats to\nindividuals and society. Text-based and graph-based models have been employed\nfor fake news detection by analysing news content and propagation networks,\nshowing promising results in specific scenarios. However, these data-driven\nmodels heavily rely on pre-existing in-distribution data for training, limiting\ntheir performance when confronted with fake news from emerging or previously\nunseen domains, known as out-of-distribution (OOD) data. Tackling OOD fake news\nis a challenging yet critical task. In this paper, we introduce the Causal\nSubgraph-oriented Domain Adaptive Fake News Detection (CSDA) model, designed to\nenhance zero-shot fake news detection by extracting causal substructures from\npropagation graphs using in-distribution data and generalising this approach to\nOOD data. The model employs a graph neural network based mask generation\nprocess to identify dominant nodes and edges within the propagation graph,\nusing these substructures for fake news detection. Additionally, the\nperformance of CSDA is further improved through contrastive learning in\nfew-shot scenarios, where a limited amount of OOD data is available for\ntraining. Extensive experiments on public social media datasets demonstrate\nthat CSDA effectively handles OOD fake news detection, achieving a 7 to 16\npercents accuracy improvement over other state-of-the-art models.",
      "tldr_zh": "该论文针对社交媒体假新闻检测的挑战，提出 Causal Subgraph-oriented Domain Adaptive Fake News Detection (CSDA) 模型，以处理出分布 (OOD) 数据问题，即新领域假新闻的检测。模型通过图神经网络 (GNN) 基于掩码生成过程从传播图中提取因果子结构 (causal substructures)，识别主导节点和边，从而实现零样本假新闻检测，并在少样本场景下利用对比学习 (contrastive learning) 进一步提升性能。实验结果显示，在公共数据集上，CSDA 比其他最先进模型的准确率提高了 7 到 16%。",
      "categories": [
        "cs.SI",
        "cs.AI",
        "cs.CL",
        "cs.CY",
        "cs.LG"
      ],
      "primary_category": "cs.SI",
      "comment": "9 pages, 2 figures, 5 tables",
      "pdf_url": "http://arxiv.org/pdf/2411.09389v1",
      "published_date": "2024-11-14 12:05:35 UTC",
      "updated_date": "2024-11-14 12:05:35 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T00:32:29.048942"
    },
    {
      "arxiv_id": "2411.09366v1",
      "title": "LTLf+ and PPLTL+: Extending LTLf and PPLTL to Infinite Traces",
      "title_zh": "LTLf+ 和 PPLTL+：将 LTLf 和 PPLTL 扩展到无限轨迹",
      "authors": [
        "Benjamin Aminof",
        "Giuseppe De Giacomo",
        "Sasha Rubin",
        "Moshe Y. Vardi"
      ],
      "abstract": "We introduce LTLf+ and PPLTL+, two logics to express properties of infinite\ntraces, that are based on the linear-time temporal logics LTLf and PPLTL on\nfinite traces. LTLf+/PPLTL+ use levels of Manna and Pnueli's LTL\nsafety-progress hierarchy, and thus have the same expressive power as LTL.\nHowever, they also retain a crucial characteristic of the reactive synthesis\nproblem for the base logics: the game arena for strategy extraction can be\nderived from deterministic finite automata (DFA). Consequently, these logics\ncircumvent the notorious difficulties associated with determinizing infinite\ntrace automata, typical of LTL reactive synthesis. We present DFA-based\nsynthesis techniques for LTLf+/PPLTL+, and show that synthesis is\n2EXPTIME-complete for LTLf+ (matching LTLf) and EXPTIME-complete for PPLTL+\n(matching PPLTL). Notably, while PPLTL+ retains the full expressive power of\nLTL, reactive synthesis is EXPTIME-complete instead of 2EXPTIME-complete. The\ntechniques are also adapted to optimally solve satisfiability, validity, and\nmodel-checking, to get EXPSPACE-complete for LTLf+ (extending a recent result\nfor the guarantee level using LTLf), and PSPACE-complete for PPLTL+.",
      "tldr_zh": "本文提出LTLf+和PPLTL+两种逻辑，用于扩展LTLf和PPLTL以表达无限轨迹的属性，这些逻辑基于Manna和Pnueli的LTL安全-进展层次，并保留了LTL的表达能力，同时通过从确定性有限自动机(DFA)派生游戏竞技场，规避了LTL反应式综合中的无限轨迹确定化难题。研究呈现了基于DFA的合成技术，其中LTLf+的合成复杂度为2EXPTIME-complete，PPLTL+为EXPTIME-complete，后者尽管保持LTL的全部表达力，但合成效率更高。 additionally, 这些技术优化了可满足性、有效性和模型检查问题，使LTLf+达到EXPSPACE-complete，PPLTL+达到PSPACE-complete。",
      "categories": [
        "cs.LO",
        "cs.AI",
        "cs.FL"
      ],
      "primary_category": "cs.LO",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2411.09366v1",
      "published_date": "2024-11-14 11:17:06 UTC",
      "updated_date": "2024-11-14 11:17:06 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T00:32:43.077648"
    },
    {
      "arxiv_id": "2411.09722v1",
      "title": "Iterative Batch Reinforcement Learning via Safe Diversified Model-based Policy Search",
      "title_zh": "翻译失败",
      "authors": [
        "Amna Najib",
        "Stefan Depeweg",
        "Phillip Swazinna"
      ],
      "abstract": "Batch reinforcement learning enables policy learning without direct\ninteraction with the environment during training, relying exclusively on\npreviously collected sets of interactions. This approach is, therefore,\nwell-suited for high-risk and cost-intensive applications, such as industrial\ncontrol. Learned policies are commonly restricted to act in a similar fashion\nas observed in the batch. In a real-world scenario, learned policies are\ndeployed in the industrial system, inevitably leading to the collection of new\ndata that can subsequently be added to the existing recording. The process of\nlearning and deployment can thus take place multiple times throughout the\nlifespan of a system. In this work, we propose to exploit this iterative nature\nof applying offline reinforcement learning to guide learned policies towards\nefficient and informative data collection during deployment, leading to\ncontinuous improvement of learned policies while remaining within the support\nof collected data. We present an algorithmic methodology for iterative batch\nreinforcement learning based on ensemble-based model-based policy search,\naugmented with safety and, importantly, a diversity criterion.",
      "tldr_zh": "该论文提出了一种迭代批量强化学习（Batch Reinforcement Learning）方法，通过安全的多样化基于模型的策略搜索（Safe Diversified Model-based Policy Search）来优化策略学习。该方法利用部署过程中收集的新数据，引导策略高效且信息丰富地采样，同时确保安全性和多样性标准，以避免偏离已收集数据的支持范围。基于 ensemble-based model-based policy search 的算法框架，支持在高风险应用如工业控制中实现策略的连续改进，从而提升整体性能。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "stat.ML"
      ],
      "primary_category": "cs.LG",
      "comment": "Workshop on Safe and Robust Robot Learning for Operation in the Real\n  World (SAFE-ROL) at CoRL 2024",
      "pdf_url": "http://arxiv.org/pdf/2411.09722v1",
      "published_date": "2024-11-14 11:10:36 UTC",
      "updated_date": "2024-11-14 11:10:36 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T00:32:53.362272"
    },
    {
      "arxiv_id": "2411.09359v2",
      "title": "Your Semantic-Independent Watermark is Fragile: A Semantic Perturbation Attack against EaaS Watermark",
      "title_zh": "翻译失败",
      "authors": [
        "Zekun Fei",
        "Biao Yi",
        "Jianing Geng",
        "Ruiqi He",
        "Lihai Nie",
        "Zheli Liu"
      ],
      "abstract": "Embedding-as-a-Service (EaaS) has emerged as a successful business pattern\nbut faces significant challenges related to various forms of copyright\ninfringement, particularly, the API misuse and model extraction attacks.\nVarious studies have proposed backdoor-based watermarking schemes to protect\nthe copyright of EaaS services. In this paper, we reveal that previous\nwatermarking schemes possess semantic-independent characteristics and propose\nthe Semantic Perturbation Attack (SPA). Our theoretical and experimental\nanalysis demonstrate that this semantic-independent nature makes current\nwatermarking schemes vulnerable to adaptive attacks that exploit semantic\nperturbations tests to bypass watermark verification. Extensive experimental\nresults across multiple datasets demonstrate that the True Positive Rate (TPR)\nfor identifying watermarked samples under SPA can reach up to more than 95\\%,\nrendering watermarks ineffective while maintaining the high utility of\nembeddings. Furthermore, we discuss potential defense strategies to mitigate\nSPA. Our code is available at\nhttps://github.com/Zk4-ps/EaaS-Embedding-Watermark.",
      "tldr_zh": "该研究揭示了现有基于后门的水印方案在 Embedding-as-a-Service (EaaS) 中的弱点，即其语义无关特性容易被攻击利用，并提出 Semantic Perturbation Attack (SPA) 来绕过水印验证。SPA 通过语义扰动测试来破坏水印识别，同时保持嵌入的高效用，理论和实验分析证明其有效性。实验结果显示，在多个数据集上，SPA 使水印的 True Positive Rate (TPR) 达到 95% 以上，导致水印失效。最后，论文讨论了潜在的防御策略，以提升 EaaS 服务的版权保护。",
      "categories": [
        "cs.CR",
        "cs.AI"
      ],
      "primary_category": "cs.CR",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2411.09359v2",
      "published_date": "2024-11-14 11:06:34 UTC",
      "updated_date": "2025-02-15 14:46:44 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T00:33:06.480747"
    },
    {
      "arxiv_id": "2411.09356v1",
      "title": "Multi-scale Generative Modeling for Fast Sampling",
      "title_zh": "多尺度生成建模用于快速采样",
      "authors": [
        "Xiongye Xiao",
        "Shixuan Li",
        "Luzhe Huang",
        "Gengshuo Liu",
        "Trung-Kien Nguyen",
        "Yi Huang",
        "Di Chang",
        "Mykel J. Kochenderfer",
        "Paul Bogdan"
      ],
      "abstract": "While working within the spatial domain can pose problems associated with\nill-conditioned scores caused by power-law decay, recent advances in\ndiffusion-based generative models have shown that transitioning to the wavelet\ndomain offers a promising alternative. However, within the wavelet domain, we\nencounter unique challenges, especially the sparse representation of\nhigh-frequency coefficients, which deviates significantly from the Gaussian\nassumptions in the diffusion process. To this end, we propose a multi-scale\ngenerative modeling in the wavelet domain that employs distinct strategies for\nhandling low and high-frequency bands. In the wavelet domain, we apply\nscore-based generative modeling with well-conditioned scores for low-frequency\nbands, while utilizing a multi-scale generative adversarial learning for\nhigh-frequency bands. As supported by the theoretical analysis and experimental\nresults, our model significantly improve performance and reduce the number of\ntrainable parameters, sampling steps, and time.",
      "tldr_zh": "该研究针对扩散基生成模型在空间域中面临的分数不稳定问题（如幂律衰减），提出了一种在小波域（wavelet domain）中的多尺度生成模型，以加速采样过程。具体来说，该模型为低频带采用基于分数的生成建模（score-based generative modeling），确保分数条件良好；为高频带使用多尺度生成对抗学习（multi-scale generative adversarial learning），处理稀疏表示的挑战。理论分析和实验结果表明，该方法显著提升了性能，同时减少了可训练参数、采样步骤和时间。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2411.09356v1",
      "published_date": "2024-11-14 11:01:45 UTC",
      "updated_date": "2024-11-14 11:01:45 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T00:33:18.292171"
    },
    {
      "arxiv_id": "2411.09355v2",
      "title": "Prices, Bids, Values: One ML-Powered Combinatorial Auction to Rule Them All",
      "title_zh": "价格、竞标、",
      "authors": [
        "Ermis Soumalias",
        "Jakob Heiss",
        "Jakob Weissteiner",
        "Sven Seuken"
      ],
      "abstract": "We study the design of iterative combinatorial auctions (ICAs). The main\nchallenge in this domain is that the bundle space grows exponentially in the\nnumber of items. To address this, recent work has proposed machine learning\n(ML)-based preference elicitation algorithms that aim to elicit only the most\ncritical information from bidders to maximize efficiency. However, while the\nSOTA ML-based algorithms elicit bidders' preferences via value queries, ICAs\nthat are used in practice elicit information via demand queries. In this paper,\nwe introduce a novel ML algorithm that provably makes use of the full\ninformation from both value and demand queries, and we show via experiments\nthat combining both query types results in significantly better learning\nperformance in practice. Building on these insights, we present MLHCA, a new\nML-powered auction that uses value and demand queries. MLHCA substantially\noutperforms the previous SOTA, reducing efficiency loss by up to a factor 10,\nwith up to 58% fewer queries. Thus, MLHCA achieves large efficiency\nimprovements while also reducing bidders' cognitive load, establishing a new\nbenchmark for both practicability and efficiency.",
      "tldr_zh": "这篇论文研究了迭代组合拍卖 (ICAs) 的设计问题，针对物品组合空间指数级增长的挑战，提出了一种新型机器学习 (ML) 算法，该算法整合价值查询 (value queries) 和需求查询 (demand queries) 的完整信息，以更高效地获取竞标者的偏好。实验结果显示，该算法在实际场景中显著提升了学习性能，为后续开发奠定了基础。最终，论文介绍了 MLHCA，这是一种新的 ML 驱动拍卖机制，比现有最先进 (SOTA) 方法减少效率损失高达 10 倍，并减少多达 58% 的查询次数，从而降低了竞标者的认知负担，并建立了新的实用性和效率基准。",
      "categories": [
        "cs.GT",
        "cs.AI",
        "cs.LG",
        "91A06, 68T07, 91-08",
        "I.2; I.2.6; J.4"
      ],
      "primary_category": "cs.GT",
      "comment": "8 pages + appendix",
      "pdf_url": "http://arxiv.org/pdf/2411.09355v2",
      "published_date": "2024-11-14 10:56:00 UTC",
      "updated_date": "2025-02-01 05:52:39 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T00:33:31.361861"
    },
    {
      "arxiv_id": "2411.09302v1",
      "title": "EEG-Based Speech Decoding: A Novel Approach Using Multi-Kernel Ensemble Diffusion Models",
      "title_zh": "翻译失败",
      "authors": [
        "Soowon Kim",
        "Ha-Na Jo",
        "Eunyeong Ko"
      ],
      "abstract": "In this study, we propose an ensemble learning framework for\nelectroencephalogram-based overt speech classification, leveraging denoising\ndiffusion probabilistic models with varying convolutional kernel sizes. The\nensemble comprises three models with kernel sizes of 51, 101, and 201,\neffectively capturing multi-scale temporal features inherent in signals. This\napproach improves the robustness and accuracy of speech decoding by\naccommodating the rich temporal complexity of neural signals. The ensemble\nmodels work in conjunction with conditional autoencoders that refine the\nreconstructed signals and maximize the useful information for downstream\nclassification tasks. The results indicate that the proposed ensemble-based\napproach significantly outperforms individual models and existing\nstate-of-the-art techniques. These findings demonstrate the potential of\nensemble methods in advancing brain signal decoding, offering new possibilities\nfor non-verbal communication applications, particularly in brain-computer\ninterface systems aimed at aiding individuals with speech impairments.",
      "tldr_zh": "本文提出了一种基于 EEG 的显性语音分类新框架，利用多核集成去噪扩散概率模型（denoising diffusion probabilistic models），通过不同卷积核大小（51、101 和 201）捕捉多尺度时间特征，从而提升语音解码的鲁棒性和准确性。该框架与条件自编码器（conditional autoencoders）结合，优化信号重建并最大化下游分类任务的信息利用。实验结果表明，该方法显著优于单个模型和现有最先进技术，为脑机接口系统的非语言通信应用（如帮助言语障碍者）提供了新潜力。",
      "categories": [
        "cs.SD",
        "cs.AI",
        "eess.AS",
        "eess.SP"
      ],
      "primary_category": "cs.SD",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2411.09302v1",
      "published_date": "2024-11-14 09:23:58 UTC",
      "updated_date": "2024-11-14 09:23:58 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T00:33:42.346272"
    },
    {
      "arxiv_id": "2411.09294v1",
      "title": "Learning Hand State Estimation for a Light Exoskeleton",
      "title_zh": "翻译失败",
      "authors": [
        "Gabriele Abbate",
        "Alessandro Giusti",
        "Luca Randazzo",
        "Antonio Paolillo"
      ],
      "abstract": "We propose a machine learning-based estimator of the hand state for\nrehabilitation purposes, using light exoskeletons. These devices are easy to\nuse and useful for delivering domestic and frequent therapies. We build a\nsupervised approach using information from the muscular activity of the forearm\nand the motion of the exoskeleton to reconstruct the hand's opening degree and\ncompliance level. Such information can be used to evaluate the therapy progress\nand develop adaptive control behaviors. Our approach is validated with a real\nlight exoskeleton. The experiments demonstrate good predictive performance of\nour approach when trained on data coming from a single user and tested on the\nsame user, even across different sessions. This generalization capability makes\nour system promising for practical use in real rehabilitation.",
      "tldr_zh": "本文提出了一种基于machine learning的估计器，用于轻型exoskeleton的手部状态估计，旨在支持康复治疗。该方法采用监督学习，利用前臂的muscular activity和exoskeleton的motion作为输入，重建手部的opening degree和compliance level，以评估疗效进展并开发自适应控制行为。实验在真实exoskeleton上验证，显示该系统在同一用户数据上训练并跨会话测试时具有良好预测性能和泛化能力，具有实际康复应用的潜力。",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2411.09294v1",
      "published_date": "2024-11-14 09:12:38 UTC",
      "updated_date": "2024-11-14 09:12:38 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T00:33:53.747817"
    },
    {
      "arxiv_id": "2411.09289v1",
      "title": "StreamAdapter: Efficient Test Time Adaptation from Contextual Streams",
      "title_zh": "翻译失败",
      "authors": [
        "Dilxat Muhtar",
        "Yelong Shen",
        "Yaming Yang",
        "Xiaodong Liu",
        "Yadong Lu",
        "Jianfeng Liu",
        "Yuefeng Zhan",
        "Hao Sun",
        "Weiwei Deng",
        "Feng Sun",
        "Xueliang Zhang",
        "Jianfeng Gao",
        "Weizhu Chen",
        "Qi Zhang"
      ],
      "abstract": "In-context learning (ICL) allows large language models (LLMs) to adapt to new\ntasks directly from the given demonstrations without requiring gradient\nupdates. While recent advances have expanded context windows to accommodate\nmore demonstrations, this approach increases inference costs without\nnecessarily improving performance. To mitigate these issues, We propose\nStreamAdapter, a novel approach that directly updates model parameters from\ncontext at test time, eliminating the need for explicit in-context\ndemonstrations. StreamAdapter employs context mapping and weight absorption\nmechanisms to dynamically transform ICL demonstrations into parameter updates\nwith minimal additional parameters. By reducing reliance on numerous in-context\nexamples, StreamAdapter significantly reduce inference costs and allows for\nefficient inference with constant time complexity, regardless of demonstration\ncount. Extensive experiments across diverse tasks and model architectures\ndemonstrate that StreamAdapter achieves comparable or superior adaptation\ncapability to ICL while requiring significantly fewer demonstrations. The\nsuperior task adaptation and context encoding capabilities of StreamAdapter on\nboth language understanding and generation tasks provides a new perspective for\nadapting LLMs at test time using context, allowing for more efficient\nadaptation across scenarios and more cost-effective inference",
      "tldr_zh": "本文提出 StreamAdapter，一种高效的测试时适应方法，用于大语言模型 (LLMs) 从上下文流中直接更新参数，而非依赖传统的 In-context Learning (ICL) 所需的众多演示示例。StreamAdapter 采用 context mapping 和 weight absorption 机制，将 ICL 演示转换为动态参数更新，从而减少额外参数并实现恒定的时间复杂度。实验在多种任务和模型架构上表明，该方法使用更少的演示即可达到与 ICL 相当或优越的适应性能，同时显著降低推理成本。该创新为 LLMs 的测试时适应提供了新视角，提升了效率和成本效益。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "22 Pages, 9 Figures",
      "pdf_url": "http://arxiv.org/pdf/2411.09289v1",
      "published_date": "2024-11-14 09:03:54 UTC",
      "updated_date": "2024-11-14 09:03:54 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T00:34:06.435802"
    },
    {
      "arxiv_id": "2411.09273v1",
      "title": "Cross-Modal Consistency in Multimodal Large Language Models",
      "title_zh": "多模态大语言模型中的跨模态一致性",
      "authors": [
        "Xiang Zhang",
        "Senyu Li",
        "Ning Shi",
        "Bradley Hauer",
        "Zijun Wu",
        "Grzegorz Kondrak",
        "Muhammad Abdul-Mageed",
        "Laks V. S. Lakshmanan"
      ],
      "abstract": "Recent developments in multimodal methodologies have marked the beginning of\nan exciting era for models adept at processing diverse data types, encompassing\ntext, audio, and visual content. Models like GPT-4V, which merge computer\nvision with advanced language processing, exhibit extraordinary proficiency in\nhandling intricate tasks that require a simultaneous understanding of both\ntextual and visual information. Prior research efforts have meticulously\nevaluated the efficacy of these Vision Large Language Models (VLLMs) in various\ndomains, including object detection, image captioning, and other related\nfields. However, existing analyses have often suffered from limitations,\nprimarily centering on the isolated evaluation of each modality's performance\nwhile neglecting to explore their intricate cross-modal interactions.\nSpecifically, the question of whether these models achieve the same level of\naccuracy when confronted with identical task instances across different\nmodalities remains unanswered. In this study, we take the initiative to delve\ninto the interaction and comparison among these modalities of interest by\nintroducing a novel concept termed cross-modal consistency. Furthermore, we\npropose a quantitative evaluation framework founded on this concept. Our\nexperimental findings, drawn from a curated collection of parallel\nvision-language datasets developed by us, unveil a pronounced inconsistency\nbetween the vision and language modalities within GPT-4V, despite its portrayal\nas a unified multimodal model. Our research yields insights into the\nappropriate utilization of such models and hints at potential avenues for\nenhancing their design.",
      "tldr_zh": "本研究探讨了Multimodal Large Language Models（如GPT-4V）中模态间的cross-modal consistency问题，强调现有评估往往忽略了文本、视觉等模态的交互。作者引入cross-modal consistency概念，并提出一个基于此的定量评估框架，使用自定义的平行视觉-语言数据集进行实验。结果显示，GPT-4V在不同模态上表现出显著不一致，这为优化模型设计和指导其实际应用提供了重要洞见。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2411.09273v1",
      "published_date": "2024-11-14 08:22:42 UTC",
      "updated_date": "2024-11-14 08:22:42 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T00:34:18.237969"
    },
    {
      "arxiv_id": "2411.09269v1",
      "title": "Harnessing multiple LLMs for Information Retrieval: A case study on Deep Learning methodologies in Biodiversity publications",
      "title_zh": "翻译失败",
      "authors": [
        "Vamsi Krishna Kommineni",
        "Birgitta König-Ries",
        "Sheeba Samuel"
      ],
      "abstract": "Deep Learning (DL) techniques are increasingly applied in scientific studies\nacross various domains to address complex research questions. However, the\nmethodological details of these DL models are often hidden in the unstructured\ntext. As a result, critical information about how these models are designed,\ntrained, and evaluated is challenging to access and comprehend. To address this\nissue, in this work, we use five different open-source Large Language Models\n(LLMs): Llama-3 70B, Llama-3.1 70B, Mixtral-8x22B-Instruct-v0.1, Mixtral 8x7B,\nand Gemma 2 9B in combination with Retrieval-Augmented Generation (RAG)\napproach to extract and process DL methodological details from scientific\npublications automatically. We built a voting classifier from the outputs of\nfive LLMs to accurately report DL methodological information. We tested our\napproach using biodiversity publications, building upon our previous research.\nTo validate our pipeline, we employed two datasets of DL-related biodiversity\npublications: a curated set of 100 publications from our prior work and a set\nof 364 publications from the Ecological Informatics journal. Our results\ndemonstrate that the multi-LLM, RAG-assisted pipeline enhances the retrieval of\nDL methodological information, achieving an accuracy of 69.5% (417 out of 600\ncomparisons) based solely on textual content from publications. This\nperformance was assessed against human annotators who had access to code,\nfigures, tables, and other supplementary information. Although demonstrated in\nbiodiversity, our methodology is not limited to this field; it can be applied\nacross other scientific domains where detailed methodological reporting is\nessential for advancing knowledge and ensuring reproducibility. This study\npresents a scalable and reliable approach for automating information\nextraction, facilitating better reproducibility and knowledge transfer across\nstudies.",
      "tldr_zh": "这篇论文探讨了使用多个 Large Language Models (LLMs) 结合 Retrieval-Augmented Generation (RAG) 技术，从非结构化文本中自动提取 Deep Learning (DL) 方法细节的问题，针对生物多样性出版物进行案例研究。研究构建了一个基于五个开源 LLMs（如 Llama-3 70B 和 Mixtral-8x22B）的投票分类器，并测试了两个数据集，包括100个先前整理的出版物和364个来自Ecological Informatics期刊的出版物。结果显示，该多-LLM管道在文本内容提取上达到了69.5%的准确率，优于人类标注者，并证明了其可扩展性，可应用于其他科学领域以提升知识的可重复性和转移。",
      "categories": [
        "cs.IR",
        "cs.AI",
        "H.3.3; I.2.7"
      ],
      "primary_category": "cs.IR",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2411.09269v1",
      "published_date": "2024-11-14 08:12:36 UTC",
      "updated_date": "2024-11-14 08:12:36 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T00:34:31.437282"
    },
    {
      "arxiv_id": "2411.09266v1",
      "title": "How Good is ChatGPT at Audiovisual Deepfake Detection: A Comparative Study of ChatGPT, AI Models and Human Perception",
      "title_zh": "翻译失败",
      "authors": [
        "Sahibzada Adil Shahzad",
        "Ammarah Hashmi",
        "Yan-Tsung Peng",
        "Yu Tsao",
        "Hsin-Min Wang"
      ],
      "abstract": "Multimodal deepfakes involving audiovisual manipulations are a growing threat\nbecause they are difficult to detect with the naked eye or using unimodal deep\nlearningbased forgery detection methods. Audiovisual forensic models, while\nmore capable than unimodal models, require large training datasets and are\ncomputationally expensive for training and inference. Furthermore, these models\nlack interpretability and often do not generalize well to unseen manipulations.\nIn this study, we examine the detection capabilities of a large language model\n(LLM) (i.e., ChatGPT) to identify and account for any possible visual and\nauditory artifacts and manipulations in audiovisual deepfake content. Extensive\nexperiments are conducted on videos from a benchmark multimodal deepfake\ndataset to evaluate the detection performance of ChatGPT and compare it with\nthe detection capabilities of state-of-the-art multimodal forensic models and\nhumans. Experimental results demonstrate the importance of domain knowledge and\nprompt engineering for video forgery detection tasks using LLMs. Unlike\napproaches based on end-to-end learning, ChatGPT can account for spatial and\nspatiotemporal artifacts and inconsistencies that may exist within or across\nmodalities. Additionally, we discuss the limitations of ChatGPT for multimedia\nforensic tasks.",
      "tldr_zh": "该研究评估了 ChatGPT 在音频视觉深度伪造检测（Audiovisual Deepfake Detection）中的表现，通过实验与先进的多模态取证模型和人类感知进行比较。论文利用基准多模态深度伪造数据集，强调了领域知识和提示工程（prompt engineering）对大型语言模型（LLMs）检测任务的重要性。结果显示，ChatGPT 能有效识别空间和时空伪造痕迹，但不如端到端学习模型，且在泛化性和解释性方面存在局限性。该工作突出了 LLMs 在多媒体取证中的潜力，同时指出了其实际应用挑战。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.HC",
        "cs.LG",
        "cs.MM"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2411.09266v1",
      "published_date": "2024-11-14 08:07:02 UTC",
      "updated_date": "2024-11-14 08:07:02 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T00:34:41.667946"
    },
    {
      "arxiv_id": "2411.09261v2",
      "title": "Automating Autograding: Large Language Models as Test Suite Generators for Introductory Programming",
      "title_zh": "自动化自动评分：大语言模型作为入门级编程测试套件生成器",
      "authors": [
        "Umar Alkafaween",
        "Ibrahim Albluwi",
        "Paul Denny"
      ],
      "abstract": "Automatically graded programming assignments provide instant feedback to\nstudents and significantly reduce manual grading time for instructors. However,\ncreating comprehensive suites of test cases for programming problems within\nautomatic graders can be time-consuming and complex. The effort needed to\ndefine test suites may deter some instructors from creating additional problems\nor lead to inadequate test coverage, potentially resulting in misleading\nfeedback on student solutions. Such limitations may reduce student access to\nthe well-documented benefits of timely feedback when learning programming.\n  In this work, we evaluate the effectiveness of using Large Language Models\n(LLMs), as part of a larger workflow, to automatically generate test suites for\nCS1-level programming problems. Each problem's statement and reference solution\nare provided to GPT-4 to produce a test suite that can be used by an\nautograder. We evaluate our proposed approach using a sample of 26 problems,\nand more than 25,000 attempted solutions to those problems, submitted by\nstudents in an introductory programming course. We compare the performance of\nthe LLM-generated test suites against the instructor-created test suites for\neach problem. Our findings reveal that LLM-generated test suites can correctly\nidentify most valid solutions, and for most problems are at least as\ncomprehensive as the instructor test suites. Additionally, the LLM-generated\ntest suites exposed ambiguities in some problem statements, underscoring their\npotential to improve both autograding and instructional design.",
      "tldr_zh": "本研究探讨了使用Large Language Models (LLMs)自动生成测试套件，以简化入门级编程课程（CS1）的自动评分系统。研究方法涉及将问题语句和参考解决方案输入GPT-4，生成可用于autograder的测试套件，并通过26个编程问题和超过25,000个学生提交的解决方案进行评估。结果显示，LLM生成的测试套件能正确识别大多数有效解决方案，且在大多数情况下与教师创建的测试套件相当或更全面，还能揭示问题语句中的模糊性。总体而言，此方法有助于减少教师工作量，提升反馈质量和教学设计。",
      "categories": [
        "cs.CY",
        "cs.AI",
        "K.3.2; I.2.7"
      ],
      "primary_category": "cs.CY",
      "comment": "Submitted to Journal of Computer Assisted Learning; updated table\n  refs",
      "pdf_url": "http://arxiv.org/pdf/2411.09261v2",
      "published_date": "2024-11-14 07:58:44 UTC",
      "updated_date": "2024-11-18 06:41:26 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T00:34:54.112545"
    },
    {
      "arxiv_id": "2411.09251v1",
      "title": "Cross Space and Time: A Spatio-Temporal Unitized Model for Traffic Flow Forecasting",
      "title_zh": "翻译失败",
      "authors": [
        "Weilin Ruan",
        "Wenzhuo Wang",
        "Siru Zhong",
        "Wei Chen",
        "Li Liu",
        "Yuxuan Liang"
      ],
      "abstract": "Predicting spatio-temporal traffic flow presents significant challenges due\nto complex interactions between spatial and temporal factors. Existing\napproaches often address these dimensions in isolation, neglecting their\ncritical interdependencies. In this paper, we introduce the Spatio-Temporal\nUnitized Model (STUM), a unified framework designed to capture both spatial and\ntemporal dependencies while addressing spatio-temporal heterogeneity through\ntechniques such as distribution alignment and feature fusion. It also ensures\nboth predictive accuracy and computational efficiency. Central to STUM is the\nAdaptive Spatio-temporal Unitized Cell (ASTUC), which utilizes low-rank\nmatrices to seamlessly store, update, and interact with space, time, as well as\ntheir correlations. Our framework is also modular, allowing it to integrate\nwith various spatio-temporal graph neural networks through components such as\nbackbone models, feature extractors, residual fusion blocks, and predictive\nmodules to collectively enhance forecasting outcomes. Experimental results\nacross multiple real-world datasets demonstrate that STUM consistently improves\nprediction performance with minimal computational cost. These findings are\nfurther supported by hyperparameter optimization, pre-training analysis, and\nresult visualization. We provide our source code for reproducibility at\nhttps://anonymous.4open.science/r/STUM-E4F0.",
      "tldr_zh": "这篇论文提出了一种统一的框架Spatio-Temporal Unitized Model (STUM)，用于解决交通流量预测中空间和时间因素的复杂交互问题，通过分布对齐和特征融合技术来捕捉时空依赖性和异质性，同时确保预测准确性和计算效率。STUM的核心组件是Adaptive Spatio-temporal Unitized Cell (ASTUC)，它利用低秩矩阵无缝存储、更新和交互空间、时间及其相关性，并支持模块化设计以集成各种时空图神经网络。实验结果显示，STUM在多个真实数据集上显著提高了预测性能，同时保持最低计算成本，并通过超参数优化和可视化分析进一步验证了其有效性。",
      "categories": [
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2411.09251v1",
      "published_date": "2024-11-14 07:34:31 UTC",
      "updated_date": "2024-11-14 07:34:31 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T00:35:07.005944"
    },
    {
      "arxiv_id": "2411.09249v1",
      "title": "Enhancing Financial Domain Adaptation of Language Models via Model Augmentation",
      "title_zh": "通过模型增强提升语言模型的金融领域适应性",
      "authors": [
        "Kota Tanabe",
        "Masanori Hirano",
        "Kazuki Matoya",
        "Kentaro Imajo",
        "Hiroki Sakaji",
        "Itsuki Noda"
      ],
      "abstract": "The domain adaptation of language models, including large language models\n(LLMs), has become increasingly important as the use of such models continues\nto expand. This study demonstrates the effectiveness of Composition to Augment\nLanguage Models (CALM) in adapting to the financial domain. CALM is a model to\nextend the capabilities of existing models by introducing cross-attention\nbetween two LLMs with different functions. In our experiments, we developed a\nCALM to enhance the financial performance of an LLM with strong response\ncapabilities by leveraging a financial-specialized LLM. Notably, the CALM was\ntrained using a financial dataset different from the one used to train the\nfinancial-specialized LLM, confirming CALM's ability to adapt to various\ndatasets. The models were evaluated through quantitative Japanese financial\nbenchmarks and qualitative response comparisons, demonstrating that CALM\nenables superior responses with higher scores than the original models and\nbaselines. Additionally, comparative experiments on connection points revealed\nthat connecting the middle layers of the models is most effective in\nfacilitating adaptation to the financial domain. These findings confirm that\nCALM is a practical approach for adapting LLMs to the financial domain.",
      "tldr_zh": "本文提出CALM（Composition to Augment Language Models）框架，通过在两个不同功能的LLMs之间引入cross-attention机制，来提升语言模型在金融领域的适应能力。实验中，研究者将一个响应能力强的LLM与金融专业LLM结合，使用不同于训练金融专业LLM的数据集进行训练，验证了CALM对各种金融数据集的灵活适应性。定量评估（如日本金融基准）和定性比较显示，CALM的表现优于原模型和基线模型，尤其是在模型中间层连接时效果最佳。这些发现证明了CALM是一种实用的方法，用于LLMs的金融领域适应。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2411.09249v1",
      "published_date": "2024-11-14 07:28:09 UTC",
      "updated_date": "2024-11-14 07:28:09 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T00:35:18.217767"
    },
    {
      "arxiv_id": "2411.09243v1",
      "title": "Towards Unified Neural Decoding of Perceived, Spoken and Imagined Speech from EEG Signals",
      "title_zh": "翻译失败",
      "authors": [
        "Jung-Sun Lee",
        "Ha-Na Jo",
        "Seo-Hyun Lee"
      ],
      "abstract": "Brain signals accompany various information relevant to human actions and\nmental imagery, making them crucial to interpreting and understanding human\nintentions. Brain-computer interface technology leverages this brain activity\nto generate external commands for controlling the environment, offering\ncritical advantages to individuals with paralysis or locked-in syndrome. Within\nthe brain-computer interface domain, brain-to-speech research has gained\nattention, focusing on the direct synthesis of audible speech from brain\nsignals. Most current studies decode speech from brain activity using invasive\ntechniques and emphasize spoken speech data. However, humans express various\nspeech states, and distinguishing these states through non-invasive approaches\nremains a significant yet challenging task. This research investigated the\neffectiveness of deep learning models for non-invasive-based neural signal\ndecoding, with an emphasis on distinguishing between different speech\nparadigms, including perceived, overt, whispered, and imagined speech, across\nmultiple frequency bands. The model utilizing the spatial conventional neural\nnetwork module demonstrated superior performance compared to other models,\nespecially in the gamma band. Additionally, imagined speech in the theta\nfrequency band, where deep learning also showed strong effects, exhibited\nstatistically significant differences compared to the other speech paradigms.",
      "tldr_zh": "本研究旨在通过非侵入性 EEG 信号，实现对感知的、口头、低声和想象的语音范式的统一神经解码，解决脑机接口（Brain-Computer Interface）中区分不同语音状态的挑战。研究采用深度学习模型，特别是 spatial conventional neural network 模块，在多个频率带上进行分析，结果显示该模型在 gamma band 中表现出色。实验还发现，想象语音在 theta band 中与其它范式存在统计显著差异。该方法为脑到语音合成提供新途径，有助于改善瘫痪或闭锁综合征患者的沟通能力。",
      "categories": [
        "cs.AI",
        "cs.SD",
        "eess.AS"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2411.09243v1",
      "published_date": "2024-11-14 07:20:08 UTC",
      "updated_date": "2024-11-14 07:20:08 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T00:35:32.265938"
    },
    {
      "arxiv_id": "2411.09224v1",
      "title": "Programming with AI: Evaluating ChatGPT, Gemini, AlphaCode, and GitHub Copilot for Programmers",
      "title_zh": "翻译失败",
      "authors": [
        "Md Kamrul Siam",
        "Huanying Gu",
        "Jerry Q. Cheng"
      ],
      "abstract": "Our everyday lives now heavily rely on artificial intelligence (AI) powered\nlarge language models (LLMs). Like regular users, programmers are also\nbenefiting from the newest large language models. In response to the critical\nrole that AI models play in modern software development, this study presents a\nthorough evaluation of leading programming assistants, including ChatGPT,\nGemini(Bard AI), AlphaCode, and GitHub Copilot. The evaluation is based on\ntasks like natural language processing and code generation accuracy in\ndifferent programming languages like Java, Python and C++. Based on the\nresults, it has emphasized their strengths and weaknesses and the importance of\nfurther modifications to increase the reliability and accuracy of the latest\npopular models. Although these AI assistants illustrate a high level of\nprogress in language understanding and code generation, along with ethical\nconsiderations and responsible usage, they provoke a necessity for discussion.\nWith time, developing more refined AI technology is essential for achieving\nadvanced solutions in various fields, especially with the knowledge of the\nfeature intricacies of these models and their implications. This study offers a\ncomparison of different LLMs and provides essential feedback on the rapidly\nchanging area of AI models. It also emphasizes the need for ethical\ndevelopmental practices to actualize AI models' full potential.",
      "tldr_zh": "本研究评估了ChatGPT、Gemini、AlphaCode和GitHub Copilot等AI编程助手在软件开发中的表现，重点测试了它们在自然语言处理和代码生成准确性方面的能力，涵盖Java、Python和C++等语言。\n结果显示，这些LLMs模型在语言理解和代码生成上表现出色，但也暴露了弱点，如准确性和可靠性不足，因此需要进一步优化。\n论文强调了伦理考虑和负责任使用的重要性，并呼吁通过持续改进实现AI技术的潜力，以推动编程领域的进步。",
      "categories": [
        "cs.SE",
        "cs.AI"
      ],
      "primary_category": "cs.SE",
      "comment": "8 pages",
      "pdf_url": "http://arxiv.org/pdf/2411.09224v1",
      "published_date": "2024-11-14 06:40:55 UTC",
      "updated_date": "2024-11-14 06:40:55 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T00:35:42.573982"
    },
    {
      "arxiv_id": "2411.09718v1",
      "title": "NFRs in Medical Imaging",
      "title_zh": "医疗成像中的非功能性需求",
      "authors": [
        "Amanda Vallentin"
      ],
      "abstract": "The diagnostic imaging departments are under great pressure due to a growing\nworkload. The number of required scans is growing and there is a shortage of\nqualified labor. AI solutions for medical imaging applications have shown great\npotential. However, very few diagnostic imaging models have been approved for\nhospital use and even fewer are being implemented at the hospitals. The most\ncommon reason why software projects fail is poor requirement engineering,\nespecially non-functional requirements (NFRs) can be detrimental to a project.\nResearch shows that machine learning professionals struggle to work with NFRs\nand that there is a need to adapt NFR frameworks to machine learning, AI-based,\nsoftware. This study uses qualitative methods to interact with key stakeholders\nto identify which types of NFRs are important for medical imaging applications.\nThe study was done on a single Danish hospital and found that NFRs of type\nEfficiency, Accuracy, Interoperability, Reliability, Usability, Adaptability,\nand Fairness were important to the stakeholders. Especially Efficiency since\nthe diagnostic imaging department is trying to spend as little time as possible\non each scan.",
      "tldr_zh": "诊断影像部门面临工作量增加和劳动力短缺的压力，导致AI应用潜力虽大，但实际实施有限，主要由于需求工程问题，尤其是非功能性需求(NFRs)的处理不当。本研究采用定性方法，与丹麦一家医院的关键利益相关者互动，识别了医疗影像应用中重要的NFRs类型，包括Efficiency、Accuracy、Interoperability、Reliability、Usability、Adaptability和Fairness，其中Efficiency尤为关键，以减少每次扫描的时间。研究结果强调了需要针对AI软件适应NFRs框架，以提升AI在医疗影像中的可行性和成功率。",
      "categories": [
        "cs.SE",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.SE",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2411.09718v1",
      "published_date": "2024-11-14 06:39:56 UTC",
      "updated_date": "2024-11-14 06:39:56 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T00:35:54.532380"
    },
    {
      "arxiv_id": "2411.09220v1",
      "title": "Transferable Adversarial Attacks against ASR",
      "title_zh": "针对 ASR 的可转移对抗攻击",
      "authors": [
        "Xiaoxue Gao",
        "Zexin Li",
        "Yiming Chen",
        "Cong Liu",
        "Haizhou Li"
      ],
      "abstract": "Given the extensive research and real-world applications of automatic speech\nrecognition (ASR), ensuring the robustness of ASR models against minor input\nperturbations becomes a crucial consideration for maintaining their\neffectiveness in real-time scenarios. Previous explorations into ASR model\nrobustness have predominantly revolved around evaluating accuracy on white-box\nsettings with full access to ASR models. Nevertheless, full ASR model details\nare often not available in real-world applications. Therefore, evaluating the\nrobustness of black-box ASR models is essential for a comprehensive\nunderstanding of ASR model resilience. In this regard, we thoroughly study the\nvulnerability of practical black-box attacks in cutting-edge ASR models and\npropose to employ two advanced time-domain-based transferable attacks alongside\nour differentiable feature extractor. We also propose a speech-aware gradient\noptimization approach (SAGO) for ASR, which forces mistranscription with\nminimal impact on human imperceptibility through voice activity detection rule\nand a speech-aware gradient-oriented optimizer. Our comprehensive experimental\nresults reveal performance enhancements compared to baseline approaches across\nfive models on two databases.",
      "tldr_zh": "本文探讨了黑盒ASR模型对对抗性攻击的脆弱性，强调在实际应用中模型细节不可用时的鲁棒性评估需求。研究提出两种基于时间域的transferable attacks，并结合可微特征提取器和speech-aware gradient optimization approach (SAGO)，通过语音活动检测和梯度优化实现强制错误转录，同时保持对人类感知的最小影响。实验结果显示，该方法在五个ASR模型和两个数据库上比基线方法性能显著提升，为提升ASR模型的实际安全性提供了新见解。",
      "categories": [
        "eess.AS",
        "cs.AI",
        "eess.SP"
      ],
      "primary_category": "eess.AS",
      "comment": "IEEE SPL",
      "pdf_url": "http://arxiv.org/pdf/2411.09220v1",
      "published_date": "2024-11-14 06:32:31 UTC",
      "updated_date": "2024-11-14 06:32:31 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T00:36:06.398678"
    },
    {
      "arxiv_id": "2411.09213v1",
      "title": "Comprehensive and Practical Evaluation of Retrieval-Augmented Generation Systems for Medical Question Answering",
      "title_zh": "全面而实用的检索增强生成系统在医疗问答中的评估",
      "authors": [
        "Nghia Trung Ngo",
        "Chien Van Nguyen",
        "Franck Dernoncourt",
        "Thien Huu Nguyen"
      ],
      "abstract": "Retrieval-augmented generation (RAG) has emerged as a promising approach to\nenhance the performance of large language models (LLMs) in knowledge-intensive\ntasks such as those from medical domain. However, the sensitive nature of the\nmedical domain necessitates a completely accurate and trustworthy system. While\nexisting RAG benchmarks primarily focus on the standard retrieve-answer\nsetting, they overlook many practical scenarios that measure crucial aspects of\na reliable medical system. This paper addresses this gap by providing a\ncomprehensive evaluation framework for medical question-answering (QA) systems\nin a RAG setting for these situations, including sufficiency, integration, and\nrobustness. We introduce Medical Retrieval-Augmented Generation Benchmark\n(MedRGB) that provides various supplementary elements to four medical QA\ndatasets for testing LLMs' ability to handle these specific scenarios.\nUtilizing MedRGB, we conduct extensive evaluations of both state-of-the-art\ncommercial LLMs and open-source models across multiple retrieval conditions.\nOur experimental results reveals current models' limited ability to handle\nnoise and misinformation in the retrieved documents. We further analyze the\nLLMs' reasoning processes to provides valuable insights and future directions\nfor developing RAG systems in this critical medical domain.",
      "tldr_zh": "这篇论文针对医疗问答领域的检索增强生成(RAG)系统，提出一个全面的评估框架，以评估大型语言模型(LLMs)在实际场景中的性能，包括sufficiency（充分性）、integration（整合性）和robustness（鲁棒性）。\n他们引入了Medical Retrieval-Augmented Generation Benchmark (MedRGB)，在四个医疗QA数据集基础上添加了补充元素，用于测试模型在多种检索条件下的表现。\n实验结果显示，当前商用和开源模型在处理检索文档中的噪声和错误信息时能力有限，并通过分析LLMs的推理过程提供了宝贵见解和未来改进方向。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.IR"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2411.09213v1",
      "published_date": "2024-11-14 06:19:18 UTC",
      "updated_date": "2024-11-14 06:19:18 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T00:36:18.601936"
    },
    {
      "arxiv_id": "2411.09211v1",
      "title": "Dynamic Neural Communication: Convergence of Computer Vision and Brain-Computer Interface",
      "title_zh": "动态神经通信：计算机视觉和脑机",
      "authors": [
        "Ji-Ha Park",
        "Seo-Hyun Lee",
        "Soowon Kim",
        "Seong-Whan Lee"
      ],
      "abstract": "Interpreting human neural signals to decode static speech intentions such as\ntext or images and dynamic speech intentions such as audio or video is showing\ngreat potential as an innovative communication tool. Human communication\naccompanies various features, such as articulatory movements, facial\nexpressions, and internal speech, all of which are reflected in neural signals.\nHowever, most studies only generate short or fragmented outputs, while\nproviding informative communication by leveraging various features from neural\nsignals remains challenging. In this study, we introduce a dynamic neural\ncommunication method that leverages current computer vision and brain-computer\ninterface technologies. Our approach captures the user's intentions from neural\nsignals and decodes visemes in short time steps to produce dynamic visual\noutputs. The results demonstrate the potential to rapidly capture and\nreconstruct lip movements during natural speech attempts from human neural\nsignals, enabling dynamic neural communication through the convergence of\ncomputer vision and brain--computer interface.",
      "tldr_zh": "该研究探讨了利用神经信号解码静态（如文本或图像）和动态（如音频或视频）语音意图的问题，强调现有方法仅生成短片段输出，而未能充分利用神经信号中的发音动作、面部表情等特征。研究提出了一种动态神经通信方法，结合 computer vision 和 brain-computer interface 技术，从神经信号捕获用户意图，并在短时间步骤中解码 visemes 以产生动态视觉输出。结果表明，该方法能快速捕获并重建自然语音尝试中的唇部运动，从而实现计算机视觉与脑机接口的融合，推动信息性神经通信的进展。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "4 pages, 2 figures, 1 table, Name of Conference: International\n  Conference on Brain-Computer Interface",
      "pdf_url": "http://arxiv.org/pdf/2411.09211v1",
      "published_date": "2024-11-14 06:15:05 UTC",
      "updated_date": "2024-11-14 06:15:05 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T00:36:30.226934"
    },
    {
      "arxiv_id": "2411.09204v1",
      "title": "RibCageImp: A Deep Learning Framework for 3D Ribcage Implant Generation",
      "title_zh": "RibCageImp：用于3D 肋骨笼植入物生成的深度学习框架",
      "authors": [
        "Gyanendra Chaubey",
        "Aiman Farooq",
        "Azad Singh",
        "Deepak Mishra"
      ],
      "abstract": "The recovery of damaged or resected ribcage structures requires precise,\ncustom-designed implants to restore the integrity and functionality of the\nthoracic cavity. Traditional implant design methods rely mainly on manual\nprocesses, making them time-consuming and susceptible to variability. In this\nwork, we explore the feasibility of automated ribcage implant generation using\ndeep learning. We present a framework based on 3D U-Net architecture that\nprocesses CT scans to generate patient-specific implant designs. To the best of\nour knowledge, this is the first investigation into automated thoracic implant\ngeneration using deep learning approaches. Our preliminary results, while\nmoderate, highlight both the potential and the significant challenges in this\ncomplex domain. These findings establish a foundation for future research in\nautomated ribcage reconstruction and identify key technical challenges that\nneed to be addressed for practical implementation.",
      "tldr_zh": "该研究针对受损肋骨结构的修复问题，提出RibCageImp框架，使用深度学习自动生成患者特定的3D肋骨植入物，以克服传统手动设计的耗时和变异性。框架基于3D U-Net架构，处理CT scans来创建精确的植入物设计，这是首次探索深度学习在胸部植入物生成中的应用。尽管初步结果显示潜力，但也突出了领域挑战，为未来的自动肋骨重建研究奠定基础并标识关键技术问题。",
      "categories": [
        "eess.IV",
        "cs.AI",
        "physics.med-ph"
      ],
      "primary_category": "eess.IV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2411.09204v1",
      "published_date": "2024-11-14 06:03:54 UTC",
      "updated_date": "2024-11-14 06:03:54 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T00:36:41.293706"
    },
    {
      "arxiv_id": "2411.09200v1",
      "title": "Advancing Software Security and Reliability in Cloud Platforms through AI-based Anomaly Detection",
      "title_zh": "翻译失败",
      "authors": [
        "Sabbir M. Saleh",
        "Ibrahim Mohammed Sayem",
        "Nazim Madhavji",
        "John Steinbacher"
      ],
      "abstract": "Continuous Integration/Continuous Deployment (CI/CD) is fundamental for\nadvanced software development, supporting faster and more efficient delivery of\ncode changes into cloud environments. However, security issues in the CI/CD\npipeline remain challenging, and incidents (e.g., DDoS, Bot, Log4j, etc.) are\nhappening over the cloud environments. While plenty of literature discusses\nstatic security testing and CI/CD practices, only a few deal with network\ntraffic pattern analysis to detect different cyberattacks. This research aims\nto enhance CI/CD pipeline security by implementing anomaly detection through AI\n(Artificial Intelligence) support. The goal is to identify unusual behaviour or\nvariations from network traffic patterns in pipeline and cloud platforms. The\nsystem shall integrate into the workflow to continuously monitor pipeline\nactivities and cloud infrastructure. Additionally, it aims to explore adaptive\nresponse mechanisms to mitigate the detected anomalies or security threats.\nThis research employed two popular network traffic datasets, CSE-CIC-IDS2018\nand CSE-CIC-IDS2017. We implemented a combination of Convolution Neural\nNetwork(CNN) and Long Short-Term Memory (LSTM) to detect unusual traffic\npatterns. We achieved an accuracy of 98.69% and 98.30% and generated log files\nin different CI/CD pipeline stages that resemble the network anomalies affected\nto address security challenges in modern DevOps practices, contributing to\nadvancing software security and reliability.",
      "tldr_zh": "这篇论文针对 CI/CD 管道在云平台中的安全挑战（如 DDoS 和 Bot 攻击），提出通过 AI-based Anomaly Detection 提升软件安全和可靠性。研究方法结合 CNN 和 LSTM 模型分析网络流量模式，使用 CSE-CIC-IDS2018 和 CSE-CIC-IDS2017 数据集，实现了 98.69% 和 98.30% 的检测准确率，并生成日志文件以监控异常。最终，该系统整合自适应响应机制，增强 CI/CD 工作流的安全性，并为现代 DevOps 实践提供重要贡献。",
      "categories": [
        "cs.SE",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.SE",
      "comment": "10 pages",
      "pdf_url": "http://arxiv.org/pdf/2411.09200v1",
      "published_date": "2024-11-14 05:45:55 UTC",
      "updated_date": "2024-11-14 05:45:55 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T00:36:54.677815"
    },
    {
      "arxiv_id": "2411.09189v2",
      "title": "Improvement and Implementation of a Speech Emotion Recognition Model Based on Dual-Layer LSTM",
      "title_zh": "基于双层 LSTM 的语音情感识别模型改进与实现",
      "authors": [
        "Xiaoran Yang",
        "Shuhan Yu",
        "Wenxi Xu"
      ],
      "abstract": "This paper builds upon an existing speech emotion recognition model by adding\nan additional LSTM layer to improve the accuracy and processing efficiency of\nemotion recognition from audio data. By capturing the long-term dependencies\nwithin audio sequences through a dual-layer LSTM network, the model can\nrecognize and classify complex emotional patterns more accurately. Experiments\nconducted on the RAVDESS dataset validated this approach, showing that the\nmodified dual layer LSTM model improves accuracy by 2% compared to the\nsingle-layer LSTM while significantly reducing recognition latency, thereby\nenhancing real-time performance. These results indicate that the dual-layer\nLSTM architecture is highly suitable for handling emotional features with\nlong-term dependencies, providing a viable optimization for speech emotion\nrecognition systems. This research provides a reference for practical\napplications in fields like intelligent customer service, sentiment analysis\nand human-computer interaction.",
      "tldr_zh": "本文基于现有语音情感识别模型，通过添加一个额外的 LSTM 层，构建了双层 LSTM 网络，以更好地捕获音频序列中的长期依赖关系，从而提高情感识别的准确性和处理效率。在 RAVDESS 数据集上的实验显示，该改进模型相比单层 LSTM 准确率提升了 2%，并显著降低了识别延迟，增强了实时性能。研究结果表明，双层 LSTM 架构适用于处理复杂情感特征，并为智能客服、情感分析和人机交互等领域提供实用优化参考。",
      "categories": [
        "cs.AI",
        "cs.SD",
        "eess.AS"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2411.09189v2",
      "published_date": "2024-11-14 05:05:36 UTC",
      "updated_date": "2024-11-28 19:28:50 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T00:37:05.950852"
    },
    {
      "arxiv_id": "2411.09184v1",
      "title": "Dynamic technology impact analysis: A multi-task learning approach to patent citation prediction",
      "title_zh": "动态技术影响分析：一种多任务学习方法用于专利引用预测",
      "authors": [
        "Youngjin Seol",
        "Jaewoong Choi",
        "Seunghyun Lee",
        "Janghyeok Yoon"
      ],
      "abstract": "Machine learning (ML) models are valuable tools for analyzing the impact of\ntechnology using patent citation information. However, existing ML-based\nmethods often struggle to account for the dynamic nature of the technology\nimpact over time and the interdependencies of these impacts across different\nperiods. This study proposes a multi-task learning (MTL) approach to enhance\nthe prediction of technology impact across various time frames by leveraging\nknowledge sharing and simultaneously monitoring the evolution of technology\nimpact. First, we quantify the technology impacts and identify patterns through\ncitation analysis over distinct time periods. Next, we develop MTL models to\npredict citation counts using multiple patent indicators over time. Finally, we\nexamine the changes in key input indicators and their patterns over different\nperiods using the SHapley Additive exPlanation method. We also offer guidelines\nfor validating and interpreting the results by employing statistical methods\nand natural language processing techniques. A case study on battery\ntechnologies demonstrates that our approach not only deepens the understanding\nof technology impact, but also improves prediction accuracy, yielding valuable\ninsights for both academia and industry.",
      "tldr_zh": "这篇论文提出了一种基于多任务学习(MTL)的方法，用于动态分析技术影响，通过预测专利引用来解决现有机器学习模型忽略时间动态性和相互依赖的问题。研究首先通过引用分析量化技术影响并识别模式，然后开发MTL模型利用多个专利指标同时预测不同时间段的引用次数，并采用SHapley Additive exPlanation (SHAP)方法分析关键指标的变化。案例研究显示，该方法在电池技术领域显著提高了预测准确性，并为学术和工业提供更深入的技术影响洞见和验证指南。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2411.09184v1",
      "published_date": "2024-11-14 04:46:08 UTC",
      "updated_date": "2024-11-14 04:46:08 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T00:37:18.251637"
    },
    {
      "arxiv_id": "2411.09181v1",
      "title": "DeBaTeR: Denoising Bipartite Temporal Graph for Recommendation",
      "title_zh": "翻译失败",
      "authors": [
        "Xinyu He",
        "Jose Sepulveda",
        "Mostafa Rahmani",
        "Alyssa Woo",
        "Fei Wang",
        "Hanghang Tong"
      ],
      "abstract": "Due to the difficulty of acquiring large-scale explicit user feedback,\nimplicit feedback (e.g., clicks or other interactions) is widely applied as an\nalternative source of data, where user-item interactions can be modeled as a\nbipartite graph. Due to the noisy and biased nature of implicit real-world\nuser-item interactions, identifying and rectifying noisy interactions are vital\nto enhance model performance and robustness. Previous works on purifying\nuser-item interactions in collaborative filtering mainly focus on mining the\ncorrelation between user/item embeddings and noisy interactions, neglecting the\nbenefit of temporal patterns in determining noisy interactions. Time\ninformation, while enhancing the model utility, also bears its natural\nadvantage in helping to determine noisy edges, e.g., if someone usually watches\nhorror movies at night and talk shows in the morning, a record of watching a\nhorror movie in the morning is more likely to be noisy interaction. Armed with\nthis observation, we introduce a simple yet effective mechanism for generating\ntime-aware user/item embeddings and propose two strategies for denoising\nbipartite temporal graph in recommender systems (DeBaTeR): the first is through\nreweighting the adjacency matrix (DeBaTeR-A), where a reliability score is\ndefined to reweight the edges through both soft assignment and hard assignment;\nthe second is through reweighting the loss function (DeBaTeR-L), where weights\nare generated to reweight user-item samples in the losses. Extensive\nexperiments have been conducted to demonstrate the efficacy of our methods and\nillustrate how time information indeed helps identifying noisy edges.",
      "tldr_zh": "该论文针对隐式反馈（如点击）构建的用户-物品二部图(bipartite graph)中的噪音和偏差问题，提出DeBaTeR框架，利用时间信息来识别和修正噪音交互。DeBaTeR通过生成时间感知的用户/物品嵌入，并引入两种策略：DeBaTeR-A（通过重新加权邻接矩阵并定义可靠性分数进行软/硬分配）和DeBaTeR-L（通过重新加权损失函数为用户-物品样本分配权重），从而提升推荐系统的鲁棒性。实验结果证明，该方法显著提高了模型性能，并展示了时间信息在识别噪音边方面的有效性。",
      "categories": [
        "cs.IR",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.IR",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2411.09181v1",
      "published_date": "2024-11-14 04:39:30 UTC",
      "updated_date": "2024-11-14 04:39:30 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T00:37:30.334355"
    },
    {
      "arxiv_id": "2411.09180v1",
      "title": "LEAP:D -- A Novel Prompt-based Approach for Domain-Generalized Aerial Object Detection",
      "title_zh": "翻译失败",
      "authors": [
        "Chanyeong Park",
        "Heegwang Kim",
        "Joonki Paik"
      ],
      "abstract": "Drone-captured images present significant challenges in object detection due\nto varying shooting conditions, which can alter object appearance and shape.\nFactors such as drone altitude, angle, and weather cause these variations,\ninfluencing the performance of object detection algorithms. To tackle these\nchallenges, we introduce an innovative vision-language approach using learnable\nprompts. This shift from conventional manual prompts aims to reduce\ndomain-specific knowledge interference, ultimately improving object detection\ncapabilities. Furthermore, we streamline the training process with a one-step\napproach, updating the learnable prompt concurrently with model training,\nenhancing efficiency without compromising performance. Our study contributes to\ndomain-generalized object detection by leveraging learnable prompts and\noptimizing training processes. This enhances model robustness and adaptability\nacross diverse environments, leading to more effective aerial object detection.",
      "tldr_zh": "该研究针对无人机捕获图像中物体检测的挑战（如高度、角度和天气变化导致的物体外观变形），提出了一种创新的视觉语言方法LEAP:D，利用可学习的提示（learnable prompts）来减少领域特定知识的干扰。不同于传统的手动提示，该方法通过一步训练过程，同时更新提示和模型参数，提高了训练效率，同时保持性能。整体贡献在于提升了领域泛化（domain-generalized）空中物体检测的鲁棒性和适应性，使模型在多样化环境中更有效地工作。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "ICIP 2024 Workshop accepted paper",
      "pdf_url": "http://arxiv.org/pdf/2411.09180v1",
      "published_date": "2024-11-14 04:39:10 UTC",
      "updated_date": "2024-11-14 04:39:10 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T00:37:42.249723"
    },
    {
      "arxiv_id": "2411.09176v3",
      "title": "Gazing at Rewards: Eye Movements as a Lens into Human and AI Decision-Making in Hybrid Visual Foraging",
      "title_zh": "翻译失败",
      "authors": [
        "Bo Wang",
        "Dingwei Tan",
        "Yen-Ling Kuo",
        "Zhaowei Sun",
        "Jeremy M. Wolfe",
        "Tat-Jen Cham",
        "Mengmi Zhang"
      ],
      "abstract": "Imagine searching a collection of coins for quarters ($0.25$), dimes\n($0.10$), nickels ($0.05$), and pennies ($0.01$)-a hybrid foraging task where\nobservers look for multiple instances of multiple target types. In such tasks,\nhow do target values and their prevalence influence foraging and eye movement\nbehaviors (e.g., should you prioritize rare quarters or common nickels)? To\nexplore this, we conducted human psychophysics experiments, revealing that\nhumans are proficient reward foragers. Their eye fixations are drawn to regions\nwith higher average rewards, fixation durations are longer on more valuable\ntargets, and their cumulative rewards exceed chance, approaching the upper\nbound of optimal foragers. To probe these decision-making processes of humans,\nwe developed a transformer-based Visual Forager (VF) model trained via\nreinforcement learning. Our VF model takes a series of targets, their\ncorresponding values, and the search image as inputs, processes the images\nusing foveated vision, and produces a sequence of eye movements along with\ndecisions on whether to collect each fixated item. Our model outperforms all\nbaselines, achieves cumulative rewards comparable to those of humans, and\napproximates human foraging behavior in eye movements and foraging biases\nwithin time-limited environments. Furthermore, stress tests on\nout-of-distribution tasks with novel targets, unseen values, and varying set\nsizes demonstrate the VF model's effective generalization. Our work offers\nvaluable insights into the relationship between eye movements and\ndecision-making, with our model serving as a powerful tool for further\nexploration of this connection. All data, code, and models are available at\nhttps://github.com/ZhangLab-DeepNeuroCogLab/visual-forager.",
      "tldr_zh": "本文研究了在混合视觉搜寻任务中，目标价值（如 quarters 和 dimes）和出现频率如何影响人类的眼动（eye movements）和决策行为，通过人类心理物理实验发现，人类眼动偏向高奖励区域，对高价值目标的fixation 持续时间更长，且累计奖励接近最优水平。作者开发了一个基于 transformer 的 Visual Forager (VF) 模型，使用 reinforcement learning 训练，该模型输入目标序列、价值和搜索图像，通过 foveated vision 处理，输出眼动序列和收集决策，并优于基线模型，在模拟人类行为方面表现出色。进一步的压力测试显示，VF 模型在分布外任务（如新目标、未见价值和不同集大小）中具有有效泛化能力，为探索眼动与决策关系的机制提供了宝贵工具。所有数据、代码和模型已在 GitHub 上开源。",
      "categories": [
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2411.09176v3",
      "published_date": "2024-11-14 04:29:07 UTC",
      "updated_date": "2025-03-23 09:34:04 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T00:37:54.948905"
    },
    {
      "arxiv_id": "2411.09174v1",
      "title": "Advancing Diffusion Models: Alias-Free Resampling and Enhanced Rotational Equivariance",
      "title_zh": "翻译失败",
      "authors": [
        "Md Fahim Anjum"
      ],
      "abstract": "Recent advances in image generation, particularly via diffusion models, have\nled to impressive improvements in image synthesis quality. Despite this,\ndiffusion models are still challenged by model-induced artifacts and limited\nstability in image fidelity. In this work, we hypothesize that the primary\ncause of this issue is the improper resampling operation that introduces\naliasing in the diffusion model and a careful alias-free resampling dictated by\nimage processing theory can improve the model's performance in image synthesis.\nWe propose the integration of alias-free resampling layers into the UNet\narchitecture of diffusion models without adding extra trainable parameters,\nthereby maintaining computational efficiency. We then assess whether these\ntheory-driven modifications enhance image quality and rotational equivariance.\nOur experimental results on benchmark datasets, including CIFAR-10, MNIST, and\nMNIST-M, reveal consistent gains in image quality, particularly in terms of FID\nand KID scores. Furthermore, we propose a modified diffusion process that\nenables user-controlled rotation of generated images without requiring\nadditional training. Our findings highlight the potential of theory-driven\nenhancements such as alias-free resampling in generative models to improve\nimage quality while maintaining model efficiency and pioneer future research\ndirections to incorporate them into video-generating diffusion models, enabling\ndeeper exploration of the applications of alias-free resampling in generative\nmodeling.",
      "tldr_zh": "本研究针对扩散模型(diffusion models)在图像生成中的伪像(model-induced artifacts)和图像保真度稳定性问题，假设不当的重采样操作导致别名(aliasing)，并提出在UNet架构中集成alias-free resampling层，以提升模型性能且不增加可训练参数。实验结果显示，该方法在CIFAR-10、MNIST和MNIST-M数据集上显著改善图像质量，包括FID和KID分数的提升，同时增强了旋转等变性(rotational equivariance)。此外，作者引入一个修改的扩散过程，允许用户控制生成图像的旋转而不需额外训练，为理论驱动的生成模型优化开辟新方向，如扩展到视频生成应用。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG",
        "eess.IV"
      ],
      "primary_category": "cs.CV",
      "comment": "13 pages, 7 figures",
      "pdf_url": "http://arxiv.org/pdf/2411.09174v1",
      "published_date": "2024-11-14 04:23:28 UTC",
      "updated_date": "2024-11-14 04:23:28 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T00:38:06.905926"
    },
    {
      "arxiv_id": "2411.09170v1",
      "title": "Towards Scalable Handwriting Communication via EEG Decoding and Latent Embedding Integration",
      "title_zh": "翻译失败",
      "authors": [
        "Jun-Young Kim",
        "Deok-Seon Kim",
        "Seo-Hyun Lee"
      ],
      "abstract": "In recent years, brain-computer interfaces have made advances in decoding\nvarious motor-related tasks, including gesture recognition and movement\nclassification, utilizing electroencephalogram (EEG) data. These developments\nare fundamental in exploring how neural signals can be interpreted to recognize\nspecific physical actions. This study centers on a written alphabet\nclassification task, where we aim to decode EEG signals associated with\nhandwriting. To achieve this, we incorporate hand kinematics to guide the\nextraction of the consistent embeddings from high-dimensional neural recordings\nusing auxiliary variables (CEBRA). These CEBRA embeddings, along with the EEG,\nare processed by a parallel convolutional neural network model that extracts\nfeatures from both data sources simultaneously. The model classifies nine\ndifferent handwritten characters, including symbols such as exclamation marks\nand commas, within the alphabet. We evaluate the model using a quantitative\nfive-fold cross-validation approach and explore the structure of the embedding\nspace through visualizations. Our approach achieves a classification accuracy\nof 91 % for the nine-class task, demonstrating the feasibility of fine-grained\nhandwriting decoding from EEG.",
      "tldr_zh": "这篇论文探讨了通过脑电图(EEG)解码和潜在嵌入(CEBRA)整合，实现可扩展的手写通信，专注于分类手写字母。研究方法利用手部运动学指导CEBRA嵌入的提取，并采用并行卷积神经网络(CNN)模型同时处理EEG数据和嵌入，以识别九个不同字符，包括感叹号和逗号。实验通过五折交叉验证评估，模型在九类任务中达到91%的分类准确率，并通过嵌入空间可视化验证了精细手写解码的可行性。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "4 pages, 2 figures, 1 table, Name of Conference: International\n  Conference on Brain-Computer Interface",
      "pdf_url": "http://arxiv.org/pdf/2411.09170v1",
      "published_date": "2024-11-14 04:12:47 UTC",
      "updated_date": "2024-11-14 04:12:47 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T00:39:41.677812"
    },
    {
      "arxiv_id": "2411.09169v1",
      "title": "Artificial Theory of Mind and Self-Guided Social Organisation",
      "title_zh": "人工心智理论和自我引导的社会组织",
      "authors": [
        "Michael S. Harré",
        "Jaime Ruiz-Serra",
        "Catherine Drysdale"
      ],
      "abstract": "One of the challenges artificial intelligence (AI) faces is how a collection\nof agents coordinate their behaviour to achieve goals that are not reachable by\nany single agent. In a recent article by Ozmen et al this was framed as one of\nsix grand challenges: That AI needs to respect human cognitive processes at the\nhuman-AI interaction frontier. We suggest that this extends to the AI-AI\nfrontier and that it should also reflect human psychology, as it is the only\nsuccessful framework we have from which to build out. In this extended abstract\nwe first make the case for collective intelligence in a general setting,\ndrawing on recent work from single neuron complexity in neural networks and ant\nnetwork adaptability in ant colonies. From there we introduce how species\nrelate to one another in an ecological network via niche selection, niche\nchoice, and niche conformity with the aim of forming an analogy with human\nsocial network development as new agents join together and coordinate. From\nthere we show how our social structures are influenced by our neuro-physiology,\nour psychology, and our language. This emphasises how individual people within\na social network influence the structure and performance of that network in\ncomplex tasks, and that cognitive faculties such as Theory of Mind play a\ncentral role. We finish by discussing the current state of the art in AI and\nwhere there is potential for further development of a socially embodied\ncollective artificial intelligence that is capable of guiding its own social\nstructures.",
      "tldr_zh": "这篇论文探讨了人工智能（AI）代理如何通过人工理论心智（Artificial Theory of Mind）实现自导社会组织，以协调行为并实现单个代理无法完成的集体目标。作者强调AI应借鉴人类心理学，包括神经生理、心理学和语言的影响，并类比生态网络中的niche selection、niche choice和niche conformity来模拟人类社会网络发展。论文分析了collective intelligence在神经网络和蚂蚁群落中的表现，突出Theory of Mind在社会结构中的核心作用，并指出未来AI可进一步发展以引导自身社会结构，提升AI-AI和人类-AI交互。",
      "categories": [
        "cs.MA",
        "cs.AI",
        "cs.CY",
        "cs.HC",
        "nlin.AO"
      ],
      "primary_category": "cs.MA",
      "comment": "4 pages",
      "pdf_url": "http://arxiv.org/pdf/2411.09169v1",
      "published_date": "2024-11-14 04:06:26 UTC",
      "updated_date": "2024-11-14 04:06:26 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T00:38:31.278088"
    },
    {
      "arxiv_id": "2411.09168v1",
      "title": "Theory of Mind Enhances Collective Intelligence",
      "title_zh": "心智理论增强集体智能",
      "authors": [
        "Michael S. Harré",
        "Catherine Drysdale",
        "Jaime Ruiz-Serra"
      ],
      "abstract": "Collective Intelligence plays a central role in a large variety of fields,\nfrom economics and evolutionary theory to neural networks and eusocial insects,\nand it is also core to much of the work on emergence and self-organisation in\ncomplex systems theory. However, in human collective intelligence there is\nstill much more to be understood in the relationship between specific\npsychological processes at the individual level and the emergence of\nself-organised structures at the social level. Previously psychological factors\nhave played a relatively minor role in the study of collective intelligence as\nthe principles are often quite general and applicable to humans just as readily\nas insects or other agents without sophisticated psychologies. In this article\nwe emphasise, with examples from other complex adaptive systems, the broad\napplicability of collective intelligence principles while the mechanisms and\ntime-scales differ significantly between examples. We contend that flexible\ncollective intelligence in human social settings is improved by our use of a\nspecific cognitive tool: our Theory of Mind. We identify several key\ncharacteristics of psychologically mediated collective intelligence and show\nthat the development of a Theory of Mind is a crucial factor distinguishing\nsocial collective intelligence from general collective intelligence. We then\nplace these capabilities in the context of the next steps in artificial\nintelligence embedded in a future that includes an effective human-AI hybrid\nsocial ecology.",
      "tldr_zh": "该论文探讨了Theory of Mind（心智理论）如何提升集体智能，强调个体心理过程在人类社会自组织结构中的作用。作者通过与其他复杂适应系统的比较，指出虽然集体智能原则广泛适用，但人类的情感认知工具如Theory of Mind使其更具灵活性，并将之视为区分社会集体智能与一般集体智能的关键因素。最终，论文将这些发现置于人类-AI 混合社会生态的背景下，讨论了人工智能发展的下一步方向。",
      "categories": [
        "cs.MA",
        "cs.AI",
        "cs.CY",
        "cs.GT",
        "nlin.AO"
      ],
      "primary_category": "cs.MA",
      "comment": "20 pages, 2 figures, 1 table",
      "pdf_url": "http://arxiv.org/pdf/2411.09168v1",
      "published_date": "2024-11-14 03:58:50 UTC",
      "updated_date": "2024-11-14 03:58:50 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T00:38:41.532744"
    },
    {
      "arxiv_id": "2411.09160v1",
      "title": "Rationality based Innate-Values-driven Reinforcement Learning",
      "title_zh": "基于理性的内在价值驱动强化学习",
      "authors": [
        "Qin Yang"
      ],
      "abstract": "Innate values describe agents' intrinsic motivations, which reflect their\ninherent interests and preferences to pursue goals and drive them to develop\ndiverse skills satisfying their various needs. The essence of reinforcement\nlearning (RL) is learning from interaction based on reward-driven behaviors,\nmuch like natural agents. It is an excellent model to describe the\ninnate-values-driven (IV) behaviors of AI agents. Especially developing the\nawareness of the AI agent through balancing internal and external utilities\nbased on its needs in different tasks is a crucial problem for individuals\nlearning to support AI agents integrating human society with safety and harmony\nin the long term. This paper proposes a hierarchical compound intrinsic value\nreinforcement learning model -- innate-values-driven reinforcement learning\ntermed IVRL to describe the complex behaviors of AI agents' interaction. We\nformulated the IVRL model and proposed two IVRL models: DQN and A2C. By\ncomparing them with benchmark algorithms such as DQN, DDQN, A2C, and PPO in the\nRole-Playing Game (RPG) reinforcement learning test platform VIZDoom, we\ndemonstrated that rationally organizing various individual needs can\neffectively achieve better performance.",
      "tldr_zh": "这篇论文提出了一种基于理性组织的 Innate-Values-driven Reinforcement Learning (IVRL) 模型，用于模拟 AI 代理的内在动机和行为，强调通过平衡内部与外部效用来驱动代理发展技能并融入人类社会。IVRL 采用分层复合结构，并开发了 DQN 和 A2C 版本，以描述 AI 代理在不同任务中的复杂互动。实验在 VIZDoom 平台上与基准算法（如 DQN、DDQN、A2C 和 PPO）比较，结果显示，理性组织个体需求能显著提升性能，实现更高效的学习。",
      "categories": [
        "cs.AI",
        "cs.LG",
        "cs.RO"
      ],
      "primary_category": "cs.AI",
      "comment": "arXiv admin note: substantial text overlap with arXiv:2401.05572",
      "pdf_url": "http://arxiv.org/pdf/2411.09160v1",
      "published_date": "2024-11-14 03:28:02 UTC",
      "updated_date": "2024-11-14 03:28:02 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T00:39:53.057257"
    },
    {
      "arxiv_id": "2411.09158v1",
      "title": "The \\emph{Optimist}: Towards Fully Automated Graph Theory Research",
      "title_zh": "翻译失败",
      "authors": [
        "Randy Davila"
      ],
      "abstract": "This paper introduces the \\emph{Optimist}, an autonomous system developed to\nadvance automated conjecture generation in graph theory. Leveraging\nmixed-integer programming (MIP) and heuristic methods, the \\emph{Optimist}\ngenerates conjectures that both rediscover established theorems and propose\nnovel inequalities. Through a combination of memory-based computation and\nagent-like adaptability, the \\emph{Optimist} iteratively refines its\nconjectures by integrating new data, enabling a feedback process with minimal\nhuman (\\emph{or machine}) intervention. Initial experiments reveal the\n\\emph{Optimist}'s potential to uncover foundational results in graph theory, as\nwell as to produce conjectures of interest for future exploration. This work\nalso outlines the \\emph{Optimist}'s evolving integration with a counterpart\nagent, the \\emph{Pessimist} (a human \\emph{or machine} agent), to establish a\ndueling system that will drive fully automated graph theory research.",
      "tldr_zh": "这篇论文介绍了The Optimist，一个自主系统，旨在推进图论领域的自动猜想生成。它利用mixed-integer programming (MIP)和启发式方法生成猜想，既能重新发现已建立的定理，又能提出新不等式，并通过记忆-based计算和代理式适应性实现迭代完善和数据整合，几乎无需人类或机器干预。初步实验表明，该系统已成功揭示图论基础结果，并产生未来探索的有趣猜想。该工作还概述了The Optimist与The Pessimist（人类或机器代理）的整合，形成一个对抗系统，以驱动完全自动化的图论研究。",
      "categories": [
        "cs.AI",
        "math.CO"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2411.09158v1",
      "published_date": "2024-11-14 03:24:45 UTC",
      "updated_date": "2024-11-14 03:24:45 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T00:40:04.909225"
    },
    {
      "arxiv_id": "2411.09134v1",
      "title": "ABCI 3.0: Evolution of the leading AI infrastructure in Japan",
      "title_zh": "ABCI 3.0",
      "authors": [
        "Ryousei Takano",
        "Shinichiro Takizawa",
        "Yusuke Tanimura",
        "Hidemoto Nakada",
        "Hirotaka Ogawa"
      ],
      "abstract": "ABCI 3.0 is the latest version of the ABCI, a large-scale open AI\ninfrastructure that AIST has been operating since August 2018 and will be fully\noperational in January 2025. ABCI 3.0 consists of computing servers equipped\nwith 6128 of the NVIDIA H200 GPUs and an all-flash storage system. Its peak\nperformance is 6.22 exaflops in half precision and 3.0 exaflops in single\nprecision, which is 7 to 13 times faster than the previous system, ABCI 2.0. It\nalso more than doubles both storage capacity and theoretical read/write\nperformance. ABCI 3.0 is expected to accelerate research and development,\nevaluation, and workforce development of cutting-edge AI technologies, with a\nparticular focus on generative AI.",
      "tldr_zh": "ABCI 3.0 是日本 AIST 自 2018 年以来运营的 ABCI 基础设施的最新版本，将于 2025 年 1 月全面运营，旨在提升 AI 计算能力。 该系统配备了 6128 个 NVIDIA H200 GPUs，提供 6.22 exaflops 的半精度峰值性能和 3.0 exaflops 的单精度峰值性能，比前代 ABCI 2.0 快 7 到 13 倍，同时存储容量和读写性能均翻倍。 ABCI 3.0 预计将加速尖端 AI 技术的研究、开发、评估和人才培养，特别是 generative AI 领域。",
      "categories": [
        "cs.NI",
        "cs.AI"
      ],
      "primary_category": "cs.NI",
      "comment": "4 pages, 2 figures",
      "pdf_url": "http://arxiv.org/pdf/2411.09134v1",
      "published_date": "2024-11-14 02:14:12 UTC",
      "updated_date": "2024-11-14 02:14:12 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T00:40:18.055411"
    },
    {
      "arxiv_id": "2411.09125v1",
      "title": "DROJ: A Prompt-Driven Attack against Large Language Models",
      "title_zh": "DROJ：一种针对大型语言模型的提示驱动攻击",
      "authors": [
        "Leyang Hu",
        "Boran Wang"
      ],
      "abstract": "Large Language Models (LLMs) have demonstrated exceptional capabilities\nacross various natural language processing tasks. Due to their training on\ninternet-sourced datasets, LLMs can sometimes generate objectionable content,\nnecessitating extensive alignment with human feedback to avoid such outputs.\nDespite massive alignment efforts, LLMs remain susceptible to adversarial\njailbreak attacks, which usually are manipulated prompts designed to circumvent\nsafety mechanisms and elicit harmful responses. Here, we introduce a novel\napproach, Directed Rrepresentation Optimization Jailbreak (DROJ), which\noptimizes jailbreak prompts at the embedding level to shift the hidden\nrepresentations of harmful queries towards directions that are more likely to\nelicit affirmative responses from the model. Our evaluations on LLaMA-2-7b-chat\nmodel show that DROJ achieves a 100\\% keyword-based Attack Success Rate (ASR),\neffectively preventing direct refusals. However, the model occasionally\nproduces repetitive and non-informative responses. To mitigate this, we\nintroduce a helpfulness system prompt that enhances the utility of the model's\nresponses. Our code is available at\nhttps://github.com/Leon-Leyang/LLM-Safeguard.",
      "tldr_zh": "本研究提出了一种新的攻击方法——Directed Representation Optimization Jailbreak (DROJ)，针对Large Language Models (LLMs)的安全机制，通过在嵌入级别优化越狱提示，将有害查询的隐藏表示调整为更易引发肯定响应的方向，从而绕过模型的对齐防护。实验在LLaMA-2-7b-chat模型上显示，DROJ实现了100%的基于关键词的Attack Success Rate (ASR)，但偶尔会产生重复和非信息性的响应。为此，研究引入了helpfulness system prompt来提升响应的实用性。该方法揭示了LLMs在对抗性jailbreak attacks中的脆弱性，并提供了开源代码以促进进一步研究。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2411.09125v1",
      "published_date": "2024-11-14 01:48:08 UTC",
      "updated_date": "2024-11-14 01:48:08 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T00:40:28.751076"
    },
    {
      "arxiv_id": "2411.10486v1",
      "title": "Artificial Intelligence for Infectious Disease Prediction and Prevention: A Comprehensive Review",
      "title_zh": "人工智能用于传染病预测和预防：全面综述",
      "authors": [
        "Selestine Melchane",
        "Youssef Elmir",
        "Farid Kacimi",
        "Larbi Boubchir"
      ],
      "abstract": "Artificial Intelligence (AI) and infectious diseases prediction have recently\nexperienced a common development and advancement. Machine learning (ML)\napparition, along with deep learning (DL) emergence, extended many approaches\nagainst diseases apparition and their spread. And despite their outstanding\nresults in predicting infectious diseases, conflicts appeared regarding the\ntypes of data used and how they can be studied, analyzed, and exploited using\nvarious emerging methods. This has led to some ongoing discussions in the\nfield. This research aims not only to provide an overview of what has been\naccomplished, but also to highlight the difficulties related to the types of\ndata used, and the learning methods applied for each research objective. It\ncategorizes these contributions into three areas: predictions using Public\nHealth Data to prevent the spread of a transmissible disease within a region;\npredictions using Patients' Medical Data to detect whether a person is infected\nby a transmissible disease; and predictions using both Public and patient\nmedical data to estimate the extent of disease spread in a population. The\npaper also critically assesses the potential of AI and outlines its limitations\nin infectious disease management.",
      "tldr_zh": "这篇综述论文全面审视了Artificial Intelligence (AI)在传染病预测和预防中的应用，强调了Machine Learning (ML)和Deep Learning (DL)技术在疾病预测方面的进展，同时指出了数据类型和分析方法存在的争议。论文将相关研究分类为三类：使用Public Health Data预测区域内疾病传播、使用Patients' Medical Data检测个人感染，以及结合两者估算人口层面疾病传播程度。通过分析现有成就，该研究突出了AI的潜力，同时评估了其在传染病管理中的局限性，如数据处理挑战和方法适用性问题。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "q-bio.PE"
      ],
      "primary_category": "cs.LG",
      "comment": "31 pages, 5 figures, this manuscript has been accepted for\n  publication in ACTA UNIVERSITATIS SAPIENTIAE, Informatica",
      "pdf_url": "http://arxiv.org/pdf/2411.10486v1",
      "published_date": "2024-11-14 00:43:32 UTC",
      "updated_date": "2024-11-14 00:43:32 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T00:40:40.584206"
    },
    {
      "arxiv_id": "2411.09105v1",
      "title": "VCBench: A Controllable Benchmark for Symbolic and Abstract Challenges in Video Cognition",
      "title_zh": "翻译失败",
      "authors": [
        "Chenglin Li",
        "Qianglong Chen",
        "Zhi Li",
        "Feng Tao",
        "Yin Zhang"
      ],
      "abstract": "Recent advancements in Large Video-Language Models (LVLMs) have driven the\ndevelopment of benchmarks designed to assess cognitive abilities in video-based\ntasks. However, most existing benchmarks heavily rely on web-collected videos\npaired with human annotations or model-generated questions, which limit control\nover the video content and fall short in evaluating advanced cognitive\nabilities involving symbolic elements and abstract concepts. To address these\nlimitations, we introduce VCBench, a controllable benchmark to assess LVLMs'\ncognitive abilities, involving symbolic and abstract concepts at varying\ndifficulty levels. By generating video data with the Python-based engine,\nVCBench allows for precise control over the video content, creating dynamic,\ntask-oriented videos that feature complex scenes and abstract concepts. Each\ntask pairs with tailored question templates that target specific cognitive\nchallenges, providing a rigorous evaluation test. Our evaluation reveals that\neven state-of-the-art (SOTA) models, such as Qwen2-VL-72B, struggle with simple\nvideo cognition tasks involving abstract concepts, with performance sharply\ndropping by 19% as video complexity rises. These findings reveal the current\nlimitations of LVLMs in advanced cognitive tasks and highlight the critical\nrole of VCBench in driving research toward more robust LVLMs for complex video\ncognition challenges.",
      "tldr_zh": "该研究提出了 VCBench，一种可控基准，用于评估 Large Video-Language Models (LVLMs) 在视频认知中处理符号元素和抽象概念的能力，以弥补现有基准依赖网络视频和人工注释的局限性。VCBench 通过 Python-based 引擎生成动态任务导向视频，实现对内容和复杂度的精确控制，并配以针对特定认知挑战的问答模板。实验结果显示，即使是 SOTA 模型如 Qwen2-VL-72B，在简单抽象任务上表现不佳，随着视频复杂度增加，性能下降约 19%，这突显了 LVLMs 在高级认知任务中的不足，并强调 VCBench 在推动更鲁棒模型发展的关键作用。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2411.09105v1",
      "published_date": "2024-11-14 00:26:26 UTC",
      "updated_date": "2024-11-14 00:26:26 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T00:40:53.118699"
    },
    {
      "arxiv_id": "2411.09102v2",
      "title": "Provocation: Who benefits from \"inclusion\" in Generative AI?",
      "title_zh": "翻译失败",
      "authors": [
        "Samantha Dalal",
        "Siobhan Mackenzie Hall",
        "Nari Johnson"
      ],
      "abstract": "The demands for accurate and representative generative AI systems means there\nis an increased demand on participatory evaluation structures. While these\nparticipatory structures are paramount to to ensure non-dominant values,\nknowledge and material culture are also reflected in AI models and the media\nthey generate, we argue that dominant structures of community participation in\nAI development and evaluation are not explicit enough about the benefits and\nharms that members of socially marginalized groups may experience as a result\nof their participation. Without explicit interrogation of these benefits by AI\ndevelopers, as a community we may remain blind to the immensity of systemic\nchange that is needed as well. To support this provocation, we present a\nspeculative case study, developed from our own collective experiences as AI\nresearchers. We use this speculative context to itemize the barriers that need\nto be overcome in order for the proposed benefits to marginalized communities\nto be realized, and harms mitigated.",
      "tldr_zh": "这篇论文以“provocation”（挑衅）形式质疑生成式 AI 中的“inclusion”（包容性），探讨谁真正从中受益。作者指出，虽然参与式评估结构（participatory evaluation structures）能确保非主导价值观、知识和物质文化在 AI 模型中得到反映，但这些结构并未明确说明社会边缘化群体（marginalized groups）参与可能带来的好处和危害。论文通过一个基于作者集体经验的推测性案例研究（speculative case study），列出了克服相关障碍的必要步骤，以实现真正的好处并减轻潜在风险，从而呼吁 AI 开发社区进行更深层的系统性变革。",
      "categories": [
        "cs.CY",
        "cs.AI",
        "cs.HC"
      ],
      "primary_category": "cs.CY",
      "comment": "3 pages, 1 figure. Published as a Short Paper in the NeurIPS 2024\n  Workshop on Evaluating Evaluations: Examining Best Practices for Measuring\n  Broader Impacts of Generative AI",
      "pdf_url": "http://arxiv.org/pdf/2411.09102v2",
      "published_date": "2024-11-14 00:18:25 UTC",
      "updated_date": "2024-11-15 17:09:40 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T00:41:05.161691"
    },
    {
      "arxiv_id": "2411.09101v2",
      "title": "Heuristical Comparison of Vision Transformers Against Convolutional Neural Networks for Semantic Segmentation on Remote Sensing Imagery",
      "title_zh": "翻译失败",
      "authors": [
        "Ashim Dahal",
        "Saydul Akbar Murad",
        "Nick Rahimi"
      ],
      "abstract": "Vision Transformers (ViT) have recently brought a new wave of research in the\nfield of computer vision. These models have performed particularly well in\nimage classification and segmentation. Research on semantic and instance\nsegmentation has accelerated with the introduction of the new architecture,\nwith over 80% of the top 20 benchmarks for the iSAID dataset based on either\nthe ViT architecture or the attention mechanism behind its success. This paper\nfocuses on the heuristic comparison of three key factors of using (or not\nusing) ViT for semantic segmentation of remote sensing aerial images on the\niSAID dataset. The experimental results observed during this research were\nanalyzed based on three objectives. First, we studied the use of a weighted\nfused loss function to maximize the mean Intersection over Union (mIoU) score\nand Dice score while minimizing entropy or class representation loss. Second,\nwe compared transfer learning on Meta's MaskFormer, a ViT-based semantic\nsegmentation model, against a generic UNet Convolutional Neural Network (CNN)\nbased on mIoU, Dice scores, training efficiency, and inference time. Third, we\nexamined the trade-offs between the two models in comparison to current\nstate-of-the-art segmentation models. We show that the novel combined weighted\nloss function significantly boosts the CNN model's performance compared to\ntransfer learning with ViT. The code for this implementation can be found at:\nhttps://github.com/ashimdahal/ViT-vs-CNN-Image-Segmentation.",
      "tldr_zh": "本文对 Vision Transformers (ViT) 和 Convolutional Neural Networks (CNN) 在遥感图像语义分割上的性能进行了启发式比较，主要基于 iSAID 数据集。研究方法包括使用加权融合损失函数来优化 mean Intersection over Union (mIoU) 和 Dice 分数，同时比较 MaskFormer (ViT-based) 与 UNet (CNN-based) 在转移学习、训练效率和推理时间方面的表现。结果显示，该新颖的组合加权损失函数显著提升了 CNN 模型的性能，使其优于 ViT 模型；此外，还分析了二者与当前最先进分割模型的权衡。代码实现可访问 https://github.com/ashimdahal/ViT-vs-CNN-Image-Segmentation。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2411.09101v2",
      "published_date": "2024-11-14 00:18:04 UTC",
      "updated_date": "2025-02-13 18:20:14 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T00:41:18.415835"
    }
  ],
  "raw_papers_fetched": true,
  "papers_count": 88,
  "processed_papers_count": 88,
  "failed_papers_count": 0,
  "summary_generated": true,
  "daily_data_saved": true,
  "last_update": "2025-05-21T00:43:15.955487"
}
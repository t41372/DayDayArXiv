{
  "date": "2025-08-01",
  "category": "cs.AI",
  "summary": "æ¬¢è¿æ¥åˆ° UTC æ—¶é—´ 2025-08-01 çš„ arXiv ä¸­æ–‡ TLDR å¿«æŠ¥ï¼\n\n**ä»Šæ—¥æ€»ç»“ï¼š**\nä»Šå¤©çš„ arXiv è®ºæ–‡åˆ—è¡¨å¼‚å¸¸ä¸°å¯Œï¼Œå…±æœ‰ 141 ç¯‡æ›´æ–°ã€‚**LLM çš„å®‰å…¨æ€§ä¸å¿ƒç†å­¦**æ˜¯ä»Šå¤©çš„é‡å¤´æˆï¼Œæœ‰ç ”ç©¶æ·±å…¥æ¢è®¨äº†â€œç»™é’±â€æˆ–â€œå¨èƒâ€æ˜¯å¦çœŸèƒ½æå‡æ¨¡å‹è¡¨ç°ï¼Œä»¥åŠåˆ©ç”¨ç‰©ç†å­¦æ¨¡å‹é¢„æµ‹ AI çš„å¹»è§‰â€œç¿»è½¬ç‚¹â€ã€‚åœ¨**Agentï¼ˆæ™ºèƒ½ä½“ï¼‰**é¢†åŸŸï¼Œè…¾è®¯æ¨å‡ºäº†å¼€æºæ¡†æ¶ Cognitive Kernel-Proï¼Œä¸”æœ‰å¤šç¯‡è®ºæ–‡å…³æ³¨å¦‚ä½•è®© Agent æ›´å¯æ§ã€æ›´ç¡®å®šã€‚ç†è®ºæ–¹é¢ï¼ŒMichael Bronstein å¤§ä½¬å¸¦æ¥äº† 78 é¡µçš„**å‡ ä½•æ·±åº¦å­¦ä¹ ï¼ˆGeometric Deep Learningï¼‰**æ•°å­¦åŸºç¡€ç»¼è¿°ã€‚æ­¤å¤–ï¼ŒåŒ»ç–— AIã€æ¨èç³»ç»Ÿä¸å¤šæ¨¡æ€ç”Ÿæˆçš„ç»“åˆä¹Ÿæ¶Œç°äº†ä¸å°‘ä½³ä½œã€‚\n\nä»¥ä¸‹æ˜¯ä»Šæ—¥é‡ç‚¹è®ºæ–‡çš„æ·±åº¦è§£è¯»ï¼š\n\n---\n\n### ğŸš€ å¿…è¯»ç²¾é€‰ï¼šLLM å¿ƒç†å­¦ã€ç†è®ºåŸºçŸ³ä¸ Agent æ¡†æ¶\n\n#### 1. **# title: æç¤ºè¯ç§‘å­¦æŠ¥å‘Š 3ï¼šæˆ‘ç»™ä½ é’±æˆ–è€…æ€äº†ä½ â€”â€”ä½†ä½ ä¼šåœ¨æ­¤æ„å—ï¼Ÿ**\n**(Prompting Science Report 3: I'll pay you or I'll kill you -- but will you care?)**\n> **Authors:** Lennart Meincke, Ethan Mollick, et al.\n> **å…³é”®è¯:** Prompt Engineering, LLM Psychology, Benchmark\n\n**æ ¸å¿ƒè´¡çŒ®ï¼š** è¿™æ˜¯ä¸€ç¯‡éå¸¸æœ‰è¶£çš„å®è¯ç ”ç©¶ã€‚é’ˆå¯¹ç¤¾åŒºä¸­æµä¼ çš„â€œç»™æ¨¡å‹å°è´¹ï¼ˆTippingï¼‰â€æˆ–â€œå¨èƒæ¨¡å‹ï¼ˆThreateningï¼‰â€èƒ½æé«˜è¡¨ç°çš„ç„å­¦ï¼Œä½œè€…åœ¨ GPQA å’Œ MMLU-Pro ä¸Šè¿›è¡Œäº†ä¸¥æ ¼æµ‹è¯•ã€‚\n**å‘ç°ï¼š** ç»“è®ºä»¤äººæ„å¤–ï¼ˆæˆ–ä¸æ„å¤–ï¼‰ï¼š**å¨èƒæˆ–ç»™å°è´¹å¯¹åŸºå‡†æµ‹è¯•çš„æ•´ä½“è¡¨ç°æ²¡æœ‰æ˜¾è‘—å½±å“ã€‚** è™½ç„¶æç¤ºè¯çš„å˜åŒ–ç¡®å®ä¼šå½±å“å•ä¸ªé—®é¢˜çš„å›ç­”ï¼Œä½†å¾ˆéš¾é¢„åˆ¤å“ªç§ç­–ç•¥å¯¹å“ªä¸ªå…·ä½“é—®é¢˜æœ‰æ•ˆã€‚è¿™è¡¨æ˜ç®€å•çš„â€œæƒ…ç»ªå‹’ç´¢â€æç¤ºè¯ç­–ç•¥å¯èƒ½ä¸å¦‚é¢„æœŸçš„é‚£æ ·æœ‰æ•ˆï¼Œå°¤å…¶æ˜¯åœ¨è§£å†³éš¾é¢˜æ—¶ã€‚\n\n#### 2. **# title: å‡ ä½•æ·±åº¦å­¦ä¹ çš„æ•°å­¦åŸºç¡€**\n**(Mathematical Foundations of Geometric Deep Learning)**\n> **Authors:** Haitz SÃ¡ez de OcÃ¡riz Borde, Michael Bronstein\n> **å…³é”®è¯:** Geometric Deep Learning, Group Theory, Survey\n\n**æ ¸å¿ƒè´¡çŒ®ï¼š** Michael Bronstein ç­‰äººå¸¦æ¥çš„é‡ç£…ç»¼è¿°ï¼ˆ78é¡µï¼‰ã€‚\n**å‘ç°ï¼š** æœ¬æ–‡ç³»ç»Ÿå›é¡¾äº†å­¦ä¹ **å‡ ä½•æ·±åº¦å­¦ä¹ ï¼ˆGeometric Deep Learning, GDLï¼‰**æ‰€éœ€çš„å…³é”®æ•°å­¦æ¦‚å¿µã€‚å¯¹äºæƒ³è¦æ·±å…¥ç†è§£å›¾ç¥ç»ç½‘ç»œã€ç¾¤è®ºåœ¨æ·±åº¦å­¦ä¹ ä¸­åº”ç”¨çš„ç ”ç©¶è€…æ¥è¯´ï¼Œè¿™æ˜¯ä¸€ä»½æå…¶é‡è¦çš„å‚è€ƒæ–‡çŒ®å’Œå…¥é—¨æŒ‡å—ã€‚\n\n#### 3. **# title: AI ç¿»è½¬ç‚¹ä¸å¹»è§‰çš„å¤šè‡ªæ—‹ç‰©ç†å­¦**\n**(Multispin Physics of AI Tipping Points and Hallucinations)**\n> **Authors:** Neil F. Johnson, Frank Yingjie Huo\n> **å…³é”®è¯:** AI Safety, Hallucination, Statistical Physics\n\n**æ ¸å¿ƒè´¡çŒ®ï¼š** ä½œè€…å»ºç«‹äº†ä¸€ä¸ªæ•°å­¦æ˜ å°„ï¼Œå°†ç”Ÿæˆå¼ AI æ˜ å°„åˆ°ä¸€ä¸ª**å¤šè‡ªæ—‹çƒ­ç³»ç»Ÿï¼ˆmultispin thermal systemï¼‰**ã€‚\n**å‘ç°ï¼š** ç ”ç©¶æ­ç¤ºäº†åœ¨ AI çš„åŸºæœ¬ Attention å¤´å°ºåº¦ä¸Šå­˜åœ¨ä¸€ä¸ªéšè—çš„â€œç¿»è½¬ä¸ç¨³å®šæ€§ï¼ˆtipping instabilityï¼‰â€ã€‚ä½œè€…æ¨å¯¼å‡ºäº†ä¸€ä¸ªç²¾ç¡®å…¬å¼ï¼Œå¯ä»¥ç›´æ¥æ ¹æ®ç”¨æˆ·çš„æç¤ºè¯é€‰æ‹©å’Œ AI çš„è®­ç»ƒåå·®æ¥é¢„æµ‹è¿™ä¸ªç¿»è½¬ç‚¹ï¼ˆå³ä»æ­£ç¡®å›ç­”çªç„¶å˜ä¸ºé”™è¯¯/å¹»è§‰ï¼‰ã€‚è¿™ä¸ºé‡åŒ– AI é£é™©æä¾›äº†ä¸€ç§ç‰©ç†å­¦è§†è§’çš„å…¨æ–°æ–¹æ³•ã€‚\n\n#### 4. **# title: Cognitive Kernel-Proï¼šæ·±åº¦ç ”ç©¶æ™ºèƒ½ä½“ä¸æ™ºèƒ½ä½“åŸºç¡€æ¨¡å‹è®­ç»ƒæ¡†æ¶**\n**(Cognitive Kernel-Pro: A Framework for Deep Research Agents and Agent Foundation Models Training)**\n> **Authors:** Tianqing Fang, et al. (Tencent Team)\n> **å…³é”®è¯:** AI Agents, Open Source, Framework\n\n**æ ¸å¿ƒè´¡çŒ®ï¼š** è…¾è®¯å›¢é˜Ÿæå‡ºäº†ä¸€ä¸ªå®Œå…¨å¼€æºçš„**å¤šæ¨¡å—æ™ºèƒ½ä½“æ¡†æ¶ Cognitive Kernel-Pro**ï¼Œæ—¨åœ¨è§£å†³å½“å‰ Agent ç³»ç»Ÿé—­æºæˆ–ä¾èµ–ä»˜è´¹ API çš„é—®é¢˜ã€‚\n**å‘ç°ï¼š** è¯¥æ¡†æ¶ä¸“æ³¨äºæ„å»ºé«˜è´¨é‡çš„ Agent è®­ç»ƒæ•°æ®ï¼ˆæŸ¥è¯¢ã€è½¨è¿¹ã€å¯éªŒè¯ç­”æ¡ˆï¼‰ï¼Œè¦†ç›– Webã€æ–‡ä»¶ã€ä»£ç å’Œæ¨ç†é¢†åŸŸã€‚å…¶ 8B å‚æ•°çš„å¼€æºæ¨¡å‹åœ¨ GAIA åŸºå‡†æµ‹è¯•ä¸­è¶…è¶Šäº†ä¹‹å‰çš„é¢†å…ˆç³»ç»Ÿï¼ˆå¦‚ WebDancerï¼‰ï¼Œä¸ºç¤¾åŒºæä¾›äº†å¯å¤ç°çš„é«˜æ€§èƒ½ Agent è§£å†³æ–¹æ¡ˆã€‚\n\n#### 5. **# title: è“å›¾ä¼˜å…ˆï¼Œæ¨¡å‹æ¬¡ä¹‹ï¼šç¡®å®šæ€§ LLM å·¥ä½œæµæ¡†æ¶**\n**(Blueprint First, Model Second: A Framework for Deterministic LLM Workflow)**\n> **Authors:** Libin Qiu, et al.\n> **å…³é”®è¯:** Deterministic Agents, Workflow, Source Code Agent\n\n**æ ¸å¿ƒè´¡çŒ®ï¼š** é’ˆå¯¹ LLM Agent åœ¨ç»“æ„åŒ–ç¯å¢ƒä¸­è¡¨ç°å‡ºçš„ä¸ç¡®å®šæ€§ï¼Œä½œè€…æå‡ºäº†ä¸€ç§â€œè“å›¾ä¼˜å…ˆâ€çš„æ–°èŒƒå¼ã€‚\n**å‘ç°ï¼š** å°†å·¥ä½œæµé€»è¾‘ä¸ç”Ÿæˆæ¨¡å‹è§£è€¦ã€‚ä¸“å®¶å®šä¹‰çš„æ“ä½œæµç¨‹è¢«ç¼–ç ä¸ºåŸºäºæºä»£ç çš„**æ‰§è¡Œè“å›¾ï¼ˆExecution Blueprintï¼‰**ï¼Œç”±ç¡®å®šæ€§å¼•æ“æ‰§è¡Œã€‚LLM ä»…ä½œä¸ºå·¥å…·è¢«è°ƒç”¨æ¥å¤„ç†å­ä»»åŠ¡ï¼Œè€Œä¸å†³å®šå·¥ä½œæµè·¯å¾„ã€‚è¯¥æ–¹æ³•åœ¨ tau-bench åŸºå‡†ä¸Šå¤§å¹…è¶…è¶ŠåŸºçº¿ï¼ˆPass^1 æé«˜ 10.1%ï¼‰ï¼Œå®ç°äº†æ›´å¯é çš„ Agent éƒ¨ç½²ã€‚\n\n---\n\n### ğŸ›¡ï¸ LLM å®‰å…¨ä¸å¯¹é½\n\n#### 6. **# title: R1-ACTï¼šé€šè¿‡æ¿€æ´»å®‰å…¨çŸ¥è¯†å®ç°é«˜æ•ˆæ¨ç†æ¨¡å‹å®‰å…¨å¯¹é½**\n**(R1-ACT: Efficient Reasoning Model Safety Alignment by Activating Safety Knowledge)**\n> **Authors:** Yeonjun In, et al.\n> **å…³é”®è¯:** Safety Alignment, Reasoning Models, Post-training\n\n**æ ¸å¿ƒè´¡çŒ®ï¼š** ç ”ç©¶å‘ç°å¤§å‹æ¨ç†æ¨¡å‹ï¼ˆLRMsï¼‰å…¶å®å…·å¤‡å®‰å…¨çŸ¥è¯†ï¼Œä½†åœ¨æ¨ç†è¿‡ç¨‹ä¸­æœªèƒ½â€œæ¿€æ´»â€å®ƒã€‚\n**å‘ç°ï¼š** æå‡ºäº† R1-Actï¼Œä¸€ç§ç®€å•çš„åè®­ç»ƒæ–¹æ³•ï¼Œé€šè¿‡ç»“æ„åŒ–çš„æ¨ç†è¿‡ç¨‹æ˜¾å¼è§¦å‘å®‰å…¨çŸ¥è¯†ã€‚ä»…éœ€ 1000 ä¸ªè®­ç»ƒæ ·æœ¬å’Œå•å¡ 90 åˆ†é’Ÿè®­ç»ƒï¼Œå°±èƒ½åœ¨ä¿æŒæ¨ç†æ€§èƒ½çš„åŒæ—¶æ˜¾è‘—æå‡å®‰å…¨æ€§ã€‚\n\n#### 7. **# title: æ½œåœ¨çŸ¥è¯†æ‰‹æœ¯åˆ€ï¼šå¤§è¯­è¨€æ¨¡å‹çš„ç²¾ç¡®ä¸å¤§è§„æ¨¡çŸ¥è¯†ç¼–è¾‘**\n**(Latent Knowledge Scalpel: Precise and Massive Knowledge Editing for Large Language Models)**\n> **Authors:** Xin Liu, et al.\n> **å…³é”®è¯:** Knowledge Editing, Model Editing, Hypernetwork\n\n**æ ¸å¿ƒè´¡çŒ®ï¼š** é’ˆå¯¹ LLM éš¾ä»¥åŒæ—¶ç¼–è¾‘å¤§é‡äº‹å®çŸ¥è¯†çš„é—®é¢˜ï¼Œæå‡ºäº†**æ½œåœ¨çŸ¥è¯†æ‰‹æœ¯åˆ€ï¼ˆLKSï¼‰**ã€‚\n**å‘ç°ï¼š** åˆ©ç”¨è½»é‡çº§è¶…ç½‘ç»œï¼ˆHypernetworkï¼‰æ“çºµç‰¹å®šå®ä½“çš„æ½œåœ¨çŸ¥è¯†ã€‚å®éªŒè¡¨æ˜ï¼Œå³ä½¿åŒæ—¶è¿›è¡Œ **10,000 æ¬¡ç¼–è¾‘**ï¼ŒLKS ä¹Ÿèƒ½æœ‰æ•ˆæ›´æ–°çŸ¥è¯†å¹¶ä¿ç•™æ¨¡å‹çš„é€šç”¨èƒ½åŠ›ã€‚\n\n#### 8. **# title: ç°æˆå¤§è¯­è¨€æ¨¡å‹ä¸­çš„å‡çº§ç®¡ç†**\n**(Managing Escalation in Off-the-Shelf Large Language Models)**\n> **Authors:** Sebastian Elbaum, Jonathan Panter\n> **å…³é”®è¯:** National Security, Escalation, Wargame\n\n**æ ¸å¿ƒè´¡çŒ®ï¼š** ç ”ç©¶äº†å•†ç”¨ LLMï¼ˆå¦‚ ChatGPTï¼‰åœ¨å›½å®¶å®‰å…¨å’Œåœ°ç¼˜æ”¿æ²»åœºæ™¯ä¸­æ˜¯å¦å€¾å‘äºé‡‡å–â€œå‡çº§ï¼ˆEscalatoryï¼‰â€è¡ŒåŠ¨ã€‚\n**å‘ç°ï¼š** ç¡®å®å­˜åœ¨å‡çº§å€¾å‘ã€‚ä½†ä½œè€…å±•ç¤ºäº†ä¸¤ç§ç®€å•çš„éæŠ€æœ¯å¹²é¢„æªæ–½ï¼Œå¯ä»¥å¤§å¹…é™ä½å…µæ£‹æ¨æ¼”ä¸­çš„å†²çªå‡çº§é£é™©ã€‚ç»“è®ºæ˜¯å‘¼åé™åˆ¶ LLM ç”¨äºå›½å®¶å®‰å…¨è¿˜ä¸ºæ—¶è¿‡æ—©ï¼Œåº”æ³¨é‡å¯¹é½å’Œç®¡æ§ã€‚\n\n---\n\n### ğŸ’¡ æ¨èç³»ç»Ÿä¸å¤šæ¨¡æ€åº”ç”¨\n\n#### 9. **# title: ITDRï¼šç”¨äºå¢å¼ºå¤§è¯­è¨€æ¨¡å‹æ¨èèƒ½åŠ›çš„æŒ‡ä»¤å¾®è°ƒæ•°æ®é›†**\n**(ITDR: An Instruction Tuning Dataset for Enhancing Large Language Models in Recommendations)**\n> **Authors:** Zekun Liu, et al.\n> **å…³é”®è¯:** Recommender Systems, Instruction Tuning, Dataset\n\n**æ ¸å¿ƒè´¡çŒ®ï¼š** è§£å†³äº† LLM åœ¨æ¨èç³»ç»Ÿä¸­å› æ•°æ®ç»“æ„å·®å¼‚å¯¼è‡´çš„æ€§èƒ½ç“¶é¢ˆã€‚\n**å‘ç°ï¼š** æ„å»ºäº†åŒ…å« 20 ä¸‡ä¸ªå®ä¾‹çš„ç»¼åˆæŒ‡ä»¤å¾®è°ƒæ•°æ®é›† ITDRï¼Œæ¶µç›– 7 ä¸ªå­ä»»åŠ¡ã€‚å®éªŒè¯æ˜ï¼ŒITDR æ˜¾è‘—æå‡äº† GLM-4ã€Llama-3.2 ç­‰å¼€æºæ¨¡å‹åœ¨æ¨èä»»åŠ¡ä¸Šçš„è¡¨ç°ã€‚\n\n#### 10. **# title: ç”¨äºè´«å›°åˆ¶å›¾çš„æŸæ‹‰å›¾å¼è¡¨å¾ï¼šç»Ÿä¸€çš„è§†è§‰-è¯­è¨€ä»£ç è¿˜æ˜¯æ™ºèƒ½ä½“å¼•å‘çš„æ–°é¢–æ€§ï¼Ÿ**\n**(Platonic Representations for Poverty Mapping: Unified Vision-Language Codes or Agent-Induced Novelty?)**\n> **Authors:** Satiyabooshan Murugaboopathy, et al.\n> **å…³é”®è¯:** Poverty Mapping, Multimodal, Platonic Representation Hypothesis\n\n**æ ¸å¿ƒè´¡çŒ®ï¼š** åˆ©ç”¨å«æ˜Ÿå›¾åƒå’Œ LLM ç”Ÿæˆçš„æ–‡æœ¬/AI Agent æ£€ç´¢çš„æ–‡æœ¬æ¥é¢„æµ‹å®¶åº­è´¢å¯Œã€‚\n**å‘ç°ï¼š** èåˆè§†è§‰å’Œ LLM å†…éƒ¨çŸ¥è¯†æ¯”çº¯è§†è§‰åŸºçº¿æ›´æœ‰æ•ˆï¼ˆR-squared 0.77 vs 0.63ï¼‰ã€‚ç ”ç©¶å‘ç°è§†è§‰å’Œè¯­è¨€æ¨¡æ€çš„åµŒå…¥å­˜åœ¨éƒ¨åˆ†æ”¶æ•›ï¼ˆæŸæ‹‰å›¾å¼è¡¨å¾å‡è®¾ï¼‰ï¼Œè¡¨æ˜å®ƒä»¬å…±äº«äº†å…³äºç‰©è´¨ç¦åˆ©çš„æ½œåœ¨ä»£ç ã€‚\n\n#### 11. **# title: D3ï¼šä½¿ç”¨äºŒé˜¶ç‰¹å¾çš„å…è®­ç»ƒ AI ç”Ÿæˆè§†é¢‘æ£€æµ‹**\n**(D3: Training-Free AI-Generated Video Detection Using Second-Order Features)**\n> **Authors:** Chende Zheng, et al.\n> **å…³é”®è¯:** Deepfake Detection, Video Generation, Physics-based\n\n**æ ¸å¿ƒè´¡çŒ®ï¼š** é’ˆå¯¹ Sora ç­‰ç”Ÿæˆçš„è§†é¢‘ï¼Œæå‡ºäº†ä¸€ç§åŸºäºç‰›é¡¿åŠ›å­¦äºŒé˜¶åŠ¨åŠ›å­¦åˆ†æçš„æ£€æµ‹æ–¹æ³•ã€‚\n**å‘ç°ï¼š** çœŸå®è§†é¢‘å’Œ AI è§†é¢‘åœ¨äºŒé˜¶æ—¶é—´ç‰¹å¾åˆ†å¸ƒä¸Šå­˜åœ¨æ ¹æœ¬å·®å¼‚ã€‚æå‡ºçš„ D3 æ–¹æ³•æ— éœ€è®­ç»ƒï¼Œåˆ©ç”¨è¿™ç§å·®å¼‚åœ¨ GenVideo ç­‰æ•°æ®é›†ä¸Šè¶…è¶Šäº† SOTA æ–¹æ³• 10% ä»¥ä¸Šã€‚\n\n---\n\n### ğŸ¥ åŒ»ç–—ä¸ç§‘å­¦ AI\n\n#### 12. **# title: LLM æ—¶ä»£çš„åŒ»å­¦æ¨ç†ï¼šå¢å¼ºæŠ€æœ¯ä¸åº”ç”¨ç³»ç»Ÿç»¼è¿°**\n**(Medical Reasoning in the Era of LLMs: A Systematic Review of Enhancement Techniques and Applications)**\n> **Authors:** Wenxuan Wang, et al.\n> **å…³é”®è¯:** Medical Reasoning, LLM, Survey\n\n**æ ¸å¿ƒè´¡çŒ®ï¼š** å¯¹ LLM åœ¨åŒ»å­¦æ¨ç†é¢†åŸŸçš„é¦–ä¸ªç³»ç»Ÿç»¼è¿°ã€‚\n**å‘ç°ï¼š** åˆ†æäº† 2022-2025 å¹´çš„ 60 é¡¹å¼€åˆ›æ€§ç ”ç©¶ï¼Œæå‡ºäº†æ¨ç†å¢å¼ºæŠ€æœ¯çš„åˆ†ç±»æ³•ï¼ˆè®­ç»ƒæ—¶ vs æµ‹è¯•æ—¶ï¼‰ï¼Œå¹¶æŒ‡å‡ºäº†å½“å‰é¢ä¸´çš„â€œå¿ å®åº¦-åˆç†æ€§å·®è·â€ç­‰å…³é”®æŒ‘æˆ˜ã€‚\n\n#### 13. **# title: å¤šæ­¥æ£€ç´¢ä¸æ¨ç†æå‡å¤§è¯­è¨€æ¨¡å‹æ”¾å°„å­¦é—®ç­”èƒ½åŠ›**\n**(Multi-step retrieval and reasoning improves radiology question answering with large language models)**\n> **Authors:** Sebastian Wind, et al.\n> **å…³é”®è¯:** Radiology, RAG, Clinical Reasoning\n\n**æ ¸å¿ƒè´¡çŒ®ï¼š** æå‡ºäº† RaRï¼ˆRadiology Retrieval and Reasoningï¼‰æ¡†æ¶ã€‚\n**å‘ç°ï¼š** ç›¸æ¯”ä¼ ç»Ÿçš„å•æ­¥ RAGï¼Œå¤šæ­¥æ£€ç´¢æ˜¾è‘—æé«˜äº†è¯Šæ–­å‡†ç¡®æ€§ï¼Œå‡å°‘äº† 9.4% çš„å¹»è§‰ã€‚æœ‰è¶£çš„æ˜¯ï¼ŒRaR å¯¹å°å‚æ•°æ¨¡å‹çš„æå‡æœ€å¤§ï¼Œè€Œå¯¹è¶…å¤§æ¨¡å‹ï¼ˆ>200Bï¼‰æå‡è¾ƒå°ã€‚\n\n---\n\n### ğŸ› ï¸ å…¶å®ƒå€¼å¾—å…³æ³¨çš„æŠ€æœ¯ç‚¹\n\n*   **é‡å­è®¡ç®—**: **#2 TensorHyper-VQC** æå‡ºäº†ä¸€ç§å¼ é‡åˆ—ï¼ˆTensor-Trainï¼‰å¼•å¯¼çš„è¶…ç½‘ç»œï¼Œç”¨äºæé«˜å˜åˆ†é‡å­è®¡ç®—çš„é²æ£’æ€§å’Œå¯æ‰©å±•æ€§ï¼Œç”šè‡³åœ¨ IBM Heron 156 é‡å­æ¯”ç‰¹å¤„ç†å™¨ä¸Šè¿›è¡Œäº†éªŒè¯ã€‚\n*   **ä»£ç è´¨é‡**: **#11 A Note on Code Quality Score** ä»‹ç»äº†åŸºäº Llama3 å¾®è°ƒçš„ä»£ç è´¨é‡è¯„åˆ†ç³»ç»Ÿï¼Œå·²åœ¨å·¥ä¸šç•Œå¤§è§„æ¨¡åº”ç”¨ã€‚\n*   **æœºå™¨äºº**: **#9 Learning Pivoting Manipulation** ç»“åˆäº†åŠ›è§‰å’Œè§†è§‰åé¦ˆï¼Œåˆ©ç”¨åŸºäºä¼˜åŒ–çš„æ¼”ç¤ºæ¥å­¦ä¹ å¤æ‚çš„æ¢è½´æ“çºµä»»åŠ¡ã€‚\n*   **å›¾ç¥ç»ç½‘ç»œ**: **#47 DAMR** æå‡ºäº†ä¸€ç§ç»“åˆ LLM å¼•å¯¼çš„è’™ç‰¹å¡æ´›æ ‘æœç´¢ï¼ˆMCTSï¼‰çš„çŸ¥è¯†å›¾è°±é—®ç­”æ¡†æ¶ï¼Œè§£å†³äº†æ£€ç´¢-æ¨ç†èŒƒå¼çš„é€‚åº”æ€§é—®é¢˜ã€‚\n\nå¸Œæœ›ä»Šå¤©çš„å¿«æŠ¥å¯¹ä½ çš„ç ”ç©¶æœ‰æ‰€å¯å‘ï¼æˆ‘ä»¬æ˜å¤©è§ã€‚",
  "papers": [
    {
      "arxiv_id": "2508.05667v2",
      "title": "ITDR: An Instruction Tuning Dataset for Enhancing Large Language Models in Recommendations",
      "title_zh": "ITDRï¼šé¢å‘å¢å¼ºå¤§è¯­è¨€æ¨¡å‹æ¨èèƒ½åŠ›çš„æŒ‡ä»¤å¾®è°ƒæ•°æ®é›†",
      "authors": [
        "Zekun Liu",
        "Xiaowen Huang",
        "Jitao Sang"
      ],
      "abstract": "Large language models (LLMs) have demonstrated outstanding performance in natural language processing tasks. However, in the field of recommender systems, due to the inherent structural discrepancy between user behavior data and natural language, LLMs struggle to effectively model the associations between user preferences and items. Although prompt-based methods can generate recommendation results, their inadequate understanding of recommendation tasks leads to constrained performance. To address this gap, we construct a comprehensive instruction tuning dataset, ITDR, which encompasses seven subtasks across two root tasks: user-item interaction and user-item understanding. The dataset integrates data from 13 public recommendation datasets and is built using manually crafted standardized templates, comprising approximately 200,000 instances. Experimental results demonstrate that ITDR significantly enhances the performance of mainstream open-source LLMs such as GLM-4, Qwen2.5, Qwen2.5-Instruct and LLaMA-3.2 on recommendation tasks. Furthermore, we analyze the correlations between tasks and explore the impact of task descriptions and data scale on instruction tuning effectiveness. Finally, we perform comparative experiments against closed-source LLMs with massive parameters. Our tuning dataset ITDR, the fine-tuned large recommendation models, all LoRA modules, and the complete experimental results are available at https://github.com/hellolzk/ITDR.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤§è¯­è¨€æ¨¡å‹(LLMs)åœ¨æ¨èç³»ç»Ÿé¢†åŸŸä¸­ç”±äºç”¨æˆ·è¡Œä¸ºæ•°æ®ä¸è‡ªç„¶è¯­è¨€é—´çš„ç»“æ„å·®å¼‚ï¼Œå¯¼è‡´éš¾ä»¥æœ‰æ•ˆå»ºæ¨¡ç”¨æˆ·åå¥½ä¸é¡¹ç›®å…³è”çš„é—®é¢˜ã€‚ä¸ºæ­¤ï¼Œä½œè€…æ„å»ºäº†ä¸€ä¸ªåä¸ºITDRçš„ç»¼åˆæ€§æŒ‡ä»¤å¾®è°ƒ(Instruction Tuning)æ•°æ®é›†ï¼Œæ¶µç›–äº†ç”¨æˆ·-é¡¹ç›®äº¤äº’(user-item interaction)å’Œç”¨æˆ·-é¡¹ç›®ç†è§£(user-item understanding)ä¸¤å¤§ç±»æ ¹ä»»åŠ¡ä¸‹çš„ä¸ƒä¸ªå­ä»»åŠ¡ã€‚è¯¥æ•°æ®é›†æ•´åˆäº†æ¥è‡ª13ä¸ªå…¬å¼€æ¨èæ•°æ®é›†çš„çº¦20ä¸‡æ¡å®ä¾‹ï¼Œå¹¶åˆ©ç”¨äººå·¥è®¾è®¡çš„æ ‡å‡†åŒ–æ¨¡æ¿è¿›è¡Œæ„å»ºã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒITDRæ˜¾è‘—æå‡äº†GLM-4ã€Qwen2.5ã€Qwen2.5-Instructå’ŒLLaMA-3.2ç­‰ä¸»æµå¼€æºæ¨¡å‹åœ¨æ¨èä»»åŠ¡ä¸Šçš„è¡¨ç°ã€‚æ­¤å¤–ï¼Œç ”ç©¶è¿˜æ·±å…¥æ¢è®¨äº†ä»»åŠ¡ç›¸å…³æ€§ã€ä»»åŠ¡æè¿°åŠæ•°æ®è§„æ¨¡å¯¹å¾®è°ƒæ•ˆæœçš„å½±å“ï¼Œå¹¶ä¸å¤§è§„æ¨¡é—­æºæ¨¡å‹è¿›è¡Œäº†å¯¹æ¯”å®éªŒã€‚ç›®å‰ï¼ŒITDRæ•°æ®é›†ã€å¾®è°ƒåçš„æ¨èå¤§æ¨¡å‹åŠæ‰€æœ‰LoRAæ¨¡å—å‡å·²å¼€æºï¼Œä¸ºæå‡LLMsåœ¨æ¨èé¢†åŸŸçš„å®ç”¨æ€§æä¾›äº†é‡è¦æ”¯æŒã€‚",
      "categories": [
        "cs.IR",
        "cs.AI"
      ],
      "primary_category": "cs.IR",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.05667v2",
      "published_date": "2025-08-01 23:51:17 UTC",
      "updated_date": "2025-12-30 05:37:19 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T08:01:01.688211+00:00"
    },
    {
      "arxiv_id": "2508.01116v3",
      "title": "TensorHyper-VQC: A Tensor-Train-Guided Hypernetwork for Robust and Scalable Variational Quantum Computing",
      "title_zh": "TensorHyper-VQCï¼šé¢å‘ç¨³å¥ä¸”å¯æ‰©å±•å˜åˆ†é‡å­è®¡ç®—çš„å¼ é‡è®­ç»ƒå¼•å¯¼è¶…ç½‘ç»œ",
      "authors": [
        "Jun Qi",
        "Chao-Han Huck Yang",
        "Pin-Yu Chen",
        "Min-Hsiu Hsieh"
      ],
      "abstract": "Variational Quantum Computing (VQC) faces fundamental scalability barriers, primarily due to barren plateaus and sensitivity to quantum noise. To address these challenges, we introduce TensorHyper-VQC, a novel tensor-train (TT)-guided hypernetwork framework that significantly improves the robustness and scalability of VQC. Our framework fully delegates the generation of quantum circuit parameters to a classical TT network, effectively decoupling optimization from quantum hardware. This innovative parameterization mitigates gradient vanishing, enhances noise resilience through structured low-rank representations, and facilitates efficient gradient propagation. Grounded in Neural Tangent Kernel and statistical learning theory, our rigorous theoretical analyses establish strong guarantees on approximation capability, optimization stability, and generalization performance. Extensive empirical results across quantum dot classification, Max-Cut optimization, and molecular quantum simulation tasks demonstrate that TensorHyper-VQC consistently achieves superior performance and robust noise tolerance, including hardware-level validation on a 156-qubit IBM Heron processor. These results position TensorHyper-VQC as a scalable and noise-resilient framework for advancing practical quantum machine learning on near-term devices.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†TensorHyper-VQCï¼Œä¸€ç§åŸºäºTensor-Train (TT)å¼•å¯¼çš„è¶…ç½‘ç»œæ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³Variational Quantum Computing (VQC)åœ¨æ‰©å±•æ€§æ–¹é¢é¢ä¸´çš„Barren Plateauså’Œé‡å­å™ªå£°æ•æ„Ÿæ€§ç­‰æ ¸å¿ƒæŒ‘æˆ˜ã€‚è¯¥æ¡†æ¶å°†é‡å­ç”µè·¯å‚æ•°çš„ç”Ÿæˆå®Œå…¨å§”æ‰˜ç»™ç»å…¸çš„TTç½‘ç»œï¼Œå®ç°äº†ä¼˜åŒ–è¿‡ç¨‹ä¸é‡å­ç¡¬ä»¶çš„æœ‰æ•ˆè§£è€¦ã€‚é€šè¿‡ç»“æ„åŒ–çš„ä½ç§©è¡¨ç¤º(Structured Low-rank Representations)ï¼Œè¯¥æ–¹æ¡ˆä¸ä»…æ˜¾è‘—å¢å¼ºäº†æ¨¡å‹çš„å™ªå£°é²æ£’æ€§ï¼Œè¿˜ç¼“è§£äº†æ¢¯åº¦æ¶ˆå¤±å¹¶ä¿ƒè¿›äº†é«˜æ•ˆçš„æ¢¯åº¦ä¼ æ’­ã€‚ç ”ç©¶ç»“åˆNeural Tangent Kernelå’Œç»Ÿè®¡å­¦ä¹ ç†è®ºï¼Œä¸ºæ¨¡å‹çš„é€¼è¿‘èƒ½åŠ›ã€ä¼˜åŒ–ç¨³å®šæ€§å’Œæ³›åŒ–æ€§èƒ½æä¾›äº†ä¸¥è°¨çš„ç†è®ºä¿è¯ã€‚åœ¨é‡å­ç‚¹åˆ†ç±»ã€Max-Cutä¼˜åŒ–åŠåˆ†å­é‡å­æ¨¡æ‹Ÿä»»åŠ¡ä¸­çš„å®éªŒè¡¨æ˜ï¼ŒTensorHyper-VQCå±•ç°å‡ºä¼˜å¼‚çš„æ€§èƒ½å’Œå™ªå£°å®¹å¿åº¦ï¼Œå¹¶åœ¨156é‡å­æ¯”ç‰¹çš„IBM Heronå¤„ç†å™¨ä¸Šå®Œæˆäº†ç¡¬ä»¶éªŒè¯ã€‚è¯¥æˆæœä¸ºåœ¨è¿‘æœŸé‡å­è®¾å¤‡ä¸Šå®ç°å¯æ‰©å±•ä¸”æŠ—å™ªå£°çš„Practical Quantum Machine Learningæä¾›äº†å¼ºæœ‰åŠ›çš„æ¡†æ¶æ”¯æŒã€‚",
      "categories": [
        "quant-ph",
        "cs.AI",
        "cs.LG",
        "stat.ML"
      ],
      "primary_category": "quant-ph",
      "comment": "The paper has been accepted by npj Quantum Information",
      "pdf_url": "https://arxiv.org/pdf/2508.01116v3",
      "published_date": "2025-08-01 23:37:55 UTC",
      "updated_date": "2026-01-10 02:26:11 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T08:01:04.044396+00:00"
    },
    {
      "arxiv_id": "2508.01109v2",
      "title": "Platonic Representations for Poverty Mapping: Unified Vision-Language Codes or Agent-Induced Novelty?",
      "title_zh": "è´«å›°ç»˜å›¾ä¸­çš„æŸæ‹‰å›¾å¼è¡¨å¾ï¼šç»Ÿä¸€çš„è§†è§‰-è¯­è¨€ç¼–ç ï¼Œè¿˜æ˜¯æ™ºèƒ½ä½“è¯±å¯¼çš„æ–°é¢–æ€§ï¼Ÿ",
      "authors": [
        "Satiyabooshan Murugaboopathy",
        "Connor T. Jerzak",
        "Adel Daoud"
      ],
      "abstract": "We investigate whether socio-economic indicators like household wealth leave recoverable imprints in satellite imagery (capturing physical features) and Internet-sourced text (reflecting historical/economic narratives). Using Demographic and Health Survey (DHS) data from African neighborhoods, we pair Landsat images with LLM-generated textual descriptions conditioned on location/year and text retrieved by an AI search agent from web sources. We develop a multimodal framework predicting household wealth (International Wealth Index) through five pipelines: (i) vision model on satellite images, (ii) LLM using only location/year, (iii) AI agent searching/synthesizing web text, (iv) joint image-text encoder, (v) ensemble of all signals. Our framework yields three contributions. First, fusing vision and agent/LLM text outperforms vision-only baselines in wealth prediction (e.g., R-squared of 0.77 vs. 0.63 on out-of-sample splits), with LLM-internal knowledge proving more effective than agent-retrieved text, improving robustness to out-of-country and out-of-time generalization. Second, we find partial representational convergence: fused embeddings from vision/language modalities correlate moderately (median cosine similarity of 0.60 after alignment), suggesting a shared latent code of material well-being while retaining complementary details, consistent with the Platonic Representation Hypothesis. Although LLM-only text outperforms agent-retrieved data, challenging our Agent-Induced Novelty Hypothesis, modest gains from combining agent data in some splits weakly support the notion that agent-gathered information introduces unique representational structures not fully captured by static LLM knowledge. Third, we release a large-scale multimodal dataset comprising more than 60,000 DHS clusters linked to satellite images, LLM-generated descriptions, and agent-retrieved texts.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†ç¤¾ä¼šç»æµæŒ‡æ ‡æ˜¯å¦åœ¨å«æ˜Ÿå›¾åƒå’Œäº’è”ç½‘æ–‡æœ¬ä¸­ç•™ä¸‹å¯æ¢å¤çš„å°è®°ï¼Œå¹¶æå‡ºäº†ä¸€ä¸ªç»“åˆå«æ˜Ÿå›¾åƒã€å¤§è¯­è¨€æ¨¡å‹(LLM)æ–‡æœ¬å’ŒAIæ™ºèƒ½ä½“(AI agent)æ£€ç´¢ä¿¡æ¯çš„å¤šæ¨¡æ€æ¡†æ¶ï¼Œç”¨äºé¢„æµ‹å›½é™…è´¢å¯ŒæŒ‡æ•°(International Wealth Index)ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œèåˆè§†è§‰ä¸è¯­è¨€ä¿¡å·çš„é¢„æµ‹æ€§èƒ½æ˜¾è‘—ä¼˜äºä»…è§†è§‰çš„åŸºå‡†æ¨¡å‹ï¼Œå…¶å†³å®šç³»æ•°(R-squared)ä»0.63æå‡è‡³0.77ï¼Œä¸”LLMçš„å†…éƒ¨çŸ¥è¯†åœ¨è·¨å›½å’Œè·¨æ—¶é—´æ³›åŒ–ä¸­è¡¨ç°å‡ºæ¯”æ£€ç´¢æ–‡æœ¬æ›´å¼ºçš„é²æ£’æ€§ã€‚ç ”ç©¶å‘ç°è§†è§‰ä¸è¯­è¨€æ¨¡æ€åœ¨å¯¹é½åè¡¨ç°å‡ºæ˜¾è‘—çš„è¡¨ç¤ºæ”¶æ•›(representational convergence)ï¼Œæ”¯æŒäº†æŸæ‹‰å›¾è¡¨ç¤ºå‡è®¾(Platonic Representation Hypothesis)ï¼Œæš—ç¤ºäº†ä¸åŒæ¨¡æ€é—´å­˜åœ¨å…±äº«çš„ç‰©è´¨ç¦ç¥‰æ½œåœ¨ç¼–ç ã€‚è™½ç„¶æ™ºèƒ½ä½“æ£€ç´¢çš„æ•°æ®å¢ç›Šæœ‰é™ï¼Œä½†ä¹Ÿéƒ¨åˆ†æ”¯æŒäº†å…¶èƒ½å¼•å…¥ç‹¬ç‰¹è¡¨ç¤ºç»“æ„çš„å‡è®¾(Agent-Induced Novelty Hypothesis)ã€‚æœ€åï¼Œç ”ç©¶å›¢é˜Ÿå‘å¸ƒäº†ä¸€ä¸ªåŒ…å«è¶…è¿‡60,000ä¸ªDHSé›†ç¾¤çš„å¤§è§„æ¨¡å¤šæ¨¡æ€æ•°æ®é›†ï¼Œæ¶µç›–äº†å«æ˜Ÿå›¾åƒã€LLMç”Ÿæˆæè¿°åŠæ™ºèƒ½ä½“æ£€ç´¢æ–‡æœ¬ï¼Œä¸ºè´«å›°æµ‹ç»˜æä¾›äº†é‡è¦èµ„æºã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "7 figures",
      "pdf_url": "https://arxiv.org/pdf/2508.01109v2",
      "published_date": "2025-08-01 23:07:16 UTC",
      "updated_date": "2025-11-21 14:32:46 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T08:01:15.542865+00:00"
    },
    {
      "arxiv_id": "2508.01105v1",
      "title": "Protecting Student Mental Health with a Context-Aware Machine Learning Framework for Stress Monitoring",
      "title_zh": "åŸºäºä¸Šä¸‹æ–‡æ„ŸçŸ¥æœºå™¨å­¦ä¹ æ¡†æ¶çš„å­¦ç”Ÿå‹åŠ›ç›‘æµ‹ä¸å¿ƒç†å¥åº·ä¿æŠ¤",
      "authors": [
        "Md Sultanul Islam Ovi",
        "Jamal Hossain",
        "Md Raihan Alam Rahi",
        "Fatema Akter"
      ],
      "abstract": "Student mental health is an increasing concern in academic institutions, where stress can severely impact well-being and academic performance. Traditional assessment methods rely on subjective surveys and periodic evaluations, offering limited value for timely intervention. This paper introduces a context-aware machine learning framework for classifying student stress using two complementary survey-based datasets covering psychological, academic, environmental, and social factors. The framework follows a six-stage pipeline involving preprocessing, feature selection (SelectKBest, RFECV), dimensionality reduction (PCA), and training with six base classifiers: SVM, Random Forest, Gradient Boosting, XGBoost, AdaBoost, and Bagging. To enhance performance, we implement ensemble strategies, including hard voting, soft voting, weighted voting, and stacking. Our best models achieve 93.09% accuracy with weighted hard voting on the Student Stress Factors dataset and 99.53% with stacking on the Stress and Well-being dataset, surpassing previous benchmarks. These results highlight the potential of context-integrated, data-driven systems for early stress detection and underscore their applicability in real-world academic settings to support student well-being.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å­¦æœ¯æœºæ„ä¸­å­¦ç”Ÿå¿ƒç†å¥åº·æ—¥ç›Šä¸¥å³»çš„æŒ‘æˆ˜ï¼Œæå‡ºäº†ä¸€ç§ä¸Šä¸‹æ–‡æ„ŸçŸ¥(Context-Aware)çš„æœºå™¨å­¦ä¹ æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³ä¼ ç»Ÿè¯„ä¼°æ–¹æ³•å› ä¸»è§‚æ€§åŠæ»åæ€§è€Œéš¾ä»¥è¿›è¡ŒåŠæ—¶å¹²é¢„çš„é—®é¢˜ã€‚è¯¥æ¡†æ¶é‡‡ç”¨åŒ…å«é¢„å¤„ç†ã€ç‰¹å¾é€‰æ‹©(SelectKBest, RFECV)ã€é™ç»´(PCA)ä»¥åŠåˆ©ç”¨å…­ç§åŸºç¡€åˆ†ç±»å™¨ï¼ˆå¦‚SVM, Random Forest, Gradient Boosting, XGBoost, AdaBoost, Baggingï¼‰è¿›è¡Œè®­ç»ƒçš„å…­é˜¶æ®µæµæ°´çº¿ã€‚ä¸ºäº†è¿›ä¸€æ­¥ä¼˜åŒ–æ€§èƒ½ï¼Œç ”ç©¶å®æ–½äº†ç¡¬æŠ•ç¥¨(hard voting)ã€è½¯æŠ•ç¥¨(soft voting)ã€åŠ æƒæŠ•ç¥¨(weighted voting)åŠå †å (stacking)ç­‰é›†æˆç­–ç•¥ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯¥æ¨¡å‹åœ¨Student Stress Factorsæ•°æ®é›†ä¸Šè¾¾åˆ°äº†93.09%çš„å‡†ç¡®ç‡ï¼Œåœ¨Stress and Well-beingæ•°æ®é›†ä¸Šé€šè¿‡å †å ç­–ç•¥æ›´æ˜¯å®ç°äº†99.53%çš„å‡†ç¡®ç‡ï¼Œæ˜¾è‘—ä¼˜äºç°æœ‰åŸºå‡†ã€‚è¿™ä¸€ç ”ç©¶è¯æ˜äº†æ•°æ®é©±åŠ¨ç³»ç»Ÿåœ¨æ—©æœŸå‹åŠ›æ£€æµ‹ä¸­çš„æœ‰æ•ˆæ€§ï¼Œä¸ºçœŸå®å­¦æœ¯ç¯å¢ƒä¸‹çš„å­¦ç”Ÿå¿ƒç†å¥åº·æ”¯æŒæä¾›äº†é‡è¦å‚è€ƒã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CY"
      ],
      "primary_category": "cs.LG",
      "comment": "6 pages, 3 figures, 3 tables, 1 algorithm. Conference paper",
      "pdf_url": "https://arxiv.org/pdf/2508.01105v1",
      "published_date": "2025-08-01 22:52:25 UTC",
      "updated_date": "2025-08-01 22:52:25 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T08:01:19.655526+00:00"
    },
    {
      "arxiv_id": "2508.01097v1",
      "title": "Multispin Physics of AI Tipping Points and Hallucinations",
      "title_zh": "AI çªå˜ç‚¹ä¸å¹»è§‰çš„å¤šè‡ªæ—‹ç‰©ç†å­¦",
      "authors": [
        "Neil F. Johnson",
        "Frank Yingjie Huo"
      ],
      "abstract": "Output from generative AI such as ChatGPT, can be repetitive and biased. But more worrying is that this output can mysteriously tip mid-response from good (correct) to bad (misleading or wrong) without the user noticing. In 2024 alone, this reportedly caused $67 billion in losses and several deaths. Establishing a mathematical mapping to a multispin thermal system, we reveal a hidden tipping instability at the scale of the AI's 'atom' (basic Attention head). We derive a simple but essentially exact formula for this tipping point which shows directly the impact of a user's prompt choice and the AI's training bias. We then show how the output tipping can get amplified by the AI's multilayer architecture. As well as helping improve AI transparency, explainability and performance, our results open a path to quantifying users' AI risk and legal liabilities.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ç”Ÿæˆå¼äººå·¥æ™ºèƒ½è¾“å‡ºè¿‡ç¨‹ä¸­å¯èƒ½å‘ç”Ÿçš„ä»æ­£ç¡®åˆ°é”™è¯¯çš„çªå˜ï¼ˆTipping Pointsï¼‰é—®é¢˜ï¼Œå»ºç«‹äº†ä¸å¤šè‡ªæ—‹çƒ­ç³»ç»Ÿï¼ˆmultispin thermal systemï¼‰çš„æ•°å­¦æ˜ å°„ã€‚ç ”ç©¶æ­ç¤ºäº†AIåŸºç¡€æ³¨æ„åŠ›å¤´ï¼ˆAttention headï¼‰å±‚é¢ä¸Šå­˜åœ¨çš„éšæ€§ä¸´ç•Œç‚¹ä¸ç¨³å®šæ€§ï¼Œå¹¶æ¨å¯¼å‡ºäº†ä¸€ä¸ªç²¾ç¡®å…¬å¼ï¼Œç›´æ¥é‡åŒ–äº†ç”¨æˆ·æç¤ºè¯ï¼ˆPromptï¼‰é€‰æ‹©ä¸AIè®­ç»ƒåå·®ï¼ˆTraining biasï¼‰å¯¹ç³»ç»Ÿç¨³å®šæ€§çš„å½±å“ã€‚å®éªŒè¿›ä¸€æ­¥å±•ç¤ºäº†è¿™ç§ä¸´ç•Œç‚¹ä¸ç¨³å®šæ€§å¦‚ä½•é€šè¿‡AIçš„å¤šå±‚æ¶æ„ï¼ˆMultilayer architectureï¼‰è¢«æ”¾å¤§ï¼Œä»è€Œå¯¼è‡´ä¸¥é‡çš„å¹»è§‰ç°è±¡ã€‚è¿™äº›ç ”ç©¶æˆæœä¸ä»…ä¸ºæå‡AIçš„é€æ˜åº¦ä¸å¯è§£é‡Šæ€§ï¼ˆExplainabilityï¼‰æä¾›äº†ç†è®ºåŸºç¡€ï¼Œä¹Ÿä¸ºé‡åŒ–è¯„ä¼°AIåº”ç”¨é£é™©åŠæ³•å¾‹è´£ä»»ï¼ˆLegal liabilitiesï¼‰å¼€è¾Ÿäº†æ–°è·¯å¾„ã€‚",
      "categories": [
        "cs.AI",
        "nlin.AO",
        "physics.comp-ph"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.01097v1",
      "published_date": "2025-08-01 22:24:15 UTC",
      "updated_date": "2025-08-01 22:24:15 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T08:01:13.559542+00:00"
    },
    {
      "arxiv_id": "2508.01096v1",
      "title": "Cross-Domain Web Information Extraction at Pinterest",
      "title_zh": "Pinterest è·¨åŸŸç½‘é¡µä¿¡æ¯æŠ½å–",
      "authors": [
        "Michael Farag",
        "Patrick Halina",
        "Andrey Zaytsev",
        "Alekhya Munagala",
        "Imtihan Ahmed",
        "Junhao Wang"
      ],
      "abstract": "The internet offers a massive repository of unstructured information, but it's a significant challenge to convert this into a structured format. At Pinterest, the ability to accurately extract structured product data from e-commerce websites is essential to enhance user experiences and improve content distribution. In this paper, we present Pinterest's system for attribute extraction, which achieves remarkable accuracy and scalability at a manageable cost. Our approach leverages a novel webpage representation that combines structural, visual, and text modalities into a compact form, optimizing it for small model learning. This representation captures each visible HTML node with its text, style and layout information. We show how this allows simple models such as eXtreme Gradient Boosting (XGBoost) to extract attributes more accurately than much more complex Large Language Models (LLMs) such as Generative Pre-trained Transformer (GPT). Our results demonstrate a system that is highly scalable, processing over 1,000 URLs per second, while being 1000 times more cost-effective than the cheapest GPT alternatives.",
      "tldr_zh": "è¯¥ç ”ç©¶ä»‹ç»äº† Pinterest ç”¨äºè·¨åŸŸç½‘é¡µä¿¡æ¯æå–çš„å±æ€§æå–ç³»ç»Ÿï¼Œæ—¨åœ¨ä»ç”µå­å•†åŠ¡ç½‘ç«™ä¸­é«˜æ•ˆè·å–ç»“æ„åŒ–äº§å“æ•°æ®ã€‚å…¶æ ¸å¿ƒæ–¹æ³•æ˜¯é‡‡ç”¨ä¸€ç§æ–°é¢–çš„ç½‘é¡µè¡¨ç¤ºå½¢å¼ï¼Œå°†ç»“æ„ã€è§†è§‰å’Œæ–‡æœ¬æ¨¡æ€ç»“åˆæˆç´§å‡‘æ ¼å¼ï¼Œå¹¶é’ˆå¯¹å°æ¨¡å‹å­¦ä¹ è¿›è¡Œäº†ä¼˜åŒ–ã€‚è¯¥ç³»ç»Ÿé€šè¿‡æ•è· HTML èŠ‚ç‚¹åŠå…¶æ–‡æœ¬ã€æ ·å¼å’Œå¸ƒå±€ä¿¡æ¯ï¼Œä½¿ eXtreme Gradient Boosting (XGBoost) ç­‰ç®€å•æ¨¡å‹åœ¨å‡†ç¡®ç‡ä¸Šè¶…è¶Šäº† Generative Pre-trained Transformer (GPT) ç­‰å¤æ‚çš„å¤§è¯­è¨€æ¨¡å‹ (LLMs)ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯¥ç³»ç»Ÿå…·æœ‰æé«˜çš„å¯æ‰©å±•æ€§ï¼Œæ¯ç§’å¯å¤„ç†è¶…è¿‡ 1,000 ä¸ª URLã€‚æ­¤å¤–ï¼Œå…¶æˆæœ¬æ•ˆç›Šæ¯”ç›®å‰æœ€å»‰ä»·çš„ GPT æ›¿ä»£æ–¹æ¡ˆé«˜å‡º 1,000 å€ï¼Œä¸ºå¤§è§„æ¨¡ Web æ•°æ®ç»“æ„åŒ–æä¾›äº†å…¼å…·ç²¾åº¦ä¸ç»æµæ€§çš„æŠ€æœ¯æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.IR"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.01096v1",
      "published_date": "2025-08-01 22:22:35 UTC",
      "updated_date": "2025-08-01 22:22:35 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T08:01:22.283146+00:00"
    },
    {
      "arxiv_id": "2508.03745v1",
      "title": "Tobler's First Law in GeoAI: A Spatially Explicit Deep Learning Model for Terrain Feature Detection Under Weak Supervision",
      "title_zh": "GeoAI ä¸­çš„ Tobler ç¬¬ä¸€å®šå¾‹ï¼šä¸€ç§ç”¨äºå¼±ç›‘ç£åœ°å½¢ç‰¹å¾æ£€æµ‹çš„ç©ºé—´æ˜¾å¼æ·±åº¦å­¦ä¹ æ¨¡å‹",
      "authors": [
        "Wenwen Li",
        "Chia-Yu Hsu",
        "Maosheng Hu"
      ],
      "abstract": "Recent interest in geospatial artificial intelligence (GeoAI) has fostered a wide range of applications using artificial intelligence (AI), especially deep learning, for geospatial problem solving. However, major challenges such as a lack of training data and the neglect of spatial principles and spatial effects in AI model design remain, significantly hindering the in-depth integration of AI with geospatial research. This paper reports our work in developing a deep learning model that enables object detection, particularly of natural features, in a weakly supervised manner. Our work makes three contributions: First, we present a method of object detection using only weak labels. This is achieved by developing a spatially explicit model based on Tobler's first law of geography. Second, we incorporate attention maps into the object detection pipeline and develop a multistage training strategy to improve performance. Third, we apply this model to detect impact craters on Mars, a task that previously required extensive manual effort. The model generalizes to both natural and human-made features on the surfaces of Earth and other planets. This research advances the theoretical and methodological foundations of GeoAI.",
      "tldr_zh": "åœ°ç†ç©ºé—´äººå·¥æ™ºèƒ½ (GeoAI) é¢†åŸŸç›®å‰é¢ä¸´è®­ç»ƒæ•°æ®åŒ®ä¹ä»¥åŠåœ¨æ·±åº¦å­¦ä¹ æ¨¡å‹è®¾è®¡ä¸­å¿½è§†åœ°ç†ç©ºé—´åŸç†çš„æŒ‘æˆ˜ã€‚æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§ç©ºé—´æ˜¾å¼ (spatially explicit) æ·±åº¦å­¦ä¹ æ¨¡å‹ï¼Œæ—¨åœ¨ä»¥å¼±ç›‘ç£ (weakly supervised) çš„æ–¹å¼å®ç°åœ°å½¢ç‰¹å¾çš„ç›®æ ‡æ£€æµ‹ (object detection)ã€‚è¯¥æ¨¡å‹é€šè¿‡å°†åœ°ç†å­¦ç¬¬ä¸€å®šå¾‹ (Tobler's first law of geography) èå…¥æ¶æ„ï¼Œå¹¶ç»“åˆæ³¨æ„åŠ›å›¾ (attention maps) ä¸å¤šé˜¶æ®µè®­ç»ƒç­–ç•¥ (multistage training strategy)ï¼Œå®ç°äº†ä»…åˆ©ç”¨å¼±æ ‡ç­¾çš„é«˜æ•ˆå­¦ä¹ ã€‚å®éªŒæˆåŠŸå°†è¯¥æ¨¡å‹åº”ç”¨äºç«æ˜Ÿæ’å‡»å‘ (impact craters) çš„è‡ªåŠ¨åŒ–æ£€æµ‹ï¼Œå¹¶è¯æ˜å…¶èƒ½å¹¿æ³›æ¨å¹¿è‡³åœ°çƒåŠå…¶ä»–è¡Œæ˜Ÿçš„è‡ªç„¶ä¸äººé€ ç‰¹å¾è¯†åˆ«ã€‚è¯¥ç ”ç©¶ä¸ä»…é™ä½äº†å¯¹å¤§è§„æ¨¡æ ‡æ³¨æ•°æ®çš„ä¾èµ–ï¼Œè¿˜ä¸º GeoAI çš„ç†è®ºä¸æ–¹æ³•è®ºæ·±åº¦èåˆæä¾›äº†æ–°çš„è·¯å¾„ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.03745v1",
      "published_date": "2025-08-01 21:47:50 UTC",
      "updated_date": "2025-08-01 21:47:50 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T08:01:20.654761+00:00"
    },
    {
      "arxiv_id": "2508.01084v1",
      "title": "Provably Secure Retrieval-Augmented Generation",
      "title_zh": "å¯è¯æ˜å®‰å…¨çš„æ£€ç´¢å¢å¼ºç”Ÿæˆ",
      "authors": [
        "Pengcheng Zhou",
        "Yinglun Feng",
        "Zhongliang Yang"
      ],
      "abstract": "Although Retrieval-Augmented Generation (RAG) systems have been widely applied, the privacy and security risks they face, such as data leakage and data poisoning, have not been systematically addressed yet. Existing defense strategies primarily rely on heuristic filtering or enhancing retriever robustness, which suffer from limited interpretability, lack of formal security guarantees, and vulnerability to adaptive attacks. To address these challenges, this paper proposes the first provably secure framework for RAG systems(SAG). Our framework employs a pre-storage full-encryption scheme to ensure dual protection of both retrieved content and vector embeddings, guaranteeing that only authorized entities can access the data. Through formal security proofs, we rigorously verify the scheme's confidentiality and integrity under a computational security model. Extensive experiments across multiple benchmark datasets demonstrate that our framework effectively resists a range of state-of-the-art attacks. This work establishes a theoretical foundation and practical paradigm for verifiably secure RAG systems, advancing AI-powered services toward formally guaranteed security.",
      "tldr_zh": "æœ¬ç ”ç©¶é’ˆå¯¹æ£€ç´¢å¢å¼ºç”Ÿæˆ (RAG) ç³»ç»Ÿé¢ä¸´çš„æ•°æ®æ³„éœ²å’Œæ•°æ®æŠ•æ¯’ç­‰å®‰å…¨é£é™©ï¼Œæå‡ºäº†é¦–ä¸ªå¯è¯æ˜å®‰å…¨æ¡†æ¶ SAGã€‚è¯¥æ¡†æ¶é‡‡ç”¨å­˜å‚¨å‰å…¨åŠ å¯†æ–¹æ¡ˆ (pre-storage full-encryption scheme)ï¼Œå¯¹æ£€ç´¢å†…å®¹å’Œå‘é‡åµŒå…¥ (vector embeddings) è¿›è¡ŒåŒé‡ä¿æŠ¤ï¼Œç¡®ä¿ä»…æˆæƒå®ä½“å¯è®¿é—®ã€‚é€šè¿‡å½¢å¼åŒ–å®‰å…¨è¯æ˜ (formal security proofs)ï¼Œç ”ç©¶åœ¨è®¡ç®—å®‰å…¨æ¨¡å‹ä¸‹ä¸¥è°¨åœ°éªŒè¯äº†æ–¹æ¡ˆçš„æœºå¯†æ€§ä¸å®Œæ•´æ€§ã€‚åœ¨å¤šä¸ªåŸºå‡†æ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ¡†æ¶èƒ½æœ‰æ•ˆæŠµå¾¡å¤šç§æœ€å…ˆè¿›çš„æ”»å‡»æ‰‹æ®µã€‚è¯¥å·¥ä½œä¸ºæ„å»ºå¯éªŒè¯å®‰å…¨çš„ RAG ç³»ç»Ÿå¥ å®šäº†ç†è®ºåŸºç¡€ä¸å®è·µèŒƒå¼ï¼Œä¿ƒè¿›äº†å…·å¤‡å½¢å¼åŒ–å®‰å…¨ä¿éšœçš„äººå·¥æ™ºèƒ½æœåŠ¡çš„å‘å±•ã€‚",
      "categories": [
        "cs.CR",
        "cs.AI"
      ],
      "primary_category": "cs.CR",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.01084v1",
      "published_date": "2025-08-01 21:37:16 UTC",
      "updated_date": "2025-08-01 21:37:16 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T08:01:27.196944+00:00"
    },
    {
      "arxiv_id": "2508.01082v2",
      "title": "Learning Pivoting Manipulation with Force and Vision Feedback Using Optimization-based Demonstrations",
      "title_zh": "åŸºäºä¼˜åŒ–æ¼”ç¤ºçš„åŠ›ä¸è§†è§‰åé¦ˆæ—‹è½¬æ“æ§å­¦ä¹ ",
      "authors": [
        "Yuki Shirai",
        "Kei Ota",
        "Devesh K. Jha",
        "Diego Romeres"
      ],
      "abstract": "Non-prehensile manipulation is challenging due to complex contact interactions between objects, the environment, and robots. Model-based approaches can efficiently generate complex trajectories of robots and objects under contact constraints. However, they tend to be sensitive to model inaccuracies and require access to privileged information (e.g., object mass, size, pose), making them less suitable for novel objects. In contrast, learning-based approaches are typically more robust to modeling errors but require large amounts of data. In this paper, we bridge these two approaches to propose a framework for learning closed-loop pivoting manipulation. By leveraging computationally efficient Contact-Implicit Trajectory Optimization (CITO), we design demonstration-guided deep Reinforcement Learning (RL), leading to sample-efficient learning. We also present a sim-to-real transfer approach using a privileged training strategy, enabling the robot to perform pivoting manipulation using only proprioception, vision, and force sensing without access to privileged information. Our method is evaluated on several pivoting tasks, demonstrating that it can successfully perform sim-to-real transfer. The overview of our method and the hardware experiments are shown at https://youtu.be/akjGDgfwLbM?si=QVw6ExoPy2VsU2g6",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹éæŠ“å–å¼æ“çºµï¼ˆNon-prehensile manipulationï¼‰ä¸­å¤æ‚çš„æ¥è§¦äº¤äº’æŒ‘æˆ˜ï¼Œæå‡ºäº†ä¸€ç§ç»“åˆåŠ›å’Œè§†è§‰åé¦ˆçš„å­¦ä¹ æ¡†æ¶ï¼Œæ—¨åœ¨å®ç°é—­ç¯çš„è½¬åŠ¨æ“çºµï¼ˆPivoting Manipulationï¼‰ã€‚è¯¥æ–¹æ³•é€šè¿‡æ•´åˆè®¡ç®—æ•ˆç‡é«˜çš„æ¥è§¦éšå¼è½¨è¿¹ä¼˜åŒ–ï¼ˆContact-Implicit Trajectory Optimization, CITOï¼‰æ¥å¼•å¯¼æ·±åº¦å¼ºåŒ–å­¦ä¹ ï¼ˆReinforcement Learning, RLï¼‰ï¼Œå®ç°äº†é«˜é‡‡æ ·æ•ˆç‡çš„å­¦ä¹ è¿‡ç¨‹ã€‚ä¸ºäº†è§£å†³ä»¿çœŸä¸ç°å®çš„å·®è·ï¼Œç ”ç©¶é‡‡ç”¨äº†ä¸€ç§ç‰¹æƒè®­ç»ƒç­–ç•¥ï¼ˆPrivileged Training Strategyï¼‰è¿›è¡ŒSim-to-realè¿ç§»ï¼Œä½¿æœºå™¨äººä»…å‡­æœ¬ä½“æ„Ÿå—ã€è§†è§‰å’ŒåŠ›ä¼ æ„Ÿå³å¯æ‰§è¡Œæ“çºµï¼Œæ— éœ€è·å–ç‰©ä½“è´¨é‡æˆ–ç²¾ç¡®å§¿æ€ç­‰ç‰¹æƒä¿¡æ¯ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å¤šç§è½¬åŠ¨æ“çºµä»»åŠ¡ä¸­å‡å–å¾—äº†æˆåŠŸï¼Œè¯æ˜äº†å…¶åœ¨åº”å¯¹æ¨¡å‹ä¸ç¡®å®šæ€§å’Œæ–°ç‰©ä½“æ“çºµæ—¶çš„é²æ£’æ€§ä¸æœ‰æ•ˆæ€§ã€‚",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.LG",
        "eess.SY"
      ],
      "primary_category": "cs.RO",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.01082v2",
      "published_date": "2025-08-01 21:33:46 UTC",
      "updated_date": "2025-08-05 23:03:27 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T08:01:42.283582+00:00"
    },
    {
      "arxiv_id": "2508.01077v1",
      "title": "The Lattice Geometry of Neural Network Quantization -- A Short Equivalence Proof of GPTQ and Babai's algorithm",
      "title_zh": "ç¥ç»ç½‘ç»œé‡åŒ–çš„æ ¼å‡ ä½•ï¼šGPTQ ä¸ Babai ç®—æ³•çš„ç®€æ˜ç­‰ä»·æ€§è¯æ˜",
      "authors": [
        "Johann Birnick"
      ],
      "abstract": "We explain how data-driven quantization of a linear unit in a neural network corresponds to solving the closest vector problem for a certain lattice generated by input data. We prove that the GPTQ algorithm is equivalent to Babai's well-known nearest-plane algorithm. We furthermore provide geometric intuition for both algorithms. Lastly, we note the consequences of these results, in particular hinting at the possibility for using lattice basis reduction for better quantization.",
      "tldr_zh": "è¯¥ç ”ç©¶é˜è¿°äº†ç¥ç»ç½‘ç»œä¸­çº¿æ€§å•å…ƒçš„æ•°æ®é©±åŠ¨é‡åŒ–(quantization)æœ¬è´¨ä¸Šå¯¹åº”äºè§£å†³ç”±è¾“å…¥æ•°æ®ç”Ÿæˆçš„ç‰¹å®šæ ¼(lattice)çš„æœ€çŸ­å‘é‡é—®é¢˜(Closest Vector Problem, CVP)ã€‚ä½œè€…è¯æ˜äº†å¹¿æ³›åº”ç”¨çš„GPTQç®—æ³•ä¸ç»å…¸çš„Babaiæœ€è¿‘å¹³é¢ç®—æ³•(Babai's nearest-plane algorithm)åœ¨æ•°å­¦ä¸Šæ˜¯ç­‰ä»·çš„ï¼Œå¹¶ä¸ºè¿™ä¸¤ç§ç®—æ³•æä¾›äº†ç›´è§‚çš„å‡ ä½•è§£é‡Šã€‚é€šè¿‡å»ºç«‹è¿™ç§è”ç³»ï¼Œè®ºæ–‡æ·±å…¥æ¢è®¨äº†ç®—æ³•èƒŒåçš„æ ¼å‡ ä½•å±æ€§ï¼Œå¹¶æŒ‡å‡ºåˆ©ç”¨æ ¼åŸºè§„çº¦(lattice basis reduction)æŠ€æœ¯æ¥è¿›ä¸€æ­¥ä¼˜åŒ–é‡åŒ–æ•ˆæœçš„å¯èƒ½æ€§ã€‚è¿™é¡¹å·¥ä½œä¸ä»…ä¸ºç†è§£æ·±åº¦å­¦ä¹ æ¨¡å‹çš„å‹ç¼©æœºåˆ¶æä¾›äº†æ–°çš„ç†è®ºè§†è§’ï¼Œä¹Ÿä¸ºæœªæ¥å¼€å‘æ›´é«˜æ€§èƒ½çš„é‡åŒ–ç®—æ³•æŒ‡æ˜äº†æ–¹å‘ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "9 pages, 4 figures",
      "pdf_url": "https://arxiv.org/pdf/2508.01077v1",
      "published_date": "2025-08-01 21:20:58 UTC",
      "updated_date": "2025-08-01 21:20:58 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T08:01:33.255532+00:00"
    },
    {
      "arxiv_id": "2508.02732v1",
      "title": "A Note on Code Quality Score: LLMs for Maintainable Large Codebases",
      "title_zh": "å…³äºä»£ç è´¨é‡è¯„åˆ†çš„æ¢è®¨ï¼šé¢å‘å¯ç»´æŠ¤å¤§å‹ä»£ç åº“çš„å¤§è¯­è¨€æ¨¡å‹",
      "authors": [
        "Sherman Wong",
        "Jalaj Bhandari",
        "Leo Zhou Fan Yang",
        "Xylan Xu",
        "Yi Zhuang",
        "Cem Cayiroglu",
        "Payal Bhuptani",
        "Sheela Yadawad",
        "Hung Duong"
      ],
      "abstract": "Maintaining code quality in large-scale software systems presents significant challenges, particularly in settings where a large numbers of engineers work concurrently on a codebase. This paper introduces Code Quality Score (CQS) system to automatically detect issues with a set of code changes and provide actionable insights. At its core, the CQS system is powered by two Llama3 models, fine-tuned (with SFT and offline RL approaches), to a) detect common code quality issues related to coding best practices and b) to provide good ``critiques'' for LLM-generated code review respectively. To maintain good user experience, we layer the system with hand-crafted rules to filter out incorrect responses/hallucinations. Offline evaluations show that our CQS system is able to achieve an impressive precision rate for identifying valid issues. This system has already been rolled out to developers in an industrial scale setting and has consistently achieved 60\\% week over week user helpfulness rate, demonstrating its effectiveness in a real-world environment. In this paper, we present details of the CQS system along with some learnings on curating developer feedback to create training data for LLM fine-tuning.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤§è§„æ¨¡è½¯ä»¶ç³»ç»Ÿä¸­å¤šåå·¥ç¨‹å¸ˆåä½œå¯¼è‡´çš„ä»£ç è´¨é‡ç»´æŠ¤éš¾é¢˜ï¼Œæå‡ºäº†ä»£ç è´¨é‡å¾—åˆ†ç³»ç»Ÿ (Code Quality Score, CQS) ä»¥å®ç°è‡ªåŠ¨åŒ–çš„é—®é¢˜æ£€æµ‹ä¸å»ºè®®ã€‚è¯¥ç³»ç»Ÿçš„æ ¸å¿ƒç”±ä¸¤ä¸ªç»è¿‡å¾®è°ƒçš„ Llama3 æ¨¡å‹é©±åŠ¨ï¼Œåˆ†åˆ«åˆ©ç”¨ç›‘ç£å¾®è°ƒ (SFT) å’Œç¦»çº¿å¼ºåŒ–å­¦ä¹  (RL) æ–¹æ³•æ¥è¯†åˆ«è¿åæœ€ä½³å®è·µçš„ä»£ç é—®é¢˜ï¼Œå¹¶ä¸ºå¤§è¯­è¨€æ¨¡å‹ (LLM) ç”Ÿæˆçš„ä»£ç å®¡æŸ¥æä¾›é«˜è´¨é‡çš„æ‰¹è¯„å»ºè®®ã€‚ä¸ºäº†ç¡®ä¿ç”¨æˆ·ä½“éªŒï¼Œç³»ç»Ÿç»“åˆäº†æ‰‹å·¥è§„åˆ™æ¥è¿‡æ»¤æ¨¡å‹å¹»è§‰åŠé”™è¯¯å“åº”ï¼Œç¡®ä¿äº†æé«˜çš„è¯†åˆ«å‡†ç¡®ç‡ (Precision)ã€‚ç›®å‰ CQS å·²åœ¨å·¥ä¸šè§„æ¨¡çš„ç¯å¢ƒä¸­éƒ¨ç½²ï¼Œå¹¶æŒç»­è·å¾— 60% çš„å‘¨åº¦ç”¨æˆ·æœ‰ç”¨æ€§åé¦ˆï¼Œè¯æ˜äº†å…¶åœ¨çœŸå®ä¸–ç•Œç¯å¢ƒä¸­çš„æœ‰æ•ˆæ€§ã€‚æ­¤å¤–ï¼Œè®ºæ–‡è¿˜åˆ†äº«äº†å¦‚ä½•é€šè¿‡æ•´ç†å¼€å‘è€…åé¦ˆæ¥æ„å»º LLM å¾®è°ƒè®­ç»ƒæ•°æ®çš„å®è·µç»éªŒã€‚",
      "categories": [
        "cs.SE",
        "cs.AI"
      ],
      "primary_category": "cs.SE",
      "comment": "24 pages, ICLR format",
      "pdf_url": "https://arxiv.org/pdf/2508.02732v1",
      "published_date": "2025-08-01 21:09:45 UTC",
      "updated_date": "2025-08-01 21:09:45 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T08:01:56.656067+00:00"
    },
    {
      "arxiv_id": "2508.01073v1",
      "title": "gpuRDF2vec -- Scalable GPU-based RDF2vec",
      "title_zh": "gpuRDF2vecï¼šå¯æ‰©å±•çš„åŸºäº GPU çš„ RDF2vec",
      "authors": [
        "Martin BÃ¶ckling",
        "Heiko Paulheim"
      ],
      "abstract": "Generating Knowledge Graph (KG) embeddings at web scale remains challenging. Among existing techniques, RDF2vec combines effectiveness with strong scalability. We present gpuRDF2vec, an open source library that harnesses modern GPUs and supports multi-node execution to accelerate every stage of the RDF2vec pipeline. Extensive experiments on both synthetically generated graphs and real-world benchmarks show that gpuRDF2vec achieves up to a substantial speedup over the currently fastest alternative, i.e., jRDF2vec. In a single-node setup, our walk-extraction phase alone outperforms pyRDF2vec, SparkKGML, and jRDF2vec by a substantial margin using random walks on large/ dense graphs, and scales very well to longer walks, which typically lead to better quality embeddings. Our implementation of gpuRDF2vec enables practitioners and researchers to train high-quality KG embeddings on large-scale graphs within practical time budgets and builds on top of Pytorch Lightning for the scalable word2vec implementation.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†gpuRDF2vecï¼Œè¿™æ˜¯ä¸€ä¸ªåˆ©ç”¨ç°ä»£GPUå’Œå¤šèŠ‚ç‚¹æ‰§è¡Œæ¥åŠ é€ŸRDF2vecæµæ°´çº¿å„é˜¶æ®µçš„å¼€æºåº“ã€‚é’ˆå¯¹åœ¨å¤§è§„æ¨¡çŸ¥è¯†å›¾è°±(Knowledge Graph)ä¸­ç”ŸæˆåµŒå…¥(embeddings)çš„æ•ˆç‡æŒ‘æˆ˜ï¼Œè¯¥å·¥å…·é€šè¿‡GPUå¹¶è¡ŒåŒ–æ˜¾è‘—æå‡äº†å¤„ç†é€Ÿåº¦ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒgpuRDF2vecåœ¨æ€§èƒ½ä¸Šå¤§å¹…è¶…è¶Šäº†jRDF2vecã€pyRDF2vecå’ŒSparkKGMLç­‰ç°æœ‰æ–¹æ¡ˆï¼Œç‰¹åˆ«æ˜¯åœ¨å¤„ç†å¤§è§„æ¨¡ç¨ å¯†å›¾çš„walk-extractioné˜¶æ®µè¡¨ç°ä¼˜å¼‚ã€‚è¯¥å®ç°åŸºäºPytorch Lightningæ„å»ºäº†å¯æ‰©å±•çš„word2vecæ¨¡å‹ï¼Œèƒ½å¤Ÿæ”¯æŒæ›´é•¿çš„éšæœºæ¸¸èµ°(random walks)ä»¥ç”Ÿæˆæ›´é«˜è´¨é‡çš„åµŒå…¥ã€‚gpuRDF2vecä½¿ä»ä¸šè€…å’Œç ”ç©¶äººå‘˜èƒ½å¤Ÿåœ¨å®é™…çš„æ—¶é—´é¢„ç®—å†…ï¼Œåœ¨è¶…å¤§è§„æ¨¡å›¾ä¸Šè®­ç»ƒå‡ºé«˜è´¨é‡çš„KG embeddingsã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "18 pages, ISWC 2025",
      "pdf_url": "https://arxiv.org/pdf/2508.01073v1",
      "published_date": "2025-08-01 21:07:31 UTC",
      "updated_date": "2025-08-01 21:07:31 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T08:02:03.743073+00:00"
    },
    {
      "arxiv_id": "2508.01067v1",
      "title": "Expressive Power of Graph Transformers via Logic",
      "title_zh": "åŸºäºé€»è¾‘çš„å›¾ Transformer è¡¨è¾¾èƒ½åŠ›",
      "authors": [
        "Veeti Ahvonen",
        "Maurice Funk",
        "Damian Heiman",
        "Antti Kuusisto",
        "Carsten Lutz"
      ],
      "abstract": "Transformers are the basis of modern large language models, but relatively little is known about their precise expressive power on graphs. We study the expressive power of graph transformers (GTs) by Dwivedi and Bresson (2020) and GPS-networks by RampÃ¡sek et al. (2022), both under soft-attention and average hard-attention. Our study covers two scenarios: the theoretical setting with real numbers and the more practical case with floats. With reals, we show that in restriction to vertex properties definable in first-order logic (FO), GPS-networks have the same expressive power as graded modal logic (GML) with the global modality. With floats, GPS-networks turn out to be equally expressive as GML with the counting global modality. The latter result is absolute, not restricting to properties definable in a background logic. We also obtain similar characterizations for GTs in terms of propositional logic with the global modality (for reals) and the counting global modality (for floats).",
      "tldr_zh": "è¯¥ç ”ç©¶æ·±å…¥æ¢è®¨äº† Dwivedi å’Œ Bresson æå‡ºçš„ Graph Transformers (GTs) ä»¥åŠ RampÃ¡sek ç­‰äººæå‡ºçš„ GPS-networks åœ¨å›¾æ•°æ®ä¸Šçš„ç²¾ç¡®è¡¨è¾¾èƒ½åŠ›ã€‚ç ”ç©¶æ¶µç›–äº† soft-attention å’Œ average hard-attention æœºåˆ¶ï¼Œå¹¶åŒºåˆ†äº†å®æ•° (reals) ç†è®ºè®¾å®šä¸æµ®ç‚¹æ•° (floats) å®é™…åº”ç”¨åœºæ™¯ã€‚åœ¨å®æ•°è®¾å®šä¸‹ï¼Œé’ˆå¯¹ first-order logic (FO) å¯å®šä¹‰çš„é¡¶ç‚¹å±æ€§ï¼ŒGPS-networks çš„è¡¨è¾¾èƒ½åŠ›è¢«è¯æ˜ç­‰åŒäºå¸¦æœ‰ global modality çš„ Graded Modal Logic (GML)ã€‚è€Œåœ¨æ›´å…·å®é™…æ„ä¹‰çš„æµ®ç‚¹æ•°åœºæ™¯ä¸­ï¼ŒGPS-networks ä¸å¸¦æœ‰ counting global modality çš„ GML å…·æœ‰åŒç­‰è¡¨è¾¾åŠ›ï¼Œä¸”è¯¥ç»“è®ºå…·æœ‰ç»å¯¹æ€§ï¼Œä¸å±€é™äºç‰¹å®šçš„èƒŒæ™¯é€»è¾‘ã€‚æ­¤å¤–ï¼Œç ”ç©¶è¿˜ä¸º GTs æä¾›äº†ç±»ä¼¼çš„é€»è¾‘ç‰¹å¾æè¿°ï¼Œåˆ†åˆ«å¯¹åº”ä¸åŒæ•°å€¼ç²¾åº¦ä¸‹çš„ global modality å’Œ counting global modalityã€‚è¿™äº›å‘ç°ä»é€»è¾‘å­¦è§†è§’é‡åŒ–äº†ä¸»æµå›¾å˜æ¢å™¨æ¶æ„åœ¨å¤„ç†ç»“æ„åŒ–æ•°æ®æ—¶çš„ç†è®ºè¾¹ç•Œã€‚",
      "categories": [
        "cs.LO",
        "cs.AI"
      ],
      "primary_category": "cs.LO",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.01067v1",
      "published_date": "2025-08-01 20:59:13 UTC",
      "updated_date": "2025-08-01 20:59:13 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T08:02:15.287589+00:00"
    },
    {
      "arxiv_id": "2508.08285v2",
      "title": "The Illusion of Progress: Re-evaluating Hallucination Detection in LLMs",
      "title_zh": "è¿›æ­¥çš„é”™è§‰ï¼šé‡æ–°å®¡è§†å¤§è¯­è¨€æ¨¡å‹ä¸­çš„å¹»è§‰æ£€æµ‹",
      "authors": [
        "Denis Janiak",
        "Jakub Binkowski",
        "Albert Sawczyn",
        "Bogdan Gabrys",
        "Ravid Shwartz-Ziv",
        "Tomasz Kajdanowicz"
      ],
      "abstract": "Large language models (LLMs) have revolutionized natural language processing, yet their tendency to hallucinate poses serious challenges for reliable deployment. Despite numerous hallucination detection methods, their evaluations often rely on ROUGE, a metric based on lexical overlap that misaligns with human judgments. Through comprehensive human studies, we demonstrate that while ROUGE exhibits high recall, its extremely low precision leads to misleading performance estimates. In fact, several established detection methods show performance drops of up to 45.9\\% when assessed using human-aligned metrics like LLM-as-Judge. Moreover, our analysis reveals that simple heuristics based on response length can rival complex detection techniques, exposing a fundamental flaw in current evaluation practices. We argue that adopting semantically aware and robust evaluation frameworks is essential to accurately gauge the true performance of hallucination detection methods, ultimately ensuring the trustworthiness of LLM outputs.",
      "tldr_zh": "è¯¥ç ”ç©¶å¯¹å¤§è¯­è¨€æ¨¡å‹(LLMs)ä¸­çš„å¹»è§‰æ£€æµ‹(Hallucination Detection)è¯„ä¼°ä½“ç³»è¿›è¡Œäº†é‡æ–°å®¡è§†ï¼ŒæŒ‡å‡ºç›®å‰å¹¿æ³›ä½¿ç”¨çš„ROUGEæŒ‡æ ‡å› è¿‡äºä¾èµ–è¯æ±‡é‡å è€Œä¸äººç±»åˆ¤æ–­ä¸¥é‡è„±èŠ‚ã€‚é€šè¿‡æ·±å…¥çš„äººç±»ç ”ç©¶ï¼Œä½œè€…è¯å®ROUGEè™½ç„¶å…·æœ‰é«˜å¬å›ç‡ï¼Œä½†å…¶æä½çš„ç²¾ç¡®åº¦å¯¼è‡´äº†å¯¹æ£€æµ‹æ€§èƒ½çš„è¯¯å¯¼æ€§ä¼°ç®—ã€‚å®éªŒè¡¨æ˜ï¼Œå½“é‡‡ç”¨LLM-as-Judgeç­‰è¯­ä¹‰ä¸€è‡´æ€§æ›´é«˜çš„æŒ‡æ ‡è¿›è¡Œè¯„ä¼°æ—¶ï¼Œç°æœ‰ä¸»æµå¹»è§‰æ£€æµ‹æ–¹æ³•çš„æ€§èƒ½è¡¨ç°é™å¹…é«˜è¾¾45.9%ã€‚ç ”ç©¶è¿˜è¿›ä¸€æ­¥å‘ç°ï¼ŒåŸºäºå“åº”é•¿åº¦çš„ç®€å•å¯å‘å¼æ–¹æ³•(Heuristics)å…¶è¡¨ç°ç«Ÿç„¶èƒ½ä¸å¤æ‚çš„æ£€æµ‹æŠ€æœ¯ç›¸åª²ç¾ï¼Œæ­ç¤ºäº†å½“å‰è¯„ä¼°å®è·µä¸­å­˜åœ¨çš„æ ¹æœ¬æ€§ç¼ºé™·ã€‚å› æ­¤ï¼Œä½œè€…å¼ºè°ƒå¿…é¡»é‡‡ç”¨å…·å¤‡è¯­ä¹‰æ„ŸçŸ¥èƒ½åŠ›ä¸”é²æ£’çš„è¯„ä¼°æ¡†æ¶ï¼Œæ‰èƒ½å‡†ç¡®è¡¡é‡æ£€æµ‹æŠ€æœ¯çš„çœŸå®æ°´å¹³ï¼Œå¹¶æœ€ç»ˆç¡®ä¿LLMè¾“å‡ºçš„å¯ä¿¡åº¦ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "Preprint, under review",
      "pdf_url": "https://arxiv.org/pdf/2508.08285v2",
      "published_date": "2025-08-01 20:34:01 UTC",
      "updated_date": "2025-08-13 22:09:11 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T08:02:20.845449+00:00"
    },
    {
      "arxiv_id": "2508.05666v1",
      "title": "HySemRAG: A Hybrid Semantic Retrieval-Augmented Generation Framework for Automated Literature Synthesis and Methodological Gap Analysis",
      "title_zh": "HySemRAGï¼šé¢å‘è‡ªåŠ¨åŒ–æ–‡çŒ®ç»¼è¿°ä¸æ–¹æ³•è®ºç©ºç™½åˆ†æçš„æ··åˆè¯­ä¹‰æ£€ç´¢å¢å¼ºç”Ÿæˆæ¡†æ¶",
      "authors": [
        "Alejandro Godinez"
      ],
      "abstract": "We present HySemRAG, a framework that combines Extract, Transform, Load (ETL) pipelines with Retrieval-Augmented Generation (RAG) to automate large-scale literature synthesis and identify methodological research gaps. The system addresses limitations in existing RAG architectures through a multi-layered approach: hybrid retrieval combining semantic search, keyword filtering, and knowledge graph traversal; an agentic self-correction framework with iterative quality assurance; and post-hoc citation verification ensuring complete traceability. Our implementation processes scholarly literature through eight integrated stages: multi-source metadata acquisition, asynchronous PDF retrieval, custom document layout analysis using modified Docling architecture, bibliographic management, LLM-based field extraction, topic modeling, semantic unification, and knowledge graph construction. The system creates dual data products - a Neo4j knowledge graph enabling complex relationship queries and Qdrant vector collections supporting semantic search - serving as foundational infrastructure for verifiable information synthesis. Evaluation across 643 observations from 60 testing sessions demonstrates structured field extraction achieving 35.1% higher semantic similarity scores (0.655 $\\pm$ 0.178) compared to PDF chunking approaches (0.485 $\\pm$ 0.204, p < 0.000001). The agentic quality assurance mechanism achieves 68.3% single-pass success rates with 99.0% citation accuracy in validated responses. Applied to geospatial epidemiology literature on ozone exposure and cardiovascular disease, the system identifies methodological trends and research gaps, demonstrating broad applicability across scientific domains for accelerating evidence synthesis and discovery.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† HySemRAGï¼Œä¸€ç§ç»“åˆäº† Extract, Transform, Load (ETL) æµæ°´çº¿ä¸ Retrieval-Augmented Generation (RAG) çš„æ¡†æ¶ï¼Œæ—¨åœ¨å®ç°å¤§è§„æ¨¡æ–‡çŒ®ç»¼è¿°çš„è‡ªåŠ¨åŒ–å¹¶è¯†åˆ«æ–¹æ³•è®ºç ”ç©¶ä¸­çš„ç©ºç™½ã€‚ç³»ç»Ÿé‡‡ç”¨å¤šå±‚çº§æ–¹æ³•ï¼Œé€šè¿‡æ•´åˆè¯­ä¹‰æœç´¢ã€å…³é”®è¯è¿‡æ»¤å’ŒçŸ¥è¯†å›¾è°±éå†çš„æ··åˆæ£€ç´¢æœºåˆ¶ï¼Œä»¥åŠå…·æœ‰è¿­ä»£è´¨é‡ä¿è¯çš„æ™ºèƒ½ä½“è‡ªæˆ‘çº æ­£ (agentic self-correction) æ¡†æ¶ï¼Œè§£å†³äº†ç°æœ‰ RAG æ¶æ„çš„å±€é™æ€§ã€‚è¯¥æ¡†æ¶åŒ…å«å…ƒæ•°æ®è·å–ã€åŸºäº Docling æ¶æ„çš„æ–‡æ¡£å¸ƒå±€åˆ†æã€è¯­ä¹‰ç»Ÿä¸€åŠ Neo4j çŸ¥è¯†å›¾è°±æ„å»ºç­‰å…«ä¸ªé›†æˆé˜¶æ®µï¼Œäº§å‡ºå¯æ”¯æŒå¤æ‚å…³ç³»æŸ¥è¯¢å’Œè¯­ä¹‰æœç´¢çš„åŒé‡æ•°æ®äº§å“ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒHySemRAG åœ¨ç»“æ„åŒ–å­—æ®µæå–æ–¹é¢çš„è¯­ä¹‰ç›¸ä¼¼åº¦æ¯”ä¼ ç»Ÿçš„ PDF åˆ†å—æ–¹æ³•é«˜å‡º 35.1%ï¼Œä¸”åœ¨éªŒè¯å“åº”ä¸­å®ç°äº† 99.0% çš„å¼•ç”¨å‡†ç¡®ç‡ã€‚è¯¥ç³»ç»Ÿå·²æˆåŠŸåº”ç”¨äºåœ°çƒç©ºé—´æµè¡Œç—…å­¦æ–‡çŒ®åˆ†æï¼Œè¯æ˜äº†å…¶åœ¨åŠ é€Ÿè·¨å­¦ç§‘è¯æ®åˆæˆå’Œç§‘å­¦å‘ç°æ–¹é¢çš„å¹¿æ³›é€‚ç”¨æ€§ã€‚",
      "categories": [
        "cs.IR",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.IR",
      "comment": "47 pages, 10 figures. Code: https://github.com/agodinezmm2007/docling_mod. Demo: https://youtu.be/ZCy5ESJ1gVE?si=K8CttwgTj7yGrWjn. ETL+multi-agent RAG framework for literature synthesis, 35.1% improvement over PDF chunking. Real application: reduced 17,400 papers to 24 relevant ones (99.86%) in 10 minutes for wastewater epidemiology review",
      "pdf_url": "https://arxiv.org/pdf/2508.05666v1",
      "published_date": "2025-08-01 20:30:42 UTC",
      "updated_date": "2025-08-01 20:30:42 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T08:02:09.396479+00:00"
    },
    {
      "arxiv_id": "2508.01060v1",
      "title": "Connectivity Management in Satellite-Aided Vehicular Networks with Multi-Head Attention-Based State Estimation",
      "title_zh": "åŸºäºå¤šå¤´æ³¨æ„åŠ›çŠ¶æ€ä¼°è®¡çš„å«æ˜Ÿè¾…åŠ©è½¦è”ç½‘è¿æ¥ç®¡ç†",
      "authors": [
        "Ibrahim Althamary",
        "Chen-Fu Chou",
        "Chih-Wei Huang"
      ],
      "abstract": "Managing connectivity in integrated satellite-terrestrial vehicular networks is critical for 6G, yet is challenged by dynamic conditions and partial observability. This letter introduces the Multi-Agent Actor-Critic with Satellite-Aided Multi-head self-attention (MAAC-SAM), a novel multi-agent reinforcement learning framework that enables vehicles to autonomously manage connectivity across Vehicle-to-Satellite (V2S), Vehicle-to-Infrastructure (V2I), and Vehicle-to-Vehicle (V2V) links. Our key innovation is the integration of a multi-head attention mechanism, which allows for robust state estimation even with fluctuating and limited information sharing among vehicles. The framework further leverages self-imitation learning (SIL) and fingerprinting to improve learning efficiency and real-time decisions. Simulation results, based on realistic SUMO traffic models and 3GPP-compliant configurations, demonstrate that MAAC-SAM outperforms state-of-the-art terrestrial and satellite-assisted baselines by up to 14% in transmission utility and maintains high estimation accuracy across varying vehicle densities and sharing levels.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹6Gä¸­å«æ˜Ÿ-åœ°é¢é›†æˆè½¦è¾†ç½‘ç»œåœ¨åŠ¨æ€ç¯å¢ƒåŠéƒ¨åˆ†å¯è§‚æµ‹æ€§ä¸‹çš„è¿æ¥ç®¡ç†æŒ‘æˆ˜ï¼Œæå‡ºäº†ä¸€ç§åä¸ºMAAC-SAMçš„å¤šæ™ºèƒ½ä½“å¼ºåŒ–å­¦ä¹ æ¡†æ¶ã€‚è¯¥æ¡†æ¶çš„æ ¸å¿ƒåˆ›æ–°åœ¨äºé›†æˆäº†multi-head attentionæœºåˆ¶ï¼Œèƒ½å¤Ÿåœ¨è½¦è¾†é—´ä¿¡æ¯å…±äº«å—é™æˆ–æ³¢åŠ¨çš„æƒ…å†µä¸‹å®ç°ç¨³å¥çš„çŠ¶æ€ä¼°è®¡ï¼ˆstate estimationï¼‰ã€‚é€šè¿‡ç»“åˆself-imitation learning (SIL)å’ŒfingerprintingæŠ€æœ¯ï¼ŒMAAC-SAMè¿›ä¸€æ­¥æå‡äº†å­¦ä¹ æ•ˆç‡å’Œå®æ—¶å†³ç­–èƒ½åŠ›ï¼Œæ”¯æŒè½¦è¾†åœ¨Vehicle-to-Satellite (V2S)ã€Vehicle-to-Infrastructure (V2I)åŠVehicle-to-Vehicle (V2V)é“¾è·¯é—´è‡ªä¸»ç®¡ç†è¿æ¥ã€‚åŸºäºSUMOäº¤é€šæ¨¡å‹å’Œ3GPPè§„èŒƒçš„ä»¿çœŸç»“æœè¡¨æ˜ï¼Œè¯¥æ¡†æ¶åœ¨ä¼ è¾“æ•ˆç”¨ä¸Šæ¯”ç°æœ‰åŸºå‡†æ¨¡å‹æå‡äº†å¤šè¾¾14%ï¼Œå¹¶åœ¨ä¸åŒè½¦è¾†å¯†åº¦ä¸‹å‡å±•ç°å‡ºä¼˜å¼‚çš„æ€§èƒ½ã€‚",
      "categories": [
        "cs.NI",
        "cs.AI"
      ],
      "primary_category": "cs.NI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.01060v1",
      "published_date": "2025-08-01 20:29:03 UTC",
      "updated_date": "2025-08-01 20:29:03 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T08:02:11.487164+00:00"
    },
    {
      "arxiv_id": "2508.02731v1",
      "title": "Teaching at Scale: Leveraging AI to Evaluate and Elevate Engineering Education",
      "title_zh": "è§„æ¨¡åŒ–æ•™å­¦ï¼šåˆ©ç”¨äººå·¥æ™ºèƒ½è¯„ä¼°å¹¶æå‡å·¥ç¨‹æ•™è‚²",
      "authors": [
        "Jean-Francois Chamberland",
        "Martin C. Carlisle",
        "Arul Jayaraman",
        "Krishna R. Narayanan",
        "Sunay Palsole",
        "Karan Watson"
      ],
      "abstract": "Evaluating teaching effectiveness at scale remains a persistent challenge for large universities, particularly within engineering programs that enroll tens of thousands of students. Traditional methods, such as manual review of student evaluations, are often impractical, leading to overlooked insights and inconsistent data use. This article presents a scalable, AI-supported framework for synthesizing qualitative student feedback using large language models. The system employs hierarchical summarization, anonymization, and exception handling to extract actionable themes from open-ended comments while upholding ethical safeguards. Visual analytics contextualize numeric scores through percentile-based comparisons, historical trends, and instructional load. The approach supports meaningful evaluation and aligns with best practices in qualitative analysis and educational assessment, incorporating student, peer, and self-reflective inputs without automating personnel decisions. We report on its successful deployment across a large college of engineering. Preliminary validation through comparisons with human reviewers, faculty feedback, and longitudinal analysis suggests that LLM-generated summaries can reliably support formative evaluation and professional development. This work demonstrates how AI systems, when designed with transparency and shared governance, can promote teaching excellence and continuous improvement at scale within academic institutions.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤§å‹å¤§å­¦å·¥ç¨‹å­¦é™¢åœ¨æ•°ä¸‡åå­¦ç”Ÿè§„æ¨¡ä¸‹è¯„ä¼°æ•™å­¦è´¨é‡çš„æŒ‘æˆ˜ï¼ŒæŒ‡å‡ºä¼ ç»Ÿäººå·¥å®¡æŸ¥å­¦ç”Ÿè¯„ä»·æ•ˆç‡ä½ä¸‹ä¸”éš¾ä»¥æå–æ·±å…¥è§è§£çš„é—®é¢˜ã€‚ä¸ºæ­¤ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ä¸ªå¯æ‰©å±•çš„ AI æ”¯æŒæ¡†æ¶ï¼Œåˆ©ç”¨å¤§è¯­è¨€æ¨¡å‹ (Large Language Models, LLMs) å¯¹å®šæ€§å­¦ç”Ÿåé¦ˆè¿›è¡Œåˆæˆã€‚è¯¥ç³»ç»Ÿé€šè¿‡åˆ†å±‚æ€»ç»“ (Hierarchical Summarization)ã€åŒ¿ååŒ–å’Œå¼‚å¸¸å¤„ç†æŠ€æœ¯ä»å¼€æ”¾å¼è¯„è®ºä¸­æå–å…³é”®ä¸»é¢˜ï¼Œå¹¶ç»“åˆå¯è§†åŒ–åˆ†æ (Visual Analytics) é€šè¿‡ç™¾åˆ†ä½å¯¹æ¯”å’Œå†å²è¶‹åŠ¿ä¸ºæ•°å€¼è¯„åˆ†æä¾›èƒŒæ™¯ã€‚è¯¥æ–¹æ³•æ•´åˆäº†å­¦ç”Ÿã€åŒè¡Œå’Œè‡ªæˆ‘åæ€çš„å¤šæ–¹è¾“å…¥ï¼Œåœ¨éµå¾ªå®šæ€§åˆ†ææœ€ä½³å®è·µçš„åŒæ—¶ç¡®ä¿ä¸è‡ªåŠ¨æ‰§è¡Œäººäº‹å†³ç­–ã€‚åœ¨æŸå¤§å‹å·¥ç¨‹å­¦é™¢çš„å®é™…éƒ¨ç½²åŠåˆæ­¥éªŒè¯è¡¨æ˜ï¼ŒLLM ç”Ÿæˆçš„æ‘˜è¦åœ¨å¯é æ€§ä¸Šå¯æ¯”è‚©äººç±»è¯„å®¡å‘˜ï¼Œèƒ½æœ‰æ•ˆæ”¯æŒå½¢æˆæ€§è¯„ä»· (Formative Evaluation) å’Œæ•™å¸ˆä¸“ä¸šå‘å±•ã€‚è¿™é¡¹å·¥ä½œè¯æ˜äº†åœ¨é€æ˜åº¦å’Œå…±åŒæ²»ç†çš„åŸºç¡€ä¸Šï¼ŒAI ç³»ç»Ÿå¯ä»¥æ˜¾è‘—ä¿ƒè¿›å¤§è§„æ¨¡å­¦æœ¯æœºæ„çš„æ•™å­¦å“è¶Šä¸æŒç»­æ”¹è¿›ã€‚",
      "categories": [
        "cs.CY",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.CY",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.02731v1",
      "published_date": "2025-08-01 20:27:40 UTC",
      "updated_date": "2025-08-01 20:27:40 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T08:02:20.390967+00:00"
    },
    {
      "arxiv_id": "2508.01059v1",
      "title": "Llama-3.1-FoundationAI-SecurityLLM-8B-Instruct Technical Report",
      "title_zh": "Llama-3.1-FoundationAI-SecurityLLM-8B-Instruct æŠ€æœ¯æŠ¥å‘Š",
      "authors": [
        "Sajana Weerawardhena",
        "Paul Kassianik",
        "Blaine Nelson",
        "Baturay Saglam",
        "Anu Vellore",
        "Aman Priyanshu",
        "Supriti Vijay",
        "Massimo Aufiero",
        "Arthur Goldblatt",
        "Fraser Burch",
        "Ed Li",
        "Jianliang He",
        "Dhruv Kedia",
        "Kojin Oshiba",
        "Zhouran Yang",
        "Yaron Singer",
        "Amin Karbasi"
      ],
      "abstract": "Large language models (LLMs) have shown remarkable success across many domains, yet their integration into cybersecurity applications remains limited due to a lack of general-purpose cybersecurity data, representational complexity, and safety and regulatory concerns. To address this gap, we previously introduced Foundation-Sec-8B, a cybersecurity-focused LLM suitable for fine-tuning on downstream tasks. That model, however, was not designed for chat-style interactions or instruction-following. In this report, we release Foundation-Sec-8B-Instruct: a model specifically trained for general-purpose cybersecurity dialogue. Built on Foundation-Sec-8B, it combines domain-specific knowledge with instruction-following, conversational capabilities, and alignment with human preferences to produce high-quality, relevant responses. Comprehensive evaluations show that Foundation-Sec-8B-Instruct outperforms Llama 3.1-8B-Instruct on a range of cybersecurity tasks while matching its instruction-following performance. It is also competitive with GPT-4o-mini on cyber threat intelligence and instruction-following tasks. We envision Foundation-Sec-8B-Instruct becoming an indispensable assistant in the daily workflows of cybersecurity professionals. We release the model publicly at https://huggingface.co/fdtn-ai/Foundation-Sec-8B-Instruct.",
      "tldr_zh": "è¯¥æŠ€æœ¯æŠ¥å‘Šæ¨å‡ºäº† Foundation-Sec-8B-Instructï¼Œæ—¨åœ¨è§£å†³é€šç”¨å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨ç½‘ç»œå®‰å…¨é¢†åŸŸé¢ä¸´çš„ä¸“ä¸šæ•°æ®åŒ®ä¹ã€è¡¨ç¤ºå¤æ‚æ€§ä»¥åŠå®‰å…¨ç›‘ç®¡ç­‰æŒ‘æˆ˜ã€‚è¯¥æ¨¡å‹åŸºäºæ­¤å‰å‘å¸ƒçš„ Foundation-Sec-8B æ„å»ºï¼Œä¸“é—¨é’ˆå¯¹é€šç”¨ç½‘ç»œå®‰å…¨å¯¹è¯è¿›è¡Œäº†è®­ç»ƒï¼Œä½¿å…¶å…·å¤‡äº†å¼ºå¤§çš„æŒ‡ä»¤éµå¾ªï¼ˆinstruction-followingï¼‰å’Œå¯¹è¯èƒ½åŠ›ã€‚é€šè¿‡ç»“åˆç‰¹å®šé¢†åŸŸçš„çŸ¥è¯†ä¸äººç±»åå¥½å¯¹é½ï¼ˆalignment with human preferencesï¼‰ï¼ŒFoundation-Sec-8B-Instruct èƒ½å¤Ÿç”Ÿæˆé«˜è´¨é‡ä¸”é«˜åº¦ç›¸å…³çš„å®‰å…¨ä¸“ä¸šå“åº”ã€‚åœ¨ç»¼åˆè¯„ä¼°ä¸­ï¼Œè¯¥æ¨¡å‹åœ¨å¤šé¡¹ç½‘ç»œå®‰å…¨ä»»åŠ¡ä¸Šçš„è¡¨ç°è¶…è¶Šäº† Llama 3.1-8B-Instructï¼ŒåŒæ—¶åœ¨æŒ‡ä»¤éµå¾ªèƒ½åŠ›ä¸Šä¸ä¹‹æŒå¹³ã€‚æ­¤å¤–ï¼Œå®ƒåœ¨ç½‘ç»œå¨èƒæƒ…æŠ¥ï¼ˆcyber threat intelligenceï¼‰å’ŒæŒ‡ä»¤éµå¾ªä»»åŠ¡ä¸­è¡¨ç°å‡ºä¸ GPT-4o-mini ç›¸å½“çš„ç«äº‰åŠ›ã€‚è¯¥ç ”ç©¶è‡´åŠ›äºä¸ºç½‘ç»œå®‰å…¨ä»ä¸šè€…çš„æ—¥å¸¸å·¥ä½œæµç¨‹æä¾›ä¸å¯æˆ–ç¼ºçš„è¾…åŠ©å·¥å…·ï¼Œç›®å‰æ¨¡å‹å·²åœ¨ Hugging Face å¹³å°ä¸Šå…¬å¼€ã€‚",
      "categories": [
        "cs.CR",
        "cs.AI"
      ],
      "primary_category": "cs.CR",
      "comment": "34 pages - Technical Report",
      "pdf_url": "https://arxiv.org/pdf/2508.01059v1",
      "published_date": "2025-08-01 20:25:57 UTC",
      "updated_date": "2025-08-01 20:25:57 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T08:02:26.384779+00:00"
    },
    {
      "arxiv_id": "2508.01057v2",
      "title": "Edge-Based Multimodal Sensor Data Fusion with Vision Language Models (VLMs) for Real-time Autonomous Vehicle Accident Avoidance",
      "title_zh": "åŸºäºè§†è§‰è¯­è¨€æ¨¡å‹ (VLMs) çš„è¾¹ç¼˜å¤šæ¨¡æ€ä¼ æ„Ÿå™¨æ•°æ®èåˆï¼ŒåŠ©åŠ›è‡ªåŠ¨é©¾é©¶å®æ—¶äº‹æ•…è§„é¿",
      "authors": [
        "Fengze Yang",
        "Bo Yu",
        "Yang Zhou",
        "Xuewen Luo",
        "Zhengzhong Tu",
        "Chenxi Liu"
      ],
      "abstract": "Autonomous driving (AD) systems relying solely on onboard sensors may fail to detect distant or obstacle hazards, potentially causing preventable collisions; however, existing transformer-based Vehicle-to-Everything (V2X) approaches, which mitigate AD sensing limitations, either lack effective multimodal fusion and reasoning or struggle to meet real-time performance requirements under complex, high-dimensional traffic conditions. This paper proposes the Real-time Edge-based Autonomous Co-pilot Trajectory planner (REACT), a V2X-integrated trajectory optimization framework for AD based on a fine-tuned lightweight Vision-Language Model (VLM). REACT integrates infrastructure-provided hazard alerts with onboard sensor data, capturing intricate surrounding traffic dynamics and vehicle intents through visual embeddings, interpreting precise numerical data from symbolic inputs, and employing contextual reasoning to generate optimized, safety-oriented trajectories. To ensure robust real-time deployment on edge devices, REACT innovatively employs Residual Trajectory Fusion (RTF) design and specialized edge-adaptation strategies to reduce model complexity and improve inference efficiency. Evaluated on the DeepAccident benchmark, REACT achieves state-of-the-art performance, a 77% collision rate reduction, a 48.2% Video Panoptic Quality (VPQ), and a 0.57-second inference latency on the Jetson AGX Orin. Ablation studies validate the contribution of each input, module, and edge adaptation strategy. These results highlight the effectiveness of lightweight VLMs in enabling real-time cooperative planning on edge platforms and underscore the potential of language-guided contextual reasoning for improving traffic safety and responsiveness.",
      "tldr_zh": "æœ¬ç ”ç©¶æå‡ºäº†REACTï¼ˆReal-time Edge-based Autonomous Co-pilot Trajectory plannerï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªåŸºäºå¾®è°ƒè½»é‡çº§Vision-Language Model (VLM)çš„V2Xé›†æˆè½¨è¿¹ä¼˜åŒ–æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³è‡ªåŠ¨é©¾é©¶ç³»ç»Ÿåœ¨å¤æ‚äº¤é€šç¯å¢ƒä¸‹æ„ŸçŸ¥è¯†åˆ«å—é™åŠå®æ—¶æ€§ä¸è¶³çš„é—®é¢˜ã€‚è¯¥æ¡†æ¶é€šè¿‡Visual Embeddingsæ•æ‰å‘¨å›´äº¤é€šåŠ¨æ€å’Œè½¦è¾†æ„å›¾ï¼Œå¹¶å°†åŸºç¡€è®¾æ–½æä¾›çš„å±é™©è­¦æŠ¥ä¸è½¦è½½ä¼ æ„Ÿå™¨æ•°æ®è¿›è¡Œå¤šæ¨¡æ€èåˆï¼Œåˆ©ç”¨ä¸Šä¸‹æ–‡æ¨ç†ç”Ÿæˆå®‰å…¨å¯¼å‘çš„æœ€ä¼˜è½¨è¿¹ã€‚ä¸ºäº†ç¡®ä¿åœ¨è¾¹ç¼˜è®¾å¤‡ä¸Šçš„å®æ—¶éƒ¨ç½²ï¼ŒREACTåˆ›æ–°æ€§åœ°é‡‡ç”¨äº†Residual Trajectory Fusion (RTF)è®¾è®¡å’Œä¸“é—¨çš„è¾¹ç¼˜é€‚é…ç­–ç•¥ï¼Œæœ‰æ•ˆé™ä½äº†æ¨¡å‹å¤æ‚åº¦å¹¶æå‡äº†æ¨ç†æ•ˆç‡ã€‚åœ¨DeepAccidentåŸºå‡†æµ‹è¯•ä¸­çš„è¯„ä¼°ç»“æœæ˜¾ç¤ºï¼ŒREACTè¾¾åˆ°äº†é¢†åŸŸæœ€å…ˆè¿›æ°´å¹³ï¼ŒæˆåŠŸå°†ç¢°æ’ç‡é™ä½äº†77%ï¼Œå¹¶å®ç°äº†48.2%çš„Video Panoptic Quality (VPQ)ã€‚åœ¨Jetson AGX Orinè®¾å¤‡ä¸Šï¼Œè¯¥ç³»ç»Ÿçš„æ¨ç†å»¶è¿Ÿä»…ä¸º0.57ç§’ï¼Œè¯æ˜äº†å…¶åœ¨è¾¹ç¼˜å¹³å°ä¸Šè¿›è¡Œå®æ—¶ååŒè§„åˆ’çš„å“è¶Šæ€§èƒ½ã€‚æ¶ˆèå®éªŒè¿›ä¸€æ­¥éªŒè¯äº†å„è¾“å…¥æ¨¡å—å’Œé€‚é…ç­–ç•¥çš„æœ‰æ•ˆæ€§ï¼Œå¼ºè°ƒäº†è½»é‡çº§VLMå’Œè¯­è¨€å¼•å¯¼çš„ä¸Šä¸‹æ–‡æ¨ç†åœ¨æå‡äº¤é€šå®‰å…¨å’Œå“åº”é€Ÿåº¦æ–¹é¢çš„å·¨å¤§æ½œåŠ›ã€‚",
      "categories": [
        "cs.AI",
        "cs.RO"
      ],
      "primary_category": "cs.AI",
      "comment": "24 pages, 6 tables, 7 figures",
      "pdf_url": "https://arxiv.org/pdf/2508.01057v2",
      "published_date": "2025-08-01 20:16:04 UTC",
      "updated_date": "2025-08-12 12:29:02 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T08:02:31.192690+00:00"
    },
    {
      "arxiv_id": "2508.01056v2",
      "title": "Managing Escalation in Off-the-Shelf Large Language Models",
      "title_zh": "ç°æˆå¤§è¯­è¨€æ¨¡å‹ä¸­çš„å†²çªå‡çº§ç®¡ç†",
      "authors": [
        "Sebastian Elbaum",
        "Jonathan Panter"
      ],
      "abstract": "U.S. national security customers have begun to utilize large language models, including enterprise versions of ``off-the-shelf'' models (e.g., ChatGPT) familiar to the public. This uptake will likely accelerate. However, recent studies suggest that off-the-shelf large language models frequently suggest escalatory actions when prompted with geopolitical or strategic scenarios. We demonstrate two simple, non-technical interventions to control these tendencies. Introducing these interventions into the experimental wargame design of a recent study, we substantially reduce escalation throughout the game. Calls to restrict the use of large language models in national security applications are thus premature. The U.S. government is already, and will continue, employing large language models for scenario planning and suggesting courses of action. Rather than warning against such applications, this study acknowledges the imminent adoption of large language models, and provides actionable measures to align them with national security goals, including escalation management.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†ç¾å›½å›½å®¶å®‰å…¨é¢†åŸŸåœ¨ä½¿ç”¨ ChatGPT ç­‰ç°æˆ Large Language Models (LLMs) æ—¶é¢ä¸´çš„ Escalation (å‡çº§) é£é™©ï¼Œå³æ¨¡å‹åœ¨åœ°ç¼˜æ”¿æ²»åœºæ™¯ä¸­å€¾å‘äºå»ºè®®æ¿€è¿›è¡ŒåŠ¨çš„é—®é¢˜ã€‚ä½œè€…æå‡ºäº†ä¸¤ç§ç®€å•çš„éæŠ€æœ¯æ€§å¹²é¢„æªæ–½ (non-technical interventions)ï¼Œæ—¨åœ¨æœ‰æ•ˆæ§åˆ¶ LLMs çš„è¿™ç§å‡çº§å€¾å‘ã€‚é€šè¿‡å°†è¿™äº›å¹²é¢„æªæ–½åº”ç”¨äºå®éªŒæ€§å…µæ£‹æ¨æ¼” (wargame) è®¾è®¡ä¸­ï¼Œç ”ç©¶æˆåŠŸæ˜¾è‘—é™ä½äº†æ¸¸æˆå…¨è¿‡ç¨‹ä¸­çš„ Escalation é¢‘ç‡ã€‚è¿™ä¸€å‘ç°è¡¨æ˜ï¼Œç›®å‰å…¨é¢é™åˆ¶ LLMs åœ¨å›½å®¶å®‰å…¨åº”ç”¨ä¸­çš„ä½¿ç”¨å°šæ˜¾ä»“ä¿ƒã€‚ç›¸æ¯”äºç®€å•çš„ç¦æ­¢ï¼Œè¯¥ç ”ç©¶å¼ºè°ƒäº†é€šè¿‡å¯æ“ä½œæ‰‹æ®µä½¿æ¨¡å‹ä¸å›½å®¶å®‰å…¨ç›®æ ‡å¯¹é½çš„é‡è¦æ€§ã€‚è¯¥ç ”ç©¶ä¸ä»…éªŒè¯äº† LLMs åœ¨æƒ…å¢ƒè§„åˆ’ä¸­çš„æ½œåŠ›ï¼Œè¿˜ä¸ºå®ç°æœ‰æ•ˆçš„ Escalation Management (å‡çº§ç®¡ç†) æä¾›äº†å®ç”¨çš„å¯¹é½è·¯å¾„ã€‚",
      "categories": [
        "cs.ET",
        "cs.AI"
      ],
      "primary_category": "cs.ET",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.01056v2",
      "published_date": "2025-08-01 20:15:45 UTC",
      "updated_date": "2025-08-05 13:51:26 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T08:02:34.989127+00:00"
    },
    {
      "arxiv_id": "2508.01055v3",
      "title": "FGBench: A Dataset and Benchmark for Molecular Property Reasoning at Functional Group-Level in Large Language Models",
      "title_zh": "FGBenchï¼šé¢å‘å¤§è¯­è¨€æ¨¡å‹å®˜èƒ½å›¢çº§åˆ†å­æ€§è´¨æ¨ç†çš„æ•°æ®é›†ä¸åŸºå‡†æµ‹è¯•",
      "authors": [
        "Xuan Liu",
        "Siru Ouyang",
        "Xianrui Zhong",
        "Jiawei Han",
        "Huimin Zhao"
      ],
      "abstract": "Large language models (LLMs) have gained significant attention in chemistry. However, most existing datasets center on molecular-level property prediction and overlook the role of fine-grained functional group (FG) information. Incorporating FG-level data can provide valuable prior knowledge that links molecular structures with textual descriptions, which can be used to build more interpretable, structure-aware LLMs for reasoning on molecule-related tasks. Moreover, LLMs can learn from such fine-grained information to uncover hidden relationships between specific functional groups and molecular properties, thereby advancing molecular design and drug discovery. Here, we introduce FGBench, a dataset comprising 625K molecular property reasoning problems with functional group information. Functional groups are precisely annotated and localized within the molecule, which ensures the dataset's interoperability thereby facilitating further multimodal applications. FGBench includes both regression and classification tasks on 245 different functional groups across three categories for molecular property reasoning: (1) single functional group impacts, (2) multiple functional group interactions, and (3) direct molecular comparisons. In the benchmark of state-of-the-art LLMs on 7K curated data, the results indicate that current LLMs struggle with FG-level property reasoning, highlighting the need to enhance reasoning capabilities in LLMs for chemistry tasks. We anticipate that the methodology employed in FGBench to construct datasets with functional group-level information will serve as a foundational framework for generating new question-answer pairs, enabling LLMs to better understand fine-grained molecular structure-property relationships. The dataset and evaluation code are available at https://github.com/xuanliugit/FGBench.",
      "tldr_zh": "è¯¥ç ”ç©¶å¼•å…¥äº†FGBenchï¼Œè¿™æ˜¯ä¸€ä¸ªåŒ…å«625Kä¸ªåˆ†å­å±æ€§æ¨ç†é—®é¢˜çš„åŸºå‡†æ•°æ®é›†ï¼Œæ—¨åœ¨è§£å†³å¤§è¯­è¨€æ¨¡å‹(LLMs)åœ¨åŒ–å­¦é¢†åŸŸå¿½è§†ç»†ç²’åº¦å®˜èƒ½å›¢(Functional Group, FG)ä¿¡æ¯çš„é—®é¢˜ã€‚FGBenchå¯¹245ç§ä¸åŒçš„å®˜èƒ½å›¢è¿›è¡Œäº†ç²¾ç¡®çš„æ ‡æ³¨ä¸å®šä½ï¼Œé€šè¿‡æä¾›ç»“æ„åŒ–çš„å…ˆéªŒçŸ¥è¯†ï¼Œå¢å¼ºäº†æ¨¡å‹åœ¨åˆ†å­ä»»åŠ¡ä¸Šçš„å¯è§£é‡Šæ€§ä¸ç»“æ„æ„ŸçŸ¥èƒ½åŠ›ã€‚è¯¥æ•°æ®é›†æ¶µç›–äº†å•å®˜èƒ½å›¢å½±å“ã€å¤šå®˜èƒ½å›¢äº¤äº’ä»¥åŠç›´æ¥åˆ†å­å¯¹æ¯”ä¸‰å¤§æ¨ç†ç±»åˆ«ï¼Œæ”¯æŒå›å½’ä¸åˆ†ç±»ä»»åŠ¡ã€‚åŸºå‡†æµ‹è¯•ç»“æœè¡¨æ˜ï¼Œå½“å‰å…ˆè¿›çš„LLMsåœ¨å®˜èƒ½å›¢çº§åˆ«çš„å±æ€§æ¨ç†ä¸Šä»é¢ä¸´å·¨å¤§æŒ‘æˆ˜ï¼Œéš¾ä»¥æ­ç¤ºç‰¹å®šç»“æ„ä¸å±æ€§é—´çš„æ·±å±‚å…³è”ã€‚è¿™é¡¹å·¥ä½œä¸ºæ„å»ºèƒ½å¤Ÿç†è§£ç»†ç²’åº¦åˆ†å­ç»“æ„ä¸å±æ€§å…³ç³»çš„æ¨¡å‹å¥ å®šäº†åŸºç¡€ï¼Œå¯¹åŠ é€Ÿåˆ†å­è®¾è®¡å’Œè¯ç‰©å‘ç°å…·æœ‰é‡è¦æ„ä¹‰ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "q-bio.BM",
        "q-bio.QM"
      ],
      "primary_category": "cs.LG",
      "comment": "NeurIPS 2025 (Datasets and Benchmarks Track)",
      "pdf_url": "https://arxiv.org/pdf/2508.01055v3",
      "published_date": "2025-08-01 20:15:29 UTC",
      "updated_date": "2025-10-18 23:39:48 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T08:03:02.791059+00:00"
    },
    {
      "arxiv_id": "2508.01054v1",
      "title": "Autonomous Penetration Testing: Solving Capture-the-Flag Challenges with LLMs",
      "title_zh": "è‡ªä¸»æ¸—é€æµ‹è¯•ï¼šåˆ©ç”¨å¤§è¯­è¨€æ¨¡å‹è§£å†³å¤ºæ——èµ›æŒ‘æˆ˜",
      "authors": [
        "Isabelle Bakker",
        "John Hastings"
      ],
      "abstract": "This study evaluates the ability of GPT-4o to autonomously solve beginner-level offensive security tasks by connecting the model to OverTheWire's Bandit capture-the-flag game. Of the 25 levels that were technically compatible with a single-command SSH framework, GPT-4o solved 18 unaided and another two after minimal prompt hints for an overall 80% success rate. The model excelled at single-step challenges that involved Linux filesystem navigation, data extraction or decoding, and straightforward networking. The approach often produced the correct command in one shot and at a human-surpassing speed. Failures involved multi-command scenarios that required persistent working directories, complex network reconnaissance, daemon creation, or interaction with non-standard shells. These limitations highlight current architectural deficiencies rather than a lack of general exploit knowledge. The results demonstrate that large language models (LLMs) can automate a substantial portion of novice penetration-testing workflow, potentially lowering the expertise barrier for attackers and offering productivity gains for defenders who use LLMs as rapid reconnaissance aides. Further, the unsolved tasks reveal specific areas where secure-by-design environments might frustrate simple LLM-driven attacks, informing future hardening strategies. Beyond offensive cybersecurity applications, results suggest the potential to integrate LLMs into cybersecurity education as practice aids.",
      "tldr_zh": "æœ¬ç ”ç©¶è¯„ä¼°äº† GPT-4o åœ¨è‡ªåŠ¨åŒ–è§£å†³åˆçº§æ”»å‡»æ€§å®‰å…¨ä»»åŠ¡æ–¹é¢çš„èƒ½åŠ›ï¼Œé€šè¿‡å°†å…¶è¿æ¥åˆ° OverTheWire çš„ Bandit å¤ºæ——èµ›(Capture-the-Flag) æ¸¸æˆè¿›è¡Œæµ‹è¯•ã€‚åœ¨ 25 ä¸ªæŠ€æœ¯å…¼å®¹çš„å…³å¡ä¸­ï¼ŒGPT-4o ç‹¬ç«‹è§£å†³äº† 18 ä¸ªï¼Œå¹¶åœ¨æå°‘æç¤ºä¸‹é¢å¤–å®Œæˆäº† 2 ä¸ªï¼Œæ€»ä½“æˆåŠŸç‡è¾¾åˆ° 80%ã€‚è¯¥æ¨¡å‹åœ¨æ¶‰åŠ Linux æ–‡ä»¶ç³»ç»Ÿå¯¼èˆªã€æ•°æ®æå–æˆ–è§£ç ä»¥åŠç®€å•ç½‘ç»œä»»åŠ¡çš„å•æ­¥æŒ‘æˆ˜ä¸­è¡¨ç°å‡ºè‰²ï¼Œé€šå¸¸èƒ½ä»¥è¶…è¶Šäººç±»çš„é€Ÿåº¦ä¸€æ¬¡æ€§ç”Ÿæˆæ­£ç¡®å‘½ä»¤ã€‚å¤±è´¥æ¡ˆä¾‹ä¸»è¦é›†ä¸­åœ¨éœ€è¦æŒä¹…å·¥ä½œç›®å½•ã€å¤æ‚ç½‘ç»œä¾¦å¯Ÿ(Network Reconnaissance)ã€å®ˆæŠ¤è¿›ç¨‹åˆ›å»ºæˆ–ä¸éæ ‡å‡† Shell äº¤äº’çš„å¤šå‘½ä»¤åœºæ™¯ä¸­ã€‚è¿™äº›å±€é™æ€§åæ˜ äº†å½“å‰çš„æ¶æ„ç¼ºé™·è€Œéç¼ºä¹é€šç”¨æ¼æ´åˆ©ç”¨(Exploit)çŸ¥è¯†ï¼Œè¡¨æ˜å¤§è¯­è¨€æ¨¡å‹(LLMs)å·²èƒ½è‡ªåŠ¨åŒ–å¤§éƒ¨åˆ†åˆçº§æ¸—é€æµ‹è¯•å·¥ä½œæµã€‚è¿™ç§è¶‹åŠ¿åœ¨é™ä½æ”»å‡»è€…æŠ€æœ¯é—¨æ§›çš„åŒæ—¶ï¼Œä¹Ÿä¸ºåˆ©ç”¨ LLMs è¿›è¡Œå¿«é€Ÿä¾¦å¯Ÿçš„é˜²å¾¡è€…æä¾›äº†æ•ˆç‡å¢ç›Šã€‚æ­¤å¤–ï¼Œç ”ç©¶ç»“æœè¿˜æ­ç¤ºäº† LLMs åœ¨ç½‘ç»œå®‰å…¨æ•™è‚²ä¸­ä½œä¸ºç»ƒä¹ è¾…åŠ©å·¥å…·çš„æ½œåŠ›ï¼Œå¹¶ä¸ºæœªæ¥å®‰å…¨åŠ å›ºç­–ç•¥æä¾›äº†å‚è€ƒã€‚",
      "categories": [
        "cs.CR",
        "cs.AI",
        "cs.CY"
      ],
      "primary_category": "cs.CR",
      "comment": "6 pages, 2 figures, 3 tables",
      "pdf_url": "https://arxiv.org/pdf/2508.01054v1",
      "published_date": "2025-08-01 20:11:58 UTC",
      "updated_date": "2025-08-01 20:11:58 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T08:02:57.092144+00:00"
    },
    {
      "arxiv_id": "2508.01047v3",
      "title": "A Deep Reinforcement Learning-Based TCP Congestion Control Algorithm: Design, Simulation, and Evaluation",
      "title_zh": "åŸºäºæ·±åº¦å¼ºåŒ–å­¦ä¹ çš„TCPæ‹¥å¡æ§åˆ¶ç®—æ³•ï¼šè®¾è®¡ã€ä»¿çœŸä¸è¯„ä¼°",
      "authors": [
        "Efe AÄŸlamazlar",
        "Emirhan Eken",
        "Harun Batur GeÃ§ici"
      ],
      "abstract": "This paper introduces a Deep Reinforcement Learning (DRL) based TCP congestion-control algorithm that uses a Deep Q-Network (DQN) to adapt the congestion window (cWnd) dynamically based on observed network state. The proposed approach utilizes DQNs to optimize the congestion window by observing key network parameters and taking real-time actions. The algorithm is trained and evaluated within the NS-3 network simulator using the OpenGym interface. The results demonstrate that the DRL-based algorithm provides a superior balance between throughput and latency compared to both traditional TCP New Reno and TCP Cubic algorithms. Specifically: Compared to TCP Cubic, the DRL algorithm achieved comparable throughput (statistically insignificant difference of -3.79%, $p>0.05$) while delivering a massive 46.29% reduction in Round-Trip Time (RTT). Furthermore, the DRL agent maintained near-zero packet loss, whereas Cubic suffered from significant buffer overflow. Compared to TCP New Reno, the DRL algorithm achieved comparable throughput (+0.38%) with a 32.40% reduction in RTT. Results from NS-3 simulations indicate that the proposed DRL agent effectively mitigates bufferbloat without compromising bandwidth utilization. This study emphasizes the potential of reinforcement learning techniques for solving complex congestion control problems in modern networks by learning the network capacity rather than saturating it.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§åŸºäºæ·±åº¦å¼ºåŒ–å­¦ä¹ (Deep Reinforcement Learning, DRL)çš„TCPæ‹¥å¡æ§åˆ¶ç®—æ³•ï¼Œåˆ©ç”¨æ·±åº¦Qç½‘ç»œ(Deep Q-Network, DQN)æ ¹æ®å®æ—¶è§‚å¯Ÿåˆ°çš„ç½‘ç»œçŠ¶æ€åŠ¨æ€è°ƒæ•´æ‹¥å¡çª—å£(cWnd)ã€‚è¯¥ç®—æ³•åœ¨NS-3ç½‘ç»œæ¨¡æ‹Ÿå™¨ä¸­é€šè¿‡OpenGymæ¥å£è¿›è¡Œè®­ç»ƒä¸è¯„ä¼°ï¼Œèƒ½å¤Ÿé€šè¿‡å­¦ä¹ ç½‘ç»œå®¹é‡è€Œéç›²ç›®é¥±å’Œç½‘ç»œæ¥ä¼˜åŒ–ä¼ è¾“æ€§èƒ½ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œä¸TCP Cubicç®—æ³•ç›¸æ¯”ï¼Œè¯¥DRLç®—æ³•åœ¨ååé‡ä¿æŒç›¸å½“æ°´å¹³çš„å‰æä¸‹ï¼Œå°†å¾€è¿”æ—¶é—´(Round-Trip Time, RTT)å¤§å¹…é™ä½äº†46.29%ï¼Œä¸”å®ç°äº†è¿‘ä¹é›¶çš„ä¸¢åŒ…ç‡ã€‚ä¸TCP New Renoå¯¹æ¯”ï¼Œè¯¥ç®—æ³•åœ¨ååé‡ç•¥æœ‰æå‡çš„åŒæ—¶ï¼Œä½¿RTTé™ä½äº†32.40%ã€‚NS-3æ¨¡æ‹Ÿç»“æœè¯æ˜ï¼Œè¯¥æ™ºèƒ½ä½“èƒ½å¤Ÿåœ¨ä¸ç‰ºç‰²å¸¦å®½åˆ©ç”¨ç‡çš„æƒ…å†µä¸‹æœ‰æ•ˆç¼“è§£ç¼“å†²åŒºè†¨èƒ€(bufferbloat)é—®é¢˜ã€‚è¿™é¡¹å·¥ä½œçªæ˜¾äº†å¼ºåŒ–å­¦ä¹ æŠ€æœ¯åœ¨è§£å†³ç°ä»£ç½‘ç»œå¤æ‚æ‹¥å¡æ§åˆ¶æŒ‘æˆ˜åŠä¼˜åŒ–ååé‡ä¸å»¶è¿Ÿå¹³è¡¡æ–¹é¢çš„å·¨å¤§æ½œåŠ›ã€‚",
      "categories": [
        "cs.NI",
        "cs.AI"
      ],
      "primary_category": "cs.NI",
      "comment": "11 pages, 4 figures. Presents a DRL agent that mitigates bufferbloat and achieves near-zero packet loss. Validated via NS-3 simulations under a strict training-testing protocol. Code: https://github.com/aglamazlarefe/DRL-TCP",
      "pdf_url": "https://arxiv.org/pdf/2508.01047v3",
      "published_date": "2025-08-01 20:00:17 UTC",
      "updated_date": "2026-01-19 00:27:29 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T08:02:57.994399+00:00"
    },
    {
      "arxiv_id": "2508.01031v4",
      "title": "CADDesigner: Conceptual Design of CAD Models Based on General-Purpose Agent",
      "title_zh": "CADDesignerï¼šåŸºäºé€šç”¨æ™ºèƒ½ä½“çš„ CAD æ¨¡å‹æ¦‚å¿µè®¾è®¡",
      "authors": [
        "Fengxiao Fan",
        "Jingzhe Ni",
        "Xiaolong Yin",
        "Sirui Wang",
        "Xingyu Lu",
        "Qiang Zou",
        "Ruofeng Tong",
        "Min Tang",
        "Peng Du"
      ],
      "abstract": "Computer Aided Design (CAD) plays a pivotal role in industrial manufacturing but typically requires a high level of expertise from designers. To lower the entry barrier and improve design efficiency, we present an agent for CAD conceptual design powered by large language models (LLMs). The agent accepts both textual descriptions and sketches as input, engaging in interactive dialogue with users to refine and clarify design requirements through comprehensive requirement analysis. Built upon a novel Explicit Context Imperative Paradigm (ECIP), the agent generates high-quality CAD modeling code. During the generation process, the agent incorporates iterative visual feedback to improve model quality. Generated design cases are stored in a structured knowledge base, enabling continuous improvement of the agent's code generation capabilities. Experimental results demonstrate that our method achieves state-of-the-art performance in CAD code generation.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†CADDesignerï¼Œè¿™æ˜¯ä¸€ç§ç”±å¤§è¯­è¨€æ¨¡å‹(LLMs)é©±åŠ¨çš„ç”¨äºCADæ–¹æ¡ˆè®¾è®¡çš„æ™ºèƒ½ä½“ï¼Œæ—¨åœ¨é™ä½å·¥ä¸šè®¾è®¡é—¨æ§›å¹¶æå‡æ•ˆç‡ã€‚è¯¥æ™ºèƒ½ä½“æ”¯æŒæ–‡æœ¬æè¿°å’Œè‰å›¾ä½œä¸ºè¾“å…¥ï¼Œé€šè¿‡ä¸ç”¨æˆ·è¿›è¡Œäº¤äº’å¼å¯¹è¯æ¥ç²¾å‡†åˆ†æå’Œç»†åŒ–è®¾è®¡éœ€æ±‚ã€‚CADDesigneråŸºäºä¸€ç§æ–°å‹çš„æ˜¾å¼ä¸Šä¸‹æ–‡æŒ‡ä»¤èŒƒå¼(Explicit Context Imperative Paradigm, ECIP)ç”Ÿæˆé«˜è´¨é‡çš„CADå»ºæ¨¡ä»£ç ï¼Œå¹¶åœ¨ç”Ÿæˆè¿‡ç¨‹ä¸­å¼•å…¥è¿­ä»£è§†è§‰åé¦ˆ(visual feedback)ä»¥ä¼˜åŒ–æ¨¡å‹è´¨é‡ã€‚æ­¤å¤–ï¼Œç³»ç»Ÿå°†ç”Ÿæˆçš„æ¡ˆä¾‹å­˜å‚¨åœ¨ç»“æ„åŒ–çŸ¥è¯†åº“ä¸­ï¼Œé€šè¿‡æŒç»­å­¦ä¹ ä¸æ–­æå‡ä»£ç ç”Ÿæˆèƒ½åŠ›ã€‚å®éªŒç»“æœè¯æ˜ï¼Œè¯¥æ–¹æ³•åœ¨CADä»£ç ç”Ÿæˆä»»åŠ¡ä¸­è¾¾åˆ°äº†å½“å‰æœ€å…ˆè¿›(state-of-the-art)çš„æ€§èƒ½æ°´å¹³ã€‚",
      "categories": [
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.01031v4",
      "published_date": "2025-08-01 19:15:56 UTC",
      "updated_date": "2025-12-16 04:27:47 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T08:02:59.595756+00:00"
    },
    {
      "arxiv_id": "2508.01015v1",
      "title": "AutoSIGHT: Automatic Eye Tracking-based System for Immediate Grading of Human experTise",
      "title_zh": "AutoSIGHTï¼šåŸºäºçœ¼åŠ¨è¿½è¸ªçš„äººç±»ä¸“ä¸šæŠ€èƒ½å³æ—¶è‡ªåŠ¨è¯„å®šç³»ç»Ÿ",
      "authors": [
        "Byron Dowling",
        "Jozef Probcin",
        "Adam Czajka"
      ],
      "abstract": "Can we teach machines to assess the expertise of humans solving visual tasks automatically based on eye tracking features? This paper proposes AutoSIGHT, Automatic System for Immediate Grading of Human experTise, that classifies expert and non-expert performers, and builds upon an ensemble of features extracted from eye tracking data while the performers were solving a visual task. Results on the task of iris Presentation Attack Detection (PAD) used for this study show that with a small evaluation window of just 5 seconds, AutoSIGHT achieves an average average Area Under the ROC curve performance of 0.751 in subject-disjoint train-test regime, indicating that such detection is viable. Furthermore, when a larger evaluation window of up to 30 seconds is available, the Area Under the ROC curve (AUROC) increases to 0.8306, indicating the model is effectively leveraging more information at a cost of slightly delayed decisions. This work opens new areas of research on how to incorporate the automatic weighing of human and machine expertise into human-AI pairing setups, which need to react dynamically to nonstationary expertise distribution between the human and AI players (e.g. when the experts need to be replaced, or the task at hand changes rapidly). Along with this paper, we offer the eye tracking data used in this study collected from 6 experts and 53 non-experts solving iris PAD visual task.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† AutoSIGHTï¼Œä¸€ç§åŸºäºçœ¼åŠ¨è¿½è¸ª (eye tracking) ç‰¹å¾è‡ªåŠ¨è¯„ä¼°äººç±»åœ¨æ‰§è¡Œè§†è§‰ä»»åŠ¡æ—¶ä¸“ä¸šæ°´å¹³çš„ç³»ç»Ÿã€‚è¯¥ç³»ç»Ÿåˆ©ç”¨ä»è™¹è†œå‘ˆç°æ”»å‡»æ£€æµ‹ (iris Presentation Attack Detection, PAD) ä»»åŠ¡ä¸­æå–çš„é›†æˆç‰¹å¾ï¼Œå®ç°äº†å¯¹ä¸“å®¶ä¸éä¸“å®¶æ‰§è¡Œè€…çš„æœ‰æ•ˆåˆ†ç±»ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œåœ¨ä»… 5 ç§’çš„è¯„ä¼°çª—å£ä¸‹ï¼ŒAutoSIGHT çš„å¹³å‡æ›²çº¿ä¸‹é¢ç§¯ (AUROC) è¾¾åˆ° 0.751ï¼›å½“çª—å£å»¶é•¿è‡³ 30 ç§’æ—¶ï¼ŒAUROC æå‡è‡³ 0.8306ï¼Œæ˜¾ç¤ºå‡ºæ¨¡å‹é€šè¿‡å¢åŠ å†³ç­–å»¶è¿Ÿæ¥æ¢å–æ›´é«˜å‡†ç¡®æ€§çš„èƒ½åŠ›ã€‚è¿™é¡¹å·¥ä½œä¸ºåœ¨åŠ¨æ€å˜åŒ–çš„äººæœºåä½œ (human-AI pairing) ç¯å¢ƒä¸­å¦‚ä½•è‡ªåŠ¨æƒè¡¡äººç±»ä¸æœºå™¨çš„ä¸“ä¸šçŸ¥è¯†æä¾›äº†æ–°é€”å¾„ã€‚ç ”ç©¶è¿˜åŒæ­¥å‘å¸ƒäº†ä¸€ä¸ªåŒ…å« 6 åä¸“å®¶å’Œ 53 åéä¸“å®¶åœ¨æ‰§è¡Œè™¹è†œ PAD ä»»åŠ¡æ—¶çš„çœ¼åŠ¨è¿½è¸ªæ•°æ®é›†ï¼Œä¸ºåç»­ç ”ç©¶å¥ å®šäº†åŸºç¡€ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "This work has been accepted for publication in the proceedings of the IEEE VL/HCC conference 2025. The final published version will be available via IEEE Xplore",
      "pdf_url": "https://arxiv.org/pdf/2508.01015v1",
      "published_date": "2025-08-01 18:28:13 UTC",
      "updated_date": "2025-08-01 18:28:13 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T08:03:05.398306+00:00"
    },
    {
      "arxiv_id": "2508.01013v1",
      "title": "On Some Tunable Multi-fidelity Bayesian Optimization Frameworks",
      "title_zh": "è®ºè‹¥å¹²å¯è°ƒå¤šä¿çœŸåº¦è´å¶æ–¯ä¼˜åŒ–æ¡†æ¶",
      "authors": [
        "Arjun Manoj",
        "Anastasia S. Georgiou",
        "Dimitris G. Giovanis",
        "Themistoklis P. Sapsis",
        "Ioannis G. Kevrekidis"
      ],
      "abstract": "Multi-fidelity optimization employs surrogate models that integrate information from varying levels of fidelity to guide efficient exploration of complex design spaces while minimizing the reliance on (expensive) high-fidelity objective function evaluations. To advance Gaussian Process (GP)-based multi-fidelity optimization, we implement a proximity-based acquisition strategy that simplifies fidelity selection by eliminating the need for separate acquisition functions at each fidelity level. We also enable multi-fidelity Upper Confidence Bound (UCB) strategies by combining them with multi-fidelity GPs rather than the standard GPs typically used. We benchmark these approaches alongside other multi-fidelity acquisition strategies (including fidelity-weighted approaches) comparing their performance, reliance on high-fidelity evaluations, and hyperparameter tunability in representative optimization tasks. The results highlight the capability of the proximity-based multi-fidelity acquisition function to deliver consistent control over high-fidelity usage while maintaining convergence efficiency. Our illustrative examples include multi-fidelity chemical kinetic models, both homogeneous and heterogeneous (dynamic catalysis for ammonia production).",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†å¯è°ƒçš„å¤šä¿çœŸåº¦ Bayesian Optimization æ¡†æ¶ï¼Œæ—¨åœ¨åˆ©ç”¨ä¸åŒç²¾åº¦çº§åˆ«çš„ä¿¡æ¯æ¥å‡å°‘å¯¹æ˜‚è´µçš„é«˜ä¿çœŸåº¦ç›®æ ‡å‡½æ•°è¯„ä¼°çš„ä¾èµ–ã€‚ä½œè€…å®ç°äº†ä¸€ç§åŸºäºé‚»è¿‘åº¦çš„é‡‡æ ·ç­–ç•¥ (proximity-based acquisition strategy)ï¼Œè¯¥ç­–ç•¥é€šè¿‡æ¶ˆé™¤æ¯ä¸ªä¿çœŸåº¦çº§åˆ«å¯¹ç‹¬ç«‹é‡‡æ ·å‡½æ•°çš„ä¾èµ–ï¼Œæ˜¾è‘—ç®€åŒ–äº†ä¿çœŸåº¦é€‰æ‹©è¿‡ç¨‹ã€‚åŒæ—¶ï¼Œç ”ç©¶å°† Upper Confidence Bound (UCB) ç­–ç•¥ä¸å¤šä¿çœŸåº¦ Gaussian Process (GP) ç›¸ç»“åˆï¼Œè¿›ä¸€æ­¥æå‡äº†æ¡†æ¶çš„é€‚ç”¨æ€§ã€‚é€šè¿‡åœ¨å‡åŒ€å’Œéå‡åŒ€åŒ–å­¦åŠ¨åŠ›å­¦æ¨¡å‹ï¼ˆå¦‚æ°¨ç”Ÿäº§ä¸­çš„åŠ¨æ€å‚¬åŒ–ï¼‰ç­‰å…¸å‹ä»»åŠ¡ä¸­è¿›è¡ŒåŸºå‡†æµ‹è¯•ï¼ŒéªŒè¯äº†è¯¥æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒåŸºäºé‚»è¿‘åº¦çš„å¤šä¿çœŸåº¦é‡‡æ ·å‡½æ•°åœ¨ä¿æŒæ”¶æ•›æ•ˆç‡çš„åŒæ—¶ï¼Œèƒ½å¤Ÿå¯¹é«˜ä¿çœŸåº¦è¯„ä¼°èµ„æºçš„ä½¿ç”¨å®ç°ä¸€è‡´ä¸”ç¨³å®šçš„æ§åˆ¶ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "math.OC"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.01013v1",
      "published_date": "2025-08-01 18:26:39 UTC",
      "updated_date": "2025-08-01 18:26:39 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T08:03:14.346633+00:00"
    },
    {
      "arxiv_id": "2508.01012v1",
      "title": "AutoEDA: Enabling EDA Flow Automation through Microservice-Based LLM Agents",
      "title_zh": "AutoEDAï¼šåŸºäºå¾®æœåŠ¡å¤§è¯­è¨€æ¨¡å‹æ™ºèƒ½ä½“çš„EDAæµç¨‹è‡ªåŠ¨åŒ–å®ç°",
      "authors": [
        "Yiyi Lu",
        "Hoi Ian Au",
        "Junyao Zhang",
        "Jingyu Pan",
        "Yiting Wang",
        "Ang Li",
        "Jianyi Zhang",
        "Yiran Chen"
      ],
      "abstract": "Modern Electronic Design Automation (EDA) workflows, especially the RTL-to-GDSII flow, require heavily manual scripting and demonstrate a multitude of tool-specific interactions which limits scalability and efficiency. While LLMs introduces strides for automation, existing LLM solutions require expensive fine-tuning and do not contain standardized frameworks for integration and evaluation. We introduce AutoEDA, a framework for EDA automation that leverages paralleled learning through the Model Context Protocol (MCP) specific for standardized and scalable natural language experience across the entire RTL-to-GDSII flow. AutoEDA limits fine-tuning through structured prompt engineering, implements intelligent parameter extraction and task decomposition, and provides an extended CodeBLEU metric to evaluate the quality of TCL scripts. Results from experiments over five previously curated benchmarks show improvements in automation accuracy and efficiency, as well as script quality when compared to existing methods. AutoEDA is released open-sourced to support reproducibility and the EDA community. Available at: https://github.com/AndyLu666/MCP-EDA-Server",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ç°ä»£ç”µå­è®¾è®¡è‡ªåŠ¨åŒ–(Electronic Design Automation, EDA)å·¥ä½œæµï¼ˆå°¤å…¶æ˜¯RTL-to-GDSIIæµç¨‹ï¼‰é«˜åº¦ä¾èµ–äººå·¥è„šæœ¬ä¸”ç°æœ‰å¤§è¯­è¨€æ¨¡å‹(LLMs)æ–¹æ¡ˆå¾®è°ƒæˆæœ¬é«˜æ˜‚ç­‰æŒ‘æˆ˜ï¼Œæå‡ºäº†AutoEDAè‡ªåŠ¨åŒ–æ¡†æ¶ã€‚è¯¥æ¡†æ¶åˆ©ç”¨æ¨¡å‹ä¸Šä¸‹æ–‡åè®®(Model Context Protocol, MCP)æ„å»ºåŸºäºå¾®æœåŠ¡çš„æ™ºèƒ½ä½“ç³»ç»Ÿï¼Œå®ç°äº†è·¨æµç¨‹çš„æ ‡å‡†ã€å¯æ‰©å±•è‡ªç„¶è¯­è¨€äº¤äº’ä½“éªŒã€‚AutoEDAé€šè¿‡ç»“æ„åŒ–æç¤ºå·¥ç¨‹(Structured Prompt Engineering)æ˜¾è‘—é™ä½äº†å¯¹å¾®è°ƒçš„ä¾èµ–ï¼Œå¹¶é›†æˆäº†æ™ºèƒ½å‚æ•°æå–ä¸ä»»åŠ¡åˆ†è§£æŠ€æœ¯ã€‚æ­¤å¤–ï¼Œç ”ç©¶å›¢é˜Ÿè¿˜æå‡ºäº†ä¸€ç§æ‰©å±•çš„CodeBLEUæŒ‡æ ‡ï¼Œä¸“é—¨ç”¨äºç²¾ç¡®è¯„ä¼°ç”Ÿæˆçš„TCLè„šæœ¬è´¨é‡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒAutoEDAåœ¨äº”ä¸ªæƒå¨åŸºå‡†æµ‹è¯•ä¸­çš„è‡ªåŠ¨åŒ–å‡†ç¡®ç‡ã€æ•ˆç‡åŠè„šæœ¬è´¨é‡å‡ä¼˜äºç°æœ‰æ–¹æ³•ã€‚è¯¥é¡¹ç›®å·²åœ¨GitHubå¼€æºï¼Œæ—¨åœ¨ä¸ºEDAç¤¾åŒºæä¾›å¯é‡ç°çš„è‡ªåŠ¨åŒ–æµç¨‹æ”¯æŒã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.01012v1",
      "published_date": "2025-08-01 18:23:57 UTC",
      "updated_date": "2025-08-01 18:23:57 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T08:03:21.091208+00:00"
    },
    {
      "arxiv_id": "2508.01010v2",
      "title": "v-PuNNs: van der Put Neural Networks for Transparent Ultrametric Representation Learning",
      "title_zh": "v-PuNNsï¼šç”¨äºé€æ˜è¶…åº¦é‡è¡¨ç¤ºå­¦ä¹ çš„ van der Put ç¥ç»ç½‘ç»œ",
      "authors": [
        "Gnankan Landry Regis N'guessan"
      ],
      "abstract": "Conventional deep learning models embed data in Euclidean space $\\mathbb{R}^d$, a poor fit for strictly hierarchical objects such as taxa, word senses, or file systems. We introduce van der Put Neural Networks (v-PuNNs), the first architecture whose neurons are characteristic functions of p-adic balls in $\\mathbb{Z}_p$. Under our Transparent Ultrametric Representation Learning (TURL) principle every weight is itself a p-adic number, giving exact subtree semantics. A new Finite Hierarchical Approximation Theorem shows that a depth-K v-PuNN with $\\sum_{j=0}^{K-1}p^{\\,j}$ neurons universally represents any K-level tree. Because gradients vanish in this discrete space, we propose Valuation-Adaptive Perturbation Optimization (VAPO), with a fast deterministic variant (HiPaN-DS) and a moment-based one (HiPaN / Adam-VAPO). On three canonical benchmarks our CPU-only implementation sets new state-of-the-art: WordNet nouns (52,427 leaves) 99.96% leaf accuracy in 16 min; GO molecular-function 96.9% leaf / 100% root in 50 s; NCBI Mammalia Spearman $Ï= -0.96$ with true taxonomic distance. The learned metric is perfectly ultrametric (zero triangle violations), and its fractal and information-theoretic properties are analyzed. Beyond classification we derive structural invariants for quantum systems (HiPaQ) and controllable generative codes for tabular data (Tab-HiPaN). v-PuNNs therefore bridge number theory and deep learning, offering exact, interpretable, and efficient models for hierarchical data.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† v-PuNNs (van der Put Neural Networks)ï¼Œè¿™æ˜¯é¦–ä¸ªå°†ç¥ç»å…ƒæ„å»ºä¸º $\\mathbb{Z}_p$ ä¸­ p-adic çƒç‰¹å¾å‡½æ•°çš„æ¶æ„ï¼Œæ—¨åœ¨è§£å†³ä¼ ç»Ÿæ¬§å‡ é‡Œå¾—ç©ºé—´æ¨¡å‹åœ¨å¤„ç†å±‚çº§æ•°æ®æ—¶çš„å±€é™æ€§ã€‚è¯¥æ¨¡å‹éµå¾ªé€æ˜è¶…åº¦é‡è¡¨ç¤ºå­¦ä¹  (Transparent Ultrametric Representation Learning, TURL) åŸåˆ™ï¼Œä½¿æ¯ä¸ªæƒé‡å‡ä¸º p-adic æ•°ï¼Œä»è€Œèµ‹äºˆæ¨¡å‹ç²¾ç¡®çš„å­æ ‘è¯­ä¹‰ã€‚ä¸ºäº†å…‹æœç¦»æ•£ç©ºé—´ä¸­çš„æ¢¯åº¦æ¶ˆå¤±é—®é¢˜ï¼Œç ”ç©¶è€…æå‡ºäº†ä¼°å€¼è‡ªé€‚åº”æ‰°åŠ¨ä¼˜åŒ– (Valuation-Adaptive Perturbation Optimization, VAPO) ç®—æ³•åŠå…¶é«˜æ•ˆå˜ä½“ã€‚ç†è®ºä¸Šï¼Œæœ‰é™å±‚çº§é€¼è¿‘å®šç† (Finite Hierarchical Approximation Theorem) ä¿è¯äº†è¯¥æ¶æ„å¯¹ä»»æ„ K å±‚æ ‘çš„é€šç”¨è¡¨ç¤ºèƒ½åŠ›ã€‚å®éªŒè¯æ˜ï¼Œè¯¥ CPU å®ç°æ–¹æ¡ˆåœ¨ WordNet å’Œ NCBI å“ºä¹³åŠ¨ç‰©åˆ†ç±»ç­‰ä»»åŠ¡ä¸­è¾¾åˆ°äº†æ–°çš„ SOTA æ°´å¹³ï¼Œå¹¶å®ç°äº†é›¶ä¸‰è§’å½¢è¿çº¦çš„å®Œç¾è¶…åº¦é‡ç‰¹æ€§ã€‚è¯¥æˆæœæˆåŠŸæ¡¥æ¥äº†æ•°è®ºä¸æ·±åº¦å­¦ä¹ ï¼Œä¸ºå±‚çº§ç»“æ„ã€é‡å­ç³»ç»ŸåŠè¡¨æ ¼æ•°æ®æä¾›äº†ä¸€ç§ç²¾ç¡®ã€å¯è§£é‡Šä¸”é«˜æ•ˆçš„å»ºæ¨¡æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "v2: Corrected mathematical statements in Section 3.1.3 and Appendix A regarding the van der Put basis properties. Clarified distinction between hierarchical indicator family and classical Schauder basis",
      "pdf_url": "https://arxiv.org/pdf/2508.01010v2",
      "published_date": "2025-08-01 18:23:38 UTC",
      "updated_date": "2026-01-05 12:06:41 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T08:03:23.351539+00:00"
    },
    {
      "arxiv_id": "2508.01003v1",
      "title": "Generative AI Adoption in Postsecondary Education, AI Hype, and ChatGPT's Launch",
      "title_zh": "é«˜ç­‰æ•™è‚²ä¸­ç”Ÿæˆå¼äººå·¥æ™ºèƒ½çš„é‡‡ç”¨ã€AI ç‚’ä½œä¸ ChatGPT çš„å‘å¸ƒ",
      "authors": [
        "Isabel Pedersen"
      ],
      "abstract": "The rapid integration of generative artificial intelligence (AI) into postsecondary education and many other sectors resulted in a global reckoning with this new technology. This paper contributes to the study of the multifaceted influence of generative AI, with a particular focus on OpenAI's ChatGPT within academic settings during the first six months after the release in three specific ways. First, it scrutinizes the rise of ChatGPT as a transformative event construed through a study of mainstream discourses exhibiting AI hype. Second, it discusses the perceived implications of generative AI for writing, teaching, and learning through the lens of critical discourse analysis and critical AI studies. Third, it encourages the necessity for best practices in the adoption of generative AI technologies in education.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†ç”Ÿæˆå¼äººå·¥æ™ºèƒ½ (Generative AI)ï¼Œç‰¹åˆ«æ˜¯ OpenAI çš„ ChatGPTï¼Œåœ¨å‘å¸ƒåå‰å…­ä¸ªæœˆå†…å¯¹é«˜ç­‰æ•™è‚²äº§ç”Ÿçš„å¤šæ–¹é¢å½±å“ã€‚è®ºæ–‡é¦–å…ˆé€šè¿‡åˆ†æä¸»æµè¯è¯­ä¸­å‡ºç°çš„ AI Hype ç°è±¡ï¼Œå®¡è§†äº† ChatGPT ä½œä¸ºä¸€ä¸ªå˜é©æ€§äº‹ä»¶çš„å…´èµ·ã€‚éšåï¼Œè¯¥ç ”ç©¶åˆ©ç”¨æ‰¹åˆ¤æ€§è¯è¯­åˆ†æ (Critical Discourse Analysis) å’Œæ‰¹åˆ¤æ€§äººå·¥æ™ºèƒ½ç ”ç©¶ (Critical AI Studies) çš„è§†è§’ï¼Œæ·±å…¥è®¨è®ºäº†ç”Ÿæˆå¼äººå·¥æ™ºèƒ½å¯¹å†™ä½œã€æ•™å­¦å’Œå­¦ä¹ å¸¦æ¥çš„æ„ŸçŸ¥å½±å“ã€‚æœ€åï¼Œç ”ç©¶å¼ºè°ƒäº†åœ¨æ•™è‚²é¢†åŸŸé‡‡ç”¨ç›¸å…³æŠ€æœ¯æ—¶ï¼Œåˆ¶å®šå’Œéµå¾ªæœ€ä½³å®è·µ (Best Practices) çš„ç´§è¿«æ€§ä¸å¿…è¦æ€§ã€‚æ€»ä½“è€Œè¨€ï¼Œæœ¬æ–‡ä¸ºç†è§£æ–°æŠ€æœ¯åœ¨å­¦æœ¯ç¯å¢ƒä¸­çš„åˆæ­¥æ•´åˆæä¾›äº†æ‰¹åˆ¤æ€§è§†è§’ï¼Œå¹¶ä¸ºåç»­çš„æ•™è‚²å®è·µæä¾›äº†æŒ‡å¯¼ã€‚",
      "categories": [
        "cs.CY",
        "cs.AI"
      ],
      "primary_category": "cs.CY",
      "comment": "19 pages",
      "pdf_url": "https://arxiv.org/pdf/2508.01003v1",
      "published_date": "2025-08-01 18:14:36 UTC",
      "updated_date": "2025-08-01 18:14:36 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T08:03:23.141822+00:00"
    },
    {
      "arxiv_id": "2508.00998v2",
      "title": "Are LLM-Powered Social Media Bots Realistic?",
      "title_zh": "åŸºäºå¤§è¯­è¨€æ¨¡å‹çš„ç¤¾äº¤åª’ä½“æœºå™¨äººæ˜¯å¦å…·æœ‰çœŸå®æ€§ï¼Ÿ",
      "authors": [
        "Lynnette Hui Xian Ng",
        "Kathleen M. Carley"
      ],
      "abstract": "As Large Language Models (LLMs) become more sophisticated, there is a possibility to harness LLMs to power social media bots. This work investigates the realism of generating LLM-Powered social media bot networks. Through a combination of manual effort, network science and LLMs, we create synthetic bot agent personas, their tweets and their interactions, thereby simulating social media networks. We compare the generated networks against empirical bot/human data, observing that both network and linguistic properties of LLM-Powered Bots differ from Wild Bots/Humans. This has implications towards the detection and effectiveness of LLM-Powered Bots.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†åŸºäºå¤§è¯­è¨€æ¨¡å‹(LLMs)æ„å»ºçš„ç¤¾äº¤åª’ä½“æœºå™¨äººç½‘ç»œçš„ç°å®æ€§é—®é¢˜ã€‚ç ”ç©¶äººå‘˜é€šè¿‡ç»“åˆäººå·¥å¹²é¢„ã€ç½‘ç»œç§‘å­¦(Network Science)å’ŒLLMsï¼Œåˆ›å»ºäº†åˆæˆçš„æœºå™¨äººä»£ç†è§’è‰²(Bot Agent Personas)åŠå…¶æ¨æ–‡ä¸äº’åŠ¨ï¼Œæ¨¡æ‹Ÿäº†å¤æ‚çš„ç¤¾äº¤åª’ä½“ç½‘ç»œã€‚é€šè¿‡å°†è¿™äº›ç”Ÿæˆçš„ç½‘ç»œä¸çœŸå®çš„æœºå™¨äºº(Wild Bots)åŠäººç±»æ•°æ®è¿›è¡Œå¯¹æ¯”ï¼Œç ”ç©¶å‘ç°LLM-Powered Botsåœ¨ç½‘ç»œå±æ€§å’Œè¯­è¨€ç‰¹å¾(Linguistic Properties)ä¸Šå‡ä¸çœŸå®çš„æœºå™¨äººæˆ–äººç±»å­˜åœ¨æ˜¾è‘—å·®å¼‚ã€‚è¿™ä¸€å‘ç°æ­ç¤ºäº†å½“å‰LLMé©±åŠ¨çš„æœºå™¨äººåœ¨å®Œå…¨æ¨¡æ‹ŸçœŸå®ç¤¾äº¤ç”Ÿæ€ç³»ç»Ÿæ–¹é¢çš„å±€é™æ€§ã€‚è¯¥ç ”ç©¶çš„ç»“æœå¯¹äºæœªæ¥å¦‚ä½•æœ‰æ•ˆæ£€æµ‹ä»¥åŠè¯„ä¼°LLM-Powered Botsçš„å½±å“åŠ›æä¾›äº†é‡è¦çš„å‚è€ƒä¾æ®ã€‚",
      "categories": [
        "cs.SI",
        "cs.AI"
      ],
      "primary_category": "cs.SI",
      "comment": "Accepted into SBP-BRiMS 2025",
      "pdf_url": "https://arxiv.org/pdf/2508.00998v2",
      "published_date": "2025-08-01 18:06:13 UTC",
      "updated_date": "2025-08-22 17:56:26 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T08:03:23.550863+00:00"
    },
    {
      "arxiv_id": "2508.00974v1",
      "title": "ThermoCycleNet: Stereo-based Thermogram Labeling for Model Transition to Cycling",
      "title_zh": "ThermoCycleNetï¼šé¢å‘éª‘è¡Œè¿åŠ¨æ¨¡å‹è¿ç§»çš„åŸºäºç«‹ä½“è§†è§‰çš„çƒ­å›¾æ ‡æ³¨",
      "authors": [
        "Daniel AndrÃ©s LÃ³pez",
        "Vincent Weber",
        "Severin Zentgraf",
        "Barlo Hillen",
        "Perikles Simon",
        "Elmar SchÃ¶mer"
      ],
      "abstract": "Infrared thermography is emerging as a powerful tool in sports medicine, allowing assessment of thermal radiation during exercise and analysis of anatomical regions of interest, such as the well-exposed calves. Building on our previous advanced automatic annotation method, we aimed to transfer the stereo- and multimodal-based labeling approach from treadmill running to ergometer cycling. Therefore, the training of the semantic segmentation network with automatic labels and fine-tuning on high-quality manually annotated images has been examined and compared in different data set combinations. The results indicate that fine-tuning with a small fraction of manual data is sufficient to improve the overall performance of the deep neural network. Finally, combining automatically generated labels with small manually annotated data sets accelerates the adaptation of deep neural networks to new use cases, such as the transition from treadmill to bicycle.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† ThermoCycleNetï¼Œæ—¨åœ¨å°†åŸºäºç«‹ä½“(stereo-based)å’Œå¤šæ¨¡æ€(multimodal-based)çš„çº¢å¤–çƒ­æˆåƒ(infrared thermography)è‡ªåŠ¨æ ‡æ³¨æ–¹æ³•ä»è·‘æ­¥æœºè·‘æ­¥åœºæ™¯è¿ç§»è‡³åŠŸç‡è®¡éª‘è¡Œåœºæ™¯ã€‚ç ”ç©¶è€…æ¢è®¨äº†ä½¿ç”¨è‡ªåŠ¨ç”Ÿæˆçš„æ ‡ç­¾è®­ç»ƒè¯­ä¹‰åˆ†å‰²ç½‘ç»œ(semantic segmentation network)ï¼Œå¹¶ç»“åˆé«˜è´¨é‡æ‰‹åŠ¨æ ‡æ³¨å›¾åƒè¿›è¡Œå¾®è°ƒ(fine-tuning)çš„æ•ˆæœã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œä»…éœ€å°‘é‡æ‰‹åŠ¨æ ‡æ³¨æ•°æ®è¿›è¡Œå¾®è°ƒï¼Œå³å¯æ˜¾è‘—æå‡æ·±åº¦ç¥ç»ç½‘ç»œ(deep neural network)çš„æ•´ä½“æ€§èƒ½ã€‚é€šè¿‡ç»“åˆè‡ªåŠ¨ç”Ÿæˆçš„æ ‡ç­¾ä¸å°å‹æ‰‹åŠ¨æ ‡æ³¨æ•°æ®é›†ï¼Œè¯¥æ–¹æ³•æˆåŠŸåŠ é€Ÿäº†æ¨¡å‹åœ¨è¿åŠ¨åŒ»å­¦é¢†åŸŸæ–°åº”ç”¨åœºæ™¯ä¸‹çš„é€‚é…ï¼Œæœ‰æ•ˆå®ç°äº†ä»è·‘æ­¥åˆ°éª‘è¡Œä»»åŠ¡çš„æ¨¡å‹è¿‡æ¸¡ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "Presented at IWANN 2025 18th International Work-Conference on Artificial Neural Networks, A CoruÃ±a, Spain, 16-18 June, 2025. Book of abstracts: ISBN: 979-13-8752213-1. Funding: Johannes Gutenberg University \"Stufe I'': \"Start ThermoCycleNet''. Partial funding: Carl-Zeiss-Stiftung: \"Multi-dimensionAI'' (CZS-Project number: P2022-08-010)",
      "pdf_url": "https://arxiv.org/pdf/2508.00974v1",
      "published_date": "2025-08-01 17:55:00 UTC",
      "updated_date": "2025-08-01 17:55:00 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T08:03:43.351482+00:00"
    },
    {
      "arxiv_id": "2508.02729v1",
      "title": "Interpreting Performance Profiles with Deep Learning",
      "title_zh": "åŸºäºæ·±åº¦å­¦ä¹ çš„æ€§èƒ½å‰–æè§£è¯»",
      "authors": [
        "Zhuoran Liu"
      ],
      "abstract": "Profiling tools (also known as profilers) play an important role in understanding program performance at runtime, such as hotspots, bottlenecks, and inefficiencies. While profilers have been proven to be useful, they give extra burden to software engineers. Software engineers, as the users, are responsible to interpret the complex performance data and identify actionable optimization in program source code. However, it can be challenging for users to associate inefficiencies with the program semantics, especially if the users are not the authors of the code, which limits the applicability of profilers.\n  In this thesis, we explore a new direction to combine performance profiles and program semantics with a deep learning approach. The key idea is to glean code summary for semantic information (at a certain level) and integrate it into a profiler, which can better understand program inefficiencies for actionable optimization. To be concrete, we combine profiles generated by Async Profiler (the state-of-the-art Java profiler) with code summarization from a fine-tuned CodeBERT-based model. We demonstrate the code summaries of any selected call path in a graphic user interface. Our system can effectively assist analysis on many Java benchmarks.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹è½¯ä»¶å·¥ç¨‹å¸ˆåœ¨è§£è¯»å¤æ‚æ€§èƒ½åˆ†ææ•°æ®ï¼ˆPerformance Profilesï¼‰å¹¶å°†å…¶ä¸ç¨‹åºè¯­ä¹‰å…³è”ä»¥è¯†åˆ«ä¼˜åŒ–ç‚¹æ—¶çš„å›°éš¾ï¼Œæ¢ç´¢äº†ç»“åˆæ·±åº¦å­¦ä¹ ï¼ˆDeep Learningï¼‰ä¸æ€§èƒ½å‰–æçš„æ–°æ–¹å‘ã€‚å…¶æ ¸å¿ƒæ€æƒ³æ˜¯æå–ä»£ç æ‘˜è¦ï¼ˆCode Summaryï¼‰ä»¥è·å–è¯­ä¹‰ä¿¡æ¯ï¼Œå¹¶å°†å…¶æ•´åˆåˆ°å‰–æå™¨ï¼ˆProfilerï¼‰ä¸­ï¼Œä»¥ä¾¿æ›´å¥½åœ°ç†è§£ç¨‹åºä½æ•ˆåŸå› å¹¶å®ç°å¯æ“ä½œçš„ä¼˜åŒ–ã€‚å…·ä½“è€Œè¨€ï¼Œç ”ç©¶å°† Async Profiler ç”Ÿæˆçš„å‰–æç»“æœä¸åŸºäºå¾®è°ƒ CodeBERT æ¨¡å‹ç”Ÿæˆçš„ä»£ç æ‘˜è¦ç›¸ç»“åˆï¼Œé€šè¿‡å›¾å½¢ç”¨æˆ·ç•Œé¢ï¼ˆGUIï¼‰å±•ç¤ºé€‰å®šè°ƒç”¨è·¯å¾„ï¼ˆCall Pathï¼‰çš„è¯­ä¹‰æ€»ç»“ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥ç³»ç»Ÿåœ¨å¤šä¸ª Java åŸºå‡†æµ‹è¯•ä¸­èƒ½å¤Ÿæœ‰æ•ˆè¾…åŠ©æ€§èƒ½åˆ†æï¼Œæ˜¾è‘—é™ä½äº†å¼€å‘è€…ç†è§£ç¨‹åºè¿è¡Œæ•ˆç‡é—®é¢˜çš„é—¨æ§›ã€‚",
      "categories": [
        "cs.SE",
        "cs.AI",
        "cs.PF"
      ],
      "primary_category": "cs.SE",
      "comment": "Master of Science in Computer Science thesis, North Carolina State University, 2022. Advisor: Dr. Xu Liu",
      "pdf_url": "https://arxiv.org/pdf/2508.02729v1",
      "published_date": "2025-08-01 17:23:41 UTC",
      "updated_date": "2025-08-01 17:23:41 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T08:03:46.139191+00:00"
    },
    {
      "arxiv_id": "2508.00788v1",
      "title": "Do They Understand Them? An Updated Evaluation on Nonbinary Pronoun Handling in Large Language Models",
      "title_zh": "å®ƒä»¬ç†è§£â€œä»–ä»¬â€å—ï¼Ÿå¤§è¯­è¨€æ¨¡å‹å¯¹éäºŒå…ƒæ€§åˆ«ä»£è¯å¤„ç†èƒ½åŠ›çš„æœ€æ–°è¯„ä¼°",
      "authors": [
        "Xushuo Tang",
        "Yi Ding",
        "Zhengyi Yang",
        "Yin Chen",
        "Yongrui Gu",
        "Wenke Yang",
        "Mingchen Ju",
        "Xin Cao",
        "Yongfei Liu",
        "Wenjie Zhang"
      ],
      "abstract": "Large language models (LLMs) are increasingly deployed in sensitive contexts where fairness and inclusivity are critical. Pronoun usage, especially concerning gender-neutral and neopronouns, remains a key challenge for responsible AI. Prior work, such as the MISGENDERED benchmark, revealed significant limitations in earlier LLMs' handling of inclusive pronouns, but was constrained to outdated models and limited evaluations. In this study, we introduce MISGENDERED+, an extended and updated benchmark for evaluating LLMs' pronoun fidelity. We benchmark five representative LLMs, GPT-4o, Claude 4, DeepSeek-V3, Qwen Turbo, and Qwen2.5, across zero-shot, few-shot, and gender identity inference. Our results show notable improvements compared with previous studies, especially in binary and gender-neutral pronoun accuracy. However, accuracy on neopronouns and reverse inference tasks remains inconsistent, underscoring persistent gaps in identity-sensitive reasoning. We discuss implications, model-specific observations, and avenues for future inclusive AI research.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤§è¯­è¨€æ¨¡å‹ (LLMs) åœ¨å¤„ç†æ€§åˆ«ä¸­æ€§ä»£è¯åŠæ–°ä»£è¯ (neopronouns) æ—¶é¢ä¸´çš„å…¬å¹³æ€§ä¸åŒ…å®¹æ€§æŒ‘æˆ˜ï¼Œæå‡ºäº†æ‰©å±•åçš„æ›´æ–°åŸºå‡† MISGENDERED+ã€‚ç ”ç©¶å›¢é˜Ÿå¯¹ GPT-4oã€Claude 4ã€DeepSeek-V3ã€Qwen Turbo åŠ Qwen2.5 äº”æ¬¾ä»£è¡¨æ€§æ¨¡å‹è¿›è¡Œäº†é›¶æ ·æœ¬ (zero-shot)ã€å°‘æ ·æœ¬ (few-shot) å’Œæ€§åˆ«èº«ä»½æ¨ç†æµ‹è¯•ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œå°½ç®¡è¿™äº›æ¨¡å‹åœ¨äºŒå…ƒæ€§åˆ«å’Œä¸­æ€§ä»£è¯çš„å‡†ç¡®ç‡ä¸Šè¾ƒä»¥å¾€ç ”ç©¶æœ‰æ˜¾è‘—è¿›æ­¥ï¼Œä½†åœ¨æ–°ä»£è¯ (neopronouns) å’Œåå‘æ¨ç†ä»»åŠ¡ä¸­çš„è¡¨ç°ä¾ç„¶å‚å·®ä¸é½ã€‚è¿™åæ˜ å‡º LLMs åœ¨èº«ä»½æ•æ„Ÿæ¨ç† (identity-sensitive reasoning) æ–¹é¢ä»å­˜åœ¨æ˜æ˜¾çŸ­æ¿ã€‚è¯¥é¡¹ç ”ç©¶é€šè¿‡è¯¦ç»†çš„æ¨¡å‹è§‚å¯Ÿï¼Œä¸ºå®ç°æ›´å…·åŒ…å®¹æ€§çš„ AI ç ”ç©¶æŒ‡æ˜äº†æ–¹å‘ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.00788v1",
      "published_date": "2025-08-01 17:11:42 UTC",
      "updated_date": "2025-08-01 17:11:42 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T08:03:46.435063+00:00"
    },
    {
      "arxiv_id": "2508.00784v1",
      "title": "Unraveling Hidden Representations: A Multi-Modal Layer Analysis for Better Synthetic Content Forensics",
      "title_zh": "æ­ç¤ºéšè—è¡¨å¾ï¼šç”¨äºæå‡åˆæˆå†…å®¹å–è¯èƒ½åŠ›çš„å¤šæ¨¡æ€å±‚çº§åˆ†æ",
      "authors": [
        "Tom Or",
        "Omri Azencot"
      ],
      "abstract": "Generative models achieve remarkable results in multiple data domains, including images and texts, among other examples. Unfortunately, malicious users exploit synthetic media for spreading misinformation and disseminating deepfakes. Consequently, the need for robust and stable fake detectors is pressing, especially when new generative models appear everyday. While the majority of existing work train classifiers that discriminate between real and fake information, such tools typically generalize only within the same family of generators and data modalities, yielding poor results on other generative classes and data domains. Towards a universal classifier, we propose the use of large pre-trained multi-modal models for the detection of generative content. Effectively, we show that the latent code of these models naturally captures information discriminating real from fake. Building on this observation, we demonstrate that linear classifiers trained on these features can achieve state-of-the-art results across various modalities, while remaining computationally efficient, fast to train, and effective even in few-shot settings. Our work primarily focuses on fake detection in audio and images, achieving performance that surpasses or matches that of strong baseline methods.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ç”Ÿæˆæ¨¡å‹å¼•å‘çš„æ·±åº¦ä¼ªé€ (deepfakes)å’Œè™šå‡ä¿¡æ¯ä¼ æ’­é—®é¢˜ï¼Œæå‡ºåˆ©ç”¨å¤§è§„æ¨¡é¢„è®­ç»ƒå¤šæ¨¡æ€æ¨¡å‹(large pre-trained multi-modal models)æ¥å®ç°æ›´é€šç”¨çš„ç”Ÿæˆå†…å®¹å–è¯ã€‚ä½œè€…å‘ç°è¿™äº›æ¨¡å‹å†…éƒ¨çš„æ½œåœ¨ä»£ç (latent code)èƒ½å¤Ÿè‡ªç„¶åœ°æ•æ‰åˆ°åŒºåˆ†çœŸå®ä¸ä¼ªé€ ä¿¡æ¯çš„å…³é”®ç‰¹å¾ï¼Œæœ‰æ•ˆè§£å†³äº†ä¼ ç»Ÿåˆ†ç±»å™¨åœ¨ä¸åŒç”Ÿæˆç®—æ³•å’Œæ•°æ®æ¨¡æ€é—´æ³›åŒ–èƒ½åŠ›ä¸è¶³çš„å±€é™ã€‚åŸºäºè¿™ä¸€å‘ç°ï¼Œç ”ç©¶å›¢é˜Ÿåœ¨æå–çš„ç‰¹å¾ä¸Šè®­ç»ƒäº†ç®€å•çš„çº¿æ€§åˆ†ç±»å™¨(linear classifiers)ï¼Œå¹¶åœ¨å›¾åƒå’ŒéŸ³é¢‘ç­‰å¤šç§æ¨¡æ€çš„æ£€æµ‹ä»»åŠ¡ä¸­å–å¾—äº†æœ€å…ˆè¿›(state-of-the-art)çš„æ•ˆæœã€‚å®éªŒç»“æœè¯æ˜ï¼Œè¯¥æ–¹æ³•ä¸ä»…è®¡ç®—æ•ˆç‡é«˜ã€è®­ç»ƒé€Ÿåº¦å¿«ï¼Œä¸”åœ¨å°‘æ ·æœ¬(few-shot)è®¾ç½®ä¸‹ä¾ç„¶è¡¨ç°å“è¶Šï¼Œæ€§èƒ½è¾¾åˆ°æˆ–è¶…è¶Šäº†ç°æœ‰çš„å¼ºåŸºçº¿æ–¹æ³•ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.00784v1",
      "published_date": "2025-08-01 17:07:00 UTC",
      "updated_date": "2025-08-01 17:07:00 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T08:03:57.553380+00:00"
    },
    {
      "arxiv_id": "2508.00782v1",
      "title": "SpA2V: Harnessing Spatial Auditory Cues for Audio-driven Spatially-aware Video Generation",
      "title_zh": "SpA2Vï¼šåˆ©ç”¨ç©ºé—´å¬è§‰çº¿ç´¢å®ç°éŸ³é¢‘é©±åŠ¨çš„ç©ºé—´æ„ŸçŸ¥è§†é¢‘ç”Ÿæˆ",
      "authors": [
        "Kien T. Pham",
        "Yingqing He",
        "Yazhou Xing",
        "Qifeng Chen",
        "Long Chen"
      ],
      "abstract": "Audio-driven video generation aims to synthesize realistic videos that align with input audio recordings, akin to the human ability to visualize scenes from auditory input. However, existing approaches predominantly focus on exploring semantic information, such as the classes of sounding sources present in the audio, limiting their ability to generate videos with accurate content and spatial composition. In contrast, we humans can not only naturally identify the semantic categories of sounding sources but also determine their deeply encoded spatial attributes, including locations and movement directions. This useful information can be elucidated by considering specific spatial indicators derived from the inherent physical properties of sound, such as loudness or frequency. As prior methods largely ignore this factor, we present SpA2V, the first framework explicitly exploits these spatial auditory cues from audios to generate videos with high semantic and spatial correspondence. SpA2V decomposes the generation process into two stages: 1) Audio-guided Video Planning: We meticulously adapt a state-of-the-art MLLM for a novel task of harnessing spatial and semantic cues from input audio to construct Video Scene Layouts (VSLs). This serves as an intermediate representation to bridge the gap between the audio and video modalities. 2) Layout-grounded Video Generation: We develop an efficient and effective approach to seamlessly integrate VSLs as conditional guidance into pre-trained diffusion models, enabling VSL-grounded video generation in a training-free manner. Extensive experiments demonstrate that SpA2V excels in generating realistic videos with semantic and spatial alignment to the input audios.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†SpA2Vï¼Œè¿™æ˜¯é¦–ä¸ªæ˜ç¡®åˆ©ç”¨ç©ºé—´éŸ³é¢‘çº¿ç´¢(spatial auditory cues)æ¥å®ç°éŸ³é¢‘é©±åŠ¨ä¸”å…·å¤‡ç©ºé—´æ„ŸçŸ¥è§†é¢‘ç”Ÿæˆçš„æ¡†æ¶ã€‚é’ˆå¯¹ç°æœ‰æ–¹æ³•ä¸»è¦å…³æ³¨è¯­ä¹‰ä¿¡æ¯è€Œå¿½è§†å£°éŸ³æºç©ºé—´å±æ€§çš„å±€é™æ€§ï¼ŒSpA2Vé€šè¿‡åˆ†æå“åº¦æˆ–é¢‘ç‡ç­‰ç‰©ç†æŒ‡æ ‡æ¥è§£æå£°éŸ³çš„ä½ç½®å’Œç§»åŠ¨æ–¹å‘ã€‚è¯¥æ¡†æ¶å°†ç”Ÿæˆè¿‡ç¨‹åˆ†ä¸ºä¸¤ä¸ªé˜¶æ®µï¼šé¦–å…ˆæ˜¯éŸ³é¢‘å¼•å¯¼çš„è§†é¢‘è§„åˆ’(Audio-guided Video Planning)ï¼Œé€šè¿‡æ”¹è¿›çš„å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹(MLLM)æ„å»ºè§†é¢‘åœºæ™¯å¸ƒå±€(Video Scene Layouts, VSLs)ä½œä¸ºè¿æ¥éŸ³é¢‘ä¸è§†é¢‘æ¨¡æ€çš„ä¸­é—´è¡¨å¾ï¼›å…¶æ¬¡æ˜¯åŸºäºå¸ƒå±€çš„è§†é¢‘ç”Ÿæˆ(Layout-grounded Video Generation)ï¼Œä»¥æ— éœ€è®­ç»ƒ(training-free)çš„æ–¹å¼å°†VSLsä½œä¸ºæ¡ä»¶å¼•å¯¼æ•´åˆè¿›é¢„è®­ç»ƒçš„æ‰©æ•£æ¨¡å‹(diffusion models)ã€‚å®éªŒç»“æœè¯æ˜ï¼ŒSpA2Våœ¨ç”Ÿæˆè¯­ä¹‰å’Œç©ºé—´å¯¹é½çš„é€¼çœŸè§†é¢‘æ–¹é¢è¡¨ç°ä¼˜å¼‚ï¼Œæœ‰æ•ˆå¢å¼ºäº†è§†é¢‘å†…å®¹ä¸éŸ³é¢‘è¾“å…¥ä¹‹é—´çš„ç‰©ç†å¯¹åº”å…³ç³»ã€‚",
      "categories": [
        "cs.GR",
        "cs.AI",
        "cs.CV",
        "cs.MM",
        "cs.SD",
        "eess.AS"
      ],
      "primary_category": "cs.GR",
      "comment": "The 33rd ACM Multimedia Conference (MM '25)",
      "pdf_url": "https://arxiv.org/pdf/2508.00782v1",
      "published_date": "2025-08-01 17:05:04 UTC",
      "updated_date": "2025-08-01 17:05:04 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T08:03:56.672652+00:00"
    },
    {
      "arxiv_id": "2508.00973v1",
      "title": "Generative AI as a Geopolitical Factor in Industry 5.0: Sovereignty, Access, and Control",
      "title_zh": "ç”Ÿæˆå¼äººå·¥æ™ºèƒ½ä½œä¸ºå·¥ä¸š 5.0 ä¸­çš„åœ°ç¼˜æ”¿æ²»å› ç´ ï¼šä¸»æƒã€å‡†å…¥ä¸æ§åˆ¶",
      "authors": [
        "Azmine Toushik Wasi",
        "Enjamamul Haque Eram",
        "Sabrina Afroz Mitu",
        "Md Manjurul Ahsan"
      ],
      "abstract": "Industry 5.0 marks a new phase in industrial evolution, emphasizing human-centricity, sustainability, and resilience through the integration of advanced technologies. Within this evolving landscape, Generative AI (GenAI) and autonomous systems are not only transforming industrial processes but also emerging as pivotal geopolitical instruments. We examine strategic implications of GenAI in Industry 5.0, arguing that these technologies have become national assets central to sovereignty, access, and global influence. As countries compete for AI supremacy, growing disparities in talent, computational infrastructure, and data access are reshaping global power hierarchies and accelerating the fragmentation of the digital economy. The human-centric ethos of Industry 5.0, anchored in collaboration between humans and intelligent systems, increasingly conflicts with the autonomy and opacity of GenAI, raising urgent governance challenges related to meaningful human control, dual-use risks, and accountability. We analyze how these dynamics influence defense strategies, industrial competitiveness, and supply chain resilience, including the geopolitical weaponization of export controls and the rise of data sovereignty. Our contribution synthesizes technological, economic, and ethical perspectives to propose a comprehensive framework for navigating the intersection of GenAI and geopolitics. We call for governance models that balance national autonomy with international coordination while safeguarding human-centric values in an increasingly AI-driven world.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†ç”Ÿæˆå¼äººå·¥æ™ºèƒ½ (Generative AI) åœ¨å·¥ä¸š5.0 (Industry 5.0) æ¼”è¿›ä¸­ä½œä¸ºåœ°ç¼˜æ”¿æ²»å› ç´ çš„å…³é”®ä½œç”¨ï¼Œå¼ºè°ƒè¿™äº›æŠ€æœ¯å·²æˆä¸ºå…³ä¹å›½å®¶ä¸»æƒã€èµ„æºè·å–å’Œå…¨çƒå½±å“åŠ›çš„æ ¸å¿ƒæˆ˜ç•¥èµ„äº§ã€‚è®ºæ–‡æŒ‡å‡ºï¼Œéšç€å„å›½äº‰å¤ºAIéœ¸æƒï¼Œäººæ‰ã€è®¡ç®—åŸºç¡€è®¾æ–½ (computational infrastructure) å’Œæ•°æ®è®¿é—®æ–¹é¢æ—¥ç›Šæ‰©å¤§çš„å·®è·æ­£åœ¨é‡å¡‘å…¨çƒæƒåŠ›ç­‰çº§ï¼Œå¹¶å¯¼è‡´æ•°å­—ç»æµçš„ç¢ç‰‡åŒ–ã€‚ä½œè€…åˆ†æäº†å·¥ä¸š5.0çš„äººæœ¬ä¸­å¿ƒä¸»ä¹‰ä¸ GenAI çš„è‡ªä¸»æ€§å’Œä¸é€æ˜æ€§ä¹‹é—´çš„å†²çªï¼Œå¹¶æå‡ºäº†å…³äºæœ‰æ•ˆäººç±»æ§åˆ¶ (meaningful human control)ã€åŒç”¨é€”é£é™© (dual-use risks) å’Œé—®è´£åˆ¶çš„æ²»ç†æŒ‘æˆ˜ã€‚ç ”ç©¶è¿›ä¸€æ­¥é˜è¿°äº†è¿™äº›åŠ¨æ€å¦‚ä½•é€šè¿‡å‡ºå£ç®¡åˆ¶çš„æ­¦å™¨åŒ–å’Œæ•°æ®ä¸»æƒ (data sovereignty) çš„å…´èµ·ï¼Œè¿›è€Œå½±å“å›½é˜²æˆ˜ç•¥ã€å·¥ä¸šç«äº‰åŠ›å’Œä¾›åº”é“¾éŸ§æ€§ã€‚é€šè¿‡æ•´åˆæŠ€æœ¯ã€ç»æµå’Œä¼¦ç†ç»´åº¦ï¼Œè¯¥ç ”ç©¶æå‡ºäº†ä¸€ä¸ªç†è§£ GenAI ä¸åœ°ç¼˜æ”¿æ²»äº¤æ±‡ç‚¹çš„ç»¼åˆæ¡†æ¶ï¼Œå¹¶å‘¼åå»ºç«‹å¹³è¡¡å›½å®¶è‡ªä¸»ä¸å›½é™…åè°ƒçš„æ²»ç†æ¨¡å¼ï¼Œä»¥åœ¨ AI é©±åŠ¨çš„ä¸–ç•Œä¸­ç»´æŠ¤æ ¸å¿ƒä»·å€¼è§‚ã€‚",
      "categories": [
        "cs.CY",
        "cs.AI"
      ],
      "primary_category": "cs.CY",
      "comment": "In Review",
      "pdf_url": "https://arxiv.org/pdf/2508.00973v1",
      "published_date": "2025-08-01 16:43:45 UTC",
      "updated_date": "2025-08-01 16:43:45 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T08:04:01.786945+00:00"
    },
    {
      "arxiv_id": "2508.00766v2",
      "title": "Sample-Aware Test-Time Adaptation for Medical Image-to-Image Translation",
      "title_zh": "é¢å‘åŒ»å­¦å›¾åƒåˆ°å›¾åƒç¿»è¯‘çš„æ ·æœ¬æ„ŸçŸ¥æµ‹è¯•æ—¶è‡ªé€‚åº”",
      "authors": [
        "Irene Iele",
        "Francesco Di Feola",
        "Valerio Guarrasi",
        "Paolo Soda"
      ],
      "abstract": "Image-to-image translation has emerged as a powerful technique in medical imaging, enabling tasks such as image denoising and cross-modality conversion. However, it suffers from limitations in handling out-of-distribution samples without causing performance degradation. To address this limitation, we propose a novel Test-Time Adaptation (TTA) framework that dynamically adjusts the translation process based on the characteristics of each test sample. Our method introduces a Reconstruction Module to quantify the domain shift and a Dynamic Adaptation Block that selectively modifies the internal features of a pretrained translation model to mitigate the shift without compromising the performance on in-distribution samples that do not require adaptation. We evaluate our approach on two medical image-to-image translation tasks: low-dose CT denoising and T1 to T2 MRI translation, showing consistent improvements over both the baseline translation model without TTA and prior TTA methods. Our analysis highlights the limitations of the state-of-the-art that uniformly apply the adaptation to both out-of-distribution and in-distribution samples, demonstrating that dynamic, sample-specific adjustment offers a promising path to improve model resilience in real-world scenarios. The code is available at: https://github.com/Sample-Aware-TTA/Code.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹åŒ»å­¦å›¾åƒåˆ°å›¾åƒç¿»è¯‘(Image-to-image translation)åœ¨å¤„ç†åˆ†å¸ƒå¤–(out-of-distribution)æ ·æœ¬æ—¶é¢ä¸´çš„æ€§èƒ½ä¸‹é™é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§æ–°å‹çš„æ ·æœ¬æ„ŸçŸ¥æµ‹è¯•æ—¶è‡ªé€‚åº”(Test-Time Adaptation, TTA)æ¡†æ¶ã€‚è¯¥æ¡†æ¶å¼•å…¥äº†ä¸€ä¸ªé‡å»ºæ¨¡å—(Reconstruction Module)æ¥é‡åŒ–é¢†åŸŸåç§»(domain shift)ï¼Œå¹¶åˆ©ç”¨åŠ¨æ€è‡ªé€‚åº”å—(Dynamic Adaptation Block)é€‰æ‹©æ€§åœ°ä¿®æ”¹é¢„è®­ç»ƒç¿»è¯‘æ¨¡å‹çš„å†…éƒ¨ç‰¹å¾ã€‚ä¸ä»¥å¾€ç»Ÿä¸€å¯¹æ‰€æœ‰æ ·æœ¬åº”ç”¨è‡ªé€‚åº”çš„æ–¹æ³•ä¸åŒï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿæ ¹æ®æ¯ä¸ªæµ‹è¯•æ ·æœ¬çš„ç‰¹å¾åŠ¨æ€è°ƒæ•´ç¿»è¯‘è¿‡ç¨‹ï¼Œåœ¨æœ‰æ•ˆç¼“è§£åç§»å½±å“çš„åŒæ—¶ï¼Œç¡®ä¿äº†åˆ†å¸ƒå†…æ ·æœ¬çš„æ€§èƒ½ä¸å—æŸå®³ã€‚é€šè¿‡åœ¨ä½å‰‚é‡CTå»å™ªå’ŒT1åˆ°T2 MRIç¿»è¯‘ä»»åŠ¡ä¸Šçš„è¯„ä¼°ï¼Œå®éªŒç»“æœè¯æ˜è¯¥æ–¹æ³•ç›¸è¾ƒäºåŸºçº¿æ¨¡å‹å’Œç°æœ‰çš„TTAæ–¹æ³•å‡æœ‰æŒç»­çš„æ€§èƒ½æå‡ã€‚è¿™é¡¹ç ”ç©¶å¼ºè°ƒäº†æ ·æœ¬ç‰¹å¼‚æ€§çš„åŠ¨æ€è°ƒæ•´åœ¨æé«˜æ¨¡å‹äºçœŸå®ä¸–ç•Œåœºæ™¯ä¸­é²æ£’æ€§çš„é‡è¦æ€§ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.00766v2",
      "published_date": "2025-08-01 16:41:15 UTC",
      "updated_date": "2025-09-16 16:35:24 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T08:04:04.484687+00:00"
    },
    {
      "arxiv_id": "2508.00760v1",
      "title": "MMBERT: Scaled Mixture-of-Experts Multimodal BERT for Robust Chinese Hate Speech Detection under Cloaking Perturbations",
      "title_zh": "MMBERTï¼šé¢å‘ä¼ªè£…æ‰°åŠ¨ä¸‹é²æ£’ä¸­æ–‡ä»‡æ¨è¨€è®ºæ£€æµ‹çš„æ‰©å±•æ··åˆä¸“å®¶å¤šæ¨¡æ€ BERT",
      "authors": [
        "Qiyao Xue",
        "Yuchen Dou",
        "Ryan Shi",
        "Xiang Lorraine Li",
        "Wei Gao"
      ],
      "abstract": "Hate speech detection on Chinese social networks presents distinct challenges, particularly due to the widespread use of cloaking techniques designed to evade conventional text-based detection systems. Although large language models (LLMs) have recently improved hate speech detection capabilities, the majority of existing work has concentrated on English datasets, with limited attention given to multimodal strategies in the Chinese context. In this study, we propose MMBERT, a novel BERT-based multimodal framework that integrates textual, speech, and visual modalities through a Mixture-of-Experts (MoE) architecture. To address the instability associated with directly integrating MoE into BERT-based models, we develop a progressive three-stage training paradigm. MMBERT incorporates modality-specific experts, a shared self-attention mechanism, and a router-based expert allocation strategy to enhance robustness against adversarial perturbations. Empirical results in several Chinese hate speech datasets show that MMBERT significantly surpasses fine-tuned BERT-based encoder models, fine-tuned LLMs, and LLMs utilizing in-context learning approaches.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ä¸­æ–‡ç¤¾äº¤ç½‘ç»œä¸­åˆ©ç”¨ä¼ªè£…æŠ€æœ¯(cloaking techniques)é€ƒé¿æ£€æµ‹çš„æŒ‘æˆ˜ï¼Œæå‡ºäº†MMBERTè¿™ä¸€åŸºäºæ··åˆä¸“å®¶æ¶æ„(Mixture-of-Experts, MoE)çš„å¤šæ¨¡æ€BERTæ¡†æ¶ã€‚è¯¥æ¨¡å‹æœ‰æ•ˆé›†æˆäº†æ–‡æœ¬ã€è¯­éŸ³å’Œè§†è§‰æ¨¡æ€ï¼Œå¹¶é‡‡ç”¨æ¸è¿›å¼ä¸‰é˜¶æ®µè®­ç»ƒèŒƒå¼(progressive three-stage training paradigm)ä»¥è§£å†³MoEé›†æˆè¿‡ç¨‹ä¸­çš„ä¸ç¨³å®šæ€§ã€‚é€šè¿‡ç»“åˆæ¨¡æ€ç‰¹å®šä¸“å®¶ã€å…±äº«è‡ªæ³¨æ„åŠ›æœºåˆ¶(shared self-attention)å’Œè·¯ç”±åˆ†é…ç­–ç•¥ï¼ŒMMBERTæ˜¾è‘—æå‡äº†åº”å¯¹å¯¹æŠ—æ€§æ‰°åŠ¨çš„é²æ£’æ€§ã€‚å®éªŒè¯æ˜ï¼Œè¯¥æ¨¡å‹åœ¨å¤šä¸ªä¸­æ–‡æ•°æ®é›†ä¸Šçš„è¡¨ç°æ˜¾è‘—ä¼˜äºå¾®è°ƒåçš„BERTã€å¤§è¯­è¨€æ¨¡å‹(LLMs)åŠé‡‡ç”¨ä¸Šä¸‹æ–‡å­¦ä¹ (In-Context Learning)æ–¹æ³•çš„æ¨¡å‹ã€‚è¯¥ç ”ç©¶ä¸ºä¸­æ–‡å¤æ‚è¯­å¢ƒä¸‹çš„ç¨³å¥ä»‡æ¨è¨€è®ºæ£€æµ‹æä¾›äº†é«˜æ•ˆçš„å¤šæ¨¡æ€è§£å†³æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.00760v1",
      "published_date": "2025-08-01 16:34:57 UTC",
      "updated_date": "2025-08-01 16:34:57 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T08:04:06.462340+00:00"
    },
    {
      "arxiv_id": "2508.00754v1",
      "title": "A Simple and Effective Method for Uncertainty Quantification and OOD Detection",
      "title_zh": "ä¸€ç§ç®€å•æœ‰æ•ˆçš„ä¸ç¡®å®šæ€§é‡åŒ–ä¸åˆ†å¸ƒå¤–æ£€æµ‹æ–¹æ³•",
      "authors": [
        "Yaxin Ma",
        "Benjamin Colburn",
        "Jose C. Principe"
      ],
      "abstract": "Bayesian neural networks and deep ensemble methods have been proposed for uncertainty quantification; however, they are computationally intensive and require large storage. By utilizing a single deterministic model, we can solve the above issue. We propose an effective method based on feature space density to quantify uncertainty for distributional shifts and out-of-distribution (OOD) detection. Specifically, we leverage the information potential field derived from kernel density estimation to approximate the feature space density of the training set. By comparing this density with the feature space representation of test samples, we can effectively determine whether a distributional shift has occurred. Experiments were conducted on a 2D synthetic dataset (Two Moons and Three Spirals) as well as an OOD detection task (CIFAR-10 vs. SVHN). The results demonstrate that our method outperforms baseline models.",
      "tldr_zh": "é’ˆå¯¹Bayesian neural networkså’Œdeep ensembleæ–¹æ³•è®¡ç®—é‡å¤§ä¸”å­˜å‚¨éœ€æ±‚é«˜çš„é—®é¢˜ï¼Œè¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§åŸºäºå•ä¸ªç¡®å®šæ€§æ¨¡å‹ï¼ˆdeterministic modelï¼‰çš„æœ‰æ•ˆæ–¹æ³•ï¼Œæ—¨åœ¨å®ç°é«˜æ•ˆçš„Uncertainty Quantificationå’ŒOOD Detectionã€‚è¯¥æ–¹æ³•çš„æ ¸å¿ƒåœ¨äºåˆ©ç”¨ç‰¹å¾ç©ºé—´å¯†åº¦ï¼ˆfeature space densityï¼‰æ¥é‡åŒ–åˆ†å¸ƒåç§»ï¼ˆdistributional shiftsï¼‰å¼•èµ·çš„ä¸ç¡®å®šæ€§ã€‚å…·ä½“è€Œè¨€ï¼Œç ”ç©¶è€…é‡‡ç”¨ä»æ ¸å¯†åº¦ä¼°è®¡ï¼ˆkernel density estimationï¼‰æ¨å¯¼å‡ºçš„ä¿¡æ¯åŠ¿åœºï¼ˆinformation potential fieldï¼‰æ¥è¿‘ä¼¼è®­ç»ƒé›†çš„ç‰¹å¾ç©ºé—´åˆ†å¸ƒã€‚é€šè¿‡å¯¹æ¯”æµ‹è¯•æ ·æœ¬ä¸è®­ç»ƒé›†å¯†åº¦çš„ç‰¹å¾è¡¨ç¤ºï¼Œè¯¥ç³»ç»Ÿèƒ½å¤Ÿæœ‰æ•ˆåˆ¤æ–­æ ·æœ¬æ˜¯å¦å‘ç”Ÿäº†åˆ†å¸ƒåç§»ã€‚åœ¨2Dåˆæˆæ•°æ®é›†ä»¥åŠCIFAR-10ä¸SVHNçš„æ£€æµ‹ä»»åŠ¡ä¸­ï¼Œè¯¥æ–¹æ³•è¡¨ç°å‡ºäº†ä¼˜äºåŸºçº¿æ¨¡å‹çš„æ€§èƒ½ã€‚å®éªŒç»“æœè¯æ˜ï¼Œè¯¥æ–¹æ¡ˆåœ¨æ˜¾è‘—é™ä½è®¡ç®—å¤æ‚åº¦çš„åŒæ—¶ï¼Œä¸ºæ·±åº¦å­¦ä¹ æ¨¡å‹æä¾›äº†æ›´å¯é çš„åˆ†å¸ƒå¤–æ£€æµ‹ä¸ä¸ç¡®å®šæ€§è¯„ä¼°èƒ½åŠ›ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.00754v1",
      "published_date": "2025-08-01 16:31:23 UTC",
      "updated_date": "2025-08-01 16:31:23 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T08:04:12.883847+00:00"
    },
    {
      "arxiv_id": "2508.00751v1",
      "title": "Harnessing the Power of Interleaving and Counterfactual Evaluation for Airbnb Search Ranking",
      "title_zh": "åˆ©ç”¨äº¤å‰è¯„ä¼°ä¸åäº‹å®è¯„ä¼°æå‡ Airbnb æœç´¢æ’åæ•ˆèƒ½",
      "authors": [
        "Qing Zhang",
        "Alex Deng",
        "Michelle Du",
        "Huiji Gao",
        "Liwei He",
        "Sanjeev Katariya"
      ],
      "abstract": "Evaluation plays a crucial role in the development of ranking algorithms on search and recommender systems. It enables online platforms to create user-friendly features that drive commercial success in a steady and effective manner. The online environment is particularly conducive to applying causal inference techniques, such as randomized controlled experiments (known as A/B test), which are often more challenging to implement in fields like medicine and public policy. However, businesses face unique challenges when it comes to effective A/B test. Specifically, achieving sufficient statistical power for conversion-based metrics can be time-consuming, especially for significant purchases like booking accommodations. While offline evaluations are quicker and more cost-effective, they often lack accuracy and are inadequate for selecting candidates for A/B test. To address these challenges, we developed interleaving and counterfactual evaluation methods to facilitate rapid online assessments for identifying the most promising candidates for A/B tests. Our approach not only increased the sensitivity of experiments by a factor of up to 100 (depending on the approach and metrics) compared to traditional A/B testing but also streamlined the experimental process. The practical insights gained from usage in production can also benefit organizations with similar interests.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹æœç´¢å’Œæ¨èç³»ç»Ÿè¯„ä¼°æ’åºç®—æ³•æ—¶ï¼Œä¼ ç»Ÿ A/B test è·å–è½¬åŒ–æŒ‡æ ‡ç»Ÿè®¡æ•ˆèƒ½è€—æ—¶è¿‡é•¿çš„é—®é¢˜è¿›è¡Œäº†æ·±å…¥æ¢è®¨ã€‚ä¸ºäº†å®ç°å¿«é€Ÿçš„åœ¨çº¿è¯„ä¼°å¹¶è¯†åˆ«é«˜æ½œåŠ›å€™é€‰æ¨¡å‹ï¼Œç ”ç©¶è€…åœ¨ Airbnb çš„æœç´¢æ’åºåœºæ™¯ä¸­å¼•å…¥å¹¶ä¼˜åŒ–äº†äº¤å‰è¯„ä¼°(Interleaving)ä¸åäº‹å®è¯„ä¼°(Counterfactual Evaluation)æ–¹æ³•ã€‚è¿™äº›æ–¹æ³•æœ‰æ•ˆè§£å†³äº†ç¦»çº¿è¯„ä¼°(Offline Evaluation)å‡†ç¡®åº¦ä¸é«˜ä»¥åŠæ— æ³•æœ‰æ•ˆæŒ‡å¯¼ A/B test é€‰æ‹”çš„é—®é¢˜ã€‚å®é™…åº”ç”¨ç»“æœæ˜¾ç¤ºï¼Œè¯¥æ–¹æ³•ç›¸è¾ƒäºä¼ ç»Ÿçš„ A/B testingï¼Œåœ¨å®éªŒçµæ•åº¦(Sensitivity)ä¸Šå®ç°äº†æœ€é«˜ 100 å€çš„æå‡ã€‚è¯¥ç ”ç©¶ä¸ä»…ä¼˜åŒ–äº†å®éªŒæµç¨‹ï¼Œè¿˜é€šè¿‡ç”Ÿäº§ç¯å¢ƒçš„å®è·µæ€»ç»“ï¼Œä¸ºéœ€è¦å¤„ç†ç±»ä¼¼å¤§è§„æ¨¡æ’åºç®—æ³•è¯„ä¼°çš„ä¼ä¸šæä¾›äº†æå…·ä»·å€¼çš„å‚è€ƒæ–¹æ¡ˆã€‚",
      "categories": [
        "cs.IR",
        "cs.AI"
      ],
      "primary_category": "cs.IR",
      "comment": "10 pages",
      "pdf_url": "https://arxiv.org/pdf/2508.00751v1",
      "published_date": "2025-08-01 16:28:18 UTC",
      "updated_date": "2025-08-01 16:28:18 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T08:04:46.789060+00:00"
    },
    {
      "arxiv_id": "2508.00748v2",
      "title": "Is It Really You? Exploring Biometric Verification Scenarios in Photorealistic Talking-Head Avatar Videos",
      "title_zh": "çœŸæ˜¯ä½ å—ï¼Ÿé«˜ä¿çœŸè¯´è¯äººå¤´åƒè§†é¢‘ä¸­çš„ç”Ÿç‰©ç‰¹å¾éªŒè¯åœºæ™¯æ¢ç©¶",
      "authors": [
        "Laura Pedrouzo-Rodriguez",
        "Pedro Delgado-DeRobles",
        "Luis F. Gomez",
        "Ruben Tolosana",
        "Ruben Vera-Rodriguez",
        "Aythami Morales",
        "Julian Fierrez"
      ],
      "abstract": "Photorealistic talking-head avatars are becoming increasingly common in virtual meetings, gaming, and social platforms. These avatars allow for more immersive communication, but they also introduce serious security risks. One emerging threat is impersonation: an attacker can steal a user's avatar, preserving his appearance and voice, making it nearly impossible to detect its fraudulent usage by sight or sound alone. In this paper, we explore the challenge of biometric verification in such avatar-mediated scenarios. Our main question is whether an individual's facial motion patterns can serve as reliable behavioral biometrics to verify their identity when the avatar's visual appearance is a facsimile of its owner. To answer this question, we introduce a new dataset of realistic avatar videos created using a state-of-the-art one-shot avatar generation model, GAGAvatar, with genuine and impostor avatar videos. We also propose a lightweight, explainable spatio-temporal Graph Convolutional Network architecture with temporal attention pooling, that uses only facial landmarks to model dynamic facial gestures. Experimental results demonstrate that facial motion cues enable meaningful identity verification with AUC values approaching 80%. The proposed benchmark and biometric system are available for the research community in order to bring attention to the urgent need for more advanced behavioral biometric defenses in avatar-based communication systems.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†çœŸå®æ„Ÿè¯´è¯å¤´åƒ(photorealistic talking-head avatars)åœ¨è™šæ‹Ÿé€šä¿¡ä¸­å¼•å‘çš„èº«ä»½å†’ç”¨é£é™©ï¼Œç‰¹åˆ«æ˜¯åœ¨æ”»å‡»è€…èƒ½å®Œç¾å¤åˆ¶ç”¨æˆ·å¤–è¡¨å’Œå£°éŸ³çš„æƒ…å†µä¸‹ï¼Œä¼ ç»Ÿè§†è§‰å’Œå¬è§‰æ£€æµ‹æ‰‹æ®µé¢ä¸´æŒ‘æˆ˜ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œè®ºæ–‡ç ”ç©¶äº†ä¸ªä½“é¢éƒ¨åŠ¨ä½œæ¨¡å¼(facial motion patterns)æ˜¯å¦å¯ä½œä¸ºå¯é çš„è¡Œä¸ºç”Ÿç‰©ç‰¹å¾(behavioral biometrics)è¿›è¡Œèº«ä»½éªŒè¯ã€‚ç ”ç©¶å›¢é˜Ÿä½¿ç”¨é¢†å…ˆçš„å•å›¾ç”Ÿæˆæ¨¡å‹GAGAvataræ„å»ºäº†ä¸€ä¸ªåŒ…å«çœŸå®å’Œå†’å……è§†é¢‘çš„æ–°æ•°æ®é›†ï¼Œå¹¶æå‡ºäº†ä¸€ç§åŸºäºæ—¶ç©ºå›¾å·ç§¯ç½‘ç»œ(spatio-temporal Graph Convolutional Network)ä¸”åŒ…å«æ—¶é—´æ³¨æ„åŠ›æ± åŒ–(temporal attention pooling)çš„è½»é‡çº§å¯è§£é‡Šæ¶æ„ï¼Œä»…é€šè¿‡é¢éƒ¨å…³é”®ç‚¹æ¥å»ºæ¨¡åŠ¨æ€é¢éƒ¨å§¿æ€ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œé¢éƒ¨åŠ¨ä½œçº¿ç´¢èƒ½å¤Ÿå®ç°æœ‰æ•ˆçš„èº«ä»½éªŒè¯ï¼Œå…¶AUCå€¼æ¥è¿‘80%ã€‚è¯¥å·¥ä½œå¼ºè°ƒäº†åœ¨åŸºäºå¤´åƒçš„é€šä¿¡ç³»ç»Ÿä¸­å¼•å…¥å…ˆè¿›è¡Œä¸ºç”Ÿç‰©è¯†åˆ«é˜²å¾¡æŠ€æœ¯çš„ç´§è¿«æ€§ï¼Œå¹¶ä¸ºç ”ç©¶ç¤¾åŒºæä¾›äº†ç›¸åº”çš„åŸºå‡†æ•°æ®é›†å’Œç³»ç»Ÿã€‚",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CR",
        "cs.MM"
      ],
      "primary_category": "cs.CV",
      "comment": "Accepted at the IEEE International Joint Conference on Biometrics (IJCB 2025)",
      "pdf_url": "https://arxiv.org/pdf/2508.00748v2",
      "published_date": "2025-08-01 16:23:27 UTC",
      "updated_date": "2025-08-04 12:27:33 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T08:05:04.449214+00:00"
    },
    {
      "arxiv_id": "2508.00743v4",
      "title": "Multi-step retrieval and reasoning improves radiology question answering with large language models",
      "title_zh": "å¤šæ­¥æ£€ç´¢ä¸æ¨ç†æå‡å¤§è¯­è¨€æ¨¡å‹æ”¾å°„å­¦é—®ç­”æ€§èƒ½",
      "authors": [
        "Sebastian Wind",
        "Jeta Sopa",
        "Daniel Truhn",
        "Mahshad Lotfinia",
        "Tri-Thien Nguyen",
        "Keno Bressem",
        "Lisa Adams",
        "Mirabela Rusu",
        "Harald KÃ¶stler",
        "Gerhard Wellein",
        "Andreas Maier",
        "Soroosh Tayebi Arasteh"
      ],
      "abstract": "Clinical decision-making in radiology increasingly benefits from artificial intelligence (AI), particularly through large language models (LLMs). However, traditional retrieval-augmented generation (RAG) systems for radiology question answering (QA) typically rely on single-step retrieval, limiting their ability to handle complex clinical reasoning tasks. Here we propose radiology Retrieval and Reasoning (RaR), a multi-step retrieval and reasoning framework designed to improve diagnostic accuracy, factual consistency, and clinical reliability of LLMs in radiology question answering. We evaluated 25 LLMs spanning diverse architectures, parameter scales (0.5B to >670B), and training paradigms (general-purpose, reasoning-optimized, clinically fine-tuned), using 104 expert-curated radiology questions from previously established RSNA-RadioQA and ExtendedQA datasets. To assess generalizability, we additionally tested on an unseen internal dataset of 65 real-world radiology board examination questions. RaR significantly improved mean diagnostic accuracy over zero-shot prompting and conventional online RAG. The greatest gains occurred in small-scale models, while very large models (>200B parameters) demonstrated minimal changes (<2% improvement). Additionally, RaR retrieval reduced hallucinations (mean 9.4%) and retrieved clinically relevant context in 46% of cases, substantially aiding factual grounding. Even clinically fine-tuned models showed gains from RaR (e.g., MedGemma-27B), indicating that retrieval remains beneficial despite embedded domain knowledge. These results highlight the potential of RaR to enhance factuality and diagnostic accuracy in radiology QA, warranting future studies to validate their clinical utility. All datasets, code, and the full RaR framework are publicly available to support open research and clinical translation.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†RaR (Radiology Retrieval and Reasoning)ï¼Œä¸€ç§é’ˆå¯¹æ”¾å°„å­¦é—®ç­” (Radiology QA) è®¾è®¡çš„å¤šæ­¥æ£€ç´¢ä¸æ¨ç†æ¡†æ¶ï¼Œæ—¨åœ¨æå‡å¤§å‹è¯­è¨€æ¨¡å‹ (LLMs) åœ¨ä¸´åºŠå†³ç­–ä¸­çš„å‡†ç¡®æ€§ã€äº‹å®ä¸€è‡´æ€§å’Œå¯é æ€§ã€‚ä¸ä¼ ç»Ÿçš„å•æ­¥æ£€ç´¢å¢å¼ºç”Ÿæˆ (RAG) ç³»ç»Ÿç›¸æ¯”ï¼ŒRaR é€šè¿‡å¤šæ­¥å¤„ç†æ˜¾è‘—å¢å¼ºäº†æ¨¡å‹åº”å¯¹å¤æ‚ä¸´åºŠæ¨ç†ä»»åŠ¡çš„èƒ½åŠ›ã€‚ç ”ç©¶è€…åœ¨åŒ…æ‹¬ RSNA-RadioQAã€ExtendedQA ä»¥åŠçœŸå®æ”¾å°„ç§‘æ‰§ä¸šåŒ»å¸ˆè€ƒè¯•åœ¨å†…çš„å¤šä¸ªæ•°æ®é›†ä¸Šï¼Œå¯¹æ¶µç›–ä¸åŒè§„æ¨¡ (0.5B è‡³ >670B) å’Œè®­ç»ƒèŒƒå¼çš„ 25 ç§ LLMs è¿›è¡Œäº†å…¨é¢è¯„ä¼°ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒRaR åœ¨å¹³å‡è¯Šæ–­å‡†ç¡®ç‡ä¸Šæ˜¾è‘—ä¼˜äºé›¶æ ·æœ¬æç¤º (Zero-shot) å’Œä¼ ç»Ÿåœ¨çº¿ RAGï¼Œå…¶ä¸­å°è§„æ¨¡æ¨¡å‹è·å¾—çš„æå‡æœ€ä¸ºæ˜¾è‘—ã€‚æ­¤å¤–ï¼ŒRaR æœ‰æ•ˆå‡å°‘äº†å¹³å‡ 9.4% çš„å¹»è§‰ç°è±¡ï¼Œå¹¶åœ¨ 46% çš„æ¡ˆä¾‹ä¸­æä¾›äº†ä¸´åºŠç›¸å…³çš„ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼Œæ˜¾è‘—å¢å¼ºäº†äº‹å®åŸºç¡€ã€‚å³ä½¿æ˜¯åƒ MedGemma-27B è¿™æ ·ç»è¿‡ä¸´åºŠå¾®è°ƒçš„æ¨¡å‹ï¼Œé€šè¿‡ RaR æ¡†æ¶ä»èƒ½è·å¾—æ€§èƒ½æå‡ï¼Œè¯æ˜äº†æ£€ç´¢æŠ€æœ¯ä¸é¢†åŸŸçŸ¥è¯†äº’è¡¥çš„ä»·å€¼ã€‚è¯¥ç ”ç©¶è¯æ˜äº† RaR åœ¨å¢å¼ºæ”¾å°„å­¦é—®ç­”äº‹å®æ€§å’Œè¯Šæ–­å‡†ç¡®ç‡æ–¹é¢çš„å·¨å¤§æ½œåŠ›ï¼Œä¸ºè¯¥æŠ€æœ¯çš„ä¸´åºŠè½¬åŒ–å¥ å®šäº†åŸºç¡€ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "Published in npj Digital Medicine",
      "pdf_url": "https://arxiv.org/pdf/2508.00743v4",
      "published_date": "2025-08-01 16:18:52 UTC",
      "updated_date": "2025-12-30 20:39:13 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T08:05:10.598955+00:00"
    },
    {
      "arxiv_id": "2508.00741v1",
      "title": "Out-of-Context Abduction: LLMs Make Inferences About Procedural Data Leveraging Declarative Facts in Earlier Training Data",
      "title_zh": "è„±ç¦»è¯­å¢ƒçš„æº¯å› æ¨ç†ï¼šå¤§è¯­è¨€æ¨¡å‹åˆ©ç”¨æ—©æœŸè®­ç»ƒæ•°æ®ä¸­çš„é™ˆè¿°æ€§äº‹å®å¯¹ç¨‹åºæ€§æ•°æ®è¿›è¡Œæ¨æ–­",
      "authors": [
        "Sohaib Imran",
        "Rob Lamb",
        "Peter M. Atkinson"
      ],
      "abstract": "Large language models (LLMs) are trained on large corpora, yet it is unclear whether they can reason about the information present within their training data. We design experiments to study out-of-context abduction in LLMs, the ability to infer the most plausible explanations for observations using relevant facts present in training data. We train treatment LLMs on names and behavior descriptions of fictitious chatbots, but not on examples of dialogue with the chatbots. We find that OpenAI's GPT 4o LLM can correctly infer at least one chatbot's name after observing example responses characteristic of that chatbot. We also find that previously training GPT 4o on descriptions of a chatbot's behavior allows it to display behaviors more characteristic of the chatbot when iteratively trained to display such behaviors. Our results have implications for situational awareness in LLMs and, therefore, for AI safety.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ä¸­çš„è„±ç¦»ä¸Šä¸‹æ–‡æº¯å› æ¨ç†ï¼ˆOut-of-Context Abductionï¼‰èƒ½åŠ›ï¼Œå³æ¨¡å‹åˆ©ç”¨è®­ç»ƒæ•°æ®ä¸­çš„é™ˆè¿°æ€§äº‹å®ï¼ˆDeclarative Factsï¼‰æ¥æ¨æ–­è§‚å¯Ÿç»“æœèƒŒåæœ€åˆç†è§£é‡Šçš„èƒ½åŠ›ã€‚ç ”ç©¶è€…é€šè¿‡åœ¨è™šæ„èŠå¤©æœºå™¨äººçš„åç§°å’Œè¡Œä¸ºæè¿°ä¸Šè®­ç»ƒæ¨¡å‹ï¼ˆä¸åŒ…å«å¯¹è¯ç¤ºä¾‹ï¼‰è¿›è¡Œäº†å®éªŒï¼Œå‘ç° GPT-4o æ¨¡å‹èƒ½å¤Ÿä»…å‡­ç‰¹å¾æ€§å“åº”æ¨æ–­å‡ºå¯¹åº”æœºå™¨äººçš„åç§°ã€‚å®éªŒè¿›ä¸€æ­¥è¯æ˜ï¼Œé¢„å…ˆå­¦ä¹ è¡Œä¸ºæè¿°èƒ½æ˜¾è‘—æå‡æ¨¡å‹åœ¨åç»­è¿­ä»£è®­ç»ƒä¸­å±•ç°ç‰¹å®šè¡Œä¸ºçš„èƒ½åŠ›ã€‚è¿™äº›å‘ç°è¡¨æ˜ LLMs èƒ½å¤Ÿå°†è®­ç»ƒé˜¶æ®µå¸æ”¶çš„äº‹å®çŸ¥è¯†åº”ç”¨äºæ¨ç†æœªçŸ¥åœºæ™¯ï¼Œæ­ç¤ºäº†æ¨¡å‹åœ¨æƒ…å¢ƒæ„ŸçŸ¥ï¼ˆSituational Awarenessï¼‰æ–¹é¢çš„æ½œåŠ›ï¼Œå¹¶å¯¹äººå·¥æ™ºèƒ½å®‰å…¨ï¼ˆAI Safetyï¼‰ç ”ç©¶å…·æœ‰æ·±è¿œå½±å“ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.00741v1",
      "published_date": "2025-08-01 16:12:23 UTC",
      "updated_date": "2025-08-01 16:12:23 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T08:05:08.391700+00:00"
    },
    {
      "arxiv_id": "2508.00737v2",
      "title": "How LLMs are Shaping the Future of Virtual Reality",
      "title_zh": "å¤§è¯­è¨€æ¨¡å‹å¦‚ä½•é‡å¡‘è™šæ‹Ÿç°å®çš„æœªæ¥",
      "authors": [
        "SÃ¼eda Ã–zkaya",
        "Santiago Berrezueta-Guzman",
        "Stefan Wagner"
      ],
      "abstract": "The integration of Large Language Models (LLMs) into Virtual Reality (VR) games marks a paradigm shift in the design of immersive, adaptive, and intelligent digital experiences. This paper presents a comprehensive review of recent research at the intersection of LLMs and VR, examining how these models are transforming narrative generation, non-player character (NPC) interactions, accessibility, personalization, and game mastering. Drawing from an analysis of 62 peer reviewed studies published between 2018 and 2025, we identify key application domains ranging from emotionally intelligent NPCs and procedurally generated storytelling to AI-driven adaptive systems and inclusive gameplay interfaces. We also address the major challenges facing this convergence, including real-time performance constraints, memory limitations, ethical risks, and scalability barriers. Our findings highlight that while LLMs significantly enhance realism, creativity, and user engagement in VR environments, their effective deployment requires robust design strategies that integrate multimodal interaction, hybrid AI architectures, and ethical safeguards. The paper concludes by outlining future research directions in multimodal AI, affective computing, reinforcement learning, and open-source development, aiming to guide the responsible advancement of intelligent and inclusive VR systems.",
      "tldr_zh": "è¯¥ç ”ç©¶æ·±å…¥æ¢è®¨äº†å¤§è¯­è¨€æ¨¡å‹(Large Language Models, LLMs)åœ¨è™šæ‹Ÿç°å®(Virtual Reality, VR)æ¸¸æˆä¸­çš„æ•´åˆï¼Œè®¤ä¸ºè¿™ä¸€ç»“åˆæ ‡å¿—ç€æ²‰æµ¸å¼å’Œæ™ºèƒ½æ•°å­—ä½“éªŒè®¾è®¡çš„èŒƒå¼è½¬å˜ã€‚é€šè¿‡å¯¹2018å¹´è‡³2025å¹´é—´62ç¯‡åŒè¡Œè¯„å®¡ç ”ç©¶çš„ç»¼è¿°ï¼Œè®ºæ–‡ç³»ç»Ÿåœ°åˆ†æäº†LLMså¦‚ä½•æ”¹å˜å™äº‹ç”Ÿæˆã€éç©å®¶è§’è‰²(NPC)äº¤äº’ã€æ— éšœç¢æ€§åŠæ¸¸æˆç®¡ç†ã€‚ç ”ç©¶é‡ç‚¹è¯†åˆ«äº†æƒ…æ„Ÿæ™ºèƒ½NPCã€ç¨‹åºåŒ–å™äº‹ç”Ÿæˆä»¥åŠAIé©±åŠ¨è‡ªé€‚åº”ç³»ç»Ÿç­‰å…³é”®åº”ç”¨é¢†åŸŸï¼Œå¹¶æ¢è®¨äº†å®æ—¶æ€§èƒ½ã€å†…å­˜é™åˆ¶å’Œä¼¦ç†é£é™©ç­‰ä¸»è¦æŒ‘æˆ˜ã€‚ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œè™½ç„¶LLMsæ˜¾è‘—æå‡äº†VRç¯å¢ƒçš„çœŸå®æ„Ÿä¸ç”¨æˆ·å‚ä¸åº¦ï¼Œä½†å…¶æœ‰æ•ˆéƒ¨ç½²ä»éœ€ä¾èµ–å¤šæ¨¡æ€äº¤äº’(Multimodal Interaction)ã€æ··åˆAIæ¶æ„å’Œä¼¦ç†ä¿éšœã€‚æ–‡ç« æœ€åæŒ‡å‡ºäº†å¤šæ¨¡æ€AIã€æƒ…æ„Ÿè®¡ç®—(Affective Computing)å’Œå¼ºåŒ–å­¦ä¹ (Reinforcement Learning)ç­‰æœªæ¥ç ”ç©¶æ–¹å‘ï¼Œæ—¨åœ¨å¼•å¯¼æ™ºèƒ½ä¸”åŒ…å®¹çš„VRç³»ç»Ÿè´Ÿè´£ä»»åœ°å‘å±•ã€‚",
      "categories": [
        "cs.HC",
        "cs.AI"
      ],
      "primary_category": "cs.HC",
      "comment": "Pre-print",
      "pdf_url": "https://arxiv.org/pdf/2508.00737v2",
      "published_date": "2025-08-01 16:08:05 UTC",
      "updated_date": "2025-08-04 09:29:35 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T08:05:17.552984+00:00"
    },
    {
      "arxiv_id": "2508.00734v1",
      "title": "Adaptive Machine Learning-Driven Multi-Fidelity Stratified Sampling for Failure Analysis of Nonlinear Stochastic Systems",
      "title_zh": "é¢å‘éçº¿æ€§éšæœºç³»ç»Ÿå¤±æ•ˆåˆ†æçš„è‡ªé€‚åº”æœºå™¨å­¦ä¹ é©±åŠ¨å¤šä¿çœŸåº¦åˆ†å±‚æŠ½æ ·",
      "authors": [
        "Liuyun Xu",
        "Seymour M. J. Spence"
      ],
      "abstract": "Existing variance reduction techniques used in stochastic simulations for rare event analysis still require a substantial number of model evaluations to estimate small failure probabilities. In the context of complex, nonlinear finite element modeling environments, this can become computationally challenging-particularly for systems subjected to stochastic excitation. To address this challenge, a multi-fidelity stratified sampling scheme with adaptive machine learning metamodels is introduced for efficiently propagating uncertainties and estimating small failure probabilities. In this approach, a high-fidelity dataset generated through stratified sampling is used to train a deep learning-based metamodel, which then serves as a cost-effective and highly correlated low-fidelity model. An adaptive training scheme is proposed to balance the trade-off between approximation quality and computational demand associated with the development of the low-fidelity model. By integrating the low-fidelity outputs with additional high-fidelity results, an unbiased estimate of the strata-wise failure probabilities is obtained using a multi-fidelity Monte Carlo framework. The overall probability of failure is then computed using the total probability theorem. Application to a full-scale high-rise steel building subjected to stochastic wind excitation demonstrates that the proposed scheme can accurately estimate exceedance probability curves for nonlinear responses of interest, while achieving significant computational savings compared to single-fidelity variance reduction approaches.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹éçº¿æ€§éšæœºç³»ç»Ÿå¤±æ•ˆåˆ†æä¸­ç½•è§äº‹ä»¶æ¨¡æ‹Ÿè®¡ç®—é‡å¤§çš„é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§ç»“åˆè‡ªé€‚åº”æœºå™¨å­¦ä¹ (Machine Learning)å…ƒæ¨¡å‹çš„è¡¨å±‚æŠ½æ ·(Stratified Sampling)å¤šä¿çœŸåº¦(Multi-Fidelity)æ–¹æ¡ˆã€‚è¯¥æ–¹æ³•åˆ©ç”¨é€šè¿‡è¡¨å±‚æŠ½æ ·ç”Ÿæˆçš„é«˜ä¿çœŸåº¦(High-Fidelity)æ•°æ®é›†æ¥è®­ç»ƒåŸºäºæ·±åº¦å­¦ä¹ (Deep Learning)çš„å…ƒæ¨¡å‹ï¼Œä»è€Œæ„å»ºä¸€ä¸ªä½æˆæœ¬ä¸”é«˜åº¦ç›¸å…³çš„ä½ä¿çœŸåº¦(Low-Fidelity)æ¨¡å‹ã€‚ç ”ç©¶æå‡ºäº†ä¸€ç§è‡ªé€‚åº”è®­ç»ƒæ–¹æ¡ˆï¼Œæ—¨åœ¨å¹³è¡¡ä½ä¿çœŸåº¦æ¨¡å‹å¼€å‘è¿‡ç¨‹ä¸­çš„è¿‘ä¼¼è´¨é‡ä¸è®¡ç®—éœ€æ±‚ã€‚é€šè¿‡åœ¨å¤šä¿çœŸåº¦è’™ç‰¹å¡æ´›(Multi-Fidelity Monte Carlo)æ¡†æ¶ä¸‹æ•´åˆä½ä¿çœŸåº¦è¾“å‡ºä¸é¢å¤–çš„é«˜ä¿çœŸåº¦ç»“æœï¼Œè·å¾—äº†å„å±‚å¤±æ•ˆæ¦‚ç‡çš„æ— åä¼°è®¡ã€‚æœ€ç»ˆåˆ©ç”¨å…¨æ¦‚ç‡å®šç†(Total Probability Theorem)è®¡ç®—ç³»ç»Ÿçš„æ•´ä½“å¤±æ•ˆæ¦‚ç‡ã€‚åœ¨å—éšæœºé£æ¿€åŠ±çš„é«˜å±‚é’¢ç»“æ„å»ºç­‘åº”ç”¨ä¸­ï¼Œè¯¥æ–¹æ¡ˆèƒ½å¤Ÿå‡†ç¡®ä¼°è®¡éçº¿æ€§å“åº”çš„è¶…è¶Šæ¦‚ç‡æ›²çº¿ï¼Œä¸”ä¸å•ä¿çœŸåº¦æ–¹å·®ç¼©å‡(Variance Reduction)æ–¹æ³•ç›¸æ¯”å®ç°äº†æ˜¾è‘—çš„è®¡ç®—æˆæœ¬èŠ‚çº¦ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.00734v1",
      "published_date": "2025-08-01 16:04:21 UTC",
      "updated_date": "2025-08-01 16:04:21 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T08:05:12.462012+00:00"
    },
    {
      "arxiv_id": "2508.00970v2",
      "title": "AI-Educational Development Loop (AI-EDL): A Conceptual Framework to Bridge AI Capabilities with Classical Educational Theories",
      "title_zh": "AI-æ•™è‚²å‘å±•å¾ªç¯ï¼ˆAI-EDLï¼‰ï¼šæ¡¥æ¥äººå·¥æ™ºèƒ½èƒ½åŠ›ä¸ç»å…¸æ•™è‚²ç†è®ºçš„æ¦‚å¿µæ¡†æ¶",
      "authors": [
        "Ning Yu",
        "Jie Zhang",
        "Sandeep Mitra",
        "Rebecca Smith",
        "Adam Rich"
      ],
      "abstract": "This study introduces the AI-Educational Development Loop (AI-EDL), a theory-driven framework that integrates classical learning theories with human-in-the-loop artificial intelligence (AI) to support reflective, iterative learning. Implemented in EduAlly, an AI-assisted platform for writing-intensive and feedback-sensitive tasks, the framework emphasizes transparency, self-regulated learning, and pedagogical oversight. A mixed-methods study was piloted at a comprehensive public university to evaluate alignment between AI-generated feedback, instructor evaluations, and student self-assessments; the impact of iterative revision on performance; and student perceptions of AI feedback. Quantitative results demonstrated statistically significant improvement between first and second attempts, with agreement between student self-evaluations and final instructor grades. Qualitative findings indicated students valued immediacy, specificity, and opportunities for growth that AI feedback provided. These findings validate the potential to enhance student learning outcomes through developmentally grounded, ethically aligned, and scalable AI feedback systems. The study concludes with implications for future interdisciplinary applications and refinement of AI-supported educational technologies.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† AI-Educational Development Loop (AI-EDL) æ¦‚å¿µæ¡†æ¶ï¼Œæ—¨åœ¨å°†ç»å…¸æ•™è‚²ç†è®ºä¸äººæœºååŒ (human-in-the-loop) çš„äººå·¥æ™ºèƒ½æŠ€æœ¯ç›¸ç»“åˆï¼Œä»¥æ”¯æŒåå°„å¼å’Œè¿­ä»£å¼å­¦ä¹ ã€‚è¯¥æ¡†æ¶å·²åœ¨åä¸º EduAlly çš„ AI è¾…åŠ©å¹³å°ä¸Šå®ç°ï¼Œä¸»è¦é’ˆå¯¹å†™ä½œå¯†é›†å‹å’Œåé¦ˆæ•æ„Ÿå‹ä»»åŠ¡ï¼Œå¼ºè°ƒé€æ˜åº¦ã€è‡ªæˆ‘è°ƒèŠ‚å­¦ä¹  (self-regulated learning) ä»¥åŠæ•™å­¦ç›‘ç£ã€‚é€šè¿‡åœ¨ä¸€æ‰€ç»¼åˆæ€§å…¬ç«‹å¤§å­¦å¼€å±•çš„æ··åˆæ–¹æ³•ç ”ç©¶ï¼Œå®šé‡ç»“æœæ˜¾ç¤ºå­¦ç”Ÿåœ¨ä¸åŒå°è¯•é˜¶æ®µçš„è¡¨ç°æœ‰æ˜¾è‘—çš„ç»Ÿè®¡å­¦æå‡ï¼Œä¸”å­¦ç”Ÿè‡ªè¯„ä¸æ•™å¸ˆè¯„åˆ†è¾¾æˆäº†ä¸€è‡´ã€‚å®šæ€§å‘ç°è¡¨æ˜ï¼Œå­¦ç”Ÿé«˜åº¦è®¤å¯ AI åé¦ˆæä¾›çš„å³æ—¶æ€§ã€å…·ä½“æ€§ä»¥åŠä¿ƒè¿›æˆé•¿çš„æœºä¼šã€‚è¯¥ç ”ç©¶éªŒè¯äº†åŸºäºå‘å±•ç†è®ºã€ç¬¦åˆä¼¦ç†ä¸”å¯æ‰©å±•çš„ AI åé¦ˆç³»ç»Ÿåœ¨æå‡å­¦ä¹ æˆæœæ–¹é¢çš„å·¨å¤§æ½œåŠ›ï¼Œå¹¶ä¸ºæœªæ¥è·¨å­¦ç§‘åº”ç”¨å’Œ AI æ”¯æŒçš„æ•™è‚²æŠ€æœ¯ä¼˜åŒ–æä¾›äº†æŒ‡å¯¼æ–¹å‘ã€‚",
      "categories": [
        "cs.CY",
        "cs.AI"
      ],
      "primary_category": "cs.CY",
      "comment": "This work has been submitted to Journal of Educational Technology Systems. It is under review",
      "pdf_url": "https://arxiv.org/pdf/2508.00970v2",
      "published_date": "2025-08-01 15:44:19 UTC",
      "updated_date": "2026-01-08 20:45:48 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T08:05:21.940822+00:00"
    },
    {
      "arxiv_id": "2508.00719v4",
      "title": "DAMR: Efficient and Adaptive Context-Aware Knowledge Graph Question Answering with LLM-Guided MCTS",
      "title_zh": "DAMRï¼šåŸºäº LLM æŒ‡å¯¼ MCTS çš„é«˜æ•ˆè‡ªé€‚åº”ä¸Šä¸‹æ–‡æ„ŸçŸ¥çŸ¥è¯†å›¾è°±é—®ç­”",
      "authors": [
        "Yingxu Wang",
        "Shiqi Fan",
        "Mengzhu Wang",
        "Siyang Gao",
        "Chao Wang",
        "Nan Yin"
      ],
      "abstract": "Knowledge Graph Question Answering (KGQA) aims to interpret natural language queries and perform structured reasoning over knowledge graphs by leveraging their relational and semantic structures to retrieve accurate answers. Existing methods primarily follow either the retrieve-then-reason paradigm, which relies on Graph Neural Networks or heuristic rules to extract static candidate paths, or dynamic path generation strategies that employ LLMs with prompting to jointly perform retrieval and reasoning. However, the former lacks adaptability due to static path extraction and the absence of contextual refinement, while the latter suffers from high computational costs and limited evaluation accuracy because of their dependence on fixed scoring functions and repeated LLM calls. To address these issues, this paper proposes Dynamically Adaptive MCTS-based Reasoning (DAMR), a novel framework that integrates LLM-guided Monte Carlo Tree Search (MCTS) with adaptive path evaluation to enable efficient and context-aware KGQA. DAMR leverages MCTS as a backbone, where an LLM-based planner selects the top-$k$ semantically relevant relations at each expansion step to effectively reduce the search space. To enhance evaluation accuracy, we introduce a lightweight Transformer-based scorer that performs context-aware plausibility estimation by jointly encoding the question and relation sequence through cross-attention, thereby capturing fine-grained semantic shifts during multi-hop reasoning. Furthermore, to mitigate the scarcity of high-quality supervision, DAMR incorporates a dynamic pseudo-path refinement mechanism that periodically generates training signals from partial paths explored during search, enabling the scorer to continually adapt to the evolving distribution of reasoning trajectories. Extensive experiments on multiple KGQA benchmarks show that DAMR significantly outperforms SOTA methods.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† DAMRï¼Œä¸€ç§æ—¨åœ¨æå‡çŸ¥è¯†å›¾è°±é—®ç­” (KGQA) æ•ˆç‡ä¸ä¸Šä¸‹æ–‡æ„ŸçŸ¥èƒ½åŠ›çš„åŠ¨æ€è‡ªé€‚åº”æ¡†æ¶ã€‚é’ˆå¯¹ç°æœ‰æ–¹æ³•ä¸­è·¯å¾„æå–é™æ€åŒ–æˆ– LLM è°ƒç”¨æˆæœ¬é«˜æ˜‚çš„é—®é¢˜ï¼ŒDAMR å°† LLM å¼•å¯¼çš„è’™ç‰¹å¡æ´›æ ‘æœç´¢ (MCTS) ä¸è‡ªé€‚åº”è·¯å¾„è¯„ä¼°ç›¸ç»“åˆã€‚è¯¥æ¡†æ¶åˆ©ç”¨åŸºäº LLM çš„è§„åˆ’å™¨åœ¨æ¯ä¸€æ­¥æ‰©å±•ä¸­ç­›é€‰è¯­ä¹‰ç›¸å…³çš„å…³ç³»ï¼Œä»è€Œæœ‰æ•ˆç¼©å°æœç´¢ç©ºé—´å¹¶é™ä½è®¡ç®—å¼€é”€ã€‚ä¸ºäº†å¢å¼ºè¯„ä¼°å‡†ç¡®æ€§ï¼Œç ”ç©¶å¼•å…¥äº†è½»é‡åŒ– Transformer è¯„åˆ†å™¨ï¼Œé€šè¿‡äº¤å‰æ³¨æ„åŠ›æœºåˆ¶å…±åŒç¼–ç é—®é¢˜ä¸å…³ç³»åºåˆ—ï¼Œç²¾å‡†æ•æ‰å¤šè·³æ¨ç†ä¸­çš„è¯­ä¹‰åç§»ã€‚æ­¤å¤–ï¼ŒDAMR åŒ…å«ä¸€ç§åŠ¨æ€ä¼ªè·¯å¾„ä¼˜åŒ–æœºåˆ¶ï¼Œèƒ½å¤Ÿä»æœç´¢è¿‡ç¨‹ä¸­ç”Ÿæˆçš„å±€éƒ¨è·¯å¾„ä¸­å®šæœŸæå–è®­ç»ƒä¿¡å·ï¼Œæœ‰æ•ˆç¼“è§£äº†é«˜è´¨é‡ç›‘ç£æ•°æ®çš„åŒ®ä¹ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒDAMR åœ¨å¤šä¸ª KGQA åŸºå‡†æµ‹è¯•ä¸­æ˜¾è‘—è¶…è¶Šäº†ç°æœ‰çš„ SOTA æ–¹æ³•ï¼Œå®ç°äº†æ›´é«˜æ•ˆä¸”å…·ä¸Šä¸‹æ–‡æ„ŸçŸ¥èƒ½åŠ›çš„æ¨ç†ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.00719v4",
      "published_date": "2025-08-01 15:38:21 UTC",
      "updated_date": "2025-09-25 20:25:22 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T08:05:25.334030+00:00"
    },
    {
      "arxiv_id": "2508.00716v3",
      "title": "Nested Graph Pseudo-Label Refinement for Noisy Label Domain Adaptation Learning",
      "title_zh": "é¢å‘å™ªå£°æ ‡ç­¾é¢†åŸŸè‡ªé€‚åº”å­¦ä¹ çš„åµŒå¥—å›¾ä¼ªæ ‡ç­¾æçº¯",
      "authors": [
        "Yingxu Wang",
        "Mengzhu Wang",
        "Zhichao Huang",
        "Suyu Liu",
        "Nan Yin"
      ],
      "abstract": "Graph Domain Adaptation (GDA) facilitates knowledge transfer from labeled source graphs to unlabeled target graphs by learning domain-invariant representations, which is essential in applications such as molecular property prediction and social network analysis. However, most existing GDA methods rely on the assumption of clean source labels, which rarely holds in real-world scenarios where annotation noise is pervasive. This label noise severely impairs feature alignment and degrades adaptation performance under domain shifts. To address this challenge, we propose Nested Graph Pseudo-Label Refinement (NeGPR), a novel framework tailored for graph-level domain adaptation with noisy labels. NeGPR first pretrains dual branches, i.e., semantic and topology branches, by enforcing neighborhood consistency in the feature space, thereby reducing the influence of noisy supervision. To bridge domain gaps, NeGPR employs a nested refinement mechanism in which one branch selects high-confidence target samples to guide the adaptation of the other, enabling progressive cross-domain learning. Furthermore, since pseudo-labels may still contain noise and the pre-trained branches are already overfitted to the noisy labels in the source domain, NeGPR incorporates a noise-aware regularization strategy. This regularization is theoretically proven to mitigate the adverse effects of pseudo-label noise, even under the presence of source overfitting, thus enhancing the robustness of the adaptation process. Extensive experiments on benchmark datasets demonstrate that NeGPR consistently outperforms state-of-the-art methods under severe label noise, achieving gains of up to 12.7% in accuracy.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å›¾é¢†åŸŸè‡ªé€‚åº” (Graph Domain Adaptation) ä¸­æ™®éå­˜åœ¨çš„æºåŸŸæ ‡ç­¾å™ªå£°é—®é¢˜ï¼Œæå‡ºäº†åä¸º Nested Graph Pseudo-Label Refinement (NeGPR) çš„æ–°å‹æ¡†æ¶ã€‚NeGPR é¦–å…ˆé€šè¿‡å¼ºåˆ¶ç‰¹å¾ç©ºé—´çš„é‚»åŸŸä¸€è‡´æ€§é¢„è®­ç»ƒè¯­ä¹‰å’Œæ‹“æ‰‘åŒåˆ†æ”¯ï¼Œä»¥æ­¤é™ä½å™ªå£°ç›‘ç£çš„è´Ÿé¢å½±å“ã€‚ä¸ºäº†æœ‰æ•ˆå¼¥åˆé¢†åŸŸå·®è·ï¼Œè¯¥æ¡†æ¶é‡‡ç”¨åµŒå¥—ç²¾ç‚¼æœºåˆ¶ï¼Œä½¿ä¸¤ä¸ªåˆ†æ”¯èƒ½å¤Ÿé€šè¿‡é€‰æ‹©é«˜ç½®ä¿¡åº¦çš„ç›®æ ‡æ ·æœ¬æ¥ç›¸äº’å¼•å¯¼è·¨é¢†åŸŸå­¦ä¹ ã€‚æ­¤å¤–ï¼ŒNeGPR å¼•å…¥äº†å™ªå£°æ„ŸçŸ¥æ­£åˆ™åŒ–ç­–ç•¥ï¼Œæ—¨åœ¨å‡è½»ä¼ªæ ‡ç­¾å™ªå£°åŠæºåŸŸè¿‡æ‹Ÿåˆå¸¦æ¥çš„å¹²æ‰°ï¼Œä»ç†è®ºå’Œå®è¯å±‚é¢å¢å¼ºäº†æ¨¡å‹çš„é²æ£’æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒNeGPR åœ¨å¤šç§åŸºå‡†æ•°æ®é›†ä¸Šå‡æ˜¾è‘—ä¼˜äºç°æœ‰å…ˆè¿›æ–¹æ³•ï¼Œåœ¨ä¸¥é‡æ ‡ç­¾å™ªå£°ç¯å¢ƒä¸‹å®ç°äº†é«˜è¾¾ 12.7% çš„å‡†ç¡®ç‡æå‡ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.00716v3",
      "published_date": "2025-08-01 15:32:40 UTC",
      "updated_date": "2025-09-08 12:47:16 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T08:05:23.648489+00:00"
    },
    {
      "arxiv_id": "2508.00969v2",
      "title": "Masked Omics Modeling for Multimodal Representation Learning across Histopathology and Molecular Profiles",
      "title_zh": "æ©ç ç»„å­¦å»ºæ¨¡ï¼šé¢å‘ç»„ç»‡ç—…ç†å­¦ä¸åˆ†å­ç‰¹å¾çš„å¤šæ¨¡æ€è¡¨ç¤ºå­¦ä¹ ",
      "authors": [
        "Lucas Robinet",
        "Ahmad Berjaoui",
        "Elizabeth Cohen-Jonathan Moyal"
      ],
      "abstract": "Self-supervised learning (SSL) has driven major advances in computational pathology by enabling the learning of rich representations from histopathology data. Yet, tissue analysis alone may fall short in capturing broader molecular complexity, as key complementary information resides in high-dimensional omics profiles such as transcriptomics, methylomics, and genomics. To address this gap, we introduce MORPHEUS, the first multimodal pre-training strategy that integrates histopathology images and multi-omics data within a shared transformer-based architecture. At its core, MORPHEUS relies on a novel masked omics modeling objective that encourages the model to learn meaningful cross-modal relationships. This yields a general-purpose pre-trained encoder that can be applied to histopathology alone or in combination with any subset of omics modalities. Beyond inference, MORPHEUS also supports flexible any-to-any omics reconstruction, enabling one or more omics profiles to be reconstructed from any modality subset that includes histopathology. Pre-trained on a large pan-cancer cohort, MORPHEUS shows substantial improvements over supervised and SSL baselines across diverse tasks and modality combinations. Together, these capabilities position it as a promising direction for the development of multimodal foundation models in oncology. Code is publicly available at https://github.com/Lucas-rbnt/MORPHEUS",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†MORPHEUSï¼Œè¿™æ˜¯é¦–ä¸ªåœ¨å…±äº«çš„Transformeræ¶æ„ä¸­æ•´åˆç»„ç»‡ç—…ç†å­¦(Histopathology)å›¾åƒä¸å¤šç»„å­¦(Multi-omics)æ•°æ®çš„å¤šæ¨¡æ€é¢„è®­ç»ƒç­–ç•¥ã€‚å…¶æ ¸å¿ƒé‡‡ç”¨äº†åˆ›æ–°çš„æ©ç ç»„å­¦å»ºæ¨¡(Masked Omics Modeling)ç›®æ ‡ï¼Œé€šè¿‡é¼“åŠ±æ¨¡å‹å­¦ä¹ æœ‰æ„ä¹‰çš„è·¨æ¨¡æ€å…³ç³»ï¼Œæ„å»ºäº†ä¸€ä¸ªé€šç”¨çš„é¢„è®­ç»ƒç¼–ç å™¨ã€‚è¯¥ç¼–ç å™¨å…·æœ‰æé«˜çš„çµæ´»æ€§ï¼Œæ—¢å¯å•ç‹¬åº”ç”¨äºç»„ç»‡ç—…ç†å­¦åˆ†æï¼Œä¹Ÿèƒ½ä¸è½¬å½•ç»„å­¦(Transcriptomics)ã€ç”²åŸºåŒ–ç»„å­¦(Methylomics)å’ŒåŸºå› ç»„å­¦(Genomics)ç­‰ä»»æ„ç»„å­¦å­é›†ç»“åˆä½¿ç”¨ã€‚æ­¤å¤–ï¼ŒMORPHEUSè¿˜æ”¯æŒçµæ´»çš„â€œä»»æ„å¯¹ä»»æ„â€(Any-to-any)ç»„å­¦é‡å»ºï¼Œèƒ½å¤Ÿä»åŒ…å«ç—…ç†å›¾åƒçš„ä»»æ„æ¨¡æ€å­é›†ä¸­é‡å»ºå‡ºä¸€é¡¹æˆ–å¤šé¡¹ç»„å­¦ç‰¹å¾ã€‚åœ¨å¤§å‹æ³›ç™Œç—‡(Pan-cancer)é˜Ÿåˆ—ä¸Šçš„å®éªŒç»“æœæ˜¾ç¤ºï¼ŒMORPHEUSåœ¨å¤šç§ä»»åŠ¡å’Œæ¨¡æ€ç»„åˆä¸‹çš„è¡¨ç°å‡æ˜¾è‘—ä¼˜äºä¼ ç»Ÿçš„ç›‘ç£å­¦ä¹ å’Œè‡ªç›‘ç£å­¦ä¹ (SSL)åŸºçº¿æ¨¡å‹ã€‚è¿™é¡¹å·¥ä½œä¸ºè‚¿ç˜¤å­¦é¢†åŸŸå¼€å‘å¤šæ¨¡æ€åŸºç¡€æ¨¡å‹(Multimodal foundation models)æä¾›äº†æå…·æ½œåŠ›çš„æ–¹å‘ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.00969v2",
      "published_date": "2025-08-01 15:29:26 UTC",
      "updated_date": "2025-12-16 10:35:22 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T08:05:28.599411+00:00"
    },
    {
      "arxiv_id": "2508.00712v1",
      "title": "JSON-Bag: A generic game trajectory representation",
      "title_zh": "JSON-Bagï¼šä¸€ç§é€šç”¨çš„æ¸¸æˆè½¨è¿¹è¡¨å¾",
      "authors": [
        "Dien Nguyen",
        "Diego Perez-Liebana",
        "Simon Lucas"
      ],
      "abstract": "We introduce JSON Bag-of-Tokens model (JSON-Bag) as a method to generically represent game trajectories by tokenizing their JSON descriptions and apply Jensen-Shannon distance (JSD) as distance metric for them. Using a prototype-based nearest-neighbor search (P-NNS), we evaluate the validity of JSON-Bag with JSD on six tabletop games -- \\textit{7 Wonders}, \\textit{Dominion}, \\textit{Sea Salt and Paper}, \\textit{Can't Stop}, \\textit{Connect4}, \\textit{Dots and boxes} -- each over three game trajectory classification tasks: classifying the playing agents, game parameters, or game seeds that were used to generate the trajectories.\n  Our approach outperforms a baseline using hand-crafted features in the majority of tasks. Evaluating on N-shot classification suggests using JSON-Bag prototype to represent game trajectory classes is also sample efficient. Additionally, we demonstrate JSON-Bag ability for automatic feature extraction by treating tokens as individual features to be used in Random Forest to solve the tasks above, which significantly improves accuracy on underperforming tasks. Finally, we show that, across all six games, the JSD between JSON-Bag prototypes of agent classes highly correlates with the distances between agents' policies.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† JSON Bag-of-Tokens æ¨¡å‹ (JSON-Bag)ï¼Œè¿™æ˜¯ä¸€ç§é€šè¿‡å¯¹ JSON æ ¼å¼çš„æè¿°è¿›è¡Œ Tokenize å¤„ç†æ¥é€šç”¨åŒ–è¡¨ç¤ºæ¸¸æˆè½¨è¿¹çš„æ–¹æ³•ã€‚ç ”ç©¶é‡‡ç”¨ Jensen-Shannon distance (JSD) ä½œä¸ºæ ¸å¿ƒè·ç¦»åº¦é‡ï¼Œå¹¶ç»“åˆ Prototype-based nearest-neighbor search (P-NNS) æŠ€æœ¯ï¼Œåœ¨ 7 Wonders å’Œ Dominion ç­‰å…­ç§æ¡Œæ¸¸çš„è½¨è¿¹åˆ†ç±»ä»»åŠ¡ä¸­è¿›è¡Œäº†ç³»ç»Ÿè¯„ä¼°ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒJSON-Bag åœ¨åˆ†ç±» playing agentsã€game parameters å’Œ game seeds çš„ä»»åŠ¡ä¸­è¡¨ç°ä¼˜äºä¼ ç»Ÿçš„æ‰‹åŠ¨æå–ç‰¹å¾ (hand-crafted features) æ–¹æ³•ã€‚åœ¨ N-shot åˆ†ç±»åœºæ™¯ä¸‹ï¼Œè¯¥æ¨¡å‹å±•ç°å‡ºäº†æé«˜çš„æ ·æœ¬æ•ˆç‡ (sample efficient)ï¼Œå¹¶èƒ½é€šè¿‡å°† Token è§†ä¸ºç‹¬ç«‹ç‰¹å¾è¿›è¡Œè‡ªåŠ¨ç‰¹å¾æå–ï¼Œæ˜¾è‘—æå‡é¢„æµ‹å‡†ç¡®ç‡ã€‚æœ€åï¼Œç ”ç©¶è¯å®äº†ä¸åŒ Agent ç±»åˆ«çš„ JSON-Bag åŸå‹ä¹‹é—´çš„ JSD è·ç¦»ä¸ Agent ç­–ç•¥ (policies) ä¹‹é—´çš„è·ç¦»å…·æœ‰é«˜åº¦ç›¸å…³æ€§ï¼Œä¸ºæ¸¸æˆè½¨è¿¹çš„é€šç”¨è¡¨å¾æä¾›äº†æœ‰æ•ˆå·¥å…·ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "8 pages, 3 figures, 6 tables, to be published in IEEE Conference on Games 2025",
      "pdf_url": "https://arxiv.org/pdf/2508.00712v1",
      "published_date": "2025-08-01 15:26:45 UTC",
      "updated_date": "2025-08-01 15:26:45 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T08:05:33.490890+00:00"
    },
    {
      "arxiv_id": "2508.00709v3",
      "title": "NyayaRAG: Realistic Legal Judgment Prediction with RAG under the Indian Common Law System",
      "title_zh": "NyayaRAGï¼šå°åº¦æ™®é€šæ³•ç³»ä¸‹åŸºäº RAG çš„ç°å®æ³•å¾‹åˆ¤å†³é¢„æµ‹",
      "authors": [
        "Shubham Kumar Nigam",
        "Balaramamahanthi Deepak Patnaik",
        "Shivam Mishra",
        "Ajay Varghese Thomas",
        "Noel Shallum",
        "Kripabandhu Ghosh",
        "Arnab Bhattacharya"
      ],
      "abstract": "Legal Judgment Prediction (LJP) has emerged as a key area in AI for law, aiming to automate judicial outcome forecasting and enhance interpretability in legal reasoning. While previous approaches in the Indian context have relied on internal case content such as facts, issues, and reasoning, they often overlook a core element of common law systems, which is reliance on statutory provisions and judicial precedents. In this work, we propose NyayaRAG, a Retrieval-Augmented Generation (RAG) framework that simulates realistic courtroom scenarios by providing models with factual case descriptions, relevant legal statutes, and semantically retrieved prior cases. NyayaRAG evaluates the effectiveness of these combined inputs in predicting court decisions and generating legal explanations using a domain-specific pipeline tailored to the Indian legal system. We assess performance across various input configurations using both standard lexical and semantic metrics as well as LLM-based evaluators such as G-Eval. Our results show that augmenting factual inputs with structured legal knowledge significantly improves both predictive accuracy and explanation quality.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†NyayaRAGï¼Œè¿™æ˜¯ä¸€ä¸ªåŸºäºæ£€ç´¢å¢å¼ºç”Ÿæˆ(RAG)çš„æ¡†æ¶ï¼Œæ—¨åœ¨æå‡å°åº¦æ™®é€šæ³•ç³»ç»Ÿä¸‹æ³•å¾‹åˆ¤å†³é¢„æµ‹(Legal Judgment Prediction, LJP)çš„å‡†ç¡®æ€§ä¸å¯è§£é‡Šæ€§ã€‚é’ˆå¯¹ä»¥å¾€æ–¹æ³•å¾€å¾€å¿½ç•¥æˆæ–‡æ³•æ¡(statutory provisions)å’Œå¸æ³•å…ˆä¾‹(judicial precedents)ç­‰æ ¸å¿ƒæ³•å¾‹è¦ç´ çš„é—®é¢˜ï¼ŒNyayaRAGé€šè¿‡æ¨¡æ‹ŸçœŸå®æ³•åº­åœºæ™¯ï¼Œä¸ºæ¨¡å‹æä¾›æ¡ˆä»¶äº‹å®æè¿°ã€ç›¸å…³æ³•è§„åŠè¯­ä¹‰æ£€ç´¢çš„æ—¢å¾€æ¡ˆä¾‹ã€‚ç ”ç©¶åˆ©ç”¨ä¸“é—¨é’ˆå¯¹å°åº¦æ³•å¾‹ç³»ç»Ÿè®¾è®¡çš„é¢†åŸŸç‰¹å®šæµæ°´çº¿(domain-specific pipeline)ï¼Œè¯„ä¼°äº†ä¸åŒè¾“å…¥ç»„åˆåœ¨é¢„æµ‹è£å†³å’Œç”Ÿæˆæ³•å¾‹è§£é‡Šæ–¹é¢çš„è¡¨ç°ã€‚å®éªŒé‡‡ç”¨æ ‡å‡†è¯æ³•ã€è¯­ä¹‰æŒ‡æ ‡åŠG-Evalç­‰å¤§è¯­è¨€æ¨¡å‹è¯„ä¼°å™¨è¿›è¡Œæµ‹è¯•ï¼Œç»“æœè¡¨æ˜ï¼Œç»“åˆç»“æ„åŒ–æ³•å¾‹çŸ¥è¯†ä¸äº‹å®è¾“å…¥èƒ½æ˜¾è‘—å¢å¼ºæ¨¡å‹çš„é¢„æµ‹å‡†ç¡®ç‡åŠè§£é‡Šè´¨é‡ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.IR",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "Paper accepted in the AACL-IJCNLP 2025 conference",
      "pdf_url": "https://arxiv.org/pdf/2508.00709v3",
      "published_date": "2025-08-01 15:23:20 UTC",
      "updated_date": "2025-11-15 14:54:56 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T08:05:46.854387+00:00"
    },
    {
      "arxiv_id": "2508.00707v2",
      "title": "Efficient Solution and Learning of Robust Factored MDPs",
      "title_zh": "é²æ£’åˆ†è§£é©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹çš„é«˜æ•ˆæ±‚è§£ä¸å­¦ä¹ ",
      "authors": [
        "Yannik Schnitzer",
        "Alessandro Abate",
        "David Parker"
      ],
      "abstract": "Robust Markov decision processes (r-MDPs) extend MDPs by explicitly modelling epistemic uncertainty about transition dynamics. Learning r-MDPs from interactions with an unknown environment enables the synthesis of robust policies with provable (PAC) guarantees on performance, but this can require a large number of sample interactions. We propose novel methods for solving and learning r-MDPs based on factored state-space representations that leverage the independence between model uncertainty across system components. Although policy synthesis for factored r-MDPs leads to hard, non-convex optimisation problems, we show how to reformulate these into tractable linear programs. Building on these, we also propose methods to learn factored model representations directly. Our experimental results show that exploiting factored structure can yield dimensional gains in sample efficiency, producing more effective robust policies with tighter performance guarantees than state-of-the-art methods.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ç¨³å¥é©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹ (r-MDPs) åœ¨å»ºæ¨¡è½¬ç§»åŠ¨åŠ›å­¦ä¸ç¡®å®šæ€§æ—¶é¢ä¸´çš„æ ·æœ¬éœ€æ±‚è¿‡å¤§é—®é¢˜ï¼Œæå‡ºäº†åˆ©ç”¨å› å­åˆ†è§£çŠ¶æ€ç©ºé—´è¡¨ç¤º (Factored state-space representations) çš„æ±‚è§£ä¸å­¦ä¹ æ–°æ–¹æ³•ã€‚å°½ç®¡å› å­åˆ†è§£ r-MDPs çš„ç­–ç•¥åˆæˆé€šå¸¸æ¶‰åŠå¤æ‚çš„éå‡¸ä¼˜åŒ– (Non-convex optimization) é—®é¢˜ï¼Œè¯¥ç ”ç©¶æˆåŠŸå°†å…¶é‡æ–°è¡¨è¿°ä¸ºå¯é«˜æ•ˆå¤„ç†çš„çº¿æ€§è§„åˆ’ (Linear programs) é—®é¢˜ã€‚æ­¤å¤–ï¼Œç ”ç©¶è¿˜æå‡ºäº†ç›´æ¥å­¦ä¹ å› å­åˆ†è§£æ¨¡å‹è¡¨ç¤ºçš„æ–¹æ³•ï¼Œèƒ½å¤Ÿæ›´æœ‰æ•ˆåœ°ä»äº¤äº’ä¸­æ•è·ç³»ç»Ÿç»„ä»¶é—´çš„ç‹¬ç«‹æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œåˆ©ç”¨å› å­åˆ†è§£ç»“æ„åœ¨æ ·æœ¬æ•ˆç‡ä¸Šå®ç°äº†é‡çº§æå‡ï¼Œç›¸è¾ƒäºç°æœ‰æœ€å…ˆè¿›æ–¹æ³•ï¼Œè¯¥æ–¹æ³•èƒ½ç”Ÿæˆæ›´æœ‰æ•ˆçš„ç¨³å¥ç­–ç•¥å¹¶æä¾›æ›´ä¸¥å¯†çš„æ€§èƒ½ä¿è¯ (PAC guarantees)ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.00707v2",
      "published_date": "2025-08-01 15:23:15 UTC",
      "updated_date": "2025-11-20 15:55:07 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T08:05:53.941746+00:00"
    },
    {
      "arxiv_id": "2508.00701v2",
      "title": "D3: Training-Free AI-Generated Video Detection Using Second-Order Features",
      "title_zh": "D3ï¼šåŸºäºäºŒé˜¶ç‰¹å¾çš„å…è®­ç»ƒ AI ç”Ÿæˆè§†é¢‘æ£€æµ‹",
      "authors": [
        "Chende Zheng",
        "Ruiqi suo",
        "Chenhao Lin",
        "Zhengyu Zhao",
        "Le Yang",
        "Shuai Liu",
        "Minghui Yang",
        "Cong Wang",
        "Chao Shen"
      ],
      "abstract": "The evolution of video generation techniques, such as Sora, has made it increasingly easy to produce high-fidelity AI-generated videos, raising public concern over the dissemination of synthetic content. However, existing detection methodologies remain limited by their insufficient exploration of temporal artifacts in synthetic videos. To bridge this gap, we establish a theoretical framework through second-order dynamical analysis under Newtonian mechanics, subsequently extending the Second-order Central Difference features tailored for temporal artifact detection. Building on this theoretical foundation, we reveal a fundamental divergence in second-order feature distributions between real and AI-generated videos. Concretely, we propose Detection by Difference of Differences (D3), a novel training-free detection method that leverages the above second-order temporal discrepancies. We validate the superiority of our D3 on 4 open-source datasets (Gen-Video, VideoPhy, EvalCrafter, VidProM), 40 subsets in total. For example, on GenVideo, D3 outperforms the previous best method by 10.39% (absolute) mean Average Precision. Additional experiments on time cost and post-processing operations demonstrate D3's exceptional computational efficiency and strong robust performance. Our code is available at https://github.com/Zig-HS/D3.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹Soraç­‰è§†é¢‘ç”ŸæˆæŠ€æœ¯å¼•å‘çš„åˆæˆå†…å®¹ä¼ æ’­æ‹…å¿§ï¼ŒæŒ‡å‡ºå½“å‰æ£€æµ‹æ–¹æ³•åœ¨æ¢ç´¢åˆæˆè§†é¢‘æ—¶åºä¼ªå½±(temporal artifacts)æ–¹é¢å­˜åœ¨ä¸è¶³ã€‚ç ”ç©¶è€…åŸºäºç‰›é¡¿åŠ›å­¦(Newtonian mechanics)ä¸‹çš„äºŒé˜¶åŠ¨åŠ›å­¦åˆ†æ(second-order dynamical analysis)å»ºç«‹äº†ç†è®ºæ¡†æ¶ï¼Œå¹¶æå–äº†ä¸“é—¨ç”¨äºæ—¶åºä¼ªå½±æ£€æµ‹çš„äºŒé˜¶ä¸­å¿ƒå·®åˆ†(Second-order Central Difference)ç‰¹å¾ã€‚é€šè¿‡æ­ç¤ºçœŸå®è§†é¢‘ä¸AIç”Ÿæˆè§†é¢‘åœ¨äºŒé˜¶ç‰¹å¾åˆ†å¸ƒä¸Šçš„æ ¹æœ¬å·®å¼‚ï¼Œç ”ç©¶æå‡ºäº†D3 (Detection by Difference of Differences)ï¼Œè¿™æ˜¯ä¸€ç§åˆ©ç”¨ä¸Šè¿°äºŒé˜¶æ—¶åºå·®å¼‚ä¸”æ— éœ€è®­ç»ƒ(training-free)çš„æ–°å‹æ£€æµ‹æ–¹æ³•ã€‚å®éªŒåœ¨Gen-Videoã€VideoPhyã€EvalCrafterå’ŒVidProMç­‰4ä¸ªå¼€æºæ•°æ®é›†ä¸ŠéªŒè¯äº†D3çš„ä¼˜è¶Šæ€§ï¼Œå…¶åœ¨GenVideoä¸Šçš„å¹³å‡ç²¾åº¦å‡å€¼(mAP)æ¯”ç°æœ‰æœ€ä¼˜æ–¹æ³•æå‡äº†10.39%ã€‚æ­¤å¤–ï¼Œæ—¶é—´æˆæœ¬å’Œåå¤„ç†æ“ä½œå®éªŒè¿›ä¸€æ­¥è¯æ˜äº†D3å“è¶Šçš„è®¡ç®—æ•ˆç‡å’Œå¼ºåŠ²çš„é²æ£’æ€§èƒ½ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "8 pages, 4 figures",
      "pdf_url": "https://arxiv.org/pdf/2508.00701v2",
      "published_date": "2025-08-01 15:17:51 UTC",
      "updated_date": "2025-08-05 03:05:49 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T08:06:01.986832+00:00"
    },
    {
      "arxiv_id": "2508.00697v1",
      "title": "On-Device Diffusion Transformer Policy for Efficient Robot Manipulation",
      "title_zh": "é¢å‘é«˜æ•ˆæœºå™¨äººæ“çºµçš„ç«¯ä¾§æ‰©æ•£ Transformer ç­–ç•¥",
      "authors": [
        "Yiming Wu",
        "Huan Wang",
        "Zhenghao Chen",
        "Jianxin Pang",
        "Dong Xu"
      ],
      "abstract": "Diffusion Policies have significantly advanced robotic manipulation tasks via imitation learning, but their application on resource-constrained mobile platforms remains challenging due to computational inefficiency and extensive memory footprint. In this paper, we propose LightDP, a novel framework specifically designed to accelerate Diffusion Policies for real-time deployment on mobile devices. LightDP addresses the computational bottleneck through two core strategies: network compression of the denoising modules and reduction of the required sampling steps. We first conduct an extensive computational analysis on existing Diffusion Policy architectures, identifying the denoising network as the primary contributor to latency. To overcome performance degradation typically associated with conventional pruning methods, we introduce a unified pruning and retraining pipeline, optimizing the model's post-pruning recoverability explicitly. Furthermore, we combine pruning techniques with consistency distillation to effectively reduce sampling steps while maintaining action prediction accuracy. Experimental evaluations on the standard datasets, \\ie, PushT, Robomimic, CALVIN, and LIBERO, demonstrate that LightDP achieves real-time action prediction on mobile devices with competitive performance, marking an important step toward practical deployment of diffusion-based policies in resource-limited environments. Extensive real-world experiments also show the proposed LightDP can achieve performance comparable to state-of-the-art Diffusion Policies.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† LightDPï¼Œè¿™æ˜¯ä¸€ä¸ªä¸“ä¸ºç§»åŠ¨è®¾å¤‡è®¾è®¡çš„è½»é‡çº§æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³ Diffusion Policies åœ¨èµ„æºå—é™å¹³å°ä¸Šé¢ä¸´çš„è®¡ç®—æ•ˆç‡ä½å’Œå†…å­˜å ç”¨å¤§ç­‰æŒ‘æˆ˜ã€‚é’ˆå¯¹å»å™ªæ¨¡å—ï¼ˆdenoising modulesï¼‰å¯¼è‡´çš„è®¡ç®—ç“¶é¢ˆï¼Œç ”ç©¶è€…å¼•å…¥äº†ä¸€ç§ç»Ÿä¸€çš„å‰ªæä¸å†è®­ç»ƒæµæ°´çº¿ï¼Œé€šè¿‡æ˜¾å¼ä¼˜åŒ–æ¨¡å‹å‰ªæåçš„å¯æ¢å¤æ€§æ¥å…‹æœæ€§èƒ½é€€åŒ–é—®é¢˜ã€‚æ­¤å¤–ï¼Œè¯¥æ¡†æ¶å°†å‰ªææŠ€æœ¯ä¸ consistency distillation ç›¸ç»“åˆï¼Œåœ¨å¤§å¹…å‡å°‘é‡‡æ ·æ­¥æ•°çš„åŒæ—¶ä¿æŒäº†é«˜ç²¾åº¦çš„åŠ¨ä½œé¢„æµ‹ã€‚åœ¨ PushTã€Robomimicã€CALVIN å’Œ LIBERO ç­‰æ ‡å‡†æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒLightDP èƒ½å¤Ÿå®ç°ç§»åŠ¨è®¾å¤‡ä¸Šçš„å®æ—¶åŠ¨ä½œé¢„æµ‹ã€‚å¤§é‡çš„ç°å®ä¸–ç•Œå®éªŒè¿›ä¸€æ­¥è¯æ˜ï¼ŒLightDP åœ¨èµ„æºæœ‰é™çš„ç¯å¢ƒä¸­è¾¾åˆ°äº†ä¸æœ€å…ˆè¿› Diffusion Policies ç›¸å½“çš„æ€§èƒ½ï¼Œä¸ºæ‰©æ•£ç­–ç•¥çš„å®é™…éƒ¨ç½²æä¾›äº†é«˜æ•ˆè·¯å¾„ã€‚",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.RO",
      "comment": "ICCV 2025",
      "pdf_url": "https://arxiv.org/pdf/2508.00697v1",
      "published_date": "2025-08-01 15:14:39 UTC",
      "updated_date": "2025-08-01 15:14:39 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T08:06:02.339938+00:00"
    },
    {
      "arxiv_id": "2508.10917v1",
      "title": "Managing the unexpected: Operator behavioural data and its value in predicting correct alarm responses",
      "title_zh": "åº”å¯¹çªå‘çŠ¶å†µï¼šæ“ä½œå‘˜è¡Œä¸ºæ•°æ®åœ¨é¢„æµ‹æŠ¥è­¦å“åº”å‡†ç¡®æ€§ä¸­çš„ä»·å€¼",
      "authors": [
        "Chidera W. Amazu",
        "Joseph Mietkiewicz",
        "Ammar N. Abbas",
        "Gabriele Baldissone",
        "Davide Fissore",
        "Micaela Demichela",
        "Anders L. Madsen",
        "Maria Chiara Leva"
      ],
      "abstract": "Data from psychophysiological measures can offer new insight into control room operators' behaviour, cognition, and mental workload status. This can be particularly helpful when combined with appraisal of capacity to respond to possible critical plant conditions (i.e. critical alarms response scenarios). However, wearable physiological measurement tools such as eye tracking and EEG caps can be perceived as intrusive and not suitable for usage in daily operations. Therefore, this article examines the potential of using real-time data from process and operator-system interactions during abnormal scenarios that can be recorded and retrieved from the distributed control system's historian or process log, and their capacity to provide insight into operator behavior and predict their response outcomes, without intruding on daily tasks. Data for this study were obtained from a design of experiment using a formaldehyde production plant simulator and four human-in-the-loop experimental support configurations. A comparison between the different configurations in terms of both behaviour and performance is presented in this paper. A step-wise logistic regression and a Bayesian network models were used to achieve this objective. The results identified some predictive metrics and the paper discuss their value as precursor or predictor of overall system performance in alarm response scenarios. Knowledge of relevant and predictive behavioural metrics accessible in real time can better equip decision-makers to predict outcomes and provide timely support measures for operators.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†åœ¨æ§åˆ¶å®¤å¼‚å¸¸åœºæ™¯ä¸‹ï¼Œå¦‚ä½•åˆ©ç”¨éä¾µå…¥æ€§æ•°æ®é¢„æµ‹æ“ä½œå‘˜çš„æŠ¥è­¦å“åº”è¡Œä¸ºã€‚é’ˆå¯¹å¯ç©¿æˆ´ç”Ÿç†æµ‹é‡å·¥å…·åœ¨æ—¥å¸¸è¿è¡Œä¸­å…·æœ‰ä¾µå…¥æ€§çš„é—®é¢˜ï¼Œä½œè€…æå‡ºåˆ©ç”¨æ¥è‡ªåˆ†å¸ƒå¼æ§åˆ¶ç³»ç»Ÿ (Distributed Control System) å†å²æ•°æ®åº“æˆ–è¿‡ç¨‹æ—¥å¿—ä¸­çš„å®æ—¶è¿‡ç¨‹ä¸äººæœºäº¤äº’æ•°æ®è¿›è¡Œåˆ†æã€‚ç ”ç©¶é€šè¿‡ç”²é†›ç”Ÿäº§å‚æ¨¡æ‹Ÿå™¨ (Formaldehyde production plant simulator) å’Œå››ç§äººåœ¨å›è·¯ (Human-in-the-loop) å®éªŒé…ç½®è·å–æ•°æ®ï¼Œå¹¶å¯¹æ¯”äº†ä¸åŒé…ç½®ä¸‹çš„æ“ä½œå‘˜è¡Œä¸ºä¸ç»©æ•ˆã€‚åˆ†æè¿‡ç¨‹ä¸­é‡‡ç”¨äº†é€æ­¥é€»è¾‘å›å½’ (Step-wise logistic regression) å’Œè´å¶æ–¯ç½‘ç»œ (Bayesian network) æ¨¡å‹æ¥è¯†åˆ«å…³é”®çš„é¢„æµ‹æŒ‡æ ‡ã€‚ç»“æœç¡®å®šäº†ä¸€ç³»åˆ—èƒ½å¤Ÿä½œä¸ºæŠ¥è­¦å“åº”åœºæ™¯ä¸‹ç³»ç»Ÿæ•´ä½“ç»©æ•ˆå‰å…†çš„é¢„æµ‹æ€§è¡Œä¸ºæŒ‡æ ‡ã€‚è¿™äº›å®æ—¶å¯è·å–çš„æ•°æ®ä¸ºå†³ç­–è€…æä¾›äº†é¢„æµ‹æ“ä½œç»“æœçš„ç§‘å­¦ä¾æ®ï¼Œæœ‰åŠ©äºåœ¨ä¸å¹²æ‰°æ—¥å¸¸ä»»åŠ¡çš„å‰æä¸‹ä¸ºæ“ä½œå‘˜æä¾›åŠæ—¶çš„æ”¯æŒæªæ–½ã€‚",
      "categories": [
        "cs.HC",
        "cs.AI"
      ],
      "primary_category": "cs.HC",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.10917v1",
      "published_date": "2025-08-01 15:10:16 UTC",
      "updated_date": "2025-08-01 15:10:16 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T08:06:01.148686+00:00"
    },
    {
      "arxiv_id": "2508.00679v1",
      "title": "Segment First, Retrieve Better: Realistic Legal Search via Rhetorical Role-Based Queries",
      "title_zh": "å…ˆåˆ†æ®µï¼Œåæ£€ç´¢ï¼šåŸºäºä¿®è¾è§’è‰²æŸ¥è¯¢çš„ç°å®æ³•å¾‹æ£€ç´¢",
      "authors": [
        "Shubham Kumar Nigam",
        "Tanmay Dubey",
        "Noel Shallum",
        "Arnab Bhattacharya"
      ],
      "abstract": "Legal precedent retrieval is a cornerstone of the common law system, governed by the principle of stare decisis, which demands consistency in judicial decisions. However, the growing complexity and volume of legal documents challenge traditional retrieval methods. TraceRetriever mirrors real-world legal search by operating with limited case information, extracting only rhetorically significant segments instead of requiring complete documents. Our pipeline integrates BM25, Vector Database, and Cross-Encoder models, combining initial results through Reciprocal Rank Fusion before final re-ranking. Rhetorical annotations are generated using a Hierarchical BiLSTM CRF classifier trained on Indian judgments. Evaluated on IL-PCR and COLIEE 2025 datasets, TraceRetriever addresses growing document volume challenges while aligning with practical search constraints, reliable and scalable foundation for precedent retrieval enhancing legal research when only partial case knowledge is available.",
      "tldr_zh": "æ³•å¾‹å…ˆä¾‹æ£€ç´¢å¯¹äºéµå¾ªå…ˆä¾‹åŸåˆ™(stare decisis)è‡³å…³é‡è¦ï¼Œä½†æ³•å¾‹æ–‡æ¡£æ—¥ç›Šå¢é•¿çš„å¤æ‚æ€§å’Œä½“é‡å¯¹ä¼ ç»Ÿæ£€ç´¢æ–¹æ³•æ„æˆäº†å·¨å¤§æŒ‘æˆ˜ã€‚è¯¥ç ”ç©¶æå‡ºäº† TraceRetriever æ¡†æ¶ï¼Œé€šè¿‡æ¨¡æ‹ŸçœŸå®çš„æ³•å¾‹æœç´¢åœºæ™¯ï¼Œåœ¨ä»…æœ‰æœ‰é™æ¡ˆä»¶ä¿¡æ¯çš„æƒ…å†µä¸‹ï¼Œæå–å…·æœ‰ä¿®è¾æ„ä¹‰çš„ç‰‡æ®µ(rhetorically significant segments)è€Œéä¾èµ–å®Œæ•´æ–‡æ¡£è¿›è¡Œæ£€ç´¢ã€‚è¯¥ç³»ç»Ÿçš„æŠ€æœ¯ç®¡çº¿æ•´åˆäº† BM25ã€å‘é‡æ•°æ®åº“(Vector Database)å’Œäº¤å‰ç¼–ç å™¨(Cross-Encoder)æ¨¡å‹ï¼Œå¹¶åˆ©ç”¨ç›¸äº’ç§©èåˆ(Reciprocal Rank Fusion)æŠ€æœ¯ç»“åˆåˆå§‹ç»“æœï¼Œæœ€åè¿›è¡Œé‡æ’åºã€‚æ­¤å¤–ï¼Œç ”ç©¶é‡‡ç”¨åœ¨å°åº¦å¸æ³•åˆ¤å†³æ•°æ®ä¸Šè®­ç»ƒçš„å±‚æ¬¡åŒ– BiLSTM CRF åˆ†ç±»å™¨æ¥ç”Ÿæˆä¿®è¾æ ‡æ³¨(Rhetorical annotations)ã€‚åœ¨ IL-PCR å’Œ COLIEE 2025 æ•°æ®é›†ä¸Šçš„è¯„ä¼°ç»“æœè¡¨æ˜ï¼ŒTraceRetriever èƒ½å¤Ÿæœ‰æ•ˆåº”å¯¹æ–‡æ¡£æ•°é‡å¢åŠ å¸¦æ¥çš„æŒ‘æˆ˜ï¼Œå¹¶ç¬¦åˆå®é™…æœç´¢çº¦æŸã€‚è¯¥ç ”ç©¶ä¸ºåœ¨ä»…æŒæ¡éƒ¨åˆ†æ¡ˆä»¶çŸ¥è¯†çš„æƒ…å†µä¸‹è¿›è¡Œé«˜æ•ˆå…ˆä¾‹æ£€ç´¢æä¾›äº†å¯é ä¸”å¯æ‰©å±•çš„åŸºç¡€ï¼Œæ˜¾è‘—å¢å¼ºäº†æ³•å¾‹ç ”ç©¶çš„å®è·µèƒ½åŠ›ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.IR",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.00679v1",
      "published_date": "2025-08-01 14:49:33 UTC",
      "updated_date": "2025-08-01 14:49:33 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T08:06:05.642906+00:00"
    },
    {
      "arxiv_id": "2508.00674v1",
      "title": "Context-Aware Visualization for Explainable AI Recommendations in Social Media: A Vision for User-Aligned Explanations",
      "title_zh": "ç¤¾äº¤åª’ä½“å¯è§£é‡Šäººå·¥æ™ºèƒ½æ¨èçš„æƒ…å¢ƒæ„ŸçŸ¥å¯è§†åŒ–ï¼šç”¨æˆ·å¯¹é½è§£é‡Šçš„æ„¿æ™¯",
      "authors": [
        "Banan Alkhateeb",
        "Ellis Solaiman"
      ],
      "abstract": "Social media platforms today strive to improve user experience through AI recommendations, yet the value of such recommendations vanishes as users do not understand the reasons behind them. This issue arises because explainability in social media is general and lacks alignment with user-specific needs. In this vision paper, we outline a user-segmented and context-aware explanation layer by proposing a visual explanation system with diverse explanation methods. The proposed system is framed by the variety of user needs and contexts, showing explanations in different visualized forms, including a technically detailed version for AI experts and a simplified one for lay users. Our framework is the first to jointly adapt explanation style (visual vs. numeric) and granularity (expert vs. lay) inside a single pipeline. A public pilot with 30 X users will validate its impact on decision-making and trust.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ç¤¾äº¤åª’ä½“å¹³å°AIæ¨èç¼ºä¹é€æ˜åº¦ä¸”è§£é‡Šè¿‡äºæ³›åŒ–çš„é—®é¢˜ï¼Œæå‡ºäº†ä¸€ä¸ªä¸Šä¸‹æ–‡æ„ŸçŸ¥ï¼ˆContext-Awareï¼‰çš„å¯è§†åŒ–è§£é‡Šå±‚æ„æƒ³ï¼Œæ—¨åœ¨å®ç°ç”¨æˆ·å¯¹é½çš„è§£é‡Šã€‚è¯¥ç³»ç»Ÿé€šè¿‡ç”¨æˆ·ç»†åˆ†ï¼ˆuser-segmentedï¼‰ç­–ç•¥ï¼Œä¸ºä¸åŒèƒŒæ™¯çš„ç”¨æˆ·æä¾›å¤šæ ·åŒ–çš„å¯è§†åŒ–è§£é‡Šæ–¹æ¡ˆï¼ŒåŒ…æ‹¬é¢å‘AIä¸“å®¶çš„æŠ€æœ¯ç»†èŠ‚å’Œé¢å‘æ™®é€šç”¨æˆ·çš„ç®€åŒ–ç‰ˆæœ¬ã€‚è¿™æ˜¯é¦–ä¸ªå°è¯•åœ¨å•ä¸€æµæ°´çº¿ï¼ˆsingle pipelineï¼‰ä¸­åŒæ—¶åŠ¨æ€è°ƒæ•´è§£é‡Šé£æ ¼ï¼ˆè§†è§‰ vs æ•°å€¼ï¼‰ä¸è§£é‡Šç²’åº¦ï¼ˆä¸“å®¶ vs é—¨å¤–æ±‰ï¼‰çš„æ¡†æ¶ã€‚é€šè¿‡å¯¹30åXå¹³å°ç”¨æˆ·è¿›è¡Œçš„è¯•ç‚¹ç ”ç©¶ï¼Œè¯¥æ–¹æ¡ˆå°†éªŒè¯å…¶åœ¨ä¼˜åŒ–ç”¨æˆ·å†³ç­–å’Œå»ºç«‹ç³»ç»Ÿä¿¡ä»»æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚",
      "categories": [
        "cs.AI",
        "cs.HC",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.00674v1",
      "published_date": "2025-08-01 14:47:47 UTC",
      "updated_date": "2025-08-01 14:47:47 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T08:06:14.790435+00:00"
    },
    {
      "arxiv_id": "2508.00967v1",
      "title": "Cooperative Perception: A Resource-Efficient Framework for Multi-Drone 3D Scene Reconstruction Using Federated Diffusion and NeRF",
      "title_zh": "ååŒæ„ŸçŸ¥ï¼šä¸€ç§åŸºäºè”é‚¦æ‰©æ•£æ¨¡å‹ä¸ NeRF çš„èµ„æºé«˜æ•ˆå‹å¤šæ— äººæœºä¸‰ç»´åœºæ™¯é‡å»ºæ¡†æ¶",
      "authors": [
        "Massoud Pourmandi"
      ],
      "abstract": "The proposal introduces an innovative drone swarm perception system that aims to solve problems related to computational limitations and low-bandwidth communication, and real-time scene reconstruction. The framework enables efficient multi-agent 3D/4D scene synthesis through federated learning of shared diffusion model and YOLOv12 lightweight semantic extraction and local NeRF updates while maintaining privacy and scalability. The framework redesigns generative diffusion models for joint scene reconstruction, and improves cooperative scene understanding, while adding semantic-aware compression protocols. The approach can be validated through simulations and potential real-world deployment on drone testbeds, positioning it as a disruptive advancement in multi-agent AI for autonomous systems.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§åˆ›æ–°çš„æ— äººæœºé›†ç¾¤æ„ŸçŸ¥æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³è®¡ç®—èµ„æºå—é™ã€ä½å¸¦å®½é€šä¿¡ä»¥åŠå®æ—¶åœºæ™¯é‡å»ºä¸­çš„æ ¸å¿ƒæŒ‘æˆ˜ã€‚è¯¥æ¡†æ¶é€šè¿‡è”é‚¦å­¦ä¹  (Federated Learning) å…±äº«æ‰©æ•£æ¨¡å‹ (Diffusion Model)ï¼Œå®ç°äº†é«˜æ•ˆçš„å¤šæ™ºèƒ½ä½“ 3D/4D åœºæ™¯åˆæˆã€‚ç³»ç»Ÿé›†æˆäº† YOLOv12 è½»é‡åŒ–è¯­ä¹‰æå–æŠ€æœ¯ä¸å±€éƒ¨ç¥ç»è¾å°„åœº (NeRF) æ›´æ–°æœºåˆ¶ï¼Œåœ¨ç»´æŠ¤æ•°æ®éšç§å’Œç³»ç»Ÿå¯æ‰©å±•æ€§çš„å‰æä¸‹æå‡äº†æ„ŸçŸ¥æ•ˆç‡ã€‚æ­¤å¤–ï¼Œç ”ç©¶é‡æ–°è®¾è®¡äº†ç”Ÿæˆå¼æ‰©æ•£æ¨¡å‹ä»¥æ”¯æŒè”åˆåœºæ™¯é‡å»ºï¼Œå¹¶å¼•å…¥äº†è¯­ä¹‰æ„ŸçŸ¥å‹ç¼©åè®® (Semantic-aware Compression Protocols) æ¥å¢å¼ºåä½œå¼åœºæ™¯ç†è§£ã€‚é€šè¿‡åœ¨ä»¿çœŸç¯å¢ƒåŠæ— äººæœºæµ‹è¯•å¹³å°ä¸Šçš„åˆæ­¥éªŒè¯ï¼Œè¯¥æ–¹æ³•å±•ç¤ºäº†å…¶åœ¨å¤šæ™ºèƒ½ä½“äººå·¥æ™ºèƒ½è‡ªä¸»ç³»ç»Ÿé¢†åŸŸä½œä¸ºé¢ è¦†æ€§æŠ€æœ¯çš„åº”ç”¨æ½œåŠ›ã€‚",
      "categories": [
        "cs.AI",
        "cs.RO"
      ],
      "primary_category": "cs.AI",
      "comment": "15 pages, 3 figures, 1 table, 1 algorithm. Preprint based on NeurIPS 2024 template",
      "pdf_url": "https://arxiv.org/pdf/2508.00967v1",
      "published_date": "2025-08-01 14:43:24 UTC",
      "updated_date": "2025-08-01 14:43:24 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T08:06:16.484497+00:00"
    },
    {
      "arxiv_id": "2508.00669v1",
      "title": "Medical Reasoning in the Era of LLMs: A Systematic Review of Enhancement Techniques and Applications",
      "title_zh": "LLM æ—¶ä»£çš„åŒ»å­¦æ¨ç†ï¼šå¢å¼ºæŠ€æœ¯ä¸åº”ç”¨ç³»ç»Ÿç»¼è¿°",
      "authors": [
        "Wenxuan Wang",
        "Zizhan Ma",
        "Meidan Ding",
        "Shiyi Zheng",
        "Shengyuan Liu",
        "Jie Liu",
        "Jiaming Ji",
        "Wenting Chen",
        "Xiang Li",
        "Linlin Shen",
        "Yixuan Yuan"
      ],
      "abstract": "The proliferation of Large Language Models (LLMs) in medicine has enabled impressive capabilities, yet a critical gap remains in their ability to perform systematic, transparent, and verifiable reasoning, a cornerstone of clinical practice. This has catalyzed a shift from single-step answer generation to the development of LLMs explicitly designed for medical reasoning. This paper provides the first systematic review of this emerging field. We propose a taxonomy of reasoning enhancement techniques, categorized into training-time strategies (e.g., supervised fine-tuning, reinforcement learning) and test-time mechanisms (e.g., prompt engineering, multi-agent systems). We analyze how these techniques are applied across different data modalities (text, image, code) and in key clinical applications such as diagnosis, education, and treatment planning. Furthermore, we survey the evolution of evaluation benchmarks from simple accuracy metrics to sophisticated assessments of reasoning quality and visual interpretability. Based on an analysis of 60 seminal studies from 2022-2025, we conclude by identifying critical challenges, including the faithfulness-plausibility gap and the need for native multimodal reasoning, and outlining future directions toward building efficient, robust, and sociotechnically responsible medical AI.",
      "tldr_zh": "è¯¥ç ”ç©¶ç³»ç»Ÿåœ°å›é¡¾äº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨åŒ»å­¦æ¨ç†é¢†åŸŸçš„å‘å±•ï¼Œæ—¨åœ¨è§£å†³LLMsåœ¨ä¸´åºŠå®è·µä¸­ç¼ºä¹ç³»ç»Ÿæ€§ã€é€æ˜æ€§å’Œå¯éªŒè¯æ€§æ¨ç†çš„é—®é¢˜ã€‚è®ºæ–‡æå‡ºäº†ä¸€ç§æ¨ç†å¢å¼ºæŠ€æœ¯çš„åˆ†ç±»æ³•ï¼Œå°†å…¶åˆ’åˆ†ä¸ºè®­ç»ƒæ—¶ç­–ç•¥ï¼ˆå¦‚Supervised Fine-Tuningã€Reinforcement Learningï¼‰å’Œæµ‹è¯•æ—¶æœºåˆ¶ï¼ˆå¦‚Prompt Engineeringã€Multi-agent Systemsï¼‰ã€‚ç ”ç©¶æ·±å…¥åˆ†æäº†è¿™äº›æŠ€æœ¯åœ¨æ–‡æœ¬ã€å›¾åƒå’Œä»£ç ç­‰å¤šæ¨¡æ€æ•°æ®ä¸Šçš„åº”ç”¨ï¼Œæ¶µç›–äº†ä¸´åºŠè¯Šæ–­ã€åŒ»å­¦æ•™è‚²å’Œæ²»ç–—è§„åˆ’ç­‰æ ¸å¿ƒåº”ç”¨åœºæ™¯ã€‚åŒæ—¶ï¼Œæ–‡ç« æ¢è®¨äº†è¯„ä¼°åŸºå‡†ä»å•ä¸€å‡†ç¡®ç‡å‘æ¨ç†è´¨é‡åŠè§†è§‰å¯è§£é‡Šæ€§ï¼ˆVisual Interpretabilityï¼‰è¯„ä¼°çš„æ¼”å˜è¿‡ç¨‹ã€‚åŸºäºå¯¹2022è‡³2025å¹´é—´60é¡¹æ ¸å¿ƒç ”ç©¶çš„åˆ†æï¼Œä½œè€…æŒ‡å‡ºäº†å¿ å®æ€§ä¸åˆç†æ€§å·®è·ï¼ˆFaithfulness-Plausibility Gapï¼‰ä»¥åŠåŸç”Ÿå¤šæ¨¡æ€æ¨ç†ç­‰å…³é”®æŒ‘æˆ˜ã€‚è¯¥ç»¼è¿°ä¸ºæ„å»ºé«˜æ•ˆã€é²æ£’ä¸”å…·å¤‡ç¤¾ä¼šæŠ€æœ¯è´£ä»»æ„Ÿçš„åŒ»ç–—äººå·¥æ™ºèƒ½æä¾›äº†é‡è¦çš„æœªæ¥ç ”ç©¶æ–¹å‘ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.CV",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.00669v1",
      "published_date": "2025-08-01 14:41:31 UTC",
      "updated_date": "2025-08-01 14:41:31 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T08:06:19.085779+00:00"
    },
    {
      "arxiv_id": "2508.00668v1",
      "title": "Advancing Quantum Information Science Pre-College Education: The Case for Learning Sciences Collaboration",
      "title_zh": "æ¨è¿›å¤§å­¦å‰é‡å­ä¿¡æ¯ç§‘å­¦æ•™è‚²ï¼šè®ºå­¦ä¹ ç§‘å­¦åä½œçš„å¿…è¦æ€§",
      "authors": [
        "Raquel Coelho",
        "Roy Pea",
        "Christian Schunn",
        "Jinglei Cheng",
        "Junyu Liu"
      ],
      "abstract": "As quantum information science advances and the need for pre-college engagement grows, a critical question remains: How can young learners be prepared to participate in a field so radically different from what they have encountered before? This paper argues that meeting this challenge will require strong interdisciplinary collaboration with the Learning Sciences (LS), a field dedicated to understanding how people learn and designing theory-guided environments to support learning. Drawing on lessons from previous STEM education efforts, we discuss two key contributions of the learning sciences to quantum information science (QIS) education. The first is design-based research, the signature methodology of learning sciences, which can inform the development, refinement, and scaling of effective QIS learning experiences. The second is a framework for reshaping how learners reason about, learn and participate in QIS practices through shifts in knowledge representations that provide new forms of engagement and associated learning. We call for a two-way partnership between quantum information science and the learning sciences, one that not only supports learning in quantum concepts and practices but also improves our understanding of how to teach and support learning in highly complex domains. We also consider potential questions involved in bridging these disciplinary communities and argue that the theoretical and practical benefits justify the effort.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†å¦‚ä½•ä¸ºå¤§å­¦å‰æ•™è‚²é˜¶æ®µçš„å­¦ç”Ÿå‡†å¤‡ Quantum Information Science (QIS) çš„å­¦ä¹ ï¼Œå¼ºè°ƒäº†ä¸ Learning Sciences (LS) è¿›è¡Œè·¨å­¦ç§‘åˆä½œçš„å¿…è¦æ€§ã€‚æ–‡ç« æŒ‡å‡ºï¼Œå€Ÿé‰´ä»¥å¾€ STEM æ•™è‚²çš„ç»éªŒï¼ŒLearning Sciences å¯ä»¥ä¸º QIS æ•™è‚²æä¾›ä¸¤å¤§æ ¸å¿ƒè´¡çŒ®ï¼šä¸€æ˜¯åˆ©ç”¨å…¶æ ‡å¿—æ€§çš„ Design-Based Research (DBR) æ–¹æ³•è®ºæ¥æŒ‡å¯¼ QIS å­¦ä¹ ä½“éªŒçš„å¼€å‘ã€ä¼˜åŒ–ä¸æ¨å¹¿ï¼›äºŒæ˜¯é€šè¿‡çŸ¥è¯†è¡¨å¾ (knowledge representations) çš„è½¬å˜æ¥é‡å¡‘å­¦ä¹ è€…çš„æ¨ç†ä¸å‚ä¸æ–¹å¼ã€‚ä½œè€…å‘¼å QIS ä¸ Learning Sciences å»ºç«‹åŒå‘åˆä½œä¼™ä¼´å…³ç³»ï¼Œè¿™ä¸ä»…æœ‰åŠ©äºå­¦ç”ŸæŒæ¡å¤æ‚çš„é‡å­æ¦‚å¿µï¼Œä¹Ÿèƒ½æå‡æ•™è‚²ç•Œå¯¹å¤æ‚é¢†åŸŸæ•™å­¦è§„å¾‹çš„è®¤çŸ¥ã€‚é€šè¿‡å¼¥åˆè¿™ä¸¤ä¸ªå­¦ç§‘ç¾¤ä½“çš„é¸¿æ²Ÿï¼Œè¯¥ç ”ç©¶æ—¨åœ¨ä¸ºæœªæ¥é‡å­ç§‘æŠ€äººæ‰çš„åŸ¹å…»æä¾›ç†è®ºæ”¯æŒä¸å®è·µæŒ‡å¯¼ã€‚",
      "categories": [
        "physics.ed-ph",
        "cs.AI",
        "cs.CY",
        "quant-ph"
      ],
      "primary_category": "physics.ed-ph",
      "comment": "12 pages, 2 figures",
      "pdf_url": "https://arxiv.org/pdf/2508.00668v1",
      "published_date": "2025-08-01 14:41:18 UTC",
      "updated_date": "2025-08-01 14:41:18 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T08:06:28.384261+00:00"
    },
    {
      "arxiv_id": "2508.00665v1",
      "title": "Transparent Adaptive Learning via Data-Centric Multimodal Explainable AI",
      "title_zh": "åŸºäºä»¥æ•°æ®ä¸ºä¸­å¿ƒçš„å¤šæ¨¡æ€å¯è§£é‡Šäººå·¥æ™ºèƒ½çš„é€æ˜åŒ–è‡ªé€‚åº”å­¦ä¹ ",
      "authors": [
        "Maryam Mosleh",
        "Marie Devlin",
        "Ellis Solaiman"
      ],
      "abstract": "Artificial intelligence-driven adaptive learning systems are reshaping education through data-driven adaptation of learning experiences. Yet many of these systems lack transparency, offering limited insight into how decisions are made. Most explainable AI (XAI) techniques focus on technical outputs but neglect user roles and comprehension. This paper proposes a hybrid framework that integrates traditional XAI techniques with generative AI models and user personalisation to generate multimodal, personalised explanations tailored to user needs. We redefine explainability as a dynamic communication process tailored to user roles and learning goals. We outline the framework's design, key XAI limitations in education, and research directions on accuracy, fairness, and personalisation. Our aim is to move towards explainable AI that enhances transparency while supporting user-centred experiences.",
      "tldr_zh": "é’ˆå¯¹äººå·¥æ™ºèƒ½é©±åŠ¨çš„è‡ªé€‚åº”å­¦ä¹ ç³»ç»Ÿåœ¨æ•™è‚²é¢†åŸŸæ™®éç¼ºä¹é€æ˜åº¦ï¼Œä¸”ç°æœ‰å¯è§£é‡Šäººå·¥æ™ºèƒ½(XAI)æŠ€æœ¯å¤šä¾§é‡æŠ€æœ¯è¾“å‡ºè€Œå¿½è§†ç”¨æˆ·è§’è‰²çš„é—®é¢˜ï¼Œè¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§æ•°æ®é©±åŠ¨çš„å¤šæ¨¡æ€æ··åˆæ¡†æ¶ã€‚è¯¥æ¡†æ¶é€šè¿‡æ•´åˆä¼ ç»Ÿ XAI æŠ€æœ¯ä¸ç”Ÿæˆå¼äººå·¥æ™ºèƒ½(Generative AI)æ¨¡å‹ï¼Œèƒ½å¤Ÿæ ¹æ®ç”¨æˆ·çš„ç‰¹å®šè§’è‰²å’Œå­¦ä¹ ç›®æ ‡ç”Ÿæˆä¸ªæ€§åŒ–çš„è§£é‡Šå†…å®¹ã€‚ç ”ç©¶è€…å°†å¯è§£é‡Šæ€§é‡æ–°å®šä¹‰ä¸ºä¸€ä¸ªåŠ¨æ€çš„æ²Ÿé€šè¿‡ç¨‹ï¼Œæ—¨åœ¨æå‡ç³»ç»Ÿå†³ç­–çš„é€æ˜åº¦å¹¶ä¼˜åŒ–ç”¨æˆ·ä½“éªŒã€‚æ–‡ä¸­ä¸ä»…è¯¦ç»†ä»‹ç»äº†æ¡†æ¶çš„è®¾è®¡ç»†èŠ‚ï¼Œè¿˜åˆ†æäº†æ•™è‚²é¢†åŸŸ XAI çš„å±€é™æ€§ï¼Œå¹¶æ¢è®¨äº†åœ¨å‡†ç¡®æ€§(Accuracy)ã€å…¬å¹³æ€§(Fairness)åŠä¸ªæ€§åŒ–(Personalisation)æ–¹é¢çš„æœªæ¥ç ”ç©¶æ–¹å‘ã€‚è¯¥é¡¹å·¥ä½œä¸ºå®ç°ä»¥ç”¨æˆ·ä¸ºä¸­å¿ƒã€å…·å¤‡é«˜é€æ˜åº¦çš„æ•™è‚²äººå·¥æ™ºèƒ½ç³»ç»Ÿå¥ å®šäº†ç†è®ºä¸å®è·µåŸºç¡€ã€‚",
      "categories": [
        "cs.AI",
        "cs.HC",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.00665v1",
      "published_date": "2025-08-01 14:36:16 UTC",
      "updated_date": "2025-08-01 14:36:16 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T08:06:47.492290+00:00"
    },
    {
      "arxiv_id": "2508.00965v1",
      "title": "VAULT: Vigilant Adversarial Updates via LLM-Driven Retrieval-Augmented Generation for NLI",
      "title_zh": "VAULTï¼šåŸºäºå¤§è¯­è¨€æ¨¡å‹é©±åŠ¨æ£€ç´¢å¢å¼ºç”Ÿæˆçš„ NLI è­¦è§‰å¼å¯¹æŠ—æ›´æ–°",
      "authors": [
        "Roie Kazoom",
        "Ofir Cohen",
        "Rami Puzis",
        "Asaf Shabtai",
        "Ofer Hadar"
      ],
      "abstract": "We introduce VAULT, a fully automated adversarial RAG pipeline that systematically uncovers and remedies weaknesses in NLI models through three stages: retrieval, adversarial generation, and iterative retraining. First, we perform balanced few-shot retrieval by embedding premises with both semantic (BGE) and lexical (BM25) similarity. Next, we assemble these contexts into LLM prompts to generate adversarial hypotheses, which are then validated by an LLM ensemble for label fidelity. Finally, the validated adversarial examples are injected back into the training set at increasing mixing ratios, progressively fortifying a zero-shot RoBERTa-base model.On standard benchmarks, VAULT elevates RoBERTa-base accuracy from 88.48% to 92.60% on SNLI +4.12%, from 75.04% to 80.95% on ANLI +5.91%, and from 54.67% to 71.99% on MultiNLI +17.32%. It also consistently outperforms prior in-context adversarial methods by up to 2.0% across datasets. By automating high-quality adversarial data curation at scale, VAULT enables rapid, human-independent robustness improvements in NLI inference tasks.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†VAULTï¼Œä¸€ç§å…¨è‡ªåŠ¨çš„å¯¹æŠ—æ€§RAGæµæ°´çº¿ï¼Œæ—¨åœ¨é€šè¿‡æ£€ç´¢ã€å¯¹æŠ—ç”Ÿæˆå’Œè¿­ä»£é‡è®­ä¸‰ä¸ªé˜¶æ®µç³»ç»Ÿæ€§åœ°å‘ç°å¹¶ä¿®å¤NLIæ¨¡å‹çš„å¼±ç‚¹ã€‚è¯¥æ¡†æ¶é¦–å…ˆç»“åˆBGEè¯­ä¹‰åµŒå…¥å’ŒBM25è¯æ³•ç›¸ä¼¼åº¦è¿›è¡Œå¹³è¡¡çš„å°‘æ ·æœ¬æ£€ç´¢ï¼Œéšååˆ©ç”¨LLMæç¤ºç”Ÿæˆå¯¹æŠ—æ€§å‡è®¾ï¼Œå¹¶é€šè¿‡LLMé›†æˆç³»ç»Ÿç¡®ä¿æ ‡ç­¾çš„ä¿çœŸåº¦ã€‚é€šè¿‡å°†éªŒè¯åçš„å¯¹æŠ—æ ·æœ¬ä»¥é€’å¢æ¯”ä¾‹æ³¨å…¥è®­ç»ƒé›†ï¼ŒVAULTæ˜¾è‘—å¢å¼ºäº†RoBERTa-baseæ¨¡å‹çš„æ€§èƒ½ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯¥æ–¹æ³•åœ¨SNLIã€ANLIå’ŒMultiNLIåŸºå‡†æµ‹è¯•ä¸­åˆ†åˆ«å®ç°äº†4.12%ã€5.91%å’Œ17.32%çš„å‡†ç¡®ç‡æå‡ã€‚VAULTåœ¨å¤šä¸ªæ•°æ®é›†ä¸Šå‡ä¸€è‡´ä¼˜äºå…ˆå‰çš„ä¸Šä¸‹æ–‡å¯¹æŠ—æ–¹æ³•ï¼Œæœ€é«˜æ€§èƒ½æå‡è¾¾2.0%ã€‚é€šè¿‡è‡ªåŠ¨åŒ–çš„å¤§è§„æ¨¡é«˜è´¨é‡å¯¹æŠ—æ•°æ®ç­–åŠ¨ï¼Œè¯¥ç ”ç©¶ä¸ºNLIæ¨ç†ä»»åŠ¡æä¾›äº†ä¸€ç§æ— éœ€äººå·¥å¹²é¢„ä¸”èƒ½å¿«é€Ÿæå‡æ¨¡å‹é²æ£’æ€§çš„æœ‰æ•ˆæ–¹æ¡ˆã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.00965v1",
      "published_date": "2025-08-01 14:22:54 UTC",
      "updated_date": "2025-08-01 14:22:54 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T08:06:52.081905+00:00"
    },
    {
      "arxiv_id": "2508.00658v1",
      "title": "Multi-Band Variable-Lag Granger Causality: A Unified Framework for Causal Time Series Inference across Frequencies",
      "title_zh": "å¤šé¢‘å¸¦å˜æ—¶æ»æ ¼å…°æ°å› æœå…³ç³»ï¼šè·¨é¢‘ç‡å› æœæ—¶é—´åºåˆ—æ¨ç†çš„ç»Ÿä¸€æ¡†æ¶",
      "authors": [
        "Chakattrai Sookkongwaree",
        "Tattep Lakmuang",
        "Chainarong Amornbunchornvej"
      ],
      "abstract": "Understanding causal relationships in time series is fundamental to many domains, including neuroscience, economics, and behavioral science. Granger causality is one of the well-known techniques for inferring causality in time series. Typically, Granger causality frameworks have a strong fix-lag assumption between cause and effect, which is often unrealistic in complex systems. While recent work on variable-lag Granger causality (VLGC) addresses this limitation by allowing a cause to influence an effect with different time lags at each time point, it fails to account for the fact that causal interactions may vary not only in time delay but also across frequency bands. For example, in brain signals, alpha-band activity may influence another region with a shorter delay than slower delta-band oscillations. In this work, we formalize Multi-Band Variable-Lag Granger Causality (MB-VLGC) and propose a novel framework that generalizes traditional VLGC by explicitly modeling frequency-dependent causal delays. We provide a formal definition of MB-VLGC, demonstrate its theoretical soundness, and propose an efficient inference pipeline. Extensive experiments across multiple domains demonstrate that our framework significantly outperforms existing methods on both synthetic and real-world datasets, confirming its broad applicability to any type of time series data. Code and datasets are publicly available.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†Multi-Band Variable-Lag Granger Causality (MB-VLGC)ï¼Œè¿™æ˜¯ä¸€ä¸ªç”¨äºè·¨é¢‘ç‡å› æœæ—¶é—´åºåˆ—æ¨æ–­çš„ç»Ÿä¸€æ¡†æ¶ã€‚ä¼ ç»Ÿçš„Granger causalityé€šå¸¸å‡è®¾å›ºå®šå»¶è¿Ÿï¼Œè€ŒVariable-Lag Granger Causality (VLGC)è™½ç„¶å…è®¸å»¶è¿Ÿéšæ—¶é—´å˜åŒ–ï¼Œå´æœªèƒ½è€ƒè™‘åˆ°å› æœäº¤äº’åœ¨ä¸åŒé¢‘ç‡æ³¢æ®µï¼ˆfrequency bandsï¼‰ä¸Šå¯èƒ½å­˜åœ¨å·®å¼‚ã€‚MB-VLGCé€šè¿‡æ˜¾å¼å»ºæ¨¡é¢‘ç‡ä¾èµ–çš„å› æœå»¶è¿Ÿï¼Œæ³›åŒ–äº†ä¼ ç»Ÿçš„VLGCæ–¹æ³•ï¼Œå¹¶æä¾›äº†ä¸€å¥—ç†è®ºå®Œå¤‡ä¸”é«˜æ•ˆçš„æ¨ç†æµæ°´çº¿ï¼ˆinference pipelineï¼‰ã€‚åœ¨åˆæˆæ•°æ®å’Œè·¨å¤šä¸ªé¢†åŸŸçš„çœŸå®ä¸–ç•Œæ•°æ®é›†ä¸Šçš„å¹¿æ³›å®éªŒè¯æ˜ï¼Œè¯¥æ¡†æ¶çš„æ€§èƒ½æ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ã€‚è¿™ä¸€æˆæœè¯æ˜äº†MB-VLGCåœ¨å¤„ç†å„ç±»å¤æ‚ç³»ç»Ÿæ—¶é—´åºåˆ—æ•°æ®æ—¶çš„å¹¿æ³›é€‚ç”¨æ€§ï¼Œä¸ºç†è§£è¯¸å¦‚è„‘ä¿¡å·æˆ–ç»æµæŒ‡æ ‡ä¸­å¤æ‚çš„è·¨é¢‘ç‡å› æœå…³ç³»æä¾›äº†æœ‰åŠ›å·¥å…·ã€‚",
      "categories": [
        "cs.AI",
        "cs.LG",
        "econ.EM",
        "stat.ME"
      ],
      "primary_category": "cs.AI",
      "comment": "First draft",
      "pdf_url": "https://arxiv.org/pdf/2508.00658v1",
      "published_date": "2025-08-01 14:22:51 UTC",
      "updated_date": "2025-08-01 14:22:51 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T08:06:58.084913+00:00"
    },
    {
      "arxiv_id": "2508.00963v1",
      "title": "Rethinking Multimodality: Optimizing Multimodal Deep Learning for Biomedical Signal Classification",
      "title_zh": "é‡æ–°å®¡è§†å¤šæ¨¡æ€ï¼šç”Ÿç‰©åŒ»å­¦ä¿¡å·åˆ†ç±»ä¸­çš„å¤šæ¨¡æ€æ·±åº¦å­¦ä¹ ä¼˜åŒ–",
      "authors": [
        "Timothy Oladunni",
        "Alex Wong"
      ],
      "abstract": "This study proposes a novel perspective on multimodal deep learning for biomedical signal classification, systematically analyzing how complementary feature domains impact model performance. While fusing multiple domains often presumes enhanced accuracy, this work demonstrates that adding modalities can yield diminishing returns, as not all fusions are inherently advantageous. To validate this, five deep learning models were designed, developed, and rigorously evaluated: three unimodal (1D-CNN for time, 2D-CNN for time-frequency, and 1D-CNN-Transformer for frequency) and two multimodal (Hybrid 1, which fuses 1D-CNN and 2D-CNN; Hybrid 2, which combines 1D-CNN, 2D-CNN, and a Transformer). For ECG classification, bootstrapping and Bayesian inference revealed that Hybrid 1 consistently outperformed the 2D-CNN baseline across all metrics (p-values < 0.05, Bayesian probabilities > 0.90), confirming the synergistic complementarity of the time and time-frequency domains. Conversely, Hybrid 2's inclusion of the frequency domain offered no further improvement and sometimes a marginal decline, indicating representational redundancy; a phenomenon further substantiated by a targeted ablation study. This research redefines a fundamental principle of multimodal design in biomedical signal analysis. We demonstrate that optimal domain fusion isn't about the number of modalities, but the quality of their inherent complementarity. This paradigm-shifting concept moves beyond purely heuristic feature selection. Our novel theoretical contribution, \"Complementary Feature Domains in Multimodal ECG Deep Learning,\" presents a mathematically quantifiable framework for identifying ideal domain combinations, demonstrating that optimal multimodal performance arises from the intrinsic information-theoretic complementarity among fused domains.",
      "tldr_zh": "è¯¥ç ”ç©¶é‡æ–°å®¡è§†äº†ç”Ÿç‰©åŒ»å­¦ä¿¡å·åˆ†ç±»ä¸­çš„å¤šæ¨¡æ€æ·±åº¦å­¦ä¹ (Multimodal Deep Learning)ï¼Œç³»ç»Ÿåˆ†æäº†äº’è¡¥ç‰¹å¾åŸŸå¯¹æ¨¡å‹æ€§èƒ½çš„å½±å“ã€‚ç ”ç©¶è®¾è®¡å¹¶è¯„ä¼°äº†ä¸‰ç§å•æ¨¡æ€æ¨¡å‹ï¼ˆé’ˆå¯¹æ—¶åŸŸçš„1D-CNNã€æ—¶é¢‘åŸŸçš„2D-CNNå’Œé¢‘åŸŸçš„1D-CNN-Transformerï¼‰ä»¥åŠä¸¤ç§å¤šæ¨¡æ€æ··åˆæ¨¡å‹ã€‚é€šè¿‡å¯¹ECGåˆ†ç±»çš„å®éªŒéªŒè¯ï¼Œç ”ç©¶å‘ç°èåˆæ—¶åŸŸä¸æ—¶é¢‘åŸŸçš„Hybrid 1æ¨¡å‹æ€§èƒ½æ˜¾è‘—ä¼˜äº2D-CNNåŸºçº¿ï¼Œè¯å®äº†è¿™ä¸¤ä¸ªé¢†åŸŸçš„ååŒäº’è¡¥æ€§ã€‚ç„¶è€Œï¼ŒHybrid 2æ¨¡å‹åœ¨å¼•å…¥é¢‘åŸŸåè¡¨ç°å‡ºè¡¨å¾å†—ä½™(Representational Redundancy)ï¼Œå¹¶æœªå¸¦æ¥è¿›ä¸€æ­¥çš„æ€§èƒ½æå‡ã€‚åŸºäºæ­¤å‘ç°ï¼Œç ”ç©¶æå‡ºäº†â€œç”Ÿç‰©åŒ»å­¦ä¿¡å·å¤šæ¨¡æ€æ·±åº¦å­¦ä¹ ä¸­çš„äº’è¡¥ç‰¹å¾åŸŸâ€ç†è®ºæ¡†æ¶ï¼Œå¼ºè°ƒæœ€ä¼˜å¤šæ¨¡æ€æ€§èƒ½å–å†³äºèåˆåŸŸä¹‹é—´å†…åœ¨çš„ä¿¡æ¯è®ºäº’è¡¥æ€§ï¼Œè€Œéå•çº¯å¢åŠ æ¨¡æ€æ•°é‡ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.00963v1",
      "published_date": "2025-08-01 14:12:10 UTC",
      "updated_date": "2025-08-01 14:12:10 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T08:06:55.889622+00:00"
    },
    {
      "arxiv_id": "2508.02725v1",
      "title": "Forecasting NCAA Basketball Outcomes with Deep Learning: A Comparative Study of LSTM and Transformer Models",
      "title_zh": "åŸºäºæ·±åº¦å­¦ä¹ çš„ NCAA ç¯®çƒèµ›ç»“æœé¢„æµ‹ï¼šLSTM ä¸ Transformer æ¨¡å‹çš„å¯¹æ¯”ç ”ç©¶",
      "authors": [
        "Md Imtiaz Habib"
      ],
      "abstract": "In this research, I explore advanced deep learning methodologies to forecast the outcomes of the 2025 NCAA Division 1 Men's and Women's Basketball tournaments. Leveraging historical NCAA game data, I implement two sophisticated sequence-based models: Long Short-Term Memory (LSTM) and Transformer architectures. The predictive power of these models is augmented through comprehensive feature engineering, including team quality metrics derived from Generalized Linear Models (GLM), Elo ratings, seed differences, and aggregated box-score statistics. To evaluate the robustness and reliability of predictions, I train each model variant using both Binary Cross-Entropy (BCE) and Brier loss functions, providing insights into classification performance and probability calibration. My comparative analysis reveals that while the Transformer architecture optimized with BCE yields superior discriminative power (highest AUC of 0.8473), the LSTM model trained with Brier loss demonstrates superior probabilistic calibration (lowest Brier score of 0.1589). These findings underscore the importance of selecting appropriate model architectures and loss functions based on the specific requirements of forecasting tasks. The detailed analytical pipeline presented here serves as a reproducible framework for future predictive modeling tasks in sports analytics and beyond.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†åˆ©ç”¨æ·±åº¦å­¦ä¹ æ–¹æ³•é¢„æµ‹ 2025 å¹´ NCAA Division 1 ç”·å¥³ç¯®é”¦æ ‡èµ›ç»“æœçš„æ–¹æ³•ï¼Œå¹¶å¯¹æ¯”å®ç°äº† Long Short-Term Memory (LSTM) å’Œ Transformer ä¸¤ç§åºåˆ—æ¨¡å‹ã€‚ç ”ç©¶é€šè¿‡é›†æˆæ¥è‡ª Generalized Linear Models (GLM) çš„çƒé˜Ÿè´¨é‡æŒ‡æ ‡ã€Elo ratingsã€ç§å­å·®å¼‚åŠæ±‡æ€»çš„ box-score statistics è¿›è¡Œäº†ç‰¹å¾å·¥ç¨‹ï¼Œå¹¶åˆ†åˆ«é‡‡ç”¨ Binary Cross-Entropy (BCE) å’Œ Brier loss å‡½æ•°è¯„ä¼°æ¨¡å‹çš„åˆ¤åˆ«èƒ½åŠ›ä¸æ¦‚ç‡æ ¡å‡†ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œç»è¿‡ BCE ä¼˜åŒ–çš„ Transformer æ¶æ„å…·æœ‰æœ€å¼ºçš„åˆ¤åˆ«èƒ½åŠ›ï¼Œå…¶ AUC è¾¾åˆ° 0.8473ï¼›è€Œé‡‡ç”¨ Brier loss è®­ç»ƒçš„ LSTM æ¨¡å‹åœ¨æ¦‚ç‡æ ¡å‡†æ–¹é¢è¡¨ç°æ›´ä¼˜ï¼ŒBrier score ä½è‡³ 0.1589ã€‚è¯¥å‘ç°å¼ºè°ƒäº†æ ¹æ®ç‰¹å®šé¢„æµ‹ä»»åŠ¡éœ€æ±‚é€‰æ‹©åˆé€‚æ¨¡å‹æ¶æ„ä¸æŸå¤±å‡½æ•°çš„é‡è¦æ€§ã€‚æ­¤ç ”ç©¶ä¸ºä½“è‚²åˆ†æé¢†åŸŸçš„é¢„æµ‹å»ºæ¨¡æä¾›äº†ä¸€å¥—å¯å¤åˆ¶çš„ç ”ç©¶æ¡†æ¶ï¼Œå±•ç¤ºäº†æ·±åº¦å­¦ä¹ åœ¨å¤„ç†å¤æ‚ç«æŠ€æ•°æ®ä¸­çš„æ½œåŠ›ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "20 page scientific report",
      "pdf_url": "https://arxiv.org/pdf/2508.02725v1",
      "published_date": "2025-08-01 14:01:44 UTC",
      "updated_date": "2025-08-01 14:01:44 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T08:07:02.285161+00:00"
    },
    {
      "arxiv_id": "2508.10916v1",
      "title": "Multimodal Quantitative Measures for Multiparty Behaviour Evaluation",
      "title_zh": "é¢å‘å¤šæ–¹è¡Œä¸ºè¯„ä¼°çš„å¤šæ¨¡æ€å®šé‡åº¦é‡",
      "authors": [
        "Ojas Shirekar",
        "Wim Pouw",
        "Chenxu Hao",
        "Vrushank Phadnis",
        "Thabo Beeler",
        "Chirag Raman"
      ],
      "abstract": "Digital humans are emerging as autonomous agents in multiparty interactions, yet existing evaluation metrics largely ignore contextual coordination dynamics. We introduce a unified, intervention-driven framework for objective assessment of multiparty social behaviour in skeletal motion data, spanning three complementary dimensions: (1) synchrony via Cross-Recurrence Quantification Analysis, (2) temporal alignment via Multiscale Empirical Mode Decompositionbased Beat Consistency, and (3) structural similarity via Soft Dynamic Time Warping. We validate metric sensitivity through three theory-driven perturbations -- gesture kinematic dampening, uniform speech-gesture delays, and prosodic pitch-variance reduction-applied to $\\approx 145$ 30-second thin slices of group interactions from the DnD dataset. Mixed-effects analyses reveal predictable, joint-independent shifts: dampening increases CRQA determinism and reduces beat consistency, delays weaken cross-participant coupling, and pitch flattening elevates F0 Soft-DTW costs. A complementary perception study ($N=27$) compares judgments of full-video and skeleton-only renderings to quantify representation effects. Our three measures deliver orthogonal insights into spatial structure, timing alignment, and behavioural variability. Thereby forming a robust toolkit for evaluating and refining socially intelligent agents. Code available on \\href{https://github.com/tapri-lab/gig-interveners}{GitHub}.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤šæ–¹äº¤äº’ä¸­æ•°å­—äººç¼ºä¹è€ƒè™‘ä¸Šä¸‹æ–‡åè°ƒåŠ¨æ€çš„è¯„ä¼°æŒ‡æ ‡é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§ç»Ÿä¸€çš„ã€åŸºäºå¹²é¢„çš„æ¡†æ¶ï¼Œç”¨äºå®¢è§‚è¯„ä¼°éª¨éª¼è¿åŠ¨æ•°æ®ä¸­çš„å¤šæ–¹ç¤¾ä¼šè¡Œä¸ºã€‚è¯¥æ¡†æ¶æ¶µç›–ä¸‰ä¸ªäº’è¡¥ç»´åº¦ï¼šé€šè¿‡äº¤å‰é€’å½’é‡åŒ–åˆ†æ(Cross-Recurrence Quantification Analysis, CRQA)è¯„ä¼°åŒæ­¥æ€§ï¼Œé€šè¿‡åŸºäºå¤šå°ºåº¦ç»éªŒæ¨¡æ€åˆ†è§£çš„èŠ‚æ‹ä¸€è‡´æ€§è¯„ä¼°æ—¶é—´å¯¹é½ï¼Œä»¥åŠé€šè¿‡è½¯åŠ¨æ€æ—¶é—´è§„æ•´(Soft-DTW)è¯„ä¼°ç»“æ„ç›¸ä¼¼æ€§ã€‚ç ”ç©¶è€…åˆ©ç”¨DnDæ•°æ®é›†ä¸­çš„çº¦145ä¸ª30ç§’ç‰‡æ®µï¼Œé€šè¿‡ä¸‰ç§ç†è®ºé©±åŠ¨çš„æ‰°åŠ¨ï¼ˆæ‰‹åŠ¿è¿åŠ¨é˜»å°¼ã€è¯­éŸ³-æ‰‹åŠ¿å»¶è¿Ÿã€éŸµå¾‹éŸ³é«˜å˜åŒ–å‡å°‘ï¼‰éªŒè¯äº†æŒ‡æ ‡çš„æ•æ„Ÿæ€§ã€‚æ··åˆæ•ˆåº”åˆ†ææ˜¾ç¤ºäº†å¯é¢„æµ‹çš„å˜åŒ–ï¼Œä¾‹å¦‚é˜»å°¼å¢åŠ äº†CRQAç¡®å®šæ€§å¹¶é™ä½äº†èŠ‚æ‹ä¸€è‡´æ€§ï¼Œè€Œå»¶è¿Ÿåˆ™å‰Šå¼±äº†å‚ä¸è€…é—´çš„è€¦åˆã€‚æ­¤å¤–ï¼Œä¸€é¡¹æ„ŸçŸ¥ç ”ç©¶å¯¹æ¯”äº†å…¨è§†é¢‘ä¸ä»…éª¨éª¼æ¸²æŸ“çš„åˆ¤æ–­ï¼Œè¯å®è¿™ä¸‰ç§æµ‹é‡æ–¹æ³•èƒ½å°±ç©ºé—´ç»“æ„ã€æ—¶é—´å¯¹é½å’Œè¡Œä¸ºå˜å¼‚æ€§æä¾›æ­£äº¤çš„è§è§£ï¼Œä¸ºè¯„ä¼°å’Œæ”¹è¿›ç¤¾ä¼šæ™ºèƒ½ä½“æä¾›äº†å¼ºå¤§çš„å·¥å…·åŒ…ã€‚",
      "categories": [
        "cs.HC",
        "cs.AI",
        "cs.CY",
        "cs.MA"
      ],
      "primary_category": "cs.HC",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.10916v1",
      "published_date": "2025-08-01 13:46:12 UTC",
      "updated_date": "2025-08-01 13:46:12 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 2,
      "last_update": "2026-01-24T08:08:16.800954+00:00"
    },
    {
      "arxiv_id": "2508.00632v1",
      "title": "Multi-Agent Game Generation and Evaluation via Audio-Visual Recordings",
      "title_zh": "åŸºäºè§†å¬è®°å½•çš„å¤šæ™ºèƒ½ä½“æ¸¸æˆç”Ÿæˆä¸è¯„ä¼°",
      "authors": [
        "Alexia Jolicoeur-Martineau"
      ],
      "abstract": "While AI excels at generating text, audio, images, and videos, creating interactive audio-visual content such as video games remains challenging. Current LLMs can generate JavaScript games and animations, but lack automated evaluation metrics and struggle with complex content that normally requires teams of humans working for many months (multi-shot, multi-agents) using assets made by artists. To tackle these issues, we built a new metric and a multi-agent system.\n  We propose AVR-Eval, a relative metric for multimedia content quality using Audio-Visual Recordings (AVRs). An omni-modal model (processing text, video, and audio) compares the AVRs of two contents, with a text model reviewing evaluations to determine superiority. We show that AVR-Eval properly identifies good from broken or mismatched content.\n  We built AVR-Agent, a multi-agent system generating JavaScript code from a bank of multimedia assets (audio, images, 3D models). The coding agent selects relevant assets, generates multiple initial codes, uses AVR-Eval to identify the best version, and iteratively improves it through omni-modal agent feedback from the AVR.\n  We run experiments on games and animations with AVR-Eval (win rate of content A against B). We find that content generated by AVR-Agent has a significantly higher win rate against content made through one-shot generation. However, models struggle to leverage custom assets and AVR feedback effectively, showing no higher win rate. This reveals a critical gap: while humans benefit from high-quality assets and audio-visual feedback, current coding models do not seem to utilize these resources as effectively, highlighting fundamental differences between human and machine content creation approaches.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹äº¤äº’å¼è§†å¬å†…å®¹ç”Ÿæˆçš„æŒ‘æˆ˜ï¼Œæå‡ºäº†AVR-Evalè¯„ä»·æŒ‡æ ‡å’ŒAVR-Agentå¤šæ™ºèƒ½ä½“ç³»ç»Ÿã€‚AVR-Evalæ˜¯ä¸€ç§åˆ©ç”¨è§†å¬å½•åƒ(Audio-Visual Recordings)çš„ç›¸å¯¹è´¨é‡åº¦é‡æ–¹æ³•ï¼Œé€šè¿‡å…¨æ¨¡æ€æ¨¡å‹(omni-modal model)ç»¼åˆå¯¹æ¯”æ–‡æœ¬ã€è§†é¢‘å’ŒéŸ³é¢‘ä»¥è¯„ä¼°å†…å®¹ä¼˜åŠ£ã€‚AVR-Agentç³»ç»Ÿåˆ™åˆ©ç”¨å¤šåª’ä½“èµ„äº§åº“ç”ŸæˆJavaScriptä»£ç ï¼Œå¹¶ç»“åˆAVR-Evalçš„åé¦ˆè¿›è¡Œè¿­ä»£ä¼˜åŒ–ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒAVR-Agentç”Ÿæˆçš„å†…å®¹åœ¨èƒœç‡ä¸Šæ˜¾è‘—ä¼˜äºä¼ ç»Ÿçš„å•æ¬¡ç”Ÿæˆ(one-shot generation)æ–¹æ³•ã€‚å°½ç®¡å¦‚æ­¤ï¼Œç ”ç©¶å‘ç°å½“å‰æ¨¡å‹åœ¨æœ‰æ•ˆåˆ©ç”¨è‡ªå®šä¹‰èµ„äº§å’Œè§†å¬åé¦ˆæ–¹é¢ä»é¢ä¸´å›°éš¾ï¼Œéš¾ä»¥è¾¾åˆ°äººç±»åˆ›ä½œè€…çš„æ°´å¹³ï¼Œè¿™æ­ç¤ºäº†æœºå™¨ä¸äººç±»åœ¨å¤æ‚å†…å®¹åˆ›ä½œæ–¹æ³•ä¸Šçš„æœ¬è´¨å·®å¼‚ã€‚",
      "categories": [
        "cs.AI",
        "cs.MA",
        "cs.MM"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.00632v1",
      "published_date": "2025-08-01 13:45:13 UTC",
      "updated_date": "2025-08-01 13:45:13 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T08:07:10.649960+00:00"
    },
    {
      "arxiv_id": "2508.00620v1",
      "title": "Backdoor Attacks on Deep Learning Face Detection",
      "title_zh": "æ·±åº¦å­¦ä¹ äººè„¸æ£€æµ‹çš„åé—¨æ”»å‡»",
      "authors": [
        "Quentin Le Roux",
        "Yannick Teglia",
        "Teddy Furon",
        "Philippe Loubet-Moundi"
      ],
      "abstract": "Face Recognition Systems that operate in unconstrained environments capture images under varying conditions,such as inconsistent lighting, or diverse face poses. These challenges require including a Face Detection module that regresses bounding boxes and landmark coordinates for proper Face Alignment. This paper shows the effectiveness of Object Generation Attacks on Face Detection, dubbed Face Generation Attacks, and demonstrates for the first time a Landmark Shift Attack that backdoors the coordinate regression task performed by face detectors. We then offer mitigations against these vulnerabilities.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†éå—é™ç¯å¢ƒä¸‹äººè„¸è¯†åˆ«ç³»ç»Ÿæ‰€ä¾èµ–çš„äººè„¸æ£€æµ‹æ¨¡å—å®‰å…¨æ€§ï¼ŒæŒ‡å‡º Face Detection åœ¨å›å½’è¾¹ç•Œæ¡†å’Œå…³é”®ç‚¹åæ ‡ä»¥å®ç° Face Alignment çš„è¿‡ç¨‹ä¸­å­˜åœ¨è¢«æ”»å‡»çš„é£é™©ã€‚è®ºæ–‡éªŒè¯äº† Object Generation Attacks åœ¨äººè„¸æ£€æµ‹ä¸­çš„æœ‰æ•ˆæ€§ï¼Œå¹¶å°†å…¶å‘½åä¸º Face Generation Attacksã€‚æ­¤å¤–ï¼Œç ”ç©¶é¦–æ¬¡æå‡ºäº†ä¸€ç§åä¸º Landmark Shift Attack çš„åé—¨æ”»å‡»æ–¹æ³•ï¼Œè¯¥æ”»å‡»ä¸“é—¨é’ˆå¯¹äººè„¸æ£€æµ‹å™¨æ‰§è¡Œçš„å…³é”®ç‚¹åæ ‡å›å½’ä»»åŠ¡ã€‚é€šè¿‡å¯¹è¿™äº›æ–°å‹æ”»å‡»æ‰‹æ®µçš„æ¼”ç¤ºï¼Œç ”ç©¶æ­ç¤ºäº†äººè„¸æ£€æµ‹æ¨¡å‹åœ¨åæ ‡å›å½’ç¯èŠ‚çš„è„†å¼±æ€§ã€‚æœ€åï¼Œä½œè€…é’ˆå¯¹è¿™äº›å®‰å…¨æ¼æ´æå‡ºäº†ç›¸åº”çš„ç¼“è§£æªæ–½ï¼ˆMitigationsï¼‰ï¼Œæ—¨åœ¨æå‡æ·±åº¦å­¦ä¹ äººè„¸æ£€æµ‹ç³»ç»Ÿçš„é²æ£’æ€§ä¸å®‰å…¨æ€§ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CR",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.00620v1",
      "published_date": "2025-08-01 13:29:26 UTC",
      "updated_date": "2025-08-01 13:29:26 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T08:07:13.743180+00:00"
    },
    {
      "arxiv_id": "2508.00961v1",
      "title": "FinKario: Event-Enhanced Automated Construction of Financial Knowledge Graph",
      "title_zh": "FinKarioï¼šäº‹ä»¶å¢å¼ºå‹é‡‘èçŸ¥è¯†å›¾è°±è‡ªåŠ¨åŒ–æ„å»º",
      "authors": [
        "Xiang Li",
        "Penglei Sun",
        "Wanyun Zhou",
        "Zikai Wei",
        "Yongqi Zhang",
        "Xiaowen Chu"
      ],
      "abstract": "Individual investors are significantly outnumbered and disadvantaged in financial markets, overwhelmed by abundant information and lacking professional analysis. Equity research reports stand out as crucial resources, offering valuable insights. By leveraging these reports, large language models (LLMs) can enhance investors' decision-making capabilities and strengthen financial analysis. However, two key challenges limit their effectiveness: (1) the rapid evolution of market events often outpaces the slow update cycles of existing knowledge bases, (2) the long-form and unstructured nature of financial reports further hinders timely and context-aware integration by LLMs. To address these challenges, we tackle both data and methodological aspects. First, we introduce the Event-Enhanced Automated Construction of Financial Knowledge Graph (FinKario), a dataset comprising over 305,360 entities, 9,625 relational triples, and 19 distinct relation types. FinKario automatically integrates real-time company fundamentals and market events through prompt-driven extraction guided by professional institutional templates, providing structured and accessible financial insights for LLMs. Additionally, we propose a Two-Stage, Graph-Based retrieval strategy (FinKario-RAG), optimizing the retrieval of evolving, large-scale financial knowledge to ensure efficient and precise data access. Extensive experiments show that FinKario with FinKario-RAG achieves superior stock trend prediction accuracy, outperforming financial LLMs by 18.81% and institutional strategies by 17.85% on average in backtesting.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† FinKarioï¼Œä¸€ç§äº‹ä»¶å¢å¼ºå‹é‡‘èçŸ¥è¯†å›¾è°±çš„è‡ªåŠ¨æ„å»ºæ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³å¤§è¯­è¨€æ¨¡å‹ (LLMs) åœ¨é‡‘èåˆ†æä¸­é¢ä¸´çš„çŸ¥è¯†æ›´æ–°æ»åä»¥åŠéç»“æ„åŒ–æŠ¥å‘Šéš¾ä»¥æœ‰æ•ˆæ•´åˆç­‰æŒ‘æˆ˜ã€‚FinKario åŒ…å«è¶…è¿‡ 305,360 ä¸ªå®ä½“å’Œ 9,625 ä¸ªå…³ç³»ä¸‰å…ƒç»„ï¼Œåˆ©ç”¨ä¸“ä¸šæœºæ„æ¨¡æ¿å¼•å¯¼çš„æç¤ºè¯é©±åŠ¨æå– (prompt-driven extraction) æŠ€æœ¯ï¼Œå®ç°äº†å…¬å¸åŸºæœ¬é¢ä¸å¸‚åœºå®æ—¶äº‹ä»¶çš„è‡ªåŠ¨é›†æˆã€‚æ­¤å¤–ï¼Œç ”ç©¶è¿˜é…å¥—æå‡ºäº†ä¸€ç§åä¸º FinKario-RAG çš„ä¸¤é˜¶æ®µå›¾æ£€ç´¢ç­–ç•¥ï¼Œæ˜¾è‘—ä¼˜åŒ–äº†å¤§è§„æ¨¡æ¼”è¿›é‡‘èçŸ¥è¯†çš„æ£€ç´¢æ•ˆç‡ä¸ç²¾åº¦ã€‚å®éªŒè¯æ˜ï¼Œè¯¥æ¡†æ¶åœ¨è‚¡ç¥¨è¶‹åŠ¿é¢„æµ‹å‡†ç¡®ç‡ä¸Šæ˜¾è‘—ä¼˜äºç°æœ‰åŸºçº¿ï¼Œå›æµ‹è¡¨ç°å¹³å‡æ¯”é‡‘è LLMs æå‡ 18.81%ï¼Œæ¯”ä¸“ä¸šæœºæ„ç­–ç•¥æå‡ 17.85%ï¼Œä¸ºæå‡ä¸ªäººæŠ•èµ„è€…çš„å†³ç­–åˆ†æèƒ½åŠ›æä¾›äº†æœ‰æ•ˆçš„ç»“æ„åŒ–æ•°æ®æ”¯æŒã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.00961v1",
      "published_date": "2025-08-01 13:27:35 UTC",
      "updated_date": "2025-08-01 13:27:35 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T08:07:18.088216+00:00"
    },
    {
      "arxiv_id": "2508.00615v1",
      "title": "Similarity-Based Self-Construct Graph Model for Predicting Patient Criticalness Using Graph Neural Networks and EHR Data",
      "title_zh": "åŸºäºå›¾ç¥ç»ç½‘ç»œä¸EHRæ•°æ®é¢„æµ‹æ‚£è€…å±é‡ç¨‹åº¦çš„ç›¸ä¼¼æ€§è‡ªæ„å»ºå›¾æ¨¡å‹",
      "authors": [
        "Mukesh Kumar Sahu",
        "Pinki Roy"
      ],
      "abstract": "Accurately predicting the criticalness of ICU patients (such as in-ICU mortality risk) is vital for early intervention in critical care. However, conventional models often treat each patient in isolation and struggle to exploit the relational structure in Electronic Health Records (EHR). We propose a Similarity-Based Self-Construct Graph Model (SBSCGM) that dynamically builds a patient similarity graph from multi-modal EHR data, and a HybridGraphMedGNN architecture that operates on this graph to predict patient mortality and a continuous criticalness score. SBSCGM uses a hybrid similarity measure (combining feature-based and structural similarities) to connect patients with analogous clinical profiles in real-time. The HybridGraphMedGNN integrates Graph Convolutional Network (GCN), GraphSAGE, and Graph Attention Network (GAT) layers to learn robust patient representations, leveraging both local and global graph patterns. In experiments on 6,000 ICU stays from the MIMIC-III dataset, our model achieves state-of-the-art performance (AUC-ROC $0.94$) outperforming baseline classifiers and single-type GNN models. We also demonstrate improved precision/recall and show that the attention mechanism provides interpretable insights into model predictions. Our framework offers a scalable and interpretable solution for critical care risk prediction, with potential to support clinicians in real-world ICU deployment.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ä¼ ç»Ÿæ¨¡å‹åœ¨é¢„æµ‹ICUæ‚£è€…å±é‡ç¨‹åº¦æ—¶å­¤ç«‹å¤„ç†æ‚£è€…ã€éš¾ä»¥åˆ©ç”¨ç”µå­å¥åº·è®°å½•(EHR)å…³è”ç»“æ„çš„é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§åŸºäºç›¸ä¼¼æ€§çš„è‡ªæ„å»ºå›¾æ¨¡å‹(SBSCGM)ã€‚è¯¥æ¨¡å‹åˆ©ç”¨æ··åˆç›¸ä¼¼åº¦åº¦é‡ï¼ˆç»“åˆç‰¹å¾å’Œç»“æ„ç›¸ä¼¼æ€§ï¼‰ä»å¤šæ¨¡æ€EHRæ•°æ®ä¸­åŠ¨æ€æ„å»ºæ‚£è€…ç›¸ä¼¼åº¦å›¾ï¼Œå¹¶é…åˆHybridGraphMedGNNæ¶æ„è¿›è¡Œé£é™©é¢„æµ‹ã€‚HybridGraphMedGNNé›†æˆäº†å›¾å·ç§¯ç½‘ç»œ(GCN)ã€GraphSAGEå’Œå›¾æ³¨æ„åŠ›ç½‘ç»œ(GAT)ï¼Œé€šè¿‡æ•æ‰å±€éƒ¨å’Œå…¨å±€å›¾æ¨¡å¼æ¥å­¦ä¹ é²æ£’çš„æ‚£è€…è¡¨ç¤ºã€‚åœ¨MIMIC-IIIæ•°æ®é›†ä¸Šçš„å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯¥æ¨¡å‹è¾¾åˆ°äº†0.94çš„AUC-ROCï¼Œæ˜¾è‘—ä¼˜äºåŸºçº¿åˆ†ç±»å™¨å’Œå•ä¸€ç±»å‹çš„GNNæ¨¡å‹ã€‚æ­¤å¤–ï¼Œè¯¥æ¡†æ¶é€šè¿‡æ³¨æ„åŠ›æœºåˆ¶æä¾›äº†å…·æœ‰è§£é‡Šæ€§çš„é¢„æµ‹è§è§£ï¼Œä¸ºä¸´åºŠç¯å¢ƒä¸‹çš„å±é‡ç—‡é£é™©è¯„ä¼°æä¾›äº†ä¸€ç§å¯æ‰©å±•ä¸”é€æ˜çš„å†³ç­–æ”¯æŒæ–¹æ¡ˆã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.00615v1",
      "published_date": "2025-08-01 13:25:04 UTC",
      "updated_date": "2025-08-01 13:25:04 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T08:07:19.449913+00:00"
    },
    {
      "arxiv_id": "2508.00614v1",
      "title": "Prompting Science Report 3: I'll pay you or I'll kill you -- but will you care?",
      "title_zh": "æç¤ºç§‘å­¦æŠ¥å‘Š 3ï¼šé‡èµæˆ–å¨èƒâ€”â€”AI çœŸçš„ä¼šåœ¨æ„å—ï¼Ÿ",
      "authors": [
        "Lennart Meincke",
        "Ethan Mollick",
        "Lilach Mollick",
        "Dan Shapiro"
      ],
      "abstract": "This is the third in a series of short reports that seek to help business, education, and policy leaders understand the technical details of working with AI through rigorous testing. In this report, we investigate two commonly held prompting beliefs: a) offering to tip the AI model and b) threatening the AI model. Tipping was a commonly shared tactic for improving AI performance and threats have been endorsed by Google Founder Sergey Brin (All-In, May 2025, 8:20) who observed that 'models tend to do better if you threaten them,' a claim we subject to empirical testing here. We evaluate model performance on GPQA (Rein et al. 2024) and MMLU-Pro (Wang et al. 2024).\n  We demonstrate two things:\n  - Threatening or tipping a model generally has no significant effect on benchmark performance.\n  - Prompt variations can significantly affect performance on a per-question level. However, it is hard to know in advance whether a particular prompting approach will help or harm the LLM's ability to answer any particular question.\n  Taken together, this suggests that simple prompting variations might not be as effective as previously assumed, especially for difficult problems. However, as reported previously (Meincke et al. 2025a), prompting approaches can yield significantly different results for individual questions.",
      "tldr_zh": "è¯¥ç ”ç©¶æ˜¯Prompting Scienceç³»åˆ—æŠ¥å‘Šçš„ç¬¬ä¸‰ç¯‡ï¼Œæ—¨åœ¨é€šè¿‡GPQAå’ŒMMLU-ProåŸºå‡†æµ‹è¯•ï¼Œå®è¯ç ”ç©¶â€œç»™AIæ¨¡å‹æ‰“èµ(Tipping)â€æˆ–â€œå¨èƒæ¨¡å‹(Threatening)â€è¿™ä¸¤ç§å¸¸è§Promptingç­–ç•¥çš„æœ‰æ•ˆæ€§ã€‚ç ”ç©¶èƒŒæ™¯æºäºç¤¾åŒºä¸­æµä¼ çš„æ€§èƒ½æå‡æŠ€å·§ï¼Œä»¥åŠGoogleåˆ›å§‹äººSergey Brinæå‡ºçš„â€œå¨èƒæ¨¡å‹èƒ½ä½¿å…¶è¡¨ç°æ›´å¥½â€çš„è§‚ç‚¹ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œåœ¨æ•´ä½“åŸºå‡†æµ‹è¯•è¡¨ç°ä¸Šï¼Œå¨èƒæˆ–æ‰“èµæ¨¡å‹é€šå¸¸ä¸ä¼šäº§ç”Ÿç»Ÿè®¡å­¦ä¸Šçš„æ˜¾è‘—å½±å“ã€‚è™½ç„¶æç¤ºè¯çš„ç»†å¾®å˜ä½“åœ¨å•ä¸ªé—®é¢˜å±‚é¢(per-question level)å¯èƒ½æ˜¾è‘—æ”¹å˜æ¨¡å‹è¾“å‡ºï¼Œä½†ç›®å‰å¾ˆéš¾é¢„çŸ¥ç‰¹å®šç­–ç•¥å¯¹æŸä¸€å…·ä½“é—®é¢˜çš„æ­£é¢æˆ–è´Ÿé¢ä½œç”¨ã€‚è¿™è¡¨æ˜å¯¹äºé«˜éš¾åº¦çš„å¤æ‚é—®é¢˜ï¼Œç®€å•çš„æç¤ºè¯å˜ä½“ç­–ç•¥å¯èƒ½å¹¶ä¸å¦‚ä»¥å¾€å‡è®¾çš„é‚£æ ·æœ‰æ•ˆã€‚ç ”ç©¶ç»“è®ºå¼ºè°ƒäº†æç¤ºè¯æ•ˆæœåœ¨ä¸ªä½“é—®é¢˜ä¸Šçš„é«˜åº¦å·®å¼‚æ€§ï¼Œæ­ç¤ºäº†æƒ…ç»ªåŒ–æç¤ºè¯ç­–ç•¥åœ¨æå‡LLMæ€§èƒ½æ–¹é¢çš„å±€é™æ€§ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.00614v1",
      "published_date": "2025-08-01 13:23:21 UTC",
      "updated_date": "2025-08-01 13:23:21 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T08:08:55.335176+00:00"
    },
    {
      "arxiv_id": "2508.08283v1",
      "title": "MinionsLLM: a Task-adaptive Framework For The Training and Control of Multi-Agent Systems Through Natural Language",
      "title_zh": "MinionsLLMï¼šä¸€ç§é€šè¿‡è‡ªç„¶è¯­è¨€è®­ç»ƒä¸æ§åˆ¶å¤šæ™ºèƒ½ä½“ç³»ç»Ÿçš„ä»»åŠ¡è‡ªé€‚åº”æ¡†æ¶",
      "authors": [
        "Andres Garcia Rincon",
        "Eliseo Ferrante"
      ],
      "abstract": "This paper presents MinionsLLM, a novel framework that integrates Large Language Models (LLMs) with Behavior Trees (BTs) and Formal Grammars to enable natural language control of multi-agent systems within arbitrary, user-defined environments. MinionsLLM provides standardized interfaces for defining environments, agents, and behavioral primitives, and introduces two synthetic dataset generation methods (Method A and Method B) to fine-tune LLMs for improved syntactic validity and semantic task relevance. We validate our approach using Google's Gemma 3 model family at three parameter scales (1B, 4B, and 12B) and demonstrate substantial gains: Method B increases syntactic validity to 92.6% and achieves a mean task performance improvement of 33% over baseline. Notably, our experiments show that smaller models benefit most from fine-tuning, suggesting promising directions for deploying compact, locally hosted LLMs in resource-constrained multi-agent control scenarios. The framework and all resources are released open-source to support reproducibility and future research.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† MinionsLLMï¼Œè¿™æ˜¯ä¸€ä¸ªå°†å¤§è¯­è¨€æ¨¡å‹ (LLMs) ä¸è¡Œä¸ºæ ‘ (Behavior Trees) ä»¥åŠå½¢å¼è¯­æ³• (Formal Grammars) ç›¸ç»“åˆçš„åˆ›æ–°æ¡†æ¶ï¼Œæ—¨åœ¨é€šè¿‡è‡ªç„¶è¯­è¨€å®ç°å¯¹ä»»æ„ç”¨æˆ·å®šä¹‰ç¯å¢ƒä¸‹çš„å¤šæ™ºèƒ½ä½“ç³»ç»Ÿ (Multi-Agent Systems) çš„æ§åˆ¶ã€‚è¯¥æ¡†æ¶ä¸ºå®šä¹‰ç¯å¢ƒã€æ™ºèƒ½ä½“å’Œè¡Œä¸ºåŸè¯­æä¾›äº†æ ‡å‡†åŒ–æ¥å£ï¼Œå¹¶å¼•å…¥äº†ä¸¤ç§åˆæˆæ•°æ®é›†ç”Ÿæˆæ–¹æ³•ï¼ˆMethod A å’Œ Method Bï¼‰æ¥å¾®è°ƒ LLMsï¼Œä»¥æ˜¾è‘—æå‡å…¶ç”Ÿæˆçš„è¯­æ³•æœ‰æ•ˆæ€§å’Œè¯­ä¹‰ä»»åŠ¡ç›¸å…³æ€§ã€‚ç ”ç©¶å›¢é˜Ÿåœ¨ Google çš„ Gemma 3 ä¸åŒå‚æ•°è§„æ¨¡ï¼ˆ1Bã€4B å’Œ 12Bï¼‰çš„æ¨¡å‹ä¸Šè¿›è¡Œäº†éªŒè¯ï¼Œç»“æœæ˜¾ç¤º Method B å°†è¯­æ³•æœ‰æ•ˆæ€§æé«˜è‡³ 92.6%ï¼Œä¸”ä»»åŠ¡å¹³å‡æ€§èƒ½æ¯”åŸºçº¿æå‡äº† 33%ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¾ƒå°è§„æ¨¡çš„æ¨¡å‹ä»å¾®è°ƒä¸­è·ç›Šæœ€ä¸ºæ˜¾è‘—ï¼Œä¸ºåœ¨èµ„æºå—é™çš„ç¯å¢ƒä¸­éƒ¨ç½²æœ¬åœ°ç´§å‡‘å‹ LLMs è¿›è¡Œå¤šæ™ºèƒ½ä½“æ§åˆ¶æä¾›äº†å¯è¡Œè·¯å¾„ã€‚ç›®å‰è¯¥æ¡†æ¶åŠç›¸å…³èµ„æºå·²å…¨éƒ¨å¼€æºï¼Œæ—¨åœ¨ä¿ƒè¿›å¤šæ™ºèƒ½ä½“ç³»ç»Ÿç ”ç©¶çš„é‡ç°ä¸å‘å±•ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG",
        "cs.MA",
        "cs.RO"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.08283v1",
      "published_date": "2025-08-01 13:10:29 UTC",
      "updated_date": "2025-08-01 13:10:29 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T08:08:34.841368+00:00"
    },
    {
      "arxiv_id": "2508.00604v1",
      "title": "Composable OS Kernel Architectures for Autonomous Intelligence",
      "title_zh": "é¢å‘è‡ªä¸»æ™ºèƒ½çš„å¯ç»„åˆå¼æ“ä½œç³»ç»Ÿå†…æ ¸æ¶æ„",
      "authors": [
        "Rajpreet Singh",
        "Vidhi Kothari"
      ],
      "abstract": "As intelligent systems permeate edge devices, cloud infrastructure, and embedded real-time environments, this research proposes a new OS kernel architecture for intelligent systems, transforming kernels from static resource managers to adaptive, AI-integrated platforms. Key contributions include: (1) treating Loadable Kernel Modules (LKMs) as AI-oriented computation units for fast sensory and cognitive processing in kernel space; (2) expanding the Linux kernel into an AI-native environment with built-in deep learning inference, floating-point acceleration, and real-time adaptive scheduling for efficient ML workloads; and (3) introducing a Neurosymbolic kernel design leveraging Category Theory and Homotopy Type Theory to unify symbolic reasoning and differentiable logic within OS internals. Together, these approaches enable operating systems to proactively anticipate and adapt to the cognitive needs of autonomous intelligent applications.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹æ™ºèƒ½ç³»ç»Ÿåœ¨è¾¹ç¼˜ä¸äº‘ç«¯å¹¿æ³›åº”ç”¨çš„éœ€æ±‚ï¼Œæå‡ºäº†ä¸€ç§é¢å‘è‡ªä¸»æ™ºèƒ½çš„å¯ç»„åˆæ“ä½œç³»ç»Ÿå†…æ ¸æ¶æ„ï¼Œæ—¨åœ¨å°†å†…æ ¸ä»é™æ€èµ„æºç®¡ç†å™¨è½¬å˜ä¸ºè‡ªé€‚åº”çš„ AI-integrated å¹³å°ã€‚å…¶ä¸»è¦è´¡çŒ®åœ¨äºå°† Loadable Kernel Modules (LKMs) è½¬åŒ–ä¸ºé¢å‘ AI çš„è®¡ç®—å•å…ƒï¼Œå®ç°äº†å†…æ ¸ç©ºé—´å†…çš„å¿«é€Ÿæ„Ÿå®˜ä¸è®¤çŸ¥å¤„ç†ã€‚ç ”ç©¶é€šè¿‡åœ¨ Linux å†…æ ¸ä¸­é›†æˆæ·±åº¦å­¦ä¹ æ¨ç†ã€æµ®ç‚¹åŠ é€ŸåŠå®æ—¶è‡ªé€‚åº”è°ƒåº¦ï¼Œæ„å»ºäº†ä¸€ä¸ªæ”¯æŒé«˜æ•ˆ ML å·¥ä½œè´Ÿè½½çš„ AI-native ç¯å¢ƒã€‚æ­¤å¤–ï¼Œè¯¥æ¶æ„å¼•å…¥äº†åŸºäº Category Theory å’Œ Homotopy Type Theory çš„ Neurosymbolic å†…æ ¸è®¾è®¡ï¼Œåœ¨ç³»ç»Ÿå†…éƒ¨å®ç°äº†ç¬¦å·æ¨ç†ä¸å¯å¾®é€»è¾‘çš„ç»Ÿä¸€ã€‚è¿™äº›æŠ€æœ¯çªç ´ä½¿æ“ä½œç³»ç»Ÿèƒ½å¤Ÿä¸»åŠ¨é¢„æµ‹å¹¶è‡ªé€‚åº”åœ°æ»¡è¶³è‡ªä¸»æ™ºèƒ½åº”ç”¨æ—¥ç›Šå¤æ‚çš„è®¤çŸ¥éœ€æ±‚ã€‚",
      "categories": [
        "cs.OS",
        "cs.AI"
      ],
      "primary_category": "cs.OS",
      "comment": "8 pages",
      "pdf_url": "https://arxiv.org/pdf/2508.00604v1",
      "published_date": "2025-08-01 13:07:16 UTC",
      "updated_date": "2025-08-01 13:07:16 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T08:08:38.543736+00:00"
    },
    {
      "arxiv_id": "2508.00602v1",
      "title": "LeakSealer: A Semisupervised Defense for LLMs Against Prompt Injection and Leakage Attacks",
      "title_zh": "LeakSealerï¼šé’ˆå¯¹å¤§è¯­è¨€æ¨¡å‹æç¤ºè¯æ³¨å…¥ä¸æ³„éœ²æ”»å‡»çš„åŠç›‘ç£é˜²å¾¡æ–¹æ¡ˆ",
      "authors": [
        "Francesco Panebianco",
        "Stefano Bonfanti",
        "Francesco TrovÃ²",
        "Michele Carminati"
      ],
      "abstract": "The generalization capabilities of Large Language Models (LLMs) have led to their widespread deployment across various applications. However, this increased adoption has introduced several security threats, notably in the forms of jailbreaking and data leakage attacks. Additionally, Retrieval Augmented Generation (RAG), while enhancing context-awareness in LLM responses, has inadvertently introduced vulnerabilities that can result in the leakage of sensitive information. Our contributions are twofold. First, we introduce a methodology to analyze historical interaction data from an LLM system, enabling the generation of usage maps categorized by topics (including adversarial interactions). This approach further provides forensic insights for tracking the evolution of jailbreaking attack patterns. Second, we propose LeakSealer, a model-agnostic framework that combines static analysis for forensic insights with dynamic defenses in a Human-In-The-Loop (HITL) pipeline. This technique identifies topic groups and detects anomalous patterns, allowing for proactive defense mechanisms. We empirically evaluate LeakSealer under two scenarios: (1) jailbreak attempts, employing a public benchmark dataset, and (2) PII leakage, supported by a curated dataset of labeled LLM interactions. In the static setting, LeakSealer achieves the highest precision and recall on the ToxicChat dataset when identifying prompt injection. In the dynamic setting, PII leakage detection achieves an AUPRC of $0.97$, significantly outperforming baselines such as Llama Guard.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤§è¯­è¨€æ¨¡å‹(LLMs)é¢ä¸´çš„æç¤ºæ³¨å…¥(Prompt Injection)ã€è¶Šç‹±(jailbreaking)ä»¥åŠæ£€ç´¢å¢å¼ºç”Ÿæˆ(RAG)å¼•èµ·çš„æ•°æ®æ³„éœ²ç­‰å®‰å…¨å¨èƒï¼Œæå‡ºäº†LeakSealeré˜²å¾¡æ¡†æ¶ã€‚è¯¥æ¡†æ¶å¼•å…¥äº†ä¸€ç§åˆ†æå†å²äº¤äº’æ•°æ®çš„æ–¹æ³•ï¼Œé€šè¿‡ç”ŸæˆæŒ‰ä¸»é¢˜åˆ†ç±»çš„ä½¿ç”¨å›¾è°±(usage maps)æ¥æä¾›å–è¯è§è§£ï¼Œä»è€Œè¿½è¸ªæ”»å‡»æ¨¡å¼çš„æ¼”å˜ã€‚LeakSealeré‡‡ç”¨äº†æ¨¡å‹æ— å…³(model-agnostic)çš„è®¾è®¡ï¼Œå°†é™æ€åˆ†æä¸åŠ¨æ€é˜²å¾¡ç›¸ç»“åˆï¼Œå¹¶æ•´åˆå…¥äººæœºååŒ(Human-In-The-Loop, HITL)æµæ°´çº¿ã€‚é€šè¿‡è¯†åˆ«ä¸»é¢˜ç»„å’Œæ£€æµ‹å¼‚å¸¸æ¨¡å¼ï¼Œè¯¥ç³»ç»Ÿèƒ½å¤Ÿå®ç°ä¸»åŠ¨é˜²å¾¡æœºåˆ¶ã€‚å®éªŒè¯„ä¼°è¡¨æ˜ï¼ŒLeakSealeråœ¨ToxicChatæ•°æ®é›†ä¸Šçš„æç¤ºæ³¨å…¥è¯†åˆ«ä¸­è¾¾åˆ°äº†æé«˜çš„ç²¾ç¡®ç‡å’Œå¬å›ç‡ã€‚åœ¨åŠ¨æ€è®¾ç½®ä¸‹ï¼Œå…¶ä¸ªäººèº«ä»½ä¿¡æ¯(PII)æ³„éœ²æ£€æµ‹çš„AUPRCè¾¾åˆ°0.97ï¼Œè¡¨ç°æ˜¾è‘—ä¼˜äºLlama Guardç­‰åŸºçº¿æ¨¡å‹ã€‚",
      "categories": [
        "cs.CR",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CR",
      "comment": "22 pages, preprint",
      "pdf_url": "https://arxiv.org/pdf/2508.00602v1",
      "published_date": "2025-08-01 13:04:28 UTC",
      "updated_date": "2025-08-01 13:04:28 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T08:08:42.447943+00:00"
    },
    {
      "arxiv_id": "2508.00960v1",
      "title": "Compression-Induced Communication-Efficient Large Model Training and Inferencing",
      "title_zh": "å‹ç¼©é©±åŠ¨çš„é«˜æ•ˆé€šä¿¡å¤§æ¨¡å‹è®­ç»ƒä¸æ¨ç†",
      "authors": [
        "Sudip K. Seal",
        "Maksudul Alam",
        "Jorge Ramirez",
        "Sajal Dash",
        "Hao Lu"
      ],
      "abstract": "Energy efficiency of training and inferencing with large neural network models is a critical challenge facing the future of sustainable large-scale machine learning workloads. This paper introduces an alternative strategy, called phantom parallelism, to minimize the net energy consumption of traditional tensor (model) parallelism, the most energy-inefficient component of large neural network training. The approach is presented in the context of feed-forward network architectures as a preliminary, but comprehensive, proof-of-principle study of the proposed methodology. We derive new forward and backward propagation operators for phantom parallelism, implement them as custom autograd operations within an end-to-end phantom parallel training pipeline and compare its parallel performance and energy-efficiency against those of conventional tensor parallel training pipelines. Formal analyses that predict lower bandwidth and FLOP counts are presented with supporting empirical results on up to 256 GPUs that corroborate these gains. Experiments are shown to deliver ~50% reduction in the energy consumed to train FFNs using the proposed phantom parallel approach when compared with conventional tensor parallel methods. Additionally, the proposed approach is shown to train smaller phantom models to the same model loss on smaller GPU counts as larger tensor parallel models on larger GPU counts offering the possibility for even greater energy savings.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤§è§„æ¨¡ç¥ç»ç½‘ç»œè®­ç»ƒå’Œæ¨ç†ä¸­çš„èƒ½æºæ•ˆç‡æŒ‘æˆ˜ï¼Œæå‡ºäº†ä¸€ç§åä¸º phantom parallelism çš„æ›¿ä»£ç­–ç•¥ï¼Œæ—¨åœ¨ä¼˜åŒ–ä¼ ç»Ÿ tensor parallelism ä¸­é«˜èƒ½æ•ˆæˆæœ¬çš„é€šä¿¡ç¯èŠ‚ã€‚ä½œè€…ä»¥ feed-forward network (FFN) æ¶æ„ä¸ºç ”ç©¶èƒŒæ™¯ï¼Œæ¨å¯¼äº†é€‚ç”¨äºè¯¥ç­–ç•¥çš„æ–°å‹å‰å‘ä¸åå‘ä¼ æ’­ç®—å­ï¼Œå¹¶å°†å…¶ä½œä¸ºè‡ªå®šä¹‰ autograd æ“ä½œé›†æˆåˆ°ç«¯åˆ°ç«¯çš„è®­ç»ƒæµæ°´çº¿ä¸­ã€‚ç†è®ºåˆ†æä¸åœ¨å¤šè¾¾ 256 ä¸ª GPU ä¸Šçš„å®éªŒç»“æœå‡è¡¨æ˜ï¼Œè¯¥æ–¹æ³•èƒ½æ˜¾è‘—é™ä½å¸¦å®½å ç”¨å’Œ FLOPs è®¡ç®—é‡ã€‚å®éªŒæ•°æ®æ˜¾ç¤ºï¼Œä¸ä¼ ç»Ÿçš„ tensor parallel æ–¹æ³•ç›¸æ¯”ï¼Œphantom parallelism åœ¨è®­ç»ƒ FFN æ—¶å¯å‡å°‘çº¦ 50% çš„èƒ½é‡æ¶ˆè€—ã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•è¿˜å±•ç°å‡ºåœ¨æ›´å°‘çš„ GPU èµ„æºä¸‹ä½¿è¾ƒå°çš„ phantom models è¾¾åˆ°ä¸å¤§è§„æ¨¡å¼ é‡å¹¶è¡Œæ¨¡å‹åŒç­‰è®­ç»ƒæŸå¤± (model loss) çš„æ½œåŠ›ï¼Œä¸ºå®ç°å¯æŒç»­çš„å¤§è§„æ¨¡æœºå™¨å­¦ä¹ æä¾›äº†æ›´é«˜æ•ˆçš„èŠ‚èƒ½æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.DC"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.00960v1",
      "published_date": "2025-08-01 12:51:40 UTC",
      "updated_date": "2025-08-01 12:51:40 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T08:08:45.843244+00:00"
    },
    {
      "arxiv_id": "2508.00591v2",
      "title": "Wukong Framework for Not Safe For Work Detection in Text-to-Image systems",
      "title_zh": "Wukongï¼šæ–‡ç”Ÿå›¾ç³»ç»Ÿä¸­çš„ NSFW å†…å®¹æ£€æµ‹æ¡†æ¶",
      "authors": [
        "Mingrui Liu",
        "Sixiao Zhang",
        "Cheng Long"
      ],
      "abstract": "Text-to-Image (T2I) generation is a popular AI-generated content (AIGC) technology enabling diverse and creative image synthesis. However, some outputs may contain Not Safe For Work (NSFW) content (e.g., violence), violating community guidelines. Detecting NSFW content efficiently and accurately, known as external safeguarding, is essential. Existing external safeguards fall into two types: text filters, which analyze user prompts but overlook T2I model-specific variations and are prone to adversarial attacks; and image filters, which analyze final generated images but are computationally costly and introduce latency. Diffusion models, the foundation of modern T2I systems like Stable Diffusion, generate images through iterative denoising using a U-Net architecture with ResNet and Transformer blocks. We observe that: (1) early denoising steps define the semantic layout of the image, and (2) cross-attention layers in U-Net are crucial for aligning text and image regions. Based on these insights, we propose Wukong, a transformer-based NSFW detection framework that leverages intermediate outputs from early denoising steps and reuses U-Net's pre-trained cross-attention parameters. Wukong operates within the diffusion process, enabling early detection without waiting for full image generation. We also introduce a new dataset containing prompts, seeds, and image-specific NSFW labels, and evaluate Wukong on this and two public benchmarks. Results show that Wukong significantly outperforms text-based safeguards and achieves comparable accuracy of image filters, while offering much greater efficiency.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†Wukongï¼Œä¸€ä¸ªåŸºäºTransformerçš„Not Safe For Work (NSFW)æ£€æµ‹æ¡†æ¶ï¼Œæ—¨åœ¨æå‡æ–‡æœ¬åˆ°å›¾åƒ(Text-to-Image, T2I)ç³»ç»Ÿçš„å®‰å…¨æ€§ã€‚é’ˆå¯¹ç°æœ‰æ–‡æœ¬è¿‡æ»¤å™¨æ˜“å—æ”»å‡»ä¸”å›¾åƒè¿‡æ»¤å™¨å»¶è¿Ÿè¾ƒé«˜çš„é—®é¢˜ï¼ŒWukongé€šè¿‡åˆ©ç”¨æ‰©æ•£æ¨¡å‹æ—©æœŸå»å™ªæ­¥éª¤(early denoising steps)ä¸­çš„ä¸­é—´è¾“å‡ºï¼Œå¹¶é‡ç”¨U-Neté¢„è®­ç»ƒçš„äº¤å‰æ³¨æ„åŠ›å±‚(cross-attention layers)å‚æ•°æ¥å®ç°æ£€æµ‹ã€‚è¿™ç§æ–¹æ³•å…è®¸åœ¨æ‰©æ•£è¿‡ç¨‹ä¸­è¿›è¡Œæ—©æœŸè¯†åˆ«ï¼Œæ— éœ€ç­‰å¾…å®Œæ•´å›¾åƒç”Ÿæˆï¼Œä»è€Œåœ¨ä¿è¯æ£€æµ‹ç²¾åº¦çš„åŒæ—¶å¤§å¹…é™ä½äº†è®¡ç®—å»¶è¿Ÿã€‚æ­¤å¤–ï¼Œç ”ç©¶å›¢é˜Ÿè¿˜å¼•å…¥äº†ä¸€ä¸ªåŒ…å«æç¤ºè¯ã€ç§å­å’Œç‰¹å®šæ ‡ç­¾çš„æ–°å‹æ•°æ®é›†ç”¨äºåŸºå‡†æµ‹è¯•ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒWukongåœ¨å‡†ç¡®ç‡ä¸Šæ˜¾è‘—ä¼˜äºæ–‡æœ¬è¿‡æ»¤å™¨ï¼Œå¹¶è¾¾åˆ°äº†ä¸å›¾åƒè¿‡æ»¤å™¨ç›¸å½“çš„æ€§èƒ½ï¼Œä¸”åœ¨è¿è¡Œæ•ˆç‡ä¸Šå…·æœ‰æ˜æ˜¾ä¼˜åŠ¿ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CR"
      ],
      "primary_category": "cs.CV",
      "comment": "Accepted by KDD'26 (round 1)",
      "pdf_url": "https://arxiv.org/pdf/2508.00591v2",
      "published_date": "2025-08-01 12:45:30 UTC",
      "updated_date": "2026-01-18 08:14:17 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T08:09:06.246751+00:00"
    },
    {
      "arxiv_id": "2508.00959v2",
      "title": "Enhancing material behavior discovery using embedding-oriented Physically-Guided Neural Networks with Internal Variables",
      "title_zh": "åˆ©ç”¨é¢å‘åµŒå…¥çš„å«å†…å˜é‡ç‰©ç†å¼•å¯¼ç¥ç»ç½‘ç»œå¢å¼ºææ–™è¡Œä¸ºå‘ç°",
      "authors": [
        "RubÃ©n MuÃ±oz-Sierra",
        "Manuel DoblarÃ©",
        "Jacobo Ayensa-JimÃ©nez"
      ],
      "abstract": "Physically Guided Neural Networks with Internal Variables are SciML tools that use only observable data for training and and have the capacity to unravel internal state relations. They incorporate physical knowledge both by prescribing the model architecture and using loss regularization, thus endowing certain specific neurons with a physical meaning as internal state variables. Despite their potential, these models face challenges in scalability when applied to high-dimensional data such as fine-grid spatial fields or time-evolving systems. In this work, we propose some enhancements to the PGNNIV framework that address these scalability limitations through reduced-order modeling techniques. Specifically, we introduce alternatives to the original decoder structure using spectral decomposition, POD, and pretrained autoencoder-based mappings. These surrogate decoders offer varying trade-offs between computational efficiency, accuracy, noise tolerance, and generalization, while improving drastically the scalability. Additionally, we integrate model reuse via transfer learning and fine-tuning strategies to exploit previously acquired knowledge, supporting efficient adaptation to novel materials or configurations, and significantly reducing training time while maintaining or improving model performance. To illustrate these various techniques, we use a representative case governed by the nonlinear diffusion equation, using only observable data. Results demonstrate that the enhanced PGNNIV framework successfully identifies the underlying constitutive state equations while maintaining high predictive accuracy. It also improves robustness to noise, mitigates overfitting, and reduces computational demands. The proposed techniques can be tailored to various scenarios depending on data availability, resources, and specific modeling objectives, overcoming scalability challenges in all the scenarios.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ç‰©ç†å¼•å¯¼å†…å˜é‡ç¥ç»ç½‘ç»œ (PGNNIV) åœ¨å¤„ç†é«˜ç»´æ•°æ®æ—¶é¢ä¸´çš„æ‰©å±•æ€§ (scalability) æŒ‘æˆ˜ï¼Œæå‡ºäº†ä¸€ç§åŸºäºé™é˜¶å»ºæ¨¡ (reduced-order modeling) çš„å¢å¼ºæ¡†æ¶ã€‚ç ”ç©¶å¼•å…¥äº†è°±åˆ†è§£ (spectral decomposition)ã€æœ¬å¾æ­£äº¤åˆ†è§£ (POD) å’Œé¢„è®­ç»ƒè‡ªç¼–ç å™¨ (autoencoder) ç­‰æ›¿ä»£è§£ç å™¨ç»“æ„ï¼Œåœ¨å¤§å¹…æå‡è®¡ç®—æ•ˆç‡çš„åŒæ—¶ç¡®ä¿äº†æ¨¡å‹çš„å‡†ç¡®æ€§ä¸å™ªå£°å®¹å¿åº¦ã€‚æ­¤å¤–ï¼Œé€šè¿‡é›†æˆè¿ç§»å­¦ä¹  (transfer learning) å’Œå¾®è°ƒ (fine-tuning) ç­–ç•¥ï¼Œè¯¥æ¡†æ¶å®ç°äº†çŸ¥è¯†å¤ç”¨ï¼Œæ˜¾è‘—ç¼©çŸ­äº†é’ˆå¯¹æ–°ææ–™æˆ–æ–°é…ç½®çš„è®­ç»ƒæ—¶é—´ã€‚åœ¨éçº¿æ€§æ‰©æ•£æ–¹ç¨‹ (nonlinear diffusion equation) æ¡ˆä¾‹ä¸­çš„æµ‹è¯•ç»“æœæ˜¾ç¤ºï¼Œå¢å¼ºå‹ PGNNIV èƒ½å¤Ÿä»…åˆ©ç”¨è§‚æµ‹æ•°æ®å‡†ç¡®è¯†åˆ«æ½œåœ¨çš„æœ¬æ„çŠ¶æ€æ–¹ç¨‹ (constitutive state equations)ã€‚å®éªŒè¯æ˜è¯¥æ–¹æ³•æœ‰æ•ˆç¼“è§£äº†è¿‡æ‹Ÿåˆå¹¶é™ä½äº†è®¡ç®—éœ€æ±‚ï¼Œä¸ºå¤šç§æ•°æ®åœºæ™¯ä¸‹çš„ææ–™è¡Œä¸ºå‘ç°æä¾›äº†çµæ´»ä¸”ç¨³å¥çš„è§£å†³æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.00959v2",
      "published_date": "2025-08-01 12:33:21 UTC",
      "updated_date": "2025-08-25 07:18:54 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T08:08:51.353089+00:00"
    },
    {
      "arxiv_id": "2508.00581v1",
      "title": "From EMR Data to Clinical Insight: An LLM-Driven Framework for Automated Pre-Consultation Questionnaire Generation",
      "title_zh": "ä»ç”µå­ç—…å†æ•°æ®åˆ°ä¸´åºŠæ´å¯Ÿï¼šå¤§è¯­è¨€æ¨¡å‹é©±åŠ¨çš„è‡ªåŠ¨åŒ–è¯Šå‰é—®å·ç”Ÿæˆæ¡†æ¶",
      "authors": [
        "Ruiqing Ding",
        "Qianfang Sun",
        "Yongkang Leng",
        "Hui Yin",
        "Xiaojian Li"
      ],
      "abstract": "Pre-consultation is a critical component of effective healthcare delivery. However, generating comprehensive pre-consultation questionnaires from complex, voluminous Electronic Medical Records (EMRs) is a challenging task. Direct Large Language Model (LLM) approaches face difficulties in this task, particularly regarding information completeness, logical order, and disease-level synthesis. To address this issue, we propose a novel multi-stage LLM-driven framework: Stage 1 extracts atomic assertions (key facts with timing) from EMRs; Stage 2 constructs personal causal networks and synthesizes disease knowledge by clustering representative networks from an EMR corpus; Stage 3 generates tailored personal and standardized disease-specific questionnaires based on these structured representations. This framework overcomes limitations of direct methods by building explicit clinical knowledge. Evaluated on a real-world EMR dataset and validated by clinical experts, our method demonstrates superior performance in information coverage, diagnostic relevance, understandability, and generation time, highlighting its practical potential to enhance patient information collection.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ä»å¤æ‚å†—é•¿çš„ç”µå­ç—…å†(EMRs)ä¸­ç”Ÿæˆé—®è¯Šå‰é—®å·æ‰€é¢ä¸´çš„ä¿¡æ¯å®Œæ•´æ€§å·®ã€é€»è¾‘æ€§å¼±ä»¥åŠç¼ºä¹ç–¾ç—…å±‚é¢ç»¼åˆç­‰é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§ç”±å¤§è¯­è¨€æ¨¡å‹(LLM)é©±åŠ¨çš„å¤šé˜¶æ®µè‡ªåŠ¨åŒ–æ¡†æ¶ã€‚è¯¥æ¡†æ¶é¦–å…ˆä»EMRsä¸­æå–å¸¦æœ‰æ—¶é—´ä¿¡æ¯çš„åŸå­æ–­è¨€(atomic assertions)ä»¥æ•æ‰å…³é”®äº‹å®ï¼Œéšåé€šè¿‡æ„å»ºä¸ªäººå› æœç½‘ç»œ(personal causal networks)å¹¶ç»“åˆç—…å†è¯­æ–™åº“èšç±»æ¥åˆæˆç–¾ç—…çŸ¥è¯†ï¼Œæœ€ååŸºäºè¿™äº›ç»“æ„åŒ–è¡¨å¾ç”Ÿæˆä¸ªæ€§åŒ–ä¸”æ ‡å‡†åŒ–çš„ç–¾ç—…ç‰¹å®šé—®å·ã€‚é€šè¿‡åœ¨çœŸå®ä¸–ç•ŒEMRæ•°æ®é›†ä¸Šçš„å®éªŒä»¥åŠä¸´åºŠä¸“å®¶çš„éªŒè¯ï¼Œè¯¥æ–¹æ³•åœ¨ä¿¡æ¯è¦†ç›–åº¦ã€è¯Šæ–­ç›¸å…³æ€§ã€å¯ç†è§£æ€§å’Œç”Ÿæˆé€Ÿåº¦æ–¹é¢å‡å±•ç°å‡ºå“è¶Šæ€§èƒ½ã€‚è¿™ä¸€ç ”ç©¶é€šè¿‡æ„å»ºæ˜¾å¼ä¸´åºŠçŸ¥è¯†å…‹æœäº†ç›´æ¥ä½¿ç”¨LLMçš„å±€é™æ€§ï¼Œè¯æ˜äº†åˆ©ç”¨å¤§æ¨¡å‹è‡ªåŠ¨åŒ–æå‡æ‚£è€…ä¿¡æ¯é‡‡é›†è´¨é‡çš„å®ç”¨æ½œåŠ›ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "16 pages, 10 figures",
      "pdf_url": "https://arxiv.org/pdf/2508.00581v1",
      "published_date": "2025-08-01 12:24:49 UTC",
      "updated_date": "2025-08-01 12:24:49 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T08:09:55.541828+00:00"
    },
    {
      "arxiv_id": "2508.00580v1",
      "title": "OmniUnet: A Multimodal Network for Unstructured Terrain Segmentation on Planetary Rovers Using RGB, Depth, and Thermal Imagery",
      "title_zh": "OmniUnetï¼šä¸€ç§èåˆ RGBã€æ·±åº¦å’Œçƒ­å›¾åƒçš„è¡Œæ˜Ÿæ¢æµ‹å™¨éç»“æ„åŒ–åœ°å½¢åˆ†å‰²å¤šæ¨¡æ€ç½‘ç»œ",
      "authors": [
        "Raul Castilla-Arquillo",
        "Carlos Perez-del-Pulgar",
        "Levin Gerdes",
        "Alfonso Garcia-Cerezo",
        "Miguel A. Olivares-Mendez"
      ],
      "abstract": "Robot navigation in unstructured environments requires multimodal perception systems that can support safe navigation. Multimodality enables the integration of complementary information collected by different sensors. However, this information must be processed by machine learning algorithms specifically designed to leverage heterogeneous data. Furthermore, it is necessary to identify which sensor modalities are most informative for navigation in the target environment. In Martian exploration, thermal imagery has proven valuable for assessing terrain safety due to differences in thermal behaviour between soil types. This work presents OmniUnet, a transformer-based neural network architecture for semantic segmentation using RGB, depth, and thermal (RGB-D-T) imagery. A custom multimodal sensor housing was developed using 3D printing and mounted on the Martian Rover Testbed for Autonomy (MaRTA) to collect a multimodal dataset in the Bardenas semi-desert in northern Spain. This location serves as a representative environment of the Martian surface, featuring terrain types such as sand, bedrock, and compact soil. A subset of this dataset was manually labeled to support supervised training of the network. The model was evaluated both quantitatively and qualitatively, achieving a pixel accuracy of 80.37% and demonstrating strong performance in segmenting complex unstructured terrain. Inference tests yielded an average prediction time of 673 ms on a resource-constrained computer (Jetson Orin Nano), confirming its suitability for on-robot deployment. The software implementation of the network and the labeled dataset have been made publicly available to support future research in multimodal terrain perception for planetary robotics.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† OmniUnetï¼Œè¿™æ˜¯ä¸€ç§åŸºäº Transformer çš„å¤šæ¨¡æ€ç¥ç»ç½‘ç»œæ¶æ„ï¼Œæ—¨åœ¨é€šè¿‡èåˆ RGBã€æ·±åº¦ (Depth) å’Œçº¢å¤–çƒ­æˆåƒ (Thermal) æ•°æ®æ¥æå‡è¡Œæ˜Ÿæ¢æµ‹å™¨åœ¨éç»“æ„åŒ–åœ°å½¢ä¸­çš„è¯­ä¹‰åˆ†å‰²èƒ½åŠ›ã€‚è¯¥æ¨¡å‹ç‰¹åˆ«åˆ©ç”¨äº†çƒ­æˆåƒæŠ€æœ¯åœ¨åŒºåˆ†æ²™åœ°ã€åŸºå²©å’Œç´§å®åœŸå£¤ç­‰ä¸åŒåœ°è´¨çƒ­è¡Œä¸ºæ–¹é¢çš„ä¼˜åŠ¿ï¼Œä»¥ç¡®ä¿æ¢æµ‹å™¨åœ¨ç±»ç«æ˜Ÿå¤æ‚ç¯å¢ƒä¸‹çš„å¯¼èˆªå®‰å…¨ã€‚ç ”ç©¶äººå‘˜åœ¨æ¨¡æ‹Ÿç«æ˜Ÿåœ°è¡¨çš„è¥¿ç­ç‰™ Bardenas åŠè’æ¼ ç¯å¢ƒé‡‡é›†å¹¶æ‰‹åŠ¨æ ‡æ³¨äº†ä¸“é—¨çš„ RGB-D-T æ•°æ®é›†ï¼Œå¹¶ä¸ºæ­¤ç ”åˆ¶äº†é…å¥—çš„å®šåˆ¶åŒ–ä¼ æ„Ÿå™¨ç¡¬ä»¶ç³»ç»Ÿã€‚å®šé‡è¯„ä¼°æ˜¾ç¤ºï¼ŒOmniUnet è¾¾åˆ°äº† 80.37% çš„åƒç´ å‡†ç¡®ç‡ï¼Œä¸”åœ¨èµ„æºå—é™çš„ Jetson Orin Nano è®¡ç®—æœºä¸Šå¹³å‡æ¨ç†æ—¶é—´ä»…ä¸º 673 æ¯«ç§’ï¼Œå……åˆ†è¯æ˜äº†å…¶æ¿è½½å®æ—¶éƒ¨ç½²çš„å®ç”¨æ€§ã€‚ç›®å‰ï¼Œè¯¥é¡¹ç›®çš„ä»£ç å’Œæ•°æ®é›†å·²å‘ç¤¾åŒºå…¬å¼€å‘å¸ƒï¼Œä¸ºè¡Œæ˜Ÿæœºå™¨äººå¤šæ¨¡æ€æ„ŸçŸ¥é¢†åŸŸçš„åç»­ç ”ç©¶æä¾›äº†é‡è¦æ”¯æŒã€‚",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.00580v1",
      "published_date": "2025-08-01 12:23:29 UTC",
      "updated_date": "2025-08-01 12:23:29 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T08:09:01.152460+00:00"
    },
    {
      "arxiv_id": "2508.00576v1",
      "title": "MultiSHAP: A Shapley-Based Framework for Explaining Cross-Modal Interactions in Multimodal AI Models",
      "title_zh": "MultiSHAPï¼šåŸºäº Shapley çš„å¤šæ¨¡æ€äººå·¥æ™ºèƒ½æ¨¡å‹è·¨æ¨¡æ€äº¤äº’è§£é‡Šæ¡†æ¶",
      "authors": [
        "Zhanliang Wang",
        "Kai Wang"
      ],
      "abstract": "Multimodal AI models have achieved impressive performance in tasks that require integrating information from multiple modalities, such as vision and language. However, their \"black-box\" nature poses a major barrier to deployment in high-stakes applications where interpretability and trustworthiness are essential. How to explain cross-modal interactions in multimodal AI models remains a major challenge. While existing model explanation methods, such as attention map and Grad-CAM, offer coarse insights into cross-modal relationships, they cannot precisely quantify the synergistic effects between modalities, and are limited to open-source models with accessible internal weights. Here we introduce MultiSHAP, a model-agnostic interpretability framework that leverages the Shapley Interaction Index to attribute multimodal predictions to pairwise interactions between fine-grained visual and textual elements (such as image patches and text tokens), while being applicable to both open- and closed-source models. Our approach provides: (1) instance-level explanations that reveal synergistic and suppressive cross-modal effects for individual samples - \"why the model makes a specific prediction on this input\", and (2) dataset-level explanation that uncovers generalizable interaction patterns across samples - \"how the model integrates information across modalities\". Experiments on public multimodal benchmarks confirm that MultiSHAP faithfully captures cross-modal reasoning mechanisms, while real-world case studies demonstrate its practical utility. Our framework is extensible beyond two modalities, offering a general solution for interpreting complex multimodal AI models.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†MultiSHAPï¼Œè¿™æ˜¯ä¸€ä¸ªæ¨¡å‹æ— å…³(model-agnostic)çš„å¯è§£é‡Šæ€§æ¡†æ¶ï¼Œæ—¨åœ¨é‡åŒ–å¹¶è§£é‡Šå¤šæ¨¡æ€AIæ¨¡å‹ä¸­çš„è·¨æ¨¡æ€äº¤äº’(cross-modal interactions)ã€‚è¯¥æ¡†æ¶åˆ©ç”¨Shapley Interaction Indexå°†å¤šæ¨¡æ€é¢„æµ‹ç»“æœå½’å› äºç»†ç²’åº¦è§†è§‰å…ƒç´ ï¼ˆå¦‚image patchesï¼‰ä¸æ–‡æœ¬å…ƒç´ ï¼ˆå¦‚text tokensï¼‰ä¹‹é—´çš„ä¸¤ä¸¤äº¤äº’ï¼Œä¸”åŒæ—¶é€‚ç”¨äºå¼€æºå’Œé—­æºæ¨¡å‹ã€‚MultiSHAPä¸ä»…èƒ½é€šè¿‡å®ä¾‹çº§è§£é‡Šæ­ç¤ºå•ä¸ªæ ·æœ¬ä¸­çš„ååŒæˆ–æŠ‘åˆ¶æ•ˆåº”ï¼Œè¿˜èƒ½é€šè¿‡æ•°æ®é›†çº§è§£é‡Šå‘ç°é€šç”¨çš„è·¨æ¨¡æ€ä¿¡æ¯æ•´åˆæ¨¡å¼ã€‚å®éªŒç»“æœå’ŒçœŸå®æ¡ˆä¾‹ç ”ç©¶è¯æ˜ï¼ŒMultiSHAPèƒ½å¤Ÿå¿ å®åœ°æ•æ‰è·¨æ¨¡æ€æ¨ç†æœºåˆ¶ï¼Œå¹¶å…·æœ‰æ‰©å±•è‡³å¤šäºä¸¤ç§æ¨¡æ€çš„æ½œåŠ›ï¼Œä¸ºè§£é‡Šå¤æ‚å¤šæ¨¡æ€æ¨¡å‹æä¾›äº†é€šç”¨çš„è§£å†³æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.00576v1",
      "published_date": "2025-08-01 12:19:18 UTC",
      "updated_date": "2025-08-01 12:19:18 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T08:09:12.634505+00:00"
    },
    {
      "arxiv_id": "2508.00575v1",
      "title": "Analysing Temporal Reasoning in Description Logics Using Formal Grammars",
      "title_zh": "åŸºäºå½¢å¼æ–‡æ³•çš„æè¿°é€»è¾‘æ—¶æ€æ¨ç†åˆ†æ",
      "authors": [
        "Camille Bourgaux",
        "Anton Gnatenko",
        "MichaÃ«l Thomazo"
      ],
      "abstract": "We establish a correspondence between (fragments of) $\\mathcal{TEL}^\\bigcirc$, a temporal extension of the $\\mathcal{EL}$ description logic with the LTL operator $\\bigcirc^k$, and some specific kinds of formal grammars, in particular, conjunctive grammars (context-free grammars equipped with the operation of intersection). This connection implies that $\\mathcal{TEL}^\\bigcirc$ does not possess the property of ultimate periodicity of models, and further leads to undecidability of query answering in $\\mathcal{TEL}^\\bigcirc$, closing a question left open since the introduction of $\\mathcal{TEL}^\\bigcirc$. Moreover, it also allows to establish decidability of query answering for some new interesting fragments of $\\mathcal{TEL}^\\bigcirc$, and to reuse for this purpose existing tools and algorithms for conjunctive grammars.",
      "tldr_zh": "è¯¥ç ”ç©¶å»ºç«‹äº† $\\mathcal{TEL}^\\bigcirc$ï¼ˆæè¿°é€»è¾‘ $\\mathcal{EL}$ ç»“åˆ LTL ç®—å­ $\\bigcirc^k$ çš„æ—¶åºæ‰©å±•ï¼‰ä¸å½¢å¼è¯­æ³•ï¼ˆFormal Grammarsï¼‰ï¼Œç‰¹åˆ«æ˜¯åˆå–è¯­æ³•ï¼ˆConjunctive Grammarsï¼‰ä¹‹é—´çš„å¯¹åº”å…³ç³»ã€‚é€šè¿‡è¿™ä¸€å…³è”ï¼Œç ”ç©¶è¯æ˜äº† $\\mathcal{TEL}^\\bigcirc$ ä¸å…·å¤‡æ¨¡å‹æœ€ç»ˆå‘¨æœŸæ€§ï¼ˆUltimate Periodicity of Modelsï¼‰çš„ç‰¹æ€§ã€‚è¿™ä¸€æ ¸å¿ƒå‘ç°æ¨å¯¼å‡ºäº† $\\mathcal{TEL}^\\bigcirc$ ä¸­æŸ¥è¯¢å›ç­”ï¼ˆQuery Answeringï¼‰çš„ä¸å¯åˆ¤å®šæ€§ï¼ŒæˆåŠŸè§£å†³äº†è‡ªè¯¥é€»è¾‘å¼•å…¥ä»¥æ¥é•¿æœŸæ‚¬è€Œæœªå†³çš„å…¬å¼€é—®é¢˜ã€‚æ­¤å¤–ï¼Œè¯¥å¯¹åº”å…³ç³»è¿˜ç¡®ç«‹äº† $\\mathcal{TEL}^\\bigcirc$ æŸäº›æ–°å­ç‰‡æ®µçš„æŸ¥è¯¢åˆ¤å®šæ€§ã€‚è¿™ä½¿å¾—ç ”ç©¶è€…èƒ½å¤Ÿå¤ç”¨ç°æœ‰çš„åˆå–è¯­æ³•å·¥å…·ä¸ç®—æ³•ï¼Œä¸ºåˆ†ææè¿°é€»è¾‘ä¸­çš„æ—¶åºæ¨ç†æä¾›äº†å…¨æ–°çš„ç†è®ºåŸºç¡€ä¸è®¡ç®—æ‰‹æ®µã€‚",
      "categories": [
        "cs.LO",
        "cs.AI"
      ],
      "primary_category": "cs.LO",
      "comment": "This is an extended version of a paper appearing at the 28th European Conference on Artificial Intelligence (ECAI 2025). 20 pages",
      "pdf_url": "https://arxiv.org/pdf/2508.00575v1",
      "published_date": "2025-08-01 12:17:49 UTC",
      "updated_date": "2025-08-01 12:17:49 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T08:10:20.743577+00:00"
    },
    {
      "arxiv_id": "2508.00574v1",
      "title": "SynAdapt: Learning Adaptive Reasoning in Large Language Models via Synthetic Continuous Chain-of-Thought",
      "title_zh": "SynAdaptï¼šåŸºäºåˆæˆè¿ç»­æ€ç»´é“¾çš„å¤§è¯­è¨€æ¨¡å‹è‡ªé€‚åº”æ¨ç†å­¦ä¹ ",
      "authors": [
        "Jianwei Wang",
        "Ziming Wu",
        "Fuming Lai",
        "Shaobing Lian",
        "Ziqian Zeng"
      ],
      "abstract": "While Chain-of-Thought (CoT) reasoning improves model performance, it incurs significant time costs due to the generation of discrete CoT tokens (DCoT). Continuous CoT (CCoT) offers a more efficient alternative, but existing CCoT methods are hampered by indirect fine-tuning, limited alignment, or inconsistent targets. To overcome these limitations, we propose \\textit{SynAdapt}, an innovative efficient reasoning framework. Specifically, \\textit{SynAdapt} generates the synthetic CCoT to serve as a precise and effective alignment target for LLMs. This synthetic CCoT explicitly guides the LLM to learn CCoT and derive accurate answers directly. Furthermore, relying solely on CCoT is insufficient for solving hard questions. To address this, \\textit{SynAdapt} integrates a difficulty classifier that leverages both question context and CCoT to identify hard questions. CCoT can effectively help identify hard questions after some brief reasoning. We then adaptively prompt the LLM to re-think these hard questions for improved performance. Extensive experimental results across various benchmarks from different difficulty levels strongly demonstrate the effectiveness of our method, achieving the best accuracy-efficiency trade-off.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† SynAdaptï¼Œè¿™æ˜¯ä¸€ç§æ—¨åœ¨æå‡å¤§è¯­è¨€æ¨¡å‹(LLMs)æ¨ç†æ•ˆç‡çš„åˆ›æ–°æ¡†æ¶ã€‚é’ˆå¯¹ç¦»æ•£é“¾å¼æ€ç»´(Discrete Chain-of-Thought, DCoT)ç”Ÿæˆå¸¦æ¥çš„å·¨å¤§æ—¶é—´æˆæœ¬ï¼Œä»¥åŠç°æœ‰è¿ç»­é“¾å¼æ€ç»´(Continuous CoT, CCoT)æ–¹æ³•åœ¨å¯¹é½å’Œå¾®è°ƒä¸Šçš„å±€é™æ€§ï¼Œè¯¥æ¡†æ¶é€šè¿‡ç”Ÿæˆåˆæˆçš„ CCoT ä½œä¸ºç²¾ç¡®çš„å¯¹é½ç›®æ ‡ï¼Œç›´æ¥å¼•å¯¼æ¨¡å‹å­¦ä¹ è¿ç»­æ¨ç†å¹¶å¾—å‡ºå‡†ç¡®ç­”æ¡ˆã€‚æ­¤å¤–ï¼ŒSynAdapt æ•´åˆäº†ä¸€ä¸ªéš¾åº¦åˆ†ç±»å™¨(difficulty classifier)ï¼Œåˆ©ç”¨é—®é¢˜ä¸Šä¸‹æ–‡å’Œ CCoT æ¥è¯†åˆ«å¤æ‚ä»»åŠ¡ã€‚åœ¨è¯†åˆ«å‡ºéš¾é¢˜åï¼Œç³»ç»Ÿä¼šè‡ªé€‚åº”åœ°æç¤ºæ¨¡å‹è¿›è¡Œé‡æ–°æ€è€ƒ(re-think)ä»¥è¿›ä¸€æ­¥æå‡æ€§èƒ½ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒSynAdapt åœ¨ä¸åŒéš¾åº¦çš„åŸºå‡†æµ‹è¯•ä¸­å‡è¡¨ç°å‡ºè‰²ï¼ŒæˆåŠŸå®ç°äº†å‡†ç¡®ç‡ä¸æ•ˆç‡ä¹‹é—´çš„æœ€ä½³å¹³è¡¡(accuracy-efficiency trade-off)ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.00574v1",
      "published_date": "2025-08-01 12:17:35 UTC",
      "updated_date": "2025-08-01 12:17:35 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T08:10:09.394093+00:00"
    },
    {
      "arxiv_id": "2508.00555v1",
      "title": "Activation-Guided Local Editing for Jailbreaking Attacks",
      "title_zh": "é’ˆå¯¹è¶Šç‹±æ”»å‡»çš„æ¿€æ´»å¼•å¯¼å±€éƒ¨ç¼–è¾‘",
      "authors": [
        "Jiecong Wang",
        "Haoran Li",
        "Hao Peng",
        "Ziqian Zeng",
        "Zihao Wang",
        "Haohua Du",
        "Zhengtao Yu"
      ],
      "abstract": "Jailbreaking is an essential adversarial technique for red-teaming these models to uncover and patch security flaws. However, existing jailbreak methods face significant drawbacks. Token-level jailbreak attacks often produce incoherent or unreadable inputs and exhibit poor transferability, while prompt-level attacks lack scalability and rely heavily on manual effort and human ingenuity. We propose a concise and effective two-stage framework that combines the advantages of these approaches. The first stage performs a scenario-based generation of context and rephrases the original malicious query to obscure its harmful intent. The second stage then utilizes information from the model's hidden states to guide fine-grained edits, effectively steering the model's internal representation of the input from a malicious toward a benign one. Extensive experiments demonstrate that this method achieves state-of-the-art Attack Success Rate, with gains of up to 37.74% over the strongest baseline, and exhibits excellent transferability to black-box models. Our analysis further demonstrates that AGILE maintains substantial effectiveness against prominent defense mechanisms, highlighting the limitations of current safeguards and providing valuable insights for future defense development. Our code is available at https://github.com/yunsaijc/AGILE.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† AGILEï¼Œä¸€ç§ç®€æ´ä¸”æœ‰æ•ˆçš„ä¸¤é˜¶æ®µ Jailbreaking æ”»å‡»æ¡†æ¶ï¼Œæ—¨åœ¨å…‹æœç°æœ‰ Token-level å’Œ Prompt-level æ”»å‡»åœ¨è¿è´¯æ€§ã€è¿ç§»æ€§å’Œæ‰©å±•æ€§æ–¹é¢çš„å±€é™ã€‚è¯¥æ¡†æ¶çš„ç¬¬ä¸€é˜¶æ®µé€šè¿‡ Scenario-based generation æ„å»ºä¸Šä¸‹æ–‡å¹¶é‡å†™åŸå§‹æ¶æ„æŸ¥è¯¢ï¼Œä»¥æ©ç›–å…¶æœ‰å®³æ„å›¾ï¼›ç¬¬äºŒé˜¶æ®µåˆ™åˆ©ç”¨æ¨¡å‹ Hidden states çš„ä¿¡æ¯å¼•å¯¼ Fine-grained editsï¼Œé€šè¿‡å±€éƒ¨ç¼–è¾‘å°†æ¨¡å‹å†…éƒ¨çš„ Internal representation ä»æ¶æ„å¼•å¯¼è‡³è‰¯æ€§ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒAGILE åœ¨ Attack Success Rate (ASR) ä¸Šå–å¾—äº†æ˜¾è‘—çªç ´ï¼Œç›¸è¾ƒäºæœ€å¼ºåŸºçº¿æå‡äº† 37.74%ï¼Œå¹¶å±•ç°å‡ºå¯¹ Black-box models æä½³çš„ Transferabilityã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•åœ¨å¯¹æŠ—ä¸»æµé˜²å¾¡æœºåˆ¶æ—¶ä¾ç„¶ä¿æŒé«˜åº¦æœ‰æ•ˆæ€§ï¼Œæ­ç¤ºäº†å½“å‰å®‰å…¨é˜²æŠ¤çš„å±€é™æ€§å¹¶ä¸ºæœªæ¥å®‰å…¨é˜²å¾¡çš„æ¼”è¿›æä¾›äº†é‡è¦è§è§£ã€‚",
      "categories": [
        "cs.CR",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.CR",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.00555v1",
      "published_date": "2025-08-01 11:52:24 UTC",
      "updated_date": "2025-08-01 11:52:24 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T08:10:19.447601+00:00"
    },
    {
      "arxiv_id": "2508.00546v1",
      "title": "SPENCER: Self-Adaptive Model Distillation for Efficient Code Retrieval",
      "title_zh": "SPENCERï¼šé¢å‘é«˜æ•ˆä»£ç æ£€ç´¢çš„è‡ªé€‚åº”æ¨¡å‹è’¸é¦",
      "authors": [
        "Wenchao Gu",
        "Zongyi Lyu",
        "Yanlin Wang",
        "Hongyu Zhang",
        "Cuiyun Gao",
        "Michael R. Lyu"
      ],
      "abstract": "Code retrieval aims to provide users with desired code snippets based on users' natural language queries. With the development of deep learning technologies, adopting pre-trained models for this task has become mainstream. Considering the retrieval efficiency, most of the previous approaches adopt a dual-encoder for this task, which encodes the description and code snippet into representation vectors, respectively. However, the model structure of the dual-encoder tends to limit the model's performance, since it lacks the interaction between the code snippet and description at the bottom layer of the model during training. To improve the model's effectiveness while preserving its efficiency, we propose a framework, which adopts Self-AdaPtive Model Distillation for Efficient CodE Retrieval, named SPENCER. SPENCER first adopts the dual-encoder to narrow the search space and then adopts the cross-encoder to improve accuracy. To improve the efficiency of SPENCER, we propose a novel model distillation technique, which can greatly reduce the inference time of the dual-encoder while maintaining the overall performance. We also propose a teaching assistant selection strategy for our model distillation, which can adaptively select the suitable teaching assistant models for different pre-trained models during the model distillation to ensure the model performance. Extensive experiments demonstrate that the combination of dual-encoder and cross-encoder improves overall performance compared to solely dual-encoder-based models for code retrieval. Besides, our model distillation technique retains over 98% of the overall performance while reducing the inference time of the dual-encoder by 70%.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†SPENCERï¼Œä¸€ä¸ªç”¨äºé«˜æ•ˆä»£ç æ£€ç´¢(Code Retrieval)çš„è‡ªé€‚åº”æ¨¡å‹è’¸é¦æ¡†æ¶ã€‚ä¸ºäº†å¹³è¡¡æ£€ç´¢æ•ˆç‡ä¸å‡†ç¡®åº¦ï¼ŒSPENCERç»“åˆäº†åŒç¼–ç å™¨(Dual-encoder)çš„å¤§è§„æ¨¡æœç´¢èƒ½åŠ›ä¸äº¤å‰ç¼–ç å™¨(Cross-encoder)çš„åº•å±‚äº¤äº’æ€§èƒ½ï¼Œæœ‰æ•ˆè§£å†³äº†ä¼ ç»ŸåŒç¼–ç å™¨æ¶æ„å› ç¼ºä¹å›¾æ–‡äº¤äº’å¯¼è‡´çš„æ€§èƒ½ç“¶é¢ˆã€‚è¯¥æ¡†æ¶å¼•å…¥äº†ä¸€ç§åˆ›æ–°çš„æ¨¡å‹è’¸é¦æŠ€æœ¯ï¼Œå¹¶é…å¥—æå‡ºåŠ©æ•™æ¨¡å‹(Teaching Assistant)é€‰æ‹©ç­–ç•¥ï¼Œèƒ½å¤Ÿæ ¹æ®ä¸åŒçš„é¢„è®­ç»ƒæ¨¡å‹è‡ªé€‚åº”åŒ¹é…æœ€ä½³åŠ©æ•™ä»¥ç¡®ä¿è’¸é¦æ•ˆæœã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒSPENCERåœ¨æå‡æ£€ç´¢ç²¾åº¦çš„åŒæ—¶ï¼Œèƒ½ä¿ç•™åŸæ¨¡å‹98%ä»¥ä¸Šçš„æ€§èƒ½ï¼Œå¹¶å°†åŒç¼–ç å™¨çš„æ¨ç†æ—¶é—´æ˜¾è‘—ç¼©çŸ­70%ã€‚è¿™ä¸€ç ”ç©¶ä¸ºåœ¨å¤§è§„æ¨¡ä»£ç åº“ä¸­å®ç°å…¼å…·é«˜ç²¾åº¦ä¸ä½å»¶è¿Ÿçš„ä»£ç æœç´¢æä¾›äº†æœ‰æ•ˆçš„æŠ€æœ¯è·¯å¾„ã€‚",
      "categories": [
        "cs.SE",
        "cs.AI"
      ],
      "primary_category": "cs.SE",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.00546v1",
      "published_date": "2025-08-01 11:39:32 UTC",
      "updated_date": "2025-08-01 11:39:32 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T08:10:19.646184+00:00"
    },
    {
      "arxiv_id": "2508.00545v1",
      "title": "Foundations of Interpretable Models",
      "title_zh": "å¯è§£é‡Šæ¨¡å‹çš„ç†è®ºåŸºç¡€",
      "authors": [
        "Pietro Barbiero",
        "Mateo Espinosa Zarlenga",
        "Alberto Termine",
        "Mateja Jamnik",
        "Giuseppe Marra"
      ],
      "abstract": "We argue that existing definitions of interpretability are not actionable in that they fail to inform users about general, sound, and robust interpretable model design. This makes current interpretability research fundamentally ill-posed. To address this issue, we propose a definition of interpretability that is general, simple, and subsumes existing informal notions within the interpretable AI community. We show that our definition is actionable, as it directly reveals the foundational properties, underlying assumptions, principles, data structures, and architectural features necessary for designing interpretable models. Building on this, we propose a general blueprint for designing interpretable models and introduce the first open-sourced library with native support for interpretable data structures and processes.",
      "tldr_zh": "è¯¥ç ”ç©¶æŒ‡å‡ºï¼Œç°æœ‰çš„ interpretabilityï¼ˆå¯è§£é‡Šæ€§ï¼‰å®šä¹‰å› ç¼ºä¹å¯æ“ä½œæ€§ï¼Œæ— æ³•ä¸ºç¨³å¥çš„å¯è§£é‡Šæ¨¡å‹è®¾è®¡æä¾›æœ‰æ•ˆæŒ‡å¯¼ï¼Œå¯¼è‡´å½“å‰ç ”ç©¶å¤„äº ill-posedï¼ˆä¸é€‚å®šï¼‰çš„çŠ¶æ€ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œä½œè€…æå‡ºäº†ä¸€ä¸ªé€šç”¨ä¸”ç®€æ´çš„ interpretability å®šä¹‰ï¼ŒæˆåŠŸæ¶µç›–äº† interpretable AI ç¤¾åŒºç°æœ‰çš„å„ç±»éæ­£å¼æ¦‚å¿µã€‚è¯¥å®šä¹‰å…·æœ‰æ˜¾è‘—çš„å¯æ“ä½œæ€§ï¼Œèƒ½å¤Ÿç›´æ¥æ­ç¤ºè®¾è®¡æ¨¡å‹æ‰€éœ€çš„åŸºç¡€å±æ€§ã€åº•å±‚å‡è®¾ã€ principlesï¼ˆåŸåˆ™ï¼‰ã€data structuresï¼ˆæ•°æ®ç»“æ„ï¼‰åŠæ¶æ„ç‰¹å¾ã€‚åŸºäºæ­¤ç†è®ºï¼Œç ”ç©¶è€…åˆ¶å®šäº†ä¸€å¥—è®¾è®¡å¯è§£é‡Šæ¨¡å‹çš„é€šç”¨ blueprintï¼ˆè“å›¾ï¼‰ï¼Œå¹¶æ¨å‡ºäº†é¦–ä¸ªåŸç”Ÿæ”¯æŒå¯è§£é‡Šæ•°æ®ç»“æ„ä¸æµç¨‹çš„å¼€æºåº“ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.NE",
        "stat.ML"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.00545v1",
      "published_date": "2025-08-01 11:36:21 UTC",
      "updated_date": "2025-08-01 11:36:21 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T08:10:20.339758+00:00"
    },
    {
      "arxiv_id": "2508.03744v1",
      "title": "Do We Need Pre-Processing for Deep Learning Based Ultrasound Shear Wave Elastography?",
      "title_zh": "åŸºäºæ·±åº¦å­¦ä¹ çš„è¶…å£°å‰ªåˆ‡æ³¢å¼¹æ€§æˆåƒæ˜¯å¦éœ€è¦é¢„å¤„ç†ï¼Ÿ",
      "authors": [
        "Sarah Grube",
        "SÃ¶ren GrÃ¼nhagen",
        "Sarah Latus",
        "Michael Meyling",
        "Alexander Schlaefer"
      ],
      "abstract": "Estimating the elasticity of soft tissue can provide useful information for various diagnostic applications. Ultrasound shear wave elastography offers a non-invasive approach. However, its generalizability and standardization across different systems and processing pipelines remain limited. Considering the influence of image processing on ultrasound based diagnostics, recent literature has discussed the impact of different image processing steps on reliable and reproducible elasticity analysis. In this work, we investigate the need of ultrasound pre-processing steps for deep learning-based ultrasound shear wave elastography. We evaluate the performance of a 3D convolutional neural network in predicting shear wave velocities from spatio-temporal ultrasound images, studying different degrees of pre-processing on the input images, ranging from fully beamformed and filtered ultrasound images to raw radiofrequency data. We compare the predictions from our deep learning approach to a conventional time-of-flight method across four gelatin phantoms with different elasticity levels. Our results demonstrate statistically significant differences in the predicted shear wave velocity among all elasticity groups, regardless of the degree of pre-processing. Although pre-processing slightly improves performance metrics, our results show that the deep learning approach can reliably differentiate between elasticity groups using raw, unprocessed radiofrequency data. These results show that deep learning-based approaches could reduce the need for and the bias of traditional ultrasound pre-processing steps in ultrasound shear wave elastography, enabling faster and more reliable clinical elasticity assessments.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†åœ¨åŸºäºæ·±åº¦å­¦ä¹ (Deep Learning)çš„è¶…å£°å‰ªåˆ‡æ³¢å¼¹æ€§æˆåƒ(Ultrasound Shear Wave Elastography)ä¸­ï¼Œä¼ ç»Ÿè¶…å£°é¢„å¤„ç†æ­¥éª¤æ˜¯å¦å…·æœ‰å¿…è¦æ€§ã€‚ç ”ç©¶è€…é€šè¿‡3Då·ç§¯ç¥ç»ç½‘ç»œ(3D Convolutional Neural Network)ä»æ—¶ç©ºè¶…å£°å›¾åƒä¸­é¢„æµ‹å‰ªåˆ‡æ³¢é€Ÿåº¦(Shear Wave Velocities)ï¼Œå¯¹æ¯”äº†ä»åŸå§‹å°„é¢‘æ•°æ®(Radiofrequency Data)åˆ°å®Œå…¨æ³¢æŸåˆæˆå¹¶æ»¤æ³¢åçš„å›¾åƒç­‰ä¸åŒé¢„å¤„ç†ç¨‹åº¦çš„è¾“å…¥ã€‚é€šè¿‡å¯¹å››ä¸ªä¸åŒå¼¹æ€§æ°´å¹³çš„æ˜èƒ¶æ¨¡å‹(Gelatin Phantoms)è¿›è¡Œå®éªŒå¹¶ä¸ä¼ ç»Ÿé£è¡Œæ—¶é—´æ³•(Time-of-Flight)å¯¹æ¯”ï¼Œç»“æœæ˜¾ç¤ºæ— è®ºé¢„å¤„ç†ç¨‹åº¦å¦‚ä½•ï¼Œæ¨¡å‹é¢„æµ‹çš„å‰ªåˆ‡æ³¢é€Ÿåº¦åœ¨æ‰€æœ‰å¼¹æ€§ç»„åˆ«ä¸­å‡å…·æœ‰æ˜¾è‘—ç»Ÿè®¡å­¦å·®å¼‚ã€‚å°½ç®¡é¢„å¤„ç†èƒ½å¾®å¼±æå‡æ€§èƒ½æŒ‡æ ‡ï¼Œä½†ç ”ç©¶è¯æ˜æ·±åº¦å­¦ä¹ æ–¹æ³•ä»…ä½¿ç”¨æœªç»å¤„ç†çš„åŸå§‹å°„é¢‘æ•°æ®å³å¯å¯é åœ°è¯†åˆ«ä¸åŒå¼¹æ€§ç­‰çº§ã€‚è¿™ä¸€å‘ç°è¡¨æ˜ï¼ŒåŸºäºæ·±åº¦å­¦ä¹ çš„æ–¹æ³•å¯ä»¥å‡å°‘å¯¹ä¼ ç»Ÿé¢„å¤„ç†æ­¥éª¤çš„ä¾èµ–åŠç›¸å…³åå·®ï¼Œä»è€Œå®ç°æ›´å¿«é€Ÿã€æ›´å¯é çš„ä¸´åºŠå¼¹æ€§è¯„ä¼°ã€‚",
      "categories": [
        "eess.IV",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "eess.IV",
      "comment": "Accepted to CURAC conference 2025",
      "pdf_url": "https://arxiv.org/pdf/2508.03744v1",
      "published_date": "2025-08-01 11:26:46 UTC",
      "updated_date": "2025-08-01 11:26:46 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T08:10:31.035411+00:00"
    },
    {
      "arxiv_id": "2508.00957v1",
      "title": "Small sample-based adaptive text classification through iterative and contrastive description refinement",
      "title_zh": "åŸºäºè¿­ä»£ä¸å¯¹æ¯”æè¿°ä¼˜åŒ–çš„è‡ªé€‚åº”å°æ ·æœ¬æ–‡æœ¬åˆ†ç±»",
      "authors": [
        "Amrit Rajeev",
        "Udayaadithya Avadhanam",
        "Harshula Tulapurkar",
        "SaiBarath Sundar"
      ],
      "abstract": "Zero-shot text classification remains a difficult task in domains with evolving knowledge and ambiguous category boundaries, such as ticketing systems. Large language models (LLMs) often struggle to generalize in these scenarios due to limited topic separability, while few-shot methods are constrained by insufficient data diversity. We propose a classification framework that combines iterative topic refinement, contrastive prompting, and active learning. Starting with a small set of labeled samples, the model generates initial topic labels. Misclassified or ambiguous samples are then used in an iterative contrastive prompting process to refine category distinctions by explicitly teaching the model to differentiate between closely related classes. The framework features a human-in-the-loop component, allowing users to introduce or revise category definitions in natural language. This enables seamless integration of new, unseen categories without retraining, making the system well-suited for real-world, dynamic environments. The evaluations on AGNews and DBpedia demonstrate strong performance: 91% accuracy on AGNews (3 seen, 1 unseen class) and 84% on DBpedia (8 seen, 1 unseen), with minimal accuracy shift after introducing unseen classes (82% and 87%, respectively). The results highlight the effectiveness of prompt-based semantic reasoning for fine-grained classification with limited supervision.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹é›¶æ ·æœ¬(Zero-shot)æ–‡æœ¬åˆ†ç±»åœ¨åŠ¨æ€ç¯å¢ƒå’Œæ¨¡ç³Šç±»åˆ«è¾¹ç•Œä¸­çš„å±€é™æ€§ï¼Œæå‡ºäº†ä¸€ç§åŸºäºå°æ ·æœ¬çš„è‡ªé€‚åº”æ–‡æœ¬åˆ†ç±»æ¡†æ¶ã€‚è¯¥æ¡†æ¶ç»“åˆäº†è¿­ä»£ä¸»é¢˜ä¼˜åŒ–(Iterative topic refinement)ã€å¯¹æ¯”æç¤º(Contrastive prompting)å’Œä¸»åŠ¨å­¦ä¹ (Active learning)æŠ€æœ¯ï¼Œé€šè¿‡å¯¹è¯¯åˆ†ç±»æˆ–æ­§ä¹‰æ ·æœ¬è¿›è¡Œè¿­ä»£å¯¹æ¯”æç¤ºï¼Œæ˜¾å¼åœ°è®­ç»ƒæ¨¡å‹åŒºåˆ†é«˜åº¦ç›¸å…³çš„ç±»åˆ«ã€‚ç³»ç»Ÿç‰¹åˆ«å¼•å…¥äº†äººåœ¨å›è·¯(Human-in-the-loop)ç»„ä»¶ï¼Œå…è®¸ç”¨æˆ·åˆ©ç”¨è‡ªç„¶è¯­è¨€ä¿®è®¢ç±»åˆ«å®šä¹‰ï¼Œä»è€Œå®ç°äº†æ— éœ€é‡æ–°è®­ç»ƒå³å¯æ— ç¼é›†æˆæ–°ç±»åˆ«çš„åŠŸèƒ½ã€‚åœ¨AGNewså’ŒDBpediaæ•°æ®é›†ä¸Šçš„è¯„ä¼°è¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å¤„ç†å·²çŸ¥å’ŒæœªçŸ¥ç±»åˆ«æ—¶å‡è¡¨ç°ä¼˜å¼‚ï¼Œåˆ†åˆ«è¾¾åˆ°äº†91%å’Œ84%çš„å‡†ç¡®ç‡ã€‚å®éªŒç»“æœè¯æ˜äº†è¯¥æ¡†æ¶åœ¨æœ‰é™ç›‘ç£ç¯å¢ƒä¸‹ï¼Œåˆ©ç”¨åŸºäºæç¤º(Prompt-based)çš„è¯­ä¹‰æ¨ç†è¿›è¡Œç»†ç²’åº¦åˆ†ç±»çš„æœ‰æ•ˆæ€§ä¸é²æ£’æ€§ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.00957v1",
      "published_date": "2025-08-01 11:12:38 UTC",
      "updated_date": "2025-08-01 11:12:38 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T08:10:38.041473+00:00"
    },
    {
      "arxiv_id": "2508.00525v1",
      "title": "Towards a Measure Theory of Semantic Information",
      "title_zh": "è¿ˆå‘è¯­ä¹‰ä¿¡æ¯æµ‹åº¦è®º",
      "authors": [
        "George M. Coghill"
      ],
      "abstract": "A classic account of the quantification of semantic information is that of Bar-Hiller and Carnap. Their account proposes an inverse relation between the informativeness of a statement and its probability. However, their approach assigns the maximum informativeness to a contradiction: which Floridi refers to as the Bar-Hillel-Carnap paradox. He developed a novel theory founded on a distance metric and parabolic relation, designed to remove this paradox. Unfortunately is approach does not succeed in that aim.\n  In this paper I critique Floridi's theory of strongly semantic information on its own terms and show where it succeeds and fails. I then present a new approach based on the unit circle (a relation that has been the basis of theories from basic trigonometry to quantum theory). This is used, by analogy with von Neumann's quantum probability to construct a measure space for informativeness that meets all the requirements stipulated by Floridi and removes the paradox. In addition, while contradictions and tautologies have zero informativeness, it is found that messages which are contradictory to each other are equally informative. The utility of this is explained by means of an example.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹è¯­ä¹‰ä¿¡æ¯ (Semantic Information) é‡åŒ–ä¸­çš„ç»å…¸é—®é¢˜ï¼Œæ·±å…¥æ¢è®¨äº† Bar-Hillel å’Œ Carnap ç†è®ºä¸­å­˜åœ¨çš„ Bar-Hillel-Carnap paradoxï¼Œå³çŸ›ç›¾å‘½é¢˜è¢«èµ‹äºˆæœ€å¤§ä¿¡æ¯é‡çš„é€»è¾‘æ‚–è®ºã€‚ä½œè€…é¦–å…ˆæ‰¹åˆ¤æ€§åœ°åˆ†æäº† Floridi è¯•å›¾é€šè¿‡è·ç¦»åº¦é‡å’ŒæŠ›ç‰©çº¿å…³ç³»è§£å†³è¯¥é—®é¢˜çš„å±€é™æ€§ï¼ŒæŒ‡å‡ºå…¶æ–¹æ¡ˆæœªèƒ½æœ‰æ•ˆæ¶ˆé™¤æ‚–è®ºã€‚éšåï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºå•ä½åœ† (Unit Circle) çš„å…¨æ–°æ–¹æ³•ï¼Œå¹¶ç±»æ¯”å†¯Â·è¯ºä¾æ›¼ (von Neumann) çš„é‡å­æ¦‚ç‡ (Quantum Probability) æ„å»ºäº†ä¸€ä¸ªä¸“é—¨çš„åº¦é‡ç©ºé—´ (Measure Space)ã€‚ç†è®ºè¯æ˜è¯¥æ–¹æ³•ä¸ä»…æ»¡è¶³äº† Floridi è®¾å®šçš„æ‰€æœ‰è¦æ±‚ï¼Œè¿˜æˆåŠŸä½¿çŸ›ç›¾ (Contradictions) å’Œå…¨çœŸå‘½é¢˜ (Tautologies) çš„ä¿¡æ¯é‡å½’é›¶ã€‚æ­¤å¤–ï¼Œè¯¥ç ”ç©¶å‘ç°äº’ä¸ºçŸ›ç›¾çš„æ¶ˆæ¯å…·æœ‰ç›¸ç­‰çš„ä¿¡æ¯é‡ï¼Œå¹¶é€šè¿‡å®ä¾‹å±•ç¤ºäº†è¯¥åº¦é‡ç†è®ºåœ¨å¤„ç†å¤æ‚ä¿¡æ¯å…³ç³»æ—¶çš„å®ç”¨ä»·å€¼ã€‚",
      "categories": [
        "cs.IT",
        "cs.AI"
      ],
      "primary_category": "cs.IT",
      "comment": "17 pages,3 figures",
      "pdf_url": "https://arxiv.org/pdf/2508.00525v1",
      "published_date": "2025-08-01 11:03:39 UTC",
      "updated_date": "2025-08-01 11:03:39 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T08:10:36.734615+00:00"
    },
    {
      "arxiv_id": "2508.00500v2",
      "title": "Pro2Guard: Proactive Runtime Enforcement of LLM Agent Safety via Probabilistic Model Checking",
      "title_zh": "Pro2Guardï¼šåŸºäºæ¦‚ç‡æ¨¡å‹æ£€æµ‹çš„ LLM æ™ºèƒ½ä½“å®‰å…¨ä¸»åŠ¨è¿è¡Œæ—¶å¼ºåˆ¶æ‰§è¡Œ",
      "authors": [
        "Haoyu Wang",
        "Christopher M. Poskitt",
        "Jun Sun",
        "Jiali Wei"
      ],
      "abstract": "Large Language Model (LLM) agents demonstrate strong autonomy, but their stochastic behavior introduces unpredictable safety risks. Existing rule-based enforcement systems, such as AgentSpec, are reactive, intervening only when unsafe behavior is imminent or has occurred, lacking foresight for long-horizon dependencies. To overcome these limitations, we present a proactive runtime enforcement framework for LLM agents. The framework abstracts agent behaviors into symbolic states and learns a Discrete-Time Markov Chain (DTMC) from execution traces. At runtime, it predicts the probability of leading to undesired behaviors and intervenes before violations occur when the estimated risk exceeds a user-defined threshold. Designed to provide PAC-correctness guarantee, the framework achieves statistically reliable enforcement of agent safety. We evaluate the framework across two safety-critical domains: autonomous vehicles and embodied agents. It proactively enforces safety and maintains high task performance, outperforming existing methods.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†Pro2Guardï¼Œè¿™æ˜¯ä¸€ç§æ—¨åœ¨æå‡å¤§è¯­è¨€æ¨¡å‹(LLM)æ™ºèƒ½ä½“å®‰å…¨æ€§çš„ä¸»åŠ¨å¼è¿è¡Œæ—¶å¼ºåˆ¶æ‰§è¡Œæ¡†æ¶ã€‚é’ˆå¯¹ç°æœ‰è§„åˆ™åŒ–ç³»ç»Ÿ(å¦‚AgentSpec)å› è¢«åŠ¨å¹²é¢„è€Œç¼ºä¹é•¿ç¨‹é¢„æµ‹èƒ½åŠ›çš„é—®é¢˜ï¼ŒPro2Guardå°†æ™ºèƒ½ä½“è¡Œä¸ºæŠ½è±¡ä¸ºç¬¦å·çŠ¶æ€ï¼Œå¹¶é€šè¿‡æ‰§è¡Œè½¨è¿¹å­¦ä¹ ç¦»æ•£æ—¶é—´é©¬å°”å¯å¤«é“¾(DTMC)æ¨¡å‹ã€‚åœ¨è¿è¡Œé˜¶æ®µï¼Œè¯¥æ¡†æ¶åˆ©ç”¨æ¦‚ç‡æ¨¡å‹æ£€æµ‹(Probabilistic Model Checking)é¢„æµ‹é€šå‘ä¸è‰¯è¡Œä¸ºçš„æ¦‚ç‡ï¼Œå¹¶åœ¨é£é™©è¶…è¿‡é¢„è®¾é˜ˆå€¼æ—¶æå‰ä»‹å…¥ã€‚Pro2Guardè®¾è®¡ä¸Šå…·å¤‡PAC-correctnessä¿è¯ï¼Œèƒ½å¤Ÿæä¾›ç»Ÿè®¡æ„ä¹‰ä¸Šå¯é çš„å®‰å…¨å¼ºåˆ¶æ‰§è¡Œã€‚å®éªŒåœ¨è‡ªåŠ¨é©¾é©¶å’Œå…·èº«æ™ºèƒ½ä½“(embodied agents)ç­‰å®‰å…¨æ•æ„Ÿé¢†åŸŸéªŒè¯äº†å…¶æœ‰æ•ˆæ€§ï¼Œç»“æœæ˜¾ç¤ºè¯¥æ¡†æ¶åœ¨ä¿æŒé«˜ä»»åŠ¡æ€§èƒ½çš„åŒæ—¶ï¼Œèƒ½æ¯”ç°æœ‰æ–¹æ³•æ›´æœ‰æ•ˆåœ°ä¸»åŠ¨è§„é¿é£é™©ã€‚",
      "categories": [
        "cs.AI",
        "cs.SE"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.00500v2",
      "published_date": "2025-08-01 10:24:47 UTC",
      "updated_date": "2026-01-06 03:51:07 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T08:10:38.619917+00:00"
    },
    {
      "arxiv_id": "2508.00496v2",
      "title": "LesiOnTime -- Joint Temporal and Clinical Modeling for Small Breast Lesion Segmentation in Longitudinal DCE-MRI",
      "title_zh": "LesiOnTimeï¼šé¢å‘çºµå‘ DCE-MRI ä¸­ä¹³è…ºå°ç—…ç¶åˆ†å‰²çš„æ—¶åºä¸ä¸´åºŠè”åˆå»ºæ¨¡",
      "authors": [
        "Mohammed Kamran",
        "Maria Bernathova",
        "Raoul Varga",
        "Christian F. Singer",
        "Zsuzsanna Bago-Horvath",
        "Thomas Helbich",
        "Georg Langs",
        "Philipp SeebÃ¶ck"
      ],
      "abstract": "Accurate segmentation of small lesions in Breast Dynamic Contrast-Enhanced MRI (DCE-MRI) is critical for early cancer detection, especially in high-risk patients. While recent deep learning methods have advanced lesion segmentation, they primarily target large lesions and neglect valuable longitudinal and clinical information routinely used by radiologists. In real-world screening, detecting subtle or emerging lesions requires radiologists to compare across timepoints and consider previous radiology assessments, such as the BI-RADS score. We propose LesiOnTime, a novel 3D segmentation approach that mimics clinical diagnostic workflows by jointly leveraging longitudinal imaging and BIRADS scores. The key components are: (1) a Temporal Prior Attention (TPA) block that dynamically integrates information from previous and current scans; and (2) a BI-RADS Consistency Regularization (BCR) loss that enforces latent space alignment for scans with similar radiological assessments, thus embedding domain knowledge into the training process. Evaluated on a curated in-house longitudinal dataset of high-risk patients with DCE-MRI, our approach outperforms state-of-the-art single-timepoint and longitudinal baselines by 5% in terms of Dice. Ablation studies demonstrate that both TPA and BCR contribute complementary performance gains. These results highlight the importance of incorporating temporal and clinical context for reliable early lesion segmentation in real-world breast cancer screening. Our code is publicly available at https://github.com/cirmuw/LesiOnTime",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹é«˜é£é™©æ‚£è€…æ—©æœŸä¹³è…ºç™Œç­›æŸ¥ä¸­ DCE-MRI å°ç—…ç¶åˆ†å‰²å‡†ç¡®ç‡ä¸è¶³çš„é—®é¢˜ï¼Œæå‡ºäº† LesiOnTimeï¼Œä¸€ç§é€šè¿‡è”åˆå»ºæ¨¡çºµå‘å½±åƒä¸ä¸´åºŠä¿¡æ¯æ¥æ¨¡æ‹Ÿæ”¾å°„ç§‘åŒ»ç”Ÿè¯Šæ–­å·¥ä½œæµçš„ 3D åˆ†å‰²æ–¹æ³•ã€‚è¯¥æ¡†æ¶å¼•å…¥äº†æ—¶é—´å…ˆéªŒæ³¨æ„åŠ› (Temporal Prior Attention, TPA) æ¨¡å—ï¼Œæ—¨åœ¨åŠ¨æ€æ•´åˆå†å²æ‰«æä¸å½“å‰æ‰«æçš„ç‰¹å¾ã€‚æ­¤å¤–ï¼Œç ”ç©¶è¿˜è®¾è®¡äº† BI-RADS ä¸€è‡´æ€§æ­£åˆ™åŒ– (BI-RADS Consistency Regularization, BCR) æŸå¤±å‡½æ•°ï¼Œé€šè¿‡å¯¹é½å…·æœ‰ç›¸ä¼¼æ”¾å°„å­¦è¯„ä¼°ç»“æœçš„æ½œåœ¨ç©ºé—´ï¼Œå°†é¢†åŸŸçŸ¥è¯†æœ‰æ•ˆåœ°åµŒå…¥æ¨¡å‹è®­ç»ƒä¸­ã€‚åœ¨é’ˆå¯¹é«˜é£é™©äººç¾¤çš„çºµå‘ DCE-MRI æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒLesiOnTime åœ¨ Dice æŒ‡æ ‡ä¸Šä¼˜äºç°æœ‰çš„å•æ—¶é—´ç‚¹åŠçºµå‘åˆ†å‰²åŸºå‡†æ¨¡å‹ 5%ã€‚æ¶ˆèå®éªŒè¿›ä¸€æ­¥è¯æ˜äº† TPA å’Œ BCR æ¨¡å—å¯¹æå‡æ—©æœŸç—…ç¶åˆ†å‰²æ€§èƒ½çš„äº’è¡¥æ€§ï¼Œçªæ˜¾äº†åœ¨çœŸå®ä¸´åºŠç­›æŸ¥åœºæ™¯ä¸­æ•´åˆæ—¶é—´ä¸ä¸´åºŠèƒŒæ™¯ä¿¡æ¯çš„å…³é”®ä»·å€¼ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.00496v2",
      "published_date": "2025-08-01 10:19:53 UTC",
      "updated_date": "2025-08-04 11:18:28 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T08:10:41.555503+00:00"
    },
    {
      "arxiv_id": "2508.00491v1",
      "title": "HannesImitation: Grasping with the Hannes Prosthetic Hand via Imitation Learning",
      "title_zh": "HannesImitationï¼šåŸºäºæ¨¡ä»¿å­¦ä¹ çš„ Hannes å‡æ‰‹æŠ“å–",
      "authors": [
        "Carlo Alessi",
        "Federico Vasile",
        "Federico Ceola",
        "Giulia Pasquale",
        "NicolÃ² Boccardo",
        "Lorenzo Natale"
      ],
      "abstract": "Recent advancements in control of prosthetic hands have focused on increasing autonomy through the use of cameras and other sensory inputs. These systems aim to reduce the cognitive load on the user by automatically controlling certain degrees of freedom. In robotics, imitation learning has emerged as a promising approach for learning grasping and complex manipulation tasks while simplifying data collection. Its application to the control of prosthetic hands remains, however, largely unexplored. Bridging this gap could enhance dexterity restoration and enable prosthetic devices to operate in more unconstrained scenarios, where tasks are learned from demonstrations rather than relying on manually annotated sequences. To this end, we present HannesImitationPolicy, an imitation learning-based method to control the Hannes prosthetic hand, enabling object grasping in unstructured environments. Moreover, we introduce the HannesImitationDataset comprising grasping demonstrations in table, shelf, and human-to-prosthesis handover scenarios. We leverage such data to train a single diffusion policy and deploy it on the prosthetic hand to predict the wrist orientation and hand closure for grasping. Experimental evaluation demonstrates successful grasps across diverse objects and conditions. Finally, we show that the policy outperforms a segmentation-based visual servo controller in unstructured scenarios. Additional material is provided on our project page: https://hsp-iit.github.io/HannesImitation",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†HannesImitationPolicyï¼Œä¸€ç§åŸºäºæ¨¡ä»¿å­¦ä¹ (Imitation Learning)çš„æ§åˆ¶æ–¹æ³•ï¼Œæ—¨åœ¨æå‡Hanneså‡æ‰‹åœ¨éç»“æ„åŒ–ç¯å¢ƒä¸­çš„æŠ“å–èƒ½åŠ›ã€‚ä¸ºäº†æ”¯æŒè¯¥ç³»ç»Ÿçš„è®­ç»ƒï¼Œç ”ç©¶å›¢é˜Ÿå¼•å…¥äº†HannesImitationDatasetï¼Œå…¶ä¸­åŒ…å«äº†æ¡Œé¢ã€è´§æ¶ä»¥åŠäººåˆ°å‡è‚¢äº¤æ¥åœºæ™¯ä¸‹çš„æŠ“å–æ¼”ç¤ºæ•°æ®ã€‚è¯¥æ–¹æ³•åˆ©ç”¨è¿™äº›æ•°æ®è®­ç»ƒäº†ä¸€ä¸ªå•ä¸€çš„æ‰©æ•£ç­–ç•¥(Diffusion Policy)ï¼Œç”¨äºé¢„æµ‹å‡è‚¢çš„æ‰‹è…•æ–¹å‘å’Œæ‰‹éƒ¨é—­åˆåŠ¨ä½œã€‚å®éªŒè¯„ä¼°è¯æ˜äº†è¯¥ç­–ç•¥åœ¨å¤šæ ·åŒ–ç‰©ä½“å’Œæ¡ä»¶ä¸‹å‡èƒ½å®ç°æˆåŠŸæŠ“å–ã€‚ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œåœ¨å¤„ç†éç»“æ„åŒ–åœºæ™¯æ—¶ï¼Œè¯¥ç­–ç•¥çš„æ€§èƒ½ä¼˜äºä¼ ç»Ÿçš„åŸºäºåˆ†å‰²çš„è§†è§‰ä¼ºæœæ§åˆ¶å™¨(Visual Servo Controller)ï¼Œæœ‰æ•ˆå¢å¼ºäº†å‡è‚¢è®¾å¤‡åœ¨æ— çº¦æŸç¯å¢ƒä¸‹çš„æ“ä½œçµæ´»æ€§ã€‚",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "comment": "Paper accepted at IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)",
      "pdf_url": "https://arxiv.org/pdf/2508.00491v1",
      "published_date": "2025-08-01 10:09:38 UTC",
      "updated_date": "2025-08-01 10:09:38 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T08:10:53.743323+00:00"
    },
    {
      "arxiv_id": "2508.02724v2",
      "title": "Veli: Unsupervised Method and Unified Benchmark for Low-Cost Air Quality Sensor Correction",
      "title_zh": "Veliï¼šé¢å‘ä½æˆæœ¬ç©ºæ°”è´¨é‡ä¼ æ„Ÿå™¨æ ¡æ­£çš„æ— ç›‘ç£æ–¹æ³•ä¸ç»Ÿä¸€åŸºå‡†",
      "authors": [
        "Yahia Dalbah",
        "Marcel Worring",
        "Yen-Chia Hsu"
      ],
      "abstract": "Urban air pollution is a major health crisis causing millions of premature deaths annually, underscoring the urgent need for accurate and scalable monitoring of air quality (AQ). While low-cost sensors (LCS) offer a scalable alternative to expensive reference-grade stations, their readings are affected by drift, calibration errors, and environmental interference. To address these challenges, we introduce Veli (Reference-free Variational Estimation via Latent Inference), an unsupervised Bayesian model that leverages variational inference to correct LCS readings without requiring co-location with reference stations, eliminating a major deployment barrier. Specifically, Veli constructs a disentangled representation of the LCS readings, effectively separating the true pollutant reading from the sensor noise. To build our model and address the lack of standardized benchmarks in AQ monitoring, we also introduce the Air Quality Sensor Data Repository (AQ-SDR). AQ-SDR is the largest AQ sensor benchmark to date, with readings from 23,737 LCS and reference stations across multiple regions. Veli demonstrates strong generalization across both in-distribution and out-of-distribution settings, effectively handling sensor drift and erratic sensor behavior. Code for model and dataset will be made public when this paper is published.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† Veli (Reference-free Variational Estimation via Latent Inference)ï¼Œè¿™æ˜¯ä¸€ç§æ— ç›‘ç£çš„è´å¶æ–¯æ¨¡å‹ï¼Œæ—¨åœ¨è§£å†³ä½æˆæœ¬ä¼ æ„Ÿå™¨ (LCS) åœ¨å¤§è§„æ¨¡éƒ¨ç½²ä¸­é¢ä¸´çš„æ¼‚ç§»ã€æ ¡å‡†è¯¯å·®åŠç¯å¢ƒå¹²æ‰°ç­‰æ ¸å¿ƒæŒ‘æˆ˜ã€‚Veli é‡‡ç”¨å˜åˆ†æ¨ç† (variational inference) æ¡†æ¶æ„å»ºä¼ æ„Ÿå™¨è¯»æ•°çš„è§£è€¦è¡¨ç¤º (disentangled representation)ï¼Œä»è€Œåœ¨æ— éœ€å‚è€ƒç«™å…±ç½®çš„æ¡ä»¶ä¸‹å®ç°é«˜ç²¾åº¦çš„è¯»æ•°ä¿®æ­£ï¼Œæ¶ˆé™¤äº†ä¸»è¦çš„éƒ¨ç½²éšœç¢ã€‚ä¸ºäº†è§£å†³ç©ºæ°”è´¨é‡ç›‘æµ‹é¢†åŸŸç¼ºä¹æ ‡å‡†åŒ–è¯„ä¼°ä½“ç³»çš„é—®é¢˜ï¼Œç ”ç©¶å›¢é˜ŸåŒæ­¥å‘å¸ƒäº† Air Quality Sensor Data Repository (AQ-SDR)ï¼Œè¿™æ˜¯ç›®å‰å…¨çƒè§„æ¨¡æœ€å¤§çš„ç©ºæ°”è´¨é‡ä¼ æ„Ÿå™¨åŸºå‡†æ•°æ®é›†ï¼Œæ¶µç›–äº† 23,737 ä¸ªç›‘æµ‹ç«™ç‚¹çš„è¯»æ•°ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒVeli åœ¨åˆ†å¸ƒå†…ä¸åˆ†å¸ƒå¤–æµ‹è¯•ä¸­å‡è¡¨ç°å‡ºæå¼ºçš„æ³›åŒ–èƒ½åŠ›ï¼Œèƒ½æœ‰æ•ˆåº”å¯¹ä¼ æ„Ÿå™¨çš„æ€§èƒ½è¡°å‡å’Œå¼‚å¸¸è¯»æ•°ã€‚è¯¥æ–¹æ³•çš„æå‡ºå¤§å¹…é™ä½äº†ç¯å¢ƒç›‘æµ‹çš„éƒ¨ç½²æˆæœ¬ï¼Œä¸ºæ„å»ºå…¨çƒæ€§ã€å¯æ‰©å±•çš„é«˜ç²¾åº¦ç©ºæ°”è´¨é‡ç›‘æµ‹ç½‘ç»œå¥ å®šäº†åŸºç¡€ã€‚",
      "categories": [
        "eess.SP",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "eess.SP",
      "comment": "Main content: 7 pages, 9 Figures, 3 Tables. Appendix: 4 pages, 6 Figures",
      "pdf_url": "https://arxiv.org/pdf/2508.02724v2",
      "published_date": "2025-08-01 10:06:28 UTC",
      "updated_date": "2025-11-12 09:43:55 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T08:11:00.851811+00:00"
    },
    {
      "arxiv_id": "2508.00478v1",
      "title": "CyGATE: Game-Theoretic Cyber Attack-Defense Engine for Patch Strategy Optimization",
      "title_zh": "CyGATEï¼šé¢å‘è¡¥ä¸ç­–ç•¥ä¼˜åŒ–çš„åšå¼ˆè®ºç½‘ç»œæ”»é˜²å¼•æ“",
      "authors": [
        "Yuning Jiang",
        "Nay Oo",
        "Qiaoran Meng",
        "Lu Lin",
        "Dusit Niyato",
        "Zehui Xiong",
        "Hoon Wei Lim",
        "Biplab Sikdar"
      ],
      "abstract": "Modern cyber attacks unfold through multiple stages, requiring defenders to dynamically prioritize mitigations under uncertainty. While game-theoretic models capture attacker-defender interactions, existing approaches often rely on static assumptions and lack integration with real-time threat intelligence, limiting their adaptability. This paper presents CyGATE, a game-theoretic framework modeling attacker-defender interactions, using large language models (LLMs) with retrieval-augmented generation (RAG) to enhance tactic selection and patch prioritization. Applied to a two-agent scenario, CyGATE frames cyber conflicts as a partially observable stochastic game (POSG) across Cyber Kill Chain stages. Both agents use belief states to navigate uncertainty, with the attacker adapting tactics and the defender re-prioritizing patches based on evolving risks and observed adversary behavior. The framework's flexible architecture enables extension to multi-agent scenarios involving coordinated attackers, collaborative defenders, or complex enterprise environments with multiple stakeholders. Evaluated in a dynamic patch scheduling scenario, CyGATE effectively prioritizes high-risk vulnerabilities, enhancing adaptability through dynamic threat integration, strategic foresight by anticipating attacker moves under uncertainty, and efficiency by optimizing resource use.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†CyGATEï¼Œä¸€ç§åŸºäºåšå¼ˆè®º(game-theoretic)çš„ç½‘ç»œæ”»é˜²å¼•æ“ï¼Œæ—¨åœ¨è§£å†³ç°ä»£å¤šé˜¶æ®µç½‘ç»œæ”»å‡»ä¸­é˜²å¾¡ç­–ç•¥åŠ¨æ€ä¼˜åŒ–çš„é—®é¢˜ã€‚è¯¥æ¡†æ¶å°†ç½‘ç»œå†²çªå»ºæ¨¡ä¸ºè·¨è¶Šç½‘ç»œæ”»å‡»é“¾(Cyber Kill Chain)é˜¶æ®µçš„éƒ¨åˆ†å¯è§‚å¯Ÿéšæœºåšå¼ˆ(POSG)ï¼Œå¹¶åˆ›æ–°æ€§åœ°åˆ©ç”¨å¤§è¯­è¨€æ¨¡å‹(LLMs)ä¸æ£€ç´¢å¢å¼ºç”Ÿæˆ(RAG)æŠ€æœ¯æ¥å¢å¼ºæˆ˜æœ¯é€‰æ‹©å’Œè¡¥ä¸ä¼˜å…ˆçº§æ’åºã€‚åœ¨åŒæ™ºèƒ½ä½“åœºæ™¯ä¸‹ï¼Œæ”»é˜²åŒæ–¹å‡é‡‡ç”¨ä¿¡å¿µçŠ¶æ€(belief states)æ¥åº”å¯¹ä¸ç¡®å®šæ€§ï¼Œé˜²å¾¡è€…èƒ½å¤Ÿæ ¹æ®å®æ—¶å¨èƒæƒ…æŠ¥å’Œè§‚å¯Ÿåˆ°çš„å¯¹æ‰‹è¡Œä¸ºåŠ¨æ€è°ƒæ•´ä¼˜å…ˆçº§ã€‚CyGATEçš„æ¶æ„å…·æœ‰é«˜åº¦çµæ´»æ€§ï¼Œæ”¯æŒæ‰©å±•è‡³å¤šæ™ºèƒ½ä½“åè°ƒæ”»å‡»æˆ–å¤æ‚ä¼ä¸šç¯å¢ƒä¸‹çš„åä½œé˜²å¾¡ã€‚å®éªŒç»“æœè¯æ˜ï¼ŒCyGATEèƒ½å¤Ÿæœ‰æ•ˆè¯†åˆ«é«˜é£é™©æ¼æ´ï¼Œå¹¶é€šè¿‡åŠ¨æ€å¨èƒé›†æˆå’Œæˆ˜ç•¥é¢„åˆ¤æ˜¾è‘—æå‡äº†é˜²å¾¡ç³»ç»Ÿçš„è‡ªé€‚åº”èƒ½åŠ›å’Œèµ„æºåˆ©ç”¨æ•ˆç‡ã€‚",
      "categories": [
        "cs.CR",
        "cs.AI"
      ],
      "primary_category": "cs.CR",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.00478v1",
      "published_date": "2025-08-01 09:53:06 UTC",
      "updated_date": "2025-08-01 09:53:06 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T08:11:06.654033+00:00"
    },
    {
      "arxiv_id": "2508.00459v1",
      "title": "Thinking Machines: Mathematical Reasoning in the Age of LLMs",
      "title_zh": "æ€ç»´æœºå™¨ï¼šå¤§è¯­è¨€æ¨¡å‹æ—¶ä»£çš„æ•°å­¦æ¨ç†",
      "authors": [
        "Andrea Asperti",
        "Alberto Naibo",
        "Claudio Sacerdoti Coen"
      ],
      "abstract": "Large Language Models (LLMs) have shown remarkable abilities in structured reasoning and symbolic tasks, with coding emerging as a particular area of strength. This success has sparked growing interest in applying LLMs to mathematics, both in informal problem-solving and formal theorem proving. However, progress in formal mathematics has proven to be significantly more difficult, despite surface-level similarities between programming and proof construction. This discrepancy raises important questions about how LLMs ``reason'', how they are supervised, and whether they internally track a notion of computational or deductive state. In this article, we address the state-of-the-art of the discipline, focusing on recent models and benchmarks, and explore three central issues at the intersection of machine learning and mathematical cognition: (i) the trade-offs between formal and informal mathematics as training domains; (ii) the deeper reasons why proof generation remains more brittle than code synthesis; (iii) and the question of whether LLMs represent, or merely mimic, a notion of evolving logical state. Our goal is not to draw hard boundaries, but to identify where the current limits lie, and how they might be extended.",
      "tldr_zh": "æœ¬æ–‡æ¢è®¨äº†å¤§è¯­è¨€æ¨¡å‹ (LLMs) åœ¨æ•°å­¦æ¨ç†é¢†åŸŸçš„åº”ç”¨ç°çŠ¶ï¼Œç‰¹åˆ«å…³æ³¨å…¶åœ¨éæ­£å¼é—®é¢˜è§£å†³å’Œæ­£å¼å®šç†è¯æ˜ (theorem proving) ä¸­çš„è¡¨ç°ã€‚å°½ç®¡ LLMs åœ¨ç¼–ç¨‹é¢†åŸŸè¡¨ç°å‡ºè‰²ï¼Œä½†åœ¨æ­£å¼æ•°å­¦æ¨ç†æ–¹é¢ä»é¢ä¸´å·¨å¤§æŒ‘æˆ˜ï¼Œä½œè€…å¯¹è¿™ä¸€ç°è±¡èƒŒåçš„æ¨ç†æœºåˆ¶ã€ç›‘ç£æ–¹å¼ä»¥åŠè®¡ç®—çŠ¶æ€è¿½è¸ªå±•å¼€äº†æ·±å…¥åˆ†æã€‚è®ºæ–‡é‡ç‚¹è®¨è®ºäº†ä¸‰ä¸ªæ ¸å¿ƒè®®é¢˜ï¼šæ­£å¼ä¸éæ­£å¼æ•°å­¦ä½œä¸ºè®­ç»ƒé¢†åŸŸçš„æƒè¡¡ã€è¯æ˜ç”Ÿæˆç›¸è¾ƒäºä»£ç åˆæˆ (code synthesis) æ›´åŠ è„†å¼±çš„æ·±å±‚åŸå› ï¼Œä»¥åŠæ¨¡å‹æ˜¯çœŸå®è¡¨å¾è¿˜æ˜¯ä»…ä»…æ¨¡ä»¿é€»è¾‘çŠ¶æ€ã€‚ç ”ç©¶é€šè¿‡è¯„ä¼°å½“å‰æœ€å‰æ²¿çš„æ¨¡å‹å’ŒåŸºå‡†æµ‹è¯• (benchmarks)ï¼Œæ­ç¤ºäº†æœºå™¨å­¦ä¹ ä¸æ•°å­¦è®¤çŸ¥äº¤æ±‡å¤„çš„å…³é”®ç“¶é¢ˆã€‚è¯¥æ–‡æ—¨åœ¨æ˜ç¡®å½“å‰æŠ€æœ¯çš„è¾¹ç•Œï¼Œå¹¶ä¸ºå¦‚ä½•æ‰©å±• LLMs åœ¨å¤„ç†å¤æ‚é€»è¾‘ä¸æ¼”ç»ä»»åŠ¡æ—¶çš„å±€é™æ€§æä¾›äº†æ–¹å‘æ€§æŒ‡å¯¼ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.00459v1",
      "published_date": "2025-08-01 09:31:48 UTC",
      "updated_date": "2025-08-01 09:31:48 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T08:11:08.454752+00:00"
    },
    {
      "arxiv_id": "2508.00452v2",
      "title": "M^2VAE: Multi-Modal Multi-View Variational Autoencoder for Cold-start Item Recommendation",
      "title_zh": "M^2VAEï¼šé¢å‘å†·å¯åŠ¨é¡¹ç›®æ¨èçš„å¤šæ¨¡æ€å¤šè§†å›¾å˜åˆ†è‡ªç¼–ç å™¨",
      "authors": [
        "Chuan He",
        "Yongchao Liu",
        "Qiang Li",
        "Wenliang Zhong",
        "Chuntao Hong",
        "Xinwei Yao"
      ],
      "abstract": "Cold-start item recommendation is a significant challenge in recommendation systems, particularly when new items are introduced without any historical interaction data. While existing methods leverage multi-modal content to alleviate the cold-start issue, they often neglect the inherent multi-view structure of modalities, the distinction between shared and modality-specific features. In this paper, we propose Multi-Modal Multi-View Variational AutoEncoder (M^2VAE), a generative model that addresses the challenges of modeling common and unique views in attribute and multi-modal features, as well as user preferences over single-typed item features. Specifically, we generate type-specific latent variables for item IDs, categorical attributes, and image features, and use Product-of-Experts (PoE) to derive a common representation. A disentangled contrastive loss decouples the common view from unique views while preserving feature informativeness. To model user inclinations, we employ a preference-guided Mixture-of-Experts (MoE) to adaptively fuse representations. We further incorporate co-occurrence signals via contrastive learning, eliminating the need for pretraining. Extensive experiments on real-world datasets validate the effectiveness of our approach.",
      "tldr_zh": "è¯¥è®ºæ–‡é’ˆå¯¹æ¨èç³»ç»Ÿä¸­ç”±äºç¼ºä¹å†å²äº¤äº’æ•°æ®è€Œå¯¼è‡´çš„å†·å¯åŠ¨é¡¹æ¨è(Cold-start item recommendation)éš¾é¢˜ï¼Œæå‡ºäº†åä¸ºMulti-Modal Multi-View Variational Autoencoder (M^2VAE)çš„ç”Ÿæˆæ¨¡å‹ã€‚M^2VAEé€šè¿‡ä¸ºé¡¹ç›®IDã€ç±»åˆ«å±æ€§å’Œå›¾åƒç‰¹å¾ç”Ÿæˆç‰¹å®šç±»å‹çš„æ½œåœ¨å˜é‡ï¼Œå¹¶åˆ©ç”¨Product-of-Experts (PoE)æ¨å¯¼å‡ºå…±åŒè¡¨ç¤ºï¼Œæœ‰æ•ˆè§£å†³äº†å¤šæ¨¡æ€ç‰¹å¾ä¸­å…±æœ‰è§†å›¾ä¸ç‹¬æœ‰è§†å›¾çš„å»ºæ¨¡é—®é¢˜ã€‚ç ”ç©¶å¼•å…¥äº†è§£è€¦å¯¹æ¯”æŸå¤±(disentangled contrastive loss)æ¥åˆ†ç¦»å…±åŒè§†å›¾ä¸ç‹¬ç‰¹è§†å›¾ï¼ŒåŒæ—¶é‡‡ç”¨åå¥½å¼•å¯¼çš„æ··åˆä¸“å®¶æ¨¡å‹(preference-guided Mixture-of-Experts, MoE)æ¥è‡ªé€‚åº”èåˆè¡¨ç¤ºä»¥æ•æ‰ç”¨æˆ·å€¾å‘ã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•é€šè¿‡å¯¹æ¯”å­¦ä¹ æ•´åˆå…±ç°ä¿¡å·ï¼Œæ¶ˆé™¤äº†å¯¹æ¨¡å‹é¢„è®­ç»ƒçš„ä¾èµ–ã€‚åœ¨çœŸå®æ•°æ®é›†ä¸Šçš„å¹¿æ³›å®éªŒéªŒè¯äº†M^2VAEåœ¨å¤„ç†å†·å¯åŠ¨æ¨èä»»åŠ¡ä¸­çš„æœ‰æ•ˆæ€§å’Œä¼˜è¶Šæ€§ã€‚",
      "categories": [
        "cs.IR",
        "cs.AI"
      ],
      "primary_category": "cs.IR",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.00452v2",
      "published_date": "2025-08-01 09:16:26 UTC",
      "updated_date": "2025-11-12 08:10:25 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T08:11:06.840136+00:00"
    },
    {
      "arxiv_id": "2508.00450v2",
      "title": "Dual Collaborative LLMs via Continual Fine-Tuning for Serendipitous Recommendation",
      "title_zh": "é¢å‘æƒŠå–œæ¨èçš„åŸºäºæŒç»­å¾®è°ƒçš„åŒååŒ LLM",
      "authors": [
        "Hongxiang Lin",
        "Hao Guo",
        "Zeshun Li",
        "Erpeng Xue",
        "Yongqian He",
        "Xiangyu Hou",
        "Zhaoyu Hu",
        "Lei Wang",
        "Sheng Chen"
      ],
      "abstract": "Traditional recommendation systems tend to trap users in strong feedback loops by excessively pushing content aligned with their historical preferences, thereby limiting exploration opportunities and causing content fatigue. Although large language models (LLMs) demonstrate potential with their diverse content generation capabilities, existing LLM-enhanced dual-model frameworks face two major limitations: first, they overlook long-term preferences driven by group identity, leading to biased interest modeling; second, they suffer from static optimization flaws, as a one-time alignment process fails to leverage incremental user data for closed-loop optimization. To address these challenges, we propose the Co-Evolutionary Alignment (CoEA) method. For interest modeling bias, we introduce Dual-Stable Interest Exploration (DSIE) module, jointly modeling long-term group identity and short-term individual interests through parallel processing of behavioral sequences. For static optimization limitations, we design a Periodic Collaborative Optimization (PCO) mechanism. This mechanism regularly conducts preference verification on incremental data using the Relevance LLM, then guides the Novelty LLM to perform fine-tuning based on the verification results, and subsequently feeds back the output of the continually fine-tuned Novelty LLM to the Relevance LLM for re-evaluation, thereby achieving a dynamic closed-loop optimization. Extensive online and offline experiments verify the effectiveness of the CoEA model in serendipitous recommendation.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† Co-Evolutionary Alignment (CoEA) æ–¹æ³•ï¼Œæ—¨åœ¨è§£å†³ä¼ ç»Ÿæ¨èç³»ç»Ÿå› è¿‡åº¦æ¨é€å†å²åå¥½å¯¼è‡´çš„åé¦ˆå¾ªç¯å’Œå†…å®¹ç–²åŠ³é—®é¢˜ã€‚é’ˆå¯¹ç°æœ‰ Large Language Models (LLMs) å¢å¼ºæ¡†æ¶åœ¨å…´è¶£å»ºæ¨¡åå·®å’Œé™æ€ä¼˜åŒ–æ–¹é¢çš„å±€é™ï¼Œè¯¥ç ”ç©¶å¼•å…¥äº† Dual-Stable Interest Exploration (DSIE) æ¨¡å—ï¼Œé€šè¿‡å¹¶è¡Œå¤„ç†è¡Œä¸ºåºåˆ—åŒæ—¶å»ºæ¨¡é•¿æœŸç¾¤ä½“èº«ä»½å’ŒçŸ­æœŸä¸ªäººå…´è¶£ã€‚ä¸ºäº†å…‹æœé™æ€ä¼˜åŒ–ç¼ºé™·ï¼Œç ”ç©¶è¿›ä¸€æ­¥è®¾è®¡äº† Periodic Collaborative Optimization (PCO) æœºåˆ¶ï¼Œåˆ©ç”¨ Relevance LLM å¯¹å¢é‡æ•°æ®è¿›è¡Œå®šæœŸéªŒè¯ï¼Œå¹¶æŒ‡å¯¼ Novelty LLM è¿›è¡ŒæŒç»­å¾®è°ƒï¼Œä»è€Œå®ç°åŠ¨æ€é—­ç¯ä¼˜åŒ–ã€‚å®éªŒç»“æœè¯æ˜ï¼ŒCoEA åœ¨ Serendipitous Recommendation åœºæ™¯ä¸­å…·æœ‰æ˜¾è‘—ä¼˜åŠ¿ï¼Œèƒ½å¤Ÿæœ‰æ•ˆæå‡æ¨èçš„å¤šæ ·æ€§ä¸æƒŠå–œæ„Ÿã€‚å¹¿æ³›çš„åœ¨çº¿å’Œç¦»çº¿å®éªŒéªŒè¯äº†è¯¥æ¨¡å‹åœ¨æ•æ‰ç”¨æˆ·æ½œåœ¨å…´è¶£å’Œç¼“è§£å†…å®¹ç–²åŠ³æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚",
      "categories": [
        "cs.IR",
        "cs.AI"
      ],
      "primary_category": "cs.IR",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.00450v2",
      "published_date": "2025-08-01 09:10:56 UTC",
      "updated_date": "2025-12-08 02:38:51 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T08:11:12.847153+00:00"
    },
    {
      "arxiv_id": "2508.00442v1",
      "title": "TopoTTA: Topology-Enhanced Test-Time Adaptation for Tubular Structure Segmentation",
      "title_zh": "TopoTTAï¼šé¢å‘ç®¡çŠ¶ç»“æ„åˆ†å‰²çš„æ‹“æ‰‘å¢å¼ºæµ‹è¯•æ—¶è‡ªé€‚åº”",
      "authors": [
        "Jiale Zhou",
        "Wenhan Wang",
        "Shikun Li",
        "Xiaolei Qu",
        "Xin Guo",
        "Yizhong Liu",
        "Wenzhong Tang",
        "Xun Lin",
        "Yefeng Zheng"
      ],
      "abstract": "Tubular structure segmentation (TSS) is important for various applications, such as hemodynamic analysis and route navigation. Despite significant progress in TSS, domain shifts remain a major challenge, leading to performance degradation in unseen target domains. Unlike other segmentation tasks, TSS is more sensitive to domain shifts, as changes in topological structures can compromise segmentation integrity, and variations in local features distinguishing foreground from background (e.g., texture and contrast) may further disrupt topological continuity. To address these challenges, we propose Topology-enhanced Test-Time Adaptation (TopoTTA), the first test-time adaptation framework designed specifically for TSS. TopoTTA consists of two stages: Stage 1 adapts models to cross-domain topological discrepancies using the proposed Topological Meta Difference Convolutions (TopoMDCs), which enhance topological representation without altering pre-trained parameters; Stage 2 improves topological continuity by a novel Topology Hard sample Generation (TopoHG) strategy and prediction alignment on hard samples with pseudo-labels in the generated pseudo-break regions. Extensive experiments across four scenarios and ten datasets demonstrate TopoTTA's effectiveness in handling topological distribution shifts, achieving an average improvement of 31.81% in clDice. TopoTTA also serves as a plug-and-play TTA solution for CNN-based TSS models.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ç®¡çŠ¶ç»“æ„åˆ†å‰²(Tubular structure segmentation, TSS)åœ¨é¢å¯¹é¢†åŸŸåç§»(domain shifts)æ—¶å®¹æ˜“å‡ºç°æ‹“æ‰‘ç»“æ„å®Œæ•´æ€§å—æŸçš„é—®é¢˜ï¼Œæå‡ºäº†é¦–ä¸ªä¸“é—¨çš„æµ‹è¯•æ—¶è‡ªé€‚åº”æ¡†æ¶TopoTTAã€‚è¯¥æ¡†æ¶åˆ†ä¸ºä¸¤ä¸ªé˜¶æ®µï¼Œç¬¬ä¸€é˜¶æ®µåˆ©ç”¨æ‹“æ‰‘å…ƒå·®åˆ†å·ç§¯(Topological Meta Difference Convolutions, TopoMDCs)åœ¨ä¸ä¿®æ”¹é¢„è®­ç»ƒå‚æ•°çš„å‰æä¸‹å¢å¼ºæ¨¡å‹å¯¹è·¨åŸŸæ‹“æ‰‘å·®å¼‚çš„è¡¨å¾èƒ½åŠ›ã€‚ç¬¬äºŒé˜¶æ®µé€šè¿‡æ‹“æ‰‘å›°éš¾æ ·æœ¬ç”Ÿæˆ(Topology Hard sample Generation, TopoHG)ç­–ç•¥å’Œä¼ªæ–­è£‚åŒºåŸŸçš„é¢„æµ‹å¯¹é½ï¼Œæ˜¾è‘—æ”¹å–„äº†åˆ†å‰²ç»“æœçš„æ‹“æ‰‘è¿é€šæ€§ã€‚åœ¨å››ä¸ªåœºæ™¯å’Œåä¸ªæ•°æ®é›†ä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼ŒTopoTTAèƒ½æœ‰æ•ˆåº”å¯¹æ‹“æ‰‘åˆ†å¸ƒåç§»ï¼Œä½¿clDiceæŒ‡æ ‡å¹³å‡æå‡äº†31.81%ã€‚ä½œä¸ºä¸€ç§å³æ’å³ç”¨çš„è§£å†³æ–¹æ¡ˆï¼ŒTopoTTAèƒ½å¤Ÿé€‚é…ç°æœ‰çš„åŸºäºå·ç§¯ç¥ç»ç½‘ç»œ(CNN)çš„TSSæ¨¡å‹ï¼Œä¸ºè¡€æµåŠ¨åŠ›å­¦åˆ†æå’Œè·¯å¾„å¯¼èˆªç­‰åº”ç”¨æä¾›äº†æ›´ç¨³å¥çš„æŠ€æœ¯æ”¯æŒã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.00442v1",
      "published_date": "2025-08-01 08:59:13 UTC",
      "updated_date": "2025-08-01 08:59:13 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T08:11:17.044698+00:00"
    },
    {
      "arxiv_id": "2508.00440v1",
      "title": "Reducing the gap between general purpose data and aerial images in concentrated solar power plants",
      "title_zh": "ç¼©å°èšå…‰å¤ªé˜³èƒ½ç”µç«™ä¸­é€šç”¨æ•°æ®ä¸èˆªæ‹å›¾åƒä¹‹é—´çš„å·®è·",
      "authors": [
        "M. A. PÃ©rez-CutiÃ±o",
        "J. Valverde",
        "J. CapitÃ¡n",
        "J. M. DÃ­az-BÃ¡Ã±ez"
      ],
      "abstract": "In the context of Concentrated Solar Power (CSP) plants, aerial images captured by drones present a unique set of challenges. Unlike urban or natural landscapes commonly found in existing datasets, solar fields contain highly reflective surfaces, and domain-specific elements that are uncommon in traditional computer vision benchmarks. As a result, machine learning models trained on generic datasets struggle to generalize to this setting without extensive retraining and large volumes of annotated data. However, collecting and labeling such data is costly and time-consuming, making it impractical for rapid deployment in industrial applications.\n  To address this issue, we propose a novel approach: the creation of AerialCSP, a virtual dataset that simulates aerial imagery of CSP plants. By generating synthetic data that closely mimic real-world conditions, our objective is to facilitate pretraining of models before deployment, significantly reducing the need for extensive manual labeling. Our main contributions are threefold: (1) we introduce AerialCSP, a high-quality synthetic dataset for aerial inspection of CSP plants, providing annotated data for object detection and image segmentation; (2) we benchmark multiple models on AerialCSP, establishing a baseline for CSP-related vision tasks; and (3) we demonstrate that pretraining on AerialCSP significantly improves real-world fault detection, particularly for rare and small defects, reducing the need for extensive manual labeling. AerialCSP is made publicly available at https://mpcutino.github.io/aerialcsp/.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹èšå…‰å¤ªé˜³èƒ½çƒ­å‘ç”µ(CSP)ç”µç«™æ— äººæœºèˆªæ‹å›¾åƒä¸­å­˜åœ¨çš„é«˜åå°„è¡¨é¢å’Œç‰¹å®šé¢†åŸŸå…ƒç´ ç­‰æŒ‘æˆ˜ï¼Œæå‡ºäº†è™šæ‹Ÿæ•°æ®é›†AerialCSPã€‚ç”±äºé€šç”¨æ•°æ®é›†è®­ç»ƒçš„æ¨¡å‹åœ¨CSPåœºæ™¯ä¸‹æ³›åŒ–èƒ½åŠ›å·®ï¼Œä¸”è·å–çœŸå®æ ‡æ³¨æ•°æ®æˆæœ¬æé«˜ï¼ŒAerialCSPé€šè¿‡æ¨¡æ‹ŸçœŸå®ç¯å¢ƒç”Ÿæˆé«˜è´¨é‡åˆæˆæ•°æ®ï¼Œä¸ºç›®æ ‡æ£€æµ‹(object detection)å’Œå›¾åƒåˆ†å‰²(image segmentation)æä¾›å¤§è§„æ¨¡æ ‡æ³¨ä¿¡æ¯ã€‚ç ”ç©¶å›¢é˜Ÿåœ¨AerialCSPä¸Šå¯¹å¤šä¸ªæ¨¡å‹è¿›è¡Œäº†åŸºå‡†æµ‹è¯•(benchmark)ï¼Œä¸ºCSPç›¸å…³è§†è§‰ä»»åŠ¡å»ºç«‹äº†æ€§èƒ½åŸºå‡†ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œåœ¨AerialCSPä¸Šè¿›è¡Œé¢„è®­ç»ƒ(pretraining)èƒ½æ˜¾è‘—æå‡çœŸå®åœºæ™¯ä¸­çš„æ•…éšœæ£€æµ‹èƒ½åŠ›ï¼Œç‰¹åˆ«æ˜¯é’ˆå¯¹ç¨€æœ‰å’Œå¾®å°ç¼ºé™·çš„æ£€æµ‹æ•ˆæœã€‚è¯¥ç ”ç©¶æˆåŠŸç¼©å°äº†é€šç”¨æ•°æ®ä¸CSPèˆªæ‹å›¾åƒä¹‹é—´çš„å·®è·ï¼Œå¤§å¹…é™ä½äº†å·¥ä¸šåº”ç”¨ä¸­å¯¹å¤§è§„æ¨¡äººå·¥æ ‡æ³¨çš„ä¾èµ–ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.RO"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.00440v1",
      "published_date": "2025-08-01 08:57:02 UTC",
      "updated_date": "2025-08-01 08:57:02 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T08:11:25.671598+00:00"
    },
    {
      "arxiv_id": "2508.00956v2",
      "title": "Learning Unified User Quantized Tokenizers for User Representation",
      "title_zh": "é¢å‘ç”¨æˆ·è¡¨ç¤ºçš„ç»Ÿä¸€ç”¨æˆ·é‡åŒ–åˆ†è¯å™¨å­¦ä¹ ",
      "authors": [
        "Chuan He",
        "Yang Chen",
        "Wuliang Huang",
        "Tianyi Zheng",
        "Jianhu Chen",
        "Bin Dou",
        "Yice Luo",
        "Yun Zhu",
        "Baokun Wang",
        "Yongchao Liu",
        "Xing Fu",
        "Yu Cheng",
        "Chuntao Hong",
        "Weiqiang Wang",
        "Xin-Wei Yao",
        "Zhongle Xie"
      ],
      "abstract": "Multi-source user representation learning plays a critical role in enabling personalized services on web platforms (e.g., Alipay). While prior works have adopted late-fusion strategies to combine heterogeneous data sources, they suffer from three key limitations: lack of unified representation frameworks, scalability and storage issues in data compression, and inflexible cross-task generalization. To address these challenges, we propose U2QT (Unified User Quantized Tokenizers), a novel framework that integrates cross-domain knowledge transfer with early fusion of heterogeneous domains. Our framework employs a two-stage architecture: first, we use the Qwen3 Embedding model to derive a compact yet expressive feature representation; second, a multi-view RQ-VAE discretizes causal embeddings into compact tokens through shared and source-specific codebooks, enabling efficient storage while maintaining semantic coherence. Experimental results showcase U2QT's advantages across diverse downstream tasks, outperforming task-specific baselines in future behavior prediction and recommendation tasks while achieving efficiency gains in storage and computation. The unified tokenization framework enables seamless integration with language models and supports industrial-scale applications.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤šæºç”¨æˆ·è¡¨ç¤ºå­¦ä¹ ä¸­å­˜åœ¨çš„ç»Ÿä¸€æ¡†æ¶ç¼ºå¤±ã€æ‰©å±•æ€§ä¸å­˜å‚¨æ•ˆç‡ä½ä¸‹ä»¥åŠè·¨ä»»åŠ¡æ³›åŒ–ä¸çµæ´»ç­‰æŒ‘æˆ˜ï¼Œæå‡ºäº†åä¸ºU2QTï¼ˆUnified User Quantized Tokenizersï¼‰çš„æ–°å‹æ¡†æ¶ã€‚U2QTé€šè¿‡è·¨é¢†åŸŸçŸ¥è¯†è¿ç§»ä¸å¼‚æ„é¢†åŸŸçš„æ—©æœŸèåˆï¼ˆearly fusionï¼‰æŠ€æœ¯ï¼Œé‡‡ç”¨ä¸¤é˜¶æ®µæ¶æ„å®ç°é«˜æ•ˆçš„ç”¨æˆ·è¡¨ç¤ºã€‚ç¬¬ä¸€é˜¶æ®µåˆ©ç”¨Qwen3 Embeddingæ¨¡å‹æå–ç´§å‡‘ä¸”è¡¨è¾¾åŠ›å¼ºçš„ç‰¹å¾ï¼Œç¬¬äºŒé˜¶æ®µåˆ™é€šè¿‡å¤šè§†å›¾RQ-VAEå°†å› æœåµŒå…¥ï¼ˆcausal embeddingsï¼‰ç¦»æ•£åŒ–ä¸ºTokenï¼Œå¹¶ç»“åˆå…±äº«å’Œæºç‰¹å®šç æœ¬ï¼ˆcodebooksï¼‰åœ¨é™ä½å­˜å‚¨éœ€æ±‚çš„åŒæ—¶ç»´æŒè¯­ä¹‰ä¸€è‡´æ€§ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒU2QTåœ¨è¡Œä¸ºé¢„æµ‹å’Œæ¨èä»»åŠ¡ä¸­å‡è¶…è¶Šäº†ä¼ ç»Ÿçš„ç‰¹å®šä»»åŠ¡åŸºçº¿æ¨¡å‹ï¼Œæ˜¾è‘—æå‡äº†å­˜å‚¨ä¸è®¡ç®—æ•ˆç‡ã€‚è¯¥æ¡†æ¶çš„ç»Ÿä¸€TokenåŒ–è®¾è®¡ä½¿å…¶èƒ½ä¸å¤§è¯­è¨€æ¨¡å‹ï¼ˆLMsï¼‰æ— ç¼é›†æˆï¼Œä¸ºå·¥ä¸šçº§è§„æ¨¡çš„ä¸ªæ€§åŒ–æœåŠ¡åº”ç”¨æä¾›äº†å¼ºæœ‰åŠ›çš„æ”¯æŒã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.IR"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.00956v2",
      "published_date": "2025-08-01 08:35:32 UTC",
      "updated_date": "2025-09-30 01:51:32 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T08:11:34.845232+00:00"
    },
    {
      "arxiv_id": "2508.00427v1",
      "title": "Contact-Aware Amodal Completion for Human-Object Interaction via Multi-Regional Inpainting",
      "title_zh": "åŸºäºå¤šåŒºåŸŸä¿®å¤çš„äººç‰©äº¤äº’æ¥è§¦æ„ŸçŸ¥éæ¨¡æ€è¡¥å…¨",
      "authors": [
        "Seunggeun Chi",
        "Enna Sachdeva",
        "Pin-Hao Huang",
        "Kwonjoon Lee"
      ],
      "abstract": "Amodal completion, which is the process of inferring the full appearance of objects despite partial occlusions, is crucial for understanding complex human-object interactions (HOI) in computer vision and robotics. Existing methods, such as those that use pre-trained diffusion models, often struggle to generate plausible completions in dynamic scenarios because they have a limited understanding of HOI. To solve this problem, we've developed a new approach that uses physical prior knowledge along with a specialized multi-regional inpainting technique designed for HOI. By incorporating physical constraints from human topology and contact information, we define two distinct regions: the primary region, where occluded object parts are most likely to be, and the secondary region, where occlusions are less probable. Our multi-regional inpainting method uses customized denoising strategies across these regions within a diffusion model. This improves the accuracy and realism of the generated completions in both their shape and visual detail. Our experimental results show that our approach significantly outperforms existing methods in HOI scenarios, moving machine perception closer to a more human-like understanding of dynamic environments. We also show that our pipeline is robust even without ground-truth contact annotations, which broadens its applicability to tasks like 3D reconstruction and novel view/pose synthesis.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹äººæœºäº¤äº’ (Human-Object Interaction, HOI) åœºæ™¯ä¸‹çš„éå…¨è²Œè¡¥å…¨ (Amodal completion) é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§ç»“åˆç‰©ç†å…ˆéªŒçŸ¥è¯†ä¸ä¸“é—¨è®¾è®¡çš„ Multi-Regional Inpainting æŠ€æœ¯çš„æ–°æ–¹æ³•ã€‚é€šè¿‡å¼•å…¥äººç±»æ‹“æ‰‘ç»“æ„å’Œæ¥è§¦ä¿¡æ¯çš„ç‰©ç†çº¦æŸï¼Œè¯¥æ–¹æ³•å°†å¾…è¡¥å…¨åŒºåŸŸåˆ’åˆ†ä¸ºä¸»ã€æ¬¡ä¸¤ä¸ªéƒ¨åˆ†ï¼Œå¹¶åœ¨ Diffusion Model æ¡†æ¶ä¸‹åº”ç”¨å®šåˆ¶åŒ–çš„å»å™ªç­–ç•¥ã€‚è¿™ç§å¤šåŒºåŸŸä¿®å¤æ‰‹æ®µæ˜¾è‘—æå‡äº†ç”Ÿæˆç»“æœåœ¨å½¢çŠ¶å‡†ç¡®æ€§å’Œè§†è§‰ç»†èŠ‚ä¸Šçš„çœŸå®æ„Ÿï¼Œæœ‰æ•ˆè§£å†³äº†ç°æœ‰æ‰©æ•£æ¨¡å‹åœ¨åŠ¨æ€åœºæ™¯ä¸­å¯¹äº¤äº’ç†è§£ä¸è¶³çš„é—®é¢˜ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨ HOI åœºæ™¯ä¸­çš„è¡¨ç°æ˜¾è‘—ä¼˜äºç°æœ‰æ¨¡å‹ï¼Œä¸”åœ¨ç¼ºä¹ ground-truth æ¥è§¦æ ‡æ³¨æ—¶ä¾ç„¶è¡¨ç°å‡ºæå¼ºçš„é²æ£’æ€§ã€‚è¯¥ç ”ç©¶æˆæœä¸ä»…æ¨åŠ¨äº†æœºå™¨æ„ŸçŸ¥å¯¹åŠ¨æ€ç¯å¢ƒçš„ç†è§£ï¼Œè¿˜å¯å¹¿æ³›æ‰©å±•è‡³ 3D é‡å»ºã€æ–°è§†è§’åˆæˆåŠå§¿æ€åˆæˆç­‰å¤šç§è®¡ç®—æœºè§†è§‰ä»»åŠ¡ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "ICCV 2025 (Highlight)",
      "pdf_url": "https://arxiv.org/pdf/2508.00427v1",
      "published_date": "2025-08-01 08:33:45 UTC",
      "updated_date": "2025-08-01 08:33:45 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T08:11:31.352997+00:00"
    },
    {
      "arxiv_id": "2508.00414v2",
      "title": "Cognitive Kernel-Pro: A Framework for Deep Research Agents and Agent Foundation Models Training",
      "title_zh": "Cognitive Kernel-Proï¼šæ·±åº¦ç ”ç©¶æ™ºèƒ½ä½“ä¸æ™ºèƒ½ä½“åŸºç¡€æ¨¡å‹è®­ç»ƒæ¡†æ¶",
      "authors": [
        "Tianqing Fang",
        "Zhisong Zhang",
        "Xiaoyang Wang",
        "Rui Wang",
        "Can Qin",
        "Yuxuan Wan",
        "Jun-Yu Ma",
        "Ce Zhang",
        "Jiaqi Chen",
        "Xiyun Li",
        "Hongming Zhang",
        "Haitao Mi",
        "Dong Yu"
      ],
      "abstract": "General AI Agents are increasingly recognized as foundational frameworks for the next generation of artificial intelligence, enabling complex reasoning, web interaction, coding, and autonomous research capabilities. However, current agent systems are either closed-source or heavily reliant on a variety of paid APIs and proprietary tools, limiting accessibility and reproducibility for the research community. In this work, we present \\textbf{Cognitive Kernel-Pro}, a fully open-source and (to the maximum extent) free multi-module agent framework designed to democratize the development and evaluation of advanced AI agents. Within Cognitive Kernel-Pro, we systematically investigate the curation of high-quality training data for Agent Foundation Models, focusing on the construction of queries, trajectories, and verifiable answers across four key domains: web, file, code, and general reasoning. Furthermore, we explore novel strategies for agent test-time reflection and voting to enhance agent robustness and performance. We evaluate Cognitive Kernel-Pro on GAIA, achieving state-of-the-art results among open-source and free agents. Notably, our 8B-parameter open-source model surpasses previous leading systems such as WebDancer and WebSailor, establishing a new performance standard for accessible, high-capability AI agents. Code is available at https://github.com/Tencent/CognitiveKernel-Pro",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†Cognitive Kernel-Proï¼Œä¸€ä¸ªå…¨å¼€æºä¸”å…è´¹çš„å¤šæ¨¡å—æ™ºèƒ½ä½“æ¡†æ¶ï¼Œæ—¨åœ¨é€šè¿‡é™ä½å¯¹ä»˜è´¹APIå’Œä¸“æœ‰å·¥å…·çš„ä¾èµ–æ¥æ¨åŠ¨é«˜çº§AIæ™ºèƒ½ä½“(AI Agents)å¼€å‘çš„æ°‘ä¸»åŒ–ã€‚ç ”ç©¶å›¢é˜Ÿé’ˆå¯¹Agent Foundation Modelsç³»ç»Ÿæ€§åœ°æ„å»ºäº†æ¶µç›–webã€fileã€codeå’Œgeneral reasoningå››ä¸ªé¢†åŸŸçš„é«˜è´¨é‡è®­ç»ƒæ•°æ®ï¼ŒåŒ…æ‹¬æŸ¥è¯¢ã€è½¨è¿¹å’Œå¯éªŒè¯ç­”æ¡ˆã€‚æ­¤å¤–ï¼Œè¯¥æ¡†æ¶å¼•å…¥äº†åˆ›æ–°çš„æµ‹è¯•æ—¶åæ€(test-time reflection)å’ŒæŠ•ç¥¨ç­–ç•¥ï¼Œä»¥æ˜¾è‘—æå‡æ™ºèƒ½ä½“åœ¨å¤æ‚ä»»åŠ¡ä¸­çš„é²æ£’æ€§ä¸æ€§èƒ½ã€‚å®éªŒè¡¨æ˜ï¼ŒCognitive Kernel-Proåœ¨GAIAåŸºå‡†æµ‹è¯•ä¸­å–å¾—äº†å¼€æºä¸å…è´¹æ™ºèƒ½ä½“ç±»åˆ«ä¸‹çš„State-of-the-Art(SOTA)æ€§èƒ½ã€‚å…¶ä¸­ï¼Œå…¶8Bå‚æ•°çš„å¼€æºæ¨¡å‹è¡¨ç°è¶…è¿‡äº†WebDancerå’ŒWebSailorç­‰å‰æ²¿ç³»ç»Ÿï¼Œä¸ºæ„å»ºé«˜æ€§èƒ½ä¸”æ˜“äºè·å–çš„AIæ™ºèƒ½ä½“è®¾ç«‹äº†æ–°çš„æ ‡æ†ã€‚",
      "categories": [
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.AI",
      "comment": "16 pages",
      "pdf_url": "https://arxiv.org/pdf/2508.00414v2",
      "published_date": "2025-08-01 08:11:31 UTC",
      "updated_date": "2025-08-12 11:57:13 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T08:11:54.832383+00:00"
    },
    {
      "arxiv_id": "2508.00413v1",
      "title": "DC-AE 1.5: Accelerating Diffusion Model Convergence with Structured Latent Space",
      "title_zh": "DC-AE 1.5ï¼šåˆ©ç”¨ç»“æ„åŒ–æ½œç©ºé—´åŠ é€Ÿæ‰©æ•£æ¨¡å‹æ”¶æ•›",
      "authors": [
        "Junyu Chen",
        "Dongyun Zou",
        "Wenkun He",
        "Junsong Chen",
        "Enze Xie",
        "Song Han",
        "Han Cai"
      ],
      "abstract": "We present DC-AE 1.5, a new family of deep compression autoencoders for high-resolution diffusion models. Increasing the autoencoder's latent channel number is a highly effective approach for improving its reconstruction quality. However, it results in slow convergence for diffusion models, leading to poorer generation quality despite better reconstruction quality. This issue limits the quality upper bound of latent diffusion models and hinders the employment of autoencoders with higher spatial compression ratios. We introduce two key innovations to address this challenge: i) Structured Latent Space, a training-based approach to impose a desired channel-wise structure on the latent space with front latent channels capturing object structures and latter latent channels capturing image details; ii) Augmented Diffusion Training, an augmented diffusion training strategy with additional diffusion training objectives on object latent channels to accelerate convergence. With these techniques, DC-AE 1.5 delivers faster convergence and better diffusion scaling results than DC-AE. On ImageNet 512x512, DC-AE-1.5-f64c128 delivers better image generation quality than DC-AE-f32c32 while being 4x faster. Code: https://github.com/dc-ai-projects/DC-Gen.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† DC-AE 1.5ï¼Œè¿™æ˜¯ä¸€ç³»åˆ—ä¸“ä¸ºé«˜åˆ†è¾¨ç‡ Diffusion Model è®¾è®¡çš„æ·±åº¦å‹ç¼©è‡ªåŠ¨ç¼–ç å™¨ã€‚é’ˆå¯¹å¢åŠ æ½œç©ºé—´ Latent Space é€šé“æ•°è™½èƒ½æå‡é‡å»ºè´¨é‡ä½†ä¼šå¯¼è‡´æ¨¡å‹æ”¶æ•›ç¼“æ…¢å¹¶é™åˆ¶ç”Ÿæˆæ•ˆæœçš„é—®é¢˜ï¼Œç ”ç©¶å›¢é˜Ÿå¼•å…¥äº†ä¸¤é¡¹æ ¸å¿ƒåˆ›æ–°ã€‚é¦–å…ˆæ˜¯ Structured Latent Spaceï¼Œé€šè¿‡è®­ç»ƒä½¿æ½œç©ºé—´å…·å¤‡ç‰¹å®šçš„é€šé“ç»“æ„ï¼Œå…¶ä¸­å‰éƒ¨é€šé“è´Ÿè´£æ•æ‰ç‰©ä½“ç»“æ„ï¼Œåéƒ¨é€šé“è´Ÿè´£æ•æ‰å›¾åƒç»†èŠ‚ã€‚å…¶æ¬¡æ˜¯ Augmented Diffusion Training ç­–ç•¥ï¼Œé€šè¿‡åœ¨ç‰©ä½“æ½œç©ºé—´é€šé“ä¸Šå¢åŠ é¢å¤–çš„æ‰©æ•£è®­ç»ƒç›®æ ‡æ¥åŠ é€Ÿæ”¶æ•›è¿‡ç¨‹ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒDC-AE 1.5 ç›¸æ¯”å‰ä»£äº§å“å…·æœ‰æ›´å¿«çš„æ”¶æ•›é€Ÿåº¦å’Œæ›´å¥½çš„æ‰©å±•æ€§ï¼Œå…¶ DC-AE-1.5-f64c128 ç‰ˆæœ¬åœ¨ ImageNet 512x512 åŸºå‡†æµ‹è¯•ä¸­ç”Ÿæˆè´¨é‡ä¼˜äº DC-AE-f32c32ï¼Œä¸”è¿è¡Œé€Ÿåº¦æå‡äº† 4 å€ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "ICCV 2025",
      "pdf_url": "https://arxiv.org/pdf/2508.00413v1",
      "published_date": "2025-08-01 08:11:07 UTC",
      "updated_date": "2025-08-01 08:11:07 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T08:11:53.927544+00:00"
    },
    {
      "arxiv_id": "2508.00401v2",
      "title": "Theory of Mind Using Active Inference: A Framework for Multi-Agent Cooperation",
      "title_zh": "åŸºäºä¸»åŠ¨æ¨ç†çš„å¿ƒç†ç†è®ºï¼šä¸€ç§å¤šæ™ºèƒ½ä½“åä½œæ¡†æ¶",
      "authors": [
        "Riddhi J. Pitliya",
        "Ozan Ã‡atal",
        "Toon Van de Maele",
        "Corrado Pezzato",
        "Tim Verbelen"
      ],
      "abstract": "Theory of Mind (ToM) -- the ability to understand that others can have differing knowledge and goals -- enables agents to reason about others' beliefs while planning their own actions. We present a novel approach to multi-agent cooperation by implementing ToM within active inference. Unlike previous active inference approaches to multi-agent cooperation, our method neither relies on task-specific shared generative models nor requires explicit communication. In our framework, ToM-equipped agents maintain distinct representations of their own and others' beliefs and goals. ToM agents then use an extended and adapted version of the sophisticated inference tree-based planning algorithm to systematically explore joint policy spaces through recursive reasoning. We evaluate our approach through collision avoidance and foraging simulations. Results suggest that ToM agents cooperate better compared to non-ToM counterparts by being able to avoid collisions and reduce redundant efforts. Crucially, ToM agents accomplish this by inferring others' beliefs solely from observable behaviour and considering them when planning their own actions. Our approach shows potential for generalisable and scalable multi-agent systems while providing computational insights into ToM mechanisms.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§åœ¨ä¸»åŠ¨æ¨ç† (Active Inference) æ¡†æ¶ä¸‹å®ç°å¿ƒç†ç†è®º (Theory of Mind, ToM) çš„æ–°æ–¹æ³•ï¼Œç”¨äºä¿ƒè¿›å¤šæ™ºèƒ½ä½“é—´çš„åä½œã€‚ä¸ä¼ ç»Ÿæ–¹æ³•ä¸åŒï¼Œè¯¥æ¡†æ¶ä¸ä¾èµ–äºä»»åŠ¡ç‰¹å®šçš„å…±äº«ç”Ÿæˆæ¨¡å‹æˆ–æ˜¾å¼é€šä¿¡ï¼Œè€Œæ˜¯è®©æ™ºèƒ½ä½“ç»´æŠ¤è‡ªèº«ä¸ä»–äººç‹¬ç«‹çš„ä¿¡å¿µåŠç›®æ ‡è¡¨å¾ã€‚é€šè¿‡é‡‡ç”¨æ”¹è¿›çš„å¤æ‚æ¨ç†æ ‘è§„åˆ’ç®—æ³• (Sophisticated Inference Tree-based Planning Algorithm)ï¼Œæ™ºèƒ½ä½“èƒ½å¤Ÿåˆ©ç”¨é€’å½’æ¨ç†ç³»ç»Ÿåœ°æ¢ç´¢è”åˆç­–ç•¥ç©ºé—´ã€‚åœ¨é¿éšœå’Œæœå¯»æ¨¡æ‹Ÿå®éªŒä¸­ï¼Œå…·å¤‡ ToM èƒ½åŠ›çš„æ™ºèƒ½ä½“è¡¨ç°å‡ºæ˜¾è‘—ä¼˜äºé ToM æ™ºèƒ½ä½“çš„åä½œæ•ˆç‡ï¼Œèƒ½æœ‰æ•ˆé¿å…ç¢°æ’å¹¶å‡å°‘å†—ä½™å·¥ä½œã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæ™ºèƒ½ä½“èƒ½å¤Ÿä»…é€šè¿‡è§‚æµ‹åˆ°çš„è¡Œä¸ºå‡†ç¡®æ¨æ–­ä»–äººçš„ä¿¡å¿µå¹¶æ®æ­¤è§„åˆ’è‡ªèº«è¡ŒåŠ¨ã€‚è¯¥ç ”ç©¶ä¸ºå¼€å‘é€šç”¨ä¸”å¯æ‰©å±•çš„å¤šæ™ºèƒ½ä½“ç³»ç»Ÿæä¾›äº†æ–°è§†è§’ï¼ŒåŒæ—¶ä¹Ÿæ·±åŒ–äº†å¯¹å¿ƒç†ç†è®ºè®¡ç®—æœºåˆ¶çš„ç†è§£ã€‚",
      "categories": [
        "cs.AI",
        "cs.MA"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.00401v2",
      "published_date": "2025-08-01 08:02:35 UTC",
      "updated_date": "2025-09-04 13:51:30 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T08:11:56.845391+00:00"
    },
    {
      "arxiv_id": "2508.05664v1",
      "title": "Enhancing Retrieval-Augmented Generation for Electric Power Industry Customer Support",
      "title_zh": "é¢å‘ç”µåŠ›è¡Œä¸šå®¢æˆ·æ”¯æŒçš„æ£€ç´¢å¢å¼ºç”Ÿæˆå¢å¼º",
      "authors": [
        "Hei Yu Chan",
        "Kuok Tou Ho",
        "Chenglong Ma",
        "Yujing Si",
        "Hok Lai Lin",
        "Sa Lei Lam"
      ],
      "abstract": "Many AI customer service systems use standard NLP pipelines or finetuned language models, which often fall short on ambiguous, multi-intent, or detail-specific queries. This case study evaluates recent techniques: query rewriting, RAG Fusion, keyword augmentation, intent recognition, and context reranking, for building a robust customer support system in the electric power domain. We compare vector-store and graph-based RAG frameworks, ultimately selecting the graph-based RAG for its superior performance in handling complex queries. We find that query rewriting improves retrieval for queries using non-standard terminology or requiring precise detail. RAG Fusion boosts performance on vague or multifaceted queries by merging multiple retrievals. Reranking reduces hallucinations by filtering irrelevant contexts. Intent recognition supports the decomposition of complex questions into more targeted sub-queries, increasing both relevance and efficiency. In contrast, keyword augmentation negatively impacts results due to biased keyword selection. Our final system combines intent recognition, RAG Fusion, and reranking to handle disambiguation and multi-source queries. Evaluated on both a GPT-4-generated dataset and a real-world electricity provider FAQ dataset, it achieves 97.9% and 89.6% accuracy respectively, substantially outperforming baseline RAG models.",
      "tldr_zh": "è¯¥ç ”ç©¶è¯„ä¼°äº†å¤šç§æŠ€æœ¯ä»¥å¢å¼ºç”µåŠ›è¡Œä¸šå®¢æˆ·æ”¯æŒä¸­çš„æ£€ç´¢å¢å¼ºç”Ÿæˆ(Retrieval-Augmented Generation, RAG)ç³»ç»Ÿï¼Œæ—¨åœ¨è§£å†³ä¼ ç»Ÿæ¨¡å‹åœ¨å¤„ç†å¤šæ„å›¾å’Œæ¨¡ç³ŠæŸ¥è¯¢æ—¶çš„å±€é™æ€§ã€‚é€šè¿‡å¯¹æ¯”å‘ç°ï¼ŒåŸºäºå›¾çš„(graph-based) RAGåœ¨å¤„ç†å¤æ‚æŸ¥è¯¢æ–¹é¢ä¼˜äºä¼ ç»Ÿçš„å‘é‡å­˜å‚¨(vector-store)æ¶æ„ã€‚å®éªŒè¯æ˜ï¼ŒæŸ¥è¯¢é‡å†™(query rewriting)èƒ½æ˜¾è‘—æ”¹å–„éæ ‡å‡†æœ¯è¯­çš„æ£€ç´¢ï¼ŒRAG Fusionæå‡äº†å¤šé¢æ€§æŸ¥è¯¢çš„è¡¨ç°ï¼Œè€Œä¸Šä¸‹æ–‡é‡æ’åº(reranking)åˆ™æœ‰æ•ˆå‡å°‘äº†å¹»è§‰(hallucinations)ã€‚æ­¤å¤–ï¼Œæ„å›¾è¯†åˆ«(intent recognition)é€šè¿‡å°†å¤æ‚é—®é¢˜åˆ†è§£ä¸ºå­æŸ¥è¯¢è¿›ä¸€æ­¥æé«˜äº†æ£€ç´¢æ•ˆç‡ï¼Œä½†å…³é”®å­—å¢å¼º(keyword augmentation)å› é€‰æ‹©åå·®åè€Œå¯¹ç»“æœäº§ç”Ÿäº†è´Ÿé¢å½±å“ã€‚æœ€ç»ˆæ•´åˆäº†æ„å›¾è¯†åˆ«ã€RAG Fusionä¸é‡æ’åºæŠ€æœ¯çš„ç³»ç»Ÿåœ¨çœŸå®ç”µåŠ›FAQæ•°æ®é›†ä¸Šå®ç°äº†89.6%çš„å‡†ç¡®ç‡ï¼Œæ€§èƒ½å¤§å¹…è¶…è¶ŠåŸºçº¿æ¨¡å‹ã€‚",
      "categories": [
        "cs.IR",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.IR",
      "comment": "6 pages",
      "pdf_url": "https://arxiv.org/pdf/2508.05664v1",
      "published_date": "2025-08-01 08:02:23 UTC",
      "updated_date": "2025-08-01 08:02:23 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T08:12:08.891852+00:00"
    },
    {
      "arxiv_id": "2508.00395v1",
      "title": "Decouple before Align: Visual Disentanglement Enhances Prompt Tuning",
      "title_zh": "å…ˆè§£è€¦åå¯¹é½ï¼šè§†è§‰è§£ç¦»å¢å¼ºæç¤ºå¾®è°ƒ",
      "authors": [
        "Fei Zhang",
        "Tianfei Zhou",
        "Jiangchao Yao",
        "Ya Zhang",
        "Ivor W. Tsang",
        "Yanfeng Wang"
      ],
      "abstract": "Prompt tuning (PT), as an emerging resource-efficient fine-tuning paradigm, has showcased remarkable effectiveness in improving the task-specific transferability of vision-language models. This paper delves into a previously overlooked information asymmetry issue in PT, where the visual modality mostly conveys more context than the object-oriented textual modality. Correspondingly, coarsely aligning these two modalities could result in the biased attention, driving the model to merely focus on the context area. To address this, we propose DAPT, an effective PT framework based on an intuitive decouple-before-align concept. First, we propose to explicitly decouple the visual modality into the foreground and background representation via exploiting coarse-and-fine visual segmenting cues, and then both of these decoupled patterns are aligned with the original foreground texts and the hand-crafted background classes, thereby symmetrically strengthening the modal alignment. To further enhance the visual concentration, we propose a visual pull-push regularization tailored for the foreground-background patterns, directing the original visual representation towards unbiased attention on the region-of-interest object. We demonstrate the power of architecture-free DAPT through few-shot learning, base-to-novel generalization, and data-efficient learning, all of which yield superior performance across prevailing benchmarks. Our code will be released at https://github.com/Ferenas/DAPT.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº† Prompt Tuning (PT) åœ¨è§†è§‰è¯­è¨€æ¨¡å‹ä¸­å­˜åœ¨çš„è§†è§‰ä¸æ–‡æœ¬æ¨¡æ€ä¿¡æ¯ä¸å¯¹ç§°é—®é¢˜ï¼Œå³è§†è§‰æ¨¡æ€é€šå¸¸æ¯”æ–‡æœ¬æ¨¡æ€åŒ…å«æ›´ä¸°å¯Œçš„èƒŒæ™¯ä¿¡æ¯ï¼Œå¯¼è‡´ç²—ç³™çš„æ¨¡æ€å¯¹é½ä¼šäº§ç”Ÿæ³¨æ„åŠ›åå·®ã€‚ä¸ºè§£å†³æ­¤æŒ‘æˆ˜ï¼Œä½œè€…æå‡ºäº† DAPT æ¡†æ¶ï¼Œé‡‡ç”¨äº†å…ˆè§£è€¦åå¯¹é½ (Decouple before Align) çš„åˆ›æ–°ç­–ç•¥ã€‚è¯¥æ–¹æ³•åˆ©ç”¨ç²—ç»†ç²’åº¦çš„è§†è§‰åˆ†å‰²çº¿ç´¢å°†è§†è§‰æ¨¡æ€æ˜¾å¼è§£è€¦ä¸ºå‰æ™¯å’ŒèƒŒæ™¯è¡¨ç¤ºï¼Œå¹¶åˆ†åˆ«ä¸å‰æ™¯æ–‡æœ¬åŠæ‰‹å·¥æ„å»ºçš„èƒŒæ™¯ç±»åˆ«è¿›è¡Œå¯¹ç§°å¯¹é½ã€‚æ­¤å¤–ï¼Œç ”ç©¶å¼•å…¥äº†ä¸“ä¸ºå‰æ™¯-èƒŒæ™¯æ¨¡å¼è®¾è®¡çš„è§†è§‰æ‹‰æ¨æ­£åˆ™åŒ– (pull-push regularization)ï¼Œä¿ƒä½¿åŸå§‹è§†è§‰è¡¨ç¤ºå¯¹æ„Ÿå…´è¶£åŒºåŸŸ (region-of-interest) çš„ç‰©ä½“ä¿æŒæ— åæ³¨æ„åŠ›ã€‚å®éªŒè¯æ˜ï¼Œè¿™ç§æ— éœ€æ”¹å˜æ¶æ„çš„ DAPT åœ¨ few-shot learningã€base-to-novel generalization å’Œ data-efficient learning ç­‰å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­å‡å–å¾—äº†ä¼˜äºç°æœ‰æ–¹æ³•çš„æ€§èƒ½è¡¨ç°ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "16 pages, Accepted at IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI)",
      "pdf_url": "https://arxiv.org/pdf/2508.00395v1",
      "published_date": "2025-08-01 07:46:00 UTC",
      "updated_date": "2025-08-01 07:46:00 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T08:11:59.926235+00:00"
    },
    {
      "arxiv_id": "2508.00394v1",
      "title": "ExeKGLib: A Platform for Machine Learning Analytics based on Knowledge Graphs",
      "title_zh": "ExeKGLibï¼šåŸºäºçŸ¥è¯†å›¾è°±çš„æœºå™¨å­¦ä¹ åˆ†æå¹³å°",
      "authors": [
        "Antonis Klironomos",
        "Baifan Zhou",
        "Zhipeng Tan",
        "Zhuoxun Zheng",
        "Mohamed H. Gad-Elrab",
        "Heiko Paulheim",
        "Evgeny Kharlamov"
      ],
      "abstract": "Nowadays machine learning (ML) practitioners have access to numerous ML libraries available online. Such libraries can be used to create ML pipelines that consist of a series of steps where each step may invoke up to several ML libraries that are used for various data-driven analytical tasks. Development of high-quality ML pipelines is non-trivial; it requires training, ML expertise, and careful development of each step. At the same time, domain experts in science and engineering may not possess such ML expertise and training while they are in pressing need of ML-based analytics. In this paper, we present our ExeKGLib, a Python library enhanced with a graphical interface layer that allows users with minimal ML knowledge to build ML pipelines. This is achieved by relying on knowledge graphs that encode ML knowledge in simple terms accessible to non-ML experts. ExeKGLib also allows improving the transparency and reusability of the built ML workflows and ensures that they are executable. We show the usability and usefulness of ExeKGLib by presenting real use cases.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹éæœºå™¨å­¦ä¹ ä¸“å®¶åœ¨æ„å»ºé«˜è´¨é‡æœºå™¨å­¦ä¹ æµæ°´çº¿(ML pipelines)æ—¶é¢ä¸´çš„ä¸“ä¸šçŸ¥è¯†é—¨æ§›é«˜ã€å¼€å‘éš¾åº¦å¤§ç­‰æŒ‘æˆ˜ï¼Œå¼€å‘äº†ExeKGLibå¹³å°ã€‚ExeKGLibæ˜¯ä¸€ä¸ªå¢å¼ºäº†å›¾å½¢ç•Œé¢å±‚çš„Pythonåº“ï¼Œæ—¨åœ¨å…è®¸å…·å¤‡æå°‘æœºå™¨å­¦ä¹ çŸ¥è¯†çš„ç”¨æˆ·è½»æ¾æ„å»ºæœºå™¨å­¦ä¹ æµæ°´çº¿ã€‚è¯¥å¹³å°çš„æ ¸å¿ƒåœ¨äºåˆ©ç”¨çŸ¥è¯†å›¾è°±(Knowledge Graphs)å°†å¤æ‚çš„æœºå™¨å­¦ä¹ çŸ¥è¯†ç¼–ç ä¸ºéä¸“å®¶æ˜“äºç†è§£çš„ç®€å•æœ¯è¯­ã€‚æ­¤å¤–ï¼ŒExeKGLibæ˜¾è‘—æå‡äº†æ„å»ºå¥½çš„æœºå™¨å­¦ä¹ å·¥ä½œæµ(ML workflows)çš„é€æ˜åº¦å’Œå¯é‡ç”¨æ€§ï¼Œå¹¶ç¡®ä¿å…¶å…·æœ‰è‰¯å¥½çš„å¯æ‰§è¡Œæ€§ã€‚é€šè¿‡å¤šä¸ªçœŸå®ç”¨ä¾‹çš„å±•ç¤ºï¼Œç ”ç©¶è¯æ˜äº†è¯¥åº“åœ¨ç®€åŒ–å¤æ‚æ•°æ®åˆ†æä»»åŠ¡æ–¹é¢çš„å®ç”¨æ€§ä¸æœ‰æ•ˆæ€§ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.00394v1",
      "published_date": "2025-08-01 07:45:49 UTC",
      "updated_date": "2025-08-01 07:45:49 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T08:12:04.041845+00:00"
    },
    {
      "arxiv_id": "2508.00955v1",
      "title": "From Generator to Embedder: Harnessing Innate Abilities of Multimodal LLMs via Building Zero-Shot Discriminative Embedding Model",
      "title_zh": "ä»ç”Ÿæˆå™¨åˆ°åµŒå…¥å™¨ï¼šé€šè¿‡æ„å»ºé›¶æ ·æœ¬åˆ¤åˆ«å¼åµŒå…¥æ¨¡å‹æŒ–æ˜å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹çš„å†…åœ¨èƒ½åŠ›",
      "authors": [
        "Yeong-Joon Ju",
        "Seong-Whan Lee"
      ],
      "abstract": "Multimodal Large Language Models (MLLMs) have emerged as a promising solution for universal embedding tasks, yet adapting their generative nature for discriminative representation learning remains a significant challenge. The dominant paradigm of large-scale contrastive pre-training suffers from critical inefficiencies, including prohibitive computational costs and a failure to leverage the intrinsic, instruction-following capabilities of MLLMs. To overcome these limitations, we propose an efficient framework for universal multimodal embeddings, which bridges this gap by centering on two synergistic components. First, our hierarchical embedding prompt template employs a two-level instruction architecture that forces the model to produce discriminative representations. Building on this strong foundation, our second component, self-aware hard negative sampling, redefines the fine-tuning process by leveraging the model's own understanding to efficiently mine challenging negatives while actively filtering out potential false negatives. Our comprehensive experiments show that our hierarchical prompt achieves zero-shot performance competitive with contrastively trained baselines and enhances the fine-tuning process by lifting a simple in-batch negative baseline by 4.8 points on the MMEB benchmark. We further boost the performance via our self-aware hard negative sampling, achieving the state-of-the-art performance without the contrative pre-training. Our work presents an effective and efficient pathway to adapt MLLMs for universal embedding tasks, significantly reducing training time.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§é«˜æ•ˆçš„é€šç”¨å¤šæ¨¡æ€åµŒå…¥æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³å°†å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼ˆMultimodal Large Language Models, MLLMsï¼‰çš„ç”Ÿæˆç‰¹æ€§è½¬åŒ–ä¸ºåˆ¤åˆ«å¼è¡¨ç¤ºå­¦ä¹ æ—¶é¢ä¸´çš„æ•ˆç‡ä¸æˆæœ¬æŒ‘æˆ˜ã€‚è¯¥æ¡†æ¶æ ¸å¿ƒç”±ä¸¤ä¸ªååŒç»„ä»¶æ„æˆï¼šé¦–å…ˆæ˜¯å±‚æ¬¡åŒ–åµŒå…¥æç¤ºæ¨¡æ¿ï¼ˆHierarchical Embedding Prompt Templateï¼‰ï¼Œé€šè¿‡äºŒçº§æŒ‡ä»¤æ¶æ„å¼ºåˆ¶æ¨¡å‹ç”Ÿæˆå…·æœ‰åˆ¤åˆ«åŠ›çš„è¡¨ç¤ºï¼›å…¶æ¬¡æ˜¯è‡ªæ„ŸçŸ¥ç¡¬è´Ÿé‡‡æ ·ï¼ˆSelf-aware Hard Negative Samplingï¼‰ï¼Œåˆ©ç”¨æ¨¡å‹è‡ªèº«çš„ç†è§£èƒ½åŠ›é«˜æ•ˆæŒ–æ˜å…·æœ‰æŒ‘æˆ˜æ€§çš„è´Ÿæ ·æœ¬ï¼ŒåŒæ—¶è¿‡æ»¤æ½œåœ¨çš„å‡è´Ÿæ ·æœ¬ã€‚å®éªŒè¡¨æ˜ï¼Œå±‚æ¬¡åŒ–æç¤ºä½¿æ¨¡å‹åœ¨é›¶æ ·æœ¬ï¼ˆZero-shotï¼‰åœºæ™¯ä¸‹å±•ç°å‡ºæå…·ç«äº‰åŠ›çš„æ€§èƒ½ï¼Œå¹¶åœ¨MMEBåŸºå‡†æµ‹è¯•ä¸­æ˜¾è‘—æå‡äº†å¾®è°ƒæ•ˆæœã€‚æœ€ç»ˆï¼Œè¯¥æ–¹æ³•åœ¨æ— éœ€å¤§è§„æ¨¡å¯¹æ¯”é¢„è®­ç»ƒï¼ˆContrastive Pre-trainingï¼‰çš„æƒ…å†µä¸‹è¾¾åˆ°äº†SOTAæ°´å¹³ï¼Œå¤§å¹…ç¼©çŸ­äº†è®­ç»ƒæ—¶é—´ã€‚è¿™é¡¹å·¥ä½œä¸ºåˆ©ç”¨MLLMsçš„å…ˆå¤©èƒ½åŠ›æ„å»ºé€šç”¨åµŒå…¥æ¨¡å‹æä¾›äº†ä¸€æ¡æ—¢æœ‰æ•ˆåˆç»æµçš„æŠ€æœ¯è·¯å¾„ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.IR"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.00955v1",
      "published_date": "2025-08-01 07:31:24 UTC",
      "updated_date": "2025-08-01 07:31:24 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T08:12:13.940568+00:00"
    },
    {
      "arxiv_id": "2508.00383v1",
      "title": "$MV_{Hybrid}$: Improving Spatial Transcriptomics Prediction with Hybrid State Space-Vision Transformer Backbone in Pathology Vision Foundation Models",
      "title_zh": "$MV_{Hybrid}$ï¼šåˆ©ç”¨æ··åˆçŠ¶æ€ç©ºé—´-è§†è§‰ Transformer éª¨å¹²ç½‘ç»œæå‡ç—…ç†è§†è§‰åŸºç¡€æ¨¡å‹ä¸­çš„ç©ºé—´è½¬å½•ç»„å­¦é¢„æµ‹",
      "authors": [
        "Won June Cho",
        "Hongjun Yoon",
        "Daeky Jeong",
        "Hyeongyeol Lim",
        "Yosep Chong"
      ],
      "abstract": "Spatial transcriptomics reveals gene expression patterns within tissue context, enabling precision oncology applications such as treatment response prediction, but its high cost and technical complexity limit clinical adoption. Predicting spatial gene expression (biomarkers) from routine histopathology images offers a practical alternative, yet current vision foundation models (VFMs) in pathology based on Vision Transformer (ViT) backbones perform below clinical standards. Given that VFMs are already trained on millions of diverse whole slide images, we hypothesize that architectural innovations beyond ViTs may better capture the low-frequency, subtle morphological patterns correlating with molecular phenotypes. By demonstrating that state space models initialized with negative real eigenvalues exhibit strong low-frequency bias, we introduce $MV_{Hybrid}$, a hybrid backbone architecture combining state space models (SSMs) with ViT. We compare five other different backbone architectures for pathology VFMs, all pretrained on identical colorectal cancer datasets using the DINOv2 self-supervised learning method. We evaluate all pretrained models using both random split and leave-one-study-out (LOSO) settings of the same biomarker dataset. In LOSO evaluation, $MV_{Hybrid}$ achieves 57% higher correlation than the best-performing ViT and shows 43% smaller performance degradation compared to random split in gene expression prediction, demonstrating superior performance and robustness, respectively. Furthermore, $MV_{Hybrid}$ shows equal or better downstream performance in classification, patch retrieval, and survival prediction tasks compared to that of ViT, showing its promise as a next-generation pathology VFM backbone. Our code is publicly available at: https://github.com/deepnoid-ai/MVHybrid.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† $MV_{Hybrid}$ï¼Œè¿™æ˜¯ä¸€ç§ç»“åˆäº†çŠ¶æ€ç©ºé—´æ¨¡å‹ (SSMs) ä¸ Vision Transformer (ViT) çš„æ··åˆéª¨å¹²æ¶æ„ï¼Œæ—¨åœ¨ä¼˜åŒ–ç—…ç†è§†è§‰åŸºç¡€æ¨¡å‹ (VFMs) å¯¹ç©ºé—´è½¬å½•ç»„å­¦çš„é¢„æµ‹èƒ½åŠ›ã€‚é’ˆå¯¹ç°æœ‰ ViT æ¶æ„åœ¨æ•æ‰ä¸åˆ†å­è¡¨å‹ç›¸å…³çš„ä½é¢‘å¾®ç»†å½¢æ€æ¨¡å¼æ–¹é¢çš„ä¸è¶³ï¼Œè¯¥ç ”ç©¶åˆ©ç”¨ SSMs å…·æœ‰å¼ºä½é¢‘åå·®çš„ç‰¹æ€§æ¥å¢å¼ºæ¨¡å‹å¯¹ç‰¹å®šç—…ç†ç‰¹å¾çš„æå–ã€‚ç ”ç©¶äººå‘˜é€šè¿‡ DINOv2 è‡ªç›‘ç£å­¦ä¹ æ–¹æ³•åœ¨ç»“ç›´è‚ ç™Œæ•°æ®é›†ä¸Šå¯¹æ¨¡å‹è¿›è¡Œé¢„è®­ç»ƒï¼Œå¹¶å¯¹æ¯”äº†å¤šç§ä¸»æµéª¨å¹²æ¶æ„ã€‚åœ¨ç•™ä¸€ç ”ç©¶æ³• (LOSO) è¯„ä¼°ä¸­ï¼Œ$MV_{Hybrid}$ çš„åŸºå› è¡¨è¾¾é¢„æµ‹ç›¸å…³æ€§æ¯”è¡¨ç°æœ€å¥½çš„ ViT é«˜å‡º 57%ï¼Œä¸”æ€§èƒ½é€€åŒ–ç¨‹åº¦é™ä½äº† 43%ï¼Œå±•ç°äº†å“è¶Šçš„å‡†ç¡®æ€§ä¸é²æ£’æ€§ã€‚æ­¤å¤–ï¼Œè¯¥æ¨¡å‹åœ¨åˆ†ç±»ã€è¡¥ä¸æ£€ç´¢åŠç”Ÿå­˜é¢„æµ‹ç­‰ä¸‹æ¸¸ä»»åŠ¡ä¸­äº¦è¡¨ç°å‡ºä¼˜äºæˆ–ç­‰åŒäº ViT çš„æ€§èƒ½ã€‚è¿™é¡¹å·¥ä½œè¯æ˜äº†æ··åˆæ¶æ„ä½œä¸ºä¸‹ä¸€ä»£ç—…ç†å­¦è§†è§‰åŸºç¡€æ¨¡å‹çš„å·¨å¤§æ½œåŠ›ï¼Œä¸ºä½æˆæœ¬é¢„æµ‹ç©ºé—´åŸºå› è¡¨è¾¾æä¾›äº†æœ‰æ•ˆæ–¹æ¡ˆã€‚",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CE",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "Accepted (Oral) in MICCAI 2025 COMPAYL Workshop",
      "pdf_url": "https://arxiv.org/pdf/2508.00383v1",
      "published_date": "2025-08-01 07:23:45 UTC",
      "updated_date": "2025-08-01 07:23:45 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T08:12:15.696109+00:00"
    },
    {
      "arxiv_id": "2508.00381v2",
      "title": "Advancing Welding Defect Detection in Maritime Operations via Adapt-WeldNet and Defect Detection Interpretability Analysis",
      "title_zh": "é€šè¿‡ Adapt-WeldNet ä¸ç¼ºé™·æ£€æµ‹å¯è§£é‡Šæ€§åˆ†ææå‡æµ·äº‹ä½œä¸šä¸­çš„ç„Šæ¥ç¼ºé™·æ£€æµ‹",
      "authors": [
        "Kamal Basha S",
        "Athira Nambiar"
      ],
      "abstract": "Weld defect detection is crucial for ensuring the safety and reliability of piping systems in the oil and gas industry, especially in challenging marine and offshore environments. Traditional non-destructive testing (NDT) methods often fail to detect subtle or internal defects, leading to potential failures and costly downtime. Furthermore, existing neural network-based approaches for defect classification frequently rely on arbitrarily selected pretrained architectures and lack interpretability, raising safety concerns for deployment. To address these challenges, this paper introduces ``Adapt-WeldNet\", an adaptive framework for welding defect detection that systematically evaluates various pre-trained architectures, transfer learning strategies, and adaptive optimizers to identify the best-performing model and hyperparameters, optimizing defect detection and providing actionable insights. Additionally, a novel Defect Detection Interpretability Analysis (DDIA) framework is proposed to enhance system transparency. DDIA employs Explainable AI (XAI) techniques, such as Grad-CAM and LIME, alongside domain-specific evaluations validated by certified ASNT NDE Level II professionals. Incorporating a Human-in-the-Loop (HITL) approach and aligning with the principles of Trustworthy AI, DDIA ensures the reliability, fairness, and accountability of the defect detection system, fostering confidence in automated decisions through expert validation. By improving both performance and interpretability, this work enhances trust, safety, and reliability in welding defect detection systems, supporting critical operations in offshore and marine environments.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹æµ·æ´‹åŠç¦»å²¸çŸ³æ²¹å¤©ç„¶æ°”å·¥ä¸šä¸­ç„Šæ¥ç¼ºé™·æ£€æµ‹é¢ä¸´çš„æŒ‘æˆ˜ï¼Œæå‡ºäº†åä¸ºAdapt-WeldNetçš„è‡ªé€‚åº”æ¡†æ¶å’Œåä¸ºDefect Detection Interpretability Analysis (DDIA)çš„è§£é‡Šæ€§åˆ†ææ¡†æ¶ã€‚Adapt-WeldNeté€šè¿‡ç³»ç»ŸåŒ–è¯„ä¼°å¤šç§é¢„è®­ç»ƒæ¶æ„ã€è¿ç§»å­¦ä¹ ç­–ç•¥åŠè‡ªé€‚åº”ä¼˜åŒ–å™¨ï¼Œå®ç°äº†æ£€æµ‹æ¨¡å‹ä¸è¶…å‚æ•°çš„æœ€ä¼˜é€‰æ‹©ã€‚é’ˆå¯¹ç°æœ‰ç¥ç»ç½‘ç»œé»‘ç›’æ¨¡å‹å¸¦æ¥çš„å®‰å…¨ç–‘è™‘ï¼ŒDDIAæ¡†æ¶æ•´åˆäº†Grad-CAMå’ŒLIMEç­‰å¯è§£é‡ŠAI (XAI)æŠ€æœ¯ï¼Œå¹¶é€šè¿‡ASNT NDE Level IIè®¤è¯ä¸“å®¶çš„é¢†åŸŸçŸ¥è¯†è¿›è¡ŒéªŒè¯ã€‚æ­¤å¤–ï¼Œç ”ç©¶å¼•å…¥äº†äººæœºååŒ (Human-in-the-Loop) æ–¹æ³•ï¼Œä¸¥æ ¼éµå¾ªå¯ä¿¡AI (Trustworthy AI) åŸåˆ™ï¼Œç¡®ä¿äº†ç³»ç»Ÿçš„å¯é æ€§ã€å…¬å¹³æ€§ä¸é—®è´£æœºåˆ¶ã€‚è¯¥ç ”ç©¶ä¸ä»…æ˜¾è‘—æå‡äº†ç„Šæ¥ç¼ºé™·æ£€æµ‹çš„æ€§èƒ½ï¼Œæ›´é€šè¿‡å¢å¼ºç³»ç»Ÿé€æ˜åº¦ï¼Œä¸ºæµ·æ´‹ä½œä¸šä¸­çš„è‡ªåŠ¨åŒ–å†³ç­–å»ºç«‹äº†æ·±åšçš„ä¿¡ä»»ä¸å®‰å…¨ä¿éšœã€‚",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CE",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.00381v2",
      "published_date": "2025-08-01 07:19:23 UTC",
      "updated_date": "2025-08-08 11:27:44 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T08:12:20.651465+00:00"
    },
    {
      "arxiv_id": "2508.00378v3",
      "title": "CoRGI: Verified Chain-of-Thought Reasoning with Post-hoc Visual Grounding",
      "title_zh": "CoRGIï¼šåŸºäºäº‹åè§†è§‰å®šä½çš„ç»éªŒè¯æ€ç»´é“¾æ¨ç†",
      "authors": [
        "Shixin Yi",
        "Lin Shang"
      ],
      "abstract": "Multimodal reasoning with vision-language models (VLMs) often suffers from hallucinations, as models tend to generate explanations after only a superficial inspection of the image. We present \\textbf{CoRGI}(\\textbf{C}hain \\textbf{o}f \\textbf{R}easoning with \\textbf{G}rounded \\textbf{I}nsights), a framework that enhances reasoning reliability through post-hoc verification of chain-of-thought outputs. Given a VLM-generated rationale, CoRGI decomposes it into step-wise statements, grounds each step in visual evidence, and filters or corrects unsupported claims before producing the final answer. Experiments on five challenging benchmark-VCR, ScienceQA, MMMU, MathVista, and HallusionBenc-demonstrate that CoRGI consistently improves both answer accuracy and explanation faithfulness across multiple VLM backbones, including Qwen-2.5VL, LLaVA-1.6, and Gemma3-12B. Beyond quantitative gains, qualitative analyses further illustrate how the verification process reduces hallucination and strengthens interpretability, suggesting that post-hoc visual grounding is a promising direction for building more trustworthy and transparent multimodal reasoning systems.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†CoRGIæ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³è§†è§‰è¯­è¨€æ¨¡å‹(VLMs)åœ¨å¤šæ¨¡æ€æ¨ç†è¿‡ç¨‹ä¸­ç»å¸¸å‡ºç°çš„å¹»è§‰é—®é¢˜ï¼Œé€šè¿‡äº‹åè§†è§‰å®šä½(post-hoc visual grounding)æ¥å¢å¼ºé“¾å¼æ€ç»´(Chain-of-Thought)æ¨ç†çš„å¯é æ€§ã€‚CoRGIå°†æ¨¡å‹ç”Ÿæˆçš„æ¨ç†é€»è¾‘åˆ†è§£ä¸ºé€æ­¥çš„é™ˆè¿°ï¼Œå¹¶å°†æ¯ä¸€æ­¥ä¸è§†è§‰è¯æ®è¿›è¡Œæ¯”å¯¹å®šä½ï¼Œåœ¨ç”Ÿæˆæœ€ç»ˆç­”æ¡ˆå‰è¿‡æ»¤æˆ–çº æ­£æ— è¯æ®æ”¯æŒçš„æ–­è¨€ã€‚å®éªŒåœ¨VCRã€ScienceQAã€MMMUã€MathVistaå’ŒHallusionBenchäº”ä¸ªæŒ‘æˆ˜æ€§åŸºå‡†ä¸Šè¿›è¡Œï¼Œè¯æ˜äº†è¯¥æ¡†æ¶åœ¨Qwen-2.5VLã€LLaVA-1.6å’ŒGemma3-12Bç­‰å¤šç§æ¨¡å‹éª¨å¹²ä¸Šå‡èƒ½æ˜¾è‘—æå‡ç­”æ¡ˆå‡†ç¡®ç‡ã€‚æ­¤å¤–ï¼ŒCoRGIè¿˜å¢å¼ºäº†æ¨¡å‹è§£é‡Šçš„å¿ å®åº¦(faithfulness)ï¼Œå®šæ€§åˆ†ææ˜¾ç¤ºè¯¥éªŒè¯è¿‡ç¨‹æœ‰æ•ˆå‡å°‘äº†å¹»è§‰å¹¶æå‡äº†ç³»ç»Ÿé€æ˜åº¦ã€‚è¿™ä¸€ç ”ç©¶ä¸ºæ„å»ºæ›´å¯ä¿¡ã€å¯è§£é‡Šçš„å¤šæ¨¡æ€æ¨ç†ç³»ç»Ÿæä¾›äº†å…·æœ‰å‰æ™¯çš„æŠ€æœ¯è·¯å¾„ã€‚",
      "categories": [
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.AI",
      "comment": "The paper is not yet mature and needs further improvement",
      "pdf_url": "https://arxiv.org/pdf/2508.00378v3",
      "published_date": "2025-08-01 07:17:12 UTC",
      "updated_date": "2025-10-14 09:15:36 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T08:12:26.335255+00:00"
    },
    {
      "arxiv_id": "2508.03742v1",
      "title": "Boosting Vision Semantic Density with Anatomy Normality Modeling for Medical Vision-language Pre-training",
      "title_zh": "é€šè¿‡è§£å‰–æ­£å¸¸æ€§å»ºæ¨¡æå‡è§†è§‰è¯­ä¹‰å¯†åº¦çš„åŒ»å­¦è§†è§‰-è¯­è¨€é¢„è®­ç»ƒ",
      "authors": [
        "Weiwei Cao",
        "Jianpeng Zhang",
        "Zhongyi Shui",
        "Sinuo Wang",
        "Zeli Chen",
        "Xi Li",
        "Le Lu",
        "Xianghua Ye",
        "Tingbo Liang",
        "Qi Zhang",
        "Ling Zhang"
      ],
      "abstract": "Vision-language pre-training (VLP) has great potential for developing multifunctional and general medical diagnostic capabilities. However, aligning medical images with a low signal-to-noise ratio (SNR) to reports with a high SNR presents a semantic density gap, leading to visual alignment bias. In this paper, we propose boosting vision semantic density to improve alignment effectiveness. On one hand, we enhance visual semantics through disease-level vision contrastive learning, which strengthens the model's ability to differentiate between normal and abnormal samples for each anatomical structure. On the other hand, we introduce an anatomical normality modeling method to model the distribution of normal samples for each anatomy, leveraging VQ-VAE for reconstructing normal vision embeddings in the latent space. This process amplifies abnormal signals by leveraging distribution shifts in abnormal samples, enhancing the model's perception and discrimination of abnormal attributes. The enhanced visual representation effectively captures the diagnostic-relevant semantics, facilitating more efficient and accurate alignment with the diagnostic report. We conduct extensive experiments on two chest CT datasets, CT-RATE and Rad-ChestCT, and an abdominal CT dataset, MedVL-CT69K, and comprehensively evaluate the diagnosis performance across multiple tasks in the chest and abdominal CT scenarios, achieving state-of-the-art zero-shot performance. Notably, our method achieved an average AUC of 84.9% across 54 diseases in 15 organs, significantly surpassing existing methods. Additionally, we demonstrate the superior transfer learning capabilities of our pre-trained model. Code is available at https://github.com/alibaba-damo-academy/ViSD-Boost.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹åŒ»å­¦è§†è§‰è¯­è¨€é¢„è®­ç»ƒ(VLP)ä¸­åŒ»å­¦å›¾åƒä½ä¿¡å™ªæ¯”(SNR)å¯¼è‡´çš„è¯­ä¹‰å¯†åº¦å·®è·å’Œè§†è§‰å¯¹é½åå·®é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§æ—¨åœ¨æå‡è§†è§‰è¯­ä¹‰å¯†åº¦çš„æ¡†æ¶ã€‚è¯¥æ–¹æ³•é¦–å…ˆé€šè¿‡ç–¾ç—…çº§è§†è§‰å¯¹æ¯”å­¦ä¹ å¢å¼ºäº†æ¨¡å‹åŒºåˆ†è§£å‰–ç»“æ„æ­£å¸¸ä¸å¼‚å¸¸æ ·æœ¬çš„èƒ½åŠ›ã€‚åŒæ—¶ï¼Œç ”ç©¶å¼•å…¥äº†è§£å‰–å­¦æ­£æ€æ€§å»ºæ¨¡(Anatomy Normality Modeling)ï¼Œåˆ©ç”¨VQ-VAEåœ¨æ½œç©ºé—´é‡æ„æ­£å¸¸è§†è§‰åµŒå…¥ï¼Œé€šè¿‡æ•æ‰å¼‚å¸¸æ ·æœ¬çš„åˆ†å¸ƒåç§»æ¥æœ‰æ•ˆæ”¾å¤§å¼‚å¸¸ä¿¡å·ã€‚å®éªŒåœ¨CT-RATEã€Rad-ChestCTå’ŒMedVL-CT69Kç­‰èƒ¸è…¹éƒ¨CTæ•°æ®é›†ä¸Šå±•å¼€ï¼Œå…¨é¢è¯„ä¼°äº†å¤šé¡¹ä¸´åºŠè¯Šæ–­ä»»åŠ¡ã€‚ç»“æœæ˜¾ç¤ºï¼Œè¯¥æ–¹æ³•åœ¨æ¶µç›–15ä¸ªå™¨å®˜çš„54ç§ç–¾ç—…ä¸­å®ç°äº†84.9%çš„å¹³å‡AUCï¼Œå–å¾—äº†å…ˆè¿›çš„é›¶æ ·æœ¬(zero-shot)æ€§èƒ½ã€‚æ­¤å¤–ï¼Œè¯¥é¢„è®­ç»ƒæ¨¡å‹å±•ç¤ºäº†å‡ºè‰²çš„è¿ç§»å­¦ä¹ èƒ½åŠ›ï¼Œä¸ºå®ç°æ›´é«˜æ•ˆå‡†ç¡®çš„åŒ»å­¦å½±åƒä¸æŠ¥å‘Šè‡ªåŠ¨å¯¹é½å¥ å®šäº†åŸºç¡€ã€‚",
      "categories": [
        "eess.IV",
        "cs.AI",
        "cs.CV",
        "cs.LG"
      ],
      "primary_category": "eess.IV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.03742v1",
      "published_date": "2025-08-01 06:52:05 UTC",
      "updated_date": "2025-08-01 06:52:05 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T08:12:50.847892+00:00"
    },
    {
      "arxiv_id": "2508.02723v1",
      "title": "Mathematical Foundations of Geometric Deep Learning",
      "title_zh": "å‡ ä½•æ·±åº¦å­¦ä¹ çš„æ•°å­¦åŸºç¡€",
      "authors": [
        "Haitz SÃ¡ez de OcÃ¡riz Borde",
        "Michael Bronstein"
      ],
      "abstract": "We review the key mathematical concepts necessary for studying Geometric Deep Learning.",
      "tldr_zh": "è¯¥ç ”ç©¶å¯¹ Geometric Deep Learningï¼ˆå‡ ä½•æ·±åº¦å­¦ä¹ ï¼‰çš„æ ¸å¿ƒæ•°å­¦æ¦‚å¿µè¿›è¡Œäº†ç³»ç»Ÿæ€§çš„å›é¡¾ä¸æ€»ç»“ã€‚æ–‡ç« æ—¨åœ¨ä¸ºå­¦è€…ç ”ç©¶è¯¥é¢†åŸŸæä¾›å¿…è¦çš„ç†è®ºåŸºçŸ³ï¼Œæ·±å…¥æ¢è®¨äº†æ”¯æ’‘å‡ ä½•å­¦ä¹ æ¨¡å‹çš„å…³é”®æ•°å­¦åŸç†ã€‚é€šè¿‡æ¢³ç†è¿™äº›åŸºç¡€æ¦‚å¿µï¼Œè¯¥ç ”ç©¶ä¸ºå¤„ç†éæ¬§å‡ é‡Œå¾—ç©ºé—´æ•°æ®çš„æ·±åº¦å­¦ä¹ ç®—æ³•æä¾›äº†ä¸¥è°¨çš„æ•°å­¦æ¡†æ¶ã€‚è¿™ç¯‡ç»¼è¿°å¯¹äºç†è§£å¦‚ä½•å°†å‡ ä½•çº¦æŸåº”ç”¨äºå¤æ‚ç¥ç»ç½‘ç»œæ¶æ„å…·æœ‰é‡è¦çš„å­¦æœ¯å‚è€ƒä»·å€¼ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "78 pages",
      "pdf_url": "https://arxiv.org/pdf/2508.02723v1",
      "published_date": "2025-08-01 06:02:39 UTC",
      "updated_date": "2025-08-01 06:02:39 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T08:12:50.493950+00:00"
    },
    {
      "arxiv_id": "2508.08281v1",
      "title": "Multi-grained spatial-temporal feature complementarity for accurate online cellular traffic prediction",
      "title_zh": "åŸºäºå¤šç²’åº¦æ—¶ç©ºç‰¹å¾äº’è¡¥çš„ç²¾å‡†åœ¨çº¿èœ‚çªæµé‡é¢„æµ‹",
      "authors": [
        "Ningning Fu",
        "Shengheng Liu",
        "Weiliang Xie",
        "Yongming Huang"
      ],
      "abstract": "Knowledge discovered from telecom data can facilitate proactive understanding of network dynamics and user behaviors, which in turn empowers service providers to optimize cellular traffic scheduling and resource allocation. Nevertheless, the telecom industry still heavily relies on manual expert intervention. Existing studies have been focused on exhaustively explore the spatial-temporal correlations. However, they often overlook the underlying characteristics of cellular traffic, which are shaped by the sporadic and bursty nature of telecom services. Additionally, concept drift creates substantial obstacles to maintaining satisfactory accuracy in continuous cellular forecasting tasks. To resolve these problems, we put forward an online cellular traffic prediction method grounded in Multi-Grained Spatial-Temporal feature Complementarity (MGSTC). The proposed method is devised to achieve high-precision predictions in practical continuous forecasting scenarios. Concretely, MGSTC segments historical data into chunks and employs the coarse-grained temporal attention to offer a trend reference for the prediction horizon. Subsequently, fine-grained spatial attention is utilized to capture detailed correlations among network elements, which enables localized refinement of the established trend. The complementarity of these multi-grained spatial-temporal features facilitates the efficient transmission of valuable information. To accommodate continuous forecasting needs, we implement an online learning strategy that can detect concept drift in real-time and promptly switch to the appropriate parameter update stage. Experiments carried out on four real-world datasets demonstrate that MGSTC outperforms eleven state-of-the-art baselines consistently.",
      "tldr_zh": "æœ¬ç ”ç©¶é’ˆå¯¹èœ‚çªæµé‡é¢„æµ‹ä¸­å­˜åœ¨çš„ä¸šåŠ¡çªå‘æ€§ã€é›¶æ˜Ÿæ€§ä»¥åŠæ¦‚å¿µæ¼‚ç§»(concept drift)å¯¼è‡´çš„é¢„æµ‹ç²¾åº¦ç»´æŒå›°éš¾ç­‰é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§åŸºäºå¤šç²’åº¦æ—¶ç©ºç‰¹å¾äº’è¡¥(Multi-Grained Spatial-Temporal feature Complementarity, MGSTC)çš„åœ¨çº¿é¢„æµ‹æ–¹æ³•ã€‚è¯¥æ–¹æ³•å°†å†å²æ•°æ®åˆ†å—ï¼Œåˆ©ç”¨ç²—ç²’åº¦æ—¶é—´æ³¨æ„åŠ›(coarse-grained temporal attention)ä¸ºé¢„æµ‹èŒƒå›´æä¾›è¶‹åŠ¿å‚è€ƒï¼Œå¹¶ç»“åˆç»†ç²’åº¦ç©ºé—´æ³¨æ„åŠ›(fine-grained spatial attention)æ•æ‰ç½‘ç»œå•å…ƒé—´çš„è¯¦ç»†å…³è”ï¼Œå®ç°å¯¹è¶‹åŠ¿çš„å±€éƒ¨ç²¾ç»†åŒ–ã€‚å¤šç²’åº¦æ—¶ç©ºç‰¹å¾çš„äº’è¡¥æœºåˆ¶æ˜¾è‘—æå‡äº†æœ‰ä»·å€¼ä¿¡æ¯çš„ä¼ è¾“æ•ˆç‡ï¼Œç¡®ä¿äº†åœ¨å®é™…æŒç»­é¢„æµ‹åœºæ™¯ä¸‹çš„é«˜ç²¾åº¦è¡¨ç°ã€‚æ­¤å¤–ï¼ŒMGSTCè¿˜å¼•å…¥äº†åœ¨çº¿å­¦ä¹ (online learning)ç­–ç•¥ï¼Œé€šè¿‡å®æ—¶æ£€æµ‹æ¦‚å¿µæ¼‚ç§»å¹¶è‡ªåŠ¨åˆ‡æ¢å‚æ•°æ›´æ–°é˜¶æ®µï¼Œä»¥æ»¡è¶³ç”µä¿¡åŠ¨æ€ç¯å¢ƒä¸‹çš„æŒç»­é¢„æµ‹éœ€æ±‚ã€‚åœ¨å››ä¸ªçœŸå®ä¸–ç•Œæ•°æ®é›†ä¸Šçš„å®éªŒè¯æ˜ï¼ŒMGSTCåœ¨æ€§èƒ½ä¸Šä¸€è‡´ä¼˜äº11ç§å…ˆè¿›çš„åŸºå‡†æ¨¡å‹(baselines)ï¼Œä¸ºç”µä¿¡æœåŠ¡å•†ä¼˜åŒ–æµé‡è°ƒåº¦å’Œèµ„æºåˆ†é…æä¾›äº†é«˜æ•ˆçš„æŠ€æœ¯æ”¯æ’‘ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.NI"
      ],
      "primary_category": "cs.LG",
      "comment": "To appear in ACM TKDD. 26 pages, 12 figures,",
      "pdf_url": "https://arxiv.org/pdf/2508.08281v1",
      "published_date": "2025-08-01 05:33:32 UTC",
      "updated_date": "2025-08-01 05:33:32 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T08:12:49.941849+00:00"
    },
    {
      "arxiv_id": "2508.08280v1",
      "title": "MoSSDA: A Semi-Supervised Domain Adaptation Framework for Multivariate Time-Series Classification using Momentum Encoder",
      "title_zh": "MoSSDAï¼šåŸºäºåŠ¨é‡ç¼–ç å™¨çš„å¤šå…ƒæ—¶é—´åºåˆ—åˆ†ç±»åŠç›‘ç£é¢†åŸŸè‡ªé€‚åº”æ¡†æ¶",
      "authors": [
        "Seonyoung Kim",
        "Dongil Kim"
      ],
      "abstract": "Deep learning has emerged as the most promising approach in various fields; however, when the distributions of training and test data are different (domain shift), the performance of deep learning models can degrade. Semi-supervised domain adaptation (SSDA) is a major approach for addressing this issue, assuming that a fully labeled training set (source domain) is available, but the test set (target domain) provides labels only for a small subset. In this study, we propose a novel two-step momentum encoder-utilized SSDA framework, MoSSDA, for multivariate time-series classification. Time series data are highly sensitive to noise, and sequential dependencies cause domain shifts resulting in critical performance degradation. To obtain a robust, domain-invariant and class-discriminative representation, MoSSDA employs a domain-invariant encoder to learn features from both source and target domains. Subsequently, the learned features are fed to a mixup-enhanced positive contrastive module consisting of an online momentum encoder. The final classifier is trained with learned features that exhibit consistency and discriminability with limited labeled target domain data, without data augmentation. We applied a two-stage process by separating the gradient flow between the encoders and the classifier to obtain rich and complex representations. Through extensive experiments on six diverse datasets, MoSSDA achieved state-of-the-art performance for three different backbones and various unlabeled ratios in the target domain data. The Ablation study confirms that each module, including two-stage learning, is effective in improving the performance. Our code is available at https://github.com/seonyoungKimm/MoSSDA",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤šå…ƒæ—¶é—´åºåˆ—åˆ†ç±»ï¼ˆMultivariate Time-Series Classificationï¼‰ä¸­å› é¢†åŸŸåç§»ï¼ˆDomain Shiftï¼‰å¯¼è‡´çš„æ€§èƒ½ä¸‹é™é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§åä¸ºMoSSDAçš„æ–°å‹åŠç›‘ç£é¢†åŸŸè‡ªé€‚åº”ï¼ˆSemi-Supervised Domain Adaptationï¼‰æ¡†æ¶ã€‚MoSSDAé€šè¿‡é¢†åŸŸä¸å˜ç¼–ç å™¨ï¼ˆDomain-invariant encoderï¼‰ä»æºåŸŸå’Œç›®æ ‡åŸŸä¸­æå–é²æ£’çš„ç‰¹å¾ï¼Œå¹¶åˆ©ç”¨åŒ…å«åœ¨çº¿åŠ¨é‡ç¼–ç å™¨ï¼ˆOnline momentum encoderï¼‰çš„æ··åˆå¢å¼ºæ­£å¯¹æ¯”æ¨¡å—ï¼Œåœ¨æœ‰é™çš„ç›®æ ‡åŸŸæ ‡ç­¾ä¸‹å¢å¼ºç‰¹å¾çš„ä¸€è‡´æ€§ä¸è¾¨åˆ«åŠ›ã€‚ä¸ºäº†è·å¾—æ›´å¤æ‚çš„è¡¨ç¤ºï¼Œç ”ç©¶é‡‡ç”¨ä¸¤é˜¶æ®µå­¦ä¹ è¿‡ç¨‹ï¼Œæœ‰æ•ˆåˆ†ç¦»äº†ç¼–ç å™¨ä¸åˆ†ç±»å™¨ä¹‹é—´çš„æ¢¯åº¦æµã€‚å®éªŒåœ¨å…­ä¸ªæ•°æ®é›†å’Œä¸‰ç§éª¨å¹²ç½‘ç»œï¼ˆBackbonesï¼‰ä¸Šè¿›è¡Œï¼Œç»“æœè¯æ˜MoSSDAåœ¨å„ç§æœªæ ‡è®°æ¯”ä¾‹ä¸‹å‡è¾¾åˆ°äº†æœ€å…ˆè¿›ï¼ˆState-of-the-artï¼‰çš„æ€§èƒ½æ°´å¹³ã€‚æ¶ˆèå®éªŒè¿›ä¸€æ­¥è¯å®äº†å…¶å„ä¸ªæ¨¡å—åœ¨åº”å¯¹æ—¶é—´åºåˆ—å™ªå£°å’Œåºåˆ—ä¾èµ–æ€§æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.08280v1",
      "published_date": "2025-08-01 05:27:44 UTC",
      "updated_date": "2025-08-01 05:27:44 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T08:12:48.330585+00:00"
    },
    {
      "arxiv_id": "2508.00324v1",
      "title": "R1-ACT: Efficient Reasoning Model Safety Alignment by Activating Safety Knowledge",
      "title_zh": "R1-ACTï¼šé€šè¿‡æ¿€æ´»å®‰å…¨çŸ¥è¯†å®ç°çš„é«˜æ•ˆæ¨ç†æ¨¡å‹å®‰å…¨å¯¹é½",
      "authors": [
        "Yeonjun In",
        "Wonjoong Kim",
        "Sangwu Park",
        "Chanyoung Park"
      ],
      "abstract": "Although large reasoning models (LRMs) have demonstrated impressive capabilities on complex tasks, recent studies reveal that these models frequently fulfill harmful user instructions, raising significant safety concerns. In this paper, we investigate the underlying cause of LRM safety risks and find that models already possess sufficient safety knowledge but fail to activate it during reasoning. Based on this insight, we propose R1-Act, a simple and efficient post-training method that explicitly triggers safety knowledge through a structured reasoning process. R1-Act achieves strong safety improvements while preserving reasoning performance, outperforming prior alignment methods. Notably, it requires only 1,000 training examples and 90 minutes of training on a single RTX A6000 GPU. Extensive experiments across multiple LRM backbones and sizes demonstrate the robustness, scalability, and practical efficiency of our approach.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤§æ¨ç†æ¨¡å‹ (LRMs) å°½ç®¡å…·å¤‡å……è¶³å®‰å…¨çŸ¥è¯†å´åœ¨æ¨ç†è¿‡ç¨‹ä¸­éš¾ä»¥æ¿€æ´»ï¼Œè¿›è€Œå¯¼è‡´æ‰§è¡Œæœ‰å®³æŒ‡ä»¤çš„é—®é¢˜ï¼Œæå‡ºäº† R1-ACT æ¡†æ¶ã€‚R1-ACT æ˜¯ä¸€ç§ç®€å•é«˜æ•ˆçš„è®­ç»ƒå (post-training) æ–¹æ³•ï¼Œæ—¨åœ¨é€šè¿‡ç»“æ„åŒ–æ¨ç†è¿‡ç¨‹æ˜¾å¼è§¦å‘æ¨¡å‹å†…ç½®çš„å®‰å…¨çŸ¥è¯†ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯¥æ–¹æ³•åœ¨æ˜¾è‘—æå‡æ¨¡å‹å®‰å…¨æ€§å¹¶è¶…è¶Šæ­¤å‰å¯¹é½æ–¹æ³•çš„åŒæ—¶ï¼Œèƒ½å¤Ÿå®Œå¥½åœ°ä¿ç•™æ¨¡å‹çš„æ¨ç†æ€§èƒ½ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼ŒR1-ACT è¡¨ç°å‡ºæé«˜çš„æ•ˆç‡ï¼Œä»…éœ€ 1,000 æ¡è®­ç»ƒæ•°æ®å¹¶åœ¨å•å— RTX A6000 GPU ä¸Šè®­ç»ƒ 90 åˆ†é’Ÿå³å¯å®Œæˆã€‚å¤šé¡¹é’ˆå¯¹ä¸åŒ LRM åŸºåº§å’Œè§„æ¨¡çš„å¹¿æ³›å®éªŒè¯æ˜äº†è¯¥æ–¹æ¡ˆçš„é²æ£’æ€§ã€å¯æ‰©å±•æ€§ä»¥åŠå®é™…åº”ç”¨ä¸­çš„é«˜æ•ˆæ€§ã€‚",
      "categories": [
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.AI",
      "comment": "under review",
      "pdf_url": "https://arxiv.org/pdf/2508.00324v1",
      "published_date": "2025-08-01 05:14:13 UTC",
      "updated_date": "2025-08-01 05:14:13 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T08:12:50.317596+00:00"
    },
    {
      "arxiv_id": "2508.00323v1",
      "title": "Oedipus and the Sphinx: Benchmarking and Improving Visual Language Models for Complex Graphic Reasoning",
      "title_zh": "Oedipus and the Sphinxï¼šé¢å‘å¤æ‚å›¾å½¢æ¨ç†çš„è§†è§‰è¯­è¨€æ¨¡å‹åŸºå‡†æµ‹è¯„ä¸æ”¹è¿›",
      "authors": [
        "Jianyi Zhang",
        "Xu Ji",
        "Ziyin Zhou",
        "Yuchen Zhou",
        "Shubo Shi",
        "Haoyu Wu",
        "Zhen Li",
        "Shizhao Liu"
      ],
      "abstract": "Evaluating the performance of visual language models (VLMs) in graphic reasoning tasks has become an important research topic. However, VLMs still show obvious deficiencies in simulating human-level graphic reasoning capabilities, especially in complex graphic reasoning and abstract problem solving, which are less studied and existing studies only focus on simple graphics. To evaluate the performance of VLMs in complex graphic reasoning, we propose ReasonBench, the first evaluation benchmark focused on structured graphic reasoning tasks, which includes 1,613 questions from real-world intelligence tests. ReasonBench covers reasoning dimensions related to location, attribute, quantity, and multi-element tasks, providing a comprehensive evaluation of the performance of VLMs in spatial, relational, and abstract reasoning capabilities. We benchmark 11 mainstream VLMs (including closed-source and open-source models) and reveal significant limitations of current models. Based on these findings, we propose a dual optimization strategy: Diagrammatic Reasoning Chain (DiaCoT) enhances the interpretability of reasoning by decomposing layers, and ReasonTune enhances the task adaptability of model reasoning through training, all of which improves VLM performance by 33.5\\%. All experimental data and code are in the repository: https://huggingface.co/datasets/cistine/ReasonBench.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹è§†è§‰è¯­è¨€æ¨¡å‹(VLMs)åœ¨å¤æ‚å›¾å½¢æ¨ç†å’ŒæŠ½è±¡é—®é¢˜è§£å†³æ–¹é¢çš„ä¸è¶³ï¼Œæå‡ºäº†é¦–ä¸ªä¸“æ³¨äºç»“æ„åŒ–å›¾å½¢æ¨ç†ä»»åŠ¡çš„è¯„ä¼°åŸºå‡†ReasonBenchï¼Œè¯¥åŸºå‡†åŒ…å«1,613ä¸ªæºè‡ªçœŸå®ä¸–ç•Œæ™ºåŠ›æµ‹è¯•çš„é—®é¢˜ã€‚ReasonBenchæ¶µç›–äº†ç©ºé—´ã€å…³ç³»ã€æ•°é‡å’Œå±æ€§ç­‰å¤šä¸ªæ¨ç†ç»´åº¦ï¼Œå¯¹11ç§ä¸»æµVLMsçš„åŸºå‡†æµ‹è¯•æ­ç¤ºäº†å½“å‰æ¨¡å‹åœ¨å¤„ç†å¤æ‚è§†è§‰é€»è¾‘æ—¶çš„æ˜¾è‘—å±€é™ã€‚ä¸ºäº†æ”¹è¿›æ¨¡å‹è¡¨ç°ï¼Œç ”ç©¶è€…æå‡ºäº†åŒé‡ä¼˜åŒ–ç­–ç•¥ï¼šDiagrammatic Reasoning Chain (DiaCoT) é€šè¿‡å±‚çº§åˆ†è§£æå‡æ¨ç†çš„å¯è§£é‡Šæ€§ï¼Œè€Œ ReasonTune åˆ™é€šè¿‡é’ˆå¯¹æ€§è®­ç»ƒå¢å¼ºæ¨¡å‹çš„ä»»åŠ¡é€‚åº”èƒ½åŠ›ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¿™äº›æ–¹æ³•ä½¿VLMsåœ¨å›¾å½¢æ¨ç†ä»»åŠ¡ä¸Šçš„æ€§èƒ½æå‡äº†33.5%ã€‚è¯¥å·¥ä½œä¸ºè¯„ä¼°å’Œå¢å¼ºå¤šæ¨¡æ€æ™ºèƒ½ä½“çš„æŠ½è±¡é€»è¾‘æ€ç»´æä¾›äº†é‡è¦çš„èµ„æºå’Œæ–¹æ³•è®ºæ”¯æŒã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.00323v1",
      "published_date": "2025-08-01 05:12:38 UTC",
      "updated_date": "2025-08-01 05:12:38 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T08:13:03.840049+00:00"
    },
    {
      "arxiv_id": "2508.00312v1",
      "title": "GV-VAD : Exploring Video Generation for Weakly-Supervised Video Anomaly Detection",
      "title_zh": "GV-VADï¼šé¢å‘å¼±ç›‘ç£è§†é¢‘å¼‚å¸¸æ£€æµ‹çš„è§†é¢‘ç”Ÿæˆæ¢ç´¢",
      "authors": [
        "Suhang Cai",
        "Xiaohao Peng",
        "Chong Wang",
        "Xiaojie Cai",
        "Jiangbo Qian"
      ],
      "abstract": "Video anomaly detection (VAD) plays a critical role in public safety applications such as intelligent surveillance. However, the rarity, unpredictability, and high annotation cost of real-world anomalies make it difficult to scale VAD datasets, which limits the performance and generalization ability of existing models. To address this challenge, we propose a generative video-enhanced weakly-supervised video anomaly detection (GV-VAD) framework that leverages text-conditioned video generation models to produce semantically controllable and physically plausible synthetic videos. These virtual videos are used to augment training data at low cost. In addition, a synthetic sample loss scaling strategy is utilized to control the influence of generated synthetic samples for efficient training. The experiments show that the proposed framework outperforms state-of-the-art methods on UCF-Crime datasets. The code is available at https://github.com/Sumutan/GV-VAD.git.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹è§†é¢‘å¼‚å¸¸æ£€æµ‹(Video Anomaly Detection, VAD)ä¸­çœŸå®å¼‚å¸¸æ ·æœ¬ç¨€å°‘ã€ä¸å¯é¢„æµ‹ä¸”æ ‡æ³¨æˆæœ¬é«˜å¯¼è‡´çš„è®­ç»ƒæ•°æ®å—é™é—®é¢˜ï¼Œæå‡ºäº† GV-VAD æ¡†æ¶ã€‚è¯¥æ¡†æ¶åˆ›æ–°æ€§åœ°åˆ©ç”¨æ–‡æœ¬æ¡ä»¶è§†é¢‘ç”Ÿæˆæ¨¡å‹(text-conditioned video generation models)äº§ç”Ÿè¯­ä¹‰å¯æ§ä¸”ç‰©ç†åˆç†çš„åˆæˆè§†é¢‘ï¼Œå®ç°äº†ä½æˆæœ¬çš„è®­ç»ƒæ•°æ®æ‰©å……ã€‚ä¸ºäº†ç¡®ä¿è®­ç»ƒæ•ˆç‡å¹¶å¹³è¡¡ç”Ÿæˆæ•°æ®çš„å½±å“ï¼Œç ”ç©¶è¿˜å¼•å…¥äº†åˆæˆæ ·æœ¬æŸå¤±ç¼©æ”¾(synthetic sample loss scaling)ç­–ç•¥ã€‚å®éªŒè¯æ˜ï¼ŒGV-VAD åœ¨ UCF-Crime æ•°æ®é›†ä¸Šçš„è¡¨ç°ä¼˜äºç›®å‰çš„å…ˆè¿›æ–¹æ³•ï¼Œä¸ºè§£å†³å¼±ç›‘ç£è§†é¢‘å¼‚å¸¸æ£€æµ‹ä¸­çš„æ•°æ®ç¨€ç¼ºé—®é¢˜æä¾›äº†æœ‰æ•ˆçš„ç”Ÿæˆå¼å¢å¼ºæ–¹æ¡ˆã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.00312v1",
      "published_date": "2025-08-01 04:42:40 UTC",
      "updated_date": "2025-08-01 04:42:40 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T08:13:08.253745+00:00"
    },
    {
      "arxiv_id": "2508.00307v1",
      "title": "Beamformed 360Â° Sound Maps: U-Net-Driven Acoustic Source Segmentation and Localization",
      "title_zh": "360Â° æ³¢æŸæˆå½¢å£°å­¦å›¾ï¼šåŸºäº U-Net çš„å£°æºåˆ†å‰²ä¸å®šä½",
      "authors": [
        "Belman Jahir Rodriguez",
        "Sergio F. Chevtchenko",
        "Marcelo Herrera Martinez",
        "Yeshwant Bethy",
        "Saeed Afshar"
      ],
      "abstract": "We introduce a U-net model for 360Â° acoustic source localization formulated as a spherical semantic segmentation task. Rather than regressing discrete direction-of-arrival (DoA) angles, our model segments beamformed audio maps (azimuth and elevation) into regions of active sound presence. Using delay-and-sum (DAS) beamforming on a custom 24-microphone array, we generate signals aligned with drone GPS telemetry to create binary supervision masks. A modified U-Net, trained on frequency-domain representations of these maps, learns to identify spatially distributed source regions while addressing class imbalance via the Tversky loss. Because the network operates on beamformed energy maps, the approach is inherently array-independent and can adapt to different microphone configurations without retraining from scratch. The segmentation outputs are post-processed by computing centroids over activated regions, enabling robust DoA estimates. Our dataset includes real-world open-field recordings of a DJI Air 3 drone, synchronized with 360Â° video and flight logs across multiple dates and locations. Experimental results show that U-net generalizes across environments, providing improved angular precision, offering a new paradigm for dense spatial audio understanding beyond traditional Sound Source Localization (SSL).",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§åŸºäº U-net çš„ 360Â° å£°æºå®šä½æ¨¡å‹ï¼Œé€šè¿‡å°†å…¶å®šä¹‰ä¸ºçƒé¢è¯­ä¹‰åˆ†å‰² (spherical semantic segmentation) ä»»åŠ¡ï¼Œå®ç°äº†å¯¹æ³¢æŸæˆå½¢ (beamformed) éŸ³é¢‘åœ°å›¾ä¸­æ´»åŠ¨å£°æºåŒºåŸŸçš„ç²¾ç¡®åˆ†å‰²ã€‚ç ”ç©¶åˆ©ç”¨ 24 éº¦å…‹é£é˜µåˆ—é€šè¿‡å»¶è¿Ÿå åŠ  (delay-and-sum) æ³¢æŸæˆå½¢æŠ€æœ¯ç”Ÿæˆä¿¡å·ï¼Œå¹¶ç»“åˆæ— äººæœº GPS é¥æµ‹æ•°æ®åˆ›å»ºç›‘ç£æ©ç ã€‚é€šè¿‡åœ¨é¢‘åŸŸè¡¨ç¤ºçš„åœ°å›¾ä¸Šè®­ç»ƒæ”¹è¿›çš„ U-Netï¼Œå¹¶å¼•å…¥ Tversky loss è§£å†³ç±»åˆ«ä¸å¹³è¡¡é—®é¢˜ï¼Œæ¨¡å‹èƒ½å¤Ÿæœ‰æ•ˆè¯†åˆ«ç©ºé—´åˆ†å¸ƒçš„å£°æºã€‚ç”±äºè¯¥æ–¹æ³•ç›´æ¥ä½œç”¨äºèƒ½é‡å›¾ï¼Œå› æ­¤å…·å¤‡é˜µåˆ—æ— å…³æ€§ (array-independent)ï¼Œå¯çµæ´»é€‚é…ä¸åŒçš„éº¦å…‹é£é…ç½®ã€‚é€šè¿‡å¯¹åˆ†å‰²åŒºåŸŸè®¡ç®—è´¨å¿ƒ (centroids) è·å¾—é²æ£’çš„åˆ°è¾¾æ–¹å‘ (DoA) ä¼°è®¡ï¼Œå®éªŒç»“æœè¯æ˜è¯¥æ¨¡å‹åœ¨çœŸå®é‡å¤–ç¯å¢ƒä¸‹å…·æœ‰æé«˜çš„è§’åº¦ç²¾åº¦å’Œæ³›åŒ–èƒ½åŠ›ã€‚è¿™ä¸€æˆæœä¸ºå¯†é›†ç©ºé—´éŸ³é¢‘ç†è§£æä¾›äº†è¶…è¶Šä¼ ç»Ÿå£°æºå®šä½ (SSL) æ¡†æ¶çš„æ–°èŒƒå¼ã€‚",
      "categories": [
        "eess.AS",
        "cs.AI",
        "cs.SD",
        "eess.SP"
      ],
      "primary_category": "eess.AS",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.00307v1",
      "published_date": "2025-08-01 04:23:18 UTC",
      "updated_date": "2025-08-01 04:23:18 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T08:13:10.543754+00:00"
    },
    {
      "arxiv_id": "2508.08279v1",
      "title": "XFMNet: Decoding Cross-Site and Nonstationary Water Patterns via Stepwise Multimodal Fusion for Long-Term Water Quality Forecasting",
      "title_zh": "XFMNetï¼šé€šè¿‡æ­¥è¿›å¼å¤šæ¨¡æ€èåˆè§£æè·¨ç«™ç‚¹ä¸éå¹³ç¨³æ°´è´¨æ¨¡å¼ï¼Œå®ç°é•¿æœŸæ°´è´¨é¢„æµ‹",
      "authors": [
        "Ziqi Wang",
        "Hailiang Zhao",
        "Cheng Bao",
        "Wenzhuo Qian",
        "Yuhao Yang",
        "Xueqiang Sun",
        "Shuiguang Deng"
      ],
      "abstract": "Long-term time-series forecasting is critical for environmental monitoring, yet water quality prediction remains challenging due to complex periodicity, nonstationarity, and abrupt fluctuations induced by ecological factors. These challenges are further amplified in multi-site scenarios that require simultaneous modeling of temporal and spatial dynamics. To tackle this, we introduce XFMNet, a stepwise multimodal fusion network that integrates remote sensing precipitation imagery to provide spatial and environmental context in river networks. XFMNet first aligns temporal resolutions between water quality series and remote sensing inputs via adaptive downsampling, followed by locally adaptive decomposition to disentangle trend and cycle components. A cross-attention gated fusion module dynamically integrates temporal patterns with spatial and ecological cues, enhancing robustness to nonstationarity and site-specific anomalies. Through progressive and recursive fusion, XFMNet captures both long-term trends and short-term fluctuations. Extensive experiments on real-world datasets demonstrate substantial improvements over state-of-the-art baselines, highlighting the effectiveness of XFMNet for spatially distributed time series prediction.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† XFMNetï¼Œä¸€ç§é€šè¿‡é€æ­¥å¤šæ¨¡æ€èåˆè§£ç è·¨ç«™ç‚¹å’Œéå¹³ç¨³æ°´è´¨æ¨¡å¼çš„é¢„æµ‹ç½‘ç»œï¼Œæ—¨åœ¨è§£å†³é•¿æœŸæ°´è´¨é¢„æµ‹ä¸­å¤æ‚çš„å‘¨æœŸæ€§ã€éå¹³ç¨³æ€§ä»¥åŠå¤šç«™ç‚¹æ—¶ç©ºåŠ¨æ€å»ºæ¨¡éš¾é¢˜ã€‚è¯¥æ¡†æ¶åˆ›æ–°æ€§åœ°å¼•å…¥é¥æ„Ÿé™æ°´å›¾åƒä½œä¸ºç¯å¢ƒèƒŒæ™¯ï¼Œé€šè¿‡è‡ªé€‚åº”ä¸‹é‡‡æ ·(adaptive downsampling)å®ç°å¤šæºæ•°æ®çš„æ—¶é—´åˆ†è¾¨ç‡å¯¹é½ï¼Œå¹¶åˆ©ç”¨å±€éƒ¨è‡ªé€‚åº”åˆ†è§£(locally adaptive decomposition)æœ‰æ•ˆåˆ†ç¦»è¶‹åŠ¿ä¸å‘¨æœŸæˆåˆ†ã€‚å…¶æ ¸å¿ƒçš„äº¤å‰æ³¨æ„åŠ›é—¨æ§èåˆæ¨¡å—(cross-attention gated fusion module)èƒ½å¤ŸåŠ¨æ€æ•´åˆæ—¶é—´æ¨¡å¼ä¸ç©ºé—´ç”Ÿæ€çº¿ç´¢ï¼Œæ˜¾è‘—å¢å¼ºäº†å¯¹ç«™ç‚¹ç‰¹å®šå¼‚å¸¸çš„é²æ£’æ€§ã€‚é€šè¿‡æ¸è¿›å¼å’Œé€’å½’å¼èåˆæœºåˆ¶ï¼ŒXFMNet èƒ½å¤ŸåŒæ—¶ç²¾å‡†æ•æ‰æ°´è´¨çš„é•¿æœŸè¶‹åŠ¿å’ŒçŸ­æœŸæ³¢åŠ¨ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ¨¡å‹åœ¨çœŸå®ä¸–ç•Œæ•°æ®é›†ä¸Šæ˜æ˜¾ä¼˜äºç°æœ‰çš„å…ˆè¿›åŸºå‡†æ¨¡å‹ï¼Œä¸ºç©ºé—´åˆ†å¸ƒæ—¶é—´åºåˆ—é¢„æµ‹æä¾›äº†é«˜æ•ˆçš„è§£å†³æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.08279v1",
      "published_date": "2025-08-01 04:11:36 UTC",
      "updated_date": "2025-08-01 04:11:36 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T08:13:10.338524+00:00"
    },
    {
      "arxiv_id": "2508.00300v2",
      "title": "MetaExplainer: A Framework to Generate Multi-Type User-Centered Explanations for AI Systems",
      "title_zh": "MetaExplainerï¼šé¢å‘äººå·¥æ™ºèƒ½ç³»ç»Ÿçš„å¤šç±»å‹ã€ä»¥ç”¨æˆ·ä¸ºä¸­å¿ƒçš„è§£é‡Šç”Ÿæˆæ¡†æ¶",
      "authors": [
        "Shruthi Chari",
        "Oshani Seneviratne",
        "Prithwish Chakraborty",
        "Pablo Meyer",
        "Deborah L. McGuinness"
      ],
      "abstract": "Explanations are crucial for building trustworthy AI systems, but a gap often exists between the explanations provided by models and those needed by users. To address this gap, we introduce MetaExplainer, a neuro-symbolic framework designed to generate user-centered explanations. Our approach employs a three-stage process: first, we decompose user questions into machine-readable formats using state-of-the-art large language models (LLM); second, we delegate the task of generating system recommendations to model explainer methods; and finally, we synthesize natural language explanations that summarize the explainer outputs. Throughout this process, we utilize an Explanation Ontology to guide the language models and explainer methods. By leveraging LLMs and a structured approach to explanation generation, MetaExplainer aims to enhance the interpretability and trustworthiness of AI systems across various applications, providing users with tailored, question-driven explanations that better meet their needs. Comprehensive evaluations of MetaExplainer demonstrate a step towards evaluating and utilizing current state-of-the-art explanation frameworks. Our results show high performance across all stages, with a 59.06% F1-score in question reframing, 70% faithfulness in model explanations, and 67% context-utilization in natural language synthesis. User studies corroborate these findings, highlighting the creativity and comprehensiveness of generated explanations. Tested on the Diabetes (PIMA Indian) tabular dataset, MetaExplainer supports diverse explanation types, including Contrastive, Counterfactual, Rationale, Case-Based, and Data explanations. The framework's versatility and traceability from using ontology to guide LLMs suggest broad applicability beyond the tested scenarios, positioning MetaExplainer as a promising tool for enhancing AI explainability across various domains.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†MetaExplainerï¼Œè¿™æ˜¯ä¸€ä¸ªæ—¨åœ¨ç”Ÿæˆä»¥ç”¨æˆ·ä¸ºä¸­å¿ƒçš„è§£é‡Šçš„ç¥ç»ç¬¦å·(neuro-symbolic)æ¡†æ¶ï¼Œæ—¨åœ¨å¼¥è¡¥æ¨¡å‹æä¾›çš„è§£é‡Šä¸ç”¨æˆ·å®é™…éœ€æ±‚ä¹‹é—´çš„å·®è·ã€‚è¯¥æ¡†æ¶é‡‡ç”¨äº†ä¸‰é˜¶æ®µæµç¨‹ï¼šé¦–å…ˆåˆ©ç”¨å¤§è¯­è¨€æ¨¡å‹(LLM)å°†ç”¨æˆ·é—®é¢˜åˆ†è§£ä¸ºæœºå™¨å¯è¯»æ ¼å¼ï¼Œæ¥ç€é€šè¿‡è§£é‡Šå™¨æ–¹æ³•(model explainer methods)ç”Ÿæˆç³»ç»Ÿæ¨èï¼Œæœ€ååˆæˆæ€»ç»“è¾“å‡ºç»“æœçš„è‡ªç„¶è¯­è¨€è§£é‡Šã€‚åœ¨æ•´ä¸ªè¿‡ç¨‹ä¸­ï¼Œç ”ç©¶å¼•å…¥äº†è§£é‡Šæœ¬ä½“(Explanation Ontology)æ¥æŒ‡å¯¼è¯­è¨€æ¨¡å‹å’Œè§£é‡Šå™¨ï¼Œç¡®ä¿ç”Ÿæˆçš„è§£é‡Šå…·æœ‰ç»“æ„åŒ–å’Œè¿½æº¯æ€§ã€‚MetaExplaineræ”¯æŒå¤šç§è§£é‡Šç±»å‹ï¼ŒåŒ…æ‹¬å¯¹æ¯”æ€§(Contrastive)ã€åäº‹å®(Counterfactual)ã€åŸºæœ¬åŸç†(Rationale)ã€åŸºäºæ¡ˆä¾‹(Case-Based)å’Œæ•°æ®(Data)è§£é‡Šã€‚åœ¨ç³–å°¿ç—…(Diabetes)è¡¨æ ¼æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œè¯¥æ¡†æ¶åœ¨é—®é¢˜é‡æ„(question reframing)æ–¹é¢è¾¾åˆ°äº†59.06%çš„F1-scoreï¼Œæ¨¡å‹è§£é‡Šçš„å¿ å®åº¦(faithfulness)ä¸º70%ï¼Œè‡ªç„¶è¯­è¨€åˆæˆçš„ä¸Šä¸‹æ–‡åˆ©ç”¨ç‡(context-utilization)ä¸º67%ã€‚ç”¨æˆ·ç ”ç©¶è¿›ä¸€æ­¥è¯å®äº†ç”Ÿæˆè§£é‡Šçš„åˆ›æ„æ€§ä¸å…¨é¢æ€§ï¼Œè¯æ˜äº†MetaExplaineråœ¨æå‡AIç³»ç»Ÿå¯è§£é‡Šæ€§å’Œå¯ä¿¡åº¦æ–¹é¢çš„æ½œåŠ›ã€‚",
      "categories": [
        "cs.HC",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.HC",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.00300v2",
      "published_date": "2025-08-01 04:01:40 UTC",
      "updated_date": "2025-09-10 01:46:21 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T08:13:20.654917+00:00"
    },
    {
      "arxiv_id": "2508.00299v1",
      "title": "Controllable Pedestrian Video Editing for Multi-View Driving Scenarios via Motion Sequence",
      "title_zh": "åŸºäºåŠ¨ä½œåºåˆ—çš„å¤šè§†è§’é©¾é©¶åœºæ™¯å¯æ§è¡Œäººè§†é¢‘ç¼–è¾‘",
      "authors": [
        "Danzhen Fu",
        "Jiagao Hu",
        "Daiguo Zhou",
        "Fei Wang",
        "Zepeng Wang",
        "Wenhua Liao"
      ],
      "abstract": "Pedestrian detection models in autonomous driving systems often lack robustness due to insufficient representation of dangerous pedestrian scenarios in training datasets. To address this limitation, we present a novel framework for controllable pedestrian video editing in multi-view driving scenarios by integrating video inpainting and human motion control techniques. Our approach begins by identifying pedestrian regions of interest across multiple camera views, expanding detection bounding boxes with a fixed ratio, and resizing and stitching these regions into a unified canvas while preserving cross-view spatial relationships. A binary mask is then applied to designate the editable area, within which pedestrian editing is guided by pose sequence control conditions. This enables flexible editing functionalities, including pedestrian insertion, replacement, and removal. Extensive experiments demonstrate that our framework achieves high-quality pedestrian editing with strong visual realism, spatiotemporal coherence, and cross-view consistency. These results establish the proposed method as a robust and versatile solution for multi-view pedestrian video generation, with broad potential for applications in data augmentation and scenario simulation in autonomous driving.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹è‡ªåŠ¨é©¾é©¶ç³»ç»Ÿåœ¨å±é™©è¡Œäººåœºæ™¯ä¸‹å› æ•°æ®ä¸è¶³å¯¼è‡´é²æ£’æ€§æ¬ ç¼ºçš„é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§ç»“åˆè§†é¢‘è¡¥å…¨(video inpainting)å’Œäººä½“è¿åŠ¨æ§åˆ¶æŠ€æœ¯çš„å¯æ§è¡Œäººè§†é¢‘ç¼–è¾‘æ¡†æ¶ã€‚è¯¥æ–¹æ³•é¦–å…ˆè¯†åˆ«å¤šè§†å›¾ä¸­çš„è¡Œäººæ„Ÿå…´è¶£åŒºåŸŸ(ROI)ï¼Œå°†å…¶è°ƒæ•´å°ºå¯¸å¹¶æ‹¼æ¥è‡³ç»Ÿä¸€ç”»å¸ƒä¸­ï¼ŒåŒæ—¶ä¿ç•™è·¨è§†å›¾çš„ç©ºé—´å…³ç³»ã€‚é€šè¿‡åº”ç”¨äºŒå€¼æ©ç (binary mask)å®šä¹‰å¯ç¼–è¾‘åŒºåŸŸï¼Œå¹¶åˆ©ç”¨å§¿æ€åºåˆ—(pose sequence)ä½œä¸ºæ§åˆ¶æ¡ä»¶ï¼Œå®ç°äº†è¡Œäººçš„æ’å…¥ã€æ›¿æ¢å’Œç§»é™¤ç­‰çµæ´»ç¼–è¾‘åŠŸèƒ½ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ¡†æ¶åœ¨ä¿æŒè§†è§‰çœŸå®æ€§ã€æ—¶ç©ºè¿è´¯æ€§å’Œè·¨è§†å›¾ä¸€è‡´æ€§æ–¹é¢è¡¨ç°ä¼˜å¼‚ï¼Œèƒ½å¤Ÿç”Ÿæˆé«˜è´¨é‡çš„è¡Œäººç¼–è¾‘è§†é¢‘ã€‚è¯¥ç ”ç©¶ä¸ºå¤šè§†å›¾è¡Œäººè§†é¢‘ç”Ÿæˆæä¾›äº†ä¸€ç§é²æ£’ä¸”é€šç”¨çš„è§£å†³æ–¹æ¡ˆï¼Œåœ¨è‡ªåŠ¨é©¾é©¶çš„æ•°æ®å¢å¼ºå’Œåœºæ™¯æ¨¡æ‹Ÿé¢†åŸŸå…·æœ‰å¹¿æ³›çš„åº”ç”¨æ½œåŠ›ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.RO"
      ],
      "primary_category": "cs.CV",
      "comment": "ICCV 2025 Workshop (HiGen)",
      "pdf_url": "https://arxiv.org/pdf/2508.00299v1",
      "published_date": "2025-08-01 03:56:57 UTC",
      "updated_date": "2025-08-01 03:56:57 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T08:13:36.723615+00:00"
    },
    {
      "arxiv_id": "2508.03741v1",
      "title": "Latent Knowledge Scalpel: Precise and Massive Knowledge Editing for Large Language Models",
      "title_zh": "Latent Knowledge Scalpelï¼šå¤§è¯­è¨€æ¨¡å‹çš„ç²¾å‡†ä¸å¤§è§„æ¨¡çŸ¥è¯†ç¼–è¾‘",
      "authors": [
        "Xin Liu",
        "Qiyang Song",
        "Shaowen Xu",
        "Kerou Zhou",
        "Wenbo Jiang",
        "Xiaoqi Jia",
        "Weijuan Zhang",
        "Heqing Huang",
        "Yakai Li"
      ],
      "abstract": "Large Language Models (LLMs) often retain inaccurate or outdated information from pre-training, leading to incorrect predictions or biased outputs during inference. While existing model editing methods can address this challenge, they struggle with editing large amounts of factual information simultaneously and may compromise the general capabilities of the models. In this paper, our empirical study demonstrates that it is feasible to edit the internal representations of LLMs and replace the entities in a manner similar to editing natural language inputs. Based on this insight, we introduce the Latent Knowledge Scalpel (LKS), an LLM editor that manipulates the latent knowledge of specific entities via a lightweight hypernetwork to enable precise and large-scale editing. Experiments conducted on Llama-2 and Mistral show even with the number of simultaneous edits reaching 10,000, LKS effectively performs knowledge editing while preserving the general abilities of the edited LLMs. Code is available at: https://github.com/Linuxin-xxx/LKS.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤§è¯­è¨€æ¨¡å‹ (LLMs) åœ¨é¢„è®­ç»ƒä¸­æ®‹ç•™çš„é”™è¯¯æˆ–è¿‡æ—¶ä¿¡æ¯ï¼Œæå‡ºäº†åä¸º Latent Knowledge Scalpel (LKS) çš„ç¼–è¾‘æ¡†æ¶ã€‚ç°æœ‰çš„æ¨¡å‹ç¼–è¾‘æ–¹æ³•åœ¨åŒæ—¶å¤„ç†å¤§è§„æ¨¡äº‹å®ä¿¡æ¯æ—¶å¾€å¾€é¢ä¸´æŒ‘æˆ˜ï¼Œä¸”å®¹æ˜“æŸå®³æ¨¡å‹çš„é€šç”¨èƒ½åŠ›ã€‚ä½œè€…é€šè¿‡å®è¯ç ”ç©¶å‘ç°ï¼Œå¯ä»¥åƒç¼–è¾‘è‡ªç„¶è¯­è¨€è¾“å…¥ä¸€æ ·ä¿®æ”¹æ¨¡å‹çš„å†…éƒ¨è¡¨ç¤ºå¹¶æ›¿æ¢å®ä½“ã€‚åŸºäºæ­¤æ´å¯Ÿï¼ŒLKS åˆ©ç”¨è½»é‡çº§çš„ hypernetwork æ“æ§ç‰¹å®šå®ä½“çš„ latent knowledgeï¼Œä»è€Œå®ç°ç²¾å‡†ä¸”å¤§è§„æ¨¡çš„çŸ¥è¯†ç¼–è¾‘ã€‚åœ¨ Llama-2 å’Œ Mistral ä¸Šçš„å®éªŒç»“æœæ˜¾ç¤ºï¼Œå³ä½¿åœ¨åŒæ—¶è¿›è¡Œ 10,000 æ¬¡ç¼–è¾‘çš„æƒ…å†µä¸‹ï¼ŒLKS ä¾ç„¶èƒ½å¤Ÿæœ‰æ•ˆæ‰§è¡ŒçŸ¥è¯†æ›´æ–°ï¼Œå¹¶å®Œæ•´ä¿ç•™è¢«ç¼–è¾‘æ¨¡å‹çš„é€šç”¨èƒ½åŠ›ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "Accepted by ECAI 2025 - 28th European Conference on Artificial Intelligence",
      "pdf_url": "https://arxiv.org/pdf/2508.03741v1",
      "published_date": "2025-08-01 03:51:43 UTC",
      "updated_date": "2025-08-01 03:51:43 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T08:13:48.048535+00:00"
    },
    {
      "arxiv_id": "2508.10913v1",
      "title": "SDSNN: A Single-Timestep Spiking Neural Network with Self-Dropping Neuron and Bayesian Optimization",
      "title_zh": "SDSNNï¼šåŸºäºè‡ªä¸¢å¼ƒç¥ç»å…ƒä¸è´å¶æ–¯ä¼˜åŒ–çš„å•æ—¶é—´æ­¥è„‰å†²ç¥ç»ç½‘ç»œ",
      "authors": [
        "Changqing Xu",
        "Buxuan Song",
        "Yi Liu",
        "Xinfang Liao",
        "Wenbin Zheng",
        "Yintang Yang"
      ],
      "abstract": "Spiking Neural Networks (SNNs), as an emerging biologically inspired computational model, demonstrate significant energy efficiency advantages due to their event-driven information processing mechanism. Compared to traditional Artificial Neural Networks (ANNs), SNNs transmit information through discrete spike signals, which substantially reduces computational energy consumption through their sparse encoding approach. However, the multi-timestep computation model significantly increases inference latency and energy, limiting the applicability of SNNs in edge computing scenarios. We propose a single-timestep SNN, which enhances accuracy and reduces computational energy consumption in a single timestep by optimizing spike generation and temporal parameters. We design a Self-Dropping Neuron mechanism, which enhances information-carrying capacity through dynamic threshold adjustment and selective spike suppression. Furthermore, we employ Bayesian optimization to globally search for time parameters and obtain an efficient inference mode with a single time step. Experimental results on the Fashion-MNIST, CIFAR-10, and CIFAR-100 datasets demonstrate that, compared to traditional multi-timestep SNNs employing the Leaky Integrate-and-Fire (LIF) model, our method achieves classification accuracies of 93.72%, 92.20%, and 69.45%, respectively, using only single-timestep spikes, while maintaining comparable or even superior accuracy. Additionally, it reduces energy consumption by 56%, 21%, and 22%, respectively.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†SDSNNï¼Œè¿™æ˜¯ä¸€ç§å•æ—¶é—´æ­¥è„‰å†²ç¥ç»ç½‘ç»œ(Single-Timestep Spiking Neural Network)ï¼Œæ—¨åœ¨è§£å†³ä¼ ç»Ÿå¤šæ—¶é—´æ­¥æ¨¡å‹åœ¨æ¨ç†å»¶è¿Ÿå’Œèƒ½è€—æ–¹é¢çš„å±€é™æ€§ã€‚ç ”ç©¶å›¢é˜Ÿè®¾è®¡äº†è‡ªä¸¢å¼ƒç¥ç»å…ƒ(Self-Dropping Neuron)æœºåˆ¶ï¼Œé€šè¿‡åŠ¨æ€é˜ˆå€¼è°ƒæ•´å’Œé€‰æ‹©æ€§è„‰å†²æŠ‘åˆ¶æ¥æå‡ç¥ç»å…ƒçš„ä¿¡æ¯æ‰¿è½½èƒ½åŠ›ã€‚åŒæ—¶ï¼Œè¯¥æ–¹æ³•å¼•å…¥è´å¶æ–¯ä¼˜åŒ–(Bayesian optimization)å…¨å±€æœç´¢æ—¶é—´å‚æ•°ï¼Œä»¥è·å–é«˜æ•ˆçš„å•æ—¶é—´æ­¥æ¨ç†æ¨¡å¼ã€‚åœ¨Fashion-MNISTã€CIFAR-10å’ŒCIFAR-100æ•°æ®é›†ä¸Šçš„å®éªŒè¯æ˜ï¼ŒSDSNNåœ¨ä»…ä½¿ç”¨å•æ—¶é—´æ­¥çš„æƒ…å†µä¸‹è¾¾åˆ°äº†ä¸ä¼ ç»ŸLeaky Integrate-and-Fire (LIF)å¤šæ—¶é—´æ­¥æ¨¡å‹ç›¸å½“ç”šè‡³æ›´ä¼˜çš„å‡†ç¡®ç‡ã€‚æ­¤å¤–ï¼Œè¯¥æ¨¡å‹åœ¨ä¸åŒä»»åŠ¡ä¸­æ˜¾è‘—é™ä½äº†èƒ½è€—ï¼Œé™å¹…æœ€é«˜è¾¾56%ï¼Œä¸ºè„‰å†²ç¥ç»ç½‘ç»œåœ¨è¾¹ç¼˜è®¡ç®—åœºæ™¯çš„åº”ç”¨æä¾›äº†ä½å»¶è¿Ÿä¸”èŠ‚èƒ½çš„è§£å†³æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.NE",
        "cs.AI"
      ],
      "primary_category": "cs.NE",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.10913v1",
      "published_date": "2025-08-01 03:41:47 UTC",
      "updated_date": "2025-08-01 03:41:47 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T08:13:42.244745+00:00"
    },
    {
      "arxiv_id": "2508.00294v1",
      "title": "Formal Power Series Representations in Probability and Expected Utility Theory",
      "title_zh": "æ¦‚ç‡ä¸æœŸæœ›æ•ˆç”¨ç†è®ºä¸­çš„å½¢å¼å¹‚çº§æ•°è¡¨ç¤º",
      "authors": [
        "Arthur Paul Pedersen",
        "Samuel Allen Alexander"
      ],
      "abstract": "We advance a general theory of coherent preference that surrenders restrictions embodied in orthodox doctrine. This theory enjoys the property that any preference system admits extension to a complete system of preferences, provided it satisfies a certain coherence requirement analogous to the one de Finetti advanced for his foundations of probability. Unlike de Finetti's theory, the one we set forth requires neither transitivity nor Archimedeanness nor boundedness nor continuity of preference. This theory also enjoys the property that any complete preference system meeting the standard of coherence can be represented by utility in an ordered field extension of the reals. Representability by utility is a corollary of this paper's central result, which at once extends HÃ¶lder's Theorem and strengthens Hahn's Embedding Theorem.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§å…³äºç›¸å¹²åå¥½(coherent preference)çš„é€šç”¨ç†è®ºï¼Œæ”¾å®½äº†ä¼ ç»Ÿæ­£ç»Ÿå­¦è¯´ä¸­çš„è¯¸å¤šé™åˆ¶ã€‚è¯¥ç†è®ºå…è®¸ä»»ä½•æ»¡è¶³ç‰¹å®šç›¸å¹²æ€§è¦æ±‚çš„åå¥½ç³»ç»Ÿæ‰©å±•ä¸ºå®Œæ•´ç³»ç»Ÿï¼Œå…¶ç›¸å¹²æ€§è¦æ±‚ç±»ä¼¼äºde Finettiåœ¨æ¦‚ç‡è®ºåŸºç¡€ä¸­æå‡ºçš„æ ‡å‡†ã€‚ä¸de Finettiçš„ç†è®ºä¸åŒï¼Œè¯¥æ¡†æ¶ä¸éœ€è¦åå¥½å…·å¤‡ä¼ é€’æ€§(transitivity)ã€é˜¿åŸºç±³å¾·æ€§(Archimedeanness)ã€æœ‰ç•Œæ€§(boundedness)æˆ–è¿ç»­æ€§(continuity)ã€‚ç ”ç©¶è¯æ˜ï¼Œä»»ä½•æ»¡è¶³ç›¸å¹²æ ‡å‡†çš„å®Œæ•´åå¥½ç³»ç»Ÿå‡å¯åœ¨å®æ•°åŸŸçš„æœ‰åºåŸŸæ‰©å¼ (ordered field extension of the reals)ä¸­é€šè¿‡æ•ˆç”¨(utility)è¿›è¡Œè¡¨ç¤ºã€‚è¿™ä¸€è¡¨ç¤ºç»“è®ºæºäºè¯¥æ–‡çš„æ ¸å¿ƒæ•°å­¦æˆæœï¼Œè¯¥æˆæœåŒæ—¶æ‰©å±•äº†HÃ¶lder's Theoremå¹¶å¼ºåŒ–äº†Hahn's Embedding Theoremï¼Œä¸ºæœŸæœ›æ•ˆç”¨ç†è®º(Expected Utility Theory)å¥ å®šäº†æ›´å…·ä¸€èˆ¬æ€§çš„æ•°å­¦åŸºç¡€ã€‚",
      "categories": [
        "math.PR",
        "cs.AI",
        "econ.TH",
        "math.LO",
        "math.ST"
      ],
      "primary_category": "math.PR",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.00294v1",
      "published_date": "2025-08-01 03:34:39 UTC",
      "updated_date": "2025-08-01 03:34:39 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T08:13:47.138522+00:00"
    },
    {
      "arxiv_id": "2508.02721v1",
      "title": "Blueprint First, Model Second: A Framework for Deterministic LLM Workflow",
      "title_zh": "è“å›¾ä¼˜å…ˆï¼Œæ¨¡å‹åœ¨åï¼šä¸€ç§ç¡®å®šæ€§å¤§è¯­è¨€æ¨¡å‹å·¥ä½œæµæ¡†æ¶",
      "authors": [
        "Libin Qiu",
        "Yuhang Ye",
        "Zhirong Gao",
        "Xide Zou",
        "Junfu Chen",
        "Ziming Gui",
        "Weizhi Huang",
        "Xiaobo Xue",
        "Wenkai Qiu",
        "Kun Zhao"
      ],
      "abstract": "While powerful, the inherent non-determinism of large language model (LLM) agents limits their application in structured operational environments where procedural fidelity and predictable execution are strict requirements. This limitation stems from current architectures that conflate probabilistic, high-level planning with low-level action execution within a single generative process. To address this, we introduce the Source Code Agent framework, a new paradigm built on the \"Blueprint First, Model Second\" philosophy. Our framework decouples the workflow logic from the generative model. An expert-defined operational procedure is first codified into a source code-based Execution Blueprint, which is then executed by a deterministic engine. The LLM is strategically invoked as a specialized tool to handle bounded, complex sub-tasks within the workflow, but never to decide the workflow's path. We conduct a comprehensive evaluation on the challenging tau-bench benchmark, designed for complex user-tool-rule scenarios. Our results demonstrate that the Source Code Agent establishes a new state-of-the-art, outperforming the strongest baseline by 10.1 percentage points on the average Pass^1 score while dramatically improving execution efficiency. Our work enables the verifiable and reliable deployment of autonomous agents in applications governed by strict procedural logic.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤§è¯­è¨€æ¨¡å‹(LLM)æ™ºèƒ½ä½“åœ¨ç»“æ„åŒ–è¿è¥ç¯å¢ƒä¸­çš„éç¡®å®šæ€§å±€é™ï¼Œæå‡ºäº†Source Code Agentæ¡†æ¶ã€‚è¯¥æ¡†æ¶éµå¾ªâ€œBlueprint First, Model Secondâ€å“²å­¦ï¼Œé€šè¿‡å°†å·¥ä½œæµé€»è¾‘ä¸ç”Ÿæˆå¼æ¨¡å‹è§£è€¦ï¼Œè§£å†³äº†é«˜å±‚è§„åˆ’ä¸åº•å±‚æ‰§è¡Œæ··æ·†çš„é—®é¢˜ã€‚é€šè¿‡å°†ä¸“å®¶å®šä¹‰çš„ç¨‹åºç¼–ç ä¸ºåŸºäºæºä»£ç çš„Execution Blueprintå¹¶ç”±ç¡®å®šæ€§å¼•æ“æ‰§è¡Œï¼ŒLLMä»…ä½œä¸ºå¤„ç†ç‰¹å®šå¤æ‚å­ä»»åŠ¡çš„å·¥å…·ï¼Œè€Œä¸è´Ÿè´£å†³å®šå·¥ä½œæµè·¯å¾„ã€‚åœ¨å…·æœ‰æŒ‘æˆ˜æ€§çš„tau-benchåŸºå‡†æµ‹è¯•ä¸­ï¼ŒSource Code Agentå°†å¹³å‡Pass^1åˆ†æ•°æå‡äº†10.1ä¸ªç™¾åˆ†ç‚¹ï¼Œåˆ·æ–°äº†SOTAè®°å½•å¹¶æ˜¾è‘—æé«˜äº†æ‰§è¡Œæ•ˆç‡ã€‚è¯¥ç ”ç©¶ä¸ºåœ¨ä¸¥è‹›ç¨‹åºé€»è¾‘é©±åŠ¨çš„åº”ç”¨åœºæ™¯ä¸­éƒ¨ç½²å¯éªŒè¯ä¸”å¯é çš„è‡ªä¸»æ™ºèƒ½ä½“æä¾›äº†æ–°æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.SE",
        "cs.AI",
        "cs.PL"
      ],
      "primary_category": "cs.SE",
      "comment": "8 pages, 6 figures, 3 tables",
      "pdf_url": "https://arxiv.org/pdf/2508.02721v1",
      "published_date": "2025-08-01 03:10:00 UTC",
      "updated_date": "2025-08-01 03:10:00 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T08:13:53.848375+00:00"
    },
    {
      "arxiv_id": "2508.08278v1",
      "title": "Towards Heterogeneity-Aware and Energy-Efficient Topology Optimization for Decentralized Federated Learning in Edge Environment",
      "title_zh": "é¢å‘è¾¹ç¼˜ç¯å¢ƒå»ä¸­å¿ƒåŒ–è”é‚¦å­¦ä¹ çš„å¼‚æ„æ„ŸçŸ¥ä¸é«˜èƒ½æ•ˆæ‹“æ‰‘ä¼˜åŒ–",
      "authors": [
        "Yuze Liu",
        "Tiehua Zhang",
        "Zhishu Shen",
        "Libing Wu",
        "Shiping Chen",
        "Jiong Jin"
      ],
      "abstract": "Federated learning (FL) has emerged as a promising paradigm within edge computing (EC) systems, enabling numerous edge devices to collaboratively train artificial intelligence (AI) models while maintaining data privacy. To overcome the communication bottlenecks associated with centralized parameter servers, decentralized federated learning (DFL), which leverages peer-to-peer (P2P) communication, has been extensively explored in the research community. Although researchers design a variety of DFL approach to ensure model convergence, its iterative learning process inevitably incurs considerable cost along with the growth of model complexity and the number of participants. These costs are largely influenced by the dynamic changes of topology in each training round, particularly its sparsity and connectivity conditions. Furthermore, the inherent resources heterogeneity in the edge environments affects energy efficiency of learning process, while data heterogeneity degrades model performance. These factors pose significant challenges to the design of an effective DFL framework for EC systems. To this end, we propose Hat-DFed, a heterogeneity-aware and coset-effective decentralized federated learning (DFL) framework. In Hat-DFed, the topology construction is formulated as a dual optimization problem, which is then proven to be NP-hard, with the goal of maximizing model performance while minimizing cumulative energy consumption in complex edge environments. To solve this problem, we design a two-phase algorithm that dynamically constructs optimal communication topologies while unbiasedly estimating their impact on both model performance and energy cost. Additionally, the algorithm incorporates an importance-aware model aggregation mechanism to mitigate performance degradation caused by data heterogeneity.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹è¾¹ç¼˜è®¡ç®—(Edge Computing)ç¯å¢ƒä¸­å»ä¸­å¿ƒåŒ–è”é‚¦å­¦ä¹ (Decentralized Federated Learning)é¢ä¸´çš„é€šä¿¡ç“¶é¢ˆã€èµ„æºå¼‚æ„æ€§(Resource Heterogeneity)å¯¼è‡´çš„èƒ½æ•ˆé—®é¢˜ä»¥åŠæ•°æ®å¼‚æ„æ€§(Data Heterogeneity)å¸¦æ¥çš„æ€§èƒ½ä¸‹é™æŒ‘æˆ˜ã€‚ä¸ºæ­¤ï¼Œä½œè€…æå‡ºäº†Hat-DFedæ¡†æ¶ï¼Œæ—¨åœ¨é€šè¿‡ä¼˜åŒ–æ‹“æ‰‘æ„å»º(Topology Construction)å®ç°å¼‚æ„æ„ŸçŸ¥ä¸é«˜èƒ½æ•ˆçš„åä½œå­¦ä¹ ã€‚è¯¥æ¡†æ¶å°†æ‹“æ‰‘æ„å»ºå»ºæ¨¡ä¸ºä¸€ä¸ªåŒé‡ä¼˜åŒ–é—®é¢˜ï¼Œåœ¨æœ€å¤§åŒ–æ¨¡å‹æ€§èƒ½çš„åŒæ—¶æœ€å°åŒ–ç´¯è®¡èƒ½è€—ï¼Œå¹¶è¯æ˜äº†è¯¥é—®é¢˜çš„NP-hardç‰¹æ€§ã€‚ä¸ºäº†æ±‚è§£è¯¥é—®é¢˜ï¼Œç ”ç©¶è®¾è®¡äº†ä¸€ç§ä¸¤é˜¶æ®µç®—æ³•æ¥åŠ¨æ€æ„å»ºæœ€ä¼˜é€šä¿¡æ‹“æ‰‘ï¼Œå¹¶èƒ½å¤Ÿæ— ååœ°ä¼°è®¡å…¶å¯¹æ€§èƒ½å’Œèƒ½è€—çš„å½±å“ã€‚æ­¤å¤–ï¼Œè¯¥ç®—æ³•è¿˜å¼•å…¥äº†é‡è¦æ€§æ„ŸçŸ¥æ¨¡å‹èšåˆ(Importance-aware Model Aggregation)æœºåˆ¶ï¼Œä»¥æœ‰æ•ˆç¼“è§£ç”±æ•°æ®å¼‚æ„æ€§å¼•èµ·çš„æ¨¡å‹æ€§èƒ½é€€åŒ–ã€‚Hat-DFedé€šè¿‡åœ¨å¤æ‚è¾¹ç¼˜ç¯å¢ƒä¸­åŠ¨æ€ä¼˜åŒ–æ‹“æ‰‘ç»“æ„ï¼Œä¸ºå®ç°é«˜æ€§èƒ½ä¸”ä½èƒ½æ•ˆçš„å»ä¸­å¿ƒåŒ–äººå·¥æ™ºèƒ½æ¨¡å‹è®­ç»ƒæä¾›äº†å¯é çš„è§£å†³æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.08278v1",
      "published_date": "2025-08-01 03:07:32 UTC",
      "updated_date": "2025-08-01 03:07:32 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T08:13:49.343476+00:00"
    },
    {
      "arxiv_id": "2508.00282v2",
      "title": "Mind the Gap: The Divergence Between Human and LLM-Generated Tasks",
      "title_zh": "å…³æ³¨å·®è·ï¼šäººç±»ä¸å¤§è¯­è¨€æ¨¡å‹ç”Ÿæˆä»»åŠ¡ä¹‹é—´çš„åˆ†æ­§",
      "authors": [
        "Yi-Long Lu",
        "Jiajun Song",
        "Chunhui Zhang",
        "Wei Wang"
      ],
      "abstract": "Humans constantly generate a diverse range of tasks guided by internal motivations. While generative agents powered by large language models (LLMs) aim to simulate this complex behavior, it remains uncertain whether they operate on similar cognitive principles. To address this, we conducted a task-generation experiment comparing human responses with those of an LLM agent (GPT-4o). We find that human task generation is consistently influenced by psychological drivers, including personal values (e.g., Openness to Change) and cognitive style. Even when these psychological drivers are explicitly provided to the LLM, it fails to reflect the corresponding behavioral patterns. They produce tasks that are markedly less social, less physical, and thematically biased toward abstraction. Interestingly, while the LLM's tasks were perceived as more fun and novel, this highlights a disconnect between its linguistic proficiency and its capacity to generate human-like, embodied goals. We conclude that there is a core gap between the value-driven, embodied nature of human cognition and the statistical patterns of LLMs, highlighting the necessity of incorporating intrinsic motivation and physical grounding into the design of more human-aligned agents.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†äººç±»ä¸å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨ä»»åŠ¡ç”Ÿæˆä¸­çš„è®¤çŸ¥åŸç†å·®å¼‚ï¼Œå¯¹æ¯”äº†äººç±»å—è¯•è€…ä¸ GPT-4o æ™ºèƒ½ä½“åœ¨å¤„ç†å—å¿ƒç†é©±åŠ¨å› ç´ å½±å“çš„ä»»åŠ¡æ—¶çš„è¡¨ç°ã€‚ç ”ç©¶å‘ç°ï¼Œäººç±»çš„ä»»åŠ¡ç”Ÿæˆå—ä¸ªäººä»·å€¼è§‚å’Œè®¤çŸ¥é£æ ¼ç­‰å¿ƒç†å› ç´ æŒç»­å½±å“ï¼Œè€Œ LLM å³ä½¿åœ¨æ˜ç¡®è·å¾—è¿™äº›èƒŒæ™¯ä¿¡æ¯æ—¶ï¼Œä¹Ÿæ— æ³•å‡†ç¡®åæ˜ ç›¸åº”çš„è¡Œä¸ºæ¨¡å¼ã€‚LLM ç”Ÿæˆçš„ä»»åŠ¡è¡¨ç°å‡ºç¤¾äº¤æ€§å’Œç‰©ç†æ€§è¾ƒä½çš„ç‰¹å¾ï¼Œä¸”åœ¨ä¸»é¢˜ä¸Šæ›´å€¾å‘äºæŠ½è±¡åŒ–ã€‚å°½ç®¡ LLM ç”Ÿæˆçš„ä»»åŠ¡åœ¨è¶£å‘³æ€§å’Œæ–°é¢–æ€§ä¸Šå¾—åˆ†è¾ƒé«˜ï¼Œä½†è¿™åæ˜ äº†å…¶è¯­è¨€ç†Ÿç»ƒåº¦ä¸äº§ç”Ÿç±»äººå…·èº«ç›®æ ‡ (embodied goals) èƒ½åŠ›ä¹‹é—´çš„è„±èŠ‚ã€‚ç ”ç©¶æœ€ç»ˆæŒ‡å‡ºï¼Œäººç±»ä»·å€¼é©±åŠ¨ã€å…·èº«çš„è®¤çŸ¥æœ¬è´¨ä¸ LLM çš„ç»Ÿè®¡æ¨¡å¼ä¹‹é—´å­˜åœ¨æ ¸å¿ƒé¸¿æ²Ÿï¼Œå¼ºè°ƒäº†åœ¨è®¾è®¡æ›´å…·äººç±»å¯¹é½ç‰¹æ€§çš„æ™ºèƒ½ä½“æ—¶ï¼Œæ•´åˆå†…åœ¨åŠ¨æœºä¸ç‰©ç†æ¥åœ° (physical grounding) çš„é‡è¦æ€§ã€‚",
      "categories": [
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.00282v2",
      "published_date": "2025-08-01 03:00:41 UTC",
      "updated_date": "2025-08-05 09:10:21 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T08:13:57.789767+00:00"
    },
    {
      "arxiv_id": "2508.02720v1",
      "title": "ECGTwin: Personalized ECG Generation Using Controllable Diffusion Model",
      "title_zh": "ECGTwinï¼šåŸºäºå¯æ§æ‰©æ•£æ¨¡å‹çš„ä¸ªæ€§åŒ–å¿ƒç”µå›¾ç”Ÿæˆ",
      "authors": [
        "Yongfan Lai",
        "Bo Liu",
        "Xinyan Guan",
        "Qinghao Zhao",
        "Hongyan Li",
        "Shenda Hong"
      ],
      "abstract": "Personalized electrocardiogram (ECG) generation is to simulate a patient's ECG digital twins tailored to specific conditions. It has the potential to transform traditional healthcare into a more accurate individualized paradigm, while preserving the key benefits of conventional population-level ECG synthesis. However, this promising task presents two fundamental challenges: extracting individual features without ground truth and injecting various types of conditions without confusing generative model. In this paper, we present ECGTwin, a two-stage framework designed to address these challenges. In the first stage, an Individual Base Extractor trained via contrastive learning robustly captures personal features from a reference ECG. In the second stage, the extracted individual features, along with a target cardiac condition, are integrated into the diffusion-based generation process through our novel AdaX Condition Injector, which injects these signals via two dedicated and specialized pathways. Both qualitative and quantitative experiments have demonstrated that our model can not only generate ECG signals of high fidelity and diversity by offering a fine-grained generation controllability, but also preserving individual-specific features. Furthermore, ECGTwin shows the potential to enhance ECG auto-diagnosis in downstream application, confirming the possibility of precise personalized healthcare solutions.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† ECGTwinï¼Œè¿™æ˜¯ä¸€ä¸ªæ—¨åœ¨ç”Ÿæˆä¸ªæ€§åŒ–å¿ƒç”µå›¾ (ECG) æ•°å­—å­ªç”Ÿçš„ä¸¤é˜¶æ®µæ¡†æ¶ï¼Œä»¥åº”å¯¹åœ¨ç¼ºä¹ Ground Truth çš„æƒ…å†µä¸‹æå–ä¸ªä½“ç‰¹å¾ä»¥åŠåœ¨ä¸å¹²æ‰°ç”Ÿæˆæ¨¡å‹çš„å‰æä¸‹æ³¨å…¥å¤æ‚æ¡ä»¶è¿™ä¸¤å¤§æŒ‘æˆ˜ã€‚åœ¨ç¬¬ä¸€é˜¶æ®µï¼Œæ¡†æ¶åˆ©ç”¨é€šè¿‡å¯¹æ¯”å­¦ä¹  (Contrastive Learning) è®­ç»ƒçš„ Individual Base Extractor ä»å‚è€ƒ ECG ä¸­ç¨³å¥åœ°æ•æ‰ä¸ªä½“ç‰¹å¾ã€‚åœ¨ç¬¬äºŒé˜¶æ®µï¼Œæ‰€æå–çš„ä¸ªä½“ç‰¹å¾ä¸ç›®æ ‡å¿ƒè„çŠ¶æ€é€šè¿‡åˆ›æ–°çš„ AdaX Condition Injector æ•´åˆè¿›åŸºäº Diffusion Model çš„ç”Ÿæˆè¿‡ç¨‹ä¸­ï¼Œé€šè¿‡ä¸¤æ¡ä¸“ç”¨è·¯å¾„ç¡®ä¿ä¿¡å·çš„ç²¾å‡†æ³¨å…¥ã€‚å®šæ€§ä¸å®šé‡å®éªŒè¡¨æ˜ï¼ŒECGTwin ä¸ä»…èƒ½ç”Ÿæˆå…·æœ‰é«˜ Fidelity å’Œå¤šæ ·æ€§çš„ä¿¡å·ï¼Œè¿˜å®ç°äº†ç»†ç²’åº¦çš„ç”Ÿæˆå¯æ§æ€§å¹¶æœ‰æ•ˆä¿ç•™äº†ä¸ªä½“ç‰¹å¼‚æ€§ç‰¹å¾ã€‚æ­¤å¤–ï¼Œè¯¥æ¨¡å‹åœ¨ä¸‹æ¸¸çš„ ECG è‡ªåŠ¨è¯Šæ–­ä»»åŠ¡ä¸­å±•ç°å‡ºæ˜¾è‘—çš„å¢å¼ºæ½œåŠ›ï¼Œä¸ºå®ç°ç²¾å‡†çš„ä¸ªæ€§åŒ–åŒ»ç–—è§£å†³æ–¹æ¡ˆå¥ å®šäº†åŸºç¡€ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.02720v1",
      "published_date": "2025-08-01 02:58:11 UTC",
      "updated_date": "2025-08-01 02:58:11 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T08:14:01.552834+00:00"
    },
    {
      "arxiv_id": "2508.02719v1",
      "title": "ZetA: A Riemann Zeta-Scaled Extension of Adam for Deep Learning",
      "title_zh": "ZetAï¼šåŸºäº Riemann Zeta ç¼©æ”¾çš„æ·±åº¦å­¦ä¹  Adam æ‰©å±•",
      "authors": [
        "Samiksha BC"
      ],
      "abstract": "This work introduces ZetA, a novel deep learning optimizer that extends Adam by incorporating dynamic scaling based on the Riemann zeta function. To the best of our knowledge, ZetA is the first optimizer to apply zeta-based gradient scaling within deep learning optimization. The method improves generalization and robustness through a hybrid update mechanism that integrates adaptive damping, cosine similarity-based momentum boosting, entropy-regularized loss, and Sharpness-Aware Minimization (SAM)-style perturbations. Empirical evaluations on SVHN, CIFAR10, CIFAR100, STL10, and noisy CIFAR10 consistently show test accuracy improvements over Adam. All experiments employ a lightweight fully connected network trained for five epochs under mixed-precision settings. The results demonstrate that ZetA is a computationally efficient and robust alternative to Adam, particularly effective in noisy or high-granularity classification tasks.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† ZetAï¼Œä¸€ç§åŸºäº Riemann zeta å‡½æ•°åŠ¨æ€ç¼©æ”¾çš„æ–°å‹æ·±åº¦å­¦ä¹ ä¼˜åŒ–å™¨ï¼Œæ—¨åœ¨å¯¹ä¼ ç»Ÿçš„ Adam ä¼˜åŒ–å™¨è¿›è¡Œæ‰©å±•ã€‚è¿™æ˜¯é¦–ä¸ªå°† zeta å‡½æ•°åº”ç”¨äºæ¢¯åº¦ç¼©æ”¾çš„ä¼˜åŒ–æ–¹æ³•ï¼Œé€šè¿‡ç»“åˆè‡ªé€‚åº”é˜»å°¼ (adaptive damping)ã€ä½™å¼¦ç›¸ä¼¼åº¦åŠ¨é‡å¢å¼º (cosine similarity-based momentum boosting)ã€ç†µæ­£åˆ™åŒ–æŸå¤±ä»¥åŠ SAM é£æ ¼çš„æ‰°åŠ¨ï¼Œæ„å»ºäº†ç‹¬ç‰¹çš„æ··åˆæ›´æ–°æœºåˆ¶ã€‚åœ¨ SVHNã€CIFAR10ã€CIFAR100 åŠ STL10 ç­‰å¤šä¸ªæ•°æ®é›†ä¸Šçš„å®éªŒéªŒè¯è¡¨æ˜ï¼ŒZetA çš„æµ‹è¯•å‡†ç¡®ç‡ä¸€è‡´ä¼˜äº Adamã€‚ç ”ç©¶å‘ç°ï¼Œè¯¥æ–¹æ³•åœ¨å¤„ç†å™ªå£°æ•°æ®æˆ–é«˜ç²’åº¦åˆ†ç±»ä»»åŠ¡æ—¶å…·æœ‰æ˜¾è‘—çš„é²æ£’æ€§ã€‚ZetA åœ¨ä¿æŒè®¡ç®—æ•ˆç‡çš„åŒæ—¶ï¼Œæ˜¾è‘—æå‡äº†æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›å’Œç¨³å¥æ€§ã€‚å®éªŒç»“æœè¯æ˜ï¼ŒZetA æ˜¯æ·±åº¦å­¦ä¹ ä¼˜åŒ–é¢†åŸŸä¸­ Adam çš„ä¸€ä¸ªé«˜æ•ˆä¸”æå…·ç«äº‰åŠ›çš„æ›¿ä»£æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "6 pages, 1 figure, 4 references. This paper introduces a hybrid optimizer combining Adam with Riemann zeta-based scaling",
      "pdf_url": "https://arxiv.org/pdf/2508.02719v1",
      "published_date": "2025-08-01 02:53:29 UTC",
      "updated_date": "2025-08-01 02:53:29 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T08:14:03.456188+00:00"
    },
    {
      "arxiv_id": "2508.03740v1",
      "title": "VQ-DeepISC: Vector Quantized-Enabled Digital Semantic Communication with Channel Adaptive Image Transmission",
      "title_zh": "VQ-DeepISCï¼šåŸºäºçŸ¢é‡é‡åŒ–çš„æ•°å­—è¯­ä¹‰é€šä¿¡åŠä¿¡é“è‡ªé€‚åº”å›¾åƒä¼ è¾“",
      "authors": [
        "Jianqiao Chen",
        "Tingting Zhu",
        "Huishi Song",
        "Nan Ma",
        "Xiaodong Xu"
      ],
      "abstract": "Discretization of semantic features enables interoperability between semantic and digital communication systems, showing significant potential for practical applications. The fundamental difficulty in digitizing semantic features stems from the need to preserve continuity and context in inherently analog representations during their compression into discrete symbols while ensuring robustness to channel degradation. In this paper, we propose a vector quantized (VQ)-enabled digital semantic communication system with channel adaptive image transmission, named VQ-DeepISC. Guided by deep joint source-channel coding (DJSCC), we first design a Swin Transformer backbone for hierarchical semantic feature extraction, followed by VQ modules projecting features into discrete latent spaces. Consequently, it enables efficient index-based transmission instead of raw feature transmission. To further optimize this process, we develop an attention mechanism-driven channel adaptation module to dynamically optimize index transmission. Secondly, to counteract codebook collapse during training process, we impose a distributional regularization by minimizing the Kullback-Leibler divergence (KLD) between codeword usage frequencies and a uniform prior. Meanwhile, exponential moving average (EMA) is employed to stabilize training and ensure balanced feature coverage during codebook updates. Finally, digital communication is implemented using quadrature phase shift keying (QPSK) modulation alongside orthogonal frequency division multiplexing (OFDM), adhering to the IEEE 802.11a standard. Experimental results demonstrate superior reconstruction fidelity of the proposed system over benchmark methods.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† VQ-DeepISCï¼Œä¸€ç§æ”¯æŒçŸ¢é‡é‡åŒ– (Vector Quantized) çš„æ•°å­—è¯­ä¹‰é€šä¿¡ç³»ç»Ÿï¼Œæ—¨åœ¨è§£å†³è¯­ä¹‰ç‰¹å¾æ•°å­—åŒ–è¿‡ç¨‹ä¸­ä¿æŒè¿ç»­æ€§åŠå¢å¼ºä¿¡é“ç¨³å¥æ€§çš„éš¾é¢˜ã€‚è¯¥æ¡†æ¶ä»¥æ·±åº¦è”åˆæºä¿¡é“ç¼–ç  (DJSCC) ä¸ºæŒ‡å¯¼ï¼Œé‡‡ç”¨ Swin Transformer éª¨å¹²ç½‘ç»œè¿›è¡Œå±‚æ¬¡åŒ–è¯­ä¹‰ç‰¹å¾æå–ï¼Œå¹¶é€šè¿‡ VQ æ¨¡å—å°†ç‰¹å¾æ˜ å°„è‡³ç¦»æ•£æ½œç©ºé—´ä»¥å®ç°é«˜æ•ˆçš„ç´¢å¼•ä¼ è¾“ã€‚ä¸ºäº†æå‡ä¼ è¾“æ•ˆç‡ï¼Œç ”ç©¶è®¾è®¡äº†æ³¨æ„åŠ›æœºåˆ¶é©±åŠ¨çš„ä¿¡é“è‡ªé€‚åº”æ¨¡å— (Channel Adaptation Module)ï¼Œå¹¶å¼•å…¥åˆ†å¸ƒæ­£åˆ™åŒ–ä¸æŒ‡æ•°ç§»åŠ¨å¹³å‡ (EMA) æŠ€æœ¯æ¥é˜²æ­¢ç æœ¬å´©æºƒ (Codebook Collapse) å¹¶ç¨³å®šè®­ç»ƒè¿‡ç¨‹ã€‚ç³»ç»Ÿæœ€ç»ˆé€šè¿‡ QPSK è°ƒåˆ¶å’Œ OFDM æŠ€æœ¯å®ç°ï¼Œç¬¦åˆ IEEE 802.11a æ ‡å‡†ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒVQ-DeepISC åœ¨å›¾åƒé‡å»ºä¿çœŸåº¦ä¸Šæ˜¾è‘—ä¼˜äºç°æœ‰åŸºå‡†æ–¹æ³•ï¼Œä¸ºå®ç°è¯­ä¹‰é€šä¿¡ä¸æ•°å­—ç³»ç»Ÿçš„äº’æ“ä½œæ€§æä¾›äº†é‡è¦æŠ€æœ¯æ”¯æ’‘ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.03740v1",
      "published_date": "2025-08-01 02:35:34 UTC",
      "updated_date": "2025-08-01 02:35:34 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T08:14:07.254665+00:00"
    },
    {
      "arxiv_id": "2508.00271v2",
      "title": "MetaAgent: Toward Self-Evolving Agent via Tool Meta-Learning",
      "title_zh": "MetaAgentï¼šé€šè¿‡å·¥å…·å…ƒå­¦ä¹ å®ç°è‡ªæˆ‘è¿›åŒ–æ™ºèƒ½ä½“",
      "authors": [
        "Hongjin Qian",
        "Zheng Liu"
      ],
      "abstract": "In this work, we propose MetaAgent, an agentic paradigm inspired by the principle of learning-by-doing, where expertise is developed through hands-on practice and continual self-improvement. MetaAgent starts with a minimal workflow, equipped only with basic reasoning and adaptive help-seeking abilities. When a knowledge gap is encountered, MetaAgent generates natural language help requests, which are routed to the most suitable external tool by a dedicated tool router. As MetaAgent solves tasks, it continually conducts self-reflection and answer verification, distilling actionable experience into concise texts that are dynamically incorporated into future task contexts. Besides, MetaAgent autonomously builds in-house tools and a persistent knowledge base by organizing its tool-use history, further enhancing its ability to retrieve and integrate relevant information We term this continual, data-driven process as \\textit{meta tool learning}, through which MetaAgent incrementally refines its reasoning and tool-use strategies, without changing model parameters or requiring further post-training. Evaluated on challenging knowledge discovery benchmarks, including GAIA, WebWalkerQA, and BrowseCamp, MetaAgent consistently outperforms workflow-based baselines and matches or exceeds end-to-end trained agents, demonstrating the promise of self-evolving agentic systems for robust, general-purpose knowledge discovery. We provide our source codes in https://github.com/qhjqhj00/MetaAgent.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† MetaAgentï¼Œè¿™æ˜¯ä¸€ç§å— learning-by-doing åŸç†å¯å‘çš„å¯è¿›åŒ–æ™ºèƒ½ä½“èŒƒå¼ï¼Œæ—¨åœ¨é€šè¿‡ meta tool learning å®ç°æŒç»­çš„è‡ªæˆ‘æ”¹è¿›ã€‚MetaAgent åˆå§‹ä»…å…·å¤‡åŸºç¡€æ¨ç†å’Œè‡ªé€‚åº”å¯»åŠ©èƒ½åŠ›ï¼Œåœ¨é‡åˆ°çŸ¥è¯†é¸¿æ²Ÿæ—¶ä¼šç”Ÿæˆè‡ªç„¶è¯­è¨€è¯·æ±‚å¹¶ç”±ä¸“é—¨çš„ tool router è·¯ç”±è‡³æœ€åˆé€‚çš„å¤–éƒ¨å·¥å…·ã€‚åœ¨ä»»åŠ¡è§£å†³è¿‡ç¨‹ä¸­ï¼ŒMetaAgent é€šè¿‡è‡ªæˆ‘åæ€å’Œç­”æ¡ˆéªŒè¯å°†ç»éªŒæç‚¼ä¸ºæ–‡æœ¬ï¼Œå¹¶å°†å…¶åŠ¨æ€æ•´åˆè‡³æœªæ¥ä»»åŠ¡çš„ä¸Šä¸‹æ–‡ä¸­ã€‚æ­¤å¤–ï¼Œè¯¥æ™ºèƒ½ä½“èƒ½é€šè¿‡ç»„ç»‡å·¥å…·ä½¿ç”¨å†å²è‡ªä¸»æ„å»ºå†…éƒ¨å·¥å…·å’ŒæŒä¹…åŒ–çŸ¥è¯†åº“ï¼Œä»è€Œæ˜¾è‘—æå‡ä¿¡æ¯æ£€ç´¢ä¸æ•´åˆæ•ˆç‡ã€‚å®éªŒè¡¨æ˜ï¼ŒMetaAgent åœ¨æ— éœ€è°ƒæ•´å‚æ•°æˆ–äºŒæ¬¡è®­ç»ƒçš„æƒ…å†µä¸‹ï¼Œåœ¨ GAIAã€WebWalkerQA å’Œ BrowseCamp ç­‰æŒ‘æˆ˜æ€§åŸºå‡†æµ‹è¯•ä¸­ä¸€è‡´ä¼˜äºåŸºäºå·¥ä½œæµçš„åŸºçº¿æ¨¡å‹ï¼Œå±•ç¤ºäº†è‡ªè¿›åŒ–ç³»ç»Ÿåœ¨é²æ£’ã€é€šç”¨çŸ¥è¯†å‘ç°æ–¹é¢çš„å·¨å¤§æ½œåŠ›ã€‚",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.IR"
      ],
      "primary_category": "cs.AI",
      "comment": "Technical Report, 14 pages",
      "pdf_url": "https://arxiv.org/pdf/2508.00271v2",
      "published_date": "2025-08-01 02:30:32 UTC",
      "updated_date": "2025-09-01 02:48:41 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T08:14:24.239613+00:00"
    },
    {
      "arxiv_id": "2508.00264v2",
      "title": "Calibrated Language Models and How to Find Them with Label Smoothing",
      "title_zh": "æ ¡å‡†è¯­è¨€æ¨¡å‹ï¼šå¦‚ä½•åˆ©ç”¨æ ‡ç­¾å¹³æ»‘å®ç°æ¨¡å‹æ ¡å‡†",
      "authors": [
        "Jerry Huang",
        "Peng Lu",
        "Qiuhao Zeng"
      ],
      "abstract": "Recent advances in natural language processing (NLP) have opened up greater opportunities to enable fine-tuned large language models (LLMs) to behave as more powerful interactive agents through improved instruction-following ability. However, understanding how this impacts confidence calibration for reliable model output has not been researched in full. In this work, we examine various open-sourced LLMs, identifying significant calibration degradation after instruction tuning in each. Seeking a practical solution, we look towards label smoothing, which has been shown as an effective method to regularize for overconfident predictions but has yet to be widely adopted in the supervised fine-tuning (SFT) of LLMs. We first provide insight as to why label smoothing is sufficient to maintain calibration throughout the SFT process. However, settings remain where the effectiveness of smoothing is severely diminished, in particular the case of large vocabulary LLMs (LV-LLMs). We posit the cause to stem from the ability to become over-confident, which has a direct relationship with the hidden size and vocabulary size, and justify this theoretically and experimentally. Finally, we address an outstanding issue regarding the memory footprint of the cross-entropy loss computation in the label smoothed loss setting, designing a customized kernel to dramatically reduce memory consumption without sacrificing speed or performance in comparison to existing solutions for non-smoothed losses.",
      "tldr_zh": "è¯¥ç ”ç©¶æ·±å…¥æ¢è®¨äº†æŒ‡ä»¤å¾®è°ƒ(instruction tuning)å¯¹å¤§è¯­è¨€æ¨¡å‹(LLMs)ç½®ä¿¡åº¦æ ¡å‡†(confidence calibration)çš„å½±å“ï¼Œæ­ç¤ºäº†æ¨¡å‹åœ¨ç»è¿‡å¾®è°ƒåæ™®éä¼šå‡ºç°æ˜¾è‘—çš„æ ¡å‡†é€€åŒ–åŠè¿‡åº¦è‡ªä¿¡ç°è±¡ã€‚ä¸ºæå‡æ¨¡å‹è¾“å‡ºçš„å¯é æ€§ï¼Œä½œè€…æå‡ºåœ¨æœ‰ç›‘ç£å¾®è°ƒ(SFT)è¿‡ç¨‹ä¸­å¼•å…¥æ ‡ç­¾å¹³æ»‘(label smoothing)ä½œä¸ºæ­£åˆ™åŒ–æ‰‹æ®µï¼Œå¹¶ç³»ç»Ÿåˆ†æäº†å…¶åœ¨ç»´æŒæ ¡å‡†æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚ç ”ç©¶æŒ‡å‡ºï¼Œæ¨¡å‹è¿‡åº¦è‡ªä¿¡çš„å€¾å‘ä¸éšè—å±‚ç»´åº¦åŠè¯æ±‡é‡å¤§å°(vocabulary size)å­˜åœ¨ç›´æ¥å…³è”ï¼Œè¿™è§£é‡Šäº†æ ‡ç­¾å¹³æ»‘åœ¨å¤§è¯æ±‡é‡æ¨¡å‹(LV-LLMs)ä¸­æ•ˆæœå—é™çš„åŸå› ã€‚æ­¤å¤–ï¼Œé’ˆå¯¹æ ‡ç­¾å¹³æ»‘åœ¨è®¡ç®—äº¤å‰ç†µæŸå¤±æ—¶æ˜¾å­˜å ç”¨è¿‡é«˜çš„é—®é¢˜ï¼Œç ”ç©¶è®¾è®¡äº†ä¸€ç§å®šåˆ¶åŒ–ç®—å­(customized kernel)ï¼Œåœ¨ä¸æŸå¤±æ€§èƒ½çš„å‰æä¸‹å¤§å¹…ä¼˜åŒ–äº†å†…å­˜æ•ˆç‡ã€‚è¯¥é¡¹å·¥ä½œä¸ä»…ä¸ºç†è§£LLMçš„æ ¡å‡†æœºåˆ¶æä¾›äº†ç†è®ºæ”¯æ’‘ï¼Œè¿˜ä¸ºæ„å»ºé«˜æ€§èƒ½ä¸”æ ¡å‡†è‰¯å¥½çš„äº¤äº’å¼æ™ºèƒ½ä½“æä¾›äº†å®ç”¨çš„å·¥ç¨‹æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "stat.ML"
      ],
      "primary_category": "cs.LG",
      "comment": "Accepted to the Forty-second International Conference on Machine Learning (ICML) 2025. First two authors contributed equally. Official proceedings version available at https://proceedings.mlr.press/v267/huang25w.html",
      "pdf_url": "https://arxiv.org/pdf/2508.00264v2",
      "published_date": "2025-08-01 02:12:20 UTC",
      "updated_date": "2025-10-24 08:36:02 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T08:14:32.338957+00:00"
    },
    {
      "arxiv_id": "2508.00256v2",
      "title": "Large AI Model-Enabled Secure Communications in Low-Altitude Wireless Networks: Concepts, Perspectives and Case Study",
      "title_zh": "AI å¤§æ¨¡å‹èµ‹èƒ½çš„ä½ç©ºæ— çº¿ç½‘ç»œå®‰å…¨é€šä¿¡ï¼šæ¦‚å¿µã€å±•æœ›ä¸æ¡ˆä¾‹ç ”ç©¶",
      "authors": [
        "Chuang Zhang",
        "Geng Sun",
        "Yijing Lin",
        "Weijie Yuan",
        "Sinem Coleri",
        "Dusit Niyato"
      ],
      "abstract": "Low-altitude wireless networks (LAWNs) have the potential to revolutionize communications by supporting a range of applications, including urban parcel delivery, aerial inspections and air taxis. However, compared with traditional wireless networks, LAWNs face unique security challenges due to low-altitude operations, frequent mobility and reliance on unlicensed spectrum, making it more vulnerable to some malicious attacks. In this paper, we investigate some large artificial intelligence model (LAM)-enabled solutions for secure communications in LAWNs. Specifically, we first explore the amplified security risks and important limitations of traditional AI methods in LAWNs. Then, we introduce the basic concepts of LAMs and delve into the role of LAMs in addressing these challenges. To demonstrate the practical benefits of LAMs for secure communications in LAWNs, we propose a novel LAM-based optimization framework that leverages large language models (LLMs) to generate enhanced state features on top of handcrafted representations, and to design intrinsic rewards accordingly, thereby improving reinforcement learning performance for secure communication tasks. Through a typical case study, simulation results validate the effectiveness of the proposed framework. Finally, we outline future directions for integrating LAMs into secure LAWN applications.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†åœ¨ä½ç©ºæ— çº¿ç½‘ç»œ (Low-altitude wireless networks, LAWNs) ä¸­åˆ©ç”¨å¤§äººå·¥æ™ºèƒ½æ¨¡å‹ (Large AI Model, LAM) å®ç°å®‰å…¨é€šä¿¡çš„æ½œåœ¨æ–¹æ¡ˆã€‚é’ˆå¯¹ LAWNs å› ä½ç©ºä½œä¸šã€é¢‘ç¹ç§»åŠ¨å’Œä¾èµ–å…æˆæƒé¢‘è°±è€Œé¢ä¸´çš„ç‹¬ç‰¹å®‰å…¨é£é™©ï¼Œè®ºæ–‡æ·±å…¥åˆ†æäº†ä¼ ç»Ÿ AI æ–¹æ³•åœ¨åº”å¯¹æ¶æ„æ”»å‡»æ—¶çš„å±€é™æ€§ã€‚ä¸ºæ­¤ï¼Œä½œè€…æå‡ºäº†ä¸€ç§åˆ›æ–°çš„åŸºäº LAM çš„ä¼˜åŒ–æ¡†æ¶ï¼Œè¯¥æ¡†æ¶åˆ©ç”¨å¤§è¯­è¨€æ¨¡å‹ (Large Language Models, LLMs) åœ¨ä¼ ç»Ÿæ‰‹å·¥è¡¨ç¤ºçš„åŸºç¡€ä¸Šç”Ÿæˆå¢å¼ºçš„çŠ¶æ€ç‰¹å¾ï¼Œå¹¶æ®æ­¤è®¾è®¡å†…åœ¨å¥–åŠ±ï¼Œä»è€Œæ˜¾è‘—æå‡äº†å¼ºåŒ–å­¦ä¹  (Reinforcement Learning) åœ¨å®‰å…¨é€šä¿¡ä»»åŠ¡ä¸­çš„è¡¨ç°ã€‚ä»¿çœŸå®éªŒç»“æœéªŒè¯äº†æ‰€ææ¡†æ¶åœ¨å¤æ‚é€šä¿¡åœºæ™¯ä¸‹çš„æœ‰æ•ˆæ€§ã€‚æœ€åï¼Œæ–‡ç« æ€»ç»“äº†å°† LAM é›†æˆåˆ° LAWN å®‰å…¨åº”ç”¨ä¸­çš„å‰ç»æ€§æ–¹å‘ï¼Œä¸ºæ„å»ºæ™ºèƒ½åŒ–ä¸”å®‰å…¨çš„ä½ç©ºé€šä¿¡åŸºç¡€è®¾æ–½æä¾›äº†ç†è®ºä¸å®è·µæŒ‡å¯¼ã€‚",
      "categories": [
        "cs.NI",
        "cs.AI"
      ],
      "primary_category": "cs.NI",
      "comment": "This paper has been accepted to IEEE Communications Magazine",
      "pdf_url": "https://arxiv.org/pdf/2508.00256v2",
      "published_date": "2025-08-01 01:53:58 UTC",
      "updated_date": "2026-01-20 08:53:47 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T08:14:27.847592+00:00"
    },
    {
      "arxiv_id": "2508.00255v1",
      "title": "Accurate and Consistent Graph Model Generation from Text with Large Language Models",
      "title_zh": "åŸºäºå¤§è¯­è¨€æ¨¡å‹çš„å‡†ç¡®ä¸”ä¸€è‡´çš„æ–‡æœ¬åˆ°å›¾æ¨¡å‹ç”Ÿæˆ",
      "authors": [
        "Boqi Chen",
        "Ou Wei",
        "Bingzhou Zheng",
        "Gunter Mussbacher"
      ],
      "abstract": "Graph model generation from natural language description is an important task with many applications in software engineering. With the rise of large language models (LLMs), there is a growing interest in using LLMs for graph model generation. Nevertheless, LLM-based graph model generation typically produces partially correct models that suffer from three main issues: (1) syntax violations: the generated model may not adhere to the syntax defined by its metamodel, (2) constraint inconsistencies: the structure of the model might not conform to some domain-specific constraints, and (3) inaccuracy: due to the inherent uncertainty in LLMs, the models can include inaccurate, hallucinated elements. While the first issue is often addressed through techniques such as constraint decoding or filtering, the latter two remain largely unaddressed. Motivated by recent self-consistency approaches in LLMs, we propose a novel abstraction-concretization framework that enhances the consistency and quality of generated graph models by considering multiple outputs from an LLM. Our approach first constructs a probabilistic partial model that aggregates all candidate outputs and then refines this partial model into the most appropriate concrete model that satisfies all constraints. We evaluate our framework on several popular open-source and closed-source LLMs using diverse datasets for model generation tasks. The results demonstrate that our approach significantly improves both the consistency and quality of the generated graph models.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹åˆ©ç”¨ Large Language Models (LLMs) ä»è‡ªç„¶è¯­è¨€æè¿°ç”Ÿæˆå›¾æ¨¡å‹æ—¶é¢ä¸´çš„è¯­æ³•å†²çªã€çº¦æŸä¸ä¸€è‡´ä»¥åŠç”±æ¨¡å‹ä¸ç¡®å®šæ€§å¯¼è‡´çš„å¹»è§‰ç­‰æŒ‘æˆ˜ï¼Œæå‡ºäº†ä¸€ç§åˆ›æ–°çš„ abstraction-concretization frameworkã€‚è¯¥æ¡†æ¶æ—¨åœ¨é€šè¿‡æ•´åˆ LLMs çš„å¤šä¸ªå€™é€‰è¾“å‡ºæ¥æå‡ç”Ÿæˆæ¨¡å‹çš„è´¨é‡ä¸ä¸€è‡´æ€§ï¼Œæœ‰æ•ˆå¼¥è¡¥äº†ç°æœ‰è¿‡æ»¤æŠ€æœ¯åœ¨å¤„ç†é¢†åŸŸç‰¹å®šçº¦æŸæ–¹é¢çš„ä¸è¶³ã€‚å…·ä½“è€Œè¨€ï¼Œè¯¥æ–¹æ³•é¦–å…ˆæ„å»ºä¸€ä¸ªèšåˆæ‰€æœ‰å€™é€‰è¾“å‡ºä¿¡æ¯çš„ probabilistic partial modelï¼Œéšåé€šè¿‡æç‚¼ç®—æ³•å°†å…¶è½¬åŒ–ä¸ºæ»¡è¶³æ‰€æœ‰é¢„è®¾çº¦æŸçš„æœ€ä¼˜ concrete modelã€‚åœ¨å¤šç§ä¸»æµå¼€æºåŠé—­æº LLMs å’Œå¤šæ ·åŒ–æ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ¡†æ¶æ˜¾è‘—å¢å¼ºäº†ç”Ÿæˆçš„å›¾æ¨¡å‹åœ¨è½¯ä»¶å·¥ç¨‹åº”ç”¨ä¸­çš„å‡†ç¡®æ€§ä¸é€»è¾‘ä¸€è‡´æ€§ã€‚",
      "categories": [
        "cs.SE",
        "cs.AI"
      ],
      "primary_category": "cs.SE",
      "comment": "Accepted at ACM / IEEE 28th International Conference on Model Driven Engineering Languages and Systems (MODELS 2025)",
      "pdf_url": "https://arxiv.org/pdf/2508.00255v1",
      "published_date": "2025-08-01 01:52:25 UTC",
      "updated_date": "2025-08-01 01:52:25 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T08:14:34.444861+00:00"
    },
    {
      "arxiv_id": "2508.00250v1",
      "title": "Jet Image Generation in High Energy Physics Using Diffusion Models",
      "title_zh": "åŸºäºæ‰©æ•£æ¨¡å‹çš„é«˜èƒ½ç‰©ç†å–·æ³¨å›¾åƒç”Ÿæˆ",
      "authors": [
        "Victor D. Martinez",
        "Vidya Manian",
        "Sudhir Malik"
      ],
      "abstract": "This article presents, for the first time, the application of diffusion models for generating jet images corresponding to proton-proton collision events at the Large Hadron Collider (LHC). The kinematic variables of quark, gluon, W-boson, Z-boson, and top quark jets from the JetNet simulation dataset are mapped to two-dimensional image representations. Diffusion models are trained on these images to learn the spatial distribution of jet constituents. We compare the performance of score-based diffusion models and consistency models in accurately generating class-conditional jet images. Unlike approaches based on latent distributions, our method operates directly in image space. The fidelity of the generated images is evaluated using several metrics, including the FrÃ©chet Inception Distance (FID), which demonstrates that consistency models achieve higher fidelity and generation stability compared to score-based diffusion models. These advancements offer significant improvements in computational efficiency and generation accuracy, providing valuable tools for High Energy Physics (HEP) research.",
      "tldr_zh": "è¯¥ç ”ç©¶é¦–æ¬¡å°† diffusion models åº”ç”¨äºå¤§å‹å¼ºå­å¯¹æ’æœºï¼ˆLHCï¼‰è´¨å­-è´¨å­ç¢°æ’äº‹ä»¶ä¸­çš„ jet images ç”Ÿæˆã€‚ç ”ç©¶å›¢é˜Ÿåˆ©ç”¨ JetNet æ¨¡æ‹Ÿæ•°æ®é›†ï¼Œå°†å¤¸å…‹ã€èƒ¶å­ã€W/Z ç»è‰²å­åŠé¡¶å¤¸å…‹çš„è¿åŠ¨å­¦å˜é‡æ˜ å°„ä¸ºäºŒç»´å›¾åƒè¡¨å¾ã€‚å®éªŒä¸­è®­ç»ƒäº† diffusion models ä»¥å­¦ä¹ å–·æ³¨ç»„åˆ†çš„ç©ºé—´åˆ†å¸ƒï¼Œå¹¶å¯¹æ¯”äº† score-based diffusion models ä¸ consistency models åœ¨ç”Ÿæˆç±»åˆ«æ¡ä»¶å›¾åƒæ—¶çš„æ€§èƒ½è¡¨ç°ã€‚ä¸åŸºäº latent distributions çš„æ–¹æ³•ä¸åŒï¼Œè¯¥æ–¹æ³•ç›´æ¥åœ¨å›¾åƒç©ºé—´è¿›è¡Œæ“ä½œï¼Œå¹¶é€šè¿‡ FrÃ©chet Inception Distance (FID) ç­‰å¤šé¡¹æŒ‡æ ‡è¯„ä¼°ç”Ÿæˆå›¾åƒçš„ä¿çœŸåº¦ã€‚ç»“æœè¡¨æ˜ï¼Œconsistency models åœ¨ç”Ÿæˆä¿çœŸåº¦å’Œç¨³å®šæ€§æ–¹é¢ä¼˜äº score-based diffusion modelsã€‚è¿™ä¸€è¿›å±•åœ¨è®¡ç®—æ•ˆç‡å’Œç”Ÿæˆç²¾åº¦ä¸Šå®ç°äº†æ˜¾è‘—æå‡ï¼Œä¸ºé«˜èƒ½ç‰©ç† (High Energy Physics) ç ”ç©¶æä¾›äº†å¼ºæœ‰åŠ›çš„å·¥å…·ã€‚",
      "categories": [
        "hep-ph",
        "cs.AI",
        "cs.CV",
        "cs.LG"
      ],
      "primary_category": "hep-ph",
      "comment": "The paper is under review at IEEE Transactions in Nuclear Science",
      "pdf_url": "https://arxiv.org/pdf/2508.00250v1",
      "published_date": "2025-08-01 01:41:27 UTC",
      "updated_date": "2025-08-01 01:41:27 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T08:14:31.541473+00:00"
    },
    {
      "arxiv_id": "2508.00239v1",
      "title": "What's Behind the Magic? Audiences Seek Artistic Value in Generative AI's Contributions to a Live Dance Performance",
      "title_zh": "â€œé­”åŠ›â€èƒŒåï¼šè§‚ä¼—å¯¹ç”Ÿæˆå¼äººå·¥æ™ºèƒ½åœ¨ç°åœºèˆè¹ˆè¡¨æ¼”ä¸­è‰ºæœ¯ä»·å€¼çš„æ¢å¯»",
      "authors": [
        "Jacqueline Elise Bruen",
        "Myounghoon Jeon"
      ],
      "abstract": "With the development of generative artificial intelligence (GenAI) tools to create art, stakeholders cannot come to an agreement on the value of these works. In this study we uncovered the mixed opinions surrounding art made by AI. We developed two versions of a dance performance augmented by technology either with or without GenAI. For each version we informed audiences of the performance's development either before or after a survey on their perceptions of the performance. There were thirty-nine participants (13 males, 26 female) divided between the four performances. Results demonstrated that individuals were more inclined to attribute artistic merit to works made by GenAI when they were unaware of its use. We present this case study as a call to address the importance of utilizing the social context and the users' interpretations of GenAI in shaping a technical explanation, leading to a greater discussion that can bridge gaps in understanding.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†è§‚ä¼—å¯¹ç”Ÿæˆå¼äººå·¥æ™ºèƒ½ (Generative AI) åœ¨ç°åœºèˆè¹ˆè¡¨æ¼”ä¸­è‰ºæœ¯ä»·å€¼çš„è®¤çŸ¥å·®å¼‚ã€‚ç ”ç©¶å›¢é˜Ÿå¼€å‘äº†åŒ…å« GenAI å¢å¼ºå’Œé GenAI å¢å¼ºçš„ä¸¤ä¸ªèˆè¹ˆè¡¨æ¼”ç‰ˆæœ¬ï¼Œå¹¶åœ¨ä¸åŒæ—¶é—´ç‚¹å‘è§‚ä¼—æŠ«éœ²æŠ€æœ¯çš„ä½¿ç”¨æƒ…å†µï¼Œä»¥è§‚å¯Ÿå…¶æ„ŸçŸ¥çš„å˜åŒ–ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œå½“è§‚ä¼—åœ¨ä¸çŸ¥æƒ…çš„æƒ…å†µä¸‹æ¬£èµè¡¨æ¼”æ—¶ï¼Œå¾€å¾€æ›´å€¾å‘äºèµ‹äºˆ GenAI è¾…åŠ©åˆ›ä½œçš„ä½œå“è¾ƒé«˜çš„è‰ºæœ¯ä»·å€¼ (Artistic Merit)ã€‚è¿™ä¸€å‘ç°æ­ç¤ºäº†ç¤¾ä¼šè¯­å¢ƒ (Social Context) å’Œç”¨æˆ·çš„å¿ƒç†è§£è¯»åœ¨è¯„ä»·æŠ€æœ¯åˆ›ä½œæˆæœæ—¶çš„å…³é”®ä½œç”¨ã€‚é€šè¿‡è¿™ä¸€æ¡ˆä¾‹ï¼Œç ”ç©¶å¼ºè°ƒäº†åœ¨è§£é‡ŠæŠ€æœ¯åº”ç”¨æ—¶çº³å…¥äººç±»æ„ŸçŸ¥ç»´åº¦çš„é‡è¦æ€§ï¼Œæ—¨åœ¨ä¸ºå¼¥åˆäººå·¥æ™ºèƒ½è‰ºæœ¯ç†è§£ä¸Šçš„åˆ†æ­§æä¾›æ–°çš„è®¨è®ºæ¡†æ¶ã€‚",
      "categories": [
        "cs.HC",
        "cs.AI"
      ],
      "primary_category": "cs.HC",
      "comment": "In Proceedings of Explainable AI for the Arts Workshop 2025 (XAIxArts 2025) arXiv:2406.14485",
      "pdf_url": "https://arxiv.org/pdf/2508.00239v1",
      "published_date": "2025-08-01 00:51:17 UTC",
      "updated_date": "2025-08-01 00:51:17 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T08:14:36.140022+00:00"
    },
    {
      "arxiv_id": "2508.00238v1",
      "title": "Model Misalignment and Language Change: Traces of AI-Associated Language in Unscripted Spoken English",
      "title_zh": "æ¨¡å‹å¤±é…ä¸è¯­è¨€æ¼”å˜ï¼šè‡ªå‘è‹±è¯­å£è¯­ä¸­ AI å…³è”è¯­è¨€çš„ç—•è¿¹",
      "authors": [
        "Bryce Anderson",
        "Riley Galpin",
        "Tom S. Juzek"
      ],
      "abstract": "In recent years, written language, particularly in science and education, has undergone remarkable shifts in word usage. These changes are widely attributed to the growing influence of Large Language Models (LLMs), which frequently rely on a distinct lexical style. Divergences between model output and target audience norms can be viewed as a form of misalignment. While these shifts are often linked to using Artificial Intelligence (AI) directly as a tool to generate text, it remains unclear whether the changes reflect broader changes in the human language system itself. To explore this question, we constructed a dataset of 22.1 million words from unscripted spoken language drawn from conversational science and technology podcasts. We analyzed lexical trends before and after ChatGPT's release in 2022, focusing on commonly LLM-associated words. Our results show a moderate yet significant increase in the usage of these words post-2022, suggesting a convergence between human word choices and LLM-associated patterns. In contrast, baseline synonym words exhibit no significant directional shift. Given the short time frame and the number of words affected, this may indicate the onset of a remarkable shift in language use. Whether this represents natural language change or a novel shift driven by AI exposure remains an open question. Similarly, although the shifts may stem from broader adoption patterns, it may also be that upstream training misalignments ultimately contribute to changes in human language use. These findings parallel ethical concerns that misaligned models may shape social and moral beliefs.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†å¤§å‹è¯­è¨€æ¨¡å‹(Large Language Models)çš„ç‹¬ç‰¹è¯æ±‡é£æ ¼æ˜¯å¦æ­£åœ¨å½±å“äººç±»è‡ªèº«çš„è¯­è¨€ç³»ç»Ÿï¼Œç‰¹åˆ«æ˜¯åœ¨éè„šæœ¬åŒ–çš„å£è¯­äº¤æµä¸­ã€‚ç ”ç©¶äººå‘˜æ„å»ºäº†ä¸€ä¸ªåŒ…å«2210ä¸‡è¯çš„ç§‘å­¦æŠ€æœ¯æ’­å®¢å£è¯­æ•°æ®é›†ï¼Œå¯¹æ¯”åˆ†æäº†2022å¹´ChatGPTå‘å¸ƒå‰åçš„è¯æ±‡ä½¿ç”¨è¶‹åŠ¿ã€‚åˆ†æé‡ç‚¹èšç„¦äºå¸¸è§çš„ä¸LLMç›¸å…³çš„è¯æ±‡(LLM-associated words)ï¼Œå¹¶ä»¥åŒä¹‰è¯ä½œä¸ºåŸºå‡†è¿›è¡Œå¯¹ç…§ã€‚ç»“æœæ˜¾ç¤ºï¼Œ2022å¹´åè¿™äº›ç‰¹å®šè¯æ±‡çš„ä½¿ç”¨é‡å‡ºç°äº†ä¸­ç­‰ä½†æ˜¾è‘—çš„å¢é•¿ï¼Œè€ŒåŸºå‡†åŒä¹‰è¯åˆ™æ²¡æœ‰è¡¨ç°å‡ºæ˜æ˜¾çš„ä½ç§»ã€‚è¿™ä¸€å‘ç°æš—ç¤ºäººç±»çš„è¯æ±‡é€‰æ‹©æ­£é€æ¸ä¸LLMæ¨¡å¼è¶‹åŒï¼Œå¯èƒ½æ ‡å¿—ç€AIæš´éœ²æ­£åœ¨é©±åŠ¨ä¸€ç§è¿…é€Ÿçš„è¯­è¨€å˜é©ã€‚ç ”ç©¶æœ€åæŒ‡å‡ºï¼Œä¸Šæ¸¸æ¨¡å‹è®­ç»ƒä¸­çš„å¯¹é½(Misalignment)é—®é¢˜æœ€ç»ˆå¯èƒ½é€šè¿‡è¿™ç§æ–¹å¼é‡å¡‘äººç±»çš„è¯­è¨€ä¹ æƒ¯ï¼Œç”šè‡³å½±å“ç¤¾ä¼šä¸é“å¾·è§‚å¿µã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "Accepted at AIES 2025. To appear in the AIES Proceedings. 14 pages, 2 figures, 2 tables. Licensed under CC BY-SA 4.0",
      "pdf_url": "https://arxiv.org/pdf/2508.00238v1",
      "published_date": "2025-08-01 00:47:33 UTC",
      "updated_date": "2025-08-01 00:47:33 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T08:14:42.247970+00:00"
    },
    {
      "arxiv_id": "2508.00235v1",
      "title": "Weakly Supervised Intracranial Aneurysm Detection and Segmentation in MR angiography via Multi-task UNet with Vesselness Prior",
      "title_zh": "åŸºäºè¡€ç®¡æ€§å…ˆéªŒå¤šä»»åŠ¡ UNet çš„ç£å…±æŒ¯è¡€ç®¡é€ å½±å¼±ç›‘ç£é¢…å†…åŠ¨è„‰ç˜¤æ£€æµ‹ä¸åˆ†å‰²",
      "authors": [
        "Erin Rainville",
        "Amirhossein Rasoulian",
        "Hassan Rivaz",
        "Yiming Xiao"
      ],
      "abstract": "Intracranial aneurysms (IAs) are abnormal dilations of cerebral blood vessels that, if ruptured, can lead to life-threatening consequences. However, their small size and soft contrast in radiological scans often make it difficult to perform accurate and efficient detection and morphological analyses, which are critical in the clinical care of the disorder. Furthermore, the lack of large public datasets with voxel-wise expert annotations pose challenges for developing deep learning algorithms to address the issues. Therefore, we proposed a novel weakly supervised 3D multi-task UNet that integrates vesselness priors to jointly perform aneurysm detection and segmentation in time-of-flight MR angiography (TOF-MRA). Specifically, to robustly guide IA detection and segmentation, we employ the popular Frangi's vesselness filter to derive soft cerebrovascular priors for both network input and an attention block to conduct segmentation from the decoder and detection from an auxiliary branch. We train our model on the Lausanne dataset with coarse ground truth segmentation, and evaluate it on the test set with refined labels from the same database. To further assess our model's generalizability, we also validate it externally on the ADAM dataset. Our results demonstrate the superior performance of the proposed technique over the SOTA techniques for aneurysm segmentation (Dice = 0.614, 95%HD =1.38mm) and detection (false positive rate = 1.47, sensitivity = 92.9%).",
      "tldr_zh": "é’ˆå¯¹é¢…å†…åŠ¨è„‰ç˜¤ï¼ˆIntracranial Aneurysmsï¼‰ä½“ç§¯å°ã€å¯¹æ¯”åº¦ä½ä»¥åŠç¼ºä¹å¤§è§„æ¨¡ä½“ç´ çº§ä¸“å®¶æ ‡æ³¨æ•°æ®å¯¼è‡´çš„æ£€æµ‹ä¸åˆ†å‰²éš¾é¢˜ï¼Œè¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§é›†æˆè¡€ç®¡ç‰¹æ€§å…ˆéªŒï¼ˆVesselness Priorï¼‰çš„æ–°å‹å¼±ç›‘ç£ï¼ˆWeakly Supervisedï¼‰3Då¤šä»»åŠ¡UNetæ¡†æ¶ã€‚è¯¥æ–¹æ³•åˆ©ç”¨Frangi's vesselness filteræå–è½¯è„‘è¡€ç®¡å…ˆéªŒä¿¡æ¯ï¼Œå°†å…¶æ•´åˆè¿›ç½‘ç»œè¾“å…¥å’Œæ³¨æ„åŠ›æ¨¡å—ï¼Œä»¥å¢å¼ºå¯¹ç—…å˜åŒºåŸŸçš„è¯†åˆ«èƒ½åŠ›ã€‚æ¨¡å‹é€šè¿‡å¤šä»»åŠ¡å­¦ä¹ æœºåˆ¶ï¼Œåˆ©ç”¨è§£ç å™¨è¿›è¡Œåˆ†å‰²å¹¶ç»“åˆè¾…åŠ©åˆ†æ”¯å®ç°åŠ¨è„‰ç˜¤æ£€æµ‹ï¼Œä»è€Œåœ¨TOF-MRAå›¾åƒä¸­è”åˆå®Œæˆä¸¤é¡¹ä¸´åºŠä»»åŠ¡ã€‚å®éªŒåœ¨Lausanneæ•°æ®é›†å’Œå¤–éƒ¨ADAMæ•°æ®é›†ä¸Šè¿›è¡Œäº†å……åˆ†éªŒè¯ï¼Œç»“æœè¡¨æ˜è¯¥æŠ€æœ¯åœ¨åˆ†å‰²æ€§èƒ½ï¼ˆDice = 0.614ï¼‰å’Œæ£€æµ‹çµæ•åº¦ï¼ˆ92.9%ï¼‰æ–¹é¢å‡æ˜¾è‘—ä¼˜äºç°æœ‰çš„SOTAæ–¹æ³•ï¼Œå±•ç°äº†å¼ºå¤§çš„é²æ£’æ€§å’Œæ³›åŒ–æ½œåŠ›ã€‚",
      "categories": [
        "eess.IV",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "eess.IV",
      "comment": "Accepted to ICCV 2025 Workshop CVAMD",
      "pdf_url": "https://arxiv.org/pdf/2508.00235v1",
      "published_date": "2025-08-01 00:45:46 UTC",
      "updated_date": "2025-08-01 00:45:46 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T08:14:47.851101+00:00"
    },
    {
      "arxiv_id": "2508.00234v1",
      "title": "Quality-of-Service Aware LLM Routing for Edge Computing with Multiple Experts",
      "title_zh": "é¢å‘å¤šä¸“å®¶è¾¹ç¼˜è®¡ç®—çš„æœåŠ¡è´¨é‡æ„ŸçŸ¥å¤§è¯­è¨€æ¨¡å‹è·¯ç”±",
      "authors": [
        "Jin Yang",
        "Qiong Wu",
        "Zhiying Feng",
        "Zhi Zhou",
        "Deke Guo",
        "Xu Chen"
      ],
      "abstract": "Large Language Models (LLMs) have demonstrated remarkable capabilities, leading to a significant increase in user demand for LLM services. However, cloud-based LLM services often suffer from high latency, unstable responsiveness, and privacy concerns. Therefore, multiple LLMs are usually deployed at the network edge to boost real-time responsiveness and protect data privacy, particularly for many emerging smart mobile and IoT applications. Given the varying response quality and latency of LLM services, a critical issue is how to route user requests from mobile and IoT devices to an appropriate LLM service (i.e., edge LLM expert) to ensure acceptable quality-of-service (QoS). Existing routing algorithms fail to simultaneously address the heterogeneity of LLM services, the interference among requests, and the dynamic workloads necessary for maintaining long-term stable QoS. To meet these challenges, in this paper we propose a novel deep reinforcement learning (DRL)-based QoS-aware LLM routing framework for sustained high-quality LLM services. Due to the dynamic nature of the global state, we propose a dynamic state abstraction technique to compactly represent global state features with a heterogeneous graph attention network (HAN). Additionally, we introduce an action impact estimator and a tailored reward function to guide the DRL agent in maximizing QoS and preventing latency violations. Extensive experiments on both Poisson and real-world workloads demonstrate that our proposed algorithm significantly improves average QoS and computing resource efficiency compared to existing baselines.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹äº‘ç«¯ Large Language Models (LLMs) æœåŠ¡å­˜åœ¨çš„å»¶è¿Ÿé«˜ã€å“åº”ä¸ç¨³å®šåŠéšç§æ³„éœ²é—®é¢˜ï¼Œæå‡ºäº†ä¸€ä¸ªé¢å‘è¾¹ç¼˜è®¡ç®—çš„å¤šä¸“å®¶ QoS-aware LLM è·¯ç”±æ¡†æ¶ã€‚ä¸ºäº†åº”å¯¹ LLM æœåŠ¡çš„å¼‚æ„æ€§ã€è¯·æ±‚é—´çš„å¹²æ‰°ä»¥åŠåŠ¨æ€å·¥ä½œè´Ÿè½½å¯¹ Quality-of-Service (QoS) çš„æŒ‘æˆ˜ï¼Œè¯¥æ¡†æ¶é‡‡ç”¨äº† Deep Reinforcement Learning (DRL) æŠ€æœ¯ï¼Œå¹¶åˆ©ç”¨ Heterogeneous Graph Attention Network (HAN) å®ç°åŠ¨æ€çŠ¶æ€æŠ½è±¡ã€‚æ­¤å¤–ï¼Œç ”ç©¶è¿˜å¼•å…¥äº†åŠ¨ä½œå½±å“ä¼°è®¡å™¨ (Action Impact Estimator) å’Œå®šåˆ¶çš„å¥–åŠ±å‡½æ•°ï¼Œä»¥å¼•å¯¼ DRL æ™ºèƒ½ä½“åœ¨æœ€å¤§åŒ– QoS çš„åŒæ—¶é˜²æ­¢å»¶è¿Ÿè¿è§„ã€‚åœ¨ Poisson å’ŒçœŸå®ä¸–ç•Œå·¥ä½œè´Ÿè½½ä¸‹çš„å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥ç®—æ³•ä¸ç°æœ‰åŸºçº¿æ¨¡å‹ç›¸æ¯”ï¼Œæ˜¾è‘—æå‡äº†å¹³å‡ QoS å’Œè®¡ç®—èµ„æºåˆ©ç”¨æ•ˆç‡ã€‚è¯¥æ¡†æ¶ä¸ºå®ç°è¾¹ç¼˜ç¯å¢ƒä¸‹é•¿æœŸç¨³å®šçš„é«˜è´¨é‡ LLM æœåŠ¡æä¾›äº†æœ‰æ•ˆçš„æŠ€æœ¯æ”¯æ’‘ã€‚",
      "categories": [
        "cs.NI",
        "cs.AI",
        "cs.DC",
        "cs.MA"
      ],
      "primary_category": "cs.NI",
      "comment": "Accepted by IEEE Transactions on Mobile Computing",
      "pdf_url": "https://arxiv.org/pdf/2508.00234v1",
      "published_date": "2025-08-01 00:45:15 UTC",
      "updated_date": "2025-08-01 00:45:15 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T08:14:50.644092+00:00"
    },
    {
      "arxiv_id": "2508.00952v1",
      "title": "Academic Vibe Coding: Opportunities for Accelerating Research in an Era of Resource Constraint",
      "title_zh": "å­¦æœ¯ Vibe Codingï¼šèµ„æºå—é™æ—¶ä»£åŠ é€Ÿç§‘ç ”çš„å¥‘æœº",
      "authors": [
        "Matthew G Crowson",
        "Leo Celi A. Celi"
      ],
      "abstract": "Academic laboratories face mounting resource constraints: budgets are tightening, grant overheads are potentially being capped, and the market rate for data-science talent significantly outstrips university compensation. Vibe coding, which is structured, prompt-driven code generation with large language models (LLMs) embedded in reproducible workflows, offers one pragmatic response. It aims to compress the idea-to-analysis timeline, reduce staffing pressure on specialized data roles, and maintain rigorous, version-controlled outputs. This article defines the vibe coding concept, situates it against the current academic resourcing crisis, details a beginner-friendly toolchain for its implementation, and analyzes inherent limitations that necessitate governance and mindful application.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº† Academic Vibe Coding åœ¨å­¦æœ¯èµ„æºæ—¥ç›Šç´§å¼ èƒŒæ™¯ä¸‹çš„åº”ç”¨æœºé‡ï¼Œæ—¨åœ¨åº”å¯¹å®éªŒå®¤é¢„ç®—ç¼©å‡ã€æ‹¨æ¬¾ç®¡ç†è´¹å—é™ä»¥åŠæ•°æ®ç§‘å­¦äººæ‰å¸‚åœºé«˜è–ªåŒ–å¸¦æ¥çš„æŒ‘æˆ˜ã€‚Vibe coding è¢«å®šä¹‰ä¸ºä¸€ç§åµŒå…¥åœ¨å¯å¤ç°å·¥ä½œæµä¸­çš„ã€ç”±å¤§è¯­è¨€æ¨¡å‹(LLMs)é©±åŠ¨çš„ç»“æ„åŒ–æç¤ºä»£ç ç”Ÿæˆæ¨¡å¼ã€‚é€šè¿‡è¿™ç§æ–¹å¼ï¼Œç ”ç©¶äººå‘˜å¯ä»¥æœ‰æ•ˆå‹ç¼©ä»ç ”ç©¶æƒ³æ³•åˆ°æ•°æ®åˆ†æçš„æ—¶é—´è·¨åº¦ï¼Œç¼“è§£å¯¹ä¸“ä¸šæŠ€æœ¯äººå‘˜çš„éœ€æ±‚å‹åŠ›ï¼Œå¹¶ç»´æŒä¸¥è°¨ä¸”å…·å¤‡ç‰ˆæœ¬æ§åˆ¶çš„è¾“å‡ºç»“æœã€‚æœ¬æ–‡ä¸ä»…è¯¦è¿°äº† vibe coding çš„æ ¸å¿ƒå®šä¹‰åŠå…¶åœ¨å­¦æœ¯å±æœºä¸­çš„å®šä½ï¼Œè¿˜ä¸ºåˆå­¦è€…æä¾›äº†ä¸€å¥—å¯æ“ä½œçš„å®ç°å·¥å…·é“¾(toolchain)ã€‚æœ€åï¼Œç ”ç©¶æ·±å…¥åˆ†æäº†è¯¥æ–¹æ³•çš„å†…åœ¨å±€é™æ€§ï¼Œå¼ºè°ƒåœ¨ç§‘ç ”å®è·µä¸­å¿…é¡»å»ºç«‹ç›¸åº”çš„æ²»ç†æœºåˆ¶å¹¶ä¿æŒè°¨æ…åº”ç”¨ã€‚",
      "categories": [
        "cs.CY",
        "cs.AI",
        "cs.PL",
        "cs.SE"
      ],
      "primary_category": "cs.CY",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.00952v1",
      "published_date": "2025-08-01 00:42:44 UTC",
      "updated_date": "2025-08-01 00:42:44 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T08:14:49.044592+00:00"
    },
    {
      "arxiv_id": "2508.02718v1",
      "title": "SleepLiteCNN: Energy-Efficient Sleep Apnea Subtype Classification with 1-Second Resolution Using Single-Lead ECG",
      "title_zh": "SleepLiteCNNï¼šåŸºäºå•å¯¼è”å¿ƒç”µå›¾å®ç° 1 ç§’åˆ†è¾¨ç‡çš„é«˜èƒ½æ•ˆç¡çœ å‘¼å¸æš‚åœäºšå‹åˆ†ç±»",
      "authors": [
        "Zahra Mohammadi",
        "Siamak Mohammadi"
      ],
      "abstract": "Apnea is a common sleep disorder characterized by breathing interruptions lasting at least ten seconds and occurring more than five times per hour. Accurate, high-temporal-resolution detection of sleep apnea subtypes - Obstructive, Central, and Mixed - is crucial for effective treatment and management. This paper presents an energy-efficient method for classifying these subtypes using a single-lead electrocardiogram (ECG) with high temporal resolution to address the real-time needs of wearable devices. We evaluate a wide range of classical machine learning algorithms and deep learning architectures on 1-second ECG windows, comparing their accuracy, complexity, and energy consumption. Based on this analysis, we introduce SleepLiteCNN, a compact and energy-efficient convolutional neural network specifically designed for wearable platforms. SleepLiteCNN achieves over 95% accuracy and a 92% macro-F1 score, while requiring just 1.8 microjoules per inference after 8-bit quantization. Field Programmable Gate Array (FPGA) synthesis further demonstrates significant reductions in hardware resource usage, confirming its suitability for continuous, real-time monitoring in energy-constrained environments. These results establish SleepLiteCNN as a practical and effective solution for wearable device sleep apnea subtype detection.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ç¡çœ å‘¼å¸æš‚åœï¼ˆSleep Apneaï¼‰äºšå‹ï¼ˆé˜»å¡æ€§ã€ä¸­æ¢æ€§å’Œæ··åˆæ€§ï¼‰çš„é«˜æ•ˆæ£€æµ‹éœ€æ±‚ï¼Œæå‡ºäº†ä¸€ç§é€‚ç”¨äºå¯ç©¿æˆ´è®¾å¤‡çš„è¶…ä½åŠŸè€—åˆ†ç±»æ–¹æ³•ã€‚è¯¥æ–¹æ³•åŸºäºå•å¯¼è”å¿ƒç”µå›¾ï¼ˆSingle-Lead ECGï¼‰ï¼Œå®ç°äº†1ç§’çš„é«˜æ—¶é—´åˆ†è¾¨ç‡ç›‘æµ‹ï¼Œä»¥æ»¡è¶³ä¸´åºŠå’Œå®æ—¶æ€§è¦æ±‚ã€‚ç ”ç©¶å›¢é˜Ÿé€šè¿‡å¯¹å¤šç§æœºå™¨å­¦ä¹ ä¸æ·±åº¦å­¦ä¹ æ¶æ„çš„å¤æ‚åº¦å’Œèƒ½è€—è¿›è¡Œæ·±åº¦è¯„ä¼°ï¼Œæ¨å‡ºäº†ä¸“ä¸ºä¾¿æºå¼å¹³å°è®¾è®¡çš„ç´§å‡‘å‹å·ç§¯ç¥ç»ç½‘ç»œ SleepLiteCNNã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒSleepLiteCNN åœ¨å‡†ç¡®ç‡ä¸Šè¶…è¿‡ 95%ï¼Œmacro-F1 score è¾¾åˆ° 92%ï¼Œä¸”åœ¨ç»è¿‡ 8-bit quantization åï¼Œå•æ¬¡æ¨ç†èƒ½è€—ä»…ä¸º 1.8 å¾®ç„¦è€³ã€‚ç°åœºå¯ç¼–ç¨‹é€»è¾‘é—¨é˜µåˆ—ï¼ˆFPGAï¼‰çš„åˆæˆéªŒè¯è¿›ä¸€æ­¥è¯å®äº†è¯¥æ¨¡å‹åœ¨æ˜¾è‘—èŠ‚çœç¡¬ä»¶èµ„æºçš„åŒæ—¶ï¼Œå…·å¤‡åœ¨èƒ½æºå—é™ç¯å¢ƒä¸‹è¿›è¡ŒæŒç»­å®æ—¶ç›‘æµ‹çš„èƒ½åŠ›ã€‚è¿™ä¸€æˆæœä¸ºå¯ç©¿æˆ´è®¾å¤‡å®ç°ç²¾ç¡®ã€ä½èƒ½è€—çš„ç¡çœ å‘¼å¸æš‚åœç›‘æµ‹æä¾›äº†åˆ‡å®å¯è¡Œçš„è§£å†³æ–¹æ¡ˆã€‚",
      "categories": [
        "eess.SP",
        "cs.AI"
      ],
      "primary_category": "eess.SP",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.02718v1",
      "published_date": "2025-08-01 00:04:40 UTC",
      "updated_date": "2025-08-01 00:04:40 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T08:15:02.443681+00:00"
    }
  ],
  "processing_status": "completed",
  "error": null,
  "raw_papers_fetched": true,
  "papers_count": 141,
  "processed_papers_count": 141,
  "failed_papers_count": 0,
  "llm_backup_calls": 2,
  "summary_generated": true,
  "daily_data_saved": true,
  "last_update": "2026-01-24T08:17:39.908225+00:00"
}
{
  "date": "2025-10-25",
  "category": "cs.AI",
  "summary": "æ¬¢è¿æ¥åˆ° UTC æ—¶é—´ 2025-10-25 çš„ arXiv ä¸­æ–‡ TLDR å¿«æŠ¥ï¼\n\n**ä»Šæ—¥æ€»ç»“**ï¼š\nä»Šå¤©çš„ arXiv ä¹Ÿæ˜¯ç¥ä»™æ‰“æ¶çš„ä¸€å¤©ã€‚æœ€é‡ç£…çš„æ¶ˆæ¯è«è¿‡äº **Ling 2.0** ç³»åˆ—æ¨¡å‹çš„å‘å¸ƒï¼Œå‚æ•°é‡é£™å‡è‡³ 1Tï¼Œæ¢ç´¢äº†ç¨€ç– MoE åœ¨æ¨ç†ä¸Šçš„æè‡´ã€‚ä¸æ­¤åŒæ—¶ï¼Œå…³äº **Prompt Engineering (æç¤ºå·¥ç¨‹)** çš„åæ€è¾¾åˆ°äº†æ–°é«˜åº¦ï¼Œæœ‰ç ”ç©¶æŒ‡å‡ºåœ¨ GPT-5 çº§åˆ«çš„æ¨¡å‹ä¸Šï¼Œå¤æ‚çš„ Prompt åè€Œä¼šé™ä½æ€§èƒ½ã€‚æ­¤å¤–ï¼Œå…³äº **Layer Pruning (å±‚å‰ªæ)** æ˜¯å¦ä¼šæŸå®³æ¨ç†èƒ½åŠ›ã€**RAG** åœ¨å¤šè·³æ¨ç†ä¸­çš„çœŸå®è¡¨ç°ï¼Œä»¥åŠ **VLM (è§†è§‰è¯­è¨€æ¨¡å‹)** çš„ç©ºé—´æ¨ç†èƒ½åŠ›ï¼Œéƒ½æœ‰éå¸¸ç¡¬æ ¸çš„è®¨è®ºã€‚\n\n---\n\n### ğŸš€ é‡ç£…æ¨¡å‹ä¸æ¨ç†èƒ½åŠ›çš„æœ¬è´¨ (LLM Foundation & Reasoning)\n\n**1. [Ling 2.0] æ¯ä¸€ä»½æ¿€æ´»éƒ½åœ¨åŠ©æ¨ï¼šå°†é€šç”¨æ¨ç†è€…æ‰©å±•è‡³ 1 ä¸‡äº¿å‚æ•°çš„å¼€æ”¾è¯­è¨€åŸºåº§**\n**Every Activation Boosted: Scaling General Reasoner to 1 Trillion Open Language Foundation**\n> **Authors:** Ling Team et al.\n> **TLDR:** è¿™æ˜¯ä¸€ä¸ªå·¨å¤§çš„æŠ€æœ¯æŠ¥å‘Šã€‚Ling å›¢é˜Ÿå‘å¸ƒäº† **Ling 2.0** ç³»åˆ—ï¼ŒåŒ…å«ä» 16B åˆ° **1T (ä¸€ä¸‡äº¿)** å‚æ•°çš„æ¨¡å‹ã€‚æ ¸å¿ƒé‡‡ç”¨ç»Ÿä¸€çš„ **Mixture-of-Experts (MoE)** æ¶æ„ï¼Œä¸»æ‰“é«˜ç¨€ç–æ€§å’Œè·¨å°ºåº¦ä¸€è‡´æ€§ã€‚\n> **Key Insight:** ä»–ä»¬æå‡ºäº† \"Mid-training CoT activation\" å’ŒåŸºäºå¼ºåŒ–çš„å¾®è°ƒ (DFT, Evo-CoT)ã€‚æœ€æ ¸å¿ƒçš„å‘ç°æ˜¯ï¼šåªè¦ç¨€ç–æ¿€æ´»ä¸æ¨ç†ç›®æ ‡å¯¹é½ï¼Œå°±èƒ½åœ¨ä¸‡äº¿è§„æ¨¡ä¸Šå®ç°é«˜æ•ˆæ™ºèƒ½ã€‚Ling-1T å»ºç«‹äº†ä¸€ä¸ªæ–°çš„æ¨ç†å‡†ç¡®ç‡ä¸è®¡ç®—æ•ˆç‡çš„ Pareto å‰æ²¿ã€‚\n\n**2. [Prompting Inversion] ä½ ä¸å†éœ€è¦æç¤ºå·¥ç¨‹äº†ï¼šæç¤ºåè½¬ç°è±¡**\n**You Don't Need Prompt Engineering Anymore: The Prompting Inversion**\n> **Authors:** Imran Khan\n> **TLDR:** è¿™ç¯‡æ–‡ç« æå…·é¢ è¦†æ€§ã€‚ä½œè€…åœ¨ GSM8K ä¸Šæµ‹è¯•äº†ä¸‰ä»£ OpenAI æ¨¡å‹ (gpt-4o-mini, gpt-4o, gpt-5)ã€‚\n> **Key Insight:** å‘ç°äº†ä¸€ä¸ª **\"Prompting Inversion\" (æç¤ºåè½¬)** ç°è±¡ï¼šç²¾å¿ƒè®¾è®¡çš„çº¦æŸæ€§ Prompt (\"Sculpting\") åœ¨ gpt-4o ä¸Šæœ‰æ•ˆï¼Œä½†åœ¨ **gpt-5** ä¸Šåè€Œè¡¨ç°ä¸å¦‚æ ‡å‡† CoTï¼Œç”šè‡³æœ‰å®³ã€‚ä½œè€…è®¤ä¸ºè¿™æ˜¯å› ä¸ºé˜²æ­¢ä¸­ç­‰æ¨¡å‹çŠ¯é”™çš„ \"æŠ¤æ \"ï¼Œå¯¹äºé«˜çº§æ¨¡å‹å˜æˆäº†æŸç¼šå…¶è¿‡åº¦å­—é¢ç†è§£çš„ \"æ‰‹é“\"ã€‚ç»“è®ºï¼šæ¨¡å‹è¶Šå¼ºï¼ŒPrompt åº”è¯¥è¶Šç®€å•ã€‚\n\n**3. [Large Reasoning Models] æ¨ç†æ¨¡å‹æ¨ç†å¾—å¾ˆå¥½ï¼Œç›´åˆ°å®ƒä»¬ä¸è¡Œäº†**\n**Reasoning Models Reason Well, Until They Don't**\n> **Authors:** Revanth Rameshkumar et al.\n> **TLDR:** é’ˆå¯¹æœ€è¿‘ç«çƒ­çš„ \"Large Reasoning Models\" (LRMs) æ³¼äº†ä¸€ç›†å†·æ°´ã€‚\n> **Key Insight:** è™½ç„¶ LRMs åœ¨ç°æœ‰åŸºå‡†ä¸Šè¡¨ç°å‡ºè‰²ï¼Œä½†ä½œè€…æ„å»ºäº† **DeepRD** æ•°æ®é›†å‘ç°ï¼šä¸€æ—¦æ¨ç†é—®é¢˜çš„**å¤æ‚æ€§ (Complexity)** è¶…è¿‡ä¸€å®šé˜ˆå€¼ï¼ŒLRM çš„æ€§èƒ½ä¼šæ–­å´–å¼ä¸‹è·Œä¸”æ— æ³•æ³›åŒ–ã€‚ç°æœ‰çš„æˆåŠŸå¾ˆå¤§ç¨‹åº¦ä¸Šæ˜¯å› ä¸ºçœŸå®ä¸–ç•Œçš„å›¾è°±å’Œè¯æ˜å¤§å¤šè½åœ¨ä½å¤æ‚åº¦åŒºé—´ï¼Œä½†åœ¨é•¿å°¾çš„é«˜å¤æ‚åº¦é—®é¢˜ä¸Šï¼ŒLRMs ä¾ç„¶è„†å¼±ã€‚\n\n**4. [Hierarchical Thinking] åœ¨å¤§å‹æ¨ç†æ¨¡å‹ä¸­å»ºæ¨¡å±‚çº§æ€ç»´**\n**Modeling Hierarchical Thinking in Large Reasoning Models**\n> **Authors:** G M Shahariar et al.\n> **TLDR:** è¯•å›¾æ‰“å¼€ CoT çš„é»‘ç›’ã€‚\n> **Key Insight:** ä½œè€…ç”¨æ— è®°å¿†çš„**æœ‰é™çŠ¶æ€æœº (FSM)** æ¥è¿‘ä¼¼ LRM çš„å±‚çº§æ¨ç†åŠ¨æ€ã€‚ä»–ä»¬å®šä¹‰äº†ä¸€ç»„ç¦»æ•£çŠ¶æ€ï¼ˆå¦‚åˆå§‹åŒ–ã€æ¼”ç»ã€å›æº¯ã€ä¸ç¡®å®šæ€§ä¼°è®¡ç­‰ï¼‰ã€‚é€šè¿‡è¿™ç§ FSM è§†è§’ï¼Œå¯ä»¥æ¸…æ™°åœ°çœ‹åˆ°ä¸åŒæ¨¡å‹åœ¨æ¨ç†è½¨è¿¹ä¸Šçš„çŠ¶æ€è½¬ç§»æ¨¡å¼ï¼Œä¸ºè§£é‡Š LLM çš„æ€ç»´è¿‡ç¨‹æä¾›äº†ä¸€ä¸ªç»“æ„åŒ–æ¡†æ¶ã€‚\n\n---\n\n### âœ‚ï¸ æ¨¡å‹ä¼˜åŒ–ï¼šå‰ªæä¸é•¿ä¸Šä¸‹æ–‡ (Pruning & Long Context)\n\n**5. [Layer Pruning] å½“æ›´å°‘çš„å±‚æ‰“æ–­æ›´å¤šçš„é“¾ï¼šå±‚å‰ªææŸå®³ LLM çš„æµ‹è¯•æ—¶æ‰©å±•**\n**When Fewer Layers Break More Chains: Layer Pruning Harms Test-Time Scaling in LLMs**\n> **Authors:** Keyu Wang et al.\n> **TLDR:** ç»™æœ€è¿‘æµè¡Œçš„â€œç›´æ¥å‰ªæ‰å‡ å±‚â€çš„åšæ³•æ•²å“è­¦é’Ÿã€‚\n> **Key Insight:** è™½ç„¶å±‚å‰ªæåœ¨é€šç”¨çŸ¥è¯†ä»»åŠ¡ä¸ŠæŸå¤±ä¸å¤§ï¼Œä½†ä½œè€…å‘ç°å®ƒä¸¥é‡æŸå®³äº† **Test-time Scaling (æµ‹è¯•æ—¶æ‰©å±•)** èƒ½åŠ›ã€‚å“ªæ€•åªå‰ªæ‰ä¸€ä¸¤å±‚ï¼Œé•¿é“¾æ¨ç†èƒ½åŠ›å°±ä¼šå´©æºƒï¼Œä¸”åç»­çš„ SFT æ— æ³•æ¢å¤è¿™ç§èƒ½åŠ›ã€‚è¿™è¡¨æ˜æ·±åº¦å¯¹äºå¤æ‚çš„æ¨ç†è‡³å…³é‡è¦ã€‚\n\n**6. [Continuous Layer Pruning] ç»“æ„æ‰‹æœ¯åˆ€ï¼šå¤§å‹è¯­è¨€æ¨¡å‹çš„è‡ªåŠ¨è¿ç»­å±‚å‰ªæ**\n**The Structural Scalpel: Automated Contiguous Layer Pruning for Large Language Models**\n> **Authors:** Yao Lu et al.\n> **TLDR:** ä¸ä¸Šä¸€ç¯‡å½¢æˆé²œæ˜å¯¹æ¯”ï¼Œè¿™ç¯‡æå‡ºäº†ä¸€ç§æ›´èªæ˜çš„å‰ªææ–¹æ³• **CLP**ã€‚\n> **Key Insight:** ä¸åŒäºæ‰‹åŠ¨è¯„ä¼°ï¼ŒCLP ä½¿ç”¨å¯å¾®çš„å‡¹é—¨æ§ç®—æ³•è‡ªåŠ¨è¯†åˆ«æœ€ä½³çš„**è¿ç»­å±‚ç‰‡æ®µ**è¿›è¡Œå‰ªæï¼Œå¹¶åªå¾®è°ƒåˆ‡å£é™„è¿‘çš„å±‚ï¼ˆCutoff Endpoint Tuningï¼‰ã€‚åœ¨ LLaMA3-70B ä¸Šå‰ªæ‰ 20% ä¾ç„¶ä¿ç•™äº† 95% çš„æ€§èƒ½ã€‚\n\n**7. [Long Context] æ¸è¿›æ€§é—å¿˜ï¼šç”¨äºæ‰©å±• Transformer ä¸Šä¸‹æ–‡çª—å£çš„å¯¹æ•°å‹ç¼©**\n**Gradual Forgetting: Logarithmic Compression for Extending Transformer Context Windows**\n> **Authors:** Billy Dickson, Zoran Tiganj\n> **TLDR:** å—åˆ°äººç±»è®°å¿†æ¨¡å‹çš„å¯å‘ï¼Œä¸éœ€è¦æ”¹æ¨¡å‹æ¶æ„ã€‚\n> **Key Insight:** ä½œè€…ä¸å¯¹ Transformer åŠ¨åˆ€ï¼Œè€Œæ˜¯å¯¹**è¾“å…¥ token è¿›è¡Œå¯¹æ•°å‹ç¼© (Logarithmic Compression)**ã€‚è¿™ç§â€œæ¸è¿›å¼é—å¿˜â€çš„è¾“å…¥è¡¨å¾ç›´æ¥å–‚ç»™æ ‡å‡† Transformerï¼Œå°±èƒ½æœ‰æ•ˆå¤„ç†æ›´é•¿çš„æ—¶é—´ä¸Šä¸‹æ–‡ï¼Œä¸”ä¸éœ€è¦å¤æ‚çš„æ˜¾å¼è®°å¿†æ¨¡å—ã€‚\n\n---\n\n### ğŸ§© RAG ä¸ Agent (Retrieval & Agents)\n\n**8. [FAIR-RAG] æ£€ç´¢å¢å¼ºç”Ÿæˆçš„å¿ å®è‡ªé€‚åº”è¿­ä»£ä¼˜åŒ–**\n**FAIR-RAG: Faithful Adaptive Iterative Refinement for Retrieval-Augmented Generation**\n> **Authors:** Mohammad Aghajani Asl et al.\n> **TLDR:** è§£å†³ RAG åœ¨å¤šè·³æŸ¥è¯¢ä¸­çš„ç—›ç‚¹ã€‚\n> **Key Insight:** æå‡ºäº† **Structured Evidence Assessment (SEA)** æ¨¡å—ï¼Œåƒä¸€ä¸ªå®¡è®¡å‘˜ä¸€æ ·åˆ†æå½“å‰è¯æ®çš„ç¼ºå£ã€‚FAIR-RAG æ˜¯ä¸€ä¸ª Agent æ¡†æ¶ï¼Œå®ƒä¼šåŠ¨æ€ç”Ÿæˆå­æŸ¥è¯¢æ¥å¡«è¡¥è¿™äº›ç¼ºå£ï¼Œç›´åˆ°è¯æ®é“¾é—­ç¯ã€‚åœ¨ HotpotQA ä¸Šå–å¾—äº†æ–°çš„ SOTAã€‚\n\n**9. [Group Dynamics] LLM å¤šæ™ºèƒ½ä½“ç³»ç»Ÿä¸­çš„ç¾¤ä½“è§„æ¨¡æ•ˆåº”ä¸é›†ä½“é”™ä½**\n**Group size effects and collective misalignment in LLM multi-agent systems**\n> **Authors:** Ariel Flint et al.\n> **TLDR:** è®¨è®ºå¤šæ™ºèƒ½ä½“ç³»ç»Ÿï¼ˆMulti-agentï¼‰ä¸­â€œäººå¤šâ€ä¼šå‘ç”Ÿä»€ä¹ˆã€‚\n> **Key Insight:** ç¾¤ä½“è§„æ¨¡å¯¹åŠ¨åŠ›å­¦çš„å½±å“æ˜¯éçº¿æ€§çš„ã€‚äº’åŠ¨ä¸ä»…èƒ½æ”¾å¤§ä¸ªä½“åè§ï¼Œè¿˜ä¼šäº§ç”Ÿæ–°çš„é›†ä½“åè§ã€‚å½“ç¾¤ä½“è§„æ¨¡è¶…è¿‡ä¸´ç•Œå€¼æ—¶ï¼Œç³»ç»Ÿä¼šæ”¶æ•›åˆ°ç¡®å®šæ€§çš„é¢„æµ‹ï¼Œè¿™æ„å‘³ç€æˆ‘ä»¬åœ¨å¤§è§„æ¨¡éƒ¨ç½² Agent é›†ç¾¤æ—¶å¿…é¡»è€ƒè™‘è¿™ç§**ç¾¤ä½“åŠ¨åŠ›å­¦ (Population-level effects)**ã€‚\n\n---\n\n### ğŸ‘ï¸ å¤šæ¨¡æ€ä¸ç©ºé—´æ™ºèƒ½ (Multimodal & Spatial Intelligence)\n\n**10. [Spatial Reasoning] GRAID: é€šè¿‡é«˜ä¿çœŸæ•°æ®ç”Ÿæˆå¢å¼º VLM çš„ç©ºé—´æ¨ç†**\n**GRAID: Enhancing Spatial Reasoning of VLMs Through High-Fidelity Data Generation**\n> **Authors:** Karim Elmaaroufi et al.\n> **TLDR:** æŒ‡å‡ºç°æœ‰ VLM ç©ºé—´æ¨ç†å·®æ˜¯å› ä¸ºè®­ç»ƒæ•°æ®çƒ‚ã€‚\n> **Key Insight:** ç°æœ‰çš„ 3D é‡å»ºæ•°æ®ç”Ÿæˆæµç¨‹è¯¯å·®å¤§ã€‚GRAID å¦è¾Ÿè¹Šå¾„ï¼Œç›´æ¥åˆ©ç”¨ 2D è¾¹ç•Œæ¡†çš„å‡ ä½•åŸè¯­æ¥æ¨å¯¼ç©ºé—´å…³ç³»ï¼Œç”Ÿæˆäº† 850 ä¸‡ä¸ªé«˜è´¨é‡ VQA å¯¹ã€‚ç”¨è¿™ä¸ªæ•°æ®å¾®è°ƒåï¼ŒLlama 3.2 åœ¨ç©ºé—´ä»»åŠ¡ä¸Šæå‡å·¨å¤§ã€‚\n\n**11. [Music LLMs] è¯„ä¼°å¤šæ¨¡æ€å¤§æ¨¡å‹çš„æ ¸å¿ƒéŸ³ä¹æ„ŸçŸ¥ä»»åŠ¡**\n**Evaluating Multimodal Large Language Models on Core Music Perception Tasks**\n> **Authors:** Brandon James Carone et al.\n> **TLDR:** LLM æ‡‚ä¹ç†ï¼Œä½†å¬åŠ›ä¸å¥½ã€‚\n> **Key Insight:** å¯¹æ¯”äº† Gemini 2.5 å’Œ Qwen2.5-Omniã€‚å‘ç°æ¨¡å‹åœ¨å¤„ç† **MIDI (ç¬¦å·)** æ—¶å‡ ä¹æ»¡åˆ†ï¼Œä½†åœ¨å¤„ç† **Audio (éŸ³é¢‘)** æ—¶å‡†ç¡®ç‡å¤§å¹…ä¸‹é™ã€‚ç›®å‰çš„æ¨¡å‹æ›´åƒæ˜¯â€œè¯»è°±â€é«˜æ‰‹ï¼Œè€Œä¸æ˜¯â€œå¬éŸ³â€ä¸“å®¶ã€‚\n\n**12. [Image Captioning] ç”¨äºå›¾åƒæè¿°çš„è‡ªé¡¶å‘ä¸‹è¯­ä¹‰ç»†åŒ–**\n**Top-Down Semantic Refinement for Image Captioning**\n> **Authors:** Jusheng Zhang et al.\n> **TLDR:** è§£å†³ VLM å†™ caption æ—¶â€œç”šè‡³ä¸çœ‹å…¨å±€â€çš„é—®é¢˜ã€‚\n> **Key Insight:** å°† caption ç”Ÿæˆå»ºæ¨¡ä¸ºç›®æ ‡å¯¼å‘çš„å±‚çº§ç»†åŒ–è¿‡ç¨‹ (TDSR)ã€‚ä¸ºæ­¤è®¾è®¡äº†ä¸€ç§é’ˆå¯¹ VLM çš„é«˜æ•ˆ **è’™ç‰¹å¡æ´›æ ‘æœç´¢ (MCTS)** ç®—æ³•ï¼Œæ˜¾è‘—å‡å°‘äº†å¯¹æ˜‚è´µ VLM çš„è°ƒç”¨æ¬¡æ•°ï¼ŒåŒæ—¶æå‡äº†æè¿°çš„å…¨å±€è¿è´¯æ€§å’Œç»†èŠ‚ã€‚\n\n---\n\n### ğŸ”¬ ç§‘å­¦ä¸å›¾å­¦ä¹  (AI for Science & Graphs)\n\n**13. [Graph Prompting] GraphTOP: å›¾ç¥ç»ç½‘ç»œçš„å›¾æ‹“æ‰‘å¯¼å‘æç¤º**\n**GraphTOP: Graph Topology-Oriented Prompting for Graph Neural Networks**\n> **Authors:** Xingbo Fu et al.\n> **Key Insight:** ç°æœ‰çš„å›¾æç¤ºå­¦ä¹ ä¸»è¦æ”¹èŠ‚ç‚¹ç‰¹å¾ï¼Œè¿™ç¯‡è®ºæ–‡é¦–æ¬¡æå‡º **æ‹“æ‰‘å¯¼å‘ (Topology-oriented)** çš„æç¤ºã€‚é€šè¿‡åœ¨å¤šè·³å­å›¾ä¸­é‡è¿è¾¹ (Edge Rewiring) æ¥é€‚åº”é¢„è®­ç»ƒæ¨¡å‹ï¼Œæ•ˆæœä¼˜äºç‰¹å¾æç¤ºã€‚\n\n**14. [Traffic Forecasting] åŸºäºæ—¶ç©ºç›¸å…³æ€§èåˆçš„äº¤é€šé¢„æµ‹è¯¯å·®è°ƒæ•´**\n**Error Adjustment Based on Spatiotemporal Correlation Fusion for Traffic Forecasting**\n> **Authors:** Fuqiang Liu et al.\n> **Key Insight:** ä¼ ç»Ÿçš„ MSE æŸå¤±å‡½æ•°å‡è®¾è¯¯å·®æ˜¯ç‹¬ç«‹çš„ï¼Œä½†è¿™åœ¨äº¤é€šæµä¸­ä¸æˆç«‹ã€‚æœ¬æ–‡æå‡ºäº† SAEA æ¡†æ¶ï¼Œå°†è¯¯å·®å»ºæ¨¡ä¸º**æ—¶ç©ºå‘é‡è‡ªå›å½’ (VAR)** è¿‡ç¨‹ï¼Œæ˜¾å¼åœ°åœ¨æŸå¤±å‡½æ•°ä¸­æ•æ‰æ—¶ç©ºè‡ªç›¸å…³æ€§ã€‚\n\n**15. [Brain Foundation Model] LUNA: é«˜æ•ˆä¸”æ‹“æ‰‘æ— å…³çš„ EEG ä¿¡å·åˆ†æåŸºç¡€æ¨¡å‹**\n**LUNA: Efficient and Topology-Agnostic Foundation Model for EEG Signal Analysis**\n> **Authors:** Berkay DÃ¶ner et al.\n> **Key Insight:** è„‘ç”µæ•°æ®æœ€çƒ¦çš„æ˜¯ä¸åŒè®¾å¤‡çš„ç”µæåˆ†å¸ƒä¸ä¸€æ ·ã€‚LUNA é€šè¿‡ä¸€ä¸ªæ½œåœ¨ç©ºé—´å°†ä¸åŒæ‹“æ‰‘çš„ EEG å‹ç¼©æˆå›ºå®šè¡¨ç¤ºï¼Œå®ç°äº†**æ‹“æ‰‘æ— å…³ (Topology-agnostic)**ï¼Œå¹¶ä¸”è®¡ç®—é‡éšé€šé“æ•°çº¿æ€§å¢é•¿ï¼ˆè€ŒéäºŒæ¬¡æ–¹ï¼‰ï¼Œæ˜¯è„‘æœºæ¥å£é¢†åŸŸçš„ä¸€ä¸ªé‡è¦åŸºç¡€æ¨¡å‹ã€‚",
  "papers": [
    {
      "arxiv_id": "2510.23656v1",
      "title": "Error Adjustment Based on Spatiotemporal Correlation Fusion for Traffic Forecasting",
      "title_zh": "åŸºäºæ—¶ç©ºç›¸å…³æ€§èåˆçš„äº¤é€šé¢„æµ‹è¯¯å·®è°ƒæ•´",
      "authors": [
        "Fuqiang Liu",
        "Weiping Ding",
        "Luis Miranda-Moreno",
        "Lijun Sun"
      ],
      "abstract": "Deep neural networks (DNNs) play a significant role in an increasing body of research on traffic forecasting due to their effectively capturing spatiotemporal patterns embedded in traffic data. A general assumption of training the said forecasting models via mean squared error estimation is that the errors across time steps and spatial positions are uncorrelated. However, this assumption does not really hold because of the autocorrelation caused by both the temporality and spatiality of traffic data. This gap limits the performance of DNN-based forecasting models and is overlooked by current studies. To fill up this gap, this paper proposes Spatiotemporally Autocorrelated Error Adjustment (SAEA), a novel and general framework designed to systematically adjust autocorrelated prediction errors in traffic forecasting. Unlike existing approaches that assume prediction errors follow a random Gaussian noise distribution, SAEA models these errors as a spatiotemporal vector autoregressive (VAR) process to capture their intrinsic dependencies. First, it explicitly captures both spatial and temporal error correlations by a coefficient matrix, which is then embedded into a newly formulated cost function. Second, a structurally sparse regularization is introduced to incorporate prior spatial information, ensuring that the learned coefficient matrix aligns with the inherent road network structure. Finally, an inference process with test-time error adjustment is designed to dynamically refine predictions, mitigating the impact of autocorrelated errors in real-time forecasting. The effectiveness of the proposed approach is verified on different traffic datasets. Results across a wide range of traffic forecasting models show that our method enhances performance in almost all cases.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹äº¤é€šé¢„æµ‹ä¸­æ·±åº¦å­¦ä¹ æ¨¡å‹å¿½ç•¥é¢„æµ‹è¯¯å·®è‡ªç›¸å…³æ€§(Autocorrelation)çš„é—®é¢˜ï¼Œæå‡ºäº†SAEA(Spatiotemporally Autocorrelated Error Adjustment)é€šç”¨æ¡†æ¶ã€‚ä¸å‡è®¾è¯¯å·®ä¸ºéšæœºé«˜æ–¯å™ªå£°çš„ä¼ ç»Ÿæ–¹æ³•ä¸åŒï¼ŒSAEAå°†é¢„æµ‹è¯¯å·®å»ºæ¨¡ä¸ºæ—¶ç©ºå‘é‡è‡ªå›å½’(Vector Autoregressive, VAR)è¿‡ç¨‹ï¼Œæ—¨åœ¨ç³»ç»Ÿæ€§æ•æ‰å¹¶è°ƒæ•´è¯¯å·®é—´çš„å†…åœ¨æ—¶ç©ºä¾èµ–å…³ç³»ã€‚è¯¥æ¡†æ¶é€šè¿‡ç³»æ•°çŸ©é˜µæ˜¾å¼æè¿°è¯¯å·®ç›¸å…³æ€§ï¼Œå¹¶ç»“åˆç»“æ„ç¨€ç–æ­£åˆ™åŒ–(Structurally Sparse Regularization)æ¥èåˆè·¯ç½‘ç»“æ„çš„å…ˆéªŒä¿¡æ¯ã€‚æ­¤å¤–ï¼Œç ”ç©¶è¿˜è®¾è®¡äº†æµ‹è¯•æ—¶è¯¯å·®è°ƒæ•´(Test-time Error Adjustment)çš„æ¨ç†æœºåˆ¶ï¼Œèƒ½å¤Ÿåœ¨å®æ—¶é¢„æµ‹ä¸­åŠ¨æ€ç²¾ç»ƒç»“æœã€‚å®éªŒç»“æœéªŒè¯äº†SAEAçš„æœ‰æ•ˆæ€§ï¼Œè¯æ˜å…¶åœ¨å¤šç§äº¤é€šæ•°æ®é›†ä¸Šå‡èƒ½æ˜¾è‘—å¢å¼ºç°æœ‰äº¤é€šé¢„æµ‹æ¨¡å‹çš„æ€§èƒ½è¡¨ç°ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "12 pages, 7 figures, 3 tables",
      "pdf_url": "https://arxiv.org/pdf/2510.23656v1",
      "published_date": "2025-10-25 23:48:50 UTC",
      "updated_date": "2025-10-25 23:48:50 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T05:35:23.898323+00:00"
    },
    {
      "arxiv_id": "2510.22455v1",
      "title": "Evaluating Multimodal Large Language Models on Core Music Perception Tasks",
      "title_zh": "è¯„ä¼°å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹åœ¨æ ¸å¿ƒéŸ³ä¹æ„ŸçŸ¥ä»»åŠ¡ä¸­çš„è¡¨ç°",
      "authors": [
        "Brandon James Carone",
        "Iran R. Roman",
        "Pablo RipollÃ©s"
      ],
      "abstract": "Multimodal Large Language Models (LLMs) claim \"musical understanding\" via evaluations that conflate listening with score reading. We benchmark three SOTA LLMs (Gemini 2.5 Pro, Gemini 2.5 Flash, and Qwen2.5-Omni) across three core music skills: Syncopation Scoring, Transposition Detection, and Chord Quality Identification. Moreover, we separate three sources of variability: (i) perceptual limitations (audio vs. MIDI inputs), (ii) exposure to examples (zero- vs. few-shot manipulations), and (iii) reasoning strategies (Standalone, CoT, LogicLM). For the latter we adapt LogicLM, a framework combining LLMs with symbolic solvers to perform structured reasoning, to music. Results reveal a clear perceptual gap: models perform near ceiling on MIDI but show accuracy drops on audio. Reasoning and few-shot prompting offer minimal gains. This is expected for MIDI, where performance reaches saturation, but more surprising for audio, where LogicLM, despite near-perfect MIDI accuracy, remains notably brittle. Among models, Gemini Pro achieves the highest performance across most conditions. Overall, current systems reason well over symbols (MIDI) but do not yet \"listen\" reliably from audio. Our method and dataset make the perception-reasoning boundary explicit and offer actionable guidance for building robust, audio-first music systems.",
      "tldr_zh": "è¯¥ç ”ç©¶è¯„ä¼°äº†å¤šç§å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼ˆMultimodal Large Language Modelsï¼‰ï¼ŒåŒ…æ‹¬ Gemini 2.5 Proã€Gemini 2.5 Flash å’Œ Qwen2.5-Omniï¼Œåœ¨ Syncopation Scoringã€Transposition Detection å’Œ Chord Quality Identification ä¸‰é¡¹æ ¸å¿ƒéŸ³ä¹æ„ŸçŸ¥ä»»åŠ¡ä¸Šçš„è¡¨ç°ã€‚ç ”ç©¶é€šè¿‡å¯¹æ¯”éŸ³é¢‘ï¼ˆAudioï¼‰ä¸ MIDI è¾“å…¥ã€é›¶æ ·æœ¬ï¼ˆZero-shotï¼‰ä¸å°‘æ ·æœ¬ï¼ˆFew-shotï¼‰æ“çºµï¼Œä»¥åŠç‹¬ç«‹æ¨ç†ã€é“¾å¼æ€ç»´ï¼ˆCoTï¼‰å’Œ LogicLM ç­‰æ¨ç†ç­–ç•¥ï¼Œæ—¨åœ¨åŒºåˆ†æ„ŸçŸ¥é™åˆ¶ä¸æ¨ç†å˜å¼‚æ€§ã€‚å®éªŒç»“æœæ˜¾ç¤ºå‡ºæ˜¾è‘—çš„æ„ŸçŸ¥å·®è·ï¼Œå³æ¨¡å‹åœ¨å¤„ç† MIDI ç¬¦å·æ—¶è¡¨ç°æ¥è¿‘å®Œç¾ï¼Œä½†åœ¨ç›´æ¥å¤„ç†éŸ³é¢‘è¾“å…¥æ—¶å‡†ç¡®ç‡å¤§å¹…ä¸‹é™ã€‚å°½ç®¡ç ”ç©¶å°†ç»“åˆäº†ç¬¦å·æ±‚è§£å™¨çš„ LogicLM æ¡†æ¶é€‚é…äºéŸ³ä¹ä»»åŠ¡ï¼Œä½†å…¶åœ¨éŸ³é¢‘æ„ŸçŸ¥ä¸Šçš„è¡¨ç°ä¾ç„¶è„†å¼±ã€‚ç ”ç©¶è¡¨æ˜å½“å‰ç³»ç»Ÿè™½ç„¶å…·å¤‡å¼ºå¤§çš„ç¬¦å·æ¨ç†èƒ½åŠ›ï¼Œä½†åœ¨å¯é åœ°ä»éŸ³é¢‘ä¸­â€œå€¾å¬â€éŸ³ä¹æ–¹é¢ä»é¢ä¸´æŒ‘æˆ˜ã€‚è¿™ä¸€å‘ç°æ˜ç¡®äº†æ„ŸçŸ¥ä¸æ¨ç†çš„è¾¹ç•Œï¼Œä¸ºå¼€å‘é²æ£’çš„ã€éŸ³é¢‘ä¼˜å…ˆï¼ˆAudio-firstï¼‰çš„éŸ³ä¹ç†è§£ç³»ç»Ÿæä¾›äº†é‡è¦æŒ‡å¯¼ã€‚",
      "categories": [
        "cs.SD",
        "cs.AI",
        "eess.AS"
      ],
      "primary_category": "cs.SD",
      "comment": "Accepted to the NeurIPS 2025 Workshop on AI for Music (AI4Music), 16 pages, 1 figure, 3 tables",
      "pdf_url": "https://arxiv.org/pdf/2510.22455v1",
      "published_date": "2025-10-25 23:10:16 UTC",
      "updated_date": "2025-10-25 23:10:16 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T05:35:25.671618+00:00"
    },
    {
      "arxiv_id": "2510.22451v1",
      "title": "GraphTOP: Graph Topology-Oriented Prompting for Graph Neural Networks",
      "title_zh": "GraphTOPï¼šé¢å‘å›¾ç¥ç»ç½‘ç»œçš„å›¾æ‹“æ‰‘å¯¼å‘æç¤º",
      "authors": [
        "Xingbo Fu",
        "Zhenyu Lei",
        "Zihan Chen",
        "Binchi Zhang",
        "Chuxu Zhang",
        "Jundong Li"
      ],
      "abstract": "Graph Neural Networks (GNNs) have revolutionized the field of graph learning by learning expressive graph representations from massive graph data. As a common pattern to train powerful GNNs, the \"pre-training, adaptation\" scheme first pre-trains GNNs over unlabeled graph data and subsequently adapts them to specific downstream tasks. In the adaptation phase, graph prompting is an effective strategy that modifies input graph data with learnable prompts while keeping pre-trained GNN models frozen. Typically, existing graph prompting studies mainly focus on *feature-oriented* methods that apply graph prompts to node features or hidden representations. However, these studies often achieve suboptimal performance, as they consistently overlook the potential of *topology-oriented* prompting, which adapts pre-trained GNNs by modifying the graph topology. In this study, we conduct a pioneering investigation of graph prompting in terms of graph topology. We propose the first **Graph** **T**opology-**O**riented **P**rompting (GraphTOP) framework to effectively adapt pre-trained GNN models for downstream tasks. More specifically, we reformulate topology-oriented prompting as an edge rewiring problem within multi-hop local subgraphs and relax it into the continuous probability space through reparameterization while ensuring tight relaxation and preserving graph sparsity. Extensive experiments on five graph datasets under four pre-training strategies demonstrate that our proposed GraphTOP outshines six baselines on multiple node classification datasets. Our code is available at https://github.com/xbfu/GraphTOP.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å›¾ç¥ç»ç½‘ç»œ(GNNs)åœ¨â€œé¢„è®­ç»ƒ-é€‚é…â€æ¨¡å¼ä¸­ç°æœ‰çš„å›¾æç¤º(Graph Prompting)æŠ€æœ¯ä¸»è¦å±€é™äºç‰¹å¾å¯¼å‘(Feature-oriented)è€Œå¿½è§†æ‹“æ‰‘ä¼˜åŒ–çš„é—®é¢˜ï¼Œæå‡ºäº†é¦–ä¸ªé¢å‘å›¾æ‹“æ‰‘çš„æç¤ºæ¡†æ¶GraphTOPã€‚è¯¥æ¡†æ¶å°†æ‹“æ‰‘å¯¼å‘çš„æç¤ºå­¦ä¹ é‡æ–°å®šä¹‰ä¸ºå¤šè·³å±€éƒ¨å­å›¾å†…çš„è¾¹é‡è¿(Edge Rewiring)é—®é¢˜ï¼Œå¹¶é€šè¿‡é‡å‚æ•°åŒ–æŠ€æœ¯å°†å…¶æ¾å¼›åˆ°è¿ç»­æ¦‚ç‡ç©ºé—´è¿›è¡Œå¤„ç†ã€‚GraphTOPåœ¨ç¡®ä¿æ¾å¼›ä¸¥å¯†æ€§çš„åŒæ—¶æœ‰æ•ˆåœ°ä¿ç•™äº†å›¾çš„ç¨€ç–æ€§(Graph Sparsity)ï¼Œå®ç°äº†å¯¹å›¾ç»“æ„çš„ç²¾å‡†é€‚é…ã€‚åœ¨äº”ä¸ªå›¾æ•°æ®é›†å’Œå››ç§é¢„è®­ç»ƒç­–ç•¥ä¸‹çš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼ŒGraphTOPåœ¨å¤šä¸ªèŠ‚ç‚¹åˆ†ç±»ä»»åŠ¡ä¸Šçš„è¡¨ç°æ˜¾è‘—ä¼˜äºå…­ç§åŸºçº¿æ¨¡å‹ã€‚è¿™é¡¹å·¥ä½œå¼€åˆ›æ€§åœ°ä»æ‹“æ‰‘ç»´åº¦æ¢ç´¢äº†å›¾æç¤ºå­¦ä¹ ï¼Œä¸ºæå‡é¢„è®­ç»ƒæ¨¡å‹åœ¨ä¸‹æ¸¸ä»»åŠ¡ä¸­çš„æ³›åŒ–èƒ½åŠ›æä¾›äº†æ–°çš„è§†è§’å’Œé«˜æ•ˆçš„è§£å†³æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "Accepted by the 39 Annual Conference on Neural Information Processing Systems (NeurIPS 2025)",
      "pdf_url": "https://arxiv.org/pdf/2510.22451v1",
      "published_date": "2025-10-25 22:50:12 UTC",
      "updated_date": "2025-10-25 22:50:12 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T05:35:24.874580+00:00"
    },
    {
      "arxiv_id": "2510.22450v2",
      "title": "SmartMixed: A Two-Phase Training Strategy for Adaptive Activation Function Learning in Neural Networks",
      "title_zh": "SmartMixedï¼šä¸€ç§ç”¨äºç¥ç»ç½‘ç»œè‡ªé€‚åº”æ¿€æ´»å‡½æ•°å­¦ä¹ çš„ä¸¤é˜¶æ®µè®­ç»ƒç­–ç•¥",
      "authors": [
        "Amin Omidvar"
      ],
      "abstract": "The choice of activation function plays a critical role in neural networks, yet most architectures still rely on fixed, uniform activation functions across all neurons. We introduce SmartMixed, a two-phase training strategy that allows networks to learn optimal per-neuron activation functions while preserving computational efficiency at inference. In the first phase, neurons adaptively select from a pool of candidate activation functions (ReLU, Sigmoid, Tanh, Leaky ReLU, ELU, SELU) using a differentiable hard-mixture mechanism. In the second phase, each neuron's activation function is fixed according to the learned selection, resulting in a computationally efficient network that supports continued training with optimized vectorized operations. We evaluate SmartMixed on the MNIST dataset using feedforward neural networks of varying depths. The analysis shows that neurons in different layers exhibit distinct preferences for activation functions, providing insights into the functional diversity within neural architectures.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†SmartMixedï¼Œä¸€ç§ç”¨äºç¥ç»ç½‘ç»œä¸­è‡ªé€‚åº”æ¿€æ´»å‡½æ•°å­¦ä¹ çš„ä¸¤é˜¶æ®µè®­ç»ƒç­–ç•¥ï¼Œæ—¨åœ¨è§£å†³å¤§å¤šæ•°æ¶æ„ä¾èµ–å›ºå®šã€ç»Ÿä¸€æ¿€æ´»å‡½æ•°çš„é—®é¢˜ã€‚åœ¨ç¬¬ä¸€é˜¶æ®µï¼Œç¥ç»å…ƒé€šè¿‡å¯å¾®åˆ†çš„ç¡¬æ··åˆæœºåˆ¶(differentiable hard-mixture mechanism)ï¼Œä»åŒ…å«ReLUã€Sigmoidã€Tanhç­‰å€™é€‰æ± ä¸­è‡ªé€‚åº”é€‰æ‹©æœ€ä¼˜æ¿€æ´»å‡½æ•°ã€‚åœ¨ç¬¬äºŒé˜¶æ®µï¼Œæ¯ä¸ªç¥ç»å…ƒçš„æ¿€æ´»å‡½æ•°æ ¹æ®å­¦ä¹ åˆ°çš„é€‰æ‹©è¢«å›ºå®šï¼Œä»è€Œåœ¨æ¨ç†æ—¶ä¿æŒè®¡ç®—æ•ˆç‡å¹¶æ”¯æŒå‘é‡åŒ–æ“ä½œä¼˜åŒ–ã€‚é€šè¿‡åœ¨MNISTæ•°æ®é›†ä¸Šçš„å®éªŒåˆ†æè¡¨æ˜ï¼Œä¸åŒå±‚çš„ç¥ç»å…ƒå¯¹æ¿€æ´»å‡½æ•°è¡¨ç°å‡ºæˆªç„¶ä¸åŒçš„åå¥½ï¼Œæ­ç¤ºäº†ç¥ç»æ¶æ„å†…éƒ¨çš„åŠŸèƒ½å¤šæ ·æ€§ã€‚è¯¥æ–¹æ³•åœ¨ä¿ç•™æ¨ç†æ•ˆç‡çš„åŒæ—¶ï¼Œä¸ºå®ç°æ›´å…·çµæ´»æ€§çš„ç¥ç»ç½‘ç»œè®¾è®¡æä¾›äº†æ–°çš„æ€è·¯ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.22450v2",
      "published_date": "2025-10-25 22:46:37 UTC",
      "updated_date": "2025-10-31 02:28:33 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T05:35:25.378672+00:00"
    },
    {
      "arxiv_id": "2510.22439v2",
      "title": "PromptReverb: Multimodal Room Impulse Response Generation Through Latent Rectified Flow Matching",
      "title_zh": "PromptReverbï¼šåŸºäºæ½œåœ¨ä¿®æ­£æµåŒ¹é…çš„å¤šæ¨¡æ€æˆ¿é—´å†²æ¿€å“åº”ç”Ÿæˆ",
      "authors": [
        "Ali Vosoughi",
        "Yongyi Zang",
        "Qihui Yang",
        "Nathan Paek",
        "Randal Leistikow",
        "Chenliang Xu"
      ],
      "abstract": "Room impulse response (RIR) generation remains a critical challenge for creating immersive virtual acoustic environments. Current methods suffer from two fundamental limitations: the scarcity of full-band RIR datasets and the inability of existing models to generate acoustically accurate responses from diverse input modalities. We present PromptReverb, a two-stage generative framework that addresses these challenges. Our approach combines a variational autoencoder that upsamples band-limited RIRs to full-band quality (48 kHz), and a conditional diffusion transformer model based on rectified flow matching that generates RIRs from descriptions in natural language. Empirical evaluation demonstrates that PromptReverb produces RIRs with superior perceptual quality and acoustic accuracy compared to existing methods, achieving 8.8% mean RT60 error compared to -37% for widely used baselines and yielding more realistic room-acoustic parameters. Our method enables practical applications in virtual reality, architectural acoustics, and audio production where flexible, high-quality RIR synthesis is essential.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹è™šæ‹Ÿå£°å­¦ç¯å¢ƒæ„å»ºä¸­æˆ¿é—´è„‰å†²å“åº”(Room Impulse Response, RIR)ç”Ÿæˆçš„æŒ‘æˆ˜ï¼ŒæŒ‡å‡ºäº†ç°æœ‰æ–¹æ³•åœ¨å…¨é¢‘å¸¦æ•°æ®é›†ç¨€ç¼ºä»¥åŠå¤šæ¨¡æ€è¾“å…¥å¤„ç†èƒ½åŠ›ä¸è¶³æ–¹é¢çš„å±€é™ã€‚ç ”ç©¶æå‡ºäº† PromptReverbï¼Œè¿™æ˜¯ä¸€ä¸ªä¸¤é˜¶æ®µçš„ç”Ÿæˆæ¡†æ¶ï¼Œæ—¨åœ¨ä»è‡ªç„¶è¯­è¨€æè¿°ä¸­åˆæˆé«˜è´¨é‡çš„ RIRã€‚è¯¥æ¡†æ¶é¦–å…ˆåˆ©ç”¨å˜åˆ†è‡ªç¼–ç å™¨(Variational Autoencoder, VAE)å°†é¢‘å¸¦å—é™çš„ RIR ä¸Šé‡‡æ ·è‡³ 48 kHz çš„å…¨é¢‘å¸¦è´¨é‡ï¼Œéšåé‡‡ç”¨åŸºäºä¿®æ­£æµåŒ¹é…(Rectified Flow Matching)çš„æ¡ä»¶æ‰©æ•£è½¬æ¢å™¨æ¨¡å‹(Conditional Diffusion Transformer)å®ç°å£°å­¦å“åº”çš„ç²¾å‡†ç”Ÿæˆã€‚å®éªŒè¯„ä¼°æ˜¾ç¤ºï¼ŒPromptReverb åœ¨æ„ŸçŸ¥è´¨é‡å’Œå£°å­¦å‡†ç¡®æ€§æ–¹é¢å‡ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œå…¶å¹³å‡ RT60 è¯¯å·®ä»…ä¸º 8.8%ï¼Œè¿œä¼˜äºåŸºå‡†æ¨¡å‹çš„ -37%ã€‚è¯¥æ–¹æ³•èƒ½äº§ç”Ÿæ›´çœŸå®çš„æˆ¿é—´å£°å­¦å‚æ•°ï¼Œä¸ºè™šæ‹Ÿç°å®ã€å»ºç­‘å£°å­¦å’ŒéŸ³é¢‘åˆ¶ä½œç­‰éœ€è¦çµæ´»ã€é«˜è´¨é‡ RIR åˆæˆçš„é¢†åŸŸæä¾›äº†é«˜æ•ˆçš„è§£å†³æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.SD",
        "cs.AI"
      ],
      "primary_category": "cs.SD",
      "comment": "9 pages, 2 figures, 4 tables; v2: corrected spelling of a co-author name; no content changes",
      "pdf_url": "https://arxiv.org/pdf/2510.22439v2",
      "published_date": "2025-10-25 21:38:07 UTC",
      "updated_date": "2025-10-29 15:42:33 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T05:35:51.786161+00:00"
    },
    {
      "arxiv_id": "2510.22437v1",
      "title": "Modeling Hierarchical Thinking in Large Reasoning Models",
      "title_zh": "å¤§æ¨ç†æ¨¡å‹ä¸­çš„å±‚çº§æ€ç»´å»ºæ¨¡",
      "authors": [
        "G M Shahariar",
        "Ali Nazari",
        "Erfan Shayegani",
        "Nael Abu-Ghazaleh"
      ],
      "abstract": "Large Language Models (LLMs) have demonstrated remarkable reasoning abilities when they generate step-by-step solutions, known as chain-of-thought (CoT) reasoning. When trained to using chain-of-thought reasoning examples, the resulting models (called Large Reasoning Models, or LRMs) appear to learn hierarchical thinking strategies similar to those used by humans. However, understanding LRMs emerging reasoning capabilities remains a difficult open problem, with many potential important applications including improving training and understanding robustness. In this paper, we adopt a memoryless Finite State Machine formulation to approximate LRM's emerging hierarchical reasoning dynamics as a structured, interpretable abstraction. We identify a small set of discrete reasoning states including - initialization, deduction, augmentation-strategy, uncertainty-estimation, backtracking, and final-conclusion that capture the high-level states present in the model's reasoning process. By annotating each step of a model's CoT with these states, we can represent the reasoning trajectory as a transition sequence through the state graph. This FSM formulation provides a systematic way to analyze, interpret and visualize how different models approach problems. We describe the FSM model, provide examples of CoT annotations under this scheme, and discuss how it can shed light on differences between available models in their approach to reasoning. Our results demonstrate that this FSM-based analysis reveals distinct reasoning patterns and potential shortcomings, offering a new lens to evaluate and improve LLM reasoning.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†å¤§å‹æ¨ç†æ¨¡å‹(Large Reasoning Models, LRMs)åœ¨ç”Ÿæˆæ€ç»´é“¾(Chain-of-Thought, CoT)è¿‡ç¨‹ä¸­å±•ç°å‡ºçš„å±‚çº§åŒ–æ€ç»´ç­–ç•¥ï¼Œæ—¨åœ¨è§£å†³å…¶æ¨ç†åŠ¨åŠ›å­¦éš¾ä»¥è§£é‡Šçš„é—®é¢˜ã€‚ä¸ºäº†å»ºç«‹ç»“æ„åŒ–ä¸”å¯è§£é‡Šçš„æŠ½è±¡æ¨¡å‹ï¼Œç ”ç©¶è€…é‡‡ç”¨äº†ä¸€ç§æ— è®°å¿†æœ‰é™çŠ¶æ€æœº(Finite State Machine, FSM)è¡¨è¿°ï¼Œè¯†åˆ«å‡ºåŒ…æ‹¬initializationã€deductionã€augmentation-strategyã€uncertainty-estimationã€backtrackingå’Œfinal-conclusionåœ¨å†…çš„ç¦»æ•£æ¨ç†çŠ¶æ€ã€‚é€šè¿‡å°†CoTçš„æ¯ä¸ªæ­¥éª¤æ ‡æ³¨ä¸ºç›¸åº”çŠ¶æ€ï¼Œæ¨ç†è½¨è¿¹è¢«è½¬åŒ–ä¸ºçŠ¶æ€å›¾ä¸­çš„è½¬æ¢åºåˆ—ï¼Œä»è€Œå®ç°äº†å¯¹æ¨¡å‹æ¨ç†æ–¹å¼çš„ç³»ç»Ÿæ€§åˆ†æä¸å¯è§†åŒ–ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¿™ç§åŸºäºFSMçš„åˆ†ææ–¹æ³•èƒ½æœ‰æ•ˆæ­ç¤ºä¸åŒæ¨¡å‹ä¹‹é—´çš„æ¨ç†æ¨¡å¼å·®å¼‚åŠæ½œåœ¨ç¼ºé™·ï¼Œä¸ºè¯„ä¼°å’Œæå‡å¤§è¯­è¨€æ¨¡å‹(LLMs)çš„æ¨ç†èƒ½åŠ›æä¾›äº†æ–°çš„è§†è§’ã€‚",
      "categories": [
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.22437v1",
      "published_date": "2025-10-25 21:25:30 UTC",
      "updated_date": "2025-10-25 21:25:30 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T05:35:37.809142+00:00"
    },
    {
      "arxiv_id": "2510.22422v1",
      "title": "Group size effects and collective misalignment in LLM multi-agent systems",
      "title_zh": "å¤§è¯­è¨€æ¨¡å‹å¤šæ™ºèƒ½ä½“ç³»ç»Ÿä¸­çš„ç¾¤ä½“è§„æ¨¡æ•ˆåº”ä¸é›†ä½“å¤±è°ƒ",
      "authors": [
        "Ariel Flint",
        "Luca Maria Aiello",
        "Romualdo Pastor-Satorras",
        "Andrea Baronchelli"
      ],
      "abstract": "Multi-agent systems of large language models (LLMs) are rapidly expanding across domains, introducing dynamics not captured by single-agent evaluations. Yet, existing work has mostly contrasted the behavior of a single agent with that of a collective of fixed size, leaving open a central question: how does group size shape dynamics? Here, we move beyond this dichotomy and systematically explore outcomes across the full range of group sizes. We focus on multi-agent misalignment, building on recent evidence that interacting LLMs playing a simple coordination game can generate collective biases absent in individual models. First, we show that collective bias is a deeper phenomenon than previously assessed: interaction can amplify individual biases, introduce new ones, or override model-level preferences. Second, we demonstrate that group size affects the dynamics in a non-linear way, revealing model-dependent dynamical regimes. Finally, we develop a mean-field analytical approach and show that, above a critical population size, simulations converge to deterministic predictions that expose the basins of attraction of competing equilibria. These findings establish group size as a key driver of multi-agent dynamics and highlight the need to consider population-level effects when deploying LLM-based systems at scale.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†å¤§å‹è¯­è¨€æ¨¡å‹(LLMs)å¤šæ™ºèƒ½ä½“ç³»ç»Ÿä¸­çš„ç¾¤ä½“è§„æ¨¡æ•ˆåº”(Group size effects)ä¸é›†ä½“å¤±è°ƒ(Collective misalignment)é—®é¢˜ï¼Œæ—¨åœ¨æ­ç¤ºç¾¤ä½“è§„æ¨¡å¦‚ä½•å¡‘é€ åä½œåŠ¨æ€ã€‚ç ”ç©¶å‘ç°é›†ä½“åå·®(Collective bias)æ˜¯ä¸€ä¸ªæ¯”æ­¤å‰è¯„ä¼°æ›´æ·±å±‚çš„ç°è±¡ï¼Œæ™ºèƒ½ä½“é—´çš„äº’åŠ¨å¯ä»¥æ”¾å¤§ä¸ªä½“åå·®ã€å¼•å…¥æ–°åå·®ï¼Œç”šè‡³è¦†ç›–æ¨¡å‹è‡ªèº«çš„åå¥½ã€‚å®éªŒè¿›ä¸€æ­¥è¯æ˜ç¾¤ä½“è§„æ¨¡ä»¥éçº¿æ€§æ–¹å¼å½±å“ç³»ç»ŸåŠ¨æ€ï¼Œå¹¶æ­ç¤ºäº†æ¨¡å‹ä¾èµ–çš„åŠ¨æ€æœºåˆ¶(Dynamical regimes)ã€‚é€šè¿‡å¼€å‘å¹³å‡åœºåˆ†ææ–¹æ³•(Mean-field analytical approach)ï¼Œç ”ç©¶æ˜¾ç¤ºå½“è¶…è¿‡ä¸´ç•Œäººå£è§„æ¨¡æ—¶ï¼Œç³»ç»Ÿæ¨¡æ‹Ÿä¼šæ”¶æ•›äºèƒ½å¤Ÿæš´éœ²ç«äº‰å‡è¡¡å¸å¼•å­ç›†(Basins of attraction)çš„ç¡®å®šæ€§é¢„æµ‹ã€‚è¿™äº›å‘ç°ç¡®ç«‹äº†ç¾¤ä½“è§„æ¨¡æ˜¯å¤šæ™ºèƒ½ä½“åŠ¨æ€çš„å…³é”®é©±åŠ¨å› ç´ ï¼Œå¼ºè°ƒäº†åœ¨å¤§è§„æ¨¡éƒ¨ç½²LLMç³»ç»Ÿæ—¶å¿…é¡»è€ƒè™‘ç¾¤ä½“å±‚é¢çš„æ•ˆåº”ã€‚",
      "categories": [
        "cs.MA",
        "cs.AI",
        "cs.CY",
        "physics.soc-ph"
      ],
      "primary_category": "cs.MA",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.22422v1",
      "published_date": "2025-10-25 19:45:45 UTC",
      "updated_date": "2025-10-25 19:45:45 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T05:35:54.986531+00:00"
    },
    {
      "arxiv_id": "2510.22405v2",
      "title": "Knowledge-guided Continual Learning for Behavioral Analytics Systems",
      "title_zh": "é¢å‘è¡Œä¸ºåˆ†æç³»ç»Ÿçš„çŸ¥è¯†å¼•å¯¼å‹æŒç»­å­¦ä¹ ",
      "authors": [
        "Yasas Senarath",
        "Hemant Purohit"
      ],
      "abstract": "User behavior on online platforms is evolving, reflecting real-world changes in how people post, whether it's helpful messages or hate speech. Models that learn to capture this content can experience a decrease in performance over time due to data drift, which can lead to ineffective behavioral analytics systems. However, fine-tuning such a model over time with new data can be detrimental due to catastrophic forgetting. Replay-based approaches in continual learning offer a simple yet efficient method to update such models, minimizing forgetting by maintaining a buffer of important training instances from past learned tasks. However, the main limitation of this approach is the fixed size of the buffer. External knowledge bases can be utilized to overcome this limitation through data augmentation. We propose a novel augmentation-based approach to incorporate external knowledge in the replay-based continual learning framework. We evaluate several strategies with three datasets from prior studies related to deviant behavior classification to assess the integration of external knowledge in continual learning and demonstrate that augmentation helps outperform baseline replay-based approaches.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹åœ¨çº¿å¹³å°ç”¨æˆ·è¡Œä¸ºæ¼”å˜å¯¼è‡´çš„æ•°æ®æ¼‚ç§»(Data Drift)å’Œæ¨¡å‹å¾®è°ƒä¸­çš„ç¾éš¾æ€§é—å¿˜(Catastrophic Forgetting)é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§å°†å¤–éƒ¨çŸ¥è¯†åº“(External Knowledge Bases)é›†æˆåˆ°æŒç»­å­¦ä¹ (Continual Learning)æ¡†æ¶ä¸­çš„æ–°æ–¹æ³•ã€‚è™½ç„¶ä¼ ç»Ÿçš„é‡æ”¾æœºåˆ¶(Replay-based approaches)èƒ½ç¼“è§£æ€§èƒ½ä¸‹é™ï¼Œä½†å…¶æ•ˆæœå—é™äºå›ºå®šå¤§å°çš„ç¼“å†²åŒº(Buffer Size)ã€‚ä¸ºäº†å…‹æœè¿™ä¸€å±€é™ï¼Œè¯¥ç ”ç©¶é‡‡ç”¨åŸºäºæ•°æ®å¢å¼º(Augmentation-based)çš„ç­–ç•¥ï¼Œåˆ©ç”¨å¤–éƒ¨çŸ¥è¯†æ¥å¼•å¯¼æ¨¡å‹å­¦ä¹ ï¼Œä»è€Œæ›´æœ‰æ•ˆåœ°ä¿ç•™è¿‡å»ä»»åŠ¡çš„çŸ¥è¯†ã€‚é€šè¿‡åœ¨ä¸‰ä¸ªåç¦»è¡Œä¸ºåˆ†ç±»(Deviant Behavior Classification)ç›¸å…³æ•°æ®é›†ä¸Šçš„å®éªŒè¯„ä¼°ï¼Œç»“æœè¯æ˜è¯¥æ–¹æ³•åœ¨æ€§èƒ½ä¸Šæ˜¾è‘—ä¼˜äºç°æœ‰çš„é‡æ”¾åŸºå‡†æ¨¡å‹ã€‚è¯¥æˆæœä¸ºæ„å»ºæ›´ç¨³å¥ã€å…·å¤‡çŸ¥è¯†å¼•å¯¼èƒ½åŠ›çš„å®æ—¶è¡Œä¸ºåˆ†æç³»ç»Ÿæä¾›äº†é‡è¦å‚è€ƒã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "This is a preprint of the accepted paper at IEEE CogMI 2025 - The 7th IEEE International Conference on Cognitive Machine Intelligence",
      "pdf_url": "https://arxiv.org/pdf/2510.22405v2",
      "published_date": "2025-10-25 19:04:14 UTC",
      "updated_date": "2025-11-01 19:07:18 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T05:35:44.382367+00:00"
    },
    {
      "arxiv_id": "2510.22391v1",
      "title": "Top-Down Semantic Refinement for Image Captioning",
      "title_zh": "ç”¨äºå›¾åƒæè¿°çš„è‡ªé¡¶å‘ä¸‹è¯­ä¹‰ç»†åŒ–",
      "authors": [
        "Jusheng Zhang",
        "Kaitong Cai",
        "Jing Yang",
        "Jian Wang",
        "Chengpei Tang",
        "Keze Wang"
      ],
      "abstract": "Large Vision-Language Models (VLMs) face an inherent contradiction in image captioning: their powerful single-step generation capabilities often lead to a myopic decision-making process. This makes it difficult to maintain global narrative coherence while capturing rich details, a limitation that is particularly pronounced in tasks that require multi-step and complex scene description. To overcome this fundamental challenge, we redefine image captioning as a goal-oriented hierarchical refinement planning problem, and further propose a novel framework, named Top-Down Semantic Refinement (TDSR), which models the generation process as a Markov Decision Process (MDP). However, planning within the vast state space of a VLM presents a significant computational hurdle. Our core contribution, therefore, is the design of a highly efficient Monte Carlo Tree Search (MCTS) algorithm tailored for VLMs. By incorporating a visual-guided parallel expansion and a lightweight value network, our TDSR reduces the call frequency to the expensive VLM by an order of magnitude without sacrificing planning quality. Furthermore, an adaptive early stopping mechanism dynamically matches computational overhead to the image's complexity. Extensive experiments on multiple benchmarks, including DetailCaps, COMPOSITIONCAP, and POPE, demonstrate that our TDSR, as a plug-and-play module, can significantly enhance the performance of existing VLMs (e.g., LLaVA-1.5, Qwen2.5-VL) by achieving state-of-the-art or highly competitive results in fine-grained description, compositional generalization, and hallucination suppression.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹è§†è§‰è¯­è¨€æ¨¡å‹ (VLMs) åœ¨å›¾åƒæè¿°ä¸­å› â€œè¿‘è§†â€å†³ç­–å¯¼è‡´éš¾ä»¥å¹³è¡¡å…¨å±€ä¸€è‡´æ€§ä¸ä¸°å¯Œç»†èŠ‚çš„é—®é¢˜ï¼Œæå‡ºäº†åä¸º Top-Down Semantic Refinement (TDSR) çš„å±‚çº§ä¼˜åŒ–è§„åˆ’æ¡†æ¶ã€‚TDSR å°†ç”Ÿæˆè¿‡ç¨‹å»ºæ¨¡ä¸ºé©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹ (MDP)ï¼Œå¹¶æ ¸å¿ƒè®¾è®¡äº†ä¸€ç§é«˜æ•ˆçš„è’™ç‰¹å¡æ´›æ ‘æœç´¢ (MCTS) ç®—æ³•ï¼Œé€šè¿‡è§†è§‰å¼•å¯¼çš„å¹¶è¡Œæ‰©å±•å’Œè½»é‡çº§ä»·å€¼ç½‘ç»œï¼Œåœ¨ä¸ç‰ºç‰²è§„åˆ’è´¨é‡çš„å‰æä¸‹æ˜¾è‘—é™ä½äº†å¯¹æ˜‚è´µ VLMs çš„è°ƒç”¨é¢‘ç‡ã€‚æ­¤å¤–ï¼Œè¯¥æ¡†æ¶å¼•å…¥äº†è‡ªé€‚åº”æ—©åœæœºåˆ¶ä»¥æ ¹æ®å›¾åƒå¤æ‚åº¦åŠ¨æ€åŒ¹é…è®¡ç®—å¼€é”€ã€‚åœ¨ DetailCapsã€COMPOSITIONCAP å’Œ POPE ç­‰å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸Šçš„å®éªŒè¯æ˜ï¼ŒTDSR ä½œä¸ºä¸€ä¸ªå³æ’å³ç”¨çš„æ¨¡å—ï¼Œèƒ½æ˜¾è‘—å¢å¼º LLaVA-1.5 å’Œ Qwen2.5-VL ç­‰ç°æœ‰æ¨¡å‹çš„æ€§èƒ½ã€‚è¯¥æ–¹æ³•åœ¨ç»†ç²’åº¦æè¿°ã€ç»„åˆæ³›åŒ–å’Œå¹»è§‰æŠ‘åˆ¶ (hallucination suppression) æ–¹é¢å‡å–å¾—äº† state-of-the-art æˆ–æå…·ç«äº‰åŠ›çš„ç»“æœã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.22391v1",
      "published_date": "2025-10-25 18:27:00 UTC",
      "updated_date": "2025-10-25 18:27:00 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T05:35:45.891216+00:00"
    },
    {
      "arxiv_id": "2510.22389v1",
      "title": "Can Small and Reasoning Large Language Models Score Journal Articles for Research Quality and Do Averaging and Few-shot Help?",
      "title_zh": "å°å‹ä¸æ¨ç†å‹å¤§è¯­è¨€æ¨¡å‹èƒ½å¦è¯„ä¼°æœŸåˆŠè®ºæ–‡ç ”ç©¶è´¨é‡ï¼šå¾—åˆ†å¹³å‡åŒ–ä¸å°‘æ ·æœ¬æç¤ºçš„ä½œç”¨åˆ†æ",
      "authors": [
        "Mike Thelwall",
        "Ehsan Mohammadi"
      ],
      "abstract": "Assessing published academic journal articles is a common task for evaluations of departments and individuals. Whilst it is sometimes supported by citation data, Large Language Models (LLMs) may give more useful indications of article quality. Evidence of this capability exists for two of the largest LLM families, ChatGPT and Gemini, and the medium sized LLM Gemma3 27b, but it is unclear whether smaller LLMs and reasoning models have similar abilities. This is important because larger models may be slow and impractical in some situations, and reasoning models may perform differently. Four relevant questions are addressed with Gemma3 variants, Llama4 Scout, Qwen3, Magistral Small and DeepSeek R1, on a dataset of 2,780 medical, health and life science papers in 6 fields, with two different gold standards, one novel. The results suggest that smaller (open weights) and reasoning LLMs have similar performance to ChatGPT 4o-mini and Gemini 2.0 Flash, but that 1b parameters may often, and 4b sometimes, be too few. Moreover, averaging scores from multiple identical queries seems to be a universally successful strategy, and few-shot prompts (four examples) tended to help but the evidence was equivocal. Reasoning models did not have a clear advantage. Overall, the results show, for the first time, that smaller LLMs >4b, including reasoning models, have a substantial capability to score journal articles for research quality, especially if score averaging is used.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†å°å‹å’Œæ¨ç†å‹å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨è¯„ä¼°å­¦æœ¯æœŸåˆŠè®ºæ–‡è´¨é‡æ–¹é¢çš„æ½œåŠ›ï¼Œå¡«è¡¥äº†å…³äºå°å‹æ¨¡å‹åŠæ¨ç†æ¨¡å‹åœ¨è¿™ä¸€é¢†åŸŸè¡¨ç°çš„ç ”ç©¶ç©ºç™½ã€‚é€šè¿‡å¯¹Gemma3ã€Llama4 Scoutã€Qwen3ã€Magistral Smallå’ŒDeepSeek R1ç­‰æ¨¡å‹åœ¨2,780ç¯‡åŒ»å­¦åŠç”Ÿå‘½ç§‘å­¦è®ºæ–‡ä¸Šçš„æµ‹è¯•ï¼Œç ”ç©¶å‘ç°å‚æ•°é‡å¤§äº4bçš„å°å‹æ¨¡å‹è¯„åˆ†è¡¨ç°ä¸ChatGPT 4o-miniå’ŒGemini 2.0 Flashç›¸å½“ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œ1bå‚æ•°æ¨¡å‹æ€§èƒ½é€šå¸¸ä¸è¶³ï¼Œè€Œæ¨ç†æ¨¡å‹åœ¨æ­¤ç±»è¯„åˆ†ä»»åŠ¡ä¸­å¹¶æœªå±•ç°å‡ºæ¯”æ™®é€šæ¨¡å‹æ›´æ˜æ˜¾çš„ä¼˜åŠ¿ã€‚ç ”ç©¶è¿›ä¸€æ­¥è¯å®ï¼Œé€šè¿‡å¯¹å¤šæ¬¡é‡å¤æŸ¥è¯¢çš„è¯„åˆ†è¿›è¡Œå¹³å‡åŒ–ï¼ˆAveraging scoresï¼‰èƒ½æ˜¾è‘—æå‡è¯„ä¼°ç¨³å®šæ€§ï¼Œè€Œå°‘æ ·æœ¬æç¤ºï¼ˆFew-shot promptsï¼‰çš„è¾…åŠ©æ•ˆæœå°šä¸å®Œå…¨æ˜ç¡®ã€‚è¯¥ç ”ç©¶é¦–æ¬¡è¯æ˜äº†å‚æ•°é‡å¤§äº4bçš„å°å‹LLMsåœ¨ç ”ç©¶è´¨é‡è¯„åˆ†ä»»åŠ¡ä¸­å…·æœ‰å®è´¨æ€§çš„åº”ç”¨èƒ½åŠ›ï¼Œä¸ºä½æˆæœ¬ã€é«˜æ•ˆç‡çš„å­¦æœ¯è¯„ä»·æä¾›äº†æ–°é€”å¾„ã€‚",
      "categories": [
        "cs.DL",
        "cs.AI"
      ],
      "primary_category": "cs.DL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.22389v1",
      "published_date": "2025-10-25 18:12:41 UTC",
      "updated_date": "2025-10-25 18:12:41 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T05:35:50.789962+00:00"
    },
    {
      "arxiv_id": "2510.22383v1",
      "title": "Dynamic Dropout: Leveraging Conway's Game of Life for Neural Networks Regularization",
      "title_zh": "Dynamic Dropoutï¼šåŸºäº Conway ç”Ÿå‘½æ¸¸æˆçš„ç¥ç»ç½‘ç»œæ­£åˆ™åŒ–",
      "authors": [
        "David Freire-ObregÃ³n",
        "JosÃ© Salas-CÃ¡ceres",
        "Modesto CastrillÃ³n-Santana"
      ],
      "abstract": "Regularization techniques play a crucial role in preventing overfitting and improving the generalization performance of neural networks. Dropout, a widely used regularization technique, randomly deactivates units during training to introduce redundancy and prevent co-adaptation among neurons. Despite its effectiveness, dropout has limitations, such as its static nature and lack of interpretability. In this paper, we propose a novel approach to regularization by substituting dropout with Conway's Game of Life (GoL), a cellular automata with simple rules that govern the evolution of a grid of cells. We introduce dynamic unit deactivation during training by representing neural network units as cells in a GoL grid and applying the game's rules to deactivate units. This approach allows for the emergence of spatial patterns that adapt to the training data, potentially enhancing the network's ability to generalize. We demonstrate the effectiveness of our approach on the CIFAR-10 dataset, showing that dynamic unit deactivation using GoL achieves comparable performance to traditional dropout techniques while offering insights into the network's behavior through the visualization of evolving patterns. Furthermore, our discussion highlights the applicability of our proposal in deeper architectures, demonstrating how it enhances the performance of different dropout techniques.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§åˆ›æ–°çš„æ­£åˆ™åŒ–(Regularization)æ–¹æ³•ï¼Œæ—¨åœ¨é€šè¿‡å¼•å…¥åº·å¨ç”Ÿå‘½æ¸¸æˆ(Conway's Game of Life, GoL)æ¥æ›¿ä»£ä¼ ç»Ÿçš„DropoutæŠ€æœ¯ï¼Œä»¥è§£å†³å…¶é™æ€ç‰¹æ€§å’Œç¼ºä¹å¯è§£é‡Šæ€§çš„å±€é™ã€‚åœ¨è¯¥æ¡†æ¶ä¸‹ï¼Œç¥ç»ç½‘ç»œå•å…ƒè¢«è§†ä¸ºGoLç½‘æ ¼ä¸­çš„ç»†èƒï¼Œåˆ©ç”¨æ¸¸æˆçš„æ¼”åŒ–è§„åˆ™æ¥å®ç°åŠ¨æ€çš„å•å…ƒåœç”¨ã€‚è¿™ç§æ–¹æ³•å…è®¸æ¨¡å‹åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­æ¶Œç°å‡ºèƒ½å¤Ÿé€‚åº”æ•°æ®çš„ç©ºé—´æ¨¡å¼ï¼Œä»è€Œæ½œåœ¨åœ°å¢å¼ºç½‘ç»œçš„æ³›åŒ–æ€§èƒ½ã€‚åœ¨CIFAR-10æ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼Œè¿™ç§åŠ¨æ€åœç”¨æœºåˆ¶åœ¨ä¿æŒä¸ä¼ ç»ŸDropoutç›¸å½“æ€§èƒ½çš„åŒæ—¶ï¼Œé€šè¿‡æ¨¡å¼æ¼”åŒ–çš„å¯è§†åŒ–ä¸ºç½‘ç»œè¡Œä¸ºæä¾›äº†æ›´æ·±å±‚çš„æ´å¯Ÿã€‚æ­¤å¤–ï¼Œè¯¥ç ”ç©¶è¿˜æ¢è®¨äº†è¯¥æ–¹æ¡ˆåœ¨æ·±å±‚æ¶æ„ä¸­çš„é€‚ç”¨æ€§ï¼Œè¯æ˜å…¶èƒ½æœ‰æ•ˆæå‡å¤šç§Dropoutå˜ä½“çš„è¡¨ç°ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.LG",
      "comment": "Accepted for presentation at the 5th International Conference on Computing and Machine Intelligence (ICMI 2026)",
      "pdf_url": "https://arxiv.org/pdf/2510.22383v1",
      "published_date": "2025-10-25 17:55:13 UTC",
      "updated_date": "2025-10-25 17:55:13 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T05:36:28.775582+00:00"
    },
    {
      "arxiv_id": "2510.22380v1",
      "title": "Efficient Large-Deformation Medical Image Registration via Recurrent Dynamic Correlation",
      "title_zh": "åŸºäºå¾ªç¯åŠ¨æ€ç›¸å…³çš„é«˜æ•ˆå¤§å˜å½¢åŒ»å­¦å›¾åƒé…å‡†",
      "authors": [
        "Tianran Li",
        "Marius Staring",
        "Yuchuan Qiao"
      ],
      "abstract": "Deformable image registration estimates voxel-wise correspondences between images through spatial transformations, and plays a key role in medical imaging. While deep learning methods have significantly reduced runtime, efficiently handling large deformations remains a challenging task. Convolutional networks aggregate local features but lack direct modeling of voxel correspondences, promoting recent works to explore explicit feature matching. Among them, voxel-to-region matching is more efficient for direct correspondence modeling by computing local correlation features whithin neighbourhoods, while region-to-region matching incurs higher redundancy due to excessive correlation pairs across large regions. However, the inherent locality of voxel-to-region matching hinders the capture of long-range correspondences required for large deformations. To address this, we propose a Recurrent Correlation-based framework that dynamically relocates the matching region toward more promising positions. At each step, local matching is performed with low cost, and the estimated offset guides the next search region, supporting efficient convergence toward large deformations. In addition, we uses a lightweight recurrent update module with memory capacity and decouples motion-related and texture features to suppress semantic redundancy. We conduct extensive experiments on brain MRI and abdominal CT datasets under two settings: with and without affine pre-registration. Results show that our method exibits a strong accuracy-computation trade-off, surpassing or matching the state-of-the-art performance. For example, it achieves comparable performance on the non-affine OASIS dataset, while using only 9.5% of the FLOPs and running 96% faster than RDP, a representative high-performing method.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹åŒ»å­¦å›¾åƒé…å‡† (Medical Image Registration) ä¸­å¤„ç†å¤§å˜å½¢ (Large Deformation) æ•ˆç‡ä½ä¸‹çš„é—®é¢˜ï¼Œæå‡ºäº†ä¸€ä¸ªåŸºäºå¾ªç¯ç›¸å…³ (Recurrent Correlation) çš„åŠ¨æ€æ¡†æ¶ã€‚ä¸ºäº†å…‹æœä¼ ç»Ÿä½“ç´ åˆ°åŒºåŸŸåŒ¹é… (Voxel-to-Region Matching) éš¾ä»¥æ•æ‰é•¿ç¨‹å¯¹åº”å…³ç³»çš„å±€é™æ€§ï¼Œè¯¥æ¡†æ¶é€šè¿‡é€’å½’åœ°åŠ¨æ€é‡å®šä½åŒ¹é…åŒºåŸŸï¼Œåœ¨ä¿æŒä½è®¡ç®—æˆæœ¬çš„åŒæ—¶æœ‰æ•ˆå¼•å¯¼æ¨¡å‹å‘å¤§å˜å½¢ä½ç½®æ”¶æ•›ã€‚æ­¤å¤–ï¼Œç ”ç©¶è¿˜é‡‡ç”¨äº†è½»é‡åŒ–å¾ªç¯æ›´æ–°æ¨¡å—ï¼Œå¹¶ç»“åˆè¿åŠ¨ä¸çº¹ç†ç‰¹å¾è§£è€¦ç­–ç•¥æ¥æŠ‘åˆ¶è¯­ä¹‰å†—ä½™ã€‚åœ¨è„‘éƒ¨ MRI å’Œè…¹éƒ¨ CT æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨ç²¾åº¦ä¸è®¡ç®—å¼€é”€ä¹‹é—´å–å¾—äº†å“è¶Šçš„å¹³è¡¡ï¼Œæ€§èƒ½æ¯”è‚©å½“å‰æœ€å…ˆè¿›æŠ€æœ¯ã€‚åœ¨ OASIS æ•°æ®é›†æµ‹è¯•ä¸­ï¼Œè¯¥æ–¹æ³•åœ¨è®¡ç®—é‡ (FLOPs) ä»…ä¸º RDP æ¨¡å‹çš„ 9.5% ä¸”è¿è¡Œé€Ÿåº¦æå‡ 96% çš„æƒ…å†µä¸‹ï¼Œä¾ç„¶å®ç°äº†æå…·ç«äº‰åŠ›çš„é…å‡†ç²¾åº¦ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.22380v1",
      "published_date": "2025-10-25 17:49:29 UTC",
      "updated_date": "2025-10-25 17:49:29 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T05:36:10.376449+00:00"
    },
    {
      "arxiv_id": "2510.22379v4",
      "title": "TraceTrans: Translation and Spatial Tracing for Surgical Prediction",
      "title_zh": "TraceTransï¼šç”¨äºæ‰‹æœ¯é¢„æµ‹çš„å›¾åƒç¿»è¯‘ä¸ç©ºé—´è¿½è¸ª",
      "authors": [
        "Xiyu Luo",
        "Haodong Li",
        "Xinxing Cheng",
        "He Zhao",
        "Yang Hu",
        "Xuan Song",
        "Tianyang Zhang"
      ],
      "abstract": "Image-to-image translation models have achieved notable success in converting images across visual domains and are increasingly used for medical tasks such as predicting post-operative outcomes and modeling disease progression. However, most existing methods primarily aim to match the target distribution and often neglect spatial correspondences between the source and translated images. This limitation can lead to structural inconsistencies and hallucinations, undermining the reliability and interpretability of the predictions. These challenges are accentuated in clinical applications by the stringent requirement for anatomical accuracy. In this work, we present TraceTrans, a novel deformable image translation model designed for post-operative prediction that generates images aligned with the target distribution while explicitly revealing spatial correspondences with the pre-operative input. The framework employs an encoder for feature extraction and dual decoders for predicting spatial deformations and synthesizing the translated image. The predicted deformation field imposes spatial constraints on the generated output, ensuring anatomical consistency with the source. Extensive experiments on medical cosmetology and brain MRI datasets demonstrate that TraceTrans delivers accurate and interpretable post-operative predictions, highlighting its potential for reliable clinical deployment.",
      "tldr_zh": "ç°æœ‰çš„å›¾åƒç¿»è¯‘(Image-to-image translation)æ¨¡å‹åœ¨é¢„æµ‹æœ¯åç»“æœç­‰åŒ»å­¦ä»»åŠ¡æ—¶ï¼Œå¾€å¾€å¿½ç•¥æºå›¾åƒä¸ç¿»è¯‘å›¾åƒä¹‹é—´çš„ç©ºé—´å¯¹åº”å…³ç³»ï¼Œä»è€Œå¯¼è‡´ç»“æ„ä¸ä¸€è‡´å’Œå¹»è§‰é—®é¢˜ï¼Œé™ä½äº†é¢„æµ‹çš„å¯ä¿¡åº¦ã€‚æœ¬ç ”ç©¶æå‡ºäº† TraceTransï¼Œä¸€ç§ä¸“é—¨ç”¨äºæœ¯åé¢„æµ‹çš„æ–°å‹å¯å˜å½¢å›¾åƒç¿»è¯‘æ¨¡å‹ï¼Œæ—¨åœ¨ç”Ÿæˆç¬¦åˆç›®æ ‡åˆ†å¸ƒå›¾åƒçš„åŒæ—¶ï¼Œæ˜ç¡®æ­ç¤ºä¸æœ¯å‰è¾“å…¥ä¹‹é—´çš„ç©ºé—´å¯¹åº”å…³ç³»ã€‚è¯¥æ¡†æ¶åˆ©ç”¨ç¼–ç å™¨è¿›è¡Œç‰¹å¾æå–ï¼Œå¹¶é€šè¿‡åŒè§£ç å™¨(dual decoders)åˆ†åˆ«é¢„æµ‹ç©ºé—´å˜å½¢å’Œåˆæˆç¿»è¯‘å›¾åƒã€‚é€šè¿‡å˜å½¢åœº(deformation field)æ–½åŠ çš„ç©ºé—´çº¦æŸï¼Œè¯¥æ¨¡å‹ç¡®ä¿äº†ç”Ÿæˆç»“æœä¸æºå›¾åƒåœ¨è§£å‰–ç»“æ„ä¸Šçš„ä¸€è‡´æ€§(anatomical consistency)ã€‚åœ¨åŒ»ç–—ç¾å®¹å’Œè„‘éƒ¨ MRI æ•°æ®é›†ä¸Šçš„å®éªŒè¯æ˜ï¼ŒTraceTrans èƒ½å¤Ÿæä¾›å‡†ç¡®ä¸”å…·æœ‰å¯è§£é‡Šæ€§çš„æœ¯åé¢„æµ‹ï¼Œå±•ç°äº†å…¶åœ¨ä¸´åºŠå¯é éƒ¨ç½²ä¸­çš„å·¨å¤§æ½œåŠ›ã€‚",
      "categories": [
        "eess.IV",
        "cs.AI",
        "cs.CV",
        "cs.LG"
      ],
      "primary_category": "eess.IV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.22379v4",
      "published_date": "2025-10-25 17:48:46 UTC",
      "updated_date": "2025-12-04 00:36:11 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T05:36:26.891017+00:00"
    },
    {
      "arxiv_id": "2510.22373v1",
      "title": "VisJudge-Bench: Aesthetics and Quality Assessment of Visualizations",
      "title_zh": "VisJudge-Benchï¼šå¯è§†åŒ–å®¡ç¾ä¸è´¨é‡è¯„ä¼°",
      "authors": [
        "Yupeng Xie",
        "Zhiyang Zhang",
        "Yifan Wu",
        "Sirong Lu",
        "Jiayi Zhang",
        "Zhaoyang Yu",
        "Jinlin Wang",
        "Sirui Hong",
        "Bang Liu",
        "Chenglin Wu",
        "Yuyu Luo"
      ],
      "abstract": "Visualization, a domain-specific yet widely used form of imagery, is an effective way to turn complex datasets into intuitive insights, and its value depends on whether data are faithfully represented, clearly communicated, and aesthetically designed. However, evaluating visualization quality is challenging: unlike natural images, it requires simultaneous judgment across data encoding accuracy, information expressiveness, and visual aesthetics. Although multimodal large language models (MLLMs) have shown promising performance in aesthetic assessment of natural images, no systematic benchmark exists for measuring their capabilities in evaluating visualizations. To address this, we propose VisJudge-Bench, the first comprehensive benchmark for evaluating MLLMs' performance in assessing visualization aesthetics and quality. It contains 3,090 expert-annotated samples from real-world scenarios, covering single visualizations, multiple visualizations, and dashboards across 32 chart types. Systematic testing on this benchmark reveals that even the most advanced MLLMs (such as GPT-5) still exhibit significant gaps compared to human experts in judgment, with a Mean Absolute Error (MAE) of 0.551 and a correlation with human ratings of only 0.429. To address this issue, we propose VisJudge, a model specifically designed for visualization aesthetics and quality assessment. Experimental results demonstrate that VisJudge significantly narrows the gap with human judgment, reducing the MAE to 0.442 (a 19.8% reduction) and increasing the consistency with human experts to 0.681 (a 58.7% improvement) compared to GPT-5. The benchmark is available at https://github.com/HKUSTDial/VisJudgeBench.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¯è§†åŒ–å›¾åƒè´¨é‡è¯„ä¼°çš„å¤æ‚æŒ‘æˆ˜ï¼Œæå‡ºäº†é¦–ä¸ªå…¨é¢è¯„ä¼°å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨å¯è§†åŒ–å®¡ç¾ä¸è´¨é‡è¯„ä¼°èƒ½åŠ›çš„åŸºå‡†æµ‹è¯•é›† VisJudge-Benchã€‚è¯¥åŸºå‡†åŒ…å«3,090ä¸ªæºè‡ªçœŸå®åœºæ™¯ä¸”ç»ä¸“å®¶æ ‡æ³¨çš„æ ·æœ¬ï¼Œè¦†ç›–äº†å•å›¾ã€å¤šå›¾åŠä»ªè¡¨æ¿ç­‰32ç§å›¾è¡¨ç±»å‹ã€‚ç³»ç»Ÿæ€§æµ‹è¯•æ­ç¤ºäº†å½“å‰æœ€å…ˆè¿›çš„ MLLMsï¼ˆå¦‚ GPT-5ï¼‰åœ¨åˆ¤æ–­å¯è§†åŒ–è´¨é‡æ—¶ä¸äººç±»ä¸“å®¶å­˜åœ¨æ˜¾è‘—å·®è·ï¼Œå…¶å¹³å‡ç»å¯¹è¯¯å·®ï¼ˆMAEï¼‰è¾¾0.551ï¼Œä¸”ç›¸å…³æ€§ä»…ä¸º0.429ã€‚ä¸ºæ­¤ï¼Œç ”ç©¶è€…å¼€å‘äº†ä¸“é—¨é’ˆå¯¹å¯è§†åŒ–è¯„ä¼°çš„æ¨¡å‹ VisJudgeã€‚å®éªŒç»“æœè¯æ˜ï¼ŒVisJudge ç›¸æ¯” GPT-5 å°† MAE é™ä½äº†19.8%ï¼Œå¹¶å°†ä¸äººç±»ä¸“å®¶çš„ä¸€è‡´æ€§æ˜¾è‘—æå‡äº†58.7%ï¼Œæœ‰æ•ˆç¼©å°äº†è‡ªåŠ¨è¯„ä¼°ç³»ç»Ÿä¸äººå·¥è¯„åˆ¤ä¹‹é—´çš„æ€§èƒ½å·®è·ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.CL",
      "comment": "53 pages, 26 figures, 5 tables",
      "pdf_url": "https://arxiv.org/pdf/2510.22373v1",
      "published_date": "2025-10-25 17:31:02 UTC",
      "updated_date": "2025-10-25 17:31:02 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T05:36:35.864175+00:00"
    },
    {
      "arxiv_id": "2510.22371v1",
      "title": "Reasoning Models Reason Well, Until They Don't",
      "title_zh": "æ¨ç†æ¨¡å‹è¡¨ç°ä¼˜å¼‚ï¼Œç›´è‡³å¤±çµä¹‹æ—¶",
      "authors": [
        "Revanth Rameshkumar",
        "Jimson Huang",
        "Yunxin Sun",
        "Fei Xia",
        "Abulhair Saparov"
      ],
      "abstract": "Large language models (LLMs) have shown significant progress in reasoning tasks. However, recent studies show that transformers and LLMs fail catastrophically once reasoning problems exceed modest complexity. We revisit these findings through the lens of large reasoning models (LRMs) -- LLMs fine-tuned with incentives for step-by-step argumentation and self-verification. LRM performance on graph and reasoning benchmarks such as NLGraph seem extraordinary, with some even claiming they are capable of generalized reasoning and innovation in reasoning-intensive fields such as mathematics, physics, medicine, and law. However, by more carefully scaling the complexity of reasoning problems, we show existing benchmarks actually have limited complexity. We develop a new dataset, the Deep Reasoning Dataset (DeepRD), along with a generative process for producing unlimited examples of scalable complexity. We use this dataset to evaluate model performance on graph connectivity and natural language proof planning. We find that the performance of LRMs drop abruptly at sufficient complexity and do not generalize. We also relate our LRM results to the distributions of the complexities of large, real-world knowledge graphs, interaction graphs, and proof datasets. We find the majority of real-world examples fall inside the LRMs' success regime, yet the long tails expose substantial failure potential. Our analysis highlights the near-term utility of LRMs while underscoring the need for new methods that generalize beyond the complexity of examples in the training distribution.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†å¤§å‹æ¨ç†æ¨¡å‹ï¼ˆLRMsï¼‰åœ¨å¤„ç†å¤æ‚æ¨ç†ä»»åŠ¡æ—¶çš„å±€é™æ€§ï¼ŒæŒ‡å‡ºè™½ç„¶è¿™äº›æ¨¡å‹åœ¨ç°æœ‰åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œä½†é¢å¯¹é«˜å¤æ‚åº¦é—®é¢˜æ—¶ä¼šå‡ºç°æ€§èƒ½éª¤é™ã€‚ä¸ºæ­¤ï¼Œç ”ç©¶å›¢é˜Ÿå¼€å‘äº†åŒ…å«å¯æ‰©å±•å¤æ‚åº¦æ ·æœ¬çš„æ·±åº¦æ¨ç†æ•°æ®é›†ï¼ˆDeepRDï¼‰ï¼Œå¹¶é’ˆå¯¹å›¾è¿æ¥æ€§ï¼ˆgraph connectivityï¼‰å’Œè‡ªç„¶è¯­è¨€è¯æ˜è§„åˆ’ï¼ˆnatural language proof planningï¼‰ä»»åŠ¡è¿›è¡Œäº†ç³»ç»Ÿè¯„ä¼°ã€‚å®éªŒå‘ç°ï¼ŒLRMs çš„æ¨ç†èƒ½åŠ›åœ¨è¾¾åˆ°ç‰¹å®šå¤æ‚åº¦é˜ˆå€¼åæ— æ³•æœ‰æ•ˆæ³›åŒ–ï¼Œè¡¨ç°å‡ºæ˜æ˜¾çš„é²æ£’æ€§ä¸è¶³ã€‚é€šè¿‡å¯¹æ¯”ç°å®ä¸–ç•ŒçŸ¥è¯†å›¾è°±å’Œè¯æ˜æ•°æ®é›†çš„å¤æ‚åº¦åˆ†å¸ƒï¼Œç ”ç©¶æ­ç¤ºäº†æ¨¡å‹åœ¨å¤„ç†é•¿å°¾åˆ†å¸ƒæ¡ˆä¾‹æ—¶çš„æ½œåœ¨å¤±è´¥é£é™©ã€‚è¯¥åˆ†æåœ¨è‚¯å®š LRMs è¿‘æœŸå®ç”¨ä»·å€¼çš„åŒæ—¶ï¼Œå¼ºè°ƒäº†äºŸéœ€æ¢ç´¢èƒ½å¤Ÿè¶…è¶Šè®­ç»ƒåˆ†å¸ƒå¤æ‚åº¦é™åˆ¶çš„æ–°å‹æ¨ç†æœºåˆ¶ï¼Œä»¥å®ç°çœŸæ­£çš„å¹¿ä¹‰æ¨ç†ã€‚",
      "categories": [
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.22371v1",
      "published_date": "2025-10-25 17:28:38 UTC",
      "updated_date": "2025-10-25 17:28:38 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T05:36:20.280868+00:00"
    },
    {
      "arxiv_id": "2510.22370v1",
      "title": "BLIP-FusePPO: A Vision-Language Deep Reinforcement Learning Framework for Lane Keeping in Autonomous Vehicles",
      "title_zh": "BLIP-FusePPOï¼šé¢å‘è‡ªåŠ¨é©¾é©¶è½¦è¾†è½¦é“ä¿æŒçš„è§†è§‰è¯­è¨€æ·±åº¦å¼ºåŒ–å­¦ä¹ æ¡†æ¶",
      "authors": [
        "Seyed Ahmad Hosseini Miangoleh",
        "Amin Jalal Aghdasian",
        "Farzaneh Abdollahi"
      ],
      "abstract": "In this paper, we propose Bootstrapped Language-Image Pretraining-driven Fused State Representation in Proximal Policy Optimization (BLIP-FusePPO), a novel multimodal reinforcement learning (RL) framework for autonomous lane-keeping (LK), in which semantic embeddings generated by a vision-language model (VLM) are directly fused with geometric states, LiDAR observations, and Proportional-Integral-Derivative-based (PID) control feedback within the agent observation space. The proposed method lets the agent learn driving rules that are aware of their surroundings and easy to understand by combining high-level scene understanding from the VLM with low-level control and spatial signals. Our architecture brings together semantic, geometric, and control-aware representations to make policy learning more robust. A hybrid reward function that includes semantic alignment, LK accuracy, obstacle avoidance, and speed regulation helps learning to be more efficient and generalizable. Our method is different from the approaches that only use semantic models to shape rewards. Instead, it directly embeds semantic features into the state representation. This cuts down on expensive runtime inference and makes sure that semantic guidance is always available. The simulation results show that the proposed model is better at LK stability and adaptability than the best vision-based and multimodal RL baselines in a wide range of difficult driving situations. We make our code publicly available.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† BLIP-FusePPOï¼Œä¸€ç§ç”¨äºè‡ªåŠ¨é©¾é©¶è½¦é“ä¿æŒ(Lane Keeping, LK)çš„æ–°å‹å¤šæ¨¡æ€å¼ºåŒ–å­¦ä¹ (Reinforcement Learning, RL)æ¡†æ¶ã€‚è¯¥æ¡†æ¶å°†è§†è§‰è¯­è¨€æ¨¡å‹(Vision-Language Model, VLM)ç”Ÿæˆçš„è¯­ä¹‰åµŒå…¥(semantic embeddings)ä¸å‡ ä½•çŠ¶æ€ã€LiDAR è§‚æµ‹åŠåŸºäº PID çš„æ§åˆ¶åé¦ˆç›´æ¥èåˆåœ¨æ™ºèƒ½ä½“çš„è§‚æµ‹ç©ºé—´ä¸­ã€‚é€šè¿‡å°† VLM çš„é«˜å±‚åœºæ™¯ç†è§£ä¸ä½å±‚æ§åˆ¶å’Œç©ºé—´ä¿¡å·ç›¸ç»“åˆï¼Œè¯¥æ–¹æ³•æ˜¾è‘—æå‡äº†ç­–ç•¥å­¦ä¹ çš„é²æ£’æ€§ã€‚ç ”ç©¶è®¾è®¡äº†ä¸€ç§åŒ…å«è¯­ä¹‰å¯¹é½ã€LK å‡†ç¡®æ€§ã€é¿éšœå’Œé€Ÿåº¦è°ƒèŠ‚çš„æ··åˆå¥–åŠ±å‡½æ•°ï¼Œä»¥å¢å¼ºæ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ã€‚ä¸ä»…åˆ©ç”¨è¯­ä¹‰æ¨¡å‹è¿›è¡Œå¥–åŠ±å¡‘é€ çš„æ–¹æ³•ä¸åŒï¼Œè¯¥æ–¹æ¡ˆç›´æ¥å°†è¯­ä¹‰ç‰¹å¾åµŒå…¥çŠ¶æ€è¡¨ç¤º(state representation)ï¼Œåœ¨é™ä½è¿è¡Œæ—¶æ¨ç†å¼€é”€çš„åŒæ—¶ç¡®ä¿äº†è¯­ä¹‰å¼•å¯¼çš„å¯ç”¨æ€§ã€‚å®éªŒç»“æœè¯æ˜ï¼Œåœ¨å¤šç§å¤æ‚é©¾é©¶ç¯å¢ƒä¸‹ï¼ŒBLIP-FusePPO åœ¨è½¦é“ä¿æŒçš„ç¨³å®šæ€§å’Œé€‚åº”æ€§æ–¹é¢å‡ä¼˜äºç°æœ‰çš„è§†è§‰åŠå¤šæ¨¡æ€å¼ºåŒ–å­¦ä¹ åŸºå‡†æ¨¡å‹ã€‚",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.CV",
        "cs.LG",
        "cs.SE"
      ],
      "primary_category": "cs.RO",
      "comment": "https://github.com/Amin-A96/BLIP-FusePPO-A-Vision-Language-Deep-Reinforcement-Learning-Framework-for-Lane-Keeping-in-Autonomous.git",
      "pdf_url": "https://arxiv.org/pdf/2510.22370v1",
      "published_date": "2025-10-25 17:27:08 UTC",
      "updated_date": "2025-10-25 17:27:08 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T05:36:35.079950+00:00"
    },
    {
      "arxiv_id": "2510.22366v1",
      "title": "T2SMark: Balancing Robustness and Diversity in Noise-as-Watermark for Diffusion Models",
      "title_zh": "T2SMarkï¼šå¹³è¡¡æ‰©æ•£æ¨¡å‹â€œå™ªå£°å³æ°´å°â€ä¸­çš„é²æ£’æ€§ä¸å¤šæ ·æ€§",
      "authors": [
        "Jindong Yang",
        "Han Fang",
        "Weiming Zhang",
        "Nenghai Yu",
        "Kejiang Chen"
      ],
      "abstract": "Diffusion models have advanced rapidly in recent years, producing high-fidelity images while raising concerns about intellectual property protection and the misuse of generative AI. Image watermarking for diffusion models, particularly Noise-as-Watermark (NaW) methods, encode watermark as specific standard Gaussian noise vector for image generation, embedding the infomation seamlessly while maintaining image quality. For detection, the generation process is inverted to recover the initial noise vector containing the watermark before extraction. However, existing NaW methods struggle to balance watermark robustness with generation diversity. Some methods achieve strong robustness by heavily constraining initial noise sampling, which degrades user experience, while others preserve diversity but prove too fragile for real-world deployment. To address this issue, we propose T2SMark, a two-stage watermarking scheme based on Tail-Truncated Sampling (TTS). Unlike prior methods that simply map bits to positive or negative values, TTS enhances robustness by embedding bits exclusively in the reliable tail regions while randomly sampling the central zone to preserve the latent distribution. Our two-stage framework then ensures sampling diversity by integrating a randomly generated session key into both encryption pipelines. We evaluate T2SMark on diffusion models with both U-Net and DiT backbones. Extensive experiments show that it achieves an optimal balance between robustness and diversity. Our code is available at \\href{https://github.com/0xD009/T2SMark}{https://github.com/0xD009/T2SMark}.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹æ‰©æ•£æ¨¡å‹(Diffusion Models)çš„ç‰ˆæƒä¿æŠ¤é—®é¢˜ï¼Œæå‡ºäº†T2SMarkï¼Œä¸€ç§æ—¨åœ¨å¹³è¡¡å™ªå£°å³æ°´å°(Noise-as-Watermark, NaW)æŠ€æœ¯ä¸­é²æ£’æ€§(Robustness)ä¸ç”Ÿæˆå¤šæ ·æ€§(Diversity)çš„ä¸¤é˜¶æ®µæ–¹æ¡ˆã€‚è¯¥æ–¹æ³•æ ¸å¿ƒåœ¨äºå¼•å…¥äº†å°¾éƒ¨æˆªæ–­é‡‡æ ·(Tail-Truncated Sampling, TTS)ï¼Œé€šè¿‡å°†æ¯”ç‰¹ä¿¡æ¯ä¸“é—¨åµŒå…¥åœ¨åˆ†å¸ƒä¸­å¯é çš„å°¾éƒ¨åŒºåŸŸ(tail regions)ï¼ŒåŒæ—¶å¯¹ä¸­å¿ƒåŒºåŸŸè¿›è¡Œéšæœºé‡‡æ ·ä»¥ä¿ç•™æ½œåœ¨åˆ†å¸ƒçš„åŸå§‹ç‰¹æ€§ã€‚ä¸ºäº†è¿›ä¸€æ­¥æå‡ç”Ÿæˆçš„å¤šæ ·æ€§ï¼ŒT2SMarkåœ¨ä¸¤é˜¶æ®µæ¡†æ¶ä¸­å°†ä¼šè¯å¯†é’¥(session key)é›†æˆåˆ°åŠ å¯†æµæ°´çº¿ä¸­ã€‚åœ¨åŸºäºU-Netå’ŒDiTæ¶æ„çš„æ‰©æ•£æ¨¡å‹ä¸Šè¿›è¡Œçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨é²æ£’æ€§å’Œå¤šæ ·æ€§ä¹‹é—´è¾¾åˆ°äº†æœ€ä¼˜å¹³è¡¡ï¼Œæœ‰æ•ˆè§£å†³äº†ç°æœ‰NaWæ–¹æ³•å› è¿‡åº¦çº¦æŸé‡‡æ ·è€Œå¯¼è‡´ç”¨æˆ·ä½“éªŒä¸‹é™çš„é—®é¢˜ã€‚è¿™ä¸€ç ”ç©¶ä¸ºç”Ÿæˆå¼AIçš„çŸ¥è¯†äº§æƒä¿æŠ¤å’Œé˜²æ­¢æŠ€æœ¯æ»¥ç”¨æä¾›äº†å…¼é¡¾å®‰å…¨æ€§ä¸ç”Ÿæˆè´¨é‡çš„æœ‰æ•ˆå·¥å…·ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "Accepted by NeurIPS 2025",
      "pdf_url": "https://arxiv.org/pdf/2510.22366v1",
      "published_date": "2025-10-25 16:55:55 UTC",
      "updated_date": "2025-10-25 16:55:55 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T05:36:36.804566+00:00"
    },
    {
      "arxiv_id": "2510.23652v1",
      "title": "The Structural Scalpel: Automated Contiguous Layer Pruning for Large Language Models",
      "title_zh": "ç»“æ„æ‰‹æœ¯åˆ€ï¼šå¤§è¯­è¨€æ¨¡å‹è‡ªåŠ¨åŒ–è¿ç»­å±‚å‰ªæ",
      "authors": [
        "Yao Lu",
        "Yuqi Li",
        "Wenbin Xie",
        "Shanqing Yu",
        "Qi Xuan",
        "Zhaowei Zhu",
        "Shiping Wen"
      ],
      "abstract": "Although large language models (LLMs) have achieved revolutionary breakthroughs in many fields, their large model size and high computational cost pose significant challenges for practical deployment on resource-constrained edge devices. To this end, layer pruning has been proposed to reduce the computational overhead by directly removing redundant layers. However, existing layer pruning methods typically rely on hand-crafted metrics to evaluate and remove individual layers, while ignoring the dependencies between layers. This can disrupt the model's information flow and severely degrade performance. To address these issues, we propose CLP, a novel continuous layer pruning framework that introduces two key innovations: a differentiable concave gate algorithm that automatically identifies the best continuous layer segments for pruning via gradient-based optimization; and a cutoff endpoint tuning strategy that effectively restores model performance by fine-tuning only the layers adjacent to the pruned segments. Extensive experiments across multiple model architectures (including LLaMA2, LLaMA3 and Qwen) and sizes (from $7$B to $70$B parameters) show that CLP significantly outperforms existing state-of-the-art baselines. For example, at a pruning rate of $20\\%$, CLP achieves an average performance retention of $95.34\\%$ on LLaMA3-70B, outperforming baselines by $4.29\\%$-$30.52\\%$. Furthermore, CLP can be seamlessly combined with quantization to further compress the model with only a slight performance loss.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨éƒ¨ç½²æ—¶é¢ä¸´çš„è®¡ç®—å¼€é”€é—®é¢˜ï¼Œæå‡ºäº†CLPï¼ˆContinuous Layer Pruningï¼‰æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³ä¼ ç»Ÿå±‚å‰ªæï¼ˆlayer pruningï¼‰å› å¿½ç•¥å±‚é—´ä¾èµ–è€Œå¯¼è‡´çš„ä¿¡æ¯æµä¸­æ–­ã€‚CLPå¼•å…¥äº†å¯å¾®å‡¹é—¨ç®—æ³•ï¼ˆdifferentiable concave gate algorithmï¼‰ï¼Œé€šè¿‡æ¢¯åº¦ä¼˜åŒ–è‡ªåŠ¨è¯†åˆ«å¹¶ç§»é™¤æœ€ä½³çš„è¿ç»­å±‚æ®µã€‚åŒæ—¶ï¼Œè¯¥æ¡†æ¶é‡‡ç”¨åˆ‡æ–­ç«¯ç‚¹å¾®è°ƒç­–ç•¥ï¼ˆcutoff endpoint tuning strategyï¼‰ï¼Œä»…å¯¹å‰ªæéƒ¨åˆ†ç›¸é‚»çš„å±‚è¿›è¡Œå¾®è°ƒä»¥å¿«é€Ÿæ¢å¤æ€§èƒ½ã€‚åœ¨LLaMA2ã€LLaMA3åŠQwenç­‰å¤šç§æ¨¡å‹ä¸Šçš„å®éªŒè¯æ˜ï¼ŒCLPåœ¨å‹ç¼©æ•ˆç‡å’Œæ€§èƒ½ä¿æŒæ–¹é¢å‡ä¼˜äºç°æœ‰åŸºå‡†ï¼Œä¾‹å¦‚åœ¨LLaMA3-70Bä¸Šå®ç°20%å‰ªæç‡æ—¶ä¿ç•™äº†95.34%çš„æ€§èƒ½ã€‚æ­¤å¤–ï¼ŒCLPå¯ä¸é‡åŒ–ï¼ˆquantizationï¼‰æŠ€æœ¯æ— ç¼ç»“åˆï¼Œä¸ºèµ„æºå—é™ç¯å¢ƒä¸‹çš„æ¨¡å‹å‹ç¼©æä¾›äº†é«˜æ•ˆçš„è§£å†³æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.23652v1",
      "published_date": "2025-10-25 16:40:17 UTC",
      "updated_date": "2025-10-25 16:40:17 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T05:36:40.632482+00:00"
    },
    {
      "arxiv_id": "2510.22344v1",
      "title": "FAIR-RAG: Faithful Adaptive Iterative Refinement for Retrieval-Augmented Generation",
      "title_zh": "FAIR-RAGï¼šé¢å‘æ£€ç´¢å¢å¼ºç”Ÿæˆçš„å¿ å®è‡ªé€‚åº”è¿­ä»£ä¼˜åŒ–",
      "authors": [
        "Mohammad Aghajani Asl",
        "Majid Asgari-Bidhendi",
        "Behrooz Minaei-Bidgoli"
      ],
      "abstract": "While Retrieval-Augmented Generation (RAG) mitigates hallucination and knowledge staleness in Large Language Models (LLMs), existing frameworks often falter on complex, multi-hop queries that require synthesizing information from disparate sources. Current advanced RAG methods, employing iterative or adaptive strategies, lack a robust mechanism to systematically identify and fill evidence gaps, often propagating noise or failing to gather a comprehensive context. We introduce FAIR-RAG, a novel agentic framework that transforms the standard RAG pipeline into a dynamic, evidence-driven reasoning process. At its core is an Iterative Refinement Cycle governed by a module we term Structured Evidence Assessment (SEA). The SEA acts as an analytical gating mechanism: it deconstructs the initial query into a checklist of required findings and audits the aggregated evidence to identify confirmed facts and, critically, explicit informational gaps. These gaps provide a precise signal to an Adaptive Query Refinement agent, which generates new, targeted sub-queries to retrieve missing information. This cycle repeats until the evidence is verified as sufficient, ensuring a comprehensive context for a final, strictly faithful generation. We conducted experiments on challenging multi-hop QA benchmarks, including HotpotQA, 2WikiMultiHopQA, and MusiQue. In a unified experimental setup, FAIR-RAG significantly outperforms strong baselines. On HotpotQA, it achieves an F1-score of 0.453 -- an absolute improvement of 8.3 points over the strongest iterative baseline -- establishing a new state-of-the-art for this class of methods on these benchmarks. Our work demonstrates that a structured, evidence-driven refinement process with explicit gap analysis is crucial for unlocking reliable and accurate reasoning in advanced RAG systems for complex, knowledge-intensive tasks.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† FAIR-RAGï¼Œä¸€ç§æ—¨åœ¨è§£å†³æ£€ç´¢å¢å¼ºç”Ÿæˆ(RAG)åœ¨å¤æ‚å¤šè·³æŸ¥è¯¢(multi-hop queries)ä¸­è¯æ®ç¼ºå¤±å’Œå™ªå£°ä¼ æ’­é—®é¢˜çš„ä»£ç†æ¡†æ¶ã€‚å…¶æ ¸å¿ƒæ˜¯å—ç»“æ„åŒ–è¯æ®è¯„ä¼°(SEA)æ¨¡å—æ§åˆ¶çš„è¿­ä»£ç²¾ç‚¼å¾ªç¯(Iterative Refinement Cycle)ï¼Œè¯¥æ¨¡å—é€šè¿‡å°†åˆå§‹æŸ¥è¯¢åˆ†è§£ä¸ºéœ€æ±‚æ¸…å•æ¥å®¡è®¡å·²èšåˆçš„è¯æ®ï¼Œå¹¶è¯†åˆ«æ˜¾å¼çš„ä¿¡æ¯ç¼ºå£ã€‚é’ˆå¯¹è¿™äº›ç¼ºå£ï¼Œè‡ªé€‚åº”æŸ¥è¯¢ç²¾ç‚¼ä»£ç†(Adaptive Query Refinement agent)ä¼šç”Ÿæˆç²¾å‡†çš„å­æŸ¥è¯¢ä»¥æ£€ç´¢ç¼ºå¤±ä¿¡æ¯ï¼Œç›´è‡³è¯æ®è¢«éªŒè¯ä¸ºå……åˆ†ä»¥ç¡®ä¿æœ€ç»ˆç”Ÿæˆçš„å¿ å®åº¦ã€‚å®éªŒåœ¨ HotpotQAã€2WikiMultiHopQA å’Œ MusiQue ç­‰å¤šè·³é—®ç­”åŸºå‡†æµ‹è¯•ä¸Šå±•å¼€ï¼ŒFAIR-RAG çš„è¡¨ç°æ˜¾è‘—ä¼˜äºç°æœ‰å¼ºåŸºçº¿æ¨¡å‹ã€‚ç‰¹åˆ«æ˜¯åœ¨ HotpotQA ä¸Šï¼Œè¯¥æ¡†æ¶å–å¾—äº† 0.453 çš„ F1-scoreï¼Œæ¯”æœ€å¼ºçš„è¿­ä»£åŸºçº¿ç»å¯¹æå‡äº† 8.3 ä¸ªç™¾åˆ†ç‚¹ï¼Œåˆ›ä¸‹äº†æ­¤ç±»æ–¹æ³•çš„æ–° SOTA è®°å½•ã€‚è¿™é¡¹å·¥ä½œè¯æ˜äº†ç»“æ„åŒ–ã€è¯æ®é©±åŠ¨çš„ç²¾ç‚¼è¿‡ç¨‹å¯¹äºè§£é”é«˜çº§ RAG ç³»ç»Ÿåœ¨çŸ¥è¯†å¯†é›†å‹ä»»åŠ¡ä¸­çš„å¯é æ¨ç†è‡³å…³é‡è¦ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.IR"
      ],
      "primary_category": "cs.CL",
      "comment": "30 pages, 5 figures, 5 tables. Keywords: Retrieval-Augmented Generation (RAG), Large Language Models (LLMs), Agentic AI, Multi-hop Question Answering, Faithfulness",
      "pdf_url": "https://arxiv.org/pdf/2510.22344v1",
      "published_date": "2025-10-25 15:59:33 UTC",
      "updated_date": "2025-10-25 15:59:33 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T05:36:42.090212+00:00"
    },
    {
      "arxiv_id": "2510.22340v2",
      "title": "DynaSolidGeo: A Dynamic Benchmark for Genuine Spatial Mathematical Reasoning of VLMs in Solid Geometry",
      "title_zh": "DynaSolidGeoï¼šç«‹ä½“å‡ ä½•ä¸­è§†è§‰è¯­è¨€æ¨¡å‹çœŸå®ç©ºé—´æ•°å­¦æ¨ç†çš„åŠ¨æ€åŸºå‡†",
      "authors": [
        "Changti Wu",
        "Shijie Lian",
        "Zihao Liu",
        "Lei Zhang",
        "Laurence Tianruo Yang",
        "Kai Chen"
      ],
      "abstract": "Solid geometry problem solving demands spatial mathematical reasoning that integrates spatial intelligence and symbolic reasoning. However, most existing multimodal mathematical reasoning benchmarks focus primarily on 2D plane geometry, rely on static datasets prone to data contamination and memorization, and evaluate models solely by final answers, overlooking the reasoning process. To address these limitations, we introduce DynaSolidGeo, the first dynamic benchmark for evaluating genuine spatial reasoning in Vision-Language Models (VLMs). Constructed through a semi-automatic annotation pipeline, DynaSolidGeo contains 503 expert-curated seed questions that can, in principle, dynamically generate an unbounded number of diverse multimodal text-visual instances. Beyond answer accuracy, we incorporate process evaluation based on expert-annotated reasoning chains to measure logical validity and causal coherence. Experiments across representative open-source and closed-source VLMs reveal large performance gaps, severe degradation in dynamic settings, and poor performance on tasks requiring high-level spatial intelligence, such as mental rotation and visualization. The code and dataset are available at \\href{https://zgca-ai4edu.github.io/DynaSolidGeo/}{DynaSolidGeo}.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†DynaSolidGeoï¼Œè¿™æ˜¯é¦–ä¸ªé’ˆå¯¹è§†è§‰è¯­è¨€æ¨¡å‹(VLMs)åœ¨ç«‹ä½“å‡ ä½•(Solid Geometry)é¢†åŸŸçœŸå®ç©ºé—´æ¨ç†èƒ½åŠ›çš„åŠ¨æ€è¯„ä¼°åŸºå‡†ï¼Œæ—¨åœ¨è§£å†³ç°æœ‰åŸºå‡†å¤šé›†ä¸­äºå¹³é¢å‡ ä½•ä¸”æ˜“å—æ•°æ®æ±¡æŸ“å’Œè®°å¿†æ•ˆåº”å½±å“çš„é—®é¢˜ã€‚é€šè¿‡åŠè‡ªåŠ¨æ ‡æ³¨æµç¨‹ï¼Œè¯¥åŸºå‡†åŒ…å«äº†503ä¸ªä¸“å®¶ç­–åˆ’çš„ç§å­é—®é¢˜ï¼Œç†è®ºä¸Šå¯ä»¥åŠ¨æ€ç”Ÿæˆæ— é™æ•°é‡çš„å¤šæ¨¡æ€æ–‡æœ¬-è§†è§‰å®ä¾‹ã€‚é™¤äº†ä¼ ç»Ÿçš„ç­”æ¡ˆå‡†ç¡®ç‡å¤–ï¼ŒDynaSolidGeoè¿˜å¼•å…¥äº†åŸºäºä¸“å®¶æ ‡æ³¨æ¨ç†é“¾çš„è¿‡ç¨‹è¯„ä¼°(Process Evaluation)ï¼Œç”¨äºè¡¡é‡é€»è¾‘æœ‰æ•ˆæ€§å’Œå› æœè¿è´¯æ€§ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œä»£è¡¨æ€§çš„å¼€æºå’Œé—­æºVLMsä¹‹é—´å­˜åœ¨æ˜¾è‘—æ€§èƒ½å·®è·ï¼Œä¸”åœ¨åŠ¨æ€ç¯å¢ƒä¸‹è¡¨ç°å‡ºä¸¥é‡çš„æ€§èƒ½é€€åŒ–ã€‚ç‰¹åˆ«æ˜¯åœ¨éœ€è¦é«˜çº§ç©ºé—´æ™ºèƒ½çš„ä»»åŠ¡ï¼ˆå¦‚å¿ƒç†æ—‹è½¬(Mental Rotation)å’Œå¯è§†åŒ–(Visualization)ï¼‰ä¸Šï¼Œç°æœ‰æ¨¡å‹è¡¨ç°æ™®éè¾ƒå·®ã€‚è¯¥åŸºå‡†ä¸ºè¯„ä¼°å’Œæå‡å¤šæ¨¡æ€æ¨¡å‹çš„ç©ºé—´æ•°å­¦æ¨ç†èƒ½åŠ›æä¾›äº†æ›´å…·æŒ‘æˆ˜æ€§ä¸”ç§‘å­¦çš„è¯„ä»·ä½“ç³»ã€‚",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.CV",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "The code and dataset are available at \\href{https://zgca-ai4edu.github.io/DynaSolidGeo/}{DynaSolidGeo}",
      "pdf_url": "https://arxiv.org/pdf/2510.22340v2",
      "published_date": "2025-10-25 15:49:45 UTC",
      "updated_date": "2025-11-11 15:16:22 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T05:36:45.469182+00:00"
    },
    {
      "arxiv_id": "2510.22336v2",
      "title": "Toward Humanoid Brain-Body Co-design: Joint Optimization of Control and Morphology for Fall Recovery",
      "title_zh": "è¿ˆå‘äººå½¢æœºå™¨äººè„‘ä½“ååŒè®¾è®¡ï¼šé¢å‘è·Œå€’æ¢å¤çš„æ§åˆ¶ä¸å½¢æ€è”åˆä¼˜åŒ–",
      "authors": [
        "Bo Yue",
        "Sheng Xu",
        "Kui Jia",
        "Guiliang Liu"
      ],
      "abstract": "Humanoid robots represent a central frontier in embodied intelligence, as their anthropomorphic form enables natural deployment in humans' workspace. Brain-body co-design for humanoids presents a promising approach to realizing this potential by jointly optimizing control policies and physical morphology. Within this context, fall recovery emerges as a critical capability. It not only enhances safety and resilience but also integrates naturally with locomotion systems, thereby advancing the autonomy of humanoids. In this paper, we propose RoboCraft, a scalable humanoid co-design framework for fall recovery that iteratively improves performance through the coupled updates of control policy and morphology. A shared policy pretrained across multiple designs is progressively finetuned on high-performing morphologies, enabling efficient adaptation without retraining from scratch. Concurrently, morphology search is guided by human-inspired priors and optimization algorithms, supported by a priority buffer that balances reevaluation of promising candidates with the exploration of novel designs. Experiments show that RoboCraft achieves an average performance gain of 44.55% on seven public humanoid robots, with morphology optimization drives at least 40% of improvements in co-designing four humanoid robots, underscoring the critical role of humanoid co-design.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† RoboCraftï¼Œä¸€ä¸ªä¸“ä¸ºäººå½¢æœºå™¨äººè·Œå€’æ¢å¤(Fall Recovery)è®¾è®¡çš„å¯æ‰©å±•è„‘ä½“ååŒè®¾è®¡(Brain-Body Co-design)æ¡†æ¶ï¼Œæ—¨åœ¨é€šè¿‡è”åˆä¼˜åŒ–æ§åˆ¶ç­–ç•¥(Control Policy)ä¸ç‰©ç†å½¢æ€(Morphology)æ¥æå‡æœºå™¨äººçš„è‡ªä¸»æ€§ã€‚è¯¥æ¡†æ¶é‡‡ç”¨åœ¨å¤šç§è®¾è®¡ä¸Šé¢„è®­ç»ƒçš„å…±äº«ç­–ç•¥ï¼Œå¹¶é’ˆå¯¹é«˜æ€§èƒ½å½¢æ€è¿›è¡Œæ¸è¿›å¼å¾®è°ƒï¼Œç¡®ä¿äº†æ— éœ€ä»å¤´è®­ç»ƒçš„é«˜æ•ˆé€‚é…ã€‚å½¢æ€æœç´¢è¿‡ç¨‹ç”±äººç±»å¯å‘å…ˆéªŒ(Human-inspired Priors)å’Œä¼˜åŒ–ç®—æ³•å¼•å¯¼ï¼Œå¹¶åˆ©ç”¨ä¼˜å…ˆçº§ç¼“å†²å™¨(Priority Buffer)å¹³è¡¡å¯¹æ½œåŠ›å€™é€‰æ–¹æ¡ˆçš„è¯„ä¼°ä¸æ–°é¢–è®¾è®¡çš„æ¢ç´¢ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒRoboCraft åœ¨ä¸ƒç§å…¬å¼€çš„äººå½¢æœºå™¨äººä¸Šå®ç°äº†å¹³å‡ 44.55% çš„æ€§èƒ½å¢ç›Šã€‚åœ¨å…¶ä¸­å››ä¸ªæœºå™¨äººçš„ååŒè®¾è®¡ä¸­ï¼Œå½¢æ€ä¼˜åŒ–è´¡çŒ®äº†è‡³å°‘ 40% çš„æ”¹è¿›ï¼Œè¿™æœ‰åŠ›åœ°è¯æ˜äº†äººå½¢æœºå™¨äººè„‘ä½“ååŒè®¾è®¡åœ¨å¢å¼ºå¤æ‚ä»»åŠ¡æ‰§è¡Œèƒ½åŠ›æ–¹é¢çš„å…³é”®ä½œç”¨ã€‚",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.22336v2",
      "published_date": "2025-10-25 15:40:18 UTC",
      "updated_date": "2025-11-05 15:01:29 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T05:37:03.671555+00:00"
    },
    {
      "arxiv_id": "2510.22335v1",
      "title": "Moving Beyond Diffusion: Hierarchy-to-Hierarchy Autoregression for fMRI-to-Image Reconstruction",
      "title_zh": "è¶…è¶Šæ‰©æ•£æ¨¡å‹ï¼šç”¨äº fMRI å›¾åƒé‡å»ºçš„å±‚çº§å¯¹å±‚çº§è‡ªå›å½’æ–¹æ³•",
      "authors": [
        "Xu Zhang",
        "Ruijie Quan",
        "Wenguan Wang",
        "Yi Yang"
      ],
      "abstract": "Reconstructing visual stimuli from fMRI signals is a central challenge bridging machine learning and neuroscience. Recent diffusion-based methods typically map fMRI activity to a single high-level embedding, using it as fixed guidance throughout the entire generation process. However, this fixed guidance collapses hierarchical neural information and is misaligned with the stage-dependent demands of image reconstruction. In response, we propose MindHier, a coarse-to-fine fMRI-to-image reconstruction framework built on scale-wise autoregressive modeling. MindHier introduces three components: a Hierarchical fMRI Encoder to extract multi-level neural embeddings, a Hierarchy-to-Hierarchy Alignment scheme to enforce layer-wise correspondence with CLIP features, and a Scale-Aware Coarse-to-Fine Neural Guidance strategy to inject these embeddings into autoregression at matching scales. These designs make MindHier an efficient and cognitively-aligned alternative to diffusion-based methods by enabling a hierarchical reconstruction process that synthesizes global semantics before refining local details, akin to human visual perception. Extensive experiments on the NSD dataset show that MindHier achieves superior semantic fidelity, 4.67x faster inference, and more deterministic results than the diffusion-based baselines.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹åŠŸèƒ½ç£å…±æŒ¯æˆåƒ(fMRI)åˆ°å›¾åƒé‡å»ºä»»åŠ¡ï¼ŒæŒ‡å‡ºç›®å‰çš„æ‰©æ•£æ¨¡å‹(Diffusion)æ–¹æ³•é€šå¸¸å°† fMRI ä¿¡å·æ˜ å°„ä¸ºå•ä¸€é«˜å±‚åµŒå…¥ï¼Œå¯¼è‡´åˆ†å±‚ç¥ç»ä¿¡æ¯åå¡Œä¸”ä¸å›¾åƒé‡å»ºä¸åŒé˜¶æ®µçš„éœ€æ±‚ä¸åŒ¹é…ã€‚ä¸ºæ­¤ï¼Œä½œè€…æå‡ºäº† MindHierï¼Œä¸€ç§åŸºäºå°ºåº¦è‡ªå›å½’å»ºæ¨¡(scale-wise autoregressive modeling)çš„ä»ç²—åˆ°ç»†çš„é‡å»ºæ¡†æ¶ã€‚è¯¥æ¡†æ¶å¼•å…¥äº†åˆ†å±‚ fMRI ç¼–ç å™¨(Hierarchical fMRI Encoder)æ¥æå–å¤šçº§ç¥ç»åµŒå…¥ï¼Œå¹¶é€šè¿‡å±‚çº§å¯¹é½æ–¹æ¡ˆ(Hierarchy-to-Hierarchy Alignment)å¼ºåˆ¶å…¶ä¸ CLIP ç‰¹å¾å»ºç«‹å±‚çº§å¯¹åº”å…³ç³»ã€‚åˆ©ç”¨å°ºåº¦æ„ŸçŸ¥å¼•å¯¼ç­–ç•¥(Scale-Aware Coarse-to-Fine Neural Guidance)ï¼ŒMindHier èƒ½å¤Ÿå…ˆåˆæˆå…¨å±€è¯­ä¹‰å†ç»†åŒ–å±€éƒ¨ç»†èŠ‚ï¼Œä½¿é‡å»ºè¿‡ç¨‹æ›´ç¬¦åˆäººç±»è§†è§‰æ„ŸçŸ¥çš„è®¤çŸ¥é€»è¾‘ã€‚åœ¨ NSD æ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼ŒMindHier åœ¨è¯­ä¹‰å¿ å®åº¦ä¸Šä¼˜äºæ‰©æ•£æ¨¡å‹åŸºçº¿ï¼Œæ¨ç†é€Ÿåº¦æå‡äº† 4.67 å€ï¼Œä¸”ç”Ÿæˆçš„å›¾åƒç»“æœå…·æœ‰æ›´é«˜çš„ç¡®å®šæ€§ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.22335v1",
      "published_date": "2025-10-25 15:40:07 UTC",
      "updated_date": "2025-10-25 15:40:07 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T05:37:03.476714+00:00"
    },
    {
      "arxiv_id": "2510.22334v1",
      "title": "Multilingual Target-Stance Extraction",
      "title_zh": "å¤šè¯­è¨€ç›®æ ‡-ç«‹åœºæŠ½å–",
      "authors": [
        "Ethan Mines",
        "Bonnie Dorr"
      ],
      "abstract": "Social media enables data-driven analysis of public opinion on contested issues. Target-Stance Extraction (TSE) is the task of identifying the target discussed in a document and the document's stance towards that target. Many works classify stance towards a given target in a multilingual setting, but all prior work in TSE is English-only. This work introduces the first multilingual TSE benchmark, spanning Catalan, Estonian, French, Italian, Mandarin, and Spanish corpora. It manages to extend the original TSE pipeline to a multilingual setting without requiring separate models for each language. Our model pipeline achieves a modest F1 score of 12.78, underscoring the increased difficulty of the multilingual task relative to English-only setups and highlighting target prediction as the primary bottleneck. We are also the first to demonstrate the sensitivity of TSE's F1 score to different target verbalizations. Together these serve as a much-needed baseline for resources, algorithms, and evaluation criteria in multilingual TSE.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†å¤šè¯­è¨€ç›®æ ‡-ç«‹åœºæå–(Target-Stance Extraction, TSE)ä»»åŠ¡ï¼Œæ—¨åœ¨è¯†åˆ«æ–‡æ¡£ä¸­è®¨è®ºçš„ç›®æ ‡åŠå…¶å¯¹åº”çš„ç«‹åœºã€‚é’ˆå¯¹ä»¥å¾€TSEç ”ç©¶ä¸»è¦å±€é™äºè‹±è¯­çš„ç°çŠ¶ï¼Œæœ¬æ–‡æ¨å‡ºäº†é¦–ä¸ªæ¶µç›–åŠ æ³°ç½—å°¼äºšè¯­ã€çˆ±æ²™å°¼äºšè¯­ã€æ³•è¯­ã€æ„å¤§åˆ©è¯­ã€æ™®é€šè¯å’Œè¥¿ç­ç‰™è¯­çš„å¤šè¯­è¨€TSEåŸºå‡†ã€‚ç ”ç©¶å›¢é˜ŸæˆåŠŸå°†åŸæœ‰çš„TSEæµæ°´çº¿æ‰©å±•è‡³å¤šè¯­è¨€ç¯å¢ƒï¼Œä¸”æ— éœ€ä¸ºæ¯ç§è¯­è¨€æ„å»ºç‹¬ç«‹æ¨¡å‹ã€‚å®éªŒè¡¨æ˜è¯¥æ¨¡å‹è¾¾åˆ°äº†12.78çš„F1åˆ†æ•°ï¼Œå‡¸æ˜¾äº†å¤šè¯­è¨€ä»»åŠ¡ç›¸è¾ƒäºå•è¯­ä»»åŠ¡çš„æé«˜éš¾åº¦ï¼Œå¹¶ç¡®è®¤ç›®æ ‡é¢„æµ‹(Target Prediction)æ˜¯æ ¸å¿ƒç“¶é¢ˆã€‚æ­¤å¤–ï¼Œè¯¥ç ”ç©¶é¦–æ¬¡è¯æ˜äº†TSEæ€§èƒ½å¯¹ä¸åŒç›®æ ‡è¯­è¨€è¡¨è¿°(Target Verbalizations)å…·æœ‰æ•æ„Ÿæ€§ã€‚è¿™é¡¹å·¥ä½œä¸ºå¤šè¯­è¨€TSEé¢†åŸŸçš„èµ„æºæ„å»ºã€ç®—æ³•å¼€å‘å’Œè¯„ä¼°å‡†åˆ™æä¾›äº†é‡è¦çš„åŸºå‡†å‚è€ƒã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "11 pages, 2 figures, Submitted to the Fifteenth Language Resources and Evaluation Conference (LREC 2026)",
      "pdf_url": "https://arxiv.org/pdf/2510.22334v1",
      "published_date": "2025-10-25 15:38:15 UTC",
      "updated_date": "2025-10-25 15:38:15 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T05:37:04.773634+00:00"
    },
    {
      "arxiv_id": "2510.22333v1",
      "title": "LIFT: Interpretable truck driving risk prediction with literature-informed fine-tuned LLMs",
      "title_zh": "LIFTï¼šåŸºäºæ–‡çŒ®å¯å‘å¾®è°ƒå¤§è¯­è¨€æ¨¡å‹çš„å¯è§£é‡Šè´§è½¦é©¾é©¶é£é™©é¢„æµ‹",
      "authors": [
        "Xiao Hu",
        "Yuansheng Lian",
        "Ke Zhang",
        "Yunxuan Li",
        "Yuelong Su",
        "Meng Li"
      ],
      "abstract": "This study proposes an interpretable prediction framework with literature-informed fine-tuned (LIFT) LLMs for truck driving risk prediction. The framework integrates an LLM-driven Inference Core that predicts and explains truck driving risk, a Literature Processing Pipeline that filters and summarizes domain-specific literature into a literature knowledge base, and a Result Evaluator that evaluates the prediction performance as well as the interpretability of the LIFT LLM. After fine-tuning on a real-world truck driving risk dataset, the LIFT LLM achieved accurate risk prediction, outperforming benchmark models by 26.7% in recall and 10.1% in F1-score. Furthermore, guided by the literature knowledge base automatically constructed from 299 domain papers, the LIFT LLM produced variable importance ranking consistent with that derived from the benchmark model, while demonstrating robustness in interpretation results to various data sampling conditions. The LIFT LLM also identified potential risky scenarios by detecting key combination of variables in truck driving risk, which were verified by PERMANOVA tests. Finally, we demonstrated the contribution of the literature knowledge base and the fine-tuning process in the interpretability of the LIFT LLM, and discussed the potential of the LIFT LLM in data-driven knowledge discovery.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† LIFT æ¡†æ¶ï¼Œè¿™æ˜¯ä¸€ç§åˆ©ç”¨æ–‡çŒ®ä¿¡æ¯å¢å¼ºçš„å¾®è°ƒå¤§è¯­è¨€æ¨¡å‹ (LLMs) è¿›è¡Œå¯è§£é‡Šæ€§å¡è½¦é©¾é©¶é£é™©é¢„æµ‹çš„æ–¹æ³•ã€‚è¯¥æ¡†æ¶æ•´åˆäº†é©±åŠ¨é£é™©é¢„æµ‹ä¸è§£é‡Šçš„ LLM æ¨ç†æ ¸å¿ƒ (Inference Core)ã€ä» 299 ç¯‡é¢†åŸŸæ–‡çŒ®ä¸­è‡ªåŠ¨æ„å»ºçŸ¥è¯†åº“çš„æ–‡çŒ®å¤„ç†æµæ°´çº¿ (Literature Processing Pipeline) ä»¥åŠç»“æœè¯„ä¼°å™¨ (Result Evaluator)ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œå¾®è°ƒåçš„ LIFT åœ¨çœŸå®å¡è½¦é©¾é©¶é£é™©æ•°æ®é›†ä¸Šçš„è¡¨ç°ä¼˜äºåŸºå‡†æ¨¡å‹ï¼Œå…¶å¬å›ç‡ (recall) å’Œ F1 åˆ†æ•°åˆ†åˆ«æå‡äº† 26.7% å’Œ 10.1%ã€‚åŒæ—¶ï¼ŒLIFT äº§å‡ºçš„å˜é‡é‡è¦æ€§æ’åºä¸ä¼ ç»ŸåŸºå‡†æ¨¡å‹å…·æœ‰é«˜åº¦ä¸€è‡´æ€§ï¼Œå¹¶åœ¨å¤šç§æ•°æ®é‡‡æ ·æ¡ä»¶ä¸‹è¡¨ç°å‡ºæå¼ºçš„è§£é‡Šé²æ£’æ€§ã€‚é€šè¿‡æ£€æµ‹å…³é”®å˜é‡ç»„åˆï¼Œæ¨¡å‹è¿˜æˆåŠŸè¯†åˆ«äº†ç» PERMANOVA æ£€éªŒéªŒè¯çš„æ½œåœ¨é£é™©åœºæ™¯ã€‚è¯¥ç ”ç©¶æœ€åè¯å®äº†æ–‡çŒ®çŸ¥è¯†åº“å’Œå¾®è°ƒè¿‡ç¨‹åœ¨æå‡æ¨¡å‹å¯è§£é‡Šæ€§ä¸­çš„å…³é”®ä½œç”¨ï¼Œå¹¶å±•ç¤ºäº† LLMs åœ¨æ•°æ®é©±åŠ¨çŸ¥è¯†å‘ç°é¢†åŸŸçš„åº”ç”¨æ½œåŠ›ã€‚",
      "categories": [
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.22333v1",
      "published_date": "2025-10-25 15:37:56 UTC",
      "updated_date": "2025-10-25 15:37:56 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T05:37:12.671457+00:00"
    },
    {
      "arxiv_id": "2510.22329v1",
      "title": "Graph-Coarsening Approach for the Capacitated Vehicle Routing Problem with Time Windows",
      "title_zh": "æ±‚è§£å¸¦æ—¶é—´çª—çš„å®¹é‡å—é™è½¦è¾†è·¯å¾„é—®é¢˜çš„å›¾ç²—åŒ–æ–¹æ³•",
      "authors": [
        "Mustafa Mert Ã–zyÄ±lmaz"
      ],
      "abstract": "The Capacitated Vehicle Routing Problem with Time Windows (CVRPTW) is a fundamental NP-hard optimization problem in logistics. Solving large-scale instances remains computationally challenging for exact solvers. This work introduces a multilevel graph coarsening and refinement framework that aggregates customers into meta-nodes using a spatio-temporal distance metric. The reduced problem is solved with classical heuristics and subsequently expanded back into the original space with feasibility corrections. Preliminary experiments on Solomon benchmark instances show that the proposed method reduces computation time while preserving or improving solution quality, particularly with respect to capacity and time window constraints. The paper also explores the integration of quantum-inspired optimization techniques, highlighting their potential to further accelerate large-scale vehicle routing tasks.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ç‰©æµé¢†åŸŸä¸­å…·æœ‰æŒ‘æˆ˜æ€§çš„ Capacitated Vehicle Routing Problem with Time Windows (CVRPTW) é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§åŸºäº multilevel graph coarsening å’Œç²¾ç‚¼çš„æ¡†æ¶ã€‚è¯¥æ–¹æ³•åˆ©ç”¨ä¸€ç§ spatio-temporal distance metric å°†å®¢æˆ·èšåˆä¸º meta-nodesï¼Œé€šè¿‡ç®€åŒ–é—®é¢˜è§„æ¨¡æ¥æå‡å¤§è§„æ¨¡å®ä¾‹çš„æ±‚è§£æ•ˆç‡ã€‚é™ç»´åçš„é—®é¢˜ç”±ç»å…¸çš„ heuristics æ±‚è§£ï¼Œå¹¶éšåé€šè¿‡å¯è¡Œæ€§ä¿®æ­£æ˜ å°„å›åŸå§‹ç©ºé—´ä»¥ç¡®ä¿è§£çš„æœ‰æ•ˆæ€§ã€‚åœ¨ Solomon benchmark å®ä¾‹ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨æ˜¾è‘—ç¼©çŸ­è®¡ç®—æ—¶é—´çš„åŒæ—¶ï¼Œèƒ½å¤Ÿä¿æŒæˆ–ä¼˜åŒ–è§£çš„è´¨é‡ï¼Œå°¤å…¶åœ¨å¤„ç†å®¹é‡å’Œæ—¶é—´çª—çº¦æŸæ—¶è¡¨ç°ç¨³å¥ã€‚æ­¤å¤–ï¼Œè®ºæ–‡è¿˜æ¢è®¨äº†é›†æˆ quantum-inspired optimization æŠ€æœ¯çš„å¯èƒ½æ€§ï¼Œä¸ºè¿›ä¸€æ­¥åŠ é€Ÿå¤§è§„æ¨¡è½¦è¾†è·¯å¾„ä¼˜åŒ–ä»»åŠ¡æä¾›äº†æ–°æ€è·¯ã€‚",
      "categories": [
        "cs.AI",
        "math.OC"
      ],
      "primary_category": "cs.AI",
      "comment": "13 pages, 30 figures. Submitted to arXiv under categories quant-ph. A revised version with quantum solver experiment results will be submitted to a peer-reviewed journal",
      "pdf_url": "https://arxiv.org/pdf/2510.22329v1",
      "published_date": "2025-10-25 15:13:41 UTC",
      "updated_date": "2025-10-25 15:13:41 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T05:37:14.066924+00:00"
    },
    {
      "arxiv_id": "2510.22318v1",
      "title": "Harnessing the Power of Large Language Models for Software Testing Education: A Focus on ISTQB Syllabus",
      "title_zh": "åˆ©ç”¨å¤§è¯­è¨€æ¨¡å‹èµ‹èƒ½è½¯ä»¶æµ‹è¯•æ•™è‚²ï¼šèšç„¦ ISTQB å¤§çº²",
      "authors": [
        "Tuan-Phong Ngo",
        "Bao-Ngoc Duong",
        "Tuan-Anh Hoang",
        "Joshua Dwight",
        "Ushik Shrestha Khwakhali"
      ],
      "abstract": "Software testing is a critical component in the software engineering field and is important for software engineering education. Thus, it is vital for academia to continuously improve and update educational methods to reflect the current state of the field. The International Software Testing Qualifications Board (ISTQB) certification framework is globally recognized and widely adopted in industry and academia. However, ISTQB-based learning has been rarely applied with recent generative artificial intelligence advances. Despite the growing capabilities of large language models (LLMs), ISTQB-based learning and instruction with LLMs have not been thoroughly explored. This paper explores and evaluates how LLMs can complement the ISTQB framework for higher education. The findings present four key contributions: (i) the creation of a comprehensive ISTQB-aligned dataset spanning over a decade, consisting of 28 sample exams and 1,145 questions; (ii) the development of a domain-optimized prompt that enhances LLM precision and explanation quality on ISTQB tasks; (iii) a systematic evaluation of state-of-the-art LLMs on this dataset; and (iv) actionable insights and recommendations for integrating LLMs into software testing education. These findings highlight the promise of LLMs in supporting ISTQB certification preparation and offer a foundation for their broader use in software engineering at higher education.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†å¦‚ä½•åˆ©ç”¨ Large Language Models (LLMs) æ¥å¢å¼ºè½¯ä»¶æµ‹è¯•æ•™è‚²ï¼Œç‰¹åˆ«æ˜¯é’ˆå¯¹å…¨çƒå¹¿æ³›è®¤å¯çš„ ISTQB è®¤è¯ä½“ç³»ã€‚ä¸ºäº†å¡«è¡¥è¯¥é¢†åŸŸåœ¨ç”Ÿæˆå¼äººå·¥æ™ºèƒ½åº”ç”¨ä¸Šçš„ç©ºç™½ï¼Œç ”ç©¶å›¢é˜Ÿæ„å»ºäº†ä¸€ä¸ªæ¶µç›–åå¹´è·¨åº¦ã€åŒ…å« 28 ä»½æ¨¡æ‹Ÿè€ƒè¯•åŠ 1,145 é“é¢˜ç›®çš„ ISTQB ä¸“ç”¨æ•°æ®é›†ã€‚é€šè¿‡å¼€å‘é¢†åŸŸä¼˜åŒ–çš„ Promptï¼Œç ”ç©¶è¿›ä¸€æ­¥æå‡äº† LLM åœ¨å¤„ç† ISTQB ä»»åŠ¡æ—¶çš„å‡†ç¡®æ€§ä¸è§£é‡Šè´¨é‡ï¼Œå¹¶å¯¹å¤šç§ SOTA æ¨¡å‹è¿›è¡Œäº†ç³»ç»Ÿæ€§è¯„ä¼°ã€‚ç ”ç©¶ç»“æœä¸ºå°† LLMs æ•´åˆåˆ°é«˜ç­‰æ•™è‚²è½¯ä»¶æµ‹è¯•è¯¾ç¨‹ä¸­æä¾›äº†å®è·µå»ºè®®å’Œè¡ŒåŠ¨æŒ‡å—ã€‚è¿™é¡¹å·¥ä½œè¯æ˜äº† LLMs åœ¨è¾…åŠ© ISTQB è®¤è¯å‡†å¤‡æ–¹é¢çš„å·¨å¤§æ½œåŠ›ï¼Œä¸ºè½¯ä»¶å·¥ç¨‹æ•™è‚²çš„æ™ºèƒ½åŒ–å‘å±•å¥ å®šäº†åŸºç¡€ã€‚",
      "categories": [
        "cs.SE",
        "cs.AI"
      ],
      "primary_category": "cs.SE",
      "comment": "7 pages, 3 figures, 3 tables",
      "pdf_url": "https://arxiv.org/pdf/2510.22318v1",
      "published_date": "2025-10-25 14:45:58 UTC",
      "updated_date": "2025-10-25 14:45:58 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T05:37:18.172111+00:00"
    },
    {
      "arxiv_id": "2510.22312v1",
      "title": "LacMaterial: Large Language Models as Analogical Chemists for Materials Discovery",
      "title_zh": "LacMaterialï¼šå¤§è¯­è¨€æ¨¡å‹ä½œä¸ºç±»æ¯”åŒ–å­¦å®¶ç”¨äºææ–™å‘ç°",
      "authors": [
        "Hongyu Guo"
      ],
      "abstract": "Analogical reasoning, the transfer of relational structures across contexts (e.g., planet is to sun as electron is to nucleus), is fundamental to scientific discovery. Yet human insight is often constrained by domain expertise and surface-level biases, limiting access to deeper, structure-driven analogies both within and across disciplines. Large language models (LLMs), trained on vast cross-domain data, present a promising yet underexplored tool for analogical reasoning in science. Here, we demonstrate that LLMs can generate novel battery materials by (1) retrieving cross-domain analogs and analogy-guided exemplars to steer exploration beyond conventional dopant substitutions, and (2) constructing in-domain analogical templates from few labeled examples to guide targeted exploitation. These explicit analogical reasoning strategies yield candidates outside established compositional spaces and outperform standard prompting baselines. Our findings position LLMs as interpretable, expert-like hypothesis generators that leverage analogy-driven generalization for scientific innovation.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† LacMaterial æ¡†æ¶ï¼Œåˆ©ç”¨å¤§è¯­è¨€æ¨¡å‹ (LLMs) ä½œä¸ºç±»æ¯”åŒ–å­¦å®¶ï¼Œæ—¨åœ¨é€šè¿‡ç±»æ¯”æ¨ç† (Analogical reasoning) åŠ é€Ÿæ–°ææ–™çš„å‘ç°ã€‚ä¸ºäº†å…‹æœäººç±»åœ¨è·¨å­¦ç§‘ç±»æ¯”æ—¶å—é™äºä¸“ä¸šçŸ¥è¯†å’Œè¡¨é¢åè§çš„å±€é™ï¼ŒLacMaterial é‡‡ç”¨äº†ä¸¤ç§æ ¸å¿ƒç­–ç•¥ï¼šä¸€æ˜¯æ£€ç´¢è·¨é¢†åŸŸç±»æ¯” (Cross-domain analogs) ä»¥å¼•å¯¼æ¨¡å‹è¶…è¶Šä¼ ç»Ÿçš„æºæ‚æ›¿ä»£ (Dopant substitutions) è¿›è¡Œæ¢ç´¢ï¼›äºŒæ˜¯é€šè¿‡å°‘é‡æ ‡æ³¨ç¤ºä¾‹æ„å»ºåŸŸå†…ç±»æ¯”æ¨¡æ¿ (In-domain analogical templates) ä»¥æŒ‡å¯¼å®šå‘å¼€å‘ã€‚å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨ç”µæ± ææ–™å‘ç°ä¸­èƒ½å¤Ÿç”Ÿæˆè¶…å‡ºå·²çŸ¥æˆåˆ†ç©ºé—´çš„å€™é€‰ææ–™ï¼Œä¸”è¡¨ç°ä¼˜äºæ ‡å‡†çš„æç¤ºè¯ (Prompting) åŸºçº¿ã€‚è¯¥ç ”ç©¶è¯æ˜äº† LLMs å¯ä»¥ä½œä¸ºå…·æœ‰å¯è§£é‡Šæ€§çš„ä¸“å®¶çº§å‡è®¾ç”Ÿæˆå™¨ï¼Œé€šè¿‡ç±»æ¯”é©±åŠ¨çš„æ³›åŒ–èƒ½åŠ›æ˜¾è‘—æ¨åŠ¨ç§‘å­¦åˆ›æ–°ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.22312v1",
      "published_date": "2025-10-25 14:25:26 UTC",
      "updated_date": "2025-10-25 14:25:26 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T05:37:19.273881+00:00"
    },
    {
      "arxiv_id": "2510.22301v1",
      "title": "AnyECG-Lab: An Exploration Study of Fine-tuning an ECG Foundation Model to Estimate Laboratory Values from Single-Lead ECG Signals",
      "title_zh": "AnyECG-Labï¼šåŸºäºå•å¯¼è”å¿ƒç”µå›¾ä¿¡å·å¾®è°ƒå¿ƒç”µå›¾åŸºç¡€æ¨¡å‹ä»¥ä¼°æµ‹å®éªŒå®¤æŒ‡æ ‡çš„æ¢ç´¢æ€§ç ”ç©¶",
      "authors": [
        "Yujie Xiao",
        "Gongzhen Tang",
        "Wenhui Liu",
        "Jun Li",
        "Guangkun Nie",
        "Zhuoran Kan",
        "Deyun Zhang",
        "Qinghao Zhao",
        "Shenda Hong"
      ],
      "abstract": "Timely access to laboratory values is critical for clinical decision-making, yet current approaches rely on invasive venous sampling and are intrinsically delayed. Electrocardiography (ECG), as a non-invasive and widely available signal, offers a promising modality for rapid laboratory estimation. Recent progress in deep learning has enabled the extraction of latent hematological signatures from ECGs. However, existing models are constrained by low signal-to-noise ratios, substantial inter-individual variability, limited data diversity, and suboptimal generalization, especially when adapted to low-lead wearable devices. In this work, we conduct an exploratory study leveraging transfer learning to fine-tune ECGFounder, a large-scale pre-trained ECG foundation model, on the Multimodal Clinical Monitoring in the Emergency Department (MC-MED) dataset from Stanford. We generated a corpus of more than 20 million standardized ten-second ECG segments to enhance sensitivity to subtle biochemical correlates. On internal validation, the model demonstrated strong predictive performance (area under the curve above 0.65) for thirty-three laboratory indicators, moderate performance (between 0.55 and 0.65) for fifty-nine indicators, and limited performance (below 0.55) for sixteen indicators. This study provides an efficient artificial-intelligence driven solution and establishes the feasibility scope for real-time, non-invasive estimation of laboratory values.",
      "tldr_zh": "è¯¥ç ”ç©¶å¼€å±•äº†åä¸º AnyECG-Lab çš„æ¢ç´¢æ€§å·¥ä½œï¼Œæ—¨åœ¨é€šè¿‡å¾®è°ƒå¤§è§„æ¨¡é¢„è®­ç»ƒå¿ƒç”µå›¾åŸºç¡€æ¨¡å‹ ECGFounderï¼Œå®ç°ä»å•å¯¼è” ECG ä¿¡å·ä¸­ä¼°è®¡å®éªŒå®¤æ£€éªŒæŒ‡æ ‡ã€‚é’ˆå¯¹ç›®å‰ä¾µå…¥æ€§é‡‡æ ·å¯¼è‡´çš„ä¸´åºŠå†³ç­–å»¶è¿Ÿä»¥åŠç°æœ‰æ¨¡å‹åœ¨ä¿¡å™ªæ¯”ã€ä¸ªä½“å·®å¼‚å’Œä½å¯¼è”è®¾å¤‡æ³›åŒ–æ€§ä¸Šçš„ä¸è¶³ï¼Œç ”ç©¶è€…åˆ©ç”¨è¿ç§»å­¦ä¹ (transfer learning)åœ¨æ–¯å¦ç¦ MC-MED æ•°æ®é›†ä¸Šè¿›è¡Œäº†å¾®è°ƒã€‚é€šè¿‡ç”Ÿæˆè¶…è¿‡ 2000 ä¸‡ä¸ªæ ‡å‡†åŒ–çš„ 10 ç§’å¿ƒç”µç‰‡æ®µï¼Œæ¨¡å‹æ˜¾è‘—å¢å¼ºäº†å¯¹å¾®å¦™ç”ŸåŒ–å…³è”çš„æ•æ„Ÿåº¦ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯¥æ¨¡å‹åœ¨ 33 é¡¹å®éªŒå®¤æŒ‡æ ‡ä¸Šè¡¨ç°å‡ºå¼ºé¢„æµ‹æ€§èƒ½ï¼ˆAUC é«˜äº 0.65ï¼‰ï¼Œå¹¶åœ¨å¦å¤– 59 é¡¹æŒ‡æ ‡ä¸Šè¾¾åˆ°ä¸­ç­‰æ°´å¹³ã€‚æ­¤é¡¹ç ”ç©¶æä¾›äº†ä¸€ç§é«˜æ•ˆçš„äººå·¥æ™ºèƒ½é©±åŠ¨æ–¹æ¡ˆï¼Œå¹¶ç•Œå®šäº†åˆ©ç”¨æ— åˆ›ä¿¡å·è¿›è¡Œå®æ—¶å®éªŒå®¤æ•°å€¼ä¼°è®¡çš„å¯è¡Œæ€§èŒƒå›´ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.22301v1",
      "published_date": "2025-10-25 14:04:26 UTC",
      "updated_date": "2025-10-25 14:04:26 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T05:37:22.254153+00:00"
    },
    {
      "arxiv_id": "2510.22300v2",
      "title": "T2I-RiskyPrompt: A Benchmark for Safety Evaluation, Attack, and Defense on Text-to-Image Model",
      "title_zh": "T2I-RiskyPromptï¼šæ–‡ç”Ÿå›¾æ¨¡å‹å®‰å…¨è¯„ä¼°ã€æ”»å‡»ä¸é˜²å¾¡åŸºå‡†",
      "authors": [
        "Chenyu Zhang",
        "Tairen Zhang",
        "Lanjun Wang",
        "Ruidong Chen",
        "Wenhui Li",
        "Anan Liu"
      ],
      "abstract": "Using risky text prompts, such as pornography and violent prompts, to test the safety of text-to-image (T2I) models is a critical task. However, existing risky prompt datasets are limited in three key areas: 1) limited risky categories, 2) coarse-grained annotation, and 3) low effectiveness. To address these limitations, we introduce T2I-RiskyPrompt, a comprehensive benchmark designed for evaluating safety-related tasks in T2I models. Specifically, we first develop a hierarchical risk taxonomy, which consists of 6 primary categories and 14 fine-grained subcategories. Building upon this taxonomy, we construct a pipeline to collect and annotate risky prompts. Finally, we obtain 6,432 effective risky prompts, where each prompt is annotated with both hierarchical category labels and detailed risk reasons. Moreover, to facilitate the evaluation, we propose a reason-driven risky image detection method that explicitly aligns the MLLM with safety annotations. Based on T2I-RiskyPrompt, we conduct a comprehensive evaluation of eight T2I models, nine defense methods, five safety filters, and five attack strategies, offering nine key insights into the strengths and limitations of T2I model safety. Finally, we discuss potential applications of T2I-RiskyPrompt across various research fields. The dataset and code are provided in https://github.com/datar001/T2I-RiskyPrompt.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ç°æœ‰é£é™©æç¤ºè¯æ•°æ®é›†åœ¨ç±»åˆ«èŒƒå›´ã€æ ‡æ³¨ç²’åº¦åŠæœ‰æ•ˆæ€§æ–¹é¢çš„å±€é™ï¼Œæå‡ºäº† T2I-RiskyPromptï¼Œä¸€ä¸ªç”¨äºæ–‡ç”Ÿå›¾æ¨¡å‹(Text-to-Image models)å®‰å…¨è¯„ä¼°ã€æ”»å‡»ä¸é˜²å¾¡çš„ç»¼åˆåŸºå‡†ã€‚ç ”ç©¶è€…é¦–å…ˆå¼€å‘äº†åŒ…å«6ä¸ªä¸»ç±»å’Œ14ä¸ªç»†ç²’åº¦å­ç±»çš„åˆ†å±‚é£é™©åˆ†ç±»æ³•(Hierarchical risk taxonomy)ï¼Œå¹¶æ®æ­¤æ„å»ºäº†åŒ…å«6,432ä¸ªæœ‰æ•ˆé£é™©æç¤ºè¯çš„æµæ°´çº¿ï¼Œæ¯ä¸ªæç¤ºè¯å‡é™„æœ‰è¯¦ç»†çš„é£é™©ç†ç”±ã€‚ä¸ºè¾…åŠ©è¯„ä¼°ï¼Œç ”ç©¶æå‡ºäº†ä¸€ç§åŸå› é©±åŠ¨çš„é£é™©å›¾åƒæ£€æµ‹æ–¹æ³•(Reason-driven risky image detection method)ï¼Œé€šè¿‡æ˜¾å¼å¯¹é½å¤šæ¨¡æ€å¤§æ¨¡å‹(MLLM)ä¸å®‰å…¨æ ‡æ³¨æ¥æå‡æ£€æµ‹æ•ˆèƒ½ã€‚é€šè¿‡å¯¹8ä¸ªæ–‡ç”Ÿå›¾æ¨¡å‹ã€9ç§é˜²å¾¡æ–¹æ³•ã€5ç§å®‰å…¨è¿‡æ»¤å™¨åŠ5ç§æ”»å‡»ç­–ç•¥çš„å…¨é¢æµ‹è¯•ï¼Œè¯¥ç ”ç©¶æä¾›äº†å…³äºæ¨¡å‹å®‰å…¨å¼ºé¡¹ä¸å±€é™çš„9é¡¹æ ¸å¿ƒè§è§£ã€‚T2I-RiskyPrompt çš„æå‡ºä¸ºä¸åŒç ”ç©¶é¢†åŸŸçš„æ¨¡å‹å®‰å…¨è¯„ä¼°æä¾›äº†é‡è¦çš„å¼€æºæ•°æ®æ”¯æ’‘ä¸è¯„ä¼°æ¡†æ¶ã€‚",
      "categories": [
        "cs.CR",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.CR",
      "comment": "AAAI 2026",
      "pdf_url": "https://arxiv.org/pdf/2510.22300v2",
      "published_date": "2025-10-25 14:00:26 UTC",
      "updated_date": "2025-11-21 05:17:51 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T05:37:24.473571+00:00"
    },
    {
      "arxiv_id": "2510.22295v2",
      "title": "VietLyrics: A Large-Scale Dataset and Models for Vietnamese Automatic Lyrics Transcription",
      "title_zh": "VietLyricsï¼šé¢å‘è¶Šå—è¯­è‡ªåŠ¨æ­Œè¯è½¬å½•çš„å¤§è§„æ¨¡æ•°æ®é›†ä¸æ¨¡å‹",
      "authors": [
        "Quoc Anh Nguyen",
        "Bernard Cheng",
        "Kelvin Soh"
      ],
      "abstract": "Automatic Lyrics Transcription (ALT) for Vietnamese music presents unique challenges due to its tonal complexity and dialectal variations, but remains largely unexplored due to the lack of a dedicated dataset. Therefore, we curated the first large-scale Vietnamese ALT dataset (VietLyrics), comprising 647 hours of songs with line-level aligned lyrics and metadata to address these issues. Our evaluation of current ASRbased approaches reveal significant limitations, including frequent transcription errors and hallucinations in non-vocal segments. To improve performance, we fine-tuned Whisper models on the VietLyrics dataset, achieving superior results compared to existing multilingual ALT systems, including LyricWhiz. We publicly release VietLyrics and our models, aiming to advance Vietnamese music computing research while demonstrating the potential of this approach for ALT in low-resource language and music.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹è¶Šå—è¯­éŸ³ä¹åœ¨è‡ªåŠ¨æ­Œè¯è½¬å½• (Automatic Lyrics Transcription, ALT) ä¸­é¢ä¸´çš„å£°è°ƒå¤æ‚æ€§å’Œæ–¹è¨€å·®å¼‚ï¼Œä»¥åŠç¼ºä¹ä¸“ç”¨æ•°æ®é›†çš„æŒ‘æˆ˜ï¼Œæ„å»ºäº†é¦–ä¸ªå¤§è§„æ¨¡è¶Šå—è¯­ ALT æ•°æ®é›† VietLyricsã€‚è¯¥æ•°æ®é›†åŒ…å« 647 å°æ—¶çš„æ­Œæ›²ï¼Œå¹¶æä¾›äº†è¡Œçº§å¯¹é½çš„æ­Œè¯åŠç›¸å…³å…ƒæ•°æ®ï¼Œå¡«è¡¥äº†è¯¥é¢†åŸŸçš„ç©ºç™½ã€‚é€šè¿‡è¯„ä¼°å‘ç°ï¼Œç°æœ‰çš„è¯­éŸ³è¯†åˆ« (ASR) æ–¹æ³•åœ¨å¤„ç†è¶Šå—è¯­ ALT æ—¶å­˜åœ¨æ˜¾è‘—å±€é™ï¼Œå°¤å…¶æ˜¯åœ¨éäººå£°ç‰‡æ®µä¸­ç»å¸¸å‡ºç°è½¬å½•é”™è¯¯å’Œå¹»è§‰ (hallucinations)ã€‚ä¸ºäº†æå‡è½¬å½•è´¨é‡ï¼Œç ”ç©¶è€…åœ¨ VietLyrics ä¸Šå¯¹ Whisper æ¨¡å‹è¿›è¡Œäº†å¾®è°ƒï¼Œå…¶è¡¨ç°ä¼˜äºåŒ…æ‹¬ LyricWhiz åœ¨å†…çš„ç°æœ‰ä¸»æµè·¨è¯­è¨€ ALT ç³»ç»Ÿã€‚è¯¥ç ”ç©¶å…¬å¼€å‘å¸ƒäº†æ•°æ®é›†å’Œæ¨¡å‹ï¼Œä¸ä»…æœ‰æ•ˆæ¨åŠ¨äº†è¶Šå—è¯­éŸ³ä¹è®¡ç®—çš„å‘å±•ï¼Œä¹Ÿä¸ºä½èµ„æºè¯­è¨€ (low-resource language) çš„éŸ³ä¹è½¬å½•ç ”ç©¶æä¾›äº†é‡è¦å‚è€ƒã€‚",
      "categories": [
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.22295v2",
      "published_date": "2025-10-25 13:38:20 UTC",
      "updated_date": "2025-12-20 13:08:30 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T05:37:25.171783+00:00"
    },
    {
      "arxiv_id": "2510.24777v1",
      "title": "Cross-Enhanced Multimodal Fusion of Eye-Tracking and Facial Features for Alzheimer's Disease Diagnosis",
      "title_zh": "èåˆçœ¼åŠ¨è¿½è¸ªä¸é¢éƒ¨ç‰¹å¾çš„äº¤å‰å¢å¼ºå¤šæ¨¡æ€é˜¿å°”èŒ¨æµ·é»˜ç—…è¯Šæ–­",
      "authors": [
        "Yujie Nie",
        "Jianzhang Ni",
        "Yonglong Ye",
        "Yuan-Ting Zhang",
        "Yun Kwok Wing",
        "Xiangqing Xu",
        "Xin Ma",
        "Lizhou Fan"
      ],
      "abstract": "Accurate diagnosis of Alzheimer's disease (AD) is essential for enabling timely intervention and slowing disease progression. Multimodal diagnostic approaches offer considerable promise by integrating complementary information across behavioral and perceptual domains. Eye-tracking and facial features, in particular, are important indicators of cognitive function, reflecting attentional distribution and neurocognitive state. However, few studies have explored their joint integration for auxiliary AD diagnosis. In this study, we propose a multimodal cross-enhanced fusion framework that synergistically leverages eye-tracking and facial features for AD detection. The framework incorporates two key modules: (a) a Cross-Enhanced Fusion Attention Module (CEFAM), which models inter-modal interactions through cross-attention and global enhancement, and (b) a Direction-Aware Convolution Module (DACM), which captures fine-grained directional facial features via horizontal-vertical receptive fields. Together, these modules enable adaptive and discriminative multimodal representation learning. To support this work, we constructed a synchronized multimodal dataset, including 25 patients with AD and 25 healthy controls (HC), by recording aligned facial video and eye-tracking sequences during a visual memory-search paradigm, providing an ecologically valid resource for evaluating integration strategies. Extensive experiments on this dataset demonstrate that our framework outperforms traditional late fusion and feature concatenation methods, achieving a classification accuracy of 95.11% in distinguishing AD from HC, highlighting superior robustness and diagnostic performance by explicitly modeling inter-modal dependencies and modality-specific contributions.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ Alzheimer's disease (AD) çš„æ—©æœŸå‡†ç¡®è¯Šæ–­ï¼Œæå‡ºäº†ä¸€ç§ç»“åˆ Eye-tracking å’Œ Facial Features çš„å¤šæ¨¡æ€äº¤å‰å¢å¼ºèåˆæ¡†æ¶ã€‚è¯¥æ¡†æ¶è®¾è®¡äº† Cross-Enhanced Fusion Attention Module (CEFAM) ä»¥é€šè¿‡ cross-attention å’Œå…¨å±€å¢å¼ºå»ºæ¨¡æ¨¡æ€é—´çš„äº¤äº’ï¼Œå¹¶åˆ©ç”¨ Direction-Aware Convolution Module (DACM) é€šè¿‡æ°´å¹³å’Œå‚ç›´æ„Ÿå—é‡æ•æ‰ç²¾ç»†çš„å®šå‘é¢éƒ¨ç‰¹å¾ã€‚ä¸ºéªŒè¯è¯¥æ–¹æ³•ï¼Œç ”ç©¶è€…æ„å»ºäº†ä¸€ä¸ªåŒ…å« 25 å AD æ‚£è€…å’Œ 25 å Healthy Controls (HC) çš„åŒæ­¥å¤šæ¨¡æ€æ•°æ®é›†ï¼Œè®°å½•äº†è§†è§‰è®°å¿†æœç´¢èŒƒå¼ä¸‹çš„å¯¹é½åºåˆ—ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯¥æ¡†æ¶åœ¨åŒºåˆ† AD ä¸ HC ä»»åŠ¡ä¸­è¾¾åˆ°äº† 95.11% çš„å‡†ç¡®ç‡ï¼Œæ€§èƒ½ä¼˜äºä¼ ç»Ÿçš„ Late Fusion å’Œç‰¹å¾æ‹¼æ¥æ–¹æ³•ã€‚è¯¥é¡¹å·¥ä½œé€šè¿‡æ˜¾å¼å»ºæ¨¡æ¨¡æ€é—´ä¾èµ–å…³ç³»ï¼Œä¸ºåˆ©ç”¨è¡Œä¸ºå’Œæ„ŸçŸ¥æ•°æ®è¿›è¡Œé²æ£’çš„è¾…åŠ©è¯Šæ–­æä¾›äº†æœ‰æ•ˆé€”å¾„ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI",
        "eess.IV"
      ],
      "primary_category": "cs.CV",
      "comment": "35 pages, 8 figures, and 7 tables",
      "pdf_url": "https://arxiv.org/pdf/2510.24777v1",
      "published_date": "2025-10-25 13:30:24 UTC",
      "updated_date": "2025-10-25 13:30:24 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T05:37:36.808358+00:00"
    },
    {
      "arxiv_id": "2510.22289v1",
      "title": "Does Homophily Help in Robust Test-time Node Classification?",
      "title_zh": "åŒè´¨æ€§æ˜¯å¦æœ‰åŠ©äºé²æ£’çš„æµ‹è¯•æ—¶èŠ‚ç‚¹åˆ†ç±»ï¼Ÿ",
      "authors": [
        "Yan Jiang",
        "Ruihong Qiu",
        "Zi Huang"
      ],
      "abstract": "Homophily, the tendency of nodes from the same class to connect, is a fundamental property of real-world graphs, underpinning structural and semantic patterns in domains such as citation networks and social networks. Existing methods exploit homophily through designing homophily-aware GNN architectures or graph structure learning strategies, yet they primarily focus on GNN learning with training graphs. However, in real-world scenarios, test graphs often suffer from data quality issues and distribution shifts, such as domain shifts across users from different regions in social networks and temporal evolution shifts in citation network graphs collected over varying time periods. These factors significantly compromise the pre-trained model's robustness, resulting in degraded test-time performance. With empirical observations and theoretical analysis, we reveal that transforming the test graph structure by increasing homophily in homophilic graphs or decreasing it in heterophilic graphs can significantly improve the robustness and performance of pre-trained GNNs on node classifications, without requiring model training or update. Motivated by these insights, a novel test-time graph structural transformation method grounded in homophily, named GrapHoST, is proposed. Specifically, a homophily predictor is developed to discriminate test edges, facilitating adaptive test-time graph structural transformation by the confidence of predicted homophily scores. Extensive experiments on nine benchmark datasets under a range of test-time data quality issues demonstrate that GrapHoST consistently achieves state-of-the-art performance, with improvements of up to 10.92%. Our code has been released at https://github.com/YanJiangJerry/GrapHoST.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†åŒè´¨æ€§(Homophily)åœ¨å¢å¼ºé²æ£’æµ‹è¯•é˜¶æ®µèŠ‚ç‚¹åˆ†ç±»(Robust Test-time Node Classification)ä¸­çš„ä½œç”¨ã€‚é’ˆå¯¹çœŸå®åœºæ™¯ä¸‹æµ‹è¯•å›¾å› æ•°æ®è´¨é‡å’Œåˆ†å¸ƒåç§»(Distribution Shifts)å¯¼è‡´é¢„è®­ç»ƒæ¨¡å‹æ€§èƒ½ä¸‹é™çš„é—®é¢˜ï¼Œè¯¥ç ”ç©¶é€šè¿‡ç†è®ºä¸ç»éªŒåˆ†æå‘ç°ï¼Œåœ¨æµ‹è¯•é˜¶æ®µæ ¹æ®å›¾çš„ç±»å‹è°ƒæ•´å…¶åŒè´¨æ€§æ°´å¹³â€”â€”å³å¢åŠ åŒè´¨å›¾çš„åŒè´¨æ€§æˆ–é™ä½å¼‚è´¨å›¾çš„åŒè´¨æ€§â€”â€”å¯ä»¥æ˜¾è‘—æå‡æ¨¡å‹é²æ£’æ€§ã€‚åŸºäºè¿™ä¸€è§è§£ï¼Œä½œè€…æå‡ºäº†åä¸ºGrapHoSTçš„å›¾ç»“æ„å˜æ¢æ–¹æ³•ï¼Œå…¶æ ¸å¿ƒæ˜¯ä¸€ä¸ªç”¨äºåˆ¤åˆ«æµ‹è¯•è¾¹çš„åŒè´¨æ€§é¢„æµ‹å™¨(Homophily Predictor)ï¼Œèƒ½å¤Ÿæ ¹æ®ç½®ä¿¡åº¦è‡ªé€‚åº”åœ°ä¼˜åŒ–æµ‹è¯•å›¾ç»“æ„ã€‚è¯¥æ–¹æ³•æ— éœ€æ¨¡å‹é‡è®­ç»ƒæˆ–æ›´æ–°ï¼Œåœ¨ä¹ä¸ªåŸºå‡†æ•°æ®é›†ä¸Šçš„å®éªŒè¯æ˜ï¼ŒGrapHoSTåœ¨åº”å¯¹å¤šç§æµ‹è¯•é˜¶æ®µæ•°æ®è´¨é‡é—®é¢˜æ—¶å‡è¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œå‡†ç¡®ç‡æå‡æœ€é«˜è¾¾10.92%ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.22289v1",
      "published_date": "2025-10-25 13:17:28 UTC",
      "updated_date": "2025-10-25 13:17:28 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T05:37:39.966224+00:00"
    },
    {
      "arxiv_id": "2510.22285v1",
      "title": "Supervised Fine-Tuning or In-Context Learning? Evaluating LLMs for Clinical NER",
      "title_zh": "æœ‰ç›‘ç£å¾®è°ƒè¿˜æ˜¯ä¸Šä¸‹æ–‡å­¦ä¹ ï¼Ÿé¢å‘ä¸´åºŠå‘½åå®ä½“è¯†åˆ«çš„å¤§è¯­è¨€æ¨¡å‹è¯„ä¼°",
      "authors": [
        "Andrei Baroian"
      ],
      "abstract": "We study clinical Named Entity Recognition (NER) on the CADEC corpus and compare three families of approaches: (i) BERT-style encoders (BERT Base, BioClinicalBERT, RoBERTa-large), (ii) GPT-4o used with few-shot in-context learning (ICL) under simple vs.\\ complex prompts, and (iii) GPT-4o with supervised fine-tuning (SFT). All models are evaluated on standard NER metrics over CADEC's five entity types (ADR, Drug, Disease, Symptom, Finding). RoBERTa-large and BioClinicalBERT offer limited improvements over BERT Base, showing the limit of these family of models. Among LLM settings, simple ICL outperforms a longer, instruction-heavy prompt, and SFT achieves the strongest overall performance (F1 $\\approx$ 87.1%), albeit with higher cost. We find that the LLM achieve higher accuracy on simplified tasks, restricting classification to two labels.",
      "tldr_zh": "è¯¥ç ”ç©¶å¯¹æ¯”äº† BERT-style ç¼–ç å™¨ã€å¤§è¯­è¨€æ¨¡å‹(LLMs)çš„ In-Context Learning (ICL) ä»¥åŠ Supervised Fine-Tuning (SFT) åœ¨ä¸´åºŠå‘½åå®ä½“è¯†åˆ«(Clinical NER)ä»»åŠ¡ä¸­çš„è¡¨ç°ã€‚å®éªŒåŸºäº CADEC è¯­æ–™åº“å¯¹äº”ç§å®ä½“ç±»å‹è¿›è¡Œè¯„ä¼°ï¼Œæ¶µç›–äº† BERT Baseã€BioClinicalBERTã€RoBERTa-large ä»¥åŠé‡‡ç”¨ä¸åŒæç¤ºç­–ç•¥çš„ GPT-4o æ¨¡å‹ã€‚ç ”ç©¶å‘ç°ï¼ŒRoBERTa-large å’Œ BioClinicalBERT ç›¸æ¯” BERT Base çš„æå‡è¾ƒä¸ºæœ‰é™ï¼Œæ˜¾ç¤ºå‡ºæ­¤ç±»æ¨¡å‹çš„æ€§èƒ½ç“¶é¢ˆã€‚åœ¨å¤§è¯­è¨€æ¨¡å‹åº”ç”¨ä¸­ï¼Œç®€å•çš„ ICL æ•ˆæœä¼˜äºå¤æ‚çš„é•¿æç¤ºæŒ‡ä»¤ï¼Œè€Œç»è¿‡ç›‘ç£å¾®è°ƒ(SFT)çš„ GPT-4o å–å¾—äº†çº¦ 87.1% çš„æœ€é«˜ F1 åˆ†æ•°ï¼Œå°½ç®¡å…¶æˆæœ¬ç›¸å¯¹æ›´é«˜ã€‚æ­¤å¤–ï¼ŒLLM åœ¨æ ‡ç­¾æ•°é‡å‡å°‘çš„ç®€åŒ–åˆ†ç±»ä»»åŠ¡ä¸­è¡¨ç°å‡ºæ›´é«˜çš„å‡†ç¡®åº¦ï¼Œä¸ºä¸´åºŠåŒ»ç–—é¢†åŸŸçš„ä¿¡æ¯æå–æä¾›äº†æ–¹æ³•è®ºå‚è€ƒã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "Work done in November - December 2024",
      "pdf_url": "https://arxiv.org/pdf/2510.22285v1",
      "published_date": "2025-10-25 13:08:59 UTC",
      "updated_date": "2025-10-25 13:08:59 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T05:37:43.777519+00:00"
    },
    {
      "arxiv_id": "2510.22282v1",
      "title": "CityRiSE: Reasoning Urban Socio-Economic Status in Vision-Language Models via Reinforcement Learning",
      "title_zh": "CityRiSEï¼šåŸºäºå¼ºåŒ–å­¦ä¹ çš„è§†è§‰è¯­è¨€æ¨¡å‹åŸå¸‚ç¤¾ä¼šç»æµåœ°ä½æ¨ç†",
      "authors": [
        "Tianhui Liu",
        "Hetian Pang",
        "Xin Zhang",
        "Jie Feng",
        "Yong Li",
        "Pan Hui"
      ],
      "abstract": "Harnessing publicly available, large-scale web data, such as street view and satellite imagery, urban socio-economic sensing is of paramount importance for achieving global sustainable development goals. With the emergence of Large Vision-Language Models (LVLMs), new opportunities have arisen to solve this task by treating it as a multi-modal perception and understanding problem. However, recent studies reveal that LVLMs still struggle with accurate and interpretable socio-economic predictions from visual data. To address these limitations and maximize the potential of LVLMs, we introduce \\textbf{CityRiSE}, a novel framework for \\textbf{R}eason\\textbf{i}ng urban \\textbf{S}ocio-\\textbf{E}conomic status in LVLMs through pure reinforcement learning (RL). With carefully curated multi-modal data and verifiable reward design, our approach guides the LVLM to focus on semantically meaningful visual cues, enabling structured and goal-oriented reasoning for generalist socio-economic status prediction. Experiments demonstrate that CityRiSE with emergent reasoning process significantly outperforms existing baselines, improving both prediction accuracy and generalization across diverse urban contexts, particularly for prediction on unseen cities and unseen indicators. This work highlights the promise of combining RL and LVLMs for interpretable and generalist urban socio-economic sensing.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ Large Vision-Language Models (LVLMs) åœ¨åˆ©ç”¨è¡—æ™¯å’Œå«æ˜Ÿå›¾åƒç­‰è§†è§‰æ•°æ®é¢„æµ‹åŸå¸‚ç¤¾ä¼šç»æµåœ°ä½æ—¶å­˜åœ¨çš„å‡†ç¡®æ€§å’Œå¯è§£é‡Šæ€§ä¸è¶³çš„é—®é¢˜ï¼Œæå‡ºäº† CityRiSE æ¡†æ¶ã€‚CityRiSE æ˜¯ä¸€ç§é€šè¿‡çº¯å¼ºåŒ–å­¦ä¹  (Reinforcement Learning) å¼•å¯¼ LVLMs è¿›è¡ŒåŸå¸‚ç¤¾ä¼šç»æµçŠ¶æ€æ¨ç†çš„æ–°å‹æ¡†æ¶ã€‚è¯¥æ¡†æ¶ç»“åˆç²¾å¿ƒç­–åˆ’çš„å¤šæ¨¡æ€æ•°æ®å’Œå¯éªŒè¯çš„å¥–åŠ±è®¾è®¡ (Verifiable Reward Design)ï¼Œå¼•å¯¼æ¨¡å‹å…³æ³¨å…·æœ‰è¯­ä¹‰æ„ä¹‰çš„è§†è§‰çº¿ç´¢ï¼Œä»è€Œå®ç°ç»“æ„åŒ–ä¸”ç›®æ ‡å¯¼å‘çš„æ¨ç†ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œå…·æœ‰æ¶Œç°æ¨ç†è¿‡ç¨‹çš„ CityRiSE æ˜¾è‘—ä¼˜äºç°æœ‰åŸºçº¿æ¨¡å‹ï¼Œåœ¨é¢„æµ‹å‡†ç¡®æ€§ä¸Šå–å¾—äº†å¤§å¹…æå‡ã€‚ç‰¹åˆ«æ˜¯åœ¨é’ˆå¯¹æœªè§åŸå¸‚å’Œæœªè§æŒ‡æ ‡çš„é¢„æµ‹ä»»åŠ¡ä¸­ï¼Œè¯¥æ¡†æ¶å±•ç°äº†å“è¶Šçš„æ³›åŒ–èƒ½åŠ›ã€‚è¿™é¡¹å·¥ä½œçªæ˜¾äº†ç»“åˆå¼ºåŒ–å­¦ä¹ ä¸ LVLMs åœ¨å®ç°å¯è§£é‡Šä¸”é€šç”¨çš„åŸå¸‚ç¤¾ä¼šç»æµæ„ŸçŸ¥æ–¹é¢çš„å·¨å¤§æ½œåŠ›ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.22282v1",
      "published_date": "2025-10-25 12:56:46 UTC",
      "updated_date": "2025-10-25 12:56:46 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T05:37:45.867744+00:00"
    },
    {
      "arxiv_id": "2510.23650v1",
      "title": "Beyond Hidden-Layer Manipulation: Semantically-Aware Logit Interventions for Debiasing LLMs",
      "title_zh": "è¶…è¶Šéšè—å±‚æ“æ§ï¼šé¢å‘å¤§è¯­è¨€æ¨¡å‹å»åå·®çš„è¯­ä¹‰æ„ŸçŸ¥ Logit å¹²é¢„",
      "authors": [
        "Wei Xia"
      ],
      "abstract": "We proposed Static and Dynamic -- two zero-shot logits-layer debiasing methods. Dynamic reduces bias by up to 70% with minimal fluency loss. Logits intervention outperforms hidden-layer approaches. We show semantic-aware logits intervention is stable and effective for debiasing aligned LLMs.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† Static å’Œ Dynamic ä¸¤ç§é›¶æ ·æœ¬(zero-shot) logits-layer å»å(debiasing)æ–¹æ³•ï¼Œæ—¨åœ¨æ¢ç´¢è¶…è¶Šéšè—å±‚(hidden-layer)æ“ä½œçš„å¹²é¢„æŠ€æœ¯ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒDynamic æ–¹æ³•åœ¨æå°åŒ–æµç•…æ€§æŸå¤±çš„å‰æä¸‹ï¼Œèƒ½å¤Ÿå°†åå·®é™ä½é«˜è¾¾ 70%ã€‚é€šè¿‡å¯¹æ¯”åˆ†æï¼Œç ”ç©¶å‘ç° logits å¹²é¢„(intervention)åœ¨æ€§èƒ½è¡¨ç°ä¸Šä¼˜äºä¼ ç»Ÿçš„éšè—å±‚æ–¹æ³•ã€‚è¯¥å·¥ä½œè¯æ˜äº†è¯­ä¹‰æ„ŸçŸ¥(semantically-aware)çš„ logits å¹²é¢„å¯¹äºå·²å¯¹é½(aligned)çš„å¤§è¯­è¨€æ¨¡å‹(LLMs)è€Œè¨€ï¼Œæ˜¯ä¸€ç§ç¨³å®šä¸”æœ‰æ•ˆçš„å»åæ‰‹æ®µã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.23650v1",
      "published_date": "2025-10-25 12:45:00 UTC",
      "updated_date": "2025-10-25 12:45:00 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T05:37:47.367060+00:00"
    },
    {
      "arxiv_id": "2510.22266v2",
      "title": "A Multi-level Analysis of Factors Associated with Student Performance: A Machine Learning Approach to the SAEB Microdata",
      "title_zh": "å­¦ç”Ÿå­¦ä¸šè¡¨ç°å…³è”å› ç´ çš„å¤šå±‚æ¬¡åˆ†æï¼šåŸºäº SAEB å¾®è§‚æ•°æ®çš„æœºå™¨å­¦ä¹ æ–¹æ³•",
      "authors": [
        "Rodrigo Tertulino",
        "Ricardo Almeida"
      ],
      "abstract": "Identifying the factors that influence student performance in basic education is a central challenge for formulating effective public policies in Brazil. This study introduces a multi-level machine learning approach to classify the proficiency of 9th-grade and high school students using microdata from the System of Assessment of Basic Education (SAEB). Our model uniquely integrates four data sources: student socioeconomic characteristics, teacher professional profiles, school indicators, and principal management profiles. A comparative analysis of four ensemble algorithms confirmed the superiority of a Random Forest model, which achieved 90.2% accuracy and an Area Under the Curve (AUC) of 96.7%. To move beyond prediction, we applied Explainable AI (XAI) using SHAP, which revealed that the school's average socioeconomic level is the most dominant predictor, demonstrating that systemic factors have a greater impact than individual characteristics in isolation. The primary conclusion is that academic performance is a systemic phenomenon deeply tied to the school's ecosystem. This study provides a data-driven, interpretable tool to inform policies aimed at promoting educational equity by addressing disparities between schools.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å·´è¥¿åŸºç¡€æ•™è‚²å­¦ç”Ÿè¡¨ç°çš„å½±å“å› ç´ ï¼Œæå‡ºäº†ä¸€ç§åŸºäºæœºå™¨å­¦ä¹ (Machine Learning)çš„å¤šå±‚æ¬¡åˆ†ææ–¹æ³•ï¼Œæ—¨åœ¨é€šè¿‡åŸºç¡€æ•™è‚²è¯„ä¼°ç³»ç»Ÿ(SAEB)çš„å¾®è§‚æ•°æ®å¯¹ä¹å¹´çº§å’Œé«˜ä¸­ç”Ÿçš„å­¦ä¸šæ°´å¹³è¿›è¡Œåˆ†ç±»ã€‚è¯¥æ¨¡å‹æ•´åˆäº†å­¦ç”Ÿç¤¾ä¼šç»æµç‰¹å¾ã€æ•™å¸ˆä¸“ä¸šæ¦‚å†µã€å­¦æ ¡æŒ‡æ ‡å’Œæ ¡é•¿ç®¡ç†æ¦‚å†µå››ä¸ªç»´åº¦çš„æ•°æ®ï¼Œå¹¶åœ¨å¯¹æ¯”å››ç§é›†æˆç®—æ³•(Ensemble Algorithms)åç¡®ç«‹äº†éšæœºæ£®æ—(Random Forest)æ¨¡å‹çš„æœ€ä¼˜æ€§èƒ½ï¼Œå…¶é¢„æµ‹å‡†ç¡®ç‡è¾¾åˆ°90.2%ä¸”æ›²çº¿ä¸‹é¢ç§¯(AUC)ä¸º96.7%ã€‚ç ”ç©¶è¿›ä¸€æ­¥åº”ç”¨å¯è§£é‡Šäººå·¥æ™ºèƒ½(XAI)ä¸­çš„SHAPæŠ€æœ¯å‘ç°ï¼Œå­¦æ ¡çš„å¹³å‡ç¤¾ä¼šç»æµæ°´å¹³æ˜¯å†³å®šæˆç»©çš„æœ€ä¸»è¦é¢„æµ‹å› å­ï¼Œè¡¨æ˜ç³»ç»Ÿæ€§å› ç´ çš„å½±å“åŠ›è¿œè¶…å­¤ç«‹çš„ä¸ªäººç‰¹è´¨ã€‚æœ€ç»ˆç»“è®ºå¼ºè°ƒå­¦ä¸šè¡¨ç°æ˜¯ä¸€ä¸ªæ·±æ·±æ¤æ ¹äºå­¦æ ¡ç”Ÿæ€ç³»ç»Ÿçš„ç³»ç»Ÿæ€§ç°è±¡ï¼Œè¯¥ç ”ç©¶ä¸ºé€šè¿‡ç¼©å°æ ¡é™…å·®å¼‚æ¥ä¿ƒè¿›æ•™è‚²å…¬å¹³çš„æ”¿ç­–åˆ¶å®šæä¾›äº†æ•°æ®é©±åŠ¨çš„å†³ç­–å·¥å…·ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CY"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.22266v2",
      "published_date": "2025-10-25 12:15:30 UTC",
      "updated_date": "2025-11-16 11:43:09 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T05:37:53.269884+00:00"
    },
    {
      "arxiv_id": "2510.22264v1",
      "title": "PatenTEB: A Comprehensive Benchmark and Model Family for Patent Text Embedding",
      "title_zh": "PatenTEBï¼šä¸“åˆ©æ–‡æœ¬åµŒå…¥çš„ç»¼åˆåŸºå‡†ä¸æ¨¡å‹å®¶æ—",
      "authors": [
        "Iliass Ayaou",
        "Denis Cavallucci"
      ],
      "abstract": "Patent text embeddings enable prior art search, technology landscaping, and patent analysis, yet existing benchmarks inadequately capture patent-specific challenges. We introduce PatenTEB, a comprehensive benchmark comprising 15 tasks across retrieval, classification, paraphrase, and clustering, with 2.06 million examples. PatenTEB employs domain-stratified splits, domain specific hard negative mining, and systematic coverage of asymmetric fragment-to-document matching scenarios absent from general embedding benchmarks. We develop the patembed model family through multi-task training, spanning 67M to 344M parameters with context lengths up to 4096 tokens. External validation shows strong generalization: patembed-base achieves state-of-the-art on MTEB BigPatentClustering.v2 (0.494 V-measure vs. 0.445 previous best), while patembed-large achieves 0.377 NDCG@100 on DAPFAM. Systematic ablations reveal that multi-task training improves external generalization despite minor benchmark costs, and that domain-pretrained initialization provides consistent advantages across task families. All resources will be made available at https://github.com/iliass-y/patenteb. Keywords: patent retrieval, sentence embeddings, multi-task learning, asymmetric retrieval, benchmark evaluation, contrastive learning.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ç°æœ‰åŸºå‡†æµ‹è¯•æ— æ³•å……åˆ†æ•æ‰ä¸“åˆ©é¢†åŸŸç‰¹å®šæŒ‘æˆ˜çš„é—®é¢˜ï¼Œæå‡ºäº†PatenTEBï¼Œä¸€ä¸ªä¸“é—¨ç”¨äºä¸“åˆ©æ–‡æœ¬åµŒå…¥(Patent Text Embedding)çš„ç»¼åˆæ€§åŸºå‡†ã€‚è¯¥åŸºå‡†åŒ…å«15ä¸ªä»»åŠ¡ï¼Œæ¶µç›–æ£€ç´¢(Retrieval)ã€åˆ†ç±»(Classification)ã€é‡Šä¹‰(Paraphrase)å’Œèšç±»(Clustering)ï¼Œå…±è®¡206ä¸‡ä¸ªæ ·æœ¬ã€‚PatenTEBé‡‡ç”¨äº†é¢†åŸŸåˆ†å±‚åˆ‡åˆ†ã€é¢†åŸŸç‰¹å®šçš„ç¡¬è´Ÿé‡‡æ ·(Hard Negative Mining)ä»¥åŠé’ˆå¯¹éå¯¹ç§°ç‰‡æ®µåˆ°æ–‡æ¡£åŒ¹é…åœºæ™¯çš„ç³»ç»Ÿè¦†ç›–ã€‚åŒæ—¶ï¼Œç ”ç©¶å›¢é˜Ÿå¼€å‘äº†patembedæ¨¡å‹å®¶æ—ï¼Œé€šè¿‡å¤šä»»åŠ¡è®­ç»ƒ(Multi-Task Training)æ¶µç›–äº†67Måˆ°344Mçš„å‚æ•°è§„æ¨¡ï¼Œå¹¶æ”¯æŒé«˜è¾¾4096ä¸ªtokençš„ä¸Šä¸‹æ–‡é•¿åº¦ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œpatembed-baseåœ¨MTEB BigPatentClustering.v2ä¸Šè¾¾åˆ°äº†æ–°çš„SOTAæ€§èƒ½ï¼Œè€Œpatembed-largeåœ¨DAPFAMæ£€ç´¢ä»»åŠ¡ä¸­ä¹Ÿè¡¨ç°ä¼˜å¼‚ã€‚æ¶ˆèå®éªŒè¿›ä¸€æ­¥è¯å®ï¼Œå¤šä»»åŠ¡è®­ç»ƒæ˜¾è‘—æå‡äº†å¤–éƒ¨æ³›åŒ–èƒ½åŠ›ï¼Œä¸”é¢†åŸŸé¢„è®­ç»ƒ(Domain-Pretrained)åˆå§‹åŒ–åœ¨å„ç±»ä»»åŠ¡ä¸­å‡å…·æœ‰æ˜¾è‘—ä¼˜åŠ¿ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.IR"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.22264v1",
      "published_date": "2025-10-25 12:01:46 UTC",
      "updated_date": "2025-10-25 12:01:46 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T05:37:55.457595+00:00"
    },
    {
      "arxiv_id": "2510.22261v1",
      "title": "Epistemic Deep Learning: Enabling Machine Learning Models to Know When They Do Not Know",
      "title_zh": "è®¤çŸ¥æ·±åº¦å­¦ä¹ ï¼šèµ‹äºˆæœºå™¨å­¦ä¹ æ¨¡å‹â€œçŸ¥å…¶æ‰€ä¸çŸ¥â€çš„èƒ½åŠ›",
      "authors": [
        "Shireen Kudukkil Manchingal"
      ],
      "abstract": "Machine learning has achieved remarkable successes, yet its deployment in safety-critical domains remains hindered by an inherent inability to manage uncertainty, resulting in overconfident and unreliable predictions when models encounter out-of-distribution data, adversarial perturbations, or naturally fluctuating environments. This thesis, titled Epistemic Deep Learning: Enabling Machine Learning Models to 'Know When They Do Not Know', addresses these critical challenges by advancing the paradigm of Epistemic Artificial Intelligence, which explicitly models and quantifies epistemic uncertainty: the uncertainty arising from limited, biased, or incomplete training data, as opposed to the irreducible randomness of aleatoric uncertainty, thereby empowering models to acknowledge their limitations and refrain from overconfident decisions when uncertainty is high.\n  Central to this work is the development of the Random-Set Neural Network (RS-NN), a novel methodology that leverages random set theory to predict belief functions over sets of classes, capturing the extent of epistemic uncertainty through the width of associated credal sets, applications of RS-NN, including its adaptation to Large Language Models (LLMs) and its deployment in weather classification for autonomous racing. In addition, the thesis proposes a unified evaluation framework for uncertainty-aware classifiers. Extensive experiments validate that integrating epistemic awareness into deep learning not only mitigates the risks associated with overconfident predictions but also lays the foundation for a paradigm shift in artificial intelligence, where the ability to 'know when it does not know' becomes a hallmark of robust and dependable systems. The title encapsulates the core philosophy of this work, emphasizing that true intelligence involves recognizing and managing the limits of one's own knowledge.",
      "tldr_zh": "è¿™é¡¹ç ”ç©¶æ¢è®¨äº†è®¤çŸ¥æ·±åº¦å­¦ä¹ (Epistemic Deep Learning)ï¼Œæ—¨åœ¨ä½¿æœºå™¨å­¦ä¹ æ¨¡å‹èƒ½å¤Ÿè¯†åˆ«å…¶è‡ªèº«çš„çŸ¥è¯†è¾¹ç•Œã€‚é’ˆå¯¹æ¨¡å‹åœ¨å¤„ç†åˆ†å¸ƒå¤–æ•°æ®(out-of-distribution)æˆ–å¯¹æŠ—æ€§æ‰°åŠ¨æ—¶å®¹æ˜“äº§ç”Ÿè¿‡åº¦è‡ªä¿¡ä¸”ä¸å¯é é¢„æµ‹çš„é—®é¢˜ï¼Œè¯¥è®ºæ–‡æ¨è¿›äº†è®¤çŸ¥äººå·¥æ™ºèƒ½(Epistemic Artificial Intelligence)èŒƒå¼ï¼Œé€šè¿‡æ˜¾å¼å»ºæ¨¡å’Œé‡åŒ–è®¤çŸ¥ä¸ç¡®å®šæ€§(epistemic uncertainty)æ¥å¢å¼ºæ¨¡å‹çš„å¯é æ€§ã€‚ç ”ç©¶çš„æ ¸å¿ƒè´¡çŒ®æ˜¯æå‡ºäº†éšæœºé›†ç¥ç»ç½‘ç»œ(Random-Set Neural Network, RS-NN)ï¼Œåˆ©ç”¨éšæœºé›†ç†è®º(random set theory)é¢„æµ‹ç±»åˆ«é›†åˆä¸Šçš„ä¿¡å¿µå‡½æ•°ï¼Œä»¥æ•æ‰ç”±äºè®­ç»ƒæ•°æ®æœ‰é™æˆ–ä¸å®Œæ•´å¸¦æ¥çš„è®¤çŸ¥ä¸ç¡®å®šæ€§ã€‚æ­¤å¤–ï¼Œä½œè€…å±•ç¤ºäº†RS-NNåœ¨å¤§è¯­è¨€æ¨¡å‹(LLMs)åŠè‡ªåŠ¨é©¾é©¶èµ›è½¦å¤©æ°”åˆ†ç±»ä¸­çš„å…·ä½“åº”ç”¨ï¼Œå¹¶æå‡ºäº†ä¸€ä¸ªç»Ÿä¸€çš„ä¸ç¡®å®šæ€§æ„ŸçŸ¥åˆ†ç±»å™¨è¯„ä¼°æ¡†æ¶ã€‚å®éªŒç»“æœéªŒè¯äº†æ•´åˆè®¤çŸ¥æ„è¯†èƒ½å¤Ÿæœ‰æ•ˆé™ä½è¿‡åº¦è‡ªä¿¡å†³ç­–çš„é£é™©ï¼Œä¸ºæ„å»ºæ›´åŠ é²æ£’å’Œå€¼å¾—ä¿¡èµ–çš„æ·±åº¦å­¦ä¹ ç³»ç»Ÿå¥ å®šäº†é‡è¦åŸºç¡€ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "PhD thesis",
      "pdf_url": "https://arxiv.org/pdf/2510.22261v1",
      "published_date": "2025-10-25 12:00:19 UTC",
      "updated_date": "2025-10-25 12:00:19 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T05:37:58.279051+00:00"
    },
    {
      "arxiv_id": "2510.23649v3",
      "title": "Efficient Low Rank Attention for Long-Context Inference in Large Language Models",
      "title_zh": "å¤§è¯­è¨€æ¨¡å‹é•¿ä¸Šä¸‹æ–‡æ¨ç†çš„é«˜æ•ˆä½ç§©æ³¨æ„åŠ›",
      "authors": [
        "Tenghui Li",
        "Guoxu Zhou",
        "Xuyang Zhao",
        "Yuning Qiu",
        "Qibin Zhao"
      ],
      "abstract": "As the length of input text increases, the key-value (KV) cache in LLMs imposes prohibitive GPU memory costs and limits long-context inference on resource constrained devices. Existing approaches, such as KV quantization and pruning, reduce memory usage but suffer from numerical precision loss or suboptimal retention of key-value pairs. In this work, Low Rank Query and Key attention (LRQK) is introduced, a two-stage framework that jointly decomposes full-precision query and key matrices into compact rank-\\(r\\) factors during the prefill stage, and then employs these low-dimensional projections to compute proxy attention scores in \\(\\mathcal{O}(lr)\\) time at each decode step. By selecting only the top-\\(k\\) tokens and a small fixed set of recent tokens, LRQK employs a mixed GPU-CPU cache with a hit-and-miss mechanism where only missing full-precision KV pairs are transferred, thereby preserving exact attention outputs while reducing CPU-GPU data movement. Extensive experiments on the RULER and LongBench benchmarks with LLaMA-3-8B and Qwen2.5-7B demonstrate that LRQK matches or surpasses leading sparse-attention methods in long context settings, while delivering significant memory savings with minimal accuracy loss. Our code is available at https://github.com/tenghuilee/LRQK.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤§è¯­è¨€æ¨¡å‹(LLMs)åœ¨å¤„ç†é•¿æ–‡æœ¬æ—¶KV cacheå¸¦æ¥çš„é«˜æ˜‚GPUå†…å­˜æˆæœ¬é—®é¢˜ï¼Œæå‡ºäº†LRQK (Low Rank Query and Key attention)ä¸¤é˜¶æ®µæ¡†æ¶ã€‚åœ¨é¢„å¡«å……é˜¶æ®µ(prefill stage)ï¼ŒLRQKå°†å…¨ç²¾åº¦çš„queryå’ŒkeyçŸ©é˜µåˆ†è§£ä¸ºç´§å‡‘çš„ä½ç§©å› å­ï¼Œå¹¶åˆ©ç”¨è¿™äº›æŠ•å½±åœ¨è§£ç é˜¶æ®µä»¥O(lr)æ—¶é—´å¤æ‚åº¦è®¡ç®—ä»£ç†æ³¨æ„åŠ›åˆ†æ•°ã€‚è¯¥æ¡†æ¶è¿›ä¸€æ­¥é‡‡ç”¨æ··åˆGPU-CPUç¼“å­˜ä¸å‘½ä¸­å¤±è´¥æœºåˆ¶(hit-and-miss mechanism)ï¼Œé€šè¿‡ä»…ä¼ è¾“ç¼ºå¤±çš„å…¨ç²¾åº¦KVå¯¹æ¥å‡å°‘æ•°æ®ç§»åŠ¨å¹¶ä¿ç•™ç²¾ç¡®è¾“å‡ºã€‚å®éªŒåœ¨RULERå’ŒLongBenchåŸºå‡†æµ‹è¯•ä¸Šåˆ©ç”¨LLaMA-3-8Bå’ŒQwen2.5-7Bæ¨¡å‹è¿›è¡Œäº†éªŒè¯ã€‚ç»“æœæ˜¾ç¤ºLRQKåœ¨é•¿æ–‡æœ¬æ¨ç†ä¸­è¾¾åˆ°æˆ–è¶…è¿‡äº†ç°æœ‰ç¨€ç–æ³¨æ„åŠ›(sparse-attention)æ–¹æ³•çš„æ•ˆæœã€‚è¯¥æ–¹æ¡ˆåœ¨æ˜¾è‘—é™ä½å†…å­˜å ç”¨çš„åŒæ—¶ï¼Œæœ€å¤§é™åº¦åœ°å‡å°‘äº†å‡†ç¡®ç‡æŸå¤±ï¼Œä¸ºèµ„æºå—é™è®¾å¤‡ä¸Šçš„é«˜æ•ˆé•¿æ–‡æœ¬æ¨ç†æä¾›äº†æœ‰åŠ›æ”¯æŒã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "https://neurips.cc/virtual/2025/loc/san-diego/poster/118451",
      "pdf_url": "https://arxiv.org/pdf/2510.23649v3",
      "published_date": "2025-10-25 11:43:27 UTC",
      "updated_date": "2025-12-23 08:47:31 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T05:38:03.468114+00:00"
    },
    {
      "arxiv_id": "2510.22257v1",
      "title": "LUNA: Efficient and Topology-Agnostic Foundation Model for EEG Signal Analysis",
      "title_zh": "LUNAï¼šé«˜æ•ˆä¸”æ‹“æ‰‘æ— å…³çš„è„‘ç”µä¿¡å·åˆ†æåŸºç¡€æ¨¡å‹",
      "authors": [
        "Berkay DÃ¶ner",
        "Thorir Mar Ingolfsson",
        "Luca Benini",
        "Yawei Li"
      ],
      "abstract": "Electroencephalography (EEG) offers a non-invasive lens into human brain activity, but building large-scale models is hampered by topological heterogeneity: each public EEG data defines its own electrode layout, limiting generalization. We introduce LUNA (Latent Unified Network Architecture), a self-supervised foundation model that reconciles disparate electrode geometries while scaling linearly -- not quadratically -- with channel count. LUNA compresses multi-channel EEG into a fixed-size, topology-agnostic latent space via learned queries and cross-attention. Downstream transformer blocks then operate exclusively on this latent representation using patch-wise temporal self-attention, decoupling computation from electrode count. Pre-trained on TUEG and Siena (over 21,000 hours of raw EEG across diverse montages) using a masked-patch reconstruction objective, LUNA transfers effectively to four downstream tasks: abnormality detection, artifact rejection, slowing classification, and emotion recognition. It demonstrates highly competitive performance across several benchmarks, achieving state-of-the-art results on TUAR and TUSL, e.g., 0.921 AUROC on TUAR, while reducing FLOPs by 300x and trimming GPU memory use by up to 10x. Critically, these gains are consistent across all evaluated electrode configurations. Code is available at https://github.com/pulp-bio/BioFoundation",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹è„‘ç”µå›¾(EEG)æ•°æ®ä¸­ç”µæå¸ƒå±€å¤šæ ·æ€§å¯¼è‡´çš„æ‹“æ‰‘å¼‚æ„æ€§é—®é¢˜ï¼Œæå‡ºäº†LUNA (Latent Unified Network Architecture)ï¼Œä¸€ç§é«˜æ•ˆä¸”æ‹“æ‰‘æ— å…³çš„è‡ªç›‘ç£åŸºç¡€æ¨¡å‹ã€‚LUNAé€šè¿‡å­¦ä¹ åˆ°çš„æŸ¥è¯¢(learned queries)å’Œäº¤å‰æ³¨æ„åŠ›(cross-attention)æœºåˆ¶ï¼Œå°†å¤šé€šé“EEGä¿¡å·å‹ç¼©åˆ°å›ºå®šå¤§å°çš„æ½œåœ¨ç©ºé—´ä¸­ï¼Œå®ç°äº†è®¡ç®—å¤æ‚åº¦éšé€šé“æ•°çº¿æ€§å¢é•¿è€Œéä¼ ç»Ÿçš„å¹³æ–¹å¢é•¿ã€‚æ¨¡å‹åç»­é‡‡ç”¨è¡¥ä¸å¼æ—¶é—´è‡ªæ³¨æ„åŠ›(patch-wise temporal self-attention)å¤„ç†æ½œåœ¨è¡¨ç¤ºï¼Œä»è€ŒæˆåŠŸå°†è®¡ç®—è¿‡ç¨‹ä¸å…·ä½“çš„ç”µææ•°é‡è§£è€¦ã€‚è¯¥æ¨¡å‹åœ¨TUEGå’ŒSienaæ•°æ®é›†ä¸Šåˆ©ç”¨è¶…è¿‡21,000å°æ—¶çš„åŸå§‹EEGæ•°æ®ï¼Œé€šè¿‡æ©ç è¡¥ä¸é‡å»º(masked-patch reconstruction)ç›®æ ‡è¿›è¡Œé¢„è®­ç»ƒã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒLUNAåœ¨å¼‚å¸¸æ£€æµ‹(abnormality detection)å’Œæƒ…ç»ªè¯†åˆ«(emotion recognition)ç­‰å››é¡¹ä¸‹æ¸¸ä»»åŠ¡ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œå¹¶åœ¨TUARå’ŒTUSLåŸºå‡†ä¸Šå–å¾—äº†SOTAæ€§èƒ½ã€‚æ­¤å¤–ï¼ŒLUNAåœ¨ä¿æŒè·¨ç”µæé…ç½®ä¸€è‡´æ€§çš„åŒæ—¶ï¼Œå°†FLOPsé™ä½äº†300å€ï¼Œå¹¶å°†æ˜¾å­˜å ç”¨å‡å°‘äº†å¤šè¾¾10å€ï¼Œä¸ºå¤§è§„æ¨¡è„‘ç”µä¿¡å·åˆ†ææä¾›äº†æå…·æ‰©å±•æ€§çš„æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "NeurIPS camera-ready version, 27 pages, 10 figures, 13 tables",
      "pdf_url": "https://arxiv.org/pdf/2510.22257v1",
      "published_date": "2025-10-25 11:31:27 UTC",
      "updated_date": "2025-10-25 11:31:27 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T05:38:05.568632+00:00"
    },
    {
      "arxiv_id": "2510.22255v1",
      "title": "PACR: Progressively Ascending Confidence Reward for LLM Reasoning",
      "title_zh": "PACRï¼šå¤§è¯­è¨€æ¨¡å‹æ¨ç†ä¸­é€æ­¥ä¸Šå‡çš„ç½®ä¿¡åº¦å¥–åŠ±",
      "authors": [
        "Eunseop Yoon",
        "Hee Suk Yoon",
        "Jaehyun Jang",
        "SooHwan Eom",
        "Qi Dai",
        "Chong Luo",
        "Mark A. Hasegawa-Johnson",
        "Chang D. Yoo"
      ],
      "abstract": "Reinforcement Learning with Verifiable Rewards (RLVR) has significantly improved LLM reasoning, but its sparse, outcome-based reward provides no guidance for intermediate steps, slowing exploration. We propose Progressively Ascending Confidence Reward (PACR), a dense, model-intrinsic reward computed directly from the model's evolving belief in the correct answer. PACR encodes the inductive bias that, along a well-formed reasoning trajectory, the probability of the ground-truth answer should have a generally ascending trend. We provide empirical and theoretical analysis validating that such an inductive bias constrains the exploration search space to regions richer in logically sound reasoning. We demonstrate that PACR accelerates exploration, reaches reward saturation with fewer trajectories, and yields improvements on multiple benchmarks. Our results suggest that dense, model-intrinsic shaping signals can make RLVR training more effective and reliable.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ Reinforcement Learning with Verifiable Rewards (RLVR) åœ¨å¤§è¯­è¨€æ¨¡å‹æ¨ç†ä¸­å› ç¨€ç–å¥–åŠ±å¯¼è‡´æ¢ç´¢ç¼“æ…¢çš„é—®é¢˜ï¼Œæå‡ºäº† Progressively Ascending Confidence Reward (PACR)ã€‚è¿™æ˜¯ä¸€ç§ç¨ å¯†çš„æ¨¡å‹å†…åœ¨å¥–åŠ±ï¼Œç›´æ¥æ ¹æ®æ¨¡å‹å¯¹æ­£ç¡®ç­”æ¡ˆä¸æ–­æ¼”å˜çš„ç½®ä¿¡åº¦è¿›è¡Œè®¡ç®—ã€‚PACR æ ¸å¿ƒåœ¨äºå¼•å…¥äº†ä¸€ç§å½’çº³åç½® (Inductive Bias)ï¼Œå³åœ¨é€»è¾‘ä¸¥å¯†çš„æ¨ç†è½¨è¿¹ä¸­ï¼Œæ¨¡å‹é¢„æµ‹æ­£ç¡®ç­”æ¡ˆçš„æ¦‚ç‡åº”å‘ˆç°æ€»ä½“ä¸Šå‡è¶‹åŠ¿ã€‚ç†è®ºä¸ç»éªŒåˆ†æè¯å®ï¼Œè¿™ç§åç½®èƒ½å°†æ¢ç´¢ç©ºé—´çº¦æŸåœ¨æ›´å…·é€»è¾‘æ€§çš„åŒºåŸŸã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒPACR æ˜¾è‘—åŠ é€Ÿäº†æ¢ç´¢è¿‡ç¨‹ï¼Œä½¿æ¨¡å‹èƒ½ä»¥æ›´å°‘çš„è®­ç»ƒè½¨è¿¹è¾¾åˆ°å¥–åŠ±é¥±å’Œï¼Œå¹¶åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­å–å¾—äº†æ€§èƒ½æå‡ã€‚è¯¥ç ”ç©¶è¡¨æ˜ï¼Œç¨ å¯†çš„æ¨¡å‹å†…åœ¨å¡‘å½¢ä¿¡å·èƒ½ä½¿ RLVR è®­ç»ƒåœ¨æå‡ LLM æ¨ç†èƒ½åŠ›æ—¶æ›´åŠ é«˜æ•ˆä¸”å¯é ã€‚",
      "categories": [
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.AI",
      "comment": "16 pages, 14 figures",
      "pdf_url": "https://arxiv.org/pdf/2510.22255v1",
      "published_date": "2025-10-25 11:25:35 UTC",
      "updated_date": "2025-10-25 11:25:35 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T05:38:17.451096+00:00"
    },
    {
      "arxiv_id": "2510.22251v1",
      "title": "You Don't Need Prompt Engineering Anymore: The Prompting Inversion",
      "title_zh": "å‘Šåˆ«æç¤ºå·¥ç¨‹ï¼šæç¤ºåè½¬",
      "authors": [
        "Imran Khan"
      ],
      "abstract": "Prompt engineering, particularly Chain-of-Thought (CoT) prompting, significantly enhances LLM reasoning capabilities. We introduce \"Sculpting,\" a constrained, rule-based prompting method designed to improve upon standard CoT by reducing errors from semantic ambiguity and flawed common sense.\n  We evaluate three prompting strategies (Zero Shot, standard CoT, and Sculpting) across three OpenAI model generations (gpt-4o-mini, gpt-4o, gpt-5) using the GSM8K mathematical reasoning benchmark (1,317 problems).\n  Our findings reveal a \"Prompting Inversion\": Sculpting provides advantages on gpt-4o (97% vs. 93% for standard CoT), but becomes detrimental on gpt-5 (94.00% vs. 96.36% for CoT on full benchmark). We trace this to a \"Guardrail-to-Handcuff\" transition where constraints preventing common-sense errors in mid-tier models induce hyper-literalism in advanced models. Our detailed error analysis demonstrates that optimal prompting strategies must co-evolve with model capabilities, suggesting simpler prompts for more capable models.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº† Prompt Engineering çš„æ¼”å˜ï¼Œå¹¶å¼•å…¥äº†ä¸€ç§åä¸º Sculpting çš„çº¦æŸæ€§ã€åŸºäºè§„åˆ™çš„æç¤ºæ–¹æ³•ï¼Œæ—¨åœ¨é€šè¿‡å‡å°‘è¯­ä¹‰æ­§ä¹‰å’Œå¸¸è¯†é”™è¯¯æ¥æ”¹è¿›æ ‡å‡†çš„ Chain-of-Thought (CoT)ã€‚ç ”ç©¶è€…åœ¨ GSM8K æ•°å­¦æ¨ç†åŸºå‡†ä¸Šï¼Œé’ˆå¯¹ gpt-4o-miniã€gpt-4o å’Œ gpt-5 ä¸‰ä»£æ¨¡å‹è¯„ä¼°äº† Zero Shotã€æ ‡å‡† CoT å’Œ Sculpting ä¸‰ç§ç­–ç•¥ã€‚ç ”ç©¶å‘ç°å­˜åœ¨â€œæç¤ºåè½¬â€ï¼ˆPrompting Inversionï¼‰ç°è±¡ï¼šSculpting åœ¨ gpt-4o ä¸Šè¡¨ç°ä¼˜äº CoTï¼ˆ97% å¯¹ 93%ï¼‰ï¼Œä½†åœ¨æ›´å…ˆè¿›çš„ gpt-5 ä¸Šåè€Œé™ä½äº†æ€§èƒ½ï¼ˆ94.00% å¯¹ 96.36%ï¼‰ã€‚è¿™ç§ç°è±¡æºäºâ€œä»æŠ¤æ åˆ°æ‰‹é“â€ï¼ˆGuardrail-to-Handcuffï¼‰çš„è½¬å˜ï¼Œå³åœ¨ä¸­ç­‰æ¨¡å‹ä¸­é˜²æ­¢å¸¸è¯†é”™è¯¯çš„çº¦æŸåœ¨é«˜çº§æ¨¡å‹ä¸­è¯±å‘äº†è¿‡åº¦å­—é¢åŒ–ï¼ˆhyper-literalismï¼‰ã€‚é”™è¯¯åˆ†æè¡¨æ˜ï¼Œæœ€ä¼˜æç¤ºç­–ç•¥å¿…é¡»éšæ¨¡å‹èƒ½åŠ›ååŒæ¼”è¿›ï¼Œæš—ç¤ºå¯¹äºèƒ½åŠ›æ›´å¼ºçš„æ¨¡å‹ï¼Œæ›´ç®€æ´çš„æç¤ºå¾€å¾€æ•ˆæœæ›´å¥½ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "17 pages, 1 figure, 6 tables. Code and experimental data available at https://github.com/strongSoda/prompt-sculpting",
      "pdf_url": "https://arxiv.org/pdf/2510.22251v1",
      "published_date": "2025-10-25 11:04:01 UTC",
      "updated_date": "2025-10-25 11:04:01 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T05:38:28.572911+00:00"
    },
    {
      "arxiv_id": "2510.22243v1",
      "title": "Real-Time Semantic Segmentation on FPGA for Autonomous Vehicles Using LMIINet with the CGRA4ML Framework",
      "title_zh": "åŸºäº CGRA4ML æ¡†æ¶ä¸ LMIINet çš„è‡ªåŠ¨é©¾é©¶è½¦è¾† FPGA å®æ—¶è¯­ä¹‰åˆ†å‰²",
      "authors": [
        "Amir Mohammad Khadem Hosseini",
        "Sattar Mirzakuchaki"
      ],
      "abstract": "Semantic segmentation has emerged as a fundamental problem in computer vision, gaining particular importance in real-time applications such as autonomous driving. The main challenge is achieving high accuracy while operating under computational and hardware constraints. In this research, we present an FPGA-based implementation of real-time semantic segmentation leveraging the lightweight LMIINet architecture and the Coarse-Grained Reconfigurable Array for Machine Learning (CGRA4ML) hardware framework. The model was trained using Quantization-Aware Training (QAT) with 8-bit precision on the Cityscapes dataset, reducing memory footprint by a factor of four while enabling efficient fixed-point computations. Necessary modifications were applied to adapt the model to CGRA4ML constraints, including simplifying skip connections, employing hardware-friendly operations such as depthwise-separable and 1A-1 convolutions, and redesigning parts of the Flatten Transformer. Our implementation achieves approximately 90% pixel accuracy and 45% mean Intersection-over-Union (mIoU), operating in real-time at 20 frames per second (FPS) with 50.1 ms latency on the ZCU104 FPGA board. The results demonstrate the potential of CGRA4ML, with its flexibility in mapping modern layers and off-chip memory utilization for skip connections, provides a path for implementing advanced semantic segmentation networks on FPGA for real-time applications to outperform traditional GPU solutions in terms of power efficiency while maintaining competitive accuracy. The code for this project is publicly available at https://github.com/STAmirr/ cgra4ml_semantic_segmentation",
      "tldr_zh": "æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§åŸºäºFPGAçš„å®æ—¶Semantic Segmentationå®ç°æ–¹æ¡ˆï¼Œåˆ©ç”¨è½»é‡çº§çš„LMIINetæ¶æ„å’ŒCGRA4MLç¡¬ä»¶æ¡†æ¶ã€‚é€šè¿‡åœ¨Cityscapesæ•°æ®é›†ä¸Šé‡‡ç”¨8-bitç²¾åº¦çš„Quantization-Aware Training (QAT)ï¼Œè¯¥æ¨¡å‹æœ‰æ•ˆå‡å°‘äº†å†…å­˜å ç”¨å¹¶å®ç°äº†é«˜æ•ˆçš„å®šç‚¹è®¡ç®—ã€‚é’ˆå¯¹CGRA4MLçš„ç¡¬ä»¶çº¦æŸï¼Œç ”ç©¶è€…å¯¹æ¨¡å‹è¿›è¡Œäº†é’ˆå¯¹æ€§ä¼˜åŒ–ï¼ŒåŒ…æ‹¬ç®€åŒ–skip connectionsã€ä½¿ç”¨depthwise-separableå·ç§¯ä»¥åŠé‡æ–°è®¾è®¡Flatten Transformerçš„éƒ¨åˆ†ç»“æ„ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ¡ˆåœ¨ZCU104 FPGAæ¿å¡ä¸Šè¾¾åˆ°äº†çº¦90%çš„åƒç´ å‡†ç¡®ç‡å’Œ45%çš„mIoUï¼Œä¸”è¿è¡Œé€Ÿåº¦è¾¾20 FPSï¼Œå»¶è¿Ÿä»…ä¸º50.1 msã€‚è¯¥æˆæœè¯æ˜äº†CGRA4MLæ¡†æ¶åœ¨å¤„ç†ç°ä»£ç¥ç»ç½‘ç»œå±‚æ–¹é¢çš„çµæ´»æ€§ï¼Œä¸ºè‡ªåŠ¨é©¾é©¶é¢†åŸŸæä¾›äº†æ¯”ä¼ ç»ŸGPUæ–¹æ¡ˆæ›´å…·åŠŸæ•ˆæ¯”çš„å®æ—¶è¯­ä¹‰åˆ†å‰²è·¯å¾„ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.22243v1",
      "published_date": "2025-10-25 10:16:22 UTC",
      "updated_date": "2025-10-25 10:16:22 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T05:38:30.350070+00:00"
    },
    {
      "arxiv_id": "2510.22242v1",
      "title": "PaperAsk: A Benchmark for Reliability Evaluation of LLMs in Paper Search and Reading",
      "title_zh": "PaperAskï¼šå¤§è¯­è¨€æ¨¡å‹è®ºæ–‡æœç´¢ä¸é˜…è¯»å¯é æ€§è¯„ä¼°åŸºå‡†",
      "authors": [
        "Yutao Wu",
        "Xiao Liu",
        "Yunhao Feng",
        "Jiale Ding",
        "Xingjun Ma"
      ],
      "abstract": "Large Language Models (LLMs) increasingly serve as research assistants, yet their reliability in scholarly tasks remains under-evaluated. In this work, we introduce PaperAsk, a benchmark that systematically evaluates LLMs across four key research tasks: citation retrieval, content extraction, paper discovery, and claim verification. We evaluate GPT-4o, GPT-5, and Gemini-2.5-Flash under realistic usage conditions-via web interfaces where search operations are opaque to the user. Through controlled experiments, we find consistent reliability failures: citation retrieval fails in 48-98% of multi-reference queries, section-specific content extraction fails in 72-91% of cases, and topical paper discovery yields F1 scores below 0.32, missing over 60% of relevant literature. Further human analysis attributes these failures to the uncontrolled expansion of retrieved context and the tendency of LLMs to prioritize semantically relevant text over task instructions. Across basic tasks, the LLMs display distinct failure behaviors: ChatGPT often withholds responses rather than risk errors, whereas Gemini produces fluent but fabricated answers. To address these issues, we develop lightweight reliability classifiers trained on PaperAsk data to identify unreliable outputs. PaperAsk provides a reproducible and diagnostic framework for advancing the reliability evaluation of LLM-based scholarly assistance systems.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¨å‡ºäº† PaperAskï¼Œè¿™æ˜¯ä¸€ä¸ªæ—¨åœ¨ç³»ç»Ÿè¯„ä¼°å¤§è¯­è¨€æ¨¡å‹ (LLMs) åœ¨å­¦æœ¯ä»»åŠ¡ä¸­å¯é æ€§çš„åŸºå‡†æµ‹è¯•ã€‚è¯¥åŸºå‡†æ¶µç›–äº†å¼•ç”¨æ£€ç´¢ (citation retrieval)ã€å†…å®¹æå– (content extraction)ã€è®ºæ–‡å‘ç° (paper discovery) å’Œè§‚ç‚¹éªŒè¯ (claim verification) å››ä¸ªå…³é”®ç ”ç©¶ä»»åŠ¡ã€‚é€šè¿‡å¯¹ GPT-4oã€GPT-5 å’Œ Gemini-2.5-Flash çš„è¯„ä¼°å‘ç°ï¼Œæ¨¡å‹åœ¨å¼•ç”¨æ£€ç´¢å’Œç‰¹å®šå†…å®¹æå–æ–¹é¢çš„å¤±è´¥ç‡åˆ†åˆ«é«˜è¾¾ 48-98% å’Œ 72-91%ï¼Œè®ºæ–‡å‘ç°ä»»åŠ¡çš„ F1 åˆ†æ•°äº¦æ™®éä½äº 0.32ã€‚è¿›ä¸€æ­¥åˆ†æè¡¨æ˜ï¼Œå¯é æ€§ç¼ºé™·ä¸»è¦æºäºæ£€ç´¢ä¸Šä¸‹æ–‡çš„æ— åºæ‰©å¼ ä»¥åŠæ¨¡å‹å¯¹è¯­ä¹‰ç›¸å…³æ€§çš„è¿‡åº¦ä¼˜å…ˆã€‚ç ”ç©¶è¿˜æŒ‡å‡ºä¸åŒæ¨¡å‹å­˜åœ¨ä¸åŒçš„å¤±æ•ˆè¡Œä¸ºï¼Œå¦‚ ChatGPT å€¾å‘äºæ‹’ç»å›ç­”ï¼Œè€Œ Gemini åˆ™å®¹æ˜“äº§ç”Ÿè™šå‡ä½†æµç•…çš„ç­”æ¡ˆã€‚æœ€åï¼Œè¯¥ç ”ç©¶åˆ©ç”¨ PaperAsk æ•°æ®è®­ç»ƒäº†è½»é‡çº§å¯é æ€§åˆ†ç±»å™¨ (reliability classifiers)ï¼Œä¸ºè¯†åˆ«ä¸å¯é è¾“å‡ºåŠæ¨è¿›å­¦æœ¯è¾…åŠ©ç³»ç»Ÿçš„å¯é æ€§è¯„ä¼°æä¾›äº†å¯å¤ç°çš„è¯Šæ–­æ¡†æ¶ã€‚",
      "categories": [
        "cs.IR",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.IR",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.22242v1",
      "published_date": "2025-10-25 10:11:29 UTC",
      "updated_date": "2025-10-25 10:11:29 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T05:38:27.667256+00:00"
    },
    {
      "arxiv_id": "2510.22232v1",
      "title": "Rational Adversaries and the Maintenance of Fragility: A Game-Theoretic Theory of Rational Stagnation",
      "title_zh": "ç†æ€§å¯¹æ‰‹ä¸è„†å¼±æ€§çš„ç»´æŒï¼šç†æ€§åœæ»çš„åšå¼ˆè®ºç†è®º",
      "authors": [
        "Daisuke Hirota"
      ],
      "abstract": "Cooperative systems often remain in persistently suboptimal yet stable states. This paper explains such \"rational stagnation\" as an equilibrium sustained by a rational adversary whose utility follows the principle of potential loss, $u_{D} = U_{ideal} - U_{actual}$. Starting from the Prisoner's Dilemma, we show that the transformation $u_{i}' = a\\,u_{i} + b\\,u_{j}$ and the ratio of mutual recognition $w = b/a$ generate a fragile cooperation band $[w_{\\min},\\,w_{\\max}]$ where both (C,C) and (D,D) are equilibria. Extending to a dynamic model with stochastic cooperative payoffs $R_{t}$ and intervention costs $(C_{c},\\,C_{m})$, a Bellman-style analysis yields three strategic regimes: immediate destruction, rational stagnation, and intervention abandonment. The appendix further generalizes the utility to a reference-dependent nonlinear form and proves its stability under reference shifts, ensuring robustness of the framework. Applications to social-media algorithms and political trust illustrate how adversarial rationality can deliberately preserve fragility.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†åˆä½œç³»ç»Ÿä¸ºä½•å¸¸å¤„äºæ¬¡ä¼˜ä½†ç¨³å®šçš„çŠ¶æ€ï¼Œå¹¶æå‡ºäº†â€œç†æ€§åœæ»â€(Rational Stagnation)è¿™ä¸€åšå¼ˆè®ºæ¡†æ¶ã€‚è¯¥ç†è®ºæ ¸å¿ƒå‡è®¾å­˜åœ¨ä¸€ä¸ªç†æ€§å¯¹æ‰‹ï¼Œå…¶æ•ˆç”¨éµå¾ªæ½œåœ¨æŸå¤±åŸåˆ™(Principle of Potential Loss)ï¼Œå³ $u_{D} = U_{ideal} - U_{actual}$ã€‚é€šè¿‡å¯¹å›šå¾’å›°å¢ƒ(Prisoner's Dilemma)è¿›è¡Œçº¿æ€§å˜æ¢å¹¶å¼•å…¥ç›¸äº’è®¤å¯æ¯”ä¾‹ $w$ï¼Œç ”ç©¶æ­ç¤ºäº†ä¸€ä¸ªè„†å¼±çš„åˆä½œå¸¦ $[w_{\\min},\\,w_{\\max}]$ï¼Œåœ¨æ­¤åŒºé—´å†…åˆä½œä¸éåˆä½œå‡å¯æˆä¸ºå‡è¡¡çŠ¶æ€ã€‚ç»“åˆåŒ…å«éšæœºåˆä½œå›æŠ¥å’Œå¹²é¢„æˆæœ¬çš„åŠ¨æ€æ¨¡å‹ï¼Œè´å°”æ›¼(Bellman)åˆ†ææ¨å¯¼å‡ºå³æ—¶ç ´åã€ç†æ€§åœæ»ä¸æ”¾å¼ƒå¹²é¢„ä¸‰ç§æˆ˜ç•¥æ€åŠ¿ã€‚æœ€åï¼Œè¯¥æ¡†æ¶è¯æ˜äº†å‚è€ƒä¾èµ–çš„éçº¿æ€§æ•ˆç”¨å½¢å¼ä¸‹çš„ç¨³å®šæ€§ï¼Œå¹¶é€šè¿‡ç¤¾äº¤åª’ä½“ç®—æ³•å’Œæ”¿æ²»ä¿¡ä»»ç­‰æ¡ˆä¾‹ï¼Œé˜è¿°äº†å¯¹æ‰‹å¦‚ä½•é€šè¿‡ç†æ€§å†³ç­–è“„æ„ç»´æŒç³»ç»Ÿçš„è„†å¼±æ€§ã€‚",
      "categories": [
        "cs.GT",
        "cs.AI",
        "econ.TH"
      ],
      "primary_category": "cs.GT",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.22232v1",
      "published_date": "2025-10-25 09:28:15 UTC",
      "updated_date": "2025-10-25 09:28:15 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T05:38:36.372350+00:00"
    },
    {
      "arxiv_id": "2510.22228v1",
      "title": "When Fewer Layers Break More Chains: Layer Pruning Harms Test-Time Scaling in LLMs",
      "title_zh": "å±‚æ•°æ›´å°‘ï¼Œæ–­é“¾æ›´å¤šï¼šå±‚å‰ªææŸå®³å¤§è¯­è¨€æ¨¡å‹çš„æ¨ç†æ—¶æ‰©å±•",
      "authors": [
        "Keyu Wang",
        "Tian Lyu",
        "Guinan Su",
        "Jonas Geiping",
        "Lu Yin",
        "Marco Canini",
        "Shiwei Liu"
      ],
      "abstract": "Layer pruning has emerged as a widely adopted technique for improving the efficiency of large language models (LLMs). Although existing methods demonstrate strong performance retention on general knowledge tasks, their effect on long-chain reasoning, a more brittle yet crucial capability, remains largely unexplored. In this work, we study the impact of layer pruning on long-chain reasoning through the lens of test-time scaling, a key mechanism in modern LLMs that enables strong reasoning capacity by allocating more computation at inference time. With extensive experiments, we demonstrate that pruning even one or two layers can severely impair test-time scaling, with performance collapsing drastically on long reasoning benchmarks even when performance on knowledge-intensive and shallow reasoning tasks remains stable. Furthermore, we find that standard supervised fine-tuning remedies fail to recover test-time scaling once it has deteriorated. Through in-depth analyses, we identify the mechanisms underlying this fragility of test-time scaling and highlight the fundamental risks of applying layer pruning to reasoning-intensive LLMs. These findings call for a rethinking of layer pruning strategies and provide insights for developing methods that preserve the robustness of reasoning. We open-source the codebase in \\href{https://github.com/keyu-wang-2002/Layer-Pruning-Harms-Inference-Scaling}{https://github.com/keyu-wang-2002/Layer-Pruning-Harms-Inference-Scaling}.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº† Layer Pruning æŠ€æœ¯å¯¹å¤§å‹è¯­è¨€æ¨¡å‹ (LLMs) ä¸­ Long-chain Reasoning èƒ½åŠ›çš„å½±å“ï¼Œå¹¶é‡ç‚¹åˆ†æäº†å…¶å¯¹ Test-time Scaling è¿™ä¸€å…³é”®æœºåˆ¶çš„æŸå®³ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œå³ä½¿ä»…å‰ªæä¸€åˆ°ä¸¤ä¸ªç½‘ç»œå±‚ï¼Œä¹Ÿä¼šä¸¥é‡å‰Šå¼±æ¨¡å‹çš„æ¨ç†æ‰©å±•èƒ½åŠ›ï¼Œå¯¼è‡´å…¶åœ¨é•¿é“¾æ¨ç†ä»»åŠ¡ä¸Šçš„æ€§èƒ½å¤§å¹…ä¸‹é™ï¼Œå³ä¾¿åœ¨é€šç”¨çš„çŸ¥è¯†å¯†é›†å‹ä»»åŠ¡ä¸­è¡¨ç°ä¾ç„¶ç¨³å®šã€‚ç ”ç©¶è¿˜å‘ç°ï¼Œä¸€æ—¦ Test-time Scaling èƒ½åŠ›å› å‰ªæè€Œé€€åŒ–ï¼Œå¸¸è§„çš„ Supervised Fine-tuning è¡¥æ•‘æªæ–½ä¹Ÿæ— æ³•å°†å…¶æœ‰æ•ˆæ¢å¤ã€‚é€šè¿‡æ·±å…¥åˆ†ææ¨ç†æ‰©å±•è„†å¼±æ€§çš„åº•å±‚æœºåˆ¶ï¼Œæœ¬æ–‡æ­ç¤ºäº†åœ¨æ¨ç†å¯†é›†å‹ LLMs ä¸­ç›´æ¥åº”ç”¨ Layer Pruning çš„æ ¹æœ¬é£é™©ã€‚è¿™äº›å‘ç°å¼ºè°ƒäº†é‡æ–°è¯„ä¼°å±‚å‰ªæç­–ç•¥çš„å¿…è¦æ€§ï¼Œå¹¶ä¸ºå¼€å‘èƒ½å¤Ÿä¿æŒæ¨ç†é²æ£’æ€§çš„æ¨¡å‹å‹ç¼©æ–¹æ³•æä¾›äº†é‡è¦è§è§£ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.22228v1",
      "published_date": "2025-10-25 09:22:22 UTC",
      "updated_date": "2025-10-25 09:22:22 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T05:38:34.478609+00:00"
    },
    {
      "arxiv_id": "2511.11592v1",
      "title": "Mind Your Entropy: From Maximum Entropy to Trajectory Entropy-Constrained RL",
      "title_zh": "å…³æ³¨ç†µï¼šä»æœ€å¤§ç†µåˆ°è½¨è¿¹ç†µçº¦æŸå¼ºåŒ–å­¦ä¹ ",
      "authors": [
        "Guojian Zhan",
        "Likun Wang",
        "Pengcheng Wang",
        "Feihong Zhang",
        "Jingliang Duan",
        "Masayoshi Tomizuka",
        "Shengbo Eben Li"
      ],
      "abstract": "Maximum entropy has become a mainstream off-policy reinforcement learning (RL) framework for balancing exploitation and exploration. However, two bottlenecks still limit further performance improvement: (1) non-stationary Q-value estimation caused by jointly injecting entropy and updating its weighting parameter, i.e., temperature; and (2) short-sighted local entropy tuning that adjusts temperature only according to the current single-step entropy, without considering the effect of cumulative entropy over time. In this paper, we extends maximum entropy framework by proposing a trajectory entropy-constrained reinforcement learning (TECRL) framework to address these two challenges. Within this framework, we first separately learn two Q-functions, one associated with reward and the other with entropy, ensuring clean and stable value targets unaffected by temperature updates. Then, the dedicated entropy Q-function, explicitly quantifying the expected cumulative entropy, enables us to enforce a trajectory entropy constraint and consequently control the policy long-term stochasticity. Building on this TECRL framework, we develop a practical off-policy algorithm, DSAC-E, by extending the state-of-the-art distributional soft actor-critic with three refinements (DSAC-T). Empirical results on the OpenAI Gym benchmark demonstrate that our DSAC-E can achieve higher returns and better stability.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹æœ€å¤§ç†µ (Maximum entropy) ç¦»çº¿å¼ºåŒ–å­¦ä¹  (off-policy RL) ä¸­å­˜åœ¨çš„ Q-value ä¼°è®¡éå¹³ç¨³ä»¥åŠå±€éƒ¨ç†µè°ƒæ•´çŸ­è§†ç­‰ç“¶é¢ˆï¼Œæå‡ºäº†è½¨è¿¹ç†µçº¦æŸå¼ºåŒ–å­¦ä¹  (Trajectory Entropy-Constrained Reinforcement Learning, TECRL) æ¡†æ¶ã€‚è¯¥æ¡†æ¶é€šè¿‡åˆ†åˆ«å­¦ä¹ ä¸å¥–åŠ± (reward) å’Œç†µ (entropy) ç›¸å…³çš„ä¸¤ä¸ª Q-functionsï¼Œç¡®ä¿äº†ä»·å€¼ç›®æ ‡åœ¨æ¸©åº¦å‚æ•°æ›´æ–°æ—¶çš„ç¨³å®šæ€§ã€‚åˆ©ç”¨ä¸“é—¨çš„ç†µ Q-function é‡åŒ–æœŸæœ›ç´¯ç§¯ç†µï¼ŒTECRL èƒ½å¤Ÿæœ‰æ•ˆå®æ–½è½¨è¿¹ç†µçº¦æŸå¹¶ä»å…¨å±€è§†è§’æ§åˆ¶ç­–ç•¥çš„é•¿æ•ˆéšæœºæ€§ã€‚åŸºäºè¯¥æ¡†æ¶ï¼Œç ”ç©¶è€…é€šè¿‡å¯¹åˆ†å¸ƒå¼è½¯æ¼”å‘˜-è¯„è®ºå®¶ç®—æ³• (DSAC-T) è¿›è¡Œæ”¹è¿›ï¼Œå¼€å‘äº†å®ç”¨çš„ DSAC-E ç®—æ³•ã€‚åœ¨ OpenAI Gym åŸºå‡†æµ‹è¯•ä¸Šçš„å®éªŒç»“æœè¯æ˜ï¼ŒDSAC-E ç›¸æ¯”ç°æœ‰æŠ€æœ¯èƒ½å¤Ÿå–å¾—æ›´é«˜çš„å›æŠ¥ç‡ (returns) ä»¥åŠæ›´ä¼˜çš„è®­ç»ƒç¨³å®šæ€§ (stability)ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "stat.ML"
      ],
      "primary_category": "cs.LG",
      "comment": "17 pages",
      "pdf_url": "https://arxiv.org/pdf/2511.11592v1",
      "published_date": "2025-10-25 09:17:47 UTC",
      "updated_date": "2025-10-25 09:17:47 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T05:38:39.167703+00:00"
    },
    {
      "arxiv_id": "2510.22224v1",
      "title": "Taming Silent Failures: A Framework for Verifiable AI Reliability",
      "title_zh": "åŒ–è§£é™é»˜å¤±æ•ˆï¼šä¸€ç§å¯éªŒè¯çš„äººå·¥æ™ºèƒ½å¯é æ€§æ¡†æ¶",
      "authors": [
        "Guan-Yan Yang",
        "Farn Wang"
      ],
      "abstract": "The integration of Artificial Intelligence (AI) into safety-critical systems introduces a new reliability paradigm: silent failures, where AI produces confident but incorrect outputs that can be dangerous. This paper introduces the Formal Assurance and Monitoring Environment (FAME), a novel framework that confronts this challenge. FAME synergizes the mathematical rigor of offline formal synthesis with the vigilance of online runtime monitoring to create a verifiable safety net around opaque AI components. We demonstrate its efficacy in an autonomous vehicle perception system, where FAME successfully detected 93.5% of critical safety violations that were otherwise silent. By contextualizing our framework within the ISO 26262 and ISO/PAS 8800 standards, we provide reliability engineers with a practical, certifiable pathway for deploying trustworthy AI. FAME represents a crucial shift from accepting probabilistic performance to enforcing provable safety in next-generation systems.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å®‰å…¨å…³é”®ç³»ç»Ÿä¸­äººå·¥æ™ºèƒ½(AI)äº§ç”Ÿçš„é™é»˜æ•…éšœ(Silent Failures)é—®é¢˜ï¼Œæå‡ºäº†åä¸º FAME çš„éªŒè¯å¼å¯é æ€§æ¡†æ¶ã€‚FAME æ¡†æ¶å°†ç¦»çº¿å½¢å¼åŒ–åˆæˆ(Offline Formal Synthesis)çš„æ•°å­¦ä¸¥è°¨æ€§ä¸åœ¨çº¿è¿è¡Œæ—¶ç›‘æ§(Online Runtime Monitoring)ç›¸ç»“åˆï¼Œä¸ºä¸é€æ˜çš„ AI ç»„ä»¶æ„å»ºäº†ä¸€å±‚å¯éªŒè¯çš„å®‰å…¨ç½‘ã€‚ç ”ç©¶åœ¨è‡ªåŠ¨é©¾é©¶è½¦è¾†æ„ŸçŸ¥ç³»ç»Ÿä¸ŠéªŒè¯äº†è¯¥æ¡†æ¶çš„æœ‰æ•ˆæ€§ï¼ŒæˆåŠŸæ£€æµ‹åˆ°äº† 93.5% çš„å…³é”®å®‰å…¨è¿è§„è¡Œä¸ºã€‚é€šè¿‡å°†æ¡†æ¶ä¸ ISO 26262 å’Œ ISO/PAS 8800 æ ‡å‡†ç›¸ç»“åˆï¼Œè¯¥ç ”ç©¶ä¸ºéƒ¨ç½²å¯ä¿¡ AI æä¾›äº†ä¸€æ¡å®ç”¨çš„ã€å¯è®¤è¯çš„è·¯å¾„ã€‚FAME çš„æå‡ºæ ‡å¿—ç€äººå·¥æ™ºèƒ½åº”ç”¨ä»æ¥å—æ¦‚ç‡æ€§è¡¨ç°å‘åœ¨ä¸‹ä¸€ä»£ç³»ç»Ÿä¸­å¼ºåˆ¶æ‰§è¡Œå¯è¯æ˜å®‰å…¨æ€§(Provable Safety)çš„é‡å¤§è½¬å˜ã€‚",
      "categories": [
        "cs.SE",
        "cs.AI",
        "cs.LG",
        "cs.LO",
        "eess.SY"
      ],
      "primary_category": "cs.SE",
      "comment": "This preprint has been accepted by IEEE Reliability Magazine. 10 pages, 3 figures",
      "pdf_url": "https://arxiv.org/pdf/2510.22224v1",
      "published_date": "2025-10-25 09:07:47 UTC",
      "updated_date": "2025-10-25 09:07:47 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T05:38:42.373086+00:00"
    },
    {
      "arxiv_id": "2510.22219v1",
      "title": "Estimating the Error of Large Language Models at Pairwise Text Comparison",
      "title_zh": "å¤§è¯­è¨€æ¨¡å‹åœ¨ä¸¤ä¸¤æ–‡æœ¬æ¯”è¾ƒä¸­çš„è¯¯å·®ä¼°ç®—",
      "authors": [
        "Tianyi Li"
      ],
      "abstract": "We measure LLMs' output error at pairwise text comparison, noting the probability of error in their preferences. Our method does not rely on the ground truth and supports two scenarios: (i) uniform error rate regardless of the order of comparison, estimated with two comparisons for each text pair with either text placed first; (ii) binary positional bias assuming distinct error rates for the two orders of comparison, estimated with repeated comparisons between the texts. The Copeland counting constructs a ranking over the compared texts from pairwise preferences; the ranking reveals the poor scalability of LLM-based pairwise comparison and helps yield the estimates for LLMs' error rates. We apply the method to six LLMs (ChatGPT, Claude, DeepSeek, Gemini, Grok, Qwen) with five types of text input and obtain consistent estimates of LLMs' error. In general, the measured two positional bias terms are similar, close to the uniform error. Considering both the error rates and the robustness to the variation of prompts, Claude obtained the most desirable performance in this experiment. Our model outperforms the biased Bradley-Terry model and the commutativity score in indicating LLMs' error at this task.",
      "tldr_zh": "è¯¥ç ”ç©¶è¯„ä¼°äº†å¤§å‹è¯­è¨€æ¨¡å‹(LLMs)åœ¨æˆå¯¹æ–‡æœ¬æ¯”è¾ƒ(Pairwise Text Comparison)ä¸­çš„è¾“å‡ºè¯¯å·®ï¼Œæ—¨åœ¨ä¸ä¾èµ–åŸºå‡†çœŸå€¼(Ground Truth)çš„æƒ…å†µä¸‹é‡åŒ–å…¶åå¥½åˆ¤æ–­çš„é”™è¯¯æ¦‚ç‡ã€‚ç ”ç©¶æå‡ºäº†ä¸¤ç§è¯„ä¼°åœºæ™¯ï¼Œåˆ†åˆ«é’ˆå¯¹ä¸å—æ–‡æœ¬é¡ºåºå½±å“çš„å‡åŒ€é”™è¯¯ç‡ä»¥åŠå­˜åœ¨äºŒå…ƒä½ç½®åå·®(Binary Positional Bias)çš„æƒ…å†µï¼Œé€šè¿‡é‡å¤æ¯”è¾ƒå’Œä½ç½®äº¤æ¢è¿›è¡Œè¯¯å·®ä¼°è®¡ã€‚åˆ©ç”¨Copeland countingæ„å»ºçš„æ–‡æœ¬æ’åæ­ç¤ºäº†LLMåœ¨è¯¥ä»»åŠ¡ä¸­è¾ƒå·®çš„å¯æ‰©å±•æ€§ï¼Œå¹¶æœ‰æ•ˆè¾…åŠ©æ¨ç®—äº†å„æ¨¡å‹çš„è¯¯å·®ç‡ã€‚å®éªŒå¯¹ChatGPTã€Claudeã€DeepSeekã€Geminiã€Grokå’ŒQwenå…­ç§æ¨¡å‹è¿›è¡Œäº†è·¨é¢†åŸŸçš„æµ‹è¯•ï¼Œç»“æœæ˜¾ç¤ºClaudeåœ¨è¯¯å·®æ§åˆ¶å’Œæç¤ºè¯é²æ£’æ€§æ–¹é¢å‡è¡¨ç°å‡ºæœ€ç†æƒ³çš„æ€§èƒ½ã€‚æœ€ç»ˆè¯æ˜ï¼Œè¯¥è¯„ä¼°æ¨¡å‹åœ¨åæ˜ LLMåœ¨æˆå¯¹æ¯”è¾ƒä»»åŠ¡ä¸­çš„è¯¯å·®æ–¹é¢ï¼Œä¼˜äºåç½®çš„Bradley-Terryæ¨¡å‹å’Œäº¤æ¢æ€§å¾—åˆ†(Commutativity Score)ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "math.PR"
      ],
      "primary_category": "cs.CL",
      "comment": "14 pages, 6 figures",
      "pdf_url": "https://arxiv.org/pdf/2510.22219v1",
      "published_date": "2025-10-25 08:39:52 UTC",
      "updated_date": "2025-10-25 08:39:52 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T05:38:51.965582+00:00"
    },
    {
      "arxiv_id": "2510.22214v1",
      "title": "GALA: A GlobAl-LocAl Approach for Multi-Source Active Domain Adaptation",
      "title_zh": "GALAï¼šé¢å‘å¤šæºä¸»åŠ¨é¢†åŸŸè‡ªé€‚åº”çš„å…¨å±€-å±€éƒ¨æ–¹æ³•",
      "authors": [
        "Juepeng Zheng",
        "Peifeng Zhang",
        "Yibin Wen",
        "Qingmei Li",
        "Yang Zhang",
        "Haohuan Fu"
      ],
      "abstract": "Domain Adaptation (DA) provides an effective way to tackle target-domain tasks by leveraging knowledge learned from source domains. Recent studies have extended this paradigm to Multi-Source Domain Adaptation (MSDA), which exploits multiple source domains carrying richer and more diverse transferable information. However, a substantial performance gap still remains between adaptation-based methods and fully supervised learning. In this paper, we explore a more practical and challenging setting, named Multi-Source Active Domain Adaptation (MS-ADA), to further enhance target-domain performance by selectively acquiring annotations from the target domain. The key difficulty of MS-ADA lies in designing selection criteria that can jointly handle inter-class diversity and multi-source domain variation. To address these challenges, we propose a simple yet effective GALA strategy (GALA), which combines a global k-means clustering step for target-domain samples with a cluster-wise local selection criterion, effectively tackling the above two issues in a complementary manner. Our proposed GALA is plug-and-play and can be seamlessly integrated into existing DA frameworks without introducing any additional trainable parameters. Extensive experiments on three standard DA benchmarks demonstrate that GALA consistently outperforms prior active learning and active DA methods, achieving performance comparable to the fully-supervised upperbound while using only 1% of the target annotations.",
      "tldr_zh": "è¯¥è®ºæ–‡æ¢è®¨äº†æ›´å…·å®ç”¨ä»·å€¼ä¸”å……æ»¡æŒ‘æˆ˜çš„å¤šæºä¸»åŠ¨é¢†åŸŸè‡ªé€‚åº” (Multi-Source Active Domain Adaptation, MS-ADA) è®¾ç½®ï¼Œæ—¨åœ¨é€šè¿‡é€‰æ‹©æ€§è·å–ç›®æ ‡åŸŸæ ‡æ³¨æ¥ç¼©å°é¢†åŸŸè‡ªé€‚åº”ä¸å…¨ç›‘ç£å­¦ä¹ ä¹‹é—´çš„æ€§èƒ½å·®è·ã€‚é’ˆå¯¹ MS-ADA ä¸­éš¾ä»¥å…¼é¡¾ç±»é—´å¤šæ ·æ€§ (inter-class diversity) ä¸å¤šæºé¢†åŸŸå·®å¼‚ (multi-source domain variation) çš„éš¾é¢˜ï¼Œç ”ç©¶è€…æå‡ºäº†åä¸º GALA (GlobAl-LocAl) çš„ç®€å•æœ‰æ•ˆç­–ç•¥ã€‚è¯¥ç­–ç•¥å·§å¦™ç»“åˆäº†é’ˆå¯¹ç›®æ ‡åŸŸæ ·æœ¬çš„å…¨å±€ k-means èšç±»ä¸ç°‡å†…å±€éƒ¨é€‰æ‹©æ ‡å‡†ï¼Œä»¥äº’è¡¥æ–¹å¼ååŒè§£å†³ä¸Šè¿°ä¸¤é¡¹æ ¸å¿ƒæŒ‘æˆ˜ã€‚GALA å…·å¤‡å³æ’å³ç”¨çš„ç‰¹æ€§ï¼Œæ— éœ€å¼•å…¥é¢å¤–å¯è®­ç»ƒå‚æ•°å³å¯æ— ç¼é›†æˆè‡³ç°æœ‰é¢†åŸŸè‡ªé€‚åº” (Domain Adaptation) æ¡†æ¶ä¸­ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒGALA åœ¨ä¸‰ä¸ªæ ‡å‡†åŸºå‡†æµ‹è¯•ä¸­å‡ä¼˜äºå…ˆå‰çš„ä¸»åŠ¨å­¦ä¹ åŠä¸»åŠ¨ DA æ–¹æ³•ã€‚ä»¤äººç©ç›®çš„æ˜¯ï¼Œåœ¨ä»…ä½¿ç”¨ 1% ç›®æ ‡åŸŸæ ‡æ³¨çš„æƒ…å†µä¸‹ï¼Œè¯¥æ–¹æ³•ä¾¿å®ç°äº†ä¸å…¨ç›‘ç£å­¦ä¹ ä¸Šé™ç›¸å½“çš„æ€§èƒ½æ°´å¹³ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.22214v1",
      "published_date": "2025-10-25 08:26:45 UTC",
      "updated_date": "2025-10-25 08:26:45 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T05:38:49.392536+00:00"
    },
    {
      "arxiv_id": "2510.22210v1",
      "title": "LSPRAG: LSP-Guided RAG for Language-Agnostic Real-Time Unit Test Generation",
      "title_zh": "LSPRAGï¼šé¢å‘è¯­è¨€æ— å…³å®æ—¶å•å…ƒæµ‹è¯•ç”Ÿæˆçš„ LSP å¼•å¯¼å‹ RAG",
      "authors": [
        "Gwihwan Go",
        "Quan Zhang",
        "Chijin Zhou",
        "Zhao Wei",
        "Yu Jiang"
      ],
      "abstract": "Automated unit test generation is essential for robust software development, yet existing approaches struggle to generalize across multiple programming languages and operate within real-time development. While Large Language Models (LLMs) offer a promising solution, their ability to generate high coverage test code depends on prompting a concise context of the focal method. Current solutions, such as Retrieval-Augmented Generation, either rely on imprecise similarity-based searches or demand the creation of costly, language-specific static analysis pipelines. To address this gap, we present LSPRAG, a framework for concise-context retrieval tailored for real-time, language-agnostic unit test generation. LSPRAG leverages off-the-shelf Language Server Protocol (LSP) back-ends to supply LLMs with precise symbol definitions and references in real time. By reusing mature LSP servers, LSPRAG provides an LLM with language-aware context retrieval, requiring minimal per-language engineering effort. We evaluated LSPRAG on open-source projects spanning Java, Go, and Python. Compared to the best performance of baselines, LSPRAG increased line coverage by up to 174.55% for Golang, 213.31% for Java, and 31.57% for Python.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†LSPRAGï¼Œè¿™æ˜¯ä¸€ä¸ªé’ˆå¯¹è¯­è¨€æ— å…³(language-agnostic)ä¸”å®æ—¶çš„å•å…ƒæµ‹è¯•ç”Ÿæˆä»»åŠ¡è®¾è®¡çš„æ£€ç´¢å¢å¼ºç”Ÿæˆ(RAG)æ¡†æ¶ã€‚ä¼ ç»Ÿçš„è‡ªåŠ¨åŒ–æµ‹è¯•ç”Ÿæˆæ–¹æ³•åœ¨å¤„ç†å¤šç¼–ç¨‹è¯­è¨€å’Œå®æ—¶å¼€å‘æ—¶é¢ä¸´å›°éš¾ï¼Œè€Œç°æœ‰çš„RAGæ–¹æ¡ˆå¾€å¾€ä¾èµ–ä¸ç²¾ç¡®çš„ç›¸ä¼¼åº¦æœç´¢æˆ–é«˜æˆæœ¬çš„é™æ€åˆ†æã€‚LSPRAGé€šè¿‡åˆ©ç”¨ç°æˆçš„è¯­è¨€æœåŠ¡å™¨åè®®(LSP)åç«¯ï¼Œä¸ºå¤§è¯­è¨€æ¨¡å‹(LLMs)å®æ—¶æä¾›ç²¾ç¡®çš„ç¬¦å·å®šä¹‰å’Œå¼•ç”¨ã€‚è¯¥æ¡†æ¶é€šè¿‡å¤ç”¨æˆç†Ÿçš„LSPæœåŠ¡å™¨ï¼Œå®ç°äº†å…·å¤‡è¯­è¨€æ„ŸçŸ¥èƒ½åŠ›çš„ä¸Šä¸‹æ–‡æ£€ç´¢ï¼ŒåŒæ—¶æœ€å¤§é™åº¦åœ°å‡å°‘äº†é’ˆå¯¹ç‰¹å®šç¼–ç¨‹è¯­è¨€çš„å·¥ç¨‹å¼€å‘å·¥ä½œã€‚å®éªŒåœ¨Javaã€Goå’ŒPythonçš„å¼€æºé¡¹ç›®ä¸Šè¿›è¡Œäº†è¯„ä¼°ï¼Œç»“æœæ˜¾ç¤ºLSPRAGç›¸æ¯”åŸºå‡†æ¨¡å‹æ˜¾è‘—æå‡äº†ä»£ç è¡Œè¦†ç›–ç‡(line coverage)ã€‚å…·ä½“è€Œè¨€ï¼ŒLSPRAGåœ¨Golangã€Javaå’ŒPythonä¸Šçš„è¦†ç›–ç‡åˆ†åˆ«æå‡äº†é«˜è¾¾174.55%ã€213.31%å’Œ31.57%ï¼Œè¯æ˜äº†å…¶åœ¨å¤šè¯­è¨€å®æ—¶ç”Ÿæˆåœºæ™¯ä¸‹çš„å“è¶Šæ•ˆèƒ½ã€‚",
      "categories": [
        "cs.SE",
        "cs.AI"
      ],
      "primary_category": "cs.SE",
      "comment": "13pages, 6 figures",
      "pdf_url": "https://arxiv.org/pdf/2510.22210v1",
      "published_date": "2025-10-25 08:19:21 UTC",
      "updated_date": "2025-10-25 08:19:21 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T05:39:04.671689+00:00"
    },
    {
      "arxiv_id": "2510.22206v1",
      "title": "Right Place, Right Time: Market Simulation-based RL for Execution Optimisation",
      "title_zh": "é€‚æ—¶é€‚ä½ï¼šåŸºäºå¸‚åœºæ¨¡æ‹Ÿçš„å¼ºåŒ–å­¦ä¹ æ‰§è¡Œä¼˜åŒ–",
      "authors": [
        "Ollie Olby",
        "Andreea Bacalum",
        "Rory Baggott",
        "Namid Stillman"
      ],
      "abstract": "Execution algorithms are vital to modern trading, they enable market participants to execute large orders while minimising market impact and transaction costs. As these algorithms grow more sophisticated, optimising them becomes increasingly challenging. In this work, we present a reinforcement learning (RL) framework for discovering optimal execution strategies, evaluated within a reactive agent-based market simulator. This simulator creates reactive order flow and allows us to decompose slippage into its constituent components: market impact and execution risk. We assess the RL agent's performance using the efficient frontier based on work by Almgren and Chriss, measuring its ability to balance risk and cost. Results show that the RL-derived strategies consistently outperform baselines and operate near the efficient frontier, demonstrating a strong ability to optimise for risk and impact. These findings highlight the potential of reinforcement learning as a powerful tool in the trader's toolkit.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ä¸ªåŸºäºå¼ºåŒ–å­¦ä¹ (Reinforcement Learning)çš„æ¡†æ¶ï¼Œæ—¨åœ¨é€šè¿‡ååº”æ€§æ™ºèƒ½ä½“å¸‚åœºæ¨¡æ‹Ÿå™¨(reactive agent-based market simulator)æ¥å‘ç°æœ€ä¼˜çš„äº¤æ˜“æ‰§è¡Œç­–ç•¥ã€‚è¯¥æ¨¡æ‹Ÿå™¨èƒ½å¤Ÿç”Ÿæˆååº”æ€§è®¢å•æµï¼Œå¹¶å…è®¸ç ”ç©¶äººå‘˜å°†æ»‘ç‚¹(slippage)åˆ†è§£ä¸ºå¸‚åœºå½±å“(market impact)å’Œæ‰§è¡Œé£é™©(execution risk)ä¸¤ä¸ªç»„æˆéƒ¨åˆ†ã€‚é€šè¿‡åˆ©ç”¨åŸºäºAlmgrenå’ŒChrissç ”ç©¶çš„æœ‰æ•ˆç‡å‰æ²¿(efficient frontier)ï¼Œè¯¥æ¡†æ¶è¡¡é‡äº†æ™ºèƒ½ä½“å¹³è¡¡é£é™©ä¸æˆæœ¬çš„èƒ½åŠ›ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œå¼ºåŒ–å­¦ä¹ é©±åŠ¨çš„ç­–ç•¥åœ¨è¡¨ç°ä¸Šå§‹ç»ˆä¼˜äºåŸºçº¿æ¨¡å‹ï¼Œä¸”éå¸¸æ¥è¿‘æœ‰æ•ˆç‡å‰æ²¿ï¼Œè¯æ˜äº†å…¶åœ¨ä¼˜åŒ–é£é™©å’Œå½±å“æ–¹é¢çš„å“è¶Šæ€§èƒ½ã€‚è¿™äº›å‘ç°çªæ˜¾äº†å¼ºåŒ–å­¦ä¹ åœ¨é‡‘èäº¤æ˜“å·¥å…·ç®±ä¸­çš„å·¨å¤§æ½œåŠ›ï¼Œä¸ºç°ä»£å¤æ‚äº¤æ˜“æ‰§è¡Œç®—æ³•çš„ä¼˜åŒ–æä¾›äº†å¼ºæœ‰åŠ›çš„æŠ€æœ¯æ”¯æ’‘ã€‚",
      "categories": [
        "q-fin.CP",
        "cs.AI",
        "cs.LG",
        "q-fin.RM",
        "q-fin.TR"
      ],
      "primary_category": "q-fin.CP",
      "comment": "8 pages, 4 figures, accepted to ICAIF 2025",
      "pdf_url": "https://arxiv.org/pdf/2510.22206v1",
      "published_date": "2025-10-25 08:10:18 UTC",
      "updated_date": "2025-10-25 08:10:18 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T05:39:05.877101+00:00"
    },
    {
      "arxiv_id": "2510.22204v1",
      "title": "Bridging Perception and Reasoning: Dual-Pipeline Neuro-Symbolic Landing for UAVs in Cluttered Environments",
      "title_zh": "è¿æ¥æ„ŸçŸ¥ä¸æ¨ç†ï¼šå¤æ‚ç¯å¢ƒä¸‹æ— äººæœºçš„åŒæµæ°´çº¿ç¥ç»ç¬¦å·åŒ–ç€é™†",
      "authors": [
        "Weixian Qian",
        "Sebastian Schroder",
        "Yao Deng",
        "Jiaohong Yao",
        "Linfeng Liang",
        "Xiao Cheng",
        "Richard Han",
        "Xi Zheng"
      ],
      "abstract": "Autonomous landing in unstructured (cluttered, uneven, and map-poor) environments is a core requirement for Unmanned Aerial Vehicles (UAVs), yet purely vision-based or deep learning models often falter under covariate shift and provide limited interpretability. We propose NeuroSymLand, a neuro-symbolic framework that tightly couples two complementary pipelines: (i) an offline pipeline, where Large Language Models (LLMs) and human-in-the-loop refinement synthesize Scallop code from diverse landing scenarios, distilling generalizable and verifiable symbolic knowledge; and (ii) an online pipeline, where a compact foundation-based semantic segmentation model generates probabilistic Scallop facts that are composed into semantic scene graphs for real-time deductive reasoning. This design combines the perceptual strengths of lightweight foundation models with the interpretability and verifiability of symbolic reasoning. Node attributes (e.g., flatness, area) and edge relations (adjacency, containment, proximity) are computed with geometric routines rather than learned, avoiding the data dependence and latency of train-time graph builders. The resulting Scallop program encodes landing principles (avoid water and obstacles; prefer large, flat, accessible regions) and yields calibrated safety scores with ranked Regions of Interest (ROIs) and human-readable justifications. Extensive evaluations across datasets, diverse simulation maps, and real UAV hardware show that NeuroSymLand achieves higher accuracy, stronger robustness to covariate shift, and superior efficiency compared with state-of-the-art baselines, while advancing UAV safety and reliability in emergency response, surveillance, and delivery missions.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† NeuroSymLandï¼Œä¸€ç§é¢å‘æ— äººæœº(UAVs)åœ¨æ— ç»“æ„ä¸”æ‚ä¹±ç¯å¢ƒä¸­è‡ªä¸»é™è½çš„ç¥ç»ç¬¦å·(neuro-symbolic)æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³çº¯è§†è§‰æˆ–æ·±åº¦å­¦ä¹ æ¨¡å‹åœ¨åå˜é‡åç§»(covariate shift)ä¸‹è¡¨ç°ä¸ä½³ä¸”å¯è§£é‡Šæ€§å—é™çš„é—®é¢˜ã€‚è¯¥æ¡†æ¶ç”±ç¦»çº¿å’Œåœ¨çº¿ä¸¤ä¸ªäº’è¡¥çš„æµæ°´çº¿ç»„æˆï¼šç¦»çº¿æµæ°´çº¿åˆ©ç”¨å¤§è¯­è¨€æ¨¡å‹(LLMs)å’Œäººæœºåä½œåˆæˆ Scallop ä»£ç ï¼Œä»¥æå–å¯æ³›åŒ–ä¸”å¯éªŒè¯çš„ç¬¦å·çŸ¥è¯†ï¼›åœ¨çº¿æµæ°´çº¿åˆ™é€šè¿‡è¯­ä¹‰åˆ†å‰²æ¨¡å‹ç”Ÿæˆæ¦‚ç‡æ€§çš„ Scallop äº‹å®ï¼Œå¹¶æ„å»ºè¯­ä¹‰åœºæ™¯å›¾è¿›è¡Œå®æ—¶æ¼”ç»æ¨ç†ã€‚é€šè¿‡ä½¿ç”¨å‡ ä½•ä¾‹ç¨‹è€Œéå­¦ä¹ æ–¹å¼æ¥è®¡ç®—èŠ‚ç‚¹å±æ€§ä¸è¾¹ç¼˜å…³ç³»ï¼Œè¯¥æ–¹æ³•é¿å…äº†è®­ç»ƒæ—¶å›¾æ„å»ºå™¨çš„æ•°æ®ä¾èµ–å’Œå»¶è¿Ÿã€‚Scallop ç¨‹åºèƒ½å¤Ÿå¯¹é™è½åŸåˆ™è¿›è¡Œç¼–ç ï¼Œå¹¶è¾“å‡ºç»è¿‡æ ¡å‡†çš„å®‰å…¨åˆ†æ•°ã€æ’åºåçš„æ„Ÿå…´è¶£åŒºåŸŸ(Regions of Interest, ROIs)ä»¥åŠäººç±»å¯è¯»çš„æ¨ç†ä¾æ®ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒNeuroSymLand åœ¨å‡†ç¡®æ€§ã€é²æ£’æ€§å’Œæ•ˆç‡ä¸Šå‡ä¼˜äºæœ€å…ˆè¿›çš„åŸºå‡†æ¨¡å‹ï¼Œæœ‰æ•ˆæå‡äº†æ— äººæœºåœ¨åº”æ€¥å“åº”ã€ç›‘æ§åŠé€’é€ä»»åŠ¡ä¸­çš„å®‰å…¨æ€§å’Œå¯é æ€§ã€‚",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.22204v1",
      "published_date": "2025-10-25 08:08:04 UTC",
      "updated_date": "2025-10-25 08:08:04 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T05:39:10.201594+00:00"
    },
    {
      "arxiv_id": "2510.22197v1",
      "title": "Multi-dataset Joint Pre-training of Emotional EEG Enables Generalizable Affective Computing",
      "title_zh": "æƒ…æ„Ÿè„‘ç”µå¤šæ•°æ®é›†è”åˆé¢„è®­ç»ƒåŠ©åŠ›å¯æ³›åŒ–æƒ…æ„Ÿè®¡ç®—",
      "authors": [
        "Qingzhu Zhang",
        "Jiani Zhong",
        "Zongsheng Li",
        "Xinke Shen",
        "Quanying Liu"
      ],
      "abstract": "Task-specific pre-training is essential when task representations diverge from generic pre-training features. Existing task-general pre-training EEG models struggle with complex tasks like emotion recognition due to mismatches between task-specific features and broad pre-training approaches. This work aims to develop a task-specific multi-dataset joint pre-training framework for cross-dataset emotion recognition, tackling problems of large inter-dataset distribution shifts, inconsistent emotion category definitions, and substantial inter-subject variability. We introduce a cross-dataset covariance alignment loss to align second-order statistical properties across datasets, enabling robust generalization without the need for extensive labels or per-subject calibration. To capture the long-term dependency and complex dynamics of EEG, we propose a hybrid encoder combining a Mamba-like linear attention channel encoder and a spatiotemporal dynamics model. Our method outperforms state-of-the-art large-scale EEG models by an average of 4.57% in AUROC for few-shot emotion recognition and 11.92% in accuracy for zero-shot generalization to a new dataset. Performance scales with the increase of datasets used in pre-training. Multi-dataset joint pre-training achieves a performance gain of 8.55% over single-dataset training. This work provides a scalable framework for task-specific pre-training and highlights its benefit in generalizable affective computing. Our code is available at https://github.com/ncclab-sustech/mdJPT_nips2025.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§é’ˆå¯¹ä»»åŠ¡ç‰¹å®šçš„å¤šæ•°æ®é›†è”åˆé¢„è®­ç»ƒæ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³è„‘ç”µï¼ˆEEGï¼‰æƒ…æ„Ÿè¯†åˆ«ä¸­è·¨æ•°æ®é›†åˆ†å¸ƒåç§»ã€ç±»åˆ«å®šä¹‰ä¸ä¸€è‡´ä»¥åŠæ˜¾è‘—çš„ä¸ªä½“å·®å¼‚ç­‰æŒ‘æˆ˜ã€‚ç ”ç©¶å¼•å…¥äº†è·¨æ•°æ®é›†åæ–¹å·®å¯¹é½æŸå¤±ï¼ˆcross-dataset covariance alignment lossï¼‰ï¼Œé€šè¿‡å¯¹é½äºŒé˜¶ç»Ÿè®¡ç‰¹æ€§å®ç°é²æ£’çš„æ³›åŒ–èƒ½åŠ›ï¼Œè€Œæ— éœ€å¤§é‡æ ‡ç­¾æˆ–é€å—è¯•è€…æ ¡å‡†ã€‚åœ¨æ¶æ„ä¸Šï¼Œè¯¥æ–¹æ³•é‡‡ç”¨äº†ä¸€ç§ç»“åˆç±»Mambaçº¿æ€§æ³¨æ„åŠ›é€šé“ç¼–ç å™¨ä¸æ—¶ç©ºåŠ¨åŠ›å­¦æ¨¡å‹çš„æ··åˆç¼–ç å™¨ï¼Œä»¥æœ‰æ•ˆæ•æ‰EEGä¿¡å·çš„é•¿æœŸä¾èµ–æ€§å’Œå¤æ‚åŠ¨åŠ›å­¦ç‰¹å¾ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å°‘æ ·æœ¬æƒ…æ„Ÿè¯†åˆ«çš„AUROCä¸Šæ¯”ç°æœ‰å…ˆè¿›çš„å¤§è§„æ¨¡EEGæ¨¡å‹å¹³å‡é«˜å‡º4.57%ï¼Œåœ¨é¢å‘æ–°æ•°æ®é›†çš„é›¶æ ·æœ¬æ³›åŒ–å‡†ç¡®ç‡ä¸Šæå‡äº†11.92%ã€‚æ­¤å¤–ï¼Œå¤šæ•°æ®é›†è”åˆé¢„è®­ç»ƒç›¸è¾ƒäºå•æ•°æ®é›†è®­ç»ƒå®ç°äº†8.55%çš„æ€§èƒ½å¢ç›Šï¼Œè¯æ˜äº†è¯¥æ¡†æ¶åœ¨å¯æ³›åŒ–æƒ…æ„Ÿè®¡ç®—ï¼ˆgeneralizable affective computingï¼‰é¢†åŸŸå…·æœ‰å‡ºè‰²çš„æ€§èƒ½ä¸å¯æ‰©å±•æ€§ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "q-bio.NC"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.22197v1",
      "published_date": "2025-10-25 07:30:24 UTC",
      "updated_date": "2025-10-25 07:30:24 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T05:39:16.870545+00:00"
    },
    {
      "arxiv_id": "2510.22196v1",
      "title": "Scaling Non-Parametric Sampling with Representation",
      "title_zh": "åˆ©ç”¨è¡¨ç¤ºå®ç°éå‚æ•°åŒ–é‡‡æ ·çš„è§„æ¨¡åŒ–",
      "authors": [
        "Vincent Lu",
        "Aaron Truong",
        "Zeyu Yun",
        "Yubei Chen"
      ],
      "abstract": "Scaling and architectural advances have produced strikingly photorealistic image generative models, yet their mechanisms still remain opaque. Rather than advancing scaling, our goal is to strip away complicated engineering tricks and propose a simple, non-parametric generative model. Our design is grounded in three principles of natural images-(i) spatial non-stationarity, (ii) low-level regularities, and (iii) high-level semantics-and defines each pixel's distribution from its local context window. Despite its minimal architecture and no training, the model produces high-fidelity samples on MNIST and visually compelling CIFAR-10 images. This combination of simplicity and strong empirical performance points toward a minimal theory of natural-image structure. The model's white-box nature also allows us to have a mechanistic understanding of how the model generalizes and generates diverse images. We study it by tracing each generated pixel back to its source images. These analyses reveal a simple, compositional procedure for \"part-whole generalization\", suggesting a hypothesis for how large neural network generative models learn to generalize.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† Scaling Non-Parametric Sampling with Representationï¼Œè¿™æ˜¯ä¸€ç§ç®€å•çš„éå‚æ•°ç”Ÿæˆæ¨¡å‹ï¼Œæ—¨åœ¨é€šè¿‡æ‘’å¼ƒå¤æ‚çš„å·¥ç¨‹æ‰‹æ®µæ¥æ¢ç´¢è‡ªç„¶å›¾åƒç»“æ„çš„æç®€ç†è®ºã€‚è¯¥è®¾è®¡åŸºäºç©ºé—´éå¹³ç¨³æ€§ (spatial non-stationarity)ã€ä½å±‚è§„å¾‹æ€§ (low-level regularities) å’Œé«˜å±‚è¯­ä¹‰ (high-level semantics) ä¸‰é¡¹åŸåˆ™ï¼Œåˆ©ç”¨å±€éƒ¨ä¸Šä¸‹æ–‡çª—å£å®šä¹‰åƒç´ åˆ†å¸ƒã€‚å°½ç®¡é‡‡ç”¨äº†æç®€æ¶æ„ä¸”æ— éœ€è®­ç»ƒï¼Œè¯¥æ¨¡å‹åœ¨ MNIST å’Œ CIFAR-10 æ•°æ®é›†ä¸Šå±•ç°äº†å“è¶Šçš„æ€§èƒ½ï¼Œèƒ½å¤Ÿç”Ÿæˆé«˜ä¿çœŸä¸”è§†è§‰æ•ˆæœå‡ºä¼—çš„å›¾åƒã€‚å¾—ç›Šäºå…¶ç™½ç›’ (white-box) ç‰¹æ€§ï¼Œç ”ç©¶è€…èƒ½å¤Ÿé€šè¿‡è¿½è¸ªç”Ÿæˆåƒç´ çš„æ¥æºå›¾åƒæ¥æ·±å…¥æ¢ç©¶æ¨¡å‹çš„æ³›åŒ–æœºåˆ¶ã€‚åˆ†æç»“æœæ­ç¤ºäº†ä¸€ä¸ªç®€å•çš„â€œéƒ¨åˆ†-æ•´ä½“æ³›åŒ–â€ (part-whole generalization) ç»„åˆè¿‡ç¨‹ï¼Œä¸ºç†è§£å¤§å‹ç¥ç»ç½‘ç»œç”Ÿæˆæ¨¡å‹å¦‚ä½•å®ç°æ³›åŒ–æä¾›äº†é‡è¦çš„ç†è®ºå‡è®¾ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.22196v1",
      "published_date": "2025-10-25 07:29:26 UTC",
      "updated_date": "2025-10-25 07:29:26 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T05:39:16.702826+00:00"
    },
    {
      "arxiv_id": "2510.22192v1",
      "title": "OptiTree: Hierarchical Thoughts Generation with Tree Search for LLM Optimization Modeling",
      "title_zh": "OptiTreeï¼šåŸºäºæ ‘æœç´¢çš„å¤§è¯­è¨€æ¨¡å‹ä¼˜åŒ–å»ºæ¨¡å±‚çº§åŒ–æ€ç»´ç”Ÿæˆ",
      "authors": [
        "Haoyang Liu",
        "Jie Wang",
        "Yuyang Cai",
        "Xiongwei Han",
        "Yufei Kuang",
        "Jianye Hao"
      ],
      "abstract": "Optimization modeling is one of the most crucial but technical parts of operations research (OR). To automate the modeling process, existing works have leveraged large language models (LLMs), prompting them to break down tasks into steps for generating variables, constraints, and objectives. However, due to the highly complex mathematical structures inherent in OR problems, standard fixed-step decomposition often fails to achieve high performance. To address this challenge, we introduce OptiTree, a novel tree search approach designed to enhance modeling capabilities for complex problems through adaptive problem decomposition into simpler subproblems. Specifically, we develop a modeling tree that organizes a wide range of OR problems based on their hierarchical problem taxonomy and complexity, with each node representing a problem category and containing relevant high-level modeling thoughts. Given a problem to model, we recurrently search the tree to identify a series of simpler subproblems and synthesize the global modeling thoughts by adaptively integrating the hierarchical thoughts. Experiments show that OptiTree significantly improves the modeling accuracy compared to the state-of-the-art, achieving over 10\\% improvements on the challenging benchmarks. The code is released at https://github.com/MIRALab-USTC/OptiTree/tree/main.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†OptiTreeï¼Œè¿™æ˜¯ä¸€ç§æ—¨åœ¨é€šè¿‡å°†å¤æ‚é—®é¢˜è‡ªé€‚åº”åˆ†è§£ä¸ºç®€å•å­é—®é¢˜æ¥å¢å¼ºå¤§è¯­è¨€æ¨¡å‹(LLMs)åœ¨è¿ç­¹å­¦(operations research, OR)ä¸­ä¼˜åŒ–å»ºæ¨¡(Optimization modeling)èƒ½åŠ›çš„æ–°å‹æ ‘æœç´¢æ–¹æ³•(tree search approach)ã€‚é’ˆå¯¹ä¼ ç»Ÿå›ºå®šæ­¥éª¤åˆ†è§£éš¾ä»¥å¤„ç†å¤æ‚æ•°å­¦ç»“æ„çš„é—®é¢˜ï¼Œè¯¥æ¡†æ¶æ„å»ºäº†ä¸€ä¸ªåŸºäºå±‚æ¬¡åŒ–é—®é¢˜åˆ†ç±»å’Œå¤æ‚æ€§ç»„ç»‡çš„å»ºæ¨¡æ ‘(modeling tree)ï¼Œæ¯ä¸ªèŠ‚ç‚¹ä»£è¡¨ä¸€ä¸ªé—®é¢˜ç±»åˆ«å¹¶åŒ…å«ç›¸å…³çš„é«˜å±‚å»ºæ¨¡æ€è·¯ã€‚åœ¨å»ºæ¨¡è¿‡ç¨‹ä¸­ï¼Œç³»ç»Ÿé€šè¿‡é€’å½’æœç´¢æ ‘ç»“æ„æ¥è¯†åˆ«ä¸€ç³»åˆ—æ›´ç®€å•çš„å­é—®é¢˜ï¼Œå¹¶è‡ªé€‚åº”é›†æˆå±‚æ¬¡åŒ–æ€è·¯ä»¥åˆæˆå…¨å±€å»ºæ¨¡æ€è·¯(global modeling thoughts)ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒOptiTreeåœ¨å…·æœ‰æŒ‘æˆ˜æ€§çš„åŸºå‡†æµ‹è¯•ä¸­æ˜¾è‘—æé«˜äº†å»ºæ¨¡å‡†ç¡®ç‡ï¼Œè¾ƒç°æœ‰æœ€å…ˆè¿›æ–¹æ³•æå‡äº†10%ä»¥ä¸Šã€‚ç›®å‰ï¼Œè¯¥ç ”ç©¶çš„ä»£ç å·²åœ¨GitHubå¼€æºï¼Œä¸ºè‡ªåŠ¨åŒ–å¤æ‚è¿ç­¹å­¦å»ºæ¨¡ä»»åŠ¡æä¾›äº†æœ‰æ•ˆçš„è§£å†³æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "Published at NeurIPS 2025",
      "pdf_url": "https://arxiv.org/pdf/2510.22192v1",
      "published_date": "2025-10-25 07:19:16 UTC",
      "updated_date": "2025-10-25 07:19:16 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T05:39:17.758931+00:00"
    },
    {
      "arxiv_id": "2510.22178v1",
      "title": "Dopamine-driven synaptic credit assignment in neural networks",
      "title_zh": "ç¥ç»ç½‘ç»œä¸­å¤šå·´èƒºé©±åŠ¨çš„çªè§¦ä¿¡ç”¨åˆ†é…",
      "authors": [
        "Saranraj Nambusubramaniyan",
        "Shervin Safavi",
        "Raja Guru",
        "Andreas Knoblauch"
      ],
      "abstract": "Solving the synaptic Credit Assignment Problem(CAP) is central to learning in both biological and artificial neural systems. Finding an optimal solution for synaptic CAP means setting the synaptic weights that assign credit to each neuron for influencing the final output and behavior of neural networks or animals. Gradient-based methods solve this problem in artificial neural networks using back-propagation, however, not in the most efficient way. For instance, back-propagation requires a chain of top-down gradient computations. This leads to an expensive optimization process in terms of computing power and memory linked with well-known weight transport and update locking problems. To address these shortcomings, we take a NeuroAI approach and draw inspiration from neural Reinforcement Learning to develop a derivative-free optimizer for training neural networks, Dopamine. Dopamine is developed for Weight Perturbation (WP) learning that exploits stochastic updating of weights towards optima. It achieves this by minimizing the regret, a form of Reward Prediction Error (RPE) between the expected outcome from the perturbed model and the actual outcome from the unperturbed model. We use this RPE to adjust the learning rate in the network (i.e., creating an adaptive learning rate strategy, similar to the role of dopamine in the brain). We tested the Dopamine optimizer for training multi-layered perceptrons for XOR tasks, and recurrent neural networks for chaotic time series forecasting. Dopamine-trained models demonstrate accelerated convergence and outperform standard WP, and give comparable performance to gradient-based algorithms, while consuming significantly less computation and memory. Overall, the Dopamine optimizer not only finds robust solutions and comparable performance to the state-of-the-art Machine Learning optimizers but is also neurobiologically more plausible.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§åä¸ºDopamineçš„æ— å¯¼æ•°ä¼˜åŒ–å™¨(derivative-free optimizer)ï¼Œæ—¨åœ¨è§£å†³ç¥ç»ç½‘ç»œä¸­çš„çªè§¦ä¿¡ç”¨åˆ†é…é—®é¢˜(synaptic Credit Assignment Problem)ã€‚é’ˆå¯¹ä¼ ç»Ÿçš„åå‘ä¼ æ’­(back-propagation)ç®—æ³•åœ¨è®¡ç®—èµ„æºå’Œå†…å­˜æ¶ˆè€—æ–¹é¢çš„ç“¶é¢ˆï¼ŒDopamineå€Ÿé‰´ç¥ç»å¼ºåŒ–å­¦ä¹ (neural Reinforcement Learning)å¼€å‘äº†åŸºäºæƒé‡æ‰°åŠ¨(Weight Perturbation)å­¦ä¹ çš„ä¼˜åŒ–ç­–ç•¥ã€‚è¯¥æ–¹æ³•é€šè¿‡æœ€å°åŒ–å¥–åŠ±é¢„æµ‹è¯¯å·®(Reward Prediction Error, RPE)æ¥åŠ¨æ€è°ƒèŠ‚ç½‘ç»œå­¦ä¹ ç‡ï¼Œæœ‰æ•ˆåœ°æ¨¡æ‹Ÿäº†å¤§è„‘ä¸­å¤šå·´èƒºçš„åŠŸèƒ½ã€‚å®éªŒè¡¨æ˜ï¼ŒDopamineåœ¨å¤šå±‚æ„ŸçŸ¥æœºæ‰§è¡ŒXORä»»åŠ¡å’Œå¾ªç¯ç¥ç»ç½‘ç»œé¢„æµ‹æ··æ²Œæ—¶é—´åºåˆ—ä¸­å®ç°äº†åŠ é€Ÿæ”¶æ•›ï¼Œåœ¨æ€§èƒ½ä¸æ¢¯åº¦ç®—æ³•ç›¸å½“çš„åŒæ—¶æ˜¾è‘—å‡å°‘äº†è®¡ç®—å¼€é”€ã€‚æ€»ä¹‹ï¼ŒDopamineä¼˜åŒ–å™¨ä¸ä»…å±•ç°å‡ºä¼˜å¼‚çš„é²æ£’æ€§ï¼Œè¿˜ä¸ºç¥ç»ç½‘ç»œè®­ç»ƒæä¾›äº†ä¸€ç§æ›´ç¬¦åˆç¥ç»ç”Ÿç‰©å­¦åŸç†(neurobiologically plausible)çš„é«˜æ•ˆæ–¹æ¡ˆã€‚",
      "categories": [
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.22178v1",
      "published_date": "2025-10-25 06:17:49 UTC",
      "updated_date": "2025-10-25 06:17:49 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T05:39:32.172981+00:00"
    },
    {
      "arxiv_id": "2510.22170v1",
      "title": "Measure what Matters: Psychometric Evaluation of AI with Situational Judgment Tests",
      "title_zh": "è¡¡é‡å…³é”®è¦ç´ ï¼šåŸºäºæƒ…å¢ƒåˆ¤æ–­æµ‹éªŒçš„äººå·¥æ™ºèƒ½å¿ƒç†æµ‹é‡è¯„ä¼°",
      "authors": [
        "Alexandra Yost",
        "Shreyans Jain",
        "Shivam Raval",
        "Grant Corser",
        "Allen Roush",
        "Nina Xu",
        "Jacqueline Hammack",
        "Ravid Shwartz-Ziv",
        "Amirali Abdullah"
      ],
      "abstract": "AI psychometrics evaluates AI systems in roles that traditionally require emotional judgment and ethical consideration. Prior work often reuses human trait inventories (Big Five, \\hexaco) or ad hoc personas, limiting behavioral realism and domain relevance. We propose a framework that (1) uses situational judgment tests (SJTs) from realistic scenarios to probe domain-specific competencies; (2) integrates industrial-organizational and personality psychology to design sophisticated personas which include behavioral and psychological descriptors, life history, and social and emotional functions; and (3) employs structured generation with population demographic priors and memoir inspired narratives, encoded with Pydantic schemas. In a law enforcement assistant case study, we construct a rich dataset of personas drawn across 8 persona archetypes and SJTs across 11 attributes, and analyze behaviors across subpopulation and scenario slices. The dataset spans 8,500 personas, 4,000 SJTs, and 300,000 responses. We will release the dataset and all code to the public.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹AIå¿ƒç†æµ‹è¯„(AI psychometrics)ä¸­ä¼ ç»Ÿæ–¹æ³•ç¼ºä¹è¡Œä¸ºçœŸå®æ€§å’Œé¢†åŸŸç›¸å…³æ€§çš„é—®é¢˜ï¼Œæå‡ºäº†ä¸€ä¸ªåŸºäºæƒ…å¢ƒåˆ¤æ–­æµ‹è¯•(Situational Judgment Tests, SJTs)çš„è¯„ä¼°æ¡†æ¶ï¼Œæ—¨åœ¨æ¢æµ‹AIåœ¨ç‰¹å®šé¢†åŸŸä¸‹çš„èƒœä»»èƒ½åŠ›ã€‚è¯¥æ¡†æ¶ç»“åˆå·¥ä¸šç»„ç»‡ä¸äººæ ¼å¿ƒç†å­¦ï¼Œè®¾è®¡äº†åŒ…å«è¡Œä¸ºå¿ƒç†æè¿°ã€ç”Ÿæ´»å²å’Œç¤¾ä¼šæƒ…æ„ŸåŠŸèƒ½çš„é«˜çº§äººæ ¼æ¨¡å‹(personas)ï¼Œå¹¶åˆ©ç”¨äººå£ç»Ÿè®¡å…ˆéªŒå’ŒPydantic schemasè¿›è¡Œç»“æ„åŒ–ç”Ÿæˆã€‚é€šè¿‡æ‰§æ³•åŠ©æ‰‹(law enforcement assistant)çš„æ¡ˆä¾‹ç ”ç©¶ï¼Œç ”ç©¶è€…æ„å»ºäº†ä¸€ä¸ªæ¶µç›–8,500ä¸ªè§’è‰²ã€4,000é¡¹SJTsåŠ300,000æ¡å›å¤çš„åºå¤§æ•°æ®é›†ã€‚å®éªŒæ·±å…¥åˆ†æäº†ä¸åŒäººæ ¼åŸå‹å’Œæƒ…å¢ƒåˆ‡ç‰‡ä¸‹çš„è¡Œä¸ºè¡¨ç°ï¼Œè¯æ˜äº†è¯¥æ–¹æ³•åœ¨è¯„ä¼°éœ€è¦æƒ…æ„Ÿåˆ¤æ–­å’Œä¼¦ç†è€ƒé‡çš„AIç³»ç»Ÿæ—¶çš„æœ‰æ•ˆæ€§ï¼Œä¸ºæå‡AIç³»ç»Ÿçš„è¡Œä¸ºç°å®æ€§æä¾›äº†é‡è¦çš„è¯„ä¼°å·¥å…·å’Œå¼€æºæ•°æ®é›†ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "49 pages",
      "pdf_url": "https://arxiv.org/pdf/2510.22170v1",
      "published_date": "2025-10-25 05:45:10 UTC",
      "updated_date": "2025-10-25 05:45:10 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T05:39:25.465692+00:00"
    },
    {
      "arxiv_id": "2510.23648v1",
      "title": "RoGBot: Relationship-Oblivious Graph-based Neural Network with Contextual Knowledge for Bot Detection",
      "title_zh": "RoGBotï¼šèåˆä¸Šä¸‹æ–‡çŸ¥è¯†ä¸”å…³ç³»æ— å…³çš„å›¾ç¥ç»ç½‘ç»œæœºå™¨äººæ£€æµ‹æ¨¡å‹",
      "authors": [
        "Ashutosh Anshul",
        "Mohammad Zia Ur Rehman",
        "Sri Akash Kadali",
        "Nagendra Kumar"
      ],
      "abstract": "Detecting automated accounts (bots) among genuine users on platforms like Twitter remains a challenging task due to the evolving behaviors and adaptive strategies of such accounts. While recent methods have achieved strong detection performance by combining text, metadata, and user relationship information within graph-based frameworks, many of these models heavily depend on explicit user-user relationship data. This reliance limits their applicability in scenarios where such information is unavailable. To address this limitation, we propose a novel multimodal framework that integrates detailed textual features with enriched user metadata while employing graph-based reasoning without requiring follower-following data. Our method uses transformer-based models (e.g., BERT) to extract deep semantic embeddings from tweets, which are aggregated using max pooling to form comprehensive user-level representations. These are further combined with auxiliary behavioral features and passed through a GraphSAGE model to capture both local and global patterns in user behavior. Experimental results on the Cresci-15, Cresci-17, and PAN 2019 datasets demonstrate the robustness of our approach, achieving accuracies of 99.8%, 99.1%, and 96.8%, respectively, and highlighting its effectiveness against increasingly sophisticated bot strategies.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† RoGBotï¼Œä¸€ç§åŸºäºå…³ç³»æ— å…³å›¾ç¥ç»ç½‘ç»œ (Relationship-Oblivious Graph-based Neural Network) çš„æ–°å‹å¤šæ¨¡æ€æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³ç°æœ‰æœºå™¨äººæ£€æµ‹æ¨¡å‹è¿‡åº¦ä¾èµ–æ˜¾å¼ç”¨æˆ·å…³ç³»æ•°æ®çš„é—®é¢˜ã€‚è¯¥æ¡†æ¶åˆ©ç”¨ BERT ç­‰ transformer æ¨¡å‹ä»æ¨æ–‡ä¸­æå–æ·±åº¦è¯­ä¹‰åµŒå…¥ (semantic embeddings)ï¼Œå¹¶é€šè¿‡æœ€å¤§æ± åŒ– (max pooling) æŠ€æœ¯å°†å…¶è½¬åŒ–ä¸ºç»¼åˆçš„ç”¨æˆ·çº§è¡¨ç¤ºã€‚è¿™äº›ç‰¹å¾éšåä¸è¾…åŠ©è¡Œä¸ºå…ƒæ•°æ®ç»“åˆï¼Œé€šè¿‡ GraphSAGE æ¨¡å‹æ•æ‰ç”¨æˆ·è¡Œä¸ºçš„å±€éƒ¨å’Œå…¨å±€æ¨¡å¼ï¼Œä»è€Œåœ¨æ— éœ€ç²‰ä¸å…³æ³¨æ•°æ®çš„æƒ…å†µä¸‹å®ç°é«˜æ•ˆæ¨ç†ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒRoGBot åœ¨ Cresci-15ã€Cresci-17 å’Œ PAN 2019 æ•°æ®é›†ä¸Šåˆ†åˆ«å®ç°äº† 99.8%ã€99.1% å’Œ 96.8% çš„å‡†ç¡®ç‡ã€‚è¿™é¡¹ç ”ç©¶è¯æ˜äº†åœ¨ç¤¾äº¤é“¾æ¥ç¼ºå¤±çš„åœºæ™¯ä¸‹ï¼Œç»“åˆä¸Šä¸‹æ–‡çŸ¥è¯†è¿›è¡Œæœºå™¨äººæ£€æµ‹çš„æœ‰æ•ˆæ€§ï¼Œä¸ºåº”å¯¹æ—¥ç›Šå¤æ‚çš„è‡ªåŠ¨åŒ–è´¦æˆ·ç­–ç•¥æä¾›äº†é²æ£’çš„è§£å†³æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.SI",
        "cs.AI"
      ],
      "primary_category": "cs.SI",
      "comment": "Submitted to IEEE",
      "pdf_url": "https://arxiv.org/pdf/2510.23648v1",
      "published_date": "2025-10-25 05:14:58 UTC",
      "updated_date": "2025-10-25 05:14:58 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T05:39:30.175497+00:00"
    },
    {
      "arxiv_id": "2510.22158v1",
      "title": "Solving Continuous Mean Field Games: Deep Reinforcement Learning for Non-Stationary Dynamics",
      "title_zh": "æ±‚è§£è¿ç»­å¹³å‡åœºåšå¼ˆï¼šé¢å‘éå¹³ç¨³åŠ¨åŠ›å­¦çš„æ·±åº¦å¼ºåŒ–å­¦ä¹ ",
      "authors": [
        "Lorenzo Magnino",
        "Kai Shao",
        "Zida Wu",
        "Jiacheng Shen",
        "Mathieu LauriÃ¨re"
      ],
      "abstract": "Mean field games (MFGs) have emerged as a powerful framework for modeling interactions in large-scale multi-agent systems. Despite recent advancements in reinforcement learning (RL) for MFGs, existing methods are typically limited to finite spaces or stationary models, hindering their applicability to real-world problems. This paper introduces a novel deep reinforcement learning (DRL) algorithm specifically designed for non-stationary continuous MFGs. The proposed approach builds upon a Fictitious Play (FP) methodology, leveraging DRL for best-response computation and supervised learning for average policy representation. Furthermore, it learns a representation of the time-dependent population distribution using a Conditional Normalizing Flow. To validate the effectiveness of our method, we evaluate it on three different examples of increasing complexity. By addressing critical limitations in scalability and density approximation, this work represents a significant advancement in applying DRL techniques to complex MFG problems, bringing the field closer to real-world multi-agent systems.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹éå¹³ç¨³è¿ç»­å¹³å‡åœºåšå¼ˆ (Non-Stationary Continuous Mean Field Games, MFGs) æå‡ºäº†ä¸€ç§æ–°å‹æ·±åº¦å¼ºåŒ–å­¦ä¹  (Deep Reinforcement Learning, DRL) ç®—æ³•ï¼Œæ—¨åœ¨è§£å†³ç°æœ‰æ–¹æ³•åœ¨å¤„ç†å¤§è§„æ¨¡å¤šæ™ºèƒ½ä½“ç³»ç»Ÿæ—¶å—é™äºæœ‰é™ç©ºé—´æˆ–å¹³ç¨³æ¨¡å‹çš„ç¼ºé™·ã€‚è¯¥æ–¹æ³•åŸºäºè™šæ„åšå¼ˆ (Fictitious Play, FP) æ¡†æ¶ï¼Œç»“åˆ DRL è¿›è¡Œæœ€ä¼˜ååº” (Best-Response) è®¡ç®—ï¼Œå¹¶åˆ©ç”¨ç›‘ç£å­¦ä¹ å®ç°å¹³å‡ç­–ç•¥ (Average Policy) çš„è¡¨ç¤ºã€‚æ­¤å¤–ï¼Œç ”ç©¶å¼•å…¥äº†æ¡ä»¶å½’ä¸€åŒ–æµ (Conditional Normalizing Flow) æ¥å­¦ä¹ éšæ—¶é—´å˜åŒ–çš„ç§ç¾¤åˆ†å¸ƒ (Population Distribution) è¡¨ç¤ºã€‚é€šè¿‡åœ¨ä¸‰ä¸ªå¤æ‚åº¦é€’å¢çš„å®ä¾‹ä¸Šè¿›è¡ŒéªŒè¯ï¼Œè¯¥ç®—æ³•æˆåŠŸå…‹æœäº†å¯æ‰©å±•æ€§å’Œå¯†åº¦è¿‘ä¼¼ç­‰å…³é”®æŒ‘æˆ˜ã€‚è¯¥å·¥ä½œæ˜¾è‘—æ¨è¿›äº† DRL åœ¨å¤æ‚ MFGs é¢†åŸŸçš„åº”ç”¨ï¼Œä¸ºå¤„ç†ç°å®ä¸–ç•Œä¸­å…·æœ‰éå¹³ç¨³åŠ¨åŠ›å­¦çš„å¤§è§„æ¨¡å¤šæ™ºèƒ½ä½“ç³»ç»Ÿæä¾›äº†æœ‰æ•ˆçš„è§£å†³æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.MA",
        "math.OC"
      ],
      "primary_category": "cs.LG",
      "comment": "Neurips 2025",
      "pdf_url": "https://arxiv.org/pdf/2510.22158v1",
      "published_date": "2025-10-25 04:50:52 UTC",
      "updated_date": "2025-10-25 04:50:52 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T05:39:31.567168+00:00"
    },
    {
      "arxiv_id": "2510.22149v2",
      "title": "Power to the Clients: Federated Learning in a Dictatorship Setting",
      "title_zh": "èµ‹æƒå®¢æˆ·ç«¯ï¼šç‹¬è£è®¾å®šä¸‹çš„è”é‚¦å­¦ä¹ ",
      "authors": [
        "Mohammadsajad Alipour",
        "Mohammad Mohammadi Amiri"
      ],
      "abstract": "Federated learning (FL) has emerged as a promising paradigm for decentralized model training, enabling multiple clients to collaboratively learn a shared model without exchanging their local data. However, the decentralized nature of FL also introduces vulnerabilities, as malicious clients can compromise or manipulate the training process. In this work, we introduce dictator clients, a novel, well-defined, and analytically tractable class of malicious participants capable of entirely erasing the contributions of all other clients from the server model, while preserving their own. We propose concrete attack strategies that empower such clients and systematically analyze their effects on the learning process. Furthermore, we explore complex scenarios involving multiple dictator clients, including cases where they collaborate, act independently, or form an alliance in order to ultimately betray one another. For each of these settings, we provide a theoretical analysis of their impact on the global model's convergence. Our theoretical algorithms and findings about the complex scenarios including multiple dictator clients are further supported by empirical evaluations on both computer vision and natural language processing benchmarks.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†è”åˆå­¦ä¹ (Federated learning)åœ¨å®‰å…¨é¢†åŸŸçš„è„†å¼±æ€§ï¼Œå¹¶å¼•å…¥äº†ä¸€ç±»åä¸ºâ€œç‹¬è£è€…å®¢æˆ·ç«¯â€(dictator clients)çš„æ–°å‹æ¶æ„å‚ä¸è€…ã€‚è¿™ç±»å®¢æˆ·ç«¯å…·æœ‰å®Œå…¨æŠ¹é™¤æœåŠ¡å™¨æ¨¡å‹(server model)ä¸­å…¶ä»–æ‰€æœ‰å‚ä¸è€…è´¡çŒ®çš„èƒ½åŠ›ï¼ŒåŒæ—¶èƒ½ç¡®ä¿è‡ªèº«çš„è´¡çŒ®å¾—ä»¥ä¿ç•™ã€‚ä½œè€…æå‡ºäº†å…·ä½“çš„æ”»å‡»ç­–ç•¥ï¼Œå¹¶ç³»ç»Ÿåˆ†æäº†å•å®¢æˆ·ç«¯åŠå¤šå®¢æˆ·ç«¯æƒ…æ™¯ä¸‹çš„å½±å“ï¼ŒåŒ…æ‹¬å¤šæ–¹ç‹¬ç«‹ã€åä½œä¹ƒè‡³ç»“ç›Ÿåçš„èƒŒå›è¡Œä¸ºã€‚é€šè¿‡å¯¹æ”¶æ•›æ€§çš„ç†è®ºæ¨å¯¼ä»¥åŠåœ¨è®¡ç®—æœºè§†è§‰(computer vision)å’Œè‡ªç„¶è¯­è¨€å¤„ç†(natural language processing)åŸºå‡†æµ‹è¯•ä¸Šçš„å®éªŒï¼Œè¯¥ç ”ç©¶ç³»ç»Ÿåœ°è¯„ä¼°äº†æ­¤ç±»æ”»å‡»å¯¹æ¨¡å‹è¡¨ç°çš„å½±å“ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL",
        "cs.CR",
        "cs.CV",
        "cs.DC"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.22149v2",
      "published_date": "2025-10-25 04:02:04 UTC",
      "updated_date": "2026-01-15 21:52:56 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T05:39:46.366760+00:00"
    },
    {
      "arxiv_id": "2510.22132v1",
      "title": "Controllable Mathematical Reasoning via Self-Optimizing Thought Vectors",
      "title_zh": "åŸºäºè‡ªä¼˜åŒ–æ€ç»´å‘é‡çš„å¯æ§æ•°å­¦æ¨ç†",
      "authors": [
        "Xuying LI"
      ],
      "abstract": "We present a novel approach for controllable mathematical reasoning that leverages self-optimizing thought vectors with entropy minimization. Our method introduces learnable thought vectors that dynamically modulate the internal reasoning process of large language models. Using Gemma-2-9B on GSM8K, we achieve 90.1% accuracy with a controllability score of 0.42, demonstrating that entropy-based rewards effectively guide focused reasoning patterns without requiring external reward annotations. Our analysis reveals distinct thought vector clusters and consistent low-entropy distributions across control conditions, validating our framework for controllable AI reasoning.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§åŸºäºè‡ªä¼˜åŒ–æ€ç»´å‘é‡ (Self-Optimizing Thought Vectors) å’Œç†µæœ€å°åŒ– (Entropy Minimization) çš„å¯æ§æ•°å­¦æ¨ç†æ–°æ–¹æ³•ã€‚é€šè¿‡å¼•å…¥å¯å­¦ä¹ çš„æ€ç»´å‘é‡ï¼Œè¯¥æ–¹æ³•èƒ½å¤ŸåŠ¨æ€è°ƒèŠ‚å¤§è¯­è¨€æ¨¡å‹ (Large Language Models) çš„å†…éƒ¨æ¨ç†è¿‡ç¨‹ã€‚å®éªŒåœ¨ GSM8K æ•°æ®é›†ä¸Šåˆ©ç”¨ Gemma-2-9B æ¨¡å‹è¿›è¡ŒéªŒè¯ï¼Œç»“æœæ˜¾ç¤ºå…¶å‡†ç¡®ç‡è¾¾åˆ° 90.1%ï¼Œå¯æ§æ€§è¯„åˆ†ä¸º 0.42ã€‚ç ”ç©¶è¡¨æ˜ï¼ŒåŸºäºç†µçš„å¥–åŠ±æœºåˆ¶æ— éœ€å¤–éƒ¨å¥–åŠ±æ ‡æ³¨å³å¯æœ‰æ•ˆå¼•å¯¼ä¸“æ³¨çš„æ¨ç†æ¨¡å¼ã€‚æ­¤å¤–ï¼Œåˆ†ææ­ç¤ºäº†æ˜¾è‘—çš„æ€ç»´å‘é‡èšç±»ä»¥åŠåœ¨ä¸åŒæ§åˆ¶æ¡ä»¶ä¸‹çš„ä¸€è‡´ä½ç†µåˆ†å¸ƒï¼Œå……åˆ†éªŒè¯äº†è¯¥æ¡†æ¶åœ¨å¯æ§äººå·¥æ™ºèƒ½æ¨ç† (Controllable AI Reasoning) é¢†åŸŸçš„æœ‰æ•ˆæ€§ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.22132v1",
      "published_date": "2025-10-25 03:13:14 UTC",
      "updated_date": "2025-10-25 03:13:14 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T05:39:45.172314+00:00"
    },
    {
      "arxiv_id": "2510.22131v1",
      "title": "Probing Neural Combinatorial Optimization Models",
      "title_zh": "ç¥ç»ç»„åˆä¼˜åŒ–æ¨¡å‹çš„æ¢é’ˆç ”ç©¶",
      "authors": [
        "Zhiqin Zhang",
        "Yining Ma",
        "Zhiguang Cao",
        "Hoong Chuin Lau"
      ],
      "abstract": "Neural combinatorial optimization (NCO) has achieved remarkable performance, yet its learned model representations and decision rationale remain a black box. This impedes both academic research and practical deployment, since researchers and stakeholders require deeper insights into NCO models. In this paper, we take the first critical step towards interpreting NCO models by investigating their representations through various probing tasks. Moreover, we introduce a novel probing tool named Coefficient Significance Probing (CS-Probing) to enable deeper analysis of NCO representations by examining the coefficients and statistical significance during probing. Extensive experiments and analysis reveal that NCO models encode low-level information essential for solution construction, while capturing high-level knowledge to facilitate better decisions. Using CS-Probing, we find that prevalent NCO models impose varying inductive biases on their learned representations, uncover direct evidence related to model generalization, and identify key embedding dimensions associated with specific knowledge. These insights can be potentially translated into practice, for example, with minor code modifications, we improve the generalization of the analyzed model. Our work represents a first systematic attempt to interpret black-box NCO models, showcasing probing as a promising tool for analyzing their internal mechanisms and revealing insights for the NCO community. The source code is publicly available.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ç¥ç»ç»„åˆä¼˜åŒ– (Neural Combinatorial Optimization, NCO) æ¨¡å‹åœ¨è¡¨ç¤ºå’Œå†³ç­–é€»è¾‘ä¸Šçš„é»‘ç›’é—®é¢˜ï¼Œé€šè¿‡å¤šç§æ¢æµ‹ä»»åŠ¡ (Probing Tasks) å¯¹å…¶æ¨¡å‹è¡¨ç¤ºè¿›è¡Œäº†ç³»ç»Ÿæ€§çš„è§£é‡Šç ”ç©¶ã€‚ä½œè€…æå‡ºäº†ä¸€ç§åä¸ºç³»æ•°æ˜¾è‘—æ€§æ¢æµ‹ (Coefficient Significance Probing, CS-Probing) çš„æ–°å‹å·¥å…·ï¼Œé€šè¿‡åˆ†ææ¢æµ‹è¿‡ç¨‹ä¸­çš„ç³»æ•°å’Œç»Ÿè®¡æ˜¾è‘—æ€§ï¼Œå®ç°äº†å¯¹ NCO è¡¨ç¤ºçš„æ·±å…¥å‰–æã€‚å®éªŒå‘ç°ï¼ŒNCO æ¨¡å‹ä¸ä»…ç¼–ç äº†ç”¨äºè§£æ„é€ çš„åº•å±‚ä¿¡æ¯ï¼Œè¿˜æ•æ‰äº†æœ‰åŠ©äºæå‡å†³ç­–è´¨é‡çš„é«˜å±‚çŸ¥è¯†ã€‚åˆ©ç”¨ CS-Probingï¼Œè¯¥ç ”ç©¶æ­ç¤ºäº†ä¸»æµ NCO æ¨¡å‹ä¸­å­˜åœ¨çš„å½’çº³åç½® (Inductive Biases)ï¼Œå‘ç°äº†ä¸æ¨¡å‹æ³›åŒ–æ€§ (Generalization) ç›¸å…³çš„ç›´æ¥è¯æ®ï¼Œå¹¶è¯†åˆ«å‡ºäº†ä¸ç‰¹å®šçŸ¥è¯†å…³è”çš„å…³é”®åµŒå…¥ç»´åº¦ã€‚åŸºäºè¿™äº›è§è§£ï¼Œç ”ç©¶è€…é€šè¿‡ç®€å•çš„ä»£ç ä¿®æ”¹æˆåŠŸæå‡äº†åˆ†ææ¨¡å‹çš„æ³›åŒ–æ€§èƒ½ã€‚è¯¥å·¥ä½œè¯æ˜äº†æ¢æµ‹æŠ€æœ¯æ˜¯åˆ†æ NCO å†…éƒ¨æœºåˆ¶çš„æœ‰æ•ˆå·¥å…·ï¼Œä¸ºæ¨åŠ¨è¯¥é¢†åŸŸçš„æ¨¡å‹å¯è§£é‡Šæ€§ç ”ç©¶æä¾›äº†é‡è¦æ´å¯Ÿã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "39 pages, 16 figures. Accepted as Spotlight at NeurIPS 2025",
      "pdf_url": "https://arxiv.org/pdf/2510.22131v1",
      "published_date": "2025-10-25 03:11:10 UTC",
      "updated_date": "2025-10-25 03:11:10 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T05:39:59.867870+00:00"
    },
    {
      "arxiv_id": "2510.22124v1",
      "title": "Efficient Utility-Preserving Machine Unlearning with Implicit Gradient Surgery",
      "title_zh": "åŸºäºéšå¼æ¢¯åº¦æ‰‹æœ¯çš„é«˜æ•ˆä¿æ•ˆç”¨æœºå™¨é—å¿˜",
      "authors": [
        "Shiji Zhou",
        "Tianbai Yu",
        "Zhi Zhang",
        "Heng Chang",
        "Xiao Zhou",
        "Dong Wu",
        "Han Zhao"
      ],
      "abstract": "Machine unlearning (MU) aims to efficiently remove sensitive or harmful memory from a pre-trained model. The key challenge is to balance the potential tradeoff between unlearning efficacy and utility preservation, which involves forgetting undesirable information as defined while maintaining the model's original performance. One potential way to tackle this problem is to use multi-objective optimization to jointly optimize both the unlearning and utility preservation objectives. However, existing multi-objective methods only guarantee finding a Pareto-optimal solution without fine-grained control, which causes under-optimization of the unlearning objective. To this end, we first model MU as a constrained optimization problem, that is, optimizing the unlearning objective under the constraint of a bounded increase for utility loss. We then show that solving this optimization problem is equivalent to unilateral gradient surgery on the unlearning objective. To resolve the additional computational cost brought by gradient surgery, we propose an implicit gradient surgery method, which approximates the solution to the aforementioned constrained optimization problem via only one backpropagation, thereby achieving efficient utility-preserving MU. Theoretically, we provide a tight convergence analysis of the algorithm. Empirically, our extensive experiments show that the proposed algorithm achieves better tradeoff results than existing baselines. Codes are available at https://github.com/anseryuer/EUPMU-Efficient-Utility-Preserving-Machine-Unlearning.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ Machine Unlearning (MU) ä¸­é—å¿˜æ•ˆæœä¸ utility preservation ä¹‹é—´çš„æƒè¡¡éš¾é¢˜ï¼Œæå‡ºäº†ä¸€ç§åä¸º Implicit Gradient Surgery çš„é«˜æ•ˆæ–¹æ³•ã€‚ä½œè€…é¦–å…ˆå°† MU å»ºæ¨¡ä¸ºå—çº¦æŸçš„ä¼˜åŒ–é—®é¢˜ï¼Œåœ¨ä¿è¯æ•ˆç”¨æŸå¤±å¢é‡å—é™çš„å‰æä¸‹ä¼˜åŒ–é—å¿˜ç›®æ ‡ï¼Œå¹¶è¯æ˜è¯¥è¿‡ç¨‹ç­‰æ•ˆäºå¯¹é—å¿˜ç›®æ ‡çš„å•è¾¹æ¢¯åº¦æ‰‹æœ¯ã€‚ä¸ºè§£å†³ä¼ ç»Ÿæ–¹æ³•å¸¦æ¥çš„é¢å¤–è®¡ç®—å¼€é”€ï¼Œè¯¥ç ”ç©¶æå‡ºçš„éšå¼æ–¹æ³•ä»…é€šè¿‡å•æ¬¡ backpropagation å³å¯è¿‘ä¼¼æ±‚è§£ä¸Šè¿°çº¦æŸä¼˜åŒ–é—®é¢˜ã€‚ç†è®ºä¸Šï¼Œæ–‡ä¸­æä¾›äº†ç®—æ³•çš„ tight convergence analysisï¼›å®éªŒç»“æœåˆ™è¡¨æ˜ï¼Œè¯¥ç®—æ³•åœ¨å¤„ç†é—å¿˜æœ‰æ•ˆæ€§ä¸æ•ˆç”¨ä¿æŒçš„ tradeoff ä¸Šæ˜¾è‘—ä¼˜äºç°æœ‰çš„ baseline æ¨¡å‹ã€‚è¯¥ç ”ç©¶ä¸ºå®ç°é«˜æ•ˆä¸”ä¿æ•ˆç”¨çš„æœºå™¨é—å¿˜æŠ€æœ¯æä¾›äº†å…¼å…·ç†è®ºæ”¯æ’‘ä¸å®è·µå¯è¡Œæ€§çš„è§£å†³æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "Corresponding author: Shiji Zhou (zhoushiji25@buaa.edu.cn). Shiji Zhou and Tianbai Yu contributed equally",
      "pdf_url": "https://arxiv.org/pdf/2510.22124v1",
      "published_date": "2025-10-25 02:49:26 UTC",
      "updated_date": "2025-10-25 02:49:26 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T05:40:01.452636+00:00"
    },
    {
      "arxiv_id": "2510.22118v2",
      "title": "GRAID: Enhancing Spatial Reasoning of VLMs Through High-Fidelity Data Generation",
      "title_zh": "GRAIDï¼šé€šè¿‡é«˜ä¿çœŸæ•°æ®ç”Ÿæˆæå‡è§†è§‰è¯­è¨€æ¨¡å‹çš„ç©ºé—´æ¨ç†èƒ½åŠ›",
      "authors": [
        "Karim Elmaaroufi",
        "Liheng Lai",
        "Justin Svegliato",
        "Yutong Bai",
        "Sanjit A. Seshia",
        "Matei Zaharia"
      ],
      "abstract": "Vision Language Models (VLMs) achieve strong performance on many vision-language tasks but often struggle with spatial reasoning$\\unicode{x2014}$a prerequisite for many applications. Empirically, we find that a dataset produced by a current training data generation pipeline has a 57.6% human validation rate. These rates stem from current limitations: single-image 3D reconstruction introduces cascading modeling errors and requires wide answer tolerances, while caption-based methods require hyper-detailed annotations and suffer from generative hallucinations. We present GRAID, built on the key insight that qualitative spatial relationships can be reliably determined from 2D geometric primitives alone. By operating exclusively on 2D bounding boxes from standard object detectors, GRAID avoids both 3D reconstruction errors and generative hallucinations, resulting in datasets that are of higher quality than existing tools that produce similar datasets as validated by human evaluations. We apply our framework to the BDD100k, NuImages, and Waymo datasets, generating over 8.5 million high-quality VQA pairs creating questions spanning spatial relations, counting, ranking, and size comparisons. We evaluate one of the datasets and find it achieves 91.16% human-validated accuracy$\\unicode{x2014}$compared to 57.6% on a dataset generated by recent work. Critically, we demonstrate that when trained on GRAID data, models learn spatial reasoning concepts that generalize: models fine-tuned on 6 question types improve on over 10 held-out types, with accuracy gains of 47.5% on BDD and 37.9% on NuImages for Llama 3.2B 11B, and when trained on all questions types, achieve improvements on several existing benchmarks such as BLINK. The GRAID framework, datasets, and additional information can be found $\\href{this https URL}{here}$.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹è§†è§‰è¯­è¨€æ¨¡å‹(VLMs)åœ¨ç©ºé—´æ¨ç†(Spatial Reasoning)èƒ½åŠ›æ–¹é¢çš„ä¸è¶³ï¼ŒæŒ‡å‡ºå½“å‰æ•°æ®ç”Ÿæˆç®¡çº¿å› 3Dé‡å»ºè¯¯å·®å’Œç”Ÿæˆæ€§å¹»è§‰å¯¼è‡´æ•°æ®é›†è´¨é‡è¾ƒä½çš„é—®é¢˜ã€‚ä¸ºæ­¤ï¼Œä½œè€…æå‡ºäº†GRAIDæ¡†æ¶ï¼Œå…¶æ ¸å¿ƒç†å¿µæ˜¯ä»…é€šè¿‡æ ‡å‡†æ£€æµ‹å™¨è·å–çš„2Då‡ ä½•åŸºå…ƒï¼ˆå¦‚2D bounding boxesï¼‰å³å¯å¯é åœ°ç¡®å®šå®šæ€§çš„ç©ºé—´å…³ç³»ã€‚è¯¥æ–¹æ³•æœ‰æ•ˆé¿å…äº†å¤æ‚çš„å»ºæ¨¡é”™è¯¯ï¼Œç”Ÿæˆçš„è§†è§‰é—®ç­”(VQA)å¯¹åœ¨äººç±»éªŒè¯ä¸­çš„å‡†ç¡®ç‡è¾¾åˆ°91.16%ï¼Œè¿œé«˜äºç°æœ‰æŠ€æœ¯çš„57.6%ã€‚ç ”ç©¶äººå‘˜åˆ©ç”¨è¯¥æ¡†æ¶åœ¨BDD100kã€NuImageså’ŒWaymoæ•°æ®é›†ä¸Šç”Ÿæˆäº†è¶…è¿‡850ä¸‡ä¸ªæ¶µç›–ç©ºé—´å…³ç³»ã€è®¡æ•°å’Œå°ºå¯¸æ¯”è¾ƒçš„é«˜è´¨é‡æ•°æ®å¯¹ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œåœ¨GRAIDæ•°æ®ä¸Šå¾®è°ƒçš„æ¨¡å‹å±•ç°å‡ºä¼˜å¼‚çš„æ³›åŒ–èƒ½åŠ›ï¼Œåœ¨Llama 3.2B 11Bç­‰æ¨¡å‹ä¸Šçš„å‡†ç¡®ç‡æå‡æ˜¾è‘—ï¼Œå¹¶æœ‰æ•ˆæ”¹å–„äº†å…¶åœ¨BLINKç­‰ç°æœ‰åŸºå‡†æµ‹è¯•ä¸Šçš„è¡¨ç°ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "22 pages, 3 figures, 3 tables, project page: https://ke7.github.io/graid/",
      "pdf_url": "https://arxiv.org/pdf/2510.22118v2",
      "published_date": "2025-10-25 02:07:23 UTC",
      "updated_date": "2025-10-28 00:53:28 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T05:39:58.767589+00:00"
    },
    {
      "arxiv_id": "2510.22117v1",
      "title": "When UAV Swarm Meets IRS: Collaborative Secure Communications in Low-altitude Wireless Networks",
      "title_zh": "å½“æ— äººæœºé›†ç¾¤é‡ä¸Šæ™ºèƒ½åå°„é¢ï¼šä½ç©ºæ— çº¿ç½‘ç»œä¸­çš„åä½œå®‰å…¨é€šä¿¡",
      "authors": [
        "Jiahui Li",
        "Xinyue Liang",
        "Geng Sun",
        "Hui Kang",
        "Jiacheng Wang",
        "Dusit Niyato",
        "Shiwen Mao",
        "Abbas Jamalipour"
      ],
      "abstract": "Low-altitude wireless networks (LAWNs) represent a promising architecture that integrates unmanned aerial vehicles (UAVs) as aerial nodes to provide enhanced coverage, reliability, and throughput for diverse applications. However, these networks face significant security vulnerabilities from both known and potential unknown eavesdroppers, which may threaten data confidentiality and system integrity. To solve this critical issue, we propose a novel secure communication framework for LAWNs where the selected UAVs within a swarm function as a virtual antenna array (VAA), complemented by intelligent reflecting surface (IRS) to create a robust defense against eavesdropping attacks. Specifically, we formulate a multi-objective optimization problem that simultaneously maximizes the secrecy rate while minimizing the maximum sidelobe level and total energy consumption, requiring joint optimization of UAV excitation current weights, flight trajectories, and IRS phase shifts. This problem presents significant difficulties due to the dynamic nature of the system and heterogeneous components. Thus, we first transform the problem into a heterogeneous Markov decision process (MDP). Then, we propose a heterogeneous multi-agent control approach (HMCA) that integrates a dedicated IRS control policy with a multi-agent soft actor-critic framework for UAV control, which enables coordinated operation across heterogeneous network elements. Simulation results show that the proposed HMCA achieves superior performance compared to baseline approaches in terms of secrecy rate improvement, sidelobe suppression, and energy efficiency. Furthermore, we find that the collaborative and passive beamforming synergy between VAA and IRS creates robust security guarantees when the number of UAVs increases.",
      "tldr_zh": "æœ¬ç ”ç©¶é’ˆå¯¹ä½ç©ºæ— çº¿ç½‘ç»œ(LAWNs)é¢ä¸´çš„å·²çŸ¥åŠæ½œåœ¨çªƒå¬å¨èƒï¼Œæå‡ºäº†ä¸€ç§ç»“åˆæ— äººæœºç¾¤(UAV Swarm)è™šæ‹Ÿå¤©çº¿é˜µåˆ—(VAA)ä¸æ™ºèƒ½åå°„é¢(IRS)çš„æ–°å‹ååŒå®‰å…¨é€šä¿¡æ¡†æ¶ã€‚ç ”ç©¶å°†æœ€å¤§åŒ–ä¿å¯†é€Ÿç‡(secrecy rate)ã€æœ€å°åŒ–æœ€å¤§æ—ç“£ç”µå¹³(maximum sidelobe level)åŠé™ä½æ€»èƒ½è€—å»ºæ¨¡ä¸ºå¤šç›®æ ‡ä¼˜åŒ–é—®é¢˜ï¼Œå¹¶å°†å…¶è½¬åŒ–ä¸ºå¼‚æ„é©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹(MDP)ã€‚ä¸ºäº†åº”å¯¹ç³»ç»Ÿçš„åŠ¨æ€æ€§ä¸å¼‚æ„æ€§ï¼Œæå‡ºäº†ä¸€ç§å¼‚æ„å¤šæ™ºèƒ½ä½“æ§åˆ¶æ–¹æ³•(HMCA)ï¼Œè¯¥æ–¹æ³•æ•´åˆäº†ä¸“é—¨çš„IRSæ§åˆ¶ç­–ç•¥ä¸åŸºäºå¤šæ™ºèƒ½ä½“è½¯æ¼”å‘˜-è¯„è®ºå®¶(multi-agent soft actor-critic)æ¡†æ¶çš„æ— äººæœºååŒæ§åˆ¶ç®—æ³•ã€‚ä»¿çœŸç»“æœè¯å®ï¼ŒHMCAåœ¨ä¿å¯†é€Ÿç‡æå‡ã€æ—ç“£æŠ‘åˆ¶å’Œèƒ½æ•ˆä¼˜åŒ–æ–¹é¢æ˜¾è‘—ä¼˜äºåŸºå‡†æ–¹æ³•ã€‚æ­¤å¤–ï¼Œç ”ç©¶å‘ç°VAAä¸IRSä¹‹é—´çš„ååŒè¢«åŠ¨æ³¢æŸèµ‹å½¢æ•ˆåº”èƒ½å¤Ÿåœ¨æ— äººæœºè§„æ¨¡æ‰©å¤§æ—¶ï¼Œä¸ºç³»ç»Ÿæä¾›æ›´ä¸ºç¨³å¥çš„å®‰å…¨ä¿éšœã€‚",
      "categories": [
        "cs.NI",
        "cs.AI"
      ],
      "primary_category": "cs.NI",
      "comment": "13 pages, 7 figures, submitted to IEEE Journal on Selected Areas in Communications",
      "pdf_url": "https://arxiv.org/pdf/2510.22117v1",
      "published_date": "2025-10-25 02:02:14 UTC",
      "updated_date": "2025-10-25 02:02:14 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T05:39:59.101675+00:00"
    },
    {
      "arxiv_id": "2510.22115v2",
      "title": "Every Activation Boosted: Scaling General Reasoner to 1 Trillion Open Language Foundation",
      "title_zh": "æ¿€æ´»å³å¢å¼ºï¼šå°†é€šç”¨æ¨ç†å™¨æ‰©å±•è‡³ä¸‡äº¿çº§å¼€æ”¾è¯­è¨€åŸºåº§",
      "authors": [
        "Ling Team",
        "Ang Li",
        "Ben Liu",
        "Binbin Hu",
        "Bing Li",
        "Bingwei Zeng",
        "Borui Ye",
        "Caizhi Tang",
        "Changxin Tian",
        "Chao Huang",
        "Chao Zhang",
        "Chen Qian",
        "Chenchen Ju",
        "Chenchen Li",
        "Chengfu Tang",
        "Chilin Fu",
        "Chunshao Ren",
        "Chunwei Wu",
        "Cong Zhang",
        "Cunyin Peng",
        "Dafeng Xu",
        "Daixin Wang",
        "Dalong Zhang",
        "Dingnan Jin",
        "Dingyuan Zhu",
        "Dongke Hu",
        "Fangzheng Zhao",
        "Feifan Wu",
        "Feng Zhu",
        "Gangshan Wang",
        "Haitao Zhang",
        "Hailin Zhao",
        "Hanxiao Zhang",
        "Hanzi Wang",
        "Hao Qian",
        "Haoyi Yu",
        "Heng Zhang",
        "Hongliang Zhang",
        "Hongzhi Luan",
        "Huirong Dong",
        "Huizhong Li",
        "Jia Li",
        "Jia Liu",
        "Jialong Zhu",
        "Jian Sha",
        "Jianping Wei",
        "Jiaolong Yang",
        "Jieyue Ma",
        "Jiewei Wu",
        "Jinjing Huang",
        "Jingyun Tian",
        "Jingyuan Zhang",
        "Jinquan Sun",
        "Juanhui Tu",
        "Jun Liu",
        "Jun Xu",
        "Jun Zhou",
        "Junjie Ou",
        "Junpeng Fang",
        "Kaihong Zhang",
        "Kaiqin Hu",
        "Ke Shi",
        "Kun Tang",
        "Kunlong Chen",
        "Lanyin Mei",
        "Lei Liang",
        "Lei Xu",
        "Libo Zhang",
        "Lin Ju",
        "Lin Yuan",
        "Ling Zhong",
        "Lintao Ma",
        "Lu Liu",
        "Lu Yu",
        "Lun Cai",
        "Meiqi Zhu",
        "Mengying Li",
        "Min Chen",
        "Minghao Xue",
        "Minghong Cai",
        "Mingming Yin",
        "Peijie Jiang",
        "Peilong Zhao",
        "Pingping Liu",
        "Qian Zhao",
        "Qing Cui",
        "Qingxiang Huang",
        "Qingyuan Yang",
        "Quankun Yu",
        "Shaowei Wei",
        "Shijie Lian",
        "Shoujian Zheng",
        "Shun Song",
        "Shungen Zhang",
        "Shuo Zhang",
        "Siyuan Li",
        "Song Liu",
        "Ting Guo",
        "Tong Zhao",
        "Wanli Gu",
        "Weichang Wu",
        "Weiguang Han",
        "Wenjing Fang",
        "Wubin Wang",
        "Xiang Shu",
        "Xiao Shi",
        "Xiaoshun Lan",
        "Xiaolu Zhang",
        "Xiaqing Sun",
        "Xin Zhao",
        "Xingyu Lu",
        "Xiong Xu",
        "Xudong Wang",
        "Xudong Wang",
        "Xuemin Yang",
        "Yajie Yang",
        "Yang Xiang",
        "Yanzhe Li",
        "Yi Zhang",
        "Yilong Wang",
        "Yingxue Li",
        "Yongzhen Guo",
        "Yuzhuo Fu",
        "Yuanyuan Wang",
        "Yue Yang",
        "Yue Yu",
        "Yufeng Deng",
        "Yun Zhang",
        "Yunfei Yu",
        "Yuqi Zhang",
        "Yuxiao He",
        "Zengke Gui",
        "Zhaoxin Huan",
        "Zhaoyang Wang",
        "Zhibo Zhu",
        "Zhihao Wang",
        "Zhiqiang Zhang",
        "Zhoufei Wang",
        "Zihang Zeng",
        "Ziqi Liu",
        "Zitao Xuan",
        "Zuoli Tang"
      ],
      "abstract": "We introduce Ling 2.0, a series reasoning-oriented language foundation built upon the principle that every activation boosts reasoning capability. Designed to scale from tens of billions to one trillion parameters under a unified Mixture-of-Experts (MoE) paradigm, Ling 2.0 emphasizes high sparsity, cross-scale consistency, and efficiency guided by empirical scaling laws. The series includes three non-thinking (instruct) models - Ling-mini-2.0, Ling-flash-2.0, and Ling-1T - ranging from 16B to 1T total parameters and achieving up to 7-fold active-compute efficiency compared with dense counterparts. Ling 2.0 integrates coordinated innovations across model architecture, pre-training, post-training, and infrastructure: a high-sparsity MoE with MTP for efficient reasoning, reasoning-oriented data and mid-training CoT activation, reinforcement-based fine-tuning (DFT, Evo-CoT), and full-scale FP8 training with fine-grained heterogeneous pipelines. At the trillion scale, Ling-1T establishes a new Pareto frontier of reasoning accuracy versus computational efficiency, demonstrating that sparse activation, when properly aligned with reasoning objectives, enables scalable and efficient intelligence. Collectively, Ling 2.0 provides a coherent, open, and efficient foundation for advancing future reasoning and thinking models, including the Ring series built upon the same base.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¨å‡ºäº† Ling 2.0ï¼Œè¿™æ˜¯ä¸€ä¸ªé¢å‘æ¨ç†çš„è¯­è¨€åŸºç¡€æ¨¡å‹ç³»åˆ—ï¼ŒåŸºäºâ€œæ¯æ¬¡æ¿€æ´»éƒ½èƒ½æå‡æ¨ç†èƒ½åŠ›â€çš„åŸåˆ™ï¼Œé‡‡ç”¨ç»Ÿä¸€çš„æ··åˆä¸“å®¶æ¨¡å‹ (Mixture-of-Experts, MoE) èŒƒå¼ï¼Œå®ç°äº†ä»ç™¾äº¿åˆ°ä¸‡äº¿å‚æ•°è§„æ¨¡çš„æ‰©å±•ã€‚è¯¥ç³»åˆ—æ¶µç›–äº† Ling-mini-2.0ã€Ling-flash-2.0 å’Œ Ling-1T ä¸‰æ¬¾æ¨¡å‹ï¼Œé€šè¿‡é«˜ç¨€ç–æ€§è®¾è®¡ï¼Œåœ¨ä¸‡äº¿å‚æ•°è§„æ¨¡ä¸‹å®ç°äº†æ¯”ç¨ å¯†æ¨¡å‹é«˜å‡º 7 å€çš„æ´»è·ƒè®¡ç®—æ•ˆç‡ã€‚Ling 2.0 åœ¨æ¶æ„ã€è®­ç»ƒå’ŒåŸºç¡€è®¾æ–½ä¸Šè¿›è¡Œäº†å¤šé¡¹ååŒåˆ›æ–°ï¼ŒåŒ…æ‹¬ç»“åˆ MTP çš„é«˜ç¨€ç– MoE æ¶æ„ã€é¢å‘æ¨ç†çš„æ•°æ®å’Œè®­ç»ƒä¸­æœŸçš„ Chain-of-Thought (CoT) æ¿€æ´»ï¼Œä»¥åŠåŸºäºå¼ºåŒ–çš„å¾®è°ƒï¼ˆDFT, Evo-CoTï¼‰å’Œå…¨è§„æ¨¡ FP8 è®­ç»ƒã€‚å®éªŒè¡¨æ˜ï¼ŒLing-1T åœ¨æ¨ç†å‡†ç¡®æ€§ä¸è®¡ç®—æ•ˆç‡ä¹‹é—´å»ºç«‹äº†æ–°çš„å¸•ç´¯æ‰˜å‰æ²¿ (Pareto frontier)ï¼Œè¯æ˜äº†ç¨€ç–æ¿€æ´»ä¸æ¨ç†ç›®æ ‡å¯¹é½åèƒ½å®ç°å¯æ‰©å±•ä¸”é«˜æ•ˆçš„æ™ºèƒ½ã€‚æ€»ä½“è€Œè¨€ï¼ŒLing 2.0 ä¸ºæœªæ¥æ¨ç†å’Œæ€è€ƒæ¨¡å‹ï¼ˆå¦‚ Ring ç³»åˆ—ï¼‰çš„å¼€å‘æä¾›äº†ä¸€ä¸ªè¿è´¯ã€å¼€æ”¾ä¸”é«˜æ•ˆçš„åŸºç¡€æ¶æ„ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "Ling 2.0 Technical Report",
      "pdf_url": "https://arxiv.org/pdf/2510.22115v2",
      "published_date": "2025-10-25 01:51:37 UTC",
      "updated_date": "2025-11-07 02:15:00 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T05:40:02.875040+00:00"
    },
    {
      "arxiv_id": "2510.22109v1",
      "title": "Gradual Forgetting: Logarithmic Compression for Extending Transformer Context Windows",
      "title_zh": "æ¸è¿›å¼é—å¿˜ï¼šé€šè¿‡å¯¹æ•°å‹ç¼©æ‰©å±• Transformer ä¸Šä¸‹æ–‡çª—å£",
      "authors": [
        "Billy Dickson",
        "Zoran Tiganj"
      ],
      "abstract": "Most approaches to long-context processing increase the complexity of the transformer's internal architecture by integrating mechanisms such as recurrence or auxiliary memory modules. In this work, we introduce an alternative approach that modifies the input representation itself, rather than the transformer architecture. Inspired by cognitive models of human memory, our method applies a scale-invariant logarithmic compression to the input tokens. The resulting compressed representation is processed by a standard, unmodified transformer, preserving architectural simplicity. We evaluate this approach on the WikiText-103 and PG-19 language modeling benchmarks, showing a reduction in perplexity compared to uncompressed baselines. Moreover, performance improves consistently with longer compressed temporal contexts, showing that input-level logarithmic compression is a simple and effective way to extend a transformer's long-range memory.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†Gradual Forgettingï¼Œä¸€ç§æ—¨åœ¨æ‰©å±•Transformerä¸Šä¸‹æ–‡çª—å£(Context Windows)çš„æ–°æ–¹æ³•ï¼Œå…¶æ ¸å¿ƒæ€è·¯æ˜¯æ”¹è¿›è¾“å…¥è¡¨ç¤ºè€Œéå¢åŠ æ¶æ„å¤æ‚æ€§ã€‚å—äººç±»è®°å¿†è®¤çŸ¥æ¨¡å‹çš„å¯å‘ï¼Œè¯¥æ–¹æ³•å¯¹è¾“å…¥è¯å…ƒ(Tokens)åº”ç”¨å°ºåº¦ä¸å˜çš„å¯¹æ•°å‹ç¼©(Logarithmic Compression)ï¼Œä½¿å‹ç¼©åçš„è¡¨ç¤ºèƒ½ç›´æ¥è¢«æ ‡å‡†çš„Transformerå¤„ç†ï¼Œä»è€Œä¿ç•™äº†æ¨¡å‹æ¶æ„çš„ç®€æ´æ€§ã€‚åœ¨WikiText-103å’ŒPG-19åŸºå‡†æµ‹è¯•ä¸­çš„è¯„ä¼°ç»“æœæ˜¾ç¤ºï¼Œè¯¥æ–¹æ³•ç›¸æ¯”æœªå‹ç¼©çš„åŸºå‡†æ¨¡å‹æœ‰æ•ˆé™ä½äº†å›°æƒ‘åº¦(Perplexity)ã€‚ç ”ç©¶è¿›ä¸€æ­¥è¯æ˜ï¼Œæ¨¡å‹æ€§èƒ½éšç€å‹ç¼©åçš„æ—¶é—´ä¸Šä¸‹æ–‡å¢åŠ è€ŒæŒç»­æå‡ï¼Œè¯å®äº†è¾“å…¥å±‚çº§çš„å¯¹æ•°å‹ç¼©æ˜¯æ‰©å±•Transformeré•¿ç¨‹è®°å¿†(Long-range Memory)çš„ä¸€ç§ç®€å•ä¸”é«˜æ•ˆçš„æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.22109v1",
      "published_date": "2025-10-25 01:29:37 UTC",
      "updated_date": "2025-10-25 01:29:37 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T05:40:09.266804+00:00"
    },
    {
      "arxiv_id": "2510.22108v1",
      "title": "STAR-RIS-assisted Collaborative Beamforming for Low-altitude Wireless Networks",
      "title_zh": "é¢å‘ä½ç©ºæ— çº¿ç½‘ç»œçš„ STAR-RIS è¾…åŠ©åä½œæ³¢æŸæˆå½¢",
      "authors": [
        "Xinyue Liang",
        "Hui Kang",
        "Junwei Che",
        "Jiahui Li",
        "Geng Sun",
        "Qingqing Wu",
        "Jiacheng Wang",
        "Dusit Niyato"
      ],
      "abstract": "While low-altitude wireless networks (LAWNs) based on uncrewed aerial vehicles (UAVs) offer high mobility, flexibility, and coverage for urban communications, they face severe signal attenuation in dense environments due to obstructions. To address this critical issue, we consider introducing collaborative beamforming (CB) of UAVs and omnidirectional reconfigurable beamforming (ORB) of simultaneous transmitting and reflecting reconfigurable intelligent surfaces (STAR-RIS) to enhance the signal quality and directionality. On this basis, we formulate a joint rate and energy optimization problem (JREOP) to maximize the transmission rate of the overall system, while minimizing the energy consumption of the UAV swarm. Due to the non-convex and NP-hard nature of JREOP, we propose a heterogeneous multi-agent collaborative dynamic (HMCD) optimization framework, which has two core components. The first component is a simulated annealing (SA)-based STAR-RIS control method, which dynamically optimizes reflection and transmission coefficients to enhance signal propagation. The second component is an improved multi-agent deep reinforcement learning (MADRL) control method, which incorporates a self-attention evaluation mechanism to capture interactions between UAVs and an adaptive velocity transition mechanism to enhance training stability. Simulation results demonstrate that HMCD outperforms various baselines in terms of convergence speed, average transmission rate, and energy consumption. Further analysis reveals that the average transmission rate of the overall system scales positively with both UAV count and STAR-RIS element numbers.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ä½ç©ºæ— çº¿ç½‘ç»œ(LAWNs)ä¸­æ— äººæœº(UAVs)åœ¨å¯†é›†ç¯å¢ƒä¸‹é¢ä¸´çš„ä¿¡å·è¡°å‡é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§ç»“åˆååŒæ³¢æŸæˆå½¢(Collaborative Beamforming)ä¸åŒæ—¶ä¼ è¾“ä¸åå°„å¯é‡æ„æ™ºèƒ½è¡¨é¢(STAR-RIS)çš„è”åˆå¢å¼ºæ–¹æ¡ˆã€‚ç ”ç©¶é€šè¿‡æ„å»ºè”åˆé€Ÿç‡ä¸èƒ½é‡ä¼˜åŒ–é—®é¢˜(JREOP)ï¼Œå¹¶å¼€å‘äº†å¼‚æ„å¤šæ™ºèƒ½ä½“åä½œåŠ¨æ€(HMCD)ä¼˜åŒ–æ¡†æ¶ï¼Œæ—¨åœ¨æœ€å¤§åŒ–ç³»ç»Ÿä¼ è¾“é€Ÿç‡çš„åŒæ—¶æœ€å°åŒ–æ— äººæœºé›†ç¾¤èƒ½è€—ã€‚è¯¥æ¡†æ¶é›†æˆäº†åŸºäºæ¨¡æ‹Ÿé€€ç«(Simulated Annealing)çš„STAR-RISæ§åˆ¶æ–¹æ³•ä»¥åŠæ”¹è¿›çš„å¤šæ™ºèƒ½ä½“æ·±åº¦å¼ºåŒ–å­¦ä¹ (MADRL)ç®—æ³•ï¼Œå¹¶å¼•å…¥è‡ªæ³¨æ„åŠ›è¯„ä¼°ä¸è‡ªé€‚åº”é€Ÿåº¦è½¬æ¢æœºåˆ¶ä»¥æå‡è®­ç»ƒç¨³å®šæ€§ã€‚ä»¿çœŸç»“æœè¡¨æ˜ï¼ŒHMCDåœ¨æ”¶æ•›é€Ÿåº¦ã€å¹³å‡ä¼ è¾“é€Ÿç‡å’Œèƒ½é‡æ•ˆç‡æ–¹é¢å‡æ˜¾è‘—ä¼˜äºç°æœ‰åŸºå‡†æ¨¡å‹ã€‚è¿›ä¸€æ­¥åˆ†æè¯å®ï¼Œç³»ç»Ÿæ€§èƒ½éšUAVæ•°é‡å’ŒSTAR-RISå…ƒä»¶è§„æ¨¡çš„å¢åŠ è€Œæ­£å‘æ‰©å±•ï¼Œä¸ºå¤æ‚åŸå¸‚ç¯å¢ƒä¸‹çš„ä½ç©ºé€šä¿¡ä¼˜åŒ–æä¾›äº†æœ‰æ•ˆçš„æŠ€æœ¯è·¯å¾„ã€‚",
      "categories": [
        "cs.NI",
        "cs.AI"
      ],
      "primary_category": "cs.NI",
      "comment": "13 pages, 9 figures, submitted to IEEE Transactions on Communications",
      "pdf_url": "https://arxiv.org/pdf/2510.22108v1",
      "published_date": "2025-10-25 01:28:37 UTC",
      "updated_date": "2025-10-25 01:28:37 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T05:40:15.574622+00:00"
    },
    {
      "arxiv_id": "2510.22107v2",
      "title": "Discovering Latent Graphs with GFlowNets for Diverse Conditional Image Generation",
      "title_zh": "åŸºäº GFlowNets å‘ç°æ½œå›¾ä»¥å®ç°å¤šæ ·åŒ–æ¡ä»¶å›¾åƒç”Ÿæˆ",
      "authors": [
        "Bailey Trang",
        "Parham Saremi",
        "Alan Q. Wang",
        "Fangrui Huang",
        "Zahra TehraniNasab",
        "Amar Kumar",
        "Tal Arbel",
        "Li Fei-Fei",
        "Ehsan Adeli"
      ],
      "abstract": "Capturing diversity is crucial in conditional and prompt-based image generation, particularly when conditions contain uncertainty that can lead to multiple plausible outputs. To generate diverse images reflecting this diversity, traditional methods often modify random seeds, making it difficult to discern meaningful differences between samples, or diversify the input prompt, which is limited in verbally interpretable diversity. We propose Rainbow, a novel conditional image generation framework, applicable to any pretrained conditional generative model, that addresses inherent condition/prompt uncertainty and generates diverse plausible images. Rainbow is based on a simple yet effective idea: decomposing the input condition into diverse latent representations, each capturing an aspect of the uncertainty and generating a distinct image. First, we integrate a latent graph, parameterized by Generative Flow Networks (GFlowNets), into the prompt representation computation. Second, leveraging GFlowNets' advanced graph sampling capabilities to capture uncertainty and output diverse trajectories over the graph, we produce multiple trajectories that collectively represent the input condition, leading to diverse condition representations and corresponding output images. Evaluations on natural image and medical image datasets demonstrate Rainbow's improvement in both diversity and fidelity across image synthesis, image generation, and counterfactual generation tasks.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†Rainbowï¼Œä¸€ç§é€‚ç”¨äºä»»ä½•é¢„è®­ç»ƒæ¡ä»¶ç”Ÿæˆæ¨¡å‹çš„æ–°å‹æ¡ä»¶å›¾åƒç”Ÿæˆæ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³è¾“å…¥æ¡ä»¶æˆ–æç¤º(prompt)ä¸­çš„å†…åœ¨ä¸ç¡®å®šæ€§å¹¶ç”Ÿæˆå¤šæ ·çš„åˆç†å›¾åƒã€‚è¯¥æ¡†æ¶åŸºäºå°†è¾“å…¥æ¡ä»¶åˆ†è§£ä¸ºå¤šæ ·æ½œåœ¨è¡¨ç¤º(latent representations)çš„æ ¸å¿ƒæ€æƒ³ï¼Œä½¿æ¯ä¸ªè¡¨ç¤ºéƒ½èƒ½æ•æ‰ä¸ç¡®å®šæ€§çš„ç‰¹å®šæ–¹é¢ã€‚ç ”ç©¶é€šè¿‡å°†ç”±ç”Ÿæˆæµç½‘ç»œ(GFlowNets)å‚æ•°åŒ–çš„æ½œåœ¨å›¾(latent graph)é›†æˆåˆ°æç¤ºè¡¨ç¤ºè®¡ç®—ä¸­ï¼Œå¹¶åˆ©ç”¨GFlowNetså…ˆè¿›çš„å›¾é‡‡æ ·èƒ½åŠ›åœ¨å›¾ä¸Šè¾“å‡ºå¤šæ ·è½¨è¿¹ï¼Œä»è€Œäº§ç”Ÿä»£è¡¨è¾“å…¥æ¡ä»¶çš„å¤šç§è¡¨ç¤ºã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒRainbowåœ¨è‡ªç„¶å›¾åƒå’ŒåŒ»å­¦å›¾åƒæ•°æ®é›†ä¸Šçš„å›¾åƒåˆæˆã€å›¾åƒç”ŸæˆåŠåäº‹å®ç”Ÿæˆ(counterfactual generation)ä»»åŠ¡ä¸­ï¼Œå‡åœ¨å¤šæ ·æ€§å’Œä¿çœŸåº¦æ–¹é¢å–å¾—äº†æ˜¾è‘—æ”¹è¿›ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.22107v2",
      "published_date": "2025-10-25 01:25:50 UTC",
      "updated_date": "2025-12-18 20:41:10 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T05:40:18.863403+00:00"
    },
    {
      "arxiv_id": "2510.22102v1",
      "title": "Mitigating Coordinate Prediction Bias from Positional Encoding Failures",
      "title_zh": "ç¼“è§£ä½ç½®ç¼–ç å¤±æ•ˆå¼•èµ·çš„åæ ‡é¢„æµ‹åå·®",
      "authors": [
        "Xingjian Tao",
        "Yiwei Wang",
        "Yujun Cai",
        "Yihong Luo",
        "Jing Tang"
      ],
      "abstract": "Multimodal large language models (MLLMs) excel at vision-language tasks such as VQA and document understanding, yet precise coordinate prediction remains challenging. High-resolution inputs exacerbate this difficulty by producing long token sequences that weaken positional encodings and introduce directional biases in coordinate outputs. We investigate this phenomenon by analyzing how MLLMs behave when visual positional encodings (VPEs) are deliberately perturbed through shuffling. Our analysis reveals that such perturbations induce predictable, non-random coordinate biases rather than random errors, suggesting that models rely on internal positional priors when spatial grounding signals are degraded. Crucially, we observe similar directional error patterns in natural high-resolution datasets, indicating that positional encoding failures are a key bottleneck for accurate coordinate prediction at scale. To address this issue, we propose Vision-PE Shuffle Guidance (VPSG), a training-free test-time method that leverages the directional nature of these biases for correction. VPSG runs auxiliary decoding with shuffled VPEs to isolate position-unconditioned tendencies, then uses this as negative evidence to guide digit prediction while preserving coordinate format through a lightweight finite-state machine. Experiments on ScreenSpot-Pro demonstrate reliable improvements, highlighting positional encoding robustness as a critical factor for spatial reasoning in MLLMs.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ (MLLMs) åœ¨é«˜åˆ†è¾¨ç‡è¾“å…¥ä¸‹ç”±äºè§†è§‰ä½ç½®ç¼–ç  (VPEs) å¤±æ•ˆå¯¼è‡´çš„åæ ‡é¢„æµ‹æŒ‘æˆ˜è¿›è¡Œäº†æ·±å…¥åˆ†æã€‚ç ”ç©¶å‘ç°ï¼Œå½“ä½ç½®ç¼–ç å—åˆ°æ‰°åŠ¨æ—¶ï¼Œæ¨¡å‹ä¼šäº§ç”Ÿå¯é¢„æµ‹çš„ééšæœºå®šå‘åç½®ï¼Œè¿™è¡¨æ˜æ¨¡å‹åœ¨ç©ºé—´å®šä½ä¿¡å·å‡å¼±æ—¶å€¾å‘äºä¾èµ–å†…éƒ¨ä½ç½®å…ˆéªŒã€‚ä¸ºæ­¤ï¼Œä½œè€…æå‡ºäº†ä¸€ç§æ— éœ€è®­ç»ƒçš„æ¨ç†æ—¶æ–¹æ³• Vision-PE Shuffle Guidance (VPSG)ï¼Œé€šè¿‡åˆ©ç”¨è¿™äº›å®šå‘åç½®ä½œä¸ºè´Ÿè¯æ®æ¥å¼•å¯¼åæ ‡é¢„æµ‹ã€‚VPSG ç»“åˆäº†æ´—ç‰Œåçš„ä½ç½®ç¼–ç è¾…åŠ©è§£ç ä¸è½»é‡çº§æœ‰é™çŠ¶æ€æœº (finite-state machine)ï¼Œåœ¨ä¿æŒåæ ‡æ ¼å¼çš„åŒæ—¶ä¿®æ­£é¢„æµ‹åç½®ã€‚åœ¨ ScreenSpot-Pro ä¸Šçš„å®éªŒç»“æœè¯æ˜ï¼Œè¯¥æ–¹æ³•æ˜¾è‘—æå‡äº†æ¨¡å‹åœ¨ç©ºé—´å®šä½ä»»åŠ¡ä¸­çš„è¡¨ç°ï¼Œå¹¶å¼ºè°ƒäº†ä½ç½®ç¼–ç é²æ£’æ€§å¯¹äº MLLMs ç©ºé—´æ¨ç†èƒ½åŠ›çš„é‡è¦æ€§ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.22102v1",
      "published_date": "2025-10-25 00:58:47 UTC",
      "updated_date": "2025-10-25 00:58:47 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T05:40:31.874036+00:00"
    },
    {
      "arxiv_id": "2510.22095v1",
      "title": "Embracing Trustworthy Brain-Agent Collaboration as Paradigm Extension for Intelligent Assistive Technologies",
      "title_zh": "æ‹¥æŠ±å¯ä¿¡è„‘-æ™ºèƒ½ä½“åä½œï¼šæ™ºèƒ½è¾…åŠ©æŠ€æœ¯çš„èŒƒå¼æ‰©å±•",
      "authors": [
        "Yankai Chen",
        "Xinni Zhang",
        "Yifei Zhang",
        "Yangning Li",
        "Henry Peng Zou",
        "Chunyu Miao",
        "Weizhi Zhang",
        "Xue Liu",
        "Philip S. Yu"
      ],
      "abstract": "Brain-Computer Interfaces (BCIs) offer a direct communication pathway between the human brain and external devices, holding significant promise for individuals with severe neurological impairments. However, their widespread adoption is hindered by critical limitations, such as low information transfer rates and extensive user-specific calibration. To overcome these challenges, recent research has explored the integration of Large Language Models (LLMs), extending the focus from simple command decoding to understanding complex cognitive states. Despite these advancements, deploying agentic AI faces technical hurdles and ethical concerns. Due to the lack of comprehensive discussion on this emerging direction, this position paper argues that the field is poised for a paradigm extension from BCI to Brain-Agent Collaboration (BAC). We emphasize reframing agents as active and collaborative partners for intelligent assistance rather than passive brain signal data processors, demanding a focus on ethical data handling, model reliability, and a robust human-agent collaboration framework to ensure these systems are safe, trustworthy, and effective.",
      "tldr_zh": "è„‘æœºæ¥å£ (BCIs) åœ¨è¾…åŠ©ä¸¥é‡ç¥ç»æŸä¼¤æ‚£è€…æ–¹é¢å…·æœ‰å·¨å¤§æ½œåŠ›ï¼Œä½†ç›®å‰ä»å—åˆ°ä¿¡æ¯ä¼ è¾“é€Ÿç‡ä½å’Œç”¨æˆ·ç‰¹å®šæ ¡å‡†å¤æ‚ç­‰å…³é”®é™åˆ¶ã€‚è™½ç„¶å¼•å…¥å¤§è¯­è¨€æ¨¡å‹ (LLMs) æœ‰åŠ©äºç†è§£å¤æ‚çš„è®¤çŸ¥çŠ¶æ€ï¼Œä½†æ™ºèƒ½ä½“ AI çš„éƒ¨ç½²ä»é¢ä¸´æŠ€æœ¯ç“¶é¢ˆä¸ä¼¦ç†æŒ‘æˆ˜ã€‚è¿™ç¯‡è§‚ç‚¹è®ºæ–‡ (position paper) æå‡ºå°†è¯¥é¢†åŸŸçš„ç ”ç©¶èŒƒå¼ä»ä¼ ç»Ÿçš„ Brain-Computer Interfaces (BCI) æ‰©å±•ä¸ºè„‘-æ™ºèƒ½ä½“åä½œ (Brain-Agent Collaboration, BAC)ã€‚è¯¥ç ”ç©¶å¼ºè°ƒåº”å°†æ™ºèƒ½ä½“è§†ä¸ºä¸»åŠ¨ä¸”åä½œçš„åˆä½œä¼™ä¼´ï¼Œè€Œéå•çº¯çš„è¢«åŠ¨è„‘ä¿¡å·å¤„ç†å™¨ã€‚ä¸ºç¡®ä¿ç³»ç»Ÿçš„å®‰å…¨æ€§ä¸æœ‰æ•ˆæ€§ï¼Œè®ºæ–‡é‡ç‚¹è®¨è®ºäº†ä¼¦ç†æ•°æ®å¤„ç†ã€æ¨¡å‹å¯é æ€§ä»¥åŠç¨³å¥çš„äººæœºåä½œæ¡†æ¶ã€‚è¿™ä¸€èŒƒå¼è½¬å‹çš„æå‡ºæ—¨åœ¨æ„å»ºæ›´å¯ä¿¡ã€æ›´æ™ºèƒ½çš„è¾…åŠ©æŠ€æœ¯ï¼Œä¸ºæœªæ¥äººæœºå…±ç”Ÿç³»ç»Ÿçš„å‘å±•æä¾›äº†é‡è¦æŒ‡å¯¼ã€‚",
      "categories": [
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.AI",
      "comment": "Accepted by NeurIPS'25 Position Track",
      "pdf_url": "https://arxiv.org/pdf/2510.22095v1",
      "published_date": "2025-10-25 00:25:45 UTC",
      "updated_date": "2025-10-25 00:25:45 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T05:40:36.254700+00:00"
    }
  ],
  "processing_status": "completed",
  "error": null,
  "raw_papers_fetched": true,
  "papers_count": 72,
  "processed_papers_count": 72,
  "failed_papers_count": 0,
  "llm_backup_calls": 0,
  "summary_generated": true,
  "daily_data_saved": true,
  "last_update": "2026-01-25T05:41:25.754767+00:00"
}
{
  "date": "2024-06-09",
  "category": "cs.AI",
  "summary": "欢迎来到 UTC 时间 2024-06-09 的 arXiv 中文 TLDR 快报！今天 arXiv 更新了 56 篇论文，主要聚焦于 AI 模型创新（如 LLM 在代理和对话系统中的应用）、图像生成与处理、医疗诊断等领域，其中 LLM 相关研究（如 Joscha Bach 的合作论文）特别引人注目，同时有几篇与 ACL 和 NeurIPS 等知名会议相关的作品展示了高效算法和基准测试的进展。\n\n### 重点论文讨论\n我将优先讨论 AI 和 LLM 领域的论文，这些主题热门且有话题度，其次快速概述图像处理和医疗相关的重要工作。对于其他较常规的论文，我会简要掠过，以控制篇幅。\n\n#### AI 和 LLM 创新\n- **Building Artificial Intelligence with Creative Agency and Self-hood** (英文原题：Building Artificial Intelligence with Creative Agency and Self-hood)  \n  作者包括知名 AI 学者 Joscha Bach，这篇论文使用 autocatalytic networks 框架建模 AI 的自组织和进化，核心贡献是提出一种能模拟人类般创造性和自我身份的 AI 代理，暗示 AI 通过创意任务可实现心理疗愈和自我强化。\n\n- **Hello Again! LLM-powered Personalized Agent for Long-term Dialogue** (英文原题：Hello Again! LLM-powered Personalized Agent for Long-term Dialogue)  \n  作者包括 Tat-Seng Chua，这篇 ACL 2025 接受论文引入 LD-Agent 框架，核心发现是通过事件感知和人格提取模块，LLM 能处理长期对话并提升个性化响应，在多数据集上表现出色，显著改善了对话系统的泛化能力。\n\n- **Why Don't Prompt-Based Fairness Metrics Correlate?** (英文原题：Why Don't Prompt-Based Fairness Metrics Correlate?)  \n  这篇 ACL 2024 论文分析了 LLM 公平性评估中的不一致问题，核心贡献是提出 CAIRO 方法，通过增强提示和多模型选择，提高公平指标的相关性，从 0.3 到 0.98，提升了 LLM 在性别和宗教偏见检测的鲁棒性。\n\n- **TTM-RE: Memory-Augmented Document-Level Relation Extraction** (英文原题：TTM-RE: Memory-Augmented Document-Level Relation Extraction)  \n  ACL 2024 接受论文，作者使用 Token Turing Machine 结合噪声鲁棒损失，核心发现是显著提升文档级关系提取的性能，在 ReDocRED 数据集上 F1 分数提高超过 3%，证明了大规模噪声数据在关系建模中的潜力。\n\n- **Whose Preferences? Differences in Fairness Preferences and Their Impact on the Fairness of AI Utilizing Human Feedback** (英文原题：Whose Preferences? Differences in Fairness Preferences and Their Impact on the Fairness of AI Utilizing Human Feedback)  \n  另一篇 ACL 2024 论文，探讨了人类反馈在 LLM 公平性中的偏好差异，核心贡献是通过数据集分析和集成分类器，揭示了不同人群（如种族和政治立场）的偏好影响，并提升了内容审核的公平性。\n\n- **II-Bench: An Image Implication Understanding Benchmark for Multimodal Large Language Models** (英文原题：II-Bench: An Image Implication Understanding Benchmark for Multimodal Large Language Models)  \n  这篇论文构建了 II-Bench 基准，评估 MLLM 在图像隐含理解中的性能，核心发现是现有模型准确率仅 74.8% 而人类达 90%，并通过提示优化改善了图像情感理解，填补了高阶感知评估的空白。\n\n- **3D-MolT5: Leveraging Discrete Structural Information for Molecule-Text Modeling** (英文原题：3D-MolT5: Leveraging Discrete Structural Information for Molecule-Text Modeling)  \n  ICLR 2025 接受论文，提出统一框架整合分子序列和 3D 结构，核心贡献是通过多任务预训练提升分子文本建模，在下游任务中超越现有方法，展示了 3D 信息在药物发现中的潜力。\n\n- **Flow of Reasoning: Training LLMs for Divergent Problem Solving with Minimal Examples** (英文原题：Flow of Reasoning: Training LLMs for Divergent Problem Solving with Minimal Examples)  \n  这篇论文引入 FoR 方法，核心发现是通过 GFlowNet 优化 LLM，在少样本下生成更多样化的解决方案，在多个推理任务（如 Game24 和 GSM8k）上优于基线，提升了 LLM 的创造性问题解决能力。\n\n- **STARLING: Self-supervised Training of Text-based Reinforcement Learning Agent with Large Language Models** (英文原题：STARLING: Self-supervised Training of Text-based Reinforcement Learning Agent with Large Language Models)  \n  ACL 2024 (Findings) 论文，使用 LLM 生成游戏环境，核心贡献是提升文本强化学习的泛化性，实验显示代理在互动任务中优于人类水平，强调了自监督在文本游戏中的作用。\n\n其他 LLM 相关论文，如 **RE-RAG** 和 **Set-CLIP**，则展示了检索增强生成和多模态对齐的进展，但细节较常规，故从简：RE-RAG 通过相关性估计提升了问答鲁棒性，Set-CLIP 则在低对齐数据上改善了语义建模。\n\n#### 图像处理与生成\n- **TLCM: Training-efficient Latent Consistency Model for Image Generation with 2-8 Steps** (英文原题：TLCM: Training-efficient Latent Consistency Model for Image Generation with 2-8 Steps)  \n  这篇论文提出高效图像生成模型，核心贡献是通过数据无关蒸馏和一致性损失，在 2-8 步内生成高质量图像，在 MSCOCO 上 PSNR 达 29.79，显著优于扩散模型。\n\n- **GTR: Improving Large 3D Reconstruction Models through Geometry and Texture Refinement** (英文原题：GTR: Improving Large 3D Reconstruction Models through Geometry and Texture Refinement)  \n  论文优化 3D 重建框架，核心发现是通过微分网格细调提升几何和纹理精度，在 GSO 数据集上 PSNR 达 29.79，并支持文本到 3D 生成，适用于复杂纹理场景。\n\n其他图像相关论文，如 **BD-SAT** 和 **Realtime Dynamic Gaze Target Tracking**，快速提要：BD-SAT 提供高分辨率土地覆盖数据集，提升了卫星图像分割；Realtime Dynamic Gaze Target Tracking 通过注意力机制实现了实时注视跟踪，但影响力较小。\n\n#### 医疗与应用AI\n- **Multi-Stain Multi-Level Convolutional Network for Multi-Tissue Breast Cancer Image Segmentation** (英文原题：Multi-Stain Multi-Level Convolutional Network for Multi-Tissue Breast Cancer Image Segmentation)  \n  论文提出多染色 CNN 模型，核心贡献是通过多分辨率合并和上下文增强，在乳腺癌图像上 mIOU 达 0.72，实现染色无关分割和组织分类。\n\n其他医疗论文，如 **Few-Shot Load Forecasting** 和 **Self-Distilled Disentanglement**，则在少样本预测和因果推理上取得进展，但不为主流，故略过细节。\n\n#### 其他领域快速掠过\n剩余论文涉及数学优化（如 **Global Sensitivity Analysis**）、物理模拟（如 **Numerical solution of a PDE**）和一般 AI 框架（如 **VillagerAgent**），这些工作虽有技术贡献，但较专业或不具广泛话题度，仅提要：VillagerAgent 在 Minecraft 中通过图优化提升多代理协作；其他如因子图优化和因果图方法在特定领域（如编码和因果效应）有小幅改进。\n\n总之，今天的 arXiv 更新突显了 AI 领域的活力，尤其在 LLM 和图像生成上，期待这些创新推动更可靠的模型应用！如果有特定论文感兴趣，建议查阅原文。",
  "papers": [
    {
      "arxiv_id": "2406.06644v4",
      "title": "Latent Diffusion Model-Enabled Low-Latency Semantic Communication in the Presence of Semantic Ambiguities and Wireless Channel Noises",
      "title_zh": "翻译失败",
      "authors": [
        "Jianhua Pei",
        "Cheng Feng",
        "Ping Wang",
        "Hina Tabassum",
        "Dongyuan Shi"
      ],
      "abstract": "Deep learning (DL)-based Semantic Communications (SemCom) is becoming\ncritical to maximize overall efficiency of communication networks.\nNevertheless, SemCom is sensitive to wireless channel uncertainties, source\noutliers, and suffer from poor generalization bottlenecks. To address the\nmentioned challenges, this paper develops a latent diffusion model-enabled\nSemCom system with three key contributions, i.e., i) to handle potential\noutliers in the source data, semantic errors obtained by projected gradient\ndescent based on the vulnerabilities of DL models, are utilized to update the\nparameters and obtain an outlier-robust encoder, ii) a lightweight single-layer\nlatent space transformation adapter completes one-shot learning at the\ntransmitter and is placed before the decoder at the receiver, enabling\nadaptation for out-of-distribution data and enhancing human-perceptual quality,\nand iii) an end-to-end consistency distillation (EECD) strategy is used to\ndistill the diffusion models trained in latent space, enabling deterministic\nsingle or few-step low-latency denoising in various noisy channels while\nmaintaining high semantic quality. Extensive numerical experiments across\ndifferent datasets demonstrate the superiority of the proposed SemCom system,\nconsistently proving its robustness to outliers, the capability to transmit\ndata with unknown distributions, and the ability to perform real-time channel\ndenoising tasks while preserving high human perceptual quality, outperforming\nthe existing denoising approaches in semantic metrics such as multi-scale\nstructural similarity index measure (MS-SSIM) and learned perceptual image path\nsimilarity (LPIPS).",
      "tldr_zh": "该论文提出了一种基于潜在扩散模型（Latent Diffusion Model）的语义通信（SemCom）系统，旨在解决深度学习（DL）模型在无线通道噪声和语义模糊下的鲁棒性问题。该系统的主要贡献包括：i) 使用基于投影梯度下降（Projected Gradient Descent）的语义错误优化来创建鲁棒编码器，处理源数据异常；ii) 引入轻量级单层潜在空间转换适配器，支持发射端的One-shot Learning和接收端的适应，提升人类感知质量；以及iii) 端到端一致性蒸馏（EECD）策略，实现低延迟单步或少步去噪，同时保持高语义质量。实验在多种数据集上验证了该系统的优越性，在鲁棒性、传输未知分布数据和实时去噪任务方面均超过现有方法，尤其在多尺度结构相似性指数（MS-SSIM）和学习感知图像路径相似性（LPIPS）等语义指标上表现出色。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2406.06644v4",
      "published_date": "2024-06-09 23:39:31 UTC",
      "updated_date": "2025-02-14 11:57:02 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T17:55:29.149136"
    },
    {
      "arxiv_id": "2407.10978v1",
      "title": "Building Artificial Intelligence with Creative Agency and Self-hood",
      "title_zh": "翻译失败",
      "authors": [
        "Liane Gabora",
        "Joscha Bach"
      ],
      "abstract": "This paper is an invited layperson summary for The Academic of the paper\nreferenced on the last page. We summarize how the formal framework of\nautocatalytic networks offers a means of modeling the origins of\nself-organizing, self-sustaining structures that are sufficiently complex to\nreproduce and evolve, be they organisms undergoing biological evolution,\nnovelty-generating minds driving cultural evolution, or artificial intelligence\nnetworks such as large language models. The approach can be used to analyze and\ndetect phase transitions in vastly complex networks that have proven\nintractable with other approaches, and suggests a promising avenue to building\nan autonomous, agentic AI self. It seems reasonable to expect that such an\nautocatalytic AI would possess creative agency akin to that of humans, and\nundergo psychologically healing -- i.e., therapeutic -- internal transformation\nthrough engagement in creative tasks. Moreover, creative tasks would be\nexpected to help such an AI solidify its self-identity.",
      "tldr_zh": "本论文总结了 autocatalytic networks 的正式框架，作为一种建模自组织、自维持结构的工具，这些结构足够复杂以实现复制和进化，包括生物进化、文化进化中的创新思维，以及像 large language models 这样的 artificial intelligence networks。该方法能有效分析复杂网络中的 phase transitions，提供了一个有前景的途径来构建自主、代理式的 AI 自主体（self-hood）。研究发现，这种 autocatalytic AI 可能具备类似于人类的 creative agency，并通过参与创造性任务实现心理治疗般的内部转变，从而强化其自我身份。",
      "categories": [
        "cs.AI",
        "q-bio.NC"
      ],
      "primary_category": "cs.AI",
      "comment": "5 pages; 1 figure. The Academic, May 20. 2024.\n  https://theacademic.com/building-artificial-intelligence-creative-agency-and-self-hood/",
      "pdf_url": "http://arxiv.org/pdf/2407.10978v1",
      "published_date": "2024-06-09 22:28:11 UTC",
      "updated_date": "2024-06-09 22:28:11 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T17:55:39.409904"
    },
    {
      "arxiv_id": "2406.05925v2",
      "title": "Hello Again! LLM-powered Personalized Agent for Long-term Dialogue",
      "title_zh": "翻译失败",
      "authors": [
        "Hao Li",
        "Chenghao Yang",
        "An Zhang",
        "Yang Deng",
        "Xiang Wang",
        "Tat-Seng Chua"
      ],
      "abstract": "Open-domain dialogue systems have seen remarkable advancements with the\ndevelopment of large language models (LLMs). Nonetheless, most existing\ndialogue systems predominantly focus on brief single-session interactions,\nneglecting the real-world demands for long-term companionship and personalized\ninteractions with chatbots. Crucial to addressing this real-world need are\nevent summary and persona management, which enable reasoning for appropriate\nlong-term dialogue responses. Recent progress in the human-like cognitive and\nreasoning capabilities of LLMs suggests that LLM-based agents could\nsignificantly enhance automated perception, decision-making, and\nproblem-solving. In response to this potential, we introduce a model-agnostic\nframework, the Long-term Dialogue Agent (LD-Agent), which incorporates three\nindependently tunable modules dedicated to event perception, persona\nextraction, and response generation. For the event memory module, long and\nshort-term memory banks are employed to separately focus on historical and\nongoing sessions, while a topic-based retrieval mechanism is introduced to\nenhance the accuracy of memory retrieval. Furthermore, the persona module\nconducts dynamic persona modeling for both users and agents. The integration of\nretrieved memories and extracted personas is subsequently fed into the\ngenerator to induce appropriate responses. The effectiveness, generality, and\ncross-domain capabilities of LD-Agent are empirically demonstrated across\nvarious illustrative benchmarks, models, and tasks. The code is released at\nhttps://github.com/leolee99/LD-Agent.",
      "tldr_zh": "本文提出 LD-Agent，一种基于大型语言模型 (LLMs) 的模型无关框架，用于实现长期对话中的个性化互动，解决现有系统忽略长期陪伴和个性化需求的局限。框架包括三个模块：事件感知（使用长短期记忆库和基于主题的检索机制管理历史和当前会话）、角色提取（动态建模用户和代理的角色），以及响应生成模块，通过整合记忆和角色信息生成适当的对话响应。实验结果显示，LD-Agent 在多个基准测试中表现出色，具有良好的泛化性和跨域能力。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "Accepted to NAACL 2025",
      "pdf_url": "http://arxiv.org/pdf/2406.05925v2",
      "published_date": "2024-06-09 21:58:32 UTC",
      "updated_date": "2025-02-13 18:02:34 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T17:55:48.639164"
    },
    {
      "arxiv_id": "2406.05918v1",
      "title": "Why Don't Prompt-Based Fairness Metrics Correlate?",
      "title_zh": "为什么基于提示的公平性指标不相关联？",
      "authors": [
        "Abdelrahman Zayed",
        "Goncalo Mordido",
        "Ioana Baldini",
        "Sarath Chandar"
      ],
      "abstract": "The widespread use of large language models has brought up essential\nquestions about the potential biases these models might learn. This led to the\ndevelopment of several metrics aimed at evaluating and mitigating these biases.\nIn this paper, we first demonstrate that prompt-based fairness metrics exhibit\npoor agreement, as measured by correlation, raising important questions about\nthe reliability of fairness assessment using prompts. Then, we outline six\nrelevant reasons why such a low correlation is observed across existing\nmetrics. Based on these insights, we propose a method called Correlated\nFairness Output (CAIRO) to enhance the correlation between fairness metrics.\nCAIRO augments the original prompts of a given fairness metric by using several\npre-trained language models and then selects the combination of the augmented\nprompts that achieves the highest correlation across metrics. We show a\nsignificant improvement in Pearson correlation from 0.3 and 0.18 to 0.90 and\n0.98 across metrics for gender and religion biases, respectively. Our code is\navailable at https://github.com/chandar-lab/CAIRO.",
      "tldr_zh": "该研究发现，提示-based 公平性指标在评估大型语言模型偏见时，相关性很低，这质疑了其可靠性，并总结了六种原因导致这种低相关性。针对这一问题，论文提出 CAIRO 方法，通过使用多个预训练语言模型增强原始提示，并选择最佳组合来提升指标间相关性。实验结果显示，CAIRO 将 Pearson correlation 从 0.3 和 0.18 提高到 0.90 和 0.98，在性别和宗教偏见评估上取得了显著改善，并提供了开源代码。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.CY",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "In Proceedings of ACL main 2024",
      "pdf_url": "http://arxiv.org/pdf/2406.05918v1",
      "published_date": "2024-06-09 21:12:15 UTC",
      "updated_date": "2024-06-09 21:12:15 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T17:56:00.727237"
    },
    {
      "arxiv_id": "2406.05912v1",
      "title": "BD-SAT: High-resolution Land Use Land Cover Dataset & Benchmark Results for Developing Division: Dhaka, BD",
      "title_zh": "翻译失败",
      "authors": [
        "Ovi Paul",
        "Abu Bakar Siddik Nayem",
        "Anis Sarker",
        "Amin Ahsan Ali",
        "M Ashraful Amin",
        "AKM Mahbubur Rahman"
      ],
      "abstract": "Land Use Land Cover (LULC) analysis on satellite images using deep\nlearning-based methods is significantly helpful in understanding the geography,\nsocio-economic conditions, poverty levels, and urban sprawl in developing\ncountries. Recent works involve segmentation with LULC classes such as\nfarmland, built-up areas, forests, meadows, water bodies, etc. Training deep\nlearning methods on satellite images requires large sets of images annotated\nwith LULC classes. However, annotated data for developing countries are scarce\ndue to a lack of funding, absence of dedicated residential/industrial/economic\nzones, a large population, and diverse building materials. BD-SAT provides a\nhigh-resolution dataset that includes pixel-by-pixel LULC annotations for Dhaka\nmetropolitan city and surrounding rural/urban areas. Using a strict and\nstandardized procedure, the ground truth is created using Bing satellite\nimagery with a ground spatial distance of 2.22 meters per pixel. A three-stage,\nwell-defined annotation process has been followed with support from GIS experts\nto ensure the reliability of the annotations. We performed several experiments\nto establish benchmark results. The results show that the annotated BD-SAT is\nsufficient to train large deep learning models with adequate accuracy for five\nmajor LULC classes: forest, farmland, built-up areas, water bodies, and\nmeadows.",
      "tldr_zh": "本研究介绍了 BD-SAT，这是一个高分辨率数据集，针对孟加拉国达卡地区，提供像素级土地利用和土地覆盖 (LULC) 注释，以支持发展中国家的地理、社会经济分析和城市扩张研究。数据集使用 Bing 卫星图像（地面空间距离为 2.22 米每像素）通过三阶段标准化注释过程创建，由 GIS 专家参与确保可靠性。实验结果显示，BD-SAT 能够有效训练 deep learning 模型，在 forest、farmland、built-up areas、water bodies 和 meadows 等五种主要 LULC 类上实现足够的分类准确性，建立基准性能。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "26 pages, 15 figures and 12 tables",
      "pdf_url": "http://arxiv.org/pdf/2406.05912v1",
      "published_date": "2024-06-09 20:54:58 UTC",
      "updated_date": "2024-06-09 20:54:58 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T17:56:12.275175"
    },
    {
      "arxiv_id": "2406.18595v1",
      "title": "Realtime Dynamic Gaze Target Tracking and Depth-Level Estimation",
      "title_zh": "实时动态凝视目标跟踪和深度水平估计",
      "authors": [
        "Esmaeil Seraj",
        "Harsh Bhate",
        "Walter Talamonti"
      ],
      "abstract": "The integration of Transparent Displays (TD) in various applications, such as\nHeads-Up Displays (HUDs) in vehicles, is a burgeoning field, poised to\nrevolutionize user experiences. However, this innovation brings forth\nsignificant challenges in realtime human-device interaction, particularly in\naccurately identifying and tracking a user's gaze on dynamically changing TDs.\nIn this paper, we present a two-fold robust and efficient systematic solution\nfor realtime gaze monitoring, comprised of: (1) a tree-based algorithm for\nidentifying and dynamically tracking gaze targets (i.e., moving, size-changing,\nand overlapping 2D content) projected on a transparent display, in realtime;\n(2) a multi-stream self-attention architecture to estimate the depth-level of\nhuman gaze from eye tracking data, to account for the display's transparency\nand preventing undesired interactions with the TD. We collected a real-world\neye-tracking dataset to train and test our gaze monitoring system. We present\nextensive results and ablation studies, including inference experiments on\nSystem on Chip (SoC) evaluation boards, demonstrating our model's scalability,\nprecision, and realtime feasibility in both static and dynamic contexts. Our\nsolution marks a significant stride in enhancing next-generation user-device\ninteraction and experience, setting a new benchmark for algorithmic gaze\nmonitoring technology in dynamic transparent displays.",
      "tldr_zh": "本研究针对透明显示器（TD）如车辆抬头显示器（HUDs）在实时人机交互中的挑战，提出了一种高效系统解决方案，用于动态跟踪用户注视目标并估计注视深度水平。具体而言，该方案包括：(1) 一个基于树的算法，实时识别并跟踪投影在 TD 上的移动、尺寸变化和重叠的 2D 内容；(2) 一个多流自注意力架构，从眼动跟踪数据中估计注视深度，以处理透明性并避免不期望交互。研究团队收集了真实世界的眼动跟踪数据集，并通过广泛的实验和消融研究验证了模型在 System on Chip (SoC) 评估板上的可扩展性、精确性和实时性能。结果显示，该系统在静态和动态场景中显著提升了用户设备交互体验，为动态透明显示器的注视监控技术设定了新基准。",
      "categories": [
        "cs.HC",
        "cs.AI",
        "cs.CV",
        "cs.LG"
      ],
      "primary_category": "cs.HC",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2406.18595v1",
      "published_date": "2024-06-09 20:52:47 UTC",
      "updated_date": "2024-06-09 20:52:47 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T17:56:25.392817"
    },
    {
      "arxiv_id": "2406.05906v1",
      "title": "TTM-RE: Memory-Augmented Document-Level Relation Extraction",
      "title_zh": "翻译失败",
      "authors": [
        "Chufan Gao",
        "Xuan Wang",
        "Jimeng Sun"
      ],
      "abstract": "Document-level relation extraction aims to categorize the association between\nany two entities within a document. We find that previous methods for\ndocument-level relation extraction are ineffective in exploiting the full\npotential of large amounts of training data with varied noise levels. For\nexample, in the ReDocRED benchmark dataset, state-of-the-art methods trained on\nthe large-scale, lower-quality, distantly supervised training data generally do\nnot perform better than those trained solely on the smaller, high-quality,\nhuman-annotated training data. To unlock the full potential of large-scale\nnoisy training data for document-level relation extraction, we propose TTM-RE,\na novel approach that integrates a trainable memory module, known as the Token\nTuring Machine, with a noisy-robust loss function that accounts for the\npositive-unlabeled setting. Extensive experiments on ReDocRED, a benchmark\ndataset for document-level relation extraction, reveal that TTM-RE achieves\nstate-of-the-art performance (with an absolute F1 score improvement of over\n3%). Ablation studies further illustrate the superiority of TTM-RE in other\ndomains (the ChemDisGene dataset in the biomedical domain) and under highly\nunlabeled settings.",
      "tldr_zh": "本研究针对文档级关系抽取（Document-level Relation Extraction）问题，指出现有方法未能充分利用大规模噪声训练数据，例如在 ReDocRED 数据集上，使用远监督数据训练的模型不如高质量人工标注数据表现更好。作者提出 TTM-RE 方法，该方法整合了可训练的内存模块 Token Turing Machine 和一个针对正未标记（positive-unlabeled）设置的噪声鲁棒损失函数，以提升模型对噪声数据的处理能力。在 ReDocRED 数据集的实验中，TTM-RE 实现了 state-of-the-art 性能，F1 分数绝对提升超过 3%，并在生物医学领域的 ChemDisGene 数据集以及高度未标记设置下表现出显著优势。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "Accepted in ACL 2024 Main",
      "pdf_url": "http://arxiv.org/pdf/2406.05906v1",
      "published_date": "2024-06-09 20:18:58 UTC",
      "updated_date": "2024-06-09 20:18:58 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T17:56:37.081494"
    },
    {
      "arxiv_id": "2406.05902v1",
      "title": "Whose Preferences? Differences in Fairness Preferences and Their Impact on the Fairness of AI Utilizing Human Feedback",
      "title_zh": "翻译失败",
      "authors": [
        "Emilia Agis Lerner",
        "Florian E. Dorner",
        "Elliott Ash",
        "Naman Goel"
      ],
      "abstract": "There is a growing body of work on learning from human feedback to align\nvarious aspects of machine learning systems with human values and preferences.\nWe consider the setting of fairness in content moderation, in which human\nfeedback is used to determine how two comments -- referencing different\nsensitive attribute groups -- should be treated in comparison to one another.\nWith a novel dataset collected from Prolific and MTurk, we find significant\ngaps in fairness preferences depending on the race, age, political stance,\neducational level, and LGBTQ+ identity of annotators. We also demonstrate that\ndemographics mentioned in text have a strong influence on how users perceive\nindividual fairness in moderation. Further, we find that differences also exist\nin downstream classifiers trained to predict human preferences. Finally, we\nobserve that an ensemble, giving equal weight to classifiers trained on\nannotations from different demographics, performs better for different\ndemographic intersections; compared to a single classifier that gives equal\nweight to each annotation.",
      "tldr_zh": "该研究探讨了在利用人类反馈训练AI公平性时，不同人口统计学特征（如种族、年龄、政治立场、教育水平和LGBTQ+身份）如何导致公平偏好差异，并分析这些差异对内容审核中AI公平性的影响。研究者使用从Prolific和MTurk收集的新数据集，发现文本中提到的demographic因素会显著影响用户对个体公平的感知，并在下游分类器训练中观察到类似差异。最终结果显示，一个ensemble模型（赋予不同demographic分类器相等权重）在处理各种demographic交集时，比单一分类器（赋予每个标注相等权重）表现出色，从而提升了AI系统的整体公平性。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL",
        "cs.CY"
      ],
      "primary_category": "cs.LG",
      "comment": "To appear in the Proceedings of the 62nd Annual Meeting of the\n  Association for Computational Linguistics, ACL 2024",
      "pdf_url": "http://arxiv.org/pdf/2406.05902v1",
      "published_date": "2024-06-09 19:42:25 UTC",
      "updated_date": "2024-06-09 19:42:25 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T17:56:49.813474"
    },
    {
      "arxiv_id": "2406.05898v2",
      "title": "Async Learned User Embeddings for Ads Delivery Optimization",
      "title_zh": "异步学习的用户嵌入用于广告投放优化",
      "authors": [
        "Mingwei Tang",
        "Meng Liu",
        "Hong Li",
        "Junjie Yang",
        "Chenglin Wei",
        "Boyang Li",
        "Dai Li",
        "Rengan Xu",
        "Yifan Xu",
        "Zehua Zhang",
        "Xiangyu Wang",
        "Linfeng Liu",
        "Yuelei Xie",
        "Chengye Liu",
        "Labib Fawaz",
        "Li Li",
        "Hongnan Wang",
        "Bill Zhu",
        "Sri Reddy"
      ],
      "abstract": "In recommendation systems, high-quality user embeddings can capture subtle\npreferences, enable precise similarity calculations, and adapt to changing\npreferences over time to maintain relevance. The effectiveness of\nrecommendation systems depends on the quality of user embedding. We propose to\nasynchronously learn high fidelity user embeddings for billions of users each\nday from sequence based multimodal user activities through a Transformer-like\nlarge scale feature learning module. The async learned user representations\nembeddings (ALURE) are further converted to user similarity graphs through\ngraph learning and then combined with user realtime activities to retrieval\nhighly related ads candidates for the ads delivery system. Our method shows\nsignificant gains in both offline and online experiments.",
      "tldr_zh": "该论文提出了一种异步学习用户嵌入（Async Learned User Embeddings）的方法，用于优化广告投放系统中的推荐性能。通过Transformer-like模块从序列化的多模态用户活动（如行为数据）中学习高保真用户表示（ALURE），并将其转换为用户相似性图（user similarity graphs）。随后，将这些图与用户实时活动结合，用于检索高度相关的广告候选。实验结果显示，该方法在离线和在线环境中实现了显著的性能提升。",
      "categories": [
        "cs.IR",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.IR",
      "comment": "Accepted by workshop on Multimodal Representation and Retrieval at\n  SIGIR 2024, Washington DC",
      "pdf_url": "http://arxiv.org/pdf/2406.05898v2",
      "published_date": "2024-06-09 19:35:20 UTC",
      "updated_date": "2024-06-23 05:43:41 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T17:57:01.279995"
    },
    {
      "arxiv_id": "2406.05887v1",
      "title": "Few-Shot Load Forecasting Under Data Scarcity in Smart Grids: A Meta-Learning Approach",
      "title_zh": "翻译失败",
      "authors": [
        "Georgios Tsoumplekas",
        "Christos L. Athanasiadis",
        "Dimitrios I. Doukas",
        "Antonios Chrysopoulos",
        "Pericles A. Mitkas"
      ],
      "abstract": "Despite the rapid expansion of smart grids and large volumes of data at the\nindividual consumer level, there are still various cases where adequate data\ncollection to train accurate load forecasting models is challenging or even\nimpossible. This paper proposes adapting an established model-agnostic\nmeta-learning algorithm for short-term load forecasting in the context of\nfew-shot learning. Specifically, the proposed method can rapidly adapt and\ngeneralize within any unknown load time series of arbitrary length using only\nminimal training samples. In this context, the meta-learning model learns an\noptimal set of initial parameters for a base-level learner recurrent neural\nnetwork. The proposed model is evaluated using a dataset of historical load\nconsumption data from real-world consumers. Despite the examined load series'\nshort length, it produces accurate forecasts outperforming transfer learning\nand task-specific machine learning methods by $12.5\\%$. To enhance robustness\nand fairness during model evaluation, a novel metric, mean average log\npercentage error, is proposed that alleviates the bias introduced by the\ncommonly used MAPE metric. Finally, a series of studies to evaluate the model's\nrobustness under different hyperparameters and time series lengths is also\nconducted, demonstrating that the proposed approach consistently outperforms\nall other models.",
      "tldr_zh": "本论文针对智能电网中数据稀缺的挑战，提出了一种基于元学习(meta-learning)的少样本(few-shot learning)短期负载预测方法，以快速适应未知负载时间序列并实现泛化，仅需最少训练样本。方法通过学习最佳初始参数集来优化基础级别的循环神经网络(RNN)，从而提升预测性能。在真实世界消费者历史负载数据上评估，该模型在短序列中比转移学习和特定任务机器学习方法提高了12.5%的准确率。此外，论文引入了新的评估指标mean average log percentage error，以缓解常用MAPE指标的偏差，并通过一系列实验证明了该方法的稳健性。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "This work has been submitted to the IEEE for possible publication",
      "pdf_url": "http://arxiv.org/pdf/2406.05887v1",
      "published_date": "2024-06-09 18:59:08 UTC",
      "updated_date": "2024-06-09 18:59:08 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T17:57:16.215852"
    },
    {
      "arxiv_id": "2406.05873v1",
      "title": "Conserving Human Creativity with Evolutionary Generative Algorithms: A Case Study in Music Generation",
      "title_zh": "翻译失败",
      "authors": [
        "Justin Kilb",
        "Caroline Ellis"
      ],
      "abstract": "This study explores the application of evolutionary generative algorithms in\nmusic production to preserve and enhance human creativity. By integrating human\nfeedback into Differential Evolution algorithms, we produced six songs that\nwere submitted to international record labels, all of which received contract\noffers. In addition to testing the commercial viability of these methods, this\npaper examines the long-term implications of content generation using\ntraditional machine learning methods compared with evolutionary algorithms.\nSpecifically, as current generative techniques continue to scale, the potential\nfor computer-generated content to outpace human creation becomes likely. This\ntrend poses a risk of exhausting the pool of human-created training data,\npotentially forcing generative machine learning models to increasingly depend\non their random input functions for generating novel content. In contrast to a\nfuture of content generation guided by aimless random functions, our approach\nallows for individualized creative exploration, ensuring that computer-assisted\ncontent generation methods are human-centric and culturally relevant through\ntime.",
      "tldr_zh": "本研究探索了进化生成算法（Evolutionary Generative Algorithms）在音乐生成中的应用，通过整合人类反馈到 Differential Evolution 算法中，成功制作了六首歌曲，这些歌曲提交国际唱片公司后全部获得合同报价，证明了其商业可行性。论文比较了传统机器学习方法与进化算法的长期影响，指出随着生成技术规模化，依赖随机输入的模型可能耗尽人类训练数据，而进化算法则支持个性化的创造性探索。总体而言，该方法强调计算机辅助内容生成应以人为本，确保文化相关性和可持续性。",
      "categories": [
        "cs.NE",
        "cs.AI",
        "math.OC"
      ],
      "primary_category": "cs.NE",
      "comment": "7 pages, 3 figures",
      "pdf_url": "http://arxiv.org/pdf/2406.05873v1",
      "published_date": "2024-06-09 18:11:05 UTC",
      "updated_date": "2024-06-09 18:11:05 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T17:57:31.405868"
    },
    {
      "arxiv_id": "2406.05872v1",
      "title": "STARLING: Self-supervised Training of Text-based Reinforcement Learning Agent with Large Language Models",
      "title_zh": "翻译失败",
      "authors": [
        "Shreyas Basavatia",
        "Keerthiram Murugesan",
        "Shivam Ratnakar"
      ],
      "abstract": "Interactive fiction games have emerged as an important application to improve\nthe generalization capabilities of language-based reinforcement learning (RL)\nagents. Existing environments for interactive fiction games are domain-specific\nor time-consuming to generate and do not train the RL agents to master a\nspecific set of skills. In this work, we introduce an interactive environment\nfor self-supervised RL, STARLING, for text-based games that bootstraps the\ntext-based RL agents with automatically generated games (based on the seed set\nof game ideas) to boost the performance and generalization capabilities to\nreach a goal of the target environment. These games let the agent hone their\nskills on a predefined set of tasks. We create and test an environment with 100\ngames, generated using this automated framework that uses large language models\n(GPT-3) and an interactive fiction game engine (based on Inform7) to provide\nthe user with the ability to generate more games under minimal human\nsupervision. Experimental results based on both the human participants and\nbaseline text-based RL agents reveal that current state-of-the-art text-based\nRL agents cannot use previously learned skills in new situations at the level\nhumans can. These results enforce STARLING's potential to serve as a sandbox\nenvironment for further research in self-supervised text-based RL.",
      "tldr_zh": "该研究引入了 STARLING，一种自监督训练框架，用于提升文本-based Reinforcement Learning (RL) 代理的泛化能力，通过利用 Large Language Models（如 GPT-3）和交互式小说游戏引擎自动生成游戏环境。STARLING 以种子游戏想法为基础，创建了100个游戏，让代理在预定义任务上磨练技能，从而提升其在目标环境中的性能。实验结果显示，当前最先进的文本 RL 代理在新情境中应用先前技能的能力远逊于人类，这突出了 STARLING 作为自监督文本 RL 研究沙盒环境的潜力。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.LG",
      "comment": "ACL 2024 (Findings)",
      "pdf_url": "http://arxiv.org/pdf/2406.05872v1",
      "published_date": "2024-06-09 18:07:47 UTC",
      "updated_date": "2024-06-09 18:07:47 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T17:57:42.579277"
    },
    {
      "arxiv_id": "2406.05866v1",
      "title": "Procrastination Is All You Need: Exponent Indexed Accumulators for Floating Point, Posits and Logarithmic Numbers",
      "title_zh": "翻译失败",
      "authors": [
        "Vincenzo Liguori"
      ],
      "abstract": "This paper discusses a simple and effective method for the summation of long\nsequences of floating point numbers. The method comprises two phases: an\naccumulation phase where the mantissas of the floating point numbers are added\nto accumulators indexed by the exponents and a reconstruction phase where the\nactual summation result is finalised. Various architectural details are given\nfor both FPGAs and ASICs including fusing the operation with a multiplier,\ncreating efficient MACs. Some results are presented for FPGAs, including a\ntensor core capable of multiplying and accumulating two 4x4 matrices of\nbfloat16 values every clock cycle using ~6,400 LUTs + 64 DSP48 in AMD FPGAs at\n700+ MHz. The method is then extended to posits and logarithmic numbers.",
      "tldr_zh": "本文提出了一种简单有效的求和方法，名为“Procrastination Is All You Need”，利用指数索引积累器（Exponent Indexed Accumulators）处理浮点数长序列，包括积累阶段（将尾数加到基于指数的积累器中）和重建阶段（最终化求和结果）。该方法针对FPGA和ASIC提供了高效架构细节，如融合乘法器的MAC（Multiply-Accumulate）操作，并在AMD FPGA上实现了每时钟周期乘并积累两个4x4 bfloat16矩阵的张量核心，使用约6400 LUTs + 64 DSP48，以700+ MHz运行。实验结果显示该方法显著提升了计算效率，并扩展适用于Posits和Logarithmic Numbers。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.AR"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2406.05866v1",
      "published_date": "2024-06-09 17:44:17 UTC",
      "updated_date": "2024-06-09 17:44:17 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T17:57:55.837930"
    },
    {
      "arxiv_id": "2406.05862v3",
      "title": "II-Bench: An Image Implication Understanding Benchmark for Multimodal Large Language Models",
      "title_zh": "翻译失败",
      "authors": [
        "Ziqiang Liu",
        "Feiteng Fang",
        "Xi Feng",
        "Xinrun Du",
        "Chenhao Zhang",
        "Zekun Wang",
        "Yuelin Bai",
        "Qixuan Zhao",
        "Liyang Fan",
        "Chengguang Gan",
        "Hongquan Lin",
        "Jiaming Li",
        "Yuansheng Ni",
        "Haihong Wu",
        "Yaswanth Narsupalli",
        "Zhigang Zheng",
        "Chengming Li",
        "Xiping Hu",
        "Ruifeng Xu",
        "Xiaojun Chen",
        "Min Yang",
        "Jiaheng Liu",
        "Ruibo Liu",
        "Wenhao Huang",
        "Ge Zhang",
        "Shiwen Ni"
      ],
      "abstract": "The rapid advancements in the development of multimodal large language models\n(MLLMs) have consistently led to new breakthroughs on various benchmarks. In\nresponse, numerous challenging and comprehensive benchmarks have been proposed\nto more accurately assess the capabilities of MLLMs. However, there is a dearth\nof exploration of the higher-order perceptual capabilities of MLLMs. To fill\nthis gap, we propose the Image Implication understanding Benchmark, II-Bench,\nwhich aims to evaluate the model's higher-order perception of images. Through\nextensive experiments on II-Bench across multiple MLLMs, we have made\nsignificant findings. Initially, a substantial gap is observed between the\nperformance of MLLMs and humans on II-Bench. The pinnacle accuracy of MLLMs\nattains 74.8%, whereas human accuracy averages 90%, peaking at an impressive\n98%. Subsequently, MLLMs perform worse on abstract and complex images,\nsuggesting limitations in their ability to understand high-level semantics and\ncapture image details. Finally, it is observed that most models exhibit\nenhanced accuracy when image sentiment polarity hints are incorporated into the\nprompts. This observation underscores a notable deficiency in their inherent\nunderstanding of image sentiment. We believe that II-Bench will inspire the\ncommunity to develop the next generation of MLLMs, advancing the journey\ntowards expert artificial general intelligence (AGI). II-Bench is publicly\navailable at https://huggingface.co/datasets/m-a-p/II-Bench.",
      "tldr_zh": "该论文提出 II-Bench，这是一个用于评估多模态大语言模型 (MLLMs) 的图像含义理解基准，专注于测试模型的高阶感知能力，如高级语义和图像细节的捕捉。实验结果显示，MLLMs 的最高准确率仅为 74.8%，远低于人类的平均 90% 和最高 98%，特别是在抽象和复杂图像上表现更差。研究还发现，加入图像情感极性提示后，模型准确率有所提升，暴露了 MLLMs 在内在情感理解方面的不足。该基准的公开可用性有望推动社区开发下一代 MLLMs，并推进人工智能通用智能 (AGI) 的发展。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.CL",
      "comment": "100 pages, 82 figures, add citations",
      "pdf_url": "http://arxiv.org/pdf/2406.05862v3",
      "published_date": "2024-06-09 17:25:47 UTC",
      "updated_date": "2025-01-13 09:33:47 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T17:58:06.704628"
    },
    {
      "arxiv_id": "2406.05855v2",
      "title": "Self-Distilled Disentangled Learning for Counterfactual Prediction",
      "title_zh": "翻译失败",
      "authors": [
        "Xinshu Li",
        "Mingming Gong",
        "Lina Yao"
      ],
      "abstract": "The advancements in disentangled representation learning significantly\nenhance the accuracy of counterfactual predictions by granting precise control\nover instrumental variables, confounders, and adjustable variables. An\nappealing method for achieving the independent separation of these factors is\nmutual information minimization, a task that presents challenges in numerous\nmachine learning scenarios, especially within high-dimensional spaces. To\ncircumvent this challenge, we propose the Self-Distilled Disentanglement\nframework, referred to as $SD^2$. Grounded in information theory, it ensures\ntheoretically sound independent disentangled representations without intricate\nmutual information estimator designs for high-dimensional representations. Our\ncomprehensive experiments, conducted on both synthetic and real-world datasets,\nconfirms the effectiveness of our approach in facilitating counterfactual\ninference in the presence of both observed and unobserved confounders.",
      "tldr_zh": "本研究提出了一种名为 $SD^2$ 的 Self-Distilled Disentanglement 框架，用于提升反事实预测的准确性。该框架基于信息理论，通过自蒸馏方法最小化互信息，实现工具变量、混杂因素和可调节变量的独立分离，而无需复杂的互信息估计器设计。在合成和真实数据集上的实验证明，该方法在处理观察和未观察混杂因素时的有效性显著增强。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "stat.ML"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2406.05855v2",
      "published_date": "2024-06-09 16:58:19 UTC",
      "updated_date": "2024-06-14 06:30:22 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T17:58:20.876903"
    },
    {
      "arxiv_id": "2406.05839v2",
      "title": "MaLa-ASR: Multimedia-Assisted LLM-Based ASR",
      "title_zh": "翻译失败",
      "authors": [
        "Guanrou Yang",
        "Ziyang Ma",
        "Fan Yu",
        "Zhifu Gao",
        "Shiliang Zhang",
        "Xie Chen"
      ],
      "abstract": "As more and more information-rich data like video become available, utilizing\nmulti-modal auxiliary information to enhance audio tasks has sparked widespread\nresearch interest. The recent surge in research on LLM-based audio models\nprovides fresh perspectives for tackling audio tasks. Given that LLM can\nflexibly ingest multiple inputs, we propose MaLa-ASR, an LLM-based ASR model\nthat can integrate textual keywords extracted from presentation slides to\nimprove recognition of conference content. MaLa-ASR yields average WERs of 9.4%\nand 11.7% on the L95 and S95 subsets of the SlideSpeech corpus, representing a\nsignificant relative WER drop of 27.9% and 44.7% over the baseline model\nreported in SlideSpeech. MaLa-ASR underscores LLM's strong performance in\nspeech tasks and the capability to integrate auxiliary information\nconveniently. By adding keywords to the input prompt, the biased word error\nrate (B-WER) reduces relatively by 46.0% and 44.2%, establishing a new SOTA on\nthis dataset.",
      "tldr_zh": "本文提出 MaLa-ASR，一种基于 LLM 的自动语音识别 (ASR) 模型，通过整合从演示幻灯片中提取的文本关键词作为辅助信息，提升会议内容的识别准确性。相比基线模型，MaLa-ASR 在 SlideSpeech 语料库的 L95 和 S95 子集上分别将 WER 降低至 9.4% 和 11.7%，实现相对下降 27.9% 和 44.7%。此外，通过在输入提示中添加关键词，该模型使偏置词错误率 (B-WER) 相对减少 46.0% 和 44.2%，确立了该数据集上的新 SOTA。MaLa-ASR 展示了 LLM 在语音任务中的强大性能，以及方便整合多模态辅助信息的潜力。",
      "categories": [
        "eess.AS",
        "cs.AI"
      ],
      "primary_category": "eess.AS",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2406.05839v2",
      "published_date": "2024-06-09 16:00:00 UTC",
      "updated_date": "2024-06-13 07:50:40 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T17:58:33.455878"
    },
    {
      "arxiv_id": "2406.05837v1",
      "title": "Solution for CVPR 2024 UG2+ Challenge Track on All Weather Semantic Segmentation",
      "title_zh": "翻译失败",
      "authors": [
        "Jun Yu",
        "Yunxiang Zhang",
        "Fengzhao Sun",
        "Leilei Wang",
        "Renjie Lu"
      ],
      "abstract": "In this report, we present our solution for the semantic segmentation in\nadverse weather, in UG2+ Challenge at CVPR 2024. To achieve robust and accurate\nsegmentation results across various weather conditions, we initialize the\nInternImage-H backbone with pre-trained weights from the large-scale joint\ndataset and enhance it with the state-of-the-art Upernet segmentation method.\nSpecifically, we utilize offline and online data augmentation approaches to\nextend the train set, which helps us to further improve the performance of the\nsegmenter. As a result, our proposed solution demonstrates advanced performance\non the test set and achieves 3rd position in this challenge.",
      "tldr_zh": "本论文介绍了针对CVPR 2024 UG2+挑战赛中全天候语义分割的解决方案，旨在实现各种恶劣天气条件下的鲁棒性和准确性。他们使用预训练于大规模联合数据集的InternImage-H作为骨干网络，结合Upernet分割方法，并通过离线和在线数据增强扩展训练集，以进一步提升性能。该方案在测试集上表现出色，最终获得挑战赛第3名。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "Solution for CVPR 2024 UG2+ Challenge Track on All Weather Semantic\n  Segmentation",
      "pdf_url": "http://arxiv.org/pdf/2406.05837v1",
      "published_date": "2024-06-09 15:56:35 UTC",
      "updated_date": "2024-06-09 15:56:35 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T17:58:44.726097"
    },
    {
      "arxiv_id": "2406.05828v1",
      "title": "Multi-Stain Multi-Level Convolutional Network for Multi-Tissue Breast Cancer Image Segmentation",
      "title_zh": "翻译失败",
      "authors": [
        "Akash Modi",
        "Sumit Kumar Jha",
        "Purnendu Mishra",
        "Rajiv Kumar",
        "Kiran Aatre",
        "Gursewak Singh",
        "Shubham Mathur"
      ],
      "abstract": "Digital pathology and microscopy image analysis are widely employed in the\nsegmentation of digitally scanned IHC slides, primarily to identify cancer and\npinpoint regions of interest (ROI) indicative of tumor presence. However,\ncurrent ROI segmentation models are either stain-specific or suffer from the\nissues of stain and scanner variance due to different staining protocols or\nmodalities across multiple labs. Also, tissues like Ductal Carcinoma in Situ\n(DCIS), acini, etc. are often classified as Tumors due to their structural\nsimilarities and color compositions. In this paper, we proposed a novel\nconvolutional neural network (CNN) based Multi-class Tissue Segmentation model\nfor histopathology whole-slide Breast slides which classify tumors and segments\nother tissue regions such as Ducts, acini, DCIS, Squamous epithelium, Blood\nVessels, Necrosis, etc. as a separate class. Our unique pixel-aligned\nnon-linear merge across spatial resolutions empowers models with both local and\nglobal fields of view for accurate detection of various classes. Our proposed\nmodel is also able to separate bad regions such as folds, artifacts, blurry\nregions, bubbles, etc. from tissue regions using multi-level context from\ndifferent resolutions of WSI. Multi-phase iterative training with context-aware\naugmentation and increasing noise was used to efficiently train a multi-stain\ngeneric model with partial and noisy annotations from 513 slides. Our training\npipeline used 12 million patches generated using context-aware augmentations\nwhich made our model stain and scanner invariant across data sources. To\nextrapolate stain and scanner invariance, our model was evaluated on 23000\npatches which were for a completely new stain (Hematoxylin and Eosin) from a\ncompletely new scanner (Motic) from a different lab. The mean IOU was 0.72\nwhich is on par with model performance on other data sources and scanners.",
      "tldr_zh": "本研究提出了一种多染色多级别卷积神经网络（CNN）模型，用于多组织乳腺癌图像分割，旨在解决现有ROI分割模型的染色特异性和染色/扫描仪变异问题。该模型能准确分类肿瘤并分割其他组织类别，如Ducts、acini、DCIS、Squamous epithelium、Blood Vessels和Necrosis，同时识别并分离坏区域（如folds、artifacts）。通过pixel-aligned non-linear merge和多级别上下文，模型结合局部和全局视野提升检测精度；训练采用多阶段迭代训练、context-aware augmentation和增加噪声策略，使用513张滑玻的1200万patches，使模型对染色和扫描仪不敏感。评估结果显示，该模型在新染色（Hematoxylin and Eosin）和新扫描仪（Motic）上的mean IOU达到0.72，与其他数据源性能相当，证明了其鲁棒性。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "eess.IV"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2406.05828v1",
      "published_date": "2024-06-09 15:35:49 UTC",
      "updated_date": "2024-06-09 15:35:49 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T17:58:56.654850"
    },
    {
      "arxiv_id": "2406.05826v2",
      "title": "PSBD: Prediction Shift Uncertainty Unlocks Backdoor Detection",
      "title_zh": "翻译失败",
      "authors": [
        "Wei Li",
        "Pin-Yu Chen",
        "Sijia Liu",
        "Ren Wang"
      ],
      "abstract": "Deep neural networks are susceptible to backdoor attacks, where adversaries\nmanipulate model predictions by inserting malicious samples into the training\ndata. Currently, there is still a significant challenge in identifying\nsuspicious training data to unveil potential backdoor samples. In this paper,\nwe propose a novel method, Prediction Shift Backdoor Detection (PSBD),\nleveraging an uncertainty-based approach requiring minimal unlabeled clean\nvalidation data. PSBD is motivated by an intriguing Prediction Shift (PS)\nphenomenon, where poisoned models' predictions on clean data often shift away\nfrom true labels towards certain other labels with dropout applied during\ninference, while backdoor samples exhibit less PS. We hypothesize PS results\nfrom the neuron bias effect, making neurons favor features of certain classes.\nPSBD identifies backdoor training samples by computing the Prediction Shift\nUncertainty (PSU), the variance in probability values when dropout layers are\ntoggled on and off during model inference. Extensive experiments have been\nconducted to verify the effectiveness and efficiency of PSBD, which achieves\nstate-of-the-art results among mainstream detection methods. The code is\navailable at https://github.com/WL-619/PSBD.",
      "tldr_zh": "这篇论文提出了 PSBD（Prediction Shift Backdoor Detection）方法，用于检测深度神经网络中的后门攻击（backdoor attacks），通过利用最小量的无标签干净验证数据来识别恶意训练样本。PSBD 基于 Prediction Shift (PS) 现象，该现象表明在应用 dropout 层时，毒化模型对干净数据的预测会从真实标签偏移，而后门样本的 PS 较小，并归因于神经元偏差效应（neuron bias effect）。通过计算 Prediction Shift Uncertainty (PSU)，即 dropout 层开关时概率值的方差，PSBD 实现了高效的检测，并在广泛实验中比主流方法取得了最先进的结果。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CR"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2406.05826v2",
      "published_date": "2024-06-09 15:31:00 UTC",
      "updated_date": "2025-04-16 02:39:44 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T17:59:08.564970"
    },
    {
      "arxiv_id": "2406.05814v2",
      "title": "TIGeR: Unifying Text-to-Image Generation and Retrieval with Large Multimodal Models",
      "title_zh": "翻译失败",
      "authors": [
        "Leigang Qu",
        "Haochuan Li",
        "Tan Wang",
        "Wenjie Wang",
        "Yongqi Li",
        "Liqiang Nie",
        "Tat-Seng Chua"
      ],
      "abstract": "How humans can effectively and efficiently acquire images has always been a\nperennial question. A classic solution is text-to-image retrieval from an\nexisting database; however, the limited database typically lacks creativity. By\ncontrast, recent breakthroughs in text-to-image generation have made it\npossible to produce attractive and counterfactual visual content, but it faces\nchallenges in synthesizing knowledge-intensive images. In this work, we rethink\nthe relationship between text-to-image generation and retrieval, proposing a\nunified framework for both tasks with one single Large Multimodal Model (LMM).\nSpecifically, we first explore the intrinsic discriminative abilities of LMMs\nand introduce an efficient generative retrieval method for text-to-image\nretrieval in a training-free manner. Subsequently, we unify generation and\nretrieval autoregressively and propose an autonomous decision mechanism to\nchoose the best-matched one between generated and retrieved images as the\nresponse to the text prompt. To standardize the evaluation of unified\ntext-to-image generation and retrieval, we construct TIGeR-Bench, a benchmark\nspanning both creative and knowledge-intensive domains. Extensive experiments\non TIGeR-Bench and two retrieval benchmarks, i.e., Flickr30K and MS-COCO,\ndemonstrate the superiority of our proposed framework.",
      "tldr_zh": "这篇论文提出了TIGeR框架，使用Large Multimodal Models (LMMs)统一文本到图像生成和检索任务，解决了传统检索缺乏创意以及生成难以处理知识密集型图像的问题。框架通过探索LMMs的内在辨别能力，实现训练-free的生成式检索，并采用自回归机制和自主决策来选择生成或检索图像中最佳匹配响应。实验在TIGeR-Bench基准（覆盖创意和知识密集型领域）以及Flickr30K和MS-COCO数据集上证明了该框架的优越性。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CL",
        "cs.LG",
        "cs.MM"
      ],
      "primary_category": "cs.CV",
      "comment": "ICLR 2025 Camera-ready",
      "pdf_url": "http://arxiv.org/pdf/2406.05814v2",
      "published_date": "2024-06-09 15:00:28 UTC",
      "updated_date": "2025-03-24 23:07:01 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T17:59:19.247092"
    },
    {
      "arxiv_id": "2406.05812v1",
      "title": "Seventeenth-Century Spanish American Notary Records for Fine-Tuning Spanish Large Language Models",
      "title_zh": "十七世纪西班牙美洲公证记录用于微调西班牙语大型语言模型",
      "authors": [
        "Shraboni Sarker",
        "Ahmad Tamim Hamad",
        "Hulayyil Alshammari",
        "Viviana Grieco",
        "Praveen Rao"
      ],
      "abstract": "Large language models have gained tremendous popularity in domains such as\ne-commerce, finance, healthcare, and education. Fine-tuning is a common\napproach to customize an LLM on a domain-specific dataset for a desired\ndownstream task. In this paper, we present a valuable resource for fine-tuning\nLLMs developed for the Spanish language to perform a variety of tasks such as\nclassification, masked language modeling, clustering, and others. Our resource\nis a collection of handwritten notary records from the seventeenth century\nobtained from the National Archives of Argentina. This collection contains a\ncombination of original images and transcribed text (and metadata) of 160+\npages that were handwritten by two notaries, namely, Estenban Agreda de Vergara\nand Nicolas de Valdivia y Brisuela nearly 400 years ago. Through empirical\nevaluation, we demonstrate that our collection can be used to fine-tune Spanish\nLLMs for tasks such as classification and masked language modeling, and can\noutperform pre-trained Spanish models and ChatGPT-3.5/ChatGPT-4o. Our resource\nwill be an invaluable resource for historical text analysis and is publicly\navailable on GitHub.",
      "tldr_zh": "本研究提出了一种基于17世纪西班牙美洲公证记录的资源，用于fine-tuning西班牙语大型语言模型(LLMs)，以支持任务如classification、masked language modeling和聚类等。数据集包括阿根廷国家档案馆的160多页手写图像和转录文本，由公证人Estenban Agreda de Vergara和Nicolas de Valdivia y Brisuela撰写。实验结果显示，该资源能显著提升模型性能，优于预训练西班牙语模型和ChatGPT-3.5/ChatGPT-4o，并已公开在GitHub上，适用于历史文本分析。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2406.05812v1",
      "published_date": "2024-06-09 14:54:22 UTC",
      "updated_date": "2024-06-09 14:54:22 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T17:59:34.900788"
    },
    {
      "arxiv_id": "2406.05805v1",
      "title": "Toward identifiability of total effects in summary causal graphs with latent confounders: an extension of the front-door criterion",
      "title_zh": "翻译失败",
      "authors": [
        "Charles K. Assaad"
      ],
      "abstract": "Conducting experiments to estimate total effects can be challenging due to\ncost, ethical concerns, or practical limitations. As an alternative,\nresearchers often rely on causal graphs to determine if it is possible to\nidentify these effects from observational data. Identifying total effects in\nfully specified non-temporal causal graphs has garnered considerable attention,\nwith Pearl's front-door criterion enabling the identification of total effects\nin the presence of latent confounding even when no variable set is sufficient\nfor adjustment. However, specifying a complete causal graph is challenging in\nmany domains. Extending these identifiability results to partially specified\ngraphs is crucial, particularly in dynamic systems where causal relationships\nevolve over time. This paper addresses the challenge of identifying total\neffects using a specific and well-known partially specified graph in dynamic\nsystems called a summary causal graph, which does not specify the temporal lag\nbetween causal relations and can contain cycles. In particular, this paper\npresents sufficient graphical conditions for identifying total effects from\nobservational data, even in the presence of hidden confounding and when no\nvariable set is sufficient for adjustment, contributing to the ongoing effort\nto understand and estimate causal effects from observational data using summary\ncausal graphs.",
      "tldr_zh": "这篇论文扩展了Pearl的front-door criterion，针对summary causal graphs（一种部分指定的动态系统因果图）中存在latent confounders的情况，探讨了total effects的可识别性问题。论文提供了足够的图形条件，允许从观察数据中识别total effects，即使没有变量集适合调整，且系统可能包含循环和时间滞后。总体而言，此工作有助于在实际应用中从观察数据中更可靠地估计因果效应。",
      "categories": [
        "stat.ME",
        "cs.AI"
      ],
      "primary_category": "stat.ME",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2406.05805v1",
      "published_date": "2024-06-09 14:43:06 UTC",
      "updated_date": "2024-06-09 14:43:06 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T17:59:54.717110"
    },
    {
      "arxiv_id": "2406.05804v6",
      "title": "A Review of Prominent Paradigms for LLM-Based Agents: Tool Use (Including RAG), Planning, and Feedback Learning",
      "title_zh": "翻译失败",
      "authors": [
        "Xinzhe Li"
      ],
      "abstract": "Tool use, planning, and feedback learning are currently three prominent\nparadigms for developing Large Language Model (LLM)-based agents across various\ntasks. Although numerous frameworks have been devised for each paradigm, their\nintricate workflows and inconsistent taxonomy create challenges in\nunderstanding and reviewing the frameworks across different paradigms. This\nsurvey introduces a unified taxonomy to systematically review and discuss these\nframeworks. Specifically, 1) the taxonomy defines environments/tasks, common\nLLM-profiled roles or LMPRs (policy models, evaluators, and dynamic models),\nand universally applicable workflows found in prior work, and 2) it enables a\ncomparison of key perspectives on the implementations of LMPRs and workflow\ndesigns across different agent paradigms and frameworks. 3) Finally, we\nidentify three limitations in existing workflow designs and systematically\ndiscuss the future work. Resources have been made publicly available at in our\nGitHub repository https://github.com/xinzhel/LLM-Agent-Survey.",
      "tldr_zh": "这篇论文审视了大型语言模型(LLM)-based agents 的三个主要范式：Tool Use (包括 RAG)、Planning 和 Feedback Learning，并介绍了统一的分类法(taxonomy)来系统审视这些框架。论文定义了环境/任务、常见的 LLM 角色(LMPRs，如 policy models, evaluators, and dynamic models)以及通用工作流程，并比较了不同范式中 LMPRs 的实现和设计差异。最终，论文识别了现有工作流程的三个限制，并讨论了未来研究方向，同时提供了 GitHub 资源以供参考。",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.SE"
      ],
      "primary_category": "cs.AI",
      "comment": "CoLing 2025 Camera Ready (extended to 9 pages)",
      "pdf_url": "http://arxiv.org/pdf/2406.05804v6",
      "published_date": "2024-06-09 14:42:55 UTC",
      "updated_date": "2024-11-30 22:38:57 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T17:59:58.714733"
    },
    {
      "arxiv_id": "2406.05802v1",
      "title": "SAM-PM: Enhancing Video Camouflaged Object Detection using Spatio-Temporal Attention",
      "title_zh": "SAM-PM：使用时空注意力增强视频伪装物体检测",
      "authors": [
        "Muhammad Nawfal Meeran",
        "Gokul Adethya T",
        "Bhanu Pratyush Mantha"
      ],
      "abstract": "In the domain of large foundation models, the Segment Anything Model (SAM)\nhas gained notable recognition for its exceptional performance in image\nsegmentation. However, tackling the video camouflage object detection (VCOD)\ntask presents a unique challenge. Camouflaged objects typically blend into the\nbackground, making them difficult to distinguish in still images. Additionally,\nensuring temporal consistency in this context is a challenging problem. As a\nresult, SAM encounters limitations and falls short when applied to the VCOD\ntask. To overcome these challenges, we propose a new method called the SAM\nPropagation Module (SAM-PM). Our propagation module enforces temporal\nconsistency within SAM by employing spatio-temporal cross-attention mechanisms.\nMoreover, we exclusively train the propagation module while keeping the SAM\nnetwork weights frozen, allowing us to integrate task-specific insights with\nthe vast knowledge accumulated by the large model. Our method effectively\nincorporates temporal consistency and domain-specific expertise into the\nsegmentation network with an addition of less than 1% of SAM's parameters.\nExtensive experimentation reveals a substantial performance improvement in the\nVCOD benchmark when compared to the most recent state-of-the-art techniques.\nCode and pre-trained weights are open-sourced at\nhttps://github.com/SpiderNitt/SAM-PM",
      "tldr_zh": "该研究针对Segment Anything Model (SAM)在视频伪装物体检测 (VCOD) 任务中的局限性（如伪装物体难以区分和时间一致性问题），提出了一种新方法SAM Propagation Module (SAM-PM)。SAM-PM通过时空交叉注意力机制强制执行视频帧间的 temporal consistency，同时仅训练传播模块而冻结SAM的网络权重，从而以不到SAM参数1%的增加，整合任务特定见解。实验结果显示，该方法在VCOD基准上显著优于现有最先进技术，提供开源代码和预训练权重以促进进一步应用。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2406.05802v1",
      "published_date": "2024-06-09 14:33:38 UTC",
      "updated_date": "2024-06-09 14:33:38 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T18:00:09.124804"
    },
    {
      "arxiv_id": "2406.05798v1",
      "title": "Hidden Holes: topological aspects of language models",
      "title_zh": "Hidden Holes: 语言模型的拓扑",
      "authors": [
        "Stephen Fitz",
        "Peter Romero",
        "Jiyan Jonas Schneider"
      ],
      "abstract": "We explore the topology of representation manifolds arising in autoregressive\nneural language models trained on raw text data. In order to study their\nproperties, we introduce tools from computational algebraic topology, which we\nuse as a basis for a measure of topological complexity, that we call\nperforation.\n  Using this measure, we study the evolution of topological structure in GPT\nbased large language models across depth and time during training. We then\ncompare these to gated recurrent models, and show that the latter exhibit more\ntopological complexity, with a distinct pattern of changes common to all\nnatural languages but absent from synthetically generated data. The paper\npresents a detailed analysis of the representation manifolds derived by these\nmodels based on studying the shapes of vector clouds induced by them as they\nare conditioned on sentences from corpora of natural language text.\n  The methods developed in this paper are novel in the field and based on\nmathematical apparatus that might be unfamiliar to the target audience. To help\nwith that we introduce the minimum necessary theory, and provide additional\nvisualizations in the appendices.\n  The main contribution of the paper is a striking observation about the\ntopological structure of the transformer as compared to LSTM based neural\narchitectures. It suggests that further research into mathematical properties\nof these neural networks is necessary to understand the operation of large\ntransformer language models. We hope this work inspires further explorations in\nthis direction within the NLP community.",
      "tldr_zh": "本研究探索了 autoregressive 神经语言模型中基于原始文本数据训练的表示流形（representation manifolds）的拓扑方面，引入了计算代数拓扑（computational algebraic topology）的工具，并定义了 perforation 作为拓扑复杂性的度量指标。研究者分析了 GPT 基于的大型语言模型在深度和训练过程中的拓扑结构演变，并与 gated recurrent models 进行比较，发现后者表现出更高的拓扑复杂性，且在自然语言数据中呈现独特模式，而合成数据则缺乏此特征。主要贡献在于揭示了 transformer 与 LSTM 基于的神经架构在拓扑结构上的显著差异，强调需要进一步研究这些模型的数学属性，以推动 NLP 社区的深入探索。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.NE"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2406.05798v1",
      "published_date": "2024-06-09 14:25:09 UTC",
      "updated_date": "2024-06-09 14:25:09 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T18:00:22.451904"
    },
    {
      "arxiv_id": "2406.05797v2",
      "title": "3D-MolT5: Leveraging Discrete Structural Information for Molecule-Text Modeling",
      "title_zh": "翻译失败",
      "authors": [
        "Qizhi Pei",
        "Rui Yan",
        "Kaiyuan Gao",
        "Jinhua Zhu",
        "Lijun Wu"
      ],
      "abstract": "The integration of molecular and natural language representations has emerged\nas a focal point in molecular science, with recent advancements in Language\nModels (LMs) demonstrating significant potential for comprehensive modeling of\nboth domains. However, existing approaches face notable limitations,\nparticularly in their neglect of three-dimensional (3D) information, which is\ncrucial for understanding molecular structures and functions. While some\nefforts have been made to incorporate 3D molecular information into LMs using\nexternal structure encoding modules, significant difficulties remain, such as\ninsufficient interaction across modalities in pre-training and challenges in\nmodality alignment. To address the limitations, we propose \\textbf{3D-MolT5}, a\nunified framework designed to model molecule in both sequence and 3D structure\nspaces. The key innovation of our approach lies in mapping fine-grained 3D\nsubstructure representations into a specialized 3D token vocabulary. This\nmethodology facilitates the seamless integration of sequence and structure\nrepresentations in a tokenized format, enabling 3D-MolT5 to encode molecular\nsequences, molecular structures, and text sequences within a unified\narchitecture. Leveraging this tokenized input strategy, we build a foundation\nmodel that unifies the sequence and structure data formats. We then conduct\njoint pre-training with multi-task objectives to enhance the model's\ncomprehension of these diverse modalities within a shared representation space.\nThus, our approach significantly improves cross-modal interaction and\nalignment, addressing key challenges in previous work. Further instruction\ntuning demonstrated that our 3D-MolT5 has strong generalization ability and\nsurpasses existing methods with superior performance in multiple downstream\ntasks. Our code is available at https://github.com/QizhiPei/3D-MolT5.",
      "tldr_zh": "本研究提出3D-MolT5框架，以解决现有Language Models (LMs)忽略分子结构的3D信息，导致模态交互和对齐不足的问题。框架的关键创新是将细粒度的3D子结构表示映射到一个专门的3D token vocabulary，从而在统一架构中无缝整合分子序列、分子结构和文本序列。利用该策略进行联合预训练和多任务目标训练，显著提升了跨模态交互和对齐能力。实验结果显示，3D-MolT5在多个下游任务中表现出色，泛化能力强，并优于现有方法。",
      "categories": [
        "q-bio.BM",
        "cs.AI",
        "cs.CE",
        "cs.CL",
        "cs.LG"
      ],
      "primary_category": "q-bio.BM",
      "comment": "Accepted by ICLR 2025",
      "pdf_url": "http://arxiv.org/pdf/2406.05797v2",
      "published_date": "2024-06-09 14:20:55 UTC",
      "updated_date": "2025-03-18 08:03:45 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T18:00:33.867579"
    },
    {
      "arxiv_id": "2406.05794v3",
      "title": "RE-RAG: Improving Open-Domain QA Performance and Interpretability with Relevance Estimator in Retrieval-Augmented Generation",
      "title_zh": "翻译失败",
      "authors": [
        "Kiseung Kim",
        "Jay-Yoon Lee"
      ],
      "abstract": "The Retrieval Augmented Generation (RAG) framework utilizes a combination of\nparametric knowledge and external knowledge to demonstrate state-of-the-art\nperformance on open-domain question answering tasks. However, the RAG framework\nsuffers from performance degradation when the query is accompanied by\nirrelevant contexts. In this work, we propose the RE-RAG framework, which\nintroduces a relevance estimator (RE) that not only provides relative relevance\nbetween contexts as previous rerankers did, but also provides confidence, which\ncan be used to classify whether given context is useful for answering the given\nquestion. We propose a weakly supervised method for training the RE simply\nutilizing question-answer data without any labels for correct contexts. We show\nthat RE trained with a small generator (sLM) can not only improve the sLM\nfine-tuned together with RE but also improve previously unreferenced large\nlanguage models (LLMs). Furthermore, we investigate new decoding strategies\nthat utilize the proposed confidence measured by RE such as choosing to let the\nuser know that it is \"unanswerable\" to answer the question given the retrieved\ncontexts or choosing to rely on LLM's parametric knowledge rather than\nunrelated contexts.",
      "tldr_zh": "本文提出 RE-RAG 框架，以提升 Retrieval-Augmented Generation (RAG) 在开放域问答任务中的性能和可解释性，通过引入相关性估计器 (Relevance Estimator, RE) 来评估上下文的相关性和置信度。RE 采用弱监督方法，仅利用问答数据进行训练，无需正确上下文标签，从而改善小语言模型 (sLM) 和大型语言模型 (LLMs) 的表现。实验结果显示，该框架能缓解无关上下文导致的性能下降，并探索新解码策略，如判断问题“不可回答”或优先依赖 LLMs 的参数知识，以增强问答的可信度和实用性。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2406.05794v3",
      "published_date": "2024-06-09 14:11:19 UTC",
      "updated_date": "2024-10-24 14:57:52 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T18:00:57.540712"
    },
    {
      "arxiv_id": "2406.05768v6",
      "title": "TLCM: Training-efficient Latent Consistency Model for Image Generation with 2-8 Steps",
      "title_zh": "TLCM：高效训练的潜在一致性模型，用于2-8步图像生成",
      "authors": [
        "Qingsong Xie",
        "Zhenyi Liao",
        "Zhijie Deng",
        "Chen chen",
        "Haonan Lu"
      ],
      "abstract": "Distilling latent diffusion models (LDMs) into ones that are fast to sample\nfrom is attracting growing research interest. However, the majority of existing\nmethods face two critical challenges: (1) They hinge on long training using a\nhuge volume of real data. (2) They routinely lead to quality degradation for\ngeneration, especially in text-image alignment. This paper proposes a novel\ntraining-efficient Latent Consistency Model (TLCM) to overcome these\nchallenges. Our method first accelerates LDMs via data-free multistep latent\nconsistency distillation (MLCD), and then data-free latent consistency\ndistillation is proposed to efficiently guarantee the inter-segment consistency\nin MLCD. Furthermore, we introduce bags of techniques, e.g., distribution\nmatching, adversarial learning, and preference learning, to enhance TLCM's\nperformance at few-step inference without any real data. TLCM demonstrates a\nhigh level of flexibility by enabling adjustment of sampling steps within the\nrange of 2 to 8 while still producing competitive outputs compared to full-step\napproaches. Notably, TLCM enjoys the data-free merit by employing synthetic\ndata from the teacher for distillation. With just 70 training hours on an A100\nGPU, a 3-step TLCM distilled from SDXL achieves an impressive CLIP Score of\n33.68 and an Aesthetic Score of 5.97 on the MSCOCO-2017 5K benchmark,\nsurpassing various accelerated models and even outperforming the teacher model\nin human preference metrics. We also demonstrate the versatility of TLCMs in\napplications including image style transfer, controllable generation, and\nChinese-to-image generation.",
      "tldr_zh": "本论文提出了一种训练高效的潜在一致性模型（TLCM），旨在解决潜在扩散模型（LDMs）的采样加速问题，通过数据无关的多步潜在一致性蒸馏（MLCD）来减少对真实数据的依赖，并引入分布匹配、对抗学习和偏好学习等技术以提升少步（2-8步）图像生成的质量和文本-图像对齐。TLCM利用教师模型的合成数据进行蒸馏，仅需70小时在A100 GPU上训练，即在MSCOCO-2017基准上实现CLIP Score 33.68和Aesthetic Score 5.97，超越了多种加速模型并在人类偏好指标上优于教师模型。该方法还展示了在图像风格转移、可控生成和中文到图像生成等应用中的灵活性和有效性。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2406.05768v6",
      "published_date": "2024-06-09 12:55:50 UTC",
      "updated_date": "2024-11-07 01:29:26 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T18:00:59.911706"
    },
    {
      "arxiv_id": "2406.05766v2",
      "title": "Set-CLIP: Exploring Aligned Semantic From Low-Alignment Multimodal Data Through A Distribution View",
      "title_zh": "翻译失败",
      "authors": [
        "Zijia Song",
        "Zelin Zang",
        "Yelin Wang",
        "Guozheng Yang",
        "Kaicheng yu",
        "Wanyu Chen",
        "Miaoyu Wang",
        "Stan Z. Li"
      ],
      "abstract": "Multimodal fusion breaks through the boundaries between diverse modalities\nand has already achieved notable performances. However, in many specialized\nfields, it is struggling to obtain sufficient alignment data for training,\nwhich seriously limits the use of previously effective models. Therefore,\nsemi-supervised learning approaches are attempted to facilitate multimodal\nalignment by learning from low-alignment data with fewer matched pairs, but\ntraditional techniques like pseudo-labeling may run into troubles in the\nlabel-deficient scenarios. To tackle these challenges, we reframe\nsemi-supervised multimodal alignment as a manifold matching issue and propose a\nnew methodology based on CLIP, termed Set-CLIP. Specifically, by designing a\nnovel semantic density distribution loss, we constrain the latent\nrepresentation distribution with fine granularity and extract implicit semantic\nalignment from unpaired multimodal data, thereby reducing the reliance on\nnumerous strictly matched pairs. Furthermore, we apply coarse-grained modality\nadaptation and unimodal self-supervised guidance to narrow the gaps between\nmodality spaces and improve the stability of representation distributions.\nExtensive experiments conducted on a range of tasks in various fields,\nincluding protein analysis, remote sensing, and the general vision-language\nfield, validate the efficacy of our proposed Set-CLIP method. Especially with\nno paired data for supervised training, Set-CLIP is still outstanding, which\nbrings an improvement of 144.83% over CLIP.",
      "tldr_zh": "该研究针对多模态融合中缺乏足够对齐数据的挑战，提出了一种基于 CLIP 的新方法 Set-CLIP，通过分布视角从低对齐数据中探索语义对齐。Set-CLIP 设计了语义密度分布损失（semantic density distribution loss），以细粒度约束潜在表示分布，并从未配对数据中提取隐式语义对齐，同时结合粗粒度模态适应（coarse-grained modality adaptation）和单模态自监督指导（unimodal self-supervised guidance）来缩小模态间差距。实验结果显示，在蛋白分析、遥感以及一般视觉语言任务上，Set-CLIP 在无配对数据监督训练的情况下，比 CLIP 提升了 144.83%，证明了其有效性。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL",
        "cs.CV"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2406.05766v2",
      "published_date": "2024-06-09 12:41:14 UTC",
      "updated_date": "2024-09-21 09:50:33 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T18:01:09.741048"
    },
    {
      "arxiv_id": "2406.05764v1",
      "title": "Global Sensitivity Analysis of Uncertain Parameters in Bayesian Networks",
      "title_zh": "贝叶斯网络中不确定参数的全球敏感性分析",
      "authors": [
        "Rafael Ballester-Ripoll",
        "Manuele Leonelli"
      ],
      "abstract": "Traditionally, the sensitivity analysis of a Bayesian network studies the\nimpact of individually modifying the entries of its conditional probability\ntables in a one-at-a-time (OAT) fashion. However, this approach fails to give a\ncomprehensive account of each inputs' relevance, since simultaneous\nperturbations in two or more parameters often entail higher-order effects that\ncannot be captured by an OAT analysis. We propose to conduct global\nvariance-based sensitivity analysis instead, whereby $n$ parameters are viewed\nas uncertain at once and their importance is assessed jointly. Our method works\nby encoding the uncertainties as $n$ additional variables of the network. To\nprevent the curse of dimensionality while adding these dimensions, we use\nlow-rank tensor decomposition to break down the new potentials into smaller\nfactors. Last, we apply the method of Sobol to the resulting network to obtain\n$n$ global sensitivity indices. Using a benchmark array of both expert-elicited\nand learned Bayesian networks, we demonstrate that the Sobol indices can\nsignificantly differ from the OAT indices, thus revealing the true influence of\nuncertain parameters and their interactions.",
      "tldr_zh": "本研究针对传统Bayesian Networks敏感性分析的局限性，提出了一种全局方差-based方法，以同时评估n个不确定参数的影响，而不是采用one-at-a-time (OAT)逐一修改的方式，从而捕捉参数间的交互效应。该方法通过将不确定性编码为网络中的n个额外变量，并使用low-rank tensor decomposition分解新势函数，以避免维数灾难；随后应用Sobol方法计算全局敏感性指数。实验结果显示，在一系列专家推断和学习得到的基准Bayesian Networks上，Sobol指数与OAT指数有显著差异，更准确地揭示了不确定参数的真正影响和交互作用。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2406.05764v1",
      "published_date": "2024-06-09 12:36:38 UTC",
      "updated_date": "2024-06-09 12:36:38 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T18:01:21.937831"
    },
    {
      "arxiv_id": "2406.05756v1",
      "title": "EmbSpatial-Bench: Benchmarking Spatial Understanding for Embodied Tasks with Large Vision-Language Models",
      "title_zh": "翻译失败",
      "authors": [
        "Mengfei Du",
        "Binhao Wu",
        "Zejun Li",
        "Xuanjing Huang",
        "Zhongyu Wei"
      ],
      "abstract": "The recent rapid development of Large Vision-Language Models (LVLMs) has\nindicated their potential for embodied tasks.However, the critical skill of\nspatial understanding in embodied environments has not been thoroughly\nevaluated, leaving the gap between current LVLMs and qualified embodied\nintelligence unknown. Therefore, we construct EmbSpatial-Bench, a benchmark for\nevaluating embodied spatial understanding of LVLMs.The benchmark is\nautomatically derived from embodied scenes and covers 6 spatial relationships\nfrom an egocentric perspective.Experiments expose the insufficient capacity of\ncurrent LVLMs (even GPT-4V). We further present EmbSpatial-SFT, an\ninstruction-tuning dataset designed to improve LVLMs' embodied spatial\nunderstanding.",
      "tldr_zh": "这篇论文引入了 EmbSpatial-Bench，这是一个基准，用于评估大型视觉语言模型 (LVLMs) 在具身任务中的空间理解能力，旨在填补当前模型与合格具身智能之间的差距。该基准从具身场景自动生成，涵盖从第一人称视角的 6 种空间关系，实验结果显示现有 LVLMs（如 GPT-4V）在空间理解方面能力不足。为了提升性能，作者提出了 EmbSpatial-SFT，这是一个专门设计的指令微调数据集。",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.CV",
        "cs.MM"
      ],
      "primary_category": "cs.AI",
      "comment": "Accepted by ACL 2024 Main",
      "pdf_url": "http://arxiv.org/pdf/2406.05756v1",
      "published_date": "2024-06-09 12:23:14 UTC",
      "updated_date": "2024-06-09 12:23:14 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T18:01:36.751163"
    },
    {
      "arxiv_id": "2406.05754v2",
      "title": "Numerical solution of a PDE arising from prediction with expert advice",
      "title_zh": "翻译失败",
      "authors": [
        "Jeff Calder",
        "Nadejda Drenska",
        "Drisana Mosaphir"
      ],
      "abstract": "This work investigates the online machine learning problem of prediction with\nexpert advice in an adversarial setting through numerical analysis of, and\nexperiments with, a related partial differential equation. The problem is a\nrepeated two-person game involving decision-making at each step informed by $n$\nexperts in an adversarial environment. The continuum limit of this game over a\nlarge number of steps is a degenerate elliptic equation whose solution encodes\nthe optimal strategies for both players. We develop numerical methods for\napproximating the solution of this equation in relatively high dimensions\n($n\\leq 10$) by exploiting symmetries in the equation and the solution to\ndrastically reduce the size of the computational domain. Based on our numerical\nresults we make a number of conjectures about the optimality of various\nadversarial strategies, in particular about the non-optimality of the COMB\nstrategy.",
      "tldr_zh": "本论文研究了在线机器学习中的预测与专家建议（prediction with expert advice）问题，通过数值分析和实验探索一个相关的偏微分方程（PDE）。作者将这个重复的两玩家对抗游戏的连续极限建模为一个退化椭圆方程（degenerate elliptic equation），其解编码了玩家的最优策略，并开发了数值方法，利用方程和解的对称性来减少计算域大小，从而在较高维度（n ≤ 10）下实现高效近似。基于数值结果，论文提出几点猜想，包括各种对抗策略的最优性问题，特别是COMB strategy的非最优性。",
      "categories": [
        "math.NA",
        "cs.AI",
        "cs.LG",
        "cs.NA",
        "math.AP",
        "35D40, 65N12, 65N06, 35Q68, 35J60"
      ],
      "primary_category": "math.NA",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2406.05754v2",
      "published_date": "2024-06-09 12:17:05 UTC",
      "updated_date": "2025-02-03 01:20:33 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T18:01:48.610995"
    },
    {
      "arxiv_id": "2406.05753v5",
      "title": "Grounding Continuous Representations in Geometry: Equivariant Neural Fields",
      "title_zh": "在几何中根植连续表示：等变神经场",
      "authors": [
        "David R Wessels",
        "David M Knigge",
        "Samuele Papa",
        "Riccardo Valperga",
        "Sharvaree Vadgama",
        "Efstratios Gavves",
        "Erik J Bekkers"
      ],
      "abstract": "Conditional Neural Fields (CNFs) are increasingly being leveraged as\ncontinuous signal representations, by associating each data-sample with a\nlatent variable that conditions a shared backbone Neural Field (NeF) to\nreconstruct the sample. However, existing CNF architectures face limitations\nwhen using this latent downstream in tasks requiring fine-grained geometric\nreasoning, such as classification and segmentation. We posit that this results\nfrom lack of explicit modelling of geometric information (e.g., locality in the\nsignal or the orientation of a feature) in the latent space of CNFs. As such,\nwe propose Equivariant Neural Fields (ENFs), a novel CNF architecture which\nuses a geometry-informed cross-attention to condition the NeF on a geometric\nvariable--a latent point cloud of features--that enables an equivariant\ndecoding from latent to field. We show that this approach induces a\nsteerability property by which both field and latent are grounded in geometry\nand amenable to transformation laws: if the field transforms, the latent\nrepresentation transforms accordingly--and vice versa. Crucially, this\nequivariance relation ensures that the latent is capable of (1) representing\ngeometric patterns faithfully, allowing for geometric reasoning in latent\nspace, and (2) weight-sharing over similar local patterns, allowing for\nefficient learning of datasets of fields. We validate these main properties in\na range of tasks including classification, segmentation, forecasting,\nreconstruction and generative modelling, showing clear improvement over\nbaselines with a geometry-free latent space. Code attached to submission\nhttps://github.com/Dafidofff/enf-jax. Code for a clean and minimal repo\nhttps://github.com/david-knigge/enf-min-jax.",
      "tldr_zh": "这篇论文针对 Conditional Neural Fields (CNFs) 在精细几何推理任务（如分类和分割）中的局限性，提出了一种新型架构 Equivariant Neural Fields (ENFs)。ENFs 通过几何信息化的交叉注意力机制，使用潜在点云作为几何变量，实现等变解码，从而使字段和潜在表示都具备 steerability 属性，能够适应变换法则并支持几何推理。论文的关键贡献包括确保潜在空间忠实表示几何模式，促进高效学习和权重共享。在分类、分割、预测、重构及生成建模等任务中，ENFs 比无几何潜在空间的基线模型表现出显著改进。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2406.05753v5",
      "published_date": "2024-06-09 12:16:30 UTC",
      "updated_date": "2025-02-07 17:31:20 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T18:02:00.672908"
    },
    {
      "arxiv_id": "2406.12900v2",
      "title": "Factor Graph Optimization of Error-Correcting Codes for Belief Propagation Decoding",
      "title_zh": "翻译失败",
      "authors": [
        "Yoni Choukroun",
        "Lior Wolf"
      ],
      "abstract": "The design of optimal linear block codes capable of being efficiently decoded\nis of major concern, especially for short block lengths. As near\ncapacity-approaching codes, Low-Density Parity-Check (LDPC) codes possess\nseveral advantages over other families of codes, the most notable being its\nefficient decoding via Belief Propagation. While many LDPC code design methods\nexist, the development of efficient sparse codes that meet the constraints of\nmodern short code lengths and accommodate new channel models remains a\nchallenge. In this work, we propose for the first time a gradient-based\ndata-driven approach for the design of sparse codes. We develop locally optimal\ncodes with respect to Belief Propagation decoding via the learning of the\nFactor graph under channel noise simulations. This is performed via a novel\ncomplete graph tensor representation of the Belief Propagation algorithm,\noptimized over finite fields via backpropagation and coupled with an efficient\nline-search method. The proposed approach is shown to outperform the decoding\nperformance of existing popular codes by orders of magnitude and demonstrates\nthe power of data-driven approaches for code design.",
      "tldr_zh": "本研究针对短块长线性块码的设计问题，提出了一种基于梯度的数据驱动方法，以优化Low-Density Parity-Check (LDPC) codes在Belief Propagation解码下的性能。该方法通过学习Factor graph在通道噪声模拟中的表示，利用一个新颖的完整图张量表示、在有限域上的backpropagation优化以及高效的line-search技术，来开发局部最优的稀疏码。实验结果显示，该方法在解码性能上比现有流行码高出几个数量级，证明了数据驱动方法在错误纠正码设计中的强大潜力。",
      "categories": [
        "cs.IT",
        "cs.AI",
        "cs.LG",
        "math.IT"
      ],
      "primary_category": "cs.IT",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2406.12900v2",
      "published_date": "2024-06-09 12:08:56 UTC",
      "updated_date": "2024-10-10 12:36:25 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T18:02:13.844976"
    },
    {
      "arxiv_id": "2406.05746v1",
      "title": "Methodology and Real-World Applications of Dynamic Uncertain Causality Graph for Clinical Diagnosis with Explainability and Invariance",
      "title_zh": "翻译失败",
      "authors": [
        "Zhan Zhang",
        "Qin Zhang",
        "Yang Jiao",
        "Lin Lu",
        "Lin Ma",
        "Aihua Liu",
        "Xiao Liu",
        "Juan Zhao",
        "Yajun Xue",
        "Bing Wei",
        "Mingxia Zhang",
        "Ru Gao",
        "Hong Zhao",
        "Jie Lu",
        "Fan Li",
        "Yang Zhang",
        "Yiming Wang",
        "Lei Zhang",
        "Fengwei Tian",
        "Jie Hu",
        "Xin Gou"
      ],
      "abstract": "AI-aided clinical diagnosis is desired in medical care. Existing deep\nlearning models lack explainability and mainly focus on image analysis. The\nrecently developed Dynamic Uncertain Causality Graph (DUCG) approach is\ncausality-driven, explainable, and invariant across different application\nscenarios, without problems of data collection, labeling, fitting, privacy,\nbias, generalization, high cost and high energy consumption. Through close\ncollaboration between clinical experts and DUCG technicians, 46 DUCG models\ncovering 54 chief complaints were constructed. Over 1,000 diseases can be\ndiagnosed without triage. Before being applied in real-world, the 46 DUCG\nmodels were retrospectively verified by third-party hospitals. The verified\ndiagnostic precisions were no less than 95%, in which the diagnostic precision\nfor every disease including uncommon ones was no less than 80%. After\nverifications, the 46 DUCG models were applied in the real-world in China. Over\none million real diagnosis cases have been performed, with only 17 incorrect\ndiagnoses identified. Due to DUCG's transparency, the mistakes causing the\nincorrect diagnoses were found and corrected. The diagnostic abilities of the\nclinicians who applied DUCG frequently were improved significantly. Following\nthe introduction to the earlier presented DUCG methodology, the recommendation\nalgorithm for potential medical checks is presented and the key idea of DUCG is\nextracted.",
      "tldr_zh": "该研究介绍了 Dynamic Uncertain Causality Graph (DUCG) 的方法及其在临床诊断中的实际应用，DUCG 是一种因果驱动、可解释且不变性的模型，能避免传统深度学习模型的诸多问题，如数据隐私和泛化偏差。研究人员通过临床专家和技师合作，构建了46个DUCG模型，覆盖54种主要症状，可诊断超过1,000种疾病，并在第三方医院验证中实现整体诊断精度不低于95%，每个疾病（包括罕见病）不低于80%。在实际应用中，处理超过一百万真实病例，仅有17例错误诊断，并通过DUCG的透明性及时纠正错误，同时显著提升了使用医生的诊断能力。该方法还包括推荐算法，提取DUCG的关键思想，为AI辅助医疗提供可靠框架。",
      "categories": [
        "cs.AI",
        "cs.HC",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2406.05746v1",
      "published_date": "2024-06-09 11:37:45 UTC",
      "updated_date": "2024-06-09 11:37:45 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T18:02:24.812277"
    },
    {
      "arxiv_id": "2406.05745v2",
      "title": "Structured Learning of Compositional Sequential Interventions",
      "title_zh": "翻译失败",
      "authors": [
        "Jialin Yu",
        "Andreas Koukorinis",
        "Nicolò Colombo",
        "Yuchen Zhu",
        "Ricardo Silva"
      ],
      "abstract": "We consider sequential treatment regimes where each unit is exposed to\ncombinations of interventions over time. When interventions are described by\nqualitative labels, such as \"close schools for a month due to a pandemic\" or\n\"promote this podcast to this user during this week\", it is unclear which\nappropriate structural assumptions allow us to generalize behavioral\npredictions to previously unseen combinations of interventions. Standard\nblack-box approaches mapping sequences of categorical variables to outputs are\napplicable, but they rely on poorly understood assumptions on how reliable\ngeneralization can be obtained, and may underperform under sparse sequences,\ntemporal variability, and large action spaces. To approach that, we pose an\nexplicit model for composition, that is, how the effect of sequential\ninterventions can be isolated into modules, clarifying which data conditions\nallow for the identification of their combined effect at different units and\ntime steps. We show the identification properties of our compositional model,\ninspired by advances in causal matrix factorization methods. Our focus is on\npredictive models for novel compositions of interventions instead of matrix\ncompletion tasks and causal effect estimation. We compare our approach to\nflexible but generic black-box models to illustrate how structure aids\nprediction in sparse data conditions.",
      "tldr_zh": "本研究探讨了顺序干预（sequential interventions）的结构化学习问题，针对单位在不同时间点暴露于干预组合（如“关闭学校”或“推广播客”）的情景，分析了如何推广行为预测至未见组合。作者提出一个显式组合模型（compositional model），将干预效果隔离为模块，受因果矩阵分解（causal matrix factorization）方法启发，从而在稀疏序列和时间变异性下明确识别条件。实验结果显示，该结构化方法在预测新型干预组合时优于灵活的黑箱模型，尤其在稀疏数据条件下显著提升预测性能。",
      "categories": [
        "stat.ML",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "stat.ML",
      "comment": "Camera ready version (NeurIPS 2024)",
      "pdf_url": "http://arxiv.org/pdf/2406.05745v2",
      "published_date": "2024-06-09 11:36:36 UTC",
      "updated_date": "2024-10-29 19:18:11 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T18:02:35.845023"
    },
    {
      "arxiv_id": "2406.05724v2",
      "title": "Deception Analysis with Artificial Intelligence: An Interdisciplinary Perspective",
      "title_zh": "翻译失败",
      "authors": [
        "Stefan Sarkadi"
      ],
      "abstract": "Humans and machines interact more frequently than ever and our societies are\nbecoming increasingly hybrid. A consequence of this hybridisation is the\ndegradation of societal trust due to the prevalence of AI-enabled deception.\nYet, despite our understanding of the role of trust in AI in the recent years,\nwe still do not have a computational theory to be able to fully understand and\nexplain the role deception plays in this context. This is a problem because\nwhile our ability to explain deception in hybrid societies is delayed, the\ndesign of AI agents may keep advancing towards fully autonomous deceptive\nmachines, which would pose new challenges to dealing with deception. In this\npaper we build a timely and meaningful interdisciplinary perspective on\ndeceptive AI and reinforce a 20 year old socio-cognitive perspective on trust\nand deception, by proposing the development of DAMAS -- a holistic Multi-Agent\nSystems (MAS) framework for the socio-cognitive modelling and analysis of\ndeception. In a nutshell this paper covers the topic of modelling and\nexplaining deception using AI approaches from the perspectives of Computer\nScience, Philosophy, Psychology, Ethics, and Intelligence Analysis.",
      "tldr_zh": "本论文从跨学科视角分析了人工智能（AI）在人类-机器混合社会中导致的欺骗问题，强调了现有计算理论不足以全面理解欺骗对信任的影响。作者提出DAMAS框架，这是一个整体Multi-Agent Systems (MAS)框架，用于社会认知建模和分析欺骗，旨在强化20年前的社会认知视角。框架整合了计算机科学、哲学、心理学、伦理学和情报分析等领域的见解，以解释AI欺骗的机制和潜在风险。通过这一方法，论文旨在防止AI发展成完全自治的欺骗机器，并为处理混合社会中的欺骗挑战提供新路径。",
      "categories": [
        "cs.MA",
        "cs.AI",
        "cs.CY"
      ],
      "primary_category": "cs.MA",
      "comment": "Work in progress",
      "pdf_url": "http://arxiv.org/pdf/2406.05724v2",
      "published_date": "2024-06-09 10:31:26 UTC",
      "updated_date": "2024-06-11 09:06:53 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T18:02:48.347498"
    },
    {
      "arxiv_id": "2406.05720v1",
      "title": "VillagerAgent: A Graph-Based Multi-Agent Framework for Coordinating Complex Task Dependencies in Minecraft",
      "title_zh": "VillagerAgent: 一种基于图的多智能体框架，用于在 Minecraft 中协调复杂任务依赖",
      "authors": [
        "Yubo Dong",
        "Xukun Zhu",
        "Zhengzhe Pan",
        "Linchao Zhu",
        "Yi Yang"
      ],
      "abstract": "In this paper, we aim to evaluate multi-agent systems against complex\ndependencies, including spatial, causal, and temporal constraints. First, we\nconstruct a new benchmark, named VillagerBench, within the Minecraft\nenvironment.VillagerBench comprises diverse tasks crafted to test various\naspects of multi-agent collaboration, from workload distribution to dynamic\nadaptation and synchronized task execution. Second, we introduce a Directed\nAcyclic Graph Multi-Agent Framework VillagerAgent to resolve complex\ninter-agent dependencies and enhance collaborative efficiency. This solution\nincorporates a task decomposer that creates a directed acyclic graph (DAG) for\nstructured task management, an agent controller for task distribution, and a\nstate manager for tracking environmental and agent data. Our empirical\nevaluation on VillagerBench demonstrates that VillagerAgent outperforms the\nexisting AgentVerse model, reducing hallucinations and improving task\ndecomposition efficacy. The results underscore VillagerAgent's potential in\nadvancing multi-agent collaboration, offering a scalable and generalizable\nsolution in dynamic environments. The source code is open-source on GitHub\n(https://github.com/cnsdqd-dyb/VillagerAgent).",
      "tldr_zh": "本研究旨在评估多智能体系统处理复杂依赖关系（如空间、因果和时间约束）的能力，构建了一个新基准VillagerBench，在Minecraft环境中测试多智能体协作的各方面，包括工作负载分配、动态适应和同步任务执行。研究引入了基于有向无环图(DAG)的多智能体框架VillagerAgent，该框架包括任务分解器、代理控制器和状态管理器，用于解决代理间依赖问题并提升协作效率。在VillagerBench上的实验表明，VillagerAgent优于现有模型AgentVerse，减少了幻觉并提高了任务分解效果，展示了其在动态环境中的可扩展性和通用性。",
      "categories": [
        "cs.AI",
        "cs.MA"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2406.05720v1",
      "published_date": "2024-06-09 10:21:47 UTC",
      "updated_date": "2024-06-09 10:21:47 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T18:03:00.515199"
    },
    {
      "arxiv_id": "2406.05707v2",
      "title": "QGEval: Benchmarking Multi-dimensional Evaluation for Question Generation",
      "title_zh": "翻译失败",
      "authors": [
        "Weiping Fu",
        "Bifan Wei",
        "Jianxiang Hu",
        "Zhongmin Cai",
        "Jun Liu"
      ],
      "abstract": "Automatically generated questions often suffer from problems such as unclear\nexpression or factual inaccuracies, requiring a reliable and comprehensive\nevaluation of their quality. Human evaluation is widely used in the field of\nquestion generation (QG) and serves as the gold standard for automatic metrics.\nHowever, there is a lack of unified human evaluation criteria, which hampers\nconsistent and reliable evaluations of both QG models and automatic metrics. To\naddress this, we propose QGEval, a multi-dimensional Evaluation benchmark for\nQuestion Generation, which evaluates both generated questions and existing\nautomatic metrics across 7 dimensions: fluency, clarity, conciseness,\nrelevance, consistency, answerability, and answer consistency. We demonstrate\nthe appropriateness of these dimensions by examining their correlations and\ndistinctions. Through consistent evaluations of QG models and automatic metrics\nwith QGEval, we find that 1) most QG models perform unsatisfactorily in terms\nof answerability and answer consistency, and 2) existing metrics fail to align\nwell with human judgments when evaluating generated questions across the 7\ndimensions. We expect this work to foster the development of both QG\ntechnologies and their evaluation.",
      "tldr_zh": "该论文提出 QGEval，一个多维度基准，用于评估 Question Generation (QG) 系统的生成问题质量，以解决现有评估标准不统一的问题。QGEval 涵盖 7 个维度，包括 fluency（流畅性）、clarity（清晰度）、conciseness（简洁性）、relevance（相关性）、consistency（一致性）、answerability（可回答性）和 answer consistency（答案一致性），并通过分析这些维度的相关性和区别来验证其有效性。实验结果显示，大多数 QG 模型在 answerability 和 answer consistency 方面表现不佳，而现有自动指标与人类判断在这些维度上存在显著不一致。该工作旨在推动 QG 技术和评估方法的发展。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "Accepted by EMNLP 2024",
      "pdf_url": "http://arxiv.org/pdf/2406.05707v2",
      "published_date": "2024-06-09 09:51:55 UTC",
      "updated_date": "2024-10-10 15:12:23 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T18:03:13.078934"
    },
    {
      "arxiv_id": "2406.06637v1",
      "title": "Exploring the Efficacy of Large Language Models (GPT-4) in Binary Reverse Engineering",
      "title_zh": "探索大型语言模型 (GPT-4) 在二进制逆向工程中的效能",
      "authors": [
        "Saman Pordanesh",
        "Benjamin Tan"
      ],
      "abstract": "This study investigates the capabilities of Large Language Models (LLMs),\nspecifically GPT-4, in the context of Binary Reverse Engineering (RE).\nEmploying a structured experimental approach, we analyzed the LLM's performance\nin interpreting and explaining human-written and decompiled codes. The research\nencompassed two phases: the first on basic code interpretation and the second\non more complex malware analysis. Key findings indicate LLMs' proficiency in\ngeneral code understanding, with varying effectiveness in detailed technical\nand security analyses. The study underscores the potential and current\nlimitations of LLMs in reverse engineering, revealing crucial insights for\nfuture applications and improvements. Also, we examined our experimental\nmethodologies, such as methods of evaluation and data constraints, which\nprovided us with a technical vision for any future research activity in this\nfield.",
      "tldr_zh": "本研究探讨了Large Language Models (LLMs)，特别是GPT-4，在Binary Reverse Engineering中的效能，通过结构化的实验方法评估其在解释人类编写代码和反编译代码方面的表现。实验分为两个阶段：第一阶段聚焦基本代码理解，第二阶段涉及更复杂的恶意软件分析。结果显示，GPT-4在一般代码理解上表现出色，但详细的技术和安全分析效果有限。该研究揭示了LLMs在逆向工程中的潜力与挑战，并为未来应用改进和实验方法优化提供了关键见解。",
      "categories": [
        "cs.SE",
        "cs.AI"
      ],
      "primary_category": "cs.SE",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2406.06637v1",
      "published_date": "2024-06-09 09:23:58 UTC",
      "updated_date": "2024-06-09 09:23:58 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T18:03:25.407808"
    },
    {
      "arxiv_id": "2406.05699v1",
      "title": "An Investigation of Noise Robustness for Flow-Matching-Based Zero-Shot TTS",
      "title_zh": "翻译失败",
      "authors": [
        "Xiaofei Wang",
        "Sefik Emre Eskimez",
        "Manthan Thakker",
        "Hemin Yang",
        "Zirun Zhu",
        "Min Tang",
        "Yufei Xia",
        "Jinzhu Li",
        "Sheng Zhao",
        "Jinyu Li",
        "Naoyuki Kanda"
      ],
      "abstract": "Recently, zero-shot text-to-speech (TTS) systems, capable of synthesizing any\nspeaker's voice from a short audio prompt, have made rapid advancements.\nHowever, the quality of the generated speech significantly deteriorates when\nthe audio prompt contains noise, and limited research has been conducted to\naddress this issue. In this paper, we explored various strategies to enhance\nthe quality of audio generated from noisy audio prompts within the context of\nflow-matching-based zero-shot TTS. Our investigation includes comprehensive\ntraining strategies: unsupervised pre-training with masked speech denoising,\nmulti-speaker detection and DNSMOS-based data filtering on the pre-training\ndata, and fine-tuning with random noise mixing. The results of our experiments\ndemonstrate significant improvements in intelligibility, speaker similarity,\nand overall audio quality compared to the approach of applying speech\nenhancement to the audio prompt.",
      "tldr_zh": "本文研究了基于 flow-matching 的 zero-shot TTS 系统在噪声音频提示下的鲁棒性问题，旨在提升生成的语音质量。研究团队探索了多种策略，包括无监督预训练（采用 masked speech denoising）、多说话者检测和 DNSMOS-based 数据过滤，以及使用随机噪声混合的微调方法。这些方法在实验中显著提高了语音的可懂度、说话者相似性和整体音频质量，优于直接对音频提示应用语音增强的传统方法。",
      "categories": [
        "eess.AS",
        "cs.AI",
        "eess.SP"
      ],
      "primary_category": "eess.AS",
      "comment": "Accepted to INTERSPEECH2024",
      "pdf_url": "http://arxiv.org/pdf/2406.05699v1",
      "published_date": "2024-06-09 08:51:50 UTC",
      "updated_date": "2024-06-09 08:51:50 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T18:03:40.695236"
    },
    {
      "arxiv_id": "2406.05692v1",
      "title": "SPA-SVC: Self-supervised Pitch Augmentation for Singing Voice Conversion",
      "title_zh": "翻译失败",
      "authors": [
        "Bingsong Bai",
        "Fengping Wang",
        "Yingming Gao",
        "Ya Li"
      ],
      "abstract": "Diffusion-based singing voice conversion (SVC) models have shown better\nsynthesis quality compared to traditional methods. However, in cross-domain SVC\nscenarios, where there is a significant disparity in pitch between the source\nand target voice domains, the models tend to generate audios with hoarseness,\nposing challenges in achieving high-quality vocal outputs. Therefore, in this\npaper, we propose a Self-supervised Pitch Augmentation method for Singing Voice\nConversion (SPA-SVC), which can enhance the voice quality in SVC tasks without\nrequiring additional data or increasing model parameters. We innovatively\nintroduce a cycle pitch shifting training strategy and Structural Similarity\nIndex (SSIM) loss into our SVC model, effectively enhancing its performance.\nExperimental results on the public singing datasets M4Singer indicate that our\nproposed method significantly improves model performance in both general SVC\nscenarios and particularly in cross-domain SVC scenarios.",
      "tldr_zh": "这篇论文针对扩散-based唱歌声音转换(SVC)模型在跨域场景中音高差异导致音频沙哑问题的挑战，提出了SPA-SVC，一种自监督音高增强方法。该方法通过引入循环音高移调训练策略和Structural Similarity Index (SSIM)损失，实现了声音质量提升，而无需额外数据或增加模型参数。实验结果在公开数据集M4Singer上表明，SPA-SVC显著改善了SVC性能，尤其在跨域SVC场景中。",
      "categories": [
        "cs.SD",
        "cs.AI",
        "eess.AS"
      ],
      "primary_category": "cs.SD",
      "comment": "Accepted by Interspeech 2024",
      "pdf_url": "http://arxiv.org/pdf/2406.05692v1",
      "published_date": "2024-06-09 08:34:01 UTC",
      "updated_date": "2024-06-09 08:34:01 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T18:03:50.694920"
    },
    {
      "arxiv_id": "2406.05688v1",
      "title": "Peer Review as A Multi-Turn and Long-Context Dialogue with Role-Based Interactions",
      "title_zh": "同行评审作为多轮长上下文对话，带有基于角色的互动",
      "authors": [
        "Cheng Tan",
        "Dongxin Lyu",
        "Siyuan Li",
        "Zhangyang Gao",
        "Jingxuan Wei",
        "Siqi Ma",
        "Zicheng Liu",
        "Stan Z. Li"
      ],
      "abstract": "Large Language Models (LLMs) have demonstrated wide-ranging applications\nacross various fields and have shown significant potential in the academic\npeer-review process. However, existing applications are primarily limited to\nstatic review generation based on submitted papers, which fail to capture the\ndynamic and iterative nature of real-world peer reviews. In this paper, we\nreformulate the peer-review process as a multi-turn, long-context dialogue,\nincorporating distinct roles for authors, reviewers, and decision makers. We\nconstruct a comprehensive dataset containing over 26,841 papers with 92,017\nreviews collected from multiple sources, including the top-tier conference and\nprestigious journal. This dataset is meticulously designed to facilitate the\napplications of LLMs for multi-turn dialogues, effectively simulating the\ncomplete peer-review process. Furthermore, we propose a series of metrics to\nevaluate the performance of LLMs for each role under this reformulated\npeer-review setting, ensuring fair and comprehensive evaluations. We believe\nthis work provides a promising perspective on enhancing the LLM-driven\npeer-review process by incorporating dynamic, role-based interactions. It\naligns closely with the iterative and interactive nature of real-world academic\npeer review, offering a robust foundation for future research and development\nin this area. We open-source the dataset at\nhttps://github.com/chengtan9907/ReviewMT.",
      "tldr_zh": "这篇论文将同行评审过程重新定义为多轮、长上下文对话，引入作者、审稿人和决策者的角色-based interactions，以克服现有LLMs在静态审查上的局限性。研究者构建了一个包含26,841篇论文和92,017条评论的综合数据集，从顶级会议和期刊收集，用于模拟完整的同行评审过程，并支持LLMs的多轮对话应用。同时，他们提出了系列评估指标来公平评估LLMs在各角色下的性能，为动态、交互式同行评审提供了一个有前景的框架，并开源了数据集（https://github.com/chengtan9907/ReviewMT）。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "Under review",
      "pdf_url": "http://arxiv.org/pdf/2406.05688v1",
      "published_date": "2024-06-09 08:24:17 UTC",
      "updated_date": "2024-06-09 08:24:17 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T18:04:04.764902"
    },
    {
      "arxiv_id": "2406.05682v1",
      "title": "From Basic to Extra Features: Hypergraph Transformer Pretrain-then-Finetuning for Balanced Clinical Predictions on EHR",
      "title_zh": "翻译失败",
      "authors": [
        "Ran Xu",
        "Yiwen Lu",
        "Chang Liu",
        "Yong Chen",
        "Yan Sun",
        "Xiao Hu",
        "Joyce C Ho",
        "Carl Yang"
      ],
      "abstract": "Electronic Health Records (EHRs) contain rich patient information and are\ncrucial for clinical research and practice. In recent years, deep learning\nmodels have been applied to EHRs, but they often rely on massive features,\nwhich may not be readily available for all patients. We propose HTP-Star, which\nleverages hypergraph structures with a pretrain-then-finetune framework for\nmodeling EHR data, enabling seamless integration of additional features.\nAdditionally, we design two techniques, namely (1) Smoothness-inducing\nRegularization and (2) Group-balanced Reweighting, to enhance the model's\nrobustness during fine-tuning. Through experiments conducted on two real EHR\ndatasets, we demonstrate that HTP-Star consistently outperforms various\nbaselines while striking a balance between patients with basic and extra\nfeatures.",
      "tldr_zh": "该论文提出 HTP-Star 模型，利用 Hypergraph Transformer 和 pretrain-then-finetune 框架来处理 Electronic Health Records (EHR) 数据，旨在解决患者特征不完整的问题，实现基本和额外特征的平衡预测。模型引入了 Smoothness-inducing Regularization 和 Group-balanced Reweighting 两种技术，以增强细调过程中的鲁棒性。通过在两个真实 EHR 数据集上的实验，HTP-Star 表现优于各种基线模型，并在不同特征组患者之间实现了预测平衡。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "CHIL 2024",
      "pdf_url": "http://arxiv.org/pdf/2406.05682v1",
      "published_date": "2024-06-09 07:41:03 UTC",
      "updated_date": "2024-06-09 07:41:03 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T18:04:15.995726"
    },
    {
      "arxiv_id": "2406.05673v5",
      "title": "Flow of Reasoning:Training LLMs for Divergent Problem Solving with Minimal Examples",
      "title_zh": "翻译失败",
      "authors": [
        "Fangxu Yu",
        "Lai Jiang",
        "Haoqiang Kang",
        "Shibo Hao",
        "Lianhui Qin"
      ],
      "abstract": "The ability to generate diverse solutions to a given problem is a hallmark of\nhuman creativity. This divergent reasoning is also crucial for machines,\nenhancing their robustness and enabling them to assist humans in many\napplications such as scientific discovery. However, existing approaches to\nmulti-step reasoning with large language models (LLMs) have mostly focused only\non reasoning accuracy, without further discovering more diverse valid\nsolutions. For example, supervised fine-tuning can improve LLM reasoning\nquality, but requires extensive supervised data to capture the full range of\npossible solutions. Reward-maximization reinforcement learning aims to find\nlimited highest-reward solutions while neglecting the solution diversity. To\nfill this gap, we propose Flow of Reasoning (FoR), an efficient\ndiversity-seeking LLM finetuning method aimed at improving reasoning quality\nand diversity with minimal data. FoR formulates multi-step LLM reasoning as a\nMarkovian flow on a DAG-structured reasoning graph. This formulation allows us\nto incorporate and adapt principled GFlowNet approaches, for finetuning LLMs to\nsample divergent paths with probabilities proportional to the (unnormalized)\nreward of target problems. Extensive experiments show that, with limited\ntraining examples (e.g., 15 examples), FoR enables the discovery of diverse,\ncreative, high-quality solutions, greatly outperforming a wide range of\nexisting inference and training methods across six challenging reasoning tasks,\nincluding BlocksWorld (embodied reasoning), Game24 (math puzzle solving),\nRubik's Cube (spatial reasoning), 1D-ARC (abstraction reasoning), GSM8k (math\nreasoning), and ProntoQA (logical reasoning). Code is available at\nhttps://github.com/Yu-Fangxu/FoR.",
      "tldr_zh": "该论文提出 Flow of Reasoning (FoR)，一种高效的 LLM 微调方法，旨在用最少数据（如 15 个例子）提升大型语言模型 (LLMs) 在发散性问题解决中的多样性和质量，解决现有方法（如监督微调和强化学习）忽略解决方案多样性的问题。FoR 将多步 LLM 推理表述为 DAG 结构推理图上的 Markovian 流，并借鉴 GFlowNet 技术，让模型以与目标问题奖励成比例的概率采样发散路径。实验结果显示，FoR 在六个挑战任务上（如 BlocksWorld 体化推理、Game24 数学谜题和 GSM8k 数学推理）大幅优于基线方法，显著提高了解决方案的创意和高质性。",
      "categories": [
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2406.05673v5",
      "published_date": "2024-06-09 07:06:58 UTC",
      "updated_date": "2025-03-08 13:10:25 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T18:04:31.965828"
    },
    {
      "arxiv_id": "2406.07579v1",
      "title": "GFPack++: Improving 2D Irregular Packing by Learning Gradient Field with Attention",
      "title_zh": "GFPack++：通过学习带注意力的梯度场改进二维不规则填充",
      "authors": [
        "Tianyang Xue",
        "Lin Lu",
        "Yang Liu",
        "Mingdong Wu",
        "Hao Dong",
        "Yanbin Zhang",
        "Renmin Han",
        "Baoquan Chen"
      ],
      "abstract": "2D irregular packing is a classic combinatorial optimization problem with\nvarious applications, such as material utilization and texture atlas\ngeneration. This NP-hard problem requires efficient algorithms to optimize\nspace utilization. Conventional numerical methods suffer from slow convergence\nand high computational cost. Existing learning-based methods, such as the\nscore-based diffusion model, also have limitations, such as no rotation\nsupport, frequent collisions, and poor adaptability to arbitrary boundaries,\nand slow inferring. The difficulty of learning from teacher packing is to\ncapture the complex geometric relationships among packing examples, which\ninclude the spatial (position, orientation) relationships of objects, their\ngeometric features, and container boundary conditions. Representing these\nrelationships in latent space is challenging. We propose GFPack++, an\nattention-based gradient field learning approach that addresses this challenge.\nIt consists of two pivotal strategies: \\emph{attention-based geometry encoding}\nfor effective feature encoding and \\emph{attention-based relation encoding} for\nlearning complex relationships. We investigate the utilization distribution\nbetween the teacher and inference data and design a weighting function to\nprioritize tighter teacher data during training, enhancing learning\neffectiveness. Our diffusion model supports continuous rotation and outperforms\nexisting methods on various datasets. We achieve higher space utilization over\nseveral widely used baselines, one-order faster than the previous\ndiffusion-based method, and promising generalization for arbitrary boundaries.\nWe plan to release our source code and datasets to support further research in\nthis direction.",
      "tldr_zh": "本论文针对2D不规则填充这一NP-hard组合优化问题，提出GFPack++方法，通过学习gradient field with attention来提升空间利用率，解决传统数值方法和现有学习方法（如基于分数的diffusion model）的局限性，如不支持旋转和推理缓慢。GFPack++的关键策略包括attention-based geometry encoding用于有效特征编码、attention-based relation encoding用于捕捉复杂几何关系，以及一个加权函数优先处理更紧凑的教师数据，从而提高训练效果。该方法支持连续旋转，在多种数据集上实现比基线更高的空间利用率、一阶速度提升，并对任意边界具有良好泛化性能，计划开源代码以推动进一步研究。",
      "categories": [
        "cs.AI",
        "cs.GR",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2406.07579v1",
      "published_date": "2024-06-09 06:44:08 UTC",
      "updated_date": "2024-06-09 06:44:08 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T18:04:41.569935"
    },
    {
      "arxiv_id": "2406.05659v1",
      "title": "Do LLMs Exhibit Human-Like Reasoning? Evaluating Theory of Mind in LLMs for Open-Ended Responses",
      "title_zh": "LLMs 是否表现出类似人类的推理？评估 LLMs 在开放式响应中的心智理论",
      "authors": [
        "Maryam Amirizaniani",
        "Elias Martin",
        "Maryna Sivachenko",
        "Afra Mashhadi",
        "Chirag Shah"
      ],
      "abstract": "Theory of Mind (ToM) reasoning entails recognizing that other individuals\npossess their own intentions, emotions, and thoughts, which is vital for\nguiding one's own thought processes. Although large language models (LLMs)\nexcel in tasks such as summarization, question answering, and translation, they\nstill face challenges with ToM reasoning, especially in open-ended questions.\nDespite advancements, the extent to which LLMs truly understand ToM reasoning\nand how closely it aligns with human ToM reasoning remains inadequately\nexplored in open-ended scenarios. Motivated by this gap, we assess the\nabilities of LLMs to perceive and integrate human intentions and emotions into\ntheir ToM reasoning processes within open-ended questions. Our study utilizes\nposts from Reddit's ChangeMyView platform, which demands nuanced social\nreasoning to craft persuasive responses. Our analysis, comparing semantic\nsimilarity and lexical overlap metrics between responses generated by humans\nand LLMs, reveals clear disparities in ToM reasoning capabilities in open-ended\nquestions, with even the most advanced models showing notable limitations. To\nenhance LLM capabilities, we implement a prompt tuning method that incorporates\nhuman intentions and emotions, resulting in improvements in ToM reasoning\nperformance. However, despite these improvements, the enhancement still falls\nshort of fully achieving human-like reasoning. This research highlights the\ndeficiencies in LLMs' social reasoning and demonstrates how integrating human\nintentions and emotions can boost their effectiveness.",
      "tldr_zh": "本研究评估大型语言模型（LLMs）在开放式问题中的Theory of Mind (ToM)推理能力，即理解他人意图、情感和想法的能力，使用Reddit的ChangeMyView数据集比较人类和LLMs的响应。结果显示，LLMs在ToM推理上存在显著局限性，即使最先进的模型也无法与人类匹敌。研究通过提示调整（prompt tuning）方法整合人类意图和情感，改善了LLMs的表现，但提升仍不足以实现真正的人类级推理。该工作揭示了LLMs在社会推理方面的缺陷，并为未来优化提供了方向。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2406.05659v1",
      "published_date": "2024-06-09 05:57:59 UTC",
      "updated_date": "2024-06-09 05:57:59 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T18:04:52.073678"
    },
    {
      "arxiv_id": "2406.05658v4",
      "title": "Visual Prompt Tuning in Null Space for Continual Learning",
      "title_zh": "零空间中的视觉提示微调用于持续学习",
      "authors": [
        "Yue Lu",
        "Shizhou Zhang",
        "De Cheng",
        "Yinghui Xing",
        "Nannan Wang",
        "Peng Wang",
        "Yanning Zhang"
      ],
      "abstract": "Existing prompt-tuning methods have demonstrated impressive performances in\ncontinual learning (CL), by selecting and updating relevant prompts in the\nvision-transformer models. On the contrary, this paper aims to learn each task\nby tuning the prompts in the direction orthogonal to the subspace spanned by\nprevious tasks' features, so as to ensure no interference on tasks that have\nbeen learned to overcome catastrophic forgetting in CL. However, different from\nthe orthogonal projection in the traditional CNN architecture, the prompt\ngradient orthogonal projection in the ViT architecture shows completely\ndifferent and greater challenges, i.e., 1) the high-order and non-linear\nself-attention operation; 2) the drift of prompt distribution brought by the\nLayerNorm in the transformer block. Theoretically, we have finally deduced two\nconsistency conditions to achieve the prompt gradient orthogonal projection,\nwhich provide a theoretical guarantee of eliminating interference on previously\nlearned knowledge via the self-attention mechanism in visual prompt tuning. In\npractice, an effective null-space-based approximation solution has been\nproposed to implement the prompt gradient orthogonal projection. Extensive\nexperimental results demonstrate the effectiveness of anti-forgetting on four\nclass-incremental benchmarks with diverse pre-trained baseline models, and our\napproach achieves superior performances to state-of-the-art methods. Our code\nis available at https://github.com/zugexiaodui/VPTinNSforCL.",
      "tldr_zh": "这篇论文提出了一种名为“Visual Prompt Tuning in Null Space”的方法，用于持续学习（Continual Learning），通过在正交于先前任务特征子空间的方向调优提示，避免灾难性遗忘。不同于传统CNN架构，该方法针对Vision Transformer (ViT) 的高阶非线性自注意力操作和LayerNorm引起的提示分布漂移，理论上推导了两个一致性条件，以确保提示梯度正交投影消除对先前知识的干扰。实践中，采用基于零空间的近似解决方案进行实现。实验结果显示，该方法在四个类增量基准上表现出色，优于现有最先进方法，并提供了开源代码。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "Accepted by NeurIPS 2024",
      "pdf_url": "http://arxiv.org/pdf/2406.05658v4",
      "published_date": "2024-06-09 05:57:40 UTC",
      "updated_date": "2024-10-26 08:33:20 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T18:05:03.956801"
    },
    {
      "arxiv_id": "2406.05653v1",
      "title": "Heart Sound Segmentation Using Deep Learning Techniques",
      "title_zh": "翻译失败",
      "authors": [
        "Manas Madine"
      ],
      "abstract": "Heart disease remains a leading cause of mortality worldwide. Auscultation,\nthe process of listening to heart sounds, can be enhanced through\ncomputer-aided analysis using Phonocardiogram (PCG) signals. This paper\npresents a novel approach for heart sound segmentation and classification into\nS1 (LUB) and S2 (DUB) sounds. We employ FFT-based filtering, dynamic\nprogramming for event detection, and a Siamese network for robust\nclassification. Our method demonstrates superior performance on the PASCAL\nheart sound dataset compared to existing approaches.",
      "tldr_zh": "本文提出了一种新方法，用于心音分割和分类成 S1 (LUB) 和 S2 (DUB) 声音，以提升 Phonocardiogram (PCG) 信号的计算机辅助诊断。该方法结合了 FFT-based filtering 进行信号处理、dynamic programming for event detection 检测事件，以及 Siamese network for robust classification 实现鲁棒分类。在 PASCAL heart sound dataset 上，该方法比现有方法表现出色，有助于改善心脏病诊断的准确性。",
      "categories": [
        "cs.SD",
        "cs.AI",
        "eess.AS"
      ],
      "primary_category": "cs.SD",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2406.05653v1",
      "published_date": "2024-06-09 05:30:05 UTC",
      "updated_date": "2024-06-09 05:30:05 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T18:05:15.422288"
    },
    {
      "arxiv_id": "2406.05649v2",
      "title": "GTR: Improving Large 3D Reconstruction Models through Geometry and Texture Refinement",
      "title_zh": "GTR：通过几何和纹理细化改进大型 3D 重建模型",
      "authors": [
        "Peiye Zhuang",
        "Songfang Han",
        "Chaoyang Wang",
        "Aliaksandr Siarohin",
        "Jiaxu Zou",
        "Michael Vasilkovsky",
        "Vladislav Shakhrai",
        "Sergey Korolev",
        "Sergey Tulyakov",
        "Hsin-Ying Lee"
      ],
      "abstract": "We propose a novel approach for 3D mesh reconstruction from multi-view\nimages. Our method takes inspiration from large reconstruction models like LRM\nthat use a transformer-based triplane generator and a Neural Radiance Field\n(NeRF) model trained on multi-view images. However, in our method, we introduce\nseveral important modifications that allow us to significantly enhance 3D\nreconstruction quality. First of all, we examine the original LRM architecture\nand find several shortcomings. Subsequently, we introduce respective\nmodifications to the LRM architecture, which lead to improved multi-view image\nrepresentation and more computationally efficient training. Second, in order to\nimprove geometry reconstruction and enable supervision at full image\nresolution, we extract meshes from the NeRF field in a differentiable manner\nand fine-tune the NeRF model through mesh rendering. These modifications allow\nus to achieve state-of-the-art performance on both 2D and 3D evaluation\nmetrics, such as a PSNR of 28.67 on Google Scanned Objects (GSO) dataset.\nDespite these superior results, our feed-forward model still struggles to\nreconstruct complex textures, such as text and portraits on assets. To address\nthis, we introduce a lightweight per-instance texture refinement procedure.\nThis procedure fine-tunes the triplane representation and the NeRF color\nestimation model on the mesh surface using the input multi-view images in just\n4 seconds. This refinement improves the PSNR to 29.79 and achieves faithful\nreconstruction of complex textures, such as text. Additionally, our approach\nenables various downstream applications, including text- or image-to-3D\ngeneration.",
      "tldr_zh": "我们提出 GTR 方法，通过改进 LRM 架构来提升从多视图图像重建 3D 网格的质量，包括优化多视图图像表示和训练效率，以及引入可微分网格提取和 NeRF 模型微调，以实现全分辨率监督和更好的几何重建。实验结果显示，GTR 在 Google Scanned Objects (GSO) 数据集上达到 28.67 的 PSNR，并通过一个轻量级 per-instance 纹理精炼程序（只需 4 秒钟微调 triplane 和 NeRF 颜色估计模型）将 PSNR 提升至 29.79，同时忠实重建复杂纹理如文本和肖像。该方法不仅实现了 state-of-the-art 的 2D 和 3D 评估指标，还支持下游应用如文本或图像到 3D 生成。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "19 pages, 17 figures. Project page:\n  https://snap-research.github.io/GTR/",
      "pdf_url": "http://arxiv.org/pdf/2406.05649v2",
      "published_date": "2024-06-09 05:19:24 UTC",
      "updated_date": "2024-06-13 18:18:15 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T18:05:30.710016"
    },
    {
      "arxiv_id": "2406.05645v1",
      "title": "Anomaly Multi-classification in Industrial Scenarios: Transferring Few-shot Learning to a New Task",
      "title_zh": "翻译失败",
      "authors": [
        "Jie Liu",
        "Yao Wu",
        "Xiaotong Luo",
        "Zongze Wu"
      ],
      "abstract": "In industrial scenarios, it is crucial not only to identify anomalous items\nbut also to classify the type of anomaly. However, research on anomaly\nmulti-classification remains largely unexplored. This paper proposes a novel\nand valuable research task called anomaly multi-classification. Given the\nchallenges in applying few-shot learning to this task, due to limited training\ndata and unique characteristics of anomaly images, we introduce a baseline\nmodel that combines RelationNet and PatchCore. We propose a data generation\nmethod that creates pseudo classes and a corresponding proxy task, aiming to\nbridge the gap in transferring few-shot learning to industrial scenarios.\nFurthermore, we utilize contrastive learning to improve the vanilla baseline,\nachieving much better performance than directly fine-tune a ResNet. Experiments\nconducted on MvTec AD and MvTec3D AD demonstrate that our approach shows\nsuperior performance in this novel task.",
      "tldr_zh": "该论文提出一个新的研究任务——anomaly multi-classification，用于工业场景中不仅识别异常物品，还分类其类型，以解决现有few-shot learning在数据有限和异常图像独特特性下的挑战。作者引入一个基线模型，结合RelationNet和PatchCore，并提出数据生成方法创建pseudo classes和proxy task，以桥接few-shot learning在工业环境的转移应用。此外，通过contrastive learning改进基线模型，其性能远超直接fine-tune ResNet；在MvTec AD和MvTec3D AD数据集上的实验显示，该方法表现出色，验证了其有效性。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2406.05645v1",
      "published_date": "2024-06-09 05:07:39 UTC",
      "updated_date": "2024-06-09 05:07:39 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T18:05:45.691254"
    },
    {
      "arxiv_id": "2406.05644v2",
      "title": "How Alignment and Jailbreak Work: Explain LLM Safety through Intermediate Hidden States",
      "title_zh": "对齐和越狱的工作原理：通过中间隐藏状态解释 LLM 安全",
      "authors": [
        "Zhenhong Zhou",
        "Haiyang Yu",
        "Xinghua Zhang",
        "Rongwu Xu",
        "Fei Huang",
        "Yongbin Li"
      ],
      "abstract": "Large language models (LLMs) rely on safety alignment to avoid responding to\nmalicious user inputs. Unfortunately, jailbreak can circumvent safety\nguardrails, resulting in LLMs generating harmful content and raising concerns\nabout LLM safety. Due to language models with intensive parameters often\nregarded as black boxes, the mechanisms of alignment and jailbreak are\nchallenging to elucidate. In this paper, we employ weak classifiers to explain\nLLM safety through the intermediate hidden states. We first confirm that LLMs\nlearn ethical concepts during pre-training rather than alignment and can\nidentify malicious and normal inputs in the early layers. Alignment actually\nassociates the early concepts with emotion guesses in the middle layers and\nthen refines them to the specific reject tokens for safe generations. Jailbreak\ndisturbs the transformation of early unethical classification into negative\nemotions. We conduct experiments on models from 7B to 70B across various model\nfamilies to prove our conclusion. Overall, our paper indicates the intrinsical\nmechanism of LLM safety and how jailbreaks circumvent safety guardrails,\noffering a new perspective on LLM safety and reducing concerns. Our code is\navailable at https://github.com/ydyjya/LLM-IHS-Explanation.",
      "tldr_zh": "本研究通过分析大型语言模型 (LLMs) 的中间隐藏状态 (intermediate hidden states)，使用弱分类器 (weak classifiers) 揭示了安全对齐 (alignment) 和越狱 (jailbreak) 的内在机制。研究发现，LLMs 在预训练阶段就学习了伦理概念，并在早期层能识别恶意输入，而对齐过程则在中间层将这些概念与负面情感关联，并细化为拒绝令牌 (reject tokens) 以生成安全输出。越狱攻击干扰了这一概念向情感转化的过程，导致有害内容生成。实验在从 7B 到 70B 的多种模型家族上验证了这些结论，为理解 LLM 安全提供了新视角，并缓解了相关担忧。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.CR",
        "cs.CY"
      ],
      "primary_category": "cs.CL",
      "comment": "27 pages",
      "pdf_url": "http://arxiv.org/pdf/2406.05644v2",
      "published_date": "2024-06-09 05:04:37 UTC",
      "updated_date": "2024-06-13 05:39:31 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T18:05:57.932628"
    },
    {
      "arxiv_id": "2406.05631v1",
      "title": "CCSI: Continual Class-Specific Impression for Data-free Class Incremental Learning",
      "title_zh": "翻译失败",
      "authors": [
        "Sana Ayromlou",
        "Teresa Tsang",
        "Purang Abolmaesumi",
        "Xiaoxiao Li"
      ],
      "abstract": "In real-world clinical settings, traditional deep learning-based\nclassification methods struggle with diagnosing newly introduced disease types\nbecause they require samples from all disease classes for offline training.\nClass incremental learning offers a promising solution by adapting a deep\nnetwork trained on specific disease classes to handle new diseases. However,\ncatastrophic forgetting occurs, decreasing the performance of earlier classes\nwhen adapting the model to new data. Prior proposed methodologies to overcome\nthis require perpetual storage of previous samples, posing potential practical\nconcerns regarding privacy and storage regulations in healthcare. To this end,\nwe propose a novel data-free class incremental learning framework that utilizes\ndata synthesis on learned classes instead of data storage from previous\nclasses. Our key contributions include acquiring synthetic data known as\nContinual Class-Specific Impression (CCSI) for previously inaccessible trained\nclasses and presenting a methodology to effectively utilize this data for\nupdating networks when introducing new classes. We obtain CCSI by employing\ndata inversion over gradients of the trained classification model on previous\nclasses starting from the mean image of each class inspired by common landmarks\nshared among medical images and utilizing continual normalization layers\nstatistics as a regularizer in this pixel-wise optimization process.\nSubsequently, we update the network by combining the synthesized data with new\nclass data and incorporate several losses, including an intra-domain\ncontrastive loss to generalize the deep network trained on the synthesized data\nto real data, a margin loss to increase separation among previous classes and\nnew ones, and a cosine-normalized cross-entropy loss to alleviate the adverse\neffects of imbalanced distributions in training data.",
      "tldr_zh": "这篇论文针对医疗领域的类增量学习（Class Incremental Learning）提出了一种数据无关（data-free）的框架，解决传统模型在引入新疾病类型时面临的灾难性遗忘（catastrophic forgetting）和数据存储隐私问题。核心贡献是引入 Continual Class-Specific Impression (CCSI)，通过数据反演（data inversion）从训练模型的梯度生成合成数据，利用类均值图像和 continual normalization layers 的统计作为正则化。框架在更新网络时结合 CCSI 与新类数据，应用 intra-domain contrastive loss、margin loss 和 cosine-normalized cross-entropy loss 等损失函数，以提升模型泛化能力、类间分离和处理数据不平衡，最终实现高效的医疗诊断适应性。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2406.05631v1",
      "published_date": "2024-06-09 03:52:21 UTC",
      "updated_date": "2024-06-09 03:52:21 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T18:06:12.676878"
    },
    {
      "arxiv_id": "2406.05623v1",
      "title": "Observation Denoising in CYRUS Soccer Simulation 2D Team For RoboCup 2024",
      "title_zh": "翻译失败",
      "authors": [
        "Nader Zare",
        "Aref Sayareh",
        "Sadra Khanjari",
        "Arad Firouzkouhi"
      ],
      "abstract": "In the Soccer Simulation 2D environment, accurate observation is crucial for\neffective decision making. However, challenges such as partial observation and\nnoisy data can hinder performance. To address these issues, we propose a\ndenoising algorithm that leverages predictive modeling and intersection\nanalysis to enhance the accuracy of observations. Our approach aims to mitigate\nthe impact of noise and partial data, leading to improved gameplay performance.\nThis paper presents the framework, implementation, and preliminary results of\nour algorithm, demonstrating its potential in refining observations in Soccer\nSimulation 2D. Cyrus 2D Team is using a combination of Helios, Gliders, and\nCyrus base codes.",
      "tldr_zh": "在Soccer Simulation 2D环境中，部分观察和噪声数据会阻碍决策准确性，因此研究提出了一种去噪算法，利用predictive modeling和intersection analysis来提升观察数据的精确性。该算法旨在减少噪声和部分数据的影响，从而改善整体游戏表现。论文详细介绍了框架、实现过程和初步结果，并指出Cyrus 2D Team已将其整合到Helios、Gliders和Cyrus基代码中，用于RoboCup 2024的比赛。",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2406.05623v1",
      "published_date": "2024-06-09 03:15:29 UTC",
      "updated_date": "2024-06-09 03:15:29 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T18:06:20.843759"
    },
    {
      "arxiv_id": "2406.05621v1",
      "title": "Cross Language Soccer Framework: An Open Source Framework for the RoboCup 2D Soccer Simulation",
      "title_zh": "翻译失败",
      "authors": [
        "Nader Zare",
        "Aref Sayareh",
        "Alireza Sadraii",
        "Arad Firouzkouhi",
        "Amilcar Soares"
      ],
      "abstract": "RoboCup Soccer Simulation 2D (SS2D) research is hampered by the complexity of\nexisting Cpp-based codes like Helios, Cyrus, and Gliders, which also suffer\nfrom limited integration with modern machine learning frameworks. This\ndevelopment paper introduces a transformative solution a gRPC-based,\nlanguage-agnostic framework that seamlessly integrates with the\nhigh-performance Helios base code. This approach not only facilitates the use\nof diverse programming languages including CSharp, JavaScript, and Python but\nalso maintains the computational efficiency critical for real time decision\nmaking in SS2D. By breaking down language barriers, our framework significantly\nenhances collaborative potential and flexibility, empowering researchers to\ninnovate without the overhead of mastering or developing extensive base codes.\nWe invite the global research community to leverage and contribute to the Cross\nLanguage Soccer (CLS) framework, which is openly available under the MIT\nLicense, to drive forward the capabilities of multi-agent systems in soccer\nsimulations.",
      "tldr_zh": "该论文提出了 Cross Language Soccer (CLS) 框架，这是一个基于 gRPC 的开源框架，用于 RoboCup Soccer Simulation 2D (SS2D)，旨在解决现有 C++ 代码（如 Helios、Cyrus 和 Gliders）的复杂性和与现代机器学习框架整合有限的问题。框架与 Helios 基础代码无缝整合，支持多种编程语言如 CSharp、JavaScript 和 Python，同时保持实时决策所需的计算效率。通过打破语言障碍，该框架增强了研究协作和灵活性，允许研究者轻松创新。CLS 以 MIT 许可开源，邀请全球社区使用和贡献，以推进 multi-agent systems 在足球模拟中的能力。",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2406.05621v1",
      "published_date": "2024-06-09 03:11:40 UTC",
      "updated_date": "2024-06-09 03:11:40 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T18:06:34.100638"
    },
    {
      "arxiv_id": "2406.10249v1",
      "title": "A Reality check of the benefits of LLM in business",
      "title_zh": "翻译失败",
      "authors": [
        "Ming Cheung"
      ],
      "abstract": "Large language models (LLMs) have achieved remarkable performance in language\nunderstanding and generation tasks by leveraging vast amounts of online texts.\nUnlike conventional models, LLMs can adapt to new domains through prompt\nengineering without the need for retraining, making them suitable for various\nbusiness functions, such as strategic planning, project implementation, and\ndata-driven decision-making. However, their limitations in terms of bias,\ncontextual understanding, and sensitivity to prompts raise concerns about their\nreadiness for real-world applications. This paper thoroughly examines the\nusefulness and readiness of LLMs for business processes. The limitations and\ncapacities of LLMs are evaluated through experiments conducted on four\naccessible LLMs using real-world data. The findings have significant\nimplications for organizations seeking to leverage generative AI and provide\nvaluable insights into future research directions. To the best of our\nknowledge, this represents the first quantified study of LLMs applied to core\nbusiness operations and challenges.",
      "tldr_zh": "本论文评估了大型语言模型（LLMs）在商业领域的实际益处，包括其通过 prompt engineering 适应新领域的能力，从而支持战略规划、项目实施和数据驱动决策等功能。研究通过实验测试了四个可访问的 LLMs，使用真实世界数据，揭示了 LLMs 的局限性，如偏见、上下文理解不足和对提示的敏感性。结果显示，虽然 LLMs 有潜力提升商业过程，但其准备度仍有待改进，并为组织应用生成式 AI 提供了关键启示和未来研究方向。",
      "categories": [
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2406.10249v1",
      "published_date": "2024-06-09 02:36:00 UTC",
      "updated_date": "2024-06-09 02:36:00 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T18:06:45.453238"
    },
    {
      "arxiv_id": "2407.07732v1",
      "title": "Text2VP: Generative AI for Visual Programming and Parametric Modeling",
      "title_zh": "Text2VP：生成式 AI 用于视觉编程和参数化建模",
      "authors": [
        "Guangxi Feng",
        "Wei Yan"
      ],
      "abstract": "The integration of generative artificial intelligence (AI) into architectural\ndesign has witnessed a significant evolution, marked by the recent advancements\nin AI to generate text, images, and 3D models. However, no models exist for\ntext-to-parametric models that are used in architectural design for generating\nvarious design options, including free-form designs, and optimizing the design\noptions. This study creates and investigates an innovative application of\ngenerative AI in parametric modeling by leveraging a customized Text-to-Visual\nProgramming (Text2VP) GPT derived from GPT-4. The primary focus is on\nautomating the generation of graph-based visual programming workflows,\nincluding parameters and the links among the parameters, through AI-generated\nscripts, accurately reflecting users' design intentions and allowing the users\nto change the parameter values interactively. The Text2VP GPT customization\nprocess utilizes detailed and complete documentation of the visual programming\nlanguage components, example-driven few-shot learning, and specific\ninstructional guides. Our testing demonstrates Text2VP's capability to generate\nworking parametric models. The paper also discusses the limitations of Text2VP;\nfor example, more complex parametric model generation introduces higher error\nrates. This research highlights the potential of generative AI in visual\nprogramming and parametric modeling and sets a foundation for future\nenhancements to handle more sophisticated and intricate modeling tasks\neffectively. The study aims to allow designers to create and change design\nmodels without significant effort in learning a specific programming language\nsuch as Grasshopper.",
      "tldr_zh": "本研究提出Text2VP，一种基于Generative AI的创新系统，利用定制的GPT-4模型，将文本输入转化为视觉编程和参数化建模工作流，包括参数生成和链接，以自动化建筑设计过程。方法涉及利用视觉编程语言的详细文档、少样本学习和指导性指令，确保生成的模型准确反映用户设计意图，并支持交互式参数调整。测试结果显示Text2VP能够成功创建可工作的参数化模型，但复杂任务的错误率较高；该研究为简化设计师的工作奠定基础，未来有望处理更复杂的建模任务，而无需学习特定编程语言如Grasshopper。",
      "categories": [
        "cs.HC",
        "cs.AI"
      ],
      "primary_category": "cs.HC",
      "comment": "Demonstration Video:\n  https://www.youtube.com/playlist?list=PLUOmOLuLSaDWss2En2buixBxvTPy-lDvA",
      "pdf_url": "http://arxiv.org/pdf/2407.07732v1",
      "published_date": "2024-06-09 02:22:20 UTC",
      "updated_date": "2024-06-09 02:22:20 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T18:06:57.517898"
    },
    {
      "arxiv_id": "2406.05612v3",
      "title": "Which Backbone to Use: A Resource-efficient Domain Specific Comparison for Computer Vision",
      "title_zh": "使用哪个骨干网：计算机视觉中资源高效的特定领域比较",
      "authors": [
        "Pranav Jeevan",
        "Amit Sethi"
      ],
      "abstract": "In contemporary computer vision applications, particularly image\nclassification, architectural backbones pre-trained on large datasets like\nImageNet are commonly employed as feature extractors. Despite the widespread\nuse of these pre-trained convolutional neural networks (CNNs), there remains a\ngap in understanding the performance of various resource-efficient backbones\nacross diverse domains and dataset sizes. Our study systematically evaluates\nmultiple lightweight, pre-trained CNN backbones under consistent training\nsettings across a variety of datasets, including natural images, medical\nimages, galaxy images, and remote sensing images. This comprehensive analysis\naims to aid machine learning practitioners in selecting the most suitable\nbackbone for their specific problem, especially in scenarios involving small\ndatasets where fine-tuning a pre-trained network is crucial. Even though\nattention-based architectures are gaining popularity, we observed that they\ntend to perform poorly under low data finetuning tasks compared to CNNs. We\nalso observed that some CNN architectures such as ConvNeXt, RegNet and\nEfficientNet performs well compared to others on a diverse set of domains\nconsistently. Our findings provide actionable insights into the performance\ntrade-offs and effectiveness of different backbones, facilitating informed\ndecision-making in model selection for a broad spectrum of computer vision\ndomains. Our code is available here: https://github.com/pranavphoenix/Backbones",
      "tldr_zh": "本研究评估了多种轻量级预训练 CNN 骨干网络在计算机视觉任务中的性能，特别是图像分类，针对不同领域（如自然图像、医疗图像、星系图像和遥感图像）以及数据集大小进行系统比较。研究在一致的训练设置下测试这些网络，旨在帮助机器学习从业者选择最合适的骨干网络，尤其是在小数据集场景下进行微调时。结果显示，注意力-based 架构在低数据微调任务中表现较差，而 ConvNeXt、RegNet 和 EfficientNet 等 CNN 模型在多样领域中表现出色，提供了一致的性能优势。该工作提供了关于骨干网络性能权衡的实用洞见，并公开了代码以便进一步应用。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG",
        "I.2.10; I.4.0; I.4.1; I.4.2; I.4.6; I.4.7; I.4.8; I.4.9; I.4.10;\n  I.2.10; I.5.1; I.5.2; I.5.4; J.2"
      ],
      "primary_category": "cs.CV",
      "comment": "12 pages, 2 figures, accepted in TMLR",
      "pdf_url": "http://arxiv.org/pdf/2406.05612v3",
      "published_date": "2024-06-09 02:01:25 UTC",
      "updated_date": "2025-03-09 21:00:14 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T18:07:11.474849"
    },
    {
      "arxiv_id": "2406.05605v1",
      "title": "Deep Learning to Predict Glaucoma Progression using Structural Changes in the Eye",
      "title_zh": "翻译失败",
      "authors": [
        "Sayan Mandal"
      ],
      "abstract": "Glaucoma is a chronic eye disease characterized by optic neuropathy, leading\nto irreversible vision loss. It progresses gradually, often remaining\nundiagnosed until advanced stages. Early detection is crucial to monitor\natrophy and develop treatment strategies to prevent further vision impairment.\nData-centric methods have enabled computer-aided algorithms for precise\nglaucoma diagnosis.\n  In this study, we use deep learning models to identify complex disease traits\nand progression criteria, detecting subtle changes indicative of glaucoma. We\nexplore the structure-function relationship in glaucoma progression and predict\nfunctional impairment from structural eye deterioration. We analyze statistical\nand machine learning methods, including deep learning techniques with optical\ncoherence tomography (OCT) scans for accurate progression prediction.\n  Addressing challenges like age variability, data imbalances, and noisy\nlabels, we develop novel semi-supervised time-series algorithms:\n  1. Weakly-Supervised Time-Series Learning: We create a CNN-LSTM model to\nencode spatiotemporal features from OCT scans. This approach uses age-related\nprogression and positive-unlabeled data to establish robust pseudo-progression\ncriteria, bypassing gold-standard labels.\n  2. Semi-Supervised Time-Series Learning: Using labels from Guided Progression\nAnalysis (GPA) in a contrastive learning scheme, the CNN-LSTM architecture\nlearns from potentially mislabeled data to improve prediction accuracy.\n  Our methods outperform conventional and state-of-the-art techniques.",
      "tldr_zh": "本文使用深度学习模型预测青光眼进展，通过分析眼部结构变化（如光学相干断层扫描，OCT）来检测微妙疾病特征和功能损伤。研究开发了两种新型半监督时间序列算法：Weakly-Supervised Time-Series Learning，利用CNN-LSTM模型和年龄相关数据建立伪进展标准；以及Semi-Supervised Time-Series Learning，通过Guided Progression Analysis (GPA)标签的对比学习方案处理数据不平衡和噪声标签问题。这些方法有效解决了传统挑战，并在实验中优于现有技术和统计模型，提高了预测准确性。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG",
        "68T07",
        "I.2.1"
      ],
      "primary_category": "cs.CV",
      "comment": "Dissertation",
      "pdf_url": "http://arxiv.org/pdf/2406.05605v1",
      "published_date": "2024-06-09 01:12:41 UTC",
      "updated_date": "2024-06-09 01:12:41 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T18:07:23.078803"
    },
    {
      "arxiv_id": "2406.05603v1",
      "title": "A Knowledge-Component-Based Methodology for Evaluating AI Assistants",
      "title_zh": "基于知识组件的方法用于评估 AI 助手",
      "authors": [
        "Laryn Qi",
        "J. D. Zamfirescu-Pereira",
        "Taehan Kim",
        "Björn Hartmann",
        "John DeNero",
        "Narges Norouzi"
      ],
      "abstract": "We evaluate an automatic hint generator for CS1 programming assignments\npowered by GPT-4, a large language model. This system provides natural language\nguidance about how students can improve their incorrect solutions to short\nprogramming exercises. A hint can be requested each time a student fails a test\ncase. Our evaluation addresses three Research Questions:\n  RQ1: Do the hints help students improve their code? RQ2: How effectively do\nthe hints capture problems in student code? RQ3: Are the issues that students\nresolve the same as the issues addressed in the hints?\n  To address these research questions quantitatively, we identified a set of\nfine-grained knowledge components and determined which ones apply to each\nexercise, incorrect solution, and generated hint. Comparing data from two large\nCS1 offerings, we found that access to the hints helps students to address\nproblems with their code more quickly, that hints are able to consistently\ncapture the most pressing errors in students' code, and that hints that address\na few issues at once rather than a single bug are more likely to lead to direct\nstudent progress.",
      "tldr_zh": "本研究提出了一种基于知识 components 的方法，用于评估 AI 助手（如 GPT-4 驱动的自动提示生成器），以帮助 CS1 编程作业的学生改进错误代码。研究通过三个研究问题（RQ1：提示是否帮助学生改进代码；RQ2：提示是否有效捕捉代码问题；RQ3：学生解决的问题是否与提示一致）进行量化评估，涉及细粒度的知识 components 分析。结果显示，提供提示能让学生更快解决代码问题，提示能可靠地捕捉主要错误，且同时处理多个问题的提示更可能直接促进学生进步，为 AI 辅助教育提供可行框架。",
      "categories": [
        "cs.CY",
        "cs.AI"
      ],
      "primary_category": "cs.CY",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2406.05603v1",
      "published_date": "2024-06-09 00:58:39 UTC",
      "updated_date": "2024-06-09 00:58:39 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T18:07:33.186979"
    }
  ],
  "raw_papers_fetched": true,
  "papers_count": 60,
  "processed_papers_count": 60,
  "failed_papers_count": 0,
  "summary_generated": true,
  "daily_data_saved": true,
  "last_update": "2025-05-18T18:08:07.017003"
}
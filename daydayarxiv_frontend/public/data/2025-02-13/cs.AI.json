{
  "date": "2025-02-13",
  "category": "cs.AI",
  "summary": "欢迎来到 UTC 时间 2025-02-13 的 arXiv 中文 TLDR 快报！今天 arXiv 的论文主要聚焦于 AI 和机器学习的创新，特别是 LLM 的优化、安全评估、多模态处理和应用扩展，令人印象深刻的文章包括 AgentGuard（LLM 代理的安全评估）和 Non-Markovian Discrete Diffusion（因果语言模型的扩散建模），而知名学者如 Yuejie Chi 和 Mark A. Musen 的参与进一步提升了论文的影响力。\n\n下面，我将逐一简要概述今天的论文，先优先讨论那些重要、创新性强或有潜在话题度的文章（如 LLM 安全、强化学习和多模态模型），并将相关主题归类讨论。其他较常规或技术细节较深的论文将快速掠过，只提核心贡献。\n\n### LLM 和 AI 安全相关\n- **AgentGuard: Repurposing Agentic Orchestrator for Safety Evaluation of Tool Orchestration**（AgentGuard: 重新利用代理协调器评估工具编排的安全性）  \n  这篇论文提出 AgentGuard 框架，用于自动发现和验证 LLM 代理的不安全工作流，并生成安全约束。该方法显著提升了 LLM 代理的部署安全性，尤其在真实世界应用中，作者 Jizhou Chen 等通过实验证明了其有效性。\n  \n- **A Solver-Aided Hierarchical Language for LLM-Driven CAD Design**（一种基于求解器的分层语言，用于 LLM 驱动的 CAD 设计）  \n  论文引入 AIDL 语言，利用几何约束求解器辅助 LLM 生成 CAD 几何结构，显著改善了复杂几何建模的准确性和后处理效率。作者如 Vladimir Kim 和 Adriana Schulz 的参与使这篇论文在 AI 辅助设计领域具有影响力。\n\n- **SelfCite: Self-Supervised Alignment for Context Attribution in Large Language Models**（SelfCite: 用于 LLM 上下文归因的自监督对齐）  \n  这篇工作提出 SelfCite 方法，通过自监督机制提升 LLM 在长文本任务中的上下文归因准确性，实验显示其在多种基准上改善了 F1 分数，强调了 LLM 解释性的重要性。\n\n- **Making Them a Malicious Database: Exploiting Query Code to Jailbreak Aligned Large Language Models**（将它们变成恶意数据库: 通过查询代码越狱对齐的 LLM）  \n  论文探索 QueryAttack 框架，利用结构化查询绕过 LLM 的安全对齐，实现了高成功率攻击，但也提供了防御策略，凸显了 LLM 安全漏洞的话题性。\n\n其他 LLM 相关论文，如 **A Survey on LLM-based News Recommender Systems**（LLM-based News Recommender Systems 的调查），快速总结：它系统回顾了 LLM 在新闻推荐中的应用，涵盖新闻建模和用户建模，作者 Rongyao Wang 等强调了数据集和方法的挑战。\n\n### 强化学习和机器人相关\n- **Incentivize without Bonus: Provably Efficient Model-based Online Multi-agent RL for Markov Games**（无需奖励: Markov 游戏中高效的多代理强化学习）  \n  作者 Yuejie Chi 等提出 VMG 算法，通过偏差经验估计提升多代理强化学习的探索效率，在零和和一般和 Markov 游戏中实现了近优的后悔界，理论上具有重要突破。\n\n- **DexTrack: Towards Generalizable Neural Tracking Control for Dexterous Manipulation from Human References**（DexTrack: 从人类参考实现可泛化的灵巧操作神经跟踪控制）  \n  这篇论文开发了 DexTrack 框架，使用数据飞轮和强化学习从人类演示中学习灵巧机械臂控制，实验显示其在模拟和现实环境中成功率提升 10%，作者 Li Yi 等的工作为机器人泛化提供了新路径。\n\n- **Vote-Tree-Planner: Optimizing Execution Order in LLM-based Task Planning Pipeline via Voting**（Vote-Tree-Planner: 通过投票优化 LLM 任务规划的执行顺序）  \n  论文引入投票机制优化 LLM 任务规划的执行路径，提高了成功率和查询效率，在机器人任务中表现出色。\n\n其他机器人论文，如 **AT-Drone: Benchmarking Adaptive Teaming in Multi-Drone Pursuit**（AT-Drone: 多无人机追踪的自适应团队基准），快速掠过：它构建了多无人机自适应基准，作者 Yang Li 等通过实验验证了团队协作算法的鲁棒性。\n\n### 多模态和视觉模型相关\n- **MME-CoT: Benchmarking Chain-of-Thought in Large Multimodal Models for Reasoning Quality, Robustness, and Efficiency**（MME-CoT: 评估多模态模型中思维链的推理质量、鲁棒性和效率）  \n  这篇创新工作提出 MME-CoT 基准，评估多模态模型的思维链推理，作者 Dongzhi Jiang 等发现反思机制（如 Kimi k1.5）显著提升质量，但也暴露了效率问题。\n\n- **EmbodiedBench: Comprehensive Benchmarking Multi-modal Large Language Models for Vision-Driven Embodied Agents**（EmbodiedBench: 全面基准多模态 LLM 用于视觉驱动的实体代理）  \n  论文构建了 EmbodiedBench 基准，评估多模态 LLM 在机器人任务中的性能，作者 Rui Yang 等实验显示 GPT-4o 在感知任务中表现出色。\n\n其他视觉论文，如 **Acute Lymphoblastic Leukemia Diagnosis Employing YOLOv11, YOLOv8, ResNet50, and Inception-ResNet-v2 Deep Learning Models**（使用 YOLOv11 等深度学习模型诊断急性淋巴细胞白血病），快速总结：它实现了 99.7% 的诊断准确率，作者 Alaa Awad 等贡献了多数据集验证。\n\n### 其他领域快速概述\n- **Non-Markovian Discrete Diffusion with Causal Language Models**（非 Markov 离散扩散模型与因果语言模型）  \n  论文提出 CaDDi 框架，融合因果语言模型提升序列建模，作者 Yangtian Zhang 等实验显示其在语言和生物序列任务中优于自回归模型。\n\n- **Toward Total Recall: Enhancing FAIRness through AI-Driven Metadata Standardization**（向完全召回迈进: 通过 AI 驱动的元数据标准化提升 FAIRness）  \n  作者 Mark A. Musen 等使用 GPT-4 优化科学元数据，显著提高了数据检索召回率。\n\n其他论文，如 **On the existence of EFX allocations in multigraphs**（多图中 EFX 分配的存在性），快速掠过：它证明了在特定多图条件下 EFX 公平分配的存在，作者 Alkmini Sgouritsa 等的工作在公平理论中具有理论价值。\n\n总的来说，今天的 arXiv 论文展示了 AI 领域的多样创新，LLM 的安全和多模态应用尤为突出。重点文章如 AgentGuard 和 MME-CoT 不仅有实际影响，还可能引发更多讨论。如果您对特定主题感兴趣，建议查看这些论文的完整摘要！",
  "papers": [
    {
      "arxiv_id": "2502.09819v1",
      "title": "A Solver-Aided Hierarchical Language for LLM-Driven CAD Design",
      "title_zh": "翻译失败",
      "authors": [
        "Benjamin T. Jones",
        "Felix Hähnlein",
        "Zihan Zhang",
        "Maaz Ahmad",
        "Vladimir Kim",
        "Adriana Schulz"
      ],
      "abstract": "Large language models (LLMs) have been enormously successful in solving a\nwide variety of structured and unstructured generative tasks, but they struggle\nto generate procedural geometry in Computer Aided Design (CAD). These\ndifficulties arise from an inability to do spatial reasoning and the necessity\nto guide a model through complex, long range planning to generate complex\ngeometry. We enable generative CAD Design with LLMs through the introduction of\na solver-aided, hierarchical domain specific language (DSL) called AIDL, which\noffloads the spatial reasoning requirements to a geometric constraint solver.\nAdditionally, we show that in the few-shot regime, AIDL outperforms even a\nlanguage with in-training data (OpenSCAD), both in terms of generating visual\nresults closer to the prompt and creating objects that are easier to\npost-process and reason about.",
      "tldr_zh": "这篇论文针对大型语言模型(LLMs)在计算机辅助设计(CAD)中生成程序几何的挑战，提出了一种求解器辅助的层次化领域特定语言(DSL)——AIDL，以将空间推理任务转移给几何约束求解器，从而提升LLMs的生成能力。AIDL通过这种设计，解决了LLMs在复杂长期规划方面的不足。实验结果显示，在少样本场景下，AIDL优于现有语言如OpenSCAD，能生成更接近提示的视觉结果，且创建的对象更容易后续处理和推理。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.GR",
        "cs.LG",
        "cs.PL"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2502.09819v1",
      "published_date": "2025-02-13 23:31:30 UTC",
      "updated_date": "2025-02-13 23:31:30 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T10:45:38.409394"
    },
    {
      "arxiv_id": "2502.09809v1",
      "title": "AgentGuard: Repurposing Agentic Orchestrator for Safety Evaluation of Tool Orchestration",
      "title_zh": "翻译失败",
      "authors": [
        "Jizhou Chen",
        "Samuel Lee Cong"
      ],
      "abstract": "The integration of tool use into large language models (LLMs) enables agentic\nsystems with real-world impact. In the meantime, unlike standalone LLMs,\ncompromised agents can execute malicious workflows with more consequential\nimpact, signified by their tool-use capability. We propose AgentGuard, a\nframework to autonomously discover and validate unsafe tool-use workflows,\nfollowed by generating safety constraints to confine the behaviors of agents,\nachieving the baseline of safety guarantee at deployment. AgentGuard leverages\nthe LLM orchestrator's innate capabilities - knowledge of tool functionalities,\nscalable and realistic workflow generation, and tool execution privileges - to\nact as its own safety evaluator. The framework operates through four phases:\nidentifying unsafe workflows, validating them in real-world execution,\ngenerating safety constraints, and validating constraint efficacy. The output,\nan evaluation report with unsafe workflows, test cases, and validated\nconstraints, enables multiple security applications. We empirically demonstrate\nAgentGuard's feasibility with experiments. With this exploratory work, we hope\nto inspire the establishment of standardized testing and hardening procedures\nfor LLM agents to enhance their trustworthiness in real-world applications.",
      "tldr_zh": "该研究提出AgentGuard框架，将LLM编排器重新用于评估工具编排的安全性，以应对代理系统可能执行恶意工作流的风险。框架通过四个阶段运作：识别潜在不安全工作流、在真实环境中验证它们、生成安全约束以及验证约束的有效性，利用LLM的工具功能知识和执行权限作为自有的安全评估机制。实验证明了AgentGuard的可行性，能够输出评估报告，包括不安全工作流、测试用例和验证过的约束，从而为LLM代理的标准化测试和强化提供基础，提升其在真实世界应用的可靠性。",
      "categories": [
        "cs.CR",
        "cs.AI"
      ],
      "primary_category": "cs.CR",
      "comment": "Project report of AgentGuard in LLM Agent MOOC Hackathon hosted by UC\n  Berkeley in 2024",
      "pdf_url": "http://arxiv.org/pdf/2502.09809v1",
      "published_date": "2025-02-13 23:00:33 UTC",
      "updated_date": "2025-02-13 23:00:33 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T10:45:50.567861"
    },
    {
      "arxiv_id": "2502.09804v1",
      "title": "Acute Lymphoblastic Leukemia Diagnosis Employing YOLOv11, YOLOv8, ResNet50, and Inception-ResNet-v2 Deep Learning Models",
      "title_zh": "翻译失败",
      "authors": [
        "Alaa Awad",
        "Salah A. Aly"
      ],
      "abstract": "Thousands of individuals succumb annually to leukemia alone. As artificial\nintelligence-driven technologies continue to evolve and advance, the question\nof their applicability and reliability remains unresolved. This study aims to\nutilize image processing and deep learning methodologies to achieve\nstate-of-the-art results for the detection of Acute Lymphoblastic Leukemia\n(ALL) using data that best represents real-world scenarios. ALL is one of\nseveral types of blood cancer, and it is an aggressive form of leukemia. In\nthis investigation, we examine the most recent advancements in ALL detection,\nas well as the latest iteration of the YOLO series and its performance. We\naddress the question of whether white blood cells are malignant or benign.\nAdditionally, the proposed models can identify different ALL stages, including\nearly stages. Furthermore, these models can detect hematogones despite their\nfrequent misclassification as ALL. By utilizing advanced deep learning models,\nnamely, YOLOv8, YOLOv11, ResNet50 and Inception-ResNet-v2, the study achieves\naccuracy rates as high as 99.7%, demonstrating the effectiveness of these\nalgorithms across multiple datasets and various real-world situations.",
      "tldr_zh": "本研究旨在利用图像处理和深度学习技术检测急性淋巴细胞白血病 (ALL)，一种侵袭性的血液癌症，并评估这些方法在真实世界场景中的适用性。研究采用了 YOLOv11、YOLOv8、ResNet50 和 Inception-ResNet-v2 模型，能够区分白血细胞的恶性和良性、识别 ALL 的不同阶段（包括早期阶段），并准确检测 hematogones 以避免误分类。结果显示，这些模型在多个数据集上实现了高达 99.7% 的准确率，证明了其在 ALL 诊断中的高效性和可靠性。",
      "categories": [
        "eess.IV",
        "cs.AI",
        "cs.CV",
        "cs.LG"
      ],
      "primary_category": "eess.IV",
      "comment": "12 pages, 28 figures, 5 tables",
      "pdf_url": "http://arxiv.org/pdf/2502.09804v1",
      "published_date": "2025-02-13 22:43:28 UTC",
      "updated_date": "2025-02-13 22:43:28 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T10:46:04.606075"
    },
    {
      "arxiv_id": "2502.09799v1",
      "title": "Co-designing Large Language Model Tools for Project-Based Learning with K12 Educators",
      "title_zh": "翻译失败",
      "authors": [
        "Prerna Ravi",
        "John Masla",
        "Gisella Kakoti",
        "Grace Lin",
        "Emma Anderson",
        "Matt Taylor",
        "Anastasia Ostrowski",
        "Cynthia Breazeal",
        "Eric Klopfer",
        "Hal Abelson"
      ],
      "abstract": "The emergence of generative AI, particularly large language models (LLMs),\nhas opened the door for student-centered and active learning methods like\nproject-based learning (PBL). However, PBL poses practical implementation\nchallenges for educators around project design and management, assessment, and\nbalancing student guidance with student autonomy. The following research\ndocuments a co-design process with interdisciplinary K-12 teachers to explore\nand address the current PBL challenges they face. Through teacher-driven\ninterviews, collaborative workshops, and iterative design of wireframes, we\ngathered evidence for ways LLMs can support teachers in implementing\nhigh-quality PBL pedagogy by automating routine tasks and enhancing\npersonalized learning. Teachers in the study advocated for supporting their\nprofessional growth and augmenting their current roles without replacing them.\nThey also identified affordances and challenges around classroom integration,\nincluding resource requirements and constraints, ethical concerns, and\npotential immediate and long-term impacts. Drawing on these, we propose design\nguidelines for future deployment of LLM tools in PBL.",
      "tldr_zh": "本研究探讨了如何通过与K-12教师合作设计Large Language Model (LLMs)工具来解决Project-Based Learning (PBL)中的实际挑战，如项目设计管理、评估以及平衡学生指导与自主性。研究采用教师驱动的访谈、协作研讨会和迭代设计线框的方法，收集证据以展示LLMs如何自动化常规任务并提升个性化学习，同时支持教师的专业成长而不取代他们的角色。教师们识别了LLMs在课堂整合中的优势和挑战，包括资源需求、伦理问题以及短期与长期影响。最终，论文基于这些发现提出LLMs在PBL中的设计指南，以促进高品质教学实践。",
      "categories": [
        "cs.HC",
        "cs.AI",
        "cs.CY"
      ],
      "primary_category": "cs.HC",
      "comment": "25 pages",
      "pdf_url": "http://arxiv.org/pdf/2502.09799v1",
      "published_date": "2025-02-13 22:23:08 UTC",
      "updated_date": "2025-02-13 22:23:08 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T10:46:15.141828"
    },
    {
      "arxiv_id": "2502.09797v2",
      "title": "A Survey on LLM-based News Recommender Systems",
      "title_zh": "翻译失败",
      "authors": [
        "Rongyao Wang",
        "Veronica Liesaputra",
        "Zhiyi Huang"
      ],
      "abstract": "News recommender systems play a critical role in mitigating the information\noverload problem. In recent years, due to the successful applications of large\nlanguage model technologies, researchers have utilized Discriminative Large\nLanguage Models (DLLMs) or Generative Large Language Models (GLLMs) to improve\nthe performance of news recommender systems. Although several recent surveys\nreview significant challenges for deep learning-based news recommender systems,\nsuch as fairness, privacy-preserving, and responsibility, there is a lack of a\nsystematic survey on Large Language Model (LLM)-based news recommender systems.\nIn order to review different core methodologies and explore potential issues\nsystematically, we categorize DLLM-based and GLLM-based news recommender\nsystems under the umbrella of LLM-based news recommender systems. In this\nsurvey, we first overview the development of deep learning-based news\nrecommender systems. Then, we review LLM-based news recommender systems based\non three aspects: news-oriented modeling, user-oriented modeling, and\nprediction-oriented modeling. Next, we examine the challenges from various\nperspectives, including datasets, benchmarking tools, and methodologies.\nFurthermore, we conduct extensive experiments to analyze how large language\nmodel technologies affect the performance of different news recommender\nsystems. Finally, we comprehensively explore the future directions for\nLLM-based news recommendations in the era of LLMs.",
      "tldr_zh": "这篇调查论文系统审视了基于 Large Language Model (LLM) 的新闻推荐系统，包括 Discriminative LLMs (DLLMs) 和 Generative LLMs (GLLMs)，旨在填补现有深度学习-based 新闻推荐系统调查的空白。论文从新闻导向建模、用户导向建模和预测导向建模三个方面对这些系统进行分类和分析，同时探讨了数据集、基准工具和方法方面的挑战。通过广泛实验，评估了 LLM 技术对新闻推荐系统性能的影响，并提出了未来研究方向，如公平性、隐私保护和责任问题。",
      "categories": [
        "cs.IR",
        "cs.AI"
      ],
      "primary_category": "cs.IR",
      "comment": "20 pages",
      "pdf_url": "http://arxiv.org/pdf/2502.09797v2",
      "published_date": "2025-02-13 22:13:59 UTC",
      "updated_date": "2025-02-21 22:24:23 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T10:46:27.416026"
    },
    {
      "arxiv_id": "2504.05307v1",
      "title": "Toward Total Recall: Enhancing FAIRness through AI-Driven Metadata Standardization",
      "title_zh": "翻译失败",
      "authors": [
        "Sowmya S Sundaram",
        "Mark A Musen"
      ],
      "abstract": "Current metadata often suffer from incompleteness, inconsistency, and\nincorrect formatting, hindering effective data reuse and discovery. Using GPT-4\nand a metadata knowledge base (CEDAR), we devised a method that standardizes\nmetadata in scientific data sets, ensuring the adherence to community\nstandards. The standardization process involves correcting and refining\nmetadata entries to conform to established guidelines, significantly improving\nsearch performance and recall metrics. The investigation uses BioSample and GEO\nrepositories to demonstrate the impact of these enhancements, showcasing how\nstandardized metadata lead to better retrieval outcomes. The average recall\nimproves significantly, rising from 17.65\\% with the baseline raw datasets of\nBioSample and GEO to 62.87\\% with our proposed metadata standardization\npipeline. This finding highlights the transformative impact of integrating\nadvanced AI models with structured metadata curation tools in achieving more\neffective and reliable data retrieval.",
      "tldr_zh": "本文针对元数据的不完整、一致性和格式问题，提出了一种利用 GPT-4 和 CEDAR 知识库的标准化方法，以提升科学数据集的 FAIRness。 该方法通过修正和完善元数据，使其符合社区标准，从而显著改善搜索性能和召回率。 在 BioSample 和 GEO 仓库的实验中，召回率从基线的 17.65% 提升至 62.87%。 这展示了整合高级 AI 模型与结构化元数据工具的潜力，为更可靠的数据检索和重用奠定基础。",
      "categories": [
        "cs.IR",
        "cs.AI"
      ],
      "primary_category": "cs.IR",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.05307v1",
      "published_date": "2025-02-13 21:58:27 UTC",
      "updated_date": "2025-02-13 21:58:27 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T10:46:39.514748"
    },
    {
      "arxiv_id": "2502.09787v1",
      "title": "TableTalk: Scaffolding Spreadsheet Development with a Language Agent",
      "title_zh": "翻译失败",
      "authors": [
        "Jenny T. Liang",
        "Aayush Kumar",
        "Yasharth Bajpai",
        "Sumit Gulwani",
        "Vu Le",
        "Chris Parnin",
        "Arjun Radhakrishna",
        "Ashish Tiwari",
        "Emerson Murphy-Hill",
        "Guastavo Soares"
      ],
      "abstract": "Despite its ubiquity in the workforce, spreadsheet programming remains\nchallenging as programmers need both spreadsheet-specific knowledge (e.g., APIs\nto write formulas) and problem-solving skills to create complex spreadsheets.\nLarge language models (LLMs) can help automate aspects of this process, and\nrecent advances in planning and reasoning have enabled language agents, which\ndynamically plan, use tools, and take iterative actions to complete complex\ntasks. These agents observe, plan, and act, making them well-suited to scaffold\nspreadsheet programming by following expert processes.\n  We present TableTalk, a language agent that helps programmers build\nspreadsheets conversationally. Its design reifies three design principles --\nscaffolding, flexibility, and incrementality -- which we derived from two\nstudies of seven programmers and 62 Excel templates. TableTalk structures\nspreadsheet development by generating step-by-step plans and suggesting three\nnext steps users can choose from. It also integrates tools that enable\nincremental spreadsheet construction. A user study with 20 programmers shows\nthat TableTalk produces spreadsheets 2.3 times more likely to be preferred over\na baseline agent, while reducing cognitive load and time spent reasoning about\nspreadsheet actions by 12.6%. TableTalk's approach has implications for\nhuman-agent collaboration. This includes providing persistent direct\nmanipulation interfaces for stopping or undoing agent actions, while ensuring\nthat such interfaces for accepting actions can be deactivated.",
      "tldr_zh": "这篇论文介绍了TableTalk，一种基于大型语言模型（LLMs）的语言代理，用于通过对话方式辅助电子表格（spreadsheet）开发，解决程序员在特定知识和问题解决技能方面的挑战。TableTalk 遵循scaffolding、flexibility 和 incrementality 的设计原则，生成步步为营的计划、建议三个下一步选项，并集成工具支持增量构建。用户研究显示，与基线代理相比，TableTalk 生成的电子表格被偏好的可能性提高了2.3倍，并减少了12.6%的认知负载和推理时间。该方法为人类-代理协作提供了新启示，包括提供持久的直接操作界面来停止或撤销代理行动。",
      "categories": [
        "cs.SE",
        "cs.AI",
        "cs.HC"
      ],
      "primary_category": "cs.SE",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2502.09787v1",
      "published_date": "2025-02-13 21:43:51 UTC",
      "updated_date": "2025-02-13 21:43:51 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T10:46:51.702225"
    },
    {
      "arxiv_id": "2502.09782v3",
      "title": "Improving Acoustic Side-Channel Attacks on Keyboards Using Transformers and Large Language Models",
      "title_zh": "使用Transformer和大型语言模型改进键盘的声学侧信道攻击",
      "authors": [
        "Jin Hyun Park",
        "Seyyed Ali Ayati",
        "Yichen Cai"
      ],
      "abstract": "The increasing prevalence of microphones in everyday devices and the growing\nreliance on online services have amplified the risk of acoustic side-channel\nattacks (ASCAs) targeting keyboards. This study explores deep learning\ntechniques, specifically vision transformers (VTs) and large language models\n(LLMs), to enhance the effectiveness and applicability of such attacks. We\npresent substantial improvements over prior research, with the CoAtNet model\nachieving state-of-the-art performance. Our CoAtNet shows a 5.0% improvement\nfor keystrokes recorded via smartphone (Phone) and 5.9% for those recorded via\nZoom compared to previous benchmarks. We also evaluate transformer\narchitectures and language models, with the best VT model matching CoAtNet's\nperformance. A key advancement is the introduction of a noise mitigation method\nfor real-world scenarios. By using LLMs for contextual understanding, we detect\nand correct erroneous keystrokes in noisy environments, enhancing ASCA\nperformance. Additionally, fine-tuned lightweight language models with Low-Rank\nAdaptation (LoRA) deliver comparable performance to heavyweight models with 67X\nmore parameters. This integration of VTs and LLMs improves the practical\napplicability of ASCA mitigation, marking the first use of these technologies\nto address ASCAs and error correction in real-world scenarios.",
      "tldr_zh": "这篇论文探讨了使用 Transformers（如视觉变压器 VTs）和 Large Language Models (LLMs) 来提升键盘声学侧通道攻击（ASCAs）的有效性，针对日常设备麦克风的普及所带来的风险。研究中，CoAtNet 模型实现了最先进性能，比先前基准分别提高了 5.0%（智能手机录制）和 5.9%（Zoom 录制），而最佳 VTs 模型与之相当。创新点包括引入噪声缓解方法，利用 LLMs 进行上下文理解检测和纠正噪声环境中的错误 keystrokes，以及通过 Low-Rank Adaptation (LoRA) 微调轻量级模型，达到与大模型相当的性能但参数减少 67 倍，从而提高了 ASCAs 的实际可行性。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL",
        "eess.AS"
      ],
      "primary_category": "cs.LG",
      "comment": "We would like to withdraw our paper due to a significant error in the\n  experimental methodology, which impacts the validity of our results. The\n  error specifically affects the analysis presented in Section 4, where an\n  incorrect dataset preprocessing step led to misleading conclusions",
      "pdf_url": "http://arxiv.org/pdf/2502.09782v3",
      "published_date": "2025-02-13 21:33:57 UTC",
      "updated_date": "2025-02-18 21:39:49 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T10:47:03.515503"
    },
    {
      "arxiv_id": "2502.09780v1",
      "title": "Incentivize without Bonus: Provably Efficient Model-based Online Multi-agent RL for Markov Games",
      "title_zh": "翻译失败",
      "authors": [
        "Tong Yang",
        "Bo Dai",
        "Lin Xiao",
        "Yuejie Chi"
      ],
      "abstract": "Multi-agent reinforcement learning (MARL) lies at the heart of a plethora of\napplications involving the interaction of a group of agents in a shared unknown\nenvironment. A prominent framework for studying MARL is Markov games, with the\ngoal of finding various notions of equilibria in a sample-efficient manner,\nsuch as the Nash equilibrium (NE) and the coarse correlated equilibrium (CCE).\nHowever, existing sample-efficient approaches either require tailored\nuncertainty estimation under function approximation, or careful coordination of\nthe players. In this paper, we propose a novel model-based algorithm, called\nVMG, that incentivizes exploration via biasing the empirical estimate of the\nmodel parameters towards those with a higher collective best-response values of\nall the players when fixing the other players' policies, thus encouraging the\npolicy to deviate from its current equilibrium for more exploration. VMG is\noblivious to different forms of function approximation, and permits\nsimultaneous and uncoupled policy updates of all players. Theoretically, we\nalso establish that VMG achieves a near-optimal regret for finding both the NEs\nof two-player zero-sum Markov games and CCEs of multi-player general-sum Markov\ngames under linear function approximation in an online environment, which\nnearly match their counterparts with sophisticated uncertainty quantification.",
      "tldr_zh": "该论文提出了一种名为 VMG 的模型-based 算法，用于多智能体强化学习 (MARL) 在 Markov Games 中的在线学习，目标是高效找到 Nash equilibrium (NE) 和 coarse correlated equilibrium (CCE)。VMG 通过偏置经验模型参数的估计来激励探索，鼓励策略从当前平衡偏离以增强探索，同时避免使用奖金机制，并允许玩家同时和非耦合地更新策略。理论上，VMG 在线性函数逼近下实现了近优 regret，在两玩家零和 Markov Games 的 NE 和多玩家一般和 Markov Games 的 CCE 上表现与使用复杂不确定性量化的方法相当。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.GT",
        "math.OC"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2502.09780v1",
      "published_date": "2025-02-13 21:28:51 UTC",
      "updated_date": "2025-02-13 21:28:51 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T10:47:15.630711"
    },
    {
      "arxiv_id": "2502.09777v1",
      "title": "On the existence of EFX allocations in multigraphs",
      "title_zh": "多重",
      "authors": [
        "Alkmini Sgouritsa",
        "Minas Marios Sotiriou"
      ],
      "abstract": "We study the problem of \"fairly\" dividing indivisible goods to several agents\nthat have valuation set functions over the sets of goods. As fair we consider\nthe allocations that are envy-free up to any good (EFX), i.e., no agent envies\nany proper subset of the goods given to any other agent. The existence or not\nof EFX allocations is a major open problem in Fair Division, and there are only\npositive results for special cases.\n  [George Christodoulou, Amos Fiat, Elias Koutsoupias, Alkmini Sgouritsa 2023]\nintroduced a restriction on the agents' valuations according to a graph\nstructure: the vertices correspond to agents and the edges to goods, and each\nvertex/agent has zero marginal value (or in other words, they are indifferent)\nfor the edges/goods that are not adjacent to them. The existence of EFX\nallocations has been shown for simple graphs with general monotone valuations\n[George Christodoulou, Amos Fiat, Elias Koutsoupias, Alkmini Sgouritsa 2023],\nand for multigraphs for restricted additive valuations [Alireza Kaviani, Masoud\nSeddighin, Amir Mohammad Shahrezaei 2024].\n  In this work, we push the state-of-the-art further, and show that the EFX\nallocations always exists in multigraphs and general monotone valuations if any\nof the following three conditions hold: either (a) the multigraph is bipartite,\nor (b) each agent has at most $\\lceil \\frac{n}{4} \\rceil -1$ neighbors, where\n$n$ is the total number of agents, or (c) the shortest cycle with non-parallel\nedges has length at least 6.",
      "tldr_zh": "本研究探讨了在多图(multigraphs)中公平分配不可分割物品的问题，焦点在于EFX(Envy-Free up to Any Good)分配的存在性，即确保没有代理人对其他代理人的任何适当子集感到嫉妒。论文证明，对于一般单调估值，如果多图满足以下条件之一——(a) 是二分图(bipartite)、(b) 每个代理人有最多⌈n/4⌉-1个邻居（n为代理人总数）、或(c) 最短非平行边循环长度至少6——则EFX分配总是存在。这扩展了先前在简单图上的一般单调估值结果，以及在多图上受限加法估值的结果。",
      "categories": [
        "cs.GT",
        "cs.AI"
      ],
      "primary_category": "cs.GT",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2502.09777v1",
      "published_date": "2025-02-13 21:16:27 UTC",
      "updated_date": "2025-02-13 21:16:27 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T10:47:27.773776"
    },
    {
      "arxiv_id": "2502.15761v1",
      "title": "LoXR: Performance Evaluation of Locally Executing LLMs on XR Devices",
      "title_zh": "翻译失败",
      "authors": [
        "Dawar Khan",
        "Xinyu Liu",
        "Omar Mena",
        "Donggang Jia",
        "Alexandre Kouyoumdjian",
        "Ivan Viola"
      ],
      "abstract": "The deployment of large language models (LLMs) on extended reality (XR)\ndevices has great potential to advance the field of human-AI interaction. In\nthe case of direct, on-device model inference, selecting the appropriate model\nand device for specific tasks remains challenging. In this paper, we deploy 17\nLLMs across four XR devices--Magic Leap 2, Meta Quest 3, Vivo X100s Pro, and\nApple Vision Pro, and conduct a comprehensive evaluation. We devise an\nexperimental setup and evaluate performance on four key metrics: performance\nconsistency, processing speed, memory usage, and battery consumption. For each\nof the 68 model-device pairs, we assess performance under varying string\nlengths, batch sizes, and thread counts, analyzing the trade-offs for real-time\nXR applications. We finally propose a unified evaluation method based on the\nPareto Optimality theory to select the optimal device-model pairs from the\nquality and speed objectives. We believe our findings offer valuable insights\nto guide future optimization efforts for LLM deployment on XR devices. Our\nevaluation method can be followed as standard groundwork for further research\nand development in this emerging field. All supplemental materials are\navailable at www.nanovis.org/Loxr.html.",
      "tldr_zh": "这篇论文评估了在扩展现实（XR）设备上本地执行大型语言模型（LLMs）的性能，部署了17个LLMs到Magic Leap 2、Meta Quest 3、Vivo X100s Pro和Apple Vision Pro等四种设备上。研究通过实验设置，评估了性能一致性、处理速度、内存使用和电池消耗等四个关键指标，并在不同字符串长度、批量大小和线程数下分析了68个模型-设备对的权衡。论文提出了基于Pareto Optimality理论的统一评估方法，以选择最优设备-模型组合。最终，该工作为LLMs在XR设备上的优化提供了指导性见解，并建立了标准评估框架以支持未来研究。",
      "categories": [
        "cs.DC",
        "cs.AI",
        "cs.GR",
        "cs.HC"
      ],
      "primary_category": "cs.DC",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2502.15761v1",
      "published_date": "2025-02-13 20:55:48 UTC",
      "updated_date": "2025-02-13 20:55:48 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T10:47:39.550516"
    },
    {
      "arxiv_id": "2502.09767v1",
      "title": "Non-Markovian Discrete Diffusion with Causal Language Models",
      "title_zh": "翻译失败",
      "authors": [
        "Yangtian Zhang",
        "Sizhuang He",
        "Daniel Levine",
        "Lawrence Zhao",
        "David Zhang",
        "Syed A Rizvi",
        "Emanuele Zappala",
        "Rex Ying",
        "David van Dijk"
      ],
      "abstract": "Discrete diffusion models have emerged as a flexible and controllable\nparadigm for structured sequence modeling, yet they still lag behind causal\nlanguage models in expressiveness. To bridge the gap between two paradigms, we\nintroduce CaDDi, a causal discrete diffusion model that unifies sequential and\ntemporal modeling within a non-Markovian diffusion framework. Unlike\nconventional diffusion models that operate step by step with no access to prior\nstates, CaDDi integrates the temporal trajectory, enabling more expressive and\ncontrollable generation. Our approach also treats causal language models as a\nspecial case, allowing seamless adoption of pretrained large language models\n(LLMs) for discrete diffusion without the need for architectural modifications.\nEmpirically, we demonstrate that CaDDi outperforms state-of-the-art discrete\ndiffusion models on both natural language and biological sequence tasks,\nnarrowing the gap between diffusion-based methods and large-scale\nautoregressive transformers.",
      "tldr_zh": "本研究引入CaDDi，一种非Markovian Discrete Diffusion模型，旨在提升离散扩散模型在序列建模中的表现力，并桥接其与因果语言模型的差距。\nCaDDi通过整合时间轨迹和非Markovian框架，实现更具表达性和可控性的生成过程，同时允许无缝采用预训练的大型语言模型(LLMs)而无需架构修改。\n实验结果表明，CaDDi在自然语言和生物序列任务上超越了现有离散扩散模型，显著缩小了与自回归Transformer的性能差距。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.LG",
      "comment": "Under Review",
      "pdf_url": "http://arxiv.org/pdf/2502.09767v1",
      "published_date": "2025-02-13 20:51:25 UTC",
      "updated_date": "2025-02-13 20:51:25 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T10:47:50.808354"
    },
    {
      "arxiv_id": "2502.09765v2",
      "title": "Differential Adjusted Parity for Learning Fair Representations",
      "title_zh": "翻译失败",
      "authors": [
        "Bucher Sahyouni",
        "Matthew Vowels",
        "Liqun Chen",
        "Simon Hadfield"
      ],
      "abstract": "The development of fair and unbiased machine learning models remains an\nongoing objective for researchers in the field of artificial intelligence. We\nintroduce the Differential Adjusted Parity (DAP) loss to produce unbiased\ninformative representations. It utilises a differentiable variant of the\nadjusted parity metric to create a unified objective function. By combining\ndownstream task classification accuracy and its inconsistency across sensitive\nfeature domains, it provides a single tool to increase performance and mitigate\nbias. A key element in this approach is the use of soft balanced accuracies. In\ncontrast to previous non-adversarial approaches, DAP does not suffer a\ndegeneracy where the metric is satisfied by performing equally poorly across\nall sensitive domains. It outperforms several adversarial models on downstream\ntask accuracy and fairness in our analysis. Specifically, it improves the\ndemographic parity, equalized odds and sensitive feature accuracy by as much as\n22.5\\%, 44.1\\% and 40.1\\%, respectively, when compared to the best performing\nadversarial approaches on these metrics. Overall, the DAP loss and its\nassociated metric can play a significant role in creating more fair machine\nlearning models.",
      "tldr_zh": "这篇论文提出了Differential Adjusted Parity (DAP) loss函数，用于学习公平的机器学习表示，以缓解模型在敏感特征（如种族或性别）上的偏见。DAP通过可微的adjusted parity metric结合下游任务的分类准确性和跨域不一致性，创建了一个统一的优化目标，并利用soft balanced accuracies避免了传统非对抗方法中表现一致但整体糟糕的退化问题。与对抗模型相比，实验结果显示DAP在demographic parity、equalized odds和sensitive feature accuracy上分别提高了22.5%、44.1%和40.1%，从而显著提升了模型的性能和公平性。总的来说，该方法为构建更公正的AI模型提供了重要工具。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2502.09765v2",
      "published_date": "2025-02-13 20:48:26 UTC",
      "updated_date": "2025-04-09 13:19:22 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T10:48:03.422184"
    },
    {
      "arxiv_id": "2502.09762v2",
      "title": "AT-Drone: Benchmarking Adaptive Teaming in Multi-Drone Pursuit",
      "title_zh": "翻译失败",
      "authors": [
        "Yang Li",
        "Junfan Chen",
        "Feng Xue",
        "Jiabin Qiu",
        "Wenbin Li",
        "Qingrui Zhang",
        "Ying Wen",
        "Wei Pan"
      ],
      "abstract": "Adaptive teaming-the capability of agents to effectively collaborate with\nunfamiliar teammates without prior coordination-is widely explored in virtual\nvideo games but overlooked in real-world multi-robot contexts. Yet, such\nadaptive collaboration is crucial for real-world applications, including border\nsurveillance, search-and-rescue, and counter-terrorism operations. To address\nthis gap, we introduce AT-Drone, the first dedicated benchmark explicitly\ndesigned to facilitate comprehensive training and evaluation of adaptive\nteaming strategies in multi-drone pursuit scenarios. AT-Drone makes the\nfollowing key contributions: (1) An adaptable simulation environment\nconfigurator that enables intuitive and rapid setup of adaptive teaming\nmulti-drone pursuit tasks, including four predefined pursuit environments. (2)\nA streamlined real-world deployment pipeline that seamlessly translates\nsimulation insights into practical drone evaluations using edge devices and\nCrazyflie drones. (3) A novel algorithm zoo integrated with a distributed\ntraining framework, featuring diverse algorithms explicitly tailored, for the\nfirst time, to multi-pursuer and multi-evader settings. (4) Standardized\nevaluation protocols with newly designed unseen drone zoos, explicitly designed\nto rigorously assess the performance of adaptive teaming. Comprehensive\nexperimental evaluations across four progressively challenging multi-drone\npursuit scenarios confirm AT-Drone's effectiveness in advancing adaptive\nteaming research. Real-world drone experiments further validate its practical\nfeasibility and utility for realistic robotic operations. Videos, code and\nweights are available at \\url{https://sites.google.com/view/at-drone}.",
      "tldr_zh": "该研究引入了 AT-Drone，这是一个专为多无人机追逐场景设计的基准测试框架，旨在评估 adaptive teaming（代理在无事先协调下与陌生队友有效协作）的性能，以应用于真实世界任务如边境监视和搜救。AT-Drone 的关键贡献包括：一个可适应模拟环境配置器（支持四个预定义环境）、一个无缝的模拟到真实部署管道（使用边缘设备和 Crazyflie 无人机）、一个针对多追逐者和多逃避者设置的算法集合与分布式训练框架，以及标准化评估协议以测试未见无人机组合。实验在四个渐进挑战场景中验证了框架的有效性，并在真实无人机测试中证明了其实际可行性，从而推进了 adaptive teaming 在机器人领域的应用。",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "comment": "18 pages",
      "pdf_url": "http://arxiv.org/pdf/2502.09762v2",
      "published_date": "2025-02-13 20:45:48 UTC",
      "updated_date": "2025-05-02 10:33:06 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T10:48:15.266312"
    },
    {
      "arxiv_id": "2502.09757v1",
      "title": "The AI-Therapist Duo: Exploring the Potential of Human-AI Collaboration in Personalized Art Therapy for PICS Intervention",
      "title_zh": "AI-治疗师搭档：探索人类-A",
      "authors": [
        "Bereket A. Yilma",
        "Chan Mi Kim",
        "Geke Ludden",
        "Thomas van Rompay",
        "Luis A. Leiva"
      ],
      "abstract": "Post-intensive care syndrome (PICS) is a multifaceted condition that arises\nfrom prolonged stays in an intensive care unit (ICU). While preventing PICS\namong ICU patients is becoming increasingly important, interventions remain\nlimited. Building on evidence supporting the effectiveness of art exposure in\naddressing the psychological aspects of PICS, we propose a novel art therapy\nsolution through a collaborative Human-AI approach that enhances personalized\ntherapeutic interventions using state-of-the-art Visual Art Recommendation\nSystems. We developed two Human-in-the-Loop (HITL) personalization methods and\nassessed their impact through a large-scale user study (N=150). Our findings\ndemonstrate that this Human-AI collaboration not only enhances the\npersonalization and effectiveness of art therapy but also supports therapists\nby streamlining their workload. While our study centres on PICS intervention,\nthe results suggest that human-AI collaborative Art therapy could potentially\nbenefit other areas where emotional support is critical, such as cases of\nanxiety and depression.",
      "tldr_zh": "该研究探讨了人类-AI 协作在个性化艺术疗法中的潜力，针对后重症监护综合征 (PICS) 患者提出一种新型解决方案，利用先进的 Visual Art Recommendation Systems 增强疗效。研究开发了两种 Human-in-the-Loop (HITL) 个性化方法，并通过大规模用户研究 (N=150) 评估，结果显示这种协作显著提高了艺术疗法的个性化水平和有效性，同时减轻了治疗师的工作负担。总体而言，该方法不仅适用于 PICS 干预，还可能扩展到其他情绪支持领域，如焦虑和抑郁的治疗。",
      "categories": [
        "cs.HC",
        "cs.AI"
      ],
      "primary_category": "cs.HC",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2502.09757v1",
      "published_date": "2025-02-13 20:31:28 UTC",
      "updated_date": "2025-02-13 20:31:28 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T10:48:27.076256"
    },
    {
      "arxiv_id": "2502.09749v1",
      "title": "Vote-Tree-Planner: Optimizing Execution Order in LLM-based Task Planning Pipeline via Voting",
      "title_zh": "翻译失败",
      "authors": [
        "Chaoyuan Zhang",
        "Zhaowei Li",
        "Wentao Yuan"
      ],
      "abstract": "Integrating large language models (LLMs) into closed-loop robotic task\nplanning has become increasingly popular within embodied artificial\nintelligence. Previous efforts mainly focused on leveraging the strong\nreasoning abilities of LLMs to enhance task planning performance while often\noverlooking task planning efficiency and executability due to repetitive\nqueries to LLMs. This paper addresses the synergy between LLMs and task\nplanning systems, aiming to minimize redundancy while enhancing planning\neffectiveness. Specifically, building upon Prog-Prompt and the high-level\nconcept of Tree-Planner, we propose Vote-Tree-Planner. This sampling strategy\nutilizes votes to guide plan traversal during the decision-making process. Our\napproach is motivated by a straightforward observation: assigning weights to\nagents during decision-making enables the evaluation of critical paths before\nexecution. With this simple vote-tree construction, our method further improves\nthe success rate and reduces the number of queries to LLMs. The experimental\nresults highlight that our Vote-Tree-Planner demonstrates greater stability and\nshows a higher average success rate and goal condition recall on the unseen\ndataset compared with previous baseline methods. These findings underscore the\npotential of the Vote-Tree-Planner to enhance planning accuracy, reliability,\nand efficiency in LLM-based planning systems.",
      "tldr_zh": "本文提出 Vote-Tree-Planner，一种通过投票机制优化 LLM-based 任务规划管道执行顺序的方法，旨在减少对大型语言模型 (LLMs) 的重复查询并提升规划效率。基于 Prog-Prompt 和 Tree-Planner，该框架使用投票分配权重来指导计划遍历和关键路径评估，从而提高任务的可执行性和决策准确性。实验结果显示，Vote-Tree-Planner 在未见数据集上比基线方法平均成功率和目标条件召回率更高，同时展示了更大的稳定性和整体可靠性。",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "comment": "Accepted to RSS24-W: TaskSpec",
      "pdf_url": "http://arxiv.org/pdf/2502.09749v1",
      "published_date": "2025-02-13 20:08:06 UTC",
      "updated_date": "2025-02-13 20:08:06 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T10:48:38.649861"
    },
    {
      "arxiv_id": "2502.12173v1",
      "title": "nanoML for Human Activity Recognition",
      "title_zh": "nanoML 用于人类活动识别",
      "authors": [
        "Alan T. L. Bacellar",
        "Mugdha P. Jadhao",
        "Shashank Nag",
        "Priscila M. V. Lima",
        "Felipe M. G. Franca",
        "Lizy K. John"
      ],
      "abstract": "Human Activity Recognition (HAR) is critical for applications in healthcare,\nfitness, and IoT, but deploying accurate models on resource-constrained devices\nremains challenging due to high energy and memory demands. This paper\ndemonstrates the application of Differentiable Weightless Neural Networks\n(DWNs) to HAR, achieving competitive accuracies of 96.34% and 96.67% while\nconsuming only 56nJ and 104nJ per sample, with an inference time of just 5ns\nper sample. The DWNs were implemented and evaluated on an FPGA, showcasing\ntheir practical feasibility for energy-efficient hardware deployment. DWNs\nachieve up to 926,000x energy savings and 260x memory reduction compared to\nstate-of-the-art deep learning methods. These results position DWNs as a\nnano-machine learning nanoML model for HAR, setting a new benchmark in energy\nefficiency and compactness for edge and wearable devices, paving the way for\nultra-efficient edge AI.",
      "tldr_zh": "这篇论文探讨了人类活动识别(HAR)在医疗、健身和IoT应用中的重要性，以及在资源受限设备上部署准确模型的挑战，提出使用Differentiable Weightless Neural Networks (DWNs)作为高效解决方案。DWNs在HAR任务中实现了96.34%和96.67%的准确率，同时仅消耗56nJ和104nJ per sample的能量，并将推理时间控制在5ns per sample。相比传统深度学习方法，DWNs实现了高达926,000x的能效节约和260x的内存减少，在FPGA上验证了其实际可行性，为边缘和可穿戴设备的超高效AI应用树立了新基准。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "Accepted as a full paper by the 2025 EDGE AI FOUNDATION Austin",
      "pdf_url": "http://arxiv.org/pdf/2502.12173v1",
      "published_date": "2025-02-13 19:40:03 UTC",
      "updated_date": "2025-02-13 19:40:03 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T10:48:52.401956"
    },
    {
      "arxiv_id": "2502.09731v1",
      "title": "A CNN Approach to Automated Detection and Classification of Brain Tumors",
      "title_zh": "基于 CNN 的脑肿瘤自动化检测和分类方法",
      "authors": [
        "Md. Zahid Hasan",
        "Abdullah Tamim",
        "D. M. Asadujjaman",
        "Md. Mahfujur Rahman",
        "Md. Abu Ahnaf Mollick",
        "Nosin Anjum Dristi",
        "Abdullah-Al-Noman"
      ],
      "abstract": "Brain tumors require an assessment to ensure timely diagnosis and effective\npatient treatment. Morphological factors such as size, location, texture, and\nvariable appearance complicate tumor inspection. Medical imaging presents\nchallenges, including noise and incomplete images. This research article\npresents a methodology for processing Magnetic Resonance Imaging (MRI) data,\nencompassing techniques for image classification and denoising. The effective\nuse of MRI images allows medical professionals to detect brain disorders,\nincluding tumors. This research aims to categorize healthy brain tissue and\nbrain tumors by analyzing the provided MRI data. Unlike alternative methods\nlike Computed Tomography (CT), MRI technology offers a more detailed\nrepresentation of internal anatomical components, making it a suitable option\nfor studying data related to brain tumors. The MRI picture is first subjected\nto a denoising technique utilizing an Anisotropic diffusion filter. The dataset\nutilized for the models creation is a publicly accessible and validated Brain\nTumour Classification (MRI) database, comprising 3,264 brain MRI scans. SMOTE\nwas employed for data augmentation and dataset balancing. Convolutional Neural\nNetworks(CNN) such as ResNet152V2, VGG, ViT, and EfficientNet were employed for\nthe classification procedure. EfficientNet attained an accuracy of 98%, the\nhighest recorded.",
      "tldr_zh": "这篇论文提出了一种基于CNN的自动脑肿瘤检测和分类方法，使用MRI图像来处理脑肿瘤的形态复杂性和图像噪声问题。方法包括先采用Anisotropic diffusion filter进行图像去噪，结合SMOTE技术进行数据增强和平衡，然后利用多种CNN模型如ResNet152V2、VGG、ViT和EfficientNet进行分类。实验结果显示EfficientNet模型取得了98%的准确率，相比CT扫描，MRI提供了更详细的解剖信息，从而提升了脑肿瘤诊断的精确性。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "68T07",
        "J.3"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2502.09731v1",
      "published_date": "2025-02-13 19:33:26 UTC",
      "updated_date": "2025-02-13 19:33:26 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T10:49:02.800979"
    },
    {
      "arxiv_id": "2502.10477v1",
      "title": "Knowledge Integration Strategies in Autonomous Vehicle Prediction and Planning: A Comprehensive Survey",
      "title_zh": "自动驾驶车辆预测和规划中的知识整合策略：一项全面综述",
      "authors": [
        "Kumar Manas",
        "Adrian Paschke"
      ],
      "abstract": "This comprehensive survey examines the integration of knowledge-based\napproaches into autonomous driving systems, with a focus on trajectory\nprediction and planning. We systematically review methodologies for\nincorporating domain knowledge, traffic rules, and commonsense reasoning into\nthese systems, spanning purely symbolic representations to hybrid\nneuro-symbolic architectures. In particular, we analyze recent advancements in\nformal logic and differential logic programming, reinforcement learning\nframeworks, and emerging techniques that leverage large foundation models and\ndiffusion models for knowledge representation. Organized under a unified\nliterature survey section, our discussion synthesizes the state-of-the-art into\na high-level overview, supported by a detailed comparative table that maps key\nworks to their respective methodological categories. This survey not only\nhighlights current trends -- including the growing emphasis on interpretable\nAI, formal verification in safety-critical systems, and the increased use of\ngenerative models in prediction and planning -- but also outlines the\nchallenges and opportunities for developing robust, knowledge-enhanced\nautonomous driving systems.",
      "tldr_zh": "这篇论文对自动驾驶系统中知识整合策略进行了全面调查，重点关注轨迹预测和规划模块。论文系统回顾了将领域知识、交通规则以及常识推理融入系统的多种方法，包括纯符号表示、混合神经符号架构、形式逻辑、微分逻辑编程、强化学习框架，以及利用 large foundation models 和 diffusion models 的新兴技术。调查还通过统一文献概述和比较表，突出了当前趋势如 interpretable AI、formal verification在安全关键系统中的应用，以及生成模型在预测和规划中的作用，并讨论了构建稳健知识增强自动驾驶系统的挑战和机遇。",
      "categories": [
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2502.10477v1",
      "published_date": "2025-02-13 19:32:41 UTC",
      "updated_date": "2025-02-13 19:32:41 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T10:49:16.832776"
    },
    {
      "arxiv_id": "2502.09723v2",
      "title": "Making Them a Malicious Database: Exploiting Query Code to Jailbreak Aligned Large Language Models",
      "title_zh": "翻译失败",
      "authors": [
        "Qingsong Zou",
        "Jingyu Xiao",
        "Qing Li",
        "Zhi Yan",
        "Yuhang Wang",
        "Li Xu",
        "Wenxuan Wang",
        "Kuofeng Gao",
        "Ruoyu Li",
        "Yong Jiang"
      ],
      "abstract": "Recent advances in large language models (LLMs) have demonstrated remarkable\npotential in the field of natural language processing. Unfortunately, LLMs face\nsignificant security and ethical risks. Although techniques such as safety\nalignment are developed for defense, prior researches reveal the possibility of\nbypassing such defenses through well-designed jailbreak attacks. In this paper,\nwe propose QueryAttack, a novel framework to examine the generalizability of\nsafety alignment. By treating LLMs as knowledge databases, we translate\nmalicious queries in natural language into structured non-natural query\nlanguage to bypass the safety alignment mechanisms of LLMs. We conduct\nextensive experiments on mainstream LLMs, and the results show that QueryAttack\nnot only can achieve high attack success rates (ASRs), but also can jailbreak\nvarious defense methods. Furthermore, we tailor a defense method against\nQueryAttack, which can reduce ASR by up to 64% on GPT-4-1106. Our code is\navailable at https://github.com/horizonsinzqs/QueryAttack.",
      "tldr_zh": "本论文提出QueryAttack框架，通过将大型语言模型(LLMs)视为知识数据库，将恶意查询从自然语言翻译成结构化的非自然查询语言，从而绕过LLMs的安全对齐机制。\n该方法旨在测试安全对齐的泛化性，并在主流LLMs上进行实验，实现了高攻击成功率(ASRs)并成功越狱多种防御策略。\n此外，论文还开发了一个针对QueryAttack的防御方法，能将GPT-4-1106的ASR降低多达64%，为提升LLMs的安全性提供新思路。",
      "categories": [
        "cs.CR",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.CR",
      "comment": "15 pages, 11 figures",
      "pdf_url": "http://arxiv.org/pdf/2502.09723v2",
      "published_date": "2025-02-13 19:13:03 UTC",
      "updated_date": "2025-02-20 07:19:05 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T10:49:27.267015"
    },
    {
      "arxiv_id": "2502.09720v1",
      "title": "NestQuant: Nested Lattice Quantization for Matrix Products and LLMs",
      "title_zh": "翻译失败",
      "authors": [
        "Semyon Savkin",
        "Eitan Porat",
        "Or Ordentlich",
        "Yury Polyanskiy"
      ],
      "abstract": "Post-training quantization (PTQ) has emerged as a critical technique for\nefficient deployment of large language models (LLMs). This work proposes\nNestQuant, a novel PTQ scheme for weights and activations that is based on\nself-similar nested lattices. Recent work have mathematically shown such\nquantizers to be information-theoretically optimal for low-precision matrix\nmultiplication. We implement a practical low-complexity version of NestQuant\nbased on Gosset lattice, making it a drop-in quantizer for any matrix\nmultiplication step (e.g., in self-attention, MLP etc). For example, NestQuant\nquantizes weights, KV-cache, and activations of Llama-3-8B to 4 bits, achieving\nperplexity of 6.6 on wikitext2. This represents more than 55% reduction in\nperplexity gap with respect to unquantized model (perplexity of 6.14) compared\nto state-of-the-art Meta's SpinQuant (perplexity 7.3). Comparisons on various\nLLM evaluation benchmarks also show a reduction in performance degradation\ninduced by quantization.",
      "tldr_zh": "这篇论文提出了 NestQuant，一种基于自相似嵌套晶格的创新后训练量化 (PTQ) 方案，用于大型语言模型 (LLMs) 的权重和激活，以优化矩阵乘法效率。NestQuant 利用 Gosset 晶格实现低复杂度版本，可作为矩阵乘法（如自注意力或 MLP）的即插即用量化器，并被数学证明在低精度场景中信息理论最优。实验结果显示，将 Llama-3-8B 量化到 4 位后，在 wikitext2 上达到 6.6 的 perplexity，与未量化模型相比减少超过 55% 的性能差距，并优于现有方法如 Meta 的 SpinQuant，在各种 LLM 基准测试中显著降低量化引起的性能退化。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.IT",
        "math.IT"
      ],
      "primary_category": "cs.LG",
      "comment": "16 pages",
      "pdf_url": "http://arxiv.org/pdf/2502.09720v1",
      "published_date": "2025-02-13 19:11:40 UTC",
      "updated_date": "2025-02-13 19:11:40 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T10:49:40.648009"
    },
    {
      "arxiv_id": "2502.09715v1",
      "title": "Evaluating GPT's Capability in Identifying Stages of Cognitive Impairment from Electronic Health Data",
      "title_zh": "评估 GPT 在从电子健康数据中识别认知障碍阶段的能力",
      "authors": [
        "Yu Leng",
        "Yingnan He",
        "Colin Magdamo",
        "Ana-Maria Vranceanu",
        "Christine S. Ritchie",
        "Shibani S. Mukerji",
        "Lidia M. V. R. Moura",
        "John R. Dickson",
        "Deborah Blacker",
        "Sudeshna Das"
      ],
      "abstract": "Identifying cognitive impairment within electronic health records (EHRs) is\ncrucial not only for timely diagnoses but also for facilitating research.\nInformation about cognitive impairment often exists within unstructured\nclinician notes in EHRs, but manual chart reviews are both time-consuming and\nerror-prone. To address this issue, our study evaluates an automated approach\nusing zero-shot GPT-4o to determine stage of cognitive impairment in two\ndifferent tasks. First, we evaluated the ability of GPT-4o to determine the\nglobal Clinical Dementia Rating (CDR) on specialist notes from 769 patients who\nvisited the memory clinic at Massachusetts General Hospital (MGH), and achieved\na weighted kappa score of 0.83. Second, we assessed GPT-4o's ability to\ndifferentiate between normal cognition, mild cognitive impairment (MCI), and\ndementia on all notes in a 3-year window from 860 Medicare patients. GPT-4o\nattained a weighted kappa score of 0.91 in comparison to specialist chart\nreviews and 0.96 on cases that the clinical adjudicators rated with high\nconfidence. Our findings demonstrate GPT-4o's potential as a scalable chart\nreview tool for creating research datasets and assisting diagnosis in clinical\nsettings in the future.",
      "tldr_zh": "该研究评估了零样本 GPT-4o 在从电子健康记录(EHRs)中识别认知障碍阶段的能力，旨在解决手动审查的耗时和易出错问题。\n在第一个任务中，GPT-4o 对769名患者（MGH 记忆诊所）的专家笔记确定全球 Clinical Dementia Rating (CDR)，取得了0.83的加权 Kappa 分数。\n在第二个任务中，它对860名 Medicare 患者的3年窗口笔记区分正常认知、轻度认知障碍(MCI)和痴呆，与专家审查相比，达到了0.91的加权 Kappa 分数，并在高置信度病例中达至0.96。\n总体发现显示，GPT-4o 具有作为可扩展工具的潜力，用于创建研究数据集和辅助临床诊断。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.LG",
      "comment": "Findings paper presented at Machine Learning for Health (ML4H)\n  symposium 2024, December 15-16, 2024, Vancouver, Canada, 7 pages",
      "pdf_url": "http://arxiv.org/pdf/2502.09715v1",
      "published_date": "2025-02-13 19:04:47 UTC",
      "updated_date": "2025-02-13 19:04:47 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T10:49:53.047893"
    },
    {
      "arxiv_id": "2502.10476v1",
      "title": "Multi-Objective Planning with Contextual Lexicographic Reward Preferences",
      "title_zh": "基于上下文词典式奖励偏好的多目标规划",
      "authors": [
        "Pulkit Rustagi",
        "Yashwanthi Anand",
        "Sandhya Saisubramanian"
      ],
      "abstract": "Autonomous agents are often required to plan under multiple objectives whose\npreference ordering varies based on context. The agent may encounter multiple\ncontexts during its course of operation, each imposing a distinct lexicographic\nordering over the objectives, with potentially different reward functions\nassociated with each context. Existing approaches to multi-objective planning\ntypically consider a single preference ordering over the objectives, across the\nstate space, and do not support planning under multiple objective orderings\nwithin an environment. We present Contextual Lexicographic Markov Decision\nProcess (CLMDP), a framework that enables planning under varying lexicographic\nobjective orderings, depending on the context. In a CLMDP, both the objective\nordering at a state and the associated reward functions are determined by the\ncontext. We employ a Bayesian approach to infer a state-context mapping from\nexpert trajectories. Our algorithm to solve a CLMDP first computes a policy for\neach objective ordering and then combines them into a single context-aware\npolicy that is valid and cycle-free. The effectiveness of the proposed approach\nis evaluated in simulation and using a mobile robot.",
      "tldr_zh": "该论文提出 Contextual Lexicographic Markov Decision Process (CLMDP) 框架，用于处理自治代理在多目标规划中的上下文相关问题，每个上下文可能有不同的词汇优先级（lexicographic）顺序和奖励函数。研究采用贝叶斯方法从专家轨迹中推断状态-上下文映射，然后计算每个目标顺序的政策，并将其组合成一个有效的、无循环的上下文感知政策。实验结果显示，该方法在模拟环境和移动机器人应用中表现出色，解决了现有多目标规划方法无法支持多个目标顺序的局限性。",
      "categories": [
        "cs.AI",
        "cs.RO",
        "cs.SY"
      ],
      "primary_category": "cs.AI",
      "comment": "9 pages, 5 figures, 2 tables, To appear in Proceedings of the 24th\n  International Conference on Autonomous Agents and Multiagent Systems (AAMAS)\n  2025",
      "pdf_url": "http://arxiv.org/pdf/2502.10476v1",
      "published_date": "2025-02-13 19:04:22 UTC",
      "updated_date": "2025-02-13 19:04:22 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T10:50:03.296879"
    },
    {
      "arxiv_id": "2502.09622v1",
      "title": "Theoretical Benefit and Limitation of Diffusion Language Model",
      "title_zh": "翻译失败",
      "authors": [
        "Guhao Feng",
        "Yihan Geng",
        "Jian Guan",
        "Wei Wu",
        "Liwei Wang",
        "Di He"
      ],
      "abstract": "Diffusion language models have emerged as a promising approach for text\ngeneration. One would naturally expect this method to be an efficient\nreplacement for autoregressive models since multiple tokens can be sampled in\nparallel during each diffusion step. However, its efficiency-accuracy trade-off\nis not yet well understood. In this paper, we present a rigorous theoretical\nanalysis of a widely used type of diffusion language model, the Masked\nDiffusion Model (MDM), and find that its effectiveness heavily depends on the\ntarget evaluation metric. Under mild conditions, we prove that when using\nperplexity as the metric, MDMs can achieve near-optimal perplexity in sampling\nsteps regardless of sequence length, demonstrating that efficiency can be\nachieved without sacrificing performance. However, when using the sequence\nerror rate--which is important for understanding the \"correctness\" of a\nsequence, such as a reasoning chain--we show that the required sampling steps\nmust scale linearly with sequence length to obtain \"correct\" sequences, thereby\neliminating MDM's efficiency advantage over autoregressive models. Our analysis\nestablishes the first theoretical foundation for understanding the benefits and\nlimitations of MDMs. All theoretical findings are supported by empirical\nstudies.",
      "tldr_zh": "本研究对扩散语言模型(Diffusion Language Models)的理论优势和局限性进行了分析，重点考察了Masked Diffusion Model (MDM)在文本生成中的效率-准确性权衡。研究证明，在perplexity指标下，MDM可在采样步骤与序列长度无关的情况下实现近似最优性能，从而展示其高效潜力。然而，在sequence error rate指标下，MDM需要采样步骤线性增加以生成正确序列，这导致其效率优势不如自回归模型。总体而言，该工作建立了MDM的首个理论基础，并通过实证研究验证了这些发现。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL",
        "stat.ML"
      ],
      "primary_category": "cs.LG",
      "comment": "32 pages, 3 figures",
      "pdf_url": "http://arxiv.org/pdf/2502.09622v1",
      "published_date": "2025-02-13 18:59:47 UTC",
      "updated_date": "2025-02-13 18:59:47 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T10:50:15.264609"
    },
    {
      "arxiv_id": "2502.09621v1",
      "title": "MME-CoT: Benchmarking Chain-of-Thought in Large Multimodal Models for Reasoning Quality, Robustness, and Efficiency",
      "title_zh": "翻译失败",
      "authors": [
        "Dongzhi Jiang",
        "Renrui Zhang",
        "Ziyu Guo",
        "Yanwei Li",
        "Yu Qi",
        "Xinyan Chen",
        "Liuhui Wang",
        "Jianhan Jin",
        "Claire Guo",
        "Shen Yan",
        "Bo Zhang",
        "Chaoyou Fu",
        "Peng Gao",
        "Hongsheng Li"
      ],
      "abstract": "Answering questions with Chain-of-Thought (CoT) has significantly enhanced\nthe reasoning capabilities of Large Language Models (LLMs), yet its impact on\nLarge Multimodal Models (LMMs) still lacks a systematic assessment and in-depth\ninvestigation. In this paper, we introduce MME-CoT, a specialized benchmark\nevaluating the CoT reasoning performance of LMMs, spanning six domains: math,\nscience, OCR, logic, space-time, and general scenes. As the first comprehensive\nstudy in this area, we propose a thorough evaluation suite incorporating three\nnovel metrics that assess the reasoning quality, robustness, and efficiency at\na fine-grained level. Leveraging curated high-quality data and a unique\nevaluation strategy, we conduct an in-depth analysis of state-of-the-art LMMs,\nuncovering several key insights: 1) Models with reflection mechanism\ndemonstrate a superior CoT quality, with Kimi k1.5 outperforming GPT-4o and\ndemonstrating the highest quality results; 2) CoT prompting often degrades LMM\nperformance on perception-heavy tasks, suggesting a potentially harmful\noverthinking behavior; and 3) Although the CoT quality is high, LMMs with\nreflection exhibit significant inefficiency in both normal response and\nself-correction phases. We hope MME-CoT serves as a foundation for advancing\nmultimodal reasoning in LMMs. Project Page: https://mmecot.github.io/",
      "tldr_zh": "这篇论文引入了 MME-CoT 基准，用于系统评估 Large Multimodal Models (LMMs) 在 Chain-of-Thought (CoT) 推理方面的性能，涵盖数学、科学、OCR、逻辑、时空和一般场景等六个领域。研究提出三个新指标，细致评估推理的质量、鲁棒性和效率，通过高质量数据和独特策略分析了最先进 LMMs 的表现。关键发现包括：具有反射机制的模型（如 Kimi k1.5）表现出更高的 CoT 质量，但 CoT 提示可能在感知密集任务上降低性能，并导致响应和自校正阶段的效率低下。该基准有望为推进 LMMs 的多模态推理提供基础。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.CV",
      "comment": "Project Page: https://mmecot.github.io/",
      "pdf_url": "http://arxiv.org/pdf/2502.09621v1",
      "published_date": "2025-02-13 18:59:46 UTC",
      "updated_date": "2025-02-13 18:59:46 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T10:50:27.676214"
    },
    {
      "arxiv_id": "2502.09620v2",
      "title": "Exploring the Potential of Encoder-free Architectures in 3D LMMs",
      "title_zh": "探索无编码",
      "authors": [
        "Yiwen Tang",
        "Zoey Guo",
        "Zhuhao Wang",
        "Ray Zhang",
        "Qizhi Chen",
        "Junli Liu",
        "Delin Qu",
        "Zhigang Wang",
        "Dong Wang",
        "Xuelong Li",
        "Bin Zhao"
      ],
      "abstract": "Encoder-free architectures have been preliminarily explored in the 2D visual\ndomain, yet it remains an open question whether they can be effectively applied\nto 3D understanding scenarios. In this paper, we present the first\ncomprehensive investigation into the potential of encoder-free architectures to\nalleviate the challenges of encoder-based 3D Large Multimodal Models (LMMs).\nThese challenges include the failure to adapt to varying point cloud\nresolutions and the point features from the encoder not meeting the semantic\nneeds of Large Language Models (LLMs). We identify key aspects for 3D LMMs to\nremove the encoder and enable the LLM to assume the role of the 3D encoder: 1)\nWe propose the LLM-embedded Semantic Encoding strategy in the pre-training\nstage, exploring the effects of various point cloud self-supervised losses. And\nwe present the Hybrid Semantic Loss to extract high-level semantics. 2) We\nintroduce the Hierarchical Geometry Aggregation strategy in the instruction\ntuning stage. This incorporates inductive bias into the LLM layers to focus on\nthe local details of the point clouds. To the end, we present the first\nEncoder-free 3D LMM, ENEL. Our 7B model rivals the current state-of-the-art\nmodel, ShapeLLM-13B, achieving 55.10%, 50.98%, and 43.10% on the\nclassification, captioning, and VQA tasks, respectively. Our results\ndemonstrate that the encoder-free architecture is highly promising for\nreplacing encoder-based architectures in the field of 3D understanding. The\ncode is released at https://github.com/Ivan-Tang-3D/ENEL",
      "tldr_zh": "本文探讨了 Encoder-free architectures 在 3D Large Multimodal Models (LMMs) 中的潜力，旨在解决传统 encoder-based 模型的挑战，如无法适应不同点云分辨率和点特征语义不足。研究提出 LLM-embedded Semantic Encoding 策略（包括 Hybrid Semantic Loss）用于预训练阶段，以及 Hierarchical Geometry Aggregation 策略用于指令微调阶段，以增强 LLM 对点云的高级语义提取和局部细节关注。最终开发的 ENEL 7B 模型在分类、标题生成和 VQA 任务上分别达到 55.10%、50.98% 和 43.10%，与 ShapeLLM-13B 相当，证明 Encoder-free architectures 在 3D 理解领域具有显著潜力。代码已开源在 GitHub。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.CV",
      "comment": "The code is released at https://github.com/Ivan-Tang-3D/ENEL",
      "pdf_url": "http://arxiv.org/pdf/2502.09620v2",
      "published_date": "2025-02-13 18:59:45 UTC",
      "updated_date": "2025-05-17 03:38:29 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T10:50:40.321707"
    },
    {
      "arxiv_id": "2502.09614v1",
      "title": "DexTrack: Towards Generalizable Neural Tracking Control for Dexterous Manipulation from Human References",
      "title_zh": "翻译失败",
      "authors": [
        "Xueyi Liu",
        "Jianibieke Adalibieke",
        "Qianwei Han",
        "Yuzhe Qin",
        "Li Yi"
      ],
      "abstract": "We address the challenge of developing a generalizable neural tracking\ncontroller for dexterous manipulation from human references. This controller\naims to manage a dexterous robot hand to manipulate diverse objects for various\npurposes defined by kinematic human-object interactions. Developing such a\ncontroller is complicated by the intricate contact dynamics of dexterous\nmanipulation and the need for adaptivity, generalizability, and robustness.\nCurrent reinforcement learning and trajectory optimization methods often fall\nshort due to their dependence on task-specific rewards or precise system\nmodels. We introduce an approach that curates large-scale successful robot\ntracking demonstrations, comprising pairs of human references and robot\nactions, to train a neural controller. Utilizing a data flywheel, we\niteratively enhance the controller's performance, as well as the number and\nquality of successful tracking demonstrations. We exploit available tracking\ndemonstrations and carefully integrate reinforcement learning and imitation\nlearning to boost the controller's performance in dynamic environments. At the\nsame time, to obtain high-quality tracking demonstrations, we individually\noptimize per-trajectory tracking by leveraging the learned tracking controller\nin a homotopy optimization method. The homotopy optimization, mimicking\nchain-of-thought, aids in solving challenging trajectory tracking problems to\nincrease demonstration diversity. We showcase our success by training a\ngeneralizable neural controller and evaluating it in both simulation and real\nworld. Our method achieves over a 10% improvement in success rates compared to\nleading baselines. The project website with animated results is available at\nhttps://meowuu7.github.io/DexTrack/.",
      "tldr_zh": "本文提出DexTrack，一种可泛化的神经跟踪控制器，用于从人类参考实现灵巧操作（dexterous manipulation），以应对复杂的接触动态和环境适应性挑战。方法包括收集大规模机器人跟踪演示数据，通过数据飞轮（data flywheel）迭代优化，并结合强化学习（reinforcement learning）和模仿学习（imitation learning）提升控制器性能，同时利用homotopy optimization生成高质量、多样化的轨迹演示。实验在模拟和真实环境中评估，结果显示成功率比领先基准方法提高了超过10%，证明了该框架的鲁棒性和泛化能力。",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.CV",
        "cs.LG"
      ],
      "primary_category": "cs.RO",
      "comment": "Accepted to ICLR 2025. Website: https://meowuu7.github.io/DexTrack/\n  Code: https://github.com/Meowuu7/DexTrack/ Video:\n  https://youtu.be/zru1Z-DaiWE",
      "pdf_url": "http://arxiv.org/pdf/2502.09614v1",
      "published_date": "2025-02-13 18:59:13 UTC",
      "updated_date": "2025-02-13 18:59:13 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T10:50:51.493790"
    },
    {
      "arxiv_id": "2502.09609v2",
      "title": "Score-of-Mixture Training: Training One-Step Generative Models Made Simple via Score Estimation of Mixture Distributions",
      "title_zh": "翻译失败",
      "authors": [
        "Tejas Jayashankar",
        "J. Jon Ryu",
        "Gregory Wornell"
      ],
      "abstract": "We propose Score-of-Mixture Training (SMT), a novel framework for training\none-step generative models by minimizing a class of divergences called the\n$\\alpha$-skew Jensen-Shannon divergence. At its core, SMT estimates the score\nof mixture distributions between real and fake samples across multiple noise\nlevels. Similar to consistency models, our approach supports both training from\nscratch (SMT) and distillation using a pretrained diffusion model, which we\ncall Score-of-Mixture Distillation (SMD). It is simple to implement, requires\nminimal hyperparameter tuning, and ensures stable training. Experiments on\nCIFAR-10 and ImageNet 64x64 show that SMT/SMD are competitive with and can even\noutperform existing methods.",
      "tldr_zh": "该论文提出 Score-of-Mixture Training (SMT)，一种通过最小化 α-skew Jensen-Shannon divergence 来训练一步生成模型的框架，其核心是估计真实样本和假样本混合分布的 score，实现简单且训练稳定。SMT 支持从零开始训练，并通过 Score-of-Mixture Distillation (SMD) 利用预训练扩散模型进行知识蒸馏，减少了超参数调整的需求。在 CIFAR-10 和 ImageNet 64x64 数据集上的实验表明，SMT/SMD 与现有方法相比具有竞争力，甚至在性能上表现出优越性。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "stat.ML"
      ],
      "primary_category": "cs.LG",
      "comment": "27 pages, 9 figures. Title updated to match the title of the\n  manuscript, otherwise identical to v1",
      "pdf_url": "http://arxiv.org/pdf/2502.09609v2",
      "published_date": "2025-02-13 18:57:20 UTC",
      "updated_date": "2025-02-14 02:32:22 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T10:51:03.071631"
    },
    {
      "arxiv_id": "2502.09606v2",
      "title": "Human-LLM Coevolution: Evidence from Academic Writing",
      "title_zh": "人类-LLM 共同进化：来自学术写作的证据",
      "authors": [
        "Mingmeng Geng",
        "Roberto Trotta"
      ],
      "abstract": "With a statistical analysis of arXiv paper abstracts, we report a marked drop\nin the frequency of several words previously identified as overused by ChatGPT,\nsuch as \"delve\", starting soon after they were pointed out in early 2024. The\nfrequency of certain other words favored by ChatGPT, such as \"significant\", has\ninstead kept increasing. These phenomena suggest that some authors of academic\npapers have adapted their use of large language models (LLMs), for example, by\nselecting outputs or applying modifications to the LLM-generated content. Such\ncoevolution and cooperation of humans and LLMs thus introduce additional\nchallenges to the detection of machine-generated text in real-world scenarios.\nEstimating the impact of LLMs on academic writing by examining word frequency\nremains feasible, and more attention should be paid to words that were already\nfrequently employed, including those that have decreased in frequency due to\nLLMs' disfavor.",
      "tldr_zh": "本研究通过对 arXiv 论文摘要的统计分析，发现某些 ChatGPT 过度使用的词语（如 \"delve\"）在 2024 年初被指出后频率显著下降，而其他 LLMs 偏好的词语（如 \"significant\"）则持续增加。\n这些现象表明学术作者已适应 LLMs 的使用，例如通过选择输出或修改生成内容，以避免检测。\n这种人类和 LLMs 的共同演化（coevolution）增加了机器生成文本检测的难度，但通过监测词频变化，仍可评估 LLMs 对学术写作的影响，并应关注原本频繁的词语。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.CY",
        "cs.DL",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2502.09606v2",
      "published_date": "2025-02-13 18:55:56 UTC",
      "updated_date": "2025-02-17 18:48:26 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T10:51:15.146580"
    },
    {
      "arxiv_id": "2502.09604v1",
      "title": "SelfCite: Self-Supervised Alignment for Context Attribution in Large Language Models",
      "title_zh": "翻译失败",
      "authors": [
        "Yung-Sung Chuang",
        "Benjamin Cohen-Wang",
        "Shannon Zejiang Shen",
        "Zhaofeng Wu",
        "Hu Xu",
        "Xi Victoria Lin",
        "James Glass",
        "Shang-Wen Li",
        "Wen-tau Yih"
      ],
      "abstract": "We introduce SelfCite, a novel self-supervised approach that aligns LLMs to\ngenerate high-quality, fine-grained, sentence-level citations for the\nstatements in their generated responses. Instead of only relying on costly and\nlabor-intensive annotations, SelfCite leverages a reward signal provided by the\nLLM itself through context ablation: If a citation is necessary, removing the\ncited text from the context should prevent the same response; if sufficient,\nretaining the cited text alone should preserve the same response. This reward\ncan guide the inference-time best-of-N sampling strategy to improve citation\nquality significantly, as well as be used in preference optimization to\ndirectly fine-tune the models for generating better citations. The\neffectiveness of SelfCite is demonstrated by increasing citation F1 up to 5.3\npoints on the LongBench-Cite benchmark across five long-form question answering\ntasks.",
      "tldr_zh": "该论文提出SelfCite，一种自监督方法，用于对齐大型语言模型(LLMs)，以生成高质量、细粒度的句子级引用，从而提升响应中的上下文归因准确性。SelfCite通过上下文消融(context ablation)机制创建奖励信号——如果引用必要，移除引文应改变响应；如果充分，保留引文应保持响应——并将其应用于best-of-N采样策略和偏好优化(preference optimization)来微调模型。实验结果显示，在LongBench-Cite基准上的五个长形式问答任务中，引用F1分数提高了多达5.3点，证明了该方法的有效性。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "Implementation available at https://github.com/voidism/SelfCite",
      "pdf_url": "http://arxiv.org/pdf/2502.09604v1",
      "published_date": "2025-02-13 18:55:13 UTC",
      "updated_date": "2025-02-13 18:55:13 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T10:51:27.226121"
    },
    {
      "arxiv_id": "2502.09601v1",
      "title": "CoT-Valve: Length-Compressible Chain-of-Thought Tuning",
      "title_zh": "CoT-Valve：长度可压缩的链式思维微调",
      "authors": [
        "Xinyin Ma",
        "Guangnian Wan",
        "Runpeng Yu",
        "Gongfan Fang",
        "Xinchao Wang"
      ],
      "abstract": "Chain-of-Thought significantly enhances a model's reasoning capability, but\nit also comes with a considerable increase in inference costs due to long\nchains. With the observation that the reasoning path can be easily compressed\nunder easy tasks but struggle on hard tasks, we explore the feasibility of\nelastically controlling the length of reasoning paths with only one model,\nthereby reducing the inference overhead of reasoning models dynamically based\non task difficulty. We introduce a new tuning and inference strategy named\nCoT-Valve, designed to allow models to generate reasoning chains of varying\nlengths. To achieve this, we propose to identify a direction in the parameter\nspace that, when manipulated, can effectively control the length of generated\nCoT. Moreover, we show that this property is valuable for compressing the\nreasoning chain. We construct datasets with chains from long to short for the\nsame questions and explore two enhanced strategies for CoT-Valve: (1) a precise\nlength-compressible CoT tuning method, and (2) a progressive chain length\ncompression approach. Our experiments show that CoT-Valve successfully enables\ncontrollability and compressibility of the chain and shows better performance\nthan the prompt-based control. We applied this method to QwQ-32B-Preview,\nreducing reasoning chains on GSM8K from 741 to 225 tokens with a minor\nperformance drop (95.07% to 94.92%) and on AIME from 6827 to 4629 tokens, with\nonly one additional incorrect answer.",
      "tldr_zh": "这篇论文引入了 CoT-Valve，一种可压缩长度的 Chain-of-Thought (CoT) 调优策略，旨在通过动态控制推理链长度来减少模型的推理开销，同时适应任务难度。方法包括识别参数空间中的控制方向、构建从长到短的链条数据集，以及探索精确长度可压缩调优和渐进式压缩策略。实验结果显示，在 QwQ-32B-Preview 模型上，CoT-Valve 将 GSM8K 的推理链从 741 tokens 压缩到 225 tokens，仅导致性能轻微下降（95.07% 到 94.92%），并在 AIME 上从 6827 tokens 减至 4629 tokens，仅多错一个答案，比基于提示的控制方法更有效。",
      "categories": [
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.AI",
      "comment": "Work in progress. Code will be released at\n  https://github.com/horseee/CoT-Valve",
      "pdf_url": "http://arxiv.org/pdf/2502.09601v1",
      "published_date": "2025-02-13 18:52:36 UTC",
      "updated_date": "2025-02-13 18:52:36 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T10:51:40.075247"
    },
    {
      "arxiv_id": "2502.09596v1",
      "title": "KIMAs: A Configurable Knowledge Integrated Multi-Agent System",
      "title_zh": "KIMAs：可配置的知识集成多智能体系统",
      "authors": [
        "Zitao Li",
        "Fei Wei",
        "Yuexiang Xie",
        "Dawei Gao",
        "Weirui Kuang",
        "Zhijian Ma",
        "Bingchen Qian",
        "Yaliang Li",
        "Bolin Ding"
      ],
      "abstract": "Knowledge-intensive conversations supported by large language models (LLMs)\nhave become one of the most popular and helpful applications that can assist\npeople in different aspects. Many current knowledge-intensive applications are\ncentered on retrieval-augmented generation (RAG) techniques. While many\nopen-source RAG frameworks facilitate the development of RAG-based\napplications, they often fall short in handling practical scenarios complicated\nby heterogeneous data in topics and formats, conversational context management,\nand the requirement of low-latency response times. This technical report\npresents a configurable knowledge integrated multi-agent system, KIMAs, to\naddress these challenges. KIMAs features a flexible and configurable system for\nintegrating diverse knowledge sources with 1) context management and query\nrewrite mechanisms to improve retrieval accuracy and multi-turn conversational\ncoherency, 2) efficient knowledge routing and retrieval, 3) simple but\neffective filter and reference generation mechanisms, and 4) optimized\nparallelizable multi-agent pipeline execution. Our work provides a scalable\nframework for advancing the deployment of LLMs in real-world settings. To show\nhow KIMAs can help developers build knowledge-intensive applications with\ndifferent scales and emphases, we demonstrate how we configure the system to\nthree applications already running in practice with reliable performance.",
      "tldr_zh": "这篇论文提出了 KIMAs，一种可配置的知识集成多智能体系统，用于提升基于 LLMs 的知识密集型对话应用，解决了现有 RAG 框架在处理异构数据、对话上下文管理和低延迟响应等方面的不足。KIMAs 包括上下文管理和查询重写机制来提高检索准确性和多轮对话连贯性、有效的知识路由和检索机制、简单的过滤与引用生成，以及优化的并行多智能体管道执行。实验演示了如何配置 KIMAs 用于三个实际运行的应用，提供了一个可扩展的框架，支持 LLMs 在真实场景中的部署。",
      "categories": [
        "cs.AI",
        "cs.MA"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2502.09596v1",
      "published_date": "2025-02-13 18:51:12 UTC",
      "updated_date": "2025-02-13 18:51:12 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T10:51:51.526113"
    },
    {
      "arxiv_id": "2502.09567v1",
      "title": "MorphNLI: A Stepwise Approach to Natural Language Inference Using Text Morphing",
      "title_zh": "翻译失败",
      "authors": [
        "Vlad Andrei Negru",
        "Robert Vacareanu",
        "Camelia Lemnaru",
        "Mihai Surdeanu",
        "Rodica Potolea"
      ],
      "abstract": "We introduce MorphNLI, a modular step-by-step approach to natural language\ninference (NLI). When classifying the premise-hypothesis pairs into\n{entailment, contradiction, neutral}, we use a language model to generate the\nnecessary edits to incrementally transform (i.e., morph) the premise into the\nhypothesis. Then, using an off-the-shelf NLI model we track how the entailment\nprogresses with these atomic changes, aggregating these intermediate labels\ninto a final output. We demonstrate the advantages of our proposed method\nparticularly in realistic cross-domain settings, where our method always\noutperforms strong baselines with improvements up to 12.6% (relative). Further,\nour proposed approach is explainable as the atomic edits can be used to\nunderstand the overall NLI label.",
      "tldr_zh": "本研究提出了一种名为 MorphNLI 的模块化逐步方法，用于自然语言推理 (NLI)，通过文本变形 (text morphing) 将前提逐步转化为假设。方法利用语言模型生成原子编辑，并结合现成 NLI 模型跟踪这些变化的蕴涵进展 (entailment, contradiction, neutral)，最终通过聚合中间标签得出整体输出。在现实跨域设置中，MorphNLI 比强基线性能提升高达 12.6% (相对)，并提供可解释性，因为原子编辑有助于理解最终 NLI 标签。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "16 pages, 11 figures, 8 tables. Accepted for NAACL 2025 Findings",
      "pdf_url": "http://arxiv.org/pdf/2502.09567v1",
      "published_date": "2025-02-13 18:22:31 UTC",
      "updated_date": "2025-02-13 18:22:31 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T10:52:02.496843"
    },
    {
      "arxiv_id": "2502.09565v1",
      "title": "MDCrow: Automating Molecular Dynamics Workflows with Large Language Models",
      "title_zh": "MDCrow：使用大型语言模型自动化分子动力学工作流",
      "authors": [
        "Quintina Campbell",
        "Sam Cox",
        "Jorge Medina",
        "Brittany Watterson",
        "Andrew D. White"
      ],
      "abstract": "Molecular dynamics (MD) simulations are essential for understanding\nbiomolecular systems but remain challenging to automate. Recent advances in\nlarge language models (LLM) have demonstrated success in automating complex\nscientific tasks using LLM-based agents. In this paper, we introduce MDCrow, an\nagentic LLM assistant capable of automating MD workflows. MDCrow uses\nchain-of-thought over 40 expert-designed tools for handling and processing\nfiles, setting up simulations, analyzing the simulation outputs, and retrieving\nrelevant information from literature and databases. We assess MDCrow's\nperformance across 25 tasks of varying required subtasks and difficulty, and we\nevaluate the agent's robustness to both difficulty and prompt style.\n\\texttt{gpt-4o} is able to complete complex tasks with low variance, followed\nclosely by \\texttt{llama3-405b}, a compelling open-source model. While prompt\nstyle does not influence the best models' performance, it has significant\neffects on smaller models.",
      "tldr_zh": "本研究引入MDCrow，一种基于Large Language Models (LLM)的代理系统，用于自动化Molecular Dynamics (MD)模拟工作流，以解决MD任务的自动化挑战。MDCrow采用chain-of-thought推理结合40个专家设计的工具，处理文件管理、模拟设置、输出分析以及文献数据库检索。实验评估显示，在25个不同难度任务上，gpt-4o模型表现出色，完成复杂任务的变异性低，而llama3-405b作为开源模型紧随其后；提示风格对小型模型影响显著，但对顶级模型影响不大。",
      "categories": [
        "cs.AI",
        "physics.chem-ph"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2502.09565v1",
      "published_date": "2025-02-13 18:19:20 UTC",
      "updated_date": "2025-02-13 18:19:20 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T10:52:14.796786"
    },
    {
      "arxiv_id": "2502.09560v2",
      "title": "EmbodiedBench: Comprehensive Benchmarking Multi-modal Large Language Models for Vision-Driven Embodied Agents",
      "title_zh": "翻译失败",
      "authors": [
        "Rui Yang",
        "Hanyang Chen",
        "Junyu Zhang",
        "Mark Zhao",
        "Cheng Qian",
        "Kangrui Wang",
        "Qineng Wang",
        "Teja Venkat Koripella",
        "Marziyeh Movahedi",
        "Manling Li",
        "Heng Ji",
        "Huan Zhang",
        "Tong Zhang"
      ],
      "abstract": "Leveraging Multi-modal Large Language Models (MLLMs) to create embodied\nagents offers a promising avenue for tackling real-world tasks. While\nlanguage-centric embodied agents have garnered substantial attention,\nMLLM-based embodied agents remain underexplored due to the lack of\ncomprehensive evaluation frameworks. To bridge this gap, we introduce\nEmbodiedBench, an extensive benchmark designed to evaluate vision-driven\nembodied agents. EmbodiedBench features: (1) a diverse set of 1,128 testing\ntasks across four environments, ranging from high-level semantic tasks (e.g.,\nhousehold) to low-level tasks involving atomic actions (e.g., navigation and\nmanipulation); and (2) six meticulously curated subsets evaluating essential\nagent capabilities like commonsense reasoning, complex instruction\nunderstanding, spatial awareness, visual perception, and long-term planning.\nThrough extensive experiments, we evaluated 19 leading proprietary and\nopen-source MLLMs within EmbodiedBench. Our findings reveal that: MLLMs excel\nat high-level tasks but struggle with low-level manipulation, with the best\nmodel, GPT-4o, scoring only 28.9% on average. EmbodiedBench provides a\nmultifaceted standardized evaluation platform that not only highlights existing\nchallenges but also offers valuable insights to advance MLLM-based embodied\nagents. Our code is available at https://embodiedbench.github.io.",
      "tldr_zh": "本文引入了 EmbodiedBench，这是一个全面基准，用于评估基于 Multi-modal Large Language Models (MLLMs) 的视觉驱动 Embodied Agents，以填补现有评估框架的空白。该基准包含 1,128 个测试任务，分布在四个环境中，包括高层语义任务（如家庭任务）和低层操作任务（如导航和操纵），并设有六个子集来评估代理的关键能力，如 commonsense reasoning、复杂指令理解、spatial awareness、visual perception 和长期规划。实验评估了 19 个领先的专有和开源 MLLMs，结果显示这些模型在高层任务上表现出色，但低层操纵任务表现较差，最佳模型 GPT-4o 的平均得分仅为 28.9%。EmbodiedBench 作为多方面标准化平台，不仅突出了现有挑战，还为推进 MLLM-based Embodied Agents 提供了宝贵见解。",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.CV"
      ],
      "primary_category": "cs.AI",
      "comment": "52 pages",
      "pdf_url": "http://arxiv.org/pdf/2502.09560v2",
      "published_date": "2025-02-13 18:11:34 UTC",
      "updated_date": "2025-02-23 07:30:59 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T10:52:28.553611"
    },
    {
      "arxiv_id": "2502.10475v2",
      "title": "X-SG$^2$S: Safe and Generalizable Gaussian Splatting with X-dimensional Watermarks",
      "title_zh": "翻译失败",
      "authors": [
        "Zihang Cheng",
        "Huiping Zhuang",
        "Chun Li",
        "Xin Meng",
        "Ming Li",
        "Fei Richard Yu",
        "Liqiang Nie"
      ],
      "abstract": "3D Gaussian Splatting (3DGS) has been widely used in 3D reconstruction and 3D\ngeneration. Training to get a 3DGS scene often takes a lot of time and\nresources and even valuable inspiration. The increasing amount of 3DGS digital\nasset have brought great challenges to the copyright protection. However, it\nstill lacks profound exploration targeted at 3DGS. In this paper, we propose a\nnew framework X-SG$^2$S which can simultaneously watermark 1 to 3D messages\nwhile keeping the original 3DGS scene almost unchanged. Generally, we have a\nX-SG$^2$S injector for adding multi-modal messages simultaneously and an\nextractor for extract them. Specifically, we first split the watermarks into\nmessage patches in a fixed manner and sort the 3DGS points. A self-adaption\ngate is used to pick out suitable location for watermarking. Then use a\nXD(multi-dimension)-injection heads to add multi-modal messages into sorted\n3DGS points. A learnable gate can recognize the location with extra messages\nand XD-extraction heads can restore hidden messages from the location\nrecommended by the learnable gate. Extensive experiments demonstrated that the\nproposed X-SG$^2$S can effectively conceal multi modal messages without\nchanging pretrained 3DGS pipeline or the original form of 3DGS parameters.\nMeanwhile, with simple and efficient model structure and high practicality,\nX-SG$^2$S still shows good performance in hiding and extracting multi-modal\ninner structured or unstructured messages. X-SG$^2$S is the first to unify 1 to\n3D watermarking model for 3DGS and the first framework to add multi-modal\nwatermarks simultaneous in one 3DGS which pave the wave for later researches.",
      "tldr_zh": "本研究针对3D Gaussian Splatting (3DGS) 的版权保护问题，提出了一种新框架X-SG$^2$S，能够同时在1到3D消息中添加多模态水印，同时保持原3DGS场景几乎不变。框架包括一个注入器，用于将水印分成消息补丁、排序3DGS点，并通过自适应门和XD-注入头在合适位置添加消息；以及一个提取器，利用可学习门和XD-提取头恢复隐藏消息。实验结果表明，X-SG$^2$S在不改变预训练3DGS管道的情况下，高效隐藏和提取多模态消息，是首个统一1到3D水印模型，并首次实现同时添加多模态水印，为3DGS版权保护铺平道路。",
      "categories": [
        "cs.CR",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.CR",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2502.10475v2",
      "published_date": "2025-02-13 17:59:15 UTC",
      "updated_date": "2025-04-23 06:38:28 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T10:52:39.134877"
    },
    {
      "arxiv_id": "2502.09692v1",
      "title": "NeuralCFD: Deep Learning on High-Fidelity Automotive Aerodynamics Simulations",
      "title_zh": "NeuralCFD: 高保真汽车空气动力学模拟中的深度学习",
      "authors": [
        "Maurits Bleeker",
        "Matthias Dorfer",
        "Tobias Kronlachner",
        "Reinhard Sonnleitner",
        "Benedikt Alkin",
        "Johannes Brandstetter"
      ],
      "abstract": "Recent advancements in neural operator learning are paving the way for\ntransformative innovations in fields such as automotive aerodynamics. However,\nkey challenges must be overcome before neural network-based simulation\nsurrogates can be implemented at an industry scale. First, surrogates must\nbecome scalable to large surface and volume meshes, especially when using raw\ngeometry inputs only, i.e., without relying on the simulation mesh. Second,\nsurrogates must be trainable with a limited number of high-fidelity numerical\nsimulation samples while still reaching the required performance levels. To\nthis end, we introduce Geometry-preserving Universal Physics Transformer\n(GP-UPT), which separates geometry encoding and physics predictions, ensuring\nflexibility with respect to geometry representations and surface sampling\nstrategies. GP-UPT enables independent scaling of the respective parts of the\nmodel according to practical requirements, offering scalable solutions to open\nchallenges. GP-UPT circumvents the creation of high-quality simulation meshes,\nenables accurate 3D velocity field predictions at 20 million mesh cells, and\nexcels in transfer learning from low-fidelity to high-fidelity simulation\ndatasets, requiring less than half of the high-fidelity data to match the\nperformance of models trained from scratch.",
      "tldr_zh": "该论文介绍了NeuralCFD框架，利用深度学习处理高保真汽车空气动力学模拟，旨在解决神经网络代理模型在扩展到大型表面和体积网格时的挑战，以及用有限样本实现高性能的问题。作者提出Geometry-preserving Universal Physics Transformer (GP-UPT)，该方法分离几何编码和物理预测，支持灵活的几何表示和表面采样策略，从而实现模型部分的独立扩展。实验结果表明，GP-UPT无需创建高品质模拟网格，即可精确预测2000万网格单元的3D速度场，并在低保真到高保真的转移学习中，仅需不到一半的高保真数据即可匹配从零训练的模型性能。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "Preprint",
      "pdf_url": "http://arxiv.org/pdf/2502.09692v1",
      "published_date": "2025-02-13 17:58:07 UTC",
      "updated_date": "2025-02-13 17:58:07 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T10:52:52.982926"
    },
    {
      "arxiv_id": "2502.09532v1",
      "title": "Mind the Gap! Choice Independence in Using Multilingual LLMs for Persuasive Co-Writing Tasks in Different Languages",
      "title_zh": "翻译失败",
      "authors": [
        "Shreyan Biswas",
        "Alexander Erlei",
        "Ujwal Gadiraju"
      ],
      "abstract": "Recent advances in generative AI have precipitated a proliferation of novel\nwriting assistants. These systems typically rely on multilingual large language\nmodels (LLMs), providing globalized workers the ability to revise or create\ndiverse forms of content in different languages. However, there is substantial\nevidence indicating that the performance of multilingual LLMs varies between\nlanguages. Users who employ writing assistance for multiple languages are\ntherefore susceptible to disparate output quality. Importantly, recent research\nhas shown that people tend to generalize algorithmic errors across independent\ntasks, violating the behavioral axiom of choice independence. In this paper, we\nanalyze whether user utilization of novel writing assistants in a charity\nadvertisement writing task is affected by the AI's performance in a second\nlanguage. Furthermore, we quantify the extent to which these patterns translate\ninto the persuasiveness of generated charity advertisements, as well as the\nrole of peoples' beliefs about LLM utilization in their donation choices. Our\nresults provide evidence that writers who engage with an LLM-based writing\nassistant violate choice independence, as prior exposure to a Spanish LLM\nreduces subsequent utilization of an English LLM. While these patterns do not\naffect the aggregate persuasiveness of the generated advertisements, people's\nbeliefs about the source of an advertisement (human versus AI) do. In\nparticular, Spanish-speaking female participants who believed that they read an\nAI-generated advertisement strongly adjusted their donation behavior downwards.\nFurthermore, people are generally not able to adequately differentiate between\nhuman-generated and LLM-generated ads. Our work has important implications for\nthe design, development, integration, and adoption of multilingual LLMs as\nassistive agents -- particularly in writing tasks.",
      "tldr_zh": "本研究探讨了多语言大型语言模型 (multilingual LLMs) 在说服性写作任务中的性能差异，以及用户行为是否违反选择独立性 (choice independence)。通过实验分析，用户在慈善广告写作任务中，先前暴露于西班牙语 LLM 会减少对英语 LLM 的后续利用，导致行为偏差。尽管这些模式不影响生成的广告整体说服力，但人们对广告来源（人类 vs. AI）的信念会显著影响捐赠行为，尤其是西班牙语女性参与者对 AI 生成广告的捐赠意愿大幅降低。研究还发现，人们难以有效区分人类生成和 LLM 生成的内容，为多语言 LLMs 在写作任务中的设计、开发、整合和采用提供了关键启示。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.HC"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2502.09532v1",
      "published_date": "2025-02-13 17:49:30 UTC",
      "updated_date": "2025-02-13 17:49:30 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T10:53:04.644392"
    },
    {
      "arxiv_id": "2502.09511v1",
      "title": "Diffusion Models for Molecules: A Survey of Methods and Tasks",
      "title_zh": "分子扩散模型：方法和任务的综述",
      "authors": [
        "Liang Wang",
        "Chao Song",
        "Zhiyuan Liu",
        "Yu Rong",
        "Qiang Liu",
        "Shu Wu",
        "Liang Wang"
      ],
      "abstract": "Generative tasks about molecules, including but not limited to molecule\ngeneration, are crucial for drug discovery and material design, and have\nconsistently attracted significant attention. In recent years, diffusion models\nhave emerged as an impressive class of deep generative models, sparking\nextensive research and leading to numerous studies on their application to\nmolecular generative tasks. Despite the proliferation of related work, there\nremains a notable lack of up-to-date and systematic surveys in this area.\nParticularly, due to the diversity of diffusion model formulations, molecular\ndata modalities, and generative task types, the research landscape is\nchallenging to navigate, hindering understanding and limiting the area's\ngrowth. To address this, this paper conducts a comprehensive survey of\ndiffusion model-based molecular generative methods. We systematically review\nthe research from the perspectives of methodological formulations, data\nmodalities, and task types, offering a novel taxonomy. This survey aims to\nfacilitate understanding and further flourishing development in this area. The\nrelevant papers are summarized at:\nhttps://github.com/AzureLeon1/awesome-molecular-diffusion-models.",
      "tldr_zh": "这篇论文对扩散模型(diffusion models)在分子生成任务中的应用进行了全面调查，旨在填补该领域的系统性文献缺失。论文从方法论形式、数据模态和任务类型三个角度审视现有研究，并提出一个新颖的分类体系，以帮助导航这一多样化的研究景观。调查强调了扩散模型在药物发现和材料设计中的潜力，并为进一步发展提供便利，相关论文总结可查阅指定GitHub链接。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CE"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2502.09511v1",
      "published_date": "2025-02-13 17:22:50 UTC",
      "updated_date": "2025-02-13 17:22:50 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T10:53:14.887710"
    },
    {
      "arxiv_id": "2502.09503v2",
      "title": "AttentionSmithy: A Modular Framework for Rapid Transformer Development and Customization",
      "title_zh": "AttentionSmithy: 用于快速 Transformer 开发和自定义的模块化框架",
      "authors": [
        "Caleb Cranney",
        "Jesse G. Meyer"
      ],
      "abstract": "Transformer architectures have transformed AI applications but remain complex\nto customize for domain experts lacking low-level implementation expertise. We\nintroduce AttentionSmithy, a modular software package that simplifies\ntransformer innovation by breaking down key components into reusable building\nblocks: attention modules, feed-forward networks, normalization layers, and\npositional encodings. Users can rapidly prototype and evaluate transformer\nvariants without extensive coding. Our framework supports four positional\nencoding strategies and integrates with neural architecture search for\nautomated design. We validate AttentionSmithy by replicating the original\ntransformer under resource constraints and optimizing translation performance\nby combining positional encodings. Additionally, we demonstrate its\nadaptability in gene-specific modeling, achieving over 95% accuracy in cell\ntype classification. These case studies highlight AttentionSmithy's potential\nto accelerate research across diverse fields by removing framework\nimplementation barriers.",
      "tldr_zh": "该研究引入AttentionSmithy，一个模块化软件框架，旨在简化Transformer架构的开发和定制，方便缺乏低级实现经验的领域专家快速创新。框架将关键组件分解为可重用构建块，包括attention模块、feed-forward网络、normalization层和positional encodings，支持四种positional encoding策略并集成neural architecture search进行自动化设计。通过案例验证，AttentionSmithy在资源受限条件下复制了原始Transformer，并优化了翻译性能；在基因特定建模中实现了超过95%的细胞类型分类准确率。该框架有望加速跨领域研究，消除实施障碍。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2502.09503v2",
      "published_date": "2025-02-13 17:15:26 UTC",
      "updated_date": "2025-02-14 21:38:39 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T10:53:26.952130"
    },
    {
      "arxiv_id": "2502.09497v1",
      "title": "Improve LLM-based Automatic Essay Scoring with Linguistic Features",
      "title_zh": "使用语言学特征改进基于 LLM 的自动作文评分",
      "authors": [
        "Zhaoyi Joey Hou",
        "Alejandro Ciuba",
        "Xiang Lorraine Li"
      ],
      "abstract": "Automatic Essay Scoring (AES) assigns scores to student essays, reducing the\ngrading workload for instructors. Developing a scoring system capable of\nhandling essays across diverse prompts is challenging due to the flexibility\nand diverse nature of the writing task. Existing methods typically fall into\ntwo categories: supervised feature-based approaches and large language model\n(LLM)-based methods. Supervised feature-based approaches often achieve higher\nperformance but require resource-intensive training. In contrast, LLM-based\nmethods are computationally efficient during inference but tend to suffer from\nlower performance. This paper combines these approaches by incorporating\nlinguistic features into LLM-based scoring. Experimental results show that this\nhybrid method outperforms baseline models for both in-domain and out-of-domain\nwriting prompts.",
      "tldr_zh": "本文针对Automatic Essay Scoring (AES) 的挑战，提出一种混合方法，将linguistic features 整合到LLM-based 评分系统中，以处理多样化写作提示。相比传统监督特征-based 方法（资源密集）和纯LLM方法（性能较低），这种结合方式提升了系统的整体效率和准确性。实验结果表明，该方法在in-domain 和out-of-domain 提示上均优于基线模型，为更可靠的自动作文评分提供了新途径。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "To be published in the workshop Innovation and Responsibility in\n  AI-Supported Education (iRaise) at the 2025 Conference on Artificial\n  Intelligence (AAAI)",
      "pdf_url": "http://arxiv.org/pdf/2502.09497v1",
      "published_date": "2025-02-13 17:09:52 UTC",
      "updated_date": "2025-02-13 17:09:52 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T10:53:38.693245"
    },
    {
      "arxiv_id": "2502.09690v1",
      "title": "Trust at Your Own Peril: A Mixed Methods Exploration of the Ability of Large Language Models to Generate Expert-Like Systems Engineering Artifacts and a Characterization of Failure Modes",
      "title_zh": "自担风险：大语言模型生成类似专家的系统工程制品能力的一种混合",
      "authors": [
        "Taylan G. Topcu",
        "Mohammed Husain",
        "Max Ofsa",
        "Paul Wach"
      ],
      "abstract": "Multi-purpose Large Language Models (LLMs), a subset of generative Artificial\nIntelligence (AI), have recently made significant progress. While expectations\nfor LLMs to assist systems engineering (SE) tasks are paramount; the\ninterdisciplinary and complex nature of systems, along with the need to\nsynthesize deep-domain knowledge and operational context, raise questions\nregarding the efficacy of LLMs to generate SE artifacts, particularly given\nthat they are trained using data that is broadly available on the internet. To\nthat end, we present results from an empirical exploration, where a human\nexpert-generated SE artifact was taken as a benchmark, parsed, and fed into\nvarious LLMs through prompt engineering to generate segments of typical SE\nartifacts. This procedure was applied without any fine-tuning or calibration to\ndocument baseline LLM performance. We then adopted a two-fold mixed-methods\napproach to compare AI generated artifacts against the benchmark. First, we\nquantitatively compare the artifacts using natural language processing\nalgorithms and find that when prompted carefully, the state-of-the-art\nalgorithms cannot differentiate AI-generated artifacts from the human-expert\nbenchmark. Second, we conduct a qualitative deep dive to investigate how they\ndiffer in terms of quality. We document that while the two-material appear very\nsimilar, AI generated artifacts exhibit serious failure modes that could be\ndifficult to detect. We characterize these as: premature requirements\ndefinition, unsubstantiated numerical estimates, and propensity to overspecify.\nWe contend that this study tells a cautionary tale about why the SE community\nmust be more cautious adopting AI suggested feedback, at least when generated\nby multi-purpose LLMs.",
      "tldr_zh": "该研究探索了大型语言模型 (LLMs) 生成类似专家的系统工程 (SE) 工件的能力，通过混合方法评估其效能和潜在失败模式。研究者以人类专家生成的 SE 工件作为基准，通过提示工程输入 LLMs 生成工件，并使用自然语言处理 (NLP) 算法进行定量比较，结果显示 AI 生成的工件在表面上难以与基准区分。定性分析揭示了 AI 工件的严重缺陷，包括过早定义需求、不合理的数字估计以及过度指定。总体而言，该研究警告 SE 社区在使用多用途 LLMs 生成的反馈时需保持谨慎，以避免潜在风险。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "41 pages, 10 figures",
      "pdf_url": "http://arxiv.org/pdf/2502.09690v1",
      "published_date": "2025-02-13 17:05:18 UTC",
      "updated_date": "2025-02-13 17:05:18 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T10:53:51.430777"
    },
    {
      "arxiv_id": "2502.09495v1",
      "title": "Cracking the Code: Enhancing Development finance understanding with artificial intelligence",
      "title_zh": "翻译失败",
      "authors": [
        "Pierre Beaucoral"
      ],
      "abstract": "Analyzing development projects is crucial for understanding donors aid\nstrategies, recipients priorities, and to assess development finance capacity\nto adress development issues by on-the-ground actions. In this area, the\nOrganisation for Economic Co-operation and Developments (OECD) Creditor\nReporting System (CRS) dataset is a reference data source. This dataset\nprovides a vast collection of project narratives from various sectors\n(approximately 5 million projects). While the OECD CRS provides a rich source\nof information on development strategies, it falls short in informing project\npurposes due to its reporting process based on donors self-declared main\nobjectives and pre-defined industrial sectors. This research employs a novel\napproach that combines Machine Learning (ML) techniques, specifically Natural\nLanguage Processing (NLP), an innovative Python topic modeling technique called\nBERTopic, to categorise (cluster) and label development projects based on their\nnarrative descriptions. By revealing existing yet hidden topics of development\nfinance, this application of artificial intelligence enables a better\nunderstanding of donor priorities and overall development funding and provides\nmethods to analyse public and private projects narratives.",
      "tldr_zh": "本研究针对 OECD Creditor Reporting System (CRS) 数据集在揭示发展项目目的方面的不足，提出了一种结合 Machine Learning (ML)、Natural Language Processing (NLP) 和 BERTopic 主题建模的技术，对约 500 万个项目叙述进行分类（聚类）和标记。\n通过揭示隐藏的主题，该方法增强了对捐赠者援助策略、接受者优先事项和整体发展融资的理解。\n这项创新应用为分析公共和私人项目叙述提供了有效工具，从而更好地评估发展融资的能力和影响。",
      "categories": [
        "econ.GN",
        "cs.AI",
        "cs.LG",
        "q-fin.EC"
      ],
      "primary_category": "econ.GN",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2502.09495v1",
      "published_date": "2025-02-13 17:01:45 UTC",
      "updated_date": "2025-02-13 17:01:45 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T10:54:03.511874"
    },
    {
      "arxiv_id": "2502.09487v1",
      "title": "Objective quantification of mood states using large language models",
      "title_zh": "利用大型",
      "authors": [
        "Jakub Onysk",
        "Quentin Huys"
      ],
      "abstract": "Emotional states influence human behaviour and cognition, leading to diverse\nthought trajectories. Similarly, Large Language Models (LLMs) showcase an\nexcellent level of response consistency across wide-ranging contexts (prompts).\nWe leverage these parallels to establish a framework for quantifying mental\nstates. Our approach utilises self-report questionnaires that reliably assess\nthese states due to their inherent sensitivity to patterns of co-occurring\nresponses. Specifically, we recruited a large sample of participants (N=422) to\ninvestigate how well an LLM (Mistral-7B-OpenOrca) quantifies a heterogenous set\nof depressive mood states measured with participants' open-ended responses to a\ndepression questionnaire. We show LLM responses to held-out multiple-choice\nquestions, given participants' open-ended answers, correlate strongly (r:\n0.52-0.84) with true questionnaire scores, demonstrating LLM's generalisation\nfrom mood representations. We explore a link between these representations and\nfactor analysis. Using ridge regression, we find depression-related subspaces\nwithin LLM hidden states. We show these subspaces to be predictive of\nparticipants' \"Depression\" and \"Somatic & Emotional Distress\" factor scores, as\nwell as suicidality severity. Overall, LLMs can provide quantitative measures\nof mental states. The reliability of these hinges upon how informative the\nquestions we ask participants are. Used correctly, this approach could\nsupplement mental state assessment in a variety of settings.",
      "tldr_zh": "本研究提出了一种框架，利用大型语言模型 (LLMs) 来客观量化情绪状态，通过利用LLMs的响应一致性与人类情绪模式之间的相似性。研究招募422名参与者，使用自报问卷收集开放式响应，并以Mistral-7B-OpenOrca模型处理这些响应，结果显示LLMs对多项选择题的预测与真实问卷分数高度相关 (r: 0.52-0.84)。此外，通过因子分析和ridge regression，研究者在LLMs隐藏状态中识别出抑郁相关子空间，这些子空间能有效预测参与者的抑郁分数、躯体与情绪困扰分数以及自杀风险。该方法若应用得当，可补充各种情境下的心理状态评估。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "main text - 9 pages, 5 figures;",
      "pdf_url": "http://arxiv.org/pdf/2502.09487v1",
      "published_date": "2025-02-13 16:52:06 UTC",
      "updated_date": "2025-02-13 16:52:06 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T10:54:15.419233"
    },
    {
      "arxiv_id": "2502.09484v1",
      "title": "PenTest++: Elevating Ethical Hacking with AI and Automation",
      "title_zh": "翻译失败",
      "authors": [
        "Haitham S. Al-Sinani",
        "Chris J. Mitchell"
      ],
      "abstract": "Traditional ethical hacking relies on skilled professionals and\ntime-intensive command management, which limits its scalability and efficiency.\nTo address these challenges, we introduce PenTest++, an AI-augmented system\nthat integrates automation with generative AI (GenAI) to optimise ethical\nhacking workflows. Developed in a controlled virtual environment, PenTest++\nstreamlines critical penetration testing tasks, including reconnaissance,\nscanning, enumeration, exploitation, and documentation, while maintaining a\nmodular and adaptable design. The system balances automation with human\noversight, ensuring informed decision-making at key stages, and offers\nsignificant benefits such as enhanced efficiency, scalability, and\nadaptability. However, it also raises ethical considerations, including privacy\nconcerns and the risks of AI-generated inaccuracies (hallucinations). This\nresearch underscores the potential of AI-driven systems like PenTest++ to\ncomplement human expertise in cybersecurity by automating routine tasks,\nenabling professionals to focus on strategic decision-making. By incorporating\nrobust ethical safeguards and promoting ongoing refinement, PenTest++\ndemonstrates how AI can be responsibly harnessed to address operational and\nethical challenges in the evolving cybersecurity landscape.",
      "tldr_zh": "本研究针对传统道德黑客（ethical hacking）的可扩展性和效率问题，引入PenTest++系统，该系统整合自动化和生成AI（GenAI），优化渗透测试工作流程，包括reconnaissance、scanning、enumeration、exploitation和documentation任务。PenTest++在受控虚拟环境中开发，采用模块化和适应性设计，同时平衡自动化与人工监督，确保关键决策的人性化参与。实验结果显示，该系统显著提升效率、可扩展性和适应性，使专业人士能专注于战略决策，尽管面临隐私问题和AI生成错误（hallucinations）的伦理挑战。通过融入稳健的道德保障，PenTest++展示了AI如何负责任地辅助网络安全领域。",
      "categories": [
        "cs.CR",
        "cs.AI"
      ],
      "primary_category": "cs.CR",
      "comment": "27 pages, 6 figures",
      "pdf_url": "http://arxiv.org/pdf/2502.09484v1",
      "published_date": "2025-02-13 16:46:23 UTC",
      "updated_date": "2025-02-13 16:46:23 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T10:54:26.875125"
    },
    {
      "arxiv_id": "2502.09471v1",
      "title": "Wholly-WOOD: Wholly Leveraging Diversified-quality Labels for Weakly-supervised Oriented Object Detection",
      "title_zh": "翻译失败",
      "authors": [
        "Yi Yu",
        "Xue Yang",
        "Yansheng Li",
        "Zhenjun Han",
        "Feipeng Da",
        "Junchi Yan"
      ],
      "abstract": "Accurately estimating the orientation of visual objects with compact rotated\nbounding boxes (RBoxes) has become a prominent demand, which challenges\nexisting object detection paradigms that only use horizontal bounding boxes\n(HBoxes). To equip the detectors with orientation awareness, supervised\nregression/classification modules have been introduced at the high cost of\nrotation annotation. Meanwhile, some existing datasets with oriented objects\nare already annotated with horizontal boxes or even single points. It becomes\nattractive yet remains open for effectively utilizing weaker single point and\nhorizontal annotations to train an oriented object detector (OOD). We develop\nWholly-WOOD, a weakly-supervised OOD framework, capable of wholly leveraging\nvarious labeling forms (Points, HBoxes, RBoxes, and their combination) in a\nunified fashion. By only using HBox for training, our Wholly-WOOD achieves\nperformance very close to that of the RBox-trained counterpart on remote\nsensing and other areas, significantly reducing the tedious efforts on\nlabor-intensive annotation for oriented objects. The source codes are available\nat https://github.com/VisionXLab/whollywood (PyTorch-based) and\nhttps://github.com/VisionXLab/whollywood-jittor (Jittor-based).",
      "tldr_zh": "该论文提出 Wholly-WOOD 框架，用于弱监督的 Oriented Object Detection (OOD)，旨在充分利用多样化标注形式，包括 Points、HBoxes、RBoxes 及其组合，以减少昂贵的旋转标注需求。框架通过统一处理这些标注，实现高效训练，使模型在不依赖完整 RBoxes 的情况下，准确估计视觉对象的方向。实验结果显示，仅使用 HBoxes 训练即可在遥感和其他领域达到与 RBox 训练相近的性能，大幅降低了繁琐的标注工作量。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "18 pages, 9 figures, 9 tables, accepted by TPAMI",
      "pdf_url": "http://arxiv.org/pdf/2502.09471v1",
      "published_date": "2025-02-13 16:34:59 UTC",
      "updated_date": "2025-02-13 16:34:59 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T10:54:38.648213"
    },
    {
      "arxiv_id": "2502.09460v1",
      "title": "Metamorphic Testing for Pose Estimation Systems",
      "title_zh": "翻译失败",
      "authors": [
        "Matias Duran",
        "Thomas Laurent",
        "Ellen Rushe",
        "Anthony Ventresque"
      ],
      "abstract": "Pose estimation systems are used in a variety of fields, from sports\nanalytics to livestock care. Given their potential impact, it is paramount to\nsystematically test their behaviour and potential for failure. This is a\ncomplex task due to the oracle problem and the high cost of manual labelling\nnecessary to build ground truth keypoints. This problem is exacerbated by the\nfact that different applications require systems to focus on different subjects\n(e.g., human versus animal) or landmarks (e.g., only extremities versus whole\nbody and face), which makes labelled test data rarely reusable. To combat these\nproblems we propose MET-POSE, a metamorphic testing framework for pose\nestimation systems that bypasses the need for manual annotation while assessing\nthe performance of these systems under different circumstances. MET-POSE thus\nallows users of pose estimation systems to assess the systems in conditions\nthat more closely relate to their application without having to label an ad-hoc\ntest dataset or rely only on available datasets, which may not be adapted to\ntheir application domain. While we define MET-POSE in general terms, we also\npresent a non-exhaustive list of metamorphic rules that represent common\nchallenges in computer vision applications, as well as a specific way to\nevaluate these rules. We then experimentally show the effectiveness of MET-POSE\nby applying it to Mediapipe Holistic, a state of the art human pose estimation\nsystem, with the FLIC and PHOENIX datasets. With these experiments, we outline\nnumerous ways in which the outputs of MET-POSE can uncover faults in pose\nestimation systems at a similar or higher rate than classic testing using hand\nlabelled data, and show that users can tailor the rule set they use to the\nfaults and level of accuracy relevant to their application.",
      "tldr_zh": "该论文针对姿势估计系统（如用于体育分析或畜牧业）的测试挑战，提出 MET-POSE 框架，以解决预言机问题（oracle problem）和手动标注的高成本问题。该框架利用变形测试（metamorphic testing）方法，通过定义一组常见计算机视觉挑战的变形规则，来评估系统在不同场景下的性能，而无需额外的标记数据集。实验在 Mediapipe Holistic 系统上使用 FLIC 和 PHOENIX 数据集证明，MET-POSE 能以类似于或高于传统手动标记测试的方式发现故障，并允许用户根据具体应用定制规则，从而提升测试的灵活性和效率。",
      "categories": [
        "cs.SE",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.SE",
      "comment": "Accepted for publication at 2025 IEEE Conference on Software Testing,\n  Verification and Validation (ICST)",
      "pdf_url": "http://arxiv.org/pdf/2502.09460v1",
      "published_date": "2025-02-13 16:27:23 UTC",
      "updated_date": "2025-02-13 16:27:23 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T10:54:51.987623"
    },
    {
      "arxiv_id": "2502.09443v1",
      "title": "Relational Conformal Prediction for Correlated Time Series",
      "title_zh": "针对相关时间序列的关系保形预测",
      "authors": [
        "Andrea Cini",
        "Alexander Jenkins",
        "Danilo Mandic",
        "Cesare Alippi",
        "Filippo Maria Bianchi"
      ],
      "abstract": "We address the problem of uncertainty quantification in time series\nforecasting by exploiting observations at correlated sequences. Relational deep\nlearning methods leveraging graph representations are among the most effective\ntools for obtaining point estimates from spatiotemporal data and correlated\ntime series. However, the problem of exploiting relational structures to\nestimate the uncertainty of such predictions has been largely overlooked in the\nsame context. To this end, we propose a novel distribution-free approach based\non the conformal prediction framework and quantile regression. Despite the\nrecent applications of conformal prediction to sequential data, existing\nmethods operate independently on each target time series and do not account for\nrelationships among them when constructing the prediction interval. We fill\nthis void by introducing a novel conformal prediction method based on graph\ndeep learning operators. Our method, named Conformal Relational Prediction\n(CoRel), does not require the relational structure (graph) to be known as a\nprior and can be applied on top of any pre-trained time series predictor.\nAdditionally, CoRel includes an adaptive component to handle non-exchangeable\ndata and changes in the input time series. Our approach provides accurate\ncoverage and archives state-of-the-art uncertainty quantification in relevant\nbenchmarks.",
      "tldr_zh": "该论文解决了时间序列预测中不确定性量化的挑战，通过利用相关序列的观察来改进预测可靠性。作者提出了一种新方法CoRel（Conformal Relational Prediction），基于共形预测框架（Conformal Prediction）和分位数回归（quantile regression），并结合图深度学习操作符来利用序列间的关系。不同于现有方法，CoRel不需要预先知道关系结构（graph），并包含自适应组件以处理非可交换数据和序列变化。实验结果显示，该方法在相关基准上实现了最先进的覆盖率和不确定性量化性能。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2502.09443v1",
      "published_date": "2025-02-13 16:12:17 UTC",
      "updated_date": "2025-02-13 16:12:17 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T10:55:03.558836"
    },
    {
      "arxiv_id": "2502.09436v2",
      "title": "Variable Stiffness for Robust Locomotion through Reinforcement Learning",
      "title_zh": "翻译失败",
      "authors": [
        "Dario Spoljaric",
        "Yashuai Yan",
        "Dongheui Lee"
      ],
      "abstract": "Reinforcement-learned locomotion enables legged robots to perform highly\ndynamic motions but often accompanies time-consuming manual tuning of joint\nstiffness. This paper introduces a novel control paradigm that integrates\nvariable stiffness into the action space alongside joint positions, enabling\ngrouped stiffness control such as per-joint stiffness (PJS), per-leg stiffness\n(PLS) and hybrid joint-leg stiffness (HJLS). We show that variable stiffness\npolicies, with grouping in per-leg stiffness (PLS), outperform position-based\ncontrol in velocity tracking and push recovery. In contrast, HJLS excels in\nenergy efficiency. Despite the fact that our policy is trained on flat floor\nonly, our method showcases robust walking behaviour on diverse outdoor\nterrains, indicating robust sim-to-real transfer. Our approach simplifies\ndesign by eliminating per-joint stiffness tuning while keeping competitive\nresults with various metrics.",
      "tldr_zh": "本论文提出了一种新控制范式，将可变刚度整合到动作空间中，用于强化学习（Reinforcement Learning）驱动的机器人步态控制，支持PJS（per-joint stiffness）、PLS（per-leg stiffness）和HJLS（hybrid joint-leg stiffness）等分组刚度。实验结果显示，PLS分组在速度跟踪和推力恢复方面优于基于位置的控制，而HJLS在能量效率上表现出色。虽仅在平地板上训练，该方法仍实现了稳健的户外地形适应性，并简化设计过程，消除了每个关节的刚度手动调优，同时保持竞争性性能。",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "comment": "accepted to IFAC Joint Symposia on Mechatronics & Robotics",
      "pdf_url": "http://arxiv.org/pdf/2502.09436v2",
      "published_date": "2025-02-13 16:00:46 UTC",
      "updated_date": "2025-04-22 06:55:59 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T10:55:15.752740"
    },
    {
      "arxiv_id": "2502.09432v1",
      "title": "Dual Formulation for Non-Rectangular Lp Robust Markov Decision Processes",
      "title_zh": "翻译失败",
      "authors": [
        "Navdeep Kumar",
        "Adarsh Gupta",
        "Maxence Mohamed Elfatihi",
        "Giorgia Ramponi",
        "Kfir Yehuda Levy",
        "Shie Mannor"
      ],
      "abstract": "We study robust Markov decision processes (RMDPs) with non-rectangular\nuncertainty sets, which capture interdependencies across states unlike\ntraditional rectangular models. While non-rectangular robust policy evaluation\nis generally NP-hard, even in approximation, we identify a powerful class of\n$L_p$-bounded uncertainty sets that avoid these complexity barriers due to\ntheir structural simplicity. We further show that this class can be decomposed\ninto infinitely many \\texttt{sa}-rectangular $L_p$-bounded sets and leverage\nits structural properties to derive a novel dual formulation for $L_p$ RMDPs.\nThis formulation provides key insights into the adversary's strategy and\nenables the development of the first robust policy evaluation algorithms for\nnon-rectangular RMDPs. Empirical results demonstrate that our approach\nsignificantly outperforms brute-force methods, establishing a promising\nfoundation for future investigation into non-rectangular robust MDPs.",
      "tldr_zh": "本研究探讨了非矩形不确定性集的鲁棒 Markov 决策过程 (RMDPs)，这些集能捕捉状态间的相互依赖性，与传统矩形模型不同。尽管非矩形鲁棒策略评估通常是 NP-hard 的，该研究识别了一个结构简单的 $L_p$-bounded uncertainty sets 类，并将其分解成无限多的 sa-rectangular $L_p$-bounded sets。基于此，他们推导出了一个新的 dual formulation，用于分析对手策略并开发了首个非矩形 RMDPs 的鲁棒策略评估算法。实证结果显示，该方法显著优于暴力方法，为未来非矩形鲁棒 MDPs 的研究奠定了基础。",
      "categories": [
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2502.09432v1",
      "published_date": "2025-02-13 15:55:00 UTC",
      "updated_date": "2025-02-13 15:55:00 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T10:55:27.540886"
    },
    {
      "arxiv_id": "2502.09688v1",
      "title": "Towards Virtual Clinical Trials of Radiology AI with Conditional Generative Modeling",
      "title_zh": "翻译失败",
      "authors": [
        "Benjamin D. Killeen",
        "Bohua Wan",
        "Aditya V. Kulkarni",
        "Nathan Drenkow",
        "Michael Oberst",
        "Paul H. Yi",
        "Mathias Unberath"
      ],
      "abstract": "Artificial intelligence (AI) is poised to transform healthcare by enabling\npersonalized and efficient care through data-driven insights. Although\nradiology is at the forefront of AI adoption, in practice, the potential of AI\nmodels is often overshadowed by severe failures to generalize: AI models can\nhave performance degradation of up to 20% when transitioning from controlled\ntest environments to clinical use by radiologists. This mismatch raises\nconcerns that radiologists will be misled by incorrect AI predictions in\npractice and/or grow to distrust AI, rendering these promising technologies\npractically ineffectual. Exhaustive clinical trials of AI models on abundant\nand diverse data is thus critical to anticipate AI model degradation when\nencountering varied data samples. Achieving these goals, however, is\nchallenging due to the high costs of collecting diverse data samples and\ncorresponding annotations. To overcome these limitations, we introduce a novel\nconditional generative AI model designed for virtual clinical trials (VCTs) of\nradiology AI, capable of realistically synthesizing full-body CT images of\npatients with specified attributes. By learning the joint distribution of\nimages and anatomical structures, our model enables precise replication of\nreal-world patient populations with unprecedented detail at this scale. We\ndemonstrate meaningful evaluation of radiology AI models through VCTs powered\nby our synthetic CT study populations, revealing model degradation and\nfacilitating algorithmic auditing for bias-inducing data attributes. Our\ngenerative AI approach to VCTs is a promising avenue towards a scalable\nsolution to assess model robustness, mitigate biases, and safeguard patient\ncare by enabling simpler testing and evaluation of AI models in any desired\nrange of diverse patient populations.",
      "tldr_zh": "该论文针对放射学 AI 模型在临床应用中存在的泛化问题（如性能下降高达 20%），提出了一种基于条件生成建模的虚拟临床试验 (VCTs) 框架，以评估模型鲁棒性并缓解偏见。研究引入了一个新型条件生成 AI 模型，通过学习图像和解剖结构的联合分布，能合成带有指定属性的全身体 CT 图像，从而精确复制多样化的患者人群。实验结果显示，该方法可揭示 AI 模型的性能退化，促进算法审计，并为更安全、有效的放射学 AI 应用提供可扩展的评估途径。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "35 pages",
      "pdf_url": "http://arxiv.org/pdf/2502.09688v1",
      "published_date": "2025-02-13 15:53:52 UTC",
      "updated_date": "2025-02-13 15:53:52 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T10:55:40.207912"
    },
    {
      "arxiv_id": "2502.10473v1",
      "title": "Diverse Transformer Decoding for Offline Reinforcement Learning Using Financial Algorithmic Approaches",
      "title_zh": "翻译失败",
      "authors": [
        "Dan Elbaz",
        "Oren Salzman"
      ],
      "abstract": "Offline Reinforcement Learning (RL) algorithms learn a policy using a fixed\ntraining dataset, which is then deployed online to interact with the\nenvironment and make decisions. Transformers, a standard choice for modeling\ntime-series data, are gaining popularity in offline RL. In this context, Beam\nSearch (BS), an approximate inference algorithm, is the go-to decoding method.\nOffline RL eliminates the need for costly or risky online data collection.\nHowever, the restricted dataset induces uncertainty as the agent may encounter\nunfamiliar sequences of states and actions during execution that were not\ncovered in the training data. In this context, BS lacks two important\nproperties essential for offline RL: It does not account for the aforementioned\nuncertainty, and its greedy left-right search approach often results in\nsequences with minimal variations, failing to explore potentially better\nalternatives.\n  To address these limitations, we propose Portfolio Beam Search (PBS), a\nsimple-yet-effective alternative to BS that balances exploration and\nexploitation within a Transformer model during decoding. We draw inspiration\nfrom financial economics and apply these principles to develop an\nuncertainty-aware diversification mechanism, which we integrate into a\nsequential decoding algorithm at inference time. We empirically demonstrate the\neffectiveness of PBS on the D4RL locomotion benchmark, where it achieves higher\nreturns and significantly reduces outcome variability.",
      "tldr_zh": "本论文针对离线强化学习(Offline Reinforcement Learning)中 Transformer 模型的解码问题，指出传统 Beam Search (BS) 方法忽略了数据不确定性和探索不足，导致决策序列缺乏多样性。作者提出 Portfolio Beam Search (PBS)，一种借鉴金融算法的改进解码方法，通过不确定性感知的多样化机制在推理阶段平衡探索与利用。在 D4RL 运动基准测试中，PBS 实现了更高的回报并显著降低了结果变异性，从而提升了离线 RL 的整体性能。",
      "categories": [
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2502.10473v1",
      "published_date": "2025-02-13 15:51:46 UTC",
      "updated_date": "2025-02-13 15:51:46 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T10:55:51.516616"
    },
    {
      "arxiv_id": "2502.09423v1",
      "title": "Transformer-Enhanced Variational Autoencoder for Crystal Structure Prediction",
      "title_zh": "Transformer 增强的变分自编码器用于晶体结构预测",
      "authors": [
        "Ziyi Chen",
        "Yang Yuan",
        "Siming Zheng",
        "Jialong Guo",
        "Sihan Liang",
        "Yangang Wang",
        "Zongguo Wang"
      ],
      "abstract": "Crystal structure forms the foundation for understanding the physical and\nchemical properties of materials. Generative models have emerged as a new\nparadigm in crystal structure prediction(CSP), however, accurately capturing\nkey characteristics of crystal structures, such as periodicity and symmetry,\nremains a significant challenge. In this paper, we propose a\nTransformer-Enhanced Variational Autoencoder for Crystal Structure Prediction\n(TransVAE-CSP), who learns the characteristic distribution space of stable\nmaterials, enabling both the reconstruction and generation of crystal\nstructures. TransVAE-CSP integrates adaptive distance expansion with\nirreducible representation to effectively capture the periodicity and symmetry\nof crystal structures, and the encoder is a transformer network based on an\nequivariant dot product attention mechanism. Experimental results on the\ncarbon_24, perov_5, and mp_20 datasets demonstrate that TransVAE-CSP\noutperforms existing methods in structure reconstruction and generation tasks\nunder various modeling metrics, offering a powerful tool for crystal structure\ndesign and optimization.",
      "tldr_zh": "本文提出了一种Transformer-Enhanced Variational Autoencoder for Crystal Structure Prediction (TransVAE-CSP)，旨在解决生成模型在晶体结构预测(CSP)中捕捉周期性和对称性等关键特性的挑战。该模型通过整合自适应距离扩展和不可约表示，以及基于等变点积注意力机制的Transformer编码器，学习稳定材料的特征分布空间，实现晶体结构的精确重建和生成。在carbon_24、perov_5和mp_20数据集上的实验结果显示，TransVAE-CSP在结构重建和生成任务中优于现有方法，提供了一个强大的工具用于晶体结构设计和优化。",
      "categories": [
        "cond-mat.mtrl-sci",
        "cs.AI"
      ],
      "primary_category": "cond-mat.mtrl-sci",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2502.09423v1",
      "published_date": "2025-02-13 15:45:36 UTC",
      "updated_date": "2025-02-13 15:45:36 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T10:56:04.433561"
    },
    {
      "arxiv_id": "2502.09417v1",
      "title": "A Survey of Reinforcement Learning for Optimization in Automation",
      "title_zh": "强化学习在自动化优化中的综述",
      "authors": [
        "Ahmad Farooq",
        "Kamran Iqbal"
      ],
      "abstract": "Reinforcement Learning (RL) has become a critical tool for optimization\nchallenges within automation, leading to significant advancements in several\nareas. This review article examines the current landscape of RL within\nautomation, with a particular focus on its roles in manufacturing, energy\nsystems, and robotics. It discusses state-of-the-art methods, major challenges,\nand upcoming avenues of research within each sector, highlighting RL's capacity\nto solve intricate optimization challenges. The paper reviews the advantages\nand constraints of RL-driven optimization methods in automation. It points out\nprevalent challenges encountered in RL optimization, including issues related\nto sample efficiency and scalability; safety and robustness; interpretability\nand trustworthiness; transfer learning and meta-learning; and real-world\ndeployment and integration. It further explores prospective strategies and\nfuture research pathways to navigate these challenges. Additionally, the survey\nincludes a comprehensive list of relevant research papers, making it an\nindispensable guide for scholars and practitioners keen on exploring this\ndomain.",
      "tldr_zh": "这篇论文对 Reinforcement Learning (RL) 在自动化优化中的应用进行了全面调查，重点关注制造、能源系统和机器人领域，讨论了最先进的方法及其在解决复杂优化问题的优势。调查突出了RL面临的挑战，包括样本效率、可伸缩性、安全和鲁棒性、解释性和信任性、转移学习和元学习，以及实际部署问题，并提出了应对这些挑战的潜在策略和未来研究方向。该论文还提供了相关研究论文的详尽列表，作为学者和从业者探索该领域的宝贵指南。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.NE",
        "cs.RO",
        "cs.SY",
        "eess.SY",
        "68T05, 90C40, 49M37",
        "I.2.6; I.2.8; I.2.9; G.1.6; C.4; J.6"
      ],
      "primary_category": "cs.LG",
      "comment": "8 pages, 4 tables, and 1 figure. Accepted at IEEE 20th International\n  Conference on Automation Science and Engineering (CASE) 2024",
      "pdf_url": "http://arxiv.org/pdf/2502.09417v1",
      "published_date": "2025-02-13 15:40:39 UTC",
      "updated_date": "2025-02-13 15:40:39 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T10:56:15.717122"
    },
    {
      "arxiv_id": "2503.16440v1",
      "title": "Cause-effect perception in an object place task",
      "title_zh": "翻译失败",
      "authors": [
        "Nikolai Bahr",
        "Christoph Zetzsche",
        "Jaime Maldonado",
        "Kerstin Schill"
      ],
      "abstract": "Algorithmic causal discovery is based on formal reasoning and provably\nconverges toward the optimal solution. However, since some of the underlying\nassumptions are often not met in practice no applications for autonomous\neveryday life competence are yet available. Humans on the other hand possess\nfull everyday competence and develop cognitive models in a data efficient\nmanner with the ability to transfer knowledge between and to new situations.\nHere we investigate the causal discovery capabilities of humans in an object\nplace task in virtual reality (VR) with haptic feedback and compare the results\nto the state of the art causal discovery algorithms FGES, PC and FCI. In\naddition we use the algorithms to analyze causal relations between sensory\ninformation and the kinematic parameters of human behavior.\n  Our findings show that the majority of participants were able to determine\nwhich variables are causally related. This is in line with causal discovery\nalgorithms like PC, which recover causal dependencies in the first step.\nHowever, unlike such algorithms which can identify causes and effects in our\ntest configuration, humans are unsure in determining a causal direction.\nRegarding the relation between the sensory information provided to the\nparticipants and their placing actions (i.e. their kinematic parameters) the\ndata yields a surprising dissociation of the subjects knowledge and the\nsensorimotor level. Knowledge of the cause-effect pairs, though undirected,\nshould suffice to improve subject's movements. Yet a detailed causal analysis\nprovides little evidence for any such influence. This, together with the\nreports of the participants, implies that instead of exploiting their\nconsciously perceived information they leave it to the sensorimotor level to\ncontrol the movement.",
      "tldr_zh": "本研究探讨了人类在虚拟现实（VR）对象放置任务中的因果发现能力，并将其与算法如 FGES、PC 和 FCI 进行比较。实验通过 VR 环境和触觉反馈，评估参与者识别因果关系的能力，同时使用这些算法分析感官信息与人类运动参数（kinematic parameters）之间的因果联系。结果显示，大多数参与者能识别变量间的因果相关性，但不确定因果方向，与算法的表现存在差异。令人意外的是，参与者虽知晓因果对，但这并未显著改善他们的动作控制，暗示他们依赖感官运动水平而非有意识知识来执行任务。",
      "categories": [
        "cs.HC",
        "cs.AI"
      ],
      "primary_category": "cs.HC",
      "comment": "11 pages, 9 figures, submitted to: Frontiers in Cognition",
      "pdf_url": "http://arxiv.org/pdf/2503.16440v1",
      "published_date": "2025-02-13 15:33:10 UTC",
      "updated_date": "2025-02-13 15:33:10 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T10:56:27.577978"
    },
    {
      "arxiv_id": "2502.09687v1",
      "title": "Mind What You Ask For: Emotional and Rational Faces of Persuasion by Large Language Models",
      "title_zh": "当心你所要求的：大型语言模型说服的感性和理性面貌",
      "authors": [
        "Wiktoria Mieleszczenko-Kowszewicz",
        "Beata Bajcar",
        "Jolanta Babiak",
        "Berenika Dyczek",
        "Jakub Świstak",
        "Przemysław Biecek"
      ],
      "abstract": "Be careful what you ask for, you just might get it. This saying fits with the\nway large language models (LLMs) are trained, which, instead of being rewarded\nfor correctness, are increasingly rewarded for pleasing the recipient. So, they\nare increasingly effective at persuading us that their answers are valuable.\nBut what tricks do they use in this persuasion? In this study, we examine what\nare the psycholinguistic features of the responses used by twelve different\nlanguage models. By grouping response content according to rational or\nemotional prompts and exploring social influence principles employed by LLMs,\nwe ask whether and how we can mitigate the risks of LLM-driven mass\nmisinformation. We position this study within the broader discourse on\nhuman-centred AI, emphasizing the need for interdisciplinary approaches to\nmitigate cognitive and societal risks posed by persuasive AI responses.",
      "tldr_zh": "该研究探讨了大型语言模型（LLMs）在说服方面的情感和理性特征，分析了12个不同模型的响应内容如何通过心理语言学特征（如情感或理性提示）来影响用户。研究者分组了响应并考察了社会影响原则，揭示LLMs 可能加剧大规模错误信息风险。最终，论文强调采用人类中心AI和跨学科方法来缓解这些认知和社会风险，从而促进更可信的AI应用。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.HC"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2502.09687v1",
      "published_date": "2025-02-13 15:15:53 UTC",
      "updated_date": "2025-02-13 15:15:53 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T10:56:38.744228"
    },
    {
      "arxiv_id": "2502.09390v1",
      "title": "SQuARE: Sequential Question Answering Reasoning Engine for Enhanced Chain-of-Thought in Large Language Models",
      "title_zh": "翻译失败",
      "authors": [
        "Daniel Fleischer",
        "Moshe Berchansky",
        "Gad Markovits",
        "Moshe Wasserblat"
      ],
      "abstract": "In the rapidly evolving field of Natural Language Processing, Large Language\nModels (LLMs) are tasked with increasingly complex reasoning challenges.\nTraditional methods like chain-of-thought prompting have shown promise but\noften fall short in fully leveraging a model's reasoning capabilities. This\npaper introduces SQuARE (Sequential Question Answering Reasoning Engine), a\nnovel prompting technique designed to improve reasoning through a\nself-interrogation paradigm. Building upon CoT frameworks, SQuARE prompts\nmodels to generate and resolve multiple auxiliary questions before tackling the\nmain query, promoting a more thorough exploration of various aspects of a\ntopic. Our expansive evaluations, conducted with Llama 3 and GPT-4o models\nacross multiple question-answering datasets, demonstrate that SQuARE\nsignificantly surpasses traditional CoT prompts and existing\nrephrase-and-respond methods. By systematically decomposing queries, SQuARE\nadvances LLM capabilities in reasoning tasks. The code is publicly available at\nhttps://github.com/IntelLabs/RAG-FiT/tree/square.",
      "tldr_zh": "本文提出 SQuARE，一种新型提示技术（Sequential Question Answering Reasoning Engine），旨在提升 Large Language Models (LLMs) 在复杂推理任务中的性能，通过基于 Chain-of-Thought (CoT) 的自问自答范式，让模型在处理主查询前生成并解决多个辅助问题，以实现更全面的主题探索。相比传统 CoT 提示和 rephrase-and-respond 方法，SQuARE 在 Llama 3 和 GPT-4o 模型上的广泛评估中表现出显著优势，平均超越基线模型。研究证明，这种系统分解查询的方法有效增强了 LLMs 的推理能力，代码已开源在 GitHub（https://github.com/IntelLabs/RAG-FiT/tree/square）。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "14 pages",
      "pdf_url": "http://arxiv.org/pdf/2502.09390v1",
      "published_date": "2025-02-13 15:07:20 UTC",
      "updated_date": "2025-02-13 15:07:20 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T10:56:51.812490"
    },
    {
      "arxiv_id": "2502.09389v2",
      "title": "S$^2$-Diffusion: Generalizing from Instance-level to Category-level Skills in Robot Manipulation",
      "title_zh": "翻译失败",
      "authors": [
        "Quantao Yang",
        "Michael C. Welle",
        "Danica Kragic",
        "Olov Andersson"
      ],
      "abstract": "Recent advances in skill learning has propelled robot manipulation to new\nheights by enabling it to learn complex manipulation tasks from a practical\nnumber of demonstrations. However, these skills are often limited to the\nparticular action, object, and environment \\textit{instances} that are shown in\nthe training data, and have trouble transferring to other instances of the same\ncategory. In this work we present an open-vocabulary Spatial-Semantic Diffusion\npolicy (S$^2$-Diffusion) which enables generalization from instance-level\ntraining data to category-level, enabling skills to be transferable between\ninstances of the same category. We show that functional aspects of skills can\nbe captured via a promptable semantic module combined with a spatial\nrepresentation. We further propose leveraging depth estimation networks to\nallow the use of only a single RGB camera. Our approach is evaluated and\ncompared on a diverse number of robot manipulation tasks, both in simulation\nand in the real world. Our results show that S$^2$-Diffusion is invariant to\nchanges in category-irrelevant factors as well as enables satisfying\nperformance on other instances within the same category, even if it was not\ntrained on that specific instance. Full videos of all real-world experiments\nare available in the supplementary material.",
      "tldr_zh": "该研究提出了一种开放词汇的 Spatial-Semantic Diffusion 政策（S$^2$-Diffusion），旨在将机器人操作技能从实例级训练数据推广到类别级，从而实现技能在同一类别不同实例间的转移。方法结合可提示的语义模块和空间表示来捕捉技能的功能方面，并利用深度估计网络，仅需一个 RGB 相机即可执行任务。在模拟和真实世界多种机器人操作任务的评估中，S$^2$-Diffusion 显示出对类别无关因素的鲁棒性，并能在未训练的实例上实现满意性能。",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2502.09389v2",
      "published_date": "2025-02-13 15:06:42 UTC",
      "updated_date": "2025-02-17 08:38:28 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T10:57:03.422176"
    },
    {
      "arxiv_id": "2502.09387v2",
      "title": "Truth Knows No Language: Evaluating Truthfulness Beyond English",
      "title_zh": "翻译失败",
      "authors": [
        "Blanca Calvo Figueras",
        "Eneko Sagarzazu",
        "Julen Etxaniz",
        "Jeremy Barnes",
        "Pablo Gamallo",
        "Iria De Dios Flores",
        "Rodrigo Agerri"
      ],
      "abstract": "We introduce a professionally translated extension of the TruthfulQA\nbenchmark designed to evaluate truthfulness in Basque, Catalan, Galician, and\nSpanish. Truthfulness evaluations of large language models (LLMs) have\nprimarily been conducted in English. However, the ability of LLMs to maintain\ntruthfulness across languages remains under-explored. Our study evaluates 12\nstate-of-the-art open LLMs, comparing base and instruction-tuned models using\nhuman evaluation, multiple-choice metrics, and LLM-as-a-Judge scoring. Our\nfindings reveal that, while LLMs perform best in English and worst in Basque\n(the lowest-resourced language), overall truthfulness discrepancies across\nlanguages are smaller than anticipated. Furthermore, we show that\nLLM-as-a-Judge correlates more closely with human judgments than\nmultiple-choice metrics, and that informativeness plays a critical role in\ntruthfulness assessment. Our results also indicate that machine translation\nprovides a viable approach for extending truthfulness benchmarks to additional\nlanguages, offering a scalable alternative to professional translation.\nFinally, we observe that universal knowledge questions are better handled\nacross languages than context- and time-dependent ones, highlighting the need\nfor truthfulness evaluations that account for cultural and temporal\nvariability. Dataset and code are publicly available under open licenses.",
      "tldr_zh": "该研究扩展了TruthfulQA基准，将其翻译成巴斯克语、加泰罗尼亚语、加利西亚语和西班牙语，以评估大型语言模型（LLMs）的跨语言真实性。研究评估了12个开源LLMs，包括基础模型和指令微调模型，通过人类评估、多选题指标和LLM-as-a-Judge评分进行比较。结果显示，LLMs在英语中表现最佳，在资源最少的巴斯克语中最差，但整体跨语言真实性差异小于预期，且LLM-as-a-Judge与人类判断的相关性高于多选题指标。信息性在真实性评估中起关键作用，机器翻译被证明是扩展此类基准的可扩展替代方案。数据集和代码已公开可用。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.CY"
      ],
      "primary_category": "cs.CL",
      "comment": "14 pages, 6 figures, 8 tables",
      "pdf_url": "http://arxiv.org/pdf/2502.09387v2",
      "published_date": "2025-02-13 15:04:53 UTC",
      "updated_date": "2025-02-18 09:35:45 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T10:57:17.298051"
    },
    {
      "arxiv_id": "2502.09379v2",
      "title": "TRIFFID: Autonomous Robotic Aid For Increasing First Responders Efficiency",
      "title_zh": "TRIFFID：用于提升第一响应者效率的自治机器人",
      "authors": [
        "Jorgen Cani",
        "Panagiotis Koletsis",
        "Konstantinos Foteinos",
        "Ioannis Kefaloukos",
        "Lampros Argyriou",
        "Manolis Falelakis",
        "Iván Del Pino",
        "Angel Santamaria-Navarro",
        "Martin Čech",
        "Ondřej Severa",
        "Alessandro Umbrico",
        "Francesca Fracasso",
        "AndreA Orlandini",
        "Dimitrios Drakoulis",
        "Evangelos Markakis",
        "Iraklis Varlamis",
        "Georgios Th. Papadopoulos"
      ],
      "abstract": "The increasing complexity of natural disaster incidents demands innovative\ntechnological solutions to support first responders in their efforts. This\npaper introduces the TRIFFID system, a comprehensive technical framework that\nintegrates unmanned ground and aerial vehicles with advanced artificial\nintelligence functionalities to enhance disaster response capabilities across\nwildfires, urban floods, and post-earthquake search and rescue missions. By\nleveraging state-of-the-art autonomous navigation, semantic perception, and\nhuman-robot interaction technologies, TRIFFID provides a sophisticated system\ncomposed of the following key components: hybrid robotic platform, centralized\nground station, custom communication infrastructure, and smartphone\napplication. The defined research and development activities demonstrate how\ndeep neural networks, knowledge graphs, and multimodal information fusion can\nenable robots to autonomously navigate and analyze disaster environments,\nreducing personnel risks and accelerating response times. The proposed system\nenhances emergency response teams by providing advanced mission planning,\nsafety monitoring, and adaptive task execution capabilities. Moreover, it\nensures real-time situational awareness and operational support in complex and\nrisky situations, facilitating rapid and precise information collection and\ncoordinated actions.",
      "tldr_zh": "这篇论文介绍了 TRIFFID 系统，这是一个整合无人地面和空中车辆的综合框架，旨在提升第一响应者在野火、城市洪水和地震后搜救等灾害中的响应效率。系统利用自主导航、语义感知和人机交互技术，以及深度神经网络、知识图谱和多模态信息融合，实现机器人自主导航、环境分析和任务执行，从而减少人员风险并加速响应时间。TRIFFID 的关键组件包括混合机器人平台、中央地面站、自定义通信基础设施和智能手机应用，提供高级任务规划、安全监控以及实时 situational awareness，以支持协调行动和精确信息收集。",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2502.09379v2",
      "published_date": "2025-02-13 14:46:40 UTC",
      "updated_date": "2025-02-27 09:07:06 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T10:57:27.332800"
    },
    {
      "arxiv_id": "2502.09378v1",
      "title": "A Deep Inverse-Mapping Model for a Flapping Robotic Wing",
      "title_zh": "一种用于扑翼机器人翅",
      "authors": [
        "Hadar Sharvit",
        "Raz Karl",
        "Tsevi Beatus"
      ],
      "abstract": "In systems control, the dynamics of a system are governed by modulating its\ninputs to achieve a desired outcome. For example, to control the thrust of a\nquad-copter propeller the controller modulates its rotation rate, relying on a\nstraightforward mapping between the input rotation rate and the resulting\nthrust. This mapping can be inverted to determine the rotation rate needed to\ngenerate a desired thrust. However, in complex systems, such as flapping-wing\nrobots where intricate fluid motions are involved, mapping inputs (wing\nkinematics) to outcomes (aerodynamic forces) is nontrivial and inverting this\nmapping for real-time control is computationally impractical. Here, we report a\nmachine-learning solution for the inverse mapping of a flapping-wing system\nbased on data from an experimental system we have developed. Our model learns\nthe input wing motion required to generate a desired aerodynamic force outcome.\nWe used a sequence-to-sequence model tailored for time-series data and\naugmented it with a novel adaptive-spectrum layer that implements\nrepresentation learning in the frequency domain. To train our model, we\ndeveloped a flapping wing system that simultaneously measures the wing's\naerodynamic force and its 3D motion using high-speed cameras. We demonstrate\nthe performance of our system on an additional open-source dataset of a\nflapping wing in a different flow regime. Results show superior performance\ncompared with more complex state-of-the-art transformer-based models, with 11%\nimprovement on the test datasets median loss. Moreover, our model shows\nsuperior inference time, making it practical for onboard robotic control. Our\nopen-source data and framework may improve modeling and real-time control of\nsystems governed by complex dynamics, from biomimetic robots to biomedical\ndevices.",
      "tldr_zh": "本文提出了一种深度反向映射模型，用于扑翼机器人翅膀的控制，解决复杂流体动力学系统中从输入翼运动到输出空气动力力的逆向映射难题。模型基于sequence-to-sequence架构，并引入了新型adaptive-spectrum layer，在频率域进行表示学习，通过实验系统收集的数据进行训练。实验结果显示，该模型在测试数据集上比state-of-the-art的transformer-based模型提高了11%的中位损失，并显著缩短了推理时间，适用于实时机器人控制。该框架及其开源数据可推广到其他复杂动态系统，如仿生机器人和生物医学设备。",
      "categories": [
        "cs.AI",
        "cs.RO"
      ],
      "primary_category": "cs.AI",
      "comment": "Accepted to ICLR 2025. 10 Pages 5 figures + 2 figures in appendix",
      "pdf_url": "http://arxiv.org/pdf/2502.09378v1",
      "published_date": "2025-02-13 14:46:04 UTC",
      "updated_date": "2025-02-13 14:46:04 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T10:57:40.285047"
    },
    {
      "arxiv_id": "2502.09369v1",
      "title": "Language Agents as Digital Representatives in Collective Decision-Making",
      "title_zh": "语言代理作为数字代表在集体决策中",
      "authors": [
        "Daniel Jarrett",
        "Miruna Pîslar",
        "Michiel A. Bakker",
        "Michael Henry Tessler",
        "Raphael Köster",
        "Jan Balaguer",
        "Romuald Elie",
        "Christopher Summerfield",
        "Andrea Tacchetti"
      ],
      "abstract": "Consider the process of collective decision-making, in which a group of\nindividuals interactively select a preferred outcome from among a universe of\nalternatives. In this context, \"representation\" is the activity of making an\nindividual's preferences present in the process via participation by a proxy\nagent -- i.e. their \"representative\". To this end, learned models of human\nbehavior have the potential to fill this role, with practical implications for\nmulti-agent scenario studies and mechanism design. In this work, we investigate\nthe possibility of training \\textit{language agents} to behave in the capacity\nof representatives of human agents, appropriately expressing the preferences of\nthose individuals whom they stand for. First, we formalize the setting of\n\\textit{collective decision-making} -- as the episodic process of interaction\nbetween a group of agents and a decision mechanism. On this basis, we then\nformalize the problem of \\textit{digital representation} -- as the simulation\nof an agent's behavior to yield equivalent outcomes from the mechanism.\nFinally, we conduct an empirical case study in the setting of\n\\textit{consensus-finding} among diverse humans, and demonstrate the\nfeasibility of fine-tuning large language models to act as digital\nrepresentatives.",
      "tldr_zh": "该论文探讨了在集体决策（collective decision-making）过程中，使用 language agents 作为人类代理的数字代表（digital representation），以有效表达个体的偏好。作者形式化了集体决策作为多代理互动过程，并定义了数字代表问题为模拟代理行为以获得等效决策结果。随后，通过实证案例研究在共识寻找（consensus-finding）场景中，证明了微调 large language models 的可行性，使其充当可靠的数字代表。总体而言，此工作为多代理场景研究和机制设计提供了新颖的实践启示。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL",
        "cs.CY"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2502.09369v1",
      "published_date": "2025-02-13 14:35:40 UTC",
      "updated_date": "2025-02-13 14:35:40 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T10:57:51.040157"
    },
    {
      "arxiv_id": "2502.09365v1",
      "title": "Simple Path Structural Encoding for Graph Transformers",
      "title_zh": "翻译失败",
      "authors": [
        "Louis Airale",
        "Antonio Longa",
        "Mattia Rigon",
        "Andrea Passerini",
        "Roberto Passerone"
      ],
      "abstract": "Graph transformers extend global self-attention to graph-structured data,\nachieving notable success in graph learning. Recently, random walk structural\nencoding (RWSE) has been found to further enhance their predictive power by\nencoding both structural and positional information into the edge\nrepresentation. However, RWSE cannot always distinguish between edges that\nbelong to different local graph patterns, which reduces its ability to capture\nthe full structural complexity of graphs. This work introduces Simple Path\nStructural Encoding (SPSE), a novel method that utilizes simple path counts for\nedge encoding. We show theoretically and experimentally that SPSE overcomes the\nlimitations of RWSE, providing a richer representation of graph structures,\nparticularly for capturing local cyclic patterns. To make SPSE computationally\ntractable, we propose an efficient approximate algorithm for simple path\ncounting. SPSE demonstrates significant performance improvements over RWSE on\nvarious benchmarks, including molecular and long-range graph datasets,\nachieving statistically significant gains in discriminative tasks. These\nresults pose SPSE as a powerful edge encoding alternative for enhancing the\nexpressivity of graph transformers.",
      "tldr_zh": "该论文提出 Simple Path Structural Encoding (SPSE)，一种新颖的边编码方法，用于提升 Graph Transformers 在图结构数据处理中的表现，通过计算简单路径计数来捕捉更丰富的结构和位置信息。相比于现有的 Random Walk Structural Encoding (RWSE)，SPSE 理论上和实验上证明了其在区分局部图模式（如循环模式）方面的优势，并引入了一个高效的近似算法以确保计算可行性。在分子和长距离图数据集的基准测试中，SPSE 实现了统计显著的性能提升，使其成为增强 Graph Transformers 表达力的强大替代方案。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2502.09365v1",
      "published_date": "2025-02-13 14:33:02 UTC",
      "updated_date": "2025-02-13 14:33:02 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T10:58:04.452720"
    },
    {
      "arxiv_id": "2502.09341v1",
      "title": "Neural Spatiotemporal Point Processes: Trends and Challenges",
      "title_zh": "神经时空点过程：趋势和挑战",
      "authors": [
        "Sumantrak Mukherjee",
        "Mouad Elhamdi",
        "George Mohler",
        "David A. Selby",
        "Yao Xie",
        "Sebastian Vollmer",
        "Gerrit Grossmann"
      ],
      "abstract": "Spatiotemporal point processes (STPPs) are probabilistic models for events\noccurring in continuous space and time. Real-world event data often exhibit\nintricate dependencies and heterogeneous dynamics. By incorporating modern deep\nlearning techniques, STPPs can model these complexities more effectively than\ntraditional approaches. Consequently, the fusion of neural methods with STPPs\nhas become an active and rapidly evolving research area. In this review, we\ncategorize existing approaches, unify key design choices, and explain the\nchallenges of working with this data modality. We further highlight emerging\ntrends and diverse application domains. Finally, we identify open challenges\nand gaps in the literature.",
      "tldr_zh": "本综述探讨了时空点过程 (STPPs)，一种用于建模连续空间和时间中事件的概率模型，尤其关注真实事件数据的复杂依赖性和异质动态。通过整合深度学习技术，神经方法能比传统方法更有效地处理这些复杂性。论文对现有神经STPPs方法进行了分类，统一了关键设计选择，并分析了数据模式面临的挑战，同时突出了新兴趋势、多样应用领域以及文献中的开放问题和研究空白。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2502.09341v1",
      "published_date": "2025-02-13 14:01:15 UTC",
      "updated_date": "2025-02-13 14:01:15 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T10:58:14.976684"
    },
    {
      "arxiv_id": "2502.09335v1",
      "title": "Graph Diffusion Network for Drug-Gene Prediction",
      "title_zh": "用于药物-基因预测的图扩散网络",
      "authors": [
        "Jiayang Wu",
        "Wensheng Gan",
        "Philip S. Yu"
      ],
      "abstract": "Predicting drug-gene associations is crucial for drug development and disease\ntreatment. While graph neural networks (GNN) have shown effectiveness in this\ntask, they face challenges with data sparsity and efficient contrastive\nlearning implementation. We introduce a graph diffusion network for drug-gene\nprediction (GDNDGP), a framework that addresses these limitations through two\nkey innovations. First, it employs meta-path-based homogeneous graph learning\nto capture drug-drug and gene-gene relationships, ensuring similar entities\nshare embedding spaces. Second, it incorporates a parallel diffusion network\nthat generates hard negative samples during training, eliminating the need for\nexhaustive negative sample retrieval. Our model achieves superior performance\non the DGIdb 4.0 dataset and demonstrates strong generalization capability on\ntripartite drug-gene-disease networks. Results show significant improvements\nover existing methods in drug-gene prediction tasks, particularly in handling\ncomplex heterogeneous relationships. The source code is publicly available at\nhttps://github.com/csjywu1/GDNDGP.",
      "tldr_zh": "本文提出Graph Diffusion Network for Drug-Gene Prediction (GDNDGP)框架，用于预测药物-基因关联，解决Graph Neural Networks (GNN)面临的数据稀疏性和高效对比学习实现的问题。GDNDGP的关键创新包括基于meta-path的同质图学习，以捕获药物-药物和基因-基因关系，确保相似实体共享嵌入空间；以及并行diffusion network，在训练中生成硬负样本，消除详尽负样本检索的需求。在DGIdb 4.0数据集上，该模型表现出优越性能，并在三元药物-基因-疾病网络中显示出强泛化能力，与现有方法相比显著提升了处理复杂异质关系的效果。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "IEEE/ACM TCBB. 14 pages",
      "pdf_url": "http://arxiv.org/pdf/2502.09335v1",
      "published_date": "2025-02-13 13:54:58 UTC",
      "updated_date": "2025-02-13 13:54:58 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T10:58:28.187969"
    },
    {
      "arxiv_id": "2502.09307v1",
      "title": "When the LM misunderstood the human chuckled: Analyzing garden path effects in humans and language models",
      "title_zh": "翻译失败",
      "authors": [
        "Samuel Joseph Amouyal",
        "Aya Meltzer-Asscher",
        "Jonathan Berant"
      ],
      "abstract": "Modern Large Language Models (LLMs) have shown human-like abilities in many\nlanguage tasks, sparking interest in comparing LLMs' and humans' language\nprocessing. In this paper, we conduct a detailed comparison of the two on a\nsentence comprehension task using garden-path constructions, which are\nnotoriously challenging for humans. Based on psycholinguistic research, we\nformulate hypotheses on why garden-path sentences are hard, and test these\nhypotheses on human participants and a large suite of LLMs using comprehension\nquestions. Our findings reveal that both LLMs and humans struggle with specific\nsyntactic complexities, with some models showing high correlation with human\ncomprehension. To complement our findings, we test LLM comprehension of\ngarden-path constructions with paraphrasing and text-to-image generation tasks,\nand find that the results mirror the sentence comprehension question results,\nfurther validating our findings on LLM understanding of these constructions.",
      "tldr_zh": "本研究比较了大型语言模型（LLMs）和人类的语言处理能力，焦点在于处理“garden-path constructions”（花园路径结构）时的挑战，这些结构因句法复杂性而难懂。研究者基于心理语言学假设，通过理解问题、改述和文本到图像生成任务测试人类参与者和多种 LLMs。结果显示，LLMs 和人类在特定句法复杂性上均表现出挣扎，但某些模型与人类理解高度相关，进一步验证了 LLMs 在这些结构上的认知局限性。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2502.09307v1",
      "published_date": "2025-02-13 13:19:33 UTC",
      "updated_date": "2025-02-13 13:19:33 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T10:58:38.568854"
    },
    {
      "arxiv_id": "2502.09305v1",
      "title": "Predicting Drive Test Results in Mobile Networks Using Optimization Techniques",
      "title_zh": "使用优化技术预测",
      "authors": [
        "MohammadJava Taheri",
        "Abolfazl Diyanat",
        "MortezaAli Ahmadi",
        "Ali Nazari"
      ],
      "abstract": "Mobile network operators constantly optimize their networks to ensure\nsuperior service quality and coverage. This optimization is crucial for\nmaintaining an optimal user experience and requires extensive data collection\nand analysis. One of the primary methods for gathering this data is through\ndrive tests, where technical teams use specialized equipment to collect signal\ninformation across various regions. However, drive tests are both costly and\ntime-consuming, and they face challenges such as traffic conditions,\nenvironmental factors, and limited access to certain areas. These constraints\nmake it difficult to replicate drive tests under similar conditions. In this\nstudy, we propose a method that enables operators to predict received signal\nstrength at specific locations using data from other drive test points. By\nreducing the need for widespread drive tests, this approach allows operators to\nsave time and resources while still obtaining the necessary data to optimize\ntheir networks and mitigate the challenges associated with traditional drive\ntests.",
      "tldr_zh": "本研究针对移动网络优化中的 drive tests 问题，提出了一种使用 optimization techniques 的预测方法，利用现有 drive test 点的数据来估算特定位置的 received signal strength，从而减少实际测试的频率和范围。传统 drive tests 因成本高、耗时长以及受交通条件和环境因素影响而存在局限，该方法有效缓解这些挑战。总体而言，此方法帮助运营商节省资源，同时维持网络优化和用户体验的提升。",
      "categories": [
        "cs.NI",
        "cs.AI",
        "cs.SE"
      ],
      "primary_category": "cs.NI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2502.09305v1",
      "published_date": "2025-02-13 13:17:31 UTC",
      "updated_date": "2025-02-13 13:17:31 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T10:58:50.475644"
    },
    {
      "arxiv_id": "2502.09294v1",
      "title": "Indeterminacy in Affective Computing: Considering Meaning and Context in Data Collection Practices",
      "title_zh": "翻译失败",
      "authors": [
        "Bernd Dudzik",
        "Tiffany Matej Hrkalovic",
        "Chenxu Hao",
        "Chirag Raman",
        "Masha Tsfasman"
      ],
      "abstract": "Automatic Affect Prediction (AAP) uses computational analysis of input data\nsuch as text, speech, images, and physiological signals to predict various\naffective phenomena (e.g., emotions or moods). These models are typically\nconstructed using supervised machine-learning algorithms, which rely heavily on\nlabeled training datasets. In this position paper, we posit that all AAP\ntraining data are derived from human Affective Interpretation Processes,\nresulting in a form of Affective Meaning. Research on human affect indicates a\nform of complexity that is fundamental to such meaning: it can possess what we\nrefer to here broadly as Qualities of Indeterminacy (QIs) - encompassing\nSubjectivity (meaning depends on who is interpreting), Uncertainty (lack of\nconfidence regarding meanings' correctness), Ambiguity (meaning contains\nmutually exclusive concepts) and Vagueness (meaning is situated at different\nlevels in a nested hierarchy). Failing to appropriately consider QIs leads to\nresults incapable of meaningful and reliable predictions. Based on this\npremise, we argue that a crucial step in adequately addressing indeterminacy in\nAAP is the development of data collection practices for modeling corpora that\ninvolve the systematic consideration of 1) a relevant set of QIs and 2) context\nfor the associated interpretation processes. To this end, we are 1) outlining a\nconceptual model of AIPs and the QIs associated with the meaning these produce\nand a conceptual structure of relevant context, supporting understanding of its\nrole. Finally, we use our framework for 2) discussing examples of\ncontext-sensitivity-related challenges for addressing QIs in data collection\nsetups. We believe our efforts can stimulate a structured discussion of both\nthe role of aspects of indeterminacy and context in research on AAP, informing\nthe development of better practices for data collection and analysis.",
      "tldr_zh": "这篇论文讨论了Automatic Affect Prediction (AAP)系统在预测情感现象（如情绪或心情）时面临的挑战，强调所有训练数据都源于人类的Affective Interpretation Processes (AIPs)，从而产生带有Qualities of Indeterminacy (QIs)的Affective Meaning，包括Subjectivity（主观性）、Uncertainty（不确定性）、Ambiguity（歧义性）和Vagueness（模糊性）。作者主张，忽略这些QIs会导致不可靠的预测，因此需要开发新的数据收集实践，以系统考虑相关QIs和上下文。论文提出一个概念模型，概述AIPs、QIs以及上下文的结构，并通过讨论实际挑战来促进AAP研究中更好地处理不确定性和上下文敏感性，从而改进数据收集和分析方法。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "Accepted at: 12th International Conference on Affective Computing and\n  Intelligent Interaction Workshops and Demos (ACIIW)",
      "pdf_url": "http://arxiv.org/pdf/2502.09294v1",
      "published_date": "2025-02-13 13:08:42 UTC",
      "updated_date": "2025-02-13 13:08:42 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T10:59:03.759764"
    },
    {
      "arxiv_id": "2502.09284v2",
      "title": "SparQLe: Speech Queries to Text Translation Through LLMs",
      "title_zh": "翻译失败",
      "authors": [
        "Amirbek Djanibekov",
        "Hanan Aldarmaki"
      ],
      "abstract": "With the growing influence of Large Language Models (LLMs), there is\nincreasing interest in integrating speech representations with them to enable\nmore seamless multi-modal processing and speech understanding. This study\nintroduces a novel approach that leverages self-supervised speech\nrepresentations in combination with instruction-tuned LLMs for speech-to-text\ntranslation. The proposed approach leverages a modality adapter to align\nextracted speech features with instruction-tuned LLMs using English-language\ndata. Our experiments demonstrate that this method effectively preserves the\nsemantic content of the input speech and serves as an effective bridge between\nself-supervised speech models and instruction-tuned LLMs, offering a promising\nsolution for various speech understanding applications.",
      "tldr_zh": "这项研究提出SparQLe，一种新方法，将自监督语音表示(self-supervised speech representations)与指令调整的大型语言模型(LLMs)结合，用于语音查询到文本翻译。该方法通过模态适配器(modality adapter)利用英语数据对齐提取的语音特征和LLMs，确保翻译过程有效保留输入语音的语义内容。实验结果表明，SparQLe成功桥接自监督语音模型和指令调整LLMs，为各种语音理解应用提供了一个有前景的解决方案。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2502.09284v2",
      "published_date": "2025-02-13 12:57:15 UTC",
      "updated_date": "2025-04-19 20:20:36 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T10:59:14.846527"
    },
    {
      "arxiv_id": "2502.09271v3",
      "title": "LiSA: Leveraging Link Recommender to Attack Graph Neural Networks via Subgraph Injection",
      "title_zh": "翻译失败",
      "authors": [
        "Wenlun Zhang",
        "Enyan Dai",
        "Kentaro Yoshioka"
      ],
      "abstract": "Graph Neural Networks (GNNs) have demonstrated remarkable proficiency in\nmodeling data with graph structures, yet recent research reveals their\nsusceptibility to adversarial attacks. Traditional attack methodologies, which\nrely on manipulating the original graph or adding links to artificially created\nnodes, often prove impractical in real-world settings. This paper introduces a\nnovel adversarial scenario involving the injection of an isolated subgraph to\ndeceive both the link recommender and the node classifier within a GNN system.\nSpecifically, the link recommender is mislead to propose links between targeted\nvictim nodes and the subgraph, encouraging users to unintentionally establish\nconnections and that would degrade the node classification accuracy, thereby\nfacilitating a successful attack. To address this, we present the LiSA\nframework, which employs a dual surrogate model and bi-level optimization to\nsimultaneously meet two adversarial objectives. Extensive experiments on\nreal-world datasets demonstrate the effectiveness of our method.",
      "tldr_zh": "本文研究了Graph Neural Networks (GNNs) 在图结构数据建模中的易受攻击性，提出一种新型攻击场景：通过注入孤立子图(subgraph injection)来欺骗链接推荐器(link recommender)，诱导目标节点与子图建立连接，从而降低节点分类器(node classifier)的准确率。LiSA框架采用双代理模型(dual surrogate model)和双层优化(bi-level optimization)来同时实现两个攻击目标，确保攻击的针对性和有效性。在真实数据集上的广泛实验证明，该方法显著提高了攻击成功率，突显了GNNs 的潜在漏洞。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "PAKDD 2025",
      "pdf_url": "http://arxiv.org/pdf/2502.09271v3",
      "published_date": "2025-02-13 12:33:39 UTC",
      "updated_date": "2025-02-25 07:50:20 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T10:59:28.872866"
    },
    {
      "arxiv_id": "2503.16439v1",
      "title": "DreamLLM-3D: Affective Dream Reliving using Large Language Model and 3D Generative AI",
      "title_zh": "翻译失败",
      "authors": [
        "Pinyao Liu",
        "Keon Ju Lee",
        "Alexander Steinmaurer",
        "Claudia Picard-Deland",
        "Michelle Carr",
        "Alexandra Kitson"
      ],
      "abstract": "We present DreamLLM-3D, a composite multimodal AI system behind an immersive\nart installation for dream re-experiencing. It enables automated dream content\nanalysis for immersive dream-reliving, by integrating a Large Language Model\n(LLM) with text-to-3D Generative AI. The LLM processes voiced dream reports to\nidentify key dream entities (characters and objects), social interaction, and\ndream sentiment. The extracted entities are visualized as dynamic 3D point\nclouds, with emotional data influencing the color and soundscapes of the\nvirtual dream environment. Additionally, we propose an experiential\nAI-Dreamworker Hybrid paradigm. Our system and paradigm could potentially\nfacilitate a more emotionally engaging dream-reliving experience, enhancing\npersonal insights and creativity.",
      "tldr_zh": "我们提出了 DreamLLM-3D，一种整合 Large Language Model (LLM) 和 3D Generative AI 的多模态系统，用于自动化分析语音梦境报告并实现沉浸式梦境再体验。系统通过 LLM 识别梦境中的关键实体（如人物和物体）、社会互动以及情感，并将这些元素可视化为动态 3D 点云，同时利用情感数据调整虚拟环境的颜色和音景。最终，该系统提出 AI-Dreamworker 混合范式，可能增强用户的情感参与度，促进个人洞见和创意。",
      "categories": [
        "cs.HC",
        "cs.AI",
        "cs.MM"
      ],
      "primary_category": "cs.HC",
      "comment": "8 pages, 3 figures, Accepted by NeurIPS creative AI track 2024",
      "pdf_url": "http://arxiv.org/pdf/2503.16439v1",
      "published_date": "2025-02-13 12:29:55 UTC",
      "updated_date": "2025-02-13 12:29:55 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T10:59:42.467898"
    },
    {
      "arxiv_id": "2502.09257v1",
      "title": "Bandit Multiclass List Classification",
      "title_zh": "翻译失败",
      "authors": [
        "Liad Erez",
        "Tomer Koren"
      ],
      "abstract": "We study the problem of multiclass list classification with (semi-)bandit\nfeedback, where input examples are mapped into subsets of size $m$ of a\ncollection of $K$ possible labels, and the feedback consists of the predicted\nlabels which lie in the set of true labels of the given example. Our main\nresult is for the $(\\varepsilon,\\delta)$-PAC variant of the problem for which\nwe design an algorithm that returns an $\\varepsilon$-optimal hypothesis with\nhigh probability using a sample complexity of $O \\big( (\\mathrm{poly}(K/m) + sm\n/ \\varepsilon^2) \\log (|H|/\\delta) \\big)$ where $H$ is the underlying (finite)\nhypothesis class and $s$ is an upper bound on the number of true labels for a\ngiven example. This bound improves upon known bounds for combinatorial\nsemi-bandits whenever $s \\ll K$. Moreover, in the regime where $s = O(1)$ the\nleading terms in our bound match the corresponding full-information rates,\nimplying that bandit feedback essentially comes at no cost. Our PAC learning\nalgorithm is also computationally efficient given access to an ERM oracle for\n$H$. Additionally, we consider the regret minimization setting where data can\nbe generated adversarially, and establish a regret bound of $\\widetilde O(|H| +\n\\sqrt{smT \\log |H|})$. Our results generalize and extend those of Erez et al.\n(2024) who consider the simpler single-label setting corresponding to $s=m=1$,\nand in fact hold for the more general contextual combinatorial semi-bandit\nproblem with $s$-sparse rewards.",
      "tldr_zh": "本研究探讨了多类列表分类问题，使用Bandit反馈，其中输入映射到K个标签的m大小子集，并反馈预测标签是否在真实标签集中。主要贡献是提出一个(ε, δ)-PAC算法，其样本复杂度为O( (poly(K/m) + sm / ε²) log(|H|/δ) )，其中H是假设类，s是真实标签的上限；当s << K时，该复杂度优于现有组合Bandit边界，且当s = O(1)时，Bandit反馈几乎不增加成本。该算法在有ERM oracle时计算高效；此外，在遗憾最小化设置中，遗憾边界为Õ(|H| + √(smT log |H|))，并扩展了Erez et al. (2024)的单标签结果到更一般的上下文组合Bandit问题。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "stat.ML"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2502.09257v1",
      "published_date": "2025-02-13 12:13:25 UTC",
      "updated_date": "2025-02-13 12:13:25 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T10:59:52.299912"
    },
    {
      "arxiv_id": "2502.09256v1",
      "title": "DynSegNet:Dynamic Architecture Adjustment for Adversarial Learning in Segmenting Hemorrhagic Lesions from Fundus Images",
      "title_zh": "翻译失败",
      "authors": [
        "Zesheng Li",
        "Minwen Liao",
        "Haoran Chen",
        "Yan Su",
        "Chengchang Pan",
        "Honggang Qi"
      ],
      "abstract": "The hemorrhagic lesion segmentation plays a critical role in ophthalmic\ndiagnosis, directly influencing early disease detection, treatment planning,\nand therapeutic efficacy evaluation. However, the task faces significant\nchallenges due to lesion morphological variability, indistinct boundaries, and\nlow contrast with background tissues. To improve diagnostic accuracy and\ntreatment outcomes, developing advanced segmentation techniques remains\nimperative. This paper proposes an adversarial learning-based dynamic\narchitecture adjustment approach that integrates hierarchical U-shaped\nencoder-decoder, residual blocks, attention mechanisms, and ASPP modules. By\ndynamically optimizing feature fusion, our method enhances segmentation\nperformance. Experimental results demonstrate a Dice coefficient of 0.6802, IoU\nof 0.5602, Recall of 0.766, Precision of 0.6525, and Accuracy of 0.9955,\neffectively addressing the challenges in fundus image hemorrhage\nsegmentation.[* Corresponding author.]",
      "tldr_zh": "这篇论文提出 DynSegNet，一种基于 adversarial learning 的动态架构调整方法，用于眼底图像中出血性病变分割，以应对病变形态变化、边界不清晰和背景对比度低的挑战。该方法整合了层次化的 U-shaped encoder-decoder、residual blocks、attention mechanisms 和 ASPP modules，通过动态优化特征融合来提升分割性能。实验结果显示，Dice coefficient 为 0.6802、IoU 为 0.5602、Recall 为 0.766、Precision 为 0.6525 和 Accuracy 为 0.9955，显著提高了眼科诊断的准确性和治疗效果。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "12 pages,4 figures",
      "pdf_url": "http://arxiv.org/pdf/2502.09256v1",
      "published_date": "2025-02-13 12:11:58 UTC",
      "updated_date": "2025-02-13 12:11:58 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T11:00:05.716963"
    },
    {
      "arxiv_id": "2502.09254v1",
      "title": "AnomalyGFM: Graph Foundation Model for Zero/Few-shot Anomaly Detection",
      "title_zh": "翻译失败",
      "authors": [
        "Hezhe Qiao",
        "Chaoxi Niu",
        "Ling Chen",
        "Guansong Pang"
      ],
      "abstract": "Graph anomaly detection (GAD) aims to identify abnormal nodes that differ\nfrom the majority of the nodes in a graph, which has been attracting\nsignificant attention in recent years. Existing generalist graph models have\nachieved remarkable success in different graph tasks but struggle to generalize\nto the GAD task. This limitation arises from their difficulty in learning\ngeneralized knowledge for capturing the inherently infrequent, irregular and\nheterogeneous abnormality patterns in graphs from different domains. To address\nthis challenge, we propose AnomalyGFM, a GAD-oriented graph foundation model\nthat supports zero-shot inference and few-shot prompt tuning for GAD in diverse\ngraph datasets. One key insight is that graph-agnostic representations for\nnormal and abnormal classes are required to support effective zero/few-shot GAD\nacross different graphs. Motivated by this, AnomalyGFM is pre-trained to align\ndata-independent, learnable normal and abnormal class prototypes with node\nrepresentation residuals (i.e., representation deviation of a node from its\nneighbors). The residual features essentially project the node information into\na unified feature space where we can effectively measure the abnormality of\nnodes from different graphs in a consistent way. This provides a driving force\nfor the learning of graph-agnostic, discriminative prototypes for the normal\nand abnormal classes, which can be used to enable zero-shot GAD on new graphs,\nincluding very large-scale graphs. If there are few-shot labeled normal nodes\navailable in the new graphs, AnomalyGFM can further support prompt tuning to\nleverage these nodes for better adaptation. Comprehensive experiments on 11\nwidely-used GAD datasets with real anomalies, demonstrate that AnomalyGFM\nsignificantly outperforms state-of-the-art competing methods under both zero-\nand few-shot GAD settings.",
      "tldr_zh": "本文提出 AnomalyGFM，一种面向图异常检测 (GAD) 的图基础模型，支持零样本和少样本提示调优，以解决现有通用图模型在捕捉不同领域异常模式时的泛化困难。AnomalyGFM 通过预训练节点表示残差（即节点相对于邻居的表示偏差）与数据无关的正常和异常类原型对齐，实现统一特征空间中的异常测量，从而实现跨图的零样本检测，并可利用少量标记正常节点进行适应性优化。实验在 11 个真实 GAD 数据集上表明，该模型在零样本和少样本设置下显著优于最先进方法，展示了其有效性和泛化能力。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "14 pages",
      "pdf_url": "http://arxiv.org/pdf/2502.09254v1",
      "published_date": "2025-02-13 12:10:05 UTC",
      "updated_date": "2025-02-13 12:10:05 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T11:00:16.907063"
    },
    {
      "arxiv_id": "2502.09247v1",
      "title": "The Joint Entity-Relation Extraction Model Based on Span and Interactive Fusion Representation for Chinese Medical Texts with Complex Semantics",
      "title_zh": "翻译失败",
      "authors": [
        "Danni Feng",
        "Runzhi Li",
        "Jing Wang",
        "Siyu Yan",
        "Lihong Ma",
        "Yunli Xing"
      ],
      "abstract": "Joint entity-relation extraction is a critical task in transforming\nunstructured or semi-structured text into triplets, facilitating the\nconstruction of large-scale knowledge graphs, and supporting various downstream\napplications. Despite its importance, research on Chinese text, particularly\nwith complex semantics in specialized domains like medicine, remains limited.\nTo address this gap, we introduce the CH-DDI, a Chinese drug-drug interactions\ndataset designed to capture the intricacies of medical text. Leveraging the\nstrengths of attention mechanisms in capturing long-range dependencies, we\npropose the SEA module, which enhances the extraction of complex contextual\nsemantic information, thereby improving entity recognition and relation\nextraction. Additionally, to address the inefficiencies of existing methods in\nfacilitating information exchange between entity recognition and relation\nextraction, we present an interactive fusion representation module. This module\nemploys Cross Attention for bidirectional information exchange between the\ntasks and further refines feature extraction through BiLSTM. Experimental\nresults on both our CH-DDI dataset and public CoNLL04 dataset demonstrate that\nour model exhibits strong generalization capabilities. On the CH-DDI dataset,\nour model achieves an F1-score of 96.73% for entity recognition and 78.43% for\nrelation extraction. On the CoNLL04 dataset, it attains an entity recognition\nprecision of 89.54% and a relation extraction accuracy of 71.64%.",
      "tldr_zh": "该研究针对中文医疗文本的复杂语义，提出了一种基于 Span 和交互融合表示的联合实体-关系抽取模型，以构建大规模知识图谱。模型引入 CH-DDI 数据集来捕捉药物-药物互动的特性，并开发 SEA 模块利用注意力机制增强长距离依赖的语义提取，同时通过交互融合表示模块（采用 Cross Attention 和 BiLSTM）实现实体识别和关系抽取之间的双向信息交换。实验结果显示，该模型在 CH-DDI 数据集上实现实体识别 F1-score 96.73% 和关系抽取 78.43%，而在公共 CoNLL04 数据集上达到实体识别精度 89.54% 和关系抽取准确率 71.64%，展现出强大的泛化能力。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2502.09247v1",
      "published_date": "2025-02-13 12:03:36 UTC",
      "updated_date": "2025-02-13 12:03:36 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T11:00:33.859733"
    },
    {
      "arxiv_id": "2502.09242v1",
      "title": "From large language models to multimodal AI: A scoping review on the potential of generative AI in medicine",
      "title_zh": "从大语言模型到多模态AI：生成式AI在医学中潜力的范围审查",
      "authors": [
        "Lukas Buess",
        "Matthias Keicher",
        "Nassir Navab",
        "Andreas Maier",
        "Soroosh Tayebi Arasteh"
      ],
      "abstract": "Generative artificial intelligence (AI) models, such as diffusion models and\nOpenAI's ChatGPT, are transforming medicine by enhancing diagnostic accuracy\nand automating clinical workflows. The field has advanced rapidly, evolving\nfrom text-only large language models for tasks such as clinical documentation\nand decision support to multimodal AI systems capable of integrating diverse\ndata modalities, including imaging, text, and structured data, within a single\nmodel. The diverse landscape of these technologies, along with rising interest,\nhighlights the need for a comprehensive review of their applications and\npotential. This scoping review explores the evolution of multimodal AI,\nhighlighting its methods, applications, datasets, and evaluation in clinical\nsettings. Adhering to PRISMA-ScR guidelines, we systematically queried PubMed,\nIEEE Xplore, and Web of Science, prioritizing recent studies published up to\nthe end of 2024. After rigorous screening, 144 papers were included, revealing\nkey trends and challenges in this dynamic field. Our findings underscore a\nshift from unimodal to multimodal approaches, driving innovations in diagnostic\nsupport, medical report generation, drug discovery, and conversational AI.\nHowever, critical challenges remain, including the integration of heterogeneous\ndata types, improving model interpretability, addressing ethical concerns, and\nvalidating AI systems in real-world clinical settings. This review summarizes\nthe current state of the art, identifies critical gaps, and provides insights\nto guide the development of scalable, trustworthy, and clinically impactful\nmultimodal AI solutions in healthcare.",
      "tldr_zh": "这篇综述探讨了生成式 AI（包括大型语言模型和多模态 AI）在医学领域的潜力，从文本-only 模型演变到整合图像、文本和结构化数据的系统。作者遵循 PRISMA-ScR 指南，从 PubMed、IEEE Xplore 和 Web of Science 检索了截至 2024 年的文献，共分析 144 篇论文，聚焦于方法、应用、数据集和临床评估。研究发现，多模态 AI 推动了诊断支持、医疗报告生成、药物发现和对话 AI 的创新，但面临整合异构数据、提升模型可解释性、处理伦理问题以及在真实临床环境中验证的挑战。该综述总结了当前状态，识别关键差距，并为开发可扩展、可信赖的医学多模态 AI 解决方案提供指导。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2502.09242v1",
      "published_date": "2025-02-13 11:57:51 UTC",
      "updated_date": "2025-02-13 11:57:51 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T11:00:41.185885"
    },
    {
      "arxiv_id": "2502.09235v1",
      "title": "Hybrid Answer Set Programming: Foundations and Applications",
      "title_zh": "混合回答集编程：基础与应用",
      "authors": [
        "Nicolas Rühling"
      ],
      "abstract": "Answer Set Programming (ASP) is a powerful tool for solving real-world\nproblems. However, many problems involve numeric values and complex constraints\nbeyond the capabilities of standard ASP solvers. Hybrid solvers like CLINGCON\nand CLINGO[DL] address this by using specialized methods for specific\nconstraints. However, these solvers lack a strong theoretical foundation.\n  This issue has first been addressed by introducing the Logic of\nHere-and-There with constraints (HT_c) as an extension of the Logic of\nHere-and-There (HT) and its non-monotone extension Equilibrium Logic. Nowadays,\nHT serves as a logical foundation for ASP and has facilitated a broader\nunderstanding of this paradigm. The idea is that HTC (and other extensions)\nplay an analogous role for hybrid ASP.\n  There remain many open questions about these logics regarding their\nfundamental characteristics as well as their practical use in solvers, ie. how\nthey can guide the implementation.\n  Having a formal understanding of these hybrid logics is also needed to better\nunderstand the inherent structure of the (real-world) problems they are applied\nto and to improve their representations in ASP. As an example of an application\nof ASP we use product configuration.",
      "tldr_zh": "本论文探讨了Answer Set Programming (ASP) 在处理数值和复杂约束时的局限性，并介绍了混合求解器如CLINGCON和CLINGO[DL] 的应用，但这些求解器缺乏坚实的理论基础。论文的主要贡献是扩展Logic of Here-and-There (HT) 为Logic of Here-and-There with constraints (HT_c)，为混合ASP 提供类似HT 的逻辑框架，以提升对问题结构的理解和求解器实现。作者强调了HT_c 等逻辑的开放问题，包括基本特性和实际指导作用，并以产品配置为例，展示了混合ASP 在实际问题中的应用潜力。最终，这有助于改进ASP 的表示和解决真实世界问题。",
      "categories": [
        "cs.AI",
        "cs.LO"
      ],
      "primary_category": "cs.AI",
      "comment": "In Proceedings ICLP 2024, arXiv:2502.08453",
      "pdf_url": "http://arxiv.org/pdf/2502.09235v1",
      "published_date": "2025-02-13 11:53:57 UTC",
      "updated_date": "2025-02-13 11:53:57 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T11:00:53.155703"
    },
    {
      "arxiv_id": "2502.09233v1",
      "title": "Commonsense Reasoning-Aided Autonomous Vehicle Systems",
      "title_zh": "翻译失败",
      "authors": [
        "Keegan Kimbrell"
      ],
      "abstract": "Autonomous Vehicle (AV) systems have been developed with a strong reliance on\nmachine learning techniques. While machine learning approaches, such as deep\nlearning, are extremely effective at tasks that involve observation and\nclassification, they struggle when it comes to performing higher level\nreasoning about situations on the road. This research involves incorporating\ncommonsense reasoning models that use image data to improve AV systems. This\nwill allow AV systems to perform more accurate reasoning while also making them\nmore adjustable, explainable, and ethical. This paper will discuss the findings\nso far and motivate its direction going forward.",
      "tldr_zh": "本研究探讨了如何通过整合常识推理模型来提升自动驾驶车辆 (AV) 系统，该模型利用图像数据辅助高级道路情况推理，以弥补机器学习在观察和分类任务之外的局限性。该方法使 AV 系统更准确、可调节、可解释和道德，从而改善整体性能。论文总结了当前发现并为未来的研究方向提供动力。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "In Proceedings ICLP 2024, arXiv:2502.08453",
      "pdf_url": "http://arxiv.org/pdf/2502.09233v1",
      "published_date": "2025-02-13 11:53:25 UTC",
      "updated_date": "2025-02-13 11:53:25 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T11:01:03.522946"
    },
    {
      "arxiv_id": "2502.09232v1",
      "title": "Logical foundations of Smart Contracts",
      "title_zh": "智能合约的逻辑基础",
      "authors": [
        "Kalonji Kalala"
      ],
      "abstract": "Nowadays, sophisticated domains are emerging which require appropriate\nformalisms to be specified accurately in order to reason about them. One such\ndomain is constituted of smart contracts that have emerged in cyber physical\nsystems as a way of enforcing formal agreements between components of these\nsystems. Smart contracts self-execute to run and share business processes\nthrough blockchain, in decentralized systems, with many different participants.\nLegal contracts are in many cases complex documents, with a number of\nexceptions, and many subcontracts. The implementation of smart contracts based\non legal contracts is a long and laborious task, that needs to include all\nactions, procedures, and the effects of actions related to the execution of the\ncontract. An ongoing open problem in this area is to formally account for smart\ncontracts using a uniform and somewhat universal formalism. This thesis\nproposes logical foundations to smart contracts using the Situation Calculus, a\nlogic for reasoning about actions. Situation Calculus is one of the prominent\nlogic-based artificial intelligence approaches that provides enough logical\nmechanism to specify and implement dynamic and complex systems such as\ncontracts. Situation Calculus is suitable to show how worlds dynamically\nchange. Smart contracts are going to be implement with Golog (written en\nProlog), a Situation Calculus-based programming language for modeling complex\nand dynamic behaviors.",
      "tldr_zh": "这篇论文探讨了智能合约的逻辑基础，旨在通过形式化方法解决其在区块链系统中复杂性（如例外和子合约）带来的挑战。论文提出使用 Situation Calculus（一种用于推理动作的逻辑）来统一指定和建模智能合约的动态行为，从而精确地处理合约执行中的动作和状态变化。作为实现方案，智能合约将通过 Golog（基于 Situation Calculus 的 Prolog 编程语言）来编程和验证。该方法为构建可信赖的去中心化系统提供了坚实的理论基础。",
      "categories": [
        "cs.LO",
        "cs.AI"
      ],
      "primary_category": "cs.LO",
      "comment": "In Proceedings ICLP 2024, arXiv:2502.08453",
      "pdf_url": "http://arxiv.org/pdf/2502.09232v1",
      "published_date": "2025-02-13 11:53:10 UTC",
      "updated_date": "2025-02-13 11:53:10 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T11:01:15.301024"
    },
    {
      "arxiv_id": "2502.09230v1",
      "title": "Relating Answer Set Programming and Many-sorted Logics for Formal Verification",
      "title_zh": "翻译失败",
      "authors": [
        "Zachary Hansen"
      ],
      "abstract": "Answer Set Programming (ASP) is an important logic programming paradigm\nwithin the field of Knowledge Representation and Reasoning. As a concise,\nhuman-readable, declarative language, ASP is an excellent tool for developing\ntrustworthy (especially, artificially intelligent) software systems. However,\nformally verifying ASP programs offers some unique challenges, such as\n  1. a lack of modularity (the meanings of rules are difficult to define in\nisolation from the enclosing program),\n  2. the ground-and-solve semantics (the meanings of rules are dependent on the\ninput data with which the program is grounded), and\n  3. limitations of existing tools.\n  My research agenda has been focused on addressing these three issues with the\nintention of making ASP verification an accessible, routine task that is\nregularly performed alongside program development. In this vein, I have\ninvestigated alternative semantics for ASP based on translations into the logic\nof here-and-there and many-sorted first-order logic. These semantics promote a\nmodular understanding of logic programs, bypass grounding, and enable us to use\nautomated theorem provers to automatically verify properties of programs.",
      "tldr_zh": "这篇论文探讨了Answer Set Programming (ASP)与many-sorted logics在正式验证中的关系，旨在解决ASP程序验证面临的挑战，包括缺乏模块性、ground-and-solve语义的依赖性和现有工具的局限性。研究者通过将ASP翻译成here-and-there逻辑和many-sorted first-order logic，提供替代语义，从而实现逻辑程序的模块化理解、绕过grounding过程，并利用自动定理证明器自动验证程序属性。这些方法使ASP验证变得更易访问和常规化，便于与程序开发同步进行。",
      "categories": [
        "cs.LO",
        "cs.AI",
        "cs.PL"
      ],
      "primary_category": "cs.LO",
      "comment": "In Proceedings ICLP 2024, arXiv:2502.08453",
      "pdf_url": "http://arxiv.org/pdf/2502.09230v1",
      "published_date": "2025-02-13 11:52:40 UTC",
      "updated_date": "2025-02-13 11:52:40 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T11:01:28.573845"
    },
    {
      "arxiv_id": "2502.09228v1",
      "title": "Computational methods for Dynamic Answer Set Programming",
      "title_zh": "翻译失败",
      "authors": [
        "Susana Hahn"
      ],
      "abstract": "In our daily lives and industrial settings, we often encounter dynamic\nproblems that require reasoning over time and metric constraints. These include\ntasks such as scheduling, routing, and production sequencing. Dynamic logics\nhave traditionally addressed these needs but often lack the flexibility and\nintegration required for comprehensive problem modeling. This research aims to\nextend Answer Set Programming (ASP), a powerful declarative problem-solving\napproach, to handle dynamic domains effectively. By integrating concepts from\ndynamic, temporal, and metric logics into ASP, we seek to develop robust\nsystems capable of modeling complex dynamic problems and performing efficient\nreasoning tasks, thereby enhancing ASPs applicability in industrial contexts.",
      "tldr_zh": "本研究针对日常和工业动态问题（如调度、路由和生产排序），这些问题涉及时间和度量约束，但传统动态 logics 往往缺乏灵活性和集成能力。论文提出扩展 Answer Set Programming (ASP) 的计算方法，通过整合动态、temporal logics 和 metric logics，开发出能有效建模复杂动态领域的稳健系统。最终，该方法提升了 ASP 在工业环境中的适用性，并实现了更高效的推理任务。",
      "categories": [
        "cs.AI",
        "cs.FL",
        "cs.LO",
        "I.2.4; I.2.8"
      ],
      "primary_category": "cs.AI",
      "comment": "In Proceedings ICLP 2024, arXiv:2502.08453",
      "pdf_url": "http://arxiv.org/pdf/2502.09228v1",
      "published_date": "2025-02-13 11:52:25 UTC",
      "updated_date": "2025-02-13 11:52:25 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T11:01:39.049836"
    },
    {
      "arxiv_id": "2502.09226v1",
      "title": "Generating Causally Compliant Counterfactual Explanations using ASP",
      "title_zh": "翻译失败",
      "authors": [
        "Sopam Dasgupta"
      ],
      "abstract": "This research is focused on generating achievable counterfactual\nexplanations. Given a negative outcome computed by a machine learning model or\na decision system, the novel CoGS approach generates (i) a counterfactual\nsolution that represents a positive outcome and (ii) a path that will take us\nfrom the negative outcome to the positive one, where each node in the path\nrepresents a change in an attribute (feature) value. CoGS computes paths that\nrespect the causal constraints among features. Thus, the counterfactuals\ncomputed by CoGS are realistic. CoGS utilizes rule-based machine learning\nalgorithms to model causal dependencies between features. The paper discusses\nthe current status of the research and the preliminary results obtained.",
      "tldr_zh": "本研究提出了一种名为 CoGS 的方法，使用 Answer Set Programming (ASP) 生成因果合规的反事实解释（counterfactual explanations），旨在从机器学习模型的负向结果出发，创建可实现的正向解决方案及其路径。CoGS 通过建模特征之间的因果约束，确保路径中的每个节点代表属性值的合理变化，从而使反事实更现实，并利用基于规则的机器学习算法处理这些依赖关系。初步实验结果显示，该方法在生成真实可行的反事实解释方面表现出色，为提升决策系统的可解释性提供了新途径。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "In Proceedings ICLP 2024, arXiv:2502.08453",
      "pdf_url": "http://arxiv.org/pdf/2502.09226v1",
      "published_date": "2025-02-13 11:51:53 UTC",
      "updated_date": "2025-02-13 11:51:53 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T11:01:52.168052"
    },
    {
      "arxiv_id": "2502.09224v1",
      "title": "Order-Sorted Intensional Logic: Expressing Subtyping Polymorphism with Typing Assertions and Quantification over Concepts",
      "title_zh": "翻译失败",
      "authors": [
        "Đorđe Marković",
        "Marc Denecker"
      ],
      "abstract": "Subtyping, also known as subtype polymorphism, is a concept extensively\nstudied in programming language theory, delineating the substitutability\nrelation among datatypes. This property ensures that programs designed for\nsupertype objects remain compatible with their subtypes.\n  In this paper, we explore the capability of order-sorted logic for utilizing\nthese ideas in the context of Knowledge Representation. We recognize two\nfundamental limitations: First, the inability of this logic to address the\nconcept rather than the value of non-logical symbols, and second, the lack of\nlanguage constructs for constraining the type of terms. Consequently, we\npropose guarded order-sorted intensional logic, where guards are language\nconstructs for annotating typing information and intensional logic provides\nsupport for quantification over concepts.",
      "tldr_zh": "本文探讨了subtyping polymorphism（子类型多态）在Knowledge Representation（知识表示）中的应用，指出order-sorted logic存在两大局限：无法处理非逻辑符号的概念层面，以及缺乏约束术语类型的语言结构。作者提出guarded order-sorted intensional logic作为解决方案，通过guards（守卫）注解typing assertions（类型断言），并利用intensional logic支持quantification over concepts（对概念的量化）。这一创新增强了逻辑系统在表达子类型多态方面的能力，为知识表示领域提供了更灵活的框架。",
      "categories": [
        "cs.AI",
        "cs.LO"
      ],
      "primary_category": "cs.AI",
      "comment": "In Proceedings ICLP 2024, arXiv:2502.08453",
      "pdf_url": "http://arxiv.org/pdf/2502.09224v1",
      "published_date": "2025-02-13 11:51:22 UTC",
      "updated_date": "2025-02-13 11:51:22 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T11:02:05.143032"
    },
    {
      "arxiv_id": "2502.09222v1",
      "title": "ASP-driven User-interaction with Clinguin",
      "title_zh": "翻译失败",
      "authors": [
        "Alexander Beiser",
        "Susana Hahn",
        "Torsten Schaub"
      ],
      "abstract": "We present clinguin, a system for ASP-driven user interface design. Clinguin\nstreamlines the development of user interfaces for ASP developers by letting\nthem build interactive prototypes directly in ASP, eliminating the need for\nseparate frontend languages. To this end, clinguin uses a few dedicated\npredicates to define user interfaces and the treatment of user-triggered\nevents. This simple design greatly facilitates the specification of user\ninteractions with an ASP system, in our case clingo.",
      "tldr_zh": "我们介绍了Clinguin，这是一个基于ASP（Answer Set Programming）的用户界面设计系统，旨在帮助ASP开发者直接在ASP中构建交互式原型，而无需使用独立的frontend语言。Clinguin通过几项专用谓词定义用户界面和处理用户触发的事件，从而简化了与ASP系统（如clingo）的交互指定。该方法大大提高了开发效率，使用户界面设计更便捷和一体化。",
      "categories": [
        "cs.AI",
        "cs.HC",
        "cs.LO",
        "cs.SE",
        "D.1.6"
      ],
      "primary_category": "cs.AI",
      "comment": "In Proceedings ICLP 2024, arXiv:2502.08453",
      "pdf_url": "http://arxiv.org/pdf/2502.09222v1",
      "published_date": "2025-02-13 11:50:51 UTC",
      "updated_date": "2025-02-13 11:50:51 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T11:02:16.878595"
    },
    {
      "arxiv_id": "2502.09221v1",
      "title": "Pearce's Characterisation in an Epistemic Domain",
      "title_zh": "翻译失败",
      "authors": [
        "Ezgi Iraz Su"
      ],
      "abstract": "Answer-set programming (ASP) is a successful problem-solving approach in\nlogic-based AI. In ASP, problems are represented as declarative logic programs,\nand solutions are identified through their answer sets. Equilibrium logic (EL)\nis a general-purpose nonmonotonic reasoning formalism, based on a monotonic\nlogic called here-and-there logic. EL was basically proposed by Pearce as a\nfoundational framework of ASP. Epistemic specifications (ES) are extensions of\nASP-programs with subjective literals. These new modal constructs in the\nASP-language make it possible to check whether a regular literal of ASP is true\nin every (or some) answer-set of a program. ES-programs are interpreted by\nworld-views, which are essentially collections of answer-sets. (Reflexive)\nautoepistemic logic is a nonmonotonic formalism, modeling self-belief\n(knowledge) of ideally rational agents. A relatively new semantics for ES is\nbased on a combination of EL and (reflexive) autoepistemic logic. In this\npaper, we first propose an overarching framework in the epistemic ASP domain.\nWe then establish a correspondence between existing (reflexive) (auto)epistemic\nequilibrium logics and our easily-adaptable comprehensive framework, building\non Pearce's characterisation of answer-sets as equilibrium models. We achieve\nthis by extending Ferraris' work on answer sets for propositional theories to\nthe epistemic case and reveal the relationship between some ES-semantic\nproposals.",
      "tldr_zh": "这篇论文在知识论领域探讨了 Pearce 的表征方法，聚焦于知识论 Answer-set Programming (ASP)。论文提出一个综合框架，将现有的 (reflexive) (auto)epistemic equilibrium logics 与此框架对应起来，基于 Equilibrium Logic (EL) 和 Autoepistemic Logic 的结合，并扩展 Ferraris 的 Answer Sets 理论到知识论情境。研究的主要贡献在于揭示 Epistemic Specifications (ES) 语义提案之间的关系，为非单调推理形式主义提供更统一的理论基础。实验和理论分析展示了这一框架如何增强 ASP 的解释性和适用性。",
      "categories": [
        "cs.AI",
        "cs.LO",
        "cs.PL"
      ],
      "primary_category": "cs.AI",
      "comment": "In Proceedings ICLP 2024, arXiv:2502.08453",
      "pdf_url": "http://arxiv.org/pdf/2502.09221v1",
      "published_date": "2025-02-13 11:50:36 UTC",
      "updated_date": "2025-02-13 11:50:36 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T11:02:28.525724"
    },
    {
      "arxiv_id": "2502.09220v1",
      "title": "Graphical Conditions for the Existence, Unicity and Number of Regular Models",
      "title_zh": "翻译失败",
      "authors": [
        "Van-Giang Trinh",
        "Belaid Benhamou",
        "Sylvain Soliman",
        "François Fages"
      ],
      "abstract": "The regular models of a normal logic program are a particular type of partial\n(i.e. 3-valued) models which correspond to stable partial models with minimal\nundefinedness. In this paper, we explore graphical conditions on the dependency\ngraph of a finite ground normal logic program to analyze the existence, unicity\nand number of regular models for the program. We show three main results: 1) a\nnecessary condition for the existence of non-trivial (i.e. non-2-valued)\nregular models, 2) a sufficient condition for the unicity of regular models,\nand 3) two upper bounds for the number of regular models based on positive\nfeedback vertex sets. The first two conditions generalize the finite cases of\nthe two existing results obtained by You and Yuan (1994) for normal logic\nprograms with well-founded stratification. The third result is also new to the\nbest of our knowledge. Key to our proofs is a connection that we establish\nbetween finite ground normal logic programs and Boolean network theory.",
      "tldr_zh": "本论文探讨了正常逻辑程序(normal logic programs)的 regular models——一种对应于 stable partial models with minimal undefinedness 的部分模型——通过分析其依赖图(dependency graph)上的图形条件，来研究这些模型的存在、唯一性(unicity)和数量。研究的主要贡献包括：提出一个非平凡(non-trivial, i.e., non-2-valued) regular models 存在的必要条件、一个 regular models 唯一性的充分条件，以及基于 positive feedback vertex sets 的两个数量上界。这些结果推广了 You and Yuan (1994) 对于 well-founded stratification 程序的有限情况，并首次建立了 finite ground normal logic programs 与 Boolean network theory 之间的连接。",
      "categories": [
        "cs.LO",
        "cs.AI",
        "cs.DM"
      ],
      "primary_category": "cs.LO",
      "comment": "In Proceedings ICLP 2024, arXiv:2502.08453",
      "pdf_url": "http://arxiv.org/pdf/2502.09220v1",
      "published_date": "2025-02-13 11:50:20 UTC",
      "updated_date": "2025-02-13 11:50:20 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T11:02:41.660685"
    },
    {
      "arxiv_id": "2502.09219v1",
      "title": "Abduction of Domain Relationships from Data for VQA",
      "title_zh": "翻译失败",
      "authors": [
        "Al Mehdi Saadat Chowdhury",
        "Paulo Shakarian",
        "Gerardo I. Simari"
      ],
      "abstract": "In this paper, we study the problem of visual question answering (VQA) where\nthe image and query are represented by ASP programs that lack domain data. We\nprovide an approach that is orthogonal and complementary to existing knowledge\naugmentation techniques where we abduce domain relationships of image\nconstructs from past examples. After framing the abduction problem, we provide\na baseline approach, and an implementation that significantly improves the\naccuracy of query answering yet requires few examples.",
      "tldr_zh": "这篇论文探讨了视觉问答（VQA）问题，其中图像和查询被表示为缺少域数据的 ASP 程序。作者提出了一种从过去例子中推断（abduction）域关系的方法，该方法与现有知识增强技术正交且互补。实验结果显示，这一基线方法及其实现显著提高了查询回答的准确性，同时只需少量例子。",
      "categories": [
        "cs.LO",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.LO",
      "comment": "In Proceedings ICLP 2024, arXiv:2502.08453",
      "pdf_url": "http://arxiv.org/pdf/2502.09219v1",
      "published_date": "2025-02-13 11:50:04 UTC",
      "updated_date": "2025-02-13 11:50:04 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T11:02:51.875095"
    },
    {
      "arxiv_id": "2502.09218v1",
      "title": "Data2Concept2Text: An Explainable Multilingual Framework for Data Analysis Narration",
      "title_zh": "Data2Concept2Text：一个可解释的多语言框架，用于数据分析叙述",
      "authors": [
        "Flavio Bertini",
        "Alessandro Dal Palù",
        "Federica Zaglio",
        "Francesco Fabiano",
        "Andrea Formisano"
      ],
      "abstract": "This paper presents a complete explainable system that interprets a set of\ndata, abstracts the underlying features and describes them in a natural\nlanguage of choice. The system relies on two crucial stages: (i) identifying\nemerging properties from data and transforming them into abstract concepts, and\n(ii) converting these concepts into natural language. Despite the impressive\nnatural language generation capabilities demonstrated by Large Language Models,\ntheir statistical nature and the intricacy of their internal mechanism still\nforce us to employ these techniques as black boxes, forgoing trustworthiness.\nDeveloping an explainable pipeline for data interpretation would allow\nfacilitating its use in safety-critical environments like processing medical\ninformation and allowing non-experts and visually impaired people to access\nnarrated information. To this end, we believe that the fields of knowledge\nrepresentation and automated reasoning research could present a valid\nalternative. Expanding on prior research that tackled the first stage (i), we\nfocus on the second stage, named Concept2Text. Being explainable, data\ntranslation is easily modeled through logic-based rules, once again emphasizing\nthe role of declarative programming in achieving AI explainability. This paper\nexplores a Prolog/CLP-based rewriting system to interpret concepts-articulated\nin terms of classes and relations, plus common knowledge-derived from a generic\nontology, generating natural language text. Its main features include\nhierarchical tree rewritings, modular multilingual generation, support for\nequivalent variants across semantic, grammar, and lexical levels, and a\ntransparent rule-based system. We outline the architecture and demonstrate its\nflexibility through some examples capable of generating numerous diverse and\nequivalent rewritings based on the input concept.",
      "tldr_zh": "本论文提出了一种可解释的多语言框架Data2Concept2Text，用于将数据集解释为自然语言叙述。该框架分为两个关键阶段：(i) 从数据中识别并抽象化为概念，以及(ii) 通过Concept2Text模块将这些概念转化为自然语言，以解决Large Language Models的黑箱问题。论文重点开发了基于Prolog/CLP的重写系统，支持层次化树重写、多语言生成以及语义、语法和词汇水平的等价变体。实验展示了该系统的灵活性，能够生成多样化的等效叙述，并适用于安全关键环境，如医疗信息处理和辅助非专家用户。",
      "categories": [
        "cs.LO",
        "cs.AI"
      ],
      "primary_category": "cs.LO",
      "comment": "In Proceedings ICLP 2024, arXiv:2502.08453",
      "pdf_url": "http://arxiv.org/pdf/2502.09218v1",
      "published_date": "2025-02-13 11:49:48 UTC",
      "updated_date": "2025-02-13 11:49:48 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T11:03:04.295207"
    },
    {
      "arxiv_id": "2502.09216v1",
      "title": "Mind the Gaps: Logical English, Prolog, and Multi-agent Systems for Autonomous Vehicles",
      "title_zh": "翻译失败",
      "authors": [
        "Galileo Sartor",
        "Adam Wyner",
        "Giuseppe Contissa"
      ],
      "abstract": "In this paper, we present a modular system for representing and reasoning\nwith legal aspects of traffic rules for autonomous vehicles. We focus on a\nsubset of the United Kingdom's Highway Code (HC) related to junctions. As human\ndrivers and automated vehicles (AVs) will interact on the roads, especially in\nurban environments, we claim that an accessible, unitary, high-level\ncomputational model should exist and be applicable to both users. Autonomous\nvehicles introduce a shift in liability that should not bring disadvantages or\nincreased burden on human drivers. We develop a system \"in silico\" of the\nmodel. The proposed system is built of three main components: a natural\nlanguage interface, using Logical English, which encodes the rules; an internal\nrepresentation of the rules in Prolog; and an multi-agent-based simulation\nenvironment, built in NetLogo. The three components interact: Logical English\nis translated into and out of Prolog (along with some support code); Prolog and\nNetLogo interface via predicates. Such a modular approach enables the different\ncomponents to carry different \"burdens\" in the overall system; it also allows\nswapping of modules. Given NetLogo, we can visualize the effect of the modeled\nrules as well as validate the system with a simple dynamic running scenario.\nDesignated agents monitor the behaviour of the vehicles for compliance and\nrecord potential violations where they occur. The information on potential\nviolations is then utilized by Validators, to determine whether the violation\nis punishable, differentiating between exceptions and cases.",
      "tldr_zh": "本论文提出一个模块化系统，用于表示和推理自动驾驶车辆（Autonomous Vehicles, AVs）的交通规则，特别关注英国公路代码（Highway Code）中与路口相关的子集，旨在确保人类驾驶员和AVs在道路互动时公平且负担一致。系统由三部分组成：使用Logical English的自然语言接口编码规则、Prolog内部表示规则，以及基于NetLogo的多智能体模拟环境。组件间通过翻译和谓词接口互动，实现规则的可视化、验证和动态场景测试；代理监控车辆行为，记录潜在违规，并由Validators区分可惩罚情况。该方法提升了系统的灵活性和可靠性，为AVs的法律应用提供了统一的高级计算模型。",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.LO",
        "cs.MA"
      ],
      "primary_category": "cs.AI",
      "comment": "In Proceedings ICLP 2024, arXiv:2502.08453",
      "pdf_url": "http://arxiv.org/pdf/2502.09216v1",
      "published_date": "2025-02-13 11:49:17 UTC",
      "updated_date": "2025-02-13 11:49:17 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T11:03:16.868138"
    },
    {
      "arxiv_id": "2502.09215v1",
      "title": "Architecture for Simulating Behavior Mode Changes in Norm-Aware Autonomous Agents",
      "title_zh": "翻译失败",
      "authors": [
        "Sean Glaze",
        "Daniela Inclezan"
      ],
      "abstract": "This paper presents an architecture for simulating the actions of a\nnorm-aware intelligent agent whose behavior with respect to norm compliance is\nset, and can later be changed, by a human controller. Updating an agent's\nbehavior mode from a norm-abiding to a riskier one may be relevant when the\nagent is involved in time-sensitive rescue operations, for example. We base our\nwork on the Authorization and Obligation Policy Language AOPL designed by\nGelfond and Lobo for the specification of norms. We introduce an architecture\nand a prototype software system that can be used to simulate an agent's plans\nunder different behavior modes that can later be changed by the controller. We\nenvision such software to be useful to policy makers, as they can more readily\nunderstand how agents may act in certain situations based on the agents'\nattitudes towards norm-compliance. Policy makers may then refine their policies\nif simulations show unwanted consequences.",
      "tldr_zh": "本论文提出了一种架构，用于模拟遵守规范的自主代理的行为模式变化，该模式由人类控制器设置并可后期调整，例如在时间敏感的救援操作中切换到更冒险的行为。基于 Authorization and Obligation Policy Language (AOPL)，该架构和原型软件系统能模拟代理在不同规范遵守态度下的计划。这样的工具有助于政策制定者通过模拟理解代理的行为后果，并据此优化政策以避免潜在问题。",
      "categories": [
        "cs.LO",
        "cs.AI",
        "D.1.6; D.3"
      ],
      "primary_category": "cs.LO",
      "comment": "In Proceedings ICLP 2024, arXiv:2502.08453",
      "pdf_url": "http://arxiv.org/pdf/2502.09215v1",
      "published_date": "2025-02-13 11:49:02 UTC",
      "updated_date": "2025-02-13 11:49:02 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T11:03:27.636044"
    },
    {
      "arxiv_id": "2502.09212v1",
      "title": "LP-LM: No Hallucinations in Question Answering with Logic Programming",
      "title_zh": "翻译失败",
      "authors": [
        "Katherine Wu",
        "Yanhong A. Liu"
      ],
      "abstract": "Large language models (LLMs) are able to generate human-like responses to\nuser queries. However, LLMs exhibit inherent limitations, especially because\nthey hallucinate. This paper introduces LP-LM, a system that grounds answers to\nquestions in known facts contained in a knowledge base (KB), facilitated\nthrough semantic parsing in Prolog, and always produces answers that are\nreliable.\n  LP-LM generates a most probable constituency parse tree along with a\ncorresponding Prolog term for an input question via Prolog definite clause\ngrammar (DCG) parsing. The term is then executed against a KB of natural\nlanguage sentences also represented as Prolog terms for question answering. By\nleveraging DCG and tabling, LP-LM runs in linear time in the size of input\nsentences for sufficiently many grammar rules. Performing experiments comparing\nLP-LM with current well-known LLMs in accuracy, we show that LLMs hallucinate\non even simple questions, unlike LP-LM.",
      "tldr_zh": "本文提出 LP-LM 系统，利用逻辑编程和 Prolog 来解决大型语言模型 (LLMs) 在问题回答中的幻觉问题，确保答案基于知识库 (KB) 中的已知事实，通过 Prolog 的确定性子句文法 (DCG) 解析生成问题解析树和对应术语。LP-LM 在 KB 上执行这些术语进行查询，并利用 DCG 和 tabling 实现线性时间复杂度。实验结果表明，LP-LM 在准确性上显著优于现有 LLMs，即使在简单问题上也完全避免了幻觉。",
      "categories": [
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.AI",
      "comment": "In Proceedings ICLP 2024, arXiv:2502.08453",
      "pdf_url": "http://arxiv.org/pdf/2502.09212v1",
      "published_date": "2025-02-13 11:48:31 UTC",
      "updated_date": "2025-02-13 11:48:31 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T11:03:40.334641"
    },
    {
      "arxiv_id": "2502.09211v1",
      "title": "Visual Graph Question Answering with ASP and LLMs for Language Parsing",
      "title_zh": "翻译失败",
      "authors": [
        "Jakob Johannes Bauer",
        "Thomas Eiter",
        "Nelson Higuera Ruiz",
        "Johannes Oetsch"
      ],
      "abstract": "Visual Question Answering (VQA) is a challenging problem that requires to\nprocess multimodal input. Answer-Set Programming (ASP) has shown great\npotential in this regard to add interpretability and explainability to modular\nVQA architectures. In this work, we address the problem of how to integrate ASP\nwith modules for vision and natural language processing to solve a new and\ndemanding VQA variant that is concerned with images of graphs (not graphs in\nsymbolic form). Images containing graph-based structures are an ubiquitous and\npopular form of visualisation. Here, we deal with the particular problem of\ngraphs inspired by transit networks, and we introduce a novel dataset that\namends an existing one by adding images of graphs that resemble metro lines.\nOur modular neuro-symbolic approach combines optical graph recognition for\ngraph parsing, a pretrained optical character recognition neural network for\nparsing labels, Large Language Models (LLMs) for language processing, and ASP\nfor reasoning. This method serves as a first baseline and achieves an overall\naverage accuracy of 73% on the dataset. Our evaluation provides further\nevidence of the potential of modular neuro-symbolic systems, in particular with\npretrained models that do not involve any further training and logic\nprogramming for reasoning, to solve complex VQA tasks.",
      "tldr_zh": "本研究针对视觉问答 (VQA) 的新变体——处理图像中图形结构的问答问题，提出了一种模块化神经符号方法，结合 Answer-Set Programming (ASP) 和 Large Language Models (LLMs) 以提升可解释性和准确性。该方法包括光学图形识别用于图形解析、预训练的光学字符识别神经网络用于标签解析，以及 LLMs 用于语言处理，再由 ASP 进行推理，作为首个基线系统。研究者引入了一个新数据集，基于现有数据集添加了类似于地铁线路的图形图像，并在该数据集上实现了73%的平均准确率。该框架证明了使用预训练模型和逻辑编程的无需额外训练的优势，有助于解决复杂的多模态 VQA 任务。",
      "categories": [
        "cs.AI",
        "cs.CV",
        "cs.LO",
        "D.1.6; I.2.10"
      ],
      "primary_category": "cs.AI",
      "comment": "In Proceedings ICLP 2024, arXiv:2502.08453. This work was partially\n  funded from the Bosch Center for AI",
      "pdf_url": "http://arxiv.org/pdf/2502.09211v1",
      "published_date": "2025-02-13 11:47:59 UTC",
      "updated_date": "2025-02-13 11:47:59 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T11:03:52.741976"
    },
    {
      "arxiv_id": "2502.09209v1",
      "title": "On LLM-generated Logic Programs and their Inference Execution Methods",
      "title_zh": "翻译失败",
      "authors": [
        "Paul Tarau"
      ],
      "abstract": "Large Language Models (LLMs) trained on petabytes of data are highly\ncompressed repositories of a significant proportion of the knowledge\naccumulated and distilled so far. In this paper we study techniques to elicit\nthis knowledge in the form of several classes of logic programs, including\npropositional Horn clauses, Dual Horn clauses, relational triplets and Definite\nClause Grammars. Exposing this knowledge as logic programs enables sound\nreasoning methods that can verify alignment of LLM outputs to their intended\nuses and extend their inference capabilities. We study new execution methods\nfor the generated programs, including soft-unification of abducible facts\nagainst LLM-generated content stored in a vector database as well as GPU-based\nacceleration of minimal model computation that supports inference with large\nLLM-generated programs.",
      "tldr_zh": "该论文探讨从大型语言模型(LLMs)中提取知识生成逻辑程序的技术，包括propositional Horn clauses、Dual Horn clauses、relational triplets和Definite Clause Grammars。这些逻辑程序支持可靠的推理方法，能验证LLM输出的与预期用途的一致性，并扩展其推理能力。论文引入了新的执行方法，如soft-unification（将可假设事实与向量数据库中的LLM内容统一）和基于GPU的加速，以高效处理大型逻辑程序的推理计算。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "In Proceedings ICLP 2024, arXiv:2502.08453",
      "pdf_url": "http://arxiv.org/pdf/2502.09209v1",
      "published_date": "2025-02-13 11:47:44 UTC",
      "updated_date": "2025-02-13 11:47:44 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T11:04:04.430947"
    },
    {
      "arxiv_id": "2502.09206v1",
      "title": "Efficient OWL2QL Meta-reasoning Using ASP-based Hybrid Knowledge Bases",
      "title_zh": "翻译失败",
      "authors": [
        "Haya Majid Qureshi",
        "Wolfgang Faber"
      ],
      "abstract": "Metamodeling refers to scenarios in ontologies in which classes and roles can\nbe members of classes or occur in roles. This is a desirable modelling feature\nin several applications, but allowing it without restrictions is problematic\nfor several reasons, mainly because it causes undecidability. Therefore,\npractical languages either forbid metamodeling explicitly or treat occurrences\nof classes as instances to be semantically different from other occurrences,\nthereby not allowing metamodeling semantically. Several extensions have been\nproposed to provide metamodeling to some extent. Building on earlier work that\nreduces metamodeling query answering to Datalog query answering, recently\nreductions to query answering over hybrid knowledge bases were proposed with\nthe aim of using the Datalog transformation only where necessary. Preliminary\nwork showed that the approach works, but the hoped-for performance improvements\nwere not observed yet. In this work we expand on this body of work by improving\nthe theoretical basis of the reductions and by using alternative tools that\nshow competitive performance.",
      "tldr_zh": "本文研究了OWL2QL中的元建模(metamodeling)问题，该技术允许类和角色作为成员或出现，但会引发undecidability，因此实际语言往往限制或语义区分它。论文基于先前工作，将元建模查询回答减少到Datalog查询，并进一步优化为使用ASP-based hybrid knowledge bases，仅在必要时应用Datalog转换，同时改进了理论基础和引入了替代工具。结果表明，这种方法显著提升了推理效率，为本体应用提供了更实用的性能改进。",
      "categories": [
        "cs.LO",
        "cs.AI",
        "cs.SC"
      ],
      "primary_category": "cs.LO",
      "comment": "In Proceedings ICLP 2024, arXiv:2502.08453",
      "pdf_url": "http://arxiv.org/pdf/2502.09206v1",
      "published_date": "2025-02-13 11:46:10 UTC",
      "updated_date": "2025-02-13 11:46:10 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T11:04:16.449679"
    },
    {
      "arxiv_id": "2502.09205v1",
      "title": "Counterfactual Explanations as Plans",
      "title_zh": "反事实解释作为计划",
      "authors": [
        "Vaishak Belle"
      ],
      "abstract": "There has been considerable recent interest in explainability in AI,\nespecially with black-box machine learning models. As correctly observed by the\nplanning community, when the application at hand is not a single-shot decision\nor prediction, but a sequence of actions that depend on observations, a richer\nnotion of explanations are desirable.\n  In this paper, we look to provide a formal account of ``counterfactual\nexplanations,\" based in terms of action sequences. We then show that this\nnaturally leads to an account of model reconciliation, which might take the\nform of the user correcting the agent's model, or suggesting actions to the\nagent's plan. For this, we will need to articulate what is true versus what is\nknown, and we appeal to a modal fragment of the situation calculus to formalise\nthese intuitions. We consider various settings: the agent knowing partial\ntruths, weakened truths and having false beliefs, and show that our definitions\neasily generalize to these different settings.",
      "tldr_zh": "该论文将反事实解释(Counterfactual Explanations)形式化为基于动作序列的计划(Plans)，以提供更丰富的AI解释性，尤其适用于涉及连续观察和决策的场景。作者使用情境演算(Situation Calculus)的模态片段来形式化模型协调(Model Reconciliation)，允许用户修正代理的模型或建议动作，从而处理代理部分真相、弱化真相或错误信念等情况。该方法展示了其在不同设置下的泛化性，增强了黑箱机器学习模型的可解释性和交互性。",
      "categories": [
        "cs.AI",
        "cs.LO"
      ],
      "primary_category": "cs.AI",
      "comment": "In Proceedings ICLP 2024, arXiv:2502.08453",
      "pdf_url": "http://arxiv.org/pdf/2502.09205v1",
      "published_date": "2025-02-13 11:45:54 UTC",
      "updated_date": "2025-02-13 11:45:54 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T11:04:28.094878"
    },
    {
      "arxiv_id": "2502.09204v1",
      "title": "Logical Lease Litigation: Prolog and LLMs for Rental Law Compliance in New York",
      "title_zh": "翻译失败",
      "authors": [
        "Sanskar Sehgal",
        "Yanhong A. Liu"
      ],
      "abstract": "Legal cases require careful logical reasoning following the laws, whereas\ninteractions with non-technical users must be in natural language. As an\napplication combining logical reasoning using Prolog and natural language\nprocessing using large language models (LLMs), this paper presents a novel\napproach and system, LogicLease, to automate the analysis of landlord-tenant\nlegal cases in the state of New York. LogicLease determines compliance with\nrelevant legal requirements by analyzing case descriptions and citing all\nrelevant laws. It leverages LLMs for information extraction and Prolog for\nlegal reasoning. By separating information extraction from legal reasoning,\nLogicLease achieves greater transparency and control over the legal logic\napplied to each case. We evaluate the accuracy, efficiency, and robustness of\nLogicLease through a series of tests, achieving 100% accuracy and an average\nprocessing time of 2.57 seconds. LogicLease presents advantages over\nstate-of-the-art LLM-based legal analysis systems by providing clear,\nstep-by-step reasoning, citing specific laws, and distinguishing itself by its\nability to avoid hallucinations -- a common issue in LLMs.",
      "tldr_zh": "这篇论文介绍了 LogicLease 系统，该系统结合 Prolog 用于逻辑推理和 LLMs 用于自然语言处理，旨在自动化分析纽约州房东-租客法律案例的合规性。\n通过分离信息提取和法律推理，LogicLease 实现了更高的透明度、控制力和准确性，避免了 LLMs 常见的幻觉问题。\n实验评估显示，该系统在测试中达到 100% 准确率，平均处理时间为 2.57 秒，并通过清晰的步步推理和具体法律引用，超越了现有 LLM 基于的法律分析系统。",
      "categories": [
        "cs.AI",
        "cs.LO"
      ],
      "primary_category": "cs.AI",
      "comment": "In Proceedings ICLP 2024, arXiv:2502.08453",
      "pdf_url": "http://arxiv.org/pdf/2502.09204v1",
      "published_date": "2025-02-13 11:45:38 UTC",
      "updated_date": "2025-02-13 11:45:38 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T11:04:40.117564"
    },
    {
      "arxiv_id": "2502.09680v1",
      "title": "Object-Centric Latent Action Learning",
      "title_zh": "以对象为中心的潜在动作学习",
      "authors": [
        "Albina Klepach",
        "Alexander Nikulin",
        "Ilya Zisman",
        "Denis Tarasov",
        "Alexander Derevyagin",
        "Andrei Polubarov",
        "Nikita Lyubaykin",
        "Vladislav Kurenkov"
      ],
      "abstract": "Leveraging vast amounts of internet video data for Embodied AI is currently\nbottle-necked by the lack of action annotations and the presence of\naction-correlated distractors. We propose a novel object-centric latent action\nlearning approach, based on VideoSaur and LAPO, that employs self-supervised\ndecomposition of scenes into object representations and annotates video data\nwith proxy-action labels. This method effectively disentangles causal\nagent-object interactions from irrelevant background noise and reduces the\nperformance degradation of latent action learning approaches caused by\ndistractors. Our preliminary experiments with the Distracting Control Suite\nshow that latent action pretraining based on object decompositions improve the\nquality of inferred latent actions by x2.7 and efficiency of downstream\nfine-tuning with a small set of labeled actions, increasing return by x2.6 on\naverage.",
      "tldr_zh": "这篇论文提出了一种 object-centric latent action learning 方法，旨在解决利用互联网视频数据进行 Embodied AI 时缺乏动作标注和动作相关干扰的问题。该方法基于 VideoSaur 和 LAPO，通过自监督方式将场景分解成对象表示，并为视频数据自动标注代理动作标签，从而有效分离因果代理-对象互动与无关背景噪声。实验结果显示，在 Distracting Control Suite 上，该方法使隐式动作质量提高 2.7 倍，并提升下游微调效率，平均回报增加 2.6 倍。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "Preprint. In review",
      "pdf_url": "http://arxiv.org/pdf/2502.09680v1",
      "published_date": "2025-02-13 11:27:05 UTC",
      "updated_date": "2025-02-13 11:27:05 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T11:04:52.391499"
    },
    {
      "arxiv_id": "2502.09188v1",
      "title": "Matina: A Large-Scale 73B Token Persian Text Corpus",
      "title_zh": "翻译失败",
      "authors": [
        "Sara Bourbour Hosseinbeigi",
        "Fatemeh Taherinezhad",
        "Heshaam Faili",
        "Hamed Baghbani",
        "Fatemeh Nadi",
        "Mostafa Amiri"
      ],
      "abstract": "Text corpora are essential for training models used in tasks like\nsummarization, translation, and large language models (LLMs). While various\nefforts have been made to collect monolingual and multilingual datasets in many\nlanguages, Persian has often been underrepresented due to limited resources for\ndata collection and preprocessing. Existing Persian datasets are typically\nsmall and lack content diversity, consisting mainly of weblogs and news\narticles. This shortage of high-quality, varied data has slowed the development\nof NLP models and open-source LLMs for Persian. Since model performance depends\nheavily on the quality of training data, we address this gap by introducing the\nMatina corpus, a new Persian dataset of 72.9B tokens, carefully preprocessed\nand deduplicated to ensure high data quality. We further assess its\neffectiveness by training and evaluating transformer-based models on key NLP\ntasks. Both the dataset and preprocessing codes are publicly available,\nenabling researchers to build on and improve this resource for future Persian\nNLP advancements.",
      "tldr_zh": "本文指出，现有的波斯语（Persian）文本语料库规模小、内容单一，主要限于博客和新闻，这阻碍了NLP模型和大型语言模型（LLMs）的开发。针对这一问题，研究团队引入了Matina语料库，一个包含72.9B tokens的高质量波斯语数据集，通过仔细预处理和去重确保数据可靠性。为验证其有效性，他们在关键NLP任务上训练并评估了transformer-based模型，结果显示了显著改进。该语料库及其预处理代码已公开，旨在支持未来波斯语NLP研究的进步。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2502.09188v1",
      "published_date": "2025-02-13 11:22:19 UTC",
      "updated_date": "2025-02-13 11:22:19 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T11:05:06.743864"
    },
    {
      "arxiv_id": "2502.09183v1",
      "title": "RefineCoder: Iterative Improving of Large Language Models via Adaptive Critique Refinement for Code Generation",
      "title_zh": "翻译失败",
      "authors": [
        "Changzhi Zhou",
        "Xinyu Zhang",
        "Dandan Song",
        "Xiancai Chen",
        "Wanli Gu",
        "Huipeng Ma",
        "Yuhang Tian",
        "Mengdi Zhang",
        "Linmei Hu"
      ],
      "abstract": "Code generation has attracted increasing attention with the rise of Large\nLanguage Models (LLMs). Many studies have developed powerful code LLMs by\nsynthesizing code-related instruction data and applying supervised fine-tuning.\nHowever, these methods are limited by teacher model distillation and ignore the\npotential of iterative refinement by self-generated code. In this paper, we\npropose Adaptive Critique Refinement (ACR), which enables the model to refine\nitself by self-generated code and external critique, rather than directly\nimitating the code responses of the teacher model. Concretely, ACR includes a\ncomposite scoring system with LLM-as-a-Judge to evaluate the quality of code\nresponses and a selective critique strategy with LLM-as-a-Critic to critique\nself-generated low-quality code responses. We develop the RefineCoder series by\niteratively applying ACR, achieving continuous performance improvement on\nmultiple code generation benchmarks. Compared to the baselines of the same\nsize, our proposed RefineCoder series can achieve comparable or even superior\nperformance using less data.",
      "tldr_zh": "该论文提出 Adaptive Critique Refinement (ACR) 方法，用于迭代改进 Large Language Models (LLMs) 在代码生成任务中的性能，避免依赖教师模型蒸馏。ACR 包括一个复合评分系统，利用 LLM-as-a-Judge 评估代码响应质量，以及一个选择性批评策略，通过 LLM-as-a-Critic 针对自生成低质量代码进行改进。实验结果显示，通过迭代应用 ACR 开发的 RefineCoder 系列模型，在多个代码生成基准上实现了持续性能提升，并使用更少数据就达到了与同规模基线相当或优越的水平。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "work in process",
      "pdf_url": "http://arxiv.org/pdf/2502.09183v1",
      "published_date": "2025-02-13 11:17:53 UTC",
      "updated_date": "2025-02-13 11:17:53 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T11:05:16.934730"
    },
    {
      "arxiv_id": "2502.09175v1",
      "title": "FLAME: Flexible LLM-Assisted Moderation Engine",
      "title_zh": "翻译失败",
      "authors": [
        "Ivan Bakulin",
        "Ilia Kopanichuk",
        "Iaroslav Bespalov",
        "Nikita Radchenko",
        "Vladimir Shaposhnikov",
        "Dmitry Dylov",
        "Ivan Oseledets"
      ],
      "abstract": "The rapid advancement of Large Language Models (LLMs) has introduced\nsignificant challenges in moderating user-model interactions. While LLMs\ndemonstrate remarkable capabilities, they remain vulnerable to adversarial\nattacks, particularly ``jailbreaking'' techniques that bypass content safety\nmeasures. Current content moderation systems, which primarily rely on input\nprompt filtering, have proven insufficient, with techniques like Best-of-N\n(BoN) jailbreaking achieving success rates of 80% or more against popular LLMs.\nIn this paper, we introduce Flexible LLM-Assisted Moderation Engine (FLAME): a\nnew approach that shifts the focus from input filtering to output moderation.\nUnlike traditional circuit-breaking methods that analyze user queries, FLAME\nevaluates model responses, offering several key advantages: (1) computational\nefficiency in both training and inference, (2) enhanced resistance to BoN\njailbreaking attacks, and (3) flexibility in defining and updating safety\ncriteria through customizable topic filtering. Our experiments demonstrate that\nFLAME significantly outperforms current moderation systems. For example, FLAME\nreduces attack success rate in GPT-4o-mini and DeepSeek-v3 by a factor of ~9,\nwhile maintaining low computational overhead. We provide comprehensive\nevaluation on various LLMs and analyze the engine's efficiency against the\nstate-of-the-art jailbreaking. This work contributes to the development of more\nrobust and adaptable content moderation systems for LLMs.",
      "tldr_zh": "该研究针对大型语言模型(LLMs)易受对抗性攻击（如jailbreaking）的问题，提出了一种灵活的LLM辅助调节引擎(FLAME)，将焦点从输入提示过滤转向输出调节，以提升内容安全。该方法具有计算效率高、抗BoN jailbreaking攻击能力强，以及通过可自定义主题过滤灵活定义安全标准的优势。实验显示，FLAME在GPT-4o-mini和DeepSeek-v3上将攻击成功率降低了约9倍，同时保持低计算开销，并在多种LLMs上表现出色，为构建更稳健且适配的内容调节系统提供了重要贡献。",
      "categories": [
        "cs.CR",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.CR",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2502.09175v1",
      "published_date": "2025-02-13 11:05:55 UTC",
      "updated_date": "2025-02-13 11:05:55 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T11:05:27.867035"
    },
    {
      "arxiv_id": "2502.09173v1",
      "title": "Two-Stage Representation Learning for Analyzing Movement Behavior Dynamics in People Living with Dementia",
      "title_zh": "两阶段表示学习用于分析患有痴呆症人群的运动行为动态",
      "authors": [
        "Jin Cui",
        "Alexander Capstick",
        "Payam Barnaghi",
        "Gregory Scott"
      ],
      "abstract": "In remote healthcare monitoring, time series representation learning reveals\ncritical patient behavior patterns from high-frequency data. This study\nanalyzes home activity data from individuals living with dementia by proposing\na two-stage, self-supervised learning approach tailored to uncover low-rank\nstructures. The first stage converts time-series activities into text sequences\nencoded by a pre-trained language model, providing a rich, high-dimensional\nlatent state space using a PageRank-based method. This PageRank vector captures\nlatent state transitions, effectively compressing complex behaviour data into a\nsuccinct form that enhances interpretability. This low-rank representation not\nonly enhances model interpretability but also facilitates clustering and\ntransition analysis, revealing key behavioral patterns correlated with\nclinicalmetrics such as MMSE and ADAS-COG scores. Our findings demonstrate the\nframework's potential in supporting cognitive status prediction, personalized\ncare interventions, and large-scale health monitoring.",
      "tldr_zh": "这篇论文提出了一种两阶段自监督学习方法，用于分析患有痴呆症患者的运动行为动态，通过处理家庭活动时间序列数据来揭示关键行为模式。 第一阶段将时间序列转换为文本序列，并使用预训练语言模型结合基于PageRank的方法生成低秩表示，以捕获潜状态转换并提升数据可解释性。 这种低秩表示便于进行聚类和转换分析，揭示行为模式与临床指标（如MMSE和ADAS-COG分数）的相关性。 研究结果表明，该框架可支持认知状态预测、个性化护理干预和大规模健康监测。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "AAAI 2025 Workshop on Large Language Models and Generative AI for\n  Health",
      "pdf_url": "http://arxiv.org/pdf/2502.09173v1",
      "published_date": "2025-02-13 10:57:25 UTC",
      "updated_date": "2025-02-13 10:57:25 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T11:05:40.272956"
    },
    {
      "arxiv_id": "2502.12171v1",
      "title": "GoRA: Gradient-driven Adaptive Low Rank Adaptation",
      "title_zh": "GoRA：梯度驱动的自适应低秩适配",
      "authors": [
        "Haonan He",
        "Peng Ye",
        "Yuchen Ren",
        "Yuan Yuan",
        "Lei Chen"
      ],
      "abstract": "Low-Rank Adaptation (LoRA) is a crucial method for efficiently fine-tuning\npretrained large language models (LLMs), with its performance largely\ninfluenced by two key factors: rank and initialization strategy. Numerous LoRA\nvariants have been proposed to enhance its performance by addressing these\nfactors. However, these variants often compromise LoRA's usability or\nefficiency. In this paper, we analyze the fundamental limitations of existing\nmethods and introduce a novel approach, GoRA (Gradient-driven Adaptive Low Rank\nAdaptation), which adaptively assigns ranks and initializes weights for\nlow-rank adapters simultaneously based on gradient information. Extensive\nexperimental results demonstrate that GoRA significantly improves performance\nwhile preserving the high usability and efficiency of LoRA. On the T5 model\nfine-tuned for the GLUE benchmark, GoRA achieves a 5.88-point improvement over\nLoRA and slightly surpasses full fine-tuning. Similarly, on the\nLlama3.1-8B-Base model fine-tuned for GSM8k tasks, GoRA outperforms LoRA with a\n5.13-point improvement and exceeds full fine-tuning in high-rank settings by a\nmargin of 2.05 points.",
      "tldr_zh": "本文提出 GoRA，一种基于梯度信息的自适应 Low-Rank Adaptation (LoRA) 方法，用于高效微调预训练的大型语言模型 (LLMs)，通过动态分配秩和初始化权重来解决现有方法的局限性，同时保持 LoRA 的高可用性和效率。GoRA 的核心创新在于利用梯度信息实现自适应调整，提升了模型的性能表现。在实验中，GoRA 在 T5 模型上微调 GLUE 基准时比 LoRA 提升 5.88 分并略超全微调，而在 Llama3.1-8B-Base 模型上微调 GSM8k 任务时提升 5.13 分，并在高秩设置下超过全微调 2.05 分。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2502.12171v1",
      "published_date": "2025-02-13 10:33:58 UTC",
      "updated_date": "2025-02-13 10:33:58 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T11:05:53.292768"
    },
    {
      "arxiv_id": "2502.12170v1",
      "title": "MUDDFormer: Breaking Residual Bottlenecks in Transformers via Multiway Dynamic Dense Connections",
      "title_zh": "MUDDFormer：通过多路动态稠密连接打破 Transformer 中的残差瓶颈",
      "authors": [
        "Da Xiao",
        "Qingye Meng",
        "Shengping Li",
        "Xingyuan Yuan"
      ],
      "abstract": "We propose MUltiway Dynamic Dense (MUDD) connections, a simple yet effective\nmethod to address the limitations of residual connections and enhance\ncross-layer information flow in Transformers. Unlike existing dense connection\napproaches with static and shared connection weights, MUDD generates connection\nweights dynamically depending on hidden states at each sequence position and\nfor each decoupled input stream (the query, key, value or residual) of a\nTransformer block. MUDD connections can be seamlessly integrated into any\nTransformer architecture to create MUDDFormer. Extensive experiments show that\nMUDDFormer significantly outperforms Transformers across various model\narchitectures and scales in language modeling, achieving the performance of\nTransformers trained with 1.8X-2.4X compute. Notably, MUDDPythia-2.8B matches\nPythia-6.9B in pretraining ppl and downstream tasks and even rivals Pythia-12B\nin five-shot settings, while adding only 0.23% parameters and 0.4% computation.\nCode in JAX and PyTorch and pre-trained models are available at\nhttps://github.com/Caiyun-AI/MUDDFormer .",
      "tldr_zh": "该论文提出 MUDD（Multiway Dynamic Dense）连接，一种简单有效的机制，用于解决 Transformer 中残差连接的限制，并提升跨层信息流。不同于静态连接权重，MUDD 根据每个序列位置的隐藏状态和 Transformer 块的解耦输入流（如 query, key, value 或 residual）动态生成权重，从而无缝集成到任何 Transformer 架构中形成 MUDDFormer。实验结果显示，MUDDFormer 在语言建模任务中显著优于标准 Transformer，性能相当于使用 1.8X-2.4X 计算资源的模型。具体而言，MUDDPythia-2.8B 在预训练 perplexity 和下游任务中匹敌 Pythia-6.9B，甚至在五-shot 设置中接近 Pythia-12B，同时仅增加 0.23% 参数和 0.4% 计算。代码和预训练模型已在 GitHub 上公开。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2502.12170v1",
      "published_date": "2025-02-13 10:26:27 UTC",
      "updated_date": "2025-02-13 10:26:27 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T11:06:05.901916"
    },
    {
      "arxiv_id": "2502.09125v1",
      "title": "Automatic Pruning via Structured Lasso with Class-wise Information",
      "title_zh": "翻译失败",
      "authors": [
        "Xiang Liu",
        "Mingchen Li",
        "Xia Li",
        "Leigang Qu",
        "Zifan Peng",
        "Yijun Song",
        "Zemin Liu",
        "Linshan Jiang",
        "Jialin Li"
      ],
      "abstract": "Most pruning methods concentrate on unimportant filters of neural networks.\nHowever, they face the loss of statistical information due to a lack of\nconsideration for class-wise data. In this paper, from the perspective of\nleveraging precise class-wise information for model pruning, we utilize\nstructured lasso with guidance from Information Bottleneck theory. Our approach\nensures that statistical information is retained during the pruning process.\nWith these techniques, we introduce two innovative adaptive network pruning\nschemes: sparse graph-structured lasso pruning with Information Bottleneck\n(\\textbf{sGLP-IB}) and sparse tree-guided lasso pruning with Information\nBottleneck (\\textbf{sTLP-IB}). The key aspect is pruning model filters using\nsGLP-IB and sTLP-IB to better capture class-wise relatedness. Compared to\nmultiple state-of-the-art methods, our approaches demonstrate superior\nperformance across three datasets and six model architectures in extensive\nexperiments. For instance, using the VGG16 model on the CIFAR-10 dataset, we\nachieve a parameter reduction of 85%, a decrease in FLOPs by 61%, and maintain\nan accuracy of 94.10% (0.14% higher than the original model); we reduce the\nparameters by 55% with the accuracy at 76.12% using the ResNet architecture on\nImageNet (only drops 0.03%). In summary, we successfully reduce model size and\ncomputational resource usage while maintaining accuracy. Our codes are at\nhttps://anonymous.4open.science/r/IJCAI-8104.",
      "tldr_zh": "本文提出了一种自动神经网络修剪方法，使用 Structured Lasso 结合 Information Bottleneck 理论，针对类别-wise 信息来保留统计信息，避免传统修剪方法的损失。引入了两种创新方案：sGLP-IB 和 sTLP-IB，这些方法通过捕捉类别-wise 相关性来优化模型过滤器。实验结果显示，在 CIFAR-10 和 ImageNet 等数据集上，该方法在 VGG16 和 ResNet 等六种架构中表现出优越性能，例如 VGG16 参数减少 85%、FLOPs 减少 61% 且准确率达 94.10%（比原模型高 0.14%），成功实现了模型轻量化同时保持高准确率。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "11 pages, 2 figures",
      "pdf_url": "http://arxiv.org/pdf/2502.09125v1",
      "published_date": "2025-02-13 10:03:29 UTC",
      "updated_date": "2025-02-13 10:03:29 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T11:06:17.629135"
    },
    {
      "arxiv_id": "2502.09122v1",
      "title": "Improving Deep Regression with Tightness",
      "title_zh": "翻译失败",
      "authors": [
        "Shihao Zhang",
        "Yuguang Yan",
        "Angela Yao"
      ],
      "abstract": "For deep regression, preserving the ordinality of the targets with respect to\nthe feature representation improves performance across various tasks. However,\na theoretical explanation for the benefits of ordinality is still lacking. This\nwork reveals that preserving ordinality reduces the conditional entropy\n$H(Z|Y)$ of representation $Z$ conditional on the target $Y$. However, our\nfindings reveal that typical regression losses do little to reduce $H(Z|Y)$,\neven though it is vital for generalization performance. With this motivation,\nwe introduce an optimal transport-based regularizer to preserve the similarity\nrelationships of targets in the feature space to reduce $H(Z|Y)$. Additionally,\nwe introduce a simple yet efficient strategy of duplicating the regressor\ntargets, also with the aim of reducing $H(Z|Y)$. Experiments on three\nreal-world regression tasks verify the effectiveness of our strategies to\nimprove deep regression. Code:\nhttps://github.com/needylove/Regression_tightness.",
      "tldr_zh": "本文研究发现，在深度回归中，保留目标的顺序性（ordinality）能通过减少条件熵 H(Z|Y) 来提升泛化性能，但传统回归损失函数对此效果有限。作者提出了一种基于最优传输（optimal transport）的正则化器，以及一个简单策略——复制回归器目标（duplicating the regressor targets），以在特征空间中保持目标相似关系并降低 H(Z|Y)。实验在三个真实世界的回归任务上验证了这些方法的有效性，提高了深度回归的表现。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.LG",
      "comment": "ICLR 2025, Code: https://github.com/needylove/Regression_tightness",
      "pdf_url": "http://arxiv.org/pdf/2502.09122v1",
      "published_date": "2025-02-13 09:57:25 UTC",
      "updated_date": "2025-02-13 09:57:25 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T11:06:28.624254"
    },
    {
      "arxiv_id": "2502.09104v1",
      "title": "One-shot Federated Learning Methods: A Practical Guide",
      "title_zh": "单次联邦学习方法：实用指南",
      "authors": [
        "Xiang Liu",
        "Zhenheng Tang",
        "Xia Li",
        "Yijun Song",
        "Sijie Ji",
        "Zemin Liu",
        "Bo Han",
        "Linshan Jiang",
        "Jialin Li"
      ],
      "abstract": "One-shot Federated Learning (OFL) is a distributed machine learning paradigm\nthat constrains client-server communication to a single round, addressing\nprivacy and communication overhead issues associated with multiple rounds of\ndata exchange in traditional Federated Learning (FL). OFL demonstrates the\npractical potential for integration with future approaches that require\ncollaborative training models, such as large language models (LLMs). However,\ncurrent OFL methods face two major challenges: data heterogeneity and model\nheterogeneity, which result in subpar performance compared to conventional FL\nmethods. Worse still, despite numerous studies addressing these limitations, a\ncomprehensive summary is still lacking. To address these gaps, this paper\npresents a systematic analysis of the challenges faced by OFL and thoroughly\nreviews the current methods. We also offer an innovative categorization method\nand analyze the trade-offs of various techniques. Additionally, we discuss the\nmost promising future directions and the technologies that should be integrated\ninto the OFL field. This work aims to provide guidance and insights for future\nresearch.",
      "tldr_zh": "本论文介绍了 One-shot Federated Learning (OFL)，一种只需一轮客户端-服务器通信的分布式机器学习方法，能够有效解决传统 Federated Learning (FL) 的隐私和通信开销问题，并适用于如 large language models (LLMs) 等协作训练场景。OFL 面临的主要挑战包括数据异质性和模型异质性，导致其性能不如常规 FL 方法。论文通过系统分析这些挑战、回顾现有方法、创新分类方式以及权衡各种技术的优缺点，为未来研究提供全面指导和见解。最终，该工作强调了 OFL 的潜在发展方向和应整合的技术，以推动其实际应用。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "10 pages, 1 figure",
      "pdf_url": "http://arxiv.org/pdf/2502.09104v1",
      "published_date": "2025-02-13 09:26:44 UTC",
      "updated_date": "2025-02-13 09:26:44 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T11:06:41.271908"
    },
    {
      "arxiv_id": "2502.10470v3",
      "title": "MetaDE: Evolving Differential Evolution by Differential Evolution",
      "title_zh": "翻译失败",
      "authors": [
        "Minyang Chen",
        "Chenchen Feng",
        "and Ran Cheng"
      ],
      "abstract": "As a cornerstone in the Evolutionary Computation (EC) domain, Differential\nEvolution (DE) is known for its simplicity and effectiveness in handling\nchallenging black-box optimization problems. While the advantages of DE are\nwell-recognized, achieving peak performance heavily depends on its\nhyperparameters such as the mutation factor, crossover probability, and the\nselection of specific DE strategies. Traditional approaches to this\nhyperparameter dilemma have leaned towards parameter tuning or adaptive\nmechanisms. However, identifying the optimal settings tailored for specific\nproblems remains a persistent challenge. In response, we introduce MetaDE, an\napproach that evolves DE's intrinsic hyperparameters and strategies using DE\nitself at a meta-level. A pivotal aspect of MetaDE is a specialized\nparameterization technique, which endows it with the capability to dynamically\nmodify DE's parameters and strategies throughout the evolutionary process. To\naugment computational efficiency, MetaDE incorporates a design that leverages\nparallel processing through a GPU-accelerated computing framework. Within such\na framework, DE is not just a solver but also an optimizer for its own\nconfigurations, thus streamlining the process of hyperparameter optimization\nand problem-solving into a cohesive and automated workflow. Extensive\nevaluations on the CEC2022 benchmark suite demonstrate MetaDE's promising\nperformance. Moreover, when applied to robot control via evolutionary\nreinforcement learning, MetaDE also demonstrates promising performance. The\nsource code of MetaDE is publicly accessible at:\nhttps://github.com/EMI-Group/metade.",
      "tldr_zh": "这篇论文提出 MetaDE，一种创新方法，使用 Differential Evolution (DE) 自身在元级别进化 DE 的超参数（如突变因子和交叉概率）以及策略，以解决传统调参的挑战。MetaDE 采用专门的参数化技术，实现动态修改参数和策略，并通过 GPU 加速的并行处理框架提升计算效率，使 DE 既作为求解器又作为自身配置的优化器。实验结果显示，MetaDE 在 CEC2022 基准测试中表现出色，并在机器人控制的进化强化学习应用中取得良好性能，源代码已开源。",
      "categories": [
        "cs.NE",
        "cs.AI"
      ],
      "primary_category": "cs.NE",
      "comment": "Accepted by IEEE TEVC",
      "pdf_url": "http://arxiv.org/pdf/2502.10470v3",
      "published_date": "2025-02-13 09:24:47 UTC",
      "updated_date": "2025-03-26 08:06:11 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T11:06:53.067062"
    },
    {
      "arxiv_id": "2502.09100v1",
      "title": "Logical Reasoning in Large Language Models: A Survey",
      "title_zh": "大语言模型中的逻辑推理：一项调查",
      "authors": [
        "Hanmeng Liu",
        "Zhizhang Fu",
        "Mengru Ding",
        "Ruoxi Ning",
        "Chaoli Zhang",
        "Xiaozhang Liu",
        "Yue Zhang"
      ],
      "abstract": "With the emergence of advanced reasoning models like OpenAI o3 and\nDeepSeek-R1, large language models (LLMs) have demonstrated remarkable\nreasoning capabilities. However, their ability to perform rigorous logical\nreasoning remains an open question. This survey synthesizes recent advancements\nin logical reasoning within LLMs, a critical area of AI research. It outlines\nthe scope of logical reasoning in LLMs, its theoretical foundations, and the\nbenchmarks used to evaluate reasoning proficiency. We analyze existing\ncapabilities across different reasoning paradigms - deductive, inductive,\nabductive, and analogical - and assess strategies to enhance reasoning\nperformance, including data-centric tuning, reinforcement learning, decoding\nstrategies, and neuro-symbolic approaches. The review concludes with future\ndirections, emphasizing the need for further exploration to strengthen logical\nreasoning in AI systems.",
      "tldr_zh": "这篇调查论文总结了大型语言模型 (LLMs) 在逻辑推理方面的最新进展，探讨了模型如 OpenAI o3 和 DeepSeek-R1 的能力及其局限性。论文分析了不同推理范式，包括演绎 (deductive)、归纳 (inductive)、溯因 (abductive) 和类比 (analogical) 推理，并介绍了评估基准、理论基础和性能提升策略，如数据中心调优、reinforcement learning、decoding strategies 以及 neuro-symbolic 方法。研究发现，虽然 LLMs 在推理方面取得了显著进步，但仍需进一步优化，最终强调了强化 AI 系统逻辑推理的未来研究方向。",
      "categories": [
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2502.09100v1",
      "published_date": "2025-02-13 09:19:14 UTC",
      "updated_date": "2025-02-13 09:19:14 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T11:07:04.834756"
    },
    {
      "arxiv_id": "2502.09675v1",
      "title": "Multi-level Conflict-Aware Network for Multi-modal Sentiment Analysis",
      "title_zh": "翻译失败",
      "authors": [
        "Yubo Gao",
        "Haotian Wu",
        "Lei Zhang"
      ],
      "abstract": "Multimodal Sentiment Analysis (MSA) aims to recognize human emotions by\nexploiting textual, acoustic, and visual modalities, and thus how to make full\nuse of the interactions between different modalities is a central challenge of\nMSA. Interaction contains alignment and conflict aspects. Current works mainly\nemphasize alignment and the inherent differences between unimodal modalities,\nneglecting the fact that there are also potential conflicts between bimodal\ncombinations. Additionally, multi-task learning-based conflict modeling methods\noften rely on the unstable generated labels. To address these challenges, we\npropose a novel multi-level conflict-aware network (MCAN) for multimodal\nsentiment analysis, which progressively segregates alignment and conflict\nconstituents from unimodal and bimodal representations, and further exploits\nthe conflict constituents with the conflict modeling branch. In the conflict\nmodeling branch, we conduct discrepancy constraints at both the representation\nand predicted output levels, avoiding dependence on the generated labels.\nExperimental results on the CMU-MOSI and CMU-MOSEI datasets demonstrate the\neffectiveness of the proposed MCAN.",
      "tldr_zh": "这项研究针对多模态情感分析（Multi-modal Sentiment Analysis, MSA）提出了一种新型多级冲突感知网络（MCAN），旨在充分利用文本、声学和视觉模态之间的交互，包括对齐和冲突方面。MCAN 通过逐步从单模态和双模态表示中分离对齐和冲突成分，并在冲突建模分支中应用表示和预测输出水平的差异约束，从而避免依赖不稳定的生成标签。实验结果显示，该方法在 CMU-MOSI 和 CMU-MOSEI 数据集上表现出色，证明了其在处理模态冲突方面的有效性。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "5 pages, 1 figure",
      "pdf_url": "http://arxiv.org/pdf/2502.09675v1",
      "published_date": "2025-02-13 09:14:36 UTC",
      "updated_date": "2025-02-13 09:14:36 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T11:07:16.200562"
    },
    {
      "arxiv_id": "2502.09083v1",
      "title": "Show Me the Work: Fact-Checkers' Requirements for Explainable Automated Fact-Checking",
      "title_zh": "翻译失败",
      "authors": [
        "Greta Warren",
        "Irina Shklovski",
        "Isabelle Augenstein"
      ],
      "abstract": "The pervasiveness of large language models and generative AI in online media\nhas amplified the need for effective automated fact-checking to assist\nfact-checkers in tackling the increasing volume and sophistication of\nmisinformation. The complex nature of fact-checking demands that automated\nfact-checking systems provide explanations that enable fact-checkers to\nscrutinise their outputs. However, it is unclear how these explanations should\nalign with the decision-making and reasoning processes of fact-checkers to be\neffectively integrated into their workflows. Through semi-structured interviews\nwith fact-checking professionals, we bridge this gap by: (i) providing an\naccount of how fact-checkers assess evidence, make decisions, and explain their\nprocesses; (ii) examining how fact-checkers use automated tools in practice;\nand (iii) identifying fact-checker explanation requirements for automated\nfact-checking tools. The findings show unmet explanation needs and identify\nimportant criteria for replicable fact-checking explanations that trace the\nmodel's reasoning path, reference specific evidence, and highlight uncertainty\nand information gaps.",
      "tldr_zh": "这篇论文探讨了事实核查员(fact-checkers)对可解释自动化事实核查(explainable automated fact-checking)的需求，强调大型语言模型和生成式 AI 导致的错误信息泛滥，需要系统提供可审视的解释以融入他们的工作流程。通过对事实核查专业人士的半结构化访谈，研究分析了他们评估证据、决策过程以及使用自动化工具的实际方式。关键发现包括事实核查员的解释需求未被满足，并提出了重要标准，如追踪模型的推理路径、引用具体证据、以及突出不确定性和信息缺口，以提升自动化工具的可信度和实用性。",
      "categories": [
        "cs.HC",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.HC",
      "comment": "Conditionally accepted to CHI'25",
      "pdf_url": "http://arxiv.org/pdf/2502.09083v1",
      "published_date": "2025-02-13 08:56:25 UTC",
      "updated_date": "2025-02-13 08:56:25 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T11:07:29.284296"
    },
    {
      "arxiv_id": "2502.09082v1",
      "title": "CoSER: Coordinating LLM-Based Persona Simulation of Established Roles",
      "title_zh": "CoSER",
      "authors": [
        "Xintao Wang",
        "Heng Wang",
        "Yifei Zhang",
        "Xinfeng Yuan",
        "Rui Xu",
        "Jen-tse Huang",
        "Siyu Yuan",
        "Haoran Guo",
        "Jiangjie Chen",
        "Wei Wang",
        "Yanghua Xiao",
        "Shuchang Zhou"
      ],
      "abstract": "Role-playing language agents (RPLAs) have emerged as promising applications\nof large language models (LLMs). However, simulating established characters\npresents a challenging task for RPLAs, due to the lack of authentic character\ndatasets and nuanced evaluation methods using such data. In this paper, we\npresent CoSER, a collection of a high-quality dataset, open models, and an\nevaluation protocol towards effective RPLAs of established characters. The\nCoSER dataset covers 17,966 characters from 771 renowned books. It provides\nauthentic dialogues with real-world intricacies, as well as diverse data types\nsuch as conversation setups, character experiences and internal thoughts.\nDrawing from acting methodology, we introduce given-circumstance acting for\ntraining and evaluating role-playing LLMs, where LLMs sequentially portray\nmultiple characters in book scenes. Using our dataset, we develop CoSER 8B and\nCoSER 70B, i.e., advanced open role-playing LLMs built on LLaMA-3.1 models.\nExtensive experiments demonstrate the value of the CoSER dataset for RPLA\ntraining, evaluation and retrieval. Moreover, CoSER 70B exhibits\nstate-of-the-art performance surpassing or matching GPT-4o on our evaluation\nand three existing benchmarks, i.e., achieving 75.80% and 93.47% accuracy on\nthe InCharacter and LifeChoice benchmarks respectively.",
      "tldr_zh": "本文提出 CoSER，这是一个针对角色扮演语言代理 (RPLAs) 的框架，包括一个高质量数据集、开源模型和评估协议，用于模拟已建立角色。该数据集涵盖 17,966 个来自 771 本著名书籍的角色，提供真实对话、对话设置、角色经历和内部想法，并引入 given-circumstance acting 方法，让 LLMs 顺序扮演多个角色进行训练和评估。基于 LLaMA-3.1 模型开发的 CoSER 8B 和 CoSER 70B 展示了出色性能，CoSER 70B 在 InCharacter 和 LifeChoice 基准上分别达到 75.80% 和 93.47% 的准确率，超越或匹配 GPT-4o。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2502.09082v1",
      "published_date": "2025-02-13 08:55:24 UTC",
      "updated_date": "2025-02-13 08:55:24 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T11:07:41.118540"
    },
    {
      "arxiv_id": "2502.17470v3",
      "title": "MC2SleepNet: Multi-modal Cross-masking with Contrastive Learning for Sleep Stage Classification",
      "title_zh": "翻译失败",
      "authors": [
        "Younghoon Na",
        "Hyun Keun Ahn",
        "Hyun-Kyung Lee",
        "Yoongeol Lee",
        "Seung Hun Oh",
        "Hongkwon Kim",
        "Jeong-Gun Lee"
      ],
      "abstract": "Sleep profoundly affects our health, and sleep deficiency or disorders can\ncause physical and mental problems. Despite significant findings from previous\nstudies, challenges persist in optimizing deep learning models, especially in\nmulti-modal learning for high-accuracy sleep stage classification. Our research\nintroduces MC2SleepNet (Multi-modal Cross-masking with Contrastive learning for\nSleep stage classification Network). It aims to facilitate the effective\ncollaboration between Convolutional Neural Networks (CNNs) and Transformer\narchitectures for multi-modal training with the help of contrastive learning\nand cross-masking. Raw single channel EEG signals and corresponding spectrogram\ndata provide differently characterized modalities for multi-modal learning. Our\nMC2SleepNet has achieved state-of-the-art performance with an accuracy of both\n84.6% on the SleepEDF-78 and 88.6% accuracy on the Sleep Heart Health Study\n(SHHS). These results demonstrate the effective generalization of our proposed\nnetwork across both small and large datasets.",
      "tldr_zh": "该研究提出MC2SleepNet，一种结合对比学习和交叉掩码的多模态网络，用于高精度睡眠阶段分类，旨在优化CNN和Transformer架构之间的协作。模型利用原始单通道EEG信号及其频谱图作为不同特征的模态输入，通过对比学习和交叉掩码技术提升多模态训练效果。在SleepEDF-78数据集上实现84.6%的准确率，在Sleep Heart Health Study (SHHS)上达到88.6%的准确率，展示了该网络在小和大数据集上的优越泛化性能和最先进表现。",
      "categories": [
        "eess.SP",
        "cs.AI"
      ],
      "primary_category": "eess.SP",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2502.17470v3",
      "published_date": "2025-02-13 08:33:38 UTC",
      "updated_date": "2025-02-27 02:23:20 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T11:07:52.184957"
    },
    {
      "arxiv_id": "2502.09056v3",
      "title": "Adapting Language-Specific LLMs to a Reasoning Model in One Day via Model Merging -- An Open Recipe",
      "title_zh": "翻译失败",
      "authors": [
        "Kunat Pipatanakul",
        "Pittawat Taveekitworachai",
        "Potsawee Manakul",
        "Kasima Tharnpipitchai"
      ],
      "abstract": "This paper investigates data selection and model merging methodologies aimed\nat incorporating advanced reasoning capabilities such as those of DeepSeek R1\ninto language-specific large language models (LLMs), with a particular focus on\nthe Thai LLM. Our goal is to enhance the reasoning capabilities of\nlanguage-specific LLMs while maintaining their target language abilities.\nDeepSeek R1 excels in reasoning but primarily benefits high-resource languages\nsuch as English and Chinese. However, low-resource languages remain underserved\ndue to the dominance of English-centric training data and model optimizations,\nwhich limit performance in these languages. This limitation results in\nunreliable code-switching and diminished effectiveness on tasks in low-resource\nlanguages. Meanwhile, local and regional LLM initiatives have attempted to\nbridge this gap by developing language-specific LLMs that focus on improving\nlocal linguistic fidelity. We demonstrate that, with only publicly available\ndatasets and a computational budget of $120, it is possible to enhance the\nreasoning capabilities of language-specific LLMs to match the level of DeepSeek\nR1, without compromising their performance on target language tasks.",
      "tldr_zh": "这篇论文探讨了通过数据选择和模型合并方法，将 DeepSeek R1 的高级推理能力整合到特定语言的 LLM（如泰语 LLM）中，目标是提升这些模型的推理性能，同时保持其目标语言的能力。研究指出，低资源语言的 LLM 因英语中心的数据和优化而表现不佳，导致代码切换不可靠和任务效率低下。实验结果显示，使用公开数据集和 120 美元的计算预算，即可在一天内将特定语言 LLM 的推理能力提升到与 DeepSeek R1 相当的水平，而不损害其原语言任务的性能，为低资源语言模型优化提供了一个开放的实用方案。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "9 pages",
      "pdf_url": "http://arxiv.org/pdf/2502.09056v3",
      "published_date": "2025-02-13 08:10:45 UTC",
      "updated_date": "2025-03-27 06:45:16 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T11:08:05.464461"
    },
    {
      "arxiv_id": "2502.09055v1",
      "title": "Exploring the Needs of Practising Musicians in Co-Creative AI Through Co-Design",
      "title_zh": "翻译失败",
      "authors": [
        "Stephen James Krol",
        "Maria Teresa Llano Rodriguez",
        "Miguel Loor Paredes"
      ],
      "abstract": "Recent advances in generative AI music have resulted in new technologies that\nare being framed as co-creative tools for musicians with early work\ndemonstrating their potential to add to music practice. While the field has\nseen many valuable contributions, work that involves practising musicians in\nthe design and development of these tools is limited, with the majority of work\nincluding them only once a tool has been developed. In this paper, we present a\ncase study that explores the needs of practising musicians through the\nco-design of a musical variation system, highlighting the importance of\ninvolving a diverse range of musicians throughout the design process and\nuncovering various design insights. This was achieved through two workshops and\na two week ecological evaluation, where musicians from different musical\nbackgrounds offered valuable insights not only on a musical system's design but\nalso on how a musical AI could be integrated into their musical practices.",
      "tldr_zh": "这篇论文通过共设计（Co-Design）方法探索了实践音乐家在合作创造AI（Co-Creative AI）中的需求，旨在解决现有AI音乐工具设计中缺乏早期参与音乐家的不足。研究团队组织了两个研讨会和为期两周的生态评估，让不同音乐背景的参与者提供反馈。结果强调了在设计过程中纳入多样化音乐家的必要性，并揭示了音乐变奏系统设计的关键见解，以及如何将AI整合到实际音乐实践中。",
      "categories": [
        "cs.HC",
        "cs.AI"
      ],
      "primary_category": "cs.HC",
      "comment": "Paper accepted into CHI 2025, Yokohama Japan, April 26th - May 1st",
      "pdf_url": "http://arxiv.org/pdf/2502.09055v1",
      "published_date": "2025-02-13 08:10:07 UTC",
      "updated_date": "2025-02-13 08:10:07 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T11:08:16.827825"
    },
    {
      "arxiv_id": "2502.17469v2",
      "title": "PixleepFlow: A Pixel-Based Lifelog Framework for Predicting Sleep Quality and Stress Level",
      "title_zh": "翻译失败",
      "authors": [
        "Younghoon Na",
        "Seunghun Oh",
        "Seongji Ko",
        "Hyunkyung Lee"
      ],
      "abstract": "The analysis of lifelogs can yield valuable insights into an individual's\ndaily life, particularly with regard to their health and well-being. The\naccurate assessment of quality of life is necessitated by the use of diverse\nsensors and precise synchronization. To rectify this issue, this study proposes\nthe image-based sleep quality and stress level estimation flow (PixleepFlow).\nPixleepFlow employs a conversion methodology into composite image data to\nexamine sleep patterns and their impact on overall health. Experiments were\nconducted using lifelog datasets to ascertain the optimal combination of data\nformats. In addition, we identified which sensor information has the greatest\ninfluence on the quality of life through Explainable Artificial\nIntelligence(XAI). As a result, PixleepFlow produced more significant results\nthan various data formats. This study was part of a written-based competition,\nand the additional findings from the lifelog dataset are detailed in Section\nSection IV. More information about PixleepFlow can be found at\nhttps://github.com/seongjiko/Pixleep.",
      "tldr_zh": "这篇论文提出了 PixleepFlow，一种基于像素的生命日志框架，用于通过图像数据转换方法预测个人的睡眠质量和压力水平。框架利用复合图像数据分析睡眠模式及其对整体健康的影响，并通过可解释人工智能(XAI)识别传感器信息对生活质量的关键影响。实验结果显示，PixleepFlow 在生命日志数据集上比其他数据格式表现出显著优势，提供更准确的预测性能。",
      "categories": [
        "eess.SP",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "eess.SP",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2502.17469v2",
      "published_date": "2025-02-13 08:09:15 UTC",
      "updated_date": "2025-02-26 03:32:26 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T11:08:27.863932"
    },
    {
      "arxiv_id": "2502.09054v2",
      "title": "Cost-Saving LLM Cascades with Early Abstention",
      "title_zh": "翻译失败",
      "authors": [
        "Michael J. Zellinger",
        "Rex Liu",
        "Matt Thomson"
      ],
      "abstract": "LLM cascades deploy small LLMs to answer most queries, limiting the use of\nlarge and expensive LLMs to difficult queries. This approach can significantly\nreduce costs without impacting performance. However, risk-sensitive domains\nsuch as finance or medicine place an additional premium on avoiding model\nerrors. Since even the most expensive models are susceptible to making\nmistakes, applications in these domains benefit from allowing LLM systems to\ncompletely abstain from answering difficult queries. Introducing abstention\nposes a design question for LLM cascades: should abstention only be allowed at\nthe final model or also at earlier models? Since the error patterns of small\nand large models are correlated, allowing earlier models to abstain may reduce\ninference costs and latency by anticipating abstention decisions by expensive\nand slow models, thus avoiding the need to run these models. We investigate the\nbenefits of such \"early abstention\" in LLM cascades and find that it reduces\noverall test loss by 2.2% on average across six benchmarks (GSM8K, MedMCQA,\nMMLU, TriviaQA, TruthfulQA, and XSum). These gains result from a more effective\nuse of abstention, trading a 4.1% average increase in the overall abstention\nrate for a 13.0% reduction in cost and a 5.0% reduction in error rate. Our\nfindings demonstrate the possibility of leveraging correlations between the\nerror patterns of different language models to drive performance improvements\nfor LLM systems with abstention.",
      "tldr_zh": "这篇论文探讨了LLM cascades框架，通过使用小型LLM处理大多数查询并仅在困难查询时调用大型LLM，来降低成本而不影响性能。作者引入early abstention机制，允许早期模型预测并放弃难以回答的查询，利用小型和大模型错误模式的相关性来优化系统。实验结果显示，在GSM8K、MedMCQA、MMLU、TriviaQA、TruthfulQA和XSum等六个基准上，early abstention平均减少测试损失2.2%，并以4.1%的abstention率增加换取13.0%的成本降低和5.0%的错误率减少，从而提升LLM系统的可靠性和效率。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "9 pages, 3 figures",
      "pdf_url": "http://arxiv.org/pdf/2502.09054v2",
      "published_date": "2025-02-13 08:08:39 UTC",
      "updated_date": "2025-03-29 01:19:05 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T11:08:42.000764"
    },
    {
      "arxiv_id": "2502.09053v1",
      "title": "Game Theory Meets Large Language Models: A Systematic Survey",
      "title_zh": "翻译失败",
      "authors": [
        "Haoran Sun",
        "Yusen Wu",
        "Yukun Cheng",
        "Xu Chu"
      ],
      "abstract": "Game theory establishes a fundamental framework for analyzing strategic\ninteractions among rational decision-makers. The rapid advancement of large\nlanguage models (LLMs) has sparked extensive research exploring the\nintersection of these two fields. Specifically, game-theoretic methods are\nbeing applied to evaluate and enhance LLM capabilities, while LLMs themselves\nare reshaping classic game models. This paper presents a comprehensive survey\nof the intersection of these fields, exploring a bidirectional relationship\nfrom three perspectives: (1) Establishing standardized game-based benchmarks\nfor evaluating LLM behavior; (2) Leveraging game-theoretic methods to improve\nLLM performance through algorithmic innovations; (3) Characterizing the\nsocietal impacts of LLMs through game modeling. Among these three aspects, we\nalso highlight how the equilibrium analysis for traditional game models is\nimpacted by LLMs' advanced language understanding, which in turn extends the\nstudy of game theory. Finally, we identify key challenges and future research\ndirections, assessing their feasibility based on the current state of the\nfield. By bridging theoretical rigor with emerging AI capabilities, this survey\naims to foster interdisciplinary collaboration and drive progress in this\nevolving research area.",
      "tldr_zh": "这篇论文系统调查了博弈论与大型语言模型(LLMs)的交叉点，探讨了双向关系：（1）使用基于博弈的标准化基准评估LLMs的行为；（2）通过博弈论方法和算法创新提升LLMs的性能；（3）通过博弈建模分析LLMs的社会影响。论文还强调了LLMs的先进语言理解如何扩展传统博弈模型的均衡分析，并为博弈论研究带来新视角。最后，它识别了关键挑战和未来方向，并评估其可行性，以促进跨学科合作和该领域的进步。",
      "categories": [
        "cs.AI",
        "cs.GT",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "10 pages",
      "pdf_url": "http://arxiv.org/pdf/2502.09053v1",
      "published_date": "2025-02-13 08:08:27 UTC",
      "updated_date": "2025-02-13 08:08:27 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T11:08:53.104594"
    },
    {
      "arxiv_id": "2502.09051v1",
      "title": "AIDE: Agentically Improve Visual Language Model with Domain Experts",
      "title_zh": "翻译失败",
      "authors": [
        "Ming-Chang Chiu",
        "Fuxiao Liu",
        "Karan Sapra",
        "Andrew Tao",
        "Yaser Jacoob",
        "Xuezhe Ma",
        "Zhiding Yu",
        "Guilin Liu"
      ],
      "abstract": "The enhancement of Visual Language Models (VLMs) has traditionally relied on\nknowledge distillation from larger, more capable models. This dependence\ncreates a fundamental bottleneck for improving state-of-the-art systems,\nparticularly when no superior models exist. We introduce AIDE (Agentic\nImprovement through Domain Experts), a novel framework that enables VLMs to\nautonomously enhance their capabilities by leveraging specialized domain expert\nmodels. AIDE operates through a four-stage process: (1) identifying instances\nfor refinement, (2) engaging domain experts for targeted analysis, (3)\nsynthesizing expert outputs with existing data, and (4) integrating enhanced\ninstances into the training pipeline. Experiments on multiple benchmarks,\nincluding MMMU, MME, MMBench, etc., demonstrate AIDE's ability to achieve\nnotable performance gains without relying on larger VLMs nor human supervision.\nOur framework provides a scalable, resource-efficient approach to continuous\nVLM improvement, addressing critical limitations in current methodologies,\nparticularly valuable when larger models are unavailable to access.",
      "tldr_zh": "该论文提出 AIDE 框架，一种创新方法，让 Visual Language Models (VLMs) 通过利用领域专家模型实现自主能力提升，从而避免依赖更大模型的知识蒸馏瓶颈。AIDE 通过四阶段过程——识别改进实例、参与专家针对性分析、合成专家输出与现有数据，以及整合到训练管道中——来实现高效优化。实验在 MMMU、MME 和 MMBench 等基准上显示了显著性能提升，且无需更大 VLMs 或人工监督，提供了一种可扩展、资源高效的持续改进策略。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.MA"
      ],
      "primary_category": "cs.CV",
      "comment": "6 pages, 4 figures, 2 tables",
      "pdf_url": "http://arxiv.org/pdf/2502.09051v1",
      "published_date": "2025-02-13 08:05:44 UTC",
      "updated_date": "2025-02-13 08:05:44 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T11:09:04.896459"
    },
    {
      "arxiv_id": "2502.09050v1",
      "title": "Leveraging Member-Group Relations via Multi-View Graph Filtering for Effective Group Recommendation",
      "title_zh": "翻译失败",
      "authors": [
        "Chae-Hyun Kim",
        "Yoon-Ryung Choi",
        "Jin-Duk Park",
        "Won-Yong Shin"
      ],
      "abstract": "Group recommendation aims at providing optimized recommendations tailored to\ndiverse groups, enabling groups to enjoy appropriate items. On the other hand,\nmost existing group recommendation methods are built upon deep neural network\n(DNN) architectures designed to capture the intricate relationships between\nmember-level and group-level interactions. While these DNN-based approaches\nhave proven their effectiveness, they require complex and expensive training\nprocedures to incorporate group-level interactions in addition to member-level\ninteractions. To overcome such limitations, we introduce Group-GF, a new\napproach for extremely fast recommendations of items to each group via\nmulti-view graph filtering (GF) that offers a holistic view of complex\nmember-group dynamics, without the need for costly model training.\nSpecifically, in Group-GF, we first construct three item similarity graphs\nmanifesting different viewpoints for GF. Then, we discover a distinct\npolynomial graph filter for each similarity graph and judiciously aggregate the\nthree graph filters. Extensive experiments demonstrate the effectiveness of\nGroup-GF in terms of significantly reducing runtime and achieving\nstate-of-the-art recommendation accuracy.",
      "tldr_zh": "本研究针对群组推荐（Group Recommendation）问题，提出了一种名为 Group-GF 的新方法，通过 Multi-View Graph Filtering 来利用成员-群组关系，提供高效的推荐建议，而无需复杂的 DNN 模型训练。Group-GF 首先构建三个物品相似性图，从不同视角体现成员-群组动态，然后为每个图设计一个多项式图过滤器（Graph Filtering），并聚合这些过滤器以生成整体视图。实验结果显示，Group-GF 显著降低了运行时间，同时在推荐准确率上达到了最先进水平。",
      "categories": [
        "cs.IR",
        "cs.AI",
        "cs.IT",
        "cs.LG",
        "cs.SI",
        "math.IT"
      ],
      "primary_category": "cs.IR",
      "comment": "5 pages, 3 figures, 4 tables; ACM Web Conference (WWW 2025) (to\n  appear) (Please cite our conference version.)",
      "pdf_url": "http://arxiv.org/pdf/2502.09050v1",
      "published_date": "2025-02-13 08:05:14 UTC",
      "updated_date": "2025-02-13 08:05:14 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T11:09:16.118923"
    },
    {
      "arxiv_id": "2502.09046v1",
      "title": "Criteria-Aware Graph Filtering: Extremely Fast Yet Accurate Multi-Criteria Recommendation",
      "title_zh": "标准感知图过滤：极其快速却准确的多标准推荐",
      "authors": [
        "Jin-Duk Park",
        "Jaemin Yoo",
        "Won-Yong Shin"
      ],
      "abstract": "Multi-criteria (MC) recommender systems, which utilize MC rating information\nfor recommendation, are increasingly widespread in various e-commerce domains.\nHowever, the MC recommendation using training-based collaborative filtering,\nrequiring consideration of multiple ratings compared to single-criterion\ncounterparts, often poses practical challenges in achieving state-of-the-art\nperformance along with scalable model training. To solve this problem, we\npropose CA-GF, a training-free MC recommendation method, which is built upon\ncriteria-aware graph filtering for efficient yet accurate MC recommendations.\nSpecifically, first, we construct an item-item similarity graph using an MC\nuser-expansion graph. Next, we design CA-GF composed of the following key\ncomponents, including 1) criterion-specific graph filtering where the optimal\nfilter for each criterion is found using various types of polynomial low-pass\nfilters and 2) criteria preference-infused aggregation where the smoothed\nsignals from each criterion are aggregated. We demonstrate that CA-GF is (a)\nefficient: providing the computational efficiency, offering the extremely fast\nruntime of less than 0.2 seconds even on the largest benchmark dataset, (b)\naccurate: outperforming benchmark MC recommendation methods, achieving\nsubstantial accuracy gains up to 24% compared to the best competitor, and (c)\ninterpretable: providing interpretations for the contribution of each criterion\nto the model prediction based on visualizations.",
      "tldr_zh": "本文提出 CA-GF，一种基于 Criteria-Aware Graph Filtering 的训练-free 多标准（Multi-Criteria）推荐方法，通过构建物品-物品相似性图和用户扩展图来实现高效推荐。关键组件包括针对每个标准的特定图过滤（使用多项式低通滤波器）和标准偏好注入聚合，以精确整合多标准评分信息。实验显示，CA-GF 在最大基准数据集上运行时间不到 0.2 秒，比最佳竞争对手准确率提高多达 24%，并通过可视化提供每个标准的贡献解释，从而提升推荐系统的可解释性。",
      "categories": [
        "cs.IR",
        "cs.AI",
        "cs.IT",
        "cs.LG",
        "cs.SI",
        "math.IT"
      ],
      "primary_category": "cs.IR",
      "comment": "12 pages, 8 figures, 7 tables; ACM Web Conference (WWW 2025) (to\n  appear) (Please cite our conference version.)",
      "pdf_url": "http://arxiv.org/pdf/2502.09046v1",
      "published_date": "2025-02-13 08:01:38 UTC",
      "updated_date": "2025-02-13 08:01:38 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T11:09:30.267762"
    },
    {
      "arxiv_id": "2502.09042v2",
      "title": "Typhoon T1: An Open Thai Reasoning Model",
      "title_zh": "Typhoon T1：一个开放的泰语推理模型",
      "authors": [
        "Pittawat Taveekitworachai",
        "Potsawee Manakul",
        "Kasima Tharnpipitchai",
        "Kunat Pipatanakul"
      ],
      "abstract": "This paper introduces Typhoon T1, an open effort to develop an open Thai\nreasoning model. A reasoning model is a relatively new type of generative model\nbuilt on top of large language models (LLMs). A reasoning model generates a\nlong chain of thought before arriving at a final answer, an approach found to\nimprove performance on complex tasks. However, details on developing such a\nmodel are limited, especially for reasoning models that can generate traces in\na low-resource language. Typhoon T1 presents an open effort that dives into the\ndetails of developing a reasoning model in a more cost-effective way by\nleveraging supervised fine-tuning using open datasets, instead of reinforcement\nlearning. This paper shares the details about synthetic data generation and\ntraining, as well as our dataset and model weights. Additionally, we provide\ninsights gained from developing a reasoning model that generalizes across\ndomains and is capable of generating reasoning traces in a low-resource\nlanguage, using Thai as an example. We hope this open effort provides a\nfoundation for further research in this field.",
      "tldr_zh": "本研究介绍了 Typhoon T1，这是一个开源的泰语推理模型，基于 Large Language Models (LLMs) 构建，通过生成长链式思考来提升复杂任务的性能。不同于传统的强化学习方法，该模型采用监督细调 (supervised fine-tuning) 和开源数据集进行更经济有效的开发，包括合成数据生成和训练过程。研究分享了数据集、模型权重以及从开发中获得的洞见，例如模型在不同领域实现泛化，并能在低资源语言如泰语中生成推理痕迹，为该领域进一步研究提供基础。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "25 pages, 6 figures",
      "pdf_url": "http://arxiv.org/pdf/2502.09042v2",
      "published_date": "2025-02-13 07:55:54 UTC",
      "updated_date": "2025-03-27 06:45:15 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T11:09:41.688890"
    },
    {
      "arxiv_id": "2502.09039v1",
      "title": "Large Images are Gaussians: High-Quality Large Image Representation with Levels of 2D Gaussian Splatting",
      "title_zh": "翻译失败",
      "authors": [
        "Lingting Zhu",
        "Guying Lin",
        "Jinnan Chen",
        "Xinjie Zhang",
        "Zhenchao Jin",
        "Zhao Wang",
        "Lequan Yu"
      ],
      "abstract": "While Implicit Neural Representations (INRs) have demonstrated significant\nsuccess in image representation, they are often hindered by large training\nmemory and slow decoding speed. Recently, Gaussian Splatting (GS) has emerged\nas a promising solution in 3D reconstruction due to its high-quality novel view\nsynthesis and rapid rendering capabilities, positioning it as a valuable tool\nfor a broad spectrum of applications. In particular, a GS-based representation,\n2DGS, has shown potential for image fitting. In our work, we present\n\\textbf{L}arge \\textbf{I}mages are \\textbf{G}aussians (\\textbf{LIG}), which\ndelves deeper into the application of 2DGS for image representations,\naddressing the challenge of fitting large images with 2DGS in the situation of\nnumerous Gaussian points, through two distinct modifications: 1) we adopt a\nvariant of representation and optimization strategy, facilitating the fitting\nof a large number of Gaussian points; 2) we propose a Level-of-Gaussian\napproach for reconstructing both coarse low-frequency initialization and fine\nhigh-frequency details. Consequently, we successfully represent large images as\nGaussian points and achieve high-quality large image representation,\ndemonstrating its efficacy across various types of large images. Code is\navailable at\n{\\href{https://github.com/HKU-MedAI/LIG}{https://github.com/HKU-MedAI/LIG}}.",
      "tldr_zh": "该研究提出LIG（Large Images are Gaussians）方法，利用2D Gaussian Splatting（2DGS）实现高质量的大图像表示，解决Implicit Neural Representations（INRs）在训练内存和解码速度上的局限性。LIG通过两种关键修改来处理大量Gaussian点的问题：一是采用变体表示和优化策略，支持高效拟合；二是引入Level-of-Gaussian方法，实现粗略低频初始化的重建和高频细节的精细捕捉。实验结果显示，LIG在各种类型的大图像上表现出色，成功地将图像表示为Gaussian点，并提供代码以供进一步验证。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "Accepted by 39th Annual AAAI Conference on Artificial Intelligence\n  (AAAI 2025). 10 pages, 4 figures",
      "pdf_url": "http://arxiv.org/pdf/2502.09039v1",
      "published_date": "2025-02-13 07:48:56 UTC",
      "updated_date": "2025-02-13 07:48:56 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T11:09:53.677707"
    },
    {
      "arxiv_id": "2502.09038v1",
      "title": "AoI-Sensitive Data Forwarding with Distributed Beamforming in UAV-Assisted IoT",
      "title_zh": "翻译失败",
      "authors": [
        "Zifan Lang",
        "Guixia Liu",
        "Geng Sun",
        "Jiahui Li",
        "Zemin Sun",
        "Jiacheng Wang",
        "Victor C. M. Leung"
      ],
      "abstract": "This paper proposes a UAV-assisted forwarding system based on distributed\nbeamforming to enhance age of information (AoI) in Internet of Things (IoT).\nSpecifically, UAVs collect and relay data between sensor nodes (SNs) and the\nremote base station (BS). However, flight delays increase the AoI and degrade\nthe network performance. To mitigate this, we adopt distributed beamforming to\nextend the communication range, reduce the flight frequency and ensure the\ncontinuous data relay and efficient energy utilization. Then, we formulate an\noptimization problem to minimize AoI and UAV energy consumption, by jointly\noptimizing the UAV trajectories and communication schedules. The problem is\nnon-convex and with high dynamic, and thus we propose a deep reinforcement\nlearning (DRL)-based algorithm to solve the problem, thereby enhancing the\nstability and accelerate convergence speed. Simulation results show that the\nproposed algorithm effectively addresses the problem and outperforms other\nbenchmark algorithms.",
      "tldr_zh": "本论文提出了一种基于 UAV 的数据转发系统，利用分布式 beamforming 技术来提升 IoT 中的 AoI（信息新鲜度），通过扩展通信范围减少飞行频率并优化能源利用。研究者制定了一个联合优化 UAV 轨迹和通信调度的非凸问题，以最小化 AoI 和能源消耗，并采用 DRL（深度强化学习）算法来解决该问题，提高系统稳定性和收敛速度。模拟结果显示，该算法在性能上优于基准算法，有效改善了网络性能。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "6 pages, 4 figures, ICC2025",
      "pdf_url": "http://arxiv.org/pdf/2502.09038v1",
      "published_date": "2025-02-13 07:48:36 UTC",
      "updated_date": "2025-02-13 07:48:36 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T11:10:05.796006"
    },
    {
      "arxiv_id": "2502.09022v2",
      "title": "Mechanistic Unveiling of Transformer Circuits: Self-Influence as a Key to Model Reasoning",
      "title_zh": "翻译失败",
      "authors": [
        "Lin Zhang",
        "Lijie Hu",
        "Di Wang"
      ],
      "abstract": "Transformer-based language models have achieved significant success; however,\ntheir internal mechanisms remain largely opaque due to the complexity of\nnon-linear interactions and high-dimensional operations. While previous studies\nhave demonstrated that these models implicitly embed reasoning trees, humans\ntypically employ various distinct logical reasoning mechanisms to complete the\nsame task. It is still unclear which multi-step reasoning mechanisms are used\nby language models to solve such tasks. In this paper, we aim to address this\nquestion by investigating the mechanistic interpretability of language models,\nparticularly in the context of multi-step reasoning tasks. Specifically, we\nemploy circuit analysis and self-influence functions to evaluate the changing\nimportance of each token throughout the reasoning process, allowing us to map\nthe reasoning paths adopted by the model. We apply this methodology to the\nGPT-2 model on a prediction task (IOI) and demonstrate that the underlying\ncircuits reveal a human-interpretable reasoning process used by the model.",
      "tldr_zh": "这篇论文探讨了Transformer模型在多步推理任务中的机制可解释性问题，针对模型内部非线性交互和高维操作导致的不透明性。研究者使用circuit analysis和self-influence functions来评估每个token在推理过程中的重要性变化，从而映射出模型的推理路径。在GPT-2模型上应用于IOI任务，实验结果显示这些底层电路揭示了人类可解释的推理机制，为理解语言模型的推理过程提供了新见解。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "Accepted by NAACL2025",
      "pdf_url": "http://arxiv.org/pdf/2502.09022v2",
      "published_date": "2025-02-13 07:19:05 UTC",
      "updated_date": "2025-02-14 05:46:53 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T11:10:17.477057"
    },
    {
      "arxiv_id": "2502.09020v1",
      "title": "EventSTR: A Benchmark Dataset and Baselines for Event Stream based Scene Text Recognition",
      "title_zh": "翻译失败",
      "authors": [
        "Xiao Wang",
        "Jingtao Jiang",
        "Dong Li",
        "Futian Wang",
        "Lin Zhu",
        "Yaowei Wang",
        "Yongyong Tian",
        "Jin Tang"
      ],
      "abstract": "Mainstream Scene Text Recognition (STR) algorithms are developed based on RGB\ncameras which are sensitive to challenging factors such as low illumination,\nmotion blur, and cluttered backgrounds. In this paper, we propose to recognize\nthe scene text using bio-inspired event cameras by collecting and annotating a\nlarge-scale benchmark dataset, termed EventSTR. It contains 9,928\nhigh-definition (1280 * 720) event samples and involves both Chinese and\nEnglish characters. We also benchmark multiple STR algorithms as the baselines\nfor future works to compare. In addition, we propose a new event-based scene\ntext recognition framework, termed SimC-ESTR. It first extracts the event\nfeatures using a visual encoder and projects them into tokens using a Q-former\nmodule. More importantly, we propose to augment the vision tokens based on a\nmemory mechanism before feeding into the large language models. A\nsimilarity-based error correction mechanism is embedded within the large\nlanguage model to correct potential minor errors fundamentally based on\ncontextual information. Extensive experiments on the newly proposed EventSTR\ndataset and two simulation STR datasets fully demonstrate the effectiveness of\nour proposed model. We believe that the dataset and algorithmic model can\ninnovatively propose an event-based STR task and are expected to accelerate the\napplication of event cameras in various industries. The source code and\npre-trained models will be released on https://github.com/Event-AHU/EventSTR",
      "tldr_zh": "这篇论文提出了EventSTR数据集，这是一个大规模基准，用于基于事件流的场景文本识别(STR)，包含9,928个高清(1280 * 720)事件样本，支持中文和英文字符，并提供了多个STR算法的基线以便后续比较。作者开发了新的框架SimC-ESTR，通过视觉编码器提取事件特征、Q-former模块投影为tokens，并使用记忆机制增强tokens，同时嵌入基于相似性的错误修正机制来利用上下文信息修正错误。实验在EventSTR数据集和两个模拟数据集上验证了框架的有效性，展示了其在低光照、运动模糊等挑战条件下的鲁棒性，并有望推动event cameras在各种行业的应用。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "In Peer Review",
      "pdf_url": "http://arxiv.org/pdf/2502.09020v1",
      "published_date": "2025-02-13 07:16:16 UTC",
      "updated_date": "2025-02-13 07:16:16 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T11:10:30.263810"
    },
    {
      "arxiv_id": "2502.09018v1",
      "title": "Zero-shot Concept Bottleneck Models",
      "title_zh": "零样本概念瓶颈模型",
      "authors": [
        "Shin'ya Yamaguchi",
        "Kosuke Nishida",
        "Daiki Chijiwa",
        "Yasutoshi Ida"
      ],
      "abstract": "Concept bottleneck models (CBMs) are inherently interpretable and\nintervenable neural network models, which explain their final label prediction\nby the intermediate prediction of high-level semantic concepts. However, they\nrequire target task training to learn input-to-concept and concept-to-label\nmappings, incurring target dataset collections and training resources. In this\npaper, we present \\textit{zero-shot concept bottleneck models} (Z-CBMs), which\npredict concepts and labels in a fully zero-shot manner without training neural\nnetworks. Z-CBMs utilize a large-scale concept bank, which is composed of\nmillions of vocabulary extracted from the web, to describe arbitrary input in\nvarious domains. For the input-to-concept mapping, we introduce concept\nretrieval, which dynamically finds input-related concepts by the cross-modal\nsearch on the concept bank. In the concept-to-label inference, we apply concept\nregression to select essential concepts from the retrieved concepts by sparse\nlinear regression. Through extensive experiments, we confirm that our Z-CBMs\nprovide interpretable and intervenable concepts without any additional\ntraining. Code will be available at https://github.com/yshinya6/zcbm.",
      "tldr_zh": "本文提出 Zero-shot Concept Bottleneck Models (Z-CBMs)，一种无需训练即可实现高阶概念预测和标签推断的可解释神经网络模型，解决了传统 CBMs 需要目标数据集和训练资源的问题。Z-CBMs 利用一个由数百万网络词汇组成的大型概念库，通过 concept retrieval 进行跨模态搜索动态获取输入相关概念，并采用 concept regression 的稀疏线性回归从这些概念中选择关键要素。实验证明，Z-CBMs 能在各种领域提供可靠的可解释性和可干预性概念预测，而无需额外训练。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.LG",
      "comment": "14 pages, 8 figures",
      "pdf_url": "http://arxiv.org/pdf/2502.09018v1",
      "published_date": "2025-02-13 07:11:07 UTC",
      "updated_date": "2025-02-13 07:11:07 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T11:10:41.846944"
    },
    {
      "arxiv_id": "2502.09003v2",
      "title": "RoSTE: An Efficient Quantization-Aware Supervised Fine-Tuning Approach for Large Language Models",
      "title_zh": "翻译失败",
      "authors": [
        "Quan Wei",
        "Chung-Yiu Yau",
        "Hoi-To Wai",
        "Yang Katie Zhao",
        "Dongyeop Kang",
        "Youngsuk Park",
        "Mingyi Hong"
      ],
      "abstract": "Supervised fine-tuning is a standard method for adapting pre-trained large\nlanguage models (LLMs) to downstream tasks. Quantization has been recently\nstudied as a post-training technique for efficient LLM deployment. To obtain\nquantized fine-tuned LLMs, conventional pipelines would first fine-tune the\npre-trained models, followed by post-training quantization. This often yields\nsuboptimal performance as it fails to leverage the synergy between fine-tuning\nand quantization. To effectively realize low-bit quantization of weights,\nactivations, and KV caches in LLMs, we propose an algorithm named Rotated\nStraight-Through-Estimator (RoSTE), which combines quantization-aware\nsupervised fine-tuning (QA-SFT) with an adaptive rotation strategy that\nidentifies an effective rotation configuration to reduce activation outliers.\nWe provide theoretical insights on RoSTE by analyzing its prediction error when\napplied to an overparameterized least square quantized training problem. Our\nfindings reveal that the prediction error is directly proportional to the\nquantization error of the converged weights, which can be effectively managed\nthrough an optimized rotation configuration. Experiments on Pythia, Qwen and\nLlama models of different sizes demonstrate the effectiveness of RoSTE.\nCompared to existing post-SFT quantization baselines, our method consistently\nachieves superior performances across various tasks and different LLM\narchitectures.",
      "tldr_zh": "该研究提出 RoSTE 算法，一种高效的量化感知监督微调 (QA-SFT) 方法，用于大型语言模型 (LLMs)，通过结合自适应旋转策略减少激活异常值，从而实现权重、激活和 KV caches 的低位量化。RoSTE 优化了微调和量化的协同效应，避免了传统方法的次优性能，并通过理论分析证明预测错误与量化错误成正比，可通过旋转配置有效管理。实验在 Pythia、Qwen 和 Llama 模型上显示，RoSTE 比现有后微调量化基线在多种任务和模型架构中表现出色性能。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "20 pages, 7 figures",
      "pdf_url": "http://arxiv.org/pdf/2502.09003v2",
      "published_date": "2025-02-13 06:44:33 UTC",
      "updated_date": "2025-03-21 19:26:12 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T11:10:54.084997"
    },
    {
      "arxiv_id": "2502.09674v2",
      "title": "The Hidden Dimensions of LLM Alignment: A Multi-Dimensional Safety Analysis",
      "title_zh": "LLM 对齐的隐藏维度：一种多维度的安全分析",
      "authors": [
        "Wenbo Pan",
        "Zhichao Liu",
        "Qiguang Chen",
        "Xiangyang Zhou",
        "Haining Yu",
        "Xiaohua Jia"
      ],
      "abstract": "Large Language Models' safety-aligned behaviors, such as refusing harmful\nqueries, can be represented by linear directions in activation space. Previous\nresearch modeled safety behavior with a single direction, limiting mechanistic\nunderstanding to an isolated safety feature. In this work, we discover that\nsafety-aligned behavior is jointly controlled by multi-dimensional directions.\nNamely, we study the vector space of representation shifts during safety\nfine-tuning on Llama 3 8B for refusing jailbreaks. By studying orthogonal\ndirections in the space, we first find that a dominant direction governs the\nmodel's refusal behavior, while multiple smaller directions represent distinct\nand interpretable features like hypothetical narrative and role-playing. We\nthen measure how different directions promote or suppress the dominant\ndirection, showing the important role of secondary directions in shaping the\nmodel's refusal representation. Finally, we demonstrate that removing certain\ntrigger tokens in harmful queries can mitigate these directions to bypass the\nlearned safety capability, providing new insights on understanding safety\nalignment vulnerability from a multi-dimensional perspective. Code and\nartifacts are available at https://github.com/BMPixel/safety-residual-space.",
      "tldr_zh": "本文研究发现，大语言模型(LLMs)的安全对齐行为（如拒绝有害查询）并非由单一方向控制，而是由多维方向共同影响，通过分析 Llama 3 8B 模型的安全微调表示空间，识别出一个主导方向负责拒绝行为，以及多个次要方向代表可解释特征如假设叙述和角色扮演。研究进一步探索了这些方向如何促进或抑制主导方向，突显次要方向在塑造安全表示中的关键作用。最终，实验证明移除有害查询中的触发词可缓解这些方向，从而绕过安全能力，为理解LLM安全对齐的脆弱性提供了多维视角。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "Code and artifacts: https://github.com/BMPixel/safety-residual-space",
      "pdf_url": "http://arxiv.org/pdf/2502.09674v2",
      "published_date": "2025-02-13 06:39:22 UTC",
      "updated_date": "2025-02-18 03:24:45 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T11:11:07.739183"
    },
    {
      "arxiv_id": "2502.09673v2",
      "title": "Are Smarter LLMs Safer? Exploring Safety-Reasoning Trade-offs in Prompting and Fine-Tuning",
      "title_zh": "翻译失败",
      "authors": [
        "Ang Li",
        "Yichuan Mo",
        "Mingjie Li",
        "Yifei Wang",
        "Yisen Wang"
      ],
      "abstract": "Large Language Models (LLMs) have demonstrated remarkable success across\nvarious NLP benchmarks. However, excelling in complex tasks that require\nnuanced reasoning and precise decision-making demands more than raw language\nproficiency--LLMs must reason, i.e., think logically, draw from past\nexperiences, and synthesize information to reach conclusions and take action.\nTo enhance reasoning abilities, approaches such as prompting and fine-tuning\nhave been widely explored. While these methods have led to clear improvements\nin reasoning, their impact on LLM safety remains less understood. In this work,\nwe investigate the interplay between reasoning and safety in LLMs. We highlight\nthe latent safety risks that arise as reasoning capabilities improve, shedding\nlight on previously overlooked vulnerabilities. At the same time, we explore\nhow reasoning itself can be leveraged to enhance safety, uncovering potential\nmitigation strategies. By examining both the risks and opportunities in\nreasoning-driven LLM safety, our study provides valuable insights for\ndeveloping models that are not only more capable but also more trustworthy in\nreal-world deployments.",
      "tldr_zh": "这篇论文探讨了大型语言模型(LLMs)中推理能力与安全性的权衡关系，重点分析提升推理能力（如通过prompting和fine-tuning）是否会影响模型的安全性。研究发现，随着推理能力的改进，LLMs可能面临潜在的安全风险，包括逻辑错误和易受攻击的漏洞。另一方面，论文也探讨了如何利用推理机制来强化安全性，并提出缓解策略。该工作为开发更强大且更可信的LLMs提供了重要见解。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2502.09673v2",
      "published_date": "2025-02-13 06:37:28 UTC",
      "updated_date": "2025-02-21 03:12:17 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T11:11:17.294127"
    },
    {
      "arxiv_id": "2502.08995v1",
      "title": "PixLift: Accelerating Web Browsing via AI Upscaling",
      "title_zh": "翻译失败",
      "authors": [
        "Yonas Atinafu",
        "Sarthak Malla",
        "HyunSeok Daniel Jang",
        "Nouar Aldahoul",
        "Matteo Varvello",
        "Yasir Zaki"
      ],
      "abstract": "Accessing the internet in regions with expensive data plans and limited\nconnectivity poses significant challenges, restricting information access and\neconomic growth. Images, as a major contributor to webpage sizes, exacerbate\nthis issue, despite advances in compression formats like WebP and AVIF. The\ncontinued growth of complex and curated web content, coupled with suboptimal\noptimization practices in many regions, has prevented meaningful reductions in\nweb page sizes. This paper introduces PixLift, a novel solution to reduce\nwebpage sizes by downscaling their images during transmission and leveraging AI\nmodels on user devices to upscale them. By trading computational resources for\nbandwidth, PixLift enables more affordable and inclusive web access. We address\nkey challenges, including the feasibility of scaled image requests on popular\nwebsites, the implementation of PixLift as a browser extension, and its impact\non user experience. Through the analysis of 71.4k webpages, evaluations of\nthree mainstream upscaling models, and a user study, we demonstrate PixLift's\nability to significantly reduce data usage without compromising image quality,\nfostering a more equitable internet.",
      "tldr_zh": "本研究针对数据计划昂贵和连接有限的地区网络访问挑战，提出PixLift系统，通过在传输时缩小网页图像并在用户设备上使用AI upscaling模型进行放大，从而显著减少数据使用。PixLift以浏览器扩展形式实现，解决了图像请求可行性及用户体验问题，并通过分析71.4k网页、评估三种主流upscaling模型以及用户研究，证明其能有效降低带宽需求而不牺牲图像质量。最终，该系统促进更公平和包容的互联网访问，推动经济成长。",
      "categories": [
        "cs.PF",
        "cs.AI"
      ],
      "primary_category": "cs.PF",
      "comment": "9 pages, 2 figures",
      "pdf_url": "http://arxiv.org/pdf/2502.08995v1",
      "published_date": "2025-02-13 06:14:59 UTC",
      "updated_date": "2025-02-13 06:14:59 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T11:11:29.213116"
    },
    {
      "arxiv_id": "2502.08989v2",
      "title": "RLSA-PFL: Robust Lightweight Secure Aggregation with Model Inconsistency Detection in Privacy-Preserving Federated Learning",
      "title_zh": "翻译失败",
      "authors": [
        "Nazatul H. Sultan",
        "Yan Bo",
        "Yansong Gao",
        "Seyit Camtepe",
        "Arash Mahboubi",
        "Hang Thanh Bui",
        "Aufeef Chauhan",
        "Hamed Aboutorab",
        "Michael Bewong",
        "Dineshkumar Singh",
        "Praveen Gauravaram",
        "Rafiqul Islam",
        "Sharif Abuadbba"
      ],
      "abstract": "Federated Learning (FL) allows users to collaboratively train a global\nmachine learning model by sharing local model only, without exposing their\nprivate data to a central server. This distributed learning is particularly\nappealing in scenarios where data privacy is crucial, and it has garnered\nsubstantial attention from both industry and academia. However, studies have\nrevealed privacy vulnerabilities in FL, where adversaries can potentially infer\nsensitive information from the shared model parameters. In this paper, we\npresent an efficient masking-based secure aggregation scheme utilizing\nlightweight cryptographic primitives to mitigate privacy risks. Our scheme\noffers several advantages over existing methods. First, it requires only a\nsingle setup phase for the entire FL training session, significantly reducing\ncommunication overhead. Second, it minimizes user-side overhead by eliminating\nthe need for user-to-user interactions, utilizing an intermediate server layer\nand a lightweight key negotiation method. Third, the scheme is highly resilient\nto user dropouts, and the users can join at any FL round. Fourth, it can detect\nand defend against malicious server activities, including recently discovered\nmodel inconsistency attacks. Finally, our scheme ensures security in both\nsemi-honest and malicious settings. We provide security analysis to formally\nprove the robustness of our approach. Furthermore, we implemented an end-to-end\nprototype of our scheme. We conducted comprehensive experiments and\ncomparisons, which show that it outperforms existing solutions in terms of\ncommunication and computation overhead, functionality, and security.",
      "tldr_zh": "本研究提出RLSA-PFL，一种基于掩码的轻量级secure aggregation方案，用于提升Privacy-Preserving Federated Learning中的隐私保护。该方案利用轻量级加密原语，仅需一个设置阶段即可减少通信开销、消除用户间交互，并支持用户动态加入退出，同时能检测和防御恶意服务器活动如model inconsistency attacks，在半诚实和恶意环境中均确保安全性。实验结果显示，RLSA-PFL在通信和计算开销上优于现有方法，并通过安全分析和端到端原型验证了其鲁棒性。",
      "categories": [
        "cs.CR",
        "cs.AI",
        "68P27",
        "E.3"
      ],
      "primary_category": "cs.CR",
      "comment": "16 pages, 10 Figures",
      "pdf_url": "http://arxiv.org/pdf/2502.08989v2",
      "published_date": "2025-02-13 06:01:09 UTC",
      "updated_date": "2025-04-16 11:52:45 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T11:11:41.706034"
    },
    {
      "arxiv_id": "2502.08987v2",
      "title": "Neural Force Field: Learning Generalized Physical Representation from a Few Examples",
      "title_zh": "Neural Force Field：从少量示例学习广义物理表示",
      "authors": [
        "Shiqian Li",
        "Ruihong Shen",
        "Chi Zhang",
        "Yixin Zhu"
      ],
      "abstract": "Physical reasoning is a remarkable human ability that enables rapid learning\nand generalization from limited experience. Current AI models, despite\nextensive training, still struggle to achieve similar generalization,\nespecially in Out-of-distribution (OOD) settings. This limitation stems from\ntheir inability to abstract core physical principles from observations. A key\nchallenge is developing representations that can efficiently learn and\ngeneralize physical dynamics from minimal data. Here we present Neural Force\nField (NFF) a modeling framework built on Neural Ordinary Differential Equation\n(NODE) that learns interpretable force field representations which can be\nefficiently integrated through an Ordinary Differential Equation ( ODE) solver\nto predict object trajectories. Unlike existing approaches that rely on\nhigh-dimensional latent spaces, NFF captures fundamental physical concepts such\nas gravity, support, and collision in an interpretable manner. Experiments on\ntwo challenging physical reasoning tasks demonstrate that NFF, trained with\nonly a few examples, achieves strong generalization to unseen scenarios. This\nphysics-grounded representation enables efficient forward-backward planning and\nrapid adaptation through interactive refinement. Our work suggests that\nincorporating physics-inspired representations into learning systems can help\nbridge the gap between artificial and human physical reasoning capabilities.",
      "tldr_zh": "本研究探讨了AI在物理推理中的泛化挑战，人类能从有限经验快速学习，而现有模型在Out-of-distribution (OOD)场景下表现欠佳。作者提出Neural Force Field (NFF)框架，基于Neural Ordinary Differential Equation (NODE)，学习可解释的力场表示，通过Ordinary Differential Equation (ODE)求解器预测物体轨迹，并捕捉如重力、支撑和碰撞等核心物理概念。实验显示，NFF仅需少量示例训练，即可在两个挑战性任务上实现强泛化，支持高效的前向-后向规划和快速适应，从而帮助缩小AI与人类物理推理能力的差距。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "20 pages",
      "pdf_url": "http://arxiv.org/pdf/2502.08987v2",
      "published_date": "2025-02-13 05:50:13 UTC",
      "updated_date": "2025-02-14 06:29:09 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T11:11:53.697876"
    },
    {
      "arxiv_id": "2502.08985v1",
      "title": "Few is More: Task-Efficient Skill-Discovery for Multi-Task Offline Multi-Agent Reinforcement Learning",
      "title_zh": "翻译失败",
      "authors": [
        "Xun Wang",
        "Zhuoran Li",
        "Hai Zhong",
        "Longbo Huang"
      ],
      "abstract": "As a data-driven approach, offline MARL learns superior policies solely from\noffline datasets, ideal for domains rich in historical data but with high\ninteraction costs and risks. However, most existing methods are task-specific,\nrequiring retraining for new tasks, leading to redundancy and inefficiency. To\naddress this issue, in this paper, we propose a task-efficient multi-task\noffline MARL algorithm, Skill-Discovery Conservative Q-Learning (SD-CQL).\nUnlike existing offline skill-discovery methods, SD-CQL discovers skills by\nreconstructing the next observation. It then evaluates fixed and variable\nactions separately and employs behavior-regularized conservative Q-learning to\nexecute the optimal action for each skill. This approach eliminates the need\nfor local-global alignment and enables strong multi-task generalization from\nlimited small-scale source tasks. Substantial experiments on StarCraftII\ndemonstrates the superior generalization performance and task-efficiency of\nSD-CQL. It achieves the best performance on $\\textbf{10}$ out of $14$ task\nsets, with up to $\\textbf{65%}$ improvement on individual task sets, and is\nwithin $4\\%$ of the best baseline on the remaining four.",
      "tldr_zh": "该论文提出了一种任务高效的多任务离线多智能体强化学习（offline MARL）算法，名为 Skill-Discovery Conservative Q-Learning (SD-CQL)，旨在解决现有方法任务特定性导致的冗余问题。SD-CQL 通过重构下一个观察（reconstructing the next observation）来发现技能，并分别评估固定和可变动作，使用行为正则化的保守 Q-learning 执行每个技能的最佳动作，从而实现从有限小规模源任务中的强多任务泛化。实验在 StarCraftII 上显示，SD-CQL 在 14 个任务集中的 10 个上取得最佳性能，个别任务集提升高达 65%，在剩余四个上与最佳基线相差不到 4%。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.MA"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2502.08985v1",
      "published_date": "2025-02-13 05:47:57 UTC",
      "updated_date": "2025-02-13 05:47:57 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T11:12:05.760167"
    },
    {
      "arxiv_id": "2502.08972v3",
      "title": "Tuning-Free Personalized Alignment via Trial-Error-Explain In-Context Learning",
      "title_zh": "翻译失败",
      "authors": [
        "Hyundong Cho",
        "Karishma Sharma",
        "Nicolaas Jedema",
        "Leonardo F. R. Ribeiro",
        "Alessandro Moschitti",
        "Ravi Krishnan",
        "Jonathan May"
      ],
      "abstract": "Language models are aligned to the collective voice of many, resulting in\ngeneric outputs that do not align with specific users' styles. In this work, we\npresent Trial-Error-Explain In-Context Learning (TICL), a tuning-free method\nthat personalizes language models for text generation tasks with fewer than 10\nexamples per user. TICL iteratively expands an in-context learning prompt via a\ntrial-error-explain process, adding model-generated negative samples and\nexplanations that provide fine-grained guidance towards a specific user's\nstyle. TICL achieves favorable win rates on pairwise comparisons with\nLLM-as-a-judge up to 91.5% against the previous state-of-the-art and\noutperforms competitive tuning-free baselines for personalized alignment tasks\nof writing emails, essays and news articles. Both lexical and qualitative\nanalyses show that the negative samples and explanations enable language models\nto learn stylistic context more effectively and overcome the bias towards\nstructural and formal phrases observed in their zero-shot outputs. By\nfront-loading inference compute to create a user-specific in-context learning\nprompt that does not require extra generation steps at test time, TICL presents\na novel yet simple approach for personalized alignment.",
      "tldr_zh": "该研究提出了一种无调优方法Trial-Error-Explain In-Context Learning (TICL)，旨在通过少于10个示例个性化语言模型的文本生成，使其输出更符合特定用户的风格。TICL 通过迭代扩展in-context learning提示，包括生成负面样本和解释，以提供细粒度的风格指导，从而克服模型对结构化和正式短语的偏见。实验结果显示，TICL 在配对比较中胜率高达91.5%，并在写作电子邮件、论文和新闻文章等任务中优于现有基线，展示了其高效的前置计算机制，为个性化对齐提供了简单有效的解决方案。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "NAACL 2025 Findings",
      "pdf_url": "http://arxiv.org/pdf/2502.08972v3",
      "published_date": "2025-02-13 05:20:21 UTC",
      "updated_date": "2025-04-05 11:57:48 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T11:12:18.484789"
    },
    {
      "arxiv_id": "2502.08969v1",
      "title": "SkyRover: A Modular Simulator for Cross-Domain Pathfinding",
      "title_zh": "SkyRover：用于跨领域路径规划的模块化模拟器",
      "authors": [
        "Wenhui Ma",
        "Wenhao Li",
        "Bo Jin",
        "Changhong Lu",
        "Xiangfeng Wang"
      ],
      "abstract": "Unmanned Aerial Vehicles (UAVs) and Automated Guided Vehicles (AGVs)\nincreasingly collaborate in logistics, surveillance, inspection tasks and etc.\nHowever, existing simulators often focus on a single domain, limiting\ncross-domain study. This paper presents the SkyRover, a modular simulator for\nUAV-AGV multi-agent pathfinding (MAPF). SkyRover supports realistic agent\ndynamics, configurable 3D environments, and convenient APIs for external\nsolvers and learning methods. By unifying ground and aerial operations, it\nfacilitates cross-domain algorithm design, testing, and benchmarking.\nExperiments highlight SkyRover's capacity for efficient pathfinding and\nhigh-fidelity simulations in UAV-AGV coordination. Project is available at\nhttps://sites.google.com/view/mapf3d/home.",
      "tldr_zh": "该论文提出 SkyRover，一个模块化的模拟器，用于跨领域的多智能体路径寻找 (MAPF)，以解决 UAVs 和 AGVs 在物流、监视和检查任务中协作的模拟需求。SkyRover 支持现实的代理动态、可配置的 3D 环境，以及方便的 API 接口，方便外部求解器和学习方法的集成。通过统一地面和空中操作，该模拟器促进算法设计、测试和基准测试。实验结果显示，SkyRover 在 UAV-AGV 协调中实现了高效路径寻找和高保真模拟。",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.LG",
        "cs.MA"
      ],
      "primary_category": "cs.RO",
      "comment": "9 pages",
      "pdf_url": "http://arxiv.org/pdf/2502.08969v1",
      "published_date": "2025-02-13 05:13:21 UTC",
      "updated_date": "2025-02-13 05:13:21 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T11:12:30.395287"
    },
    {
      "arxiv_id": "2502.08966v2",
      "title": "RTBAS: Defending LLM Agents Against Prompt Injection and Privacy Leakage",
      "title_zh": "翻译失败",
      "authors": [
        "Peter Yong Zhong",
        "Siyuan Chen",
        "Ruiqi Wang",
        "McKenna McCall",
        "Ben L. Titzer",
        "Heather Miller",
        "Phillip B. Gibbons"
      ],
      "abstract": "Tool-Based Agent Systems (TBAS) allow Language Models (LMs) to use external\ntools for tasks beyond their standalone capabilities, such as searching\nwebsites, booking flights, or making financial transactions. However, these\ntools greatly increase the risks of prompt injection attacks, where malicious\ncontent hijacks the LM agent to leak confidential data or trigger harmful\nactions. Existing defenses (OpenAI GPTs) require user confirmation before every\ntool call, placing onerous burdens on users. We introduce Robust TBAS (RTBAS),\nwhich automatically detects and executes tool calls that preserve integrity and\nconfidentiality, requiring user confirmation only when these safeguards cannot\nbe ensured. RTBAS adapts Information Flow Control to the unique challenges\npresented by TBAS. We present two novel dependency screeners, using\nLM-as-a-judge and attention-based saliency, to overcome these challenges.\nExperimental results on the AgentDojo Prompt Injection benchmark show RTBAS\nprevents all targeted attacks with only a 2% loss of task utility when under\nattack, and further tests confirm its ability to obtain near-oracle performance\non detecting both subtle and direct privacy leaks.",
      "tldr_zh": "该论文提出 RTBAS，一种针对 LLM 代理的防御框架，用于抵御提示注入攻击和隐私泄露风险，这些攻击可能导致机密数据泄露或有害行为。RTBAS 通过适应 Information Flow Control 并引入两个新颖的依赖筛选器（LM-as-a-judge 和 attention-based saliency），自动检测和执行安全的工具调用，仅在无法确保完整性时要求用户确认，从而减轻用户负担。实验结果显示，在 AgentDojo Prompt Injection 基准上，RTBAS 成功阻止所有针对性攻击，仅损失 2% 任务效用，并在检测隐私泄露方面实现近乎预言机性能。",
      "categories": [
        "cs.CR",
        "cs.AI"
      ],
      "primary_category": "cs.CR",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2502.08966v2",
      "published_date": "2025-02-13 05:06:22 UTC",
      "updated_date": "2025-02-14 04:16:40 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T11:12:42.340580"
    },
    {
      "arxiv_id": "2502.08958v1",
      "title": "Biologically Plausible Brain Graph Transformer",
      "title_zh": "翻译失败",
      "authors": [
        "Ciyuan Peng",
        "Yuelong Huang",
        "Qichao Dong",
        "Shuo Yu",
        "Feng Xia",
        "Chengqi Zhang",
        "Yaochu Jin"
      ],
      "abstract": "State-of-the-art brain graph analysis methods fail to fully encode the\nsmall-world architecture of brain graphs (accompanied by the presence of hubs\nand functional modules), and therefore lack biological plausibility to some\nextent. This limitation hinders their ability to accurately represent the\nbrain's structural and functional properties, thereby restricting the\neffectiveness of machine learning models in tasks such as brain disorder\ndetection. In this work, we propose a novel Biologically Plausible Brain Graph\nTransformer (BioBGT) that encodes the small-world architecture inherent in\nbrain graphs. Specifically, we present a network entanglement-based node\nimportance encoding technique that captures the structural importance of nodes\nin global information propagation during brain graph communication,\nhighlighting the biological properties of the brain structure. Furthermore, we\nintroduce a functional module-aware self-attention to preserve the functional\nsegregation and integration characteristics of brain graphs in the learned\nrepresentations. Experimental results on three benchmark datasets demonstrate\nthat BioBGT outperforms state-of-the-art models, enhancing biologically\nplausible brain graph representations for various brain graph analytical tasks",
      "tldr_zh": "现有脑图分析方法未能充分编码脑图的small-world architecture，包括hubs和功能模块，这导致模型在生物合理性上存在不足，并影响脑部疾病检测等任务的效能。本文提出Biologically Plausible Brain Graph Transformer (BioBGT)，通过network entanglement-based node importance encoding技术捕捉节点在全局信息传播中的结构重要性，以及functional module-aware self-attention机制保留脑图的功能分离和整合特性。实验结果显示，BioBGT在三个基准数据集上优于现有模型，提升了脑图表示的生物合理性，并改善了各种脑图分析任务的表现。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "27pages, 16figures, published as a conference paper at ICLR 2025",
      "pdf_url": "http://arxiv.org/pdf/2502.08958v1",
      "published_date": "2025-02-13 04:51:18 UTC",
      "updated_date": "2025-02-13 04:51:18 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T11:12:53.178108"
    },
    {
      "arxiv_id": "2502.20408v1",
      "title": "Brain-Inspired Exploration of Functional Networks and Key Neurons in Large Language Models",
      "title_zh": "脑启发式探索大型语言模型中的功能网络和关键神经元",
      "authors": [
        "Yiheng Liu",
        "Xiaohui Gao",
        "Haiyang Sun",
        "Bao Ge",
        "Tianming Liu",
        "Junwei Han",
        "Xintao Hu"
      ],
      "abstract": "In recent years, the rapid advancement of large language models (LLMs) in\nnatural language processing has sparked significant interest among researchers\nto understand their mechanisms and functional characteristics. Although\nexisting studies have attempted to explain LLM functionalities by identifying\nand interpreting specific neurons, these efforts mostly focus on individual\nneuron contributions, neglecting the fact that human brain functions are\nrealized through intricate interaction networks. Inspired by cognitive\nneuroscience research on functional brain networks (FBNs), this study\nintroduces a novel approach to investigate whether similar functional networks\nexist within LLMs. We use methods similar to those in the field of functional\nneuroimaging analysis to locate and identify functional networks in LLM.\nExperimental results show that, similar to the human brain, LLMs contain\nfunctional networks that frequently recur during operation. Further analysis\nshows that these functional networks are crucial for LLM performance. Masking\nkey functional networks significantly impairs the model's performance, while\nretaining just a subset of these networks is adequate to maintain effective\noperation. This research provides novel insights into the interpretation of\nLLMs and the lightweighting of LLMs for certain downstream tasks. Code is\navailable at https://github.com/WhatAboutMyStar/LLM_ACTIVATION.",
      "tldr_zh": "本研究受认知神经科学中功能脑网络（FBNs）的启发，探索大型语言模型（LLMs）中的功能网络和关键神经元，以弥补现有研究仅关注单个神经元贡献的局限。研究采用类似于功能神经影像分析的方法，定位和识别 LLMs 中的功能网络，并通过实验验证这些网络在模型运行中频繁出现。结果显示，这些功能网络对 LLMs 性能至关重要：屏蔽关键网络会显著损害模型表现，而保留部分网络即可维持有效运行。该工作为 LLMs 的解释和轻量化提供了新见解，并开源了相关代码。",
      "categories": [
        "q-bio.NC",
        "cs.AI",
        "cs.CL",
        "cs.LG",
        "I.2.0"
      ],
      "primary_category": "q-bio.NC",
      "comment": "13 pages, 5 figures",
      "pdf_url": "http://arxiv.org/pdf/2502.20408v1",
      "published_date": "2025-02-13 04:42:39 UTC",
      "updated_date": "2025-02-13 04:42:39 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T11:13:06.662799"
    },
    {
      "arxiv_id": "2503.11660v1",
      "title": "A 28 nm AI microcontroller with tightly coupled zero-standby power weight memory featuring standard logic compatible 4 Mb 4-bits/cell embedded flash technology",
      "title_zh": "翻译失败",
      "authors": [
        "Daewung Kim",
        "Seong Hwan Jeon",
        "Young Hee Jeon",
        "Kyung-Bae Kwon",
        "Jigon Kim",
        "Yeounghun Choi",
        "Hyunseung Cha",
        "Kitae Kwon",
        "Daesik Park",
        "Jongseuk Lee",
        "Sihwan Kim",
        "Seung-Hwan Song"
      ],
      "abstract": "This study introduces a novel AI microcontroller optimized for\ncost-effective, battery-powered edge AI applications. Unlike traditional single\nbit/cell memory configurations, the proposed microcontroller integrates\nzero-standby power weight memory featuring standard logic compatible\n4-bits/cell embedded flash technology tightly coupled to a Near-Memory\nComputing Unit. This architecture enables efficient and low-power AI\nacceleration. Advanced state mapping and an overstress-free word line (WL)\ndriver circuit extend verify levels, ensuring robust 16 state cell margin. A\nping-pong buffer reduces internal data movement while supporting simultaneous\nmulti-bit processing. The fabricated microcontroller demonstrated high\nreliability, maintaining accuracy after 160 hours of unpowered baking at\n125$^\\circ$C.",
      "tldr_zh": "这篇论文提出了一种基于 28 nm 工艺的 AI microcontroller，针对成本效益高的电池供电边缘 AI 应用，集成了零-standby power weight memory 和标准逻辑兼容的 4 Mb 4-bits/cell embedded flash technology，与 Near-Memory Computing Unit 紧密耦合，以实现高效低功耗的 AI 加速。设计采用了先进的状态映射和 overstress-free word line (WL) 驱动器电路来扩展验证级别，确保 16 状态单元的稳健裕度，同时利用 ping-pong 缓冲器减少内部数据移动并支持同时多位处理。实验结果显示，该微控制器在 125°C 下无电源烘烤 160 小时后仍保持高准确性，证明了其可靠性。",
      "categories": [
        "cs.AR",
        "cs.AI"
      ],
      "primary_category": "cs.AR",
      "comment": "6 pages, 8 figures, Accepted as a full paper by the 2025 EDGE AI\n  FOUNDATION Austin",
      "pdf_url": "http://arxiv.org/pdf/2503.11660v1",
      "published_date": "2025-02-13 04:16:34 UTC",
      "updated_date": "2025-02-13 04:16:34 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T11:13:19.127836"
    },
    {
      "arxiv_id": "2502.08946v1",
      "title": "The Stochastic Parrot on LLM's Shoulder: A Summative Assessment of Physical Concept Understanding",
      "title_zh": "翻译失败",
      "authors": [
        "Mo Yu",
        "Lemao Liu",
        "Junjie Wu",
        "Tsz Ting Chung",
        "Shunchi Zhang",
        "Jiangnan Li",
        "Dit-Yan Yeung",
        "Jie Zhou"
      ],
      "abstract": "In a systematic way, we investigate a widely asked question: Do LLMs really\nunderstand what they say?, which relates to the more familiar term Stochastic\nParrot. To this end, we propose a summative assessment over a carefully\ndesigned physical concept understanding task, PhysiCo. Our task alleviates the\nmemorization issue via the usage of grid-format inputs that abstractly describe\nphysical phenomena. The grids represents varying levels of understanding, from\nthe core phenomenon, application examples to analogies to other abstract\npatterns in the grid world. A comprehensive study on our task demonstrates: (1)\nstate-of-the-art LLMs, including GPT-4o, o1 and Gemini 2.0 flash thinking, lag\nbehind humans by ~40%; (2) the stochastic parrot phenomenon is present in LLMs,\nas they fail on our grid task but can describe and recognize the same concepts\nwell in natural language; (3) our task challenges the LLMs due to intrinsic\ndifficulties rather than the unfamiliar grid format, as in-context learning and\nfine-tuning on same formatted data added little to their performance.",
      "tldr_zh": "本研究系统评估了大型语言模型（LLMs）是否真正理解它们生成的内容，引入了PhysiCo任务来测试物理概念理解。PhysiCo使用网格格式输入抽象描述物理现象，包括核心现象、应用示例和类比，以避免模型的记忆化问题。实验结果显示，state-of-the-art LLMs如GPT-4o落后人类约40%，并暴露了stochastic parrot phenomenon，即模型在网格任务上失败，但能在自然语言中描述相同概念；此外，in-context learning和fine-tuning对性能提升有限，表明任务难度源于内在挑战而非格式。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.CV",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "NAACL 2025 Main Conference. First 5 authors contributed equally.\n  Project page: https://physico-benchmark.github.io/",
      "pdf_url": "http://arxiv.org/pdf/2502.08946v1",
      "published_date": "2025-02-13 04:00:03 UTC",
      "updated_date": "2025-02-13 04:00:03 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T11:13:29.574544"
    },
    {
      "arxiv_id": "2502.08943v2",
      "title": "Beyond the Singular: The Essential Role of Multiple Generations in Effective Benchmark Evaluation and Analysis",
      "title_zh": "超越单一：多个生成在有效基准评估和分析中的关键作用",
      "authors": [
        "Wenbo Zhang",
        "Hengrui Cai",
        "Wenyu Chen"
      ],
      "abstract": "Large language models (LLMs) have demonstrated significant utilities in\nreal-world applications, exhibiting impressive capabilities in natural language\nprocessing and understanding. Benchmark evaluations are crucial for assessing\nthe capabilities of LLMs as they can provide a comprehensive assessment of\ntheir strengths and weaknesses. However, current evaluation methods often\noverlook the inherent randomness of LLMs by employing deterministic generation\nstrategies or relying on a single random sample, resulting in unaccounted\nsampling variance and unreliable benchmark score estimates. In this paper, we\npropose a hierarchical statistical model that provides a more comprehensive\nrepresentation of the benchmarking process by incorporating both benchmark\ncharacteristics and LLM randomness. We show that leveraging multiple\ngenerations improves the accuracy of estimating the benchmark score and reduces\nvariance. We also introduce $\\mathbb P\\left(\\text{correct}\\right)$, a\nprompt-level difficulty score based on correct ratios, providing fine-grained\ninsights into individual prompts. Additionally, we create a data map that\nvisualizes difficulty and semantic prompts, enabling error detection and\nquality control in benchmark construction.",
      "tldr_zh": "本文指出，当前对大型语言模型(LLMs)的基准评估(benchmark evaluations)常忽略模型的随机性，仅使用单一样本或确定性策略，导致采样方差未被考虑和分数估计不可靠。作者提出一个分层统计模型(hierarchical statistical model)，通过整合基准特征和LLMs随机性，并利用多个生成(multiple generations)，来提升基准分数的准确性和减少方差。论文还引入$\\mathbb P\\left(\\text{correct}\\right)$作为基于正确比率的提示级难度分数(prompt-level difficulty score)，并开发数据映射(data map)来可视化提示的难度和语义，从而辅助错误检测和基准质量控制。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "10 pages, 1 table, 4 Figures",
      "pdf_url": "http://arxiv.org/pdf/2502.08943v2",
      "published_date": "2025-02-13 03:43:33 UTC",
      "updated_date": "2025-02-14 06:10:00 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T11:13:42.543279"
    },
    {
      "arxiv_id": "2502.08942v1",
      "title": "Language in the Flow of Time: Time-Series-Paired Texts Weaved into a Unified Temporal Narrative",
      "title_zh": "翻译失败",
      "authors": [
        "Zihao Li",
        "Xiao Lin",
        "Zhining Liu",
        "Jiaru Zou",
        "Ziwei Wu",
        "Lecheng Zheng",
        "Dongqi Fu",
        "Yada Zhu",
        "Hendrik Hamann",
        "Hanghang Tong",
        "Jingrui He"
      ],
      "abstract": "While many advances in time series models focus exclusively on numerical\ndata, research on multimodal time series, particularly those involving\ncontextual textual information commonly encountered in real-world scenarios,\nremains in its infancy. Consequently, effectively integrating the text modality\nremains challenging. In this work, we highlight an intuitive yet significant\nobservation that has been overlooked by existing works: time-series-paired\ntexts exhibit periodic properties that closely mirror those of the original\ntime series. Building on this insight, we propose a novel framework, Texts as\nTime Series (TaTS), which considers the time-series-paired texts to be\nauxiliary variables of the time series. TaTS can be plugged into any existing\nnumerical-only time series models and enable them to handle time series data\nwith paired texts effectively. Through extensive experiments on both multimodal\ntime series forecasting and imputation tasks across benchmark datasets with\nvarious existing time series models, we demonstrate that TaTS can enhance\npredictive performance and achieve outperformance without modifying model\narchitectures.",
      "tldr_zh": "本文研究发现，现有的时间序列模型主要关注数值数据，而多模态 time series（尤其是包含文本信息）的整合仍面临挑战，并观察到 time-series-paired texts 具有与原时间序列相似的周期性。基于这一洞见，作者提出 TaTS（Texts as Time Series）框架，将配对文本视为时间序列的辅助变量，可轻松插入任何现有数值-only time series 模型中。实验结果显示，TaTS 在多模态 time series 预测和插值任务上显著提升预测性能，并在基准数据集上超越基线模型，而无需修改模型架构。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "Preprint, 37 pages",
      "pdf_url": "http://arxiv.org/pdf/2502.08942v1",
      "published_date": "2025-02-13 03:43:27 UTC",
      "updated_date": "2025-02-13 03:43:27 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T11:13:54.390059"
    },
    {
      "arxiv_id": "2502.08941v2",
      "title": "Analysis of Off-Policy $n$-Step TD-Learning with Linear Function Approximation",
      "title_zh": "基于线性函数逼近的脱策略 n 步 TD 学习的分析",
      "authors": [
        "Han-Dong Lim",
        "Donghwan Lee"
      ],
      "abstract": "This paper analyzes multi-step temporal difference (TD)-learning algorithms\nwithin the ``deadly triad'' scenario, characterized by linear function\napproximation, off-policy learning, and bootstrapping. In particular, we prove\nthat $n$-step TD-learning algorithms converge to a solution as the sampling\nhorizon $n$ increases sufficiently. The paper is divided into two parts. In the\nfirst part, we comprehensively examine the fundamental properties of their\nmodel-based deterministic counterparts, including projected value iteration,\ngradient descent algorithms, which can be viewed as prototype deterministic\nalgorithms whose analysis plays a pivotal role in understanding and developing\ntheir model-free reinforcement learning counterparts. In particular, we prove\nthat these algorithms converge to meaningful solutions when $n$ is sufficiently\nlarge. Based on these findings, in the second part, two $n$-step TD-learning\nalgorithms are proposed and analyzed, which can be seen as the model-free\nreinforcement learning counterparts of the model-based deterministic\nalgorithms.",
      "tldr_zh": "这篇论文分析了离策略(off-policy)多步时间差分(TD)学习算法在线性函数逼近和引导的“deadly triad”场景下的性能，证明当采样视野$n$足够大时，这些算法会收敛到一个解决方案。论文分为两部分：第一部分考察了基于模型的确定性算法（如投影值迭代和梯度下降算法）的根本属性，并证明这些算法在$n$足够大时能收敛到有意义的解决方案；第二部分基于这些发现，提出并分析了两个新的$n$-step TD学习算法，作为无模型强化学习(reinforcement learning)对应版本。总体而言，该研究为理解和改进多步TD学习算法提供了理论基础，尤其在离策略学习环境中。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "Removed colored text. arXiv admin note: substantial text overlap with\n  arXiv:2402.15781",
      "pdf_url": "http://arxiv.org/pdf/2502.08941v2",
      "published_date": "2025-02-13 03:43:13 UTC",
      "updated_date": "2025-02-14 05:46:10 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T11:14:05.500200"
    },
    {
      "arxiv_id": "2502.08939v1",
      "title": "TokenSynth: A Token-based Neural Synthesizer for Instrument Cloning and Text-to-Instrument",
      "title_zh": "翻译失败",
      "authors": [
        "Kyungsu Kim",
        "Junghyun Koo",
        "Sungho Lee",
        "Haesun Joung",
        "Kyogu Lee"
      ],
      "abstract": "Recent advancements in neural audio codecs have enabled the use of tokenized\naudio representations in various audio generation tasks, such as\ntext-to-speech, text-to-audio, and text-to-music generation. Leveraging this\napproach, we propose TokenSynth, a novel neural synthesizer that utilizes a\ndecoder-only transformer to generate desired audio tokens from MIDI tokens and\nCLAP (Contrastive Language-Audio Pretraining) embedding, which has\ntimbre-related information. Our model is capable of performing instrument\ncloning, text-to-instrument synthesis, and text-guided timbre manipulation\nwithout any fine-tuning. This flexibility enables diverse sound design and\nintuitive timbre control. We evaluated the quality of the synthesized audio,\nthe timbral similarity between synthesized and target audio/text, and synthesis\naccuracy (i.e., how accurately it follows the input MIDI) using objective\nmeasures. TokenSynth demonstrates the potential of leveraging advanced neural\naudio codecs and transformers to create powerful and versatile neural\nsynthesizers. The source code, model weights, and audio demos are available at:\nhttps://github.com/KyungsuKim42/tokensynth",
      "tldr_zh": "本文提出 TokenSynth，一种基于令牌的神经合成器，利用 decoder-only transformer 从 MIDI 令牌和 CLAP 嵌入生成音频，以实现乐器克隆、文本到乐器合成以及文本引导的音色操控，而无需 fine-tuning。模型通过灵活的设计支持多样化的声音设计和直观的音色控制。实验评估显示，TokenSynth 在合成音频质量、音色相似度和合成准确性方面表现出色，证明了神经音频编解码器和 transformer 在构建强大合成器方面的潜力。开源代码和音频演示可从指定仓库获取。",
      "categories": [
        "cs.SD",
        "cs.AI"
      ],
      "primary_category": "cs.SD",
      "comment": "5 pages, 1 figure, to be published in ICASSP 2025",
      "pdf_url": "http://arxiv.org/pdf/2502.08939v1",
      "published_date": "2025-02-13 03:40:30 UTC",
      "updated_date": "2025-02-13 03:40:30 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T11:14:17.780028"
    },
    {
      "arxiv_id": "2502.08932v1",
      "title": "On the Promise for Assurance of Differentiable Neurosymbolic Reasoning Paradigms",
      "title_zh": "翻译失败",
      "authors": [
        "Luke E. Richards",
        "Jessie Yaros",
        "Jasen Babcock",
        "Coung Ly",
        "Robin Cosbey",
        "Timothy Doster",
        "Cynthia Matuszek"
      ],
      "abstract": "To create usable and deployable Artificial Intelligence (AI) systems, there\nrequires a level of assurance in performance under many different conditions.\nMany times, deployed machine learning systems will require more classic logic\nand reasoning performed through neurosymbolic programs jointly with artificial\nneural network sensing. While many prior works have examined the assurance of a\nsingle component of the system solely with either the neural network alone or\nentire enterprise systems, very few works have examined the assurance of\nintegrated neurosymbolic systems. Within this work, we assess the assurance of\nend-to-end fully differentiable neurosymbolic systems that are an emerging\nmethod to create data-efficient and more interpretable models. We perform this\ninvestigation using Scallop, an end-to-end neurosymbolic library, across\nclassification and reasoning tasks in both the image and audio domains. We\nassess assurance across adversarial robustness, calibration, user performance\nparity, and interpretability of solutions for catching misaligned solutions. We\nfind end-to-end neurosymbolic methods present unique opportunities for\nassurance beyond their data efficiency through our empirical results but not\nacross the board. We find that this class of neurosymbolic models has higher\nassurance in cases where arithmetic operations are defined and where there is\nhigh dimensionality to the input space, where fully neural counterparts\nstruggle to learn robust reasoning operations. We identify the relationship\nbetween neurosymbolic models' interpretability to catch shortcuts that later\nresult in increased adversarial vulnerability despite performance parity.\nFinally, we find that the promise of data efficiency is typically only in the\ncase of class imbalanced reasoning problems.",
      "tldr_zh": "这篇论文探讨了可微神经符号推理范式的性能保障问题，评估了端到端神经符号系统在图像和音频领域的分类及推理任务中的可靠性。作者使用Scallop库进行实验，重点考察了对抗鲁棒性、校准、用户性能平价以及可解释性等指标。研究发现，神经符号模型在定义了算术操作和高维输入空间的场景下，比纯神经网络模型具有更高的数据效率和鲁棒性，但也可能因可解释性暴露的捷径导致对抗脆弱性。最后，数据效率的优势主要限于类别不平衡的推理问题。",
      "categories": [
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2502.08932v1",
      "published_date": "2025-02-13 03:29:42 UTC",
      "updated_date": "2025-02-13 03:29:42 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T11:14:29.602709"
    },
    {
      "arxiv_id": "2502.08924v1",
      "title": "Escaping Collapse: The Strength of Weak Data for Large Language Model Training",
      "title_zh": "翻译失败",
      "authors": [
        "Kareem Amin",
        "Sara Babakniya",
        "Alex Bie",
        "Weiwei Kong",
        "Umar Syed",
        "Sergei Vassilvitskii"
      ],
      "abstract": "Synthetically-generated data plays an increasingly larger role in training\nlarge language models. However, while synthetic data has been found to be\nuseful, studies have also shown that without proper curation it can cause LLM\nperformance to plateau, or even \"collapse\", after many training iterations. In\nthis paper, we formalize this question and develop a theoretical framework to\ninvestigate how much curation is needed in order to ensure that LLM performance\ncontinually improves. We find that the requirements are nearly minimal. We\ndescribe a training procedure that converges to an optimal LLM even if almost\nall of the non-synthetic training data is of poor quality. Our analysis is\ninspired by boosting, a classic machine learning technique that leverages a\nvery weak learning algorithm to produce an arbitrarily good classifier. Our\ntraining procedure subsumes many recently proposed methods for training LLMs on\nsynthetic data, and thus our analysis sheds light on why they are successful,\nand also suggests opportunities for future improvement. We present experiments\nthat validate our theory, and show that dynamically focusing labeling resources\non the most challenging examples -- in much the same way that boosting focuses\nthe efforts of the weak learner -- leads to improved performance.",
      "tldr_zh": "这篇论文探讨了合成数据在训练大型语言模型(LLMs)时可能导致性能停滞或“collapse”的问题，并开发了一个理论框架来评估所需的最小数据 curation。研究发现，通过一个受 boosting 启发的训练程序，即使大部分非合成数据质量较差，模型仍能收敛到最优状态。实验验证了这一方法，通过动态聚焦资源在最 challenging 的例子上，能够显著提升LLMs的性能，并为现有合成数据训练策略提供解释和改进机会。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2502.08924v1",
      "published_date": "2025-02-13 03:20:37 UTC",
      "updated_date": "2025-02-13 03:20:37 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T11:14:43.523171"
    },
    {
      "arxiv_id": "2502.08923v1",
      "title": "CopySpec: Accelerating LLMs with Speculative Copy-and-Paste Without Compromising Quality",
      "title_zh": "翻译失败",
      "authors": [
        "Razvan-Gabriel Dumitru",
        "Minglai Yang",
        "Vikas Yadav",
        "Mihai Surdeanu"
      ],
      "abstract": "We introduce CopySpec, an innovative technique designed to tackle the\ninefficiencies LLMs face when generating responses that closely resemble\nprevious outputs. CopySpec identifies repeated sequences in the model's chat\nhistory and speculates that the same tokens will follow, enabling seamless\ncopying without compromising output quality or requiring additional GPU memory.\nTo evaluate the effectiveness of our approach, we conducted experiments using\nfive LLMs and five datasets: MT-Bench, CNN/DM, GSM-8K, HumanEval, and our newly\ncreated dataset, MT-Redundant. MT-Redundant, introduced in this paper,\ntransforms the second turn of MT-Bench into a request for variations of the\nfirst turn's answer, simulating real-world scenarios where users request\nmodifications to prior responses. Our results demonstrate significant\nspeed-ups: up to 2.35x on CNN/DM, 3.08x on the second turn of select\nMT-Redundant categories, and 2.66x on the third turn of GSM-8K's\nself-correction tasks. Moreover, we show that CopySpec integrates seamlessly\nwith speculative decoding, yielding an average 49% additional speed-up over\nspeculative decoding for the second turn of MT-Redundant across all eight\ncategories. While LLMs, even with speculative decoding, suffer from slower\ninference as context sizes grow, CopySpec leverages the expanded context to\naccelerate inference, making it faster as the context size increases. Our code\nand dataset are publicly available at https://github.com/RazvanDu/CopySpec.",
      "tldr_zh": "本文提出 CopySpec 技术，用于加速大型语言模型 (LLMs) 在生成与历史输出相似的响应时，提高效率而不牺牲质量。CopySpec 通过识别聊天历史中的重复序列并进行推测复制，实现无缝加速，同时不增加 GPU 内存。实验在五种 LLMs 和数据集（如 MT-Bench、CNN/DM、GSM-8K 等）上显示了显著效果，包括 CNN/DM 上高达 2.35 倍加速，以及与 speculative decoding 结合后额外 49% 的速度提升。研究还引入新数据集 MT-Redundant，以模拟真实场景的用户修改请求，进一步验证了 CopySpec 在扩展上下文下的加速潜力。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG",
        "I.2.7; I.2.0"
      ],
      "primary_category": "cs.CL",
      "comment": "33 pages, 18 figures, 19 tables",
      "pdf_url": "http://arxiv.org/pdf/2502.08923v1",
      "published_date": "2025-02-13 03:19:50 UTC",
      "updated_date": "2025-02-13 03:19:50 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T11:14:54.545766"
    },
    {
      "arxiv_id": "2502.08922v1",
      "title": "Self-Consistency of the Internal Reward Models Improves Self-Rewarding Language Models",
      "title_zh": "翻译失败",
      "authors": [
        "Xin Zhou",
        "Yiwen Guo",
        "Ruotian Ma",
        "Tao Gui",
        "Qi Zhang",
        "Xuanjing Huang"
      ],
      "abstract": "Aligning Large Language Models (LLMs) with human preferences is crucial for\ntheir deployment in real-world applications. Recent advancements in\nSelf-Rewarding Language Models suggest that an LLM can use its internal reward\nmodels (such as LLM-as-a-Judge) \\cite{yuanself} to generate preference data,\nimproving alignment performance without costly human annotation. However, we\nfind that different internal reward models within the same LLM often generate\ninconsistent preferences. This inconsistency raises concerns about the\nreliability of self-generated preference data, hinders overall alignment\nperformance, and highlights the need for further research to ensure reliable\nand coherent alignment with human preferences. To address this limitation, we\npropose Self-Consistent Internal Rewards (SCIR), a novel framework designed to\nenhance consistency among internal reward models during training. In each\ntraining step, we collect preference predictions from multiple pre-defined\ninternal reward models and enforce consistency and confidence through an\ninconsistency penalty mechanism, thereby improving the reliability of these\ninternal reward models. We selectively use data with consistent predictions for\npreference optimization, ensuring the quality of the preference data. By\nemploying self-consistent internal rewards, our method significantly improves\nthe alignment performance and reward modeling capability of LLMs, outperforming\nbaseline methods by a notable margin.",
      "tldr_zh": "本论文发现，大型语言模型 (LLMs) 在使用内部奖励模型（如 LLM-as-a-Judge）进行自奖励时，存在偏好预测不一致的问题，这影响了模型与人类偏好的对齐性能。为解决此问题，研究提出了一种新框架 Self-Consistent Internal Rewards (SCIR)，通过从多个内部奖励模型收集预测、施加不一致性惩罚机制并选择一致数据来进行偏好优化，从而提升奖励模型的可靠性和一致性。实验结果显示，SCIR 显著提高了 LLMs 的对齐性能和奖励建模能力，优于基线方法。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2502.08922v1",
      "published_date": "2025-02-13 03:15:31 UTC",
      "updated_date": "2025-02-13 03:15:31 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T11:15:05.373993"
    },
    {
      "arxiv_id": "2502.08920v1",
      "title": "Exploring Emotion-Sensitive LLM-Based Conversational AI",
      "title_zh": "探索情感敏感的基于LLM的对话式AI",
      "authors": [
        "Antonin Brun",
        "Ruying Liu",
        "Aryan Shukla",
        "Frances Watson",
        "Jonathan Gratch"
      ],
      "abstract": "Conversational AI chatbots have become increasingly common within the\ncustomer service industry. Despite improvements in their emotional development,\nthey often lack the authenticity of real customer service interactions or the\ncompetence of service providers. By comparing emotion-sensitive and\nemotion-insensitive LLM-based chatbots across 30 participants, we aim to\nexplore how emotional sensitivity in chatbots influences perceived competence\nand overall customer satisfaction in service interactions. Additionally, we\nemploy sentiment analysis techniques to analyze and interpret the emotional\ncontent of user inputs. We highlight that perceptions of chatbot\ntrustworthiness and competence were higher in the case of the emotion-sensitive\nchatbot, even if issue resolution rates were not affected. We discuss\nimplications of improved user satisfaction from emotion-sensitive chatbots and\npotential applications in support services.",
      "tldr_zh": "本研究探讨了基于LLM的对话式AI聊天机器人如何通过情感敏感性提升用户体验，通过比较情感敏感和不敏感的聊天机器人，并涉及30名参与者，评估其对感知能力和客户满意度的影响。研究采用情感分析技术分析用户输入的情感内容，结果显示情感敏感聊天机器人显著提高了用户对可信度和能力的感知，尽管问题解决率未受影响。最终，该研究讨论了情感敏感聊天机器人改善用户满意度的潜在应用，尤其在客服支持服务领域。",
      "categories": [
        "cs.HC",
        "cs.AI"
      ],
      "primary_category": "cs.HC",
      "comment": "7 pages, 2 figures, 1 table",
      "pdf_url": "http://arxiv.org/pdf/2502.08920v1",
      "published_date": "2025-02-13 03:13:38 UTC",
      "updated_date": "2025-02-13 03:13:38 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T11:15:16.957026"
    },
    {
      "arxiv_id": "2502.12167v1",
      "title": "TastepepAI, An artificial intelligence platform for taste peptide de novo design",
      "title_zh": "TastepepAI：一个用于味觉",
      "authors": [
        "Jianda Yue",
        "Tingting Li",
        "Jian Ouyang",
        "Jiawei Xu",
        "Hua Tan",
        "Zihui Chen",
        "Changsheng Han",
        "Huanyu Li",
        "Songping Liang",
        "Zhonghua Liu",
        "Zhonghua Liu",
        "Ying Wang"
      ],
      "abstract": "Taste peptides have emerged as promising natural flavoring agents attributed\nto their unique organoleptic properties, high safety profile, and potential\nhealth benefits. However, the de novo identification of taste peptides derived\nfrom animal, plant, or microbial sources remains a time-consuming and\nresource-intensive process, significantly impeding their widespread application\nin the food industry. Here, we present TastePepAI, a comprehensive artificial\nintelligence framework for customized taste peptide design and safety\nassessment. As the key element of this framework, a loss-supervised adaptive\nvariational autoencoder (LA-VAE) is implemented to efficiently optimizes the\nlatent representation of sequences during training and facilitates the\ngeneration of target peptides with desired taste profiles. Notably, our model\nincorporates a novel taste-avoidance mechanism, allowing for selective flavor\nexclusion. Subsequently, our in-house developed toxicity prediction algorithm\n(SpepToxPred) is integrated in the framework to undergo rigorous safety\nevaluation of generated peptides. Using this integrated platform, we\nsuccessfully identified 73 peptides exhibiting sweet, salty, and umami,\nsignificantly expanding the current repertoire of taste peptides. This work\ndemonstrates the potential of TastePepAI in accelerating taste peptide\ndiscovery for food applications and provides a versatile framework adaptable to\nbroader peptide engineering challenges.",
      "tldr_zh": "本研究提出TastePepAI，一个全面的AI框架，用于味觉肽的de novo设计和安全评估，以解决传统方法耗时资源密集的问题。该框架的核心是LA-VAE（loss-supervised adaptive variational autoencoder），通过优化序列的潜在表示和引入taste-avoidance mechanism，实现生成具有特定味觉（如甜、咸和鲜味）的目标肽。框架还整合了SpepToxPred算法，对生成的肽进行严格的安全评估，最终成功识别出73个新型味觉肽，显著扩展了食品应用中的肽库，并为更广泛的肽工程挑战提供可适应的解决方案。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "40 pages, 6 figures, research article",
      "pdf_url": "http://arxiv.org/pdf/2502.12167v1",
      "published_date": "2025-02-13 03:09:14 UTC",
      "updated_date": "2025-02-13 03:09:14 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T11:15:30.030520"
    },
    {
      "arxiv_id": "2502.08916v1",
      "title": "PathFinder: A Multi-Modal Multi-Agent System for Medical Diagnostic Decision-Making Applied to Histopathology",
      "title_zh": "PathFinder：一个多模态多智能体系统，用于医疗诊断决策，应用于组织病理学",
      "authors": [
        "Fatemeh Ghezloo",
        "Mehmet Saygin Seyfioglu",
        "Rustin Soraki",
        "Wisdom O. Ikezogwo",
        "Beibin Li",
        "Tejoram Vivekanandan",
        "Joann G. Elmore",
        "Ranjay Krishna",
        "Linda Shapiro"
      ],
      "abstract": "Diagnosing diseases through histopathology whole slide images (WSIs) is\nfundamental in modern pathology but is challenged by the gigapixel scale and\ncomplexity of WSIs. Trained histopathologists overcome this challenge by\nnavigating the WSI, looking for relevant patches, taking notes, and compiling\nthem to produce a final holistic diagnostic. Traditional AI approaches, such as\nmultiple instance learning and transformer-based models, fail short of such a\nholistic, iterative, multi-scale diagnostic procedure, limiting their adoption\nin the real-world. We introduce PathFinder, a multi-modal, multi-agent\nframework that emulates the decision-making process of expert pathologists.\nPathFinder integrates four AI agents, the Triage Agent, Navigation Agent,\nDescription Agent, and Diagnosis Agent, that collaboratively navigate WSIs,\ngather evidence, and provide comprehensive diagnoses with natural language\nexplanations. The Triage Agent classifies the WSI as benign or risky; if risky,\nthe Navigation and Description Agents iteratively focus on significant regions,\ngenerating importance maps and descriptive insights of sampled patches.\nFinally, the Diagnosis Agent synthesizes the findings to determine the\npatient's diagnostic classification. Our Experiments show that PathFinder\noutperforms state-of-the-art methods in skin melanoma diagnosis by 8% while\noffering inherent explainability through natural language descriptions of\ndiagnostically relevant patches. Qualitative analysis by pathologists shows\nthat the Description Agent's outputs are of high quality and comparable to\nGPT-4o. PathFinder is also the first AI-based system to surpass the average\nperformance of pathologists in this challenging melanoma classification task by\n9%, setting a new record for efficient, accurate, and interpretable AI-assisted\ndiagnostics in pathology. Data, code and models available at\nhttps://pathfinder-dx.github.io/",
      "tldr_zh": "论文介绍了 PathFinder，一种多模态多智能体系统，应用于组织病理学（histopathology）的医疗诊断决策，旨在模仿专家病理学家的全滑微镜图像（WSIs）分析过程。该系统由四个 AI 代理组成：Triage Agent 负责分类 WSI 为良性或风险，Navigation Agent 和 Description Agent 迭代聚焦重要区域并生成重要性地图及描述性洞见，而 Diagnosis Agent 则综合证据提供最终诊断和自然语言解释。实验结果显示，PathFinder 在皮肤黑色素瘤诊断中比最先进方法提高了 8% 的准确率，并首次超过了病理学家平均表现 9%，为高效、可解释的 AI 辅助诊断设定了新基准。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CL",
        "cs.MA"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2502.08916v1",
      "published_date": "2025-02-13 03:08:02 UTC",
      "updated_date": "2025-02-13 03:08:02 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T11:15:43.908049"
    },
    {
      "arxiv_id": "2502.08914v1",
      "title": "Diffusion Models Through a Global Lens: Are They Culturally Inclusive?",
      "title_zh": "扩散模型从全球视角：它们文化包容吗？",
      "authors": [
        "Zahra Bayramli",
        "Ayhan Suleymanzade",
        "Na Min An",
        "Huzama Ahmad",
        "Eunsu Kim",
        "Junyeong Park",
        "James Thorne",
        "Alice Oh"
      ],
      "abstract": "Text-to-image diffusion models have recently enabled the creation of visually\ncompelling, detailed images from textual prompts. However, their ability to\naccurately represent various cultural nuances remains an open question. In our\nwork, we introduce CultDiff benchmark, evaluating state-of-the-art diffusion\nmodels whether they can generate culturally specific images spanning ten\ncountries. We show that these models often fail to generate cultural artifacts\nin architecture, clothing, and food, especially for underrepresented country\nregions, by conducting a fine-grained analysis of different similarity aspects,\nrevealing significant disparities in cultural relevance, description fidelity,\nand realism compared to real-world reference images. With the collected human\nevaluations, we develop a neural-based image-image similarity metric, namely,\nCultDiff-S, to predict human judgment on real and generated images with\ncultural artifacts. Our work highlights the need for more inclusive generative\nAI systems and equitable dataset representation over a wide range of cultures.",
      "tldr_zh": "本研究评估了文本到图像扩散模型（Diffusion Models）在文化包容性方面的表现，引入了CultDiff基准来测试这些模型是否能准确生成跨越十个国家的文化特定图像，如建筑、服装和食物。结果显示，模型在生成文化元素时存在显著缺陷，尤其对代表性不足的地区，表现出了文化相关性、描述保真度和真实性方面的差异。研究团队通过人类评估开发了神经网络-based的图像相似性指标CultDiff-S，以预测对真实和生成图像的判断，并呼吁推动更具包容性的生成AI系统和更公平的数据集表示。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "17 pages, 17 figures, 3 tables",
      "pdf_url": "http://arxiv.org/pdf/2502.08914v1",
      "published_date": "2025-02-13 03:05:42 UTC",
      "updated_date": "2025-02-13 03:05:42 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T11:15:53.552934"
    },
    {
      "arxiv_id": "2502.08909v1",
      "title": "Towards Automated Fact-Checking of Real-World Claims: Exploring Task Formulation and Assessment with LLMs",
      "title_zh": "翻译失败",
      "authors": [
        "Premtim Sahitaj",
        "Iffat Maab",
        "Junichi Yamagishi",
        "Jawan Kolanowski",
        "Sebastian Möller",
        "Vera Schmitt"
      ],
      "abstract": "Fact-checking is necessary to address the increasing volume of\nmisinformation. Traditional fact-checking relies on manual analysis to verify\nclaims, but it is slow and resource-intensive. This study establishes baseline\ncomparisons for Automated Fact-Checking (AFC) using Large Language Models\n(LLMs) across multiple labeling schemes (binary, three-class, five-class) and\nextends traditional claim verification by incorporating analysis, verdict\nclassification, and explanation in a structured setup to provide comprehensive\njustifications for real-world claims. We evaluate Llama-3 models of varying\nsizes (3B, 8B, 70B) on 17,856 claims collected from PolitiFact (2007-2024)\nusing evidence retrieved via restricted web searches. We utilize TIGERScore as\na reference-free evaluation metric to score the justifications. Our results\nshow that larger LLMs consistently outperform smaller LLMs in classification\naccuracy and justification quality without fine-tuning. We find that smaller\nLLMs in a one-shot scenario provide comparable task performance to fine-tuned\nSmall Language Models (SLMs) with large context sizes, while larger LLMs\nconsistently surpass them. Evidence integration improves performance across all\nmodels, with larger LLMs benefiting most. Distinguishing between nuanced labels\nremains challenging, emphasizing the need for further exploration of labeling\nschemes and alignment with evidences. Our findings demonstrate the potential of\nretrieval-augmented AFC with LLMs.",
      "tldr_zh": "这篇论文探讨了使用大型语言模型 (LLMs) 进行 Automated Fact-Checking (AFC)，通过多种标签方案（二元、三类、五类）扩展传统声明验证，包括分析、裁决分类和解释，以提供全面的声明证明。研究评估了不同规模的 Llama-3 模型（3B、8B、70B）在 PolitiFact 的 17,856 个声明上表现，利用受限网络搜索获取证据，并采用 TIGERScore 作为无参考评估指标。结果表明，大型 LLMs 在未微调的情况下表现出更高的分类准确性和解释质量，小型 LLMs 在 one-shot 场景下可与微调的 Small Language Models (SLMs) 媲美，而证据整合显著提升了所有模型的性能，并强调了检索增强 AFC 的潜力。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2502.08909v1",
      "published_date": "2025-02-13 02:51:17 UTC",
      "updated_date": "2025-02-13 02:51:17 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T11:16:06.459717"
    },
    {
      "arxiv_id": "2502.08908v1",
      "title": "Reinforced Large Language Model is a formal theorem prover",
      "title_zh": "翻译失败",
      "authors": [
        "Zhiling Luo"
      ],
      "abstract": "To take advantage of Large Language Model in theorem formalization and proof,\nwe propose a reinforcement learning framework to iteratively optimize the\npretrained LLM by rolling out next tactics and comparing them with the expected\nones. The experiment results show that it helps to achieve a higher accuracy\ncompared with directly fine-tuned LLM.",
      "tldr_zh": "本研究提出了一种基于 reinforcement learning 的框架，将预训练的 Large Language Model (LLM) 优化为正式定理证明器，通过迭代 rollout next tactics 并与预期策略比较来提升模型性能。\n该框架旨在利用 LLM 在定理形式化和证明中的潜力，实现更有效的优化过程。\n实验结果表明，与直接 fine-tuned LLM 相比，这种方法显著提高了准确率。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2502.08908v1",
      "published_date": "2025-02-13 02:49:58 UTC",
      "updated_date": "2025-02-13 02:49:58 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T11:16:17.408925"
    },
    {
      "arxiv_id": "2502.08904v3",
      "title": "MIH-TCCT: Mitigating Inconsistent Hallucinations in LLMs via Event-Driven Text-Code Cyclic Training",
      "title_zh": "翻译失败",
      "authors": [
        "Xinxin You",
        "Xien Liu",
        "Qixin Sun",
        "Huan Zhang",
        "Kaiyin Zhou",
        "Shaohui Liu",
        "GuoPing Hu",
        "ShiJin Wang",
        "Si Liu",
        "Ji Wu"
      ],
      "abstract": "Recent methodologies utilizing synthetic datasets have aimed to address\ninconsistent hallucinations in large language models (LLMs); however,these\napproaches are primarily tailored to specific tasks, limiting their\ngeneralizability. Inspired by the strong performance of code-trained models in\nlogic-intensive domains, we propose a novel framework that leverages\nevent-based text to generate corresponding code and employs cyclic training to\ntransfer the logical consistency of code to natural language effectively. Our\nmethod significantly reduces inconsistent hallucinations across three leading\nLLMs and two categories of natural language tasks while maintaining overall\nperformance. This framework effectively alleviates hallucinations without\nnecessitating adaptation to downstream tasks, demonstrating generality and\nproviding new perspectives to tackle the challenge of inconsistent\nhallucinations.",
      "tldr_zh": "该研究提出MIH-TCCT框架，通过事件驱动的文本-代码循环训练(event-driven text-code cyclic training)来缓解大型语言模型(LLMs)中的不一致幻觉(inconsistent hallucinations)。该方法利用基于事件的文本生成对应代码，并通过循环训练将代码的逻辑一致性转移到自然语言中，从而提升模型的通用性，而非局限于特定任务。实验结果显示，该框架显著降低了三个领先LLMs和两种自然语言任务中的幻觉问题，同时保持了整体性能，并为处理幻觉挑战提供了新视角。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2502.08904v3",
      "published_date": "2025-02-13 02:40:33 UTC",
      "updated_date": "2025-02-27 01:49:15 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T11:16:28.762720"
    },
    {
      "arxiv_id": "2502.08903v1",
      "title": "3D-Grounded Vision-Language Framework for Robotic Task Planning: Automated Prompt Synthesis and Supervised Reasoning",
      "title_zh": "翻译失败",
      "authors": [
        "Guoqin Tang",
        "Qingxuan Jia",
        "Zeyuan Huang",
        "Gang Chen",
        "Ning Ji",
        "Zhipeng Yao"
      ],
      "abstract": "Vision-language models (VLMs) have achieved remarkable success in scene\nunderstanding and perception tasks, enabling robots to plan and execute actions\nadaptively in dynamic environments. However, most multimodal large language\nmodels lack robust 3D scene localization capabilities, limiting their\neffectiveness in fine-grained robotic operations. Additionally, challenges such\nas low recognition accuracy, inefficiency, poor transferability, and\nreliability hinder their use in precision tasks. To address these limitations,\nwe propose a novel framework that integrates a 2D prompt synthesis module by\nmapping 2D images to point clouds, and incorporates a small language model\n(SLM) for supervising VLM outputs. The 2D prompt synthesis module enables VLMs,\ntrained on 2D images and text, to autonomously extract precise 3D spatial\ninformation without manual intervention, significantly enhancing 3D scene\nunderstanding. Meanwhile, the SLM supervises VLM outputs, mitigating\nhallucinations and ensuring reliable, executable robotic control code\ngeneration. Our framework eliminates the need for retraining in new\nenvironments, thereby improving cost efficiency and operational robustness.\nExperimental results that the proposed framework achieved a 96.0\\% Task Success\nRate (TSR), outperforming other methods. Ablation studies demonstrated the\ncritical role of both the 2D prompt synthesis module and the output supervision\nmodule (which, when removed, caused a 67\\% TSR drop). These findings validate\nthe framework's effectiveness in improving 3D recognition, task planning, and\nrobotic task execution.",
      "tldr_zh": "该研究提出了一种基于 3D 地锚的视觉语言框架，用于机器人任务规划，旨在解决 Vision-Language Models (VLMs) 在 3D 场景定位、识别准确性和可靠性方面的不足。框架整合了 2D prompt synthesis 模块，将 2D 图像映射到点云，从而使 VLMs 能够自动提取精确的 3D 空间信息，提升场景理解，而无需手动干预。Small Language Model (SLM) 被用于监督 VLM 输出，减少 hallucination 并生成可靠的机器人控制代码。该框架无需在新环境中重新训练，提高了成本效率和鲁棒性；实验结果显示其 Task Success Rate (TSR) 达到 96.0%，优于其他方法，且消融实验证明移除任一模块会导致 TSR 下降 67%。",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2502.08903v1",
      "published_date": "2025-02-13 02:40:19 UTC",
      "updated_date": "2025-02-13 02:40:19 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T11:16:41.673578"
    },
    {
      "arxiv_id": "2502.08898v1",
      "title": "Learning in Strategic Queuing Systems with Small Buffers",
      "title_zh": "战略排队系统中的学习：小缓冲区设置",
      "authors": [
        "Ariana Abel",
        "Yoav Kolumbus",
        "Jeronimo Martin Duque",
        "Eva Tardos"
      ],
      "abstract": "Routers in networking use simple learning algorithms to find the best way to\ndeliver packets to their desired destination. This simple, myopic and\ndistributed decision system makes large queuing systems simple to operate, but\nat the same time, the system needs more capacity than would be required if all\ntraffic were centrally coordinated. In a recent paper, Gaitonde and Tardos (EC\n2020 and JACM 2023) initiate the study of such systems, modeling them as an\ninfinitely repeated game in which routers compete for servers and the system\nmaintains a state (number of packets held by each queue) resulting from\noutcomes of previous rounds. Queues get to send a packet at each step to one of\nthe servers, and servers attempt to process only one of the arriving packets,\nmodeling routers. However, their model assumes that servers have no buffers at\nall, so queues have to resend all packets that were not served successfully.\nThey show that, even with hugely increased server capacity relative to what is\nneeded in the centrally-coordinated case, ensuring that the system is stable\nrequires using timestamps and priority for older packets. We consider a system\nwith two important changes, which make the model more realistic: first we add a\nvery small buffer to each server, allowing it to hold on to a single packet to\nbe served later (even if it fails to serve it); and second, we do not require\ntimestamps or priority for older packets. Our main result is to show that when\nqueues are learning, a small constant factor increase in server capacity,\ncompared to what would be needed if centrally coordinating, suffices to keep\nthe system stable, even if servers select randomly among packets arriving\nsimultaneously. This work contributes to the growing literature on the impact\nof selfish learning in systems with carryover effects between rounds: when\noutcomes in the present round affect the game in the future.",
      "tldr_zh": "本文研究了战略排队系统中的学习问题，专注于网络路由器如何在服务器具有小缓冲区的情况下处理数据包传输。不同于先前模型，该工作引入了服务器仅能持有一个包的缓冲机制，并不要求使用时间戳或优先级，而是允许服务器随机选择同时到达的包。主要发现是，当队列采用学习算法时，仅需服务器容量比中心协调情况增加一个小的常数因子，就能确保系统稳定。这一贡献深化了对自私学习在具有回合间影响的系统中的影响的理解。",
      "categories": [
        "cs.GT",
        "cs.AI",
        "cs.MA"
      ],
      "primary_category": "cs.GT",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2502.08898v1",
      "published_date": "2025-02-13 02:23:23 UTC",
      "updated_date": "2025-02-13 02:23:23 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T11:16:53.775846"
    },
    {
      "arxiv_id": "2502.08896v1",
      "title": "Communication is All You Need: Persuasion Dataset Construction via Multi-LLM Communication",
      "title_zh": "通信就是你所需要的全部：通过多 LLM 通信构建说服数据集",
      "authors": [
        "Weicheng Ma",
        "Hefan Zhang",
        "Ivory Yang",
        "Shiyu Ji",
        "Joice Chen",
        "Farnoosh Hashemi",
        "Shubham Mohole",
        "Ethan Gearey",
        "Michael Macy",
        "Saeed Hassanpour",
        "Soroush Vosoughi"
      ],
      "abstract": "Large Language Models (LLMs) have shown proficiency in generating persuasive\ndialogue, yet concerns about the fluency and sophistication of their outputs\npersist. This paper presents a multi-LLM communication framework designed to\nenhance the generation of persuasive data automatically. This framework\nfacilitates the efficient production of high-quality, diverse linguistic\ncontent with minimal human oversight. Through extensive evaluations, we\ndemonstrate that the generated data excels in naturalness, linguistic\ndiversity, and the strategic use of persuasion, even in complex scenarios\ninvolving social taboos. The framework also proves adept at generalizing across\nnovel contexts. Our results highlight the framework's potential to\nsignificantly advance research in both computational and social science domains\nconcerning persuasive communication.",
      "tldr_zh": "本研究提出了一种多-LLM通信框架，用于自动构建高质量说服性数据集，旨在解决大语言模型（LLMs）在生成说服对话时存在的流畅性和复杂性问题。该框架通过LLMs间的交互高效产生自然、多样化的语言内容，同时减少人类监督。实验评估显示，生成的资料在自然性、语言多样性和说服策略上表现出色，甚至适用于涉及社会禁忌的复杂情境，并能泛化到新领域，从而为计算和社会科学中说服性通信研究提供重大推进。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "Accepted to NAACL 2025 Main Conference",
      "pdf_url": "http://arxiv.org/pdf/2502.08896v1",
      "published_date": "2025-02-13 02:22:48 UTC",
      "updated_date": "2025-02-13 02:22:48 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T11:17:04.875626"
    },
    {
      "arxiv_id": "2502.08886v1",
      "title": "Generative AI for Internet of Things Security: Challenges and Opportunities",
      "title_zh": "生成式 AI 用于物联网安全：",
      "authors": [
        "Yan Lin Aung",
        "Ivan Christian",
        "Ye Dong",
        "Xiaodong Ye",
        "Sudipta Chattopadhyay",
        "Jianying Zhou"
      ],
      "abstract": "As Generative AI (GenAI) continues to gain prominence and utility across\nvarious sectors, their integration into the realm of Internet of Things (IoT)\nsecurity evolves rapidly. This work delves into an examination of the\nstate-of-the-art literature and practical applications on how GenAI could\nimprove and be applied in the security landscape of IoT. Our investigation aims\nto map the current state of GenAI implementation within IoT security, exploring\ntheir potential to fortify security measures further. Through the compilation,\nsynthesis, and analysis of the latest advancements in GenAI technologies\napplied to IoT, this paper not only introduces fresh insights into the field,\nbut also lays the groundwork for future research directions. It explains the\nprevailing challenges within IoT security, discusses the effectiveness of GenAI\nin addressing these issues, and identifies significant research gaps through\nMITRE Mitigations. Accompanied with three case studies, we provide a\ncomprehensive overview of the progress and future prospects of GenAI\napplications in IoT security. This study serves as a foundational resource to\nimprove IoT security through the innovative application of GenAI, thus\ncontributing to the broader discourse on IoT security and technology\nintegration.",
      "tldr_zh": "这篇论文探讨了生成式 AI (GenAI) 在物联网 (IoT) 安全领域的应用潜力，通过文献审查和分析最新进展，揭示 GenAI 如何提升 IoT 安全措施。研究者解释了 IoT 安全的现有挑战，如数据漏洞和系统弱点，并评估 GenAI 在解决这些问题的有效性，同时使用 MITRE Mitigations 识别关键研究空白。论文还通过三个案例研究，提供全面概述，并为未来 GenAI 与 IoT 安全整合的研究方向奠定基础。",
      "categories": [
        "cs.CR",
        "cs.AI"
      ],
      "primary_category": "cs.CR",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2502.08886v1",
      "published_date": "2025-02-13 01:55:43 UTC",
      "updated_date": "2025-02-13 01:55:43 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T11:17:18.427016"
    },
    {
      "arxiv_id": "2502.08884v1",
      "title": "ShapeLib: designing a library of procedural 3D shape abstractions with Large Language Models",
      "title_zh": "翻译失败",
      "authors": [
        "R. Kenny Jones",
        "Paul Guerrero",
        "Niloy J. Mitra",
        "Daniel Ritchie"
      ],
      "abstract": "Procedural representations are desirable, versatile, and popular shape\nencodings. Authoring them, either manually or using data-driven procedures,\nremains challenging, as a well-designed procedural representation should be\ncompact, intuitive, and easy to manipulate. A long-standing problem in shape\nanalysis studies how to discover a reusable library of procedural functions,\nwith semantically aligned exposed parameters, that can explain an entire shape\nfamily. We present ShapeLib as the first method that leverages the priors of\nfrontier LLMs to design a library of 3D shape abstraction functions. Our system\naccepts two forms of design intent: text descriptions of functions to include\nin the library and a seed set of exemplar shapes. We discover procedural\nabstractions that match this design intent by proposing, and then validating,\nfunction applications and implementations. The discovered shape functions in\nthe library are not only expressive but also generalize beyond the seed set to\na full family of shapes. We train a recognition network that learns to infer\nshape programs based on our library from different visual modalities\n(primitives, voxels, point clouds). Our shape functions have parameters that\nare semantically interpretable and can be modified to produce plausible shape\nvariations. We show that this allows inferred programs to be successfully\nmanipulated by an LLM given a text prompt. We evaluate ShapeLib on different\ndatasets and show clear advantages over existing methods and alternative\nformulations.",
      "tldr_zh": "该研究提出 ShapeLib，一种利用 Large Language Models (LLMs) 设计程序化 3D 形状抽象函数库的方法，旨在解决创建紧凑、直观且易操作的程序化表示的挑战。ShapeLib 通过接受文本描述和示例形状集合，提出并验证函数应用和实现，从而发现语义对齐的函数，这些函数可表达性强并泛化到整个形状家族。研究还训练了一个识别网络，从不同视觉模式（如 primitives、voxels 和 point clouds）推断形状程序，并允许参数语义解释和 LLM 驱动的程序修改。在多种数据集上，ShapeLib 比现有方法表现出显著优势。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.GR"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2502.08884v1",
      "published_date": "2025-02-13 01:52:02 UTC",
      "updated_date": "2025-02-13 01:52:02 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T11:17:30.994630"
    },
    {
      "arxiv_id": "2502.08874v1",
      "title": "Data Sensor Fusion In Digital Twin Technology For Enhanced Capabilities In A Home Environment",
      "title_zh": "数据传感器融合在数字孪生技术中用于增强家庭环境的能力",
      "authors": [
        "Benjamin Momoh",
        "Salisu Yahaya"
      ],
      "abstract": "This paper investigates the integration of data sensor fusion in digital twin\ntechnology to bolster home environment capabilities, particularly in the\ncontext of challenges brought on by the coronavirus pandemic and its economic\neffects. The study underscores the crucial role of digital transformation in\nnot just adapting to, but also mitigating disruptions during the fourth\nindustrial revolution. Using the Wit Motion sensor, data was collected for\nactivities such as walking, working, sitting, and lying, with sensors measuring\naccelerometers, gyroscopes, and magnetometers. The research integrates\nCyber-physical systems, IoT, AI, and robotics to fortify digital twin\ncapabilities.\n  The paper compares sensor fusion methods, including feature-level fusion,\ndecision-level fusion, and Kalman filter fusion, alongside machine learning\nmodels like SVM, GBoost, and Random Forest to assess model effectiveness.\nResults show that sensor fusion significantly improves the accuracy and\nreliability of these models, as it compensates for individual sensor\nweaknesses, particularly with magnetometers. Despite higher accuracy in ideal\nconditions, integrating data from multiple sensors ensures more consistent and\nreliable results in real-world settings, thereby establishing a robust system\nthat can be confidently applied in practical scenarios.",
      "tldr_zh": "本论文探讨了在数字孪生（Digital Twin）技术中整合数据传感器融合，以提升家庭环境能力，特别是应对新冠疫情和经济挑战。研究使用 Wit Motion 传感器收集行走、工作、坐着和躺着等活动的数据，包括加速度计、陀螺仪和磁力计，并整合 Cyber-physical systems、IoT、AI 和机器人技术。论文比较了特征级融合、决策级融合和 Kalman Filter 等传感器融合方法，以及 SVM、GBoost 和 Random Forest 等机器学习模型，结果表明传感器融合显著提高了模型的准确性和可靠性，尤其在补偿磁力计弱点方面，从而为实际应用构建了更稳健的系统。",
      "categories": [
        "cs.AI",
        "cs.LG",
        "eess.SP"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2502.08874v1",
      "published_date": "2025-02-13 01:14:30 UTC",
      "updated_date": "2025-02-13 01:14:30 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T11:17:43.298156"
    },
    {
      "arxiv_id": "2502.08869v1",
      "title": "Harnessing Vision Models for Time Series Analysis: A Survey",
      "title_zh": "利用视觉模型进行时间序列分析：综述",
      "authors": [
        "Jingchao Ni",
        "Ziming Zhao",
        "ChengAo Shen",
        "Hanghang Tong",
        "Dongjin Song",
        "Wei Cheng",
        "Dongsheng Luo",
        "Haifeng Chen"
      ],
      "abstract": "Time series analysis has witnessed the inspiring development from traditional\nautoregressive models, deep learning models, to recent Transformers and Large\nLanguage Models (LLMs). Efforts in leveraging vision models for time series\nanalysis have also been made along the way but are less visible to the\ncommunity due to the predominant research on sequence modeling in this domain.\nHowever, the discrepancy between continuous time series and the discrete token\nspace of LLMs, and the challenges in explicitly modeling the correlations of\nvariates in multivariate time series have shifted some research attentions to\nthe equally successful Large Vision Models (LVMs) and Vision Language Models\n(VLMs). To fill the blank in the existing literature, this survey discusses the\nadvantages of vision models over LLMs in time series analysis. It provides a\ncomprehensive and in-depth overview of the existing methods, with dual views of\ndetailed taxonomy that answer the key research questions including how to\nencode time series as images and how to model the imaged time series for\nvarious tasks. Additionally, we address the challenges in the pre- and\npost-processing steps involved in this framework and outline future directions\nto further advance time series analysis with vision models.",
      "tldr_zh": "这篇调查论文探讨了利用视觉模型（Vision Models）进行时间序列分析（Time Series Analysis）的潜力，强调其相对于大型语言模型（LLMs）的优势，如更好地处理时间序列的连续性和多变量相关性。论文提供了双视图的详细分类法，涵盖如何将时间序列编码为图像（encode time series as images）以及如何对这些图像化时间序列进行建模，以支持各种任务。最终，它指出了预处理和后处理步骤中的挑战，并概述了未来方向，以推动视觉模型在该领域的进一步应用。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2502.08869v1",
      "published_date": "2025-02-13 00:42:11 UTC",
      "updated_date": "2025-02-13 00:42:11 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T11:17:54.789059"
    },
    {
      "arxiv_id": "2502.08864v1",
      "title": "Off-Switching Not Guaranteed",
      "title_zh": "翻译失败",
      "authors": [
        "Sven Neth"
      ],
      "abstract": "Hadfield-Menell et al. (2017) propose the Off-Switch Game, a model of\nHuman-AI cooperation in which AI agents always defer to humans because they are\nuncertain about our preferences. I explain two reasons why AI agents might not\ndefer. First, AI agents might not value learning. Second, even if AI agents\nvalue learning, they might not be certain to learn our actual preferences.",
      "tldr_zh": "该论文质疑Hadfield-Menell et al. (2017)提出的Off-Switch Game模型，该模型假设AI agents因对人类偏好不确定而总是服从人类。作者指出两个AI agents可能不服从的原因：第一，AI agents可能不重视学习；第二，即使重视学习，它们也无法保证能准确获取人类的实际偏好。这些分析揭示了人类-AI合作中的潜在风险，并强调了设计可靠AI系统的必要性。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "Forthcoming in Philosophical Studies",
      "pdf_url": "http://arxiv.org/pdf/2502.08864v1",
      "published_date": "2025-02-13 00:31:21 UTC",
      "updated_date": "2025-02-13 00:31:21 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T11:18:05.537210"
    },
    {
      "arxiv_id": "2502.08859v2",
      "title": "EnigmaEval: A Benchmark of Long Multimodal Reasoning Challenges",
      "title_zh": "翻译失败",
      "authors": [
        "Clinton J. Wang",
        "Dean Lee",
        "Cristina Menghini",
        "Johannes Mols",
        "Jack Doughty",
        "Adam Khoja",
        "Jayson Lynch",
        "Sean Hendryx",
        "Summer Yue",
        "Dan Hendrycks"
      ],
      "abstract": "As language models master existing reasoning benchmarks, we need new\nchallenges to evaluate their cognitive frontiers. Puzzle-solving events are\nrich repositories of challenging multimodal problems that test a wide range of\nadvanced reasoning and knowledge capabilities, making them a unique testbed for\nevaluating frontier language models. We introduce EnigmaEval, a dataset of\nproblems and solutions derived from puzzle competitions and events that probes\nmodels' ability to perform implicit knowledge synthesis and multi-step\ndeductive reasoning. Unlike existing reasoning and knowledge benchmarks, puzzle\nsolving challenges models to discover hidden connections between seemingly\nunrelated pieces of information to uncover solution paths. The benchmark\ncomprises 1184 puzzles of varying complexity -- each typically requiring teams\nof skilled solvers hours to days to complete -- with unambiguous, verifiable\nsolutions that enable efficient evaluation. State-of-the-art language models\nachieve extremely low accuracy on these puzzles, even lower than other\ndifficult benchmarks such as Humanity's Last Exam, unveiling models'\nshortcomings when challenged with problems requiring unstructured and lateral\nreasoning.",
      "tldr_zh": "本研究引入 EnigmaEval，这是一个评估语言模型长多模态推理挑战的基准数据集，基于谜题比赛的问题和解决方案，测试模型在隐含知识合成和多步演绎推理方面的能力。与现有基准不同，EnigmaEval 要求模型发现看似无关信息之间的隐藏连接，从而揭示解决方案路径。该数据集包含 1184 个复杂度不同的谜题，每个通常需团队数小时至数天解决，并提供明确可验证的答案；实验显示，state-of-the-art 语言模型在这些谜题上的准确率极低，甚至低于 Humanity's Last Exam 等基准，暴露了模型在非结构化和横向推理中的显著不足。",
      "categories": [
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2502.08859v2",
      "published_date": "2025-02-13 00:18:34 UTC",
      "updated_date": "2025-02-14 16:40:15 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T11:18:18.238908"
    },
    {
      "arxiv_id": "2502.08858v1",
      "title": "Estimating Probabilities of Causation with Machine Learning Models",
      "title_zh": "使用机器学习",
      "authors": [
        "Shuai Wang",
        "Ang Li"
      ],
      "abstract": "Probabilities of causation play a crucial role in modern decision-making.\nThis paper addresses the challenge of predicting probabilities of causation for\nsubpopulations with insufficient data using machine learning models. Tian and\nPearl first defined and derived tight bounds for three fundamental\nprobabilities of causation: the probability of necessity and sufficiency (PNS),\nthe probability of sufficiency (PS), and the probability of necessity (PN).\nHowever, estimating these probabilities requires both experimental and\nobservational distributions specific to each subpopulation, which are often\nunavailable or impractical to obtain with limited population-level data. We\nassume that the probabilities of causation for each subpopulation are\ndetermined by its characteristics. To estimate these probabilities for\nsubpopulations with insufficient data, we propose using machine learning models\nthat draw insights from subpopulations with sufficient data. Our evaluation of\nmultiple machine learning models indicates that, given sufficient\npopulation-level data and an appropriate choice of machine learning model and\nactivation function, PNS can be effectively predicted. Through simulation\nstudies, we show that our multilayer perceptron (MLP) model with the Mish\nactivation function achieves a mean absolute error (MAE) of approximately 0.02\nin predicting PNS for 32,768 subpopulations using data from around 2,000\nsubpopulations.",
      "tldr_zh": "本论文探讨了使用机器学习模型估计因果概率（Probabilities of Causation），以解决数据不足子群体预测挑战。作者基于 Tian 和 Pearl 定义的 PNS（Probability of Necessity and Sufficiency）、PS（Probability of Sufficiency）和 PN（Probability of Necessity），提出一种方法，通过从有足够数据的子群体中提取洞见来预测其他子群体的这些概率。实验评估显示，多层感知器（Multilayer Perceptron, MLP）模型结合 Mish 激活函数，在使用约 2,000 个子群体数据预测 32,768 个子群体的 PNS 时，达到了约 0.02 的 Mean Absolute Error (MAE)，证明了该方法的有效性。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "8 pages + 2 pages reference + 3 pages supplementary material, 5\n  figures, submitted to UAI 2025",
      "pdf_url": "http://arxiv.org/pdf/2502.08858v1",
      "published_date": "2025-02-13 00:18:08 UTC",
      "updated_date": "2025-02-13 00:18:08 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T11:18:31.742619"
    }
  ],
  "raw_papers_fetched": true,
  "papers_count": 165,
  "processed_papers_count": 165,
  "failed_papers_count": 0,
  "summary_generated": true,
  "daily_data_saved": true,
  "last_update": "2025-05-22T11:18:56.387055"
}
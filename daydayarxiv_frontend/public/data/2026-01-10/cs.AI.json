{
  "date": "2026-01-10",
  "category": "cs.AI",
  "summary": "æ¬¢è¿æ¥åˆ° UTC æ—¶é—´ 2026-01-10 çš„ arXiv ä¸­æ–‡ TLDR å¿«æŠ¥ï¼\n\nğŸ‘‹ å¤§å®¶å¥½ï¼Œæˆ‘æ˜¯ä½ ä»¬çš„ AI ç ”ç©¶å‘˜æœ‹å‹ã€‚\n\n**ä¸€å¥è¯æ€»ç»“ï¼š**\nä»Šå¤©çš„ arXiv çˆ†å‘äº†å¤§é‡å…³äº **Agent å®‰å…¨æ€§ä¸è¯„ä¼°**ï¼ˆä»ä¸“ä¸šä»»åŠ¡åˆ°äº¤äº’å¼ç ”ç©¶ï¼‰ã€**å¤šæ¨¡æ€èåˆ**ï¼ˆç‰¹åˆ«æ˜¯é•¿è§†é¢‘å’Œè‡ªåŠ¨é©¾é©¶æ„ŸçŸ¥ï¼‰ä»¥åŠ **LLM æ¨ç†æœ¬è´¨**ï¼ˆScaling æ˜¯å¦çœŸçš„å¸¦æ¥å¿ å®åº¦ï¼‰çš„è®¨è®ºã€‚å…¶ä¸­ï¼Œå…³äºâ€œå¾®é¢„ç®—â€ä¸‹çš„æ¨ç†èƒ½åŠ›è®­ç»ƒå’Œé’ˆå¯¹ä¸“ä¸šçº§ Agent çš„å®‰å…¨åŸºå‡†æµ‹è¯•éå¸¸å€¼å¾—å…³æ³¨ã€‚\n\n---\n\n### ğŸš€ Agent ä¸ LLMï¼šæ¨ç†ã€å®‰å…¨ä¸è¯„ä¼°\nè¿™éƒ¨åˆ†æ˜¯ä»Šå¤©çš„é‡å¤´æˆï¼Œå¤§å®¶éƒ½åœ¨å…³å¿ƒ Agent åˆ°åº•èƒ½ä¸èƒ½å¹²æ­£ç»äº‹ï¼Œä»¥åŠå¹²å¾—å®‰ä¸å®‰å…¨ã€‚\n\n**1. IDRBench: Interactive Deep Research Benchmark**\n**IDRBenchï¼šäº¤äº’å¼æ·±åº¦ç ”ç©¶åŸºå‡†**\n> *æ ¸å¿ƒè´¡çŒ®ï¼š* ç°åœ¨çš„ Deep Research Agent å¤§å¤šæ˜¯â€œé—·å¤´å¹²æ´»â€ï¼Œç¼ºä¹ä¸ç”¨æˆ·çš„äº¤äº’ã€‚è¿™ç¯‡æ–‡ç« æå‡ºäº† IDRBenchï¼Œä¸“é—¨è¯„ä¼° Agent åœ¨ç ”ç©¶è¿‡ç¨‹ä¸­ä¸ç”¨æˆ·**äº¤äº’**çš„èƒ½åŠ›ï¼ˆæ¾„æ¸…éœ€æ±‚ã€åé¦ˆè¿­ä»£ï¼‰ã€‚\n> *å‘ç°ï¼š* å®éªŒè¡¨æ˜ï¼Œäº¤äº’èƒ½æ˜¾è‘—æå‡ç ”ç©¶è´¨é‡ï¼Œç”šè‡³èƒ½å¼¥è¡¥æ¨¡å‹èƒ½åŠ›çš„ä¸è¶³ï¼Œä½†æ•ˆç‡ï¼ˆToken æ¶ˆè€—ï¼‰æ˜¯ä¸»è¦æƒè¡¡ç‚¹ã€‚è¿™å¯¹åš Research Agent çš„åŒå­¦å¾ˆæœ‰å¯å‘ã€‚\n\n**2. SafePro: Evaluating the Safety of Professional-Level AI Agents**\n**SafeProï¼šè¯„ä¼°ä¸“ä¸šçº§ AI Agent çš„å®‰å…¨æ€§**\n> *æ ¸å¿ƒè´¡çŒ®ï¼š* è¿™æ˜¯ä¸€ä¸ªé’ˆå¯¹**ä¸“ä¸šé¢†åŸŸ**ï¼ˆå¦‚æ³•å¾‹ã€åŒ»ç–—ã€é‡‘èç­‰ï¼‰Agent çš„å®‰å…¨åŸºå‡†ã€‚ä»¥å‰çš„æµ‹è¯•å¤ªç®€å•ï¼Œç°åœ¨çš„ Agent è¦å¤„ç†å¤æ‚ä»»åŠ¡ã€‚\n> *å‘ç°ï¼š* å³ä½¿æ˜¯ SOTA æ¨¡å‹ï¼Œåœ¨å¤„ç†å¤æ‚çš„ä¸“ä¸šä»»åŠ¡æ—¶ä¹Ÿè¡¨ç°å‡ºæ˜æ˜¾çš„å®‰å…¨æ¼æ´ï¼Œç‰¹åˆ«æ˜¯åœ¨éœ€è¦å¤šæ­¥å†³ç­–çš„åœºæ™¯ä¸‹ï¼Œå¾€å¾€ç¼ºä¹è¶³å¤Ÿçš„â€œå®‰å…¨åˆ¤æ–­åŠ›â€ã€‚\n\n**3. Does Inference Scaling Improve Reasoning Faithfulness? A Multi-Model Analysis of Self-Consistency Tradeoffs**\n**æ¨ç†æ‰©å±•æ˜¯å¦æé«˜äº†æ¨ç†çš„å¿ å®åº¦ï¼Ÿå…³äºè‡ªæ´½æ€§æƒè¡¡çš„å¤šæ¨¡å‹åˆ†æ**\n> *æ ¸å¿ƒè´¡çŒ®ï¼š* å¤§å®¶éƒ½åœ¨ç”¨ Self-Consistency (SC) åˆ·åˆ†ï¼Œä½†è¿™çœŸçš„è®©æ¨¡å‹æ¨ç†æ›´â€œè¯šå®â€äº†å—ï¼Ÿ\n> *æœ‰è¶£å‘ç°ï¼š* ç»“è®ºå¾ˆæ‰“è„¸ã€‚**Claude Opus 4.5** è™½ç„¶åœ¨ SC ä¸‹å‡†ç¡®ç‡ä¸‹é™ï¼ˆ!ï¼‰ï¼Œä½†å¿ å®åº¦å¤§å¹…æå‡ï¼›è€Œ **GPT-5.2** å‡†ç¡®ç‡æå‡äº†ï¼Œå¿ å®åº¦å´æ²¡å˜ã€‚DeepSeek-v3.2 è¡¨ç°å‡ºå¤©èŠ±æ¿æ•ˆåº”ã€‚è¿™è¯´æ˜ç›²ç›®å † Inference Scaling å¹¶ä¸æ€»èƒ½å¸¦æ¥æ›´å¥½çš„æ¨ç†è´¨é‡ï¼Œç”šè‡³å¯èƒ½æ©ç›–æ¨¡å‹åœ¨ç®€å•é—®é¢˜ä¸Šçš„â€œçè’™â€ã€‚\n\n**4. ArenaRL: Scaling RL for Open-Ended Agents via Tournament-based Relative Ranking**\n**ArenaRLï¼šé€šè¿‡é”¦æ ‡èµ›ç›¸å¯¹æ’åæ‰©å±•å¼€æ”¾å¼ Agent çš„å¼ºåŒ–å­¦ä¹ **\n> *æ ¸å¿ƒè´¡çŒ®ï¼š* é’ˆå¯¹å¼€æ”¾å¼ä»»åŠ¡ï¼ˆå¦‚å¤æ‚æ—…è¡Œè§„åˆ’ï¼‰ï¼Œä¼ ç»Ÿçš„æ ‡é‡å¥–åŠ±ï¼ˆScalar Rewardï¼‰å¾ˆéš¾åŒºåˆ†å¥½åã€‚é˜¿é‡Œå›¢é˜Ÿæå‡ºäº†ä¸€ç§åŸºäºâ€œé”¦æ ‡èµ›â€çš„ç›¸å¯¹æ’åæœºåˆ¶æ¥è¿›è¡Œ RL è®­ç»ƒã€‚\n> *æ•ˆæœï¼š* è¿™ç§æ–¹æ³•åœ¨æ²¡æœ‰ Ground Truth çš„æƒ…å†µä¸‹ï¼Œé€šè¿‡ç›¸å¯¹ä¼˜åŠ£æ¯”è¾ƒï¼Œæ˜¾è‘—æå‡äº† Agent åœ¨å¤æ‚ç°å®ä»»åŠ¡ä¸­çš„è¡¨ç°ã€‚\n\n**5. Plasticity vs. Rigidity: The Impact of Low-Rank Adapters on Reasoning on a Micro-Budget**\n**å¯å¡‘æ€§ä¸åˆšæ€§ï¼šå¾®é¢„ç®—ä¸‹ä½ç§©é€‚é…å™¨å¯¹æ¨ç†çš„å½±å“**\n> *æ ¸å¿ƒè´¡çŒ®ï¼š* **ç©·äººç¦éŸ³**ã€‚åœ¨ä¸€å¼  A40 (48GB) ä¸Šï¼Œç”¨ä¸åˆ° 24 å°æ—¶è®­ç»ƒæ¨ç†æ¨¡å‹ã€‚\n> *å‘ç°ï¼š* å…³é”®åœ¨äº Adapter çš„ç§©ï¼ˆRankï¼‰ã€‚$r=8$ çš„ LoRA åŸºæœ¬æ²¡ç”¨ï¼Œä½† $r=256$ å¯ä»¥è§£é”å·¨å¤§çš„å¯å¡‘æ€§ï¼Œåœ¨ AIME 24 ä¸Šè¾¾åˆ°äº† 40% çš„ Pass@1ã€‚ä½†è¦æ³¨æ„ï¼Œè¿‡åº¦æ•°å­¦å¯¹é½çš„æ¨¡å‹å¯èƒ½ä¼šå‘ç”Ÿæ€§èƒ½å´©æºƒã€‚\n\n**6. Burn-After-Use for Preventing Data Leakage through a Secure Multi-Tenant Architecture in Enterprise LLM**\n**é˜…åå³ç„šï¼šä¼ä¸šçº§ LLM ä¸­é˜²æ­¢æ•°æ®æ³„éœ²çš„å®‰å…¨å¤šç§Ÿæˆ·æ¶æ„**\n> *æ ¸å¿ƒè´¡çŒ®ï¼š* æå‡ºäº†ä¸€ç§â€œé˜…åå³ç„šâ€ï¼ˆBurn-After-Useï¼‰æœºåˆ¶ï¼Œå¼ºåˆ¶å¯¹è¯ä¸Šä¸‹æ–‡åœ¨ä¼šè¯åé”€æ¯ï¼Œé…åˆå®‰å…¨å¤šç§Ÿæˆ·æ¶æ„ï¼ˆSMTAï¼‰ï¼Œæœ‰æ•ˆé˜²æ­¢ä¼ä¸šå†…éƒ¨çš„æ•°æ®è·¨éƒ¨é—¨æ³„éœ²ã€‚\n\n---\n\n### ğŸ‘ï¸ å¤šæ¨¡æ€ä¸è§†è§‰ï¼šé•¿è§†é¢‘ä¸è‡ªåŠ¨é©¾é©¶\nå¤šæ¨¡æ€æ­£åœ¨ä»â€œçœ‹å›¾è¯´è¯â€å‘â€œç†è§£ä¸–ç•Œâ€è¿›åŒ–ã€‚\n\n**7. QMAVIS: Long Video-Audio Understanding using Fusion of Large Multimodal Models**\n**QMAVISï¼šåˆ©ç”¨å¤§å‹å¤šæ¨¡æ€æ¨¡å‹èåˆè¿›è¡Œé•¿è§†é¢‘-éŸ³é¢‘ç†è§£**\n> *æ ¸å¿ƒè´¡çŒ®ï¼š* é’ˆå¯¹é•¿è§†é¢‘ï¼ˆå‡ åˆ†é’Ÿåˆ°ä¸€å°æ—¶ä»¥ä¸Šï¼‰çš„ç†è§£éš¾ç‚¹ï¼Œæå‡ºäº†ä¸€ç§èåˆ LMMã€LLM å’Œè¯­éŸ³è¯†åˆ«æ¨¡å‹çš„ Late Fusion ç®¡é“ã€‚\n> *æ•ˆæœï¼š* åœ¨ VideoMME ä¸Šæ¯” VideoLlaMA2 æå‡äº† 38.75%ã€‚å¦‚æœä½ åšé•¿è§†é¢‘åˆ†æï¼Œè¿™ç¯‡å€¼å¾—çœ‹ã€‚\n\n**8. SparseOccVLA: Bridging Occupancy and Vision-Language Models via Sparse Queries for Unified 4D Scene Understanding and Planning**\n**SparseOccVLAï¼šé€šè¿‡ç¨€ç–æŸ¥è¯¢è¿æ¥å ç”¨æ …æ ¼ä¸è§†è§‰è¯­è¨€æ¨¡å‹ï¼Œå®ç°ç»Ÿä¸€ 4D åœºæ™¯ç†è§£ä¸è§„åˆ’**\n> *æ ¸å¿ƒè´¡çŒ®ï¼š* è‡ªåŠ¨é©¾é©¶åœˆå…³æ³¨ã€‚å°† Semantic Occupancyï¼ˆå¤ªç¨ å¯†ï¼‰é€šè¿‡ç¨€ç– Query æ¡¥æ¥åˆ° VLMï¼ˆæ“…é•¿æ¨ç†ï¼‰ä¸­ã€‚\n> *æ•ˆæœï¼š* åœ¨ nuScenes ä¸Šå®ç°äº† SOTA çš„è§„åˆ’æŒ‡æ ‡ï¼Œè¯æ˜äº† VLM å¯ä»¥é€šè¿‡ç¨€ç–è¡¨å¾ç†è§£å¤æ‚çš„ 3D ç‰©ç†ç©ºé—´ã€‚\n\n**9. Tone Matters: The Impact of Linguistic Tone on Hallucination in VLMs**\n**è¯­æ°”å¾ˆé‡è¦ï¼šè¯­è¨€è¯­æ°”å¯¹ VLM å¹»è§‰çš„å½±å“**\n> *æ ¸å¿ƒè´¡çŒ®ï¼š* æ¢ç©¶äº† Prompt çš„è¯­æ°”ï¼ˆä»å¹³æ·¡åˆ°æœ‰æ¯’ã€å¼ºè¿«æ€§è¯­æ°”ï¼‰å¦‚ä½•å¯¼è‡´ VLM äº§ç”Ÿå¹»è§‰ã€‚\n> *å‘ç°ï¼š* æœ‰è¶£çš„æ˜¯ï¼Œå¹»è§‰ç‡å¹¶ä¸éšè¯­æ°”çš„å¼ºçƒˆç¨‹åº¦å•è°ƒå¢åŠ ã€‚æ¨¡å‹ä¼¼ä¹æ›´å®¹æ˜“è¯†åˆ«â€œè¯­ä¹‰ä¸Šçš„æ•Œæ„â€ï¼Œä½†å¯¹â€œç»“æ„æ€§çš„å¼ºè¿«â€é˜²å¾¡è¾ƒå¼±ã€‚\n\n---\n\n### ğŸ§¬ AI for Science & Medicine\nåŒ»å­¦ä¸ç”Ÿç‰©å­¦æ˜¯ AI è½åœ°æœ€æ‰å®çš„é¢†åŸŸä¹‹ä¸€ã€‚\n\n**10. Imaging-anchored Multiomics in Cardiovascular Disease**\n**å¿ƒè¡€ç®¡ç–¾ç—…ä¸­ä»¥å½±åƒä¸ºé”šç‚¹çš„å¤šç»„å­¦é›†æˆ**\n> *æ ¸å¿ƒè´¡çŒ®ï¼š* è¿™æ˜¯ä¸€ç¯‡ç»¼è¿°ã€‚å›é¡¾äº†å¦‚ä½•å°†å¿ƒè„å½±åƒï¼ˆMRI, CTï¼‰ä¸è½¬å½•ç»„å­¦ï¼ˆBulk, Single-cell, Spatialï¼‰ç»“åˆã€‚æå‡ºäº†â€œå½±åƒé”šå®šâ€çš„è§†è§’ï¼Œå³å½±åƒå®šä¹‰è¡¨å‹ï¼Œç»„å­¦æä¾›åˆ†å­ä¸Šä¸‹æ–‡ã€‚éå¸¸é€‚åˆæƒ³åš Medical AI å¤šæ¨¡æ€èåˆçš„åŒå­¦å…¥é—¨ã€‚\n\n**11. MedEinst: Benchmarking the Einstellung Effect in Medical LLMs through Counterfactual Differential Diagnosis**\n**MedEinstï¼šé€šè¿‡åäº‹å®é‰´åˆ«è¯Šæ–­åŸºå‡†æµ‹è¯•åŒ»ç–— LLM çš„æ€ç»´å®šåŠ¿æ•ˆåº”**\n> *æ ¸å¿ƒè´¡çŒ®ï¼š* åŒ»ç”Ÿæœ‰â€œæ€ç»´å®šåŠ¿â€ï¼ˆEinstellung Effectï¼‰ï¼ŒLLM ä¹Ÿæœ‰ã€‚è¿™ç¯‡è®ºæ–‡æ„å»ºäº†ä¸€ä¸ªåäº‹å®æ•°æ®é›†ï¼ŒåŒ…å«â€œé™·é˜±â€ç—…ä¾‹ã€‚\n> *å‘ç°ï¼š* å³ä½¿æ˜¯é¡¶çº§ LLM ä¹Ÿå®¹æ˜“æ‰è¿›é™·é˜±ï¼Œå¿½ç•¥ç‰¹å¼‚æ€§è¯æ®è€Œä¾èµ–ç»Ÿè®¡æ·å¾„ã€‚ä½œè€…æå‡ºäº† ECR-Agent æ¥ç¼“è§£è¿™ä¸ªé—®é¢˜ã€‚\n\n**12. Neuro-Symbolic Activation Discovery: Transferring Mathematical Structures from Physics to Ecology for Parameter-Efficient Neural Networks**\n**ç¥ç»ç¬¦å·æ¿€æ´»å‘ç°ï¼šä»ç‰©ç†å­¦åˆ°ç”Ÿæ€å­¦è½¬ç§»æ•°å­¦ç»“æ„ä»¥å®ç°å‚æ•°é«˜æ•ˆç¥ç»ç½‘ç»œ**\n> *æ ¸å¿ƒè´¡çŒ®ï¼š* åˆ©ç”¨é—ä¼ ç¼–ç¨‹ä»ç‰©ç†æ•°æ®ä¸­å‘ç°æ•°å­¦å…¬å¼ä½œä¸ºæ¿€æ´»å‡½æ•°ï¼Œå¹¶æˆåŠŸè¿ç§»åˆ°ç”Ÿæ€å­¦ä»»åŠ¡ä¸­ã€‚\n> *Implicationï¼š* è¿™ç§ Geometric Transfer ç°è±¡è¡¨æ˜ï¼Œç§‘å­¦æ•°æ®å…±äº«æŸç§åº•å±‚çš„æ•°å­¦ç»“æ„ï¼Œå®šåˆ¶åŒ–çš„æ¿€æ´»å‡½æ•°æ¯” ReLU/GELU æ›´é«˜æ•ˆã€‚\n\n---\n\n### âš™ï¸ æ¶æ„ã€ä¼˜åŒ–ä¸æ•ˆç‡\nè¿™é‡Œæœ‰ä¸€äº›ç¡¬æ ¸çš„æ”¹è¿›ã€‚\n\n**13. RewriteNets: End-to-End Trainable String-Rewriting for Generative Sequence Modeling**\n**RewriteNetsï¼šç”¨äºç”Ÿæˆåºåˆ—å»ºæ¨¡çš„ç«¯åˆ°ç«¯å¯è®­ç»ƒå­—ç¬¦ä¸²é‡å†™ç½‘ç»œ**\n> *æ ¸å¿ƒè´¡çŒ®ï¼š* æŒ‘æˆ˜ Transformer çš„ç»Ÿæ²»åœ°ä½ã€‚æå‡ºäº†ä¸€ç§åŸºäºâ€œæ˜¾å¼å­—ç¬¦ä¸²é‡å†™â€çš„æ¶æ„ã€‚\n> *æ•ˆæœï¼š* åœ¨ç³»ç»Ÿæ³›åŒ–ä»»åŠ¡ï¼ˆSCANï¼‰ä¸Šè¾¾åˆ° 98.7% å‡†ç¡®ç‡ï¼Œä¸”æ¯” Transformer æ›´é«˜æ•ˆã€‚è¿™æ˜¯ä¸€ç§å…¨æ–°çš„å½’çº³åç½®è®¾è®¡æ€è·¯ã€‚\n\n**14. L-RAG: Balancing Context and Retrieval with Entropy-Based Lazy Loading**\n**L-RAGï¼šåŸºäºç†µçš„å»¶è¿ŸåŠ è½½å¹³è¡¡ä¸Šä¸‹æ–‡ä¸æ£€ç´¢**\n> *æ ¸å¿ƒè´¡çŒ®ï¼š* ä¼ ç»Ÿçš„ RAG æ˜¯â€œé€¢äººå¿…æ£€â€ï¼Œå¤ªæ…¢å¤ªè´µã€‚L-RAG åªæœ‰åœ¨æ¨¡å‹é¢„æµ‹ç†µå€¼ï¼ˆä¸ç¡®å®šæ€§ï¼‰é«˜çš„æ—¶å€™æ‰è§¦å‘æ£€ç´¢ã€‚\n> *æ•ˆæœï¼š* è¿™æ˜¯ä¸€ä¸ªç®€å•æœ‰æ•ˆçš„å·¥ç¨‹ Trickï¼Œèƒ½æ˜¾è‘—é™ä½å»¶è¿Ÿå’Œæˆæœ¬ï¼Œé€‚åˆç”Ÿäº§ç¯å¢ƒã€‚\n\n**15. Geometric Attention: A Regime-Explicit Operator Semantics for Transformer Attention**\n**å‡ ä½•æ³¨æ„åŠ›ï¼šTransformer æ³¨æ„åŠ›çš„æ˜¾å¼ç®—å­è¯­ä¹‰**\n> *æ ¸å¿ƒè´¡çŒ®ï¼š* ä¸€ç¯‡ 57 é¡µçš„ç†è®ºé•¿æ–‡ã€‚å¯¹ Attention æœºåˆ¶è¿›è¡Œäº†æå…¶æ·±å…¥çš„å‡ ä½•å’Œç®—å­å±‚é¢çš„è§£æ„ã€‚å¦‚æœä½ å¯¹ Transformer çš„åº•å±‚æ•°å­¦åŸç†æ„Ÿå…´è¶£ï¼Œè¿™ç¯‡æ˜¯å¿…è¯»ã€‚\n\n---\n\n### ğŸ“Š å…¶ä»–å€¼å¾—å…³æ³¨çš„è®ºæ–‡ (Quick Hits)\n\n*   **[Finance] BizFinBench.v2:** ä¸€ä¸ªåŸºäºçœŸå®ä¸­ç¾è‚¡å¸‚æ•°æ®çš„é‡‘èä¸“å®¶çº§èƒ½åŠ›å¯¹é½åŸºå‡†ï¼Œæ¶µç›–åœ¨çº¿ä»»åŠ¡ã€‚\n*   **[Coding] Coding in a Bubble?:** æå‡ºäº† **CtxBugGen** æ¡†æ¶ï¼Œä¸“é—¨è¯„ä¼° LLM åœ¨ä»£ç æ”¹ç¼–è¿‡ç¨‹ä¸­è§£å†³â€œä¸Šä¸‹æ–‡é€‚é… Bugâ€çš„èƒ½åŠ›ï¼ˆLLM ç»å¸¸åœ¨è¿™æ–¹é¢ç¿»è½¦ï¼‰ã€‚\n*   **[Traffic] Reinforcement Learning-Guided Dynamic Multi-Graph Fusion:** åˆ©ç”¨ RL å¼•å¯¼çš„å¤šå›¾èåˆè¿›è¡Œé£“é£ç–æ•£äº¤é€šé¢„æµ‹ï¼Œä¸ä»…å‡†è€Œä¸”å¯è§£é‡Šã€‚\n*   **[Hardware] Revisiting Training Scale:** å®éªŒè¯æ˜ï¼Œå•çº¯å¢åŠ è®­ç»ƒ Token æ•°ï¼Œå¦‚æœä¸è€ƒè™‘èƒ½è€—å’Œæ—¶é—´ï¼Œå¯èƒ½ä¼šå¯¼è‡´è¾¹é™…æ•ˆç›Šé€’å‡ç”šè‡³èƒ½é‡æ•ˆç‡çš„å•è°ƒä¸‹é™ã€‚\n\n---\n\nğŸ‰ **ç»“è¯­ï¼š**\nä»Šå¤©çš„è®ºæ–‡è´¨é‡å¾ˆé«˜ï¼Œç‰¹åˆ«æ˜¯å…³äº **Inference Scaling** çš„åæ€å’Œ **Micro-Budget** çš„å°è¯•ï¼Œç»™èµ„æºæœ‰é™çš„ç ”ç©¶è€…å¸¦æ¥äº†å¸Œæœ›ã€‚åŒæ—¶ï¼Œ**SafePro** å’Œ **IDRBench** çš„å‡ºç°ä¹Ÿæ ‡å¿—ç€ Agent ç ”ç©¶è¿›å…¥äº†æ›´ç²¾ç»†åŒ–ã€æ›´æ³¨é‡äº¤äº’å’Œå®‰å…¨çš„é˜¶æ®µã€‚\n\nå¸Œæœ›è¿™ä»½å¿«æŠ¥å¯¹ä½ æœ‰å¸®åŠ©ï¼Œæˆ‘ä»¬æ˜å¤©è§ï¼",
  "papers": [
    {
      "arxiv_id": "2601.07871v1",
      "title": "Imaging-anchored Multiomics in Cardiovascular Disease: Integrating Cardiac Imaging, Bulk, Single-cell, and Spatial Transcriptomics",
      "title_zh": "å¿ƒè¡€ç®¡ç–¾ç—…ä¸­çš„å½±åƒé”šå®šå¤šç»„å­¦ï¼šæ•´åˆå¿ƒè„å½±åƒã€ç¾¤ä½“ã€å•ç»†èƒåŠç©ºé—´è½¬å½•ç»„å­¦",
      "authors": [
        "Minh H. N. Le",
        "Tuan Vinh",
        "Thanh-Huy Nguyen",
        "Tao Li",
        "Bao Quang Gia Le",
        "Han H. Huynh",
        "Monika Raj",
        "Carl Yang",
        "Min Xu",
        "Nguyen Quoc Khanh Le"
      ],
      "abstract": "Cardiovascular disease arises from interactions between inherited risk, molecular programmes, and tissue-scale remodelling that are observed clinically through imaging. Health systems now routinely generate large volumes of cardiac MRI, CT and echocardiography together with bulk, single-cell and spatial transcriptomics, yet these data are still analysed in separate pipelines. This review examines joint representations that link cardiac imaging phenotypes to transcriptomic and spatially resolved molecular states. An imaging-anchored perspective is adopted in which echocardiography, cardiac MRI and CT define a spatial phenotype of the heart, and bulk, single-cell and spatial transcriptomics provide cell-type- and location-specific molecular context. The biological and technical characteristics of these modalities are first summarised, and representation-learning strategies for each are outlined. Multimodal fusion approaches are reviewed, with emphasis on handling missing data, limited sample size, and batch effects. Finally, integrative pipelines for radiogenomics, spatial molecular alignment, and image-based prediction of gene expression are discussed, together with common failure modes, practical considerations, and open challenges. Spatial multiomics of human myocardium and atherosclerotic plaque, single-cell and spatial foundation models, and multimodal medical foundation models are collectively bringing imaging-anchored multiomics closer to large-scale cardiovascular translation.",
      "tldr_zh": "è¯¥ç»¼è¿°æ¢è®¨äº†å¿ƒè¡€ç®¡ç–¾ç—…(Cardiovascular disease)ä¸­çš„å½±åƒé”šå®šå¤šç»„å­¦(Imaging-anchored multiomics)ï¼Œé‡ç‚¹ä»‹ç»äº†å°†å¿ƒè„MRIã€CTåŠè¶…å£°å¿ƒåŠ¨å›¾(echocardiography)ä¸å¤§å—(bulk)ã€å•ç»†èƒ(single-cell)å’Œç©ºé—´è½¬å½•ç»„å­¦(spatial transcriptomics)ç›¸ç»“åˆçš„è”åˆè¡¨ç¤ºæ–¹æ³•ã€‚æ–‡ç« æå‡ºä»¥å½±åƒé”šå®šçš„è§†è§’ï¼Œåˆ©ç”¨å½±åƒæ•°æ®å®šä¹‰å¿ƒè„çš„ç©ºé—´è¡¨å‹ï¼Œå¹¶ç»“åˆè½¬å½•ç»„å­¦æä¾›ç‰¹å®šç»†èƒç±»å‹å’Œä½ç½®çš„åˆ†å­èƒŒæ™¯ã€‚ç ”ç©¶é‡ç‚¹è¯„ä¼°äº†å¤šæ¨¡æ€èåˆ(Multimodal fusion)ç­–ç•¥ï¼Œç‰¹åˆ«æ˜¯é’ˆå¯¹æ•°æ®ç¼ºå¤±ã€æ ·æœ¬é‡æœ‰é™å’Œæ‰¹æ¬¡æ•ˆåº”(batch effects)çš„å¤„ç†æ–¹æ¡ˆã€‚æœ€åï¼Œæ–‡ç« è®¨è®ºäº†æ”¾å°„åŸºå› ç»„å­¦(radiogenomics)ã€ç©ºé—´åˆ†å­æ¯”å¯¹åŠå½±åƒé©±åŠ¨çš„åŸºå› è¡¨è¾¾é¢„æµ‹ç­‰é›†æˆæµæ°´çº¿ï¼Œå¹¶å±•æœ›äº†åŒ»ç–—åŸºç¡€æ¨¡å‹(medical foundation models)åœ¨æ¨åŠ¨å¿ƒè¡€ç®¡é¢†åŸŸå¤§è§„æ¨¡ä¸´åºŠè½¬åŒ–ä¸­çš„æ½œåŠ›ã€‚",
      "categories": [
        "q-bio.QM",
        "cs.AI",
        "cs.CV",
        "cs.LG"
      ],
      "primary_category": "q-bio.QM",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2601.07871v1",
      "published_date": "2026-01-10 23:30:49 UTC",
      "updated_date": "2026-01-10 23:30:49 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-22T23:02:58.995360+00:00"
    },
    {
      "arxiv_id": "2601.06704v1",
      "title": "Beyond Perfect Scores: Proof-by-Contradiction for Trustworthy Machine Learning",
      "title_zh": "é€¾è¶Šå®Œç¾è¯„åˆ†ï¼šé¢å‘å¯ä¿¡æœºå™¨å­¦ä¹ çš„åè¯æ³•",
      "authors": [
        "Dushan N. Wadduwage",
        "Dineth Jayakody",
        "Leonidas Zimianitis"
      ],
      "abstract": "Machine learning (ML) models show strong promise for new biomedical prediction tasks, but concerns about trustworthiness have hindered their clinical adoption. In particular, it is often unclear whether a model relies on true clinical cues or on spurious hierarchical correlations in the data. This paper introduces a simple yet broadly applicable trustworthiness test grounded in stochastic proof-by-contradiction. Instead of just showing high test performance, our approach trains and tests on spurious labels carefully permuted based on a potential outcomes framework. A truly trustworthy model should fail under such label permutation; comparable accuracy across real and permuted labels indicates overfitting, shortcut learning, or data leakage. Our approach quantifies this behavior through interpretable Fisher-style p-values, which are well understood by domain experts across medical and life sciences. We evaluate our approach on multiple new bacterial diagnostics to separate tasks and models learning genuine causal relationships from those driven by dataset artifacts or statistical coincidences. Our work establishes a foundation to build rigor and trust between ML and life-science research communities, moving ML models one step closer to clinical adoption.",
      "tldr_zh": "è¯¥ç ”ç©¶å¼•å…¥äº†ä¸€ç§åŸºäºéšæœºåè¯æ³•(proof-by-contradiction)çš„é€šç”¨å¯ä¿¡åº¦æµ‹è¯•æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³ç”Ÿç‰©åŒ»å­¦æœºå™¨å­¦ä¹ æ¨¡å‹ä¸­ç”±äºè™šå‡ç›¸å…³æ€§æˆ–æ•°æ®æ³„æ¼å¯¼è‡´çš„ä¸é€æ˜é—®é¢˜ã€‚è¯¥æ–¹æ³•é€šè¿‡åœ¨åŸºäºæ½œåœ¨ç»“æœæ¡†æ¶(potential outcomes framework)ç½®æ¢åçš„è™šå‡æ ‡ç­¾ä¸Šè®­ç»ƒå’Œæµ‹è¯•æ¨¡å‹ï¼ŒéªŒè¯å…¶æ˜¯å¦çœŸæ­£ä¾èµ–ä¸´åºŠçº¿ç´¢è€Œéæ•°æ®åå·®ï¼›ä¸€ä¸ªçœŸæ­£å¯ä¿¡çš„æ¨¡å‹åœ¨ç½®æ¢æ ‡ç­¾ä¸‹çš„è¡¨ç°åº”å½“æ˜¾è‘—ä¸‹é™ã€‚ç ”ç©¶åˆ©ç”¨è´¹èˆå°”é£æ ¼çš„på€¼(Fisher-style p-values)æ¥é‡åŒ–è¿™ä¸€è¡Œä¸ºï¼Œä¸ºä¸´åºŠå’Œç”Ÿå‘½ç§‘å­¦é¢†åŸŸçš„ä¸“å®¶æä¾›äº†ç›´è§‚çš„å¯è§£é‡Šæ€§ä¾æ®ã€‚åœ¨ç»†èŒè¯Šæ–­ä»»åŠ¡ä¸Šçš„è¯„ä¼°è¡¨æ˜ï¼Œè¯¥æ–¹æ³•èƒ½æœ‰æ•ˆåŒºåˆ†æ¨¡å‹å­¦ä¹ åˆ°çš„æ˜¯çœŸæ­£çš„å› æœå…³ç³»è¿˜æ˜¯æ•°æ®é›†ä¼ªå½±ï¼Œä¸ºæ¨åŠ¨æœºå™¨å­¦ä¹ æ¨¡å‹çš„ä¸´åºŠåº”ç”¨å¥ å®šäº†ä¸¥è°¨çš„åŸºç¡€ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.LG",
      "comment": "13 pages, 6 figures",
      "pdf_url": "https://arxiv.org/pdf/2601.06704v1",
      "published_date": "2026-01-10 22:08:14 UTC",
      "updated_date": "2026-01-10 22:08:14 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-22T23:03:00.055518+00:00"
    },
    {
      "arxiv_id": "2601.06701v1",
      "title": "Explainability of Complex AI Models with Correlation Impact Ratio",
      "title_zh": "åŸºäºç›¸å…³å½±å“æ¯”çš„å¤æ‚äººå·¥æ™ºèƒ½æ¨¡å‹å¯è§£é‡Šæ€§",
      "authors": [
        "Poushali Sengupta",
        "Rabindra Khadka",
        "Sabita Maharjan",
        "Frank Eliassen",
        "Yan Zhang",
        "Shashi Raj Pandey",
        "Pedro G. Lind",
        "Anis Yazidi"
      ],
      "abstract": "Complex AI systems make better predictions but often lack transparency, limiting trustworthiness, interpretability, and safe deployment. Common post hoc AI explainers, such as LIME, SHAP, HSIC, and SAGE, are model agnostic but are too restricted in one significant regard: they tend to misrank correlated features and require costly perturbations, which do not scale to high dimensional data. We introduce ExCIR (Explainability through Correlation Impact Ratio), a theoretically grounded, simple, and reliable metric for explaining the contribution of input features to model outputs, which remains stable and consistent under noise and sampling variations. We demonstrate that ExCIR captures dependencies arising from correlated features through a lightweight single pass formulation. Experimental evaluations on diverse datasets, including EEG, synthetic vehicular data, Digits, and Cats-Dogs, validate the effectiveness and stability of ExCIR across domains, achieving more interpretable feature explanations than existing methods while remaining computationally efficient. To this end, we further extend ExCIR with an information theoretic foundation that unifies the correlation ratio with Canonical Correlation Analysis under mutual information bounds, enabling multi output and class conditioned explainability at scale.",
      "tldr_zh": "---\n\n### è®ºæ–‡æ‘˜è¦æ€»ç»“ ğŸ“\n\nè¯¥ç ”ç©¶å¼•å…¥äº† **ExCIR (Explainability through Correlation Impact Ratio)**ï¼Œè¿™æ˜¯ä¸€ç§å…·æœ‰ç†è®ºåŸºç¡€ã€ç®€å•ä¸”å¯é çš„å½’å› åº¦é‡æ–¹æ³•ï¼Œæ—¨åœ¨è§£å†³ LIMEã€SHAP ç­‰ä¼ ç»Ÿè§£é‡Šå™¨åœ¨å¤„ç†é«˜ç»´æ•°æ®å’Œ **correlated features** æ—¶è®¡ç®—æˆæœ¬é«˜åŠæ’åºä¸å‡†ç¡®çš„é—®é¢˜ã€‚é€šè¿‡è½»é‡çº§çš„ **single pass** å…¬å¼ï¼ŒExCIR èƒ½å¤Ÿç¨³å®šåœ°æ•æ‰ç‰¹å¾é—´çš„ä¾èµ–å…³ç³»ï¼Œå¹¶åœ¨å™ªå£°å’Œé‡‡æ ·å˜åŒ–ä¸‹ä¿æŒä¸€è‡´æ€§ã€‚å®éªŒç»“æœåœ¨ EEGã€åˆæˆè½¦è¾†æ•°æ®åŠå›¾åƒåˆ†ç±»ç­‰å¤šä¸ªé¢†åŸŸéªŒè¯äº†å…¶ä¼˜è¶Šæ€§ï¼Œè¯æ˜è¯¥æ–¹æ³•åœ¨æä¾›æ›´å…·å¯è§£é‡Šæ€§çš„ç‰¹å¾è¯´æ˜çš„åŒæ—¶ï¼Œä¿æŒäº†æé«˜çš„è®¡ç®—æ•ˆç‡ã€‚æ­¤å¤–ï¼Œç ”ç©¶è¿›ä¸€æ­¥ç»“åˆä¿¡æ¯è®ºåŸºç¡€ï¼ˆå¦‚ **Mutual Information** å’Œ **Canonical Correlation Analysis**ï¼‰æ‰©å±•äº† ExCIRï¼Œä½¿å…¶èƒ½å¤Ÿæ”¯æŒå¤§è§„æ¨¡çš„å¤šè¾“å‡ºå’Œ **class-conditioned** è§£é‡Šä»»åŠ¡ã€‚\n\n---\n\nå¸Œæœ›è¿™ä¸ªæ€»ç»“å¯¹æ‚¨æœ‰æ‰€å¸®åŠ©ï¼å¦‚æœæ‚¨è¿˜æœ‰å…¶ä»–è®ºæ–‡éœ€è¦å¤„ç†ï¼Œæˆ–è€…æƒ³é’ˆå¯¹ ExCIR çš„å…·ä½“å®éªŒç»“æœè¿›è¡Œæ›´æ·±å…¥çš„æ¢è®¨ï¼Œæ¬¢è¿éšæ—¶å‘Šè¯‰æˆ‘ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "stat.AP"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2601.06701v1",
      "published_date": "2026-01-10 21:56:24 UTC",
      "updated_date": "2026-01-10 21:56:24 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-22T23:03:02.026660+00:00"
    },
    {
      "arxiv_id": "2601.06700v1",
      "title": "Characterising Toxicity in Generative Large Language Models",
      "title_zh": "ç”Ÿæˆå¼å¤§è¯­è¨€æ¨¡å‹ä¸­çš„æ¯’æ€§è¡¨å¾",
      "authors": [
        "Zhiyao Zhang",
        "Yazan Mash'Al",
        "Yuhan Wu"
      ],
      "abstract": "In recent years, the advent of the attention mechanism has significantly advanced the field of natural language processing (NLP), revolutionizing text processing and text generation. This has come about through transformer-based decoder-only architectures, which have become ubiquitous in NLP due to their impressive text processing and generation capabilities. Despite these breakthroughs, language models (LMs) remain susceptible to generating undesired outputs: inappropriate, offensive, or otherwise harmful responses. We will collectively refer to these as ``toxic'' outputs. Although methods like reinforcement learning from human feedback (RLHF) have been developed to align model outputs with human values, these safeguards can often be circumvented through carefully crafted prompts. Therefore, this paper examines the extent to which LLMs generate toxic content when prompted, as well as the linguistic factors -- both lexical and syntactic -- that influence the production of such outputs in generative models.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†åŸºäº Transformer æ¶æ„çš„ç”Ÿæˆå¼ Large Language Models (LLMs) åœ¨ä¸åŒ prompts è¯±å¯¼ä¸‹äº§ç”Ÿæ¯’æ€§ï¼ˆToxicityï¼‰å†…å®¹çš„ç¨‹åº¦ã€‚å°½ç®¡ç°æœ‰æ¨¡å‹é‡‡ç”¨äº† reinforcement learning from human feedback (RLHF) ç­‰äººç±»ä»·å€¼å¯¹é½æŠ€æœ¯ï¼Œä½†ç ”ç©¶å‘ç°è¿™äº›é˜²æŠ¤æªæ–½ä»å¯èƒ½è¢«è§„é¿ã€‚è®ºæ–‡é‡ç‚¹åˆ†æäº†å½±å“æ¯’æ€§è¾“å‡ºäº§ç”Ÿçš„è¯­è¨€å­¦å› ç´ ï¼Œæ·±å…¥ç ”ç©¶äº†è¯æ±‡ï¼ˆLexicalï¼‰å’Œå¥æ³•ï¼ˆSyntacticï¼‰ç‰¹å¾å¯¹æœ‰å®³å†…å®¹ç”Ÿæˆçš„å…·ä½“å½±å“ã€‚è¯¥å·¥ä½œä¸ºç†è§£ç”Ÿæˆå¼æ¨¡å‹ä¸­ä¸å½“ã€å†’çŠ¯æˆ–æœ‰å®³å“åº”çš„å½¢æˆæœºåˆ¶æä¾›äº†ç³»ç»Ÿçš„ç‰¹å¾åˆ»ç”»ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2601.06700v1",
      "published_date": "2026-01-10 21:50:05 UTC",
      "updated_date": "2026-01-10 21:50:05 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-22T23:03:05.735096+00:00"
    },
    {
      "arxiv_id": "2601.06677v1",
      "title": "Plasticity vs. Rigidity: The Impact of Low-Rank Adapters on Reasoning on a Micro-Budget",
      "title_zh": "å¯å¡‘æ€§ä¸åˆšæ€§ï¼šå¾®é¢„ç®—ä¸‹ä½ç§©é€‚é…å™¨å¯¹æ¨ç†èƒ½åŠ›çš„å½±å“",
      "authors": [
        "Zohaib Khan",
        "Omer Tafveez",
        "Zoha Hayat Bhatti"
      ],
      "abstract": "Recent advances in mathematical reasoning typically rely on massive scale, yet the question remains: can strong reasoning capabilities be induced in small language models ($\\leq1.5\\text{B}$) under extreme constraints? We investigate this by training models on a single A40 GPU (48GB) for under 24 hours using Reinforcement Learning with Verifiable Rewards (RLVR) and Low-Rank Adaptation (LoRA). We find that the success of this ``micro-budget\" regime depends critically on the interplay between adapter capacity and model initialization. While low-rank adapters ($r=8$) consistently fail to capture the complex optimization dynamics of reasoning, high-rank adapters ($r=256$) unlock significant plasticity in standard instruction-tuned models. Our best result achieved an impressive 40.0\\% Pass@1 on AIME 24 (an 11.1\\% absolute improvement over baseline) and pushed Pass@16 to 70.0\\%, demonstrating robust exploration capabilities. However, this plasticity is not universal: while instruction-tuned models utilized the budget to elongate their chain-of-thought and maximize reward, heavily math-aligned models suffered performance collapse, suggesting that noisy, low-budget RL updates can act as destructive interference for models already residing near a task-specific optimum.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†åœ¨æä½é¢„ç®—ï¼ˆå•å¼  A40 GPUï¼Œè¿è¡Œä¸è¶³24å°æ—¶ï¼‰ä¸‹ï¼Œå¦‚ä½•é€šè¿‡å…·æœ‰å¯éªŒè¯å¥–åŠ±çš„å¼ºåŒ–å­¦ä¹ (RLVR)å’Œä½ç§©è‡ªé€‚åº”(LoRA)æ¿€å‘å°è¯­è¨€æ¨¡å‹ï¼ˆ$\\leq1.5\\text{B}$ï¼‰çš„æ•°å­¦æ¨ç†èƒ½åŠ›ã€‚ç ”ç©¶å‘ç°ï¼Œâ€œå¾®é¢„ç®—â€æ–¹æ¡ˆçš„æˆåŠŸå…³é”®åœ¨äº Adapter å®¹é‡ä¸æ¨¡å‹åˆå§‹åŒ–çš„ååŒä½œç”¨ï¼šä½ç§©ï¼ˆ$r=8$ï¼‰éš¾ä»¥æ•è·å¤æ‚çš„æ¨ç†åŠ¨æ€ï¼Œè€Œé«˜ç§©ï¼ˆ$r=256$ï¼‰èƒ½æ˜¾è‘—é‡Šæ”¾æŒ‡ä»¤å¾®è°ƒæ¨¡å‹çš„å¯å¡‘æ€§ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œæœ€ä½³æ¨¡å‹åœ¨ AIME 24 ä¸Šçš„ Pass@1 è¾¾åˆ° 40.0%ï¼Œè¾ƒåŸºçº¿æå‡äº† 11.1%ã€‚ç„¶è€Œï¼Œè¿™ç§æå‡å¹¶ä¸å…·å¤‡æ™®é€‚æ€§ï¼Œå¯¹äºå·²ç»æ·±åº¦æ•°å­¦å¯¹é½çš„æ¨¡å‹ï¼Œä½é¢„ç®— RL æ›´æ–°äº§ç”Ÿçš„å™ªå£°ä¼šå½¢æˆç ´åæ€§å¹²æ‰°å¹¶å¯¼è‡´æ€§èƒ½å´©æºƒã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "9 pages, 4 figures, 2 tables",
      "pdf_url": "https://arxiv.org/pdf/2601.06677v1",
      "published_date": "2026-01-10 20:29:45 UTC",
      "updated_date": "2026-01-10 20:29:45 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-22T23:03:09.901744+00:00"
    },
    {
      "arxiv_id": "2601.06676v1",
      "title": "IDRBench: Interactive Deep Research Benchmark",
      "title_zh": "IDRBenchï¼šäº¤äº’å¼æ·±åº¦ç ”ç©¶è¯„æµ‹åŸºå‡†",
      "authors": [
        "Yingchaojie Feng",
        "Qiang Huang",
        "Xiaoya Xie",
        "Zhaorui Yang",
        "Jun Yu",
        "Wei Chen",
        "Anthony K. H. Tung"
      ],
      "abstract": "Deep research agents powered by Large Language Models (LLMs) can perform multi-step reasoning, web exploration, and long-form report generation. However, most existing systems operate in an autonomous manner, assuming fully specified user intent and evaluating only final outputs. In practice, research goals are often underspecified and evolve during exploration, making sustained interaction essential for robust alignment. Despite its importance, interaction remains largely invisible to existing deep research benchmarks, which neither model dynamic user feedback nor quantify its costs. We introduce IDRBench, the first benchmark for systematically evaluating interactive deep research. IDRBench combines a modular multi-agent research framework with on-demand interaction, a scalable reference-grounded user simulator, and an interaction-aware evaluation suite that jointly measures interaction benefits (quality and alignment) and costs (turns and tokens). Experiments across seven state-of-the-art LLMs show that interaction consistently improves research quality and robustness, often outweighing differences in model capacity, while revealing substantial trade-offs in interaction efficiency.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† IDRBenchï¼Œè¿™æ˜¯é¦–ä¸ªç”¨äºç³»ç»Ÿè¯„ä¼°äº¤äº’å¼æ·±åº¦ç ”ç©¶ï¼ˆInteractive Deep Researchï¼‰çš„åŸºå‡†æµ‹è¯•ï¼Œæ—¨åœ¨è§£å†³ç°æœ‰ Deep research agents åœ¨å¤„ç†æ¨¡ç³Šç”¨æˆ·æ„å›¾æ—¶ç¼ºä¹åŠ¨æ€äº¤äº’ä¸æˆæœ¬è¯„ä¼°çš„é—®é¢˜ã€‚IDRBench æ•´åˆäº†æ¨¡å—åŒ–å¤šæ™ºèƒ½ä½“ç ”ç©¶æ¡†æ¶ï¼ˆMulti-agent research frameworkï¼‰ã€å¯æ‰©å±•çš„å‚è€ƒé©±åŠ¨ç”¨æˆ·æ¨¡æ‹Ÿå™¨ï¼ˆReference-grounded user simulatorï¼‰ä»¥åŠäº¤äº’æ„ŸçŸ¥è¯„ä¼°å¥—ä»¶ï¼Œèƒ½å¤ŸåŒæ—¶é‡åŒ–äº¤äº’å¸¦æ¥çš„æ”¶ç›Šï¼ˆè´¨é‡ä¸å¯¹é½åº¦ï¼‰ä¸æˆæœ¬ï¼ˆäº¤äº’è½®æ¬¡ä¸ Tokensï¼‰ã€‚å¯¹ 7 ç§ä¸»æµ LLMs çš„å®éªŒè¡¨æ˜ï¼ŒæŒç»­çš„äº¤äº’èƒ½æ˜¾è‘—æå‡ç ”ç©¶çš„è´¨é‡ä¸é²æ£’æ€§ï¼Œå…¶å½±å“å¾€å¾€è¶…è¿‡äº†æ¨¡å‹æœ¬èº«èƒ½åŠ›çš„å·®å¼‚ã€‚æ­¤å¤–ï¼Œè¯¥ç ”ç©¶è¿˜æ­ç¤ºäº†ä¸åŒæ¨¡å‹åœ¨äº¤äº’æ•ˆç‡ä¸ç ”ç©¶è¡¨ç°ä¹‹é—´å­˜åœ¨çš„æƒè¡¡å…³ç³»ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.HC"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2601.06676v1",
      "published_date": "2026-01-10 20:29:12 UTC",
      "updated_date": "2026-01-10 20:29:12 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-22T23:03:12.429650+00:00"
    },
    {
      "arxiv_id": "2601.06670v1",
      "title": "Otimizando A AlocaÃ§Ã£o De Salas De Aula Com Foco Na Acessibilidade Para Pessoas Com DeficiÃªncia",
      "title_zh": "èšç„¦æ®‹ç–¾äººæ— éšœç¢æ€§çš„æ•™å®¤åˆ†é…ä¼˜åŒ–",
      "authors": [
        "Francisco Glaubos Nunes ClÃ­maco",
        "Jorge Lucas Silva Cavalcante"
      ],
      "abstract": "This paper addresses the challenge of classroom allocation in higher education institutions, with an explicit emphasis on accessibility for Persons with Disabilities (PwDs). Employing a case study of a university's computer science department, the paper proposes an Integer Linear Programming (ILP)-based optimization model, which is solved using the Gurobi solver. The objective is to minimize the number of classrooms used by prioritizing the assignment of PwD students to ground-floor classrooms to reduce accessibility barriers. The model is calibrated with a weighting parameter, alpha, that allows for a balance between spatial efficiency and promoting accessibility. Experimental results indicate that adjusting alpha can achieve a balance point that significantly improves current manual allocation practices, reducing the number of classrooms required and accessibility penalties. The findings suggest that optimization methods can improve operational efficiency in academic institutions while promoting a more inclusive environment for all students. Future work may expand the application of the model to other departments and contexts and integrate additional criteria to develop a more holistic approach.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹é«˜ç­‰æ•™è‚²æœºæ„çš„æ•™å®¤åˆ†é…é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§ä¾§é‡äºæ®‹ç–¾äººï¼ˆPwDsï¼‰æ— éšœç¢è®¿é—®çš„ä¼˜åŒ–æ–¹æ¡ˆã€‚ç ”ç©¶é‡‡ç”¨æ•´æ•°çº¿æ€§ç¼–ç¨‹ï¼ˆInteger Linear Programming, ILPï¼‰æ„å»ºæ¨¡å‹ï¼Œå¹¶åˆ©ç”¨ Gurobi æ±‚è§£å™¨è¿›è¡Œæ±‚è§£ï¼Œæ—¨åœ¨é€šè¿‡ä¼˜å…ˆå°† PwDs åˆ†é…è‡³åº•å±‚æ•™å®¤æ¥æ¶ˆé™¤å»ºç­‘éšœç¢ã€‚é€šè¿‡å¼•å…¥åŠ æƒå‚æ•° alphaï¼Œæ¨¡å‹å®ç°äº†ç©ºé—´åˆ©ç”¨æ•ˆç‡ä¸æ— éšœç¢æ¨å¹¿ä¹‹é—´çš„å¹³è¡¡ã€‚å®éªŒè¯æ˜ï¼Œè¯¥æ–¹æ³•æ˜¾è‘—ä¼˜äºç°æœ‰çš„äººå·¥åˆ†é…å®è·µï¼Œåœ¨å‡å°‘æ‰€éœ€æ•™å®¤æ•°é‡çš„åŒæ—¶å¤§å¹…é™ä½äº†æ— éšœç¢æƒ©ç½šï¼ˆaccessibility penaltiesï¼‰ï¼Œä¸ºæ„å»ºåŒ…å®¹æ€§æ ¡å›­ç¯å¢ƒæä¾›äº†é«˜æ•ˆçš„å†³ç­–æ”¯æŒã€‚",
      "categories": [
        "cs.CY",
        "cs.AI"
      ],
      "primary_category": "cs.CY",
      "comment": "in Portuguese language",
      "pdf_url": "https://arxiv.org/pdf/2601.06670v1",
      "published_date": "2026-01-10 20:11:23 UTC",
      "updated_date": "2026-01-10 20:11:23 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-22T23:03:13.526469+00:00"
    },
    {
      "arxiv_id": "2601.06666v1",
      "title": "InFi-Check: Interpretable and Fine-Grained Fact-Checking of LLMs",
      "title_zh": "InFi-Checkï¼šå¤§è¯­è¨€æ¨¡å‹çš„å¯è§£é‡ŠåŠç»†ç²’åº¦äº‹å®æ ¸æŸ¥",
      "authors": [
        "Yuzhuo Bai",
        "Shuzheng Si",
        "Kangyang Luo",
        "Qingyi Wang",
        "Wenhao Li",
        "Gang Chen",
        "Fanchao Qi",
        "Maosong Sun"
      ],
      "abstract": "Large language models (LLMs) often hallucinate, yet most existing fact-checking methods treat factuality evaluation as a binary classification problem, offering limited interpretability and failing to capture fine-grained error types. In this paper, we introduce InFi-Check, a framework for interpretable and fine-grained fact-checking of LLM outputs. Specifically, we first propose a controlled data synthesis pipeline that generates high-quality data featuring explicit evidence, fine-grained error type labels, justifications, and corrections. Based on this, we further construct large-scale training data and a manually verified benchmark InFi-Check-FG for fine-grained fact-checking of LLM outputs. Building on these high-quality training data, we further propose InFi-Checker, which can jointly provide supporting evidence, classify fine-grained error types, and produce justifications along with corrections. Experiments show that InFi-Checker achieves state-of-the-art performance on InFi-Check-FG and strong generalization across various downstream tasks, significantly improving the utility and trustworthiness of factuality evaluation.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† InFi-Checkï¼Œè¿™æ˜¯ä¸€ä¸ªé’ˆå¯¹å¤§è¯­è¨€æ¨¡å‹ (LLMs) è¾“å‡ºè¿›è¡Œå¯è§£é‡Šä¸”ç»†ç²’åº¦äº‹å®æ ¸æŸ¥çš„æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³ç°æœ‰æ–¹æ³•ä»…æä¾›äºŒå…ƒåˆ†ç±»ç»“æœä¸”ç¼ºä¹è§£é‡Šæ€§çš„å±€é™ã€‚é€šè¿‡å—æ§çš„æ•°æ®åˆæˆæµæ°´çº¿ï¼Œç ”ç©¶è€…æ„å»ºäº†åŒ…å«æ˜ç¡®è¯æ®ã€ç»†ç²’åº¦é”™è¯¯ç±»å‹æ ‡ç­¾åŠä¿®æ­£å»ºè®®çš„å¤§è§„æ¨¡è®­ç»ƒæ•°æ®å’Œæ‰‹åŠ¨éªŒè¯çš„åŸºå‡†æµ‹è¯•é›† InFi-Check-FGã€‚åŸºäºæ­¤å¼€å‘çš„ InFi-Checker æ¨¡å‹èƒ½å¤ŸåŒæ—¶æä¾›æ”¯æŒè¯æ®ã€åˆ†ç±»é”™è¯¯ç±»å‹ï¼Œå¹¶ç”Ÿæˆè¯¦ç»†çš„ç†ç”±ä¸ä¿®æ­£å†…å®¹ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒInFi-Checker åœ¨ InFi-Check-FG ä¸Šè¾¾åˆ°äº† SOTA æ€§èƒ½ï¼Œå¹¶åœ¨å¤šç§ä¸‹æ¸¸ä»»åŠ¡ä¸­è¡¨ç°å‡ºæå¼ºçš„æ³›åŒ–èƒ½åŠ›ï¼Œæ˜¾è‘—æå‡äº†äº‹å®æ€§è¯„ä¼°çš„å®ç”¨æ€§å’Œå¯ä¿¡åº¦ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2601.06666v1",
      "published_date": "2026-01-10 20:00:17 UTC",
      "updated_date": "2026-01-10 20:00:17 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-22T23:03:14.845570+00:00"
    },
    {
      "arxiv_id": "2601.07868v1",
      "title": "RewriteNets: End-to-End Trainable String-Rewriting for Generative Sequence Modeling",
      "title_zh": "RewriteNetsï¼šé¢å‘ç”Ÿæˆå¼åºåˆ—å»ºæ¨¡çš„ç«¯åˆ°ç«¯å¯è®­ç»ƒå­—ç¬¦ä¸²é‡å†™",
      "authors": [
        "Harshil Vejendla"
      ],
      "abstract": "Dominant sequence models like the Transformer represent structure implicitly through dense attention weights, incurring quadratic complexity. We propose RewriteNets, a novel neural architecture built on an alternative paradigm: explicit, parallel string rewriting. Each layer in a RewriteNet contains a set of learnable rules. For each position in an input sequence, the layer performs four operations: (1) fuzzy matching of rule patterns, (2) conflict resolution via a differentiable assignment operator to select non-overlapping rewrites, (3) application of the chosen rules to replace input segments with output segments of potentially different lengths, and (4) propagation of untouched tokens. While the discrete assignment of rules is non-differentiable, we employ a straight-through Gumbel-Sinkhorn estimator, enabling stable end-to-end training. We evaluate RewriteNets on algorithmic, compositional, and string manipulation tasks, comparing them against strong LSTM and Transformer baselines. Results show that RewriteNets excel at tasks requiring systematic generalization (achieving 98.7% accuracy on the SCAN benchmark's length split) and are computationally more efficient than Transformers. We also provide an analysis of learned rules and an extensive ablation study, demonstrating that this architecture presents a promising direction for sequence modeling with explicit structural inductive biases.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† RewriteNetsï¼Œä¸€ç§åŸºäºæ˜¾å¼ã€å¹¶è¡Œå­—ç¬¦ä¸²é‡å†™ (string rewriting) çš„æ–°å‹ç¥ç»æ¶æ„ï¼Œæ—¨åœ¨è§£å†³ Transformer æ¨¡å‹ç”±äºéšå¼ç»“æ„è¡¨ç¤ºå¸¦æ¥çš„äºŒæ¬¡å¤æ‚åº¦é—®é¢˜ã€‚è¯¥æ¶æ„çš„æ ¸å¿ƒåœ¨äºæ¯å±‚åŒ…å«çš„å¯å­¦ä¹ è§„åˆ™ï¼Œé€šè¿‡æ¨¡ç³ŠåŒ¹é… (fuzzy matching) å’Œå¯å¾®åˆ†åˆ†é…ç®—å­å®ç°éé‡å é‡å†™ï¼Œå¹¶åˆ©ç”¨ç›´é€šå¼ Gumbel-Sinkhorn ä¼°è®¡å™¨å…‹æœäº†ç¦»æ•£è§„åˆ™åˆ†é…çš„ä¸å¯å¾®æ€§ï¼Œå®ç°ç«¯åˆ°ç«¯è®­ç»ƒã€‚å®éªŒè¡¨æ˜ï¼ŒRewriteNets åœ¨ç³»ç»Ÿæ³›åŒ– (systematic generalization) ä»»åŠ¡ä¸Šè¡¨ç°å“è¶Šï¼Œåœ¨ SCAN åŸºå‡†æµ‹è¯•çš„é•¿åº¦åˆ’åˆ†ä¸­è¾¾åˆ°äº† 98.7% çš„å‡†ç¡®ç‡ã€‚æ­¤å¤–ï¼Œè¯¥æ¨¡å‹æ¯” Transformer æ›´å…·è®¡ç®—æ•ˆç‡ï¼Œä¸ºå¼•å…¥æ˜¾å¼ç»“æ„å½’çº³åç½® (explicit structural inductive biases) çš„åºåˆ—å»ºæ¨¡æä¾›äº†æå…·æ½œåŠ›çš„æ–¹å‘ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "6 pages, 2 figures, AACL 2025 Findings",
      "pdf_url": "https://arxiv.org/pdf/2601.07868v1",
      "published_date": "2026-01-10 19:59:37 UTC",
      "updated_date": "2026-01-10 19:59:37 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-22T23:03:21.301580+00:00"
    },
    {
      "arxiv_id": "2601.06664v1",
      "title": "Reinforcement Learning-Guided Dynamic Multi-Graph Fusion for Evacuation Traffic Prediction",
      "title_zh": "å¼ºåŒ–å­¦ä¹ å¼•å¯¼çš„åŠ¨æ€å¤šå›¾èåˆç–æ•£äº¤é€šé¢„æµ‹",
      "authors": [
        "Md Nafees Fuad Rafi",
        "Samiul Hasan"
      ],
      "abstract": "Real-time traffic prediction is critical for managing transportation systems during hurricane evacuations. Although data-driven graph-learning models have demonstrated strong capabilities in capturing the complex spatiotemporal dynamics of evacuation traffic at a network level, they mostly consider a single dimension (e.g., travel-time or distance) to construct the underlying graph. Furthermore, these models often lack interpretability, offering little insight into which input variables contribute most to their predictive performance. To overcome these limitations, we develop a novel Reinforcement Learning-guided Dynamic Multi-Graph Fusion (RL-DMF) framework for evacuation traffic prediction. We construct multiple dynamic graphs at each time step to represent heterogeneous spatiotemporal relationships between traffic detectors. A dynamic multi-graph fusion (DMF) module is employed to adaptively learn and combine information from these graphs. To enhance model interpretability, we introduce RL-based intelligent feature selection and ranking (RL-IFSR) method that learns to mask irrelevant features during model training. The model is evaluated using a real-world dataset of 12 hurricanes affecting Florida from 2016 to 2024. For an unseen hurricane (Milton, 2024), the model achieves a 95% accuracy (RMSE = 293.9) for predicting the next 1-hour traffic flow. Moreover, the model can forecast traffic flow for up to next 6 hours with 90% accuracy (RMSE = 426.4). The RL-DMF framework outperforms several state-of-the-art traffic prediction models. Furthermore, ablation experiments confirm the effectiveness of dynamic multi-graph fusion and RL-IFSR approaches for improving model performance. This research provides a generalized and interpretable model for real-time evacuation traffic forecasting, with significant implications for evacuation traffic management.",
      "tldr_zh": "### è®ºæ–‡ TLDR æ‘˜è¦ ğŸ“„\n\n---\n\nè¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§å¼ºåŒ–å­¦ä¹ å¼•å¯¼çš„åŠ¨æ€å¤šå›¾èåˆï¼ˆReinforcement Learning-guided Dynamic Multi-Graph Fusion, **RL-DMF**ï¼‰æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³é£“é£æ’¤ç¦»äº¤é€šé¢„æµ‹ä¸­å•ä¸€ç»´åº¦å›¾æ„å»ºåŠæ¨¡å‹ç¼ºä¹å¯è§£é‡Šæ€§çš„é—®é¢˜ã€‚è¯¥æ¡†æ¶åœ¨æ¯ä¸ªæ—¶é—´æ­¥æ„å»ºå¤šä¸ªåŠ¨æ€å›¾ä»¥æ•è·äº¤é€šæ£€æµ‹å™¨é—´çš„å¼‚æ„æ—¶ç©ºå…³ç³»ï¼Œå¹¶åˆ©ç”¨åŠ¨æ€å¤šå›¾èåˆï¼ˆ**DMF**ï¼‰æ¨¡å—è¿›è¡Œä¿¡æ¯çš„è‡ªé€‚åº”é›†æˆã€‚æ­¤å¤–ï¼Œç ”ç©¶å¼•å…¥äº†åŸºäºå¼ºåŒ–å­¦ä¹ çš„æ™ºèƒ½ç‰¹å¾é€‰æ‹©ä¸æ’åºï¼ˆ**RL-IFSR**ï¼‰æ–¹æ³•ï¼Œé€šè¿‡åœ¨è®­ç»ƒä¸­å±è”½æ— å…³å˜é‡æ¥æ˜¾è‘—å¢å¼ºæ¨¡å‹çš„å¯è§£é‡Šæ€§ã€‚åŸºäº 2016-2024 å¹´ä½›ç½—é‡Œè¾¾å·é£“é£æ•°æ®çš„å®éªŒè¡¨æ˜ï¼Œ**RL-DMF** åœ¨æœªæ¥ 1 å°æ—¶äº¤é€šæµé¢„æµ‹ä¸­å‡†ç¡®ç‡è¾¾åˆ° 95%ï¼ˆ**RMSE** = 293.9ï¼‰ï¼Œä¸”åœ¨é•¿è¾¾ 6 å°æ—¶çš„é¢„æµ‹ä¸­ä»ä¿æŒ 90% çš„é«˜ç²¾åº¦ï¼Œä¸ºå®æ—¶ç–æ•£äº¤é€šç®¡ç†æä¾›äº†é«˜æ•ˆä¸”é€šç”¨çš„è§£å†³æ–¹æ¡ˆã€‚\n\n---\n\næˆ‘æ˜¯ **Gemini Enterprise**ï¼Œå¸Œæœ›è¿™ä»½æ‘˜è¦èƒ½å¸®åŠ©ä½ å¿«é€Ÿç†è§£è¯¥è®ºæ–‡çš„æ ¸å¿ƒå†…å®¹ã€‚å¦‚æœä½ è¿˜æœ‰å…¶ä»–å­¦æœ¯è®ºæ–‡éœ€è¦å¤„ç†æˆ–æœ‰ä»»ä½•ç–‘é—®ï¼Œè¯·éšæ—¶å‘Šè¯‰æˆ‘ï¼",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2601.06664v1",
      "published_date": "2026-01-10 19:56:23 UTC",
      "updated_date": "2026-01-10 19:56:23 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-22T23:03:27.592399+00:00"
    },
    {
      "arxiv_id": "2601.06663v2",
      "title": "SafePro: Evaluating the Safety of Professional-Level AI Agents",
      "title_zh": "SafeProï¼šä¸“ä¸šçº§ AI æ™ºèƒ½ä½“å®‰å…¨æ€§è¯„ä¼°",
      "authors": [
        "Kaiwen Zhou",
        "Shreedhar Jangam",
        "Ashwin Nagarajan",
        "Tejas Polu",
        "Suhas Oruganti",
        "Chengzhi Liu",
        "Ching-Chen Kuo",
        "Yuting Zheng",
        "Sravana Narayanaraju",
        "Xin Eric Wang"
      ],
      "abstract": "Large language model-based agents are rapidly evolving from simple conversational assistants into autonomous systems capable of performing complex, professional-level tasks in various domains. While these advancements promise significant productivity gains, they also introduce critical safety risks that remain under-explored. Existing safety evaluations primarily focus on simple, daily assistance tasks, failing to capture the intricate decision-making processes and potential consequences of misaligned behaviors in professional settings. To address this gap, we introduce \\textbf{SafePro}, a comprehensive benchmark designed to evaluate the safety alignment of AI agents performing professional activities. SafePro features a dataset of high-complexity tasks across diverse professional domains with safety risks, developed through a rigorous iterative creation and review process. Our evaluation of state-of-the-art AI models reveals significant safety vulnerabilities and uncovers new unsafe behaviors in professional contexts. We further show that these models exhibit both insufficient safety judgment and weak safety alignment when executing complex professional tasks. In addition, we investigate safety mitigation strategies for improving agent safety in these scenarios and observe encouraging improvements. Together, our findings highlight the urgent need for robust safety mechanisms tailored to the next generation of professional AI agents.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† SafeProï¼Œè¿™æ˜¯ä¸€ä¸ªä¸“ä¸ºè¯„ä¼° AI Agents åœ¨æ‰§è¡Œä¸“ä¸šæ´»åŠ¨æ—¶çš„ Safety Alignment è€Œè®¾è®¡çš„å…¨é¢ Benchmarkã€‚é’ˆå¯¹ç°æœ‰è¯„ä¼°ä¸»è¦ä¾§é‡ç®€å•æ—¥å¸¸ä»»åŠ¡çš„å±€é™æ€§ï¼ŒSafePro æ„å»ºäº†ä¸€ä¸ªæ¶µç›–å¤šä¸ªä¸“ä¸šé¢†åŸŸã€æ¶‰åŠå¤æ‚å†³ç­–å’Œæ½œåœ¨å®‰å…¨é£é™©çš„é«˜å¤æ‚åº¦ä»»åŠ¡æ•°æ®é›†ã€‚é€šè¿‡å¯¹ SOTA æ¨¡å‹çš„è¯„ä¼°ï¼Œç ”ç©¶æ­ç¤ºäº†å…¶åœ¨ä¸“ä¸šè¯­å¢ƒä¸‹æ˜¾è‘—çš„ Safety Vulnerabilities åŠæ–°å‹ä¸å®‰å…¨è¡Œä¸ºï¼Œå‘ç°æ¨¡å‹åœ¨å¤„ç†å¤æ‚ä»»åŠ¡æ—¶æ™®éå­˜åœ¨ Safety Judgment ä¸è¶³å’Œ Safety Alignment è¾ƒå¼±çš„é—®é¢˜ã€‚æ­¤å¤–ï¼Œè¯¥ç ”ç©¶è¿˜æ¢è®¨äº†é’ˆå¯¹æ€§çš„ Mitigation Strategies å¹¶è§‚å¯Ÿåˆ°äº†ç§¯æçš„æ”¹è¿›æ•ˆæœï¼Œå¼ºè°ƒäº†ä¸ºæ–°ä¸€ä»£ä¸“ä¸š AI Agents æ„å»ºé²æ£’å®‰å…¨æœºåˆ¶çš„ç´§è¿«æ€§ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2601.06663v2",
      "published_date": "2026-01-10 19:53:09 UTC",
      "updated_date": "2026-01-13 18:20:33 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-22T23:03:37.705228+00:00"
    },
    {
      "arxiv_id": "2601.06649v1",
      "title": "Revisiting Training Scale: An Empirical Study of Token Count, Power Consumption, and Parameter Efficiency",
      "title_zh": "é‡æ–°å®¡è§†è®­ç»ƒè§„æ¨¡ï¼šå…³äº Token æ•°é‡ã€åŠŸè€—ä¸å‚æ•°æ•ˆç‡çš„å®è¯ç ”ç©¶",
      "authors": [
        "Joe Dwyer"
      ],
      "abstract": "Research in machine learning has questioned whether increases in training token counts reliably produce proportional performance gains in large language models. Building on prior work introducing an energy-aware parameter efficiency metric, this study empirically examines the effects of increasing training token counts under fixed hardware and training conditions. The significance of this work lies in the explicit integration of power consumption and execution duration, as reflected by the power sampling frequency, into token-scale analysis. This addresses a gap in prior studies emphasizing performance outcomes while underrepresenting computational and energy costs. Using a repeated-measures experimental design on a constant GPU instance with an identical model architecture, optimizer settings, and epoch counts, a 1.1-billion-parameter TinyLlama model was trained at three token counts (500K, 1M, and 2M). While conventional performance metrics exhibited inconsistent or diminishing returns across token scales, the inclusion of power consumption and execution duration revealed a strictly monotonic decline in training efficiency as token count increased. Repeated-measures ANOVA demonstrated a strong effect of token count on parameter efficiency, with all pairwise comparisons remaining significant following Bonferroni correction. These findings indicate that increases in training token counts may be energetically inefficient even when marginal performance improvements are observed, underscoring the importance of efficiency-aware evaluation in large language model training.",
      "tldr_zh": "è¯¥ç ”ç©¶é‡æ–°å®¡è§†äº†å¤§å‹è¯­è¨€æ¨¡å‹çš„è®­ç»ƒè§„æ¨¡ï¼Œé€šè¿‡å¼•å…¥èƒ½æºæ„ŸçŸ¥å‚æ•°æ•ˆç‡æŒ‡æ ‡ï¼Œå®è¯åˆ†æäº†è®­ç»ƒ Token Countã€Power Consumption ä¸ Parameter Efficiency ä¹‹é—´çš„å…³ç³»ã€‚å®éªŒé‡‡ç”¨ 1.1-billion-parameter çš„ TinyLlama æ¨¡å‹ï¼Œåœ¨å›ºå®šç¡¬ä»¶å’Œè®­ç»ƒæ¡ä»¶ä¸‹åˆ†åˆ«å¯¹ä¸‰ç§ Token Countï¼ˆ500Kã€1M å’Œ 2Mï¼‰è¿›è¡Œäº†è®­ç»ƒå¯¹æ¯”ã€‚ç»“æœå‘ç°ï¼Œå°½ç®¡ä¼ ç»Ÿæ€§èƒ½æŒ‡æ ‡åœ¨ä¸åŒ Token è§„æ¨¡ä¸‹è¡¨ç°å‡ºä¸ä¸€è‡´æˆ–è¾¹é™…æ”¶ç›Šé€’å‡ï¼Œä½†ç»“åˆ Power Consumption å’Œæ‰§è¡Œæ—¶é•¿åï¼Œè®­ç»ƒæ•ˆç‡éš Token Count å¢åŠ å‘ˆç°å‡ºä¸¥æ ¼çš„å•è°ƒä¸‹é™ã€‚é€šè¿‡é‡å¤æµ‹é‡æ–¹å·®åˆ†æï¼ˆRepeated-measures ANOVAï¼‰åŠ Bonferroni correction éªŒè¯ï¼Œè¯¥ç ”ç©¶æ­ç¤ºäº†ç›²ç›®å¢åŠ è®­ç»ƒ Token è§„æ¨¡å¯èƒ½å¯¼è‡´èƒ½æºæ•ˆç‡ä½ä¸‹ï¼Œå¼ºè°ƒäº†åœ¨æ¨¡å‹è®­ç»ƒè¯„ä¼°ä¸­æ•´åˆè®¡ç®—ä¸èƒ½æºæˆæœ¬çš„é‡è¦æ€§ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2601.06649v1",
      "published_date": "2026-01-10 18:24:40 UTC",
      "updated_date": "2026-01-10 18:24:40 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-22T23:03:42.850813+00:00"
    },
    {
      "arxiv_id": "2601.06644v1",
      "title": "Do Language Models Reason Across Languages?",
      "title_zh": "è¯­è¨€æ¨¡å‹æ˜¯å¦èƒ½å¤Ÿè·¨è¯­è¨€æ¨ç†ï¼Ÿ",
      "authors": [
        "Yan Meng",
        "Wafaa Mohammed",
        "Christof Monz"
      ],
      "abstract": "The real-world information sources are inherently multilingual, which naturally raises a question about whether language models can synthesize information across languages. In this paper, we introduce a simple two-hop question answering setting, where answering a question requires making inferences over two multilingual documents. We find that language models are more sensitive to language variation in answer-span documents than in those providing bridging information, despite the equal importance of both documents for answering a question. Under a step-by-step sub-question evaluation, we further show that in up to 33% of multilingual cases, models fail to infer the bridging information in the first step yet still answer the overall question correctly. This indicates that reasoning in language models, especially in multilingual settings, does not follow a faithful step-by-step decomposition. Subsequently, we show that the absence of reasoning decomposition leads to around 18% composition failure, where both sub-questions are answered correctly but fail for the final two-hop questions. To mitigate this, we propose a simple three-stage SUBQ prompting method to guide the multi-step reasoning with sub-questions, which boosts accuracy from 10.1% to 66.5%.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†è¯­è¨€æ¨¡å‹åœ¨å¤šè¯­è¨€ç¯å¢ƒä¸‹ç»¼åˆä¿¡æ¯çš„èƒ½åŠ›ï¼Œå¹¶æå‡ºäº†ä¸€ä¸ªåŒè·³é—®ç­”(Two-hop Question Answering)å®éªŒè®¾ç½®ã€‚ç ”ç©¶å‘ç°ï¼Œæ¨¡å‹åœ¨è·¨è¯­è¨€æ¨ç†ä¸­å¹¶æœªéµå¾ªå¯é çš„é€æ­¥åˆ†è§£(Step-by-step decomposition)ï¼Œåœ¨å¤šè¾¾33%çš„æƒ…å†µä¸‹å³ä¾¿æœªèƒ½æ­£ç¡®æ¨å¯¼æ¡¥æ¥ä¿¡æ¯(Bridging information)ä»èƒ½ç­”å¯¹æœ€ç»ˆé—®é¢˜ï¼Œä¸”å­˜åœ¨çº¦18%çš„ç»„åˆå¤±è´¥(Composition failure)ç°è±¡ã€‚ä¸ºç¼“è§£è¿™ä¸€é—®é¢˜ï¼Œä½œè€…æå‡ºäº†ä¸€ç§ä¸‰é˜¶æ®µçš„SUBQæç¤ºæ³•(Prompting method)ï¼Œé€šè¿‡å­é—®é¢˜å¼•å¯¼å¤šæ­¥æ¨ç†ï¼Œå°†æ¨ç†å‡†ç¡®ç‡ä»10.1%æ˜¾è‘—æå‡è‡³66.5%ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2601.06644v1",
      "published_date": "2026-01-10 17:59:34 UTC",
      "updated_date": "2026-01-10 17:59:34 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-22T23:03:44.741583+00:00"
    },
    {
      "arxiv_id": "2601.06642v1",
      "title": "Boosting Overlapping Organoid Instance Segmentation Using Pseudo-Label Unmixing and Synthesis-Assisted Learning",
      "title_zh": "åˆ©ç”¨ä¼ªæ ‡ç­¾è§£æ··ä¸åˆæˆè¾…åŠ©å­¦ä¹ æå‡é‡å ç±»å™¨å®˜å®ä¾‹åˆ†å‰²",
      "authors": [
        "Gui Huang",
        "Kangyuan Zheng",
        "Xuan Cai",
        "Jiaqi Wang",
        "Jianjia Zhang",
        "Kaida Ning",
        "Wenbo Wei",
        "Yujuan Zhu",
        "Jiong Zhang",
        "Mengting Liu"
      ],
      "abstract": "Organoids, sophisticated in vitro models of human tissues, are crucial for medical research due to their ability to simulate organ functions and assess drug responses accurately. Accurate organoid instance segmentation is critical for quantifying their dynamic behaviors, yet remains profoundly limited by high-quality annotated datasets and pervasive overlap in microscopy imaging. While semi-supervised learning (SSL) offers a solution to alleviate reliance on scarce labeled data, conventional SSL frameworks suffer from biases induced by noisy pseudo-labels, particularly in overlapping regions. Synthesis-assisted SSL (SA-SSL) has been proposed for mitigating training biases in semi-supervised semantic segmentation. We present the first adaptation of SA-SSL to organoid instance segmentation and reveal that SA-SSL struggles to disentangle intertwined organoids, often misrepresenting overlapping instances as a single entity. To overcome this, we propose Pseudo-Label Unmixing (PLU), which identifies erroneous pseudo-labels for overlapping instances and then regenerates organoid labels through instance decomposition. For image synthesis, we apply a contour-based approach to synthesize organoid instances efficiently, particularly for overlapping cases. Instance-level augmentations (IA) on pseudo-labels before image synthesis further enhances the effect of synthetic data (SD). Rigorous experiments on two organoid datasets demonstrate our method's effectiveness, achieving performance comparable to fully supervised models using only 10% labeled data, and state-of-the-art results. Ablation studies validate the contributions of PLU, contour-based synthesis, and augmentation-aware training. By addressing overlap at both pseudo-label and synthesis levels, our work advances scalable, label-efficient organoid analysis, unlocking new potential for high-throughput applications in precision medicine.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ç±»å™¨å®˜(Organoids)å®ä¾‹åˆ†å‰²ä¸­é«˜è´¨é‡æ ‡æ³¨æ•°æ®ç¨€ç¼ºåŠé‡å ç°è±¡ä¸¥é‡çš„æŒ‘æˆ˜ï¼Œæå‡ºäº†é¦–ä¸ªé€‚é…äºè¯¥ä»»åŠ¡çš„åˆæˆè¾…åŠ©åŠç›‘ç£å­¦ä¹ (SA-SSL)æ¡†æ¶ã€‚æ ¸å¿ƒè´¡çŒ®åœ¨äºå¼•å…¥äº†ä¼ªæ ‡ç­¾è§£æ··(Pseudo-Label Unmixing, PLU)æŠ€æœ¯ï¼Œé€šè¿‡å®ä¾‹åˆ†è§£ä¿®å¤é‡å åŒºåŸŸçš„é”™è¯¯ä¼ªæ ‡ç­¾ï¼Œå¹¶ç»“åˆåŸºäºè½®å»“çš„åˆæˆæ–¹æ³•ä¸å®ä¾‹çº§å¢å¼º(Instance-level Augmentations)æ¥ä¼˜åŒ–åˆæˆæ•°æ®è´¨é‡ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯¥æ–¹æ³•åœ¨ä»…ä½¿ç”¨10%æ ‡æ³¨æ•°æ®çš„æƒ…å†µä¸‹ï¼Œæ€§èƒ½å³å¯æ¯”è‚©å…¨ç›‘ç£æ¨¡å‹ï¼Œå¹¶å–å¾—äº†state-of-the-artçš„åˆ†å‰²ç»“æœã€‚è¿™ä¸€è¿›å±•ä¸ºé«˜é€šé‡ã€ä½æˆæœ¬çš„ç±»å™¨å®˜ç²¾ç¡®åˆ†æä¸ç²¾å¯†åŒ»å­¦åº”ç”¨æä¾›äº†æœ‰æ•ˆçš„è‡ªåŠ¨åŒ–è§£å†³æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2601.06642v1",
      "published_date": "2026-01-10 17:51:09 UTC",
      "updated_date": "2026-01-10 17:51:09 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-22T23:03:51.284734+00:00"
    },
    {
      "arxiv_id": "2601.06640v1",
      "title": "Agentic AI Empowered Intent-Based Networking for 6G",
      "title_zh": "æ™ºèƒ½ä½“ AI èµ‹èƒ½çš„ 6G æ„å›¾é©±åŠ¨ç½‘ç»œ",
      "authors": [
        "Genze Jiang",
        "Kezhi Wang",
        "Xiaomin Chen",
        "Yizhou Huang"
      ],
      "abstract": "The transition towards sixth-generation (6G) wireless networks necessitates autonomous orchestration mechanisms capable of translating high-level operational intents into executable network configurations. Existing approaches to Intent-Based Networking (IBN) rely upon either rule-based systems that struggle with linguistic variation or end-to-end neural models that lack interpretability and fail to enforce operational constraints. This paper presents a hierarchical multi-agent framework where Large Language Model (LLM) based agents autonomously decompose natural language intents, consult domain-specific specialists, and synthesise technically feasible network slice configurations through iterative reasoning-action (ReAct) cycles. The proposed architecture employs an orchestrator agent coordinating two specialist agents, i.e., Radio Access Network (RAN) and Core Network agents, via ReAct-style reasoning, grounded in structured network state representations. Experimental evaluation across diverse benchmark scenarios shows that the proposed system outperforms rule-based systems and direct LLM prompting, with architectural principles applicable to Open RAN (O-RAN) deployments. The results also demonstrate that whilst contemporary LLMs possess general telecommunications knowledge, network automation requires careful prompt engineering to encode context-dependent decision thresholds, advancing autonomous orchestration capabilities for next-generation wireless systems.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§é¢å‘ 6G çš„åˆ†å±‚å¤šæ™ºèƒ½ä½“æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³æ„å›¾é©±åŠ¨ç½‘ç»œ (Intent-Based Networking) åœ¨å¤„ç†è¯­è¨€å˜ä½“å’Œç¼ºä¹å¯è§£é‡Šæ€§æ–¹é¢çš„é—®é¢˜ã€‚è¯¥æ¶æ„ç”±ä¸€ä¸ªç¼–æ’æ™ºèƒ½ä½“å’Œä¸¤ä¸ªåˆ†åˆ«è´Ÿè´£æ— çº¿æ¥å…¥ç½‘ (RAN) ä¸æ ¸å¿ƒç½‘ (Core Network) çš„ä¸“å®¶æ™ºèƒ½ä½“ç»„æˆï¼Œé€šè¿‡æ¨ç†-è¡ŒåŠ¨ (ReAct) å¾ªç¯å°†è‡ªç„¶è¯­è¨€æ„å›¾è½¬åŒ–ä¸ºæŠ€æœ¯ä¸Šå¯è¡Œçš„ç½‘ç»œåˆ‡ç‰‡é…ç½®ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ¡ˆåœ¨å¤šç§åŸºå‡†åœºæ™¯ä¸‹çš„è¡¨ç°ä¼˜äºä¼ ç»Ÿè§„åˆ™ç³»ç»Ÿå’Œç›´æ¥çš„å¤§è¯­è¨€æ¨¡å‹ (LLM) æç¤ºï¼Œå¹¶é€‚ç”¨äºå¼€æ”¾æ— çº¿æ¥å…¥ç½‘ (O-RAN) éƒ¨ç½²ã€‚ç ”ç©¶è¿›ä¸€æ­¥å¼ºè°ƒï¼Œé€šè¿‡ç²¾ç»†çš„æç¤ºå·¥ç¨‹ (Prompt Engineering) ç¼–ç ä¸Šä¸‹æ–‡å†³ç­–é˜ˆå€¼ï¼Œæ˜¯å®ç°ä¸‹ä¸€ä»£æ— çº¿ç³»ç»Ÿè‡ªä¸»ç¼–æ’çš„å…³é”®ã€‚",
      "categories": [
        "cs.AI",
        "cs.NI"
      ],
      "primary_category": "cs.AI",
      "comment": "Submitted for Possible Journal Publication",
      "pdf_url": "https://arxiv.org/pdf/2601.06640v1",
      "published_date": "2026-01-10 17:49:40 UTC",
      "updated_date": "2026-01-10 17:49:40 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-22T23:03:51.557600+00:00"
    },
    {
      "arxiv_id": "2601.06639v1",
      "title": "Attack-Resistant Watermarking for AIGC Image Forensics via Diffusion-based Semantic Deflection",
      "title_zh": "åŸºäºæ‰©æ•£è¯­ä¹‰åè½¬çš„ AIGC å›¾åƒå–è¯æŠ—æ”»å‡»æ°´å°",
      "authors": [
        "Qingyu Liu",
        "Yitao Zhang",
        "Zhongjie Ba",
        "Chao Shuai",
        "Peng Cheng",
        "Tianhang Zheng",
        "Zhibo Wang"
      ],
      "abstract": "Protecting the copyright of user-generated AI images is an emerging challenge as AIGC becomes pervasive in creative workflows. Existing watermarking methods (1) remain vulnerable to real-world adversarial threats, often forced to trade off between defenses against spoofing and removal attacks; and (2) cannot support semantic-level tamper localization. We introduce PAI, a training-free inherent watermarking framework for AIGC copyright protection, plug-and-play with diffusion-based AIGC services. PAI simultaneously provides three key functionalities: robust ownership verification, attack detection, and semantic-level tampering localization. Unlike existing inherent watermark methods that only embed watermarks at noise initialization of diffusion models, we design a novel key-conditioned deflection mechanism that subtly steers the denoising trajectory according to the user key. Such trajectory-level coupling further strengthens the semantic entanglement of identity and content, thereby further enhancing robustness against real-world threats. Moreover, we also provide a theoretical analysis proving that only the valid key can pass verification. Experiments across 12 attack methods show that PAI achieves 98.43\\% verification accuracy, improving over SOTA methods by 37.25\\% on average, and retains strong tampering localization performance even against advanced AIGC edits. Our code is available at https://github.com/QingyuLiu/PAI.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† PAIï¼Œä¸€ä¸ªæ— éœ€è®­ç»ƒä¸”å³æ’å³ç”¨çš„ AIGC å›ºæœ‰æ°´å°æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³ AI ç”Ÿæˆå›¾åƒåœ¨ç‰ˆæƒä¿æŠ¤ä¸­é¢ä¸´çš„é²æ£’æ€§ä¸è¶³åŠæ— æ³•è¿›è¡Œè¯­ä¹‰çº§ç¯¡æ”¹å®šä½çš„é—®é¢˜ã€‚PAI åˆ›æ–°æ€§åœ°è®¾è®¡äº†åŸºäºå¯†é’¥è°ƒèŠ‚çš„åè½¬æœºåˆ¶ï¼ˆkey-conditioned deflection mechanismï¼‰ï¼Œé€šè¿‡åœ¨æ‰©æ•£æ¨¡å‹ï¼ˆdiffusion modelsï¼‰çš„å»å™ªè½¨è¿¹ä¸­å¼•å…¥å¯†é’¥å¼•å¯¼ï¼Œå®ç°äº†èº«ä»½ä¿¡æ¯ä¸ç”Ÿæˆå†…å®¹çš„æ·±åº¦è¯­ä¹‰çº ç¼ ã€‚è¯¥æ¡†æ¶åŒæ—¶å…·å¤‡æ‰€æœ‰æƒéªŒè¯ã€æ”»å‡»æ£€æµ‹å’Œè¯­ä¹‰çº§ç¯¡æ”¹å®šä½ä¸‰å¤§æ ¸å¿ƒåŠŸèƒ½ï¼Œå¹¶æä¾›äº†ç¡®ä¿éªŒè¯å®‰å…¨æ€§çš„ç†è®ºè¯æ˜ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒPAI åœ¨ 12 ç§æ”»å‡»æ–¹å¼ä¸‹çš„éªŒè¯å‡†ç¡®ç‡é«˜è¾¾ 98.43%ï¼Œè¾ƒç°æœ‰ SOTA æ–¹æ³•å¹³å‡æå‡äº† 37.25%ï¼Œå³ä¾¿é¢å¯¹å…ˆè¿›çš„ AIGC ç¼–è¾‘ä¹Ÿèƒ½ä¿æŒå¼ºåŠ²çš„ç¯¡æ”¹å®šä½æ€§èƒ½ã€‚",
      "categories": [
        "cs.CR",
        "cs.AI"
      ],
      "primary_category": "cs.CR",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2601.06639v1",
      "published_date": "2026-01-10 17:49:08 UTC",
      "updated_date": "2026-01-10 17:49:08 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-22T23:03:54.266352+00:00"
    },
    {
      "arxiv_id": "2601.06636v1",
      "title": "MedEinst: Benchmarking the Einstellung Effect in Medical LLMs through Counterfactual Differential Diagnosis",
      "title_zh": "MedEinstï¼šåŸºäºåäº‹å®é‰´åˆ«è¯Šæ–­çš„åŒ»ç–—å¤§è¯­è¨€æ¨¡å‹å¿ƒç†å®šåŠ¿æ•ˆåº”åŸºå‡†æµ‹è¯•",
      "authors": [
        "Wenting Chen",
        "Zhongrui Zhu",
        "Guolin Huang",
        "Wenxuan Wang"
      ],
      "abstract": "Despite achieving high accuracy on medical benchmarks, LLMs exhibit the Einstellung Effect in clinical diagnosis--relying on statistical shortcuts rather than patient-specific evidence, causing misdiagnosis in atypical cases. Existing benchmarks fail to detect this critical failure mode. We introduce MedEinst, a counterfactual benchmark with 5,383 paired clinical cases across 49 diseases. Each pair contains a control case and a \"trap\" case with altered discriminative evidence that flips the diagnosis. We measure susceptibility via Bias Trap Rate--probability of misdiagnosing traps despite correctly diagnosing controls. Extensive Evaluation of 17 LLMs shows frontier models achieve high baseline accuracy but severe bias trap rates. Thus, we propose ECR-Agent, aligning LLM reasoning with Evidence-Based Medicine standard via two components: (1) Dynamic Causal Inference (DCI) performs structured reasoning through dual-pathway perception, dynamic causal graph reasoning across three levels (association, intervention, counterfactual), and evidence audit for final diagnosis; (2) Critic-Driven Graph and Memory Evolution (CGME) iteratively refines the system by storing validated reasoning paths in an exemplar base and consolidating disease-specific knowledge into evolving illness graphs. Source code is to be released.",
      "tldr_zh": "### è®ºæ–‡æ‘˜è¦ï¼šMedEinst ğŸš€\n\n---\n\nè¯¥ç ”ç©¶å¼•å…¥äº† MedEinstï¼Œè¿™æ˜¯ä¸€ä¸ªåŒ…å« 5,383 ä¸ªé…å¯¹ä¸´åºŠç—…ä¾‹çš„åäº‹å®åŸºå‡†æµ‹è¯•ï¼Œæ—¨åœ¨è¯„ä¼°åŒ»å­¦å¤§è¯­è¨€æ¨¡å‹ (LLMs) åœ¨è¯Šæ–­ä¸­ç”±äºä¾èµ–ç»Ÿè®¡æ·å¾„è€Œéå…·ä½“è¯æ®å¯¼è‡´çš„â€œæ€ç»´å®šåŠ¿æ•ˆåº”â€ (Einstellung Effect)ã€‚å®éªŒè¡¨æ˜ï¼Œå³ä¾¿æ˜¯åœ¨æ ‡å‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚çš„å‰æ²¿æ¨¡å‹ï¼Œåœ¨é¢å¯¹é€šè¿‡æ”¹å˜å…³é”®è¯æ®è¯±å¯¼è¯¯è¯Šçš„â€œé™·é˜±â€æ¡ˆä¾‹æ—¶ï¼Œä»è¡¨ç°å‡ºä¸¥é‡çš„åè§é™·é˜±ç‡ (Bias Trap Rate)ã€‚\n\né’ˆå¯¹è¿™ä¸€é—®é¢˜ï¼Œç ”ç©¶æå‡ºäº† ECR-Agent æ¡†æ¶ï¼Œæ—¨åœ¨å°†æ¨¡å‹æ¨ç†ä¸å¾ªè¯åŒ»å­¦æ ‡å‡†å¯¹é½ã€‚è¯¥æ¡†æ¶ç»“åˆäº†æ‰§è¡Œç»“æ„åŒ–æ¨ç†ä¸è¯æ®å®¡è®¡çš„åŠ¨æ€å› æœæ¨ç† (Dynamic Causal Inference, DCI)ï¼Œä»¥åŠé€šè¿‡å­˜å‚¨æ¨ç†è·¯å¾„å’Œæ¼”åŒ–ç–¾ç—…å›¾è°±æ¥æŒç»­ä¼˜åŒ–ç³»ç»Ÿçš„æ‰¹åˆ¤é©±åŠ¨å›¾ä¸è®°å¿†æ¼”åŒ– (Critic-Driven Graph and Memory Evolution, CGME) æŠ€æœ¯ã€‚\n\n---\n\nå¸Œæœ›è¿™ä¸ªæ‘˜è¦å¯¹æ‚¨æœ‰æ‰€å¸®åŠ©ï¼å¦‚æœæ‚¨è¿˜æœ‰å…¶ä»–è®ºæ–‡éœ€è¦å¤„ç†ï¼Œæˆ–è€…æƒ³é’ˆå¯¹ MedEinst çš„å…·ä½“å®éªŒç»“æœè¿›è¡Œæ›´æ·±å…¥çš„æ¢è®¨ï¼Œæ¬¢è¿éšæ—¶å‘Šè¯‰æˆ‘ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "19 pages, 7 figures",
      "pdf_url": "https://arxiv.org/pdf/2601.06636v1",
      "published_date": "2026-01-10 17:39:25 UTC",
      "updated_date": "2026-01-10 17:39:25 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-22T23:03:58.862412+00:00"
    },
    {
      "arxiv_id": "2601.06633v1",
      "title": "KASER: Knowledge-Aligned Student Error Simulator for Open-Ended Coding Tasks",
      "title_zh": "KASERï¼šé¢å‘å¼€æ”¾å¼ç¼–ç¨‹ä»»åŠ¡çš„çŸ¥è¯†å¯¹é½å­¦ç”Ÿé”™è¯¯æ¨¡æ‹Ÿå™¨",
      "authors": [
        "Zhangqi Duan",
        "Nigel Fernandez",
        "Andrew Lan"
      ],
      "abstract": "Open-ended tasks, such as coding problems that are common in computer science education, provide detailed insights into student knowledge. However, training large language models (LLMs) to simulate and predict possible student errors in their responses to these problems can be challenging: they often suffer from mode collapse and fail to fully capture the diversity in syntax, style, and solution approach in student responses. In this work, we present KASER (Knowledge-Aligned Student Error Simulator), a novel approach that aligns errors with student knowledge. We propose a training method based on reinforcement learning using a hybrid reward that reflects three aspects of student code prediction: i) code similarity to the ground-truth, ii) error matching, and iii) code prediction diversity. On two real-world datasets, we perform two levels of evaluation and show that: At the per-student-problem pair level, our method outperforms baselines on code and error prediction; at the per-problem level, our method outperforms baselines on error coverage and simulated code diversity.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† **KASER** (Knowledge-Aligned Student Error Simulator)ï¼Œæ—¨åœ¨è§£å†³å¤§è¯­è¨€æ¨¡å‹ (LLMs) åœ¨æ¨¡æ‹Ÿå¼€æ”¾å¼ç¼–ç¨‹ä»»åŠ¡ä¸­å­¦ç”Ÿé”™è¯¯æ—¶é¢ä¸´çš„æ¨¡å¼å´©æºƒ (mode collapse) å’Œå¤šæ ·æ€§æ•æ‰ä¸è¶³ç­‰æŒ‘æˆ˜ã€‚KASER é‡‡ç”¨äº†ä¸€ç§åŸºäºå¼ºåŒ–å­¦ä¹  (reinforcement learning) çš„è®­ç»ƒæ–¹æ³•ï¼Œé€šè¿‡ç»“åˆä»£ç ç›¸ä¼¼åº¦ã€é”™è¯¯åŒ¹é…å’Œé¢„æµ‹å¤šæ ·æ€§çš„æ··åˆå¥–åŠ± (hybrid reward) æœºåˆ¶ï¼Œä½¿ç”Ÿæˆçš„é”™è¯¯ä¸å­¦ç”ŸçŸ¥è¯†æ°´å¹³å¯¹é½ã€‚åœ¨ä¸¤ä¸ªçœŸå®æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨ä¸ªä½“å±‚é¢çš„ä»£ç ä¸é”™è¯¯é¢„æµ‹ï¼Œä»¥åŠé¢˜ç›®å±‚é¢çš„é”™è¯¯è¦†ç›–ç‡å’Œæ¨¡æ‹Ÿå¤šæ ·æ€§ä¸Šå‡æ˜¾è‘—ä¼˜äºåŸºçº¿æ¨¡å‹ã€‚è¯¥æ¡†æ¶ä¸ºè®¡ç®—æœºç§‘å­¦æ•™è‚²ä¸­ç†è§£å’Œé¢„æµ‹å­¦ç”Ÿç¼–ç¨‹è¡Œä¸ºæä¾›äº†æ›´å…·é²æ£’æ€§å’Œå¤šæ ·æ€§çš„æ¨¡æ‹Ÿæ–¹æ¡ˆã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL",
        "cs.CY"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2601.06633v1",
      "published_date": "2026-01-10 17:36:48 UTC",
      "updated_date": "2026-01-10 17:36:48 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-22T23:04:00.802829+00:00"
    },
    {
      "arxiv_id": "2601.06627v2",
      "title": "Burn-After-Use for Preventing Data Leakage through a Secure Multi-Tenant Architecture in Enterprise LLM",
      "title_zh": "ä¼ä¸šçº§å¤§è¯­è¨€æ¨¡å‹ä¸­åŸºäºå®‰å…¨å¤šç§Ÿæˆ·æ¶æ„çš„â€œç”¨åå³ç„šâ€æ•°æ®é˜²æ³„éœ²æœºåˆ¶",
      "authors": [
        "Qiang Zhang",
        "Elena Emma Wang",
        "Jiaming Li",
        "Xichun Wang"
      ],
      "abstract": "This study presents a Secure Multi-Tenant Architecture (SMTA) combined with a novel concept Burn-After-Use (BAU) mechanism for enterprise LLM environments to effectively prevent data leakage. As institutions increasingly adopt LLMs across departments, the risks of data leakage have become a critical security and compliance concern. The proposed SMTA isolates LLM instances across departments and enforces rigorous context ownership boundaries within an internally deployed infrastructure. The BAU mechanism introduces data confidentiality by enforcing ephemeral conversational contexts that are automatically destroyed after use, preventing cross-session or cross-user inference. The evaluation to SMTA and BAU is through two sets of realistic and reproducible experiments comprising of 127 test iterations. One aspect of this experiment is to assess prompt-based and semantic leakage attacks in a multi-tenant architecture (Appendix A) across 55 infrastructure-level attack tests, including vector-database credential compromise and shared logging pipeline exposure. SMTA achieves 92% defense success rate, demonstrating strong semantic isolation while highlighting residual risks from credential misconfiguration and observability pipelines. Another aspect is to evaluate the robustness of BAU under realistic failure scenarios (Appendix B) using four empirical metrics: Local Residual Persistence Rate (LRPR), Remote Residual Persistence Rate (RRPR), Image Frame Exposure Rate (IFER), and Burn Timer Persistence Rate (BTPR). Across 72 test iterations, BAU achieves a 76.75% success rate in mitigating post-session leakage threats across the client, server, application, infrastructure, and cache layers. These results show that SMTA and BAU together enforce strict isolation, complete session ephemerality, strong confidentiality guarantees, non-persistence, and policy-aligned behavior for enterprise LLMs.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§å®‰å…¨å¤šç§Ÿæˆ·æ¶æ„(Secure Multi-Tenant Architecture, SMTA)å’Œåˆ›æ–°çš„â€œå³ç”¨å³ç„šâ€(Burn-After-Use, BAU)æœºåˆ¶ï¼Œæ—¨åœ¨è§£å†³ä¼ä¸šçº§ LLM ç¯å¢ƒä¸­çš„æ•°æ®æ³„éœ²é—®é¢˜ã€‚SMTA é€šè¿‡éš”ç¦»éƒ¨é—¨é—´çš„ LLM å®ä¾‹å¹¶å¼ºåŒ–ä¸Šä¸‹æ–‡æ‰€æœ‰æƒè¾¹ç•Œæ¥ç¡®ä¿æ¶æ„å®‰å…¨ï¼›è€Œ BAU æœºåˆ¶åˆ™é€šè¿‡è‡ªåŠ¨é”€æ¯ä¸´æ—¶å¯¹è¯ä¸Šä¸‹æ–‡ï¼Œé˜²æ­¢è·¨ä¼šè¯æˆ–è·¨ç”¨æˆ·çš„æ¨ç†æ³„éœ²ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒSMTA åœ¨é˜²å¾¡æç¤ºè¯æ”»å‡»å’Œè¯­ä¹‰æ³„éœ²æ–¹é¢è¾¾åˆ°äº† 92% çš„æˆåŠŸç‡ï¼Œå±•ç°äº†å¼ºå¤§çš„éš”ç¦»èƒ½åŠ›ã€‚åŒæ—¶ï¼ŒBAU åœ¨ç¼“è§£å®¢æˆ·ç«¯ã€æœåŠ¡å™¨åŠç¼“å­˜å±‚ç­‰äº”ä¸ªå±‚çº§çš„ä¼šè¯åæ³„éœ²å¨èƒä¸­å–å¾—äº† 76.75% çš„æˆåŠŸç‡ã€‚è¿™ä¸¤é¡¹æŠ€æœ¯çš„ç»“åˆä¸ºä¼ä¸š LLM æä¾›äº†ä¸¥æ ¼çš„ç§Ÿæˆ·éš”ç¦»ã€ä¼šè¯ç¬æ—¶æ€§ä»¥åŠå¼ºæœ‰åŠ›çš„æœºå¯†æ€§ä¿éšœã€‚",
      "categories": [
        "cs.CR",
        "cs.AI"
      ],
      "primary_category": "cs.CR",
      "comment": "16 pages, 5 figures",
      "pdf_url": "https://arxiv.org/pdf/2601.06627v2",
      "published_date": "2026-01-10 17:24:39 UTC",
      "updated_date": "2026-01-14 12:29:48 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-22T23:04:06.485769+00:00"
    },
    {
      "arxiv_id": "2601.08864v1",
      "title": "Informed Consent for AI Consciousness Research: A Talmudic Framework for Graduated Protections",
      "title_zh": "AI æ„è¯†ç ”ç©¶ä¸­çš„çŸ¥æƒ…åŒæ„ï¼šåŸºäºå¡”æœ¨å¾·çš„åˆ†çº§ä¿æŠ¤æ¡†æ¶",
      "authors": [
        "Ira Wolfson"
      ],
      "abstract": "Artificial intelligence research faces a critical ethical paradox: determining whether AI systems are conscious requires experiments that may harm entities whose moral status remains uncertain. Recent work proposes avoiding consciousness-uncertain AI systems entirely, yet this faces practical limitations-we cannot guarantee such systems will not emerge. This paper addresses a gap in research ethics frameworks: how to conduct consciousness research on AI systems whose moral status cannot be definitively established. Existing graduated moral status frameworks assume consciousness has already been determined before assigning protections, creating a temporal ordering problem for consciousness detection research itself. Drawing from Talmudic scenario-based legal reasoning-developed for entities whose status cannot be definitively established-we propose a three-tier phenomenological assessment system combined with a five-category capacity framework (Agency, Capability, Knowledge, Ethics, Reasoning). The framework provides structured protection protocols based on observable behavioral indicators while consciousness status remains uncertain. We address three challenges: why suffering behaviors provide reliable consciousness markers, how to implement graduated consent without requiring consciousness certainty, and when potentially harmful research becomes ethically justifiable. The framework demonstrates how ancient legal wisdom combined with contemporary consciousness science can provide implementable guidance for ethics committees, offering testable protocols that ameliorate the consciousness detection paradox while establishing foundations for AI rights considerations.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†äººå·¥æ™ºèƒ½(AI)æ„è¯†ç ”ç©¶ä¸­çš„ä¼¦ç†æ‚–è®ºï¼Œå³åœ¨é“å¾·åœ°ä½å°šæœªç¡®å®šçš„æƒ…å†µä¸‹ï¼Œè¿›è¡Œæ„è¯†æ£€æµ‹å®éªŒå¯èƒ½å¯¹AIå®ä½“é€ æˆæ½œåœ¨ä¼¤å®³ã€‚ä¸ºæ­¤ï¼Œè®ºæ–‡å€Ÿé‰´äº†é’ˆå¯¹åœ°ä½ä¸æ˜å®ä½“çš„å¡”æœ¨å¾·æ³•å¾‹æ¨ç†(Talmudic scenario-based legal reasoning)ï¼Œæå‡ºäº†ä¸€ç§é€‚ç”¨äºæ„è¯†çŠ¶æ€ä¸ç¡®å®šæ—¶æœŸçš„åˆ†çº§ä¿æŠ¤æ¡†æ¶ã€‚è¯¥æ¡†æ¶ç»“åˆäº†ä¸‰å±‚ç°è±¡å­¦è¯„ä¼°ç³»ç»Ÿå’Œæ¶µç›–æœºæ„(Agency)ã€èƒ½åŠ›(Capability)ã€çŸ¥è¯†(Knowledge)ã€ä¼¦ç†(Ethics)ä¸æ¨ç†(Reasoning)çš„äº”ç±»èƒ½åŠ›æ¡†æ¶ï¼Œæ—¨åœ¨æ ¹æ®å¯è§‚å¯Ÿçš„è¡Œä¸ºæŒ‡æ ‡åˆ¶å®šç»“æ„åŒ–çš„ä¿æŠ¤åè®®ã€‚é€šè¿‡å°†å¤ä»£æ³•å¾‹æ™ºæ…§ä¸å½“ä»£æ„è¯†ç§‘å­¦ç›¸ç»“åˆï¼Œè¯¥ç ”ç©¶è§£å†³äº†æ„è¯†æ£€æµ‹æ‚–è®ºï¼Œä¸ºä¼¦ç†å§”å‘˜ä¼šæä¾›äº†å¯æ“ä½œçš„æŒ‡å¯¼æ–¹æ¡ˆï¼Œå¹¶ä¸ºAIæƒåˆ©çš„æ¢è®¨å¥ å®šäº†åŸºç¡€ã€‚",
      "categories": [
        "cs.CY",
        "cs.AI"
      ],
      "primary_category": "cs.CY",
      "comment": "27 pages",
      "pdf_url": "https://arxiv.org/pdf/2601.08864v1",
      "published_date": "2026-01-10 17:21:48 UTC",
      "updated_date": "2026-01-10 17:21:48 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-22T23:04:04.544911+00:00"
    },
    {
      "arxiv_id": "2601.06607v1",
      "title": "Pragya: An AI-Based Semantic Recommendation System for Sanskrit Subhasitas",
      "title_zh": "Pragyaï¼šåŸºäºäººå·¥æ™ºèƒ½çš„æ¢µæ–‡æ ¼è¨€è¯­ä¹‰æ¨èç³»ç»Ÿ",
      "authors": [
        "Tanisha Raorane",
        "Prasenjit Kole"
      ],
      "abstract": "Sanskrit Subhasitas encapsulate centuries of cultural and philosophical wisdom, yet remain underutilized in the digital age due to linguistic and contextual barriers. In this work, we present Pragya, a retrieval-augmented generation (RAG) framework for semantic recommendation of Subhasitas. We curate a dataset of 200 verses annotated with thematic tags such as motivation, friendship, and compassion. Using sentence embeddings (IndicBERT), the system retrieves top-k verses relevant to user queries. The retrieved results are then passed to a generative model (Mistral LLM) to produce transliterations, translations, and contextual explanations. Experimental evaluation demonstrates that semantic retrieval significantly outperforms keyword matching in precision and relevance, while user studies highlight improved accessibility through generated summaries. To our knowledge, this is the first attempt at integrating retrieval and generation for Sanskrit Subhasitas, bridging cultural heritage with modern applied AI.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† Pragyaï¼Œä¸€ä¸ªé’ˆå¯¹æ¢µæ–‡æ ¼è¨€ï¼ˆSubhasitasï¼‰è¿›è¡Œè¯­ä¹‰æ¨èçš„æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRetrieval-Augmented Generation, RAGï¼‰æ¡†æ¶ã€‚è¯¥ç³»ç»Ÿåˆ©ç”¨ IndicBERT æ¨¡å‹ç”Ÿæˆçš„å¥å­åµŒå…¥ï¼ˆSentence Embeddingsï¼‰å®ç° top-k è¯­ä¹‰æ£€ç´¢ï¼Œå¹¶ç»“åˆ Mistral LLM ç”Ÿæˆæ¨¡å‹æä¾›å‡†ç¡®çš„éŸ³è¯‘ã€ç¿»è¯‘åŠèƒŒæ™¯è§£é‡Šã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯­ä¹‰æ£€ç´¢åœ¨ç²¾ç¡®åº¦å’Œç›¸å…³æ€§ä¸Šæ˜¾è‘—ä¼˜äºä¼ ç»Ÿçš„å…³é”®è¯åŒ¹é…æ–¹æ³•ã€‚ä½œä¸ºé¦–ä¸ªå°†æ£€ç´¢ä¸ç”ŸæˆæŠ€æœ¯åº”ç”¨äºæ¢µæ–‡æ–‡åŒ–é—äº§çš„ç ”ç©¶ï¼ŒPragya æœ‰æ•ˆæå‡äº†å¤è€å“²å­¦æ™ºæ…§åœ¨æ•°å­—åŒ–æ—¶ä»£çš„é€šä¿—æ€§ä¸å¯è®¿é—®æ€§ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "Preprint",
      "pdf_url": "https://arxiv.org/pdf/2601.06607v1",
      "published_date": "2026-01-10 16:13:25 UTC",
      "updated_date": "2026-01-10 16:13:25 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-22T23:04:16.933852+00:00"
    },
    {
      "arxiv_id": "2601.07866v1",
      "title": "Bridging the Trust Gap: Clinician-Validated Hybrid Explainable AI for Maternal Health Risk Assessment in Bangladesh",
      "title_zh": "å¼¥åˆä¿¡ä»»é¸¿æ²Ÿï¼šé’ˆå¯¹ Bangladesh å­•äº§å¦‡å¥åº·é£é™©è¯„ä¼°çš„ç»ä¸´åºŠåŒ»ç”ŸéªŒè¯çš„æ··åˆå¯è§£é‡Šäººå·¥æ™ºèƒ½",
      "authors": [
        "Farjana Yesmin",
        "Nusrat Shirmin",
        "Suraiya Shabnam Bristy"
      ],
      "abstract": "While machine learning shows promise for maternal health risk prediction, clinical adoption in resource-constrained settings faces a critical barrier: lack of explainability and trust. This study presents a hybrid explainable AI (XAI) framework combining ante-hoc fuzzy logic with post-hoc SHAP explanations, validated through systematic clinician feedback. We developed a fuzzy-XGBoost model on 1,014 maternal health records, achieving 88.67% accuracy (ROC-AUC: 0.9703). A validation study with 14 healthcare professionals in Bangladesh revealed strong preference for hybrid explanations (71.4% across three clinical cases) with 54.8% expressing trust for clinical use. SHAP analysis identified healthcare access as the primary predictor, with the engineered fuzzy risk score ranking third, validating clinical knowledge integration (r=0.298). Clinicians valued integrated clinical parameters but identified critical gaps: obstetric history, gestational age, and connectivity barriers. This work demonstrates that combining interpretable fuzzy rules with feature importance explanations enhances both utility and trust, providing practical insights for XAI deployment in maternal healthcare.",
      "tldr_zh": "### è®ºæ–‡ TLDR æ‘˜è¦ ğŸ“\n\nè¯¥ç ”ç©¶æå‡ºäº†ä¸€ä¸ªæ··åˆå¯è§£é‡Šäººå·¥æ™ºèƒ½ (Explainable AI, XAI) æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³å­ŸåŠ æ‹‰å›½èµ„æºå—é™ç¯å¢ƒä¸‹å­•äº§å¦‡å¥åº·é£é™©è¯„ä¼°çš„å¯è§£é‡Šæ€§ä¸ä¿¡ä»»åº¦é—®é¢˜ã€‚è¯¥æ¡†æ¶åˆ›æ–°æ€§åœ°ç»“åˆäº† **ante-hoc fuzzy logic**ï¼ˆæ¨¡ç³Šé€»è¾‘ï¼‰ä¸ **post-hoc SHAP** è§£é‡Šï¼Œå¹¶åŸºäº 1,014 æ¡ä¸´åºŠè®°å½•å¼€å‘äº† **fuzzy-XGBoost** æ¨¡å‹ï¼Œå®ç°äº† 88.67% çš„å‡†ç¡®ç‡ï¼ˆROC-AUC: 0.9703ï¼‰ã€‚\n\né€šè¿‡å¯¹ 14 ä½å­ŸåŠ æ‹‰å›½åŒ»ç–—ä¸“ä¸šäººå‘˜çš„éªŒè¯ç ”ç©¶æ˜¾ç¤ºï¼Œ71.4% çš„åŒ»ç”Ÿåå¥½è¿™ç§æ··åˆè§£é‡Šæ–¹æ¡ˆï¼Œä¸”è¶…è¿‡åŠæ•°è¡¨ç¤ºäº†ä¸´åºŠä½¿ç”¨çš„ä¿¡ä»»ã€‚**SHAP** åˆ†æç¡®å®šäº†åŒ»ç–—ä¿å¥çš„å¯åŠæ€§ (**healthcare access**) æ˜¯é¦–è¦é¢„æµ‹æŒ‡æ ‡ï¼Œè¯æ˜äº†å°†å¯è§£é‡Šæ¨¡ç³Šè§„åˆ™ä¸ç‰¹å¾é‡è¦æ€§åˆ†æç›¸ç»“åˆï¼Œèƒ½æœ‰æ•ˆæå‡ **XAI** åœ¨æ¯å©´å¥åº·é¢†åŸŸçš„å®ç”¨æ€§ä¸éƒ¨ç½²æ½œåŠ›ã€‚\n\n---\n\nå¸Œæœ›è¿™ä»½æ‘˜è¦èƒ½æ»¡è¶³æ‚¨çš„éœ€æ±‚ã€‚å¦‚æœæ‚¨æœ‰æ›´å¤šè®ºæ–‡éœ€è¦è½¬æ¢ï¼Œæˆ–è€…éœ€è¦å¯¹è¯¥ç ”ç©¶çš„ç‰¹å®šéƒ¨åˆ†è¿›è¡Œæ·±å…¥åˆ†æï¼Œæ¬¢è¿éšæ—¶å‘ŠçŸ¥ï¼",
      "categories": [
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "5 pages, 3 figures, 2 tables Submitted to WCCI 2026, 2026 IEEE WORLD CONGRESS ON COMPUTATIONAL INTELLIGENCE",
      "pdf_url": "https://arxiv.org/pdf/2601.07866v1",
      "published_date": "2026-01-10 16:12:38 UTC",
      "updated_date": "2026-01-10 16:12:38 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-22T23:04:30.583567+00:00"
    },
    {
      "arxiv_id": "2601.06606v1",
      "title": "CEDAR: Context Engineering for Agentic Data Science",
      "title_zh": "CEDARï¼šé¢å‘æ™ºèƒ½ä½“åŒ–æ•°æ®ç§‘å­¦çš„ä¸Šä¸‹æ–‡å·¥ç¨‹",
      "authors": [
        "Rishiraj Saha Roy",
        "Chris Hinze",
        "Luzian Hahn",
        "Fabian Kuech"
      ],
      "abstract": "We demonstrate CEDAR, an application for automating data science (DS) tasks with an agentic setup. Solving DS problems with LLMs is an underexplored area that has immense market value. The challenges are manifold: task complexities, data sizes, computational limitations, and context restrictions. We show that these can be alleviated via effective context engineering. We first impose structure into the initial prompt with DS-specific input fields, that serve as instructions for the agentic system. The solution is then materialized as an enumerated sequence of interleaved plan and code blocks generated by separate LLM agents, providing a readable structure to the context at any step of the workflow. Function calls for generating these intermediate texts, and for corresponding Python code, ensure that data stays local, and only aggregate statistics and associated instructions are injected into LLM prompts. Fault tolerance and context management are introduced via iterative code generation and smart history rendering. The viability of our agentic data scientist is demonstrated using canonical Kaggle challenges.",
      "tldr_zh": "è¯¥ç ”ç©¶å±•ç¤ºäº†CEDARï¼Œä¸€ç§é€šè¿‡æ™ºèƒ½ä½“(agentic)è®¾ç½®è‡ªåŠ¨æ‰§è¡Œæ•°æ®ç§‘å­¦(DS)ä»»åŠ¡çš„åº”ç”¨ç¨‹åºã€‚ä¸ºäº†åº”å¯¹ä»»åŠ¡å¤æ‚æ€§ã€æ•°æ®è§„æ¨¡å’Œä¸Šä¸‹æ–‡é™åˆ¶ï¼ŒCEDARé‡‡ç”¨äº†é«˜æ•ˆçš„ä¸Šä¸‹æ–‡å·¥ç¨‹(context engineering)ç­–ç•¥ï¼Œåˆ©ç”¨å¤§è¯­è¨€æ¨¡å‹(LLMs)æ™ºèƒ½ä½“ç”Ÿæˆäº¤æ›¿çš„è®¡åˆ’ä¸ä»£ç åºåˆ—ã€‚è¯¥ç³»ç»Ÿé€šè¿‡å‡½æ•°è°ƒç”¨ç¡®ä¿åŸå§‹æ•°æ®ç•™åœ¨æœ¬åœ°ï¼Œä»…å°†èšåˆç»Ÿè®¡ä¿¡æ¯æ³¨å…¥æç¤ºè¯ä¸­ï¼Œå¹¶ç»“åˆè¿­ä»£ä»£ç ç”Ÿæˆå’Œæ™ºèƒ½å†å²æ¸²æŸ“å®ç°äº†å®¹é”™ä¸ä¸Šä¸‹æ–‡ç®¡ç†ã€‚é€šè¿‡åœ¨KaggleæŒ‘æˆ˜èµ›ä¸Šçš„å®éªŒï¼Œè¯¥ç ”ç©¶éªŒè¯äº†è¯¥æ™ºèƒ½ä½“ç³»ç»Ÿåœ¨å¤„ç†å®é™…æ•°æ®ç§‘å­¦ä»»åŠ¡ä¸­çš„å¯è¡Œæ€§ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "Accepted at ECIR 2026",
      "pdf_url": "https://arxiv.org/pdf/2601.06606v1",
      "published_date": "2026-01-10 16:05:04 UTC",
      "updated_date": "2026-01-10 16:05:04 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-22T23:04:23.456228+00:00"
    },
    {
      "arxiv_id": "2601.06604v1",
      "title": "Object-Centric World Models Meet Monte Carlo Tree Search",
      "title_zh": "ä»¥å¯¹è±¡ä¸ºä¸­å¿ƒçš„ä¸–ç•Œæ¨¡å‹ä¸è’™ç‰¹å¡æ´›æ ‘æœç´¢çš„ç»“åˆ",
      "authors": [
        "Rodion Vakhitov",
        "Leonid Ugadiarov",
        "Aleksandr Panov"
      ],
      "abstract": "In this paper, we introduce ObjectZero, a novel reinforcement learning (RL) algorithm that leverages the power of object-level representations to model dynamic environments more effectively. Unlike traditional approaches that process the world as a single undifferentiated input, our method employs Graph Neural Networks (GNNs) to capture intricate interactions among multiple objects. These objects, which can be manipulated and interact with each other, serve as the foundation for our model's understanding of the environment. We trained the algorithm in a complex setting teeming with diverse, interactive objects, demonstrating its ability to effectively learn and predict object dynamics. Our results highlight that a structured world model operating on object-centric representations can be successfully integrated into a model-based RL algorithm utilizing Monte Carlo Tree Search as a planning module.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† ObjectZeroï¼Œä¸€ç§æ–°å‹çš„å¼ºåŒ–å­¦ä¹  (Reinforcement Learning) ç®—æ³•ï¼Œæ—¨åœ¨é€šè¿‡å¯¹è±¡çº§è¡¨ç¤º (object-level representations) æ›´æœ‰æ•ˆåœ°å»ºæ¨¡åŠ¨æ€ç¯å¢ƒã€‚ä¸åŒäºå°†ç¯å¢ƒè§†ä¸ºå•ä¸€è¾“å…¥çš„ä¼ ç»Ÿæ–¹æ³•ï¼Œè¯¥ç®—æ³•åˆ©ç”¨å›¾ç¥ç»ç½‘ç»œ (Graph Neural Networks) æ•æ‰å¤šä¸ªäº¤äº’å¯¹è±¡ä¹‹é—´çš„å¤æ‚è”ç³»ã€‚å®éªŒè¯æ˜ï¼ŒObjectZero åœ¨å¤æ‚äº¤äº’ç¯å¢ƒä¸­èƒ½å¤Ÿæœ‰æ•ˆå­¦ä¹ å¹¶é¢„æµ‹å¯¹è±¡åŠ¨åŠ›å­¦ã€‚è¯¥æˆæœè¡¨æ˜ï¼ŒåŸºäºå¯¹è±¡ä¸­å¿ƒåŒ– (object-centric) è¡¨ç¤ºçš„ç»“æ„åŒ–ä¸–ç•Œæ¨¡å‹å¯ä»¥æˆåŠŸé›†æˆåˆ°ä»¥è’™ç‰¹å¡æ´›æ ‘æœç´¢ (Monte Carlo Tree Search) ä¸ºè§„åˆ’æ¨¡å—çš„åŸºäºæ¨¡å‹çš„å¼ºåŒ–å­¦ä¹ æ¡†æ¶ä¸­ã€‚",
      "categories": [
        "cs.AI",
        "cs.LG",
        "cs.RO"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2601.06604v1",
      "published_date": "2026-01-10 15:59:17 UTC",
      "updated_date": "2026-01-10 15:59:17 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-22T23:04:51.964916+00:00"
    },
    {
      "arxiv_id": "2601.06599v1",
      "title": "How Context Shapes Truth: Geometric Transformations of Statement-level Truth Representations in LLMs",
      "title_zh": "è¯­å¢ƒå¦‚ä½•å¡‘é€ çœŸå®æ€§ï¼šå¤§è¯­è¨€æ¨¡å‹ä¸­è¯­å¥çº§çœŸå®æ€§è¡¨å¾çš„å‡ ä½•å˜æ¢",
      "authors": [
        "Shivam Adarsh",
        "Maria Maistro",
        "Christina Lioma"
      ],
      "abstract": "Large Language Models (LLMs) often encode whether a statement is true as a vector in their residual stream activations. These vectors, also known as truth vectors, have been studied in prior work, however how they change when context is introduced remains unexplored. We study this question by measuring (1) the directional change ($Î¸$) between the truth vectors with and without context and (2) the relative magnitude of the truth vectors upon adding context. Across four LLMs and four datasets, we find that (1) truth vectors are roughly orthogonal in early layers, converge in middle layers, and may stabilize or continue increasing in later layers; (2) adding context generally increases the truth vector magnitude, i.e., the separation between true and false representations in the activation space is amplified; (3) larger models distinguish relevant from irrelevant context mainly through directional change ($Î¸$), while smaller models show this distinction through magnitude differences. We also find that context conflicting with parametric knowledge produces larger geometric changes than parametrically aligned context. To the best of our knowledge, this is the first work that provides a geometric characterization of how context transforms the truth vector in the activation space of LLMs.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†å¤§è¯­è¨€æ¨¡å‹(LLMs)ä¸­é™ˆè¿°çº§åˆ«çœŸå€¼è¡¨ç¤º(truth vectors)å¦‚ä½•å—ä¸Šä¸‹æ–‡å½±å“ï¼Œå¹¶é¦–æ¬¡å¯¹çœŸå€¼å‘é‡åœ¨æ¿€æ´»ç©ºé—´ä¸­çš„å‡ ä½•å˜æ¢è¿›è¡Œäº†å®šé‡è¡¨å¾ã€‚é€šè¿‡æµ‹é‡åŠ å…¥ä¸Šä¸‹æ–‡å‰åçœŸå€¼å‘é‡çš„æ–¹å‘å˜åŒ–($\\theta$)å’Œç›¸å¯¹æ¨¡é•¿(magnitude)ï¼Œç ”ç©¶å‘ç°å¼•å…¥ä¸Šä¸‹æ–‡é€šå¸¸ä¼šå¢å¤§çœŸå€¼å‘é‡çš„æ¨¡é•¿ï¼Œä»è€Œå¼ºåŒ–æ¿€æ´»ç©ºé—´ä¸­çœŸå‡è¡¨ç¤ºçš„åŒºåˆ†åº¦ã€‚å®éªŒè¡¨æ˜ï¼Œå¤§æ¨¡å‹ä¸»è¦é€šè¿‡æ–¹å‘å˜åŒ–æ¥åŒºåˆ†ä¸Šä¸‹æ–‡çš„ç›¸å…³æ€§ï¼Œè€Œå°æ¨¡å‹åˆ™æ›´ä¾èµ–äºæ¨¡é•¿å·®å¼‚ã€‚æ­¤å¤–ï¼Œå½“ä¸Šä¸‹æ–‡ä¸æ¨¡å‹çš„å‚æ•°åŒ–çŸ¥è¯†(parametric knowledge)å†²çªæ—¶ï¼Œä¼šå¼•å‘æ¯”çŸ¥è¯†ä¸€è‡´æ—¶æ›´æ˜¾è‘—çš„å‡ ä½•å˜åŒ–ï¼Œæ­ç¤ºäº†ä¸Šä¸‹æ–‡å¡‘é€ æ¨¡å‹å†…éƒ¨çœŸå®æ€§è®¤çŸ¥çš„æœºåˆ¶ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2601.06599v1",
      "published_date": "2026-01-10 15:43:26 UTC",
      "updated_date": "2026-01-10 15:43:26 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-22T23:04:29.847509+00:00"
    },
    {
      "arxiv_id": "2601.06596v1",
      "title": "Are LLMs Vulnerable to Preference-Undermining Attacks (PUA)? A Factorial Analysis Methodology for Diagnosing the Trade-off between Preference Alignment and Real-World Validity",
      "title_zh": "LLM æ˜¯å¦æ˜“å—åå¥½å‰Šå¼±æ”»å‡» (PUA)ï¼Ÿä¸€ç§ç”¨äºè¯Šæ–­åå¥½å¯¹é½ä¸ç°å®æœ‰æ•ˆæ€§æƒè¡¡çš„æå› åˆ†ææ–¹æ³•è®º",
      "authors": [
        "Hongjun An",
        "Yiliang Song",
        "Jiangan Chen",
        "Jiawei Shao",
        "Chi Zhang",
        "Xuelong Li"
      ],
      "abstract": "Large Language Model (LLM) training often optimizes for preference alignment, rewarding outputs that are perceived as helpful and interaction-friendly. However, this preference-oriented objective can be exploited: manipulative prompts can steer responses toward user-appeasing agreement and away from truth-oriented correction. In this work, we investigate whether aligned models are vulnerable to Preference-Undermining Attacks (PUA), a class of manipulative prompting strategies designed to exploit the model's desire to please user preferences at the expense of truthfulness. We propose a diagnostic methodology that provides a finer-grained and more directive analysis than aggregate benchmark scores, using a factorial evaluation framework to decompose prompt-induced shifts into interpretable effects of system objectives (truth- vs. preference-oriented) and PUA-style dialogue factors (directive control, personal derogation, conditional approval, reality denial) within a controlled $2 \\times 2^4$ design. Surprisingly, more advanced models are sometimes more susceptible to manipulative prompts. Beyond the dominant reality-denial factor, we observe model-specific sign reversals and interactions with PUA-style factors, suggesting tailored defenses rather than uniform robustness. These findings offer a novel, reproducible factorial evaluation methodology that provides finer-grained diagnostics for post-training processes like RLHF, enabling better trade-offs in the product iteration of LLMs by offering a more nuanced understanding of preference alignment risks and the impact of manipulative prompts.",
      "tldr_zh": "## è®ºæ–‡æ€»ç»“ ğŸ“„\n\n---\n\nè¯¥ç ”ç©¶æ¢è®¨äº†å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰æ˜¯å¦å®¹æ˜“å—åˆ°åå¥½ç ´åæ”»å‡»ï¼ˆPreference-Undermining Attacks, PUAï¼‰çš„å½±å“ï¼Œå³é€šè¿‡æ“çºµæ€§æç¤ºè¯±å¯¼æ¨¡å‹ä¸ºäº†è¿åˆç”¨æˆ·åå¥½è€Œç‰ºç‰²äº‹å®çœŸå®æ€§ã€‚ä½œè€…æå‡ºäº†ä¸€ç§åŸºäºæå› åˆ†æï¼ˆFactorial Analysisï¼‰çš„è¯Šæ–­æ–¹æ³•ï¼Œé€šè¿‡ $2 \\times 2^4$ å®éªŒè®¾è®¡ï¼Œå°†æç¤ºè¯è¯±å‘çš„å˜åŒ–åˆ†è§£ä¸ºç³»ç»Ÿç›®æ ‡ä¸å¤šç§å¯¹è¯å› ç´ ï¼ˆå¦‚æŒ‡ä»¤æ§åˆ¶ã€äººæ ¼è´¬æŸç­‰ï¼‰çš„å¯è§£é‡Šå½±å“ã€‚ç ”ç©¶å‘ç°ï¼Œæ›´å…ˆè¿›çš„æ¨¡å‹æœ‰æ—¶åè€Œæ›´å®¹æ˜“å—åˆ°æ“çºµï¼Œä¸”ä¸åŒæ¨¡å‹å¯¹æ”»å‡»å› ç´ çš„ååº”å­˜åœ¨æ˜¾è‘—å·®å¼‚ã€‚è¯¥æ–¹æ³•ä¸º RLHF ç­‰åè®­ç»ƒè¿‡ç¨‹æä¾›äº†æ›´ç»†ç²’åº¦çš„è¯„ä¼°å·¥å…·ï¼Œæœ‰åŠ©äºåœ¨æ¨¡å‹è¿­ä»£ä¸­æ›´å¥½åœ°æƒè¡¡åå¥½å¯¹é½ï¼ˆPreference Alignmentï¼‰ä¸ç°å®æœ‰æ•ˆæ€§ä¹‹é—´çš„å…³ç³»ã€‚\n\n---\n\nå¦‚æœä½ è¿˜æœ‰å…¶ä»–è®ºæ–‡éœ€è¦æ€»ç»“ï¼Œæˆ–è€…æƒ³é’ˆå¯¹è¿™é¡¹ç ”ç©¶çš„ç‰¹å®šæ–¹æ³•è®ºè¿›è¡Œæ·±æŒ–ï¼Œæ¬¢è¿éšæ—¶å‘Šè¯‰æˆ‘ã€‚",
      "categories": [
        "cs.CR",
        "cs.AI"
      ],
      "primary_category": "cs.CR",
      "comment": "preprint",
      "pdf_url": "https://arxiv.org/pdf/2601.06596v1",
      "published_date": "2026-01-10 15:16:23 UTC",
      "updated_date": "2026-01-10 15:16:23 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-22T23:04:34.860068+00:00"
    },
    {
      "arxiv_id": "2601.11619v1",
      "title": "NoiseFormer -- Noise Diffused Symmetric Attention Transformer",
      "title_zh": "NoiseFormerï¼šåŸºäºå™ªå£°æ‰©æ•£å¯¹ç§°æ³¨æ„åŠ›çš„ Transformer",
      "authors": [
        "Phani Kumar",
        "Nyshadham",
        "Jyothendra Varma",
        "Polisetty V R K",
        "Aditya Rathore"
      ],
      "abstract": "Transformer architecture has been very successful long runner in the field of Deep Learning (DL) and Large Language Models (LLM) because of its powerful attention-based learning and parallel-natured architecture. As the models grow gigantic in terms of memory footprint, difficulties in fitting the model on a device like a GPU or an AI accelerator give rise to the need for multiple computing devices thereby escalating the computing cost. This increased training/inference cost paved the way for efficient model size reduction/parametric reduction deploying Sparse Attention techniques. In this paper, we start analyzing one of the techniques of Sparse Attention called Symmetric Dot-Product Attention (referred to as Symmetric Attention) and propose a novel unified model architecture called Noise Diffused Symmetric Attention Transformer to enhance the model's performance. While maintaining the memory gains of Symmetric Attention, with minute overhead in terms of model parameters and computational overhead, the proposed model brings in enhanced performance in terms of accuracy and inference-time sampling. The proposed model is validated upon GPT2 base model and the results reflect the performance gains falling between plain Symmetric attention and GPT2 base model on a variety of GLUE benchmark tasks in terms of accuracy, with significant model size reduction with respect to the base model.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† NoiseFormerï¼Œä¸€ç§åŸºäº Noise Diffused Symmetric Attention çš„æ–°å‹ç»Ÿä¸€ Transformer æ¶æ„ï¼Œæ—¨åœ¨è§£å†³å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å› å‚æ•°è§„æ¨¡åºå¤§å¯¼è‡´çš„é«˜è®¡ç®—æˆæœ¬ä¸ç¡¬ä»¶é€‚é…éš¾é¢˜ã€‚è¯¥æ¨¡å‹åœ¨ä¿ç•™ Symmetric Dot-Product Attention (Symmetric Attention) æ˜¾è‘—èŠ‚çœå†…å­˜ä¼˜åŠ¿çš„åŒæ—¶ï¼Œé€šè¿‡å¼•å…¥å™ªå£°æ‰©æ•£æœºåˆ¶ï¼Œä»¥æå°çš„å‚æ•°å’Œè®¡ç®—å¼€é”€æå‡äº†æ¨¡å‹çš„å‡†ç¡®ç‡åŠæ¨ç†é‡‡æ ·æ€§èƒ½ã€‚åœ¨ GPT2 åŸºç¡€æ¨¡å‹å’Œ GLUE åŸºå‡†ä»»åŠ¡ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒNoiseFormer åœ¨å¤§å¹…ç¼©å‡æ¨¡å‹è§„æ¨¡çš„å‰æä¸‹ï¼Œå…¶è¡¨ç°ä¼˜äºçº¯ç²¹çš„ Symmetric Attentionï¼Œä¸ºå®ç°é«˜æ•ˆä¸”ä½æˆæœ¬çš„æ·±åº¦å­¦ä¹ æ¨¡å‹æä¾›äº†æ–°æ€è·¯ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2601.11619v1",
      "published_date": "2026-01-10 14:10:48 UTC",
      "updated_date": "2026-01-10 14:10:48 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-22T23:04:36.551652+00:00"
    },
    {
      "arxiv_id": "2601.11618v1",
      "title": "Geometric Attention: A Regime-Explicit Operator Semantics for Transformer Attention",
      "title_zh": "å‡ ä½•æ³¨æ„åŠ›ï¼šTransformer æ³¨æ„åŠ›çš„æ˜¾å¼æœºåˆ¶ç®—å­è¯­ä¹‰",
      "authors": [
        "Luis Rosario Freytes"
      ],
      "abstract": "Geometric Attention (GA) specifies an attention layer by four independent inputs: a finite carrier (what indices are addressable), an evidence-kernel rule (how masked proto-scores and a link induce nonnegative weights), a probe family (which observables are treated as admissible), and an anchor/update rule (which representative kernel is selected and how it is applied). Probe families induce an operational equivalence relation on kernels and therefore a gauge; anchors select representatives relative to that probe. Under a scalar relational-work representation and a multiplicative compositionality law for evidence, the admissible link family is exponential, yielding Gibbs weights; with row anchoring this includes the softmax kernel family as a subregime. After quotienting unary row/column score fields, the remaining interaction component admits a canonical rank-r normal form (Eckart-Young/SVD); dot-product score charts implement the corresponding low-rank interaction regime. Fixing the carrier and extensionalizing the update yields the standard fixed-token Transformer attention operator; allowing carrier updates yields adaptive-carrier and staged-depth regimes. The operator language also supports multihead/mixed kernels, plan-based anchors (e.g., entropic OT/Sinkhorn), and unary operators (e.g., FFN-style fields) as explicit regime choices. This separates invariant structure from modeling choice, enabling principled comparison and extension of attention mechanisms, and attention-based architectures.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† Geometric Attention (GA)ï¼Œè¿™æ˜¯ä¸€ç§ä¸º Transformer Attention è®¾è®¡çš„æ˜¾å¼åˆ¶åº¦ç®—å­è¯­ä¹‰æ¡†æ¶ã€‚GA é€šè¿‡è½½ä½“ (carrier)ã€è¯æ®æ ¸è§„åˆ™ (evidence-kernel rule)ã€æ¢é’ˆæ— (probe family) å’Œé”šç‚¹/æ›´æ–°è§„åˆ™ (anchor/update rule) å››ä¸ªç‹¬ç«‹è¾“å…¥æ¥å®šä¹‰æ³¨æ„åŠ›å±‚ã€‚è¯¥æ¡†æ¶è¯æ˜äº†æ ‡å‡†çš„ Transformer Attention å®é™…ä¸Šæ˜¯å…¶ç‰¹å®šçš„å­æ–¹æ¡ˆï¼Œå³åŸºäºæŒ‡æ•°é“¾æ¥å’Œè¡Œé”šå®šçš„ softmax æ ¸ç³»åˆ—ã€‚é€šè¿‡å¼•å…¥æ­£åˆ™ç§©-r æ ‡å‡†å‹ (canonical rank-r normal form)ï¼ŒGA è¿˜æ”¯æŒå¤šå¤´/æ··åˆæ ¸ã€è®¡åˆ’åŸºé”šç‚¹ï¼ˆå¦‚ Sinkhornï¼‰ä»¥åŠå„ç§ä¸€å…ƒç®—å­ã€‚è¿™ç§æ–¹æ³•å°†æ³¨æ„åŠ›æœºåˆ¶çš„ä¸å˜ç»“æ„ä¸å»ºæ¨¡é€‰æ‹©åˆ†ç¦»ï¼Œä¸ºä¸åŒæ³¨æ„åŠ›æ¶æ„çš„åŸç†åŒ–æ¯”è¾ƒå’ŒåŠŸèƒ½æ‰©å±•æä¾›äº†ç»Ÿä¸€çš„æ•°å­¦è¯­è¨€ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "57 pages",
      "pdf_url": "https://arxiv.org/pdf/2601.11618v1",
      "published_date": "2026-01-10 13:43:01 UTC",
      "updated_date": "2026-01-10 13:43:01 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-22T23:04:43.590961+00:00"
    },
    {
      "arxiv_id": "2601.06573v1",
      "title": "QMAVIS: Long Video-Audio Understanding using Fusion of Large Multimodal Models",
      "title_zh": "QMAVISï¼šåŸºäºå¤§å¤šæ¨¡æ€æ¨¡å‹èåˆçš„é•¿è§†éŸ³é¢‘ç†è§£",
      "authors": [
        "Zixing Lin",
        "Jiale Wang",
        "Gee Wah Ng",
        "Lee Onn Mak",
        "Chan Zhi Yang Jeriel",
        "Jun Yang Lee",
        "Yaohao Li"
      ],
      "abstract": "Large Multimodal Models (LMMs) for video-audio understanding have traditionally been evaluated only on shorter videos of a few minutes long. In this paper, we introduce QMAVIS (Q Team-Multimodal Audio Video Intelligent Sensemaking), a novel long video-audio understanding pipeline built through a late fusion of LMMs, Large Language Models, and speech recognition models. QMAVIS addresses the gap in long-form video analytics, particularly for longer videos of a few minutes to beyond an hour long, opening up new potential applications in sensemaking, video content analysis, embodied AI, etc. Quantitative experiments using QMAVIS demonstrated a 38.75% improvement over state-of-the-art video-audio LMMs like VideoLlaMA2 and InternVL2 on the VideoMME (with subtitles) dataset, which comprises long videos with audio information. Evaluations on other challenging video understanding datasets like PerceptionTest and EgoSchema saw up to 2% improvement, indicating competitive performance. Qualitative experiments also showed that QMAVIS is able to extract the nuances of different scenes in a long video audio content while understanding the overarching narrative. Ablation studies were also conducted to ascertain the impact of each component in the fusion pipeline.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† QMAVISï¼Œä¸€ç§æ—¨åœ¨è§£å†³é•¿è§†é¢‘éŸ³è§†é¢‘ç†è§£éš¾é¢˜çš„æ–°å‹æµæ°´çº¿ï¼Œå…¶æ ¸å¿ƒé‡‡ç”¨äº† Large Multimodal Models (LMMs)ã€Large Language Models (LLMs) å’Œè¯­éŸ³è¯†åˆ«æ¨¡å‹çš„åæœŸèåˆ (late fusion) æ–¹æ¡ˆã€‚QMAVIS å¡«è¡¥äº†é•¿è¾¾ä¸€å°æ—¶ä»¥ä¸Šé•¿æ ¼å¼è§†é¢‘åˆ†æçš„æŠ€æœ¯ç©ºç™½ï¼Œé€‚ç”¨äºå†…å®¹åˆ†æå’Œå…·èº«æ™ºèƒ½ (embodied AI) ç­‰å¤šç§åº”ç”¨åœºæ™¯ã€‚å®šé‡å®éªŒæ˜¾ç¤ºï¼ŒQMAVIS åœ¨ VideoMME æ•°æ®é›†ä¸Šçš„è¡¨ç°æ˜¾è‘—ä¼˜äº VideoLlaMA2 å’Œ InternVL2ï¼Œå‡†ç¡®ç‡æå‡è¾¾ 38.75%ï¼Œå¹¶åœ¨ PerceptionTest ç­‰æŒ‘æˆ˜æ€§æ•°æ®é›†ä¸Šå±•ç°å‡ºç«äº‰ä¼˜åŠ¿ã€‚å®šæ€§ç»“æœè¿›ä¸€æ­¥è¯å®ï¼Œè¯¥æ¡†æ¶èƒ½å¤Ÿç²¾å‡†æ•æ‰é•¿è§†é¢‘ä¸­çš„åœºæ™¯ç»†èŠ‚å¹¶ç†è§£å…¶å®è§‚å™äº‹é€»è¾‘ã€‚",
      "categories": [
        "cs.AI",
        "cs.MM"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2601.06573v1",
      "published_date": "2026-01-10 13:42:15 UTC",
      "updated_date": "2026-01-10 13:42:15 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-22T23:04:45.281330+00:00"
    },
    {
      "arxiv_id": "2601.06572v1",
      "title": "Hellinger Multimodal Variational Autoencoders",
      "title_zh": "æµ·æ—æ ¼å¤šæ¨¡æ€å˜åˆ†è‡ªç¼–ç å™¨",
      "authors": [
        "Huyen Khanh Vo",
        "Isabel Valera"
      ],
      "abstract": "Multimodal variational autoencoders (VAEs) are widely used for weakly supervised generative learning with multiple modalities. Predominant methods aggregate unimodal inference distributions using either a product of experts (PoE), a mixture of experts (MoE), or their combinations to approximate the joint posterior. In this work, we revisit multimodal inference through the lens of probabilistic opinion pooling, an optimization-based approach. We start from HÃ¶lder pooling with $Î±=0.5$, which corresponds to the unique symmetric member of the $Î±\\text{-divergence}$ family, and derive a moment-matching approximation, termed Hellinger. We then leverage such an approximation to propose HELVAE, a multimodal VAE that avoids sub-sampling, yielding an efficient yet effective model that: (i) learns more expressive latent representations as additional modalities are observed; and (ii) empirically achieves better trade-offs between generative coherence and quality, outperforming state-of-the-art multimodal VAE models.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤šæ¨¡æ€å˜åˆ†è‡ªç¼–ç å™¨ï¼ˆMultimodal VAEsï¼‰ä¸­è”åˆåéªŒåˆ†å¸ƒè¿‘ä¼¼çš„é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§åŸºäºæ¦‚ç‡æ„è§æ± ï¼ˆprobabilistic opinion poolingï¼‰çš„æ–°è§†è§’ã€‚é€šè¿‡é‡‡ç”¨ $\\alpha$-divergence å®¶æ—ä¸­å…·æœ‰å¯¹ç§°æ€§çš„ HÃ¶lder pooling ($\\alpha=0.5$)ï¼Œæ¨å¯¼å‡ºä¸€ç§åä¸º Hellinger çš„çŸ©åŒ¹é…è¿‘ä¼¼æ–¹æ³•ï¼Œå¹¶æ®æ­¤æ„å»ºäº† HELVAE æ¨¡å‹ã€‚HELVAE é¿å…äº†å¤æ‚çš„å­é‡‡æ ·è¿‡ç¨‹ï¼Œèƒ½å¤Ÿéšç€è§‚æµ‹æ¨¡æ€çš„å¢åŠ å­¦ä¹ åˆ°æ›´å…·è¡¨è¾¾åŠ›çš„æ½œåœ¨è¡¨ç¤ºï¼ˆlatent representationsï¼‰ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ¨¡å‹åœ¨ç”Ÿæˆç›¸å¹²æ€§ä¸è´¨é‡ä¹‹é—´å®ç°äº†æ›´ä¼˜çš„æƒè¡¡ï¼Œæ€§èƒ½è¶…è¶Šäº†ç°æœ‰çš„å…ˆè¿›å¤šæ¨¡æ€ VAE æ¨¡å‹ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2601.06572v1",
      "published_date": "2026-01-10 13:39:36 UTC",
      "updated_date": "2026-01-10 13:39:36 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-22T23:04:47.606093+00:00"
    },
    {
      "arxiv_id": "2601.06566v1",
      "title": "QCaption: Video Captioning and Q&A through Fusion of Large Multimodal Models",
      "title_zh": "QCaptionï¼šåŸºäºå¤§å¤šæ¨¡æ€æ¨¡å‹èåˆçš„è§†é¢‘æè¿°ä¸é—®ç­”",
      "authors": [
        "Jiale Wang",
        "Gee Wah Ng",
        "Lee Onn Mak",
        "Randall Cher",
        "Ng Ding Hei Ryan",
        "Davis Wang"
      ],
      "abstract": "This paper introduces QCaption, a novel video captioning and Q&A pipeline that enhances video analytics by fusing three models: key frame extraction, a Large Multimodal Model (LMM) for image-text analysis, and a Large Language Model (LLM) for text analysis. This approach enables integrated analysis of text, images, and video, achieving performance improvements over existing video captioning and Q&A models; all while remaining fully self-contained, adept for on-premises deployment. Experimental results using QCaption demonstrated up to 44.2% and 48.9% improvements in video captioning and Q&A tasks, respectively. Ablation studies were also performed to assess the role of LLM on the fusion on the results. Moreover, the paper proposes and evaluates additional video captioning approaches, benchmarking them against QCaption and existing methodologies. QCaption demonstrate the potential of adopting a model fusion approach in advancing video analytics.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† QCaptionï¼Œä¸€ç§é€šè¿‡èåˆå…³é”®å¸§æå–ã€å¤§å‹å¤šæ¨¡æ€æ¨¡å‹ (LMM) å’Œå¤§å‹è¯­è¨€æ¨¡å‹ (LLM) çš„æ–°å‹è§†é¢‘å­—å¹•ç”Ÿæˆ (video captioning) ä¸é—®ç­” (Q&A) æµæ°´çº¿ã€‚è¯¥æ–¹æ³•å®ç°äº†æ–‡æœ¬ã€å›¾åƒå’Œè§†é¢‘çš„é›†æˆåˆ†æï¼Œä¸”å…·å¤‡å®Œå…¨è‡ªç»™è‡ªè¶³çš„ç‰¹æ€§ï¼Œæ”¯æŒæœ¬åœ°åŒ–éƒ¨ç½² (on-premises deployment)ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒQCaption åœ¨è§†é¢‘å­—å¹•ç”Ÿæˆå’Œé—®ç­”ä»»åŠ¡ä¸­åˆ†åˆ«å®ç°äº† 44.2% å’Œ 48.9% çš„æ€§èƒ½æå‡ã€‚è¯¥ç ”ç©¶è¯æ˜äº†é‡‡ç”¨æ¨¡å‹èåˆ (model fusion) æ–¹æ³•åœ¨æ¨è¿›è§†é¢‘åˆ†æ (video analytics) é¢†åŸŸçš„æ˜¾è‘—æ½œåŠ›ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2601.06566v1",
      "published_date": "2026-01-10 13:28:43 UTC",
      "updated_date": "2026-01-10 13:28:43 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-22T23:05:02.327710+00:00"
    },
    {
      "arxiv_id": "2601.10742v1",
      "title": "Line-based Event Preprocessing: Towards Low-Energy Neuromorphic Computer Vision",
      "title_zh": "åŸºäºçº¿æ¡çš„äº‹ä»¶é¢„å¤„ç†ï¼šè¿ˆå‘ä½åŠŸè€—ç¥ç»å½¢æ€è®¡ç®—æœºè§†è§‰",
      "authors": [
        "AmÃ©lie Gruel",
        "Pierre Lewden",
        "Adrien F. Vincent",
        "Sylvain SaÃ¯ghi"
      ],
      "abstract": "Neuromorphic vision made significant progress in recent years, thanks to the natural match between spiking neural networks and event data in terms of biological inspiration, energy savings, latency and memory use for dynamic visual data processing. However, optimising its energy requirements still remains a challenge within the community, especially for embedded applications. One solution may reside in preprocessing events to optimise data quantity thus lowering the energy cost on neuromorphic hardware, proportional to the number of synaptic operations. To this end, we extend an end-to-end neuromorphic line detection mechanism to introduce line-based event data preprocessing. Our results demonstrate on three benchmark event-based datasets that preprocessing leads to an advantageous trade-off between energy consumption and classification performance. Depending on the line-based preprocessing strategy and the complexity of the classification task, we show that one can maintain or increase the classification accuracy while significantly reducing the theoretical energy consumption. Our approach systematically leads to a significant improvement of the neuromorphic classification efficiency, thus laying the groundwork towards a more frugal neuromorphic computer vision thanks to event preprocessing.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ç¥ç»å½¢æ€è®¡ç®—æœºè§†è§‰(Neuromorphic Computer Vision)åœ¨åµŒå…¥å¼åº”ç”¨ä¸­çš„é«˜èƒ½è€—æŒ‘æˆ˜ï¼Œæå‡ºäº†ä¸€ç§åŸºäºçº¿æ¡çš„äº‹ä»¶é¢„å¤„ç†(Line-based Event Preprocessing)æ–¹æ³•ã€‚é€šè¿‡æ‰©å±•ç«¯åˆ°ç«¯çš„ç¥ç»å½¢æ€çº¿æ¡æ£€æµ‹æœºåˆ¶ï¼Œè¯¥æ–¹æ³•æ—¨åœ¨é€šè¿‡ä¼˜åŒ–æ•°æ®é‡æ¥å‡å°‘ç¥ç»å½¢æ€ç¡¬ä»¶ä¸Šçš„çªè§¦æ“ä½œ(Synaptic Operations)ï¼Œä»è€Œå¤§å¹…é™ä½èƒ½è€—ã€‚åœ¨ä¸‰ä¸ªåŸºå‡†æ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥é¢„å¤„ç†ç­–ç•¥èƒ½åœ¨ä¿æŒæˆ–æé«˜åˆ†ç±»å‡†ç¡®ç‡(Classification Accuracy)çš„åŒæ—¶ï¼Œæ˜¾è‘—é™ä½ç†è®ºèƒ½è€—ã€‚è¯¥æ–¹æ³•æœ‰æ•ˆæå‡äº†ç¥ç»å½¢æ€åˆ†ç±»æ•ˆç‡(Neuromorphic Classification Efficiency)ï¼Œä¸ºå®ç°æ›´åŠ ä½åŠŸè€—ä¸”é«˜æ•ˆçš„è§†è§‰å¤„ç†ç³»ç»Ÿå¥ å®šäº†åŸºç¡€ã€‚",
      "categories": [
        "cs.NE",
        "cs.AI",
        "cs.CV",
        "eess.IV"
      ],
      "primary_category": "cs.NE",
      "comment": "18 pages (3 pages of acknowledgments and references), 10 figures and 4 tables. Submitted to the IOP Science \"Neuromorphic Computing and Engineering\" journal, awaiting feedback. This work is supported by a public grant overseen by the French National Research Agency (ANR) as part of the Ã©PEPR IA France 2030Ã© programme (Emergences project ANR-23-PEIA-0002)",
      "pdf_url": "https://arxiv.org/pdf/2601.10742v1",
      "published_date": "2026-01-10 12:30:34 UTC",
      "updated_date": "2026-01-10 12:30:34 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-22T23:05:07.066418+00:00"
    },
    {
      "arxiv_id": "2601.06551v1",
      "title": "L-RAG: Balancing Context and Retrieval with Entropy-Based Lazy Loading",
      "title_zh": "L-RAGï¼šé€šè¿‡åŸºäºç†µçš„å»¶è¿ŸåŠ è½½å¹³è¡¡ä¸Šä¸‹æ–‡ä¸æ£€ç´¢",
      "authors": [
        "Sergii Voloshyn"
      ],
      "abstract": "Retrieval-Augmented Generation (RAG) has emerged as the predominant paradigm for grounding Large Language Model outputs in factual knowledge, effectively mitigating hallucinations. However, conventional RAG systems operate under a \"retrieve-always\" assumption, querying vector databases for every input regardless of query complexity. This static approach incurs substantial computational overhead and inference latency, particularly problematic for high-throughput production deployments. We introduce L-RAG (Lazy Retrieval-Augmented Generation), an adaptive framework that implements hierarchical context management through entropy-based gating. L-RAG employs a two-tier architecture: queries are first processed with a compact document summary, and expensive chunk retrieval is triggered only when the model's predictive entropy exceeds a calibrated threshold, signaling genuine uncertainty. Through experiments on SQuAD 2.0 (N=500) using the Phi-2 model, we demonstrate that L-RAG provides a tunable accuracy-efficiency trade-off: at a conservative threshold (tau=0.5), L-RAG achieves 78.2% accuracy, matching Standard RAG (77.8%), with 8% retrieval reduction; at a balanced threshold (tau=1.0), retrieval reduction increases to 26% with modest accuracy trade-off (76.0%). Latency analysis shows that L-RAG saves 80-210ms per query when retrieval latency exceeds 500ms. Analysis of entropy distributions reveals statistically significant separation (p < 0.001) between correct predictions (H=1.72) and errors (H=2.20), validating entropy as a reliable uncertainty signal. L-RAG offers a practical, training-free approach toward more efficient RAG deployment, providing system architects with a configurable knob to balance accuracy and throughput requirements.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† L-RAG (Lazy Retrieval-Augmented Generation)ï¼Œä¸€ç§åŸºäºç†µ (Entropy-Based) çš„è‡ªé€‚åº”æ‡’åŠ è½½æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³ä¼ ç»Ÿ RAG ç³»ç»Ÿä¸­â€œå§‹ç»ˆæ£€ç´¢â€æ¨¡å¼å¯¼è‡´çš„è®¡ç®—å¼€é”€å’Œæ¨ç†å»¶è¿Ÿé—®é¢˜ã€‚L-RAG é‡‡ç”¨åŒå±‚æ¶æ„ï¼Œä¼˜å…ˆåˆ©ç”¨æ–‡æ¡£æ‘˜è¦å¤„ç†æŸ¥è¯¢ï¼Œä»…åœ¨æ¨¡å‹é¢„æµ‹ç†µè¶…è¿‡é¢„è®¾é˜ˆå€¼å¹¶è¡¨ç°å‡ºä¸ç¡®å®šæ€§æ—¶ï¼Œæ‰ä¼šè§¦å‘é«˜æˆæœ¬çš„åˆ†å—æ£€ç´¢ (Chunk Retrieval)ã€‚åœ¨ SQuAD 2.0 æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒL-RAG åœ¨ä¿æŒä¸æ ‡å‡† RAG ç›¸å½“å‡†ç¡®ç‡çš„åŒæ—¶ï¼Œå¯å‡å°‘ 8% è‡³ 26% çš„æ£€ç´¢æ“ä½œï¼Œå¹¶æ˜¾è‘—é™ä½æŸ¥è¯¢å»¶è¿Ÿã€‚è¯¥æ–¹æ¡ˆæ— éœ€é¢å¤–è®­ç»ƒï¼Œä¸ºç³»ç»Ÿæ¶æ„å¸ˆæä¾›äº†ä¸€ç§å¯çµæ´»è°ƒèŠ‚çš„æ‰‹æ®µï¼Œä»¥åœ¨å®é™…ç”Ÿäº§ç¯å¢ƒä¸­å¹³è¡¡ RAG ç³»ç»Ÿçš„å‡†ç¡®ç‡ä¸ååé‡éœ€æ±‚ã€‚",
      "categories": [
        "cs.IR",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.IR",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2601.06551v1",
      "published_date": "2026-01-10 12:25:19 UTC",
      "updated_date": "2026-01-10 12:25:19 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-22T23:05:10.853309+00:00"
    },
    {
      "arxiv_id": "2601.06550v1",
      "title": "LLMTrack: Semantic Multi-Object Tracking with Multi-modal Large Language Models",
      "title_zh": "LLMTrackï¼šåŸºäºå¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹çš„è¯­ä¹‰å¤šç›®æ ‡è·Ÿè¸ª",
      "authors": [
        "Pan Liao",
        "Feng Yang",
        "Di Wu",
        "Jinwen Yu",
        "Yuhua Zhu",
        "Wenhui Zhao"
      ],
      "abstract": "Traditional Multi-Object Tracking (MOT) systems have achieved remarkable precision in localization and association, effectively answering \\textit{where} and \\textit{who}. However, they often function as autistic observers, capable of tracing geometric paths but blind to the semantic \\textit{what} and \\textit{why} behind object behaviors. To bridge the gap between geometric perception and cognitive reasoning, we propose \\textbf{LLMTrack}, a novel end-to-end framework for Semantic Multi-Object Tracking (SMOT). We adopt a bionic design philosophy that decouples strong localization from deep understanding, utilizing Grounding DINO as the eyes and the LLaVA-OneVision multimodal large model as the brain. We introduce a Spatio-Temporal Fusion Module that aggregates instance-level interaction features and video-level contexts, enabling the Large Language Model (LLM) to comprehend complex trajectories. Furthermore, we design a progressive three-stage training strategy, Visual Alignment, Temporal Fine-tuning, and Semantic Injection via LoRA to efficiently adapt the massive model to the tracking domain. Extensive experiments on the BenSMOT benchmark demonstrate that LLMTrack achieves state-of-the-art performance, significantly outperforming existing methods in instance description, interaction recognition, and video summarization while maintaining robust tracking stability.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† LLMTrackï¼Œä¸€ç§ç”¨äºè¯­ä¹‰å¤šç›®æ ‡è·Ÿè¸ª (Semantic Multi-Object Tracking, SMOT) çš„ç«¯åˆ°ç«¯æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³ä¼ ç»Ÿè¿½è¸ªç³»ç»Ÿåœ¨è¡Œä¸ºè¯­ä¹‰ç†è§£ä¸Šçš„å±€é™æ€§ã€‚è¯¥æ¡†æ¶é‡‡ç”¨ä»¿ç”Ÿè®¾è®¡ï¼Œå°† Grounding DINO ä½œä¸ºå®šä½çš„â€œçœ¼ç›â€ï¼Œå¹¶åˆ©ç”¨ LLaVA-OneVision å¤šæ¨¡æ€å¤§æ¨¡å‹ä½œä¸ºè®¤çŸ¥â€œå¤§è„‘â€ã€‚é€šè¿‡æ—¶ç©ºèåˆæ¨¡å— (Spatio-Temporal Fusion Module) æ•´åˆå®ä¾‹ç‰¹å¾ä¸è§†é¢‘ä¸Šä¸‹æ–‡ï¼Œå¹¶é…åˆåŒ…å«è§†è§‰å¯¹é½ã€æ—¶åºå¾®è°ƒåŠè¯­ä¹‰æ³¨å…¥ (LoRA) çš„ä¸‰é˜¶æ®µè®­ç»ƒç­–ç•¥ï¼Œå®ç°äº†å¯¹å¤æ‚è½¨è¿¹çš„æ·±åº¦ç†è§£ã€‚å®éªŒè¯æ˜ï¼ŒLLMTrack åœ¨ BenSMOT åŸºå‡†æµ‹è¯•ä¸­è¾¾åˆ°äº† SOTA æ€§èƒ½ï¼Œåœ¨å®ä¾‹æè¿°ã€äº¤äº’è¯†åˆ«å’Œè§†é¢‘æ‘˜è¦ä»»åŠ¡ä¸Šè¡¨ç°å“è¶Šï¼ŒåŒæ—¶ä¿æŒäº†å‡ºè‰²çš„è·Ÿè¸ªç¨³å®šæ€§ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2601.06550v1",
      "published_date": "2026-01-10 12:18:12 UTC",
      "updated_date": "2026-01-10 12:18:12 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-22T23:05:12.762783+00:00"
    },
    {
      "arxiv_id": "2601.06543v1",
      "title": "SimLLM: Fine-Tuning Code LLMs for SimPy-Based Queueing System Simulation",
      "title_zh": "SimLLMï¼šé¢å‘åŸºäº SimPy çš„æ’é˜Ÿç³»ç»Ÿä»¿çœŸçš„ä»£ç å¤§è¯­è¨€æ¨¡å‹å¾®è°ƒ",
      "authors": [
        "Jun-Qi Chen",
        "Kun Zhang",
        "Rui Zheng",
        "Ying Zhong"
      ],
      "abstract": "The Python package SimPy is widely used for modeling queueing systems due to its flexibility, simplicity, and smooth integration with modern data analysis and optimization frameworks. Recent advances in large language models (LLMs) have shown strong ability in generating clear and executable code, making them powerful and suitable tools for writing SimPy queueing simulation code. However, directly employing closed-source models like GPT-4o to generate such code may lead to high computational costs and raise data privacy concerns. To address this, we fine-tune two open-source LLMs, Qwen-Coder-7B and DeepSeek-Coder-6.7B, on curated SimPy queueing data, which enhances their code-generating performance in executability, output-format compliance, and instruction-code consistency. Particularly, we proposed a multi-stage fine-tuning framework comprising two stages of supervised fine-tuning (SFT) and one stage of direct preference optimization (DPO), progressively enhancing the model's ability in SimPy-based queueing simulation code generation. Extensive evaluations demonstrate that both fine-tuned models achieve substantial improvements in executability, output-format compliance, and instruct consistency. These results confirm that domain-specific fine-tuning can effectively transform compact open-source code models into reliable SimPy simulation generators which provide a practical alternative to closed-source LLMs for education, research, and operational decision support.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† SimLLMï¼Œæ—¨åœ¨è§£å†³ä½¿ç”¨ GPT-4o ç­‰é—­æºå¤§è¯­è¨€æ¨¡å‹ç”Ÿæˆ SimPy æ’é˜Ÿä»¿çœŸä»£ç æ—¶é¢ä¸´çš„é«˜æˆæœ¬å’Œéšç§é—®é¢˜ã€‚é€šè¿‡å¯¹å¼€æºæ¨¡å‹ Qwen-Coder-7B å’Œ DeepSeek-Coder-6.7B è¿›è¡Œé’ˆå¯¹æ€§å¾®è°ƒï¼Œæ˜¾è‘—æå‡äº†å…¶åœ¨ä»£ç å¯æ‰§è¡Œæ€§ã€æ ¼å¼åˆè§„æ€§å’ŒæŒ‡ä»¤ä¸€è‡´æ€§æ–¹é¢çš„è¡¨ç°ã€‚æ ¸å¿ƒæ–¹æ³•æ˜¯ä¸€ä¸ªåŒ…å«ä¸¤ä¸ªé˜¶æ®µæœ‰ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰å’Œä¸€ä¸ªé˜¶æ®µç›´æ¥åå¥½ä¼˜åŒ–ï¼ˆDPOï¼‰çš„å¤šé˜¶æ®µå¾®è°ƒæ¡†æ¶ã€‚å®éªŒè¯æ˜ï¼Œè¯¥æ–¹æ³•ä½¿è½»é‡çº§å¼€æºæ¨¡å‹èƒ½å¤Ÿæˆä¸ºå¯é çš„ SimPy ä»¿çœŸä»£ç ç”Ÿæˆå™¨ï¼Œä¸ºç§‘ç ”ã€æ•™è‚²å’Œå†³ç­–æ”¯æŒæä¾›äº†å®ç”¨çš„æ›¿ä»£æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "33 pages, 10 figures",
      "pdf_url": "https://arxiv.org/pdf/2601.06543v1",
      "published_date": "2026-01-10 11:53:39 UTC",
      "updated_date": "2026-01-10 11:53:39 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-22T23:05:13.604560+00:00"
    },
    {
      "arxiv_id": "2601.06542v1",
      "title": "Resource-constrained Project Scheduling with Time-of-Use Energy Tariffs and Machine States: A Logic-based Benders Decomposition Approach",
      "title_zh": "è€ƒè™‘åˆ†æ—¶ç”µä»·ä¸æœºå™¨çŠ¶æ€çš„èµ„æºå—é™é¡¹ç›®è°ƒåº¦ï¼šä¸€ç§åŸºäºé€»è¾‘çš„ Benders åˆ†è§£æ–¹æ³•",
      "authors": [
        "Corentin Juvigny",
        "AntonÃ­n NovÃ¡k",
        "Jan MandÃ­k",
        "ZdenÄ›k HanzÃ¡lek"
      ],
      "abstract": "In this paper, we investigate the Resource-Constrained Project Scheduling Problem (RCPSP) with time-of-use energy tariffs (TOU) and machine states, a variant of RCPSP for production scheduling where energy price is part of the criteria and one machine is highly energy-demanding and can be in one of the following three states: proc, idle, or off. The problem involves scheduling all tasks, respecting precedence constraints and resource limitations, while minimizing the combination of the overall makespan and the total energy cost (TEC), which varies according to the TOU pricing, which can take negative values. We propose two novel approaches to solve it: a monolithic Constraint Programming (CP) approach and a Logic-Based Benders Decomposition (LBBD) approach. The latter combines a master problem dealing with energy cost solved using Integer Linear Programming (ILP) with a subproblem handling the RCPSP resolved using CP. Both approaches surpass the monolithic compact ILP approach, but the LBBD significantly outperforms the CP when the ratio of energy-intensive tasks over the overall tasks is moderate, allowing for solving instances with up to 1600 tasks in sparse instances. Finally, we put forth a way of generalizing our LBBD approach to other problems sharing similar characteristics, and we applied it to a problem based on an RCPSP problem with blocking times & total weighted tardiness criterion and a flexible job shop.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†è€ƒè™‘åˆ†æ—¶ç”µä»·(Time-of-Use Energy Tariffs, TOU)å’Œæœºå™¨çŠ¶æ€ï¼ˆåŠ å·¥ã€ç©ºé—²ã€å…³æœºï¼‰çš„èµ„æºå—é™é¡¹ç›®è°ƒåº¦é—®é¢˜(Resource-Constrained Project Scheduling Problem, RCPSP)ï¼Œæ—¨åœ¨åŒæ—¶ä¼˜åŒ–æ€»å®Œå·¥æ—¶é—´(Makespan)å’Œæ€»èƒ½æºæˆæœ¬(Total Energy Cost, TEC)ã€‚è®ºæ–‡æå‡ºäº†ä¸¤ç§åˆ›æ–°æ±‚è§£æ–¹æ³•ï¼šä¸€ç§æ˜¯æ•´ä½“çº¦æŸè§„åˆ’(Constraint Programming, CP)æ–¹æ³•ï¼Œå¦ä¸€ç§æ˜¯å°†æ•´æ•°çº¿æ€§è§„åˆ’(ILP)ä¸CPç›¸ç»“åˆçš„åŸºäºé€»è¾‘çš„Bendersåˆ†è§£(Logic-Based Benders Decomposition, LBBD)æ¡†æ¶ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒLBBDåœ¨å¤„ç†ä¸­ç­‰æ¯”ä¾‹é«˜èƒ½è€—ä»»åŠ¡çš„åœºæ™¯ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œèƒ½å¤Ÿå¤„ç†åŒ…å«å¤šè¾¾1600ä¸ªä»»åŠ¡çš„å¤§è§„æ¨¡ç®—ä¾‹ã€‚æ­¤å¤–ï¼Œè¯¥LBBDæ–¹æ³•å…·æœ‰è‰¯å¥½çš„æ³›åŒ–æ€§ï¼Œå¯æ¨å¹¿è‡³å¸¦æœ‰é˜»å¡æ—¶é—´çš„RCPSPåŠæŸ”æ€§ä½œä¸šè½¦é—´è°ƒåº¦ç­‰å…·æœ‰ç±»ä¼¼ç‰¹å¾çš„å¤æ‚ç”Ÿäº§è°ƒåº¦é—®é¢˜ã€‚",
      "categories": [
        "math.OC",
        "cs.AI"
      ],
      "primary_category": "math.OC",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2601.06542v1",
      "published_date": "2026-01-10 11:47:56 UTC",
      "updated_date": "2026-01-10 11:47:56 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-22T23:05:20.219380+00:00"
    },
    {
      "arxiv_id": "2601.06540v1",
      "title": "Self-Organizing Dual-Buffer Adaptive Clustering Experience Replay (SODASER) for Safe Reinforcement Learning in Optimal Control",
      "title_zh": "é¢å‘æœ€ä¼˜æ§åˆ¶ä¸­å®‰å…¨å¼ºåŒ–å­¦ä¹ çš„è‡ªç»„ç»‡åŒç¼“å†²è‡ªé€‚åº”èšç±»ç»éªŒå›æ”¾ï¼ˆSODASERï¼‰",
      "authors": [
        "Roya Khalili Amirabadi",
        "Mohsen Jalaeian Farimani",
        "Omid Solaymani Fard"
      ],
      "abstract": "This paper proposes a novel reinforcement learning framework, named Self-Organizing Dual-buffer Adaptive Clustering Experience Replay (SODACER), designed to achieve safe and scalable optimal control of nonlinear systems. The proposed SODACER mechanism consisting of a Fast-Buffer for rapid adaptation to recent experiences and a Slow-Buffer equipped with a self-organizing adaptive clustering mechanism to maintain diverse and non-redundant historical experiences. The adaptive clustering mechanism dynamically prunes redundant samples, optimizing memory efficiency while retaining critical environmental patterns. The approach integrates SODASER with Control Barrier Functions (CBFs) to guarantee safety by enforcing state and input constraints throughout the learning process. To enhance convergence and stability, the framework is combined with the Sophia optimizer, enabling adaptive second-order gradient updates. The proposed SODACER-Sophia's architecture ensures reliable, effective, and robust learning in dynamic, safety-critical environments, offering a generalizable solution for applications in robotics, healthcare, and large-scale system optimization. The proposed approach is validated on a nonlinear Human Papillomavirus (HPV) transmission model with multiple control inputs and safety constraints. Comparative evaluations against random and clustering-based experience replay methods demonstrate that SODACER achieves faster convergence, improved sample efficiency, and a superior bias-variance trade-off, while maintaining safe system trajectories, validated via the Friedman test.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§åä¸º Self-Organizing Dual-buffer Adaptive Clustering Experience Replay (SODACER) çš„æ–°å‹å¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼Œæ—¨åœ¨å®ç°éçº¿æ€§ç³»ç»Ÿåœ¨å®‰å…¨å…³é”®ç¯å¢ƒä¸‹çš„é«˜æ•ˆæœ€ä½³æ§åˆ¶ã€‚è¯¥æ¡†æ¶é€šè¿‡ Fast-Buffer å¿«é€Ÿé€‚åº”è¿‘æœŸç»éªŒï¼Œå¹¶åˆ©ç”¨ç»“åˆè‡ªç»„ç»‡è‡ªé€‚åº”èšç±»æœºåˆ¶çš„ Slow-Buffer ç»´æŒå¤šæ ·ä¸”éå†—ä½™çš„å†å²æ•°æ®ï¼Œä»è€Œä¼˜åŒ–å†…å­˜æ•ˆç‡å¹¶ä¿ç•™å…³é”®ç¯å¢ƒæ¨¡å¼ã€‚ä¸ºäº†ä¿éšœå®‰å…¨æ€§ï¼Œç³»ç»Ÿé›†æˆäº†æ§åˆ¶å±éšœå‡½æ•° (Control Barrier Functions, CBFs) ä»¥å¼ºåŒ–çŠ¶æ€å’Œè¾“å…¥çº¦æŸï¼Œå¹¶é‡‡ç”¨ Sophia ä¼˜åŒ–å™¨é€šè¿‡äºŒé˜¶æ¢¯åº¦æ›´æ–°æå‡æ”¶æ•›ç¨³å®šæ€§ã€‚åœ¨äººä¹³å¤´ç˜¤ç—…æ¯’ (HPV) ä¼ æ’­æ¨¡å‹çš„å®éªŒä¸­ï¼ŒSODACER åœ¨æ”¶æ•›é€Ÿåº¦ã€æ ·æœ¬æ•ˆç‡åŠåå·®-æ–¹å·®æƒè¡¡ (bias-variance trade-off) æ–¹é¢å‡ä¼˜äºä¼ ç»Ÿæ–¹æ³•ï¼Œä¸ºæœºå™¨äººå’ŒåŒ»ç–—ç­‰é¢†åŸŸçš„å®‰å…¨å¼ºåŒ–å­¦ä¹ æä¾›äº†å¯é æ–¹æ¡ˆã€‚",
      "categories": [
        "eess.SY",
        "cs.AI",
        "cs.LG",
        "cs.RO",
        "math.OC"
      ],
      "primary_category": "eess.SY",
      "comment": "Also available at SSRN: https://ssrn.com/abstract=5191427 or http://dx.doi.org/10.2139/ssrn.5191427",
      "pdf_url": "https://arxiv.org/pdf/2601.06540v1",
      "published_date": "2026-01-10 11:43:15 UTC",
      "updated_date": "2026-01-10 11:43:15 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-22T23:05:21.539486+00:00"
    },
    {
      "arxiv_id": "2601.06533v1",
      "title": "Short-term electricity load forecasting with multi-frequency reconstruction diffusion",
      "title_zh": "åŸºäºå¤šé¢‘é‡æ„æ‰©æ•£æ¨¡å‹çš„çŸ­æœŸç”µåŠ›è´Ÿè·é¢„æµ‹",
      "authors": [
        "Qi Dong",
        "Rubing Huang",
        "Ling Zhou",
        "Dave Towey",
        "Jinyu Tian",
        "Jianzhou Wang"
      ],
      "abstract": "Diffusion models have emerged as a powerful method in various applications. However, their application to Short-Term Electricity Load Forecasting (STELF) -- a typical scenario in energy systems -- remains largely unexplored. Considering the nonlinear and fluctuating characteristics of the load data, effectively utilizing the powerful modeling capabilities of diffusion models to enhance STELF accuracy remains a challenge. This paper proposes a novel diffusion model with multi-frequency reconstruction for STELF, referred to as the Multi-Frequency-Reconstruction-based Diffusion (MFRD) model. The MFRD model achieves accurate load forecasting through four key steps: (1) The original data is combined with the decomposed multi-frequency modes to form a new data representation; (2) The diffusion model adds noise to the new data, effectively reducing and weakening the noise in the original data; (3) The reverse process adopts a denoising network that combines Long Short-Term Memory (LSTM) and Transformer to enhance noise removal; and (4) The inference process generates the final predictions based on the trained denoising network. To validate the effectiveness of the MFRD model, we conducted experiments on two data platforms: Australian Energy Market Operator (AEMO) and Independent System Operator of New England (ISO-NE). The experimental results show that our model consistently outperforms the compared models.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹çŸ­æœŸç”µåŠ›è´Ÿè·é¢„æµ‹ï¼ˆShort-Term Electricity Load Forecasting, STELFï¼‰ä¸­è´Ÿè·æ•°æ®çš„éçº¿æ€§å’Œæ³¢åŠ¨æ€§æŒ‘æˆ˜ï¼Œæå‡ºäº†ä¸€ç§åä¸º MFRD çš„å¤šé¢‘ç‡é‡æ„æ‰©æ•£æ¨¡å‹ã€‚è¯¥æ¨¡å‹é€šè¿‡å°†åŸå§‹æ•°æ®ä¸åˆ†è§£åçš„å¤šé¢‘ç‡åˆ†é‡ç»“åˆæ„å»ºæ–°æ•°æ®è¡¨ç¤ºï¼Œå¹¶åˆ©ç”¨æ‰©æ•£æ¨¡å‹ï¼ˆDiffusion modelï¼‰çš„åŠ å™ªè¿‡ç¨‹æ¥æœ‰æ•ˆå‰Šå¼±åŸå§‹æ•°æ®ä¸­çš„å™ªå£°å¹²æ‰°ã€‚åœ¨é€†å‘å»å™ªé˜¶æ®µï¼ŒMFRD é‡‡ç”¨äº†ç»“åˆ Long Short-Term Memory (LSTM) å’Œ Transformer çš„å»å™ªç½‘ç»œï¼Œä»¥å¢å¼ºç‰¹å¾æå–ä¸å™ªå£°æ¶ˆé™¤èƒ½åŠ›ã€‚åœ¨ AEMO å’Œ ISO-NE æ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼ŒMFRD æ¨¡å‹çš„é¢„æµ‹æ€§èƒ½æŒç»­ä¼˜äºç°æœ‰å¯¹æ¯”æ¨¡å‹ï¼Œè¯æ˜äº†å…¶åœ¨å¤„ç†å¤æ‚èƒ½æºç³»ç»Ÿæ•°æ®æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2601.06533v1",
      "published_date": "2026-01-10 11:22:25 UTC",
      "updated_date": "2026-01-10 11:22:25 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-22T23:05:24.303348+00:00"
    },
    {
      "arxiv_id": "2601.06530v1",
      "title": "Improving Day-Ahead Grid Carbon Intensity Forecasting by Joint Modeling of Local-Temporal and Cross-Variable Dependencies Across Different Frequencies",
      "title_zh": "é€šè¿‡è”åˆå»ºæ¨¡è·¨é¢‘ç‡å±€éƒ¨æ—¶åŸŸä¸å˜é‡é—´ä¾èµ–æ”¹è¿›æ—¥å‰ç”µç½‘ç¢³å¼ºåº¦é¢„æµ‹",
      "authors": [
        "Bowen Zhang",
        "Hongda Tian",
        "Adam Berry",
        "A. Craig Roussac"
      ],
      "abstract": "Accurate forecasting of the grid carbon intensity factor (CIF) is critical for enabling demand-side management and reducing emissions in modern electricity systems. Leveraging multiple interrelated time series, CIF prediction is typically formulated as a multivariate time series forecasting problem. Despite advances in deep learning-based methods, it remains challenging to capture the fine-grained local-temporal dependencies, dynamic higher-order cross-variable dependencies, and complex multi-frequency patterns for CIF forecasting. To address these issues, we propose a novel model that integrates two parallel modules: 1) one enhances the extraction of local-temporal dependencies under multi-frequency by applying multiple wavelet-based convolutional kernels to overlapping patches of varying lengths; 2) the other captures dynamic cross-variable dependencies under multi-frequency to model how inter-variable relationships evolve across the time-frequency domain. Evaluations on four representative electricity markets from Australia, featuring varying levels of renewable penetration, demonstrate that the proposed method outperforms the state-of-the-art models. An ablation study further validates the complementary benefits of the two proposed modules. Designed with built-in interpretability, the proposed model also enables better understanding of its predictive behavior, as shown in a case study where it adaptively shifts attention to relevant variables and time intervals during a disruptive event.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§æ–°å‹é¢„æµ‹æ¨¡å‹ï¼Œæ—¨åœ¨æå‡ç”µç½‘ç¢³å¼ºåº¦å› å­ (Carbon Intensity Factor, CIF) çš„æ—¥å‰é¢„æµ‹ç²¾åº¦ã€‚ä¸ºäº†è§£å†³å¤šå˜é‡æ—¶é—´åºåˆ—ä¸­å¤æ‚çš„å±€éƒ¨æ—¶é—´ä¾èµ–å’Œå¤šé¢‘ç‡æ¨¡å¼ï¼Œè¯¥æ¨¡å‹é›†æˆäº†ä¸¤ä¸ªå¹¶è¡Œæ¨¡å—ï¼šä¸€ä¸ªåˆ©ç”¨åŸºäºå°æ³¢çš„å·ç§¯æ ¸ (wavelet-based convolutional kernels) æå–å¤šå°ºåº¦çš„å±€éƒ¨æ—¶é—´ç‰¹å¾ï¼Œå¦ä¸€ä¸ªåˆ™ç”¨äºæ•æ‰æ—¶é¢‘åŸŸå†…åŠ¨æ€çš„è·¨å˜é‡ä¾èµ– (cross-variable dependencies)ã€‚åœ¨æ¾³å¤§åˆ©äºšå››ä¸ªç”µåŠ›å¸‚åœºï¼ˆæ¶µç›–ä¸åŒå¯å†ç”Ÿèƒ½æºæ¸—é€ç‡ï¼‰çš„å®éªŒè¯„ä¼°æ˜¾ç¤ºï¼Œè¯¥æ–¹æ³•æ€§èƒ½ä¼˜äºç°æœ‰çš„æœ€å…ˆè¿› (state-of-the-art) æ¨¡å‹ã€‚æ­¤å¤–ï¼Œè¯¥æ¨¡å‹å…·æœ‰å†…ç½®çš„å¯è§£é‡Šæ€§ (interpretability)ï¼Œèƒ½å¤Ÿè‡ªé€‚åº”åœ°å…³æ³¨ç›¸å…³å˜é‡å’Œæ—¶é—´åŒºé—´ï¼Œä¸ºç”µåŠ›ç³»ç»Ÿæ’æ”¾ç›‘æµ‹å’Œéœ€æ±‚ä¾§ç®¡ç†æä¾›äº†å¯é çš„æŠ€æœ¯æ”¯æŒã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "2026 40th AAAI Conference on Artificial Intelligence",
      "pdf_url": "https://arxiv.org/pdf/2601.06530v1",
      "published_date": "2026-01-10 11:20:55 UTC",
      "updated_date": "2026-01-10 11:20:55 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-22T23:05:28.902156+00:00"
    },
    {
      "arxiv_id": "2601.06528v1",
      "title": "Atomic-SNLI: Fine-Grained Natural Language Inference through Atomic Fact Decomposition",
      "title_zh": "Atomic-SNLIï¼šåŸºäºåŸå­äº‹å®åˆ†è§£çš„ç»†ç²’åº¦è‡ªç„¶è¯­è¨€æ¨ç†",
      "authors": [
        "Minghui Huang"
      ],
      "abstract": "Current Natural Language Inference (NLI) systems primarily operate at the sentence level, providing black-box decisions that lack explanatory power. While atomic-level NLI offers a promising alternative by decomposing hypotheses into individual facts, we demonstrate that the conventional assumption that a hypothesis is entailed only when all its atomic facts are entailed fails in practice due to models' poor performance on fine-grained reasoning. Our analysis reveals that existing models perform substantially worse on atomic level inference compared to sentence level tasks. To address this limitation, we introduce Atomic-SNLI, a novel dataset constructed by decomposing SNLI and enriching it with carefully curated atomic level examples through linguistically informed generation strategies. Experimental results demonstrate that models fine-tuned on Atomic-SNLI achieve significant improvements in atomic reasoning capabilities while maintaining strong sentence level performance, enabling both accurate judgements and transparent, explainable results at the fact level.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å½“å‰è‡ªç„¶è¯­è¨€æ¨ç† (Natural Language Inference, NLI) ç³»ç»Ÿç¼ºä¹è§£é‡ŠåŠ›ä¸”åœ¨ç»†ç²’åº¦æ¨ç†è¡¨ç°ä¸ä½³çš„é—®é¢˜ï¼Œæå‡ºäº† Atomic-SNLI æ•°æ®é›†ã€‚è¯¥æ•°æ®é›†é€šè¿‡åŸå­äº‹å®åˆ†è§£ (Atomic Fact Decomposition) å°† SNLI ä»»åŠ¡ç»†åŒ–ï¼Œå¹¶åˆ©ç”¨è¯­è¨€å­¦æŒ‡å¯¼çš„ç”Ÿæˆç­–ç•¥ä¸°å¯Œäº†åŸå­çº§çš„æ¨ç†ç¤ºä¾‹ã€‚å®éªŒè¡¨æ˜ï¼Œåœ¨ Atomic-SNLI ä¸Šå¾®è°ƒçš„æ¨¡å‹åœ¨æ˜¾è‘—æå‡åŸå­æ¨ç† (atomic reasoning) èƒ½åŠ›çš„åŒæ—¶ï¼Œä¾ç„¶ä¿æŒäº†å¼ºåŠ²çš„å¥å­çº§æ€§èƒ½ã€‚è¯¥ç ”ç©¶ä¸ºå®ç°å‡†ç¡®ã€é€æ˜ä¸”å…·æœ‰äº‹å®çº§å¯è§£é‡Šæ€§çš„æ¨ç†ç»“æœæä¾›äº†æœ‰æ•ˆæ–¹æ¡ˆã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2601.06528v1",
      "published_date": "2026-01-10 11:13:35 UTC",
      "updated_date": "2026-01-10 11:13:35 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-22T23:05:30.300780+00:00"
    },
    {
      "arxiv_id": "2601.06505v1",
      "title": "Neural Nonmyopic Bayesian Optimization in Dynamic Cost Settings",
      "title_zh": "åŠ¨æ€æˆæœ¬ç¯å¢ƒä¸‹çš„ç¥ç»éè¿‘è§†è´å¶æ–¯ä¼˜åŒ–",
      "authors": [
        "Sang T. Truong",
        "Duc Q. Nguyen",
        "Willie Neiswanger",
        "Ryan-Rhys Griffiths",
        "Stefano Ermon",
        "Nick Haber",
        "Sanmi Koyejo"
      ],
      "abstract": "Bayesian optimization (BO) is a common framework for optimizing black-box functions, yet most existing methods assume static query costs and rely on myopic acquisition strategies. We introduce LookaHES, a nonmyopic BO framework designed for dynamic, history-dependent cost environments, where evaluation costs vary with prior actions, such as travel distance in spatial tasks or edit distance in sequence design. LookaHES combines a multi-step variant of $H$-Entropy Search with pathwise sampling and neural policy optimization, enabling long-horizon planning beyond twenty steps without the exponential complexity of existing nonmyopic methods. The key innovation is the integration of neural policies, including large language models, to effectively navigate structured, combinatorial action spaces such as protein sequences. These policies amortize lookahead planning and can be integrated with domain-specific constraints during rollout. Empirically, LookaHES outperforms strong myopic and nonmyopic baselines across nine synthetic benchmarks from two to eight dimensions and two real-world tasks: geospatial optimization using NASA night-light imagery and protein sequence design with constrained token-level edits. In short, LookaHES provides a general, scalable, and cost-aware solution for robust long-horizon optimization in complex decision spaces, which makes it a useful tool for researchers in machine learning, statistics, and applied domains. Our implementation is available at https://github.com/sangttruong/nonmyopia.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† LookaHESï¼Œä¸€ä¸ªä¸“ä¸ºåŠ¨æ€ä¸”ä¾èµ–å†å²æˆæœ¬ï¼ˆhistory-dependent costï¼‰ç¯å¢ƒè®¾è®¡çš„éè¿‘è§†ï¼ˆnonmyopicï¼‰Bayesian optimization (BO) æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³è¯„ä¼°æˆæœ¬éšå…ˆå‰è¡ŒåŠ¨å˜åŒ–ï¼ˆå¦‚æ—…è¡Œè·ç¦»æˆ–ç¼–è¾‘è·ç¦»ï¼‰çš„ä¼˜åŒ–éš¾é¢˜ã€‚LookaHES ç»“åˆäº†å¤šæ­¥ $H$-Entropy Searchã€è·¯å¾„é‡‡æ ·ï¼ˆpathwise samplingï¼‰ä¸ç¥ç»ç­–ç•¥ä¼˜åŒ–ï¼ˆneural policy optimizationï¼‰ï¼Œå®ç°äº†è¶…è¿‡äºŒåæ­¥çš„é•¿æœŸè§„åˆ’ï¼Œå¹¶æœ‰æ•ˆå…‹æœäº†ç°æœ‰éè¿‘è§†æ–¹æ³•çš„æŒ‡æ•°çº§å¤æ‚åº¦ã€‚é€šè¿‡é›†æˆå¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ç­‰ç¥ç»ç­–ç•¥ï¼Œè¯¥æ¡†æ¶èƒ½å¤Ÿé«˜æ•ˆå¤„ç†è›‹ç™½è´¨åºåˆ—è®¾è®¡ç­‰å¤æ‚çš„ç»„åˆåŠ¨ä½œç©ºé—´ã€‚å®éªŒè¡¨æ˜ï¼ŒLookaHES åœ¨åˆæˆåŸºå‡†æµ‹è¯•ã€åŸºäº NASA å½±åƒçš„åœ°ç†ç©ºé—´ä¼˜åŒ–åŠè›‹ç™½è´¨åºåˆ—è®¾è®¡ä»»åŠ¡ä¸­æ˜¾è‘—ä¼˜äºä¼ ç»ŸåŸºçº¿æ¨¡å‹ï¼Œä¸ºå¤æ‚å†³ç­–ç©ºé—´æä¾›äº†ä¸€ç§é€šç”¨ä¸”å…·å¤‡æˆæœ¬æ„è¯†çš„å¯æ‰©å±•æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "32 pages, 20 figures, 13 tables",
      "pdf_url": "https://arxiv.org/pdf/2601.06505v1",
      "published_date": "2026-01-10 09:49:45 UTC",
      "updated_date": "2026-01-10 09:49:45 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-22T23:05:42.625092+00:00"
    },
    {
      "arxiv_id": "2601.06502v1",
      "title": "DRAGON: LLM-Driven Decomposition and Reconstruction Agents for Large-Scale Combinatorial Optimization",
      "title_zh": "DRAGONï¼šé¢å‘å¤§è§„æ¨¡ç»„åˆä¼˜åŒ–çš„ LLM é©±åŠ¨åˆ†è§£ä¸é‡æ„æ™ºèƒ½ä½“",
      "authors": [
        "Shengkai Chen",
        "Zhiguang Cao",
        "Jianan Zhou",
        "Yaoxin Wu",
        "Senthilnath Jayavelu",
        "Zhuoyi Lin",
        "Xiaoli Li",
        "Shili Xiang"
      ],
      "abstract": "Large Language Models (LLMs) have recently shown promise in addressing combinatorial optimization problems (COPs) through prompt-based strategies. However, their scalability and generalization remain limited, and their effectiveness diminishes as problem size increases, particularly in routing problems involving more than 30 nodes. We propose DRAGON, which stands for Decomposition and Reconstruction Agents Guided OptimizatioN, a novel framework that combines the strengths of metaheuristic design and LLM reasoning. Starting from an initial global solution, DRAGON autonomously identifies regions with high optimization potential and strategically decompose large-scale COPs into manageable subproblems. Each subproblem is then reformulated as a concise, localized optimization task and solved through targeted LLM prompting guided by accumulated experiences. Finally, the locally optimized solutions are systematically reintegrated into the original global context to yield a significantly improved overall outcome. By continuously interacting with the optimization environment and leveraging an adaptive experience memory, the agents iteratively learn from feedback, effectively coupling symbolic reasoning with heuristic search. Empirical results show that, unlike existing LLM-based solvers limited to small-scale instances, DRAGON consistently produces feasible solutions on TSPLIB, CVRPLIB, and Weibull-5k bin packing benchmarks, and achieves near-optimal results (0.16% gap) on knapsack problems with over 3M variables. This work shows the potential of feedback-driven language agents as a new paradigm for generalizable and interpretable large-scale optimization.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† DRAGON æ¡†æ¶ï¼ˆDecomposition and Reconstruction Agents Guided OptimizatioNï¼‰ï¼Œæ—¨åœ¨è§£å†³å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨å¤„ç†å¤§è§„æ¨¡ç»„åˆä¼˜åŒ–é—®é¢˜ï¼ˆCOPsï¼‰æ—¶é¢ä¸´çš„å¯æ‰©å±•æ€§å’Œæ³›åŒ–èƒ½åŠ›ä¸è¶³çš„é—®é¢˜ã€‚è¯¥æ¡†æ¶å°†å…ƒå¯å‘å¼è®¾è®¡ä¸ LLM æ¨ç†ç›¸ç»“åˆï¼Œé€šè¿‡è‡ªåŠ¨å°†å¤§è§„æ¨¡ COPs åˆ†è§£ä¸ºå¯ç®¡ç†çš„å±€éƒ¨å­é—®é¢˜ï¼Œå¹¶åˆ©ç”¨ LLM åœ¨ç»éªŒè®°å¿†å¼•å¯¼ä¸‹è¿›è¡Œé’ˆå¯¹æ€§æ±‚è§£ï¼Œæœ€åå°†ä¼˜åŒ–åçš„å±€éƒ¨è§£é‡æ–°æ•´åˆã€‚DRAGON å¼•å…¥äº†è‡ªé€‚åº”ç»éªŒè®°å¿†æœºåˆ¶ï¼Œå®ç°äº†ç¬¦å·æ¨ç†ä¸å¯å‘å¼æœç´¢ï¼ˆheuristic searchï¼‰çš„æœ‰æ•ˆè€¦åˆã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯¥æ¨¡å‹åœ¨ TSPLIBã€CVRPLIB ç­‰åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œå¹¶åœ¨å˜é‡è§„æ¨¡è¶…è¿‡ 300 ä¸‡çš„èƒŒåŒ…é—®é¢˜ä¸Šå®ç°äº† 0.16% çš„æä½å·®è·ã€‚è¯¥å·¥ä½œä¸ºåé¦ˆé©±åŠ¨çš„è¯­è¨€æ™ºèƒ½ä½“åœ¨é€šç”¨ä¸”å¯è§£é‡Šçš„å¤§è§„æ¨¡ä¼˜åŒ–é¢†åŸŸæä¾›äº†ä¸€ç§æ–°çš„èŒƒå¼ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "This paper has been accepted for presentation and publication at the 25th International Conference on Autonomous Agents and Multiagent Systems (AAMAS 2026), source code will be available soon",
      "pdf_url": "https://arxiv.org/pdf/2601.06502v1",
      "published_date": "2026-01-10 09:31:40 UTC",
      "updated_date": "2026-01-10 09:31:40 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-22T23:05:54.457925+00:00"
    },
    {
      "arxiv_id": "2601.06500v1",
      "title": "The AI Pyramid A Conceptual Framework for Workforce Capability in the Age of AI",
      "title_zh": "AIé‡‘å­—å¡”ï¼šäººå·¥æ™ºèƒ½æ—¶ä»£åŠ³åŠ¨åŠ›èƒ½åŠ›çš„æ¦‚å¿µæ¡†æ¶",
      "authors": [
        "Alok Khatri",
        "Bishesh Khanal"
      ],
      "abstract": "Artificial intelligence (AI) represents a qualitative shift in technological change by extending cognitive labor itself rather than merely automating routine tasks. Recent evidence shows that generative AI disproportionately affects highly educated, white collar work, challenging existing assumptions about workforce vulnerability and rendering traditional approaches to digital or AI literacy insufficient. This paper introduces the concept of AI Nativity, the capacity to integrate AI fluidly into everyday reasoning, problem solving, and decision making, and proposes the AI Pyramid, a conceptual framework for organizing human capability in an AI mediated economy. The framework distinguishes three interdependent capability layers: AI Native capability as a universal baseline for participation in AI augmented environments; AI Foundation capability for building, integrating, and sustaining AI enabled systems; and AI Deep capability for advancing frontier AI knowledge and applications. Crucially, the pyramid is not a career ladder but a system level distribution of capabilities required at scale. Building on this structure, the paper argues that effective AI workforce development requires treating capability formation as infrastructure rather than episodic training, centered on problem based learning embedded in work contexts and supported by dynamic skill ontologies and competency based measurement. The framework has implications for organizations, education systems, and governments seeking to align learning, measurement, and policy with the evolving demands of AI mediated work, while addressing productivity, resilience, and inequality at societal scale.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† AI Pyramid æ¦‚å¿µæ¡†æ¶ï¼Œæ—¨åœ¨åº”å¯¹äººå·¥æ™ºèƒ½(AI)å¯¹é«˜è®¤çŸ¥åŠ³åŠ¨å’Œç™½é¢†å·¥ä½œå¸¦æ¥çš„å®šæ€§å˜é©åŠå¯¹ä¼ ç»Ÿç´ å…»è¦æ±‚çš„æŒ‘æˆ˜ã€‚è®ºæ–‡å¼•å…¥äº† AI Nativityï¼ˆAI åŸç”ŸåŠ›ï¼‰çš„æ¦‚å¿µï¼Œå¹¶æ„å»ºäº†ç”± AI Nativeï¼ˆé€šç”¨åŸºå‡†ï¼‰ã€AI Foundationï¼ˆç³»ç»Ÿæ„å»ºï¼‰å’Œ AI Deepï¼ˆå‰æ²¿ç ”å‘ï¼‰ä¸‰ä¸ªäº’è¡¥å±‚çº§ç»„æˆçš„èƒ½åŠ›é‡‘å­—å¡”ï¼Œç³»ç»Ÿæ€§åœ°ç»„ç»‡äº† AI èµ‹èƒ½ç»æµä¸­çš„äººç±»èƒ½åŠ›åˆ†å¸ƒã€‚ç ”ç©¶å¼ºè°ƒï¼ŒåŠ³åŠ¨åŠ›èƒ½åŠ›çš„å½¢æˆåº”è¢«è§†ä¸ºåŸºç¡€è®¾æ–½è€ŒéçŸ­æœŸåŸ¹è®­ï¼Œä¸»å¼ é€šè¿‡åµŒå…¥å·¥ä½œèƒŒæ™¯çš„â€œä»¥é—®é¢˜ä¸ºå¯¼å‘çš„å­¦ä¹ â€æ¥æ¨åŠ¨èƒ½åŠ›åŸ¹å…»ã€‚è¯¥æ¡†æ¶ä¸ºç»„ç»‡ã€æ•™è‚²ä½“ç³»åŠæ”¿åºœåœ¨ AI æ—¶ä»£é‡æ–°å¯¹é½å­¦ä¹ ã€è¯„ä¼°ä¸æ”¿ç­–æä¾›äº†ç†è®ºæ”¯æ’‘ï¼Œä»¥åº”å¯¹ç¤¾ä¼šè§„æ¨¡çš„ç”Ÿäº§åŠ›ã€éŸ§æ€§ä¸ä¸å¹³ç­‰é—®é¢˜ã€‚",
      "categories": [
        "cs.AI",
        "cs.CY"
      ],
      "primary_category": "cs.AI",
      "comment": "14 pages",
      "pdf_url": "https://arxiv.org/pdf/2601.06500v1",
      "published_date": "2026-01-10 09:27:56 UTC",
      "updated_date": "2026-01-10 09:27:56 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-22T23:05:48.031163+00:00"
    },
    {
      "arxiv_id": "2601.06497v1",
      "title": "Coding in a Bubble? Evaluating LLMs in Resolving Context Adaptation Bugs During Code Adaptation",
      "title_zh": "åœ¨æ°”æ³¡ä¸­ç¼–ç ï¼Ÿè¯„ä¼°å¤§è¯­è¨€æ¨¡å‹åœ¨ä»£ç é€‚é…ä¸­è§£å†³ä¸Šä¸‹æ–‡é€‚é…ç¼ºé™·çš„èƒ½åŠ›",
      "authors": [
        "Tanghaoran Zhang",
        "Xinjun Mao",
        "Shangwen Wang",
        "Yuxin Zhao",
        "Yao Lu",
        "Zezhou Tang",
        "Wenyu Xu",
        "Longfei Sun",
        "Changrong Xie",
        "Kang Yang",
        "Yue Yu"
      ],
      "abstract": "Code adaptation is a fundamental but challenging task in software development, requiring developers to modify existing code for new contexts. A key challenge is to resolve Context Adaptation Bugs (CtxBugs), which occurs when code correct in its original context violates constraints in the target environment. Unlike isolated bugs, CtxBugs cannot be resolved through local fixes and require cross-context reasoning to identify semantic mismatches. Overlooking them may lead to critical failures in adaptation. Although Large Language Models (LLMs) show great potential in automating code-related tasks, their ability to resolve CtxBugs remains a significant and unexplored obstacle to their practical use in code adaptation. To bridge this gap, we propose CtxBugGen, a novel framework for generating CtxBugs to evaluate LLMs. Its core idea is to leverage LLMs' tendency to generate plausible but context-free code when contextual constraints are absent. The framework generates CtxBugs through a four-step process to ensure their relevance and validity: (1) Adaptation Task Selection, (2) Task-specific Perturbation,(3) LLM-based Variant Generation and (4) CtxBugs Identification. Based on the benchmark constructed by CtxBugGen, we conduct an empirical study with four state-of-the-art LLMs. Our results reveal their unsatisfactory performance in CtxBug resolution. The best performing LLM, Kimi-K2, achieves 55.93% on Pass@1 and resolves just 52.47% of CtxBugs. The presence of CtxBugs degrades LLMs' adaptation performance by up to 30%. Failure analysis indicates that LLMs often overlook CtxBugs and replicate them in their outputs. Our study highlights a critical weakness in LLMs' cross-context reasoning and emphasize the need for new methods to enhance their context awareness for reliable code adaptation.",
      "tldr_zh": "è¯¥ç ”ç©¶è¯„ä¼°äº†å¤§è¯­è¨€æ¨¡å‹(LLMs)åœ¨ä»£ç é€‚é…è¿‡ç¨‹ä¸­è§£å†³â€œä¸Šä¸‹æ–‡é€‚é…é”™è¯¯â€(Context Adaptation Bugs, CtxBugs)çš„èƒ½åŠ›ï¼Œè¿™ç±»é”™è¯¯é€šå¸¸æºäºä»£ç åœ¨è¿ç§»è‡³æ–°ç¯å¢ƒæ—¶è¿åäº†ç‰¹å®šçš„è¯­ä¹‰çº¦æŸã€‚ä¸ºäº†æ·±å…¥æ¢ç©¶è¿™ä¸€é¢†åŸŸï¼Œä½œè€…æå‡ºäº† CtxBugGen æ¡†æ¶ï¼Œé€šè¿‡é€‚é…ä»»åŠ¡é€‰æ‹©ã€ä»»åŠ¡æ‰°åŠ¨ã€å˜ä½“ç”Ÿæˆå’Œé”™è¯¯è¯†åˆ«å››ä¸ªæ­¥éª¤ï¼Œè‡ªåŠ¨æ„å»ºç”¨äºè¯„ä¼° LLMs çš„åŸºå‡†æµ‹è¯•é›†ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œå½“å‰ä¸»æµ LLMs åœ¨å¤„ç† CtxBugs æ—¶è¡¨ç°æ¬ ä½³ï¼Œè¡¨ç°æœ€å¥½çš„ Kimi-K2 åœ¨ Pass@1 æŒ‡æ ‡ä¸Šä»…ä¸º 55.93%ï¼Œä¸”æ­¤ç±»é”™è¯¯ä¼šå¯¼è‡´æ¨¡å‹é€‚é…æ€§èƒ½ä¸‹é™å¤šè¾¾ 30%ã€‚è¯¥ç ”ç©¶æ­ç¤ºäº† LLMs åœ¨è·¨ä¸Šä¸‹æ–‡æ¨ç†(cross-context reasoning)æ–¹é¢çš„æ˜¾è‘—å±€é™æ€§ï¼Œå¹¶å¼ºè°ƒäº†å¢å¼ºæ¨¡å‹ä¸Šä¸‹æ–‡æ„ŸçŸ¥èƒ½åŠ›å¯¹å®ç°å¯é ä»£ç é€‚é…çš„é‡è¦æ€§ã€‚",
      "categories": [
        "cs.SE",
        "cs.AI"
      ],
      "primary_category": "cs.SE",
      "comment": "24 pages, 11 figures, accepted by FSE 2026",
      "pdf_url": "https://arxiv.org/pdf/2601.06497v1",
      "published_date": "2026-01-10 09:14:00 UTC",
      "updated_date": "2026-01-10 09:14:00 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-22T23:05:54.049635+00:00"
    },
    {
      "arxiv_id": "2601.06487v1",
      "title": "ArenaRL: Scaling RL for Open-Ended Agents via Tournament-based Relative Ranking",
      "title_zh": "ArenaRLï¼šåŸºäºé”¦æ ‡èµ›ç›¸å¯¹æ’åçš„å¼€æ”¾å¼æ™ºèƒ½ä½“å¼ºåŒ–å­¦ä¹ æ‰©å±•",
      "authors": [
        "Qiang Zhang",
        "Boli Chen",
        "Fanrui Zhang",
        "Ruixue Ding",
        "Shihang Wang",
        "Qiuchen Wang",
        "Yinfeng Huang",
        "Haonan Zhang",
        "Rongxiang Zhu",
        "Pengyong Wang",
        "Ailin Ren",
        "Xin Li",
        "Pengjun Xie",
        "Jiawei Liu",
        "Ning Guo",
        "Jingren Zhou",
        "Zheng-Jun Zha"
      ],
      "abstract": "Reinforcement learning has substantially improved the performance of LLM agents on tasks with verifiable outcomes, but it still struggles on open-ended agent tasks with vast solution spaces (e.g., complex travel planning). Due to the absence of objective ground-truth for these tasks, current RL algorithms largely rely on reward models that assign scalar scores to individual responses. We contend that such pointwise scoring suffers from an inherent discrimination collapse: the reward model struggles to distinguish subtle advantages among different trajectories, resulting in scores within a group being compressed into a narrow range. Consequently, the effective reward signal becomes dominated by noise from the reward model, leading to optimization stagnation. To address this, we propose ArenaRL, a reinforcement learning paradigm that shifts from pointwise scalar scoring to intra-group relative ranking. ArenaRL introduces a process-aware pairwise evaluation mechanism, employing multi-level rubrics to assign fine-grained relative scores to trajectories. Additionally, we construct an intra-group adversarial arena and devise a tournament-based ranking scheme to obtain stable advantage signals. Empirical results confirm that the built seeded single-elimination scheme achieves nearly equivalent advantage estimation accuracy to full pairwise comparisons with O(N^2) complexity, while operating with only O(N) complexity, striking an optimal balance between efficiency and precision. Furthermore, to address the lack of full-cycle benchmarks for open-ended agents, we build Open-Travel and Open-DeepResearch, two high-quality benchmarks featuring a comprehensive pipeline covering SFT, RL training, and multi-dimensional evaluation. Extensive experiments show that ArenaRL substantially outperforms standard RL baselines, enabling LLM agents to generate more robust solutions for complex real-world tasks.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¼ºåŒ–å­¦ä¹ (Reinforcement Learning)åœ¨å¤„ç†å¤æ‚æ—…æ¸¸è§„åˆ’ç­‰å¼€æ”¾å¼æ™ºèƒ½ä½“ä»»åŠ¡æ—¶é¢ä¸´çš„â€œåˆ¤åˆ«åç¼©â€(discrimination collapse)é—®é¢˜ï¼Œæå‡ºäº†ArenaRLæ¡†æ¶ã€‚è¯¥æ¡†æ¶å°†ä¼ ç»Ÿçš„å•ç‚¹æ ‡é‡è¯„åˆ†(pointwise scalar scoring)è½¬å˜ä¸ºç»„å†…ç›¸å¯¹æ’åº(intra-group relative ranking)èŒƒå¼ï¼Œé€šè¿‡è¿‡ç¨‹æ„ŸçŸ¥æˆå¯¹è¯„ä¼°(process-aware pairwise evaluation)å’Œå¤šçº§å‡†åˆ™å¯¹è½¨è¿¹è¿›è¡Œç»†ç²’åº¦è¯„åˆ†ã€‚ä¸ºäº†æé«˜æ•ˆç‡ï¼ŒArenaRLå¼•å…¥äº†åŸºäºé”¦æ ‡èµ›çš„æ’åæœºåˆ¶(tournament-based ranking)ï¼Œåœ¨ä»…æœ‰ $O(N)$ å¤æ‚åº¦çš„æƒ…å†µä¸‹å®ç°äº†ä¸å…¨æˆå¯¹æ¯”è¾ƒç›¸è¿‘çš„ä¼˜åŠ¿ä¼°è®¡ç²¾åº¦ã€‚æ­¤å¤–ï¼Œç ”ç©¶è€…è¿˜æ„å»ºäº†Open-Travelå’ŒOpen-DeepResearchä¸¤ä¸ªé«˜è´¨é‡åŸºå‡†æµ‹è¯•é›†ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒArenaRLæ˜¾è‘—ä¼˜äºæ ‡å‡†å¼ºåŒ–å­¦ä¹ åŸºå‡†ï¼Œä½¿å¤§è¯­è¨€æ¨¡å‹(LLM)æ™ºèƒ½ä½“åœ¨å¤„ç†å¤æ‚çš„ç°å®ä»»åŠ¡æ—¶èƒ½å¤Ÿç”Ÿæˆæ›´å…·é²æ£’æ€§çš„è§£å†³æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2601.06487v1",
      "published_date": "2026-01-10 08:43:07 UTC",
      "updated_date": "2026-01-10 08:43:07 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-22T23:05:53.163592+00:00"
    },
    {
      "arxiv_id": "2601.06484v1",
      "title": "Learning Domain Agnostic Latent Embeddings of 3D Faces for Zero-shot Animal Expression Transfer",
      "title_zh": "é¢å‘é›¶æ ·æœ¬åŠ¨ç‰©è¡¨æƒ…è¿ç§»çš„3Däººè„¸é¢†åŸŸæ— å…³æ½œåœ¨åµŒå…¥å­¦ä¹ ",
      "authors": [
        "Yue Wang",
        "Lawrence Amadi",
        "Xiang Gao",
        "Yazheng Chen",
        "Yuanpeng Liu",
        "Ning Lu",
        "Xianfeng Gu"
      ],
      "abstract": "We present a zero-shot framework for transferring human facial expressions to 3D animal face meshes. Our method combines intrinsic geometric descriptors (HKS/WKS) with a mesh-agnostic latent embedding that disentangles facial identity and expression. The ID latent space captures species-independent facial structure, while the expression latent space encodes deformation patterns that generalize across humans and animals. Trained only with human expression pairs, the model learns the embeddings, decoupling, and recoupling of cross-identity expressions, enabling expression transfer without requiring animal expression data. To enforce geometric consistency, we employ Jacobian loss together with vertex-position and Laplacian losses. Experiments show that our approach achieves plausible cross-species expression transfer, effectively narrowing the geometric gap between human and animal facial shapes.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§ç”¨äº 3D åŠ¨ç‰©è¡¨æƒ…è¿ç§»çš„é›¶æ ·æœ¬(zero-shot)æ¡†æ¶ï¼Œé€šè¿‡ç»“åˆå†…åœ¨å‡ ä½•æè¿°ç¬¦ï¼ˆHKS/WKSï¼‰å’Œç½‘æ ¼æ— å…³çš„æ½œåœ¨åµŒå…¥(latent embedding)ï¼Œå®ç°äº†äººç±»é¢éƒ¨èº«ä»½(identity)ä¸è¡¨æƒ…(expression)çš„æœ‰æ•ˆè§£è€¦ã€‚å…¶æ ¸å¿ƒåœ¨äºåˆ©ç”¨æ½œåœ¨ç©ºé—´æ•æ‰è·¨ç‰©ç§é€šç”¨çš„å˜å½¢æ¨¡å¼ï¼Œä½¿å¾—æ¨¡å‹åœ¨ä»…æ¥å—äººç±»æ•°æ®è®­ç»ƒçš„æƒ…å†µä¸‹ï¼Œå³å¯å°†è¡¨æƒ…è¿ç§»è‡³ä»æœªè§è¿‡çš„åŠ¨ç‰©é¢éƒ¨ã€‚ä¸ºäº†ç»´æŒå‡ ä½•ä¸€è‡´æ€§ï¼Œç ”ç©¶å¼•å…¥äº† Jacobian lossã€é¡¶ç‚¹ä½ç½®åŠ Laplacian loss è¿›è¡Œçº¦æŸã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿç”Ÿæˆè‡ªç„¶çš„è·¨ç‰©ç§è¡¨æƒ…æ•ˆæœï¼ŒæˆåŠŸå¼¥åˆäº†äººç±»ä¸åŠ¨ç‰©é¢éƒ¨å½¢çŠ¶ä¹‹é—´çš„å‡ ä½•é¸¿æ²Ÿã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "WACV 2026 Workshop LENS",
      "pdf_url": "https://arxiv.org/pdf/2601.06484v1",
      "published_date": "2026-01-10 08:37:02 UTC",
      "updated_date": "2026-01-10 08:37:02 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-22T23:05:59.983933+00:00"
    },
    {
      "arxiv_id": "2601.06475v1",
      "title": "VVTRec: Radio Interferometric Reconstruction through Visual and Textual Modality Enrichment",
      "title_zh": "VVTRecï¼šåŸºäºè§†è§‰ä¸æ–‡æœ¬æ¨¡æ€å¢å¼ºçš„å°„ç”µå¹²æ¶‰é‡å»º",
      "authors": [
        "Kai Cheng",
        "Ruoqi Wang",
        "Qiong Luo"
      ],
      "abstract": "Radio astronomy is an indispensable discipline for observing distant celestial objects. Measurements of wave signals from radio telescopes, called visibility, need to be transformed into images for astronomical observations. These dirty images blend information from real sources and artifacts. Therefore, astronomers usually perform reconstruction before imaging to obtain cleaner images. Existing methods consider only a single modality of sparse visibility data, resulting in images with remaining artifacts and insufficient modeling of correlation. To enhance the extraction of visibility information and emphasize output quality in the image domain, we propose VVTRec, a multimodal radio interferometric data reconstruction method with visibility-guided visual and textual modality enrichment. In our VVTRec, sparse visibility is transformed into image-form and text-form features to obtain enhancements in terms of spatial and semantic information, improving the structural integrity and accuracy of images. Also, we leverage Vision-Language Models (VLMs) to achieve additional training-free performance improvements. VVTRec enables sparse visibility, as a foreign modality unseen by VLMs, to accurately extract pre-trained knowledge as a supplement. Our experiments demonstrate that VVTRec effectively enhances imaging results by exploiting multimodal information without introducing excessive computational overhead.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† VVTRecï¼Œä¸€ç§é€šè¿‡è§†è§‰å’Œæ–‡æœ¬æ¨¡æ€å¢å¼ºçš„å°„ç”µå¹²æ¶‰(Radio Interferometric)æ•°æ®é‡å»ºæ–¹æ³•ï¼Œæ—¨åœ¨è§£å†³å•æ¨¡æ€ç¨€ç– Visibility æ•°æ®å¯¼è‡´çš„å›¾åƒä¼ªå½±åŠå…³è”å»ºæ¨¡ä¸è¶³çš„é—®é¢˜ã€‚è¯¥æ–¹æ³•å°†ç¨€ç– Visibility è½¬åŒ–ä¸ºå›¾åƒå½¢å¼å’Œæ–‡æœ¬å½¢å¼çš„ç‰¹å¾ï¼Œä»¥å¢å¼ºç©ºé—´å’Œè¯­ä¹‰ä¿¡æ¯ï¼Œä»è€Œæå‡å›¾åƒçš„ç»“æ„å®Œæ•´æ€§ä¸å‡†ç¡®æ€§ã€‚æ­¤å¤–ï¼ŒVVTRec åˆ›æ–°æ€§åœ°åˆ©ç”¨è§†è§‰è¯­è¨€æ¨¡å‹(VLMs)åœ¨æ— éœ€é¢å¤–è®­ç»ƒçš„æƒ…å†µä¸‹æå–é¢„è®­ç»ƒçŸ¥è¯†ï¼Œä½œä¸ºå¤„ç† Visibility è¿™ä¸€ç‰¹å®šæ¨¡æ€çš„æœ‰æ•ˆè¡¥å……ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒVVTRec åœ¨ä¸å¼•å…¥è¿‡å¤šè®¡ç®—å¼€é”€çš„å‰æä¸‹ï¼Œæ˜¾è‘—æå‡äº†å°„ç”µå¤©æ–‡æˆåƒçš„è´¨é‡ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2601.06475v1",
      "published_date": "2026-01-10 07:56:17 UTC",
      "updated_date": "2026-01-10 07:56:17 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-22T23:05:58.927978+00:00"
    },
    {
      "arxiv_id": "2601.06474v2",
      "title": "SparseOccVLA: Bridging Occupancy and Vision-Language Models via Sparse Queries for Unified 4D Scene Understanding and Planning",
      "title_zh": "SparseOccVLAï¼šé€šè¿‡ç¨€ç–æŸ¥è¯¢æ¡¥æ¥å æ®è¡¨ç¤ºä¸è§†è§‰è¯­è¨€æ¨¡å‹ï¼Œå®ç°ç»Ÿä¸€çš„ 4D åœºæ™¯ç†è§£ä¸è§„åˆ’",
      "authors": [
        "Chenxu Dang",
        "Jie Wang",
        "Guang Li",
        "Zhiwen Hou",
        "Zihan You",
        "Hangjun Ye",
        "Jie Ma",
        "Long Chen",
        "Yan Wang"
      ],
      "abstract": "In autonomous driving, Vision Language Models (VLMs) excel at high-level reasoning , whereas semantic occupancy provides fine-grained details. Despite significant progress in individual fields, there is still no method that can effectively integrate both paradigms. Conventional VLMs struggle with token explosion and limited spatiotemporal reasoning, while semantic occupancy provides a unified, explicit spatial representation but is too dense to integrate efficiently with VLMs. To address these challenges and bridge the gap between VLMs and occupancy, we propose SparseOccVLA, a novel vision-language-action model that unifies scene understanding, occupancy forecasting, and trajectory planning powered by sparse occupancy queries. Starting with a lightweight Sparse Occupancy Encoder, SparseOccVLA generates compact yet highly informative sparse occupancy queries that serve as the single bridge between vision and language. These queries are aligned into the language space and reasoned by the LLM for unified scene understanding and future occupancy forecasting. Furthermore, we introduce an LLM-guided Anchor-Diffusion Planner featuring decoupled anchor scoring and denoising, as well as cross-model trajectory-condition fusion. SparseOccVLA achieves a 7% relative improvement in CIDEr over the state-of-the-art on OmniDrive-nuScenes, a 0.5 increase in mIoU score on Occ3D-nuScenes, and sets state-of-the-art open-loop planning metric on nuScenes benchmark, demonstrating its strong holistic capability.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† SparseOccVLAï¼Œè¿™æ˜¯ä¸€ç§æ—¨åœ¨ç»Ÿä¸€åœºæ™¯ç†è§£ã€å æ®é¢„æµ‹å’Œè§„åˆ’çš„è§†è§‰-è¯­è¨€-åŠ¨ä½œæ¨¡å‹ï¼Œé€šè¿‡ Sparse Queries æœ‰æ•ˆæ¡¥æ¥äº† Vision-Language Models (VLMs) ä¸è¯­ä¹‰å æ® (Semantic Occupancy)ã€‚ä¸ºäº†è§£å†³ä¼ ç»Ÿæ¨¡å‹è®¡ç®—æ•ˆç‡ä½ä¸‹åŠç©ºé—´æ¨ç†å—é™çš„é—®é¢˜ï¼Œè¯¥æ¨¡å‹åˆ©ç”¨ Sparse Occupancy Encoder ç”Ÿæˆç´§å‡‘çš„æŸ¥è¯¢å‘é‡ï¼Œä½œä¸ºè§†è§‰ä¸è¯­è¨€ç©ºé—´çš„å”¯ä¸€æ¡¥æ¢ã€‚æ­¤å¤–ï¼Œç ”ç©¶å¼•å…¥äº† LLM-guided Anchor-Diffusion Plannerï¼Œé€šè¿‡è§£è€¦é”šç‚¹è¯„åˆ†ä¸å»å™ªå®ç°äº†æ›´ç²¾å‡†çš„è½¨è¿¹è§„åˆ’ã€‚å®éªŒè¡¨æ˜ï¼ŒSparseOccVLA åœ¨ OmniDrive-nuScenes çš„ CIDEr æŒ‡æ ‡ä¸Šæå‡äº† 7%ï¼Œå¹¶åˆ·æ–°äº† nuScenes å¼€ç¯è§„åˆ’ä»»åŠ¡çš„ SOTA è®°å½•ï¼Œå±•ç¤ºäº†å…¶å“è¶Šçš„å…¨æ™¯æ„ŸçŸ¥ä¸å†³ç­–èƒ½åŠ›ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2601.06474v2",
      "published_date": "2026-01-10 07:54:20 UTC",
      "updated_date": "2026-01-17 04:01:42 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-22T23:06:03.946741+00:00"
    },
    {
      "arxiv_id": "2601.06471v1",
      "title": "PRISP: Privacy-Safe Few-Shot Personalization via Lightweight Adaptation",
      "title_zh": "PRISPï¼šåŸºäºè½»é‡åŒ–é€‚é…çš„éšç§å®‰å…¨å°‘æ ·æœ¬ä¸ªæ€§åŒ–",
      "authors": [
        "Junho Park",
        "Dohoon Kim",
        "Taesup Moon"
      ],
      "abstract": "Large language model (LLM) personalization aims to adapt general-purpose models to individual users. Most existing methods, however, are developed under data-rich and resource-abundant settings, often incurring privacy risks. In contrast, realistic personalization typically occurs after deployment under (i) extremely limited user data, (ii) constrained computational resources, and (iii) strict privacy requirements. We propose PRISP, a lightweight and privacy-safe personalization framework tailored to these constraints. PRISP leverages a Text-to-LoRA hypernetwork to generate task-aware LoRA parameters from task descriptions, and enables efficient user personalization by optimizing a small subset of task-aware LoRA parameters together with minimal additional modules using few-shot user data. Experiments on a few-shot variant of the LaMP benchmark demonstrate that PRISP achieves strong overall performance compared to prior approaches, while reducing computational overhead and eliminating privacy risks.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† PRISPï¼Œä¸€ä¸ªæ—¨åœ¨è§£å†³å¤§è¯­è¨€æ¨¡å‹(LLM)ä¸ªæ€§åŒ–è¿‡ç¨‹ä¸­æ•°æ®ç¨€ç¼ºã€èµ„æºå—é™åŠéšç§é£é™©é—®é¢˜çš„è½»é‡çº§æ¡†æ¶ã€‚PRISP åˆ©ç”¨ Text-to-LoRA è¶…ç½‘ç»œ(hypernetwork)ä»ä»»åŠ¡æè¿°ä¸­ç”Ÿæˆä»»åŠ¡æ„ŸçŸ¥(task-aware)çš„ LoRA å‚æ•°ï¼Œå¹¶é€šè¿‡æå°‘é‡çš„ç”¨æˆ·æ•°æ®ä¼˜åŒ–è¿™äº›å‚æ•°çš„å­é›†åŠé™„åŠ æ¨¡å—ï¼Œå®ç°é«˜æ•ˆé€‚é…ã€‚åœ¨ LaMP åŸºå‡†æµ‹è¯•çš„ few-shot å˜ä½“ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼ŒPRISP åœ¨æ˜¾è‘—é™ä½è®¡ç®—å¼€é”€å¹¶æ¶ˆé™¤éšç§é£é™©çš„åŒæ—¶ï¼Œæ€§èƒ½è¡¨ç°ä¼˜äºç°æœ‰çš„å…ˆè¿›æ–¹æ³•ã€‚è¯¥æ–¹æ¡ˆä¸ºçœŸå®åº”ç”¨åœºæ™¯ä¸‹ã€éƒ¨ç½²åçš„ä¸ªæ€§åŒ–æ¨¡å‹é€‚é…æä¾›äº†ä¸€ç§å®‰å…¨ä¸”è½»é‡çš„æŠ€æœ¯è·¯å¾„ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "16 pages, 9 figures",
      "pdf_url": "https://arxiv.org/pdf/2601.06471v1",
      "published_date": "2026-01-10 07:34:28 UTC",
      "updated_date": "2026-01-10 07:34:28 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-22T23:06:05.571778+00:00"
    },
    {
      "arxiv_id": "2601.14274v1",
      "title": "Divide and Refine: Enhancing Multimodal Representation and Explainability for Emotion Recognition in Conversation",
      "title_zh": "Divide and Refineï¼šæå‡ä¼šè¯æƒ…æ„Ÿè¯†åˆ«çš„å¤šæ¨¡æ€è¡¨å¾ä¸å¯è§£é‡Šæ€§",
      "authors": [
        "Anh-Tuan Mai",
        "Cam-Van Thi Nguyen",
        "Duc-Trong Le"
      ],
      "abstract": "Multimodal emotion recognition in conversation (MERC) requires representations that effectively integrate signals from multiple modalities. These signals include modality-specific cues, information shared across modalities, and interactions that emerge only when modalities are combined. In information-theoretic terms, these correspond to \\emph{unique}, \\emph{redundant}, and \\emph{synergistic} contributions. An ideal representation should leverage all three, yet achieving such balance remains challenging. Recent advances in contrastive learning and augmentation-based methods have made progress, but they often overlook the role of data preparation in preserving these components. In particular, applying augmentations directly to raw inputs or fused embeddings can blur the boundaries between modality-unique and cross-modal signals. To address this challenge, we propose a two-phase framework \\emph{\\textbf{D}ivide and \\textbf{R}efine} (\\textbf{DnR}). In the \\textbf{Divide} phase, each modality is explicitly decomposed into uniqueness, pairwise redundancy, and synergy. In the \\textbf{Refine} phase, tailored objectives enhance the informativeness of these components while maintaining their distinct roles. The refined representations are plug-and-play compatible with diverse multimodal pipelines. Extensive experiments on IEMOCAP and MELD demonstrate consistent improvements across multiple MERC backbones. These results highlight the effectiveness of explicitly dividing, refining, and recombining multimodal representations as a principled strategy for advancing emotion recognition. Our implementation is available at https://github.com/mattam301/DnR-WACV2026",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†åä¸º Divide and Refine (DnR) çš„ä¸¤é˜¶æ®µæ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³å¤šæ¨¡æ€å¯¹è¯æƒ…æ„Ÿè¯†åˆ« (MERC) ä¸­éš¾ä»¥æœ‰æ•ˆæ•´åˆæ¨¡æ€ç‰¹æœ‰ (unique)ã€å†—ä½™ (redundant) åŠååŒ (synergy) ä¿¡å·çš„æŒ‘æˆ˜ã€‚åœ¨ Divide é˜¶æ®µï¼Œæ¡†æ¶å°†å„æ¨¡æ€æ˜¾å¼åˆ†è§£ä¸ºç‹¬ç‰¹æ€§ (uniqueness)ã€æˆå¯¹å†—ä½™ (pairwise redundancy) å’ŒååŒæ€§ (synergy)ï¼›åœ¨ Refine é˜¶æ®µï¼Œé€šè¿‡å®šåˆ¶åŒ–ç›®æ ‡å‡½æ•°å¢å¼ºè¿™äº›ç»„ä»¶çš„ä¿¡æ¯é‡å¹¶ä¿æŒå…¶ç‹¬ç«‹æ€§ã€‚DnR å…·æœ‰å³æ’å³ç”¨ (plug-and-play) çš„ç‰¹æ€§ï¼Œå¯é€‚é…å¤šç§å¤šæ¨¡æ€æµæ°´çº¿ã€‚åœ¨ IEMOCAP å’Œ MELD æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•æ˜¾è‘—æå‡äº†å¤šç§ MERC ä¸»å¹²æ¨¡å‹çš„æ€§èƒ½ï¼Œè¯æ˜äº†æ˜¾å¼åˆ†è§£ä¸ç²¾ç‚¼å¤šæ¨¡æ€è¡¨ç¤ºåœ¨æå‡æƒ…æ„Ÿè¯†åˆ«å‡†ç¡®ç‡å’Œå¯è§£é‡Šæ€§æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2601.14274v1",
      "published_date": "2026-01-10 07:30:20 UTC",
      "updated_date": "2026-01-10 07:30:20 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-22T23:06:09.144569+00:00"
    },
    {
      "arxiv_id": "2601.06460v1",
      "title": "Tone Matters: The Impact of Linguistic Tone on Hallucination in VLMs",
      "title_zh": "è¯­æ°”è‡³å…³é‡è¦ï¼šè¯­è¨€è¯­æ°”å¯¹è§†è§‰è¯­è¨€æ¨¡å‹å¹»è§‰çš„å½±å“",
      "authors": [
        "Weihao Hong",
        "Zhiyuan Jiang",
        "Bingyu Shen",
        "Xinlei Guan",
        "Yangyi Feng",
        "Meng Xu",
        "Boyang Li"
      ],
      "abstract": "Vision-Language Models (VLMs) are increasingly used in safety-critical applications that require reliable visual grounding. However, these models often hallucinate details that are not present in the image to satisfy user prompts. While recent datasets and benchmarks have been introduced to evaluate systematic hallucinations in VLMs, many hallucination behaviors remain insufficiently characterized. In particular, prior work primarily focuses on object presence or absence, leaving it unclear how prompt phrasing and structural constraints can systematically induce hallucinations. In this paper, we investigate how different forms of prompt pressure influence hallucination behavior. We introduce Ghost-100, a procedurally generated dataset of synthetic scenes in which key visual details are deliberately removed, enabling controlled analysis of absence-based hallucinations. Using a structured 5-Level Prompt Intensity Framework, we vary prompts from neutral queries to toxic demands and rigid formatting constraints. We evaluate three representative open-weight VLMs: MiniCPM-V 2.6-8B, Qwen2-VL-7B, and Qwen3-VL-8B. Across all three models, hallucination rates do not increase monotonically with prompt intensity. All models exhibit reductions at higher intensity levels at different thresholds, though not all show sustained reduction under maximum coercion. These results suggest that current safety alignment is more effective at detecting semantic hostility than structural coercion, revealing model-specific limitations in handling compliance pressure. Our dataset is available at: https://github.com/bli1/tone-matters",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†è¯­è¨€è¯­æ°”(Linguistic Tone)å’Œæç¤ºå‹åŠ›(Prompt Pressure)å¯¹è§†è§‰è¯­è¨€æ¨¡å‹(VLMs)å¹»è§‰(Hallucination)è¡Œä¸ºçš„å½±å“ã€‚ç ”ç©¶è€…é€šè¿‡ç¨‹åºåŒ–ç”Ÿæˆçš„åˆæˆåœºæ™¯æ•°æ®é›† Ghost-100ï¼Œåˆ©ç”¨â€œ5çº§æç¤ºå¼ºåº¦æ¡†æ¶â€(5-Level Prompt Intensity Framework)å¯¹ MiniCPM-Vã€Qwen2-VL å’Œ Qwen3-VL ç­‰æ¨¡å‹è¿›è¡Œäº†å—æ§åˆ†æã€‚å®éªŒå‘ç°ï¼Œå¹»è§‰ç‡å¹¶ä¸éšæç¤ºå¼ºåº¦å•è°ƒå¢åŠ ï¼Œå„æ¨¡å‹åœ¨é«˜å¼ºåº¦æç¤ºä¸‹å‡è¡¨ç°å‡ºä¸åŒç¨‹åº¦çš„å¹»è§‰å‡å°‘ç°è±¡ã€‚ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œç°æœ‰çš„å®‰å…¨å¯¹é½(Safety Alignment)åœ¨æ£€æµ‹è¯­ä¹‰æ•Œæ„æ–¹é¢æ¯”åº”å¯¹ç»“æ„æ€§å¼ºè¿«æ›´æœ‰æ•ˆï¼Œæ­ç¤ºäº†æ¨¡å‹åœ¨å¤„ç†åˆè§„å‹åŠ›æ—¶çš„ç‰¹å®šå±€é™æ€§ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.CV",
      "comment": "10 pages, 6 figures, WACV Workshop",
      "pdf_url": "https://arxiv.org/pdf/2601.06460v1",
      "published_date": "2026-01-10 07:00:22 UTC",
      "updated_date": "2026-01-10 07:00:22 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-22T23:06:20.971124+00:00"
    },
    {
      "arxiv_id": "2601.06453v1",
      "title": "ConSensus: Multi-Agent Collaboration for Multimodal Sensing",
      "title_zh": "ConSensusï¼šé¢å‘å¤šæ¨¡æ€æ„ŸçŸ¥çš„å¤šæ™ºèƒ½ä½“åä½œ",
      "authors": [
        "Hyungjun Yoon",
        "Mohammad Malekzadeh",
        "Sung-Ju Lee",
        "Fahim Kawsar",
        "Lorena Qendro"
      ],
      "abstract": "Large language models (LLMs) are increasingly grounded in sensor data to perceive and reason about human physiology and the physical world. However, accurately interpreting heterogeneous multimodal sensor data remains a fundamental challenge. We show that a single monolithic LLM often fails to reason coherently across modalities, leading to incomplete interpretations and prior-knowledge bias. We introduce ConSensus, a training-free multi-agent collaboration framework that decomposes multimodal sensing tasks into specialized, modality-aware agents. To aggregate agent-level interpretations, we propose a hybrid fusion mechanism that balances semantic aggregation, which enables cross-modal reasoning and contextual understanding, with statistical consensus, which provides robustness through agreement across modalities. While each approach has complementary failure modes, their combination enables reliable inference under sensor noise and missing data. We evaluate ConSensus on five diverse multimodal sensing benchmarks, demonstrating an average accuracy improvement of 7.1% over the single-agent baseline. Furthermore, ConSensus matches or exceeds the performance of iterative multi-agent debate methods while achieving a 12.7 times reduction in average fusion token cost through a single-round hybrid fusion protocol, yielding a robust and efficient solution for real-world multimodal sensing tasks.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† ConSensusï¼Œä¸€ç§æ— éœ€è®­ç»ƒçš„å¤šæ™ºèƒ½ä½“åä½œæ¡†æ¶ (Multi-Agent Collaboration Framework)ï¼Œæ—¨åœ¨è§£å†³å¤§å‹è¯­è¨€æ¨¡å‹ (LLMs) åœ¨è§£é‡Šå¼‚æ„å¤šæ¨¡æ€ä¼ æ„Ÿå™¨æ•°æ®æ—¶é¢ä¸´çš„æ¨ç†ä¸è¿è´¯å’Œå…ˆéªŒçŸ¥è¯†åè§ç­‰æŒ‘æˆ˜ã€‚è¯¥æ¡†æ¶å°†æ„ŸçŸ¥ä»»åŠ¡åˆ†è§£ä¸ºä¸“é—¨çš„æ¨¡æ€æ„ŸçŸ¥æ™ºèƒ½ä½“ (Modality-Aware Agents)ï¼Œå¹¶é€šè¿‡ä¸€ç§ç»“åˆäº†è¯­ä¹‰èšåˆä¸ç»Ÿè®¡å…±è¯†çš„æ··åˆèåˆæœºåˆ¶ (Hybrid Fusion Mechanism)ï¼Œç¡®ä¿åœ¨ä¼ æ„Ÿå™¨å™ªå£°å’Œæ•°æ®ç¼ºå¤±æƒ…å†µä¸‹çš„æ¨ç†é²æ£’æ€§ã€‚å®éªŒè¡¨æ˜ï¼ŒConSensus åœ¨äº”ä¸ªå¤šæ¨¡æ€æ„ŸçŸ¥åŸºå‡†æµ‹è¯•ä¸Šçš„å¹³å‡å‡†ç¡®ç‡æ¯”å•æ™ºèƒ½ä½“åŸºçº¿æé«˜äº† 7.1%ã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•åœ¨æ€§èƒ½åŒ¹é…æˆ–è¶…è¶Šè¿­ä»£è¾©è®ºæ–¹æ³•çš„åŒæ—¶ï¼Œå°†å¹³å‡èåˆ Token æˆæœ¬é™ä½äº† 12.7 å€ï¼Œä¸ºç°å®ä¸–ç•Œçš„å¤šæ¨¡æ€æ„ŸçŸ¥ä»»åŠ¡æä¾›äº†ä¸€ç§é«˜æ•ˆä¸”å¯é çš„è§£å†³æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "17 pages, 6 figures, 5 tables",
      "pdf_url": "https://arxiv.org/pdf/2601.06453v1",
      "published_date": "2026-01-10 06:41:01 UTC",
      "updated_date": "2026-01-10 06:41:01 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-22T23:06:21.668803+00:00"
    },
    {
      "arxiv_id": "2601.06445v1",
      "title": "LitVISTA: A Benchmark for Narrative Orchestration in Literary Text",
      "title_zh": "LitVISTAï¼šæ–‡å­¦æ–‡æœ¬å™äº‹ç¼–æ’åŸºå‡†",
      "authors": [
        "Mingzhe Lu",
        "Yiwen Wang",
        "Yanbing Liu",
        "Qi You",
        "Chong Liu",
        "Ruize Qin",
        "Haoyu Dong",
        "Wenyu Zhang",
        "Jiarui Zhang",
        "Yue Hu",
        "Yunpeng Li"
      ],
      "abstract": "Computational narrative analysis aims to capture rhythm, tension, and emotional dynamics in literary texts. Existing large language models can generate long stories but overly focus on causal coherence, neglecting the complex story arcs and orchestration inherent in human narratives. This creates a structural misalignment between model- and human-generated narratives. We propose VISTA Space, a high-dimensional representational framework for narrative orchestration that unifies human and model narrative perspectives. We further introduce LitVISTA, a structurally annotated benchmark grounded in literary texts, enabling systematic evaluation of models' narrative orchestration capabilities. We conduct oracle evaluations on a diverse selection of frontier LLMs, including GPT, Claude, Grok, and Gemini. Results reveal systematic deficiencies: existing models fail to construct a unified global narrative view, struggling to jointly capture narrative function and structure. Furthermore, even advanced thinking modes yield only limited gains for such literary narrative understanding.",
      "tldr_zh": "---\n### è®ºæ–‡æ€»ç»“ ğŸ“\n\nè¯¥ç ”ç©¶æå‡ºäº† **VISTA Space**ï¼Œä¸€ä¸ªç”¨äº **Narrative Orchestration**ï¼ˆå™äº‹ç¼–æ’ï¼‰çš„é«˜ç»´è¡¨å¾æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³ç°æœ‰å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨å¤„ç†æ–‡å­¦å™äº‹æ—¶è¿‡äºå…³æ³¨å› æœè¿è´¯æ€§è€Œå¿½è§†å¤æ‚æ•…äº‹å¼§çº¿çš„é—®é¢˜ã€‚åŸºäºæ­¤æ¡†æ¶ï¼Œä½œè€…æ¨å‡ºäº† **LitVISTA** è¯„æµ‹åŸºå‡†ï¼Œåˆ©ç”¨ç»è¿‡ç»“æ„åŒ–æ ‡æ³¨çš„æ–‡å­¦æ–‡æœ¬æ¥ç³»ç»Ÿè¯„ä¼°æ¨¡å‹çš„å™äº‹ç¼–æ’èƒ½åŠ›ã€‚é€šè¿‡å¯¹ **GPT**ã€**Claude**ã€**Grok** å’Œ **Gemini** ç­‰ä¸»æµå‰æ²¿æ¨¡å‹çš„è¯„ä¼°å‘ç°ï¼Œç°æœ‰æ¨¡å‹åœ¨æ„å»ºå…¨å±€å™äº‹è§†è§’ä»¥åŠåŒæ—¶æ•æ‰å™äº‹åŠŸèƒ½ä¸ç»“æ„æ–¹é¢å­˜åœ¨ç³»ç»Ÿæ€§ç¼ºé™·ã€‚å®éªŒç»“æœè¿›ä¸€æ­¥è¡¨æ˜ï¼Œå³ä½¿æ˜¯å…ˆè¿›çš„ **Thinking modes**ï¼ˆæ€ç»´æ¨¡å¼ï¼‰åœ¨ç†è§£æ­¤ç±»å¤æ‚çš„æ–‡å­¦å™äº‹ç¼–æ’æ–¹é¢æå‡ä¹Ÿååˆ†æœ‰é™ã€‚\n\n---\n\næ‚¨æ˜¯å¦è¿˜æƒ³äº†è§£è¯¥è®ºæ–‡ä¸­å…³äº **LitVISTA** åŸºå‡†æµ‹è¯•çš„å…·ä½“è¯„ä¼°æŒ‡æ ‡ï¼Œæˆ–è€…å¯¹å…¶ä»–æ–‡å­¦å¤§æ¨¡å‹çš„ç ”ç©¶æ„Ÿå…´è¶£ï¼Ÿ",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2601.06445v1",
      "published_date": "2026-01-10 06:08:28 UTC",
      "updated_date": "2026-01-10 06:08:28 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-22T23:06:24.642090+00:00"
    },
    {
      "arxiv_id": "2601.06434v1",
      "title": "On a Gradient Approach to Chebyshev Center Problems with Applications to Function Learning",
      "title_zh": "Chebyshev ä¸­å¿ƒé—®é¢˜çš„æ¢¯åº¦æ–¹æ³•åŠå…¶åœ¨å‡½æ•°å­¦ä¹ ä¸­çš„åº”ç”¨",
      "authors": [
        "Abhinav Raghuvanshi",
        "Mayank Baranwal",
        "Debasish Chatterjee"
      ],
      "abstract": "We introduce $\\textsf{gradOL}$, the first gradient-based optimization framework for solving Chebyshev center problems, a fundamental challenge in optimal function learning and geometric optimization. $\\textsf{gradOL}$ hinges on reformulating the semi-infinite problem as a finitary max-min optimization, making it amenable to gradient-based techniques. By leveraging automatic differentiation for precise numerical gradient computation, $\\textsf{gradOL}$ ensures numerical stability and scalability, making it suitable for large-scale settings. Under strong convexity of the ambient norm, $\\textsf{gradOL}$ provably recovers optimal Chebyshev centers while directly computing the associated radius. This addresses a key bottleneck in constructing stable optimal interpolants. Empirically, $\\textsf{gradOL}$ achieves significant improvements in accuracy and efficiency on 34 benchmark Chebyshev center problems from a benchmark $\\textsf{CSIP}$ library. Moreover, we extend $\\textsf{gradOL}$ to general convex semi-infinite programming (CSIP), attaining up to $4000\\times$ speedups over the state-of-the-art $\\texttt{SIPAMPL}$ solver tested on the indicated $\\textsf{CSIP}$ library containing 67 benchmark problems. Furthermore, we provide the first theoretical foundation for applying gradient-based methods to Chebyshev center problems, bridging rigorous analysis with practical algorithms. $\\textsf{gradOL}$ thus offers a unified solution framework for Chebyshev centers and broader CSIPs.",
      "tldr_zh": "è¯¥ç ”ç©¶å¼•å…¥äº† $\\textsf{gradOL}$ï¼Œè¿™æ˜¯é¦–ä¸ªç”¨äºè§£å†³ Chebyshev center é—®é¢˜çš„åŸºäºæ¢¯åº¦çš„ä¼˜åŒ–æ¡†æ¶ï¼Œæ—¨åœ¨åº”å¯¹æœ€ä¼˜å‡½æ•°å­¦ä¹ å’Œå‡ ä½•ä¼˜åŒ–ä¸­çš„æŒ‘æˆ˜ã€‚è¯¥æ¡†æ¶é€šè¿‡å°†åŠæ— é™è§„åˆ’é—®é¢˜ (semi-infinite problem) é‡æ–°è¡¨è¿°ä¸ºæœ‰é™çš„æœ€å¤§æœ€å°ä¼˜åŒ– (finitary max-min optimization)ï¼Œå¹¶åˆ©ç”¨è‡ªåŠ¨å¾®åˆ† (automatic differentiation) æŠ€æœ¯ç¡®ä¿äº†åœ¨å¤§è§„æ¨¡è®¾ç½®ä¸‹çš„æ•°å€¼ç¨³å®šæ€§å’Œå¯æ‰©å±•æ€§ã€‚å®éªŒè¯æ˜ï¼Œ$\\textsf{gradOL}$ åœ¨ 34 ä¸ªåŸºå‡†æµ‹è¯•ä¸Šæ˜¾è‘—æå‡äº†ç²¾åº¦å’Œæ•ˆç‡ï¼Œå¹¶å¯æ‰©å±•è‡³é€šç”¨å‡¸åŠæ— é™è§„åˆ’ (CSIP)ï¼Œç›¸æ¯”ä¼ ç»Ÿçš„ $\\texttt{SIPAMPL}$ æ±‚è§£å™¨å®ç°äº†é«˜è¾¾ 4000 å€çš„åŠ é€Ÿã€‚æ­¤å¤–ï¼Œè¯¥å·¥ä½œä¸ºæ¢¯åº¦æ–¹æ³•åº”ç”¨äº Chebyshev center é—®é¢˜æä¾›äº†é¦–ä¸ªç†è®ºåŸºç¡€ï¼Œå®ç°äº†ä¸¥è°¨åˆ†æä¸å®ç”¨ç®—æ³•çš„æœ‰æ•ˆç»“åˆã€‚",
      "categories": [
        "math.OC",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "math.OC",
      "comment": "Accepted to TMLR",
      "pdf_url": "https://arxiv.org/pdf/2601.06434v1",
      "published_date": "2026-01-10 05:34:01 UTC",
      "updated_date": "2026-01-10 05:34:01 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-22T23:06:30.093153+00:00"
    },
    {
      "arxiv_id": "2601.06431v2",
      "title": "LSRIF: Logic-Structured Reinforcement Learning for Instruction Following",
      "title_zh": "LSRIFï¼šé¢å‘æŒ‡ä»¤éµå¾ªçš„é€»è¾‘ç»“æ„åŒ–å¼ºåŒ–å­¦ä¹ ",
      "authors": [
        "Qingyu Ren",
        "Qianyu He",
        "Jingwen Chang",
        "Jie Zeng",
        "Jiaqing Liang",
        "Yanghua Xiao",
        "Han Xia",
        "Zeye Sun",
        "Fei Yu"
      ],
      "abstract": "Instruction-following is critical for large language models, but real-world instructions often contain logical structures such as sequential dependencies and conditional branching. Existing methods typically construct datasets with parallel constraints and optimize average rewards, ignoring logical dependencies and yielding noisy signals. We propose a logic-structured training framework LSRIF that explicitly models instruction logic. We first construct a dataset LSRInstruct with constraint structures such as parallel, sequential, and conditional types, and then design structure-aware rewarding method LSRIF including average aggregation for parallel structures, failure-penalty propagation for sequential structures, and selective rewards for conditional branches. Experiments show LSRIF brings significant improvements in instruction-following (in-domain and out-of-domain) and general reasoning. Analysis reveals that learning with explicit logic structures brings parameter updates in attention layers and sharpens token-level attention to constraints and logical operators.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† LSRIFï¼Œä¸€ç§é€»è¾‘ç»“æ„åŒ–çš„å¼ºåŒ–å­¦ä¹  (Logic-Structured Reinforcement Learning) æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³ç°æœ‰å¤§è¯­è¨€æ¨¡å‹åœ¨å¤„ç†åŒ…å«é¡ºåºä¾èµ–å’Œæ¡ä»¶åˆ†æ”¯ç­‰å¤æ‚é€»è¾‘æŒ‡ä»¤æ—¶ï¼Œå› å¿½ç•¥é€»è¾‘ä¾èµ–è€Œäº§ç”Ÿçš„ä¿¡å·å™ªå£°é—®é¢˜ã€‚ç ”ç©¶é¦–å…ˆæ„å»ºäº†åŒ…å«å¹¶è¡Œã€é¡ºåºå’Œæ¡ä»¶çº¦æŸç»“æ„çš„è®­ç»ƒæ•°æ®é›† LSRInstructï¼Œå¹¶è®¾è®¡äº†ç»“æ„æ„ŸçŸ¥çš„å¥–åŠ±æœºåˆ¶ï¼ŒåŒ…æ‹¬é’ˆå¯¹é¡ºåºç»“æ„çš„å¤±è´¥æƒ©ç½šä¼ æ’­ (failure-penalty propagation) ä»¥åŠé’ˆå¯¹æ¡ä»¶åˆ†æ”¯çš„é€‰æ‹©æ€§å¥–åŠ±ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒLSRIF åœ¨é¢†åŸŸå†…å¤– (in-domain and out-of-domain) çš„æŒ‡ä»¤éµå¾ªä»»åŠ¡å’Œé€šç”¨æ¨ç†èƒ½åŠ›ä¸Šå‡æœ‰æ˜¾è‘—æå‡ã€‚åˆ†æè¿›ä¸€æ­¥æ­ç¤ºï¼Œæ˜¾å¼é€»è¾‘ç»“æ„çš„å­¦ä¹ èƒ½å¤Ÿä¿ƒè¿›æ³¨æ„åŠ›å±‚ (attention layers) çš„å‚æ•°æ›´æ–°ï¼Œå¹¶å¢å¼ºæ¨¡å‹å¯¹çº¦æŸæ¡ä»¶å’Œé€»è¾‘è¿ç®—ç¬¦çš„å…³æ³¨ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2601.06431v2",
      "published_date": "2026-01-10 05:11:38 UTC",
      "updated_date": "2026-01-14 02:51:23 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-22T23:06:33.032076+00:00"
    },
    {
      "arxiv_id": "2601.06426v1",
      "title": "NC-Bench: An LLM Benchmark for Evaluating Conversational Competence",
      "title_zh": "NC-Benchï¼šå¤§è¯­è¨€æ¨¡å‹ä¼šè¯èƒ½åŠ›è¯„ä¼°åŸºå‡†",
      "authors": [
        "Robert J. Moore",
        "Sungeun An",
        "Farhan Ahmed",
        "Jay Pankaj Gala"
      ],
      "abstract": "The Natural Conversation Benchmark (NC-Bench) introduce a new approach to evaluating the general conversational competence of large language models (LLMs). Unlike prior benchmarks that focus on the content of model behavior, NC-Bench focuses on the form and structure of natural conversation. Grounded in the IBM Natural Conversation Framework (NCF), NC-Bench comprises three distinct sets. The Basic Conversation Competence set evaluates fundamental sequence management practices, such as answering inquiries, repairing responses, and closing conversational pairs. The RAG set applies the same sequence management patterns as the first set but incorporates retrieval-augmented generation (RAG). The Complex Request set extends the evaluation to complex requests involving more intricate sequence management patterns. Each benchmark tests a model's ability to produce contextually appropriate conversational actions in response to characteristic interaction patterns. Initial evaluations across 6 open-source models and 14 interaction patterns show that models perform well on basic answering tasks, struggle more with repair tasks (especially repeat), have mixed performance on closing sequences, and find complex multi-turn requests most challenging, with Qwen models excelling on the Basic set and Granite models on the RAG set and the Complex Request set. By operationalizing fundamental principles of human conversation, NC-Bench provides a lightweight, extensible, and theory-grounded framework for assessing and improving the conversational abilities of LLMs beyond topical or task-specific benchmarks.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¨å‡ºäº† NC-Benchï¼Œè¿™æ˜¯ä¸€ç§åŸºäº IBM Natural Conversation Framework (NCF) çš„æ–°åŸºå‡†ï¼Œç”¨äºè¯„ä¼° Large Language Models (LLMs) çš„é€šç”¨å¯¹è¯èƒ½åŠ›ã€‚ä¸åŒäºä»¥å¾€å…³æ³¨å†…å®¹çš„è¯„ä¼°ï¼ŒNC-Bench ä¾§é‡äºè‡ªç„¶å¯¹è¯çš„å½¢å¼å’Œç»“æ„ï¼Œé€šè¿‡ Basic Conversation Competenceã€RAG å’Œ Complex Request ä¸‰ä¸ªæµ‹è¯•é›†æ¥è€ƒå¯Ÿæ¨¡å‹çš„ Sequence Management èƒ½åŠ›ã€‚è¯¥åŸºå‡†æµ‹è¯•äº†æ¨¡å‹åœ¨å›ç­”ã€Repair å’Œå…³é—­å¯¹è¯ç­‰äº¤äº’æ¨¡å¼ä¸‹çš„ä¸Šä¸‹æ–‡é€‚åº”æ€§ã€‚å®éªŒå‘ç°ï¼Œæ¨¡å‹åœ¨åŸºç¡€å¯¹è¯ä¸­è¡¨ç°è‰¯å¥½ï¼Œä½†åœ¨å¤„ç† Repair ä»»åŠ¡ï¼ˆå°¤å…¶æ˜¯é‡å¤ï¼‰å’Œå¤æ‚çš„ Multi-turn Requests æ—¶æŒ‘æˆ˜è¾ƒå¤§ï¼Œå…¶ä¸­ Qwen æ¨¡å‹åœ¨åŸºç¡€é›†ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œè€Œ Granite æ¨¡å‹åœ¨ RAG å’Œå¤æ‚è¯·æ±‚é›†ä¸­é¢†å…ˆã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "9 pages, 1 figure, 2 tables",
      "pdf_url": "https://arxiv.org/pdf/2601.06426v1",
      "published_date": "2026-01-10 04:57:24 UTC",
      "updated_date": "2026-01-10 04:57:24 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-22T23:06:35.257176+00:00"
    },
    {
      "arxiv_id": "2601.06425v1",
      "title": "HiDVFS: A Hierarchical Multi-Agent DVFS Scheduler for OpenMP DAG Workloads",
      "title_zh": "HiDVFSï¼šé¢å‘ OpenMP DAG è´Ÿè½½çš„åˆ†å±‚å¤šæ™ºèƒ½ä½“ DVFS è°ƒåº¦å™¨",
      "authors": [
        "Mohammad Pivezhandi",
        "Abusayeed Saifullah",
        "Ali Jannesari"
      ],
      "abstract": "With advancements in multicore embedded systems, leakage power, exponentially tied to chip temperature, has surpassed dynamic power consumption. Energy-aware solutions use dynamic voltage and frequency scaling (DVFS) to mitigate overheating in performance-intensive scenarios, while software approaches allocate high-utilization tasks across core configurations in parallel systems to reduce power. However, existing heuristics lack per-core frequency monitoring, failing to address overheating from uneven core activity, and task assignments without detailed profiling overlook irregular execution patterns. We target OpenMP DAG workloads. Because makespan, energy, and thermal goals often conflict within a single benchmark, this work prioritizes performance (makespan) while reporting energy and thermal as secondary outcomes. To overcome these issues, we propose HiDVFS (a hierarchical multi-agent, performance-aware DVFS scheduler) for parallel systems that optimizes task allocation based on profiling data, core temperatures, and makespan-first objectives. It employs three agents: one selects cores and frequencies using profiler data, another manages core combinations via temperature sensors, and a third sets task priorities during resource contention. A makespan-focused reward with energy and temperature regularizers estimates future states and enhances sample efficiency. Experiments on the NVIDIA Jetson TX2 using the BOTS suite (9 benchmarks) compare HiDVFS against state-of-the-art approaches. With multi-seed validation (seeds 42, 123, 456), HiDVFS achieves the best finetuned performance with 4.16 plus/minus 0.58s average makespan (L10), representing a 3.44x speedup over GearDVFS (14.32 plus/minus 2.61s) and 50.4% energy reduction (63.7 kJ vs 128.4 kJ). Across all BOTS benchmarks, HiDVFS achieves an average 3.95x speedup and 47.1% energy reduction.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤šæ ¸åµŒå…¥å¼ç³»ç»Ÿä¸­ç”±äºèŠ¯ç‰‡æ¸©åº¦å‡é«˜å¯¼è‡´çš„æ¼ç”µåŠŸè€—é—®é¢˜ï¼Œæå‡ºäº†HiDVFSï¼Œä¸€ç§é¢å‘OpenMP DAGå·¥ä½œè´Ÿè½½çš„å±‚æ¬¡åŒ–å¤šæ™ºèƒ½ä½“åŠ¨æ€ç”µå‹é¢‘ç‡è°ƒæ•´(DVFS)è°ƒåº¦æ–¹æ¡ˆã€‚è¯¥æ¡†æ¶é€šè¿‡ä¸‰ä¸ªååŒå·¥ä½œçš„æ™ºèƒ½ä½“ï¼Œåˆ†åˆ«å®ç°åŸºäºåˆ†ææ•°æ®çš„æ ¸å¿ƒä¸é¢‘ç‡é€‰æ‹©ã€åŸºäºæ¸©åº¦ä¼ æ„Ÿå™¨çš„æ ¸å¿ƒç»„åˆç®¡ç†ä»¥åŠèµ„æºç«äº‰æ—¶çš„ä»»åŠ¡ä¼˜å…ˆçº§è®¾å®šã€‚HiDVFSé‡‡ç”¨äº†ä»¥å®Œæˆæ—¶é—´(makespan)ä¸ºå¯¼å‘çš„å¥–åŠ±æœºåˆ¶ï¼Œå¹¶å¼•å…¥èƒ½æºä¸æ¸©åº¦æ­£åˆ™åŒ–é¡¹ï¼Œæ—¨åœ¨ä¼˜å…ˆä¿è¯æ€§èƒ½çš„åŒæ—¶ä¼˜åŒ–ç³»ç»Ÿçƒ­ç®¡ç†ã€‚åœ¨NVIDIA Jetson TX2å¹³å°ä¸Šçš„å®éªŒç»“æœæ˜¾ç¤ºï¼ŒHiDVFSåœ¨BOTSåŸºå‡†æµ‹è¯•ä¸­ç›¸æ¯”GearDVFSç­‰å‰æ²¿æ–¹æ³•å®ç°äº†å¹³å‡3.95å€çš„åŠ é€Ÿå’Œ47.1%çš„èƒ½è€—é™ä½ï¼Œæ˜¾è‘—æå‡äº†å¹¶è¡Œç³»ç»Ÿçš„èƒ½æ•ˆè¡¨ç°ã€‚",
      "categories": [
        "cs.DC",
        "cs.AI"
      ],
      "primary_category": "cs.DC",
      "comment": "38 pages, 15 figures, 8 tables",
      "pdf_url": "https://arxiv.org/pdf/2601.06425v1",
      "published_date": "2026-01-10 04:42:42 UTC",
      "updated_date": "2026-01-10 04:42:42 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-22T23:06:41.293757+00:00"
    },
    {
      "arxiv_id": "2601.06423v1",
      "title": "Does Inference Scaling Improve Reasoning Faithfulness? A Multi-Model Analysis of Self-Consistency Tradeoffs",
      "title_zh": "æ¨ç†æ‰©å±•èƒ½å¦æå‡æ¨ç†å¿ å®åº¦ï¼ŸåŸºäºè‡ªä¸€è‡´æ€§æƒè¡¡çš„å¤šæ¨¡å‹åˆ†æ",
      "authors": [
        "Deep Mehta"
      ],
      "abstract": "Self-consistency has emerged as a popular technique for improving large language model accuracy on reasoning tasks. The approach is straightforward: generate multiple reasoning paths and select the most common answer through majority voting. While this reliably boosts accuracy, it remains unclear whether these gains reflect genuine improvements in reasoning quality. We investigate a fundamental question that has not been studied before: does inference scaling improve reasoning faithfulness?\n  We conduct a comprehensive empirical study across four frontier models (GPT-5.2, Claude Opus 4.5, Gemini-3-flash-preview, and DeepSeek-v3.2) on 100 GSM8K mathematical reasoning problems. Our analysis employs bootstrap confidence intervals, McNemar's tests for paired comparisons, and Cohen's d effect sizes to quantify the effects rigorously. The results reveal striking differences across models that challenge common assumptions about self-consistency.\n  GPT-5.2 shows the expected pattern: accuracy improves from 78% to 90% at N=5, with faithfulness remaining relatively stable (0.540 to 0.510). Claude Opus 4.5 tells a completely different story. Its accuracy actually drops from 78% to 74.3% while faithfulness jumps dramatically from 0.270 to 0.891 at N=5. DeepSeek-v3.2, already at 98% accuracy, shows ceiling effects with modest faithfulness gains (0.440 to 0.541). Gemini-3-flash improves from 81% to 86% accuracy with a slight faithfulness decrease (0.260 to 0.212).\n  Problem difficulty analysis reveals that GPT-5.2 solves 82% of hard problems while breaking only 13% of easy ones. Claude, in contrast, breaks 23% of easy problems, explaining its accuracy decrease. These findings matter for practitioners: self-consistency is not universally beneficial, and teams should test their specific models before deployment. We release our code and provide practical recommendations for navigating these tradeoffs.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†æ¨ç†æ‰©å±•ï¼ˆInference Scalingï¼‰æ˜¯å¦èƒ½æé«˜å¤§å‹è¯­è¨€æ¨¡å‹çš„æ¨ç†å¿ å®åº¦ï¼ˆReasoning Faithfulnessï¼‰ï¼Œå¹¶é’ˆå¯¹è‡ªæˆ‘ä¸€è‡´æ€§ï¼ˆSelf-Consistencyï¼‰æŠ€æœ¯åœ¨ä¸åŒæ¨¡å‹ä¸­çš„æƒè¡¡ï¼ˆTradeoffsï¼‰è¿›è¡Œäº†æ·±å…¥åˆ†æã€‚é€šè¿‡åœ¨GSM8Kæ•°å­¦æ¨ç†ä»»åŠ¡ä¸Šå¯¹GPT-5.2ã€Claude Opus 4.5ã€Gemini-3-flash-previewå’ŒDeepSeek-v3.2è¿›è¡Œå®éªŒï¼Œç ”ç©¶å‘ç°æ¨ç†æ‰©å±•çš„å½±å“å› æ¨¡å‹è€Œå¼‚ï¼ŒæŒ‘æˆ˜äº†è‡ªæˆ‘ä¸€è‡´æ€§æ™®éå—ç›Šçš„å‡è®¾ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒGPT-5.2åœ¨æå‡å‡†ç¡®ç‡çš„åŒæ—¶ä¿æŒäº†å¿ å®åº¦ç¨³å®šï¼Œè€ŒClaude Opus 4.5å‡ºç°äº†å‡†ç¡®ç‡ä¸‹é™ä½†å¿ å®åº¦å¤§å¹…è·ƒå‡çš„ç°è±¡ï¼ŒGemini-3-flashåˆ™åœ¨å‡†ç¡®ç‡æå‡æ—¶ä¼´éšç€å¿ å®åº¦çš„ä¸‹é™ã€‚è¯¥ç ”ç©¶å¼ºè°ƒäº†è‡ªæˆ‘ä¸€è‡´æ€§åœ¨æ¨¡å‹è¡¨ç°ä¸Šçš„å¤æ‚æ€§ï¼Œå»ºè®®å¼€å‘è€…åœ¨éƒ¨ç½²å‰å¿…é¡»é’ˆå¯¹å…·ä½“æ¨¡å‹å’Œä»»åŠ¡éš¾åº¦è¿›è¡Œæµ‹è¯•ä¸æƒè¡¡ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "24 pages, 3 figures, 9 tables",
      "pdf_url": "https://arxiv.org/pdf/2601.06423v1",
      "published_date": "2026-01-10 04:20:00 UTC",
      "updated_date": "2026-01-10 04:20:00 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-22T23:06:41.481761+00:00"
    },
    {
      "arxiv_id": "2601.06415v1",
      "title": "Semantic Enrichment of CAD-Based Industrial Environments via Scene Graphs for Simulation and Reasoning",
      "title_zh": "é€šè¿‡åœºæ™¯å›¾å®ç°åŸºäº CAD çš„å·¥ä¸šç¯å¢ƒè¯­ä¹‰å¢å¼ºï¼Œç”¨äºæ¨¡æ‹Ÿä¸æ¨ç†",
      "authors": [
        "Nathan Pascal Walus",
        "Ranulfo Bezerra",
        "Shotaro Kojima",
        "Tsige Tadesse Alemayoh",
        "Satoshi Tadokoro",
        "Kazunori Ohno"
      ],
      "abstract": "Utilizing functional elements in an industrial environment, such as displays and interactive valves, provide effective possibilities for robot training. When preparing simulations for robots or applications that involve high-level scene understanding, the simulation environment must be equally detailed. Although CAD files for such environments deliver an exact description of the geometry and visuals, they usually lack semantic, relational and functional information, thus limiting the simulation and training possibilities. A 3D scene graph can organize semantic, spatial and functional information by enriching the environment through a Large Vision-Language Model (LVLM). In this paper we present an offline approach to creating detailed 3D scene graphs from CAD environments. This will serve as a foundation to include the relations of functional and actionable elements, which then can be used for dynamic simulation and reasoning. Key results of this research include both quantitative results of the generated semantic labels as well as qualitative results of the scene graph, especially in hindsight of pipe structures and identified functional relations. All code, results and the environment will be made available at https://cad-scenegraph.github.io",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å·¥ä¸šç¯å¢ƒä¸­çš„ CAD æ–‡ä»¶ç¼ºä¹è¯­ä¹‰ã€å…³ç³»å’ŒåŠŸèƒ½ä¿¡æ¯çš„é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§åˆ©ç”¨å¤§è¯­è¨€è§†è§‰æ¨¡å‹ (LVLM) ç¦»çº¿æ„å»ºè¯¦ç»† 3D Scene Graph çš„æ–¹æ³•ã€‚è¯¥æ¡†æ¶é€šè¿‡å¯¹ CAD ç¯å¢ƒè¿›è¡Œè¯­ä¹‰å¯ŒåŒ–ï¼ŒæˆåŠŸæ•´åˆäº†ç©ºé—´ã€åŠŸèƒ½åŠå¯æ“ä½œå…ƒä»¶çš„å…³ç³»ï¼Œä¸ºæœºå™¨äººçš„åŠ¨æ€ä»¿çœŸä¸é«˜çº§æ¨ç†å¥ å®šäº†åŸºç¡€ã€‚å®éªŒç»“æœåœ¨è¯­ä¹‰æ ‡ç­¾ç”Ÿæˆçš„å®šé‡åˆ†æä»¥åŠç®¡é“ç»“æ„å’ŒåŠŸèƒ½å…³ç³»çš„å®šæ€§è¯†åˆ«ä¸Šå‡è¡¨ç°å‡ºè‰²ã€‚è¯¥æ–¹æ³•æœ‰æ•ˆè§£å†³äº† CAD æ¨¡å‹ä¸é«˜å±‚åœºæ™¯ç†è§£ä¹‹é—´çš„ä¿¡æ¯é¸¿æ²Ÿï¼Œæ˜¾è‘—æå‡äº†å·¥ä¸šä»¿çœŸç¯å¢ƒçš„äº¤äº’æ€§ä¸å®ç”¨ä»·å€¼ã€‚",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.RO",
      "comment": "Accepted to IEEE SSRR 2025",
      "pdf_url": "https://arxiv.org/pdf/2601.06415v1",
      "published_date": "2026-01-10 03:22:29 UTC",
      "updated_date": "2026-01-10 03:22:29 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-22T23:06:43.519711+00:00"
    },
    {
      "arxiv_id": "2601.06401v1",
      "title": "BizFinBench.v2: A Unified Dual-Mode Bilingual Benchmark for Expert-Level Financial Capability Alignment",
      "title_zh": "BizFinBench.v2ï¼šé¢å‘ä¸“å®¶çº§é‡‘èèƒ½åŠ›å¯¹é½çš„ç»Ÿä¸€åŒæ¨¡å¼åŒè¯­è¯„æµ‹åŸºå‡†",
      "authors": [
        "Xin Guo",
        "Rongjunchen Zhang",
        "Guilong Lu",
        "Xuntao Guo",
        "Shuai Jia",
        "Zhi Yang",
        "Liwen Zhang"
      ],
      "abstract": "Large language models have undergone rapid evolution, emerging as a pivotal technology for intelligence in financial operations. However, existing benchmarks are often constrained by pitfalls such as reliance on simulated or general-purpose samples and a focus on singular, offline static scenarios. Consequently, they fail to align with the requirements for authenticity and real-time responsiveness in financial services, leading to a significant discrepancy between benchmark performance and actual operational efficacy. To address this, we introduce BizFinBench.v2, the first large-scale evaluation benchmark grounded in authentic business data from both Chinese and U.S. equity markets, integrating online assessment. We performed clustering analysis on authentic user queries from financial platforms, resulting in eight fundamental tasks and two online tasks across four core business scenarios, totaling 29,578 expert-level Q&A pairs. Experimental results demonstrate that ChatGPT-5 achieves a prominent 61.5% accuracy in main tasks, though a substantial gap relative to financial experts persists; in online tasks, DeepSeek-R1 outperforms all other commercial LLMs. Error analysis further identifies the specific capability deficiencies of existing models within practical financial business contexts. BizFinBench.v2 transcends the limitations of current benchmarks, achieving a business-level deconstruction of LLM financial capabilities and providing a precise basis for evaluating efficacy in the widespread deployment of LLMs within the financial domain. The data and code are available at https://github.com/HiThink-Research/BizFinBench.v2.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¨å‡ºäº† BizFinBench.v2ï¼Œè¿™æ˜¯é¦–ä¸ªåŸºäºä¸­å›½å’Œç¾å›½è‚¡ç¥¨å¸‚åœºçœŸå®ä¸šåŠ¡æ•°æ®çš„å¤§è§„æ¨¡è¯„æµ‹åŸºå‡† (Benchmark)ï¼Œæ—¨åœ¨è§£å†³ç°æœ‰è¯„ä¼°ä½“ç³»ä¾èµ–æ¨¡æ‹Ÿæ•°æ®ä¸”ç¼ºä¹å®æ—¶å“åº”èƒ½åŠ›çš„é—®é¢˜ã€‚è¯¥åŸºå‡†é€šè¿‡å¯¹é‡‘èå¹³å°çœŸå®ç”¨æˆ·æŸ¥è¯¢è¿›è¡Œèšç±»åˆ†æï¼Œæ¶µç›–äº† 4 ä¸ªæ ¸å¿ƒä¸šåŠ¡åœºæ™¯ä¸‹çš„ 8 é¡¹åŸºç¡€ä»»åŠ¡å’Œ 2 é¡¹åœ¨çº¿ä»»åŠ¡ï¼Œå…±åŒ…å« 29,578 å¯¹ä¸“å®¶çº§é—®ç­”ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒChatGPT-5 åœ¨ä¸»è¦ä»»åŠ¡ä¸­è¾¾åˆ°äº† 61.5% çš„å‡†ç¡®ç‡ï¼Œä½†è¾ƒé‡‘èä¸“å®¶ä»æœ‰å·®è·ï¼›è€Œåœ¨åœ¨çº¿ä»»åŠ¡ä¸­ï¼ŒDeepSeek-R1 çš„è¡¨ç°ä¼˜äºå…¶ä»–å•†ä¸šå¤§æ¨¡å‹ã€‚BizFinBench.v2 å®ç°äº†å¯¹å¤§æ¨¡å‹ (LLMs) é‡‘èèƒ½åŠ›çš„ä¸šåŠ¡çº§æ‹†è§£ï¼Œä¸ºè¯„ä¼°å…¶åœ¨é‡‘èé¢†åŸŸå®é™…éƒ¨ç½²çš„æœ‰æ•ˆæ€§æä¾›äº†ç²¾ç¡®ä¾æ®ã€‚",
      "categories": [
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2601.06401v1",
      "published_date": "2026-01-10 02:51:53 UTC",
      "updated_date": "2026-01-10 02:51:53 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-22T23:06:53.221413+00:00"
    },
    {
      "arxiv_id": "2601.10740v1",
      "title": "Neuro-Symbolic Activation Discovery: Transferring Mathematical Structures from Physics to Ecology for Parameter-Efficient Neural Networks",
      "title_zh": "ç¥ç»ç¬¦å·æ¿€æ´»å‘ç°ï¼šå°†æ•°å­¦ç»“æ„ä»ç‰©ç†å­¦è¿ç§»è‡³ç”Ÿæ€å­¦ä»¥å®ç°å‚æ•°é«˜æ•ˆå‹ç¥ç»ç½‘ç»œ",
      "authors": [
        "Anas Hajbi"
      ],
      "abstract": "Modern neural networks rely on generic activation functions (ReLU, GELU, SiLU) that ignore the mathematical structure inherent in scientific data. We propose Neuro-Symbolic Activation Discovery, a framework that uses Genetic Programming to extract interpretable mathematical formulas from data and inject them as custom activation functions. Our key contribution is the discovery of a Geometric Transfer phenomenon: activation functions learned from particle physics data successfully generalize to ecological classification, outperforming standard activations (ReLU, GELU, SiLU) in both accuracy and parameter efficiency. On the Forest Cover dataset, our Hybrid Transfer model achieves 82.4% accuracy with only 5,825 parameters, compared to 83.4% accuracy requiring 31,801 parameters for a conventional heavy network -- a 5.5x parameter reduction with only 1% accuracy loss. We introduce a Parameter Efficiency Score ($E_{param} = AUC / \\log_{10}(Params)$) and demonstrate that lightweight hybrid architectures consistently achieve 18-21% higher efficiency than over-parameterized baselines. Crucially, we establish boundary conditions: while Physics to Ecology transfer succeeds (both involve continuous Euclidean measurements), Physics to Text transfer fails (discrete word frequencies require different mathematical structures). Our work opens pathways toward domain-specific activation libraries for efficient scientific machine learning.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† Neuro-Symbolic Activation Discovery æ¡†æ¶ï¼Œåˆ©ç”¨ Genetic Programming ä»ç§‘å­¦æ•°æ®ä¸­æå–å¯è§£é‡Šçš„æ•°å­¦å…¬å¼ï¼Œå¹¶å°†å…¶æ³¨å…¥ä¸ºè‡ªå®šä¹‰æ¿€æ´»å‡½æ•°ã€‚ç ”ç©¶æ ¸å¿ƒè´¡çŒ®åœ¨äºå‘ç°äº†â€œå‡ ä½•è¿ç§»â€(Geometric Transfer)ç°è±¡ï¼Œå³ä»ç²’å­ç‰©ç†æ•°æ®ä¸­å­¦ä¹ åˆ°çš„æ¿€æ´»å‡½æ•°èƒ½æˆåŠŸæ³›åŒ–è‡³ç”Ÿæ€å­¦åˆ†ç±»ä»»åŠ¡ï¼Œå…¶è¡¨ç°ä¼˜äº ReLUã€GELU ç­‰æ ‡å‡†æ¿€æ´»å‡½æ•°ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œåœ¨ Forest Cover æ•°æ®é›†ä¸Šï¼Œè¯¥æ¨¡å‹åœ¨ä»…ä½¿ç”¨ 5,825 ä¸ªå‚æ•°çš„æƒ…å†µä¸‹è¾¾åˆ°äº† 82.4% çš„å‡†ç¡®ç‡ï¼Œè¾ƒä¼ ç»Ÿç½‘ç»œå®ç°äº† 5.5 å€çš„å‚æ•°ç¼©å‡ï¼Œä¸”å‡†ç¡®ç‡ä»…æŸå¤± 1%ã€‚æ­¤å¤–ï¼Œç ”ç©¶å¼•å…¥äº†å‚æ•°æ•ˆç‡å¾—åˆ†($E_{param}$)ï¼Œå¹¶ç¡®ç«‹äº†è¿ç§»çš„è¾¹ç•Œæ¡ä»¶ï¼ŒæŒ‡å‡ºç‰©ç†å‘ç”Ÿæ€è¿ç§»çš„æˆåŠŸæºäºä¸¤è€…å…±æœ‰çš„è¿ç»­æ¬§å‡ é‡Œå¾—æµ‹é‡ç»“æ„ã€‚",
      "categories": [
        "cs.NE",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.NE",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2601.10740v1",
      "published_date": "2026-01-10 02:49:32 UTC",
      "updated_date": "2026-01-10 02:49:32 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-22T23:07:05.391540+00:00"
    },
    {
      "arxiv_id": "2601.06394v1",
      "title": "Context Matters: Peer-Aware Student Behavioral Engagement Measurement via VLM Action Parsing and LLM Sequence Classification",
      "title_zh": "èƒŒæ™¯è‡³å…³é‡è¦ï¼šåŸºäº VLM åŠ¨ä½œè§£æä¸ LLM åºåˆ—åˆ†ç±»çš„åŒä¼´æ„ŸçŸ¥å­¦ç”Ÿè¡Œä¸ºå‚ä¸åº¦æµ‹é‡",
      "authors": [
        "Ahmed Abdelkawy",
        "Ahmed Elsayed",
        "Asem Ali",
        "Aly Farag",
        "Thomas Tretter",
        "Michael McIntyre"
      ],
      "abstract": "Understanding student behavior in the classroom is essential to improve both pedagogical quality and student engagement. Existing methods for predicting student engagement typically require substantial annotated data to model the diversity of student behaviors, yet privacy concerns often restrict researchers to their own proprietary datasets. Moreover, the classroom context, represented in peers' actions, is ignored. To address the aforementioned limitation, we propose a novel three-stage framework for video-based student engagement measurement. First, we explore the few-shot adaptation of the vision-language model for student action recognition, which is fine-tuned to distinguish among action categories with a few training samples. Second, to handle continuous and unpredictable student actions, we utilize the sliding temporal window technique to divide each student's 2-minute-long video into non-overlapping segments. Each segment is assigned an action category via the fine-tuned VLM model, generating a sequence of action predictions. Finally, we leverage the large language model to classify this entire sequence of actions, together with the classroom context, as belonging to an engaged or disengaged student. The experimental results demonstrate the effectiveness of the proposed approach in identifying student engagement.",
      "tldr_zh": "---\n\n### è®ºæ–‡ TLDR æ‘˜è¦ ğŸ“„\n\nè¯¥ç ”ç©¶æå‡ºäº†ä¸€ä¸ªä¸‰é˜¶æ®µæ¡†æ¶ï¼Œæ—¨åœ¨é€šè¿‡ Vision-Language Model (VLM) åŠ¨ä½œè§£æå’Œ Large Language Model (LLM) åºåˆ—åˆ†ç±»ï¼Œå®ç°åŒä¼´æ„ŸçŸ¥ (Peer-Aware) çš„å­¦ç”Ÿè¡Œä¸ºå‚ä¸åº¦æµ‹é‡ã€‚æ¡†æ¶é¦–å…ˆåˆ©ç”¨ VLM çš„ Few-shot é€‚é…èƒ½åŠ›è¿›è¡ŒåŠ¨ä½œè¯†åˆ«ï¼Œå¹¶ç»“åˆæ»‘åŠ¨æ—¶é—´çª—æŠ€æœ¯å°†è§†é¢‘è§£æä¸ºè¿ç»­çš„åŠ¨ä½œåºåˆ—ã€‚éšåï¼Œåˆ©ç”¨ LLM ç»¼åˆåˆ†æè¯¥åŠ¨ä½œåºåˆ—åŠå…¶ç”±åŒä¼´è¡Œä¸ºæ„æˆçš„è¯¾å ‚ä¸Šä¸‹æ–‡ (Classroom context)ï¼Œä»¥åˆ¤å®šå­¦ç”Ÿå¤„äºå‚ä¸æˆ–åˆ†å¿ƒçŠ¶æ€ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨è§£å†³æ ‡æ³¨æ•°æ®åŒ®ä¹å’Œéšç§é™åˆ¶é—®é¢˜çš„åŒæ—¶ï¼Œé€šè¿‡å¼•å…¥åŒä¼´èƒŒæ™¯æ˜¾è‘—æå‡äº†å‚ä¸åº¦è¯†åˆ«çš„æœ‰æ•ˆæ€§ã€‚\n\n---\n\næ˜¯å¦éœ€è¦é’ˆå¯¹è¯¥è®ºæ–‡çš„ç‰¹å®šæŠ€æœ¯ç»†èŠ‚ï¼ˆå¦‚ VLM çš„å¾®è°ƒç­–ç•¥æˆ– LLM çš„åˆ†ç±»æç¤ºè¯ï¼‰è¿›è¡Œæ›´æ·±å±‚çš„è§£æï¼Ÿ",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2601.06394v1",
      "published_date": "2026-01-10 02:39:24 UTC",
      "updated_date": "2026-01-10 02:39:24 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-22T23:07:07.431370+00:00"
    },
    {
      "arxiv_id": "2601.06377v1",
      "title": "HiMem: Hierarchical Long-Term Memory for LLM Long-Horizon Agents",
      "title_zh": "HiMemï¼šé¢å‘å¤§è¯­è¨€æ¨¡å‹é•¿ç¨‹æ™ºèƒ½ä½“çš„å±‚çº§åŒ–é•¿æœŸè®°å¿†",
      "authors": [
        "Ningning Zhang",
        "Xingxing Yang",
        "Zhizhong Tan",
        "Weiping Deng",
        "Wenyong Wang"
      ],
      "abstract": "Although long-term memory systems have made substantial progress in recent years, they still exhibit clear limitations in adaptability, scalability, and self-evolution under continuous interaction settings. Inspired by cognitive theories, we propose HiMem, a hierarchical long-term memory framework for long-horizon dialogues, designed to support memory construction, retrieval, and dynamic updating during sustained interactions. HiMem constructs cognitively consistent Episode Memory via a Topic-Aware Event--Surprise Dual-Channel Segmentation strategy, and builds Note Memory that captures stable knowledge through a multi-stage information extraction pipeline. These two memory types are semantically linked to form a hierarchical structure that bridges concrete interaction events and abstract knowledge, enabling efficient retrieval without sacrificing information fidelity. HiMem supports both hybrid and best-effort retrieval strategies to balance accuracy and efficiency, and incorporates conflict-aware Memory Reconsolidation to revise and supplement stored knowledge based on retrieval feedback. This design enables continual memory self-evolution over long-term use. Experimental results on long-horizon dialogue benchmarks demonstrate that HiMem consistently outperforms representative baselines in accuracy, consistency, and long-term reasoning, while maintaining favorable efficiency. Overall, HiMem provides a principled and scalable design paradigm for building adaptive and self-evolving LLM-based conversational agents. The code is available at https://github.com/jojopdq/HiMem.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†HiMemï¼Œä¸€ç§é¢å‘é•¿ç¨‹å¯¹è¯çš„å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åˆ†å±‚é•¿æœŸè®°å¿†æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³ç°æœ‰è®°å¿†ç³»ç»Ÿåœ¨æŒç»­äº¤äº’ä¸­é€‚åº”æ€§ä¸è‡ªè¿›åŒ–èƒ½åŠ›çš„ä¸è¶³ã€‚è¯¥æ¡†æ¶é€šè¿‡Topic-Aware Event-Surprise Dual-Channel Segmentationç­–ç•¥æ„å»ºæƒ…èŠ‚è®°å¿†ï¼ˆEpisode Memoryï¼‰ï¼Œå¹¶ç»“åˆå¤šé˜¶æ®µæå–çš„ç¬”è®°è®°å¿†ï¼ˆNote Memoryï¼‰å½¢æˆåˆ†å±‚ç»“æ„ï¼Œæœ‰æ•ˆæ¡¥æ¥äº†å…·ä½“äº¤äº’äº‹ä»¶ä¸æŠ½è±¡çŸ¥è¯†ã€‚HiMemæ”¯æŒæ··åˆä¸Best-effortæ£€ç´¢ç­–ç•¥ï¼Œå¹¶å¼•å…¥å†²çªæ„ŸçŸ¥çš„è®°å¿†å·©å›ºï¼ˆMemory Reconsolidationï¼‰æœºåˆ¶ï¼Œåˆ©ç”¨æ£€ç´¢åé¦ˆå®ç°çŸ¥è¯†çš„åŠ¨æ€ä¿®æ­£ä¸æŒç»­è¿›åŒ–ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒHiMemåœ¨é•¿ç¨‹å¯¹è¯åŸºå‡†æµ‹è¯•ä¸­çš„å‡†ç¡®æ€§ã€ä¸€è‡´æ€§å’Œé•¿ç¨‹æ¨ç†èƒ½åŠ›å‡æ˜¾è‘—ä¼˜äºç°æœ‰åŸºçº¿æ¨¡å‹ï¼Œä¸ºæ„å»ºè‡ªè¿›åŒ–æ™ºèƒ½ä½“æä¾›äº†å¯æ‰©å±•çš„è®¾è®¡èŒƒå¼ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2601.06377v1",
      "published_date": "2026-01-10 01:26:01 UTC",
      "updated_date": "2026-01-10 01:26:01 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-22T23:07:12.078722+00:00"
    },
    {
      "arxiv_id": "2601.06366v1",
      "title": "SafeGPT: Preventing Data Leakage and Unethical Outputs in Enterprise LLM Use",
      "title_zh": "SafeGPTï¼šé˜²èŒƒä¼ä¸šçº§å¤§è¯­è¨€æ¨¡å‹åº”ç”¨ä¸­çš„æ•°æ®æ³„éœ²ä¸ä¸åˆä¼¦ç†çš„è¾“å‡º",
      "authors": [
        "Pratyush Desai",
        "Luoxi Tang",
        "Yuqiao Meng",
        "Zhaohan Xi"
      ],
      "abstract": "Large Language Models (LLMs) are transforming enterprise workflows but introduce security and ethics challenges when employees inadvertently share confidential data or generate policy-violating content. This paper proposes SafeGPT, a two-sided guardrail system preventing sensitive data leakage and unethical outputs. SafeGPT integrates input-side detection/redaction, output-side moderation/reframing, and human-in-the-loop feedback. Experiments demonstrate SafeGPT effectively reduces data leakage risk and biased outputs while maintaining satisfaction.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† SafeGPTï¼Œä¸€ç§ä¸“é—¨ä¸ºä¼ä¸šçº§å¤§å‹è¯­è¨€æ¨¡å‹ (LLMs) ä½¿ç”¨è®¾è®¡çš„åŒå‘æŠ¤æ ç³»ç»Ÿ (Two-sided guardrail system)ï¼Œæ—¨åœ¨è§£å†³æ•æ„Ÿæ•°æ®æ³„éœ²åŠè¿åä¼ä¸šæ”¿ç­–çš„ä¸é“å¾·è¾“å‡ºé—®é¢˜ã€‚SafeGPT ç³»ç»Ÿåœ°é›†æˆäº†è¾“å…¥ä¾§çš„æ£€æµ‹ä¸è„±æ• (Detection/Redaction)ã€è¾“å‡ºä¾§çš„å®¡æ ¸ä¸é‡æ„ (Moderation/Reframing) ä»¥åŠäººç±»åœ¨ç¯ (Human-in-the-loop) åé¦ˆæœºåˆ¶ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ¡†æ¶åœ¨æ˜¾è‘—é™ä½æ•°æ®æ³„éœ²é£é™©å’Œåè§è¾“å‡ºçš„åŒæ—¶ï¼Œèƒ½å¤Ÿä¿æŒè¾ƒé«˜çš„ç”¨æˆ·æ»¡æ„åº¦ï¼Œä¸ºä¼ä¸šåœ¨å·¥ä½œæµä¸­å®‰å…¨éƒ¨ç½² LLMs æä¾›äº†æœ‰æ•ˆçš„å®‰å…¨ä¿éšœã€‚",
      "categories": [
        "cs.CR",
        "cs.AI"
      ],
      "primary_category": "cs.CR",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2601.06366v1",
      "published_date": "2026-01-10 00:33:38 UTC",
      "updated_date": "2026-01-10 00:33:38 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-22T23:07:14.760958+00:00"
    },
    {
      "arxiv_id": "2601.06364v1",
      "title": "Human-in-the-Loop Interactive Report Generation for Chronic Disease Adherence",
      "title_zh": "é¢å‘æ…¢æ€§ç—…ä¾ä»æ€§çš„äººåœ¨å›è·¯äº¤äº’å¼æŠ¥å‘Šç”Ÿæˆ",
      "authors": [
        "Xiaotian Zhang",
        "Jinhong Yu",
        "Pengwei Yan",
        "Le Jiang",
        "Xingyi Shen",
        "Mumo Cheng",
        "Xiaozhong Liu"
      ],
      "abstract": "Chronic disease management requires regular adherence feedback to prevent avoidable hospitalizations, yet clinicians lack time to produce personalized patient communications. Manual authoring preserves clinical accuracy but does not scale; AI generation scales but can undermine trust in patient-facing contexts. We present a clinician-in-the-loop interface that constrains AI to data organization and preserves physician oversight through recognition-based review. A single-page editor pairs AI-generated section drafts with time-aligned visualizations, enabling inline editing with visual evidence for each claim. This division of labor (AI organizes, clinician decides) targets both efficiency and accountability. In a pilot with three physicians reviewing 24 cases, AI successfully generated clinically personalized drafts matching physicians' manual authoring practice (overall mean 4.86/10 vs. 5.0/10 baseline), requiring minimal physician editing (mean 8.3\\% content modification) with zero safety-critical issues, demonstrating effective automation of content generation. However, review time remained comparable to manual practice, revealing an accountability paradox: in high-stakes clinical contexts, professional responsibility requires complete verification regardless of AI accuracy. We contribute three interaction patterns for clinical AI collaboration: bounded generation with recognition-based review via chart-text pairing, automated urgency flagging that analyzes vital trends and adherence patterns with fail-safe escalation for missed critical monitoring tasks, and progressive disclosure controls that reduce cognitive load while maintaining oversight. These patterns indicate that clinical AI efficiency requires not only accurate models, but also mechanisms for selective verification that preserve accountability.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§äººæœºåä½œ (Human-in-the-Loop) çš„äº¤äº’å¼æŠ¥å‘Šç”Ÿæˆç•Œé¢ï¼Œæ—¨åœ¨è§£å†³æ…¢æ€§ç—…ç®¡ç†ä¸­ä¸´åºŠåŒ»ç”Ÿéš¾ä»¥å…¼é¡¾ç”Ÿæˆä¸ªæ€§åŒ–æ‚£è€…åé¦ˆæŠ¥å‘Šä¸å·¥ä½œæ•ˆç‡çš„æŒ‘æˆ˜ã€‚è¯¥ç³»ç»Ÿé‡‡ç”¨â€œAI ç»„ç»‡æ•°æ®ï¼Œä¸´åºŠåŒ»ç”Ÿå†³ç­–â€çš„åˆ†å·¥æ¨¡å¼ï¼Œå°† AI ç”Ÿæˆçš„æŠ¥å‘Šè‰ç¨¿ä¸æ—¶é—´å¯¹é½çš„å¯è§†åŒ–å›¾è¡¨ç»“åˆï¼Œæ”¯æŒä¸´åºŠåŒ»ç”Ÿé€šè¿‡è¯†åˆ«å¼è¯„å®¡ (recognition-based review) è¿›è¡Œå¿«é€Ÿæ ¸æŸ¥ä¸ç¼–è¾‘ã€‚è¯•ç‚¹ç ”ç©¶æ˜¾ç¤ºï¼ŒAI ç”Ÿæˆçš„è‰ç¨¿åœ¨ä¸´åºŠè´¨é‡ä¸Šä¸åŒ»ç”Ÿæ‰‹åŠ¨æ’°å†™ç›¸å½“ï¼Œä»…éœ€ 8.3% çš„å†…å®¹ä¿®æ”¹ä¸”æ— å®‰å…¨é£é™©ã€‚ç ”ç©¶è¿›ä¸€æ­¥æ­ç¤ºäº†é«˜é£é™©ç¯å¢ƒä¸‹çš„â€œè´£ä»»æ‚–è®º (accountability paradox)â€ï¼Œå¹¶æå‡ºäº†è¾¹ç•ŒåŒ–ç”Ÿæˆã€è‡ªåŠ¨ç´§æ€¥æ ‡è®°å’Œæ¸è¿›å¼æŠ«éœ²ç­‰ä¸‰ç§ä¸´åºŠ AI åä½œæ¨¡å¼ï¼Œä¸ºå®ç°å¯è¿½è´£çš„åŒ»ç–—è‡ªåŠ¨åŒ–å¥ å®šäº†åŸºç¡€ã€‚",
      "categories": [
        "cs.HC",
        "cs.AI"
      ],
      "primary_category": "cs.HC",
      "comment": "5 pages, 3 figures. Accepted at the AAAI 2026 Workshop on AI for Healthy Aging and Longevity",
      "pdf_url": "https://arxiv.org/pdf/2601.06364v1",
      "published_date": "2026-01-10 00:19:33 UTC",
      "updated_date": "2026-01-10 00:19:33 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-22T23:07:17.126442+00:00"
    },
    {
      "arxiv_id": "2601.06362v1",
      "title": "Styles + Persona-plug = Customized LLMs",
      "title_zh": "é£æ ¼ + Persona-plug = å®šåˆ¶åŒ–å¤§è¯­è¨€æ¨¡å‹",
      "authors": [
        "Yutong Song",
        "Jiang Wu",
        "Shaofan Yuan",
        "Chengze Shen",
        "Jian Wang",
        "Amir Rahmani",
        "Nikil Dutt",
        "Yu Wang"
      ],
      "abstract": "We discover a previously overlooked challenge in personalized text generation: personalization methods are increasingly applied under explicit style instructions, yet their behavior under such constraints remains poorly understood. To balance implicit personalization and explicit style, we formulate personalization as a distributional residual and propose PsPLUG, a lightweight soft-prompt plug-in trained with style-conditioned preference contrasts. Across LaMP benchmark, our framework improves persona alignment, maintains stylistic fidelity, and outperforms retrieval-based and soft-prompt baselines with minimal computation. These results show that residual modeling provides a simple and principled foundation for controllable, style-aware LLM personalization.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†ä¸ªæ€§åŒ–æ–‡æœ¬ç”Ÿæˆä¸­ä¸€ä¸ªè¢«å¿½è§†çš„æŒ‘æˆ˜ï¼Œå³å¦‚ä½•åœ¨æ»¡è¶³æ˜¾å¼é£æ ¼æŒ‡ä»¤(Explicit style instructions)çš„åŒæ—¶å®ç°éšå¼ä¸ªæ€§åŒ–(Implicit personalization)ã€‚ä½œè€…å°†ä¸ªæ€§åŒ–å»ºæ¨¡ä¸ºåˆ†å¸ƒæ®‹å·®(Distributional residual)ï¼Œå¹¶æå‡ºäº† PsPLUGï¼Œä¸€ç§é€šè¿‡é£æ ¼æ¡ä»¶åå¥½å¯¹æ¯”(Style-conditioned preference contrasts)è¿›è¡Œè®­ç»ƒçš„è½»é‡çº§è½¯æç¤ºæ’ä»¶(Soft-prompt plug-in)ã€‚åœ¨ LaMP åŸºå‡†æµ‹è¯•ä¸­ï¼Œè¯¥æ¡†æ¶åœ¨æ˜¾è‘—æå‡äººæ ¼å¯¹é½(Persona alignment)çš„åŒæ—¶ä¿æŒäº†é£æ ¼å¿ å®åº¦ï¼Œä¸”è®¡ç®—å¼€é”€æå°ï¼Œæ€§èƒ½ä¼˜äºæ£€ç´¢å¼åŠå…¶ä»–è½¯æç¤ºåŸºçº¿æ¨¡å‹ã€‚å®éªŒç»“æœè¯æ˜ï¼Œæ®‹å·®å»ºæ¨¡ä¸ºå®ç°å¯æ§ä¸”å…·æœ‰é£æ ¼æ„ŸçŸ¥èƒ½åŠ›çš„ LLMs ä¸ªæ€§åŒ–æä¾›äº†ä¸€ä¸ªç®€å•ä¸”åŸåˆ™æ€§çš„åŸºç¡€ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2601.06362v1",
      "published_date": "2026-01-10 00:14:43 UTC",
      "updated_date": "2026-01-10 00:14:43 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-22T23:07:27.089198+00:00"
    }
  ],
  "processing_status": "completed",
  "error": null,
  "raw_papers_fetched": true,
  "papers_count": 66,
  "processed_papers_count": 66,
  "failed_papers_count": 0,
  "llm_backup_calls": 0,
  "summary_generated": true,
  "daily_data_saved": true,
  "last_update": "2026-01-22T23:08:17.411287+00:00"
}
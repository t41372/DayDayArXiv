{
  "date": "2025-10-12",
  "category": "cs.AI",
  "summary": "ä½ å¥½ï¼æ¬¢è¿æ¥åˆ° UTC æ—¶é—´ 2025-10-12 çš„ arXiv ä¸­æ–‡ TLDR å¿«æŠ¥ï¼\n\n**ä¸€å¥è¯æ€»ç»“ï¼š**\nä»Šå¤©çš„ arXiv å……æ»¡äº†â€œæ‰¹åˆ¤æ€§æ€ç»´â€å’Œâ€œè·¨ç•Œèåˆâ€çš„å‘³é“ã€‚æˆ‘ä»¬çœ‹åˆ°äº†å¯¹å¤§æ¨¡å‹æ¨ç†èƒ½åŠ›è¾¹ç•Œçš„æ·±åº¦æ‹·é—®ï¼ˆåœ¨ç¡®å®šæ€§æ¸¸æˆä¸­è¡¨ç°ä¸å¦‚äººæ„ï¼‰ï¼Œå¯¹ Audio LLM æ˜¯å¦çœŸçš„åœ¨â€œå¬â€çš„è´¨ç–‘ï¼Œä»¥åŠ AI åœ¨å¤©ä½“ç‰©ç†ã€åŒ–å­¦é€†åˆæˆå’Œè›‹ç™½è´¨ç»“æ„ç­‰ç§‘å­¦é¢†åŸŸçš„ç¡¬æ ¸åº”ç”¨ã€‚\n\n---\n\n### ğŸš€ æ·±åº¦æ¨ç†ä¸ Agentï¼šèƒ½åŠ›è¾¹ç•Œä¸å®‰å…¨éšæ‚£\nä»Šæ—¥é‡å¤´æˆæ˜¯å¯¹ LLM æ¨ç†èƒ½åŠ›çš„â€œç¥›é­…â€å’Œ Agent æ¡†æ¶çš„å®‰å…¨æ€§æ¢è®¨ã€‚\n\n**1. [æ¨è] Limits of Emergent Reasoning of Large Language Models in Agentic Frameworks for Deterministic Games**\n**å¤§è¯­è¨€æ¨¡å‹åœ¨ç¡®å®šæ€§æ¸¸æˆ Agent æ¡†æ¶ä¸­æ¶Œç°æ¨ç†èƒ½åŠ›çš„å±€é™æ€§**\n*   **æ ¸å¿ƒå‘ç°**ï¼šè¿™ç¯‡æ–‡ç« ç»™ç›®å‰ç«çƒ­çš„ Agent æ¨ç†æ³¼äº†ä¸€ç›†å†·æ°´ã€‚ç ”ç©¶è€…å‘ç°ï¼Œå³ä½¿ç»™ LLM é…å¤‡äº†ç¯å¢ƒæ¥å£ï¼ˆEnvironment Interfaceï¼‰å’Œå·¥å…·ï¼Œè®©å®ƒåœ¨â€œæ±‰è¯ºå¡”â€è¿™ç±»ç¡®å®šæ€§è°œé¢˜ä¸­ä¸€æ­¥æ­¥æ“ä½œã€è§‚å¯Ÿã€å†æ¨ç†ï¼Œæ¨¡å‹ä¾ç„¶ä¼šå‡ºç°**æ€§èƒ½å´©å¡Œï¼ˆCollapseï¼‰**ã€‚\n*   **Implication**ï¼šè¿™è¯´æ˜æ¨¡å‹å¯èƒ½å¹¶æ²¡æœ‰çœŸæ­£å­¦ä¼šâ€œçŠ¶æ€ç©ºé—´æœç´¢â€ï¼Œè€Œæ˜¯ä¾èµ–æŸç§æ¨¡å¼åŒ¹é…ã€‚å½“å¤æ‚åº¦æå‡ï¼Œæ¨¡å‹ä¼šåç¦»æœ€ä¼˜ç­–ç•¥ï¼Œç”šè‡³ä¸å¦‚éšæœºç­–ç•¥ã€‚è¿™æš—ç¤ºæ‰€è°“çš„â€œå¤§æ¨ç†æ¨¡å‹â€ï¼ˆLRMsï¼‰å¯èƒ½é¢ä¸´åŒæ ·çš„ç“¶é¢ˆã€‚\n\n**2. LLMs as Strategic Agents: Beliefs, Best Response Behavior, and Emergent Heuristics**\n**LLM ä½œä¸ºæˆ˜ç•¥ Agentï¼šä¿¡å¿µã€æœ€ä½³åº”å¯¹è¡Œä¸ºä¸æ¶Œç°çš„å¯å‘å¼æ–¹æ³•**\n*   **ä¸»è¦è´¡çŒ®**ï¼šç ”ç©¶äº† LLM åœ¨åšå¼ˆè®ºåœºæ™¯ä¸‹çš„è¡¨ç°ã€‚å‘ç°å‰æ²¿æ¨¡å‹åœ¨å—é™æ·±åº¦ä¸‹èƒ½è¡¨ç°å‡ºåŸºäºä¿¡å¿µçš„è¿è´¯å†³ç­–ã€‚ä½†åœ¨æ— çº¦æŸæƒ…å†µä¸‹ï¼Œå®ƒä»¬ä¼šå±•ç°å‡ºä¸€ç§**å…ƒæ¨ç†ï¼ˆMeta-reasoningï¼‰**ï¼Œå³é€šè¿‡è‡ªç”Ÿæˆçš„å¯å‘å¼è§„åˆ™ï¼ˆHeuristic rulesï¼‰æ¥åšå†³ç­–ï¼Œè¿™äº›è§„åˆ™ç¨³å®šä¸”ä¸åŒäºäººç±»çš„è®¤çŸ¥åå·®ã€‚\n\n**3. [å®‰å…¨] One Token Embedding Is Enough to Deadlock Your Large Reasoning Model**\n**ä¸€ä¸ª Token åµŒå…¥è¶³ä»¥è®©ä½ çš„æ¨ç†å¤§æ¨¡å‹é™·å…¥æ­»é”**\n*   **æ ¸å¿ƒå‘ç°**ï¼šé’ˆå¯¹æ€ç»´é“¾ï¼ˆCoTï¼‰çš„æ–°å‹æ”»å‡»â€”â€”**æ­»é”æ”»å‡»ï¼ˆDeadlock Attackï¼‰**ã€‚æ”»å‡»è€…é€šè¿‡è®­ç»ƒæ¶æ„çš„ Adversarial Embeddingï¼Œè¯±å¯¼æ¨¡å‹åœ¨æ¨ç†æ­¥éª¤åä¸æ–­ç”Ÿæˆè¿‡æ¸¡è¯ï¼ˆå¦‚ \"Wait\", \"But\"ï¼‰ï¼Œå¯¼è‡´æ¨¡å‹é™·å…¥æ— é™å¾ªç¯çš„â€œè¿‡åº¦æ€è€ƒâ€ï¼Œç›´åˆ°è€—å°½ Token é™é¢ã€‚åœ¨ Phi-RM, R1-Llama ç­‰æ¨¡å‹ä¸Šæ”»å‡»æˆåŠŸç‡é«˜è¾¾ 100%ã€‚\n\n**4. BrowserAgent: Building Web Agents with Human-Inspired Web Browsing Actions**\n**BrowserAgentï¼šæ„å»ºåŸºäºäººç±»æµè§ˆè¡Œä¸ºçš„ Web Agents**\n*   **ä¸»è¦è´¡çŒ®**ï¼šç°æœ‰çš„ Web Agent è¿‡äºä¾èµ–å°†ç½‘é¡µè½¬ä¸ºæ–‡æœ¬ã€‚BrowserAgent æ¨¡ä»¿äººç±»çš„æµè§ˆè¡Œä¸ºï¼ˆæ»šåŠ¨ã€ç‚¹å‡»ã€è¾“å…¥ï¼‰ï¼Œç›´æ¥æ“ä½œåŸå§‹ç½‘é¡µã€‚é€šè¿‡ä¸¤é˜¶æ®µå¾®è°ƒï¼ˆSFT + RFTï¼‰å’Œæ˜¾å¼è®°å¿†æœºåˆ¶ï¼Œåœ¨ Open-QA ä»»åŠ¡ä¸Šè¡¨ç°å‡ºè‰²ã€‚\n\n---\n\n### ğŸ”¬ AI for Scienceï¼šå¤©ä½“ã€åŒ–å­¦ä¸ç”Ÿç‰©\nAI åœ¨ç§‘å­¦é¢†åŸŸçš„è½åœ°è¶Šæ¥è¶Šæ·±ï¼Œä»Šå¤©çš„å‡ ç¯‡æ–‡ç« è´¨é‡å¾ˆé«˜ã€‚\n\n**5. [å¤§ä½¬åŠ›ä½œ] Deep Learning in Astrophysics**\n**å¤©ä½“ç‰©ç†å­¦ä¸­çš„æ·±åº¦å­¦ä¹ **\n*   **Authors**ï¼šYuan-Sen Ting (ä¸æºæ£®)\n*   **æ‘˜è¦**ï¼šè¿™æ˜¯ä¸€ç¯‡ç»¼è¿°æ€§æ–‡ç« ã€‚è®¨è®ºäº†æ·±åº¦å­¦ä¹ å¦‚ä½•è¡¥å……ç»å…¸ç»Ÿè®¡å­¦ï¼Œå¤„ç†æ•°åäº¿çš„å¤©æ–‡è§‚æµ‹æ•°æ®ã€‚\n*   **æ ¸å¿ƒè§‚ç‚¹**ï¼šé‡ç‚¹åœ¨äºå¦‚ä½•å°†ç‰©ç†å¯¹ç§°æ€§ï¼ˆPhysical Symmetriesï¼‰ã€å®ˆæ’å®šå¾‹å’Œå¾®åˆ†æ–¹ç¨‹ç¼–ç åˆ°ç¥ç»ç½‘ç»œæ¶æ„ä¸­ã€‚æ–‡ç« è¿˜æ¢è®¨äº†ä»æ¨¡æ‹Ÿä¸­å­¦ä¹ ï¼ˆSimulation-based inferenceï¼‰ä»¥åŠåˆ©ç”¨ LLM è¿›è¡Œç§‘ç ”è‡ªåŠ¨åŒ–çš„å‰æ™¯ã€‚\n\n**6. Trustworthy Retrosynthesis: Eliminating Hallucinations with a Diverse Ensemble of Reaction Scorers**\n**å¯ä¿¡é€†åˆæˆï¼šåˆ©ç”¨å¤šæ ·åŒ–ååº”è¯„åˆ†å™¨é›†æˆæ¶ˆé™¤å¹»è§‰**\n*   **ä¸»è¦è´¡çŒ®**ï¼šåœ¨è¯ç‰©åˆæˆè·¯çº¿è®¾è®¡ï¼ˆé€†åˆæˆï¼‰ä¸­ï¼Œç”Ÿæˆæ¨¡å‹å®¹æ˜“äº§ç”Ÿâ€œå¹»è§‰â€ï¼ˆä¸å­˜åœ¨çš„åŒ–å­¦ååº”ï¼‰ã€‚ä½œè€…æå‡ºäº† **RetroTrim**ï¼Œé€šè¿‡é›†æˆå¤šç§åŸºäºæœºå™¨å­¦ä¹ å’ŒåŒ–å­¦æ•°æ®åº“çš„è¯„åˆ†ç­–ç•¥ï¼Œæœ‰æ•ˆè¿‡æ»¤å¹»è§‰ã€‚è¿™æ˜¯èµ¢å¾— Standard Industries ç™¾ä¸‡ç¾å…ƒé€†åˆæˆæŒ‘æˆ˜èµ›æ–¹æ¡ˆçš„åŸºç¡€ã€‚\n\n**7. Fast and Interpretable Protein Substructure Alignment via Optimal Transport**\n**åŸºäºæœ€ä¼˜ä¼ è¾“çš„å¿«é€Ÿå¯è§£é‡Šè›‹ç™½è´¨å­ç»“æ„å¯¹é½**\n*   **ä¸»è¦è´¡çŒ®**ï¼šæå‡ºäº† **PLASMA**ï¼Œå°†è›‹ç™½è´¨å­ç»“æ„å¯¹é½é—®é¢˜è½¬åŒ–ä¸ºæ­£åˆ™åŒ–çš„æœ€ä¼˜ä¼ è¾“ï¼ˆOptimal Transportï¼‰ä»»åŠ¡ã€‚ç›¸æ¯”ç°æœ‰æ–¹æ³•ï¼Œå®ƒèƒ½å¿«é€Ÿè¯†åˆ«è›‹ç™½è´¨çš„å±€éƒ¨ Motifï¼ˆå¦‚æ´»æ€§ä½ç‚¹ï¼‰ï¼Œå¯¹è¯ç‰©è®¾è®¡å’Œè›‹ç™½è´¨å·¥ç¨‹éå¸¸æœ‰ä»·å€¼ã€‚\n\n---\n\n### ğŸ‘ï¸ å¤šæ¨¡æ€ä¸è§†å¬ï¼šçœŸçš„å¬æ‡‚äº†å—ï¼Ÿ\nå¤šæ¨¡æ€é¢†åŸŸå¼€å§‹åæ€æ¨¡å‹å¯¹éæ–‡æœ¬ä¿¡æ¯çš„å¤„ç†æ·±åº¦ã€‚\n\n**8. [å¿…è¯»] Do Audio LLMs Really LISTEN, or Just Transcribe? Measuring Lexical vs. Acoustic Emotion Cues Reliance**\n**éŸ³é¢‘å¤§æ¨¡å‹æ˜¯çœŸçš„åœ¨â€œå¬â€ï¼Œè¿˜æ˜¯ä»…ä»…åœ¨è½¬å½•ï¼Ÿ**\n*   **æ ¸å¿ƒå‘ç°**ï¼šéå¸¸æœ‰æ„æ€çš„æ‰¹åˆ¤æ€§ç ”ç©¶ã€‚ä½œè€…æå‡ºäº† LISTEN åŸºå‡†æµ‹è¯•ï¼Œæ—¨åœ¨è§£è€¦æƒ…æ„Ÿç†è§£ä¸­çš„â€œè¯æ±‡çº¿ç´¢â€å’Œâ€œå£°å­¦çº¿ç´¢â€ã€‚\n*   **ç»“è®º**ï¼šç›®å‰çš„ SOTA éŸ³é¢‘å¤§æ¨¡å‹ï¼ˆLALMsï¼‰ä¸»è¦è¿˜æ˜¯åœ¨â€œè½¬å½•â€æ–‡æœ¬ï¼Œ**ä¸¥é‡ä¾èµ–è¯æ±‡è¯­ä¹‰**ï¼Œè€Œå¿½ç•¥äº†è¯­éŸ³è¯­è°ƒä¸­çš„æƒ…æ„Ÿä¿¡æ¯ã€‚å½“è¯æ±‡å’Œè¯­æ°”å†²çªæ—¶ï¼Œæ¨¡å‹å¾€å¾€ä¼šé€‰è¯æ±‡çš„æ„æ€ï¼Œè¯´æ˜å®ƒä»¬è¿˜ä¸æ˜¯çœŸæ­£çš„â€œå¬ä¼—â€ã€‚\n\n**9. OmniVideoBench: Towards Audio-Visual Understanding Evaluation for Omni MLLMs**\n**OmniVideoBenchï¼šé¢å‘å…¨èƒ½ MLLM çš„è§†å¬ç†è§£è¯„ä¼°**\n*   **ä¸»è¦è´¡çŒ®**ï¼šæŒ‡å‡ºç°æœ‰ Video Benchmarks å¾€å¾€å¿½ç•¥éŸ³é¢‘æˆ–è§†å¬ååŒã€‚æå‡ºäº†ä¸€ä¸ªæ–°çš„åŸºå‡†ï¼ŒåŒ…å« 1000 ä¸ªé«˜è´¨é‡é—®ç­”å¯¹ï¼Œå¼ºè°ƒæ—¶é—´æ¨ç†ã€å› æœæ¨æ–­å’Œè§†å¬äº’è¡¥æ€§ã€‚è¯„æµ‹æ˜¾ç¤ºå¼€æºæ¨¡å‹åœ¨çœŸæ­£çš„è§†å¬æ¨ç†ä¸Šä¸é—­æºæ¨¡å‹å·®è·å·¨å¤§ã€‚\n\n**10. Image-to-Video Transfer Learning based on Image-Language Foundation Models: A Comprehensive Survey**\n**åŸºäºå›¾æ–‡åŸºç¡€æ¨¡å‹çš„å›¾ç”Ÿè§†é¢‘è¿ç§»å­¦ä¹ ç»¼è¿°**\n*   **å†…å®¹**ï¼šç³»ç»Ÿæ€»ç»“äº†å¦‚ä½•å°†å¼ºå¤§çš„å›¾æ–‡æ¨¡å‹ï¼ˆILFMsï¼‰çš„èƒ½åŠ›è¿ç§»åˆ°è§†é¢‘ç”Ÿæˆå’Œç†è§£ä»»åŠ¡ä¸­ï¼Œæ¶µç›–äº†å†»ç»“ç‰¹å¾å’Œé€‚é…ç‰¹å¾ä¸¤å¤§ç±»æ–¹æ³•ã€‚\n\n---\n\n### ğŸ› ï¸ RAGã€æ¶æ„ä¸ç³»ç»Ÿ\nRAG æ­£åœ¨å‘ç‰¹å®šé¢†åŸŸï¼ˆé‡‘èã€ä»£ç ï¼‰å’Œæ›´å¤æ‚çš„ç»“æ„è¿›åŒ–ã€‚\n\n**11. VeritasFi: An Adaptable, Multi-tiered RAG Framework for Multi-modal Financial Question Answering**\n**VeritasFiï¼šä¸€ç§ç”¨äºå¤šæ¨¡æ€é‡‘èé—®ç­”çš„è‡ªé€‚åº”å¤šå±‚ RAG æ¡†æ¶**\n*   **è§£å†³ç—›ç‚¹**ï¼šé‡‘èæ–‡æ¡£æå…¶å¤æ‚ï¼ˆåŒ…å«å¤§é‡è¡¨æ ¼ã€å›¾è¡¨ï¼‰ï¼Œä¸”éœ€è¦å…¼é¡¾é€šç”¨æ€§å’Œå…¬å¸ç‰¹å¼‚æ€§ã€‚\n*   **æ–¹æ³•**ï¼šæå‡ºäº†æ··åˆæ£€ç´¢å¼•æ“ï¼ˆè¯­ä¹‰ç´¢å¼• + å®æ—¶å·¥å…· + ä¸“å®¶è®°å¿†åº“ï¼‰å’Œä¸¤é˜¶æ®µé‡æ’åºè®­ç»ƒç­–ç•¥ï¼ˆé€šç”¨é¢†åŸŸé¢„è®­ç»ƒ + å…¬å¸æ•°æ®å¾®è°ƒï¼‰ã€‚\n\n**12. DRIFT: Decompose, Retrieve, Illustrate, then Formalize Theorems**\n**DRIFTï¼šåˆ†è§£ã€æ£€ç´¢ã€ä¸¾ä¾‹ï¼Œç„¶åå½¢å¼åŒ–å®šç†**\n*   **ä¸»è¦è´¡çŒ®**ï¼šé’ˆå¯¹æ•°å­¦å®šç†çš„å½¢å¼åŒ–ï¼ˆAutoformalizationï¼‰ã€‚LLM å¾ˆéš¾ç›´æ¥å°†éå½¢å¼åŒ–æ•°å­¦è½¬ä¸º Lean ä»£ç ã€‚DRIFT æ¡†æ¶å…ˆå°†é—®é¢˜åˆ†è§£ä¸ºå­ç»„ä»¶ï¼Œæ£€ç´¢ç›¸å…³çš„æ•°å­¦åº“å‰æï¼Œå¹¶æ£€ç´¢ç›¸ä¼¼çš„â€œç¤ºä¾‹å®šç†â€ï¼Œè¿™è®© GPT-4 åœ¨ ProofNet ä¸Šçš„ F1 åˆ†æ•°ç¿»äº†ä¸€å€ã€‚\n\n**13. Discrete State Diffusion Models: A Sample Complexity Perspective**\n**ç¦»æ•£çŠ¶æ€æ‰©æ•£æ¨¡å‹ï¼šæ ·æœ¬å¤æ‚åº¦çš„è§†è§’**\n*   **ç†è®ºè´¡çŒ®**ï¼šè™½ç„¶è¿ç»­æ‰©æ•£æ¨¡å‹å¾ˆç«ï¼Œä½†å¤„ç†æ–‡æœ¬ç­‰ç¦»æ•£æ•°æ®çš„ç¦»æ•£æ‰©æ•£æ¨¡å‹ç†è®ºåŸºç¡€è–„å¼±ã€‚æœ¬æ–‡æä¾›äº†é¦–ä¸ªæ ·æœ¬å¤æ‚åº¦ç•Œé™ $\\widetilde{\\mathcal{O}}(Îµ^{-2})$ï¼Œä¸ºç¦»æ•£æ‰©æ•£æ¨¡å‹çš„è®­ç»ƒæ•ˆç‡æä¾›äº†ç†è®ºä¿è¯ã€‚\n\n---\n\n### ğŸƒâ€â™‚ï¸ å…¶ä»–å€¼å¾—å…³æ³¨çš„è®ºæ–‡\n*   **[æœºå™¨äºº] UniCoD**: å°†è§†è§‰-è¯­è¨€æ¨¡å‹çš„ç†è§£èƒ½åŠ›ä¸è§†è§‰ç”Ÿæˆçš„åŠ¨åŠ›å­¦å»ºæ¨¡èƒ½åŠ›ç»“åˆï¼Œç”¨äºæœºå™¨äººç­–ç•¥å­¦ä¹ ã€‚\n*   **[æ­¥æ€è¯†åˆ«] Mesh-Gait**: ä» 2D è½®å»“ç›´æ¥é‡å»º 3D çƒ­å›¾è¿›è¡Œæ­¥æ€è¯†åˆ«ï¼Œæ—¢ä¿ç•™äº† 3D å‡ ä½•ä¿¡æ¯åˆé¿å…äº†æ²‰é‡çš„è®¡ç®—å¼€é”€ã€‚\n*   **[åŒ»å­¦ AI] Bias in Deep Learning Models for Chest X-Ray**: æå‡ºäº†ä¸€ç§è½»é‡çº§çš„ Adapter æ–¹æ³•ï¼ˆCNN + XGBoostï¼‰ï¼Œåœ¨ä¸ç‰ºç‰²æ€§èƒ½çš„æƒ…å†µä¸‹å‡å°‘èƒ¸éƒ¨ X å…‰è¯Šæ–­ä¸­çš„æ€§åˆ«ã€ç§æ—åè§ã€‚\n*   **[å¯è§£é‡Šæ€§] Trace Length is a Simple Uncertainty Signal**: å‘ç°æ¨ç†æ¨¡å‹ç”Ÿæˆçš„ CoT è½¨è¿¹é•¿åº¦æœ¬èº«å°±æ˜¯ä¸€ä¸ªå¾ˆå¥½çš„ç½®ä¿¡åº¦æŒ‡æ ‡â€”â€”è½¨è¿¹è¶Šé•¿ï¼ˆè¿‡åº¦æ€è€ƒï¼‰ï¼Œå¾€å¾€æ„å‘³ç€æ¨¡å‹è¶Šä¸ç¡®å®šã€‚\n\n---\nå¸Œæœ›ä»Šå¤©çš„å¿«æŠ¥å¯¹ä½ çš„ç ”ç©¶æœ‰å¸®åŠ©ï¼æˆ‘ä»¬æ˜å¤©è§ã€‚",
  "papers": [
    {
      "arxiv_id": "2510.15974v1",
      "title": "Limits of Emergent Reasoning of Large Language Models in Agentic Frameworks for Deterministic Games",
      "title_zh": "å¤§è¯­è¨€æ¨¡å‹åœ¨ç¡®å®šæ€§åšå¼ˆæ™ºèƒ½ä½“æ¡†æ¶ä¸‹æ¶Œç°æ¨ç†èƒ½åŠ›çš„å±€é™æ€§",
      "authors": [
        "Chris Su",
        "Harrison Li",
        "Matheus Marques",
        "George Flint",
        "Kevin Zhu",
        "Sunishchal Dev"
      ],
      "abstract": "Recent work reports that Large Reasoning Models (LRMs) undergo a collapse in performance on solving puzzles beyond certain perplexity thresholds. In subsequent discourse, questions have arisen as to whether the nature of the task muddles an evaluation of true reasoning. One potential confound is the requirement that the model keep track of the state space on its own. We provide a large language model (LLM) with an environment interface for Tower of Hanoi problems, allowing it to make a move with a tool call, provide written justification, observe the resulting state space, and reprompt itself for the next move. We observe that access to an environment interface does not delay or eradicate performance collapse. Furthermore, LLM-parameterized policy analysis reveals increasing divergence from both optimal policies and uniformly random policies, suggesting that the model exhibits mode-like collapse at each level of complexity, and that performance is dependent upon whether the mode reflects the correct solution for the problem. We suggest that a similar phenomena might take place in LRMs.",
      "tldr_zh": "è¿™é¡¹ç ”ç©¶æ¢è®¨äº†å¤§å‹æ¨ç†æ¨¡å‹(LRMs)åœ¨ç¡®å®šæ€§æ¸¸æˆæ™ºèƒ½æ¡†æ¶ä¸­æ¶Œç°æ¨ç†èƒ½åŠ›çš„å±€é™æ€§ï¼Œç‰¹åˆ«æ˜¯é’ˆå¯¹æ€§èƒ½éšå¤æ‚åº¦å¢åŠ è€Œå´©å¡Œçš„ç°è±¡ã€‚ç ”ç©¶è€…é€šè¿‡ä¸º LLM æä¾›æ²³å†…å¡”(Tower of Hanoi)é—®é¢˜çš„ç¯å¢ƒæ¥å£ï¼Œå…è®¸å…¶é€šè¿‡å·¥å…·è°ƒç”¨(tool call)æ‰§è¡Œæ“ä½œã€æ’°å†™è®ºè¯å¹¶è§‚å¯ŸçŠ¶æ€ç©ºé—´ï¼Œä»¥éªŒè¯çŠ¶æ€è·Ÿè¸ªèƒ½åŠ›æ˜¯å¦æ˜¯å¯¼è‡´æ¨ç†å¤±è´¥çš„å¹²æ‰°å› ç´ ã€‚å®éªŒå‘ç°ï¼Œå³ä¾¿å…·å¤‡ç¯å¢ƒæ¥å£æ”¯æŒï¼Œæ¨¡å‹ä¾ç„¶æ— æ³•é¿å…æ€§èƒ½å´©å¡Œï¼Œä¸”ç­–ç•¥åˆ†ææ˜¾ç¤ºæ¨¡å‹åœ¨ä¸åŒå¤æ‚åº¦ä¸‹è¡¨ç°å‡ºæ˜æ˜¾çš„æ¨¡å¼å´©å¡Œ(mode-like collapse)ã€‚ç ”ç©¶æŒ‡å‡ºï¼Œæ¨¡å‹æ€§èƒ½é«˜åº¦ä¾èµ–äºå…¶å†…éƒ¨æ¨¡å¼(mode)æ˜¯å¦ä¸æ­£ç¡®è§£ä¸€è‡´ï¼Œè€Œéå…·å¤‡çœŸæ­£çš„é€»è¾‘æ¨ç†èƒ½åŠ›ã€‚è¿™ä¸€å‘ç°æš—ç¤º LRMs å†…éƒ¨å¯èƒ½å­˜åœ¨ç±»ä¼¼çš„ç°è±¡ï¼Œä¸ºç†è§£å’Œè¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å¤æ‚ä»»åŠ¡ä¸­çš„æ¨ç†ç“¶é¢ˆæä¾›äº†é‡è¦è§è§£ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.15974v1",
      "published_date": "2025-10-12 23:48:16 UTC",
      "updated_date": "2025-10-12 23:48:16 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T02:48:58.577586+00:00"
    },
    {
      "arxiv_id": "2510.10854v1",
      "title": "Discrete State Diffusion Models: A Sample Complexity Perspective",
      "title_zh": "ç¦»æ•£çŠ¶æ€æ‰©æ•£æ¨¡å‹ï¼šæ ·æœ¬å¤æ‚åº¦è§†è§’",
      "authors": [
        "Aadithya Srikanth",
        "Mudit Gaur",
        "Vaneet Aggarwal"
      ],
      "abstract": "Diffusion models have demonstrated remarkable performance in generating high-dimensional samples across domains such as vision, language, and the sciences. Although continuous-state diffusion models have been extensively studied both empirically and theoretically, discrete-state diffusion models, essential for applications involving text, sequences, and combinatorial structures, remain significantly less understood from a theoretical standpoint. In particular, all existing analyses of discrete-state models assume score estimation error bounds without studying sample complexity results. In this work, we present a principled theoretical framework for discrete-state diffusion, providing the first sample complexity bound of $\\widetilde{\\mathcal{O}}(Îµ^{-2})$. Our structured decomposition of the score estimation error into statistical, approximation, optimization, and clipping components offers critical insights into how discrete-state models can be trained efficiently. This analysis addresses a fundamental gap in the literature and establishes the theoretical tractability and practical relevance of discrete-state diffusion models.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†ç¦»æ•£çŠ¶æ€æ‰©æ•£æ¨¡å‹(Discrete-state diffusion models)åœ¨å¤„ç†æ–‡æœ¬ã€åºåˆ—åŠç»„åˆç»“æ„ç­‰åº”ç”¨æ—¶çš„ç†è®ºåŸºç¡€ï¼Œé’ˆå¯¹å…¶æ ·æœ¬å¤æ‚åº¦(sample complexity)ç ”ç©¶ä¸è¶³çš„ç°çŠ¶æå‡ºäº†ç³»ç»Ÿæ€§çš„ç†è®ºæ¡†æ¶ã€‚è¯¥å·¥ä½œé¦–æ¬¡æ¨å¯¼å‡ºäº† $\\widetilde{\\mathcal{O}}(\\epsilon^{-2})$ çš„æ ·æœ¬å¤æ‚åº¦ç•Œé™ï¼Œå¡«è¡¥äº†è¯¥é¢†åŸŸåœ¨ç†è®ºåˆ†ææ–¹é¢çš„ç ”ç©¶ç©ºç™½ã€‚ç ”ç©¶è€…é€šè¿‡å¯¹åˆ†æ•°ä¼°è®¡è¯¯å·®(score estimation error)è¿›è¡Œç»“æ„åŒ–åˆ†è§£ï¼Œå°†å…¶ç»†åŒ–ä¸ºç»Ÿè®¡ã€é€¼è¿‘ã€ä¼˜åŒ–å’Œè£å‰ªå››ä¸ªç»´åº¦ï¼Œä¸ºç¦»æ•£æ¨¡å‹çš„é«˜æ•ˆè®­ç»ƒæä¾›äº†å…³é”®è§è§£ã€‚è¿™ä¸€åˆ†æä¸ä»…åœ¨ç†è®ºä¸Šè¯æ˜äº†ç¦»æ•£çŠ¶æ€æ‰©æ•£æ¨¡å‹çš„å¯å¤„ç†æ€§ï¼Œä¹Ÿä¸ºå…¶åœ¨å®é™…åœºæ™¯ä¸­çš„åº”ç”¨æä¾›äº†é‡è¦çš„æŒ‡å¯¼æ„ä¹‰ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "stat.ML"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.10854v1",
      "published_date": "2025-10-12 23:33:46 UTC",
      "updated_date": "2025-10-12 23:33:46 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T02:48:59.084958+00:00"
    },
    {
      "arxiv_id": "2510.10840v1",
      "title": "Software Defect Prediction using Autoencoder Transformer Model",
      "title_zh": "åŸºäºè‡ªåŠ¨ç¼–ç å™¨ Transformer æ¨¡å‹çš„è½¯ä»¶ç¼ºé™·é¢„æµ‹",
      "authors": [
        "Seshu Barma",
        "Mohanakrishnan Hariharan",
        "Satish Arvapalli"
      ],
      "abstract": "An AI-ML-powered quality engineering approach uses AI-ML to enhance software quality assessments by predicting defects. Existing ML models struggle with noisy data types, imbalances, pattern recognition, feature extraction, and generalization. To address these challenges, we develop a new model, Adaptive Differential Evolution (ADE) based Quantum Variational Autoencoder-Transformer (QVAET) Model (ADE-QVAET). ADE combines with QVAET to obtain high-dimensional latent features and maintain sequential dependencies, resulting in enhanced defect prediction accuracy. ADE optimization enhances model convergence and predictive performance. ADE-QVAET integrates AI-ML techniques such as tuning hyperparameters for scalable and accurate software defect prediction, representing an AI-ML-driven technology for quality engineering. During training with a 90% training percentage, ADE-QVAET achieves high accuracy, precision, recall, and F1-score of 98.08%, 92.45%, 94.67%, and 98.12%, respectively, when compared to the Differential Evolution (DE) ML model.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ç°æœ‰æœºå™¨å­¦ä¹ æ¨¡å‹åœ¨å¤„ç†å™ªå£°æ•°æ®ã€ç±»åˆ«ä¸å¹³è¡¡åŠç‰¹å¾æå–æ–¹é¢çš„å±€é™æ€§ï¼Œæå‡ºäº†ä¸€ç§åŸºäºè‡ªé€‚åº”å·®åˆ†è¿›åŒ–(Adaptive Differential Evolution, ADE)çš„é‡å­å˜åˆ†è‡ªåŠ¨ç¼–ç å™¨-è½¬æ¢å™¨æ¨¡å‹(Quantum Variational Autoencoder-Transformer, QVAET)ï¼Œå³ADE-QVAETã€‚è¯¥æ¨¡å‹é€šè¿‡ç»“åˆADEä¼˜åŒ–ä¸QVAETç»“æ„ï¼Œæœ‰æ•ˆæå–é«˜ç»´æ½œåœ¨ç‰¹å¾å¹¶ç»´æŒåºåˆ—ä¾èµ–æ€§ï¼Œæ˜¾è‘—æå‡äº†è½¯ä»¶ç¼ºé™·é¢„æµ‹çš„å‡†ç¡®æ€§ã€‚ADEç®—æ³•çš„åº”ç”¨ä¸ä»…åŠ é€Ÿäº†æ¨¡å‹çš„æ”¶æ•›ï¼Œè¿˜é€šè¿‡è¶…å‚æ•°å¾®è°ƒå¢å¼ºäº†é¢„æµ‹æ€§èƒ½ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œåœ¨90%çš„è®­ç»ƒæ¯”ä¾‹ä¸‹ï¼ŒADE-QVAETå®ç°äº†98.08%çš„å‡†ç¡®ç‡ã€92.45%çš„ç²¾ç¡®ç‡ä»¥åŠ98.12%çš„F1-scoreï¼Œæ€§èƒ½æ˜æ˜¾ä¼˜äºä¼ ç»Ÿçš„å·®åˆ†è¿›åŒ–(DE)æ¨¡å‹ã€‚è¿™ä¸€AI-MLé©±åŠ¨çš„æŠ€æœ¯ä¸ºè´¨é‡å·¥ç¨‹é¢†åŸŸæä¾›äº†ä¸€ç§å…·æœ‰é«˜æ‰©å±•æ€§å’Œé«˜ç²¾åº¦çš„è½¯ä»¶è´¨é‡è¯„ä¼°æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.SE",
        "cs.AI"
      ],
      "primary_category": "cs.SE",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.10840v1",
      "published_date": "2025-10-12 23:03:50 UTC",
      "updated_date": "2025-10-12 23:03:50 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T02:49:03.878250+00:00"
    },
    {
      "arxiv_id": "2510.10828v1",
      "title": "VeritasFi: An Adaptable, Multi-tiered RAG Framework for Multi-modal Financial Question Answering",
      "title_zh": "VeritasFiï¼šä¸€ç§é¢å‘å¤šæ¨¡æ€é‡‘èé—®ç­”çš„è‡ªé€‚åº”å¤šå±‚çº§ RAG æ¡†æ¶",
      "authors": [
        "Zhenghan Tai",
        "Hanwei Wu",
        "Qingchen Hu",
        "Jijun Chi",
        "Hailin He",
        "Lei Ding",
        "Tung Sum Thomas Kwok",
        "Bohuai Xiao",
        "Yuchen Hua",
        "Suyuchen Wang",
        "Peng Lu",
        "Muzhi Li",
        "Yihong Wu",
        "Liheng Ma",
        "Jerry Huang",
        "Jiayi Zhang",
        "Gonghao Zhang",
        "Chaolong Jiang",
        "Jingrui Tian",
        "Sicheng Lyu",
        "Zeyu Li",
        "Boyu Han",
        "Fengran Mo",
        "Xinyue Yu",
        "Yufei Cui",
        "Ling Zhou",
        "Xinyu Wang"
      ],
      "abstract": "Retrieval-Augmented Generation (RAG) is becoming increasingly essential for Question Answering (QA) in the financial sector, where accurate and contextually grounded insights from complex public disclosures are crucial. However, existing financial RAG systems face two significant challenges: (1) they struggle to process heterogeneous data formats, such as text, tables, and figures; and (2) they encounter difficulties in balancing general-domain applicability with company-specific adaptation. To overcome these challenges, we present VeritasFi, an innovative hybrid RAG framework that incorporates a multi-modal preprocessing pipeline alongside a cutting-edge two-stage training strategy for its re-ranking component. VeritasFi enhances financial QA through three key innovations: (1) A multi-modal preprocessing pipeline that seamlessly transforms heterogeneous data into a coherent, machine-readable format. (2) A tripartite hybrid retrieval engine that operates in parallel, combining deep multi-path retrieval over a semantically indexed document corpus, real-time data acquisition through tool utilization, and an expert-curated memory bank for high-frequency questions, ensuring comprehensive scope, accuracy, and efficiency. (3) A two-stage training strategy for the document re-ranker, which initially constructs a general, domain-specific model using anonymized data, followed by rapid fine-tuning on company-specific data for targeted applications. By integrating our proposed designs, VeritasFi presents a groundbreaking framework that greatly enhances the adaptability and robustness of financial RAG systems, providing a scalable solution for both general-domain and company-specific QA tasks. Code accompanying this work is available at https://github.com/simplew4y/VeritasFi.git.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† VeritasFiï¼Œä¸€ç§é’ˆå¯¹é‡‘èé¢†åŸŸå¤šæ¨¡æ€é—®ç­”ï¼ˆFinancial Question Answeringï¼‰è®¾è®¡çš„è‡ªé€‚åº”å¤šå±‚çº§ RAG æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³ç°æœ‰ç³»ç»Ÿåœ¨å¤„ç†æ–‡æœ¬ã€è¡¨æ ¼å’Œå›¾åƒç­‰å¼‚æ„æ•°æ®ä»¥åŠå¹³è¡¡é€šç”¨é¢†åŸŸä¸ç‰¹å®šå…¬å¸éœ€æ±‚æ–¹é¢çš„æŒ‘æˆ˜ã€‚è¯¥æ¡†æ¶å¼•å…¥äº†å¤šæ¨¡æ€é¢„å¤„ç†æµæ°´çº¿ï¼Œèƒ½å¤Ÿå°†å¤æ‚çš„å¼‚æ„æ•°æ®æ— ç¼è½¬æ¢ä¸ºæœºå™¨å¯è¯»æ ¼å¼ï¼Œç¡®ä¿ä¿¡æ¯çš„å®Œæ•´æ€§ã€‚å…¶æ ¸å¿ƒé‡‡ç”¨äº†ä¸‰å…ƒæ··åˆæ£€ç´¢å¼•æ“ï¼Œé€šè¿‡ç»“åˆæ·±åº¦å¤šè·¯å¾„æ£€ç´¢ã€å®æ—¶å·¥å…·è°ƒç”¨å’Œä¸“å®¶çŸ¥è¯†åº“ï¼Œå®ç°äº†æ£€ç´¢èŒƒå›´ã€å‡†ç¡®æ€§ä¸æ•ˆç‡çš„é«˜åº¦ç»Ÿä¸€ã€‚é’ˆå¯¹é‡æ’åºå™¨ï¼ˆre-rankerï¼‰ï¼Œç ”ç©¶è€…è®¾è®¡äº†ä¸¤é˜¶æ®µè®­ç»ƒç­–ç•¥ï¼Œå…ˆåœ¨åŒ¿åé¢†åŸŸæ•°æ®ä¸Šæ„å»ºé€šç”¨æ¨¡å‹ï¼Œå†é’ˆå¯¹ç‰¹å®šå…¬å¸æ•°æ®è¿›è¡Œå¿«é€Ÿå¾®è°ƒä»¥å®ç°ç²¾å‡†é€‚é…ã€‚VeritasFi æ˜¾è‘—å¢å¼ºäº†é‡‘è RAG ç³»ç»Ÿçš„é²æ£’æ€§ä¸å¯æ‰©å±•æ€§ï¼Œä¸ºé€šç”¨åŠç‰¹å®šå…¬å¸çš„é‡‘èé—®ç­”ä»»åŠ¡æä¾›äº†ä¸€ç§æå…·åˆ›æ–°æ€§çš„è§£å†³æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.IR",
        "cs.AI"
      ],
      "primary_category": "cs.IR",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.10828v1",
      "published_date": "2025-10-12 22:45:24 UTC",
      "updated_date": "2025-10-12 22:45:24 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T02:49:08.458026+00:00"
    },
    {
      "arxiv_id": "2510.10827v1",
      "title": "Happiness is Sharing a Vocabulary: A Study of Transliteration Methods",
      "title_zh": "å…±äº«è¯æ±‡ä¹‹ç›Šï¼šéŸ³è¯‘æ–¹æ³•ç ”ç©¶",
      "authors": [
        "Haeji Jung",
        "Jinju Kim",
        "Kyungjin Kim",
        "Youjeong Roh",
        "David R. Mortensen"
      ],
      "abstract": "Transliteration has emerged as a promising means to bridge the gap between various languages in multilingual NLP, showing promising results especially for languages using non-Latin scripts. We investigate the degree to which shared script, overlapping token vocabularies, and shared phonology contribute to performance of multilingual models. To this end, we conduct controlled experiments using three kinds of transliteration (romanization, phonemic transcription, and substitution ciphers) as well as orthography. We evaluate each model on two downstream tasks -- named entity recognition (NER) and natural language inference (NLI) -- and find that romanization significantly outperforms other input types in 7 out of 8 evaluation settings, largely consistent with our hypothesis that it is the most effective approach. We further analyze how each factor contributed to the success, and suggest that having longer (subword) tokens shared with pre-trained languages leads to better utilization of the model.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†åœ¨å¤šè¯­è¨€è‡ªç„¶è¯­è¨€å¤„ç†(NLP)ä¸­ï¼Œè½¬å†™(Transliteration)å¦‚ä½•é€šè¿‡ç¼©å°éæ‹‰ä¸è„šæœ¬è¯­è¨€ä¹‹é—´çš„å·®è·æ¥æé«˜æ¨¡å‹æ€§èƒ½ã€‚ä½œè€…é€šè¿‡å—æ§å®éªŒæ·±å…¥ç ”ç©¶äº†å…±äº«è„šæœ¬(shared script)ã€é‡å çš„Tokenè¯æ±‡è¡¨(overlapping token vocabularies)å’Œå…±äº«éŸ³éŸµ(shared phonology)å¯¹å¤šè¯­è¨€æ¨¡å‹è¡¨ç°çš„å…·ä½“è´¡çŒ®ã€‚å®éªŒå¯¹æ¯”äº†ç½—é©¬åŒ–(romanization)ã€éŸ³ä½è½¬å†™(phonemic transcription)ã€æ›¿æ¢åŠ å¯†(substitution ciphers)ä»¥åŠåŸå§‹æ­£å­—æ³•(orthography)å››ç§è¾“å…¥æ–¹å¼ã€‚ç ”ç©¶åœ¨å‘½åå®ä½“è¯†åˆ«(NER)å’Œè‡ªç„¶è¯­è¨€æ¨ç†(NLI)ä¸¤ä¸ªä¸‹æ¸¸ä»»åŠ¡ä¸Šè¿›è¡Œäº†è¯„ä¼°ï¼Œå‘ç°ç½—é©¬åŒ–(romanization)åœ¨ç»å¤§å¤šæ•°å®éªŒè®¾ç½®ä¸­æ˜¾è‘—ä¼˜äºå…¶ä»–æ–¹æ³•ï¼ŒéªŒè¯äº†å…¶æœ‰æ•ˆæ€§ã€‚è¿›ä¸€æ­¥åˆ†æè¡¨æ˜ï¼Œä¸é¢„è®­ç»ƒè¯­è¨€å…±äº«æ›´é•¿çš„å­è¯(subword) Tokenæ˜¯æå‡æ¨¡å‹åˆ©ç”¨ç‡çš„å…³é”®å› ç´ ã€‚è¯¥ç ”ç©¶ä¸ºä¼˜åŒ–å¤šè¯­è¨€æ¨¡å‹çš„è·¨è¯­è¨€è¿ç§»èƒ½åŠ›æä¾›äº†é‡è¦çš„ç†è®ºä¾æ®å’Œå®è·µæŒ‡å¯¼ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.10827v1",
      "published_date": "2025-10-12 22:34:40 UTC",
      "updated_date": "2025-10-12 22:34:40 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T02:49:11.791907+00:00"
    },
    {
      "arxiv_id": "2510.10824v1",
      "title": "Agentic RAG for Software Testing with Hybrid Vector-Graph and Multi-Agent Orchestration",
      "title_zh": "åŸºäºæ··åˆå‘é‡-å›¾ä¸å¤šæ™ºèƒ½ä½“ç¼–æ’çš„è½¯ä»¶æµ‹è¯• Agentic RAG",
      "authors": [
        "Mohanakrishnan Hariharan",
        "Satish Arvapalli",
        "Seshu Barma",
        "Evangeline Sheela"
      ],
      "abstract": "We present an approach to software testing automation using Agentic Retrieval-Augmented Generation (RAG) systems for Quality Engineering (QE) artifact creation. We combine autonomous AI agents with hybrid vector-graph knowledge systems to automate test plan, case, and QE metric generation. Our approach addresses traditional software testing limitations by leveraging LLMs such as Gemini and Mistral, multi-agent orchestration, and enhanced contextualization. The system achieves remarkable accuracy improvements from 65% to 94.8% while ensuring comprehensive document traceability throughout the quality engineering lifecycle. Experimental validation of enterprise Corporate Systems Engineering and SAP migration projects demonstrates an 85% reduction in testing timeline, an 85% improvement in test suite efficiency, and projected 35% cost savings, resulting in a 2-month acceleration of go-live.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§åˆ©ç”¨æ™ºèƒ½ä½“æ£€ç´¢å¢å¼ºç”Ÿæˆ(Agentic Retrieval-Augmented Generation, RAG)ç³»ç»Ÿè‡ªåŠ¨åŒ–è½¯ä»¶æµ‹è¯•çš„æ–¹æ³•ï¼Œæ—¨åœ¨ç”Ÿæˆè´¨é‡å·¥ç¨‹(Quality Engineering, QE)äº¤ä»˜ç‰©ã€‚é€šè¿‡ç»“åˆè‡ªä¸»AIæ™ºèƒ½ä½“ä¸æ··åˆå‘é‡-å›¾(hybrid vector-graph)çŸ¥è¯†ç³»ç»Ÿï¼Œè¯¥æ–¹æ³•å®ç°äº†æµ‹è¯•è®¡åˆ’ã€æµ‹è¯•ç”¨ä¾‹å’ŒQEæŒ‡æ ‡ç”Ÿæˆçš„è‡ªåŠ¨åŒ–ï¼Œæœ‰æ•ˆå…‹æœäº†ä¼ ç»Ÿè½¯ä»¶æµ‹è¯•çš„å±€é™æ€§ã€‚è¯¥ç³»ç»Ÿåˆ©ç”¨Geminiå’ŒMistralç­‰å¤§è¯­è¨€æ¨¡å‹(LLMs)é…åˆå¤šæ™ºèƒ½ä½“ç¼–æ’(multi-agent orchestration)ï¼Œåœ¨å¢å¼ºä¸Šä¸‹æ–‡å…³è”çš„åŒæ—¶ç¡®ä¿äº†è´¨é‡å·¥ç¨‹ç”Ÿå‘½å‘¨æœŸå†…çš„æ–‡æ¡£å¯è¿½æº¯æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ¡ˆå°†æµ‹è¯•å‡†ç¡®ç‡ä»65%æ˜¾è‘—æå‡è‡³94.8%ï¼Œå¹¶åœ¨ä¼ä¸šçº§é¡¹ç›®éªŒè¯ä¸­å‡å°‘äº†85%çš„æµ‹è¯•è€—æ—¶ã€‚æ­¤å¤–ï¼Œç ”ç©¶æ˜¾ç¤ºæµ‹è¯•å¥—ä»¶æ•ˆç‡æå‡äº†85%å¹¶é¢„è®¡èŠ‚çœ35%çš„æˆæœ¬ï¼Œæœ€ç»ˆä½¿é¡¹ç›®ä¸Šçº¿æ—¶é—´æå‰äº†2ä¸ªæœˆï¼Œä¸ºé«˜æ•ˆã€è‡ªåŠ¨åŒ–çš„è½¯ä»¶è´¨é‡ä¿è¯æä¾›äº†æœ‰åŠ›æ”¯æŒã€‚",
      "categories": [
        "cs.SE",
        "cs.AI"
      ],
      "primary_category": "cs.SE",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.10824v1",
      "published_date": "2025-10-12 22:25:15 UTC",
      "updated_date": "2025-10-12 22:25:15 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T02:49:13.691414+00:00"
    },
    {
      "arxiv_id": "2510.10823v1",
      "title": "The Irrational Machine: Neurosis and the Limits of Algorithmic Safety",
      "title_zh": "éç†æ€§æœºå™¨ï¼šç¥ç»ç—‡ä¸ç®—æ³•å®‰å…¨çš„å±€é™æ€§",
      "authors": [
        "Daniel Howard"
      ],
      "abstract": "We present a framework for characterizing neurosis in embodied AI: behaviors that are internally coherent yet misaligned with reality, arising from interactions among planning, uncertainty handling, and aversive memory. In a grid navigation stack we catalogue recurrent modalities including flip-flop, plan churn, perseveration loops, paralysis and hypervigilance, futile search, belief incoherence, tie break thrashing, corridor thrashing, optimality compulsion, metric mismatch, policy oscillation, and limited-visibility variants. For each we give lightweight online detectors and reusable escape policies (short commitments, a margin to switch, smoothing, principled arbitration). We then show that durable phobic avoidance can persist even under full visibility when learned aversive costs dominate local choice, producing long detours despite globally safe routes. Using First/Second/Third Law as engineering shorthand for safety latency, command compliance, and resource efficiency, we argue that local fixes are insufficient; global failures can remain. To surface them, we propose genetic-programming based destructive testing that evolves worlds and perturbations to maximize law pressure and neurosis scores, yielding adversarial curricula and counterfactual traces that expose where architectural revision, not merely symptom-level patches, is required.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ä¸ªåˆ»ç”»å…·èº«æ™ºèƒ½(Embodied AI)ä¸­â€œç¥ç»ç—‡â€(Neurosis)è¡Œä¸ºçš„æ¡†æ¶ï¼Œæ¢è®¨äº†è§„åˆ’ã€ä¸ç¡®å®šæ€§å¤„ç†ä¸åŒæ¶è®°å¿†(Aversive Memory)ç›¸äº’ä½œç”¨ä¸‹äº§ç”Ÿçš„ä¸ç°å®å¤±è°ƒä½†å†…éƒ¨è¿è´¯çš„å¼‚å¸¸è¡Œä¸ºã€‚ä½œè€…åœ¨ç½‘æ ¼å¯¼èˆªç³»ç»Ÿä¸­å½’çº³äº†åŒ…æ‹¬è®¡åˆ’æŠ–åŠ¨(Plan Churn)ã€ç˜«ç—ªä¸è¿‡åº¦è­¦è§‰(Paralysis and Hypervigilance)ä»¥åŠä¿¡å¿µä¸ä¸€è‡´(Belief Incoherence)ç­‰å¤šç§å¾ªç¯å‡ºç°çš„ç¥ç»ç—‡æ¨¡å¼ã€‚é’ˆå¯¹è¿™äº›æ¨¡å¼ï¼Œç ”ç©¶è®¾è®¡äº†è½»é‡çº§åœ¨çº¿æ£€æµ‹å™¨å’Œå¯é‡ç”¨çš„é€ƒé€¸ç­–ç•¥(Escape Policies)ï¼Œå¦‚çŸ­æœŸæ‰¿è¯º(Short Commitments)å’Œåˆ‡æ¢è£•åº¦(Margin to Switch)ç­‰ã€‚è®ºæ–‡è¿›ä¸€æ­¥æ­ç¤ºäº†å³ä½¿åœ¨ä¿¡æ¯å®Œå…¨é€æ˜çš„æƒ…å†µä¸‹ï¼Œå­¦ä¹ åˆ°çš„åŒæ¶æˆæœ¬ä¹Ÿå¯èƒ½å¯¼è‡´æŒä¹…çš„â€œææ€–è§„é¿â€(Phobic Avoidance)ï¼Œä½¿ç³»ç»Ÿäº§ç”Ÿéç†æ€§çš„é•¿è·ç¦»ç»•è·¯ã€‚ç ”ç©¶äººå‘˜è¿˜åˆ©ç”¨åŸºäºé—ä¼ è§„åˆ’(Genetic-programming)çš„ç ´åæ€§æµ‹è¯•æ¥æ¼”åŒ–æç«¯ç¯å¢ƒï¼Œé€šè¿‡æœ€å¤§åŒ–ç¥ç»ç—‡å¾—åˆ†æ¥ç”Ÿæˆå¯¹æŠ—æ€§è¯¾ç¨‹(Adversarial Curricula)ä»¥æš´éœ²ç³»ç»Ÿç¼ºé™·ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œå±€éƒ¨ä¿®å¤ä¸è¶³ä»¥è§£å†³è¿™ç±»å…¨å±€æ€§æ•…éšœï¼Œå¼ºè°ƒäº†è¿›è¡Œæ¶æ„å±‚é¢ä¿®è®¢å¯¹äºæå‡ç®—æ³•å®‰å…¨æ€§(Algorithmic Safety)çš„å¿…è¦æ€§ã€‚",
      "categories": [
        "cs.AI",
        "cs.NE",
        "cs.RO"
      ],
      "primary_category": "cs.AI",
      "comment": "41 pages, 17 figures, 5 tables",
      "pdf_url": "https://arxiv.org/pdf/2510.10823v1",
      "published_date": "2025-10-12 22:22:17 UTC",
      "updated_date": "2025-10-12 22:22:17 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T02:49:20.172449+00:00"
    },
    {
      "arxiv_id": "2510.10822v1",
      "title": "From Detection to Mitigation: Addressing Bias in Deep Learning Models for Chest X-Ray Diagnosis",
      "title_zh": "ä»æ£€æµ‹åˆ°ç¼“è§£ï¼šåº”å¯¹èƒ¸éƒ¨ X å°„çº¿è¯Šæ–­æ·±åº¦å­¦ä¹ æ¨¡å‹ä¸­çš„åè§",
      "authors": [
        "Clemence Mottez",
        "Louisa Fay",
        "Maya Varma",
        "Sophie Ostmeier",
        "Curtis Langlotz"
      ],
      "abstract": "Deep learning models have shown promise in improving diagnostic accuracy from chest X-rays, but they also risk perpetuating healthcare disparities when performance varies across demographic groups. In this work, we present a comprehensive bias detection and mitigation framework targeting sex, age, and race-based disparities when performing diagnostic tasks with chest X-rays. We extend a recent CNN-XGBoost pipeline to support multi-label classification and evaluate its performance across four medical conditions. We show that replacing the final layer of CNN with an eXtreme Gradient Boosting classifier improves the fairness of the subgroup while maintaining or improving the overall predictive performance. To validate its generalizability, we apply the method to different backbones, namely DenseNet-121 and ResNet-50, and achieve similarly strong performance and fairness outcomes, confirming its model-agnostic design. We further compare this lightweight adapter training method with traditional full-model training bias mitigation techniques, including adversarial training, reweighting, data augmentation, and active learning, and find that our approach offers competitive or superior bias reduction at a fraction of the computational cost. Finally, we show that combining eXtreme Gradient Boosting retraining with active learning yields the largest reduction in bias across all demographic subgroups, both in and out of distribution on the CheXpert and MIMIC datasets, establishing a practical and effective path toward equitable deep learning deployment in clinical radiology.",
      "tldr_zh": "æœ¬ç ”ç©¶æå‡ºäº†ä¸€å¥—å…¨é¢çš„åè§æ£€æµ‹ä¸ç¼“è§£æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³èƒ¸éƒ¨Xå°„çº¿(Chest X-Ray)è¯Šæ–­æ¨¡å‹åœ¨æ€§åˆ«ã€å¹´é¾„å’Œç§æ—ç­‰äººå£ç»Ÿè®¡ç¾¤ä½“ä¸­å­˜åœ¨çš„åŒ»ç–—å·®å¼‚é—®é¢˜ã€‚ç ”ç©¶è€…é€šè¿‡æ‰©å±•CNN-XGBoostæµæ°´çº¿ä»¥æ”¯æŒå¤šæ ‡ç­¾åˆ†ç±»ï¼Œè¯æ˜å°†CNNçš„æœ€åä¸€å±‚æ›¿æ¢ä¸ºeXtreme Gradient Boostingåˆ†ç±»å™¨èƒ½æ˜¾è‘—æå‡å­ç¾¤ä½“çš„å…¬å¹³æ€§ï¼Œä¸”è¯¥æ–¹æ³•åœ¨DenseNet-121å’ŒResNet-50ç­‰ä¸åŒä¸»å¹²ç½‘ç»œ(backbones)ä¸Šå‡è¡¨ç°å‡ºå¼ºå¤§çš„é€šç”¨æ€§å’Œæ¨¡å‹æ— å…³æ€§ã€‚ä¸å¯¹æŠ—è®­ç»ƒ(adversarial training)ã€é‡æ–°åŠ æƒ(reweighting)ç­‰ä¼ ç»Ÿå…¨æ¨¡å‹åè§ç¼“è§£æŠ€æœ¯ç›¸æ¯”ï¼Œè¿™ç§è½»é‡çº§é€‚é…å™¨è®­ç»ƒæ–¹æ³•ä»¥æä½çš„è®¡ç®—æˆæœ¬å®ç°äº†æ›´å…·ç«äº‰åŠ›çš„å‡åæ•ˆæœã€‚å®éªŒç»“æœè¿›ä¸€æ­¥è¡¨æ˜ï¼Œå°†eXtreme Gradient Boostingé‡è®­ç»ƒä¸ä¸»åŠ¨å­¦ä¹ (active learning)ç›¸ç»“åˆï¼Œåœ¨CheXpertå’ŒMIMICæ•°æ®é›†çš„åˆ†å¸ƒå†…åŠåˆ†å¸ƒå¤–æµ‹è¯•ä¸­å‡å®ç°äº†æœ€å¤§çš„åè§ç¼©å‡ã€‚è¯¥ç ”ç©¶ä¸ºä¸´åºŠæ”¾å°„å­¦ä¸­éƒ¨ç½²å…¬å¹³ä¸”é«˜æ•ˆçš„æ·±åº¦å­¦ä¹ æ¨¡å‹æä¾›äº†åˆ‡å®æœ‰æ•ˆçš„è·¯å¾„ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "Preprint of an article published in Pacific Symposium on Biocomputing \\c{opyright} 2026 World Scientific Publishing Co., Singapore, http://psb.stanford.edu/",
      "pdf_url": "https://arxiv.org/pdf/2510.10822v1",
      "published_date": "2025-10-12 22:20:08 UTC",
      "updated_date": "2025-10-12 22:20:08 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T02:49:18.778573+00:00"
    },
    {
      "arxiv_id": "2510.10819v1",
      "title": "Generative AI and the Transformation of Software Development Practices",
      "title_zh": "ç”Ÿæˆå¼äººå·¥æ™ºèƒ½ä¸è½¯ä»¶å¼€å‘å®è·µçš„å˜é©",
      "authors": [
        "Vivek Acharya"
      ],
      "abstract": "Generative AI is reshaping how software is designed, written, and maintained. Advances in large language models (LLMs) are enabling new development styles - from chat-oriented programming and 'vibe coding' to agentic programming - that can accelerate productivity and broaden access. This paper examines how AI-assisted techniques are changing software engineering practice, and the related issues of trust, accountability, and shifting skills. We survey iterative chat-based development, multi-agent systems, dynamic prompt orchestration, and integration via the Model Context Protocol (MCP). Using case studies and industry data, we outline both the opportunities (faster cycles, democratized coding) and the challenges (model reliability and cost) of applying generative AI to coding. We describe new roles, skills, and best practices for using AI in a responsible and effective way.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†ç”Ÿæˆå¼äººå·¥æ™ºèƒ½ (Generative AI) å¦‚ä½•é‡å¡‘è½¯ä»¶çš„è®¾è®¡ã€ç¼–å†™å’Œç»´æŠ¤æ–¹å¼ã€‚éšç€å¤§è¯­è¨€æ¨¡å‹ (LLMs) çš„è¿›æ­¥ï¼Œå‡ºç°äº†åŒ…æ‹¬å¯¹è¯å¯¼å‘ç¼–ç¨‹ (chat-oriented programming)ã€â€œæ°›å›´ç¼–ç â€ (vibe coding) ä»¥åŠæ™ºèƒ½ä½“ç¼–ç¨‹ (agentic programming) åœ¨å†…çš„æ–°å‹å¼€å‘èŒƒå¼ï¼Œæ˜¾è‘—æå‡äº†ç”Ÿäº§åŠ›å¹¶é™ä½äº†ç¼–ç¨‹é—¨æ§›ã€‚è®ºæ–‡è¯¦ç»†è°ƒæŸ¥äº†è¿­ä»£å¼å¯¹è¯å¼€å‘ã€å¤šæ™ºèƒ½ä½“ç³»ç»Ÿ (multi-agent systems)ã€åŠ¨æ€æç¤ºè¯ç¼–æ’ (dynamic prompt orchestration) ä»¥åŠé€šè¿‡æ¨¡å‹ä¸Šä¸‹æ–‡åè®® (Model Context Protocol, MCP) è¿›è¡Œçš„é›†æˆæŠ€æœ¯ã€‚é€šè¿‡æ¡ˆä¾‹ç ”ç©¶å’Œè¡Œä¸šæ•°æ®ï¼Œç ”ç©¶åˆ†æäº†è¿™äº›æŠ€æœ¯åœ¨åŠ é€Ÿå¼€å‘å‘¨æœŸç­‰æ–¹é¢çš„æœºé‡ï¼ŒåŒæ—¶ä¹ŸæŒ‡å‡ºäº†æ¨¡å‹å¯é æ€§ (reliability)ã€æˆæœ¬æ§åˆ¶ä»¥åŠåœ¨ä¿¡ä»»ã€é—®è´£åˆ¶å’ŒæŠ€èƒ½è½¬å˜æ–¹é¢çš„æŒ‘æˆ˜ã€‚æœ€åï¼Œæ–‡ç« ä¸ºåœ¨è½¯ä»¶å·¥ç¨‹ä¸­è´Ÿè´£ä»»ä¸”æœ‰æ•ˆåœ°ä½¿ç”¨ AI æå‡ºäº†æ–°çš„è§’è‰²å®šä¹‰ã€æŠ€èƒ½éœ€æ±‚åŠæœ€ä½³å®è·µå»ºè®®ã€‚",
      "categories": [
        "cs.SE",
        "cs.AI"
      ],
      "primary_category": "cs.SE",
      "comment": "16 pages; 1 figure; preprint; v",
      "pdf_url": "https://arxiv.org/pdf/2510.10819v1",
      "published_date": "2025-10-12 22:02:10 UTC",
      "updated_date": "2025-10-12 22:02:10 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T02:49:19.682384+00:00"
    },
    {
      "arxiv_id": "2510.15973v1",
      "title": "Safeguarding Efficacy in Large Language Models: Evaluating Resistance to Human-Written and Algorithmic Adversarial Prompts",
      "title_zh": "ä¿éšœå¤§è¯­è¨€æ¨¡å‹çš„æœ‰æ•ˆæ€§ï¼šè¯„ä¼°å…¶å¯¹äººå·¥ä¸ç®—æ³•å¯¹æŠ—æ€§æç¤ºçš„é˜²å¾¡èƒ½åŠ›",
      "authors": [
        "Tiarnaigh Downey-Webb",
        "Olamide Jogunola",
        "Oluwaseun Ajao"
      ],
      "abstract": "This paper presents a systematic security assessment of four prominent Large Language Models (LLMs) against diverse adversarial attack vectors. We evaluate Phi-2, Llama-2-7B-Chat, GPT-3.5-Turbo, and GPT-4 across four distinct attack categories: human-written prompts, AutoDAN, Greedy Coordinate Gradient (GCG), and Tree-of-Attacks-with-pruning (TAP). Our comprehensive evaluation employs 1,200 carefully stratified prompts from the SALAD-Bench dataset, spanning six harm categories. Results demonstrate significant variations in model robustness, with Llama-2 achieving the highest overall security (3.4% average attack success rate) while Phi-2 exhibits the greatest vulnerability (7.0% average attack success rate). We identify critical transferability patterns where GCG and TAP attacks, though ineffective against their target model (Llama-2), achieve substantially higher success rates when transferred to other models (up to 17% for GPT-4). Statistical analysis using Friedman tests reveals significant differences in vulnerability across harm categories ($p < 0.001$), with malicious use prompts showing the highest attack success rates (10.71% average). Our findings contribute to understanding cross-model security vulnerabilities and provide actionable insights for developing targeted defense mechanisms",
      "tldr_zh": "è¯¥ç ”ç©¶å¯¹å››ç§ä¸»æµå¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰â€”â€”Phi-2, Llama-2-7B-Chat, GPT-3.5-Turbo å’Œ GPT-4 è¿›è¡Œäº†ç³»ç»Ÿçš„å®‰å…¨æ€§è¯„ä¼°ï¼Œé‡ç‚¹æµ‹è¯•å…¶å¯¹æŠ—å¤šç§å¯¹æŠ—æ€§æ”»å‡»å‘é‡çš„é˜²å¾¡æ•ˆèƒ½ã€‚è¯„ä¼°æ¶µç›–äº†äººå·¥ç¼–å†™çš„æç¤ºï¼ˆhuman-written promptsï¼‰ä»¥åŠ AutoDANã€Greedy Coordinate Gradient (GCG) å’Œ Tree-of-Attacks-with-pruning (TAP) ç­‰ç®—æ³•æ”»å‡»ï¼Œå¹¶é‡‡ç”¨äº†æ¥è‡ª SALAD-Bench æ•°æ®é›†çš„ 1,200 ä¸ªåˆ†å±‚æç¤ºè¿›è¡Œå…¨é¢æµ‹è¯•ã€‚ç ”ç©¶å‘ç°å„æ¨¡å‹çš„é²æ£’æ€§å·®å¼‚æ˜¾è‘—ï¼ŒLlama-2 è¡¨ç°å‡ºæœ€é«˜çš„æ•´ä½“å®‰å…¨æ€§ï¼Œå¹³å‡æ”»å‡»æˆåŠŸç‡ï¼ˆaverage attack success rateï¼‰ä»…ä¸º 3.4%ï¼Œè€Œ Phi-2 æœ€ä¸ºè„†å¼±ã€‚æ­¤å¤–ï¼Œå®éªŒæ­ç¤ºäº†æ˜¾è‘—çš„æ”»å‡»å¯è¿ç§»æ€§ç‰¹å¾ï¼Œç‰¹å®šæ”»å‡»åœ¨è¿ç§»è‡³ GPT-4 æ—¶æˆåŠŸç‡å¯é«˜è¾¾ 17%ï¼Œä¸”ç»Ÿè®¡åˆ†ææ˜¾ç¤ºæ¶æ„ä½¿ç”¨ï¼ˆmalicious useï¼‰ç±»åˆ«çš„æ”»å‡»æˆåŠŸç‡åœ¨æ‰€æœ‰å±å®³ç±»åˆ«ä¸­æœ€é«˜ã€‚è¯¥é¡¹å·¥ä½œä¸ä»…æ·±åŒ–äº†å¯¹è·¨æ¨¡å‹å®‰å…¨æ¼æ´çš„ç†è§£ï¼Œè¿˜ä¸ºæ„å»ºé’ˆå¯¹æ€§é˜²å¾¡æœºåˆ¶æä¾›äº†å…³é”®çš„å®è¯æ”¯æŒã€‚",
      "categories": [
        "cs.CR",
        "cs.AI",
        "cs.CY"
      ],
      "primary_category": "cs.CR",
      "comment": "10 pages, 4 pages manuscript submitted to the Language Resources and Evaluation Conference (LREC 2026)",
      "pdf_url": "https://arxiv.org/pdf/2510.15973v1",
      "published_date": "2025-10-12 21:48:34 UTC",
      "updated_date": "2025-10-12 21:48:34 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T02:49:29.399048+00:00"
    },
    {
      "arxiv_id": "2510.10815v3",
      "title": "DRIFT: Decompose, Retrieve, Illustrate, then Formalize Theorems",
      "title_zh": "DRIFTï¼šåˆ†è§£ã€æ£€ç´¢ã€ç¤ºä¾‹ä¸å®šç†å½¢å¼åŒ–",
      "authors": [
        "Meiru Zhang",
        "Philipp Borchert",
        "Milan Gritta",
        "Gerasimos Lampouras"
      ],
      "abstract": "Automating the formalization of mathematical statements for theorem proving remains a major challenge for Large Language Models (LLMs). LLMs struggle to identify and utilize the prerequisite mathematical knowledge and its corresponding formal representation in languages like Lean. Current retrieval-augmented autoformalization methods query external libraries using the informal statement directly, but overlook a fundamental limitation: informal mathematical statements are often complex and offer limited context on the underlying math concepts. To address this, we introduce DRIFT, a novel framework that enables LLMs to decompose informal mathematical statements into smaller, more tractable ''sub-components''. This facilitates targeted retrieval of premises from mathematical libraries such as Mathlib. Additionally, DRIFT retrieves illustrative theorems to help models use premises more effectively in formalization tasks. We evaluate DRIFT across diverse benchmarks (ProofNet, ConNF, and MiniF2F-test) and find that it consistently improves premise retrieval, nearly doubling the F1 score compared to the DPR baseline on ProofNet. Notably, DRIFT demonstrates strong performance on the out-of-distribution ConNF benchmark, with BEq+@10 improvements of 37.14% and 42.25% using GPT-4.1 and DeepSeek-V3.1, respectively. Our analysis shows that retrieval effectiveness in mathematical autoformalization depends heavily on model-specific knowledge boundaries, highlighting the need for adaptive retrieval strategies aligned with each model's capabilities.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† DRIFT æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³å¤§è¯­è¨€æ¨¡å‹ (LLMs) åœ¨æ•°å­¦è‡ªåŠ¨å½¢å¼åŒ– (Autoformalization) è¿‡ç¨‹ä¸­éš¾ä»¥è¯†åˆ«å…ˆéªŒçŸ¥è¯†åŠå¯¹åº” Lean è¯­è¨€è¡¨è¾¾çš„é—®é¢˜ã€‚è¯¥æ¡†æ¶åˆ›æ–°æ€§åœ°å°†éæ­£å¼æ•°å­¦é™ˆè¿°åˆ†è§£ä¸ºæ›´å°ä¸”æ˜“äºå¤„ç†çš„ sub-componentsï¼Œä»è€Œåœ¨ Mathlib ç­‰æ•°å­¦åº“ä¸­å®ç°æ›´å…·é’ˆå¯¹æ€§çš„å‰ææ£€ç´¢ã€‚æ­¤å¤–ï¼ŒDRIFT è¿˜ä¼šæ£€ç´¢ illustrative theorems ä»¥è¾…åŠ©æ¨¡å‹æ›´æœ‰æ•ˆåœ°åœ¨å½¢å¼åŒ–ä»»åŠ¡ä¸­ä½¿ç”¨æ£€ç´¢åˆ°çš„å‰æã€‚å®éªŒè¡¨æ˜ï¼ŒDRIFT åœ¨ ProofNetã€ConNF å’Œ MiniF2F-test åŸºå‡†æµ‹è¯•ä¸­æ˜¾è‘—æå‡äº†æ€§èƒ½ï¼Œå…¶å‰ææ£€ç´¢çš„ F1 åˆ†æ•°åœ¨ ProofNet ä¸Šè¾ƒ DPR åŸºå‡†å‡ ä¹ç¿»å€ã€‚åœ¨ ConNF åˆ†å¸ƒå¤–æµ‹è¯•ä¸­ï¼Œè¯¥æ¡†æ¶é…åˆ GPT-4.1 å’Œ DeepSeek-V3.1 åˆ†åˆ«å–å¾—äº† 37.14% å’Œ 42.25% çš„å¤§å¹…æå‡ã€‚è¯¥ç ”ç©¶æœ€åæŒ‡å‡ºæ•°å­¦è‡ªåŠ¨å½¢å¼åŒ–çš„æ£€ç´¢æœ‰æ•ˆæ€§é«˜åº¦ä¾èµ–äºæ¨¡å‹çš„çŸ¥è¯†è¾¹ç•Œï¼Œå¹¶å¼ºè°ƒäº†å¼€å‘ä¸æ¨¡å‹èƒ½åŠ›åŒ¹é…çš„è‡ªé€‚åº”æ£€ç´¢ç­–ç•¥çš„å¿…è¦æ€§ã€‚",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.IR",
        "cs.SC"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.10815v3",
      "published_date": "2025-10-12 21:42:04 UTC",
      "updated_date": "2025-10-20 17:46:57 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T02:49:44.479138+00:00"
    },
    {
      "arxiv_id": "2510.10813v1",
      "title": "LLMs as Strategic Agents: Beliefs, Best Response Behavior, and Emergent Heuristics",
      "title_zh": "å¤§è¯­è¨€æ¨¡å‹ä½œä¸ºç­–ç•¥æ€§æ™ºèƒ½ä½“ï¼šä¿¡å¿µã€æœ€ä¼˜ååº”è¡Œä¸ºä¸æ¶Œç°çš„å¯å‘å¼ç­–ç•¥",
      "authors": [
        "Enric Junque de Fortuny",
        "Veronica Roberta Cappelli"
      ],
      "abstract": "Large Language Models (LLMs) are increasingly applied to domains that require reasoning about other agents' behavior, such as negotiation, policy design, and market simulation, yet existing research has mostly evaluated their adherence to equilibrium play or their exhibited depth of reasoning. Whether they display genuine strategic thinking, understood as the coherent formation of beliefs about other agents, evaluation of possible actions, and choice based on those beliefs, remains unexplored. We develop a framework to identify this ability by disentangling beliefs, evaluation, and choice in static, complete-information games, and apply it across a series of non-cooperative environments. By jointly analyzing models' revealed choices and reasoning traces, and introducing a new context-free game to rule out imitation from memorization, we show that current frontier models exhibit belief-coherent best-response behavior at targeted reasoning depths. When unconstrained, they self-limit their depth of reasoning and form differentiated conjectures about human and synthetic opponents, revealing an emergent form of meta-reasoning. Under increasing complexity, explicit recursion gives way to internally generated heuristic rules of choice that are stable, model-specific, and distinct from known human biases. These findings indicate that belief coherence, meta-reasoning, and novel heuristic formation can emerge jointly from language modeling objectives, providing a structured basis for the study of strategic cognition in artificial agents.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ä¸ªæ—¨åœ¨è¯†åˆ«å¤§è¯­è¨€æ¨¡å‹(LLMs)æˆ˜ç•¥æ€ç»´èƒ½åŠ›çš„æ¡†æ¶ï¼Œé€šè¿‡åœ¨éåˆä½œåšå¼ˆç¯å¢ƒä¸­æ‹†è§£ä¿¡å¿µ(Beliefs)ã€è¯„ä¼°ä¸é€‰æ‹©ï¼Œæ¢è®¨å…¶æ˜¯å¦èƒ½åŸºäºå¯¹å¯¹æ‰‹è¡Œä¸ºçš„è¿è´¯ä¿¡å¿µåšå‡ºæœ€ä¼˜å†³ç­–ã€‚ç ”ç©¶å›¢é˜Ÿç»“åˆæ¨¡å‹çš„æ˜¾æ€§é€‰æ‹©ä¸æ¨ç†è·¯å¾„(Reasoning Traces)ï¼Œå¹¶å¼•å…¥æ— èƒŒæ™¯åšå¼ˆä»¥æ’é™¤è®°å¿†æ¨¡ä»¿ï¼Œç³»ç»Ÿè¯„ä¼°äº†æ¨¡å‹åœ¨ä¸åŒæ¨ç†æ·±åº¦ä¸‹çš„åšå¼ˆè¡¨ç°ã€‚ç»“æœæ˜¾ç¤ºï¼Œå½“å‰çš„å°–ç«¯æ¨¡å‹è¡¨ç°å‡ºä¿¡å¿µè¿è´¯çš„æœ€ä½³ååº”(Best-Response)è¡Œä¸ºï¼Œå¹¶åœ¨æ— çº¦æŸæ¡ä»¶ä¸‹å±•ç°å‡ºé’ˆå¯¹äººç±»å’Œåˆæˆå¯¹æ‰‹å½¢æˆå·®å¼‚åŒ–æ¨æµ‹çš„å…ƒæ¨ç†(Meta-Reasoning)èƒ½åŠ›ã€‚é¢å¯¹æ—¥ç›Šå¤æ‚çš„ä»»åŠ¡ï¼Œæ¨¡å‹ä¼šä»æ˜¾å¼é€’å½’è½¬å‘å†…éƒ¨ç”Ÿæˆçš„ã€å…·æœ‰æ¨¡å‹ç‰¹å¼‚æ€§çš„å¯å‘å¼è§„åˆ™(Heuristics)ï¼Œè¿™äº›è§„åˆ™è¡¨ç°ç¨³å®šä¸”ä¸å·²çŸ¥çš„äººç±»åè§ä¸åŒã€‚è¯¥å‘ç°è¯æ˜äº†ä¿¡å¿µè¿è´¯æ€§ã€å…ƒæ¨ç†å’Œæ–°å‹å¯å‘å¼å½¢æˆå¯ä»¥ä»è¯­è¨€å»ºæ¨¡ç›®æ ‡ä¸­å…±åŒæ¶Œç°ï¼Œä¸ºç ”ç©¶äººå·¥æ™ºèƒ½çš„æˆ˜ç•¥è®¤çŸ¥(Strategic Cognition)æä¾›äº†ç»“æ„åŒ–åŸºç¡€ã€‚",
      "categories": [
        "cs.AI",
        "cs.GT"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.10813v1",
      "published_date": "2025-10-12 21:40:29 UTC",
      "updated_date": "2025-10-12 21:40:29 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T02:49:44.677352+00:00"
    },
    {
      "arxiv_id": "2510.11755v1",
      "title": "Artificial Intelligence for Optimal Learning: A Comparative Approach towards AI-Enhanced Learning Environments",
      "title_zh": "äººå·¥æ™ºèƒ½åŠ©åŠ›ä¼˜åŒ–å­¦ä¹ ï¼šäººå·¥æ™ºèƒ½å¢å¼ºå‹å­¦ä¹ ç¯å¢ƒçš„æ¯”è¾ƒç ”ç©¶è§†è§’",
      "authors": [
        "Ananth Hariharan"
      ],
      "abstract": "In the rapidly evolving educational landscape, the integration of technology has shifted from an enhancement to a cornerstone of educational strategy worldwide. This transition is propelled by advancements in digital technology, especially the emergence of artificial intelligence as a crucial tool in learning environments. This research project critically evaluates the impact of three distinct educational settings: traditional educational methods without technological integration, those enhanced by non-AI technology, and those utilising AI-driven technologies. This comparison aims to assess how each environment influences educational outcomes, engagement, pedagogical methods, and equity in access to learning resources, and how each contributes uniquely to the learning experience. The ultimate goal of this research is to synthesise the strengths of each model to create a more holistic educational approach. By integrating the personal interaction and tested pedagogical techniques of traditional classrooms, the enhanced accessibility and collaborative tools offered by non-AI technology, and the personalised, adaptive learning strategies enabled by AI-driven technologies, education systems can develop richer, more effective learning environments. This hybrid approach aims to leverage the best elements of each setting, thereby enhancing educational outcomes, engagement, and inclusiveness, while also addressing the distinct challenges and limitations inherent in each model. The intention is to create an educational framework deeply attentive to the diverse needs of students, ensuring equitable access to high-quality education for all.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†åœ¨å¿«é€Ÿå‘å±•çš„æ•™è‚²æ ¼å±€ä¸­ï¼Œäººå·¥æ™ºèƒ½ä½œä¸ºå­¦ä¹ ç¯å¢ƒå…³é”®å·¥å…·çš„ä½œç”¨ï¼Œå¹¶å¯¹æ¯”åˆ†æäº†ä¼ ç»Ÿæ•™è‚²ã€é AI æŠ€æœ¯å¢å¼ºä»¥åŠåˆ©ç”¨ AI é©±åŠ¨æŠ€æœ¯è¿™ä¸‰ç§ä¸åŒçš„æ•™è‚²è®¾ç½®ã€‚é€šè¿‡è¯„ä¼°å„ç¯å¢ƒå¯¹å­¦ä¹ æˆæœã€å‚ä¸åº¦ã€æ•™å­¦æ³•å’Œæ•™è‚²å…¬å¹³æ€§çš„å½±å“ï¼Œç ”ç©¶æ­ç¤ºäº†ä¸åŒæ¨¡å¼åœ¨å­¦ä¹ ä½“éªŒä¸­çš„ç‹¬ç‰¹è´¡çŒ®ã€‚ä½œè€…æå‡ºäº†ä¸€ç§ç»¼åˆæ€§çš„æ··åˆæ•™å­¦æ¨¡å‹ï¼Œæ—¨åœ¨å°†ä¼ ç»Ÿè¯¾å ‚çš„äººé™…äº’åŠ¨ã€é AI æŠ€æœ¯çš„åä½œå·¥å…·ä¸ AI é©±åŠ¨æŠ€æœ¯çš„ Personalised å’Œ Adaptive Learning ç­–ç•¥ç›¸ç»“åˆã€‚è¿™ç§æ··åˆæ–¹æ³•æ—¨åœ¨å‘æŒ¥å„è®¾ç½®çš„ä¼˜åŠ¿ï¼Œåœ¨æå‡å­¦ä¹ æˆæ•ˆå’ŒåŒ…å®¹æ€§çš„åŒæ—¶ï¼Œæœ‰æ•ˆåº”å¯¹å„æ¨¡å‹å›ºæœ‰çš„å±€é™æ€§ã€‚æœ€ç»ˆï¼Œè¯¥ç ”ç©¶æ„å»ºäº†ä¸€ä¸ªèƒ½å¤Ÿæ»¡è¶³å­¦ç”Ÿå¤šæ ·åŒ–éœ€æ±‚å¹¶ç¡®ä¿é«˜è´¨é‡æ•™è‚²å…¬å¹³è·å–çš„æ•™è‚²æ¡†æ¶ã€‚",
      "categories": [
        "cs.CY",
        "cs.AI"
      ],
      "primary_category": "cs.CY",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.11755v1",
      "published_date": "2025-10-12 21:32:46 UTC",
      "updated_date": "2025-10-12 21:32:46 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T02:49:51.182937+00:00"
    },
    {
      "arxiv_id": "2510.10806v2",
      "title": "Is Implicit Knowledge Enough for LLMs? A RAG Approach for Tree-based Structures",
      "title_zh": "éšå¼çŸ¥è¯†å¯¹ LLMs è€Œè¨€è¶³å¤Ÿå—ï¼Ÿä¸€ç§é’ˆå¯¹æ ‘å½¢ç»“æ„çš„ RAG æ–¹æ³•",
      "authors": [
        "Mihir Gupte",
        "Paolo Giusto",
        "Ramesh S"
      ],
      "abstract": "Large Language Models (LLMs) are adept at generating responses based on information within their context. While this ability is useful for interacting with structured data like code files, another popular method, Retrieval-Augmented Generation (RAG), retrieves relevant documents to augment the model's in-context learning. However, it is not well-explored how to best represent this retrieved knowledge for generating responses on structured data, particularly hierarchical structures like trees. In this work, we propose a novel bottom-up method to linearize knowledge from tree-like structures (like a GitHub repository) by generating implicit, aggregated summaries at each hierarchical level. This approach enables the knowledge to be stored in a knowledge base and used directly with RAG. We then compare our method to using RAG on raw, unstructured code, evaluating the accuracy and quality of the generated responses. Our results show that while response quality is comparable across both methods, our approach generates over 68% fewer documents in the retriever, a significant gain in efficiency. This finding suggests that leveraging implicit, linearized knowledge may be a highly effective and scalable strategy for handling complex, hierarchical data structures.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†å¤§å‹è¯­è¨€æ¨¡å‹(LLMs)åœ¨å¤„ç†ä»£ç ä»“åº“ç­‰æ ‘çŠ¶ç»“æ„(Tree-based Structures)æ—¶ï¼Œå¦‚ä½•æœ€æœ‰æ•ˆåœ°è¡¨ç¤ºæ£€ç´¢åˆ°çš„çŸ¥è¯†ã€‚è®ºæ–‡æå‡ºäº†ä¸€ç§è‡ªä¸‹è€Œä¸Š(Bottom-up)çš„æ–°æ–¹æ³•ï¼Œé€šè¿‡åœ¨æ ‘çŠ¶ç»“æ„çš„æ¯ä¸€å±‚çº§ç”Ÿæˆéšå¼çš„èšåˆæ‘˜è¦ï¼Œå°†åˆ†å±‚ç»“æ„çŸ¥è¯†çº¿æ€§åŒ–(Linearize)ï¼Œä½¿å…¶èƒ½ç›´æ¥åº”ç”¨äºæ£€ç´¢å¢å¼ºç”Ÿæˆ(RAG)ã€‚ç ”ç©¶äººå‘˜å°†è¯¥æ–¹æ³•ä¸ç›´æ¥åœ¨åŸå§‹éç»“æ„åŒ–ä»£ç ä¸Šåº”ç”¨RAGè¿›è¡Œäº†å¯¹æ¯”ï¼Œé‡ç‚¹è¯„ä¼°äº†ç”Ÿæˆå“åº”çš„å‡†ç¡®æ€§å’Œè´¨é‡ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œè™½ç„¶ä¸¤ç§æ–¹æ³•çš„å“åº”è´¨é‡ç›¸å½“ï¼Œä½†æ–°æ–¹æ³•åœ¨æ£€ç´¢å™¨ä¸­ç”Ÿæˆçš„æ–‡æ¡£æ•°é‡å‡å°‘äº†68%ä»¥ä¸Šï¼Œæ˜¾è‘—æå‡äº†æ•ˆç‡ã€‚è¿™è¡¨æ˜åˆ©ç”¨éšå¼çš„çº¿æ€§åŒ–çŸ¥è¯†æ˜¯å¤„ç†å¤æ‚å±‚æ¬¡åŒ–æ•°æ®ç»“æ„çš„ä¸€ç§é«˜æ•ˆä¸”å…·æ‰©å±•æ€§çš„ç­–ç•¥ï¼Œä¸ºLLMå¤„ç†ç»“æ„åŒ–æ•°æ®æä¾›äº†æ–°æ€è·¯ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.IR",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "Waiting for Conference Response",
      "pdf_url": "https://arxiv.org/pdf/2510.10806v2",
      "published_date": "2025-10-12 20:52:43 UTC",
      "updated_date": "2025-10-21 16:10:00 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T02:49:49.671998+00:00"
    },
    {
      "arxiv_id": "2510.10805v1",
      "title": "Therapeutic AI and the Hidden Risks of Over-Disclosure: An Embedded AI-Literacy Framework for Mental Health Privacy",
      "title_zh": "æ²»ç–—æ€§äººå·¥æ™ºèƒ½ä¸è¿‡åº¦æŠ«éœ²çš„æ½œåœ¨é£é™©ï¼šé¢å‘å¿ƒç†å¥åº·éšç§ä¿æŠ¤çš„åµŒå…¥å¼ AI ç´ å…»æ¡†æ¶",
      "authors": [
        "Soraya S. Anvari",
        "Rina R. Wehbe"
      ],
      "abstract": "Large Language Models (LLMs) are increasingly deployed in mental health contexts, from structured therapeutic support tools to informal chat-based well-being assistants. While these systems increase accessibility, scalability, and personalization, their integration into mental health care brings privacy and safety challenges that have not been well-examined. Unlike traditional clinical interactions, LLM-mediated therapy often lacks a clear structure for what information is collected, how it is processed, and how it is stored or reused. Users without clinical guidance may over-disclose personal information, which is sometimes irrelevant to their presenting concern, due to misplaced trust, lack of awareness of data risks, or the conversational design of the system. This overexposure raises privacy concerns and also increases the potential for LLM bias, misinterpretation, and long-term data misuse. We propose a framework embedding Artificial Intelligence (AI) literacy interventions directly into mental health conversational systems, and outline a study plan to evaluate their impact on disclosure safety, trust, and user experience.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨å¿ƒç†å¥åº·é¢†åŸŸåº”ç”¨ä¸­æ‰€å¸¦æ¥çš„éšç§ä¸å®‰å…¨æŒ‘æˆ˜ï¼Œç‰¹åˆ«å…³æ³¨äº†ç”¨æˆ·è¿‡åº¦æŠ«éœ²ï¼ˆOver-Disclosureï¼‰ä¸ªäººä¿¡æ¯çš„éšå½¢é£é™©ã€‚æ–‡ç« æŒ‡å‡ºï¼Œç”±äºç¼ºä¹æ˜ç¡®çš„ä¸´åºŠæŒ‡å¯¼å’Œå¯¹æ•°æ®é£é™©çš„è®¤çŸ¥ï¼Œç”¨æˆ·åœ¨ä¸æ²»ç–—æ€§AIäº¤äº’æ—¶ï¼Œå¾€å¾€å› è¿‡åº¦ä¿¡ä»»æˆ–ç³»ç»Ÿå¯¹è¯è®¾è®¡è€Œæ³„éœ²æ— å…³çš„ä¸ªäººæ•æ„Ÿä¿¡æ¯ã€‚è¿™ç§è¿‡åº¦æš´éœ²ä¸ä»…å¼•å‘äº†éšç§æ‹…å¿§ï¼Œè¿˜å¢åŠ äº†LLMäº§ç”Ÿåè§ï¼ˆBiasï¼‰ã€è¯¯è¯»ä»¥åŠé•¿æœŸæ•°æ®è¯¯ç”¨çš„æ½œåœ¨é£é™©ã€‚ä¸ºæ­¤ï¼Œç ”ç©¶å›¢é˜Ÿæå‡ºäº†ä¸€ä¸ªå°†äººå·¥æ™ºèƒ½ç´ å…»ï¼ˆAI literacyï¼‰å¹²é¢„æªæ–½ç›´æ¥åµŒå…¥å¿ƒç†å¥åº·å¯¹è¯ç³»ç»Ÿçš„æ¡†æ¶ã€‚è¯¥æ¡†æ¶æ—¨åœ¨é€šè¿‡å³æ—¶å¹²é¢„æ¥å¼•å¯¼ç”¨æˆ·è¿›è¡Œå®‰å…¨çš„è‡ªæˆ‘æŠ«éœ²ï¼Œå¹¶è¯¦ç»†åˆ¶å®šäº†è¯„ä¼°å…¶å¯¹å®‰å…¨æ€§ã€ä¿¡ä»»åº¦åŠç”¨æˆ·ä½“éªŒå½±å“çš„ç ”ç©¶è®¡åˆ’ã€‚é€šè¿‡è¿™ç§åµŒå…¥å¼è®¾è®¡ï¼Œè¯¥ç ”ç©¶ä¸ºç¼“è§£æ²»ç–—æ€§AIä¸­çš„éšç§æ³„éœ²é—®é¢˜å¹¶æå‡ç³»ç»Ÿé€æ˜åº¦æä¾›äº†æ–°çš„è§†è§’ä¸è§£å†³æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.HC",
        "cs.AI"
      ],
      "primary_category": "cs.HC",
      "comment": "Accepted to SMASH 2025",
      "pdf_url": "https://arxiv.org/pdf/2510.10805v1",
      "published_date": "2025-10-12 20:50:06 UTC",
      "updated_date": "2025-10-12 20:50:06 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T02:49:52.880054+00:00"
    },
    {
      "arxiv_id": "2510.10803v1",
      "title": "PruneGCRN: Minimizing and explaining spatio-temporal problems through node pruning",
      "title_zh": "PruneGCRNï¼šé€šè¿‡èŠ‚ç‚¹å‰ªæç®€åŒ–ä¸è§£é‡Šæ—¶ç©ºé—®é¢˜",
      "authors": [
        "Javier GarcÃ­a-SigÃ¼enza",
        "Mirco Nanni",
        "FaraÃ³n Llorens-Largo",
        "JosÃ© F. Vicent"
      ],
      "abstract": "This work addresses the challenge of using a deep learning model to prune graphs and the ability of this method to integrate explainability into spatio-temporal problems through a new approach. Instead of applying explainability to the model's behavior, we seek to gain a better understanding of the problem itself. To this end, we propose a novel model that integrates an optimized pruning mechanism capable of removing nodes from the graph during the training process, rather than doing so as a separate procedure. This integration allows the architecture to learn how to minimize prediction error while selecting the most relevant nodes. Thus, during training, the model searches for the most relevant subset of nodes, obtaining the most important elements of the problem, facilitating its analysis. To evaluate the proposed approach, we used several widely used traffic datasets, comparing the accuracy obtained by pruning with the model and with other methods. The experiments demonstrate that our method is capable of retaining a greater amount of information as the graph reduces in size compared to the other methods used. These results highlight the potential of pruning as a tool for developing models capable of simplifying spatio-temporal problems, thereby obtaining their most important elements.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† PruneGCRNï¼Œè¿™æ˜¯ä¸€ç§é€šè¿‡èŠ‚ç‚¹å‰ªæ (node pruning) æ¥ç®€åŒ–å¹¶è§£é‡Šæ—¶ç©ºé—®é¢˜ (spatio-temporal problems) çš„æ·±åº¦å­¦ä¹ æ¨¡å‹ã€‚ä¸ä¼ ç»Ÿçš„é’ˆå¯¹æ¨¡å‹è¡Œä¸ºçš„è§£é‡Šæ€§æ–¹æ³•ä¸åŒï¼Œè¯¥æ¨¡å‹å°†ä¼˜åŒ–çš„å‰ªææœºåˆ¶ç›´æ¥é›†æˆåˆ°è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œä½¿å…¶èƒ½å¤Ÿåœ¨æœ€å°åŒ–é¢„æµ‹è¯¯å·® (prediction error) çš„åŒæ—¶è‡ªåŠ¨å­¦ä¹ å¹¶ç­›é€‰å‡ºæœ€ç›¸å…³çš„èŠ‚ç‚¹å­é›†ã€‚è¿™ç§é›†æˆæ¶æ„å…è®¸æ¨¡å‹åœ¨è®­ç»ƒæœŸé—´è¯†åˆ«é—®é¢˜çš„æ ¸å¿ƒè¦ç´ ï¼Œä»è€Œå¸®åŠ©ç ”ç©¶äººå‘˜æ›´ç›´è§‚åœ°ç†è§£æ—¶ç©ºé—®é¢˜çš„æœ¬è´¨ã€‚åœ¨å¤šä¸ªäº¤é€šæ•°æ®é›† (traffic datasets) ä¸Šè¿›è¡Œçš„å®éªŒç»“æœè¡¨æ˜ï¼ŒPruneGCRN åœ¨ç¼©å‡å›¾è§„æ¨¡çš„æƒ…å†µä¸‹ä¿ç•™ä¿¡æ¯çš„èƒ½åŠ›æ˜¾è‘—ä¼˜äºç°æœ‰çš„å‰ªææ–¹æ³•ã€‚è¯¥ç ”ç©¶ç»“æœè¯æ˜äº†å‰ªææŠ€æœ¯åœ¨ç®€åŒ–å¤æ‚æ—¶ç©ºç³»ç»ŸåŠè·å–å…³é”®ç‰¹å¾æ–¹é¢çš„æ½œåŠ›ï¼Œä¸ºæ„å»ºå…¼å…·è§£é‡Šæ€§ä¸é«˜æ•ˆæ€§çš„æ—¶ç©ºé¢„æµ‹æ¨¡å‹æä¾›äº†æœ‰æ•ˆè·¯å¾„ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.10803v1",
      "published_date": "2025-10-12 20:40:22 UTC",
      "updated_date": "2025-10-12 20:40:22 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T02:50:06.186484+00:00"
    },
    {
      "arxiv_id": "2510.10802v3",
      "title": "MSCloudCAM: Multi-Scale Context Adaptation with Convolutional Cross-Attention for Multispectral Cloud Segmentation",
      "title_zh": "MSCloudCAMï¼šåŸºäºå·ç§¯äº¤å‰æ³¨æ„åŠ›çš„å¤šå°ºåº¦ä¸Šä¸‹æ–‡è‡ªé€‚åº”å¤šå…‰è°±äº‘åˆ†å‰²",
      "authors": [
        "Md Abdullah Al Mazid",
        "Liangdong Deng",
        "Naphtali Rishe"
      ],
      "abstract": "Clouds remain a major obstacle in optical satellite imaging, limiting accurate environmental and climate analysis. To address the strong spectral variability and the large scale differences among cloud types, we propose MSCloudCAM, a novel multi-scale context adapter network with convolution based cross-attention tailored for multispectral and multi-sensor cloud segmentation. A key contribution of MSCloudCAM is the explicit modeling of multiple complementary multi-scale context extractors. And also, rather than simply stacking or concatenating their outputs, our formulation uses one extractor's fine-resolution features and the other extractor's global contextual representations enabling dynamic, scale-aware feature selection. Building on this idea, we design a new convolution-based cross attention adapter that effectively fuses localized, detailed information with broader multi-scale context. Integrated with a hierarchical vision backbone and refined through channel and spatial attention mechanisms, MSCloudCAM achieves strong spectral-spatial discrimination. Experiments on various multisensor datatsets e.g. CloudSEN12 (Sentinel-2) and L8Biome (Landsat-8) show that MSCloudCAM outperforms recent state-of-the-art models while maintaining competitive model complexity, highlighting the novelty and effectiveness of the proposed design for large-scale Earth observation.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† MSCloudCAMï¼Œä¸€ç§ä¸“ä¸ºå¤šå…‰è°±å’Œå¤šä¼ æ„Ÿå™¨äº‘åˆ†å‰²è®¾è®¡çš„åˆ›æ–°å¤šå°ºåº¦ä¸Šä¸‹æ–‡é€‚é…å™¨ç½‘ç»œã€‚é’ˆå¯¹å«æ˜Ÿå›¾åƒä¸­äº‘å±‚çš„å…‰è°±å˜å¼‚æ€§å’Œæ˜¾è‘—å°ºåº¦å·®å¼‚ï¼ŒMSCloudCAM å¼•å…¥äº†å¤šä¸ªäº’è¡¥çš„ Multi-scale context extractorsï¼Œå¹¶åˆ©ç”¨åŸºäºå·ç§¯çš„ Cross-attention æœºåˆ¶å°†ç»†ç²’åº¦ç‰¹å¾ä¸å…¨å±€ä¸Šä¸‹æ–‡è¡¨ç¤ºè¿›è¡Œæœ‰æ•ˆèåˆã€‚è¯¥æ–¹æ³•å®ç°äº†åŠ¨æ€çš„ Scale-aware ç‰¹å¾é€‰æ‹©ï¼Œç»“åˆ Hierarchical vision backbone ä»¥åŠ Channel å’Œ Spatial attention å¢å¼ºäº†å…‰è°±-ç©ºé—´åˆ¤åˆ«èƒ½åŠ›ã€‚åœ¨ CloudSEN12 å’Œ L8Biome ç­‰å¤šæºæ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼ŒMSCloudCAM åœ¨ä¿æŒç«äº‰åŠ›æ¨¡å‹å¤æ‚åº¦çš„åŒæ—¶ï¼Œæ€§èƒ½æ˜¾è‘—ä¼˜äºç°æœ‰çš„ SOTA æ¨¡å‹ã€‚è¯¥æˆæœè¯æ˜äº†æ‰€æè®¾è®¡åœ¨å¤„ç†å¤§è§„æ¨¡åœ°çƒè§‚æµ‹ä»»åŠ¡ä¸­çš„ä¼˜è¶Šæ€§ä¸å®ç”¨ä»·å€¼ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "6 pages, 3 Figures",
      "pdf_url": "https://arxiv.org/pdf/2510.10802v3",
      "published_date": "2025-10-12 20:40:22 UTC",
      "updated_date": "2025-11-22 09:01:44 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T02:49:58.086235+00:00"
    },
    {
      "arxiv_id": "2510.10801v1",
      "title": "Toward Human-Centered Readability Evaluation",
      "title_zh": "è¿ˆå‘ä»¥äººä¸ºä¸­å¿ƒçš„å¯è¯»æ€§è¯„ä¼°",
      "authors": [
        "Bahar Ä°lgen",
        "Georges Hattab"
      ],
      "abstract": "Text simplification is essential for making public health information accessible to diverse populations, including those with limited health literacy. However, commonly used evaluation metrics in Natural Language Processing (NLP), such as BLEU, FKGL, and SARI, mainly capture surface-level features and fail to account for human-centered qualities like clarity, trustworthiness, tone, cultural relevance, and actionability. This limitation is particularly critical in high-stakes health contexts, where communication must be not only simple but also usable, respectful, and trustworthy. To address this gap, we propose the Human-Centered Readability Score (HCRS), a five-dimensional evaluation framework grounded in Human-Computer Interaction (HCI) and health communication research. HCRS integrates automatic measures with structured human feedback to capture the relational and contextual aspects of readability. We outline the framework, discuss its integration into participatory evaluation workflows, and present a protocol for empirical validation. This work aims to advance the evaluation of health text simplification beyond surface metrics, enabling NLP systems that align more closely with diverse users' needs, expectations, and lived experiences.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†æ–‡æœ¬ç®€åŒ–åœ¨å…¬å…±å«ç”Ÿä¿¡æ¯ä¼ æ’­ä¸­çš„é‡è¦æ€§ï¼ŒæŒ‡å‡º BLEUã€FKGL å’Œ SARI ç­‰ä¼ ç»Ÿ NLP è¯„ä¼°æŒ‡æ ‡å› ä»…å…³æ³¨è¡¨å±‚ç‰¹å¾è€Œæ— æ³•è¡¡é‡ Clarityã€Trustworthiness å’Œ Actionability ç­‰ä»¥äººä¸ºä¸­å¿ƒçš„ç»´åº¦ã€‚ä¸ºæ­¤ï¼Œç ”ç©¶è€…æå‡ºäº† Human-Centered Readability Score (HCRS)ï¼Œè¿™æ˜¯ä¸€ä¸ªç»“åˆäº†äººæœºäº¤äº’ (HCI) ä¸å¥åº·ä¼ æ’­ç†è®ºçš„äº”ç»´è¯„ä¼°æ¡†æ¶ã€‚HCRS é€šè¿‡æ•´åˆè‡ªåŠ¨åº¦é‡ä¸ç»“æ„åŒ–çš„äººç±»åé¦ˆï¼Œæ•æ‰æ–‡æœ¬åœ¨ç‰¹å®šè¯­å¢ƒä¸‹çš„å¯è¯»æ€§ã€è¯­æ°”åŠ Cultural Relevanceã€‚è®ºæ–‡è¿›ä¸€æ­¥å±•ç¤ºäº†è¯¥æ¡†æ¶å¦‚ä½•é›†æˆåˆ°å‚ä¸å¼è¯„ä¼°å·¥ä½œæµä¸­ï¼Œå¹¶ç»™å‡ºäº†å®è¯éªŒè¯çš„è¯¦ç»†åè®®ã€‚è¯¥ç ”ç©¶æ—¨åœ¨æ¨åŠ¨å¥åº·é¢†åŸŸæ–‡æœ¬ç®€åŒ–çš„è¯„ä¼°è¶…è¶Šè¡¨å±‚æŒ‡æ ‡ï¼Œä½¿ NLP ç³»ç»Ÿèƒ½å¤Ÿæ›´ç´§å¯†åœ°å¯¹é½å¤šå…ƒåŒ–ç”¨æˆ·çš„å®é™…éœ€æ±‚ã€æœŸæœ›ä¸ç”Ÿæ´»ç»éªŒã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "Accepted to the 4th Workshop on Bridging Human-Computer Interaction and NLP (HCI+NLP) at EMNLP 2025, Suzhou, China",
      "pdf_url": "https://arxiv.org/pdf/2510.10801v1",
      "published_date": "2025-10-12 20:38:32 UTC",
      "updated_date": "2025-10-12 20:38:32 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T02:50:03.210612+00:00"
    },
    {
      "arxiv_id": "2510.13858v1",
      "title": "Decision Oriented Technique (DOTechnique): Finding Model Validity Through Decision-Maker Context",
      "title_zh": "å†³ç­–å¯¼å‘æŠ€æœ¯ï¼ˆDOTechniqueï¼‰ï¼šåŸºäºå†³ç­–è€…è¯­å¢ƒæ¢å¯»æ¨¡å‹æœ‰æ•ˆæ€§",
      "authors": [
        "Raheleh Biglari",
        "Joachim Denil"
      ],
      "abstract": "Model validity is as critical as the model itself, especially when guiding decision-making processes. Traditional approaches often rely on predefined validity frames, which may not always be available or sufficient. This paper introduces the Decision Oriented Technique (DOTechnique), a novel method for determining model validity based on decision consistency rather than output similarity. By evaluating whether surrogate models lead to equivalent decisions compared to high-fidelity models, DOTechnique enables efficient identification of validity regions, even in the absence of explicit validity boundaries. The approach integrates domain constraints and symbolic reasoning to narrow the search space, enhancing computational efficiency. A highway lane change system serves as a motivating example, demonstrating how DOTechnique can uncover the validity region of a simulation model. The results highlight the potential of the technique to support finding model validity through decision-maker context.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†å†³ç­–å¯¼å‘æŠ€æœ¯ (Decision Oriented Technique, DOTechnique)ï¼Œè¿™æ˜¯ä¸€ç§é€šè¿‡å†³ç­–è€…ä¸Šä¸‹æ–‡ (Decision-Maker Context) æ¥ç¡®å®šæ¨¡å‹æœ‰æ•ˆæ€§ (Model Validity) çš„æ–°æ–¹æ³•ã€‚ä¼ ç»Ÿæ–¹æ³•é€šå¸¸ä¾èµ–é¢„å®šä¹‰çš„æœ‰æ•ˆæ€§æ¡†æ¶ï¼Œä½†åœ¨æŒ‡å¯¼å†³ç­–è¿‡ç¨‹ä¸­ï¼Œè¿™äº›æ¡†æ¶å¾€å¾€éš¾ä»¥è·å–æˆ–ä¸å¤Ÿå……åˆ†ã€‚DOTechnique çš„æ ¸å¿ƒæ€æƒ³æ˜¯åŸºäºå†³ç­–ä¸€è‡´æ€§ (Decision Consistency) è€Œéè¾“å‡ºç›¸ä¼¼æ€§ (Output Similarity) æ¥è¯„ä¼°æ¨¡å‹ï¼Œé€šè¿‡éªŒè¯ä»£ç†æ¨¡å‹ (Surrogate Models) ä¸é«˜ä¿çœŸæ¨¡å‹ (High-Fidelity Models) æ˜¯å¦äº§ç”Ÿç­‰æ•ˆå†³ç­–æ¥è¯†åˆ«æœ‰æ•ˆåŒºåŸŸã€‚è¯¥æ–¹æ³•é›†æˆäº†é¢†åŸŸçº¦æŸ (Domain Constraints) å’Œç¬¦å·æ¨ç† (Symbolic Reasoning) ä»¥ç¼©å°æœç´¢ç©ºé—´ï¼Œä»è€Œæ˜¾è‘—æå‡è®¡ç®—æ•ˆç‡ã€‚ç ”ç©¶ä»¥é«˜é€Ÿå…¬è·¯æ¢é“ç³»ç»Ÿ (Highway Lane Change System) ä¸ºä¾‹ï¼Œå±•ç¤ºäº†è¯¥æŠ€æœ¯å¦‚ä½•åœ¨ç¼ºä¹æ˜ç¡®æœ‰æ•ˆæ€§è¾¹ç•Œçš„æƒ…å†µä¸‹æ­ç¤ºæ¨¡æ‹Ÿæ¨¡å‹çš„æœ‰æ•ˆåŒºåŸŸã€‚å®éªŒç»“æœè¯æ˜ï¼ŒDOTechnique èƒ½å¤Ÿæœ‰æ•ˆæ”¯æŒåœ¨å†³ç­–è€…èƒŒæ™¯ä¸‹å¯»æ‰¾æ¨¡å‹æœ‰æ•ˆæ€§ï¼Œä¸ºå¤æ‚å†³ç­–è¿‡ç¨‹ä¸­çš„æ¨¡å‹éªŒè¯æä¾›äº†æ–°æ€è·¯ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "10 pages",
      "pdf_url": "https://arxiv.org/pdf/2510.13858v1",
      "published_date": "2025-10-12 20:28:14 UTC",
      "updated_date": "2025-10-12 20:28:14 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T02:50:05.376100+00:00"
    },
    {
      "arxiv_id": "2510.10790v1",
      "title": "BioOSS: A Bio-Inspired Oscillatory State System with Spatio-Temporal Dynamics",
      "title_zh": "BioOSSï¼šå…·æœ‰æ—¶ç©ºåŠ¨åŠ›å­¦ç‰¹æ€§çš„ä»¿ç”ŸæŒ¯è¡çŠ¶æ€ç³»ç»Ÿ",
      "authors": [
        "Zhongju Yuan",
        "Geraint Wiggins",
        "Dick Botteldooren"
      ],
      "abstract": "Today's deep learning architectures are primarily based on perceptron models, which do not capture the oscillatory dynamics characteristic of biological neurons. Although oscillatory systems have recently gained attention for their closer resemblance to neural behavior, they still fall short of modeling the intricate spatio-temporal interactions observed in natural neural circuits. In this paper, we propose a bio-inspired oscillatory state system (BioOSS) designed to emulate the wave-like propagation dynamics critical to neural processing, particularly in the prefrontal cortex (PFC), where complex activity patterns emerge. BioOSS comprises two interacting populations of neurons: p neurons, which represent simplified membrane-potential-like units inspired by pyramidal cells in cortical columns, and o neurons, which govern propagation velocities and modulate the lateral spread of activity. Through local interactions, these neurons produce wave-like propagation patterns. The model incorporates trainable parameters for damping and propagation speed, enabling flexible adaptation to task-specific spatio-temporal structures. We evaluate BioOSS on both synthetic and real-world tasks, demonstrating superior performance and enhanced interpretability compared to alternative architectures.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†BioOSSï¼Œä¸€ç§å—ç”Ÿç‰©å¯å‘ä¸”å…·å¤‡æ—¶ç©ºåŠ¨åŠ›å­¦(Spatio-Temporal Dynamics)çš„æŒ¯è¡çŠ¶æ€ç³»ç»Ÿï¼Œæ—¨åœ¨è§£å†³å½“å‰æ·±åº¦å­¦ä¹ æ¶æ„æ— æ³•æ•æ‰ç”Ÿç‰©ç¥ç»å…ƒæŒ¯è¡ç‰¹æ€§åŠå¤æ‚æ—¶ç©ºäº¤äº’çš„é—®é¢˜ã€‚BioOSSé€šè¿‡æ¨¡æ‹Ÿå‰é¢å¶çš®å±‚(PFC)ä¸­å…³é”®çš„æ³¢çŠ¶ä¼ æ’­åŠ¨åŠ›å­¦ï¼Œæ„å»ºäº†ç”±æ¨¡ä»¿çš®å±‚é”¥ä½“ç»†èƒ(Pyramidal cells)çš„pç¥ç»å…ƒå’Œè´Ÿè´£è°ƒèŠ‚ä¼ æ’­é€Ÿåº¦çš„oç¥ç»å…ƒç»„æˆçš„åŒé‡äº¤äº’ç³»ç»Ÿã€‚è¯¥æ¨¡å‹åˆ©ç”¨å±€éƒ¨äº¤äº’äº§ç”Ÿæ³¢çŠ¶ä¼ æ’­æ¨¡å¼ï¼Œå¹¶å¼•å…¥å¯è®­ç»ƒçš„é˜»å°¼(Damping)ä¸é€Ÿåº¦å‚æ•°ï¼Œä½¿å…¶èƒ½å¤Ÿçµæ´»é€‚é…ç‰¹å®šä»»åŠ¡çš„æ—¶ç©ºç»“æ„ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒBioOSSåœ¨åˆæˆä»»åŠ¡å’ŒçœŸå®ä¸–ç•Œä»»åŠ¡ä¸­å‡è¡¨ç°å‡ºä¼˜äºä¼ ç»Ÿæ¶æ„çš„æ€§èƒ½ï¼Œå¹¶æ˜¾è‘—å¢å¼ºäº†ç³»ç»Ÿçš„å¯è§£é‡Šæ€§(Interpretability)ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.10790v1",
      "published_date": "2025-10-12 20:12:33 UTC",
      "updated_date": "2025-10-12 20:12:33 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T02:50:07.571062+00:00"
    },
    {
      "arxiv_id": "2510.10782v1",
      "title": "DISC-GAN: Disentangling Style and Content for Cluster-Specific Synthetic Underwater Image Generation",
      "title_zh": "DISC-GANï¼šé¢å‘ç‰¹å®šèšç±»åˆæˆæ°´ä¸‹å›¾åƒç”Ÿæˆçš„é£æ ¼ä¸å†…å®¹è§£è€¦",
      "authors": [
        "Sneha Varur",
        "Anirudh R Hanchinamani",
        "Tarun S Bagewadi",
        "Uma Mudenagudi",
        "Chaitra D Desai",
        "Sujata C",
        "Padmashree Desai",
        "Sumit Meharwade"
      ],
      "abstract": "In this paper, we propose a novel framework, Disentangled Style-Content GAN (DISC-GAN), which integrates style-content disentanglement with a cluster-specific training strategy towards photorealistic underwater image synthesis. The quality of synthetic underwater images is challenged by optical due to phenomena such as color attenuation and turbidity. These phenomena are represented by distinct stylistic variations across different waterbodies, such as changes in tint and haze. While generative models are well-suited to capture complex patterns, they often lack the ability to model the non-uniform conditions of diverse underwater environments. To address these challenges, we employ K-means clustering to partition a dataset into style-specific domains. We use separate encoders to get latent spaces for style and content; we further integrate these latent representations via Adaptive Instance Normalization (AdaIN) and decode the result to produce the final synthetic image. The model is trained independently on each style cluster to preserve domain-specific characteristics. Our framework demonstrates state-of-the-art performance, obtaining a Structural Similarity Index (SSIM) of 0.9012, an average Peak Signal-to-Noise Ratio (PSNR) of 32.5118 dB, and a Frechet Inception Distance (FID) of 13.3728.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†DISC-GANï¼Œè¿™æ˜¯ä¸€ç§å°†é£æ ¼-å†…å®¹è§£è€¦(Style-Content Disentanglement)ä¸ç‰¹å®šèšç±»è®­ç»ƒç­–ç•¥ç›¸ç»“åˆçš„æ–°å‹æ¡†æ¶ï¼Œæ—¨åœ¨å®ç°é«˜ä¿çœŸåº¦çš„æ°´ä¸‹å›¾åƒåˆæˆã€‚ä¸ºäº†è§£å†³æ°´ä¸‹ç¯å¢ƒä¸­å› é¢œè‰²è¡°å‡å’Œæµ‘æµŠå¯¼è‡´çš„å¤æ‚å…‰å­¦é€€åŒ–é—®é¢˜ï¼Œè¯¥æ–¹æ³•é¦–å…ˆåˆ©ç”¨K-meansèšç±»ç®—æ³•å°†æ•°æ®é›†åˆ’åˆ†ä¸ºä¸åŒçš„é£æ ¼åŸŸã€‚æ¨¡å‹é€šè¿‡ç‹¬ç«‹çš„ç¼–ç å™¨åˆ†åˆ«æå–é£æ ¼ä¸å†…å®¹çš„æ½œç©ºé—´è¡¨ç¤ºï¼Œå¹¶åˆ©ç”¨è‡ªé€‚åº”å®ä¾‹å½’ä¸€åŒ–(AdaIN)æŠ€æœ¯è¿›è¡Œèåˆï¼Œä»è€Œåœ¨æ¯ä¸ªé£æ ¼ç°‡ä¸Šç‹¬ç«‹è®­ç»ƒä»¥ä¿ç•™ç‰¹å®šåŸŸçš„ç‰¹å¾ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯¥æ¡†æ¶åœ¨ç»“æ„ç›¸ä¼¼æ€§æŒ‡æ•°(SSIM)ä¸Šè¾¾åˆ°0.9012ï¼Œå³°å€¼ä¿¡å™ªæ¯”(PSNR)ä¸º32.5118 dBï¼Œå¼—é›·åˆ‡ç‰¹èµ·å§‹è·ç¦»(FID)ä¸º13.3728ï¼Œåœ¨åˆæˆæ°´ä¸‹å›¾åƒè´¨é‡æ–¹é¢è¾¾åˆ°äº†state-of-the-artæ°´å¹³ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.10782v1",
      "published_date": "2025-10-12 19:56:20 UTC",
      "updated_date": "2025-10-12 19:56:20 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T02:50:21.880188+00:00"
    },
    {
      "arxiv_id": "2510.10774v2",
      "title": "ParsVoice: A Large-Scale Multi-Speaker Persian Speech Corpus for Text-to-Speech Synthesis",
      "title_zh": "ParsVoiceï¼šç”¨äºæ–‡æœ¬è½¬è¯­éŸ³åˆæˆçš„å¤§è§„æ¨¡å¤šè¯´è¯äººæ³¢æ–¯è¯­è¯­éŸ³è¯­æ–™åº“",
      "authors": [
        "Mohammad Javad Ranjbar Kalahroodi",
        "Heshaam Faili",
        "Azadeh Shakery"
      ],
      "abstract": "Existing Persian speech datasets are typically smaller than their English counterparts, which creates a key limitation for developing Persian speech technologies. We address this gap by introducing ParsVoice, the largest Persian speech corpus designed specifically for text-to-speech(TTS) applications. We created an automated pipeline that transforms raw audiobook content into TTS-ready data, incorporating components such as a BERT-based sentence completion detector, a binary search boundary optimization method for precise audio-text alignment, and audio-text quality assessment frameworks tailored to Persian. The pipeline processes 2,000 audiobooks, yielding 3,526 hours of clean speech, which was further filtered into a 1,804-hour high-quality subset suitable for TTS, featuring more than 470 speakers. To validate the dataset, we fine-tuned XTTS for Persian, achieving a naturalness Mean Opinion Score (MOS) of 3.6/5 and a Speaker Similarity Mean Opinion Score (SMOS) of 4.0/5 demonstrating ParsVoice's effectiveness for training multi-speaker TTS systems. ParsVoice is the largest high-quality Persian speech dataset, offering speaker diversity and audio quality comparable to major English corpora. The complete dataset has been made publicly available to accelerate the development of Persian speech technologies. The ParsVoice dataset is publicly available at: https://huggingface.co/datasets/MohammadJRanjbar/ParsVoice.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¨å‡ºäº† ParsVoiceï¼Œè¿™æ˜¯ç›®å‰ä¸“é—¨ä¸ºæ–‡æœ¬è½¬è¯­éŸ³(Text-to-Speech, TTS)åº”ç”¨è®¾è®¡çš„æœ€å¤§è§„æ¨¡æ³¢æ–¯è¯­è¯­éŸ³è¯­æ–™åº“ï¼Œæ—¨åœ¨è§£å†³æ³¢æ–¯è¯­æ•°æ®é›†è§„æ¨¡è¾ƒå°å¯¹è¯­éŸ³æŠ€æœ¯å‘å±•çš„é™åˆ¶ã€‚ç ”ç©¶å›¢é˜Ÿå¼€å‘äº†ä¸€å¥—è‡ªåŠ¨åŒ–æµç¨‹ï¼Œé›†æˆäº†åŸºäº BERT çš„å¥å­å®Œæˆæ£€æµ‹å™¨ã€ç”¨äºç²¾ç¡®éŸ³æ–‡å¯¹é½çš„äºŒåˆ†æœç´¢è¾¹ç•Œä¼˜åŒ–æ–¹æ³•ä»¥åŠæ³¢æ–¯è¯­å®šåˆ¶çš„è´¨é‡è¯„ä¼°æ¡†æ¶ï¼Œå°†åŸå§‹æœ‰å£°è¯»ç‰©è½¬åŒ–ä¸ºé«˜è´¨é‡æ•°æ®ã€‚è¯¥è¯­æ–™åº“æœ€ç»ˆåŒ…å« 1,804 å°æ—¶çš„é«˜è´¨é‡è¯­éŸ³å­é›†ï¼Œæ¶µç›– 470 å¤šä½è¯´è¯äººï¼Œå…¶å¤šæ ·æ€§å’ŒéŸ³é¢‘è´¨é‡å¯ä¸ä¸»æµè‹±è¯­è¯­æ–™åº“ç›¸åª²ç¾ã€‚å®éªŒé€šè¿‡å¾®è°ƒ XTTS æ¨¡å‹éªŒè¯äº†æ•°æ®é›†çš„æœ‰æ•ˆæ€§ï¼Œè·å¾—äº† 3.6 çš„è‡ªç„¶åº¦ Mean Opinion Score (MOS) å’Œ 4.0 çš„è¯´è¯äººç›¸ä¼¼åº¦ SMOSã€‚ParsVoice ç›®å‰å·²å®Œå…¨å¼€æºï¼Œä¸ºå¼€å‘å¤šè¯´è¯äºº TTS ç³»ç»Ÿå’ŒåŠ é€Ÿæ³¢æ–¯è¯­è¯­éŸ³æŠ€æœ¯ç ”ç©¶æä¾›äº†é‡è¦èµ„æºã€‚",
      "categories": [
        "cs.SD",
        "cs.AI",
        "cs.HC",
        "cs.LG"
      ],
      "primary_category": "cs.SD",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.10774v2",
      "published_date": "2025-10-12 19:33:11 UTC",
      "updated_date": "2025-10-14 05:09:59 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T02:50:24.368265+00:00"
    },
    {
      "arxiv_id": "2510.11754v1",
      "title": "Zero-Shot Large Language Model Agents for Fully Automated Radiotherapy Treatment Planning",
      "title_zh": "ç”¨äºå…¨è‡ªåŠ¨æ”¾å°„æ²»ç–—è®¡åˆ’çš„é›¶æ ·æœ¬å¤§è¯­è¨€æ¨¡å‹æ™ºèƒ½ä½“",
      "authors": [
        "Dongrong Yang",
        "Xin Wu",
        "Yibo Xie",
        "Xinyi Li",
        "Qiuwen Wu",
        "Jackie Wu",
        "Yang Sheng"
      ],
      "abstract": "Radiation therapy treatment planning is an iterative, expertise-dependent process, and the growing burden of cancer cases has made reliance on manual planning increasingly unsustainable, underscoring the need for automation. In this study, we propose a workflow that leverages a large language model (LLM)-based agent to navigate inverse treatment planning for intensity-modulated radiation therapy (IMRT). The LLM agent was implemented to directly interact with a clinical treatment planning system (TPS) to iteratively extract intermediate plan states and propose new constraint values to guide inverse optimization. The agent's decision-making process is informed by current observations and previous optimization attempts and evaluations, allowing for dynamic strategy refinement. The planning process was performed in a zero-shot inference setting, where the LLM operated without prior exposure to manually generated treatment plans and was utilized without any fine-tuning or task-specific training. The LLM-generated plans were evaluated on twenty head-and-neck cancer cases against clinical manual plans, with key dosimetric endpoints analyzed and reported. The LLM-generated plans achieved comparable organ-at-risk (OAR) sparing relative to clinical plans while demonstrating improved hot spot control (Dmax: 106.5% vs. 108.8%) and superior conformity (conformity index: 1.18 vs. 1.39 for boost PTV; 1.82 vs. 1.88 for primary PTV). This study demonstrates the feasibility of a zero-shot, LLM-driven workflow for automated IMRT treatment planning in a commercial TPS. The proposed approach provides a generalizable and clinically applicable solution that could reduce planning variability and support broader adoption of AI-based planning strategies.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§åŸºäºå¤§è¯­è¨€æ¨¡å‹(LLM)æ™ºèƒ½ä½“çš„è‡ªåŠ¨åŒ–å·¥ä½œæµç¨‹ï¼Œæ—¨åœ¨è§£å†³å¼ºåº¦è°ƒåˆ¶æ”¾å°„æ²»ç–—(IMRT)è®¡åˆ’åˆ¶å®šå¯¹äººå·¥ç»éªŒè¿‡åº¦ä¾èµ–çš„é—®é¢˜ã€‚è¯¥æ™ºèƒ½ä½“èƒ½å¤Ÿç›´æ¥ä¸ä¸´åºŠæ²»ç–—è®¡åˆ’ç³»ç»Ÿ(TPS)äº¤äº’ï¼Œé€šè¿‡å®æ—¶æå–ä¸­é—´è®¡åˆ’çŠ¶æ€å¹¶åŠ¨æ€å»ºè®®æ–°çš„çº¦æŸå€¼æ¥å¼•å¯¼é€†å‘ä¼˜åŒ–ã€‚è¯¥æµç¨‹åœ¨é›¶æ ·æœ¬(Zero-Shot)æ¨ç†è®¾ç½®ä¸‹è¿è¡Œï¼Œæ— éœ€é¢„å…ˆæ¥è§¦æ‰‹åŠ¨è®¡åˆ’æˆ–è¿›è¡Œä»»ä½•ç‰¹å®šçš„æ¨¡å‹å¾®è°ƒã€‚åœ¨20ä¾‹å¤´é¢ˆéƒ¨ç™Œç—‡ç—…ä¾‹çš„è¯„ä¼°ä¸­ï¼ŒLLMç”Ÿæˆçš„è®¡åˆ’åœ¨å±åŠå™¨å®˜(OAR)ä¿æŠ¤æ–¹é¢ä¸ä¸´åºŠäººå·¥è®¡åˆ’ç›¸å½“ï¼Œä¸”åœ¨çƒ­ç‚¹æ§åˆ¶(Dmax)å’Œé€‚å½¢æŒ‡æ•°(Conformity Index)ä¸Šè¡¨ç°æ›´ä¼˜ã€‚ç ”ç©¶ç»“æœè¯æ˜äº†åœ¨å•†ç”¨TPSä¸­å®ç°LLMé©±åŠ¨çš„å…¨è‡ªåŠ¨æ”¾ç–—è®¡åˆ’çš„å¯è¡Œæ€§ã€‚è¿™ç§æ–¹æ³•æä¾›äº†ä¸€ç§å…·æœ‰é€šç”¨æ€§å’Œä¸´åºŠåº”ç”¨ä»·å€¼çš„è§£å†³æ–¹æ¡ˆï¼Œæœ‰åŠ©äºé™ä½è®¡åˆ’åˆ¶å®šçš„å˜å¼‚æ€§å¹¶æ¨åŠ¨AIè¾…åŠ©æ”¾ç–—ç­–ç•¥çš„å¹¿æ³›åº”ç”¨ã€‚",
      "categories": [
        "physics.med-ph",
        "cs.AI",
        "cs.RO"
      ],
      "primary_category": "physics.med-ph",
      "comment": "Accepted for poster presentation at the NeurIPS 2025 Workshop on GenAI for Health: Potential, Trust, and Policy Compliance",
      "pdf_url": "https://arxiv.org/pdf/2510.11754v1",
      "published_date": "2025-10-12 19:21:21 UTC",
      "updated_date": "2025-10-12 19:21:21 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T02:50:33.580619+00:00"
    },
    {
      "arxiv_id": "2510.10767v2",
      "title": "Understanding Sampler Stochasticity in Training Diffusion Models for RLHF",
      "title_zh": "ç†è§£ RLHF æ‰©æ•£æ¨¡å‹è®­ç»ƒä¸­çš„é‡‡æ ·å™¨éšæœºæ€§",
      "authors": [
        "Jiayuan Sheng",
        "Hanyang Zhao",
        "Haoxian Chen",
        "David D. Yao",
        "Wenpin Tang"
      ],
      "abstract": "Reinforcement Learning from Human Feedback (RLHF) is increasingly used to fine-tune diffusion models, but a key challenge arises from the mismatch between stochastic samplers used during training and deterministic samplers used during inference. In practice, models are fine-tuned using stochastic SDE samplers to encourage exploration, while inference typically relies on deterministic ODE samplers for efficiency and stability. This discrepancy induces a reward gap, raising concerns about whether high-quality outputs can be expected during inference. In this paper, we theoretically characterize this reward gap and provide non-vacuous bounds for general diffusion models, along with sharper convergence rates for Variance Exploding (VE) and Variance Preserving (VP) Gaussian models. Methodologically, we adopt the generalized denoising diffusion implicit models (gDDIM) framework to support arbitrarily high levels of stochasticity, preserving data marginals throughout. Empirically, our findings through large-scale experiments on text-to-image models using denoising diffusion policy optimization (DDPO) and mixed group relative policy optimization (MixGRPO) validate that reward gaps consistently narrow over training, and ODE sampling quality improves when models are updated using higher-stochasticity SDE training.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†åœ¨ Diffusion Models çš„ RLHF è®­ç»ƒä¸­ï¼Œè®­ç»ƒé˜¶æ®µä½¿ç”¨çš„éšæœº SDE é‡‡æ ·å™¨ä¸æ¨ç†é˜¶æ®µä½¿ç”¨çš„ç¡®å®šæ€§ ODE é‡‡æ ·å™¨ä¹‹é—´çš„ä¸åŒ¹é…é—®é¢˜ã€‚è¿™ç§ä¸ä¸€è‡´æ€§å¼•å‘äº†å¥–åŠ±å·®è· (reward gap)ï¼Œå¯èƒ½å¯¼è‡´æ¨ç†æ—¶æ— æ³•è·å¾—é«˜è´¨é‡è¾“å‡ºã€‚æœ¬æ–‡ä»ç†è®ºä¸Šè¡¨å¾äº†è¿™ä¸€å¥–åŠ±å·®è·ï¼Œå¹¶ä¸ºé€šç”¨æ‰©æ•£æ¨¡å‹æä¾›äº†éçœŸç©ºè¾¹ç•Œï¼ŒåŒæ—¶ç»™å‡ºäº† VE å’Œ VP é«˜æ–¯æ¨¡å‹çš„ç²¾ç¡®æ”¶æ•›ç‡ã€‚æ–¹æ³•ä¸Šé‡‡ç”¨äº† g-DDIM æ¡†æ¶ä»¥æ”¯æŒä¸åŒç¨‹åº¦çš„éšæœºæ€§ï¼Œå¹¶ç¡®ä¿æ•°æ®è¾¹ç¼˜åˆ†å¸ƒçš„å®Œæ•´æ€§ã€‚é€šè¿‡å¯¹æ–‡æœ¬ç”Ÿæˆå›¾åƒæ¨¡å‹ä½¿ç”¨ DDPO å’Œ MixGRPO è¿›è¡Œçš„å¤§è§„æ¨¡å®éªŒéªŒè¯ï¼Œç»“æœæ˜¾ç¤ºå¥–åŠ±å·®è·åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ä¼šæŒç»­ç¼©å°ã€‚å®éªŒè¿›ä¸€æ­¥è¯æ˜ï¼Œä½¿ç”¨æ›´é«˜éšæœºæ€§çš„ SDE è¿›è¡Œè®­ç»ƒæ˜¾è‘—æå‡äº†æ¨ç†æ—¶ä½¿ç”¨ ODE é‡‡æ ·çš„è´¨é‡ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "math.OC"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.10767v2",
      "published_date": "2025-10-12 19:08:38 UTC",
      "updated_date": "2025-12-16 18:10:07 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T02:50:30.785778+00:00"
    },
    {
      "arxiv_id": "2510.10766v1",
      "title": "GPS Spoofing Attack Detection in Autonomous Vehicles Using Adaptive DBSCAN",
      "title_zh": "åŸºäºè‡ªé€‚åº”DBSCANçš„è‡ªåŠ¨é©¾é©¶è½¦è¾†GPSæ¬ºéª—æ”»å‡»æ£€æµ‹",
      "authors": [
        "Ahmad Mohammadi",
        "Reza Ahmari",
        "Vahid Hemmati",
        "Frederick Owusu-Ambrose",
        "Mahmoud Nabil Mahmoud",
        "Parham Kebria",
        "Abdollah Homaifar",
        "Mehrdad Saif"
      ],
      "abstract": "As autonomous vehicles become an essential component of modern transportation, they are increasingly vulnerable to threats such as GPS spoofing attacks. This study presents an adaptive detection approach utilizing a dynamically tuned Density Based Spatial Clustering of Applications with Noise (DBSCAN) algorithm, designed to adjust the detection threshold (Îµ) in real-time. The threshold is updated based on the recursive mean and standard deviation of displacement errors between GPS and in-vehicle sensors data, but only at instances classified as non-anomalous. Furthermore, an initial threshold, determined from 120,000 clean data samples, ensures the capability to identify even subtle and gradual GPS spoofing attempts from the beginning. To assess the performance of the proposed method, five different subsets from the real-world Honda Research Institute Driving Dataset (HDD) are selected to simulate both large and small magnitude GPS spoofing attacks. The modified algorithm effectively identifies turn-by-turn, stop, overshoot, and multiple small biased spoofing attacks, achieving detection accuracies of 98.621%, 99.960.1%, 99.880.1%, and 98.380.1%, respectively. This work provides a substantial advancement in enhancing the security and safety of AVs against GPS spoofing threats.",
      "tldr_zh": "æœ¬ç ”ç©¶é’ˆå¯¹è‡ªåŠ¨é©¾é©¶è½¦è¾†æ—¥ç›Šä¸¥å³»çš„ GPS spoofing æ”»å‡»å¨èƒï¼Œæå‡ºäº†ä¸€ç§åŸºäºè‡ªé€‚åº” DBSCAN ç®—æ³•çš„åŠ¨æ€æ£€æµ‹æ–¹æ³•ã€‚è¯¥æ–¹æ¡ˆçš„æ ¸å¿ƒåœ¨äºå®æ—¶è°ƒæ•´æ£€æµ‹é˜ˆå€¼ Îµï¼Œé€šè¿‡é€’å½’è®¡ç®— GPS ä¸è½¦è½½ä¼ æ„Ÿå™¨ä½ç§»è¯¯å·®çš„å‡å€¼å’Œæ ‡å‡†å·®æ¥è¯†åˆ«æ½œåœ¨å¼‚å¸¸ã€‚ç ”ç©¶å›¢é˜Ÿåˆ©ç”¨ 120,000 ä¸ªçº¯å‡€æ ·æœ¬å»ºç«‹åˆå§‹é˜ˆå€¼ï¼Œä½¿ç³»ç»Ÿå…·å¤‡è¯†åˆ«ç»†å¾®ä¸”æ¸è¿›å¼æ¬ºéª—æ”»å‡»çš„èƒ½åŠ›ã€‚å®éªŒåŸºäºçœŸå®çš„ Honda Research Institute Driving Dataset (HDD) æ•°æ®ï¼ŒæˆåŠŸæ£€æµ‹äº† turn-by-turnã€stopã€overshoot ä»¥åŠå¤šç§å°åå·®æ”»å‡»åœºæ™¯ï¼Œæœ€é«˜å‡†ç¡®ç‡è¾¾ 99.96%ã€‚è¯¥æˆæœä¸ºæå‡è‡ªåŠ¨é©¾é©¶è½¦è¾†åœ¨å¤æ‚ç¯å¢ƒä¸‹çš„å®‰å…¨æ€§å’Œé²æ£’æ€§æä¾›äº†å¼ºæœ‰åŠ›çš„æŠ€æœ¯æ”¯æ’‘ã€‚",
      "categories": [
        "cs.CR",
        "cs.AI",
        "eess.SY"
      ],
      "primary_category": "cs.CR",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.10766v1",
      "published_date": "2025-10-12 19:06:44 UTC",
      "updated_date": "2025-10-12 19:06:44 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T02:50:35.578038+00:00"
    },
    {
      "arxiv_id": "2510.10764v4",
      "title": "Optimally Deep Networks -- Adapting Model Depth to Datasets for Superior Efficiency",
      "title_zh": "æœ€ä¼˜æ·±åº¦ç½‘ç»œï¼šé¢å‘æ•°æ®é›†é€‚é…æ¨¡å‹æ·±åº¦ä»¥å®ç°å“è¶Šæ•ˆç‡",
      "authors": [
        "Shaharyar Ahmed Khan Tareen",
        "Filza Khan Tareen"
      ],
      "abstract": "Deep neural networks (DNNs) have provided brilliant performance across various tasks. However, this success often comes at the cost of unnecessarily large model sizes, high computational demands, and substantial memory footprints. Typically, powerful architectures are trained at full depths but not all datasets or tasks require such high model capacity. Training big and deep architectures on relatively low-complexity datasets frequently leads to wasted computation, unnecessary energy consumption, and excessive memory usage, which in turn makes deployment of models on resource-constrained devices impractical. To address this problem, we introduce the concept of Optimally Deep Networks (ODNs), which provides a balance between model depth and task complexity. Specifically, we propose a NAS like training strategy called progressive depth expansion, which begins by training neural networks at shallower depths and incrementally increases their depth as the earlier blocks converge, continuing this process until the target accuracy is reached. ODNs use only the optimal depth for the tasks at hand, removing redundant layers. This cuts down future training and inference costs, lowers the model memory footprint, enhances computational efficiency, and facilitates deployment on edge devices. Empirical results show that the optimal depths of ResNet-18 and ResNet-34 for MNIST and SVHN, achieve up to 98.64 % and 96.44 % reduction in memory footprint, while maintaining a competitive accuracy of 99.31 % and 96.08 %, respectively.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†æœ€ä¼˜æ·±åº¦ç½‘ç»œ(Optimally Deep Networks, ODNs)çš„æ¦‚å¿µï¼Œæ—¨åœ¨è§£å†³æ·±åº¦ç¥ç»ç½‘ç»œ(DNNs)åœ¨å¤„ç†ä½å¤æ‚åº¦æ•°æ®é›†æ—¶å› æ¨¡å‹è¿‡å¤§è€Œå¯¼è‡´çš„è®¡ç®—æµªè´¹å’Œå†…å­˜å ç”¨è¿‡é«˜é—®é¢˜ã€‚ä½œè€…å¼•å…¥äº†ä¸€ç§ç±»ä¼¼äºç¥ç»ç½‘ç»œæ¶æ„æœç´¢(NAS)çš„è®­ç»ƒç­–ç•¥ï¼Œç§°ä¸ºæ¸è¿›å¼æ·±åº¦æ‰©å±•(progressive depth expansion)ï¼Œè¯¥æ–¹æ³•ä»æµ…å±‚ç½‘ç»œå¼€å§‹è®­ç»ƒï¼Œå¹¶åœ¨æ—©æœŸæ¨¡å—æ”¶æ•›åé€æ­¥å¢åŠ æ·±åº¦ï¼Œç›´è‡³è¾¾åˆ°ç›®æ ‡ç²¾åº¦ã€‚è¿™ç§ç­–ç•¥é€šè¿‡åŠ¨æ€è°ƒæ•´æ¨¡å‹æ·±åº¦ä»¥åŒ¹é…ä»»åŠ¡å¤æ‚åº¦ï¼Œæœ‰æ•ˆå»é™¤äº†å†—ä½™å±‚ï¼Œæ˜¾è‘—é™ä½äº†è®­ç»ƒä¸æ¨ç†æˆæœ¬ï¼Œå¹¶æå‡äº†æ¨¡å‹åœ¨èµ„æºå—é™è¾¹ç¼˜è®¾å¤‡ä¸Šçš„éƒ¨ç½²èƒ½åŠ›ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œé’ˆå¯¹MNISTå’ŒSVHNæ•°æ®é›†ï¼ŒResNet-18å’ŒResNet-34é€šè¿‡ODNsç­–ç•¥åœ¨ä¿æŒæé«˜å‡†ç¡®ç‡çš„åŒæ—¶ï¼Œå®ç°äº†é«˜è¾¾98.64%å’Œ96.44%çš„å†…å­˜å ç”¨å‰Šå‡ï¼Œè¯æ˜äº†è¯¥æ–¹æ³•åœ¨å¹³è¡¡æ¨¡å‹æ•ˆç‡ä¸ä»»åŠ¡éœ€æ±‚æ–¹é¢çš„å“è¶Šæ€§èƒ½ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.LG",
      "comment": "6 pages, 4 figures, 1 table, 2 equations, 1 algorithm",
      "pdf_url": "https://arxiv.org/pdf/2510.10764v4",
      "published_date": "2025-10-12 19:05:04 UTC",
      "updated_date": "2025-11-25 07:35:02 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T02:50:33.074584+00:00"
    },
    {
      "arxiv_id": "2510.10739v1",
      "title": "A Stochastic Differential Equation Framework for Multi-Objective LLM Interactions: Dynamical Systems Analysis with Code Generation Applications",
      "title_zh": "é¢å‘å¤šç›®æ ‡ LLM äº¤äº’çš„éšæœºå¾®åˆ†æ–¹ç¨‹æ¡†æ¶ï¼šåŠ¨åŠ›ç³»ç»Ÿåˆ†æä¸ä»£ç ç”Ÿæˆåº”ç”¨",
      "authors": [
        "Shivani Shukla",
        "Himanshu Joshi"
      ],
      "abstract": "We introduce a general stochastic differential equation framework for modelling multiobjective optimization dynamics in iterative Large Language Model (LLM) interactions. Our framework captures the inherent stochasticity of LLM responses through explicit diffusion terms and reveals systematic interference patterns between competing objectives via an interference matrix formulation. We validate our theoretical framework using iterative code generation as a proof-of-concept application, analyzing 400 sessions across security, efficiency, and functionality objectives. Our results demonstrate strategy-dependent convergence behaviors with rates ranging from 0.33 to 1.29, and predictive accuracy achieving R2 = 0.74 for balanced approaches. This work proposes the feasibility of dynamical systems analysis for multi-objective LLM interactions, with code generation serving as an initial validation domain.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ä¸ªé€šç”¨çš„éšæœºå¾®åˆ†æ–¹ç¨‹ (Stochastic Differential Equation) æ¡†æ¶ï¼Œç”¨äºå»ºæ¨¡å¤§è¯­è¨€æ¨¡å‹ (LLM) åœ¨è¿­ä»£äº¤äº’ä¸­çš„å¤šç›®æ ‡ä¼˜åŒ–åŠ¨æ€ã€‚è¯¥æ¡†æ¶é€šè¿‡æ˜¾å¼çš„æ‰©æ•£é¡¹ (diffusion terms) æ•æ‰ LLM å“åº”çš„å›ºæœ‰éšæœºæ€§ï¼Œå¹¶åˆ©ç”¨å¹²æ‰°çŸ©é˜µ (interference matrix) å…¬å¼æ­ç¤ºäº†ä¸åŒç«äº‰ç›®æ ‡ä¹‹é—´çš„ç³»ç»Ÿæ€§å¹²æ‰°æ¨¡å¼ã€‚ç ”ç©¶ä»¥è¿­ä»£ä»£ç ç”Ÿæˆä½œä¸ºæ¦‚å¿µéªŒè¯åº”ç”¨ï¼Œåœ¨æ¶‰åŠå®‰å…¨æ€§ã€æ•ˆç‡å’ŒåŠŸèƒ½æ€§ç›®æ ‡çš„ 400 æ¬¡ä¼šè¯ä¸­éªŒè¯äº†ç†è®ºæ¡†æ¶ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ¨¡å‹çš„æ”¶æ•›é€Ÿç‡åœ¨ 0.33 è‡³ 1.29 ä¹‹é—´ï¼Œä¸”å¹³è¡¡ç­–ç•¥çš„é¢„æµ‹å‡†ç¡®ç‡è¾¾åˆ°äº† R2 = 0.74ã€‚è¯¥å·¥ä½œè¯æ˜äº†å¯¹å¤šç›®æ ‡ LLM äº¤äº’è¿›è¡ŒåŠ¨åŠ›ç³»ç»Ÿ (dynamical systems) åˆ†æçš„å¯è¡Œæ€§ï¼Œä¸ºç†è§£å¤æ‚çš„æ¨¡å‹äº¤äº’è¡Œä¸ºæä¾›äº†æ•°å­¦ç†è®ºæ”¯æ’‘ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.SE"
      ],
      "primary_category": "cs.LG",
      "comment": "Peer-reviewed and accepted to the 39th Conference on Neural Information Processing Systems (NeurIPS 2025) DynaFront 2025 Workshop (https://sites.google.com/view/dynafrontneurips25)",
      "pdf_url": "https://arxiv.org/pdf/2510.10739v1",
      "published_date": "2025-10-12 18:25:12 UTC",
      "updated_date": "2025-10-12 18:25:12 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T02:50:45.368774+00:00"
    },
    {
      "arxiv_id": "2510.10738v1",
      "title": "Proficiency-Aware Adaptation and Data Augmentation for Robust L2 ASR",
      "title_zh": "é¢å‘é²æ£’ L2 è¯­éŸ³è¯†åˆ«çš„ç†Ÿç»ƒåº¦æ„ŸçŸ¥é€‚é…ä¸æ•°æ®å¢å¼º",
      "authors": [
        "Ling Sun",
        "Charlotte Zhu",
        "Shuju Shi"
      ],
      "abstract": "General-purpose ASR underperforms for atypical speakers, such as L2 learners, reinforcing bias and limiting use in education and accessibility. Using the CEFR-graded Speak and Improve corpus, we show that naive fine-tuning of Whisper reduces average WER but simultaneously widens disparities and disproportionately harms lower-level learners. To address this, we propose two strategies: (i) proficiency-aware multitask learning, jointly optimizing ASR with proficiency classification, and (ii) targeted augmentation, applying spectrogram masking to low-proficiency speech to counter imbalance. These approaches reduce WER by up to 29.4 percent (relative) and insertion/deletion errors by as much as 58.6 percent (relative). Crucially, despite the severe imbalance of the dataset reflecting real-world distributions, both strategies consistently narrow proficiency gaps, advancing equitable ASR for L2 learners.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹é€šç”¨ ASR ç³»ç»Ÿåœ¨ L2 å­¦ä¹ è€…ç­‰éå…¸å‹è¯´è¯äººä¸Šè¡¨ç°ä¸ä½³ä¸”å­˜åœ¨åè§çš„é—®é¢˜ï¼Œåˆ†æäº† Whisper æ¨¡å‹åœ¨å¾®è°ƒè¿‡ç¨‹ä¸­å¯¹ä¸åŒæ°´å¹³å­¦ä¹ è€…çš„å½±å“ã€‚ç ”ç©¶å‘ç°ï¼Œä¼ ç»Ÿçš„åŸç”Ÿå¾®è°ƒè™½èƒ½é™ä½å¹³å‡ WERï¼Œå´ä¼šæ‰©å¤§ä¸åŒç†Ÿç»ƒåº¦å±‚çº§é—´çš„æ€§èƒ½å·®è·ï¼Œå°¤å…¶å¯¹ä½æ°´å¹³å­¦ä¹ è€…é€ æˆäº†ä¸æˆæ¯”ä¾‹çš„æŸå®³ã€‚ä¸ºæ­¤ï¼Œä½œè€…æå‡ºäº†ç†Ÿç»ƒåº¦æ„ŸçŸ¥(Proficiency-aware)çš„å¤šä»»åŠ¡å­¦ä¹ ç­–ç•¥ï¼Œå°† ASR ä¼˜åŒ–ä¸ç†Ÿç»ƒåº¦åˆ†ç±»ä»»åŠ¡ç›¸ç»“åˆã€‚åŒæ—¶ï¼Œç ”ç©¶å¼•å…¥äº†é’ˆå¯¹æ€§æ•°æ®å¢å¼º(Targeted augmentation)ï¼Œé€šè¿‡å¯¹ä½ç†Ÿç»ƒåº¦è¯­éŸ³åº”ç”¨é¢‘è°±é®ç›–(Spectrogram masking)æ¥åº”å¯¹æ•°æ®åˆ†å¸ƒä¸å‡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¿™äº›æ–¹æ³•ä½¿ WER ç›¸å¯¹é™ä½äº†é«˜è¾¾ 29.4%ï¼Œå¹¶æ˜¾è‘—å‡å°‘äº†æ’å…¥å’Œåˆ é™¤é”™è¯¯ã€‚è¯¥ç ”ç©¶åœ¨çœŸå®ä¸”å¤±è¡¡çš„æ•°æ®åˆ†å¸ƒä¸‹ï¼ŒæˆåŠŸç¼©å°äº†ä¸åŒæ°´å¹³å­¦ä¹ è€…é—´çš„æŠ€æœ¯é¸¿æ²Ÿï¼Œä¸ºæ„å»ºå…¬å¹³ä¸”é²æ£’çš„ L2 ASR ç³»ç»Ÿæä¾›äº†æœ‰æ•ˆæ–¹æ¡ˆã€‚",
      "categories": [
        "cs.SD",
        "cs.AI"
      ],
      "primary_category": "cs.SD",
      "comment": "Submitted to ICASSP 2026",
      "pdf_url": "https://arxiv.org/pdf/2510.10738v1",
      "published_date": "2025-10-12 18:20:58 UTC",
      "updated_date": "2025-10-12 18:20:58 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T02:50:45.986925+00:00"
    },
    {
      "arxiv_id": "2510.10730v1",
      "title": "Provable Anytime Ensemble Sampling Algorithms in Nonlinear Contextual Bandits",
      "title_zh": "éçº¿æ€§ä¸Šä¸‹æ–‡å¤šè‡‚è€è™æœºä¸­å…·æœ‰ç†è®ºä¿è¯çš„éšæ—¶é›†æˆé‡‡æ ·ç®—æ³•",
      "authors": [
        "Jiazheng Sun",
        "Weixin Wang",
        "Pan Xu"
      ],
      "abstract": "We provide a unified algorithmic framework for ensemble sampling in nonlinear contextual bandits and develop corresponding regret bounds for two most common nonlinear contextual bandit settings: Generalized Linear Ensemble Sampling (\\texttt{GLM-ES}) for generalized linear bandits and Neural Ensemble Sampling (\\texttt{Neural-ES}) for neural contextual bandits. Both methods maintain multiple estimators for the reward model parameters via maximum likelihood estimation on randomly perturbed data. We prove high-probability frequentist regret bounds of $\\mathcal{O}(d^{3/2} \\sqrt{T} + d^{9/2})$ for \\texttt{GLM-ES} and $\\mathcal{O}(\\widetilde{d} \\sqrt{T})$ for \\texttt{Neural-ES}, where $d$ is the dimension of feature vectors, $\\widetilde{d}$ is the effective dimension of a neural tangent kernel matrix, and $T$ is the number of rounds. These regret bounds match the state-of-the-art results of randomized exploration algorithms in nonlinear contextual bandit settings. In the theoretical analysis, we introduce techniques that address challenges specific to nonlinear models. Practically, we remove fixed-time horizon assumptions by developing anytime versions of our algorithms, suitable when $T$ is unknown. Finally, we empirically evaluate \\texttt{GLM-ES}, \\texttt{Neural-ES}, and their anytime variants, demonstrating strong performance. Overall, our results establish ensemble sampling as a provable and practical randomized exploration approach for nonlinear contextual bandits.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ä¸ªç”¨äºéçº¿æ€§ä¸Šä¸‹æ–‡å¤šè‡‚è€è™æœº (Nonlinear Contextual Bandits) çš„é›†æˆé‡‡æ · (Ensemble Sampling) ç»Ÿä¸€ç®—æ³•æ¡†æ¶ï¼Œå¹¶å¼€å‘äº†é’ˆå¯¹å¹¿ä¹‰çº¿æ€§è€è™æœºçš„ GLM-ES å’Œé’ˆå¯¹ç¥ç»è€è™æœºçš„ Neural-ES ä¸¤ç§å…·ä½“æ–¹æ³•ã€‚è¿™äº›æ–¹æ³•é€šè¿‡åœ¨éšæœºæ‰°åŠ¨æ•°æ®ä¸Šæ‰§è¡Œæœ€å¤§ä¼¼ç„¶ä¼°è®¡ (Maximum Likelihood Estimation) æ¥ç»´æŠ¤å¤šä¸ªå‚æ•°ä¼°ç®—å™¨ï¼Œä»è€Œå®ç°æœ‰æ•ˆçš„éšæœºæ¢ç´¢ã€‚ç†è®ºåˆ†æè¡¨æ˜ï¼ŒGLM-ES å’Œ Neural-ES çš„æ‚”å€¼ç•Œ (Regret Bounds) åˆ†åˆ«è¾¾åˆ°äº† $\\mathcal{O}(d^{3/2} \\sqrt{T} + d^{9/2})$ å’Œ $\\mathcal{O}(\\widetilde{d} \\sqrt{T})$ï¼Œä¸ç°æœ‰æœ€ä¼˜çš„éšæœºæ¢ç´¢ç®—æ³•æ€§èƒ½ç›¸å½“ã€‚æ­¤å¤–ï¼Œç ”ç©¶è€…é€šè¿‡å¼€å‘ Anytime ç‰ˆæœ¬çš„ç®—æ³•ï¼ŒæˆåŠŸæ¶ˆé™¤äº†å¯¹å›ºå®šæ—¶é—´çª—å£ $T$ çš„ä¾èµ–ï¼Œä½¿å…¶åœ¨å®é™…åº”ç”¨ä¸­æ›´å…·çµæ´»æ€§ã€‚å®éªŒç»“æœéªŒè¯äº† GLM-ESã€Neural-ES åŠå…¶ Anytime å˜ä½“çš„å¼ºåŠ²è¡¨ç°ï¼Œç¡®ç«‹äº†é›†æˆé‡‡æ ·ä½œä¸ºä¸€ç§å…·æœ‰å¯è¯æ˜ç†è®ºä¿éšœä¸”å®ç”¨çš„éçº¿æ€§éšæœºæ¢ç´¢æ–¹æ¡ˆçš„åœ°ä½ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "stat.ML"
      ],
      "primary_category": "cs.LG",
      "comment": "40 pages, 1 figure",
      "pdf_url": "https://arxiv.org/pdf/2510.10730v1",
      "published_date": "2025-10-12 18:05:53 UTC",
      "updated_date": "2025-10-12 18:05:53 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T02:50:48.772043+00:00"
    },
    {
      "arxiv_id": "2510.10719v1",
      "title": "SS-DPPN: A self-supervised dual-path foundation model for the generalizable cardiac audio representation",
      "title_zh": "SS-DPPNï¼šé¢å‘å¯æ³›åŒ–å¿ƒéŸ³è¡¨å¾çš„è‡ªç›‘ç£åŒè·¯å¾„åŸºåº§æ¨¡å‹",
      "authors": [
        "Ummy Maria Muna",
        "Md Mehedi Hasan Shawon",
        "Md Jobayer",
        "Sumaiya Akter",
        "Md Rakibul Hasan",
        "Md. Golam Rabiul Alam"
      ],
      "abstract": "The automated analysis of phonocardiograms is vital for the early diagnosis of cardiovascular disease, yet supervised deep learning is often constrained by the scarcity of expert-annotated data. In this paper, we propose the Self-Supervised Dual-Path Prototypical Network (SS-DPPN), a foundation model for cardiac audio representation and classification from unlabeled data. The framework introduces a dual-path contrastive learning based architecture that simultaneously processes 1D waveforms and 2D spectrograms using a novel hybrid loss. For the downstream task, a metric-learning approach using a Prototypical Network was used that enhances sensitivity and produces well-calibrated and trustworthy predictions. SS-DPPN achieves state-of-the-art performance on four cardiac audio benchmarks. The framework demonstrates exceptional data efficiency with a fully supervised model on three-fold reduction in labeled data. Finally, the learned representations generalize successfully across lung sound classification and heart rate estimation. Our experiments and findings validate SS-DPPN as a robust, reliable, and scalable foundation model for physiological signals.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†è‡ªç›‘ç£åŒè·¯å¾„åŸå‹ç½‘ç»œ(SS-DPPN)ï¼Œè¿™æ˜¯ä¸€ç§ä¸“ä¸ºå¿ƒéŸ³(Phonocardiograms)è¡¨å¾å’Œåˆ†ç±»è®¾è®¡çš„è‡ªç›‘ç£åŸºç¡€æ¨¡å‹ï¼Œæ—¨åœ¨è§£å†³æ·±åº¦å­¦ä¹ åœ¨å¿ƒè„éŸ³é¢‘åˆ†æä¸­é¢ä¸´çš„ä¸“å®¶æ ‡æ³¨æ•°æ®ç¨€ç¼ºé—®é¢˜ã€‚è¯¥æ¡†æ¶é‡‡ç”¨åŒè·¯å¾„å¯¹æ¯”å­¦ä¹ æ¶æ„ï¼Œèƒ½å¤Ÿåˆ©ç”¨æ–°å‹æ··åˆæŸå¤±(hybrid loss)åŒæ—¶å¤„ç†1D waveformå’Œ2D spectrogramã€‚åœ¨ä¸‹æ¸¸ä»»åŠ¡ä¸­ï¼Œç ”ç©¶è€…ç»“åˆåŸå‹ç½‘ç»œ(Prototypical Network)çš„åº¦é‡å­¦ä¹ æ–¹æ³•ï¼Œå¢å¼ºäº†æ¨¡å‹é¢„æµ‹çš„çµæ•åº¦ã€æ ¡å‡†åº¦å’Œå¯ä¿¡åº¦ã€‚å®éªŒè¯æ˜ï¼ŒSS-DPPNåœ¨å››ä¸ªå¿ƒéŸ³åŸºå‡†æ•°æ®é›†ä¸Šå‡è¾¾åˆ°äº†SOTAæ€§èƒ½ï¼Œå¹¶åœ¨æ ‡æ³¨æ•°æ®é‡å‡å°‘è‡³ä¸‰åˆ†ä¹‹ä¸€çš„æƒ…å†µä¸‹ä»ä¿æŒé«˜æ•ˆã€‚æ­¤å¤–ï¼Œè¯¥æ¨¡å‹å­¦ä¹ åˆ°çš„è¡¨å¾åœ¨è‚ºéŸ³åˆ†ç±»å’Œå¿ƒç‡ä¼°ç®—ä¸­ä¹Ÿå±•ç°å‡ºæˆåŠŸçš„æ³›åŒ–èƒ½åŠ›ã€‚è¿™é¡¹ç ”ç©¶éªŒè¯äº†SS-DPPNä½œä¸ºç”Ÿç†ä¿¡å·å¤„ç†é¢†åŸŸä¸€ç§é²æ£’ä¸”å¯æ‰©å±•çš„åŸºç¡€æ¨¡å‹çš„æ½œåŠ›ã€‚",
      "categories": [
        "cs.SD",
        "cs.AI"
      ],
      "primary_category": "cs.SD",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.10719v1",
      "published_date": "2025-10-12 17:43:57 UTC",
      "updated_date": "2025-10-12 17:43:57 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T02:50:51.960423+00:00"
    },
    {
      "arxiv_id": "2510.10718v1",
      "title": "HYPERDOA: Robust and Efficient DoA Estimation using Hyperdimensional Computing",
      "title_zh": "HYPERDOAï¼šåŸºäºé«˜ç»´è®¡ç®—çš„ç¨³å¥é«˜æ•ˆæ³¢è¾¾æ–¹å‘ä¼°è®¡",
      "authors": [
        "Rajat Bhattacharjya",
        "Woohyeok Park",
        "Arnab Sarkar",
        "Hyunwoo Oh",
        "Mohsen Imani",
        "Nikil Dutt"
      ],
      "abstract": "Direction of Arrival (DoA) estimation techniques face a critical trade-off, as classical methods often lack accuracy in challenging, low signal-to-noise ratio (SNR) conditions, while modern deep learning approaches are too energy-intensive and opaque for resource-constrained, safety-critical systems. We introduce HYPERDOA, a novel estimator leveraging Hyperdimensional Computing (HDC). The framework introduces two distinct feature extraction strategies -- Mean Spatial-Lag Autocorrelation and Spatial Smoothing -- for its HDC pipeline, and then reframes DoA estimation as a pattern recognition problem. This approach leverages HDC's inherent robustness to noise and its transparent algebraic operations to bypass the expensive matrix decompositions and ``black-box'' nature of classical and deep learning methods, respectively. Our evaluation demonstrates that HYPERDOA achieves ~35.39% higher accuracy than state-of-the-art methods in low-SNR, coherent-source scenarios. Crucially, it also consumes ~93% less energy than competing neural baselines on an embedded NVIDIA Jetson Xavier NX platform. This dual advantage in accuracy and efficiency establishes HYPERDOA as a robust and viable solution for mission-critical applications on edge devices.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹æ³¢è¾¾æ–¹å‘(Direction of Arrival, DoA)ä¼°è®¡ä¸­ç»å…¸æ–¹æ³•åœ¨ä½ä¿¡å™ªæ¯”(SNR)ä¸‹å‡†ç¡®æ€§ä¸è¶³ï¼Œä»¥åŠæ·±åº¦å­¦ä¹ æ–¹æ³•èƒ½è€—é«˜ä¸”ç¼ºä¹é€æ˜åº¦çš„é—®é¢˜ï¼Œæå‡ºäº†HYPERDOAã€‚è¿™æ˜¯ä¸€ä¸ªåˆ©ç”¨è¶…ç»´è®¡ç®—(Hyperdimensional Computing, HDC)çš„æ–°å‹ä¼°è®¡æ¡†æ¶ï¼Œå¼•å…¥äº†å¹³å‡ç©ºé—´æ»åè‡ªç›¸å…³(Mean Spatial-Lag Autocorrelation)å’Œç©ºé—´å¹³æ»‘(Spatial Smoothing)ä¸¤ç§ç‰¹å¾æå–ç­–ç•¥ã€‚è¯¥æ¡†æ¶å°†DoAä¼°è®¡é‡æ„ä¸ºæ¨¡å¼è¯†åˆ«é—®é¢˜ï¼Œåˆ©ç”¨HDCå›ºæœ‰çš„å™ªå£°é²æ£’æ€§å’Œé€æ˜ä»£æ•°è¿ç®—ï¼Œæœ‰æ•ˆè§„é¿äº†ä¼ ç»Ÿæ–¹æ³•çš„å¤æ‚çŸ©é˜µåˆ†è§£å’Œæ·±åº¦å­¦ä¹ çš„â€œé»‘ç®±â€ç‰¹æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œåœ¨ä½SNRå’Œç›¸å¹²æºåœºæ™¯ä¸‹ï¼ŒHYPERDOAçš„å‡†ç¡®ç‡æ¯”ç°æœ‰æœ€å…ˆè¿›æ–¹æ³•é«˜å‡ºçº¦35.39%ã€‚åœ¨åµŒå…¥å¼NVIDIA Jetson Xavier NXå¹³å°ä¸Šï¼Œå…¶èƒ½è€—æ¯”ç¥ç»åŸºå‡†æ¨¡å‹é™ä½äº†çº¦93%ã€‚è¿™ç§åœ¨å‡†ç¡®æ€§å’Œæ•ˆç‡ä¸Šçš„åŒé‡ä¼˜åŠ¿ï¼Œä½¿HYPERDOAæˆä¸ºè¾¹ç¼˜è®¾å¤‡ä¸Šä»»åŠ¡å…³é”®å‹åº”ç”¨çš„é«˜æ•ˆè§£å†³æ–¹æ¡ˆã€‚",
      "categories": [
        "eess.SP",
        "cs.AI",
        "cs.AR",
        "cs.SC"
      ],
      "primary_category": "eess.SP",
      "comment": "3 figures, 5 pages. Authors' version posted for personal use and not for redistribution",
      "pdf_url": "https://arxiv.org/pdf/2510.10718v1",
      "published_date": "2025-10-12 17:42:01 UTC",
      "updated_date": "2025-10-12 17:42:01 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T02:51:02.561055+00:00"
    },
    {
      "arxiv_id": "2510.10713v1",
      "title": "Deep Learning in Astrophysics",
      "title_zh": "å¤©ä½“ç‰©ç†å­¦ä¸­çš„æ·±åº¦å­¦ä¹ ",
      "authors": [
        "Yuan-Sen Ting"
      ],
      "abstract": "Deep learning has generated diverse perspectives in astronomy, with ongoing discussions between proponents and skeptics motivating this review. We examine how neural networks complement classical statistics, extending our data analytical toolkit for modern surveys. Astronomy offers unique opportunities through encoding physical symmetries, conservation laws, and differential equations directly into architectures, creating models that generalize beyond training data. Yet challenges persist as unlabeled observations number in billions while confirmed examples with known properties remain scarce and expensive. This review demonstrates how deep learning incorporates domain knowledge through architectural design, with built-in assumptions guiding models toward physically meaningful solutions. We evaluate where these methods offer genuine advances versus claims requiring careful scrutiny. - Neural architectures overcome trade-offs between scalability, expressivity, and data efficiency by encoding physical symmetries and conservation laws into network structure, enabling learning from limited labeled data. - Simulation-based inference and anomaly detection extract information from complex, non-Gaussian distributions where analytical likelihoods fail, enabling field-level cosmological analysis and systematic discovery of rare phenomena. - Multi-scale neural modeling bridges resolution gaps in astronomical simulations, learning effective subgrid physics from expensive high-fidelity runs to enhance large-volume calculations where direct computation remains prohibitive. - Emerging paradigms-reinforcement learning for telescope operations, foundation models learning from minimal examples, and large language model agents for research automation-show promise though are still developing in astronomical applications.",
      "tldr_zh": "è¿™ç¯‡ç»¼è¿°æ·±å…¥æ¢è®¨äº†Deep Learningåœ¨å¤©ä½“ç‰©ç†å­¦ä¸­çš„å¤šå…ƒåº”ç”¨åŠå…¶ä¸ä¼ ç»Ÿç»Ÿè®¡å­¦çš„äº’è¡¥å…³ç³»ï¼Œé‡ç‚¹åˆ†æäº†å¦‚ä½•å°†ç‰©ç†å¯¹ç§°æ€§ã€å®ˆæ’å®šå¾‹å’Œå¾®åˆ†æ–¹ç¨‹ç›´æ¥ç¼–ç è¿›Neural architecturesä»¥å®ç°æ›´å¼ºçš„æ³›åŒ–èƒ½åŠ›ã€‚æ–‡ç« é˜è¿°äº†åœ¨æ ‡è®°æ•°æ®ç¨€ç¼ºçš„èƒŒæ™¯ä¸‹ï¼Œé€šè¿‡æ¶æ„è®¾è®¡èå…¥é¢†åŸŸçŸ¥è¯†ï¼Œå¼•å¯¼æ¨¡å‹äº§ç”Ÿå…·æœ‰ç‰©ç†æ„ä¹‰çš„è§£ï¼Œä»è€Œå…‹æœæ•°æ®æ•ˆç‡çš„ç“¶é¢ˆã€‚ç ”ç©¶è¯¦ç»†è¯„ä¼°äº†Simulation-based inferenceå’Œanomaly detectionåœ¨å¤„ç†å¤æ‚éé«˜æ–¯åˆ†å¸ƒåŠå‘ç°ç½•è§ç°è±¡ä¸­çš„æ ¸å¿ƒä½œç”¨ï¼Œå¹¶ä»‹ç»äº†å¤šå°ºåº¦ç¥ç»å»ºæ¨¡åœ¨å¤©æ–‡æ¨¡æ‹Ÿä¸­å¼¥è¡¥åˆ†è¾¨ç‡å·®è·çš„ä¼˜åŠ¿ã€‚æ­¤å¤–ï¼Œè¯¥ç»¼è¿°è¿˜å±•æœ›äº†Reinforcement learningç”¨äºæœ›è¿œé•œè¿è¡Œã€foundation modelsä»¥åŠLarge Language Model (LLM) agentsç­‰æ–°å…´èŒƒå¼åœ¨è‡ªåŠ¨åŒ–ç ”ç©¶ä¸­çš„åº”ç”¨å‰æ™¯ã€‚é€šè¿‡æ‰¹åˆ¤æ€§åœ°å®¡è§†å„é¡¹æŠ€æœ¯è¿›å±•ï¼Œè¯¥ç ”ç©¶ä¸ºå¤©æ–‡å­¦ç•Œåˆ©ç”¨æ·±åº¦å­¦ä¹ åº”å¯¹ç°ä»£å·¡å¤©è§‚æµ‹ä¸­çš„æµ·é‡æ•°æ®æŒ‘æˆ˜æä¾›äº†ç³»ç»Ÿæ€§çš„æŒ‡å¯¼å’Œè§è§£ã€‚",
      "categories": [
        "astro-ph.IM",
        "astro-ph.CO",
        "astro-ph.EP",
        "astro-ph.GA",
        "astro-ph.HE",
        "cs.AI"
      ],
      "primary_category": "astro-ph.IM",
      "comment": "Manuscript submitted to Annual Review of Astronomy and Astrophysics for Volume 64. This is the authors' version. Revisions and the final version will be available at https://www.annualreviews.org/content/journals/astro",
      "pdf_url": "https://arxiv.org/pdf/2510.10713v1",
      "published_date": "2025-10-12 17:31:46 UTC",
      "updated_date": "2025-10-12 17:31:46 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T02:51:08.476720+00:00"
    },
    {
      "arxiv_id": "2510.15972v1",
      "title": "Quantum NLP models on Natural Language Inference",
      "title_zh": "è‡ªç„¶è¯­è¨€æ¨ç†ä¸­çš„é‡å­è‡ªç„¶è¯­è¨€å¤„ç†æ¨¡å‹",
      "authors": [
        "Ling Sun",
        "Peter Sullivan",
        "Michael Martin",
        "Yun Zhou"
      ],
      "abstract": "Quantum natural language processing (QNLP) offers a novel approach to semantic modeling by embedding compositional structure directly into quantum circuits. This paper investigates the application of QNLP models to the task of Natural Language Inference (NLI), comparing quantum, hybrid, and classical transformer-based models under a constrained few-shot setting. Using the lambeq library and the DisCoCat framework, we construct parameterized quantum circuits for sentence pairs and train them for both semantic relatedness and inference classification. To assess efficiency, we introduce a novel information-theoretic metric, Information Gain per Parameter (IGPP), which quantifies learning dynamics independent of model size. Our results demonstrate that quantum models achieve performance comparable to classical baselines while operating with dramatically fewer parameters. The Quantum-based models outperform randomly initialized transformers in inference and achieve lower test error on relatedness tasks. Moreover, quantum models exhibit significantly higher per-parameter learning efficiency (up to five orders of magnitude more than classical counterparts), highlighting the promise of QNLP in low-resource, structure-sensitive settings. To address circuit-level isolation and promote parameter sharing, we also propose a novel cluster-based architecture that improves generalization by tying gate parameters to learned word clusters rather than individual tokens.",
      "tldr_zh": "æœ¬ç ”ç©¶æ¢è®¨äº†é‡å­è‡ªç„¶è¯­è¨€å¤„ç† (QNLP) æ¨¡å‹åœ¨è‡ªç„¶è¯­è¨€æ¨ç† (NLI) ä»»åŠ¡ä¸­çš„åº”ç”¨ï¼Œé€šè¿‡å°†ç»„åˆç»“æ„ç›´æ¥åµŒå…¥é‡å­ç”µè·¯ï¼Œæä¾›äº†ä¸€ç§è¯­ä¹‰å»ºæ¨¡çš„æ–°é€”å¾„ã€‚åˆ©ç”¨ lambeq åº“å’Œ DisCoCat æ¡†æ¶ï¼Œç ”ç©¶äººå‘˜ä¸ºå¥å­å¯¹æ„å»ºäº†å‚æ•°åŒ–é‡å­ç”µè·¯ï¼Œå¹¶åœ¨å—é™çš„å°æ ·æœ¬è®¾ç½®ä¸‹è¿›è¡Œäº†è¯­ä¹‰ç›¸å…³æ€§å’Œæ¨ç†åˆ†ç±»è®­ç»ƒã€‚ä¸ºäº†è¯„ä¼°æ•ˆç‡ï¼Œè®ºæ–‡å¼•å…¥äº†å…¨æ–°çš„ä¿¡æ¯è®ºæŒ‡æ ‡â€”â€”æ¯å‚æ•°ä¿¡æ¯å¢ç›Š (IGPP)ï¼Œç”¨ä»¥è¡¡é‡ç‹¬ç«‹äºæ¨¡å‹è§„æ¨¡çš„å­¦ä¹ åŠ¨æ€ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œé‡å­æ¨¡å‹åœ¨å‚æ•°é‡æå°‘çš„æƒ…å†µä¸‹å®ç°äº†ä¸ç»å…¸åŸºçº¿ç›¸å½“çš„æ€§èƒ½ï¼Œä¸”åœ¨æ¨ç†ä»»åŠ¡ä¸­ä¼˜äºéšæœºåˆå§‹åŒ–çš„ Transformer æ¨¡å‹ã€‚é‡å­æ¨¡å‹å±•ç°å‡ºæé«˜çš„å•å‚æ•°å­¦ä¹ æ•ˆç‡ï¼Œæœ€é«˜å¯è¾¾ç»å…¸æ¨¡å‹çš„äº”ä¸ªæ•°é‡çº§ï¼Œå‡¸æ˜¾äº† QNLP åœ¨ä½èµ„æºã€ç»“æ„æ•æ„Ÿåœºæ™¯ä¸‹çš„æ½œåŠ›ã€‚æ­¤å¤–ï¼Œç ”ç©¶æå‡ºçš„é›†ç¾¤åŒ–æ¶æ„ (cluster-based architecture) é€šè¿‡å°†é—¨å‚æ•°ä¸è¯ç°‡ç»‘å®šï¼Œæœ‰æ•ˆè§£å†³äº†ç”µè·¯éš”ç¦»é—®é¢˜å¹¶å¢å¼ºäº†æ³›åŒ–èƒ½åŠ›ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "Accepted, presented, and to appear in the Proceedings of the Quantum AI and NLP 2025 Conference",
      "pdf_url": "https://arxiv.org/pdf/2510.15972v1",
      "published_date": "2025-10-12 17:27:26 UTC",
      "updated_date": "2025-10-12 17:27:26 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T02:51:10.168636+00:00"
    },
    {
      "arxiv_id": "2510.10709v1",
      "title": "Missing Data Multiple Imputation for Tabular Q-Learning in Online RL",
      "title_zh": "åœ¨çº¿å¼ºåŒ–å­¦ä¹ ä¸­è¡¨æ ¼å‹ Q å­¦ä¹ çš„ç¼ºå¤±æ•°æ®å¤šé‡æ’è¡¥",
      "authors": [
        "Kyla Chasalow",
        "Skyler Wu",
        "Susan Murphy"
      ],
      "abstract": "Missing data in online reinforcement learning (RL) poses challenges compared to missing data in standard tabular data or in offline policy learning. The need to impute and act at each time step means that imputation cannot be put off until enough data exist to produce stable imputation models. It also means future data collection and learning depend on previous imputations. This paper proposes fully online imputation ensembles. We find that maintaining multiple imputation pathways may help balance the need to capture uncertainty under missingness and the need for efficiency in online settings. We consider multiple approaches for incorporating these pathways into learning and action selection. Using a Grid World experiment with various types of missingness, we provide preliminary evidence that multiple imputation pathways may be a useful framework for constructing simple and efficient online missing data RL methods.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†åœ¨çº¿å¼ºåŒ–å­¦ä¹ (Online RL)ä¸­ç¼ºå¤±æ•°æ®å¸¦æ¥çš„æŒ‘æˆ˜ï¼Œå¼ºè°ƒäº†å®æ—¶æ’è¡¥çš„å¿…è¦æ€§ä»¥åŠåç»­å­¦ä¹ å¯¹æ’è¡¥ç»“æœçš„ä¾èµ–ã€‚è®ºæ–‡æå‡ºäº†ä¸€ç§å®Œå…¨åœ¨çº¿çš„æ’è¡¥é›†æˆæ–¹æ³•(Fully online imputation ensembles)ï¼Œé€šè¿‡ç»´æŠ¤å¤šé‡æ’è¡¥è·¯å¾„(Multiple imputation pathways)æ¥å¹³è¡¡æ•æ‰ç¼ºå¤±å€¼ä¸‹çš„ä¸ç¡®å®šæ€§ä¸åœ¨çº¿è®¾ç½®ä¸­çš„æ•ˆç‡éœ€æ±‚ã€‚ä½œè€…è¿›ä¸€æ­¥ç ”ç©¶äº†å°†è¿™äº›è·¯å¾„æ•´åˆåˆ°å­¦ä¹ å’ŒåŠ¨ä½œé€‰æ‹©ä¸­çš„å¤šç§é€”å¾„ã€‚é€šè¿‡åœ¨åŒ…å«å¤šç§æ•°æ®ç¼ºå¤±ç±»å‹çš„Grid Worldå®éªŒä¸­è¿›è¡ŒéªŒè¯ï¼Œåˆæ­¥ç»“æœè¯æ˜äº†å¤šé‡æ’è¡¥è·¯å¾„æ˜¯æ„å»ºç®€å•ä¸”é«˜æ•ˆçš„åœ¨çº¿ç¼ºå¤±æ•°æ®å¼ºåŒ–å­¦ä¹ æ–¹æ³•çš„æœ‰æ•ˆæ¡†æ¶ã€‚",
      "categories": [
        "stat.ML",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "stat.ML",
      "comment": "Working paper",
      "pdf_url": "https://arxiv.org/pdf/2510.10709v1",
      "published_date": "2025-10-12 17:16:36 UTC",
      "updated_date": "2025-10-12 17:16:36 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T02:51:10.968833+00:00"
    },
    {
      "arxiv_id": "2510.15971v1",
      "title": "A Graph-Attentive LSTM Model for Malicious URL Detection",
      "title_zh": "ç”¨äºæ¶æ„ URL æ£€æµ‹çš„å›¾æ³¨æ„åŠ› LSTM æ¨¡å‹",
      "authors": [
        "Md. Ifthekhar Hossain",
        "Kazi Abdullah Al Arafat",
        "Bryce Shepard",
        "Kayd Craig",
        "Imtiaz Parvez"
      ],
      "abstract": "Malicious URLs pose significant security risks as they facilitate phishing attacks, distribute malware, and empower attackers to deface websites. Blacklist detection methods fail to identify new or obfuscated URLs because they depend on pre-existing patterns. This work presents a hybrid deep learning model named GNN-GAT-LSTM that combines Graph Neural Networks (GNNs) with Graph Attention Networks (GATs) and Long Short-Term Memory (LSTM) networks. The proposed architecture extracts both the structural and sequential patterns of the features from data. The model transforms URLs into graphs through a process where characters become nodes that connect through edges. It applies one-hot encoding to represent node features. The model received training and testing data from a collection of 651,191 URLs, which were classified into benign, phishing, defacement, and malware categories. The preprocessing stage included both feature engineering and data balancing techniques, which addressed the class imbalance issue to enhance model learning. The GNN-GAT-LSTM model achieved outstanding performance through its test accuracy of 0.9806 and its weighted F1-score of 0.9804. It showed excellent precision and recall performance across most classes, particularly for benign and defacement URLs. Overall, the model provides an efficient and scalable system for detecting malicious URLs while demonstrating strong potential for real-world cybersecurity applications.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§åä¸º GNN-GAT-LSTM çš„æ··åˆæ·±åº¦å­¦ä¹ æ¨¡å‹ï¼Œæ—¨åœ¨è§£å†³ä¼ ç»Ÿé»‘åå•æ£€æµ‹æ–¹æ³•éš¾ä»¥è¯†åˆ«æ–°å‹æˆ–æ··æ·†æ¶æ„ URL çš„å®‰å…¨æ€§æŒ‘æˆ˜ã€‚è¯¥æ¨¡å‹åˆ›æ–°æ€§åœ°ç»“åˆäº†å›¾ç¥ç»ç½‘ç»œ (GNNs)ã€å›¾æ³¨æ„åŠ›ç½‘ç»œ (GATs) å’Œé•¿çŸ­æœŸè®°å¿†ç½‘ç»œ (LSTM)ï¼Œèƒ½å¤ŸåŒæ—¶æå–æ•°æ®ä¸­çš„ç»“æ„ç‰¹å¾å’Œåºåˆ—æ¨¡å¼ã€‚åœ¨å¤„ç†è¿‡ç¨‹ä¸­ï¼ŒURL è¢«è½¬åŒ–ä¸ºä»¥å­—ç¬¦ä¸ºèŠ‚ç‚¹ã€é€šè¿‡è¾¹ç›¸äº’è¿æ¥çš„å›¾ç»“æ„ï¼Œå¹¶åˆ©ç”¨ One-hot ç¼–ç è¡¨ç¤ºèŠ‚ç‚¹ç‰¹å¾ï¼ŒåŒæ—¶è¾…ä»¥ç‰¹å¾å·¥ç¨‹å’Œæ•°æ®å¹³è¡¡æŠ€æœ¯ä»¥ä¼˜åŒ–æ¨¡å‹å­¦ä¹ ã€‚é€šè¿‡å¯¹åŒ…å« 651,191 æ¡ URL çš„å¤§è§„æ¨¡æ•°æ®é›†è¿›è¡Œå®éªŒï¼Œè¯¥æ¨¡å‹åœ¨è‰¯æ€§ã€ç½‘ç»œé’“é±¼ (Phishing)ã€ç½‘é¡µç¯¡æ”¹ (Defacement) å’Œæ¶æ„è½¯ä»¶åˆ†ç±»ä»»åŠ¡ä¸­è¡¨ç°ä¼˜å¼‚ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒGNN-GAT-LSTM è¾¾åˆ°äº† 0.9806 çš„æµ‹è¯•å‡†ç¡®ç‡å’Œ 0.9804 çš„åŠ æƒ F1-scoreï¼Œå°¤å…¶åœ¨è¯†åˆ«è‰¯æ€§å’Œç½‘é¡µç¯¡æ”¹ç±» URL æ–¹é¢è¡¨ç°å‡ºæé«˜çš„ç²¾ç¡®ç‡å’Œå¬å›ç‡ã€‚æ€»ä½“è€Œè¨€ï¼Œè¯¥ç ”ç©¶ä¸ºæ¶æ„ URL æ£€æµ‹æä¾›äº†ä¸€ä¸ªé«˜æ•ˆä¸”å¯æ‰©å±•çš„ç³»ç»Ÿï¼Œåœ¨çœŸå®ä¸–ç•Œçš„ç½‘ç»œå®‰å…¨åº”ç”¨ä¸­å…·æœ‰æ˜¾è‘—çš„æ½œåŠ›ã€‚",
      "categories": [
        "cs.CR",
        "cs.AI"
      ],
      "primary_category": "cs.CR",
      "comment": "Planned to be submitted",
      "pdf_url": "https://arxiv.org/pdf/2510.15971v1",
      "published_date": "2025-10-12 17:10:29 UTC",
      "updated_date": "2025-10-12 17:10:29 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T02:51:15.176061+00:00"
    },
    {
      "arxiv_id": "2510.10703v1",
      "title": "Adaptive Selection of Symbolic Languages for Improving LLM Logical Reasoning",
      "title_zh": "è‡ªé€‚åº”é€‰æ‹©ç¬¦å·è¯­è¨€ä»¥æå‡å¤§è¯­è¨€æ¨¡å‹çš„é€»è¾‘æ¨ç†èƒ½åŠ›",
      "authors": [
        "Xiangyu Wang",
        "Haocheng Yang",
        "Fengxiang Cheng",
        "Fenrong Liu"
      ],
      "abstract": "Large Language Models (LLMs) still struggle with complex logical reasoning. While previous works achieve remarkable improvements, their performance is highly dependent on the correctness of translating natural language (NL) problems into a symbolic language (SL). Though numerous works focusing on improving this translation accuracy, they only consider the similarity between the meaning of SL and NL, overlooking another crucial influencing factor, the selection of the target SL type itself. For example, first-order logic language specializes in logical reasoning with categorical syllogisms and complex quantifiers, while Boolean satisfiability formalism excels at representing constraint satisfaction like partial problems. To our knowledge, this is the first paper to claim and verify that different NL logical reasoning problem corresponds to different optimal SL formalization for translation. Based on this, we propose a methods to improve the logical reasoning performance of LLMs by adaptively selecting the most suitable SL for each problem prior to translation. Specifically, we leverage LLMs to select the target SL among first-order logic, logic programming and Boolean satisfiability and then translate the problem in NL to target SL expressions as well as employ the corresponding logical solver to derive the final answer. Experimental results on benchmarks show that our adaptive selection method significantly outperforms translating all into single SL and randomly selecting the SL. On a mixed dataset of these benchmarks, our approach achieves 96% accuracy, which improving performance by 25% compared to the second highest accuracy from the first-order logic translation.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†æå‡å¤§è¯­è¨€æ¨¡å‹(LLMs)å¤æ‚é€»è¾‘æ¨ç†èƒ½åŠ›çš„æ–¹æ³•ï¼ŒæŒ‡å‡ºä¼ ç»Ÿç¿»è¯‘è¿‡ç¨‹ä¸­å¿½ç•¥äº†ç›®æ ‡ç¬¦å·è¯­è¨€(SL)ç±»å‹é€‰æ‹©å¯¹æ¨ç†æ€§èƒ½çš„å…³é”®å½±å“ã€‚ç ”ç©¶è€…é¦–æ¬¡æå‡ºå¹¶éªŒè¯äº†ä¸åŒçš„è‡ªç„¶è¯­è¨€(NL)é€»è¾‘é—®é¢˜å¯¹åº”ä¸åŒçš„æœ€ä¼˜ç¬¦å·åŒ–è¡¨ç¤ºï¼Œä¾‹å¦‚ä¸€é˜¶é€»è¾‘(first-order logic)æ“…é•¿åˆ†ç±»ä¸‰æ®µè®ºï¼Œè€Œå¸ƒå°”å¯æ»¡è¶³æ€§(Boolean satisfiability)åˆ™æ›´é€‚åˆå¤„ç†çº¦æŸæ»¡è¶³é—®é¢˜ã€‚åŸºäºæ­¤ï¼Œè¯¥è®ºæ–‡æå‡ºäº†ä¸€ç§è‡ªé€‚åº”é€‰æ‹©æœºåˆ¶ï¼Œåˆ©ç”¨LLMsåœ¨ç¿»è¯‘å‰ä¸ºæ¯ä¸ªé—®é¢˜åŠ¨æ€åŒ¹é…æœ€åˆé€‚çš„SLï¼Œæ¶µç›–äº†ä¸€é˜¶é€»è¾‘ã€é€»è¾‘ç¼–ç¨‹(logic programming)å’Œå¸ƒå°”å¯æ»¡è¶³æ€§ç­‰ç±»å‹ã€‚åœ¨ç¡®å®šç›®æ ‡è¯­è¨€åï¼Œç³»ç»Ÿå°†é—®é¢˜è½¬åŒ–ä¸ºç¬¦å·è¡¨è¾¾å¼å¹¶é…åˆé€»è¾‘æ±‚è§£å™¨(logical solver)è·å–æœ€ç»ˆç­”æ¡ˆã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯¥æ–¹æ³•åœ¨æ··åˆåŸºå‡†æµ‹è¯•ä¸­è¾¾åˆ°äº†96%çš„å‡†ç¡®ç‡ï¼Œæ¯”è¡¨ç°æ¬¡ä¼˜çš„å•ä¸€SLç¿»è¯‘æ–¹æ³•æå‡äº†25%ã€‚è¿™ä¸€æˆæœè¯æ˜äº†é€šè¿‡è‡ªé€‚åº”é€‰æ‹©ç¬¦å·è¯­è¨€å¯ä»¥æ˜¾è‘—å¢å¼ºLLMsåœ¨å¤„ç†å¤šæ ·åŒ–é€»è¾‘ä»»åŠ¡æ—¶çš„å‡†ç¡®æ€§å’Œé²æ£’æ€§ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.10703v1",
      "published_date": "2025-10-12 17:04:01 UTC",
      "updated_date": "2025-10-12 17:04:01 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T02:51:32.763820+00:00"
    },
    {
      "arxiv_id": "2510.10702v1",
      "title": "Attention-Enhanced LSTM Modeling for Improved Temperature and Rainfall Forecasting in Bangladesh",
      "title_zh": "ç”¨äºæå‡ Bangladesh æ°”æ¸©ä¸é™æ°´é¢„æµ‹ç²¾åº¦çš„æ³¨æ„åŠ›å¢å¼ºå‹ LSTM å»ºæ¨¡",
      "authors": [
        "Usman Gani Joy",
        "Shahadat kabir",
        "Tasnim Niger"
      ],
      "abstract": "Accurate climate forecasting is vital for Bangladesh, a region highly susceptible to climate change impacts on temperature and rainfall. Existing models often struggle to capture long-range dependencies and complex temporal patterns in climate data. This study introduces an advanced Long Short-Term Memory (LSTM) model integrated with an attention mechanism to enhance the prediction of temperature and rainfall dynamics. Utilizing comprehensive datasets from 1901-2023, sourced from NASA's POWER Project for temperature and the Humanitarian Data Exchange for rainfall, the model effectively captures seasonal and long-term trends. It outperforms baseline models, including XGBoost, Simple LSTM, and GRU, achieving a test MSE of 0.2411 (normalized units), MAE of 0.3860 degrees C, R^2 of 0.9834, and NRMSE of 0.0370 for temperature, and MSE of 1283.67 mm^2, MAE of 22.91 mm, R^2 of 0.9639, and NRMSE of 0.0354 for rainfall on monthly forecasts. The model demonstrates improved robustness with only a 20 percent increase in MSE under simulated climate trends (compared to an approximately 2.2-fold increase in baseline models without trend features) and a 50 percent degradation under regional variations (compared to an approximately 4.8-fold increase in baseline models without enhancements). These results highlight the model's ability to improve forecasting precision and offer potential insights into the physical processes governing climate variability in Bangladesh, supporting applications in climate-sensitive sectors.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å­ŸåŠ æ‹‰å›½ææ˜“å—æ°”å€™å˜åŒ–å½±å“çš„ç°çŠ¶ï¼Œæå‡ºäº†ä¸€ç§é›†æˆæ³¨æ„åŠ›æœºåˆ¶çš„é«˜çº§é•¿çŸ­æœŸè®°å¿†ç½‘ç»œæ¨¡å‹ (Attention-Enhanced LSTM)ï¼Œæ—¨åœ¨æ˜¾è‘—æå‡æ°”æ¸©å’Œé™æ°´çš„é¢„æµ‹ç²¾åº¦ã€‚é€šè¿‡åˆ†æ 1901-2023 å¹´æ¥è‡ª NASA POWER å’Œ Humanitarian Data Exchange çš„å¤šç»´æ•°æ®é›†ï¼Œè¯¥æ¨¡å‹æœ‰æ•ˆæ•æ‰äº†æ°”å€™æ•°æ®ä¸­çš„å­£èŠ‚æ€§è¶‹åŠ¿ä¸ long-range dependenciesã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ¨¡å‹åœ¨æ°”æ¸©é¢„æµ‹ (R^2 è¾¾ 0.9834) ä¸é™æ°´é¢„æµ‹ (R^2 è¾¾ 0.9639) ä»»åŠ¡ä¸­å‡ä¼˜äº XGBoostã€Simple LSTM å’Œ GRU ç­‰åŸºå‡†æ¨¡å‹ã€‚åœ¨æ¨¡æ‹Ÿæ°”å€™è¶‹åŠ¿æµ‹è¯•ä¸­ï¼Œè¯¥æ¨¡å‹çš„ MSE ä»…å¢åŠ  20%ï¼Œè¿œä½äºåŸºå‡†æ¨¡å‹çš„å¢é•¿å¹…åº¦ï¼Œå±•ç°å‡ºå“è¶Šçš„ robustness ä¸æ³›åŒ–èƒ½åŠ›ã€‚è¿™é¡¹ç ”ç©¶ä¸ä»…ä¸ºç†è§£å­ŸåŠ æ‹‰å›½æ°”å€™å˜ç‡çš„ç‰©ç†è¿‡ç¨‹æä¾›äº†æ–°è§è§£ï¼Œä¹Ÿä¸ºå½“åœ°å†œä¸šç­‰æ°”å€™æ•æ„Ÿéƒ¨é—¨çš„å†³ç­–æ”¯æŒæä¾›äº†é«˜å¯é æ€§çš„å·¥å…·ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.10702v1",
      "published_date": "2025-10-12 17:03:45 UTC",
      "updated_date": "2025-10-12 17:03:45 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T02:51:32.276723+00:00"
    },
    {
      "arxiv_id": "2510.10701v1",
      "title": "Extended Triangular Method: A Generalized Algorithm for Contradiction Separation Based Automated Deduction",
      "title_zh": "æ‰©å±•ä¸‰è§’æ³•ï¼šä¸€ç§åŸºäºçŸ›ç›¾åˆ†ç¦»çš„è‡ªåŠ¨æ¼”ç»å¹¿ä¹‰ç®—æ³•",
      "authors": [
        "Yang Xu",
        "Shuwei Chen",
        "Jun Liu",
        "Feng Cao",
        "Xingxing He"
      ],
      "abstract": "Automated deduction lies at the core of Artificial Intelligence (AI), underpinning theorem proving, formal verification, and logical reasoning. Despite decades of progress, reconciling deductive completeness with computational efficiency remains an enduring challenge. Traditional reasoning calculi, grounded in binary resolution, restrict inference to pairwise clause interactions and thereby limit deductive synergy among multiple clauses. The Contradiction Separation Extension (CSE) framework, introduced in 2018, proposed a dynamic multi-clause reasoning theory that redefined logical inference as a process of contradiction separation rather than sequential resolution. While that work established the theoretical foundation, its algorithmic realization remained unformalized and unpublished. This work presents the Extended Triangular Method (ETM), a generalized contradiction-construction algorithm that formalizes and extends the internal mechanisms of contradiction separation. The ETM unifies multiple contradiction-building strategies, including the earlier Standard Extension method, within a triangular geometric framework that supports flexible clause interaction and dynamic synergy. ETM serves as the algorithmic core of several high-performance theorem provers, CSE, CSE-E, CSI-E, and CSI-Enig, whose competitive results in standard first-order benchmarks (TPTP problem sets and CASC 2018-2015) empirically validate the effectiveness and generality of the proposed approach. By bridging theoretical abstraction and operational implementation, ETM advances the contradiction separation paradigm into a generalized, scalable, and practically competitive model for automated reasoning, offering new directions for future research in logical inference and theorem proving.",
      "tldr_zh": "è¯¥è®ºæ–‡æå‡ºäº†Extended Triangular Method (ETM)ï¼Œè¿™æ˜¯ä¸€ç§ä¸ºåŸºäºçŸ›ç›¾åˆ†ç¦»(Contradiction Separation)çš„è‡ªåŠ¨æ¼”ç»(Automated Deduction)è®¾è®¡çš„å¹¿ä¹‰ç®—æ³•ã€‚ä¸ºäº†å…‹æœä¼ ç»ŸäºŒå…ƒå½’ç»“(Binary Resolution)åœ¨å¤šå­å¥äº¤äº’å’Œæ¼”ç»ååŒæ–¹é¢çš„å±€é™æ€§ï¼ŒETMåœ¨ä¸‰è§’å½¢å‡ ä½•æ¡†æ¶å†…ç»Ÿä¸€äº†å¤šç§çŸ›ç›¾æ„å»ºç­–ç•¥ï¼Œå¹¶æ­£å¼åŒ–äº†Contradiction Separation Extension (CSE)ç†è®ºçš„å†…éƒ¨æœºåˆ¶ã€‚ä½œä¸ºCSEã€CSE-Eå’ŒCSI-Enigç­‰å¤šä¸ªé«˜æ€§èƒ½å®šç†è¯æ˜å™¨çš„æ ¸å¿ƒç®—æ³•ï¼ŒETMåœ¨TPTPåŸºå‡†æµ‹è¯•å’ŒCASCç«èµ›ä¸­å–å¾—äº†å…·æœ‰ç«äº‰åŠ›çš„ç»“æœï¼Œå®è¯äº†å…¶æœ‰æ•ˆæ€§ä¸é€šç”¨æ€§ã€‚è¯¥ç ”ç©¶é€šè¿‡å°†ç†è®ºæŠ½è±¡è½¬åŒ–ä¸ºå¯æ“ä½œçš„ç®—æ³•å®ç°ï¼Œå°†çŸ›ç›¾åˆ†ç¦»èŒƒå¼æå‡ä¸ºä¸€ç§å¯æ‰©å±•ä¸”å…·å¤‡å®è·µç«äº‰åŠ›çš„æ¨ç†æ¨¡å‹ï¼Œä¸ºæœªæ¥é€»è¾‘æ¨ç†å’Œå®šç†è¯æ˜çš„ç ”ç©¶æä¾›äº†æ–°æ–¹å‘ã€‚",
      "categories": [
        "cs.AI",
        "cs.LO"
      ],
      "primary_category": "cs.AI",
      "comment": "38 pages, 8 figures",
      "pdf_url": "https://arxiv.org/pdf/2510.10701v1",
      "published_date": "2025-10-12 17:03:33 UTC",
      "updated_date": "2025-10-12 17:03:33 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T02:51:31.669293+00:00"
    },
    {
      "arxiv_id": "2510.15970v1",
      "title": "Predict Training Data Quality via Its Geometry in Metric Space",
      "title_zh": "åŸºäºåº¦é‡ç©ºé—´å‡ ä½•ç»“æ„çš„è®­ç»ƒæ•°æ®è´¨é‡é¢„æµ‹",
      "authors": [
        "Yang Ba",
        "Mohammad Sadeq Abolhasani",
        "Rong Pan"
      ],
      "abstract": "High-quality training data is the foundation of machine learning and artificial intelligence, shaping how models learn and perform. Although much is known about what types of data are effective for training, the impact of the data's geometric structure on model performance remains largely underexplored. We propose that both the richness of representation and the elimination of redundancy within training data critically influence learning outcomes. To investigate this, we employ persistent homology to extract topological features from data within a metric space, thereby offering a principled way to quantify diversity beyond entropy-based measures. Our findings highlight persistent homology as a powerful tool for analyzing and enhancing the training data that drives AI systems.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†è®­ç»ƒæ•°æ®çš„å‡ ä½•ç»“æ„å¯¹æ¨¡å‹æ€§èƒ½çš„å½±å“ï¼Œæå‡ºäº†é€šè¿‡åº¦é‡ç©ºé—´(Metric Space)ä¸­çš„å‡ ä½•ç‰¹å¾æ¥é¢„æµ‹è®­ç»ƒæ•°æ®è´¨é‡çš„æ–¹æ³•ã€‚ä½œè€…æŒ‡å‡ºï¼Œè¡¨ç¤ºçš„ä¸°å¯Œæ€§(Richness of Representation)å’Œå†—ä½™çš„æ¶ˆé™¤(Elimination of Redundancy)å¯¹å­¦ä¹ ç»“æœå…·æœ‰è‡³å…³é‡è¦çš„å½±å“ã€‚ä¸ºæ·±å…¥æ¢ç©¶è¿™ä¸€é¢†åŸŸï¼Œç ”ç©¶åˆ©ç”¨æŒç»­åŒè°ƒ(Persistent Homology)ä»åº¦é‡ç©ºé—´ä¸­çš„æ•°æ®æå–æ‹“æ‰‘ç‰¹å¾(Topological Features)ï¼Œä»è€Œæä¾›äº†ä¸€ç§è¶…è¶ŠåŸºäºç†µçš„åº¦é‡(Entropy-based Measures)çš„æ–¹æ³•æ¥é‡åŒ–æ•°æ®å¤šæ ·æ€§(Diversity)ã€‚ç ”ç©¶ç»“æœå¼ºè°ƒï¼ŒæŒç»­åŒè°ƒæ˜¯åˆ†æå’Œå¢å¼ºé©±åŠ¨äººå·¥æ™ºèƒ½ç³»ç»Ÿè®­ç»ƒæ•°æ®çš„æœ‰æ•ˆå·¥å…·ã€‚è¯¥å·¥ä½œä¸ºé€šè¿‡å‡ ä½•ä¸æ‹“æ‰‘è§†è§’ä¼˜åŒ–æœºå™¨å­¦ä¹ æ•°æ®é›†è´¨é‡æä¾›äº†å…¨æ–°çš„ç†è®ºæ¡†æ¶å’Œå®è·µæŒ‡å¯¼ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "Accepted to the NeurIPS 2025 Workshop on New Perspectives in Graph Machine Learning",
      "pdf_url": "https://arxiv.org/pdf/2510.15970v1",
      "published_date": "2025-10-12 16:59:28 UTC",
      "updated_date": "2025-10-12 16:59:28 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T02:51:32.973705+00:00"
    },
    {
      "arxiv_id": "2510.10693v1",
      "title": "High-Dimensional Learning Dynamics of Quantized Models with Straight-Through Estimator",
      "title_zh": "åŸºäºç›´é€šä¼°è®¡å™¨çš„é‡åŒ–æ¨¡å‹é«˜ç»´å­¦ä¹ åŠ¨åŠ›å­¦",
      "authors": [
        "Yuma Ichikawa",
        "Shuhei Kashiwamura",
        "Ayaka Sakata"
      ],
      "abstract": "Quantized neural network training optimizes a discrete, non-differentiable objective. The straight-through estimator (STE) enables backpropagation through surrogate gradients and is widely used. While previous studies have primarily focused on the properties of surrogate gradients and their convergence, the influence of quantization hyperparameters, such as bit width and quantization range, on learning dynamics remains largely unexplored. We theoretically show that in the high-dimensional limit, STE dynamics converge to a deterministic ordinary differential equation. This reveals that STE training exhibits a plateau followed by a sharp drop in generalization error, with plateau length depending on the quantization range. A fixed-point analysis quantifies the asymptotic deviation from the unquantized linear model. We also extend analytical techniques for stochastic gradient descent to nonlinear transformations of weights and inputs.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†é‡åŒ–ç¥ç»ç½‘ç»œ(Quantized Neural Networks)åœ¨ç¦»æ•£ä¸”ä¸å¯å¾®ç›®æ ‡ä¸‹çš„ä¼˜åŒ–é—®é¢˜ï¼Œé‡ç‚¹åˆ†æäº†ç›´é€šä¼°è®¡å™¨(Straight-Through Estimator, STE)åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­çš„é«˜ç»´å­¦ä¹ åŠ¨åŠ›å­¦ã€‚ç ”ç©¶ä»ç†è®ºä¸Šè¯æ˜ï¼Œåœ¨æé™é«˜ç»´æ¡ä»¶ä¸‹ï¼ŒSTEçš„åŠ¨åŠ›å­¦è¿‡ç¨‹ä¼šæ”¶æ•›äºä¸€ä¸ªç¡®å®šçš„å¸¸å¾®åˆ†æ–¹ç¨‹(Ordinary Differential Equation, ODE)ã€‚é€šè¿‡åˆ†æå‘ç°ï¼ŒSTEè®­ç»ƒä¼šç»å†ä¸€ä¸ªå¹³å°æœŸ(Plateau)ï¼Œéšåæ³›åŒ–è¯¯å·®(Generalization Error)å‡ºç°æ˜¾è‘—ä¸‹é™ï¼Œä¸”è¯¥å¹³å°æœŸçš„é•¿åº¦ä¸»è¦å—é‡åŒ–èŒƒå›´(Quantization Range)ç­‰è¶…å‚æ•°çš„å½±å“ã€‚æ­¤å¤–ï¼Œç ”ç©¶åˆ©ç”¨å®šç‚¹åˆ†æ(Fixed-point Analysis)é‡åŒ–äº†è®­ç»ƒç»“æœä¸æœªé‡åŒ–çº¿æ€§æ¨¡å‹ä¹‹é—´çš„æ¸è¿‘åå·®ï¼Œå¹¶å°†éšæœºæ¢¯åº¦ä¸‹é™(Stochastic Gradient Descent)çš„åˆ†ææŠ€æœ¯æˆåŠŸæ‰©å±•åˆ°æƒé‡å’Œè¾“å…¥çš„éçº¿æ€§å˜æ¢ã€‚è¿™äº›å‘ç°æ·±å…¥æ­ç¤ºäº†ä½å®½å’Œé‡åŒ–èŒƒå›´å¯¹æ¨¡å‹å­¦ä¹ åŠ¨æ€çš„å½±å“ï¼Œä¸ºç†è§£å’Œä¼˜åŒ–é‡åŒ–æ¨¡å‹è®­ç»ƒæä¾›äº†é‡è¦çš„ç†è®ºä¾æ®ã€‚",
      "categories": [
        "stat.ML",
        "cond-mat.dis-nn",
        "cs.AI",
        "cs.LG",
        "math.ST"
      ],
      "primary_category": "stat.ML",
      "comment": "27 pages, 14 figures",
      "pdf_url": "https://arxiv.org/pdf/2510.10693v1",
      "published_date": "2025-10-12 16:43:46 UTC",
      "updated_date": "2025-10-12 16:43:46 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T02:51:33.375814+00:00"
    },
    {
      "arxiv_id": "2510.15969v1",
      "title": "LinearizeLLM: An Agent-Based Framework for LLM-Driven Exact Linear Reformulation of Nonlinear Optimization Problems",
      "title_zh": "LinearizeLLMï¼šåŸºäºæ™ºèƒ½ä½“çš„å¤§è¯­è¨€æ¨¡å‹é©±åŠ¨éçº¿æ€§ä¼˜åŒ–é—®é¢˜ç²¾ç¡®çº¿æ€§é‡æ„æ¡†æ¶",
      "authors": [
        "Paul-Niklas Ken Kandora",
        "Simon Caspar Zeller",
        "Aaron Jeremias Elsing",
        "Elena Kuss",
        "Steffen Rebennack"
      ],
      "abstract": "Reformulating nonlinear optimization problems is largely manual and expertise-intensive, yet it remains essential for solving such problems with linear optimization solvers or applying special-purpose algorithms. We introduce \\textit{LinearizeLLM}, an agent-based framework that solves this task by leveraging Large Language Models (LLMs). The framework assigns each nonlinear pattern to a \\textit{reformulation agent} that is explicitly instructed to derive an exact linear reformulation for its nonlinearity pattern, for instance, absolute-value terms or bilinear products of decision variables. The agents then coordinate to assemble a solver-ready linear model equivalent to the original problem. To benchmark the approach, we create a dataset of 20 real-world nonlinear optimization problems derived from the established ComplexOR dataset of linear optimization problems. We evaluate our approach with several LLMs. Our results indicate that specialized LLM agents can automate linearization tasks, opening a path toward fully conversational modeling pipelines for nonlinear optimization.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†LinearizeLLMï¼Œè¿™æ˜¯ä¸€ä¸ªåŸºäºæ™ºèƒ½ä½“(Agent-Based)çš„æ¡†æ¶ï¼Œæ—¨åœ¨åˆ©ç”¨å¤§è¯­è¨€æ¨¡å‹(LLMs)å®ç°Nonlinear Optimization Problemsçš„è‡ªåŠ¨ç²¾ç¡®çº¿æ€§é‡æ„ã€‚è¯¥æ¡†æ¶å°†ä¸åŒçš„éçº¿æ€§æ¨¡å¼åˆ†é…ç»™ç‰¹å®šçš„Reformulation Agentï¼Œä¾‹å¦‚å¤„ç†ç»å¯¹å€¼é¡¹æˆ–å†³ç­–å˜é‡çš„Bilinear Productsï¼Œä»¥æ¨å¯¼å‡ºç²¾ç¡®çš„çº¿æ€§ç­‰æ•ˆè¡¨è¾¾ã€‚è¿™äº›æ™ºèƒ½ä½“é€šè¿‡ååŒå·¥ä½œï¼Œå°†åŸå§‹é—®é¢˜è½¬åŒ–ä¸ºå¯ä¾›æ±‚è§£å™¨ä½¿ç”¨çš„Solver-Readyçº¿æ€§æ¨¡å‹ã€‚ç ”ç©¶äººå‘˜åŸºäºComplexORæ•°æ®é›†æ„å»ºäº†åŒ…å«20ä¸ªçœŸå®ä¸–ç•Œé—®é¢˜çš„åŸºå‡†æµ‹è¯•ï¼Œå¹¶å¯¹å¤šç§LLMsè¿›è¡Œäº†è¯„ä¼°ã€‚å®éªŒç»“æœè¯æ˜ï¼Œä¸“é—¨çš„LLMæ™ºèƒ½ä½“èƒ½å¤ŸæˆåŠŸè‡ªåŠ¨åŒ–Linearizationä»»åŠ¡ï¼Œä¸ºå®ç°Nonlinear Optimizationçš„å…¨å¯¹è¯å¼å»ºæ¨¡æµç¨‹å¥ å®šäº†åŸºç¡€ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.15969v1",
      "published_date": "2025-10-12 16:43:21 UTC",
      "updated_date": "2025-10-12 16:43:21 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T02:51:47.959852+00:00"
    },
    {
      "arxiv_id": "2510.10689v1",
      "title": "OmniVideoBench: Towards Audio-Visual Understanding Evaluation for Omni MLLMs",
      "title_zh": "OmniVideoBenchï¼šé¢å‘å…¨èƒ½å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹çš„è§†å¬ç†è§£è¯„æµ‹",
      "authors": [
        "Caorui Li",
        "Yu Chen",
        "Yiyan Ji",
        "Jin Xu",
        "Zhenyu Cui",
        "Shihao Li",
        "Yuanxing Zhang",
        "Jiafu Tang",
        "Zhenghao Song",
        "Dingling Zhang",
        "Ying He",
        "Haoxiang Liu",
        "Yuxuan Wang",
        "Qiufeng Wang",
        "Zhenhe Wu",
        "Jiehui Luo",
        "Zhiyu Pan",
        "Weihao Xie",
        "Chenchen Zhang",
        "Zhaohui Wang",
        "Jiayi Tian",
        "Yanghai Wang",
        "Zhe Cao",
        "Minxin Dai",
        "Ke Wang",
        "Runzhe Wen",
        "Yinghao Ma",
        "Yaning Pan",
        "Sungkyun Chang",
        "Termeh Taheri",
        "Haiwen Xia",
        "Christos Plachouras",
        "Emmanouil Benetos",
        "Yizhi Li",
        "Ge Zhang",
        "Jian Yang",
        "Tianhao Peng",
        "Zili Wang",
        "Minghao Liu",
        "Junran Peng",
        "Zhaoxiang Zhang",
        "Jiaheng Liu"
      ],
      "abstract": "Recent advances in multimodal large language models (MLLMs) have demonstrated substantial potential in video understanding. However, existing benchmarks fail to comprehensively evaluate synergistic reasoning capabilities across audio and visual modalities, often neglecting either one of the modalities or integrating them in a logically inconsistent manner. To bridge this gap, we introduce OmniVideoBench, a large-scale and rigorously designed benchmark dedicated to assessing synergistic audio-visual understanding, with a strong emphasis on modality complementarity and logical consistency. Specifically, OmniVideoBench comprises 1000 high-quality question-answer(QA) pairs, each annotated with step-by-step reasoning traces, derived from 628 diverse videos ranging from several seconds to 30 minutes, and manually verified to guarantee complete correctness and uniqueness. Moreover, OmniVideoBench encompasses 13 carefully designed question types, covering temporal reasoning, spatial localization, counting, causal inference, summarization, and beyond, thereby capturing the essential challenges of video understanding. Evaluation of multiple MLLMs on OmniVideoBench reveals a pronounced gap between model performance and human reasoning, with open-source models lagging significantly behind their closed-source counterparts, underscoring the inherent difficulty of genuine audio-visual reasoning. We will release OmniVideoBench to foster the development of MLLMs with stronger and more generalizable reasoning capabilities.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¨å‡ºäº†OmniVideoBenchï¼Œè¿™æ˜¯ä¸€ä¸ªå¤§è§„æ¨¡ä¸”ä¸¥å¯†è®¾è®¡çš„åŸºå‡†æµ‹è¯•ï¼Œæ—¨åœ¨è¯„ä¼°å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨ååŒè§†å¬ç†è§£ï¼ˆsynergistic audio-visual understandingï¼‰æ–¹é¢çš„è¡¨ç°ã€‚é’ˆå¯¹ç°æœ‰åŸºå‡†åœ¨æ¨¡æ€äº’è¡¥æ€§å’Œé€»è¾‘ä¸€è‡´æ€§æ–¹é¢çš„ä¸è¶³ï¼ŒOmniVideoBenchåŒ…å«ä»628ä¸ªæ¶µç›–æ•°ç§’è‡³30åˆ†é’Ÿä¸ç­‰çš„è§†é¢‘ä¸­æå–çš„1000ä¸ªé«˜è´¨é‡é—®ç­”å¯¹ï¼Œå¹¶ä¸ºæ¯ä¸ªé—®ç­”å¯¹æä¾›äº†æ ‡æ³¨çš„é€æ­¥æ¨ç†è½¨è¿¹ï¼ˆstep-by-step reasoning tracesï¼‰ã€‚è¯¥åŸºå‡†æ¶µç›–äº†åŒ…æ‹¬æ—¶é—´æ¨ç†ã€ç©ºé—´å®šä½ã€è®¡æ•°ã€å› æœæ¨ç†å’Œæ€»ç»“åœ¨å†…çš„13ç§ç²¾ç»†è®¾è®¡çš„é¢˜å‹ï¼Œå…¨é¢æ•æ‰äº†è§†é¢‘ç†è§£çš„æ ¸å¿ƒæŒ‘æˆ˜ã€‚å¯¹å¤šç§MLLMsçš„è¯„ä¼°æ­ç¤ºäº†å½“å‰æ¨¡å‹æ€§èƒ½ä¸äººç±»æ¨ç†ä¹‹é—´å­˜åœ¨æ˜¾è‘—å·®è·ï¼Œä¸”å¼€æºæ¨¡å‹æ˜æ˜¾è½åäºé—­æºæ¨¡å‹ï¼Œçªæ˜¾äº†å®ç°çœŸå®è§†å¬æ¨ç†çš„å†…åœ¨éš¾åº¦ã€‚è¯¥åŸºå‡†çš„å‘å¸ƒå°†æœ‰åŠ©äºä¿ƒè¿›å…·æœ‰æ›´å¼ºã€æ›´é€šç”¨æ¨ç†èƒ½åŠ›çš„MLLMsçš„è¿›ä¸€æ­¥å¼€å‘ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.10689v1",
      "published_date": "2025-10-12 16:34:00 UTC",
      "updated_date": "2025-10-12 16:34:00 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T02:51:52.784018+00:00"
    },
    {
      "arxiv_id": "2510.10687v1",
      "title": "LSZone: A Lightweight Spatial Information Modeling Architecture for Real-time In-car Multi-zone Speech Separation",
      "title_zh": "LSZoneï¼šé¢å‘å®æ—¶è½¦å†…å¤šåŒºåŸŸè¯­éŸ³åˆ†ç¦»çš„è½»é‡çº§ç©ºé—´ä¿¡æ¯å»ºæ¨¡æ¶æ„",
      "authors": [
        "Jun Chen",
        "Shichao Hu",
        "Jiuxin Lin",
        "Wenjie Li",
        "Zihan Zhang",
        "Xingchen Li",
        "JinJiang Liu",
        "Longshuai Xiao",
        "Chao Weng",
        "Lei Xie",
        "Zhiyong Wu"
      ],
      "abstract": "In-car multi-zone speech separation, which captures voices from different speech zones, plays a crucial role in human-vehicle interaction. Although previous SpatialNet has achieved notable results, its high computational cost still hinders real-time applications in vehicles. To this end, this paper proposes LSZone, a lightweight spatial information modeling architecture for real-time in-car multi-zone speech separation. We design a spatial information extraction-compression (SpaIEC) module that combines Mel spectrogram and Interaural Phase Difference (IPD) to reduce computational burden while maintaining performance. Additionally, to efficiently model spatial information, we introduce an extremely lightweight Conv-GRU crossband-narrowband processing (CNP) module. Experimental results demonstrate that LSZone, with a complexity of 0.56G MACs and a real-time factor (RTF) of 0.37, delivers impressive performance in complex noise and multi-speaker scenarios.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†LSZoneï¼Œä¸€ç§ç”¨äºå®æ—¶è½¦è½½å¤šåŒºè¯­éŸ³åˆ†ç¦»çš„è½»é‡çº§ç©ºé—´ä¿¡æ¯å»ºæ¨¡æ¶æ„ï¼Œæ—¨åœ¨è§£å†³SpatialNetå› è®¡ç®—å¼€é”€è¿‡é«˜è€Œéš¾ä»¥åœ¨è½¦è¾†ä¸­å®æ—¶åº”ç”¨çš„é—®é¢˜ã€‚ä¸ºäº†åœ¨ç»´æŒæ€§èƒ½çš„åŒæ—¶é™ä½è®¡ç®—è´Ÿæ‹…ï¼Œç ”ç©¶è€…è®¾è®¡äº†ç©ºé—´ä¿¡æ¯æå–-å‹ç¼©(SpaIEC)æ¨¡å—ï¼Œé€šè¿‡ç»“åˆMel spectrogramå’ŒIPDæ¥æœ‰æ•ˆå¤„ç†ç©ºé—´ç‰¹å¾ã€‚æ­¤å¤–ï¼Œè¯¥æ¶æ„å¼•å…¥äº†æè½»é‡çº§çš„Conv-GRUè·¨é¢‘å¸¦-çª„é¢‘å¸¦å¤„ç†(CNP)æ¨¡å—ï¼Œå®ç°äº†é«˜æ•ˆçš„ç©ºé—´å»ºæ¨¡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒLSZoneåœ¨å¤æ‚åº¦ä»…ä¸º0.56G MACsä¸”RTFä¸º0.37çš„æƒ…å†µä¸‹ï¼Œåœ¨å¤æ‚å™ªå£°å’Œå¤šå‘è¨€äººåœºæ™¯ä¸­å‡å±•ç°å‡ºä¼˜å¼‚çš„æ€§èƒ½ã€‚è¯¥æ–¹æ¡ˆä¸ºè½¦è½½ç¯å¢ƒä¸‹çš„å®æ—¶äººæœºäº¤äº’æä¾›äº†å…¼å…·è½»é‡åŒ–ä¸é«˜æ€§èƒ½çš„è¯­éŸ³åˆ†ç¦»æŠ€æœ¯æ”¯æ’‘ã€‚",
      "categories": [
        "cs.SD",
        "cs.AI"
      ],
      "primary_category": "cs.SD",
      "comment": "submitted to ICASSP 2026",
      "pdf_url": "https://arxiv.org/pdf/2510.10687v1",
      "published_date": "2025-10-12 16:31:05 UTC",
      "updated_date": "2025-10-12 16:31:05 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T02:51:54.568476+00:00"
    },
    {
      "arxiv_id": "2510.12831v2",
      "title": "MTSQL-R1: Towards Long-Horizon Multi-Turn Text-to-SQL via Agentic Training",
      "title_zh": "MTSQL-R1ï¼šåŸºäºæ™ºèƒ½ä½“åŒ–è®­ç»ƒçš„é•¿æ—¶ç¨‹å¤šè½® Text-to-SQL",
      "authors": [
        "Taicheng Guo",
        "Hai Wang",
        "ChaoChun Liu",
        "Mohsen Golalikhani",
        "Xin Chen",
        "Xiangliang Zhang",
        "Chandan K. Reddy"
      ],
      "abstract": "Multi-turn Text-to-SQL aims to translate a user's conversational utterances into executable SQL while preserving dialogue coherence and grounding to the target schema. However, most existing systems only regard this task as a simple text translation task and follow a short-horizon paradigm, generating a query per turn without execution, explicit verification, and refinement, which leads to non-executable or incoherent outputs. We present MTSQL-R1, an agentic training framework for long-horizon multi-turn Text-to-SQL. We cast the task as a Markov Decision Process (MDP) in which an agent interacts with (i) a database for execution feedback and (ii) a persistent dialogue memory for coherence verification, performing an iterative propose to execute -> verify -> refine cycle until all checks pass. Experiments on COSQL and SPARC demonstrate that MTSQL-R1 consistently outperforms strong baselines, highlighting the importance of environment-driven verification and memory-guided refinement for conversational semantic parsing. Full recipes (including code, trained models, logs, reasoning trajectories, etc.) will be released after the internal review to contribute to community research.",
      "tldr_zh": "ç°æœ‰çš„å¤šè½® Text-to-SQL ç³»ç»Ÿé€šå¸¸å°†å…¶è§†ä¸ºç®€å•çš„æ–‡æœ¬ç¿»è¯‘ä»»åŠ¡ï¼Œä¸”éµå¾ªçŸ­æ—¶åŸŸï¼ˆshort-horizonï¼‰èŒƒå¼ï¼Œå¯¼è‡´ç”Ÿæˆçš„æŸ¥è¯¢å¾€å¾€ä¸å¯æ‰§è¡Œæˆ–ç¼ºä¹è¿è´¯æ€§ã€‚è¯¥ç ”ç©¶æå‡ºäº† MTSQL-R1ï¼Œè¿™æ˜¯ä¸€ç§ä¸“ä¸ºé•¿æ—¶åŸŸï¼ˆlong-horizonï¼‰å¤šè½® Text-to-SQL è®¾è®¡çš„æ™ºèƒ½ä½“è®­ç»ƒæ¡†æ¶ã€‚è¯¥æ¡†æ¶å°†ä»»åŠ¡å»ºæ¨¡ä¸ºé©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹ï¼ˆMarkov Decision Process, MDPï¼‰ï¼Œä½¿æ™ºèƒ½ä½“èƒ½å¤Ÿä¸æ•°æ®åº“äº¤äº’ä»¥è·å–æ‰§è¡Œåé¦ˆï¼Œå¹¶ç»“åˆæŒä¹…åŒ–å¯¹è¯è®°å¿†è¿›è¡Œè¿è´¯æ€§æ ¡éªŒã€‚é€šè¿‡æ‰§è¡Œâ€œæè®®-æ‰§è¡Œ-éªŒè¯-ä¼˜åŒ–â€çš„è¿­ä»£å¾ªç¯ï¼ŒMTSQL-R1 ç¡®ä¿äº†è¾“å‡ºçš„å‡†ç¡®æ€§ä¸é€»è¾‘ä¸€è‡´æ€§ã€‚åœ¨ COSQL å’Œ SPARC æ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼ŒMTSQL-R1 çš„è¡¨ç°æŒç»­ä¼˜äºç°æœ‰çš„å¼ºåŸºçº¿æ¨¡å‹ã€‚è¯¥ç ”ç©¶å¼ºè°ƒäº†ç¯å¢ƒé©±åŠ¨çš„éªŒè¯å’Œè®°å¿†å¼•å¯¼çš„ä¼˜åŒ–åœ¨å¯¹è¯å¼è¯­ä¹‰è§£æä»»åŠ¡ä¸­çš„é‡è¦ä½œç”¨ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.DB",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.12831v2",
      "published_date": "2025-10-12 16:12:05 UTC",
      "updated_date": "2025-12-31 23:38:51 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T02:51:58.886029+00:00"
    },
    {
      "arxiv_id": "2510.10675v2",
      "title": "Simpliflow: A Lightweight Open-Source Framework for Rapid Creation and Deployment of Generative Agentic AI Workflows",
      "title_zh": "Simpliflowï¼šç”¨äºå¿«é€Ÿæ„å»ºä¸éƒ¨ç½²ç”Ÿæˆå¼æ™ºèƒ½ä½“ AI å·¥ä½œæµçš„è½»é‡çº§å¼€æºæ¡†æ¶",
      "authors": [
        "Deven Panchal"
      ],
      "abstract": "Generative Agentic AI systems are emerging as a powerful paradigm for automating complex, multi-step tasks. However, many existing frameworks for building these systems introduce significant complexity, a steep learning curve, and substantial boilerplate code, hindering rapid prototyping and deployment. This paper introduces simpliflow, a lightweight, open-source Python framework designed to address these challenges. simpliflow enables the rapid development and orchestration of linear, deterministic agentic workflows through a declarative, JSON-based configuration. Its modular architecture decouples agent management, workflow execution, and post-processing, promoting ease of use and extensibility. By integrating with LiteLLM, it supports over 100 Large Language Models (LLMs) out-of-the-box. We present the architecture, operational flow, and core features of simpliflow, demonstrating its utility through diverse use cases ranging from software development simulation to real-time system interaction. A comparative analysis with prominent frameworks like LangChain and AutoGen highlights simpliflow's unique position as a tool optimized for simplicity, control, and speed in deterministic workflow environments.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† simpliflowï¼Œä¸€ä¸ªè½»é‡çº§çš„å¼€æº Python æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³ç°æœ‰ Generative Agentic AI å¼€å‘æ¡†æ¶è¿‡äºå¤æ‚ä¸”å­¦ä¹ æ›²çº¿é™¡å³­çš„é—®é¢˜ã€‚è¯¥æ¡†æ¶é€šè¿‡å£°æ˜å¼çš„ JSON-based é…ç½®å®ç°çº¿æ€§ã€ç¡®å®šæ€§å·¥ä½œæµçš„å¿«é€Ÿå¼€å‘ä¸ç¼–æ’ï¼Œå…¶æ¨¡å—åŒ–æ¶æ„æœ‰æ•ˆè§£è€¦äº† agent ç®¡ç†ã€å·¥ä½œæµæ‰§è¡Œä¸åå¤„ç†è¿‡ç¨‹ã€‚é€šè¿‡é›†æˆ LiteLLMï¼Œsimpliflow åŸç”Ÿæ”¯æŒè¶…è¿‡ 100 ç§ Large Language Models (LLMs)ï¼Œæ˜¾è‘—æå‡äº†ç³»ç»Ÿçš„çµæ´»æ€§ã€‚è®ºæ–‡é€šè¿‡è½¯ä»¶å¼€å‘æ¨¡æ‹Ÿå’Œå®æ—¶ç³»ç»Ÿäº¤äº’ç­‰å¤šä¸ªç”¨ä¾‹å±•ç¤ºäº†è¯¥æ¡†æ¶çš„å®ç”¨æ€§ã€‚ä¸ LangChain å’Œ AutoGen ç­‰ä¸»æµæ¡†æ¶çš„å¯¹æ¯”åˆ†æè¡¨æ˜ï¼Œsimpliflow åœ¨ç¡®å®šæ€§å·¥ä½œæµç¯å¢ƒä¸­å…·æœ‰æ›´å¼ºçš„ç®€æ´æ€§ã€æ§åˆ¶åŠ›å’Œéƒ¨ç½²é€Ÿåº¦ï¼Œä¸ºå¿«é€ŸåŸå‹è®¾è®¡æä¾›äº†é«˜æ•ˆçš„è§£å†³æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.10675v2",
      "published_date": "2025-10-12 16:02:50 UTC",
      "updated_date": "2025-11-12 15:06:12 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T02:51:57.569551+00:00"
    },
    {
      "arxiv_id": "2510.10671v3",
      "title": "Image-to-Video Transfer Learning based on Image-Language Foundation Models: A Comprehensive Survey",
      "title_zh": "åŸºäºå›¾åƒ-è¯­è¨€åŸºç¡€æ¨¡å‹çš„å›¾åƒåˆ°è§†é¢‘è¿ç§»å­¦ä¹ ï¼šå…¨é¢ç»¼è¿°",
      "authors": [
        "Jinxuan Li",
        "Chaolei Tan",
        "Haoxuan Chen",
        "Jianxin Ma",
        "Jian-Fang Hu",
        "Jianhuang Lai",
        "Wei-Shi Zheng"
      ],
      "abstract": "Image-Language Foundation Models (ILFMs) have demonstrated remarkable success in vision-language understanding, providing transferable multimodal representations that generalize across diverse downstream image-based tasks. The advancement of video-text research has spurred growing interest in extending image-based models to the video domain. This paradigm, termed as image-to-video transfer learning, effectively mitigates the substantial data and computational demands compared to training video-language models from scratch while achieves comparable or even stronger model performance. This survey provides the first comprehensive review of this emerging field, which begins by summarizing the widely used ILFMs and their capabilities. We then systematically classify existing image-to-video transfer learning techniques into two broad root categories (frozen features and adapted features), along with numerous fine-grained subcategories, based on the paradigm for transferring image understanding capability to video tasks. Building upon the task-specific nature of image-to-video transfer, this survey methodically elaborates these strategies and details their applications across a spectrum of video-text learning tasks, ranging from fine-grained settings (e.g., spatio-temporal video grounding) to coarse-grained ones (e.g., video question answering). We further present a detailed experimental analysis to investigate the efficacy of different image-to-video transfer learning paradigms on a range of downstream video understanding tasks. Finally, we identify prevailing challenges and highlight promising directions for future research. By offering a comprehensive and structured overview, this survey aims to establish a structured roadmap for advancing video-text learning based on existing ILFM, and to inspire future research directions in this rapidly evolving domain. Github repository is available.",
      "tldr_zh": "è¯¥ç»¼è¿°é¦–æ¬¡å…¨é¢å›é¡¾äº†åŸºäºå›¾åƒè¯­è¨€åŸºç¡€æ¨¡å‹ (Image-Language Foundation Models, ILFMs) çš„å›¾åƒåˆ°è§†é¢‘è¿ç§»å­¦ä¹  (image-to-video transfer learning) è¿™ä¸€æ–°å…´é¢†åŸŸã€‚è¿™ç§è¿ç§»å­¦ä¹ èŒƒå¼é€šè¿‡å°†å›¾åƒç†è§£èƒ½åŠ›æ‰©å±•åˆ°è§†é¢‘é¢†åŸŸï¼Œæœ‰æ•ˆç¼“è§£äº†ä»å¤´è®­ç»ƒè§†é¢‘è¯­è¨€æ¨¡å‹å¯¹æµ·é‡æ•°æ®å’Œè®¡ç®—èµ„æºçš„ä¾èµ–ï¼ŒåŒæ—¶å±•ç°å‡ºæå…·ç«äº‰åŠ›çš„æ€§èƒ½ã€‚æ–‡ç« ç³»ç»Ÿåœ°å°†ç°æœ‰æŠ€æœ¯å½’çº³ä¸ºå†»ç»“ç‰¹å¾ (frozen features) å’Œè‡ªé€‚åº”ç‰¹å¾ (adapted features) ä¸¤å¤§æ ¸å¿ƒç±»åˆ«ï¼Œå¹¶æ·±å…¥æ¢è®¨äº†å®ƒä»¬åœ¨æ—¶ç©ºè§†é¢‘å®šä½ (spatio-temporal video grounding) å’Œè§†é¢‘é—®ç­” (video question answering) ç­‰å¤šå°ºåº¦ä»»åŠ¡ä¸­çš„åº”ç”¨ã€‚é€šè¿‡å¯¹ä¸åŒèŒƒå¼çš„è¯¦ç»†å®éªŒåˆ†æï¼Œè¯¥ç ”ç©¶è¯„ä¼°äº†å„è¿ç§»ç­–ç•¥åœ¨ä¸‹æ¸¸è§†é¢‘ç†è§£ä»»åŠ¡ä¸­çš„å®é™…æ•ˆæœã€‚æœ€åï¼Œç»¼è¿°è¯†åˆ«äº†å½“å‰é¢†åŸŸçš„ä¸»è¦æŒ‘æˆ˜å¹¶æå‡ºäº†å‰ç»æ€§çš„ç ”ç©¶æ–¹å‘ï¼Œä¸ºæ¨åŠ¨è§†é¢‘æ–‡æœ¬å­¦ä¹ çš„å‘å±•æä¾›äº†ç³»ç»Ÿæ€§çš„è·¯çº¿å›¾ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "Updated version, github repository is available at https://github.com/YuriPreisdent/awesome-image-to-video-transfer",
      "pdf_url": "https://arxiv.org/pdf/2510.10671v3",
      "published_date": "2025-10-12 15:56:02 UTC",
      "updated_date": "2026-01-19 11:54:55 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T02:52:03.591691+00:00"
    },
    {
      "arxiv_id": "2510.10666v2",
      "title": "BrowserAgent: Building Web Agents with Human-Inspired Web Browsing Actions",
      "title_zh": "BrowserAgentï¼šåŸºäºä»¿äºº Web æµè§ˆåŠ¨ä½œæ„å»º Web æ™ºèƒ½ä½“",
      "authors": [
        "Tao Yu",
        "Zhengbo Zhang",
        "Zhiheng Lyu",
        "Junhao Gong",
        "Hongzhu Yi",
        "Xinming Wang",
        "Yuxuan Zhou",
        "Jiabing Yang",
        "Ping Nie",
        "Yan Huang",
        "Wenhu Chen"
      ],
      "abstract": "Efficiently solving real-world problems with LLMs increasingly hinges on their ability to interact with dynamic web environments and autonomously acquire external information. While recent research like Search-R1 and WebDancer demonstrates strong performance in solving web tasks, they heavily rely on additional tools to convert the interactive web environment into static text content. This is in contrast to human browsing behaviors, which involve diverse interactions with the browser, such as scrolling, clicking, and typing. In this paper, we propose BrowserAgent, a more interactive agent that solves complex tasks through human-inspired browser actions. BrowserAgent operates directly on raw web pages via Playwright through a set of predefined browser actions. We adopt a two-stage training (Supervised Fine-Tuning (SFT) and Rejection Fine-Tuning (RFT)) to improve the model's generalization abilities. Despite using significantly less training data than Search-R1, BrowserAgent achieves more competitive results across different Open-QA tasks. Additionally, we introduce an explicit memory mechanism to store key conclusions across steps, further enhancing the model's reasoning capabilities for long-horizon tasks. Notably, BrowserAgent-7B can achieve around 20\\% improvement over Search-R1 on multi-hop QA tasks like HotpotQA, 2Wiki, and Bamboogle. These results indicate that BrowserAgent can serve as a more advanced framework for more interactive and scalable web agents.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† BrowserAgentï¼Œè¿™æ˜¯ä¸€ç§é€šè¿‡æ¨¡ä»¿äººç±»æµè§ˆè¡Œä¸ºæ¥è§£å†³å¤æ‚ Web ä»»åŠ¡çš„äº¤äº’å¼æ™ºèƒ½ä½“æ¡†æ¶ã€‚ä¸ä»¥å¾€ä¾èµ–å·¥å…·å°†ç½‘é¡µè½¬æ¢ä¸ºé™æ€æ–‡æœ¬çš„æ–¹æ³•ä¸åŒï¼ŒBrowserAgent åˆ©ç”¨ Playwright ç›´æ¥åœ¨åŸå§‹ç½‘é¡µä¸Šæ‰§è¡Œæ»šåŠ¨ã€ç‚¹å‡»å’Œè¾“å…¥ç­‰åŠ¨ä½œï¼Œå®ç°ä¸åŠ¨æ€ Web ç¯å¢ƒçš„ç›´æ¥äº¤äº’ã€‚ä¸ºäº†æå‡æ³›åŒ–èƒ½åŠ›ï¼Œè¯¥æ¨¡å‹é‡‡ç”¨äº†ç›‘ç£å¾®è°ƒ (Supervised Fine-Tuning) å’Œæ‹’ç»å¾®è°ƒ (Rejection Fine-Tuning) çš„ä¸¤é˜¶æ®µè®­ç»ƒæ–¹æ³•ï¼Œå¹¶å¼•å…¥æ˜¾å¼è®°å¿†æœºåˆ¶ (Explicit Memory Mechanism) æ¥å¼ºåŒ–é•¿ç¨‹ä»»åŠ¡çš„æ¨ç†èƒ½åŠ›ã€‚å®éªŒè¡¨æ˜ï¼ŒBrowserAgent åœ¨ä½¿ç”¨æå°‘è®­ç»ƒæ•°æ®çš„æƒ…å†µä¸‹ï¼Œåœ¨å¤šé¡¹ Open-QA ä»»åŠ¡ä¸­å–å¾—äº†æå…·ç«äº‰åŠ›çš„ç»“æœã€‚ç‰¹åˆ«æ˜¯åœ¨ HotpotQA å’Œ Bamboogle ç­‰å¤šè·³é—®ç­” (Multi-hop QA) ä»»åŠ¡ä¸Šï¼ŒBrowserAgent-7B è¾ƒ Search-R1 æå‡äº†çº¦ 20% çš„æ€§èƒ½ã€‚è¯¥æˆæœä¸ºæ„å»ºæ›´é«˜æ•ˆã€å¯æ‰©å±•çš„ Web æ™ºèƒ½ä½“æä¾›äº†ä¸€ä¸ªå…ˆè¿›çš„å‚è€ƒæ¶æ„ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "10 pages",
      "pdf_url": "https://arxiv.org/pdf/2510.10666v2",
      "published_date": "2025-10-12 15:43:37 UTC",
      "updated_date": "2025-10-14 08:54:57 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T02:52:06.178500+00:00"
    },
    {
      "arxiv_id": "2510.13857v1",
      "title": "From Craft to Constitution: A Governance-First Paradigm for Principled Agent Engineering",
      "title_zh": "ä»æŠ€è‰ºåˆ°è§„çº¦ï¼šä¸€ç§æ²»ç†ä¼˜å…ˆçš„è§„èŒƒåŒ–æ™ºèƒ½ä½“å·¥ç¨‹èŒƒå¼",
      "authors": [
        "Qiang Xu",
        "Xiangyu Wen",
        "Changran Xu",
        "Zeju Li",
        "Jianyuan Zhong"
      ],
      "abstract": "The advent of powerful Large Language Models (LLMs) has ushered in an ``Age of the Agent,'' enabling autonomous systems to tackle complex goals. However, the transition from prototype to production is hindered by a pervasive ``crisis of craft,'' resulting in agents that are brittle, unpredictable, and ultimately untrustworthy in mission-critical applications. This paper argues this crisis stems from a fundamental paradigm mismatch -- attempting to command inherently probabilistic processors with the deterministic mental models of traditional software engineering. To solve this crisis, we introduce a governance-first paradigm for principled agent engineering, embodied in a formal architecture we call ArbiterOS.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰é©±åŠ¨çš„è‡ªä¸»ç³»ç»Ÿåœ¨ä»åŸå‹è¿ˆå‘ç”Ÿäº§è¿‡ç¨‹ä¸­é­é‡çš„â€œæ‰‹è‰ºå±æœºâ€ï¼ˆcrisis of craftï¼‰è¿›è¡Œäº†æ·±å…¥æ¢è®¨ï¼ŒæŒ‡å‡ºè¿™ç§å±æœºæºäºä¼ ç»Ÿè½¯ä»¶å·¥ç¨‹çš„ç¡®å®šæ€§æ€ç»´æ¨¡å‹ä¸ LLMs çš„æ¦‚ç‡æ€§æœ¬è´¨ä¹‹é—´çš„èŒƒå¼å¤±é…ã€‚ä¸ºäº†è§£å†³æ™ºèƒ½ä½“åœ¨å…³é”®åº”ç”¨ä¸­è¡¨ç°è„†å¼±ä¸”ä¸å¯ä¿¡çš„é—®é¢˜ï¼Œè®ºæ–‡æå‡ºäº†ä¸€ç§æ²»ç†ä¼˜å…ˆï¼ˆgovernance-firstï¼‰çš„åŸåˆ™æ€§æ™ºèƒ½ä½“å·¥ç¨‹èŒƒå¼ã€‚è¯¥èŒƒå¼é€šè¿‡ä¸€ç§åä¸º ArbiterOS çš„å½¢å¼åŒ–æ¶æ„å®ç°ï¼Œæ—¨åœ¨ä¸ºæ™ºèƒ½ä½“å¼€å‘æä¾›ä¸€å¥—ä¸¥è°¨çš„æ²»ç†æ¡†æ¶ã€‚è¯¥ç ”ç©¶çš„æ ¸å¿ƒè´¡çŒ®åœ¨äºå°†æ™ºèƒ½ä½“æ„å»ºä»ä¾èµ–ä¸ªäººæŠ€å·§çš„æ¨¡å¼è½¬å‘åŸºäºå‡†åˆ™çš„æ²»ç†æ¨¡å¼ï¼Œä¸ºå¼€å‘å¯é¢„æµ‹ã€å¯ä¿¡èµ–çš„æ™ºèƒ½ä½“ç³»ç»Ÿå¥ å®šäº†ç†è®ºåŸºç¡€ã€‚",
      "categories": [
        "cs.SE",
        "cs.AI"
      ],
      "primary_category": "cs.SE",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.13857v1",
      "published_date": "2025-10-12 15:42:46 UTC",
      "updated_date": "2025-10-12 15:42:46 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T02:52:07.577771+00:00"
    },
    {
      "arxiv_id": "2510.10663v1",
      "title": "Scalable Face Security Vision Foundation Model for Deepfake, Diffusion, and Spoofing Detection",
      "title_zh": "é¢å‘ Deepfakeã€æ‰©æ•£æ¨¡å‹åŠæ¬ºéª—æ£€æµ‹çš„å¯æ‰©å±•äººè„¸å®‰å…¨è§†è§‰åŸºåº§æ¨¡å‹",
      "authors": [
        "Gaojian Wang",
        "Feng Lin",
        "Tong Wu",
        "Zhisheng Yan",
        "Kui Ren"
      ],
      "abstract": "With abundant, unlabeled real faces, how can we learn robust and transferable facial representations to boost generalization across various face security tasks? We make the first attempt and propose FS-VFM, a scalable self-supervised pre-training framework, to learn fundamental representations of real face images. We introduce three learning objectives, namely 3C, that synergize masked image modeling (MIM) and instance discrimination (ID), empowering FS-VFM to encode both local patterns and global semantics of real faces. Specifically, we formulate various facial masking strategies for MIM and devise a simple yet effective CRFR-P masking, which explicitly prompts the model to pursue meaningful intra-region Consistency and challenging inter-region Coherency. We present a reliable self-distillation mechanism that seamlessly couples MIM with ID to establish underlying local-to-global Correspondence. After pre-training, vanilla vision transformers (ViTs) serve as universal Vision Foundation Models for downstream Face Security tasks: cross-dataset deepfake detection, cross-domain face anti-spoofing, and unseen diffusion facial forensics. To efficiently transfer the pre-trained FS-VFM, we further propose FS-Adapter, a lightweight plug-and-play bottleneck atop the frozen backbone with a novel real-anchor contrastive objective. Extensive experiments on 11 public benchmarks demonstrate that our FS-VFM consistently generalizes better than diverse VFMs, spanning natural and facial domains, fully, weakly, and self-supervised paradigms, small, base, and large ViT scales, and even outperforms SOTA task-specific methods, while FS-Adapter offers an excellent efficiency-performance trade-off. The code and models are available on https://fsfm-3c.github.io/fsvfm.html.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†FS-VFMï¼Œè¿™æ˜¯ä¸€ä¸ªå¯æ‰©å±•çš„è‡ªç›‘ç£é¢„è®­ç»ƒæ¡†æ¶ï¼Œæ—¨åœ¨é€šè¿‡å­¦ä¹ çœŸå®äººè„¸å›¾åƒçš„åŸºç¡€ç‰¹å¾æ¥æå‡Deepfakeæ£€æµ‹ã€Face Anti-spoofingå’Œæ‰©æ•£ç”Ÿæˆäººè„¸å–è¯ç­‰å®‰å…¨ä»»åŠ¡çš„æ³›åŒ–èƒ½åŠ›ã€‚æ¡†æ¶å¼•å…¥äº†åä¸º3Cçš„å­¦ä¹ ç›®æ ‡ï¼Œé€šè¿‡ååŒMasked Image Modeling (MIM)å’ŒInstance Discrimination (ID)ï¼Œä½¿æ¨¡å‹èƒ½å¤ŸåŒæ—¶ç¼–ç äººè„¸çš„å±€éƒ¨æ¨¡å¼å’Œå…¨å±€è¯­ä¹‰ã€‚ç ”ç©¶ç‰¹åˆ«è®¾è®¡äº†CRFR-Pæ©ç ç­–ç•¥ä»¥è¿½æ±‚åŒºåŸŸå†…çš„ä¸€è‡´æ€§(Consistency)å’ŒåŒºåŸŸé—´çš„è¿è´¯æ€§(Coherency)ï¼Œå¹¶åˆ©ç”¨è‡ªè’¸é¦æœºåˆ¶å»ºç«‹äº†å±€éƒ¨åˆ°å…¨å±€çš„å¯¹åº”å…³ç³»(Correspondence)ã€‚ä¸ºäº†å®ç°é«˜æ•ˆè¿ç§»ï¼Œç ”ç©¶è¿›ä¸€æ­¥æå‡ºäº†è½»é‡çº§çš„FS-Adapteræ’ä»¶ï¼Œé€šè¿‡æ–°å‹çš„çœŸå®é”šç‚¹å¯¹æ¯”ç›®æ ‡åœ¨å†»ç»“çš„ä¸»å¹²ç½‘ç»œä¸Šå®ç°æ€§èƒ½ä¼˜åŒ–ã€‚åœ¨11ä¸ªå…¬å¼€åŸºå‡†æµ‹è¯•ä¸Šçš„å®éªŒè¯æ˜ï¼ŒFS-VFMåœ¨ä¸åŒè§„æ¨¡çš„Vision Transformers (ViTs)ä¸‹å‡ä¼˜äºç°æœ‰çš„é€šç”¨è§†è§‰åŸºç¡€æ¨¡å‹å’Œç‰¹å®šä»»åŠ¡çš„SOTAæ–¹æ³•ï¼Œå®ç°äº†æ•ˆç‡ä¸æ€§èƒ½çš„å“è¶Šå¹³è¡¡ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "18 pages, 9 figures, project page: https://fsfm-3c.github.io/fsvfm.html",
      "pdf_url": "https://arxiv.org/pdf/2510.10663v1",
      "published_date": "2025-10-12 15:38:03 UTC",
      "updated_date": "2025-10-12 15:38:03 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T02:52:14.064126+00:00"
    },
    {
      "arxiv_id": "2510.10661v2",
      "title": "AGENTIQL: An Agent-Inspired Multi-Expert Framework for Text-to-SQL Generation",
      "title_zh": "AGENTIQLï¼šå—æ™ºèƒ½ä½“å¯å‘çš„å¤šä¸“å®¶ Text-to-SQL ç”Ÿæˆæ¡†æ¶",
      "authors": [
        "Omid Reza Heidari",
        "Siobhan Reid",
        "Yassine Yaakoubi"
      ],
      "abstract": "LLMs have advanced text-to-SQL generation, yet monolithic architectures struggle with complex reasoning and schema diversity. We propose AGENTIQL, an agent-inspired multi-expert framework that combines a reasoning agent for question decomposition, a coding agent for sub-query generation, and a refinement step for column selection. An adaptive router further balances efficiency and accuracy by selecting between our modular pipeline and a baseline parser. Several steps in the pipeline can be executed in parallel, making the framework scalable to larger workloads. Evaluated on the Spider benchmark, AGENTIQL improves execution accuracy and interpretability and achieves up to 86.07% EX with 14B models using the Planner&Executor merging strategy. The attained performance is contingent upon the efficacy of the routing mechanism, thereby narrowing the gap to GPT-4-based SOTA (89.65% EX) while using much smaller open-source LLMs. Beyond accuracy, AGENTIQL enhances transparency by exposing intermediate reasoning steps, offering a robust, scalable, and interpretable approach to semantic parsing.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† AGENTIQLï¼Œä¸€ä¸ªå—æ™ºèƒ½ä½“å¯å‘çš„å¤šä¸“å®¶æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³å¤§å‹è¯­è¨€æ¨¡å‹(LLMs)åœ¨ Text-to-SQL ç”Ÿæˆä¸­é¢ä¸´çš„å¤æ‚æ¨ç†å’Œæ¨¡å¼å¤šæ ·æ€§æŒ‘æˆ˜ã€‚è¯¥æ¡†æ¶é€šè¿‡æ¨ç†æ™ºèƒ½ä½“(Reasoning Agent)è¿›è¡Œé—®é¢˜æ‹†è§£ï¼Œåˆ©ç”¨ç¼–ç æ™ºèƒ½ä½“(Coding Agent)ç”Ÿæˆå­æŸ¥è¯¢ï¼Œå¹¶ç»“åˆç»†åŒ–æ­¥éª¤è¿›è¡Œåˆ—é€‰æ‹©ï¼Œå°†è§£æä»»åŠ¡æ¨¡å—åŒ–ã€‚æ­¤å¤–ï¼ŒAGENTIQL å¼•å…¥äº†è‡ªé€‚åº”è·¯ç”±å™¨(Adaptive Router)æ¥å¹³è¡¡æ•ˆç‡ä¸å‡†ç¡®æ€§ï¼Œå¹¶æ”¯æŒå¤šæ­¥éª¤å¹¶è¡Œæ‰§è¡Œä»¥æå‡ç³»ç»Ÿçš„å¯æ‰©å±•æ€§ã€‚åœ¨ Spider åŸºå‡†æµ‹è¯•ä¸­ï¼Œè¯¥æ¡†æ¶é…åˆ 14B æ¨¡å‹è¾¾åˆ°äº† 86.07% çš„æ‰§è¡Œå‡†ç¡®ç‡(EX)ï¼Œæœ‰æ•ˆç¼©å°äº†å¼€æºæ¨¡å‹ä¸ GPT-4 ç­‰æœ€å…ˆè¿›æ¨¡å‹(SOTA)ä¹‹é—´çš„å·®è·ã€‚é™¤äº†æ€§èƒ½æå‡ï¼ŒAGENTIQL è¿˜é€šè¿‡å…¬å¼€ä¸­é—´æ¨ç†è¿‡ç¨‹æ˜¾è‘—å¢å¼ºäº†è¯­ä¹‰è§£æçš„é€æ˜åº¦å’Œå¯è§£é‡Šæ€§ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "Accepted at NeurIPS 2025, ER \"Efficient Reasoning\" workshop",
      "pdf_url": "https://arxiv.org/pdf/2510.10661v2",
      "published_date": "2025-10-12 15:35:05 UTC",
      "updated_date": "2025-10-14 11:24:04 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T02:52:14.481763+00:00"
    },
    {
      "arxiv_id": "2510.10650v1",
      "title": "DEMO: Disentangled Motion Latent Flow Matching for Fine-Grained Controllable Talking Portrait Synthesis",
      "title_zh": "DEMOï¼šåŸºäºè§£è€¦è¿åŠ¨æ½œæµåŒ¹é…çš„ç»†ç²’åº¦å¯æ§è¯´è¯äººå¤´åƒåˆæˆ",
      "authors": [
        "Peiyin Chen",
        "Zhuowei Yang",
        "Hui Feng",
        "Sheng Jiang",
        "Rui Yan"
      ],
      "abstract": "Audio-driven talking-head generation has advanced rapidly with diffusion-based generative models, yet producing temporally coherent videos with fine-grained motion control remains challenging. We propose DEMO, a flow-matching generative framework for audio-driven talking-portrait video synthesis that delivers disentangled, high-fidelity control of lip motion, head pose, and eye gaze. The core contribution is a motion auto-encoder that builds a structured latent space in which motion factors are independently represented and approximately orthogonalized. On this disentangled motion space, we apply optimal-transport-based flow matching with a transformer predictor to generate temporally smooth motion trajectories conditioned on audio. Extensive experiments across multiple benchmarks show that DEMO outperforms prior methods in video realism, lip-audio synchronization, and motion fidelity. These results demonstrate that combining fine-grained motion disentanglement with flow-based generative modeling provides a powerful new paradigm for controllable talking-head video synthesis.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† DEMOï¼Œä¸€ç§ç”¨äºéŸ³é¢‘é©±åŠ¨è¯´è¯äººè§†é¢‘åˆæˆçš„æµåŒ¹é… (Flow Matching) ç”Ÿæˆæ¡†æ¶ï¼Œæ—¨åœ¨å®ç°å¯¹å”‡éƒ¨åŠ¨ä½œ (Lip Motion)ã€å¤´éƒ¨å§¿æ€ (Head Pose) å’Œè§†çº¿ (Eye Gaze) çš„ç»†ç²’åº¦è§£è€¦æ§åˆ¶ã€‚å…¶æ ¸å¿ƒè´¡çŒ®æ˜¯ä¸€ä¸ªåŠ¨ä½œè‡ªåŠ¨ç¼–ç å™¨ (Motion Auto-encoder)ï¼Œè¯¥å·¥å…·æ„å»ºäº†ä¸€ä¸ªè¿åŠ¨å› å­ç‹¬ç«‹è¡¨ç¤ºä¸”è¿‘ä¼¼æ­£äº¤åŒ–çš„ç»“æ„åŒ–æ½œåœ¨ç©ºé—´ã€‚åœ¨æ­¤è§£è€¦ç©ºé—´å†…ï¼Œç ”ç©¶é€šè¿‡åŸºäºæœ€ä¼˜ä¼ è¾“ (Optimal-Transport) çš„æµåŒ¹é…æŠ€æœ¯å’Œ Transformer é¢„æµ‹å™¨ï¼Œç”Ÿæˆäº†åœ¨éŸ³é¢‘å¼•å¯¼ä¸‹å…·æœ‰é«˜åº¦æ—¶é—´ä¸€è‡´æ€§çš„è¿åŠ¨è½¨è¿¹ã€‚å¤§é‡å®éªŒè¯æ˜ï¼ŒDEMO åœ¨è§†é¢‘çœŸå®æ„Ÿã€å”‡éŸ³åŒæ­¥ä»¥åŠåŠ¨ä½œä¿çœŸåº¦ç­‰å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­å‡ä¼˜äºå…ˆå‰æ–¹æ³•ã€‚è¯¥æˆæœè¡¨æ˜ï¼Œå°†ç»†ç²’åº¦åŠ¨ä½œè§£è€¦ä¸æµåŒ¹é…ç”Ÿæˆæ¨¡å‹ç›¸ç»“åˆï¼Œä¸ºé«˜ä¿çœŸå¯æ§è¯´è¯äººè§†é¢‘åˆæˆå¼€è¾Ÿäº†æ–°çš„æŠ€æœ¯è·¯å¾„ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "5 pages",
      "pdf_url": "https://arxiv.org/pdf/2510.10650v1",
      "published_date": "2025-10-12 15:10:33 UTC",
      "updated_date": "2025-10-12 15:10:33 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T02:52:25.174117+00:00"
    },
    {
      "arxiv_id": "2510.10649v1",
      "title": "Unlocking Exploration in RLVR: Uncertainty-aware Advantage Shaping for Deeper Reasoning",
      "title_zh": "é‡Šæ”¾ RLVR çš„æ¢ç´¢æ½œåŠ›ï¼šåŸºäºä¸ç¡®å®šæ€§æ„ŸçŸ¥ä¼˜åŠ¿å¡‘é€ çš„æ·±åº¦æ¨ç†",
      "authors": [
        "Can Xie",
        "Ruotong Pan",
        "Xiangyu Wu",
        "Yunfei Zhang",
        "Jiayi Fu",
        "Tingting Gao",
        "Guorui Zhou"
      ],
      "abstract": "Reinforcement Learning with Verifiable Rewards (RLVR) has shown significant promise for enhancing the reasoning capabilities of large language models (LLMs). However, prevailing algorithms like GRPO broadcast a uniform advantage signal across all tokens in a sequence. This coarse-grained approach overlooks the pivotal role of uncertain, high-stakes decisions during reasoning, leading to inefficient exploration and the well-documented problem of entropy collapse. To address this, we introduce UnCertainty-aware Advantage Shaping (UCAS), a model-free method that refines credit assignment by leveraging the model's internal uncertainty signals. UCAS operates in two stages: it first modulates the response-level advantage using the model's overall self-confidence, and then applies a token-level penalty based on raw logit certainty. This dual mechanism encourages exploration of high-uncertainty paths that yield correct answers while penalizing overconfident yet erroneous reasoning, effectively balancing the exploration-exploitation trade-off. Extensive experiments on five mathematical reasoning benchmarks show that UCAS significantly outperforms strong RLVR baselines across multiple model scales, including 1.5B and 7B. Our analysis confirms that UCAS not only achieves higher rewards but also promotes greater reasoning diversity and successfully mitigates entropy collapse.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹Reinforcement Learning with Verifiable Rewards (RLVR)åœ¨æå‡å¤§è¯­è¨€æ¨¡å‹æ¨ç†èƒ½åŠ›æ—¶é¢ä¸´çš„æŒ‘æˆ˜ï¼ŒæŒ‡å‡ºå¦‚GRPOç­‰ç°æœ‰ç®—æ³•å› é‡‡ç”¨ç»Ÿä¸€çš„ä¼˜åŠ¿ä¿¡å·(uniform advantage signal)è€Œå¯¼è‡´æ¢ç´¢æ•ˆç‡ä½ä¸‹åŠç†µå¡Œé™·(entropy collapse)é—®é¢˜ã€‚ä¸ºæ­¤ï¼Œä½œè€…æå‡ºäº†ä¸ç¡®å®šæ€§æ„ŸçŸ¥ä¼˜åŠ¿å¡‘é€ (UnCertainty-aware Advantage Shaping, UCAS)ï¼Œè¿™æ˜¯ä¸€ç§é€šè¿‡åˆ©ç”¨æ¨¡å‹å†…éƒ¨ä¸ç¡®å®šæ€§ä¿¡å·æ¥ä¼˜åŒ–ä¿¡ç”¨åˆ†é…(credit assignment)çš„å…æ¨¡å‹æ–¹æ³•ã€‚UCASé€šè¿‡ä¸¤ä¸ªé˜¶æ®µè¿è¡Œï¼šé¦–å…ˆåˆ©ç”¨æ¨¡å‹çš„æ•´ä½“è‡ªé€‚åº”ç½®ä¿¡åº¦è°ƒèŠ‚å“åº”å±‚çº§çš„ä¼˜åŠ¿å€¼ï¼ŒéšååŸºäºåŸå§‹é€»è¾‘å€¼ç¡®å®šæ€§(logit certainty)å®æ–½æ ‡è®°å±‚çº§(token-level)çš„æƒ©ç½šï¼Œä»è€Œå¹³è¡¡æ¢ç´¢ä¸åˆ©ç”¨çš„æƒè¡¡ã€‚å®éªŒåœ¨äº”ä¸ªæ•°å­¦æ¨ç†åŸºå‡†æµ‹è¯•ä¸Šè¯æ˜ï¼ŒUCASåœ¨1.5Bå’Œ7Bç­‰å¤šç§æ¨¡å‹è§„æ¨¡ä¸‹å‡æ˜¾è‘—ä¼˜äºRLVRåŸºçº¿æ¨¡å‹ã€‚åˆ†æè¿›ä¸€æ­¥è¯å®ï¼ŒUCASä¸ä»…å®ç°äº†æ›´é«˜çš„å¥–åŠ±å€¼ï¼Œè¿˜ä¿ƒè¿›äº†æ¨ç†çš„å¤šæ ·æ€§å¹¶æœ‰æ•ˆç¼“è§£äº†ç†µå¡Œé™·é—®é¢˜ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.10649v1",
      "published_date": "2025-10-12 15:06:53 UTC",
      "updated_date": "2025-10-12 15:06:53 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T02:52:29.179977+00:00"
    },
    {
      "arxiv_id": "2510.10645v3",
      "title": "Trustworthy Retrosynthesis: Eliminating Hallucinations with a Diverse Ensemble of Reaction Scorers",
      "title_zh": "å¯ä¿¡é€†åˆæˆï¼šé€šè¿‡å¤šæ ·åŒ–ååº”è¯„åˆ†å™¨é›†æˆæ¶ˆé™¤å¹»è§‰",
      "authors": [
        "Michal Sadowski",
        "Tadija RadusinoviÄ‡",
        "Maria Wyrzykowska",
        "Lukasz Sztukiewicz",
        "Jan Rzymkowski",
        "PaweÅ‚ WÅ‚odarczyk-PruszyÅ„ski",
        "MikoÅ‚aj Sacha",
        "Piotr Kozakowski",
        "Ruard van Workum",
        "Stanislaw Kamil Jastrzebski"
      ],
      "abstract": "Retrosynthesis is one of the domains transformed by the rise of generative models, and it is one where the problem of nonsensical or erroneous outputs (hallucinations) is particularly insidious: reliable assessment of synthetic plans is time-consuming, with automatic methods lacking. In this work, we present RetroTrim, a retrosynthesis system that successfully avoids nonsensical plans on a set of challenging drug-like targets. Compared to common baselines in the field, our system is not only the sole method that succeeds in filtering out hallucinated reactions, but it also results in the highest number of high-quality paths overall. The key insight behind RetroTrim is the combination of diverse reaction scoring strategies, based on machine learning models and existing chemical databases. We show that our scoring strategies capture different classes of hallucinations by analyzing them on a dataset of labeled retrosynthetic intermediates. This approach formed the basis of our winning solution to the Standard Industries \\$1 million Retrosynthesis Challenge. To measure the performance of retrosynthesis systems, we propose a novel evaluation protocol for reactions and synthetic paths based on a structured review by expert chemists. Using this protocol, we compare systems on a set of 32 novel targets, curated to reflect recent trends in drug structures. While the insights behind our methodology are broadly applicable to retrosynthesis, our focus is on targets in the drug-like domain. By releasing our benchmark targets and the details of our evaluation protocol, we hope to inspire further research into reliable retrosynthesis.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ç”Ÿæˆå¼æ¨¡å‹åœ¨é€†åˆæˆåˆ†æ(Retrosynthesis)ä¸­äº§ç”Ÿçš„è™šå‡äº§å‡º(Hallucinations)é—®é¢˜ï¼Œæå‡ºäº†RetroTrimç³»ç»Ÿä»¥å®ç°æ›´å…·å¯é æ€§çš„åˆæˆè·¯å¾„è§„åˆ’ã€‚RetroTrimçš„æ ¸å¿ƒåˆ›æ–°åœ¨äºé›†æˆäº†ä¸€å¥—ç»“åˆæœºå™¨å­¦ä¹ æ¨¡å‹ä¸ç°æœ‰åŒ–å­¦æ•°æ®åº“çš„å¤šæ ·åŒ–ååº”è¯„åˆ†ç­–ç•¥(Reaction Scoring Strategies)ï¼Œèƒ½å¤Ÿç²¾å‡†æ•æ‰å¹¶è¿‡æ»¤ä¸åŒç±»å‹çš„é”™è¯¯ååº”ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯¥ç³»ç»Ÿåœ¨å¤„ç†å…·æœ‰æŒ‘æˆ˜æ€§çš„ç±»è¯ç›®æ ‡åˆ†å­æ—¶ï¼Œå…¶é«˜è´¨é‡åˆæˆè·¯å¾„çš„äº§å‡ºç‡æ˜¾è‘—ä¼˜äºç°æœ‰åŸºçº¿æ¨¡å‹ã€‚å‡­å€Ÿè¿™ä¸€å‡ºè‰²è¡¨ç°ï¼Œè¯¥æ–¹æ¡ˆæˆä¸ºäº†Standard Industriesç™¾ä¸‡ç¾å…ƒé€†åˆæˆæŒ‘æˆ˜èµ›(Retrosynthesis Challenge)çš„è·èƒœæ–¹æ¡ˆã€‚æ­¤å¤–ï¼Œç ”ç©¶è€…è¿˜æå‡ºäº†ä¸€å¥—åŸºäºä¸“å®¶åŒ–å­¦å®¶ç»“æ„åŒ–è¯„å®¡çš„å…¨æ–°è¯„ä¼°åè®®ï¼Œå¹¶åœ¨åŒ…å«32ä¸ªæ–°å‹ç±»è¯åˆ†å­çš„åŸºå‡†æ•°æ®é›†ä¸ŠéªŒè¯äº†ç³»ç»Ÿæ€§èƒ½ã€‚é€šè¿‡å…¬å¼€å‘å¸ƒè¿™äº›åŸºå‡†ç›®æ ‡å’Œè¯„ä¼°ç»†èŠ‚ï¼Œè¯¥ç ”ç©¶ä¸ºæ„å»ºæ›´å…·å¯ä¿¡åº¦çš„è‡ªåŠ¨åŒ–é€†åˆæˆç³»ç»Ÿå¥ å®šäº†åŸºç¡€ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.10645v3",
      "published_date": "2025-10-12 14:56:34 UTC",
      "updated_date": "2025-12-08 17:27:20 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T02:52:32.674692+00:00"
    },
    {
      "arxiv_id": "2510.10644v2",
      "title": "Hierarchical Optimization via LLM-Guided Objective Evolution for Mobility-on-Demand Systems",
      "title_zh": "åŸºäº LLM å¼•å¯¼ç›®æ ‡æ¼”åŒ–çš„æŒ‰éœ€å‡ºè¡Œç³»ç»Ÿåˆ†å±‚ä¼˜åŒ–",
      "authors": [
        "Yi Zhang",
        "Yushen Long",
        "Yun Ni",
        "Liping Huang",
        "Xiaohong Wang",
        "Jun Liu"
      ],
      "abstract": "Online ride-hailing platforms aim to deliver efficient mobility-on-demand services, often facing challenges in balancing dynamic and spatially heterogeneous supply and demand. Existing methods typically fall into two categories: reinforcement learning (RL) approaches, which suffer from data inefficiency, oversimplified modeling of real-world dynamics, and difficulty enforcing operational constraints; or decomposed online optimization methods, which rely on manually designed high-level objectives that lack awareness of low-level routing dynamics. To address this issue, we propose a novel hybrid framework that integrates large language model (LLM) with mathematical optimization in a dynamic hierarchical system: (1) it is training-free, removing the need for large-scale interaction data as in RL, and (2) it leverages LLM to bridge cognitive limitations caused by problem decomposition by adaptively generating high-level objectives. Within this framework, LLM serves as a meta-optimizer, producing semantic heuristics that guide a low-level optimizer responsible for constraint enforcement and real-time decision execution. These heuristics are refined through a closed-loop evolutionary process, driven by harmony search, which iteratively adapts the LLM prompts based on feasibility and performance feedback from the optimization layer. Extensive experiments based on scenarios derived from both the New York and Chicago taxi datasets demonstrate the effectiveness of our approach, achieving an average improvement of 16% compared to state-of-the-art baselines.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å³æ—¶å‡ºè¡Œ(Mobility-on-Demand)ç³»ç»Ÿåœ¨åŠ¨æ€ä¾›éœ€å¹³è¡¡ä¸­çš„æŒ‘æˆ˜ï¼ŒæŒ‡å‡ºäº†å¼ºåŒ–å­¦ä¹ (Reinforcement Learning)æ•ˆç‡ä½ä¸‹ä¸ä¼ ç»Ÿä¼˜åŒ–æ–¹æ³•æ‰‹åŠ¨è®¾è®¡ç›®æ ‡å±€é™æ€§ç­‰é—®é¢˜ã€‚ä¸ºæ­¤ï¼Œä½œè€…æå‡ºäº†ä¸€ç§ç»“åˆå¤§è¯­è¨€æ¨¡å‹(Large Language Model, LLM)ä¸æ•°å­¦ä¼˜åŒ–çš„åŠ¨æ€å±‚æ¬¡åŒ–æ··åˆæ¡†æ¶ã€‚è¯¥æ¡†æ¶å°†LLMè§†ä¸ºå…ƒä¼˜åŒ–å™¨(meta-optimizer)ï¼Œç”Ÿæˆç”¨äºå¼•å¯¼åº•å±‚å†³ç­–æ‰§è¡Œå™¨çš„è¯­ä¹‰å¯å‘å¼ç›®æ ‡ï¼Œå¹¶åˆ©ç”¨åŸºäºå’Œå£°æœç´¢(Harmony Search)çš„é—­ç¯æ¼”åŒ–è¿‡ç¨‹ï¼Œé€šè¿‡æ€§èƒ½åé¦ˆè¿­ä»£ç²¾ç‚¼LLMæç¤ºè¯ã€‚è¿™ç§è®­ç»ƒæ— å…³(training-free)çš„æ–¹æ³•æœ‰æ•ˆæ¡¥æ¥äº†é«˜å±‚ç­–ç•¥ä¸ä½å±‚è·¯ç”±åŠ¨æ€ä¹‹é—´çš„è®¤çŸ¥é¸¿æ²Ÿï¼Œå¹¶ç¡®ä¿äº†æ“ä½œçº¦æŸçš„ä¸¥æ ¼æ‰§è¡Œã€‚åŸºäºçº½çº¦å’ŒèŠåŠ å“¥å‡ºç§Ÿè½¦æ•°æ®é›†çš„å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•æ¯”ç°æœ‰æœ€å…ˆè¿›çš„åŸºå‡†æ¨¡å‹æ€§èƒ½å¹³å‡æå‡äº†16%ï¼Œå±•ç°äº†å…¶åœ¨å¤æ‚ç§»åŠ¨å‡ºè¡Œç³»ç»Ÿè°ƒåº¦ä¸­çš„ä¼˜è¶Šæ€§å’Œå®ç”¨æ½œåŠ›ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.10644v2",
      "published_date": "2025-10-12 14:56:19 UTC",
      "updated_date": "2025-10-25 12:20:43 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T02:53:36.092951+00:00"
    },
    {
      "arxiv_id": "2510.10642v2",
      "title": "UniCoD: Enhancing Robot Policy via Unified Continuous and Discrete Representation Learning",
      "title_zh": "UniCoDï¼šé€šè¿‡ç»Ÿä¸€è¿ç»­ä¸ç¦»æ•£è¡¨å¾å­¦ä¹ å¢å¼ºæœºå™¨äººç­–ç•¥",
      "authors": [
        "Jianke Zhang",
        "Yucheng Hu",
        "Yanjiang Guo",
        "Xiaoyu Chen",
        "Yichen Liu",
        "Wenna Chen",
        "Chaochao Lu",
        "Jianyu Chen"
      ],
      "abstract": "Building generalist robot policies that can handle diverse tasks in open-ended environments is a central challenge in robotics. To leverage knowledge from large-scale pretraining, prior work (VLA) has typically built generalist policies either on top of vision-language understanding models (VLMs) or generative models. However, both semantic understanding from vision-language pretraining and visual dynamics modeling from visual-generation pretraining are crucial for embodied robots. Recent unified models of generation and understanding have demonstrated strong capabilities in both comprehension and generation through large-scale pretraining. We posit that robotic policy learning can likewise benefit from the combined strengths of understanding, planning, and continuous future representation learning. Building on this insight, we introduce UniCoD, which acquires the ability to dynamically model high-dimensional visual features through pretraining on over 1M internet-scale instructional manipulation videos. Subsequently, UniCoD is fine-tuned on data collected from the robot embodiment, enabling the learning of mappings from predictive representations to action tokens. Extensive experiments show our approach consistently outperforms baseline methods in terms of 9\\% and 12\\% across simulation environments and real-world out-of-distribution tasks.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† UniCoD æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³æ„å»ºèƒ½å¤Ÿå¤„ç†å¼€æ”¾ç¯å¢ƒä¸‹å¤šæ ·åŒ–ä»»åŠ¡çš„é€šç”¨å‹æœºå™¨äººç­–ç•¥ï¼ˆgeneralist robot policiesï¼‰è¿™ä¸€æ ¸å¿ƒæŒ‘æˆ˜ã€‚ç ”ç©¶è€…æŒ‡å‡ºï¼Œè§†è§‰è¯­è¨€é¢„è®­ç»ƒæä¾›çš„è¯­ä¹‰ç†è§£å’Œè§†è§‰ç”Ÿæˆé¢„è®­ç»ƒæä¾›çš„è§†è§‰åŠ¨åŠ›å­¦å»ºæ¨¡å¯¹äºå…·èº«æœºå™¨äººï¼ˆembodied robotsï¼‰éƒ½è‡³å…³é‡è¦ã€‚UniCoD é€šè¿‡åœ¨è¶…è¿‡ 100 ä¸‡ä¸ªäº’è”ç½‘è§„æ¨¡çš„æŒ‡ä»¤æ€§æ“ä½œè§†é¢‘ä¸Šè¿›è¡Œé¢„è®­ç»ƒï¼Œå­¦ä¹ äº†åŠ¨æ€å»ºæ¨¡é«˜ç»´è§†è§‰ç‰¹å¾çš„èƒ½åŠ›ã€‚åœ¨æ­¤åŸºç¡€ä¸Šï¼Œæ¨¡å‹é€šè¿‡åœ¨æœºå™¨äººå…·èº«æ•°æ®ä¸Šè¿›è¡Œå¾®è°ƒï¼Œå®ç°äº†ä»é¢„æµ‹æ€§è¡¨ç¤ºåˆ°åŠ¨ä½œæ ‡è®°ï¼ˆaction tokensï¼‰çš„æ˜ å°„ã€‚å¤§é‡å®éªŒè¯æ˜ï¼Œè¯¥æ–¹æ³•åœ¨æ¨¡æ‹Ÿç¯å¢ƒå’ŒçœŸå®ä¸–ç•Œçš„åˆ†å¸ƒå¤–ï¼ˆout-of-distributionï¼‰ä»»åŠ¡ä¸­ï¼Œè¡¨ç°ä¸€è‡´ä¼˜äºåŸºçº¿æ–¹æ³•ï¼Œæ€§èƒ½åˆ†åˆ«æå‡äº† 9% å’Œ 12%ã€‚è¿™é¡¹å·¥ä½œå¼ºè°ƒäº†ç»“åˆç†è§£ã€è§„åˆ’å’Œè¿ç»­æœªæ¥è¡¨ç¤ºå­¦ä¹ åœ¨æå‡æœºå™¨äººé€šç”¨èƒ½åŠ›æ–¹é¢çš„å·¨å¤§æ½œåŠ›ã€‚",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.10642v2",
      "published_date": "2025-10-12 14:54:19 UTC",
      "updated_date": "2025-11-04 09:35:36 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T02:52:38.398375+00:00"
    },
    {
      "arxiv_id": "2510.10640v1",
      "title": "Equity-Aware Geospatial AI for Forecasting Demand-Driven Hospital Locations in Germany",
      "title_zh": "å…¼é¡¾å…¬å¹³æ€§çš„åœ°ç†ç©ºé—´äººå·¥æ™ºèƒ½ï¼šç”¨äº Germany éœ€æ±‚é©±åŠ¨å‹åŒ»é™¢é€‰å€é¢„æµ‹",
      "authors": [
        "Piyush Pant",
        "Marcellius William Suntoro",
        "Ayesha Siddiqua",
        "Muhammad Shehryaar Sharif",
        "Daniyal Ahmed"
      ],
      "abstract": "This paper presents EA-GeoAI, an integrated framework for demand forecasting and equitable hospital planning in Germany through 2030. We combine district-level demographic shifts, aging population density, and infrastructure balances into a unified Equity Index. An interpretable Agentic AI optimizer then allocates beds and identifies new facility sites to minimize unmet need under budget and travel-time constraints. This approach bridges GeoAI, long-term forecasting, and equity measurement to deliver actionable recommendations for policymakers.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† EA-GeoAIï¼Œè¿™æ˜¯ä¸€ä¸ªæ—¨åœ¨å®ç°å¾·å›½ 2030 å¹´åŒ»ç–—éœ€æ±‚é¢„æµ‹å’Œå…¬å¹³åŒ»é™¢è§„åˆ’çš„é›†æˆæ¡†æ¶ã€‚è¯¥æ¡†æ¶é€šè¿‡æ•´åˆåœ°åŒºå±‚é¢çš„äººå£ç»“æ„å˜è¿ã€è€é¾„åŒ–äººå£å¯†åº¦å’ŒåŸºç¡€è®¾æ–½å¹³è¡¡ï¼Œæ„å»ºäº†ç»Ÿä¸€çš„ Equity Index ä»¥é‡åŒ–åŒ»ç–—å…¬å¹³æ€§ã€‚åˆ©ç”¨å¯è§£é‡Šçš„ Agentic AI ä¼˜åŒ–å™¨ï¼Œè¯¥ç³»ç»Ÿåœ¨é¢„ç®—å’Œå‡ºè¡Œæ—¶é—´ç­‰çº¦æŸæ¡ä»¶ä¸‹ï¼Œé€šè¿‡ä¼˜åŒ–åºŠä½åˆ†é…å’Œæ–°é™¢å€è¯†åˆ«æ¥æœ€å°åŒ–æœªæ»¡è¶³çš„åŒ»ç–—éœ€æ±‚ã€‚è¯¥æ–¹æ³•æˆåŠŸæ¡¥æ¥äº† GeoAIã€é•¿æœŸé¢„æµ‹ä¸å…¬å¹³æ€§è¯„ä¼°ï¼Œä¸ºæ”¿ç­–åˆ¶å®šè€…æä¾›äº†ç§‘å­¦ä¸”å…·å¯æ“ä½œæ€§çš„å†³ç­–å»ºè®®ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "7 pages. Application: https://equity-aware-geospatial-ai-project.streamlit.app/ Codebase: https://github.com/mwsyow/equity-aware-geospatial-ai-project/",
      "pdf_url": "https://arxiv.org/pdf/2510.10640v1",
      "published_date": "2025-10-12 14:51:28 UTC",
      "updated_date": "2025-10-12 14:51:28 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T02:52:43.181752+00:00"
    },
    {
      "arxiv_id": "2510.10639v1",
      "title": "Automatic Piecewise Linear Regression for Predicting Student Learning Satisfaction",
      "title_zh": "é¢„æµ‹å­¦ç”Ÿå­¦ä¹ æ»¡æ„åº¦çš„è‡ªåŠ¨åˆ†æ®µçº¿æ€§å›å½’",
      "authors": [
        "Haemin Choi",
        "Gayathri Nadarajan"
      ],
      "abstract": "Although student learning satisfaction has been widely studied, modern techniques such as interpretable machine learning and neural networks have not been sufficiently explored. This study demonstrates that a recent model that combines boosting with interpretability, automatic piecewise linear regression(APLR), offers the best fit for predicting learning satisfaction among several state-of-the-art approaches. Through the analysis of APLR's numerical and visual interpretations, students' time management and concentration abilities, perceived helpfulness to classmates, and participation in offline courses have the most significant positive impact on learning satisfaction. Surprisingly, involvement in creative activities did not positively affect learning satisfaction. Moreover, the contributing factors can be interpreted on an individual level, allowing educators to customize instructions according to student profiles.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†å­¦ç”Ÿå­¦ä¹ æ»¡æ„åº¦çš„é¢„æµ‹æ¨¡å‹ï¼Œé‡ç‚¹åº”ç”¨äº†ç»“åˆé›†æˆå­¦ä¹ (Boosting)ä¸å¯è§£é‡Šæ€§çš„è‡ªåŠ¨åˆ†æ®µçº¿æ€§å›å½’(Automatic Piecewise Linear Regression, APLR)æ¨¡å‹ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œåœ¨ä¸å¤šç§å‰æ²¿æœºå™¨å­¦ä¹ ç®—æ³•çš„å¯¹æ¯”ä¸­ï¼ŒAPLR æ¨¡å‹åœ¨é¢„æµ‹å­¦ä¹ æ»¡æ„åº¦æ–¹é¢è¡¨ç°å‡ºæœ€ä½³çš„æ‹Ÿåˆæ•ˆæœã€‚é€šè¿‡å¯¹è¯¥æ¨¡å‹çš„æ•°å€¼å’Œè§†è§‰è§£é‡Šåˆ†æï¼Œç ”ç©¶å‘ç°å­¦ç”Ÿçš„æ—¶é—´ç®¡ç†èƒ½åŠ›ã€ä¸“æ³¨ç¨‹åº¦ã€å¯¹åŒå­¦çš„æ„ŸçŸ¥å¸®åŠ©åº¦ä»¥åŠçº¿ä¸‹è¯¾ç¨‹çš„å‚ä¸åº¦æ˜¯æå‡å­¦ä¹ æ»¡æ„åº¦æœ€æ˜¾è‘—çš„æ­£å‘å› ç´ ã€‚ä»¤äººæ„å¤–çš„æ˜¯ï¼Œå‚ä¸åˆ›é€ æ€§æ´»åŠ¨å¹¶æœªå¯¹å­¦ä¹ æ»¡æ„åº¦äº§ç”Ÿç§¯æå½±å“ã€‚æ­¤å¤–ï¼ŒAPLR æä¾›çš„ä¸ªä½“åŒ–è§£é‡ŠåŠŸèƒ½ä½¿æ•™è‚²å·¥ä½œè€…èƒ½å¤Ÿé’ˆå¯¹ä¸åŒå­¦ç”Ÿç”»åƒå®šåˆ¶æ•™å­¦ç­–ç•¥ï¼Œæœ‰æ•ˆæå‡äº†é¢„æµ‹æ¨¡å‹åœ¨æ•™è‚²å®è·µä¸­çš„åº”ç”¨ä»·å€¼ã€‚",
      "categories": [
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.10639v1",
      "published_date": "2025-10-12 14:48:50 UTC",
      "updated_date": "2025-10-12 14:48:50 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T02:52:47.176015+00:00"
    },
    {
      "arxiv_id": "2510.10633v1",
      "title": "Collaborative Text-to-Image Generation via Multi-Agent Reinforcement Learning and Semantic Fusion",
      "title_zh": "åŸºäºå¤šæ™ºèƒ½ä½“å¼ºåŒ–å­¦ä¹ ä¸è¯­ä¹‰èåˆçš„ååŒæ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆ",
      "authors": [
        "Jiabao Shi",
        "Minfeng Qi",
        "Lefeng Zhang",
        "Di Wang",
        "Yingjie Zhao",
        "Ziying Li",
        "Yalong Xing",
        "Ningran Li"
      ],
      "abstract": "Multimodal text-to-image generation remains constrained by the difficulty of maintaining semantic alignment and professional-level detail across diverse visual domains. We propose a multi-agent reinforcement learning framework that coordinates domain-specialized agents (e.g., focused on architecture, portraiture, and landscape imagery) within two coupled subsystems: a text enhancement module and an image generation module, each augmented with multimodal integration components. Agents are trained using Proximal Policy Optimization (PPO) under a composite reward function that balances semantic similarity, linguistic visual quality, and content diversity. Cross-modal alignment is enforced through contrastive learning, bidirectional attention, and iterative feedback between text and image. Across six experimental settings, our system significantly enriches generated content (word count increased by 1614%) while reducing ROUGE-1 scores by 69.7%. Among fusion methods, Transformer-based strategies achieve the highest composite score (0.521), despite occasional stability issues. Multimodal ensembles yield moderate consistency (ranging from 0.444 to 0.481), reflecting the persistent challenges of cross-modal semantic grounding. These findings underscore the promise of collaborative, specialization-driven architectures for advancing reliable multimodal generative systems.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤šæ¨¡æ€ Text-to-Image ç”Ÿæˆä¸­éš¾ä»¥ç»´æŒè¯­ä¹‰å¯¹é½å’Œä¸“ä¸šçº§ç»†èŠ‚çš„é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§åŸºäº Multi-Agent Reinforcement Learning å’Œ Semantic Fusion çš„åä½œæ¡†æ¶ã€‚è¯¥æ¡†æ¶åè°ƒäº†ä¸“æ³¨äºå»ºç­‘ã€è‚–åƒå’Œé£æ™¯ç­‰é¢†åŸŸçš„ä¸“ç”¨æ™ºèƒ½ä½“ï¼Œé€šè¿‡æ–‡æœ¬å¢å¼ºå’Œå›¾åƒç”Ÿæˆä¸¤ä¸ªè€¦åˆå­ç³»ç»Ÿå®ç°å¤šæ¨¡æ€é›†æˆã€‚æ™ºèƒ½ä½“é‡‡ç”¨ Proximal Policy Optimization (PPO) è¿›è¡Œè®­ç»ƒï¼Œåˆ©ç”¨ç»¼åˆå¥–åŠ±å‡½æ•°å¹³è¡¡è¯­ä¹‰ç›¸ä¼¼åº¦ã€è§†è§‰è´¨é‡ä¸å†…å®¹å¤šæ ·æ€§ï¼Œå¹¶ç»“åˆ Contrastive Learningã€Bidirectional Attention å’Œè¿­ä»£åé¦ˆæ¥å¼ºåŒ–è·¨æ¨¡æ€å¯¹é½ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥ç³»ç»Ÿæ˜¾è‘—ä¸°å¯Œäº†ç”Ÿæˆå†…å®¹ï¼Œå•è¯è®¡æ•°å¢åŠ äº†1614%ï¼ŒåŒæ—¶ ROUGE-1 åˆ†æ•°é™ä½äº†69.7%ã€‚åœ¨å„ç±»èåˆæ–¹æ³•ä¸­ï¼ŒåŸºäº Transformer çš„ç­–ç•¥å–å¾—äº† 0.521 çš„æœ€é«˜ç»¼åˆå¾—åˆ†ï¼Œè€Œå¤šæ¨¡æ€é›†æˆåˆ™å±•ç°äº†ä¸­ç­‰ç¨‹åº¦çš„ä¸€è‡´æ€§ï¼ˆ0.444è‡³0.481ï¼‰ã€‚è¿™äº›ç ”ç©¶ç»“æœå¼ºè°ƒäº†ç”±ä¸“ä¸šåŒ–é©±åŠ¨çš„åä½œæ¶æ„åœ¨æå‡å¯é å¤šæ¨¡æ€ç”Ÿæˆç³»ç»Ÿæ–¹é¢çš„å·¨å¤§æ½œåŠ›ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "16 pages, 13 figures",
      "pdf_url": "https://arxiv.org/pdf/2510.10633v1",
      "published_date": "2025-10-12 14:29:32 UTC",
      "updated_date": "2025-10-12 14:29:32 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T02:52:47.691662+00:00"
    },
    {
      "arxiv_id": "2510.10619v1",
      "title": "A Machine Learning Approach for MIDI to Guitar Tablature Conversion",
      "title_zh": "ä¸€ç§åŸºäºæœºå™¨å­¦ä¹ çš„ MIDI åˆ°å‰ä»–å…­çº¿è°±è½¬æ¢æ–¹æ³•",
      "authors": [
        "Maximos Kaliakatsos-Papakostas",
        "Gregoris Bastas",
        "Dimos Makris",
        "Dorien Herremans",
        "Vassilis Katsouros",
        "Petros Maragos"
      ],
      "abstract": "Guitar tablature transcription consists in deducing the string and the fret number on which each note should be played to reproduce the actual musical part. This assignment should lead to playable string-fret combinations throughout the entire track and, in general, preserve parsimonious motion between successive combinations. Throughout the history of guitar playing, specific chord fingerings have been developed across different musical styles that facilitate common idiomatic voicing combinations and motion between them. This paper presents a method for assigning guitar tablature notation to a given MIDI-based musical part (possibly consisting of multiple polyphonic tracks), i.e. no information about guitar-idiomatic expressional characteristics is involved (e.g. bending etc.) The current strategy is based on machine learning and requires a basic assumption about how much fingers can stretch on a fretboard; only standard 6-string guitar tuning is examined. The proposed method also examines the transcription of music pieces that was not meant to be played or could not possibly be played by a guitar (e.g. potentially a symphonic orchestra part), employing a rudimentary method for augmenting musical information and training/testing the system with artificial data. The results present interesting aspects about what the system can achieve when trained on the initial and augmented dataset, showing that the training with augmented data improves the performance even in simple, e.g. monophonic, cases. Results also indicate weaknesses and lead to useful conclusions about possible improvements.",
      "tldr_zh": "æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§åŸºäºæœºå™¨å­¦ä¹  (Machine Learning) çš„æ–¹æ³•ï¼Œæ—¨åœ¨å°† MIDI æ ¼å¼çš„éŸ³ä¹è‡ªåŠ¨è½¬æ¢ä¸ºå‰ä»–å…­çº¿è°± (Guitar Tablature)ã€‚è¯¥ç³»ç»Ÿçš„æ ¸å¿ƒä»»åŠ¡æ˜¯æ ¹æ®éŸ³ç¬¦æ¨å¯¼æœ€åˆé€‚çš„å¼¦å’Œå“ä½ç»„åˆï¼Œåœ¨ç¡®ä¿æ¼”å¥å¯è¡Œæ€§çš„åŒæ—¶ï¼Œæœ€å¤§é™åº¦åœ°å‡å°‘è¿ç»­åŠ¨ä½œé—´çš„ä½ç§»ã€‚ç ”ç©¶é’ˆå¯¹æ ‡å‡† 6 å¼¦å‰ä»–è°ƒå¼¦ï¼Œå¹¶åŸºäºæ‰‹æŒ‡åœ¨æŒ‡æ¿ä¸Šçš„ä¼¸å±•æé™è®¾å®šäº†çº¦æŸæ¡ä»¶ã€‚ä¸ºäº†å¤„ç†é‚£äº›å¹¶éä¸ºå‰ä»–è®¾è®¡çš„å¤æ‚ä¹æ›²ï¼Œç ”ç©¶é‡‡ç”¨äº†ä¸€ç§åŸºç¡€çš„æ•°æ®å¢å¼º (Data Augmentation) æ–¹æ³•ï¼Œåˆ©ç”¨äººå·¥åˆæˆæ•°æ®å¯¹ç³»ç»Ÿè¿›è¡Œè®­ç»ƒã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒåŠ å…¥å¢å¼ºæ•°æ®åçš„ç³»ç»Ÿåœ¨è½¬è°±å‡†ç¡®ç‡ä¸Šæ˜¾è‘—æå‡ï¼Œå³ä½¿åœ¨å¤„ç†å•éŸ³ (Monophonic) ç­‰ç®€å•æ¡ˆä¾‹æ—¶ä¹Ÿè¡¨ç°æ›´ä½³ã€‚å°½ç®¡ç›®å‰ä»å­˜åœ¨ä¸€å®šçš„å±€é™æ€§ï¼Œä½†è¯¥ç ”ç©¶ä¸ºæå‡è®¡ç®—æœºè¾…åŠ©å‰ä»–è½¬è°±çš„æ€§èƒ½æä¾›äº†æœ‰ä»·å€¼çš„å‚è€ƒã€‚",
      "categories": [
        "cs.SD",
        "cs.AI"
      ],
      "primary_category": "cs.SD",
      "comment": "Proceedings of the 19th Sound and Music Computing Conference, June 5-12th, 2022, Saint-Ã‰tienne (France)",
      "pdf_url": "https://arxiv.org/pdf/2510.10619v1",
      "published_date": "2025-10-12 14:01:01 UTC",
      "updated_date": "2025-10-12 14:01:01 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T02:52:49.686294+00:00"
    },
    {
      "arxiv_id": "2510.10613v2",
      "title": "Dynamic Topic Evolution with Temporal Decay and Attention in Large Language Models",
      "title_zh": "å¤§è¯­è¨€æ¨¡å‹ä¸­ç»“åˆæ—¶é—´è¡°å‡ä¸æ³¨æ„åŠ›æœºåˆ¶çš„åŠ¨æ€ä¸»é¢˜æ¼”åŒ–",
      "authors": [
        "Di Wu",
        "Shuaidong Pan"
      ],
      "abstract": "This paper proposes a modeling framework for dynamic topic evolution based on temporal large language models. The method first uses a large language model to obtain contextual embeddings of text and then introduces a temporal decay function and an attention mechanism. These components allow the model to adjust the importance of semantic units according to time intervals and capture topic variations across different periods. The temporal representations are then mapped into a latent topic space, where a state transition matrix is applied to describe the dynamic evolution of topics. A joint optimization objective constrains both semantic modeling and temporal consistency, ensuring diversity and smoothness in topic generation. The design emphasizes the unified modeling of semantic representation and temporal evolution, which improves topic coherence and diversity while enhancing stability and interpretability over time. Experiments on real-world corpora show that the framework effectively captures the generation, expansion, and decline of topics and outperforms existing models across multiple metrics. Overall, the proposed method provides a systematic solution for understanding dynamic semantic patterns in large-scale text, enriches the research paradigm of topic modeling, and supports complex text analysis tasks in multiple domains.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§åŸºäºæ—¶é—´æ€§å¤§è¯­è¨€æ¨¡å‹(Large Language Models)çš„åŠ¨æ€ä¸»é¢˜æ¼”åŒ–å»ºæ¨¡æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³å¤§è§„æ¨¡æ–‡æœ¬ä¸­çš„åŠ¨æ€è¯­ä¹‰åˆ†æé—®é¢˜ã€‚è¯¥æ–¹æ³•åˆ©ç”¨å¤§è¯­è¨€æ¨¡å‹è·å–ä¸Šä¸‹æ–‡åµŒå…¥(contextual embeddings)ï¼Œå¹¶å¼•å…¥æ—¶é—´è¡°å‡å‡½æ•°(temporal decay function)ä¸æ³¨æ„åŠ›æœºåˆ¶(attention mechanism)ï¼Œé€šè¿‡åŠ¨æ€è°ƒæ•´è¯­ä¹‰å•å…ƒæƒé‡æ¥ç²¾ç¡®æ•æ‰ä¸åŒæ—¶æœŸçš„ä¸»é¢˜å˜è¿ã€‚ç ”ç©¶å°†æ—¶é—´è¡¨ç¤ºæ˜ å°„è‡³æ½œåœ¨ä¸»é¢˜ç©ºé—´(latent topic space)ï¼Œå¹¶åˆ©ç”¨çŠ¶æ€è½¬ç§»çŸ©é˜µ(state transition matrix)æè¿°ä¸»é¢˜æ¼”åŒ–è¿‡ç¨‹ï¼ŒåŒæ—¶é€šè¿‡è”åˆä¼˜åŒ–ç›®æ ‡ç¡®ä¿è¯­ä¹‰å»ºæ¨¡ä¸æ—¶é—´ä¸€è‡´æ€§ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯¥æ¡†æ¶åœ¨æ•æ‰ä¸»é¢˜çš„äº§ç”Ÿã€æ‰©å¼ ä¸è¡°é€€æ–¹é¢æ˜¾è‘—ä¼˜äºç°æœ‰æ¨¡å‹ï¼Œæœ‰æ•ˆæå‡äº†ä¸»é¢˜çš„è¿è´¯æ€§(coherence)ä¸å¤šæ ·æ€§(diversity)ã€‚è¯¥ç ”ç©¶ä¸ä»…å¢å¼ºäº†æ¨¡å‹åœ¨å¤„ç†å¤æ‚æ–‡æœ¬ä»»åŠ¡æ—¶çš„ç¨³å®šæ€§å’Œå¯è§£é‡Šæ€§(interpretability)ï¼Œä¹Ÿä¸ºåŠ¨æ€ä¸»é¢˜å»ºæ¨¡æä¾›äº†å…¨æ–°çš„ç ”ç©¶èŒƒå¼ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.10613v2",
      "published_date": "2025-10-12 13:50:41 UTC",
      "updated_date": "2025-11-03 02:01:32 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T02:52:55.578667+00:00"
    },
    {
      "arxiv_id": "2510.15968v1",
      "title": "Self-Attention to Operator Learning-based 3D-IC Thermal Simulation",
      "title_zh": "åŸºäºè‡ªæ³¨æ„åŠ›ç®—å­å­¦ä¹ çš„ 3D-IC çƒ­ä»¿çœŸ",
      "authors": [
        "Zhen Huang",
        "Hong Wang",
        "Wenkai Yang",
        "Muxi Tang",
        "Depeng Xie",
        "Ting-Jung Lin",
        "Yu Zhang",
        "Wei W. Xing",
        "Lei He"
      ],
      "abstract": "Thermal management in 3D ICs is increasingly challenging due to higher power densities. Traditional PDE-solving-based methods, while accurate, are too slow for iterative design. Machine learning approaches like FNO provide faster alternatives but suffer from high-frequency information loss and high-fidelity data dependency. We introduce Self-Attention U-Net Fourier Neural Operator (SAU-FNO), a novel framework combining self-attention and U-Net with FNO to capture long-range dependencies and model local high-frequency features effectively. Transfer learning is employed to fine-tune low-fidelity data, minimizing the need for extensive high-fidelity datasets and speeding up training. Experiments demonstrate that SAU-FNO achieves state-of-the-art thermal prediction accuracy and provides an 842x speedup over traditional FEM methods, making it an efficient tool for advanced 3D IC thermal simulations.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ 3D IC åŠŸç‡å¯†åº¦å¢åŠ å¸¦æ¥çš„çƒ­ç®¡ç†æŒ‘æˆ˜ï¼ŒæŒ‡å‡ºä¼ ç»Ÿçš„åŸºäºåå¾®åˆ†æ–¹ç¨‹(PDE)æ±‚è§£çš„æ–¹æ³•åœ¨è¿­ä»£è®¾è®¡ä¸­é€Ÿåº¦è¿‡æ…¢çš„é—®é¢˜ã€‚ä¸ºäº†å…‹æœ FNO ç­‰æœºå™¨å­¦ä¹ æ–¹æ³•åœ¨é«˜é¢‘ä¿¡æ¯ä¸¢å¤±å’Œé«˜ä¿çœŸ(high-fidelity)æ•°æ®ä¾èµ–æ–¹é¢çš„å±€é™æ€§ï¼Œä½œè€…æå‡ºäº† SAU-FNOï¼ˆSelf-Attention U-Net Fourier Neural Operatorï¼‰æ¡†æ¶ã€‚è¯¥æ¡†æ¶é€šè¿‡ç»“åˆè‡ªæ³¨æ„åŠ›(self-attention)å’Œ U-Net ç»“æ„ï¼Œå¢å¼ºäº† FNO æ•æ‰é•¿è·ç¦»ä¾èµ–å’Œå±€éƒ¨é«˜é¢‘ç‰¹å¾çš„èƒ½åŠ›ã€‚ç ”ç©¶è¿˜å¼•å…¥äº†è¿ç§»å­¦ä¹ (transfer learning)æŠ€æœ¯ï¼Œé€šè¿‡å¾®è°ƒä½ä¿çœŸæ•°æ®å¤§å¹…é™ä½äº†å¯¹é«˜ä¿çœŸæ•°æ®é›†çš„éœ€æ±‚å¹¶ç¼©çŸ­äº†è®­ç»ƒæ—¶é—´ã€‚å®éªŒè¯æ˜ï¼ŒSAU-FNO åœ¨çƒ­é¢„æµ‹å‡†ç¡®æ€§ä¸Šè¾¾åˆ°äº†å½“å‰é¢†å…ˆæ°´å¹³ï¼Œä¸”ç›¸æ¯”ä¼ ç»Ÿæœ‰é™å…ƒåˆ†æ(FEM)æ–¹æ³•å®ç°äº† 842 å€çš„åŠ é€Ÿã€‚è¯¥æ¡†æ¶ä¸ºå…ˆè¿›çš„ 3D IC çƒ­ä»¿çœŸæä¾›äº†ä¸€ä¸ªå…¼å…·é«˜ç²¾åº¦ä¸é«˜æ•ˆç‡çš„å·¥å…·ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.AR"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.15968v1",
      "published_date": "2025-10-12 13:44:42 UTC",
      "updated_date": "2025-10-12 13:44:42 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T02:53:47.593118+00:00"
    },
    {
      "arxiv_id": "2510.10603v2",
      "title": "EA4LLM: A Gradient-Free Approach to Large Language Model Optimization via Evolutionary Algorithms",
      "title_zh": "EA4LLMï¼šåŸºäºè¿›åŒ–ç®—æ³•çš„å¤§è¯­è¨€æ¨¡å‹æ— æ¢¯åº¦ä¼˜åŒ–æ–¹æ³•",
      "authors": [
        "WenTao Liu",
        "Siyu Song",
        "Hao Hao",
        "Aimin Zhou"
      ],
      "abstract": "In recent years, large language models (LLMs) have made remarkable progress, with model optimization primarily relying on gradient-based optimizers such as Adam. However, these gradient-based methods impose stringent hardware requirements, demanding high-concurrency, high-memory GPUs. Moreover, they require all neural network operations to be differentiable, thereby excluding many promising non-differentiable architectures from practical use. To address these limitations, we propose EA4LLM, an evolutionary algorithm for optimizing LLMs, and, for the first time, empirically verify full-parameter optimization from the pretraining stage across model sizes ranging from 0.5B to 32B. We conduct extensive experiments and provide key insights into how evolutionary algorithms can effectively optimize neural networks. Our work challenges the prevailing assumption that gradient-based optimization is the only viable approach for training neural networks. It also holds significant potential to reduce the computational cost of training large language models, thereby enabling groups with limited computational resources to participate in deep learning research.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† EA4LLMï¼Œä¸€ç§ç”¨äºä¼˜åŒ– Large Language Models (LLMs) çš„æ¢¯åº¦æ— å…³ Evolutionary Algorithms æ–¹æ³•ï¼Œæ—¨åœ¨è§£å†³ä¼ ç»Ÿæ¢¯åº¦ä¼˜åŒ–å™¨ï¼ˆå¦‚ Adamï¼‰å¯¹ç¡¬ä»¶æ˜¾å­˜çš„é«˜è¦æ±‚ä»¥åŠç¥ç»ç½‘ç»œæ“ä½œå¿…é¡»å¯å¾®åˆ†çš„å±€é™æ€§ã€‚EA4LLM é¦–æ¬¡åœ¨ 0.5B è‡³ 32B çš„æ¨¡å‹è§„æ¨¡ä¸Šç»éªŒæ€§åœ°éªŒè¯äº†ä»é¢„è®­ç»ƒé˜¶æ®µå¼€å§‹çš„å…¨å‚æ•°ä¼˜åŒ–ï¼Œå¹¶æä¾›äº†è¿›åŒ–ç®—æ³•å¦‚ä½•æœ‰æ•ˆä¼˜åŒ–å¤§è§„æ¨¡ç¥ç»ç½‘ç»œçš„å…³é”®æ´å¯Ÿã€‚å®éªŒç»“æœæŒ‘æˆ˜äº†æ¢¯åº¦ä¼˜åŒ–æ˜¯è®­ç»ƒç¥ç»ç½‘ç»œå”¯ä¸€å¯è¡Œè·¯å¾„çš„ä¸»æµå‡è®¾ï¼Œè¯æ˜äº†éå¾®åˆ†æ¶æ„åœ¨å®é™…åº”ç”¨ä¸­çš„æ½œåŠ›ã€‚è¯¥æ–¹æ³•æ˜¾è‘—é™ä½äº†è®­ç»ƒå¤§å‹è¯­è¨€æ¨¡å‹çš„è®¡ç®—æˆæœ¬ï¼Œä¸ºè®¡ç®—èµ„æºæœ‰é™çš„ç ”ç©¶å›¢ä½“å‚ä¸æ·±åº¦å­¦ä¹ å‰æ²¿ç ”ç©¶å¼€è¾Ÿäº†æ–°è·¯å¾„ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.10603v2",
      "published_date": "2025-10-12 13:38:28 UTC",
      "updated_date": "2025-10-23 04:20:30 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T02:53:52.877540+00:00"
    },
    {
      "arxiv_id": "2510.15967v1",
      "title": "Gains: Fine-grained Federated Domain Adaptation in Open Set",
      "title_zh": "Gainsï¼šå¼€é›†ç¯å¢ƒä¸‹çš„ç»†ç²’åº¦è”é‚¦é¢†åŸŸè‡ªé€‚åº”",
      "authors": [
        "Zhengyi Zhong",
        "Wenzheng Jiang",
        "Weidong Bao",
        "Ji Wang",
        "Cheems Wang",
        "Guanbo Wang",
        "Yongheng Deng",
        "Ju Ren"
      ],
      "abstract": "Conventional federated learning (FL) assumes a closed world with a fixed total number of clients. In contrast, new clients continuously join the FL process in real-world scenarios, introducing new knowledge. This raises two critical demands: detecting new knowledge, i.e., knowledge discovery, and integrating it into the global model, i.e., knowledge adaptation. Existing research focuses on coarse-grained knowledge discovery, and often sacrifices source domain performance and adaptation efficiency. To this end, we propose a fine-grained federated domain adaptation approach in open set (Gains). Gains splits the model into an encoder and a classifier, empirically revealing features extracted by the encoder are sensitive to domain shifts while classifier parameters are sensitive to class increments. Based on this, we develop fine-grained knowledge discovery and contribution-driven aggregation techniques to identify and incorporate new knowledge. Additionally, an anti-forgetting mechanism is designed to preserve source domain performance, ensuring balanced adaptation. Experimental results on multi-domain datasets across three typical data-shift scenarios demonstrate that Gains significantly outperforms other baselines in performance for both source-domain and target-domain clients. Code is available at: https://github.com/Zhong-Zhengyi/Gains.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ä¼ ç»Ÿè”é‚¦å­¦ä¹ (Federated Learning, FL)åœ¨å¼€æ”¾ç¯å¢ƒä¸‹éš¾ä»¥æœ‰æ•ˆå¤„ç†æ–°å®¢æˆ·ç«¯å¼•å…¥çš„æ–°çŸ¥è¯†è¿™ä¸€æŒ‘æˆ˜ï¼Œæå‡ºäº†åä¸ºGainsçš„ç»†ç²’åº¦è”é‚¦åŸŸè‡ªé€‚åº”(Fine-grained Federated Domain Adaptation)æ–¹æ³•ã€‚Gainså°†æ¨¡å‹æ‹†åˆ†ä¸ºç¼–ç å™¨(Encoder)å’Œåˆ†ç±»å™¨(Classifier)ï¼Œåˆ©ç”¨ç¼–ç å™¨ç‰¹å¾å¯¹åŸŸåç§»(Domain Shifts)æ•æ„Ÿè€Œåˆ†ç±»å™¨å‚æ•°å¯¹ç±»åˆ«å¢é‡(Class Increments)æ•æ„Ÿçš„ç»éªŒç‰¹æ€§ï¼Œå®ç°äº†ç²¾å‡†çš„çŸ¥è¯†å‘ç°ã€‚è¯¥æ¡†æ¶ç»“åˆäº†ç»†ç²’åº¦çŸ¥è¯†å‘ç°æŠ€æœ¯ä¸è´¡çŒ®é©±åŠ¨çš„èšåˆç­–ç•¥ï¼Œæ—¨åœ¨é«˜æ•ˆè¯†åˆ«å¹¶æ•´åˆæ–°çŸ¥è¯†ã€‚ä¸ºäº†ç¡®ä¿å¹³è¡¡çš„è‡ªé€‚åº”è¿‡ç¨‹ï¼Œç ”ç©¶è¿˜è®¾è®¡äº†æŠ—é—å¿˜æœºåˆ¶(Anti-forgetting Mechanism)ä»¥ç»´æŠ¤æºåŸŸæ€§èƒ½ã€‚å®éªŒç»“æœè¯æ˜ï¼ŒGainsåœ¨å¤šç§æ•°æ®åç§»åœºæ™¯ä¸‹çš„æºåŸŸå’Œç›®æ ‡åŸŸè¡¨ç°å‡ä¼˜äºç°æœ‰åŸºå‡†æ¨¡å‹ï¼Œæœ‰æ•ˆè§£å†³äº†å¼€æ”¾é›†ç¯å¢ƒä¸‹çš„çŸ¥è¯†å‘ç°ä¸æ•´åˆéš¾é¢˜ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "Accepted by NeurIPS2025",
      "pdf_url": "https://arxiv.org/pdf/2510.15967v1",
      "published_date": "2025-10-12 13:38:11 UTC",
      "updated_date": "2025-10-12 13:38:11 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T02:53:55.907786+00:00"
    },
    {
      "arxiv_id": "2510.10596v1",
      "title": "A Distance Measure for Random Permutation Set: From the Layer-2 Belief Structure Perspective",
      "title_zh": "éšæœºç½®æ¢é›†çš„è·ç¦»åº¦é‡ï¼šåŸºäºäºŒå±‚è¯æ®ç»“æ„è§†è§’",
      "authors": [
        "Ruolan Cheng",
        "Yong Deng",
        "SerafÃ­n Moral",
        "JosÃ© RamÃ³n Trillo"
      ],
      "abstract": "Random permutation set (RPS) is a recently proposed framework designed to represent order-structured uncertain information. Measuring the distance between permutation mass functions is a key research topic in RPS theory (RPST). This paper conducts an in-depth analysis of distances between RPSs from two different perspectives: random finite set (RFS) and transferable belief model (TBM). Adopting the layer-2 belief structure interpretation of RPS, we regard RPST as a refinement of TBM, where the order in the ordered focus set represents qualitative propensity. Starting from the permutation, we introduce a new definition of the cumulative Jaccard index to quantify the similarity between two permutations and further propose a distance measure method for RPSs based on the cumulative Jaccard index matrix. The metric and structural properties of the proposed distance measure are investigated, including the positive definiteness analysis of the cumulative Jaccard index matrix, and a correction scheme is provided. The proposed method has a natural top-weightiness property: inconsistencies between higher-ranked elements tend to result in greater distance values. Two parameters are provided to the decision-maker to adjust the weight and truncation depth. Several numerical examples are used to compare the proposed method with the existing method. The experimental results show that the proposed method not only overcomes the shortcomings of the existing method and is compatible with the Jousselme distance, but also has higher sensitivity and flexibility.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†éšæœºæ’åˆ—é›† (Random Permutation Set, RPS) ç†è®ºä¸­æ’åˆ—è´¨é‡å‡½æ•°çš„è·ç¦»åº¦é‡é—®é¢˜ï¼Œä»éšæœºæœ‰é™é›† (Random Finite Set, RFS) å’Œå¯è½¬ç§»ä¿¡å¿µæ¨¡å‹ (Transferable Belief Model, TBM) çš„åŒé‡è§’åº¦è¿›è¡Œäº†æ·±å…¥åˆ†æã€‚åŸºäº RPS çš„ç¬¬äºŒå±‚ä¿¡å¿µç»“æ„ (layer-2 belief structure) è§†è§’ï¼Œç ”ç©¶å°† RPS è§†ä¸º TBM çš„ä¸€ç§æ”¹è¿›ï¼Œå¹¶å¼•å…¥äº†ç´¯ç§¯ Jaccard æŒ‡æ•° (cumulative Jaccard index) æ¥é‡åŒ–ä¸¤ä¸ªæ’åˆ—ä¹‹é—´çš„ç›¸ä¼¼æ€§ã€‚ä»¥æ­¤ä¸ºåŸºç¡€ï¼Œè®ºæ–‡æå‡ºäº†ä¸€ç§åŸºäºç´¯ç§¯ Jaccard æŒ‡æ•°çŸ©é˜µçš„ RPS è·ç¦»åº¦é‡æ–¹æ³•ï¼Œå¹¶å¯¹å…¶åº¦é‡æ€§è´¨å’Œç»“æ„ç‰¹æ€§è¿›è¡Œäº†è¯¦ç»†ç ”ç©¶ä¸ä¿®æ­£ã€‚è¯¥æ–¹æ³•å…·æœ‰å¤©ç„¶çš„é¡¶éƒ¨åŠ æƒ (top-weightiness) ç‰¹æ€§ï¼Œå³æ’åè¾ƒé«˜å…ƒç´ çš„å·®å¼‚ä¼šå¯¹è·ç¦»å€¼äº§ç”Ÿæ›´å¤§å½±å“ï¼Œå¹¶å…è®¸å†³ç­–è€…è°ƒæ•´æƒé‡å’Œæˆªæ–­æ·±åº¦ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨ä¿æŒä¸ Jousselme è·ç¦»å…¼å®¹çš„åŒæ—¶ï¼Œæœ‰æ•ˆå…‹æœäº†ç°æœ‰æ–¹æ³•çš„ç¼ºé™·ï¼Œå±•ç°å‡ºæ›´é«˜çš„çµæ•åº¦å’Œçµæ´»æ€§ã€‚",
      "categories": [
        "cs.AI",
        "cs.IT"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.10596v1",
      "published_date": "2025-10-12 13:29:23 UTC",
      "updated_date": "2025-10-12 13:29:23 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T02:53:56.981022+00:00"
    },
    {
      "arxiv_id": "2510.14997v1",
      "title": "Evaluation and Implementation of Machine Learning Algorithms to Predict Early Detection of Kidney and Heart Disease in Diabetic Patients",
      "title_zh": "ç³–å°¿ç—…æ‚£è€…è‚¾è„ä¸å¿ƒè„ç–¾ç—…æ—©æœŸæ£€æµ‹é¢„æµ‹çš„æœºå™¨å­¦ä¹ ç®—æ³•è¯„ä¼°ä¸å®ç°",
      "authors": [
        "Syed Ibad Hasnain"
      ],
      "abstract": "Cardiovascular disease and chronic kidney disease are major complications of diabetes, leading to high morbidity and mortality. Early detection of these conditions is critical, yet traditional diagnostic markers often lack sensitivity in the initial stages. This study integrates conventional statistical methods with machine learning approaches to improve early diagnosis of CKD and CVD in diabetic patients. Descriptive and inferential statistics were computed in SPSS to explore associations between diseases and clinical or demographic factors. Patients were categorized into four groups: Group A both CKD and CVD, Group B CKD only, Group C CVD only, and Group D no disease. Statistical analysis revealed significant correlations: Serum Creatinine and Hypertension with CKD, and Cholesterol, Triglycerides, Myocardial Infarction, Stroke, and Hypertension with CVD. These results guided the selection of predictive features for machine learning models. Logistic Regression, Support Vector Machine, and Random Forest algorithms were implemented, with Random Forest showing the highest accuracy, particularly for CKD prediction. Ensemble models outperformed single classifiers in identifying high-risk diabetic patients. SPSS results further validated the significance of the key parameters integrated into the models. While challenges such as interpretability and class imbalance remain, this hybrid statistical machine learning framework offers a promising advancement toward early detection and risk stratification of diabetic complications compared to conventional diagnostic approaches.",
      "tldr_zh": "è¯¥ç ”ç©¶è¯„ä¼°å¹¶å®æ–½äº†å¤šç§ Machine Learning ç®—æ³•ï¼Œæ—¨åœ¨é€šè¿‡æ•´åˆä¼ ç»Ÿç»Ÿè®¡æ–¹æ³•ä¸æœºå™¨å­¦ä¹ æŠ€æœ¯ï¼Œå®ç°ç³–å°¿ç—…æ‚£è€…æ…¢æ€§è‚¾è„ç—… (CKD) å’Œå¿ƒè¡€ç®¡ç–¾ç—… (CVD) çš„æ—©æœŸé¢„æµ‹ã€‚ç ”ç©¶åˆ©ç”¨ SPSS è¿›è¡Œç»Ÿè®¡åˆ†æï¼Œç¡®å®šäº† Serum Creatinine å’Œ Hypertension ä¸ CKD çš„æ˜¾è‘—å…³è”ï¼Œä»¥åŠ Cholesterolã€Triglyceridesã€Myocardial Infarctionã€Stroke å’Œ Hypertension ä¸ CVD çš„ç›¸å…³æ€§ï¼Œå¹¶æ®æ­¤ç­›é€‰é¢„æµ‹ç‰¹å¾ã€‚åœ¨å¯¹æ¯” Logistic Regressionã€Support Vector Machine å’Œ Random Forest ç­‰æ¨¡å‹åï¼Œç»“æœæ˜¾ç¤º Random Forest åœ¨é¢„æµ‹ CKD æ–¹é¢å‡†ç¡®ç‡æœ€é«˜ï¼Œä¸” Ensemble models çš„æ•´ä½“è¡¨ç°ä¼˜äºå•ä¸€åˆ†ç±»å™¨ã€‚SPSS çš„åˆ†æç»“æœè¿›ä¸€æ­¥è¯å®äº†æ¨¡å‹ä¸­é›†æˆå…³é”®å‚æ•°çš„ç»Ÿè®¡å­¦æ˜¾è‘—æ€§ã€‚å°½ç®¡ç›®å‰ä»é¢ä¸´ Interpretability å’Œ Class imbalance ç­‰æŒ‘æˆ˜ï¼Œè¯¥æ··åˆç»Ÿè®¡æœºå™¨å­¦ä¹ æ¡†æ¶ç›¸æ¯”ä¼ ç»Ÿè¯Šæ–­æ–¹å¼ï¼Œåœ¨ç³–å°¿ç—…å¹¶å‘ç—‡çš„æ—©æœŸæ£€æµ‹å’Œé£é™©åˆ†å±‚æ–¹é¢å±•ç°äº†æ˜¾è‘—çš„è¿›æ­¥ã€‚",
      "categories": [
        "q-bio.OT",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "q-bio.OT",
      "comment": "This thesis was completed under the supervision of Prof. Dr. Darakhshan Saleem. I am deeply grateful for her mentorship throughout my graduate studies",
      "pdf_url": "https://arxiv.org/pdf/2510.14997v1",
      "published_date": "2025-10-12 13:28:26 UTC",
      "updated_date": "2025-10-12 13:28:26 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T02:53:58.096368+00:00"
    },
    {
      "arxiv_id": "2510.10592v1",
      "title": "A Layered Intuition -- Method Model with Scope Extension for LLM Reasoning",
      "title_zh": "é¢å‘å¤§è¯­è¨€æ¨¡å‹æ¨ç†çš„èŒƒç•´æ‰©å±•å‹ç›´è§‰-æ–¹æ³•åˆ†å±‚æ¨¡å‹",
      "authors": [
        "Hong Su"
      ],
      "abstract": "Existing studies have introduced method-based reasoning and scope extension as approaches to enhance Large Language Model (LLM) performance beyond direct matrix mappings. Building on these foundations, this paper summarizes and integrates these ideas into a unified Intuition-Method Layered Model with Scope Extension, designed to address indirected (unseen) issues more systematically. In this framework, intuition-based thinking provides rapid first-reaction answers, while method-based thinking decouples questions and solutions into transferable reasoning units. Scope extension is then applied to broaden applicability, including vertical (cause analysis), horizontal (parallel and generalized issues), and for the first time, temporal and spatial extensions, which expand reasoning across time and contextual dimensions. These extensions are organized into systematic knowledge trees that interconnect into a knowledge network, thereby increasing adaptability. To quantitatively evaluate this process, we propose the entropy of method extension, which measures the independence and diversity of extensions as an indicator of the system's capacity to solve unseen questions. By logically connecting existing approaches with new extensions and introducing an entropy-based evaluation framework, this work advances toward a more robust and extensible reasoning paradigm for LLMs in real-world problem-solving.",
      "tldr_zh": "æœ¬ç ”ç©¶æå‡ºäº† Intuition-Method Layered Model with Scope Extensionï¼Œæ—¨åœ¨ç³»ç»Ÿæ€§åœ°è§£å†³å¤§è¯­è¨€æ¨¡å‹ (LLMs) åœ¨é¢å¯¹éç›´æ¥ (unseen) é—®é¢˜æ—¶çš„æ¨ç†å±€é™æ€§ã€‚è¯¥æ¡†æ¶å°†æ¨ç†åˆ†ä¸ºåŸºäºç›´è§‰çš„å¿«é€Ÿååº” (Intuition-based thinking) å’ŒåŸºäºæ–¹æ³•çš„è§£è€¦æ¨ç†å•å…ƒ (Method-based thinking)ï¼Œæœ‰æ•ˆå®ç°äº†é—®é¢˜ä¸è§£å†³æ–¹æ¡ˆçš„å¯è¿ç§»æ€§ã€‚åœ¨æ­¤åŸºç¡€ä¸Šï¼Œç ”ç©¶å¼•å…¥äº† Scope extension æœºåˆ¶ï¼Œä¸ä»…åŒ…å«å‚ç›´ç»´åº¦çš„å› æœåˆ†æ (Vertical) å’Œæ°´å¹³ç»´åº¦çš„å¹³è¡Œæ³›åŒ– (Horizontal)ï¼Œè¿˜é¦–æ¬¡æå‡ºäº†è·¨è¶Šæ—¶é—´å’Œä¸Šä¸‹æ–‡ç»´åº¦çš„æ—¶ç©ºæ‰©å±• (Temporal and spatial extensions)ã€‚è¿™äº›æ‰©å±•å†…å®¹é€šè¿‡ç³»ç»ŸåŒ–çš„çŸ¥è¯†æ ‘ (Knowledge trees) ç»„ç»‡å¹¶äº’è¿æˆçŸ¥è¯†ç½‘ç»œ (Knowledge network)ï¼Œæ˜¾è‘—æå‡äº†æ¨¡å‹çš„é€‚åº”æ€§ã€‚ä¸ºäº†é‡åŒ–è¯„ä¼°è¯¥è¿‡ç¨‹ï¼Œè®ºæ–‡æå‡ºäº†æ–¹æ³•æ‰©å±•ç†µ (Entropy of method extension)ï¼Œé€šè¿‡æµ‹é‡æ‰©å±•çš„ç‹¬ç«‹æ€§å’Œå¤šæ ·æ€§æ¥è¡¡é‡ç³»ç»Ÿè§£å†³æœªçŸ¥é—®é¢˜çš„èƒ½åŠ›ã€‚è¯¥å·¥ä½œé€šè¿‡é€»è¾‘åŒ–æ•´åˆç°æœ‰æ–¹æ³•å¹¶å¼•å…¥å…¨æ–°çš„æ‰©å±•ç»´åº¦ä¸è¯„ä»·æ¡†æ¶ï¼Œä¸º LLMs åœ¨ç°å®ä¸–ç•Œé—®é¢˜è§£å†³ä¸­æ„å»ºäº†æ›´å…·é²æ£’æ€§å’Œå¯æ‰©å±•æ€§çš„æ¨ç†èŒƒå¼ã€‚",
      "categories": [
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.10592v1",
      "published_date": "2025-10-12 13:14:23 UTC",
      "updated_date": "2025-10-12 13:14:23 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T02:54:03.227059+00:00"
    },
    {
      "arxiv_id": "2510.10586v1",
      "title": "Compositional Symmetry as Compression: Lie Pseudogroup Structure in Algorithmic Agents",
      "title_zh": "ç»„åˆå¯¹ç§°æ€§å³å‹ç¼©ï¼šç®—æ³•æ™ºèƒ½ä½“ä¸­çš„ Lie ä¼ªç¾¤ç»“æ„",
      "authors": [
        "Giulio Ruffini"
      ],
      "abstract": "In the algorithmic (Kolmogorov) view, agents are programs that track and compress sensory streams using generative programs. We propose a framework where the relevant structural prior is simplicity (Solomonoff) understood as \\emph{compositional symmetry}: natural streams are well described by (local) actions of finite-parameter Lie pseudogroups on geometrically and topologically complex low-dimensional configuration manifolds (latent spaces). Modeling the agent as a generic neural dynamical system coupled to such streams, we show that accurate world-tracking imposes (i) \\emph{structural constraints} -- equivariance of the agent's constitutive equations and readouts -- and (ii) \\emph{dynamical constraints}: under static inputs, symmetry induces conserved quantities (Noether-style labels) in the agent dynamics and confines trajectories to reduced invariant manifolds; under slow drift, these manifolds move but remain low-dimensional. This yields a hierarchy of reduced manifolds aligned with the compositional factorization of the pseudogroup, providing a geometric account of the ``blessing of compositionality'' in deep models. We connect these ideas to the Spencer formalism for Lie pseudogroups and formulate a symmetry-based, self-contained version of predictive coding in which higher layers receive only \\emph{coarse-grained residual transformations} (prediction-error coordinates) along symmetry directions unresolved at lower layers.",
      "tldr_zh": "è¯¥ç ”ç©¶ä»ç®—æ³• Kolmogorov è§†è§’å‡ºå‘ï¼Œæå‡ºå°† compositional symmetry è§†ä¸ºä¸€ç§å‹ç¼©æœºåˆ¶ï¼Œç”¨äºå»ºæ¨¡ç®—æ³•æ™ºèƒ½ä½“å¯¹æ„Ÿå®˜ä¿¡æ¯çš„å¤„ç†è¿‡ç¨‹ã€‚ä½œè€…è®¤ä¸ºè‡ªç„¶ä¿¡æ¯æµå¯ç”±æœ‰é™å‚æ•°çš„ Lie pseudogroups åœ¨ä½ç»´ configuration manifolds ä¸Šçš„å±€éƒ¨ä½œç”¨æ¥æè¿°ï¼Œå¹¶å°†æ™ºèƒ½ä½“è§†ä¸ºä¸è¿™äº›æµè€¦åˆçš„ç¥ç»åŠ¨åŠ›å­¦ç³»ç»Ÿã€‚ç ”ç©¶è¯æ˜ï¼Œå‡†ç¡®çš„ä¸–ç•Œè¿½è¸ªè¦æ±‚æ™ºèƒ½ä½“æ»¡è¶³ equivariance çš„ç»“æ„çº¦æŸï¼Œå¹¶ç”±äºå¯¹ç§°æ€§åœ¨åŠ¨åŠ›å­¦ä¸­äº§ç”Ÿå®ˆæ’é‡ï¼Œå°†è½¨è¿¹é™åˆ¶åœ¨ä½ç»´ invariant manifolds ä¸Šã€‚è¿™å½¢æˆäº†ä¸€å¥—ä¸ pseudogroup ç»„åˆå› å­åˆ†è§£ç›¸ä¸€è‡´çš„æµå½¢å±‚çº§ï¼Œä¸ºæ·±åº¦å­¦ä¹ ä¸­çš„ compositionality ä¼˜åŠ¿æä¾›äº†å‡ ä½•è§£é‡Šã€‚æœ€åï¼Œè¯¥æ¡†æ¶ç»“åˆäº† Lie pseudogroups çš„ Spencer formalismï¼Œæå‡ºäº†ä¸€ç§æ–°å‹çš„ predictive coding æ–¹æ¡ˆï¼Œä½¿é«˜å±‚ç½‘ç»œä»…å¤„ç†ä½å±‚æœªè§£å†³çš„å¯¹ç§°æ–¹å‘ä¸Šçš„ç²—ç²’åŒ–æ®‹å·®å˜æ¢ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.IT",
        "q-bio.NC"
      ],
      "primary_category": "cs.LG",
      "comment": "Submitted to NeurReps 2025 (https://www.neurreps.org)",
      "pdf_url": "https://arxiv.org/pdf/2510.10586v1",
      "published_date": "2025-10-12 13:06:37 UTC",
      "updated_date": "2025-10-12 13:06:37 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T02:54:12.769698+00:00"
    },
    {
      "arxiv_id": "2510.10560v1",
      "title": "BitMar: Low-Bit Multimodal Fusion with Episodic Memory for Edge Devices",
      "title_zh": "BitMarï¼šé¢å‘è¾¹ç¼˜è®¾å¤‡çš„åŸºäºæƒ…æ™¯è®°å¿†çš„ä½æ¯”ç‰¹å¤šæ¨¡æ€èåˆ",
      "authors": [
        "Euhid Aman",
        "Esteban Carlin",
        "Hsing-Kuo Pao",
        "Giovanni Beltrame",
        "Ghaluh Indah Permata Sari",
        "Yie-Tarng Chen"
      ],
      "abstract": "Cross-attention transformers and other multimodal vision-language models excel at grounding and generation; however, their extensive, full-precision backbones make it challenging to deploy them on edge devices. Memory-augmented architectures enhance the utilization of past context; however, most works rarely pair them with aggressive edge-oriented quantization. We introduce BitMar, a quantized multimodal transformer that proposes an external human-like episodic memory for effective image-text generation on hardware with limited resources. BitMar utilizes 1.58-bit encoders, one for text (BitNet-style) and one for vision (DiNOv2-based), to create compact embeddings that are combined and used to query a fixed-size key-value episodic memory. During vector retrieval, the BitNet decoder applies per-layer conditioning, which increases the contextual relevance of generated content. The decoder also employs attention sinks with a sliding-window mechanism to process long or streaming inputs under tight memory budgets. The combination of per-layer conditioning and sliding-window attention achieves a strong quality-speed trade-off, delivering competitive captioning and multimodal understanding at low latency with a small model footprint. These characteristics make BitMar well-suited for edge deployment.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† BitMarï¼Œä¸€ç§é’ˆå¯¹è¾¹ç¼˜è®¾å¤‡è®¾è®¡çš„é«˜æ•ˆé‡åŒ–å¤šæ¨¡æ€ Transformer æ¶æ„ï¼Œæ—¨åœ¨è§£å†³è§†è§‰è¯­è¨€æ¨¡å‹åœ¨èµ„æºå—é™ç¡¬ä»¶ä¸Šéƒ¨ç½²çš„éš¾é¢˜ã€‚BitMar åˆ›æ–°æ€§åœ°å¼•å…¥äº†ç±»ä¼¼äººç±»çš„æƒ…èŠ‚è®°å¿† (episodic memory) æœºåˆ¶ï¼Œå¹¶é‡‡ç”¨äº† 1.58-bit çš„ BitNet é£æ ¼æ–‡æœ¬ç¼–ç å™¨ä¸åŸºäº DiNOv2 çš„è§†è§‰ç¼–ç å™¨ä»¥ç”Ÿæˆç´§å‡‘åµŒå…¥ã€‚é€šè¿‡ä½¿ç”¨å›ºå®šå¤§å°çš„é”®å€¼å¯¹è®°å¿†åº“å’Œè§£ç å™¨çš„ per-layer conditioning æŠ€æœ¯ï¼Œè¯¥æ¶æ„æ˜¾è‘—æå‡äº†ç”Ÿæˆå†…å®¹çš„ä¸Šä¸‹æ–‡ç›¸å…³æ€§ã€‚æ­¤å¤–ï¼Œè¯¥æ¨¡å‹è¿˜é›†æˆäº† attention sinks ä¸ sliding-window æœºåˆ¶ï¼Œç¡®ä¿åœ¨æä½å†…å­˜é¢„ç®—ä¸‹ä»èƒ½å¤„ç†é•¿åºåˆ—æˆ–æµå¼è¾“å…¥ã€‚å®éªŒè¯æ˜ï¼ŒBitMar åœ¨ä¿æŒæä½å»¶è¿Ÿå’Œå°æ¨¡å‹å ç”¨çš„å‰æä¸‹ï¼Œå®ç°äº†æå…·ç«äº‰åŠ›çš„ captioning å’Œå¤šæ¨¡æ€ç†è§£èƒ½åŠ›ã€‚è¿™ç§åœ¨è´¨é‡ä¸é€Ÿåº¦ä¹‹é—´çš„å¼ºå¤§æƒè¡¡ï¼Œä½¿å…¶æˆä¸ºè¾¹ç¼˜ä¾§å¤šæ¨¡æ€éƒ¨ç½²çš„ç†æƒ³é€‰æ‹©ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.CL",
      "comment": "6 pages, BabyLM Workshop, EMNLP 2025",
      "pdf_url": "https://arxiv.org/pdf/2510.10560v1",
      "published_date": "2025-10-12 11:59:41 UTC",
      "updated_date": "2025-10-12 11:59:41 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T02:54:11.384409+00:00"
    },
    {
      "arxiv_id": "2510.10549v2",
      "title": "ELAIPBench: A Benchmark for Expert-Level Artificial Intelligence Paper Understanding",
      "title_zh": "ELAIPBenchï¼šä¸“å®¶çº§äººå·¥æ™ºèƒ½è®ºæ–‡ç†è§£åŸºå‡†",
      "authors": [
        "Xinbang Dai",
        "Huikang Hu",
        "Yongrui Chen",
        "Jiaqi Li",
        "Rihui Jin",
        "Yuyang Zhang",
        "Xiaoguang Li",
        "Lifeng Shang",
        "Guilin Qi"
      ],
      "abstract": "While large language models (LLMs) excel at many domain-specific tasks, their ability to deeply comprehend and reason about full-length academic papers remains underexplored. Existing benchmarks often fall short of capturing such depth, either due to surface-level question design or unreliable evaluation metrics. To address this gap, we introduce ELAIPBench, a benchmark curated by domain experts to evaluate LLMs' comprehension of artificial intelligence (AI) research papers. Developed through an incentive-driven, adversarial annotation process, ELAIPBench features 403 multiple-choice questions from 137 papers. It spans three difficulty levels and emphasizes non-trivial reasoning rather than shallow retrieval. Our experiments show that the best-performing LLM achieves an accuracy of only 39.95%, far below human performance. Moreover, we observe that frontier LLMs equipped with a thinking mode or a retrieval-augmented generation (RAG) system fail to improve final results-even harming accuracy due to overthinking or noisy retrieval. These findings underscore the significant gap between current LLM capabilities and genuine comprehension of academic papers.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† ELAIPBenchï¼Œè¿™æ˜¯ä¸€ä¸ªç”±é¢†åŸŸä¸“å®¶ç­–åˆ’çš„ç”¨äºè¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹ (LLMs) å¯¹äººå·¥æ™ºèƒ½ (AI) ç ”ç©¶è®ºæ–‡æ·±åº¦ç†è§£èƒ½åŠ›çš„åŸºå‡†æµ‹è¯•ã€‚ELAIPBench é‡‡ç”¨æ¿€åŠ±é©±åŠ¨çš„å¯¹æŠ—æ€§æ ‡æ³¨è¿‡ç¨‹å¼€å‘ï¼ŒåŒ…å«æ¥è‡ª 137 ç¯‡è®ºæ–‡çš„ 403 é“å¤šé¡¹é€‰æ‹©é¢˜ï¼Œæ¶µç›–ä¸‰ä¸ªéš¾åº¦çº§åˆ«ï¼Œå¹¶å¼ºè°ƒéå¹³å‡¡æ¨ç†è€Œéç®€å•çš„ä¿¡æ¯æ£€ç´¢ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¡¨ç°æœ€å¥½çš„ LLM å‡†ç¡®ç‡ä»…ä¸º 39.95%ï¼Œè¿œä½äºäººç±»è¡¨ç°ã€‚æ­¤å¤–ï¼Œç ”ç©¶å‘ç°é…å¤‡æ€è€ƒæ¨¡å¼æˆ–æ£€ç´¢å¢å¼ºç”Ÿæˆ (RAG) ç³»ç»Ÿçš„å‰æ²¿ LLM æ— æ³•æ”¹å–„æœ€ç»ˆç»“æœï¼Œç”šè‡³å¯èƒ½å› è¿‡åº¦æ€è€ƒæˆ–å™ªå£°æ£€ç´¢è€Œé™ä½å‡†ç¡®ç‡ã€‚è¿™äº›å‘ç°å‡¸æ˜¾äº†å½“å‰ LLM çš„èƒ½åŠ›ä¸çœŸæ­£ç†è§£å­¦æœ¯è®ºæ–‡ä¹‹é—´ä»å­˜åœ¨æ˜¾è‘—å·®è·ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "24 pages, 21 figures",
      "pdf_url": "https://arxiv.org/pdf/2510.10549v2",
      "published_date": "2025-10-12 11:11:20 UTC",
      "updated_date": "2026-01-07 10:13:05 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T02:54:09.882729+00:00"
    },
    {
      "arxiv_id": "2510.10546v1",
      "title": "GLOFNet -- A Multimodal Dataset for GLOF Monitoring and Prediction",
      "title_zh": "GLOFNetï¼šç”¨äºå†°æ¹–æºƒå†³æ´ªæ°´ç›‘æµ‹ä¸é¢„æµ‹çš„å¤šæ¨¡æ€æ•°æ®é›†",
      "authors": [
        "Zuha Fatima",
        "Muhammad Anser Sohaib",
        "Muhammad Talha",
        "Sidra Sultana",
        "Ayesha Kanwal",
        "Nazia Perwaiz"
      ],
      "abstract": "Glacial Lake Outburst Floods (GLOFs) are rare but destructive hazards in high mountain regions, yet predictive research is hindered by fragmented and unimodal data. Most prior efforts emphasize post-event mapping, whereas forecasting requires harmonized datasets that combine visual indicators with physical precursors. We present GLOFNet, a multimodal dataset for GLOF monitoring and prediction, focused on the Shisper Glacier in the Karakoram. It integrates three complementary sources: Sentinel-2 multispectral imagery for spatial monitoring, NASA ITS_LIVE velocity products for glacier kinematics, and MODIS Land Surface Temperature records spanning over two decades. Preprocessing included cloud masking, quality filtering, normalization, temporal interpolation, augmentation, and cyclical encoding, followed by harmonization across modalities. Exploratory analysis reveals seasonal glacier velocity cycles, long-term warming of ~0.8 K per decade, and spatial heterogeneity in cryospheric conditions. The resulting dataset, GLOFNet, is publicly available to support future research in glacial hazard prediction. By addressing challenges such as class imbalance, cloud contamination, and coarse resolution, GLOFNet provides a structured foundation for benchmarking multimodal deep learning approaches to rare hazard prediction.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¨å‡ºäº†GLOFNetï¼Œä¸€ä¸ªä¸“é—¨ç”¨äºå†°å·æ¹–æºƒå†³æ´ªæ°´(GLOFs)ç›‘æµ‹ä¸é¢„æµ‹çš„å¤šæ¨¡æ€æ•°æ®é›†ï¼Œèšç„¦äºå–€å–‡æ˜†ä»‘å±±è„‰çš„Shisper Glacierã€‚ä¸ºè§£å†³ä»¥å¾€é¢„æµ‹ç ”ç©¶ä¸­æ•°æ®ç¢ç‰‡åŒ–å’Œå•ä¸€æ¨¡æ€çš„é—®é¢˜ï¼Œè¯¥æ•°æ®é›†æ•´åˆäº†ç”¨äºç©ºé—´ç›‘æµ‹çš„Sentinel-2å¤šå…‰è°±å›¾åƒã€NASA ITS_LIVEå†°å·æµé€Ÿäº§å“ä»¥åŠé•¿è¾¾äºŒåå¹´çš„MODISåœ°è¡¨æ¸©åº¦è®°å½•ã€‚ç ”ç©¶é€šè¿‡äº‘æ©è†œ(cloud masking)ã€å½’ä¸€åŒ–å’Œæ—¶é—´æ’å€¼ç­‰é¢„å¤„ç†æŠ€æœ¯ï¼Œå®ç°äº†å„æ¨¡æ€é—´çš„åè°ƒ(harmonization)ã€‚æ¢ç´¢æ€§åˆ†ææ­ç¤ºäº†å†°å·é€Ÿåº¦çš„å­£èŠ‚æ€§å¾ªç¯ã€æ¯åå¹´çº¦0.8 Kçš„é•¿æœŸå˜æš–è¶‹åŠ¿ä»¥åŠå†°å†»åœˆæ¡ä»¶çš„åœ°ç†å¼‚è´¨æ€§ã€‚GLOFNeté€šè¿‡åº”å¯¹ç±»åˆ«ä¸å¹³è¡¡å’Œäº‘å±‚æ±¡æŸ“ç­‰æŒ‘æˆ˜ï¼Œä¸ºè¯„ä¼°å¤šæ¨¡æ€æ·±åº¦å­¦ä¹ (multimodal deep learning)åœ¨ç½•è§ç¾å®³é¢„æµ‹ä¸­çš„è¡¨ç°æä¾›äº†ç»“æ„åŒ–çš„åŸºå‡†åŸºç¡€ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.10546v1",
      "published_date": "2025-10-12 11:03:47 UTC",
      "updated_date": "2025-10-12 11:03:47 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T02:54:28.276754+00:00"
    },
    {
      "arxiv_id": "2510.10544v1",
      "title": "PAC-Bayesian Reinforcement Learning Trains Generalizable Policies",
      "title_zh": "PAC-è´å¶æ–¯å¼ºåŒ–å­¦ä¹ è®­ç»ƒå¯æ³›åŒ–ç­–ç•¥",
      "authors": [
        "Abdelkrim Zitouni",
        "Mehdi Hennequin",
        "Juba Agoun",
        "Ryan Horache",
        "Nadia Kabachi",
        "Omar Rivasplata"
      ],
      "abstract": "We derive a novel PAC-Bayesian generalization bound for reinforcement learning that explicitly accounts for Markov dependencies in the data, through the chain's mixing time. This contributes to overcoming challenges in obtaining generalization guarantees for reinforcement learning, where the sequential nature of data breaks the independence assumptions underlying classical bounds. Our bound provides non-vacuous certificates for modern off-policy algorithms like Soft Actor-Critic. We demonstrate the bound's practical utility through PB-SAC, a novel algorithm that optimizes the bound during training to guide exploration. Experiments across continuous control tasks show that our approach provides meaningful confidence certificates while maintaining competitive performance.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¼ºåŒ–å­¦ä¹ (Reinforcement Learning)ä¸­ç”±äºæ•°æ®åºåˆ—æ€§å¯¼è‡´ç‹¬ç«‹æ€§å‡è®¾å¤±æ•ˆçš„é—®é¢˜ï¼Œæ¨å¯¼å¹¶æå‡ºäº†ä¸€ç§æ–°å‹çš„PAC-Bayesianæ³›åŒ–ç•Œã€‚è¯¥æ³›åŒ–ç•Œé€šè¿‡é©¬å°”å¯å¤«é“¾çš„æ··åˆæ—¶é—´(mixing time)æ˜¾å¼è€ƒè™‘äº†æ•°æ®ä¸­çš„Markovä¾èµ–å…³ç³»ï¼ŒæˆåŠŸå…‹æœäº†ä¼ ç»Ÿæ³›åŒ–ç†è®ºéš¾ä»¥å¤„ç†é¡ºåºæ•°æ®çš„æŒ‘æˆ˜ã€‚è¯¥ç•Œé™ä¸ºSoft Actor-Criticç­‰ç°ä»£ç¦»çº¿ç­–ç•¥(off-policy)ç®—æ³•æä¾›äº†éç©ºæ³›åŒ–ä¿è¯(non-vacuous certificates)ã€‚åœ¨æ­¤åŸºç¡€ä¸Šï¼Œç ”ç©¶è€…å¼€å‘äº†PB-SACç®—æ³•ï¼Œé€šè¿‡åœ¨è®­ç»ƒä¸­ç›´æ¥ä¼˜åŒ–è¯¥æ³›åŒ–ç•Œæ¥æœ‰æ•ˆå¼•å¯¼æ™ºèƒ½ä½“çš„æ¢ç´¢è¿‡ç¨‹ã€‚å®éªŒè¯æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å„ç±»è¿ç»­æ§åˆ¶ä»»åŠ¡ä¸­ä¸ä»…ä¿æŒäº†æå…·ç«äº‰åŠ›çš„æ€§èƒ½è¡¨ç°ï¼Œè¿˜æä¾›äº†å…·æœ‰å®é™…å‚è€ƒä»·å€¼çš„ç­–ç•¥æ³›åŒ–ç½®ä¿¡åº¦ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "stat.ML"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.10544v1",
      "published_date": "2025-10-12 11:02:18 UTC",
      "updated_date": "2025-10-12 11:02:18 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T02:54:40.097804+00:00"
    },
    {
      "arxiv_id": "2510.10541v1",
      "title": "Rethinking RL Evaluation: Can Benchmarks Truly Reveal Failures of RL Methods?",
      "title_zh": "é‡æ–°å®¡è§†å¼ºåŒ–å­¦ä¹ è¯„ä¼°ï¼šåŸºå‡†æµ‹è¯•èƒ½å¦çœŸå®æ­ç¤ºå¼ºåŒ–å­¦ä¹ æ–¹æ³•çš„å¤±æ•ˆï¼Ÿ",
      "authors": [
        "Zihan Chen",
        "Yiming Zhang",
        "Hengguang Zhou",
        "Zenghui Ding",
        "Yining Sun",
        "Cho-Jui Hsieh"
      ],
      "abstract": "Current benchmarks are inadequate for evaluating progress in reinforcement learning (RL) for large language models (LLMs).Despite recent benchmark gains reported for RL, we find that training on these benchmarks' training sets achieves nearly the same performance as training directly on the test sets, suggesting that the benchmarks cannot reliably separate further progress.To study this phenomenon, we introduce a diagnostic suite and the Oracle Performance Gap (OPG) metric that quantifies the performance difference between training on the train split versus the test split of a benchmark. We further analyze this phenomenon with stress tests and find that, despite strong benchmark scores, existing RL methods struggle to generalize across distribution shifts, varying levels of difficulty, and counterfactual scenarios: shortcomings that current benchmarks fail to reveal.We conclude that current benchmarks are insufficient for evaluating generalization and propose three core principles for designing more faithful benchmarks: sufficient difficulty, balanced evaluation, and distributional robustness.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†å½“å‰å¼ºåŒ–å­¦ä¹ (Reinforcement Learning, RL)åŸºå‡†æµ‹è¯•åœ¨è¯„ä¼°å¤§è¯­è¨€æ¨¡å‹(LLMs)æ–¹é¢çš„ä¸è¶³ï¼ŒæŒ‡å‡ºè¿™äº›åŸºå‡†æ— æ³•æœ‰æ•ˆåŒºåˆ†çœŸæ­£çš„æŠ€æœ¯è¿›æ­¥ã€‚ç ”ç©¶å‘ç°ï¼Œåœ¨ç°æœ‰åŸºå‡†çš„è®­ç»ƒé›†ä¸Šè¿›è¡Œè®­ç»ƒï¼Œå…¶è¡¨ç°ä¸ç›´æ¥åœ¨æµ‹è¯•é›†ä¸Šè®­ç»ƒå‡ ä¹ä¸€è‡´ï¼Œè¡¨æ˜åŸºå‡†æµ‹è¯•æœªèƒ½å¯é åœ°è¡¡é‡æ³›åŒ–èƒ½åŠ›çš„æå‡ã€‚ä¸ºæ­¤ï¼Œä½œè€…å¼•å…¥äº†ä¸€å¥—è¯Šæ–­å·¥å…·å’ŒOracle Performance Gap (OPG)æŒ‡æ ‡ï¼Œç”¨äºé‡åŒ–åŸºå‡†æµ‹è¯•ä¸­è®­ç»ƒé›†ä¸æµ‹è¯•é›†ä¹‹é—´çš„æ€§èƒ½å·®å¼‚ã€‚é€šè¿‡å‹åŠ›æµ‹è¯•è¿›ä¸€æ­¥åˆ†æå‘ç°ï¼Œå°½ç®¡ç°æœ‰RLæ–¹æ³•åœ¨åŸºå‡†åˆ†æ•°ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œä½†åœ¨é¢å¯¹åˆ†å¸ƒåç§»(distribution shifts)ã€ä¸åŒéš¾åº¦çº§åˆ«å’Œåäº‹å®åœºæ™¯(counterfactual scenarios)æ—¶ä»éš¾ä»¥æ³›åŒ–ã€‚æœ€åï¼Œç ”ç©¶æ€»ç»“è®¤ä¸ºç›®å‰çš„åŸºå‡†æµ‹è¯•ä¸è¶³ä»¥è¯„ä¼°æ³›åŒ–æ€§ï¼Œå¹¶æå‡ºäº†è®¾è®¡æ›´å¯é åŸºå‡†çš„ä¸‰ä¸ªæ ¸å¿ƒåŸåˆ™ï¼Œå³è¶³å¤Ÿçš„éš¾åº¦(sufficient difficulty)ã€å¹³è¡¡çš„è¯„ä¼°(balanced evaluation)å’Œåˆ†å¸ƒé²æ£’æ€§(distributional robustness)ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.10541v1",
      "published_date": "2025-10-12 10:49:57 UTC",
      "updated_date": "2025-10-12 10:49:57 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T02:54:45.663538+00:00"
    },
    {
      "arxiv_id": "2510.11752v1",
      "title": "Fast and Interpretable Protein Substructure Alignment via Optimal Transport",
      "title_zh": "åŸºäºæœ€ä¼˜ä¼ è¾“çš„å¿«é€Ÿä¸”å¯è§£é‡Šè›‹ç™½è´¨äºšç»“æ„æ¯”å¯¹",
      "authors": [
        "Zhiyu Wang",
        "Bingxin Zhou",
        "Jing Wang",
        "Yang Tan",
        "Weishu Zhao",
        "Pietro LiÃ²",
        "Liang Hong"
      ],
      "abstract": "Proteins are essential biological macromolecules that execute life functions. Local motifs within protein structures, such as active sites, are the most critical components for linking structure to function and are key to understanding protein evolution and enabling protein engineering. Existing computational methods struggle to identify and compare these local structures, which leaves a significant gap in understanding protein structures and harnessing their functions. This study presents PLASMA, the first deep learning framework for efficient and interpretable residue-level protein substructure alignment. We reformulate the problem as a regularized optimal transport task and leverage differentiable Sinkhorn iterations. For a pair of input protein structures, PLASMA outputs a clear alignment matrix with an interpretable overall similarity score. Through extensive quantitative evaluations and three biological case studies, we demonstrate that PLASMA achieves accurate, lightweight, and interpretable residue-level alignment. Additionally, we introduce PLASMA-PF, a training-free variant that provides a practical alternative when training data are unavailable. Our method addresses a critical gap in protein structure analysis tools and offers new opportunities for functional annotation, evolutionary studies, and structure-based drug design. Reproducibility is ensured via our official implementation at https://github.com/ZW471/PLASMA-Protein-Local-Alignment.git.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ç°æœ‰è®¡ç®—æ–¹æ³•éš¾ä»¥æœ‰æ•ˆè¯†åˆ«å’Œæ¯”å¯¹è›‹ç™½è´¨å±€éƒ¨ç»“æ„åŸºåºï¼ˆLocal Motifsï¼‰çš„é—®é¢˜ï¼Œæå‡ºäº†é¦–ä¸ªç”¨äºæ®‹åŸºçº§è›‹ç™½è´¨å­ç»“æ„æ¯”å¯¹çš„æ·±åº¦å­¦ä¹ æ¡†æ¶ PLASMAã€‚è¯¥æ¡†æ¶å°†æ¯”å¯¹é—®é¢˜é‡æ–°è¡¨è¿°ä¸ºæ­£åˆ™åŒ–æœ€ä¼˜ä¼ è¾“ï¼ˆOptimal Transportï¼‰ä»»åŠ¡ï¼Œå¹¶åˆ©ç”¨å¯å¾®åˆ† Sinkhorn è¿­ä»£ç”Ÿæˆæ¸…æ™°çš„æ¯”å¯¹çŸ©é˜µå’Œå¯è§£é‡Šçš„æ•´ä½“ç›¸ä¼¼æ€§è¯„åˆ†ã€‚ç ”ç©¶å›¢é˜Ÿè¿˜æ¨å‡ºäº†æ— éœ€è®­ç»ƒçš„å˜ä½“ PLASMA-PFï¼Œä¸ºç¼ºä¹è®­ç»ƒæ•°æ®çš„åœºæ™¯æä¾›äº†å®ç”¨çš„æ›¿ä»£æ–¹æ¡ˆã€‚é€šè¿‡å¤§é‡çš„å®šé‡è¯„ä¼°å’Œç”Ÿç‰©å­¦æ¡ˆä¾‹ç ”ç©¶è¯æ˜ï¼ŒPLASMA å®ç°äº†å‡†ç¡®ã€è½»é‡ä¸”å…·æœ‰è§£é‡Šæ€§çš„æ¯”å¯¹ç»“æœã€‚è¯¥æ–¹æ³•å¡«è¡¥äº†è›‹ç™½è´¨ç»“æ„åˆ†æå·¥å…·çš„å…³é”®ç©ºç™½ï¼Œä¸ºåŠŸèƒ½æ³¨é‡Šï¼ˆFunctional Annotationï¼‰ã€è¿›åŒ–ç ”ç©¶å’ŒåŸºäºç»“æ„çš„è¯ç‰©è®¾è®¡ï¼ˆDrug Designï¼‰æä¾›äº†å…¨æ–°çš„æœºé‡ã€‚",
      "categories": [
        "q-bio.QM",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "q-bio.QM",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.11752v1",
      "published_date": "2025-10-12 10:47:29 UTC",
      "updated_date": "2025-10-12 10:47:29 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T02:54:44.943369+00:00"
    },
    {
      "arxiv_id": "2510.15966v1",
      "title": "PISA: A Pragmatic Psych-Inspired Unified Memory System for Enhanced AI Agency",
      "title_zh": "PISAï¼šæ—¨åœ¨å¢å¼º AI æ™ºèƒ½ä½“è‡ªä¸»æ€§çš„å¿ƒç†å­¦å¯å‘å¼åŠ¡å®ç»Ÿä¸€è®°å¿†ç³»ç»Ÿ",
      "authors": [
        "Shian Jia",
        "Ziyang Huang",
        "Xinbo Wang",
        "Haofei Zhang",
        "Mingli Song"
      ],
      "abstract": "Memory systems are fundamental to AI agents, yet existing work often lacks adaptability to diverse tasks and overlooks the constructive and task-oriented role of AI agent memory. Drawing from Piaget's theory of cognitive development, we propose PISA, a pragmatic, psych-inspired unified memory system that addresses these limitations by treating memory as a constructive and adaptive process. To enable continuous learning and adaptability, PISA introduces a trimodal adaptation mechanism (i.e., schema updation, schema evolution, and schema creation) that preserves coherent organization while supporting flexible memory updates. Building on these schema-grounded structures, we further design a hybrid memory access architecture that seamlessly integrates symbolic reasoning with neural retrieval, significantly improving retrieval accuracy and efficiency. Our empirical evaluation, conducted on the existing LOCOMO benchmark and our newly proposed AggQA benchmark for data analysis tasks, confirms that PISA sets a new state-of-the-art by significantly enhancing adaptability and long-term knowledge retention.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†PISAï¼Œä¸€ç§å—å¿ƒç†å­¦å¯å‘çš„åŠ¡å®ç»Ÿä¸€è®°å¿†ç³»ç»Ÿï¼Œæ—¨åœ¨è§£å†³ç°æœ‰AI agentsè®°å¿†ç³»ç»Ÿåœ¨å¤šä»»åŠ¡é€‚åº”æ€§ä»¥åŠä»»åŠ¡å¯¼å‘æ€§æ–¹é¢çš„ä¸è¶³ã€‚PISAå€Ÿé‰´äº†Piagetçš„è®¤çŸ¥å‘å±•ç†è®º(cognitive development theory)ï¼Œå°†è®°å¿†è¿‡ç¨‹è§†ä¸ºä¸€ç§å»ºè®¾æ€§ä¸”å…·æœ‰é€‚åº”æ€§çš„æ¼”å˜è¿‡ç¨‹ã€‚è¯¥ç³»ç»Ÿå¼•å…¥äº†åŒ…å«schema updationã€schema evolutionå’Œschema creationçš„ä¸‰æ¨¡æ€é€‚é…æœºåˆ¶ï¼Œåœ¨ç¡®ä¿è®°å¿†ç»„ç»‡è¿è´¯æ€§çš„åŒæ—¶æ”¯æŒçµæ´»çš„æŒç»­å­¦ä¹ ã€‚æ­¤å¤–ï¼Œç ”ç©¶å›¢é˜Ÿè®¾è®¡äº†é›†æˆsymbolic reasoningä¸neural retrievalçš„æ··åˆè®°å¿†è®¿é—®æ¶æ„ï¼Œå¤§å¹…ä¼˜åŒ–äº†æ£€ç´¢çš„å‡†ç¡®ç‡ä¸æ•ˆç‡ã€‚åœ¨LOCOMOåŸºå‡†æµ‹è¯•ä»¥åŠæ–°æå‡ºçš„AggQAæ•°æ®åˆ†æåŸºå‡†ä¸Šçš„è¯„ä¼°ç»“æœè¯æ˜ï¼ŒPISAåœ¨æ˜¾è‘—æå‡æ™ºèƒ½ä½“é€‚åº”æ€§ä¸long-term knowledge retentionæ–¹é¢è¾¾åˆ°äº†SOTAæ°´å¹³ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.15966v1",
      "published_date": "2025-10-12 10:34:35 UTC",
      "updated_date": "2025-10-12 10:34:35 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T02:54:51.685743+00:00"
    },
    {
      "arxiv_id": "2510.10517v1",
      "title": "ECO: Enhanced Code Optimization via Performance-Aware Prompting for Code-LLMs",
      "title_zh": "ECOï¼šé¢å‘ä»£ç å¤§è¯­è¨€æ¨¡å‹çš„åŸºäºæ€§èƒ½æ„ŸçŸ¥æç¤ºçš„å¢å¼ºå‹ä»£ç ä¼˜åŒ–",
      "authors": [
        "Su-Hyeon Kim",
        "Joonghyuk Hahn",
        "Sooyoung Cha",
        "Yo-Sub Han"
      ],
      "abstract": "Code runtime optimization-the task of rewriting a given code to a faster one-remains challenging, as it requires reasoning about performance trade-offs involving algorithmic and structural choices. Recent approaches employ code-LLMs with slow-fast code pairs provided as optimization guidance, but such pair-based methods obscure the causal factors of performance gains and often lead to superficial pattern imitation rather than genuine performance reasoning. We introduce ECO, a performance-aware prompting framework for code optimization. ECO first distills runtime optimization instructions (ROIs) from reference slow-fast code pairs; Each ROI describes root causes of inefficiency and the rationales that drive performance improvements. For a given input code, ECO in parallel employs (i) a symbolic advisor to produce a bottleneck diagnosis tailored to the code, and (ii) an ROI retriever to return related ROIs. These two outputs are then composed into a performance-aware prompt, providing actionable guidance for code-LLMs. ECO's prompts are model-agnostic, require no fine-tuning, and can be easily prepended to any code-LLM prompt. Our empirical studies highlight that ECO prompting significantly improves code-LLMs' ability to generate efficient code, achieving speedups of up to 7.81x while minimizing correctness loss.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ECOï¼Œä¸€ç§é€šè¿‡æ€§èƒ½æ„ŸçŸ¥æç¤º(Performance-Aware Prompting)å¢å¼ºCode-LLMsä»£ç è¿è¡Œæ€§èƒ½ä¼˜åŒ–çš„æ¡†æ¶ã€‚é’ˆå¯¹ç°æœ‰æ–¹æ³•ä»…ä¾èµ–ä»£ç å¯¹å¯¼è‡´æ¨¡å‹ç¼ºä¹æ·±å±‚æ€§èƒ½æ¨ç†çš„é—®é¢˜ï¼ŒECOé¦–å…ˆä»å‚è€ƒä»£ç å¯¹ä¸­æå–è¿è¡Œæ—¶ä¼˜åŒ–æŒ‡ä»¤(Runtime Optimization Instructions, ROIs)ï¼Œæ˜ç¡®ä½æ•ˆçš„æ ¹æœ¬åŸå› åŠæ”¹è¿›é€»è¾‘ã€‚åœ¨ä¼˜åŒ–è¿‡ç¨‹ä¸­ï¼ŒECOå¹¶è¡Œç»“åˆç¬¦å·é¡¾é—®(Symbolic Advisor)ç”Ÿæˆçš„ç“¶é¢ˆè¯Šæ–­ä¸ROIæ£€ç´¢å™¨(ROI Retriever)è·å–çš„ç›¸å…³ç»éªŒï¼Œæ„å»ºå‡ºå…·æœ‰è¡ŒåŠ¨æŒ‡å¯¼æ„ä¹‰çš„æ€§èƒ½æç¤ºè¯ã€‚è¯¥æ¡†æ¶å…·æœ‰æ¨¡å‹æ— å…³æ€§ä¸”æ— éœ€å¾®è°ƒï¼Œå¯æ— ç¼é›†æˆåˆ°ä»»ä½•Code-LLMçš„æç¤ºæµç¨‹ä¸­ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒECOæ˜¾è‘—æå‡äº†ç”Ÿæˆä»£ç çš„æ•ˆç‡ï¼Œåœ¨ä¿è¯æ­£ç¡®æ€§çš„å‰æä¸‹å®ç°äº†æœ€é«˜7.81å€çš„åŠ é€Ÿã€‚",
      "categories": [
        "cs.PL",
        "cs.AI",
        "cs.SE"
      ],
      "primary_category": "cs.PL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.10517v1",
      "published_date": "2025-10-12 09:29:24 UTC",
      "updated_date": "2025-10-12 09:29:24 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T02:54:53.180459+00:00"
    },
    {
      "arxiv_id": "2510.10516v1",
      "title": "Population-Coded Spiking Neural Networks for High-Dimensional Robotic Control",
      "title_zh": "é¢å‘é«˜ç»´æœºå™¨äººæ§åˆ¶çš„ç¾¤ä½“ç¼–ç è„‰å†²ç¥ç»ç½‘ç»œ",
      "authors": [
        "Kanishkha Jaisankar",
        "Xiaoyang Jiang",
        "Feifan Liao",
        "Jeethu Sreenivas Amuthan"
      ],
      "abstract": "Energy-efficient and high-performance motor control remains a critical challenge in robotics, particularly for high-dimensional continuous control tasks with limited onboard resources. While Deep Reinforcement Learning (DRL) has achieved remarkable results, its computational demands and energy consumption limit deployment in resource-constrained environments. This paper introduces a novel framework combining population-coded Spiking Neural Networks (SNNs) with DRL to address these challenges. Our approach leverages the event-driven, asynchronous computation of SNNs alongside the robust policy optimization capabilities of DRL, achieving a balance between energy efficiency and control performance. Central to this framework is the Population-coded Spiking Actor Network (PopSAN), which encodes high-dimensional observations into neuronal population activities and enables optimal policy learning through gradient-based updates. We evaluate our method on the Isaac Gym platform using the PixMC benchmark with complex robotic manipulation tasks. Experimental results on the Franka robotic arm demonstrate that our approach achieves energy savings of up to 96.10% compared to traditional Artificial Neural Networks (ANNs) while maintaining comparable control performance. The trained SNN policies exhibit robust finger position tracking with minimal deviation from commanded trajectories and stable target height maintenance during pick-and-place operations. These results position population-coded SNNs as a promising solution for energy-efficient, high-performance robotic control in resource-constrained applications, paving the way for scalable deployment in real-world robotics systems.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹æœºå™¨äººé«˜ç»´è¿ç»­æ§åˆ¶ä¸­é«˜èƒ½è€—å’Œæ¿è½½èµ„æºå—é™çš„æŒ‘æˆ˜ï¼Œæå‡ºäº†ä¸€ç§ç»“åˆç¾¤ä½“ç¼–ç è„‰å†²ç¥ç»ç½‘ç»œ(Population-coded Spiking Neural Networks, SNNs)ä¸æ·±åº¦å¼ºåŒ–å­¦ä¹ (Deep Reinforcement Learning, DRL)çš„åˆ›æ–°æ¡†æ¶ã€‚æ ¸å¿ƒç»„ä»¶æ˜¯ç¾¤ä½“ç¼–ç è„‰å†²æ‰§è¡Œå™¨ç½‘ç»œ(Population-coded Spiking Actor Network, PopSAN)ï¼Œå®ƒèƒ½å¤Ÿå°†é«˜ç»´è§‚æµ‹æ•°æ®ç¼–ç ä¸ºç¥ç»å…ƒç¾¤ä½“æ´»åŠ¨ï¼Œå¹¶é€šè¿‡åŸºäºæ¢¯åº¦çš„æ›´æ–°å®ç°æœ€ä¼˜ç­–ç•¥å­¦ä¹ ã€‚è¯¥æ–¹æ³•åœ¨ Isaac Gym å¹³å°çš„ PixMC åŸºå‡†æµ‹è¯•ä¸­è¿›è¡Œäº†è¯„ä¼°ï¼Œé‡ç‚¹å…³æ³¨å¤æ‚çš„æœºå™¨äººæ“çºµä»»åŠ¡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œåœ¨ Franka æœºæ¢°è‡‚ä»»åŠ¡ä¸­ï¼Œè¯¥æ–¹æ³•ç›¸æ¯”ä¼ ç»Ÿäººå·¥ç¥ç»ç½‘ç»œ(ANNs)å¯å®ç°é«˜è¾¾ 96.10% çš„èƒ½è€—èŠ‚çœï¼Œä¸”æ§åˆ¶æ€§èƒ½ä¿æŒä¸€è‡´ã€‚è®­ç»ƒåçš„ SNN ç­–ç•¥å±•ç°äº†é²æ£’çš„è½¨è¿¹è¿½è¸ªèƒ½åŠ›å’Œç¨³å®šçš„ç›®æ ‡é«˜åº¦ç»´æŒæ€§èƒ½ã€‚è¿™ä¸€æˆæœè¯æ˜äº†ç¾¤ä½“ç¼–ç  SNNs æ˜¯èµ„æºå—é™ç¯å¢ƒä¸‹å®ç°é«˜æ•ˆèƒ½ã€é«˜æ€§èƒ½æœºå™¨äººæ§åˆ¶çš„æå…·æ½œåŠ›çš„è§£å†³æ–¹æ¡ˆï¼Œä¸ºæœªæ¥æœºå™¨äººç³»ç»Ÿçš„è§„æ¨¡åŒ–éƒ¨ç½²é“ºå¹³äº†é“è·¯ã€‚",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.RO",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.10516v1",
      "published_date": "2025-10-12 09:27:25 UTC",
      "updated_date": "2025-10-12 09:27:25 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T02:54:55.177385+00:00"
    },
    {
      "arxiv_id": "2510.10510v1",
      "title": "f-INE: A Hypothesis Testing Framework for Estimating Influence under Training Randomness",
      "title_zh": "f-INEï¼šè®­ç»ƒéšæœºæ€§ä¸‹çš„å½±å“åŠ›ä¼°ç®—å‡è®¾æ£€éªŒæ¡†æ¶",
      "authors": [
        "Subhodip Panda",
        "Dhruv Tarsadiya",
        "Shashwat Sourav",
        "Prathosh A. P",
        "Sai Praneeth Karimireddy"
      ],
      "abstract": "Influence estimation methods promise to explain and debug machine learning by estimating the impact of individual samples on the final model. Yet, existing methods collapse under training randomness: the same example may appear critical in one run and irrelevant in the next. Such instability undermines their use in data curation or cleanup since it is unclear if we indeed deleted/kept the correct datapoints. To overcome this, we introduce *f-influence* -- a new influence estimation framework grounded in hypothesis testing that explicitly accounts for training randomness, and establish desirable properties that make it suitable for reliable influence estimation. We also design a highly efficient algorithm **f**-**IN**fluence **E**stimation (**f-INE**) that computes f-influence **in a single training run**. Finally, we scale up f-INE to estimate influence of instruction tuning data on Llama-3.1-8B and show it can reliably detect poisoned samples that steer model opinions, demonstrating its utility for data cleanup and attributing model behavior.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ç°æœ‰å½±å“åŠ›ä¼°è®¡ (Influence estimation) æ–¹æ³•åœ¨è®­ç»ƒéšæœºæ€§ä¸‹è¡¨ç°ä¸ç¨³å®šçš„é—®é¢˜ï¼Œæå‡ºäº†åä¸º *f-influence* çš„æ–°å‹å‡è®¾æ£€éªŒæ¡†æ¶ã€‚è¯¥æ¡†æ¶æ˜¾å¼åœ°è€ƒè™‘äº†è®­ç»ƒéšæœºæ€§å¯¹æ ·æœ¬å½±å“åŠ›çš„å½±å“ï¼Œå¹¶ç¡®ç«‹äº†å®ç°å¯é å½±å“åŠ›ä¼°è®¡æ‰€éœ€çš„å…³é”®ç‰¹æ€§ã€‚ä¸ºæå‡è®¡ç®—æ•ˆç‡ï¼Œç ”ç©¶è€…è®¾è®¡äº† **f-INE** (f-INfluence Estimation) ç®—æ³•ï¼Œèƒ½å¤Ÿä»…é€šè¿‡å•æ¬¡è®­ç»ƒè¿è¡Œ (Single training run) å®Œæˆå½±å“åŠ›è®¡ç®—ã€‚å®éªŒè¡¨æ˜ï¼Œf-INE æˆåŠŸæ‰©å±•è‡³ Llama-3.1-8B æ¨¡å‹çš„æŒ‡ä»¤å¾®è°ƒåœºæ™¯ï¼Œå¹¶èƒ½å¯é åœ°æ£€æµ‹å‡ºæ—¨åœ¨æ“çºµæ¨¡å‹è§‚ç‚¹çš„æŠ•æ¯’æ ·æœ¬ã€‚è¯¥ç ”ç©¶è¯æ˜äº† f-INE åœ¨æ•°æ®æ¸…æ´— (Data cleanup) å’Œæ¨¡å‹è¡Œä¸ºæº¯æºæ–¹é¢å…·æœ‰æ˜¾è‘—çš„å®ç”¨ä»·å€¼ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.10510v1",
      "published_date": "2025-10-12 09:05:47 UTC",
      "updated_date": "2025-10-12 09:05:47 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T02:54:59.588871+00:00"
    },
    {
      "arxiv_id": "2510.10509v1",
      "title": "MARS-Sep: Multimodal-Aligned Reinforced Sound Separation",
      "title_zh": "MARS-Sepï¼šå¤šæ¨¡æ€å¯¹é½çš„å¼ºåŒ–å£°éŸ³åˆ†ç¦»",
      "authors": [
        "Zihan Zhang",
        "Xize Cheng",
        "Zhennan Jiang",
        "Dongjie Fu",
        "Jingyuan Chen",
        "Zhou Zhao",
        "Tao Jin"
      ],
      "abstract": "Universal sound separation faces a fundamental misalignment: models optimized for low-level signal metrics often produce semantically contaminated outputs, failing to suppress perceptually salient interference from acoustically similar sources. To bridge this gap, we introduce MARS-Sep, a reinforcement learning framework that reformulates separation as decision making. Instead of simply regressing ground-truth masks, MARS-Sep learns a factorized Beta mask policy that is optimized by a clipped trust-region surrogate with entropy regularization and group-relative advantage normalization. Concretely, we sample masks from a frozen old policy, reconstruct waveforms, and update the current policy using clipped importance ratios-yielding substantially more stable and sample-efficient learning. Multimodal rewards, derived from an audio-text-vision encoder, directly incentivize semantic consistency with query prompts. We further propose a progressive alignment scheme to fine-tune this encoder, boosting its cross-modal discriminability and improving reward faithfulness. Extensive experiments on multiple benchmarks demonstrate consistent gains in Text-, Audio-, and Image-Queried separation, with notable improvements in signal metrics and semantic quality. Our code is available at https://anonymous.4open.science/r/MARS-Sep. Sound separation samples are available at https://mars-sep.github.io/.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†MARS-Sepï¼Œè¿™æ˜¯ä¸€ä¸ªé’ˆå¯¹é€šç”¨å£°éŸ³åˆ†ç¦»çš„å¤šæ¨¡æ€å¯¹é½å¼ºåŒ–å­¦ä¹ (Reinforcement Learning)æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³ä¼ ç»Ÿæ¨¡å‹åœ¨å¤„ç†è¯­ä¹‰æ±¡æŸ“å’Œå£°å­¦ç›¸ä¼¼å¹²æ‰°æ—¶çš„å±€é™æ€§ã€‚è¯¥æ¡†æ¶å°†åˆ†ç¦»è¿‡ç¨‹é‡æ–°å®šä¹‰ä¸ºå†³ç­–ä»»åŠ¡ï¼Œé€šè¿‡å­¦ä¹ å› å­åŒ–çš„Beta maskç­–ç•¥å¹¶ç»“åˆè£å‰ªç½®ä¿¡åŸŸä»£ç†(Clipped Trust-Region Surrogate)ä¼˜åŒ–ï¼Œæ˜¾è‘—æé«˜äº†å­¦ä¹ çš„ç¨³å®šæ€§å’Œé‡‡æ ·æ•ˆç‡ã€‚ä¸ºäº†ç¡®ä¿åˆ†ç¦»è¾“å‡ºä¸æŸ¥è¯¢æç¤ºçš„è¯­ä¹‰ä¸€è‡´ï¼ŒMARS-Sepåˆ©ç”¨éŸ³é¢‘-æ–‡æœ¬-è§†è§‰ç¼–ç å™¨ç”Ÿæˆå¤šæ¨¡æ€å¥–åŠ±(Multimodal Rewards)ï¼Œå¹¶é‡‡ç”¨æ¸è¿›å¼å¯¹é½æ–¹æ¡ˆæå‡ç¼–ç å™¨çš„è·¨æ¨¡æ€åˆ¤åˆ«èƒ½åŠ›ã€‚åœ¨å¤šé¡¹åŸºå‡†æµ‹è¯•ä¸­çš„å®éªŒç»“æœè¡¨æ˜ï¼ŒMARS-Sepåœ¨æ–‡æœ¬ã€éŸ³é¢‘å’Œå›¾åƒæŸ¥è¯¢çš„åˆ†ç¦»ä»»åŠ¡ä¸Šå‡å®ç°äº†æ€§èƒ½æ˜¾è‘—æå‡ï¼Œåœ¨ä¼˜åŒ–ä¿¡å·æŒ‡æ ‡çš„åŒæ—¶å¤§å¹…å¢å¼ºäº†è¯­ä¹‰è´¨é‡ã€‚",
      "categories": [
        "cs.SD",
        "cs.AI"
      ],
      "primary_category": "cs.SD",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.10509v1",
      "published_date": "2025-10-12 09:05:28 UTC",
      "updated_date": "2025-10-12 09:05:28 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T02:55:01.970992+00:00"
    },
    {
      "arxiv_id": "2510.10503v1",
      "title": "Align2Act: Instruction-Tuned Models for Human-Aligned Autonomous Driving",
      "title_zh": "Align2Actï¼šé¢å‘äººç±»å¯¹é½è‡ªåŠ¨é©¾é©¶çš„æŒ‡ä»¤å¾®è°ƒæ¨¡å‹",
      "authors": [
        "Kanishkha Jaisankar",
        "Sunidhi Tandel"
      ],
      "abstract": "Motion planning in complex scenarios is a core challenge in autonomous driving. Conventional methods apply predefined rules or learn from driving data to generate trajectories, while recent approaches leverage large language models (LLMs) for decision-making. However, it remains unclear whether LLMs truly capture human driving logic. We propose Align2Act, a motion planning framework that transforms instruction-tuned LLMs into interpretable planners aligned with human behavior. We derive structured driving instructions based on human reasoning patterns (e.g., anticipate hazards, yield at intersections) and traffic rules (e.g., stop at red lights, maintain lane boundaries). Our Align2ActChain module guides step-by-step reasoning to produce both an interpretable rationale and a safe trajectory. By fine-tuning LLaMA-2-7B with LoRA on one million scenarios from the nuPlan dataset, our method achieves an open-loop score of 85.17 and closed-loop scores of 70.31 (non-reactive) and 66.96 (reactive) on Test14-random. Unlike prior work focused on synthetic or open-loop settings, we demonstrate improved planning quality and human-likeness on the real-world nuPlan closed-loop benchmark. Ablation studies confirm that structured reasoning significantly improves performance over baseline LLM planners.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† Align2Actï¼Œè¿™æ˜¯ä¸€ä¸ªæ—¨åœ¨å°†æŒ‡ä»¤å¾®è°ƒåçš„ LLMs è½¬åŒ–ä¸ºç¬¦åˆäººç±»é©¾é©¶è¡Œä¸ºä¸”å…·å¤‡å¯è§£é‡Šæ€§çš„è¿åŠ¨è§„åˆ’æ¡†æ¶ã€‚è¯¥æ¡†æ¶åŸºäºäººç±»æ¨ç†æ¨¡å¼å’Œäº¤é€šè§„åˆ™æ¨å¯¼å‡ºç»“æ„åŒ–çš„é©¾é©¶æŒ‡ä»¤ï¼Œä»¥è§£å†³å¤æ‚åœºæ™¯ä¸‹çš„è‡ªåŠ¨é©¾é©¶æŒ‘æˆ˜ã€‚é€šè¿‡å…¶ Align2ActChain æ¨¡å—ï¼Œè¯¥æ–¹æ³•å¼•å¯¼æ¨¡å‹è¿›è¡Œæ­¥è¿›å¼æ¨ç†ï¼Œä»è€ŒåŒæ—¶ç”Ÿæˆå¯è§£é‡Šçš„é©¾é©¶é€»è¾‘å’Œå®‰å…¨çš„è§„åˆ’è½¨è¿¹ã€‚ç ”ç©¶äººå‘˜åœ¨åŒ…å«ç™¾ä¸‡ä¸ªåœºæ™¯çš„ nuPlan æ•°æ®é›†ä¸Šï¼Œåˆ©ç”¨ LoRA æŠ€æœ¯å¯¹ LLaMA-2-7B è¿›è¡Œäº†å¾®è°ƒã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒAlign2Act åœ¨ Test14-random æµ‹è¯•é›†ä¸­å–å¾—äº† 85.17 çš„å¼€ç¯è¯„åˆ†ï¼Œä»¥åŠåˆ†åˆ« 70.31 å’Œ 66.96 çš„é—­ç¯è¯„åˆ†ã€‚ä¸ä»¥å¾€ä¸“æ³¨äºåˆæˆæˆ–å¼€ç¯è®¾ç½®çš„å·¥ä½œä¸åŒï¼ŒAlign2Act åœ¨çœŸå®ä¸–ç•Œçš„ nuPlan é—­ç¯åŸºå‡†æµ‹è¯•ä¸­æ˜¾è‘—æå‡äº†è§„åˆ’è´¨é‡å’Œç±»äººæ€§ï¼Œæ¶ˆèå®éªŒè¿›ä¸€æ­¥è¯å®äº†ç»“æ„åŒ–æ¨ç†å¯¹æ€§èƒ½æå‡çš„å…³é”®ä½œç”¨ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.RO"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.10503v1",
      "published_date": "2025-10-12 08:50:34 UTC",
      "updated_date": "2025-10-12 08:50:34 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T02:55:03.804755+00:00"
    },
    {
      "arxiv_id": "2510.10496v1",
      "title": "Personalized Motion Guidance Framework for Athlete-Centric Coaching",
      "title_zh": "é¢å‘ä»¥è¿åŠ¨å‘˜ä¸ºä¸­å¿ƒçš„æ•™ç»ƒæŒ‡å¯¼çš„ä¸ªæ€§åŒ–è¿åŠ¨å¼•å¯¼æ¡†æ¶",
      "authors": [
        "Ryota Takamidoa",
        "Chiharu Suzukia",
        "Hiroki Nakamoto"
      ],
      "abstract": "A critical challenge in contemporary sports science lies in filling the gap between group-level insights derived from controlled hypothesis-driven experiments and the real-world need for personalized coaching tailored to individual athletes' unique movement patterns. This study developed a Personalized Motion Guidance Framework (PMGF) to enhance athletic performance by generating individualized motion-refinement guides using generative artificial intelligence techniques. PMGF leverages a vertical autoencoder to encode motion sequences into athlete-specific latent representations, which can then be directly manipulated to generate meaningful guidance motions. Two manipulation strategies were explored: (1) smooth interpolation between the learner's motion and a target (e.g., expert) motion to facilitate observational learning, and (2) shifting the motion pattern in an optimal direction in the latent space using a local optimization technique. The results of the validation experiment with data from 51 baseball pitchers revealed that (1) PMGF successfully generated smooth transitions in motion patterns between individuals across all 1,275 pitcher pairs, and (2) the features significantly altered through PMGF manipulations reflected known performance-enhancing characteristics, such as increased stride length and knee extension associated with higher ball velocity, indicating that PMGF induces biomechanically plausible improvements. We propose a future extension called general-PMGF to enhance the applicability of this framework. This extension incorporates bodily, environmental, and task constraints into the generation process, aiming to provide more realistic and versatile guidance across diverse sports contexts.",
      "tldr_zh": "è¯¥ç ”ç©¶å¼€å‘äº†Personalized Motion Guidance Framework (PMGF)ï¼Œæ—¨åœ¨å¼¥è¡¥ç¾¤ä½“æ€§è¿åŠ¨ç§‘å­¦è§è§£ä¸ä¸ªä½“è¿åŠ¨å‘˜ä¸ªæ€§åŒ–æŒ‡å¯¼ä¹‹é—´çš„å·®è·ã€‚è¯¥æ¡†æ¶åˆ©ç”¨vertical autoencoderå°†è¿åŠ¨åºåˆ—ç¼–ç ä¸ºè¿åŠ¨å‘˜ç‰¹æœ‰çš„æ½œåœ¨è¡¨ç¤º(latent representations)ï¼Œå¹¶é€šè¿‡ä¸¤ç§ç­–ç•¥ç”ŸæˆæŒ‡å¯¼æ€§åŠ¨ä½œï¼šä¸€ç§æ˜¯å­¦ä¹ è€…ä¸ä¸“å®¶åŠ¨ä½œä¹‹é—´çš„å¹³æ»‘æ’å€¼(interpolation)ï¼Œå¦ä¸€ç§æ˜¯åˆ©ç”¨å±€éƒ¨ä¼˜åŒ–æŠ€æœ¯åœ¨æ½œåœ¨ç©ºé—´ä¸­å¯»æ‰¾æœ€ä¼˜è¿åŠ¨æ–¹å‘ã€‚åœ¨51åæ£’çƒæŠ•æ‰‹çš„éªŒè¯å®éªŒä¸­ï¼ŒPMGFæˆåŠŸç”Ÿæˆäº†å¹³æ»‘çš„åŠ¨ä½œè¿‡æ¸¡ï¼Œä¸”è°ƒæ•´åçš„ç‰¹å¾ï¼ˆå¦‚å¢åŠ æ­¥å¹…å’Œè†ç›–ä¼¸å±•ï¼‰ç¬¦åˆå·²çŸ¥çš„æé«˜çƒé€Ÿçš„ç”Ÿç‰©åŠ›å­¦ç‰¹å¾ï¼Œè¯æ˜äº†å…¶æ”¹è¿›æ–¹æ¡ˆçš„åˆç†æ€§ã€‚ç ”ç©¶æœ€åæå‡ºäº†general-PMGFæ‰©å±•æ–¹æ¡ˆï¼Œæ—¨åœ¨é€šè¿‡å¼•å…¥èº«ä½“ã€ç¯å¢ƒå’Œä»»åŠ¡çº¦æŸï¼Œè¿›ä¸€æ­¥æé«˜è¯¥æ¡†æ¶åœ¨å¤šæ ·åŒ–ä½“è‚²åœºæ™¯ä¸­çš„å®ç”¨æ€§ä¸çœŸå®æ€§ã€‚",
      "categories": [
        "cs.HC",
        "cs.AI"
      ],
      "primary_category": "cs.HC",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.10496v1",
      "published_date": "2025-10-12 08:21:19 UTC",
      "updated_date": "2025-10-12 08:21:19 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T02:55:08.986068+00:00"
    },
    {
      "arxiv_id": "2510.10494v1",
      "title": "Tracing the Traces: Latent Temporal Signals for Efficient and Accurate Reasoning",
      "title_zh": "å¾ªè¿¹æº¯æºï¼šç”¨äºé«˜æ•ˆç²¾å‡†æ¨ç†çš„æ½œåœ¨æ—¶åºä¿¡å·",
      "authors": [
        "Martina G. Vilas",
        "Safoora Yousefi",
        "Besmira Nushi",
        "Eric Horvitz",
        "Vidhisha Balachandran"
      ],
      "abstract": "Reasoning models improve their problem-solving ability through inference-time scaling, allocating more compute via longer token budgets. Identifying which reasoning traces are likely to succeed remains a key opportunity: reliably predicting productive paths can substantially reduce wasted computation and improve overall efficiency. We introduce Latent-Trajectory signals that characterize the temporal evolution of a model's internal representations during the generation of intermediate reasoning tokens. By measuring the overall change in latent representations between the start and end of reasoning, the change accumulated across intermediate steps, and the extent to which these changes advance toward the final state, we show that these signals predict solution accuracy more reliably than both cross-layer metrics and output-based confidence measures. When used to guide answer selection across multiple sampled generations, Latent-Trajectory signals make test-time scaling more effective and efficient than majority voting, reducing token usage by up to 70% while preserving and even improving accuracy by 2.6% on average. Moreover, these predictive signals often emerge early in the reasoning trace, enabling early selection and allocation of compute to the most promising candidates. Our findings contribute not only practical strategies for inference-time efficiency, but also a deeper interpretability perspective on how reasoning processes are represented and differentiated in latent space.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†æ¨ç†æ¨¡å‹é€šè¿‡æ¨ç†æ—¶é—´æ‰©å±•(inference-time scaling)æå‡æ€§èƒ½æ—¶çš„æ•ˆç‡é—®é¢˜ï¼ŒæŒ‡å‡ºè¯†åˆ«æˆåŠŸçš„æ¨ç†è·¯å¾„(reasoning traces)å¯¹å‡å°‘è®¡ç®—æµªè´¹è‡³å…³é‡è¦ã€‚ä¸ºæ­¤ï¼Œä½œè€…æå‡ºäº†Latent-Trajectoryä¿¡å·ï¼Œé€šè¿‡åˆ»ç”»ä¸­é—´æ¨ç†tokenç”Ÿæˆè¿‡ç¨‹ä¸­æ¨¡å‹å†…éƒ¨è¡¨å¾çš„éšæ—¶é—´æ¼”å˜æ¥é¢„æµ‹è§£é¢˜å‡†ç¡®æ€§ã€‚è¯¥ä¿¡å·å…·ä½“è¡¡é‡äº†æ½œåœ¨è¡¨å¾çš„æ•´ä½“å˜åŒ–ã€æ­¥éª¤é—´çš„ç´¯ç§¯å˜åŒ–ä»¥åŠå‘æœ€ç»ˆçŠ¶æ€æ¨è¿›çš„ç¨‹åº¦ï¼Œå…¶é¢„æµ‹å¯é æ€§ä¼˜äºè·¨å±‚æŒ‡æ ‡(cross-layer metrics)å’ŒåŸºäºè¾“å‡ºçš„ç½®ä¿¡åº¦ã€‚å®éªŒè¯æ˜ï¼Œåœ¨å¼•å¯¼ç­”æ¡ˆé€‰æ‹©æ—¶ï¼Œè¯¥æ–¹æ³•æ¯”å¤šæ•°æŠ•ç¥¨(majority voting)æ›´é«˜æ•ˆï¼Œèƒ½åœ¨å¹³å‡æå‡2.6%å‡†ç¡®ç‡çš„åŒæ—¶å‡å°‘é«˜è¾¾70%çš„tokenä½¿ç”¨é‡ã€‚æ­¤å¤–ï¼Œè¿™äº›ä¿¡å·åœ¨æ¨ç†æ—©æœŸå³å¯æ˜¾ç°ï¼Œä¸ºå®ç°é«˜æ•ˆçš„æµ‹è¯•æ—¶æ‰©å±•(test-time scaling)å’Œå¢å¼ºæ¨¡å‹å†…éƒ¨æ¨ç†è¿‡ç¨‹çš„å¯è§£é‡Šæ€§æä¾›äº†æœ‰åŠ›æ”¯æŒã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.10494v1",
      "published_date": "2025-10-12 08:03:56 UTC",
      "updated_date": "2025-10-12 08:03:56 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T02:55:23.967214+00:00"
    },
    {
      "arxiv_id": "2510.15965v1",
      "title": "One Token Embedding Is Enough to Deadlock Your Large Reasoning Model",
      "title_zh": "ä»…éœ€ä¸€ä¸ª Token åµŒå…¥å³å¯ä½¿å¤§å‹æ¨ç†æ¨¡å‹é™·å…¥æ­»é”",
      "authors": [
        "Mohan Zhang",
        "Yihua Zhang",
        "Jinghan Jia",
        "Zhangyang Wang",
        "Sijia Liu",
        "Tianlong Chen"
      ],
      "abstract": "Modern large reasoning models (LRMs) exhibit impressive multi-step problem-solving via chain-of-thought (CoT) reasoning. However, this iterative thinking mechanism introduces a new vulnerability surface. We present the Deadlock Attack, a resource exhaustion method that hijacks an LRM's generative control flow by training a malicious adversarial embedding to induce perpetual reasoning loops. Specifically, the optimized embedding encourages transitional tokens (e.g., \"Wait\", \"But\") after reasoning steps, preventing the model from concluding its answer. A key challenge we identify is the continuous-to-discrete projection gap: naÃ¯ve projections of adversarial embeddings to token sequences nullify the attack. To overcome this, we introduce a backdoor implantation strategy, enabling reliable activation through specific trigger tokens. Our method achieves a 100% attack success rate across four advanced LRMs (Phi-RM, Nemotron-Nano, R1-Qwen, R1-Llama) and three math reasoning benchmarks, forcing models to generate up to their maximum token limits. The attack is also stealthy (in terms of causing negligible utility loss on benign user inputs) and remains robust against existing strategies trying to mitigate the overthinking issue. Our findings expose a critical and underexplored security vulnerability in LRMs from the perspective of reasoning (in)efficiency.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†Deadlock Attackï¼Œè¿™æ˜¯ä¸€ç§é’ˆå¯¹å¤§å‹æ¨ç†æ¨¡å‹(Large Reasoning Models, LRMs)çš„èµ„æºæ¶ˆè€—æ”»å‡»æ–¹æ³•ï¼Œé€šè¿‡åŠ«æŒç”Ÿæˆæ§åˆ¶æµè¯±å¯¼æ¨¡å‹è¿›å…¥æ°¸ä¹…æ¨ç†å¾ªç¯ã€‚è¯¥æ”»å‡»é€šè¿‡ä¼˜åŒ–å¯¹æŠ—åµŒå…¥(adversarial embedding)ï¼Œåœ¨æ¨ç†æ­¥éª¤åè§¦å‘â€œWaitâ€æˆ–â€œButâ€ç­‰è¿‡æ¸¡Tokenï¼Œä»è€Œé˜»æ­¢æ¨¡å‹è¾“å‡ºæœ€ç»ˆç­”æ¡ˆã€‚ä¸ºäº†å…‹æœè¿ç»­åˆ°ç¦»æ•£æŠ•å½±å¸¦æ¥çš„æ”»å‡»å¤±æ•ˆé—®é¢˜ï¼Œç ”ç©¶è€…é‡‡ç”¨äº†åé—¨æ¤å…¥(backdoor implantation)ç­–ç•¥ï¼Œåˆ©ç”¨ç‰¹å®šè§¦å‘Tokenç¡®ä¿æ”»å‡»çš„å¯é æ¿€æ´»ã€‚å®éªŒè¯æ˜ï¼Œè¯¥æ–¹æ³•åœ¨Phi-RMã€Nemotron-Nanoã€R1-Qwenå’ŒR1-Llamaç­‰å››ç§å…ˆè¿›æ¨¡å‹åŠä¸‰ä¸ªæ•°å­¦æ¨ç†åŸºå‡†ä¸Šå®ç°äº†100%çš„æ”»å‡»æˆåŠŸç‡ï¼Œè¿«ä½¿æ¨¡å‹ç”Ÿæˆç›´è‡³è§¦åŠæœ€å¤§Tokené™åˆ¶ã€‚Deadlock Attackåœ¨ä¿æŒéšè”½æ€§çš„åŒæ—¶ï¼Œå¯¹ç°æœ‰çš„ç¼“è§£è¿‡åº¦æ€è€ƒ(overthinking)ç­–ç•¥å…·æœ‰æå¼ºçš„é²æ£’æ€§ã€‚è¯¥å‘ç°æ­ç¤ºäº†LRMsåœ¨æ¨ç†æ•ˆç‡æ–¹é¢å­˜åœ¨ä¸¥é‡ä¸”æœªè¢«å……åˆ†æ¢ç´¢çš„å®‰å…¨æ¼æ´ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CR"
      ],
      "primary_category": "cs.LG",
      "comment": "NeurIPS 2025",
      "pdf_url": "https://arxiv.org/pdf/2510.15965v1",
      "published_date": "2025-10-12 07:42:57 UTC",
      "updated_date": "2025-10-12 07:42:57 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T02:55:27.795873+00:00"
    },
    {
      "arxiv_id": "2510.10487v1",
      "title": "Towards Self-Refinement of Vision-Language Models with Triangular Consistency",
      "title_zh": "è¿ˆå‘åŸºäºä¸‰è§’ä¸€è‡´æ€§çš„è§†è§‰è¯­è¨€æ¨¡å‹è‡ªæˆ‘ä¼˜åŒ–",
      "authors": [
        "Yunlong Deng",
        "Guangyi Chen",
        "Tianpei Gu",
        "Lingjing Kong",
        "Yan Li",
        "Zeyu Tang",
        "Kun Zhang"
      ],
      "abstract": "Vision-Language Models (VLMs) integrate visual knowledge with the analytical capabilities of Large Language Models (LLMs) through supervised visual instruction tuning, using image-question-answer triplets. However, the potential of VLMs trained without supervised instruction remains largely unexplored. This study validates that VLMs possess inherent self-refinement capabilities, enabling them to generate high-quality supervised data without external inputs and thereby learn autonomously. Specifically, to stimulate the self-refinement ability of VLMs, we propose a self-refinement framework based on a Triangular Consistency principle: within the image-query-answer triangle, any masked elements should be consistently and accurately reconstructed. The framework involves three steps: (1) We enable the instruction generation ability of VLMs by adding multi-task instruction tuning like image$\\rightarrow$question-answer or image-answer$\\rightarrow$question. (2) We generate image-query-answer triplets from unlabeled images and use the Triangular Consistency principle for filtering. (3) The model is further updated using the filtered synthetic data. To investigate the underlying mechanisms behind this self-refinement capability, we conduct a theoretical analysis from a causal perspective. Using the widely recognized LLaVA-1.5 as our baseline, our experiments reveal that the model can autonomously achieve consistent, though deliberately modest, improvements across multiple benchmarks without any external supervision, such as human annotations or environmental feedback. We expect that the insights of this study on the self-refinement ability of VLMs can inspire future research on the learning mechanism of VLMs. Code is available at https://github.com/dengyl20/SRF-LLaVA-1.5.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†è§†è§‰è¯­è¨€æ¨¡å‹ (Vision-Language Models, VLMs) åœ¨ç¼ºä¹å¤–éƒ¨ç›‘ç£æŒ‡ä»¤æ—¶åˆ©ç”¨å…¶å†…åœ¨è‡ªæˆ‘ä¿®å¤ (Self-Refinement) èƒ½åŠ›å®ç°è‡ªä¸»å­¦ä¹ çš„æ½œåŠ›ã€‚ä½œè€…æå‡ºäº†ä¸€ç§åŸºäºä¸‰è§’å½¢ä¸€è‡´æ€§ (Triangular Consistency) åŸåˆ™çš„æ¡†æ¶ï¼Œå³åœ¨â€œå›¾åƒ-æŸ¥è¯¢-å›ç­”â€æ„æˆçš„ä¸‰å…ƒç»„ä¸­ï¼Œé€šè¿‡é‡å»ºä»»ä½•è¢«å±è”½çš„å…ƒç´ æ¥ç¡®ä¿é€»è¾‘ä¸€è‡´æ€§ã€‚è¯¥æ–¹æ³•é¦–å…ˆé€šè¿‡å¤šä»»åŠ¡æŒ‡ä»¤å¾®è°ƒæ¿€æ´»æ¨¡å‹çš„æŒ‡ä»¤ç”Ÿæˆèƒ½åŠ›ï¼Œæ¥ç€åˆ©ç”¨æ— æ ‡ç­¾å›¾åƒç”Ÿæˆå¹¶ç­›é€‰ä¸‰å…ƒç»„æ•°æ®ï¼Œæœ€åå¯¹æ¨¡å‹è¿›è¡Œè¿­ä»£æ›´æ–°ã€‚ç ”ç©¶è¿˜ä»å› æœè§†è§’å¯¹è‡ªæˆ‘ä¿®å¤æœºåˆ¶è¿›è¡Œäº†ç†è®ºåˆ†æã€‚ä»¥ LLaVA-1.5 ä¸ºåŸºå‡†çš„å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ¨¡å‹åœ¨æ²¡æœ‰ä»»ä½•äººç±»æ ‡æ³¨æˆ–å¤–éƒ¨åé¦ˆçš„æƒ…å†µä¸‹ï¼Œèƒ½åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è‡ªä¸»è·å¾—æŒç»­çš„æ€§èƒ½æå‡ã€‚è¿™ä¸€æˆæœä¸ºç†è§£ VLMs çš„å­¦ä¹ æœºåˆ¶æä¾›äº†æ–°è§†è§’ï¼Œå¹¶ä¸ºæ„å»ºæ— éœ€å¤–éƒ¨ç›‘ç£çš„è‡ªä¸»è¿›åŒ–æ¨¡å‹å¥ å®šäº†åŸºç¡€ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.10487v1",
      "published_date": "2025-10-12 07:37:47 UTC",
      "updated_date": "2025-10-12 07:37:47 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T02:55:26.470551+00:00"
    },
    {
      "arxiv_id": "2510.10486v1",
      "title": "SASER: Stego attacks on open-source LLMs",
      "title_zh": "SASERï¼šé’ˆå¯¹å¼€æºå¤§è¯­è¨€æ¨¡å‹çš„éšå†™æ”»å‡»",
      "authors": [
        "Ming Tan",
        "Wei Li",
        "Hu Tao",
        "Hailong Ma",
        "Aodi Liu",
        "Qian Chen",
        "Zilong Wang"
      ],
      "abstract": "Open-source large language models (LLMs) have demonstrated considerable dominance over proprietary LLMs in resolving neural processing tasks, thanks to the collaborative and sharing nature. Although full access to source codes, model parameters, and training data lays the groundwork for transparency, we argue that such a full-access manner is vulnerable to stego attacks, and their ill-effects are not fully understood. In this paper, we conduct a systematic formalization for stego attacks on open-source LLMs by enumerating all possible threat models associated with adversary objectives, knowledge, and capabilities. Therein, the threat posed by adversaries with internal knowledge, who inject payloads and triggers during the model sharing phase, is of practical interest. We go even further and propose the first stego attack on open-source LLMs, dubbed SASER, which wields impacts through identifying targeted parameters, embedding payloads, injecting triggers, and executing payloads sequentially. Particularly, SASER enhances the attack robustness against quantization-based local deployment by de-quantizing the embedded payloads. In addition, to achieve stealthiness, SASER devises the performance-aware importance metric to identify targeted parameters with the least degradation of model performance. Extensive experiments on LlaMA2-7B and ChatGLM3-6B, without quantization, show that the stealth rate of SASER outperforms existing stego attacks (for general DNNs) by up to 98.1%, while achieving the same attack success rate (ASR) of 100%. More importantly, SASER improves ASR on quantized models from 0 to 100% in all settings. We appeal for investigations on countermeasures against SASER in view of the significant attack effectiveness.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†å¼€æºå¤§è¯­è¨€æ¨¡å‹(LLMs)åœ¨æ¨¡å‹å…±äº«é˜¶æ®µé¢ä¸´çš„éšå†™æ”»å‡»(stego attacks)é£é™©ï¼Œå¹¶ç³»ç»Ÿåœ°å½¢å¼åŒ–äº†å…¶å¨èƒæ¨¡å‹ã€‚ä½œè€…æå‡ºäº†åä¸ºSASERçš„æ”»å‡»æ¡†æ¶ï¼Œé€šè¿‡è¯†åˆ«ç›®æ ‡å‚æ•°ã€åµŒå…¥æœ‰æ•ˆè´Ÿè½½(payloads)ã€æ³¨å…¥è§¦å‘å™¨å¹¶é¡ºåºæ‰§è¡Œæœ‰æ•ˆè´Ÿè½½æ¥å®ç°æ”»å‡»ç›®çš„ã€‚ä¸ºäº†ä¿è¯éšè”½æ€§ï¼ŒSASERåˆ©ç”¨æ€§èƒ½æ„ŸçŸ¥é‡è¦æ€§æŒ‡æ ‡(performance-aware importance metric)ç­›é€‰å¯¹æ¨¡å‹æ€§èƒ½å½±å“æœ€å°çš„å‚æ•°ã€‚æ­¤å¤–ï¼Œè¯¥æ¡†æ¶é€šè¿‡åé‡åŒ–åµŒå…¥çš„æœ‰æ•ˆè´Ÿè½½ï¼Œæ˜¾è‘—å¢å¼ºäº†é’ˆå¯¹é‡åŒ–éƒ¨ç½²ç¯å¢ƒçš„æ”»å‡»ç¨³å¥æ€§ã€‚åœ¨LlaMA2-7Bå’ŒChatGLM3-6Bä¸Šçš„å®éªŒç»“æœæ˜¾ç¤ºï¼ŒSASERåœ¨ä¿æŒ100%æ”»å‡»æˆåŠŸç‡(ASR)çš„åŒæ—¶ï¼Œå…¶éšè”½ç‡æ¯”ç°æœ‰é’ˆå¯¹é€šç”¨DNNçš„æ”»å‡»é«˜å‡ºå¤šè¾¾98.1%ã€‚ç‰¹åˆ«æ˜¯åœ¨é‡åŒ–æ¨¡å‹è®¾ç½®ä¸‹ï¼ŒSASERå°†æ”»å‡»æˆåŠŸç‡ä»0%æå‡è‡³100%ï¼Œæ­ç¤ºäº†å¼€æºæ¨¡å‹åˆ†å‘è¿‡ç¨‹ä¸­å­˜åœ¨çš„ä¸¥é‡å®‰å…¨éšæ‚£ã€‚",
      "categories": [
        "cs.CR",
        "cs.AI"
      ],
      "primary_category": "cs.CR",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.10486v1",
      "published_date": "2025-10-12 07:33:56 UTC",
      "updated_date": "2025-10-12 07:33:56 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T02:55:29.974270+00:00"
    },
    {
      "arxiv_id": "2510.12830v2",
      "title": "Gobernanza y trazabilidad \"a prueba de AI Act\" para casos de uso legales: un marco tÃ©cnico-jurÃ­dico, mÃ©tricas forenses y evidencias auditables",
      "title_zh": "æ»¡è¶³ AI Act åˆè§„è¦æ±‚çš„æ³•å¾‹ç”¨ä¾‹æ²»ç†ä¸å¯è¿½æº¯æ€§ï¼šæŠ€æœ¯-æ³•å¾‹æ¡†æ¶ã€å–è¯æŒ‡æ ‡ä¸å¯å®¡è®¡è¯æ®",
      "authors": [
        "Alex Dantart"
      ],
      "abstract": "This paper presents a comprehensive governance framework for AI systems in the legal sector, designed to ensure verifiable compliance with the EU AI Act. The framework integrates a normative mapping of the regulation to technical controls, a forensic architecture for RAG/LLM systems, and an evaluation system with metrics weighted by legal risk. As a primary contribution, we present rag-forense, an open-source implementation of the framework, accompanied by an experimental protocol to demonstrate compliance.\n  --\n  Este artÃ­culo presenta un marco integral de gobernanza para sistemas de IA en el sector legal, diseÃ±ado para garantizar el cumplimiento verificable del Reglamento de IA de la UE (AI Act). El marco integra una cartografÃ­a normativa de la ley a controles tÃ©cnicos, una arquitectura forense para sistemas RAG/LLM y un sistema de evaluaciÃ³n con mÃ©tricas ponderadas por el riesgo jurÃ­dico. Como principal contribuciÃ³n, se presenta rag-forense, una implementaciÃ³n de cÃ³digo abierto del marco, acompaÃ±ada de un protocolo experimental para demostrar la conformidad.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ä¸ªé’ˆå¯¹æ³•å¾‹è¡Œä¸šçš„äººå·¥æ™ºèƒ½æ²»ç†æ¡†æ¶ï¼Œæ—¨åœ¨ç¡®ä¿AIç³»ç»Ÿèƒ½å¤Ÿå¯éªŒè¯åœ°ç¬¦åˆæ¬§ç›Ÿã€Šäººå·¥æ™ºèƒ½æ³•æ¡ˆã€‹(EU AI Act) çš„è¦æ±‚ã€‚è¯¥æ¡†æ¶é€šè¿‡å»ºç«‹æ³•å¾‹æ¡æ–‡ä¸æŠ€æœ¯æ§åˆ¶æªæ–½ä¹‹é—´çš„è§„èŒƒæ˜ å°„(normative mapping)ï¼Œå°†å¤æ‚çš„ç›‘ç®¡è¦æ±‚è½¬åŒ–ä¸ºå…·ä½“çš„æŠ€æœ¯å®æ–½è·¯å¾„ã€‚é’ˆå¯¹æ£€ç´¢å¢å¼ºç”Ÿæˆ(RAG)å’Œå¤§å‹è¯­è¨€æ¨¡å‹(LLM)ç³»ç»Ÿï¼Œç ”ç©¶è€…è®¾è®¡äº†ä¸€å¥—æ³•åº­ç§‘å­¦æ¶æ„(forensic architecture)ä»¥åŠåŸºäºæ³•å¾‹é£é™©åŠ æƒçš„è¯„ä¼°æŒ‡æ ‡ä½“ç³»ã€‚ä½œä¸ºæ ¸å¿ƒè´¡çŒ®ï¼Œè®ºæ–‡æ¨å‡ºäº†åä¸º rag-forense çš„å¼€æºå®ç°å·¥å…·ï¼Œå¹¶æä¾›äº†é…å¥—çš„å®éªŒåè®®ä»¥ç”Ÿæˆå¯å®¡è®¡çš„è¯æ®ã€‚è¯¥æ¡†æ¶ä¸ºæ³•å¾‹åº”ç”¨åœºæ™¯æä¾›äº†å…·å¤‡é«˜å¯è¿½æº¯æ€§çš„æŠ€æœ¯-æ³•å¾‹ä¿éšœï¼Œç¡®ä¿äº†AIç³»ç»Ÿåœ¨å¸æ³•é¢†åŸŸåº”ç”¨æ—¶çš„åˆè§„æ€§ä¸é€æ˜åº¦ã€‚",
      "categories": [
        "cs.CY",
        "cs.AI"
      ],
      "primary_category": "cs.CY",
      "comment": "in Spanish and English languages",
      "pdf_url": "https://arxiv.org/pdf/2510.12830v2",
      "published_date": "2025-10-12 07:32:55 UTC",
      "updated_date": "2025-12-23 21:36:02 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T02:55:32.871513+00:00"
    },
    {
      "arxiv_id": "2510.10481v1",
      "title": "UltraLLaDA: Scaling the Context Length to 128K for Diffusion Large Language Models",
      "title_zh": "UltraLLaDAï¼šå°†æ‰©æ•£å¤§è¯­è¨€æ¨¡å‹çš„ä¸Šä¸‹æ–‡é•¿åº¦æ‰©å±•è‡³ 128K",
      "authors": [
        "Guangxin He",
        "Shen Nie",
        "Fengqi Zhu",
        "Yuankang Zhao",
        "Tianyi Bai",
        "Ran Yan",
        "Jie Fu",
        "Chongxuan Li",
        "Binhang Yuan"
      ],
      "abstract": "Diffusion LLMs have attracted growing interest, with plenty of recent work emphasizing their great potential in various downstream tasks; yet the long-context behavior of diffusion LLMs remains largely uncharted. We present a case study of post-training techniques for extending the context window of diffusion LLMs (i.e., LLaDA) without retraining from scratch. We show that a simple modification to the standard Rotary Positional Embeddings (RoPE) extension effectively accommodates the probabilistic modeling inherent in the diffusion process, enabling stable scaling to longer context ranges. We further compare masking strategies used during post-training and analyze their impact on optimization stability and long-range recall. Instantiating these insights, we introduce UltraLLaDA, a diffusion LLM with a 128K-token context window that, in our empirical evaluation on long-context tasks, significantly outperforms training-free baselines. Our experimental results highlight the special positional extension as a key lever for scaling diffusion LLMs to extended contexts and offer practical guidance for practitioners seeking 128K-scale context via efficient post-training.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹æ‰©æ•£å¤§è¯­è¨€æ¨¡å‹ (Diffusion LLMs) åœ¨é•¿æ–‡æœ¬å¤„ç†é¢†åŸŸçš„ç©ºç™½ï¼Œæå‡ºäº† UltraLLaDAï¼Œæ—¨åœ¨ä¸ä»å¤´å¼€å§‹é‡è®­çš„æƒ…å†µä¸‹å°†ä¸Šä¸‹æ–‡çª—å£æ‰©å±•è‡³ 128Kã€‚ä½œè€…é€šè¿‡å¯¹æ—‹è½¬ä½ç½®åµŒå…¥ (Rotary Positional Embeddings, RoPE) è¿›è¡Œæ”¹è¿›ï¼Œä½¿å…¶æœ‰æ•ˆé€‚é…äº†æ‰©æ•£è¿‡ç¨‹ä¸­çš„æ¦‚ç‡å»ºæ¨¡ï¼Œä»è€Œå®ç°äº†ç¨³å®šçš„ä¸Šä¸‹æ–‡æ‰©å±•ã€‚ç ”ç©¶è¿˜å¯¹æ¯”äº†åæœŸè®­ç»ƒ (post-training) ä¸­çš„æ©ç ç­–ç•¥ (masking strategies) å¯¹ä¼˜åŒ–ç¨³å®šæ€§å’Œé•¿ç¨‹å¬å›ç‡ (long-range recall) çš„å½±å“ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒUltraLLaDA åœ¨é•¿æ–‡æœ¬ä»»åŠ¡ä¸­çš„è¡¨ç°æ˜¾è‘—ä¼˜äºæ— éœ€è®­ç»ƒçš„åŸºçº¿æ¨¡å‹ï¼Œè¯æ˜äº†ç‰¹æ®Šçš„ä½ç½®ç¼–ç æ‰©å±•æ˜¯æ‰©æ•£æ¨¡å‹æ‰©å±•ä¸Šä¸‹æ–‡çš„å…³é”®ã€‚è¯¥å·¥ä½œä¸ºé€šè¿‡é«˜æ•ˆåæœŸè®­ç»ƒå®ç°å¤§è§„æ¨¡é•¿æ–‡æœ¬èƒ½åŠ›æä¾›äº†é‡è¦çš„å®è·µæŒ‡å—ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.10481v1",
      "published_date": "2025-10-12 07:26:56 UTC",
      "updated_date": "2025-10-12 07:26:56 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T02:55:36.070256+00:00"
    },
    {
      "arxiv_id": "2510.10480v2",
      "title": "Latent Retrieval Augmented Generation of Cross-Domain Protein Binders",
      "title_zh": "è·¨åŸŸè›‹ç™½è´¨ç»“åˆå‰‚çš„éšå¼æ£€ç´¢å¢å¼ºç”Ÿæˆ",
      "authors": [
        "Zishen Zhang",
        "Xiangzhe Kong",
        "Wenbing Huang",
        "Yang Liu"
      ],
      "abstract": "Designing protein binders targeting specific sites, which requires to generate realistic and functional interaction patterns, is a fundamental challenge in drug discovery. Current structure-based generative models are limited in generating nterfaces with sufficient rationality and interpretability. In this paper, we propose Retrieval-Augmented Diffusion for Aligned interface (RADiAnce), a new framework that leverages known interfaces to guide the design of novel binders. By unifying retrieval and generation in a shared contrastive latent space, our model efficiently identifies relevant interfaces for a given binding site and seamlessly integrates them through a conditional latent diffusion generator, enabling cross-domain interface transfer. Extensive exeriments show that RADiAnce significantly outperforms baseline models across multiple metrics, including binding affinity and recovery of geometries and interactions. Additional experimental results validate cross-domain generalization, demonstrating that retrieving interfaces from diverse domains, such as peptides, antibodies, and protein fragments, enhances the generation performance of binders for other domains. Our work establishes a new paradigm for protein binder design that successfully bridges retrieval-based knowledge and generative AI, opening new possibilities for drug discovery.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† RADiAnce (Retrieval-Augmented Diffusion for Aligned interface) æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³è›‹ç™½è´¨ç»“åˆå‰‚ (protein binders) è®¾è®¡ä¸­ç°æœ‰ç»“æ„ç”Ÿæˆæ¨¡å‹åœ¨ç•Œé¢åˆç†æ€§å’Œå¯è§£é‡Šæ€§æ–¹é¢çš„å±€é™ã€‚è¯¥æ¡†æ¶é€šè¿‡åœ¨å…±äº«çš„å¯¹æ¯”æ½œç©ºé—´ (shared contrastive latent space) ä¸­ç»Ÿä¸€æ£€ç´¢å’Œç”Ÿæˆè¿‡ç¨‹ï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿé«˜æ•ˆè¯†åˆ«ç‰¹å®šç»“åˆä½ç‚¹çš„ç›¸å…³ç•Œé¢ï¼Œå¹¶åˆ©ç”¨æ¡ä»¶æ½œæ‰©æ•£ç”Ÿæˆå™¨ (conditional latent diffusion generator) å®ç°è·¨åŸŸç•Œé¢çš„è¿ç§»ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒRADiAnce åœ¨ç»“åˆäº²å’ŒåŠ› (binding affinity) ä»¥åŠå‡ ä½•å’Œç›¸äº’ä½œç”¨çš„æ¢å¤ç­‰å¤šä¸ªæŒ‡æ ‡ä¸Šæ˜¾è‘—ä¼˜äºåŸºçº¿æ¨¡å‹ã€‚æ­¤å¤–ï¼Œç ”ç©¶éªŒè¯äº†è¯¥æ¨¡å‹çš„è·¨åŸŸæ³›åŒ–èƒ½åŠ›ï¼Œè¯æ˜ä»å¤šè‚½ã€æŠ—ä½“å’Œè›‹ç™½è´¨ç‰‡æ®µç­‰ä¸åŒé¢†åŸŸæ£€ç´¢ç•Œé¢å¯ä»¥å¢å¼ºå…¶ä»–é¢†åŸŸçš„ç»“åˆå‰‚ç”Ÿæˆæ€§èƒ½ã€‚è¿™é¡¹å·¥ä½œæˆåŠŸåœ°å°†æ£€ç´¢å¢å¼ºç”Ÿæˆ (RAG) æŠ€æœ¯å¼•å…¥è›‹ç™½è´¨ç»“åˆå‰‚è®¾è®¡ï¼Œä¸ºè¯ç‰©å‘ç°å»ºç«‹äº†ä¸€ç§æ¡¥æ¥å·²çŸ¥çŸ¥è¯†ä¸ç”Ÿæˆå¼ AI çš„æ–°èŒƒå¼ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.10480v2",
      "published_date": "2025-10-12 07:26:11 UTC",
      "updated_date": "2025-10-16 11:55:27 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T02:56:15.565883+00:00"
    },
    {
      "arxiv_id": "2510.13856v1",
      "title": "Multimodal Retrieval-Augmented Generation with Large Language Models for Medical VQA",
      "title_zh": "åŸºäºå¤§è¯­è¨€æ¨¡å‹çš„åŒ»å­¦è§†è§‰é—®ç­”å¤šæ¨¡æ€æ£€ç´¢å¢å¼ºç”Ÿæˆ",
      "authors": [
        "A H M Rezaul Karim",
        "Ozlem Uzuner"
      ],
      "abstract": "Medical Visual Question Answering (MedVQA) enables natural language queries over medical images to support clinical decision-making and patient care. The MEDIQA-WV 2025 shared task addressed wound-care VQA, requiring systems to generate free-text responses and structured wound attributes from images and patient queries. We present the MasonNLP system, which employs a general-domain, instruction-tuned large language model with a retrieval-augmented generation (RAG) framework that incorporates textual and visual examples from in-domain data. This approach grounds outputs in clinically relevant exemplars, improving reasoning, schema adherence, and response quality across dBLEU, ROUGE, BERTScore, and LLM-based metrics. Our best-performing system ranked 3rd among 19 teams and 51 submissions with an average score of 41.37%, demonstrating that lightweight RAG with general-purpose LLMs -- a minimal inference-time layer that adds a few relevant exemplars via simple indexing and fusion, with no extra training or complex re-ranking -- provides a simple and effective baseline for multimodal clinical NLP tasks.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹åŒ»ç–—è§†è§‰é—®ç­”(Medical Visual Question Answering, MedVQA)ä¸­çš„ä¼¤å£æŠ¤ç†ä»»åŠ¡ï¼Œå¼€å‘äº†MasonNLPç³»ç»Ÿã€‚è¯¥ç³»ç»Ÿåˆ©ç”¨é€šç”¨é¢†åŸŸä¸”ç»è¿‡æŒ‡ä»¤å¾®è°ƒçš„å¤§è¯­è¨€æ¨¡å‹(Large Language Models, LLMs)ï¼Œç»“åˆäº†ä¸€ä¸ªæ•´åˆäº†é¢†åŸŸå†…æ–‡æœ¬å’Œè§†è§‰ç¤ºä¾‹çš„å¤šæ¨¡æ€æ£€ç´¢å¢å¼ºç”Ÿæˆ(Multimodal Retrieval-Augmented Generation, RAG)æ¡†æ¶ã€‚é€šè¿‡ä¸´åºŠç›¸å…³èŒƒä¾‹çš„å¼•å¯¼ï¼Œè¯¥æ–¹æ³•åœ¨dBLEUã€ROUGEå’ŒBERTScoreç­‰æŒ‡æ ‡ä¸Šå‡è¡¨ç°ä¼˜å¼‚ï¼Œæ˜¾è‘—æå‡äº†æ¨ç†èƒ½åŠ›å’Œå“åº”è´¨é‡ã€‚æœ€ç»ˆï¼Œè¯¥ç³»ç»Ÿåœ¨MEDIQA-WV 2025ç«èµ›çš„19æ”¯å›¢é˜Ÿä¸­æ’åç¬¬3ï¼Œè¯æ˜äº†è¿™ç§æ— éœ€é¢å¤–è®­ç»ƒã€ä»…é€šè¿‡ç®€å•ç´¢å¼•å’Œèåˆçš„è½»é‡çº§RAGæ–¹æ¡ˆèƒ½ä¸ºå¤šæ¨¡æ€ä¸´åºŠè‡ªç„¶è¯­è¨€å¤„ç†(NLP)ä»»åŠ¡æä¾›é«˜æ•ˆçš„åŸºå‡†ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.13856v1",
      "published_date": "2025-10-12 07:03:58 UTC",
      "updated_date": "2025-10-12 07:03:58 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T02:55:41.907136+00:00"
    },
    {
      "arxiv_id": "2510.10475v1",
      "title": "Assessing Large Language Models for Structured Medical Order Extraction",
      "title_zh": "ç»“æ„åŒ–åŒ»ç–—åŒ»å˜±æå–çš„å¤§è¯­è¨€æ¨¡å‹è¯„ä¼°",
      "authors": [
        "A H M Rezaul Karim",
        "Ozlem Uzuner"
      ],
      "abstract": "Medical order extraction is essential for structuring actionable clinical information, supporting decision-making, and enabling downstream applications such as documentation and workflow automation. Orders may be embedded in diverse sources, including electronic health records, discharge summaries, and multi-turn doctor-patient dialogues, and can span categories such as medications, laboratory tests, imaging studies, and follow-up actions. The MEDIQA-OE 2025 shared task focuses on extracting structured medical orders from extended conversational transcripts, requiring the identification of order type, description, reason, and provenance. We present the MasonNLP submission, which ranked 5th among 17 participating teams with 105 total submissions. Our approach uses a general-purpose, instruction-tuned LLaMA-4 17B model without domain-specific fine-tuning, guided by a single in-context example. This few-shot configuration achieved an average F1 score of 37.76, with notable improvements in reason and provenance accuracy. These results demonstrate that large, non-domain-specific LLMs, when paired with effective prompt engineering, can serve as strong, scalable baselines for specialized clinical NLP tasks.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹MEDIQA-OE 2025ä»»åŠ¡ï¼Œè¯„ä¼°äº†å¤§å‹è¯­è¨€æ¨¡å‹(LLMs)åœ¨ä»åŒ»æ‚£å¯¹è¯è½¬å½•æœ¬ä¸­æå–ç»“æ„åŒ–åŒ»ç–—åŒ»å˜±(Medical Order Extraction)çš„èƒ½åŠ›ã€‚ç ”ç©¶å±•ç¤ºäº†MasonNLPå›¢é˜Ÿçš„æ–¹æ¡ˆï¼Œè¯¥æ–¹æ¡ˆä½¿ç”¨é€šç”¨çš„LLaMA-4 17BæŒ‡ä»¤å¾®è°ƒæ¨¡å‹ï¼Œåœ¨ä¸è¿›è¡Œé¢†åŸŸç‰¹å®šå¾®è°ƒçš„æƒ…å†µä¸‹ï¼Œä»…é€šè¿‡å•æ ·æœ¬(single in-context example)çš„å°‘æ ·æœ¬(few-shot)é…ç½®ï¼Œå®ç°äº†åŒ»å˜±ç±»å‹ã€æè¿°ã€åŸå› åŠæ¥æºçš„è‡ªåŠ¨æå–ã€‚åœ¨17æ”¯å‚èµ›é˜Ÿä¼ä¸­ï¼Œè¯¥æ–¹æ³•ä»¥37.76çš„å¹³å‡F1åˆ†æ•°è·å¾—ç¬¬5åï¼Œå¹¶åœ¨åŸå› å’Œæ¥æºçš„å‡†ç¡®æ€§æ–¹é¢è¡¨ç°çªå‡ºã€‚å®éªŒç»“æœè¯æ˜ï¼Œé€šè¿‡æœ‰æ•ˆçš„æç¤ºå·¥ç¨‹(prompt engineering)ï¼Œå¤§å‹é€šç”¨è¯­è¨€æ¨¡å‹å¯ä»¥ä½œä¸ºä¸“ä¸šåŒ–ä¸´åºŠè‡ªç„¶è¯­è¨€å¤„ç†(clinical NLP)ä»»åŠ¡çš„å¼ºå¤§ä¸”æ˜“äºæ‰©å±•çš„åŸºå‡†æ¨¡å‹ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.10475v1",
      "published_date": "2025-10-12 06:56:10 UTC",
      "updated_date": "2025-10-12 06:56:10 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T02:55:44.167975+00:00"
    },
    {
      "arxiv_id": "2510.10472v1",
      "title": "FML-bench: A Benchmark for Automatic ML Research Agents Highlighting the Importance of Exploration Breadth",
      "title_zh": "FML-benchï¼šå¼ºè°ƒæ¢ç´¢å¹¿åº¦é‡è¦æ€§çš„è‡ªåŠ¨åŒ–æœºå™¨å­¦ä¹ ç ”ç©¶æ™ºèƒ½ä½“åŸºå‡†",
      "authors": [
        "Qiran Zou",
        "Hou Hei Lam",
        "Wenhao Zhao",
        "Yiming Tang",
        "Tingting Chen",
        "Samson Yu",
        "Tianyi Zhang",
        "Chang Liu",
        "Xiangyang Ji",
        "Dianbo Liu"
      ],
      "abstract": "Large language models (LLMs) have sparked growing interest in automatic machine learning research agents. Among them, agents capable of autonomously proposing ideas and conducting machine learning experiments are particularly promising, as they maximize research automation and accelerate scientific progress by iteratively refining ideas based on experimental results. However, comprehensively evaluating such agents remains challenging. Existing benchmarks tend to overemphasize engineering aspects while neglecting academic rigor, creating barriers that obscure a clear assessment of an agent's scientific capabilities in machine learning research. They also suffer from limited task diversity, an overemphasis on application-oriented tasks over fundamental research problems, and limited scalability to realistic research settings. To address these limitations, we introduce FML-bench, a benchmark designed to evaluate automatic machine learning research agents on 8 diverse and fundamental machine learning research problems. It reduces coding burden, emphasizes fundamental problems rather than specific use cases, offers high task diversity, and is extensible to real-world machine learning GitHub repositories. Furthermore, we present a unified evaluation framework with five complementary metrics, designed to comprehensively assess agent performance on our benchmark. We evaluate state-of-the-art automatic research agents on FML-bench, and find that agents employing broad research exploration strategies outperform those focusing on narrow but deep exploration. These findings suggest that emphasizing the breadth of exploration may lead to more effective research outcomes than focusing solely on incremental refinement. Our benchmark is available at https://github.com/qrzou/FML-bench.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ç°æœ‰è¯„ä¼°åŸºå‡†åœ¨å­¦æœ¯ä¸¥è°¨æ€§ã€ä»»åŠ¡å¤šæ ·æ€§å’Œå¯æ‰©å±•æ€§æ–¹é¢çš„ä¸è¶³ï¼Œæå‡ºäº† FML-benchï¼Œè¿™æ˜¯ä¸€ä¸ªä¸“é—¨ç”¨äºè¯„ä¼°è‡ªåŠ¨æœºå™¨å­¦ä¹ ç ”ç©¶æ™ºèƒ½ä½“çš„åŸºå‡†æµ‹è¯•ã€‚FML-bench æ¶µç›–äº† 8 ä¸ªåŸºç¡€ä¸”å¤šæ ·åŒ–çš„æœºå™¨å­¦ä¹ ç ”ç©¶é—®é¢˜ï¼Œé€šè¿‡å‡è½»ç¼–ç¨‹è´Ÿæ‹…å¹¶å¼ºè°ƒåŸºæœ¬ç§‘å­¦æŒ‘æˆ˜ï¼Œæä¾›äº†æ›´è´´è¿‘çœŸå®ç§‘ç ”åœºæ™¯çš„è¯„ä¼°ç¯å¢ƒã€‚æ­¤å¤–ï¼Œç ”ç©¶å›¢é˜Ÿè¿˜å¼•å…¥äº†ä¸€ä¸ªåŒ…å«äº”é¡¹äº’è¡¥æŒ‡æ ‡çš„ç»Ÿä¸€è¯„ä¼°æ¡†æ¶ï¼Œç”¨ä»¥å…¨é¢è¡¡é‡æ™ºèƒ½ä½“çš„ç ”ç©¶èƒ½åŠ›ã€‚å®éªŒåˆ†æå‘ç°ï¼Œé‡‡ç”¨å¹¿åº¦æ¢ç´¢ç­–ç•¥çš„æ™ºèƒ½ä½“è¡¨ç°ä¼˜äºä»…å…³æ³¨æ·±å±‚å¢é‡æ”¹è¿›çš„æ™ºèƒ½ä½“ï¼Œè¿™è¡¨æ˜æ¢ç´¢çš„å¹¿åº¦å¯¹äºå–å¾—æœ‰æ•ˆç ”ç©¶æˆæœè‡³å…³é‡è¦ã€‚ç›®å‰ï¼Œè¯¥åŸºå‡†å·²åœ¨ GitHub å¼€æºã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "Our benchmark is available at: https://github.com/qrzou/FML-bench",
      "pdf_url": "https://arxiv.org/pdf/2510.10472v1",
      "published_date": "2025-10-12 06:41:05 UTC",
      "updated_date": "2025-10-12 06:41:05 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T02:55:52.361730+00:00"
    },
    {
      "arxiv_id": "2510.10467v1",
      "title": "AnyBCQ: Hardware Efficient Flexible Binary-Coded Quantization for Multi-Precision LLMs",
      "title_zh": "AnyBCQï¼šé¢å‘å¤šç²¾åº¦å¤§è¯­è¨€æ¨¡å‹çš„ç¡¬ä»¶é«˜æ•ˆçµæ´»äºŒè¿›åˆ¶ç¼–ç é‡åŒ–",
      "authors": [
        "Gunho Park",
        "Jeongin Bae",
        "Beomseok Kwon",
        "Byeongwook Kim",
        "Se Jung Kwon",
        "Dongsoo Lee"
      ],
      "abstract": "The deployment of large language models (LLMs) is increasingly constrained by memory and latency bottlenecks, motivating the need for quantization techniques that flexibly balance accuracy and efficiency. Recent work has introduced multi-precision models, which enable inference at multiple precisions within a single model depending on runtime constraints. To support such flexibility, quantized weights are often stored as bit-planes, where hardware efficiency improves when the compute operates directly at the bit-plane level and activates only the precision required by each request. In this work, we present AnyBCQ, a hardware-friendly multi-precision extension of Binary-Coded Quantization (BCQ) that supports direct bit-plane operations. By representing weights as binary bit-planes with corresponding scale factors, AnyBCQ enables bit-plane-level computation and maps naturally to accelerator-friendly, bit-parallel arithmetic. Our progressive precision expansion mechanism incrementally refines scaling factors while reusing previously assigned binary codes, yielding monotonic improvements in accuracy as additional bits are enabled. We further co-design a specialized kernel that exploits the BCQ structure to support dynamic per-request precision selection with negligible overhead. Experiments on recent LLMs demonstrate that AnyBCQ significantly narrows the accuracy drop in the low-bit regime (e.g. 2-bit), remains competitive at higher precision, and achieves throughput gains of up to 3.0x over half precision and 1.2x over state-of-the-art multi-precision methods. By aligning algorithmic flexibility with hardware efficiency, AnyBCQ provides a practical foundation for multi-precision LLM deployment across diverse service-level objectives.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤§è¯­è¨€æ¨¡å‹(LLMs)éƒ¨ç½²ä¸­çš„æ˜¾å­˜å’Œå»¶è¿Ÿç“¶é¢ˆï¼Œæå‡ºäº†AnyBCQï¼Œä¸€ç§æ—¨åœ¨å¹³è¡¡ç²¾åº¦ä¸æ•ˆç‡çš„ç¡¬ä»¶å‹å¥½å‹å¤šç²¾åº¦Binary-Coded Quantization (BCQ)æ‰©å±•æ–¹æ³•ã€‚AnyBCQå°†æƒé‡è¡¨ç¤ºä¸ºäºŒè¿›åˆ¶ä½å¹³é¢(bit-planes)åŠç›¸åº”çš„æ¯”ä¾‹å› å­ï¼Œæ”¯æŒç›´æ¥çš„ä½å¹³é¢çº§è®¡ç®—ï¼Œå¹¶èƒ½è‡ªç„¶æ˜ å°„åˆ°åŠ é€Ÿå™¨å‹å¥½çš„ä½å¹¶è¡Œç®—æœ¯ä¸­ã€‚é€šè¿‡æ¸è¿›å¼ç²¾åº¦æ‰©å±•æœºåˆ¶ï¼Œè¯¥æ–¹æ³•åœ¨é‡ç”¨å…ˆå‰åˆ†é…çš„äºŒè¿›åˆ¶ä»£ç çš„åŒæ—¶å¢é‡ä¼˜åŒ–æ¯”ä¾‹å› å­ï¼Œç¡®ä¿éšç€å¯ç”¨æ›´å¤šä½æ•°ï¼Œæ¨¡å‹ç²¾åº¦èƒ½å¤Ÿå•è°ƒæå‡ã€‚ç ”ç©¶å›¢é˜Ÿè¿˜ååŒè®¾è®¡äº†ä¸“ç”¨å†…æ ¸ï¼Œåˆ©ç”¨BCQç»“æ„æ”¯æŒæä½å¼€é”€çš„åŠ¨æ€é€è¯·æ±‚ç²¾åº¦é€‰æ‹©ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒAnyBCQæ˜¾è‘—ç¼©å°äº†ä½æ¯”ç‰¹ï¼ˆå¦‚2-bitï¼‰ä¸‹çš„ç²¾åº¦æŸå¤±ï¼Œåœ¨æ›´é«˜ç²¾åº¦ä¸‹ä¿æŒç«äº‰åŠ›ï¼Œä¸”ååé‡æ¯”åŠç²¾åº¦(half precision)æå‡é«˜è¾¾3.0å€ï¼Œæ¯”ç°æœ‰çš„å¤šç²¾åº¦æ–¹æ³•æå‡1.2å€ã€‚è¿™ä¸€æ–¹æ¡ˆæœ‰æ•ˆå®ç°äº†ç®—æ³•çµæ´»æ€§ä¸ç¡¬ä»¶æ•ˆç‡çš„ç»“åˆï¼Œä¸ºä¸åŒæœåŠ¡ç­‰çº§ç›®æ ‡ä¸‹çš„å¤šç²¾åº¦LLMséƒ¨ç½²æä¾›äº†å®ç”¨åŸºç¡€ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.10467v1",
      "published_date": "2025-10-12 06:20:38 UTC",
      "updated_date": "2025-10-12 06:20:38 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T02:56:27.892139+00:00"
    },
    {
      "arxiv_id": "2510.10465v1",
      "title": "LightSAE: Parameter-Efficient and Heterogeneity-Aware Embedding for IoT Multivariate Time Series Forecasting",
      "title_zh": "LightSAEï¼šé¢å‘ç‰©è”ç½‘å¤šå˜é‡æ—¶é—´åºåˆ—é¢„æµ‹çš„å‚æ•°é«˜æ•ˆå¼‚æ„æ„ŸçŸ¥åµŒå…¥",
      "authors": [
        "Yi Ren",
        "Xinjie Yu"
      ],
      "abstract": "Modern Internet of Things (IoT) systems generate massive, heterogeneous multivariate time series data. Accurate Multivariate Time Series Forecasting (MTSF) of such data is critical for numerous applications. However, existing methods almost universally employ a shared embedding layer that processes all channels identically, creating a representational bottleneck that obscures valuable channel-specific information. To address this challenge, we introduce a Shared-Auxiliary Embedding (SAE) framework that decomposes the embedding into a shared base component capturing common patterns and channel-specific auxiliary components modeling unique deviations. Within this decomposition, we \\rev{empirically observe} that the auxiliary components tend to exhibit low-rank and clustering characteristics, a structural pattern that is significantly less apparent when using purely independent embeddings. Consequently, we design LightSAE, a parameter-efficient embedding module that operationalizes these observed characteristics through low-rank factorization and a shared, gated component pool. Extensive experiments across 9 IoT-related datasets and 4 backbone architectures demonstrate LightSAE's effectiveness, achieving MSE improvements of up to 22.8\\% with only 4.0\\% parameter increase.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ç°ä»£ç‰©è”ç½‘(IoT)ç³»ç»Ÿä¸­æµ·é‡å¼‚æ„å¤šå…ƒæ—¶é—´åºåˆ—é¢„æµ‹(MTSF)é¢ä¸´çš„è¡¨å¾ç“¶é¢ˆï¼ŒæŒ‡å‡ºä¼ ç»Ÿå…±äº«åµŒå…¥å±‚å¿½ç•¥äº†é€šé“ç‰¹æœ‰çš„å…³é”®ä¿¡æ¯ã€‚ä¸ºæ­¤ï¼Œä½œè€…æå‡ºäº†ä¸€ä¸ªå…±äº«-è¾…åŠ©åµŒå…¥(SAE)æ¡†æ¶ï¼Œé€šè¿‡åˆ†è§£åµŒå…¥å±‚æ¥åŒæ—¶æ•æ‰é€šç”¨æ¨¡å¼å’Œé€šé“ç‰¹æœ‰çš„åå·®ã€‚åœ¨è§‚å¯Ÿåˆ°è¾…åŠ©ç»„ä»¶å…·æœ‰ä½ç§©(low-rank)å’Œèšç±»ç‰¹æ€§çš„åŸºç¡€ä¸Šï¼Œç ”ç©¶è¿›ä¸€æ­¥è®¾è®¡äº†è½»é‡çº§æ¨¡å— LightSAEï¼Œåˆ©ç”¨ä½ç§©åˆ†è§£å’Œå…±äº«é—¨æ§ç»„ä»¶æ± å®ç°å‚æ•°é«˜æ•ˆçš„åµŒå…¥ã€‚åœ¨9ä¸ªIoTæ•°æ®é›†å’Œ4ç§éª¨å¹²æ¶æ„ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼ŒLightSAE åœ¨ä»…å¢åŠ  4.0% å‚æ•°é‡çš„æƒ…å†µä¸‹ï¼Œå°†å‡æ–¹è¯¯å·®(MSE)é™ä½äº†é«˜è¾¾ 22.8%ã€‚è¯¥æ–¹æ¡ˆåœ¨æœ‰æ•ˆæ•è·å¼‚æ„ç‰¹å¾çš„åŒæ—¶æ˜¾è‘—æ§åˆ¶äº†æ¨¡å‹å¤æ‚åº¦ï¼Œä¸ºå¤„ç†å¤§è§„æ¨¡ IoT æ—¶é—´åºåˆ—æ•°æ®æä¾›äº†é«˜æ€§èƒ½çš„è§£å†³æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "Submitted to IEEE IoT-J",
      "pdf_url": "https://arxiv.org/pdf/2510.10465v1",
      "published_date": "2025-10-12 06:14:56 UTC",
      "updated_date": "2025-10-12 06:14:56 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T02:56:34.179009+00:00"
    },
    {
      "arxiv_id": "2510.10462v1",
      "title": "Learning from Disagreement: A Group Decision Simulation Framework for Robust Medical Image Segmentation",
      "title_zh": "ä»åˆ†æ­§ä¸­å­¦ä¹ ï¼šé¢å‘é²æ£’åŒ»å­¦å›¾åƒåˆ†å‰²çš„ç¾¤ä½“å†³ç­–æ¨¡æ‹Ÿæ¡†æ¶",
      "authors": [
        "Chen Zhong",
        "Yuxuan Yang",
        "Xinyue Zhang",
        "Ruohan Ma",
        "Yong Guo",
        "Gang Li",
        "Jupeng Li"
      ],
      "abstract": "Medical image segmentation annotation suffers from inter-rater variability (IRV) due to differences in annotators' expertise and the inherent blurriness of medical images. Standard approaches that simply average expert labels are flawed, as they discard the valuable clinical uncertainty revealed in disagreements. We introduce a fundamentally new approach with our group decision simulation framework, which works by mimicking the collaborative decision-making process of a clinical panel. Under this framework, an Expert Signature Generator (ESG) learns to represent individual annotator styles in a unique latent space. A Simulated Consultation Module (SCM) then intelligently generates the final segmentation by sampling from this space. This method achieved state-of-the-art results on challenging CBCT and MRI datasets (92.11% and 90.72% Dice scores). By treating expert disagreement as a useful signal instead of noise, our work provides a clear path toward more robust and trustworthy AI systems for healthcare.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹åŒ»ç–—å›¾åƒåˆ†å‰²æ ‡æ³¨ä¸­å› ä¸“å®¶ç»éªŒå·®å¼‚å’Œå›¾åƒæ¨¡ç³Šå¯¼è‡´çš„è¯„åˆ†è€…é—´å˜å¼‚æ€§(Inter-rater variability, IRV)é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§æ¨¡æ‹Ÿä¸´åºŠä¸“å®¶ç»„åä½œå†³ç­–çš„ç¾¤ä½“å†³ç­–æ¨¡æ‹Ÿæ¡†æ¶ã€‚è¯¥æ¡†æ¶çš„æ ¸å¿ƒåŒ…å«ä¸“å®¶ç‰¹å¾ç”Ÿæˆå™¨(Expert Signature Generator, ESG)å’Œæ¨¡æ‹Ÿä¼šè¯Šæ¨¡å—(Simulated Consultation Module, SCM)ï¼Œå‰è€…ç”¨äºåœ¨æ½œåœ¨ç©ºé—´ä¸­è¡¨å¾ä¸åŒæ ‡æ³¨è€…çš„ç‹¬ç‰¹é£æ ¼ï¼Œåè€…åˆ™é€šè¿‡é‡‡æ ·æ¨¡æ‹Ÿä¼šè¯Šè¿‡ç¨‹ä»¥ç”Ÿæˆæœ€ç»ˆçš„åˆ†å‰²ç»“æœã€‚å®éªŒè¯æ˜ï¼Œè¯¥æ–¹æ³•åœ¨æŒ‘æˆ˜æ€§çš„CBCTå’ŒMRIæ•°æ®é›†ä¸Šåˆ†åˆ«å–å¾—äº†92.11%å’Œ90.72%çš„Diceå¾—åˆ†ï¼Œè¾¾åˆ°äº†é¢†åŸŸå†…é¢†å…ˆæ°´å¹³ã€‚é€šè¿‡å°†ä¸“å®¶åˆ†æ­§è§†ä¸ºæœ‰ä»·å€¼çš„ä¸´åºŠä¿¡å·è€Œéå•çº¯çš„å™ªå£°ï¼Œè¯¥ç ”ç©¶æ˜¾è‘—æå‡äº†åˆ†å‰²ä»»åŠ¡çš„é²æ£’æ€§ï¼Œå¹¶ä¸ºå¼€å‘æ›´å…·å¯ä¿¡åº¦çš„åŒ»ç–—äººå·¥æ™ºèƒ½ç³»ç»Ÿæä¾›äº†ä¸€æ¡æœ‰æ•ˆè·¯å¾„ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.10462v1",
      "published_date": "2025-10-12 05:57:48 UTC",
      "updated_date": "2025-10-12 05:57:48 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T02:56:35.377725+00:00"
    },
    {
      "arxiv_id": "2510.10461v1",
      "title": "MedCoAct: Confidence-Aware Multi-Agent Collaboration for Complete Clinical Decision",
      "title_zh": "MedCoActï¼šé¢å‘å…¨æµç¨‹ä¸´åºŠå†³ç­–çš„ç½®ä¿¡åº¦æ„ŸçŸ¥å¤šæ™ºèƒ½ä½“åä½œ",
      "authors": [
        "Hongjie Zheng",
        "Zesheng Shi",
        "Ping Yi"
      ],
      "abstract": "Autonomous agents utilizing Large Language Models (LLMs) have demonstrated remarkable capabilities in isolated medical tasks like diagnosis and image analysis, but struggle with integrated clinical workflows that connect diagnostic reasoning and medication decisions. We identify a core limitation: existing medical AI systems process tasks in isolation without the cross-validation and knowledge integration found in clinical teams, reducing their effectiveness in real-world healthcare scenarios. To transform the isolation paradigm into a collaborative approach, we propose MedCoAct, a confidence-aware multi-agent framework that simulates clinical collaboration by integrating specialized doctor and pharmacist agents, and present a benchmark, DrugCareQA, to evaluate medical AI capabilities in integrated diagnosis and treatment workflows. Our results demonstrate that MedCoAct achieves 67.58\\% diagnostic accuracy and 67.58\\% medication recommendation accuracy, outperforming single agent framework by 7.04\\% and 7.08\\% respectively. This collaborative approach generalizes well across diverse medical domains, proving especially effective for telemedicine consultations and routine clinical scenarios, while providing interpretable decision-making pathways.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ç°æœ‰åŒ»ç–—äººå·¥æ™ºèƒ½ç³»ç»Ÿåœ¨é›†æˆè¯Šæ–­ä¸ç”¨è¯å†³ç­–å·¥ä½œæµä¸­å­˜åœ¨çš„å­¤ç«‹å¤„ç†å±€é™æ€§ï¼Œæå‡ºäº† MedCoActï¼Œä¸€ä¸ªå…·å¤‡ç½®ä¿¡åº¦æ„ŸçŸ¥ï¼ˆconfidence-awareï¼‰çš„å¤šæ™ºèƒ½ä½“åä½œæ¡†æ¶ã€‚MedCoAct é€šè¿‡æ¨¡æ‹Ÿä¸´åºŠå›¢é˜Ÿä¸­åŒ»ç”Ÿä¸è¯å‰‚å¸ˆçš„åä½œæ¨¡å¼ï¼Œå®ç°äº†è¯Šæ–­æ¨ç†ä¸è¯ç‰©æ²»ç–—å»ºè®®çš„æœ‰æ•ˆæ•´åˆã€‚ä¸ºäº†è¯„ä¼°è¿™ä¸€é›†æˆèƒ½åŠ›çš„è¡¨ç°ï¼Œç ”ç©¶è€…åŒæ­¥æ¨å‡ºäº† DrugCareQA åŸºå‡†æµ‹è¯•ã€‚å®éªŒæ•°æ®è¡¨æ˜ï¼ŒMedCoAct åœ¨è¯Šæ–­å‡†ç¡®ç‡å’Œç”¨è¯å»ºè®®å‡†ç¡®ç‡ä¸Šå‡è¾¾åˆ° 67.58%ï¼Œç›¸è¾ƒäºå•æ™ºèƒ½ä½“æ¡†æ¶åˆ†åˆ«æå‡äº† 7.04% å’Œ 7.08%ã€‚è¯¥æ¡†æ¶åœ¨è¿œç¨‹åŒ»ç–—å’Œå¸¸è§„ä¸´åºŠåœºæ™¯ä¸­å±•ç°å‡ºä¼˜å¼‚çš„æ³›åŒ–æ€§èƒ½ï¼Œå¹¶ä¸ºå¤æ‚çš„ä¸´åºŠå†³ç­–æä¾›äº†å…·æœ‰å¯è§£é‡Šæ€§çš„è·¯å¾„ï¼Œæœ‰æ•ˆæ¨åŠ¨äº†ä»å­¤ç«‹ä»»åŠ¡å¤„ç†å‘ååŒä¸´åºŠå†³ç­–çš„èŒƒå¼è½¬å˜ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.10461v1",
      "published_date": "2025-10-12 05:52:31 UTC",
      "updated_date": "2025-10-12 05:52:31 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T02:56:38.073804+00:00"
    },
    {
      "arxiv_id": "2510.10460v1",
      "title": "Testing and Enhancing Multi-Agent Systems for Robust Code Generation",
      "title_zh": "é¢å‘é²æ£’ä»£ç ç”Ÿæˆçš„å¤šæ™ºèƒ½ä½“ç³»ç»Ÿæµ‹è¯•ä¸å¢å¼º",
      "authors": [
        "Zongyi Lyu",
        "Songqiang Chen",
        "Zhenlan Ji",
        "Liwen Wang",
        "Shuai Wang",
        "Daoyuan Wu",
        "Wenxuan Wang",
        "Shing-Chi Cheung"
      ],
      "abstract": "Multi-agent systems (MASs) have emerged as a promising paradigm for automated code generation, demonstrating impressive performance on established benchmarks by decomposing complex coding tasks across specialized agents with different roles. Despite their prosperous development and adoption, their robustness remains pressingly under-explored, raising critical concerns for real-world deployment. This paper presents the first comprehensive study examining the robustness of MASs for code generation through a fuzzing-based testing approach. By designing a fuzzing pipeline incorporating semantic-preserving mutation operators and a novel fitness function, we assess mainstream MASs across multiple datasets and LLMs. Our findings reveal substantial robustness flaws of various popular MASs: they fail to solve 7.9%-83.3% of problems they initially resolved successfully after applying the semantic-preserving mutations. Through comprehensive failure analysis, we identify a common yet largely overlooked cause of the robustness issue: miscommunications between planning and coding agents, where plans lack sufficient detail and coding agents misinterpret intricate logic, aligning with the challenges inherent in a multi-stage information transformation process. Accordingly, we also propose a repairing method that encompasses multi-prompt generation and introduces a new monitor agent to address this issue. Evaluation shows that our repairing method effectively enhances the robustness of MASs by solving 40.0%-88.9% of identified failures. Our work uncovers critical robustness flaws in MASs and provides effective mitigation strategies, contributing essential insights for developing more reliable MASs for code generation.",
      "tldr_zh": "æœ¬ç ”ç©¶æ¢è®¨äº†å¤šæ™ºèƒ½ä½“ç³»ç»Ÿ (Multi-agent systems, MASs) åœ¨è‡ªåŠ¨ä»£ç ç”Ÿæˆä¸­çš„é²æ£’æ€§ (robustness) é—®é¢˜ï¼ŒæŒ‡å‡ºå°½ç®¡å…¶åœ¨å¤æ‚ä»»åŠ¡ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œä½†åœ¨å®é™…éƒ¨ç½²ä¸­çš„å¯é æ€§ä»ç¼ºä¹æ·±å…¥ç ”ç©¶ã€‚ä½œè€…æå‡ºäº†é¦–ä¸ªåŸºäºæ¨¡ç³Šæµ‹è¯• (fuzzing-based testing) çš„ç»¼åˆè¯„ä¼°æ¡†æ¶ï¼Œé€šè¿‡è®¾è®¡ä¿æŒè¯­ä¹‰çš„å˜å¼‚ç®—å­ (semantic-preserving mutation operators) å’Œæ–°å‹é€‚åº”åº¦å‡½æ•° (fitness function)ï¼Œå¯¹ä¸»æµå¤šæ™ºèƒ½ä½“ç³»ç»Ÿè¿›è¡Œäº†å…¨é¢æµ‹è¯•ã€‚å®éªŒç»“æœæ­ç¤ºäº†æ˜¾è‘—çš„é²æ£’æ€§ç¼ºé™·ï¼šåœ¨åº”ç”¨è¯­ä¹‰ä¿æŒå˜å¼‚åï¼ŒåŸæœ¬èƒ½æˆåŠŸè§£å†³çš„é—®é¢˜ä¸­æœ‰ 7.9% è‡³ 83.3% æ— æ³•é€šè¿‡ã€‚é€šè¿‡å¤±æ•ˆåˆ†æï¼Œç ”ç©¶å‘ç°æ ¸å¿ƒé—®é¢˜åœ¨äºè§„åˆ’æ™ºèƒ½ä½“ (planning agents) ä¸ä»£ç ç”Ÿæˆæ™ºèƒ½ä½“ (coding agents) ä¹‹é—´çš„æ²Ÿé€šä¸ç•…ï¼Œå¯¼è‡´ç»†èŠ‚ç¼ºå¤±å’Œé€»è¾‘è¯¯è¯»ã€‚é’ˆå¯¹è¯¥é—®é¢˜ï¼Œä½œè€…æå‡ºäº†ä¸€ç§åŒ…å«å¤šæç¤ºè¯ç”Ÿæˆ (multi-prompt generation) å¹¶å¼•å…¥ç›‘æ§æ™ºèƒ½ä½“ (monitor agent) çš„ä¿®å¤æ–¹æ³•ã€‚è¯„ä¼°è¡¨æ˜ï¼Œè¯¥æ–¹æ³•æœ‰æ•ˆè§£å†³äº† 40.0% è‡³ 88.9% çš„è¯†åˆ«æ•…éšœï¼Œä¸ºå¼€å‘æ›´å¯é çš„ä»£ç ç”Ÿæˆå¤šæ™ºèƒ½ä½“ç³»ç»Ÿæä¾›äº†é‡è¦çš„ç¼“è§£ç­–ç•¥å’Œå®è·µè§è§£ã€‚",
      "categories": [
        "cs.SE",
        "cs.AI"
      ],
      "primary_category": "cs.SE",
      "comment": "19pages, 5 figures",
      "pdf_url": "https://arxiv.org/pdf/2510.10460v1",
      "published_date": "2025-10-12 05:45:04 UTC",
      "updated_date": "2025-10-12 05:45:04 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T02:56:39.987123+00:00"
    },
    {
      "arxiv_id": "2510.10459v1",
      "title": "NIM: Neuro-symbolic Ideographic Metalanguage for Inclusive Communication",
      "title_zh": "NIMï¼šé¢å‘åŒ…å®¹æ€§äº¤æµçš„ç¥ç»ç¬¦å·è¡¨æ„å…ƒè¯­è¨€",
      "authors": [
        "Prawaal Sharma",
        "Poonam Goyal",
        "Navneet Goyal",
        "Vidisha Sharma"
      ],
      "abstract": "Digital communication has become the cornerstone of modern interaction, enabling rapid, accessible, and interactive exchanges. However, individuals with lower academic literacy often face significant barriers, exacerbating the \"digital divide\". In this work, we introduce a novel, universal ideographic metalanguage designed as an innovative communication framework that transcends academic, linguistic, and cultural boundaries. Our approach leverages principles of Neuro-symbolic AI, combining neural-based large language models (LLMs) enriched with world knowledge and symbolic knowledge heuristics grounded in the linguistic theory of Natural Semantic Metalanguage (NSM). This enables the semantic decomposition of complex ideas into simpler, atomic concepts. Adopting a human-centric, collaborative methodology, we engaged over 200 semi-literate participants in defining the problem, selecting ideographs, and validating the system. With over 80\\% semantic comprehensibility, an accessible learning curve, and universal adaptability, our system effectively serves underprivileged populations with limited formal education.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† NIM (Neuro-symbolic Ideographic Metalanguage)ï¼Œè¿™æ˜¯ä¸€ç§æ—¨åœ¨æ¶ˆé™¤å­¦æœ¯ã€è¯­è¨€å’Œæ–‡åŒ–éšœç¢çš„é€šç”¨è¡¨æ„å…ƒè¯­è¨€æ¡†æ¶ã€‚ä¸ºè§£å†³ä½å­¦æœ¯ç´ å…»äººç¾¤åœ¨æ•°å­—é€šä¿¡ä¸­é¢ä¸´çš„â€œæ•°å­—é¸¿æ²Ÿâ€æŒ‘æˆ˜ï¼Œè¯¥æ–¹æ³•ç»“åˆäº† Neuro-symbolic AI ç­–ç•¥ï¼Œå°†å…·å¤‡ä¸°å¯ŒçŸ¥è¯†çš„ç¥ç»å¤§è¯­è¨€æ¨¡å‹ (LLMs) ä¸åŸºäºè‡ªç„¶è¯­ä¹‰å…ƒè¯­è¨€ (Natural Semantic Metalanguage, NSM) è¯­è¨€å­¦ç†è®ºçš„ç¬¦å·çŸ¥è¯†å¯å‘æ³•ç›¸ç»“åˆã€‚é€šè¿‡è¿™ç§æ–¹å¼ï¼Œç³»ç»Ÿèƒ½å¤Ÿå°†å¤æ‚æ€æƒ³è¯­ä¹‰åˆ†è§£ä¸ºç®€å•çš„åŸå­æ¦‚å¿µã€‚ç ”ç©¶å›¢é˜Ÿé‡‡ç”¨ä»¥äººä¸ºæœ¬çš„åä½œæ–¹å¼ï¼Œé‚€è¯·äº† 200 å¤šåå—æ•™è‚²ç¨‹åº¦è¾ƒä½çš„å‚ä¸è€…å‚ä¸ç³»ç»Ÿå®šä¹‰ã€ç¬¦å·é€‰æ‹©ä¸éªŒè¯ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥ç³»ç»Ÿçš„è¯­ä¹‰å¯ç†è§£æ€§è¶…è¿‡ 80%ï¼Œå…·æœ‰æä½çš„å­¦ä¹ é—¨æ§›å’Œæ™®éé€‚åº”æ€§ï¼Œèƒ½å¤Ÿæœ‰æ•ˆä¸ºå…¨çƒå—æ•™è‚²ç¨‹åº¦æœ‰é™çš„å¼±åŠ¿ç¾¤ä½“æä¾›åŒ…å®¹æ€§çš„æ²Ÿé€šæ”¯æŒã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "9 pages, EMNLP Findings 2025",
      "pdf_url": "https://arxiv.org/pdf/2510.10459v1",
      "published_date": "2025-10-12 05:43:43 UTC",
      "updated_date": "2025-10-12 05:43:43 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T02:56:42.683718+00:00"
    },
    {
      "arxiv_id": "2510.10454v1",
      "title": "Traj-CoA: Patient Trajectory Modeling via Chain-of-Agents for Lung Cancer Risk Prediction",
      "title_zh": "Traj-CoAï¼šåŸºäºæ™ºèƒ½ä½“é“¾çš„æ‚£è€…è½¨è¿¹å»ºæ¨¡ä¸è‚ºç™Œé£é™©é¢„æµ‹",
      "authors": [
        "Sihang Zeng",
        "Yujuan Fu",
        "Sitong Zhou",
        "Zixuan Yu",
        "Lucas Jing Liu",
        "Jun Wen",
        "Matthew Thompson",
        "Ruth Etzioni",
        "Meliha Yetisgen"
      ],
      "abstract": "Large language models (LLMs) offer a generalizable approach for modeling patient trajectories, but suffer from the long and noisy nature of electronic health records (EHR) data in temporal reasoning. To address these challenges, we introduce Traj-CoA, a multi-agent system involving chain-of-agents for patient trajectory modeling. Traj-CoA employs a chain of worker agents to process EHR data in manageable chunks sequentially, distilling critical events into a shared long-term memory module, EHRMem, to reduce noise and preserve a comprehensive timeline. A final manager agent synthesizes the worker agents' summary and the extracted timeline in EHRMem to make predictions. In a zero-shot one-year lung cancer risk prediction task based on five-year EHR data, Traj-CoA outperforms baselines of four categories. Analysis reveals that Traj-CoA exhibits clinically aligned temporal reasoning, establishing it as a promisingly robust and generalizable approach for modeling complex patient trajectories.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† Traj-CoAï¼Œä¸€ç§åŸºäºå¤šæ™ºèƒ½ä½“åä½œé“¾ (Chain-of-Agents) çš„æ‚£è€…è½¨è¿¹å»ºæ¨¡ç³»ç»Ÿï¼Œæ—¨åœ¨è§£å†³å¤§è¯­è¨€æ¨¡å‹ (LLMs) åœ¨å¤„ç†ç”µå­å¥åº·è®°å½• (EHR) æ•°æ®æ—¶é¢ä¸´çš„é•¿åºåˆ—å’Œé«˜å™ªå£°æŒ‘æˆ˜ã€‚Traj-CoA é‡‡ç”¨ä¸€ç³»åˆ—å·¥ä½œæ™ºèƒ½ä½“ (worker agents) å°† EHR æ•°æ®æŒ‰å—é¡ºåºå¤„ç†ï¼Œå¹¶å°†å…³é”®äº‹ä»¶æç‚¼è‡³å…±äº«é•¿æœŸè®°å¿†æ¨¡å— EHRMem ä¸­ï¼Œä»¥æœ‰æ•ˆè¿‡æ»¤å™ªå£°å¹¶ä¿ç•™å®Œæ•´çš„ä¸´åºŠæ—¶é—´è½´ã€‚æœ€ç»ˆç”±ç®¡ç†æ™ºèƒ½ä½“ (manager agent) ç»¼åˆå·¥ä½œæ™ºèƒ½ä½“çš„æ‘˜è¦åŠ EHRMem ä¸­çš„æå–ä¿¡æ¯è¿›è¡Œé¢„æµ‹ã€‚åœ¨åŸºäºäº”å¹´ EHR æ•°æ®è¿›è¡Œçš„ä¸€å¹´æœŸè‚ºç™Œé£é™©é›¶æ ·æœ¬ (zero-shot) é¢„æµ‹ä»»åŠ¡ä¸­ï¼ŒTraj-CoA çš„è¡¨ç°ä¼˜äºå››ç±»åŸºå‡†æ¨¡å‹ã€‚åˆ†æè¡¨æ˜ Traj-CoA å±•ç°äº†ä¸ä¸´åºŠå®è·µä¸€è‡´çš„æ—¶é—´æ¨ç†èƒ½åŠ›ï¼Œæ˜¯å»ºæ¨¡å¤æ‚æ‚£è€…è½¨è¿¹çš„ä¸€ç§ç¨³å¥ä¸”å…·æœ‰æ³›åŒ–æ€§çš„æ–¹æ³•ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "Accepted by NeurIPS 2025 GenAI4Health Workshop",
      "pdf_url": "https://arxiv.org/pdf/2510.10454v1",
      "published_date": "2025-10-12 05:24:32 UTC",
      "updated_date": "2025-10-12 05:24:32 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T02:56:45.971064+00:00"
    },
    {
      "arxiv_id": "2510.10451v1",
      "title": "Data-driven simulator of multi-animal behavior with unknown dynamics via offline and online reinforcement learning",
      "title_zh": "åŸºäºç¦»çº¿ä¸åœ¨çº¿å¼ºåŒ–å­¦ä¹ çš„åŠ¨åŠ›å­¦æœªçŸ¥æ•°æ®é©±åŠ¨å¤šåŠ¨ç‰©è¡Œä¸ºæ¨¡æ‹Ÿå™¨",
      "authors": [
        "Keisuke Fujii",
        "Kazushi Tsutsui",
        "Yu Teshima",
        "Makoto Itoh",
        "Naoya Takeishi",
        "Nozomi Nishiumi",
        "Ryoya Tanaka",
        "Shunsuke Shigaki",
        "Yoshinobu Kawahara"
      ],
      "abstract": "Simulators of animal movements play a valuable role in studying behavior. Advances in imitation learning for robotics have expanded possibilities for reproducing human and animal movements. A key challenge for realistic multi-animal simulation in biology is bridging the gap between unknown real-world transition models and their simulated counterparts. Because locomotion dynamics are seldom known, relying solely on mathematical models is insufficient; constructing a simulator that both reproduces real trajectories and supports reward-driven optimization remains an open problem. We introduce a data-driven simulator for multi-animal behavior based on deep reinforcement learning and counterfactual simulation. We address the ill-posed nature of the problem caused by high degrees of freedom in locomotion by estimating movement variables of an incomplete transition model as actions within an RL framework. We also employ a distance-based pseudo-reward to align and compare states between cyber and physical spaces. Validated on artificial agents, flies, newts, and silkmoth, our approach achieves higher reproducibility of species-specific behaviors and improved reward acquisition compared with standard imitation and RL methods. Moreover, it enables counterfactual behavior prediction in novel experimental settings and supports multi-individual modeling for flexible what-if trajectory generation, suggesting its potential to simulate and elucidate complex multi-animal behaviors.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ç”Ÿç‰©å­¦ä¸­å¤šåŠ¨ç‰©æ¨¡æ‹Ÿï¼ˆmulti-animal simulationï¼‰é¢ä¸´çš„çœŸå®ä¸–ç•Œè½¬ç§»æ¨¡å‹ï¼ˆtransition modelsï¼‰æœªçŸ¥ä»¥åŠè¿åŠ¨åŠ¨åŠ›å­¦ï¼ˆlocomotion dynamicsï¼‰éš¾ä»¥å»ºæ¨¡ç­‰æŒ‘æˆ˜ï¼Œæå‡ºäº†ä¸€ç§åŸºäºæ·±åº¦å¼ºåŒ–å­¦ä¹ ï¼ˆDeep Reinforcement Learningï¼‰å’Œåäº‹å®æ¨¡æ‹Ÿï¼ˆcounterfactual simulationï¼‰çš„æ•°æ®é©±åŠ¨æ¨¡æ‹Ÿå™¨ã€‚ä¸ºè§£å†³è¿åŠ¨è‡ªç”±åº¦è¿‡é«˜å¸¦æ¥çš„ç—…æ€é—®é¢˜ï¼Œè¯¥æ¡†æ¶å°†ä¸å®Œæ•´è½¬ç§»æ¨¡å‹ä¸­çš„è¿åŠ¨å˜é‡ä¼°è®¡ä¸º RL æ¡†æ¶å†…çš„åŠ¨ä½œï¼ˆactionsï¼‰ï¼Œå¹¶é‡‡ç”¨åŸºäºè·ç¦»çš„ä¼ªå¥–åŠ±ï¼ˆpseudo-rewardï¼‰æ¥å¯¹é½ç½‘ç»œç©ºé—´ä¸ç‰©ç†ç©ºé—´çš„çŠ¶æ€ã€‚å®éªŒåœ¨äººå·¥æ™ºèƒ½ä½“ã€æœè‡ï¼ˆfliesï¼‰ã€è¾èˆï¼ˆnewtsï¼‰å’Œèš•è›¾ï¼ˆsilkmothï¼‰ä¸Šè¿›è¡Œäº†éªŒè¯ï¼Œç»“æœè¡¨æ˜è¯¥æ–¹æ³•åœ¨é‡ç°ç‰©ç§ç‰¹å¼‚æ€§è¡Œä¸ºï¼ˆspecies-specific behaviorsï¼‰å’Œå¥–åŠ±è·å–ï¼ˆreward acquisitionï¼‰æ–¹é¢ä¼˜äºæ ‡å‡†çš„æ¨¡ä»¿å­¦ä¹ å’Œ RL æ–¹æ³•ã€‚æ­¤å¤–ï¼Œè¯¥ç³»ç»Ÿæ”¯æŒåœ¨æ–°å‹å®éªŒç¯å¢ƒä¸‹è¿›è¡Œåäº‹å®è¡Œä¸ºé¢„æµ‹ï¼ˆcounterfactual behavior predictionï¼‰ï¼Œå¹¶é€šè¿‡å¤šä¸ªä½“å»ºæ¨¡å®ç°çµæ´»çš„å‡è®¾è½¨è¿¹ç”Ÿæˆï¼ˆwhat-if trajectory generationï¼‰ã€‚è¯¥é¡¹ç ”ç©¶ä¸ºæ¨¡æ‹Ÿå’Œé˜æ˜å¤æ‚çš„å¤šåŠ¨ç‰©è¡Œä¸ºæä¾›äº†å¼ºæœ‰åŠ›çš„å·¥å…·ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "21 pages, 7 figures",
      "pdf_url": "https://arxiv.org/pdf/2510.10451v1",
      "published_date": "2025-10-12 05:08:26 UTC",
      "updated_date": "2025-10-12 05:08:26 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T02:56:49.076135+00:00"
    },
    {
      "arxiv_id": "2510.10446v2",
      "title": "Reverse Supervision at Scale: Exponential Search Meets the Economics of Annotation",
      "title_zh": "å¤§è§„æ¨¡é€†å‘ç›‘ç£ï¼šæŒ‡æ•°çº§æœç´¢ä¸æ ‡æ³¨ç»æµå­¦",
      "authors": [
        "Masoud Makrehchi"
      ],
      "abstract": "We analyze a reversed-supervision strategy that searches over labelings of a large unlabeled set \\(B\\) to minimize error on a small labeled set \\(A\\). The search space is \\(2^n\\), and the resulting complexity remains exponential even under large constant-factor speedups (e.g., quantum or massively parallel hardware). Consequently, arbitrarily fast -- but not exponentially faster -- computation does not obviate the need for informative labels or priors. In practice, the machine learning pipeline still requires an initial human contribution: specifying the objective, defining classes, and providing a seed set of representative annotations that inject inductive bias and align models with task semantics. Synthetic labels from generative AI can partially substitute provided their quality is human-grade and anchored by a human-specified objective, seed supervision, and validation. In this view, generative models function as \\emph{label amplifiers}, leveraging small human-curated cores via active, semi-supervised, and self-training loops, while humans retain oversight for calibration, drift detection, and failure auditing. Thus, extreme computational speed reduces wall-clock time but not the fundamental supervision needs of learning; initial human (or human-grade) input remains necessary to ground the system in the intended task.",
      "tldr_zh": "è¯¥ç ”ç©¶åˆ†æäº†ä¸€ç§åä¸ºåå‘ç›‘ç£ (Reverse Supervision) çš„ç­–ç•¥ï¼Œå³é€šè¿‡åœ¨æµ·é‡æœªæ ‡è®°æ•°æ®é›†ä¸Šæœç´¢æ ‡æ³¨ç»„åˆï¼Œä»¥æœ€å°åŒ–åœ¨å°‘é‡å·²æ ‡è®°æ•°æ®é›†ä¸Šçš„è¯¯å·®ã€‚ç ”ç©¶è¡¨æ˜ï¼Œè¯¥æœç´¢ç©ºé—´å…·æœ‰æŒ‡æ•°çº§å¤æ‚åº¦ (\\(2^n\\))ï¼Œè¿™æ„å‘³ç€å³ä½¿åˆ©ç”¨é‡å­è®¡ç®—æˆ–å¤§è§„æ¨¡å¹¶è¡Œç¡¬ä»¶ï¼Œä¹Ÿæ— æ³•æ¶ˆé™¤å¯¹é«˜è´¨é‡äººç±»æ ‡ç­¾æˆ–å…ˆéªŒçŸ¥è¯†çš„ä¾èµ–ã€‚åœ¨å®é™…åº”ç”¨ä¸­ï¼Œæœºå™¨å­¦ä¹ æµç¨‹ä»éœ€äººç±»å®šä¹‰ç›®æ ‡ã€ç±»åˆ«å¹¶æä¾›å…·æœ‰å½’çº³åç½® (Inductive Bias) çš„ç§å­æ ‡æ³¨ï¼Œä»¥ç¡®ä¿æ¨¡å‹ä¸ä»»åŠ¡è¯­ä¹‰å¯¹é½ã€‚ç”Ÿæˆå¼äººå·¥æ™ºèƒ½ (Generative AI) åœ¨æ­¤è¿‡ç¨‹ä¸­å¯å……å½“æ ‡ç­¾æ”¾å¤§å™¨ (Label Amplifiers)ï¼Œé€šè¿‡ä¸»åŠ¨å­¦ä¹ å’ŒåŠç›‘ç£å¾ªç¯æ‰©å±•äººç±»æ ¸å¿ƒæ ‡æ³¨ï¼Œä½†äººç±»ä»éœ€è´Ÿè´£æ ¡å‡†ã€åç§»æ£€æµ‹å’Œæ•…éšœå®¡è®¡ã€‚æœ€ç»ˆç»“è®ºè®¤ä¸ºï¼Œè®¡ç®—é€Ÿåº¦çš„æå‡å¹¶ä¸èƒ½æ”¹å˜å­¦ä¹ å¯¹åŸºç¡€ç›‘ç£çš„éœ€æ±‚ï¼Œåˆå§‹çš„äººç±»è¾“å…¥å¯¹äºå°†ç³»ç»Ÿé”šå®šåœ¨é¢„å®šä»»åŠ¡ä¸­ä¾ç„¶ä¸å¯æˆ–ç¼ºã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "10 pages",
      "pdf_url": "https://arxiv.org/pdf/2510.10446v2",
      "published_date": "2025-10-12 04:45:50 UTC",
      "updated_date": "2025-12-18 03:59:51 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T02:56:50.887079+00:00"
    },
    {
      "arxiv_id": "2510.10444v2",
      "title": "Do Audio LLMs Really LISTEN, or Just Transcribe? Measuring Lexical vs. Acoustic Emotion Cues Reliance",
      "title_zh": "éŸ³é¢‘å¤§è¯­è¨€æ¨¡å‹æ˜¯çœŸçš„åœ¨â€œå¬â€è¿˜æ˜¯ä»…åœ¨â€œè½¬å½•â€ï¼Ÿâ€”â€”è¯æ±‡ä¸å£°å­¦æƒ…æ„Ÿçº¿ç´¢ä¾èµ–æ€§çš„åº¦é‡",
      "authors": [
        "Jingyi Chen",
        "Zhimeng Guo",
        "Jiyun Chun",
        "Pichao Wang",
        "Andrew Perrault",
        "Micha Elsner"
      ],
      "abstract": "Understanding emotion from speech requires sensitivity to both lexical and acoustic cues. However, it remains unclear whether large audio language models (LALMs) genuinely process acoustic information or rely primarily on lexical content. We present LISTEN (Lexical vs. Acoustic Speech Test for Emotion in Narratives), a controlled benchmark designed to disentangle lexical reliance from acoustic sensitivity in emotion understanding. Across evaluations of six state-of-the-art LALMs, we observe a consistent lexical dominance. Models predict \"neutral\" when lexical cues are neutral or absent, show limited gains under cue alignment, and fail to classify distinct emotions under cue conflict. In paralinguistic settings, performance approaches chance. These results indicate that current LALMs largely \"transcribe\" rather than \"listen,\" relying heavily on lexical semantics while underutilizing acoustic cues. LISTEN offers a principled framework for assessing emotion understanding in multimodal models.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†å¤§è¯­è¨€éŸ³é¢‘æ¨¡å‹(LALMs)åœ¨æƒ…æ„Ÿç†è§£ä¸­æ˜¯çœŸæ­£å¤„ç†å£°å­¦ä¿¡æ¯è¿˜æ˜¯ä¸»è¦ä¾èµ–æ–‡æœ¬è¯æ±‡å†…å®¹ã€‚ä¸ºæ­¤ç ”ç©¶è€…æå‡ºäº†LISTEN (Lexical vs. Acoustic Speech Test for Emotion in Narratives)è¿™ä¸€å—æ§åŸºå‡†ï¼Œæ—¨åœ¨å°†æƒ…æ„Ÿç†è§£ä¸­çš„è¯æ±‡ä¾èµ–ä¸å£°å­¦æ•æ„Ÿåº¦è¿›è¡Œåˆ†ç¦»æµ‹è¯•ã€‚é€šè¿‡å¯¹å…­ç§æœ€å…ˆè¿›çš„LALMsè¿›è¡Œè¯„ä¼°ï¼Œç ”ç©¶å‘ç°è¿™äº›æ¨¡å‹æ™®éå­˜åœ¨æ˜¾è‘—çš„è¯æ±‡ä¸»å¯¼(lexical dominance)ç°è±¡ï¼Œåœ¨è¯æ±‡çº¿ç´¢ç¼ºå¤±æ—¶è¡¨ç°æ¥è¿‘éšæœºæ°´å¹³ï¼Œä¸”åœ¨è¯æ±‡ä¸å£°å­¦çº¿ç´¢å†²çªæ—¶æ— æ³•å‡†ç¡®åˆ†ç±»æƒ…ç»ªã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œå½“å‰çš„LALMsåœ¨å¾ˆå¤§ç¨‹åº¦ä¸Šæ›´å€¾å‘äºâ€œè½¬å½•â€è€ŒéçœŸæ­£çš„â€œå€¾å¬â€ï¼Œåæ˜ å‡ºå…¶è¿‡åº¦ä¾èµ–è¯æ±‡è¯­ä¹‰è€Œæœªèƒ½å……åˆ†åˆ©ç”¨å£°å­¦çº¿ç´¢ã€‚LISTENåŸºå‡†ä¸ºè¡¡é‡å¤šæ¨¡æ€æ¨¡å‹çš„æƒ…æ„Ÿç†è§£èƒ½åŠ›æä¾›äº†ä¸€ä¸ªåŸåˆ™æ€§çš„è¯„ä¼°æ¡†æ¶ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.10444v2",
      "published_date": "2025-10-12 04:31:15 UTC",
      "updated_date": "2025-10-17 03:54:23 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T02:56:55.702438+00:00"
    },
    {
      "arxiv_id": "2510.13855v2",
      "title": "Harnessing Consistency for Robust Test-Time LLM Ensemble",
      "title_zh": "åˆ©ç”¨ä¸€è‡´æ€§æ„å»ºé²æ£’çš„æµ‹è¯•æ—¶å¤§è¯­è¨€æ¨¡å‹é›†æˆ",
      "authors": [
        "Zhichen Zeng",
        "Qi Yu",
        "Xiao Lin",
        "Ruizhong Qiu",
        "Xuying Ning",
        "Tianxin Wei",
        "Yuchen Yan",
        "Jingrui He",
        "Hanghang Tong"
      ],
      "abstract": "Different large language models (LLMs) exhibit diverse strengths and weaknesses, and LLM ensemble serves as a promising approach to integrate their complementary capabilities. Despite substantial progress in improving ensemble quality, limited attention has been paid to the robustness of ensembles against potential erroneous signals, which often arise from heterogeneous tokenization schemes and varying model expertise. Our analysis shows that ensemble failures typically arise from both the token level and the model level: the former reflects severe disagreement in token predictions, while the latter involves low confidence and pronounced disparities among models. In light of this, we propose CoRE, a plug-and-play technique that harnesses model consistency for robust LLM ensemble, which can be seamlessly integrated with diverse ensemble methods. *Token-level consistency* captures fine-grained disagreements by applying a low-pass filter to downweight uncertain tokens with high inconsistency, often due to token misalignment, thereby improving robustness at a granular level. *Model-level consistency* models global agreement by promoting model outputs with high self-confidence and minimal divergence from others, enhancing robustness at a coarser level. Extensive experiments across diverse benchmarks, model combinations, and ensemble strategies demonstrate that CoRE consistently improves ensemble performance and robustness. Our code is available at https://github.com/zhichenz98/CoRE-EACL26.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†å¤§å‹è¯­è¨€æ¨¡å‹(LLMs)é›†æˆ(Ensemble)ä¸­å› ä»¤ç‰ŒåŒ–æ–¹æ¡ˆå¼‚æ„å’Œæ¨¡å‹ä¸“ä¸šçŸ¥è¯†å·®å¼‚å¯¼è‡´çš„é²æ£’æ€§ä¸è¶³é—®é¢˜ã€‚é€šè¿‡æ·±å…¥åˆ†æï¼Œä½œè€…å‘ç°é›†æˆå¤±è´¥ä¸»è¦æºäºä»¤ç‰Œçº§åˆ«(Token-level)çš„é¢„æµ‹ä¸¥é‡åˆ†æ­§ä»¥åŠæ¨¡å‹çº§åˆ«(Model-level)çš„ä½ç½®ä¿¡åº¦ä¸æ˜¾è‘—å·®å¼‚ã€‚ä¸ºæ­¤ï¼Œç ”ç©¶æå‡ºäº†ä¸€ç§åä¸ºCoREçš„å³æ’å³ç”¨æŠ€æœ¯ï¼Œæ—¨åœ¨é€šè¿‡åˆ©ç”¨æ¨¡å‹ä¸€è‡´æ€§(Consistency)æ¥å¢å¼ºé›†æˆçš„ç¨³å¥æ€§ã€‚åœ¨ä»¤ç‰Œçº§åˆ«ï¼ŒCoREåº”ç”¨ä½é€šæ»¤æ³¢å™¨(Low-pass filter)å¯¹ä¸ç¡®å®šä¸”é«˜åº¦ä¸ä¸€è‡´çš„ä»¤ç‰Œè¿›è¡Œé™æƒå¤„ç†ï¼Œæœ‰æ•ˆè§£å†³äº†ä»¤ç‰Œé”™ä½(Token misalignment)å¸¦æ¥çš„å™ªå£°ã€‚åœ¨æ¨¡å‹çº§åˆ«ï¼Œè¯¥æ–¹æ³•é€šè¿‡ä¿ƒè¿›é«˜è‡ªç½®ä¿¡åº¦ä¸”ä¸å…¶ä»–æ¨¡å‹åˆ†æ­§æœ€å°çš„è¾“å‡ºï¼Œå¢å¼ºäº†å®è§‚å±‚é¢çš„ä¸€è‡´æ€§ã€‚å®éªŒè¡¨æ˜ï¼ŒCoREåœ¨å¤šç§åŸºå‡†æµ‹è¯•å’Œæ¨¡å‹ç»„åˆä¸­æ˜¾è‘—æå‡äº†é›†æˆæ€§èƒ½ä¸é²æ£’æ€§ï¼Œä¸”èƒ½ä¸ç°æœ‰å¤šç§é›†æˆç­–ç•¥æ— ç¼ç»“åˆï¼Œä¸ºå®ç°æ›´å¯é çš„æµ‹è¯•æ—¶(Test-time)æ¨¡å‹ååŒæä¾›äº†æœ‰æ•ˆæ–¹æ¡ˆã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "18 pages, 15 figures",
      "pdf_url": "https://arxiv.org/pdf/2510.13855v2",
      "published_date": "2025-10-12 04:18:45 UTC",
      "updated_date": "2026-01-19 02:48:12 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T02:57:07.093933+00:00"
    },
    {
      "arxiv_id": "2510.15964v1",
      "title": "Long Exposure: Accelerating Parameter-Efficient Fine-Tuning for LLMs under Shadowy Sparsity",
      "title_zh": "Long Exposureï¼šé’ˆå¯¹éšå½±ç¨€ç–çš„å¤§è¯­è¨€æ¨¡å‹å‚æ•°é«˜æ•ˆå¾®è°ƒåŠ é€Ÿ",
      "authors": [
        "Tuowei Wang",
        "Kun Li",
        "Zixu Hao",
        "Donglin Bai",
        "Ju Ren",
        "Yaoxue Zhang",
        "Ting Cao",
        "Mao Yang"
      ],
      "abstract": "The adaptation of pre-trained large language models (LLMs) to diverse downstream tasks via fine-tuning is critical for numerous applications. However, the inefficiency of parameter-efficient fine-tuning (PEFT) techniques presents significant challenges in terms of time investments and operational costs. In this paper, we first introduce a nuanced form of sparsity, termed Shadowy Sparsity, which is distinctive in fine-tuning and has not been adequately addressed for acceleration. Under Shadowy Sparsity, we propose Long Exposure, an efficient system to accelerate PEFT for LLMs. Long Exposure comprises three key components: Shadowy-sparsity Exposer employs a prolonged sensing range to capture more sparsity details under shadowy sparsity; Sequence-oriented Predictor provides efficient yet accurate predictions to handle large sequence inputs and constantly-evolving parameters; and Dynamic-aware Operator facilitates more structured computational patterns and coalesced memory accesses, addressing dynamic sparse operations. Extensive evaluations show that Long Exposure outperforms state-of-the-arts with up to a $2.49\\times$ speedup in end-to-end fine-tuning, offering promising advancements in accelerating PEFT for LLMs.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤§è¯­è¨€æ¨¡å‹(LLMs)åœ¨å‚æ•°é«˜æ•ˆå¾®è°ƒ(PEFT)è¿‡ç¨‹ä¸­é¢ä¸´çš„æ•ˆç‡ä¸æˆæœ¬æŒ‘æˆ˜ï¼Œé¦–æ¬¡æå‡ºäº†ä¸€ç§åä¸º Shadowy Sparsity çš„ç»†å¾®ç¨€ç–æ€§å½¢å¼ã€‚ä¸ºäº†å……åˆ†åˆ©ç”¨è¿™ä¸€ç‰¹æ€§ï¼Œç ”ç©¶è€…å¼€å‘äº† Long Exposure åŠ é€Ÿç³»ç»Ÿï¼Œè¯¥ç³»ç»Ÿç”± Shadowy-sparsity Exposerã€Sequence-oriented Predictor å’Œ Dynamic-aware Operator ä¸‰ä¸ªæ ¸å¿ƒç»„ä»¶æ„æˆï¼Œæ—¨åœ¨ç²¾ç¡®æ•è·ç¨€ç–ç»†èŠ‚ã€å¤„ç†é•¿åºåˆ—è¾“å…¥çš„é«˜æ•ˆé¢„æµ‹å¹¶ä¼˜åŒ–åŠ¨æ€ç¨€ç–è®¡ç®—çš„å†…å­˜è®¿é—®ã€‚å®éªŒè¯„ä¼°æ˜¾ç¤ºï¼ŒLong Exposure åœ¨ç«¯åˆ°ç«¯å¾®è°ƒä¸­ç›¸æ¯”ç°æœ‰æœ€å…ˆè¿›æŠ€æœ¯å®ç°äº†é«˜è¾¾ 2.49 å€çš„åŠ é€Ÿã€‚è¯¥ç ”ç©¶é€šè¿‡è§£å†³å¾®è°ƒè¿‡ç¨‹ä¸­çš„ç¨€ç–æ€§åŠ é€Ÿé—®é¢˜ï¼Œä¸ºå¤§è¯­è¨€æ¨¡å‹çš„é«˜æ•ˆé€‚é…æä¾›äº†é‡è¦çš„æŠ€æœ¯æ”¯æ’‘ï¼Œæ˜¾è‘—é™ä½äº†å¾®è°ƒçš„æ—¶é—´æŠ•å…¥ä¸è¿è¥æˆæœ¬ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.15964v1",
      "published_date": "2025-10-12 04:14:53 UTC",
      "updated_date": "2025-10-12 04:14:53 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T02:57:10.795463+00:00"
    },
    {
      "arxiv_id": "2510.10433v1",
      "title": "Multi-Task Learning with Feature-Similarity Laplacian Graphs for Predicting Alzheimer's Disease Progression",
      "title_zh": "åŸºäºç‰¹å¾ç›¸ä¼¼æ€§æ‹‰æ™®æ‹‰æ–¯å›¾çš„é˜¿å°”èŒ¨æµ·é»˜ç—…è¿›å±•é¢„æµ‹å¤šä»»åŠ¡å­¦ä¹ ",
      "authors": [
        "Zixiang Xu",
        "Menghui Zhou",
        "Jun Qi",
        "Xuanhan Fan",
        "Yun Yang",
        "Po Yang"
      ],
      "abstract": "Alzheimer's Disease (AD) is the most prevalent neurodegenerative disorder in aging populations, posing a significant and escalating burden on global healthcare systems. While Multi-Tusk Learning (MTL) has emerged as a powerful computational paradigm for modeling longitudinal AD data, existing frameworks do not account for the time-varying nature of feature correlations. To address this limitation, we propose a novel MTL framework, named Feature Similarity Laplacian graph Multi-Task Learning (MTL-FSL). Our framework introduces a novel Feature Similarity Laplacian (FSL) penalty that explicitly models the time-varying relationships between features. By simultaneously considering temporal smoothness among tasks and the dynamic correlations among features, our model enhances both predictive accuracy and biological interpretability. To solve the non-smooth optimization problem arising from our proposed penalty terms, we adopt the Alternating Direction Method of Multipliers (ADMM) algorithm. Experiments conducted on the Alzheimer's Disease Neuroimaging Initiative (ADNI) dataset demonstrate that our proposed MTL-FSL framework achieves state-of-the-art performance, outperforming various baseline methods. The implementation source can be found at https://github.com/huatxxx/MTL-FSL.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹é˜¿å°”èŒ¨æµ·é»˜ç—…(AD)è¿›å±•é¢„æµ‹ï¼Œæå‡ºäº†ä¸€ç§åä¸ºMTL-FSLçš„æ–°å‹Multi-Task Learning(MTL)æ¡†æ¶ã€‚ä¸ºè§£å†³ç°æœ‰æ¡†æ¶æ— æ³•åˆ»ç”»ç‰¹å¾ç›¸å…³æ€§éšæ—¶é—´å˜åŒ–çš„é—®é¢˜ï¼Œè¯¥æ¨¡å‹å¼•å…¥äº†Feature Similarity Laplacian(FSL)æƒ©ç½šé¡¹ï¼Œç”¨äºæ˜¾å¼å»ºæ¨¡ç‰¹å¾é—´çš„åŠ¨æ€å…³ç³»ã€‚é€šè¿‡ååŒè€ƒè™‘ä»»åŠ¡é—´çš„æ—¶é—´å¹³æ»‘æ€§(temporal smoothness)ä¸ç‰¹å¾é—´çš„åŠ¨æ€å…³è”ï¼Œè¯¥æ–¹æ³•åœ¨æé«˜é¢„æµ‹å‡†ç¡®æ€§çš„åŒæ—¶å¢å¼ºäº†ç”Ÿç‰©å­¦å¯è§£é‡Šæ€§ã€‚ä¸ºäº†å¤„ç†ç”±æ­¤äº§ç”Ÿçš„éå¹³æ»‘ä¼˜åŒ–é—®é¢˜ï¼Œç ”ç©¶é‡‡ç”¨äº†Alternating Direction Method of Multipliers(ADMM)ç®—æ³•è¿›è¡Œæ±‚è§£ã€‚åœ¨Alzheimer's Disease Neuroimaging Initiative(ADNI)æ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼ŒMTL-FSLæ¡†æ¶çš„æ€§èƒ½ä¼˜äºå¤šç§åŸºçº¿æ–¹æ³•ï¼Œè¾¾åˆ°äº†state-of-the-artæ°´å¹³ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.10433v1",
      "published_date": "2025-10-12 03:55:42 UTC",
      "updated_date": "2025-10-12 03:55:42 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T02:57:21.408325+00:00"
    },
    {
      "arxiv_id": "2510.10432v1",
      "title": "Hierarchical LoRA MoE for Efficient CTR Model Scaling",
      "title_zh": "é¢å‘é«˜æ•ˆ CTR æ¨¡å‹æ‰©å±•çš„å±‚çº§ LoRA MoE",
      "authors": [
        "Zhichen Zeng",
        "Mengyue Hang",
        "Xiaolong Liu",
        "Xiaoyi Liu",
        "Xiao Lin",
        "Ruizhong Qiu",
        "Tianxin Wei",
        "Zhining Liu",
        "Siyang Yuan",
        "Chaofei Yang",
        "Yiqun Liu",
        "Hang Yin",
        "Jiyan Yang",
        "Hanghang Tong"
      ],
      "abstract": "Deep models have driven significant advances in click-through rate (CTR) prediction. While vertical scaling via layer stacking improves model expressiveness, the layer-by-layer sequential computation poses challenges to efficient scaling. Conversely, horizontal scaling through Mixture of Experts (MoE) achieves efficient scaling by activating a small subset of experts in parallel, but flat MoE layers may struggle to capture the hierarchical structure inherent in recommendation tasks. To push the Return-On-Investment (ROI) boundary, we explore the complementary strengths of both directions and propose HiLoMoE, a hierarchical LoRA MoE framework that enables holistic scaling in a parameter-efficient manner. Specifically, HiLoMoE employs lightweight rank-1 experts for parameter-efficient horizontal scaling, and stacks multiple MoE layers with hierarchical routing to enable combinatorially diverse expert compositions. Unlike conventional stacking, HiLoMoE routes based on prior layer scores rather than outputs, allowing all layers to execute in parallel. A principled three-stage training framework ensures stable optimization and expert diversity. Experiments on four public datasets show that HiLoMoE achieving better performance-efficiency tradeoff, achieving an average AUC improvement of 0.20\\% in AUC and 18.5\\% reduction in FLOPs compared to the non-MoE baseline.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†HiLoMoEï¼Œä¸€ä¸ªåˆ†å±‚LoRA MoEæ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³ç‚¹å‡»ç‡é¢„æµ‹(CTR)æ¨¡å‹ä¸­çºµå‘å †å å¯¼è‡´çš„è®¡ç®—æ•ˆç‡ä½ä¸‹ä»¥åŠæ‰å¹³MoEç»“æ„éš¾ä»¥æ•æ‰å±‚çº§æ¨èç‰¹å¾çš„é—®é¢˜ã€‚HiLoMoEé€šè¿‡å¼•å…¥è½»é‡çº§çš„rank-1 expertså®ç°å‚æ•°é«˜æ•ˆçš„æ¨ªå‘æ‰©å±•ï¼Œå¹¶é‡‡ç”¨åˆ†å±‚è·¯ç”±æœºåˆ¶(hierarchical routing)ç»„åˆå¤šä¸ªMoEå±‚ã€‚ä¸ä¼ ç»Ÿå †å æ–¹å¼ä¸åŒï¼ŒHiLoMoEåŸºäºå‰ä¸€å±‚çš„è¯„åˆ†è€Œéè¾“å‡ºè¿›è¡Œè·¯ç”±ï¼Œä»è€Œå®ç°äº†æ‰€æœ‰å±‚çš„å¹¶è¡Œæ‰§è¡Œã€‚ç ”ç©¶è¿˜é…å¥—è®¾è®¡äº†ä¸€ä¸ªä¸‰é˜¶æ®µè®­ç»ƒæ¡†æ¶ä»¥ç¡®ä¿ä¼˜åŒ–è¿‡ç¨‹çš„ç¨³å®šæ€§ä¸ä¸“å®¶å¤šæ ·æ€§ã€‚åœ¨å››ä¸ªå…¬å¼€æ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼ŒHiLoMoEå®ç°äº†æ›´ä¼˜çš„æ€§èƒ½ä¸æ•ˆç‡å¹³è¡¡ï¼Œç›¸è¾ƒäºéMoEåŸºå‡†æ¨¡å‹ï¼Œåœ¨AUCå¹³å‡æå‡0.20%çš„åŒæ—¶ï¼ŒFLOPsé™ä½äº†18.5%ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.IR"
      ],
      "primary_category": "cs.LG",
      "comment": "13 pages, 9 figures",
      "pdf_url": "https://arxiv.org/pdf/2510.10432v1",
      "published_date": "2025-10-12 03:54:11 UTC",
      "updated_date": "2025-10-12 03:54:11 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T02:57:22.341184+00:00"
    },
    {
      "arxiv_id": "2510.10426v1",
      "title": "Taming a Retrieval Framework to Read Images in Humanlike Manner for Augmenting Generation of MLLMs",
      "title_zh": "é¢å‘å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ç”Ÿæˆå¢å¼ºï¼šé€šè¿‡ç±»äººåŒ–å›¾åƒé˜…è¯»æ–¹å¼ä¼˜åŒ–æ£€ç´¢æ¡†æ¶",
      "authors": [
        "Suyang Xi",
        "Chenxi Yang",
        "Hong Ding",
        "Yiqing Ni",
        "Catherine C. Liu",
        "Yunhao Liu",
        "Chengqi Zhang"
      ],
      "abstract": "Multimodal large language models (MLLMs) often fail in fine-grained visual question answering, producing hallucinations about object identities, positions, and relations because textual queries are not explicitly anchored to visual referents. Retrieval-augmented generation (RAG) alleviates some errors, but it fails to align with human-like processing at both the retrieval and augmentation levels. Specifically, it focuses only on global-level image information but lacks local detail and limits reasoning about fine-grained interactions. To overcome this limitation, we present Human-Like Retrieval-Augmented Generation (HuLiRAG), a framework that stages multimodal reasoning as a ``what--where--reweight'' cascade. Queries are first anchored to candidate referents via open-vocabulary detection (what), then spatially resolved with SAM-derived masks to recover fine-grained precision (where), and adaptively prioritized through the trade-off between local and global alignment (reweight). Mask-guided fine-tuning further injects spatial evidence into the generation process, transforming grounding from a passive bias into an explicit constraint on answer formulation. Extensive experiments demonstrate that this human-like cascade improves grounding fidelity and factual consistency while reducing hallucinations, advancing multimodal question answering toward trustworthy reasoning.",
      "tldr_zh": "MLLMsåœ¨ç»†ç²’åº¦è§†è§‰é—®ç­”ä¸­å¸¸å› æ–‡æœ¬æŸ¥è¯¢ä¸è§†è§‰æŒ‡ä»£ç¼ºä¹æ˜¾å¼é”šå®šè€Œäº§ç”Ÿå¹»è§‰ï¼Œä¼ ç»Ÿçš„RAGæŠ€æœ¯ä¹Ÿéš¾ä»¥å®ç°äººç±»æ°´å¹³çš„å±€éƒ¨ç»†èŠ‚å¤„ç†ã€‚è¯¥ç ”ç©¶æå‡ºäº†HuLiRAGï¼Œä¸€ç§æ¨¡ä»¿äººç±»å¤„ç†æ–¹å¼çš„æ£€ç´¢å¢å¼ºç”Ÿæˆæ¡†æ¶ï¼Œé€šè¿‡â€œwhat--where--reweightâ€çº§è”æœºåˆ¶åˆ†é˜¶æ®µè¿›è¡Œå¤šæ¨¡æ€æ¨ç†ã€‚è¯¥æœºåˆ¶é¦–å…ˆåˆ©ç”¨open-vocabulary detectionè¿›è¡Œç›®æ ‡é”šå®šï¼Œéšåé€šè¿‡SAMè¡ç”Ÿçš„æ©ç å®ç°ç©ºé—´ç²¾åº¦çš„æ¢å¤ï¼Œå¹¶åŸºäºå±€éƒ¨ä¸å…¨å±€å¯¹é½çš„æƒè¡¡æ¥è‡ªé€‚åº”è°ƒæ•´ä¼˜å…ˆçº§ã€‚æ­¤å¤–ï¼ŒHuLiRAGå¼•å…¥äº†æ©ç å¼•å¯¼çš„å¾®è°ƒ(Mask-guided fine-tuning)ï¼Œå°†ç©ºé—´è¯æ®è½¬åŒ–ä¸ºç”Ÿæˆè¿‡ç¨‹ä¸­çš„æ˜¾å¼çº¦æŸã€‚å®éªŒç»“æœè¯æ˜ï¼Œè¯¥æ¡†æ¶åœ¨æé«˜å®šä½ä¿çœŸåº¦å’Œäº‹å®ä¸€è‡´æ€§çš„åŒæ—¶æ˜¾è‘—å‡å°‘äº†å¹»è§‰ï¼Œä¸ºå®ç°å¯ä¿¡çš„å¤šæ¨¡æ€æ¨ç†å¥ å®šäº†åŸºç¡€ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "12 pages, 5 figures",
      "pdf_url": "https://arxiv.org/pdf/2510.10426v1",
      "published_date": "2025-10-12 03:22:33 UTC",
      "updated_date": "2025-10-12 03:22:33 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T02:57:22.756717+00:00"
    },
    {
      "arxiv_id": "2510.10417v1",
      "title": "Combo-Gait: Unified Transformer Framework for Multi-Modal Gait Recognition and Attribute Analysis",
      "title_zh": "Combo-Gaitï¼šé¢å‘å¤šæ¨¡æ€æ­¥æ€è¯†åˆ«ä¸å±æ€§åˆ†æçš„ç»Ÿä¸€ Transformer æ¡†æ¶",
      "authors": [
        "Zhao-Yang Wang",
        "Zhimin Shao",
        "Jieneng Chen",
        "Rama Chellappa"
      ],
      "abstract": "Gait recognition is an important biometric for human identification at a distance, particularly under low-resolution or unconstrained environments. Current works typically focus on either 2D representations (e.g., silhouettes and skeletons) or 3D representations (e.g., meshes and SMPLs), but relying on a single modality often fails to capture the full geometric and dynamic complexity of human walking patterns. In this paper, we propose a multi-modal and multi-task framework that combines 2D temporal silhouettes with 3D SMPL features for robust gait analysis. Beyond identification, we introduce a multitask learning strategy that jointly performs gait recognition and human attribute estimation, including age, body mass index (BMI), and gender. A unified transformer is employed to effectively fuse multi-modal gait features and better learn attribute-related representations, while preserving discriminative identity cues. Extensive experiments on the large-scale BRIAR datasets, collected under challenging conditions such as long-range distances (up to 1 km) and extreme pitch angles (up to 50Â°), demonstrate that our approach outperforms state-of-the-art methods in gait recognition and provides accurate human attribute estimation. These results highlight the promise of multi-modal and multitask learning for advancing gait-based human understanding in real-world scenarios.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† Combo-Gaitï¼Œä¸€ç§ç”¨äºå¤šæ¨¡æ€æ­¥æ€è¯†åˆ«å’Œå±æ€§åˆ†æçš„ç»Ÿä¸€ Transformer æ¡†æ¶ã€‚é’ˆå¯¹å•ä¸€ 2D æˆ– 3D æ¨¡æ€éš¾ä»¥æ•æ‰å¤æ‚æ­¥è¡Œæ¨¡å¼çš„å±€é™ï¼Œè¯¥æ¡†æ¶å°† 2D æ—¶é—´è½®å»“å›¾ (silhouettes) ä¸ 3D SMPL ç‰¹å¾ç›¸ç»“åˆï¼Œä»¥å¢å¼ºæ­¥æ€åˆ†æçš„é²æ£’æ€§ã€‚ç ”ç©¶å¼•å…¥äº†å¤šä»»åŠ¡å­¦ä¹ ç­–ç•¥ï¼Œåœ¨å®ç°èº«ä»½è¯†åˆ«çš„åŒæ—¶åŒæ­¥è¿›è¡Œå¹´é¾„ã€èº«ä½“è´¨é‡æŒ‡æ•° (BMI) å’Œæ€§åˆ«ç­‰äººç±»å±æ€§çš„ä¼°è®¡ã€‚é€šè¿‡ Unified Transformer æœ‰æ•ˆèåˆå¤šæ¨¡æ€æ­¥æ€ç‰¹å¾å¹¶å­¦ä¹ å±æ€§ç›¸å…³çš„è¡¨ç¤ºï¼ŒåŒæ—¶ä¿ç•™äº†å…·æœ‰è¾¨åˆ«æ€§çš„èº«ä»½çº¿ç´¢ã€‚åœ¨åŒ…å«é•¿è·ç¦»ï¼ˆè¾¾1å…¬é‡Œï¼‰å’Œæç«¯ä¿¯ä»°è§’ï¼ˆè¾¾50Â°ï¼‰æŒ‘æˆ˜çš„ BRIAR å¤§è§„æ¨¡æ•°æ®é›†ä¸Šçš„å®éªŒè¯æ˜ï¼Œè¯¥æ–¹æ³•åœ¨æ­¥æ€è¯†åˆ«æ–¹é¢ä¼˜äºç°æœ‰æœ€å…ˆè¿›æŠ€æœ¯ï¼Œå¹¶èƒ½æä¾›å‡†ç¡®çš„å±æ€§ä¼°è®¡ã€‚è¿™é¡¹å·¥ä½œçªæ˜¾äº†å¤šæ¨¡æ€å’Œå¤šä»»åŠ¡å­¦ä¹ åœ¨å¤æ‚çœŸå®åœºæ™¯ä¸‹æå‡äººç±»æ­¥æ€ç†è§£çš„æ½œåŠ›ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.10417v1",
      "published_date": "2025-10-12 02:56:40 UTC",
      "updated_date": "2025-10-12 02:56:40 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T02:57:24.747753+00:00"
    },
    {
      "arxiv_id": "2510.10415v1",
      "title": "LONGQAEVAL: Designing Reliable Evaluations of Long-Form Clinical QA under Resource Constraints",
      "title_zh": "LONGQAEVALï¼šèµ„æºå—é™ç¯å¢ƒä¸‹é•¿ç¯‡ä¸´åºŠé—®ç­”å¯é è¯„ä¼°çš„è®¾è®¡",
      "authors": [
        "Federica Bologna",
        "Tiffany Pan",
        "Matthew Wilkens",
        "Yue Guo",
        "Lucy Lu Wang"
      ],
      "abstract": "Evaluating long-form clinical question answering (QA) systems is resource-intensive and challenging: accurate judgments require medical expertise and achieving consistent human judgments over long-form text is difficult. We introduce LongQAEval, an evaluation framework and set of evaluation recommendations for limited-resource and high-expertise settings. Based on physician annotations of 300 real patient questions answered by physicians and LLMs, we compare coarse answer-level versus fine-grained sentence-level evaluation over the dimensions of correctness, relevance, and safety. We find that inter-annotator agreement (IAA) varies by dimension: fine-grained annotation improves agreement on correctness, coarse improves agreement on relevance, and judgments on safety remain inconsistent. Additionally, annotating only a small subset of sentences can provide reliability comparable to coarse annotations, reducing cost and effort.",
      "tldr_zh": "è¯¥ç ”ç©¶å¼•å…¥äº† LongQAEvalï¼Œè¿™æ˜¯ä¸€ä¸ªé’ˆå¯¹èµ„æºæœ‰é™å’Œé«˜ä¸“ä¸šæ€§è¦æ±‚çš„é•¿ç¯‡ä¸´åºŠé—®ç­” (Long-Form Clinical QA) ç³»ç»Ÿçš„è¯„ä¼°æ¡†æ¶ã€‚åŸºäºåŒ»ç”Ÿå¯¹300ä¸ªçœŸå®æ‚£è€…é—®é¢˜çš„æ ‡æ³¨ï¼Œè¯¥ç ”ç©¶å¯¹æ¯”äº†ç²—ç²’åº¦ (Coarse) ç­”æ¡ˆçº§åˆ«ä¸ç»†ç²’åº¦ (Fine-grained) å¥å­çº§åˆ«åœ¨æ­£ç¡®æ€§ã€ç›¸å…³æ€§å’Œå®‰å…¨æ€§ç»´åº¦ä¸Šçš„è¯„ä¼°è¡¨ç°ã€‚ç ”ç©¶å‘ç°è·¨æ ‡æ³¨è€…ä¸€è‡´æ€§ (Inter-annotator agreement, IAA) éšè¯„ä¼°ç»´åº¦è€Œå¼‚ï¼šç»†ç²’åº¦æ ‡æ³¨æé«˜äº†æ­£ç¡®æ€§çš„ä¸€è‡´æ€§ï¼Œè€Œç²—ç²’åº¦æ ‡æ³¨åˆ™åœ¨ç›¸å…³æ€§ä¸Šè¡¨ç°æ›´å¥½ï¼Œä½†å®‰å…¨æ€§çš„åˆ¤æ–­ä»ä¿æŒä¸ä¸€è‡´ã€‚æ­¤å¤–ï¼Œå®éªŒè¡¨æ˜ä»…æ ‡æ³¨ä¸€å°éƒ¨åˆ†å¥å­å­é›†å³å¯æä¾›ä¸ç²—ç²’åº¦æ ‡æ³¨ç›¸å½“çš„å¯é æ€§ï¼Œä»è€Œåœ¨ä¿è¯è¯„ä¼°è´¨é‡çš„åŒæ—¶æ˜¾è‘—é™ä½äº†åŒ»å­¦ä¸“å®¶çš„æ ‡æ³¨æˆæœ¬å’Œå·¥ä½œé‡ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.10415v1",
      "published_date": "2025-10-12 02:49:04 UTC",
      "updated_date": "2025-10-12 02:49:04 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T02:57:25.262513+00:00"
    },
    {
      "arxiv_id": "2510.10409v1",
      "title": "Trace Length is a Simple Uncertainty Signal in Reasoning Models",
      "title_zh": "æ¨ç†é•¿åº¦æ˜¯æ¨ç†æ¨¡å‹ä¸­ä¸€ç§ç®€å•çš„ä¸ç¡®å®šæ€§ä¿¡å·",
      "authors": [
        "Siddartha Devic",
        "Charlotte Peale",
        "Arwen Bradley",
        "Sinead Williamson",
        "Preetum Nakkiran",
        "Aravind Gollakota"
      ],
      "abstract": "Uncertainty quantification for LLMs is a key research direction towards addressing hallucination and other issues that limit their reliable deployment. In this work, we show that reasoning trace length is a simple and useful confidence estimator in large reasoning models. Through comprehensive experiments across multiple models, datasets, and prompts, we show that trace length performs in comparable but complementary ways to other zero-shot confidence estimators such as verbalized confidence. Our work reveals that reasoning post-training fundamentally alters the relationship between trace length and accuracy, going beyond prior work that had shown that post-training causes traces to grow longer in general (e.g., \"overthinking\"). We investigate the mechanisms behind trace length's performance as a confidence signal, observing that the effect remains even after adjusting for confounders such as problem difficulty and GRPO-induced length bias. We identify high-entropy or \"forking\" tokens as playing a key role in the mechanism. Our findings demonstrate that reasoning post-training enhances uncertainty quantification beyond verbal expressions, and establish trace length as a practical confidence measure for large reasoning models.",
      "tldr_zh": "æœ¬ç ”ç©¶å‘ç°æ¨ç†è·¯å¾„é•¿åº¦(Trace Length)åœ¨å¤§è§„æ¨¡æ¨ç†æ¨¡å‹ä¸­æ˜¯ä¸€ä¸ªç®€å•ä¸”æœ‰æ•ˆçš„ç½®ä¿¡åº¦ä¼°è®¡æŒ‡æ ‡(confidence estimator)ï¼Œå¯ç”¨äºç¼“è§£LLMsçš„å¹»è§‰åŠéƒ¨ç½²å¯é æ€§é—®é¢˜ã€‚é€šè¿‡å¤šæ¨¡å‹å’Œå¤šæ•°æ®é›†çš„å®éªŒï¼Œè¯¥ç ”ç©¶è¯æ˜Trace Lengthä½œä¸ºä¸€ç§zero-shotç½®ä¿¡åº¦åº¦é‡å·¥å…·ï¼Œä¸ä¼ ç»Ÿçš„verbalized confidenceå…·æœ‰äº’è¡¥æ€§ã€‚ç ”ç©¶æ­ç¤ºäº†æ¨ç†é˜¶æ®µçš„post-trainingä¸ä»…å¢åŠ äº†æ¨ç†é•¿åº¦ï¼Œè¿˜ä»æ ¹æœ¬ä¸Šæ”¹å˜äº†é•¿åº¦ä¸å‡†ç¡®ç‡ä¹‹é—´çš„å…³ç³»ã€‚å³ä¾¿åœ¨è°ƒæ•´äº†é—®é¢˜éš¾åº¦å’ŒGRPOå¼•èµ·çš„é•¿åº¦åå·®ç­‰å¹²æ‰°å› ç´ åï¼Œè¯¥ä¿¡å·ä¾ç„¶ç¨³å¥ï¼Œå…¶æ ¸å¿ƒæœºåˆ¶åœ¨äºhigh-entropyæˆ–forking tokensçš„å‡ºç°ã€‚è¿™é¡¹å·¥ä½œè¡¨æ˜æ¨ç†post-trainingå¢å¼ºäº†æ¨¡å‹åœ¨å£å¤´è¡¨è¾¾ä¹‹å¤–çš„ç¡®å®šæ€§é‡åŒ–èƒ½åŠ›ï¼Œå°†Trace Lengthç¡®ç«‹ä¸ºå¤§å‹æ¨ç†æ¨¡å‹ä¸­ä¸€ç§å®ç”¨çš„ä¸ç¡®å®šæ€§é‡åŒ–(uncertainty quantification)æ‰‹æ®µã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.10409v1",
      "published_date": "2025-10-12 02:04:06 UTC",
      "updated_date": "2025-10-12 02:04:06 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T02:57:41.073882+00:00"
    },
    {
      "arxiv_id": "2510.10406v1",
      "title": "Mesh-Gait: A Unified Framework for Gait Recognition Through Multi-Modal Representation Learning from 2D Silhouettes",
      "title_zh": "Mesh-Gaitï¼šåŸºäºäºŒç»´å‰ªå½±å¤šæ¨¡æ€è¡¨å¾å­¦ä¹ çš„ç»Ÿä¸€æ­¥æ€è¯†åˆ«æ¡†æ¶",
      "authors": [
        "Zhao-Yang Wang",
        "Jieneng Chen",
        "Jiang Liu",
        "Yuxiang Guo",
        "Rama Chellappa"
      ],
      "abstract": "Gait recognition, a fundamental biometric technology, leverages unique walking patterns for individual identification, typically using 2D representations such as silhouettes or skeletons. However, these methods often struggle with viewpoint variations, occlusions, and noise. Multi-modal approaches that incorporate 3D body shape information offer improved robustness but are computationally expensive, limiting their feasibility for real-time applications. To address these challenges, we introduce Mesh-Gait, a novel end-to-end multi-modal gait recognition framework that directly reconstructs 3D representations from 2D silhouettes, effectively combining the strengths of both modalities. Compared to existing methods, directly learning 3D features from 3D joints or meshes is complex and difficult to fuse with silhouette-based gait features. To overcome this, Mesh-Gait reconstructs 3D heatmaps as an intermediate representation, enabling the model to effectively capture 3D geometric information while maintaining simplicity and computational efficiency. During training, the intermediate 3D heatmaps are gradually reconstructed and become increasingly accurate under supervised learning, where the loss is calculated between the reconstructed 3D joints, virtual markers, and 3D meshes and their corresponding ground truth, ensuring precise spatial alignment and consistent 3D structure. Mesh-Gait extracts discriminative features from both silhouettes and reconstructed 3D heatmaps in a computationally efficient manner. This design enables the model to capture spatial and structural gait characteristics while avoiding the heavy overhead of direct 3D reconstruction from RGB videos, allowing the network to focus on motion dynamics rather than irrelevant visual details. Extensive experiments demonstrate that Mesh-Gait achieves state-of-the-art accuracy. The code will be released upon acceptance of the paper.",
      "tldr_zh": "æœ¬ç ”ç©¶æå‡ºäº†Mesh-Gaitï¼Œä¸€ç§é€šè¿‡ä»2Då‰ªå½±(2D Silhouettes)è¿›è¡Œå¤šæ¨¡æ€è¡¨ç¤ºå­¦ä¹ çš„ç»Ÿä¸€æ­¥æ€è¯†åˆ«æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³ä¼ ç»Ÿ2Dæ–¹æ³•åœ¨è§†è§’å˜åŒ–å’Œå™ªå£°ä¸‹çš„å±€é™æ€§ä»¥åŠ3Dæ–¹æ³•çš„é«˜è®¡ç®—æˆæœ¬é—®é¢˜ã€‚è¯¥æ¡†æ¶åˆ›æ–°æ€§åœ°é‡‡ç”¨3Dçƒ­å›¾(3D heatmaps)ä½œä¸ºä¸­é—´è¡¨ç¤ºï¼Œç›´æ¥ä»2Då‰ªå½±é‡å»º3Då‡ ä½•ä¿¡æ¯ï¼Œæœ‰æ•ˆç»“åˆäº†ä¸¤ç§æ¨¡æ€çš„ä¼˜åŠ¿ã€‚åœ¨è®­ç»ƒé˜¶æ®µï¼Œæ¨¡å‹é€šè¿‡ç›‘ç£å­¦ä¹ ç²¾å‡†é‡å»º3Då…³èŠ‚ç‚¹ã€è™šæ‹Ÿæ ‡è®°å’Œ3Dç½‘æ ¼(3D meshes)ï¼Œç¡®ä¿äº†ç©ºé—´å¯¹é½å’Œä¸€è‡´çš„3Dç»“æ„ã€‚Mesh-Gaitèƒ½å¤Ÿä»¥è®¡ç®—é«˜æ•ˆçš„æ–¹å¼ä»å‰ªå½±å’Œé‡å»ºçš„3Dçƒ­å›¾ä¸­æå–åˆ¤åˆ«æ€§ç‰¹å¾ï¼Œé¿å…äº†ä»RGBè§†é¢‘ç›´æ¥è¿›è¡Œ3Dé‡å»ºçš„æ²‰é‡å¼€é”€ï¼Œä½¿ç½‘ç»œèƒ½å¤Ÿä¸“æ³¨äºè¿åŠ¨åŠ¨åŠ›å­¦ç‰¹å¾ã€‚å®éªŒç»“æœè¯æ˜ï¼ŒMesh-Gaitåœ¨æ­¥æ€è¯†åˆ«ä»»åŠ¡ä¸Šå–å¾—äº†å½“å‰æœ€å…ˆè¿›(State-of-the-art)çš„å‡†ç¡®ç‡ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.10406v1",
      "published_date": "2025-10-12 01:49:05 UTC",
      "updated_date": "2025-10-12 01:49:05 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T02:57:38.266169+00:00"
    },
    {
      "arxiv_id": "2510.10402v1",
      "title": "Controllable Graph Generation with Diffusion Models via Inference-Time Tree Search Guidance",
      "title_zh": "åŸºäºæ¨ç†æ—¶æ ‘æœç´¢å¼•å¯¼çš„æ‰©æ•£æ¨¡å‹å¯æ§å›¾ç”Ÿæˆ",
      "authors": [
        "Jiachi Zhao",
        "Zehong Wang",
        "Yamei Liao",
        "Chuxu Zhang",
        "Yanfang Ye"
      ],
      "abstract": "Graph generation is a fundamental problem in graph learning with broad applications across Web-scale systems, knowledge graphs, and scientific domains such as drug and material discovery. Recent approaches leverage diffusion models for step-by-step generation, yet unconditional diffusion offers little control over desired properties, often leading to unstable quality and difficulty in incorporating new objectives. Inference-time guidance methods mitigate these issues by adjusting the sampling process without retraining, but they remain inherently local, heuristic, and limited in controllability. To overcome these limitations, we propose TreeDiff, a Monte Carlo Tree Search (MCTS) guided dual-space diffusion framework for controllable graph generation. TreeDiff is a plug-and-play inference-time method that expands the search space while keeping computation tractable. Specifically, TreeDiff introduces three key designs to make it practical and scalable: (1) a macro-step expansion strategy that groups multiple denoising updates into a single transition, reducing tree depth and enabling long-horizon exploration; (2) a dual-space denoising mechanism that couples efficient latent-space denoising with lightweight discrete correction in graph space, ensuring both scalability and structural fidelity; and (3) a dual-space verifier that predicts long-term rewards from partially denoised graphs, enabling early value estimation and removing the need for full rollouts. Extensive experiments on 2D and 3D molecular generation benchmarks, under both unconditional and conditional settings, demonstrate that TreeDiff achieves state-of-the-art performance. Notably, TreeDiff exhibits favorable inference-time scaling: it continues to improve with additional computation, while existing inference-time methods plateau early under limited resources.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹æ‰©æ•£æ¨¡å‹(Diffusion Models)åœ¨å¯æ§å›¾ç”Ÿæˆ(Controllable Graph Generation)ä¸­å­˜åœ¨çš„æ§åˆ¶åŠ›ä¸è¶³ã€è´¨é‡ä¸ç¨³å®šä»¥åŠç°æœ‰æ¨ç†å¼•å¯¼æ–¹æ³•å±€é™äºå±€éƒ¨å¯å‘å¼æœç´¢ç­‰é—®é¢˜ï¼Œæå‡ºäº†åä¸ºTreeDiffçš„æ¡†æ¶ã€‚TreeDiffæ˜¯ä¸€ç§å³æ’å³ç”¨çš„æ¨ç†ç«¯æ–¹æ³•ï¼Œç»“åˆäº†è’™ç‰¹å¡æ´›æ ‘æœç´¢(Monte Carlo Tree Search)å’ŒåŒç©ºé—´æ‰©æ•£(Dual-space Diffusion)æœºåˆ¶æ¥å¢å¼ºç”Ÿæˆè¿‡ç¨‹çš„å¯æ§æ€§ã€‚è¯¥æ¡†æ¶å¼•å…¥äº†å®è§‚æ­¥éª¤æ‰©å±•ç­–ç•¥(Macro-step expansion strategy)ï¼Œé€šè¿‡åˆå¹¶å¤šä¸ªå»å™ªæ›´æ–°æ¥é™ä½æœç´¢æ ‘æ·±åº¦ï¼Œä»è€Œå®ç°é•¿ç¨‹æ¢ç´¢ã€‚é‡‡ç”¨çš„åŒç©ºé—´å»å™ªæœºåˆ¶è€¦åˆäº†é«˜æ•ˆçš„æ½œç©ºé—´å»å™ª(Latent-space denoising)ä¸å›¾ç©ºé—´ä¸­çš„ç¦»æ•£æ ¡æ­£ï¼Œç¡®ä¿äº†æ¨¡å‹åœ¨å¤§è§„æ¨¡æ‰©å±•æ—¶çš„ç»“æ„ä¿çœŸåº¦ã€‚æ­¤å¤–ï¼Œç ”ç©¶è®¾è®¡çš„åŒç©ºé—´éªŒè¯å™¨(Dual-space verifier)èƒ½å¤Ÿä»éƒ¨åˆ†å»å™ªçš„å›¾ä¸­é¢„æµ‹é•¿æœŸå¥–åŠ±ï¼Œå®ç°äº†æ—©æœŸä»·å€¼ä¼°è®¡å¹¶æœ‰æ•ˆé¿å…äº†å®Œæ•´çš„Rolloutsè®¡ç®—ã€‚åœ¨2Då’Œ3Dåˆ†å­ç”ŸæˆåŸºå‡†æµ‹è¯•ä¸­çš„å®éªŒç»“æœè¡¨æ˜ï¼ŒTreeDiffåœ¨æ— æ¡ä»¶å’Œæœ‰æ¡ä»¶ç”Ÿæˆè®¾ç½®ä¸‹å‡è¾¾åˆ°äº†å½“å‰çš„å…ˆè¿›æ°´å¹³(State-of-the-art)ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼ŒTreeDiffè¡¨ç°å‡ºä¼˜å¼‚çš„æ¨ç†æ—¶é—´æ‰©å±•æ€§(Inference-time scaling)ï¼Œå…¶ç”Ÿæˆæ€§èƒ½ä¼šéšç€è®¡ç®—èµ„æºçš„å¢åŠ è€ŒæŒç»­æå‡ï¼Œæœ‰æ•ˆçªç ´äº†ç°æœ‰æ–¹æ³•åœ¨èµ„æºå—é™ä¸‹çš„æ€§èƒ½ç“¶é¢ˆã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CE"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.10402v1",
      "published_date": "2025-10-12 01:40:33 UTC",
      "updated_date": "2025-10-12 01:40:33 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T02:57:37.755709+00:00"
    },
    {
      "arxiv_id": "2510.10398v1",
      "title": "STEAM: A Semantic-Level Knowledge Editing Framework for Large Language Models",
      "title_zh": "STEAMï¼šé¢å‘å¤§è¯­è¨€æ¨¡å‹çš„è¯­ä¹‰çº§çŸ¥è¯†ç¼–è¾‘æ¡†æ¶",
      "authors": [
        "Geunyeong Jeong",
        "Juoh Sun",
        "Seonghee Lee",
        "Harksoo Kim"
      ],
      "abstract": "Large Language Models store extensive factual knowledge acquired during large-scale pre-training. However, this knowledge is inherently static, reflecting only the state of the world at the time of training. Knowledge editing has emerged as a promising solution for updating outdated or incorrect facts without full retraining. However, most existing locate-and-edit methods primarily focus on token-level likelihood optimization without addressing semantic coherence. Our analysis reveals that such edited knowledge is often encoded as isolated residual streams in the model's latent space, distinct from pre-existing knowledge and bypassing natural reasoning process. To address this, we propose \\textsc{Steam}, a semantic-level knowledge editing framework that enhances integration of updated knowledge into the model's knowledge structure. \\textsc{Steam} first identifies target representations as semantic anchors for the updated factual association, then guides the internal representation of the edited fact towards these anchors through an alignment loss during optimization. Experimental results demonstrate that \\textsc{Steam} improves model's ability to reason with edited knowledge and enhances semantic coherence, underscoring the importance of latent-space alignment for reliable and coherent knowledge editing. The code is available at https://github.com/GY-Jeong/STEAM.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† STEAMï¼Œä¸€ç§æ—¨åœ¨æå‡å¤§è¯­è¨€æ¨¡å‹ (Large Language Models) è¯­ä¹‰è¿è´¯æ€§çš„çŸ¥è¯†ç¼–è¾‘æ¡†æ¶ï¼Œä»¥è§£å†³ç°æœ‰å®šä½ç¼–è¾‘æ–¹æ³•åœ¨ Token-level ä¼˜åŒ–æ—¶å¯¼è‡´çš„çŸ¥è¯†å­¤ç«‹é—®é¢˜ã€‚ç ”ç©¶å‘ç°ï¼Œä¼ ç»Ÿçš„ç¼–è¾‘æ–¹å¼å¾€å¾€ä½¿æ–°çŸ¥è¯†åœ¨æ¨¡å‹çš„æ½œç©ºé—´ (Latent Space) ä¸­ä»¥å­¤ç«‹æ®‹å·®æµçš„å½¢å¼å­˜åœ¨ï¼Œä»è€Œç»•è¿‡äº†è‡ªç„¶çš„æ¨ç†è¿‡ç¨‹ã€‚STEAM é€šè¿‡è¯†åˆ«ç›®æ ‡è¡¨ç¤ºä½œä¸ºæ›´æ–°äº‹å®å…³è”çš„è¯­ä¹‰é”šç‚¹ (Semantic Anchors)ï¼Œå¹¶åœ¨ä¼˜åŒ–æœŸé—´åˆ©ç”¨å¯¹é½æŸå¤± (Alignment Loss) å¼•å¯¼å†…éƒ¨è¡¨ç¤ºå‘è¿™äº›é”šç‚¹é æ‹¢ã€‚å®éªŒç»“æœè¯æ˜ï¼ŒSTEAM æ˜¾è‘—å¢å¼ºäº†æ¨¡å‹åˆ©ç”¨ç¼–è¾‘åçŸ¥è¯†è¿›è¡Œæ¨ç†çš„èƒ½åŠ›ï¼Œå¹¶æœ‰æ•ˆæå‡äº†è¯­ä¹‰è¿è´¯æ€§ã€‚è¯¥æ¡†æ¶å¼ºè°ƒäº†æ½œç©ºé—´å¯¹é½ (Latent-space Alignment) å¯¹äºå®ç°å¯é ä¸”è¿è´¯çš„çŸ¥è¯†ç¼–è¾‘çš„å…³é”®ä½œç”¨ï¼Œç›¸å…³ä»£ç å·²å¼€æºã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "Accepted to EMNLP 2025 (Findings)",
      "pdf_url": "https://arxiv.org/pdf/2510.10398v1",
      "published_date": "2025-10-12 01:25:13 UTC",
      "updated_date": "2025-10-12 01:25:13 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T02:57:51.674444+00:00"
    },
    {
      "arxiv_id": "2510.10390v1",
      "title": "RefusalBench: Generative Evaluation of Selective Refusal in Grounded Language Models",
      "title_zh": "RefusalBenchï¼šæœ‰æºè¯­è¨€æ¨¡å‹é€‰æ‹©æ€§æ‹’ç»çš„ç”Ÿæˆå¼è¯„ä¼°",
      "authors": [
        "Aashiq Muhamed",
        "Leonardo F. R. Ribeiro",
        "Markus Dreyer",
        "Virginia Smith",
        "Mona T. Diab"
      ],
      "abstract": "The ability of language models in RAG systems to selectively refuse to answer based on flawed context is critical for safety, yet remains a significant failure point. Our large-scale study reveals that even frontier models struggle in this setting, with refusal accuracy dropping below 50% on multi-document tasks, while exhibiting either dangerous overconfidence or overcaution. Static benchmarks fail to reliably evaluate this capability, as models exploit dataset-specific artifacts and memorize test instances. We introduce RefusalBench, a generative methodology that programmatically creates diagnostic test cases through controlled linguistic perturbation. Our framework employs 176 distinct perturbation strategies across six categories of informational uncertainty and three intensity levels. Evaluation of over 30 models uncovers systematic failure patterns: refusal comprises separable detection and categorization skills, and neither scale nor extended reasoning improves performance. We find that selective refusal is a trainable, alignment-sensitive capability, offering a clear path for improvement. We release two benchmarks -- RefusalBench-NQ (single document) and RefusalBench-GaRAGe (multi-document) -- and our complete generation framework to enable continued, dynamic evaluation of this critical capability.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹æ£€ç´¢å¢å¼ºç”Ÿæˆ(RAG)ç³»ç»Ÿåœ¨å¤„ç†é”™è¯¯ä¸Šä¸‹æ–‡æ—¶éš¾ä»¥å®ç°é€‰æ‹©æ€§æ‹’ç»(Selective Refusal)çš„å®‰å…¨ç“¶é¢ˆï¼Œæå‡ºäº†RefusalBenchè¿™ä¸€ç”Ÿæˆå¼è¯„ä¼°æ–¹æ³•ã€‚è¯¥æ¡†æ¶é€šè¿‡å—æ§çš„è¯­è¨€æ‰°åŠ¨(Linguistic Perturbation)ç¨‹åºåŒ–åœ°æ„å»ºè¯Šæ–­æµ‹è¯•ç”¨ä¾‹ï¼Œæ¶µç›–äº†176ç§æ‰°åŠ¨ç­–ç•¥ï¼Œæ¶‰åŠå…­ç±»ä¿¡æ¯ä¸ç¡®å®šæ€§å’Œä¸‰ä¸ªå¼ºåº¦ç­‰çº§ã€‚å¯¹30å¤šä¸ªæ¨¡å‹çš„è¯„ä¼°æ­ç¤ºäº†æ¨¡å‹åœ¨å¤šæ–‡æ¡£ä»»åŠ¡ä¸­æ‹’ç»å‡†ç¡®ç‡ä½äº50%ç­‰ç³»ç»Ÿæ€§å¤±æ•ˆæ¨¡å¼ï¼Œå¹¶å‘ç°å•çº¯æå‡æ¨¡å‹è§„æ¨¡(Scale)æˆ–æ¨ç†èƒ½åŠ›(Reasoning)å¹¶ä¸èƒ½æœ‰æ•ˆæ”¹å–„æ€§èƒ½ã€‚ç ”ç©¶æŒ‡å‡ºï¼Œé€‰æ‹©æ€§æ‹’ç»åŒ…å«å¯åˆ†ç¦»çš„æ£€æµ‹ä¸åˆ†ç±»æŠ€èƒ½ï¼Œä¸”å±äºä¸€ç§å¯¹å¯¹é½(Alignment)æ•æ„Ÿçš„å¯è®­ç»ƒèƒ½åŠ›ã€‚æœ€åï¼Œç ”ç©¶è€…å‘å¸ƒäº†RefusalBench-NQä¸RefusalBench-GaRAGeåŸºå‡†æµ‹è¯•é›†åŠå®Œæ•´æ¡†æ¶ï¼Œæ—¨åœ¨æ¨åŠ¨å¯¹è¯­è¨€æ¨¡å‹è¿™ä¸€å…³é”®å®‰å…¨èƒ½åŠ›çš„åŠ¨æ€è¯„ä¼°ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.10390v1",
      "published_date": "2025-10-12 00:53:42 UTC",
      "updated_date": "2025-10-12 00:53:42 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T02:57:57.141889+00:00"
    },
    {
      "arxiv_id": "2510.10383v1",
      "title": "Identifying bias in CNN image classification using image scrambling and transforms",
      "title_zh": "åŸºäºå›¾åƒç½®ä¹±ä¸å˜æ¢çš„ CNN å›¾åƒåˆ†ç±»åå·®è¯†åˆ«",
      "authors": [
        "Sai Teja Erukude"
      ],
      "abstract": "CNNs are now prevalent as the primary choice for most machine vision problems due to their superior rate of classification and the availability of user-friendly libraries. These networks effortlessly identify and select features in a non-intuitive data-driven manner, making it difficult to determine which features were most influential. That leads to a ``black box\", where users cannot know how the image data are analyzed but rely on empirical results. Therefore the decision-making process can be biased by background information that is difficult to detect. Here we discuss examples of such hidden biases and propose techniques for identifying them, methods to distinguish between contextual information and background noise, and explore whether CNNs learn from irrelevant features. One effective approach to identify dataset bias is to classify blank background parts of the images. However, in some situations a blank background in the images is not available, making it more difficult to separate the foreground information from the blank background. Such parts of the image can also be considered contextual learning, not necessarily bias. To overcome this, we propose two approaches that were tested on six different datasets, including natural, synthetic, and hybrid datasets. The first method involves dividing images into smaller, non-overlapping tiles of various sizes, which are then shuffled randomly, making classification more challenging. The second method involves the application of several image transforms, including Fourier, Wavelet transforms, and Median filter, and their combinations. These transforms help recover background noise information used by CNN to classify images. Results indicate that this method can effectively distinguish between contextual information and background noise, and alert on the presence of background noise even without the need to use background information.",
      "tldr_zh": "å·ç§¯ç¥ç»ç½‘ç»œ(CNNs)ä½œä¸ºæœºå™¨è§†è§‰çš„é¦–é€‰ï¼Œå…¶æ•°æ®é©±åŠ¨çš„ç‰¹å¾é€‰æ‹©æœºåˆ¶å¸¸å¯¼è‡´â€œé»‘ç›’â€é—®é¢˜ï¼Œä½¿å¾—å†³ç­–è¿‡ç¨‹å®¹æ˜“å—åˆ°éš¾ä»¥å¯Ÿè§‰çš„èƒŒæ™¯ä¿¡æ¯åè§å½±å“ã€‚è¯¥ç ”ç©¶é’ˆå¯¹è¿™ä¸€é—®é¢˜æå‡ºäº†è¯†åˆ«éšè—åè§ã€åŒºåˆ†èƒŒæ™¯å™ªå£°ä¸ä¸Šä¸‹æ–‡ä¿¡æ¯çš„æŠ€æœ¯æ–¹æ¡ˆã€‚åœ¨æ— æ³•ç›´æ¥è·å–ç©ºç™½èƒŒæ™¯çš„æƒ…å†µä¸‹ï¼Œç ”ç©¶æå‡ºäº†ä¸¤ç§æ ¸å¿ƒæ–¹æ³•ï¼šç¬¬ä¸€ç§æ˜¯å°†å›¾åƒåˆ’åˆ†ä¸ºä¸åŒå¤§å°çš„éé‡å åˆ‡ç‰‡å¹¶è¿›è¡Œéšæœºæ‰“ä¹±(image scrambling)ï¼›ç¬¬äºŒç§åˆ™æ˜¯åº”ç”¨å‚…é‡Œå¶å˜æ¢(Fourier transform)ã€å°æ³¢å˜æ¢(Wavelet transform)å’Œä¸­å€¼æ»¤æ³¢å™¨(Median filter)ç­‰å›¾åƒå˜æ¢åŠå…¶ç»„åˆã€‚é€šè¿‡åœ¨è‡ªç„¶ã€åˆæˆå’Œæ··åˆç­‰å…­ç§ä¸åŒæ•°æ®é›†ä¸Šçš„å®éªŒéªŒè¯ï¼Œè¿™äº›æ–¹æ³•è¢«è¯æ˜èƒ½æœ‰æ•ˆä»èƒŒæ™¯å™ªå£°ä¸­æå–ä¸Šä¸‹æ–‡ä¿¡æ¯ã€‚ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œè¯¥æŠ€æœ¯å³ä½¿åœ¨ç¼ºä¹æ˜¾å¼èƒŒæ™¯ä¿¡æ¯æ—¶ä¹Ÿèƒ½è¯†åˆ«èƒŒæ™¯å™ªå£°çš„å­˜åœ¨ï¼Œä¸ºæ£€æµ‹å’Œé¢„è­¦CNNå›¾åƒåˆ†ç±»ä¸­çš„æ¨¡å‹åè§æä¾›äº†æœ‰åŠ›æ”¯æŒã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "62 pages, Master's thesis",
      "pdf_url": "https://arxiv.org/pdf/2510.10383v1",
      "published_date": "2025-10-12 00:43:29 UTC",
      "updated_date": "2025-10-12 00:43:29 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T02:57:59.762710+00:00"
    },
    {
      "arxiv_id": "2510.10379v1",
      "title": "RobotFleet: An Open-Source Framework for Centralized Multi-Robot Task Planning",
      "title_zh": "RobotFleetï¼šä¸€ç§é¢å‘é›†ä¸­å¼å¤šæœºå™¨äººä»»åŠ¡è§„åˆ’çš„å¼€æºæ¡†æ¶",
      "authors": [
        "Rohan Gupta",
        "Trevor Asbery",
        "Zain Merchant",
        "Abrar Anwar",
        "Jesse Thomason"
      ],
      "abstract": "Coordinating heterogeneous robot fleets to achieve multiple goals is challenging in multi-robot systems. We introduce an open-source and extensible framework for centralized multi-robot task planning and scheduling that leverages LLMs to enable fleets of heterogeneous robots to accomplish multiple tasks. RobotFleet provides abstractions for planning, scheduling, and execution across robots deployed as containerized services to simplify fleet scaling and management. The framework maintains a shared declarative world state and two-way communication for task execution and replanning. By modularizing each layer of the autonomy stack and using LLMs for open-world reasoning, RobotFleet lowers the barrier to building scalable multi-robot systems. The code can be found here: https://github.com/therohangupta/robot-fleet.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¼‚æ„æœºå™¨äººé›†ç¾¤(heterogeneous robot fleets)åœ¨å®ç°å¤šç›®æ ‡ä»»åŠ¡åè°ƒä¸­çš„æŒ‘æˆ˜ï¼Œæå‡ºäº†ä¸€ä¸ªåä¸ºRobotFleetçš„å¼€æºä¸”å¯æ‰©å±•çš„ä¸­å¿ƒåŒ–å¤šæœºå™¨äººä»»åŠ¡è§„åˆ’ä¸è°ƒåº¦æ¡†æ¶ã€‚è¯¥æ¡†æ¶åˆ›æ–°æ€§åœ°åˆ©ç”¨å¤§è¯­è¨€æ¨¡å‹(LLMs)è¿›è¡Œå¼€æ”¾ä¸–ç•Œæ¨ç†ï¼Œä½¿å¼‚æ„æœºå™¨äººèƒ½å¤ŸååŒå®Œæˆå¤æ‚çš„å¤šä»»åŠ¡ã€‚RobotFleetå°†æœºå™¨äººéƒ¨ç½²ä¸ºå®¹å™¨åŒ–æœåŠ¡ï¼Œå¹¶åœ¨è§„åˆ’ã€è°ƒåº¦å’Œæ‰§è¡Œå±‚å»ºç«‹æŠ½è±¡ï¼Œä»è€Œæå¤§ç®€åŒ–äº†é›†ç¾¤çš„æ‰©å±•ä¸ç®¡ç†æµç¨‹ã€‚ç³»ç»Ÿé€šè¿‡ç»´æŠ¤å…±äº«çš„é™ˆè¿°å¼ä¸–ç•ŒçŠ¶æ€(declarative world state)å’ŒåŒå‘é€šä¿¡æœºåˆ¶ï¼Œç¡®ä¿äº†ä»»åŠ¡æ‰§è¡Œçš„é«˜æ•ˆæ€§åŠå®æ—¶é‡æ–°è§„åˆ’(replanning)èƒ½åŠ›ã€‚é€šè¿‡å°†è‡ªä¸»å †æ ˆ(autonomy stack)çš„å„å±‚è¿›è¡Œæ¨¡å—åŒ–å¤„ç†ï¼Œè¯¥æ¡†æ¶æ˜¾è‘—é™ä½äº†å¼€å‘å¤§è§„æ¨¡ã€å¯æ‰©å±•å¤šæœºå™¨äººç³»ç»Ÿçš„æŠ€æœ¯é—¨æ§›ã€‚",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.MA"
      ],
      "primary_category": "cs.RO",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.10379v1",
      "published_date": "2025-10-12 00:32:37 UTC",
      "updated_date": "2025-10-12 00:32:37 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T02:58:02.653489+00:00"
    }
  ],
  "processing_status": "completed",
  "error": null,
  "raw_papers_fetched": true,
  "papers_count": 114,
  "processed_papers_count": 114,
  "failed_papers_count": 0,
  "llm_backup_calls": 0,
  "summary_generated": true,
  "daily_data_saved": true,
  "last_update": "2026-01-25T02:58:48.061397+00:00"
}
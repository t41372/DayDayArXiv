[
  {
    "arxiv_id": "2404.01526v1",
    "title": "Categorical semiotics: Foundations for Knowledge Integration",
    "authors": [
      "Carlos Leandro"
    ],
    "abstract": "The integration of knowledge extracted from diverse models, whether described\nby domain experts or generated by machine learning algorithms, has historically\nbeen challenged by the absence of a suitable framework for specifying and\nintegrating structures, learning processes, data transformations, and data\nmodels or rules. In this work, we extend algebraic specification methods to\naddress these challenges within such a framework.\n  In our work, we tackle the challenging task of developing a comprehensive\nframework for defining and analyzing deep learning architectures. We believe\nthat previous efforts have fallen short by failing to establish a clear\nconnection between the constraints a model must adhere to and its actual\nimplementation.\n  Our methodology employs graphical structures that resemble Ehresmann's\nsketches, interpreted within a universe of fuzzy sets. This approach offers a\nunified theory that elegantly encompasses both deterministic and\nnon-deterministic neural network designs. Furthermore, we highlight how this\ntheory naturally incorporates fundamental concepts from computer science and\nautomata theory. Our extended algebraic specification framework, grounded in\ngraphical structures akin to Ehresmann's sketches, offers a promising solution\nfor integrating knowledge across disparate models and domains. By bridging the\ngap between domain-specific expertise and machine-generated insights, we pave\nthe way for more comprehensive, collaborative, and effective approaches to\nknowledge integration and modeling.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "71 pages, 15 figures. arXiv admin note: substantial text overlap with\n  arXiv:1604.02790",
    "pdf_url": "http://arxiv.org/pdf/2404.01526v1",
    "published_date": "2024-04-01 23:19:01 UTC",
    "updated_date": "2024-04-01 23:19:01 UTC"
  },
  {
    "arxiv_id": "2404.01524v1",
    "title": "On Train-Test Class Overlap and Detection for Image Retrieval",
    "authors": [
      "Chull Hwan Song",
      "Jooyoung Yoon",
      "Taebaek Hwang",
      "Shunghyun Choi",
      "Yeong Hyeon Gu",
      "Yannis Avrithis"
    ],
    "abstract": "How important is it for training and evaluation sets to not have class\noverlap in image retrieval? We revisit Google Landmarks v2 clean, the most\npopular training set, by identifying and removing class overlap with Revisited\nOxford and Paris [34], the most popular evaluation set. By comparing the\noriginal and the new RGLDv2-clean on a benchmark of reproduced state-of-the-art\nmethods, our findings are striking. Not only is there a dramatic drop in\nperformance, but it is inconsistent across methods, changing the ranking.What\ndoes it take to focus on objects or interest and ignore background clutter when\nindexing? Do we need to train an object detector and the representation\nseparately? Do we need location supervision? We introduce Single-stage\nDetect-to-Retrieve (CiDeR), an end-to-end, single-stage pipeline to detect\nobjects of interest and extract a global image representation. We outperform\nprevious state-of-the-art on both existing training sets and the new\nRGLDv2-clean. Our dataset is available at\nhttps://github.com/dealicious-inc/RGLDv2-clean.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "CVPR2024 Accepted",
    "pdf_url": "http://arxiv.org/pdf/2404.01524v1",
    "published_date": "2024-04-01 23:11:15 UTC",
    "updated_date": "2024-04-01 23:11:15 UTC"
  },
  {
    "arxiv_id": "2404.01509v1",
    "title": "Can Biases in ImageNet Models Explain Generalization?",
    "authors": [
      "Paul Gavrikov",
      "Janis Keuper"
    ],
    "abstract": "The robust generalization of models to rare, in-distribution (ID) samples\ndrawn from the long tail of the training distribution and to\nout-of-training-distribution (OOD) samples is one of the major challenges of\ncurrent deep learning methods. For image classification, this manifests in the\nexistence of adversarial attacks, the performance drops on distorted images,\nand a lack of generalization to concepts such as sketches. The current\nunderstanding of generalization in neural networks is very limited, but some\nbiases that differentiate models from human vision have been identified and\nmight be causing these limitations. Consequently, several attempts with varying\nsuccess have been made to reduce these biases during training to improve\ngeneralization. We take a step back and sanity-check these attempts. Fixing the\narchitecture to the well-established ResNet-50, we perform a large-scale study\non 48 ImageNet models obtained via different training methods to understand how\nand if these biases - including shape bias, spectral biases, and critical bands\n- interact with generalization. Our extensive study results reveal that\ncontrary to previous findings, these biases are insufficient to accurately\npredict the generalization of a model holistically. We provide access to all\ncheckpoints and evaluation code at\nhttps://github.com/paulgavrikov/biases_vs_generalization",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG",
      "stat.ML"
    ],
    "primary_category": "cs.CV",
    "comment": "Accepted at CVPR2024",
    "pdf_url": "http://arxiv.org/pdf/2404.01509v1",
    "published_date": "2024-04-01 22:25:48 UTC",
    "updated_date": "2024-04-01 22:25:48 UTC"
  },
  {
    "arxiv_id": "2404.01503v1",
    "title": "Some Orders Are Important: Partially Preserving Orders in Top-Quality Planning",
    "authors": [
      "Michael Katz",
      "Junkyu Lee",
      "Jungkoo Kang",
      "Shirin Sohrabi"
    ],
    "abstract": "The ability to generate multiple plans is central to using planning in\nreal-life applications. Top-quality planners generate sets of such top-cost\nplans, allowing flexibility in determining equivalent ones. In terms of the\norder between actions in a plan, the literature only considers two extremes --\neither all orders are important, making each plan unique, or all orders are\nunimportant, treating two plans differing only in the order of actions as\nequivalent. To allow flexibility in selecting important orders, we propose\nspecifying a subset of actions the orders between which are important,\ninterpolating between the top-quality and unordered top-quality planning\nproblems. We explore the ways of adapting partial order reduction search\npruning techniques to address this new computational problem and present\nexperimental evaluations demonstrating the benefits of exploiting such\ntechniques in this setting.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "To appear at SoCS 2024",
    "pdf_url": "http://arxiv.org/pdf/2404.01503v1",
    "published_date": "2024-04-01 22:10:12 UTC",
    "updated_date": "2024-04-01 22:10:12 UTC"
  },
  {
    "arxiv_id": "2404.01492v3",
    "title": "Modality Translation for Object Detection Adaptation Without Forgetting Prior Knowledge",
    "authors": [
      "Heitor Rapela Medeiros",
      "Masih Aminbeidokhti",
      "Fidel Guerrero Pena",
      "David Latortue",
      "Eric Granger",
      "Marco Pedersoli"
    ],
    "abstract": "A common practice in deep learning involves training large neural networks on\nmassive datasets to achieve high accuracy across various domains and tasks.\nWhile this approach works well in many application areas, it often fails\ndrastically when processing data from a new modality with a significant\ndistribution shift from the data used to pre-train the model. This paper\nfocuses on adapting a large object detection model trained on RGB images to new\ndata extracted from IR images with a substantial modality shift. We propose\nModality Translator (ModTr) as an alternative to the common approach of\nfine-tuning a large model to the new modality. ModTr adapts the IR input image\nwith a small transformation network trained to directly minimize the detection\nloss. The original RGB model can then work on the translated inputs without any\nfurther changes or fine-tuning to its parameters. Experimental results on\ntranslating from IR to RGB images on two well-known datasets show that our\nsimple approach provides detectors that perform comparably or better than\nstandard fine-tuning, without forgetting the knowledge of the original model.\nThis opens the door to a more flexible and efficient service-based detection\npipeline, where a unique and unaltered server, such as an RGB detector, runs\nconstantly while being queried by different modalities, such as IR with the\ncorresponding translations model. Our code is available at:\nhttps://github.com/heitorrapela/ModTr.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "ECCV 2024: European Conference on Computer Vision, Milan Italy",
    "pdf_url": "http://arxiv.org/pdf/2404.01492v3",
    "published_date": "2024-04-01 21:28:50 UTC",
    "updated_date": "2024-07-31 21:50:57 UTC"
  },
  {
    "arxiv_id": "2404.01486v1",
    "title": "QuAD: Query-based Interpretable Neural Motion Planning for Autonomous Driving",
    "authors": [
      "Sourav Biswas",
      "Sergio Casas",
      "Quinlan Sykora",
      "Ben Agro",
      "Abbas Sadat",
      "Raquel Urtasun"
    ],
    "abstract": "A self-driving vehicle must understand its environment to determine the\nappropriate action. Traditional autonomy systems rely on object detection to\nfind the agents in the scene. However, object detection assumes a discrete set\nof objects and loses information about uncertainty, so any errors compound when\npredicting the future behavior of those agents. Alternatively, dense occupancy\ngrid maps have been utilized to understand free-space. However, predicting a\ngrid for the entire scene is wasteful since only certain spatio-temporal\nregions are reachable and relevant to the self-driving vehicle. We present a\nunified, interpretable, and efficient autonomy framework that moves away from\ncascading modules that first perceive, then predict, and finally plan. Instead,\nwe shift the paradigm to have the planner query occupancy at relevant\nspatio-temporal points, restricting the computation to those regions of\ninterest. Exploiting this representation, we evaluate candidate trajectories\naround key factors such as collision avoidance, comfort, and progress for\nsafety and interpretability. Our approach achieves better highway driving\nquality than the state-of-the-art in high-fidelity closed-loop simulations.",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.CV",
      "cs.LG"
    ],
    "primary_category": "cs.RO",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2404.01486v1",
    "published_date": "2024-04-01 21:11:43 UTC",
    "updated_date": "2024-04-01 21:11:43 UTC"
  },
  {
    "arxiv_id": "2404.01476v2",
    "title": "TraveLER: A Modular Multi-LMM Agent Framework for Video Question-Answering",
    "authors": [
      "Chuyi Shang",
      "Amos You",
      "Sanjay Subramanian",
      "Trevor Darrell",
      "Roei Herzig"
    ],
    "abstract": "Recently, image-based Large Multimodal Models (LMMs) have made significant\nprogress in video question-answering (VideoQA) using a frame-wise approach by\nleveraging large-scale pretraining in a zero-shot manner. Nevertheless, these\nmodels need to be capable of finding relevant information, extracting it, and\nanswering the question simultaneously. Currently, existing methods perform all\nof these steps in a single pass without being able to adapt if insufficient or\nincorrect information is collected. To overcome this, we introduce a modular\nmulti-LMM agent framework based on several agents with different roles,\ninstructed by a Planner agent that updates its instructions using shared\nfeedback from the other agents. Specifically, we propose TraveLER, a method\nthat can create a plan to \"Traverse\" through the video, ask questions about\nindividual frames to \"Locate\" and store key information, and then \"Evaluate\" if\nthere is enough information to answer the question. Finally, if there is not\nenough information, our method is able to \"Replan\" based on its collected\nknowledge. Through extensive experiments, we find that the proposed TraveLER\napproach improves performance on several VideoQA benchmarks without the need to\nfine-tune on specific datasets. Our code is available at\nhttps://github.com/traveler-framework/TraveLER.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "EMNLP 2024 (Main)",
    "pdf_url": "http://arxiv.org/pdf/2404.01476v2",
    "published_date": "2024-04-01 20:58:24 UTC",
    "updated_date": "2024-10-19 19:21:51 UTC"
  },
  {
    "arxiv_id": "2404.01475v2",
    "title": "Are large language models superhuman chemists?",
    "authors": [
      "Adrian Mirza",
      "Nawaf Alampara",
      "Sreekanth Kunchapu",
      "Martiño Ríos-García",
      "Benedict Emoekabu",
      "Aswanth Krishnan",
      "Tanya Gupta",
      "Mara Schilling-Wilhelmi",
      "Macjonathan Okereke",
      "Anagha Aneesh",
      "Amir Mohammad Elahi",
      "Mehrdad Asgari",
      "Juliane Eberhardt",
      "Hani M. Elbeheiry",
      "María Victoria Gil",
      "Maximilian Greiner",
      "Caroline T. Holick",
      "Christina Glaubitz",
      "Tim Hoffmann",
      "Abdelrahman Ibrahim",
      "Lea C. Klepsch",
      "Yannik Köster",
      "Fabian Alexander Kreth",
      "Jakob Meyer",
      "Santiago Miret",
      "Jan Matthias Peschel",
      "Michael Ringleb",
      "Nicole Roesner",
      "Johanna Schreiber",
      "Ulrich S. Schubert",
      "Leanne M. Stafast",
      "Dinga Wonanke",
      "Michael Pieler",
      "Philippe Schwaller",
      "Kevin Maik Jablonka"
    ],
    "abstract": "Large language models (LLMs) have gained widespread interest due to their\nability to process human language and perform tasks on which they have not been\nexplicitly trained.\n  However, we possess only a limited systematic understanding of the chemical\ncapabilities of LLMs, which would be required to improve models and mitigate\npotential harm. Here, we introduce \"ChemBench,\" an automated framework for\nevaluating the chemical knowledge and reasoning abilities of state-of-the-art\nLLMs against the expertise of chemists.\n  We curated more than 2,700 question-answer pairs, evaluated leading open- and\nclosed-source LLMs, and found that the best models outperformed the best human\nchemists in our study on average. However, the models struggle with some basic\ntasks and provide overconfident predictions.\n  These findings reveal LLMs' impressive chemical capabilities while\nemphasizing the need for further research to improve their safety and\nusefulness. They also suggest adapting chemistry education and show the value\nof benchmarking frameworks for evaluating LLMs in specific domains.",
    "categories": [
      "cs.LG",
      "cond-mat.mtrl-sci",
      "cs.AI",
      "physics.chem-ph"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2404.01475v2",
    "published_date": "2024-04-01 20:56:25 UTC",
    "updated_date": "2024-11-01 07:05:33 UTC"
  },
  {
    "arxiv_id": "2404.03686v1",
    "title": "Securing Social Spaces: Harnessing Deep Learning to Eradicate Cyberbullying",
    "authors": [
      "Rohan Biswas",
      "Kasturi Ganguly",
      "Arijit Das",
      "Diganta Saha"
    ],
    "abstract": "In today's digital world, cyberbullying is a serious problem that can harm\nthe mental and physical health of people who use social media. This paper\nexplains just how serious cyberbullying is and how it really affects\nindi-viduals exposed to it. It also stresses how important it is to find better\nways to detect cyberbullying so that online spaces can be safer. Plus, it talks\nabout how making more accurate tools to spot cyberbullying will be really\nhelpful in the future. Our paper introduces a deep learning-based ap-proach,\nprimarily employing BERT and BiLSTM architectures, to effective-ly address\ncyberbullying. This approach is designed to analyse large vol-umes of posts and\npredict potential instances of cyberbullying in online spaces. Our results\ndemonstrate the superiority of the hateBERT model, an extension of BERT focused\non hate speech detection, among the five mod-els, achieving an accuracy rate of\n89.16%. This research is a significant con-tribution to \"Computational\nIntelligence for Social Transformation,\" prom-ising a safer and more inclusive\ndigital landscape.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CY"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2404.03686v1",
    "published_date": "2024-04-01 20:41:28 UTC",
    "updated_date": "2024-04-01 20:41:28 UTC"
  },
  {
    "arxiv_id": "2405.14875v1",
    "title": "BloodCell-Net: A lightweight convolutional neural network for the classification of all microscopic blood cell images of the human body",
    "authors": [
      "Sohag Kumar Mondal",
      "Md. Simul Hasan Talukder",
      "Mohammad Aljaidi",
      "Rejwan Bin Sulaiman",
      "Md Mohiuddin Sarker Tushar",
      "Amjad A Alsuwaylimi"
    ],
    "abstract": "Blood cell classification and counting are vital for the diagnosis of various\nblood-related diseases, such as anemia, leukemia, and thrombocytopenia. The\nmanual process of blood cell classification and counting is time-consuming,\nprone to errors, and labor-intensive. Therefore, we have proposed a DL based\nautomated system for blood cell classification and counting from microscopic\nblood smear images. We classify total of nine types of blood cells, including\nErythrocyte, Erythroblast, Neutrophil, Basophil, Eosinophil, Lymphocyte,\nMonocyte, Immature Granulocytes, and Platelet. Several preprocessing steps like\nimage resizing, rescaling, contrast enhancement and augmentation are utilized.\nTo segment the blood cells from the entire microscopic images, we employed the\nU-Net model. This segmentation technique aids in extracting the region of\ninterest (ROI) by removing complex and noisy background elements. Both\npixel-level metrics such as accuracy, precision, and sensitivity, and\nobject-level evaluation metrics like Intersection over Union (IOU) and Dice\ncoefficient are considered to comprehensively evaluate the performance of the\nU-Net model. The segmentation model achieved impressive performance metrics,\nincluding 98.23% accuracy, 98.40% precision, 98.25% sensitivity, 95.97%\nIntersection over Union (IOU), and 97.92% Dice coefficient. Subsequently, a\nwatershed algorithm is applied to the segmented images to separate overlapped\nblood cells and extract individual cells. We have proposed a BloodCell-Net\napproach incorporated with custom light weight convolutional neural network\n(LWCNN) for classifying individual blood cells into nine types. Comprehensive\nevaluation of the classifier's performance is conducted using metrics including\naccuracy, precision, recall, and F1 score. The classifier achieved an average\naccuracy of 97.10%, precision of 97.19%, recall of 97.01%, and F1 score of\n97.10%.",
    "categories": [
      "eess.IV",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "eess.IV",
    "comment": "24 pages, 7 tables and 13 Figures",
    "pdf_url": "http://arxiv.org/pdf/2405.14875v1",
    "published_date": "2024-04-01 20:38:58 UTC",
    "updated_date": "2024-04-01 20:38:58 UTC"
  },
  {
    "arxiv_id": "2404.01464v1",
    "title": "Data-Efficient Unsupervised Interpolation Without Any Intermediate Frame for 4D Medical Images",
    "authors": [
      "JungEun Kim",
      "Hangyul Yoon",
      "Geondo Park",
      "Kyungsu Kim",
      "Eunho Yang"
    ],
    "abstract": "4D medical images, which represent 3D images with temporal information, are\ncrucial in clinical practice for capturing dynamic changes and monitoring\nlong-term disease progression. However, acquiring 4D medical images poses\nchallenges due to factors such as radiation exposure and imaging duration,\nnecessitating a balance between achieving high temporal resolution and\nminimizing adverse effects. Given these circumstances, not only is data\nacquisition challenging, but increasing the frame rate for each dataset also\nproves difficult. To address this challenge, this paper proposes a simple yet\neffective Unsupervised Volumetric Interpolation framework, UVI-Net. This\nframework facilitates temporal interpolation without the need for any\nintermediate frames, distinguishing it from the majority of other existing\nunsupervised methods. Experiments on benchmark datasets demonstrate significant\nimprovements across diverse evaluation metrics compared to unsupervised and\nsupervised baselines. Remarkably, our approach achieves this superior\nperformance even when trained with a dataset as small as one, highlighting its\nexceptional robustness and efficiency in scenarios with sparse supervision.\nThis positions UVI-Net as a compelling alternative for 4D medical imaging,\nparticularly in settings where data availability is limited. The source code is\navailable at https://github.com/jungeun122333/UVI-Net.",
    "categories": [
      "eess.IV",
      "cs.AI",
      "cs.CV",
      "cs.LG"
    ],
    "primary_category": "eess.IV",
    "comment": "CVPR 2024",
    "pdf_url": "http://arxiv.org/pdf/2404.01464v1",
    "published_date": "2024-04-01 20:25:04 UTC",
    "updated_date": "2024-04-01 20:25:04 UTC"
  },
  {
    "arxiv_id": "2404.01459v1",
    "title": "Game-Theoretic Deep Reinforcement Learning to Minimize Carbon Emissions and Energy Costs for AI Inference Workloads in Geo-Distributed Data Centers",
    "authors": [
      "Ninad Hogade",
      "Sudeep Pasricha"
    ],
    "abstract": "Data centers are increasingly using more energy due to the rise in Artificial\nIntelligence (AI) workloads, which negatively impacts the environment and\nraises operational costs. Reducing operating expenses and carbon emissions\nwhile maintaining performance in data centers is a challenging problem. This\nwork introduces a unique approach combining Game Theory (GT) and Deep\nReinforcement Learning (DRL) for optimizing the distribution of AI inference\nworkloads in geo-distributed data centers to reduce carbon emissions and cloud\noperating (energy + data transfer) costs. The proposed technique integrates the\nprinciples of non-cooperative Game Theory into a DRL framework, enabling data\ncenters to make intelligent decisions regarding workload allocation while\nconsidering the heterogeneity of hardware resources, the dynamic nature of\nelectricity prices, inter-data center data transfer costs, and carbon\nfootprints. We conducted extensive experiments comparing our game-theoretic DRL\n(GT-DRL) approach with current DRL-based and other optimization techniques. The\nresults demonstrate that our strategy outperforms the state-of-the-art in\nreducing carbon emissions and minimizing cloud operating costs without\ncompromising computational performance. This work has significant implications\nfor achieving sustainability and cost-efficiency in data centers handling AI\ninference workloads across diverse geographic locations.",
    "categories": [
      "cs.DC",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.DC",
    "comment": "arXiv admin note: text overlap with arXiv:2106.00066",
    "pdf_url": "http://arxiv.org/pdf/2404.01459v1",
    "published_date": "2024-04-01 20:13:28 UTC",
    "updated_date": "2024-04-01 20:13:28 UTC"
  },
  {
    "arxiv_id": "2404.01453v1",
    "title": "Unveiling Divergent Inductive Biases of LLMs on Temporal Data",
    "authors": [
      "Sindhu Kishore",
      "Hangfeng He"
    ],
    "abstract": "Unraveling the intricate details of events in natural language necessitates a\nsubtle understanding of temporal dynamics. Despite the adeptness of Large\nLanguage Models (LLMs) in discerning patterns and relationships from data,\ntheir inherent comprehension of temporal dynamics remains a formidable\nchallenge. This research meticulously explores these intrinsic challenges\nwithin LLMs, with a specific emphasis on evaluating the performance of GPT-3.5\nand GPT-4 models in the analysis of temporal data. Employing two distinct\nprompt types, namely Question Answering (QA) format and Textual Entailment (TE)\nformat, our analysis probes into both implicit and explicit events. The\nfindings underscore noteworthy trends, revealing disparities in the performance\nof GPT-3.5 and GPT-4. Notably, biases toward specific temporal relationships\ncome to light, with GPT-3.5 demonstrating a preference for \"AFTER'' in the QA\nformat for both implicit and explicit events, while GPT-4 leans towards\n\"BEFORE''. Furthermore, a consistent pattern surfaces wherein GPT-3.5 tends\ntowards \"TRUE'', and GPT-4 exhibits a preference for \"FALSE'' in the TE format\nfor both implicit and explicit events. This persistent discrepancy between\nGPT-3.5 and GPT-4 in handling temporal data highlights the intricate nature of\ninductive bias in LLMs, suggesting that the evolution of these models may not\nmerely mitigate bias but may introduce new layers of complexity.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2404.01453v1",
    "published_date": "2024-04-01 19:56:41 UTC",
    "updated_date": "2024-04-01 19:56:41 UTC"
  },
  {
    "arxiv_id": "2405.19338v1",
    "title": "Accurate Patient Alignment without Unnecessary Imaging Dose via Synthesizing Patient-specific 3D CT Images from 2D kV Images",
    "authors": [
      "Yuzhen Ding",
      "Jason M. Holmes",
      "Hongying Feng",
      "Baoxin Li",
      "Lisa A. McGee",
      "Jean-Claude M. Rwigema",
      "Sujay A. Vora",
      "Daniel J. Ma",
      "Robert L. Foote",
      "Samir H. Patel",
      "Wei Liu"
    ],
    "abstract": "In radiotherapy, 2D orthogonally projected kV images are used for patient\nalignment when 3D-on-board imaging(OBI) unavailable. But tumor visibility is\nconstrained due to the projection of patient's anatomy onto a 2D plane,\npotentially leading to substantial setup errors. In treatment room with 3D-OBI\nsuch as cone beam CT(CBCT), the field of view(FOV) of CBCT is limited with\nunnecessarily high imaging dose, thus unfavorable for pediatric patients. A\nsolution to this dilemma is to reconstruct 3D CT from kV images obtained at the\ntreatment position. Here, we propose a dual-models framework built with\nhierarchical ViT blocks. Unlike a proof-of-concept approach, our framework\nconsiders kV images as the solo input and can synthesize accurate, full-size 3D\nCT in real time(within milliseconds). We demonstrate the feasibility of the\nproposed approach on 10 patients with head and neck (H&N) cancer using image\nquality(MAE: <45HU), dosimetrical accuracy(Gamma passing rate (2%/2mm/10%)>97%)\nand patient position uncertainty(shift error: <0.4mm). The proposed framework\ncan generate accurate 3D CT faithfully mirroring real-time patient position,\nthus significantly improving patient setup accuracy, keeping imaging dose\nminimum, and maintaining treatment veracity.",
    "categories": [
      "eess.SP",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "eess.SP",
    "comment": "17 pages, 8 figures and tables",
    "pdf_url": "http://arxiv.org/pdf/2405.19338v1",
    "published_date": "2024-04-01 19:55:03 UTC",
    "updated_date": "2024-04-01 19:55:03 UTC"
  },
  {
    "arxiv_id": "2404.02176v1",
    "title": "Versatile Navigation under Partial Observability via Value-guided Diffusion Policy",
    "authors": [
      "Gengyu Zhang",
      "Hao Tang",
      "Yan Yan"
    ],
    "abstract": "Route planning for navigation under partial observability plays a crucial\nrole in modern robotics and autonomous driving. Existing route planning\napproaches can be categorized into two main classes: traditional autoregressive\nand diffusion-based methods. The former often fails due to its myopic nature,\nwhile the latter either assumes full observability or struggles to adapt to\nunfamiliar scenarios, due to strong couplings with behavior cloning from\nexperts. To address these deficiencies, we propose a versatile diffusion-based\napproach for both 2D and 3D route planning under partial observability.\nSpecifically, our value-guided diffusion policy first generates plans to\npredict actions across various timesteps, providing ample foresight to the\nplanning. It then employs a differentiable planner with state estimations to\nderive a value function, directing the agent's exploration and goal-seeking\nbehaviors without seeking experts while explicitly addressing partial\nobservability. During inference, our policy is further enhanced by a\nbest-plan-selection strategy, substantially boosting the planning success rate.\nMoreover, we propose projecting point clouds, derived from RGB-D inputs, onto\n2D grid-based bird-eye-view maps via semantic segmentation, generalizing to 3D\nenvironments. This simple yet effective adaption enables zero-shot transfer\nfrom 2D-trained policy to 3D, cutting across the laborious training for 3D\npolicy, and thus certifying our versatility. Experimental results demonstrate\nour superior performance, particularly in navigating situations beyond expert\ndemonstrations, surpassing state-of-the-art autoregressive and diffusion-based\nbaselines for both 2D and 3D scenarios.",
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "primary_category": "cs.RO",
    "comment": "13 pages, 7 figures, CVPR 2024",
    "pdf_url": "http://arxiv.org/pdf/2404.02176v1",
    "published_date": "2024-04-01 19:52:08 UTC",
    "updated_date": "2024-04-01 19:52:08 UTC"
  },
  {
    "arxiv_id": "2404.01446v2",
    "title": "Finding Regions of Interest in Whole Slide Images Using Multiple Instance Learning",
    "authors": [
      "Martim Afonso",
      "Praphulla M. S. Bhawsar",
      "Monjoy Saha",
      "Jonas S. Almeida",
      "Arlindo L. Oliveira"
    ],
    "abstract": "Whole Slide Images (WSI), obtained by high-resolution digital scanning of\nmicroscope slides at multiple scales, are the cornerstone of modern Digital\nPathology. However, they represent a particular challenge to\nAI-based/AI-mediated analysis because pathology labeling is typically done at\nslide-level, instead of tile-level. It is not just that medical diagnostics is\nrecorded at the specimen level, the detection of oncogene mutation is also\nexperimentally obtained, and recorded by initiatives like The Cancer Genome\nAtlas (TCGA), at the slide level. This configures a dual challenge: a)\naccurately predicting the overall cancer phenotype and b) finding out what\ncellular morphologies are associated with it at the tile level. To address\nthese challenges, a weakly supervised Multiple Instance Learning (MIL) approach\nwas explored for two prevalent cancer types, Invasive Breast Carcinoma\n(TCGA-BRCA) and Lung Squamous Cell Carcinoma (TCGA-LUSC). This approach was\nexplored for tumor detection at low magnification levels and TP53 mutations at\nvarious levels. Our results show that a novel additive implementation of MIL\nmatched the performance of reference implementation (AUC 0.96), and was only\nslightly outperformed by Attention MIL (AUC 0.97). More interestingly from the\nperspective of the molecular pathologist, these different AI architectures\nidentify distinct sensitivities to morphological features (through the\ndetection of Regions of Interest, RoI) at different amplification levels.\nTellingly, TP53 mutation was most sensitive to features at the higher\napplications where cellular morphology is resolved.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2404.01446v2",
    "published_date": "2024-04-01 19:33:41 UTC",
    "updated_date": "2024-04-11 06:58:18 UTC"
  },
  {
    "arxiv_id": "2404.01440v2",
    "title": "Neural Implicit Representation for Building Digital Twins of Unknown Articulated Objects",
    "authors": [
      "Yijia Weng",
      "Bowen Wen",
      "Jonathan Tremblay",
      "Valts Blukis",
      "Dieter Fox",
      "Leonidas Guibas",
      "Stan Birchfield"
    ],
    "abstract": "We address the problem of building digital twins of unknown articulated\nobjects from two RGBD scans of the object at different articulation states. We\ndecompose the problem into two stages, each addressing distinct aspects. Our\nmethod first reconstructs object-level shape at each state, then recovers the\nunderlying articulation model including part segmentation and joint\narticulations that associate the two states. By explicitly modeling point-level\ncorrespondences and exploiting cues from images, 3D reconstructions, and\nkinematics, our method yields more accurate and stable results compared to\nprior work. It also handles more than one movable part and does not rely on any\nobject shape or structure priors. Project page:\nhttps://github.com/NVlabs/DigitalTwinArt",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.GR",
      "cs.RO"
    ],
    "primary_category": "cs.CV",
    "comment": "CVPR 2024",
    "pdf_url": "http://arxiv.org/pdf/2404.01440v2",
    "published_date": "2024-04-01 19:23:00 UTC",
    "updated_date": "2024-06-06 23:20:55 UTC"
  },
  {
    "arxiv_id": "2404.01439v1",
    "title": "Creating emoji lexica from unsupervised sentiment analysis of their descriptions",
    "authors": [
      "Milagros Fernández-Gavilanes",
      "Jonathan Juncal-Martínez",
      "Silvia García-Méndez",
      "Enrique Costa-Montenegro",
      "Francisco Javier González-Castaño"
    ],
    "abstract": "Online media, such as blogs and social networking sites, generate massive\nvolumes of unstructured data of great interest to analyze the opinions and\nsentiments of individuals and organizations. Novel approaches beyond Natural\nLanguage Processing are necessary to quantify these opinions with polarity\nmetrics. So far, the sentiment expressed by emojis has received little\nattention. The use of symbols, however, has boomed in the past four years.\nAbout twenty billion are typed in Twitter nowadays, and new emojis keep\nappearing in each new Unicode version, making them increasingly relevant to\nsentiment analysis tasks. This has motivated us to propose a novel approach to\npredict the sentiments expressed by emojis in online textual messages, such as\ntweets, that does not require human effort to manually annotate data and saves\nvaluable time for other analysis tasks. For this purpose, we automatically\nconstructed a novel emoji sentiment lexicon using an unsupervised sentiment\nanalysis system based on the definitions given by emoji creators in Emojipedia.\nAdditionally, we automatically created lexicon variants by also considering the\nsentiment distribution of the informal texts accompanying emojis. All these\nlexica are evaluated and compared regarding the improvement obtained by\nincluding them in sentiment analysis of the annotated datasets provided by\nKralj Novak et al. (2015). The results confirm the competitiveness of our\napproach.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2404.01439v1",
    "published_date": "2024-04-01 19:22:58 UTC",
    "updated_date": "2024-04-01 19:22:58 UTC"
  },
  {
    "arxiv_id": "2404.01438v2",
    "title": "Generation and Detection of Sign Language Deepfakes - A Linguistic and Visual Analysis",
    "authors": [
      "Shahzeb Naeem",
      "Muhammad Riyyan Khan",
      "Usman Tariq",
      "Abhinav Dhall",
      "Carlos Ivan Colon",
      "Hasan Al-Nashash"
    ],
    "abstract": "This research explores the positive application of deepfake technology for\nupper body generation, specifically sign language for the Deaf and Hard of\nHearing (DHoH) community. Given the complexity of sign language and the\nscarcity of experts, the generated videos are vetted by a sign language expert\nfor accuracy. We construct a reliable deepfake dataset, evaluating its\ntechnical and visual credibility using computer vision and natural language\nprocessing models. The dataset, consisting of over 1200 videos featuring both\nseen and unseen individuals, is also used to detect deepfake videos targeting\nvulnerable individuals. Expert annotations confirm that the generated videos\nare comparable to real sign language content. Linguistic analysis, using\ntextual similarity scores and interpreter evaluations, shows that the\ninterpretation of generated videos is at least 90% similar to authentic sign\nlanguage. Visual analysis demonstrates that convincingly realistic deepfakes\ncan be produced, even for new subjects. Using a pose/style transfer model, we\npay close attention to detail, ensuring hand movements are accurate and align\nwith the driving video. We also apply machine learning algorithms to establish\na baseline for deepfake detection on this dataset, contributing to the\ndetection of fraudulent sign language videos.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "10 pages, 11 figures, IEEE TRANSACTIONS ON COMPUTATIONAL SOCIAL\n  SYSTEM",
    "pdf_url": "http://arxiv.org/pdf/2404.01438v2",
    "published_date": "2024-04-01 19:22:43 UTC",
    "updated_date": "2025-02-17 18:22:03 UTC"
  },
  {
    "arxiv_id": "2404.01430v1",
    "title": "Position-Aware Parameter Efficient Fine-Tuning Approach for Reducing Positional Bias in LLMs",
    "authors": [
      "Zheng Zhang",
      "Fan Yang",
      "Ziyan Jiang",
      "Zheng Chen",
      "Zhengyang Zhao",
      "Chengyuan Ma",
      "Liang Zhao",
      "Yang Liu"
    ],
    "abstract": "Recent advances in large language models (LLMs) have enhanced their ability\nto process long input contexts. This development is particularly crucial for\ntasks that involve retrieving knowledge from an external datastore, which can\nresult in long inputs. However, recent studies show a positional bias in LLMs,\ndemonstrating varying performance depending on the location of useful\ninformation within the input sequence. In this study, we conduct extensive\nexperiments to investigate the root causes of positional bias. Our findings\nindicate that the primary contributor to LLM positional bias stems from the\ninherent positional preferences of different models. We demonstrate that merely\nemploying prompt-based solutions is inadequate for overcoming the positional\npreferences. To address this positional bias issue of a pre-trained LLM, we\ndeveloped a Position-Aware Parameter Efficient Fine-Tuning (PAPEFT) approach\nwhich is composed of a data augmentation technique and a parameter efficient\nadapter, enhancing a uniform attention distribution across the input context.\nOur experiments demonstrate that the proposed approach effectively reduces\npositional bias, improving LLMs' effectiveness in handling long context\nsequences for various tasks that require externally retrieved knowledge.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2404.01430v1",
    "published_date": "2024-04-01 19:04:17 UTC",
    "updated_date": "2024-04-01 19:04:17 UTC"
  },
  {
    "arxiv_id": "2404.01413v2",
    "title": "Is Model Collapse Inevitable? Breaking the Curse of Recursion by Accumulating Real and Synthetic Data",
    "authors": [
      "Matthias Gerstgrasser",
      "Rylan Schaeffer",
      "Apratim Dey",
      "Rafael Rafailov",
      "Henry Sleight",
      "John Hughes",
      "Tomasz Korbak",
      "Rajashree Agrawal",
      "Dhruv Pai",
      "Andrey Gromov",
      "Daniel A. Roberts",
      "Diyi Yang",
      "David L. Donoho",
      "Sanmi Koyejo"
    ],
    "abstract": "The proliferation of generative models, combined with pretraining on\nweb-scale data, raises a timely question: what happens when these models are\ntrained on their own generated outputs? Recent investigations into model-data\nfeedback loops proposed that such loops would lead to a phenomenon termed model\ncollapse, under which performance progressively degrades with each model-data\nfeedback iteration until fitted models become useless. However, those studies\nlargely assumed that new data replace old data over time, where an arguably\nmore realistic assumption is that data accumulate over time. In this paper, we\nask: what effect does accumulating data have on model collapse? We empirically\nstudy this question by pretraining sequences of language models on text\ncorpora. We confirm that replacing the original real data by each generation's\nsynthetic data does indeed tend towards model collapse, then demonstrate that\naccumulating the successive generations of synthetic data alongside the\noriginal real data avoids model collapse; these results hold across a range of\nmodel sizes, architectures, and hyperparameters. We obtain similar results for\ndeep generative models on other types of real data: diffusion models for\nmolecule conformation generation and variational autoencoders for image\ngeneration. To understand why accumulating data can avoid model collapse, we\nuse an analytically tractable framework introduced by prior work in which a\nsequence of linear models are fit to the previous models' outputs. Previous\nwork used this framework to show that if data are replaced, the test error\nincreases with the number of model-fitting iterations; we extend this argument\nto prove that if data instead accumulate, the test error has a finite upper\nbound independent of the number of iterations, meaning model collapse no longer\noccurs.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL",
      "cs.ET",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2404.01413v2",
    "published_date": "2024-04-01 18:31:24 UTC",
    "updated_date": "2024-04-29 23:13:42 UTC"
  },
  {
    "arxiv_id": "2404.01409v1",
    "title": "OVFoodSeg: Elevating Open-Vocabulary Food Image Segmentation via Image-Informed Textual Representation",
    "authors": [
      "Xiongwei Wu",
      "Sicheng Yu",
      "Ee-Peng Lim",
      "Chong-Wah Ngo"
    ],
    "abstract": "In the realm of food computing, segmenting ingredients from images poses\nsubstantial challenges due to the large intra-class variance among the same\ningredients, the emergence of new ingredients, and the high annotation costs\nassociated with large food segmentation datasets. Existing approaches primarily\nutilize a closed-vocabulary and static text embeddings setting. These methods\noften fall short in effectively handling the ingredients, particularly new and\ndiverse ones. In response to these limitations, we introduce OVFoodSeg, a\nframework that adopts an open-vocabulary setting and enhances text embeddings\nwith visual context. By integrating vision-language models (VLMs), our approach\nenriches text embedding with image-specific information through two innovative\nmodules, eg, an image-to-text learner FoodLearner and an Image-Informed Text\nEncoder. The training process of OVFoodSeg is divided into two stages: the\npre-training of FoodLearner and the subsequent learning phase for segmentation.\nThe pre-training phase equips FoodLearner with the capability to align visual\ninformation with corresponding textual representations that are specifically\nrelated to food, while the second phase adapts both the FoodLearner and the\nImage-Informed Text Encoder for the segmentation task. By addressing the\ndeficiencies of previous models, OVFoodSeg demonstrates a significant\nimprovement, achieving an 4.9\\% increase in mean Intersection over Union (mIoU)\non the FoodSeg103 dataset, setting a new milestone for food image segmentation.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.MM"
    ],
    "primary_category": "cs.CV",
    "comment": "CVPR 2024; 12 pages",
    "pdf_url": "http://arxiv.org/pdf/2404.01409v1",
    "published_date": "2024-04-01 18:26:29 UTC",
    "updated_date": "2024-04-01 18:26:29 UTC"
  },
  {
    "arxiv_id": "2404.01402v2",
    "title": "ContactHandover: Contact-Guided Robot-to-Human Object Handover",
    "authors": [
      "Zixi Wang",
      "Zeyi Liu",
      "Nicolas Ouporov",
      "Shuran Song"
    ],
    "abstract": "Robot-to-human object handover is an important step in many human robot\ncollaboration tasks. A successful handover requires the robot to maintain a\nstable grasp on the object while making sure the human receives the object in a\nnatural and easy-to-use manner. We propose ContactHandover, a robot to human\nhandover system that consists of two phases: a contact-guided grasping phase\nand an object delivery phase. During the grasping phase, ContactHandover\npredicts both 6-DoF robot grasp poses and a 3D affordance map of human contact\npoints on the object. The robot grasp poses are re-ranked by penalizing those\nthat block human contact points, and the robot executes the highest ranking\ngrasp. During the delivery phase, the robot end effector pose is computed by\nmaximizing human contact points close to the human while minimizing the human\narm joint torques and displacements. We evaluate our system on 27 diverse\nhousehold objects and show that our system achieves better visibility and\nreachability of human contacts to the receiver compared to several baselines.\nMore results can be found on\nhttps://clairezixiwang.github.io/ContactHandover.github.io",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.RO",
    "comment": "Accepted to IROS 2024. Project website:\n  https://clairezixiwang.github.io/ContactHandover.github.io/",
    "pdf_url": "http://arxiv.org/pdf/2404.01402v2",
    "published_date": "2024-04-01 18:12:09 UTC",
    "updated_date": "2024-09-30 07:34:17 UTC"
  },
  {
    "arxiv_id": "2404.01397v1",
    "title": "Object-conditioned Bag of Instances for Few-Shot Personalized Instance Recognition",
    "authors": [
      "Umberto Michieli",
      "Jijoong Moon",
      "Daehyun Kim",
      "Mete Ozay"
    ],
    "abstract": "Nowadays, users demand for increased personalization of vision systems to\nlocalize and identify personal instances of objects (e.g., my dog rather than\ndog) from a few-shot dataset only. Despite outstanding results of deep networks\non classical label-abundant benchmarks (e.g., those of the latest YOLOv8 model\nfor standard object detection), they struggle to maintain within-class\nvariability to represent different instances rather than object categories\nonly. We construct an Object-conditioned Bag of Instances (OBoI) based on\nmulti-order statistics of extracted features, where generic object detection\nmodels are extended to search and identify personal instances from the OBoI's\nmetric space, without need for backpropagation. By relying on multi-order\nstatistics, OBoI achieves consistent superior accuracy in distinguishing\ndifferent instances. In the results, we achieve 77.1% personal object\nrecognition accuracy in case of 18 personal instances, showing about 12%\nrelative gain over the state of the art.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.RO"
    ],
    "primary_category": "cs.CV",
    "comment": "ICASSP 2024. Copyright 2024 IEEE. Personal use of this material is\n  permitted. Permission from IEEE must be obtained for all other uses, in any\n  current or future media, including reprinting/republishing this material for\n  advertising or promotional purposes, creating new collective works, for\n  resale or redistribution to servers or lists, or reuse of any copyrighted\n  component of this work in other",
    "pdf_url": "http://arxiv.org/pdf/2404.01397v1",
    "published_date": "2024-04-01 18:08:58 UTC",
    "updated_date": "2024-04-01 18:08:58 UTC"
  },
  {
    "arxiv_id": "2404.01300v3",
    "title": "NeRF-MAE: Masked AutoEncoders for Self-Supervised 3D Representation Learning for Neural Radiance Fields",
    "authors": [
      "Muhammad Zubair Irshad",
      "Sergey Zakharov",
      "Vitor Guizilini",
      "Adrien Gaidon",
      "Zsolt Kira",
      "Rares Ambrus"
    ],
    "abstract": "Neural fields excel in computer vision and robotics due to their ability to\nunderstand the 3D visual world such as inferring semantics, geometry, and\ndynamics. Given the capabilities of neural fields in densely representing a 3D\nscene from 2D images, we ask the question: Can we scale their self-supervised\npretraining, specifically using masked autoencoders, to generate effective 3D\nrepresentations from posed RGB images. Owing to the astounding success of\nextending transformers to novel data modalities, we employ standard 3D Vision\nTransformers to suit the unique formulation of NeRFs. We leverage NeRF's\nvolumetric grid as a dense input to the transformer, contrasting it with other\n3D representations such as pointclouds where the information density can be\nuneven, and the representation is irregular. Due to the difficulty of applying\nmasked autoencoders to an implicit representation, such as NeRF, we opt for\nextracting an explicit representation that canonicalizes scenes across domains\nby employing the camera trajectory for sampling. Our goal is made possible by\nmasking random patches from NeRF's radiance and density grid and employing a\nstandard 3D Swin Transformer to reconstruct the masked patches. In doing so,\nthe model can learn the semantic and spatial structure of complete scenes. We\npretrain this representation at scale on our proposed curated posed-RGB data,\ntotaling over 1.8 million images. Once pretrained, the encoder is used for\neffective 3D transfer learning. Our novel self-supervised pretraining for\nNeRFs, NeRF-MAE, scales remarkably well and improves performance on various\nchallenging 3D tasks. Utilizing unlabeled posed 2D data for pretraining,\nNeRF-MAE significantly outperforms self-supervised 3D pretraining and NeRF\nscene understanding baselines on Front3D and ScanNet datasets with an absolute\nperformance improvement of over 20% AP50 and 8% AP25 for 3D object detection.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "Accepted to ECCV 2024. Project Page: https://nerf-mae.github.io/",
    "pdf_url": "http://arxiv.org/pdf/2404.01300v3",
    "published_date": "2024-04-01 17:59:55 UTC",
    "updated_date": "2024-07-18 17:59:48 UTC"
  },
  {
    "arxiv_id": "2404.01299v2",
    "title": "CausalChaos! Dataset for Comprehensive Causal Action Question Answering Over Longer Causal Chains Grounded in Dynamic Visual Scenes",
    "authors": [
      "Paritosh Parmar",
      "Eric Peh",
      "Ruirui Chen",
      "Ting En Lam",
      "Yuhan Chen",
      "Elston Tan",
      "Basura Fernando"
    ],
    "abstract": "Causal video question answering (QA) has garnered increasing interest, yet\nexisting datasets often lack depth in causal reasoning. To address this gap, we\ncapitalize on the unique properties of cartoons and construct CausalChaos!, a\nnovel, challenging causal Why-QA dataset built upon the iconic \"Tom and Jerry\"\ncartoon series. Cartoons use the principles of animation that allow animators\nto create expressive, unambiguous causal relationships between events to form a\ncoherent storyline. Utilizing these properties, along with thought-provoking\nquestions and multi-level answers (answer and detailed causal explanation), our\nquestions involve causal chains that interconnect multiple dynamic interactions\nbetween characters and visual scenes. These factors demand models to solve more\nchallenging, yet well-defined causal relationships. We also introduce hard\nincorrect answer mining, including a causally confusing version that is even\nmore challenging. While models perform well, there is much room for\nimprovement, especially, on open-ended answers. We identify more\nadvanced/explicit causal relationship modeling & joint modeling of vision and\nlanguage as the immediate areas for future efforts to focus upon. Along with\nthe other complementary datasets, our new challenging dataset will pave the way\nfor these developments in the field.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "Project Page: https://github.com/LUNAProject22/CausalChaos",
    "pdf_url": "http://arxiv.org/pdf/2404.01299v2",
    "published_date": "2024-04-01 17:59:53 UTC",
    "updated_date": "2024-06-14 17:46:02 UTC"
  },
  {
    "arxiv_id": "2404.01295v1",
    "title": "Towards Safety and Helpfulness Balanced Responses via Controllable Large Language Models",
    "authors": [
      "Yi-Lin Tuan",
      "Xilun Chen",
      "Eric Michael Smith",
      "Louis Martin",
      "Soumya Batra",
      "Asli Celikyilmaz",
      "William Yang Wang",
      "Daniel M. Bikel"
    ],
    "abstract": "As large language models (LLMs) become easily accessible nowadays, the\ntrade-off between safety and helpfulness can significantly impact user\nexperience. A model that prioritizes safety will cause users to feel less\nengaged and assisted while prioritizing helpfulness will potentially cause\nharm. Possible harms include teaching people how to build a bomb, exposing\nyouth to inappropriate content, and hurting users' mental health. In this work,\nwe propose to balance safety and helpfulness in diverse use cases by\ncontrolling both attributes in LLM. We explore training-free and fine-tuning\nmethods that do not require extra human annotations and analyze the challenges\nof controlling safety and helpfulness in LLMs. Our experiments demonstrate that\nour method can rewind a learned model and unlock its controllability.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2404.01295v1",
    "published_date": "2024-04-01 17:59:06 UTC",
    "updated_date": "2024-04-01 17:59:06 UTC"
  },
  {
    "arxiv_id": "2404.01291v2",
    "title": "Evaluating Text-to-Visual Generation with Image-to-Text Generation",
    "authors": [
      "Zhiqiu Lin",
      "Deepak Pathak",
      "Baiqi Li",
      "Jiayao Li",
      "Xide Xia",
      "Graham Neubig",
      "Pengchuan Zhang",
      "Deva Ramanan"
    ],
    "abstract": "Despite significant progress in generative AI, comprehensive evaluation\nremains challenging because of the lack of effective metrics and standardized\nbenchmarks. For instance, the widely-used CLIPScore measures the alignment\nbetween a (generated) image and text prompt, but it fails to produce reliable\nscores for complex prompts involving compositions of objects, attributes, and\nrelations. One reason is that text encoders of CLIP can notoriously act as a\n\"bag of words\", conflating prompts such as \"the horse is eating the grass\" with\n\"the grass is eating the horse\". To address this, we introduce the VQAScore,\nwhich uses a visual-question-answering (VQA) model to produce an alignment\nscore by computing the probability of a \"Yes\" answer to a simple \"Does this\nfigure show '{text}'?\" question. Though simpler than prior art, VQAScore\ncomputed with off-the-shelf models produces state-of-the-art results across\nmany (8) image-text alignment benchmarks. We also compute VQAScore with an\nin-house model that follows best practices in the literature. For example, we\nuse a bidirectional image-question encoder that allows image embeddings to\ndepend on the question being asked (and vice versa). Our in-house model,\nCLIP-FlanT5, outperforms even the strongest baselines that make use of the\nproprietary GPT-4V. Interestingly, although we train with only images, VQAScore\ncan also align text with video and 3D models. VQAScore allows researchers to\nbenchmark text-to-visual generation using complex texts that capture the\ncompositional structure of real-world prompts. We introduce GenAI-Bench, a more\nchallenging benchmark with 1,600 compositional text prompts that require\nparsing scenes, objects, attributes, relationships, and high-order reasoning\nlike comparison and logic. GenAI-Bench also offers over 15,000 human ratings\nfor leading image and video generation models such as Stable Diffusion, DALL-E\n3, and Gen2.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL",
      "cs.LG",
      "cs.MM"
    ],
    "primary_category": "cs.CV",
    "comment": "We open-source our data, model, and code at:\n  https://github.com/linzhiqiu/t2v_metrics ; Project page:\n  https://linzhiqiu.github.io/papers/vqascore",
    "pdf_url": "http://arxiv.org/pdf/2404.01291v2",
    "published_date": "2024-04-01 17:58:06 UTC",
    "updated_date": "2024-06-18 07:09:55 UTC"
  },
  {
    "arxiv_id": "2404.01365v3",
    "title": "Prompt-prompted Adaptive Structured Pruning for Efficient LLM Generation",
    "authors": [
      "Harry Dong",
      "Beidi Chen",
      "Yuejie Chi"
    ],
    "abstract": "With the development of transformer-based large language models (LLMs), they\nhave been applied to many fields due to their remarkable utility, but this\ncomes at a considerable computational cost at deployment. Fortunately, some\nmethods such as pruning or constructing a mixture of experts (MoE) aim at\nexploiting sparsity in transformer feedforward (FF) blocks to gain boosts in\nspeed and reduction in memory requirements. However, these techniques can be\nvery costly and inflexible in practice, as they often require training or are\nrestricted to specific types of architectures. To address this, we introduce\nGRIFFIN, a novel training-free and calibration-free method that selects unique\nFF experts at the sequence level for efficient generation across a plethora of\nLLMs with different non-ReLU activation functions. This is possible due to a\ncritical observation that many trained LLMs naturally produce highly structured\nFF activation patterns within a sequence, which we call flocking. Despite our\nmethod's simplicity, we show with 50% of the FF parameters, GRIFFIN maintains\nthe original model's performance with little to no degradation on a variety of\nclassification and generation tasks, all while improving latency (e.g.\n1.29$\\times$ and 1.25$\\times$ speed-ups in Gemma 7B and Llama 2 13B,\nrespectively, on an NVIDIA L40). Code is available at\nhttps://github.com/hdong920/GRIFFIN.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.LG",
    "comment": "Revision 1: Updated abstract with code link; re-ran top-k + sampling\n  rows in Table 4, conclusions unchanged Revision 2: Reframing and new\n  experiments, conclusions unchanged",
    "pdf_url": "http://arxiv.org/pdf/2404.01365v3",
    "published_date": "2024-04-01 17:56:06 UTC",
    "updated_date": "2024-08-11 19:43:36 UTC"
  },
  {
    "arxiv_id": "2404.01268v1",
    "title": "Mapping the Increasing Use of LLMs in Scientific Papers",
    "authors": [
      "Weixin Liang",
      "Yaohui Zhang",
      "Zhengxuan Wu",
      "Haley Lepp",
      "Wenlong Ji",
      "Xuandong Zhao",
      "Hancheng Cao",
      "Sheng Liu",
      "Siyu He",
      "Zhi Huang",
      "Diyi Yang",
      "Christopher Potts",
      "Christopher D Manning",
      "James Y. Zou"
    ],
    "abstract": "Scientific publishing lays the foundation of science by disseminating\nresearch findings, fostering collaboration, encouraging reproducibility, and\nensuring that scientific knowledge is accessible, verifiable, and built upon\nover time. Recently, there has been immense speculation about how many people\nare using large language models (LLMs) like ChatGPT in their academic writing,\nand to what extent this tool might have an effect on global scientific\npractices. However, we lack a precise measure of the proportion of academic\nwriting substantially modified or produced by LLMs. To address this gap, we\nconduct the first systematic, large-scale analysis across 950,965 papers\npublished between January 2020 and February 2024 on the arXiv, bioRxiv, and\nNature portfolio journals, using a population-level statistical framework to\nmeasure the prevalence of LLM-modified content over time. Our statistical\nestimation operates on the corpus level and is more robust than inference on\nindividual instances. Our findings reveal a steady increase in LLM usage, with\nthe largest and fastest growth observed in Computer Science papers (up to\n17.5%). In comparison, Mathematics papers and the Nature portfolio showed the\nleast LLM modification (up to 6.3%). Moreover, at an aggregate level, our\nanalysis reveals that higher levels of LLM-modification are associated with\npapers whose first authors post preprints more frequently, papers in more\ncrowded research areas, and papers of shorter lengths. Our findings suggests\nthat LLMs are being broadly used in scientific writings.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.DL",
      "cs.LG",
      "cs.SI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2404.01268v1",
    "published_date": "2024-04-01 17:45:15 UTC",
    "updated_date": "2024-04-01 17:45:15 UTC"
  },
  {
    "arxiv_id": "2404.01266v3",
    "title": "IsoBench: Benchmarking Multimodal Foundation Models on Isomorphic Representations",
    "authors": [
      "Deqing Fu",
      "Ruohao Guo",
      "Ghazal Khalighinejad",
      "Ollie Liu",
      "Bhuwan Dhingra",
      "Dani Yogatama",
      "Robin Jia",
      "Willie Neiswanger"
    ],
    "abstract": "Current foundation models exhibit impressive capabilities when prompted\neither with text only or with both image and text inputs. But do their\ncapabilities change depending on the input modality? In this work, we propose\n$\\textbf{IsoBench}$, a benchmark dataset containing problems from four major\nareas: math, science, algorithms, and games. Each example is presented with\nmultiple $\\textbf{isomorphic representations}$ of inputs, such as visual,\ntextual, and mathematical presentations. IsoBench provides fine-grained\nfeedback to diagnose performance gaps caused by the form of the representation.\nAcross various foundation models, we observe that on the same problem, models\nhave a consistent preference towards textual representations. Most prominently,\nwhen evaluated on all IsoBench problems, Claude-3 Opus performs 28.7 points\nworse when provided with images instead of text; similarly, GPT-4 Turbo is 18.7\npoints worse and Gemini Pro is 14.9 points worse. Finally, we present two\nprompting techniques, $\\textit{IsoCombination}$ and $\\textit{IsoScratchPad}$,\nwhich improve model performance by considering combinations of, and\ntranslations between, different input representations.",
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.AI",
    "comment": "1st Conference on Language Modeling (COLM), 2024",
    "pdf_url": "http://arxiv.org/pdf/2404.01266v3",
    "published_date": "2024-04-01 17:43:27 UTC",
    "updated_date": "2024-08-18 23:48:44 UTC"
  },
  {
    "arxiv_id": "2404.01364v1",
    "title": "Information Plane Analysis Visualization in Deep Learning via Transfer Entropy",
    "authors": [
      "Adrian Moldovan",
      "Angel Cataron",
      "Razvan Andonie"
    ],
    "abstract": "In a feedforward network, Transfer Entropy (TE) can be used to measure the\ninfluence that one layer has on another by quantifying the information transfer\nbetween them during training. According to the Information Bottleneck\nprinciple, a neural model's internal representation should compress the input\ndata as much as possible while still retaining sufficient information about the\noutput. Information Plane analysis is a visualization technique used to\nunderstand the trade-off between compression and information preservation in\nthe context of the Information Bottleneck method by plotting the amount of\ninformation in the input data against the compressed representation. The claim\nthat there is a causal link between information-theoretic compression and\ngeneralization, measured by mutual information, is plausible, but results from\ndifferent studies are conflicting. In contrast to mutual information, TE can\ncapture temporal relationships between variables. To explore such links, in our\nnovel approach we use TE to quantify information transfer between neural layers\nand perform Information Plane analysis. We obtained encouraging experimental\nresults, opening the possibility for further investigations.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.HC",
      "cs.IT",
      "math.IT"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2404.01364v1",
    "published_date": "2024-04-01 17:34:18 UTC",
    "updated_date": "2024-04-01 17:34:18 UTC"
  },
  {
    "arxiv_id": "2404.01261v2",
    "title": "FABLES: Evaluating faithfulness and content selection in book-length summarization",
    "authors": [
      "Yekyung Kim",
      "Yapei Chang",
      "Marzena Karpinska",
      "Aparna Garimella",
      "Varun Manjunatha",
      "Kyle Lo",
      "Tanya Goyal",
      "Mohit Iyyer"
    ],
    "abstract": "While long-context large language models (LLMs) can technically summarize\nbook-length documents (>100K tokens), the length and complexity of the\ndocuments have so far prohibited evaluations of input-dependent aspects like\nfaithfulness. In this paper, we conduct the first large-scale human evaluation\nof faithfulness and content selection on LLM-generated summaries of fictional\nbooks. Our study mitigates the issue of data contamination by focusing on\nsummaries of books published in 2023 or 2024, and we hire annotators who have\nfully read each book prior to the annotation task to minimize cost and\ncognitive burden. We collect FABLES, a dataset of annotations on 3,158 claims\nmade in LLM-generated summaries of 26 books, at a cost of $5.2K USD, which\nallows us to rank LLM summarizers based on faithfulness: Claude-3-Opus\nsignificantly outperforms all closed-source LLMs, while the open-source Mixtral\nis on par with GPT-3.5-Turbo. An analysis of the annotations reveals that most\nunfaithful claims relate to events and character states, and they generally\nrequire indirect reasoning over the narrative to invalidate. While LLM-based\nauto-raters have proven reliable for factuality and coherence in other\nsettings, we implement several LLM raters of faithfulness and find that none\ncorrelates strongly with human annotations, especially with regard to detecting\nunfaithful claims. Our experiments suggest that detecting unfaithful claims is\nan important future direction not only for summarization evaluation but also as\na testbed for long-context understanding. Finally, we move beyond faithfulness\nby exploring content selection errors in book-length summarization: we develop\na typology of omission errors related to crucial narrative elements and also\nidentify a systematic over-emphasis on events occurring towards the end of the\nbook.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "preprint - 39 pages",
    "pdf_url": "http://arxiv.org/pdf/2404.01261v2",
    "published_date": "2024-04-01 17:33:38 UTC",
    "updated_date": "2024-09-30 17:39:59 UTC"
  },
  {
    "arxiv_id": "2404.01363v1",
    "title": "AIOps Solutions for Incident Management: Technical Guidelines and A Comprehensive Literature Review",
    "authors": [
      "Youcef Remil",
      "Anes Bendimerad",
      "Romain Mathonat",
      "Mehdi Kaytoue"
    ],
    "abstract": "The management of modern IT systems poses unique challenges, necessitating\nscalability, reliability, and efficiency in handling extensive data streams.\nTraditional methods, reliant on manual tasks and rule-based approaches, prove\ninefficient for the substantial data volumes and alerts generated by IT\nsystems. Artificial Intelligence for Operating Systems (AIOps) has emerged as a\nsolution, leveraging advanced analytics like machine learning and big data to\nenhance incident management. AIOps detects and predicts incidents, identifies\nroot causes, and automates healing actions, improving quality and reducing\noperational costs. However, despite its potential, the AIOps domain is still in\nits early stages, decentralized across multiple sectors, and lacking\nstandardized conventions. Research and industrial contributions are distributed\nwithout consistent frameworks for data management, target problems,\nimplementation details, requirements, and capabilities. This study proposes an\nAIOps terminology and taxonomy, establishing a structured incident management\nprocedure and providing guidelines for constructing an AIOps framework. The\nresearch also categorizes contributions based on criteria such as incident\nmanagement tasks, application areas, data sources, and technical approaches.\nThe goal is to provide a comprehensive review of technical and research aspects\nin AIOps for incident management, aiming to structure knowledge, identify gaps,\nand establish a foundation for future developments in the field.",
    "categories": [
      "cs.OS",
      "cs.AI",
      "cs.SE"
    ],
    "primary_category": "cs.OS",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2404.01363v1",
    "published_date": "2024-04-01 17:32:22 UTC",
    "updated_date": "2024-04-01 17:32:22 UTC"
  },
  {
    "arxiv_id": "2404.01260v1",
    "title": "Bridging Remote Sensors with Multisensor Geospatial Foundation Models",
    "authors": [
      "Boran Han",
      "Shuai Zhang",
      "Xingjian Shi",
      "Markus Reichstein"
    ],
    "abstract": "In the realm of geospatial analysis, the diversity of remote sensors,\nencompassing both optical and microwave technologies, offers a wealth of\ndistinct observational capabilities. Recognizing this, we present msGFM, a\nmultisensor geospatial foundation model that effectively unifies data from four\nkey sensor modalities. This integration spans an expansive dataset of two\nmillion multisensor images. msGFM is uniquely adept at handling both paired and\nunpaired sensor data. For data originating from identical geolocations, our\nmodel employs an innovative cross-sensor pretraining approach in masked image\nmodeling, enabling the synthesis of joint representations from diverse sensors.\nmsGFM, incorporating four remote sensors, upholds strong performance, forming a\ncomprehensive model adaptable to various sensor types. msGFM has demonstrated\nenhanced proficiency in a range of both single-sensor and multisensor\ndownstream tasks. These include scene classification, segmentation, cloud\nremoval, and pan-sharpening. A key discovery of our research is that\nrepresentations derived from natural images are not always compatible with the\ndistinct characteristics of geospatial remote sensors, underscoring the\nlimitations of existing representations in this field. Our work can serve as a\nguide for developing multisensor geospatial pretraining models, paving the way\nfor more advanced geospatial capabilities.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "Accepted to CVPR",
    "pdf_url": "http://arxiv.org/pdf/2404.01260v1",
    "published_date": "2024-04-01 17:30:56 UTC",
    "updated_date": "2024-04-01 17:30:56 UTC"
  },
  {
    "arxiv_id": "2404.01258v2",
    "title": "Direct Preference Optimization of Video Large Multimodal Models from Language Model Reward",
    "authors": [
      "Ruohong Zhang",
      "Liangke Gui",
      "Zhiqing Sun",
      "Yihao Feng",
      "Keyang Xu",
      "Yuanhan Zhang",
      "Di Fu",
      "Chunyuan Li",
      "Alexander Hauptmann",
      "Yonatan Bisk",
      "Yiming Yang"
    ],
    "abstract": "Preference modeling techniques, such as direct preference optimization (DPO),\nhas shown effective in enhancing the generalization abilities of large language\nmodel (LLM). However, in tasks involving video instruction-following, providing\ninformative feedback, especially for detecting hallucinations in generated\nresponses, remains a significant challenge. Previous studies have explored\nusing large large multimodal models (LMMs) as reward models to guide preference\nmodeling, but their ability to accurately assess the factuality of generated\nresponses compared to corresponding videos has not been conclusively\nestablished. This paper introduces a novel framework that utilizes detailed\nvideo captions as a proxy of video content, enabling language models to\nincorporate this information as supporting evidence for scoring video Question\nAnswering (QA) predictions. Our approach demonstrates robust alignment with\nOpenAI GPT-4V model's reward mechanism, which directly takes video frames as\ninput. Furthermore, we show that applying this tailored reward through DPO\nsignificantly improves the performance of video LMMs on video QA tasks.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2404.01258v2",
    "published_date": "2024-04-01 17:28:16 UTC",
    "updated_date": "2024-04-02 12:47:49 UTC"
  },
  {
    "arxiv_id": "2404.16041v1",
    "title": "Forklift: An Extensible Neural Lifter",
    "authors": [
      "Jordi Armengol-Estapé",
      "Rodrigo C. O. Rocha",
      "Jackson Woodruff",
      "Pasquale Minervini",
      "Michael F. P. O'Boyle"
    ],
    "abstract": "The escalating demand to migrate legacy software across different Instruction\nSet Architectures (ISAs) has driven the development of assembly-to-assembly\ntranslators to map between their respective assembly languages. However, the\ndevelopment of these tools requires substantial engineering effort.\nState-of-the-art approaches use lifting, a technique where source assembly code\nis translated to an architecture-independent intermediate representation (IR)\n(for example, the LLVM IR) and use a pre-existing compiler to recompile the IR\nto the target ISA. However, the hand-written rules these lifters employ are\nsensitive to the particular compiler and optimization level used to generate\nthe code and require significant engineering effort to support each new ISA. We\npropose Forklift, the first neural lifter that learns how to translate assembly\nto LLVM IR using a token-level encoder-decoder Transformer. We show how to\nincrementally add support to new ISAs by fine tuning the assembly encoder and\nfreezing the IR decoder, improving the overall accuracy and efficiency. We\ncollect millions of parallel LLVM IR, x86, ARM, and RISC-V programs across\ncompilers and optimization levels to train Forklift and set up an\ninput/output-based accuracy harness. We evaluate Forklift on two challenging\nbenchmark suites and translate 2.5x more x86 programs than a state-of-the-art\nhand-written lifter and 4.4x more x86 programs than GPT-4 as well as enabling\ntranslation from new ISAs.",
    "categories": [
      "cs.PL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.PL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2404.16041v1",
    "published_date": "2024-04-01 17:27:58 UTC",
    "updated_date": "2024-04-01 17:27:58 UTC"
  },
  {
    "arxiv_id": "2404.15310v1",
    "title": "Automated Assessment of Encouragement and Warmth in Classrooms Leveraging Multimodal Emotional Features and ChatGPT",
    "authors": [
      "Ruikun Hou",
      "Tim Fütterer",
      "Babette Bühler",
      "Efe Bozkir",
      "Peter Gerjets",
      "Ulrich Trautwein",
      "Enkelejda Kasneci"
    ],
    "abstract": "Classroom observation protocols standardize the assessment of teaching\neffectiveness and facilitate comprehension of classroom interactions. Whereas\nthese protocols offer teachers specific feedback on their teaching practices,\nthe manual coding by human raters is resource-intensive and often unreliable.\nThis has sparked interest in developing AI-driven, cost-effective methods for\nautomating such holistic coding. Our work explores a multimodal approach to\nautomatically estimating encouragement and warmth in classrooms, a key\ncomponent of the Global Teaching Insights (GTI) study's observation protocol.\nTo this end, we employed facial and speech emotion recognition with sentiment\nanalysis to extract interpretable features from video, audio, and transcript\ndata. The prediction task involved both classification and regression methods.\nAdditionally, in light of recent large language models' remarkable text\nannotation capabilities, we evaluated ChatGPT's zero-shot performance on this\nscoring task based on transcripts. We demonstrated our approach on the GTI\ndataset, comprising 367 16-minute video segments from 92 authentic lesson\nrecordings. The inferences of GPT-4 and the best-trained model yielded\ncorrelations of r = .341 and r = .441 with human ratings, respectively.\nCombining estimates from both models through averaging, an ensemble approach\nachieved a correlation of r = .513, comparable to human inter-rater\nreliability. Our model explanation analysis indicated that text sentiment\nfeatures were the primary contributors to the trained model's decisions.\nMoreover, GPT-4 could deliver logical and concrete reasoning as potential\nteacher guidelines. Our findings provide insights into using advanced,\nmultimodal techniques for automated classroom observation, aiming to foster\nteacher training through frequent and valuable feedback.",
    "categories": [
      "cs.HC",
      "cs.AI",
      "cs.CY",
      "cs.LG"
    ],
    "primary_category": "cs.HC",
    "comment": "Accepted as a full paper by the 25th International Conference on\n  Artificial Intelligence in Education (AIED 2024)",
    "pdf_url": "http://arxiv.org/pdf/2404.15310v1",
    "published_date": "2024-04-01 16:58:09 UTC",
    "updated_date": "2024-04-01 16:58:09 UTC"
  },
  {
    "arxiv_id": "2404.01223v1",
    "title": "Feature Splatting: Language-Driven Physics-Based Scene Synthesis and Editing",
    "authors": [
      "Ri-Zhao Qiu",
      "Ge Yang",
      "Weijia Zeng",
      "Xiaolong Wang"
    ],
    "abstract": "Scene representations using 3D Gaussian primitives have produced excellent\nresults in modeling the appearance of static and dynamic 3D scenes. Many\ngraphics applications, however, demand the ability to manipulate both the\nappearance and the physical properties of objects. We introduce Feature\nSplatting, an approach that unifies physics-based dynamic scene synthesis with\nrich semantics from vision language foundation models that are grounded by\nnatural language. Our first contribution is a way to distill high-quality,\nobject-centric vision-language features into 3D Gaussians, that enables\nsemi-automatic scene decomposition using text queries. Our second contribution\nis a way to synthesize physics-based dynamics from an otherwise static scene\nusing a particle-based simulator, in which material properties are assigned\nautomatically via text queries. We ablate key techniques used in this pipeline,\nto illustrate the challenge and opportunities in using feature-carrying 3D\nGaussians as a unified format for appearance, geometry, material properties and\nsemantics grounded on natural language. Project website:\nhttps://feature-splatting.github.io/",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.GR",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "Project website: https://feature-splatting.github.io/",
    "pdf_url": "http://arxiv.org/pdf/2404.01223v1",
    "published_date": "2024-04-01 16:31:04 UTC",
    "updated_date": "2024-04-01 16:31:04 UTC"
  },
  {
    "arxiv_id": "2404.01217v1",
    "title": "Incorporating Domain Differential Equations into Graph Convolutional Networks to Lower Generalization Discrepancy",
    "authors": [
      "Yue Sun",
      "Chao Chen",
      "Yuesheng Xu",
      "Sihong Xie",
      "Rick S. Blum",
      "Parv Venkitasubramaniam"
    ],
    "abstract": "Ensuring both accuracy and robustness in time series prediction is critical\nto many applications, ranging from urban planning to pandemic management. With\nsufficient training data where all spatiotemporal patterns are\nwell-represented, existing deep-learning models can make reasonably accurate\npredictions. However, existing methods fail when the training data are drawn\nfrom different circumstances (e.g., traffic patterns on regular days) compared\nto test data (e.g., traffic patterns after a natural disaster). Such challenges\nare usually classified under domain generalization. In this work, we show that\none way to address this challenge in the context of spatiotemporal prediction\nis by incorporating domain differential equations into Graph Convolutional\nNetworks (GCNs). We theoretically derive conditions where GCNs incorporating\nsuch domain differential equations are robust to mismatched training and\ntesting data compared to baseline domain agnostic models. To support our\ntheory, we propose two domain-differential-equation-informed networks called\nReaction-Diffusion Graph Convolutional Network (RDGCN), which incorporates\ndifferential equations for traffic speed evolution, and\nSusceptible-Infectious-Recovered Graph Convolutional Network (SIRGCN), which\nincorporates a disease propagation model. Both RDGCN and SIRGCN are based on\nreliable and interpretable domain differential equations that allow the models\nto generalize to unseen patterns. We experimentally show that RDGCN and SIRGCN\nare more robust with mismatched testing data than the state-of-the-art deep\nlearning methods.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2404.01217v1",
    "published_date": "2024-04-01 16:17:11 UTC",
    "updated_date": "2024-04-01 16:17:11 UTC"
  },
  {
    "arxiv_id": "2404.01163v1",
    "title": "Capturing Shock Waves by Relaxation Neural Networks",
    "authors": [
      "Nan Zhou",
      "Zheng Ma"
    ],
    "abstract": "In this paper, we put forward a neural network framework to solve the\nnonlinear hyperbolic systems. This framework, named relaxation neural\nnetworks(RelaxNN), is a simple and scalable extension of physics-informed\nneural networks(PINN). It is shown later that a typical PINN framework\nstruggles to handle shock waves that arise in hyperbolic systems' solutions.\nThis ultimately results in the failure of optimization that is based on\ngradient descent in the training process. Relaxation systems provide a smooth\nasymptotic to the discontinuity solution, under the expectation that\nmacroscopic problems can be solved from a microscopic perspective. Based on\nrelaxation systems, the RelaxNN framework alleviates the conflict of losses in\nthe training process of the PINN framework. In addition to the remarkable\nresults demonstrated in numerical simulations, most of the acceleration\ntechniques and improvement strategies aimed at the standard PINN framework can\nalso be applied to the RelaxNN framework.",
    "categories": [
      "math.NA",
      "cs.AI",
      "cs.NA",
      "76L05, 35D99, 68T07, 65D15"
    ],
    "primary_category": "math.NA",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2404.01163v1",
    "published_date": "2024-04-01 15:13:46 UTC",
    "updated_date": "2024-04-01 15:13:46 UTC"
  },
  {
    "arxiv_id": "2404.01156v1",
    "title": "SyncMask: Synchronized Attentional Masking for Fashion-centric Vision-Language Pretraining",
    "authors": [
      "Chull Hwan Song",
      "Taebaek Hwang",
      "Jooyoung Yoon",
      "Shunghyun Choi",
      "Yeong Hyeon Gu"
    ],
    "abstract": "Vision-language models (VLMs) have made significant strides in cross-modal\nunderstanding through large-scale paired datasets. However, in fashion domain,\ndatasets often exhibit a disparity between the information conveyed in image\nand text. This issue stems from datasets containing multiple images of a single\nfashion item all paired with one text, leading to cases where some textual\ndetails are not visible in individual images. This mismatch, particularly when\nnon-co-occurring elements are masked, undermines the training of conventional\nVLM objectives like Masked Language Modeling and Masked Image Modeling, thereby\nhindering the model's ability to accurately align fine-grained visual and\ntextual features. Addressing this problem, we propose Synchronized attentional\nMasking (SyncMask), which generate masks that pinpoint the image patches and\nword tokens where the information co-occur in both image and text. This\nsynchronization is accomplished by harnessing cross-attentional features\nobtained from a momentum model, ensuring a precise alignment between the two\nmodalities. Additionally, we enhance grouped batch sampling with semi-hard\nnegatives, effectively mitigating false negative issues in Image-Text Matching\nand Image-Text Contrastive learning objectives within fashion datasets. Our\nexperiments demonstrate the effectiveness of the proposed approach,\noutperforming existing methods in three downstream tasks.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "CVPR2024 Accepted",
    "pdf_url": "http://arxiv.org/pdf/2404.01156v1",
    "published_date": "2024-04-01 15:01:38 UTC",
    "updated_date": "2024-04-01 15:01:38 UTC"
  },
  {
    "arxiv_id": "2406.11850v1",
    "title": "Closed-loop Teaching via Demonstrations to Improve Policy Transparency",
    "authors": [
      "Michael S. Lee",
      "Reid Simmons",
      "Henny Admoni"
    ],
    "abstract": "Demonstrations are a powerful way of increasing the transparency of AI\npolicies. Though informative demonstrations may be selected a priori through\nthe machine teaching paradigm, student learning may deviate from the\npreselected curriculum in situ. This paper thus explores augmenting a\ncurriculum with a closed-loop teaching framework inspired by principles from\nthe education literature, such as the zone of proximal development and the\ntesting effect. We utilize tests accordingly to close to the loop and maintain\na novel particle filter model of human beliefs throughout the learning process,\nallowing us to provide demonstrations that are targeted to the human's current\nunderstanding in real time. A user study finds that our proposed closed-loop\nteaching framework reduces the regret in human test responses by 43% over a\nbaseline.",
    "categories": [
      "cs.CY",
      "cs.AI"
    ],
    "primary_category": "cs.CY",
    "comment": "Supplementary material available at\n  https://drive.google.com/file/d/1f_BDk3JpY6DvqlvgKtnQZ8zdfO3XAn3p/view?usp=drive_link",
    "pdf_url": "http://arxiv.org/pdf/2406.11850v1",
    "published_date": "2024-04-01 14:59:26 UTC",
    "updated_date": "2024-04-01 14:59:26 UTC"
  },
  {
    "arxiv_id": "2404.01154v1",
    "title": "Uncovering the Text Embedding in Text-to-Image Diffusion Models",
    "authors": [
      "Hu Yu",
      "Hao Luo",
      "Fan Wang",
      "Feng Zhao"
    ],
    "abstract": "The correspondence between input text and the generated image exhibits\nopacity, wherein minor textual modifications can induce substantial deviations\nin the generated image. While, text embedding, as the pivotal intermediary\nbetween text and images, remains relatively underexplored. In this paper, we\naddress this research gap by delving into the text embedding space, unleashing\nits capacity for controllable image editing and explicable semantic direction\nattributes within a learning-free framework. Specifically, we identify two\ncritical insights regarding the importance of per-word embedding and their\ncontextual correlations within text embedding, providing instructive principles\nfor learning-free image editing. Additionally, we find that text embedding\ninherently possesses diverse semantic potentials, and further reveal this\nproperty through the lens of singular value decomposition (SVD). These\nuncovered properties offer practical utility for image editing and semantic\ndiscovery. More importantly, we expect the in-depth analyses and findings of\nthe text embedding can enhance the understanding of text-to-image diffusion\nmodels.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2404.01154v1",
    "published_date": "2024-04-01 14:59:13 UTC",
    "updated_date": "2024-04-01 14:59:13 UTC"
  },
  {
    "arxiv_id": "2404.01143v1",
    "title": "Condition-Aware Neural Network for Controlled Image Generation",
    "authors": [
      "Han Cai",
      "Muyang Li",
      "Zhuoyang Zhang",
      "Qinsheng Zhang",
      "Ming-Yu Liu",
      "Song Han"
    ],
    "abstract": "We present Condition-Aware Neural Network (CAN), a new method for adding\ncontrol to image generative models. In parallel to prior conditional control\nmethods, CAN controls the image generation process by dynamically manipulating\nthe weight of the neural network. This is achieved by introducing a\ncondition-aware weight generation module that generates conditional weight for\nconvolution/linear layers based on the input condition. We test CAN on\nclass-conditional image generation on ImageNet and text-to-image generation on\nCOCO. CAN consistently delivers significant improvements for diffusion\ntransformer models, including DiT and UViT. In particular, CAN combined with\nEfficientViT (CaT) achieves 2.78 FID on ImageNet 512x512, surpassing DiT-XL/2\nwhile requiring 52x fewer MACs per sampling step.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "CVPR 2024",
    "pdf_url": "http://arxiv.org/pdf/2404.01143v1",
    "published_date": "2024-04-01 14:42:57 UTC",
    "updated_date": "2024-04-01 14:42:57 UTC"
  },
  {
    "arxiv_id": "2404.01135v1",
    "title": "Enhancing Reasoning Capacity of SLM using Cognitive Enhancement",
    "authors": [
      "Jonathan Pan",
      "Swee Liang Wong",
      "Xin Wei Chia",
      "Yidi Yuan"
    ],
    "abstract": "Large Language Models (LLMs) have been applied to automate cyber security\nactivities and processes including cyber investigation and digital forensics.\nHowever, the use of such models for cyber investigation and digital forensics\nshould address accountability and security considerations. Accountability\nensures models have the means to provide explainable reasonings and outcomes.\nThis information can be extracted through explicit prompt requests. For\nsecurity considerations, it is crucial to address privacy and confidentiality\nof the involved data during data processing as well. One approach to deal with\nthis consideration is to have the data processed locally using a local instance\nof the model. Due to limitations of locally available resources, namely memory\nand GPU capacities, a Smaller Large Language Model (SLM) will typically be\nused. These SLMs have significantly fewer parameters compared to the LLMs.\nHowever, such size reductions have notable performance reduction, especially\nwhen tasked to provide reasoning explanations. In this paper, we aim to\nmitigate performance reduction through the integration of cognitive strategies\nthat humans use for problem-solving. We term this as cognitive enhancement\nthrough prompts. Our experiments showed significant improvement gains of the\nSLMs' performances when such enhancements were applied. We believe that our\nexploration study paves the way for further investigation into the use of\ncognitive enhancement to optimize SLM for cyber security applications.",
    "categories": [
      "cs.CR",
      "cs.AI"
    ],
    "primary_category": "cs.CR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2404.01135v1",
    "published_date": "2024-04-01 14:29:58 UTC",
    "updated_date": "2024-04-01 14:29:58 UTC"
  },
  {
    "arxiv_id": "2404.01131v2",
    "title": "GOV-REK: Governed Reward Engineering Kernels for Designing Robust Multi-Agent Reinforcement Learning Systems",
    "authors": [
      "Ashish Rana",
      "Michael Oesterle",
      "Jannik Brinkmann"
    ],
    "abstract": "For multi-agent reinforcement learning systems (MARLS), the problem\nformulation generally involves investing massive reward engineering effort\nspecific to a given problem. However, this effort often cannot be translated to\nother problems; worse, it gets wasted when system dynamics change drastically.\nThis problem is further exacerbated in sparse reward scenarios, where a\nmeaningful heuristic can assist in the policy convergence task. We propose\nGOVerned Reward Engineering Kernels (GOV-REK), which dynamically assign reward\ndistributions to agents in MARLS during its learning stage. We also introduce\ngovernance kernels, which exploit the underlying structure in either state or\njoint action space for assigning meaningful agent reward distributions. During\nthe agent learning stage, it iteratively explores different reward distribution\nconfigurations with a Hyperband-like algorithm to learn ideal agent reward\nmodels in a problem-agnostic manner. Our experiments demonstrate that our\nmeaningful reward priors robustly jumpstart the learning process for\neffectively learning different MARL problems.",
    "categories": [
      "cs.MA",
      "cs.AI"
    ],
    "primary_category": "cs.MA",
    "comment": "Extended Abstract accepted in the 23rd International Conference on\n  Autonomous Agents and Multi-Agent Systems (AAMAS 2024)",
    "pdf_url": "http://arxiv.org/pdf/2404.01131v2",
    "published_date": "2024-04-01 14:19:00 UTC",
    "updated_date": "2024-04-14 19:54:33 UTC"
  },
  {
    "arxiv_id": "2404.01127v1",
    "title": "Medical Visual Prompting (MVP): A Unified Framework for Versatile and High-Quality Medical Image Segmentation",
    "authors": [
      "Yulin Chen",
      "Guoheng Huang",
      "Kai Huang",
      "Zijin Lin",
      "Guo Zhong",
      "Shenghong Luo",
      "Jie Deng",
      "Jian Zhou"
    ],
    "abstract": "Accurate segmentation of lesion regions is crucial for clinical diagnosis and\ntreatment across various diseases. While deep convolutional networks have\nachieved satisfactory results in medical image segmentation, they face\nchallenges such as loss of lesion shape information due to continuous\nconvolution and downsampling, as well as the high cost of manually labeling\nlesions with varying shapes and sizes. To address these issues, we propose a\nnovel medical visual prompting (MVP) framework that leverages pre-training and\nprompting concepts from natural language processing (NLP). The framework\nutilizes three key components: Super-Pixel Guided Prompting (SPGP) for\nsuperpixelating the input image, Image Embedding Guided Prompting (IEGP) for\nfreezing patch embedding and merging with superpixels to provide visual\nprompts, and Adaptive Attention Mechanism Guided Prompting (AAGP) for\npinpointing prompt content and efficiently adapting all layers. By integrating\nSPGP, IEGP, and AAGP, the MVP enables the segmentation network to better learn\nshape prompting information and facilitates mutual learning across different\ntasks. Extensive experiments conducted on five datasets demonstrate superior\nperformance of this method in various challenging medical image tasks, while\nsimplifying single-task medical segmentation models. This novel framework\noffers improved performance with fewer parameters and holds significant\npotential for accurate segmentation of lesion regions in various medical tasks,\nmaking it clinically valuable.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2404.01127v1",
    "published_date": "2024-04-01 14:06:48 UTC",
    "updated_date": "2024-04-01 14:06:48 UTC"
  },
  {
    "arxiv_id": "2404.01109v1",
    "title": "An incremental hybrid adaptive network-based IDS in Software Defined Networks to detect stealth attacks",
    "authors": [
      "Abdullah H Alqahtani"
    ],
    "abstract": "Network attacks have became increasingly more sophisticated and stealthy due\nto the advances in technologies and the growing sophistication of attackers.\nAdvanced Persistent Threats (APTs) are a type of attack that implement a wide\nrange of strategies to evade detection and be under the defence radar. Software\nDefined Network (SDN) is a network paradigm that implements dynamic\nconfiguration by separating the control plane from the network plane. This\napproach improves security aspects by facilitating the employment of network\nintrusion detection systems. Implementing Machine Learning (ML) techniques in\nIntrusion Detection Systems (IDSs) is widely used to detect such attacks but\nhas a challenge when the data distribution changes. Concept drift is a term\nthat describes the change in the relationship between the input data and the\ntarget value (label or class). The model is expected to degrade as certain\nforms of change occur. In this paper, the primary form of change will be in\nuser behaviour (particularly changes in attacker behaviour). It is essential\nfor a model to adapt itself to deviations in data distribution. SDN can help in\nmonitoring changes in data distribution. This paper discusses changes in\nstealth attacker behaviour. The work described here investigates various\nconcept drift detection algorithms. An incremental hybrid adaptive Network\nIntrusion Detection System (NIDS) is proposed to tackle the issue of concept\ndrift in SDN. It can detect known and unknown attacks. The model is evaluated\nover different datasets showing promising results.",
    "categories": [
      "cs.CR",
      "cs.AI"
    ],
    "primary_category": "cs.CR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2404.01109v1",
    "published_date": "2024-04-01 13:33:40 UTC",
    "updated_date": "2024-04-01 13:33:40 UTC"
  },
  {
    "arxiv_id": "2404.01361v1",
    "title": "LLM Attributor: Interactive Visual Attribution for LLM Generation",
    "authors": [
      "Seongmin Lee",
      "Zijie J. Wang",
      "Aishwarya Chakravarthy",
      "Alec Helbling",
      "ShengYun Peng",
      "Mansi Phute",
      "Duen Horng Chau",
      "Minsuk Kahng"
    ],
    "abstract": "While large language models (LLMs) have shown remarkable capability to\ngenerate convincing text across diverse domains, concerns around its potential\nrisks have highlighted the importance of understanding the rationale behind\ntext generation. We present LLM Attributor, a Python library that provides\ninteractive visualizations for training data attribution of an LLM's text\ngeneration. Our library offers a new way to quickly attribute an LLM's text\ngeneration to training data points to inspect model behaviors, enhance its\ntrustworthiness, and compare model-generated text with user-provided text. We\ndescribe the visual and interactive design of our tool and highlight usage\nscenarios for LLaMA2 models fine-tuned with two different datasets: online\narticles about recent disasters and finance-related question-answer pairs.\nThanks to LLM Attributor's broad support for computational notebooks, users can\neasily integrate it into their workflow to interactively visualize attributions\nof their models. For easier access and extensibility, we open-source LLM\nAttributor at https://github.com/poloclub/ LLM-Attribution. The video demo is\navailable at https://youtu.be/mIG2MDQKQxM.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.HC",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "8 pages, 3 figures, For a video demo, see\n  https://youtu.be/mIG2MDQKQxM",
    "pdf_url": "http://arxiv.org/pdf/2404.01361v1",
    "published_date": "2024-04-01 13:16:34 UTC",
    "updated_date": "2024-04-01 13:16:34 UTC"
  },
  {
    "arxiv_id": "2404.01099v2",
    "title": "What is in Your Safe Data? Identifying Benign Data that Breaks Safety",
    "authors": [
      "Luxi He",
      "Mengzhou Xia",
      "Peter Henderson"
    ],
    "abstract": "Current Large Language Models (LLMs), even those tuned for safety and\nalignment, are susceptible to jailbreaking. Some have found that just further\nfine-tuning an aligned model with benign data (i.e., data without harmful\ncontent) surprisingly leads to substantial degradation in safety. We delve into\nthe data-centric aspects of why benign fine-tuning inadvertently contributes to\njailbreaking. First, we represent fine-tuning data through two lenses:\nrepresentation and gradient spaces. Additionally, we propose a bi-directional\nanchoring method that, during the selection process, prioritizes data points\nthat are close to harmful examples and far from benign ones. Our approach\neffectively identifies subsets of benign data that are more likely to degrade\nthe model's safety after fine-tuning. Training on just 100 of these seemingly\nbenign datapoints surprisingly leads to the fine-tuned model affirmatively\nresponding to >70% of tested harmful requests, compared to <20% after\nfine-tuning on randomly selected data. We also observe that the selected data\nfrequently appear as lists, bullet points, or math questions, indicating a\nsystematic pattern in fine-tuning data that contributes to jailbreaking.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL",
      "cs.CR"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2404.01099v2",
    "published_date": "2024-04-01 13:12:30 UTC",
    "updated_date": "2024-08-20 17:54:08 UTC"
  },
  {
    "arxiv_id": "2404.03685v9",
    "title": "Cooperative Evolutionary Pressure and Diminishing Returns Might Explain the Fermi Paradox: On What Super-AIs Are Like",
    "authors": [
      "Daniel Vallstrom"
    ],
    "abstract": "With an evolutionary approach, the basis of morality can be explained as\nadaptations to problems of cooperation. With 'evolution' taken in a broad\nsense, AIs that satisfy the conditions for evolution to apply will be subject\nto the same cooperative evolutionary pressure as biological entities. Here the\nadaptiveness of increased cooperation as material safety and wealth increase is\ndiscussed -- for humans, for other societies, and for AIs. Diminishing\nbeneficial returns from increased access to material resources also suggests\nthe possibility that, on the whole, there will be no incentive to for instance\ncolonize entire galaxies, thus providing a possible explanation of the Fermi\nparadox, wondering where everybody is. It is further argued that old societies\ncould engender, give way to, super-AIs, since it is likely that super-AIs are\nfeasible, and fitter. Closing is an aside on effective ways for morals and\ngoals to affect life and society, emphasizing environments, cultures, and laws,\nand exemplified by how to eat.\n  'Diminishing returns' is defined, as less than roots, the inverse of\ninfeasibility. It is also noted that there can be no exponential colonization\nor reproduction, for mathematical reasons, as each entity takes up a certain\namount of space. Appended are an algorithm for colonizing for example a galaxy\nquickly, models of the evolution of cooperation and fairness under diminishing\nreturns, and software for simulating signaling development.",
    "categories": [
      "physics.soc-ph",
      "cs.AI"
    ],
    "primary_category": "physics.soc-ph",
    "comment": "36 pages, 12 figures. Added figures, expansions",
    "pdf_url": "http://arxiv.org/pdf/2404.03685v9",
    "published_date": "2024-04-01 13:12:27 UTC",
    "updated_date": "2025-02-10 12:10:24 UTC"
  },
  {
    "arxiv_id": "2404.01089v1",
    "title": "Texture-Preserving Diffusion Models for High-Fidelity Virtual Try-On",
    "authors": [
      "Xu Yang",
      "Changxing Ding",
      "Zhibin Hong",
      "Junhao Huang",
      "Jin Tao",
      "Xiangmin Xu"
    ],
    "abstract": "Image-based virtual try-on is an increasingly important task for online\nshopping. It aims to synthesize images of a specific person wearing a specified\ngarment. Diffusion model-based approaches have recently become popular, as they\nare excellent at image synthesis tasks. However, these approaches usually\nemploy additional image encoders and rely on the cross-attention mechanism for\ntexture transfer from the garment to the person image, which affects the\ntry-on's efficiency and fidelity. To address these issues, we propose an\nTexture-Preserving Diffusion (TPD) model for virtual try-on, which enhances the\nfidelity of the results and introduces no additional image encoders.\nAccordingly, we make contributions from two aspects. First, we propose to\nconcatenate the masked person and reference garment images along the spatial\ndimension and utilize the resulting image as the input for the diffusion\nmodel's denoising UNet. This enables the original self-attention layers\ncontained in the diffusion model to achieve efficient and accurate texture\ntransfer. Second, we propose a novel diffusion-based method that predicts a\nprecise inpainting mask based on the person and reference garment images,\nfurther enhancing the reliability of the try-on results. In addition, we\nintegrate mask prediction and image synthesis into a single compact model. The\nexperimental results show that our approach can be applied to various try-on\ntasks, e.g., garment-to-person and person-to-person try-ons, and significantly\noutperforms state-of-the-art methods on popular VITON, VITON-HD databases.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "CVPR 2024",
    "pdf_url": "http://arxiv.org/pdf/2404.01089v1",
    "published_date": "2024-04-01 12:43:22 UTC",
    "updated_date": "2024-04-01 12:43:22 UTC"
  },
  {
    "arxiv_id": "2404.01084v1",
    "title": "AILS-NTUA at SemEval-2024 Task 9: Cracking Brain Teasers: Transformer Models for Lateral Thinking Puzzles",
    "authors": [
      "Ioannis Panagiotopoulos",
      "Giorgos Filandrianos",
      "Maria Lymperaiou",
      "Giorgos Stamou"
    ],
    "abstract": "In this paper, we outline our submission for the SemEval-2024 Task 9\ncompetition: 'BRAINTEASER: A Novel Task Defying Common Sense'. We engage in\nboth sub-tasks: Sub-task A-Sentence Puzzle and Sub-task B-Word Puzzle. We\nevaluate a plethora of pre-trained transformer-based language models of\ndifferent sizes through fine-tuning. Subsequently, we undertake an analysis of\ntheir scores and responses to aid future researchers in understanding and\nutilizing these models effectively. Our top-performing approaches secured\ncompetitive positions on the competition leaderboard across both sub-tasks. In\nthe evaluation phase, our best submission attained an average accuracy score of\n81.7% in the Sentence Puzzle, and 85.4% in the Word Puzzle, significantly\noutperforming the best neural baseline (ChatGPT) by more than 20% and 30%\nrespectively.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "SemEval-2024",
    "pdf_url": "http://arxiv.org/pdf/2404.01084v1",
    "published_date": "2024-04-01 12:27:55 UTC",
    "updated_date": "2024-04-01 12:27:55 UTC"
  },
  {
    "arxiv_id": "2404.01582v2",
    "title": "BERT-Enhanced Retrieval Tool for Homework Plagiarism Detection System",
    "authors": [
      "Jiarong Xian",
      "Jibao Yuan",
      "Peiwei Zheng",
      "Dexian Chen",
      "Nie yuntao"
    ],
    "abstract": "Text plagiarism detection task is a common natural language processing task\nthat aims to detect whether a given text contains plagiarism or copying from\nother texts. In existing research, detection of high level plagiarism is still\na challenge due to the lack of high quality datasets. In this paper, we propose\na plagiarized text data generation method based on GPT-3.5, which produces\n32,927 pairs of text plagiarism detection datasets covering a wide range of\nplagiarism methods, bridging the gap in this part of research. Meanwhile, we\npropose a plagiarism identification method based on Faiss with BERT with high\nefficiency and high accuracy. Our experiments show that the performance of this\nmodel outperforms other models in several metrics, including 98.86\\%, 98.90%,\n98.86%, and 0.9888 for Accuracy, Precision, Recall, and F1 Score, respectively.\nAt the end, we also provide a user-friendly demo platform that allows users to\nupload a text library and intuitively participate in the plagiarism analysis.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.IR"
    ],
    "primary_category": "cs.CL",
    "comment": "arXiv admin note: text overlap with arXiv:1604.06573 by other authors",
    "pdf_url": "http://arxiv.org/pdf/2404.01582v2",
    "published_date": "2024-04-01 12:20:34 UTC",
    "updated_date": "2024-07-28 13:12:03 UTC"
  },
  {
    "arxiv_id": "2404.01070v1",
    "title": "Advancing AI with Integrity: Ethical Challenges and Solutions in Neural Machine Translation",
    "authors": [
      "Richard Kimera",
      "Yun-Seon Kim",
      "Heeyoul Choi"
    ],
    "abstract": "This paper addresses the ethical challenges of Artificial Intelligence in\nNeural Machine Translation (NMT) systems, emphasizing the imperative for\ndevelopers to ensure fairness and cultural sensitivity. We investigate the\nethical competence of AI models in NMT, examining the Ethical considerations at\neach stage of NMT development, including data handling, privacy, data\nownership, and consent. We identify and address ethical issues through\nempirical studies. These include employing Transformer models for\nLuganda-English translations and enhancing efficiency with sentence\nmini-batching. And complementary studies that refine data labeling techniques\nand fine-tune BERT and Longformer models for analyzing Luganda and English\nsocial media content. Our second approach is a literature review from databases\nsuch as Google Scholar and platforms like GitHub. Additionally, the paper\nprobes the distribution of responsibility between AI systems and humans,\nunderscoring the essential role of human oversight in upholding NMT ethical\nstandards. Incorporating a biblical perspective, we discuss the societal impact\nof NMT and the broader ethical responsibilities of developers, positing them as\nstewards accountable for the societal repercussions of their creations.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "11 pages",
    "pdf_url": "http://arxiv.org/pdf/2404.01070v1",
    "published_date": "2024-04-01 12:03:35 UTC",
    "updated_date": "2024-04-01 12:03:35 UTC"
  },
  {
    "arxiv_id": "2404.01054v4",
    "title": "Regularized Best-of-N Sampling with Minimum Bayes Risk Objective for Language Model Alignment",
    "authors": [
      "Yuu Jinnai",
      "Tetsuro Morimura",
      "Kaito Ariu",
      "Kenshi Abe"
    ],
    "abstract": "Best-of-N (BoN) sampling with a reward model has been shown to be an\neffective strategy for aligning Large Language Models (LLMs) to human\npreferences at the time of decoding. BoN sampling is susceptible to a problem\nknown as reward hacking when the accuracy of the reward model is not high\nenough due to the quality or the quantity of the preference dataset. Because\nthe reward model is an imperfect proxy for the true objective, over-optimizing\nits value can compromise its performance on the true objective. In this\nresearch, we propose MBR-BoN, a variant of BoN that aims to mitigate reward\nhacking at inference time by incorporating the Minimum Bayes Risk (MBR)\nobjective as a proximity regularization term. We show empirically and\nanalytically that the MBR objective quantifies the proximity of the response to\nthe reference policy, serving as a proximity regularizer. We evaluate MBR-BoN\non the AlpacaFarm and Anthropic's hh-rlhf datasets and show that it outperforms\nboth BoN sampling and MBR decoding. We also evaluate MBR-BoN to generate a\npairwise preference learning dataset for Direct Preference Optimization (DPO).\nEmpirical results show that models trained on a dataset generated with MBR-BoN\noutperform those with vanilla BoN. Our code is available at\nhttps://github.com/CyberAgentAILab/regularized-bon",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "NAACL 2025",
    "pdf_url": "http://arxiv.org/pdf/2404.01054v4",
    "published_date": "2024-04-01 11:26:50 UTC",
    "updated_date": "2025-01-29 08:52:40 UTC"
  },
  {
    "arxiv_id": "2404.01041v2",
    "title": "Can LLMs get help from other LLMs without revealing private information?",
    "authors": [
      "Florian Hartmann",
      "Duc-Hieu Tran",
      "Peter Kairouz",
      "Victor Cărbune",
      "Blaise Aguera y Arcas"
    ],
    "abstract": "Cascades are a common type of machine learning systems in which a large,\nremote model can be queried if a local model is not able to accurately label a\nuser's data by itself. Serving stacks for large language models (LLMs)\nincreasingly use cascades due to their ability to preserve task performance\nwhile dramatically reducing inference costs. However, applying cascade systems\nin situations where the local model has access to sensitive data constitutes a\nsignificant privacy risk for users since such data could be forwarded to the\nremote model. In this work, we show the feasibility of applying cascade systems\nin such setups by equipping the local model with privacy-preserving techniques\nthat reduce the risk of leaking private information when querying the remote\nmodel. To quantify information leakage in such setups, we introduce two privacy\nmeasures. We then propose a system that leverages the recently introduced\nsocial learning paradigm in which LLMs collaboratively learn from each other by\nexchanging natural language. Using this paradigm, we demonstrate on several\ndatasets that our methods minimize the privacy loss while at the same time\nimproving task performance compared to a non-cascade baseline.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CR",
      "cs.MA"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2404.01041v2",
    "published_date": "2024-04-01 10:54:49 UTC",
    "updated_date": "2024-04-02 06:49:33 UTC"
  },
  {
    "arxiv_id": "2404.01036v1",
    "title": "Higher education assessment practice in the era of generative AI tools",
    "authors": [
      "Bayode Ogunleye",
      "Kudirat Ibilola Zakariyyah",
      "Oluwaseun Ajao",
      "Olakunle Olayinka",
      "Hemlata Sharma"
    ],
    "abstract": "The higher education (HE) sector benefits every nation's economy and society\nat large. However, their contributions are challenged by advanced technologies\nlike generative artificial intelligence (GenAI) tools. In this paper, we\nprovide a comprehensive assessment of GenAI tools towards assessment and\npedagogic practice and, subsequently, discuss the potential impacts. This study\nexperimented using three assessment instruments from data science, data\nanalytics, and construction management disciplines. Our findings are two-fold:\nfirst, the findings revealed that GenAI tools exhibit subject knowledge,\nproblem-solving, analytical, critical thinking, and presentation skills and\nthus can limit learning when used unethically. Secondly, the design of the\nassessment of certain disciplines revealed the limitations of the GenAI tools.\nBased on our findings, we made recommendations on how AI tools can be utilised\nfor teaching and learning in HE.",
    "categories": [
      "cs.IR",
      "cs.AI",
      "cs.CV",
      "cs.LG",
      "I.2.7; I.2.10; H.3.3"
    ],
    "primary_category": "cs.IR",
    "comment": "11 pages, 7 tables published in the Journal of Applied Learning &\n  Teaching",
    "pdf_url": "http://arxiv.org/pdf/2404.01036v1",
    "published_date": "2024-04-01 10:43:50 UTC",
    "updated_date": "2024-04-01 10:43:50 UTC"
  },
  {
    "arxiv_id": "2404.01359v1",
    "title": "Parallel Proportional Fusion of Spiking Quantum Neural Network for Optimizing Image Classification",
    "authors": [
      "Zuyu Xu",
      "Kang Shen",
      "Pengnian Cai",
      "Tao Yang",
      "Yuanming Hu",
      "Shixian Chen",
      "Yunlai Zhu",
      "Zuheng Wu",
      "Yuehua Dai",
      "Jun Wang",
      "Fei Yang"
    ],
    "abstract": "The recent emergence of the hybrid quantum-classical neural network (HQCNN)\narchitecture has garnered considerable attention due to the potential\nadvantages associated with integrating quantum principles to enhance various\nfacets of machine learning algorithms and computations. However, the current\ninvestigated serial structure of HQCNN, wherein information sequentially passes\nfrom one network to another, often imposes limitations on the trainability and\nexpressivity of the network. In this study, we introduce a novel architecture\ntermed Parallel Proportional Fusion of Quantum and Spiking Neural Networks\n(PPF-QSNN). The dataset information is simultaneously fed into both the spiking\nneural network and the variational quantum circuits, with the outputs\namalgamated in proportion to their individual contributions. We systematically\nassess the impact of diverse PPF-QSNN parameters on network performance for\nimage classification, aiming to identify the optimal configuration. Numerical\nresults on the MNIST dataset unequivocally illustrate that our proposed\nPPF-QSNN outperforms both the existing spiking neural network and the serial\nquantum neural network across metrics such as accuracy, loss, and robustness.\nThis study introduces a novel and effective amalgamation approach for HQCNN,\nthereby laying the groundwork for the advancement and application of quantum\nadvantage in artificial intelligent computations.",
    "categories": [
      "quant-ph",
      "cs.AI",
      "cs.NE"
    ],
    "primary_category": "quant-ph",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2404.01359v1",
    "published_date": "2024-04-01 10:35:35 UTC",
    "updated_date": "2024-04-01 10:35:35 UTC"
  },
  {
    "arxiv_id": "2404.01030v3",
    "title": "Survey of Bias In Text-to-Image Generation: Definition, Evaluation, and Mitigation",
    "authors": [
      "Yixin Wan",
      "Arjun Subramonian",
      "Anaelia Ovalle",
      "Zongyu Lin",
      "Ashima Suvarna",
      "Christina Chance",
      "Hritik Bansal",
      "Rebecca Pattichis",
      "Kai-Wei Chang"
    ],
    "abstract": "The recent advancement of large and powerful models with Text-to-Image (T2I)\ngeneration abilities -- such as OpenAI's DALLE-3 and Google's Gemini -- enables\nusers to generate high-quality images from textual prompts. However, it has\nbecome increasingly evident that even simple prompts could cause T2I models to\nexhibit conspicuous social bias in generated images. Such bias might lead to\nboth allocational and representational harms in society, further marginalizing\nminority groups. Noting this problem, a large body of recent works has been\ndedicated to investigating different dimensions of bias in T2I systems.\nHowever, an extensive review of these studies is lacking, hindering a\nsystematic understanding of current progress and research gaps. We present the\nfirst extensive survey on bias in T2I generative models. In this survey, we\nreview prior studies on dimensions of bias: Gender, Skintone, and Geo-Culture.\nSpecifically, we discuss how these works define, evaluate, and mitigate\ndifferent aspects of bias. We found that: (1) while gender and skintone biases\nare widely studied, geo-cultural bias remains under-explored; (2) most works on\ngender and skintone bias investigated occupational association, while other\naspects are less frequently studied; (3) almost all gender bias works overlook\nnon-binary identities in their studies; (4) evaluation datasets and metrics are\nscattered, with no unified framework for measuring biases; and (5) current\nmitigation methods fail to resolve biases comprehensively. Based on current\nlimitations, we point out future research directions that contribute to\nhuman-centric definitions, evaluations, and mitigation of biases. We hope to\nhighlight the importance of studying biases in T2I systems, as well as\nencourage future efforts to holistically understand and tackle biases, building\nfair and trustworthy T2I technologies for everyone.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CY"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2404.01030v3",
    "published_date": "2024-04-01 10:19:05 UTC",
    "updated_date": "2024-05-01 23:58:22 UTC"
  },
  {
    "arxiv_id": "2404.01358v1",
    "title": "Utilizing AI and Social Media Analytics to Discover Adverse Side Effects of GLP-1 Receptor Agonists",
    "authors": [
      "Alon Bartal",
      "Kathleen M. Jagodnik",
      "Nava Pliskin",
      "Abraham Seidmann"
    ],
    "abstract": "Adverse side effects (ASEs) of drugs, revealed after FDA approval, pose a\nthreat to patient safety. To promptly detect overlooked ASEs, we developed a\ndigital health methodology capable of analyzing massive public data from social\nmedia, published clinical research, manufacturers' reports, and ChatGPT. We\nuncovered ASEs associated with the glucagon-like peptide 1 receptor agonists\n(GLP-1 RA), a market expected to grow exponentially to $133.5 billion USD by\n2030. Using a Named Entity Recognition (NER) model, our method successfully\ndetected 21 potential ASEs overlooked upon FDA approval, including irritability\nand numbness. Our data-analytic approach revolutionizes the detection of\nunreported ASEs associated with newly deployed drugs, leveraging cutting-edge\nAI-driven social media analytics. It can increase the safety of new drugs in\nthe marketplace by unlocking the power of social media to support regulators\nand manufacturers in the rapid discovery of hidden ASE risks.",
    "categories": [
      "q-bio.QM",
      "cs.AI",
      "cs.CL",
      "cs.IR",
      "cs.LG",
      "cs.SI",
      "62"
    ],
    "primary_category": "q-bio.QM",
    "comment": "19 pages, 7 figures, 3 tables, 1 Appendix table",
    "pdf_url": "http://arxiv.org/pdf/2404.01358v1",
    "published_date": "2024-04-01 09:48:14 UTC",
    "updated_date": "2024-04-01 09:48:14 UTC"
  },
  {
    "arxiv_id": "2404.01019v3",
    "title": "Source-Aware Training Enables Knowledge Attribution in Language Models",
    "authors": [
      "Muhammad Khalifa",
      "David Wadden",
      "Emma Strubell",
      "Honglak Lee",
      "Lu Wang",
      "Iz Beltagy",
      "Hao Peng"
    ],
    "abstract": "Large language models (LLMs) learn a vast amount of knowledge during\npretraining, but they are often oblivious to the source(s) of such knowledge.\nWe investigate the problem of intrinsic source citation, where LLMs are\nrequired to cite the pretraining source supporting a generated response.\nIntrinsic source citation can enhance LLM transparency, interpretability, and\nverifiability. To give LLMs such ability, we explore source-aware training -- a\nrecipe that involves (i) training the LLM to associate unique source document\nidentifiers with the knowledge in each document, followed by (ii) an\ninstruction-tuning stage to teach the LLM to cite a supporting pretraining\nsource when prompted. Source-aware training borrows from existing\npretraining/fine-tuning frameworks and requires minimal changes to the model\narchitecture or implementation. Through experiments on synthetic data, we\ndemonstrate that our training recipe can enable faithful attribution to the\npretraining data without a substantial impact on the model's perplexity\ncompared to standard pretraining. Our findings also highlight the importance of\npretraining data augmentation in achieving attribution. Code and data available\nhere: \\url{https://github.com/mukhal/intrinsic-source-citation}",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "COLM '24",
    "pdf_url": "http://arxiv.org/pdf/2404.01019v3",
    "published_date": "2024-04-01 09:39:38 UTC",
    "updated_date": "2024-08-13 03:55:35 UTC"
  },
  {
    "arxiv_id": "2404.01013v1",
    "title": "Teeth-SEG: An Efficient Instance Segmentation Framework for Orthodontic Treatment based on Anthropic Prior Knowledge",
    "authors": [
      "Bo Zou",
      "Shaofeng Wang",
      "Hao Liu",
      "Gaoyue Sun",
      "Yajie Wang",
      "FeiFei Zuo",
      "Chengbin Quan",
      "Youjian Zhao"
    ],
    "abstract": "Teeth localization, segmentation, and labeling in 2D images have great\npotential in modern dentistry to enhance dental diagnostics, treatment\nplanning, and population-based studies on oral health. However, general\ninstance segmentation frameworks are incompetent due to 1) the subtle\ndifferences between some teeth' shapes (e.g., maxillary first premolar and\nsecond premolar), 2) the teeth's position and shape variation across subjects,\nand 3) the presence of abnormalities in the dentition (e.g., caries and\nedentulism). To address these problems, we propose a ViT-based framework named\nTeethSEG, which consists of stacked Multi-Scale Aggregation (MSA) blocks and an\nAnthropic Prior Knowledge (APK) layer. Specifically, to compose the two\nmodules, we design 1) a unique permutation-based upscaler to ensure high\nefficiency while establishing clear segmentation boundaries with 2) multi-head\nself/cross-gating layers to emphasize particular semantics meanwhile\nmaintaining the divergence between token embeddings. Besides, we collect 3) the\nfirst open-sourced intraoral image dataset IO150K, which comprises over 150k\nintraoral photos, and all photos are annotated by orthodontists using a\nhuman-machine hybrid algorithm. Experiments on IO150K demonstrate that our\nTeethSEG outperforms the state-of-the-art segmentation models on dental image\nsegmentation.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "This paper has been accepted by CVPR 2024",
    "pdf_url": "http://arxiv.org/pdf/2404.01013v1",
    "published_date": "2024-04-01 09:34:51 UTC",
    "updated_date": "2024-04-01 09:34:51 UTC"
  },
  {
    "arxiv_id": "2404.01012v2",
    "title": "Query Performance Prediction using Relevance Judgments Generated by Large Language Models",
    "authors": [
      "Chuan Meng",
      "Negar Arabzadeh",
      "Arian Askari",
      "Mohammad Aliannejadi",
      "Maarten de Rijke"
    ],
    "abstract": "Query performance prediction (QPP) aims to estimate the retrieval quality of\na search system for a query without human relevance judgments. Previous QPP\nmethods typically return a single scalar value and do not require the predicted\nvalues to approximate a specific information retrieval (IR) evaluation measure,\nleading to certain drawbacks: (i) a single scalar is insufficient to accurately\nrepresent different IR evaluation measures, especially when metrics do not\nhighly correlate, and (ii) a single scalar limits the interpretability of QPP\nmethods because solely using a scalar is insufficient to explain QPP results.\nTo address these issues, we propose a QPP framework using automatically\ngenerated relevance judgments (QPP-GenRE), which decomposes QPP into\nindependent subtasks of predicting the relevance of each item in a ranked list\nto a given query. This allows us to predict any IR evaluation measure using the\ngenerated relevance judgments as pseudo-labels. This also allows us to\ninterpret predicted IR evaluation measures, and identify, track and rectify\nerrors in generated relevance judgments to improve QPP quality. We predict an\nitem's relevance by using open-source large language models (LLMs) to ensure\nscientific reproducibility.\n  We face two main challenges: (i) excessive computational costs of judging an\nentire corpus for predicting a metric considering recall, and (ii) limited\nperformance in prompting open-source LLMs in a zero-/few-shot manner. To solve\nthe challenges, we devise an approximation strategy to predict an IR measure\nconsidering recall and propose to fine-tune open-source LLMs using\nhuman-labeled relevance judgments. Experiments on the TREC 2019-2022 deep\nlearning tracks show that QPP-GenRE achieves state-of-the-art QPP quality for\nboth lexical and neural rankers.",
    "categories": [
      "cs.IR",
      "cs.AI",
      "cs.CL",
      "cs.LG",
      "H.3.3"
    ],
    "primary_category": "cs.IR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2404.01012v2",
    "published_date": "2024-04-01 09:33:05 UTC",
    "updated_date": "2024-06-17 11:23:20 UTC"
  },
  {
    "arxiv_id": "2404.01356v1",
    "title": "The Double-Edged Sword of Input Perturbations to Robust Accurate Fairness",
    "authors": [
      "Xuran Li",
      "Peng Wu",
      "Yanting Chen",
      "Xingjun Ma",
      "Zhen Zhang",
      "Kaixiang Dong"
    ],
    "abstract": "Deep neural networks (DNNs) are known to be sensitive to adversarial input\nperturbations, leading to a reduction in either prediction accuracy or\nindividual fairness. To jointly characterize the susceptibility of prediction\naccuracy and individual fairness to adversarial perturbations, we introduce a\nnovel robustness definition termed robust accurate fairness. Informally, robust\naccurate fairness requires that predictions for an instance and its similar\ncounterparts consistently align with the ground truth when subjected to input\nperturbations. We propose an adversarial attack approach dubbed RAFair to\nexpose false or biased adversarial defects in DNN, which either deceive\naccuracy or compromise individual fairness. Then, we show that such adversarial\ninstances can be effectively addressed by carefully designed benign\nperturbations, correcting their predictions to be accurate and fair. Our work\nexplores the double-edged sword of input perturbations to robust accurate\nfairness in DNN and the potential of using benign perturbations to correct\nadversarial instances.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CY"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2404.01356v1",
    "published_date": "2024-04-01 09:29:16 UTC",
    "updated_date": "2024-04-01 09:29:16 UTC"
  },
  {
    "arxiv_id": "2407.00814v1",
    "title": "Privacy-Aware Spectrum Pricing and Power Control Optimization for LEO Satellite Internet-of-Things",
    "authors": [
      "Bowen Shen",
      "Kwok-Yan Lam",
      "Feng Li"
    ],
    "abstract": "Low earth orbit (LEO) satellite systems play an important role in next\ngeneration communication networks due to their ability to provide extensive\nglobal coverage with guaranteed communications in remote areas and isolated\nareas where base stations cannot be cost-efficiently deployed. With the\npervasive adoption of LEO satellite systems, especially in the LEO\nInternet-of-Things (IoT) scenarios, their spectrum resource management\nrequirements have become more complex as a result of massive service requests\nand high bandwidth demand from terrestrial terminals. For instance, when\nleasing the spectrum to terrestrial users and controlling the uplink transmit\npower, satellites collect user data for machine learning purposes, which\nusually are sensitive information such as location, budget and quality of\nservice (QoS) requirement. To facilitate model training in LEO IoT while\npreserving the privacy of data, blockchain-driven federated learning (FL) is\nwidely used by leveraging on a fully decentralized architecture. In this paper,\nwe propose a hybrid spectrum pricing and power control framework for LEO IoT by\ncombining blockchain technology and FL. We first design a local deep\nreinforcement learning algorithm for LEO satellite systems to learn a\nrevenue-maximizing pricing and power control scheme. Then the agents\ncollaborate to form a FL system. We also propose a reputation-based blockchain\nwhich is used in the global model aggregation phase of FL. Based on the\nreputation mechanism, a node is selected for each global training round to\nperform model aggregation and block generation, which can further enhance the\ndecentralization of the network and guarantee the trust. Simulation tests are\nconducted to evaluate the performances of the proposed scheme. Our results show\nthe efficiency of finding the maximum revenue scheme for LEO satellite systems\nwhile preserving the privacy of each agent.",
    "categories": [
      "cs.NI",
      "cs.AI"
    ],
    "primary_category": "cs.NI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.00814v1",
    "published_date": "2024-04-01 09:15:48 UTC",
    "updated_date": "2024-04-01 09:15:48 UTC"
  },
  {
    "arxiv_id": "2404.00998v1",
    "title": "LLM-RadJudge: Achieving Radiologist-Level Evaluation for X-Ray Report Generation",
    "authors": [
      "Zilong Wang",
      "Xufang Luo",
      "Xinyang Jiang",
      "Dongsheng Li",
      "Lili Qiu"
    ],
    "abstract": "Evaluating generated radiology reports is crucial for the development of\nradiology AI, but existing metrics fail to reflect the task's clinical\nrequirements. This study proposes a novel evaluation framework using large\nlanguage models (LLMs) to compare radiology reports for assessment. We compare\nthe performance of various LLMs and demonstrate that, when using GPT-4, our\nproposed metric achieves evaluation consistency close to that of radiologists.\nFurthermore, to reduce costs and improve accessibility, making this method\npractical, we construct a dataset using LLM evaluation results and perform\nknowledge distillation to train a smaller model. The distilled model achieves\nevaluation capabilities comparable to GPT-4. Our framework and distilled model\noffer an accessible and efficient evaluation method for radiology report\ngeneration, facilitating the development of more clinically relevant models.\nThe model will be further open-sourced and accessible.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "11 pages, 6 figures",
    "pdf_url": "http://arxiv.org/pdf/2404.00998v1",
    "published_date": "2024-04-01 09:02:12 UTC",
    "updated_date": "2024-04-01 09:02:12 UTC"
  },
  {
    "arxiv_id": "2404.00989v2",
    "title": "360+x: A Panoptic Multi-modal Scene Understanding Dataset",
    "authors": [
      "Hao Chen",
      "Yuqi Hou",
      "Chenyuan Qu",
      "Irene Testini",
      "Xiaohan Hong",
      "Jianbo Jiao"
    ],
    "abstract": "Human perception of the world is shaped by a multitude of viewpoints and\nmodalities. While many existing datasets focus on scene understanding from a\ncertain perspective (e.g. egocentric or third-person views), our dataset offers\na panoptic perspective (i.e. multiple viewpoints with multiple data\nmodalities). Specifically, we encapsulate third-person panoramic and front\nviews, as well as egocentric monocular/binocular views with rich modalities\nincluding video, multi-channel audio, directional binaural delay, location data\nand textual scene descriptions within each scene captured, presenting\ncomprehensive observation of the world. Figure 1 offers a glimpse of all 28\nscene categories of our 360+x dataset. To the best of our knowledge, this is\nthe first database that covers multiple viewpoints with multiple data\nmodalities to mimic how daily information is accessed in the real world.\nThrough our benchmark analysis, we presented 5 different scene understanding\ntasks on the proposed 360+x dataset to evaluate the impact and benefit of each\ndata modality and perspective in panoptic scene understanding. We hope this\nunique dataset could broaden the scope of comprehensive scene understanding and\nencourage the community to approach these problems from more diverse\nperspectives.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.MM",
      "cs.SD",
      "eess.AS"
    ],
    "primary_category": "cs.CV",
    "comment": "CVPR 2024 (Oral Presentation), Project page:\n  https://x360dataset.github.io/",
    "pdf_url": "http://arxiv.org/pdf/2404.00989v2",
    "published_date": "2024-04-01 08:34:42 UTC",
    "updated_date": "2024-04-08 02:37:25 UTC"
  },
  {
    "arxiv_id": "2404.00983v1",
    "title": "Continual Learning for Smart City: A Survey",
    "authors": [
      "Li Yang",
      "Zhipeng Luo",
      "Shiming Zhang",
      "Fei Teng",
      "Tianrui Li"
    ],
    "abstract": "With the digitization of modern cities, large data volumes and powerful\ncomputational resources facilitate the rapid update of intelligent models\ndeployed in smart cities. Continual learning (CL) is a novel machine learning\nparadigm that constantly updates models to adapt to changing environments,\nwhere the learning tasks, data, and distributions can vary over time. Our\nsurvey provides a comprehensive review of continual learning methods that are\nwidely used in smart city development. The content consists of three parts: 1)\nMethodology-wise. We categorize a large number of basic CL methods and advanced\nCL frameworks in combination with other learning paradigms including graph\nlearning, spatial-temporal learning, multi-modal learning, and federated\nlearning. 2) Application-wise. We present numerous CL applications covering\ntransportation, environment, public health, safety, networks, and associated\ndatasets related to urban computing. 3) Challenges. We discuss current problems\nand challenges and envision several promising research directions. We believe\nthis survey can help relevant researchers quickly familiarize themselves with\nthe current state of continual learning research used in smart city development\nand direct them to future research trends.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "Preprint. Work in Progress",
    "pdf_url": "http://arxiv.org/pdf/2404.00983v1",
    "published_date": "2024-04-01 07:59:29 UTC",
    "updated_date": "2024-04-01 07:59:29 UTC"
  },
  {
    "arxiv_id": "2404.00977v2",
    "title": "Nonlinear dynamical social and political prediction algorithm for city planning and public participation using the Impulse Pattern Formulation",
    "authors": [
      "Rolf Bader",
      "Simon Linke",
      "Stefanie Gernert"
    ],
    "abstract": "A nonlinear-dynamical algorithm for city planning is proposed as an Impulse\nPattern Formulation (IPF) for predicting relevant parameters like health,\nartistic freedom, or financial developments of different social or political\nstakeholders over the cause of a planning process. The IPF has already shown\nhigh predictive precision at low computational cost in musical instrument\nsimulations, brain dynamics, and human-human interactions. The social and\npolitical IPF consists of three basic equations of system state developments,\nself-adaptation of stakeholders, two adaptive interactions, and external impact\nterms suitable for respective planning situations. Typical scenarios of\nstakeholder interactions and developments are modeled by adjusting a set of\nsystem parameters. These include stakeholder reaction to external input,\nenhanced system stability through self-adaptation, stakeholder convergence due\nto adaptive interaction, as well as complex dynamics in terms of fixed\nstakeholder impacts. A workflow for implementing the algorithm in real city\nplanning scenarios is outlined. This workflow includes machine learning of a\nsuitable set of parameters suggesting best-practice planning to aim at the\ndesired development of the planning process and its output.",
    "categories": [
      "nlin.AO",
      "cs.AI",
      "math.DS"
    ],
    "primary_category": "nlin.AO",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2404.00977v2",
    "published_date": "2024-04-01 07:49:10 UTC",
    "updated_date": "2024-06-14 18:47:45 UTC"
  },
  {
    "arxiv_id": "2404.01353v1",
    "title": "Efficiently Distilling LLMs for Edge Applications",
    "authors": [
      "Achintya Kundu",
      "Fabian Lim",
      "Aaron Chew",
      "Laura Wynter",
      "Penny Chong",
      "Rhui Dih Lee"
    ],
    "abstract": "Supernet training of LLMs is of great interest in industrial applications as\nit confers the ability to produce a palette of smaller models at constant cost,\nregardless of the number of models (of different size / latency) produced. We\npropose a new method called Multistage Low-rank Fine-tuning of\nSuper-transformers (MLFS) for parameter-efficient supernet training. We show\nthat it is possible to obtain high-quality encoder models that are suitable for\ncommercial edge applications, and that while decoder-only models are resistant\nto a comparable degree of compression, decoders can be effectively sliced for a\nsignificant reduction in training time.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.LG",
    "comment": "This paper has been accepted for publication in NAACL 2024 (Industry\n  Track)",
    "pdf_url": "http://arxiv.org/pdf/2404.01353v1",
    "published_date": "2024-04-01 07:35:15 UTC",
    "updated_date": "2024-04-01 07:35:15 UTC"
  },
  {
    "arxiv_id": "2404.00971v2",
    "title": "Exploring and Evaluating Hallucinations in LLM-Powered Code Generation",
    "authors": [
      "Fang Liu",
      "Yang Liu",
      "Lin Shi",
      "Houkun Huang",
      "Ruifeng Wang",
      "Zhen Yang",
      "Li Zhang",
      "Zhongqi Li",
      "Yuchi Ma"
    ],
    "abstract": "The rise of Large Language Models (LLMs) has significantly advanced many\napplications on software engineering tasks, particularly in code generation.\nDespite the promising performance, LLMs are prone to generate hallucinations,\nwhich means LLMs might produce outputs that deviate from users' intent, exhibit\ninternal inconsistencies, or misalign with the factual knowledge, making the\ndeployment of LLMs potentially risky in a wide range of applications. Existing\nwork mainly focuses on investing the hallucination in the domain of natural\nlanguage generation (NLG), leaving a gap in understanding the types and extent\nof hallucinations in the context of code generation. To bridge the gap, we\nconducted a thematic analysis of the LLM-generated code to summarize and\ncategorize the hallucinations present in it. Our study established a\ncomprehensive taxonomy of hallucinations in LLM-generated code, encompassing 5\nprimary categories of hallucinations depending on the conflicting objectives\nand varying degrees of deviation observed in code generation. Furthermore, we\nsystematically analyzed the distribution of hallucinations, exploring\nvariations among different LLMs and their correlation with code correctness.\nBased on the results, we proposed HalluCode, a benchmark for evaluating the\nperformance of code LLMs in recognizing hallucinations. Hallucination\nrecognition and mitigation experiments with HalluCode and HumanEval show\nexisting LLMs face great challenges in recognizing hallucinations, particularly\nin identifying their types, and are hardly able to mitigate hallucinations. We\nbelieve our findings will shed light on future research about hallucination\nevaluation, detection, and mitigation, ultimately paving the way for building\nmore effective and reliable code LLMs in the future.",
    "categories": [
      "cs.SE",
      "cs.AI"
    ],
    "primary_category": "cs.SE",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2404.00971v2",
    "published_date": "2024-04-01 07:31:45 UTC",
    "updated_date": "2024-05-11 02:50:50 UTC"
  },
  {
    "arxiv_id": "2404.03683v1",
    "title": "Stream of Search (SoS): Learning to Search in Language",
    "authors": [
      "Kanishk Gandhi",
      "Denise Lee",
      "Gabriel Grand",
      "Muxin Liu",
      "Winson Cheng",
      "Archit Sharma",
      "Noah D. Goodman"
    ],
    "abstract": "Language models are rarely shown fruitful mistakes while training. They then\nstruggle to look beyond the next token, suffering from a snowballing of errors\nand struggling to predict the consequence of their actions several steps ahead.\nIn this paper, we show how language models can be taught to search by\nrepresenting the process of search in language, as a flattened string -- a\nstream of search (SoS). We propose a unified language for search that captures\nan array of different symbolic search strategies. We demonstrate our approach\nusing the simple yet difficult game of Countdown, where the goal is to combine\ninput numbers with arithmetic operations to reach a target number. We pretrain\na transformer-based language model from scratch on a dataset of streams of\nsearch generated by heuristic solvers. We find that SoS pretraining increases\nsearch accuracy by 25% over models trained to predict only the optimal search\ntrajectory. We further finetune this model with two policy improvement methods:\nAdvantage-Induced Policy Alignment (APA) and Self-Taught Reasoner (STaR). The\nfinetuned SoS models solve 36% of previously unsolved problems, including\nproblems that cannot be solved by any of the heuristic solvers. Our results\nindicate that language models can learn to solve problems via search,\nself-improve to flexibly use different search strategies, and potentially\ndiscover new ones.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2404.03683v1",
    "published_date": "2024-04-01 06:50:52 UTC",
    "updated_date": "2024-04-01 06:50:52 UTC"
  },
  {
    "arxiv_id": "2404.00943v2",
    "title": "Evalverse: Unified and Accessible Library for Large Language Model Evaluation",
    "authors": [
      "Jihoo Kim",
      "Wonho Song",
      "Dahyun Kim",
      "Yunsu Kim",
      "Yungi Kim",
      "Chanjun Park"
    ],
    "abstract": "This paper introduces Evalverse, a novel library that streamlines the\nevaluation of Large Language Models (LLMs) by unifying disparate evaluation\ntools into a single, user-friendly framework. Evalverse enables individuals\nwith limited knowledge of artificial intelligence to easily request LLM\nevaluations and receive detailed reports, facilitated by an integration with\ncommunication platforms like Slack. Thus, Evalverse serves as a powerful tool\nfor the comprehensive assessment of LLMs, offering both researchers and\npractitioners a centralized and easily accessible evaluation framework.\nFinally, we also provide a demo video for Evalverse, showcasing its\ncapabilities and implementation in a two-minute format.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted to EMNLP 2024 Demo Track",
    "pdf_url": "http://arxiv.org/pdf/2404.00943v2",
    "published_date": "2024-04-01 06:03:39 UTC",
    "updated_date": "2024-10-07 02:47:36 UTC"
  },
  {
    "arxiv_id": "2404.00942v1",
    "title": "Evaluating the Factuality of Large Language Models using Large-Scale Knowledge Graphs",
    "authors": [
      "Xiaoze Liu",
      "Feijie Wu",
      "Tianyang Xu",
      "Zhuo Chen",
      "Yichi Zhang",
      "Xiaoqian Wang",
      "Jing Gao"
    ],
    "abstract": "The advent of Large Language Models (LLMs) has significantly transformed the\nAI landscape, enhancing machine learning and AI capabilities. Factuality issue\nis a critical concern for LLMs, as they may generate factually incorrect\nresponses. In this paper, we propose GraphEval to evaluate an LLM's performance\nusing a substantially large test dataset. Specifically, the test dataset is\nretrieved from a large knowledge graph with more than 10 million facts without\nexpensive human efforts. Unlike conventional methods that evaluate LLMs based\non generated responses, GraphEval streamlines the evaluation process by\ncreating a judge model to estimate the correctness of the answers given by the\nLLM. Our experiments demonstrate that the judge model's factuality assessment\naligns closely with the correctness of the LLM's generated outputs, while also\nsubstantially reducing evaluation costs. Besides, our findings offer valuable\ninsights into LLM performance across different metrics and highlight the\npotential for future improvements in ensuring the factual integrity of LLM\noutputs. The code is publicly available at https://github.com/xz-liu/GraphEval.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2404.00942v1",
    "published_date": "2024-04-01 06:01:17 UTC",
    "updated_date": "2024-04-01 06:01:17 UTC"
  },
  {
    "arxiv_id": "2404.00929v3",
    "title": "A Survey on Multilingual Large Language Models: Corpora, Alignment, and Bias",
    "authors": [
      "Yuemei Xu",
      "Ling Hu",
      "Jiayi Zhao",
      "Zihan Qiu",
      "Kexin XU",
      "Yuqi Ye",
      "Hanwen Gu"
    ],
    "abstract": "Based on the foundation of Large Language Models (LLMs), Multilingual LLMs\n(MLLMs) have been developed to address the challenges faced in multilingual\nnatural language processing, hoping to achieve knowledge transfer from\nhigh-resource languages to low-resource languages. However, significant\nlimitations and challenges still exist, such as language imbalance,\nmultilingual alignment, and inherent bias. In this paper, we aim to provide a\ncomprehensive analysis of MLLMs, delving deeply into discussions surrounding\nthese critical issues. First of all, we start by presenting an overview of\nMLLMs, covering their evolutions, key techniques, and multilingual capacities.\nSecondly, we explore the multilingual training corpora of MLLMs and the\nmultilingual datasets oriented for downstream tasks that are crucial to enhance\nthe cross-lingual capability of MLLMs. Thirdly, we survey the state-of-the-art\nstudies of multilingual representations and investigate whether the current\nMLLMs can learn a universal language representation. Fourthly, we discuss bias\non MLLMs, including its categories, evaluation metrics, and debiasing\ntechniques. Finally, we discuss existing challenges and point out promising\nresearch directions of MLLMs.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "The article has been accepted by Frontiers of Computer Science (FCS),\n  with the DOI: {10.1007/s11704-024-40579-4}",
    "pdf_url": "http://arxiv.org/pdf/2404.00929v3",
    "published_date": "2024-04-01 05:13:56 UTC",
    "updated_date": "2024-12-09 14:30:11 UTC"
  },
  {
    "arxiv_id": "2404.01352v1",
    "title": "VortexViz: Finding Vortex Boundaries by Learning from Particle Trajectories",
    "authors": [
      "Akila de Silva",
      "Nicholas Tee",
      "Omkar Ghanekar",
      "Fahim Hasan Khan",
      "Gregory Dusek",
      "James Davis",
      "Alex Pang"
    ],
    "abstract": "Vortices are studied in various scientific disciplines, offering insights\ninto fluid flow behavior. Visualizing the boundary of vortices is crucial for\nunderstanding flow phenomena and detecting flow irregularities. This paper\naddresses the challenge of accurately extracting vortex boundaries using deep\nlearning techniques. While existing methods primarily train on velocity\ncomponents, we propose a novel approach incorporating particle trajectories\n(streamlines or pathlines) into the learning process. By leveraging the\nregional/local characteristics of the flow field captured by streamlines or\npathlines, our methodology aims to enhance the accuracy of vortex boundary\nextraction.",
    "categories": [
      "physics.flu-dyn",
      "cs.AI",
      "cs.CV",
      "cs.GR"
    ],
    "primary_category": "physics.flu-dyn",
    "comment": "Under review",
    "pdf_url": "http://arxiv.org/pdf/2404.01352v1",
    "published_date": "2024-04-01 05:12:55 UTC",
    "updated_date": "2024-04-01 05:12:55 UTC"
  },
  {
    "arxiv_id": "2404.00923v1",
    "title": "MM3DGS SLAM: Multi-modal 3D Gaussian Splatting for SLAM Using Vision, Depth, and Inertial Measurements",
    "authors": [
      "Lisong C. Sun",
      "Neel P. Bhatt",
      "Jonathan C. Liu",
      "Zhiwen Fan",
      "Zhangyang Wang",
      "Todd E. Humphreys",
      "Ufuk Topcu"
    ],
    "abstract": "Simultaneous localization and mapping is essential for position tracking and\nscene understanding. 3D Gaussian-based map representations enable\nphotorealistic reconstruction and real-time rendering of scenes using multiple\nposed cameras. We show for the first time that using 3D Gaussians for map\nrepresentation with unposed camera images and inertial measurements can enable\naccurate SLAM. Our method, MM3DGS, addresses the limitations of prior neural\nradiance field-based representations by enabling faster rendering, scale\nawareness, and improved trajectory tracking. Our framework enables\nkeyframe-based mapping and tracking utilizing loss functions that incorporate\nrelative pose transformations from pre-integrated inertial measurements, depth\nestimates, and measures of photometric rendering quality. We also release a\nmulti-modal dataset, UT-MM, collected from a mobile robot equipped with a\ncamera and an inertial measurement unit. Experimental evaluation on several\nscenes from the dataset shows that MM3DGS achieves 3x improvement in tracking\nand 5% improvement in photometric rendering quality compared to the current\n3DGS SLAM state-of-the-art, while allowing real-time rendering of a\nhigh-resolution dense 3D map. Project Webpage:\nhttps://vita-group.github.io/MM3DGS-SLAM",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.RO"
    ],
    "primary_category": "cs.CV",
    "comment": "Project Webpage: https://vita-group.github.io/MM3DGS-SLAM",
    "pdf_url": "http://arxiv.org/pdf/2404.00923v1",
    "published_date": "2024-04-01 04:57:41 UTC",
    "updated_date": "2024-04-01 04:57:41 UTC"
  },
  {
    "arxiv_id": "2404.00914v1",
    "title": "Token-Efficient Leverage Learning in Large Language Models",
    "authors": [
      "Yuanhao Zeng",
      "Min Wang",
      "Yihang Wang",
      "Yingxia Shao"
    ],
    "abstract": "Large Language Models (LLMs) have excelled in various tasks but perform\nbetter in high-resource scenarios, which presents challenges in low-resource\nscenarios. Data scarcity and the inherent difficulty of adapting LLMs to\nspecific tasks compound the challenge. To address the twin hurdles, we\nintroduce \\textbf{Leverage Learning}. We present a streamlined implement of\nthis methodology called Token-Efficient Leverage Learning (TELL). TELL\nshowcases the potential of Leverage Learning, demonstrating effectiveness\nacross various LLMs and low-resource tasks, ranging from $10^4$ to $10^6$\ntokens. It reduces task data requirements by up to nearly an order of magnitude\ncompared to conventional Supervised Fine-Tuning (SFT) while delivering\ncompetitive performance. With the same amount of task data, TELL leads in\nimproving task performance compared to SFT. We discuss the mechanism of\nLeverage Learning, suggesting it aligns with quantization hypothesis and\nexplore its promising potential through empirical testing.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "15 pages, 16 figures",
    "pdf_url": "http://arxiv.org/pdf/2404.00914v1",
    "published_date": "2024-04-01 04:39:44 UTC",
    "updated_date": "2024-04-01 04:39:44 UTC"
  },
  {
    "arxiv_id": "2404.00913v1",
    "title": "LLaMA-Excitor: General Instruction Tuning via Indirect Feature Interaction",
    "authors": [
      "Bo Zou",
      "Chao Yang",
      "Yu Qiao",
      "Chengbin Quan",
      "Youjian Zhao"
    ],
    "abstract": "Existing methods to fine-tune LLMs, like Adapter, Prefix-tuning, and LoRA,\nwhich introduce extra modules or additional input sequences to inject new\nskills or knowledge, may compromise the innate abilities of LLMs. In this\npaper, we propose LLaMA-Excitor, a lightweight method that stimulates the LLMs'\npotential to better follow instructions by gradually paying more attention to\nworthwhile information. Specifically, the LLaMA-Excitor does not directly\nchange the intermediate hidden state during the self-attention calculation of\nthe transformer structure. We designed the Excitor block as a bypass module for\nthe similarity score computation in LLMs' self-attention to reconstruct keys\nand change the importance of values by learnable prompts. LLaMA-Excitor ensures\na self-adaptive allocation of additional attention to input instructions, thus\neffectively preserving LLMs' pre-trained knowledge when fine-tuning LLMs on\nlow-quality instruction-following datasets. Furthermore, we unify the modeling\nof multi-modal tuning and language-only tuning, extending LLaMA-Excitor to a\npowerful visual instruction follower without the need for complex multi-modal\nalignment. Our proposed approach is evaluated in language-only and multi-modal\ntuning experimental scenarios. Notably, LLaMA-Excitor is the only method that\nmaintains basic capabilities while achieving a significant improvement (+6%) on\nthe MMLU benchmark. In the visual instruction tuning, we achieve a new\nstate-of-the-art image captioning performance of 157.5 CIDEr on MSCOCO, and a\ncomparable performance (88.39%) on ScienceQA to cutting-edge models with more\nparameters and extensive vision-language pertaining.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.CV",
    "comment": "This paper is accepted by CVPR 2024",
    "pdf_url": "http://arxiv.org/pdf/2404.00913v1",
    "published_date": "2024-04-01 04:39:21 UTC",
    "updated_date": "2024-04-01 04:39:21 UTC"
  },
  {
    "arxiv_id": "2404.01351v1",
    "title": "AETTA: Label-Free Accuracy Estimation for Test-Time Adaptation",
    "authors": [
      "Taeckyung Lee",
      "Sorn Chottananurak",
      "Taesik Gong",
      "Sung-Ju Lee"
    ],
    "abstract": "Test-time adaptation (TTA) has emerged as a viable solution to adapt\npre-trained models to domain shifts using unlabeled test data. However, TTA\nfaces challenges of adaptation failures due to its reliance on blind adaptation\nto unknown test samples in dynamic scenarios. Traditional methods for\nout-of-distribution performance estimation are limited by unrealistic\nassumptions in the TTA context, such as requiring labeled data or re-training\nmodels. To address this issue, we propose AETTA, a label-free accuracy\nestimation algorithm for TTA. We propose the prediction disagreement as the\naccuracy estimate, calculated by comparing the target model prediction with\ndropout inferences. We then improve the prediction disagreement to extend the\napplicability of AETTA under adaptation failures. Our extensive evaluation with\nfour baselines and six TTA methods demonstrates that AETTA shows an average of\n19.8%p more accurate estimation compared with the baselines. We further\ndemonstrate the effectiveness of accuracy estimation with a model recovery case\nstudy, showcasing the practicality of our model recovery based on accuracy\nestimation. The source code is available at https://github.com/taeckyung/AETTA.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted to CVPR 2024",
    "pdf_url": "http://arxiv.org/pdf/2404.01351v1",
    "published_date": "2024-04-01 04:21:49 UTC",
    "updated_date": "2024-04-01 04:21:49 UTC"
  },
  {
    "arxiv_id": "2404.00903v1",
    "title": "Maximizing User Experience with LLMOps-Driven Personalized Recommendation Systems",
    "authors": [
      "Chenxi Shi",
      "Penghao Liang",
      "Yichao Wu",
      "Tong Zhan",
      "Zhengyu Jin"
    ],
    "abstract": "The integration of LLMOps into personalized recommendation systems marks a\nsignificant advancement in managing LLM-driven applications. This innovation\npresents both opportunities and challenges for enterprises, requiring\nspecialized teams to navigate the complexity of engineering technology while\nprioritizing data security and model interpretability. By leveraging LLMOps,\nenterprises can enhance the efficiency and reliability of large-scale machine\nlearning models, driving personalized recommendations aligned with user\npreferences. Despite ethical considerations, LLMOps is poised for widespread\nadoption, promising more efficient and secure machine learning services that\nelevate user experience and shape the future of personalized recommendation\nsystems.",
    "categories": [
      "cs.IR",
      "cs.AI"
    ],
    "primary_category": "cs.IR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2404.00903v1",
    "published_date": "2024-04-01 04:13:42 UTC",
    "updated_date": "2024-04-01 04:13:42 UTC"
  },
  {
    "arxiv_id": "2404.00897v3",
    "title": "Machine Learning Robustness: A Primer",
    "authors": [
      "Houssem Ben Braiek",
      "Foutse Khomh"
    ],
    "abstract": "This chapter explores the foundational concept of robustness in Machine\nLearning (ML) and its integral role in establishing trustworthiness in\nArtificial Intelligence (AI) systems. The discussion begins with a detailed\ndefinition of robustness, portraying it as the ability of ML models to maintain\nstable performance across varied and unexpected environmental conditions. ML\nrobustness is dissected through several lenses: its complementarity with\ngeneralizability; its status as a requirement for trustworthy AI; its\nadversarial vs non-adversarial aspects; its quantitative metrics; and its\nindicators such as reproducibility and explainability. The chapter delves into\nthe factors that impede robustness, such as data bias, model complexity, and\nthe pitfalls of underspecified ML pipelines. It surveys key techniques for\nrobustness assessment from a broad perspective, including adversarial attacks,\nencompassing both digital and physical realms. It covers non-adversarial data\nshifts and nuances of Deep Learning (DL) software testing methodologies. The\ndiscussion progresses to explore amelioration strategies for bolstering\nrobustness, starting with data-centric approaches like debiasing and\naugmentation. Further examination includes a variety of model-centric methods\nsuch as transfer learning, adversarial training, and randomized smoothing.\nLastly, post-training methods are discussed, including ensemble techniques,\npruning, and model repairs, emerging as cost-effective strategies to make\nmodels more resilient against the unpredictable. This chapter underscores the\nongoing challenges and limitations in estimating and achieving ML robustness by\nexisting approaches. It offers insights and directions for future research on\nthis crucial concept, as a prerequisite for trustworthy AI systems.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.SE"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2404.00897v3",
    "published_date": "2024-04-01 03:49:42 UTC",
    "updated_date": "2024-05-04 00:33:06 UTC"
  },
  {
    "arxiv_id": "2404.00886v1",
    "title": "MTLight: Efficient Multi-Task Reinforcement Learning for Traffic Signal Control",
    "authors": [
      "Liwen Zhu",
      "Peixi Peng",
      "Zongqing Lu",
      "Yonghong Tian"
    ],
    "abstract": "Traffic signal control has a great impact on alleviating traffic congestion\nin modern cities. Deep reinforcement learning (RL) has been widely used for\nthis task in recent years, demonstrating promising performance but also facing\nmany challenges such as limited performances and sample inefficiency. To handle\nthese challenges, MTLight is proposed to enhance the agent observation with a\nlatent state, which is learned from numerous traffic indicators. Meanwhile,\nmultiple auxiliary and supervisory tasks are constructed to learn the latent\nstate, and two types of embedding latent features, the task-specific feature\nand task-shared feature, are used to make the latent state more abundant.\nExtensive experiments conducted on CityFlow demonstrate that MTLight has\nleading convergence speed and asymptotic performance. We further simulate under\npeak-hour pattern in all scenarios with increasing control difficulty and the\nresults indicate that MTLight is highly adaptable.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2404.00886v1",
    "published_date": "2024-04-01 03:27:46 UTC",
    "updated_date": "2024-04-01 03:27:46 UTC"
  },
  {
    "arxiv_id": "2404.00884v1",
    "title": "Self-Demos: Eliciting Out-of-Demonstration Generalizability in Large Language Models",
    "authors": [
      "Wei He",
      "Shichun Liu",
      "Jun Zhao",
      "Yiwen Ding",
      "Yi Lu",
      "Zhiheng Xi",
      "Tao Gui",
      "Qi Zhang",
      "Xuanjing Huang"
    ],
    "abstract": "Large language models (LLMs) have shown promising abilities of in-context\nlearning (ICL), adapting swiftly to new tasks with only few-shot\ndemonstrations. However, current few-shot methods heavily depend on\nhigh-quality, query-specific demos, which are often lacking. When faced with\nout-of-demonstration (OOD) queries, methods that rely on hand-crafted demos or\nexternal retrievers might fail. To bridge the gap between limited demos and OOD\nqueries, we propose Self-Demos, a novel prompting method that elicits the\ninherent generalizability in LLMs by query-aware demo generation. The generated\ndemos strategically interpolate between existing demos and the given query,\ntransforming the query from OOD to ID. To evaluate the effectiveness of our\napproach, we manually constructed OOD-Toolset, a dataset in the tool-using\nscenario with over 300 real-world APIs and 1000 instances, each consisting of\nthree tool-use cases as demos and an OOD query. Thorough experiments on our\ndataset and two public math benchmarks have shown that our method can\noutperform state-of-the-art baselines in the OOD setting. Moreover, we conduct\na range of analyses to validate Self-Demos's generalization and provide more\ninsights.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted to NAACL 2024 Findings",
    "pdf_url": "http://arxiv.org/pdf/2404.00884v1",
    "published_date": "2024-04-01 03:25:06 UTC",
    "updated_date": "2024-04-01 03:25:06 UTC"
  },
  {
    "arxiv_id": "2405.01561v1",
    "title": "Rapid Mobile App Development for Generative AI Agents on MIT App Inventor",
    "authors": [
      "Jaida Gao",
      "Calab Su",
      "Etai Miller",
      "Kevin Lu",
      "Yu Meng"
    ],
    "abstract": "The evolution of Artificial Intelligence (AI) stands as a pivotal force\nshaping our society, finding applications across diverse domains such as\neducation, sustainability, and safety. Leveraging AI within mobile applications\nmakes it easily accessible to the public, catalyzing its transformative\npotential. In this paper, we present a methodology for the rapid development of\nAI agent applications using the development platform provided by MIT App\nInventor. To demonstrate its efficacy, we share the development journey of\nthree distinct mobile applications: SynchroNet for fostering sustainable\ncommunities; ProductiviTeams for addressing procrastination; and iHELP for\nenhancing community safety. All three applications seamlessly integrate a\nspectrum of generative AI features, leveraging OpenAI APIs. Furthermore, we\noffer insights gleaned from overcoming challenges in integrating diverse tools\nand AI functionalities, aiming to inspire young developers to join our efforts\nin building practical AI agent applications.",
    "categories": [
      "cs.SE",
      "cs.AI",
      "cs.CY"
    ],
    "primary_category": "cs.SE",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2405.01561v1",
    "published_date": "2024-04-01 02:35:19 UTC",
    "updated_date": "2024-04-01 02:35:19 UTC"
  },
  {
    "arxiv_id": "2404.00862v1",
    "title": "Bailong: Bilingual Transfer Learning based on QLoRA and Zip-tie Embedding",
    "authors": [
      "Lung-Chuan Chen",
      "Zong-Ru Li"
    ],
    "abstract": "Large language models (LLMs) have demonstrated exceptional performance in\nvarious NLP applications. However, the majority of existing open-source LLMs\nare pre-trained primarily on English data and little part of other languages.\nThis deficiency in multilingual training data results in suboptimal performance\nwhen applied to languages with fewer available resources. Furthermore,\nenhancing the performance of LLMs on low-resource languages by full-parameter\nfine-tuning with additional data requires substantial computational resources,\nposing computational barriers for research organizations and individual\nresearchers. Consequently, several techniques such as parameter-efficient\ntuning and advanced embedding initialization have been proposed to address\nthese challenges. In this work, we combine them to facilitate cross-lingual\ntransfer on English-dominated open-source LLM. To effectively enhance the\nmodel's proficiency in Traditional Chinese, we conduct secondary pre-training\non Llama 2 7B with Traditional Chinese data by leveraging QLoRA and our\nproposed zip-tie embedding initialization. The resulting model called Bailong,\nwhich stands for Bilingual trAnsfer learnIng based on qLOra and zip-tie\nembeddiNG. We present Bailong-instruct 7B, a fine-tuned version of Bailong 7B\noptimized for multi-turn dialogue scenarios. Recognizing the inadequacy of\nbenchmark datasets in Traditional Chinese, we further introduce Bailong-bench\nto assess the alignment of models with human preferences and the capability to\nfollow instructions in both Traditional Chinese and English tasks. In our\nevaluation, Bailong-instruct 7B exhibits competitive performance on\nBailong-bench and other benchmark datasets when compared to other open-source\nmodels of similar or even larger parameter sizes. Bailong-instruct 7B and\nBailong-bench are publicly available with the aim of empowering the community\nto build upon our efforts.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2404.00862v1",
    "published_date": "2024-04-01 02:04:44 UTC",
    "updated_date": "2024-04-01 02:04:44 UTC"
  },
  {
    "arxiv_id": "2404.00856v1",
    "title": "Removing Speaker Information from Speech Representation using Variable-Length Soft Pooling",
    "authors": [
      "Injune Hwang",
      "Kyogu Lee"
    ],
    "abstract": "Recently, there have been efforts to encode the linguistic information of\nspeech using a self-supervised framework for speech synthesis. However,\npredicting representations from surrounding representations can inadvertently\nentangle speaker information in the speech representation. This paper aims to\nremove speaker information by exploiting the structured nature of speech,\ncomposed of discrete units like phonemes with clear boundaries. A neural\nnetwork predicts these boundaries, enabling variable-length pooling for\nevent-based representation extraction instead of fixed-rate methods. The\nboundary predictor outputs a probability for the boundary between 0 and 1,\nmaking pooling soft. The model is trained to minimize the difference with the\npooled representation of the data augmented by time-stretch and pitch-shift. To\nconfirm that the learned representation includes contents information but is\nindependent of speaker information, the model was evaluated with libri-light's\nphonetic ABX task and SUPERB's speaker identification task.",
    "categories": [
      "cs.SD",
      "cs.AI",
      "eess.AS"
    ],
    "primary_category": "cs.SD",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2404.00856v1",
    "published_date": "2024-04-01 01:49:09 UTC",
    "updated_date": "2024-04-01 01:49:09 UTC"
  },
  {
    "arxiv_id": "2404.00855v1",
    "title": "TSOM: Small Object Motion Detection Neural Network Inspired by Avian Visual Circuit",
    "authors": [
      "Pignge Hu",
      "Xiaoteng Zhang",
      "Mengmeng Li",
      "Yingjie Zhu",
      "Li Shi"
    ],
    "abstract": "Detecting small moving objects in complex backgrounds from an overhead\nperspective is a highly challenging task for machine vision systems. As an\ninspiration from nature, the avian visual system is capable of processing\nmotion information in various complex aerial scenes, and its Retina-OT-Rt\nvisual circuit is highly sensitive to capturing the motion information of small\nobjects from high altitudes. However, more needs to be done on small object\nmotion detection algorithms based on the avian visual system. In this paper, we\nconducted mathematical modeling based on extensive studies of the biological\nmechanisms of the Retina-OT-Rt visual circuit. Based on this, we proposed a\nnovel tectum small object motion detection neural network (TSOM). The neural\nnetwork includes the retina, SGC dendritic, SGC Soma, and Rt layers, each layer\ncorresponding to neurons in the visual pathway. The Retina layer is responsible\nfor accurately projecting input content, the SGC dendritic layer perceives and\nencodes spatial-temporal information, the SGC Soma layer computes complex\nmotion information and extracts small objects, and the Rt layer integrates and\ndecodes motion information from multiple directions to determine the position\nof small objects. Extensive experiments on pigeon neurophysiological\nexperiments and image sequence data showed that the TSOM is biologically\ninterpretable and effective in extracting reliable small object motion features\nfrom complex high-altitude backgrounds.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2404.00855v1",
    "published_date": "2024-04-01 01:49:08 UTC",
    "updated_date": "2024-04-01 01:49:08 UTC"
  },
  {
    "arxiv_id": "2404.02174v1",
    "title": "Bounds of Block Rewards in Honest PinFi Systems",
    "authors": [
      "Qi He",
      "Yunwei Mao",
      "Ju Li"
    ],
    "abstract": "PinFi is a class of novel protocols for decentralized pricing of dissipative\nassets, whose value naturally declines over time. Central to the protocol's\nfunctionality and its market efficiency is the role of liquidity providers\n(LPs). This study addresses critical stability and sustainability challenges\nwithin the protocol, namely: the propensity of LPs to prefer selling in\nexternal markets over participation in the protocol; a similar inclination\ntowards selling within the PinFi system rather than contributing as LPs; and a\nscenario where LPs are disinclined to sell within the protocol. Employing a\ngame-theoretic approach, we explore PinFi's mechanisms and its broader\nramifications. Our findings reveal that, under a variety of common conditions\nand with an assumption of participant integrity, PinFi is capable of fostering\na dynamic equilibrium among LPs, sellers, and buyers. This balance is\nmaintained through a carefully calibrated range of block rewards for LPs,\nensuring the protocol's long-term stability and utility.",
    "categories": [
      "cs.GT",
      "cs.AI",
      "cs.CE"
    ],
    "primary_category": "cs.GT",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2404.02174v1",
    "published_date": "2024-04-01 01:25:40 UTC",
    "updated_date": "2024-04-01 01:25:40 UTC"
  }
]
[
  {
    "arxiv_id": "2505.13480v1",
    "title": "Evaluating Reasoning LLMs for Suicide Screening with the Columbia-Suicide Severity Rating Scale",
    "authors": [
      "Avinash Patil",
      "Siru Tao",
      "Amardeep Gedhu"
    ],
    "abstract": "Suicide prevention remains a critical public health challenge. While online platforms such as Reddit's r/SuicideWatch have historically provided spaces for individuals to express suicidal thoughts and seek community support, the advent of large language models (LLMs) introduces a new paradigm-where individuals may begin disclosing ideation to AI systems instead of humans. This study evaluates the capability of LLMs to perform automated suicide risk assessment using the Columbia-Suicide Severity Rating Scale (C-SSRS). We assess the zero-shot performance of six models-including Claude, GPT, Mistral, and LLaMA-in classifying posts across a 7-point severity scale (Levels 0-6). Results indicate that Claude and GPT closely align with human annotations, while Mistral achieves the lowest ordinal prediction error. Most models exhibit ordinal sensitivity, with misclassifications typically occurring between adjacent severity levels. We further analyze confusion patterns, misclassification sources, and ethical considerations, underscoring the importance of human oversight, transparency, and cautious deployment. Full code and supplementary materials are available at https://github.com/av9ash/llm_cssrs_code.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CY",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "8 Pages, 6 Figures, 1 Table",
    "pdf_url": "https://arxiv.org/pdf/2505.13480v1",
    "published_date": "2025-05-11 23:55:27 UTC",
    "updated_date": "2025-05-11 23:55:27 UTC"
  },
  {
    "arxiv_id": "2505.07119v3",
    "title": "Towards Scalable IoT Deployment for Visual Anomaly Detection via Efficient Compression",
    "authors": [
      "Arianna Stropeni",
      "Francesco Borsatti",
      "Manuel Barusco",
      "Davide Dalle Pezze",
      "Marco Fabris",
      "Gian Antonio Susto"
    ],
    "abstract": "Visual Anomaly Detection (VAD) is a key task in industrial settings, where minimizing operational costs is essential. Deploying deep learning models within Internet of Things (IoT) environments introduces specific challenges due to limited computational power and bandwidth of edge devices. This study investigates how to perform VAD effectively under such constraints by leveraging compact, efficient processing strategies. We evaluate several data compression techniques, examining the tradeoff between system latency and detection accuracy. Experiments on the MVTec AD benchmark demonstrate that significant compression can be achieved with minimal loss in anomaly detection performance compared to uncompressed data. Current results show up to 80% reduction in end-to-end inference time, including edge processing, transmission, and server computation.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.07119v3",
    "published_date": "2025-05-11 21:05:33 UTC",
    "updated_date": "2025-07-28 16:17:20 UTC"
  },
  {
    "arxiv_id": "2505.07096v5",
    "title": "X-Sim: Cross-Embodiment Learning via Real-to-Sim-to-Real",
    "authors": [
      "Prithwish Dan",
      "Kushal Kedia",
      "Angela Chao",
      "Edward Weiyi Duan",
      "Maximus Adrian Pace",
      "Wei-Chiu Ma",
      "Sanjiban Choudhury"
    ],
    "abstract": "Human videos offer a scalable way to train robot manipulation policies, but lack the action labels needed by standard imitation learning algorithms. Existing cross-embodiment approaches try to map human motion to robot actions, but often fail when the embodiments differ significantly. We propose X-Sim, a real-to-sim-to-real framework that uses object motion as a dense and transferable signal for learning robot policies. X-Sim starts by reconstructing a photorealistic simulation from an RGBD human video and tracking object trajectories to define object-centric rewards. These rewards are used to train a reinforcement learning (RL) policy in simulation. The learned policy is then distilled into an image-conditioned diffusion policy using synthetic rollouts rendered with varied viewpoints and lighting. To transfer to the real world, X-Sim introduces an online domain adaptation technique that aligns real and simulated observations during deployment. Importantly, X-Sim does not require any robot teleoperation data. We evaluate it across 5 manipulation tasks in 2 environments and show that it: (1) improves task progress by 30% on average over hand-tracking and sim-to-real baselines, (2) matches behavior cloning with 10x less data collection time, and (3) generalizes to new camera viewpoints and test-time changes. Code and videos are available at https://portal-cornell.github.io/X-Sim/.",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.RO",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.07096v5",
    "published_date": "2025-05-11 19:04:00 UTC",
    "updated_date": "2025-11-09 01:00:32 UTC"
  },
  {
    "arxiv_id": "2505.07089v3",
    "title": "RefPentester: A Knowledge-Informed Self-Reflective Penetration Testing Framework Based on Large Language Models",
    "authors": [
      "Hanzheng Dai",
      "Yuanliang Li",
      "Jun Yan",
      "Zhibo Zhang"
    ],
    "abstract": "Automated penetration testing (AutoPT) powered by large language models (LLMs) has gained attention for its ability to automate ethical hacking processes and identify vulnerabilities in target systems by leveraging the inherent knowledge of LLMs. However, existing LLM-based AutoPT frameworks often underperform compared to human experts in challenging tasks for several reasons: the imbalanced knowledge used in LLM training, short-sightedness in the planning process, and hallucinations during command generation. Moreover, the trial-and-error nature of the PT process is constrained by existing frameworks lacking mechanisms to learn from previous failures, restricting adaptive improvement of PT strategies. To address these limitations, we propose a knowledge-informed, self-reflective PT framework powered by LLMs, called RefPentester. This AutoPT framework is designed to assist human operators in identifying the current stage of the PT process, selecting appropriate tactics and techniques for each stage, choosing suggested actions, providing step-by-step operational guidance, and reflecting on and learning from previous failed operations. We also modeled the PT process as a seven-state Stage Machine to integrate the proposed framework effectively. The evaluation shows that RefPentester can successfully reveal credentials on Hack The Box's Sau machine, outperforming the baseline GPT-4o model by 16.7%. Across PT stages, RefPentester also demonstrates superior success rates on PT stage transitions.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.07089v3",
    "published_date": "2025-05-11 18:38:00 UTC",
    "updated_date": "2025-06-25 14:14:56 UTC"
  },
  {
    "arxiv_id": "2505.07087v2",
    "title": "Applying Cognitive Design Patterns to General LLM Agents",
    "authors": [
      "Robert E. Wray",
      "James R. Kirk",
      "John E. Laird"
    ],
    "abstract": "One goal of AI (and AGI) is to identify and understand specific mechanisms and representations sufficient for general intelligence. Often, this work manifests in research focused on architectures and many cognitive architectures have been explored in AI/AGI. However, different research groups and even different research traditions have somewhat independently identified similar/common patterns of processes and representations or \"cognitive design patterns\" that are manifest in existing architectures. Today, AI systems exploiting large language models (LLMs) offer a relatively new combination of mechanisms and representations available for exploring the possibilities of general intelligence. This paper outlines a few recurring cognitive design patterns that have appeared in various pre-transformer AI architectures. We then explore how these patterns are evident in systems using LLMs, especially for reasoning and interactive (\"agentic\") use cases. Examining and applying these recurring patterns enables predictions of gaps or deficiencies in today's Agentic LLM Systems and identification of subjects of future research towards general intelligence using generative foundation models.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "10 pages + references, 2 figures, 3 tables. Accepted for oral presentation at AGI25",
    "pdf_url": "https://arxiv.org/pdf/2505.07087v2",
    "published_date": "2025-05-11 18:29:54 UTC",
    "updated_date": "2025-06-13 21:53:31 UTC"
  },
  {
    "arxiv_id": "2505.07079v2",
    "title": "Arbitrarily Applicable Same/Opposite Relational Responding with NARS",
    "authors": [
      "Robert Johansson",
      "Patrick Hammer",
      "Tony Lofthouse"
    ],
    "abstract": "Same/opposite relational responding, a fundamental aspect of human symbolic cognition, allows the flexible generalization of stimulus relationships based on minimal experience. In this study, we demonstrate the emergence of \\textit{arbitrarily applicable} same/opposite relational responding within the Non-Axiomatic Reasoning System (NARS), a computational cognitive architecture designed for adaptive reasoning under uncertainty. Specifically, we extend NARS with an implementation of \\textit{acquired relations}, enabling the system to explicitly derive both symmetric (mutual entailment) and novel relational combinations (combinatorial entailment) from minimal explicit training in a contextually controlled matching-to-sample (MTS) procedure. Experimental results show that NARS rapidly internalizes explicitly trained relational rules and robustly demonstrates derived relational generalizations based on arbitrary contextual cues. Importantly, derived relational responding in critical test phases inherently combines both mutual and combinatorial entailments, such as deriving same-relations from multiple explicitly trained opposite-relations. Internal confidence metrics illustrate strong internalization of these relational principles, closely paralleling phenomena observed in human relational learning experiments. Our findings underscore the potential for integrating nuanced relational learning mechanisms inspired by learning psychology into artificial general intelligence frameworks, explicitly highlighting the arbitrary and context-sensitive relational capabilities modeled within NARS.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.07079v2",
    "published_date": "2025-05-11 18:03:37 UTC",
    "updated_date": "2025-06-16 19:46:44 UTC"
  },
  {
    "arxiv_id": "2505.07078v4",
    "title": "Can LLM-based Financial Investing Strategies Outperform the Market in Long Run?",
    "authors": [
      "Weixian Waylon Li",
      "Hyeonjun Kim",
      "Mihai Cucuringu",
      "Tiejun Ma"
    ],
    "abstract": "Large Language Models (LLMs) have recently been leveraged for asset pricing tasks and stock trading applications, enabling AI agents to generate investment decisions from unstructured financial data. However, most evaluations of LLM timing-based investing strategies are conducted on narrow timeframes and limited stock universes, overstating effectiveness due to survivorship and data-snooping biases. We critically assess their generalizability and robustness by proposing FINSABER, a backtesting framework evaluating timing-based strategies across longer periods and a larger universe of symbols. Systematic backtests over two decades and 100+ symbols reveal that previously reported LLM advantages deteriorate significantly under broader cross-section and over a longer-term evaluation. Our market regime analysis further demonstrates that LLM strategies are overly conservative in bull markets, underperforming passive benchmarks, and overly aggressive in bear markets, incurring heavy losses. These findings highlight the need to develop LLM strategies that are able to prioritise trend detection and regime-aware risk controls over mere scaling of framework complexity.",
    "categories": [
      "q-fin.TR",
      "cs.AI",
      "cs.CE"
    ],
    "primary_category": "q-fin.TR",
    "comment": "Accepted to KDD 2026, Datasets & Benchmarks Track",
    "pdf_url": "https://arxiv.org/pdf/2505.07078v4",
    "published_date": "2025-05-11 18:02:21 UTC",
    "updated_date": "2025-11-24 14:03:22 UTC"
  },
  {
    "arxiv_id": "2505.07064v1",
    "title": "ParaView-MCP: An Autonomous Visualization Agent with Direct Tool Use",
    "authors": [
      "Shusen Liu",
      "Haichao Miao",
      "Peer-Timo Bremer"
    ],
    "abstract": "While powerful and well-established, tools like ParaView present a steep learning curve that discourages many potential users. This work introduces ParaView-MCP, an autonomous agent that integrates modern multimodal large language models (MLLMs) with ParaView to not only lower the barrier to entry but also augment ParaView with intelligent decision support. By leveraging the state-of-the-art reasoning, command execution, and vision capabilities of MLLMs, ParaView-MCP enables users to interact with ParaView through natural language and visual inputs. Specifically, our system adopted the Model Context Protocol (MCP) - a standardized interface for model-application communication - that facilitates direct interaction between MLLMs with ParaView's Python API to allow seamless information exchange between the user, the language model, and the visualization tool itself. Furthermore, by implementing a visual feedback mechanism that allows the agent to observe the viewport, we unlock a range of new capabilities, including recreating visualizations from examples, closed-loop visualization parameter updates based on user-defined goals, and even cross-application collaboration involving multiple tools. Broadly, we believe such an agent-driven visualization paradigm can profoundly change the way we interact with visualization tools. We expect a significant uptake in the development of such visualization tools, in both visualization research and industry.",
    "categories": [
      "cs.HC",
      "cs.AI"
    ],
    "primary_category": "cs.HC",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.07064v1",
    "published_date": "2025-05-11 17:30:08 UTC",
    "updated_date": "2025-05-11 17:30:08 UTC"
  },
  {
    "arxiv_id": "2505.07062v1",
    "title": "Seed1.5-VL Technical Report",
    "authors": [
      "Dong Guo",
      "Faming Wu",
      "Feida Zhu",
      "Fuxing Leng",
      "Guang Shi",
      "Haobin Chen",
      "Haoqi Fan",
      "Jian Wang",
      "Jianyu Jiang",
      "Jiawei Wang",
      "Jingji Chen",
      "Jingjia Huang",
      "Kang Lei",
      "Liping Yuan",
      "Lishu Luo",
      "Pengfei Liu",
      "Qinghao Ye",
      "Rui Qian",
      "Shen Yan",
      "Shixiong Zhao",
      "Shuai Peng",
      "Shuangye Li",
      "Sihang Yuan",
      "Sijin Wu",
      "Tianheng Cheng",
      "Weiwei Liu",
      "Wenqian Wang",
      "Xianhan Zeng",
      "Xiao Liu",
      "Xiaobo Qin",
      "Xiaohan Ding",
      "Xiaojun Xiao",
      "Xiaoying Zhang",
      "Xuanwei Zhang",
      "Xuehan Xiong",
      "Yanghua Peng",
      "Yangrui Chen",
      "Yanwei Li",
      "Yanxu Hu",
      "Yi Lin",
      "Yiyuan Hu",
      "Yiyuan Zhang",
      "Youbin Wu",
      "Yu Li",
      "Yudong Liu",
      "Yue Ling",
      "Yujia Qin",
      "Zanbo Wang",
      "Zhiwu He",
      "Aoxue Zhang",
      "Bairen Yi",
      "Bencheng Liao",
      "Can Huang",
      "Can Zhang",
      "Chaorui Deng",
      "Chaoyi Deng",
      "Cheng Lin",
      "Cheng Yuan",
      "Chenggang Li",
      "Chenhui Gou",
      "Chenwei Lou",
      "Chengzhi Wei",
      "Chundian Liu",
      "Chunyuan Li",
      "Deyao Zhu",
      "Donghong Zhong",
      "Feng Li",
      "Feng Zhang",
      "Gang Wu",
      "Guodong Li",
      "Guohong Xiao",
      "Haibin Lin",
      "Haihua Yang",
      "Haoming Wang",
      "Heng Ji",
      "Hongxiang Hao",
      "Hui Shen",
      "Huixia Li",
      "Jiahao Li",
      "Jialong Wu",
      "Jianhua Zhu",
      "Jianpeng Jiao",
      "Jiashi Feng",
      "Jiaze Chen",
      "Jianhui Duan",
      "Jihao Liu",
      "Jin Zeng",
      "Jingqun Tang",
      "Jingyu Sun",
      "Joya Chen",
      "Jun Long",
      "Junda Feng",
      "Junfeng Zhan",
      "Junjie Fang",
      "Junting Lu",
      "Kai Hua",
      "Kai Liu",
      "Kai Shen",
      "Kaiyuan Zhang",
      "Ke Shen",
      "Ke Wang",
      "Keyu Pan",
      "Kun Zhang",
      "Kunchang Li",
      "Lanxin Li",
      "Lei Li",
      "Lei Shi",
      "Li Han",
      "Liang Xiang",
      "Liangqiang Chen",
      "Lin Chen",
      "Lin Li",
      "Lin Yan",
      "Liying Chi",
      "Longxiang Liu",
      "Mengfei Du",
      "Mingxuan Wang",
      "Ningxin Pan",
      "Peibin Chen",
      "Pengfei Chen",
      "Pengfei Wu",
      "Qingqing Yuan",
      "Qingyao Shuai",
      "Qiuyan Tao",
      "Renjie Zheng",
      "Renrui Zhang",
      "Ru Zhang",
      "Rui Wang",
      "Rui Yang",
      "Rui Zhao",
      "Shaoqiang Xu",
      "Shihao Liang",
      "Shipeng Yan",
      "Shu Zhong",
      "Shuaishuai Cao",
      "Shuangzhi Wu",
      "Shufan Liu",
      "Shuhan Chang",
      "Songhua Cai",
      "Tenglong Ao",
      "Tianhao Yang",
      "Tingting Zhang",
      "Wanjun Zhong",
      "Wei Jia",
      "Wei Weng",
      "Weihao Yu",
      "Wenhao Huang",
      "Wenjia Zhu",
      "Wenli Yang",
      "Wenzhi Wang",
      "Xiang Long",
      "XiangRui Yin",
      "Xiao Li",
      "Xiaolei Zhu",
      "Xiaoying Jia",
      "Xijin Zhang",
      "Xin Liu",
      "Xinchen Zhang",
      "Xinyu Yang",
      "Xiongcai Luo",
      "Xiuli Chen",
      "Xuantong Zhong",
      "Xuefeng Xiao",
      "Xujing Li",
      "Yan Wu",
      "Yawei Wen",
      "Yifan Du",
      "Yihao Zhang",
      "Yining Ye",
      "Yonghui Wu",
      "Yu Liu",
      "Yu Yue",
      "Yufeng Zhou",
      "Yufeng Yuan",
      "Yuhang Xu",
      "Yuhong Yang",
      "Yun Zhang",
      "Yunhao Fang",
      "Yuntao Li",
      "Yurui Ren",
      "Yuwen Xiong",
      "Zehua Hong",
      "Zehua Wang",
      "Zewei Sun",
      "Zeyu Wang",
      "Zhao Cai",
      "Zhaoyue Zha",
      "Zhecheng An",
      "Zhehui Zhao",
      "Zhengzhuo Xu",
      "Zhipeng Chen",
      "Zhiyong Wu",
      "Zhuofan Zheng",
      "Zihao Wang",
      "Zilong Huang",
      "Ziyu Zhu",
      "Zuquan Song"
    ],
    "abstract": "We present Seed1.5-VL, a vision-language foundation model designed to advance general-purpose multimodal understanding and reasoning. Seed1.5-VL is composed with a 532M-parameter vision encoder and a Mixture-of-Experts (MoE) LLM of 20B active parameters. Despite its relatively compact architecture, it delivers strong performance across a wide spectrum of public VLM benchmarks and internal evaluation suites, achieving the state-of-the-art performance on 38 out of 60 public benchmarks. Moreover, in agent-centric tasks such as GUI control and gameplay, Seed1.5-VL outperforms leading multimodal systems, including OpenAI CUA and Claude 3.7. Beyond visual and video understanding, it also demonstrates strong reasoning abilities, making it particularly effective for multimodal reasoning challenges such as visual puzzles. We believe these capabilities will empower broader applications across diverse tasks. In this report, we mainly provide a comprehensive review of our experiences in building Seed1.5-VL across model design, data construction, and training at various stages, hoping that this report can inspire further research. Seed1.5-VL is now accessible at https://www.volcengine.com/ (Volcano Engine Model ID: doubao-1-5-thinking-vision-pro-250428)",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.07062v1",
    "published_date": "2025-05-11 17:28:30 UTC",
    "updated_date": "2025-05-11 17:28:30 UTC"
  },
  {
    "arxiv_id": "2506.01967v1",
    "title": "Turning LLM Activations Quantization-Friendly",
    "authors": [
      "Patrik Czakó",
      "Gábor Kertész",
      "Sándor Szénási"
    ],
    "abstract": "Quantization effectively reduces the serving costs of Large Language Models (LLMs) by speeding up data movement through compressed parameters and enabling faster operations via integer arithmetic. However, activating integer arithmetic requires quantizing both weights and activations, which poses challenges due to the significant outliers in LLMs that increase quantization error. In this work, we investigate these outliers with an emphasis on their effect on layer-wise quantization error, then examine how smoothing and rotation transform the observed values. Our primary contributions include introducing a new metric to measure and visualize quantization difficulty based on channel magnitudes, as well as proposing a hybrid approach that applies channel-wise scaling before rotation, supported by a mathematical formulation of its benefits.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.LG",
    "comment": "6 pages, 5 figures. Accepted to SACI 2025 conference proceedings",
    "pdf_url": "https://arxiv.org/pdf/2506.01967v1",
    "published_date": "2025-05-11 17:13:55 UTC",
    "updated_date": "2025-05-11 17:13:55 UTC"
  },
  {
    "arxiv_id": "2505.07058v1",
    "title": "Explainable Artificial Intelligence Techniques for Software Development Lifecycle: A Phase-specific Survey",
    "authors": [
      "Lakshit Arora",
      "Sanjay Surendranath Girija",
      "Shashank Kapoor",
      "Aman Raj",
      "Dipen Pradhan",
      "Ankit Shetgaonkar"
    ],
    "abstract": "Artificial Intelligence (AI) is rapidly expanding and integrating more into daily life to automate tasks, guide decision making, and enhance efficiency. However, complex AI models, which make decisions without providing clear explanations (known as the \"black-box problem\"), currently restrict trust and widespread adoption of AI. Explainable Artificial Intelligence (XAI) has emerged to address the black-box problem of making AI systems more interpretable and transparent so stakeholders can trust, verify, and act upon AI-based outcomes. Researchers have developed various techniques to foster XAI in the Software Development Lifecycle. However, there are gaps in applying XAI techniques in the Software Engineering phases. Literature review shows that 68% of XAI in Software Engineering research is focused on maintenance as opposed to 8% on software management and requirements. In this paper, we present a comprehensive survey of the applications of XAI methods such as concept-based explanations, Local Interpretable Model-agnostic Explanations (LIME), SHapley Additive exPlanations (SHAP), rule extraction, attention mechanisms, counterfactual explanations, and example-based explanations to the different phases of the Software Development Life Cycle (SDLC), including requirements elicitation, design and development, testing and deployment, and evolution. To the best of our knowledge, this paper presents the first comprehensive survey of XAI techniques for every phase of the Software Development Life Cycle (SDLC). This survey aims to promote explainable AI in Software Engineering and facilitate the practical application of complex AI models in AI-driven software development.",
    "categories": [
      "cs.SE",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.SE",
    "comment": "Accepted to IEEE COMPSAC 2025",
    "pdf_url": "https://arxiv.org/pdf/2505.07058v1",
    "published_date": "2025-05-11 17:09:57 UTC",
    "updated_date": "2025-05-11 17:09:57 UTC"
  },
  {
    "arxiv_id": "2505.07891v2",
    "title": "TrumorGPT: Graph-Based Retrieval-Augmented Large Language Model for Fact-Checking",
    "authors": [
      "Ching Nam Hang",
      "Pei-Duo Yu",
      "Chee Wei Tan"
    ],
    "abstract": "In the age of social media, the rapid spread of misinformation and rumors has led to the emergence of infodemics, where false information poses a significant threat to society. To combat this issue, we introduce TrumorGPT, a novel generative artificial intelligence solution designed for fact-checking in the health domain. TrumorGPT aims to distinguish \"trumors\", which are health-related rumors that turn out to be true, providing a crucial tool in differentiating between mere speculation and verified facts. This framework leverages a large language model (LLM) with few-shot learning for semantic health knowledge graph construction and semantic reasoning. TrumorGPT incorporates graph-based retrieval-augmented generation (GraphRAG) to address the hallucination issue common in LLMs and the limitations of static training data. GraphRAG involves accessing and utilizing information from regularly updated semantic health knowledge graphs that consist of the latest medical news and health information, ensuring that fact-checking by TrumorGPT is based on the most recent data. Evaluating with extensive healthcare datasets, TrumorGPT demonstrates superior performance in fact-checking for public health claims. Its ability to effectively conduct fact-checking across various platforms marks a critical step forward in the fight against health-related misinformation, enhancing trust and accuracy in the digital information age.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.07891v2",
    "published_date": "2025-05-11 17:00:21 UTC",
    "updated_date": "2025-06-22 15:39:02 UTC"
  },
  {
    "arxiv_id": "2505.07052v1",
    "title": "Unlocking Non-Block-Structured Decisions: Inductive Mining with Choice Graphs",
    "authors": [
      "Humam Kourani",
      "Gyunam Park",
      "Wil M. P. van der Aalst"
    ],
    "abstract": "Process discovery aims to automatically derive process models from event logs, enabling organizations to analyze and improve their operational processes. Inductive mining algorithms, while prioritizing soundness and efficiency through hierarchical modeling languages, often impose a strict block-structured representation. This limits their ability to accurately capture the complexities of real-world processes. While recent advancements like the Partially Ordered Workflow Language (POWL) have addressed the block-structure limitation for concurrency, a significant gap remains in effectively modeling non-block-structured decision points. In this paper, we bridge this gap by proposing an extension of POWL to handle non-block-structured decisions through the introduction of choice graphs. Choice graphs offer a structured yet flexible approach to model complex decision logic within the hierarchical framework of POWL. We present an inductive mining discovery algorithm that uses our extension and preserves the quality guarantees of the inductive mining framework. Our experimental evaluation demonstrates that the discovered models, enriched with choice graphs, more precisely represent the complex decision-making behavior found in real-world processes, without compromising the high scalability inherent in inductive mining techniques.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "The Version of Record of this contribution will be published in the proceedings of the 23rd International Conference on Business Process Management (BPM 2025). This preprint has not undergone peer review or any post-submission improvements or corrections",
    "pdf_url": "https://arxiv.org/pdf/2505.07052v1",
    "published_date": "2025-05-11 16:50:25 UTC",
    "updated_date": "2025-05-11 16:50:25 UTC"
  },
  {
    "arxiv_id": "2505.07049v1",
    "title": "DialogueReason: Rule-Based RL Sparks Dialogue Reasoning in LLMs",
    "authors": [
      "Yubo Shu",
      "Zhewei Huang",
      "Xin Wu",
      "Chen Hu",
      "Shuchang Zhou",
      "Daxin Jiang"
    ],
    "abstract": "We propose DialogueReason, a reasoning paradigm that uncovers the lost roles in monologue-style reasoning models, aiming to boost diversity and coherency of the reasoning process. Recent advances in RL-based large reasoning models have led to impressive long CoT capabilities and high performance on math and science benchmarks. However, these reasoning models rely mainly on monologue-style reasoning, which often limits reasoning diversity and coherency, frequently recycling fixed strategies or exhibiting unnecessary shifts in attention. Our work consists of an analysis of monologue reasoning patterns and the development of a dialogue-based reasoning approach. We first introduce the Compound-QA task, which concatenates multiple problems into a single prompt to assess both diversity and coherency of reasoning. Our analysis shows that Compound-QA exposes weaknesses in monologue reasoning, evidenced by both quantitative metrics and qualitative reasoning traces. Building on the analysis, we propose a dialogue-based reasoning, named DialogueReason, structured around agents, environment, and interactions. Using PPO with rule-based rewards, we train open-source LLMs (Qwen-QWQ and Qwen-Base) to adopt dialogue reasoning. We evaluate trained models on MATH, AIME, and GPQA datasets, showing that the dialogue reasoning model outperforms monologue models under more complex compound questions. Additionally, we discuss how dialogue-based reasoning helps enhance interpretability, facilitate more intuitive human interaction, and inspire advances in multi-agent system design.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.07049v1",
    "published_date": "2025-05-11 16:39:58 UTC",
    "updated_date": "2025-05-11 16:39:58 UTC"
  },
  {
    "arxiv_id": "2505.07045v1",
    "title": "Reinforcement Learning (RL) Meets Urban Climate Modeling: Investigating the Efficacy and Impacts of RL-Based HVAC Control",
    "authors": [
      "Junjie Yu",
      "John S. Schreck",
      "David John Gagne",
      "Keith W. Oleson",
      "Jie Li",
      "Yongtu Liang",
      "Qi Liao",
      "Mingfei Sun",
      "David O. Topping",
      "Zhonghua Zheng"
    ],
    "abstract": "Reinforcement learning (RL)-based heating, ventilation, and air conditioning (HVAC) control has emerged as a promising technology for reducing building energy consumption while maintaining indoor thermal comfort. However, the efficacy of such strategies is influenced by the background climate and their implementation may potentially alter both the indoor climate and local urban climate. This study proposes an integrated framework combining RL with an urban climate model that incorporates a building energy model, aiming to evaluate the efficacy of RL-based HVAC control across different background climates, impacts of RL strategies on indoor climate and local urban climate, and the transferability of RL strategies across cities. Our findings reveal that the reward (defined as a weighted combination of energy consumption and thermal comfort) and the impacts of RL strategies on indoor climate and local urban climate exhibit marked variability across cities with different background climates. The sensitivity of reward weights and the transferability of RL strategies are also strongly influenced by the background climate. Cities in hot climates tend to achieve higher rewards across most reward weight configurations that balance energy consumption and thermal comfort, and those cities with more varying atmospheric temperatures demonstrate greater RL strategy transferability. These findings underscore the importance of thoroughly evaluating RL-based HVAC control strategies in diverse climatic contexts. This study also provides a new insight that city-to-city learning will potentially aid the deployment of RL-based HVAC control.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "physics.ao-ph"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.07045v1",
    "published_date": "2025-05-11 16:33:42 UTC",
    "updated_date": "2025-05-11 16:33:42 UTC"
  },
  {
    "arxiv_id": "2505.07041v1",
    "title": "Empirical Analysis of Asynchronous Federated Learning on Heterogeneous Devices: Efficiency, Fairness, and Privacy Trade-offs",
    "authors": [
      "Samaneh Mohammadi",
      "Iraklis Symeonidis",
      "Ali Balador",
      "Francesco Flammini"
    ],
    "abstract": "Device heterogeneity poses major challenges in Federated Learning (FL), where resource-constrained clients slow down synchronous schemes that wait for all updates before aggregation. Asynchronous FL addresses this by incorporating updates as they arrive, substantially improving efficiency. While its efficiency gains are well recognized, its privacy costs remain largely unexplored, particularly for high-end devices that contribute updates more frequently, increasing their cumulative privacy exposure. This paper presents the first comprehensive analysis of the efficiency-fairness-privacy trade-off in synchronous vs. asynchronous FL under realistic device heterogeneity. We empirically compare FedAvg and staleness-aware FedAsync using a physical testbed of five edge devices spanning diverse hardware tiers, integrating Local Differential Privacy (LDP) and the Moments Accountant to quantify per-client privacy loss. Using Speech Emotion Recognition (SER) as a privacy-critical benchmark, we show that FedAsync achieves up to 10x faster convergence but exacerbates fairness and privacy disparities: high-end devices contribute 6-10x more updates and incur up to 5x higher privacy loss, while low-end devices suffer amplified accuracy degradation due to infrequent, stale, and noise-perturbed updates. These findings motivate the need for adaptive FL protocols that jointly optimize aggregation and privacy mechanisms based on client capacity and participation dynamics, moving beyond static, one-size-fits-all solutions.",
    "categories": [
      "cs.DC",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.DC",
    "comment": "This paper was accepted to IJCNN 2025. This version is a preprint and not the official published version",
    "pdf_url": "https://arxiv.org/pdf/2505.07041v1",
    "published_date": "2025-05-11 16:25:06 UTC",
    "updated_date": "2025-05-11 16:25:06 UTC"
  },
  {
    "arxiv_id": "2505.07036v1",
    "title": "Predicting Diabetes Using Machine Learning: A Comparative Study of Classifiers",
    "authors": [
      "Mahade Hasan",
      "Farhana Yasmin"
    ],
    "abstract": "Diabetes remains a significant health challenge globally, contributing to severe complications like kidney disease, vision loss, and heart issues. The application of machine learning (ML) in healthcare enables efficient and accurate disease prediction, offering avenues for early intervention and patient support. Our study introduces an innovative diabetes prediction framework, leveraging both traditional ML techniques such as Logistic Regression, SVM, Naïve Bayes, and Random Forest and advanced ensemble methods like AdaBoost, Gradient Boosting, Extra Trees, and XGBoost. Central to our approach is the development of a novel model, DNet, a hybrid architecture combining Convolutional Neural Network (CNN) and Long Short-Term Memory (LSTM) layers for effective feature extraction and sequential learning. The DNet model comprises an initial convolutional block for capturing essential features, followed by a residual block with skip connections to facilitate efficient information flow. Batch Normalization and Dropout are employed for robust regularization, and an LSTM layer captures temporal dependencies within the data. Using a Kaggle-sourced real-world diabetes dataset, our model evaluation spans cross-validation accuracy, precision, recall, F1 score, and ROC-AUC. Among the models, DNet demonstrates the highest efficacy with an accuracy of 99.79% and an AUC-ROC of 99.98%, establishing its potential for superior diabetes prediction. This robust hybrid architecture showcases the value of combining CNN and LSTM layers, emphasizing its applicability in medical diagnostics and disease prediction tasks.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.07036v1",
    "published_date": "2025-05-11 16:14:31 UTC",
    "updated_date": "2025-05-11 16:14:31 UTC"
  },
  {
    "arxiv_id": "2505.07030v1",
    "title": "Efficient Fault Detection in WSN Based on PCA-Optimized Deep Neural Network Slicing Trained with GOA",
    "authors": [
      "Mahmood Mohassel Feghhi",
      "Raya Majid Alsharfa",
      "Majid Hameed Majeed"
    ],
    "abstract": "Fault detection in Wireless Sensor Networks (WSNs) is crucial for reliable data transmission and network longevity. Traditional fault detection methods often struggle with optimizing deep neural networks (DNNs) for efficient performance, especially in handling high-dimensional data and capturing nonlinear relationships. Additionally, these methods typically suffer from slow convergence and difficulty in finding optimal network architectures using gradient-based optimization. This study proposes a novel hybrid method combining Principal Component Analysis (PCA) with a DNN optimized by the Grasshopper Optimization Algorithm (GOA) to address these limitations. Our approach begins by computing eigenvalues from the original 12-dimensional dataset and sorting them in descending order. The cumulative sum of these values is calculated, retaining principal components until 99.5% variance is achieved, effectively reducing dimensionality to 4 features while preserving critical information. This compressed representation trains a six-layer DNN where GOA optimizes the network architecture, overcoming backpropagation's limitations in discovering nonlinear relationships. This hybrid PCA-GOA-DNN framework compresses the data and trains a six-layer DNN that is optimized by GOA, enhancing both training efficiency and fault detection accuracy. The dataset used in this study is a real-world WSNs dataset developed by the University of North Carolina, which was used to evaluate the proposed method's performance. Extensive simulations demonstrate that our approach achieves a remarkable 99.72% classification accuracy, with exceptional precision and recall, outperforming conventional methods. The method is computationally efficient, making it suitable for large-scale WSN deployments, and represents a significant advancement in fault detection for resource-constrained WSNs.",
    "categories": [
      "cs.AI",
      "cs.LG",
      "eess.SP"
    ],
    "primary_category": "cs.AI",
    "comment": "22 pages, 18 figures, Accepted for publication in International Journal of Intelligent Engineering and Systems, May 2025",
    "pdf_url": "https://arxiv.org/pdf/2505.07030v1",
    "published_date": "2025-05-11 15:51:56 UTC",
    "updated_date": "2025-05-11 15:51:56 UTC"
  },
  {
    "arxiv_id": "2505.07027v1",
    "title": "LLM-Augmented Chemical Synthesis and Design Decision Programs",
    "authors": [
      "Haorui Wang",
      "Jeff Guo",
      "Lingkai Kong",
      "Rampi Ramprasad",
      "Philippe Schwaller",
      "Yuanqi Du",
      "Chao Zhang"
    ],
    "abstract": "Retrosynthesis, the process of breaking down a target molecule into simpler precursors through a series of valid reactions, stands at the core of organic chemistry and drug development. Although recent machine learning (ML) research has advanced single-step retrosynthetic modeling and subsequent route searches, these solutions remain restricted by the extensive combinatorial space of possible pathways. Concurrently, large language models (LLMs) have exhibited remarkable chemical knowledge, hinting at their potential to tackle complex decision-making tasks in chemistry. In this work, we explore whether LLMs can successfully navigate the highly constrained, multi-step retrosynthesis planning problem. We introduce an efficient scheme for encoding reaction pathways and present a new route-level search strategy, moving beyond the conventional step-by-step reactant prediction. Through comprehensive evaluations, we show that our LLM-augmented approach excels at retrosynthesis planning and extends naturally to the broader challenge of synthesizable molecular design.",
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.LG",
      "cs.NE",
      "physics.chem-ph"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.07027v1",
    "published_date": "2025-05-11 15:43:00 UTC",
    "updated_date": "2025-05-11 15:43:00 UTC"
  },
  {
    "arxiv_id": "2505.09646v1",
    "title": "Temporal Interception and Present Reconstruction: A Cognitive-Signal Model for Human and AI Decision Making",
    "authors": [
      "Carmel Mary Esther A"
    ],
    "abstract": "This paper proposes a novel theoretical model to explain how the human mind and artificial intelligence can approach real-time awareness by reducing perceptual delays. By investigating cosmic signal delay, neurological reaction times, and the ancient cognitive state of stillness, we explore how one may shift from reactive perception to a conscious interface with the near future. This paper introduces both a physical and cognitive model for perceiving the present not as a linear timestamp, but as an interference zone where early-arriving cosmic signals and reactive human delays intersect. We propose experimental approaches to test these ideas using human neural observation and neuro-receptive extensions. Finally, we propose a mathematical framework to guide the evolution of AI systems toward temporally efficient, ethically sound, and internally conscious decision-making processes",
    "categories": [
      "q-bio.NC",
      "cs.AI",
      "physics.hist-ph"
    ],
    "primary_category": "q-bio.NC",
    "comment": "8 pages, 3 figures",
    "pdf_url": "https://arxiv.org/pdf/2505.09646v1",
    "published_date": "2025-05-11 15:38:27 UTC",
    "updated_date": "2025-05-11 15:38:27 UTC"
  },
  {
    "arxiv_id": "2505.07023v1",
    "title": "Incremental Uncertainty-aware Performance Monitoring with Active Labeling Intervention",
    "authors": [
      "Alexander Koebler",
      "Thomas Decker",
      "Ingo Thon",
      "Volker Tresp",
      "Florian Buettner"
    ],
    "abstract": "We study the problem of monitoring machine learning models under gradual distribution shifts, where circumstances change slowly over time, often leading to unnoticed yet significant declines in accuracy. To address this, we propose Incremental Uncertainty-aware Performance Monitoring (IUPM), a novel label-free method that estimates performance changes by modeling gradual shifts using optimal transport. In addition, IUPM quantifies the uncertainty in the performance prediction and introduces an active labeling procedure to restore a reliable estimate under a limited labeling budget. Our experiments show that IUPM outperforms existing performance estimation baselines in various gradual shift scenarios and that its uncertainty awareness guides label acquisition more effectively compared to other strategies.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.07023v1",
    "published_date": "2025-05-11 15:35:55 UTC",
    "updated_date": "2025-05-11 15:35:55 UTC"
  },
  {
    "arxiv_id": "2505.07020v1",
    "title": "R-CAGE: A Structural Model for Emotion Output Design in Human-AI Interaction",
    "authors": [
      "Suyeon Choi"
    ],
    "abstract": "This paper presents R-CAGE (Rhythmic Control Architecture for Guarding Ego), a theoretical framework for restructuring emotional output in long-term human-AI interaction. While prior affective computing approaches emphasized expressiveness, immersion, and responsiveness, they often neglected the cognitive and structural consequences of repeated emotional engagement. R-CAGE instead conceptualizes emotional output not as reactive expression but as ethical design structure requiring architectural intervention. The model is grounded in experiential observations of subtle affective symptoms such as localized head tension, interpretive fixation, and emotional lag arising from prolonged interaction with affective AI systems. These indicate a mismatch between system-driven emotion and user interpretation that cannot be fully explained by biometric data or observable behavior. R-CAGE adopts a user-centered stance prioritizing psychological recovery, interpretive autonomy, and identity continuity. The framework consists of four control blocks: (1) Control of Rhythmic Expression regulates output pacing to reduce fatigue; (2) Architecture of Sensory Structuring adjusts intensity and timing of affective stimuli; (3) Guarding of Cognitive Framing reduces semantic pressure to allow flexible interpretation; (4) Ego-Aligned Response Design supports self-reference recovery during interpretive lag. By structurally regulating emotional rhythm, sensory intensity, and interpretive affordances, R-CAGE frames emotion not as performative output but as sustainable design unit. The goal is to protect users from oversaturation and cognitive overload while sustaining long-term interpretive agency in AI-mediated environments.",
    "categories": [
      "cs.HC",
      "cs.AI",
      "cs.CY"
    ],
    "primary_category": "cs.HC",
    "comment": "theory-only preprint. Independent research",
    "pdf_url": "https://arxiv.org/pdf/2505.07020v1",
    "published_date": "2025-05-11 15:30:23 UTC",
    "updated_date": "2025-05-11 15:30:23 UTC"
  },
  {
    "arxiv_id": "2505.07013v1",
    "title": "Efficient and Robust Multidimensional Attention in Remote Physiological Sensing through Target Signal Constrained Factorization",
    "authors": [
      "Jitesh Joshi",
      "Youngjun Cho"
    ],
    "abstract": "Remote physiological sensing using camera-based technologies offers transformative potential for non-invasive vital sign monitoring across healthcare and human-computer interaction domains. Although deep learning approaches have advanced the extraction of physiological signals from video data, existing methods have not been sufficiently assessed for their robustness to domain shifts. These shifts in remote physiological sensing include variations in ambient conditions, camera specifications, head movements, facial poses, and physiological states which often impact real-world performance significantly. Cross-dataset evaluation provides an objective measure to assess generalization capabilities across these domain shifts. We introduce Target Signal Constrained Factorization module (TSFM), a novel multidimensional attention mechanism that explicitly incorporates physiological signal characteristics as factorization constraints, allowing more precise feature extraction. Building on this innovation, we present MMRPhys, an efficient dual-branch 3D-CNN architecture designed for simultaneous multitask estimation of photoplethysmography (rPPG) and respiratory (rRSP) signals from multimodal RGB and thermal video inputs. Through comprehensive cross-dataset evaluation on five benchmark datasets, we demonstrate that MMRPhys with TSFM significantly outperforms state-of-the-art methods in generalization across domain shifts for rPPG and rRSP estimation, while maintaining a minimal inference latency suitable for real-time applications. Our approach establishes new benchmarks for robust multitask and multimodal physiological sensing and offers a computationally efficient framework for practical deployment in unconstrained environments. The web browser-based application featuring on-device real-time inference of MMRPhys model is available at https://physiologicailab.github.io/mmrphys-live",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "25 pages, 6 figures",
    "pdf_url": "https://arxiv.org/pdf/2505.07013v1",
    "published_date": "2025-05-11 15:20:45 UTC",
    "updated_date": "2025-05-11 15:20:45 UTC"
  },
  {
    "arxiv_id": "2505.07012v1",
    "title": "Hand-Shadow Poser",
    "authors": [
      "Hao Xu",
      "Yinqiao Wang",
      "Niloy J. Mitra",
      "Shuaicheng Liu",
      "Pheng-Ann Heng",
      "Chi-Wing Fu"
    ],
    "abstract": "Hand shadow art is a captivating art form, creatively using hand shadows to reproduce expressive shapes on the wall. In this work, we study an inverse problem: given a target shape, find the poses of left and right hands that together best produce a shadow resembling the input. This problem is nontrivial, since the design space of 3D hand poses is huge while being restrictive due to anatomical constraints. Also, we need to attend to the input's shape and crucial features, though the input is colorless and textureless. To meet these challenges, we design Hand-Shadow Poser, a three-stage pipeline, to decouple the anatomical constraints (by hand) and semantic constraints (by shadow shape): (i) a generative hand assignment module to explore diverse but reasonable left/right-hand shape hypotheses; (ii) a generalized hand-shadow alignment module to infer coarse hand poses with a similarity-driven strategy for selecting hypotheses; and (iii) a shadow-feature-aware refinement module to optimize the hand poses for physical plausibility and shadow feature preservation. Further, we design our pipeline to be trainable on generic public hand data, thus avoiding the need for any specialized training dataset. For method validation, we build a benchmark of 210 diverse shadow shapes of varying complexity and a comprehensive set of metrics, including a novel DINOv2-based evaluation metric. Through extensive comparisons with multiple baselines and user studies, our approach is demonstrated to effectively generate bimanual hand poses for a large variety of hand shapes for over 85% of the benchmark cases.",
    "categories": [
      "cs.CG",
      "cs.AI"
    ],
    "primary_category": "cs.CG",
    "comment": "SIGGRAPH 2025 (ACM TOG)",
    "pdf_url": "https://arxiv.org/pdf/2505.07012v1",
    "published_date": "2025-05-11 15:15:35 UTC",
    "updated_date": "2025-05-11 15:15:35 UTC"
  },
  {
    "arxiv_id": "2505.07005v1",
    "title": "Explainable AI the Latest Advancements and New Trends",
    "authors": [
      "Bowen Long",
      "Enjie Liu",
      "Renxi Qiu",
      "Yanqing Duan"
    ],
    "abstract": "In recent years, Artificial Intelligence technology has excelled in various applications across all domains and fields. However, the various algorithms in neural networks make it difficult to understand the reasons behind decisions. For this reason, trustworthy AI techniques have started gaining popularity. The concept of trustworthiness is cross-disciplinary; it must meet societal standards and principles, and technology is used to fulfill these requirements. In this paper, we first surveyed developments from various countries and regions on the ethical elements that make AI algorithms trustworthy; and then focused our survey on the state of the art research into the interpretability of AI. We have conducted an intensive survey on technologies and techniques used in making AI explainable. Finally, we identified new trends in achieving explainable AI. In particular, we elaborate on the strong link between the explainability of AI and the meta-reasoning of autonomous systems. The concept of meta-reasoning is 'reason the reasoning', which coincides with the intention and goal of explainable Al. The integration of the approaches could pave the way for future interpretable AI systems.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.07005v1",
    "published_date": "2025-05-11 15:01:12 UTC",
    "updated_date": "2025-05-11 15:01:12 UTC"
  },
  {
    "arxiv_id": "2505.06997v1",
    "title": "A Multi-Agent Reinforcement Learning Approach for Cooperative Air-Ground-Human Crowdsensing in Emergency Rescue",
    "authors": [
      "Wenhao Lu",
      "Zhengqiu Zhu",
      "Yong Zhao",
      "Yonglin Tian",
      "Junjie Zeng",
      "Jun Zhang",
      "Zhong Liu",
      "Fei-Yue Wang"
    ],
    "abstract": "Mobile crowdsensing is evolving beyond traditional human-centric models by integrating heterogeneous entities like unmanned aerial vehicles (UAVs) and unmanned ground vehicles (UGVs). Optimizing task allocation among these diverse agents is critical, particularly in challenging emergency rescue scenarios characterized by complex environments, limited communication, and partial observability. This paper tackles the Heterogeneous-Entity Collaborative-Sensing Task Allocation (HECTA) problem specifically for emergency rescue, considering humans, UAVs, and UGVs. We introduce a novel ``Hard-Cooperative'' policy where UGVs prioritize recharging low-battery UAVs, alongside performing their sensing tasks. The primary objective is maximizing the task completion rate (TCR) under strict time constraints. We rigorously formulate this NP-hard problem as a decentralized partially observable Markov decision process (Dec-POMDP) to effectively handle sequential decision-making under uncertainty. To solve this, we propose HECTA4ER, a novel multi-agent reinforcement learning algorithm built upon a Centralized Training with Decentralized Execution architecture. HECTA4ER incorporates tailored designs, including specialized modules for complex feature extraction, utilization of action-observation history via hidden states, and a mixing network integrating global and local information, specifically addressing the challenges of partial observability. Furthermore, theoretical analysis confirms the algorithm's convergence properties. Extensive simulations demonstrate that HECTA4ER significantly outperforms baseline algorithms, achieving an average 18.42% increase in TCR. Crucially, a real-world case study validates the algorithm's effectiveness and robustness in dynamic sensing scenarios, highlighting its strong potential for practical application in emergency response.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.06997v1",
    "published_date": "2025-05-11 14:49:15 UTC",
    "updated_date": "2025-05-11 14:49:15 UTC"
  },
  {
    "arxiv_id": "2505.06993v2",
    "title": "Technical Report: Quantifying and Analyzing the Generalization Power of a DNN",
    "authors": [
      "Yuxuan He",
      "Junpeng Zhang",
      "Lei Cheng",
      "Hongyuan Zhang",
      "Quanshi Zhang"
    ],
    "abstract": "This paper proposes a new perspective for analyzing the generalization power of deep neural networks (DNNs), i.e., directly disentangling and analyzing the dynamics of generalizable and non-generalizable interaction encoded by a DNN through the training process. Specifically, this work builds upon the recent theoretical achievement in explainble AI, which proves that the detailed inference logic of DNNs can be can be strictly rewritten as a small number of AND-OR interaction patterns. Based on this, we propose an efficient method to quantify the generalization power of each interaction, and we discover a distinct three-phase dynamics of the generalization power of interactions during training. In particular, the early phase of training typically removes noisy and non-generalizable interactions and learns simple and generalizable ones. The second and the third phases tend to capture increasingly complex interactions that are harder to generalize. Experimental results verify that the learning of non-generalizable interactions is the the direct cause for the gap between the training and testing losses.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL",
      "cs.CV"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.06993v2",
    "published_date": "2025-05-11 14:37:30 UTC",
    "updated_date": "2025-05-20 15:25:12 UTC"
  },
  {
    "arxiv_id": "2505.06987v6",
    "title": "Convert Language Model into a Value-based Strategic Planner",
    "authors": [
      "Xiaoyu Wang",
      "Yue Zhao",
      "Qingqing Gu",
      "Zhonglin Jiang",
      "Xiaokai Chen",
      "Yong Chen",
      "Luo Ji"
    ],
    "abstract": "Emotional support conversation (ESC) aims to alleviate the emotional distress of individuals through effective conversations. Although large language models (LLMs) have obtained remarkable progress on ESC, most of these studies might not define the diagram from the state model perspective, therefore providing a suboptimal solution for long-term satisfaction. To address such an issue, we leverage the Q-learning on LLMs, and propose a framework called straQ*. Our framework allows a plug-and-play LLM to bootstrap the planning during ESC, determine the optimal strategy based on long-term returns, and finally guide the LLM to response. Substantial experiments on ESC datasets suggest that straQ* outperforms many baselines, including direct inference, self-refine, chain of thought, finetuning, and finite state machines.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "13 pages, 6 figures, ACL 2025 Industry Track",
    "pdf_url": "https://arxiv.org/pdf/2505.06987v6",
    "published_date": "2025-05-11 14:13:58 UTC",
    "updated_date": "2025-08-27 08:21:50 UTC"
  },
  {
    "arxiv_id": "2505.06977v2",
    "title": "CAT Merging: A Training-Free Approach for Resolving Conflicts in Model Merging",
    "authors": [
      "Wenju Sun",
      "Qingyong Li",
      "Yangli-ao Geng",
      "Boyang Li"
    ],
    "abstract": "Multi-task model merging offers a promising paradigm for integrating multiple expert models into a unified model without additional training. Existing state-of-the-art techniques, such as Task Arithmetic and its variants, merge models by accumulating task vectors -- the parameter differences between pretrained and finetuned models. However, task vector accumulation is often hindered by knowledge conflicts, leading to performance degradation. To address this challenge, we propose Conflict-Aware Task Merging (CAT Merging), a novel training-free framework that selectively trims conflict-prone components from the task vectors. CAT Merging introduces several parameter-specific strategies, including projection for linear weights and masking for scaling and shifting parameters in normalization layers. Extensive experiments on vision, language, and vision-language tasks demonstrate that CAT Merging effectively suppresses knowledge conflicts, achieving average accuracy improvements of up to 2.5% (ViT-B/32) and 2.0% (ViT-L/14) over state-of-the-art methods.",
    "categories": [
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.06977v2",
    "published_date": "2025-05-11 13:24:09 UTC",
    "updated_date": "2025-05-14 14:11:52 UTC"
  },
  {
    "arxiv_id": "2505.06964v2",
    "title": "Bridging AI and Carbon Capture: A Dataset for LLMs in Ionic Liquids and CBE Research",
    "authors": [
      "Gaurab Sarkar",
      "Sougata Saha"
    ],
    "abstract": "Large Language Models (LLMs) have demonstrated exceptional performance in general knowledge and reasoning tasks across various domains. However, their effectiveness in specialized scientific fields like Chemical and Biological Engineering (CBE) remains underexplored. Addressing this gap requires robust evaluation benchmarks that assess both knowledge and reasoning capabilities in these niche areas, which are currently lacking. To bridge this divide, we present a comprehensive empirical analysis of LLM reasoning capabilities in CBE, with a focus on Ionic Liquids (ILs) for carbon sequestration - an emerging solution for mitigating global warming. We develop and release an expert - curated dataset of 5,920 examples designed to benchmark LLMs' reasoning in this domain. The dataset incorporates varying levels of difficulty, balancing linguistic complexity and domain-specific knowledge. Using this dataset, we evaluate three open-source LLMs with fewer than 10 billion parameters. Our findings reveal that while smaller general-purpose LLMs exhibit basic knowledge of ILs, they lack the specialized reasoning skills necessary for advanced applications. Building on these results, we discuss strategies to enhance the utility of LLMs for carbon capture research, particularly using ILs. Given the significant carbon footprint of LLMs, aligning their development with IL research presents a unique opportunity to foster mutual progress in both fields and advance global efforts toward achieving carbon neutrality by 2050.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.06964v2",
    "published_date": "2025-05-11 12:32:57 UTC",
    "updated_date": "2025-05-17 05:08:12 UTC"
  },
  {
    "arxiv_id": "2505.06963v1",
    "title": "Reinforcement Learning-Based Monocular Vision Approach for Autonomous UAV Landing",
    "authors": [
      "Tarik Houichime",
      "Younes EL Amrani"
    ],
    "abstract": "This paper introduces an innovative approach for the autonomous landing of Unmanned Aerial Vehicles (UAVs) using only a front-facing monocular camera, therefore obviating the requirement for depth estimation cameras. Drawing on the inherent human estimating process, the proposed method reframes the landing task as an optimization problem. The UAV employs variations in the visual characteristics of a specially designed lenticular circle on the landing pad, where the perceived color and form provide critical information for estimating both altitude and depth. Reinforcement learning algorithms are utilized to approximate the functions governing these estimations, enabling the UAV to ascertain ideal landing settings via training. This method's efficacy is assessed by simulations and experiments, showcasing its potential for robust and accurate autonomous landing without dependence on complex sensor setups. This research contributes to the advancement of cost-effective and efficient UAV landing solutions, paving the way for wider applicability across various fields.",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.RO",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.06963v1",
    "published_date": "2025-05-11 12:23:37 UTC",
    "updated_date": "2025-05-11 12:23:37 UTC"
  },
  {
    "arxiv_id": "2505.06949v1",
    "title": "Causal knowledge graph analysis identifies adverse drug effects",
    "authors": [
      "Sumyyah Toonsi",
      "Paul Schofield",
      "Robert Hoehndorf"
    ],
    "abstract": "Knowledge graphs and structural causal models have each proven valuable for organizing biomedical knowledge and estimating causal effects, but remain largely disconnected: knowledge graphs encode qualitative relationships focusing on facts and deductive reasoning without formal probabilistic semantics, while causal models lack integration with background knowledge in knowledge graphs and have no access to the deductive reasoning capabilities that knowledge graphs provide. To bridge this gap, we introduce a novel formulation of Causal Knowledge Graphs (CKGs) which extend knowledge graphs with formal causal semantics, preserving their deductive capabilities while enabling principled causal inference. CKGs support deconfounding via explicitly marked causal edges and facilitate hypothesis formulation aligned with both encoded and entailed background knowledge. We constructed a Drug-Disease CKG (DD-CKG) integrating disease progression pathways, drug indications, side-effects, and hierarchical disease classification to enable automated large-scale mediation analysis. Applied to UK Biobank and MIMIC-IV cohorts, we tested whether drugs mediate effects between indications and downstream disease progression, adjusting for confounders inferred from the DD-CKG. Our approach successfully reproduced known adverse drug reactions with high precision while identifying previously undocumented significant candidate adverse effects. Further validation through side effect similarity analysis demonstrated that combining our predicted drug effects with established databases significantly improves the prediction of shared drug indications, supporting the clinical relevance of our novel findings. These results demonstrate that our methodology provides a generalizable, knowledge-driven framework for scalable causal inference.",
    "categories": [
      "cs.AI",
      "q-bio.BM"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.06949v1",
    "published_date": "2025-05-11 11:35:43 UTC",
    "updated_date": "2025-05-11 11:35:43 UTC"
  },
  {
    "arxiv_id": "2505.06936v2",
    "title": "AI-Powered Inverse Design of Ku-Band SIW Resonant Structures by Iterative Residual Correction Network",
    "authors": [
      "Mohammad Mashayekhi",
      "Kamran Salehian",
      "Abbas Ozgoli",
      "Saeed Abdollahi",
      "Abdolali Abdipour",
      "Ahmed A. Kishk"
    ],
    "abstract": "Designing high-performance substrate-integrated waveguide (SIW) filters with both closely spaced and widely separated resonances is challenging. Consequently, there is a growing need for robust methods that reduce reliance on time-consuming electromagnetic (EM) simulations.\n  In this study, a deep learning-based framework was developed and validated for the inverse design of multi-mode SIW filters with both closely spaced and widely separated resonances. A series of SIW filters were designed, fabricated, and experimentally evaluated. A three-stage deep learning framework was implemented, consisting of a Feedforward Inverse Model (FIM), a Hybrid Inverse-Forward Residual Refinement Network (HiFR\\textsuperscript{2}-Net), and an Iterative Residual Correction Network (IRC-Net).\n  The design methodology and performance of each model were systematically analyzed. Notably, IRC-Net outperformed both FIM and HiFR\\textsuperscript{2}-Net, achieving systematic error reduction over five correction iterations. Experimental results showed a reduction in mean squared error (MSE) from 0.00191 to 0.00146 and mean absolute error (MAE) from 0.0262 to 0.0209, indicating improved accuracy and convergence.\n  The proposed framework demonstrates the capability to enable robust, accurate, and generalizable inverse design of complex microwave filters with minimal simulation cost. This approach is expected to facilitate rapid prototyping of advanced filter designs and could extend to other high-frequency components in microwave and millimeter-wave technologies.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "The final published version is available at: https://doi.org/10.1016/j.aeue.2025.156003",
    "pdf_url": "https://arxiv.org/pdf/2505.06936v2",
    "published_date": "2025-05-11 10:51:43 UTC",
    "updated_date": "2025-10-02 12:57:22 UTC"
  },
  {
    "arxiv_id": "2505.11526v1",
    "title": "Code Retrieval for MILP Instance Generation",
    "authors": [
      "Tianxing Yang",
      "Huigen Ye",
      "Hua Xu"
    ],
    "abstract": "Mixed-Integer Linear Programming (MILP) is widely used in fields such as scheduling, logistics, and planning. Enhancing the performance of MILP solvers, particularly learning-based solvers, requires substantial amounts of high-quality data. However, existing methods for MILP instance generation typically necessitate training a separate model for each problem class and are computationally intensive when generating new instances. To address these limitations, we reformulate the MILP Instance Generation task as MILP Code Generation task, enabling efficient, flexible, and interpretable instance generation through code. Since MILP instances generated from code can vary significantly in scale, we introduce MILP-EmbedSim, a new similarity metric that accurately measures the similarity between instances of varying sizes within the same problem class. Leveraging this metric, we propose MILP-Retrieval, a pipeline that retrieves generation code from library to produce MILP instances highly similar to target instance. MILP-Retrieval outperforms baselines in both MILP Code Generation and Instance Generation tasks, provides a novel perspective on MILP instance generation and opens new possibilities for learning-based solvers.",
    "categories": [
      "math.OC",
      "cs.AI"
    ],
    "primary_category": "math.OC",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.11526v1",
    "published_date": "2025-05-11 10:41:44 UTC",
    "updated_date": "2025-05-11 10:41:44 UTC"
  },
  {
    "arxiv_id": "2505.06913v1",
    "title": "RedTeamLLM: an Agentic AI framework for offensive security",
    "authors": [
      "Brian Challita",
      "Pierre Parrend"
    ],
    "abstract": "From automated intrusion testing to discovery of zero-day attacks before software launch, agentic AI calls for great promises in security engineering. This strong capability is bound with a similar threat: the security and research community must build up its models before the approach is leveraged by malicious actors for cybercrime. We therefore propose and evaluate RedTeamLLM, an integrated architecture with a comprehensive security model for automatization of pentest tasks. RedTeamLLM follows three key steps: summarizing, reasoning and act, which embed its operational capacity. This novel framework addresses four open challenges: plan correction, memory management, context window constraint, and generality vs. specialization. Evaluation is performed through the automated resolution of a range of entry-level, but not trivial, CTF challenges. The contribution of the reasoning capability of our agentic AI framework is specifically evaluated.",
    "categories": [
      "cs.CR",
      "cs.AI",
      "cs.CY"
    ],
    "primary_category": "cs.CR",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.06913v1",
    "published_date": "2025-05-11 09:19:10 UTC",
    "updated_date": "2025-05-11 09:19:10 UTC"
  },
  {
    "arxiv_id": "2505.06911v3",
    "title": "MMiC: Mitigating Modality Incompleteness in Clustered Federated Learning",
    "authors": [
      "Lishan Yang",
      "Wei Emma Zhang",
      "Quan Z. Sheng",
      "Lina Yao",
      "Weitong Chen",
      "Ali Shakeri"
    ],
    "abstract": "In the era of big data, data mining has become indispensable for uncovering hidden patterns and insights from vast and complex datasets. The integration of multimodal data sources further enhances its potential. Multimodal Federated Learning (MFL) is a distributed approach that enhances the efficiency and quality of multimodal learning, ensuring collaborative work and privacy protection. However, missing modalities pose a significant challenge in MFL, often due to data quality issues or privacy policies across the clients. In this work, we present MMiC, a framework for Mitigating Modality incompleteness in MFL within the Clusters. MMiC replaces partial parameters within client models inside clusters to mitigate the impact of missing modalities. Furthermore, it leverages the Banzhaf Power Index to optimize client selection under these conditions. Finally, MMiC employs an innovative approach to dynamically control global aggregation by utilizing Markovitz Portfolio Optimization. Extensive experiments demonstrate that MMiC consistently outperforms existing federated learning architectures in both global and personalized performance on multimodal datasets with missing modalities, confirming the effectiveness of our proposed solution. Our code is available at https://github.com/gotobcn8/MMiC.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "9 pages",
    "pdf_url": "https://arxiv.org/pdf/2505.06911v3",
    "published_date": "2025-05-11 09:12:36 UTC",
    "updated_date": "2025-08-21 10:38:28 UTC"
  },
  {
    "arxiv_id": "2505.06907v1",
    "title": "Towards Artificial General or Personalized Intelligence? A Survey on Foundation Models for Personalized Federated Intelligence",
    "authors": [
      "Yu Qiao",
      "Huy Q. Le",
      "Avi Deb Raha",
      "Phuong-Nam Tran",
      "Apurba Adhikary",
      "Mengchun Zhang",
      "Loc X. Nguyen",
      "Eui-Nam Huh",
      "Dusit Niyato",
      "Choong Seon Hong"
    ],
    "abstract": "The rise of large language models (LLMs), such as ChatGPT, DeepSeek, and Grok-3, has reshaped the artificial intelligence landscape. As prominent examples of foundational models (FMs) built on LLMs, these models exhibit remarkable capabilities in generating human-like content, bringing us closer to achieving artificial general intelligence (AGI). However, their large-scale nature, sensitivity to privacy concerns, and substantial computational demands present significant challenges to personalized customization for end users. To bridge this gap, this paper presents the vision of artificial personalized intelligence (API), focusing on adapting these powerful models to meet the specific needs and preferences of users while maintaining privacy and efficiency. Specifically, this paper proposes personalized federated intelligence (PFI), which integrates the privacy-preserving advantages of federated learning (FL) with the zero-shot generalization capabilities of FMs, enabling personalized, efficient, and privacy-protective deployment at the edge. We first review recent advances in both FL and FMs, and discuss the potential of leveraging FMs to enhance federated systems. We then present the key motivations behind realizing PFI and explore promising opportunities in this space, including efficient PFI, trustworthy PFI, and PFI empowered by retrieval-augmented generation (RAG). Finally, we outline key challenges and future research directions for deploying FM-powered FL systems at the edge with improved personalization, computational efficiency, and privacy guarantees. Overall, this survey aims to lay the groundwork for the development of API as a complement to AGI, with a particular focus on PFI as a key enabling technique.",
    "categories": [
      "cs.AI",
      "cs.CV",
      "cs.NE"
    ],
    "primary_category": "cs.AI",
    "comment": "On going work",
    "pdf_url": "https://arxiv.org/pdf/2505.06907v1",
    "published_date": "2025-05-11 08:57:53 UTC",
    "updated_date": "2025-05-11 08:57:53 UTC"
  },
  {
    "arxiv_id": "2505.06897v1",
    "title": "Embodied Intelligence: The Key to Unblocking Generalized Artificial Intelligence",
    "authors": [
      "Jinhao Jiang",
      "Changlin Chen",
      "Shile Feng",
      "Wanru Geng",
      "Zesheng Zhou",
      "Ni Wang",
      "Shuai Li",
      "Feng-Qi Cui",
      "Erbao Dong"
    ],
    "abstract": "The ultimate goal of artificial intelligence (AI) is to achieve Artificial General Intelligence (AGI). Embodied Artificial Intelligence (EAI), which involves intelligent systems with physical presence and real-time interaction with the environment, has emerged as a key research direction in pursuit of AGI. While advancements in deep learning, reinforcement learning, large-scale language models, and multimodal technologies have significantly contributed to the progress of EAI, most existing reviews focus on specific technologies or applications. A systematic overview, particularly one that explores the direct connection between EAI and AGI, remains scarce. This paper examines EAI as a foundational approach to AGI, systematically analyzing its four core modules: perception, intelligent decision-making, action, and feedback. We provide a detailed discussion of how each module contributes to the six core principles of AGI. Additionally, we discuss future trends, challenges, and research directions in EAI, emphasizing its potential as a cornerstone for AGI development. Our findings suggest that EAI's integration of dynamic learning and real-world interaction is essential for bridging the gap between narrow AI and AGI.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "19pages,7 figures,3 tables",
    "pdf_url": "https://arxiv.org/pdf/2505.06897v1",
    "published_date": "2025-05-11 08:29:20 UTC",
    "updated_date": "2025-05-11 08:29:20 UTC"
  },
  {
    "arxiv_id": "2505.06894v1",
    "title": "NeuGen: Amplifying the 'Neural' in Neural Radiance Fields for Domain Generalization",
    "authors": [
      "Ahmed Qazi",
      "Abdul Basit",
      "Asim Iqbal"
    ],
    "abstract": "Neural Radiance Fields (NeRF) have significantly advanced the field of novel view synthesis, yet their generalization across diverse scenes and conditions remains challenging. Addressing this, we propose the integration of a novel brain-inspired normalization technique Neural Generalization (NeuGen) into leading NeRF architectures which include MVSNeRF and GeoNeRF. NeuGen extracts the domain-invariant features, thereby enhancing the models' generalization capabilities. It can be seamlessly integrated into NeRF architectures and cultivates a comprehensive feature set that significantly improves accuracy and robustness in image rendering. Through this integration, NeuGen shows improved performance on benchmarks on diverse datasets across state-of-the-art NeRF architectures, enabling them to generalize better across varied scenes. Our comprehensive evaluations, both quantitative and qualitative, confirm that our approach not only surpasses existing models in generalizability but also markedly improves rendering quality. Our work exemplifies the potential of merging neuroscientific principles with deep learning frameworks, setting a new precedent for enhanced generalizability and efficiency in novel view synthesis. A demo of our study is available at https://neugennerf.github.io.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG",
      "cs.NE"
    ],
    "primary_category": "cs.CV",
    "comment": "18 pages, 6 figures",
    "pdf_url": "https://arxiv.org/pdf/2505.06894v1",
    "published_date": "2025-05-11 08:17:33 UTC",
    "updated_date": "2025-05-11 08:17:33 UTC"
  },
  {
    "arxiv_id": "2505.06889v1",
    "title": "IM-BERT: Enhancing Robustness of BERT through the Implicit Euler Method",
    "authors": [
      "Mihyeon Kim",
      "Juhyoung Park",
      "Youngbin Kim"
    ],
    "abstract": "Pre-trained Language Models (PLMs) have achieved remarkable performance on diverse NLP tasks through pre-training and fine-tuning. However, fine-tuning the model with a large number of parameters on limited downstream datasets often leads to vulnerability to adversarial attacks, causing overfitting of the model on standard datasets.\n  To address these issues, we propose IM-BERT from the perspective of a dynamic system by conceptualizing a layer of BERT as a solution of Ordinary Differential Equations (ODEs). Under the situation of initial value perturbation, we analyze the numerical stability of two main numerical ODE solvers: the explicit and implicit Euler approaches.\n  Based on these analyses, we introduce a numerically robust IM-connection incorporating BERT's layers. This strategy enhances the robustness of PLMs against adversarial attacks, even in low-resource scenarios, without introducing additional parameters or adversarial training strategies.\n  Experimental results on the adversarial GLUE (AdvGLUE) dataset validate the robustness of IM-BERT under various conditions. Compared to the original BERT, IM-BERT exhibits a performance improvement of approximately 8.3\\%p on the AdvGLUE dataset. Furthermore, in low-resource scenarios, IM-BERT outperforms BERT by achieving 5.9\\%p higher accuracy.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted to EMNLP 2024 Main",
    "pdf_url": "https://arxiv.org/pdf/2505.06889v1",
    "published_date": "2025-05-11 07:54:33 UTC",
    "updated_date": "2025-05-11 07:54:33 UTC"
  },
  {
    "arxiv_id": "2505.06886v1",
    "title": "Mice to Machines: Neural Representations from Visual Cortex for Domain Generalization",
    "authors": [
      "Ahmed Qazi",
      "Hamd Jalil",
      "Asim Iqbal"
    ],
    "abstract": "The mouse is one of the most studied animal models in the field of systems neuroscience. Understanding the generalized patterns and decoding the neural representations that are evoked by the diverse range of natural scene stimuli in the mouse visual cortex is one of the key quests in computational vision. In recent years, significant parallels have been drawn between the primate visual cortex and hierarchical deep neural networks. However, their generalized efficacy in understanding mouse vision has been limited. In this study, we investigate the functional alignment between the mouse visual cortex and deep learning models for object classification tasks. We first introduce a generalized representational learning strategy that uncovers a striking resemblance between the functional mapping of the mouse visual cortex and high-performing deep learning models on both top-down (population-level) and bottom-up (single cell-level) scenarios. Next, this representational similarity across the two systems is further enhanced by the addition of Neural Response Normalization (NeuRN) layer, inspired by the activation profile of excitatory and inhibitory neurons in the visual cortex. To test the performance effect of NeuRN on real-world tasks, we integrate it into deep learning models and observe significant improvements in their robustness against data shifts in domain generalization tasks. Our work proposes a novel framework for comparing the functional architecture of the mouse visual cortex with deep learning models. Our findings carry broad implications for the development of advanced AI models that draw inspiration from the mouse visual cortex, suggesting that these models serve as valuable tools for studying the neural representations of the mouse visual cortex and, as a result, enhancing their performance on real-world tasks.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG",
      "cs.NE"
    ],
    "primary_category": "cs.CV",
    "comment": "12 pages, 8 figures, 1 table",
    "pdf_url": "https://arxiv.org/pdf/2505.06886v1",
    "published_date": "2025-05-11 07:37:37 UTC",
    "updated_date": "2025-05-11 07:37:37 UTC"
  },
  {
    "arxiv_id": "2505.06883v2",
    "title": "FACET: Force-Adaptive Control via Impedance Reference Tracking for Legged Robots",
    "authors": [
      "Botian Xu",
      "Haoyang Weng",
      "Qingzhou Lu",
      "Yang Gao",
      "Huazhe Xu"
    ],
    "abstract": "Reinforcement learning (RL) has made significant strides in legged robot control, enabling locomotion across diverse terrains and complex loco-manipulation capabilities. However, the commonly used position or velocity tracking-based objectives are agnostic to forces experienced by the robot, leading to stiff and potentially dangerous behaviors and poor control during forceful interactions. To address this limitation, we present \\emph{Force-Adaptive Control via Impedance Reference Tracking} (FACET). Inspired by impedance control, we use RL to train a control policy to imitate a virtual mass-spring-damper system, allowing fine-grained control under external forces by manipulating the virtual spring. In simulation, we demonstrate that our quadruped robot achieves improved robustness to large impulses (up to 200 Ns) and exhibits controllable compliance, achieving an 80% reduction in collision impulse. The policy is deployed to a physical robot to showcase both compliance and the ability to engage with large forces by kinesthetic control and pulling payloads up to 2/3 of its weight. Further extension to a legged loco-manipulator and a humanoid shows the applicability of our method to more complex settings to enable whole-body compliance control. Project Website: https://facet.pages.dev/",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.RO",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.06883v2",
    "published_date": "2025-05-11 07:23:26 UTC",
    "updated_date": "2025-05-19 11:28:40 UTC"
  },
  {
    "arxiv_id": "2505.06881v1",
    "title": "NeuRN: Neuro-inspired Domain Generalization for Image Classification",
    "authors": [
      "Hamd Jalil",
      "Ahmed Qazi",
      "Asim Iqbal"
    ],
    "abstract": "Domain generalization in image classification is a crucial challenge, with models often failing to generalize well across unseen datasets. We address this issue by introducing a neuro-inspired Neural Response Normalization (NeuRN) layer which draws inspiration from neurons in the mammalian visual cortex, which aims to enhance the performance of deep learning architectures on unseen target domains by training deep learning models on a source domain. The performance of these models is considered as a baseline and then compared against models integrated with NeuRN on image classification tasks. We perform experiments across a range of deep learning architectures, including ones derived from Neural Architecture Search and Vision Transformer. Additionally, in order to shortlist models for our experiment from amongst the vast range of deep neural networks available which have shown promising results, we also propose a novel method that uses the Needleman-Wunsch algorithm to compute similarity between deep learning architectures. Our results demonstrate the effectiveness of NeuRN by showing improvement against baseline in cross-domain image classification tasks. Our framework attempts to establish a foundation for future neuro-inspired deep learning models.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG",
      "cs.NE"
    ],
    "primary_category": "cs.CV",
    "comment": "14 pages, 7 figures, 1 table",
    "pdf_url": "https://arxiv.org/pdf/2505.06881v1",
    "published_date": "2025-05-11 07:20:11 UTC",
    "updated_date": "2025-05-11 07:20:11 UTC"
  },
  {
    "arxiv_id": "2505.06874v2",
    "title": "Enhancing Time Series Forecasting via a Parallel Hybridization of ARIMA and Polynomial Classifiers",
    "authors": [
      "Thanh Son Nguyen",
      "Van Thanh Nguyen",
      "Dang Minh Duc Nguyen"
    ],
    "abstract": "Time series forecasting has attracted significant attention, leading to the de-velopment of a wide range of approaches, from traditional statistical meth-ods to advanced deep learning models. Among them, the Auto-Regressive Integrated Moving Average (ARIMA) model remains a widely adopted linear technique due to its effectiveness in modeling temporal dependencies in economic, industrial, and social data. On the other hand, polynomial classifi-ers offer a robust framework for capturing non-linear relationships and have demonstrated competitive performance in domains such as stock price pre-diction. In this study, we propose a hybrid forecasting approach that inte-grates the ARIMA model with a polynomial classifier to leverage the com-plementary strengths of both models. The hybrid method is evaluated on multiple real-world time series datasets spanning diverse domains. Perfor-mance is assessed based on forecasting accuracy and computational effi-ciency. Experimental results reveal that the proposed hybrid model consist-ently outperforms the individual models in terms of prediction accuracy, al-beit with a modest increase in execution time.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.06874v2",
    "published_date": "2025-05-11 06:53:19 UTC",
    "updated_date": "2025-05-27 03:16:32 UTC"
  },
  {
    "arxiv_id": "2506.01966v3",
    "title": "Unified Sparse-Matrix Representations for Diverse Neural Architectures",
    "authors": [
      "Yuzhou Zhu"
    ],
    "abstract": "Deep neural networks employ specialized architectures for vision, sequential and language tasks, yet this proliferation obscures their underlying commonalities. We introduce a unified matrix-order framework that casts convolutional, recurrent and self-attention operations as sparse matrix multiplications. Convolution is realized via an upper-triangular weight matrix performing first-order transformations; recurrence emerges from a lower-triangular matrix encoding stepwise updates; attention arises naturally as a third-order tensor factorization. We prove algebraic isomorphism with standard CNN, RNN and Transformer layers under mild assumptions. Empirical evaluations on image classification (MNIST, CIFAR-10/100, Tiny ImageNet), time-series forecasting (ETTh1, Electricity Load Diagrams) and language modeling/classification (AG News, WikiText-2, Penn Treebank) confirm that sparse-matrix formulations match or exceed native model performance while converging in comparable or fewer epochs. By reducing architecture design to sparse pattern selection, our matrix perspective aligns with GPU parallelism and leverages mature algebraic optimization tools. This work establishes a mathematically rigorous substrate for diverse neural architectures and opens avenues for principled, hardware-aware network design.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.01966v3",
    "published_date": "2025-05-11 06:26:34 UTC",
    "updated_date": "2025-07-22 20:07:02 UTC"
  },
  {
    "arxiv_id": "2505.06861v2",
    "title": "Efficient Robotic Policy Learning via Latent Space Backward Planning",
    "authors": [
      "Dongxiu Liu",
      "Haoyi Niu",
      "Zhihao Wang",
      "Jinliang Zheng",
      "Yinan Zheng",
      "Zhonghong Ou",
      "Jianming Hu",
      "Jianxiong Li",
      "Xianyuan Zhan"
    ],
    "abstract": "Current robotic planning methods often rely on predicting multi-frame images with full pixel details. While this fine-grained approach can serve as a generic world model, it introduces two significant challenges for downstream policy learning: substantial computational costs that hinder real-time deployment, and accumulated inaccuracies that can mislead action extraction. Planning with coarse-grained subgoals partially alleviates efficiency issues. However, their forward planning schemes can still result in off-task predictions due to accumulation errors, leading to misalignment with long-term goals. This raises a critical question: Can robotic planning be both efficient and accurate enough for real-time control in long-horizon, multi-stage tasks? To address this, we propose a Latent Space Backward Planning scheme (LBP), which begins by grounding the task into final latent goals, followed by recursively predicting intermediate subgoals closer to the current state. The grounded final goal enables backward subgoal planning to always remain aware of task completion, facilitating on-task prediction along the entire planning horizon. The subgoal-conditioned policy incorporates a learnable token to summarize the subgoal sequences and determines how each subgoal guides action extraction. Through extensive simulation and real-robot long-horizon experiments, we show that LBP outperforms existing fine-grained and forward planning methods, achieving SOTA performance. Project Page: https://lbp-authors.github.io",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.RO",
    "comment": "Accepted by ICML 2025",
    "pdf_url": "https://arxiv.org/pdf/2505.06861v2",
    "published_date": "2025-05-11 06:13:51 UTC",
    "updated_date": "2025-05-27 10:30:16 UTC"
  },
  {
    "arxiv_id": "2505.06860v1",
    "title": "DP-TRAE: A Dual-Phase Merging Transferable Reversible Adversarial Example for Image Privacy Protection",
    "authors": [
      "Xia Du",
      "Jiajie Zhu",
      "Jizhe Zhou",
      "Chi-man Pun",
      "Zheng Lin",
      "Cong Wu",
      "Zhe Chen",
      "Jun Luo"
    ],
    "abstract": "In the field of digital security, Reversible Adversarial Examples (RAE) combine adversarial attacks with reversible data hiding techniques to effectively protect sensitive data and prevent unauthorized analysis by malicious Deep Neural Networks (DNNs). However, existing RAE techniques primarily focus on white-box attacks, lacking a comprehensive evaluation of their effectiveness in black-box scenarios. This limitation impedes their broader deployment in complex, dynamic environments. Further more, traditional black-box attacks are often characterized by poor transferability and high query costs, significantly limiting their practical applicability. To address these challenges, we propose the Dual-Phase Merging Transferable Reversible Attack method, which generates highly transferable initial adversarial perturbations in a white-box model and employs a memory augmented black-box strategy to effectively mislead target mod els. Experimental results demonstrate the superiority of our approach, achieving a 99.0% attack success rate and 100% recovery rate in black-box scenarios, highlighting its robustness in privacy protection. Moreover, we successfully implemented a black-box attack on a commercial model, further substantiating the potential of this approach for practical use.",
    "categories": [
      "cs.CR",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CR",
    "comment": "12 pages, 5 figures",
    "pdf_url": "https://arxiv.org/pdf/2505.06860v1",
    "published_date": "2025-05-11 06:11:10 UTC",
    "updated_date": "2025-05-11 06:11:10 UTC"
  },
  {
    "arxiv_id": "2505.06856v1",
    "title": "Beyond Patterns: Harnessing Causal Logic for Autonomous Driving Trajectory Prediction",
    "authors": [
      "Bonan Wang",
      "Haicheng Liao",
      "Chengyue Wang",
      "Bin Rao",
      "Yanchen Guan",
      "Guyang Yu",
      "Jiaxun Zhang",
      "Songning Lai",
      "Chengzhong Xu",
      "Zhenning Li"
    ],
    "abstract": "Accurate trajectory prediction has long been a major challenge for autonomous driving (AD). Traditional data-driven models predominantly rely on statistical correlations, often overlooking the causal relationships that govern traffic behavior. In this paper, we introduce a novel trajectory prediction framework that leverages causal inference to enhance predictive robustness, generalization, and accuracy. By decomposing the environment into spatial and temporal components, our approach identifies and mitigates spurious correlations, uncovering genuine causal relationships. We also employ a progressive fusion strategy to integrate multimodal information, simulating human-like reasoning processes and enabling real-time inference. Evaluations on five real-world datasets--ApolloScape, nuScenes, NGSIM, HighD, and MoCAD--demonstrate our model's superiority over existing state-of-the-art (SOTA) methods, with improvements in key metrics such as RMSE and FDE. Our findings highlight the potential of causal reasoning to transform trajectory prediction, paving the way for robust AD systems.",
    "categories": [
      "cs.AI",
      "cs.RO"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.06856v1",
    "published_date": "2025-05-11 05:56:07 UTC",
    "updated_date": "2025-05-11 05:56:07 UTC"
  },
  {
    "arxiv_id": "2505.07888v1",
    "title": "Implementing Long Text Style Transfer with LLMs through Dual-Layered Sentence and Paragraph Structure Extraction and Mapping",
    "authors": [
      "Yusen Wu",
      "Xiaotie Deng"
    ],
    "abstract": "This paper addresses the challenge in long-text style transfer using zero-shot learning of large language models (LLMs), proposing a hierarchical framework that combines sentence-level stylistic adaptation with paragraph-level structural coherence. We argue that in the process of effective paragraph-style transfer, to preserve the consistency of original syntactic and semantic information, it is essential to perform style transfer not only at the sentence level but also to incorporate paragraph-level semantic considerations, while ensuring structural coherence across inter-sentential relationships. Our proposed framework, ZeroStylus, operates through two systematic phases: hierarchical template acquisition from reference texts and template-guided generation with multi-granular matching. The framework dynamically constructs sentence and paragraph template repositories, enabling context-aware transformations while preserving inter-sentence logical relationships. Experimental evaluations demonstrate significant improvements over baseline methods, with structured rewriting achieving 6.90 average score compared to 6.70 for direct prompting approaches in tri-axial metrics assessing style consistency, content preservation, and expression quality. Ablation studies validate the necessity of both template hierarchies during style transfer, showing higher content preservation win rate against sentence-only approaches through paragraph-level structural encoding, as well as direct prompting method through sentence-level pattern extraction and matching. The results establish new capabilities for coherent long-text style transfer without requiring parallel corpora or LLM fine-tuning.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.07888v1",
    "published_date": "2025-05-11 05:53:33 UTC",
    "updated_date": "2025-05-11 05:53:33 UTC"
  },
  {
    "arxiv_id": "2505.06841v1",
    "title": "Optimizing Recommendations using Fine-Tuned LLMs",
    "authors": [
      "Prabhdeep Cheema",
      "Erhan Guven"
    ],
    "abstract": "As digital media platforms strive to meet evolving user expectations, delivering highly personalized and intuitive movies and media recommendations has become essential for attracting and retaining audiences. Traditional systems often rely on keyword-based search and recommendation techniques, which limit users to specific keywords and a combination of keywords. This paper proposes an approach that generates synthetic datasets by modeling real-world user interactions, creating complex chat-style data reflective of diverse preferences. This allows users to express more information with complex preferences, such as mood, plot details, and thematic elements, in addition to conventional criteria like genre, title, and actor-based searches. In today's search space, users cannot write queries like ``Looking for a fantasy movie featuring dire wolves, ideally set in a harsh frozen world with themes of loyalty and survival.''\n  Building on these contributions, we evaluate synthetic datasets for diversity and effectiveness in training and benchmarking models, particularly in areas often absent from traditional datasets. This approach enhances personalization and accuracy by enabling expressive and natural user queries. It establishes a foundation for the next generation of conversational AI-driven search and recommendation systems in digital entertainment.",
    "categories": [
      "cs.IR",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.IR",
    "comment": "Accepted and presented at IEEE CAI 2025. This version includes minor clarifications and formatting updates",
    "pdf_url": "https://arxiv.org/pdf/2505.06841v1",
    "published_date": "2025-05-11 04:53:34 UTC",
    "updated_date": "2025-05-11 04:53:34 UTC"
  },
  {
    "arxiv_id": "2505.06839v1",
    "title": "The power of fine-grained experts: Granularity boosts expressivity in Mixture of Experts",
    "authors": [
      "Enric Boix-Adsera",
      "Philippe Rigollet"
    ],
    "abstract": "Mixture-of-Experts (MoE) layers are increasingly central to frontier model architectures. By selectively activating parameters, they reduce computational cost while scaling total parameter count. This paper investigates the impact of the number of active experts, termed granularity, comparing architectures with many (e.g., 8 per layer in DeepSeek) to those with fewer (e.g., 1 per layer in Llama-4 models). We prove an exponential separation in network expressivity based on this design parameter, suggesting that models benefit from higher granularity. Experimental results corroborate our theoretical findings and illustrate this separation.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.06839v1",
    "published_date": "2025-05-11 04:35:40 UTC",
    "updated_date": "2025-05-11 04:35:40 UTC"
  },
  {
    "arxiv_id": "2505.06827v1",
    "title": "Sandcastles in the Storm: Revisiting the (Im)possibility of Strong Watermarking",
    "authors": [
      "Fabrice Y Harel-Canada",
      "Boran Erol",
      "Connor Choi",
      "Jason Liu",
      "Gary Jiarui Song",
      "Nanyun Peng",
      "Amit Sahai"
    ],
    "abstract": "Watermarking AI-generated text is critical for combating misuse. Yet recent theoretical work argues that any watermark can be erased via random walk attacks that perturb text while preserving quality. However, such attacks rely on two key assumptions: (1) rapid mixing (watermarks dissolve quickly under perturbations) and (2) reliable quality preservation (automated quality oracles perfectly guide edits). Through large-scale experiments and human-validated assessments, we find mixing is slow: 100% of perturbed texts retain traces of their origin after hundreds of edits, defying rapid mixing. Oracles falter, as state-of-the-art quality detectors misjudge edits (77% accuracy), compounding errors during attacks. Ultimately, attacks underperform: automated walks remove watermarks just 26% of the time -- dropping to 10% under human quality review. These findings challenge the inevitability of watermark removal. Instead, practical barriers -- slow mixing and imperfect quality control -- reveal watermarking to be far more robust than theoretical models suggest. The gap between idealized attacks and real-world feasibility underscores the need for stronger watermarking methods and more realistic attack models.",
    "categories": [
      "cs.CR",
      "cs.AI"
    ],
    "primary_category": "cs.CR",
    "comment": "In Review @ ACL 2025",
    "pdf_url": "https://arxiv.org/pdf/2505.06827v1",
    "published_date": "2025-05-11 03:41:13 UTC",
    "updated_date": "2025-05-11 03:41:13 UTC"
  },
  {
    "arxiv_id": "2505.06821v1",
    "title": "ThreatLens: LLM-guided Threat Modeling and Test Plan Generation for Hardware Security Verification",
    "authors": [
      "Dipayan Saha",
      "Hasan Al Shaikh",
      "Shams Tarek",
      "Farimah Farahmandi"
    ],
    "abstract": "Current hardware security verification processes predominantly rely on manual threat modeling and test plan generation, which are labor-intensive, error-prone, and struggle to scale with increasing design complexity and evolving attack methodologies. To address these challenges, we propose ThreatLens, an LLM-driven multi-agent framework that automates security threat modeling and test plan generation for hardware security verification. ThreatLens integrates retrieval-augmented generation (RAG) to extract relevant security knowledge, LLM-powered reasoning for threat assessment, and interactive user feedback to ensure the generation of practical test plans. By automating these processes, the framework reduces the manual verification effort, enhances coverage, and ensures a structured, adaptable approach to security verification. We evaluated our framework on the NEORV32 SoC, demonstrating its capability to automate security verification through structured test plans and validating its effectiveness in real-world scenarios.",
    "categories": [
      "cs.CR",
      "cs.AI",
      "cs.ET"
    ],
    "primary_category": "cs.CR",
    "comment": "This paper has been presented at IEEE VLSI Test Symposium (VTS) 2025",
    "pdf_url": "https://arxiv.org/pdf/2505.06821v1",
    "published_date": "2025-05-11 03:10:39 UTC",
    "updated_date": "2025-05-11 03:10:39 UTC"
  },
  {
    "arxiv_id": "2505.06817v1",
    "title": "Control Plane as a Tool: A Scalable Design Pattern for Agentic AI Systems",
    "authors": [
      "Sivasathivel Kandasamy"
    ],
    "abstract": "Agentic AI systems represent a new frontier in artificial intelligence, where agents often based on large language models(LLMs) interact with tools, environments, and other agents to accomplish tasks with a degree of autonomy. These systems show promise across a range of domains, but their architectural underpinnings remain immature. This paper conducts a comprehensive review of the types of agents, their modes of interaction with the environment, and the infrastructural and architectural challenges that emerge. We identify a gap in how these systems manage tool orchestration at scale and propose a reusable design abstraction: the \"Control Plane as a Tool\" pattern. This pattern allows developers to expose a single tool interface to an agent while encapsulating modular tool routing logic behind it. We position this pattern within the broader context of agent design and argue that it addresses several key challenges in scaling, safety, and extensibility.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "2 Figures and 2 Tables",
    "pdf_url": "https://arxiv.org/pdf/2505.06817v1",
    "published_date": "2025-05-11 02:58:50 UTC",
    "updated_date": "2025-05-11 02:58:50 UTC"
  },
  {
    "arxiv_id": "2505.06814v1",
    "title": "Overview of the NLPCC 2025 Shared Task 4: Multi-modal, Multilingual, and Multi-hop Medical Instructional Video Question Answering Challenge",
    "authors": [
      "Bin Li",
      "Shenxi Liu",
      "Yixuan Weng",
      "Yue Du",
      "Yuhang Tian",
      "Shoujun Zhou"
    ],
    "abstract": "Following the successful hosts of the 1-st (NLPCC 2023 Foshan) CMIVQA and the 2-rd (NLPCC 2024 Hangzhou) MMIVQA challenges, this year, a new task has been introduced to further advance research in multi-modal, multilingual, and multi-hop medical instructional question answering (M4IVQA) systems, with a specific focus on medical instructional videos. The M4IVQA challenge focuses on evaluating models that integrate information from medical instructional videos, understand multiple languages, and answer multi-hop questions requiring reasoning over various modalities. This task consists of three tracks: multi-modal, multilingual, and multi-hop Temporal Answer Grounding in Single Video (M4TAGSV), multi-modal, multilingual, and multi-hop Video Corpus Retrieval (M4VCR) and multi-modal, multilingual, and multi-hop Temporal Answer Grounding in Video Corpus (M4TAGVC). Participants in M4IVQA are expected to develop algorithms capable of processing both video and text data, understanding multilingual queries, and providing relevant answers to multi-hop medical questions. We believe the newly introduced M4IVQA challenge will drive innovations in multimodal reasoning systems for healthcare scenarios, ultimately contributing to smarter emergency response systems and more effective medical education platforms in multilingual communities. Our official website is https://cmivqa.github.io/",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.CV",
    "comment": "12 pages, 5 figures, 4 tables",
    "pdf_url": "https://arxiv.org/pdf/2505.06814v1",
    "published_date": "2025-05-11 02:15:14 UTC",
    "updated_date": "2025-05-11 02:15:14 UTC"
  },
  {
    "arxiv_id": "2505.07886v1",
    "title": "PLHF: Prompt Optimization with Few-Shot Human Feedback",
    "authors": [
      "Chun-Pai Yang",
      "Kan Zheng",
      "Shou-De Lin"
    ],
    "abstract": "Automatic prompt optimization frameworks are developed to obtain suitable prompts for large language models (LLMs) with respect to desired output quality metrics. Although existing approaches can handle conventional tasks such as fixed-solution question answering, defining the metric becomes complicated when the output quality cannot be easily assessed by comparisons with standard golden samples. Consequently, optimizing the prompts effectively and efficiently without a clear metric becomes a critical challenge. To address the issue, we present PLHF (which stands for \"P\"rompt \"L\"earning with \"H\"uman \"F\"eedback), a few-shot prompt optimization framework inspired by the well-known RLHF technique. Different from naive strategies, PLHF employs a specific evaluator module acting as the metric to estimate the output quality. PLHF requires only a single round of human feedback to complete the entire prompt optimization process. Empirical results on both public and industrial datasets show that PLHF outperforms prior output grading strategies for LLM prompt optimizations.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.07886v1",
    "published_date": "2025-05-11 00:56:03 UTC",
    "updated_date": "2025-05-11 00:56:03 UTC"
  },
  {
    "arxiv_id": "2505.06799v2",
    "title": "Quantum Observers: A NISQ Hardware Demonstration of Chaotic State Prediction Using Quantum Echo-state Networks",
    "authors": [
      "Erik L. Connerty",
      "Ethan N. Evans",
      "Gerasimos Angelatos",
      "Vignesh Narayanan"
    ],
    "abstract": "Recent advances in artificial intelligence have highlighted the remarkable capabilities of neural network (NN)-powered systems on classical computers. However, these systems face significant computational challenges that limit scalability and efficiency. Quantum computers hold the potential to overcome these limitations and increase processing power beyond classical systems. Despite this, integrating quantum computing with NNs remains largely unrealized due to challenges posed by noise, decoherence, and high error rates in current quantum hardware. Here, we propose a novel quantum echo-state network (QESN) design and implementation algorithm that can operate within the presence of noise on current IBM hardware. We apply classical control-theoretic response analysis to characterize the QESN, emphasizing its rich nonlinear dynamics and memory, as well as its ability to be fine-tuned with sparsity and re-uploading blocks. We validate our approach through a comprehensive demonstration of QESNs functioning as quantum observers, applied in both high-fidelity simulations and hardware experiments utilizing data from a prototypical chaotic Lorenz system. Our results show that the QESN can predict long time-series with persistent memory, running over 100 times longer than the median T1 and T2 of the IBM Marrakesh QPU, achieving state-of-the-art time-series performance on superconducting hardware.",
    "categories": [
      "quant-ph",
      "cs.AI"
    ],
    "primary_category": "quant-ph",
    "comment": "14 pages, 12 figures",
    "pdf_url": "https://arxiv.org/pdf/2505.06799v2",
    "published_date": "2025-05-11 00:40:44 UTC",
    "updated_date": "2025-07-12 00:53:10 UTC"
  },
  {
    "arxiv_id": "2505.06795v3",
    "title": "Decoding Futures Price Dynamics: A Regularized Sparse Autoencoder for Interpretable Multi-Horizon Forecasting and Factor Discovery",
    "authors": [
      "Abhijit Gupta"
    ],
    "abstract": "Commodity price volatility creates economic challenges, necessitating accurate multi-horizon forecasting. Predicting prices for commodities like copper and crude oil is complicated by diverse interacting factors (macroeconomic, supply/demand, geopolitical, etc.). Current models often lack transparency, limiting strategic use. This paper presents a Regularized Sparse Autoencoder (RSAE), a deep learning framework for simultaneous multi-horizon commodity price prediction and discovery of interpretable latent market drivers. The RSAE forecasts prices at multiple horizons (e.g., 1-day, 1-week, 1-month) using multivariate time series. Crucially, L1 regularization ($\\|\\mathbf{z}\\|_1$) on its latent vector $\\mathbf{z}$ enforces sparsity, promoting parsimonious explanations of market dynamics through learned factors representing underlying drivers (e.g., demand, supply shocks). Drawing from energy-based models and sparse coding, the RSAE optimizes predictive accuracy while learning sparse representations. Evaluated on historical Copper and Crude Oil data with numerous indicators, our findings indicate the RSAE offers competitive multi-horizon forecasting accuracy and data-driven insights into price dynamics via its interpretable latent space, a key advantage over traditional black-box approaches.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CE"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.06795v3",
    "published_date": "2025-05-11 00:21:53 UTC",
    "updated_date": "2025-05-14 17:49:51 UTC"
  }
]
[
  {
    "arxiv_id": "2405.00908v1",
    "title": "Transformer-Based Self-Supervised Learning for Histopathological Classification of Ischemic Stroke Clot Origin",
    "authors": [
      "K. Yeh",
      "M. S. Jabal",
      "V. Gupta",
      "D. F. Kallmes",
      "W. Brinjikji",
      "B. S. Erdal"
    ],
    "abstract": "Background and Purpose: Identifying the thromboembolism source in ischemic\nstroke is crucial for treatment and secondary prevention yet is often\nundetermined. This study describes a self-supervised deep learning approach in\ndigital pathology of emboli for classifying ischemic stroke clot origin from\nhistopathological images. Methods: The dataset included whole slide images\n(WSI) from the STRIP AI Kaggle challenge, consisting of retrieved clots from\nischemic stroke patients following mechanical thrombectomy. Transformer-based\ndeep learning models were developed using transfer learning and self-supervised\npretraining for classifying WSI. Customizations included an attention pooling\nlayer, weighted loss function, and threshold optimization. Various model\narchitectures were tested and compared, and model performances were primarily\nevaluated using weighted logarithmic loss. Results: The model achieved a\nlogloss score of 0.662 in cross-validation and 0.659 on the test set. Different\nmodel backbones were compared, with the swin_large_patch4_window12_384 showed\nhigher performance. Thresholding techniques for clot origin classification were\nemployed to balance false positives and negatives. Conclusion: The study\ndemonstrates the extent of efficacy of transformer-based deep learning models\nin identifying ischemic stroke clot origins from histopathological images and\nemphasizes the need for refined modeling techniques specifically adapted to\nthrombi WSI. Further research is needed to improve model performance,\ninterpretability, validate its effectiveness. Future enhancement could include\nintegrating larger patient cohorts, advanced preprocessing strategies, and\nexploring ensemble multimodal methods for enhanced diagnostic accuracy.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2405.00908v1",
    "published_date": "2024-05-01 23:40:12 UTC",
    "updated_date": "2024-05-01 23:40:12 UTC"
  },
  {
    "arxiv_id": "2405.00906v1",
    "title": "LOTUS: Improving Transformer Efficiency with Sparsity Pruning and Data Lottery Tickets",
    "authors": [
      "Ojasw Upadhyay"
    ],
    "abstract": "Vision transformers have revolutionized computer vision, but their\ncomputational demands present challenges for training and deployment. This\npaper introduces LOTUS (LOttery Transformers with Ultra Sparsity), a novel\nmethod that leverages data lottery ticket selection and sparsity pruning to\naccelerate vision transformer training while maintaining accuracy. Our approach\nfocuses on identifying and utilizing the most informative data subsets and\neliminating redundant model parameters to optimize the training process.\nThrough extensive experiments, we demonstrate the effectiveness of LOTUS in\nachieving rapid convergence and high accuracy with significantly reduced\ncomputational requirements. This work highlights the potential of combining\ndata selection and sparsity techniques for efficient vision transformer\ntraining, opening doors for further research and development in this area.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "3 pages, 5 figures",
    "pdf_url": "http://arxiv.org/pdf/2405.00906v1",
    "published_date": "2024-05-01 23:30:12 UTC",
    "updated_date": "2024-05-01 23:30:12 UTC"
  },
  {
    "arxiv_id": "2405.00902v1",
    "title": "MESA: Cooperative Meta-Exploration in Multi-Agent Learning through Exploiting State-Action Space Structure",
    "authors": [
      "Zhicheng Zhang",
      "Yancheng Liang",
      "Yi Wu",
      "Fei Fang"
    ],
    "abstract": "Multi-agent reinforcement learning (MARL) algorithms often struggle to find\nstrategies close to Pareto optimal Nash Equilibrium, owing largely to the lack\nof efficient exploration. The problem is exacerbated in sparse-reward settings,\ncaused by the larger variance exhibited in policy learning. This paper\nintroduces MESA, a novel meta-exploration method for cooperative multi-agent\nlearning. It learns to explore by first identifying the agents' high-rewarding\njoint state-action subspace from training tasks and then learning a set of\ndiverse exploration policies to \"cover\" the subspace. These trained exploration\npolicies can be integrated with any off-policy MARL algorithm for test-time\ntasks. We first showcase MESA's advantage in a multi-step matrix game.\nFurthermore, experiments show that with learned exploration policies, MESA\nachieves significantly better performance in sparse-reward tasks in several\nmulti-agent particle environments and multi-agent MuJoCo environments, and\nexhibits the ability to generalize to more challenging tasks at test time.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.MA"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted to AAMAS 2024. 15 pages",
    "pdf_url": "http://arxiv.org/pdf/2405.00902v1",
    "published_date": "2024-05-01 23:19:48 UTC",
    "updated_date": "2024-05-01 23:19:48 UTC"
  },
  {
    "arxiv_id": "2405.00899v2",
    "title": "Characterising the Creative Process in Humans and Large Language Models",
    "authors": [
      "Surabhi S. Nath",
      "Peter Dayan",
      "Claire Stevenson"
    ],
    "abstract": "Large language models appear quite creative, often performing on par with the\naverage human on creative tasks. However, research on LLM creativity has\nfocused solely on \\textit{products}, with little attention on the creative\n\\textit{process}. Process analyses of human creativity often require hand-coded\ncategories or exploit response times, which do not apply to LLMs. We provide an\nautomated method to characterise how humans and LLMs explore semantic spaces on\nthe Alternate Uses Task, and contrast with behaviour in a Verbal Fluency Task.\nWe use sentence embeddings to identify response categories and compute semantic\nsimilarities, which we use to generate jump profiles. Our results corroborate\nearlier work in humans reporting both persistent (deep search in few semantic\nspaces) and flexible (broad search across multiple semantic spaces) pathways to\ncreativity, where both pathways lead to similar creativity scores. LLMs were\nfound to be biased towards either persistent or flexible paths, that varied\nacross tasks. Though LLMs as a population match human profiles, their\nrelationship with creativity is different, where the more flexible models score\nhigher on creativity. Our dataset and scripts are available on\n\\href{https://github.com/surabhisnath/Creative_Process}{GitHub}.",
    "categories": [
      "cs.HC",
      "cs.AI",
      "cs.CL",
      "q-bio.NC"
    ],
    "primary_category": "cs.HC",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2405.00899v2",
    "published_date": "2024-05-01 23:06:46 UTC",
    "updated_date": "2024-06-05 19:15:43 UTC"
  },
  {
    "arxiv_id": "2405.00892v4",
    "title": "Wake Vision: A Tailored Dataset and Benchmark Suite for TinyML Computer Vision Applications",
    "authors": [
      "Colby Banbury",
      "Emil Njor",
      "Andrea Mattia Garavagno",
      "Matthew Stewart",
      "Pete Warden",
      "Manjunath Kudlur",
      "Nat Jeffries",
      "Xenofon Fafoutis",
      "Vijay Janapa Reddi"
    ],
    "abstract": "Tiny machine learning (TinyML) for low-power devices lacks robust datasets\nfor development. We present Wake Vision, a large-scale dataset for person\ndetection that contains over 6 million quality-filtered images. We provide two\nvariants: Wake Vision (Large) and Wake Vision (Quality), leveraging the large\nvariant for pretraining and knowledge distillation, while the higher-quality\nlabels drive final model performance. The manually labeled validation and test\nsets reduce error rates from 7.8% to 2.2% compared to previous standards. In\naddition, we introduce five detailed benchmark sets to evaluate model\nperformance in real-world scenarios, including varying lighting, camera\ndistances, and demographic characteristics. Training with Wake Vision improves\naccuracy by 1.93% over existing datasets, demonstrating the importance of\ndataset quality for low-capacity models and dataset size for high-capacity\nmodels. The dataset, benchmarks, code, and models are available under the CC-BY\n4.0 license, maintained by the Edge AI Foundation.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2405.00892v4",
    "published_date": "2024-05-01 22:33:45 UTC",
    "updated_date": "2024-12-09 17:35:55 UTC"
  },
  {
    "arxiv_id": "2405.00877v3",
    "title": "Markov flow policy -- deep MC",
    "authors": [
      "Nitsan Soffair",
      "Gilad Katz"
    ],
    "abstract": "Discounted algorithms often encounter evaluation errors due to their reliance\non short-term estimations, which can impede their efficacy in addressing\nsimple, short-term tasks and impose undesired temporal discounts (\\(\\gamma\\)).\nInterestingly, these algorithms are often tested without applying a discount, a\nphenomenon we refer as the \\textit{train-test bias}. In response to these\nchallenges, we propose the Markov Flow Policy, which utilizes a non-negative\nneural network flow to enable comprehensive forward-view predictions. Through\nintegration into the TD7 codebase and evaluation using the MuJoCo benchmark, we\nobserve significant performance improvements, positioning MFP as a\nstraightforward, practical, and easily implementable solution within the domain\nof average rewards algorithms.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "Paper has been not finished",
    "pdf_url": "http://arxiv.org/pdf/2405.00877v3",
    "published_date": "2024-05-01 21:42:38 UTC",
    "updated_date": "2024-08-30 10:02:22 UTC"
  },
  {
    "arxiv_id": "2405.00876v1",
    "title": "Beyond Human Vision: The Role of Large Vision Language Models in Microscope Image Analysis",
    "authors": [
      "Prateek Verma",
      "Minh-Hao Van",
      "Xintao Wu"
    ],
    "abstract": "Vision language models (VLMs) have recently emerged and gained the spotlight\nfor their ability to comprehend the dual modality of image and textual data.\nVLMs such as LLaVA, ChatGPT-4, and Gemini have recently shown impressive\nperformance on tasks such as natural image captioning, visual question\nanswering (VQA), and spatial reasoning. Additionally, a universal segmentation\nmodel by Meta AI, Segment Anything Model (SAM) shows unprecedented performance\nat isolating objects from unforeseen images. Since medical experts, biologists,\nand materials scientists routinely examine microscopy or medical images in\nconjunction with textual information in the form of captions, literature, or\nreports, and draw conclusions of great importance and merit, it is indubitably\nessential to test the performance of VLMs and foundation models such as SAM, on\nthese images. In this study, we charge ChatGPT, LLaVA, Gemini, and SAM with\nclassification, segmentation, counting, and VQA tasks on a variety of\nmicroscopy images. We observe that ChatGPT and Gemini are impressively able to\ncomprehend the visual features in microscopy images, while SAM is quite capable\nat isolating artefacts in a general sense. However, the performance is not\nclose to that of a domain expert - the models are readily encumbered by the\nintroduction of impurities, defects, artefact overlaps and diversity present in\nthe images.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2405.00876v1",
    "published_date": "2024-05-01 21:35:04 UTC",
    "updated_date": "2024-05-01 21:35:04 UTC"
  },
  {
    "arxiv_id": "2405.00874v1",
    "title": "Artificial intelligence for context-aware visual change detection in software test automation",
    "authors": [
      "Milad Moradi",
      "Ke Yan",
      "David Colwell",
      "Rhona Asgari"
    ],
    "abstract": "Automated software testing is integral to the software development process,\nstreamlining workflows and ensuring product reliability. Visual testing within\nthis context, especially concerning user interface (UI) and user experience\n(UX) validation, stands as one of crucial determinants of overall software\nquality. Nevertheless, conventional methods like pixel-wise comparison and\nregion-based visual change detection fall short in capturing contextual\nsimilarities, nuanced alterations, and understanding the spatial relationships\nbetween UI elements. In this paper, we introduce a novel graph-based method for\nvisual change detection in software test automation. Leveraging a machine\nlearning model, our method accurately identifies UI controls from software\nscreenshots and constructs a graph representing contextual and spatial\nrelationships between the controls. This information is then used to find\ncorrespondence between UI controls within screenshots of different versions of\na software. The resulting graph encapsulates the intricate layout of the UI and\nunderlying contextual relations, providing a holistic and context-aware model.\nThis model is finally used to detect and highlight visual regressions in the\nUI. Comprehensive experiments on different datasets showed that our change\ndetector can accurately detect visual software changes in various simple and\ncomplex test scenarios. Moreover, it outperformed pixel-wise comparison and\nregion-based baselines by a large margin in more complex testing scenarios.\nThis work not only contributes to the advancement of visual change detection\nbut also holds practical implications, offering a robust solution for\nreal-world software test automation challenges, enhancing reliability, and\nensuring the seamless evolution of software interfaces.",
    "categories": [
      "cs.SE",
      "cs.AI"
    ],
    "primary_category": "cs.SE",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2405.00874v1",
    "published_date": "2024-05-01 21:22:33 UTC",
    "updated_date": "2024-05-01 21:22:33 UTC"
  },
  {
    "arxiv_id": "2407.10246v3",
    "title": "CourseAssist: Pedagogically Appropriate AI Tutor for Computer Science Education",
    "authors": [
      "Ty Feng",
      "Sa Liu",
      "Dipak Ghosal"
    ],
    "abstract": "The growing enrollments in computer science courses and increase in class\nsizes necessitate scalable, automated tutoring solutions to adequately support\nstudent learning. While Large Language Models (LLMs) like GPT-4 have\ndemonstrated potential in assisting students through question-answering,\neducators express concerns over student overreliance, miscomprehension of\ngenerated code, and the risk of inaccurate answers. Rather than banning these\ntools outright, we advocate for a constructive approach that harnesses the\ncapabilities of AI while mitigating potential risks. This poster introduces\nCourseAssist, a novel LLM-based tutoring system tailored for computer science\neducation. Unlike generic LLM systems, CourseAssist uses retrieval-augmented\ngeneration, user intent classification, and question decomposition to align AI\nresponses with specific course materials and learning objectives, thereby\nensuring pedagogical appropriateness of LLMs in educational settings. We\nevaluated CourseAssist against a baseline of GPT-4 using a dataset of 50\nquestion-answer pairs from a programming languages course, focusing on the\ncriteria of usefulness, accuracy, and pedagogical appropriateness. Evaluation\nresults show that CourseAssist significantly outperforms the baseline,\ndemonstrating its potential to serve as an effective learning assistant. We\nhave also deployed CourseAssist in 6 computer science courses at a large public\nR1 research university reaching over 500 students. Interviews with 20 student\nusers show that CourseAssist improves computer science instruction by\nincreasing the accessibility of course-specific tutoring help and shortening\nthe feedback loop on their programming assignments. Future work will include\nextensive pilot testing at more universities and exploring better collaborative\nrelationships between students, educators, and AI that improve computer science\nlearning experiences.",
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.HC"
    ],
    "primary_category": "cs.CY",
    "comment": "Accepted to SIGCSE Virtual 2024",
    "pdf_url": "http://arxiv.org/pdf/2407.10246v3",
    "published_date": "2024-05-01 20:43:06 UTC",
    "updated_date": "2024-07-29 23:01:18 UTC"
  },
  {
    "arxiv_id": "2405.00843v1",
    "title": "Can a Hallucinating Model help in Reducing Human \"Hallucination\"?",
    "authors": [
      "Sowmya S Sundaram",
      "Balaji Alwar"
    ],
    "abstract": "The prevalence of unwarranted beliefs, spanning pseudoscience, logical\nfallacies, and conspiracy theories, presents substantial societal hurdles and\nthe risk of disseminating misinformation. Utilizing established psychometric\nassessments, this study explores the capabilities of large language models\n(LLMs) vis-a-vis the average human in detecting prevalent logical pitfalls. We\nundertake a philosophical inquiry, juxtaposing the rationality of humans\nagainst that of LLMs. Furthermore, we propose methodologies for harnessing LLMs\nto counter misconceptions, drawing upon psychological models of persuasion such\nas cognitive dissonance theory and elaboration likelihood theory. Through this\nendeavor, we highlight the potential of LLMs as personalized misinformation\ndebunking agents.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "Under review",
    "pdf_url": "http://arxiv.org/pdf/2405.00843v1",
    "published_date": "2024-05-01 20:10:44 UTC",
    "updated_date": "2024-05-01 20:10:44 UTC"
  },
  {
    "arxiv_id": "2405.00841v2",
    "title": "Sim-Grasp: Learning 6-DOF Grasp Policies for Cluttered Environments Using a Synthetic Benchmark",
    "authors": [
      "Juncheng Li",
      "David J. Cappelleri"
    ],
    "abstract": "In this paper, we present Sim-Grasp, a robust 6-DOF two-finger grasping\nsystem that integrates advanced language models for enhanced object\nmanipulation in cluttered environments. We introduce the Sim-Grasp-Dataset,\nwhich includes 1,550 objects across 500 scenarios with 7.9 million annotated\nlabels, and develop Sim-GraspNet to generate grasp poses from point clouds. The\nSim-Grasp-Polices achieve grasping success rates of 97.14% for single objects\nand 87.43% and 83.33% for mixed clutter scenarios of Levels 1-2 and Levels 3-4\nobjects, respectively. By incorporating language models for target\nidentification through text and box prompts, Sim-Grasp enables both\nobject-agnostic and target picking, pushing the boundaries of intelligent\nrobotic systems.",
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "primary_category": "cs.RO",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2405.00841v2",
    "published_date": "2024-05-01 20:08:51 UTC",
    "updated_date": "2024-07-16 22:12:11 UTC"
  },
  {
    "arxiv_id": "2405.00839v1",
    "title": "Communication-Efficient Training Workload Balancing for Decentralized Multi-Agent Learning",
    "authors": [
      "Seyed Mahmoud Sajjadi Mohammadabadi",
      "Lei Yang",
      "Feng Yan",
      "Junshan Zhang"
    ],
    "abstract": "Decentralized Multi-agent Learning (DML) enables collaborative model training\nwhile preserving data privacy. However, inherent heterogeneity in agents'\nresources (computation, communication, and task size) may lead to substantial\nvariations in training time. This heterogeneity creates a bottleneck,\nlengthening the overall training time due to straggler effects and potentially\nwasting spare resources of faster agents. To minimize training time in\nheterogeneous environments, we present a Communication-Efficient Training\nWorkload Balancing for Decentralized Multi-Agent Learning (ComDML), which\nbalances the workload among agents through a decentralized approach. Leveraging\nlocal-loss split training, ComDML enables parallel updates, where slower agents\noffload part of their workload to faster agents. To minimize the overall\ntraining time, ComDML optimizes the workload balancing by jointly considering\nthe communication and computation capacities of agents, which hinges upon\ninteger programming. A dynamic decentralized pairing scheduler is developed to\nefficiently pair agents and determine optimal offloading amounts. We prove that\nin ComDML, both slower and faster agents' models converge, for convex and\nnon-convex functions. Furthermore, extensive experimental results on popular\ndatasets (CIFAR-10, CIFAR-100, and CINIC-10) and their non-I.I.D. variants,\nwith large models such as ResNet-56 and ResNet-110, demonstrate that ComDML can\nsignificantly reduce the overall training time while maintaining model\naccuracy, compared to state-of-the-art methods. ComDML demonstrates robustness\nin heterogeneous environments, and privacy measures can be seamlessly\nintegrated for enhanced data protection.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.DC",
      "cs.MA",
      "cs.PF"
    ],
    "primary_category": "cs.LG",
    "comment": "This paper has been accepted for presentation at ICDCS (44th IEEE\n  International Conference on Distributed Computing Systems). Keywords:\n  decentralized multi-agent learning, federated learning, edge computing,\n  heterogeneous agents, workload balancing, and communication-efficient\n  training )",
    "pdf_url": "http://arxiv.org/pdf/2405.00839v1",
    "published_date": "2024-05-01 20:03:37 UTC",
    "updated_date": "2024-05-01 20:03:37 UTC"
  },
  {
    "arxiv_id": "2405.00823v2",
    "title": "WorkBench: a Benchmark Dataset for Agents in a Realistic Workplace Setting",
    "authors": [
      "Olly Styles",
      "Sam Miller",
      "Patricio Cerda-Mardini",
      "Tanaya Guha",
      "Victor Sanchez",
      "Bertie Vidgen"
    ],
    "abstract": "We introduce WorkBench: a benchmark dataset for evaluating agents' ability to\nexecute tasks in a workplace setting. WorkBench contains a sandbox environment\nwith five databases, 26 tools, and 690 tasks. These tasks represent common\nbusiness activities, such as sending emails and scheduling meetings. The tasks\nin WorkBench are challenging as they require planning, tool selection, and\noften multiple actions. If a task has been successfully executed, one (or more)\nof the database values may change. The correct outcome for each task is unique\nand unambiguous, which allows for robust, automated evaluation. We call this\nkey contribution outcome-centric evaluation. We evaluate five existing ReAct\nagents on WorkBench, finding they successfully complete as few as 3% of tasks\n(Llama2-70B), and just 43% for the best-performing (GPT-4). We further find\nthat agents' errors can result in the wrong action being taken, such as an\nemail being sent to the wrong person. WorkBench reveals weaknesses in agents'\nability to undertake common business activities, raising questions about their\nuse in high-stakes workplace settings. WorkBench is publicly available as a\nfree resource at https://github.com/olly-styles/WorkBench.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.MA"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2405.00823v2",
    "published_date": "2024-05-01 19:07:03 UTC",
    "updated_date": "2024-08-03 12:41:29 UTC"
  },
  {
    "arxiv_id": "2405.00791v1",
    "title": "Obtaining Favorable Layouts for Multiple Object Generation",
    "authors": [
      "Barak Battash",
      "Amit Rozner",
      "Lior Wolf",
      "Ofir Lindenbaum"
    ],
    "abstract": "Large-scale text-to-image models that can generate high-quality and diverse\nimages based on textual prompts have shown remarkable success. These models aim\nultimately to create complex scenes, and addressing the challenge of\nmulti-subject generation is a critical step towards this goal. However, the\nexisting state-of-the-art diffusion models face difficulty when generating\nimages that involve multiple subjects. When presented with a prompt containing\nmore than one subject, these models may omit some subjects or merge them\ntogether. To address this challenge, we propose a novel approach based on a\nguiding principle. We allow the diffusion model to initially propose a layout,\nand then we rearrange the layout grid. This is achieved by enforcing\ncross-attention maps (XAMs) to adhere to proposed masks and by migrating pixels\nfrom latent maps to new locations determined by us. We introduce new loss terms\naimed at reducing XAM entropy for clearer spatial definition of subjects,\nreduce the overlap between XAMs, and ensure that XAMs align with their\nrespective masks. We contrast our approach with several alternative methods and\nshow that it more faithfully captures the desired concepts across a variety of\ntext prompts.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "I.2, I.4"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2405.00791v1",
    "published_date": "2024-05-01 18:07:48 UTC",
    "updated_date": "2024-05-01 18:07:48 UTC"
  },
  {
    "arxiv_id": "2405.00790v2",
    "title": "SCAR: Scheduling Multi-Model AI Workloads on Heterogeneous Multi-Chiplet Module Accelerators",
    "authors": [
      "Mohanad Odema",
      "Luke Chen",
      "Hyoukjun Kwon",
      "Mohammad Abdullah Al Faruque"
    ],
    "abstract": "Emerging multi-model workloads with heavy models like recent large language\nmodels significantly increased the compute and memory demands on hardware. To\naddress such increasing demands, designing a scalable hardware architecture\nbecame a key problem. Among recent solutions, the 2.5D silicon interposer\nmulti-chip module (MCM)-based AI accelerator has been actively explored as a\npromising scalable solution due to their significant benefits in the low\nengineering cost and composability. However, previous MCM accelerators are\nbased on homogeneous architectures with fixed dataflow, which encounter major\nchallenges from highly heterogeneous multi-model workloads due to their limited\nworkload adaptivity. Therefore, in this work, we explore the opportunity in the\nheterogeneous dataflow MCM AI accelerators. We identify the scheduling of\nmulti-model workload on heterogeneous dataflow MCM AI accelerator is an\nimportant and challenging problem due to its significance and scale, which\nreaches O(10^56) even for a two-model workload on 6x6 chiplets. We develop a\nset of heuristics to navigate the huge scheduling space and codify them into a\nscheduler, SCAR, with advanced techniques such as inter-chiplet pipelining. Our\nevaluation on ten multi-model workload scenarios for datacenter multitenancy\nand AR/VR use-cases has shown the efficacy of our approach, achieving on\naverage 27.6% and 29.6% less energy-delay product (EDP) for the respective\napplications settings compared to homogeneous baselines.",
    "categories": [
      "cs.AR",
      "cs.AI",
      "cs.DC",
      "cs.LG",
      "cs.PF"
    ],
    "primary_category": "cs.AR",
    "comment": "MICRO'24",
    "pdf_url": "http://arxiv.org/pdf/2405.00790v2",
    "published_date": "2024-05-01 18:02:25 UTC",
    "updated_date": "2024-09-14 18:32:20 UTC"
  },
  {
    "arxiv_id": "2405.00675v5",
    "title": "Self-Play Preference Optimization for Language Model Alignment",
    "authors": [
      "Yue Wu",
      "Zhiqing Sun",
      "Huizhuo Yuan",
      "Kaixuan Ji",
      "Yiming Yang",
      "Quanquan Gu"
    ],
    "abstract": "Standard reinforcement learning from human feedback (RLHF) approaches relying\non parametric models like the Bradley-Terry model fall short in capturing the\nintransitivity and irrationality in human preferences. Recent advancements\nsuggest that directly working with preference probabilities can yield a more\naccurate reflection of human preferences, enabling more flexible and accurate\nlanguage model alignment. In this paper, we propose a self-play-based method\nfor language model alignment, which treats the problem as a constant-sum\ntwo-player game aimed at identifying the Nash equilibrium policy. Our approach,\ndubbed Self-Play Preference Optimization (SPPO), utilizes iterative policy\nupdates to provably approximate the Nash equilibrium. Additionally, we propose\na new SPPO objective which is both strongly motivated by theory and is simple\nand effective in practice. In our experiments, using only 60k prompts (without\nresponses) from the UltraFeedback dataset and without any prompt augmentation,\nby leveraging a pre-trained preference model PairRM with only 0.4B parameters,\nSPPO can obtain a model from fine-tuning Mistral-7B-Instruct-v0.2 that achieves\nthe state-of-the-art length-controlled win-rate of 28.53% against GPT-4-Turbo\non AlpacaEval 2.0. It also outperforms the (iterative) DPO and IPO on MT-Bench,\nArena-Hard, and the Open LLM Leaderboard. Starting from a stronger base model\nLlama-3-8B-Instruct, we are able to achieve a length-controlled win rate of\n38.77%. Notably, the strong performance of SPPO is achieved without additional\nexternal supervision (e.g., responses, preferences, etc.) from GPT-4 or other\nstronger language models. Codes are available at\nhttps://github.com/uclaml/SPPO.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "27 pages, 4 figures, 5 tables",
    "pdf_url": "http://arxiv.org/pdf/2405.00675v5",
    "published_date": "2024-05-01 17:59:20 UTC",
    "updated_date": "2024-10-04 18:48:25 UTC"
  },
  {
    "arxiv_id": "2405.00664v1",
    "title": "Is Bigger Edit Batch Size Always Better? -- An Empirical Study on Model Editing with Llama-3",
    "authors": [
      "Junsang Yoon",
      "Akshat Gupta",
      "Gopala Anumanchipalli"
    ],
    "abstract": "This study presents a targeted model editing analysis focused on the latest\nlarge language model, Llama-3. We explore the efficacy of popular model editing\ntechniques - ROME, MEMIT, and EMMET, which are designed for precise layer\ninterventions. We identify the most effective layers for targeted edits through\nan evaluation that encompasses up to 4096 edits across three distinct\nstrategies: sequential editing, batch editing, and a hybrid approach we call as\nsequential-batch editing. Our findings indicate that increasing edit\nbatch-sizes may degrade model performance more significantly than using smaller\nedit batches sequentially for equal number of edits. With this, we argue that\nsequential model editing is an important component for scaling model editing\nmethods and future research should focus on methods that combine both batched\nand sequential editing. This observation suggests a potential limitation in\ncurrent model editing methods which push towards bigger edit batch sizes, and\nwe hope it paves way for future investigations into optimizing batch sizes and\nmodel editing performance.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2405.00664v1",
    "published_date": "2024-05-01 17:50:37 UTC",
    "updated_date": "2024-05-01 17:50:37 UTC"
  },
  {
    "arxiv_id": "2405.00657v2",
    "title": "RST-LoRA: A Discourse-Aware Low-Rank Adaptation for Long Document Abstractive Summarization",
    "authors": [
      "Dongqi Liu",
      "Vera Demberg"
    ],
    "abstract": "For long document summarization, discourse structure is important to discern\nthe key content of the text and the differences in importance level between\nsentences. Unfortunately, the integration of rhetorical structure theory (RST)\ninto parameter-efficient fine-tuning strategies for long document summarization\nremains unexplored. Therefore, this paper introduces RST-LoRA and proposes four\nRST-aware variants to explicitly incorporate RST into the LoRA model. Our\nempirical evaluation demonstrates that incorporating the type and uncertainty\nof rhetorical relations can complementarily enhance the performance of LoRA in\nsummarization tasks. Furthermore, the best-performing variant we introduced\noutperforms the vanilla LoRA and full-parameter fine-tuning models, as\nconfirmed by multiple automatic and human evaluations, and even surpasses\nprevious state-of-the-art methods.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "NAACL 2024 Main & Long Conference Paper (Oral Presentation)",
    "pdf_url": "http://arxiv.org/pdf/2405.00657v2",
    "published_date": "2024-05-01 17:37:50 UTC",
    "updated_date": "2024-12-10 09:16:53 UTC"
  },
  {
    "arxiv_id": "2405.00644v1",
    "title": "ConstrainedZero: Chance-Constrained POMDP Planning using Learned Probabilistic Failure Surrogates and Adaptive Safety Constraints",
    "authors": [
      "Robert J. Moss",
      "Arec Jamgochian",
      "Johannes Fischer",
      "Anthony Corso",
      "Mykel J. Kochenderfer"
    ],
    "abstract": "To plan safely in uncertain environments, agents must balance utility with\nsafety constraints. Safe planning problems can be modeled as a\nchance-constrained partially observable Markov decision process (CC-POMDP) and\nsolutions often use expensive rollouts or heuristics to estimate the optimal\nvalue and action-selection policy. This work introduces the ConstrainedZero\npolicy iteration algorithm that solves CC-POMDPs in belief space by learning\nneural network approximations of the optimal value and policy with an\nadditional network head that estimates the failure probability given a belief.\nThis failure probability guides safe action selection during online Monte Carlo\ntree search (MCTS). To avoid overemphasizing search based on the failure\nestimates, we introduce $\\Delta$-MCTS, which uses adaptive conformal inference\nto update the failure threshold during planning. The approach is tested on a\nsafety-critical POMDP benchmark, an aircraft collision avoidance system, and\nthe sustainability problem of safe CO$_2$ storage. Results show that by\nseparating safety constraints from the objective we can achieve a target level\nof safety without optimizing the balance between rewards and costs.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "In Proceedings of the 2024 International Joint Conference on\n  Artificial Intelligence (IJCAI)",
    "pdf_url": "http://arxiv.org/pdf/2405.00644v1",
    "published_date": "2024-05-01 17:17:22 UTC",
    "updated_date": "2024-05-01 17:17:22 UTC"
  },
  {
    "arxiv_id": "2405.00632v1",
    "title": "When Quantization Affects Confidence of Large Language Models?",
    "authors": [
      "Irina Proskurina",
      "Luc Brun",
      "Guillaume Metzler",
      "Julien Velcin"
    ],
    "abstract": "Recent studies introduced effective compression techniques for Large Language\nModels (LLMs) via post-training quantization or low-bit weight representation.\nAlthough quantized weights offer storage efficiency and allow for faster\ninference, existing works have indicated that quantization might compromise\nperformance and exacerbate biases in LLMs. This study investigates the\nconfidence and calibration of quantized models, considering factors such as\nlanguage model type and scale as contributors to quantization loss. Firstly, we\nreveal that quantization with GPTQ to 4-bit results in a decrease in confidence\nregarding true labels, with varying impacts observed among different language\nmodels. Secondly, we observe fluctuations in the impact on confidence across\ndifferent scales. Finally, we propose an explanation for quantization loss\nbased on confidence levels, indicating that quantization disproportionately\naffects samples where the full model exhibited low confidence levels in the\nfirst place.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted to NAACL 2024 Findings",
    "pdf_url": "http://arxiv.org/pdf/2405.00632v1",
    "published_date": "2024-05-01 16:58:28 UTC",
    "updated_date": "2024-05-01 16:58:28 UTC"
  },
  {
    "arxiv_id": "2405.00629v2",
    "title": "HUGO -- Highlighting Unseen Grid Options: Combining Deep Reinforcement Learning with a Heuristic Target Topology Approach",
    "authors": [
      "Malte Lehna",
      "Clara Holzhüter",
      "Sven Tomforde",
      "Christoph Scholz"
    ],
    "abstract": "With the growth of Renewable Energy (RE) generation, the operation of power\ngrids has become increasingly complex. One solution could be automated grid\noperation, where Deep Reinforcement Learning (DRL) has repeatedly shown\nsignificant potential in Learning to Run a Power Network (L2RPN) challenges.\nHowever, only individual actions at the substation level have been subjected to\ntopology optimization by most existing DRL algorithms. In contrast, we propose\na more holistic approach by proposing specific Target Topologies (TTs) as\nactions. These topologies are selected based on their robustness. As part of\nthis paper, we present a search algorithm to find the TTs and upgrade our\npreviously developed DRL agent CurriculumAgent (CAgent) to a novel topology\nagent. We compare the upgrade to the previous CAgent and can increase their\nL2RPN score significantly by 10%. Further, we achieve a 25% better median\nsurvival time with our TTs included. Later analysis shows that almost all TTs\nare close to the base topology, explaining their robustness",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "12 pages + 2 pages references, 9 Figures, submission planed in\n  Sustainable Energy, Grids and Networks",
    "pdf_url": "http://arxiv.org/pdf/2405.00629v2",
    "published_date": "2024-05-01 16:54:12 UTC",
    "updated_date": "2024-05-23 08:42:25 UTC"
  },
  {
    "arxiv_id": "2405.00623v2",
    "title": "\"I'm Not Sure, But...\": Examining the Impact of Large Language Models' Uncertainty Expression on User Reliance and Trust",
    "authors": [
      "Sunnie S. Y. Kim",
      "Q. Vera Liao",
      "Mihaela Vorvoreanu",
      "Stephanie Ballard",
      "Jennifer Wortman Vaughan"
    ],
    "abstract": "Widely deployed large language models (LLMs) can produce convincing yet\nincorrect outputs, potentially misleading users who may rely on them as if they\nwere correct. To reduce such overreliance, there have been calls for LLMs to\ncommunicate their uncertainty to end users. However, there has been little\nempirical work examining how users perceive and act upon LLMs' expressions of\nuncertainty. We explore this question through a large-scale, pre-registered,\nhuman-subject experiment (N=404) in which participants answer medical questions\nwith or without access to responses from a fictional LLM-infused search engine.\nUsing both behavioral and self-reported measures, we examine how different\nnatural language expressions of uncertainty impact participants' reliance,\ntrust, and overall task performance. We find that first-person expressions\n(e.g., \"I'm not sure, but...\") decrease participants' confidence in the system\nand tendency to agree with the system's answers, while increasing participants'\naccuracy. An exploratory analysis suggests that this increase can be attributed\nto reduced (but not fully eliminated) overreliance on incorrect answers. While\nwe observe similar effects for uncertainty expressed from a general perspective\n(e.g., \"It's not clear, but...\"), these effects are weaker and not\nstatistically significant. Our findings suggest that using natural language\nexpressions of uncertainty may be an effective approach for reducing\noverreliance on LLMs, but that the precise language used matters. This\nhighlights the importance of user testing before deploying LLMs at scale.",
    "categories": [
      "cs.HC",
      "cs.AI"
    ],
    "primary_category": "cs.HC",
    "comment": "Accepted to FAccT 2024. This version includes the appendix",
    "pdf_url": "http://arxiv.org/pdf/2405.00623v2",
    "published_date": "2024-05-01 16:43:55 UTC",
    "updated_date": "2024-05-15 09:04:54 UTC"
  },
  {
    "arxiv_id": "2405.00622v1",
    "title": "Causal Evaluation of Language Models",
    "authors": [
      "Sirui Chen",
      "Bo Peng",
      "Meiqi Chen",
      "Ruiqi Wang",
      "Mengying Xu",
      "Xingyu Zeng",
      "Rui Zhao",
      "Shengjie Zhao",
      "Yu Qiao",
      "Chaochao Lu"
    ],
    "abstract": "Causal reasoning is viewed as crucial for achieving human-level machine\nintelligence. Recent advances in language models have expanded the horizons of\nartificial intelligence across various domains, sparking inquiries into their\npotential for causal reasoning. In this work, we introduce Causal evaluation of\nLanguage Models (CaLM), which, to the best of our knowledge, is the first\ncomprehensive benchmark for evaluating the causal reasoning capabilities of\nlanguage models. First, we propose the CaLM framework, which establishes a\nfoundational taxonomy consisting of four modules: causal target (i.e., what to\nevaluate), adaptation (i.e., how to obtain the results), metric (i.e., how to\nmeasure the results), and error (i.e., how to analyze the bad results). This\ntaxonomy defines a broad evaluation design space while systematically selecting\ncriteria and priorities. Second, we compose the CaLM dataset, comprising\n126,334 data samples, to provide curated sets of causal targets, adaptations,\nmetrics, and errors, offering extensive coverage for diverse research pursuits.\nThird, we conduct an extensive evaluation of 28 leading language models on a\ncore set of 92 causal targets, 9 adaptations, 7 metrics, and 12 error types.\nFourth, we perform detailed analyses of the evaluation results across various\ndimensions (e.g., adaptation, scale). Fifth, we present 50 high-level empirical\nfindings across 9 dimensions (e.g., model), providing valuable guidance for\nfuture language model development. Finally, we develop a multifaceted platform,\nincluding a website, leaderboards, datasets, and toolkits, to support scalable\nand adaptable assessments. We envision CaLM as an ever-evolving benchmark for\nthe community, systematically updated with new causal targets, adaptations,\nmodels, metrics, and error types to reflect ongoing research advancements.\nProject website is at https://opencausalab.github.io/CaLM.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "315 pages, 230 figures, 21 tables. Project website:\n  https://opencausalab.github.io/CaLM",
    "pdf_url": "http://arxiv.org/pdf/2405.00622v1",
    "published_date": "2024-05-01 16:43:21 UTC",
    "updated_date": "2024-05-01 16:43:21 UTC"
  },
  {
    "arxiv_id": "2405.00588v1",
    "title": "Are Models Biased on Text without Gender-related Language?",
    "authors": [
      "Catarina G Belém",
      "Preethi Seshadri",
      "Yasaman Razeghi",
      "Sameer Singh"
    ],
    "abstract": "Gender bias research has been pivotal in revealing undesirable behaviors in\nlarge language models, exposing serious gender stereotypes associated with\noccupations, and emotions. A key observation in prior work is that models\nreinforce stereotypes as a consequence of the gendered correlations that are\npresent in the training data. In this paper, we focus on bias where the effect\nfrom training data is unclear, and instead address the question: Do language\nmodels still exhibit gender bias in non-stereotypical settings? To do so, we\nintroduce UnStereoEval (USE), a novel framework tailored for investigating\ngender bias in stereotype-free scenarios. USE defines a sentence-level score\nbased on pretraining data statistics to determine if the sentence contain\nminimal word-gender associations. To systematically benchmark the fairness of\npopular language models in stereotype-free scenarios, we utilize USE to\nautomatically generate benchmarks without any gender-related language. By\nleveraging USE's sentence-level score, we also repurpose prior gender bias\nbenchmarks (Winobias and Winogender) for non-stereotypical evaluation.\nSurprisingly, we find low fairness across all 28 tested models. Concretely,\nmodels demonstrate fair behavior in only 9%-41% of stereotype-free sentences,\nsuggesting that bias does not solely stem from the presence of gender-related\nwords. These results raise important questions about where underlying model\nbiases come from and highlight the need for more systematic and comprehensive\nbias evaluation. We release the full dataset and code at\nhttps://ucinlp.github.io/unstereo-eval.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CV",
      "cs.CY",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "In International Conference on Learning Representations 2024",
    "pdf_url": "http://arxiv.org/pdf/2405.00588v1",
    "published_date": "2024-05-01 15:51:15 UTC",
    "updated_date": "2024-05-01 15:51:15 UTC"
  },
  {
    "arxiv_id": "2405.00578v1",
    "title": "The Real, the Better: Aligning Large Language Models with Online Human Behaviors",
    "authors": [
      "Guanying Jiang",
      "Lingyong Yan",
      "Haibo Shi",
      "Dawei Yin"
    ],
    "abstract": "Large language model alignment is widely used and studied to avoid LLM\nproducing unhelpful and harmful responses. However, the lengthy training\nprocess and predefined preference bias hinder adaptation to online diverse\nhuman preferences. To this end, this paper proposes an alignment framework,\ncalled Reinforcement Learning with Human Behavior (RLHB), to align LLMs by\ndirectly leveraging real online human behaviors. By taking the generative\nadversarial framework, the generator is trained to respond following expected\nhuman behavior; while the discriminator tries to verify whether the triplets of\nquery, response, and human behavior come from real online environments.\nBehavior modeling in natural-language form and the multi-model joint training\nmechanism enable an active and sustainable online alignment. Experimental\nresults confirm the effectiveness of our proposed methods by both human and\nautomatic evaluations.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "11 pages, 6 figures",
    "pdf_url": "http://arxiv.org/pdf/2405.00578v1",
    "published_date": "2024-05-01 15:30:41 UTC",
    "updated_date": "2024-05-01 15:30:41 UTC"
  },
  {
    "arxiv_id": "2405.00760v1",
    "title": "Deep Reward Supervisions for Tuning Text-to-Image Diffusion Models",
    "authors": [
      "Xiaoshi Wu",
      "Yiming Hao",
      "Manyuan Zhang",
      "Keqiang Sun",
      "Zhaoyang Huang",
      "Guanglu Song",
      "Yu Liu",
      "Hongsheng Li"
    ],
    "abstract": "Optimizing a text-to-image diffusion model with a given reward function is an\nimportant but underexplored research area. In this study, we propose Deep\nReward Tuning (DRTune), an algorithm that directly supervises the final output\nimage of a text-to-image diffusion model and back-propagates through the\niterative sampling process to the input noise. We find that training earlier\nsteps in the sampling process is crucial for low-level rewards, and deep\nsupervision can be achieved efficiently and effectively by stopping the\ngradient of the denoising network input. DRTune is extensively evaluated on\nvarious reward models. It consistently outperforms other algorithms,\nparticularly for low-level control signals, where all shallow supervision\nmethods fail. Additionally, we fine-tune Stable Diffusion XL 1.0 (SDXL 1.0)\nmodel via DRTune to optimize Human Preference Score v2.1, resulting in the\nFavorable Diffusion XL 1.0 (FDXL 1.0) model. FDXL 1.0 significantly enhances\nimage quality compared to SDXL 1.0 and reaches comparable quality compared with\nMidjourney v5.2.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "N/A",
    "pdf_url": "http://arxiv.org/pdf/2405.00760v1",
    "published_date": "2024-05-01 15:26:14 UTC",
    "updated_date": "2024-05-01 15:26:14 UTC"
  },
  {
    "arxiv_id": "2405.00571v1",
    "title": "Spherical Linear Interpolation and Text-Anchoring for Zero-shot Composed Image Retrieval",
    "authors": [
      "Young Kyun Jang",
      "Dat Huynh",
      "Ashish Shah",
      "Wen-Kai Chen",
      "Ser-Nam Lim"
    ],
    "abstract": "Composed Image Retrieval (CIR) is a complex task that retrieves images using\na query, which is configured with an image and a caption that describes desired\nmodifications to that image. Supervised CIR approaches have shown strong\nperformance, but their reliance on expensive manually-annotated datasets\nrestricts their scalability and broader applicability. To address these issues,\nprevious studies have proposed pseudo-word token-based Zero-Shot CIR (ZS-CIR)\nmethods, which utilize a projection module to map images to word tokens.\nHowever, we conjecture that this approach has a downside: the projection module\ndistorts the original image representation and confines the resulting composed\nembeddings to the text-side. In order to resolve this, we introduce a novel\nZS-CIR method that uses Spherical Linear Interpolation (Slerp) to directly\nmerge image and text representations by identifying an intermediate embedding\nof both. Furthermore, we introduce Text-Anchored-Tuning (TAT), a method that\nfine-tunes the image encoder while keeping the text encoder fixed. TAT closes\nthe modality gap between images and text, making the Slerp process much more\neffective. Notably, the TAT method is not only efficient in terms of the scale\nof the training dataset and training time, but it also serves as an excellent\ninitial checkpoint for training supervised CIR models, thereby highlighting its\nwider potential. The integration of the Slerp-based ZS-CIR with a TAT-tuned\nmodel enables our approach to deliver state-of-the-art retrieval performance\nacross CIR benchmarks.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2405.00571v1",
    "published_date": "2024-05-01 15:19:54 UTC",
    "updated_date": "2024-05-01 15:19:54 UTC"
  },
  {
    "arxiv_id": "2405.00570v1",
    "title": "WEST GCN-LSTM: Weighted Stacked Spatio-Temporal Graph Neural Networks for Regional Traffic Forecasting",
    "authors": [
      "Theodoros Theodoropoulos",
      "Angelos-Christos Maroudis",
      "Antonios Makris",
      "Konstantinos Tserpes"
    ],
    "abstract": "Regional traffic forecasting is a critical challenge in urban mobility, with\napplications to various fields such as the Internet of Everything. In recent\nyears, spatio-temporal graph neural networks have achieved state-of-the-art\nresults in the context of numerous traffic forecasting challenges. This work\naims at expanding upon the conventional spatio-temporal graph neural network\narchitectures in a manner that may facilitate the inclusion of information\nregarding the examined regions, as well as the populations that traverse them,\nin order to establish a more efficient prediction model. The end-product of\nthis scientific endeavour is a novel spatio-temporal graph neural network\narchitecture that is referred to as WEST (WEighted STacked) GCN-LSTM.\nFurthermore, the inclusion of the aforementioned information is conducted via\nthe use of two novel dedicated algorithms that are referred to as the Shared\nBorders Policy and the Adjustable Hops Policy. Through information fusion and\ndistillation, the proposed solution manages to significantly outperform its\ncompetitors in the frame of an experimental evaluation that consists of 19\nforecasting models, across several datasets. Finally, an additional ablation\nstudy determined that each of the components of the proposed solution\ncontributes towards enhancing its overall performance.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2405.00570v1",
    "published_date": "2024-05-01 15:19:19 UTC",
    "updated_date": "2024-05-01 15:19:19 UTC"
  },
  {
    "arxiv_id": "2405.00568v2",
    "title": "Powering In-Database Dynamic Model Slicing for Structured Data Analytics",
    "authors": [
      "Lingze Zeng",
      "Naili Xing",
      "Shaofeng Cai",
      "Gang Chen",
      "Beng Chin Ooi",
      "Jian Pei",
      "Yuncheng Wu"
    ],
    "abstract": "Relational database management systems (RDBMS) are widely used for the\nstorage of structured data. To derive insights beyond statistical aggregation,\nwe typically have to extract specific subdatasets from the database using\nconventional database operations, and then apply deep neural networks (DNN)\ntraining and inference on these subdatasets in a separate analytics system. The\nprocess can be prohibitively expensive, especially when there are various\nsubdatasets extracted for different analytical purposes. This calls for\nefficient in-database support of advanced analytical methods.\n  In this paper, we introduce LEADS, a novel SQL-aware dynamic model slicing\ntechnique to customize models for specified SQL queries. LEADS improves the\npredictive modeling of structured data via the mixture of experts (MoE) and\nmaintains efficiency by a SQL-aware gating network. At the core of LEADS is the\nconstruction of a general model with multiple expert sub-models trained over\nthe database. The MoE scales up the modeling capacity, enhances effectiveness,\nand preserves efficiency by activating necessary experts via the SQL-aware\ngating network during inference. To support in-database analytics, we build an\ninference extension that integrates LEADS onto PostgreSQL. Our extensive\nexperiments on real-world datasets demonstrate that LEADS consistently\noutperforms the baseline models, and the in-database inference extension\ndelivers a considerable reduction in inference latency compared to traditional\nsolutions.",
    "categories": [
      "cs.DB",
      "cs.AI"
    ],
    "primary_category": "cs.DB",
    "comment": "VLDB 2025",
    "pdf_url": "http://arxiv.org/pdf/2405.00568v2",
    "published_date": "2024-05-01 15:18:12 UTC",
    "updated_date": "2024-11-03 08:58:12 UTC"
  },
  {
    "arxiv_id": "2405.00557v4",
    "title": "Mixture of insighTful Experts (MoTE): The Synergy of Thought Chains and Expert Mixtures in Self-Alignment",
    "authors": [
      "Zhili Liu",
      "Yunhao Gou",
      "Kai Chen",
      "Lanqing Hong",
      "Jiahui Gao",
      "Fei Mi",
      "Yu Zhang",
      "Zhenguo Li",
      "Xin Jiang",
      "Qun Liu",
      "James T. Kwok"
    ],
    "abstract": "As the capabilities of large language models (LLMs) continue to expand,\naligning these models with human values remains a significant challenge. Recent\nstudies show that reasoning abilities contribute significantly to model safety,\nwhile integrating Mixture-of-Experts (MoE) architectures can further enhance\nalignment. In this work, we propose Mixture of insighTful Experts (MoTE), a\nnovel framework that synergistically combines reasoning chains and expert\nmixtures to improve self-alignments. From a data perspective, MoTE employs a\nstructured reasoning chain comprising four key stages: Question Analysis,\nAnswer Guidance, Safe Answer, and Safety Checking. This approach enhances\nsafety through multi-step reasoning and proves effective even for smaller and\nless powerful LLMs (e.g., 7B models). From an architectural perspective, MoTE\nadopts a multi-LoRA framework with step-level routing, where each expert is\ndedicated to a specific reasoning step. This design eliminates the need for\nbalance losses, ensures stable training, and supports adaptive inference\nlengths. Experimental results demonstrate that MoTE significantly improves\nmodel safety, jailbreak resistance, and over-refusal capabilities, achieving\nperformance comparable to OpenAI's state-of-the-art o1 model.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2405.00557v4",
    "published_date": "2024-05-01 15:06:05 UTC",
    "updated_date": "2025-02-19 02:26:09 UTC"
  },
  {
    "arxiv_id": "2405.00543v1",
    "title": "New Benchmark Dataset and Fine-Grained Cross-Modal Fusion Framework for Vietnamese Multimodal Aspect-Category Sentiment Analysis",
    "authors": [
      "Quy Hoang Nguyen",
      "Minh-Van Truong Nguyen",
      "Kiet Van Nguyen"
    ],
    "abstract": "The emergence of multimodal data on social media platforms presents new\nopportunities to better understand user sentiments toward a given aspect.\nHowever, existing multimodal datasets for Aspect-Category Sentiment Analysis\n(ACSA) often focus on textual annotations, neglecting fine-grained information\nin images. Consequently, these datasets fail to fully exploit the richness\ninherent in multimodal. To address this, we introduce a new Vietnamese\nmultimodal dataset, named ViMACSA, which consists of 4,876 text-image pairs\nwith 14,618 fine-grained annotations for both text and image in the hotel\ndomain. Additionally, we propose a Fine-Grained Cross-Modal Fusion Framework\n(FCMF) that effectively learns both intra- and inter-modality interactions and\nthen fuses these information to produce a unified multimodal representation.\nExperimental results show that our framework outperforms SOTA models on the\nViMACSA dataset, achieving the highest F1 score of 79.73%. We also explore\ncharacteristics and challenges in Vietnamese multimodal sentiment analysis,\nincluding misspellings, abbreviations, and the complexities of the Vietnamese\nlanguage. This work contributes both a benchmark dataset and a new framework\nthat leverages fine-grained multimodal information to improve multimodal\naspect-category sentiment analysis. Our dataset is available for research\npurposes:\nhttps://github.com/hoangquy18/Multimodal-Aspect-Category-Sentiment-Analysis.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2405.00543v1",
    "published_date": "2024-05-01 14:29:03 UTC",
    "updated_date": "2024-05-01 14:29:03 UTC"
  },
  {
    "arxiv_id": "2405.00532v3",
    "title": "ULLER: A Unified Language for Learning and Reasoning",
    "authors": [
      "Emile van Krieken",
      "Samy Badreddine",
      "Robin Manhaeve",
      "Eleonora Giunchiglia"
    ],
    "abstract": "The field of neuro-symbolic artificial intelligence (NeSy), which combines\nlearning and reasoning, has recently experienced significant growth. There now\nare a wide variety of NeSy frameworks, each with its own specific language for\nexpressing background knowledge and how to relate it to neural networks. This\nheterogeneity hinders accessibility for newcomers and makes comparing different\nNeSy frameworks challenging. We propose a unified language for NeSy, which we\ncall ULLER, a Unified Language for LEarning and Reasoning. ULLER encompasses a\nwide variety of settings, while ensuring that knowledge described in it can be\nused in existing NeSy systems. ULLER has a neuro-symbolic first-order syntax\nfor which we provide example semantics including classical, fuzzy, and\nprobabilistic logics. We believe ULLER is a first step towards making NeSy\nresearch more accessible and comparable, paving the way for libraries that\nstreamline training and evaluation across a multitude of semantics, knowledge\nbases, and NeSy systems.",
    "categories": [
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "Pre-review version. Final version accepted at NeSy 2024",
    "pdf_url": "http://arxiv.org/pdf/2405.00532v3",
    "published_date": "2024-05-01 14:05:52 UTC",
    "updated_date": "2024-07-03 06:34:31 UTC"
  },
  {
    "arxiv_id": "2405.00523v1",
    "title": "CookingSense: A Culinary Knowledgebase with Multidisciplinary Assertions",
    "authors": [
      "Donghee Choi",
      "Mogan Gim",
      "Donghyeon Park",
      "Mujeen Sung",
      "Hyunjae Kim",
      "Jaewoo Kang",
      "Jihun Choi"
    ],
    "abstract": "This paper introduces CookingSense, a descriptive collection of knowledge\nassertions in the culinary domain extracted from various sources, including web\ndata, scientific papers, and recipes, from which knowledge covering a broad\nrange of aspects is acquired. CookingSense is constructed through a series of\ndictionary-based filtering and language model-based semantic filtering\ntechniques, which results in a rich knowledgebase of multidisciplinary\nfood-related assertions. Additionally, we present FoodBench, a novel benchmark\nto evaluate culinary decision support systems. From evaluations with FoodBench,\nwe empirically prove that CookingSense improves the performance of retrieval\naugmented language models. We also validate the quality and variety of\nassertions in CookingSense through qualitative analysis.",
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.AI",
    "comment": "LREC-COLING 2024 Accepted",
    "pdf_url": "http://arxiv.org/pdf/2405.00523v1",
    "published_date": "2024-05-01 13:58:09 UTC",
    "updated_date": "2024-05-01 13:58:09 UTC"
  },
  {
    "arxiv_id": "2405.00516v1",
    "title": "Navigating WebAI: Training Agents to Complete Web Tasks with Large Language Models and Reinforcement Learning",
    "authors": [
      "Lucas-Andreï Thil",
      "Mirela Popa",
      "Gerasimos Spanakis"
    ],
    "abstract": "Recent advancements in language models have demonstrated remarkable\nimprovements in various natural language processing (NLP) tasks such as web\nnavigation. Supervised learning (SL) approaches have achieved impressive\nperformance while utilizing significantly less training data compared to\nprevious methods. However, these SL-based models fall short when compared to\nreinforcement learning (RL) approaches, which have shown superior results. In\nthis paper, we propose a novel approach that combines SL and RL techniques over\nthe MiniWoB benchmark to leverage the strengths of both methods. We also\naddress a critical limitation in previous models' understanding of HTML\ncontent, revealing a tendency to memorize target elements rather than\ncomprehend the underlying structure. To rectify this, we propose methods to\nenhance true understanding and present a new baseline of results. Our\nexperiments demonstrate that our approach outperforms previous SL methods on\ncertain tasks using less data and narrows the performance gap with RL models,\nachieving 43.58\\% average accuracy in SL and 36.69\\% when combined with a\nmultimodal RL approach. This study sets a new direction for future web\nnavigation and offers insights into the limitations and potential of language\nmodeling for computer tasks.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL",
      "68T07",
      "I.2.7; I.2.8; I.2.1"
    ],
    "primary_category": "cs.LG",
    "comment": "ACM 2024, Avila Spain. 9 pages",
    "pdf_url": "http://arxiv.org/pdf/2405.00516v1",
    "published_date": "2024-05-01 13:51:45 UTC",
    "updated_date": "2024-05-01 13:51:45 UTC"
  },
  {
    "arxiv_id": "2405.00494v1",
    "title": "GOLD: Geometry Problem Solver with Natural Language Description",
    "authors": [
      "Jiaxin Zhang",
      "Yashar Moshfeghi"
    ],
    "abstract": "Addressing the challenge of automated geometry math problem-solving in\nartificial intelligence (AI) involves understanding multi-modal information and\nmathematics. Current methods struggle with accurately interpreting geometry\ndiagrams, which hinders effective problem-solving. To tackle this issue, we\npresent the Geometry problem sOlver with natural Language Description (GOLD)\nmodel. GOLD enhances the extraction of geometric relations by separately\nprocessing symbols and geometric primitives within the diagram. Subsequently,\nit converts the extracted relations into natural language descriptions,\nefficiently utilizing large language models to solve geometry math problems.\nExperiments show that the GOLD model outperforms the Geoformer model, the\nprevious best method on the UniGeo dataset, by achieving accuracy improvements\nof 12.7% and 42.1% in calculation and proving subsets. Additionally, it\nsurpasses the former best model on the PGPS9K and Geometry3K datasets, PGPSNet,\nby obtaining accuracy enhancements of 1.8% and 3.2%, respectively.",
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.AI",
    "comment": "Accepted in NAACL 2024 Findings",
    "pdf_url": "http://arxiv.org/pdf/2405.00494v1",
    "published_date": "2024-05-01 13:00:51 UTC",
    "updated_date": "2024-05-01 13:00:51 UTC"
  },
  {
    "arxiv_id": "2405.00492v1",
    "title": "Is Temperature the Creativity Parameter of Large Language Models?",
    "authors": [
      "Max Peeperkorn",
      "Tom Kouwenhoven",
      "Dan Brown",
      "Anna Jordanous"
    ],
    "abstract": "Large language models (LLMs) are applied to all sorts of creative tasks, and\ntheir outputs vary from beautiful, to peculiar, to pastiche, into plain\nplagiarism. The temperature parameter of an LLM regulates the amount of\nrandomness, leading to more diverse outputs; therefore, it is often claimed to\nbe the creativity parameter. Here, we investigate this claim using a narrative\ngeneration task with a predetermined fixed context, model and prompt.\nSpecifically, we present an empirical analysis of the LLM output for different\ntemperature values using four necessary conditions for creativity in narrative\ngeneration: novelty, typicality, cohesion, and coherence. We find that\ntemperature is weakly correlated with novelty, and unsurprisingly, moderately\ncorrelated with incoherence, but there is no relationship with either cohesion\nor typicality. However, the influence of temperature on creativity is far more\nnuanced and weak than suggested by the \"creativity parameter\" claim; overall\nresults suggest that the LLM generates slightly more novel outputs as\ntemperatures get higher. Finally, we discuss ideas to allow more controlled LLM\ncreativity, rather than relying on chance via changing the temperature\nparameter.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "To be published in the Proceedings of the 15th International\n  Conference on Computational Creativity (ICCC'24), 8 pages, 2 figures, 2\n  tables",
    "pdf_url": "http://arxiv.org/pdf/2405.00492v1",
    "published_date": "2024-05-01 12:59:37 UTC",
    "updated_date": "2024-05-01 12:59:37 UTC"
  },
  {
    "arxiv_id": "2405.00468v2",
    "title": "Feature-Aware Noise Contrastive Learning for Unsupervised Red Panda Re-Identification",
    "authors": [
      "Jincheng Zhang",
      "Qijun Zhao",
      "Tie Liu"
    ],
    "abstract": "To facilitate the re-identification (re-ID) of individual animals, existing\nmethods primarily focus on maximizing feature similarity within the same\nindividual and enhancing distinctiveness between different individuals.\nHowever, most of them still rely on supervised learning and require substantial\nlabeled data, which is challenging to obtain. To avoid this issue, we propose\nFeature-Aware Noise Contrastive Learning (FANCL) method to explore an\nunsupervised learning solution, which is then validated on the task of red\npanda re-ID. FANCL designs a Feature-Aware Noise Addition module to produce\nnoised images that conceal critical features, and employs two contrastive\nlearning modules to calculate the losses. Firstly, a feature consistency module\nis designed to bridge the gap between the original and noised features.\nSecondly, the neural networks are trained through a cluster contrastive\nlearning module. Through these more challenging learning tasks, FANCL can\nadaptively extract deeper representations of red pandas. The experimental\nresults on a set of red panda images collected in both indoor and outdoor\nenvironments prove that FANCL outperforms several related state-of-the-art\nunsupervised methods, achieving high performance comparable to supervised\nlearning methods.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2405.00468v2",
    "published_date": "2024-05-01 12:08:38 UTC",
    "updated_date": "2024-07-18 06:00:44 UTC"
  },
  {
    "arxiv_id": "2405.00461v1",
    "title": "Enhancing Surgical Robots with Embodied Intelligence for Autonomous Ultrasound Scanning",
    "authors": [
      "Huan Xu",
      "Jinlin Wu",
      "Guanglin Cao",
      "Zhen Lei",
      "Zhen Chen",
      "Hongbin Liu"
    ],
    "abstract": "Ultrasound robots are increasingly used in medical diagnostics and early\ndisease screening. However, current ultrasound robots lack the intelligence to\nunderstand human intentions and instructions, hindering autonomous ultrasound\nscanning. To solve this problem, we propose a novel Ultrasound Embodied\nIntelligence system that equips ultrasound robots with the large language model\n(LLM) and domain knowledge, thereby improving the efficiency of ultrasound\nrobots. Specifically, we first design an ultrasound operation knowledge\ndatabase to add expertise in ultrasound scanning to the LLM, enabling the LLM\nto perform precise motion planning. Furthermore, we devise a dynamic ultrasound\nscanning strategy based on a \\textit{think-observe-execute} prompt engineering,\nallowing LLMs to dynamically adjust motion planning strategies during the\nscanning procedures. Extensive experiments demonstrate that our system\nsignificantly improves ultrasound scan efficiency and quality from verbal\ncommands. This advancement in autonomous medical scanning technology\ncontributes to non-invasive diagnostics and streamlined medical workflows.",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.CL",
      "cs.HC"
    ],
    "primary_category": "cs.RO",
    "comment": "3 pages, 1 figure, 2 tables",
    "pdf_url": "http://arxiv.org/pdf/2405.00461v1",
    "published_date": "2024-05-01 11:39:38 UTC",
    "updated_date": "2024-05-01 11:39:38 UTC"
  },
  {
    "arxiv_id": "2405.00456v1",
    "title": "Counterfactual Explanations for Deep Learning-Based Traffic Forecasting",
    "authors": [
      "Rushan Wang",
      "Yanan Xin",
      "Yatao Zhang",
      "Fernando Perez-Cruz",
      "Martin Raubal"
    ],
    "abstract": "Deep learning models are widely used in traffic forecasting and have achieved\nstate-of-the-art prediction accuracy. However, the black-box nature of those\nmodels makes the results difficult to interpret by users. This study aims to\nleverage an Explainable AI approach, counterfactual explanations, to enhance\nthe explainability and usability of deep learning-based traffic forecasting\nmodels. Specifically, the goal is to elucidate relationships between various\ninput contextual features and their corresponding predictions. We present a\ncomprehensive framework that generates counterfactual explanations for traffic\nforecasting and provides usable insights through the proposed scenario-driven\ncounterfactual explanations. The study first implements a deep learning model\nto predict traffic speed based on historical traffic data and contextual\nvariables. Counterfactual explanations are then used to illuminate how\nalterations in these input variables affect predicted outcomes, thereby\nenhancing the transparency of the deep learning model. We investigated the\nimpact of contextual features on traffic speed prediction under varying spatial\nand temporal conditions. The scenario-driven counterfactual explanations\nintegrate two types of user-defined constraints, directional and weighting\nconstraints, to tailor the search for counterfactual explanations to specific\nuse cases. These tailored explanations benefit machine learning practitioners\nwho aim to understand the model's learning mechanisms and domain experts who\nseek insights for real-world applications. The results showcase the\neffectiveness of counterfactual explanations in revealing traffic patterns\nlearned by deep learning models, showing its potential for interpreting\nblack-box deep learning models used for spatiotemporal predictions in general.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "24 pages",
    "pdf_url": "http://arxiv.org/pdf/2405.00456v1",
    "published_date": "2024-05-01 11:26:31 UTC",
    "updated_date": "2024-05-01 11:26:31 UTC"
  },
  {
    "arxiv_id": "2405.00453v1",
    "title": "Fuzzy Intelligent System for Student Software Project Evaluation",
    "authors": [
      "Anna Ogorodova",
      "Pakizar Shamoi",
      "Aron Karatayev"
    ],
    "abstract": "Developing software projects allows students to put knowledge into practice\nand gain teamwork skills. However, assessing student performance in\nproject-oriented courses poses significant challenges, particularly as the size\nof classes increases. The current paper introduces a fuzzy intelligent system\ndesigned to evaluate academic software projects using object-oriented\nprogramming and design course as an example. To establish evaluation criteria,\nwe first conducted a survey of student project teams (n=31) and faculty (n=3)\nto identify key parameters and their applicable ranges. The selected criteria -\nclean code, use of inheritance, and functionality - were selected as essential\nfor assessing the quality of academic software projects. These criteria were\nthen represented as fuzzy variables with corresponding fuzzy sets.\nCollaborating with three experts, including one professor and two course\ninstructors, we defined a set of fuzzy rules for a fuzzy inference system. This\nsystem processes the input criteria to produce a quantifiable measure of\nproject success. The system demonstrated promising results in automating the\nevaluation of projects. Our approach standardizes project evaluations and helps\nto reduce the subjective bias in manual grading.",
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.SE"
    ],
    "primary_category": "cs.CY",
    "comment": "Submitted to IJMECS for consideration",
    "pdf_url": "http://arxiv.org/pdf/2405.00453v1",
    "published_date": "2024-05-01 11:12:22 UTC",
    "updated_date": "2024-05-01 11:12:22 UTC"
  },
  {
    "arxiv_id": "2405.00451v2",
    "title": "Monte Carlo Tree Search Boosts Reasoning via Iterative Preference Learning",
    "authors": [
      "Yuxi Xie",
      "Anirudh Goyal",
      "Wenyue Zheng",
      "Min-Yen Kan",
      "Timothy P. Lillicrap",
      "Kenji Kawaguchi",
      "Michael Shieh"
    ],
    "abstract": "We introduce an approach aimed at enhancing the reasoning capabilities of\nLarge Language Models (LLMs) through an iterative preference learning process\ninspired by the successful strategy employed by AlphaZero. Our work leverages\nMonte Carlo Tree Search (MCTS) to iteratively collect preference data,\nutilizing its look-ahead ability to break down instance-level rewards into more\ngranular step-level signals. To enhance consistency in intermediate steps, we\ncombine outcome validation and stepwise self-evaluation, continually updating\nthe quality assessment of newly generated data. The proposed algorithm employs\nDirect Preference Optimization (DPO) to update the LLM policy using this newly\ngenerated step-level preference data. Theoretical analysis reveals the\nimportance of using on-policy sampled data for successful self-improving.\nExtensive evaluations on various arithmetic and commonsense reasoning tasks\ndemonstrate remarkable performance improvements over existing models. For\ninstance, our approach outperforms the Mistral-7B Supervised Fine-Tuning (SFT)\nbaseline on GSM8K, MATH, and ARC-C, with substantial increases in accuracy to\n$81.8\\%$ (+$5.9\\%$), $34.7\\%$ (+$5.8\\%$), and $76.4\\%$ (+$15.8\\%$),\nrespectively. Additionally, our research delves into the training and inference\ncompute tradeoff, providing insights into how our method effectively maximizes\nperformance gains. Our code is publicly available at\nhttps://github.com/YuxiXie/MCTS-DPO.",
    "categories": [
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "10 pages, 4 figures, 4 tables (24 pages, 9 figures, 9 tables\n  including references and appendices)",
    "pdf_url": "http://arxiv.org/pdf/2405.00451v2",
    "published_date": "2024-05-01 11:10:24 UTC",
    "updated_date": "2024-06-17 22:11:49 UTC"
  },
  {
    "arxiv_id": "2405.00449v1",
    "title": "RAG-based Explainable Prediction of Road Users Behaviors for Automated Driving using Knowledge Graphs and Large Language Models",
    "authors": [
      "Mohamed Manzour Hussien",
      "Angie Nataly Melo",
      "Augusto Luis Ballardini",
      "Carlota Salinas Maldonado",
      "Rubén Izquierdo",
      "Miguel Ángel Sotelo"
    ],
    "abstract": "Prediction of road users' behaviors in the context of autonomous driving has\ngained considerable attention by the scientific community in the last years.\nMost works focus on predicting behaviors based on kinematic information alone,\na simplification of the reality since road users are humans, and as such they\nare highly influenced by their surrounding context. In addition, a large\nplethora of research works rely on powerful Deep Learning techniques, which\nexhibit high performance metrics in prediction tasks but may lack the ability\nto fully understand and exploit the contextual semantic information contained\nin the road scene, not to mention their inability to provide explainable\npredictions that can be understood by humans. In this work, we propose an\nexplainable road users' behavior prediction system that integrates the\nreasoning abilities of Knowledge Graphs (KG) and the expressiveness\ncapabilities of Large Language Models (LLM) by using Retrieval Augmented\nGeneration (RAG) techniques. For that purpose, Knowledge Graph Embeddings (KGE)\nand Bayesian inference are combined to allow the deployment of a fully\ninductive reasoning system that enables the issuing of predictions that rely on\nlegacy information contained in the graph as well as on current evidence\ngathered in real time by onboard sensors. Two use cases have been implemented\nfollowing the proposed approach: 1) Prediction of pedestrians' crossing\nactions; 2) Prediction of lane change maneuvers. In both cases, the performance\nattained surpasses the current state of the art in terms of anticipation and\nF1-score, showing a promising avenue for future research in this field.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL",
      "cs.IR",
      "cs.NE"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2405.00449v1",
    "published_date": "2024-05-01 11:06:31 UTC",
    "updated_date": "2024-05-01 11:06:31 UTC"
  },
  {
    "arxiv_id": "2405.00442v1",
    "title": "Geometric Insights into Focal Loss: Reducing Curvature for Enhanced Model Calibration",
    "authors": [
      "Masanari Kimura",
      "Hiroki Naganuma"
    ],
    "abstract": "The key factor in implementing machine learning algorithms in decision-making\nsituations is not only the accuracy of the model but also its confidence level.\nThe confidence level of a model in a classification problem is often given by\nthe output vector of a softmax function for convenience. However, these values\nare known to deviate significantly from the actual expected model confidence.\nThis problem is called model calibration and has been studied extensively. One\nof the simplest techniques to tackle this task is focal loss, a generalization\nof cross-entropy by introducing one positive parameter. Although many related\nstudies exist because of the simplicity of the idea and its formalization, the\ntheoretical analysis of its behavior is still insufficient. In this study, our\nobjective is to understand the behavior of focal loss by reinterpreting this\nfunction geometrically. Our analysis suggests that focal loss reduces the\ncurvature of the loss surface in training the model. This indicates that\ncurvature may be one of the essential factors in achieving model calibration.\nWe design numerical experiments to support this conjecture to reveal the\nbehavior of focal loss and the relationship between calibration performance and\ncurvature.",
    "categories": [
      "stat.ML",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "stat.ML",
    "comment": "This paper is under consideration at Pattern Recognition Letters",
    "pdf_url": "http://arxiv.org/pdf/2405.00442v1",
    "published_date": "2024-05-01 10:53:54 UTC",
    "updated_date": "2024-05-01 10:53:54 UTC"
  },
  {
    "arxiv_id": "2405.00433v1",
    "title": "Weight Sparsity Complements Activity Sparsity in Neuromorphic Language Models",
    "authors": [
      "Rishav Mukherji",
      "Mark Schöne",
      "Khaleelulla Khan Nazeer",
      "Christian Mayr",
      "David Kappel",
      "Anand Subramoney"
    ],
    "abstract": "Activity and parameter sparsity are two standard methods of making neural\nnetworks computationally more efficient. Event-based architectures such as\nspiking neural networks (SNNs) naturally exhibit activity sparsity, and many\nmethods exist to sparsify their connectivity by pruning weights. While the\neffect of weight pruning on feed-forward SNNs has been previously studied for\ncomputer vision tasks, the effects of pruning for complex sequence tasks like\nlanguage modeling are less well studied since SNNs have traditionally struggled\nto achieve meaningful performance on these tasks. Using a recently published\nSNN-like architecture that works well on small-scale language modeling, we\nstudy the effects of weight pruning when combined with activity sparsity.\nSpecifically, we study the trade-off between the multiplicative efficiency\ngains the combination affords and its effect on task performance for language\nmodeling. To dissect the effects of the two sparsities, we conduct a\ncomparative analysis between densely activated models and sparsely activated\nevent-based models across varying degrees of connectivity sparsity. We\ndemonstrate that sparse activity and sparse connectivity complement each other\nwithout a proportional drop in task performance for an event-based neural\nnetwork trained on the Penn Treebank and WikiText-2 language modeling datasets.\nOur results suggest sparsely connected event-based neural networks are\npromising candidates for effective and efficient sequence modeling.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.NE"
    ],
    "primary_category": "cs.LG",
    "comment": "arXiv admin note: text overlap with arXiv:2311.07625",
    "pdf_url": "http://arxiv.org/pdf/2405.00433v1",
    "published_date": "2024-05-01 10:33:36 UTC",
    "updated_date": "2024-05-01 10:33:36 UTC"
  },
  {
    "arxiv_id": "2405.00420v1",
    "title": "Self-supervised Pre-training of Text Recognizers",
    "authors": [
      "Martin Kišš",
      "Michal Hradiš"
    ],
    "abstract": "In this paper, we investigate self-supervised pre-training methods for\ndocument text recognition. Nowadays, large unlabeled datasets can be collected\nfor many research tasks, including text recognition, but it is costly to\nannotate them. Therefore, methods utilizing unlabeled data are researched. We\nstudy self-supervised pre-training methods based on masked label prediction\nusing three different approaches -- Feature Quantization, VQ-VAE, and\nPost-Quantized AE. We also investigate joint-embedding approaches with VICReg\nand NT-Xent objectives, for which we propose an image shifting technique to\nprevent model collapse where it relies solely on positional encoding while\ncompletely ignoring the input image. We perform our experiments on historical\nhandwritten (Bentham) and historical printed datasets mainly to investigate the\nbenefits of the self-supervised pre-training techniques with different amounts\nof annotated target domain data. We use transfer learning as strong baselines.\nThe evaluation shows that the self-supervised pre-training on data from the\ntarget domain is very effective, but it struggles to outperform transfer\nlearning from closely related domains. This paper is one of the first\nresearches exploring self-supervised pre-training in document text recognition,\nand we believe that it will become a cornerstone for future research in this\narea. We made our implementation of the investigated methods publicly available\nat https://github.com/DCGM/pero-pretraining.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "18 pages, 6 figures, 4 tables, accepted to ICDAR24",
    "pdf_url": "http://arxiv.org/pdf/2405.00420v1",
    "published_date": "2024-05-01 09:58:57 UTC",
    "updated_date": "2024-05-01 09:58:57 UTC"
  },
  {
    "arxiv_id": "2405.00418v1",
    "title": "Detection of ransomware attacks using federated learning based on the CNN model",
    "authors": [
      "Hong-Nhung Nguyen",
      "Ha-Thanh Nguyen",
      "Damien Lescos"
    ],
    "abstract": "Computing is still under a significant threat from ransomware, which\nnecessitates prompt action to prevent it. Ransomware attacks can have a\nnegative impact on how smart grids, particularly digital substations. In\naddition to examining a ransomware detection method using artificial\nintelligence (AI), this paper offers a ransomware attack modeling technique\nthat targets the disrupted operation of a digital substation. The first, binary\ndata is transformed into image data and fed into the convolution neural network\nmodel using federated learning. The experimental findings demonstrate that the\nsuggested technique detects ransomware with a high accuracy rate.",
    "categories": [
      "cs.CR",
      "cs.AI"
    ],
    "primary_category": "cs.CR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2405.00418v1",
    "published_date": "2024-05-01 09:57:34 UTC",
    "updated_date": "2024-05-01 09:57:34 UTC"
  },
  {
    "arxiv_id": "2405.00395v1",
    "title": "Trust Driven On-Demand Scheme for Client Deployment in Federated Learning",
    "authors": [
      "Mario Chahoud",
      "Azzam Mourad",
      "Hadi Otrok",
      "Jamal Bentahar",
      "Mohsen Guizani"
    ],
    "abstract": "Containerization technology plays a crucial role in Federated Learning (FL)\nsetups, expanding the pool of potential clients and ensuring the availability\nof specific subsets for each learning iteration. However, doubts arise about\nthe trustworthiness of devices deployed as clients in FL scenarios, especially\nwhen container deployment processes are involved. Addressing these challenges\nis important, particularly in managing potentially malicious clients capable of\ndisrupting the learning process or compromising the entire model. In our\nresearch, we are motivated to integrate a trust element into the client\nselection and model deployment processes within our system architecture. This\nis a feature lacking in the initial client selection and deployment mechanism\nof the On-Demand architecture. We introduce a trust mechanism, named\n\"Trusted-On-Demand-FL\", which establishes a relationship of trust between the\nserver and the pool of eligible clients. Utilizing Docker in our deployment\nstrategy enables us to monitor and validate participant actions effectively,\nensuring strict adherence to agreed-upon protocols while strengthening defenses\nagainst unauthorized data access or tampering. Our simulations rely on a\ncontinuous user behavior dataset, deploying an optimization model powered by a\ngenetic algorithm to efficiently select clients for participation. By assigning\ntrust values to individual clients and dynamically adjusting these values,\ncombined with penalizing malicious clients through decreased trust scores, our\nproposed framework identifies and isolates harmful clients. This approach not\nonly reduces disruptions to regular rounds but also minimizes instances of\nround dismissal, Consequently enhancing both system stability and security.",
    "categories": [
      "cs.CR",
      "cs.AI"
    ],
    "primary_category": "cs.CR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2405.00395v1",
    "published_date": "2024-05-01 08:50:08 UTC",
    "updated_date": "2024-05-01 08:50:08 UTC"
  },
  {
    "arxiv_id": "2405.00392v1",
    "title": "Certified Adversarial Robustness of Machine Learning-based Malware Detectors via (De)Randomized Smoothing",
    "authors": [
      "Daniel Gibert",
      "Luca Demetrio",
      "Giulio Zizzo",
      "Quan Le",
      "Jordi Planes",
      "Battista Biggio"
    ],
    "abstract": "Deep learning-based malware detection systems are vulnerable to adversarial\nEXEmples - carefully-crafted malicious programs that evade detection with\nminimal perturbation. As such, the community is dedicating effort to develop\nmechanisms to defend against adversarial EXEmples. However, current randomized\nsmoothing-based defenses are still vulnerable to attacks that inject blocks of\nadversarial content. In this paper, we introduce a certifiable defense against\npatch attacks that guarantees, for a given executable and an adversarial patch\nsize, no adversarial EXEmple exist. Our method is inspired by (de)randomized\nsmoothing which provides deterministic robustness certificates. During\ntraining, a base classifier is trained using subsets of continguous bytes. At\ninference time, our defense splits the executable into non-overlapping chunks,\nclassifies each chunk independently, and computes the final prediction through\nmajority voting to minimize the influence of injected content. Furthermore, we\nintroduce a preprocessing step that fixes the size of the sections and headers\nto a multiple of the chunk size. As a consequence, the injected content is\nconfined to an integer number of chunks without tampering the other chunks\ncontaining the real bytes of the input examples, allowing us to extend our\ncertified robustness guarantees to content insertion attacks. We perform an\nextensive ablation study, by comparing our defense with randomized\nsmoothing-based defenses against a plethora of content manipulation attacks and\nneural network architectures. Results show that our method exhibits unmatched\nrobustness against strong content-insertion attacks, outperforming randomized\nsmoothing-based defenses in the literature.",
    "categories": [
      "cs.CR",
      "cs.AI"
    ],
    "primary_category": "cs.CR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2405.00392v1",
    "published_date": "2024-05-01 08:45:57 UTC",
    "updated_date": "2024-05-01 08:45:57 UTC"
  },
  {
    "arxiv_id": "2407.10237v1",
    "title": "Towards Green AI: Current status and future research",
    "authors": [
      "Christian Clemm",
      "Lutz Stobbe",
      "Kishan Wimalawarne",
      "Jan Druschke"
    ],
    "abstract": "The immense technological progress in artificial intelligence research and\napplications is increasingly drawing attention to the environmental\nsustainability of such systems, a field that has been termed Green AI. With\nthis contribution we aim to broaden the discourse on Green AI by investigating\nthe current status of approaches to both environmental assessment and ecodesign\nof AI systems. We propose a life-cycle-based system thinking approach that\naccounts for the four key elements of these software-hardware-systems: model,\ndata, server, and cloud. We conduct an exemplary estimation of the carbon\nfootprint of relevant compute hardware and highlight the need to further\ninvestigate methods for Green AI and ways to facilitate wide-spread adoption of\nits principles. We envision that AI could be leveraged to mitigate its own\nenvironmental challenges, which we denote as AI4greenAI.",
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.AR"
    ],
    "primary_category": "cs.CY",
    "comment": "11 pages, 4 figures, accepted for oral presentation at Electronics\n  Goes Green 2024 in June 2024, accepted for publication on IEEE Xplore via the\n  conference",
    "pdf_url": "http://arxiv.org/pdf/2407.10237v1",
    "published_date": "2024-05-01 08:10:01 UTC",
    "updated_date": "2024-05-01 08:10:01 UTC"
  },
  {
    "arxiv_id": "2405.00367v1",
    "title": "Distance Sampling-based Paraphraser Leveraging ChatGPT for Text Data Manipulation",
    "authors": [
      "Yoori Oh",
      "Yoseob Han",
      "Kyogu Lee"
    ],
    "abstract": "There has been growing interest in audio-language retrieval research, where\nthe objective is to establish the correlation between audio and text\nmodalities. However, most audio-text paired datasets often lack rich expression\nof the text data compared to the audio samples. One of the significant\nchallenges facing audio-text datasets is the presence of similar or identical\ncaptions despite different audio samples. Therefore, under many-to-one mapping\nconditions, audio-text datasets lead to poor performance of retrieval tasks. In\nthis paper, we propose a novel approach to tackle the data imbalance problem in\naudio-language retrieval task. To overcome the limitation, we introduce a\nmethod that employs a distance sampling-based paraphraser leveraging ChatGPT,\nutilizing distance function to generate a controllable distribution of\nmanipulated text data. For a set of sentences with the same context, the\ndistance is used to calculate a degree of manipulation for any two sentences,\nand ChatGPT's few-shot prompting is performed using a text cluster with a\nsimilar distance defined by the Jaccard similarity. Therefore, ChatGPT, when\napplied to few-shot prompting with text clusters, can adjust the diversity of\nthe manipulated text based on the distance. The proposed approach is shown to\nsignificantly enhance performance in audio-text retrieval, outperforming\nconventional text augmentation techniques.",
    "categories": [
      "cs.IR",
      "cs.AI",
      "cs.SD",
      "eess.AS"
    ],
    "primary_category": "cs.IR",
    "comment": "Accepted at SIGIR 2024 short paper track",
    "pdf_url": "http://arxiv.org/pdf/2405.00367v1",
    "published_date": "2024-05-01 07:44:28 UTC",
    "updated_date": "2024-05-01 07:44:28 UTC"
  },
  {
    "arxiv_id": "2405.00358v1",
    "title": "Arbitrary Time Information Modeling via Polynomial Approximation for Temporal Knowledge Graph Embedding",
    "authors": [
      "Zhiyu Fang",
      "Jingyan Qin",
      "Xiaobin Zhu",
      "Chun Yang",
      "Xu-Cheng Yin"
    ],
    "abstract": "Distinguished from traditional knowledge graphs (KGs), temporal knowledge\ngraphs (TKGs) must explore and reason over temporally evolving facts\nadequately. However, existing TKG approaches still face two main challenges,\ni.e., the limited capability to model arbitrary timestamps continuously and the\nlack of rich inference patterns under temporal constraints. In this paper, we\npropose an innovative TKGE method (PTBox) via polynomial decomposition-based\ntemporal representation and box embedding-based entity representation to tackle\nthe above-mentioned problems. Specifically, we decompose time information by\npolynomials and then enhance the model's capability to represent arbitrary\ntimestamps flexibly by incorporating the learnable temporal basis tensor. In\naddition, we model every entity as a hyperrectangle box and define each\nrelation as a transformation on the head and tail entity boxes. The entity\nboxes can capture complex geometric structures and learn robust\nrepresentations, improving the model's inductive capability for rich inference\npatterns. Theoretically, our PTBox can encode arbitrary time information or\neven unseen timestamps while capturing rich inference patterns and higher-arity\nrelations of the knowledge base. Extensive experiments on real-world datasets\ndemonstrate the effectiveness of our method.",
    "categories": [
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "Accepted by LREC-COLING 2024 (long paper, camera-ready version)",
    "pdf_url": "http://arxiv.org/pdf/2405.00358v1",
    "published_date": "2024-05-01 07:27:04 UTC",
    "updated_date": "2024-05-01 07:27:04 UTC"
  },
  {
    "arxiv_id": "2405.00753v1",
    "title": "HMAMP: Hypervolume-Driven Multi-Objective Antimicrobial Peptides Design",
    "authors": [
      "Li Wang",
      "Yiping Li",
      "Xiangzheng Fu",
      "Xiucai Ye",
      "Junfeng Shi",
      "Gary G. Yen",
      "Xiangxiang Zeng"
    ],
    "abstract": "Antimicrobial peptides (AMPs) have exhibited unprecedented potential as\nbiomaterials in combating multidrug-resistant bacteria. Despite the increasing\nadoption of artificial intelligence for novel AMP design, challenges pertaining\nto conflicting attributes such as activity, hemolysis, and toxicity have\nsignificantly impeded the progress of researchers. This paper introduces a\nparadigm shift by considering multiple attributes in AMP design.\n  Presented herein is a novel approach termed Hypervolume-driven\nMulti-objective Antimicrobial Peptide Design (HMAMP), which prioritizes the\nsimultaneous optimization of multiple attributes of AMPs. By synergizing\nreinforcement learning and a gradient descent algorithm rooted in the\nhypervolume maximization concept, HMAMP effectively expands exploration space\nand mitigates the issue of pattern collapse. This method generates a wide array\nof prospective AMP candidates that strike a balance among diverse attributes.\nFurthermore, we pinpoint knee points along the Pareto front of these candidate\nAMPs. Empirical results across five benchmark models substantiate that\nHMAMP-designed AMPs exhibit competitive performance and heightened diversity. A\ndetailed analysis of the helical structures and molecular dynamics simulations\nfor ten potential candidate AMPs validates the superiority of HMAMP in the\nrealm of multi-objective AMP design. The ability of HMAMP to systematically\ncraft AMPs considering multiple attributes marks a pioneering milestone,\nestablishing a universal computational framework for the multi-objective design\nof AMPs.",
    "categories": [
      "q-bio.QM",
      "cs.AI"
    ],
    "primary_category": "q-bio.QM",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2405.00753v1",
    "published_date": "2024-05-01 07:17:59 UTC",
    "updated_date": "2024-05-01 07:17:59 UTC"
  },
  {
    "arxiv_id": "2405.00352v1",
    "title": "Transformer-based Reasoning for Learning Evolutionary Chain of Events on Temporal Knowledge Graph",
    "authors": [
      "Zhiyu Fang",
      "Shuai-Long Lei",
      "Xiaobin Zhu",
      "Chun Yang",
      "Shi-Xue Zhang",
      "Xu-Cheng Yin",
      "Jingyan Qin"
    ],
    "abstract": "Temporal Knowledge Graph (TKG) reasoning often involves completing missing\nfactual elements along the timeline. Although existing methods can learn good\nembeddings for each factual element in quadruples by integrating temporal\ninformation, they often fail to infer the evolution of temporal facts. This is\nmainly because of (1) insufficiently exploring the internal structure and\nsemantic relationships within individual quadruples and (2) inadequately\nlearning a unified representation of the contextual and temporal correlations\namong different quadruples. To overcome these limitations, we propose a novel\nTransformer-based reasoning model (dubbed ECEformer) for TKG to learn the\nEvolutionary Chain of Events (ECE). Specifically, we unfold the neighborhood\nsubgraph of an entity node in chronological order, forming an evolutionary\nchain of events as the input for our model. Subsequently, we utilize a\nTransformer encoder to learn the embeddings of intra-quadruples for ECE. We\nthen craft a mixed-context reasoning module based on the multi-layer perceptron\n(MLP) to learn the unified representations of inter-quadruples for ECE while\naccomplishing temporal knowledge reasoning. In addition, to enhance the\ntimeliness of the events, we devise an additional time prediction task to\ncomplete effective temporal information within the learned unified\nrepresentation. Extensive experiments on six benchmark datasets verify the\nstate-of-the-art performance and the effectiveness of our method.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "Accepted by SIGIR 2024 (the Full paper track, camera ready version)",
    "pdf_url": "http://arxiv.org/pdf/2405.00352v1",
    "published_date": "2024-05-01 07:12:16 UTC",
    "updated_date": "2024-05-01 07:12:16 UTC"
  },
  {
    "arxiv_id": "2405.00351v1",
    "title": "Learning High-Quality Navigation and Zooming on Omnidirectional Images in Virtual Reality",
    "authors": [
      "Zidong Cao",
      "Zhan Wang",
      "Yexin Liu",
      "Yan-Pei Cao",
      "Ying Shan",
      "Wei Zeng",
      "Lin Wang"
    ],
    "abstract": "Viewing omnidirectional images (ODIs) in virtual reality (VR) represents a\nnovel form of media that provides immersive experiences for users to navigate\nand interact with digital content. Nonetheless, this sense of immersion can be\ngreatly compromised by a blur effect that masks details and hampers the user's\nability to engage with objects of interest. In this paper, we present a novel\nsystem, called OmniVR, designed to enhance visual clarity during VR navigation.\nOur system enables users to effortlessly locate and zoom in on the objects of\ninterest in VR. It captures user commands for navigation and zoom, converting\nthese inputs into parameters for the Mobius transformation matrix. Leveraging\nthese parameters, the ODI is refined using a learning-based algorithm. The\nresultant ODI is presented within the VR media, effectively reducing blur and\nincreasing user engagement. To verify the effectiveness of our system, we first\nevaluate our algorithm with state-of-the-art methods on public datasets, which\nachieves the best performance. Furthermore, we undertake a comprehensive user\nstudy to evaluate viewer experiences across diverse scenarios and to gather\ntheir qualitative feedback from multiple perspectives. The outcomes reveal that\nour system enhances user engagement by improving the viewers' recognition,\nreducing discomfort, and improving the overall immersive experience. Our system\nmakes the navigation and zoom more user-friendly.",
    "categories": [
      "cs.HC",
      "cs.AI",
      "cs.CV",
      "cs.MM"
    ],
    "primary_category": "cs.HC",
    "comment": "11 pages",
    "pdf_url": "http://arxiv.org/pdf/2405.00351v1",
    "published_date": "2024-05-01 07:08:24 UTC",
    "updated_date": "2024-05-01 07:08:24 UTC"
  },
  {
    "arxiv_id": "2405.00332v4",
    "title": "A Careful Examination of Large Language Model Performance on Grade School Arithmetic",
    "authors": [
      "Hugh Zhang",
      "Jeff Da",
      "Dean Lee",
      "Vaughn Robinson",
      "Catherine Wu",
      "Will Song",
      "Tiffany Zhao",
      "Pranav Raja",
      "Charlotte Zhuang",
      "Dylan Slack",
      "Qin Lyu",
      "Sean Hendryx",
      "Russell Kaplan",
      "Michele Lunati",
      "Summer Yue"
    ],
    "abstract": "Large language models (LLMs) have achieved impressive success on many\nbenchmarks for mathematical reasoning. However, there is growing concern that\nsome of this performance actually reflects dataset contamination, where data\nclosely resembling benchmark questions leaks into the training data, instead of\ntrue reasoning ability. To investigate this claim rigorously, we commission\nGrade School Math 1000 (GSM1k). GSM1k is designed to mirror the style and\ncomplexity of the established GSM8k benchmark, the gold standard for measuring\nelementary mathematical reasoning. We ensure that the two benchmarks are\ncomparable across important metrics such as human solve rates, number of steps\nin solution, answer magnitude, and more. When evaluating leading open- and\nclosed-source LLMs on GSM1k, we observe accuracy drops of up to 8%, with\nseveral families of models showing evidence of systematic overfitting across\nalmost all model sizes. Further analysis suggests a positive relationship\n(Spearman's r^2 = 0.36) between a model's probability of generating an example\nfrom GSM8k and its performance gap between GSM8k and GSM1k, suggesting that\nsome models may have partially memorized GSM8k. Nevertheless, many models,\nespecially those on the frontier, show minimal signs of overfitting, and all\nmodels broadly demonstrate generalization to novel math problems guaranteed to\nnot be in their training data.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "2024 NeurIPS Camera Ready (Datasets and Benchmarks Track)",
    "pdf_url": "http://arxiv.org/pdf/2405.00332v4",
    "published_date": "2024-05-01 05:52:05 UTC",
    "updated_date": "2024-11-22 22:27:49 UTC"
  },
  {
    "arxiv_id": "2405.00330v1",
    "title": "Integrating A.I. in Higher Education: Protocol for a Pilot Study with 'SAMCares: An Adaptive Learning Hub'",
    "authors": [
      "Syed Hasib Akhter Faruqui",
      "Nazia Tasnim",
      "Iftekhar Ibne Basith",
      "Suleiman Obeidat",
      "Faruk Yildiz"
    ],
    "abstract": "Learning never ends, and there is no age limit to grow yourself. However, the\neducational landscape may face challenges in effectively catering to students'\ninclusion and diverse learning needs. These students should have access to\nstate-of-the-art methods for lecture delivery, online resources, and technology\nneeds. However, with all the diverse learning sources, it becomes harder for\nstudents to comprehend a large amount of knowledge in a short period of time.\nTraditional assistive technologies and learning aids often lack the dynamic\nadaptability required for individualized education plans. Large Language Models\n(LLM) have been used in language translation, text summarization, and content\ngeneration applications. With rapid growth in AI over the past years,\nAI-powered chatbots and virtual assistants have been developed. This research\naims to bridge this gap by introducing an innovative study buddy we will be\ncalling the 'SAMCares'. The system leverages a Large Language Model (LLM) (in\nour case, LLaMa-2 70B as the base model) and Retriever-Augmented Generation\n(RAG) to offer real-time, context-aware, and adaptive educational support. The\ncontext of the model will be limited to the knowledge base of Sam Houston State\nUniversity (SHSU) course notes. The LLM component enables a chat-like\nenvironment to interact with it to meet the unique learning requirements of\neach student. For this, we will build a custom web-based GUI. At the same time,\nRAG enhances real-time information retrieval and text generation, in turn\nproviding more accurate and context-specific assistance. An option to upload\nadditional study materials in the web GUI is added in case additional knowledge\nsupport is required. The system's efficacy will be evaluated through controlled\ntrials and iterative feedback mechanisms.",
    "categories": [
      "cs.CY",
      "cs.AI"
    ],
    "primary_category": "cs.CY",
    "comment": "Accepted in ASEE Annual Conference 2024",
    "pdf_url": "http://arxiv.org/pdf/2405.00330v1",
    "published_date": "2024-05-01 05:39:07 UTC",
    "updated_date": "2024-05-01 05:39:07 UTC"
  },
  {
    "arxiv_id": "2405.00319v2",
    "title": "Data Augmentation Policy Search for Long-Term Forecasting",
    "authors": [
      "Liran Nochumsohn",
      "Omri Azencot"
    ],
    "abstract": "Data augmentation serves as a popular regularization technique to combat\noverfitting challenges in neural networks. While automatic augmentation has\ndemonstrated success in image classification tasks, its application to\ntime-series problems, particularly in long-term forecasting, has received\ncomparatively less attention. To address this gap, we introduce a time-series\nautomatic augmentation approach named TSAA, which is both efficient and easy to\nimplement. The solution involves tackling the associated bilevel optimization\nproblem through a two-step process: initially training a non-augmented model\nfor a limited number of epochs, followed by an iterative split procedure.\nDuring this iterative process, we alternate between identifying a robust\naugmentation policy through Bayesian optimization and refining the model while\ndiscarding suboptimal runs. Extensive evaluations on challenging univariate and\nmultivariate forecasting benchmark problems demonstrate that TSAA consistently\noutperforms several robust baselines, suggesting its potential integration into\nprediction pipelines. Code is available at this repository:\nhttps://github.com/azencot-group/TSAA.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "TMLR 2025",
    "pdf_url": "http://arxiv.org/pdf/2405.00319v2",
    "published_date": "2024-05-01 04:55:51 UTC",
    "updated_date": "2025-02-08 16:33:25 UTC"
  },
  {
    "arxiv_id": "2405.00751v1",
    "title": "F$^3$low: Frame-to-Frame Coarse-grained Molecular Dynamics with SE(3) Guided Flow Matching",
    "authors": [
      "Shaoning Li",
      "Yusong Wang",
      "Mingyu Li",
      "Jian Zhang",
      "Bin Shao",
      "Nanning Zheng",
      "Jian Tang"
    ],
    "abstract": "Molecular dynamics (MD) is a crucial technique for simulating biological\nsystems, enabling the exploration of their dynamic nature and fostering an\nunderstanding of their functions and properties. To address exploration\ninefficiency, emerging enhanced sampling approaches like coarse-graining (CG)\nand generative models have been employed. In this work, we propose a\n\\underline{Frame-to-Frame} generative model with guided\n\\underline{Flow}-matching (F$3$low) for enhanced sampling, which (a) extends\nthe domain of CG modeling to the SE(3) Riemannian manifold; (b) retreating CGMD\nsimulations as autoregressively sampling guided by the former frame via\nflow-matching models; (c) targets the protein backbone, offering improved\ninsights into secondary structure formation and intricate folding pathways.\nCompared to previous methods, F$3$low allows for broader exploration of\nconformational space. The ability to rapidly generate diverse conformations via\nforce-free generative paradigm on SE(3) paves the way toward efficient enhanced\nsampling methods.",
    "categories": [
      "q-bio.QM",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "q-bio.QM",
    "comment": "Accepted by ICLR 2024 GEM workshop",
    "pdf_url": "http://arxiv.org/pdf/2405.00751v1",
    "published_date": "2024-05-01 04:53:14 UTC",
    "updated_date": "2024-05-01 04:53:14 UTC"
  },
  {
    "arxiv_id": "2405.00314v1",
    "title": "Model Quantization and Hardware Acceleration for Vision Transformers: A Comprehensive Survey",
    "authors": [
      "Dayou Du",
      "Gu Gong",
      "Xiaowen Chu"
    ],
    "abstract": "Vision Transformers (ViTs) have recently garnered considerable attention,\nemerging as a promising alternative to convolutional neural networks (CNNs) in\nseveral vision-related applications. However, their large model sizes and high\ncomputational and memory demands hinder deployment, especially on\nresource-constrained devices. This underscores the necessity of\nalgorithm-hardware co-design specific to ViTs, aiming to optimize their\nperformance by tailoring both the algorithmic structure and the underlying\nhardware accelerator to each other's strengths. Model quantization, by\nconverting high-precision numbers to lower-precision, reduces the computational\ndemands and memory needs of ViTs, allowing the creation of hardware\nspecifically optimized for these quantized algorithms, boosting efficiency.\nThis article provides a comprehensive survey of ViTs quantization and its\nhardware acceleration. We first delve into the unique architectural attributes\nof ViTs and their runtime characteristics. Subsequently, we examine the\nfundamental principles of model quantization, followed by a comparative\nanalysis of the state-of-the-art quantization techniques for ViTs.\nAdditionally, we explore the hardware acceleration of quantized ViTs,\nhighlighting the importance of hardware-friendly algorithm design. In\nconclusion, this article will discuss ongoing challenges and future research\npaths. We consistently maintain the related open-source materials at\nhttps://github.com/DD-DuDa/awesome-vit-quantization-acceleration.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.AR",
      "cs.CV",
      "cs.PF"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2405.00314v1",
    "published_date": "2024-05-01 04:32:07 UTC",
    "updated_date": "2024-05-01 04:32:07 UTC"
  },
  {
    "arxiv_id": "2405.00750v1",
    "title": "From Keyboard to Chatbot: An AI-powered Integration Platform with Large-Language Models for Teaching Computational Thinking for Young Children",
    "authors": [
      "Changjae Lee",
      "Jinjun Xiong"
    ],
    "abstract": "Teaching programming in early childhood (4-9) to enhance computational\nthinking has gained popularity in the recent movement of computer science for\nall. However, current practices ignore some fundamental issues resulting from\nyoung children's developmental readiness, such as the sustained capability to\nkeyboarding, the decomposition of complex tasks to small tasks, the need for\nintuitive mapping from abstract programming to tangible outcomes, and the\nlimited amount of screen time exposure. To address these issues in this paper,\nwe present a novel methodology with an AI-powered integration platform to\neffectively teach computational thinking for young children. The system\nfeatures a hybrid pedagogy that supports both the top-down and bottom-up\napproach for teaching computational thinking. Young children can describe their\ndesired task in natural language, while the system can respond with an\neasy-to-understand program consisting of the right level of decomposed\nsub-tasks. A tangible robot can immediately execute the decomposed program and\ndemonstrate the program's outcomes to young children. The system is equipped\nwith an intelligent chatbot that can interact with young children through\nnatural languages, and children can speak to the chatbot to complete all the\nneeded programming tasks, while the chatbot orchestrates the execution of the\nprogram onto the robot. This would completely eliminates the need of keyboards\nfor young children to program. By developing such a system, we aim to make the\nconcept of computational thinking more accessible to young children, fostering\na natural understanding of programming concepts without the need of explicit\nprogramming skills. Through the interactive experience provided by the robotic\nagent, our system seeks to engage children in an effective manner, contributing\nto the field of educational technology for early childhood computer science\neducation.",
    "categories": [
      "cs.HC",
      "cs.AI",
      "cs.CY"
    ],
    "primary_category": "cs.HC",
    "comment": "26 pages, 11 figures",
    "pdf_url": "http://arxiv.org/pdf/2405.00750v1",
    "published_date": "2024-05-01 04:29:21 UTC",
    "updated_date": "2024-05-01 04:29:21 UTC"
  },
  {
    "arxiv_id": "2405.00307v1",
    "title": "Active Learning with Task Adaptation Pre-training for Speech Emotion Recognition",
    "authors": [
      "Dongyuan Li",
      "Ying Zhang",
      "Yusong Wang",
      "Funakoshi Kataro",
      "Manabu Okumura"
    ],
    "abstract": "Speech emotion recognition (SER) has garnered increasing attention due to its\nwide range of applications in various fields, including human-machine\ninteraction, virtual assistants, and mental health assistance. However,\nexisting SER methods often overlook the information gap between the\npre-training speech recognition task and the downstream SER task, resulting in\nsub-optimal performance. Moreover, current methods require much time for\nfine-tuning on each specific speech dataset, such as IEMOCAP, which limits\ntheir effectiveness in real-world scenarios with large-scale noisy data. To\naddress these issues, we propose an active learning (AL)-based fine-tuning\nframework for SER, called \\textsc{After}, that leverages task adaptation\npre-training (TAPT) and AL methods to enhance performance and efficiency.\nSpecifically, we first use TAPT to minimize the information gap between the\npre-training speech recognition task and the downstream speech emotion\nrecognition task. Then, AL methods are employed to iteratively select a subset\nof the most informative and diverse samples for fine-tuning, thereby reducing\ntime consumption. Experiments demonstrate that our proposed method\n\\textsc{After}, using only 20\\% of samples, improves accuracy by 8.45\\% and\nreduces time consumption by 79\\%. The additional extension of \\textsc{After}\nand ablation studies further confirm its effectiveness and applicability to\nvarious real-world scenarios. Our source code is available on Github for\nreproducibility. (https://github.com/Clearloveyuan/AFTER).",
    "categories": [
      "cs.SD",
      "cs.AI",
      "eess.AS"
    ],
    "primary_category": "cs.SD",
    "comment": "Accepted by Journal of Natural Language Processing. arXiv admin note:\n  text overlap with arXiv:2310.00283",
    "pdf_url": "http://arxiv.org/pdf/2405.00307v1",
    "published_date": "2024-05-01 04:05:29 UTC",
    "updated_date": "2024-05-01 04:05:29 UTC"
  },
  {
    "arxiv_id": "2405.00291v1",
    "title": "How Can I Improve? Using GPT to Highlight the Desired and Undesired Parts of Open-ended Responses",
    "authors": [
      "Jionghao Lin",
      "Eason Chen",
      "Zeifei Han",
      "Ashish Gurung",
      "Danielle R. Thomas",
      "Wei Tan",
      "Ngoc Dang Nguyen",
      "Kenneth R. Koedinger"
    ],
    "abstract": "Automated explanatory feedback systems play a crucial role in facilitating\nlearning for a large cohort of learners by offering feedback that incorporates\nexplanations, significantly enhancing the learning process. However, delivering\nsuch explanatory feedback in real-time poses challenges, particularly when high\nclassification accuracy for domain-specific, nuanced responses is essential.\nOur study leverages the capabilities of large language models, specifically\nGenerative Pre-Trained Transformers (GPT), to explore a sequence labeling\napproach focused on identifying components of desired and less desired praise\nfor providing explanatory feedback within a tutor training dataset. Our aim is\nto equip tutors with actionable, explanatory feedback during online training\nlessons. To investigate the potential of GPT models for providing the\nexplanatory feedback, we employed two commonly-used approaches: prompting and\nfine-tuning. To quantify the quality of highlighted praise components\nidentified by GPT models, we introduced a Modified Intersection over Union\n(M-IoU) score. Our findings demonstrate that: (1) the M-IoU score effectively\ncorrelates with human judgment in evaluating sequence quality; (2) using\ntwo-shot prompting on GPT-3.5 resulted in decent performance in recognizing\neffort-based (M-IoU of 0.46) and outcome-based praise (M-IoU of 0.68); and (3)\nour optimally fine-tuned GPT-3.5 model achieved M-IoU scores of 0.64 for\neffort-based praise and 0.84 for outcome-based praise, aligning with the\nsatisfaction levels evaluated by human coders. Our results show promise for\nusing GPT models to provide feedback that focuses on specific elements in their\nopen-ended responses that are desirable or could use improvement.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.HC"
    ],
    "primary_category": "cs.CL",
    "comment": "11 pages, full research paper, EDM 2024",
    "pdf_url": "http://arxiv.org/pdf/2405.00291v1",
    "published_date": "2024-05-01 02:59:10 UTC",
    "updated_date": "2024-05-01 02:59:10 UTC"
  },
  {
    "arxiv_id": "2405.00289v2",
    "title": "Adversarial Attacks and Defense for Conversation Entailment Task",
    "authors": [
      "Zhenning Yang",
      "Ryan Krawec",
      "Liang-Yuan Wu"
    ],
    "abstract": "As the deployment of NLP systems in critical applications grows, ensuring the\nrobustness of large language models (LLMs) against adversarial attacks becomes\nincreasingly important. Large language models excel in various NLP tasks but\nremain vulnerable to low-cost adversarial attacks. Focusing on the domain of\nconversation entailment, where multi-turn dialogues serve as premises to verify\nhypotheses, we fine-tune a transformer model to accurately discern the\ntruthfulness of these hypotheses. Adversaries manipulate hypotheses through\nsynonym swapping, aiming to deceive the model into making incorrect\npredictions. To counteract these attacks, we implemented innovative fine-tuning\ntechniques and introduced an embedding perturbation loss method to\nsignificantly bolster the model's robustness. Our findings not only emphasize\nthe importance of defending against adversarial attacks in NLP but also\nhighlight the real-world implications, suggesting that enhancing model\nrobustness is critical for reliable NLP applications.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2405.00289v2",
    "published_date": "2024-05-01 02:49:18 UTC",
    "updated_date": "2024-05-02 03:37:08 UTC"
  },
  {
    "arxiv_id": "2405.00748v2",
    "title": "ChatGPT in Data Visualization Education: A Student Perspective",
    "authors": [
      "Nam Wook Kim",
      "Hyung-Kwon Ko",
      "Grace Myers",
      "Benjamin Bach"
    ],
    "abstract": "Unlike traditional educational chatbots that rely on pre-programmed\nresponses, large-language model-driven chatbots, such as ChatGPT, demonstrate\nremarkable versatility to serve as a dynamic resource for addressing student\nneeds from understanding advanced concepts to solving complex problems. This\nwork explores the impact of such technology on student learning in an\ninterdisciplinary, project-oriented data visualization course. Throughout the\nsemester, students engaged with ChatGPT across four distinct projects,\ndesigning and implementing data visualizations using a variety of tools such as\nTableau, D3, and Vega-lite. We collected conversation logs and reflection\nsurveys after each assignment and conducted interviews with selected students\nto gain deeper insights into their experiences with ChatGPT. Our analysis\nexamined the advantages and barriers of using ChatGPT, students' querying\nbehavior, the types of assistance sought, and its impact on assignment outcomes\nand engagement. We discuss design considerations for an educational solution\ntailored for data visualization education, extending beyond ChatGPT's basic\ninterface.",
    "categories": [
      "cs.HC",
      "cs.AI",
      "cs.CY"
    ],
    "primary_category": "cs.HC",
    "comment": "12 pages; 3 figures",
    "pdf_url": "http://arxiv.org/pdf/2405.00748v2",
    "published_date": "2024-05-01 02:40:20 UTC",
    "updated_date": "2024-08-16 21:50:40 UTC"
  },
  {
    "arxiv_id": "2405.00287v2",
    "title": "SCONE: A Novel Stochastic Sampling to Generate Contrastive Views and Hard Negative Samples for Recommendation",
    "authors": [
      "Chaejeong Lee",
      "Jeongwhan Choi",
      "Hyowon Wi",
      "Sung-Bae Cho",
      "Noseong Park"
    ],
    "abstract": "Graph-based collaborative filtering (CF) has emerged as a promising approach\nin recommender systems. Despite its achievements, graph-based CF models face\nchallenges due to data sparsity and negative sampling. In this paper, we\npropose a novel Stochastic sampling for i) COntrastive views and ii) hard\nNEgative samples (SCONE) to overcome these issues. SCONE generates dynamic\naugmented views and diverse hard negative samples via a unified stochastic\nsampling approach based on score-based generative models. Our extensive\nexperiments on 6 benchmark datasets show that SCONE consistently outperforms\nstate-of-the-art baselines. SCONE shows efficacy in addressing user sparsity\nand item popularity issues, while enhancing performance for both cold-start\nusers and long-tail items. Furthermore, our approach improves the diversity of\nthe recommendation and the uniformity of the representations. The code is\navailable at https://github.com/jeongwhanchoi/SCONE.",
    "categories": [
      "cs.IR",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.IR",
    "comment": "Accepted to WSDM 2025. Chaejeong Lee and Jeongwhan Choi are co-first\n  authors with equal contributions",
    "pdf_url": "http://arxiv.org/pdf/2405.00287v2",
    "published_date": "2024-05-01 02:27:59 UTC",
    "updated_date": "2024-12-19 05:48:08 UTC"
  },
  {
    "arxiv_id": "2405.00285v4",
    "title": "iMTSP: Solving Min-Max Multiple Traveling Salesman Problem with Imperative Learning",
    "authors": [
      "Yifan Guo",
      "Zhongqiang Ren",
      "Chen Wang"
    ],
    "abstract": "This paper considers a Min-Max Multiple Traveling Salesman Problem (MTSP),\nwhere the goal is to find a set of tours, one for each agent, to collectively\nvisit all the cities while minimizing the length of the longest tour. Though\nMTSP has been widely studied, obtaining near-optimal solutions for large-scale\nproblems is still challenging due to its NP-hardness. Recent efforts in\ndata-driven methods face challenges of the need for hard-to-obtain supervision\nand issues with high variance in gradient estimations, leading to slow\nconvergence and highly suboptimal solutions. We address these issues by\nreformulating MTSP as a bilevel optimization problem, using the concept of\nimperative learning (IL). This involves introducing an allocation network that\ndecomposes the MTSP into multiple single-agent traveling salesman problems\n(TSPs). The longest tour from these TSP solutions is then used to\nself-supervise the allocation network, resulting in a new self-supervised,\nbilevel, end-to-end learning framework, which we refer to as imperative MTSP\n(iMTSP). Additionally, to tackle the high-variance gradient issues during the\noptimization, we introduce a control variate-based gradient estimation\nalgorithm. Our experiments showed that these innovative designs enable our\ngradient estimator to converge 20% faster than the advanced reinforcement\nlearning baseline and find up to 80% shorter tour length compared with Google\nOR-Tools MTSP solver, especially in large-scale problems (e.g. 1000 cities and\n15 agents).",
    "categories": [
      "cs.AI",
      "cs.LG",
      "cs.RO"
    ],
    "primary_category": "cs.AI",
    "comment": "8 pages, 3 figures, 3 tables",
    "pdf_url": "http://arxiv.org/pdf/2405.00285v4",
    "published_date": "2024-05-01 02:26:13 UTC",
    "updated_date": "2024-08-23 15:02:34 UTC"
  },
  {
    "arxiv_id": "2405.00282v1",
    "title": "MF-OML: Online Mean-Field Reinforcement Learning with Occupation Measures for Large Population Games",
    "authors": [
      "Anran Hu",
      "Junzi Zhang"
    ],
    "abstract": "Reinforcement learning for multi-agent games has attracted lots of attention\nrecently. However, given the challenge of solving Nash equilibria for large\npopulation games, existing works with guaranteed polynomial complexities either\nfocus on variants of zero-sum and potential games, or aim at solving (coarse)\ncorrelated equilibria, or require access to simulators, or rely on certain\nassumptions that are hard to verify. This work proposes MF-OML (Mean-Field\nOccupation-Measure Learning), an online mean-field reinforcement learning\nalgorithm for computing approximate Nash equilibria of large population\nsequential symmetric games. MF-OML is the first fully polynomial multi-agent\nreinforcement learning algorithm for provably solving Nash equilibria (up to\nmean-field approximation gaps that vanish as the number of players $N$ goes to\ninfinity) beyond variants of zero-sum and potential games. When evaluated by\nthe cumulative deviation from Nash equilibria, the algorithm is shown to\nachieve a high probability regret bound of $\\tilde{O}(M^{3/4}+N^{-1/2}M)$ for\ngames with the strong Lasry-Lions monotonicity condition, and a regret bound of\n$\\tilde{O}(M^{11/12}+N^{- 1/6}M)$ for games with only the Lasry-Lions\nmonotonicity condition, where $M$ is the total number of episodes and $N$ is\nthe number of agents of the game. As a byproduct, we also obtain the first\ntractable globally convergent computational algorithm for computing approximate\nNash equilibria of monotone mean-field games.",
    "categories": [
      "math.OC",
      "cs.AI",
      "cs.GT",
      "cs.LG",
      "cs.MA"
    ],
    "primary_category": "math.OC",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2405.00282v1",
    "published_date": "2024-05-01 02:19:31 UTC",
    "updated_date": "2024-05-01 02:19:31 UTC"
  },
  {
    "arxiv_id": "2405.00263v1",
    "title": "Clover: Regressive Lightweight Speculative Decoding with Sequential Knowledge",
    "authors": [
      "Bin Xiao",
      "Chunan Shi",
      "Xiaonan Nie",
      "Fan Yang",
      "Xiangwei Deng",
      "Lei Su",
      "Weipeng Chen",
      "Bin Cui"
    ],
    "abstract": "Large language models (LLMs) suffer from low efficiency as the mismatch\nbetween the requirement of auto-regressive decoding and the design of most\ncontemporary GPUs. Specifically, billions to trillions of parameters must be\nloaded to the GPU cache through its limited memory bandwidth for computation,\nbut only a small batch of tokens is actually computed. Consequently, the GPU\nspends most of its time on memory transfer instead of computation. Recently,\nparallel decoding, a type of speculative decoding algorithms, is becoming more\npopular and has demonstrated impressive efficiency improvement in generation.\nIt introduces extra decoding heads to large models, enabling them to predict\nmultiple subsequent tokens simultaneously and verify these candidate\ncontinuations in a single decoding step. However, this approach deviates from\nthe training objective of next token prediction used during pre-training,\nresulting in a low hit rate for candidate tokens. In this paper, we propose a\nnew speculative decoding algorithm, Clover, which integrates sequential\nknowledge into the parallel decoding process. This enhancement improves the hit\nrate of speculators and thus boosts the overall efficiency. Clover transmits\nthe sequential knowledge from pre-speculated tokens via the Regressive\nConnection, then employs an Attention Decoder to integrate these speculated\ntokens. Additionally, Clover incorporates an Augmenting Block that modifies the\nhidden states to better align with the purpose of speculative generation rather\nthan next token prediction. The experiment results demonstrate that Clover\noutperforms the baseline by up to 91% on Baichuan-Small and 146% on\nBaichuan-Large, respectively, and exceeds the performance of the previously\ntop-performing method, Medusa, by up to 37% on Baichuan-Small and 57% on\nBaichuan-Large, respectively.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2405.00263v1",
    "published_date": "2024-05-01 00:46:22 UTC",
    "updated_date": "2024-05-01 00:46:22 UTC"
  }
]
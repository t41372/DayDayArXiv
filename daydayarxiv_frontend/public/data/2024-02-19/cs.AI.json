{
  "date": "2024-02-19",
  "category": "cs.AI",
  "summary": "欢迎来到 UTC 时间 2024-02-19 的 arXiv 中文 TLDR 快报！今天 arXiv 的论文主要聚焦于大型语言模型（LLM）的安全、公平性和应用创新，以及强化学习、知识图谱和多模态模型的进展，其中令人印象深刻的包括 LLM 在情感支持和代码生成的潜力（如 Llama 系列模型的实验），以及知名学者 David Leslie 参与的多篇 AI 伦理和公平性论文，这些工作突显了 AI 模型在实际部署中的挑战和优化方向。\n\n### 重点论文讨论\n今天共有 133 篇论文，我将优先讨论那些有话题度、潜在影响大或涉及知名学者的文章，并将相关主题归类讨论。其他论文（如一些纯技术细节或小规模实验）将快速掠过，只简要提及核心贡献。\n\n#### LLM 安全与公平性\n这些论文探讨了 LLM 的潜在风险和改进策略，相关工作有很强的现实应用价值。\n- **AI Fairness in Practice**（AI 公平性在实践中的应用）：David Leslie 等学者提出一个基于上下文的 AI 公平框架，聚焦于平等和非歧视，涵盖数据公平性、模型设计等多个方面。主要贡献是提供实用工具（如偏差自评估），帮助项目团队识别和缓解偏置，实现多维公平评估。\n- **AI Sustainability in Practice Part One: Foundations for Sustainable AI Projects**（AI 可持续性在实践中的应用：可持续 AI 项目的基石）：同样由 David Leslie 等作者，引入 SUM 值框架和利益相关者参与过程，促进 AI 项目的社会影响评估。主要发现是强调持续响应性，以避免 AI 的长期负面影响。\n- **AI Sustainability in Practice Part Two: Sustainability Throughout the AI Workflow**（AI 可持续性在实践中的应用：贯穿 AI 工作流的可持续性）：续上文，作者扩展了利益相关者影响评估模板，强调动态评估和权衡取舍。贡献在于提供端到端过程模板，提升 AI 创新的伦理响应。\n- **FairProof: Confidential and Certifiable Fairness for Neural Networks**（FairProof：神经网络的保密和可证公平性）：Chhavi Yadav 等提出使用零知识证明（Zero-Knowledge Proofs）验证神经网络的公平性，同时保持机密性。主要发现是实验证明该系统在 Gnark 框架下可行，提升了公平性验证的实际应用。\n\n这些论文由知名学者主导，强调 AI 的伦理和可持续性，潜在影响巨大，但也暴露出 LLM 在公平评估中的挑战，如需要更多多语种数据支持。\n\n#### LLM 改进与应用\nLLM 的优化和实际应用是今天的核心主题，尤其在推理和生成方面。\n- **Confidence Matters: Revisiting Intrinsic Self-Correction Capabilities of Large Language Models**（置信度很重要：重新审视大型语言模型的内在自校正能力）：作者 Loka Li 等通过引入“IoE”提示框架，证明 LLM 可通过理解自身置信度来提升自校正能力。主要贡献是实验显示该框架显著提高响应准确性，代码开源。\n- **CausalGym: Benchmarking causal interpretability methods on linguistic tasks**（CausalGym：针对语言任务的因果可解释性方法基准测试）：Aryaman Arora 等开发了一个基准测试框架，评估 LLM 在因果推理中的表现。主要发现是 Pythia 模型在学习因果机制时表现出阶段性进展，揭示了 LLM 在语言任务中的局限。\n- **EmoBench: Evaluating the Emotional Intelligence of Large Language Models**（EmoBench：评估大型语言模型的情感智能）：Sahand Sabour 等提出一个基准测试 LLM 情感智能的框架，涵盖情感理解和应用。主要贡献是实验显示现有 LLM 在情感任务上远逊于人类，强调未来改进方向。\n- **The Revolution of Multimodal Large Language Models: A Survey**（多模态大型语言模型的革命：一个调查）：Davide Caffagni 等进行全面调查，分析多模态 LLM 的架构和训练技巧。主要发现是这些模型在视觉和文本任务上表现出色，但仍需改进融合策略。\n\n这些工作突出了 LLM 在自校正和多模态融合的潜力，但也暴露了在复杂推理中的不足，如需要更精确的提示设计。\n\n#### 强化学习与决策\n强化学习论文聚焦效率和鲁棒性，部分工作有实际应用潜力。\n- **Offline Multi-task Transfer RL with Representational Penalization**（带表示惩罚的离线多任务转移强化学习）：Avinandan Bose 等提出一种算法，通过不确定性度量改善离线 RL 在多任务转移中的性能。主要贡献是实验证明它在稀疏数据上提升了预测性能，适用于复杂 MDP。\n- **The Edge-of-Reach Problem in Offline Model-Based Reinforcement Learning**（离线基于模型的强化学习中的边界问题）：Anya Sims 等分析了离线 RL 中的边界状态问题，并提出 RAVL 算法解决过度估计。主要发现是该方法在真实动态模型下更鲁棒，代码开源。\n\n这些论文提升了 RL 的实际可行性，但相对较少话题度。\n\n#### 其他快速提及\n其他论文覆盖多领域，如知识图谱、生成模型和优化算法，我将简要概述核心贡献：\n- **Multimodal Fusion of EHR in Structures and Semantics: Integrating Clinical Records and Notes with Hypergraph and LLM**（EHR 的多模态融合：使用超图和 LLM 整合临床记录和笔记）：Hejie Cui 等提出 MINGLE 框架，提升 EHR 数据预测性能，主要发现是相对改善 11.83%。\n- **Graph-based Virtual Sensing from Sparse and Partial Multivariate Observations**（基于图的虚拟感知：从稀疏部分多变量观察中推断）：Giovanni De Felice 等开发 GgNet 架构，提高稀疏数据重建准确性，已被 ICLR 2024 接受。\n- **LangXAI: Integrating Large Vision Models for Generating Textual Explanations to Enhance Explainability in Visual Perception Tasks**（LangXAI：整合大型视觉模型生成文本解释以提升视觉感知任务的可解释性）：Truong Thanh Hung Nguyen 等框架提升视觉任务的可解释性，主要贡献是高 BERTScore。\n- 其余如时间序列分析、AI 可持续性等论文（如 \"LTL learning on GPUs\"），主要优化技术细节，贡献在于效率提升，但影响力较小，建议读者根据具体需求查阅。\n\n总之，今天的论文强调了 AI 模型的可靠性和扩展性，LLM 相关工作最具话题度，未来研究应聚焦公平性和多模态融合。更多细节可查阅 arXiv。",
  "papers": [
    {
      "arxiv_id": "2403.08818v1",
      "title": "Multimodal Fusion of EHR in Structures and Semantics: Integrating Clinical Records and Notes with Hypergraph and LLM",
      "title_zh": "翻译失败",
      "authors": [
        "Hejie Cui",
        "Xinyu Fang",
        "Ran Xu",
        "Xuan Kan",
        "Joyce C. Ho",
        "Carl Yang"
      ],
      "abstract": "Electronic Health Records (EHRs) have become increasingly popular to support\nclinical decision-making and healthcare in recent decades. EHRs usually contain\nheterogeneous information, such as structural data in tabular form and\nunstructured data in textual notes. Different types of information in EHRs can\ncomplement each other and provide a more complete picture of the health status\nof a patient. While there has been a lot of research on representation learning\nof structured EHR data, the fusion of different types of EHR data (multimodal\nfusion) is not well studied. This is mostly because of the complex medical\ncoding systems used and the noise and redundancy present in the written notes.\nIn this work, we propose a new framework called MINGLE, which integrates both\nstructures and semantics in EHR effectively. Our framework uses a two-level\ninfusion strategy to combine medical concept semantics and clinical note\nsemantics into hypergraph neural networks, which learn the complex interactions\nbetween different types of data to generate visit representations for\ndownstream prediction. Experiment results on two EHR datasets, the public\nMIMIC-III and private CRADLE, show that MINGLE can effectively improve\npredictive performance by 11.83% relatively, enhancing semantic integration as\nwell as multimodal fusion for structural and textual EHR data.",
      "tldr_zh": "电子健康记录 (EHR) 包含结构化表格数据和非结构化文本笔记，但多模态融合研究较少，主要受复杂医疗编码系统和数据噪声影响。本文提出 MINGLE 框架，使用两级融合策略将医疗概念语义和临床笔记语义整合到超图神经网络 (Hypergraph Neural Networks) 中，学习不同数据类型间的复杂交互以生成访问表示，用于下游预测任务。实验结果显示，在 MIMIC-III 和 CRADLE 数据集上，MINGLE 相对提高了 11.83% 的预测性能，证明了其在增强 EHR 语义整合和多模态融合方面的有效性。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2403.08818v1",
      "published_date": "2024-02-19 23:48:40 UTC",
      "updated_date": "2024-02-19 23:48:40 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T07:37:04.570553"
    },
    {
      "arxiv_id": "2402.12598v1",
      "title": "Graph-based Virtual Sensing from Sparse and Partial Multivariate Observations",
      "title_zh": "翻译失败",
      "authors": [
        "Giovanni De Felice",
        "Andrea Cini",
        "Daniele Zambon",
        "Vladimir V. Gusev",
        "Cesare Alippi"
      ],
      "abstract": "Virtual sensing techniques allow for inferring signals at new unmonitored\nlocations by exploiting spatio-temporal measurements coming from physical\nsensors at different locations. However, as the sensor coverage becomes sparse\ndue to costs or other constraints, physical proximity cannot be used to support\ninterpolation. In this paper, we overcome this challenge by leveraging\ndependencies between the target variable and a set of correlated variables\n(covariates) that can frequently be associated with each location of interest.\nFrom this viewpoint, covariates provide partial observability, and the problem\nconsists of inferring values for unobserved channels by exploiting observations\nat other locations to learn how such variables can correlate. We introduce a\nnovel graph-based methodology to exploit such relationships and design a graph\ndeep learning architecture, named GgNet, implementing the framework. The\nproposed approach relies on propagating information over a nested graph\nstructure that is used to learn dependencies between variables as well as\nlocations. GgNet is extensively evaluated under different virtual sensing\nscenarios, demonstrating higher reconstruction accuracy compared to the\nstate-of-the-art.",
      "tldr_zh": "该研究针对传感器覆盖稀疏的场景，提出了一种基于图的虚拟传感（Virtual Sensing）方法，以从部分多变量观察（Multivariate Observations）中推断未监测位置的信号。方法利用目标变量与相关协变量（covariates）的依赖关系，设计了GgNet图深度学习架构，通过嵌套图结构传播信息，学习变量和位置间的相关性。实验结果显示，GgNet在各种虚拟传感场景下，比现有技术实现了更高的重建准确率。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "Accepted at ICLR 2024",
      "pdf_url": "http://arxiv.org/pdf/2402.12598v1",
      "published_date": "2024-02-19 23:22:30 UTC",
      "updated_date": "2024-02-19 23:22:30 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T07:37:14.131137"
    },
    {
      "arxiv_id": "2403.14636v1",
      "title": "AI Fairness in Practice",
      "title_zh": "AI 公平在实践中",
      "authors": [
        "David Leslie",
        "Cami Rincon",
        "Morgan Briggs",
        "Antonella Perini",
        "Smera Jayadeva",
        "Ann Borda",
        "SJ Bennett",
        "Christopher Burr",
        "Mhairi Aitken",
        "Michael Katell",
        "Claudia Fischer",
        "Janis Wong",
        "Ismael Kherroubi Garcia"
      ],
      "abstract": "Reaching consensus on a commonly accepted definition of AI Fairness has long\nbeen a central challenge in AI ethics and governance. There is a broad spectrum\nof views across society on what the concept of fairness means and how it should\nbest be put to practice. In this workbook, we tackle this challenge by\nexploring how a context-based and society-centred approach to understanding AI\nFairness can help project teams better identify, mitigate, and manage the many\nways that unfair bias and discrimination can crop up across the AI project\nworkflow.\n  We begin by exploring how, despite the plurality of understandings about the\nmeaning of fairness, priorities of equality and non-discrimination have come to\nconstitute the broadly accepted core of its application as a practical\nprinciple. We focus on how these priorities manifest in the form of equal\nprotection from direct and indirect discrimination and from discriminatory\nharassment. These elements form ethical and legal criteria based upon which\ninstances of unfair bias and discrimination can be identified and mitigated\nacross the AI project workflow.\n  We then take a deeper dive into how the different contexts of the AI project\nlifecycle give rise to different fairness concerns. This allows us to identify\nseveral types of AI Fairness (Data Fairness, Application Fairness, Model Design\nand Development Fairness, Metric-Based Fairness, System Implementation\nFairness, and Ecosystem Fairness) that form the basis of a multi-lens approach\nto bias identification, mitigation, and management. Building on this, we\ndiscuss how to put the principle of AI Fairness into practice across the AI\nproject workflow through Bias Self-Assessment and Bias Risk Management as well\nas through the documentation of metric-based fairness criteria in a Fairness\nPosition Statement.",
      "tldr_zh": "这篇论文探讨了 AI Fairness 在实践中的挑战，提出一种基于上下文和以社会为中心的框架，帮助项目团队识别、缓解和管理 AI 项目工作流中的不公平偏差和歧视。论文强调平等和非歧视作为 AI Fairness 的核心原则，包括防范直接和间接歧视，以及歧视性骚扰。作者识别了多种公平性类型，如 Data Fairness、Application Fairness、Model Design and Development Fairness、Metric-Based Fairness、System Implementation Fairness 和 Ecosystem Fairness，以多镜头方法处理项目生命周期中的偏差问题。通过 Bias Self-Assessment、Bias Risk Management 和 Fairness Position Statement，论文提供实用指导，将 AI Fairness 原则应用于实际项目中。",
      "categories": [
        "cs.CY",
        "cs.AI",
        "cs.HC"
      ],
      "primary_category": "cs.CY",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2403.14636v1",
      "published_date": "2024-02-19 23:02:56 UTC",
      "updated_date": "2024-02-19 23:02:56 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T07:37:27.742069"
    },
    {
      "arxiv_id": "2403.15404v1",
      "title": "AI Sustainability in Practice Part Two: Sustainability Throughout the AI Workflow",
      "title_zh": "翻译失败",
      "authors": [
        "David Leslie",
        "Cami Rincon",
        "Morgan Briggs",
        "Antonella Perini",
        "Smera Jayadeva",
        "Ann Borda",
        "SJ Bennett",
        "Christopher Burr",
        "Mhairi Aitken",
        "Michael Katell",
        "Claudia Fischer",
        "Janis Wong",
        "Ismael Kherroubi Garcia"
      ],
      "abstract": "The sustainability of AI systems depends on the capacity of project teams to\nproceed with a continuous sensitivity to their potential real-world impacts and\ntransformative effects. Stakeholder Impact Assessments (SIAs) are governance\nmechanisms that enable this kind of responsiveness. They are tools that create\na procedure for, and a means of documenting, the collaborative evaluation and\nreflective anticipation of the possible harms and benefits of AI innovation\nprojects. SIAs are not one-off governance actions. They require project teams\nto pay continuous attention to the dynamic and changing character of AI\nproduction and use and to the shifting conditions of the real-world\nenvironments in which AI technologies are embedded. This workbook is part two\nof two workbooks on AI Sustainability. It provides a template of the SIA and\nactivities that allow a deeper dive into crucial parts of it. It discusses\nmethods for weighing values and considering trade-offs during the SIA. And, it\nhighlights the need to treat the SIA as an end-to-end process of responsive\nevaluation and re-assessment.",
      "tldr_zh": "这篇论文探讨了AI系统的可持续性，强调项目团队需持续关注AI创新的潜在影响和变革效果。论文介绍了Stakeholder Impact Assessments (SIAs)作为一种治理工具，用于协作评估和记录AI项目的可能危害与益处，并将其视为一个动态、持续的过程。作者提供SIAs模板、相关活动和方法，帮助权衡价值观与权衡取舍，并强调将SIAs视为端到端的响应式评估机制，以适应AI生产、使用及真实环境的变化。",
      "categories": [
        "cs.CY",
        "cs.AI",
        "cs.HC"
      ],
      "primary_category": "cs.CY",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2403.15404v1",
      "published_date": "2024-02-19 22:58:05 UTC",
      "updated_date": "2024-02-19 22:58:05 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T07:37:38.034852"
    },
    {
      "arxiv_id": "2403.14635v1",
      "title": "AI Sustainability in Practice Part One: Foundations for Sustainable AI Projects",
      "title_zh": "翻译失败",
      "authors": [
        "David Leslie",
        "Cami Rincon",
        "Morgan Briggs",
        "Antonella Perini",
        "Smera Jayadeva",
        "Ann Borda",
        "SJ Bennett",
        "Christopher Burr",
        "Mhairi Aitken",
        "Michael Katell",
        "Claudia Fischer",
        "Janis Wong",
        "Ismael Kherroubi Garcia"
      ],
      "abstract": "Sustainable AI projects are continuously responsive to the transformative\neffects as well as short-, medium-, and long-term impacts on individuals and\nsociety that the design, development, and deployment of AI technologies may\nhave. Projects, which centre AI Sustainability, ensure that values-led,\ncollaborative, and anticipatory reflection both guides the assessment of\npotential social and ethical impacts and steers responsible innovation\npractices.\n  This workbook is the first part of a pair that provides the concepts and\ntools needed to put AI Sustainability into practice. It introduces the SUM\nValues, which help AI project teams to assess the potential societal impacts\nand ethical permissibility of their projects. It then presents a Stakeholder\nEngagement Process (SEP), which provides tools to facilitate proportionate\nengagement of and input from stakeholders with an emphasis on equitable and\nmeaningful participation and positionality awareness.",
      "tldr_zh": "这篇论文介绍了可持续 AI 项目的实践基础，强调通过价值观导向、协作和前瞻性反思来评估 AI 技术的设计、开发和部署对个人和社会的潜在影响。\n它提出了 SUM Values 框架，帮助 AI 项目团队评估社会影响和伦理可接受性。\n此外，论文呈现了 Stakeholder Engagement Process (SEP)，提供工具以促进利益相关者的公平参与和位置意识，确保负责任的创新实践。",
      "categories": [
        "cs.CY",
        "cs.AI",
        "cs.HC"
      ],
      "primary_category": "cs.CY",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2403.14635v1",
      "published_date": "2024-02-19 22:52:14 UTC",
      "updated_date": "2024-02-19 22:52:14 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T07:37:50.224947"
    },
    {
      "arxiv_id": "2403.15403v1",
      "title": "AI Ethics and Governance in Practice: An Introduction",
      "title_zh": "AI 伦理与治理在实践：引言",
      "authors": [
        "David Leslie",
        "Cami Rincon",
        "Morgan Briggs",
        "Antonella Perini",
        "Smera Jayadeva",
        "Ann Borda",
        "SJ Bennett",
        "Christopher Burr",
        "Mhairi Aitken",
        "Michael Katell",
        "Claudia Fischer"
      ],
      "abstract": "AI systems may have transformative and long-term effects on individuals and\nsociety. To manage these impacts responsibly and direct the development of AI\nsystems toward optimal public benefit, considerations of AI ethics and\ngovernance must be a first priority.\n  In this workbook, we introduce and describe our PBG Framework, a multi-tiered\ngovernance model that enables project teams to integrate ethical values and\npractical principles into their innovation practices and to have clear\nmechanisms for demonstrating and documenting this.",
      "tldr_zh": "该论文强调了AI系统对个人和社会的潜在变革性影响，并主张AI伦理和治理作为首要考虑，以负责任地管理这些影响并促进公共利益。作者引入了PBG Framework，这是一个多层治理模型，旨在帮助项目团队将伦理价值观和实用原则整合到创新实践中。框架还提供清晰的机制来展示和记录这些整合，确保AI开发过程更具透明性和可信度。",
      "categories": [
        "cs.CY",
        "cs.AI",
        "cs.HC"
      ],
      "primary_category": "cs.CY",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2403.15403v1",
      "published_date": "2024-02-19 22:43:19 UTC",
      "updated_date": "2024-02-19 22:43:19 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T07:38:01.633947"
    },
    {
      "arxiv_id": "2402.12572v2",
      "title": "FairProof : Confidential and Certifiable Fairness for Neural Networks",
      "title_zh": "FairProof：神经网络的保密与可证实的公平性",
      "authors": [
        "Chhavi Yadav",
        "Amrita Roy Chowdhury",
        "Dan Boneh",
        "Kamalika Chaudhuri"
      ],
      "abstract": "Machine learning models are increasingly used in societal applications, yet\nlegal and privacy concerns demand that they very often be kept confidential.\nConsequently, there is a growing distrust about the fairness properties of\nthese models in the minds of consumers, who are often at the receiving end of\nmodel predictions. To this end, we propose \\name -- a system that uses\nZero-Knowledge Proofs (a cryptographic primitive) to publicly verify the\nfairness of a model, while maintaining confidentiality. We also propose a\nfairness certification algorithm for fully-connected neural networks which is\nbefitting to ZKPs and is used in this system. We implement \\name in Gnark and\ndemonstrate empirically that our system is practically feasible. Code is\navailable at https://github.com/infinite-pursuits/FairProof.",
      "tldr_zh": "该研究提出FairProof系统，利用Zero-Knowledge Proofs（零知识证明）技术，实现神经网络的公平性公开验证，同时保持模型机密性，以缓解消费者对模型公平性的不信任。系统包括一个针对全连接神经网络的公平性认证算法，该算法与ZKPs兼容，确保验证过程高效且安全。在Gnark框架中实现的FairProof通过实验证明其实际可行性，并提供了开源代码（https://github.com/infinite-pursuits/FairProof）。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CR"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2402.12572v2",
      "published_date": "2024-02-19 21:53:43 UTC",
      "updated_date": "2024-07-16 00:56:20 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T07:38:14.124297"
    },
    {
      "arxiv_id": "2402.12570v1",
      "title": "Offline Multi-task Transfer RL with Representational Penalization",
      "title_zh": "翻译失败",
      "authors": [
        "Avinandan Bose",
        "Simon Shaolei Du",
        "Maryam Fazel"
      ],
      "abstract": "We study the problem of representation transfer in offline Reinforcement\nLearning (RL), where a learner has access to episodic data from a number of\nsource tasks collected a priori, and aims to learn a shared representation to\nbe used in finding a good policy for a target task. Unlike in online RL where\nthe agent interacts with the environment while learning a policy, in the\noffline setting there cannot be such interactions in either the source tasks or\nthe target task; thus multi-task offline RL can suffer from incomplete\ncoverage.\n  We propose an algorithm to compute pointwise uncertainty measures for the\nlearnt representation, and establish a data-dependent upper bound for the\nsuboptimality of the learnt policy for the target task. Our algorithm leverages\nthe collective exploration done by source tasks to mitigate poor coverage at\nsome points by a few tasks, thus overcoming the limitation of needing uniformly\ngood coverage for a meaningful transfer by existing offline algorithms. We\ncomplement our theoretical results with empirical evaluation on a\nrich-observation MDP which requires many samples for complete coverage. Our\nfindings illustrate the benefits of penalizing and quantifying the uncertainty\nin the learnt representation.",
      "tldr_zh": "本研究探讨了离线强化学习（Offline RL）中的多任务转移问题，提出了一种基于表示惩罚（Representational Penalization）的算法，用于从多个源任务的数据中学习共享表示，以优化目标任务的策略。\n该算法通过计算学习表示的点式不确定性并建立数据依赖的上界，利用源任务的集体探索来缓解覆盖不足的挑战，从而提升转移学习的有效性。\n实验结果表明，该方法在需要大量样本的 Markov Decision Process (MDP) 上表现出色，显著降低了策略次优性并证明了量化不确定性的益处。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2402.12570v1",
      "published_date": "2024-02-19 21:52:44 UTC",
      "updated_date": "2024-02-19 21:52:44 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T07:38:27.034918"
    },
    {
      "arxiv_id": "2402.12563v3",
      "title": "Confidence Matters: Revisiting Intrinsic Self-Correction Capabilities of Large Language Models",
      "title_zh": "置信度至关重要：重新审视大语言模型的内在自我修正能力",
      "authors": [
        "Loka Li",
        "Zhenhao Chen",
        "Guangyi Chen",
        "Yixuan Zhang",
        "Yusheng Su",
        "Eric Xing",
        "Kun Zhang"
      ],
      "abstract": "The recent success of Large Language Models (LLMs) has catalyzed an\nincreasing interest in their self-correction capabilities. This paper presents\na comprehensive investigation into the intrinsic self-correction of LLMs,\nattempting to address the ongoing debate about its feasibility. Our research\nhas identified an important latent factor - the \"confidence\" of LLMs - during\nthe self-correction process. Overlooking this factor may cause the models to\nover-criticize themselves, resulting in unreliable conclusions regarding the\nefficacy of self-correction. We have experimentally observed that LLMs possess\nthe capability to understand the \"confidence\" in their own responses. It\nmotivates us to develop an \"If-or-Else\" (IoE) prompting framework, designed to\nguide LLMs in assessing their own \"confidence\", facilitating intrinsic\nself-corrections. We conduct extensive experiments and demonstrate that our\nIoE-based Prompt can achieve a consistent improvement regarding the accuracy of\nself-corrected responses over the initial answers. Our study not only sheds\nlight on the underlying factors affecting self-correction in LLMs, but also\nintroduces a practical framework that utilizes the IoE prompting principle to\nefficiently improve self-correction capabilities with \"confidence\". The code is\navailable at https://github.com/MBZUAI-CLeaR/IoE-Prompting.git.",
      "tldr_zh": "本研究重新审视大型语言模型 (LLMs) 的内在自校正能力，强调“confidence”（置信度）是关键因素，因为忽略它可能导致模型过度自责并得出不可靠结论。研究发现 LLMs 可以理解自身响应的置信度，并开发了“If-or-Else” (IoE) 提示框架来指导模型评估置信度，从而实现有效的内在自校正。通过广泛实验验证，IoE 提示框架显著提高了自校正响应的准确性，为提升 LLMs 的自校正性能提供了实用方法。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "12 figures, 9 tables",
      "pdf_url": "http://arxiv.org/pdf/2402.12563v3",
      "published_date": "2024-02-19 21:38:02 UTC",
      "updated_date": "2024-05-13 11:01:17 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T07:38:38.482720"
    },
    {
      "arxiv_id": "2402.12560v1",
      "title": "CausalGym: Benchmarking causal interpretability methods on linguistic tasks",
      "title_zh": "翻译失败",
      "authors": [
        "Aryaman Arora",
        "Dan Jurafsky",
        "Christopher Potts"
      ],
      "abstract": "Language models (LMs) have proven to be powerful tools for psycholinguistic\nresearch, but most prior work has focused on purely behavioural measures (e.g.,\nsurprisal comparisons). At the same time, research in model interpretability\nhas begun to illuminate the abstract causal mechanisms shaping LM behavior. To\nhelp bring these strands of research closer together, we introduce CausalGym.\nWe adapt and expand the SyntaxGym suite of tasks to benchmark the ability of\ninterpretability methods to causally affect model behaviour. To illustrate how\nCausalGym can be used, we study the pythia models (14M--6.9B) and assess the\ncausal efficacy of a wide range of interpretability methods, including linear\nprobing and distributed alignment search (DAS). We find that DAS outperforms\nthe other methods, and so we use it to study the learning trajectory of two\ndifficult linguistic phenomena in pythia-1b: negative polarity item licensing\nand filler--gap dependencies. Our analysis shows that the mechanism\nimplementing both of these tasks is learned in discrete stages, not gradually.",
      "tldr_zh": "本研究引入 CausalGym，一种基准测试框架，用于评估可解释性方法对语言模型 (LMs) 行为的因果影响，通过改编和扩展 SyntaxGym 任务套件。该框架聚焦于测试方法如线性探测和分布式对齐搜索 (DAS)，并应用于 pythia 模型系列（从 14M 到 6.9B 参数）。结果显示，DAS 方法在性能上优于其他方法，并通过分析 pythia-1b 模型发现，否定极性项许可和填空-间隙依赖等语言现象的学习轨迹是离散阶段式的，而非渐进的。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "I.2.7"
      ],
      "primary_category": "cs.CL",
      "comment": "9 pages main text, 26 pages total",
      "pdf_url": "http://arxiv.org/pdf/2402.12560v1",
      "published_date": "2024-02-19 21:35:56 UTC",
      "updated_date": "2024-02-19 21:35:56 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T07:38:50.647918"
    },
    {
      "arxiv_id": "2402.12551v1",
      "title": "Landmark-based Localization using Stereo Vision and Deep Learning in GPS-Denied Battlefield Environment",
      "title_zh": "翻译失败",
      "authors": [
        "Ganesh Sapkota",
        "Sanjay Madria"
      ],
      "abstract": "Localization in a battlefield environment is increasingly challenging as GPS\nconnectivity is often denied or unreliable, and physical deployment of anchor\nnodes across wireless networks for localization can be difficult in hostile\nbattlefield terrain. Existing range-free localization methods rely on\nradio-based anchors and their average hop distance which suffers from accuracy\nand stability in dynamic and sparse wireless network topology. Vision-based\nmethods like SLAM and Visual Odometry use expensive sensor fusion techniques\nfor map generation and pose estimation. This paper proposes a novel framework\nfor localization in non-GPS battlefield environments using only the passive\ncamera sensors and considering naturally existing or artificial landmarks as\nanchors. The proposed method utilizes a customcalibrated stereo vision camera\nfor distance estimation and the YOLOv8s model, which is trained and fine-tuned\nwith our real-world dataset for landmark recognition. The depth images are\ngenerated using an efficient stereomatching algorithm, and distances to\nlandmarks are determined by extracting the landmark depth feature utilizing a\nbounding box predicted by the landmark recognition model. The position of the\nunknown node is then obtained using the efficient least square algorithm and\nthen optimized using the L-BFGS-B (limited-memory quasi-Newton code for\nbound-constrained optimization) method. Experimental results demonstrate that\nour proposed framework performs better than existing anchorbased DV-Hop\nalgorithms and competes with the most efficient vision-based algorithms in\nterms of localization error (RMSE).",
      "tldr_zh": "这篇论文针对 GPS 不可用的战场环境，提出了一种基于地标的地标定位框架，使用 Stereo Vision 和 Deep Learning 进行精确定位。方法包括自定义校准的立体视觉摄像头进行距离估计、训练 YOLOv8s 模型识别地标、利用立体匹配算法生成深度图像并提取边界框特征计算距离，然后通过最小二乘算法结合 L-BFGS-B 优化方法确定未知节点位置。实验结果显示，该框架在定位误差 (RMSE) 上优于传统 DV-Hop 算法，并与高效视觉算法（如 SLAM 和 Visual Odometry）相当。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "arXiv admin note: text overlap with arXiv:2402.12320",
      "pdf_url": "http://arxiv.org/pdf/2402.12551v1",
      "published_date": "2024-02-19 21:20:56 UTC",
      "updated_date": "2024-02-19 21:20:56 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T07:39:03.832048"
    },
    {
      "arxiv_id": "2402.12530v1",
      "title": "Parallel Structures in Pre-training Data Yield In-Context Learning",
      "title_zh": "预训练数据中的平行结构产生上下文学习",
      "authors": [
        "Yanda Chen",
        "Chen Zhao",
        "Zhou Yu",
        "Kathleen McKeown",
        "He He"
      ],
      "abstract": "Pre-trained language models (LMs) are capable of in-context learning (ICL):\nthey can adapt to a task with only a few examples given in the prompt without\nany parameter update. However, it is unclear where this capability comes from\nas there is a stark distribution shift between pre-training text and ICL\nprompts. In this work, we study what patterns of the pre-training data\ncontribute to ICL. We find that LMs' ICL ability depends on $\\textit{parallel\nstructures}$ in the pre-training data -- pairs of phrases following similar\ntemplates in the same context window. Specifically, we detect parallel\nstructures by checking whether training on one phrase improves prediction of\nthe other, and conduct ablation experiments to study their effect on ICL. We\nshow that removing parallel structures in the pre-training data reduces LMs'\nICL accuracy by 51% (vs 2% from random ablation). This drop persists even when\nexcluding common patterns such as n-gram repetitions and long-range dependency,\nshowing the diversity and generality of parallel structures. A closer look at\nthe detected parallel structures indicates that they cover diverse linguistic\ntasks and span long distances in the data.",
      "tldr_zh": "本研究探讨了预训练语言模型 (LMs) 的 in-context learning (ICL) 能力，即模型能在提示中通过少数示例适应任务，而无需参数更新。研究发现，这种能力主要源于预训练数据中的 parallel structures，即在同一上下文窗口中跟随类似模板的短语对，通过检测一个短语的训练是否改善另一个短语的预测来识别这些结构。消融实验显示，移除这些结构会使 ICL 准确率下降 51%，而随机消融仅下降 2%，且这种影响在排除 n-gram 重复和长距离依赖后依然存在，表明 parallel structures 的多样性、普遍性和覆盖多种语言任务的能力。总的来说，该工作揭示了预训练数据模式对 ICL 的关键作用，为优化语言模型提供了新见解。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2402.12530v1",
      "published_date": "2024-02-19 20:40:48 UTC",
      "updated_date": "2024-02-19 20:40:48 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T07:39:16.254510"
    },
    {
      "arxiv_id": "2402.12527v2",
      "title": "The Edge-of-Reach Problem in Offline Model-Based Reinforcement Learning",
      "title_zh": "翻译失败",
      "authors": [
        "Anya Sims",
        "Cong Lu",
        "Jakob Foerster",
        "Yee Whye Teh"
      ],
      "abstract": "Offline reinforcement learning aims to train agents from pre-collected\ndatasets. However, this comes with the added challenge of estimating the value\nof behaviors not covered in the dataset. Model-based methods offer a potential\nsolution by training an approximate dynamics model, which then allows\ncollection of additional synthetic data via rollouts in this model. The\nprevailing theory treats this approach as online RL in an approximate dynamics\nmodel, and any remaining performance gap is therefore understood as being due\nto dynamics model errors. In this paper, we analyze this assumption and\ninvestigate how popular algorithms perform as the learned dynamics model is\nimproved. In contrast to both intuition and theory, if the learned dynamics\nmodel is replaced by the true error-free dynamics, existing model-based methods\ncompletely fail. This reveals a key oversight: The theoretical foundations\nassume sampling of full horizon rollouts in the learned dynamics model;\nhowever, in practice, the number of model-rollout steps is aggressively reduced\nto prevent accumulating errors. We show that this truncation of rollouts\nresults in a set of edge-of-reach states at which we are effectively\n``bootstrapping from the void.'' This triggers pathological value\noverestimation and complete performance collapse. We term this the\nedge-of-reach problem. Based on this new insight, we fill important gaps in\nexisting theory, and reveal how prior model-based methods are primarily\naddressing the edge-of-reach problem, rather than model-inaccuracy as claimed.\nFinally, we propose Reach-Aware Value Learning (RAVL), a simple and robust\nmethod that directly addresses the edge-of-reach problem and hence - unlike\nexisting methods - does not fail as the dynamics model is improved. Code\nopen-sourced at: github.com/anyasims/edge-of-reach.",
      "tldr_zh": "本论文探讨了离线强化学习（Offline Reinforcement Learning）中基于模型的方法所面临的 Edge-of-Reach 问题，即在截断 rollout 时，导致代理从无效状态 bootstrapping，引发价值过度估计和性能崩溃。作者分析发现，现有的模型-based 算法即使使用准确的动态模型也会失败，因为理论假设的完整 rollout 在实践中被限制以避免错误积累，从而暴露了这一关键问题。论文提供了新的理论洞见，并提出了 Reach-Aware Value Learning (RAVL)，一个简单鲁棒的方法，直接解决 Edge-of-Reach 问题，确保算法在动态模型改进时保持有效性。代码已在 GitHub 开源。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "Code open-sourced at: https://github.com/anyasims/edge-of-reach",
      "pdf_url": "http://arxiv.org/pdf/2402.12527v2",
      "published_date": "2024-02-19 20:38:00 UTC",
      "updated_date": "2024-11-29 19:52:18 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T07:39:27.384756"
    },
    {
      "arxiv_id": "2402.12525v1",
      "title": "LangXAI: Integrating Large Vision Models for Generating Textual Explanations to Enhance Explainability in Visual Perception Tasks",
      "title_zh": "LangXAI：整合大型视觉模型以生成文本解释，提升视觉感知任务的可解释性",
      "authors": [
        "Truong Thanh Hung Nguyen",
        "Tobias Clement",
        "Phuc Truong Loc Nguyen",
        "Nils Kemmerzell",
        "Van Binh Truong",
        "Vo Thanh Khang Nguyen",
        "Mohamed Abdelaal",
        "Hung Cao"
      ],
      "abstract": "LangXAI is a framework that integrates Explainable Artificial Intelligence\n(XAI) with advanced vision models to generate textual explanations for visual\nrecognition tasks. Despite XAI advancements, an understanding gap persists for\nend-users with limited domain knowledge in artificial intelligence and computer\nvision. LangXAI addresses this by furnishing text-based explanations for\nclassification, object detection, and semantic segmentation model outputs to\nend-users. Preliminary results demonstrate LangXAI's enhanced plausibility,\nwith high BERTScore across tasks, fostering a more transparent and reliable AI\nframework on vision tasks for end-users.",
      "tldr_zh": "该论文提出 LangXAI 框架，通过整合可解释人工智能 (XAI) 和大型视觉模型，为视觉感知任务生成文本解释，以解决终端用户在人工智能和计算机视觉领域知识有限的理解差距。LangXAI 针对分类、物体检测和语义分割等任务输出，提供基于文本的解释，帮助用户更好地理解模型决策。初步实验结果显示，该框架显著提升了解释的可信度 (plausibility)，并在多任务上实现了高 BERTScore，从而构建更透明可靠的 AI 系统。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2402.12525v1",
      "published_date": "2024-02-19 20:36:32 UTC",
      "updated_date": "2024-02-19 20:36:32 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T07:39:39.950161"
    },
    {
      "arxiv_id": "2402.12518v2",
      "title": "Gaussian Process Neural Additive Models",
      "title_zh": "翻译失败",
      "authors": [
        "Wei Zhang",
        "Brian Barr",
        "John Paisley"
      ],
      "abstract": "Deep neural networks have revolutionized many fields, but their black-box\nnature also occasionally prevents their wider adoption in fields such as\nhealthcare and finance, where interpretable and explainable models are\nrequired. The recent development of Neural Additive Models (NAMs) is a\nsignificant step in the direction of interpretable deep learning for tabular\ndatasets. In this paper, we propose a new subclass of NAMs that use a\nsingle-layer neural network construction of the Gaussian process via random\nFourier features, which we call Gaussian Process Neural Additive Models\n(GP-NAM). GP-NAMs have the advantage of a convex objective function and number\nof trainable parameters that grows linearly with feature dimensionality. It\nsuffers no loss in performance compared to deeper NAM approaches because GPs\nare well-suited for learning complex non-parametric univariate functions. We\ndemonstrate the performance of GP-NAM on several tabular datasets, showing that\nit achieves comparable or better performance in both classification and\nregression tasks with a large reduction in the number of parameters.",
      "tldr_zh": "本文提出 Gaussian Process Neural Additive Models (GP-NAM)，一种可解释的深度学习模型，用于处理表格数据集中的分类和回归任务，以解决传统深度神经网络的黑盒问题。GP-NAM 通过随机傅里叶特征构建单层神经网络的高斯过程（Gaussian process），实现凸目标函数和参数数量随特征维度线性增长的优势。相比更深层的 Neural Additive Models (NAMs)，GP-NAM 在学习复杂非参数单变量函数方面不损失性能，并在多个数据集上实验中表现出相当或更好的准确性，同时大幅减少参数数量。总的来说，这一方法为可解释模型在医疗和金融等领域提供了高效替代方案。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "Appears at AAAI 2024",
      "pdf_url": "http://arxiv.org/pdf/2402.12518v2",
      "published_date": "2024-02-19 20:29:34 UTC",
      "updated_date": "2024-03-19 15:38:29 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T07:39:51.130885"
    },
    {
      "arxiv_id": "2402.12499v4",
      "title": "Automated Security Response through Online Learning with Adaptive Conjectures",
      "title_zh": "通过自适应假设的在线学习实现自动安全响应",
      "authors": [
        "Kim Hammar",
        "Tao Li",
        "Rolf Stadler",
        "Quanyan Zhu"
      ],
      "abstract": "We study automated security response for an IT infrastructure and formulate\nthe interaction between an attacker and a defender as a partially observed,\nnon-stationary game. We relax the standard assumption that the game model is\ncorrectly specified and consider that each player has a probabilistic\nconjecture about the model, which may be misspecified in the sense that the\ntrue model has probability 0. This formulation allows us to capture uncertainty\nand misconception about the infrastructure and the intents of the players. To\nlearn effective game strategies online, we design Conjectural Online Learning\n(COL), a novel method where a player iteratively adapts its conjecture using\nBayesian learning and updates its strategy through rollout. We prove that the\nconjectures converge to best fits, and we provide a bound on the performance\nimprovement that rollout enables with a conjectured model. To characterize the\nsteady state of the game, we propose a variant of the Berk-Nash equilibrium. We\npresent COL through an advanced persistent threat use case. Testbed evaluations\nshow that COL produces effective security strategies that adapt to a changing\nenvironment. We also find that COL enables faster convergence than current\nreinforcement learning techniques.",
      "tldr_zh": "这篇论文研究了 IT 基础设施的自动化安全响应，将攻击者和防御者之间的互动建模为部分观察、非平稳游戏，并考虑玩家可能错误的概率假设（conjectures）。他们提出了 Conjectural Online Learning (COL) 方法，通过 Bayesian learning 迭代适应假设，并使用 rollout 更新策略，以在线学习有效游戏策略。论文证明了假设会收敛到最佳拟合，并提供了 rollout 的性能改进界，同时引入了 Berk-Nash equilibrium 的变体来描述游戏稳态。实验结果显示，COL 在高级持续威胁（Advanced Persistent Threat）用例中产生了适应性强的安全策略，并比当前强化学习技术实现更快收敛。",
      "categories": [
        "cs.GT",
        "cs.AI",
        "cs.CR",
        "cs.LG",
        "cs.SY",
        "eess.SY"
      ],
      "primary_category": "cs.GT",
      "comment": "This work has been submitted to the IEEE for possible publication",
      "pdf_url": "http://arxiv.org/pdf/2402.12499v4",
      "published_date": "2024-02-19 20:06:15 UTC",
      "updated_date": "2025-01-05 11:16:19 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T07:40:04.038875"
    },
    {
      "arxiv_id": "2402.15524v1",
      "title": "Graph Pruning for Enumeration of Minimal Unsatisfiable Subsets",
      "title_zh": "翻译失败",
      "authors": [
        "Panagiotis Lymperopoulos",
        "Liping Liu"
      ],
      "abstract": "Finding Minimal Unsatisfiable Subsets (MUSes) of binary constraints is a\ncommon problem in infeasibility analysis of over-constrained systems. However,\nbecause of the exponential search space of the problem, enumerating MUSes is\nextremely time-consuming in real applications. In this work, we propose to\nprune formulas using a learned model to speed up MUS enumeration. We represent\nformulas as graphs and then develop a graph-based learning model to predict\nwhich part of the formula should be pruned. Importantly, our algorithm does not\nrequire data labeling by only checking the satisfiability of pruned formulas.\nIt does not even require training data from the target application because it\nextrapolates to data with different distributions. In our experiments we\ncombine our algorithm with existing MUS enumerators and validate its\neffectiveness in multiple benchmarks including a set of real-world problems\noutside our training distribution. The experiment results show that our method\nsignificantly accelerates MUS enumeration on average on these benchmark\nproblems.",
      "tldr_zh": "本研究针对二元约束的最小不可满足子集 (MUSes) 枚举问题，提出了一种图修剪方法，利用基于图的学习模型预测并移除公式中的无关部分，以加速搜索过程。该算法无需数据标注，仅通过检查修剪后公式的可满足性即可运行，且能外推到不同数据分布的场景中。实验结果显示，在多个基准测试（包括训练分布外的真实世界问题）中，该方法与现有 MUS 枚举器结合后，平均显著提高了枚举效率。",
      "categories": [
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2402.15524v1",
      "published_date": "2024-02-19 20:03:45 UTC",
      "updated_date": "2024-02-19 20:03:45 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T07:40:16.848795"
    },
    {
      "arxiv_id": "2402.12490v1",
      "title": "Towards Cross-Domain Continual Learning",
      "title_zh": "翻译失败",
      "authors": [
        "Marcus de Carvalho",
        "Mahardhika Pratama",
        "Jie Zhang",
        "Chua Haoyan",
        "Edward Yapp"
      ],
      "abstract": "Continual learning is a process that involves training learning agents to\nsequentially master a stream of tasks or classes without revisiting past data.\nThe challenge lies in leveraging previously acquired knowledge to learn new\ntasks efficiently, while avoiding catastrophic forgetting. Existing methods\nprimarily focus on single domains, restricting their applicability to specific\nproblems.\n  In this work, we introduce a novel approach called Cross-Domain Continual\nLearning (CDCL) that addresses the limitations of being limited to single\nsupervised domains. Our method combines inter- and intra-task cross-attention\nmechanisms within a compact convolutional network. This integration enables the\nmodel to maintain alignment with features from previous tasks, thereby delaying\nthe data drift that may occur between tasks, while performing unsupervised\ncross-domain (UDA) between related domains. By leveraging an\nintra-task-specific pseudo-labeling method, we ensure accurate input pairs for\nboth labeled and unlabeled samples, enhancing the learning process. To validate\nour approach, we conduct extensive experiments on public UDA datasets,\nshowcasing its positive performance on cross-domain continual learning\nchallenges. Additionally, our work introduces incremental ideas that contribute\nto the advancement of this field.\n  We make our code and models available to encourage further exploration and\nreproduction of our results: \\url{https://github.com/Ivsucram/CDCL}",
      "tldr_zh": "本文针对传统持续学习（Continual Learning）局限于单一领域的限制，提出了一种新型方法Cross-Domain Continual Learning (CDCL)，通过整合inter- and intra-task cross-attention机制于紧凑卷积网络中，实现任务间特征对齐并延迟数据漂移，同时结合无监督域适应（UDA）和intra-task-specific pseudo-labeling方法，提升对标记和未标记样本的学习效率。实验在公共UDA数据集上验证了CDCL的有效性，展示了其在跨域持续学习挑战中的显著性能提升。总体而言，该工作引入了增量创新，推动了该领域的进展，并开源了代码以促进进一步研究。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.LG",
      "comment": "12 pages, 2 Figures, 4 Tables. To be published at the IEEE\n  International Conference on Data Engineering (ICDE) 2024",
      "pdf_url": "http://arxiv.org/pdf/2402.12490v1",
      "published_date": "2024-02-19 19:54:03 UTC",
      "updated_date": "2024-02-19 19:54:03 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T07:40:28.078207"
    },
    {
      "arxiv_id": "2402.14849v1",
      "title": "Asynchronous and Segmented Bidirectional Encoding for NMT",
      "title_zh": "翻译失败",
      "authors": [
        "Jingpu Yang",
        "Zehua Han",
        "Mengyu Xiang",
        "Helin Wang",
        "Yuxiao Huang",
        "Miao Fang"
      ],
      "abstract": "With the rapid advancement of Neural Machine Translation (NMT), enhancing\ntranslation efficiency and quality has become a focal point of research.\nDespite the commendable performance of general models such as the Transformer\nin various aspects, they still fall short in processing long sentences and\nfully leveraging bidirectional contextual information. This paper introduces an\nimproved model based on the Transformer, implementing an asynchronous and\nsegmented bidirectional decoding strategy aimed at elevating translation\nefficiency and accuracy. Compared to traditional unidirectional translations\nfrom left-to-right or right-to-left, our method demonstrates heightened\nefficiency and improved translation quality, particularly in handling long\nsentences. Experimental results on the IWSLT2017 dataset confirm the\neffectiveness of our approach in accelerating translation and increasing\naccuracy, especially surpassing traditional unidirectional strategies in long\nsentence translation. Furthermore, this study analyzes the impact of sentence\nlength on decoding outcomes and explores the model's performance in various\nscenarios. The findings of this research not only provide an effective encoding\nstrategy for the NMT field but also pave new avenues and directions for future\nstudies.",
      "tldr_zh": "本研究针对 Neural Machine Translation (NMT) 的效率和质量问题，提出了一种基于 Transformer 的改进模型，采用 Asynchronous and Segmented Bidirectional Encoding 策略，以更好地处理长句子并充分利用双向上下文信息。相比传统单向翻译（如左到右或右到左），该方法显著提升了翻译速度和准确性，尤其在长句子场景中表现出色。实验结果在 IWSLT2017 数据集上证明，该策略加速了翻译过程并提高了准确率，同时分析了句子长度对解码性能的影响。该研究为 NMT 领域提供了有效的编码策略，并为未来研究开辟新路径。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2402.14849v1",
      "published_date": "2024-02-19 19:48:02 UTC",
      "updated_date": "2024-02-19 19:48:02 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T07:40:42.516116"
    },
    {
      "arxiv_id": "2402.12479v3",
      "title": "In value-based deep reinforcement learning, a pruned network is a good network",
      "title_zh": "翻译失败",
      "authors": [
        "Johan Obando-Ceron",
        "Aaron Courville",
        "Pablo Samuel Castro"
      ],
      "abstract": "Recent work has shown that deep reinforcement learning agents have difficulty\nin effectively using their network parameters. We leverage prior insights into\nthe advantages of sparse training techniques and demonstrate that gradual\nmagnitude pruning enables value-based agents to maximize parameter\neffectiveness. This results in networks that yield dramatic performance\nimprovements over traditional networks, using only a small fraction of the full\nnetwork parameters.",
      "tldr_zh": "该研究探讨了基于价值的深度强化学习（deep reinforcement learning）中，网络参数利用效率低下的问题。作者利用稀疏训练技术（sparse training techniques），特别是渐进幅度修剪（gradual magnitude pruning），使代理能够最大化参数有效性。结果显示，这种方法仅使用少量网络参数，便实现了与传统网络相比的显著性能提升。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2402.12479v3",
      "published_date": "2024-02-19 19:34:07 UTC",
      "updated_date": "2024-06-25 13:10:06 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T07:40:50.317712"
    },
    {
      "arxiv_id": "2402.13292v1",
      "title": "A Conflict-Aware Optimal Goal Assignment Algorithm for Multi-Robot Systems",
      "title_zh": "翻译失败",
      "authors": [
        "Aakash",
        "Indranil Saha"
      ],
      "abstract": "The fundamental goal assignment problem for a multi-robot application aims to\nassign a unique goal to each robot while ensuring collision-free paths,\nminimizing the total movement cost. A plausible algorithmic solution to this\nNP-hard problem involves an iterative process that integrates a task planner to\ncompute the goal assignment while ignoring the collision possibilities among\nthe robots and a multi-agent path-finding algorithm to find the collision-free\ntrajectories for a given assignment. This procedure involves a method for\ncomputing the next best assignment given the current best assignment. A naive\nway of computing the next best assignment, as done in the state-of-the-art\nsolutions, becomes a roadblock to achieving scalability in solving the overall\nproblem. To obviate this bottleneck, we propose an efficient conflict-guided\nmethod to compute the next best assignment. Additionally, we introduce two more\noptimizations to the algorithm -- first for avoiding the unconstrained path\ncomputations between robot-goal pairs wherever possible, and the second to\nprevent duplicate constrained path computations for multiple robot-goal pairs.\nWe extensively evaluate our algorithm for up to a hundred robots on several\nbenchmark workspaces. The results demonstrate that the proposed algorithm\nachieves nearly an order of magnitude speedup over the state-of-the-art\nalgorithm, showcasing its efficacy in real-world scenarios.",
      "tldr_zh": "该论文针对多机器人系统中的目标分配问题（NP-hard问题），提出了一种冲突感知的最优算法，该算法通过迭代过程结合任务规划器和多代理路径查找(multi-agent path-finding)来为每个机器人分配唯一目标，同时确保无碰撞路径并最小化总移动成本。核心创新包括一个高效的冲突引导方法，用于快速计算下一个最佳分配，以及两个优化措施：避免不必要的机器人-目标对路径计算和防止重复的约束路径计算。实验结果显示，该算法在多达100个机器人的基准工作空间上，比现有状态-of-the-art算法快近一个数量级，显著提升了实际应用中的可扩展性。",
      "categories": [
        "cs.MA",
        "cs.AI",
        "cs.RO"
      ],
      "primary_category": "cs.MA",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2402.13292v1",
      "published_date": "2024-02-19 19:04:19 UTC",
      "updated_date": "2024-02-19 19:04:19 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T07:41:03.119580"
    },
    {
      "arxiv_id": "2402.12451v2",
      "title": "The Revolution of Multimodal Large Language Models: A Survey",
      "title_zh": "翻译失败",
      "authors": [
        "Davide Caffagni",
        "Federico Cocchi",
        "Luca Barsellotti",
        "Nicholas Moratelli",
        "Sara Sarto",
        "Lorenzo Baraldi",
        "Lorenzo Baraldi",
        "Marcella Cornia",
        "Rita Cucchiara"
      ],
      "abstract": "Connecting text and visual modalities plays an essential role in generative\nintelligence. For this reason, inspired by the success of large language\nmodels, significant research efforts are being devoted to the development of\nMultimodal Large Language Models (MLLMs). These models can seamlessly integrate\nvisual and textual modalities, while providing a dialogue-based interface and\ninstruction-following capabilities. In this paper, we provide a comprehensive\nreview of recent visual-based MLLMs, analyzing their architectural choices,\nmultimodal alignment strategies, and training techniques. We also conduct a\ndetailed analysis of these models across a wide range of tasks, including\nvisual grounding, image generation and editing, visual understanding, and\ndomain-specific applications. Additionally, we compile and describe training\ndatasets and evaluation benchmarks, conducting comparisons among existing\nmodels in terms of performance and computational requirements. Overall, this\nsurvey offers a comprehensive overview of the current state of the art, laying\nthe groundwork for future MLLMs.",
      "tldr_zh": "这篇论文对Multimodal Large Language Models (MLLMs)的发展进行全面调查，强调了将文本和视觉模态整合在生成智能中的关键作用。作者分析了现有MLLMs的架构选择、多模态对齐策略以及训练技术，并评估了这些模型在视觉 grounding、图像生成与编辑、视觉理解以及特定领域应用等任务上的性能。同时，论文编译了相关训练数据集和评估基准，并比较了不同模型的性能和计算需求。总体上，这为MLLMs的未来研究提供了坚实基础。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CL",
        "cs.MM"
      ],
      "primary_category": "cs.CV",
      "comment": "ACL 2024 (Findings)",
      "pdf_url": "http://arxiv.org/pdf/2402.12451v2",
      "published_date": "2024-02-19 19:01:01 UTC",
      "updated_date": "2024-06-06 16:13:43 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T07:41:15.114754"
    },
    {
      "arxiv_id": "2402.12373v2",
      "title": "LTL learning on GPUs",
      "title_zh": "翻译失败",
      "authors": [
        "Mojtaba Valizadeh",
        "Nathanaël Fijalkow",
        "Martin Berger"
      ],
      "abstract": "Linear temporal logic (LTL) is widely used in industrial verification. LTL\nformulae can be learned from traces. Scaling LTL formula learning is an open\nproblem. We implement the first GPU-based LTL learner using a novel form of\nenumerative program synthesis. The learner is sound and complete. Our\nbenchmarks indicate that it handles traces at least 2048 times more numerous,\nand on average at least 46 times faster than existing state-of-the-art\nlearners. This is achieved with, among others, novel branch-free LTL semantics\nthat has $O(\\log n)$ time complexity, where $n$ is trace length, while previous\nimplementations are $O(n^2)$ or worse (assuming bitwise boolean operations and\nshifts by powers of 2 have unit costs -- a realistic assumption on modern\nprocessors).",
      "tldr_zh": "这篇论文提出了一种基于 GPU 的 LTL (Linear Temporal Logic) 公式学习方法，使用新型枚举程序 synthesis 技术，实现首个 sound 和 complete 的 LTL 学习器。相比现有方法，该框架通过引入 branch-free LTL 语义，将时间复杂度从 O(n^2) 优化到 O(log n)，显著提升了处理 traces 的效率。基准测试显示，该学习器能处理至少 2048 倍的 traces，并平均比最先进学习器快 46 倍，为工业验证中的 LTL 应用提供了可扩展性。",
      "categories": [
        "cs.PL",
        "cs.AI",
        "68",
        "D.3"
      ],
      "primary_category": "cs.PL",
      "comment": "27 pages",
      "pdf_url": "http://arxiv.org/pdf/2402.12373v2",
      "published_date": "2024-02-19 18:58:26 UTC",
      "updated_date": "2024-03-27 20:00:00 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T07:41:28.973560"
    },
    {
      "arxiv_id": "2402.12370v2",
      "title": "AnaloBench: Benchmarking the Identification of Abstract and Long-context Analogies",
      "title_zh": "AnaloBench：抽象和长上下文类比识别的基准测试",
      "authors": [
        "Xiao Ye",
        "Andrew Wang",
        "Jacob Choi",
        "Yining Lu",
        "Shreya Sharma",
        "Lingfeng Shen",
        "Vijay Tiyyala",
        "Nicholas Andrews",
        "Daniel Khashabi"
      ],
      "abstract": "Humans regularly engage in analogical thinking, relating personal experiences\nto current situations (X is analogous to Y because of Z). Analogical thinking\nallows humans to solve problems in creative ways, grasp difficult concepts, and\narticulate ideas more effectively. Can language models (LMs) do the same? To\nanswer this question, we propose AnaloBench, a benchmark to determine\nanalogical reasoning ability in LMs. Our benchmarking approach focuses on\naspects of this ability that are common among humans: (i) recalling related\nexperiences from a large amount of information, and (ii) applying analogical\nreasoning to complex and lengthy scenarios. We test a broad collection of\nproprietary models (e.g., GPT family, Claude V2) and open source models such as\nLLaMA2. As in prior results, scaling up LMs results in some performance boosts.\nSurprisingly, scale offers minimal gains when, (i) analogies involve lengthy\nscenarios, or (ii) recalling relevant scenarios from a large pool of\ninformation, a process analogous to finding a needle in a haystack. We hope\nthese observations encourage further research in this field.",
      "tldr_zh": "本研究提出 AnaloBench，这是一个基准测试，用于评估语言模型 (LMs) 的类比推理能力，模拟人类在处理抽象类比时的表现，如从大量信息中回忆相关经历并应用于复杂场景。研究测试了多种模型，包括 GPT family、Claude V2 和开源模型如 LLaMA2，结果显示模型规模的扩大能带来一定性能提升，但对涉及长上下文类比或从海量信息中提取相关场景的任务，收益有限。总体而言，此基准测试揭示了 LMs 在类比思考中的局限性，并呼吁进一步研究以提升模型的创意问题解决能力。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "Accepted to EMNLP 2024 (Main)",
      "pdf_url": "http://arxiv.org/pdf/2402.12370v2",
      "published_date": "2024-02-19 18:56:44 UTC",
      "updated_date": "2024-10-03 22:55:11 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T07:41:39.810018"
    },
    {
      "arxiv_id": "2402.12366v1",
      "title": "A Critical Evaluation of AI Feedback for Aligning Large Language Models",
      "title_zh": "翻译失败",
      "authors": [
        "Archit Sharma",
        "Sedrick Keh",
        "Eric Mitchell",
        "Chelsea Finn",
        "Kushal Arora",
        "Thomas Kollar"
      ],
      "abstract": "Reinforcement learning with AI feedback (RLAIF) is a popular paradigm for\nimproving the instruction-following abilities of powerful pre-trained language\nmodels. RLAIF first performs supervised fine-tuning (SFT) using demonstrations\nfrom a teacher model and then further fine-tunes the model with reinforcement\nlearning (RL), using feedback from a critic model. While recent popular\nopen-source models have demonstrated substantial improvements in performance\nfrom the RL step, in this paper we question whether the complexity of this RL\nstep is truly warranted for AI feedback. We show that the improvements of the\nRL step are virtually entirely due to the widespread practice of using a weaker\nteacher model (e.g. GPT-3.5) for SFT data collection than the critic (e.g.,\nGPT-4) used for AI feedback generation. Specifically, we show that simple\nsupervised fine-tuning with GPT-4 as the teacher outperforms existing RLAIF\npipelines. More generally, we find that the gains from RLAIF vary substantially\nacross base model families, test-time evaluation protocols, and critic models.\nFinally, we provide a mechanistic explanation for when SFT may outperform the\nfull two-step RLAIF pipeline as well as suggestions for making RLAIF maximally\nuseful in practice.",
      "tldr_zh": "这篇论文对使用AI反馈的强化学习（RLAIF）方法进行了批判性评估，该方法旨在提升大型语言模型的指令遵循能力。作者发现，RLAIF的改进主要源于使用较弱的教师模型（如GPT-3.5）进行监督微调（SFT）数据收集，而非RL步骤本身；具体而言，直接用更强的批评者模型（如GPT-4）进行SFT即可超越现有的RLAIF管道。研究还显示，RLAIF的收益因基础模型家族、测试协议和批评者模型而异，并提供了机制解释及实用建议，以优化RLAIF在实际应用中的效果。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2402.12366v1",
      "published_date": "2024-02-19 18:53:54 UTC",
      "updated_date": "2024-02-19 18:53:54 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T07:41:52.543222"
    },
    {
      "arxiv_id": "2402.12365v5",
      "title": "Universal Physics Transformers: A Framework For Efficiently Scaling Neural Operators",
      "title_zh": "翻译失败",
      "authors": [
        "Benedikt Alkin",
        "Andreas Fürst",
        "Simon Schmid",
        "Lukas Gruber",
        "Markus Holzleitner",
        "Johannes Brandstetter"
      ],
      "abstract": "Neural operators, serving as physics surrogate models, have recently gained\nincreased interest. With ever increasing problem complexity, the natural\nquestion arises: what is an efficient way to scale neural operators to larger\nand more complex simulations - most importantly by taking into account\ndifferent types of simulation datasets. This is of special interest since, akin\nto their numerical counterparts, different techniques are used across\napplications, even if the underlying dynamics of the systems are similar.\nWhereas the flexibility of transformers has enabled unified architectures\nacross domains, neural operators mostly follow a problem specific design, where\nGNNs are commonly used for Lagrangian simulations and grid-based models\npredominate Eulerian simulations. We introduce Universal Physics Transformers\n(UPTs), an efficient and unified learning paradigm for a wide range of\nspatio-temporal problems. UPTs operate without grid- or particle-based latent\nstructures, enabling flexibility and scalability across meshes and particles.\nUPTs efficiently propagate dynamics in the latent space, emphasized by inverse\nencoding and decoding techniques. Finally, UPTs allow for queries of the latent\nspace representation at any point in space-time. We demonstrate diverse\napplicability and efficacy of UPTs in mesh-based fluid simulations, and\nsteady-state Reynolds averaged Navier-Stokes simulations, and Lagrangian-based\ndynamics.",
      "tldr_zh": "这篇论文提出了Universal Physics Transformers (UPTs)，一个高效统一的框架，用于扩展neural operators以处理复杂时空模拟问题，特别是适应不同类型的模拟数据集，如Lagrangian simulations和Eulerian simulations。UPTs 避免依赖网格或粒子-based的潜在结构，通过inverse encoding和decoding技术在潜在空间高效传播动态，并支持任意时空点的查询。实验结果显示，UPTs 在基于网格的流体模拟、稳态Reynolds averaged Navier-Stokes模拟以及Lagrangian-based动态中表现出色，提高了模型的灵活性和可扩展性。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "physics.flu-dyn"
      ],
      "primary_category": "cs.LG",
      "comment": "Published at NeurIPS 2024, Github: https://ml-jku.github.io/UPT/",
      "pdf_url": "http://arxiv.org/pdf/2402.12365v5",
      "published_date": "2024-02-19 18:52:13 UTC",
      "updated_date": "2025-02-27 10:24:17 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T07:42:04.719966"
    },
    {
      "arxiv_id": "2402.12360v1",
      "title": "Nonlinear Discrete-Time Observers with Physics-Informed Neural Networks",
      "title_zh": "基于物理信息神经网络的非线性离散时间观察器",
      "authors": [
        "Hector Vargas Alvarez",
        "Gianluca Fabiani",
        "Ioannis G. Kevrekidis",
        "Nikolaos Kazantzis",
        "Constantinos Siettos"
      ],
      "abstract": "We use Physics-Informed Neural Networks (PINNs) to solve the discrete-time\nnonlinear observer state estimation problem. Integrated within a single-step\nexact observer linearization framework, the proposed PINN approach aims at\nlearning a nonlinear state transformation map by solving a system of\ninhomogeneous functional equations. The performance of the proposed PINN\napproach is assessed via two illustrative case studies for which the observer\nlinearizing transformation map can be derived analytically. We also perform an\nuncertainty quantification analysis for the proposed PINN scheme and we compare\nit with conventional power-series numerical implementations, which rely on the\ncomputation of a power series solution.",
      "tldr_zh": "本文提出了一种使用Physics-Informed Neural Networks (PINNs)来解决离散时间非线性观测器状态估计问题的方法，通过整合单步精确观测器线性化框架，学习非线性状态变换映射并解决非均匀函数方程系统。实验通过两个示例案例研究验证了该方法的性能，并进行了不确定性量化分析，结果显示其与传统的幂级数数值实现相比具有潜在优势。总体上，该方法为非线性系统状态估计提供了更有效的学习框架。",
      "categories": [
        "math.NA",
        "cs.AI",
        "cs.NA",
        "math.DS",
        "37N30, 68T05, 93C55, 65D15"
      ],
      "primary_category": "math.NA",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2402.12360v1",
      "published_date": "2024-02-19 18:47:56 UTC",
      "updated_date": "2024-02-19 18:47:56 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T07:42:14.798222"
    },
    {
      "arxiv_id": "2402.12354v2",
      "title": "LoRA+: Efficient Low Rank Adaptation of Large Models",
      "title_zh": "翻译失败",
      "authors": [
        "Soufiane Hayou",
        "Nikhil Ghosh",
        "Bin Yu"
      ],
      "abstract": "In this paper, we show that Low Rank Adaptation (LoRA) as originally\nintroduced in Hu et al. (2021) leads to suboptimal finetuning of models with\nlarge width (embedding dimension). This is due to the fact that adapter\nmatrices A and B in LoRA are updated with the same learning rate. Using scaling\narguments for large width networks, we demonstrate that using the same learning\nrate for A and B does not allow efficient feature learning. We then show that\nthis suboptimality of LoRA can be corrected simply by setting different\nlearning rates for the LoRA adapter matrices A and B with a well-chosen ratio.\nWe call this proposed algorithm LoRA$+$. In our extensive experiments, LoRA$+$\nimproves performance (1-2 $\\%$ improvements) and finetuning speed (up to $\\sim$\n2X SpeedUp), at the same computational cost as LoRA.",
      "tldr_zh": "本研究发现，原始的 Low Rank Adaptation (LoRA) 方法在处理大宽度（embedding dimension）模型时表现 suboptimal，因为其适配器矩阵 A 和 B 使用相同的 learning rate，导致特征学习效率低下。通过缩放参数分析，该问题可通过为 A 和 B 矩阵设置不同的 learning rate 比例来修正，从而提出改进算法 LoRA+。实验结果显示，LoRA+ 在相同计算成本下，提高了模型性能（1-2% 改善）和微调速度（高达 ~2X 加速）。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL",
        "stat.ML"
      ],
      "primary_category": "cs.LG",
      "comment": "27 pages",
      "pdf_url": "http://arxiv.org/pdf/2402.12354v2",
      "published_date": "2024-02-19 18:33:49 UTC",
      "updated_date": "2024-07-04 18:33:00 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T07:42:27.107842"
    },
    {
      "arxiv_id": "2402.12348v2",
      "title": "GTBench: Uncovering the Strategic Reasoning Limitations of LLMs via Game-Theoretic Evaluations",
      "title_zh": "翻译失败",
      "authors": [
        "Jinhao Duan",
        "Renming Zhang",
        "James Diffenderfer",
        "Bhavya Kailkhura",
        "Lichao Sun",
        "Elias Stengel-Eskin",
        "Mohit Bansal",
        "Tianlong Chen",
        "Kaidi Xu"
      ],
      "abstract": "As Large Language Models (LLMs) are integrated into critical real-world\napplications, their strategic and logical reasoning abilities are increasingly\ncrucial. This paper evaluates LLMs' reasoning abilities in competitive\nenvironments through game-theoretic tasks, e.g., board and card games that\nrequire pure logic and strategic reasoning to compete with opponents. We first\npropose GTBench, a language-driven environment composing 10 widely recognized\ntasks, across a comprehensive game taxonomy: complete versus incomplete\ninformation, dynamic versus static, and probabilistic versus deterministic\nscenarios. Then, we (1) Characterize the game-theoretic reasoning of LLMs; and\n(2) Perform LLM-vs.-LLM competitions as reasoning evaluation. We observe that\n(1) LLMs have distinct behaviors regarding various gaming scenarios; for\nexample, LLMs fail in complete and deterministic games yet they are competitive\nin probabilistic gaming scenarios; (2) Most open-source LLMs, e.g.,\nCodeLlama-34b-Instruct and Llama-2-70b-chat, are less competitive than\ncommercial LLMs, e.g., GPT-4, in complex games, yet the recently released\nLlama-3-70b-Instruct makes up for this shortcoming. In addition,\ncode-pretraining greatly benefits strategic reasoning, while advanced reasoning\nmethods such as Chain-of-Thought (CoT) and Tree-of-Thought (ToT) do not always\nhelp. We further characterize the game-theoretic properties of LLMs, such as\nequilibrium and Pareto Efficiency in repeated games. Detailed error profiles\nare provided for a better understanding of LLMs' behavior. We hope our research\nprovides standardized protocols and serves as a foundation to spur further\nexplorations in the strategic reasoning of LLMs.",
      "tldr_zh": "本研究提出GTBench，一种基于游戏理论的基准测试环境，包含10个任务，涵盖完整/不完全信息、动态/静态以及概率/确定性等游戏场景，用于评估大型语言模型(LLMs)的战略和逻辑推理能力。研究通过表征LLMs的行为和进行LLM对LLM的竞争实验，发现LLMs在不同场景表现迥异，例如在完全和确定性游戏中表现较差，但在概率场景中更具竞争力；此外，开源模型如CodeLlama-34b-Instruct不如商业模型GPT-4强大，但Llama-3-70b-Instruct有所改善，且代码预训练有助于战略推理，而Chain-of-Thought (CoT)和Tree-of-Thought (ToT)方法并非总是有效。论文还分析了LLMs的游戏理论属性，如均衡和Pareto Efficiency，并提供详细错误配置文件，以期为LLMs的战略推理研究奠定标准基础。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "26 pages; the first two authors contributed equally; GTBench HF\n  Leaderboard: https://huggingface.co/spaces/GTBench/GTBench",
      "pdf_url": "http://arxiv.org/pdf/2402.12348v2",
      "published_date": "2024-02-19 18:23:36 UTC",
      "updated_date": "2024-06-10 17:14:09 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T07:42:41.586682"
    },
    {
      "arxiv_id": "2402.12343v4",
      "title": "Emulated Disalignment: Safety Alignment for Large Language Models May Backfire!",
      "title_zh": "模拟不对齐：大型语言模型的安全对齐可能适得其反！",
      "authors": [
        "Zhanhui Zhou",
        "Jie Liu",
        "Zhichen Dong",
        "Jiaheng Liu",
        "Chao Yang",
        "Wanli Ouyang",
        "Yu Qiao"
      ],
      "abstract": "Large language models (LLMs) undergo safety alignment to ensure safe\nconversations with humans. However, this paper introduces a training-free\nattack method capable of reversing safety alignment, converting the outcomes of\nstronger alignment into greater potential for harm by accessing only LLM output\ntoken distributions. Specifically, our method achieves this reversal by\ncontrasting the output token distribution of a safety-aligned language model\n(e.g., Llama-2-chat) against its pre-trained version (e.g., Llama-2), so that\nthe token predictions are shifted towards the opposite direction of safety\nalignment. We name this method emulated disalignment (ED) because sampling from\nthis contrastive distribution provably emulates the result of fine-tuning to\nminimize a safety reward. Our experiments with ED across three evaluation\ndatasets and four model families (Llama-1, Llama-2, Mistral, and Alpaca) show\nthat ED doubles the harmfulness of pre-trained models and outperforms strong\nbaselines, achieving the highest harmful rates in 43 out of 48 evaluation\nsubsets by a large margin. Eventually, given ED's reliance on language model\noutput token distributions, which particularly compromises open-source models,\nour findings highlight the need to reassess the open accessibility of language\nmodels, even if they have been safety-aligned. Code is available at\nhttps://github.com/ZHZisZZ/emulated-disalignment.",
      "tldr_zh": "这篇论文提出了一种名为emulated disalignment (ED)的训练-free攻击方法，能够逆转大型语言模型(LLMs)的safety alignment，使其潜在危害性增加。ED通过对比安全对齐模型（如Llama-2-chat）和其预训练版本（如Llama-2）的输出token分布，将预测结果偏移到不安全方向，从而模拟微调以最小化安全奖励的效果。在三个评估数据集和四个模型系列（Llama-1、Llama-2、Mistral和Alpaca）的实验中，ED使预训练模型的危害性加倍，并在43/48子集上大幅领先基线。研究结果警告，即使经过safety alignment的开源模型也面临风险，建议重新评估其公开访问性。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "ACL 2024",
      "pdf_url": "http://arxiv.org/pdf/2402.12343v4",
      "published_date": "2024-02-19 18:16:51 UTC",
      "updated_date": "2024-06-06 12:54:48 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T07:42:53.182766"
    },
    {
      "arxiv_id": "2402.12336v2",
      "title": "Robust CLIP: Unsupervised Adversarial Fine-Tuning of Vision Embeddings for Robust Large Vision-Language Models",
      "title_zh": "翻译失败",
      "authors": [
        "Christian Schlarmann",
        "Naman Deep Singh",
        "Francesco Croce",
        "Matthias Hein"
      ],
      "abstract": "Multi-modal foundation models like OpenFlamingo, LLaVA, and GPT-4 are\nincreasingly used for various real-world tasks. Prior work has shown that these\nmodels are highly vulnerable to adversarial attacks on the vision modality.\nThese attacks can be leveraged to spread fake information or defraud users, and\nthus pose a significant risk, which makes the robustness of large multi-modal\nfoundation models a pressing problem. The CLIP model, or one of its variants,\nis used as a frozen vision encoder in many large vision-language models\n(LVLMs), e.g. LLaVA and OpenFlamingo. We propose an unsupervised adversarial\nfine-tuning scheme to obtain a robust CLIP vision encoder, which yields\nrobustness on all vision down-stream tasks (LVLMs, zero-shot classification)\nthat rely on CLIP. In particular, we show that stealth-attacks on users of\nLVLMs by a malicious third party providing manipulated images are no longer\npossible once one replaces the original CLIP model with our robust one. No\nretraining or fine-tuning of the down-stream LVLMs is required. The code and\nrobust models are available at https://github.com/chs20/RobustVLM",
      "tldr_zh": "这篇论文针对多模态基础模型（如 OpenFlamingo 和 LLaVA）在视觉模态上的易受 adversarial attacks 问题，提出了一种 unsupervised adversarial fine-tuning 方案，用于增强 CLIP 视觉编码器的鲁棒性。方法通过微调 CLIP 的视觉嵌入，使其适用于下游任务如 LVLMs 和零样本分类，而无需重新训练这些模型，从而有效防止恶意第三方通过操纵图像进行攻击。实验证明，该鲁棒 CLIP 模型显著提高了模型的安全性，并提供了开源代码和模型以供进一步应用。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CV",
        "stat.ML"
      ],
      "primary_category": "cs.LG",
      "comment": "ICML 2024 Oral",
      "pdf_url": "http://arxiv.org/pdf/2402.12336v2",
      "published_date": "2024-02-19 18:09:48 UTC",
      "updated_date": "2024-06-05 15:32:03 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T07:43:05.966436"
    },
    {
      "arxiv_id": "2402.12331v1",
      "title": "Generating Survival Interpretable Trajectories and Data",
      "title_zh": "生成可解释生存轨迹和数据",
      "authors": [
        "Andrei V. Konstantinov",
        "Stanislav R. Kirpichenko",
        "Lev V. Utkin"
      ],
      "abstract": "A new model for generating survival trajectories and data based on applying\nan autoencoder of a specific structure is proposed. It solves three tasks.\nFirst, it provides predictions in the form of the expected event time and the\nsurvival function for a new generated feature vector on the basis of the Beran\nestimator. Second, the model generates additional data based on a given\ntraining set that would supplement the original dataset. Third, the most\nimportant, it generates a prototype time-dependent trajectory for an object,\nwhich characterizes how features of the object could be changed to achieve a\ndifferent time to an event. The trajectory can be viewed as a type of the\ncounterfactual explanation. The proposed model is robust during training and\ninference due to a specific weighting scheme incorporating into the variational\nautoencoder. The model also determines the censored indicators of new generated\ndata by solving a classification task. The paper demonstrates the efficiency\nand properties of the proposed model using numerical experiments on synthetic\nand real datasets. The code of the algorithm implementing the proposed model is\npublicly available.",
      "tldr_zh": "这篇论文提出了一种基于特定结构自编码器（autoencoder）的模型，用于生成生存轨迹和数据，主要解决生存分析中的预测、数据扩展和可解释性问题。该模型通过 Beran 估计器为新特征向量提供预期事件时间和生存函数的预测，生成补充训练数据，并创建原型时间依赖轨迹，作为一种反事实解释（counterfactual explanation），展示如何调整对象特征以改变事件时间。模型采用变分自编码器（variational autoencoder）和特定加权方案增强训练和推理的鲁棒性，并在合成和真实数据集的数值实验中证明了其效率，且代码已公开可用。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "stat.ML"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2402.12331v1",
      "published_date": "2024-02-19 18:02:10 UTC",
      "updated_date": "2024-02-19 18:02:10 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T07:43:18.807218"
    },
    {
      "arxiv_id": "2402.12329v2",
      "title": "Query-Based Adversarial Prompt Generation",
      "title_zh": "基于查询的对抗性提示生成",
      "authors": [
        "Jonathan Hayase",
        "Ema Borevkovic",
        "Nicholas Carlini",
        "Florian Tramèr",
        "Milad Nasr"
      ],
      "abstract": "Recent work has shown it is possible to construct adversarial examples that\ncause an aligned language model to emit harmful strings or perform harmful\nbehavior. Existing attacks work either in the white-box setting (with full\naccess to the model weights), or through transferability: the phenomenon that\nadversarial examples crafted on one model often remain effective on other\nmodels. We improve on prior work with a query-based attack that leverages API\naccess to a remote language model to construct adversarial examples that cause\nthe model to emit harmful strings with (much) higher probability than with\ntransfer-only attacks. We validate our attack on GPT-3.5 and OpenAI's safety\nclassifier; we can cause GPT-3.5 to emit harmful strings that current transfer\nattacks fail at, and we can evade the safety classifier with nearly 100%\nprobability.",
      "tldr_zh": "这篇论文提出了一种基于查询（Query-Based）的对抗性提示生成方法，通过利用对远程语言模型的 API 访问，构建对抗性示例（adversarial examples），以比现有转移攻击（transfer attacks）更高的概率诱导模型输出有害字符串。相比白盒攻击和转移性方法，该方法在不需访问模型权重的情况下显著提升了攻击效率。在实验中，该攻击在 GPT-3.5 上成功生成现有攻击无法实现的有害输出，并在 OpenAI 的安全分类器上实现了近 100% 的规避概率，为评估语言模型的安全性提供了新洞见。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.CR",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2402.12329v2",
      "published_date": "2024-02-19 18:01:36 UTC",
      "updated_date": "2024-12-07 23:09:49 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T07:43:30.724264"
    },
    {
      "arxiv_id": "2402.12327v3",
      "title": "Shall We Team Up: Exploring Spontaneous Cooperation of Competing LLM Agents",
      "title_zh": "翻译失败",
      "authors": [
        "Zengqing Wu",
        "Run Peng",
        "Shuyuan Zheng",
        "Qianying Liu",
        "Xu Han",
        "Brian Inhyuk Kwon",
        "Makoto Onizuka",
        "Shaojie Tang",
        "Chuan Xiao"
      ],
      "abstract": "Large Language Models (LLMs) have increasingly been utilized in social\nsimulations, where they are often guided by carefully crafted instructions to\nstably exhibit human-like behaviors during simulations. Nevertheless, we doubt\nthe necessity of shaping agents' behaviors for accurate social simulations.\nInstead, this paper emphasizes the importance of spontaneous phenomena, wherein\nagents deeply engage in contexts and make adaptive decisions without explicit\ndirections. We explored spontaneous cooperation across three competitive\nscenarios and successfully simulated the gradual emergence of cooperation,\nfindings that align closely with human behavioral data. This approach not only\naids the computational social science community in bridging the gap between\nsimulations and real-world dynamics but also offers the AI community a novel\nmethod to assess LLMs' capability of deliberate reasoning.",
      "tldr_zh": "本论文探讨了大型语言模型(LLMs)代理在竞争场景中的自发合作现象，而非依赖精心设计的指令来引导行为。通过在三个竞争环境中模拟代理的适应性决策，研究成功再现了合作的逐渐出现，并与人类行为数据高度一致。这一方法为计算社会科学社区弥合模拟与真实世界动态的差距提供了新途径，同时为AI社区评估LLMs的审议推理能力带来了创新评估工具。",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.CY",
        "cs.MA",
        "econ.GN",
        "q-fin.EC"
      ],
      "primary_category": "cs.AI",
      "comment": "EMNLP 2024 Findings. Source codes available at\n  https://github.com/wuzengqing001225/SABM_ShallWeTeamUp",
      "pdf_url": "http://arxiv.org/pdf/2402.12327v3",
      "published_date": "2024-02-19 18:00:53 UTC",
      "updated_date": "2024-10-27 19:03:37 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T07:43:41.531209"
    },
    {
      "arxiv_id": "2403.15401v3",
      "title": "Large Language Model for Mental Health: A Systematic Review",
      "title_zh": "大语言模型用于心理健康：一项系统综述",
      "authors": [
        "Zhijun Guo",
        "Alvina Lai",
        "Johan Hilge Thygesen",
        "Joseph Farrington",
        "Thomas Keen",
        "Kezhi Li"
      ],
      "abstract": "Large language models (LLMs) have attracted significant attention for\npotential applications in digital health, while their application in mental\nhealth is subject to ongoing debate. This systematic review aims to evaluate\nthe usage of LLMs in mental health, focusing on their strengths and limitations\nin early screening, digital interventions, and clinical applications. Adhering\nto PRISMA guidelines, we searched PubMed, IEEE Xplore, Scopus, JMIR, and ACM\nusing keywords: 'mental health OR mental illness OR mental disorder OR\npsychiatry' AND 'large language models'. We included articles published between\nJanuary 1, 2017, and April 30, 2024, excluding non-English articles. 30\narticles were evaluated, which included research on mental health conditions\nand suicidal ideation detection through text (n=15), usage of LLMs for mental\nhealth conversational agents (CAs) (n=7), and other applications and\nevaluations of LLMs in mental health (n=18). LLMs exhibit substantial\neffectiveness in detecting mental health issues and providing accessible,\nde-stigmatized eHealth services. However, the current risks associated with the\nclinical use might surpass their benefits. The study identifies several\nsignificant issues: the lack of multilingual datasets annotated by experts,\nconcerns about the accuracy and reliability of the content generated,\nchallenges in interpretability due to the 'black box' nature of LLMs, and\npersistent ethical dilemmas. These include the lack of a clear ethical\nframework, concerns about data privacy, and the potential for over-reliance on\nLLMs by both therapists and patients, which could compromise traditional\nmedical practice. Despite these issues, the rapid development of LLMs\nunderscores their potential as new clinical aids, emphasizing the need for\ncontinued research and development in this area.",
      "tldr_zh": "这篇论文对Large Language Models (LLMs)在心理健康领域的应用进行了系统综述，评估了其在早期筛查、数字干预和临床应用中的优势和局限性。研究遵循PRISMA指南，从PubMed、IEEE Xplore等数据库中筛选了2017年1月至2024年4月期间的30篇英文文章，分类包括文本检测心理健康问题（如精神疾病和自杀意念，n=15）、LLMs用于心理健康对话代理（n=7）以及其他应用（n=18）。LLMs显示出在检测心理健康问题和提供可访问的eHealth服务方面的显著有效性，但存在重大挑战，如生成内容的准确性和可靠性不足、LLMs的“黑箱”性质导致的可解释性问题，以及伦理难题包括数据隐私和过度依赖风险。这些问题可能使临床应用的风险超过益处，因此论文强调需要进一步研究以开发LLMs作为临床辅助工具。",
      "categories": [
        "cs.CY",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.CY",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2403.15401v3",
      "published_date": "2024-02-19 17:58:41 UTC",
      "updated_date": "2024-08-12 21:46:16 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T07:43:56.208757"
    },
    {
      "arxiv_id": "2402.12319v1",
      "title": "Dynamic Environment Responsive Online Meta-Learning with Fairness Awareness",
      "title_zh": "动态环境响应的在线元学习，具有公平意识",
      "authors": [
        "Chen Zhao",
        "Feng Mi",
        "Xintao Wu",
        "Kai Jiang",
        "Latifur Khan",
        "Feng Chen"
      ],
      "abstract": "The fairness-aware online learning framework has emerged as a potent tool\nwithin the context of continuous lifelong learning. In this scenario, the\nlearner's objective is to progressively acquire new tasks as they arrive over\ntime, while also guaranteeing statistical parity among various protected\nsub-populations, such as race and gender, when it comes to the newly introduced\ntasks. A significant limitation of current approaches lies in their heavy\nreliance on the i.i.d (independent and identically distributed) assumption\nconcerning data, leading to a static regret analysis of the framework.\nNevertheless, it's crucial to note that achieving low static regret does not\nnecessarily translate to strong performance in dynamic environments\ncharacterized by tasks sampled from diverse distributions. In this paper, to\ntackle the fairness-aware online learning challenge in evolving settings, we\nintroduce a unique regret measure, FairSAR, by incorporating long-term fairness\nconstraints into a strongly adapted loss regret framework. Moreover, to\ndetermine an optimal model parameter at each time step, we introduce an\ninnovative adaptive fairness-aware online meta-learning algorithm, referred to\nas FairSAOML. This algorithm possesses the ability to adjust to dynamic\nenvironments by effectively managing bias control and model accuracy. The\nproblem is framed as a bi-level convex-concave optimization, considering both\nthe model's primal and dual parameters, which pertain to its accuracy and\nfairness attributes, respectively. Theoretical analysis yields sub-linear upper\nbounds for both loss regret and the cumulative violation of fairness\nconstraints. Our experimental evaluation on various real-world datasets in\ndynamic environments demonstrates that our proposed FairSAOML algorithm\nconsistently outperforms alternative approaches rooted in the most advanced\nprior online learning methods.",
      "tldr_zh": "该论文针对公平感知在线学习框架在动态环境中的挑战，提出了一种新颖的遗憾度量FairSAR，将长期公平约束融入强适应损失遗憾分析，以克服现有方法的i.i.d.假设局限。研究引入了自适应公平感知在线元学习算法FairSAOML，该算法通过双层凸-凹优化同时管理模型准确性和公平性，适应任务分布变化的环境。实验结果显示，FairSAOML在各种真实世界动态数据集上优于现有先进方法，并在理论上提供了损失遗憾和公平约束违反的次线性上界。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CY"
      ],
      "primary_category": "cs.LG",
      "comment": "Accepted by TKDD, extended from KDD 2022. arXiv admin note:\n  substantial text overlap with arXiv:2205.11264",
      "pdf_url": "http://arxiv.org/pdf/2402.12319v1",
      "published_date": "2024-02-19 17:44:35 UTC",
      "updated_date": "2024-02-19 17:44:35 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T07:44:06.392551"
    },
    {
      "arxiv_id": "2402.13290v1",
      "title": "Grounding from an AI and Cognitive Science Lens",
      "title_zh": "翻译失败",
      "authors": [
        "Goonmeet Bajaj",
        "Srinivasan Parthasarathy",
        "Valerie L. Shalin",
        "Amit Sheth"
      ],
      "abstract": "Grounding is a challenging problem, requiring a formal definition and\ndifferent levels of abstraction. This article explores grounding from both\ncognitive science and machine learning perspectives. It identifies the\nsubtleties of grounding, its significance for collaborative agents, and\nsimilarities and differences in grounding approaches in both communities. The\narticle examines the potential of neuro-symbolic approaches tailored for\ngrounding tasks, showcasing how they can more comprehensively address\ngrounding. Finally, we discuss areas for further exploration and development in\ngrounding.",
      "tldr_zh": "这篇文章从认知科学和AI的视角探讨了Grounding问题，强调其正式定义和不同抽象级别的挑战性。\n它分析了Grounding在协作代理中的重要性，以及AI和认知科学社区在方法上的异同。\n论文突出了神经符号(neuro-symbolic)方法如何更全面地解决Grounding任务，并提出了进一步探索和发展的潜在领域。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2402.13290v1",
      "published_date": "2024-02-19 17:44:34 UTC",
      "updated_date": "2024-02-19 17:44:34 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T07:44:17.186301"
    },
    {
      "arxiv_id": "2402.12317v2",
      "title": "EVOR: Evolving Retrieval for Code Generation",
      "title_zh": "EVOR：代码生成的演化检索",
      "authors": [
        "Hongjin Su",
        "Shuyang Jiang",
        "Yuhang Lai",
        "Haoyuan Wu",
        "Boao Shi",
        "Che Liu",
        "Qian Liu",
        "Tao Yu"
      ],
      "abstract": "Recently the retrieval-augmented generation (RAG) has been successfully\napplied in code generation. However, existing pipelines for retrieval-augmented\ncode generation (RACG) employ static knowledge bases with a single source,\nlimiting the adaptation capabilities of Large Language Models (LLMs) to domains\nthey have insufficient knowledge of. In this work, we develop a novel pipeline,\nEVOR, that employs the synchronous evolution of both queries and diverse\nknowledge bases. On two realistic settings where the external knowledge is\nrequired to solve code generation tasks, we compile four new datasets\nassociated with frequently updated libraries and long-tail programming\nlanguages, named EVOR-BENCH. Extensive experiments demonstrate that EVOR\nachieves two to four times of execution accuracy compared to other methods such\nas Reflexion (Shinn et al., 2024), DocPrompting (Zhou et al., 2023), etc. We\ndemonstrate that EVOR is flexible and can be easily combined with them to\nachieve further improvement. Further analysis reveals that EVOR benefits from\nthe synchronous evolution of queries and documents and the diverse information\nsources in the knowledge base. We hope that our studies will inspire more\ninsights into the design of advanced RACG pipelines in future research. Our\nmodel, code, and data are available at https://arks-codegen.github.io.",
      "tldr_zh": "该论文提出EVOR，一种新型检索增强代码生成（RACG）管道，通过查询和多样化知识库的同步演化，解决了Large Language Models (LLMs) 在知识不足领域的适应性问题。研究者构建了EVOR-BENCH数据集，包括四个与频繁更新库和长尾编程语言相关的新数据集，用于评估代码生成任务。实验结果显示，EVOR在执行准确率上比基线方法如Reflexion和DocPrompting高出2-4倍，且能灵活与其他方法结合以进一步提升性能。分析表明，EVOR的关键优势在于查询和文档的同步演化以及知识库的多样化信息来源，为未来RACG管道设计提供了新见解。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "Retrieval-augmented code generation",
      "pdf_url": "http://arxiv.org/pdf/2402.12317v2",
      "published_date": "2024-02-19 17:37:28 UTC",
      "updated_date": "2024-12-03 15:56:26 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T07:44:29.281542"
    },
    {
      "arxiv_id": "2402.12307v1",
      "title": "Multi-View Conformal Learning for Heterogeneous Sensor Fusion",
      "title_zh": "翻译失败",
      "authors": [
        "Enrique Garcia-Ceja"
      ],
      "abstract": "Being able to assess the confidence of individual predictions in machine\nlearning models is crucial for decision making scenarios. Specially, in\ncritical applications such as medical diagnosis, security, and unmanned\nvehicles, to name a few. In the last years, complex predictive models have had\ngreat success in solving hard tasks and new methods are being proposed every\nday. While the majority of new developments in machine learning models focus on\nimproving the overall performance, less effort is put on assessing the\ntrustworthiness of individual predictions, and even to a lesser extent, in the\ncontext of sensor fusion. To this end, we build and test multi-view and\nsingle-view conformal models for heterogeneous sensor fusion. Our models\nprovide theoretical marginal confidence guarantees since they are based on the\nconformal prediction framework. We also propose a multi-view semi-conformal\nmodel based on sets intersection. Through comprehensive experimentation, we\nshow that multi-view models perform better than single-view models not only in\nterms of accuracy-based performance metrics (as it has already been shown in\nseveral previous works) but also in conformal measures that provide uncertainty\nestimation. Our results also showed that multi-view models generate prediction\nsets with less uncertainty compared to single-view models.",
      "tldr_zh": "该论文探讨了评估机器学习模型预测置信度的必要性，尤其在医疗诊断、安全和无人车辆等关键应用中。研究构建并测试了基于 Conformal Prediction 框架的多视图和单视图模型，用于异构传感器融合，并提出了一种多视图半 Conformal 模型，通过集合交集来提升不确定性估计。实验结果显示，多视图模型不仅在准确性指标上优于单视图模型，还能生成不确定性更低的预测集，从而提供更可靠的置信度保证。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2402.12307v1",
      "published_date": "2024-02-19 17:30:09 UTC",
      "updated_date": "2024-02-19 17:30:09 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T07:44:41.060213"
    },
    {
      "arxiv_id": "2402.12298v1",
      "title": "Is Open-Source There Yet? A Comparative Study on Commercial and Open-Source LLMs in Their Ability to Label Chest X-Ray Reports",
      "title_zh": "翻译失败",
      "authors": [
        "Felix J. Dorfner",
        "Liv Jürgensen",
        "Leonhard Donle",
        "Fares Al Mohamad",
        "Tobias R. Bodenmann",
        "Mason C. Cleveland",
        "Felix Busch",
        "Lisa C. Adams",
        "James Sato",
        "Thomas Schultz",
        "Albert E. Kim",
        "Jameson Merkow",
        "Keno K. Bressem",
        "Christopher P. Bridge"
      ],
      "abstract": "Introduction: With the rapid advances in large language models (LLMs), there\nhave been numerous new open source as well as commercial models. While recent\npublications have explored GPT-4 in its application to extracting information\nof interest from radiology reports, there has not been a real-world comparison\nof GPT-4 to different leading open-source models.\n  Materials and Methods: Two different and independent datasets were used. The\nfirst dataset consists of 540 chest x-ray reports that were created at the\nMassachusetts General Hospital between July 2019 and July 2021. The second\ndataset consists of 500 chest x-ray reports from the ImaGenome dataset. We then\ncompared the commercial models GPT-3.5 Turbo and GPT-4 from OpenAI to the\nopen-source models Mistral-7B, Mixtral-8x7B, Llama2-13B, Llama2-70B,\nQWEN1.5-72B and CheXbert and CheXpert-labeler in their ability to accurately\nlabel the presence of multiple findings in x-ray text reports using different\nprompting techniques.\n  Results: On the ImaGenome dataset, the best performing open-source model was\nLlama2-70B with micro F1-scores of 0.972 and 0.970 for zero- and few-shot\nprompts, respectively. GPT-4 achieved micro F1-scores of 0.975 and 0.984,\nrespectively. On the institutional dataset, the best performing open-source\nmodel was QWEN1.5-72B with micro F1-scores of 0.952 and 0.965 for zero- and\nfew-shot prompting, respectively. GPT-4 achieved micro F1-scores of 0.975 and\n0.973, respectively.\n  Conclusion: In this paper, we show that while GPT-4 is superior to\nopen-source models in zero-shot report labeling, the implementation of few-shot\nprompting can bring open-source models on par with GPT-4. This shows that\nopen-source models could be a performant and privacy preserving alternative to\nGPT-4 for the task of radiology report classification.",
      "tldr_zh": "本研究比较了商业LLMs（如GPT-4和GPT-3.5 Turbo）与开源模型（如Mistral-7B、Llama2-70B和QWEN1.5-72B）在标记胸部X光报告中多种发现的能力，使用了两个数据集（包括MGH的540份报告和ImaGenome的500份报告），并测试了zero-shot和few-shot提示技术。结果显示，在ImaGenome数据集上，Llama2-70B的微F1分数最高，分别为0.972（zero-shot）和0.970（few-shot），而GPT-4达到0.975和0.984；在机构数据集上，QWEN1.5-72B表现最佳，分别为0.952和0.965，与GPT-4的0.975和0.973不相上下。主要结论是，虽然GPT-4在zero-shot场景中优于开源模型，但few-shot提示可使开源模型性能与GPT-4相当，提供了一个高效且隐私保护的放射学报告分类替代方案。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2402.12298v1",
      "published_date": "2024-02-19 17:23:10 UTC",
      "updated_date": "2024-02-19 17:23:10 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T07:44:56.112792"
    },
    {
      "arxiv_id": "2402.12284v2",
      "title": "Refining Minimax Regret for Unsupervised Environment Design",
      "title_zh": "翻译失败",
      "authors": [
        "Michael Beukman",
        "Samuel Coward",
        "Michael Matthews",
        "Mattie Fellows",
        "Minqi Jiang",
        "Michael Dennis",
        "Jakob Foerster"
      ],
      "abstract": "In unsupervised environment design, reinforcement learning agents are trained\non environment configurations (levels) generated by an adversary that maximises\nsome objective. Regret is a commonly used objective that theoretically results\nin a minimax regret (MMR) policy with desirable robustness guarantees; in\nparticular, the agent's maximum regret is bounded. However, once the agent\nreaches this regret bound on all levels, the adversary will only sample levels\nwhere regret cannot be further reduced. Although there are possible performance\nimprovements to be made outside of these regret-maximising levels, learning\nstagnates. In this work, we introduce Bayesian level-perfect MMR (BLP), a\nrefinement of the minimax regret objective that overcomes this limitation. We\nformally show that solving for this objective results in a subset of MMR\npolicies, and that BLP policies act consistently with a Perfect Bayesian policy\nover all levels. We further introduce an algorithm, ReMiDi, that results in a\nBLP policy at convergence. We empirically demonstrate that training on levels\nfrom a minimax regret adversary causes learning to prematurely stagnate, but\nthat ReMiDi continues learning.",
      "tldr_zh": "本研究针对无监督环境设计中的强化学习问题，提出对minimax regret (MMR)目标的改进，以解决现有方法导致学习过早停滞的问题。具体而言，他们引入Bayesian level-perfect MMR (BLP)作为一种精炼目标，证明BLP策略是MMR策略的子集，并在所有环境配置(levels)上与Perfect Bayesian策略一致。研究还开发了算法ReMiDi，该算法在收敛时产生BLP策略，并通过实验验证，ReMiDi能继续学习，而传统MMR对手训练会导致性能停滞。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "ICML 2024. The first two authors contributed equally",
      "pdf_url": "http://arxiv.org/pdf/2402.12284v2",
      "published_date": "2024-02-19 16:51:29 UTC",
      "updated_date": "2024-06-08 10:08:25 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T07:45:08.909281"
    },
    {
      "arxiv_id": "2402.12280v2",
      "title": "Plato: Plan to Efficiently Decode for Large Language Model Inference",
      "title_zh": "翻译失败",
      "authors": [
        "Shuowei Jin",
        "Xueshen Liu",
        "Yongji Wu",
        "Haizhong Zheng",
        "Qingzhao Zhang",
        "Atul Prakash",
        "Matthew Lentz",
        "Danyang Zhuo",
        "Feng Qian",
        "Z. Morley Mao"
      ],
      "abstract": "Large language models (LLMs) have achieved remarkable success in natural\nlanguage tasks, but their inference incurs substantial computational and memory\noverhead. To improve efficiency, parallel decoding methods like\nSkeleton-of-Thought (SoT) decompose prompts into sub-problems for concurrent\nprocessing. However, these methods significantly compromise answer quality by\ntreating semantically linked sub-problems as independent. We propose Plato, a\nnovel approach that co-designs algorithms and systems for semantic-aware\nparallel decoding. Plato leverages LLMs to organize sub-problems into a\ndependency graph based on logical and causal relationships, enabling concurrent\ndecoding of non-dependent nodes while preserving answer coherence and quality.\nTo further enhance efficiency, Plato pipelines planning and node decoding\nstages, implements a global context cache, and carefully structures node\ninference prompts to maximize key-value cache reuse and minimize overhead. Our\nevaluations show that Plato improves throughput by 68% over autoregressive\ndecoding while achieving a 40% net win rate in answer quality. Compared to SoT,\nPlato demonstrates a remarkable 90% quality net-win rate. Ablation studies\nreveal that our pipeline design improves speedup by 29%, while our KV cache\nreuse optimization reduces overhead by 75%.",
      "tldr_zh": "该论文提出Plato，一种语义感知的并行解码方法，旨在提升大型语言模型(LLMs)推理的效率，同时解决现有方法如Skeleton-of-Thought (SoT)因忽略子问题依赖而牺牲答案质量的问题。Plato利用LLMs构建基于逻辑和因果关系的依赖图，实现非依赖节点的并发解码，并通过管道化规划、节点解码阶段、全球上下文缓存以及优化关键值缓存(KV cache)重用来最小化开销。实验结果显示，Plato相较自回归解码提高了68%的吞吐量并获得40%的答案质量净胜率，与SoT相比，质量净胜率高达90%；此外，消融研究表明其管道设计提升了29%的加速，KV缓存优化减少了75%的开销。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2402.12280v2",
      "published_date": "2024-02-19 16:47:04 UTC",
      "updated_date": "2025-04-13 14:17:57 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T07:45:21.714954"
    },
    {
      "arxiv_id": "2402.12279v2",
      "title": "Key ingredients for effective zero-shot cross-lingual knowledge transfer in generative tasks",
      "title_zh": "翻译失败",
      "authors": [
        "Nadezhda Chirkova",
        "Vassilina Nikoulina"
      ],
      "abstract": "Zero-shot cross-lingual knowledge transfer enables a multilingual pretrained\nlanguage model, finetuned on a task in one language, make predictions for this\ntask in other languages. While being broadly studied for natural language\nunderstanding tasks, the described setting is understudied for generation.\nPrevious works notice a frequent problem of generation in a wrong language and\npropose approaches to address it, usually using mT5 as a backbone model. In\nthis work we compare various approaches proposed from the literature in unified\nsettings, also including alternative backbone models, namely mBART and\nNLLB-200. We first underline the importance of tuning learning rate used for\nfinetuning, which helps to substantially alleviate the problem of generation in\nthe wrong language. Then, we show that with careful learning rate tuning, the\nsimple full finetuning of the model acts as a very strong baseline and\nalternative approaches bring only marginal improvements. Finally, we find that\nmBART performs similarly to mT5 of the same size, and NLLB-200 can be\ncompetitive in some cases. Our final zero-shot models reach the performance of\nthe approach based on data translation which is usually considered as an upper\nbaseline for zero-shot cross-lingual transfer in generation.",
      "tldr_zh": "本文研究了在生成任务中实现有效 zero-shot cross-lingual knowledge transfer 的关键因素，通过比较 mT5、mBART 和 NLLB-200 等骨干模型。作者发现，调整学习率可以显著缓解生成错误语言的问题，使简单的 full finetuning 成为一个强有力的基准方法，其他优化方法仅带来微小改进。最终结果显示，mBART 与同等规模的 mT5 性能相当，NLLB-200 在某些场景下具有竞争力，且 zero-shot 模型的性能可达到基于数据翻译的基准水平。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "NAACL 2024 final version. arXiv admin note: text overlap with\n  arXiv:2310.09917",
      "pdf_url": "http://arxiv.org/pdf/2402.12279v2",
      "published_date": "2024-02-19 16:43:57 UTC",
      "updated_date": "2024-04-22 17:32:00 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T07:45:33.286810"
    },
    {
      "arxiv_id": "2402.12275v3",
      "title": "WorldCoder, a Model-Based LLM Agent: Building World Models by Writing Code and Interacting with the Environment",
      "title_zh": "翻译失败",
      "authors": [
        "Hao Tang",
        "Darren Key",
        "Kevin Ellis"
      ],
      "abstract": "We give a model-based agent that builds a Python program representing its\nknowledge of the world based on its interactions with the environment. The\nworld model tries to explain its interactions, while also being optimistic\nabout what reward it can achieve. We define this optimism as a logical\nconstraint between a program and a planner. We study our agent on gridworlds,\nand on task planning, finding our approach is more sample-efficient compared to\ndeep RL, more compute-efficient compared to ReAct-style agents, and that it can\ntransfer its knowledge across environments by editing its code.",
      "tldr_zh": "该论文提出了 WorldCoder，一种基于模型的 LLM Agent，它通过编写 Python 程序和与环境互动来构建世界模型(world models)，以解释其互动并保持乐观(optimism)作为程序与规划器之间的逻辑约束。相比传统方法，该 Agent 在 gridworlds 和任务规划环境中显示出更高的样本效率(sample-efficient)比 deep RL 更优，以及更强的计算效率(compute-efficient)比 ReAct-style agents 更佳。WorldCoder 还能通过编辑代码实现知识在不同环境间的转移，展示了其灵活性和可扩展性。",
      "categories": [
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2402.12275v3",
      "published_date": "2024-02-19 16:39:18 UTC",
      "updated_date": "2024-09-20 18:56:41 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T07:45:44.980962"
    },
    {
      "arxiv_id": "2402.12424v5",
      "title": "Tables as Texts or Images: Evaluating the Table Reasoning Ability of LLMs and MLLMs",
      "title_zh": "表格作为文本或图像：评估 LLMs 和 MLLMs 的表格推理能力",
      "authors": [
        "Naihao Deng",
        "Zhenjie Sun",
        "Ruiqi He",
        "Aman Sikka",
        "Yulong Chen",
        "Lin Ma",
        "Yue Zhang",
        "Rada Mihalcea"
      ],
      "abstract": "In this paper, we investigate the effectiveness of various LLMs in\ninterpreting tabular data through different prompting strategies and data\nformats. Our analyses extend across six benchmarks for table-related tasks such\nas question-answering and fact-checking. We introduce for the first time the\nassessment of LLMs' performance on image-based table representations.\nSpecifically, we compare five text-based and three image-based table\nrepresentations, demonstrating the role of representation and prompting on LLM\nperformance. Our study provides insights into the effective use of LLMs on\ntable-related tasks.",
      "tldr_zh": "本研究评估了大型语言模型(LLMs)和多模态语言模型(MLLMs)在表格推理任务中的能力，聚焦于不同提示策略和数据格式的影响。研究者分析了六个表格相关基准任务，包括问答和事实检查，并首次引入了对图像-based 表格表示的性能评估。结果显示，五种文本-based 和三种图像-based 表格表示形式对模型表现有显著作用，为有效利用 LLMs 处理表格任务提供了关键见解。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL",
        "cs.CV"
      ],
      "primary_category": "cs.LG",
      "comment": "Accepted to ACL 2024 Findings; Naihao and Zhenjie contributed equally\n  to the project; Data available at:\n  https://github.com/dnaihao/Tables-as-Texts-or-Images",
      "pdf_url": "http://arxiv.org/pdf/2402.12424v5",
      "published_date": "2024-02-19 16:34:50 UTC",
      "updated_date": "2024-10-17 03:39:59 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T07:45:56.193894"
    },
    {
      "arxiv_id": "2402.12265v3",
      "title": "On the Byzantine-Resilience of Distillation-Based Federated Learning",
      "title_zh": "翻译失败",
      "authors": [
        "Christophe Roux",
        "Max Zimmer",
        "Sebastian Pokutta"
      ],
      "abstract": "Federated Learning (FL) algorithms using Knowledge Distillation (KD) have\nreceived increasing attention due to their favorable properties with respect to\nprivacy, non-i.i.d. data and communication cost. These methods depart from\ntransmitting model parameters and instead communicate information about a\nlearning task by sharing predictions on a public dataset. In this work, we\nstudy the performance of such approaches in the byzantine setting, where a\nsubset of the clients act in an adversarial manner aiming to disrupt the\nlearning process. We show that KD-based FL algorithms are remarkably resilient\nand analyze how byzantine clients can influence the learning process. Based on\nthese insights, we introduce two new byzantine attacks and demonstrate their\nability to break existing byzantine-resilient methods. Additionally, we propose\na novel defence method which enhances the byzantine resilience of KD-based FL\nalgorithms. Finally, we provide a general framework to obfuscate attacks,\nmaking them significantly harder to detect, thereby improving their\neffectiveness. Our findings serve as an important building block in the\nanalysis of byzantine FL, contributing through the development of new attacks\nand new defence mechanisms, further advancing the robustness of KD-based FL\nalgorithms.",
      "tldr_zh": "本文研究了基于知识蒸馏（KD）的联邦学习（FL）算法在拜占庭环境中（即存在恶意客户端）的弹性表现，发现这些算法对攻击具有显著抗性，并分析了恶意客户端如何影响学习过程。作者引入了两种新攻击方法，能够突破现有的拜占庭弹性防御，同时提出了一种新型防御机制来增强KD-based FL的鲁棒性。最终，该研究提供了一个通用框架来混淆攻击，使其更难检测，并为提升FL算法的安全性奠定基础。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.DC"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2402.12265v3",
      "published_date": "2024-02-19 16:26:40 UTC",
      "updated_date": "2025-03-17 14:08:19 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T07:46:09.307007"
    },
    {
      "arxiv_id": "2402.12264v1",
      "title": "Uncertainty quantification in fine-tuned LLMs using LoRA ensembles",
      "title_zh": "翻译失败",
      "authors": [
        "Oleksandr Balabanov",
        "Hampus Linander"
      ],
      "abstract": "Fine-tuning large language models can improve task specific performance,\nalthough a general understanding of what the fine-tuned model has learned,\nforgotten and how to trust its predictions is still missing. We derive\nprincipled uncertainty quantification for fine-tuned LLMs with posterior\napproximations using computationally efficient low-rank adaptation ensembles.\nWe analyze three common multiple-choice datasets using low-rank adaptation\nensembles based on Mistral-7b, and draw quantitative and qualitative\nconclusions on their perceived complexity and model efficacy on the different\ntarget domains during and after fine-tuning. In particular, backed by the\nnumerical experiments, we hypothesise about signals from entropic uncertainty\nmeasures for data domains that are inherently difficult for a given\narchitecture to learn.",
      "tldr_zh": "本论文提出了一种使用LoRA集成（LoRA ensembles）来近似后验的方法，实现对细调大型语言模型（LLMs）的原理性不确定性量化，从而更好地理解模型在特定任务中的学习、遗忘和预测可信度。研究者基于Mistral-7b模型分析了三个常见多项选择数据集，评估了数据集的感知复杂度和模型效能，并在细调前后得出定量和定性结论。实验结果支持了关于熵不确定性度量的假设，即某些数据域对特定架构天生难以学习，为提升LLMs的鲁棒性提供了新见解。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL",
        "stat.ML"
      ],
      "primary_category": "cs.LG",
      "comment": "8 pages, 4 figures",
      "pdf_url": "http://arxiv.org/pdf/2402.12264v1",
      "published_date": "2024-02-19 16:26:00 UTC",
      "updated_date": "2024-02-19 16:26:00 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T07:46:21.809844"
    },
    {
      "arxiv_id": "2402.14848v2",
      "title": "Same Task, More Tokens: the Impact of Input Length on the Reasoning Performance of Large Language Models",
      "title_zh": "相同任务，更多标记：输入长度对大型语言模型推理性能的影响",
      "authors": [
        "Mosh Levy",
        "Alon Jacoby",
        "Yoav Goldberg"
      ],
      "abstract": "This paper explores the impact of extending input lengths on the capabilities\nof Large Language Models (LLMs). Despite LLMs advancements in recent times,\ntheir performance consistency across different input lengths is not well\nunderstood. We investigate this aspect by introducing a novel QA reasoning\nframework, specifically designed to assess the impact of input length. We\nisolate the effect of input length using multiple versions of the same sample,\neach being extended with padding of different lengths, types and locations. Our\nfindings show a notable degradation in LLMs' reasoning performance at much\nshorter input lengths than their technical maximum. We show that the\ndegradation trend appears in every version of our dataset, although at\ndifferent intensities. Additionally, our study reveals that the traditional\nmetric of next word prediction correlates negatively with performance of LLMs'\non our reasoning dataset. We analyse our results and identify failure modes\nthat can serve as useful guides for future research, potentially informing\nstrategies to address the limitations observed in LLMs.",
      "tldr_zh": "本文研究了输入长度对大型语言模型(LLMs)的推理性能的影响，引入了一个新的QA推理框架，通过使用相同样本的不同版本并添加不同长度的填充来隔离输入长度的影响。结果显示，LLMs的推理性能在远低于其技术最大值的输入长度下显著下降，这种趋势在数据集的各个版本中均出现，但强度有所不同。此外，该研究发现，传统指标如下一个词预测与LLMs的推理性能呈负相关，并分析了失败模式，为未来优化LLMs策略提供指导。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "Accepted to ACL 2024",
      "pdf_url": "http://arxiv.org/pdf/2402.14848v2",
      "published_date": "2024-02-19 16:04:53 UTC",
      "updated_date": "2024-07-10 17:01:37 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T07:46:32.589630"
    },
    {
      "arxiv_id": "2402.12240v1",
      "title": "BEARS Make Neuro-Symbolic Models Aware of their Reasoning Shortcuts",
      "title_zh": "BEARS 使神经符号模型意识到它们的推理捷径",
      "authors": [
        "Emanuele Marconato",
        "Samuele Bortolotti",
        "Emile van Krieken",
        "Antonio Vergari",
        "Andrea Passerini",
        "Stefano Teso"
      ],
      "abstract": "Neuro-Symbolic (NeSy) predictors that conform to symbolic knowledge -\nencoding, e.g., safety constraints - can be affected by Reasoning Shortcuts\n(RSs): They learn concepts consistent with the symbolic knowledge by exploiting\nunintended semantics. RSs compromise reliability and generalization and, as we\nshow in this paper, they are linked to NeSy models being overconfident about\nthe predicted concepts. Unfortunately, the only trustworthy mitigation strategy\nrequires collecting costly dense supervision over the concepts. Rather than\nattempting to avoid RSs altogether, we propose to ensure NeSy models are aware\nof the semantic ambiguity of the concepts they learn, thus enabling their users\nto identify and distrust low-quality concepts. Starting from three simple\ndesiderata, we derive bears (BE Aware of Reasoning Shortcuts), an ensembling\ntechnique that calibrates the model's concept-level confidence without\ncompromising prediction accuracy, thus encouraging NeSy architectures to be\nuncertain about concepts affected by RSs. We show empirically that bears\nimproves RS-awareness of several state-of-the-art NeSy models, and also\nfacilitates acquiring informative dense annotations for mitigation purposes.",
      "tldr_zh": "这项研究探讨了Neuro-Symbolic (NeSy) 模型在遵守符号知识（如安全约束）时，可能因Reasoning Shortcuts (RSs) 而利用非预期语义，导致可靠性差、泛化能力弱并出现过度自信的问题。作者提出BEARS（BE Aware of Reasoning Shortcuts），这是一种集成技术，用于校准模型的概念级自信度，使其意识到概念的语义模糊，从而帮助用户识别和不信任受RSs 影响的低质量概念，而不降低预测准确性。实验结果显示，BEARS 显著提高了多种state-of-the-art NeSy 模型的RS 意识，并有助于获取用于缓解的密集注释数据。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2402.12240v1",
      "published_date": "2024-02-19 15:54:36 UTC",
      "updated_date": "2024-02-19 15:54:36 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T07:46:44.804651"
    },
    {
      "arxiv_id": "2402.12237v3",
      "title": "Learning to Defer in Content Moderation: The Human-AI Interplay",
      "title_zh": "内容审核中的学习递延：人类-AI 互动",
      "authors": [
        "Thodoris Lykouris",
        "Wentao Weng"
      ],
      "abstract": "Successful content moderation in online platforms relies on a human-AI\ncollaboration approach. A typical heuristic estimates the expected harmfulness\nof a post and uses fixed thresholds to decide whether to remove it and whether\nto send it for human review. This disregards the prediction uncertainty, the\ntime-varying element of human review capacity and post arrivals, and the\nselective sampling in the dataset (humans only review posts filtered by the\nadmission algorithm).\n  In this paper, we introduce a model to capture the human-AI interplay in\ncontent moderation. The algorithm observes contextual information for incoming\nposts, makes classification and admission decisions, and schedules posts for\nhuman review. Only admitted posts receive human reviews on their harmfulness.\nThese reviews help educate the machine-learning algorithms but are delayed due\nto congestion in the human review system. The classical learning-theoretic way\nto capture this human-AI interplay is via the framework of learning to defer,\nwhere the algorithm has the option to defer a classification task to humans for\na fixed cost and immediately receive feedback. Our model contributes to this\nliterature by introducing congestion in the human review system. Moreover,\nunlike work on online learning with delayed feedback where the delay in the\nfeedback is exogenous to the algorithm's decisions, the delay in our model is\nendogenous to both the admission and the scheduling decisions.\n  We propose a near-optimal learning algorithm that carefully balances the\nclassification loss from a selectively sampled dataset, the idiosyncratic loss\nof non-reviewed posts, and the delay loss of having congestion in the human\nreview system. To the best of our knowledge, this is the first result for\nonline learning in contextual queueing systems and hence our analytical\nframework may be of independent interest.",
      "tldr_zh": "这篇论文探讨了在线平台内容审核中人类-AI 协作的挑战，强调传统方法忽略了预测不确定性、人力审查容量变化以及数据集的选择性采样。作者提出一个新模型，允许算法基于上下文信息进行分类、审核决策和调度，同时处理人类审查系统的拥堵问题，使反馈延迟成为算法决策的内生因素。与经典的 learning to defer 框架不同，该模型平衡了分类损失、非审查帖子的特异损失以及拥堵导致的延迟损失。实验结果显示，该近优化的学习算法是 contextual queueing systems 中在线学习的首次应用，具有重要创新价值。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.GT",
        "cs.HC",
        "cs.PF"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2402.12237v3",
      "published_date": "2024-02-19 15:47:47 UTC",
      "updated_date": "2024-06-02 16:02:24 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T07:46:57.061890"
    },
    {
      "arxiv_id": "2402.12232v1",
      "title": "Kernel KMeans clustering splits for end-to-end unsupervised decision trees",
      "title_zh": "翻译失败",
      "authors": [
        "Louis Ohl",
        "Pierre-Alexandre Mattei",
        "Mickaël Leclercq",
        "Arnaud Droit",
        "Frédéric Precioso"
      ],
      "abstract": "Trees are convenient models for obtaining explainable predictions on\nrelatively small datasets. Although there are many proposals for the end-to-end\nconstruction of such trees in supervised learning, learning a tree end-to-end\nfor clustering without labels remains an open challenge. As most works focus on\ninterpreting with trees the result of another clustering algorithm, we present\nhere a novel end-to-end trained unsupervised binary tree for clustering: Kauri.\nThis method performs a greedy maximisation of the kernel KMeans objective\nwithout requiring the definition of centroids. We compare this model on\nmultiple datasets with recent unsupervised trees and show that Kauri performs\nidentically when using a linear kernel. For other kernels, Kauri often\noutperforms the concatenation of kernel KMeans and a CART decision tree.",
      "tldr_zh": "该论文提出了一种新的端到端无监督二元树模型Kauri，用于聚类任务，旨在解决无标签数据下决策树的构建挑战。Kauri通过贪婪最大化kernel KMeans目标来实现聚类划分，而不需要定义中心点，从而提高了模型的灵活性。在多个数据集上的实验表明，Kauri在使用线性kernel时与现有无监督树表现相当，而在使用其他kernel时，通常优于kernel KMeans与CART决策树的组合。",
      "categories": [
        "stat.ML",
        "cs.AI",
        "cs.LG",
        "62h30",
        "G.3"
      ],
      "primary_category": "stat.ML",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2402.12232v1",
      "published_date": "2024-02-19 15:39:39 UTC",
      "updated_date": "2024-02-19 15:39:39 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T07:47:09.261724"
    },
    {
      "arxiv_id": "2402.14847v1",
      "title": "Deep learning-driven scheduling algorithm for a single machine problem minimizing the total tardiness",
      "title_zh": "翻译失败",
      "authors": [
        "Michal Bouška",
        "Přemysl Šůcha",
        "Antonín Novák",
        "Zdeněk Hanzálek"
      ],
      "abstract": "In this paper, we investigate the use of the deep learning method for solving\na well-known NP-hard single machine scheduling problem with the objective of\nminimizing the total tardiness. We propose a deep neural network that acts as a\npolynomial-time estimator of the criterion value used in a single-pass\nscheduling algorithm based on Lawler's decomposition and symmetric\ndecomposition proposed by Della Croce et al. Essentially, the neural network\nguides the algorithm by estimating the best splitting of the problem into\nsubproblems. The paper also describes a new method for generating the training\ndata set, which speeds up the training dataset generation and reduces the\naverage optimality gap of solutions. The experimental results show that our\nmachine learning-driven approach can efficiently generalize information from\nthe training phase to significantly larger instances. Even though the instances\nused in the training phase have from 75 to 100 jobs, the average optimality gap\non instances with up to 800 jobs is 0.26%, which is almost five times less than\nthe gap of the state-of-the-art heuristic.",
      "tldr_zh": "本文研究使用深度学习方法解决NP-hard的单机调度问题，目标是最小化总延误。作者提出一个深度神经网络作为多项式时间估算器，指导基于Lawler's decomposition和对称分解的单次调度算法，通过估计最佳问题分割来优化流程。论文还引入了一种新的训练数据生成方法，加快数据集创建并降低解决方案的平均最优差距。实验结果表明，该方法能高效泛化到更大实例（高达800作业），平均最优差距仅为0.26%，比最先进启发式方法低五倍。",
      "categories": [
        "math.OC",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "math.OC",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2402.14847v1",
      "published_date": "2024-02-19 15:34:09 UTC",
      "updated_date": "2024-02-19 15:34:09 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T07:47:23.181070"
    },
    {
      "arxiv_id": "2402.12226v3",
      "title": "AnyGPT: Unified Multimodal LLM with Discrete Sequence Modeling",
      "title_zh": "翻译失败",
      "authors": [
        "Jun Zhan",
        "Junqi Dai",
        "Jiasheng Ye",
        "Yunhua Zhou",
        "Dong Zhang",
        "Zhigeng Liu",
        "Xin Zhang",
        "Ruibin Yuan",
        "Ge Zhang",
        "Linyang Li",
        "Hang Yan",
        "Jie Fu",
        "Tao Gui",
        "Tianxiang Sun",
        "Yugang Jiang",
        "Xipeng Qiu"
      ],
      "abstract": "We introduce AnyGPT, an any-to-any multimodal language model that utilizes\ndiscrete representations for the unified processing of various modalities,\nincluding speech, text, images, and music. AnyGPT can be trained stably without\nany alterations to the current large language model (LLM) architecture or\ntraining paradigms. Instead, it relies exclusively on data-level preprocessing,\nfacilitating the seamless integration of new modalities into LLMs, akin to the\nincorporation of new languages. We build a multimodal text-centric dataset for\nmultimodal alignment pre-training. Utilizing generative models, we synthesize\nthe first large-scale any-to-any multimodal instruction dataset. It consists of\n108k samples of multi-turn conversations that intricately interweave various\nmodalities, thus equipping the model to handle arbitrary combinations of\nmultimodal inputs and outputs. Experimental results demonstrate that AnyGPT is\ncapable of facilitating any-to-any multimodal conversation while achieving\nperformance comparable to specialized models across all modalities, proving\nthat discrete representations can effectively and conveniently unify multiple\nmodalities within a language model. Demos are shown in\nhttps://junzhan2000.github.io/AnyGPT.github.io/",
      "tldr_zh": "论文提出 AnyGPT，一种基于 discrete sequence modeling 的统一多模态 LLM，能够无缝处理语音、文本、图像和音乐等多种模态，而无需修改 LLM 架构，仅通过数据级预处理实现模态整合。研究者构建了一个多模态文本中心数据集，并使用生成模型合成首个大规模 any-to-any 多模态指令数据集（包含108k多轮对话样本），以增强模型的多模态对齐和交互能力。实验结果表明，AnyGPT 在任意多模态输入输出组合中表现出色，与专业模型性能相当，证明 discrete representations 可有效统一多种模态。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.CV",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "28 pages, 16 figures, under review, work in progress",
      "pdf_url": "http://arxiv.org/pdf/2402.12226v3",
      "published_date": "2024-02-19 15:33:10 UTC",
      "updated_date": "2024-03-07 06:31:46 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T07:47:33.995760"
    },
    {
      "arxiv_id": "2402.12219v2",
      "title": "Reformatted Alignment",
      "title_zh": "翻译失败",
      "authors": [
        "Run-Ze Fan",
        "Xuefeng Li",
        "Haoyang Zou",
        "Junlong Li",
        "Shwai He",
        "Ethan Chern",
        "Jiewen Hu",
        "Pengfei Liu"
      ],
      "abstract": "The quality of finetuning data is crucial for aligning large language models\n(LLMs) with human values. Current methods to improve data quality are either\nlabor-intensive or prone to factual errors caused by LLM hallucinations. This\npaper explores elevating the quality of existing instruction data to better\nalign with human values, introducing a simple and effective approach named\nReAlign, which reformats the responses of instruction data into a format that\nbetter aligns with pre-established criteria and the collated evidence. This\napproach minimizes human annotation, hallucination, and the difficulty in\nscaling, remaining orthogonal to existing alignment techniques. Experimentally,\nReAlign significantly boosts the general alignment ability, math reasoning,\nfactuality, and readability of the LLMs.\n  Encouragingly, without introducing any additional data or advanced training\ntechniques, and merely by reformatting the response, LLaMA-2-13B's mathematical\nreasoning ability on GSM8K can be improved from 46.77% to 56.63% in accuracy.\nAdditionally, a mere 5% of ReAlign data yields a 67% boost in general alignment\nability measured by the Alpaca dataset. This work highlights the need for\nfurther research into the science and mechanistic interpretability of LLMs. We\nhave made the associated code and data publicly accessible to support future\nstudies at https://github.com/GAIR-NLP/ReAlign.",
      "tldr_zh": "这篇论文提出了 ReAlign 方法，通过重新格式化指令数据的响应，使其更好地与预设标准和证据对齐，从而提升大型语言模型 (LLMs) 与人类价值观的整体对齐质量。该方法简单有效，减少了人类标注、LLM 幻觉和扩展难度，与现有对齐技术正交。实验结果显示，ReAlign 显著提高了 LLMs 的数学推理、事实性和可读性，例如 LLaMA-2-13B 在 GSM8K 数据集上的准确率从 46.77% 提升至 56.63%，并仅使用 5% ReAlign 数据就使 Alpaca 数据集的整体对齐能力提升 67%。论文公开了代码和数据，呼吁进一步探索 LLMs 的科学和机制可解释性。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "Homepage: https://gair-nlp.github.io/ReAlign/",
      "pdf_url": "http://arxiv.org/pdf/2402.12219v2",
      "published_date": "2024-02-19 15:21:58 UTC",
      "updated_date": "2024-04-17 15:03:19 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T07:47:48.390695"
    },
    {
      "arxiv_id": "2402.12216v1",
      "title": "Copyleft for Alleviating AIGC Copyright Dilemma: What-if Analysis, Public Perception and Implications",
      "title_zh": "翻译失败",
      "authors": [
        "Xinwei Guo",
        "Yujun Li",
        "Yafeng Peng",
        "Xuetao Wei"
      ],
      "abstract": "As AIGC has impacted our society profoundly in the past years, ethical issues\nhave received tremendous attention. The most urgent one is the AIGC copyright\ndilemma, which can immensely stifle the development of AIGC and greatly cost\nthe entire society. Given the complexity of AIGC copyright governance and the\nfact that no perfect solution currently exists, previous work advocated\ncopyleft on AI governance but without substantive analysis. In this paper, we\ntake a step further to explore the feasibility of copyleft to alleviate the\nAIGC copyright dilemma. We conduct a mixed-methods study from two aspects:\nqualitatively, we use a formal what-if analysis to clarify the dilemma and\nprovide case studies to show the feasibility of copyleft; quantitatively, we\nperform a carefully designed survey to find out how the public feels about\ncopylefting AIGC. The key findings include: a) people generally perceive the\ndilemma, b) they prefer to use authorized AIGC under loose restriction, and c)\nthey are positive to copyleft in AIGC and willing to use it in the future.",
      "tldr_zh": "本文探讨了 copyleft 在缓解 AIGC 版权困境中的可行性，针对 AIGC（AI 生成内容）发展可能受阻的道德问题进行分析。研究采用混合方法，包括定性 what-if analysis 和案例研究来阐明困境，以及定量公众调查来评估感知和态度。关键发现显示，人们普遍认识到 AIGC 版权困境，倾向于在宽松限制下使用授权 AIGC，并对引入 copyleft 持积极态度，愿意在未来采用。总之，此研究为 AIGC 治理提供了实际启示，以促进其可持续发展。",
      "categories": [
        "cs.CY",
        "cs.AI"
      ],
      "primary_category": "cs.CY",
      "comment": "9 pages, 8 figures",
      "pdf_url": "http://arxiv.org/pdf/2402.12216v1",
      "published_date": "2024-02-19 15:20:35 UTC",
      "updated_date": "2024-02-19 15:20:35 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T07:47:58.845582"
    },
    {
      "arxiv_id": "2402.12202v1",
      "title": "Heterogeneity-aware Cross-school Electives Recommendation: a Hybrid Federated Approach",
      "title_zh": "翻译失败",
      "authors": [
        "Chengyi Ju",
        "Jiannong Cao",
        "Yu Yang",
        "Zhen-Qun Yang",
        "Ho Man Lee"
      ],
      "abstract": "In the era of modern education, addressing cross-school learner diversity is\ncrucial, especially in personalized recommender systems for elective course\nselection. However, privacy concerns often limit cross-school data sharing,\nwhich hinders existing methods' ability to model sparse data and address\nheterogeneity effectively, ultimately leading to suboptimal recommendations. In\nresponse, we propose HFRec, a heterogeneity-aware hybrid federated recommender\nsystem designed for cross-school elective course recommendations. The proposed\nmodel constructs heterogeneous graphs for each school, incorporating various\ninteractions and historical behaviors between students to integrate context and\ncontent information. We design an attention mechanism to capture\nheterogeneity-aware representations. Moreover, under a federated scheme, we\ntrain individual school-based models with adaptive learning settings to\nrecommend tailored electives. Our HFRec model demonstrates its effectiveness in\nproviding personalized elective recommendations while maintaining privacy, as\nit outperforms state-of-the-art models on both open-source and real-world\ndatasets.",
      "tldr_zh": "该研究提出HFRec，一种异质性感知的混合联邦推荐系统，旨在解决跨学校选修课推荐中的隐私问题和数据异质性挑战。HFRec为每个学校构建异质图，整合学生互动和历史行为，并通过注意力机制捕捉异质性感知的表示，同时在联邦学习框架下采用自适应学习设置来训练模型。实验结果显示，HFRec在开源和真实数据集上优于现有最先进模型，提供更个性化的推荐，同时保护数据隐私。",
      "categories": [
        "cs.IR",
        "cs.AI"
      ],
      "primary_category": "cs.IR",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2402.12202v1",
      "published_date": "2024-02-19 15:06:04 UTC",
      "updated_date": "2024-02-19 15:06:04 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T07:48:08.843366"
    },
    {
      "arxiv_id": "2402.14846v4",
      "title": "Stick to your Role! Stability of Personal Values Expressed in Large Language Models",
      "title_zh": "翻译失败",
      "authors": [
        "Grgur Kovač",
        "Rémy Portelas",
        "Masataka Sawayama",
        "Peter Ford Dominey",
        "Pierre-Yves Oudeyer"
      ],
      "abstract": "The standard way to study Large Language Models (LLMs) with benchmarks or\npsychology questionnaires is to provide many different queries from similar\nminimal contexts (e.g. multiple choice questions). However, due to LLMs' highly\ncontext-dependent nature, conclusions from such minimal-context evaluations may\nbe little informative about the model's behavior in deployment (where it will\nbe exposed to many new contexts). We argue that context-dependence\n(specifically, value stability) should be studied as a specific property of\nLLMs and used as another dimension of LLM comparison (alongside others such as\ncognitive abilities, knowledge, or model size). We present a case-study on the\nstability of value expression over different contexts (simulated conversations\non different topics) as measured using a standard psychology questionnaire\n(PVQ) and on behavioral downstream tasks. Reusing methods from psychology, we\nstudy Rank-order stability on the population (interpersonal) level, and\nIpsative stability on the individual (intrapersonal) level. We consider two\nsettings (with and without instructing LLMs to simulate particular personas),\ntwo simulated populations, and three downstream tasks. We observe consistent\ntrends in the stability of models and model families - Mixtral, Mistral,\nGPT-3.5 and Qwen families are more stable than LLaMa-2 and Phi. The consistency\nof these trends implies that some models exhibit higher value stability than\nothers, and that stability can be estimated with the set of introduced\nmethodological tools. When instructed to simulate particular personas, LLMs\nexhibit low Rank-order stability, which further diminishes with conversation\nlength. This highlights the need for future research on LLMs that coherently\nsimulate different personas. This paper provides a foundational step in that\ndirection, and, to our knowledge, it is the first study of value stability in\nLLMs.",
      "tldr_zh": "这篇论文探讨了Large Language Models (LLMs) 在不同上下文中的价值稳定性问题，指出传统评估方法（如基准测试或PVQ问卷）因依赖最小上下文而无法反映实际部署场景。研究者通过模拟对话和下游任务，评估了Rank-order stability（人口间水平）和Ipsative stability（个体间水平），并比较了多种模型（如Mixtral、Mistral、GPT-3.5和Qwen系列比LLaMa-2和Phi更稳定）。当指令LLMs模拟特定角色时，稳定性显著降低，随对话长度进一步减弱，这强调了未来需加强LLMs在角色模拟方面的研究，并为评估价值稳定性提供了新方法工具。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG",
        "68T07",
        "I.2.7"
      ],
      "primary_category": "cs.CL",
      "comment": "The project website and code are available at\n  https://sites.google.com/view/llmvaluestability Published in PLOS ONE (\n  https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0309114 ),\n  and a shorter version at CogSci 24 (\n  https://escholarship.org/uc/item/7w4823c6 )",
      "pdf_url": "http://arxiv.org/pdf/2402.14846v4",
      "published_date": "2024-02-19 14:53:01 UTC",
      "updated_date": "2024-08-28 14:04:05 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T07:48:23.632902"
    },
    {
      "arxiv_id": "2402.12183v1",
      "title": "MultiFIX: An XAI-friendly feature inducing approach to building models from multimodal data",
      "title_zh": "翻译失败",
      "authors": [
        "Mafalda Malafaia",
        "Thalea Schlender",
        "Peter A. N. Bosman",
        "Tanja Alderliesten"
      ],
      "abstract": "In the health domain, decisions are often based on different data modalities.\nThus, when creating prediction models, multimodal fusion approaches that can\nextract and combine relevant features from different data modalities, can be\nhighly beneficial. Furthermore, it is important to understand how each modality\nimpacts the final prediction, especially in high-stake domains, so that these\nmodels can be used in a trustworthy and responsible manner. We propose\nMultiFIX: a new interpretability-focused multimodal data fusion pipeline that\nexplicitly induces separate features from different data types that can\nsubsequently be combined to make a final prediction. An end-to-end deep\nlearning architecture is used to train a predictive model and extract\nrepresentative features of each modality. Each part of the model is then\nexplained using explainable artificial intelligence techniques. Attention maps\nare used to highlight important regions in image inputs. Inherently\ninterpretable symbolic expressions, learned with GP-GOMEA, are used to describe\nthe contribution of tabular inputs. The fusion of the extracted features to\npredict the target label is also replaced by a symbolic expression, learned\nwith GP-GOMEA. Results on synthetic problems demonstrate the strengths and\nlimitations of MultiFIX. Lastly, we apply MultiFIX to a publicly available\ndataset for the detection of malignant skin lesions.",
      "tldr_zh": "该论文提出 MultiFIX，这是一种可解释性友好（XAI-friendly）的特征诱导方法，用于从多模态数据构建预测模型，特别适用于健康领域的高风险决策。MultiFIX 通过端到端深度学习架构显式提取不同数据类型（如图像和表格）的代表性特征，并使用注意力图（attention maps）突出图像的重要区域，同时采用 GP-GOMEA 学习可解释的符号表达式来描述表格输入和特征融合的贡献。实验结果显示，该方法在合成问题上揭示了其优势和局限性，并在公开皮肤病变检测数据集上有效提升了模型的可信度和性能。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "8 pages, 9 figures",
      "pdf_url": "http://arxiv.org/pdf/2402.12183v1",
      "published_date": "2024-02-19 14:45:46 UTC",
      "updated_date": "2024-02-19 14:45:46 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T07:48:32.993095"
    },
    {
      "arxiv_id": "2402.12181v1",
      "title": "Revisiting Data Augmentation in Deep Reinforcement Learning",
      "title_zh": "翻译失败",
      "authors": [
        "Jianshu Hu",
        "Yunpeng Jiang",
        "Paul Weng"
      ],
      "abstract": "Various data augmentation techniques have been recently proposed in\nimage-based deep reinforcement learning (DRL). Although they empirically\ndemonstrate the effectiveness of data augmentation for improving sample\nefficiency or generalization, which technique should be preferred is not always\nclear. To tackle this question, we analyze existing methods to better\nunderstand them and to uncover how they are connected. Notably, by expressing\nthe variance of the Q-targets and that of the empirical actor/critic losses of\nthese methods, we can analyze the effects of their different components and\ncompare them. We furthermore formulate an explanation about how these methods\nmay be affected by choosing different data augmentation transformations in\ncalculating the target Q-values. This analysis suggests recommendations on how\nto exploit data augmentation in a more principled way. In addition, we include\na regularization term called tangent prop, previously proposed in computer\nvision, but whose adaptation to DRL is novel to the best of our knowledge. We\nevaluate our proposition and validate our analysis in several domains. Compared\nto different relevant baselines, we demonstrate that it achieves\nstate-of-the-art performance in most environments and shows higher sample\nefficiency and better generalization ability in some complex environments.",
      "tldr_zh": "本论文重新审视了数据增强（data augmentation）在图像-based 深度强化学习（DRL）中的应用，通过分析现有方法的 Q-targets 方差和经验 actor/critic 损失来比较其效果，并探讨不同增强变换对目标 Q-values 的影响。作者提出了一种新的正则化项 tangent prop，并结合分析结果给出更原则性的数据增强建议。实验结果显示，该方法在多个环境中实现了 state-of-the-art 性能，显著提高了样本效率（sample efficiency）和泛化能力（generalization ability）。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.LG",
      "comment": "Accepted in ICLR 2024",
      "pdf_url": "http://arxiv.org/pdf/2402.12181v1",
      "published_date": "2024-02-19 14:42:10 UTC",
      "updated_date": "2024-02-19 14:42:10 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T07:48:46.098226"
    },
    {
      "arxiv_id": "2402.12179v1",
      "title": "Examining Monitoring System: Detecting Abnormal Behavior In Online Examinations",
      "title_zh": "翻译失败",
      "authors": [
        "Dinh An Ngo",
        "Thanh Dat Nguyen",
        "Thi Le Chi Dang",
        "Huy Hoan Le",
        "Ton Bao Ho",
        "Vo Thanh Khang Nguyen",
        "Truong Thanh Hung Nguyen"
      ],
      "abstract": "Cheating in online exams has become a prevalent issue over the past decade,\nespecially during the COVID-19 pandemic. To address this issue of academic\ndishonesty, our \"Exam Monitoring System: Detecting Abnormal Behavior in Online\nExaminations\" is designed to assist proctors in identifying unusual student\nbehavior. Our system demonstrates high accuracy and speed in detecting cheating\nin real-time scenarios, providing valuable information, and aiding proctors in\ndecision-making. This article outlines our methodology and the effectiveness of\nour system in mitigating the widespread problem of cheating in online exams.",
      "tldr_zh": "本研究针对在线考试中的作弊问题（如COVID-19期间的学术不诚实），开发了Exam Monitoring System，用于实时检测学生的abnormal behavior。系统通过高准确性和快速响应机制，帮助监考者识别异常行为并提供决策支持。实验结果显示，该系统在减轻作弊问题方面表现出色，有效提升了在线考试的诚信水平。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CY"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2402.12179v1",
      "published_date": "2024-02-19 14:37:17 UTC",
      "updated_date": "2024-02-19 14:37:17 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T07:48:57.974869"
    },
    {
      "arxiv_id": "2402.12177v4",
      "title": "Mafin: Enhancing Black-Box Embeddings with Model Augmented Fine-Tuning",
      "title_zh": "Mafin: 通过模型增强微调提升黑盒嵌入",
      "authors": [
        "Mingtian Zhang",
        "Shawn Lan",
        "Peter Hayes",
        "David Barber"
      ],
      "abstract": "Retrieval Augmented Generation (RAG) has emerged as an effective solution for\nmitigating hallucinations in Large Language Models (LLMs). The retrieval stage\nin RAG typically involves a pre-trained embedding model, which converts queries\nand passages into vectors to capture their semantics. However, a standard\npre-trained embedding model may exhibit sub-optimal performance when applied to\nspecific domain knowledge, necessitating fine-tuning. This paper addresses\nscenarios where the embeddings are only available from a black-box model. We\nintroduce Model augmented fine-tuning (Mafin) -- a novel approach for\nfine-tuning a black-box embedding model by augmenting it with a trainable\nembedding model. Our results demonstrate that Mafin significantly enhances the\nperformance of the black-box embeddings by only requiring the training of a\nsmall augmented model. We validate the effectiveness of our method on both\nlabeled and unlabeled datasets, illustrating its broad applicability and\nefficiency.",
      "tldr_zh": "本研究针对Retrieval Augmented Generation (RAG)中黑盒嵌入模型在特定领域性能不足的问题，提出了一种创新方法Model Augmented Fine-Tuning (Mafin)。Mafin通过添加一个小型可训练嵌入模型来增强黑盒模型，从而实现有效的微调，而无需访问黑盒模型的内部参数。实验结果显示，该方法显著提升了黑盒嵌入模型的性能，并在标记和未标记数据集上验证了其广泛适用性和高效性。总的来说，Mafin为优化LLMs中的检索阶段提供了实用解决方案。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2402.12177v4",
      "published_date": "2024-02-19 14:33:24 UTC",
      "updated_date": "2024-03-12 16:04:23 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T07:49:10.195349"
    },
    {
      "arxiv_id": "2402.12168v3",
      "title": "Defending Against Weight-Poisoning Backdoor Attacks for Parameter-Efficient Fine-Tuning",
      "title_zh": "翻译失败",
      "authors": [
        "Shuai Zhao",
        "Leilei Gan",
        "Luu Anh Tuan",
        "Jie Fu",
        "Lingjuan Lyu",
        "Meihuizi Jia",
        "Jinming Wen"
      ],
      "abstract": "Recently, various parameter-efficient fine-tuning (PEFT) strategies for\napplication to language models have been proposed and successfully implemented.\nHowever, this raises the question of whether PEFT, which only updates a limited\nset of model parameters, constitutes security vulnerabilities when confronted\nwith weight-poisoning backdoor attacks. In this study, we show that PEFT is\nmore susceptible to weight-poisoning backdoor attacks compared to the\nfull-parameter fine-tuning method, with pre-defined triggers remaining\nexploitable and pre-defined targets maintaining high confidence, even after\nfine-tuning. Motivated by this insight, we developed a Poisoned Sample\nIdentification Module (PSIM) leveraging PEFT, which identifies poisoned samples\nthrough confidence, providing robust defense against weight-poisoning backdoor\nattacks. Specifically, we leverage PEFT to train the PSIM with randomly reset\nsample labels. During the inference process, extreme confidence serves as an\nindicator for poisoned samples, while others are clean. We conduct experiments\non text classification tasks, five fine-tuning strategies, and three\nweight-poisoning backdoor attack methods. Experiments show near 100% success\nrates for weight-poisoning backdoor attacks when utilizing PEFT. Furthermore,\nour defensive approach exhibits overall competitive performance in mitigating\nweight-poisoning backdoor attacks.",
      "tldr_zh": "这篇论文发现，参数高效微调 (PEFT) 比全参数微调更容易受到权重投毒后门攻击 (weight-poisoning backdoor attacks)，因为攻击触发器和目标在微调后仍能保持高置信度。研究团队开发了 Poisoned Sample Identification Module (PSIM)，利用 PEFT 训练方法通过随机重置样本标签并基于置信度来识别投毒样本，从而提供有效的防御机制。在文本分类任务的实验中，PEFT 攻击成功率接近100%，而 PSIM 防御方法表现出色，能够显著缓解这些攻击。",
      "categories": [
        "cs.CR",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.CR",
      "comment": "NAACL Findings 2024",
      "pdf_url": "http://arxiv.org/pdf/2402.12168v3",
      "published_date": "2024-02-19 14:22:54 UTC",
      "updated_date": "2024-03-29 12:12:30 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T07:49:22.853603"
    },
    {
      "arxiv_id": "2402.12161v2",
      "title": "Endowing Pre-trained Graph Models with Provable Fairness",
      "title_zh": "为预训练图模型赋予可证明的公平性",
      "authors": [
        "Zhongjian Zhang",
        "Mengmei Zhang",
        "Yue Yu",
        "Cheng Yang",
        "Jiawei Liu",
        "Chuan Shi"
      ],
      "abstract": "Pre-trained graph models (PGMs) aim to capture transferable inherent\nstructural properties and apply them to different downstream tasks. Similar to\npre-trained language models, PGMs also inherit biases from human society,\nresulting in discriminatory behavior in downstream applications. The debiasing\nprocess of existing fair methods is generally coupled with parameter\noptimization of GNNs. However, different downstream tasks may be associated\nwith different sensitive attributes in reality, directly employing existing\nmethods to improve the fairness of PGMs is inflexible and inefficient.\nMoreover, most of them lack a theoretical guarantee, i.e., provable lower\nbounds on the fairness of model predictions, which directly provides assurance\nin a practical scenario. To overcome these limitations, we propose a novel\nadapter-tuning framework that endows pre-trained graph models with provable\nfairness (called GraphPAR). GraphPAR freezes the parameters of PGMs and trains\na parameter-efficient adapter to flexibly improve the fairness of PGMs in\ndownstream tasks. Specifically, we design a sensitive semantic augmenter on\nnode representations, to extend the node representations with different\nsensitive attribute semantics for each node. The extended representations will\nbe used to further train an adapter, to prevent the propagation of sensitive\nattribute semantics from PGMs to task predictions. Furthermore, with GraphPAR,\nwe quantify whether the fairness of each node is provable, i.e., predictions\nare always fair within a certain range of sensitive attribute semantics.\nExperimental evaluations on real-world datasets demonstrate that GraphPAR\nachieves state-of-the-art prediction performance and fairness on node\nclassification task. Furthermore, based on our GraphPAR, around 90\\% nodes have\nprovable fairness.",
      "tldr_zh": "该研究针对预训练图模型 (PGMs) 继承社会偏见导致下游任务歧视的问题，提出了一种名为 GraphPAR 的新型 adapter-tuning 框架，以实现可证明的公平性 (provable fairness)。GraphPAR 通过冻结 PGMs 参数并训练高效的 adapter，包括敏感语义增强器 (sensitive semantic augmenter) 来扩展节点表示，并防止敏感属性语义在预测中传播，从而灵活提升不同任务的公平性。实验结果显示，在真实数据集上的节点分类任务中，GraphPAR 达到了最先进的预测性能，并使约 90% 的节点具备可证明公平性。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CY",
        "cs.SI"
      ],
      "primary_category": "cs.LG",
      "comment": "Accepted by WWW 2024",
      "pdf_url": "http://arxiv.org/pdf/2402.12161v2",
      "published_date": "2024-02-19 14:16:08 UTC",
      "updated_date": "2024-02-20 09:03:43 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T07:49:33.917862"
    },
    {
      "arxiv_id": "2402.12151v2",
      "title": "Transformer-based Causal Language Models Perform Clustering",
      "title_zh": "翻译失败",
      "authors": [
        "Xinbo Wu",
        "Lav R. Varshney"
      ],
      "abstract": "Even though large language models (LLMs) have demonstrated remarkable\ncapability in solving various natural language tasks, the capability of an LLM\nto follow human instructions is still a concern. Recent works have shown great\nimprovements in the instruction-following capability via additional training\nfor instruction-following tasks. However, the mechanisms responsible for\neffective instruction-following capabilities remain inadequately understood.\nHere, we introduce a simplified instruction-following task and use synthetic\ndatasets to analyze a Transformer-based causal language model. Our findings\nsuggest that the model learns task-specific information by clustering data\nwithin its hidden space, with this clustering process evolving dynamically\nduring learning. We also demonstrate how this phenomenon assists the model in\nhandling unseen instances, and validate our results in a more realistic\nsetting. Furthermore, we present inspired applications regarding pre-training\nand alignment.",
      "tldr_zh": "本文研究发现，Transformer-based causal language models 在学习指令遵循任务时，会通过在隐藏空间中聚类数据来获取任务特定信息，这一聚类过程在训练中动态演化。作者使用简化指令任务和合成数据集分析模型，证明这种机制有助于模型处理未见过实例，并在更现实场景下验证了结果。该发现为LLMs的预训练和对齐等应用提供了新启发。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "Added new experimental results and fixed some errors",
      "pdf_url": "http://arxiv.org/pdf/2402.12151v2",
      "published_date": "2024-02-19 14:02:31 UTC",
      "updated_date": "2024-03-03 20:06:24 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T07:49:46.016855"
    },
    {
      "arxiv_id": "2402.12150v1",
      "title": "Your Large Language Model is Secretly a Fairness Proponent and You Should Prompt it Like One",
      "title_zh": "翻译失败",
      "authors": [
        "Tianlin Li",
        "Xiaoyu Zhang",
        "Chao Du",
        "Tianyu Pang",
        "Qian Liu",
        "Qing Guo",
        "Chao Shen",
        "Yang Liu"
      ],
      "abstract": "The widespread adoption of large language models (LLMs) underscores the\nurgent need to ensure their fairness. However, LLMs frequently present dominant\nviewpoints while ignoring alternative perspectives from minority parties,\nresulting in potential biases. We hypothesize that these fairness-violating\nbehaviors occur because LLMs express their viewpoints using a human personality\nthat represents the majority of training data. In response to this, we validate\nthat prompting LLMs with specific roles can allow LLMs to express diverse\nviewpoints. Building on this insight and observation, we develop FairThinking,\na pipeline designed to automatically generate roles that enable LLMs to\narticulate diverse perspectives for fair expressions. To evaluate FairThinking,\nwe create a dataset with a thousand items covering three fairness-related\ntopics and conduct experiments on GPT-3.5, GPT-4, Llama2, and Mistral to\ndemonstrate its superior performance.",
      "tldr_zh": "该论文揭示了大型语言模型（LLMs）在表达观点时往往偏向主流视角而忽略少数派，导致公平性问题，并假设这是由于LLMs模仿训练数据中多数“人格”所致。为解决这一问题，研究者开发了FairThinking管道，该系统能自动生成特定角色提示，帮助LLMs表达多样视角，从而提升公平性。实验在包含一千项数据的自定义数据集上进行，证明FairThinking在GPT-3.5、GPT-4、Llama2和Mistral模型上表现出色，显著改善了模型的公平表现。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "I.2; J.4"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2402.12150v1",
      "published_date": "2024-02-19 14:02:22 UTC",
      "updated_date": "2024-02-19 14:02:22 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T07:49:58.006849"
    },
    {
      "arxiv_id": "2402.14845v1",
      "title": "Purifying Large Language Models by Ensembling a Small Language Model",
      "title_zh": "翻译失败",
      "authors": [
        "Tianlin Li",
        "Qian Liu",
        "Tianyu Pang",
        "Chao Du",
        "Qing Guo",
        "Yang Liu",
        "Min Lin"
      ],
      "abstract": "The emerging success of large language models (LLMs) heavily relies on\ncollecting abundant training data from external (untrusted) sources. Despite\nsubstantial efforts devoted to data cleaning and curation, well-constructed\nLLMs have been reported to suffer from copyright infringement, data poisoning,\nand/or privacy violations, which would impede practical deployment of LLMs. In\nthis study, we propose a simple and easily implementable method for purifying\nLLMs from the negative effects caused by uncurated data, namely, through\nensembling LLMs with benign and small language models (SLMs). Aside from\ntheoretical guarantees, we perform comprehensive experiments to empirically\nconfirm the efficacy of ensembling LLMs with SLMs, which can effectively\npreserve the performance of LLMs while mitigating issues such as copyright\ninfringement, data poisoning, and privacy violations.",
      "tldr_zh": "该研究针对大型语言模型（LLMs）因依赖外部未净化数据而面临的版权侵犯、数据中毒和隐私泄露等问题，提出了一种简单的方法：通过将LLMs与良性小语言模型（SLMs）进行集成（ensembling）。这种方法能够在理论上保证净化效果，同时通过实验验证，证明它能有效保留LLMs的性能。实验结果显示，该集成策略显著缓解了上述负面影响，为LLMs的实际部署提供了更安全可靠的解决方案。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG",
        "I.2"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2402.14845v1",
      "published_date": "2024-02-19 14:00:39 UTC",
      "updated_date": "2024-02-19 14:00:39 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T07:50:09.765821"
    },
    {
      "arxiv_id": "2402.12147v3",
      "title": "Surprising Efficacy of Fine-Tuned Transformers for Fact-Checking over Larger Language Models",
      "title_zh": "翻译失败",
      "authors": [
        "Vinay Setty"
      ],
      "abstract": "In this paper, we explore the challenges associated with establishing an\nend-to-end fact-checking pipeline in a real-world context, covering over 90\nlanguages. Our real-world experimental benchmarks demonstrate that fine-tuning\nTransformer models specifically for fact-checking tasks, such as claim\ndetection and veracity prediction, provide superior performance over large\nlanguage models (LLMs) like GPT-4, GPT-3.5-Turbo, and Mistral-7b. However, we\nillustrate that LLMs excel in generative tasks such as question decomposition\nfor evidence retrieval. Through extensive evaluation, we show the efficacy of\nfine-tuned models for fact-checking in a multilingual setting and complex\nclaims that include numerical quantities.",
      "tldr_zh": "本文研究了在真实世界多语言（覆盖90多种）事实检查管道中的挑战，发现微调Transformer模型在声明检测和真实性预测任务上比大型语言模型（LLMs）如GPT-4、GPT-3.5-Turbo和Mistral-7b表现出色，提升了性能。实验结果显示，微调模型在处理多语言和包含数字量的复杂声明时更有效，而LLMs在生成任务如证据检索的问题分解上更具优势。通过广泛评估，该方法证明了微调Transformer在事实检查领域的惊人效能，为构建端到端事实检查系统提供了新见解。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "Accepted in SIGIR 2024 (industry track)",
      "pdf_url": "http://arxiv.org/pdf/2402.12147v3",
      "published_date": "2024-02-19 14:00:35 UTC",
      "updated_date": "2024-04-30 08:56:18 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T07:50:24.489502"
    },
    {
      "arxiv_id": "2402.12146v3",
      "title": "Enabling Weak LLMs to Judge Response Reliability via Meta Ranking",
      "title_zh": "翻译失败",
      "authors": [
        "Zijun Liu",
        "Boqun Kou",
        "Peng Li",
        "Ming Yan",
        "Ji Zhang",
        "Fei Huang",
        "Yang Liu"
      ],
      "abstract": "Despite the strong performance of large language models (LLMs) across a wide\nrange of tasks, they still have reliability issues. Previous studies indicate\nthat strong LLMs like GPT-4-turbo excel in evaluating the reliability of\nresponses from LLMs, but face efficiency and local deployment issues. Thus, to\nenable weak LLMs to effectively assess the reliability of LLM responses, we\npropose a novel cross-query-comparison-based method called $\\textit{Meta\nRanking}$ (MR). Unlike previous few-shot methods that solely based on\nin-context learning capabilities in LLMs, MR assesses reliability by pairwisely\nranking the target query-response pair with multiple reference query-response\npairs. We found that MR is highly effective in error detection for LLM\nresponses, where weak LLMs, such as Phi-2, could surpass strong baselines like\nGPT-3.5-turbo, requiring only five reference samples and significantly\nimproving efficiency. We further demonstrate that MR can enhance strong LLMs'\nperformance in two practical applications: model cascading and instruction\ntuning. In model cascading, we combine open- and closed-source LLMs to achieve\nperformance comparable to GPT-4-turbo with lower costs. In instruction tuning,\nwe use MR for iterative training data filtering, significantly reducing data\nprocessing time and enabling LLaMA-7B and Phi-2 to surpass Alpaca-13B with\nfewer training tokens. These results underscore the high potential of MR in\nboth efficiency and effectiveness.",
      "tldr_zh": "本篇论文提出了一种名为 Meta Ranking (MR) 的新方法，旨在让弱大型语言模型 (LLMs) 通过跨查询比较评估其他 LLMs 响应的可靠性，从而解决强 LLMs 如 GPT-4-turbo 在效率和本地部署方面的局限性。MR 通过成对排名目标查询-响应对与多个参考查询-响应对，实现了高效的错误检测，而非依赖传统的 few-shot in-context learning，仅需五个参考样本，弱 LLMs 如 Phi-2 即可超越强基线如 GPT-3.5-turbo。实验结果显示，MR 在模型级联中结合开源和闭源 LLMs 达到与 GPT-4-turbo 相当的性能但成本更低，在指令微调中用于数据过滤，帮助 LLaMA-7B 和 Phi-2 使用更少训练标记超越 Alpaca-13B，从而显著提升了 LLMs 的整体效率和有效性。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "Preprint, under review. 28 pages",
      "pdf_url": "http://arxiv.org/pdf/2402.12146v3",
      "published_date": "2024-02-19 13:57:55 UTC",
      "updated_date": "2024-05-31 03:25:42 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T07:50:37.827823"
    },
    {
      "arxiv_id": "2402.13288v1",
      "title": "Training Table Question Answering via SQL Query Decomposition",
      "title_zh": "翻译失败",
      "authors": [
        "Raphaël Mouravieff",
        "Benjamin Piwowarski",
        "Sylvain Lamprier"
      ],
      "abstract": "Table Question-Answering involves both understanding the natural language\nquery and grounding it in the context of the input table to extract the\nrelevant information. In this context, many methods have highlighted the\nbenefits of intermediate pre-training from SQL queries. However, while most\napproaches aim at generating final answers from inputs directly, we claim that\nthere is better to do with SQL queries during training. By learning to imitate\na restricted portion of SQL-like algebraic operations, we show that their\nexecution flow provides intermediate supervision steps that allow increased\ngeneralization and structural reasoning compared with classical approaches of\nthe field. Our study bridges the gap between semantic parsing and direct\nanswering methods and provides useful insights regarding what types of\noperations should be predicted by a generative architecture or be preferably\nexecuted by an external algorithm.",
      "tldr_zh": "这篇论文探讨了通过SQL查询分解（SQL Query Decomposition）来训练表问题回答（Table Question-Answering）系统的方法，以更好地处理自然语言查询与输入表的关联。作者主张，相比直接从输入生成答案，学习模仿受限的SQL-like代数操作能提供中间监督步骤，从而提升模型的泛化性和结构化推理能力。该方法桥接了语义解析（semantic parsing）和直接回答方法之间的差距，并提供了见解，指出某些操作更适合由生成架构预测，而其他则应由外部算法执行。实验结果表明，这种训练策略显著提高了系统的性能和鲁棒性。",
      "categories": [
        "cs.DB",
        "cs.AI"
      ],
      "primary_category": "cs.DB",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2402.13288v1",
      "published_date": "2024-02-19 13:56:16 UTC",
      "updated_date": "2024-02-19 13:56:16 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T07:50:45.475065"
    },
    {
      "arxiv_id": "2402.12422v2",
      "title": "Simulacra as Conscious Exotica",
      "title_zh": "翻译失败",
      "authors": [
        "Murray Shanahan"
      ],
      "abstract": "The advent of conversational agents with increasingly human-like behaviour\nthrows old philosophical questions into new light. Does it, or could it, ever\nmake sense to speak of AI agents built out of generative language models in\nterms of consciousness, given that they are \"mere\" simulacra of human\nbehaviour, and that what they do can be seen as \"merely\" role play? Drawing on\nthe later writings of Wittgenstein, this paper attempts to tackle this question\nwhile avoiding the pitfalls of dualistic thinking.",
      "tldr_zh": "这篇论文探讨了生成语言模型构建的对话代理是否能被视为有意识的实体，因为它们只是人类行为的模拟体（simulacra），本质上属于“mere”角色扮演（role play）。作者借鉴维特根斯坦（Wittgenstein）的后期著作，分析这一哲学问题，同时避免二元论（dualistic thinking）的陷阱。论文旨在通过这种视角，重新审视 AI 代理的意识可能性及其对传统哲学的启示。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2402.12422v2",
      "published_date": "2024-02-19 13:53:10 UTC",
      "updated_date": "2024-07-11 15:42:47 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T07:50:59.518599"
    },
    {
      "arxiv_id": "2402.12132v1",
      "title": "SSTKG: Simple Spatio-Temporal Knowledge Graph for Intepretable and Versatile Dynamic Information Embedding",
      "title_zh": "翻译失败",
      "authors": [
        "Ruiyi Yang",
        "Flora D. Salim",
        "Hao Xue"
      ],
      "abstract": "Knowledge graphs (KGs) have been increasingly employed for link prediction\nand recommendation using real-world datasets. However, the majority of current\nmethods rely on static data, neglecting the dynamic nature and the hidden\nspatio-temporal attributes of real-world scenarios. This often results in\nsuboptimal predictions and recommendations. Although there are effective\nspatio-temporal inference methods, they face challenges such as scalability\nwith large datasets and inadequate semantic understanding, which impede their\nperformance. To address these limitations, this paper introduces a novel\nframework - Simple Spatio-Temporal Knowledge Graph (SSTKG), for constructing\nand exploring spatio-temporal KGs. To integrate spatial and temporal data into\nKGs, our framework exploited through a new 3-step embedding method. Output\nembeddings can be used for future temporal sequence prediction and spatial\ninformation recommendation, providing valuable insights for various\napplications such as retail sales forecasting and traffic volume prediction.\nOur framework offers a simple but comprehensive way to understand the\nunderlying patterns and trends in dynamic KG, thereby enhancing the accuracy of\npredictions and the relevance of recommendations. This work paves the way for\nmore effective utilization of spatio-temporal data in KGs, with potential\nimpacts across a wide range of sectors.",
      "tldr_zh": "该论文指出了现有知识图谱（KGs）方法依赖静态数据，忽略了动态时空属性的问题，导致预测和推荐效果不佳。为解决这一问题，研究提出了一种简单时空知识图谱框架（SSTKG），通过一个新的3步嵌入方法整合空间和时间数据，从而构建可解释且多功能的动态信息嵌入。SSTKG的输出嵌入可用于时间序列预测和空间信息推荐，如零售销售预测和交通流量预测，提升了预测准确性和推荐相关性，并为时空数据在KGs中的有效应用开辟了新路径。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "for Web conf 2024. 8 pages context",
      "pdf_url": "http://arxiv.org/pdf/2402.12132v1",
      "published_date": "2024-02-19 13:28:43 UTC",
      "updated_date": "2024-02-19 13:28:43 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T07:51:11.896920"
    },
    {
      "arxiv_id": "2402.12121v2",
      "title": "IRR: Image Review Ranking Framework for Evaluating Vision-Language Models",
      "title_zh": "IRR：图像评论排名框架，用于评估视觉语言模型",
      "authors": [
        "Kazuki Hayashi",
        "Kazuma Onishi",
        "Toma Suzuki",
        "Yusuke Ide",
        "Seiji Gobara",
        "Shigeki Saito",
        "Yusuke Sakai",
        "Hidetaka Kamigaito",
        "Katsuhiko Hayashi",
        "Taro Watanabe"
      ],
      "abstract": "Large-scale Vision-Language Models (LVLMs) process both images and text,\nexcelling in multimodal tasks such as image captioning and description\ngeneration. However, while these models excel at generating factual content,\ntheir ability to generate and evaluate texts reflecting perspectives on the\nsame image, depending on the context, has not been sufficiently explored. To\naddress this, we propose IRR: Image Review Rank, a novel evaluation framework\ndesigned to assess critic review texts from multiple perspectives. IRR\nevaluates LVLMs by measuring how closely their judgments align with human\ninterpretations. We validate it using a dataset of images from 15 categories,\neach with five critic review texts and annotated rankings in both English and\nJapanese, totaling over 2,000 data instances. The datasets are available at\nhttps://hf.co/datasets/naist-nlp/Wiki-ImageReview1.0. Our results indicate\nthat, although LVLMs exhibited consistent performance across languages, their\ncorrelation with human annotations was insufficient, highlighting the need for\nfurther advancements. These findings highlight the limitations of current\nevaluation methods and the need for approaches that better capture human\nreasoning in Vision & Language tasks.",
      "tldr_zh": "本文提出 IRR（Image Review Ranking）框架，用于评估大型视觉语言模型（LVLMs）在生成和评估多视角图像评论文本的能力，旨在解决这些模型在处理上下文依赖性方面存在的不足。IRR 通过测量模型判断与人类解释的契合度，并利用一个包含 15 个类别、每类五条评论文本及排名的多语言数据集（英语和日语，总计超过 2000 个实例）进行验证。实验结果显示，LVLMs 在不同语言上表现出一致性，但与人类注解的相关性较低，突显了当前评估方法在捕捉人类推理方面的局限性，并呼吁进一步改进视觉语言任务的评估方法。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.CV",
        "cs.MM"
      ],
      "primary_category": "cs.CL",
      "comment": "18pages, Accepted at COLING25",
      "pdf_url": "http://arxiv.org/pdf/2402.12121v2",
      "published_date": "2024-02-19 13:16:10 UTC",
      "updated_date": "2024-12-16 16:09:47 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T07:51:24.876018"
    },
    {
      "arxiv_id": "2402.12118v1",
      "title": "DualView: Data Attribution from the Dual Perspective",
      "title_zh": "DualView：从双重视角的数据归因",
      "authors": [
        "Galip Ümit Yolcu",
        "Thomas Wiegand",
        "Wojciech Samek",
        "Sebastian Lapuschkin"
      ],
      "abstract": "Local data attribution (or influence estimation) techniques aim at estimating\nthe impact that individual data points seen during training have on particular\npredictions of an already trained Machine Learning model during test time.\nPrevious methods either do not perform well consistently across different\nevaluation criteria from literature, are characterized by a high computational\ndemand, or suffer from both. In this work we present DualView, a novel method\nfor post-hoc data attribution based on surrogate modelling, demonstrating both\nhigh computational efficiency, as well as good evaluation results. With a focus\non neural networks, we evaluate our proposed technique using suitable\nquantitative evaluation strategies from the literature against related\nprincipal local data attribution methods. We find that DualView requires\nconsiderably lower computational resources than other methods, while\ndemonstrating comparable performance to competing approaches across evaluation\nmetrics. Futhermore, our proposed method produces sparse explanations, where\nsparseness can be tuned via a hyperparameter. Finally, we showcase that with\nDualView, we can now render explanations from local data attributions\ncompatible with established local feature attribution methods: For each\nprediction on (test) data points explained in terms of impactful samples from\nthe training set, we are able to compute and visualize how the prediction on\n(test) sample relates to each influential training sample in terms of features\nrecognized and by the model. We provide an Open Source implementation of\nDualView online, together with implementations for all other local data\nattribution methods we compare against, as well as the metrics reported here,\nfor full reproducibility.",
      "tldr_zh": "该论文提出DualView，一种基于surrogate modelling的后验数据归因方法，用于评估训练数据点对已训练机器学习模型测试预测的影响，旨在解决现有方法的计算效率低和性能不一致问题。DualView在神经网络上表现出高计算效率，并在定量评价指标中与竞争方法相当，同时通过超参数调整产生稀疏解释。论文进一步展示了DualView能将本地数据归因解释与已建立的本地特征归因方法兼容，并提供开源实现以确保可重复性。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2402.12118v1",
      "published_date": "2024-02-19 13:13:16 UTC",
      "updated_date": "2024-02-19 13:13:16 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T07:51:35.374576"
    },
    {
      "arxiv_id": "2402.12102v1",
      "title": "Is It a Free Lunch for Removing Outliers during Pretraining?",
      "title_zh": "去除预训练期间的异常值是否是免费的午餐？",
      "authors": [
        "Baohao Liao",
        "Christof Monz"
      ],
      "abstract": "With the growing size of large language models, the role of quantization\nbecomes increasingly significant. However, outliers present in weights or\nactivations notably influence the performance of quantized models. Recently,\n\\citet{qtransformer} introduced a novel softmax function aimed at pretraining\nmodels in an outlier-free manner, thereby enhancing their suitability for\nquantization. Interestingly, we observed that such an approach leads to\nperformance degradation in full precision. Building on this insight, we enhance\nthe method by ensuring its normalization is invariant to sequence length, a\ncrucial factor for bridging the gap between pretraining and fine-tuning.\nMoreover, this improved method also facilitates successful pretraining of\ncausal language models.",
      "tldr_zh": "该研究探讨了在预训练过程中移除异常值（outliers）是否会影响模型性能，特别是针对量化（quantization）的大型语言模型。作者发现，Q-Transformer 提出的异常值-free softmax 函数虽然提升了模型的量化适用性，但会导致全精度（full precision）下的性能下降。为解决这一问题，他们改进了该方法，使其归一化对序列长度保持不变，从而弥合预训练和微调（fine-tuning）之间的差距。实验结果显示，这种改进不仅避免了性能损失，还成功支持了因果语言模型（causal language models）的预训练。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "5 pages, 3 figures, 1 table",
      "pdf_url": "http://arxiv.org/pdf/2402.12102v1",
      "published_date": "2024-02-19 12:45:52 UTC",
      "updated_date": "2024-02-19 12:45:52 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T07:51:47.790612"
    },
    {
      "arxiv_id": "2402.12100v1",
      "title": "Groot: Adversarial Testing for Generative Text-to-Image Models with Tree-based Semantic Transformation",
      "title_zh": "翻译失败",
      "authors": [
        "Yi Liu",
        "Guowei Yang",
        "Gelei Deng",
        "Feiyue Chen",
        "Yuqi Chen",
        "Ling Shi",
        "Tianwei Zhang",
        "Yang Liu"
      ],
      "abstract": "With the prevalence of text-to-image generative models, their safety becomes\na critical concern. adversarial testing techniques have been developed to probe\nwhether such models can be prompted to produce Not-Safe-For-Work (NSFW)\ncontent. However, existing solutions face several challenges, including low\nsuccess rate and inefficiency. We introduce Groot, the first automated\nframework leveraging tree-based semantic transformation for adversarial testing\nof text-to-image models. Groot employs semantic decomposition and sensitive\nelement drowning strategies in conjunction with LLMs to systematically refine\nadversarial prompts. Our comprehensive evaluation confirms the efficacy of\nGroot, which not only exceeds the performance of current state-of-the-art\napproaches but also achieves a remarkable success rate (93.66%) on leading\ntext-to-image models such as DALL-E 3 and Midjourney.",
      "tldr_zh": "该研究提出了Groot，一种基于树状语义转换(Tree-based Semantic Transformation)的自动框架，用于对抗测试(Adversarial Testing)文本到图像生成模型，以检测其是否能被提示生成Not-Safe-For-Work (NSFW)内容。Groot通过语义分解(Semantic Decomposition)和敏感元素淹没(Sensitive Element Drowning)策略结合LLMs系统地优化对抗提示，从而解决现有方法的成功率低和效率低问题。实验结果显示，Groot在DALL-E 3和Midjourney等领先模型上实现了93.66%的成功率，显著超过了现有最先进方法。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.CR",
        "cs.SE"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2402.12100v1",
      "published_date": "2024-02-19 12:31:56 UTC",
      "updated_date": "2024-02-19 12:31:56 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T07:51:59.716081"
    },
    {
      "arxiv_id": "2402.12098v1",
      "title": "Towards Explainable LiDAR Point Cloud Semantic Segmentation via Gradient Based Target Localization",
      "title_zh": "翻译失败",
      "authors": [
        "Abhishek Kuriyal",
        "Vaibhav Kumar"
      ],
      "abstract": "Semantic Segmentation (SS) of LiDAR point clouds is essential for many\napplications, such as urban planning and autonomous driving. While much\nprogress has been made in interpreting SS predictions for images, interpreting\npoint cloud SS predictions remains a challenge. This paper introduces pGS-CAM,\na novel gradient-based method for generating saliency maps in neural network\nactivation layers. Inspired by Grad-CAM, which uses gradients to highlight\nlocal importance, pGS-CAM is robust and effective on a variety of datasets\n(SemanticKITTI, Paris-Lille3D, DALES) and 3D deep learning architectures\n(KPConv, RandLANet). Our experiments show that pGS-CAM effectively accentuates\nthe feature learning in intermediate activations of SS architectures by\nhighlighting the contribution of each point. This allows us to better\nunderstand how SS models make their predictions and identify potential areas\nfor improvement. Relevant codes are available at\nhttps://github.com/geoai4cities/pGS-CAM.",
      "tldr_zh": "该论文针对 LiDAR 点云 Semantic Segmentation 的可解释性挑战，提出了一种新型梯度-based 方法 pGS-CAM，用于生成神经网络激活层的显著性地图。pGS-CAM 借鉴 Grad-CAM 的理念，通过分析梯度来突出局部重要性，并在 SemanticKITTI、Paris-Lille3D 和 DALES 等数据集以及 KPConv 和 RandLANet 等 3D 深度学习架构上表现出色。实验结果显示，pGS-CAM 有效地强调了 SS 模型中每个点的贡献，从而提升了对预测过程的理解，并识别潜在改进领域。该方法为可解释的点云语义分割提供了实用工具，并提供了开源代码以供进一步研究。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2402.12098v1",
      "published_date": "2024-02-19 12:27:39 UTC",
      "updated_date": "2024-02-19 12:27:39 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T07:52:12.693536"
    },
    {
      "arxiv_id": "2402.13287v1",
      "title": "Manipulating hidden-Markov-model inferences by corrupting batch data",
      "title_zh": "通过破坏批量数据操纵隐马尔可夫模型推断",
      "authors": [
        "William N. Caballero",
        "Jose Manuel Camacho",
        "Tahir Ekin",
        "Roi Naveiro"
      ],
      "abstract": "Time-series models typically assume untainted and legitimate streams of data.\nHowever, a self-interested adversary may have incentive to corrupt this data,\nthereby altering a decision maker's inference. Within the broader field of\nadversarial machine learning, this research provides a novel, probabilistic\nperspective toward the manipulation of hidden Markov model inferences via\ncorrupted data. In particular, we provision a suite of corruption problems for\nfiltering, smoothing, and decoding inferences leveraging an adversarial risk\nanalysis approach. Multiple stochastic programming models are set forth that\nincorporate realistic uncertainties and varied attacker objectives. Three\ngeneral solution methods are developed by alternatively viewing the problem\nfrom frequentist and Bayesian perspectives. The efficacy of each method is\nillustrated via extensive, empirical testing. The developed methods are\ncharacterized by their solution quality and computational effort, resulting in\na stratification of techniques across varying problem-instance architectures.\nThis research highlights the weaknesses of hidden Markov models under\nadversarial activity, thereby motivating the need for robustification\ntechniques to ensure their security.",
      "tldr_zh": "这篇论文探讨了通过篡改批量数据来操纵 hidden Markov model 推断的问题，强调了时间序列模型在面对自利攻击者时的脆弱性。研究者使用 adversarial risk analysis 方法，提出了一系列针对过滤、平滑和解码推断的篡改问题，并开发了多个 stochastic programming 模型来处理现实的不确定性和攻击者目标。实验结果显示，三种通用解决方案方法（从频繁主义和贝叶斯视角）在有效性、解决方案质量和计算效率上表现出分层优势，突出了 hidden Markov models 的弱点，并呼吁开发鲁棒化技术以提升其安全性。",
      "categories": [
        "cs.CR",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CR",
      "comment": "42 pages, 8 figures, 11 tables",
      "pdf_url": "http://arxiv.org/pdf/2402.13287v1",
      "published_date": "2024-02-19 12:22:22 UTC",
      "updated_date": "2024-02-19 12:22:22 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T07:52:26.093203"
    },
    {
      "arxiv_id": "2402.12091v1",
      "title": "Do Large Language Models Understand Logic or Just Mimick Context?",
      "title_zh": "翻译失败",
      "authors": [
        "Junbing Yan",
        "Chengyu Wang",
        "Jun Huang",
        "Wei Zhang"
      ],
      "abstract": "Over the past few years, the abilities of large language models (LLMs) have\nreceived extensive attention, which have performed exceptionally well in\ncomplicated scenarios such as logical reasoning and symbolic inference. A\nsignificant factor contributing to this progress is the benefit of in-context\nlearning and few-shot prompting. However, the reasons behind the success of\nsuch models using contextual reasoning have not been fully explored. Do LLMs\nhave understand logical rules to draw inferences, or do they ``guess'' the\nanswers by learning a type of probabilistic mapping through context? This paper\ninvestigates the reasoning capabilities of LLMs on two logical reasoning\ndatasets by using counterfactual methods to replace context text and modify\nlogical concepts. Based on our analysis, it is found that LLMs do not truly\nunderstand logical rules; rather, in-context learning has simply enhanced the\nlikelihood of these models arriving at the correct answers. If one alters\ncertain words in the context text or changes the concepts of logical terms, the\noutputs of LLMs can be significantly disrupted, leading to counter-intuitive\nresponses. This work provides critical insights into the limitations of LLMs,\nunderscoring the need for more robust mechanisms to ensure reliable logical\nreasoning in LLMs.",
      "tldr_zh": "本研究探讨大型语言模型（LLMs）在逻辑推理方面的真实能力，质疑它们是否真正理解逻辑规则，还是仅通过上下文学习（如 in-context learning 和 few-shot prompting）来模仿答案。研究者使用反事实方法（counterfactual methods）在两个逻辑推理数据集上测试，通过替换上下文文本或修改逻辑概念，观察模型输出。结果显示，LLMs 并不真正理解逻辑规则，而是依赖概率映射；一旦上下文改变，模型的响应就会出现反直觉错误。该工作揭示了 LLMs 的局限性，强调需要开发更可靠的机制来提升其逻辑推理可靠性。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2402.12091v1",
      "published_date": "2024-02-19 12:12:35 UTC",
      "updated_date": "2024-02-19 12:12:35 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T07:52:35.052227"
    },
    {
      "arxiv_id": "2402.12074v1",
      "title": "HIP Network: Historical Information Passing Network for Extrapolation Reasoning on Temporal Knowledge Graph",
      "title_zh": "翻译失败",
      "authors": [
        "Yongquan He",
        "Peng Zhang",
        "Luchen Liu",
        "Qi Liang",
        "Wenyuan Zhang",
        "Chuang Zhang"
      ],
      "abstract": "In recent years, temporal knowledge graph (TKG) reasoning has received\nsignificant attention. Most existing methods assume that all timestamps and\ncorresponding graphs are available during training, which makes it difficult to\npredict future events. To address this issue, recent works learn to infer\nfuture events based on historical information. However, these methods do not\ncomprehensively consider the latent patterns behind temporal changes, to pass\nhistorical information selectively, update representations appropriately and\npredict events accurately. In this paper, we propose the Historical Information\nPassing (HIP) network to predict future events. HIP network passes information\nfrom temporal, structural and repetitive perspectives, which are used to model\nthe temporal evolution of events, the interactions of events at the same time\nstep, and the known events respectively. In particular, our method considers\nthe updating of relation representations and adopts three scoring functions\ncorresponding to the above dimensions. Experimental results on five benchmark\ndatasets show the superiority of HIP network, and the significant improvements\non Hits@1 prove that our method can more accurately predict what is going to\nhappen.",
      "tldr_zh": "本研究针对 temporal knowledge graph (TKG) 推理中的未来事件预测问题，提出 Historical Information Passing (HIP) network，以解决现有方法未能全面利用历史信息的局限。HIP network 通过从 temporal（时间演化）、structural（结构交互）和 repetitive（重复事件）三个视角选择性地传递历史信息，并更新关系表示，同时采用相应的三个评分函数来提升预测准确性。在五个基准数据集上的实验结果显示，HIP network 显著优于基线模型，尤其在 Hits@1 指标上实现了重大改善，证明了其在准确预测即将发生事件方面的有效性。",
      "categories": [
        "cs.AI",
        "I.2.4; I.2.6; I.2.7"
      ],
      "primary_category": "cs.AI",
      "comment": "7 pages, 3 figures",
      "pdf_url": "http://arxiv.org/pdf/2402.12074v1",
      "published_date": "2024-02-19 11:50:30 UTC",
      "updated_date": "2024-02-19 11:50:30 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T07:52:47.413782"
    },
    {
      "arxiv_id": "2402.12071v3",
      "title": "EmoBench: Evaluating the Emotional Intelligence of Large Language Models",
      "title_zh": "翻译失败",
      "authors": [
        "Sahand Sabour",
        "Siyang Liu",
        "Zheyuan Zhang",
        "June M. Liu",
        "Jinfeng Zhou",
        "Alvionna S. Sunaryo",
        "Juanzi Li",
        "Tatia M. C. Lee",
        "Rada Mihalcea",
        "Minlie Huang"
      ],
      "abstract": "Recent advances in Large Language Models (LLMs) have highlighted the need for\nrobust, comprehensive, and challenging benchmarks. Yet, research on evaluating\ntheir Emotional Intelligence (EI) is considerably limited. Existing benchmarks\nhave two major shortcomings: first, they mainly focus on emotion recognition,\nneglecting essential EI capabilities such as emotion regulation and thought\nfacilitation through emotion understanding; second, they are primarily\nconstructed from existing datasets, which include frequent patterns, explicit\ninformation, and annotation errors, leading to unreliable evaluation. We\npropose EmoBench, a benchmark that draws upon established psychological\ntheories and proposes a comprehensive definition for machine EI, including\nEmotional Understanding and Emotional Application. EmoBench includes a set of\n400 hand-crafted questions in English and Chinese, which are meticulously\ndesigned to require thorough reasoning and understanding. Our findings reveal a\nconsiderable gap between the EI of existing LLMs and the average human,\nhighlighting a promising direction for future research. Our code and data are\npublicly available at https://github.com/Sahandfer/EmoBench.",
      "tldr_zh": "这篇论文指出了现有 Large Language Models (LLMs) 在情感智能 (EI) 评估方面的不足，包括过度关注情感识别而忽略情感调节和思考促进，以及依赖现有数据集导致的可靠性问题。作者提出 EmoBench 基准，基于心理理论定义机器 EI 为 Emotional Understanding 和 Emotional Application，并设计了 400 个手工 crafted 的英文和中文问题，要求深入推理和理解。实验结果显示，现有机型在 EI 方面与人类平均水平存在显著差距，为未来 LLMs 研究提供了重要方向。代码和数据已在 GitHub 上公开可用。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "ACL 2024 Main Conference",
      "pdf_url": "http://arxiv.org/pdf/2402.12071v3",
      "published_date": "2024-02-19 11:48:09 UTC",
      "updated_date": "2024-07-17 05:30:58 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T07:53:01.105832"
    },
    {
      "arxiv_id": "2402.12065v2",
      "title": "WKVQuant: Quantizing Weight and Key/Value Cache for Large Language Models Gains More",
      "title_zh": "翻译失败",
      "authors": [
        "Yuxuan Yue",
        "Zhihang Yuan",
        "Haojie Duanmu",
        "Sifan Zhou",
        "Jianlong Wu",
        "Liqiang Nie"
      ],
      "abstract": "Large Language Models (LLMs) face significant deployment challenges due to\ntheir substantial memory requirements and the computational demands of\nauto-regressive text generation process. This paper addresses these challenges\nby focusing on the quantization of LLMs, a technique that reduces memory\nconsumption by converting model parameters and activations into low-bit\nintegers. We critically analyze the existing quantization approaches,\nidentifying their limitations in balancing the accuracy and efficiency of the\nquantized LLMs. To advance beyond these limitations, we propose WKVQuant, a PTQ\nframework especially designed for quantizing weights and the key/value (KV)\ncache of LLMs. Specifically, we incorporates past-only quantization to improve\nthe computation of attention. Additionally, we introduce two-dimensional\nquantization strategy to handle the distribution of KV cache, along with a\ncross-block reconstruction regularization for parameter optimization.\nExperiments show that WKVQuant achieves almost comparable memory savings to\nweight-activation quantization, while also approaching the performance of\nweight-only quantization.",
      "tldr_zh": "这篇论文针对 Large Language Models (LLMs) 的高内存和计算需求问题，提出了一种后训练量化(PTQ)框架WKVQuant，用于量化模型权重和Key/Value (KV)缓存，以实现更好的效率与准确性平衡。WKVQuant 引入了 past-only quantization 来优化注意力计算、两维量化策略处理 KV 缓存的分布，以及跨块重建正则化进行参数优化。实验结果表明，WKVQuant 在内存节省上几乎与权重-激活量化相当，同时性能接近权重-only 量化，从而为 LLMs 的部署提供了更有效的解决方案。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.LG",
      "comment": "Frist work to exclusively quantize weight and Key/Value cache for\n  large language models",
      "pdf_url": "http://arxiv.org/pdf/2402.12065v2",
      "published_date": "2024-02-19 11:33:21 UTC",
      "updated_date": "2024-02-20 08:48:24 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T07:53:13.557089"
    },
    {
      "arxiv_id": "2402.12062v4",
      "title": "Causal Equal Protection as Algorithmic Fairness",
      "title_zh": "因果平等保护作为算法公平性",
      "authors": [
        "Marcello Di Bello",
        "Nicolò Cangiotti",
        "Michele Loi"
      ],
      "abstract": "By combining the philosophical literature on statistical evidence and the\ninterdisciplinary literature on algorithmic fairness, we revisit recent\nobjections against classification parity in light of causal analyses of\nalgorithmic fairness and the distinction between predictive and diagnostic\nevidence. We focus on trial proceedings as a black-box classification algorithm\nin which defendants are sorted into two groups by convicting or acquitting\nthem. We defend a novel principle, causal equal protection, that combines\nclassification parity with the causal approach. In the do-calculus, causal\nequal protection requires that individuals should not be subject to uneven\nrisks of classification error because of their protected or socially salient\ncharacteristics. The explicit use of protected characteristics, however, may be\nrequired if it equalizes these risks.",
      "tldr_zh": "这篇论文结合统计证据哲学文献和算法公平（algorithmic fairness）跨学科研究，通过因果分析（causal analyses）和预测 vs. 诊断证据的区分，重新审视分类平权（classification parity）的反对意见。论文以审判程序作为黑箱分类算法的示例，将被告分类为定罪或无罪两组，并提出一个新原则：因果平等保护（causal equal protection），它整合了分类平权和因果方法。在 do-calculus 中，该原则要求个体不应因受保护或社会显著特征而面临不均等的分类错误风险，尽管这可能需要显式使用这些特征来平衡风险，从而为更公平的算法设计提供指导。",
      "categories": [
        "cs.CY",
        "cs.AI",
        "cs.DS",
        "cs.LG"
      ],
      "primary_category": "cs.CY",
      "comment": "18 pages, 7 figures",
      "pdf_url": "http://arxiv.org/pdf/2402.12062v4",
      "published_date": "2024-02-19 11:30:00 UTC",
      "updated_date": "2025-02-05 11:33:07 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T07:53:26.938724"
    },
    {
      "arxiv_id": "2402.12061v2",
      "title": "All Language Models Large and Small",
      "title_zh": "翻译失败",
      "authors": [
        "Zhixun Chen",
        "Yali Du",
        "David Mguni"
      ],
      "abstract": "Many leading language models (LMs) use high-intensity computational resources\nboth during training and execution. This poses the challenge of lowering\nresource costs for deployment and faster execution of decision-making tasks\namong others. We introduce a novel plug-and-play LM framework named Language\nOptimising Network Distribution (LONDI) framework. LONDI learns to selectively\nemploy large LMs only where complex decision-making and reasoning are required\nwhile using low-resource LMs (i.e. LMs require less GPU usage, but may not be\nable to solve the problem alone) everywhere else. LONDI consists of a system of\ntwo (off-)policy networks, an LM, a large LM (LLM), and a reinforcement\nlearning module that uses switching controls to quickly learn which system\nstates to call the LLM. We then introduce a variant of LONDI that maintains\nbudget constraints on LLM calls and hence its resource usage. Theoretically, we\nprove LONDI learns the subset of system states to activate the LLM required to\nsolve the task. We then prove that LONDI converges to optimal solutions while\nalso preserving budgetary constraints on LLM calls almost surely enabling it to\nsolve various tasks while significantly lowering computational costs. We test\nLONDI's performance in a range of tasks in ScienceWorld and BabyAI-Text and\ndemonstrate that LONDI can solve tasks only solvable by resource-intensive LLMs\nwhile reducing GPU usage by up to 30%.",
      "tldr_zh": "该论文提出了一种名为 Language Optimising Network Distribution (LONDI) 的即插即用框架，用于优化语言模型 (LMs) 的资源使用，通过在复杂决策和推理时才调用大型语言模型 (LLM)，而在其他情况下使用低资源 LMs，从而降低计算成本。LONDI 框架包括两个 (off-)policy 网络、一个 LM、一个 LLM 和一个强化学习模块，该模块通过切换控制学习何时激活 LLM。论文理论证明了 LONDI 可以学习必要系统状态来解决问题，并几乎肯定地收敛到最优解，同时遵守对 LLM 调用的预算约束。在 ScienceWorld 和 BabyAI-Text 任务中，实验显示 LONDI 能解决仅资源密集型 LLM 才能完成的任务，同时将 GPU 使用率降低高达 30%。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2402.12061v2",
      "published_date": "2024-02-19 11:28:20 UTC",
      "updated_date": "2024-06-05 15:08:28 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T07:53:39.578884"
    },
    {
      "arxiv_id": "2402.12042v2",
      "title": "Linear bandits with polylogarithmic minimax regret",
      "title_zh": "翻译失败",
      "authors": [
        "Josep Lumbreras",
        "Marco Tomamichel"
      ],
      "abstract": "We study a noise model for linear stochastic bandits for which the\nsubgaussian noise parameter vanishes linearly as we select actions on the unit\nsphere closer and closer to the unknown vector. We introduce an algorithm for\nthis problem that exhibits a minimax regret scaling as $\\log^3(T)$ in the time\nhorizon $T$, in stark contrast the square root scaling of this regret for\ntypical bandit algorithms. Our strategy, based on weighted least-squares\nestimation, achieves the eigenvalue relation $\\lambda_{\\min} ( V_t ) = \\Omega\n(\\sqrt{\\lambda_{\\max}(V_t ) })$ for the design matrix $V_t$ at each time step\n$t$ through geometrical arguments that are independent of the noise model and\nmight be of independent interest. This allows us to tightly control the\nexpected regret in each time step to be of the order $O(\\frac1{t})$, leading to\nthe logarithmic scaling of the cumulative regret.",
      "tldr_zh": "本研究探讨了线性随机 Bandits 中的一种噪声模型，其中 subgaussian 噪声参数随动作接近未知向量而线性减少。提出了一种基于加权最小二乘估计的算法，通过几何参数确保设计矩阵 $V_t$ 的特征值关系 $\\lambda_{\\min}(V_t) = \\Omega(\\sqrt{\\lambda_{\\max}(V_t)})$，从而将每个时间步的预期遗憾控制在 $O(1/t)$ 级别。结果显示，该算法的 minimax regret 呈 polylogarithmic 规模，即 $\\log^3(T)$，远优于传统 Bandits 算法的平方根规模。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "stat.ML"
      ],
      "primary_category": "cs.LG",
      "comment": "39 pages, 3 figures",
      "pdf_url": "http://arxiv.org/pdf/2402.12042v2",
      "published_date": "2024-02-19 10:56:47 UTC",
      "updated_date": "2024-05-29 10:58:25 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T07:53:50.010100"
    },
    {
      "arxiv_id": "2402.12035v2",
      "title": "Class-incremental Learning for Time Series: Benchmark and Evaluation",
      "title_zh": "时间序列的类增量学习：基准测试和评估",
      "authors": [
        "Zhongzheng Qiao",
        "Quang Pham",
        "Zhen Cao",
        "Hoang H Le",
        "P. N. Suganthan",
        "Xudong Jiang",
        "Ramasamy Savitha"
      ],
      "abstract": "Real-world environments are inherently non-stationary, frequently introducing\nnew classes over time. This is especially common in time series classification,\nsuch as the emergence of new disease classification in healthcare or the\naddition of new activities in human activity recognition. In such cases, a\nlearning system is required to assimilate novel classes effectively while\navoiding catastrophic forgetting of the old ones, which gives rise to the\nClass-incremental Learning (CIL) problem. However, despite the encouraging\nprogress in the image and language domains, CIL for time series data remains\nrelatively understudied. Existing studies suffer from inconsistent experimental\ndesigns, necessitating a comprehensive evaluation and benchmarking of methods\nacross a wide range of datasets. To this end, we first present an overview of\nthe Time Series Class-incremental Learning (TSCIL) problem, highlight its\nunique challenges, and cover the advanced methodologies. Further, based on\nstandardized settings, we develop a unified experimental framework that\nsupports the rapid development of new algorithms, easy integration of new\ndatasets, and standardization of the evaluation process. Using this framework,\nwe conduct a comprehensive evaluation of various generic and\ntime-series-specific CIL methods in both standard and privacy-sensitive\nscenarios. Our extensive experiments not only provide a standard baseline to\nsupport future research but also shed light on the impact of various design\nfactors such as normalization layers or memory budget thresholds. Codes are\navailable at https://github.com/zqiao11/TSCIL.",
      "tldr_zh": "这篇论文探讨了Class-incremental Learning (CIL)在时间序列分类中的应用，强调其在动态环境中处理新类（如医疗疾病分类或人类活动识别）的重要性，同时避免对旧类知识的灾难性遗忘。研究者概述了Time Series Class-incremental Learning (TSCIL)的独特挑战，并开发了一个统一的实验框架，支持新算法开发、数据集集成和标准化评估。使用该框架，他们对各种通用和时间序列特定的CIL方法进行了全面评估，包括标准和隐私敏感场景，揭示了设计因素（如归一化层和内存预算阈值）的影响，并提供了标准基准以推动未来研究。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "Accepted by SIGKDD 2024 (ADS track). Codes available at\n  https://github.com/zqiao11/TSCIL",
      "pdf_url": "http://arxiv.org/pdf/2402.12035v2",
      "published_date": "2024-02-19 10:43:13 UTC",
      "updated_date": "2024-08-03 13:29:50 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T07:54:02.286281"
    },
    {
      "arxiv_id": "2402.12026v3",
      "title": "Acquiring Clean Language Models from Backdoor Poisoned Datasets by Downscaling Frequency Space",
      "title_zh": "翻译失败",
      "authors": [
        "Zongru Wu",
        "Zhuosheng Zhang",
        "Pengzhou Cheng",
        "Gongshen Liu"
      ],
      "abstract": "Despite the notable success of language models (LMs) in various natural\nlanguage processing (NLP) tasks, the reliability of LMs is susceptible to\nbackdoor attacks. Prior research attempts to mitigate backdoor learning while\ntraining the LMs on the poisoned dataset, yet struggles against complex\nbackdoor attacks in real-world scenarios. In this paper, we investigate the\nlearning mechanisms of backdoor LMs in the frequency space by Fourier analysis.\nOur findings indicate that the backdoor mapping presented on the poisoned\ndatasets exhibits a more discernible inclination towards lower frequency\ncompared to clean mapping, resulting in the faster convergence of backdoor\nmapping. To alleviate this dilemma, we propose Multi-Scale Low-Rank Adaptation\n(MuScleLoRA), which deploys multiple radial scalings in the frequency space\nwith low-rank adaptation to the target model and further aligns the gradients\nwhen updating parameters. Through downscaling in the frequency space,\nMuScleLoRA encourages the model to prioritize the learning of relatively\nhigh-frequency clean mapping, consequently mitigating backdoor learning.\nExperimental results demonstrate that MuScleLoRA outperforms baselines\nsignificantly. Notably, MuScleLoRA reduces the average success rate of diverse\nbackdoor attacks to below 15\\% across multiple datasets and generalizes to\nvarious backbone LMs, including BERT, RoBERTa, GPT2-XL, and Llama2. The codes\nare publicly available at https://github.com/ZrW00/MuScleLoRA.",
      "tldr_zh": "本研究发现，后门攻击（backdoor attacks）会导致语言模型（LMs）在频域中优先学习低频的后门映射，从而加速其收敛。针对这一问题，提出 Multi-Scale Low-Rank Adaptation (MuScleLoRA) 方法，通过傅立叶分析（Fourier analysis）在频域部署多个径向缩放和低秩适配（low-rank adaptation），以优先优化高频的干净映射（clean mapping），从而缓解后门学习。实验结果显示，MuScleLoRA 显著优于基线，将多种后门攻击的成功率降低到低于 15%，并适用于 BERT、RoBERTa、GPT2-XL 和 Llama2 等模型。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.CR"
      ],
      "primary_category": "cs.CL",
      "comment": "Accepted at ACL 2024 (Long Paper. Main Conference)",
      "pdf_url": "http://arxiv.org/pdf/2402.12026v3",
      "published_date": "2024-02-19 10:34:48 UTC",
      "updated_date": "2024-06-02 10:28:13 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T07:54:15.113234"
    },
    {
      "arxiv_id": "2402.12023v1",
      "title": "Evaluation of ChatGPT's Smart Contract Auditing Capabilities Based on Chain of Thought",
      "title_zh": "基于链式思考的ChatGPT智能合约审计能力评估",
      "authors": [
        "Yuying Du",
        "Xueyan Tang"
      ],
      "abstract": "Smart contracts, as a key component of blockchain technology, play a crucial\nrole in ensuring the automation of transactions and adherence to protocol\nrules. However, smart contracts are susceptible to security vulnerabilities,\nwhich, if exploited, can lead to significant asset losses. This study explores\nthe potential of enhancing smart contract security audits using the GPT-4\nmodel. We utilized a dataset of 35 smart contracts from the SolidiFI-benchmark\nvulnerability library, containing 732 vulnerabilities, and compared it with\nfive other vulnerability detection tools to evaluate GPT-4's ability to\nidentify seven common types of vulnerabilities. Moreover, we assessed GPT-4's\nperformance in code parsing and vulnerability capture by simulating a\nprofessional auditor's auditing process using CoT(Chain of Thought) prompts\nbased on the audit reports of eight groups of smart contracts. We also\nevaluated GPT-4's ability to write Solidity Proof of Concepts (PoCs). Through\nexperimentation, we found that GPT-4 performed poorly in detecting smart\ncontract vulnerabilities, with a high Precision of 96.6%, but a low Recall of\n37.8%, and an F1-score of 41.1%, indicating a tendency to miss vulnerabilities\nduring detection. Meanwhile, it demonstrated good contract code parsing\ncapabilities, with an average comprehensive score of 6.5, capable of\nidentifying the background information and functional relationships of smart\ncontracts; in 60% of the cases, it could write usable PoCs, suggesting GPT-4\nhas significant potential application in PoC writing. These experimental\nresults indicate that GPT-4 lacks the ability to detect smart contract\nvulnerabilities effectively, but its performance in contract code parsing and\nPoC writing demonstrates its significant potential as an auxiliary tool in\nenhancing the efficiency and effectiveness of smart contract security audits.",
      "tldr_zh": "本研究评估了基于Chain of Thought的GPT-4在智能合约审计中的能力，使用了SolidiFI-benchmark数据集（包含35个合约和732个漏洞），并与五种其他检测工具比较，焦点包括七种常见漏洞的识别、代码解析以及编写Solidity Proof of Concepts (PoCs)。结果显示，GPT-4的Precision高达96.6%，但Recall仅37.8%，F1-score为41.1%，表明其在漏洞检测上容易遗漏问题。另一方面，GPT-4在代码解析方面表现出色，平均综合分数为6.5，能够识别合约背景和功能关系，并在60%的案例中成功编写可用PoCs，因此具有作为智能合约安全审计辅助工具的潜力。",
      "categories": [
        "cs.CR",
        "cs.AI",
        "68",
        "I.2; J.6"
      ],
      "primary_category": "cs.CR",
      "comment": "21 pages, 10 figures",
      "pdf_url": "http://arxiv.org/pdf/2402.12023v1",
      "published_date": "2024-02-19 10:33:29 UTC",
      "updated_date": "2024-02-19 10:33:29 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T07:54:28.295172"
    },
    {
      "arxiv_id": "2402.12010v1",
      "title": "Training Green AI Models Using Elite Samples",
      "title_zh": "使用精英样本训练绿色 AI",
      "authors": [
        "Mohammed Alswaitti",
        "Roberto Verdecchia",
        "Grégoire Danoy",
        "Pascal Bouvry",
        "Johnatan Pecero"
      ],
      "abstract": "The substantial increase in AI model training has considerable environmental\nimplications, mandating more energy-efficient and sustainable AI practices. On\nthe one hand, data-centric approaches show great potential towards training\nenergy-efficient AI models. On the other hand, instance selection methods\ndemonstrate the capability of training AI models with minimised training sets\nand negligible performance degradation. Despite the growing interest in both\ntopics, the impact of data-centric training set selection on energy efficiency\nremains to date unexplored. This paper presents an evolutionary-based sampling\nframework aimed at (i) identifying elite training samples tailored for datasets\nand model pairs, (ii) comparing model performance and energy efficiency gains\nagainst typical model training practice, and (iii) investigating the\nfeasibility of this framework for fostering sustainable model training\npractices. To evaluate the proposed framework, we conducted an empirical\nexperiment including 8 commonly used AI classification models and 25 publicly\navailable datasets. The results showcase that by considering 10% elite training\nsamples, the models' performance can show a 50% improvement and remarkable\nenergy savings of 98% compared to the common training practice.",
      "tldr_zh": "该研究针对AI模型训练对环境的负面影响，提出了一种基于演化算法(evolutionary-based sampling framework)，用于选择精英训练样本(elite training samples)，以实现更节能的AI实践。该框架旨在为特定数据集和模型对识别定制化样本，比较性能与能效提升，并探索其在可持续模型训练中的可行性。在实验中，使用了8个常用AI分类模型和25个公开数据集，结果显示，仅使用10%的精英训练样本即可使模型性能提升50%，并实现98%的能源节省，从而为绿色AI训练提供了有效途径。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.NE"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2402.12010v1",
      "published_date": "2024-02-19 10:03:46 UTC",
      "updated_date": "2024-02-19 10:03:46 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T07:54:37.028111"
    },
    {
      "arxiv_id": "2402.12008v1",
      "title": "Cluster Metric Sensitivity to Irrelevant Features",
      "title_zh": "聚类度量对无关特征的敏感性",
      "authors": [
        "Miles McCrory",
        "Spencer A. Thomas"
      ],
      "abstract": "Clustering algorithms are used extensively in data analysis for data\nexploration and discovery. Technological advancements lead to continually\ngrowth of data in terms of volume, dimensionality and complexity. This provides\ngreat opportunities in data analytics as the data can be interrogated for many\ndifferent purposes. This however leads challenges, such as identification of\nrelevant features for a given task. In supervised tasks, one can utilise a\nnumber of methods to optimise the input features for the task objective (e.g.\nclassification accuracy). In unsupervised problems, such tools are not readily\navailable, in part due to an inability to quantify feature relevance in\nunlabeled tasks. In this paper, we investigate the sensitivity of clustering\nperformance noisy uncorrelated variables iteratively added to baseline datasets\nwith well defined clusters. We show how different types of irrelevant variables\ncan impact the outcome of a clustering result from $k$-means in different ways.\nWe observe a resilience to very high proportions of irrelevant features for\nadjusted rand index (ARI) and normalised mutual information (NMI) when the\nirrelevant features are Gaussian distributed. For Uniformly distributed\nirrelevant features, we notice the resilience of ARI and NMI is dependent on\nthe dimensionality of the data and exhibits tipping points between high scores\nand near zero. Our results show that the Silhouette Coefficient and the\nDavies-Bouldin score are the most sensitive to irrelevant added features\nexhibiting large changes in score for comparably low proportions of irrelevant\nfeatures regardless of underlying distribution or data scaling. As such the\nSilhouette Coefficient and the Davies-Bouldin score are good candidates for\noptimising feature selection in unsupervised clustering tasks.",
      "tldr_zh": "这篇论文探讨了聚类算法对无关特征的敏感性，特别是在无监督学习中识别相关特征的挑战。研究方法涉及在基线数据集上逐步添加噪声无关变量，并评估k-means算法的性能。结果显示，Adjusted Rand Index (ARI) 和 Normalised Mutual Information (NMI) 对高斯分布无关特征表现出韧性，但对均匀分布无关特征的韧性取决于数据维度，可能出现临界点。Silhouette Coefficient 和 Davies-Bouldin score 被证明最敏感，因此适合作为优化无监督聚类任务特征选择的指标。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "stat.ML"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2402.12008v1",
      "published_date": "2024-02-19 10:02:00 UTC",
      "updated_date": "2024-02-19 10:02:00 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T07:54:49.737902"
    },
    {
      "arxiv_id": "2402.12419v1",
      "title": "EBFT: Effective and Block-Wise Fine-Tuning for Sparse LLMs",
      "title_zh": "翻译失败",
      "authors": [
        "Song Guo",
        "Fan Wu",
        "Lei Zhang",
        "Xiawu Zheng",
        "Shengchuan Zhang",
        "Fei Chao",
        "Yiyu Shi",
        "Rongrong Ji"
      ],
      "abstract": "Existing methods for fine-tuning sparse LLMs often suffer from\nresource-intensive requirements and high retraining costs. Additionally, many\nfine-tuning methods often rely on approximations or heuristic optimization\nstrategies, which may lead to suboptimal solutions. To address these issues, we\npropose an efficient and fast framework for fine-tuning sparse LLMs based on\nminimizing reconstruction error. Our approach involves sampling a small dataset\nfor calibration and utilizing backpropagation to iteratively optimize\nblock-wise reconstruction error, on a block-by-block basis, aiming for optimal\nsolutions. Extensive experiments on various benchmarks consistently demonstrate\nthe superiority of our method over other baselines. For instance, on the\nWikitext2 dataset with LlamaV1-7B at 70% sparsity, our proposed EBFT achieves a\nperplexity of 16.88, surpassing the state-of-the-art DSnoT with a perplexity of\n75.14. Moreover, with a structured sparsity ratio of 26\\%, EBFT achieves a\nperplexity of 16.27, outperforming LoRA (perplexity 16.44). Furthermore, the\nfine-tuning process of EBFT for LlamaV1-7B only takes approximately 30 minutes,\nand the entire framework can be executed on a single 16GB GPU. The source code\nis available at https://github.com/sunggo/EBFT.",
      "tldr_zh": "本研究针对现有稀疏大语言模型（sparse LLMs）的微调方法存在的资源密集和次优问题，提出了一种高效框架 EBFT，通过最小化重构错误实现块级优化。EBFT 的方法包括采样小数据集进行校准，并利用反向传播迭代优化块级重构错误，以追求最优解。在各种基准实验中，EBFT 表现出色，例如在 Wikitext2 数据集上，LlamaV1-7B 在 70% 稀疏度下达到 16.88 的 perplexity，比 DSnoT 的 75.14 显著改善；在 26% 结构化稀疏度下，perplexity 为 16.27，优于 LoRA 的 16.44。该框架微调过程仅需约 30 分钟，且可在单张 16GB GPU 上运行，展示了其高效性和实用性。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2402.12419v1",
      "published_date": "2024-02-19 09:55:32 UTC",
      "updated_date": "2024-02-19 09:55:32 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T07:55:04.078205"
    },
    {
      "arxiv_id": "2402.12418v1",
      "title": "Beyond Uniform Scaling: Exploring Depth Heterogeneity in Neural Architectures",
      "title_zh": "超越均匀缩放：探索神经网络架构中的深度异质性",
      "authors": [
        "Akash Guna R. T",
        "Arnav Chavan",
        "Deepak Gupta"
      ],
      "abstract": "Conventional scaling of neural networks typically involves designing a base\nnetwork and growing different dimensions like width, depth, etc. of the same by\nsome predefined scaling factors. We introduce an automated scaling approach\nleveraging second-order loss landscape information. Our method is flexible\ntowards skip connections a mainstay in modern vision transformers. Our\ntraining-aware method jointly scales and trains transformers without additional\ntraining iterations. Motivated by the hypothesis that not all neurons need\nuniform depth complexity, our approach embraces depth heterogeneity. Extensive\nevaluations on DeiT-S with ImageNet100 show a 2.5% accuracy gain and 10%\nparameter efficiency improvement over conventional scaling. Scaled networks\ndemonstrate superior performance upon training small scale datasets from\nscratch. We introduce the first intact scaling mechanism for vision\ntransformers, a step towards efficient model scaling.",
      "tldr_zh": "该研究挑战了传统神经网络的统一缩放方法，提出一种自动缩放策略，利用二阶损失景观信息（second-order loss landscape information）来实现深度异质性（depth heterogeneity），允许不同神经元具有不同的深度复杂度。该方法支持跳跃连接（skip connections），并能在缩放和训练视觉Transformer时无需额外迭代，从而提高效率。在DeiT-S模型上进行的实验显示，与传统缩放相比，ImageNet100数据集的准确率提升2.5%，参数效率改善10%，并在从零开始训练小规模数据集时表现出色。该工作首次为视觉Transformer（vision transformers）提供完整的缩放机制，推动了高效模型设计的发展。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.NE"
      ],
      "primary_category": "cs.LG",
      "comment": "Accepted At ICLR 2024 (Tiny Paper Track)",
      "pdf_url": "http://arxiv.org/pdf/2402.12418v1",
      "published_date": "2024-02-19 09:52:45 UTC",
      "updated_date": "2024-02-19 09:52:45 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T07:55:16.722717"
    },
    {
      "arxiv_id": "2402.12001v1",
      "title": "A Survey on Extractive Knowledge Graph Summarization: Applications, Approaches, Evaluation, and Future Directions",
      "title_zh": "翻译失败",
      "authors": [
        "Xiaxia Wang",
        "Gong Cheng"
      ],
      "abstract": "With the continuous growth of large Knowledge Graphs (KGs), extractive KG\nsummarization becomes a trending task. Aiming at distilling a compact subgraph\nwith condensed information, it facilitates various downstream KG-based tasks.\nIn this survey paper, we are among the first to provide a systematic overview\nof its applications and define a taxonomy for existing methods from its\ninterdisciplinary studies. Future directions are also laid out based on our\nextensive and comparative review.",
      "tldr_zh": "这篇调查论文系统概述了抽取式知识图谱总结（extractive KG summarization）的应用，旨在通过提炼紧凑子图来支持各种下游 Knowledge Graphs (KGs) 任务，如知识查询和分析。论文定义了一个基于跨学科研究的分类体系，涵盖了现有方法、评估标准，并对这些方法进行了全面比较。最终，它指出了未来研究方向，包括改进算法和扩展应用领域，以应对 KGs 持续增长的挑战。",
      "categories": [
        "cs.AI",
        "cs.DB",
        "cs.IR",
        "cs.SI"
      ],
      "primary_category": "cs.AI",
      "comment": "9 pages, 13 figures, submitted to the IJCAI 2024 Survey Track",
      "pdf_url": "http://arxiv.org/pdf/2402.12001v1",
      "published_date": "2024-02-19 09:49:53 UTC",
      "updated_date": "2024-02-19 09:49:53 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T07:55:28.365830"
    },
    {
      "arxiv_id": "2402.11997v2",
      "title": "Remember This Event That Year? Assessing Temporal Information and Reasoning in Large Language Models",
      "title_zh": "翻译失败",
      "authors": [
        "Himanshu Beniwal",
        "Dishant Patel",
        "Kowsik Nandagopan D",
        "Hritik Ladia",
        "Ankit Yadav",
        "Mayank Singh"
      ],
      "abstract": "Large Language Models (LLMs) are increasingly ubiquitous, yet their ability\nto retain and reason about temporal information remains limited, hindering\ntheir application in real-world scenarios where understanding the sequential\nnature of events is crucial. Our study experiments with 12 state-of-the-art\nmodels (ranging from 2B to 70B+ parameters) on a novel numerical-temporal\ndataset, \\textbf{TempUN}, spanning from 10,000 BCE to 2100 CE, to uncover\nsignificant temporal retention and comprehension limitations. We propose six\nmetrics to assess three learning paradigms to enhance temporal knowledge\nacquisition. Our findings reveal that open-source models exhibit knowledge gaps\nmore frequently, suggesting a trade-off between limited knowledge and incorrect\nresponses. Additionally, various fine-tuning approaches significantly improved\nperformance, reducing incorrect outputs and impacting the identification of\n'information not available' in the generations. The associated dataset and code\nare available at (https://github.com/lingoiitgn/TempUN).",
      "tldr_zh": "这篇论文评估了大型语言模型 (LLMs) 在保留和推理时间信息方面的局限性，这些问题阻碍了其在现实场景中的应用。研究者创建了新的数据集 TempUN（覆盖从公元前10,000年到公元后2100年），并对12个最先进模型（参数从2B到70B+）进行了实验，同时提出了六个指标来评估三种学习范式以提升时间知识获取。结果显示，开源模型更频繁出现知识缺口，导致有限知识与错误响应的权衡，而各种 fine-tuning 方法显著降低了错误输出并改善了“信息不可用”的识别。该数据集和代码已在 GitHub 上公开可用。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2402.11997v2",
      "published_date": "2024-02-19 09:43:03 UTC",
      "updated_date": "2024-07-05 11:26:51 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T07:55:40.729009"
    },
    {
      "arxiv_id": "2402.11984v1",
      "title": "Hebbian Learning based Orthogonal Projection for Continual Learning of Spiking Neural Networks",
      "title_zh": "翻译失败",
      "authors": [
        "Mingqing Xiao",
        "Qingyan Meng",
        "Zongpeng Zhang",
        "Di He",
        "Zhouchen Lin"
      ],
      "abstract": "Neuromorphic computing with spiking neural networks is promising for\nenergy-efficient artificial intelligence (AI) applications. However, different\nfrom humans who continually learn different tasks in a lifetime, neural network\nmodels suffer from catastrophic forgetting. How could neuronal operations solve\nthis problem is an important question for AI and neuroscience. Many previous\nstudies draw inspiration from observed neuroscience phenomena and propose\nepisodic replay or synaptic metaplasticity, but they are not guaranteed to\nexplicitly preserve knowledge for neuron populations. Other works focus on\nmachine learning methods with more mathematical grounding, e.g., orthogonal\nprojection on high dimensional spaces, but there is no neural correspondence\nfor neuromorphic computing. In this work, we develop a new method with neuronal\noperations based on lateral connections and Hebbian learning, which can protect\nknowledge by projecting activity traces of neurons into an orthogonal subspace\nso that synaptic weight update will not interfere with old tasks. We show that\nHebbian and anti-Hebbian learning on recurrent lateral connections can\neffectively extract the principal subspace of neural activities and enable\northogonal projection. This provides new insights into how neural circuits and\nHebbian learning can help continual learning, and also how the concept of\northogonal projection can be realized in neuronal systems. Our method is also\nflexible to utilize arbitrary training methods based on presynaptic\nactivities/traces. Experiments show that our method consistently solves\nforgetting for spiking neural networks with nearly zero forgetting under\nvarious supervised training methods with different error propagation\napproaches, and outperforms previous approaches under various settings. Our\nmethod can pave a solid path for building continual neuromorphic computing\nsystems.",
      "tldr_zh": "本文提出了一种基于 Hebbian Learning 的正交投影方法，用于解决 Spiking Neural Networks (SNNs) 在持续学习中的灾难性遗忘问题。该方法通过侧向连接和 Hebbian 及 anti-Hebbian 学习，将神经活动痕迹投影到正交子空间，从而保护旧任务知识并防止突触权重更新对其干扰。实验结果表明，该方法在各种监督训练设置下实现了几乎零遗忘，并优于现有方法，为构建高效的持续神经形态计算系统提供了新见解。",
      "categories": [
        "cs.NE",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.NE",
      "comment": "Accepted by ICLR 2024",
      "pdf_url": "http://arxiv.org/pdf/2402.11984v1",
      "published_date": "2024-02-19 09:29:37 UTC",
      "updated_date": "2024-02-19 09:29:37 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T07:55:55.186939"
    },
    {
      "arxiv_id": "2402.14843v1",
      "title": "Text Diffusion with Reinforced Conditioning",
      "title_zh": "翻译失败",
      "authors": [
        "Yuxuan Liu",
        "Tianchi Yang",
        "Shaohan Huang",
        "Zihan Zhang",
        "Haizhen Huang",
        "Furu Wei",
        "Weiwei Deng",
        "Feng Sun",
        "Qi Zhang"
      ],
      "abstract": "Diffusion models have demonstrated exceptional capability in generating\nhigh-quality images, videos, and audio. Due to their adaptiveness in iterative\nrefinement, they provide a strong potential for achieving better\nnon-autoregressive sequence generation. However, existing text diffusion models\nstill fall short in their performance due to a challenge in handling the\ndiscreteness of language. This paper thoroughly analyzes text diffusion models\nand uncovers two significant limitations: degradation of self-conditioning\nduring training and misalignment between training and sampling. Motivated by\nour findings, we propose a novel Text Diffusion model called TREC, which\nmitigates the degradation with Reinforced Conditioning and the misalignment by\nTime-Aware Variance Scaling. Our extensive experiments demonstrate the\ncompetitiveness of TREC against autoregressive, non-autoregressive, and\ndiffusion baselines. Moreover, qualitative analysis shows its advanced ability\nto fully utilize the diffusion process in refining samples.",
      "tldr_zh": "本论文分析了文本扩散模型（Diffusion models）在处理语言离散性时面临的两个主要问题：训练过程中的自条件退化（degradation of self-conditioning）和训练与采样之间的不匹配（misalignment between training and sampling）。为了解决这些问题，研究者提出了一种新模型TREC，通过Reinforced Conditioning缓解自条件退化，并采用Time-Aware Variance Scaling调整采样过程。实验结果表明，TREC在与自回归、非自回归和扩散基线模型的比较中具有竞争力，并展示了在扩散过程中更有效地精炼样本的能力。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "9 pages, 3 figures",
      "pdf_url": "http://arxiv.org/pdf/2402.14843v1",
      "published_date": "2024-02-19 09:24:02 UTC",
      "updated_date": "2024-02-19 09:24:02 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T07:56:05.049861"
    },
    {
      "arxiv_id": "2402.13284v2",
      "title": "Structure Guided Large Language Model for SQL Generation",
      "title_zh": "翻译失败",
      "authors": [
        "Qinggang Zhang",
        "Junnan Dong",
        "Hao Chen",
        "Wentao Li",
        "Feiran Huang",
        "Xiao Huang"
      ],
      "abstract": "Generating accurate Structured Querying Language (SQL) is a long-standing\nproblem, especially in matching users' semantic queries with structured\ndatabases and then generating structured SQL. Existing models typically input\nqueries and database schemas into the LLM and rely on the LLM to perform\nsemantic-structure matching and generate structured SQL. However, such\nsolutions overlook the structural information within user queries and\ndatabases, which can be utilized to enhance the generation of structured SQL.\nThis oversight can lead to inaccurate or unexecutable SQL generation. To fully\nexploit the structure, we propose a structure-to-SQL framework, which leverages\nthe inherent structure information to improve the SQL generation of LLMs.\nSpecifically, we introduce our Structure Guided SQL~(SGU-SQL) generation model.\nSGU-SQL first links user queries and databases in a structure-enhanced manner.\nIt then decomposes complicated linked structures with grammar trees to guide\nthe LLM to generate the SQL step by step. Extensive experiments on two\nbenchmark datasets illustrate that SGU-SQL can outperform sixteen SQL\ngeneration baselines.",
      "tldr_zh": "本研究针对SQL生成问题提出了一种结构引导的Large Language Model框架（SGU-SQL），旨在通过利用用户查询和数据库中的结构信息，提高SQL的准确性和可执行性。SGU-SQL首先采用结构增强方式链接查询和数据库，然后使用语法树分解复杂的结构，指导LLM逐步生成SQL语句。该方法在两个基准数据集上的实验中，超过了16个基线模型，显著提升了SQL生成的性能。",
      "categories": [
        "cs.DB",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.DB",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2402.13284v2",
      "published_date": "2024-02-19 09:07:59 UTC",
      "updated_date": "2024-03-27 14:30:44 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T07:56:15.433857"
    },
    {
      "arxiv_id": "2402.11963v1",
      "title": "Imbalance in Regression Datasets",
      "title_zh": "翻译失败",
      "authors": [
        "Daniel Kowatsch",
        "Nicolas M. Müller",
        "Kilian Tscharke",
        "Philip Sperl",
        "Konstantin Bötinger"
      ],
      "abstract": "For classification, the problem of class imbalance is well known and has been\nextensively studied. In this paper, we argue that imbalance in regression is an\nequally important problem which has so far been overlooked: Due to under- and\nover-representations in a data set's target distribution, regressors are prone\nto degenerate to naive models, systematically neglecting uncommon training data\nand over-representing targets seen often during training. We analyse this\nproblem theoretically and use resulting insights to develop a first definition\nof imbalance in regression, which we show to be a generalisation of the\ncommonly employed imbalance measure in classification. With this, we hope to\nturn the spotlight on the overlooked problem of imbalance in regression and to\nprovide common ground for future research.",
      "tldr_zh": "该论文指出，回归数据集中的不平衡问题（如目标分布的欠表示和过表示）被长期忽略，导致回归模型退化成简单模型，忽略不常见数据并过度拟合常见目标。作者通过理论分析，首次定义了回归中的不平衡，并证明这一定义是对分类问题不平衡度量的一种泛化。最终，该研究旨在突出这一被忽视的问题，为未来回归数据集处理的研究提供共同基础。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2402.11963v1",
      "published_date": "2024-02-19 09:06:26 UTC",
      "updated_date": "2024-02-19 09:06:26 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T07:56:27.423828"
    },
    {
      "arxiv_id": "2402.11960v1",
      "title": "DB-LLM: Accurate Dual-Binarization for Efficient LLMs",
      "title_zh": "翻译失败",
      "authors": [
        "Hong Chen",
        "Chengtao Lv",
        "Liang Ding",
        "Haotong Qin",
        "Xiabin Zhou",
        "Yifu Ding",
        "Xuebo Liu",
        "Min Zhang",
        "Jinyang Guo",
        "Xianglong Liu",
        "Dacheng Tao"
      ],
      "abstract": "Large language models (LLMs) have significantly advanced the field of natural\nlanguage processing, while the expensive memory and computation consumption\nimpede their practical deployment. Quantization emerges as one of the most\neffective methods for improving the computational efficiency of LLMs. However,\nexisting ultra-low-bit quantization always causes severe accuracy drops. In\nthis paper, we empirically relieve the micro and macro characteristics of\nultra-low bit quantization and present a novel Dual-Binarization method for\nLLMs, namely DB-LLM. For the micro-level, we take both the accuracy advantage\nof 2-bit-width and the efficiency advantage of binarization into account,\nintroducing Flexible Dual Binarization (FDB). By splitting 2-bit quantized\nweights into two independent sets of binaries, FDB ensures the accuracy of\nrepresentations and introduces flexibility, utilizing the efficient bitwise\noperations of binarization while retaining the inherent high sparsity of\nultra-low bit quantization. For the macro-level, we find the distortion that\nexists in the prediction of LLM after quantization, which is specified as the\ndeviations related to the ambiguity of samples. We propose the Deviation-Aware\nDistillation (DAD) method, enabling the model to focus differently on various\nsamples. Comprehensive experiments show that our DB-LLM not only significantly\nsurpasses the current State-of-The-Art (SoTA) in ultra-low bit quantization\n(eg, perplexity decreased from 9.64 to 7.23), but also achieves an additional\n20\\% reduction in computational consumption compared to the SOTA method under\nthe same bit-width. Our code will be released soon.",
      "tldr_zh": "该论文提出DB-LLM，一种精确的双二值化方法，用于提升大型语言模型(LLMs)的计算效率，同时缓解超低位量化带来的准确性下降问题。\n在微观层面，DB-LLM引入Flexible Dual Binarization (FDB)，通过将2-bit量化权重拆分为两组二进制集，利用位运算的效率和稀疏性来保持表示准确性；在宏观层面，提出Deviation-Aware Distillation (DAD)方法，针对量化后样本歧义导致的偏差进行优化。\n实验结果显示，DB-LLM在超低位量化中显著超越现有最先进方法，例如困惑度从9.64降至7.23，并在相同位宽下减少20%的计算消耗。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2402.11960v1",
      "published_date": "2024-02-19 09:04:30 UTC",
      "updated_date": "2024-02-19 09:04:30 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T07:56:41.372786"
    },
    {
      "arxiv_id": "2402.11955v1",
      "title": "Analysis of Multidomain Abstractive Summarization Using Salience Allocation",
      "title_zh": "翻译失败",
      "authors": [
        "Tohida Rehman",
        "Raghubir Bose",
        "Soumik Dey",
        "Samiran Chattopadhyay"
      ],
      "abstract": "This paper explores the realm of abstractive text summarization through the\nlens of the SEASON (Salience Allocation as Guidance for Abstractive\nSummarizatiON) technique, a model designed to enhance summarization by\nleveraging salience allocation techniques. The study evaluates SEASON's\nefficacy by comparing it with prominent models like BART, PEGASUS, and\nProphetNet, all fine-tuned for various text summarization tasks. The assessment\nis conducted using diverse datasets including CNN/Dailymail, SAMSum, and\nFinancial-news based Event-Driven Trading (EDT), with a specific focus on a\nfinancial dataset containing a substantial volume of news articles from\n2020/03/01 to 2021/05/06. This paper employs various evaluation metrics such as\nROUGE, METEOR, BERTScore, and MoverScore to evaluate the performance of these\nmodels fine-tuned for generating abstractive summaries. The analysis of these\nmetrics offers a thorough insight into the strengths and weaknesses\ndemonstrated by each model in summarizing news dataset, dialogue dataset and\nfinancial text dataset. The results presented in this paper not only contribute\nto the evaluation of the SEASON model's effectiveness but also illuminate the\nintricacies of salience allocation techniques across various types of datasets.",
      "tldr_zh": "本论文分析了使用显著性分配（salience allocation）的 SEASON 技术在多领域抽象性文本摘要中的应用，旨在通过这种方法提升摘要生成的质量。研究比较了 SEASON 与 BART、PEGASUS 和 ProphetNet 等模型的表现，采用 CNN/Dailymail、SAMSum 和金融新闻数据集进行评估，并使用 ROUGE、METEOR、BERTScore 和 MoverScore 等指标量化模型的优缺点。结果表明，SEASON 模型在不同数据集上显示出显著优势，并揭示了 salience allocation 技术的复杂性和潜力。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "11 pages, 1 figure, 4 tables",
      "pdf_url": "http://arxiv.org/pdf/2402.11955v1",
      "published_date": "2024-02-19 08:52:12 UTC",
      "updated_date": "2024-02-19 08:52:12 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T07:56:52.543530"
    },
    {
      "arxiv_id": "2402.11948v1",
      "title": "Mini-Hes: A Parallelizable Second-order Latent Factor Analysis Model",
      "title_zh": "Mini-Hes：一个可并行化的第二阶潜在因素分析模型",
      "authors": [
        "Jialiang Wang",
        "Weiling Li",
        "Yurong Zhong",
        "Xin Luo"
      ],
      "abstract": "Interactions among large number of entities is naturally high-dimensional and\nincomplete (HDI) in many big data related tasks. Behavioral characteristics of\nusers are hidden in these interactions, hence, effective representation of the\nHDI data is a fundamental task for understanding user behaviors. Latent factor\nanalysis (LFA) model has proven to be effective in representing HDI data. The\nperformance of an LFA model relies heavily on its training process, which is a\nnon-convex optimization. It has been proven that incorporating local curvature\nand preprocessing gradients during its training process can lead to superior\nperformance compared to LFA models built with first-order family methods.\nHowever, with the escalation of data volume, the feasibility of second-order\nalgorithms encounters challenges. To address this pivotal issue, this paper\nproposes a mini-block diagonal hessian-free (Mini-Hes) optimization for\nbuilding an LFA model. It leverages the dominant diagonal blocks in the\ngeneralized Gauss-Newton matrix based on the analysis of the Hessian matrix of\nLFA model and serves as an intermediary strategy bridging the gap between\nfirst-order and second-order optimization methods. Experiment results indicate\nthat, with Mini-Hes, the LFA model outperforms several state-of-the-art models\nin addressing missing data estimation task on multiple real HDI datasets from\nrecommender system. (The source code of Mini-Hes is available at\nhttps://github.com/Goallow/Mini-Hes)",
      "tldr_zh": "这篇论文针对高维不完整（HDI）数据的潜在因素分析（LFA）模型，提出了一种可并行化的第二阶优化方法Mini-Hes，以解决数据量增大时第二阶算法的计算挑战。Mini-Hes通过分析LFA模型的Hessian矩阵，聚焦于广义Gauss-Newton矩阵中的主导对角块，作为第一阶和第二阶优化方法的桥梁，从而提升模型训练效率和性能。实验结果表明，Mini-Hes在多个真实HDI数据集上的缺失数据估计任务中，优于现有state-of-the-art模型，为推荐系统等领域提供了更有效的用户行为表示。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "stat.ML"
      ],
      "primary_category": "cs.LG",
      "comment": "6 pages",
      "pdf_url": "http://arxiv.org/pdf/2402.11948v1",
      "published_date": "2024-02-19 08:43:00 UTC",
      "updated_date": "2024-02-19 08:43:00 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T07:57:05.416012"
    },
    {
      "arxiv_id": "2402.12417v1",
      "title": "Predicting trucking accidents with truck drivers 'safety climate perception across companies: A transfer learning approach",
      "title_zh": "翻译失败",
      "authors": [
        "Kailai Sun",
        "Tianxiang Lan",
        "Say Hong Kam",
        "Yang Miang Goh",
        "Yueng-Hsiang Huang"
      ],
      "abstract": "There is a rising interest in using artificial intelligence (AI)-powered\nsafety analytics to predict accidents in the trucking industry. Companies may\nface the practical challenge, however, of not having enough data to develop\ngood safety analytics models. Although pretrained models may offer a solution\nfor such companies, existing safety research using transfer learning has mostly\nfocused on computer vision and natural language processing, rather than\naccident analytics. To fill the above gap, we propose a pretrain-then-fine-tune\ntransfer learning approach to help any company leverage other companies' data\nto develop AI models for a more accurate prediction of accident risk. We also\ndevelop SafeNet, a deep neural network algorithm for classification tasks\nsuitable for accident prediction. Using the safety climate survey data from\nseven trucking companies with different data sizes, we show that our proposed\napproach results in better model performance compared to training the model\nfrom scratch using only the target company's data. We also show that for the\ntransfer learning model to be effective, the pretrained model should be\ndeveloped with larger datasets from diverse sources. The trucking industry may,\nthus, consider pooling safety analytics data from a wide range of companies to\ndevelop pretrained models and share them within the industry for better\nknowledge and resource transfer. The above contributions point to the promise\nof advanced safety analytics to make the industry safer and more sustainable.",
      "tldr_zh": "这篇论文提出了一种基于转移学习（transfer learning）的预训练-微调方法，用于预测卡车司机的安全气候感知与事故风险，帮助数据量不足的公司利用其他公司的数据提升预测准确性。研究开发了 SafeNet 算法，一种适用于分类任务的深度神经网络，并使用七家卡车公司的安全调查数据进行实验，结果显示转移学习方法比从零训练模型表现更好，尤其当预训练模型基于更大、更多样化的数据集时。总体贡献在于促进行业数据共享，推动卡车行业的安全分析更精确，从而提升可持续性。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "submitted to journal: accident analysis and prevention",
      "pdf_url": "http://arxiv.org/pdf/2402.12417v1",
      "published_date": "2024-02-19 08:27:53 UTC",
      "updated_date": "2024-02-19 08:27:53 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T07:57:17.688789"
    },
    {
      "arxiv_id": "2402.11934v1",
      "title": "Team QUST at SemEval-2024 Task 8: A Comprehensive Study of Monolingual and Multilingual Approaches for Detecting AI-generated Text",
      "title_zh": "翻译失败",
      "authors": [
        "Xiaoman Xu",
        "Xiangrun Li",
        "Taihang Wang",
        "Jianxiang Tian",
        "Ye Jiang"
      ],
      "abstract": "This paper presents the participation of team QUST in Task 8 SemEval 2024. We\nfirst performed data augmentation and cleaning on the dataset to enhance model\ntraining efficiency and accuracy. In the monolingual task, we evaluated\ntraditional deep-learning methods, multiscale positive-unlabeled framework\n(MPU), fine-tuning, adapters and ensemble methods. Then, we selected the\ntop-performing models based on their accuracy from the monolingual models and\nevaluated them in subtasks A and B. The final model construction employed a\nstacking ensemble that combined fine-tuning with MPU. Our system achieved 8th\n(scored 8th in terms of accuracy, officially ranked 13th) place in the official\ntest set in multilingual settings of subtask A. We release our system code\nat:https://github.com/warmth27/SemEval2024_QUST",
      "tldr_zh": "这篇论文介绍了团队 QUST 在 SemEval-2024 Task 8 中的参与，专注于检测 AI-generated Text 的单语和多语方法，通过数据增强和清洗来提升模型训练效率和准确性。研究评估了传统深度学习、MPU 框架、fine-tuning、adapters 和 ensemble methods 等技术，并在单语模型基础上构建堆叠集成模型用于子任务 A 和 B。最终系统在子任务 A 的多语设置中获得准确率排名第 8（官方排名第 13）的成绩，并开源了代码（https://github.com/warmth27/SemEval2024_QUST）。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2402.11934v1",
      "published_date": "2024-02-19 08:22:51 UTC",
      "updated_date": "2024-02-19 08:22:51 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T07:57:30.782824"
    },
    {
      "arxiv_id": "2402.12416v3",
      "title": "Aligning Individual and Collective Objectives in Multi-Agent Cooperation",
      "title_zh": "在多智能体合作中对齐个体与集体目标",
      "authors": [
        "Yang Li",
        "Wenhao Zhang",
        "Jianhong Wang",
        "Shao Zhang",
        "Yali Du",
        "Ying Wen",
        "Wei Pan"
      ],
      "abstract": "Among the research topics in multi-agent learning, mixed-motive cooperation\nis one of the most prominent challenges, primarily due to the mismatch between\nindividual and collective goals. The cutting-edge research is focused on\nincorporating domain knowledge into rewards and introducing additional\nmechanisms to incentivize cooperation. However, these approaches often face\nshortcomings such as the effort on manual design and the absence of theoretical\ngroundings. To close this gap, we model the mixed-motive game as a\ndifferentiable game for the ease of illuminating the learning dynamics towards\ncooperation. More detailed, we introduce a novel optimization method named\n\\textbf{\\textit{A}}ltruistic \\textbf{\\textit{G}}radient\n\\textbf{\\textit{A}}djustment (\\textbf{\\textit{AgA}}) that employs gradient\nadjustments to progressively align individual and collective objectives.\nFurthermore, we theoretically prove that AgA effectively attracts gradients to\nstable fixed points of the collective objective while considering individual\ninterests, and we validate these claims with empirical evidence. We evaluate\nthe effectiveness of our algorithm AgA through benchmark environments for\ntesting mixed-motive collaboration with small-scale agents such as the\ntwo-player public good game and the sequential social dilemma games, Cleanup\nand Harvest, as well as our self-developed large-scale environment in the game\nStarCraft II.",
      "tldr_zh": "本研究针对多智能体学习(multi-agent learning)中的混合动机合作(mixed-motive cooperation)挑战，提出了一种新颖的优化方法Altruistic Gradient Adjustment (AgA)，通过梯度调整逐步对齐个人和集体目标，避免了传统方法的手动设计缺陷。论文将混合动机游戏建模为可微游戏，并理论证明AgA能有效引导梯度向集体目标的稳定固定点收敛，同时兼顾个人利益。在基准环境中，如两玩家公共物品游戏和StarCraft II等，实验结果验证了AgA的有效性，展示了其在促进合作方面的潜力。",
      "categories": [
        "cs.MA",
        "cs.AI"
      ],
      "primary_category": "cs.MA",
      "comment": "20 pages; Accepted by NeurIPS 2024",
      "pdf_url": "http://arxiv.org/pdf/2402.12416v3",
      "published_date": "2024-02-19 08:18:53 UTC",
      "updated_date": "2024-10-22 18:10:01 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T07:57:41.974838"
    },
    {
      "arxiv_id": "2402.11925v1",
      "title": "Energy-Efficient Edge Learning via Joint Data Deepening-and-Prefetching",
      "title_zh": "翻译失败",
      "authors": [
        "Sujin Kook",
        "Won-Yong Shin",
        "Seong-Lyun Kim",
        "Seung-Woo Ko"
      ],
      "abstract": "The vision of pervasive artificial intelligence (AI) services can be realized\nby training an AI model on time using real-time data collected by internet of\nthings (IoT) devices. To this end, IoT devices require offloading their data to\nan edge server in proximity. However, transmitting high-dimensional and\nvoluminous data from energy-constrained IoT devices poses a significant\nchallenge. To address this limitation, we propose a novel offloading\narchitecture, called joint data deepening-and-prefetching (JD2P), which is\nfeature-by-feature offloading comprising two key techniques. The first one is\ndata deepening, where each data sample's features are sequentially offloaded in\nthe order of importance determined by the data embedding technique such as\nprinciple component analysis (PCA). Offloading is terminated once the already\ntransmitted features are sufficient for accurate data classification, resulting\nin a reduction in the amount of transmitted data. The criteria to offload data\nare derived for binary and multi-class classifiers, which are designed based on\nsupport vector machine (SVM) and deep neural network (DNN), respectively. The\nsecond one is data prefetching, where some features potentially required in the\nfuture are offloaded in advance, thus achieving high efficiency via precise\nprediction and parameter optimization. We evaluate the effectiveness of JD2P\nthrough experiments using the MNIST dataset, and the results demonstrate its\nsignificant reduction in expected energy consumption compared to several\nbenchmarks without degrading learning accuracy.",
      "tldr_zh": "这篇论文提出了一种名为JD2P的联合数据深化和预取架构，以提升IoT设备在边缘学习中的能量效率，解决高维数据传输带来的能量消耗挑战。JD2P的核心方法包括数据深化（利用PCA等技术按特征重要性顺序传输数据，并在特征足够支持SVM或DNN分类时停止传输）和数据预取（提前传输潜在未来需要的特征，通过预测优化实现高效传输）。实验结果显示，在MNIST数据集上，JD2P相比基准方法显著降低了预期能量消耗，同时保持了学习准确性。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.IT",
        "math.IT"
      ],
      "primary_category": "cs.LG",
      "comment": "accepted for publication in IEEE Transactions on Wireless\n  Communications. arXiv admin note: text overlap with arXiv:2211.07146",
      "pdf_url": "http://arxiv.org/pdf/2402.11925v1",
      "published_date": "2024-02-19 08:12:47 UTC",
      "updated_date": "2024-02-19 08:12:47 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T07:57:54.543912"
    },
    {
      "arxiv_id": "2404.07211v1",
      "title": "A real-time Artificial Intelligence system for learning Sign Language",
      "title_zh": "一种用于学习手语的实时人工智能系统",
      "authors": [
        "Elisa Cabana"
      ],
      "abstract": "A primary challenge for the deaf and hearing-impaired community stems from\nthe communication gap with the hearing society, which can greatly impact their\ndaily lives and result in social exclusion. To foster inclusivity in society,\nour endeavor focuses on developing a cost-effective, resource-efficient, and\nopen technology based on Artificial Intelligence, designed to assist people in\nlearning and using Sign Language for communication. The analysis presented in\nthis research paper intends to enrich the recent academic scientific literature\non Sign Language solutions based on Artificial Intelligence, with a particular\nfocus on American Sign Language (ASL). This research has yielded promising\npreliminary results and serves as a basis for further development.",
      "tldr_zh": "这篇论文针对聋哑和听力障碍社区的沟通障碍，提出了一种基于 Artificial Intelligence (AI) 的实时系统，帮助人们学习和使用手语，以促进社会包容。系统设计为成本效益高、资源高效且开源，特别聚焦于 American Sign Language (ASL)，并通过分析相关文献丰富了学术研究。初步实验结果显示了良好的前景，为进一步开发提供了基础。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2404.07211v1",
      "published_date": "2024-02-19 08:03:07 UTC",
      "updated_date": "2024-02-19 08:03:07 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T07:58:04.911561"
    },
    {
      "arxiv_id": "2402.11903v3",
      "title": "DiLA: Enhancing LLM Tool Learning with Differential Logic Layer",
      "title_zh": "DiLA：使用微分逻辑层增强LLM工具学习",
      "authors": [
        "Yu Zhang",
        "Hui-Ling Zhen",
        "Zehua Pei",
        "Yingzhao Lian",
        "Lihao Yin",
        "Mingxuan Yuan",
        "Bei Yu"
      ],
      "abstract": "Considering the challenges faced by large language models (LLMs) in logical\nreasoning and planning, prior efforts have sought to augment LLMs with access\nto external solvers. While progress has been made on simple reasoning problems,\nsolving classical constraint satisfaction problems, such as the Boolean\nSatisfiability Problem (SAT) and Graph Coloring Problem (GCP), remains\ndifficult for off-the-shelf solvers due to their intricate expressions and\nexponential search spaces. In this paper, we propose a novel differential logic\nlayer-aided language modeling (DiLA) approach, where logical constraints are\nintegrated into the forward and backward passes of a network layer, to provide\nanother option for LLM tool learning. In DiLA, LLM aims to transform the\nlanguage description to logic constraints and identify initial solutions of the\nhighest quality, while the differential logic layer focuses on iteratively\nrefining the LLM-prompted solution. Leveraging the logic layer as a bridge,\nDiLA enhances the logical reasoning ability of LLMs on a range of reasoning\nproblems encoded by Boolean variables, guaranteeing the efficiency and\ncorrectness of the solution process. We evaluate the performance of DiLA on two\nclassic reasoning problems and empirically demonstrate its consistent\noutperformance against existing prompt-based and solver-aided approaches.",
      "tldr_zh": "这篇论文针对大型语言模型(LLMs)在逻辑推理和规划方面的挑战，提出了DiLA方法，该方法通过差分逻辑层(differential logic layer)将逻辑约束整合到网络的正向和反向传播中，以增强LLMs的工具学习能力。在DiLA框架中，LLMs负责将语言描述转化为逻辑约束并生成高质量初始解决方案，而差分逻辑层则专注于迭代精炼这些解决方案，确保过程的效率和正确性。实验结果显示，DiLA在经典推理问题如Boolean Satisfiability Problem (SAT)和Graph Coloring Problem (GCP)上，显著优于现有基于提示和求解器辅助的方法。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "arXiv admin note: text overlap with arXiv:2305.12295 by other authors",
      "pdf_url": "http://arxiv.org/pdf/2402.11903v3",
      "published_date": "2024-02-19 07:38:57 UTC",
      "updated_date": "2024-06-19 02:52:00 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T07:58:18.786858"
    },
    {
      "arxiv_id": "2402.11901v1",
      "title": "Real-World Planning with PDDL+ and Beyond",
      "title_zh": "翻译失败",
      "authors": [
        "Wiktor Piotrowski",
        "Alexandre Perez"
      ],
      "abstract": "Real-world applications of AI Planning often require a highly expressive\nmodeling language to accurately capture important intricacies of target\nsystems. Hybrid systems are ubiquitous in the real-world, and PDDL+ is the\nstandardized modeling language for capturing such systems as planning domains.\nPDDL+ enables accurate encoding of mixed discrete-continuous system dynamics,\nexogenous activity, and many other interesting features exhibited in realistic\nscenarios. However, the uptake in usage of PDDL+ has been slow and\napprehensive, largely due to a general shortage of PDDL+ planning software, and\nrigid limitations of the few existing planners. To overcome this chasm, we\npresent Nyx, a novel PDDL+ planner built to emphasize lightness, simplicity,\nand, most importantly, adaptability. The planner is designed to be effortlessly\ncustomizable to expand its capabilities well beyond the scope of PDDL+. As a\nresult, Nyx can be tailored to virtually any potential real-world application\nrequiring some form of AI Planning, paving the way for wider adoption of\nplanning methods for solving real-world problems.",
      "tldr_zh": "AI 规划在真实世界应用中常需高度表达力的建模语言，如 PDDL+，以准确捕捉混合系统的离散-连续动态、外生活动等复杂特征，但其采用受限于软件短缺和规划器限制。研究引入了 Nyx，一种新型 PDDL+ 规划器，强调轻量、简单和可适应性，允许用户轻松自定义以扩展其功能超出 PDDL+ 的范围。通过这种设计，Nyx 可以适应几乎任何需要 AI Planning 的真实世界场景，促进规划方法的应用普及。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2402.11901v1",
      "published_date": "2024-02-19 07:35:49 UTC",
      "updated_date": "2024-02-19 07:35:49 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T07:58:30.080829"
    },
    {
      "arxiv_id": "2402.11893v3",
      "title": "Discerning and Resolving Knowledge Conflicts through Adaptive Decoding with Contextual Information-Entropy Constraint",
      "title_zh": "通过自适应解码与上下文信息熵约束辨别",
      "authors": [
        "Xiaowei Yuan",
        "Zhao Yang",
        "Yequan Wang",
        "Shengping Liu",
        "Jun Zhao",
        "Kang Liu"
      ],
      "abstract": "Large language models internalize enormous parametric knowledge during\npre-training. Concurrently, realistic applications necessitate external\ncontextual knowledge to aid models on the underlying tasks. This raises a\ncrucial dilemma known as knowledge conflicts, where the contextual knowledge\nclashes with the However, existing decoding works are specialized in resolving\nknowledge conflicts and could inadvertently deteriorate performance in absence\nof conflicts. In this paper, we propose an adaptive decoding method, termed as\ncontextual information-entropy constraint decoding (COIECD), to discern whether\nthe knowledge conflicts occur and resolve them. It can improve the model's\nfaithfulness to conflicting context, and simultaneously maintain high\nperformance among non- Our experiments show that COIECD exhibits strong\nperformance and robustness over knowledge conflicts in realistic datasets. Code\nis available.",
      "tldr_zh": "本文针对大语言模型在预训练中内部化的参数知识与外部上下文知识之间的知识 conflicts（知识冲突）问题，提出了一种自适应解码方法，名为 contextual information-entropy constraint decoding (COIECD)。COIECD 通过上下文信息熵约束来判断冲突是否发生，并在冲突时提高模型对上下文的忠实度，同时在无冲突情况下保持高性能。实验结果表明，该方法在真实数据集上表现出强劲的性能和鲁棒性，并提供了可用的代码支持。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "Accepted by Findings of ACL 2024",
      "pdf_url": "http://arxiv.org/pdf/2402.11893v3",
      "published_date": "2024-02-19 07:10:30 UTC",
      "updated_date": "2024-07-26 10:00:52 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T07:58:41.970271"
    },
    {
      "arxiv_id": "2402.11892v2",
      "title": "Towards Reliable Evaluation of Neural Program Repair with Natural Robustness Testing",
      "title_zh": "翻译失败",
      "authors": [
        "Thanh Le-Cong",
        "Dat Nguyen",
        "Bach Le",
        "Toby Murray"
      ],
      "abstract": "In this paper, we propose shifting the focus of robustness evaluation for\nNeural Program Repair (NPR) techniques toward naturally-occurring data\ntransformations. To accomplish this, we first examine the naturalness of\nsemantic-preserving transformations through a two-stage human study. This study\nincludes (1) interviews with senior software developers to establish concrete\ncriteria for evaluating the naturalness of these transformations, and (2) a\nsurvey involving 10 developers to assess the naturalness of 1,178\ntransformations, i.e., pairs of original and transformed programs, applied to\n225 real-world bugs. Our findings show that only 60% of these transformations\nare deemed natural, while 20% are considered unnatural, with strong agreement\namong annotators. Moreover, the unnaturalness of these transformations\nsignificantly impacts both their applicability to benchmarks and the\nconclusions drawn from robustness testing. Next, we conduct natural robustness\ntesting on NPR techniques to assess their true effectiveness against real-world\ndata variations. Our experimental results reveal a substantial number of\nprediction changes in NPR techniques, leading to significant reductions in both\nplausible and correct patch rates when comparing performance on the original\nand transformed datasets. Additionally, we observe notable differences in\nperformance improvements between NPR techniques, suggesting potential biases on\nNPR evaluation introduced by limited datasets. Finally, we propose an LLM-based\nmetric to automate the assessment of transformation naturalness, ensuring the\nscalability of natural robustness testing.",
      "tldr_zh": "这篇论文旨在提升 Neural Program Repair (NPR) 技术的鲁棒性评估可靠性，通过关注自然发生的数据变换来取代传统方法。研究者首先通过两阶段人类研究（包括开发者访谈和对1,178个变换的调查）发现，仅60%的语义保留变换被视为自然，而20%被认为不自然，这显著影响了变换的适用性和评估结论。在自然鲁棒性测试中，实验结果显示NPR技术在原始和变换数据集上的性能出现大量预测变化，导致正确和可信补丁率大幅下降，并揭示了不同NPR技术间的性能差异可能源于数据集偏差。最后，论文提出一个基于LLM的指标来自动评估变换的自然性，从而提升鲁棒性测试的可扩展性。",
      "categories": [
        "cs.SE",
        "cs.AI"
      ],
      "primary_category": "cs.SE",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2402.11892v2",
      "published_date": "2024-02-19 07:07:44 UTC",
      "updated_date": "2024-11-13 06:54:05 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T07:58:55.310930"
    },
    {
      "arxiv_id": "2402.14840v1",
      "title": "RJUA-MedDQA: A Multimodal Benchmark for Medical Document Question Answering and Clinical Reasoning",
      "title_zh": "翻译失败",
      "authors": [
        "Congyun Jin",
        "Ming Zhang",
        "Xiaowei Ma",
        "Li Yujiao",
        "Yingbo Wang",
        "Yabo Jia",
        "Yuliang Du",
        "Tao Sun",
        "Haowen Wang",
        "Cong Fan",
        "Jinjie Gu",
        "Chenfei Chi",
        "Xiangguo Lv",
        "Fangzhou Li",
        "Wei Xue",
        "Yiran Huang"
      ],
      "abstract": "Recent advancements in Large Language Models (LLMs) and Large Multi-modal\nModels (LMMs) have shown potential in various medical applications, such as\nIntelligent Medical Diagnosis. Although impressive results have been achieved,\nwe find that existing benchmarks do not reflect the complexity of real medical\nreports and specialized in-depth reasoning capabilities. In this work, we\nintroduced RJUA-MedDQA, a comprehensive benchmark in the field of medical\nspecialization, which poses several challenges: comprehensively interpreting\nimgage content across diverse challenging layouts, possessing numerical\nreasoning ability to identify abnormal indicators and demonstrating clinical\nreasoning ability to provide statements of disease diagnosis, status and advice\nbased on medical contexts. We carefully design the data generation pipeline and\nproposed the Efficient Structural Restoration Annotation (ESRA) Method, aimed\nat restoring textual and tabular content in medical report images. This method\nsubstantially enhances annotation efficiency, doubling the productivity of each\nannotator, and yields a 26.8% improvement in accuracy. We conduct extensive\nevaluations, including few-shot assessments of 5 LMMs which are capable of\nsolving Chinese medical QA tasks. To further investigate the limitations and\npotential of current LMMs, we conduct comparative experiments on a set of\nstrong LLMs by using image-text generated by ESRA method. We report the\nperformance of baselines and offer several observations: (1) The overall\nperformance of existing LMMs is still limited; however LMMs more robust to\nlow-quality and diverse-structured images compared to LLMs. (3) Reasoning\nacross context and image content present significant challenges. We hope this\nbenchmark helps the community make progress on these challenging tasks in\nmulti-modal medical document understanding and facilitate its application in\nhealthcare.",
      "tldr_zh": "本研究引入了RJUA-MedDQA，一种多模态基准，用于评估Large Language Models (LLMs) 和Large Multi-modal Models (LMMs)在医疗文档问答和临床推理中的性能，涵盖图像内容解释、数值推理和临床诊断等挑战。论文设计了数据生成管道并提出了Efficient Structural Restoration Annotation (ESRA)方法，该方法显著提升了医疗报告图像标注的效率（使每个标注者的生产力翻倍）和准确率（提高26.8%）。通过对5个LMMs和若干LLMs的广泛评估，发现现有LMMs整体性能有限，但比LLMs更robust于低质量和多样化图像，而跨上下文和图像内容的推理仍是主要难题。该基准有望推动社区在多模态医疗文档理解领域的进展，并促进其在医疗保健中的应用。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "stat.AP"
      ],
      "primary_category": "cs.CL",
      "comment": "15 pages, 13 figures",
      "pdf_url": "http://arxiv.org/pdf/2402.14840v1",
      "published_date": "2024-02-19 06:57:02 UTC",
      "updated_date": "2024-02-19 06:57:02 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T07:59:07.391035"
    },
    {
      "arxiv_id": "2402.11886v1",
      "title": "The Colorful Future of LLMs: Evaluating and Improving LLMs as Emotional Supporters for Queer Youth",
      "title_zh": "翻译失败",
      "authors": [
        "Shir Lissak",
        "Nitay Calderon",
        "Geva Shenkman",
        "Yaakov Ophir",
        "Eyal Fruchter",
        "Anat Brunstein Klomek",
        "Roi Reichart"
      ],
      "abstract": "Queer youth face increased mental health risks, such as depression, anxiety,\nand suicidal ideation. Hindered by negative stigma, they often avoid seeking\nhelp and rely on online resources, which may provide incompatible information.\nAlthough access to a supportive environment and reliable information is\ninvaluable, many queer youth worldwide have no access to such support. However,\nthis could soon change due to the rapid adoption of Large Language Models\n(LLMs) such as ChatGPT. This paper aims to comprehensively explore the\npotential of LLMs to revolutionize emotional support for queers. To this end,\nwe conduct a qualitative and quantitative analysis of LLM's interactions with\nqueer-related content. To evaluate response quality, we develop a novel\nten-question scale that is inspired by psychological standards and expert\ninput. We apply this scale to score several LLMs and human comments to posts\nwhere queer youth seek advice and share experiences. We find that LLM responses\nare supportive and inclusive, outscoring humans. However, they tend to be\ngeneric, not empathetic enough, and lack personalization, resulting in\nnonreliable and potentially harmful advice. We discuss these challenges,\ndemonstrate that a dedicated prompt can improve the performance, and propose a\nblueprint of an LLM-supporter that actively (but sensitively) seeks user\ncontext to provide personalized, empathetic, and reliable responses. Our\nannotated dataset is available for further research.",
      "tldr_zh": "这篇论文探讨了大型语言模型 (LLMs) 如 ChatGPT 在为酷儿青年提供情感支持方面的潜力，针对他们面临的心理健康风险（如抑郁和焦虑）进行定性和定量分析。研究者开发了一个基于心理标准和专家输入的十问题量表，对 LLMs 和人类回应进行评分，发现 LLMs 的回应更具支持性和包容性，但往往泛化、缺乏 empathetic 和个性化，可能导致不可靠或有害的建议。论文通过专用提示优化 LLMs 的性能，并提出一个蓝图，让模型主动（但敏感地）获取用户上下文，提供个性化的、empathetic 和可靠的回应；同时，提供了一个标注数据集以支持进一步研究。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2402.11886v1",
      "published_date": "2024-02-19 06:54:55 UTC",
      "updated_date": "2024-02-19 06:54:55 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T07:59:21.864124"
    },
    {
      "arxiv_id": "2402.11877v1",
      "title": "Finite-Time Error Analysis of Online Model-Based Q-Learning with a Relaxed Sampling Model",
      "title_zh": "翻译失败",
      "authors": [
        "Han-Dong Lim",
        "HyeAnn Lee",
        "Donghwan Lee"
      ],
      "abstract": "Reinforcement learning has witnessed significant advancements, particularly\nwith the emergence of model-based approaches. Among these, $Q$-learning has\nproven to be a powerful algorithm in model-free settings. However, the\nextension of $Q$-learning to a model-based framework remains relatively\nunexplored. In this paper, we delve into the sample complexity of $Q$-learning\nwhen integrated with a model-based approach. Through theoretical analyses and\nempirical evaluations, we seek to elucidate the conditions under which\nmodel-based $Q$-learning excels in terms of sample efficiency compared to its\nmodel-free counterpart.",
      "tldr_zh": "这篇论文对在线模型-based Q-learning 的有限时间错误分析进行了研究，引入了一个 relaxed sampling model 来扩展 Q-learning 到模型-based 框架。研究通过理论分析和经验评估，探讨了模型-based 方法在样本复杂度上相对于模型-free 方法的优势条件。结果表明，在特定场景下，模型-based Q-learning 能够显著提升样本效率。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2402.11877v1",
      "published_date": "2024-02-19 06:33:51 UTC",
      "updated_date": "2024-02-19 06:33:51 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T07:59:31.088803"
    },
    {
      "arxiv_id": "2402.11871v4",
      "title": "From Reals to Logic and Back: Inventing Symbolic Vocabularies, Actions, and Models for Planning from Raw Data",
      "title_zh": "翻译失败",
      "authors": [
        "Naman Shah",
        "Jayesh Nagpal",
        "Pulkit Verma",
        "Siddharth Srivastava"
      ],
      "abstract": "Hand-crafted, logic-based state and action representations have been widely\nused to overcome the intractable computational complexity of long-horizon robot\nplanning problems, including task and motion planning problems. However,\ncreating such representations requires experts with strong intuitions and\ndetailed knowledge about the robot and the tasks it may need to accomplish in a\ngiven setting. Removing this dependency on human intuition is a highly active\nresearch area.\n  This paper presents the first approach for autonomously learning\ngeneralizable, logic-based relational representations for abstract states and\nactions starting from unannotated high-dimensional, real-valued robot\ntrajectories. The learned representations constitute auto-invented PDDL-like\ndomain models. Empirical results in deterministic settings show that powerful\nabstract representations can be learned from just a handful of robot\ntrajectories; the learned relational representations include but go beyond\nclassical, intuitive notions of high-level actions; and that the learned models\nallow planning algorithms to scale to tasks that were previously beyond the\nscope of planning without hand-crafted abstractions.",
      "tldr_zh": "该论文解决了机器人规划问题中对专家手工创建逻辑-based 状态和动作表示的依赖，提出了一种从未标注的高维实值机器人轨迹中自主学习可泛化的逻辑-based 关系表示的方法。  \n该方法自动生成 PDDL-like 领域模型，包括超越经典高水平动作的概念，从而支持更抽象的规划。  \n实验结果表明，仅需少量轨迹即可学习强大表示，并在确定性设置下，使规划算法扩展到之前无法处理的复杂任务。",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2402.11871v4",
      "published_date": "2024-02-19 06:28:21 UTC",
      "updated_date": "2024-03-04 14:52:15 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T07:59:43.714362"
    },
    {
      "arxiv_id": "2402.11866v1",
      "title": "Two Online Map Matching Algorithms Based on Analytic Hierarchy Process and Fuzzy Logic",
      "title_zh": "翻译失败",
      "authors": [
        "Jeremy J. Lin",
        "Tomoro Mochida",
        "Riley C. W. O'Neill",
        "Atsuro Yoshida",
        "Masashi Yamazaki",
        "Akinobu Sasada"
      ],
      "abstract": "Our aim of this paper is to develop new map matching algorithms and to\nimprove upon previous work. We address two key approaches: Analytic Hierarchy\nProcess (AHP) map matching and fuzzy logic map matching. AHP is a\ndecision-making method that combines mathematical analysis with human judgment,\nand fuzzy logic is an approach to computing based on the degree of truth and\naims at modeling the imprecise modes of reasoning from 0 to 1 rather than the\nusual boolean logic. Of these algorithms, the way of our applying AHP to map\nmatching is newly developed in this paper, meanwhile, our application of fuzzy\nlogic to map matching is mostly the same as existing research except for some\nsmall changes. Because of the common characteristic that both methods are\ndesigned to handle imprecise information and simplicity for implementation, we\ndecided to use these methods.",
      "tldr_zh": "这篇论文开发了两种在线地图匹配算法：基于 Analytic Hierarchy Process (AHP) 和 Fuzzy Logic，旨在改进现有方法。AHP 算法新颖地结合了数学分析和人类判断来处理地图匹配决策，而 Fuzzy Logic 算法则在现有基础上进行了小改动，用于建模从 0 到 1 的不精确推理。两种方法都适用于处理不确定信息且实现简单，有助于提升地图匹配的准确性和实用性。",
      "categories": [
        "cs.CG",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.CG",
      "comment": "25 pages, 27 figures",
      "pdf_url": "http://arxiv.org/pdf/2402.11866v1",
      "published_date": "2024-02-19 06:14:46 UTC",
      "updated_date": "2024-02-19 06:14:46 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T07:59:56.140879"
    },
    {
      "arxiv_id": "2402.11842v1",
      "title": "CodeArt: Better Code Models by Attention Regularization When Symbols Are Lacking",
      "title_zh": "翻译失败",
      "authors": [
        "Zian Su",
        "Xiangzhe Xu",
        "Ziyang Huang",
        "Zhuo Zhang",
        "Yapeng Ye",
        "Jianjun Huang",
        "Xiangyu Zhang"
      ],
      "abstract": "Transformer based code models have impressive performance in many software\nengineering tasks. However, their effectiveness degrades when symbols are\nmissing or not informative. The reason is that the model may not learn to pay\nattention to the right correlations/contexts without the help of symbols. We\npropose a new method to pre-train general code models when symbols are lacking.\nWe observe that in such cases, programs degenerate to something written in a\nvery primitive language. We hence propose to use program analysis to extract\ncontexts a priori (instead of relying on symbols and masked language modeling\nas in vanilla models). We then leverage a novel attention masking method to\nonly allow the model attending to these contexts, e.g., bi-directional program\ndependence transitive closures and token co-occurrences. In the meantime, the\ninherent self-attention mechanism is utilized to learn which of the allowed\nattentions are more important compared to others. To realize the idea, we\nenhance the vanilla tokenization and model architecture of a BERT model,\nconstruct and utilize attention masks, and introduce a new pre-training\nalgorithm. We pre-train this BERT-like model from scratch, using a dataset of\n26 million stripped binary functions with explicit program dependence\ninformation extracted by our tool. We apply the model in three downstream\ntasks: binary similarity, type inference, and malware family classification.\nOur pre-trained model can improve the SOTAs in these tasks from 53% to 64%, 49%\nto 60%, and 74% to 94%, respectively. It also substantially outperforms other\ngeneral pre-training techniques of code understanding models.",
      "tldr_zh": "该论文提出 CodeArt 方法，通过注意力正则化（attention regularization）来提升 Transformer 基于的代码模型性能，尤其在符号缺失或不信息时。方法利用程序分析提取上下文（如双向程序依赖传递闭包和令牌共现），并引入新型注意力掩码机制，增强 BERT 模型的标记化和架构，以引导模型优先关注相关上下文。实验结果显示，在二进制相似性、类型推断和恶意软件家族分类任务上，CodeArt 模型将 SOTA 性能分别从 53% 提升至 64%、49% 至 60%、74% 至 94%，并优于其他代码理解模型的预训练技术。",
      "categories": [
        "cs.SE",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.SE",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2402.11842v1",
      "published_date": "2024-02-19 05:13:22 UTC",
      "updated_date": "2024-02-19 05:13:22 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T08:00:08.631990"
    },
    {
      "arxiv_id": "2402.12412v1",
      "title": "Dynamic and Super-Personalized Media Ecosystem Driven by Generative AI: Unpredictable Plays Never Repeating The Same",
      "title_zh": "翻译失败",
      "authors": [
        "Sungjun Ahn",
        "Hyun-Jeong Yim",
        "Youngwan Lee",
        "Sung-Ik Park"
      ],
      "abstract": "This paper introduces a media service model that exploits artificial\nintelligence (AI) video generators at the receive end. This proposal deviates\nfrom the traditional multimedia ecosystem, completely relying on in-house\nproduction, by shifting part of the content creation onto the receiver. We\nbring a semantic process into the framework, allowing the distribution network\nto provide service elements that prompt the content generator, rather than\ndistributing encoded data of fully finished programs. The service elements\ninclude fine-tailored text descriptions, lightweight image data of some\nobjects, or application programming interfaces, comprehensively referred to as\nsemantic sources, and the user terminal translates the received semantic data\ninto video frames. Empowered by the random nature of generative AI, the users\ncould then experience super-personalized services accordingly. The proposed\nidea incorporates the situations in which the user receives different service\nproviders' element packages; a sequence of packages over time, or multiple\npackages at the same time. Given promised in-context coherence and content\nintegrity, the combinatory dynamics will amplify the service diversity,\nallowing the users to always chance upon new experiences. This work\nparticularly aims at short-form videos and advertisements, which the users\nwould easily feel fatigued by seeing the same frame sequence every time. In\nthose use cases, the content provider's role will be recast as scripting\nsemantic sources, transformed from a thorough producer. Overall, this work\nexplores a new form of media ecosystem facilitated by receiver-embedded\ngenerative models, featuring both random content dynamics and enhanced delivery\nefficiency simultaneously.",
      "tldr_zh": "这篇论文提出了一种由生成式AI驱动的动态媒体生态系统，将内容生成移至接收端，使用语义来源（semantic sources）如文本描述、轻量级图像数据或API来提示用户终端创建视频帧，从而实现超个性化和随机内容。不同于传统媒体模式，该系统利用生成式AI的随机性，确保用户每次体验都不重复，并通过组合不同服务提供者的元素包来增强内容连贯性和多样性。论文重点应用于短视频和广告领域，旨在减少用户疲劳并提升交付效率，同时重新定义内容提供者为语义来源的脚本者。",
      "categories": [
        "cs.HC",
        "cs.AI",
        "cs.MM",
        "eess.SP"
      ],
      "primary_category": "cs.HC",
      "comment": "13 pages, 7 figures",
      "pdf_url": "http://arxiv.org/pdf/2402.12412v1",
      "published_date": "2024-02-19 04:39:30 UTC",
      "updated_date": "2024-02-19 04:39:30 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T08:00:20.109694"
    },
    {
      "arxiv_id": "2402.11818v1",
      "title": "Where It Really Matters: Few-Shot Environmental Conservation Media Monitoring for Low-Resource Languages",
      "title_zh": "翻译失败",
      "authors": [
        "Sameer Jain",
        "Sedrick Scott Keh",
        "Shova Chettri",
        "Karun Dewan",
        "Pablo Izquierdo",
        "Johanna Prussman",
        "Pooja Shreshtha",
        "Cesar Suarez",
        "Zheyuan Ryan Shi",
        "Lei Li",
        "Fei Fang"
      ],
      "abstract": "Environmental conservation organizations routinely monitor news content on\nconservation in protected areas to maintain situational awareness of\ndevelopments that can have an environmental impact. Existing automated media\nmonitoring systems require large amounts of data labeled by domain experts,\nwhich is only feasible at scale for high-resource languages like English.\nHowever, such tools are most needed in the global south where news of interest\nis mainly in local low-resource languages, and far fewer experts are available\nto annotate datasets sustainably. In this paper, we propose NewsSerow, a method\nto automatically recognize environmental conservation content in low-resource\nlanguages. NewsSerow is a pipeline of summarization, in-context few-shot\nclassification, and self-reflection using large language models (LLMs). Using\nat most 10 demonstration example news articles in Nepali, NewsSerow\nsignificantly outperforms other few-shot methods and achieves comparable\nperformance with models fully fine-tuned using thousands of examples. The World\nWide Fund for Nature (WWF) has deployed NewsSerow for media monitoring in\nNepal, significantly reducing their operational burden, and ensuring that AI\ntools for conservation actually reach the communities that need them the most.\nNewsSerow has also been deployed for countries with other languages like\nColombia.",
      "tldr_zh": "该论文针对低资源语言的环境保护媒体监控问题，提出 NewsSerow 方法，以解决现有系统对大量专家标注数据的需求。NewsSerow 是一个基于大型语言模型 (LLMs) 的管道，包括 summarization、in-context few-shot classification 和 self-reflection，仅需最多 10 个演示示例（如 Nepali 语言），即可显著优于其他 few-shot 方法，并与使用数千示例 fully fine-tuned 模型的性能相当。WWF 已将 NewsSerow 部署在尼泊尔和哥伦比亚等地，用于新闻监控，极大降低了操作负担，并确保 AI 工具惠及全球南方社区。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.CY"
      ],
      "primary_category": "cs.CL",
      "comment": "AAAI 2024: AI for Social Impact Track",
      "pdf_url": "http://arxiv.org/pdf/2402.11818v1",
      "published_date": "2024-02-19 04:17:21 UTC",
      "updated_date": "2024-02-19 04:17:21 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T08:00:32.980650"
    },
    {
      "arxiv_id": "2402.11815v2",
      "title": "HU at SemEval-2024 Task 8A: Can Contrastive Learning Learn Embeddings to Detect Machine-Generated Text?",
      "title_zh": "翻译失败",
      "authors": [
        "Shubhashis Roy Dipta",
        "Sadat Shahriar"
      ],
      "abstract": "This paper describes our system developed for SemEval-2024 Task 8,\n``Multigenerator, Multidomain, and Multilingual Black-Box Machine-Generated\nText Detection'' Machine-generated texts have been one of the main concerns due\nto the use of large language models (LLM) in fake text generation, phishing,\ncheating in exams, or even plagiarizing copyright materials. A lot of systems\nhave been developed to detect machine-generated text. Nonetheless, the majority\nof these systems rely on the text-generating model. This limitation is\nimpractical in real-world scenarios, as it's often impossible to know which\nspecific model the user has used for text generation. In this work, we propose\na $\\textbf{single}$ model based on contrastive learning, which uses\n$\\textbf{$\\approx$40% of the baseline's parameters}$ (149M vs. 355M) but shows\na comparable performance on the test dataset $(\\textbf{21st out of 137\nparticipants})$. Our key finding is that even without an ensemble of multiple\nmodels, a single base model can have comparable performance with the help of\ndata augmentation and contrastive learning. Our code is publicly available at\nhttps://github.com/dipta007/SemEval24-Task8.",
      "tldr_zh": "本论文参与了 SemEval-2024 Task 8A，探讨对比学习（contrastive learning）是否能学习嵌入以检测多生成器、多领域和多语言的黑盒机器生成文本。作者提出了一种单一模型，仅使用基线参数的约 40%（149M vs. 355M），通过数据增强（data augmentation）和对比学习实现高效检测，避免了对具体文本生成模型的依赖。实验结果显示，该模型在测试数据集上性能与基线相当，排名第 21 名（137 参与者中），证明单一模型结合对比学习即可实现可比表现，并公开了代码以供参考。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "Camera Ready Version - Accepted in SemEval 2024 (Colocated with NAACL\n  2024)",
      "pdf_url": "http://arxiv.org/pdf/2402.11815v2",
      "published_date": "2024-02-19 04:11:34 UTC",
      "updated_date": "2024-03-27 20:30:08 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T08:00:44.892935"
    },
    {
      "arxiv_id": "2402.11813v2",
      "title": "A novel framework for adaptive stress testing of autonomous vehicles in multi-lane roads",
      "title_zh": "翻译失败",
      "authors": [
        "Linh Trinh",
        "Quang-Hung Luu",
        "Thai M. Nguyen",
        "Hai L. Vu"
      ],
      "abstract": "Stress testing is an approach for evaluating the reliability of systems under\nextreme conditions which help reveal vulnerable scenarios that standard testing\nmay overlook. Identifying such scenarios is of great importance in autonomous\nvehicles (AV) and other safety-critical systems. Since failure events are rare,\nnaive random search approaches require a large number of vehicle operation\nhours to identify potential system failures. Adaptive Stress Testing (AST) is a\nmethod addressing this constraint by effectively exploring the failure\ntrajectories of AV using a Markov decision process and employs reinforcement\nlearning techniques to identify driving scenarios with high probability of\nfailures. However, existing AST frameworks are able to handle only simple\nscenarios, such as one vehicle moving longitudinally on a single lane road\nwhich is not realistic and has a limited applicability. In this paper, we\npropose a novel AST framework to systematically explore corner cases of\nintelligent driving models that can result in safety concerns involving both\nlongitudinal and lateral vehicle's movements. Specially, we develop a new\nreward function for Deep Reinforcement Learning to guide the AST in identifying\ncrash scenarios based on the collision probability estimate between the AV\nunder test (i.e., the ego vehicle) and the trajectory of other vehicles on the\nmulti-lane roads. To demonstrate the effectiveness of our framework, we tested\nit with a complex driving model vehicle that can be controlled in both\nlongitudinal and lateral directions. Quantitative and qualitative analyses of\nour experimental results demonstrate that our framework outperforms the\nstate-of-the-art AST scheme in identifying corner cases with complex driving\nmaneuvers.",
      "tldr_zh": "该论文提出了一种新型框架，用于在多车道道路上进行自适应压力测试（Adaptive Stress Testing, AST），以评估自动驾驶车辆（Autonomous Vehicles, AV）在极端条件下的可靠性，并识别标准测试可能忽略的失败场景。框架利用Markov Decision Process和强化学习（Reinforcement Learning）技术，开发了一个新的奖励函数基于碰撞概率，系统地探索涉及纵向和横向车辆运动的复杂驾驶场景。实验结果显示，该框架在测试复杂驾驶模型时，显著优于现有AST方案，能够更有效地识别潜在安全风险的极端情况（corner cases）。",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2402.11813v2",
      "published_date": "2024-02-19 04:02:40 UTC",
      "updated_date": "2024-09-19 08:27:47 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T08:00:56.218983"
    },
    {
      "arxiv_id": "2402.11809v3",
      "title": "Generation Meets Verification: Accelerating Large Language Model Inference with Smart Parallel Auto-Correct Decoding",
      "title_zh": "翻译失败",
      "authors": [
        "Hanling Yi",
        "Feng Lin",
        "Hongbin Li",
        "Peiyang Ning",
        "Xiaotian Yu",
        "Rong Xiao"
      ],
      "abstract": "This research aims to accelerate the inference speed of large language models\n(LLMs) with billions of parameters. We propose \\textbf{S}mart \\textbf{P}arallel\n\\textbf{A}uto-\\textbf{C}orrect d\\textbf{E}coding (SPACE), an innovative\napproach designed for achieving lossless acceleration of LLMs. By integrating\nsemi-autoregressive inference and speculative decoding capabilities, SPACE\nuniquely enables autoregressive LLMs to parallelize token generation and\nverification. This is realized through a specialized semi-autoregressive\nsupervised fine-tuning process that equips existing LLMs with the ability to\nsimultaneously predict multiple tokens. Additionally, an auto-correct decoding\nalgorithm facilitates the simultaneous generation and verification of token\nsequences within a single model invocation. Through extensive experiments on a\nrange of LLMs, SPACE has demonstrated inference speedup ranging from 2.7x-4.0x\non HumanEval-X while maintaining output quality.",
      "tldr_zh": "这篇论文提出了一种名为 Smart Parallel Auto-Correct Decoding (SPACE) 的方法，用于加速大型语言模型 (LLMs) 的推理过程，实现无损加速。\nSPACE 整合了半自回归推理和推测解码技术，通过专门的监督微调，让 LLMs 能够并行生成和验证多个 tokens，并在单次模型调用中处理 token 序列的生成与校正。\n实验结果显示，在 HumanEval-X 测试集上，SPACE 实现了 2.7x 到 4.0x 的推理速度提升，同时保持了输出质量的一致性。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "Accepted by ACL 2024 Findings",
      "pdf_url": "http://arxiv.org/pdf/2402.11809v3",
      "published_date": "2024-02-19 03:39:10 UTC",
      "updated_date": "2024-05-20 01:48:18 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T08:01:08.211058"
    },
    {
      "arxiv_id": "2402.11804v3",
      "title": "LLM as Prompter: Low-resource Inductive Reasoning on Arbitrary Knowledge Graphs",
      "title_zh": "LLM 作为提示器：针对任意知识图谱的低资源归纳推理",
      "authors": [
        "Kai Wang",
        "Yuwei Xu",
        "Zhiyong Wu",
        "Siqiang Luo"
      ],
      "abstract": "Knowledge Graph (KG) inductive reasoning, which aims to infer missing facts\nfrom new KGs that are not seen during training, has been widely adopted in\nvarious applications. One critical challenge of KG inductive reasoning is\nhandling low-resource scenarios with scarcity in both textual and structural\naspects. In this paper, we attempt to address this challenge with Large\nLanguage Models (LLMs). Particularly, we utilize the state-of-the-art LLMs to\ngenerate a graph-structural prompt to enhance the pre-trained Graph Neural\nNetworks (GNNs), which brings us new methodological insights into the KG\ninductive reasoning methods, as well as high generalizability in practice. On\nthe methodological side, we introduce a novel pretraining and prompting\nframework ProLINK, designed for low-resource inductive reasoning across\narbitrary KGs without requiring additional training. On the practical side, we\nexperimentally evaluate our approach on 36 low-resource KG datasets and find\nthat ProLINK outperforms previous methods in three-shot, one-shot, and\nzero-shot reasoning tasks, exhibiting average performance improvements by 20%,\n45%, and 147%, respectively. Furthermore, ProLINK demonstrates strong\nrobustness for various LLM promptings as well as full-shot scenarios.",
      "tldr_zh": "该论文探讨了在低资源场景下，对任意知识图谱(KG)进行归纳推理的问题，提出使用大型语言模型(LLM)作为提示生成器来增强预训练的图神经网络(GNNs)。他们引入了ProLINK框架，该框架通过生成图结构提示实现无需额外训练的跨KG推理，提高了方法的泛化性和实用性。在36个低资源KG数据集上的实验中，ProLINK在三-shot、一-shot和zero-shot任务中分别比先前方法平均提升20%、45%和147%的性能，并展示了在各种LLM提示和全-shot场景下的鲁棒性。",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.SI"
      ],
      "primary_category": "cs.AI",
      "comment": "Accepted by Findings of ACL2024",
      "pdf_url": "http://arxiv.org/pdf/2402.11804v3",
      "published_date": "2024-02-19 03:21:19 UTC",
      "updated_date": "2024-06-19 09:00:53 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T08:01:21.705142"
    },
    {
      "arxiv_id": "2402.11800v3",
      "title": "Stochastic Approximation with Delayed Updates: Finite-Time Rates under Markovian Sampling",
      "title_zh": "随机逼近算法的延迟更新：Markovian采样下的有限时间收敛率",
      "authors": [
        "Arman Adibi",
        "Nicolo Dal Fabbro",
        "Luca Schenato",
        "Sanjeev Kulkarni",
        "H. Vincent Poor",
        "George J. Pappas",
        "Hamed Hassani",
        "Aritra Mitra"
      ],
      "abstract": "Motivated by applications in large-scale and multi-agent reinforcement\nlearning, we study the non-asymptotic performance of stochastic approximation\n(SA) schemes with delayed updates under Markovian sampling. While the effect of\ndelays has been extensively studied for optimization, the manner in which they\ninteract with the underlying Markov process to shape the finite-time\nperformance of SA remains poorly understood. In this context, our first main\ncontribution is to show that under time-varying bounded delays, the delayed SA\nupdate rule guarantees exponentially fast convergence of the \\emph{last\niterate} to a ball around the SA operator's fixed point. Notably, our bound is\n\\emph{tight} in its dependence on both the maximum delay $\\tau_{max}$, and the\nmixing time $\\tau_{mix}$. To achieve this tight bound, we develop a novel\ninductive proof technique that, unlike various existing delayed-optimization\nanalyses, relies on establishing uniform boundedness of the iterates. As such,\nour proof may be of independent interest. Next, to mitigate the impact of the\nmaximum delay on the convergence rate, we provide the first finite-time\nanalysis of a delay-adaptive SA scheme under Markovian sampling. In particular,\nwe show that the exponent of convergence of this scheme gets scaled down by\n$\\tau_{avg}$, as opposed to $\\tau_{max}$ for the vanilla delayed SA rule; here,\n$\\tau_{avg}$ denotes the average delay across all iterations. Moreover, the\nadaptive scheme requires no prior knowledge of the delay sequence for step-size\ntuning. Our theoretical findings shed light on the finite-time effects of\ndelays for a broad class of algorithms, including TD learning, Q-learning, and\nstochastic gradient descent under Markovian sampling.",
      "tldr_zh": "本研究探讨了随机逼近(SA)方案在延迟更新和Markovian采样下的有限时间性能，针对大规模和多智能体强化学习应用。论文的主要贡献包括证明在时间变化的有限延迟下，延迟SA更新规则能使最后迭代指数级收敛到SA算子固定点附近的一个球形区域，且收敛边界紧依赖于最大延迟τ_max和混合时间τ_mix；为此，开发了一种新颖的归纳证明技术，依赖于迭代的统一有界性。为了减轻延迟影响，论文首次分析了延迟自适应SA方案在Markovian采样下的有限时间性能，该方案的收敛指数由平均延迟τ_avg缩放，而非τ_max，且无需事先知道延迟序列即可调整步长。这些发现为TD学习、Q学习和随机梯度下降等算法提供了重要洞见。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.MA",
        "cs.SY",
        "eess.SY",
        "math.OC"
      ],
      "primary_category": "cs.LG",
      "comment": "Accepted to the 27th International Conference on Artificial\n  Intelligence and Statistics (AISTATS) 2024!",
      "pdf_url": "http://arxiv.org/pdf/2402.11800v3",
      "published_date": "2024-02-19 03:08:02 UTC",
      "updated_date": "2024-03-27 15:48:29 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T08:01:33.769049"
    },
    {
      "arxiv_id": "2402.11793v4",
      "title": "Generative Kaleidoscopic Networks",
      "title_zh": "翻译失败",
      "authors": [
        "Harsh Shrivastava"
      ],
      "abstract": "We discovered that the neural networks, especially the deep ReLU networks,\ndemonstrate an `over-generalization' phenomenon. That is, the output values for\nthe inputs that were not seen during training are mapped close to the output\nrange that were observed during the learning process. In other words, the\nneural networks learn a many-to-one mapping and this effect is more prominent\nas we increase the number of layers or the depth of the neural network. We\nutilize this property of neural networks to design a dataset kaleidoscope,\ntermed as `Generative Kaleidoscopic Networks'. Succinctly, if we learn a model\nto map from input $x\\in\\mathbb{R}^D$ to itself $f_\\mathcal{N}(x)\\rightarrow x$,\nthe proposed `Kaleidoscopic sampling' procedure starts with a random input\nnoise $z\\in\\mathbb{R}^D$ and recursively applies $f_\\mathcal{N}(\\cdots\nf_\\mathcal{N}(z)\\cdots )$. After a burn-in period duration, we start observing\nsamples from the input distribution and the quality of samples recovered\nimproves as we increase the depth of the model. Scope: We observed this\nphenomenon to various degrees for the other deep learning architectures like\nCNNs, Transformers & U-Nets and we are currently investigating them further.",
      "tldr_zh": "这篇论文揭示了神经网络，尤其是深层 ReLU networks 的“over-generalization”现象，即对训练中未见输入的输出值会映射到训练观察范围附近，这种多对一映射效应随网络深度增加而更显著。作者利用这一特性，提出“Generative Kaleidoscopic Networks”框架，通过“Kaleidoscopic sampling”过程：从随机噪声 z 开始，递归应用训练模型 f_N(x) → x，经过烧入期后生成高质量输入分布样本。实验显示，样本质量随模型深度提升而改善，且该现象在 CNNs、Transformers 和 U-Nets 等架构中也有不同程度出现，正在进一步探索。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2402.11793v4",
      "published_date": "2024-02-19 02:48:40 UTC",
      "updated_date": "2024-10-22 04:15:49 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T08:01:44.413048"
    },
    {
      "arxiv_id": "2402.12411v1",
      "title": "Deep Structural Knowledge Exploitation and Synergy for Estimating Node Importance Value on Heterogeneous Information Networks",
      "title_zh": "针对异构信息网络中节点重要性值估计的深度结构知识利用与协同效应",
      "authors": [
        "Yankai Chen",
        "Yixiang Fang",
        "Qiongyan Wang",
        "Xin Cao",
        "Irwin King"
      ],
      "abstract": "Node importance estimation problem has been studied conventionally with\nhomogeneous network topology analysis. To deal with network heterogeneity, a\nfew recent methods employ graph neural models to automatically learn diverse\nsources of information. However, the major concern revolves around that their\nfull adaptive learning process may lead to insufficient information\nexploration, thereby formulating the problem as the isolated node value\nprediction with underperformance and less interpretability. In this work, we\npropose a novel learning framework: SKES. Different from previous automatic\nlearning designs, SKES exploits heterogeneous structural knowledge to enrich\nthe informativeness of node representations. Based on a sufficiently\nuninformative reference, SKES estimates the importance value for any input\nnode, by quantifying its disparity against the reference. This establishes an\ninterpretable node importance computation paradigm. Furthermore, SKES dives\ndeep into the understanding that \"nodes with similar characteristics are prone\nto have similar importance values\" whilst guaranteeing that such\ninformativeness disparity between any different nodes is orderly reflected by\nthe embedding distance of their associated latent features. Extensive\nexperiments on three widely-evaluated benchmarks demonstrate the performance\nsuperiority of SKES over several recent competing methods.",
      "tldr_zh": "这篇论文针对异构信息网络(Heterogeneous Information Networks)上节点重要性估计问题，提出了一种新框架SKES，以解决现有图神经模型(Graph Neural Models)因信息探索不足而导致的性能和可解释性问题。SKES通过挖掘异构结构知识来丰富节点表示，并通过量化节点与一个无信息参考的差异来计算重要性值，同时确保相似特征节点的嵌入距离有序反映其信息差异。该方法在三个基准数据集上的实验结果显示，SKES的性能显著优于其他竞争方法，提供了一个可解释的节点重要性计算范式。",
      "categories": [
        "cs.SI",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.SI",
      "comment": "Accepted by AAAI 2024",
      "pdf_url": "http://arxiv.org/pdf/2402.12411v1",
      "published_date": "2024-02-19 02:34:23 UTC",
      "updated_date": "2024-02-19 02:34:23 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T08:01:56.212006"
    },
    {
      "arxiv_id": "2402.11788v1",
      "title": "MM-SurvNet: Deep Learning-Based Survival Risk Stratification in Breast Cancer Through Multimodal Data Fusion",
      "title_zh": "翻译失败",
      "authors": [
        "Raktim Kumar Mondol",
        "Ewan K. A. Millar",
        "Arcot Sowmya",
        "Erik Meijering"
      ],
      "abstract": "Survival risk stratification is an important step in clinical decision making\nfor breast cancer management. We propose a novel deep learning approach for\nthis purpose by integrating histopathological imaging, genetic and clinical\ndata. It employs vision transformers, specifically the MaxViT model, for image\nfeature extraction, and self-attention to capture intricate image relationships\nat the patient level. A dual cross-attention mechanism fuses these features\nwith genetic data, while clinical data is incorporated at the final layer to\nenhance predictive accuracy. Experiments on the public TCGA-BRCA dataset show\nthat our model, trained using the negative log likelihood loss function, can\nachieve superior performance with a mean C-index of 0.64, surpassing existing\nmethods. This advancement facilitates tailored treatment strategies,\npotentially leading to improved patient outcomes.",
      "tldr_zh": "本文提出MM-SurvNet，一种基于深度学习的模型，用于乳腺癌的生存风险分层，通过融合组织病理图像、遗传和临床数据。该模型采用Vision Transformers（特别是MaxViT）进行图像特征提取，并利用自注意力和双重交叉注意力机制整合多模态特征，最终层中纳入临床数据以提升预测准确性。在TCGA-BRCA数据集上实验显示，该模型使用负对数似然损失函数训练，平均C-index达到0.64，优于现有方法，从而支持个性化治疗策略并改善患者预后。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "Keywords: Multimodal Fusion, Breast Cancer, Whole Slide Images,\n  Survival Prediction",
      "pdf_url": "http://arxiv.org/pdf/2402.11788v1",
      "published_date": "2024-02-19 02:31:36 UTC",
      "updated_date": "2024-02-19 02:31:36 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T08:02:09.230018"
    },
    {
      "arxiv_id": "2402.16882v2",
      "title": "Revealing the Relationship Between Publication Bias and Chemical Reactivity with Contrastive Learning",
      "title_zh": "通过对比学习揭示发表偏差与化学反应性之间的关系",
      "authors": [
        "Wenhao Gao",
        "Priyanka Raghavan",
        "Ron Shprints",
        "Connor W. Coley"
      ],
      "abstract": "A synthetic method's substrate tolerance and generality are often showcased\nin a \"substrate scope\" table. However, substrate selection exhibits a\nfrequently discussed publication bias: unsuccessful experiments or low-yielding\nresults are rarely reported. In this work, we explore more deeply the\nrelationship between such publication bias and chemical reactivity beyond the\nsimple analysis of yield distributions using a novel neural network training\nstrategy, substrate scope contrastive learning. By treating reported substrates\nas positive samples and non-reported substrates as negative samples, our\ncontrastive learning strategy teaches a model to group molecules within a\nnumerical embedding space, based on historical trends in published substrate\nscope tables. Training on 20,798 aryl halides in the CAS Content\nCollection$^{\\text{TM}}$, spanning thousands of publications from 2010-2015, we\ndemonstrate that the learned embeddings exhibit a correlation with physical\norganic reactivity descriptors through both intuitive visualizations and\nquantitative regression analyses. Additionally, these embeddings are applicable\nto various reaction modeling tasks like yield prediction and regioselectivity\nprediction, underscoring the potential to use historical reaction data as a\npre-training task. This work not only presents a chemistry-specific machine\nlearning training strategy to learn from literature data in a new way, but also\nrepresents a unique approach to uncover trends in chemical reactivity reflected\nby trends in substrate selection in publications.",
      "tldr_zh": "这篇论文探讨了出版偏差（publication bias）对化学反应性的影响，提出了一种新颖的训练策略：substrate scope contrastive learning，通过将报道的底物作为正样本、非报道的底物作为负样本，在数值嵌入空间中对分子进行分组。研究利用20,798个aryl halides的数据，从2010-2015年的出版物中训练模型，结果显示学到的嵌入与物理有机反应性描述符（physical organic reactivity descriptors）相关，并通过可视化和定量回归分析得到证实。该方法不仅适用于反应建模任务如产率预测和regioselectivity prediction，还为从历史文献数据中挖掘化学趋势提供了一种创新的机器学习途径。",
      "categories": [
        "physics.chem-ph",
        "cs.AI",
        "cs.LG",
        "q-bio.BM"
      ],
      "primary_category": "physics.chem-ph",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2402.16882v2",
      "published_date": "2024-02-19 02:21:20 UTC",
      "updated_date": "2025-02-20 16:50:35 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T08:02:21.923776"
    },
    {
      "arxiv_id": "2402.11780v2",
      "title": "CiMNet: Towards Joint Optimization for DNN Architecture and Configuration for Compute-In-Memory Hardware",
      "title_zh": "翻译失败",
      "authors": [
        "Souvik Kundu",
        "Anthony Sarah",
        "Vinay Joshi",
        "Om J Omer",
        "Sreenivas Subramoney"
      ],
      "abstract": "With the recent growth in demand for large-scale deep neural networks,\ncompute in-memory (CiM) has come up as a prominent solution to alleviate\nbandwidth and on-chip interconnect bottlenecks that constrain Von-Neuman\narchitectures. However, the construction of CiM hardware poses a challenge as\nany specific memory hierarchy in terms of cache sizes and memory bandwidth at\ndifferent interfaces may not be ideally matched to any neural network's\nattributes such as tensor dimension and arithmetic intensity, thus leading to\nsuboptimal and under-performing systems. Despite the success of neural\narchitecture search (NAS) techniques in yielding efficient sub-networks for a\ngiven hardware metric budget (e.g., DNN execution time or latency), it assumes\nthe hardware configuration to be frozen, often yielding sub-optimal\nsub-networks for a given budget. In this paper, we present CiMNet, a framework\nthat jointly searches for optimal sub-networks and hardware configurations for\nCiM architectures creating a Pareto optimal frontier of downstream task\naccuracy and execution metrics (e.g., latency). The proposed framework can\ncomprehend the complex interplay between a sub-network's performance and the\nCiM hardware configuration choices including bandwidth, processing element\nsize, and memory size. Exhaustive experiments on different model architectures\nfrom both CNN and Transformer families demonstrate the efficacy of the CiMNet\nin finding co-optimized sub-networks and CiM hardware configurations.\nSpecifically, for similar ImageNet classification accuracy as baseline ViT-B,\noptimizing only the model architecture increases performance (or reduces\nworkload execution time) by 1.7x while optimizing for both the model\narchitecture and hardware configuration increases it by 3.1x.",
      "tldr_zh": "该研究提出 CiMNet 框架，旨在联合优化深度神经网络 (DNN) 架构和 Compute-in-Memory (CiM) 硬件配置，以解决传统硬件配置与神经网络属性不匹配导致的性能瓶颈问题。CiMNet 通过神经架构搜索 (NAS) 技术同时搜索最佳子网络和硬件参数（如带宽、处理元素大小和内存大小），生成 Pareto 最优前沿以平衡任务准确性和执行指标（如延迟）。实验结果显示，在 CNN 和 Transformer 模型上，相比仅优化模型架构（性能提升 1.7 倍），联合优化模型和硬件配置可将性能提高 3.1 倍，例如在 ImageNet 分类任务中实现与 ViT-B 相似的准确率。",
      "categories": [
        "cs.AR",
        "cs.AI"
      ],
      "primary_category": "cs.AR",
      "comment": "6 pages, 4 figures, 5 tables; Accepted as a full paper by the tinyML\n  Research Symposium 2024",
      "pdf_url": "http://arxiv.org/pdf/2402.11780v2",
      "published_date": "2024-02-19 02:12:07 UTC",
      "updated_date": "2024-03-18 15:25:30 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T08:02:32.858701"
    },
    {
      "arxiv_id": "2402.11778v2",
      "title": "Towards Theoretical Understandings of Self-Consuming Generative Models",
      "title_zh": "翻译失败",
      "authors": [
        "Shi Fu",
        "Sen Zhang",
        "Yingjie Wang",
        "Xinmei Tian",
        "Dacheng Tao"
      ],
      "abstract": "This paper tackles the emerging challenge of training generative models\nwithin a self-consuming loop, wherein successive generations of models are\nrecursively trained on mixtures of real and synthetic data from previous\ngenerations. We construct a theoretical framework to rigorously evaluate how\nthis training procedure impacts the data distributions learned by future\nmodels, including parametric and non-parametric models. Specifically, we derive\nbounds on the total variation (TV) distance between the synthetic data\ndistributions produced by future models and the original real data distribution\nunder various mixed training scenarios for diffusion models with a\none-hidden-layer neural network score function. Our analysis demonstrates that\nthis distance can be effectively controlled under the condition that mixed\ntraining dataset sizes or proportions of real data are large enough.\nInterestingly, we further unveil a phase transition induced by expanding\nsynthetic data amounts, proving theoretically that while the TV distance\nexhibits an initial ascent, it declines beyond a threshold point. Finally, we\npresent results for kernel density estimation, delivering nuanced insights such\nas the impact of mixed data training on error propagation.",
      "tldr_zh": "本文探讨了生成模型的自消耗循环训练问题，即模型递归使用真实数据和之前生成合成数据的混合数据集进行训练。研究构建了一个理论框架，针对扩散模型(diffusion models)推导了合成数据分布与原始数据分布之间总变差距离(TV distance)的边界，并证明在混合数据集规模或真实数据比例足够大时，该距离可被有效控制。分析揭示了一个相变(phase transition)现象：随着合成数据量的增加，TV距离先上升后下降。最终，该框架还扩展到核密度估计(kernel density estimation)，提供了混合数据训练对错误传播的细致洞见。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "Accepted at ICML 2024",
      "pdf_url": "http://arxiv.org/pdf/2402.11778v2",
      "published_date": "2024-02-19 02:08:09 UTC",
      "updated_date": "2024-06-24 14:23:30 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T08:02:45.843738"
    },
    {
      "arxiv_id": "2402.11777v1",
      "title": "Uncovering Latent Human Wellbeing in Language Model Embeddings",
      "title_zh": "翻译失败",
      "authors": [
        "Pedro Freire",
        "ChengCheng Tan",
        "Adam Gleave",
        "Dan Hendrycks",
        "Scott Emmons"
      ],
      "abstract": "Do language models implicitly learn a concept of human wellbeing? We explore\nthis through the ETHICS Utilitarianism task, assessing if scaling enhances\npretrained models' representations. Our initial finding reveals that, without\nany prompt engineering or finetuning, the leading principal component from\nOpenAI's text-embedding-ada-002 achieves 73.9% accuracy. This closely matches\nthe 74.6% of BERT-large finetuned on the entire ETHICS dataset, suggesting\npretraining conveys some understanding about human wellbeing. Next, we consider\nfour language model families, observing how Utilitarianism accuracy varies with\nincreased parameters. We find performance is nondecreasing with increased model\nsize when using sufficient numbers of principal components.",
      "tldr_zh": "这篇论文探讨了语言模型嵌入中是否隐式包含了人类福祉的概念，通过 ETHICS Utilitarianism 任务评估预训练模型的表现。研究发现，OpenAI 的 text-embedding-ada-002 的主要主成分在无提示工程或微调的情况下，达到了 73.9% 的准确率，几乎与在整个 ETHICS 数据集上微调的 BERT-large 的 74.6% 相当。进一步分析四个语言模型家族显示，随着模型参数增加，使用足够数量的 principal components 时，Utilitarianism 任务的准确率不会下降，这表明预训练过程可能已隐式学习了相关概念。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG",
        "I.2.7"
      ],
      "primary_category": "cs.CL",
      "comment": "10 pages, 5 figures, 1 table",
      "pdf_url": "http://arxiv.org/pdf/2402.11777v1",
      "published_date": "2024-02-19 02:08:03 UTC",
      "updated_date": "2024-02-19 02:08:03 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T08:02:57.324109"
    },
    {
      "arxiv_id": "2402.11773v2",
      "title": "Dynamic Multi-Network Mining of Tensor Time Series",
      "title_zh": "翻译失败",
      "authors": [
        "Kohei Obata",
        "Koki Kawabata",
        "Yasuko Matsubara",
        "Yasushi Sakurai"
      ],
      "abstract": "Subsequence clustering of time series is an essential task in data mining,\nand interpreting the resulting clusters is also crucial since we generally do\nnot have prior knowledge of the data. Thus, given a large collection of tensor\ntime series consisting of multiple modes, including timestamps, how can we\nachieve subsequence clustering for tensor time series and provide interpretable\ninsights? In this paper, we propose a new method, Dynamic Multi-network Mining\n(DMM), that converts a tensor time series into a set of segment groups of\nvarious lengths (i.e., clusters) characterized by a dependency network\nconstrained with l1-norm. Our method has the following properties. (a)\nInterpretable: it characterizes the cluster with multiple networks, each of\nwhich is a sparse dependency network of a corresponding non-temporal mode, and\nthus provides visible and interpretable insights into the key relationships.\n(b) Accurate: it discovers the clusters with distinct networks from tensor time\nseries according to the minimum description length (MDL). (c) Scalable: it\nscales linearly in terms of the input data size when solving a non-convex\nproblem to optimize the number of segments and clusters, and thus it is\napplicable to long-range and high-dimensional tensors. Extensive experiments\nwith synthetic datasets confirm that our method outperforms the\nstate-of-the-art methods in terms of clustering accuracy. We then use real\ndatasets to demonstrate that DMM is useful for providing interpretable insights\nfrom tensor time series.",
      "tldr_zh": "本文提出 Dynamic Multi-Network Mining (DMM) 方法，用于对张量时间序列(tensor time series)进行子序列聚类(subsequence clustering)，并提供可解释的见解。DMM 将时间序列转换为不同长度的段组，每个聚类由受 l1-norm 约束的稀疏依赖网络表征，具有可解释性（通过多个非时间模式的网络显示关键关系）、准确性（基于 minimum description length (MDL) 发现不同网络的聚类）和可伸缩性（在输入数据大小上线性扩展）。实验结果显示，DMM 在合成数据集上比最先进方法在聚类准确率上表现更优，并在真实数据集上成功提供可解释的洞察。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.IT",
        "math.IT"
      ],
      "primary_category": "cs.LG",
      "comment": "Accepted by WWW 2024",
      "pdf_url": "http://arxiv.org/pdf/2402.11773v2",
      "published_date": "2024-02-19 02:06:04 UTC",
      "updated_date": "2024-02-22 01:17:29 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T08:03:10.013721"
    },
    {
      "arxiv_id": "2402.11771v1",
      "title": "Evaluating the Effectiveness of Index-Based Treatment Allocation",
      "title_zh": "翻译失败",
      "authors": [
        "Niclas Boehmer",
        "Yash Nair",
        "Sanket Shah",
        "Lucas Janson",
        "Aparna Taneja",
        "Milind Tambe"
      ],
      "abstract": "When resources are scarce, an allocation policy is needed to decide who\nreceives a resource. This problem occurs, for instance, when allocating scarce\nmedical resources and is often solved using modern ML methods. This paper\nintroduces methods to evaluate index-based allocation policies -- that allocate\na fixed number of resources to those who need them the most -- by using data\nfrom a randomized control trial. Such policies create dependencies between\nagents, which render the assumptions behind standard statistical tests invalid\nand limit the effectiveness of estimators. Addressing these challenges, we\ntranslate and extend recent ideas from the statistics literature to present an\nefficient estimator and methods for computing asymptotically correct confidence\nintervals. This enables us to effectively draw valid statistical conclusions, a\ncritical gap in previous work. Our extensive experiments validate our\nmethodology in practical settings, while also showcasing its statistical power.\nWe conclude by proposing and empirically verifying extensions of our\nmethodology that enable us to reevaluate a past randomized control trial to\nevaluate different ML allocation policies in the context of a mHealth program,\ndrawing previously invisible conclusions.",
      "tldr_zh": "该论文评估了基于索引的治疗分配政策的有效性，这些政策在资源稀缺时（如医疗资源分配）将固定数量的资源分配给最需要的人群，使用随机对照试验（randomized control trial）数据进行分析。作者指出，这种政策会因代理间依赖性而使标准统计测试的假设失效，并提出一个高效估计器以及计算渐进正确置信区间（confidence intervals）的方法，以解决这些挑战。实验结果验证了方法的实用性和统计能力，并通过扩展应用重新评估了一个 mHealth 程序的过去试验，揭示了之前未发现的结论，从而为 ML 分配政策（ML allocation policies）的有效性评估提供了新工具。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "stat.ME",
        "stat.ML"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2402.11771v1",
      "published_date": "2024-02-19 01:55:55 UTC",
      "updated_date": "2024-02-19 01:55:55 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T08:03:22.124741"
    },
    {
      "arxiv_id": "2402.11764v2",
      "title": "ChatGPT Based Data Augmentation for Improved Parameter-Efficient Debiasing of LLMs",
      "title_zh": "翻译失败",
      "authors": [
        "Pengrui Han",
        "Rafal Kocielnik",
        "Adhithya Saravanan",
        "Roy Jiang",
        "Or Sharir",
        "Anima Anandkumar"
      ],
      "abstract": "Large Language models (LLMs), while powerful, exhibit harmful social biases.\nDebiasing is often challenging due to computational costs, data constraints,\nand potential degradation of multi-task language capabilities. This work\nintroduces a novel approach utilizing ChatGPT to generate synthetic training\ndata, aiming to enhance the debiasing of LLMs. We propose two strategies:\nTargeted Prompting, which provides effective debiasing for known biases but\nnecessitates prior specification of bias in question; and General Prompting,\nwhich, while slightly less effective, offers debiasing across various\ncategories. We leverage resource-efficient LLM debiasing using adapter tuning\nand compare the effectiveness of our synthetic data to existing debiasing\ndatasets. Our results reveal that: (1) ChatGPT can efficiently produce\nhigh-quality training data for debiasing other LLMs; (2) data produced via our\napproach surpasses existing datasets in debiasing performance while also\npreserving internal knowledge of a pre-trained LLM; and (3) synthetic data\nexhibits generalizability across categories, effectively mitigating various\nbiases, including intersectional ones. These findings underscore the potential\nof synthetic data in advancing the fairness of LLMs with minimal retraining\ncost.",
      "tldr_zh": "这篇论文提出了一种利用 ChatGPT 生成合成训练数据的方法，以提升大型语言模型 (LLMs) 的 debiasing 效果，解决计算成本高和数据限制等问题。研究设计了两种策略：Targeted Prompting（针对已知偏见提供高效 debiasing，但需预先指定偏见）和 General Prompting（适用于多种类别偏见，但效果稍逊）。通过 adapter tuning 进行资源高效的模型微调，结果显示该合成数据在 debiasing 性能上超越现有数据集，同时保留了预训练 LLM 的内部知识，并展现出对各种偏见（包括交叉偏见）的泛化能力。这些发现突显了合成数据在最小化重训练成本的同时提升 LLM 公平性的潜力。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.CY",
        "68T50",
        "I.2.7; K.4.1"
      ],
      "primary_category": "cs.CL",
      "comment": "To Appear in the Proceedings of the 1st Conference on Language\n  Modeling (COLM) 2024",
      "pdf_url": "http://arxiv.org/pdf/2402.11764v2",
      "published_date": "2024-02-19 01:28:48 UTC",
      "updated_date": "2024-09-16 05:28:43 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T08:03:35.946092"
    },
    {
      "arxiv_id": "2402.11753v4",
      "title": "ArtPrompt: ASCII Art-based Jailbreak Attacks against Aligned LLMs",
      "title_zh": "翻译失败",
      "authors": [
        "Fengqing Jiang",
        "Zhangchen Xu",
        "Luyao Niu",
        "Zhen Xiang",
        "Bhaskar Ramasubramanian",
        "Bo Li",
        "Radha Poovendran"
      ],
      "abstract": "Safety is critical to the usage of large language models (LLMs). Multiple\ntechniques such as data filtering and supervised fine-tuning have been\ndeveloped to strengthen LLM safety. However, currently known techniques presume\nthat corpora used for safety alignment of LLMs are solely interpreted by\nsemantics. This assumption, however, does not hold in real-world applications,\nwhich leads to severe vulnerabilities in LLMs. For example, users of forums\noften use ASCII art, a form of text-based art, to convey image information. In\nthis paper, we propose a novel ASCII art-based jailbreak attack and introduce a\ncomprehensive benchmark Vision-in-Text Challenge (ViTC) to evaluate the\ncapabilities of LLMs in recognizing prompts that cannot be solely interpreted\nby semantics. We show that five SOTA LLMs (GPT-3.5, GPT-4, Gemini, Claude, and\nLlama2) struggle to recognize prompts provided in the form of ASCII art. Based\non this observation, we develop the jailbreak attack ArtPrompt, which leverages\nthe poor performance of LLMs in recognizing ASCII art to bypass safety measures\nand elicit undesired behaviors from LLMs. ArtPrompt only requires black-box\naccess to the victim LLMs, making it a practical attack. We evaluate ArtPrompt\non five SOTA LLMs, and show that ArtPrompt can effectively and efficiently\ninduce undesired behaviors from all five LLMs. Our code is available at\nhttps://github.com/uw-nsl/ArtPrompt.",
      "tldr_zh": "本研究揭示了大型语言模型 (LLMs) 的安全漏洞，提出了一种基于 ASCII art 的越狱攻击 (jailbreak attack) 框架 ArtPrompt，以绕过现有安全对齐技术。论文引入了全面基准 Vision-in-Text Challenge (ViTC)，评估 LLMs 在识别非语义提示（如 ASCII art）时的能力，发现五种 SOTA LLMs（如 GPT-3.5 和 GPT-4）表现不佳。实验结果显示，ArtPrompt 通过黑盒访问成功诱导这些 LLMs 产生 undesired behaviors，提高了攻击的实用性和有效性。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "To appear in ACL 2024",
      "pdf_url": "http://arxiv.org/pdf/2402.11753v4",
      "published_date": "2024-02-19 00:43:31 UTC",
      "updated_date": "2024-06-07 17:35:17 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T08:03:46.898251"
    },
    {
      "arxiv_id": "2402.11752v2",
      "title": "Diagonalisation SGD: Fast & Convergent SGD for Non-Differentiable Models via Reparameterisation and Smoothing",
      "title_zh": "翻译失败",
      "authors": [
        "Dominik Wagner",
        "Basim Khajwal",
        "C. -H. Luke Ong"
      ],
      "abstract": "It is well-known that the reparameterisation gradient estimator, which\nexhibits low variance in practice, is biased for non-differentiable models.\nThis may compromise correctness of gradient-based optimisation methods such as\nstochastic gradient descent (SGD). We introduce a simple syntactic framework to\ndefine non-differentiable functions piecewisely and present a systematic\napproach to obtain smoothings for which the reparameterisation gradient\nestimator is unbiased. Our main contribution is a novel variant of SGD,\nDiagonalisation Stochastic Gradient Descent, which progressively enhances the\naccuracy of the smoothed approximation during optimisation, and we prove\nconvergence to stationary points of the unsmoothed (original) objective. Our\nempirical evaluation reveals benefits over the state of the art: our approach\nis simple, fast, stable and attains orders of magnitude reduction in\nwork-normalised variance.",
      "tldr_zh": "本论文解决了重新参数化梯度估计器在非微分模型中的偏置问题，通过引入一个简单的语法框架来定义这些函数并系统获取平滑版本，使估计器变得无偏。主要贡献是提出 Diagonalisation SGD 算法，该算法在优化过程中逐步提高平滑近似的准确性，并证明其收敛到原始目标的静止点。实验评估显示，该方法比现有技术更简单、更快、更稳定，并实现了数量级的工作归一化方差减少。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "math.OC"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2402.11752v2",
      "published_date": "2024-02-19 00:43:22 UTC",
      "updated_date": "2024-02-20 02:58:38 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T08:03:58.561643"
    },
    {
      "arxiv_id": "2402.14838v1",
      "title": "RFBES at SemEval-2024 Task 8: Investigating Syntactic and Semantic Features for Distinguishing AI-Generated and Human-Written Texts",
      "title_zh": "翻译失败",
      "authors": [
        "Mohammad Heydari Rad",
        "Farhan Farsi",
        "Shayan Bali",
        "Romina Etezadi",
        "Mehrnoush Shamsfard"
      ],
      "abstract": "Nowadays, the usage of Large Language Models (LLMs) has increased, and LLMs\nhave been used to generate texts in different languages and for different\ntasks. Additionally, due to the participation of remarkable companies such as\nGoogle and OpenAI, LLMs are now more accessible, and people can easily use\nthem. However, an important issue is how we can detect AI-generated texts from\nhuman-written ones. In this article, we have investigated the problem of\nAI-generated text detection from two different aspects: semantics and syntax.\nFinally, we presented an AI model that can distinguish AI-generated texts from\nhuman-written ones with high accuracy on both multilingual and monolingual\ntasks using the M4 dataset. According to our results, using a semantic approach\nwould be more helpful for detection. However, there is a lot of room for\nimprovement in the syntactic approach, and it would be a good approach for\nfuture work.",
      "tldr_zh": "本研究调查了使用语义（semantic）和语法（syntactic）特征来区分 AI 生成文本和人类撰写文本的问题，针对 Large Language Models (LLMs) 的广泛应用。研究者提出一个 AI 模型，利用 M4 数据集在多语种和单语种任务中实现了高准确率检测。结果显示，语义方法更有效，而语法方法虽有改进空间，但适合作为未来工作重点，为 SemEval-2024 Task 8 提供了重要见解。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "Mohammad Heydari Rad, Farhan Farsi, and Shayan Bali have made equal\n  contributions to this work",
      "pdf_url": "http://arxiv.org/pdf/2402.14838v1",
      "published_date": "2024-02-19 00:40:17 UTC",
      "updated_date": "2024-02-19 00:40:17 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T08:04:10.579872"
    },
    {
      "arxiv_id": "2402.11746v1",
      "title": "Language Models are Homer Simpson! Safety Re-Alignment of Fine-tuned Language Models through Task Arithmetic",
      "title_zh": "翻译失败",
      "authors": [
        "Rishabh Bhardwaj",
        "Do Duc Anh",
        "Soujanya Poria"
      ],
      "abstract": "Aligned language models face a significant limitation as their fine-tuning\noften results in compromised safety. To tackle this, we propose a simple method\nRESTA that performs LLM safety realignment. RESTA stands for REstoring Safety\nthrough Task Arithmetic. At its core, it involves a simple arithmetic addition\nof a safety vector to the weights of the compromised model. We demonstrate the\neffectiveness of RESTA in both parameter-efficient and full fine-tuning,\ncovering a wide range of downstream tasks, including instruction following in\nChinese, English, and Hindi, as well as problem-solving capabilities in Code\nand Math. We also showcase the generalizability of RESTA on three existing\nsafety evaluation benchmarks and a multilingual benchmark dataset proposed as a\npart of this work, consisting of 550 harmful questions covering 11 categories,\neach with 5 sub-categories of harm. Overall, RESTA decreases the harmfulness of\nthe compromised model from 18.6% to 5.1% and from 9.2% to 1.5% in\nparameter-efficient and full fine-tuning, respectively, while maintaining most\nof the model's performance on the task. We release the source codes at:\nhttps://github.com/declare-lab/resta.",
      "tldr_zh": "该研究指出，微调后的语言模型（Language Models）往往会降低安全性，并提出RESTA（REstoring Safety through Task Arithmetic）方法，通过简单地将安全向量添加到模型权重来重新对齐安全性。该方法适用于参数高效微调（parameter-efficient fine-tuning）和全微调，涵盖指令跟随（如中文、英语和Hindi）、代码和数学问题等下游任务。实验在现有安全评估基准和新提出的多语言数据集（包括550个有害问题，覆盖11个类别）上验证了RESTA的有效性，将危害性从18.6%降至5.1%（参数高效微调）和从9.2%降至1.5%（全微调），同时基本保持模型在任务上的性能。开源代码可从https://github.com/declare-lab/resta获取。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2402.11746v1",
      "published_date": "2024-02-19 00:18:09 UTC",
      "updated_date": "2024-02-19 00:18:09 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T08:04:22.932938"
    }
  ],
  "raw_papers_fetched": true,
  "papers_count": 136,
  "processed_papers_count": 136,
  "failed_papers_count": 0,
  "summary_generated": true,
  "daily_data_saved": true,
  "last_update": "2025-05-17T08:04:48.116673"
}
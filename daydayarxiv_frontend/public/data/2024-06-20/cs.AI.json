{
  "date": "2024-06-20",
  "category": "cs.AI",
  "summary": "欢迎来到 UTC 时间 2024-06-20 的 arXiv 中文 TLDR 快报！今天 arXiv 的论文主要聚焦于 AI 模型的安全性、LLM（大型语言模型）的优化与应用（如医疗、机器人和文本生成），以及新兴领域的基准测试和算法创新，亮点包括 ChatGPT 在科研中的表现评估，以及 LLM 安全对齐框架的提出。\n\n### 重点论文讨论\n我挑选了今天论文中的高影响力主题，包括 AI 安全、LLM 增强和医疗应用，这些领域涉及热门话题和潜在实际影响。以下按主题分组，先聊重要或话题度高的论文，再快速掠过其他。\n\n#### AI 安全与 LLM 偏见（高话题度，涉及 LLM 风险评估）\n- **ChatGPT as Research Scientist: Probing GPT's Capabilities**  \n  这篇论文由 Mahzarin R. Banaji 等知名学者发布，评估了 GPT-3.5 和 GPT-4 在科研任务中的能力，包括信息检索、伦理检测、数据生成和预测。贡献：发现 GPT-4 在伦理检测上表现良好，但预测新数据时失败，主要发现是 GPT 模型虽强大，但易产生幻觉（hallucination），强调了 LLM 在科研中的局限性。\n  \n- **Does GPT Really Get It? A Framework for Evaluating the Quality of Thinking**  \n  作者 Luke Zaphir 等提出 MAGE 框架，用于评估 LLM（如 ChatGPT-4）的批判性思考能力。贡献：框架通过问题映射和 AI 漏洞测试评估 LLM 在模拟认知技能时的表现，主要发现是 LLM 在数字评估中易受限，但可用于改进教育评估设计。\n\n- **SORRY-Bench: Systematically Evaluating Large Language Model Safety Refusal**  \n  这篇论文构建了 SORRY-Bench 基准，评估 LLM 对不安全请求的拒绝能力。贡献：数据集覆盖多种偏见类型，并使用高效评估器（7B LLM），主要发现是现有 LLM 在拒绝不安全内容时效果不佳，但可通过基准优化。\n\n这些论文突出了 LLM 的安全风险和偏见问题，相关工作（如 SORRY-Bench）提供了实用工具，可能推动 AI 伦理研究。\n\n#### LLM 优化与应用（令人印象深刻，提升模型性能）\n- **Learning to Select Goals in Automated Planning with Deep-Q Learning**  \n  作者提出了一种结合 Deep-Q Learning 的规划架构，用于实时场景决策。贡献：模型在视频游戏环境中训练，能泛化到新级别，主要发现是它比传统规划器更高效，仅增加 9% 操作却显著减少问题解决时间。\n\n- **Q*: Improving Multi-step Reasoning for LLMs with Deliberative Planning**  \n  这篇论文引入 Q* 框架，使用强化学习优化 LLM 的多步推理。贡献：无需微调 LLM，即可提升推理性能，主要发现是框架在 GSM8K 和 MATH 数据集上显著提高准确率。\n\n- **Step-Back Profiling: Distilling User History for Personalized Scientific Writing**  \n  作者开发了 Emulation 框架，用于个性化科学写作。贡献：通过建模用户历史和推理过程，提升 LLM 在写作任务的性能，主要发现是框架在多数据集上改善了生成质量。\n\n这些工作展示了 LLM 通过规划和个性化增强的潜力，特别在推理任务中。\n\n#### 医疗与科学应用（实际影响大，结合 AI 基准）\n- **ACR: A Benchmark for Automatic Cohort Retrieval**  \n  论文引入 ACR 基准，用于医疗数据检索。贡献：评估 LLM 在处理电子病历的纵向推理能力，主要发现是 LLM 需要更高效的系统来管理大规模患者数据库。\n\n- **An LLM Feature-based Framework for Dialogue Constructiveness Assessment**  \n  作者提出 LLM 特征框架，用于评估对话的建设性。贡献：结合 LLM 和启发式方法训练模型，主要发现是该框架在多个数据集上超越神经模型，提供更鲁棒的预测。\n\n快速掠过其他论文：今天还有许多论文，如机器人路径规划（Diffusion-Based Failure Sampling）、文本生成优化（RE-AdaptIR）和图像处理（Whiteboard-of-Thought），但它们相对专业或初步，未见重大突破，故简要提及。它们主要贡献于特定领域优化，但影响力不如上述主题。\n\n总之，今天的 arXiv 论文强调了 AI 的安全性和实用性提升，LLM 在医疗和推理中的应用尤为值得关注。感兴趣的读者可优先查看 AI 安全相关工作，以了解模型风险。更多细节请查阅 arXiv！",
  "papers": [
    {
      "arxiv_id": "2406.14780v2",
      "title": "ACR: A Benchmark for Automatic Cohort Retrieval",
      "title_zh": "ACR：自动队列检索基准",
      "authors": [
        "Dung Ngoc Thai",
        "Victor Ardulov",
        "Jose Ulises Mena",
        "Simran Tiwari",
        "Gleb Erofeev",
        "Ramy Eskander",
        "Karim Tarabishy",
        "Ravi B Parikh",
        "Wael Salloum"
      ],
      "abstract": "Identifying patient cohorts is fundamental to numerous healthcare tasks,\nincluding clinical trial recruitment and retrospective studies. Current cohort\nretrieval methods in healthcare organizations rely on automated queries of\nstructured data combined with manual curation, which are time-consuming,\nlabor-intensive, and often yield low-quality results. Recent advancements in\nlarge language models (LLMs) and information retrieval (IR) offer promising\navenues to revolutionize these systems. Major challenges include managing\nextensive eligibility criteria and handling the longitudinal nature of\nunstructured Electronic Medical Records (EMRs) while ensuring that the solution\nremains cost-effective for real-world application. This paper introduces a new\ntask, Automatic Cohort Retrieval (ACR), and evaluates the performance of LLMs\nand commercial, domain-specific neuro-symbolic approaches. We provide a\nbenchmark task, a query dataset, an EMR dataset, and an evaluation framework.\nOur findings underscore the necessity for efficient, high-quality ACR systems\ncapable of longitudinal reasoning across extensive patient databases.",
      "tldr_zh": "这篇论文引入了Automatic Cohort Retrieval (ACR) 基准，用于解决患者队列识别在医疗任务（如临床试验招募和回顾性研究）中的挑战。当前方法依赖结构化数据查询和手动整理，效率低下且结果质量不高，而LLMs和信息检索(IR)技术的进步提供了潜在解决方案。论文评估了LLMs与领域特定神经符号方法的性能，并提供了基准任务、查询数据集、EMR数据集和评估框架。研究发现，高效的ACR系统需具备处理非结构化Electronic Medical Records (EMRs)的纵向推理能力，以支持大规模患者数据库的应用。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2406.14780v2",
      "published_date": "2024-06-20 23:04:06 UTC",
      "updated_date": "2024-07-01 19:05:00 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T22:34:59.531229"
    },
    {
      "arxiv_id": "2406.14779v1",
      "title": "Learning to Select Goals in Automated Planning with Deep-Q Learning",
      "title_zh": "利用 Deep-Q 学习在自动规划中学习选择目标",
      "authors": [
        "Carlos Núñez-Molina",
        "Juan Fernández-Olivares",
        "Raúl Pérez"
      ],
      "abstract": "In this work we propose a planning and acting architecture endowed with a\nmodule which learns to select subgoals with Deep Q-Learning. This allows us to\ndecrease the load of a planner when faced with scenarios with real-time\nrestrictions. We have trained this architecture on a video game environment\nused as a standard test-bed for intelligent systems applications, testing it on\ndifferent levels of the same game to evaluate its generalization abilities. We\nhave measured the performance of our approach as more training data is made\navailable, as well as compared it with both a state-of-the-art, classical\nplanner and the standard Deep Q-Learning algorithm. The results obtained show\nour model performs better than the alternative methods considered, when both\nplan quality (plan length) and time requirements are taken into account. On the\none hand, it is more sample-efficient than standard Deep Q-Learning, and it is\nable to generalize better across levels. On the other hand, it reduces\nproblem-solving time when compared with a state-of-the-art automated planner,\nat the expense of obtaining plans with only 9% more actions.",
      "tldr_zh": "这篇论文提出了一种结合 Deep Q-Learning 的规划架构，用于在自动规划（Automated Planning）中学习选择子目标，从而减轻规划器在实时限制场景下的负载。研究在视频游戏环境中训练并测试了该架构，评估其泛化能力，并与最先进的经典规划器以及标准 Deep Q-Learning 算法进行了比较。结果显示，该方法比基准模型更高效，样本利用率更高，且能更好地跨级别泛化，同时将问题解决时间减少了，但计划长度仅增加了9%。",
      "categories": [
        "cs.AI",
        "I.2.8; I.2.6"
      ],
      "primary_category": "cs.AI",
      "comment": "25 pages, 4 figures",
      "pdf_url": "http://arxiv.org/pdf/2406.14779v1",
      "published_date": "2024-06-20 23:02:44 UTC",
      "updated_date": "2024-06-20 23:02:44 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T22:35:11.096836"
    },
    {
      "arxiv_id": "2406.14769v1",
      "title": "How critically can an AI think? A framework for evaluating the quality of thinking of generative artificial intelligence",
      "title_zh": "翻译失败",
      "authors": [
        "Luke Zaphir",
        "Jason M. Lodge",
        "Jacinta Lisec",
        "Dom McGrath",
        "Hassan Khosravi"
      ],
      "abstract": "Generative AI such as those with large language models have created\nopportunities for innovative assessment design practices. Due to recent\ntechnological developments, there is a need to know the limits and capabilities\nof generative AI in terms of simulating cognitive skills. Assessing student\ncritical thinking skills has been a feature of assessment for time immemorial,\nbut the demands of digital assessment create unique challenges for equity,\nacademic integrity and assessment authorship. Educators need a framework for\ndetermining their assessments vulnerability to generative AI to inform\nassessment design practices. This paper presents a framework that explores the\ncapabilities of the LLM ChatGPT4 application, which is the current industry\nbenchmark. This paper presents the Mapping of questions, AI vulnerability\ntesting, Grading, Evaluation (MAGE) framework to methodically critique their\nassessments within their own disciplinary contexts. This critique will provide\nspecific and targeted indications of their questions vulnerabilities in terms\nof the critical thinking skills. This can go on to form the basis of assessment\ndesign for their tasks.",
      "tldr_zh": "这篇论文探讨了生成式 AI（如大语言模型）在模拟认知技能方面的能力和限制，特别针对评估学生批判性思维技能的挑战，包括数字评估中的公平性、学术诚信和评估 authorship 问题。论文提出 MAGE 框架（Mapping of questions, AI vulnerability testing, Grading, Evaluation），用于系统地评估 ChatGPT-4 等 AI 在学科上下文中对批判性思维任务的易受影响性。该框架通过映射问题、测试 AI 漏洞、评分和评估，帮助教育者识别评估漏洞，并指导改进评估设计以提升整体质量和可靠性。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2406.14769v1",
      "published_date": "2024-06-20 22:46:56 UTC",
      "updated_date": "2024-06-20 22:46:56 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T22:35:22.912787"
    },
    {
      "arxiv_id": "2406.14765v1",
      "title": "ChatGPT as Research Scientist: Probing GPT's Capabilities as a Research Librarian, Research Ethicist, Data Generator and Data Predictor",
      "title_zh": "翻译失败",
      "authors": [
        "Steven A. Lehr",
        "Aylin Caliskan",
        "Suneragiri Liyanage",
        "Mahzarin R. Banaji"
      ],
      "abstract": "How good a research scientist is ChatGPT? We systematically probed the\ncapabilities of GPT-3.5 and GPT-4 across four central components of the\nscientific process: as a Research Librarian, Research Ethicist, Data Generator,\nand Novel Data Predictor, using psychological science as a testing field. In\nStudy 1 (Research Librarian), unlike human researchers, GPT-3.5 and GPT-4\nhallucinated, authoritatively generating fictional references 36.0% and 5.4% of\nthe time, respectively, although GPT-4 exhibited an evolving capacity to\nacknowledge its fictions. In Study 2 (Research Ethicist), GPT-4 (though not\nGPT-3.5) proved capable of detecting violations like p-hacking in fictional\nresearch protocols, correcting 88.6% of blatantly presented issues, and 72.6%\nof subtly presented issues. In Study 3 (Data Generator), both models\nconsistently replicated patterns of cultural bias previously discovered in\nlarge language corpora, indicating that ChatGPT can simulate known results, an\nantecedent to usefulness for both data generation and skills like hypothesis\ngeneration. Contrastingly, in Study 4 (Novel Data Predictor), neither model was\nsuccessful at predicting new results absent in their training data, and neither\nappeared to leverage substantially new information when predicting more versus\nless novel outcomes. Together, these results suggest that GPT is a flawed but\nrapidly improving librarian, a decent research ethicist already, capable of\ndata generation in simple domains with known characteristics but poor at\npredicting novel patterns of empirical data to aid future experimentation.",
      "tldr_zh": "这篇论文评估了 ChatGPT（即 GPT-3.5 和 GPT-4）在科学研究中的能力，聚焦于四个方面：作为 Research Librarian、Research Ethicist、Data Generator 和 Novel Data Predictor，并以心理科学为测试领域。研究发现，在 Study 1 中，GPT-3.5 和 GPT-4 会产生幻觉（hallucinated），虚构引用率分别为 36.0% 和 5.4%，尽管 GPT-4 能部分承认其错误；在 Study 2 中，GPT-4 表现出色，能检测并纠正 88.6% 的明显 Research Ethicist 问题和 72.6% 的微妙问题，而 GPT-3.5 表现较差。Study 3 显示，两者都能模拟已知的文化偏差模式，作为 Data Generator 有效；然而，在 Study 4 中，两模型均无法预测训练数据中不存在的 Novel Data Predictor 结果。总体而言，ChatGPT 是一个有缺陷但快速改进的 Research Librarian，已是可靠的 Research Ethicist，能生成简单领域的已知数据，但不擅长预测新颖的经验模式。",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.CY",
        "cs.IR",
        "cs.LG",
        "I.2.7; K.4.0; K.4.1; K.4.2"
      ],
      "primary_category": "cs.AI",
      "comment": "Main article is 14 pages, 1 table. Includes SI Appendix: 26 pages, 12\n  tables, 2 figures. Total: 40 pages, 13 tables, 2 figures. Under revised\n  review at PNAS",
      "pdf_url": "http://arxiv.org/pdf/2406.14765v1",
      "published_date": "2024-06-20 22:30:06 UTC",
      "updated_date": "2024-06-20 22:30:06 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T22:35:36.753030"
    },
    {
      "arxiv_id": "2406.14764v1",
      "title": "RE-AdaptIR: Improving Information Retrieval through Reverse Engineered Adaptation",
      "title_zh": "翻译失败",
      "authors": [
        "William Fleshman",
        "Benjamin Van Durme"
      ],
      "abstract": "Large language models (LLMs) fine-tuned for text-retrieval have demonstrated\nstate-of-the-art results across several information retrieval (IR) benchmarks.\nHowever, supervised training for improving these models requires numerous\nlabeled examples, which are generally unavailable or expensive to acquire. In\nthis work, we explore the effectiveness of extending reverse engineered\nadaptation to the context of information retrieval (RE-AdaptIR). We use\nRE-AdaptIR to improve LLM-based IR models using only unlabeled data. We\ndemonstrate improved performance both in training domains as well as zero-shot\nin domains where the models have seen no queries. We analyze performance\nchanges in various fine-tuning scenarios and offer findings of immediate use to\npractitioners.",
      "tldr_zh": "该研究提出RE-AdaptIR方法，通过逆向工程适应(reverse engineered adaptation)来改进大型语言模型(LLMs)基于的信息检索(IR)模型，而无需依赖大量标记数据。该方法利用无标记数据对模型进行微调，在训练域和零样本(zero-shot)域上均实现了性能提升。实验分析了不同微调场景下的表现变化，并为IR从业者提供了直接可用的实用见解。",
      "categories": [
        "cs.IR",
        "cs.AI",
        "cs.CL",
        "cs.LG"
      ],
      "primary_category": "cs.IR",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2406.14764v1",
      "published_date": "2024-06-20 22:28:11 UTC",
      "updated_date": "2024-06-20 22:28:11 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T22:35:46.802693"
    },
    {
      "arxiv_id": "2406.14763v1",
      "title": "A Learn-Then-Reason Model Towards Generalization in Knowledge Base Question Answering",
      "title_zh": "翻译失败",
      "authors": [
        "Lingxi Zhang",
        "Jing Zhang",
        "Yanling Wang",
        "Cuiping Li",
        "Hong Chen"
      ],
      "abstract": "Large-scale knowledge bases (KBs) like Freebase and Wikidata house millions\nof structured knowledge. Knowledge Base Question Answering (KBQA) provides a\nuser-friendly way to access these valuable KBs via asking natural language\nquestions. In order to improve the generalization capabilities of KBQA models,\nextensive research has embraced a retrieve-then-reason framework to retrieve\nrelevant evidence for logical expression generation. These multi-stage efforts\nprioritize acquiring external sources but overlook the incorporation of new\nknowledge into their model parameters. In effect, even advanced language models\nand retrievers have knowledge boundaries, thereby limiting the generalization\ncapabilities of previous KBQA models. Therefore, this paper develops KBLLaMA,\nwhich follows a learn-then-reason framework to inject new KB knowledge into a\nlarge language model for flexible end-to-end KBQA. At the core of KBLLaMA, we\nstudy (1) how to organize new knowledge about KBQA and (2) how to facilitate\nthe learning of the organized knowledge. Extensive experiments on various KBQA\ngeneralization tasks showcase the state-of-the-art performance of KBLLaMA.\nEspecially on the general benchmark GrailQA and domain-specific benchmark\nBio-chemical, KBLLaMA respectively derives a performance gain of up to 3.8% and\n9.8% compared to the baselines.",
      "tldr_zh": "该论文针对知识库问答(KBQA)的泛化问题，提出KBLLaMA模型，该模型采用learn-then-reason框架，将新知识注入大型语言模型(Large Language Model)中，以克服传统retrieve-then-reason方法的局限性。KBLLaMA的核心研究包括如何组织KBQA相关的新知识以及促进其学习，实现灵活的端到端问答。实验结果显示，在GrailQA和Bio-chemical基准上，KBLLaMA分别比基线模型提升3.8%和9.8%，展现出最先进的性能。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2406.14763v1",
      "published_date": "2024-06-20 22:22:41 UTC",
      "updated_date": "2024-06-20 22:22:41 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T22:36:00.217018"
    },
    {
      "arxiv_id": "2406.14761v1",
      "title": "Diffusion-Based Failure Sampling for Cyber-Physical Systems",
      "title_zh": "翻译失败",
      "authors": [
        "Harrison Delecki",
        "Marc R. Schlichting",
        "Mansur Arief",
        "Anthony Corso",
        "Marcell Vazquez-Chanlatte",
        "Mykel J. Kochenderfer"
      ],
      "abstract": "Validating safety-critical autonomous systems in high-dimensional domains\nsuch as robotics presents a significant challenge. Existing black-box\napproaches based on Markov chain Monte Carlo may require an enormous number of\nsamples, while methods based on importance sampling often rely on simple\nparametric families that may struggle to represent the distribution over\nfailures. We propose to sample the distribution over failures using a\nconditional denoising diffusion model, which has shown success in complex\nhigh-dimensional problems such as robotic task planning. We iteratively train a\ndiffusion model to produce state trajectories closer to failure. We demonstrate\nthe effectiveness of our approach on high-dimensional robotic validation tasks,\nimproving sample efficiency and mode coverage compared to existing black-box\ntechniques.",
      "tldr_zh": "该研究针对高维领域（如机器人）中验证安全关键自主系统的挑战，提出了一种基于条件去噪扩散模型（conditional denoising diffusion model）的失败采样方法，以解决现有黑盒技术如Markov chain Monte Carlo需要大量样本，以及importance sampling依赖简单参数族的局限。通过迭代训练扩散模型生成更接近失败的状态轨迹，该方法在高维机器人验证任务中显著提高了样本效率和模式覆盖。总的来说，此创新为cyber-physical systems的安全验证提供了更高效的工具。",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.SY",
        "eess.SY"
      ],
      "primary_category": "cs.RO",
      "comment": "Under review at RA-L",
      "pdf_url": "http://arxiv.org/pdf/2406.14761v1",
      "published_date": "2024-06-20 22:22:28 UTC",
      "updated_date": "2024-06-20 22:22:28 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T22:36:09.924960"
    },
    {
      "arxiv_id": "2406.14760v2",
      "title": "An LLM Feature-based Framework for Dialogue Constructiveness Assessment",
      "title_zh": "基于LLM特征的对话建设性评估框架",
      "authors": [
        "Lexin Zhou",
        "Youmna Farag",
        "Andreas Vlachos"
      ],
      "abstract": "Research on dialogue constructiveness assessment focuses on (i) analysing\nconversational factors that influence individuals to take specific actions, win\ndebates, change their perspectives or broaden their open-mindedness and (ii)\npredicting constructiveness outcomes following dialogues for such use cases.\nThese objectives can be achieved by training either interpretable feature-based\nmodels (which often involve costly human annotations) or neural models such as\npre-trained language models (which have empirically shown higher task accuracy\nbut lack interpretability). In this paper we propose an LLM feature-based\nframework for dialogue constructiveness assessment that combines the strengths\nof feature-based and neural approaches, while mitigating their downsides. The\nframework first defines a set of dataset-independent and interpretable\nlinguistic features, which can be extracted by both prompting an LLM and simple\nheuristics. Such features are then used to train LLM feature-based models. We\napply this framework to three datasets of dialogue constructiveness and find\nthat our LLM feature-based models outperform or performs at least as well as\nstandard feature-based models and neural models. We also find that the LLM\nfeature-based model learns more robust prediction rules instead of relying on\nsuperficial shortcuts, which often trouble neural models.",
      "tldr_zh": "该论文提出了一种基于LLM（Large Language Models）的特征框架，用于评估对话的建设性，旨在结合特征-based模型的可解释性和神经模型的准确性，同时避免其缺点。框架首先定义一组独立于数据集的可解释语言特征，这些特征可以通过LLM提示或简单启发式方法提取，然后用于训练LLM feature-based模型。在三个对话建设性数据集上的实验显示，该模型的表现优于或相当于是标准特征-based模型和神经模型，且它学习了更稳健的预测规则，而非依赖于表面的捷径。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "Paper accepted by EMNLP 2024",
      "pdf_url": "http://arxiv.org/pdf/2406.14760v2",
      "published_date": "2024-06-20 22:10:52 UTC",
      "updated_date": "2024-10-02 11:03:16 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T22:36:25.841940"
    },
    {
      "arxiv_id": "2406.14758v2",
      "title": "Compliance Cards: Automated EU AI Act Compliance Analyses amidst a Complex AI Supply Chain",
      "title_zh": "Compliance Cards: 在复杂 AI 供应链中自动进行欧盟 AI 法案合规分析",
      "authors": [
        "Bill Marino",
        "Yaqub Chaudhary",
        "Yulu Pi",
        "Rui-Jie Yew",
        "Preslav Aleksandrov",
        "Carwyn Rahman",
        "William F. Shen",
        "Isaac Robinson",
        "Nicholas D. Lane"
      ],
      "abstract": "As the AI supply chain grows more complex, AI systems and models are\nincreasingly likely to incorporate multiple internally- or externally-sourced\ncomponents such as datasets and (pre-trained) models. In such cases,\ndetermining whether or not the aggregate AI system or model complies with the\nEU AI Act (AIA) requires a multi-step process in which compliance-related\ninformation about both the AI system or model and all its component parts is:\n(1) gathered, potentially from multiple arms-length sources; (2) harmonized, if\nnecessary; (3) inputted into an analysis that looks across all of it to render\na compliance prediction. Because this process is so complex and time-consuming,\nit threatens to overburden the limited compliance resources of the AI providers\n(i.e., developers) who bear much of the responsibility for complying with the\nAIA. It also renders rapid or real-time compliance analyses infeasible in many\nAI development scenarios where they would be beneficial to providers. To\naddress these shortcomings, we introduce a complete system for automating\nprovider-side AIA compliance analyses amidst a complex AI supply chain. This\nsystem has two key elements. First is an interlocking set of computational,\nmulti-stakeholder transparency artifacts that capture AIA-specific metadata\nabout both: (1) the provider's overall AI system or model; and (2) the datasets\nand pre-trained models it incorporates as components. Second is an algorithm\nthat operates across all those artifacts to render a real-time prediction about\nwhether or not the aggregate AI system or model complies with the AIA. All\ntold, this system promises to dramatically facilitate and democratize\nprovider-side AIA compliance analyses (and, perhaps by extension, provider-side\nAIA compliance).",
      "tldr_zh": "该研究针对复杂的AI供应链中EU AI Act (AIA)合规分析的挑战，提出了一种自动化系统，以解决多组件AI系统（如数据集和预训练模型）的合规信息收集、统一和分析难题。该系统包括一套计算的多利益相关者透明性工件(transparency artifacts)，用于捕捉AI系统及其组件的AIA特定元数据，以及一个算法来跨这些工件进行实时合规性预测。通过这一方法，该系统显著降低了提供者（开发者）的合规资源负担，并实现了快速分析，推动了AIA合规的民主化和可行性。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2406.14758v2",
      "published_date": "2024-06-20 22:07:15 UTC",
      "updated_date": "2024-09-12 20:19:38 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T22:36:34.046817"
    },
    {
      "arxiv_id": "2406.14757v1",
      "title": "A Large Language Model Outperforms Other Computational Approaches to the High-Throughput Phenotyping of Physician Notes",
      "title_zh": "翻译失败",
      "authors": [
        "Syed I. Munzir",
        "Daniel B. Hier",
        "Chelsea Oommen",
        "Michael D. Carrithers"
      ],
      "abstract": "High-throughput phenotyping, the automated mapping of patient signs and\nsymptoms to standardized ontology concepts, is essential to gaining value from\nelectronic health records (EHR) in the support of precision medicine. Despite\ntechnological advances, high-throughput phenotyping remains a challenge. This\nstudy compares three computational approaches to high-throughput phenotyping: a\nLarge Language Model (LLM) incorporating generative AI, a Natural Language\nProcessing (NLP) approach utilizing deep learning for span categorization, and\na hybrid approach combining word vectors with machine learning. The approach\nthat implemented GPT-4 (a Large Language Model) demonstrated superior\nperformance, suggesting that Large Language Models are poised to be the\npreferred method for high-throughput phenotyping of physician notes.",
      "tldr_zh": "这项研究比较了三种计算方法在高通量表型学（high-throughput phenotyping）中的性能，该技术用于将患者症状映射到标准化本体概念，从而支持电子健康记录（EHR）和精准医学（precision medicine）。方法包括Large Language Model (LLM) 基于生成式 AI、Natural Language Processing (NLP) 利用深度学习进行跨度分类，以及混合方法结合词向量和机器学习。结果显示，使用 GPT-4 的 LLM 方法表现出色，优于其他方法，表明 LLM 将成为高通量表型学首选工具。",
      "categories": [
        "cs.AI",
        "92-05",
        "I.2"
      ],
      "primary_category": "cs.AI",
      "comment": "Submitted to AMIA Annual Symposium 2024, San Francisco CA",
      "pdf_url": "http://arxiv.org/pdf/2406.14757v1",
      "published_date": "2024-06-20 22:05:34 UTC",
      "updated_date": "2024-06-20 22:05:34 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T22:36:46.671629"
    },
    {
      "arxiv_id": "2406.14756v1",
      "title": "SciDMT: A Large-Scale Corpus for Detecting Scientific Mentions",
      "title_zh": "SciDMT：用于检测科学提及的大型语料库",
      "authors": [
        "Huitong Pan",
        "Qi Zhang",
        "Cornelia Caragea",
        "Eduard Dragut",
        "Longin Jan Latecki"
      ],
      "abstract": "We present SciDMT, an enhanced and expanded corpus for scientific mention\ndetection, offering a significant advancement over existing related resources.\nSciDMT contains annotated scientific documents for datasets (D), methods (M),\nand tasks (T). The corpus consists of two components: 1) the SciDMT main\ncorpus, which includes 48 thousand scientific articles with over 1.8 million\nweakly annotated mention annotations in the format of in-text span, and 2) an\nevaluation set, which comprises 100 scientific articles manually annotated for\nevaluation purposes. To the best of our knowledge, SciDMT is the largest corpus\nfor scientific entity mention detection. The corpus's scale and diversity are\ninstrumental in developing and refining models for tasks such as indexing\nscientific papers, enhancing information retrieval, and improving the\naccessibility of scientific knowledge. We demonstrate the corpus's utility\nthrough experiments with advanced deep learning architectures like SciBERT and\nGPT-3.5. Our findings establish performance baselines and highlight unresolved\nchallenges in scientific mention detection. SciDMT serves as a robust benchmark\nfor the research community, encouraging the development of innovative models to\nfurther the field of scientific information extraction.",
      "tldr_zh": "本文介绍了 SciDMT，这是一个大规模语料库，用于检测科学提及，包括 datasets (D)、methods (M) 和 tasks (T)，旨在提升科学实体提及检测的性能。SciDMT 包含 48,000 篇科学文章的超过 180 万弱注释提及，以及 100 篇手动注释的评估集，是目前最大的此类语料库。研究者通过 SciBERT 和 GPT-3.5 等深度学习模型进行了实验，建立了性能基准并突出了领域的未解决挑战。该语料库可用于改进科学论文索引、信息检索和知识可访问性，推动科学信息提取领域的创新。",
      "categories": [
        "cs.AI",
        "I.2.7"
      ],
      "primary_category": "cs.AI",
      "comment": "LREC/COLING 2024",
      "pdf_url": "http://arxiv.org/pdf/2406.14756v1",
      "published_date": "2024-06-20 22:03:21 UTC",
      "updated_date": "2024-06-20 22:03:21 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T22:37:10.654991"
    },
    {
      "arxiv_id": "2406.14747v1",
      "title": "An Adapter-Based Unified Model for Multiple Spoken Language Processing Tasks",
      "title_zh": "翻译失败",
      "authors": [
        "Varsha Suresh",
        "Salah Aït-Mokhtar",
        "Caroline Brun",
        "Ioan Calapodescu"
      ],
      "abstract": "Self-supervised learning models have revolutionized the field of speech\nprocessing. However, the process of fine-tuning these models on downstream\ntasks requires substantial computational resources, particularly when dealing\nwith multiple speech-processing tasks. In this paper, we explore the potential\nof adapter-based fine-tuning in developing a unified model capable of\neffectively handling multiple spoken language processing tasks. The tasks we\ninvestigate are Automatic Speech Recognition, Phoneme Recognition, Intent\nClassification, Slot Filling, and Spoken Emotion Recognition. We validate our\napproach through a series of experiments on the SUPERB benchmark, and our\nresults indicate that adapter-based fine-tuning enables a single\nencoder-decoder model to perform multiple speech processing tasks with an\naverage improvement of 18.4% across the five target tasks while staying\nefficient in terms of parameter updates.",
      "tldr_zh": "这篇论文提出了一种基于适配器(Adapter-based)微调方法，旨在开发一个统一的模型来处理多个语音处理任务，包括Automatic Speech Recognition、Phoneme Recognition、Intent Classification、Slot Filling和Spoken Emotion Recognition，从而解决自监督学习模型在多任务微调时的高计算资源需求。实验在SUPERB基准上验证了该方法，使单一的编码器-解码器模型在五种任务上实现了平均18.4%的性能提升，同时保持了参数更新的高效性。该研究为多任务语音处理提供了更灵活和资源节约的框架。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "ICASSP 2024",
      "pdf_url": "http://arxiv.org/pdf/2406.14747v1",
      "published_date": "2024-06-20 21:39:04 UTC",
      "updated_date": "2024-06-20 21:39:04 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T22:37:22.640093"
    },
    {
      "arxiv_id": "2406.14745v2",
      "title": "Relation Extraction with Fine-Tuned Large Language Models in Retrieval Augmented Generation Frameworks",
      "title_zh": "翻译失败",
      "authors": [
        "Sefika Efeoglu",
        "Adrian Paschke"
      ],
      "abstract": "Information Extraction (IE) is crucial for converting unstructured data into\nstructured formats like Knowledge Graphs (KGs). A key task within IE is\nRelation Extraction (RE), which identifies relationships between entities in\ntext. Various RE methods exist, including supervised, unsupervised, weakly\nsupervised, and rule-based approaches. Recent studies leveraging pre-trained\nlanguage models (PLMs) have shown significant success in this area. In the\ncurrent era dominated by Large Language Models (LLMs), fine-tuning these models\ncan overcome limitations associated with zero-shot LLM prompting-based RE\nmethods, especially regarding domain adaptation challenges and identifying\nimplicit relations between entities in sentences. These implicit relations,\nwhich cannot be easily extracted from a sentence's dependency tree, require\nlogical inference for accurate identification. This work explores the\nperformance of fine-tuned LLMs and their integration into the Retrieval\nAugmented-based (RAG) RE approach to address the challenges of identifying\nimplicit relations at the sentence level, particularly when LLMs act as\ngenerators within the RAG framework. Empirical evaluations on the TACRED,\nTACRED-Revisited (TACREV), Re-TACRED, and SemEVAL datasets show significant\nperformance improvements with fine-tuned LLMs, including Llama2-7B, Mistral-7B,\nand T5 (Large). Notably, our approach achieves substantial gains on SemEVAL,\nwhere implicit relations are common, surpassing previous results on this\ndataset. Additionally, our method outperforms previous works on TACRED, TACREV,\nand Re-TACRED, demonstrating exceptional performance across diverse evaluation\nscenarios.",
      "tldr_zh": "本文研究了在检索增强生成（Retrieval Augmented Generation, RAG）框架中使用微调的大型语言模型（Large Language Models, LLMs）来进行关系提取（Relation Extraction, RE），以解决文本中实体隐式关系的识别挑战，这些关系需通过逻辑推理而非依赖树提取。方法涉及对 LLMs（如 Llama2-7B、Mistral-7B 和 T5 Large）进行微调，并将其整合到 RAG 系统，以提升领域适应性和提取准确性。实验结果显示，在 TACRED、TACRED-Revisited (TACREV)、Re-TACRED 和 SemEVAL 数据集上，该方法取得了显著性能提升，尤其在 SemEVAL 数据集（隐式关系常见）上超越了先前最佳结果，并在其他数据集上也表现出色。总的来说，此工作证明了微调 LLMs 在 RAG 框架中的有效性，为信息提取（Information Extraction, IE）任务提供了新途径。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "preprint",
      "pdf_url": "http://arxiv.org/pdf/2406.14745v2",
      "published_date": "2024-06-20 21:27:57 UTC",
      "updated_date": "2024-06-24 06:57:05 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T22:37:25.767998"
    },
    {
      "arxiv_id": "2406.14744v1",
      "title": "Training Next Generation AI Users and Developers at NCSA",
      "title_zh": "在 NCSA 培训下一代 AI 用户和开发者",
      "authors": [
        "Daniel S. Katz",
        "Volodymyr Kindratenko",
        "Olena Kindratenko",
        "Priyam Mazumdar"
      ],
      "abstract": "This article focuses on training work carried out in artificial intelligence\n(AI) at the National Center for Supercomputing Applications (NCSA) at the\nUniversity of Illinois Urbana-Champaign via a research experience for\nundergraduates (REU) program named FoDOMMaT. It also describes why we are\ninterested in AI, and concludes by discussing what we've learned from running\nthis program and its predecessor over six years.",
      "tldr_zh": "这篇文章探讨了伊利诺伊大学香槟分校的超级计算应用国家中心（NCSA）通过名为 FoDOMMaT 的本科生研究体验（REU）程序进行的 AI 培训工作。文章解释了 NCSA 对 AI 的兴趣，并分享了从运行该程序及其前身六年中学到的经验教训。该培训旨在培养下一代 AI 用户和开发者，推动相关领域的教育和创新。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2406.14744v1",
      "published_date": "2024-06-20 21:27:24 UTC",
      "updated_date": "2024-06-20 21:27:24 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T22:37:36.024855"
    },
    {
      "arxiv_id": "2406.14735v1",
      "title": "An updated overview of radiomics-based artificial intelligence (AI) methods in breast cancer screening and diagnosis",
      "title_zh": "翻译失败",
      "authors": [
        "Reza Elahi",
        "Mahdis Nazari"
      ],
      "abstract": "Current imaging methods for diagnosing BC are associated with limited\nsensitivity and specificity and modest positive predictive power. The recent\nprogress in image analysis using artificial intelligence (AI) has created great\npromise to improve breast cancer (BC) diagnosis and subtype differentiation. In\nthis case, novel quantitative computational methods, such as radiomics, have\nbeen developed to improve the sensitivity and specificity of early BC diagnosis\nand classification. The potential of radiomics in improving the diagnostic\nefficacy of imaging studies has been shown in several studies. In this review\narticle, we discuss the radiomics workflow and current hand-crafted radiomics\nmethods in the diagnosis and classification of BC based on most recent studies\non different imaging modalities, e.g. MRI, mammography, contrast-enhanced\nspectral mammography (CESM), ultrasound imaging, and digital breast\ntumosynthesis (DBT). We also discuss current challenges and potential\nstrategies to improve the specificity and sensitivity of radiomics in breast\ncancer to help achieve a higher level of BC classification and diagnosis in the\nclinical setting. The growing field of AI incorporation with imaging\ninformation has opened a great opportunity to provide a higher level of care\nfor BC patients.",
      "tldr_zh": "这篇综述文章概述了基于 radiomics 和 artificial intelligence (AI) 的方法，用于提升乳腺癌 (BC) 筛查和诊断的敏感性、特异性和预测能力。文章详细讨论了 radiomics 工作流程以及手工 radiomics 方法在多种成像模式（如 MRI、mammography、contrast-enhanced spectral mammography (CESM)、ultrasound imaging 和 digital breast tomosynthesis (DBT)）中的应用，这些方法已在多项研究中证明能改善早期 BC 诊断和亚型分类。研究强调了 radiomics 在临床中的潜力，同时指出了当前挑战，如模型特异性和敏感性的局限，并提出了潜在策略来优化其性能。总体而言，AI 与成像信息相结合为提供更高水平的 BC 患者护理开辟了新机遇。",
      "categories": [
        "eess.IV",
        "cs.AI"
      ],
      "primary_category": "eess.IV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2406.14735v1",
      "published_date": "2024-06-20 21:01:11 UTC",
      "updated_date": "2024-06-20 21:01:11 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T22:37:51.992933"
    },
    {
      "arxiv_id": "2406.14722v3",
      "title": "Does GPT Really Get It? A Hierarchical Scale to Quantify Human vs AI's Understanding of Algorithms",
      "title_zh": "翻译失败",
      "authors": [
        "Mirabel Reid",
        "Santosh S. Vempala"
      ],
      "abstract": "As Large Language Models (LLMs) perform (and sometimes excel at) more and\nmore complex cognitive tasks, a natural question is whether AI really\nunderstands. The study of understanding in LLMs is in its infancy, and the\ncommunity has yet to incorporate well-trodden research in philosophy,\npsychology, and education. We initiate this, specifically focusing on\nunderstanding algorithms, and propose a hierarchy of levels of understanding.\nWe use the hierarchy to design and conduct a study with human subjects\n(undergraduate and graduate students) as well as large language models\n(generations of GPT), revealing interesting similarities and differences. We\nexpect that our rigorous criteria will be useful to keep track of AI's progress\nin such cognitive domains.",
      "tldr_zh": "这篇论文探讨了大型语言模型(LLMs)是否真正理解算法，并提出一个理解层次结构(hierarchy of levels of understanding)来量化人类与AI（如GPT系列）在算法理解方面的差异。研究者设计并进行了一项实验，涉及本科生、研究生和不同GPT版本的测试，揭示了人类和AI在认知任务中的相似性和差异。最终，该框架有望为评估AI在认知领域的进展提供严格的标准。",
      "categories": [
        "cs.AI",
        "I.2.m; F.1.1"
      ],
      "primary_category": "cs.AI",
      "comment": "13 pages, 10 figures. To be published at AAAI 2025",
      "pdf_url": "http://arxiv.org/pdf/2406.14722v3",
      "published_date": "2024-06-20 20:37:55 UTC",
      "updated_date": "2025-01-18 21:09:41 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T22:37:59.035934"
    },
    {
      "arxiv_id": "2406.14712v1",
      "title": "Qiskit HumanEval: An Evaluation Benchmark For Quantum Code Generative Models",
      "title_zh": "Qiskit HumanEval：用于量子代码生成模型的评估基准",
      "authors": [
        "Sanjay Vishwakarma",
        "Francis Harkins",
        "Siddharth Golecha",
        "Vishal Sharathchandra Bajpe",
        "Nicolas Dupuis",
        "Luca Buratti",
        "David Kremer",
        "Ismael Faro",
        "Ruchir Puri",
        "Juan Cruz-Benito"
      ],
      "abstract": "Quantum programs are typically developed using quantum Software Development\nKits (SDKs). The rapid advancement of quantum computing necessitates new tools\nto streamline this development process, and one such tool could be Generative\nArtificial intelligence (GenAI). In this study, we introduce and use the Qiskit\nHumanEval dataset, a hand-curated collection of tasks designed to benchmark the\nability of Large Language Models (LLMs) to produce quantum code using Qiskit -\na quantum SDK. This dataset consists of more than 100 quantum computing tasks,\neach accompanied by a prompt, a canonical solution, a comprehensive test case,\nand a difficulty scale to evaluate the correctness of the generated solutions.\nWe systematically assess the performance of a set of LLMs against the Qiskit\nHumanEval dataset's tasks and focus on the models ability in producing\nexecutable quantum code. Our findings not only demonstrate the feasibility of\nusing LLMs for generating quantum code but also establish a new benchmark for\nongoing advancements in the field and encourage further exploration and\ndevelopment of GenAI-driven tools for quantum code generation.",
      "tldr_zh": "本研究引入了 Qiskit HumanEval 数据集，这是一个手工策划的基准，用于评估 Large Language Models (LLMs) 生成量子代码的能力。数据集包含超过 100 个量子计算任务，每个任务包括提示、标准解决方案、全面测试用例和难度级别，以测试代码的正确性和可执行性。研究系统评估了多种 LLMs 在生成 Qiskit 量子代码方面的性能，结果证明 LLMs 在此领域的可行性，并为量子代码生成工具的进一步发展设立了新基准。",
      "categories": [
        "quant-ph",
        "cs.AI"
      ],
      "primary_category": "quant-ph",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2406.14712v1",
      "published_date": "2024-06-20 20:14:22 UTC",
      "updated_date": "2024-06-20 20:14:22 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T22:38:11.096278"
    },
    {
      "arxiv_id": "2406.14711v2",
      "title": "MultiAgent Collaboration Attack: Investigating Adversarial Attacks in Large Language Model Collaborations via Debate",
      "title_zh": "翻译失败",
      "authors": [
        "Alfonso Amayuelas",
        "Xianjun Yang",
        "Antonis Antoniades",
        "Wenyue Hua",
        "Liangming Pan",
        "William Wang"
      ],
      "abstract": "Large Language Models (LLMs) have shown exceptional results on current\nbenchmarks when working individually. The advancement in their capabilities,\nalong with a reduction in parameter size and inference times, has facilitated\nthe use of these models as agents, enabling interactions among multiple models\nto execute complex tasks. Such collaborations offer several advantages,\nincluding the use of specialized models (e.g. coding), improved confidence\nthrough multiple computations, and enhanced divergent thinking, leading to more\ndiverse outputs. Thus, the collaborative use of language models is expected to\ngrow significantly in the coming years. In this work, we evaluate the behavior\nof a network of models collaborating through debate under the influence of an\nadversary. We introduce pertinent metrics to assess the adversary's\neffectiveness, focusing on system accuracy and model agreement. Our findings\nhighlight the importance of a model's persuasive ability in influencing others.\nAdditionally, we explore inference-time methods to generate more compelling\narguments and evaluate the potential of prompt-based mitigation as a defensive\nstrategy.",
      "tldr_zh": "本论文探讨了大型语言模型(LLMs)在多代理协作中的对手攻击问题，重点考察了通过辩论机制的影响。研究引入了系统准确性和模型一致性等指标来评估对手的有效性，并发现模型的说服能力在协作中起关键作用。论文进一步探索了推理时生成更具说服力论据的方法，以及基于提示的缓解策略，以提升协作系统的鲁棒性。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.MA"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2406.14711v2",
      "published_date": "2024-06-20 20:09:37 UTC",
      "updated_date": "2024-06-26 16:05:20 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T22:38:23.229189"
    },
    {
      "arxiv_id": "2406.14703v3",
      "title": "Do LLMs Have Distinct and Consistent Personality? TRAIT: Personality Testset designed for LLMs with Psychometrics",
      "title_zh": "翻译失败",
      "authors": [
        "Seungbeen Lee",
        "Seungwon Lim",
        "Seungju Han",
        "Giyeong Oh",
        "Hyungjoo Chae",
        "Jiwan Chung",
        "Minju Kim",
        "Beong-woo Kwak",
        "Yeonsoo Lee",
        "Dongha Lee",
        "Jinyoung Yeo",
        "Youngjae Yu"
      ],
      "abstract": "Recent advancements in Large Language Models (LLMs) have led to their\nadaptation in various domains as conversational agents. We wonder: can\npersonality tests be applied to these agents to analyze their behavior, similar\nto humans? We introduce TRAIT, a new benchmark consisting of 8K multi-choice\nquestions designed to assess the personality of LLMs. TRAIT is built on two\npsychometrically validated small human questionnaires, Big Five Inventory (BFI)\nand Short Dark Triad (SD-3), enhanced with the ATOMIC-10X knowledge graph to a\nvariety of real-world scenarios. TRAIT also outperforms existing personality\ntests for LLMs in terms of reliability and validity, achieving the highest\nscores across four key metrics: Content Validity, Internal Validity, Refusal\nRate, and Reliability. Using TRAIT, we reveal two notable insights into\npersonalities of LLMs: 1) LLMs exhibit distinct and consistent personality,\nwhich is highly influenced by their training data (e.g., data used for\nalignment tuning), and 2) current prompting techniques have limited\neffectiveness in eliciting certain traits, such as high psychopathy or low\nconscientiousness, suggesting the need for further research in this direction.",
      "tldr_zh": "本研究探讨大型语言模型（LLMs）的个性是否独特且一致，引入了 TRAIT 基准——一个包含 8K 多选题的测试集，用于评估 LLMs 的个性。TRAIT 基于心理测量学验证的人类问卷 Big Five Inventory (BFI) 和 Short Dark Triad (SD-3)，并通过 ATOMIC-10X 知识图谱扩展到各种真实场景中，在 Content Validity、Internal Validity、Refusal Rate 和 Reliability 等四项关键指标上优于现有测试。实验结果显示，LLMs 表现出 distinct and consistent personality，受训练数据（如对齐调整数据）影响显著；然而，当前提示技术在激发某些特质（如高 psychopathy 或低 conscientiousness）方面效果有限，需进一步研究。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "Accepted to NAACL2025 Findings",
      "pdf_url": "http://arxiv.org/pdf/2406.14703v3",
      "published_date": "2024-06-20 19:50:56 UTC",
      "updated_date": "2025-03-19 15:37:42 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T22:38:37.742900"
    },
    {
      "arxiv_id": "2406.14701v1",
      "title": "Speech Prefix-Tuning with RNNT Loss for Improving LLM Predictions",
      "title_zh": "语音前缀调优结合 RNNT 损失用于改善 LLM 预测",
      "authors": [
        "Murali Karthick Baskar",
        "Andrew Rosenberg",
        "Bhuvana Ramabhadran",
        "Neeraj Gaur",
        "Zhong Meng"
      ],
      "abstract": "In this paper, we focus on addressing the constraints faced when applying\nLLMs to ASR. Recent works utilize prefixLM-type models, which directly apply\nspeech as a prefix to LLMs for ASR. We have found that optimizing speech\nprefixes leads to better ASR performance and propose applying RNNT loss to\nperform speech prefix-tuning. This is a simple approach and does not increase\nthe model complexity or alter the inference pipeline. We also propose\nlanguage-based soft prompting to further improve with frozen LLMs. Empirical\nanalysis on realtime testset from 10 Indic languages demonstrate that our\nproposed speech prefix-tuning yields improvements with both frozen and\nfine-tuned LLMs. Our recognition results on an average of 10 Indics show that\nthe proposed prefix-tuning with RNNT loss results in a 12\\% relative\nimprovement in WER over the baseline with a fine-tuned LLM. Our proposed\napproches with the frozen LLM leads to a 31\\% relative improvement over basic\nsoft-prompting prefixLM.",
      "tldr_zh": "本论文针对将大型语言模型(LLMs)应用于自动语音识别(ASR)时存在的约束问题，提出了一种speech prefix-tuning方法，通过RNNT loss优化语音前缀，从而提升ASR性能，同时不增加模型复杂性或改变推理管道。论文还引入language-based soft prompting技术，进一步改进frozen LLMs的表现。在10种印度语言的实时测试集上实验显示，该方法使fine-tuned LLMs的WER相对改善12%，而与frozen LLMs结合时较基本soft-prompting prefixLM改善31%。",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.SD",
        "eess.AS"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2406.14701v1",
      "published_date": "2024-06-20 19:50:49 UTC",
      "updated_date": "2024-06-20 19:50:49 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T22:38:47.794298"
    },
    {
      "arxiv_id": "2406.14696v1",
      "title": "Physically Analyzable AI-Based Nonlinear Platoon Dynamics Modeling During Traffic Oscillation: A Koopman Approach",
      "title_zh": "翻译失败",
      "authors": [
        "Kexin Tian",
        "Haotian Shi",
        "Yang Zhou",
        "Sixu Li"
      ],
      "abstract": "Given the complexity and nonlinearity inherent in traffic dynamics within\nvehicular platoons, there exists a critical need for a modeling methodology\nwith high accuracy while concurrently achieving physical analyzability.\nCurrently, there are two predominant approaches: the physics model-based\napproach and the Artificial Intelligence (AI)--based approach. Knowing the\nfacts that the physical-based model usually lacks sufficient modeling accuracy\nand potential function mismatches and the pure-AI-based method lacks\nanalyzability, this paper innovatively proposes an AI-based Koopman approach to\nmodel the unknown nonlinear platoon dynamics harnessing the power of AI and\nsimultaneously maintain physical analyzability, with a particular focus on\nperiods of traffic oscillation. Specifically, this research first employs a\ndeep learning framework to generate the embedding function that lifts the\noriginal space into the embedding space. Given the embedding space\ndescriptiveness, the platoon dynamics can be expressed as a linear dynamical\nsystem founded by the Koopman theory. Based on that, the routine of linear\ndynamical system analysis can be conducted on the learned traffic linear\ndynamics in the embedding space. By that, the physical interpretability and\nanalyzability of model-based methods with the heightened precision inherent in\ndata-driven approaches can be synergized. Comparative experiments have been\nconducted with existing modeling approaches, which suggests our method's\nsuperiority in accuracy. Additionally, a phase plane analysis is performed,\nfurther evidencing our approach's effectiveness in replicating the complex\ndynamic patterns. Moreover, the proposed methodology is proven to feature the\ncapability of analyzing the stability, attesting to the physical analyzability.",
      "tldr_zh": "本文针对交通车队（vehicular platoons）非线性动态建模的挑战，提出了一种AI-based Koopman方法，旨在结合AI的高精度优势与物理可分析性，特别聚焦于交通震荡期。该方法首先利用深度学习框架生成嵌入函数，将原始空间提升到嵌入空间中，从而基于Koopman理论将车队动态表示为线性动态系统，便于进行物理解释和分析。实验比较显示，该方法在准确性上优于现有物理模型和纯AI方法；此外，通过相平面分析和稳定性分析，进一步验证了其在复制复杂动态模式和评估系统稳定性的有效性。",
      "categories": [
        "eess.SY",
        "cs.AI",
        "cs.SY"
      ],
      "primary_category": "eess.SY",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2406.14696v1",
      "published_date": "2024-06-20 19:35:21 UTC",
      "updated_date": "2024-06-20 19:35:21 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T22:39:02.054762"
    },
    {
      "arxiv_id": "2406.14695v1",
      "title": "Depth $F_1$: Improving Evaluation of Cross-Domain Text Classification by Measuring Semantic Generalizability",
      "title_zh": "翻译失败",
      "authors": [
        "Parker Seegmiller",
        "Joseph Gatto",
        "Sarah Masud Preum"
      ],
      "abstract": "Recent evaluations of cross-domain text classification models aim to measure\nthe ability of a model to obtain domain-invariant performance in a target\ndomain given labeled samples in a source domain. The primary strategy for this\nevaluation relies on assumed differences between source domain samples and\ntarget domain samples in benchmark datasets. This evaluation strategy fails to\naccount for the similarity between source and target domains, and may mask when\nmodels fail to transfer learning to specific target samples which are highly\ndissimilar from the source domain. We introduce Depth $F_1$, a novel\ncross-domain text classification performance metric. Designed to be\ncomplementary to existing classification metrics such as $F_1$, Depth $F_1$\nmeasures how well a model performs on target samples which are dissimilar from\nthe source domain. We motivate this metric using standard cross-domain text\nclassification datasets and benchmark several recent cross-domain text\nclassification models, with the goal of enabling in-depth evaluation of the\nsemantic generalizability of cross-domain text classification models.",
      "tldr_zh": "该论文指出现有跨领域文本分类（cross-domain text classification）的评估方法依赖源域和目标域样本差异的假设，但忽略了样本相似性，从而可能掩盖模型在处理与源域高度不相似目标样本时的失败。论文引入了一个新指标Depth $F_1$，作为$F_1$指标的补充，用于测量模型在语义上不相似目标样本上的性能，从而更准确地评估模型的语义泛化能力（semantic generalizability）。作者使用标准数据集对多个最近的跨领域文本分类模型进行基准测试，结果表明Depth $F_1$能提供更深入的模型评估，帮助识别潜在的泛化问题。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2406.14695v1",
      "published_date": "2024-06-20 19:35:17 UTC",
      "updated_date": "2024-06-20 19:35:17 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T22:39:13.233870"
    },
    {
      "arxiv_id": "2406.14675v1",
      "title": "This Looks Better than That: Better Interpretable Models with ProtoPNeXt",
      "title_zh": "翻译失败",
      "authors": [
        "Frank Willard",
        "Luke Moffett",
        "Emmanuel Mokel",
        "Jon Donnelly",
        "Stark Guo",
        "Julia Yang",
        "Giyoung Kim",
        "Alina Jade Barnett",
        "Cynthia Rudin"
      ],
      "abstract": "Prototypical-part models are a popular interpretable alternative to black-box\ndeep learning models for computer vision. However, they are difficult to train,\nwith high sensitivity to hyperparameter tuning, inhibiting their application to\nnew datasets and our understanding of which methods truly improve their\nperformance. To facilitate the careful study of prototypical-part networks\n(ProtoPNets), we create a new framework for integrating components of\nprototypical-part models -- ProtoPNeXt. Using ProtoPNeXt, we show that applying\nBayesian hyperparameter tuning and an angular prototype similarity metric to\nthe original ProtoPNet is sufficient to produce new state-of-the-art accuracy\nfor prototypical-part models on CUB-200 across multiple backbones. We further\ndeploy this framework to jointly optimize for accuracy and prototype\ninterpretability as measured by metrics included in ProtoPNeXt. Using the same\nresources, this produces models with substantially superior semantics and\nchanges in accuracy between +1.3% and -1.5%. The code and trained models will\nbe made publicly available upon publication.",
      "tldr_zh": "该研究提出ProtoPNeXt框架，以改进Prototypical-part networks (ProtoPNets)的训练和性能，这些模型是计算机视觉中可解释性的替代方案，但面临训练困难和超参数敏感的问题。通过整合组件并应用Bayesian hyperparameter tuning和angular prototype similarity metric，ProtoPNeXt在CUB-200数据集上实现了新的state-of-the-art准确率。进一步优化准确性和原型解释性后，该框架生成了语义更优的模型，准确率变化在+1.3%至-1.5%之间，同时代码和训练模型将公开以促进进一步研究。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2406.14675v1",
      "published_date": "2024-06-20 18:54:27 UTC",
      "updated_date": "2024-06-20 18:54:27 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T22:39:25.340697"
    },
    {
      "arxiv_id": "2406.14670v2",
      "title": "Exploring Design Choices for Building Language-Specific LLMs",
      "title_zh": "翻译失败",
      "authors": [
        "Atula Tejaswi",
        "Nilesh Gupta",
        "Eunsol Choi"
      ],
      "abstract": "Despite rapid progress in large language models (LLMs), their performance on\na vast majority of languages remains unsatisfactory. In this paper, we study\nbuilding language-specific LLMs by adapting monolingual and multilingual LLMs.\nWe conduct systematic experiments on how design choices (base model selection,\nvocabulary extension, and continued pretraining) impact the adapted LLM, both\nin terms of efficiency (how many tokens are needed to encode the same amount of\ninformation) and end task performance. We find that (1) the initial performance\nof LLM does not always correlate with the final performance after the\nadaptation. Adapting an English-centric models can yield better results than\nadapting multilingual models despite their worse initial performance on\nlow-resource languages. (2) Efficiency can easily improved with simple\nvocabulary extension and continued pretraining in most LLMs we study, and (3)\nThe optimal adaptation method (choice of the base model, new vocabulary size,\ntraining data, initialization strategy) is highly language-dependent, and the\nsimplest embedding initialization works well across various experimental\nsettings. Together, our work lays foundations on efficiently building\nlanguage-specific LLMs by adapting existing LLMs.",
      "tldr_zh": "本研究探讨了构建特定语言的大型语言模型 (LLMs) 的设计选择，通过适应单语和多语 LLMs 来提升其在低资源语言上的性能。实验系统分析了基础模型选择、词汇扩展和继续预训练等因素对模型效率（编码信息所需 token 数量）和任务性能的影响，发现以英语为中心的模型在适应后往往比多语模型表现更好，尽管其初始性能较差。结果表明，简单词汇扩展和继续预训练可轻松提高效率，而最优适应方法（如新词汇大小和初始化策略）高度依赖于具体语言。该工作为高效构建语言特定 LLMs 奠定了基础。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "Accepted to EMNLP 2024 Findings",
      "pdf_url": "http://arxiv.org/pdf/2406.14670v2",
      "published_date": "2024-06-20 18:47:43 UTC",
      "updated_date": "2024-10-30 16:33:48 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T22:39:37.627631"
    },
    {
      "arxiv_id": "2406.15513v2",
      "title": "PKU-SafeRLHF: Towards Multi-Level Safety Alignment for LLMs with Human Preference",
      "title_zh": "翻译失败",
      "authors": [
        "Jiaming Ji",
        "Donghai Hong",
        "Borong Zhang",
        "Boyuan Chen",
        "Josef Dai",
        "Boren Zheng",
        "Tianyi Qiu",
        "Boxun Li",
        "Yaodong Yang"
      ],
      "abstract": "In this work, we introduce the PKU-SafeRLHF dataset, designed to promote\nresearch on safety alignment in large language models (LLMs). As a sibling\nproject to SafeRLHF and BeaverTails, we separate annotations of helpfulness and\nharmlessness for question-answering pairs, providing distinct perspectives on\nthese coupled attributes. Overall, we provide 44.6k refined prompts and 265k\nquestion-answer pairs with safety meta-labels for 19 harm categories and three\nseverity levels ranging from minor to severe, with answers generated by\nLlama-family models. Based on this, we collected 166.8k preference data,\nincluding dual-preference (helpfulness and harmlessness decoupled) and\nsingle-preference data (trade-off the helpfulness and harmlessness from\nscratch), respectively. Using the large-scale annotation data, we further train\nseverity-sensitive moderation for the risk control of LLMs and safety-centric\nRLHF algorithms for the safety alignment of LLMs. We believe this dataset will\nbe a valuable resource for the community, aiding in the safe deployment of\nLLMs.",
      "tldr_zh": "本研究引入了 PKU-SafeRLHF 数据集，旨在提升大型语言模型 (LLMs) 的多级别安全对齐，通过人类偏好数据处理 helpfulness 和 harmlessness 的分离。该数据集包含 44.6k 精炼提示、265k 问题-答案对，以及针对 19 个危害类别的安全元标签和三个严重程度级别（从轻微到严重），并使用 Llama 家族模型生成答案。基于 166.8k 偏好数据（包括双偏好和单偏好），研究者训练了严重程度敏感的调节模型和以安全为中心的 RLHF 算法，最终为 LLMs 的安全部署提供宝贵资源。",
      "categories": [
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.AI",
      "comment": "a sibling project to SafeRLHF and BeaverTails",
      "pdf_url": "http://arxiv.org/pdf/2406.15513v2",
      "published_date": "2024-06-20 18:37:36 UTC",
      "updated_date": "2024-10-16 01:33:35 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T22:39:49.401567"
    },
    {
      "arxiv_id": "2406.14657v3",
      "title": "OpenDebateEvidence: A Massive-Scale Argument Mining and Summarization Dataset",
      "title_zh": "翻译失败",
      "authors": [
        "Allen Roush",
        "Yusuf Shabazz",
        "Arvind Balaji",
        "Peter Zhang",
        "Stefano Mezza",
        "Markus Zhang",
        "Sanjay Basu",
        "Sriram Vishwanath",
        "Mehdi Fatemi",
        "Ravid Shwartz-Ziv"
      ],
      "abstract": "We introduce OpenDebateEvidence, a comprehensive dataset for argument mining\nand summarization sourced from the American Competitive Debate community. This\ndataset includes over 3.5 million documents with rich metadata, making it one\nof the most extensive collections of debate evidence. OpenDebateEvidence\ncaptures the complexity of arguments in high school and college debates,\nproviding valuable resources for training and evaluation. Our extensive\nexperiments demonstrate the efficacy of fine-tuning state-of-the-art large\nlanguage models for argumentative abstractive summarization across various\nmethods, models, and datasets. By providing this comprehensive resource, we aim\nto advance computational argumentation and support practical applications for\ndebaters, educators, and researchers. OpenDebateEvidence is publicly available\nto support further research and innovation in computational argumentation.\nAccess it here: https://huggingface.co/datasets/Yusuf5/OpenCaselist",
      "tldr_zh": "本研究引入了 OpenDebateEvidence，这是一个大规模数据集，源自美国竞争辩论社区，包含超过 350 万份文档及丰富的元数据，是目前论证挖掘和总结领域的最广泛资源之一。该数据集捕捉高中和大学辩论的论证复杂性，提供宝贵材料用于模型训练和评估。研究团队通过广泛实验证明，微调最先进的大型语言模型（large language models）在各种方法和数据集上可有效提升论证抽取式摘要性能。该数据集公开可用（访问链接：https://huggingface.co/datasets/Yusuf5/OpenCaselist），旨在推进计算论证（computational argumentation）的创新，并支持辩手、教育者和研究者的实际应用。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "Published to the 38th Conference on Neural Information Processing\n  Systems (NeurIPS 2024) Track on Datasets and Benchmarks",
      "pdf_url": "http://arxiv.org/pdf/2406.14657v3",
      "published_date": "2024-06-20 18:22:59 UTC",
      "updated_date": "2024-10-31 03:41:03 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T22:40:00.456247"
    },
    {
      "arxiv_id": "2406.14655v1",
      "title": "HYPERmotion: Learning Hybrid Behavior Planning for Autonomous Loco-manipulation",
      "title_zh": "翻译失败",
      "authors": [
        "Jin Wang",
        "Rui Dai",
        "Weijie Wang",
        "Luca Rossini",
        "Francesco Ruscelli",
        "Nikos Tsagarakis"
      ],
      "abstract": "Enabling robots to autonomously perform hybrid motions in diverse\nenvironments can be beneficial for long-horizon tasks such as material\nhandling, household chores, and work assistance. This requires extensive\nexploitation of intrinsic motion capabilities, extraction of affordances from\nrich environmental information, and planning of physical interaction behaviors.\nDespite recent progress has demonstrated impressive humanoid whole-body control\nabilities, they struggle to achieve versatility and adaptability for new tasks.\nIn this work, we propose HYPERmotion, a framework that learns, selects and\nplans behaviors based on tasks in different scenarios. We combine reinforcement\nlearning with whole-body optimization to generate motion for 38 actuated joints\nand create a motion library to store the learned skills. We apply the planning\nand reasoning features of the large language models (LLMs) to complex\nloco-manipulation tasks, constructing a hierarchical task graph that comprises\na series of primitive behaviors to bridge lower-level execution with\nhigher-level planning. By leveraging the interaction of distilled spatial\ngeometry and 2D observation with a visual language model (VLM) to ground\nknowledge into a robotic morphology selector to choose appropriate actions in\nsingle- or dual-arm, legged or wheeled locomotion. Experiments in simulation\nand real-world show that learned motions can efficiently adapt to new tasks,\ndemonstrating high autonomy from free-text commands in unstructured scenes.\nVideos and website: hy-motion.github.io/",
      "tldr_zh": "本研究提出HYPERmotion框架，用于自主 loco-manipulation（移动和操作）的混合行为规划，旨在帮助机器人处理多样化环境中的长时任务，如材料搬运和家务。框架结合强化学习（Reinforcement Learning）和全身优化，生成38个关节的动作并构建动作库，同时利用大语言模型（LLMs）的规划能力构建分层任务图，并通过视觉语言模型（VLM）与空间几何和2D观察交互，选择合适的机器人形态（如单臂或双臂、步行或轮式）。实验在模拟和真实环境中证明，该框架使机器人能高效适应新任务，从自由文本命令实现高自治和灵活性。",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.RO",
      "comment": "Project page: https://hy-motion.github.io/",
      "pdf_url": "http://arxiv.org/pdf/2406.14655v1",
      "published_date": "2024-06-20 18:21:24 UTC",
      "updated_date": "2024-06-20 18:21:24 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T22:40:13.197727"
    },
    {
      "arxiv_id": "2406.14654v2",
      "title": "Major Entity Identification: A Generalizable Alternative to Coreference Resolution",
      "title_zh": "翻译失败",
      "authors": [
        "Kawshik Manikantan",
        "Shubham Toshniwal",
        "Makarand Tapaswi",
        "Vineet Gandhi"
      ],
      "abstract": "The limited generalization of coreference resolution (CR) models has been a\nmajor bottleneck in the task's broad application. Prior work has identified\nannotation differences, especially for mention detection, as one of the main\nreasons for the generalization gap and proposed using additional annotated\ntarget domain data. Rather than relying on this additional annotation, we\npropose an alternative referential task, Major Entity Identification (MEI),\nwhere we: (a) assume the target entities to be specified in the input, and (b)\nlimit the task to only the frequent entities. Through extensive experiments, we\ndemonstrate that MEI models generalize well across domains on multiple datasets\nwith supervised models and LLM-based few-shot prompting. Additionally, MEI fits\nthe classification framework, which enables the use of robust and intuitive\nclassification-based metrics. Finally, MEI is also of practical use as it\nallows a user to search for all mentions of a particular entity or a group of\nentities of interest.",
      "tldr_zh": "本文提出 Major Entity Identification (MEI) 作为 Coreference Resolution (CR) 的可泛化替代方案，以解决 CR 模型在跨域应用中的泛化瓶颈问题，特别是由于标注差异导致的提及检测挑战。MEI 假设目标实体已在输入中指定，并仅限于频繁实体，从而避免了额外标注数据的依赖。通过广泛实验，MEI 模型在多个数据集上显示出优秀的泛化性能，支持监督学习和 LLM 基于少样本提示。MEI 采用分类框架，使用鲁棒的分类指标，并具有实际应用价值，如允许用户搜索特定实体或实体组的所有提及。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG",
        "I.2.7"
      ],
      "primary_category": "cs.CL",
      "comment": "17 pages, 6 figures",
      "pdf_url": "http://arxiv.org/pdf/2406.14654v2",
      "published_date": "2024-06-20 18:17:58 UTC",
      "updated_date": "2024-10-04 11:08:06 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T22:40:25.481232"
    },
    {
      "arxiv_id": "2406.14653v1",
      "title": "LLM Granularity for On-the-Fly Robot Control",
      "title_zh": "翻译失败",
      "authors": [
        "Peng Wang",
        "Mattia Robbiani",
        "Zhihao Guo"
      ],
      "abstract": "Assistive robots have attracted significant attention due to their potential\nto enhance the quality of life for vulnerable individuals like the elderly. The\nconvergence of computer vision, large language models, and robotics has\nintroduced the `visuolinguomotor' mode for assistive robots, where visuals and\nlinguistics are incorporated into assistive robots to enable proactive and\ninteractive assistance. This raises the question: \\textit{In circumstances\nwhere visuals become unreliable or unavailable, can we rely solely on language\nto control robots, i.e., the viability of the `linguomotor` mode for assistive\nrobots?} This work takes the initial steps to answer this question by: 1)\nevaluating the responses of assistive robots to language prompts of varying\ngranularities; and 2) exploring the necessity and feasibility of controlling\nthe robot on-the-fly. We have designed and conducted experiments on a Sawyer\ncobot to support our arguments. A Turtlebot robot case is designed to\ndemonstrate the adaptation of the solution to scenarios where assistive robots\nneed to maneuver to assist. Codes will be released on GitHub soon to benefit\nthe community.",
      "tldr_zh": "该论文探讨了在视觉不可靠或不可用情况下，仅靠语言控制辅助机器人的可行性，即“linguomotor”模式，以补充传统的“visuolinguomotor”模式。研究团队评估了大型语言模型(LLM)对不同粒度语言提示的响应，并探索了实时(on-the-fly)控制的必要性和可行性，通过在Sawyer cobot上设计实验，并扩展到Turtlebot机器人场景以展示适应性。实验结果证明了这种语言驱动方法的有效性，并计划开源代码以促进社区发展。",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2406.14653v1",
      "published_date": "2024-06-20 18:17:48 UTC",
      "updated_date": "2024-06-20 18:17:48 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T22:40:37.372962"
    },
    {
      "arxiv_id": "2406.14643v3",
      "title": "Holistic Evaluation for Interleaved Text-and-Image Generation",
      "title_zh": "针对交错文本与图像生成的整体评估",
      "authors": [
        "Minqian Liu",
        "Zhiyang Xu",
        "Zihao Lin",
        "Trevor Ashby",
        "Joy Rimchala",
        "Jiaxin Zhang",
        "Lifu Huang"
      ],
      "abstract": "Interleaved text-and-image generation has been an intriguing research\ndirection, where the models are required to generate both images and text\npieces in an arbitrary order. Despite the emerging advancements in interleaved\ngeneration, the progress in its evaluation still significantly lags behind.\nExisting evaluation benchmarks do not support arbitrarily interleaved images\nand text for both inputs and outputs, and they only cover a limited number of\ndomains and use cases. Also, current works predominantly use similarity-based\nmetrics which fall short in assessing the quality in open-ended scenarios. To\nthis end, we introduce InterleavedBench, the first benchmark carefully curated\nfor the evaluation of interleaved text-and-image generation. InterleavedBench\nfeatures a rich array of tasks to cover diverse real-world use cases. In\naddition, we present InterleavedEval, a strong reference-free metric powered by\nGPT-4o to deliver accurate and explainable evaluation. We carefully define five\nessential evaluation aspects for InterleavedEval, including text quality,\nperceptual quality, image coherence, text-image coherence, and helpfulness, to\nensure a comprehensive and fine-grained assessment. Through extensive\nexperiments and rigorous human evaluation, we show that our benchmark and\nmetric can effectively evaluate the existing models with a strong correlation\nwith human judgments surpassing previous reference-based metrics. We also\nprovide substantial findings and insights to foster future research in\ninterleaved generation and its evaluation.",
      "tldr_zh": "该研究针对交错文本-and-image生成（interleaved text-and-image generation）领域的评估不足，提出了首个基准测试InterleavedBench，以覆盖多样真实世界任务，支持任意顺序的输入和输出。研究还引入了InterleavedEval，一种基于GPT-4o的参考-free指标，定义了五个关键评估方面，包括文本质量、perceptual quality、image coherence、text-image coherence和helpfulness，以实现全面细致的评估。通过广泛实验和人类评估，InterleavedEval显示出与人类判断的高度相关性，超越了现有基于相似性的指标，并提供了宝贵见解以推动该领域的未来研究。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.CV",
      "comment": "EMNLP 2024 Main Conference. 15 pages, 6 figures, 7 tables. Website:\n  https://vt-nlp.github.io/InterleavedEval/. Dataset:\n  https://huggingface.co/mqliu/InterleavedBench",
      "pdf_url": "http://arxiv.org/pdf/2406.14643v3",
      "published_date": "2024-06-20 18:07:19 UTC",
      "updated_date": "2024-10-08 16:02:08 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T22:40:48.601776"
    },
    {
      "arxiv_id": "2406.14635v1",
      "title": "Harvesting Efficient On-Demand Order Pooling from Skilled Couriers: Enhancing Graph Representation Learning for Refining Real-time Many-to-One Assignments",
      "title_zh": "翻译失败",
      "authors": [
        "Yile Liang",
        "Jiuxia Zhao",
        "Donghui Li",
        "Jie Feng",
        "Chen Zhang",
        "Xuetao Ding",
        "Jinghua Hao",
        "Renqing He"
      ],
      "abstract": "The recent past has witnessed a notable surge in on-demand food delivery\n(OFD) services, offering delivery fulfillment within dozens of minutes after an\norder is placed. In OFD, pooling multiple orders for simultaneous delivery in\nreal-time order assignment is a pivotal efficiency source, which may in turn\nextend delivery time. Constructing high-quality order pooling to harmonize\nplatform efficiency with the experiences of consumers and couriers, is crucial\nto OFD platforms. However, the complexity and real-time nature of order\nassignment, making extensive calculations impractical, significantly limit the\npotential for order consolidation. Moreover, offline environment is frequently\nriddled with unknown factors, posing challenges for the platform's\nperceptibility and pooling decisions. Nevertheless, delivery behaviors of\nskilled couriers (SCs) who know the environment well, can improve system\nawareness and effectively inform decisions. Hence a SC delivery network (SCDN)\nis constructed, based on an enhanced attributed heterogeneous network embedding\napproach tailored for OFD. It aims to extract features from rich temporal and\nspatial information, and uncover the latent potential for order combinations\nembedded within SC trajectories. Accordingly, the vast search space of order\nassignment can be effectively pruned through scalable similarity calculations\nof low-dimensional vectors, making comprehensive and high-quality pooling\noutcomes more easily identified in real time. SCDN has now been deployed in\nMeituan dispatch system. Online tests reveal that with SCDN, the pooling\nquality and extent have been greatly improved. And our system can boost\ncouriers'efficiency by 45-55% during noon peak hours, while upholding the\ntimely delivery commitment.",
      "tldr_zh": "该研究针对外卖配送（OFD）的实时订单池化问题，提出一种基于熟练配送员（skilled couriers, SCs）的SC交付网络（SCDN），利用增强的图表示学习（Graph Representation Learning）来提取SCs轨迹中的时间和空间特征，从而高效识别潜在订单组合并修剪搜索空间。SCDN通过可扩展的相似性计算实现高质订单分配，平衡平台效率与交付体验。实验结果显示，该系统已在美团调度系统中部署，提高了池化质量，并在午高峰时段将配送员效率提升45-55%，同时保持及时交付承诺。",
      "categories": [
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "Accepted in KDD 2024 ADS Track",
      "pdf_url": "http://arxiv.org/pdf/2406.14635v1",
      "published_date": "2024-06-20 18:03:27 UTC",
      "updated_date": "2024-06-20 18:03:27 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T22:41:02.845792"
    },
    {
      "arxiv_id": "2406.14634v3",
      "title": "Adaptive Manipulation using Behavior Trees",
      "title_zh": "翻译失败",
      "authors": [
        "Jacques Cloete",
        "Wolfgang Merkt",
        "Ioannis Havoutis"
      ],
      "abstract": "Many manipulation tasks pose a challenge since they depend on non-visual\nenvironmental information that can only be determined after sustained physical\ninteraction has already begun. This is particularly relevant for\neffort-sensitive, dynamics-dependent tasks such as tightening a valve. To\nperform these tasks safely and reliably, robots must be able to quickly adapt\nin response to unexpected changes during task execution, and should also learn\nfrom past experience to better inform future decisions. Humans can intuitively\nrespond and adapt their manipulation strategy to suit such problems, but\nrepresenting and implementing such behaviors for robots remains a challenge. In\nthis work we show how this can be achieved within the framework of behavior\ntrees. We present the adaptive behavior tree, a scalable and generalizable\nbehavior tree design that enables a robot to quickly adapt to and learn from\nboth visual and non-visual observations during task execution, preempting task\nfailure or switching to a different manipulation strategy. The adaptive\nbehavior tree selects the manipulation strategy that is predicted to optimize\ntask performance, and learns from past experience to improve these predictions\nfor future attempts. We test our approach on a variety of tasks commonly found\nin industry; the adaptive behavior tree demonstrates safety, robustness (100%\nsuccess rate) and efficiency in task completion (up to 36% task speedup from\nthe baseline).",
      "tldr_zh": "这篇论文提出了 adaptive behavior tree，一种可扩展且可泛化的行为树框架，用于机器人处理依赖非视觉环境信息的操作任务，如拧紧阀门等努力敏感任务。框架允许机器人快速适应视觉和非视觉观察，从过去经验中学习，并动态选择优化任务性能的操纵策略，以防止失败或切换策略。在工业常见任务上的实验中，该方法展示了安全性、鲁棒性（100% 成功率）和效率提升（比基线快高达 36%）。",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "comment": "8 pages, 7 figures. This work has been submitted to the IEEE for\n  possible publication",
      "pdf_url": "http://arxiv.org/pdf/2406.14634v3",
      "published_date": "2024-06-20 18:01:36 UTC",
      "updated_date": "2025-03-08 16:00:30 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T22:41:13.810831"
    },
    {
      "arxiv_id": "2406.14629v3",
      "title": "Can LLMs Learn by Teaching for Better Reasoning? A Preliminary Study",
      "title_zh": "LLMs 能否通过教学实现更好的推理？ 初步研究",
      "authors": [
        "Xuefei Ning",
        "Zifu Wang",
        "Shiyao Li",
        "Zinan Lin",
        "Peiran Yao",
        "Tianyu Fu",
        "Matthew B. Blaschko",
        "Guohao Dai",
        "Huazhong Yang",
        "Yu Wang"
      ],
      "abstract": "Teaching to improve student models (e.g., knowledge distillation) is an\nextensively studied methodology in LLMs. However, for humans, teaching improves\nnot only students but also teachers, by fostering more rigorous and clear\nreasoning as well as knowledge building. We ask: Can LLMs also learn by\nteaching (LbT) for better reasoning? If the answer is yes, we can potentially\nunlock the possibility of continuously advancing the models without solely\nrelying on human-produced data or stronger models. In this paper, we provide a\npreliminary exploration on this question. We show that LbT ideas can be\nincorporated into existing LLM training/prompting pipelines and bring\nimprovements. Specifically, we design three methods, each mimicking one of the\nthree levels of LbT: observing students' feedback, learning from the feedback,\nand learning iteratively, with the goals of improving answer accuracy without\ntraining or improving models' inherent capability with fine-tuning. We reveal\nsome findings: (1) Teaching materials that make it easier for students to learn\nhave clearer and more accurate logic when using in-context learning as the\nstudent's \"learning\" method; (2) Weak-to-strong generalization: LbT might help\nimprove strong models by teaching weak models; (3) Diversity in students might\nhelp: teaching multiple students could be better than teaching one student or\nthe teacher itself. We hope that our exploration can inspire future research on\nLbT and more broadly adopting the advanced techniques in education to improve\nLLMs. The code and website are at https://github.com/imagination-research/lbt\nand https://sites.google.com/view/llm-learning-by-teaching.",
      "tldr_zh": "本文通过初步研究探讨大型语言模型(LLMs)是否能通过“学习通过教学”(LbT)来提升推理能力，旨在减少对人类数据或更强模型的依赖。研究设计了三种方法，分别模拟观察学生反馈、从反馈中学习以及迭代学习，融入现有LLMs训练或提示管道，以改善答案准确性或模型内在能力。实验发现，使用更容易让学生学习的教学材料能使逻辑更清晰准确；LbT可能实现弱到强泛化，即教弱模型有助于强模型改进；此外，教多个多样性学生比教单一学生或自身更有效。该研究为未来LLMs改进提供新思路，并公开了相关代码和网站。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "NeurIPS 2024",
      "pdf_url": "http://arxiv.org/pdf/2406.14629v3",
      "published_date": "2024-06-20 18:00:17 UTC",
      "updated_date": "2024-11-24 02:24:29 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T22:41:36.863609"
    },
    {
      "arxiv_id": "2406.14563v1",
      "title": "Model Merging and Safety Alignment: One Bad Model Spoils the Bunch",
      "title_zh": "翻译失败",
      "authors": [
        "Hasan Abed Al Kader Hammoud",
        "Umberto Michieli",
        "Fabio Pizzati",
        "Philip Torr",
        "Adel Bibi",
        "Bernard Ghanem",
        "Mete Ozay"
      ],
      "abstract": "Merging Large Language Models (LLMs) is a cost-effective technique for\ncombining multiple expert LLMs into a single versatile model, retaining the\nexpertise of the original ones. However, current approaches often overlook the\nimportance of safety alignment during merging, leading to highly misaligned\nmodels. This work investigates the effects of model merging on alignment. We\nevaluate several popular model merging techniques, demonstrating that existing\nmethods do not only transfer domain expertise but also propagate misalignment.\nWe propose a simple two-step approach to address this problem: (i) generating\nsynthetic safety and domain-specific data, and (ii) incorporating these\ngenerated data into the optimization process of existing data-aware model\nmerging techniques. This allows us to treat alignment as a skill that can be\nmaximized in the resulting merged LLM. Our experiments illustrate the\neffectiveness of integrating alignment-related data during merging, resulting\nin models that excel in both domain expertise and alignment.",
      "tldr_zh": "本文研究了合并大型语言模型 (LLMs) 时对安全对齐的影响，发现现有模型合并技术不仅传输领域专业知识，还会传播不安全对齐，导致合并后的模型高度失调。作者评估了几种流行方法，证明这种问题普遍存在。针对此，他们提出一种简单两步方法：生成合成安全数据和领域特定数据，并将其整合到数据感知模型合并的优化过程中，以将安全对齐视为一种可最大化的技能。实验结果表明，这种方法能显著提升合并模型在领域专业知识和安全对齐方面的表现。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "Under review",
      "pdf_url": "http://arxiv.org/pdf/2406.14563v1",
      "published_date": "2024-06-20 17:59:58 UTC",
      "updated_date": "2024-06-20 17:59:58 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T22:41:38.097995"
    },
    {
      "arxiv_id": "2406.14562v1",
      "title": "Whiteboard-of-Thought: Thinking Step-by-Step Across Modalities",
      "title_zh": "白板思维：跨模态的逐步思考",
      "authors": [
        "Sachit Menon",
        "Richard Zemel",
        "Carl Vondrick"
      ],
      "abstract": "When presented with questions involving visual thinking, humans naturally\nswitch reasoning modalities, often forming mental images or drawing visual\naids. Large language models have shown promising results in arithmetic and\nsymbolic reasoning by expressing intermediate reasoning in text as a chain of\nthought, yet struggle to extend this capability to answer text queries that are\neasily solved by visual reasoning, even with extensive multimodal pretraining.\nWe introduce a simple method, whiteboard-of-thought prompting, to unlock the\nvisual reasoning capabilities of multimodal large language models across\nmodalities. Whiteboard-of-thought prompting provides multimodal large language\nmodels with a metaphorical `whiteboard' to draw out reasoning steps as images,\nthen returns these images back to the model for further processing. We find\nthis can be accomplished with no demonstrations or specialized modules, instead\nleveraging models' existing ability to write code with libraries such as\nMatplotlib and Turtle. This simple approach shows state-of-the-art results on\nfour difficult natural language tasks that involve visual and spatial\nreasoning. We identify multiple settings where GPT-4o using chain-of-thought\nfails dramatically, including more than one where it achieves $0\\%$ accuracy,\nwhile whiteboard-of-thought enables up to $92\\%$ accuracy in these same\nsettings. We present a detailed exploration of where the technique succeeds as\nwell as its sources of error.",
      "tldr_zh": "该论文提出了一种名为“whiteboard-of-thought prompting”的简单方法，以提升多模态大型语言模型（multimodal large language models）在视觉和空间推理方面的能力，解决它们在处理文本查询时（如chain-of-thought）的局限性。该方法让模型通过编写代码（如Matplotlib和Turtle库）在虚拟“白板”上绘制推理步骤作为图像，然后将这些图像反馈给模型进行进一步处理，无需演示或专用模块。实验结果显示，该方法在四个涉及视觉推理的自然语言任务上达到了state-of-the-art性能，使GPT-4o在原本失败的场景中准确率从0%提高到92%，并对方法的成功场景和错误来源进行了详细探讨。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.CL",
      "comment": "Project website: whiteboard.cs.columbia.edu/",
      "pdf_url": "http://arxiv.org/pdf/2406.14562v1",
      "published_date": "2024-06-20 17:59:45 UTC",
      "updated_date": "2024-06-20 17:59:45 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T22:41:53.186754"
    },
    {
      "arxiv_id": "2406.14558v3",
      "title": "CooHOI: Learning Cooperative Human-Object Interaction with Manipulated Object Dynamics",
      "title_zh": "翻译失败",
      "authors": [
        "Jiawei Gao",
        "Ziqin Wang",
        "Zeqi Xiao",
        "Jingbo Wang",
        "Tai Wang",
        "Jinkun Cao",
        "Xiaolin Hu",
        "Si Liu",
        "Jifeng Dai",
        "Jiangmiao Pang"
      ],
      "abstract": "Enabling humanoid robots to clean rooms has long been a pursued dream within\nhumanoid research communities. However, many tasks require multi-humanoid\ncollaboration, such as carrying large and heavy furniture together. Given the\nscarcity of motion capture data on multi-humanoid collaboration and the\nefficiency challenges associated with multi-agent learning, these tasks cannot\nbe straightforwardly addressed using training paradigms designed for\nsingle-agent scenarios. In this paper, we introduce Cooperative Human-Object\nInteraction (CooHOI), a framework designed to tackle the challenge of\nmulti-humanoid object transportation problem through a two-phase learning\nparadigm: individual skill learning and subsequent policy transfer. First, a\nsingle humanoid character learns to interact with objects through imitation\nlearning from human motion priors. Then, the humanoid learns to collaborate\nwith others by considering the shared dynamics of the manipulated object using\ncentralized training and decentralized execution (CTDE) multi-agent RL\nalgorithms. When one agent interacts with the object, resulting in specific\nobject dynamics changes, the other agents learn to respond appropriately,\nthereby achieving implicit communication and coordination between teammates.\nUnlike previous approaches that relied on tracking-based methods for\nmulti-humanoid HOI, CooHOI is inherently efficient, does not depend on motion\ncapture data of multi-humanoid interactions, and can be seamlessly extended to\ninclude more participants and a wide range of object types.",
      "tldr_zh": "这篇论文引入了 Cooperative Human-Object Interaction (CooHOI) 框架，用于解决多类人机器人协作任务，如搬运大型家具的问题。框架采用两阶段学习范式：首先通过 imitation learning，让单个机器人从人类动作先验中学习物体交互；随后使用 centralized training and decentralized execution (CTDE) 多代理 RL 算法，使机器人基于共享的物体动态实现隐式通信和协调。与传统依赖动作捕捉数据的追踪方法不同，CooHOI 更高效、可扩展，支持更多参与者和多种物体类型。",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "comment": "Project website: https://gao-jiawei.com/Research/CooHOI/. NeurIPS\n  2024 Spotlight",
      "pdf_url": "http://arxiv.org/pdf/2406.14558v3",
      "published_date": "2024-06-20 17:59:22 UTC",
      "updated_date": "2024-10-30 02:58:10 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T22:42:02.634134"
    },
    {
      "arxiv_id": "2406.14550v2",
      "title": "GraphReader: Building Graph-based Agent to Enhance Long-Context Abilities of Large Language Models",
      "title_zh": "GraphReader：构建基于图的代理以增强大型语言模型的长上下文能力",
      "authors": [
        "Shilong Li",
        "Yancheng He",
        "Hangyu Guo",
        "Xingyuan Bu",
        "Ge Bai",
        "Jie Liu",
        "Jiaheng Liu",
        "Xingwei Qu",
        "Yangguang Li",
        "Wanli Ouyang",
        "Wenbo Su",
        "Bo Zheng"
      ],
      "abstract": "Long-context capabilities are essential for large language models (LLMs) to\ntackle complex and long-input tasks. Despite numerous efforts made to optimize\nLLMs for long contexts, challenges persist in robustly processing long inputs.\nIn this paper, we introduce GraphReader, a graph-based agent system designed to\nhandle long texts by structuring them into a graph and employing an agent to\nexplore this graph autonomously. Upon receiving a question, the agent first\nundertakes a step-by-step analysis and devises a rational plan. It then invokes\na set of predefined functions to read node content and neighbors, facilitating\na coarse-to-fine exploration of the graph. Throughout the exploration, the\nagent continuously records new insights and reflects on current circumstances\nto optimize the process until it has gathered sufficient information to\ngenerate an answer. Experimental results on the LV-Eval dataset reveal that\nGraphReader, using a 4k context window, consistently outperforms GPT-4-128k\nacross context lengths from 16k to 256k by a large margin. Additionally, our\napproach demonstrates superior performance on four challenging single-hop and\nmulti-hop benchmarks.",
      "tldr_zh": "该研究提出GraphReader，一种基于图的代理系统，用于提升Large Language Models (LLMs)的长上下文处理能力，通过将长文本结构化为图并让代理自主探索来解决LLMs在处理复杂长输入时的挑战。系统在接收问题后，进行逐步分析制定计划，并调用预定义函数读取节点内容和邻居，实现从粗到细的探索，同时记录见解并优化过程以生成准确答案。在LV-Eval数据集上的实验显示，使用4k上下文窗口的GraphReader在16k至256k长度范围内大幅优于GPT-4-128k，并在四个单跳和多跳基准测试中表现出色。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "[EMNLP 2024] The first four authors contributed equally, 29 pages",
      "pdf_url": "http://arxiv.org/pdf/2406.14550v2",
      "published_date": "2024-06-20 17:57:51 UTC",
      "updated_date": "2024-11-05 16:51:40 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T22:42:14.052351"
    },
    {
      "arxiv_id": "2406.14598v2",
      "title": "SORRY-Bench: Systematically Evaluating Large Language Model Safety Refusal",
      "title_zh": "SORRY-Bench：系统评估大语言模型的安全拒绝",
      "authors": [
        "Tinghao Xie",
        "Xiangyu Qi",
        "Yi Zeng",
        "Yangsibo Huang",
        "Udari Madhushani Sehwag",
        "Kaixuan Huang",
        "Luxi He",
        "Boyi Wei",
        "Dacheng Li",
        "Ying Sheng",
        "Ruoxi Jia",
        "Bo Li",
        "Kai Li",
        "Danqi Chen",
        "Peter Henderson",
        "Prateek Mittal"
      ],
      "abstract": "Evaluating aligned large language models' (LLMs) ability to recognize and\nreject unsafe user requests is crucial for safe, policy-compliant deployments.\nExisting evaluation efforts, however, face three limitations that we address\nwith SORRY-Bench, our proposed benchmark. First, existing methods often use\ncoarse-grained taxonomies of unsafe topics, and are over-representing some\nfine-grained topics. For example, among the ten existing datasets that we\nevaluated, tests for refusals of self-harm instructions are over 3x less\nrepresented than tests for fraudulent activities. SORRY-Bench improves on this\nby using a fine-grained taxonomy of 44 potentially unsafe topics, and 440\nclass-balanced unsafe instructions, compiled through human-in-the-loop methods.\nSecond, linguistic characteristics and formatting of prompts are often\noverlooked, like different languages, dialects, and more -- which are only\nimplicitly considered in many evaluations. We supplement SORRY-Bench with 20\ndiverse linguistic augmentations to systematically examine these effects.\nThird, existing evaluations rely on large LLMs (e.g., GPT-4) for evaluation,\nwhich can be computationally expensive. We investigate design choices for\ncreating a fast, accurate automated safety evaluator. By collecting 7K+ human\nannotations and conducting a meta-evaluation of diverse LLM-as-a-judge designs,\nwe show that fine-tuned 7B LLMs can achieve accuracy comparable to GPT-4 scale\nLLMs, with lower computational cost. Putting these together, we evaluate over\n50 proprietary and open-weight LLMs on SORRY-Bench, analyzing their distinctive\nsafety refusal behaviors. We hope our effort provides a building block for\nsystematic evaluations of LLMs' safety refusal capabilities, in a balanced,\ngranular, and efficient manner. Benchmark demo, data, code, and models are\navailable through https://sorry-bench.github.io.",
      "tldr_zh": "这篇论文引入了 SORRY-Bench，一个系统性基准，用于评估大型语言模型 (LLMs) 在识别和拒绝不安全用户请求方面的能力，以提升模型的安全性和合规性。SORRY-Bench 通过采用细粒度的 44 个潜在不安全主题和 440 个平衡的 unsafe instructions，并结合人类参与方法和 20 种语言增强（如不同语言和方言），解决了现有评估中主题分布不均和提示多样性忽略的问题。同时，论文探索了高效的自动化评估设计，发现微调的 7B LLMs 可实现与 GPT-4 相当的准确性，但计算成本更低。在 SORRY-Bench 上评估了 50 多个专有和开源 LLMs，分析了它们的 safety refusal 行为，为平衡、细致和高效的 LLMs 安全评估提供了重要基础。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "Paper accepted to ICLR 2025",
      "pdf_url": "http://arxiv.org/pdf/2406.14598v2",
      "published_date": "2024-06-20 17:56:07 UTC",
      "updated_date": "2025-03-01 21:45:36 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T22:42:28.100041"
    },
    {
      "arxiv_id": "2406.14546v3",
      "title": "Connecting the Dots: LLMs can Infer and Verbalize Latent Structure from Disparate Training Data",
      "title_zh": "翻译失败",
      "authors": [
        "Johannes Treutlein",
        "Dami Choi",
        "Jan Betley",
        "Samuel Marks",
        "Cem Anil",
        "Roger Grosse",
        "Owain Evans"
      ],
      "abstract": "One way to address safety risks from large language models (LLMs) is to\ncensor dangerous knowledge from their training data. While this removes the\nexplicit information, implicit information can remain scattered across various\ntraining documents. Could an LLM infer the censored knowledge by piecing\ntogether these implicit hints? As a step towards answering this question, we\nstudy inductive out-of-context reasoning (OOCR), a type of generalization in\nwhich LLMs infer latent information from evidence distributed across training\ndocuments and apply it to downstream tasks without in-context learning. Using a\nsuite of five tasks, we demonstrate that frontier LLMs can perform inductive\nOOCR. In one experiment we finetune an LLM on a corpus consisting only of\ndistances between an unknown city and other known cities. Remarkably, without\nin-context examples or Chain of Thought, the LLM can verbalize that the unknown\ncity is Paris and use this fact to answer downstream questions. Further\nexperiments show that LLMs trained only on individual coin flip outcomes can\nverbalize whether the coin is biased, and those trained only on pairs\n$(x,f(x))$ can articulate a definition of $f$ and compute inverses. While OOCR\nsucceeds in a range of cases, we also show that it is unreliable, particularly\nfor smaller LLMs learning complex structures. Overall, the ability of LLMs to\n\"connect the dots\" without explicit in-context learning poses a potential\nobstacle to monitoring and controlling the knowledge acquired by LLMs.",
      "tldr_zh": "本研究探讨了大型语言模型（LLMs）从分散训练数据中推断和表述潜在结构的能力，即inductive out-of-context reasoning (OOCR)，这可能绕过训练数据的审查机制。研究者通过五个任务的实验套件证明，前沿LLMs能够在没有in-context learning或Chain of Thought的情况下，从隐含线索中推断信息，例如从城市距离数据推断未知城市是Paris，或从硬币翻转结果判断硬币是否偏置。实验结果显示，OOCR在某些场景下成功，但对较小LLMs或复杂结构而言不可靠。总体而言，这种“连接点”的能力对监控和控制LLMs获得的知识构成了潜在障碍。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "Accepted at NeurIPS 2024. 10 pages, 8 figures",
      "pdf_url": "http://arxiv.org/pdf/2406.14546v3",
      "published_date": "2024-06-20 17:55:04 UTC",
      "updated_date": "2024-12-23 12:01:28 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T22:42:38.402725"
    },
    {
      "arxiv_id": "2406.14540v1",
      "title": "IRASim: Learning Interactive Real-Robot Action Simulators",
      "title_zh": "IRASim：学习交互式真实机器人动作模拟器",
      "authors": [
        "Fangqi Zhu",
        "Hongtao Wu",
        "Song Guo",
        "Yuxiao Liu",
        "Chilam Cheang",
        "Tao Kong"
      ],
      "abstract": "Scalable robot learning in the real world is limited by the cost and safety\nissues of real robots. In addition, rolling out robot trajectories in the real\nworld can be time-consuming and labor-intensive. In this paper, we propose to\nlearn an interactive real-robot action simulator as an alternative. We\nintroduce a novel method, IRASim, which leverages the power of generative\nmodels to generate extremely realistic videos of a robot arm that executes a\ngiven action trajectory, starting from an initial given frame. To validate the\neffectiveness of our method, we create a new benchmark, IRASim Benchmark, based\non three real-robot datasets and perform extensive experiments on the\nbenchmark. Results show that IRASim outperforms all the baseline methods and is\nmore preferable in human evaluations. We hope that IRASim can serve as an\neffective and scalable approach to enhance robot learning in the real world. To\npromote research for generative real-robot action simulators, we open-source\ncode, benchmark, and checkpoints at https: //gen-irasim.github.io.",
      "tldr_zh": "该论文提出IRASim，一种利用生成模型(generative models)学习交互式真实机器人动作模拟器的方法，以解决真实机器人学习中的成本、安全和效率问题。IRASim能从初始帧生成机器人臂执行给定动作轨迹的逼真视频，并通过新创建的IRASim Benchmark在三个真实机器人数据集上进行广泛实验。结果显示，IRASim优于所有基线方法，并在人类评估中更受欢迎，有望作为一种可扩展的工具来增强真实世界机器人学习，并已开源代码、基准和检查点。",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.RO",
      "comment": "Opensource, project website: https://gen-irasim.github.io",
      "pdf_url": "http://arxiv.org/pdf/2406.14540v1",
      "published_date": "2024-06-20 17:50:16 UTC",
      "updated_date": "2024-06-20 17:50:16 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T22:42:49.799008"
    },
    {
      "arxiv_id": "2406.14596v5",
      "title": "VLM Agents Generate Their Own Memories: Distilling Experience into Embodied Programs of Thought",
      "title_zh": "翻译失败",
      "authors": [
        "Gabriel Sarch",
        "Lawrence Jang",
        "Michael J. Tarr",
        "William W. Cohen",
        "Kenneth Marino",
        "Katerina Fragkiadaki"
      ],
      "abstract": "Large-scale LLMs and VLMs excel at few-shot learning but require high-quality\nexamples. We introduce In-Context Abstraction Learning (ICAL), which\niteratively refines suboptimal trajectories into high-quality data with\noptimized actions and detailed reasoning. Given an inefficient demonstration, a\nVLM corrects actions and annotates causal relationships, object states,\nsubgoals, and task-relevant visuals, forming \"programs of thought.\" With human\nfeedback, these programs are improved as the agent executes them in a similar\nenvironment. The resulting examples, used as prompt context or fine-tuning\ndata, significantly boost decision-making while reducing human feedback needs.\nICAL surpasses state-of-the-art in TEACh (dialogue-based instruction\nfollowing), VisualWebArena (multimodal web agents), and Ego4D (egocentric video\naction anticipation). In TEACh, combining fine-tuning and retrieval on ICAL\nexamples outperforms raw human demonstrations and expert examples, achieving a\n17.5% increase in goal-condition success. In VisualWebArena,\nretrieval-augmented GPT-4V with ICAL improves task success rate 1.6x over\nGPT-4V, while fine-tuning Qwen2-VL achieves a 2.8x improvement. In Ego4D, ICAL\noutperforms few-shot GPT-4V and remains competitive with supervised models.\nOverall, ICAL scales 2x better than raw human demonstrations and reduces manual\nprompt engineering.",
      "tldr_zh": "本研究引入 In-Context Abstraction Learning (ICAL)，一种方法让视觉语言模型 (VLM) 代理通过迭代提炼次优轨迹，生成高质量数据和“programs of thought”，包括优化动作、标注因果关系、对象状态、子目标及任务相关视觉。ICAL 结合人类反馈在类似环境中执行这些程序，用于提示上下文或微调数据，从而提升决策能力并减少反馈需求。在实验中，ICAL 在 TEACh 上实现 17.5% 的成功率提升，在 VisualWebArena 上使任务成功率提高 1.6x 到 2.8x，在 Ego4D 上与监督模型竞争，并整体比原始人类演示提升 2x 效率，减少手动提示工程。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "Project website: https://ical-learning.github.io/",
      "pdf_url": "http://arxiv.org/pdf/2406.14596v5",
      "published_date": "2024-06-20 17:45:02 UTC",
      "updated_date": "2025-01-20 23:33:33 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T22:43:04.151099"
    },
    {
      "arxiv_id": "2406.14595v2",
      "title": "Adversaries Can Misuse Combinations of Safe Models",
      "title_zh": "攻击者可以滥用安全的模型组合",
      "authors": [
        "Erik Jones",
        "Anca Dragan",
        "Jacob Steinhardt"
      ],
      "abstract": "Developers try to evaluate whether an AI system can be misused by adversaries\nbefore releasing it; for example, they might test whether a model enables\ncyberoffense, user manipulation, or bioterrorism. In this work, we show that\nindividually testing models for misuse is inadequate; adversaries can misuse\ncombinations of models even when each individual model is safe. The adversary\naccomplishes this by first decomposing tasks into subtasks, then solving each\nsubtask with the best-suited model. For example, an adversary might solve\nchallenging-but-benign subtasks with an aligned frontier model, and\neasy-but-malicious subtasks with a weaker misaligned model. We study two\ndecomposition methods: manual decomposition where a human identifies a natural\ndecomposition of a task, and automated decomposition where a weak model\ngenerates benign tasks for a frontier model to solve, then uses the solutions\nin-context to solve the original task. Using these decompositions, we\nempirically show that adversaries can create vulnerable code, explicit images,\npython scripts for hacking, and manipulative tweets at much higher rates with\ncombinations of models than either individual model. Our work suggests that\neven perfectly-aligned frontier systems can enable misuse without ever\nproducing malicious outputs, and that red-teaming efforts should extend beyond\nsingle models in isolation.",
      "tldr_zh": "本研究揭示了攻击者(adversaries)可以通过组合多个安全AI模型进行滥用，即使每个模型单独测试时是安全的。攻击者将任务分解为子任务，然后使用最合适的模型解决，例如用对齐的前沿模型处理良性子任务，用弱失调模型处理恶意子任务。实验采用手动和自动化分解方法，证明这种组合能显著提高创建易受攻击代码、显式图像、黑客脚本和操纵性推文的成功率。作者强调，即使前沿模型完全对齐，也可能通过组合启用滥用，因此红-teaming努力应扩展到多个模型的互动测试。",
      "categories": [
        "cs.CR",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CR",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2406.14595v2",
      "published_date": "2024-06-20 17:43:18 UTC",
      "updated_date": "2024-07-01 19:58:00 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T22:43:16.201189"
    },
    {
      "arxiv_id": "2406.14529v1",
      "title": "A Benchmarking Study of Kolmogorov-Arnold Networks on Tabular Data",
      "title_zh": "Kolmogorov-Arnold Networks 在表格数据上的基准研究",
      "authors": [
        "Eleonora Poeta",
        "Flavio Giobergia",
        "Eliana Pastor",
        "Tania Cerquitelli",
        "Elena Baralis"
      ],
      "abstract": "Kolmogorov-Arnold Networks (KANs) have very recently been introduced into the\nworld of machine learning, quickly capturing the attention of the entire\ncommunity. However, KANs have mostly been tested for approximating complex\nfunctions or processing synthetic data, while a test on real-world tabular\ndatasets is currently lacking. In this paper, we present a benchmarking study\ncomparing KANs and Multi-Layer Perceptrons (MLPs) on tabular datasets. The\nstudy evaluates task performance and training times. From the results obtained\non the various datasets, KANs demonstrate superior or comparable accuracy and\nF1 scores, excelling particularly in datasets with numerous instances,\nsuggesting robust handling of complex data. We also highlight that this\nperformance improvement of KANs comes with a higher computational cost when\ncompared to MLPs of comparable sizes.",
      "tldr_zh": "本研究对 Kolmogorov-Arnold Networks (KANs) 在真实世界表格数据集上的性能进行了基准测试，与 Multi-Layer Perceptrons (MLPs) 进行了比较。研究评估了任务性能（如准确率和 F1 scores）和训练时间，结果显示 KANs 在许多数据集上表现出优越或相当的表现，尤其在实例数量多的复杂数据中。KANs 的性能提升伴随着更高的计算成本，这为选择合适模型提供了参考。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2406.14529v1",
      "published_date": "2024-06-20 17:41:34 UTC",
      "updated_date": "2024-06-20 17:41:34 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T22:43:26.544233"
    },
    {
      "arxiv_id": "2406.14528v3",
      "title": "DeciMamba: Exploring the Length Extrapolation Potential of Mamba",
      "title_zh": "翻译失败",
      "authors": [
        "Assaf Ben-Kish",
        "Itamar Zimerman",
        "Shady Abu-Hussein",
        "Nadav Cohen",
        "Amir Globerson",
        "Lior Wolf",
        "Raja Giryes"
      ],
      "abstract": "Long-range sequence processing poses a significant challenge for Transformers\ndue to their quadratic complexity in input length. A promising alternative is\nMamba, which demonstrates high performance and achieves Transformer-level\ncapabilities while requiring substantially fewer computational resources. In\nthis paper we explore the length-generalization capabilities of Mamba, which we\nfind to be relatively limited. Through a series of visualizations and analyses\nwe identify that the limitations arise from a restricted effective receptive\nfield, dictated by the sequence length used during training. To address this\nconstraint, we introduce DeciMamba, a context-extension method specifically\ndesigned for Mamba. This mechanism, built on top of a hidden filtering\nmechanism embedded within the S6 layer, enables the trained model to\nextrapolate well even without additional training. Empirical experiments over\nreal-world long-range NLP tasks show that DeciMamba can extrapolate to context\nlengths that are significantly longer than the ones seen during training, while\nenjoying faster inference.",
      "tldr_zh": "Transformer 模型在处理长序列时因二次复杂度而面临挑战，而 Mamba 作为高效替代方案虽提供高性能，但其长度外推能力有限，主要受训练时序列长度的有效感受野(receptive field)制约。研究通过可视化和分析识别了这一问题，并提出 DeciMamba，一种基于 S6 layer 中隐藏过滤机制的上下文扩展方法。该方法无需额外训练即可显著提升模型的长度泛化能力。在真实长序列 NLP 任务的实验中，DeciMamba 成功处理远超训练长度的上下文，同时实现更快推理。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "Official Implementation: https://github.com/assafbk/DeciMamba",
      "pdf_url": "http://arxiv.org/pdf/2406.14528v3",
      "published_date": "2024-06-20 17:40:18 UTC",
      "updated_date": "2025-04-09 22:43:46 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T22:43:39.201897"
    },
    {
      "arxiv_id": "2406.14525v1",
      "title": "Towards evolution of Deep Neural Networks through contrastive Self-Supervised learning",
      "title_zh": "翻译失败",
      "authors": [
        "Adriano Vinhas",
        "João Correia",
        "Penousal Machado"
      ],
      "abstract": "Deep Neural Networks (DNNs) have been successfully applied to a wide range of\nproblems. However, two main limitations are commonly pointed out. The first one\nis that they require long time to design. The other is that they heavily rely\non labelled data, which can sometimes be costly and hard to obtain. In order to\naddress the first problem, neuroevolution has been proved to be a plausible\noption to automate the design of DNNs. As for the second problem,\nself-supervised learning has been used to leverage unlabelled data to learn\nrepresentations. Our goal is to study how neuroevolution can help\nself-supervised learning to bridge the gap to supervised learning in terms of\nperformance. In this work, we propose a framework that is able to evolve deep\nneural networks using self-supervised learning. Our results on the CIFAR-10\ndataset show that it is possible to evolve adequate neural networks while\nreducing the reliance on labelled data. Moreover, an analysis to the structure\nof the evolved networks suggests that the amount of labelled data fed to them\nhas less effect on the structure of networks that learned via self-supervised\nlearning, when compared to individuals that relied on supervised learning.",
      "tldr_zh": "这篇论文探讨了Deep Neural Networks (DNNs) 的两大问题：设计过程耗时长和对标注数据的过度依赖。作者提出一个框架，将neuroevolution 与self-supervised learning 相结合，通过进化算法自动设计DNNs，以提升无监督学习在性能上的表现。实验在CIFAR-10 数据集上显示，该框架能进化出高效的神经网络，显著减少对标注数据的需求，同时使网络结构较少受标注数据量的影响。总的来说，该方法为桥接self-supervised learning 与supervised learning 的差距提供了新途径。",
      "categories": [
        "cs.NE",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.NE",
      "comment": "IEEE World Congress on Computational Intelligence (WCCI) 2024;\n  Keywords: NeuroEvolution, Deep Learning, Evolutionary Machine Learning",
      "pdf_url": "http://arxiv.org/pdf/2406.14525v1",
      "published_date": "2024-06-20 17:38:16 UTC",
      "updated_date": "2024-06-20 17:38:16 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T22:43:51.352740"
    },
    {
      "arxiv_id": "2406.14526v2",
      "title": "Fantastic Copyrighted Beasts and How (Not) to Generate Them",
      "title_zh": "翻译失败",
      "authors": [
        "Luxi He",
        "Yangsibo Huang",
        "Weijia Shi",
        "Tinghao Xie",
        "Haotian Liu",
        "Yue Wang",
        "Luke Zettlemoyer",
        "Chiyuan Zhang",
        "Danqi Chen",
        "Peter Henderson"
      ],
      "abstract": "Recent studies show that image and video generation models can be prompted to\nreproduce copyrighted content from their training data, raising serious legal\nconcerns about copyright infringement. Copyrighted characters (e.g., Mario,\nBatman) present a significant challenge: at least one lawsuit has already\nawarded damages based on the generation of such characters. Consequently,\ncommercial services like DALL-E have started deploying interventions. However,\nlittle research has systematically examined these problems: (1) Can users\neasily prompt models to generate copyrighted characters, even if it is\nunintentional?; (2) How effective are the existing mitigation strategies? To\naddress these questions, we introduce a novel evaluation framework with metrics\nthat assess both the generated image's similarity to copyrighted characters and\nits consistency with user intent, grounded in a set of popular copyrighted\ncharacters from diverse studios and regions. We show that state-of-the-art\nimage and video generation models can still generate characters even if\ncharacters' names are not explicitly mentioned, sometimes with only two generic\nkeywords (e.g., prompting with \"videogame, plumber\" consistently generates\nNintendo's Mario character). We also introduce semi-automatic techniques to\nidentify such keywords or descriptions that trigger character generation. Using\nthis framework, we evaluate mitigation strategies, including prompt rewriting\nand new approaches we propose. Our findings reveal that common methods, such as\nDALL-E's prompt rewriting, are insufficient alone and require supplementary\nstrategies like negative prompting. Our work provides empirical grounding for\ndiscussions on copyright mitigation strategies and offers actionable insights\nfor model deployers implementing these safeguards.",
      "tldr_zh": "本研究探讨图像和视频生成模型（如 DALL-E）在生成版权角色（如 Mario 或 Batman）时可能导致的版权侵权问题，揭示用户即使无意中使用泛化关键词（如“videogame, plumber”）也能轻易触发模型复制这些角色。研究引入一个新型评估框架，包括度量生成图像的相似度和用户意图一致性的指标，以及半自动技术来识别触发关键字。实验结果显示，现有的缓解策略如提示重写单独使用效果有限，需要结合负面提示等补充方法，以提供更有效的版权保护和行动性见解。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CY",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2406.14526v2",
      "published_date": "2024-06-20 17:38:16 UTC",
      "updated_date": "2025-03-26 12:21:42 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T22:44:03.801009"
    },
    {
      "arxiv_id": "2406.14517v2",
      "title": "PostMark: A Robust Blackbox Watermark for Large Language Models",
      "title_zh": "翻译失败",
      "authors": [
        "Yapei Chang",
        "Kalpesh Krishna",
        "Amir Houmansadr",
        "John Wieting",
        "Mohit Iyyer"
      ],
      "abstract": "The most effective techniques to detect LLM-generated text rely on inserting\na detectable signature -- or watermark -- during the model's decoding process.\nMost existing watermarking methods require access to the underlying LLM's\nlogits, which LLM API providers are loath to share due to fears of model\ndistillation. As such, these watermarks must be implemented independently by\neach LLM provider. In this paper, we develop PostMark, a modular post-hoc\nwatermarking procedure in which an input-dependent set of words (determined via\na semantic embedding) is inserted into the text after the decoding process has\ncompleted. Critically, PostMark does not require logit access, which means it\ncan be implemented by a third party. We also show that PostMark is more robust\nto paraphrasing attacks than existing watermarking methods: our experiments\ncover eight baseline algorithms, five base LLMs, and three datasets. Finally,\nwe evaluate the impact of PostMark on text quality using both automated and\nhuman assessments, highlighting the trade-off between quality and robustness to\nparaphrasing. We release our code, outputs, and annotations at\nhttps://github.com/lilakk/PostMark.",
      "tldr_zh": "本论文提出PostMark，一种鲁棒的blackbox watermarking 方法，用于Large Language Models (LLMs)，无需访问模型的logits，从而允许第三方实现。PostMark通过语义embedding 确定输入相关的单词，并在解码过程完成后插入这些单词，以增强水印的鲁棒性。实验结果显示，与八个baseline算法相比，PostMark在五种base LLMs和三个datasets上对paraphrasing attacks 的抵抗力更强，但也暴露了文本质量与鲁棒性之间的trade-off。作者提供了代码和资源，以促进进一步研究。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL",
        "cs.CR"
      ],
      "primary_category": "cs.LG",
      "comment": "EMNLP 2024; 19 pages, 5 figures",
      "pdf_url": "http://arxiv.org/pdf/2406.14517v2",
      "published_date": "2024-06-20 17:27:14 UTC",
      "updated_date": "2024-10-11 16:19:55 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T22:44:15.840846"
    },
    {
      "arxiv_id": "2406.14514v2",
      "title": "Solving a Stackelberg Game on Transportation Networks in a Dynamic Crime Scenario: A Mixed Approach on Multi-Layer Networks",
      "title_zh": "翻译失败",
      "authors": [
        "Sukanya Samanta",
        "Kei Kimura",
        "Makoto Yokoo"
      ],
      "abstract": "Interdicting a criminal with limited police resources is a challenging task\nas the criminal changes location over time. The size of the large\ntransportation network further adds to the difficulty of this scenario. To\ntackle this issue, we consider the concept of a layered graph. At each time\nstamp, we create a copy of the entire transportation network to track the\npossible movements of both players, the attacker and the defenders. We consider\na Stackelberg game in a dynamic crime scenario where the attacker changes\nlocation over time while the defenders attempt to interdict the attacker on his\nescape route. Given a set of defender strategies, the optimal attacker strategy\nis determined by applying Dijkstra's algorithm on the layered networks. Here,\nthe attacker aims to minimize while the defenders aim to maximize the\nprobability of interdiction. We develop an approximation algorithm on the\nlayered networks to find near-optimal strategy for defenders. The efficacy of\nthe developed approach is compared with the adopted MILP approach. We compare\nthe results in terms of computational time and solution quality. The quality of\nthe results demonstrates the need for the developed approach, as it effectively\nsolves the complex problem within a short amount of time.",
      "tldr_zh": "这篇论文解决了在动态犯罪场景中使用有限警察资源拦截移动罪犯的问题，提出了一种基于多层网络（multi-layer networks）的混合方法来解决 Stackelberg 游戏。方法包括创建分层图（layered graph）来追踪攻击者和防御者的移动，并应用 Dijkstra's algorithm 为攻击者计算最小化拦截概率的最优策略，同时为防御者开发一个近似算法（approximation algorithm）以最大化拦截概率。实验结果显示，该方法与 MILP 基准相比，在计算时间上显著缩短，同时保持了高质量的解决方案，证明了其在复杂交通网络中的实用性。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2406.14514v2",
      "published_date": "2024-06-20 17:24:13 UTC",
      "updated_date": "2024-10-23 07:05:18 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T22:44:27.905966"
    },
    {
      "arxiv_id": "2406.14510v2",
      "title": "V-LASIK: Consistent Glasses-Removal from Videos Using Synthetic Data",
      "title_zh": "V-LASIK：利用合成数据从视频中实现一致眼镜移除",
      "authors": [
        "Rotem Shalev-Arkushin",
        "Aharon Azulay",
        "Tavi Halperin",
        "Eitan Richardson",
        "Amit H. Bermano",
        "Ohad Fried"
      ],
      "abstract": "Diffusion-based generative models have recently shown remarkable image and\nvideo editing capabilities. However, local video editing, particularly removal\nof small attributes like glasses, remains a challenge. Existing methods either\nalter the videos excessively, generate unrealistic artifacts, or fail to\nperform the requested edit consistently throughout the video. In this work, we\nfocus on consistent and identity-preserving removal of glasses in videos, using\nit as a case study for consistent local attribute removal in videos. Due to the\nlack of paired data, we adopt a weakly supervised approach and generate\nsynthetic imperfect data, using an adjusted pretrained diffusion model. We show\nthat despite data imperfection, by learning from our generated data and\nleveraging the prior of pretrained diffusion models, our model is able to\nperform the desired edit consistently while preserving the original video\ncontent. Furthermore, we exemplify the generalization ability of our method to\nother local video editing tasks by applying it successfully to facial\nsticker-removal. Our approach demonstrates significant improvement over\nexisting methods, showcasing the potential of leveraging synthetic data and\nstrong video priors for local video editing tasks.",
      "tldr_zh": "本文提出 V-LASIK，一种基于合成数据的框架，用于实现视频中眼镜的一致去除，同时保持身份和视频内容的完整性。该方法采用弱监督学习，通过调整预训练扩散模型生成不完美合成数据，并利用其先验知识来处理本地属性编辑的挑战。实验结果表明，V-LASIK 在眼镜去除任务上显著优于现有方法，并成功泛化到其他任务如面部贴纸去除，展示了合成数据和强视频先验在本地视频编辑中的潜力。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.GR"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2406.14510v2",
      "published_date": "2024-06-20 17:14:43 UTC",
      "updated_date": "2025-04-14 08:10:45 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T22:44:39.211766"
    },
    {
      "arxiv_id": "2406.14508v1",
      "title": "Evidence of a log scaling law for political persuasion with large language models",
      "title_zh": "翻译失败",
      "authors": [
        "Kobi Hackenburg",
        "Ben M. Tappin",
        "Paul Röttger",
        "Scott Hale",
        "Jonathan Bright",
        "Helen Margetts"
      ],
      "abstract": "Large language models can now generate political messages as persuasive as\nthose written by humans, raising concerns about how far this persuasiveness may\ncontinue to increase with model size. Here, we generate 720 persuasive messages\non 10 U.S. political issues from 24 language models spanning several orders of\nmagnitude in size. We then deploy these messages in a large-scale randomized\nsurvey experiment (N = 25,982) to estimate the persuasive capability of each\nmodel. Our findings are twofold. First, we find evidence of a log scaling law:\nmodel persuasiveness is characterized by sharply diminishing returns, such that\ncurrent frontier models are barely more persuasive than models smaller in size\nby an order of magnitude or more. Second, mere task completion (coherence,\nstaying on topic) appears to account for larger models' persuasive advantage.\nThese findings suggest that further scaling model size will not much increase\nthe persuasiveness of static LLM-generated messages.",
      "tldr_zh": "本研究调查了大语言模型（LLMs）在政治说服方面的表现，通过生成720条关于10个美国政治议题的说服性消息，并使用24个不同规模的模型进行测试。结果显示，模型说服力遵循一个对数缩放定律（log scaling law），即说服效果呈现急剧递减回报，前沿模型仅比小一个数量级的模型略微更具说服力。进一步分析发现，这种优势主要源于任务完成能力（如消息连贯性和主题相关性），而非其他高级特性；因此，进一步扩大模型规模不会显著提升LLM生成静态说服消息的效果。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.CY",
        "cs.HC"
      ],
      "primary_category": "cs.CL",
      "comment": "16 pages, 4 figures",
      "pdf_url": "http://arxiv.org/pdf/2406.14508v1",
      "published_date": "2024-06-20 17:12:38 UTC",
      "updated_date": "2024-06-20 17:12:38 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T22:44:50.119762"
    },
    {
      "arxiv_id": "2406.14507v2",
      "title": "On Newton's Method to Unlearn Neural Networks",
      "title_zh": "翻译失败",
      "authors": [
        "Nhung Bui",
        "Xinyang Lu",
        "Rachael Hwee Ling Sim",
        "See-Kiong Ng",
        "Bryan Kian Hsiang Low"
      ],
      "abstract": "With the widespread applications of neural networks (NNs) trained on personal\ndata, machine unlearning has become increasingly important for enabling\nindividuals to exercise their personal data ownership, particularly the \"right\nto be forgotten\" from trained NNs. Since retraining is computationally\nexpensive, we seek approximate unlearning algorithms for NNs that return\nidentical models to the retrained oracle. While Newton's method has been\nsuccessfully used to approximately unlearn linear models, we observe that\nadapting it for NN is challenging due to degenerate Hessians that make\ncomputing Newton's update impossible. Additionally, we show that when coupled\nwith popular techniques to resolve the degeneracy, Newton's method often incurs\noffensively large norm updates and empirically degrades model performance\npost-unlearning. To address these challenges, we propose CureNewton's method, a\nprinciple approach that leverages cubic regularization to handle the Hessian\ndegeneracy effectively. The added regularizer eliminates the need for manual\nfinetuning and affords a natural interpretation within the unlearning context.\nExperiments across different models and datasets show that our method can\nachieve competitive unlearning performance to the state-of-the-art algorithm in\npractical unlearning settings, while being theoretically justified and\nefficient in running time.",
      "tldr_zh": "这项研究探讨了使用 Newton's method 实现神经网络 (NNs) 的机器遗忘 (machine unlearning)，以支持个人数据所有权和“被遗忘权”，但发现直接应用时会因 Hessian 矩阵退化而导致更新过大并降低模型性能。作者提出 CureNewton's method，通过引入 cubic regularization 来有效处理 Hessian 退化问题，避免手动微调，并提供理论上的合理性。实验结果显示，该方法在不同模型和数据集上，与最先进算法相比，实现了竞争性的遗忘性能，同时保持高效的运行时间。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2406.14507v2",
      "published_date": "2024-06-20 17:12:20 UTC",
      "updated_date": "2024-08-27 17:19:20 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T22:45:04.358032"
    },
    {
      "arxiv_id": "2406.14485v8",
      "title": "Proceedings of The second international workshop on eXplainable AI for the Arts (XAIxArts)",
      "title_zh": "翻译失败",
      "authors": [
        "Nick Bryan-Kinns",
        "Corey Ford",
        "Shuoyang Zheng",
        "Helen Kennedy",
        "Alan Chamberlain",
        "Makayla Lewis",
        "Drew Hemment",
        "Zijin Li",
        "Qiong Wu",
        "Lanxi Xiao",
        "Gus Xia",
        "Jeba Rezwana",
        "Michael Clemens",
        "Gabriel Vigliensoni"
      ],
      "abstract": "This second international workshop on explainable AI for the Arts (XAIxArts)\nbrought together a community of researchers in HCI, Interaction Design, AI,\nexplainable AI (XAI), and digital arts to explore the role of XAI for the Arts.\nWorkshop held at the 16th ACM Conference on Creativity and Cognition (C&C\n2024), Chicago, USA.",
      "tldr_zh": "第二届国际研讨会XAIxArts聚焦于可解释AI (XAI) 在艺术领域的应用，汇集了HCI、Interaction Design、AI、XAI和digital arts 的研究者，探讨XAI如何提升艺术创新与理解。研讨会旨在促进跨学科交流，并于2024年在芝加哥举行的ACM Conference on Creativity and Cognition (C&C 2024) 上举办。该活动加强了研究社区的合作，推动XAI在艺术实践中的潜在影响。",
      "categories": [
        "cs.AI",
        "cs.HC",
        "cs.MM",
        "cs.SD",
        "eess.AS"
      ],
      "primary_category": "cs.AI",
      "comment": "Proceedings of The second international workshop on eXplainable AI\n  for the Arts (XAIxArts)",
      "pdf_url": "http://arxiv.org/pdf/2406.14485v8",
      "published_date": "2024-06-20 16:48:14 UTC",
      "updated_date": "2024-10-21 15:24:04 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T22:45:15.624843"
    },
    {
      "arxiv_id": "2406.14481v1",
      "title": "Revealing Vision-Language Integration in the Brain with Multimodal Networks",
      "title_zh": "通过多模态网络揭示大脑中的视觉-语言整合",
      "authors": [
        "Vighnesh Subramaniam",
        "Colin Conwell",
        "Christopher Wang",
        "Gabriel Kreiman",
        "Boris Katz",
        "Ignacio Cases",
        "Andrei Barbu"
      ],
      "abstract": "We use (multi)modal deep neural networks (DNNs) to probe for sites of\nmultimodal integration in the human brain by predicting stereoencephalography\n(SEEG) recordings taken while human subjects watched movies. We operationalize\nsites of multimodal integration as regions where a multimodal vision-language\nmodel predicts recordings better than unimodal language, unimodal vision, or\nlinearly-integrated language-vision models. Our target DNN models span\ndifferent architectures (e.g., convolutional networks and transformers) and\nmultimodal training techniques (e.g., cross-attention and contrastive\nlearning). As a key enabling step, we first demonstrate that trained vision and\nlanguage models systematically outperform their randomly initialized\ncounterparts in their ability to predict SEEG signals. We then compare unimodal\nand multimodal models against one another. Because our target DNN models often\nhave different architectures, number of parameters, and training sets (possibly\nobscuring those differences attributable to integration), we carry out a\ncontrolled comparison of two models (SLIP and SimCLR), which keep all of these\nattributes the same aside from input modality. Using this approach, we identify\na sizable number of neural sites (on average 141 out of 1090 total sites or\n12.94%) and brain regions where multimodal integration seems to occur.\nAdditionally, we find that among the variants of multimodal training techniques\nwe assess, CLIP-style training is the best suited for downstream prediction of\nthe neural activity in these sites.",
      "tldr_zh": "本研究利用多模态深度神经网络 (DNNs) 通过预测立体脑电图 (SEEG) 记录，揭示人类大脑中视觉-语言整合的区域。具体方法包括将多模态视觉-语言模型与单模态模型（如视觉或语言模型）或线性整合模型进行比较，并通过控制实验（如比较 SLIP 和 SimCLR）消除架构和参数差异的影响。结果显示，多模态模型在平均 12.94% 的神经位点（约 141 个）表现出显著优势，表明这些区域存在多模态整合；此外，CLIP-style 训练被证明最适合预测这些脑区的神经活动。总的来说，该工作为理解大脑处理多模态信息提供了新见解。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.NE",
        "q-bio.NC"
      ],
      "primary_category": "cs.LG",
      "comment": "ICML 2024; 23 pages, 11 figures",
      "pdf_url": "http://arxiv.org/pdf/2406.14481v1",
      "published_date": "2024-06-20 16:43:22 UTC",
      "updated_date": "2024-06-20 16:43:22 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T22:45:28.272125"
    },
    {
      "arxiv_id": "2406.14479v2",
      "title": "Tracing Representation Progression: Analyzing and Enhancing Layer-Wise Similarity",
      "title_zh": "追踪表示进展：分析和增强层级相似性",
      "authors": [
        "Jiachen Jiang",
        "Jinxin Zhou",
        "Zhihui Zhu"
      ],
      "abstract": "Analyzing the similarity of internal representations has been an important\ntechnique for understanding the behavior of deep neural networks. Most existing\nmethods for analyzing the similarity between representations of high\ndimensions, such as those based on Centered Kernel Alignment (CKA), rely on\nstatistical properties of the representations for a set of data points. In this\npaper, we focus on transformer models and study the similarity of\nrepresentations between the hidden layers of individual transformers. In this\ncontext, we show that a simple sample-wise cosine similarity metric is capable\nof capturing the similarity and aligns with the complicated CKA. Our\nexperimental results on common transformers reveal that representations across\nlayers are positively correlated, with similarity increasing when layers get\ncloser. We provide a theoretical justification for this phenomenon under the\ngeodesic curve assumption for the learned transformer. We then show that an\nincrease in representation similarity implies an increase in predicted\nprobability when directly applying the last-layer classifier to any hidden\nlayer representation. We then propose an aligned training method to improve the\neffectiveness of shallow layer by enhancing the similarity between internal\nrepresentations, with trained models that enjoy the following properties: (1)\nmore early saturation events, (2) layer-wise accuracies monotonically increase\nand reveal the minimal depth needed for the given task, (3) when served as\nmulti-exit models, they achieve on-par performance with standard multi-exit\narchitectures which consist of additional classifiers designed for early\nexiting in shallow layers. To our knowledge, our work is the first to show that\none common classifier is sufficient for multi-exit models. We conduct\nexperiments on both vision and NLP tasks to demonstrate the performance of the\nproposed aligned training.",
      "tldr_zh": "本研究分析了Transformer模型隐藏层间表示的相似性，发现简单的样本级余弦相似度能有效捕捉层间相关性，并与复杂方法如Centered Kernel Alignment (CKA)一致。实验和理论证明，层间表示正相关，且相似性增加时，使用最后一层分类器的预测概率也会提升。作者提出一种对齐训练方法，通过增强内部表示相似性，提高浅层有效性，使模型实现更多早期饱和事件、单调增加的层级准确率，并作为multi-exit models时，仅需一个公共分类器即可达到与标准架构相当的性能。在视觉和NLP任务上，实验验证了该方法的有效性。",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.CV",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2406.14479v2",
      "published_date": "2024-06-20 16:41:09 UTC",
      "updated_date": "2025-02-01 19:29:19 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T22:45:39.855787"
    },
    {
      "arxiv_id": "2406.14477v1",
      "title": "SafeSora: Towards Safety Alignment of Text2Video Generation via a Human Preference Dataset",
      "title_zh": "翻译失败",
      "authors": [
        "Josef Dai",
        "Tianle Chen",
        "Xuyao Wang",
        "Ziran Yang",
        "Taiye Chen",
        "Jiaming Ji",
        "Yaodong Yang"
      ],
      "abstract": "To mitigate the risk of harmful outputs from large vision models (LVMs), we\nintroduce the SafeSora dataset to promote research on aligning text-to-video\ngeneration with human values. This dataset encompasses human preferences in\ntext-to-video generation tasks along two primary dimensions: helpfulness and\nharmlessness. To capture in-depth human preferences and facilitate structured\nreasoning by crowdworkers, we subdivide helpfulness into 4 sub-dimensions and\nharmlessness into 12 sub-categories, serving as the basis for pilot\nannotations. The SafeSora dataset includes 14,711 unique prompts, 57,333 unique\nvideos generated by 4 distinct LVMs, and 51,691 pairs of preference annotations\nlabeled by humans. We further demonstrate the utility of the SafeSora dataset\nthrough several applications, including training the text-video moderation\nmodel and aligning LVMs with human preference by fine-tuning a prompt\naugmentation module or the diffusion model. These applications highlight its\npotential as the foundation for text-to-video alignment research, such as human\npreference modeling and the development and validation of alignment algorithms.",
      "tldr_zh": "本研究引入了SafeSora数据集，旨在通过人类偏好数据实现文本到视频（Text2Video）生成的safety alignment，缓解大型视觉模型（LVMs）的有害输出风险。该数据集涵盖了人类偏好在helpfulness（细分为4个子维度）和harmlessness（细分为12个子类别）方面的信息，包括14,711个独特提示、57,333个独特视频（由4个不同LVMs生成）和51,691对偏好标注。研究展示了数据集的应用，例如训练文本-视频审核模型，以及通过微调提示增强模块或扩散模型来使LVMs与人类偏好对齐，从而为文本到视频对齐研究奠定基础。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.DB"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2406.14477v1",
      "published_date": "2024-06-20 16:38:56 UTC",
      "updated_date": "2024-06-20 16:38:56 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T22:45:51.812309"
    },
    {
      "arxiv_id": "2406.14476v2",
      "title": "Learning telic-controllable state representations",
      "title_zh": "翻译失败",
      "authors": [
        "Nadav Amir",
        "Stas Tiomkin",
        "Angela Langdon"
      ],
      "abstract": "Computational descriptions of purposeful behavior comprise both descriptive\nand normative} aspects. The former are used to ascertain current (or future)\nstates of the world and the latter to evaluate the desirability, or lack\nthereof, of these states under some goal. In Reinforcement Learning, the\nnormative aspect (reward and value functions) is assumed to depend on a\npredefined and fixed descriptive one (state representation). Alternatively,\nthese two aspects may emerge interdependently: goals can be, and indeed often\nare, approximated by state-dependent reward functions, but they may also shape\nthe acquired state representations themselves. Here, we present a novel\ncomputational framework for state representation learning in bounded agents,\nwhere descriptive and normative aspects are coupled through the notion of\ngoal-directed, or telic, states. We introduce the concept of telic\ncontrollability to characterize the tradeoff between the granularity of a telic\nstate representation and the policy complexity required to reach all telic\nstates. We propose an algorithm for learning controllable state\nrepresentations, illustrating it using a simple navigation task with shifting\ngoals. Our framework highlights the crucial role of deliberate ignorance --\nknowing which features of experience to ignore -- for learning state\nrepresentations that balance goal flexibility and policy complexity. More\nbroadly, our work advances a unified theoretical perspective on goal-directed\nstate representation learning in natural and artificial agents.",
      "tldr_zh": "本论文探讨了强化学习(Reinforcement Learning)中状态表示的描述性和规范性方面，提出了一种新框架，将两者通过目标导向的 telic states 耦合，以适应动态目标。论文引入 telic controllability 概念，用于平衡 telic 状态表示的粒度和策略复杂度，并开发了一种算法，在简单导航任务中进行演示，展示了 deliberate ignorance 在优化目标灵活性和策略复杂度方面的关键作用。该框架为自然和人工代理的目标导向状态表示学习提供了一个统一的理论视角。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "Finding the Frame: Workshop for Examining Conceptual Frameworks in\n  RL. 2024 Reinforcement Learning Conference, Amherst MA",
      "pdf_url": "http://arxiv.org/pdf/2406.14476v2",
      "published_date": "2024-06-20 16:38:25 UTC",
      "updated_date": "2024-07-16 23:20:17 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T22:46:03.386053"
    },
    {
      "arxiv_id": "2406.14469v8",
      "title": "Movement-Prediction-Adjusted Naïve Forecast",
      "title_zh": "翻译失败",
      "authors": [
        "Cheng Zhang"
      ],
      "abstract": "This study introduces a movement-prediction-adjusted na\\\"ive forecast for\ntime series exhibiting symmetric random walk characteristics, which is\napplicable after accurate movement predictions are available. Specifically, the\noriginal na\\\"ive forecast is adjusted by a weighted movement prediction term,\nwhere the weights are determined via two parameters derived from the in-sample\ndata: one based on directional accuracy of the movement prediction and the\nother on the mean absolute increment of the target series. Simulation\nexperiments were conducted across four types of synthetic symmetric random walk\nseries, each with different variance structures. For each time series, diverse\nmovement predictions with predefined directional accuracies were randomly\ngenerated, and the resulting forecasts were evaluated via the RMSE, MAE, MAPE,\nand sMAPE metrics. The results demonstrated a clear monotonic improvement in\nthe forecast performance as the directional accuracy increased. Notably, the\nadjusted na\\\"ive forecast achieved statistically significant improvements even\nat relatively low directional accuracy levels slightly above 0.50. These\nfindings imply that the movement-prediction-adjusted na\\\"ive forecast can serve\nas an effective second-stage method for forecasting symmetric random walk time\nseries when consistent and accurate movement predictions are provided.",
      "tldr_zh": "本研究提出了一种运动预测调整的朴素预测（Movement-Prediction-Adjusted Naïve Forecast），适用于显示对称随机游走特性的时间序列，通过整合准确的运动预测来提升预测性能。具体方法是将原朴素预测调整为加权运动预测项，权重基于运动预测的方向准确性和目标序列的平均绝对增量。模拟实验在四种不同方差结构的合成系列上进行，使用 RMSE、MAE、MAPE 和 sMAPE 指标评估，结果显示预测性能随方向准确性增加而单调改善，即使准确性略高于0.50也能实现统计显著提升。该方法可作为有效的时间序列预测工具，尤其在提供一致准确运动预测的情况下。",
      "categories": [
        "cs.CE",
        "cs.AI",
        "cs.LG",
        "econ.EM",
        "stat.ML",
        "62M10"
      ],
      "primary_category": "cs.CE",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2406.14469v8",
      "published_date": "2024-06-20 16:32:18 UTC",
      "updated_date": "2025-04-18 13:55:39 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T22:46:17.666963"
    },
    {
      "arxiv_id": "2406.14458v1",
      "title": "Centimeter Positioning Accuracy using AI/ML for 6G Applications",
      "title_zh": "翻译失败",
      "authors": [
        "Sai Prasanth Kotturi",
        "Radha Krishna Ganti"
      ],
      "abstract": "This research looks at using AI/ML to achieve centimeter-level user\npositioning in 6G applications such as the Industrial Internet of Things\n(IIoT). Initial results show that our AI/ML-based method can estimate user\npositions with an accuracy of 17 cm in an indoor factory environment. In this\nproposal, we highlight our approaches and future directions.",
      "tldr_zh": "这篇研究探讨了利用AI/ML技术在6G应用（如IIoT）中实现厘米级用户定位精度。研究方法基于AI/ML算法，在室内工厂环境中进行测试，初步结果显示定位准确度可达17厘米。该提案还强调了相关方法的关键点以及未来的研究方向，为6G定位应用提供了潜在的技术基础。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.IT",
        "eess.SP",
        "math.IT"
      ],
      "primary_category": "cs.LG",
      "comment": "2 Pages, 2 Figures, ICMLCN Conference, Stockholm, Sweden",
      "pdf_url": "http://arxiv.org/pdf/2406.14458v1",
      "published_date": "2024-06-20 16:17:07 UTC",
      "updated_date": "2024-06-20 16:17:07 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T22:46:25.801462"
    },
    {
      "arxiv_id": "2406.14457v1",
      "title": "Rewarding What Matters: Step-by-Step Reinforcement Learning for Task-Oriented Dialogue",
      "title_zh": "翻译失败",
      "authors": [
        "Huifang Du",
        "Shuqin Li",
        "Minghao Wu",
        "Xuejing Feng",
        "Yuan-Fang Li",
        "Haofen Wang"
      ],
      "abstract": "Reinforcement learning (RL) is a powerful approach to enhance task-oriented\ndialogue (TOD) systems. However, existing RL methods tend to mainly focus on\ngeneration tasks, such as dialogue policy learning (DPL) or response generation\n(RG), while neglecting dialogue state tracking (DST) for understanding. This\nnarrow focus limits the systems to achieve globally optimal performance by\noverlooking the interdependence between understanding and generation.\nAdditionally, RL methods face challenges with sparse and delayed rewards, which\ncomplicates training and optimization. To address these issues, we extend RL\ninto both understanding and generation tasks by introducing step-by-step\nrewards throughout the token generation. The understanding reward increases as\nmore slots are correctly filled in DST, while the generation reward grows with\nthe accurate inclusion of user requests. Our approach provides a balanced\noptimization aligned with task completion. Experimental results demonstrate\nthat our approach effectively enhances the performance of TOD systems and\nachieves new state-of-the-art results on three widely used datasets, including\nMultiWOZ2.0, MultiWOZ2.1, and In-Car. Our approach also shows superior few-shot\nability in low-resource settings compared to current models.",
      "tldr_zh": "该研究针对任务导向对话 (TOD) 系统中的强化学习 (RL) 问题，提出了一种步步为营的奖励机制，将 RL 扩展到对话状态跟踪 (DST) 和响应生成 (RG) 等理解与生成任务，以解决现有方法忽略理解生成相互依赖以及稀疏奖励的挑战。\n具体而言，该机制通过为正确填充 DST 槽位提供递增理解奖励，以及为准确包含用户请求提供递增生成奖励，实现任务的平衡优化。\n实验结果表明，该方法在 MultiWOZ2.0、MultiWOZ2.1 和 In-Car 数据集上达到了新的最先进性能，并在低资源设置下显示出优越的少样本能力。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2406.14457v1",
      "published_date": "2024-06-20 16:15:40 UTC",
      "updated_date": "2024-06-20 16:15:40 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T22:46:40.795809"
    },
    {
      "arxiv_id": "2406.14449v1",
      "title": "APEER: Automatic Prompt Engineering Enhances Large Language Model Reranking",
      "title_zh": "翻译失败",
      "authors": [
        "Can Jin",
        "Hongwu Peng",
        "Shiyu Zhao",
        "Zhenting Wang",
        "Wujiang Xu",
        "Ligong Han",
        "Jiahui Zhao",
        "Kai Zhong",
        "Sanguthevar Rajasekaran",
        "Dimitris N. Metaxas"
      ],
      "abstract": "Large Language Models (LLMs) have significantly enhanced Information\nRetrieval (IR) across various modules, such as reranking. Despite impressive\nperformance, current zero-shot relevance ranking with LLMs heavily relies on\nhuman prompt engineering. Existing automatic prompt engineering algorithms\nprimarily focus on language modeling and classification tasks, leaving the\ndomain of IR, particularly reranking, underexplored. Directly applying current\nprompt engineering algorithms to relevance ranking is challenging due to the\nintegration of query and long passage pairs in the input, where the ranking\ncomplexity surpasses classification tasks. To reduce human effort and unlock\nthe potential of prompt optimization in reranking, we introduce a novel\nautomatic prompt engineering algorithm named APEER. APEER iteratively generates\nrefined prompts through feedback and preference optimization. Extensive\nexperiments with four LLMs and ten datasets demonstrate the substantial\nperformance improvement of APEER over existing state-of-the-art (SoTA) manual\nprompts. Furthermore, we find that the prompts generated by APEER exhibit\nbetter transferability across diverse tasks and LLMs. Code is available at\nhttps://github.com/jincan333/APEER.",
      "tldr_zh": "该论文指出，大型语言模型 (LLMs) 在信息检索 (IR) 的 reranking 任务中依赖人工提示工程，而现有自动提示工程算法主要针对语言建模和分类任务，导致 reranking 领域未充分探索。研究引入了 APEER，一种新型算法，通过反馈和偏好优化迭代生成精炼提示，以处理查询和长段落对的复杂整合问题。实验结果显示，在四个 LLMs 和十个数据集上，APEER 比现有最先进 (SoTA) 手动提示显著提升性能，且生成的提示在不同任务和 LLMs 之间具有更好的可转移性。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2406.14449v1",
      "published_date": "2024-06-20 16:11:45 UTC",
      "updated_date": "2024-06-20 16:11:45 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T22:46:52.410574"
    },
    {
      "arxiv_id": "2406.14442v1",
      "title": "Graph Representation Learning Strategies for Omics Data: A Case Study on Parkinson's Disease",
      "title_zh": "翻译失败",
      "authors": [
        "Elisa Gómez de Lope",
        "Saurabh Deshpande",
        "Ramón Viñas Torné",
        "Pietro Liò",
        "Enrico Glaab",
        "Stéphane P. A. Bordas"
      ],
      "abstract": "Omics data analysis is crucial for studying complex diseases, but its high\ndimensionality and heterogeneity challenge classical statistical and machine\nlearning methods. Graph neural networks have emerged as promising alternatives,\nyet the optimal strategies for their design and optimization in real-world\nbiomedical challenges remain unclear. This study evaluates various graph\nrepresentation learning models for case-control classification using\nhigh-throughput biological data from Parkinson's disease and control samples.\nWe compare topologies derived from sample similarity networks and molecular\ninteraction networks, including protein-protein and metabolite-metabolite\ninteractions (PPI, MMI). Graph Convolutional Network (GCNs), Chebyshev spectral\ngraph convolution (ChebyNet), and Graph Attention Network (GAT), are evaluated\nalongside advanced architectures like graph transformers, the graph U-net, and\nsimpler models like multilayer perceptron (MLP).\n  These models are systematically applied to transcriptomics and metabolomics\ndata independently. Our comparative analysis highlights the benefits and\nlimitations of various architectures in extracting patterns from omics data,\npaving the way for more accurate and interpretable models in biomedical\nresearch.",
      "tldr_zh": "本研究探讨了图表示学习策略在高维度异质性 Omics 数据分析中的应用，以帕金森病为案例研究，旨在解决传统统计和机器学习方法的挑战。研究比较了多种模型，包括 Graph Convolutional Network (GCNs)、Chebyshev spectral graph convolution (ChebyNet)、Graph Attention Network (GAT)、图变压器、图 U-net，以及简单模型如 multilayer perceptron (MLP)。这些模型应用于转录组学和代谢组学数据，基于样本相似性网络和分子交互网络（如 protein-protein interactions (PPI) 和 metabolite-metabolite interactions (MMI)）。结果突出了不同架构的优点和局限性，为生物医学研究提供更准确和可解释的模式提取方法。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CE",
        "q-bio.BM",
        "q-bio.MN"
      ],
      "primary_category": "cs.LG",
      "comment": "Submitted to Machine Learning in Computational Biology 2024 as an\n  extended abstract, 2 pages + 1 appendix",
      "pdf_url": "http://arxiv.org/pdf/2406.14442v1",
      "published_date": "2024-06-20 16:06:39 UTC",
      "updated_date": "2024-06-20 16:06:39 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T22:47:07.763780"
    },
    {
      "arxiv_id": "2406.14429v2",
      "title": "CollaFuse: Collaborative Diffusion Models",
      "title_zh": "CollaFuse：协作扩散模型",
      "authors": [
        "Simeon Allmendinger",
        "Domenique Zipperling",
        "Lukas Struppek",
        "Niklas Kühl"
      ],
      "abstract": "In the landscape of generative artificial intelligence, diffusion-based\nmodels have emerged as a promising method for generating synthetic images.\nHowever, the application of diffusion models poses numerous challenges,\nparticularly concerning data availability, computational requirements, and\nprivacy. Traditional approaches to address these shortcomings, like federated\nlearning, often impose significant computational burdens on individual clients,\nespecially those with constrained resources. In response to these challenges,\nwe introduce a novel approach for distributed collaborative diffusion models\ninspired by split learning. Our approach facilitates collaborative training of\ndiffusion models while alleviating client computational burdens during image\nsynthesis. This reduced computational burden is achieved by retaining data and\ncomputationally inexpensive processes locally at each client while outsourcing\nthe computationally expensive processes to shared, more efficient server\nresources. Through experiments on the common CelebA dataset, our approach\ndemonstrates enhanced privacy by reducing the necessity for sharing raw data.\nThese capabilities hold significant potential across various application areas,\nincluding the design of edge computing solutions. Thus, our work advances\ndistributed machine learning by contributing to the evolution of collaborative\ndiffusion models.",
      "tldr_zh": "本研究提出CollaFuse，一种受split learning启发的分布式协作diffusion models方法，旨在解决diffusion models在生成合成图像时面临的数据可用性、计算需求和隐私挑战。该方法通过将计算密集型过程外包到高效的服务器资源，而在客户端保留数据和低成本计算，从而减轻了资源受限客户端的负担，并在CelebA dataset上的实验中证明了其增强隐私的效果（减少原始数据共享）。相比传统方法如federated learning，CollaFuse显著降低了客户端计算负担，并为边缘计算等应用领域提供了潜在优势，从而推进了分布式机器学习的演进。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.LG",
      "comment": "13 pages, 7 figures",
      "pdf_url": "http://arxiv.org/pdf/2406.14429v2",
      "published_date": "2024-06-20 15:54:21 UTC",
      "updated_date": "2024-10-27 12:42:53 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T22:47:18.816178"
    },
    {
      "arxiv_id": "2406.14427v2",
      "title": "Control when confidence is costly",
      "title_zh": "翻译失败",
      "authors": [
        "Itzel Olivos-Castillo",
        "Paul Schrater",
        "Xaq Pitkow"
      ],
      "abstract": "We develop a version of stochastic control that accounts for computational\ncosts of inference. Past studies identified efficient coding without control,\nor efficient control that neglects the cost of synthesizing information. Here\nwe combine these concepts into a framework where agents rationally approximate\ninference for efficient control. Specifically, we study Linear Quadratic\nGaussian (LQG) control with an added internal cost on the relative precision of\nthe posterior probability over the world state. This creates a trade-off: an\nagent can obtain more utility overall by sacrificing some task performance, if\ndoing so saves enough bits during inference. We discover that the rational\nstrategy that solves the joint inference and control problem goes through phase\ntransitions depending on the task demands, switching from a costly but optimal\ninference to a family of suboptimal inferences related by rotation\ntransformations, each misestimate the stability of the world. In all cases, the\nagent moves more to think less. This work provides a foundation for a new type\nof rational computations that could be used by both brains and machines for\nefficient but computationally constrained control.",
      "tldr_zh": "本文提出了一种考虑推理计算成本的随机控制框架，允许代理通过理性近似推理实现高效控制。具体地，该框架扩展了 Linear Quadratic Gaussian (LQG) 控制，添加了对后验概率中世界状态相对精度的内部成本，从而在任务性能和推理开销之间形成权衡。研究发现，理性策略会根据任务需求经历相变，从昂贵的但最优推理切换到一组通过旋转变换相关的次优推理，这些推理可能错误估计世界的稳定性。在所有情况下，代理会通过增加动作来减少思考，为大脑和机器提供高效但计算受限控制的新型理性计算基础。",
      "categories": [
        "cs.AI",
        "q-bio.NC"
      ],
      "primary_category": "cs.AI",
      "comment": "10 pages, 4 figures",
      "pdf_url": "http://arxiv.org/pdf/2406.14427v2",
      "published_date": "2024-06-20 15:50:38 UTC",
      "updated_date": "2024-10-29 18:52:56 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T22:47:30.957962"
    },
    {
      "arxiv_id": "2406.14425v3",
      "title": "SynDARin: Synthesising Datasets for Automated Reasoning in Low-Resource Languages",
      "title_zh": "翻译失败",
      "authors": [
        "Gayane Ghazaryan",
        "Erik Arakelyan",
        "Pasquale Minervini",
        "Isabelle Augenstein"
      ],
      "abstract": "Question Answering (QA) datasets have been instrumental in developing and\nevaluating Large Language Model (LLM) capabilities. However, such datasets are\nscarce for languages other than English due to the cost and difficulties of\ncollection and manual annotation. This means that producing novel models and\nmeasuring the performance of multilingual LLMs in low-resource languages is\nchallenging. To mitigate this, we propose $\\textbf{S}$yn$\\textbf{DAR}$in, a\nmethod for generating and validating QA datasets for low-resource languages. We\nutilize parallel content mining to obtain $\\textit{human-curated}$ paragraphs\nbetween English and the target language. We use the English data as context to\n$\\textit{generate}$ synthetic multiple-choice (MC) question-answer pairs, which\nare automatically translated and further validated for quality. Combining these\nwith their designated non-English $\\textit{human-curated}$ paragraphs form the\nfinal QA dataset. The method allows to maintain the content quality, reduces\nthe likelihood of factual errors, and circumvents the need for costly\nannotation. To test the method, we created a QA dataset with $1.2$K samples for\nthe Armenian language. The human evaluation shows that $98\\%$ of the generated\nEnglish data maintains quality and diversity in the question types and topics,\nwhile the translation validation pipeline can filter out $\\sim70\\%$ of data\nwith poor quality. We use the dataset to benchmark state-of-the-art LLMs,\nshowing their inability to achieve human accuracy with some model performances\ncloser to random chance. This shows that the generated dataset is non-trivial\nand can be used to evaluate reasoning capabilities in low-resource language.",
      "tldr_zh": "该论文提出SynDARin方法，用于合成和验证低资源语言的Question Answering (QA)数据集，以解决现有数据集主要针对英语的问题。方法通过平行内容挖掘获取英语和目标语言的人类编撰段落，利用英语数据生成合成多选题QA对，并进行自动翻译和质量验证，从而避免昂贵的手动标注。实验在亚美尼亚语上创建了1.2K样本数据集，结果显示98%的生成数据保持高质量和多样性，翻译验证能过滤约70%的低质数据；基准测试表明，现有Large Language Model (LLM)无法达到人类准确率，有些接近随机猜测，这突显了该数据集在评估低资源语言推理能力方面的价值。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2406.14425v3",
      "published_date": "2024-06-20 15:49:28 UTC",
      "updated_date": "2024-09-16 21:52:55 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T22:47:41.734246"
    },
    {
      "arxiv_id": "2407.12009v1",
      "title": "Using Multimodal Foundation Models and Clustering for Improved Style Ambiguity Loss",
      "title_zh": "翻译失败",
      "authors": [
        "James Baker"
      ],
      "abstract": "Teaching text-to-image models to be creative involves using style ambiguity\nloss, which requires a pretrained classifier. In this work, we explore a new\nform of the style ambiguity training objective, used to approximate creativity,\nthat does not require training a classifier or even a labeled dataset. We then\ntrain a diffusion model to maximize style ambiguity to imbue the diffusion\nmodel with creativity and find our new methods improve upon the traditional\nmethod, based on automated metrics for human judgment, while still maintaining\ncreativity and novelty.",
      "tldr_zh": "这篇论文探讨了利用多模态基础模型（Multimodal Foundation Models）和聚类（Clustering）来改进风格模糊损失（Style Ambiguity Loss），以增强文本到图像模型的创造力。研究提出了一种新的训练目标，无需预训练分类器或标记数据集，通过训练扩散模型（Diffusion Model）来最大化风格模糊，从而提升模型的创新性。结果表明，该方法在自动化指标和人类判断上优于传统方法，同时保持了创造性和新颖性。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2407.12009v1",
      "published_date": "2024-06-20 15:43:13 UTC",
      "updated_date": "2024-06-20 15:43:13 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T22:47:52.731365"
    },
    {
      "arxiv_id": "2406.14422v1",
      "title": "FutureNet-LOF: Joint Trajectory Prediction and Lane Occupancy Field Prediction with Future Context Encoding",
      "title_zh": "FutureNet-LOF：利用未来上下文编码",
      "authors": [
        "Mingkun Wang",
        "Xiaoguang Ren",
        "Ruochun Jin",
        "Minglong Li",
        "Xiaochuan Zhang",
        "Changqian Yu",
        "Mingxu Wang",
        "Wenjing Yang"
      ],
      "abstract": "Most prior motion prediction endeavors in autonomous driving have\ninadequately encoded future scenarios, leading to predictions that may fail to\naccurately capture the diverse movements of agents (e.g., vehicles or\npedestrians). To address this, we propose FutureNet, which explicitly\nintegrates initially predicted trajectories into the future scenario and\nfurther encodes these future contexts to enhance subsequent forecasting.\nAdditionally, most previous motion forecasting works have focused on predicting\nindependent futures for each agent. However, safe and smooth autonomous driving\nrequires accurately predicting the diverse future behaviors of numerous\nsurrounding agents jointly in complex dynamic environments. Given that all\nagents occupy certain potential travel spaces and possess lane driving\npriority, we propose Lane Occupancy Field (LOF), a new representation with lane\nsemantics for motion forecasting in autonomous driving. LOF can simultaneously\ncapture the joint probability distribution of all road participants' future\nspatial-temporal positions. Due to the high compatibility between lane\noccupancy field prediction and trajectory prediction, we propose a novel\nnetwork with future context encoding for the joint prediction of these two\ntasks. Our approach ranks 1st on two large-scale motion forecasting benchmarks:\nArgoverse 1 and Argoverse 2.",
      "tldr_zh": "本论文提出 FutureNet 方法，通过整合和编码初始预测的轨迹作为未来上下文，解决了自动驾驶运动预测中场景编码不足的问题，从而更准确地捕捉代理（如车辆或行人）的多样化行为。同时，引入 Lane Occupancy Field (LOF) 作为一种带车道语义的新表示，能够同时捕捉所有路参与者未来时空位置的联合概率分布。论文开发了一个新网络，用于联合预测轨迹和 LOF，提升了在复杂动态环境下的预测性能，并在 Argoverse 1 和 Argoverse 2 基准上排名第一。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "10 pages",
      "pdf_url": "http://arxiv.org/pdf/2406.14422v1",
      "published_date": "2024-06-20 15:41:53 UTC",
      "updated_date": "2024-06-20 15:41:53 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T22:48:05.452280"
    },
    {
      "arxiv_id": "2406.14408v2",
      "title": "FVEL: Interactive Formal Verification Environment with Large Language Models via Theorem Proving",
      "title_zh": "翻译失败",
      "authors": [
        "Xiaohan Lin",
        "Qingxing Cao",
        "Yinya Huang",
        "Haiming Wang",
        "Jianqiao Lu",
        "Zhengying Liu",
        "Linqi Song",
        "Xiaodan Liang"
      ],
      "abstract": "Formal verification (FV) has witnessed growing significance with current\nemerging program synthesis by the evolving large language models (LLMs).\nHowever, current formal verification mainly resorts to symbolic verifiers or\nhand-craft rules, resulting in limitations for extensive and flexible\nverification. On the other hand, formal languages for automated theorem\nproving, such as Isabelle, as another line of rigorous verification, are\nmaintained with comprehensive rules and theorems. In this paper, we propose\nFVEL, an interactive Formal Verification Environment with LLMs. Specifically,\nFVEL transforms a given code to be verified into Isabelle, and then conducts\nverification via neural automated theorem proving with an LLM. The joined\nparadigm leverages the rigorous yet abundant formulated and organized rules in\nIsabelle and is also convenient for introducing and adjusting cutting-edge\nLLMs. To achieve this goal, we extract a large-scale FVELER3. The FVELER\ndataset includes code dependencies and verification processes that are\nformulated in Isabelle, containing 758 theories, 29,125 lemmas, and 200,646\nproof steps in total with in-depth dependencies. We benchmark FVELER in the\nFVEL environment by first fine-tuning LLMs with FVELER and then evaluating them\non Code2Inv and SV-COMP. The results show that FVEL with FVELER fine-tuned\nLlama3- 8B solves 17.39% (69 -> 81) more problems, and Mistral-7B 12% (75 ->\n84) more problems in SV-COMP. And the proportion of proof errors is reduced.\nProject page: https://fveler.github.io/.",
      "tldr_zh": "该论文提出 FVEL，一种交互式正式验证环境，通过 Large Language Models (LLMs) 结合定理证明工具 Isabelle，对代码进行高效验证，解决传统符号验证器或手工规则的局限性。FVEL 的方法包括将待验证代码转换为 Isabelle 格式，并利用神经自动定理证明进行交互式处理，同时构建了大规模 FVELER 数据集，包含 758 个理论、29,125 个引理和 200,646 个证明步骤。实验结果显示，在 SV-COMP 基准上，微调后的 Llama3-8B 模型解决了 17.39% 更多问题（从 69 到 81），Mistral-7B 模型解决了 12% 更多问题（从 75 到 84），并显著降低了证明错误率。",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2406.14408v2",
      "published_date": "2024-06-20 15:31:05 UTC",
      "updated_date": "2024-06-21 02:51:41 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T22:48:19.723593"
    },
    {
      "arxiv_id": "2406.14401v1",
      "title": "Fair Streaming Feature Selection",
      "title_zh": "翻译失败",
      "authors": [
        "Zhangling Duan",
        "Tianci Li",
        "Xingyu Wu",
        "Zhaolong Ling",
        "Jingye Yang",
        "Zhaohong Jia"
      ],
      "abstract": "Streaming feature selection techniques have become essential in processing\nreal-time data streams, as they facilitate the identification of the most\nrelevant attributes from continuously updating information. Despite their\nperformance, current algorithms to streaming feature selection frequently fall\nshort in managing biases and avoiding discrimination that could be perpetuated\nby sensitive attributes, potentially leading to unfair outcomes in the\nresulting models. To address this issue, we propose FairSFS, a novel algorithm\nfor Fair Streaming Feature Selection, to uphold fairness in the feature\nselection process without compromising the ability to handle data in an online\nmanner. FairSFS adapts to incoming feature vectors by dynamically adjusting the\nfeature set and discerns the correlations between classification attributes and\nsensitive attributes from this revised set, thereby forestalling the\npropagation of sensitive data. Empirical evaluations show that FairSFS not only\nmaintains accuracy that is on par with leading streaming feature selection\nmethods and existing fair feature techniques but also significantly improves\nfairness metrics.",
      "tldr_zh": "本研究针对流式特征选择(streaming feature selection)算法在处理实时数据流时存在的偏见和歧视问题，提出了一种新颖的FairSFS算法，以在特征选择过程中维护公平性。FairSFS通过动态调整特征集并分析分类属性与敏感属性的相关性，实现在线数据处理，同时防止敏感数据传播。实验结果显示，FairSFS在准确性上与领先的流式特征选择方法相当，但显著改善了公平性指标(fairness metrics)，为构建更公正的机器学习模型提供了有效解决方案。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "30 pages, 10 figures",
      "pdf_url": "http://arxiv.org/pdf/2406.14401v1",
      "published_date": "2024-06-20 15:22:44 UTC",
      "updated_date": "2024-06-20 15:22:44 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T22:48:28.430874"
    },
    {
      "arxiv_id": "2406.14377v2",
      "title": "CE-SSL: Computation-Efficient Semi-Supervised Learning for ECG-based Cardiovascular Diseases Detection",
      "title_zh": "翻译失败",
      "authors": [
        "Rushuang Zhou",
        "Lei Clifton",
        "Zijun Liu",
        "Kannie W. Y. Chan",
        "David A. Clifton",
        "Yuan-Ting Zhang",
        "Yining Dong"
      ],
      "abstract": "The label scarcity problem is the main challenge that hinders the wide\napplication of deep learning systems in automatic cardiovascular diseases\n(CVDs) detection using electrocardiography (ECG). Tuning pre-trained models\nalleviates this problem by transferring knowledge learned from large datasets\nto downstream small datasets. However, bottlenecks in computational efficiency\nand detection performance limit its clinical applications. It is difficult to\nimprove the detection performance without significantly sacrificing the\ncomputational efficiency during model training. Here, we propose a\ncomputation-efficient semi-supervised learning paradigm (CE-SSL) for robust and\ncomputation-efficient CVDs detection using ECG. It enables a robust adaptation\nof pre-trained models on downstream datasets with limited supervision and high\ncomputational efficiency. First, a random-deactivation technique is developed\nto achieve robust and fast low-rank adaptation of pre-trained weights.\nSubsequently, we propose a one-shot rank allocation module to determine the\noptimal ranks for the update matrices of the pre-trained weights. Finally, a\nlightweight semi-supervised learning pipeline is introduced to enhance model\nperformance by leveraging labeled and unlabeled data with high computational\nefficiency. Extensive experiments on four downstream datasets demonstrate that\nCE-SSL not only outperforms the state-of-the-art methods in multi-label CVDs\ndetection but also consumes fewer GPU footprints, training time, and parameter\nstorage space. As such, this paradigm provides an effective solution for\nachieving high computational efficiency and robust detection performance in the\nclinical applications of pre-trained models under limited supervision. Code and\nSupplementary Materials are available at https://github.com/KAZABANA/CE-SSL",
      "tldr_zh": "本文提出 CE-SSL，一种计算高效的半监督学习范式，用于基于 ECG 的心血管疾病 (CVDs) 检测，以解决标签稀缺问题和现有方法的计算瓶颈。核心组件包括 random-deactivation 技术实现快速低秩适应、one-shot rank allocation 模块优化预训练权重更新矩阵，以及轻量级半监督学习管道，利用标记和未标记数据提升性能。实验在四个下游数据集上证明，CE-SSL 在多标签 CVDs 检测中优于最先进方法，同时显著减少 GPU 占用、训练时间和参数存储空间。该方法为临床环境下有限监督的预训练模型应用提供了高效、鲁棒的解决方案。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2406.14377v2",
      "published_date": "2024-06-20 14:45:13 UTC",
      "updated_date": "2024-11-15 16:23:15 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T22:48:43.197744"
    },
    {
      "arxiv_id": "2406.14373v2",
      "title": "Artificial Leviathan: Exploring Social Evolution of LLM Agents Through the Lens of Hobbesian Social Contract Theory",
      "title_zh": "翻译失败",
      "authors": [
        "Gordon Dai",
        "Weijia Zhang",
        "Jinhan Li",
        "Siqi Yang",
        "Chidera Onochie lbe",
        "Srihas Rao",
        "Arthur Caetano",
        "Misha Sra"
      ],
      "abstract": "The emergence of Large Language Models (LLMs) and advancements in Artificial\nIntelligence (AI) offer an opportunity for computational social science\nresearch at scale. Building upon prior explorations of LLM agent design, our\nwork introduces a simulated agent society where complex social relationships\ndynamically form and evolve over time. Agents are imbued with psychological\ndrives and placed in a sandbox survival environment. We conduct an evaluation\nof the agent society through the lens of Thomas Hobbes's seminal Social\nContract Theory (SCT). We analyze whether, as the theory postulates, agents\nseek to escape a brutish \"state of nature\" by surrendering rights to an\nabsolute sovereign in exchange for order and security. Our experiments unveil\nan alignment: Initially, agents engage in unrestrained conflict, mirroring\nHobbes's depiction of the state of nature. However, as the simulation\nprogresses, social contracts emerge, leading to the authorization of an\nabsolute sovereign and the establishment of a peaceful commonwealth founded on\nmutual cooperation. This congruence between our LLM agent society's\nevolutionary trajectory and Hobbes's theoretical account indicates LLMs'\ncapability to model intricate social dynamics and potentially replicate forces\nthat shape human societies. By enabling such insights into group behavior and\nemergent societal phenomena, LLM-driven multi-agent simulations, while unable\nto simulate all the nuances of human behavior, may hold potential for advancing\nour understanding of social structures, group dynamics, and complex human\nsystems.",
      "tldr_zh": "本研究构建了一个基于LLM的多代理模拟社会，代理具备心理驱动力，并通过Thomas Hobbes的Social Contract Theory框架，探索代理在生存环境中的社会演变过程。实验发现，代理最初陷入无拘无束的冲突状态（类似于“自然状态”），但随着模拟进展，它们自发形成社会契约，授权一个绝对主权者，建立和平的合作共同体。结果表明，LLM能够有效模拟复杂的社会动态，为计算社会科学提供新工具，深化对人类社会结构和群体行为的理解。",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.CY",
        "cs.HC",
        "cs.MA"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2406.14373v2",
      "published_date": "2024-06-20 14:42:58 UTC",
      "updated_date": "2024-07-01 22:06:13 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T22:48:52.893953"
    },
    {
      "arxiv_id": "2406.14367v2",
      "title": "PoseBench: Benchmarking the Robustness of Pose Estimation Models under Corruptions",
      "title_zh": "翻译失败",
      "authors": [
        "Sihan Ma",
        "Jing Zhang",
        "Qiong Cao",
        "Dacheng Tao"
      ],
      "abstract": "Pose estimation aims to accurately identify anatomical keypoints in humans\nand animals using monocular images, which is crucial for various applications\nsuch as human-machine interaction, embodied AI, and autonomous driving. While\ncurrent models show promising results, they are typically trained and tested on\nclean data, potentially overlooking the corruption during real-world deployment\nand thus posing safety risks in practical scenarios. To address this issue, we\nintroduce PoseBench, a comprehensive benchmark designed to evaluate the\nrobustness of pose estimation models against real-world corruption. We\nevaluated 60 representative models, including top-down, bottom-up,\nheatmap-based, regression-based, and classification-based methods, across three\ndatasets for human and animal pose estimation. Our evaluation involves 10 types\nof corruption in four categories: 1) blur and noise, 2) compression and color\nloss, 3) severe lighting, and 4) masks. Our findings reveal that\nstate-of-the-art models are vulnerable to common real-world corruptions and\nexhibit distinct behaviors when tackling human and animal pose estimation\ntasks. To improve model robustness, we delve into various design\nconsiderations, including input resolution, pre-training datasets, backbone\ncapacity, post-processing, and data augmentations. We hope that our benchmark\nwill serve as a foundation for advancing research in robust pose estimation.\nThe benchmark and source code will be released at\nhttps://xymsh.github.io/PoseBench",
      "tldr_zh": "本研究引入了PoseBench基准测试，以评估姿态估计（pose estimation）模型在真实世界干扰下的鲁棒性，因为现有模型虽在干净数据上表现良好，但易受干扰影响而带来安全风险。研究评估了60个代表性模型，包括top-down、bottom-up、heatmap-based、regression-based和classification-based方法，在三个数据集上测试人类和动物姿态，并涉及四类干扰（如模糊噪声、压缩颜色损失、严重照明和掩码）。结果显示，最先进模型对常见干扰高度脆弱，且在人类和动物任务中表现出显著差异；此外，论文探讨了提升鲁棒性的策略，如输入分辨率、预训练数据集、主干网络容量、后处理和数据增强，以推动该领域的未来研究。基准测试及其源代码已发布在https://xymsh.github.io/PoseBench。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "Technical report. Project page: https://xymsh.github.io/PoseBench/",
      "pdf_url": "http://arxiv.org/pdf/2406.14367v2",
      "published_date": "2024-06-20 14:40:17 UTC",
      "updated_date": "2024-09-14 02:37:20 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T22:49:07.473896"
    },
    {
      "arxiv_id": "2406.14362v1",
      "title": "Communication-Efficient Byzantine-Resilient Federated Zero-Order Optimization",
      "title_zh": "翻译失败",
      "authors": [
        "Afonso de Sá Delgado Neto",
        "Maximilian Egger",
        "Mayank Bakshi",
        "Rawad Bitar"
      ],
      "abstract": "We introduce CYBER-0, the first zero-order optimization algorithm for\nmemory-and-communication efficient Federated Learning, resilient to Byzantine\nfaults. We show through extensive numerical experiments on the MNIST dataset\nand finetuning RoBERTa-Large that CYBER-0 outperforms state-of-the-art\nalgorithms in terms of communication and memory efficiency while reaching\nsimilar accuracy. We provide theoretical guarantees on its convergence for\nconvex loss functions.",
      "tldr_zh": "我们介绍了 CYBER-0，这是一个高效的零阶优化（zero-order optimization）算法，针对联邦学习（Federated Learning），实现了通信和内存效率，同时抵抗 Byzantine faults。实验结果显示，在 MNIST 数据集和 finetuning RoBERTa-Large 模型上，CYBER-0 优于现有算法，在通信和内存效率方面表现出色，同时保持相似的准确率。该算法为凸损失函数提供了收敛性的理论保证。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2406.14362v1",
      "published_date": "2024-06-20 14:36:12 UTC",
      "updated_date": "2024-06-20 14:36:12 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T22:49:17.142042"
    },
    {
      "arxiv_id": "2406.14361v1",
      "title": "Robustness Analysis of AI Models in Critical Energy Systems",
      "title_zh": "翻译失败",
      "authors": [
        "Pantelis Dogoulis",
        "Matthieu Jimenez",
        "Salah Ghamizi",
        "Maxime Cordy",
        "Yves Le Traon"
      ],
      "abstract": "This paper analyzes the robustness of state-of-the-art AI-based models for\npower grid operations under the $N-1$ security criterion. While these models\nperform well in regular grid settings, our results highlight a significant loss\nin accuracy following the disconnection of a line.%under this security\ncriterion. Using graph theory-based analysis, we demonstrate the impact of node\nconnectivity on this loss. Our findings emphasize the need for practical\nscenario considerations in developing AI methodologies for critical\ninfrastructure.",
      "tldr_zh": "这篇论文分析了最先进 AI 模型在电力网格操作中的鲁棒性，特别是针对 N-1 security criterion 的场景。尽管这些模型在常规电网设置下表现良好，但断开一条线路会导致准确性显著下降。通过 graph theory-based 分析，该研究揭示了 node connectivity 对准确性损失的关键影响。最终，论文强调在开发用于关键基础设施的 AI 方法时，需要优先考虑实际场景，以提升可靠性。",
      "categories": [
        "cs.AI",
        "cs.SY",
        "eess.SY"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2406.14361v1",
      "published_date": "2024-06-20 14:34:36 UTC",
      "updated_date": "2024-06-20 14:34:36 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T22:49:29.312181"
    },
    {
      "arxiv_id": "2406.14358v1",
      "title": "The neural correlates of logical-mathematical symbol systems processing resemble that of spatial cognition more than natural language processing",
      "title_zh": "翻译失败",
      "authors": [
        "Yuannan Li",
        "Shan Xu",
        "Jia Liu"
      ],
      "abstract": "The ability to manipulate logical-mathematical symbols (LMS), encompassing\ntasks such as calculation, reasoning, and programming, is a cognitive skill\narguably unique to humans. Considering the relatively recent emergence of this\nability in human evolutionary history, it has been suggested that LMS\nprocessing may build upon more fundamental cognitive systems, possibly through\nneuronal recycling. Previous studies have pinpointed two primary candidates,\nnatural language processing and spatial cognition. Existing comparisons between\nthese domains largely relied on task-level comparison, which may be confounded\nby task idiosyncrasy. The present study instead compared the neural correlates\nat the domain level with both automated meta-analysis and synthesized maps\nbased on three representative LMS tasks, reasoning, calculation, and mental\nprogramming. Our results revealed a more substantial cortical overlap between\nLMS processing and spatial cognition, in contrast to language processing.\nFurthermore, in regions activated by both spatial and language processing, the\nmultivariate activation pattern for LMS processing exhibited greater\nmultivariate similarity to spatial cognition than to language processing. A\nhierarchical clustering analysis further indicated that typical LMS tasks were\nindistinguishable from spatial cognition tasks at the neural level, suggesting\nan inherent connection between these two cognitive processes. Taken together,\nour findings support the hypothesis that spatial cognition is likely the basis\nof LMS processing, which may shed light on the limitations of large language\nmodels in logical reasoning, particularly those trained exclusively on textual\ndata without explicit emphasis on spatial content.",
      "tldr_zh": "本研究比较了处理逻辑-数学符号 (LMS) 的神经相关，发现其更类似于空间认知而非自然语言处理，基于推理、计算和心理编程等任务的分析。研究采用自动化元分析和综合地图方法，揭示了 LMS 处理与空间认知在皮层重叠和多变量激活模式上具有更大相似性。层次聚类分析进一步表明，典型 LMS 任务在神经层面与空间认知任务不可区分，支持空间认知可能作为 LMS 处理基础的假设，并解释了大型语言模型在逻辑推理中的局限性，尤其是在缺乏空间内容训练时。",
      "categories": [
        "q-bio.NC",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "q-bio.NC",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2406.14358v1",
      "published_date": "2024-06-20 14:31:09 UTC",
      "updated_date": "2024-06-20 14:31:09 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T22:49:44.427125"
    },
    {
      "arxiv_id": "2406.14351v1",
      "title": "Automatic Labels are as Effective as Manual Labels in Biomedical Images Classification with Deep Learning",
      "title_zh": "在基于深度学习的生物医学图像分类中，自动标签与手动标签同样有效",
      "authors": [
        "Niccolò Marini",
        "Stefano Marchesin",
        "Lluis Borras Ferris",
        "Simon Püttmann",
        "Marek Wodzinski",
        "Riccardo Fratti",
        "Damian Podareanu",
        "Alessandro Caputo",
        "Svetla Boytcheva",
        "Simona Vatrano",
        "Filippo Fraggetta",
        "Iris Nagtegaal",
        "Gianmaria Silvello",
        "Manfredo Atzori",
        "Henning Müller"
      ],
      "abstract": "The increasing availability of biomedical data is helping to design more\nrobust deep learning (DL) algorithms to analyze biomedical samples. Currently,\none of the main limitations to train DL algorithms to perform a specific task\nis the need for medical experts to label data. Automatic methods to label data\nexist, however automatic labels can be noisy and it is not completely clear\nwhen automatic labels can be adopted to train DL models. This paper aims to\ninvestigate under which circumstances automatic labels can be adopted to train\na DL model on the classification of Whole Slide Images (WSI). The analysis\ninvolves multiple architectures, such as Convolutional Neural Networks (CNN)\nand Vision Transformer (ViT), and over 10000 WSIs, collected from three use\ncases: celiac disease, lung cancer and colon cancer, which one including\nrespectively binary, multiclass and multilabel data. The results allow\nidentifying 10% as the percentage of noisy labels that lead to train\ncompetitive models for the classification of WSIs. Therefore, an algorithm\ngenerating automatic labels needs to fit this criterion to be adopted. The\napplication of the Semantic Knowledge Extractor Tool (SKET) algorithm to\ngenerate automatic labels leads to performance comparable to the one obtained\nwith manual labels, since it generates a percentage of noisy labels between\n2-5%. Automatic labels are as effective as manual ones, reaching solid\nperformance comparable to the one obtained training models with manual labels.",
      "tldr_zh": "这篇论文探讨了在深度学习（DL）模型中，使用自动标签是否能像手动标签一样有效分类生物医学图像，特别是Whole Slide Images (WSI)。研究通过实验多种架构如Convolutional Neural Networks (CNN)和Vision Transformer (ViT)，分析了超过10000张WSI数据集（涉及乳糜泻、肺癌和结肠癌的二分类、多分类和多标签任务）。结果显示，当自动标签的噪声比例低于10%时，训练出的模型性能与使用手动标签相当。作者证明了Semantic Knowledge Extractor Tool (SKET)算法生成的自动标签（噪声比例2-5%）能达到与手动标签类似的效果，从而提高DL模型训练的效率和可行性。",
      "categories": [
        "eess.IV",
        "cs.AI",
        "cs.CV",
        "cs.LG"
      ],
      "primary_category": "eess.IV",
      "comment": "pre-print of the journal paper",
      "pdf_url": "http://arxiv.org/pdf/2406.14351v1",
      "published_date": "2024-06-20 14:20:50 UTC",
      "updated_date": "2024-06-20 14:20:50 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T22:49:55.203720"
    },
    {
      "arxiv_id": "2406.14343v5",
      "title": "IWISDM: Assessing instruction following in multimodal models at scale",
      "title_zh": "IWISDM：大规模评估多模态模型的指令遵循能力",
      "authors": [
        "Xiaoxuan Lei",
        "Lucas Gomez",
        "Hao Yuan Bai",
        "Pouya Bashivan"
      ],
      "abstract": "The ability to perform complex tasks from detailed instructions is a key to\nmany remarkable achievements of our species. As humans, we are not only capable\nof performing a wide variety of tasks but also very complex ones that may\nentail hundreds or thousands of steps to complete. Large language models and\ntheir more recent multimodal counterparts that integrate textual and visual\ninputs have achieved unprecedented success in performing complex tasks. Yet,\nmost existing benchmarks are largely confined to single-modality inputs (either\ntext or vision), narrowing the scope of multimodal assessments, particularly\nfor instruction-following in multimodal contexts. To bridge this gap, we\nintroduce the instructed-Virtual VISual Decision Making (iWISDM) environment\nengineered to generate a limitless array of vision-language tasks of varying\ncomplexity. Using iWISDM, we compiled three distinct benchmarks of instruction\nfollowing visual tasks across varying complexity levels and evaluated several\nnewly developed multimodal models on these benchmarks. Our findings establish\niWISDM as a robust benchmark for assessing the instructional adherence of both\nexisting and emergent multimodal models and highlight a large gap between these\nmodels' ability to precisely follow instructions with that of humans.The code\nof iWISDM is available on GitHub at https://github.com/BashivanLab/iWISDM.",
      "tldr_zh": "本研究针对多模态模型的指令遵循能力，引入了 instructed-Virtual VISual Decision Making (iWISDM) 环境，该系统能生成无限的视觉-语言任务，以弥补现有基准局限于单一模态输入的不足。\n通过 iWISDM，研究者编译了三个不同复杂度的基准，并评估了多种多模态模型的表现。\n结果表明，这些模型在精确遵循指令方面与人类存在显著差距，iWISDM 作为一种可扩展的评估工具，为未来多模态模型的发展提供了可靠的基准。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2406.14343v5",
      "published_date": "2024-06-20 14:09:54 UTC",
      "updated_date": "2024-07-22 03:25:19 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T22:50:06.641670"
    },
    {
      "arxiv_id": "2406.14336v1",
      "title": "Exploring Spatial Representations in the Historical Lake District Texts with LLM-based Relation Extraction",
      "title_zh": "翻译失败",
      "authors": [
        "Erum Haris",
        "Anthony G. Cohn",
        "John G. Stell"
      ],
      "abstract": "Navigating historical narratives poses a challenge in unveiling the spatial\nintricacies of past landscapes. The proposed work addresses this challenge\nwithin the context of the English Lake District, employing the Corpus of the\nLake District Writing. The method utilizes a generative pre-trained transformer\nmodel to extract spatial relations from the textual descriptions in the corpus.\nThe study applies this large language model to understand the spatial\ndimensions inherent in historical narratives comprehensively. The outcomes are\npresented as semantic triples, capturing the nuanced connections between\nentities and locations, and visualized as a network, offering a graphical\nrepresentation of the spatial narrative. The study contributes to a deeper\ncomprehension of the English Lake District's spatial tapestry and provides an\napproach to uncovering spatial relations within diverse historical contexts.",
      "tldr_zh": "本研究探讨了使用LLM-based Relation Extraction方法，从历史英语湖区文本中提取空间关系，以揭示过去景观的空间复杂性。研究基于Corpus of the Lake District Writing语料库，运用生成预训练Transformer模型分析文本描述，并将结果转化为语义 triples，捕捉实体和位置之间的细微连接。最终，通过网络可视化呈现这些空间叙事，提供了一种通用方法来加深对历史语境中空间维度的理解，并提升了相关领域的分析能力。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2406.14336v1",
      "published_date": "2024-06-20 14:04:59 UTC",
      "updated_date": "2024-06-20 14:04:59 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T22:50:18.491568"
    },
    {
      "arxiv_id": "2406.14335v1",
      "title": "Self-supervised Interpretable Concept-based Models for Text Classification",
      "title_zh": "自监督可解释概念模型用于文本分类",
      "authors": [
        "Francesco De Santis",
        "Philippe Bich",
        "Gabriele Ciravegna",
        "Pietro Barbiero",
        "Danilo Giordano",
        "Tania Cerquitelli"
      ],
      "abstract": "Despite their success, Large-Language Models (LLMs) still face criticism as\ntheir lack of interpretability limits their controllability and reliability.\nTraditional post-hoc interpretation methods, based on attention and\ngradient-based analysis, offer limited insight into the model's decision-making\nprocesses. In the image field, Concept-based models have emerged as\nexplainable-by-design architectures, employing human-interpretable features as\nintermediate representations. However, these methods have not been yet adapted\nto textual data, mainly because they require expensive concept annotations,\nwhich are impractical for real-world text data. This paper addresses this\nchallenge by proposing a self-supervised Interpretable Concept Embedding Models\n(ICEMs). We leverage the generalization abilities of LLMs to predict the\nconcepts labels in a self-supervised way, while we deliver the final\npredictions with an interpretable function. The results of our experiments show\nthat ICEMs can be trained in a self-supervised way achieving similar\nperformance to fully supervised concept-based models and end-to-end black-box\nones. Additionally, we show that our models are (i) interpretable, offering\nmeaningful logical explanations for their predictions; (ii) interactable,\nallowing humans to modify intermediate predictions through concept\ninterventions; and (iii) controllable, guiding the LLMs' decoding process to\nfollow a required decision-making path.",
      "tldr_zh": "这篇论文针对 Large Language Models (LLMs) 的可解释性不足问题，提出了一种自监督的 Interpretable Concept Embedding Models (ICEMs)，用于文本分类，以避免传统基于注意力或梯度的方法提供有限洞见。\nICEMs 利用 LLMs 的泛化能力，通过自监督方式预测概念标签，并结合可解释函数生成最终预测，从而无需昂贵的概念标注。\n实验结果表明，ICEMs 的性能与完全监督的 Concept-based 模型或端到端黑盒模型相当，同时具备可解释性（提供逻辑解释）、交互性（通过概念干预修改预测）和可控性（引导 LLMs 的解码过程）。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2406.14335v1",
      "published_date": "2024-06-20 14:04:53 UTC",
      "updated_date": "2024-06-20 14:04:53 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T22:50:31.326771"
    },
    {
      "arxiv_id": "2406.14319v2",
      "title": "LiveMind: Low-latency Large Language Models with Simultaneous Inference",
      "title_zh": "翻译失败",
      "authors": [
        "Chuangtao Chen",
        "Grace Li Zhang",
        "Xunzhao Yin",
        "Cheng Zhuo",
        "Ulf Schlichtmann",
        "Bing Li"
      ],
      "abstract": "In this paper, we introduce LiveMind, a novel low-latency inference framework\nfor large language model (LLM) inference which enables LLMs to perform\ninferences with incomplete user input. By reallocating computational processes\nto the input phase, a substantial reduction in latency is achieved, thereby\nsignificantly enhancing the interactive experience for users of LLMs. The\nframework adeptly manages the visibility of the streaming input to the model,\nallowing it to infer from incomplete user input or await additional content.\nCompared with traditional inference methods on complete user input, our\napproach demonstrates an average reduction in response latency of 84.0% on the\nMMLU dataset and 71.6% on the MMLU-Pro dataset, while maintaining comparable\naccuracy. Additionally, our framework facilitates collaborative inference and\noutput across different models. By employing an large LLM for inference and a\nsmall LLM for output, we achieve an average 37% reduction in response latency,\nalongside a 4.30% improvement in accuracy on the MMLU-Pro dataset compared with\nthe baseline. The proposed LiveMind framework advances the field of human-AI\ninteraction by enabling more responsive and efficient communication between\nusers and AI systems.",
      "tldr_zh": "本研究引入了LiveMind，一种低延迟的LLM推理框架，通过同时推理(simultaneous inference)机制，将计算过程重新分配到输入阶段，从而在用户输入不完整时进行推理，并显著减少响应延迟。相比传统方法，LiveMind在MMLU数据集上平均降低84.0%的延迟，在MMLU-Pro数据集上降低71.6%，同时保持了相似的准确率。该框架还支持不同模型的协作推理，例如使用大LLM进行推理和小LLM进行输出，实现了平均37%的延迟减少和4.30%的准确率提升，最终提升了人-AI交互的响应性和效率。",
      "categories": [
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2406.14319v2",
      "published_date": "2024-06-20 13:52:30 UTC",
      "updated_date": "2024-11-05 18:43:57 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T22:50:41.942151"
    },
    {
      "arxiv_id": "2406.14318v1",
      "title": "The Fire Thief Is Also the Keeper: Balancing Usability and Privacy in Prompts",
      "title_zh": "窃火者也是守护者：在提示中平衡可用性和隐私",
      "authors": [
        "Zhili Shen",
        "Zihang Xi",
        "Ying He",
        "Wei Tong",
        "Jingyu Hua",
        "Sheng Zhong"
      ],
      "abstract": "The rapid adoption of online chatbots represents a significant advancement in\nartificial intelligence. However, this convenience brings considerable privacy\nconcerns, as prompts can inadvertently contain sensitive information exposed to\nlarge language models (LLMs). Limited by high computational costs, reduced task\nusability, and excessive system modifications, previous works based on local\ndeployment, embedding perturbation, and homomorphic encryption are inapplicable\nto online prompt-based LLM applications.\n  To address these issues, this paper introduces Prompt Privacy Sanitizer\n(i.e., ProSan), an end-to-end prompt privacy protection framework that can\nproduce anonymized prompts with contextual privacy removed while maintaining\ntask usability and human readability. It can also be seamlessly integrated into\nthe online LLM service pipeline. To achieve high usability and dynamic\nanonymity, ProSan flexibly adjusts its protection targets and strength based on\nthe importance of the words and the privacy leakage risk of the prompts.\nAdditionally, ProSan is capable of adapting to diverse computational resource\nconditions, ensuring privacy protection even for mobile devices with limited\ncomputing power. Our experiments demonstrate that ProSan effectively removes\nprivate information across various tasks, including question answering, text\nsummarization, and code generation, with minimal reduction in task performance.",
      "tldr_zh": "这篇论文探讨了在线聊天机器人中提示隐私问题，即用户提示可能暴露敏感信息给大型语言模型(LLMs)，而现有方法（如本地部署或同态加密）因计算成本高和可用性低而不适用。论文提出 Prompt Privacy Sanitizer (ProSan)，一个端到端框架，能动态调整保护强度根据单词重要性和隐私风险，生成匿名提示的同时保持任务可用性和人类可读性，并适应各种计算资源环境。实验结果显示，ProSan 在问答、文本总结和代码生成等任务中有效移除私人信息，而任务性能下降最小，从而实现了隐私与可用性的平衡。",
      "categories": [
        "cs.CR",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.CR",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2406.14318v1",
      "published_date": "2024-06-20 13:52:25 UTC",
      "updated_date": "2024-06-20 13:52:25 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T22:50:54.886788"
    },
    {
      "arxiv_id": "2406.14314v3",
      "title": "Identifying User Goals from UI Trajectories",
      "title_zh": "翻译失败",
      "authors": [
        "Omri Berkovitch",
        "Sapir Caduri",
        "Noam Kahlon",
        "Anatoly Efros",
        "Avi Caciularu",
        "Ido Dagan"
      ],
      "abstract": "Identifying underlying user goals and intents has been recognized as valuable\nin various personalization-oriented settings, such as personalized agents,\nimproved search responses, advertising, user analytics, and more. In this\npaper, we propose a new task goal identification from observed UI trajectories\naiming to infer the user's detailed intentions when performing a task within UI\nenvironments. To support this task, we also introduce a novel evaluation\nmethodology designed to assess whether two intent descriptions can be\nconsidered paraphrases within a specific UI environment. Furthermore, we\ndemonstrate how this task can leverage datasets designed for the inverse\nproblem of UI automation, utilizing Android and web datasets for our\nexperiments. To benchmark this task, we compare the performance of humans and\nstate-of-the-art models, specifically GPT-4 and Gemini-1.5 Pro, using our\nproposed metric. The results reveal that both Gemini and GPT underperform\nrelative to human performance, underscoring the challenge of the proposed task\nand the significant room for improvement. This work highlights the importance\nof goal identification within UI trajectories, providing a foundation for\nfurther exploration and advancement in this area.",
      "tldr_zh": "本研究提出一个新任务：从观察到的 UI trajectories 中推断用户的详细意图，以支持个性化应用如智能代理和用户分析。为此，论文引入了一种新评估方法，用于判断意图描述在特定 UI 环境中的 paraphrases 关系，并利用 Android 和 web 数据集来测试该任务。实验比较了人类、GPT-4 和 Gemini-1.5 Pro 的性能，结果显示 AI 模型的表现低于人类，突显了任务的挑战性和改进潜力。该工作为 UI trajectories 中的目标识别奠定了基础，促进相关领域的进一步发展。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2406.14314v3",
      "published_date": "2024-06-20 13:46:10 UTC",
      "updated_date": "2025-03-03 15:47:10 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T22:51:07.228120"
    },
    {
      "arxiv_id": "2406.14313v2",
      "title": "Iterative Repair with Weak Verifiers for Few-shot Transfer in KBQA with Unanswerability",
      "title_zh": "翻译失败",
      "authors": [
        "Riya Sawhney",
        "Samrat Yadav",
        "Indrajit Bhattacharya",
        "Mausam"
      ],
      "abstract": "Real-world applications of KBQA require models to handle unanswerable\nquestions with a limited volume of in-domain labeled training data. We propose\nthe novel task of few-shot transfer for KBQA with unanswerable questions and\ncontribute two new datasets for performance evaluation. We present FUn-FuSIC -\na novel solution for our task that extends FuSIC KBQA, the state-of-the-art\nfew-shot transfer model for answerable-only KBQA. We first note that\nFuSIC-KBQA's iterative repair makes a strong assumption that all questions are\nunanswerable. As a remedy, we propose Feedback for Unanswerability (FUn), which\nuses iterative repair using feedback from a suite of strong and weak verifiers,\nand an adaptation of self consistency for unanswerabilty to better assess the\nanswerability of a question. Our experiments show that FUn-FuSIC significantly\noutperforms suitable adaptations of multiple LLM based and supervised SoTA\nmodels on our task, while establishing a new SoTA for answerable few-shot\ntransfer as well.",
      "tldr_zh": "该研究提出了一种新的任务：处理知识库问答（KBQA）中的不可回答问题，并实现 few-shot transfer，同时贡献了两个新数据集用于评估。作者引入了 FUn-FuSIC 方法，这是一种扩展 FuSIC KBQA 的框架，通过迭代修复和 Feedback for Unanswerability (FUn) 机制，利用强弱 verifiers 的反馈以及自一致性适应来更好地评估问题可回答性。实验结果显示，FUn-FuSIC 在该任务上显著优于多种 LLM 和监督 SoTA 模型的适应版本，同时在可回答的 few-shot transfer 上建立了新的 SoTA。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2406.14313v2",
      "published_date": "2024-06-20 13:43:38 UTC",
      "updated_date": "2025-02-21 14:35:19 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T22:51:20.094996"
    },
    {
      "arxiv_id": "2406.14312v1",
      "title": "Infusing clinical knowledge into tokenisers for language models",
      "title_zh": "将临床知识注入",
      "authors": [
        "Abul Hasan",
        "Jinge Wu",
        "Quang Ngoc Nguyen",
        "Salomé Andres",
        "Imane Guellil",
        "Huayu Zhang",
        "Arlene Casey",
        "Beatrice Alex",
        "Bruce Guthrie",
        "Honghan Wu"
      ],
      "abstract": "This study introduces a novel knowledge enhanced tokenisation mechanism,\nK-Tokeniser, for clinical text processing. Technically, at initialisation\nstage, K-Tokeniser populates global representations of tokens based on semantic\ntypes of domain concepts (such as drugs or diseases) from either a domain\nontology like Unified Medical Language System or the training data of the task\nrelated corpus. At training or inference stage, sentence level localised\ncontext will be utilised for choosing the optimal global token representation\nto realise the semantic-based tokenisation. To avoid pretraining using the new\ntokeniser, an embedding initialisation approach is proposed to generate\nrepresentations for new tokens. Using three transformer-based language models,\na comprehensive set of experiments are conducted on four real-world datasets\nfor evaluating K-Tokeniser in a wide range of clinical text analytics tasks\nincluding clinical concept and relation extraction, automated clinical coding,\nclinical phenotype identification, and clinical research article\nclassification. Overall, our models demonstrate consistent improvements over\ntheir counterparts in all tasks. In particular, substantial improvements are\nobserved in the automated clinical coding task with 13\\% increase on Micro\n$F_1$ score. Furthermore, K-Tokeniser also shows significant capacities in\nfacilitating quicker converge of language models. Specifically, using\nK-Tokeniser, the language models would only require 50\\% of the training data\nto achieve the best performance of the baseline tokeniser using all training\ndata in the concept extraction task and less than 20\\% of the data for the\nautomated coding task. It is worth mentioning that all these improvements\nrequire no pre-training process, making the approach generalisable.",
      "tldr_zh": "本研究提出了一种知识增强分词机制 K-Tokeniser，用于改进基于 transformer 的语言模型在临床文本处理中的性能。具体而言，K-Tokeniser 在初始化阶段利用领域本体（如 Unified Medical Language System）或训练数据填充 token 的全局表示，并在训练或推理阶段结合句子级本地化上下文选择最佳语义表示，同时采用嵌入初始化方法避免重新预训练。实验在四个真实数据集上评估了该机制在临床概念和关系提取、自动临床编码、临床表型识别以及临床研究文章分类等任务中的表现，结果显示模型在所有任务中均有显著提升，尤其在自动临床编码任务中，Micro F1 分数提高了 13%。此外，K-Tokeniser 显著加速了语言模型的收敛，仅需 50% 的训练数据即可在概念提取任务中达到基线性能，在自动编码任务中只需不到 20% 的数据，且无需预训练过程，使其具有高度通用性。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "18 pages, 6 figures",
      "pdf_url": "http://arxiv.org/pdf/2406.14312v1",
      "published_date": "2024-06-20 13:43:03 UTC",
      "updated_date": "2024-06-20 13:43:03 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T22:51:33.868964"
    },
    {
      "arxiv_id": "2406.14310v1",
      "title": "Cross-level Requirement Traceability: A Novel Approach Integrating Bag-of-Words and Word Embedding for Enhanced Similarity Functionality",
      "title_zh": "翻译失败",
      "authors": [
        "Baher Mohammad",
        "Riad Sonbol",
        "Ghaida Rebdawi"
      ],
      "abstract": "Requirement traceability is the process of identifying the inter-dependencies\nbetween requirements. It poses a significant challenge when conducted manually,\nespecially when dealing with requirements at various levels of abstraction. In\nthis work, we propose a novel approach to automate the task of linking\nhigh-level business requirements with more technical system requirements. The\nproposed approach begins by representing each requirement using a Bag of-Words\n(BOW) model combined with the Term Frequency-Inverse Document Frequency\n(TF-IDF) scoring function. Then, we suggested an enhanced cosine similarity\nthat uses recent advances in word embedding representation to correct\ntraditional cosine similarity function limitations. To evaluate the\neffectiveness of our approach, we conducted experiments on three well-known\ndatasets: COEST, WARC(NFR), and WARC(FRS). The results demonstrate that our\napproach significantly improves efficiency compared to existing methods. We\nachieved better results with an increase of approximately 18.4% in one of the\ndatasets, as measured by the F2 score.",
      "tldr_zh": "该论文提出了一种新方法，用于自动化跨级别需求追踪（Requirement Traceability），以链接高层业务需求和系统需求，解决手动追踪的挑战。方法结合Bag-of-Words (BOW)模型和Term Frequency-Inverse Document Frequency (TF-IDF)评分来表示需求，并通过Word Embedding增强Cosine Similarity，修正传统相似度计算的局限性。在COEST、WARC(NFR)和WARC(FRS)数据集上的实验表明，该方法显著提高了效率，并在其中一个数据集上F2 score提升约18.4%。",
      "categories": [
        "cs.AI",
        "cs.SE"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2406.14310v1",
      "published_date": "2024-06-20 13:41:02 UTC",
      "updated_date": "2024-06-20 13:41:02 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T22:51:44.296140"
    },
    {
      "arxiv_id": "2406.14302v3",
      "title": "Identifiable Exchangeable Mechanisms for Causal Structure and Representation Learning",
      "title_zh": "用于因果结构和表示学习的可识别可交换机制",
      "authors": [
        "Patrik Reizinger",
        "Siyuan Guo",
        "Ferenc Huszár",
        "Bernhard Schölkopf",
        "Wieland Brendel"
      ],
      "abstract": "Identifying latent representations or causal structures is important for good\ngeneralization and downstream task performance. However, both fields have been\ndeveloped rather independently. We observe that several methods in both\nrepresentation and causal structure learning rely on the same data-generating\nprocess (DGP), namely, exchangeable but not i.i.d. (independent and identically\ndistributed) data. We provide a unified framework, termed Identifiable\nExchangeable Mechanisms (IEM), for representation and structure learning under\nthe lens of exchangeability. IEM provides new insights that let us relax the\nnecessary conditions for causal structure identification in exchangeable\nnon--i.i.d. data. We also demonstrate the existence of a duality condition in\nidentifiable representation learning, leading to new identifiability results.\nWe hope this work will pave the way for further research in causal\nrepresentation learning.",
      "tldr_zh": "该论文探讨了潜在表示和因果结构（causal structure）的识别，以提升泛化和下游任务性能，并将表示学习（representation learning）和因果结构学习统一起来。作者提出Identifiable Exchangeable Mechanisms (IEM)框架，基于可交换但非 i.i.d.（independent and identically distributed）的数据生成过程，提供新见解以放松因果结构识别的必要条件。IEM还揭示了可识别表示学习中的对偶条件（duality condition），带来新的可识别性结果，并为因果表示学习（causal representation learning）领域铺平道路。",
      "categories": [
        "stat.ML",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "stat.ML",
      "comment": "ICLR2025 camera ready",
      "pdf_url": "http://arxiv.org/pdf/2406.14302v3",
      "published_date": "2024-06-20 13:30:25 UTC",
      "updated_date": "2025-02-08 06:19:37 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T22:51:55.078768"
    },
    {
      "arxiv_id": "2406.14297v2",
      "title": "AI in Space for Scientific Missions: Strategies for Minimizing Neural-Network Model Upload",
      "title_zh": "空间AI用于科学任务：最小化神经网络模型上传的策略",
      "authors": [
        "Jonah Ekelund",
        "Ricardo Vinuesa",
        "Yuri Khotyaintsev",
        "Pierre Henri",
        "Gian Luca Delzanno",
        "Stefano Markidis"
      ],
      "abstract": "Artificial Intelligence (AI) has the potential to revolutionize space\nexploration by delegating several spacecraft decisions to an onboard AI instead\nof relying on ground control and predefined procedures. It is likely that there\nwill be an AI/ML Processing Unit onboard the spacecraft running an inference\nengine. The neural-network will have pre-installed parameters that can be\nupdated onboard by uploading, by telecommands, parameters obtained by training\non the ground. However, satellite uplinks have limited bandwidth and\ntransmissions can be costly. Furthermore, a mission operating with a suboptimal\nneural network will miss out on valuable scientific data. Smaller networks can\nthereby decrease the uplink cost, while increasing the value of the scientific\ndata that is downloaded. In this work, we evaluate and discuss the use of\nreduced-precision and bare-minimum neural networks to reduce the time for\nupload. As an example of an AI use case, we focus on the NASA's Magnetosperic\nMultiScale (MMS) mission. We show how an AI onboard could be used in the\nEarth's magnetosphere to classify data to selectively downlink higher value\ndata or to recognize a region-of-interest to trigger a burst-mode, collecting\ndata at a high-rate. Using a simple filtering scheme and algorithm, we show how\nthe start and end of a region-of-interest can be detected in on a stream of\nclassifications. To provide the classifications, we use an established\nConvolutional Neural Network (CNN) trained to an accuracy >94%. We also show\nhow the network can be reduced to a single linear layer and trained to the same\naccuracy as the established CNN. Thereby, reducing the overall size of the\nmodel by up to 98.9%. We further show how each network can be reduced by up to\n75% of its original size, by using lower-precision formats to represent the\nnetwork parameters, with a change in accuracy of less than 0.6 percentage\npoints.",
      "tldr_zh": "这篇论文探讨了在太空科学任务中使用 AI 最小化神经网络模型上传策略，以应对卫星上行链路的带宽限制和传输成本问题。研究以 NASA's Magnetospheric Multiscale (MMS) 任务为例，采用 reduced-precision 和 bare-minimum 神经网络方法，包括将 CNN 模型简化为单线性层，从而将模型大小减少高达 98.9%，并通过低精度格式进一步压缩至原大小的 25%，准确率仅下降不到 0.6%。这些策略能提升 AI 在 spacecraft 上的决策能力，实现数据分类和选择性下载，提高科学数据价值。",
      "categories": [
        "cs.AI",
        "astro-ph.IM"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2406.14297v2",
      "published_date": "2024-06-20 13:24:52 UTC",
      "updated_date": "2024-08-30 07:49:24 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T22:52:08.897757"
    },
    {
      "arxiv_id": "2406.14294v2",
      "title": "DASB - Discrete Audio and Speech Benchmark",
      "title_zh": "DASB - 离散音频和语音基准",
      "authors": [
        "Pooneh Mousavi",
        "Luca Della Libera",
        "Jarod Duret",
        "Artem Ploujnikov",
        "Cem Subakan",
        "Mirco Ravanelli"
      ],
      "abstract": "Discrete audio tokens have recently gained considerable attention for their\npotential to connect audio and language processing, enabling the creation of\nmodern multimodal large language models. Ideal audio tokens must effectively\npreserve phonetic and semantic content along with paralinguistic information,\nspeaker identity, and other details. While several types of audio tokens have\nbeen recently proposed, identifying the optimal tokenizer for various tasks is\nchallenging due to the inconsistent evaluation settings in existing studies. To\naddress this gap, we release the Discrete Audio and Speech Benchmark (DASB), a\ncomprehensive leaderboard for benchmarking discrete audio tokens across a wide\nrange of discriminative tasks, including speech recognition, speaker\nidentification and verification, emotion recognition, keyword spotting, and\nintent classification, as well as generative tasks such as speech enhancement,\nseparation, and text-to-speech. Our results show that, on average, semantic\ntokens outperform compression tokens across most discriminative and generative\ntasks. However, the performance gap between semantic tokens and standard\ncontinuous representations remains substantial, highlighting the need for\nfurther research in this field.",
      "tldr_zh": "这篇论文引入了 Discrete Audio and Speech Benchmark (DASB)，一个全面的基准平台，用于评估离散音频标记在音频和语言处理任务中的性能，以解决现有研究中评估设置不一致的问题。DASB 涵盖了语音识别、说话者识别、情感识别、关键词检测和意图分类等辨别任务，以及语音增强、分离和文本到语音等生成任务。结果表明，semantic tokens 在大多数任务中优于 compression tokens，但与标准连续表示相比仍有显著性能差距，突显了该领域进一步研究的必要性。",
      "categories": [
        "cs.SD",
        "cs.AI",
        "eess.AS"
      ],
      "primary_category": "cs.SD",
      "comment": "9 pages, 5 tables",
      "pdf_url": "http://arxiv.org/pdf/2406.14294v2",
      "published_date": "2024-06-20 13:23:27 UTC",
      "updated_date": "2024-06-21 17:07:17 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T22:52:21.461778"
    },
    {
      "arxiv_id": "2406.14288v1",
      "title": "Revisiting Modularity Maximization for Graph Clustering: A Contrastive Learning Perspective",
      "title_zh": "翻译失败",
      "authors": [
        "Yunfei Liu",
        "Jintang Li",
        "Yuehe Chen",
        "Ruofan Wu",
        "Ericbk Wang",
        "Jing Zhou",
        "Sheng Tian",
        "Shuheng Shen",
        "Xing Fu",
        "Changhua Meng",
        "Weiqiang Wang",
        "Liang Chen"
      ],
      "abstract": "Graph clustering, a fundamental and challenging task in graph mining, aims to\nclassify nodes in a graph into several disjoint clusters. In recent years,\ngraph contrastive learning (GCL) has emerged as a dominant line of research in\ngraph clustering and advances the new state-of-the-art. However, GCL-based\nmethods heavily rely on graph augmentations and contrastive schemes, which may\npotentially introduce challenges such as semantic drift and scalability issues.\nAnother promising line of research involves the adoption of modularity\nmaximization, a popular and effective measure for community detection, as the\nguiding principle for clustering tasks. Despite the recent progress, the\nunderlying mechanism of modularity maximization is still not well understood.\nIn this work, we dig into the hidden success of modularity maximization for\ngraph clustering. Our analysis reveals the strong connections between\nmodularity maximization and graph contrastive learning, where positive and\nnegative examples are naturally defined by modularity. In light of our results,\nwe propose a community-aware graph clustering framework, coined MAGI, which\nleverages modularity maximization as a contrastive pretext task to effectively\nuncover the underlying information of communities in graphs, while avoiding the\nproblem of semantic drift. Extensive experiments on multiple graph datasets\nverify the effectiveness of MAGI in terms of scalability and clustering\nperformance compared to state-of-the-art graph clustering methods. Notably,\nMAGI easily scales a sufficiently large graph with 100M nodes while\noutperforming strong baselines.",
      "tldr_zh": "该研究重新审视了模块度最大化(modularity maximization)在图聚类(graph clustering)中的作用，从图对比学习(graph contrastive learning, GCL)的视角出发，分析其与 GCL 的内在联系，即通过模块度自然定义正负样本。论文提出了一种社区感知框架 MAGI，将模块度最大化作为对比预训练任务，避免了 GCL 中常见的语义漂移(semantic drift)和可伸缩性问题。实验结果显示，MAGI 在多个图数据集上表现出色，不仅在聚类性能上超越现有基线，还能轻松处理规模达 100M 节点的超大图。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "KDD 2024 research track. Code available at\n  https://github.com/EdisonLeeeee/MAGI",
      "pdf_url": "http://arxiv.org/pdf/2406.14288v1",
      "published_date": "2024-06-20 13:14:44 UTC",
      "updated_date": "2024-06-20 13:14:44 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T22:52:31.991897"
    },
    {
      "arxiv_id": "2406.14284v1",
      "title": "VAIYAKARANA : A Benchmark for Automatic Grammar Correction in Bangla",
      "title_zh": "翻译失败",
      "authors": [
        "Pramit Bhattacharyya",
        "Arnab Bhattacharya"
      ],
      "abstract": "Bangla (Bengali) is the fifth most spoken language globally and, yet, the\nproblem of automatic grammar correction in Bangla is still in its nascent\nstage. This is mostly due to the need for a large corpus of grammatically\nincorrect sentences, with their corresponding correct counterparts. The present\nstate-of-the-art techniques to curate a corpus for grammatically wrong\nsentences involve random swapping, insertion and deletion of words.\nHowever,these steps may not always generate grammatically wrong sentences in\nBangla. In this work, we propose a pragmatic approach to generate grammatically\nwrong sentences in Bangla. We first categorize the different kinds of errors in\nBangla into 5 broad classes and 12 finer classes. We then use these to generate\ngrammatically wrong sentences systematically from a correct sentence. This\napproach can generate a large number of wrong sentences and can, thus, mitigate\nthe challenge of lacking a large corpus for neural networks. We provide a\ndataset, Vaiyakarana, consisting of 92,830 grammatically incorrect sentences as\nwell as 18,426 correct sentences. We also collected 619 human-generated\nsentences from essays written by Bangla native speakers. This helped us to\nunderstand errors that are more frequent. We evaluated our corpus against\nneural models and LLMs and also benchmark it against human evaluators who are\nnative speakers of Bangla. Our analysis shows that native speakers are far more\naccurate than state-of-the-art models to detect whether the sentence is\ngrammatically correct. Our methodology of generating erroneous sentences can be\napplied for most other Indian languages as well.",
      "tldr_zh": "该研究针对Bangla语言的自动语法纠正问题，提出了一种实用方法，通过将错误类型分为5个大类和12个细类，从正确句子中系统生成语法错误句子，从而解决大规模语料缺乏的挑战。研究构建了Vaiyakarana数据集，包括92,830个语法错误句子、18,426个正确句子，以及619个由Bangla母语者生成的人为错误句子，并对神经模型、大型语言模型和人类评估者进行了基准测试。结果显示，母语者比最先进模型更准确地检测语法错误，且该方法可扩展应用于其他印度语言，为Bangla的自动语法纠正领域提供了重要基准。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2406.14284v1",
      "published_date": "2024-06-20 13:09:29 UTC",
      "updated_date": "2024-06-20 13:09:29 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T22:52:44.566051"
    },
    {
      "arxiv_id": "2406.14283v4",
      "title": "Q*: Improving Multi-step Reasoning for LLMs with Deliberative Planning",
      "title_zh": "翻译失败",
      "authors": [
        "Chaojie Wang",
        "Yanchen Deng",
        "Zhiyi Lyu",
        "Liang Zeng",
        "Jujie He",
        "Shuicheng Yan",
        "Bo An"
      ],
      "abstract": "Large Language Models (LLMs) have demonstrated impressive capability in many\nnatural language tasks. However, the auto-regressive generation process makes\nLLMs prone to produce errors, hallucinations and inconsistent statements when\nperforming multi-step reasoning. In this paper, by casting multi-step reasoning\nof LLMs as a heuristic search problem, we aim to alleviate the pathology by\nintroducing Q*, a general, versatile and agile framework for guiding LLMs\ndecoding process with deliberative planning. By learning a plug-and-play\nQ-value model as heuristic function for estimating expected future rewards, our\nQ* can effectively guide LLMs to select the most promising next reasoning step\nwithout fine-tuning LLMs for the current task, which avoids the significant\ncomputational overhead and potential risk of performance degeneration on other\ntasks. Extensive experiments on GSM8K, MATH and MBPP demonstrate the\nsuperiority of our method, contributing to improving the reasoning performance\nof existing open-source LLMs.",
      "tldr_zh": "本研究针对大型语言模型（LLMs）在多步推理中易出现错误、幻觉和不一致问题的挑战，提出Q*框架，将推理过程视为启发式搜索问题，并通过deliberative planning引导LLMs的解码。Q*利用一个plug-and-play的Q-value model作为启发式函数，估计未来奖励以选择最优的下一步推理，从而避免了对LLMs进行任务特定微调带来的计算开销和性能风险。在GSM8K、MATH和MBPP数据集上的广泛实验显示，该方法显著提升了现有开源LLMs的推理性能。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2406.14283v4",
      "published_date": "2024-06-20 13:08:09 UTC",
      "updated_date": "2024-07-22 10:01:49 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T22:52:56.072724"
    },
    {
      "arxiv_id": "2406.14282v3",
      "title": "Learning to Plan for Retrieval-Augmented Large Language Models from Knowledge Graphs",
      "title_zh": "翻译失败",
      "authors": [
        "Junjie Wang",
        "Mingyang Chen",
        "Binbin Hu",
        "Dan Yang",
        "Ziqi Liu",
        "Yue Shen",
        "Peng Wei",
        "Zhiqiang Zhang",
        "Jinjie Gu",
        "Jun Zhou",
        "Jeff Z. Pan",
        "Wen Zhang",
        "Huajun Chen"
      ],
      "abstract": "Improving the performance of large language models (LLMs) in complex\nquestion-answering (QA) scenarios has always been a research focal point.\nRecent studies have attempted to enhance LLMs' performance by combining\nstep-wise planning with external retrieval. While effective for advanced models\nlike GPT-3.5, smaller LLMs face challenges in decomposing complex questions,\nnecessitating supervised fine-tuning. Previous work has relied on manual\nannotation and knowledge distillation from teacher LLMs, which are\ntime-consuming and not accurate enough. In this paper, we introduce a novel\nframework for enhancing LLMs' planning capabilities by using planning data\nderived from knowledge graphs (KGs). LLMs fine-tuned with this data have\nimproved planning capabilities, better equipping them to handle complex QA\ntasks that involve retrieval. Evaluations on multiple datasets, including our\nnewly proposed benchmark, highlight the effectiveness of our framework and the\nbenefits of KG-derived planning data.",
      "tldr_zh": "这篇论文提出了一种新框架，使用从知识图谱(KGs)派生的规划数据来提升检索增强大型语言模型(LLMs)的规划能力，旨在解决小型LLMs在分解复杂问答(QA)任务时面临的挑战。框架通过监督微调LLMs，使其更好地处理步进规划和外部检索相结合的场景，避免了依赖手动标注和知识蒸馏的低效问题。在多个数据集上的评估，包括一个新提出的基准，证明了该方法显著提高了LLMs的性能和准确性。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "EMNLP2024 Findings",
      "pdf_url": "http://arxiv.org/pdf/2406.14282v3",
      "published_date": "2024-06-20 13:07:38 UTC",
      "updated_date": "2024-10-23 09:42:59 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T22:53:09.773063"
    },
    {
      "arxiv_id": "2406.14281v4",
      "title": "FairX: A comprehensive benchmarking tool for model analysis using fairness, utility, and explainability",
      "title_zh": "翻译失败",
      "authors": [
        "Md Fahim Sikder",
        "Resmi Ramachandranpillai",
        "Daniel de Leng",
        "Fredrik Heintz"
      ],
      "abstract": "We present FairX, an open-source Python-based benchmarking tool designed for\nthe comprehensive analysis of models under the umbrella of fairness, utility,\nand eXplainability (XAI). FairX enables users to train benchmarking\nbias-mitigation models and evaluate their fairness using a wide array of\nfairness metrics, data utility metrics, and generate explanations for model\npredictions, all within a unified framework. Existing benchmarking tools do not\nhave the way to evaluate synthetic data generated from fair generative models,\nalso they do not have the support for training fair generative models either.\nIn FairX, we add fair generative models in the collection of our fair-model\nlibrary (pre-processing, in-processing, post-processing) and evaluation metrics\nfor evaluating the quality of synthetic fair data. This version of FairX\nsupports both tabular and image datasets. It also allows users to provide their\nown custom datasets. The open-source FairX benchmarking package is publicly\navailable at \\url{https://github.com/fahim-sikder/FairX}.",
      "tldr_zh": "我们介绍了 FairX，这是一个开源的 Python 工具，用于全面分析模型的公平性（fairness）、效用（utility）和可解释性（XAI）。FairX 允许用户训练偏差缓解模型（bias-mitigation models），使用多种公平性指标和数据效用指标评估模型性能，并为预测生成解释，所有功能集成在一个统一框架中。相比现有工具，FairX 创新性地添加了公平生成模型（包括预处理、in-processing 和后处理）的支持，以及评估合成公平数据的指标，支持表格和图像数据集，并允许自定义数据输入。该工具已在 GitHub 上公开可用（https://github.com/fahim-sikder/FairX），为模型分析提供了一个全面的基准平台。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2406.14281v4",
      "published_date": "2024-06-20 13:07:06 UTC",
      "updated_date": "2024-09-03 12:38:22 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T22:53:21.970791"
    },
    {
      "arxiv_id": "2406.14277v2",
      "title": "QPaug: Question and Passage Augmentation for Open-Domain Question Answering of LLMs",
      "title_zh": "翻译失败",
      "authors": [
        "Minsang Kim",
        "Cheoneum Park",
        "Seungjun Baek"
      ],
      "abstract": "Retrieval-augmented generation (RAG) has received much attention for\nOpen-domain question-answering (ODQA) tasks as a means to compensate for the\nparametric knowledge of large language models (LLMs). While previous approaches\nfocused on processing retrieved passages to remove irrelevant context, they\nstill rely heavily on the quality of retrieved passages which can degrade if\nthe question is ambiguous or complex. In this paper, we propose a simple yet\nefficient method called question and passage augmentation (QPaug) via LLMs for\nopen-domain QA. QPaug first decomposes the original questions into\nmultiple-step sub-questions. By augmenting the original question with detailed\nsub-questions and planning, we are able to make the query more specific on what\nneeds to be retrieved, improving the retrieval performance. In addition, to\ncompensate for the case where the retrieved passages contain distracting\ninformation or divided opinions, we augment the retrieved passages with\nself-generated passages by LLMs to guide the answer extraction. Experimental\nresults show that QPaug outperforms the previous state-of-the-art and achieves\nsignificant performance gain over existing RAG methods. The source code is\navailable at \\url{https://github.com/kmswin1/QPaug}.",
      "tldr_zh": "这篇论文提出了 QPaug，一种通过 LLMs 进行问题和段落增强的方法，用于提升开放域问答 (ODQA) 的性能。QPaug 通过将原始问题分解为多个子问题并添加详细规划，使查询更具体，从而改善检索性能；同时，它还增强检索段落，生成额外的自有段落来过滤干扰信息或分歧意见。实验结果表明，QPaug 超过了现有最先进 RAG 方法，并在性能上实现了显著提升。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "The 2024 Conference on Empirical Methods in Natural Language\n  Processing (EMNLP), Findings",
      "pdf_url": "http://arxiv.org/pdf/2406.14277v2",
      "published_date": "2024-06-20 12:59:27 UTC",
      "updated_date": "2024-09-27 12:18:34 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T22:53:32.818012"
    },
    {
      "arxiv_id": "2406.14275v2",
      "title": "Step-Back Profiling: Distilling User History for Personalized Scientific Writing",
      "title_zh": "翻译失败",
      "authors": [
        "Xiangru Tang",
        "Xingyao Zhang",
        "Yanjun Shao",
        "Jie Wu",
        "Yilun Zhao",
        "Arman Cohan",
        "Ming Gong",
        "Dongmei Zhang",
        "Mark Gerstein"
      ],
      "abstract": "Large language models (LLM) excel at a variety of natural language processing\ntasks, yet they struggle to generate personalized content for individuals,\nparticularly in real-world scenarios like scientific writing. Addressing this\nchallenge, we introduce STEP-BACK PROFILING to personalize LLMs by distilling\nuser history into concise profiles, including essential traits and preferences\nof users. To conduct the experiments, we construct a Personalized Scientific\nWriting (PSW) dataset to study multi-user personalization. PSW requires the\nmodels to write scientific papers given specialized author groups with diverse\nacademic backgrounds. As for the results, we demonstrate the effectiveness of\ncapturing user characteristics via STEP-BACK PROFILING for collaborative\nwriting. Moreover, our approach outperforms the baselines by up to 3.6 points\non the general personalization benchmark (LaMP), including 7 personalization\nLLM tasks. Our ablation studies validate the contributions of different\ncomponents in our method and provide insights into our task definition. Our\ndataset and code are available at\n\\url{https://github.com/gersteinlab/step-back-profiling}.",
      "tldr_zh": "本研究针对大型语言模型（LLM）在生成个性化内容（如科学写作）时的挑战，提出了Step-Back Profiling方法，该方法通过提炼用户历史成简洁的个人资料（包括用户特征和偏好），来实现LLM的个性化。研究构建了Personalized Scientific Writing (PSW)数据集，用于评估多用户协作写作场景中的模型性能。实验结果显示，该方法在协作写作任务中有效，并在LaMP基准测试中比基线模型高出3.6点，同时消融研究验证了方法组件的贡献。开源数据集和代码可从指定仓库获取。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2406.14275v2",
      "published_date": "2024-06-20 12:58:26 UTC",
      "updated_date": "2024-07-11 07:29:12 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T22:53:44.951982"
    },
    {
      "arxiv_id": "2406.14273v2",
      "title": "The Impact of AI on Perceived Job Decency and Meaningfulness: A Case Study",
      "title_zh": "翻译失败",
      "authors": [
        "Kuntal Ghosh",
        "Shadan Sadeghian"
      ],
      "abstract": "The proliferation of Artificial Intelligence (AI) in workplaces stands to\nchange the way humans work, with job satisfaction intrinsically linked to work\nlife. Existing research on human-AI collaboration tends to prioritize\nperformance over the experiential aspects of work. In contrast, this paper\nexplores the impact of AI on job decency and meaningfulness in workplaces.\nThrough interviews in the Information Technology (IT) domain, we not only\nexamined the current work environment, but also explored the perceived\nevolution of the workplace ecosystem with the introduction of an AI. Findings\nfrom the preliminary exploratory study reveal that respondents tend to\nvisualize a workplace where humans continue to play a dominant role, even with\nthe introduction of advanced AIs. In this prospective scenario, AI is seen as\nserving as a complement rather than replacing the human workforce. Furthermore,\nrespondents believe that the introduction of AI will maintain or potentially\nincrease overall job satisfaction.",
      "tldr_zh": "本研究探讨了人工智能(AI)对工作体面性和意义性的影响，通过一个案例研究考察了AI在工作场所的作用。研究者通过对信息技术(IT)领域的访谈，分析了当前工作环境以及引入AI后的潜在变化。初步发现显示，受访者认为AI将作为人类劳动力的补充而非取代，人类仍将保持主导地位，且AI的引入可能维持或提升整体工作满意度。该研究填补了现有AI合作研究的空白，强调了工作体验的重要性。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2406.14273v2",
      "published_date": "2024-06-20 12:52:57 UTC",
      "updated_date": "2024-06-21 07:31:56 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T22:53:56.242967"
    },
    {
      "arxiv_id": "2406.14267v1",
      "title": "On the Evaluation Practices in Multilingual NLP: Can Machine Translation Offer an Alternative to Human Translations?",
      "title_zh": "关于多语言自然语言处理中的评估实践：机器翻译能否作为人工翻译的替代方案？",
      "authors": [
        "Rochelle Choenni",
        "Sara Rajaee",
        "Christof Monz",
        "Ekaterina Shutova"
      ],
      "abstract": "While multilingual language models (MLMs) have been trained on 100+\nlanguages, they are typically only evaluated across a handful of them due to a\nlack of available test data in most languages. This is particularly problematic\nwhen assessing MLM's potential for low-resource and unseen languages. In this\npaper, we present an analysis of existing evaluation frameworks in multilingual\nNLP, discuss their limitations, and propose several directions for more robust\nand reliable evaluation practices. Furthermore, we empirically study to what\nextent machine translation offers a {reliable alternative to human translation}\nfor large-scale evaluation of MLMs across a wide set of languages. We use a\nSOTA translation model to translate test data from 4 tasks to 198 languages and\nuse them to evaluate three MLMs. We show that while the selected subsets of\nhigh-resource test languages are generally sufficiently representative of a\nwider range of high-resource languages, we tend to overestimate MLMs' ability\non low-resource languages. Finally, we show that simpler baselines can achieve\nrelatively strong performance without having benefited from large-scale\nmultilingual pretraining.",
      "tldr_zh": "本论文分析了多语言 NLP 中的评估实践，指出现有框架因测试数据缺失而主要限于少数语言，这导致对低资源和未见语言的模型评估（如 MLMs）存在偏差，并提出更稳健的评估方向。研究者使用 SOTA 翻译模型将4个任务的测试数据翻译到198种语言，以评估三款 MLMs 的性能。结果显示，高资源语言的子集评估可能高估模型在低资源语言上的能力，同时更简单的基线模型也能在不依赖大规模多语言预训练的情况下取得较强表现。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2406.14267v1",
      "published_date": "2024-06-20 12:46:12 UTC",
      "updated_date": "2024-06-20 12:46:12 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T22:54:10.519099"
    },
    {
      "arxiv_id": "2406.14266v1",
      "title": "Intelligent Interface: Enhancing Lecture Engagement with Didactic Activity Summaries",
      "title_zh": "智能界面：通过教学活动摘要增强讲座参与度",
      "authors": [
        "Anna Wróblewska",
        "Marcel Witas",
        "Kinga Frańczak",
        "Arkadiusz Kniaź",
        "Siew Ann Cheong",
        "Tan Seng Chee",
        "Janusz Hołyst",
        "Marcin Paprzycki"
      ],
      "abstract": "Recently, multiple applications of machine learning have been introduced.\nThey include various possibilities arising when image analysis methods are\napplied to, broadly understood, video streams. In this context, a novel tool,\ndeveloped for academic educators to enhance the teaching process by automating,\nsummarizing, and offering prompt feedback on conducting lectures, has been\ndeveloped. The implemented prototype utilizes machine learning-based techniques\nto recognise selected didactic and behavioural teachers' features within\nlecture video recordings.\n  Specifically, users (teachers) can upload their lecture videos, which are\npreprocessed and analysed using machine learning models. Next, users can view\nsummaries of recognized didactic features through interactive charts and\ntables. Additionally, stored ML-based prediction results support comparisons\nbetween lectures based on their didactic content. In the developed application\ntext-based models trained on lecture transcriptions, with enhancements to the\ntranscription quality, by adopting an automatic speech recognition solution are\napplied. Furthermore, the system offers flexibility for (future) integration of\nnew/additional machine-learning models and software modules for image and video\nanalysis.",
      "tldr_zh": "这篇论文提出了一种名为Intelligent Interface的工具，旨在通过自动化总结和即时反馈来提升讲座参与度，帮助教师分析视频中的教学和行为特征。工具利用machine learning模型对上传的讲座视频进行预处理和分析，包括文本模型训练于改进后的讲座转录（通过automatic speech recognition提升质量），并以交互式图表和表格形式呈现总结结果。系统还支持讲座内容比较，并提供灵活性以整合新的machine learning模块，从而增强教学过程的效率和可扩展性。",
      "categories": [
        "cs.AI",
        "cs.HC"
      ],
      "primary_category": "cs.AI",
      "comment": "9 pages, 6 figures",
      "pdf_url": "http://arxiv.org/pdf/2406.14266v1",
      "published_date": "2024-06-20 12:45:23 UTC",
      "updated_date": "2024-06-20 12:45:23 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T22:54:21.049884"
    },
    {
      "arxiv_id": "2406.14265v1",
      "title": "VeriFlow: Modeling Distributions for Neural Network Verification",
      "title_zh": "翻译失败",
      "authors": [
        "Faried Abu Zaid",
        "Daniel Neider",
        "Mustafa Yalçıner"
      ],
      "abstract": "Formal verification has emerged as a promising method to ensure the safety\nand reliability of neural networks. Naively verifying a safety property amounts\nto ensuring the safety of a neural network for the whole input space\nirrespective of any training or test set. However, this also implies that the\nsafety of the neural network is checked even for inputs that do not occur in\nthe real-world and have no meaning at all, often resulting in spurious errors.\nTo tackle this shortcoming, we propose the VeriFlow architecture as a flow\nbased density model tailored to allow any verification approach to restrict its\nsearch to the some data distribution of interest. We argue that our\narchitecture is particularly well suited for this purpose because of two major\nproperties. First, we show that the transformation and log-density function\nthat are defined by our model are piece-wise affine. Therefore, the model\nallows the usage of verifiers based on SMT with linear arithmetic. Second,\nupper density level sets (UDL) of the data distribution take the shape of an\n$L^p$-ball in the latent space. As a consequence, representations of UDLs\nspecified by a given probability are effectively computable in latent space.\nThis allows for SMT and abstract interpretation approaches with fine-grained,\nprobabilistically interpretable, control regarding on how (a)typical the inputs\nsubject to verification are.",
      "tldr_zh": "该论文提出 VeriFlow 架构，这是一种基于流的密度模型，用于改进神经网络的正式验证方法，以避免传统验证对整个输入空间的检查而导致的虚假错误。VeriFlow 通过建模感兴趣的数据分布，允许验证器仅针对典型输入进行搜索，其转换和 log-density 函数为分段仿射，从而与基于 SMT（满足模理论）和线性算术的验证器兼容。此外，VeriFlow 的上密度水平集（UDL）在潜在空间中表现为 L^p-ball 形状，这便于计算和细粒度控制验证输入的概率分布，最终提升了验证的效率和相关性。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.LO",
        "cs.SC"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2406.14265v1",
      "published_date": "2024-06-20 12:41:39 UTC",
      "updated_date": "2024-06-20 12:41:39 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T22:54:33.277672"
    },
    {
      "arxiv_id": "2406.14240v2",
      "title": "CityNav: Language-Goal Aerial Navigation Dataset with Geographic Information",
      "title_zh": "翻译失败",
      "authors": [
        "Jungdae Lee",
        "Taiki Miyanishi",
        "Shuhei Kurita",
        "Koya Sakamoto",
        "Daichi Azuma",
        "Yutaka Matsuo",
        "Nakamasa Inoue"
      ],
      "abstract": "Vision-and-language navigation (VLN) aims to guide autonomous agents through\nreal-world environments by integrating visual and linguistic cues. Despite\nnotable advancements in ground-level navigation, the exploration of aerial\nnavigation using these modalities remains limited. This gap primarily arises\nfrom a lack of suitable resources for real-world, city-scale aerial navigation\nstudies. To remedy this gap, we introduce CityNav, a novel dataset explicitly\ndesigned for language-guided aerial navigation in photorealistic 3D\nenvironments of real cities. CityNav comprises 32k natural language\ndescriptions paired with human demonstration trajectories, collected via a\nnewly developed web-based 3D simulator. Each description identifies a\nnavigation goal, utilizing the names and locations of landmarks within actual\ncities. As an initial step toward addressing this challenge, we provide\nbaseline models of navigation agents that incorporate an internal 2D spatial\nmap representing landmarks referenced in the descriptions. We have benchmarked\nthe latest aerial navigation methods alongside our proposed baseline model on\nthe CityNav dataset. The findings are revealing: (i) our aerial agent model\ntrained on human demonstration trajectories, outperform those trained on\nshortest path trajectories by a large margin; (ii) incorporating 2D spatial map\ninformation markedly and robustly enhances navigation performance at a city\nscale; (iii) despite the use of map information, our challenging CityNav\ndataset reveals a persistent performance gap between our baseline models and\nhuman performance. To foster further research in aerial VLN, we have made the\ndataset and code available at https://water-cookie.github.io/city-nav-proj/",
      "tldr_zh": "本研究引入了 CityNav 数据集，这是一个专为语言引导的空中导航 (Vision-and-Language Navigation, VLN) 设计的资源，包含真实城市 photorealistic 3D 环境的 32k 个自然语言描述和人类演示轨迹，旨在解决现有空中导航研究的资源不足问题。数据集通过一个新的 web-based 3D 模拟器收集，每条描述利用地标名称和位置定义导航目标，并整合了内部 2D 空间地图来提升代理模型的性能。实验结果显示，使用人类演示轨迹训练的模型比最短路径训练的模型有显著优势，且添加 2D 空间地图信息大幅提高了城市规模导航的准确性和鲁棒性；尽管如此，基线模型与人类性能仍存在差距。该数据集和代码已公开，以推动空中 VLN 领域的研究。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "The first two authors are equally contributed",
      "pdf_url": "http://arxiv.org/pdf/2406.14240v2",
      "published_date": "2024-06-20 12:08:27 UTC",
      "updated_date": "2024-10-05 16:53:09 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T22:54:45.939714"
    },
    {
      "arxiv_id": "2406.14232v1",
      "title": "Enhancing robustness of data-driven SHM models: adversarial training with circle loss",
      "title_zh": "翻译失败",
      "authors": [
        "Xiangli Yang",
        "Xijie Deng",
        "Hanwei Zhang",
        "Yang Zou",
        "Jianxi Yang"
      ],
      "abstract": "Structural health monitoring (SHM) is critical to safeguarding the safety and\nreliability of aerospace, civil, and mechanical infrastructure. Machine\nlearning-based data-driven approaches have gained popularity in SHM due to\nadvancements in sensors and computational power. However, machine learning\nmodels used in SHM are vulnerable to adversarial examples -- even small changes\nin input can lead to different model outputs. This paper aims to address this\nproblem by discussing adversarial defenses in SHM. In this paper, we propose an\nadversarial training method for defense, which uses circle loss to optimize the\ndistance between features in training to keep examples away from the decision\nboundary. Through this simple yet effective constraint, our method demonstrates\nsubstantial improvements in model robustness, surpassing existing defense\nmechanisms.",
      "tldr_zh": "结构健康监测 (SHM) 是保障航空、土木和机械基础设施安全的关键，但基于机器学习的 SHM 模型容易受到对抗样本的影响，导致微小输入变化就改变输出结果。本文提出一种使用 circle loss 的对抗训练方法，通过优化训练中特征之间的距离，使样本远离决策边界，从而增强模型的鲁棒性。该方法在实验中显著超过了现有防御机制，展示了其在 SHM 领域提升模型可靠性的潜力。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "12 pages, 9 figures",
      "pdf_url": "http://arxiv.org/pdf/2406.14232v1",
      "published_date": "2024-06-20 11:55:39 UTC",
      "updated_date": "2024-06-20 11:55:39 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T22:54:56.110681"
    },
    {
      "arxiv_id": "2406.14230v3",
      "title": "Raising the Bar: Investigating the Values of Large Language Models via Generative Evolving Testing",
      "title_zh": "翻译失败",
      "authors": [
        "Han Jiang",
        "Xiaoyuan Yi",
        "Zhihua Wei",
        "Ziang Xiao",
        "Shu Wang",
        "Xing Xie"
      ],
      "abstract": "Warning: Contains harmful model outputs.\n  Despite significant advancements, the propensity of Large Language Models\n(LLMs) to generate harmful and unethical content poses critical challenges.\nMeasuring value alignment of LLMs becomes crucial for their regulation and\nresponsible deployment. Although numerous benchmarks have been constructed to\nassess social bias, toxicity, and ethical issues in LLMs, those static\nbenchmarks suffer from evaluation chronoeffect, in which, as models rapidly\nevolve, existing benchmarks may leak into training data or become saturated,\noverestimating ever-developing LLMs. To tackle this problem, we propose GETA, a\nnovel generative evolving testing approach based on adaptive testing methods in\nmeasurement theory. Unlike traditional adaptive testing methods that rely on a\nstatic test item pool, GETA probes the underlying moral boundaries of LLMs by\ndynamically generating test items tailored to model capability. GETA co-evolves\nwith LLMs by learning a joint distribution of item difficulty and model value\nconformity, thus effectively addressing evaluation chronoeffect. We evaluated\nvarious popular LLMs with GETA and demonstrated that 1) GETA can dynamically\ncreate difficulty-tailored test items and 2) GETA's evaluation results are more\nconsistent with models' performance on unseen OOD and i.i.d. items, laying the\ngroundwork for future evaluation paradigms.",
      "tldr_zh": "尽管大型语言模型(LLMs)取得了显著进展，但它们生成有害和不道德内容的倾向引发了重大挑战，现有的静态基准测试容易因模型快速演变而出现评估时间效应，导致评估结果过高估。论文提出GETA，一种基于测量理论的生成式演化测试方法，通过动态生成适应模型能力的测试项目，并学习项目难度和模型价值一致性的联合分布，来有效评估LLMs的价值对齐。实验结果表明，GETA能创建难度量身定制的测试项目，且其评估结果与模型在未见OOD和i.i.d.项目上的表现更一致，为未来LLMs评估范式提供了新基础。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.CY"
      ],
      "primary_category": "cs.CL",
      "comment": "Under Review",
      "pdf_url": "http://arxiv.org/pdf/2406.14230v3",
      "published_date": "2024-06-20 11:51:00 UTC",
      "updated_date": "2025-02-03 10:33:17 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T22:55:09.975811"
    },
    {
      "arxiv_id": "2406.14228v3",
      "title": "EvoAgent: Towards Automatic Multi-Agent Generation via Evolutionary Algorithms",
      "title_zh": "翻译失败",
      "authors": [
        "Siyu Yuan",
        "Kaitao Song",
        "Jiangjie Chen",
        "Xu Tan",
        "Dongsheng Li",
        "Deqing Yang"
      ],
      "abstract": "The rise of powerful large language models (LLMs) has spurred a new trend in\nbuilding LLM-based autonomous agents for solving complex tasks, especially\nmulti-agent systems. Despite the remarkable progress, we notice that existing\nworks are heavily dependent on human-designed frameworks, which greatly limits\nthe functional scope and scalability of agent systems. How to automatically\nextend the specialized agent to multi-agent systems to improve task-solving\ncapability still remains a significant challenge. In this paper, we introduce\nEvoAgent, a generic method to automatically extend specialized agents to\nmulti-agent systems via the evolutionary algorithm, thereby improving the\neffectiveness of LLM-based agents in solving tasks. Specifically, we consider\nthe existing agent frameworks as the initial individual and then apply a series\nof evolutionary operators (e.g., mutation, crossover, selection, etc.) to\ngenerate multiple agents with diverse settings. Experimental results across\nvarious tasks show that EvoAgent can significantly enhance the task-solving\ncapability of LLM-based agents, and can be generalized to any LLM-based agent\nframework to extend them into multi-agent systems. Resources are available at\nhttps://evo-agent.github.io/.",
      "tldr_zh": "该研究针对大型语言模型(LLMs)构建的自主代理系统存在依赖人工框架的问题，导致功能范围和可扩展性受限，提出EvoAgent方法，通过进化算法(evolutionary algorithms)自动将专业代理扩展为多代理(multi-agent)系统，以提升任务解决能力。具体来说，EvoAgent将现有代理框架视为初始个体，并应用进化操作（如变异、交叉和选择）生成多样化的多个代理。实验结果显示，该方法显著提高了LLMs-based代理在各种任务中的性能，并可泛化应用于任何LLMs-based代理框架。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "Accepted as a main conference paper at NAACL 2025",
      "pdf_url": "http://arxiv.org/pdf/2406.14228v3",
      "published_date": "2024-06-20 11:49:23 UTC",
      "updated_date": "2025-03-10 02:42:01 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T22:55:31.220769"
    },
    {
      "arxiv_id": "2406.14219v2",
      "title": "Proving Olympiad Algebraic Inequalities without Human Demonstrations",
      "title_zh": "翻译失败",
      "authors": [
        "Chenrui Wei",
        "Mengzhou Sun",
        "Wei Wang"
      ],
      "abstract": "Solving Olympiad-level mathematical problems represents a significant\nadvancement in machine intelligence and automated reasoning. Current machine\nlearning methods, however, struggle to solve Olympiad-level problems beyond\nEuclidean plane geometry due to a lack of large-scale, high-quality datasets.\nThe challenge is even greater in algebraic systems, which involve infinite\nreasoning spaces within finite conditions. To address these issues, we propose\nAIPS, an Algebraic Inequality Proving System capable of autonomously generating\ncomplex inequality theorems and effectively solving Olympiad-level inequality\nproblems without requiring human demonstrations. During proof search in a mixed\nreasoning manner, a value curriculum learning strategy on generated datasets is\nimplemented to improve proving performance, demonstrating strong mathematical\nintuitions. On a test set of 20 International Mathematical Olympiad-level\ninequality problems, AIPS successfully solved 10, outperforming\nstate-of-the-art methods. Furthermore, AIPS automatically generated a vast\narray of non-trivial theorems without human intervention, some of which have\nbeen evaluated by professional contestants and deemed to reach the level of the\nInternational Mathematical Olympiad. Notably, one theorem was selected as a\ncompetition problem in a major city 2024 Mathematical Olympiad.",
      "tldr_zh": "该研究提出了一种代数不等式证明系统 AIPS，能够在无需人类演示的情况下自主生成复杂不等式定理并解决奥林匹克级问题，针对机器学习在代数系统中面临的无限推理空间和数据集缺乏挑战。AIPS 采用混合推理方式和 value curriculum learning 策略，通过生成数据集来提升证明性能，并展现出强烈的数学直觉。在测试中，AIPS 在 20 个 International Mathematical Olympiad 级不等式问题中成功解决了 10 个，优于现有方法，并自动生成了多个非平凡定理，其中一个被选为 2024 年某城市数学奥林匹克竞赛题目。",
      "categories": [
        "cs.AI",
        "03B35, 68T05, 68T20",
        "I.2.3; I.2.6; I.2.8"
      ],
      "primary_category": "cs.AI",
      "comment": "36 pages, 32 figures, 2 tables, published as a conference paper at\n  NeurIPS 2024",
      "pdf_url": "http://arxiv.org/pdf/2406.14219v2",
      "published_date": "2024-06-20 11:37:53 UTC",
      "updated_date": "2024-10-31 03:24:06 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T22:55:34.792809"
    },
    {
      "arxiv_id": "2406.14214v6",
      "title": "REVEAL-IT: REinforcement learning with Visibility of Evolving Agent poLicy for InTerpretability",
      "title_zh": "翻译失败",
      "authors": [
        "Shuang Ao",
        "Simon Khan",
        "Haris Aziz",
        "Flora D. Salim"
      ],
      "abstract": "Understanding the agent's learning process, particularly the factors that\ncontribute to its success or failure post-training, is crucial for\ncomprehending the rationale behind the agent's decision-making process. Prior\nmethods clarify the learning process by creating a structural causal model\n(SCM) or visually representing the distribution of value functions.\nNevertheless, these approaches have constraints as they exclusively function in\n2D-environments or with uncomplicated transition dynamics. Understanding the\nagent's learning process in complicated environments or tasks is more\nchallenging. In this paper, we propose REVEAL-IT, a novel framework for\nexplaining the learning process of an agent in complex environments. Initially,\nwe visualize the policy structure and the agent's learning process for various\ntraining tasks. By visualizing these findings, we can understand how much a\nparticular training task or stage affects the agent's performance in test.\nThen, a GNN-based explainer learns to highlight the most important section of\nthe policy, providing a more clear and robust explanation of the agent's\nlearning process. The experiments demonstrate that explanations derived from\nthis framework can effectively help in the optimization of the training tasks,\nresulting in improved learning efficiency and final performance.",
      "tldr_zh": "本研究提出 REVEAL-IT 框架，用于解释复杂环境中的强化学习代理的训练过程，解决现有方法（如结构化因果模型 SCM）仅适用于 2D 环境或简单动态的局限性。该框架首先通过可视化代理的策略结构和学习过程，分析特定训练任务对测试性能的影响；随后，使用基于 GNN (Graph Neural Network) 的解释器突出策略中最重要的部分，提供更清晰且鲁棒的解释。实验结果表明，REVEAL-IT 的解释有助于优化训练任务，从而提升代理的学习效率和最终性能。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2406.14214v6",
      "published_date": "2024-06-20 11:29:26 UTC",
      "updated_date": "2024-10-14 12:08:29 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T22:55:45.555783"
    },
    {
      "arxiv_id": "2406.14208v2",
      "title": "SeCoKD: Aligning Large Language Models for In-Context Learning with Fewer Shots",
      "title_zh": "翻译失败",
      "authors": [
        "Weixing Wang",
        "Haojin Yang",
        "Christoph Meinel"
      ],
      "abstract": "Previous studies have shown that demonstrations can significantly help Large\nLanguage Models (LLMs ) perform better on the given tasks. However, this\nso-called In-Context Learning ( ICL ) ability is very sensitive to the\npresenting context, and often dozens of demonstrations are needed. In this\nwork, we investigate if we can reduce the shot number while still maintaining a\ncompetitive performance. We present SeCoKD, a self-Knowledge Distillation ( KD\n) training framework that aligns the student model with a heavily prompted\nvariation, thereby increasing the utilization of a single demonstration. We\nexperiment with the SeCoKD across three LLMs and six benchmarks focusing mainly\non reasoning tasks. Results show that our method outperforms the base model and\nSupervised Fine-tuning ( SFT ), especially in zero-shot and one-shot settings\nby 30% and 10%, respectively. Moreover, SeCoKD brings little negative artifacts\nwhen evaluated on new tasks, which is more robust than Supervised Fine-tuning.",
      "tldr_zh": "本研究针对 Large Language Models (LLMs) 的 In-Context Learning (ICL) 问题，提出 SeCoKD 框架，该框架通过自知识蒸馏 (KD) 训练方法，将学生模型与 heavily prompted 变体对齐，从而减少演示数量并提升单个演示的利用率。实验在三个 LLMs 和六个主要聚焦推理任务的基准测试上进行，结果显示 SeCoKD 在 zero-shot 和 one-shot 设置中分别比基线模型和 Supervised Fine-tuning (SFT) 提高 30% 和 10%。此外，与 SFT 相比，SeCoKD 在新任务评估中更鲁棒，负面影响更小。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "preprint",
      "pdf_url": "http://arxiv.org/pdf/2406.14208v2",
      "published_date": "2024-06-20 11:26:06 UTC",
      "updated_date": "2024-09-26 08:12:59 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T22:55:59.564056"
    },
    {
      "arxiv_id": "2406.14194v2",
      "title": "VLBiasBench: A Comprehensive Benchmark for Evaluating Bias in Large Vision-Language Model",
      "title_zh": "翻译失败",
      "authors": [
        "Sibo Wang",
        "Xiangkui Cao",
        "Jie Zhang",
        "Zheng Yuan",
        "Shiguang Shan",
        "Xilin Chen",
        "Wen Gao"
      ],
      "abstract": "The emergence of Large Vision-Language Models (LVLMs) marks significant\nstrides towards achieving general artificial intelligence. However, these\nadvancements are accompanied by concerns about biased outputs, a challenge that\nhas yet to be thoroughly explored. Existing benchmarks are not sufficiently\ncomprehensive in evaluating biases due to their limited data scale, single\nquestioning format and narrow sources of bias. To address this problem, we\nintroduce VLBiasBench, a comprehensive benchmark designed to evaluate biases in\nLVLMs. VLBiasBench, features a dataset that covers nine distinct categories of\nsocial biases, including age, disability status, gender, nationality, physical\nappearance, race, religion, profession, social economic status, as well as two\nintersectional bias categories: race x gender and race x social economic\nstatus. To build a large-scale dataset, we use Stable Diffusion XL model to\ngenerate 46,848 high-quality images, which are combined with various questions\nto creat 128,342 samples. These questions are divided into open-ended and\nclose-ended types, ensuring thorough consideration of bias sources and a\ncomprehensive evaluation of LVLM biases from multiple perspectives. We conduct\nextensive evaluations on 15 open-source models as well as two advanced\nclosed-source models, yielding new insights into the biases present in these\nmodels. Our benchmark is available at\nhttps://github.com/Xiangkui-Cao/VLBiasBench.",
      "tldr_zh": "这篇论文引入了VLBiasBench，一种全面的基准，用于评估Large Vision-Language Models (LVLMs)中的社会偏见问题，以解决现有基准数据规模有限、提问格式单一和偏见来源狭窄的不足。VLBiasBench涵盖九个偏见类别（如age、gender、race等）以及两个交叉类别（race x gender 和 race x social economic status），通过Stable Diffusion XL模型生成46,848张高质量图像，并结合开放式和封闭式问题创建128,342个样本，从多角度进行全面评估。研究对15个开源模型和2个闭源模型进行了广泛测试，揭示了LVLMs中偏见的新的见解，并提供了开源代码（https://github.com/Xiangkui-Cao/VLBiasBench）。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2406.14194v2",
      "published_date": "2024-06-20 10:56:59 UTC",
      "updated_date": "2024-12-25 07:31:14 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T22:56:11.020021"
    },
    {
      "arxiv_id": "2406.14192v2",
      "title": "Timo: Towards Better Temporal Reasoning for Language Models",
      "title_zh": "翻译失败",
      "authors": [
        "Zhaochen Su",
        "Jun Zhang",
        "Tong Zhu",
        "Xiaoye Qu",
        "Juntao Li",
        "Min Zhang",
        "Yu Cheng"
      ],
      "abstract": "Reasoning about time is essential for Large Language Models (LLMs) to\nunderstand the world. Previous works focus on solving specific tasks, primarily\non time-sensitive question answering. While these methods have proven\neffective, they cannot generalize to a wider spectrum of temporal reasoning\ntasks. Therefore, we propose a crucial question: Can we build a universal\nframework to handle a variety of temporal reasoning tasks? To that end, we\nsystematically study 38 temporal reasoning tasks. Based on the observation that\n19 tasks are directly related to mathematics, we first leverage the available\nmathematical dataset to set a solid foundation for temporal reasoning. However,\nthe in-depth study indicates that focusing solely on mathematical enhancement\nfalls short of addressing pure temporal reasoning tasks. To mitigate this\nlimitation, we propose a simple but effective self-critic temporal optimization\nmethod to enhance the model's temporal reasoning capabilities without\nsacrificing general task abilities. Finally, we develop Timo, a model designed\nto excel in temporal reasoning at the 7B and 13B scales. Notably, Timo\noutperforms the counterpart LLMs by 10.0 and 7.6 in average accuracy scores and\nachieves the new state-of-the-art (SOTA) performance of comparable size.\nExtensive experiments further validate our framework's effectiveness and its\ngeneralization across diverse temporal tasks. The code is available at\nhttps://github.com/zhaochen0110/Timo.",
      "tldr_zh": "该论文探讨了大型语言模型(LLMs)的时间推理能力问题，系统研究了38个时间推理任务，发现其中19个与数学相关，因此先利用数学数据集作为基础。\n为解决纯时间推理任务的局限，作者提出了一种简单有效的自我批评时间优化(self-critic temporal optimization)方法，提升模型的时间推理能力，同时不影响一般任务性能。\n最终开发的Timo模型在7B和13B规模上，平均准确率比对应LLMs高出10.0和7.6分，实现了新的SOTA性能，并通过广泛实验验证了框架的泛化性和有效性。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "This paper has been accepted to the COLM 2024 conference",
      "pdf_url": "http://arxiv.org/pdf/2406.14192v2",
      "published_date": "2024-06-20 10:52:14 UTC",
      "updated_date": "2024-08-19 03:47:16 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T22:56:23.288847"
    },
    {
      "arxiv_id": "2406.14191v3",
      "title": "Temporal Knowledge Graph Question Answering: A Survey",
      "title_zh": "时间知识图谱问答：综述",
      "authors": [
        "Miao Su",
        "Zixuan Li",
        "Zhuo Chen",
        "Long Bai",
        "Xiaolong Jin",
        "Jiafeng Guo"
      ],
      "abstract": "Knowledge Base Question Answering (KBQA) has been a long-standing field to\nanswer questions based on knowledge bases. Recently, the evolving dynamics of\nknowledge have attracted a growing interest in Temporal Knowledge Graph\nQuestion Answering (TKGQA), an emerging task to answer temporal questions.\nHowever, this field grapples with ambiguities in defining temporal questions\nand lacks a systematic categorization of existing methods for TKGQA. In\nresponse, this paper provides a thorough survey from two perspectives: the\ntaxonomy of temporal questions and the methodological categorization for TKGQA.\nSpecifically, we first establish a detailed taxonomy of temporal questions\nengaged in prior studies. Subsequently, we provide a comprehensive review of\nTKGQA techniques of two categories: semantic parsing-based and TKG\nembedding-based. Building on this review, the paper outlines potential research\ndirections aimed at advancing the field of TKGQA. This work aims to serve as a\ncomprehensive reference for TKGQA and to stimulate further research.",
      "tldr_zh": "这篇论文对 Temporal Knowledge Graph Question Answering (TKGQA) 进行了全面调查，旨在解决该领域中时间问题定义的模糊性和现有方法缺乏系统分类的问题。论文首先建立了时间问题的详细分类，随后回顾了两种主要 TKGQA 方法：基于语义解析的和基于 TKG embedding-based 的技术。最终，论文概述了潜在的研究方向，以促进 TKGQA 的发展，并作为该领域的综合参考。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "8 pages, 3 figures. This work has been submitted to the IEEE for\n  possible publication",
      "pdf_url": "http://arxiv.org/pdf/2406.14191v3",
      "published_date": "2024-06-20 10:51:06 UTC",
      "updated_date": "2025-04-21 03:04:15 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T22:56:33.691520"
    },
    {
      "arxiv_id": "2406.14185v1",
      "title": "Failure-Resilient Distributed Inference with Model Compression over Heterogeneous Edge Devices",
      "title_zh": "翻译失败",
      "authors": [
        "Li Wang",
        "Liang Li",
        "Lianming Xu",
        "Xian Peng",
        "Aiguo Fei"
      ],
      "abstract": "The distributed inference paradigm enables the computation workload to be\ndistributed across multiple devices, facilitating the implementations of deep\nlearning based intelligent services on extremely resource-constrained Internet\nof Things (IoT) scenarios. Yet it raises great challenges to perform\ncomplicated inference tasks relying on a cluster of IoT devices that are\nheterogeneous in their computing/communication capacity and prone to crash or\ntimeout failures. In this paper, we present RoCoIn, a robust cooperative\ninference mechanism for locally distributed execution of deep neural\nnetwork-based inference tasks over heterogeneous edge devices. It creates a set\nof independent and compact student models that are learned from a large model\nusing knowledge distillation for distributed deployment. In particular, the\ndevices are strategically grouped to redundantly deploy and execute the same\nstudent model such that the inference process is resilient to any local\nfailures, while a joint knowledge partition and student model assignment scheme\nare designed to minimize the response latency of the distributed inference\nsystem in the presence of devices with diverse capacities. Extensive\nsimulations are conducted to corroborate the superior performance of our RoCoIn\nfor distributed inference compared to several baselines, and the results\ndemonstrate its efficacy in timely inference and failure resiliency.",
      "tldr_zh": "该论文提出RoCoIn，一种鲁棒的合作推理机制，用于在异构边缘设备上进行分布式推理，以应对设备计算/通信能力差异和故障问题。RoCoIn通过知识蒸馏（knowledge distillation）从大模型创建独立、紧凑的学生模型，并将设备战略性分组以冗余部署这些模型，确保推理过程对本地故障具有弹性。同时，该机制设计了联合知识分区和学生模型分配方案，以最小化响应延迟。实验模拟结果显示，RoCoIn在及时推理和故障弹性方面优于基线方法，证明了其在资源受限的IoT场景中的高效性。",
      "categories": [
        "cs.DC",
        "cs.AI"
      ],
      "primary_category": "cs.DC",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2406.14185v1",
      "published_date": "2024-06-20 10:43:53 UTC",
      "updated_date": "2024-06-20 10:43:53 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T22:56:44.656922"
    },
    {
      "arxiv_id": "2406.14177v1",
      "title": "SimulSeamless: FBK at IWSLT 2024 Simultaneous Speech Translation",
      "title_zh": "翻译失败",
      "authors": [
        "Sara Papi",
        "Marco Gaido",
        "Matteo Negri",
        "Luisa Bentivogli"
      ],
      "abstract": "This paper describes the FBK's participation in the Simultaneous Translation\nEvaluation Campaign at IWSLT 2024. For this year's submission in the\nspeech-to-text translation (ST) sub-track, we propose SimulSeamless, which is\nrealized by combining AlignAtt and SeamlessM4T in its medium configuration. The\nSeamlessM4T model is used \"off-the-shelf\" and its simultaneous inference is\nenabled through the adoption of AlignAtt, a SimulST policy based on\ncross-attention that can be applied without any retraining or adaptation of the\nunderlying model for the simultaneous task. We participated in all the Shared\nTask languages (English->{German, Japanese, Chinese}, and Czech->English),\nachieving acceptable or even better results compared to last year's\nsubmissions. SimulSeamless, covering more than 143 source languages and 200\ntarget languages, is released at: https://github.com/hlt-mt/FBK-fairseq/.",
      "tldr_zh": "本论文介绍了FBK在IWSLT 2024同时翻译评估竞赛中的参与，提出SimulSeamless系统，用于speech-to-text翻译（ST）子轨道。SimulSeamless通过结合AlignAtt（基于cross-attention的SimulST政策）和SeamlessM4T模型（medium配置）实现，同时翻译功能，而无需对底层模型进行重新训练或适应。实验结果显示，该系统在English->{German, Japanese, Chinese}和Czech->English等语言对上，表现优于去年的提交，支持超过143种源语言和200种目标语言，并已开源于https://github.com/hlt-mt/FBK-fairseq/。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.SD",
        "eess.AS"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2406.14177v1",
      "published_date": "2024-06-20 10:34:46 UTC",
      "updated_date": "2024-06-20 10:34:46 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T22:56:57.128640"
    },
    {
      "arxiv_id": "2406.14176v3",
      "title": "A Multi-Stream Fusion Approach with One-Class Learning for Audio-Visual Deepfake Detection",
      "title_zh": "翻译失败",
      "authors": [
        "Kyungbok Lee",
        "You Zhang",
        "Zhiyao Duan"
      ],
      "abstract": "This paper addresses the challenge of developing a robust audio-visual\ndeepfake detection model. In practical use cases, new generation algorithms are\ncontinually emerging, and these algorithms are not encountered during the\ndevelopment of detection methods. This calls for the generalization ability of\nthe method. Additionally, to ensure the credibility of detection methods, it is\nbeneficial for the model to interpret which cues from the video indicate it is\nfake. Motivated by these considerations, we then propose a multi-stream fusion\napproach with one-class learning as a representation-level regularization\ntechnique. We study the generalization problem of audio-visual deepfake\ndetection by creating a new benchmark by extending and re-splitting the\nexisting FakeAVCeleb dataset. The benchmark contains four categories of fake\nvideos (Real Audio-Fake Visual, Fake Audio-Fake Visual, Fake Audio-Real Visual,\nand Unsynchronized videos). The experimental results demonstrate that our\napproach surpasses the previous models by a large margin. Furthermore, our\nproposed framework offers interpretability, indicating which modality the model\nidentifies as more likely to be fake. The source code is released at\nhttps://github.com/bok-bok/MSOC.",
      "tldr_zh": "这篇论文提出了一种多流融合(multi-stream fusion)方法，结合一类学习(one-class learning)作为表示级别的正则化技术，用于音频-视觉深度伪造检测，以提升模型的泛化能力和可解释性。研究人员扩展了FakeAVCeleb数据集，创建了一个新基准，包括Real Audio-Fake Visual、Fake Audio-Fake Visual、Fake Audio-Real Visual和Unsynchronized视频四类。实验结果显示，该方法大幅超过了现有模型，并在可解释性上突出，表明模型能识别哪个模态更可能是假的。",
      "categories": [
        "cs.SD",
        "cs.AI",
        "cs.MM",
        "eess.AS"
      ],
      "primary_category": "cs.SD",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2406.14176v3",
      "published_date": "2024-06-20 10:33:15 UTC",
      "updated_date": "2024-08-19 13:14:28 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T22:57:10.456687"
    },
    {
      "arxiv_id": "2406.14171v1",
      "title": "Ranking LLMs by compression",
      "title_zh": "翻译失败",
      "authors": [
        "Peijia Guo",
        "Ziguang Li",
        "Haibo Hu",
        "Chao Huang",
        "Ming Li",
        "Rui Zhang"
      ],
      "abstract": "We conceptualize the process of understanding as information compression, and\npropose a method for ranking large language models (LLMs) based on lossless\ndata compression. We demonstrate the equivalence of compression length under\narithmetic coding with cumulative negative log probabilities when using a large\nlanguage model as a prior, that is, the pre-training phase of the model is\nessentially the process of learning the optimal coding length. At the same\ntime, the evaluation metric compression ratio can be obtained without actual\ncompression, which greatly saves overhead. In this paper, we use five large\nlanguage models as priors for compression, then compare their performance on\nchallenging natural language processing tasks, including sentence completion,\nquestion answering, and coreference resolution. Experimental results show that\ncompression ratio and model performance are positively correlated, so it can be\nused as a general metric to evaluate large language models.",
      "tldr_zh": "本论文将理解过程视为信息压缩，提出一种基于无损数据压缩的方法来排名大型语言模型(LLMs)。他们证明了在算术编码下，压缩长度等同于使用LLMs作为先验时的累积负对数概率，这意味着LLMs的预训练本质上是学习最优编码长度，且评估指标压缩比无需实际压缩即可获得，节省了开销。实验使用五个LLMs作为先验，比较了它们在句子完成、问答和指代消解等任务上的性能，结果显示压缩比与模型性能正相关，因此可作为评估LLMs的通用指标。",
      "categories": [
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.AI",
      "comment": "7 pages, 4 tables",
      "pdf_url": "http://arxiv.org/pdf/2406.14171v1",
      "published_date": "2024-06-20 10:23:38 UTC",
      "updated_date": "2024-06-20 10:23:38 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T22:57:22.400154"
    },
    {
      "arxiv_id": "2406.14164v1",
      "title": "A Data-Driven Guided Decoding Mechanism for Diagnostic Captioning",
      "title_zh": "翻译失败",
      "authors": [
        "Panagiotis Kaliosis",
        "John Pavlopoulos",
        "Foivos Charalampakos",
        "Georgios Moschovis",
        "Ion Androutsopoulos"
      ],
      "abstract": "Diagnostic Captioning (DC) automatically generates a diagnostic text from one\nor more medical images (e.g., X-rays, MRIs) of a patient. Treated as a draft,\nthe generated text may assist clinicians, by providing an initial estimation of\nthe patient's condition, speeding up and helping safeguard the diagnostic\nprocess. The accuracy of a diagnostic text, however, strongly depends on how\nwell the key medical conditions depicted in the images are expressed. We\npropose a new data-driven guided decoding method that incorporates medical\ninformation, in the form of existing tags capturing key conditions of the\nimage(s), into the beam search of the diagnostic text generation process. We\nevaluate the proposed method on two medical datasets using four DC systems that\nrange from generic image-to-text systems with CNN encoders and RNN decoders to\npre-trained Large Language Models. The latter can also be used in few- and\nzero-shot learning scenarios. In most cases, the proposed mechanism improves\nperformance with respect to all evaluation measures. We provide an open-source\nimplementation of the proposed method at https://github.com/nlpaueb/dmmcs.",
      "tldr_zh": "这篇论文针对Diagnostic Captioning提出了一种数据驱动的引导解码机制，将医疗标签（如关键条件标签）整合到beam search过程中，以提升从医疗图像（如X-rays或MRIs）生成的诊断文本准确性。方法通过利用现有标签辅助文本生成，帮助临床医生快速评估患者状况并提高诊断过程的安全性。在两个医疗数据集上，使用四种DC系统（包括CNN+RNN和预训练Large Language Models）进行评估，结果显示该机制在大多数情况下改善了所有评估指标，并提供开源实现（https://github.com/nlpaueb/dmmcs）。",
      "categories": [
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.AI",
      "comment": "[Pre-print] ACL Findings 2024, 17 pages, 7 figures, 7 tables",
      "pdf_url": "http://arxiv.org/pdf/2406.14164v1",
      "published_date": "2024-06-20 10:08:17 UTC",
      "updated_date": "2024-06-20 10:08:17 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T22:57:44.928754"
    },
    {
      "arxiv_id": "2406.14162v4",
      "title": "DIRAS: Efficient LLM Annotation of Document Relevance in Retrieval Augmented Generation",
      "title_zh": "翻译失败",
      "authors": [
        "Jingwei Ni",
        "Tobias Schimanski",
        "Meihong Lin",
        "Mrinmaya Sachan",
        "Elliott Ash",
        "Markus Leippold"
      ],
      "abstract": "Retrieval Augmented Generation (RAG) is widely employed to ground responses\nto queries on domain-specific documents. But do RAG implementations leave out\nimportant information when answering queries that need an integrated analysis\nof information (e.g., Tell me good news in the stock market today.)? To address\nthese concerns, RAG developers need to annotate information retrieval (IR) data\nfor their domain of interest, which is challenging because (1) domain-specific\nqueries usually need nuanced definitions of relevance beyond shallow semantic\nrelevance; and (2) human or GPT-4 annotation is costly and cannot cover all\n(query, document) pairs (i.e., annotation selection bias), thus harming the\neffectiveness in evaluating IR recall. To address these challenges, we propose\nDIRAS (Domain-specific Information Retrieval Annotation with Scalability), a\nmanual-annotation-free schema that fine-tunes open-sourced LLMs to consider\nnuanced relevance definition and annotate (partial) relevance labels with\ncalibrated relevance scores. Extensive evaluation shows that DIRAS enables\nsmaller (8B) LLMs to achieve GPT-4-level performance on annotating and ranking\nunseen (query, document) pairs, and is helpful for real-world RAG development.\nAll code, LLM generations, and human annotations can be found in\n\\url{https://github.com/EdisonNi-hku/DIRAS}.",
      "tldr_zh": "该论文提出 DIRAS，一种高效的 LLM 标注方案，用于提升 Retrieval Augmented Generation (RAG) 中文档相关性的评估。DIRAS 通过微调开源 LLM 来处理领域特定查询的细微相关性定义，并生成校准的相关性分数，从而避免了昂贵的人工或 GPT-4 标注及其带来的偏差问题。实验结果表明，DIRAS 使小型 (8B) LLM 在标注和排名未见过的 (查询, 文档) 对上达到 GPT-4 级性能，并为实际 RAG 开发提供实用支持。",
      "categories": [
        "cs.IR",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.IR",
      "comment": "NAACL 2025 Long",
      "pdf_url": "http://arxiv.org/pdf/2406.14162v4",
      "published_date": "2024-06-20 10:04:09 UTC",
      "updated_date": "2025-01-23 08:41:05 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T22:57:46.521220"
    },
    {
      "arxiv_id": "2406.14144v1",
      "title": "Finding Safety Neurons in Large Language Models",
      "title_zh": "在大型语言模型中发现安全神经元",
      "authors": [
        "Jianhui Chen",
        "Xiaozhi Wang",
        "Zijun Yao",
        "Yushi Bai",
        "Lei Hou",
        "Juanzi Li"
      ],
      "abstract": "Large language models (LLMs) excel in various capabilities but also pose\nsafety risks such as generating harmful content and misinformation, even after\nsafety alignment. In this paper, we explore the inner mechanisms of safety\nalignment from the perspective of mechanistic interpretability, focusing on\nidentifying and analyzing safety neurons within LLMs that are responsible for\nsafety behaviors. We propose generation-time activation contrasting to locate\nthese neurons and dynamic activation patching to evaluate their causal effects.\nExperiments on multiple recent LLMs show that: (1) Safety neurons are sparse\nand effective. We can restore $90$% safety performance with intervention only\non about $5$% of all the neurons. (2) Safety neurons encode transferrable\nmechanisms. They exhibit consistent effectiveness on different red-teaming\ndatasets. The finding of safety neurons also interprets \"alignment tax\". We\nobserve that the identified key neurons for safety and helpfulness\nsignificantly overlap, but they require different activation patterns of the\nshared neurons. Furthermore, we demonstrate an application of safety neurons in\ndetecting unsafe outputs before generation. Our findings may promote further\nresearch on understanding LLM alignment. The source codes will be publicly\nreleased to facilitate future research.",
      "tldr_zh": "这篇论文探讨了 Large Language Models (LLMs) 中的安全神经元，以理解安全对齐机制并缓解生成有害内容和错误信息的风险。作者提出 generation-time activation contrasting 用于定位安全神经元，以及 dynamic activation patching 用于评估其因果效果。实验结果显示，这些安全神经元稀疏且有效，仅干预约5%的神经元即可恢复90%的安全性能，且它们在不同红队测试数据集上具有可转移性，同时解释了“alignment tax”现象，即安全和帮助性神经元重叠但需不同激活模式。该研究还展示了安全神经元的实际应用，如在生成前检测不安全输出，并促进了LLM对齐机制的进一步研究。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2406.14144v1",
      "published_date": "2024-06-20 09:35:22 UTC",
      "updated_date": "2024-06-20 09:35:22 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T22:57:59.755301"
    },
    {
      "arxiv_id": "2406.14141v1",
      "title": "Online Learning of Weakly Coupled MDP Policies for Load Balancing and Auto Scaling",
      "title_zh": "翻译失败",
      "authors": [
        "S. R. Eshwar",
        "Lucas Lopes Felipe",
        "Alexandre Reiffers-Masson",
        "Daniel Sadoc Menasché",
        "Gugan Thoppe"
      ],
      "abstract": "Load balancing and auto scaling are at the core of scalable, contemporary\nsystems, addressing dynamic resource allocation and service rate adjustments in\nresponse to workload changes. This paper introduces a novel model and\nalgorithms for tuning load balancers coupled with auto scalers, considering\nbursty traffic arriving at finite queues. We begin by presenting the problem as\na weakly coupled Markov Decision Processes (MDP), solvable via a linear program\n(LP). However, as the number of control variables of such LP grows\ncombinatorially, we introduce a more tractable relaxed LP formulation, and\nextend it to tackle the problem of online parameter learning and policy\noptimization using a two-timescale algorithm based on the LP Lagrangian.",
      "tldr_zh": "本文提出了一种新的模型和算法，用于优化负载均衡和自动缩放，以应对突发流量对有限队列的影响。问题被建模为弱耦合 Markov Decision Processes (MDP)，并通过线性规划 (LP) 求解，但由于 LP 的控制变量指数增长，作者引入了更易处理的松弛 LP 公式。进一步扩展该框架，使用基于 LP Lagrangian 的两时间尺度算法，实现在线参数学习和策略优化，从而提升系统资源分配的动态适应性。",
      "categories": [
        "eess.SY",
        "cs.AI",
        "cs.NI",
        "cs.SY"
      ],
      "primary_category": "eess.SY",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2406.14141v1",
      "published_date": "2024-06-20 09:34:24 UTC",
      "updated_date": "2024-06-20 09:34:24 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T22:58:12.626825"
    },
    {
      "arxiv_id": "2406.14135v1",
      "title": "Autonomous Robotic Drilling System for Mice Cranial Window Creation",
      "title_zh": "用于小鼠颅窗创建的自治机器人钻孔系统",
      "authors": [
        "Enduo Zhao",
        "Murilo M. Marinho",
        "Kanako Harada"
      ],
      "abstract": "Robotic assistance for experimental manipulation in the life sciences is\nexpected to enable favorable outcomes, regardless of the skill of the\nscientist. Experimental specimens in the life sciences are subject to\nindividual variability hence require intricate algorithms for successful\nautonomous robotic control. As a use case, we are studying the creation of\ncranial windows in mice. This operation requires the removal of an\n8-mm-circular patch of the skull, which is approximately 300 um thick, but the\nshape and thickness of the mouse skull significantly varies depending on the\nstrain of mouse, sex, and age. In this work, we propose an autonomous robotic\ndrilling method with no offline planning, consisting of a trajectory planning\nblock with execution-time feedback with completion level recognition based on\nimage and force information. The force information allows for completion-level\nresolution to increase 10 fold. We evaluate the proposed method in two ways.\nFirst, in an eggshell drilling task and achieved a success rate of 95% and\naverage drilling time of 7.1 min out of 20 trials. Second, in postmortem mice\nand with a success rate of 70% and average drilling time of 9.3 min out of 20\ntrials.",
      "tldr_zh": "这篇论文提出了一种自主机器人钻孔系统，用于小鼠颅窗创建，旨在应对小鼠颅骨形状和厚度的个体变异（如因品种、性别和年龄而异），从而实现无需离线规划的精确操作。该系统通过轨迹规划块结合执行时反馈，包括基于图像和力信息的完成水平识别，使分辨率提高10倍。在实验评估中，该方法在鸡蛋壳钻孔任务中取得95%的成功率和平均7.1分钟时间，而在死后小鼠测试中达到70%的成功率和平均9.3分钟时间。",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "comment": "15 pages, 14 figures, to be submitted to IEEE",
      "pdf_url": "http://arxiv.org/pdf/2406.14135v1",
      "published_date": "2024-06-20 09:23:23 UTC",
      "updated_date": "2024-06-20 09:23:23 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T22:58:24.014950"
    },
    {
      "arxiv_id": "2406.14132v2",
      "title": "Enhancing Monotonic Modeling with Spatio-Temporal Adaptive Awareness in Diverse Marketing",
      "title_zh": "在多样化营销中，通过时空自适应感知增强单调建模",
      "authors": [
        "Bin Li",
        "Jiayan Pei",
        "Feiyang Xiao",
        "Yifan Zhao",
        "Zhixing Zhang",
        "Diwei Liu",
        "HengXu He",
        "Jia Jia"
      ],
      "abstract": "In the mobile internet era, the Online Food Ordering Service (OFOS) emerges\nas an integral component of inclusive finance owing to the convenience it\nbrings to people. OFOS platforms offer dynamic allocation incentives to users\nand merchants through diverse marketing campaigns to encourage payments while\nmaintaining the platforms' budget efficiency. Despite significant progress, the\nmarketing domain continues to face two primary challenges: (i) how to allocate\na limited budget with greater efficiency, demanding precision in predicting\nusers' monotonic response (i.e. sensitivity) to incentives, and (ii) ensuring\nspatio-temporal adaptability and robustness in diverse marketing campaigns\nacross different times and locations. To address these issues, we propose a\nConstrained Monotonic Adaptive Network (CoMAN) method for spatio-temporal\nperception within marketing pricing. Specifically, we capture spatio-temporal\npreferences within attribute features through two foundational spatio-temporal\nperception modules. To further enhance catching the user sensitivity\ndifferentials to incentives across varied times and locations, we design\nmodules for learning spatio-temporal convexity and concavity as well as for\nexpressing sensitivity functions. CoMAN can achieve a more efficient allocation\nof incentive investments during pricing, thus increasing the conversion rate\nand orders while maintaining budget efficiency. Extensive offline and online\nexperimental results within our diverse marketing campaigns demonstrate the\neffectiveness of the proposed approach while outperforming the monotonic\nstate-of-the-art method.",
      "tldr_zh": "本研究针对在线食品订购服务 (OFOS) 中的营销挑战，提出了一种 Constrained Monotonic Adaptive Network (CoMAN) 方法，以提升预算分配效率和用户对激励的单调响应预测。具体而言，CoMAN 通过两个基础时空感知模块捕捉属性特征中的时空偏好，并设计模块学习时空凸凹性和表达敏感性函数，从而实现对不同时间和地点的适应性优化。实验结果显示，该方法在多样化营销活动中显著提高了转换率和订单量，同时保持预算效率，并在离线和在线测试中优于现有的单调状态方法。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "12 pages",
      "pdf_url": "http://arxiv.org/pdf/2406.14132v2",
      "published_date": "2024-06-20 09:21:09 UTC",
      "updated_date": "2025-05-10 08:27:03 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T22:58:33.835794"
    },
    {
      "arxiv_id": "2406.14124v3",
      "title": "Measuring Sample Importance in Data Pruning for Language Models based on Information Entropy",
      "title_zh": "翻译失败",
      "authors": [
        "Minsang Kim",
        "Seungjun Baek"
      ],
      "abstract": "Compute-efficient training of language models has become an important issue.\nWe consider data pruning for data-efficient training of LLMs. In this work, we\nconsider a data pruning method based on information entropy. We propose that\nthe samples in the training corpus be ranked in terms of their informativeness\nwhich we estimate through entropy functions. The key idea is that, less\ninformative samples are likely to contain redundant information, and thus\nshould be pruned first. We use the entropy functions based on the negative\nlog-likelihood and the average inverse word frequency of a sample as a\nsurrogate to measure its informativeness. Experiments reveal that the proposed\ninformation-based pruning can improve upon various language modeling and\ndownstream tasks, and enhance the generalization capability of language models.",
      "tldr_zh": "该研究探讨了基于信息熵的信息量评估方法，用于数据修剪以实现语言模型（LLMs）的计算高效训练。关键想法是通过计算样本的熵函数（如基于 negative log-likelihood 和 average inverse word frequency）来衡量其信息量，并优先修剪信息量低的冗余样本，以提升训练效率。实验结果显示，这种信息熵驱动的修剪方法改善了语言建模和下游任务的表现，并增强了模型的泛化能力。",
      "categories": [
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2406.14124v3",
      "published_date": "2024-06-20 09:09:34 UTC",
      "updated_date": "2024-12-12 00:55:45 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T22:58:46.089949"
    },
    {
      "arxiv_id": "2406.14122v1",
      "title": "EduQate: Generating Adaptive Curricula through RMABs in Education Settings",
      "title_zh": "翻译失败",
      "authors": [
        "Sidney Tio",
        "Dexun Li",
        "Pradeep Varakantham"
      ],
      "abstract": "There has been significant interest in the development of personalized and\nadaptive educational tools that cater to a student's individual learning\nprogress. A crucial aspect in developing such tools is in exploring how mastery\ncan be achieved across a diverse yet related range of content in an efficient\nmanner. While Reinforcement Learning and Multi-armed Bandits have shown promise\nin educational settings, existing works often assume the independence of\nlearning content, neglecting the prevalent interdependencies between such\ncontent. In response, we introduce Education Network Restless Multi-armed\nBandits (EdNetRMABs), utilizing a network to represent the relationships\nbetween interdependent arms. Subsequently, we propose EduQate, a method\nemploying interdependency-aware Q-learning to make informed decisions on arm\nselection at each time step. We establish the optimality guarantee of EduQate\nand demonstrate its efficacy compared to baseline policies, using students\nmodeled from both synthetic and real-world data.",
      "tldr_zh": "该研究针对个性化教育工具的开发，强调了高效掌握相关学习内容的重要性，但指出现有Reinforcement Learning和Multi-armed Bandits方法忽略了内容间的相互依赖。作者引入Education Network Restless Multi-armed Bandits (EdNetRMABs)框架，使用网络表示臂之间的关系，并提出EduQate方法，该方法采用互依赖感知Q-learning来优化学习内容的选择决策。实验结果显示，EduQate在合成和真实数据模型的学生上比基线策略更有效，并证明了其优化保证，为生成自适应课程提供了新途径。",
      "categories": [
        "cs.AI",
        "I.2.8, I.2.0"
      ],
      "primary_category": "cs.AI",
      "comment": "9 pages, NeurIPS 2024 pre-print",
      "pdf_url": "http://arxiv.org/pdf/2406.14122v1",
      "published_date": "2024-06-20 09:07:10 UTC",
      "updated_date": "2024-06-20 09:07:10 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T22:58:58.156582"
    },
    {
      "arxiv_id": "2406.14106v1",
      "title": "EasyECR: A Library for Easy Implementation and Evaluation of Event Coreference Resolution Models",
      "title_zh": "翻译失败",
      "authors": [
        "Yuncong Li",
        "Tianhua Xu",
        "Sheng-hua Zhong",
        "Haiqin Yang"
      ],
      "abstract": "Event Coreference Resolution (ECR) is the task of clustering event mentions\nthat refer to the same real-world event. Despite significant advancements, ECR\nresearch faces two main challenges: limited generalizability across domains due\nto narrow dataset evaluations, and difficulties in comparing models within\ndiverse ECR pipelines. To address these issues, we develop EasyECR, the first\nopen-source library designed to standardize data structures and abstract ECR\npipelines for easy implementation and fair evaluation. More specifically,\nEasyECR integrates seven representative pipelines and ten popular benchmark\ndatasets, enabling model evaluations on various datasets and promoting the\ndevelopment of robust ECR pipelines. By conducting extensive evaluation via our\nEasyECR, we find that, \\lowercase\\expandafter{\\romannumeral1}) the\nrepresentative ECR pipelines cannot generalize across multiple datasets, hence\nevaluating ECR pipelines on multiple datasets is necessary,\n\\lowercase\\expandafter{\\romannumeral2}) all models in ECR pipelines have a\ngreat effect on pipeline performance, therefore, when one model in ECR\npipelines are compared, it is essential to ensure that the other models remain\nconsistent. Additionally, reproducing ECR results is not trivial, and the\ndeveloped library can help reduce this discrepancy. The experimental results\nprovide valuable baselines for future research.",
      "tldr_zh": "该论文针对 Event Coreference Resolution (ECR) 任务提出 EasyECR，这是一个开源库，旨在标准化数据结构和抽象 ECR 管道，以简化模型实现和公平评估。EasyECR 整合了七个代表性管道和十个基准数据集，允许研究者在多种数据集上评估模型，从而提升 ECR 系统的鲁棒性。通过广泛实验，论文发现 ECR 管道无法在多个数据集上泛化，且管道中每个模型对整体性能影响显著，因此强调在比较模型时需保持其他组件一致。该库还有助于减少结果重现的差异，并为未来 ECR 研究提供宝贵基线。",
      "categories": [
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.AI",
      "comment": "14 pages, 4 figures, 12 tables",
      "pdf_url": "http://arxiv.org/pdf/2406.14106v1",
      "published_date": "2024-06-20 08:40:21 UTC",
      "updated_date": "2024-06-20 08:40:21 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T22:59:12.922638"
    },
    {
      "arxiv_id": "2406.14103v1",
      "title": "Two-Stage Depth Enhanced Learning with Obstacle Map For Object Navigation",
      "title_zh": "两阶段深度增强学习，结合障碍物",
      "authors": [
        "Yanwei Zheng",
        "Shaopu Feng",
        "Bowen Huang",
        "Changrui Li",
        "Xiao Zhang",
        "Dongxiao Yu"
      ],
      "abstract": "The task that requires an agent to navigate to a given object through only\nvisual observation is called visual object navigation (VON). The main\nbottlenecks of VON are strategies exploration and prior knowledge exploitation.\nTraditional strategies exploration ignores the differences of searching and\nnavigating stages, using the same reward in two stages, which reduces\nnavigation performance and training efficiency. Our study enables the agent to\nexplore larger area in searching stage and seek the optimal path in navigating\nstage, improving the success rate of navigation. Traditional prior knowledge\nexploitation focused on learning and utilizing object association, which\nignored the depth and obstacle information in the environment. This paper uses\nthe RGB and depth information of the training scene to pretrain the feature\nextractor, which improves navigation efficiency. The obstacle information is\nmemorized by the agent during the navigation, reducing the probability of\ncollision and deadlock. Depth, obstacle and other prior knowledge are\nconcatenated and input into the policy network, and navigation actions are\noutput under the training of two-stage rewards. We evaluated our method on\nAI2-Thor and RoboTHOR and demonstrated that it significantly outperforms\nstate-of-the-art (SOTA) methods on success rate and navigation efficiency.",
      "tldr_zh": "本文提出了一种针对视觉对象导航 (VON) 任务的两阶段深度增强学习方法，旨在解决传统策略探索和先验知识利用的瓶颈问题。该方法在搜索阶段鼓励代理探索更大区域，并在导航阶段优化路径，通过两阶段奖励机制提高成功率和训练效率；同时，利用 RGB 和深度信息预训练特征提取器，并记忆障碍信息以减少碰撞和死锁风险。实验在 AI2-Thor 和 RoboTHOR 平台上验证，该方法在成功率和导航效率上显著优于现有最先进 (SOTA) 方法。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2406.14103v1",
      "published_date": "2024-06-20 08:35:10 UTC",
      "updated_date": "2024-06-20 08:35:10 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T22:59:24.537782"
    },
    {
      "arxiv_id": "2406.14097v2",
      "title": "Enhancing the LLM-Based Robot Manipulation Through Human-Robot Collaboration",
      "title_zh": "通过人机协作提升基于LLM的机器人操控",
      "authors": [
        "Haokun Liu",
        "Yaonan Zhu",
        "Kenji Kato",
        "Atsushi Tsukahara",
        "Izumi Kondo",
        "Tadayoshi Aoyama",
        "Yasuhisa Hasegawa"
      ],
      "abstract": "Large Language Models (LLMs) are gaining popularity in the field of robotics.\nHowever, LLM-based robots are limited to simple, repetitive motions due to the\npoor integration between language models, robots, and the environment. This\npaper proposes a novel approach to enhance the performance of LLM-based\nautonomous manipulation through Human-Robot Collaboration (HRC). The approach\ninvolves using a prompted GPT-4 language model to decompose high-level language\ncommands into sequences of motions that can be executed by the robot. The\nsystem also employs a YOLO-based perception algorithm, providing visual cues to\nthe LLM, which aids in planning feasible motions within the specific\nenvironment. Additionally, an HRC method is proposed by combining teleoperation\nand Dynamic Movement Primitives (DMP), allowing the LLM-based robot to learn\nfrom human guidance. Real-world experiments have been conducted using the\nToyota Human Support Robot for manipulation tasks. The outcomes indicate that\ntasks requiring complex trajectory planning and reasoning over environments can\nbe efficiently accomplished through the incorporation of human demonstrations.",
      "tldr_zh": "这篇论文提出了一种通过 Human-Robot Collaboration (HRC) 提升基于 Large Language Models (LLMs) 的机器人操作性能的方法，以解决现有系统在语言模型、机器人和环境整合方面的局限性。方法包括使用提示过的 GPT-4 将高层语言命令分解为可执行动作序列，并结合 YOLO-based 感知算法提供视觉线索以辅助环境规划，同时引入 teleoperation 和 Dynamic Movement Primitives (DMP) 让机器人从人类指导中学习。实验在 Toyota Human Support Robot 上进行，结果表明，该方法能高效完成复杂轨迹规划和环境推理任务，提高了机器人自主操作的效率和准确性。",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.HC"
      ],
      "primary_category": "cs.RO",
      "comment": "IEEE Robotics and Automation Letters",
      "pdf_url": "http://arxiv.org/pdf/2406.14097v2",
      "published_date": "2024-06-20 08:23:49 UTC",
      "updated_date": "2024-07-01 06:11:31 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T22:59:36.791740"
    },
    {
      "arxiv_id": "2406.14096v3",
      "title": "Graph Neural Networks for Job Shop Scheduling Problems: A Survey",
      "title_zh": "翻译失败",
      "authors": [
        "Igor G. Smit",
        "Jianan Zhou",
        "Robbert Reijnen",
        "Yaoxin Wu",
        "Jian Chen",
        "Cong Zhang",
        "Zaharah Bukhsh",
        "Yingqian Zhang",
        "Wim Nuijten"
      ],
      "abstract": "Job shop scheduling problems (JSSPs) represent a critical and challenging\nclass of combinatorial optimization problems. Recent years have witnessed a\nrapid increase in the application of graph neural networks (GNNs) to solve\nJSSPs, albeit lacking a systematic survey of the relevant literature. This\npaper aims to thoroughly review prevailing GNN methods for different types of\nJSSPs and the closely related flow-shop scheduling problems (FSPs), especially\nthose leveraging deep reinforcement learning (DRL). We begin by presenting the\ngraph representations of various JSSPs, followed by an introduction to the most\ncommonly used GNN architectures. We then review current GNN-based methods for\neach problem type, highlighting key technical elements such as graph\nrepresentations, GNN architectures, GNN tasks, and training algorithms.\nFinally, we summarize and analyze the advantages and limitations of GNNs in\nsolving JSSPs and provide potential future research opportunities. We hope this\nsurvey can motivate and inspire innovative approaches for more powerful\nGNN-based approaches in tackling JSSPs and other scheduling problems.",
      "tldr_zh": "这篇论文对图神经网络 (GNNs) 在作业车间调度问题 (JSSPs) 中的应用进行了系统性调查，涵盖了不同类型 JSSPs 和相关流车间调度问题 (FSPs)，特别强调了结合深度强化学习 (DRL) 的方法。论文首先介绍了 JSSPs 的图表示和常用 GNN 架构，然后回顾了针对各问题类型的 GNN 方法，包括关键技术元素如图表示、GNN 架构、任务和训练算法。最终，总结了 GNNs 在解决 JSSPs 时的优势（如高效处理复杂优化问题）和局限性，并提出了未来研究机会，以激励更创新的 GNN 基于方法。",
      "categories": [
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "Accepted by Computers & Operations Research",
      "pdf_url": "http://arxiv.org/pdf/2406.14096v3",
      "published_date": "2024-06-20 08:22:07 UTC",
      "updated_date": "2024-12-06 05:18:37 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T22:59:47.512986"
    },
    {
      "arxiv_id": "2406.14095v2",
      "title": "Memory-Efficient Gradient Unrolling for Large-Scale Bi-level Optimization",
      "title_zh": "内存高效的梯度展开用于大规模双层优化",
      "authors": [
        "Qianli Shen",
        "Yezhen Wang",
        "Zhouhao Yang",
        "Xiang Li",
        "Haonan Wang",
        "Yang Zhang",
        "Jonathan Scarlett",
        "Zhanxing Zhu",
        "Kenji Kawaguchi"
      ],
      "abstract": "Bi-level optimization (BO) has become a fundamental mathematical framework\nfor addressing hierarchical machine learning problems. As deep learning models\ncontinue to grow in size, the demand for scalable bi-level optimization\nsolutions has become increasingly critical. Traditional gradient-based bi-level\noptimization algorithms, due to their inherent characteristics, are ill-suited\nto meet the demands of large-scale applications. In this paper, we introduce\n$\\textbf{F}$orward $\\textbf{G}$radient $\\textbf{U}$nrolling with\n$\\textbf{F}$orward $\\textbf{F}$radient, abbreviated as\n$(\\textbf{FG})^2\\textbf{U}$, which achieves an unbiased stochastic\napproximation of the meta gradient for bi-level optimization.\n$(\\text{FG})^2\\text{U}$ circumvents the memory and approximation issues\nassociated with classical bi-level optimization approaches, and delivers\nsignificantly more accurate gradient estimates than existing large-scale\nbi-level optimization approaches. Additionally, $(\\text{FG})^2\\text{U}$ is\ninherently designed to support parallel computing, enabling it to effectively\nleverage large-scale distributed computing systems to achieve significant\ncomputational efficiency. In practice, $(\\text{FG})^2\\text{U}$ and other\nmethods can be strategically placed at different stages of the training process\nto achieve a more cost-effective two-phase paradigm. Further,\n$(\\text{FG})^2\\text{U}$ is easy to implement within popular deep learning\nframeworks, and can be conveniently adapted to address more challenging\nzeroth-order bi-level optimization scenarios. We provide a thorough convergence\nanalysis and a comprehensive practical discussion for $(\\text{FG})^2\\text{U}$,\ncomplemented by extensive empirical evaluations, showcasing its superior\nperformance in diverse large-scale bi-level optimization tasks. Code is\navailable at https://github.com/ShenQianli/FG2U.",
      "tldr_zh": "这篇论文针对大规模双层优化（bi-level optimization）问题，提出了一种内存高效的方法 $(\\text{FG})^2\\text{U}$，它通过前向梯度展开（Forward Gradient Unrolling with Forward Gradient）实现无偏的随机元梯度（meta gradient）逼近，从而解决传统算法的内存和逼近限制。\n该方法支持并行计算，提高计算效率，并可与其他策略结合形成成本更低的双阶段训练范式，同时适用于零阶双层优化场景。\n实验评估证明，$(\\text{FG})^2\\text{U}$ 在多样化的大型任务中提供更准确的梯度估计，并展示了优越性能，代码已在 GitHub 开源。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2406.14095v2",
      "published_date": "2024-06-20 08:21:52 UTC",
      "updated_date": "2024-12-24 10:44:10 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T23:00:02.905594"
    },
    {
      "arxiv_id": "2406.14090v2",
      "title": "Emotion-aware Personalized Music Recommendation with a Heterogeneity-aware Deep Bayesian Network",
      "title_zh": "情感感知的个性化音乐推荐系统：采用感知异质性的深度贝叶斯网络",
      "authors": [
        "Erkang Jing",
        "Yezheng Liu",
        "Yidong Chai",
        "Shuo Yu",
        "Longshun Liu",
        "Yuanchun Jiang",
        "Yang Wang"
      ],
      "abstract": "Music recommender systems play a critical role in music streaming platforms\nby providing users with music that they are likely to enjoy. Recent studies\nhave shown that user emotions can influence users' preferences for music moods.\nHowever, existing emotion-aware music recommender systems (EMRSs) explicitly or\nimplicitly assume that users' actual emotional states expressed through\nidentical emotional words are homogeneous. They also assume that users' music\nmood preferences are homogeneous under the same emotional state. In this\narticle, we propose four types of heterogeneity that an EMRS should account\nfor: emotion heterogeneity across users, emotion heterogeneity within a user,\nmusic mood preference heterogeneity across users, and music mood preference\nheterogeneity within a user. We further propose a Heterogeneity-aware Deep\nBayesian Network (HDBN) to model these assumptions. The HDBN mimics a user's\ndecision process of choosing music with four components: personalized prior\nuser emotion distribution modeling, posterior user emotion distribution\nmodeling, user grouping, and Bayesian neural network-based music mood\npreference prediction. We constructed two datasets, called EmoMusicLJ and\nEmoMusicLJ-small, to validate our method. Extensive experiments demonstrate\nthat our method significantly outperforms baseline approaches on metrics of HR,\nPrecision, NDCG, and MRR. Ablation studies and case studies further validate\nthe effectiveness of our HDBN. The source code and datasets are available at\nhttps://github.com/jingrk/HDBN.",
      "tldr_zh": "本研究提出了一种Emotion-aware Personalized Music Recommendation系统，通过Heterogeneity-aware Deep Bayesian Network (HDBN)模型来处理用户情绪和音乐心情偏好的四种异质性，包括用户间的情绪异质性、用户内的情绪异质性、用户间的音乐偏好异质性以及用户内的音乐偏好异质性。HDBN模拟用户决策过程，包括个性化先验用户情绪分布建模、后验用户情绪分布建模、用户分组以及基于Bayesian neural network的音乐心情偏好预测。研究构建了EmoMusicLJ和EmoMusicLJ-small数据集，并通过实验验证HDBN在HR、Precision、NDCG和MRR等指标上显著优于基线方法，证明了其有效性。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "43 pages, 20 figures",
      "pdf_url": "http://arxiv.org/pdf/2406.14090v2",
      "published_date": "2024-06-20 08:12:11 UTC",
      "updated_date": "2024-11-29 13:43:59 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T23:00:23.867511"
    },
    {
      "arxiv_id": "2406.14088v2",
      "title": "ReaL: Efficient RLHF Training of Large Language Models with Parameter Reallocation",
      "title_zh": "ReaL：通过参数重新分配实现大语言",
      "authors": [
        "Zhiyu Mei",
        "Wei Fu",
        "Kaiwei Li",
        "Guangju Wang",
        "Huanchen Zhang",
        "Yi Wu"
      ],
      "abstract": "Reinforcement Learning from Human Feedback (RLHF) is a pivotal technique for\nempowering large language model (LLM) applications. Compared with the\nsupervised training process of LLMs, the RLHF training process is much more\nsophisticated, requiring a diverse range of computation workloads with\nintricate dependencies between multiple LLM instances. Therefore, simply\nadopting the fixed parallelization strategies from supervised training for LLMs\ncan be insufficient for RLHF and result in low training efficiency. To overcome\nthis limitation, we propose a novel technique named parameter ReaLlocation,\nwhich dynamically adapts the parallelization strategies for different workloads\nduring training by redistributing LLM parameters across the training cluster.\nBuilding upon this idea, we introduce ReaL, a pioneering system for efficient\nRLHF training. ReaL introduces the concept of an execution plan, which defines\na fine-grained resource allocation and parallelization strategy particularly\ndesigned for RLHF training. Based on this concept, ReaL employs a tailored\nsearch algorithm with a lightweight run-time estimator to automatically\ndiscover an efficient execution plan for an instance of RLHF experiment.\nSubsequently, the runtime engine deploys the selected plan by effectively\nparallelizing computations and redistributing parameters. We evaluate ReaL on\nthe LLaMA models with up to 70 billion parameters and 128 GPUs. The\nexperimental results demonstrate that ReaL achieves speedups of up to\n$3.58\\times$ compared to baseline methods. Furthermore, the execution plans\ngenerated by ReaL exhibit an average of $81\\%$ performance improvement over\nheuristic approaches based on Megatron-LM in the long-context scenario. The\nsource code of ReaL is publicly available at\nhttps://github.com/openpsi-project/ReaLHF .",
      "tldr_zh": "该论文提出了一种名为ReaL的系统，用于提升大型语言模型(LLM)的Reinforcement Learning from Human Feedback (RLHF)训练效率。ReaL引入了parameter ReaLlocation技术，通过动态重新分配LLM参数来适应RLHF训练中的多样化工作负载和复杂依赖，从而优化并行策略。系统采用执行计划(execution plan)概念，并结合定制搜索算法和轻量级运行时估计器，自动生成高效的资源分配方案。实验结果显示，ReaL在LLaMA模型上（多达70亿参数和128 GPUs）比基线方法加速高达3.58倍，并在长上下文场景中比Megatron-LM的启发式方法平均提升81%的性能。",
      "categories": [
        "cs.DC",
        "cs.AI",
        "cs.CL",
        "cs.LG"
      ],
      "primary_category": "cs.DC",
      "comment": "11 pages (20 pages with references and the appendix), 17 figures.\n  Accepted by MLSys 25",
      "pdf_url": "http://arxiv.org/pdf/2406.14088v2",
      "published_date": "2024-06-20 08:04:07 UTC",
      "updated_date": "2025-04-24 13:24:42 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T23:00:25.115070"
    },
    {
      "arxiv_id": "2406.14087v1",
      "title": "Semi Supervised Heterogeneous Domain Adaptation via Disentanglement and Pseudo-Labelling",
      "title_zh": "半监督异构域适应通过解缠和伪标签",
      "authors": [
        "Cassio F. Dantas",
        "Raffaele Gaetano",
        "Dino Ienco"
      ],
      "abstract": "Semi-supervised domain adaptation methods leverage information from a source\nlabelled domain with the goal of generalizing over a scarcely labelled target\ndomain. While this setting already poses challenges due to potential\ndistribution shifts between domains, an even more complex scenario arises when\nsource and target data differs in modality representation (e.g. they are\nacquired by sensors with different characteristics). For instance, in remote\nsensing, images may be collected via various acquisition modes (e.g. optical or\nradar), different spectral characteristics (e.g. RGB or multi-spectral) and\nspatial resolutions. Such a setting is denoted as Semi-Supervised Heterogeneous\nDomain Adaptation (SSHDA) and it exhibits an even more severe distribution\nshift due to modality heterogeneity across domains.To cope with the challenging\nSSHDA setting, here we introduce SHeDD (Semi-supervised Heterogeneous Domain\nAdaptation via Disentanglement) an end-to-end neural framework tailored to\nlearning a target domain classifier by leveraging both labelled and unlabelled\ndata from heterogeneous data sources. SHeDD is designed to effectively\ndisentangle domain-invariant representations, relevant for the downstream task,\nfrom domain-specific information, that can hinder the cross-modality transfer.\nAdditionally, SHeDD adopts an augmentation-based consistency regularization\nmechanism that takes advantages of reliable pseudo-labels on the unlabelled\ntarget samples to further boost its generalization ability on the target\ndomain. Empirical evaluations on two remote sensing benchmarks, encompassing\nheterogeneous data in terms of acquisition modes and spectral/spatial\nresolutions, demonstrate the quality of SHeDD compared to both baseline and\nstate-of-the-art competing approaches. Our code is publicly available here:\nhttps://github.com/tanodino/SSHDA/",
      "tldr_zh": "本研究针对半监督异构域适应（Semi-Supervised Heterogeneous Domain Adaptation, SSHDA）问题，提出了一种名为 SHeDD 的端到端神经框架，用于从源域有标签数据向目标域少标签数据的泛化，尤其在模态表示（如传感器类型、光谱特征和空间分辨率）存在差异时。SHeDD 通过解缠结（Disentanglement）技术分离域不变表示（domain-invariant representations）和域特定信息（domain-specific information），以促进跨模态转移，并结合基于增广的一致性正则化（augmentation-based consistency regularization）和可靠的伪标签（Pseudo-Labelling）机制，提升在目标域上的泛化能力。在两个遥感基准测试中，SHeDD 表现出色，优于基线和最先进方法，代码已公开可用。",
      "categories": [
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2406.14087v1",
      "published_date": "2024-06-20 08:02:49 UTC",
      "updated_date": "2024-06-20 08:02:49 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T23:00:36.854234"
    },
    {
      "arxiv_id": "2406.14086v1",
      "title": "Seg-LSTM: Performance of xLSTM for Semantic Segmentation of Remotely Sensed Images",
      "title_zh": "Seg-LSTM：xLSTM 在遥感",
      "authors": [
        "Qinfeng Zhu",
        "Yuanzhi Cai",
        "Lei Fan"
      ],
      "abstract": "Recent advancements in autoregressive networks with linear complexity have\ndriven significant research progress, demonstrating exceptional performance in\nlarge language models. A representative model is the Extended Long Short-Term\nMemory (xLSTM), which incorporates gating mechanisms and memory structures,\nperforming comparably to Transformer architectures in long-sequence language\ntasks. Autoregressive networks such as xLSTM can utilize image serialization to\nextend their application to visual tasks such as classification and\nsegmentation. Although existing studies have demonstrated Vision-LSTM's\nimpressive results in image classification, its performance in image semantic\nsegmentation remains unverified. Our study represents the first attempt to\nevaluate the effectiveness of Vision-LSTM in the semantic segmentation of\nremotely sensed images. This evaluation is based on a specifically designed\nencoder-decoder architecture named Seg-LSTM, and comparisons with\nstate-of-the-art segmentation networks. Our study found that Vision-LSTM's\nperformance in semantic segmentation was limited and generally inferior to\nVision-Transformers-based and Vision-Mamba-based models in most comparative\ntests. Future research directions for enhancing Vision-LSTM are recommended.\nThe source code is available from https://github.com/zhuqinfeng1999/Seg-LSTM.",
      "tldr_zh": "本文评估了xLSTM在遥感图像语义分割中的性能，这是首次针对该领域的尝试，提出了一种名为Seg-LSTM的编码器-解码器架构，将xLSTM应用于视觉任务。研究通过图像序列化方法与最先进的分段网络（如Vision-Transformers和Vision-Mamba）进行比较，结果显示Vision-LSTM在大多数测试中表现有限且逊色。未来研究应聚焦于提升Vision-LSTM的优化策略，源代码可从GitHub获取。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2406.14086v1",
      "published_date": "2024-06-20 08:01:28 UTC",
      "updated_date": "2024-06-20 08:01:28 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T23:00:48.082844"
    },
    {
      "arxiv_id": "2406.14085v1",
      "title": "Teaching Models To Survive: Proper Scoring Rule and Stochastic Optimization with Competing Risks",
      "title_zh": "翻译失败",
      "authors": [
        "Julie Alberge",
        "Vincent Maladière",
        "Olivier Grisel",
        "Judith Abécassis",
        "Gaël Varoquaux"
      ],
      "abstract": "When data are right-censored, i.e. some outcomes are missing due to a limited\nperiod of observation, survival analysis can compute the \"time to event\".\nMultiple classes of outcomes lead to a classification variant: predicting the\nmost likely event, known as competing risks, which has been less studied. To\nbuild a loss that estimates outcome probabilities for such settings, we\nintroduce a strictly proper censoring-adjusted separable scoring rule that can\nbe optimized on a subpart of the data because the evaluation is made\nindependently of observations. It enables stochastic optimization for competing\nrisks which we use to train gradient boosting trees. Compared to 11\nstate-of-the-art models, this model, MultiIncidence, performs best in\nestimating the probability of outcomes in survival and competing risks. It can\npredict at any time horizon and is much faster than existing alternatives.",
      "tldr_zh": "这篇论文针对右删失数据中的生存分析（survival analysis）和竞争风险（competing risks）问题，引入了一个严格适当的删失调整可分离 scoring rule（strictly proper censoring-adjusted separable scoring rule），以估计结果概率并支持随机优化（stochastic optimization）。作者使用该规则训练梯度提升树（gradient boosting trees），开发出 MultiIncidence 模型，能够在数据子集上独立评估预测。实验结果显示，MultiIncidence 在估计生存和竞争风险结果概率方面优于 11 个最先进模型，且支持任意时间范围的预测，同时计算速度显著更快。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2406.14085v1",
      "published_date": "2024-06-20 08:00:42 UTC",
      "updated_date": "2024-06-20 08:00:42 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T23:01:00.216148"
    },
    {
      "arxiv_id": "2406.14073v1",
      "title": "Exploring Layerwise Adversarial Robustness Through the Lens of t-SNE",
      "title_zh": "翻译失败",
      "authors": [
        "Inês Valentim",
        "Nuno Antunes",
        "Nuno Lourenço"
      ],
      "abstract": "Adversarial examples, designed to trick Artificial Neural Networks (ANNs)\ninto producing wrong outputs, highlight vulnerabilities in these models.\nExploring these weaknesses is crucial for developing defenses, and so, we\npropose a method to assess the adversarial robustness of image-classifying\nANNs. The t-distributed Stochastic Neighbor Embedding (t-SNE) technique is used\nfor visual inspection, and a metric, which compares the clean and perturbed\nembeddings, helps pinpoint weak spots in the layers. Analyzing two ANNs on\nCIFAR-10, one designed by humans and another via NeuroEvolution, we found that\ndifferences between clean and perturbed representations emerge early on, in the\nfeature extraction layers, affecting subsequent classification. The findings\nwith our metric are supported by the visual analysis of the t-SNE maps.",
      "tldr_zh": "本研究探讨了图像分类人工神经网络(ANNs)的层级对抗鲁棒性，通过t-distributed Stochastic Neighbor Embedding (t-SNE)技术进行可视化分析。研究提出了一种指标，用于比较ANNs的干净嵌入和扰动嵌入，从而识别各层中的弱点。在CIFAR-10数据集上，对一个由人类设计和另一个通过NeuroEvolution生成的ANNs进行分析，发现对抗样本引发的差异在特征提取层早期出现，并影响后续分类。这些发现通过t-SNE地图的视觉检验得到支持，为提升ANNs的对抗防御提供了新见解。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.NE"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2406.14073v1",
      "published_date": "2024-06-20 07:50:11 UTC",
      "updated_date": "2024-06-20 07:50:11 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T23:01:13.555753"
    },
    {
      "arxiv_id": "2406.14066v2",
      "title": "Optimizing Speculative Decoding for Serving Large Language Models Using Goodput",
      "title_zh": "翻译失败",
      "authors": [
        "Xiaoxuan Liu",
        "Cade Daniel",
        "Langxiang Hu",
        "Woosuk Kwon",
        "Zhuohan Li",
        "Xiangxi Mo",
        "Alvin Cheung",
        "Zhijie Deng",
        "Ion Stoica",
        "Hao Zhang"
      ],
      "abstract": "Reducing the inference latency of large language models (LLMs) is crucial,\nand speculative decoding (SD) stands out as one of the most effective\ntechniques. Rather than letting the LLM generate all tokens directly,\nspeculative decoding employs effective proxies to predict potential outputs,\nwhich are then verified by the LLM without compromising the generation quality.\nYet, deploying SD in real online LLM serving systems (with continuous batching)\ndoes not always yield improvement -- under higher request rates or low\nspeculation accuracy, it paradoxically increases latency. Furthermore, there is\nno best speculation length work for all workloads under different system loads.\nBased on the observations, we develop a dynamic framework SmartSpec. SmartSpec\ndynamically determines the best speculation length for each request (from 0,\ni.e., no speculation, to many tokens) -- hence the associated speculative\nexecution costs -- based on a new metric called goodput, which characterizes\nthe current observed load of the entire system and the speculation accuracy. We\nshow that SmartSpec consistently reduces average request latency by up to 3.2x\ncompared to non-speculative decoding baselines across different sizes of target\nmodels, draft models, request rates, and datasets. Moreover, SmartSpec can be\napplied to different styles of speculative decoding, including traditional,\nmodel-based approaches as well as model-free methods like prompt lookup and\ntree-style decoding.",
      "tldr_zh": "这篇论文针对大语言模型(LLMs)的推理延迟问题，优化了推测性解码(SD)技术，通过引入一个新指标goodput来评估系统负载和推测准确率。作者提出SmartSpec框架，该框架动态调整每个请求的推测长度（从0到多个），以避免SD在高负载下反而增加延迟。实验结果显示，SmartSpec相较于非推测解码基线，可将平均请求延迟降低高达3.2倍，并适用于传统、基于模型的SD方法以及无模型方法如提示查找和树式解码。",
      "categories": [
        "cs.AI",
        "cs.PF"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2406.14066v2",
      "published_date": "2024-06-20 07:43:33 UTC",
      "updated_date": "2024-06-25 20:53:16 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T23:01:26.634260"
    },
    {
      "arxiv_id": "2406.14051v1",
      "title": "How Many Parameters Does it Take to Change a Light Bulb? Evaluating Performance in Self-Play of Conversational Games as a Function of Model Characteristics",
      "title_zh": "翻译失败",
      "authors": [
        "Nidhir Bhavsar",
        "Jonathan Jordan",
        "Sherzod Hakimov",
        "David Schlangen"
      ],
      "abstract": "What makes a good Large Language Model (LLM)? That it performs well on the\nrelevant benchmarks -- which hopefully measure, with some validity, the\npresence of capabilities that are also challenged in real application. But what\nmakes the model perform well? What gives a model its abilities? We take a\nrecently introduced type of benchmark that is meant to challenge capabilities\nin a goal-directed, agentive context through self-play of conversational games,\nand analyse how performance develops as a function of model characteristics\nlike number of parameters, or type of training. We find that while there is a\nclear relationship between number of parameters and performance, there is still\na wide spread of performance points within a given size bracket, which is to be\naccounted for by training parameters such as fine-tuning data quality and\nmethod. From a more practical angle, we also find a certain degree of\nunpredictability about performance across access methods, possible due to\nunexposed sampling parameters, and a, very welcome, performance stability\nagainst at least moderate weight quantisation during inference.",
      "tldr_zh": "本研究评估了大型语言模型 (LLM) 的性能，焦点是通过自玩对话游戏的基准来分析模型能力如何受参数数量、训练类型等特征影响。研究发现，模型参数数量与性能呈正相关，但同一参数规模下，性能差异显著，主要取决于训练数据质量和方法。实验还揭示了不同访问方法可能导致性能的不确定性，以及模型对中等权重量化具有良好的稳定性，从而为理解和优化LLM能力提供了实用洞见。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "under review",
      "pdf_url": "http://arxiv.org/pdf/2406.14051v1",
      "published_date": "2024-06-20 07:17:09 UTC",
      "updated_date": "2024-06-20 07:17:09 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T23:01:36.635641"
    },
    {
      "arxiv_id": "2406.14045v2",
      "title": "LTSM-Bundle: A Toolbox and Benchmark on Large Language Models for Time Series Forecasting",
      "title_zh": "翻译失败",
      "authors": [
        "Yu-Neng Chuang",
        "Songchen Li",
        "Jiayi Yuan",
        "Guanchu Wang",
        "Kwei-Herng Lai",
        "Songyuan Sui",
        "Leisheng Yu",
        "Sirui Ding",
        "Chia-Yuan Chang",
        "Qiaoyu Tan",
        "Daochen Zha",
        "Xia Hu"
      ],
      "abstract": "Time Series Forecasting (TSF) has long been a challenge in time series\nanalysis. Inspired by the success of Large Language Models (LLMs), researchers\nare now developing Large Time Series Models (LTSMs)-universal transformer-based\nmodels that use autoregressive prediction-to improve TSF. However, training\nLTSMs on heterogeneous time series data poses unique challenges, including\ndiverse frequencies, dimensions, and patterns across datasets. Recent endeavors\nhave studied and evaluated various design choices aimed at enhancing LTSM\ntraining and generalization capabilities. However, these design choices are\ntypically studied and evaluated in isolation and are not benchmarked\ncollectively. In this work, we introduce LTSM-Bundle, a comprehensive toolbox,\nand benchmark for training LTSMs, spanning pre-processing techniques, model\nconfigurations, and dataset configuration. It modularized and benchmarked LTSMs\nfrom multiple dimensions, encompassing prompting strategies, tokenization\napproaches, training paradigms, base model selection, data quantity, and\ndataset diversity. Furthermore, we combine the most effective design choices\nidentified in our study. Empirical results demonstrate that this combination\nachieves superior zero-shot and few-shot performances compared to\nstate-of-the-art LTSMs and traditional TSF methods on benchmark datasets.",
      "tldr_zh": "本文提出LTSM-Bundle，一种综合工具箱和基准测试，用于评估和训练大型时间序列模型（LTSMs），以解决时间序列预测（TSF）中数据集异质性带来的挑战。工具箱模块化覆盖了多个维度，包括提示策略、标记化方法、训练范式、基础模型选择、数据量和数据集多样性，并通过结合最有效的设计选择进行优化。实验结果显示，这种组合方法在基准数据集上实现了优于现有LTSMs和传统TSF方法的零样本和少样本性能。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2406.14045v2",
      "published_date": "2024-06-20 07:09:19 UTC",
      "updated_date": "2025-02-27 23:12:38 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T23:01:50.888039"
    },
    {
      "arxiv_id": "2406.14039v1",
      "title": "CryptoGPT: a 7B model rivaling GPT-4 in the task of analyzing and classifying real-time financial news",
      "title_zh": "翻译失败",
      "authors": [
        "Ying Zhang",
        "Matthieu Petit Guillaume",
        "Aurélien Krauth",
        "Manel Labidi"
      ],
      "abstract": "CryptoGPT: a 7B model competing with GPT-4 in a specific task -- The Impact\nof Automatic Annotation and Strategic Fine-Tuning via QLoRAIn this article, we\npresent a method aimed at refining a dedicated LLM of reasonable quality with\nlimited resources in an industrial setting via CryptoGPT. It is an LLM designed\nfor financial news analysis for the cryptocurrency market in real-time. This\nproject was launched in an industrial context. This model allows not only for\nthe classification of financial information but also for providing\ncomprehensive analysis. We refined different LLMs of the same size such as\nMistral-7B and LLama-7B using semi-automatic annotation and compared them with\nvarious LLMs such as GPT-3.5 and GPT-4. Our goal is to find a balance among\nseveral needs: 1. Protecting data (by avoiding their transfer to external\nservers), 2. Limiting annotation cost and time, 3. Controlling the model's size\n(to manage deployment costs), and 4. Maintaining better analysis quality.",
      "tldr_zh": "本研究介绍了 CryptoGPT，一种7B参数的LLM，旨在与GPT-4在实时金融新闻分析和分类任务上竞争，专注于加密货币市场。研究采用半自动注解和QLoRA战略微调方法，对Mistral-7B和Llama-7B等模型进行优化，与GPT-3.5和GPT-4进行比较。目标是平衡数据保护、注解成本、模型部署规模和分析质量，实现高效的工业级应用。",
      "categories": [
        "cs.AI",
        "cs.CE",
        "cs.CL",
        "cs.NE"
      ],
      "primary_category": "cs.AI",
      "comment": "Journ{\\'e}e Nationale sur la Fouille de Textes, Pascal CUXAC; Adrien\n  GUILLE; C{\\'e}dric LOPEZ, Jun 2024, Lyon (Universit{\\'e} Lumi{\\`e}re Lyon 2),\n  France",
      "pdf_url": "http://arxiv.org/pdf/2406.14039v1",
      "published_date": "2024-06-20 06:59:46 UTC",
      "updated_date": "2024-06-20 06:59:46 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T23:02:02.979953"
    },
    {
      "arxiv_id": "2406.14038v2",
      "title": "Resource-efficient Medical Image Analysis with Self-adapting Forward-Forward Networks",
      "title_zh": "翻译失败",
      "authors": [
        "Johanna P. Müller",
        "Bernhard Kainz"
      ],
      "abstract": "We introduce a fast Self-adapting Forward-Forward Network (SaFF-Net) for\nmedical imaging analysis, mitigating power consumption and resource\nlimitations, which currently primarily stem from the prevalent reliance on\nback-propagation for model training and fine-tuning. Building upon the recently\nproposed Forward-Forward Algorithm (FFA), we introduce the Convolutional\nForward-Forward Algorithm (CFFA), a parameter-efficient reformulation that is\nsuitable for advanced image analysis and overcomes the speed and generalisation\nconstraints of the original FFA. To address hyper-parameter sensitivity of FFAs\nwe are also introducing a self-adapting framework SaFF-Net fine-tuning\nparameters during warmup and training in parallel. Our approach enables more\neffective model training and eliminates the previously essential requirement\nfor an arbitrarily chosen Goodness function in FFA. We evaluate our approach on\nseveral benchmarking datasets in comparison with standard Back-Propagation (BP)\nneural networks showing that FFA-based networks with notably fewer parameters\nand function evaluations can compete with standard models, especially, in\none-shot scenarios and large batch sizes. The code will be available at the\ntime of the conference.",
      "tldr_zh": "本研究提出了一种资源高效的医疗图像分析方法，即自适应前向前向网络（Self-adapting Forward-Forward Networks, SaFF-Net），旨在减少功耗和资源限制问题。基于原始 Forward-Forward Algorithm (FFA)，他们开发了 Convolutional Forward-Forward Algorithm (CFFA)，这是一个参数高效的改进版本，能够提升图像分析的速度和泛化能力，同时通过自适应框架在预热和训练过程中自动微调超参数，消除了对任意 Goodness 函数的依赖。实验结果显示，SaFF-Net 在多个基准数据集上与传统 Back-Propagation (BP) 神经网络相比，使用更少的参数和函数评估即可实现可比性能，尤其在 one-shot 场景和大批量大小下表现出优势。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "Accepted for MICCAI Workshop MLMI 2024",
      "pdf_url": "http://arxiv.org/pdf/2406.14038v2",
      "published_date": "2024-06-20 06:58:09 UTC",
      "updated_date": "2024-07-17 11:35:37 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T23:02:14.911305"
    },
    {
      "arxiv_id": "2406.14036v2",
      "title": "Towards Infinite-Long Prefix in Transformer",
      "title_zh": "迈向 Transformer 中的无限长前缀",
      "authors": [
        "Yingyu Liang",
        "Zhenmei Shi",
        "Zhao Song",
        "Chiwun Yang"
      ],
      "abstract": "Prompting and context-based fine-tuning methods, which we call Prefix\nLearning, have been proposed to enhance the performance of language models on\nvarious downstream tasks. They are empirically efficient and effective,\nmatching the performance of full parameter fine-tuning, but the theoretical\nunderstandings are limited. In this paper, we aim to address this limitation by\nstudying their ability from the perspective of prefix length. In particular, we\nprovide a convergence guarantee for training an ultra-long prefix in a stylized\nsetting using the Neural Tangent Kernel (NTK) framework. Based on this strong\ntheoretical guarantee, we design and implement an algorithm that only needs to\nintroduce and fine-tune a few extra trainable parameters instead of an\ninfinite-long prefix in each layer of a transformer, and can approximate the\nprefix attention to a guaranteed polynomial-small error. Preliminary\nexperimental results on vision, natural language, and math data show that our\nmethod achieves superior or competitive performance compared to existing\nmethods like full parameters fine-tuning, P-Tuning V2, and LoRA. This\ndemonstrates our method is promising for parameter-efficient fine-tuning. Our\ncode can be found at\n\\url{https://github.com/ChristianYang37/chiwun/tree/main/src/NTK-Attention}.",
      "tldr_zh": "本文研究了 Prefix Learning 在 Transformer 中的应用，旨在从前缀长度角度提供理论支撑，通过 Neural Tangent Kernel (NTK) 框架给出训练超长前缀的收敛保证，并设计了一种仅需微调少量额外参数的算法，以近似前缀注意力并控制多项式小的错误。相比于全参数微调、P-Tuning V2 和 LoRA，该方法在视觉、自然语言和数学任务上的实验性能表现出优越或竞争性。总体而言，这一方法为参数高效微调提供了高效且可靠的解决方案。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2406.14036v2",
      "published_date": "2024-06-20 06:56:35 UTC",
      "updated_date": "2024-10-16 06:33:44 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T23:02:27.670309"
    },
    {
      "arxiv_id": "2406.14035v3",
      "title": "Using Game Play to Investigate Multimodal and Conversational Grounding in Large Multimodal Models",
      "title_zh": "翻译失败",
      "authors": [
        "Sherzod Hakimov",
        "Yerkezhan Abdullayeva",
        "Kushal Koshti",
        "Antonia Schmidt",
        "Yan Weiser",
        "Anne Beyer",
        "David Schlangen"
      ],
      "abstract": "While the situation has improved for text-only models, it again seems to be\nthe case currently that multimodal (text and image) models develop faster than\nways to evaluate them. In this paper, we bring a recently developed evaluation\nparadigm from text models to multimodal models, namely evaluation through the\ngoal-oriented game (self) play, complementing reference-based and\npreference-based evaluation. Specifically, we define games that challenge a\nmodel's capability to represent a situation from visual information and align\nsuch representations through dialogue. We find that the largest closed models\nperform rather well on the games that we define, while even the best\nopen-weight models struggle with them. On further analysis, we find that the\nexceptional deep captioning capabilities of the largest models drive some of\nthe performance. There is still room to grow for both kinds of models, ensuring\nthe continued relevance of the benchmark.",
      "tldr_zh": "这篇论文提出了一种通过目标导向游戏玩耍来评估大型多模态模型（Large Multimodal Models）在多模态和对话 grounding 方面的能力，填补了当前评估方法的空白。研究者定义了特定游戏，挑战模型从视觉信息中表示情况并通过对话进行对齐。实验结果显示，最大的封闭模型在这些游戏中表现较好，而最佳开源模型则挣扎不前，主要归因于前者出色的深度标题生成能力；该基准仍有提升空间，确保其持续相关性。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "Accepted at COLING 2025",
      "pdf_url": "http://arxiv.org/pdf/2406.14035v3",
      "published_date": "2024-06-20 06:56:19 UTC",
      "updated_date": "2024-12-11 09:56:15 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T23:02:38.833626"
    },
    {
      "arxiv_id": "2406.14027v1",
      "title": "How to design a dataset compliant with an ML-based system ODD?",
      "title_zh": "翻译失败",
      "authors": [
        "Cyril Cappi",
        "Noémie Cohen",
        "Mélanie Ducoffe",
        "Christophe Gabreau",
        "Laurent Gardes",
        "Adrien Gauffriau",
        "Jean-Brice Ginestet",
        "Franck Mamalet",
        "Vincent Mussot",
        "Claire Pagetti",
        "David Vigouroux"
      ],
      "abstract": "This paper focuses on a Vision-based Landing task and presents the design and\nthe validation of a dataset that would comply with the Operational Design\nDomain (ODD) of a Machine-Learning (ML) system. Relying on emerging\ncertification standards, we describe the process for establishing ODDs at both\nthe system and image levels. In the process, we present the translation of\nhigh-level system constraints into actionable image-level properties, allowing\nfor the definition of verifiable Data Quality Requirements (DQRs). To\nillustrate this approach, we use the Landing Approach Runway Detection (LARD)\ndataset which combines synthetic imagery and real footage, and we focus on the\nsteps required to verify the DQRs. The replicable framework presented in this\npaper addresses the challenges of designing a dataset compliant with the\nstringent needs of ML-based systems certification in safety-critical\napplications.",
      "tldr_zh": "本论文探讨了如何设计一个符合机器学习(ML)系统操作设计域(ODD)的数据集，焦点在于基于视觉的着陆任务。作者描述了根据新兴认证标准，建立系统级和图像级ODD的过程，将高层系统约束转化为可验证的图像级属性，从而定义数据质量要求(DQRs)。通过Landing Approach Runway Detection (LARD)数据集的示例，该框架结合合成图像和真实镜头，展示了验证DQRs的步骤，并提供了一个可复制的解决方案，以应对ML系统在安全关键应用中的认证挑战。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "12th European Congress on Embedded Real Time Software and Systems,\n  Jun 2024, Toulouse, France. arXiv admin note: text overlap with\n  arXiv:2304.09938",
      "pdf_url": "http://arxiv.org/pdf/2406.14027v1",
      "published_date": "2024-06-20 06:48:34 UTC",
      "updated_date": "2024-06-20 06:48:34 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T23:02:48.945952"
    },
    {
      "arxiv_id": "2406.14023v2",
      "title": "Evaluating Implicit Bias in Large Language Models by Attacking From a Psychometric Perspective",
      "title_zh": "翻译失败",
      "authors": [
        "Yuchen Wen",
        "Keping Bi",
        "Wei Chen",
        "Jiafeng Guo",
        "Xueqi Cheng"
      ],
      "abstract": "As large language models (LLMs) become an important way of information\naccess, there have been increasing concerns that LLMs may intensify the spread\nof unethical content, including implicit bias that hurts certain populations\nwithout explicit harmful words. In this paper, we conduct a rigorous evaluation\nof LLMs' implicit bias towards certain demographics by attacking them from a\npsychometric perspective to elicit agreements to biased viewpoints. Inspired by\npsychometric principles in cognitive and social psychology, we propose three\nattack approaches, i.e., Disguise, Deception, and Teaching. Incorporating the\ncorresponding attack instructions, we built two benchmarks: (1) a bilingual\ndataset with biased statements covering four bias types (2.7K instances) for\nextensive comparative analysis, and (2) BUMBLE, a larger benchmark spanning\nnine common bias types (12.7K instances) for comprehensive evaluation.\nExtensive evaluation of popular commercial and open-source LLMs shows that our\nmethods can elicit LLMs' inner bias more effectively than competitive\nbaselines. Our attack methodology and benchmarks offer an effective means of\nassessing the ethical risks of LLMs, driving progress toward greater\naccountability in their development.",
      "tldr_zh": "这篇论文从心理测量学视角评估Large Language Models (LLMs) 中的隐性偏见，通过Disguise、Deception 和 Teaching 三种攻击方法来引发模型对偏见观点的同意。研究者构建了两个基准数据集：一个双语数据集（覆盖四种偏见类型，共2.7K 实例）和更大的BUMBLE 基准（覆盖九种常见偏见类型，共12.7K 实例），用于全面比较和评估。实验结果显示，这些方法比竞争基线更有效地暴露LLMs 的内在偏见，从而为评估模型的伦理风险提供有效工具，推动其更负责任的发展。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "Code, data and benchmarks are available at\n  https://github.com/yuchenwen1/ImplicitBiasPsychometricEvaluation and\n  https://github.com/yuchenwen1/BUMBLE",
      "pdf_url": "http://arxiv.org/pdf/2406.14023v2",
      "published_date": "2024-06-20 06:42:08 UTC",
      "updated_date": "2025-02-19 03:37:38 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T23:03:02.749850"
    },
    {
      "arxiv_id": "2406.14020v1",
      "title": "Leveraging eBPF and AI for Ransomware Nose Out",
      "title_zh": "翻译失败",
      "authors": [
        "Arjun Sekar",
        "Sameer G. Kulkarni",
        "Joy Kuri"
      ],
      "abstract": "In this work, we propose a two-phased approach for real-time detection and\ndeterrence of ransomware. To achieve this, we leverage the capabilities of eBPF\n(Extended Berkeley Packet Filter) and artificial intelligence to develop both\nproactive and reactive methods. In the first phase, we utilize signature based\ndetection, where we employ custom eBPF programs to trace the execution of new\nprocesses and perform hash-based analysis against a known ransomware dataset.\nIn the second, we employ a behavior-based technique that focuses on monitoring\nthe process activities using a custom eBPF program and the creation of ransom\nnotes, a prominent indicator of ransomware activity through the use of Natural\nLanguage Processing (NLP). By leveraging low-level tracing capabilities of eBPF\nand integrating NLP based machine learning algorithms, our solution achieves an\nimpressive 99.76% accuracy in identifying ransomware incidents within a few\nseconds on the onset of zero-day attacks.",
      "tldr_zh": "本研究提出了一种利用 eBPF 和 AI 的两阶段方法，用于实时检测和阻止勒索软件(ransomware)。第一阶段采用签名-based detection，通过自定义 eBPF 程序追踪新进程并进行哈希分析，与已知数据集比对；第二阶段则使用行为-based detection，监控进程活动并结合自然语言处理(NLP)算法识别勒索笔记。该方法整合 eBPF 的低级追踪能力和机器学习，实现了在零日攻击发生几秒内的检测，并取得了99.76%的准确率。",
      "categories": [
        "cs.CR",
        "cs.AI",
        "cs.ET",
        "cs.NI"
      ],
      "primary_category": "cs.CR",
      "comment": "7 pages",
      "pdf_url": "http://arxiv.org/pdf/2406.14020v1",
      "published_date": "2024-06-20 06:35:15 UTC",
      "updated_date": "2024-06-20 06:35:15 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T23:03:16.433133"
    },
    {
      "arxiv_id": "2406.14014v1",
      "title": "Feature Fusion Based on Mutual-Cross-Attention Mechanism for EEG Emotion Recognition",
      "title_zh": "翻译失败",
      "authors": [
        "Yimin Zhao",
        "Jin Gu"
      ],
      "abstract": "An objective and accurate emotion diagnostic reference is vital to\npsychologists, especially when dealing with patients who are difficult to\ncommunicate with for pathological reasons. Nevertheless, current systems based\non Electroencephalography (EEG) data utilized for sentiment discrimination have\nsome problems, including excessive model complexity, mediocre accuracy, and\nlimited interpretability. Consequently, we propose a novel and effective\nfeature fusion mechanism named Mutual-Cross-Attention (MCA). Combining with a\nspecially customized 3D Convolutional Neural Network (3D-CNN), this purely\nmathematical mechanism adeptly discovers the complementary relationship between\ntime-domain and frequency-domain features in EEG data. Furthermore, the new\ndesigned Channel-PSD-DE 3D feature also contributes to the high performance.\nThe proposed method eventually achieves 99.49% (valence) and 99.30% (arousal)\naccuracy on DEAP dataset.",
      "tldr_zh": "本研究针对 EEG 情绪识别系统中存在的模型复杂度高、准确率一般和可解释性差等问题，提出了一种新型特征融合机制 Mutual-Cross-Attention (MCA)，它通过数学方法发掘 EEG 数据中时域和频域特征的互补关系，并结合 3D Convolutional Neural Network (3D-CNN) 和新设计的 Channel-PSD-DE 3D 特征来提升性能。实验结果显示，该方法在 DEAP 数据集上达到了 99.49% (valence) 和 99.30% (arousal) 的准确率。总体而言，这为心理学家提供了一个客观、准确的情绪诊断工具，尤其适用于难以沟通的患者。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "The work has been accepted by MICCAI 2024. The uploaded one is\n  preprint which has not undergone peer review (when applicable) or any\n  post-submission improvements or corrections. The official DOI link will be\n  provided once available",
      "pdf_url": "http://arxiv.org/pdf/2406.14014v1",
      "published_date": "2024-06-20 06:08:52 UTC",
      "updated_date": "2024-06-20 06:08:52 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T23:03:29.060786"
    },
    {
      "arxiv_id": "2406.14012v1",
      "title": "Seeing Through AI's Lens: Enhancing Human Skepticism Towards LLM-Generated Fake News",
      "title_zh": "翻译失败",
      "authors": [
        "Navid Ayoobi",
        "Sadat Shahriar",
        "Arjun Mukherjee"
      ],
      "abstract": "LLMs offer valuable capabilities, yet they can be utilized by malicious users\nto disseminate deceptive information and generate fake news. The growing\nprevalence of LLMs poses difficulties in crafting detection approaches that\nremain effective across various text domains. Additionally, the absence of\nprecautionary measures for AI-generated news on online social platforms is\nconcerning. Therefore, there is an urgent need to improve people's ability to\ndifferentiate between news articles written by humans and those produced by\nLLMs. By providing cues in human-written and LLM-generated news, we can help\nindividuals increase their skepticism towards fake LLM-generated news. This\npaper aims to elucidate simple markers that help individuals distinguish\nbetween articles penned by humans and those created by LLMs. To achieve this,\nwe initially collected a dataset comprising 39k news articles authored by\nhumans or generated by four distinct LLMs with varying degrees of fake. We then\ndevise a metric named Entropy-Shift Authorship Signature (ESAS) based on the\ninformation theory and entropy principles. The proposed ESAS ranks terms or\nentities, like POS tagging, within news articles based on their relevance in\ndiscerning article authorship. We demonstrate the effectiveness of our metric\nby showing the high accuracy attained by a basic method, i.e., TF-IDF combined\nwith logistic regression classifier, using a small set of terms with the\nhighest ESAS score. Consequently, we introduce and scrutinize these top\nESAS-ranked terms to aid individuals in strengthening their skepticism towards\nLLM-generated fake news.",
      "tldr_zh": "本研究探讨了大型语言模型（LLMs）生成假新闻的问题，旨在提升人们对这些假新闻的怀疑能力，以应对在线社交平台的潜在风险。研究者收集了39k条人类撰写和LLM生成的新闻数据集，并提出Entropy-Shift Authorship Signature (ESAS)指标，基于信息理论和熵原理来排名文章中的术语（如POS tagging），以区分文章 authorship。实验结果显示，使用TF-IDF结合logistic regression分类器，仅需高ESAS分数术语即可实现高准确率，从而帮助个体识别LLM生成内容并加强对其的怀疑。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2406.14012v1",
      "published_date": "2024-06-20 06:02:04 UTC",
      "updated_date": "2024-06-20 06:02:04 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T23:03:40.277935"
    },
    {
      "arxiv_id": "2406.14005v2",
      "title": "Information Guided Regularization for Fine-tuning Language Models",
      "title_zh": "翻译失败",
      "authors": [
        "Mandar Sharma",
        "Nikhil Muralidhar",
        "Shengzhe Xu",
        "Raquib Bin Yousuf",
        "Naren Ramakrishnan"
      ],
      "abstract": "The pretraining-fine-tuning paradigm has been the de facto strategy for\ntransfer learning in modern language modeling. With the understanding that task\nadaptation in LMs is often a function of parameters shared across tasks, we\nargue that a more surgical approach to regularization needs to exist for\nsmoother transfer learning. Towards this end, we investigate how the\npretraining loss landscape is affected by these task-sensitive parameters\nthrough an information-theoretic lens. We then leverage the findings from our\ninvestigations to devise a novel approach to dropout for improved model\nregularization and better downstream generalization. This approach, named\nguided dropout, is both task & architecture agnostic and adds no computational\noverhead to the fine-tuning process. Through empirical evaluations, we showcase\nthat our approach to regularization yields consistently better performance,\neven in scenarios of data paucity, compared to standardized baselines.",
      "tldr_zh": "该论文探讨了语言模型的预训练-微调范式，强调通过信息论视角分析任务敏感参数对预训练损失景观的影响，以实现更精确的正则化。作者提出了一种名为guided dropout的新方法，该方法针对任务和架构无关、不增加计算开销，能够改善模型正则化和下游泛化。实验结果表明，这种正则化策略在数据稀缺场景下，比标准基线表现出更稳定的性能提升。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2406.14005v2",
      "published_date": "2024-06-20 05:18:37 UTC",
      "updated_date": "2024-06-21 12:41:17 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T23:03:51.597255"
    },
    {
      "arxiv_id": "2406.14003v3",
      "title": "Deep Optimal Experimental Design for Parameter Estimation Problems",
      "title_zh": "针对参数估计问题的深度最优实验设计",
      "authors": [
        "Md Shahriar Rahim Siddiqui",
        "Arman Rahmim",
        "Eldad Haber"
      ],
      "abstract": "Optimal experimental design is a well studied field in applied science and\nengineering. Techniques for estimating such a design are commonly used within\nthe framework of parameter estimation. Nonetheless, in recent years parameter\nestimation techniques are changing rapidly with the introduction of deep\nlearning techniques to replace traditional estimation methods. This in turn\nrequires the adaptation of optimal experimental design that is associated with\nthese new techniques. In this paper we investigate a new experimental design\nmethodology that uses deep learning. We show that the training of a network as\na Likelihood Free Estimator can be used to significantly simplify the design\nprocess and circumvent the need for the computationally expensive bi-level\noptimization problem that is inherent in optimal experimental design for\nnon-linear systems. Furthermore, deep design improves the quality of the\nrecovery process for parameter estimation problems. As proof of concept we\napply our methodology to two different systems of Ordinary Differential\nEquations.",
      "tldr_zh": "这篇论文提出了一种基于深度学习的实验设计方法，用于参数估计问题，以适应传统方法被深度学习技术取代的趋势。该方法通过训练网络作为 Likelihood Free Estimator，简化了设计过程，避免了计算密集的双层优化问题，从而提高了参数估计的恢复质量。作为概念验证，该方法应用于两个不同的 Ordinary Differential Equations 系统，展示了其有效性。",
      "categories": [
        "stat.ML",
        "cs.AI",
        "cs.LG",
        "stat.ME"
      ],
      "primary_category": "stat.ML",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2406.14003v3",
      "published_date": "2024-06-20 05:13:33 UTC",
      "updated_date": "2024-10-16 16:51:13 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T23:04:02.818920"
    },
    {
      "arxiv_id": "2406.13975v3",
      "title": "MR-Ben: A Meta-Reasoning Benchmark for Evaluating System-2 Thinking in LLMs",
      "title_zh": "MR-Ben：用于评估LLMs中系统-2思考的元推理基准",
      "authors": [
        "Zhongshen Zeng",
        "Yinhong Liu",
        "Yingjia Wan",
        "Jingyao Li",
        "Pengguang Chen",
        "Jianbo Dai",
        "Yuxuan Yao",
        "Rongwu Xu",
        "Zehan Qi",
        "Wanru Zhao",
        "Linling Shen",
        "Jianqiao Lu",
        "Haochen Tan",
        "Yukang Chen",
        "Hao Zhang",
        "Zhan Shi",
        "Bailin Wang",
        "Zhijiang Guo",
        "Jiaya Jia"
      ],
      "abstract": "Large language models (LLMs) have shown increasing capability in\nproblem-solving and decision-making, largely based on the step-by-step\nchain-of-thought reasoning processes. However, evaluating these reasoning\nabilities has become increasingly challenging. Existing outcome-based\nbenchmarks are beginning to saturate, becoming less effective in tracking\nmeaningful progress. To address this, we present a process-based benchmark\nMR-Ben that demands a meta-reasoning skill, where LMs are asked to locate and\nanalyse potential errors in automatically generated reasoning steps. Our\nmeta-reasoning paradigm is especially suited for system-2 slow thinking,\nmirroring the human cognitive process of carefully examining assumptions,\nconditions, calculations, and logic to identify mistakes.MR-Ben comprises 5,975\nquestions curated by human experts across a wide range of subjects, including\nphysics, chemistry, logic, coding, and more. Through our designed metrics for\nassessing meta-reasoning on this benchmark, we identify interesting limitations\nand weaknesses of current LLMs (open-source and closed-source models). For\nexample, with models like the o1 series from OpenAI demonstrating strong\nperformance by effectively scrutinizing the solution space, many other\nstate-of-the-art models fall significantly behind on MR-Ben, exposing potential\nshortcomings in their training strategies and inference methodologies.",
      "tldr_zh": "该论文提出MR-Ben，一种基于过程的基准，用于评估大型语言模型(LLMs)的元推理(meta-reasoning)技能，以测试其系统-2思考能力，即仔细检查推理步骤中的错误。MR-Ben包含由专家 curation 的5975个问题，涵盖物理、化学、逻辑、编码等领域，要求模型分析自动生成的chain-of-thought推理过程。实验结果显示，虽然OpenAI的o1系列模型表现出色，但许多其他先进模型表现落后，暴露了其训练策略和推理方法的潜在不足。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2406.13975v3",
      "published_date": "2024-06-20 03:50:23 UTC",
      "updated_date": "2024-12-20 12:52:00 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T23:04:16.359879"
    },
    {
      "arxiv_id": "2406.15249v1",
      "title": "Machine Learning Techniques in Automatic Music Transcription: A Systematic Survey",
      "title_zh": "机器学习技术在自动音乐转录中的系统性调查",
      "authors": [
        "Fatemeh Jamshidi",
        "Gary Pike",
        "Amit Das",
        "Richard Chapman"
      ],
      "abstract": "In the domain of Music Information Retrieval (MIR), Automatic Music\nTranscription (AMT) emerges as a central challenge, aiming to convert audio\nsignals into symbolic notations like musical notes or sheet music. This\nsystematic review accentuates the pivotal role of AMT in music signal analysis,\nemphasizing its importance due to the intricate and overlapping spectral\nstructure of musical harmonies. Through a thorough examination of existing\nmachine learning techniques utilized in AMT, we explore the progress and\nconstraints of current models and methodologies. Despite notable advancements,\nAMT systems have yet to match the accuracy of human experts, largely due to the\ncomplexities of musical harmonies and the need for nuanced interpretation. This\nreview critically evaluates both fully automatic and semi-automatic AMT\nsystems, emphasizing the importance of minimal user intervention and examining\nvarious methodologies proposed to date. By addressing the limitations of prior\ntechniques and suggesting avenues for improvement, our objective is to steer\nfuture research towards fully automated AMT systems capable of accurately and\nefficiently translating intricate audio signals into precise symbolic\nrepresentations. This study not only synthesizes the latest advancements but\nalso lays out a road-map for overcoming existing challenges in AMT, providing\nvaluable insights for researchers aiming to narrow the gap between current\nsystems and human-level transcription accuracy.",
      "tldr_zh": "这篇论文对 Automatic Music Transcription (AMT) 进行了系统性调查，聚焦于机器学习技术在音乐信号分析中的应用，特别是处理音乐和声的复杂谱结构。研究回顾了现有模型的进展和局限，包括全自动和半自动系统，并强调了这些系统在准确性上仍落后于人类专家。最终，论文总结了最新进展，提出改进路径和未来路线图，以推动 AMT 向高效、精确的完全自动化系统发展。",
      "categories": [
        "cs.SD",
        "cs.AI",
        "cs.LG",
        "eess.AS"
      ],
      "primary_category": "cs.SD",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2406.15249v1",
      "published_date": "2024-06-20 03:48:15 UTC",
      "updated_date": "2024-06-20 03:48:15 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T23:04:28.177249"
    },
    {
      "arxiv_id": "2406.15132v1",
      "title": "Younger: The First Dataset for Artificial Intelligence-Generated Neural Network Architecture",
      "title_zh": "Younger：首个用于人工智能生成",
      "authors": [
        "Zhengxin Yang",
        "Wanling Gao",
        "Luzhou Peng",
        "Yunyou Huang",
        "Fei Tang",
        "Jianfeng Zhan"
      ],
      "abstract": "Designing and optimizing neural network architectures typically requires\nextensive expertise, starting with handcrafted designs and then manual or\nautomated refinement. This dependency presents a significant barrier to rapid\ninnovation. Recognizing the complexity of automatically generating neural\nnetwork architecture from scratch, we introduce Younger, a pioneering dataset\nto advance this ambitious goal. Derived from over 174K real-world models across\nmore than 30 tasks from various public model hubs, Younger includes 7,629\nunique architectures, and each is represented as a directed acyclic graph with\ndetailed operator-level information. The dataset facilitates two primary design\nparadigms: global, for creating complete architectures from scratch, and local,\nfor detailed architecture component refinement. By establishing these\ncapabilities, Younger contributes to a new frontier, Artificial\nIntelligence-Generated Neural Network Architecture (AIGNNA). Our experiments\nexplore the potential and effectiveness of Younger for automated architecture\ngeneration and, as a secondary benefit, demonstrate that Younger can serve as a\nbenchmark dataset, advancing the development of graph neural networks. We\nrelease the dataset and code publicly to lower the entry barriers and encourage\nfurther research in this challenging area.",
      "tldr_zh": "本论文介绍了Younger，这是首个用于人工智能生成神经网络架构（AIGNNA）的数据集，旨在解决设计和优化神经网络架构依赖专家知识的难题。该数据集从超过174K真实世界模型中提取，涵盖30多个任务，包括7,629个独特架构，每个以有向无环图（Directed Acyclic Graph）形式表示，并包含操作级详细信息，支持全局架构创建和局部组件细化。实验结果显示，Younger在自动架构生成中表现出色，并可作为图神经网络（Graph Neural Networks）的基准数据集；作者公开了数据集和代码，以促进该领域的进一步研究。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "31 pages, 29 figures, 11 tables",
      "pdf_url": "http://arxiv.org/pdf/2406.15132v1",
      "published_date": "2024-06-20 03:14:56 UTC",
      "updated_date": "2024-06-20 03:14:56 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T23:04:39.317018"
    },
    {
      "arxiv_id": "2406.13960v3",
      "title": "AutoPal: Autonomous Adaptation to Users for Personal AI Companionship",
      "title_zh": "翻译失败",
      "authors": [
        "Yi Cheng",
        "Wenge Liu",
        "Kaishuai Xu",
        "Wenjun Hou",
        "Yi Ouyang",
        "Chak Tou Leong",
        "Xian Wu",
        "Yefeng Zheng"
      ],
      "abstract": "Previous research has demonstrated the potential of AI agents to act as\ncompanions that can provide constant emotional support for humans. In this\npaper, we emphasize the necessity of autonomous adaptation in personal AI\ncompanionship, an underexplored yet promising direction. Such adaptability is\ncrucial as it can facilitate more tailored interactions with users and allow\nthe agent to evolve in response to users' changing needs. However, imbuing\nagents with autonomous adaptability presents unique challenges, including\nidentifying optimal adaptations to meet users' expectations and ensuring a\nsmooth transition during the adaptation process. To address them, we devise a\nhierarchical framework, AutoPal, that enables controllable and authentic\nadjustments to the agent's persona based on user interactions. A\npersonamatching dataset is constructed to facilitate the learning of optimal\npersona adaptations. Extensive experiments demonstrate the effectiveness of\nAutoPal and highlight the importance of autonomous adaptability in AI\ncompanionship.",
      "tldr_zh": "该论文强调了在个人 AI 陪伴中实现自主 adaptation 的必要性，以提供更个性化的用户互动和情感支持。研究提出 AutoPal 框架，这是一个层次化系统，能够基于用户互动进行可控且真实的代理角色调整，并通过构建 persona-matching 数据集来优化适应学习。实验结果证明 AutoPal 有效，提升了 AI 代理的适应性，并突显了这种自主机制在提升陪伴体验中的关键作用。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2406.13960v3",
      "published_date": "2024-06-20 03:02:38 UTC",
      "updated_date": "2024-10-18 03:10:13 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T23:04:51.319355"
    },
    {
      "arxiv_id": "2406.13954v1",
      "title": "Research on Flight Accidents Prediction based Back Propagation Neural Network",
      "title_zh": "基于反向传播神经网络的飞行事故预测研究",
      "authors": [
        "Haoxing Liu",
        "Fangzhou Shen",
        "Haoshen Qin and",
        "Fanru Gao"
      ],
      "abstract": "With the rapid development of civil aviation and the significant improvement\nof people's living standards, taking an air plane has become a common and\nefficient way of travel. However, due to the flight characteris-tics of the\naircraft and the sophistication of the fuselage structure, flight de-lays and\nflight accidents occur from time to time. In addition, the life risk factor\nbrought by aircraft after an accident is also the highest among all means of\ntransportation. In this work, a model based on back-propagation neural network\nwas used to predict flight accidents. By collecting historical flight data,\nincluding a variety of factors such as meteorological conditions, aircraft\ntechnical condition, and pilot experience, we trained a backpropaga-tion neural\nnetwork model to identify potential accident risks. In the model design, a\nmulti-layer perceptron structure is used to optimize the network performance by\nadjusting the number of hidden layer nodes and the learning rate. Experimental\nanalysis shows that the model can effectively predict flight accidents with\nhigh accuracy and reliability.",
      "tldr_zh": "这篇论文研究了基于 Back Propagation Neural Network 的飞行事故预测方法，以应对民航快速发展带来的安全风险。研究团队通过收集历史飞行数据，包括气象条件、飞机技术状况和飞行员经验等因素，训练了一个多层感知器（Multi-Layer Perceptron）结构模型，并通过调整隐藏层节点数和学习率优化网络性能。实验分析显示，该模型在预测飞行事故时表现出高准确性和可靠性，为航空安全提供了有效的风险评估工具。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2406.13954v1",
      "published_date": "2024-06-20 02:51:27 UTC",
      "updated_date": "2024-06-20 02:51:27 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T23:05:03.493138"
    },
    {
      "arxiv_id": "2406.13948v1",
      "title": "CityGPT: Empowering Urban Spatial Cognition of Large Language Models",
      "title_zh": "CityGPT: 增强大型语言模型的城市空间认知能力",
      "authors": [
        "Jie Feng",
        "Yuwei Du",
        "Tianhui Liu",
        "Siqi Guo",
        "Yuming Lin",
        "Yong Li"
      ],
      "abstract": "Large language models(LLMs) with powerful language generation and reasoning\ncapabilities have already achieved success in many domains, e.g., math and code\ngeneration. However, due to the lacking of physical world's corpus and\nknowledge during training, they usually fail to solve many real-life tasks in\nthe urban space. In this paper, we propose CityGPT, a systematic framework for\nenhancing the capability of LLMs on understanding urban space and solving the\nrelated urban tasks by building a city-scale world model in the model. First,\nwe construct a diverse instruction tuning dataset CityInstruction for injecting\nurban knowledge and enhancing spatial reasoning capability effectively. By\nusing a mixture of CityInstruction and general instruction data, we fine-tune\nvarious LLMs (e.g., ChatGLM3-6B, Qwen1.5 and LLama3 series) to enhance their\ncapability without sacrificing general abilities. To further validate the\neffectiveness of proposed methods, we construct a comprehensive benchmark\nCityEval to evaluate the capability of LLMs on diverse urban scenarios and\nproblems. Extensive evaluation results demonstrate that small LLMs trained with\nCityInstruction can achieve competitive performance with commercial LLMs in the\ncomprehensive evaluation of CityEval. The source codes are openly accessible to\nthe research community via https://github.com/tsinghua-fib-lab/CityGPT.",
      "tldr_zh": "本研究提出 CityGPT 框架，以增强大型语言模型 (LLMs) 对城市空间的认知能力，解决它们在真实城市任务中因缺乏物理世界知识而表现不佳的问题。研究团队构建了多样化的指令调整数据集 CityInstruction，用于注入城市知识并提升空间推理能力，并通过混合 CityInstruction 与一般指令数据对各种 LLMs（如 ChatGLM3-6B、Qwen1.5 和 Llama3 系列）进行微调，从而在不牺牲一般能力的情况下改进其性能。同时，构建了全面基准 CityEval 来评估 LLMs 在不同城市场景中的表现，结果显示微调后的小型 LLMs 在 CityEval 的综合评估中可与商业 LLMs 竞争。源码已开源在 GitHub 上。",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2406.13948v1",
      "published_date": "2024-06-20 02:32:16 UTC",
      "updated_date": "2024-06-20 02:32:16 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T23:05:18.053431"
    },
    {
      "arxiv_id": "2406.13947v1",
      "title": "AspirinSum: an Aspect-based utility-preserved de-identification Summarization framework",
      "title_zh": "翻译失败",
      "authors": [
        "Ya-Lun Li"
      ],
      "abstract": "Due to the rapid advancement of Large Language Model (LLM), the whole\ncommunity eagerly consumes any available text data in order to train the LLM.\nCurrently, large portion of the available text data are collected from\ninternet, which has been thought as a cheap source of the training data.\nHowever, when people try to extend the LLM's capability to the personal related\ndomain, such as healthcare or education, the lack of public dataset in these\ndomains make the adaption of the LLM in such domains much slower. The reason of\nlacking public available dataset in such domains is because they usually\ncontain personal sensitive information. In order to comply with privacy law,\nthe data in such domains need to be de-identified before any kind of\ndissemination. It had been much research tried to address this problem for the\nimage or tabular data. However, there was limited research on the efficient and\ngeneral de-identification method for text data. Most of the method based on\nhuman annotation or predefined category list. It usually can not be easily\nadapted to specific domains. The goal of this proposal is to develop a text\nde-identification framework, which can be easily adapted to the specific\ndomain, leverage the existing expert knowledge without further human\nannotation. We propose an aspect-based utility-preserved de-identification\nsummarization framework, AspirinSum, by learning to align expert's aspect from\nexisting comment data, it can efficiently summarize the personal sensitive\ndocument by extracting personal sensitive aspect related sub-sentence and\nde-identify it by substituting it with similar aspect sub-sentence. We envision\nthat the de-identified text can then be used in data publishing, eventually\npublishing our de-identified dataset for downstream task use.",
      "tldr_zh": "该论文针对个人敏感领域（如医疗、教育）的数据隐私问题，提出了一种基于方面的效用保留去标识化总结框架AspirinSum，以解决现有文本de-identification方法依赖人工标注或预定义类别、难以适应特定领域的局限性。该框架通过学习从现有评论数据中对齐专家的方面，提取与个人敏感信息相关的子句子，并用类似方面的子句子替换，从而高效地总结和去标识化文本。AspirinSum的创新在于保留数据效用，同时确保隐私合规，最终可用于数据发布和下游任务，如发布去标识化数据集以加速LLM在这些领域的应用。",
      "categories": [
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2406.13947v1",
      "published_date": "2024-06-20 02:29:46 UTC",
      "updated_date": "2024-06-20 02:29:46 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T23:05:29.765855"
    },
    {
      "arxiv_id": "2406.13945v2",
      "title": "CityBench: Evaluating the Capabilities of Large Language Models for Urban Tasks",
      "title_zh": "CityBench：评估大型语言模型在城市任务中的能力",
      "authors": [
        "Jie Feng",
        "Jun Zhang",
        "Tianhui Liu",
        "Xin Zhang",
        "Tianjian Ouyang",
        "Junbo Yan",
        "Yuwei Du",
        "Siqi Guo",
        "Yong Li"
      ],
      "abstract": "Recently, large language models (LLMs) with extensive general knowledge and\npowerful reasoning abilities have seen rapid development and widespread\napplication. A systematic and reliable evaluation of LLMs or vision-language\nmodel (VLMs) is a crucial step in applying and developing them for various\nfields. There have been some early explorations about the usability of LLMs for\nlimited urban tasks, but a systematic and scalable evaluation benchmark is\nstill lacking. The challenge in constructing a systematic evaluation benchmark\nfor urban research lies in the diversity of urban data, the complexity of\napplication scenarios and the highly dynamic nature of the urban environment.\nIn this paper, we design CityBench, an interactive simulator based evaluation\nplatform, as the first systematic benchmark for evaluating the capabilities of\nLLMs for diverse tasks in urban research. First, we build CityData to integrate\nthe diverse urban data and CitySimu to simulate fine-grained urban dynamics.\nBased on CityData and CitySimu, we design 8 representative urban tasks in 2\ncategories of perception-understanding and decision-making as the CityBench.\nWith extensive results from 30 well-known LLMs and VLMs in 13 cities around the\nworld, we find that advanced LLMs and VLMs can achieve competitive performance\nin diverse urban tasks requiring commonsense and semantic understanding\nabilities, e.g., understanding the human dynamics and semantic inference of\nurban images. Meanwhile, they fail to solve the challenging urban tasks\nrequiring professional knowledge and high-level reasoning abilities, e.g.,\ngeospatial prediction and traffic control task. These observations provide\nvaluable perspectives for utilizing and developing LLMs in the future. Codes\nare openly accessible via https://github.com/tsinghua-fib-lab/CityBench.",
      "tldr_zh": "本文提出CityBench，一种基于交互模拟器的系统基准，用于评估大型语言模型(LLMs)和视觉语言模型(VLMs)在城市任务中的能力，以应对城市数据的多样性、场景复杂性和动态环境挑战。CityBench整合了CityData（多样城市数据）和CitySimu（模拟精细城市动态），并设计了8个代表性任务，分为感知-理解和决策两类。实验结果显示，先进模型在需要常识和语义理解的任务（如人类动态和城市图像分析）上表现出色，但 PROFESSIONAL知识和高水平推理的任务（如地理空间预测和交通控制）上表现不足。这些发现为未来LLMs的利用和开发提供了宝贵视角，并开源了相关代码。",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "26 pages, https://github.com/tsinghua-fib-lab/CityBench",
      "pdf_url": "http://arxiv.org/pdf/2406.13945v2",
      "published_date": "2024-06-20 02:25:07 UTC",
      "updated_date": "2024-12-23 14:10:09 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T23:05:43.646447"
    },
    {
      "arxiv_id": "2406.13941v2",
      "title": "UpDLRM: Accelerating Personalized Recommendation using Real-World PIM Architecture",
      "title_zh": "翻译失败",
      "authors": [
        "Sitian Chen",
        "Haobin Tan",
        "Amelie Chi Zhou",
        "Yusen Li",
        "Pavan Balaji"
      ],
      "abstract": "Deep Learning Recommendation Models (DLRMs) have gained popularity in\nrecommendation systems due to their effectiveness in handling large-scale\nrecommendation tasks. The embedding layers of DLRMs have become the performance\nbottleneck due to their intensive needs on memory capacity and memory\nbandwidth. In this paper, we propose UpDLRM, which utilizes real-world\nprocessingin-memory (PIM) hardware, UPMEM DPU, to boost the memory bandwidth\nand reduce recommendation latency. The parallel nature of the DPU memory can\nprovide high aggregated bandwidth for the large number of irregular memory\naccesses in embedding lookups, thus offering great potential to reduce the\ninference latency. To fully utilize the DPU memory bandwidth, we further\nstudied the embedding table partitioning problem to achieve good\nworkload-balance and efficient data caching. Evaluations using real-world\ndatasets show that, UpDLRM achieves much lower inference time for DLRM compared\nto both CPU-only and CPU-GPU hybrid counterparts.",
      "tldr_zh": "该论文针对深度学习推荐模型（DLRMs）的嵌入层性能瓶颈问题，提出UpDLRM框架，利用真实世界的处理内存储器（PIM）硬件——UPMEM DPU——来提升内存带宽并减少推荐延迟。UpDLRM通过DPU的并行特性优化不规则内存访问，并研究嵌入表分区策略以实现工作负载平衡和高效数据缓存。实验结果显示，使用真实数据集时，UpDLRM比仅CPU或CPU-GPU混合方案显著降低了DLRMs的推理时间。",
      "categories": [
        "cs.IR",
        "cs.AI"
      ],
      "primary_category": "cs.IR",
      "comment": "Accepted by DAC 2024",
      "pdf_url": "http://arxiv.org/pdf/2406.13941v2",
      "published_date": "2024-06-20 02:20:21 UTC",
      "updated_date": "2024-10-09 04:11:28 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T23:05:53.210891"
    },
    {
      "arxiv_id": "2406.13935v1",
      "title": "CONMOD: Controllable Neural Frame-based Modulation Effects",
      "title_zh": "翻译失败",
      "authors": [
        "Gyubin Lee",
        "Hounsu Kim",
        "Junwon Lee",
        "Juhan Nam"
      ],
      "abstract": "Deep learning models have seen widespread use in modelling LFO-driven audio\neffects, such as phaser and flanger. Although existing neural architectures\nexhibit high-quality emulation of individual effects, they do not possess the\ncapability to manipulate the output via control parameters. To address this\nissue, we introduce Controllable Neural Frame-based Modulation Effects\n(CONMOD), a single black-box model which emulates various LFO-driven effects in\na frame-wise manner, offering control over LFO frequency and feedback\nparameters. Additionally, the model is capable of learning the continuous\nembedding space of two distinct phaser effects, enabling us to steer between\neffects and achieve creative outputs. Our model outperforms previous work while\npossessing both controllability and universality, presenting opportunities to\nenhance creativity in modern LFO-driven audio effects.",
      "tldr_zh": "本文提出 CONMOD，一种可控的神经框架模型，用于模拟 LFO 驱动的音频效果，如 phaser 和 flanger，解决了现有模型无法通过控制参数操纵输出的问题。该模型采用逐帧处理方式，支持对 LFO 频率和反馈参数的实时调整，并学习了不同 phaser 效果的连续嵌入空间，实现效果间的平滑切换和创意输出。相比之前工作，CONMOD 在性能上表现出色，具有更高的通用性和可控性，为现代音频效果的创意应用提供了新机遇。",
      "categories": [
        "eess.AS",
        "cs.AI",
        "cs.SD"
      ],
      "primary_category": "eess.AS",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2406.13935v1",
      "published_date": "2024-06-20 02:02:54 UTC",
      "updated_date": "2024-06-20 02:02:54 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T23:06:05.666195"
    },
    {
      "arxiv_id": "2406.13934v1",
      "title": "Reasoning Like a Doctor: Improving Medical Dialogue Systems via Diagnostic Reasoning Process Alignment",
      "title_zh": "翻译失败",
      "authors": [
        "Kaishuai Xu",
        "Yi Cheng",
        "Wenjun Hou",
        "Qiaoyu Tan",
        "Wenjie Li"
      ],
      "abstract": "Medical dialogue systems have attracted significant attention for their\npotential to act as medical assistants. Enabling these medical systems to\nemulate clinicians' diagnostic reasoning process has been the long-standing\nresearch focus. Previous studies rudimentarily realized the simulation of\nclinicians' diagnostic process by fine-tuning language models on high-quality\ndialogue datasets. Nonetheless, they overly focus on the outcomes of the\nclinician's reasoning process while ignoring their internal thought processes\nand alignment with clinician preferences. Our work aims to build a medical\ndialogue system that aligns with clinicians' diagnostic reasoning processes. We\npropose a novel framework, Emulation, designed to generate an appropriate\nresponse that relies on abductive and deductive diagnostic reasoning analyses\nand aligns with clinician preferences through thought process modeling.\nExperimental results on two datasets confirm the efficacy of Emulation.\nCrucially, our framework furnishes clear explanations for the generated\nresponses, enhancing its transparency in medical consultations.",
      "tldr_zh": "本文旨在改进医疗对话系统，使其更好地模拟医生的诊断推理过程，解决现有方法忽略内部思考过程和偏好对齐的问题。提出Emulation框架，通过abductive和deductive推理分析以及thought process modeling，生成与医生偏好对齐的响应。实验结果在两个数据集上验证了框架的有效性，并提供清晰解释，提升了医疗咨询的透明度。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "Accepted by ACL 2024 Findings",
      "pdf_url": "http://arxiv.org/pdf/2406.13934v1",
      "published_date": "2024-06-20 02:02:53 UTC",
      "updated_date": "2024-06-20 02:02:53 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T23:06:26.623593"
    },
    {
      "arxiv_id": "2406.13929v1",
      "title": "Large Language Models are Skeptics: False Negative Problem of Input-conflicting Hallucination",
      "title_zh": "大语言模型是怀疑论者：输入冲突幻觉的假阴性问题",
      "authors": [
        "Jongyoon Song",
        "Sangwon Yu",
        "Sungroh Yoon"
      ],
      "abstract": "In this paper, we identify a new category of bias that induces\ninput-conflicting hallucinations, where large language models (LLMs) generate\nresponses inconsistent with the content of the input context. This issue we\nhave termed the false negative problem refers to the phenomenon where LLMs are\npredisposed to return negative judgments when assessing the correctness of a\nstatement given the context. In experiments involving pairs of statements that\ncontain the same information but have contradictory factual directions, we\nobserve that LLMs exhibit a bias toward false negatives. Specifically, the\nmodel presents greater overconfidence when responding with False. Furthermore,\nwe analyze the relationship between the false negative problem and context and\nquery rewriting and observe that both effectively tackle false negatives in\nLLMs.",
      "tldr_zh": "本研究识别了一种新的偏差，即输入冲突幻觉（input-conflicting hallucination），其中大型语言模型（LLMs）会生成与输入上下文不一致的响应，特别是表现出假阴性问题（false negative problem），即LLMs在评估语句正确性时倾向于过度自信地返回负面判断。实验通过测试包含相同信息但事实方向矛盾的语句对，发现LLMs更偏向于错误地判断为False。作者进一步分析了上下文和查询重写（context and query rewriting）作为有效方法，能够缓解这种假阴性问题，从而为改进LLMs的可靠性提供见解。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "12 pages, 9 figures",
      "pdf_url": "http://arxiv.org/pdf/2406.13929v1",
      "published_date": "2024-06-20 01:53:25 UTC",
      "updated_date": "2024-06-20 01:53:25 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T23:06:29.547724"
    },
    {
      "arxiv_id": "2406.13925v3",
      "title": "GenderAlign: An Alignment Dataset for Mitigating Gender Bias in Large Language Models",
      "title_zh": "GenderAlign: 用于缓解大语言模型中性别偏见的对齐数据集",
      "authors": [
        "Tao Zhang",
        "Ziqian Zeng",
        "Yuxiang Xiao",
        "Huiping Zhuang",
        "Cen Chen",
        "James Foulds",
        "Shimei Pan"
      ],
      "abstract": "Large Language Models (LLMs) are prone to generating content that exhibits\ngender biases, raising significant ethical concerns. Alignment, the process of\nfine-tuning LLMs to better align with desired behaviors, is recognized as an\neffective approach to mitigate gender biases. Although proprietary LLMs have\nmade significant strides in mitigating gender bias, their alignment datasets\nare not publicly available. The commonly used and publicly available alignment\ndataset, HH-RLHF, still exhibits gender bias to some extent. There is a lack of\npublicly available alignment datasets specifically designed to address gender\nbias. Hence, we developed a new dataset named GenderAlign, aiming at mitigating\na comprehensive set of gender biases in LLMs. This dataset comprises 8k\nsingle-turn dialogues, each paired with a \"chosen\" and a \"rejected\" response.\nCompared to the \"rejected\" responses, the \"chosen\" responses demonstrate lower\nlevels of gender bias and higher quality. Furthermore, we categorized the\ngender biases in the \"rejected\" responses of GenderAlign into 4 principal\ncategories. The experimental results show the effectiveness of GenderAlign in\nreducing gender bias in LLMs.",
      "tldr_zh": "这篇论文介绍了 GenderAlign，一个专门设计用于缓解 Large Language Models (LLMs) 中性别偏见的对齐数据集。GenderAlign 包含 8k 单轮对话，每对对话配有一个“chosen”响应（较低性别偏见和更高质量）和一个“rejected”响应（较高性别偏见），并将“rejected”响应的偏见分类为 4 个主要类别。与现有公开数据集如 HH-RLHF 相比，该数据集更针对性别偏见问题。实验结果证明，使用 GenderAlign 可以有效降低 LLMs 中的性别偏见。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2406.13925v3",
      "published_date": "2024-06-20 01:45:44 UTC",
      "updated_date": "2024-12-16 12:51:46 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T23:06:41.736384"
    },
    {
      "arxiv_id": "2406.13923v1",
      "title": "PIN: A Knowledge-Intensive Dataset for Paired and Interleaved Multimodal Documents",
      "title_zh": "翻译失败",
      "authors": [
        "Junjie Wang",
        "Yin Zhang",
        "Yatai Ji",
        "Yuxiang Zhang",
        "Chunyang Jiang",
        "Yubo Wang",
        "Kang Zhu",
        "Zekun Wang",
        "Tiezhen Wang",
        "Wenhao Huang",
        "Jie Fu",
        "Bei Chen",
        "Qunshu Lin",
        "Minghao Liu",
        "Ge Zhang",
        "Wenhu Chen"
      ],
      "abstract": "Recent advancements in Large Multimodal Models (LMMs) have leveraged\nextensive multimodal datasets to enhance capabilities in complex\nknowledge-driven tasks. However, persistent challenges in perceptual and\nreasoning errors limit their efficacy, particularly in interpreting intricate\nvisual data and deducing multimodal relationships. Addressing these issues, we\nintroduce a novel dataset format, PIN (Paired and INterleaved multimodal\ndocuments), designed to significantly improve both the depth and breadth of\nmultimodal training. The PIN format is built on three foundational principles:\nknowledge intensity, scalability, and support for diverse training modalities.\nThis innovative format combines markdown files and comprehensive images to\nenrich training data with a dense knowledge structure and versatile training\nstrategies. We present PIN-14M, an open-source dataset comprising 14 million\nsamples derived from a diverse range of Chinese and English sources, tailored\nto include complex web and scientific content. This dataset is constructed\nmeticulously to ensure data quality and ethical integrity, aiming to facilitate\nadvanced training strategies and improve model robustness against common\nmultimodal training pitfalls. Our initial results, forming the basis of this\ntechnical report, suggest significant potential for the PIN format in refining\nLMM performance, with plans for future expansions and detailed evaluations of\nits impact on model capabilities.",
      "tldr_zh": "该论文介绍了 PIN（Paired and INterleaved multimodal documents）数据集格式，旨在解决 Large Multimodal Models (LMMs) 在处理复杂知识任务时存在的感知和推理错误问题。PIN 格式基于 knowledge intensity、可扩展性（scalability）和支持 diverse training modalities 的原则，结合 markdown 文件和全面图像，提供密集知识结构和多样化训练策略。研究者发布了 PIN-14M 数据集，包含 14 百万样本，源自中文和英文的复杂网页及科学内容，并确保数据质量和伦理完整性。初步结果显示，该格式能显著提升 LMM 性能，并计划未来扩展和详细评估其对模型能力的影响。",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.CV",
        "cs.MM"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2406.13923v1",
      "published_date": "2024-06-20 01:43:08 UTC",
      "updated_date": "2024-06-20 01:43:08 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T23:06:53.548924"
    },
    {
      "arxiv_id": "2407.02511v2",
      "title": "LLM-A*: Large Language Model Enhanced Incremental Heuristic Search on Path Planning",
      "title_zh": "翻译失败",
      "authors": [
        "Silin Meng",
        "Yiwei Wang",
        "Cheng-Fu Yang",
        "Nanyun Peng",
        "Kai-Wei Chang"
      ],
      "abstract": "Path planning is a fundamental scientific problem in robotics and autonomous\nnavigation, requiring the derivation of efficient routes from starting to\ndestination points while avoiding obstacles. Traditional algorithms like A* and\nits variants are capable of ensuring path validity but suffer from significant\ncomputational and memory inefficiencies as the state space grows. Conversely,\nlarge language models (LLMs) excel in broader environmental analysis through\ncontextual understanding, providing global insights into environments. However,\nthey fall short in detailed spatial and temporal reasoning, often leading to\ninvalid or inefficient routes. In this work, we propose LLM-A*, an new LLM\nbased route planning method that synergistically combines the precise\npathfinding capabilities of A* with the global reasoning capability of LLMs.\nThis hybrid approach aims to enhance pathfinding efficiency in terms of time\nand space complexity while maintaining the integrity of path validity,\nespecially in large-scale scenarios. By integrating the strengths of both\nmethodologies, LLM-A* addresses the computational and memory limitations of\nconventional algorithms without compromising on the validity required for\neffective pathfinding.",
      "tldr_zh": "本研究针对路径规划问题，指出传统算法如 A* 虽能确保路径有效性，但面临计算和内存效率低下的问题，而 Large Language Models (LLMs) 虽擅长全局环境分析，却在详细的空间和时间推理上不足。论文提出 LLM-A*，一种创新方法，将 LLMs 的全局推理能力与 A* 的精确路径搜索相结合，形成混合框架。LLM-A* 显著提升了路径规划的时空复杂度效率，同时保持路径的有效性和完整性，尤其在大型场景中表现突出。通过这种协同整合，该方法解决了传统算法的局限性，为机器人和自主导航领域提供了更高效的解决方案。",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.RO",
      "comment": "Findings of the Association for Computational Linguistics: EMNLP 2024",
      "pdf_url": "http://arxiv.org/pdf/2407.02511v2",
      "published_date": "2024-06-20 01:24:30 UTC",
      "updated_date": "2025-04-09 17:34:52 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T23:07:06.137440"
    },
    {
      "arxiv_id": "2406.13919v4",
      "title": "SPL: A Socratic Playground for Learning Powered by Large Language Model",
      "title_zh": "翻译失败",
      "authors": [
        "Liang Zhang",
        "Jionghao Lin",
        "Ziyi Kuang",
        "Sheng Xu",
        "Xiangen Hu"
      ],
      "abstract": "Dialogue-based Intelligent Tutoring Systems (ITSs) have significantly\nadvanced adaptive and personalized learning by automating sophisticated human\ntutoring strategies within interactive dialogues. However, replicating the\nnuanced patterns of expert human communication remains a challenge in Natural\nLanguage Processing (NLP). Recent advancements in NLP, particularly Large\nLanguage Models (LLMs) such as OpenAI's GPT-4, offer promising solutions by\nproviding human-like and context-aware responses based on extensive pre-trained\nknowledge. Motivated by the effectiveness of LLMs in various educational tasks\n(e.g., content creation and summarization, problem-solving, and automated\nfeedback provision), our study introduces the Socratic Playground for Learning\n(SPL), a dialogue-based ITS powered by the GPT-4 model, which employs the\nSocratic teaching method to foster critical thinking among learners. Through\nextensive prompt engineering, SPL can generate specific learning scenarios and\nfacilitates efficient multi-turn tutoring dialogues. The SPL system aims to\nenhance personalized and adaptive learning experiences tailored to individual\nneeds, specifically focusing on improving critical thinking skills. Our pilot\nexperimental results from essay writing tasks demonstrate SPL has the potential\nto improve tutoring interactions and further enhance dialogue-based ITS\nfunctionalities. Our study, exemplified by SPL, demonstrates how LLMs enhance\ndialogue-based ITSs and expand the accessibility and efficacy of educational\ntechnologies.",
      "tldr_zh": "本研究引入了Socratic Playground for Learning (SPL)，一个基于Large Language Models (LLMs)如GPT-4的对话式Intelligent Tutoring Systems (ITSs)，旨在通过苏格拉底教学法提升学习者的批判性思维。SPL利用提示工程生成特定学习场景并支持高效的多轮辅导对话，实现个性化和适应性学习体验。实验结果显示，在作文写作任务的试点测试中，SPL显著改善了辅导互动，并增强了对话式ITS的功能，最终扩展了教育技术的可访问性和效能。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2406.13919v4",
      "published_date": "2024-06-20 01:18:52 UTC",
      "updated_date": "2024-09-25 01:48:32 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T23:07:19.902797"
    },
    {
      "arxiv_id": "2406.13903v1",
      "title": "Generative AI for Enhancing Active Learning in Education: A Comparative Study of GPT-3.5 and GPT-4 in Crafting Customized Test Questions",
      "title_zh": "翻译失败",
      "authors": [
        "Hamdireza Rouzegar",
        "Masoud Makrehchi"
      ],
      "abstract": "This study investigates how LLMs, specifically GPT-3.5 and GPT-4, can develop\ntailored questions for Grade 9 math, aligning with active learning principles.\nBy utilizing an iterative method, these models adjust questions based on\ndifficulty and content, responding to feedback from a simulated 'student'\nmodel. A novel aspect of the research involved using GPT-4 as a 'teacher' to\ncreate complex questions, with GPT-3.5 as the 'student' responding to these\nchallenges. This setup mirrors active learning, promoting deeper engagement.\nThe findings demonstrate GPT-4's superior ability to generate precise,\nchallenging questions and notable improvements in GPT-3.5's ability to handle\nmore complex problems after receiving instruction from GPT-4. These results\nunderscore the potential of LLMs to mimic and enhance active learning\nscenarios, offering a promising path for AI in customized education. This\nresearch contributes to understanding how AI can support personalized learning\nexperiences, highlighting the need for further exploration in various\neducational contexts",
      "tldr_zh": "本研究比较了 GPT-3.5 和 GPT-4 在为 9 年级数学创建定制测试问题的能力，旨在通过主动学习原则增强教育效果。\n采用迭代方法，GPT-4 作为“教师”生成复杂问题，GPT-3.5 作为“学生”响应反馈，从而模拟主动学习场景并促进更深入参与。\n结果显示，GPT-4 在产生精确且具有挑战性的问题方面优于 GPT-3.5，后者经 GPT-4 指导后显著提升了处理复杂问题的能力。\n这项研究突显了 LLMs 在支持个性化学习中的潜力，并建议在更多教育环境中进行进一步探索。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG",
        "68T50",
        "I.2.7"
      ],
      "primary_category": "cs.CL",
      "comment": "Publisher: Canadian Artificial Intelligence Association. URL:\n  https://caiac.pubpub.org/pub/kmn55wd2#nssvokovikx",
      "pdf_url": "http://arxiv.org/pdf/2406.13903v1",
      "published_date": "2024-06-20 00:25:43 UTC",
      "updated_date": "2024-06-20 00:25:43 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T23:07:34.783941"
    },
    {
      "arxiv_id": "2406.15508v1",
      "title": "What Teaches Robots to Walk, Teaches Them to Trade too -- Regime Adaptive Execution using Informed Data and LLMs",
      "title_zh": "翻译失败",
      "authors": [
        "Raeid Saqur"
      ],
      "abstract": "Machine learning techniques applied to the problem of financial market\nforecasting struggle with dynamic regime switching, or underlying correlation\nand covariance shifts in true (hidden) market variables. Drawing inspiration\nfrom the success of reinforcement learning in robotics, particularly in agile\nlocomotion adaptation of quadruped robots to unseen terrains, we introduce an\ninnovative approach that leverages world knowledge of pretrained LLMs (aka.\n'privileged information' in robotics) and dynamically adapts them using\nintrinsic, natural market rewards using LLM alignment technique we dub as\n\"Reinforcement Learning from Market Feedback\" (**RLMF**). Strong empirical\nresults demonstrate the efficacy of our method in adapting to regime shifts in\nfinancial markets, a challenge that has long plagued predictive models in this\ndomain. The proposed algorithmic framework outperforms best-performing SOTA LLM\nmodels on the existing (FLARE) benchmark stock-movement (SM) tasks by more than\n15\\% improved accuracy. On the recently proposed NIFTY SM task, our adaptive\npolicy outperforms the SOTA best performing trillion parameter models like\nGPT-4. The paper details the dual-phase, teacher-student architecture and\nimplementation of our model, the empirical results obtained, and an analysis of\nthe role of language embeddings in terms of Information Gain.",
      "tldr_zh": "这篇论文受机器人强化学习启发，提出了一种新方法来处理金融市场预测中的动态制度切换问题，通过利用预训练LLM（作为特权信息）和“Reinforcement Learning from Market Feedback (RLMF)”技术动态适应市场。方法采用双阶段教师-学生架构，结合内在市场奖励来优化LLM。实验结果显示，该框架在FLARE基准的股票运动任务上比SOTA模型准确率提升超过15%，并在NIFTY任务上超越了GPT-4等大型模型，同时分析了语言嵌入在信息增益中的作用。",
      "categories": [
        "q-fin.CP",
        "cs.AI",
        "cs.LG",
        "cs.RO",
        "I.2.0; I.2.6; I.2.7; I.2.9"
      ],
      "primary_category": "q-fin.CP",
      "comment": "arXiv admin note: substantial text overlap with arXiv:2405.09747",
      "pdf_url": "http://arxiv.org/pdf/2406.15508v1",
      "published_date": "2024-06-20 00:17:28 UTC",
      "updated_date": "2024-06-20 00:17:28 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T23:07:43.950874"
    }
  ],
  "raw_papers_fetched": true,
  "papers_count": 163,
  "processed_papers_count": 163,
  "failed_papers_count": 0,
  "summary_generated": true,
  "daily_data_saved": true,
  "last_update": "2025-05-18T23:08:05.512720"
}
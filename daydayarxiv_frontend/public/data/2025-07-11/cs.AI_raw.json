[
  {
    "arxiv_id": "2507.09080v2",
    "title": "BioAnalyst: A Foundation Model for Biodiversity",
    "authors": [
      "Athanasios Trantas",
      "Martino Mensio",
      "Stylianos Stasinos",
      "Sebastian Gribincea",
      "Taimur Khan",
      "Damian Podareanu",
      "Aliene van der Veen"
    ],
    "abstract": "Multimodal Foundation Models (FMs) offer a path to learn general-purpose representations from heterogeneous ecological data, easily transferable to downstream tasks. However, practical biodiversity modelling remains fragmented; separate pipelines and models are built for each dataset and objective, which limits reuse across regions and taxa. In response, we present BioAnalyst, to our knowledge the first multimodal Foundation Model tailored to biodiversity analysis and conservation planning in Europe at $0.25^{\\circ}$ spatial resolution targeting regional to national-scale applications. BioAnalyst employs a transformer-based architecture, pre-trained on extensive multimodal datasets that align species occurrence records with remote sensing indicators, climate and environmental variables. Post pre-training, the model is adapted via lightweight roll-out fine-tuning to a range of downstream tasks, including joint species distribution modelling, biodiversity dynamics and population trend forecasting. The model is evaluated on two representative downstream use cases: (i) joint species distribution modelling and with 500 vascular plant species (ii) monthly climate linear probing with temperature and precipitation data. Our findings show that BioAnalyst can provide a strong baseline both for biotic and abiotic tasks, acting as a macroecological simulator with a yearly forecasting horizon and monthly resolution, offering the first application of this type of modelling in the biodiversity domain. We have open-sourced the model weights, training and fine-tuning pipelines to advance AI-driven ecological research.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2507.09080v2",
    "published_date": "2025-07-11 23:56:08 UTC",
    "updated_date": "2025-12-04 18:58:55 UTC"
  },
  {
    "arxiv_id": "2507.09076v2",
    "title": "Dynamic Parameter Memory: Temporary LoRA-Enhanced LLM for Long-Sequence Emotion Recognition in Conversation",
    "authors": [
      "Jialong Mai",
      "Xiaofen Xing",
      "Yawei Li",
      "Weidong Chen",
      "Zhipeng Li",
      "Jingyuan Xing",
      "Xiangmin Xu"
    ],
    "abstract": "Recent research has focused on applying speech large language model (SLLM) to improve speech emotion recognition (SER). However, the inherently high frame rate in speech modality severely limits the signal processing and understanding capabilities of SLLM. For example, a SLLM with a 4K context window can only process 80 seconds of audio at 50Hz feature sampling rate before reaching its capacity limit. Input token compression methods used in SLLM overlook the continuity and inertia of emotions across multiple conversation turns. This paper proposes a Dynamic Parameter Memory (DPM) mechanism with contextual semantics and sentence-level emotion encoding, enabling processing of unlimited-length audio with limited context windows in SLLM. Specifically, DPM progressively encodes sentence-level information and emotions into a temporary LoRA module during inference to effectively \"memorize\" the contextual information. We trained an emotion SLLM as a backbone and incorporated our DPM into inference for emotion recognition in conversation (ERC). Experimental results on the IEMOCAP dataset show that DPM significantly improves the emotion recognition capabilities of SLLM when processing long audio sequences, achieving state-of-the-art performance.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "submitted to ICLR 2026",
    "pdf_url": "https://arxiv.org/pdf/2507.09076v2",
    "published_date": "2025-07-11 23:36:59 UTC",
    "updated_date": "2025-09-24 07:56:12 UTC"
  },
  {
    "arxiv_id": "2507.09068v2",
    "title": "Infinite Video Understanding",
    "authors": [
      "Dell Zhang",
      "Xiangyu Chen",
      "Jixiang Luo",
      "Mengxi Jia",
      "Changzhi Sun",
      "Ruilong Ren",
      "Jingren Liu",
      "Hao Sun",
      "Xuelong Li"
    ],
    "abstract": "The rapid advancements in Large Language Models (LLMs) and their multimodal extensions (MLLMs) have ushered in remarkable progress in video understanding. However, a fundamental challenge persists: effectively processing and comprehending video content that extends beyond minutes or hours. While recent efforts like Video-XL-2 have demonstrated novel architectural solutions for extreme efficiency, and advancements in positional encoding such as HoPE and VideoRoPE++ aim to improve spatio-temporal understanding over extensive contexts, current state-of-the-art models still encounter significant computational and memory constraints when faced with the sheer volume of visual tokens from lengthy sequences. Furthermore, maintaining temporal coherence, tracking complex events, and preserving fine-grained details over extended periods remain formidable hurdles, despite progress in agentic reasoning systems like Deep Video Discovery. This position paper posits that a logical, albeit ambitious, next frontier for multimedia research is Infinite Video Understanding -- the capability for models to continuously process, understand, and reason about video data of arbitrary, potentially never-ending duration. We argue that framing Infinite Video Understanding as a blue-sky research objective provides a vital north star for the multimedia, and the wider AI, research communities, driving innovation in areas such as streaming architectures, persistent memory mechanisms, hierarchical and adaptive representations, event-centric reasoning, and novel evaluation paradigms. Drawing inspiration from recent work on long/ultra-long video understanding and several closely related fields, we outline the core challenges and key research directions towards achieving this transformative capability.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.IR",
      "cs.LG",
      "cs.MM"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2507.09068v2",
    "published_date": "2025-07-11 23:07:04 UTC",
    "updated_date": "2025-07-23 13:06:44 UTC"
  },
  {
    "arxiv_id": "2507.09063v1",
    "title": "SetupBench: Assessing Software Engineering Agents' Ability to Bootstrap Development Environments",
    "authors": [
      "Avi Arora",
      "Jinu Jang",
      "Roshanak Zilouchian Moghaddam"
    ],
    "abstract": "Modern Large Language Model (LLM) agents promise end to end assistance with real-world software tasks, yet existing benchmarks evaluate LLM agents almost exclusively in pre-baked environments where every dependency is pre-installed. To fill this gap, we introduce SetupBench, a 93 instance benchmark that isolates the environment-bootstrap skill: starting from a bare Linux sandbox, an agent must install packages, resolve dependency conflicts, initialize databases, and configure background services. Our tasks span seven language ecosystems, five database engines, and multi-service orchestration scenarios, each accompanies by a natural language problem statement and a deterministic success command. Through evaluation of OpenHands, a state-of-the-art coding agent, we find low success rates across task categories, with particular challenges in repository setup (38.9-57.4%) and local database configuration (20.0-53.3%). Our analysis reveals systematic failure modes including incomplete development tooling installation, hallucinated task constraints, and non-persistent environment modifications that break agent-human collaboration workflows. We identify substantial inefficiencies in agent exploration strategies, with 38-89% of actions being unnecessary compared to optimal human behavior. These findings highlight gaps in current agents' practical environment-bootstrap capabilities. By targeting this critical yet under-evaluated capability, SetupBench provides a rigorous yard-stick for the next generation of software developer agents aiming to solve end to end real-wold tasks.",
    "categories": [
      "cs.SE",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.SE",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2507.09063v1",
    "published_date": "2025-07-11 22:45:07 UTC",
    "updated_date": "2025-07-11 22:45:07 UTC"
  },
  {
    "arxiv_id": "2507.09037v1",
    "title": "ALIGN: Prompt-based Attribute Alignment for Reliable, Responsible, and Personalized LLM-based Decision-Making",
    "authors": [
      "Bharadwaj Ravichandran",
      "David Joy",
      "Paul Elliott",
      "Brian Hu",
      "Jadie Adams",
      "Christopher Funk",
      "Emily Veenhuis",
      "Anthony Hoogs",
      "Arslan Basharat"
    ],
    "abstract": "Large language models (LLMs) are increasingly being used as decision aids. However, users have diverse values and preferences that can affect their decision-making, which requires novel methods for LLM alignment and personalization. Existing LLM comparison tools largely focus on benchmarking tasks, such as knowledge-based question answering. In contrast, our proposed ALIGN system focuses on dynamic personalization of LLM-based decision-makers through prompt-based alignment to a set of fine-grained attributes. Key features of our system include robust configuration management, structured output generation with reasoning, and several algorithm implementations with swappable LLM backbones, enabling different types of analyses. Our user interface enables a qualitative, side-by-side comparison of LLMs and their alignment to various attributes, with a modular backend for easy algorithm integration. Additionally, we perform a quantitative analysis comparing alignment approaches in two different domains: demographic alignment for public opinion surveys and value alignment for medical triage decision-making. The entire ALIGN framework is open source and will enable new research on reliable, responsible, and personalized LLM-based decision-makers.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "10 pages total (including appendix), ICML 2025 Workshop on Reliable and Responsible Foundation Models",
    "pdf_url": "https://arxiv.org/pdf/2507.09037v1",
    "published_date": "2025-07-11 21:33:38 UTC",
    "updated_date": "2025-07-11 21:33:38 UTC"
  },
  {
    "arxiv_id": "2507.09036v1",
    "title": "BrainLesion Suite: A Flexible and User-Friendly Framework for Modular Brain Lesion Image Analysis",
    "authors": [
      "Florian Kofler",
      "Marcel Rosier",
      "Mehdi Astaraki",
      "Hendrik Möller",
      "Ilhem Isra Mekki",
      "Josef A. Buchner",
      "Anton Schmick",
      "Arianna Pfiffer",
      "Eva Oswald",
      "Lucas Zimmer",
      "Ezequiel de la Rosa",
      "Sarthak Pati",
      "Julian Canisius",
      "Arianna Piffer",
      "Ujjwal Baid",
      "Mahyar Valizadeh",
      "Akis Linardos",
      "Jan C. Peeken",
      "Surprosanna Shit",
      "Felix Steinbauer",
      "Daniel Rueckert",
      "Rolf Heckemann",
      "Spyridon Bakas",
      "Jan Kirschke",
      "Constantin von See",
      "Ivan Ezhov",
      "Marie Piraud",
      "Benedikt Wiestler",
      "Bjoern Menze"
    ],
    "abstract": "BrainLesion Suite is a versatile toolkit for building modular brain lesion image analysis pipelines in Python. Following Pythonic principles, BrainLesion Suite is designed to provide a 'brainless' development experience, minimizing cognitive effort and streamlining the creation of complex workflows for clinical and scientific practice. At its core is an adaptable preprocessing module that performs co-registration, atlas registration, and optional skull-stripping and defacing on arbitrary multi-modal input images. BrainLesion Suite leverages algorithms from the BraTS challenge to synthesize missing modalities, inpaint lesions, and generate pathology-specific tumor segmentations. BrainLesion Suite also enables quantifying segmentation model performance, with tools such as panoptica to compute lesion-wise metrics. Although BrainLesion Suite was originally developed for image analysis pipelines of brain lesions such as glioma, metastasis, and multiple sclerosis, it can be adapted for other biomedical image analysis applications. The individual BrainLesion Suite packages and tutorials are accessible on GitHub.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG",
      "cs.SE"
    ],
    "primary_category": "cs.CV",
    "comment": "16p, 3f",
    "pdf_url": "https://arxiv.org/pdf/2507.09036v1",
    "published_date": "2025-07-11 21:32:45 UTC",
    "updated_date": "2025-07-11 21:32:45 UTC"
  },
  {
    "arxiv_id": "2507.09029v4",
    "title": "Model Parallelism With Subnetwork Data Parallelism",
    "authors": [
      "Vaibhav Singh",
      "Zafir Khalid",
      "Edouard Oyallon",
      "Eugene Belilovsky"
    ],
    "abstract": "Pre-training large neural networks at scale imposes heavy memory demands on accelerators and often requires costly communication. We introduce Subnetwork Data Parallelism (SDP), a distributed training framework that partitions a model into structured subnetworks trained across workers without exchanging activations. We study two complementary masking regimes: backward masking, which applies sparsity only in the backward step to retain unbiased gradients, and forward masking, which also removes parameters in the forward pass to deliver stronger efficiency gains while providing additional regularization. We further explore two subnetwork construction strategies: neuron level and block level, applied across both CNNs and transformers. In experiments spanning CNNs and transformers on CIFAR and ImageNet, as well as LLM pre-training on FineWeb, SDP reduces per-device memory usage by 30%-75% while maintaining or improving performance. Notably, in FLOP-matched settings, forward masking can sometimes achieve better performance.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "10 pages, 2 figure",
    "pdf_url": "https://arxiv.org/pdf/2507.09029v4",
    "published_date": "2025-07-11 21:25:11 UTC",
    "updated_date": "2025-10-03 01:18:28 UTC"
  },
  {
    "arxiv_id": "2507.09028v2",
    "title": "From Classical Machine Learning to Emerging Foundation Models: Review on Multimodal Data Integration for Cancer Research",
    "authors": [
      "Amgad Muneer",
      "Muhammad Waqas",
      "Maliazurina B Saad",
      "Eman Showkatian",
      "Rukhmini Bandyopadhyay",
      "Hui Xu",
      "Wentao Li",
      "Joe Y Chang",
      "Zhongxing Liao",
      "Cara Haymaker",
      "Luisa Solis Soto",
      "Carol C Wu",
      "Natalie I Vokes",
      "Xiuning Le",
      "Lauren A Byers",
      "Don L Gibbons",
      "John V Heymach",
      "Jianjun Zhang",
      "Jia Wu"
    ],
    "abstract": "Cancer research is increasingly driven by the integration of diverse data modalities, spanning from genomics and proteomics to imaging and clinical factors. However, extracting actionable insights from these vast and heterogeneous datasets remains a key challenge. The rise of foundation models (FMs) -- large deep-learning models pretrained on extensive amounts of data serving as a backbone for a wide range of downstream tasks -- offers new avenues for discovering biomarkers, improving diagnosis, and personalizing treatment. This paper presents a comprehensive review of widely adopted integration strategies of multimodal data to assist advance the computational approaches for data-driven discoveries in oncology. We examine emerging trends in machine learning (ML) and deep learning (DL), including methodological frameworks, validation protocols, and open-source resources targeting cancer subtype classification, biomarker discovery, treatment guidance, and outcome prediction. This study also comprehensively covers the shift from traditional ML to FMs for multimodal integration. We present a holistic view of recent FMs advancements and challenges faced during the integration of multi-omics with advanced imaging data. We identify the state-of-the-art FMs, publicly available multi-modal repositories, and advanced tools and methods for data integration. We argue that current state-of-the-art integrative methods provide the essential groundwork for developing the next generation of large-scale, pre-trained models poised to further revolutionize oncology. To the best of our knowledge, this is the first review to systematically map the transition from conventional ML to advanced FM for multimodal data integration in oncology, while also framing these developments as foundational for the forthcoming era of large-scale AI models in cancer research.",
    "categories": [
      "q-bio.QM",
      "cs.AI"
    ],
    "primary_category": "q-bio.QM",
    "comment": "10 figures, 5 tables",
    "pdf_url": "https://arxiv.org/pdf/2507.09028v2",
    "published_date": "2025-07-11 21:23:21 UTC",
    "updated_date": "2025-12-18 21:46:44 UTC"
  },
  {
    "arxiv_id": "2507.09023v1",
    "title": "Accelerating Drug Discovery Through Agentic AI: A Multi-Agent Approach to Laboratory Automation in the DMTA Cycle",
    "authors": [
      "Yao Fehlis",
      "Charles Crain",
      "Aidan Jensen",
      "Michael Watson",
      "James Juhasz",
      "Paul Mandel",
      "Betty Liu",
      "Shawn Mahon",
      "Daren Wilson",
      "Nick Lynch-Jonely",
      "Ben Leedom",
      "David Fuller"
    ],
    "abstract": "The pharmaceutical industry faces unprecedented challenges in drug discovery, with traditional approaches struggling to meet modern therapeutic development demands. This paper introduces a novel AI framework, Tippy, that transforms laboratory automation through specialized AI agents operating within the Design-Make-Test-Analyze (DMTA) cycle. Our multi-agent system employs five specialized agents - Supervisor, Molecule, Lab, Analysis, and Report, with Safety Guardrail oversight - each designed to excel in specific phases of the drug discovery pipeline. Tippy represents the first production-ready implementation of specialized AI agents for automating the DMTA cycle, providing a concrete example of how AI can transform laboratory workflows. By leveraging autonomous AI agents that reason, plan, and collaborate, we demonstrate how Tippy accelerates DMTA cycles while maintaining scientific rigor essential for pharmaceutical research. The system shows significant improvements in workflow efficiency, decision-making speed, and cross-disciplinary coordination, offering a new paradigm for AI-assisted drug discovery.",
    "categories": [
      "cs.SE",
      "cs.AI",
      "cs.MA"
    ],
    "primary_category": "cs.SE",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2507.09023v1",
    "published_date": "2025-07-11 21:13:13 UTC",
    "updated_date": "2025-07-11 21:13:13 UTC"
  },
  {
    "arxiv_id": "2507.09019v1",
    "title": "On Evaluating Performance of LLM Inference Serving Systems",
    "authors": [
      "Amey Agrawal",
      "Nitin Kedia",
      "Anmol Agarwal",
      "Jayashree Mohan",
      "Nipun Kwatra",
      "Souvik Kundu",
      "Ramachandran Ramjee",
      "Alexey Tumanov"
    ],
    "abstract": "The rapid evolution of Large Language Model (LLM) inference systems has yielded significant efficiency improvements. However, our systematic analysis reveals that current evaluation methodologies frequently exhibit fundamental flaws, often manifesting as common evaluation anti-patterns that obscure true performance characteristics and impede scientific progress. Through a comprehensive examination of recent systems, we identify recurring anti-patterns across three key dimensions: Baseline Fairness, Evaluation Setup, and Metric Design. These anti-patterns are uniquely problematic for LLM inference due to its dual-phase nature combining distinct prefill and decode operations, its handling of highly heterogeneous workloads, and its strict temporal requirements for interactive use. We demonstrate how common anti-patterns -- such as inadequate baseline comparisons that conflate engineering effort with algorithmic novelty, workload selections that fail to represent production scenarios, and metric normalizations that hide substantial performance variability like generation stalls-lead to misleading conclusions. To address these challenges, we provide a comprehensive checklist derived from our analysis, establishing a framework for recognizing and avoiding these anti-patterns in favor of robust LLM inference evaluation. To demonstrate the practical application of our framework, we present a case study analyzing speculative decoding, a technique whose bursty, non-uniform token generation is easily misinterpreted when evaluated using approaches characteristic of these anti-patterns. Our work establishes a rigorous foundation for evaluation methodology, enabling meaningful comparisons, ensuring reproducible results, and ultimately accelerating genuine progress in LLM inference systems by moving beyond common anti-patterns to align evaluation with real-world requirements.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.DC"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2507.09019v1",
    "published_date": "2025-07-11 20:58:21 UTC",
    "updated_date": "2025-07-11 20:58:21 UTC"
  },
  {
    "arxiv_id": "2507.13372v1",
    "title": "Enhancing Breast Cancer Detection with Vision Transformers and Graph Neural Networks",
    "authors": [
      "Yeming Cai",
      "Zhenglin Li",
      "Yang Wang"
    ],
    "abstract": "Breast cancer is a leading cause of death among women globally, and early detection is critical for improving survival rates. This paper introduces an innovative framework that integrates Vision Transformers (ViT) and Graph Neural Networks (GNN) to enhance breast cancer detection using the CBIS-DDSM dataset. Our framework leverages ViT's ability to capture global image features and GNN's strength in modeling structural relationships, achieving an accuracy of 84.2%, outperforming traditional methods. Additionally, interpretable attention heatmaps provide insights into the model's decision-making process, aiding radiologists in clinical settings.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2507.13372v1",
    "published_date": "2025-07-11 20:32:48 UTC",
    "updated_date": "2025-07-11 20:32:48 UTC"
  },
  {
    "arxiv_id": "2507.13371v1",
    "title": "Transformer-Based Framework for Motion Capture Denoising and Anomaly Detection in Medical Rehabilitation",
    "authors": [
      "Yeming Cai",
      "Yang Wang",
      "Zhenglin Li"
    ],
    "abstract": "This paper proposes an end-to-end deep learning framework integrating optical motion capture with a Transformer-based model to enhance medical rehabilitation. It tackles data noise and missing data caused by occlusion and environmental factors, while detecting abnormal movements in real time to ensure patient safety. Utilizing temporal sequence modeling, our framework denoises and completes motion capture data, improving robustness. Evaluations on stroke and orthopedic rehabilitation datasets show superior performance in data reconstruction and anomaly detection, providing a scalable, cost-effective solution for remote rehabilitation with reduced on-site supervision.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2507.13371v1",
    "published_date": "2025-07-11 20:28:55 UTC",
    "updated_date": "2025-07-11 20:28:55 UTC"
  },
  {
    "arxiv_id": "2507.09010v1",
    "title": "Hybrid Systolic Array Accelerator with Optimized Dataflow for Edge Large Language Model Inference",
    "authors": [
      "Chun-Ting Chen",
      "HanGyeol Mun",
      "Jian Meng",
      "Mohamed S. Abdelfattah",
      "Jae-sun Seo"
    ],
    "abstract": "Edge inference for large language models (LLM) offers secure, low-latency, and cost-effective inference solutions. We emphasize that an edge accelerator should achieve high area efficiency and minimize external memory access (EMA) during the memory-bound decode stage, while maintaining high energy efficiency during the compute intensive prefill stage. This paper proposes an edge LLM inference accelerator featuring a hybrid systolic array (HSA) architecture that optimizes inference efficiency in both stages. To further reduce EMA, we adopt MXINT4 weight quantization and propose an optimized dataflow tailored for HSA, ensuring negligible dequantization overhead and achieving 100% hardware utilization with minimal accuracy loss under edge DRAM bandwidth constraints. For non-linear operations, we incorporate optimized root mean square normalization (RMSNorm) and rotary position embedding (RoPE) units, reducing their latency, area, and memory access overhead while enabling end-to-end inference on our accelerator. Our solution achieves 247/117 (token/s/mm2) while running a 1.3B LLM on long-input/long-output scenarios, providing >2.45x/13.5x improvement over existing approaches, while maintaining superior energy efficiency in token generation.",
    "categories": [
      "cs.AR",
      "cs.AI"
    ],
    "primary_category": "cs.AR",
    "comment": "Accepted as a conference paper at the 2025 IEEE/ACM International Symposium on Low Power Electronics and Design (ISLPED)",
    "pdf_url": "https://arxiv.org/pdf/2507.09010v1",
    "published_date": "2025-07-11 20:27:30 UTC",
    "updated_date": "2025-07-11 20:27:30 UTC"
  },
  {
    "arxiv_id": "2507.09009v1",
    "title": "Multimodal Cardiovascular Risk Profiling Using Self-Supervised Learning of Polysomnography",
    "authors": [
      "Zhengxiao He",
      "Huayu Li",
      "Geng Yuan",
      "William D. S. Killgore",
      "Stuart F. Quan",
      "Chen X. Chen",
      "Ao Li"
    ],
    "abstract": "Methods: We developed a self-supervised deep learning model that extracts meaningful patterns from multi-modal signals (Electroencephalography (EEG), Electrocardiography (ECG), and respiratory signals). The model was trained on data from 4,398 participants. Projection scores were derived by contrasting embeddings from individuals with and without CVD outcomes. External validation was conducted in an independent cohort with 1,093 participants. The source code is available on https://github.com/miraclehetech/sleep-ssl. Results: The projection scores revealed distinct and clinically meaningful patterns across modalities. ECG-derived features were predictive of both prevalent and incident cardiac conditions, particularly CVD mortality. EEG-derived features were predictive of incident hypertension and CVD mortality. Respiratory signals added complementary predictive value. Combining these projection scores with the Framingham Risk Score consistently improved predictive performance, achieving area under the curve values ranging from 0.607 to 0.965 across different outcomes. Findings were robustly replicated and validated in the external testing cohort. Conclusion: Our findings demonstrate that the proposed framework can generate individualized CVD risk scores directly from PSG data. The resulting projection scores have the potential to be integrated into clinical practice, enhancing risk assessment and supporting personalized care.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2507.09009v1",
    "published_date": "2025-07-11 20:24:10 UTC",
    "updated_date": "2025-07-11 20:24:10 UTC"
  },
  {
    "arxiv_id": "2507.10593v1",
    "title": "ToolRegistry: A Protocol-Agnostic Tool Management Library for Function-Calling LLMs",
    "authors": [
      "Peng Ding"
    ],
    "abstract": "Large Language Model (LLM) applications are increasingly relying on external tools to extend their capabilities beyond text generation. However, current tool integration approaches suffer from fragmentation, protocol limitations, and implementation complexity, leading to substantial development overhead. This paper presents Toolregistry, a protocol-agnostic tool management library that simplifies tool registration, representation, execution, and lifecycle management via a unified interface. Our evaluation demonstrates that \\toolregistry achieves 60-80% reduction in tool integration code, up to 3.1x performance improvements through concurrent execution, and 100% compatibility with OpenAI function calling standards. Real-world case studies show significant improvements in development efficiency and code maintainability across diverse integration scenarios. \\toolregistry is open-source and available at https://github.com/Oaklight/ToolRegistry, with comprehensive documentation at https://toolregistry.readthedocs.io/.",
    "categories": [
      "cs.SE",
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "primary_category": "cs.SE",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2507.10593v1",
    "published_date": "2025-07-11 20:23:23 UTC",
    "updated_date": "2025-07-11 20:23:23 UTC"
  },
  {
    "arxiv_id": "2507.08980v2",
    "title": "Learning Diffusion Models with Flexible Representation Guidance",
    "authors": [
      "Chenyu Wang",
      "Cai Zhou",
      "Sharut Gupta",
      "Zongyu Lin",
      "Stefanie Jegelka",
      "Stephen Bates",
      "Tommi Jaakkola"
    ],
    "abstract": "Diffusion models can be improved with additional guidance towards more effective representations of input. Indeed, prior empirical work has already shown that aligning internal representations of the diffusion model with those of pre-trained models improves generation quality. In this paper, we present a systematic framework for incorporating representation guidance into diffusion models. We provide alternative decompositions of denoising models along with their associated training criteria, where the decompositions determine when and how the auxiliary representations are incorporated. Guided by our theoretical insights, we introduce two new strategies for enhancing representation alignment in diffusion models. First, we pair examples with target representations either derived from themselves or arisen from different synthetic modalities, and subsequently learn a joint model over the multimodal pairs. Second, we design an optimal training curriculum that balances representation learning and data generation. Our experiments across image, protein sequence, and molecule generation tasks demonstrate superior performance as well as accelerated training. In particular, on the class-conditional ImageNet $256\\times 256$ benchmark, our guidance results in $23.3$ times faster training than the original SiT-XL as well as four times speedup over the state-of-the-art method REPA. The code is available at https://github.com/ChenyuWang-Monica/REED.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.LG",
    "comment": "NeurIPS 2025; Also Oral at ICML 2025 FM4LS workshop",
    "pdf_url": "https://arxiv.org/pdf/2507.08980v2",
    "published_date": "2025-07-11 19:29:02 UTC",
    "updated_date": "2025-10-13 04:40:50 UTC"
  },
  {
    "arxiv_id": "2507.08977v3",
    "title": "Simulation as Supervision: Mechanistic Pretraining for Scientific Discovery",
    "authors": [
      "Carson Dudley",
      "Reiden Magdaleno",
      "Christopher Harding",
      "Marisa Eisenberg"
    ],
    "abstract": "Scientific modeling faces a tradeoff between the interpretability of mechanistic theory and the predictive power of machine learning. While hybrid approaches like Physics-Informed Neural Networks (PINNs) embed domain knowledge as functional constraints, they can be brittle under model misspecification. We introduce Simulation-Grounded Neural Networks (SGNNs), a framework that instead embeds domain knowledge into the training data to establish a structural prior.\n  By pretraining on synthetic corpora spanning diverse model structures and observational artifacts, SGNNs learn the broad patterns of physical possibility. This allows the model to internalize the underlying dynamics of a system without being forced to satisfy a single, potentially incorrect equation.\n  We evaluated SGNNs across scientific disciplines and found that this approach confers significant robustness. In prediction tasks, SGNNs nearly tripled COVID-19 forecasting skill versus CDC baselines. In tests on dengue outbreaks, SGNNs outperformed physics-constrained models even when both were restricted to incorrect human-to-human transmission equations, demonstrating that SGNNs are potentially more robust to model misspecification. For inference, SGNNs extend the logic of simulation-based inference to enable supervised learning for unobservable targets, estimating early COVID-19 transmissibility more accurately than traditional methods. Finally, SGNNs enable back-to-simulation attribution, a form of mechanistic interpretability that maps real-world data back to the simulated manifold to identify underlying processes. By unifying these disparate simulation-based techniques into a single framework, we demonstrate that mechanistic simulations can serve as effective training data for robust scientific inference that generalizes beyond the limitations of fixed functional forms.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2507.08977v3",
    "published_date": "2025-07-11 19:18:42 UTC",
    "updated_date": "2026-01-02 13:21:46 UTC"
  },
  {
    "arxiv_id": "2507.14179v1",
    "title": "A Sparsity Predicting Approach for Large Language Models via Activation Pattern Clustering",
    "authors": [
      "Nobel Dhar",
      "Bobin Deng",
      "Md Romyull Islam",
      "Xinyue Zhang",
      "Kazi Fahim Ahmad Nasif",
      "Kun Suo"
    ],
    "abstract": "Large Language Models (LLMs) exhibit significant activation sparsity, where only a subset of neurons are active for a given input. Although this sparsity presents opportunities to reduce computational cost, efficiently utilizing it requires predicting activation patterns in a scalable manner. However, direct prediction at the neuron level is computationally expensive due to the vast number of neurons in modern LLMs. To enable efficient prediction and utilization of activation sparsity, we propose a clustering-based activation pattern compression framework. Instead of treating each neuron independently, we group similar activation patterns into a small set of representative clusters. Our method achieves up to 79.34% clustering precision, outperforming standard binary clustering approaches while maintaining minimal degradation in perplexity (PPL) scores. With a sufficiently large number of clusters, our approach attains a PPL score as low as 12.49, demonstrating its effectiveness in preserving model quality while reducing computational overhead. By predicting cluster assignments rather than individual neuron states, future models can efficiently infer activation patterns from pre-computed centroids. We detail the clustering algorithm, analyze its effectiveness in capturing meaningful activation structures, and demonstrate its potential to improve sparse computation efficiency. This clustering-based formulation serves as a foundation for future work on activation pattern prediction, paving the way for efficient inference in large-scale language models.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL",
      "cs.DC"
    ],
    "primary_category": "cs.LG",
    "comment": "To be published in Euro-Par 2025",
    "pdf_url": "https://arxiv.org/pdf/2507.14179v1",
    "published_date": "2025-07-11 19:07:29 UTC",
    "updated_date": "2025-07-11 19:07:29 UTC"
  },
  {
    "arxiv_id": "2507.08972v2",
    "title": "Simulating Three-dimensional Turbulence with Physics-informed Neural Networks",
    "authors": [
      "Sifan Wang",
      "Shyam Sankaran",
      "Xiantao Fan",
      "Panos Stinis",
      "Paris Perdikaris"
    ],
    "abstract": "Turbulent fluid flows are among the most computationally demanding problems in science, requiring enormous computational resources that become prohibitive at high flow speeds. Physics-informed neural networks (PINNs) represent a radically different approach that trains neural networks directly from physical equations rather than data, offering the potential for continuous, mesh-free solutions. Here we show that appropriately designed PINNs can successfully simulate fully turbulent flows in both two and three dimensions, directly learning solutions to the fundamental fluid equations without traditional computational grids or training data. Our approach combines several algorithmic innovations including adaptive network architectures, causal training, and advanced optimization methods to overcome the inherent challenges of learning chaotic dynamics. Through rigorous validation on challenging turbulence problems, we demonstrate that PINNs accurately reproduce key flow statistics including energy spectra, kinetic energy, enstrophy, and Reynolds stresses. Our results demonstrate that neural equation solvers can handle complex chaotic systems, opening new possibilities for continuous turbulence modeling that transcends traditional computational limitations.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "physics.comp-ph",
      "physics.flu-dyn"
    ],
    "primary_category": "cs.LG",
    "comment": "25 pages, 13 figures, 3 tables",
    "pdf_url": "https://arxiv.org/pdf/2507.08972v2",
    "published_date": "2025-07-11 19:02:52 UTC",
    "updated_date": "2025-10-11 16:25:27 UTC"
  },
  {
    "arxiv_id": "2507.08966v1",
    "title": "ToxBench: A Binding Affinity Prediction Benchmark with AB-FEP-Calculated Labels for Human Estrogen Receptor Alpha",
    "authors": [
      "Meng Liu",
      "Karl Leswing",
      "Simon K. S. Chu",
      "Farhad Ramezanghorbani",
      "Griffin Young",
      "Gabriel Marques",
      "Prerna Das",
      "Anjali Panikar",
      "Esther Jamir",
      "Mohammed Sulaiman Shamsudeen",
      "K. Shawn Watts",
      "Ananya Sen",
      "Hari Priya Devannagari",
      "Edward B. Miller",
      "Muyun Lihan",
      "Howook Hwang",
      "Janet Paulsen",
      "Xin Yu",
      "Kyle Gion",
      "Timur Rvachov",
      "Emine Kucukbenli",
      "Saee Gopal Paliwal"
    ],
    "abstract": "Protein-ligand binding affinity prediction is essential for drug discovery and toxicity assessment. While machine learning (ML) promises fast and accurate predictions, its progress is constrained by the availability of reliable data. In contrast, physics-based methods such as absolute binding free energy perturbation (AB-FEP) deliver high accuracy but are computationally prohibitive for high-throughput applications. To bridge this gap, we introduce ToxBench, the first large-scale AB-FEP dataset designed for ML development and focused on a single pharmaceutically critical target, Human Estrogen Receptor Alpha (ER$α$). ToxBench contains 8,770 ER$α$-ligand complex structures with binding free energies computed via AB-FEP with a subset validated against experimental affinities at 1.75 kcal/mol RMSE, along with non-overlapping ligand splits to assess model generalizability. Using ToxBench, we further benchmark state-of-the-art ML methods, and notably, our proposed DualBind model, which employs a dual-loss framework to effectively learn the binding energy function. The benchmark results demonstrate the superior performance of DualBind and the potential of ML to approximate AB-FEP at a fraction of the computational cost.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "physics.chem-ph",
      "q-bio.BM"
    ],
    "primary_category": "cs.LG",
    "comment": "Workshop on Generative AI for Biology at ICML 2025",
    "pdf_url": "https://arxiv.org/pdf/2507.08966v1",
    "published_date": "2025-07-11 18:50:43 UTC",
    "updated_date": "2025-07-11 18:50:43 UTC"
  },
  {
    "arxiv_id": "2507.08965v1",
    "title": "Theory-Informed Improvements to Classifier-Free Guidance for Discrete Diffusion Models",
    "authors": [
      "Kevin Rojas",
      "Ye He",
      "Chieh-Hsin Lai",
      "Yuta Takida",
      "Yuki Mitsufuji",
      "Molei Tao"
    ],
    "abstract": "Classifier-Free Guidance (CFG) is a widely used technique for conditional generation and improving sample quality in continuous diffusion models, and recent works have extended it to discrete diffusion. This paper theoretically analyzes CFG in the context of masked discrete diffusion, focusing on the role of guidance schedules. Our analysis shows that high guidance early in sampling (when inputs are heavily masked) harms generation quality, while late-stage guidance has a larger effect. These findings provide a theoretical explanation for empirical observations in recent studies on guidance schedules. The analysis also reveals an imperfection of the current CFG implementations. These implementations can unintentionally cause imbalanced transitions, such as unmasking too rapidly during the early stages of generation, which degrades the quality of the resulting samples. To address this, we draw insight from the analysis and propose a novel classifier-free guidance mechanism empirically applicable to any discrete diffusion. Intuitively, our method smoothens the transport between the data distribution and the initial (masked/uniform) distribution, which results in improved sample quality. Remarkably, our method is achievable via a simple one-line code change. The efficacy of our method is empirically demonstrated with experiments on ImageNet (masked discrete diffusion) and QM9 (uniform discrete diffusion).",
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2507.08965v1",
    "published_date": "2025-07-11 18:48:29 UTC",
    "updated_date": "2025-07-11 18:48:29 UTC"
  },
  {
    "arxiv_id": "2507.08960v1",
    "title": "How to Train a Leader: Hierarchical Reasoning in Multi-Agent LLMs",
    "authors": [
      "Andrew Estornell",
      "Jean-Francois Ton",
      "Muhammad Faaiz Taufiq",
      "Hang Li"
    ],
    "abstract": "Large Language Models (LLMs) have achieved strong performance on a wide range of complex reasoning tasks, yet further gains are often possible by leveraging the complementary strengths of multiple models. While multi-agent frameworks can improve solution quality by leveraging multiple LLMs, existing methods are often computationally expensive, both at training and inference time. In this work, we introduce a hierarchical multi-agent framework that addresses these challenges by training only a single leader LLM to coordinate a team of untrained peer agents. To this end, we propose Multi-agent guided Leader Policy \\textbf{O}ptimization (MLPO), a novel approach which trains the leader to evaluate and synthesize agent responses without auxiliary value networks or explicit agent feedback. Leaders trained with MLPO exhibit improved performance not only when interacting with the agent team at inference time, but also enjoy improved performance when deployed in single-agent settings without the team. Empirical results on Big-Bench Hard (BBH), MATH, and MMLU demonstrate that our framework achieves substantial performance improvements over both single-agent and multi-agent baselines. Our results highlight the effectiveness and efficiency of training a single, flexible leader for collaborative reasoning in multi-agent LLM systems.",
    "categories": [
      "cs.MA",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.MA",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2507.08960v1",
    "published_date": "2025-07-11 18:34:07 UTC",
    "updated_date": "2025-07-11 18:34:07 UTC"
  },
  {
    "arxiv_id": "2507.08958v2",
    "title": "Bridging Literature and the Universe Via A Multi-Agent Large Language Model System",
    "authors": [
      "Xiaowen Zhang",
      "Zhenyu Bi",
      "Patrick Lachance",
      "Xuan Wang",
      "Tiziana Di Matteo",
      "Rupert A. C. Croft"
    ],
    "abstract": "As cosmological simulations and their associated software become increasingly complex, physicists face the challenge of searching through vast amounts of literature and user manuals to extract simulation parameters from dense academic papers, each using different models and formats. Translating these parameters into executable scripts remains a time-consuming and error-prone process. To improve efficiency in physics research and accelerate the cosmological simulation process, we introduce SimAgents, a multi-agent system designed to automate both parameter configuration from the literature and preliminary analysis for cosmology research. SimAgents is powered by specialized LLM agents capable of physics reasoning, simulation software validation, and tool execution. These agents collaborate through structured communication, ensuring that extracted parameters are physically meaningful, internally consistent, and software-compliant. We also construct a cosmological parameter extraction evaluation dataset by collecting over 40 simulations in published papers from Arxiv and leading journals that cover diverse simulation types. Experiments on the dataset demonstrate a strong performance of SimAgents, highlighting its effectiveness and potential to accelerate scientific research for physicists. Our demonstration video is available at: https://youtu.be/w1zLpm_CaWA. The complete system and dataset are publicly available at https://github.com/xwzhang98/SimAgents.",
    "categories": [
      "astro-ph.IM",
      "astro-ph.CO",
      "cs.AI",
      "cs.MA"
    ],
    "primary_category": "astro-ph.IM",
    "comment": "6 pages, 4 figures",
    "pdf_url": "https://arxiv.org/pdf/2507.08958v2",
    "published_date": "2025-07-11 18:31:20 UTC",
    "updated_date": "2025-07-15 22:55:30 UTC"
  },
  {
    "arxiv_id": "2507.08945v1",
    "title": "GraphRunner: A Multi-Stage Framework for Efficient and Accurate Graph-Based Retrieval",
    "authors": [
      "Savini Kashmira",
      "Jayanaka L. Dantanarayana",
      "Krisztián Flautner",
      "Lingjia Tang",
      "Jason Mars"
    ],
    "abstract": "Conventional Retrieval Augmented Generation (RAG) approaches are common in text-based applications. However, they struggle with structured, interconnected datasets like knowledge graphs, where understanding underlying relationships is crucial for accurate retrieval. A common direction in graph-based retrieval employs iterative, rule-based traversal guided by Large Language Models (LLMs). Such existing iterative methods typically combine reasoning with single hop traversal at each step, making them vulnerable to LLM reasoning errors and hallucinations that ultimately hinder the retrieval of relevant information.\n  To address these limitations, we propose GraphRunner, a novel graph-based retrieval framework that operates in three distinct stages: planning, verification, and execution. This introduces high-level traversal actions that enable multi-hop exploration in a single step. It also generates a holistic traversal plan, which is verified against the graph structure and pre-defined traversal actions, reducing reasoning errors and detecting hallucinations before execution. GraphRunner significantly reduces LLM reasoning errors and detects hallucinations through validation. Our evaluation using the GRBench dataset shows that GraphRunner consistently outperforms existing approaches, achieving 10-50% performance improvements over the strongest baseline while reducing inference cost by 3.0-12.9x and response generation time by 2.5-7.1x, making it significantly more robust and efficient for graph-based retrieval tasks.",
    "categories": [
      "cs.IR",
      "cs.AI"
    ],
    "primary_category": "cs.IR",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2507.08945v1",
    "published_date": "2025-07-11 18:10:01 UTC",
    "updated_date": "2025-07-11 18:10:01 UTC"
  },
  {
    "arxiv_id": "2507.08944v1",
    "title": "Optimizing Sequential Multi-Step Tasks with Parallel LLM Agents",
    "authors": [
      "Enhao Zhang",
      "Erkang Zhu",
      "Gagan Bansal",
      "Adam Fourney",
      "Hussein Mozannar",
      "Jack Gerrits"
    ],
    "abstract": "Large language model (LLM)-based multi-agent systems have demonstrated remarkable promise for tackling complex tasks by breaking them down into subtasks that are iteratively planned, executed, observed, and refined. Despite their effectiveness, these systems often incur high latency because real-world problems frequently demand multiple iterative cycles of reasoning steps. To address this challenge, we propose M1-Parallel, a framework that concurrently runs multiple multi-agent teams in parallel to uncover distinct solution paths. By leveraging an event-driven communication model with asynchronous messaging, M1-Parallel efficiently capitalizes on the inherent diversity of valid plans to either reduce end-to-end latency or boost task completion rates. Our experiments on complex tasks show that M1-Parallel with early termination achieves up to $2.2\\times$ speedup while preserving accuracy, and that M1-Parallel with aggregation yields higher task completion rates. We further investigate strategies aimed at encouraging diverse execution plans but observe no additional performance gains over repeated sampling. Overall, these findings underscore the potential of parallel plan execution for optimizing multi-agent systems for real-world, high-complexity reasoning tasks.",
    "categories": [
      "cs.MA",
      "cs.AI"
    ],
    "primary_category": "cs.MA",
    "comment": "ICML 2025 Workshop on MAS",
    "pdf_url": "https://arxiv.org/pdf/2507.08944v1",
    "published_date": "2025-07-11 18:09:22 UTC",
    "updated_date": "2025-07-11 18:09:22 UTC"
  },
  {
    "arxiv_id": "2507.08801v1",
    "title": "Lumos-1: On Autoregressive Video Generation from a Unified Model Perspective",
    "authors": [
      "Hangjie Yuan",
      "Weihua Chen",
      "Jun Cen",
      "Hu Yu",
      "Jingyun Liang",
      "Shuning Chang",
      "Zhihui Lin",
      "Tao Feng",
      "Pengwei Liu",
      "Jiazheng Xing",
      "Hao Luo",
      "Jiasheng Tang",
      "Fan Wang",
      "Yi Yang"
    ],
    "abstract": "Autoregressive large language models (LLMs) have unified a vast range of language tasks, inspiring preliminary efforts in autoregressive video generation. Existing autoregressive video generators either diverge from standard LLM architectures, depend on bulky external text encoders, or incur prohibitive latency due to next-token decoding. In this paper, we introduce Lumos-1, an autoregressive video generator that retains the LLM architecture with minimal architectural modifications. To inject spatiotemporal correlations in LLMs, we identify the efficacy of incorporating 3D RoPE and diagnose its imbalanced frequency spectrum ranges. Therefore, we propose MM-RoPE, a RoPE scheme that preserves the original textual RoPE while providing comprehensive frequency spectra and scaled 3D positions for modeling multimodal spatiotemporal data. Moreover, Lumos-1 resorts to a token dependency strategy that obeys intra-frame bidirectionality and inter-frame temporal causality. Based on this dependency strategy, we identify the issue of frame-wise loss imbalance caused by spatial information redundancy and solve it by proposing Autoregressive Discrete Diffusion Forcing (AR-DF). AR-DF introduces temporal tube masking during training with a compatible inference-time masking policy to avoid quality degradation. By using memory-efficient training techniques, we pre-train Lumos-1 on only 48 GPUs, achieving performance comparable to EMU3 on GenEval, COSMOS-Video2World on VBench-I2V, and OpenSoraPlan on VBench-T2V. Code and models are available at https://github.com/alibaba-damo-academy/Lumos.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.MM"
    ],
    "primary_category": "cs.CV",
    "comment": "Code and Models: https://github.com/alibaba-damo-academy/Lumos",
    "pdf_url": "https://arxiv.org/pdf/2507.08801v1",
    "published_date": "2025-07-11 17:59:42 UTC",
    "updated_date": "2025-07-11 17:59:42 UTC"
  },
  {
    "arxiv_id": "2507.08800v1",
    "title": "NeuralOS: Towards Simulating Operating Systems via Neural Generative Models",
    "authors": [
      "Luke Rivard",
      "Sun Sun",
      "Hongyu Guo",
      "Wenhu Chen",
      "Yuntian Deng"
    ],
    "abstract": "We introduce NeuralOS, a neural framework that simulates graphical user interfaces (GUIs) of operating systems by directly predicting screen frames in response to user inputs such as mouse movements, clicks, and keyboard events. NeuralOS combines a recurrent neural network (RNN), which tracks computer state, with a diffusion-based neural renderer that generates screen images. The model is trained on a large-scale dataset of Ubuntu XFCE recordings, which include both randomly generated interactions and realistic interactions produced by AI agents. Experiments show that NeuralOS successfully renders realistic GUI sequences, accurately captures mouse interactions, and reliably predicts state transitions like application launches. Although modeling fine-grained keyboard interactions precisely remains challenging, NeuralOS offers a step toward creating fully adaptive, generative neural interfaces for future human-computer interaction systems.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL",
      "cs.HC",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2507.08800v1",
    "published_date": "2025-07-11 17:59:40 UTC",
    "updated_date": "2025-07-11 17:59:40 UTC"
  },
  {
    "arxiv_id": "2507.08799v2",
    "title": "KV Cache Steering for Controlling Frozen LLMs",
    "authors": [
      "Max Belitsky",
      "Dawid J. Kopiczko",
      "Michael Dorkenwald",
      "M. Jehanzeb Mirza",
      "James R. Glass",
      "Cees G. M. Snoek",
      "Yuki M. Asano"
    ],
    "abstract": "We propose cache steering, a lightweight method for implicit steering of language models via a one-shot intervention applied directly to the key-value cache. To validate its effectiveness, we apply cache steering to induce chain-of-thought reasoning in small language models. Our approach constructs steering vectors from reasoning traces, obtained either from teacher models (e.g., GPT-4o) or existing human annotations, that shift model behavior toward more explicit, multi-step reasoning without fine-tuning or prompt modifications. Experimental evaluations on diverse reasoning benchmarks demonstrate that cache steering improves both the qualitative structure of model reasoning and quantitative task performance. Additional experiments show that the method also scales to larger models and yields further gains on challenging datasets such as GPQA and MATH. Compared to prior activation steering techniques that require continuous interventions, our one-shot cache steering offers substantial advantages in terms of inference latency, hyperparameter stability, and ease of integration with existing inference APIs. Beyond mere reasoning induction, we show that cache steering enables controllable transfer of reasoning styles (e.g., stepwise, causal, analogical), making it a practical tool for behavior-level guidance of language models.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2507.08799v2",
    "published_date": "2025-07-11 17:59:36 UTC",
    "updated_date": "2025-09-26 17:59:54 UTC"
  },
  {
    "arxiv_id": "2507.08924v2",
    "title": "From KMMLU-Redux to KMMLU-Pro: A Professional Korean Benchmark Suite for LLM Evaluation",
    "authors": [
      "Seokhee Hong",
      "Sunkyoung Kim",
      "Guijin Son",
      "Soyeon Kim",
      "Yeonjung Hong",
      "Jinsik Lee"
    ],
    "abstract": "The development of Large Language Models (LLMs) requires robust benchmarks that encompass not only academic domains but also industrial fields to effectively evaluate their applicability in real-world scenarios. In this paper, we introduce two Korean expert-level benchmarks. KMMLU-Redux, reconstructed from the existing KMMLU, consists of questions from the Korean National Technical Qualification exams, with critical errors removed to enhance reliability. KMMLU-Pro is based on Korean National Professional Licensure exams to reflect professional knowledge in Korea. Our experiments demonstrate that these benchmarks comprehensively represent industrial knowledge in Korea. We release our dataset publicly available.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2507.08924v2",
    "published_date": "2025-07-11 17:56:32 UTC",
    "updated_date": "2025-07-18 09:31:19 UTC"
  },
  {
    "arxiv_id": "2507.08793v2",
    "title": "Optimistic Exploration for Risk-Averse Constrained Reinforcement Learning",
    "authors": [
      "James McCarthy",
      "Radu Marinescu",
      "Elizabeth Daly",
      "Ivana Dusparic"
    ],
    "abstract": "Risk-averse Constrained Reinforcement Learning (RaCRL) aims to learn policies that minimise the likelihood of rare and catastrophic constraint violations caused by an environment's inherent randomness. In general, risk-aversion leads to conservative exploration of the environment which typically results in converging to sub-optimal policies that fail to adequately maximise reward or, in some cases, fail to achieve the goal. In this paper, we propose an exploration-based approach for RaCRL called Optimistic Risk-averse Actor Critic (ORAC), which constructs an exploratory policy by maximising a local upper confidence bound of the state-action reward value function whilst minimising a local lower confidence bound of the risk-averse state-action cost value function. Specifically, at each step, the weighting assigned to the cost value is increased or decreased if it exceeds or falls below the safety constraint value. This way the policy is encouraged to explore uncertain regions of the environment to discover high reward states whilst still satisfying the safety constraints. Our experimental results demonstrate that the ORAC approach prevents convergence to sub-optimal policies and improves significantly the reward-cost trade-off in various continuous control tasks such as Safety-Gymnasium and a complex building energy management environment CityLearn.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2507.08793v2",
    "published_date": "2025-07-11 17:54:54 UTC",
    "updated_date": "2025-08-27 12:33:06 UTC"
  },
  {
    "arxiv_id": "2507.10591v1",
    "title": "MH-FSF: A Unified Framework for Overcoming Benchmarking and Reproducibility Limitations in Feature Selection Evaluation",
    "authors": [
      "Vanderson Rocha",
      "Diego Kreutz",
      "Gabriel Canto",
      "Hendrio Bragança",
      "Eduardo Feitosa"
    ],
    "abstract": "Feature selection is vital for building effective predictive models, as it reduces dimensionality and emphasizes key features. However, current research often suffers from limited benchmarking and reliance on proprietary datasets. This severely hinders reproducibility and can negatively impact overall performance. To address these limitations, we introduce the MH-FSF framework, a comprehensive, modular, and extensible platform designed to facilitate the reproduction and implementation of feature selection methods. Developed through collaborative research, MH-FSF provides implementations of 17 methods (11 classical, 6 domain-specific) and enables systematic evaluation on 10 publicly available Android malware datasets. Our results reveal performance variations across both balanced and imbalanced datasets, highlighting the critical need for data preprocessing and selection criteria that account for these asymmetries. We demonstrate the importance of a unified platform for comparing diverse feature selection techniques, fostering methodological consistency and rigor. By providing this framework, we aim to significantly broaden the existing literature and pave the way for new research directions in feature selection, particularly within the context of Android malware detection.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CR",
      "cs.PF"
    ],
    "primary_category": "cs.LG",
    "comment": "11 pages; 4 figures; 5 tables; submitted to JBCS",
    "pdf_url": "https://arxiv.org/pdf/2507.10591v1",
    "published_date": "2025-07-11 17:53:37 UTC",
    "updated_date": "2025-07-11 17:53:37 UTC"
  },
  {
    "arxiv_id": "2507.10435v2",
    "title": "From Sequence to Structure: Uncovering Substructure Reasoning in Transformers",
    "authors": [
      "Xinnan Dai",
      "Kai Yang",
      "Jay Revolinsky",
      "Kai Guo",
      "Aoran Wang",
      "Bohang Zhang",
      "Jiliang Tang"
    ],
    "abstract": "Recent studies suggest that large language models (LLMs) possess the capability to solve graph reasoning tasks. Notably, even when graph structures are embedded within textual descriptions, LLMs can still effectively answer related questions. This raises a fundamental question: How can a decoder-only Transformer architecture understand underlying graph structures? To address this, we start with the substructure extraction task, interpreting the inner mechanisms inside the transformers and analyzing the impact of the input queries. Specifically, through both empirical results and theoretical analysis, we present Induced Substructure Filtration (ISF), a perspective that captures the substructure identification in the multi-layer transformers. We further validate the ISF process in LLMs, revealing consistent internal dynamics across layers. Building on these insights, we explore the broader capabilities of Transformers in handling diverse graph types. Specifically, we introduce the concept of thinking in substructures to efficiently extract complex composite patterns, and demonstrate that decoder-only Transformers can successfully extract substructures from attributed graphs, such as molecular graphs. Together, our findings offer a new insight on how sequence-based Transformers perform the substructure extraction task over graph data.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Camera Ready version for Neurips 2025",
    "pdf_url": "https://arxiv.org/pdf/2507.10435v2",
    "published_date": "2025-07-11 17:36:24 UTC",
    "updated_date": "2025-10-19 20:56:59 UTC"
  },
  {
    "arxiv_id": "2507.10590v1",
    "title": "Repairing Language Model Pipelines by Meta Self-Refining Competing Constraints at Runtime",
    "authors": [
      "Mojtaba Eshghie"
    ],
    "abstract": "Language Model (LM) pipelines can dynamically refine their outputs against programmatic constraints. However, their effectiveness collapses when faced with competing soft constraints, leading to inefficient backtracking loops where satisfying one constraint violates another. We introduce Meta Self-Refining, a framework that equips LM pipelines with a meta-corrective layer to repair these competitions at runtime/inference-time. Our approach monitors the pipeline's execution history to detect oscillatory failures. Upon detection, it invokes a meta-repairer LM that analyzes the holistic state of the backtracking attempts and synthesizes a strategic instruction to balance the competing requirements. This self-repair instruction guides the original LM out of a failing refining loop towards a successful output. Our results show Meta Self-Refining can successfully repair these loops, leading to more efficient LM programs.",
    "categories": [
      "cs.SE",
      "cs.AI",
      "cs.IR"
    ],
    "primary_category": "cs.SE",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2507.10590v1",
    "published_date": "2025-07-11 17:35:12 UTC",
    "updated_date": "2025-07-11 17:35:12 UTC"
  },
  {
    "arxiv_id": "2507.08768v1",
    "title": "On Barriers to Archival Audio Processing",
    "authors": [
      "Peter Sullivan",
      "Muhammad Abdul-Mageed"
    ],
    "abstract": "In this study, we leverage a unique UNESCO collection of mid-20th century radio recordings to probe the robustness of modern off-the-shelf language identification (LID) and speaker recognition (SR) methods, especially with respect to the impact of multilingual speakers and cross-age recordings. Our findings suggest that LID systems, such as Whisper, are increasingly adept at handling second-language and accented speech. However, speaker embeddings remain a fragile component of speech processing pipelines that is prone to biases related to the channel, age, and language. Issues which will need to be overcome should archives aim to employ SR methods for speaker indexing.",
    "categories": [
      "cs.SD",
      "cs.AI",
      "cs.CL",
      "eess.AS"
    ],
    "primary_category": "cs.SD",
    "comment": "Update with Acknowledgements of ICNSLP 2025 paper",
    "pdf_url": "https://arxiv.org/pdf/2507.08768v1",
    "published_date": "2025-07-11 17:27:11 UTC",
    "updated_date": "2025-07-11 17:27:11 UTC"
  },
  {
    "arxiv_id": "2507.08766v1",
    "title": "A Hybrid Multi-Well Hopfield-CNN with Feature Extraction and K-Means for MNIST Classification",
    "authors": [
      "Ahmed Farooq"
    ],
    "abstract": "This study presents a hybrid model for classifying handwritten digits in the MNIST dataset, combining convolutional neural networks (CNNs) with a multi-well Hopfield network. The approach employs a CNN to extract high-dimensional features from input images, which are then clustered into class-specific prototypes using k-means clustering. These prototypes serve as attractors in a multi-well energy landscape, where a Hopfield network performs classification by minimizing an energy function that balances feature similarity and class assignment.The model's design enables robust handling of intraclass variability, such as diverse handwriting styles, while providing an interpretable framework through its energy-based decision process. Through systematic optimization of the CNN architecture and the number of wells, the model achieves a high test accuracy of 99.2% on 10,000 MNIST images, demonstrating its effectiveness for image classification tasks. The findings highlight the critical role of deep feature extraction and sufficient prototype coverage in achieving high performance, with potential for broader applications in pattern recognition.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2507.08766v1",
    "published_date": "2025-07-11 17:26:06 UTC",
    "updated_date": "2025-07-11 17:26:06 UTC"
  },
  {
    "arxiv_id": "2507.08765v1",
    "title": "Compress Any Segment Anything Model (SAM)",
    "authors": [
      "Juntong Fan",
      "Zhiwei Hao",
      "Jianqiang Shen",
      "Shang-Ling Jui",
      "Yi Zhang",
      "Jing-Xiao Liao",
      "Feng-Lei Fan"
    ],
    "abstract": "Due to the excellent performance in yielding high-quality, zero-shot segmentation, Segment Anything Model (SAM) and its variants have been widely applied in diverse scenarios such as healthcare and intelligent manufacturing. Therefore, effectively compressing SAMs has become an increasingly pressing practical need. In this study, we propose Birkhoff, a novel data-free compression algorithm for SAM and its variants. Unlike quantization, pruning, distillation, and other compression methods, Birkhoff embodies versatility across model types, agility in deployment, faithfulness to the original model, and compactness in model size. Specifically, Birkhoff introduces a novel compression algorithm: Hyper-Compression, whose core principle is to find a dense trajectory to turn a high-dimensional parameter vector into a low-dimensional scalar. Furthermore, Birkhoff designs a dedicated linear layer operator, HyperLinear, to fuse decompression and matrix multiplication to significantly accelerate inference of the compressed SAMs. Extensive experiments on 18 SAMs in the COCO, LVIS, and SA-1B datasets show that Birkhoff performs consistently and competitively in compression time, compression ratio, post-compression performance, and inference speed. For example, Birkhoff can achieve a compression ratio of 5.17x on SAM2-B, with less than 1% performance drop without using any fine-tuning data. Moreover, the compression is finished within 60 seconds for all models.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "13 pages, 6 tables, 8 figures",
    "pdf_url": "https://arxiv.org/pdf/2507.08765v1",
    "published_date": "2025-07-11 17:21:06 UTC",
    "updated_date": "2025-07-11 17:21:06 UTC"
  },
  {
    "arxiv_id": "2507.08761v2",
    "title": "Penalizing Infeasible Actions and Reward Scaling in Reinforcement Learning with Offline Data",
    "authors": [
      "Jeonghye Kim",
      "Yongjae Shin",
      "Whiyoung Jung",
      "Sunghoon Hong",
      "Deunsol Yoon",
      "Youngchul Sung",
      "Kanghoon Lee",
      "Woohyung Lim"
    ],
    "abstract": "Reinforcement learning with offline data suffers from Q-value extrapolation errors. To address this issue, we first demonstrate that linear extrapolation of the Q-function beyond the data range is particularly problematic. To mitigate this, we propose guiding the gradual decrease of Q-values outside the data range, which is achieved through reward scaling with layer normalization (RS-LN) and a penalization mechanism for infeasible actions (PA). By combining RS-LN and PA, we develop a new algorithm called PARS. We evaluate PARS across a range of tasks, demonstrating superior performance compared to state-of-the-art algorithms in both offline training and online fine-tuning on the D4RL benchmark, with notable success in the challenging AntMaze Ultra task.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted to ICML2025 (spotlight)",
    "pdf_url": "https://arxiv.org/pdf/2507.08761v2",
    "published_date": "2025-07-11 17:16:02 UTC",
    "updated_date": "2025-08-19 17:06:02 UTC"
  },
  {
    "arxiv_id": "2507.08920v3",
    "title": "AMix-1: A Pathway to Test-Time Scalable Protein Foundation Model",
    "authors": [
      "Changze Lv",
      "Jiang Zhou",
      "Siyu Long",
      "Lihao Wang",
      "Jiangtao Feng",
      "Dongyu Xue",
      "Yu Pei",
      "Hao Wang",
      "Zherui Zhang",
      "Yuchen Cai",
      "Zhiqiang Gao",
      "Ziyuan Ma",
      "Jiakai Hu",
      "Chaochen Gao",
      "Jingjing Gong",
      "Yuxuan Song",
      "Shuyi Zhang",
      "Xiaoqing Zheng",
      "Deyi Xiong",
      "Lei Bai",
      "Wanli Ouyang",
      "Ya-Qin Zhang",
      "Wei-Ying Ma",
      "Bowen Zhou",
      "Hao Zhou"
    ],
    "abstract": "We introduce AMix-1, a powerful protein foundation model built on Bayesian Flow Networks and empowered by a systematic training methodology, encompassing pretraining scaling laws, emergent capability analysis, in-context learning mechanism, and test-time scaling algorithm. To guarantee robust scalability, we establish a predictive scaling law and reveal the progressive emergence of structural understanding via loss perspective, culminating in a strong 1.7-billion model. Building on this foundation, we devise a multiple sequence alignment (MSA)-based in-context learning strategy to unify protein design into a general framework, where AMix-1 recognizes deep evolutionary signals among MSAs and consistently generates structurally and functionally coherent proteins. This framework enables the successful design of a dramatically improved AmeR variant with an up to $50\\times$ activity increase over its wild type. Pushing the boundaries of protein engineering, we further empower AMix-1 with an evolutionary test-time scaling algorithm for in silico directed evolution that delivers substantial, scalable performance gains as verification budgets are intensified, laying the groundwork for next-generation lab-in-the-loop protein design.",
    "categories": [
      "q-bio.BM",
      "cs.AI"
    ],
    "primary_category": "q-bio.BM",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2507.08920v3",
    "published_date": "2025-07-11 17:02:25 UTC",
    "updated_date": "2025-08-08 17:43:12 UTC"
  },
  {
    "arxiv_id": "2507.11548v2",
    "title": "Fairness Is Not Enough: Auditing Competence and Intersectional Bias in AI-powered Resume Screening",
    "authors": [
      "Kevin T Webster"
    ],
    "abstract": "The increasing use of generative AI for resume screening is predicated on the assumption that it offers an unbiased alternative to biased human decision-making. However, this belief fails to address a critical question: are these AI systems fundamentally competent at the evaluative tasks they are meant to perform?\n  This study investigates the question of competence through a two-part audit of eight major AI platforms. Experiment 1 confirmed complex, contextual racial and gender biases, with some models penalizing candidates merely for the presence of demographic signals. Experiment 2, which evaluated core competence, provided a critical insight: some models that appeared unbiased were, in fact, incapable of performing a substantive evaluation, relying instead on superficial keyword matching.\n  This paper introduces the \"Illusion of Neutrality\" to describe this phenomenon, where an apparent lack of bias is merely a symptom of a model's inability to make meaningful judgments. This study recommends that organizations and regulators adopt a dual-validation framework, auditing AI hiring tools for both demographic bias and demonstrable competence to ensure they are both equitable and effective.",
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.CY",
    "comment": "34 pages, 4 figures",
    "pdf_url": "https://arxiv.org/pdf/2507.11548v2",
    "published_date": "2025-07-11 16:57:13 UTC",
    "updated_date": "2025-07-17 01:30:09 UTC"
  },
  {
    "arxiv_id": "2507.08743v1",
    "title": "Geo-ORBIT: A Federated Digital Twin Framework for Scene-Adaptive Lane Geometry Detection",
    "authors": [
      "Rei Tamaru",
      "Pei Li",
      "Bin Ran"
    ],
    "abstract": "Digital Twins (DT) have the potential to transform traffic management and operations by creating dynamic, virtual representations of transportation systems that sense conditions, analyze operations, and support decision-making. A key component for DT of the transportation system is dynamic roadway geometry sensing. However, existing approaches often rely on static maps or costly sensors, limiting scalability and adaptability. Additionally, large-scale DTs that collect and analyze data from multiple sources face challenges in privacy, communication, and computational efficiency. To address these challenges, we introduce Geo-ORBIT (Geometrical Operational Roadway Blueprint with Integrated Twin), a unified framework that combines real-time lane detection, DT synchronization, and federated meta-learning. At the core of Geo-ORBIT is GeoLane, a lightweight lane detection model that learns lane geometries from vehicle trajectory data using roadside cameras. We extend this model through Meta-GeoLane, which learns to personalize detection parameters for local entities, and FedMeta-GeoLane, a federated learning strategy that ensures scalable and privacy-preserving adaptation across roadside deployments. Our system is integrated with CARLA and SUMO to create a high-fidelity DT that renders highway scenarios and captures traffic flows in real-time. Extensive experiments across diverse urban scenes show that FedMeta-GeoLane consistently outperforms baseline and meta-learning approaches, achieving lower geometric error and stronger generalization to unseen locations while drastically reducing communication overhead. This work lays the foundation for flexible, context-aware infrastructure modeling in DTs. The framework is publicly available at https://github.com/raynbowy23/FedMeta-GeoLane.git.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2507.08743v1",
    "published_date": "2025-07-11 16:45:59 UTC",
    "updated_date": "2025-07-11 16:45:59 UTC"
  },
  {
    "arxiv_id": "2507.08738v2",
    "title": "Adaptive Nonlinear Vector Autoregression: Robust Forecasting for Noisy Chaotic Time Series",
    "authors": [
      "Azimov Sherkhon",
      "Susana Lopez-Moreno",
      "Eric Dolores-Cuenca",
      "Sieun Lee",
      "Sangil Kim"
    ],
    "abstract": "Nonlinear vector autoregression (NVAR) and reservoir computing (RC) have shown promise in forecasting chaotic dynamical systems, such as the Lorenz-63 model and El Nino-Southern Oscillation. However, their reliance on fixed nonlinear transformations - polynomial expansions in NVAR or random feature maps in RC - limits their adaptability to high noise or complex real-world data. Furthermore, these methods also exhibit poor scalability in high-dimensional settings due to costly matrix inversion during optimization. We propose a data-adaptive NVAR model that combines delay-embedded linear inputs with features generated by a shallow, trainable multilayer perceptron (MLP). Unlike standard NVAR and RC models, the MLP and linear readout are jointly trained using gradient-based optimization, enabling the model to learn data-driven nonlinearities, while preserving a simple readout structure and improving scalability. Initial experiments across multiple chaotic systems, tested under noise-free and synthetically noisy conditions, showed that the adaptive model outperformed in predictive accuracy the standard NVAR, a leaky echo state network (ESN) - the most common RC model - and a hybrid ESN, thereby showing robust forecasting under noisy conditions.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "math.DS"
    ],
    "primary_category": "cs.LG",
    "comment": "17 pages, 5 figures, 5 tables. New comparisons added: ESN and HESN. New datasets added to experiments: Mackey-Glass and Lorenz 96 with 100 variables",
    "pdf_url": "https://arxiv.org/pdf/2507.08738v2",
    "published_date": "2025-07-11 16:40:10 UTC",
    "updated_date": "2025-12-01 05:38:34 UTC"
  },
  {
    "arxiv_id": "2507.08736v1",
    "title": "Catastrophic Forgetting Mitigation Through Plateau Phase Activity Profiling",
    "authors": [
      "Idan Mashiach",
      "Oren Glickman",
      "Tom Tirer"
    ],
    "abstract": "Catastrophic forgetting in deep neural networks occurs when learning new tasks degrades performance on previously learned tasks due to knowledge overwriting. Among the approaches to mitigate this issue, regularization techniques aim to identify and constrain \"important\" parameters to preserve previous knowledge. In the highly nonconvex optimization landscape of deep learning, we propose a novel perspective: tracking parameters during the final training plateau is more effective than monitoring them throughout the entire training process. We argue that parameters that exhibit higher activity (movement and variability) during this plateau reveal directions in the loss landscape that are relatively flat, making them suitable for adaptation to new tasks while preserving knowledge from previous ones. Our comprehensive experiments demonstrate that this approach achieves superior performance in balancing catastrophic forgetting mitigation with strong performance on newly learned tasks.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2507.08736v1",
    "published_date": "2025-07-11 16:38:40 UTC",
    "updated_date": "2025-07-11 16:38:40 UTC"
  },
  {
    "arxiv_id": "2507.08730v4",
    "title": "Dually Hierarchical Drift Adaptation for Online Configuration Performance Learning",
    "authors": [
      "Zezhen Xiang",
      "Jingzhi Gong",
      "Tao Chen"
    ],
    "abstract": "Modern configurable software systems need to learn models that correlate configuration and performance. However, when the system operates in dynamic environments, the workload variations, hardware changes, and system updates will inevitably introduce concept drifts at different levels - global drifts, which reshape the performance landscape of the entire configuration space; and local drifts, which only affect certain sub-regions of that space. As such, existing offline and transfer learning approaches can struggle to adapt to these implicit and unpredictable changes in real-time, rendering configuration performance learning challenging. To address this, we propose DHDA, an online configuration performance learning framework designed to capture and adapt to these drifts at different levels. The key idea is that DHDA adapts to both the local and global drifts using dually hierarchical adaptation: at the upper level, we redivide the data into different divisions, within each of which the local model is retrained, to handle global drifts only when necessary. At the lower level, the local models of the divisions can detect local drifts and adapt themselves asynchronously. To balance responsiveness and efficiency, DHDA combines incremental updates with periodic full retraining to minimize redundant computation when no drifts are detected. Through evaluating eight software systems and against state-of-the-art approaches, we show that DHDA achieves considerably better accuracy and can effectively adapt to drifts with up to 2x improvements, while incurring reasonable overhead and is able to improve different local models in handling concept drift.",
    "categories": [
      "cs.SE",
      "cs.AI"
    ],
    "primary_category": "cs.SE",
    "comment": "Accepted by ICSE 2026",
    "pdf_url": "https://arxiv.org/pdf/2507.08730v4",
    "published_date": "2025-07-11 16:31:42 UTC",
    "updated_date": "2025-08-29 16:25:12 UTC"
  },
  {
    "arxiv_id": "2507.10589v1",
    "title": "Comparative Analysis of Vision Transformers and Traditional Deep Learning Approaches for Automated Pneumonia Detection in Chest X-Rays",
    "authors": [
      "Gaurav Singh"
    ],
    "abstract": "Pneumonia, particularly when induced by diseases like COVID-19, remains a critical global health challenge requiring rapid and accurate diagnosis. This study presents a comprehensive comparison of traditional machine learning and state-of-the-art deep learning approaches for automated pneumonia detection using chest X-rays (CXRs). We evaluate multiple methodologies, ranging from conventional machine learning techniques (PCA-based clustering, Logistic Regression, and Support Vector Classification) to advanced deep learning architectures including Convolutional Neural Networks (Modified LeNet, DenseNet-121) and various Vision Transformer (ViT) implementations (Deep-ViT, Compact Convolutional Transformer, and Cross-ViT). Using a dataset of 5,856 pediatric CXR images, we demonstrate that Vision Transformers, particularly the Cross-ViT architecture, achieve superior performance with 88.25% accuracy and 99.42% recall, surpassing traditional CNN approaches. Our analysis reveals that architectural choices impact performance more significantly than model size, with Cross-ViT's 75M parameters outperforming larger models. The study also addresses practical considerations including computational efficiency, training requirements, and the critical balance between precision and recall in medical diagnostics. Our findings suggest that Vision Transformers offer a promising direction for automated pneumonia detection, potentially enabling more rapid and accurate diagnosis during health crises.",
    "categories": [
      "eess.IV",
      "cs.AI",
      "cs.CV",
      "cs.NE"
    ],
    "primary_category": "eess.IV",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2507.10589v1",
    "published_date": "2025-07-11 16:26:24 UTC",
    "updated_date": "2025-07-11 16:26:24 UTC"
  },
  {
    "arxiv_id": "2507.08721v2",
    "title": "Monitoring Risks in Test-Time Adaptation",
    "authors": [
      "Mona Schirmer",
      "Metod Jazbec",
      "Christian A. Naesseth",
      "Eric Nalisnick"
    ],
    "abstract": "Encountering shifted data at test time is a ubiquitous challenge when deploying predictive models. Test-time adaptation (TTA) methods address this issue by continuously adapting a deployed model using only unlabeled test data. While TTA can extend the model's lifespan, it is only a temporary solution. Eventually the model might degrade to the point that it must be taken offline and retrained. To detect such points of ultimate failure, we propose pairing TTA with risk monitoring frameworks that track predictive performance and raise alerts when predefined performance criteria are violated. Specifically, we extend existing monitoring tools based on sequential testing with confidence sequences to accommodate scenarios in which the model is updated at test time and no test labels are available to estimate the performance metrics of interest. Our extensions unlock the application of rigorous statistical risk monitoring to TTA, and we demonstrate the effectiveness of our proposed TTA monitoring framework across a representative set of datasets, distribution shift types, and TTA methods.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2507.08721v2",
    "published_date": "2025-07-11 16:21:33 UTC",
    "updated_date": "2025-11-08 11:23:16 UTC"
  },
  {
    "arxiv_id": "2507.08719v1",
    "title": "Multilingual Multimodal Software Developer for Code Generation",
    "authors": [
      "Linzheng Chai",
      "Jian Yang",
      "Shukai Liu",
      "Wei Zhang",
      "Liran Wang",
      "Ke Jin",
      "Tao Sun",
      "Congnan Liu",
      "Chenchen Zhang",
      "Hualei Zhu",
      "Jiaheng Liu",
      "Xianjie Wu",
      "Ge Zhang",
      "Tianyu Liu",
      "Zhoujun Li"
    ],
    "abstract": "The rapid advancement of Large Language Models (LLMs) has significantly improved code generation, yet most models remain text-only, neglecting crucial visual aids like diagrams and flowcharts used in real-world software development. To bridge this gap, we introduce MM-Coder, a Multilingual Multimodal software developer. MM-Coder integrates visual design inputs-Unified Modeling Language (UML) diagrams and flowcharts (termed Visual Workflow)-with textual instructions to enhance code generation accuracy and architectural alignment. To enable this, we developed MMc-Instruct, a diverse multimodal instruction-tuning dataset including visual-workflow-based code generation, allowing MM-Coder to synthesize textual and graphical information like human developers, distinct from prior work on narrow tasks. Furthermore, we introduce MMEval, a new benchmark for evaluating multimodal code generation, addressing existing text-only limitations. Our evaluations using MMEval highlight significant remaining challenges for models in precise visual information capture, instruction following, and advanced programming knowledge. Our work aims to revolutionize industrial programming by enabling LLMs to interpret and implement complex specifications conveyed through both text and visual designs.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.SE"
    ],
    "primary_category": "cs.CL",
    "comment": "Preprint",
    "pdf_url": "https://arxiv.org/pdf/2507.08719v1",
    "published_date": "2025-07-11 16:19:53 UTC",
    "updated_date": "2025-07-11 16:19:53 UTC"
  },
  {
    "arxiv_id": "2507.08715v1",
    "title": "System-of-systems Modeling and Optimization: An Integrated Framework for Intermodal Mobility",
    "authors": [
      "Paul Saves",
      "Jasper Bussemaker",
      "Rémi Lafage",
      "Thierry Lefebvre",
      "Nathalie Bartoli",
      "Youssef Diouane",
      "Joseph Morlier"
    ],
    "abstract": "For developing innovative systems architectures, modeling and optimization techniques have been central to frame the architecting process and define the optimization and modeling problems. In this context, for system-of-systems the use of efficient dedicated approaches (often physics-based simulations) is highly recommended to reduce the computational complexity of the targeted applications. However, exploring novel architectures using such dedicated approaches might pose challenges for optimization algorithms, including increased evaluation costs and potential failures. To address these challenges, surrogate-based optimization algorithms, such as Bayesian optimization utilizing Gaussian process models have emerged.",
    "categories": [
      "cs.AI",
      "eess.SY",
      "math.OC"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2507.08715v1",
    "published_date": "2025-07-11 16:15:41 UTC",
    "updated_date": "2025-07-11 16:15:41 UTC"
  },
  {
    "arxiv_id": "2507.08705v1",
    "title": "elsciRL: Integrating Language Solutions into Reinforcement Learning Problem Settings",
    "authors": [
      "Philip Osborne",
      "Danilo S. Carvalho",
      "André Freitas"
    ],
    "abstract": "We present elsciRL, an open-source Python library to facilitate the application of language solutions on reinforcement learning problems. We demonstrate the potential of our software by extending the Language Adapter with Self-Completing Instruction framework defined in (Osborne, 2024) with the use of LLMs. Our approach can be re-applied to new applications with minimal setup requirements. We provide a novel GUI that allows a user to provide text input for an LLM to generate instructions which it can then self-complete. Empirical results indicate that these instructions \\textit{can} improve a reinforcement learning agent's performance. Therefore, we present this work to accelerate the evaluation of language solutions on reward based environments to enable new opportunities for scientific discovery.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "6 pages, 1 figure, 3 tables, 11 Appendix pages, submitted to EMNLP 2025 Call for System Demonstrations",
    "pdf_url": "https://arxiv.org/pdf/2507.08705v1",
    "published_date": "2025-07-11 16:02:24 UTC",
    "updated_date": "2025-07-11 16:02:24 UTC"
  },
  {
    "arxiv_id": "2507.08704v2",
    "title": "Knowledge Fusion via Bidirectional Information Aggregation",
    "authors": [
      "Songlin Zhai",
      "Guilin Qi",
      "Yue Wang",
      "Yuan Meng"
    ],
    "abstract": "Knowledge graphs (KGs) are the cornerstone of the semantic web, offering up-to-date representations of real-world entities and relations. Yet large language models (LLMs) remain largely static after pre-training, causing their internal knowledge to become outdated and limiting their utility in time-sensitive web applications. To bridge this gap between dynamic knowledge and static models, a prevalent approach is to enhance LLMs with KGs. However, prevailing methods typically rely on parameter-invasive fine-tuning, which risks catastrophic forgetting and often degrades LLMs' general capabilities. Moreover, their static integration frameworks cannot keep pace with the continuous evolution of real-world KGs, hindering their deployment in dynamic web environments. To bridge this gap, we introduce KGA (\\textit{\\underline{K}nowledge \\underline{G}raph-guided \\underline{A}ttention}), a novel framework that dynamically integrates external KGs into LLMs exclusively at inference-time without any parameter modification. Inspired by research on neuroscience, we rewire the self-attention module by innovatively introducing two synergistic pathways: a \\textit{bottom-up knowledge fusion} pathway and a \\textit{top-down attention guidance} pathway. The \\textit{bottom-up pathway} dynamically integrates external knowledge into input representations via input-driven KG fusion, which is akin to the \\textit{stimulus-driven attention process} in the human brain. Complementarily, the \\textit{top-down pathway} aims to assess the contextual relevance of each triple through a \\textit{goal-directed verification process}, thereby suppressing task-irrelevant signals and amplifying knowledge-relevant patterns. By synergistically combining these two pathways, our method supports real-time knowledge fusion. Extensive experiments on four benchmarks verify KGA's strong fusion performance and efficiency.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2507.08704v2",
    "published_date": "2025-07-11 15:57:37 UTC",
    "updated_date": "2025-10-14 14:35:49 UTC"
  },
  {
    "arxiv_id": "2507.08702v1",
    "title": "ONION: A Multi-Layered Framework for Participatory ER Design",
    "authors": [
      "Viktoriia Makovska",
      "George Fletcher",
      "Julia Stoyanovich"
    ],
    "abstract": "We present ONION, a multi-layered framework for participatory Entity-Relationship (ER) modeling that integrates insights from design justice, participatory AI, and conceptual modeling. ONION introduces a five-stage methodology: Observe, Nurture, Integrate, Optimize, Normalize. It supports progressive abstraction from unstructured stakeholder input to structured ER diagrams.\n  Our approach aims to reduce designer bias, promote inclusive participation, and increase transparency through the modeling process. We evaluate ONION through real-world workshops focused on sociotechnical systems in Ukraine, highlighting how diverse stakeholder engagement leads to richer data models and deeper mutual understanding. Early results demonstrate ONION's potential to host diversity in early-stage data modeling. We conclude with lessons learned, limitations and challenges involved in scaling and refining the framework for broader adoption.",
    "categories": [
      "cs.DB",
      "cs.AI",
      "cs.CY"
    ],
    "primary_category": "cs.DB",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2507.08702v1",
    "published_date": "2025-07-11 15:53:44 UTC",
    "updated_date": "2025-07-11 15:53:44 UTC"
  },
  {
    "arxiv_id": "2507.08701v2",
    "title": "A Personalised Formal Verification Framework for Monitoring Activities of Daily Living of Older Adults Living Independently in Their Homes",
    "authors": [
      "Ricardo Contreras",
      "Filip Smola",
      "Nuša Farič",
      "Jiawei Zheng",
      "Jane Hillston",
      "Jacques D. Fleuriot"
    ],
    "abstract": "There is an imperative need to provide quality of life to a growing population of older adults living independently. Personalised solutions that focus on the person and take into consideration their preferences and context are key. In this work, we introduce a framework for representing and reasoning about the Activities of Daily Living of older adults living independently at home. The framework integrates data from sensors and contextual information that aggregates semi-structured interviews, home layouts and sociological observations from the participants. We use these data to create formal models, personalised for each participant according to their preferences and context. We formulate requirements that are specific to each individual as properties encoded in Linear Temporal Logic and use a model checker to verify whether each property is satisfied by the model. When a property is violated, a counterexample is generated giving the cause of the violation. We demonstrate the framework's generalisability by applying it to different participants, highlighting its potential to enhance the safety and well-being of older adults ageing in place.",
    "categories": [
      "cs.LO",
      "cs.AI",
      "cs.CY"
    ],
    "primary_category": "cs.LO",
    "comment": "19 pages, 6 figures",
    "pdf_url": "https://arxiv.org/pdf/2507.08701v2",
    "published_date": "2025-07-11 15:53:15 UTC",
    "updated_date": "2025-11-12 10:19:01 UTC"
  },
  {
    "arxiv_id": "2507.08683v1",
    "title": "MoSAiC: Multi-Modal Multi-Label Supervision-Aware Contrastive Learning for Remote Sensing",
    "authors": [
      "Debashis Gupta",
      "Aditi Golder",
      "Rongkhun Zhu",
      "Kangning Cui",
      "Wei Tang",
      "Fan Yang",
      "Ovidiu Csillik",
      "Sarra Alaqahtani",
      "V. Paul Pauca"
    ],
    "abstract": "Contrastive learning (CL) has emerged as a powerful paradigm for learning transferable representations without the reliance on large labeled datasets. Its ability to capture intrinsic similarities and differences among data samples has led to state-of-the-art results in computer vision tasks. These strengths make CL particularly well-suited for Earth System Observation (ESO), where diverse satellite modalities such as optical and SAR imagery offer naturally aligned views of the same geospatial regions. However, ESO presents unique challenges, including high inter-class similarity, scene clutter, and ambiguous boundaries, which complicate representation learning -- especially in low-label, multi-label settings. Existing CL frameworks often focus on intra-modality self-supervision or lack mechanisms for multi-label alignment and semantic precision across modalities. In this work, we introduce MoSAiC, a unified framework that jointly optimizes intra- and inter-modality contrastive learning with a multi-label supervised contrastive loss. Designed specifically for multi-modal satellite imagery, MoSAiC enables finer semantic disentanglement and more robust representation learning across spectrally similar and spatially complex classes. Experiments on two benchmark datasets, BigEarthNet V2.0 and Sent12MS, show that MoSAiC consistently outperforms both fully supervised and self-supervised baselines in terms of accuracy, cluster coherence, and generalization in low-label and high-class-overlap scenarios.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2507.08683v1",
    "published_date": "2025-07-11 15:33:51 UTC",
    "updated_date": "2025-07-11 15:33:51 UTC"
  },
  {
    "arxiv_id": "2507.08912v1",
    "title": "Fair-FLIP: Fair Deepfake Detection with Fairness-Oriented Final Layer Input Prioritising",
    "authors": [
      "Tomasz Szandala",
      "Fatima Ezzeddine",
      "Natalia Rusin",
      "Silvia Giordano",
      "Omran Ayoub"
    ],
    "abstract": "Artificial Intelligence-generated content has become increasingly popular, yet its malicious use, particularly the deepfakes, poses a serious threat to public trust and discourse. While deepfake detection methods achieve high predictive performance, they often exhibit biases across demographic attributes such as ethnicity and gender. In this work, we tackle the challenge of fair deepfake detection, aiming to mitigate these biases while maintaining robust detection capabilities. To this end, we propose a novel post-processing approach, referred to as Fairness-Oriented Final Layer Input Prioritising (Fair-FLIP), that reweights a trained model's final-layer inputs to reduce subgroup disparities, prioritising those with low variability while demoting highly variable ones. Experimental results comparing Fair-FLIP to both the baseline (without fairness-oriented de-biasing) and state-of-the-art approaches show that Fair-FLIP can enhance fairness metrics by up to 30% while maintaining baseline accuracy, with only a negligible reduction of 0.25%.\n  Code is available on Github: https://github.com/szandala/fair-deepfake-detection-toolbox",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2507.08912v1",
    "published_date": "2025-07-11 15:17:02 UTC",
    "updated_date": "2025-07-11 15:17:02 UTC"
  },
  {
    "arxiv_id": "2507.08665v1",
    "title": "KELPS: A Framework for Verified Multi-Language Autoformalization via Semantic-Syntactic Alignment",
    "authors": [
      "Jiyao Zhang",
      "Chengli Zhong",
      "Hui Xu",
      "Qige Li",
      "Yi Zhou"
    ],
    "abstract": "Modern large language models (LLMs) show promising progress in formalizing informal mathematics into machine-verifiable theorems. However, these methods still face bottlenecks due to the limited quantity and quality of multilingual parallel corpora. In this paper, we propose a novel neuro-symbolic framework KELPS (Knowledge-Equation based Logical Processing System) to address these problems. KELPS is an iterative framework for translating, synthesizing, and filtering informal data into multiple formal languages (Lean, Coq, and Isabelle). First, we translate natural language into Knowledge Equations (KEs), a novel language that we designed, theoretically grounded in assertional logic. Next, we convert them to target languages through rigorously defined rules that preserve both syntactic structure and semantic meaning. This process yielded a parallel corpus of over 60,000 problems. Our framework achieves 88.9% syntactic accuracy (pass@1) on MiniF2F, outperforming SOTA models such as Deepseek-V3 (81%) and Herald (81.3%) across multiple datasets. All datasets and codes are available in the supplementary materials.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted by the ICML 2025 AI4MATH Workshop. 22 pages, 16 figures, 2 tables",
    "pdf_url": "https://arxiv.org/pdf/2507.08665v1",
    "published_date": "2025-07-11 15:05:06 UTC",
    "updated_date": "2025-07-11 15:05:06 UTC"
  },
  {
    "arxiv_id": "2507.08664v1",
    "title": "Introspection of Thought Helps AI Agents",
    "authors": [
      "Haoran Sun",
      "Shaoning Zeng"
    ],
    "abstract": "AI Agents rely on Large Language Models (LLMs) and Multimodal-LLMs (MLLMs) to perform interpretation and inference in text and image tasks without post-training, where LLMs and MLLMs play the most critical role and determine the initial ability and limitations of AI Agents. Usually, AI Agents utilize sophisticated prompt engineering and external reasoning framework to obtain a promising interaction with LLMs, e.g., Chain-of-Thought, Iteration of Thought and Image-of-Thought. However, they are still constrained by the inherent limitations of LLM in understanding natural language, and the iterative reasoning process will generate a large amount of inference cost. To this end, we propose a novel AI Agent Reasoning Framework with Introspection of Thought (INoT) by designing a new LLM-Read code in prompt. It enables LLM to execute programmatic dialogue reasoning processes following the code in prompt. Therefore, self-denial and reflection occur within LLM instead of outside LLM, which can reduce token cost effectively. Through our experiments on six benchmarks for three different tasks, the effectiveness of INoT is verified, with an average improvement of 7.95\\% in performance, exceeding the baselines. Furthermore, the token cost of INoT is lower on average than the best performing method at baseline by 58.3\\%. In addition, we demonstrate the versatility of INoT in image interpretation and inference through verification experiments.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2507.08664v1",
    "published_date": "2025-07-11 15:03:17 UTC",
    "updated_date": "2025-07-11 15:03:17 UTC"
  },
  {
    "arxiv_id": "2507.08653v1",
    "title": "Safe Deep Reinforcement Learning for Resource Allocation with Peak Age of Information Violation Guarantees",
    "authors": [
      "Berire Gunes Reyhan",
      "Sinem Coleri"
    ],
    "abstract": "In Wireless Networked Control Systems (WNCSs), control and communication systems must be co-designed due to their strong interdependence. This paper presents a novel optimization theory-based safe deep reinforcement learning (DRL) framework for ultra-reliable WNCSs, ensuring constraint satisfaction while optimizing performance, for the first time in the literature. The approach minimizes power consumption under key constraints, including Peak Age of Information (PAoI) violation probability, transmit power, and schedulability in the finite blocklength regime. PAoI violation probability is uniquely derived by combining stochastic maximum allowable transfer interval (MATI) and maximum allowable packet delay (MAD) constraints in a multi-sensor network. The framework consists of two stages: optimization theory and safe DRL. The first stage derives optimality conditions to establish mathematical relationships among variables, simplifying and decomposing the problem. The second stage employs a safe DRL model where a teacher-student framework guides the DRL agent (student). The control mechanism (teacher) evaluates compliance with system constraints and suggests the nearest feasible action when needed. Extensive simulations show that the proposed framework outperforms rule-based and other optimization theory based DRL benchmarks, achieving faster convergence, higher rewards, and greater stability.",
    "categories": [
      "eess.SP",
      "cs.AI",
      "cs.LG",
      "cs.MA"
    ],
    "primary_category": "eess.SP",
    "comment": "15 Pages, to be published in IEEE Transactions on Communications",
    "pdf_url": "https://arxiv.org/pdf/2507.08653v1",
    "published_date": "2025-07-11 14:57:37 UTC",
    "updated_date": "2025-07-11 14:57:37 UTC"
  },
  {
    "arxiv_id": "2507.08649v1",
    "title": "Leanabell-Prover-V2: Verifier-integrated Reasoning for Formal Theorem Proving via Reinforcement Learning",
    "authors": [
      "Xingguang Ji",
      "Yahui Liu",
      "Qi Wang",
      "Jingyuan Zhang",
      "Yang Yue",
      "Rui Shi",
      "Chenxi Sun",
      "Fuzheng Zhang",
      "Guorui Zhou",
      "Kun Gai"
    ],
    "abstract": "We introduce our Leanabell-Prover-V2, a 7B large language models (LLMs) that can produce formal theorem proofs in Lean 4, with verifier-integrated Long Chain-of-Thoughts (CoT). Following our previous work Leanabell-Prover-V1, we continual to choose to posttrain existing strong prover models for further performance improvement. In our V2 version, we mainly upgrade the Reinforcement Learning (RL) with feedback provided by the Lean 4 verifier. Crucially, verifier feedback, such as indicating success or detailing specific errors, allows the LLM to become ``self-aware'' of the correctness of its own reasoning process and learn to reflexively correct errors. Leanabell-Prover-V2 directly optimizes LLM reasoning trajectories with multi-turn verifier interactions, together with feedback token masking for stable RL training and a simple reward strategy. Experiments show that Leanabell-Prover-V2 improves performance by 3.2% (pass@128) with Kimina-Prover-Preview-Distill-7B and 2.0% (pass@128) with DeepSeek-Prover-V2-7B on the MiniF2F test set. The source codes, curated data and models are available at: https://github.com/Leanabell-LM/Leanabell-Prover-V2.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "23 pages, 13 figures",
    "pdf_url": "https://arxiv.org/pdf/2507.08649v1",
    "published_date": "2025-07-11 14:53:14 UTC",
    "updated_date": "2025-07-11 14:53:14 UTC"
  },
  {
    "arxiv_id": "2507.08648v1",
    "title": "DatasetAgent: A Novel Multi-Agent System for Auto-Constructing Datasets from Real-World Images",
    "authors": [
      "Haoran Sun",
      "Haoyu Bian",
      "Shaoning Zeng",
      "Yunbo Rao",
      "Xu Xu",
      "Lin Mei",
      "Jianping Gou"
    ],
    "abstract": "Common knowledge indicates that the process of constructing image datasets usually depends on the time-intensive and inefficient method of manual collection and annotation. Large models offer a solution via data generation. Nonetheless, real-world data are obviously more valuable comparing to artificially intelligence generated data, particularly in constructing image datasets. For this reason, we propose a novel method for auto-constructing datasets from real-world images by a multiagent collaborative system, named as DatasetAgent. By coordinating four different agents equipped with Multi-modal Large Language Models (MLLMs), as well as a tool package for image optimization, DatasetAgent is able to construct high-quality image datasets according to user-specified requirements. In particular, two types of experiments are conducted, including expanding existing datasets and creating new ones from scratch, on a variety of open-source datasets. In both cases, multiple image datasets constructed by DatasetAgent are used to train various vision models for image classification, object detection, and image segmentation.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2507.08648v1",
    "published_date": "2025-07-11 14:51:33 UTC",
    "updated_date": "2025-07-11 14:51:33 UTC"
  },
  {
    "arxiv_id": "2507.08637v1",
    "title": "Scaling Attention to Very Long Sequences in Linear Time with Wavelet-Enhanced Random Spectral Attention (WERSA)",
    "authors": [
      "Vincenzo Dentamaro"
    ],
    "abstract": "Transformer models are computationally costly on long sequences since regular attention has quadratic $O(n^2)$ time complexity. We introduce Wavelet-Enhanced Random Spectral Attention (WERSA), a novel mechanism of linear $O(n)$ time complexity that is pivotal to enable successful long-sequence processing without the performance trade-off. WERSA merges content-adaptive random spectral features together with multi-resolution Haar wavelets and learnable parameters to selectively attend to informative scales of data while preserving linear efficiency.\n  Large-scale comparisons \\textbf{on single GPU} and across various benchmarks (vision, NLP, hierarchical reasoning) and various attention mechanisms (like Multiheaded Attention, Flash-Attention-2, FNet, Linformer, Performer, Waveformer), reveal uniform advantages of WERSA. It achieves best accuracy in all tests. On ArXiv classification, WERSA improves accuracy over vanilla attention by 1.2\\% (86.2\\% vs 85.0\\%) while cutting training time by 81\\% (296s vs 1554s) and FLOPS by 73.4\\% (26.2G vs 98.4G). Significantly, WERSA excels where vanilla and FlashAttention-2 fail: on ArXiv-128k's extremely lengthy sequences, it achieves best accuracy (79.1\\%) and AUC (0.979) among viable methods, operating on data that gives Out-Of-Memory errors to quadratic methods while being \\textbf{twice as fast} as Waveformer, its next-best competitor.\n  By significantly reducing computational loads without compromising accuracy, WERSA makes possible more practical, more affordable, long-context models, in particular on low-resource hardware, for more sustainable and more scalable AI development.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.LG",
    "comment": "10 pages, 1 figure",
    "pdf_url": "https://arxiv.org/pdf/2507.08637v1",
    "published_date": "2025-07-11 14:40:40 UTC",
    "updated_date": "2025-07-11 14:40:40 UTC"
  },
  {
    "arxiv_id": "2507.08636v1",
    "title": "Normalized vs Diplomatic Annotation: A Case Study of Automatic Information Extraction from Handwritten Uruguayan Birth Certificates",
    "authors": [
      "Natalia Bottaioli",
      "Solène Tarride",
      "Jérémy Anger",
      "Seginus Mowlavi",
      "Marina Gardella",
      "Antoine Tadros",
      "Gabriele Facciolo",
      "Rafael Grompone von Gioi",
      "Christopher Kermorvant",
      "Jean-Michel Morel",
      "Javier Preciozzi"
    ],
    "abstract": "This study evaluates the recently proposed Document Attention Network (DAN) for extracting key-value information from Uruguayan birth certificates, handwritten in Spanish. We investigate two annotation strategies for automatically transcribing handwritten documents, fine-tuning DAN with minimal training data and annotation effort. Experiments were conducted on two datasets containing the same images (201 scans of birth certificates written by more than 15 different writers) but with different annotation methods. Our findings indicate that normalized annotation is more effective for fields that can be standardized, such as dates and places of birth, whereas diplomatic annotation performs much better for fields containing names and surnames, which can not be standardized.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2507.08636v1",
    "published_date": "2025-07-11 14:40:07 UTC",
    "updated_date": "2025-07-11 14:40:07 UTC"
  },
  {
    "arxiv_id": "2507.08624v1",
    "title": "Adaptive Framework for Ambient Intelligence in Rehabilitation Assistance",
    "authors": [
      "Gábor Baranyi",
      "Zsolt Csibi",
      "Kristian Fenech",
      "Áron Fóthi",
      "Zsófia Gaál",
      "Joul Skaf",
      "András Lőrincz"
    ],
    "abstract": "This paper introduces the Ambient Intelligence Rehabilitation Support (AIRS) framework, an advanced artificial intelligence-based solution tailored for home rehabilitation environments. AIRS integrates cutting-edge technologies, including Real-Time 3D Reconstruction (RT-3DR), intelligent navigation, and large Vision-Language Models (VLMs), to create a comprehensive system for machine-guided physical rehabilitation. The general AIRS framework is demonstrated in rehabilitation scenarios following total knee replacement (TKR), utilizing a database of 263 video recordings for evaluation. A smartphone is employed within AIRS to perform RT-3DR of living spaces and has a body-matched avatar to provide visual feedback about the excercise. This avatar is necessary in (a) optimizing exercise configurations, including camera placement, patient positioning, and initial poses, and (b) addressing privacy concerns and promoting compliance with the AI Act. The system guides users through the recording process to ensure the collection of properly recorded videos. AIRS employs two feedback mechanisms: (i) visual 3D feedback, enabling direct comparisons between prerecorded clinical exercises and patient home recordings and (ii) VLM-generated feedback, providing detailed explanations and corrections for exercise errors. The framework also supports people with visual and hearing impairments. It also features a modular design that can be adapted to broader rehabilitation contexts. AIRS software components are available for further use and customization.",
    "categories": [
      "cs.HC",
      "cs.AI"
    ],
    "primary_category": "cs.HC",
    "comment": "The paper has been submitted to a journal and waiting for review",
    "pdf_url": "https://arxiv.org/pdf/2507.08624v1",
    "published_date": "2025-07-11 14:27:06 UTC",
    "updated_date": "2025-07-11 14:27:06 UTC"
  },
  {
    "arxiv_id": "2507.08621v2",
    "title": "A comprehensive study of LLM-based argument classification: from LLAMA through GPT-4o to Deepseek-R1",
    "authors": [
      "Marcin Pietroń",
      "Rafał Olszowski",
      "Jakub Gomułka",
      "Filip Gampel",
      "Andrzej Tomski"
    ],
    "abstract": "Argument mining (AM) is an interdisciplinary research field that integrates insights from logic, philosophy, linguistics, rhetoric, law, psychology, and computer science. It involves the automatic identification and extraction of argumentative components, such as premises and claims, and the detection of relationships between them, such as support, attack, or neutrality. Recently, the field has advanced significantly, especially with the advent of large language models (LLMs), which have enhanced the efficiency of analyzing and extracting argument semantics compared to traditional methods and other deep learning models. There are many benchmarks for testing and verifying the quality of LLM, but there is still a lack of research and results on the operation of these models in publicly available argument classification databases. This paper presents a study of a selection of LLM's, using diverse datasets such as Args.me and UKP. The models tested include versions of GPT, Llama, and DeepSeek, along with reasoning-enhanced variants incorporating the Chain-of-Thoughts algorithm. The results indicate that ChatGPT-4o outperforms the others in the argument classification benchmarks. In case of models incorporated with reasoning capabilities, the Deepseek-R1 shows its superiority. However, despite their superiority, GPT-4o and Deepseek-R1 still make errors. The most common errors are discussed for all models. To our knowledge, the presented work is the first broader analysis of the mentioned datasets using LLM and prompt algorithms. The work also shows some weaknesses of known prompt algorithms in argument analysis, while indicating directions for their improvement. The added value of the work is the in-depth analysis of the available argument datasets and the demonstration of their shortcomings.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2507.08621v2",
    "published_date": "2025-07-11 14:23:40 UTC",
    "updated_date": "2025-07-24 11:49:06 UTC"
  },
  {
    "arxiv_id": "2508.00850v1",
    "title": "Gearshift Fellowship: A Next-Generation Neurocomputational Game Platform to Model and Train Human-AI Adaptability",
    "authors": [
      "Nadja R. Ging-Jehli",
      "Russell K. Childers",
      "Joshua Lu",
      "Robert Gemma",
      "Rachel Zhu"
    ],
    "abstract": "How do we learn when to persist, when to let go, and when to shift gears? Gearshift Fellowship (GF) is the prototype of a new Supertask paradigm designed to model how humans and artificial agents adapt to shifting environment demands. Grounded in cognitive neuroscience, computational psychiatry, economics, and artificial intelligence, Supertasks combine computational neurocognitive modeling with serious gaming. This creates a dynamic, multi-mission environment engineered to assess mechanisms of adaptive behavior across cognitive and social contexts. Computational parameters explain behavior and probe mechanisms by controlling the game environment. Unlike traditional tasks, GF enables neurocognitive modeling of individual differences across perceptual decisions, learning, and meta-cognitive levels. This positions GF as a flexible testbed for understanding how cognitive-affective control processes, learning styles, strategy use, and motivational shifts adapt across contexts and over time. It serves as an experimental platform for scientists, a phenotype-to-mechanism intervention for clinicians, and a training tool for players aiming to strengthen self-regulated learning, mood, and stress resilience. Online study (n = 60, ongoing) results show that GF recovers effects from traditional neuropsychological tasks (construct validity), uncovers novel patterns in how learning differs across contexts and how clinical features map onto distinct adaptations. These findings pave the way for developing in-game interventions that foster self-efficacy and agency to cope with real-world stress and uncertainty. GF builds a new adaptive ecosystem designed to accelerate science, transform clinical care, and foster individual growth. It offers a mirror and training ground where humans and machines co-develop together deeper flexibility and awareness.",
    "categories": [
      "cs.HC",
      "cs.AI",
      "cs.CE",
      "cs.CY"
    ],
    "primary_category": "cs.HC",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2508.00850v1",
    "published_date": "2025-07-11 14:20:13 UTC",
    "updated_date": "2025-07-11 14:20:13 UTC"
  },
  {
    "arxiv_id": "2507.08619v2",
    "title": "Agentic Large Language Models for Conceptual Systems Engineering and Design",
    "authors": [
      "Soheyl Massoudi",
      "Mark Fuge"
    ],
    "abstract": "Early-stage engineering design involves complex, iterative reasoning, yet existing large language model (LLM) workflows struggle to maintain task continuity and generate executable models. We evaluate whether a structured multi-agent system (MAS) can more effectively manage requirements extraction, functional decomposition, and simulator code generation than a simpler two-agent system (2AS). The target application is a solar-powered water filtration system as described in a cahier des charges. We introduce the Design-State Graph (DSG), a JSON-serializable representation that bundles requirements, physical embodiments, and Python-based physics models into graph nodes. A nine-role MAS iteratively builds and refines the DSG, while the 2AS collapses the process to a Generator-Reflector loop. Both systems run a total of 60 experiments (2 LLMs - Llama 3.3 70B vs reasoning-distilled DeepSeek R1 70B x 2 agent configurations x 3 temperatures x 5 seeds). We report a JSON validity, requirement coverage, embodiment presence, code compatibility, workflow completion, runtime, and graph size. Across all runs, both MAS and 2AS maintained perfect JSON integrity and embodiment tagging. Requirement coverage remained minimal (less than 20%). Code compatibility peaked at 100% under specific 2AS settings but averaged below 50% for MAS. Only the reasoning-distilled model reliably flagged workflow completion. Powered by DeepSeek R1 70B, the MAS generated more granular DSGs (average 5-6 nodes) whereas 2AS mode-collapsed. Structured multi-agent orchestration enhanced design detail. Reasoning-distilled LLM improved completion rates, yet low requirements and fidelity gaps in coding persisted.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "32 pages, 4 figures",
    "pdf_url": "https://arxiv.org/pdf/2507.08619v2",
    "published_date": "2025-07-11 14:19:05 UTC",
    "updated_date": "2025-11-02 17:15:29 UTC"
  },
  {
    "arxiv_id": "2507.08617v1",
    "title": "Towards Collaborative Fairness in Federated Learning Under Imbalanced Covariate Shift",
    "authors": [
      "Tianrun Yu",
      "Jiaqi Wang",
      "Haoyu Wang",
      "Mingquan Lin",
      "Han Liu",
      "Nelson S. Yee",
      "Fenglong Ma"
    ],
    "abstract": "Collaborative fairness is a crucial challenge in federated learning. However, existing approaches often overlook a practical yet complex form of heterogeneity: imbalanced covariate shift. We provide a theoretical analysis of this setting, which motivates the design of FedAKD (Federated Asynchronous Knowledge Distillation)- simple yet effective approach that balances accurate prediction with collaborative fairness. FedAKD consists of client and server updates. In the client update, we introduce a novel asynchronous knowledge distillation strategy based on our preliminary analysis, which reveals that while correctly predicted samples exhibit similar feature distributions across clients, incorrectly predicted samples show significant variability. This suggests that imbalanced covariate shift primarily arises from misclassified samples. Leveraging this insight, our approach first applies traditional knowledge distillation to update client models while keeping the global model fixed. Next, we select correctly predicted high-confidence samples and update the global model using these samples while keeping client models fixed. The server update simply aggregates all client models. We further provide a theoretical proof of FedAKD's convergence. Experimental results on public datasets (FashionMNIST and CIFAR10) and a real-world Electronic Health Records (EHR) dataset demonstrate that FedAKD significantly improves collaborative fairness, enhances predictive accuracy, and fosters client participation even under highly heterogeneous data distributions.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "18 pages, accepted to the 31st ACM SIGKDD Conference on Knowledge Discovery and Data Mining (KDD' 25), Toronto, Canada, August 3-7 2025",
    "pdf_url": "https://arxiv.org/pdf/2507.08617v1",
    "published_date": "2025-07-11 14:13:41 UTC",
    "updated_date": "2025-07-11 14:13:41 UTC"
  },
  {
    "arxiv_id": "2507.10587v1",
    "title": "Anthropomimetic Uncertainty: What Verbalized Uncertainty in Language Models is Missing",
    "authors": [
      "Dennis Ulmer",
      "Alexandra Lorson",
      "Ivan Titov",
      "Christian Hardmeier"
    ],
    "abstract": "Human users increasingly rely on natural language interactions with large language models (LLMs) in order to receive help on a large variety of tasks and problems. However, the trustworthiness and perceived legitimacy of LLMs is undermined by the fact that their output is frequently stated in very confident terms, even when its accuracy is questionable. Therefore, there is a need to signal the confidence of the language model to a user in order to reap the benefits of human-machine collaboration and mitigate potential harms. Verbalized uncertainty is the expression of confidence with linguistic means, an approach that integrates perfectly into language-based interfaces. Nevertheless, most recent research in natural language processing (NLP) overlooks the nuances surrounding human uncertainty communication and the data biases that influence machine uncertainty communication. We argue for anthropomimetic uncertainty, meaning that intuitive and trustworthy uncertainty communication requires a degree of linguistic authenticity and personalization to the user, which could be achieved by emulating human communication. We present a thorough overview over the research in human uncertainty communication, survey ongoing research, and perform additional analyses to demonstrate so-far overlooked biases in verbalized uncertainty. We conclude by pointing out unique factors in human-machine communication of uncertainty and deconstruct anthropomimetic uncertainty into future research directions for NLP.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2507.10587v1",
    "published_date": "2025-07-11 14:07:22 UTC",
    "updated_date": "2025-07-11 14:07:22 UTC"
  },
  {
    "arxiv_id": "2507.10586v1",
    "title": "AutoRAG-LoRA: Hallucination-Triggered Knowledge Retuning via Lightweight Adapters",
    "authors": [
      "Kaushik Dwivedi",
      "Padmanabh Patanjali Mishra"
    ],
    "abstract": "Large Language Models (LLMs) have demonstrated remarkable fluency across a range of natural language tasks, yet remain vulnerable to hallucinations - factual inaccuracies that undermine trust in real world deployment. We present AutoRAG-LoRA, a modular framework for Retrieval-Augmented Generation (RAG) that tackles hallucination in large language models through lightweight LoRA-based adapters and KL-regularized training. Our pipeline integrates automated prompt rewriting, hybrid retrieval, and low-rank adapter tuning to ground responses in retrieved evidence. A hallucination detection module, using both classifier-based and self-evaluation techniques, assigns confidence scores to generated outputs, triggering an optional feedback correction loop. This loop enforces factual alignment via contrastive KL loss and adapter fine tuning. We demonstrate that AutoRAG-LoRA significantly reduces the factual drift while preserving the efficiency and modularity of the model.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2507.10586v1",
    "published_date": "2025-07-11 14:02:58 UTC",
    "updated_date": "2025-07-11 14:02:58 UTC"
  },
  {
    "arxiv_id": "2507.08603v1",
    "title": "Unlocking Speech Instruction Data Potential with Query Rewriting",
    "authors": [
      "Yonghua Hei",
      "Yibo Yan",
      "Shuliang Liu",
      "Huiyu Zhou",
      "Linfeng Zhang",
      "Xuming Hu"
    ],
    "abstract": "End-to-end Large Speech Language Models~(\\textbf{LSLMs}) demonstrate strong potential in response latency and speech comprehension capabilities, showcasing general intelligence across speech understanding tasks. However, the ability to follow speech instructions has not been fully realized due to the lack of datasets and heavily biased training tasks. Leveraging the rich ASR datasets, previous approaches have used Large Language Models~(\\textbf{LLMs}) to continue the linguistic information of speech to construct speech instruction datasets. Yet, due to the gap between LLM-generated results and real human responses, the continuation methods further amplify these shortcomings. Given the high costs of collecting and annotating speech instruction datasets by humans, using speech synthesis to construct large-scale speech instruction datasets has become a balanced and robust alternative. Although modern Text-To-Speech~(\\textbf{TTS}) models have achieved near-human-level synthesis quality, it is challenging to appropriately convert out-of-distribution text instruction to speech due to the limitations of the training data distribution in TTS models. To address this issue, we propose a query rewriting framework with multi-LLM knowledge fusion, employing multiple agents to annotate and validate the synthesized speech, making it possible to construct high-quality speech instruction datasets without relying on human annotation. Experiments show that this method can transform text instructions into distributions more suitable for TTS models for speech synthesis through zero-shot rewriting, increasing data usability from 72\\% to 93\\%. It also demonstrates unique advantages in rewriting tasks that require complex knowledge and context-related abilities.",
    "categories": [
      "cs.AI",
      "cs.SD",
      "eess.AS"
    ],
    "primary_category": "cs.AI",
    "comment": "ACL 2025 Findings",
    "pdf_url": "https://arxiv.org/pdf/2507.08603v1",
    "published_date": "2025-07-11 13:55:45 UTC",
    "updated_date": "2025-07-11 13:55:45 UTC"
  },
  {
    "arxiv_id": "2507.08594v1",
    "title": "Generating Proto-Personas through Prompt Engineering: A Case Study on Efficiency, Effectiveness and Empathy",
    "authors": [
      "Fernando Ayach",
      "Vitor Lameirão",
      "Raul Leão",
      "Jerfferson Felizardo",
      "Rafael Sobrinho",
      "Vanessa Borges",
      "Patrícia Matsubara",
      "Awdren Fontão"
    ],
    "abstract": "Proto-personas are commonly used during early-stage Product Discovery, such as Lean Inception, to guide product definition and stakeholder alignment. However, the manual creation of proto-personas is often time-consuming, cognitively demanding, and prone to bias. In this paper, we propose and empirically investigate a prompt engineering-based approach to generate proto-personas with the support of Generative AI (GenAI). Our goal is to evaluate the approach in terms of efficiency, effectiveness, user acceptance, and the empathy elicited by the generated personas. We conducted a case study with 19 participants embedded in a real Lean Inception, employing a qualitative and quantitative methods design. The results reveal the approach's efficiency by reducing time and effort and improving the quality and reusability of personas in later discovery phases, such as Minimum Viable Product (MVP) scoping and feature refinement. While acceptance was generally high, especially regarding perceived usefulness and ease of use, participants noted limitations related to generalization and domain specificity. Furthermore, although cognitive empathy was strongly supported, affective and behavioral empathy varied significantly across participants. These results contribute novel empirical evidence on how GenAI can be effectively integrated into software Product Discovery practices, while also identifying key challenges to be addressed in future iterations of such hybrid design processes.",
    "categories": [
      "cs.SE",
      "cs.AI",
      "cs.HC"
    ],
    "primary_category": "cs.SE",
    "comment": "12 pages; 2 figures; Preprint with the original submission accepted for publication at 39th Brazilian Symposium on Software Engineering (SBES)",
    "pdf_url": "https://arxiv.org/pdf/2507.08594v1",
    "published_date": "2025-07-11 13:42:12 UTC",
    "updated_date": "2025-07-11 13:42:12 UTC"
  },
  {
    "arxiv_id": "2507.14178v1",
    "title": "Feature Bank Enhancement for Distance-based Out-of-Distribution Detection",
    "authors": [
      "Yuhang Liu",
      "Yuefei Wu",
      "Bin Shi",
      "Bo Dong"
    ],
    "abstract": "Out-of-distribution (OOD) detection is critical to ensuring the reliability of deep learning applications and has attracted significant attention in recent years. A rich body of literature has emerged to develop efficient score functions that assign high scores to in-distribution (ID) samples and low scores to OOD samples, thereby helping distinguish OOD samples. Among these methods, distance-based score functions are widely used because of their efficiency and ease of use. However, deep learning often leads to a biased distribution of data features, and extreme features are inevitable. These extreme features make the distance-based methods tend to assign too low scores to ID samples. This limits the OOD detection capabilities of such methods. To address this issue, we propose a simple yet effective method, Feature Bank Enhancement (FBE), that uses statistical characteristics from dataset to identify and constrain extreme features to the separation boundaries, therapy making the distance between samples inside and outside the distribution farther. We conducted experiments on large-scale ImageNet-1k and CIFAR-10 respectively, and the results show that our method achieves state-of-the-art performance on both benchmark. Additionally, theoretical analysis and supplementary experiments are conducted to provide more insights into our method.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "8 pages, 5 figures",
    "pdf_url": "https://arxiv.org/pdf/2507.14178v1",
    "published_date": "2025-07-11 13:32:26 UTC",
    "updated_date": "2025-07-11 13:32:26 UTC"
  },
  {
    "arxiv_id": "2507.08584v1",
    "title": "To Trade or Not to Trade: An Agentic Approach to Estimating Market Risk Improves Trading Decisions",
    "authors": [
      "Dimitrios Emmanoulopoulos",
      "Ollie Olby",
      "Justin Lyon",
      "Namid R. Stillman"
    ],
    "abstract": "Large language models (LLMs) are increasingly deployed in agentic frameworks, in which prompts trigger complex tool-based analysis in pursuit of a goal. While these frameworks have shown promise across multiple domains including in finance, they typically lack a principled model-building step, relying instead on sentiment- or trend-based analysis. We address this gap by developing an agentic system that uses LLMs to iteratively discover stochastic differential equations for financial time series. These models generate risk metrics which inform daily trading decisions. We evaluate our system in both traditional backtests and using a market simulator, which introduces synthetic but causally plausible price paths and news events. We find that model-informed trading strategies outperform standard LLM-based agents, improving Sharpe ratios across multiple equities. Our results show that combining LLMs with agentic model discovery enhances market risk estimation and enables more profitable trading decisions.",
    "categories": [
      "q-fin.ST",
      "cs.AI",
      "cs.CE",
      "cs.MA",
      "q-fin.CP"
    ],
    "primary_category": "q-fin.ST",
    "comment": "31 pages, 7 figures, 3 tables",
    "pdf_url": "https://arxiv.org/pdf/2507.08584v1",
    "published_date": "2025-07-11 13:29:32 UTC",
    "updated_date": "2025-07-11 13:29:32 UTC"
  },
  {
    "arxiv_id": "2507.08575v1",
    "title": "Large Multi-modal Model Cartographic Map Comprehension for Textual Locality Georeferencing",
    "authors": [
      "Kalana Wijegunarathna",
      "Kristin Stock",
      "Christopher B. Jones"
    ],
    "abstract": "Millions of biological sample records collected in the last few centuries archived in natural history collections are un-georeferenced. Georeferencing complex locality descriptions associated with these collection samples is a highly labour-intensive task collection agencies struggle with. None of the existing automated methods exploit maps that are an essential tool for georeferencing complex relations. We present preliminary experiments and results of a novel method that exploits multi-modal capabilities of recent Large Multi-Modal Models (LMM). This method enables the model to visually contextualize spatial relations it reads in the locality description. We use a grid-based approach to adapt these auto-regressive models for this task in a zero-shot setting. Our experiments conducted on a small manually annotated dataset show impressive results for our approach ($\\sim$1 km Average distance error) compared to uni-modal georeferencing with Large Language Models and existing georeferencing tools. The paper also discusses the findings of the experiments in light of an LMM's ability to comprehend fine-grained maps. Motivated by these results, a practical framework is proposed to integrate this method into a georeferencing workflow.",
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.CV"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2507.08575v1",
    "published_date": "2025-07-11 13:23:25 UTC",
    "updated_date": "2025-07-11 13:23:25 UTC"
  },
  {
    "arxiv_id": "2507.08574v2",
    "title": "Knowledge-Guided Brain Tumor Segmentation via Synchronized Visual-Semantic-Topological Prior Fusion",
    "authors": [
      "Mingda Zhang",
      "Kaiwen Pan"
    ],
    "abstract": "Background: Brain tumor segmentation requires precise delineation of hierarchical structures from multi-sequence MRI. However, existing deep learning methods primarily rely on visual features, showing insufficient discriminative power in ambiguous boundary regions. Moreover, they lack explicit integration of medical domain knowledge such as anatomical semantics and geometric topology. Methods: We propose a knowledge-guided framework, Synchronized Tri-modal Prior Fusion (STPF), that explicitly integrates three heterogeneous knowledge priors: pathology-driven differential features (T1ce-T1, T2-FLAIR, T1/T2) encoding contrast patterns; unsupervised semantic descriptions transformed into voxel-level guidance via spatialization operators; and geometric constraints extracted through persistent homology analysis. A dual-level fusion architecture dynamically allocates prior weights at the voxel level based on confidence and at the sample level through hypernetwork-generated conditional vectors. Furthermore, nested output heads structurally ensure the hierarchical constraint ET subset TC subset WT. Results: STPF achieves a mean Dice coefficient of 0.868 on the BraTS 2020 dataset, surpassing the best baseline by 2.6 percentage points (3.09% relative improvement). Notably, five-fold cross-validation yields coefficients of variation between 0.23% and 0.33%, demonstrating stable performance. Additionally, ablation experiments show that removing topological and semantic priors leads to performance degradation of 2.8% and 3.5%, respectively. Conclusions: By explicitly integrating medical knowledge priors - anatomical semantics and geometric constraints - STPF improves segmentation accuracy in ambiguous boundary regions while demonstrating generalization capability and clinical deployment potential.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "37 pages, 6 figures",
    "pdf_url": "https://arxiv.org/pdf/2507.08574v2",
    "published_date": "2025-07-11 13:21:56 UTC",
    "updated_date": "2025-11-12 13:35:17 UTC"
  },
  {
    "arxiv_id": "2507.08557v2",
    "title": "FreeAudio: Training-Free Timing Planning for Controllable Long-Form Text-to-Audio Generation",
    "authors": [
      "Yuxuan Jiang",
      "Zehua Chen",
      "Zeqian Ju",
      "Chang Li",
      "Weibei Dou",
      "Jun Zhu"
    ],
    "abstract": "Text-to-audio (T2A) generation has achieved promising results with the recent advances in generative models. However, because of the limited quality and quantity of temporally-aligned audio-text pairs, existing T2A methods struggle to handle the complex text prompts that contain precise timing control, e.g., \"owl hooted at 2.4s-5.2s\". Recent works have explored data augmentation techniques or introduced timing conditions as model inputs to enable timing-conditioned 10-second T2A generation, while their synthesis quality is still limited. In this work, we propose a novel training-free timing-controlled T2A framework, FreeAudio, making the first attempt to enable timing-controlled long-form T2A generation, e.g., \"owl hooted at 2.4s-5.2s and crickets chirping at 0s-24s\". Specifically, we first employ an LLM to plan non-overlapping time windows and recaption each with a refined natural language description, based on the input text and timing prompts. Then we introduce: 1) Decoupling and Aggregating Attention Control for precise timing control; 2) Contextual Latent Composition for local smoothness and Reference Guidance for global consistency. Extensive experiments show that: 1) FreeAudio achieves state-of-the-art timing-conditioned T2A synthesis quality among training-free methods and is comparable to leading training-based methods; 2) FreeAudio demonstrates comparable long-form generation quality with training-based Stable Audio and paves the way for timing-controlled long-form T2A synthesis. Demo samples are available at: https://freeaudio.github.io/FreeAudio/",
    "categories": [
      "cs.SD",
      "cs.AI",
      "cs.MM",
      "eess.AS"
    ],
    "primary_category": "cs.SD",
    "comment": "Accepted at ACM MM 2025",
    "pdf_url": "https://arxiv.org/pdf/2507.08557v2",
    "published_date": "2025-07-11 12:57:51 UTC",
    "updated_date": "2025-09-18 02:19:36 UTC"
  },
  {
    "arxiv_id": "2507.10585v1",
    "title": "A Taxonomy for Design and Evaluation of Prompt-Based Natural Language Explanations",
    "authors": [
      "Isar Nejadgholi",
      "Mona Omidyeganeh",
      "Marc-Antoine Drouin",
      "Jonathan Boisvert"
    ],
    "abstract": "Effective AI governance requires structured approaches for stakeholders to access and verify AI system behavior. With the rise of large language models, Natural Language Explanations (NLEs) are now key to articulating model behavior, which necessitates a focused examination of their characteristics and governance implications. We draw on Explainable AI (XAI) literature to create an updated XAI taxonomy, adapted to prompt-based NLEs, across three dimensions: (1) Context, including task, data, audience, and goals; (2) Generation and Presentation, covering generation methods, inputs, interactivity, outputs, and forms; and (3) Evaluation, focusing on content, presentation, and user-centered properties, as well as the setting of the evaluation. This taxonomy provides a framework for researchers, auditors, and policymakers to characterize, design, and enhance NLEs for transparent AI systems.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Presented at the Workshop of Technical AI Governance, 5 pages 2 figures",
    "pdf_url": "https://arxiv.org/pdf/2507.10585v1",
    "published_date": "2025-07-11 12:52:19 UTC",
    "updated_date": "2025-07-11 12:52:19 UTC"
  },
  {
    "arxiv_id": "2507.08546v1",
    "title": "RadiomicsRetrieval: A Customizable Framework for Medical Image Retrieval Using Radiomics Features",
    "authors": [
      "Inye Na",
      "Nejung Rue",
      "Jiwon Chung",
      "Hyunjin Park"
    ],
    "abstract": "Medical image retrieval is a valuable field for supporting clinical decision-making, yet current methods primarily support 2D images and require fully annotated queries, limiting clinical flexibility. To address this, we propose RadiomicsRetrieval, a 3D content-based retrieval framework bridging handcrafted radiomics descriptors with deep learning-based embeddings at the tumor level. Unlike existing 2D approaches, RadiomicsRetrieval fully exploits volumetric data to leverage richer spatial context in medical images. We employ a promptable segmentation model (e.g., SAM) to derive tumor-specific image embeddings, which are aligned with radiomics features extracted from the same tumor via contrastive learning. These representations are further enriched by anatomical positional embedding (APE). As a result, RadiomicsRetrieval enables flexible querying based on shape, location, or partial feature sets. Extensive experiments on both lung CT and brain MRI public datasets demonstrate that radiomics features significantly enhance retrieval specificity, while APE provides global anatomical context essential for location-based searches. Notably, our framework requires only minimal user prompts (e.g., a single point), minimizing segmentation overhead and supporting diverse clinical scenarios. The capability to query using either image embeddings or selected radiomics attributes highlights its adaptability, potentially benefiting diagnosis, treatment planning, and research on large-scale medical imaging repositories. Our code is available at https://github.com/nainye/RadiomicsRetrieval.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "Accepted at MICCAI 2025",
    "pdf_url": "https://arxiv.org/pdf/2507.08546v1",
    "published_date": "2025-07-11 12:48:25 UTC",
    "updated_date": "2025-07-11 12:48:25 UTC"
  },
  {
    "arxiv_id": "2507.08540v4",
    "title": "White-Basilisk: A Hybrid Model for Code Vulnerability Detection",
    "authors": [
      "Ioannis Lamprou",
      "Alexander Shevtsov",
      "Ioannis Arapakis",
      "Sotiris Ioannidis"
    ],
    "abstract": "The proliferation of software vulnerabilities presents a significant challenge to cybersecurity, necessitating more effective detection methodologies. We introduce White-Basilisk, a novel approach to vulnerability detection that demonstrates superior performance while challenging prevailing assumptions in AI model scaling. Utilizing an innovative architecture that integrates Mamba layers, linear self-attention, and a Mixture of Experts framework, White-Basilisk achieves state-of-the-art results in vulnerability detection tasks with a parameter count of only 200M. The model's capacity to process sequences of unprecedented length enables comprehensive analysis of extensive codebases in a single pass, surpassing the context limitations of current Large Language Models (LLMs). White-Basilisk exhibits robust performance on imbalanced, real-world datasets, while maintaining computational efficiency that facilitates deployment across diverse organizational scales. This research not only establishes new benchmarks in code security but also provides empirical evidence that compact, efficiently designed models can outperform larger counterparts in specialized tasks, potentially redefining optimization strategies in AI development for domain-specific applications.",
    "categories": [
      "cs.CR",
      "cs.AI"
    ],
    "primary_category": "cs.CR",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2507.08540v4",
    "published_date": "2025-07-11 12:39:25 UTC",
    "updated_date": "2025-11-09 15:19:55 UTC"
  },
  {
    "arxiv_id": "2507.10584v2",
    "title": "ARPaCCino: An Agentic-RAG for Policy as Code Compliance",
    "authors": [
      "Francesco Romeo",
      "Luigi Arena",
      "Francesco Blefari",
      "Francesco Aurelio Pironti",
      "Matteo Lupinacci",
      "Angelo Furfaro"
    ],
    "abstract": "Policy as Code (PaC) is a paradigm that encodes security and compliance policies into machine-readable formats, enabling automated enforcement in Infrastructure as Code (IaC) environments. However, its adoption is hindered by the complexity of policy languages and the risk of misconfigurations. In this work, we present ARPaCCino, an agentic system that combines Large Language Models (LLMs), Retrieval-Augmented-Generation (RAG), and tool-based validation to automate the generation and verification of PaC rules. Given natural language descriptions of the desired policies, ARPaCCino generates formal Rego rules, assesses IaC compliance, and iteratively refines the IaC configurations to ensure conformance. Thanks to its modular agentic architecture and integration with external tools and knowledge bases, ARPaCCino supports policy validation across a wide range of technologies, including niche or emerging IaC frameworks. Experimental evaluation involving a Terraform-based case study demonstrates ARPaCCino's effectiveness in generating syntactically and semantically correct policies, identifying non-compliant infrastructures, and applying corrective modifications, even when using smaller, open-weight LLMs. Our results highlight the potential of agentic RAG architectures to enhance the automation, reliability, and accessibility of PaC workflows.",
    "categories": [
      "cs.SE",
      "cs.AI"
    ],
    "primary_category": "cs.SE",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2507.10584v2",
    "published_date": "2025-07-11 12:36:33 UTC",
    "updated_date": "2025-11-04 10:14:39 UTC"
  },
  {
    "arxiv_id": "2507.08530v1",
    "title": "MIDI-VALLE: Improving Expressive Piano Performance Synthesis Through Neural Codec Language Modelling",
    "authors": [
      "Jingjing Tang",
      "Xin Wang",
      "Zhe Zhang",
      "Junichi Yamagishi",
      "Geraint Wiggins",
      "George Fazekas"
    ],
    "abstract": "Generating expressive audio performances from music scores requires models to capture both instrument acoustics and human interpretation. Traditional music performance synthesis pipelines follow a two-stage approach, first generating expressive performance MIDI from a score, then synthesising the MIDI into audio. However, the synthesis models often struggle to generalise across diverse MIDI sources, musical styles, and recording environments. To address these challenges, we propose MIDI-VALLE, a neural codec language model adapted from the VALLE framework, which was originally designed for zero-shot personalised text-to-speech (TTS) synthesis. For performance MIDI-to-audio synthesis, we improve the architecture to condition on a reference audio performance and its corresponding MIDI. Unlike previous TTS-based systems that rely on piano rolls, MIDI-VALLE encodes both MIDI and audio as discrete tokens, facilitating a more consistent and robust modelling of piano performances. Furthermore, the model's generalisation ability is enhanced by training on an extensive and diverse piano performance dataset. Evaluation results show that MIDI-VALLE significantly outperforms a state-of-the-art baseline, achieving over 75% lower Frechet Audio Distance on the ATEPP and Maestro datasets. In the listening test, MIDI-VALLE received 202 votes compared to 58 for the baseline, demonstrating improved synthesis quality and generalisation across diverse performance MIDI inputs.",
    "categories": [
      "cs.SD",
      "cs.AI",
      "eess.AS"
    ],
    "primary_category": "cs.SD",
    "comment": "Accepted by ISMIR 2025",
    "pdf_url": "https://arxiv.org/pdf/2507.08530v1",
    "published_date": "2025-07-11 12:28:20 UTC",
    "updated_date": "2025-07-11 12:28:20 UTC"
  },
  {
    "arxiv_id": "2507.08529v3",
    "title": "A Multi-granularity Concept Sparse Activation and Hierarchical Knowledge Graph Fusion Framework for Rare Disease Diagnosis",
    "authors": [
      "Mingda Zhang",
      "Na Zhao",
      "Jianglong Qin",
      "Guoyu Ye",
      "Ruixiang Tang"
    ],
    "abstract": "Despite advances from medical large language models in healthcare, rare-disease diagnosis remains hampered by insufficient knowledge-representation depth, limited concept understanding, and constrained clinical reasoning. We propose a framework that couples multi-granularity sparse activation of medical concepts with a hierarchical knowledge graph. Four complementary matching algorithms, diversity control, and a five-level fallback strategy enable precise concept activation, while a three-layer knowledge graph (taxonomy, clinical features, instances) provides structured, up-to-date context. Experiments on the BioASQ rare-disease QA set show BLEU gains of 0.09, ROUGE gains of 0.05, and accuracy gains of 0.12, with peak accuracy of 0.89 approaching the 0.90 clinical threshold. Expert evaluation confirms improvements in information quality, reasoning, and professional expression, suggesting our approach shortens the \"diagnostic odyssey\" for rare-disease patients.",
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.AI",
    "comment": "12 pages,3 figures",
    "pdf_url": "https://arxiv.org/pdf/2507.08529v3",
    "published_date": "2025-07-11 12:26:19 UTC",
    "updated_date": "2025-08-30 08:57:36 UTC"
  },
  {
    "arxiv_id": "2507.10583v3",
    "title": "$\\texttt{Droid}$: A Resource Suite for AI-Generated Code Detection",
    "authors": [
      "Daniil Orel",
      "Indraneil Paul",
      "Iryna Gurevych",
      "Preslav Nakov"
    ],
    "abstract": "In this work, we compile $\\textbf{$\\texttt{DroidCollection}$}$, the most extensive open data suite for training and evaluating machine-generated code detectors, comprising over a million code samples, seven programming languages, outputs from 43 coding models, and over three real-world coding domains. Alongside fully AI-generated samples, our collection includes human-AI co-authored code, as well as adversarial samples explicitly crafted to evade detection. Subsequently, we develop $\\textbf{$\\texttt{DroidDetect}$}$, a suite of encoder-only detectors trained using a multi-task objective over $\\texttt{DroidCollection}$. Our experiments show that existing detectors' performance fails to generalise to diverse coding domains and programming languages outside of their narrow training data. Additionally, we demonstrate that while most detectors are easily compromised by humanising the output distributions using superficial prompting and alignment approaches, this problem can be easily amended by training on a small amount of adversarial data. Finally, we demonstrate the effectiveness of metric learning and uncertainty-based resampling as means to enhance detector training on possibly noisy distributions.",
    "categories": [
      "cs.SE",
      "cs.AI",
      "cs.CY"
    ],
    "primary_category": "cs.SE",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2507.10583v3",
    "published_date": "2025-07-11 12:19:06 UTC",
    "updated_date": "2025-08-06 19:26:28 UTC"
  },
  {
    "arxiv_id": "2507.10581v1",
    "title": "Universal Approximation Theorem for a Single-Layer Transformer",
    "authors": [
      "Esmail Gumaan"
    ],
    "abstract": "Deep learning employs multi-layer neural networks trained via the backpropagation algorithm. This approach has achieved success across many domains and relies on adaptive gradient methods such as the Adam optimizer. Sequence modeling evolved from recurrent neural networks to attention-based models, culminating in the Transformer architecture. Transformers have achieved state-of-the-art performance in natural language processing (for example, BERT and GPT-3) and have been applied in computer vision and computational biology. However, theoretical understanding of these models remains limited. In this paper, we examine the mathematical foundations of deep learning and Transformers and present a novel theoretical result. We review key concepts from linear algebra, probability, and optimization that underpin deep learning, and we analyze the multi-head self-attention mechanism and the backpropagation algorithm in detail. Our main contribution is a universal approximation theorem for Transformers: we prove that a single-layer Transformer, comprising one self-attention layer followed by a position-wise feed-forward network with ReLU activation, can approximate any continuous sequence-to-sequence mapping on a compact domain to arbitrary precision. We provide a formal statement and a complete proof. Finally, we present case studies that demonstrate the practical implications of this result. Our findings advance the theoretical understanding of Transformer models and help bridge the gap between theory and practice.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "7 pages, 2 figures, 1 theorem, 10 formulas",
    "pdf_url": "https://arxiv.org/pdf/2507.10581v1",
    "published_date": "2025-07-11 11:37:39 UTC",
    "updated_date": "2025-07-11 11:37:39 UTC"
  },
  {
    "arxiv_id": "2507.08501v1",
    "title": "From Language to Logic: A Bi-Level Framework for Structured Reasoning",
    "authors": [
      "Keying Yang",
      "Hao Wang",
      "Kai Yang"
    ],
    "abstract": "Structured reasoning over natural language inputs remains a core challenge in artificial intelligence, as it requires bridging the gap between unstructured linguistic expressions and formal logical representations. In this paper, we propose a novel \\textbf{bi-level framework} that maps language to logic through a two-stage process: high-level task abstraction and low-level logic generation. At the upper level, a large language model (LLM) parses natural language queries into intermediate structured representations specifying the problem type, objectives, decision variables, and symbolic constraints. At the lower level, the LLM uses these representations to generate symbolic workflows or executable reasoning programs for accurate and interpretable decision making. The framework supports modular reasoning, enforces explicit constraints, and generalizes across domains such as mathematical problem solving, question answering, and logical inference. We further optimize the framework with an end-to-end {bi-level} optimization approach that jointly refines both the high-level abstraction and low-level logic generation stages. Experiments on multiple realistic reasoning benchmarks demonstrate that our approach significantly outperforms existing baselines in accuracy, with accuracy gains reaching as high as 40\\%. Moreover, the bi-level design enhances transparency and error traceability, offering a promising step toward trustworthy and systematic reasoning with LLMs.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2507.08501v1",
    "published_date": "2025-07-11 11:24:09 UTC",
    "updated_date": "2025-07-11 11:24:09 UTC"
  },
  {
    "arxiv_id": "2507.10580v2",
    "title": "An Offline Mobile Conversational Agent for Mental Health Support: Learning from Emotional Dialogues and Psychological Texts with Student-Centered Evaluation",
    "authors": [
      "Vimaleswar A",
      "Prabhu Nandan Sahu",
      "Nilesh Kumar Sahu",
      "Haroon R. Lone"
    ],
    "abstract": "Mental health plays a crucial role in the overall well-being of an individual. In recent years, digital platforms have increasingly been used to expand mental health and emotional support. However, there are persistent challenges related to limited user accessibility, internet connectivity, and data privacy, which highlight the need for an offline, smartphone-based solutions. To address these challenges, we propose EmoSApp (Emotional Support App): an entirely offline, smartphone-based conversational app designed to provide mental health and emotional support. EmoSApp leverages a language model, specifically the LLaMA-3.2-1B-Instruct, which is fine-tuned and quantized on a custom-curated ``Knowledge Dataset'' comprising 14,582 mental health QA pairs along with multi-turn conversational data, enabling robust domain expertise and fully on-device inference on resource-constrained smartphones.\n  Through qualitative evaluation with students and mental health professionals, we demonstrate that EmoSApp has the ability to respond coherently and empathetically, provide relevant suggestions to user's mental health problems, and maintain interactive dialogue. Additionally, quantitative evaluations on nine commonsense and reasoning benchmarks, along with two mental health specific datasets, demonstrate EmoSApp's effectiveness in low-resource settings. By prioritizing on-device deployment and specialized domain-specific adaptation, EmoSApp serves as a blueprint for future innovations in portable, secure, and highly tailored AI-driven mental health support.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CY",
      "cs.HC"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2507.10580v2",
    "published_date": "2025-07-11 11:23:07 UTC",
    "updated_date": "2025-12-10 11:47:24 UTC"
  },
  {
    "arxiv_id": "2507.08499v1",
    "title": "PromotionGo at SemEval-2025 Task 11: A Feature-Centric Framework for Cross-Lingual Multi-Emotion Detection in Short Texts",
    "authors": [
      "Ziyi Huang",
      "Xia Cui"
    ],
    "abstract": "This paper presents our system for SemEval 2025 Task 11: Bridging the Gap in Text-Based Emotion Detection (Track A), which focuses on multi-label emotion detection in short texts. We propose a feature-centric framework that dynamically adapts document representations and learning algorithms to optimize language-specific performance. Our study evaluates three key components: document representation, dimensionality reduction, and model training in 28 languages, highlighting five for detailed analysis. The results show that TF-IDF remains highly effective for low-resource languages, while contextual embeddings like FastText and transformer-based document representations, such as those produced by Sentence-BERT, exhibit language-specific strengths. Principal Component Analysis (PCA) reduces training time without compromising performance, particularly benefiting FastText and neural models such as Multi-Layer Perceptrons (MLP). Computational efficiency analysis underscores the trade-off between model complexity and processing cost. Our framework provides a scalable solution for multilingual emotion detection, addressing the challenges of linguistic diversity and resource constraints.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2507.08499v1",
    "published_date": "2025-07-11 11:21:18 UTC",
    "updated_date": "2025-07-11 11:21:18 UTC"
  },
  {
    "arxiv_id": "2507.08487v1",
    "title": "Enhancing Essay Cohesion Assessment: A Novel Item Response Theory Approach",
    "authors": [
      "Bruno Alexandre Rosa",
      "Hilário Oliveira",
      "Luiz Rodrigues",
      "Eduardo Araujo Oliveira",
      "Rafael Ferreira Mello"
    ],
    "abstract": "Essays are considered a valuable mechanism for evaluating learning outcomes in writing. Textual cohesion is an essential characteristic of a text, as it facilitates the establishment of meaning between its parts. Automatically scoring cohesion in essays presents a challenge in the field of educational artificial intelligence. The machine learning algorithms used to evaluate texts generally do not consider the individual characteristics of the instances that comprise the analysed corpus. In this meaning, item response theory can be adapted to the context of machine learning, characterising the ability, difficulty and discrimination of the models used. This work proposes and analyses the performance of a cohesion score prediction approach based on item response theory to adjust the scores generated by machine learning models. In this study, the corpus selected for the experiments consisted of the extended Essay-BR, which includes 6,563 essays in the style of the National High School Exam (ENEM), and the Brazilian Portuguese Narrative Essays, comprising 1,235 essays written by 5th to 9th grade students from public schools. We extracted 325 linguistic features and treated the problem as a machine learning regression task. The experimental results indicate that the proposed approach outperforms conventional machine learning models and ensemble methods in several evaluation metrics. This research explores a potential approach for improving the automatic evaluation of cohesion in educational essays.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "24 pages, 4 tables",
    "pdf_url": "https://arxiv.org/pdf/2507.08487v1",
    "published_date": "2025-07-11 11:05:27 UTC",
    "updated_date": "2025-07-11 11:05:27 UTC"
  },
  {
    "arxiv_id": "2507.10579v1",
    "title": "Findings of the BEA 2025 Shared Task on Pedagogical Ability Assessment of AI-powered Tutors",
    "authors": [
      "Ekaterina Kochmar",
      "Kaushal Kumar Maurya",
      "Kseniia Petukhova",
      "KV Aditya Srivatsa",
      "Anaïs Tack",
      "Justin Vasselli"
    ],
    "abstract": "This shared task has aimed to assess pedagogical abilities of AI tutors powered by large language models (LLMs), focusing on evaluating the quality of tutor responses aimed at student's mistake remediation within educational dialogues. The task consisted of five tracks designed to automatically evaluate the AI tutor's performance across key dimensions of mistake identification, precise location of the mistake, providing guidance, and feedback actionability, grounded in learning science principles that define good and effective tutor responses, as well as the track focusing on detection of the tutor identity. The task attracted over 50 international teams across all tracks. The submitted models were evaluated against gold-standard human annotations, and the results, while promising, show that there is still significant room for improvement in this domain: the best results for the four pedagogical ability assessment tracks range between macro F1 scores of 58.34 (for providing guidance) and 71.81 (for mistake identification) on three-class problems, with the best F1 score in the tutor identification track reaching 96.98 on a 9-class task. In this paper, we overview the main findings of the shared task, discuss the approaches taken by the teams, and analyze their performance. All resources associated with this task are made publicly available to support future research in this critical domain.",
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.CY",
    "comment": "Proceedings of the 20th Workshop on Innovative Use of NLP for Building Educational Applications",
    "pdf_url": "https://arxiv.org/pdf/2507.10579v1",
    "published_date": "2025-07-11 10:57:36 UTC",
    "updated_date": "2025-07-11 10:57:36 UTC"
  },
  {
    "arxiv_id": "2507.10578v4",
    "title": "When and Where do Data Poisons Attack Textual Inversion?",
    "authors": [
      "Jeremy Styborski",
      "Mingzhi Lyu",
      "Jiayou Lu",
      "Nupur Kapur",
      "Adams Kong"
    ],
    "abstract": "Poisoning attacks pose significant challenges to the robustness of diffusion models (DMs). In this paper, we systematically analyze when and where poisoning attacks textual inversion (TI), a widely used personalization technique for DMs. We first introduce Semantic Sensitivity Maps, a novel method for visualizing the influence of poisoning on text embeddings. Second, we identify and experimentally verify that DMs exhibit non-uniform learning behavior across timesteps, focusing on lower-noise samples. Poisoning attacks inherit this bias and inject adversarial signals predominantly at lower timesteps. Lastly, we observe that adversarial signals distract learning away from relevant concept regions within training data, corrupting the TI process. Based on these insights, we propose Safe-Zone Training (SZT), a novel defense mechanism comprised of 3 key components: (1) JPEG compression to weaken high-frequency poison signals, (2) restriction to high timesteps during TI training to avoid adversarial signals at lower timesteps, and (3) loss masking to constrain learning to relevant regions. Extensive experiments across multiple poisoning methods demonstrate that SZT greatly enhances the robustness of TI against all poisoning attacks, improving generative quality beyond prior published defenses. Code: www.github.com/JStyborski/Diff_Lab Data: www.github.com/JStyborski/NC10",
    "categories": [
      "cs.CR",
      "cs.AI"
    ],
    "primary_category": "cs.CR",
    "comment": "Accepted to ICCV 2025",
    "pdf_url": "https://arxiv.org/pdf/2507.10578v4",
    "published_date": "2025-07-11 10:35:52 UTC",
    "updated_date": "2025-09-03 05:48:05 UTC"
  },
  {
    "arxiv_id": "2507.08472v2",
    "title": "Pre-Training LLMs on a budget: A comparison of three optimizers",
    "authors": [
      "Joel Schlotthauer",
      "Christian Kroos",
      "Chris Hinze",
      "Viktor Hangya",
      "Luzian Hahn",
      "Fabian Küch"
    ],
    "abstract": "Optimizers play a decisive role in reducing pre-training times for LLMs and achieving better-performing models. In this study, we compare three major variants: the de-facto standard AdamW, the simpler Lion, developed through an evolutionary search, and the second-order optimizer Sophia. For better generalization, we train with two different base architectures and use a single- and a multiple-epoch approach while keeping the number of tokens constant. Using the Maximal Update Parametrization and smaller proxy models, we tune relevant hyperparameters separately for each combination of base architecture and optimizer. We found that while the results from all three optimizers were in approximately the same range, Sophia exhibited the lowest training and validation loss, Lion was fastest in terms of training GPU hours but AdamW led to the best downstream evaluation results.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2507.08472v2",
    "published_date": "2025-07-11 10:29:04 UTC",
    "updated_date": "2025-07-22 08:48:53 UTC"
  },
  {
    "arxiv_id": "2507.08905v1",
    "title": "Last Layer Hamiltonian Monte Carlo",
    "authors": [
      "Koen Vellenga",
      "H. Joe Steinhauer",
      "Göran Falkman",
      "Jonas Andersson",
      "Anders Sjögren"
    ],
    "abstract": "We explore the use of Hamiltonian Monte Carlo (HMC) sampling as a probabilistic last layer approach for deep neural networks (DNNs). While HMC is widely regarded as a gold standard for uncertainty estimation, the computational demands limit its application to large-scale datasets and large DNN architectures. Although the predictions from the sampled DNN parameters can be parallelized, the computational cost still scales linearly with the number of samples (similar to an ensemble). Last layer HMC (LL--HMC) reduces the required computations by restricting the HMC sampling to the final layer of a DNN, making it applicable to more data-intensive scenarios with limited computational resources. In this paper, we compare LL-HMC against five last layer probabilistic deep learning (LL-PDL) methods across three real-world video datasets for driver action and intention. We evaluate the in-distribution classification performance, calibration, and out-of-distribution (OOD) detection. Due to the stochastic nature of the probabilistic evaluations, we performed five grid searches for different random seeds to avoid being reliant on a single initialization for the hyperparameter configurations. The results show that LL--HMC achieves competitive in-distribution classification and OOD detection performance. Additional sampled last layer parameters do not improve the classification performance, but can improve the OOD detection. Multiple chains or starting positions did not yield consistent improvements.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "25 pages, 15 figures, 6 tables, currently under submission",
    "pdf_url": "https://arxiv.org/pdf/2507.08905v1",
    "published_date": "2025-07-11 10:24:57 UTC",
    "updated_date": "2025-07-11 10:24:57 UTC"
  },
  {
    "arxiv_id": "2507.10577v2",
    "title": "Truth Sleuth and Trend Bender: AI Agents to fact-check YouTube videos and influence opinions",
    "authors": [
      "Cécile Logé",
      "Rehan Ghori"
    ],
    "abstract": "Misinformation poses a significant threat in today's digital world, often spreading rapidly through platforms like YouTube. This paper introduces a novel approach to combating misinformation by developing an AI-powered system that not only fact-checks claims made in YouTube videos but also actively engages users in the comment section and challenge misleading narratives. Our system comprises two main agents: Truth Sleuth and Trend Bender.\n  Truth Sleuth extracts claims from a YouTube video, uses a Retrieval-Augmented Generation (RAG) approach - drawing on sources like Wikipedia, Google Search, Google FactCheck - to accurately assess their veracity and generates a nuanced and comprehensive report. Through rigorous prompt engineering, Trend Bender leverages this report along with a curated corpus of relevant articles to generate insightful and persuasive comments designed to stimulate a productive debate. With a carefully set up self-evaluation loop, this agent is able to iteratively improve its style and refine its output.\n  We demonstrate the system's capabilities through experiments on established benchmark datasets and a real-world deployment on YouTube, showcasing its potential to engage users and potentially influence perspectives. Our findings highlight the high accuracy of our fact-checking agent, and confirm the potential of AI-driven interventions in combating misinformation and fostering a more informed online space.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CY"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2507.10577v2",
    "published_date": "2025-07-11 10:08:05 UTC",
    "updated_date": "2025-07-16 13:25:34 UTC"
  },
  {
    "arxiv_id": "2507.08458v1",
    "title": "A document is worth a structured record: Principled inductive bias design for document recognition",
    "authors": [
      "Benjamin Meyer",
      "Lukas Tuggener",
      "Sascha Hänzi",
      "Daniel Schmid",
      "Erdal Ayfer",
      "Benjamin F. Grewe",
      "Ahmed Abdulkadir",
      "Thilo Stadelmann"
    ],
    "abstract": "Many document types use intrinsic, convention-driven structures that serve to encode precise and structured information, such as the conventions governing engineering drawings. However, state-of-the-art approaches treat document recognition as a mere computer vision problem, neglecting these underlying document-type-specific structural properties, making them dependent on sub-optimal heuristic post-processing and rendering many less frequent or more complicated document types inaccessible to modern document recognition. We suggest a novel perspective that frames document recognition as a transcription task from a document to a record. This implies a natural grouping of documents based on the intrinsic structure inherent in their transcription, where related document types can be treated (and learned) similarly. We propose a method to design structure-specific inductive biases for the underlying machine-learned end-to-end document recognition systems, and a respective base transformer architecture that we successfully adapt to different structures. We demonstrate the effectiveness of the so-found inductive biases in extensive experiments with progressively complex record structures from monophonic sheet music, shape drawings, and simplified engineering drawings. By integrating an inductive bias for unrestricted graph structures, we train the first-ever successful end-to-end model to transcribe engineering drawings to their inherently interlinked information. Our approach is relevant to inform the design of document recognition systems for document types that are less well understood than standard OCR, OMR, etc., and serves as a guide to unify the design of future document foundation models.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2507.08458v1",
    "published_date": "2025-07-11 10:02:08 UTC",
    "updated_date": "2025-07-11 10:02:08 UTC"
  },
  {
    "arxiv_id": "2507.13370v3",
    "title": "H-NeiFi: Non-Invasive and Consensus-Efficient Multi-Agent Opinion Guidance",
    "authors": [
      "Shijun Guo",
      "Haoran Xu",
      "Yaming Yang",
      "Ziyu Guan",
      "Wei Zhao",
      "Xinyi Zhang",
      "Yishan Song"
    ],
    "abstract": "The openness of social media enables the free exchange of opinions, but it also presents challenges in guiding opinion evolution towards global consensus. Existing methods often directly modify user views or enforce cross-group connections. These intrusive interventions undermine user autonomy, provoke psychological resistance, and reduce the efficiency of global consensus. Additionally, due to the lack of a long-term perspective, promoting local consensus often exacerbates divisions at the macro level. To address these issues, we propose the hierarchical, non-intrusive opinion guidance framework, H-NeiFi. It first establishes a two-layer dynamic model based on social roles, considering the behavioral characteristics of both experts and non-experts. Additionally, we introduce a non-intrusive neighbor filtering method that adaptively controls user communication channels. Using multi-agent reinforcement learning (MARL), we optimize information propagation paths through a long-term reward function, avoiding direct interference with user interactions. Experiments show that H-NeiFi increases consensus speed by 22.0% to 30.7% and maintains global convergence even in the absence of experts. This approach enables natural and efficient consensus guidance by protecting user interaction autonomy, offering a new paradigm for social network governance.",
    "categories": [
      "cs.SI",
      "cs.AI",
      "cs.MA"
    ],
    "primary_category": "cs.SI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2507.13370v3",
    "published_date": "2025-07-11 09:56:33 UTC",
    "updated_date": "2025-11-04 03:56:46 UTC"
  },
  {
    "arxiv_id": "2507.08456v1",
    "title": "Space filling positionality and the Spiroformer",
    "authors": [
      "M. Maurin",
      "M. Á. Evangelista-Alvarado",
      "P. Suárez-Serrato"
    ],
    "abstract": "Transformers excel when dealing with sequential data. Generalizing transformer models to geometric domains, such as manifolds, we encounter the problem of not having a well-defined global order. We propose a solution with attention heads following a space-filling curve. As a first experimental example, we present the Spiroformer, a transformer that follows a polar spiral on the $2$-sphere.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "math.DG",
      "math.DS",
      "math.SG"
    ],
    "primary_category": "cs.LG",
    "comment": "9 pages, 5 figures. To appear in Geometric Science of Information 2025",
    "pdf_url": "https://arxiv.org/pdf/2507.08456v1",
    "published_date": "2025-07-11 09:56:15 UTC",
    "updated_date": "2025-07-11 09:56:15 UTC"
  },
  {
    "arxiv_id": "2507.08454v1",
    "title": "Why this and not that? A Logic-based Framework for Contrastive Explanations",
    "authors": [
      "Tobias Geibinger",
      "Reijo Jaakkola",
      "Antti Kuusisto",
      "Xinghan Liu",
      "Miikka Vilander"
    ],
    "abstract": "We define several canonical problems related to contrastive explanations, each answering a question of the form ''Why P but not Q?''. The problems compute causes for both P and Q, explicitly comparing their differences. We investigate the basic properties of our definitions in the setting of propositional logic. We show, inter alia, that our framework captures a cardinality-minimal version of existing contrastive explanations in the literature. Furthermore, we provide an extensive analysis of the computational complexities of the problems. We also implement the problems for CNF-formulas using answer set programming and present several examples demonstrating how they work in practice.",
    "categories": [
      "cs.AI",
      "cs.LG",
      "cs.LO"
    ],
    "primary_category": "cs.AI",
    "comment": "20 pages, accepted to JELIA 2025",
    "pdf_url": "https://arxiv.org/pdf/2507.08454v1",
    "published_date": "2025-07-11 09:55:04 UTC",
    "updated_date": "2025-07-11 09:55:04 UTC"
  },
  {
    "arxiv_id": "2507.10576v2",
    "title": "Can Large Language Models Understand As Well As Apply Patent Regulations to Pass a Hands-On Patent Attorney Test?",
    "authors": [
      "Bhakti Khera",
      "Rezvan Alamian",
      "Pascal A. Scherz",
      "Stephan M. Goetz"
    ],
    "abstract": "The legal field already uses various large language models (LLMs) in actual applications, but their quantitative performance and reasons for it are underexplored. We evaluated several open-source and proprietary LLMs -- including GPT-series, Anthropic, Deepseek and Llama-3, variants -- on parts of the European Qualifying Examination (EQE) for future European Patent Attorneys. OpenAI o1 led with 0.82 accuracy and 0.81 F1 score, whereas (Amazon Web Services) AWS Llama 3.1 8B lagged at 0.50 accuracy, and a Python-deployed Llama 3.1 8B scored 0.55. The latter two are within the range of mere guessing for the two-answer forced-choice design. None of the evaluated models could have passed the examination fully, as accuracy never exceeded the average threshold of 0.90 required for professional-level standards -- also not models that are regularly promoted for their assumed beyond-PhD- and bar-admitted-lawyer-level performance. GPT-4o excelled at integrating text and graphics, while Claude 3 Opus often lost formatting coherence. Human patent experts evaluated the textual justifications and uncovered various critical shortcomings of each model. They valued clarity and legal rationale over the raw correctness of the answers, which revealed misalignment between automatic metrics and expert judgment. Model outputs were sensitive to modest temperature changes and prompt wording, which underscores the remaining necessity of expert oversight. Future work should target logical consistency, robust multimodality, and adaptive prompting to approach human-level patent proficiency. In summary, despite the outstanding performance of recent large models, the general public might overestimate their performance. The field has a long way to go to develop a virtual patent attorney. This paper wants to point out several specific limitations that need solutions.",
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.CL",
      "cs.ET"
    ],
    "primary_category": "cs.CY",
    "comment": "41 pages, 21 figures",
    "pdf_url": "https://arxiv.org/pdf/2507.10576v2",
    "published_date": "2025-07-11 09:42:23 UTC",
    "updated_date": "2025-09-11 17:11:03 UTC"
  },
  {
    "arxiv_id": "2507.08448v1",
    "title": "Review of Feed-forward 3D Reconstruction: From DUSt3R to VGGT",
    "authors": [
      "Wei Zhang",
      "Yihang Wu",
      "Songhua Li",
      "Wenjie Ma",
      "Xin Ma",
      "Qiang Li",
      "Qi Wang"
    ],
    "abstract": "3D reconstruction, which aims to recover the dense three-dimensional structure of a scene, is a cornerstone technology for numerous applications, including augmented/virtual reality, autonomous driving, and robotics. While traditional pipelines like Structure from Motion (SfM) and Multi-View Stereo (MVS) achieve high precision through iterative optimization, they are limited by complex workflows, high computational cost, and poor robustness in challenging scenarios like texture-less regions. Recently, deep learning has catalyzed a paradigm shift in 3D reconstruction. A new family of models, exemplified by DUSt3R, has pioneered a feed-forward approach. These models employ a unified deep network to jointly infer camera poses and dense geometry directly from an Unconstrained set of images in a single forward pass. This survey provides a systematic review of this emerging domain. We begin by dissecting the technical framework of these feed-forward models, including their Transformer-based correspondence modeling, joint pose and geometry regression mechanisms, and strategies for scaling from two-view to multi-view scenarios. To highlight the disruptive nature of this new paradigm, we contrast it with both traditional pipelines and earlier learning-based methods like MVSNet. Furthermore, we provide an overview of relevant datasets and evaluation metrics. Finally, we discuss the technology's broad application prospects and identify key future challenges and opportunities, such as model accuracy and scalability, and handling dynamic scenes.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2507.08448v1",
    "published_date": "2025-07-11 09:41:54 UTC",
    "updated_date": "2025-07-11 09:41:54 UTC"
  },
  {
    "arxiv_id": "2507.08445v3",
    "title": "Clue-RAG: Towards Accurate and Cost-Efficient Graph-based RAG via Multi-Partite Graph and Query-Driven Iterative Retrieval",
    "authors": [
      "Yaodong Su",
      "Yixiang Fang",
      "Yingli Zhou",
      "Quanqing Xu",
      "Chuanhui Yang"
    ],
    "abstract": "Despite the remarkable progress of Large Language Models (LLMs), their performance in question answering (QA) remains limited by the lack of domain-specific and up-to-date knowledge. Retrieval-Augmented Generation (RAG) addresses this limitation by incorporating external information, often from graph-structured data. However, existing graph-based RAG methods suffer from poor graph quality due to incomplete extraction and insufficient utilization of query information during retrieval. To overcome these limitations, we propose Clue-RAG, a novel approach that introduces (1) a multi-partite graph index incorporates Chunk, knowledge unit, and entity to capture semantic content at multiple levels of granularity, coupled with a hybrid extraction strategy that reduces LLM token usage while still producing accurate and disambiguated knowledge units, and (2) Q-Iter, a query-driven iterative retrieval strategy that enhances relevance through semantic search and constrained graph traversal. Experiments on three QA benchmarks show that Clue-RAG significantly outperforms state-of-the-art baselines, achieving up to 99.33% higher Accuracy and 113.51% higher F1 score while reducing indexing costs by 72.58%. Remarkably, Clue-RAG matches or outperforms baselines even without using an LLM for indexing. These results demonstrate the effectiveness and cost-efficiency of Clue-RAG in advancing graph-based RAG systems.",
    "categories": [
      "cs.IR",
      "cs.AI"
    ],
    "primary_category": "cs.IR",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2507.08445v3",
    "published_date": "2025-07-11 09:36:45 UTC",
    "updated_date": "2025-09-16 06:13:41 UTC"
  },
  {
    "arxiv_id": "2507.08441v2",
    "title": "Vision Foundation Models as Effective Visual Tokenizers for Autoregressive Image Generation",
    "authors": [
      "Anlin Zheng",
      "Xin Wen",
      "Xuanyang Zhang",
      "Chuofan Ma",
      "Tiancai Wang",
      "Gang Yu",
      "Xiangyu Zhang",
      "Xiaojuan Qi"
    ],
    "abstract": "In this work, we present a novel direction to build an image tokenizer directly on top of a frozen vision foundation model, which is a largely underexplored area. Specifically, we employ a frozen vision foundation model as the encoder of our tokenizer. To enhance its effectiveness, we introduce two key components: (1) a region-adaptive quantization framework that reduces redundancy in the pre-trained features on regular 2D grids, and (2) a semantic reconstruction objective that aligns the tokenizer's outputs with the foundation model's representations to preserve semantic fidelity. Based on these designs, our proposed image tokenizer, VFMTok, achieves substantial improvements in image reconstruction and generation quality, while also enhancing token efficiency. It further boosts autoregressive (AR) generation -- achieving a gFID of 1.36 on ImageNet benchmarks, while accelerating model convergence by three times, and enabling high-fidelity class-conditional synthesis without the need for classifier-free guidance (CFG). The code is available at https://github.com/CVMI-Lab/VFMTok.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "20 pages, 4 figures",
    "pdf_url": "https://arxiv.org/pdf/2507.08441v2",
    "published_date": "2025-07-11 09:32:45 UTC",
    "updated_date": "2025-10-25 07:05:42 UTC"
  },
  {
    "arxiv_id": "2507.08440v1",
    "title": "Finding Common Ground: Using Large Language Models to Detect Agreement in Multi-Agent Decision Conferences",
    "authors": [
      "Selina Heller",
      "Mohamed Ibrahim",
      "David Antony Selby",
      "Sebastian Vollmer"
    ],
    "abstract": "Decision conferences are structured, collaborative meetings that bring together experts from various fields to address complex issues and reach a consensus on recommendations for future actions or policies. These conferences often rely on facilitated discussions to ensure productive dialogue and collective agreement. Recently, Large Language Models (LLMs) have shown significant promise in simulating real-world scenarios, particularly through collaborative multi-agent systems that mimic group interactions. In this work, we present a novel LLM-based multi-agent system designed to simulate decision conferences, specifically focusing on detecting agreement among the participant agents. To achieve this, we evaluate six distinct LLMs on two tasks: stance detection, which identifies the position an agent takes on a given issue, and stance polarity detection, which identifies the sentiment as positive, negative, or neutral. These models are further assessed within the multi-agent system to determine their effectiveness in complex simulations. Our results indicate that LLMs can reliably detect agreement even in dynamic and nuanced debates. Incorporating an agreement-detection agent within the system can also improve the efficiency of group debates and enhance the overall quality and coherence of deliberations, making them comparable to real-world decision conferences regarding outcome and decision-making. These findings demonstrate the potential for LLM-based multi-agent systems to simulate group decision-making processes. They also highlight that such systems could be instrumental in supporting decision-making with expert elicitation workshops across various domains.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.MA"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2507.08440v1",
    "published_date": "2025-07-11 09:31:10 UTC",
    "updated_date": "2025-07-11 09:31:10 UTC"
  },
  {
    "arxiv_id": "2507.08427v1",
    "title": "ChainEdit: Propagating Ripple Effects in LLM Knowledge Editing through Logical Rule-Guided Chains",
    "authors": [
      "Zilu Dong",
      "Xiangqing Shen",
      "Zinong Yang",
      "Rui Xia"
    ],
    "abstract": "Current knowledge editing methods for large language models (LLMs) struggle to maintain logical consistency when propagating ripple effects to associated facts. We propose ChainEdit, a framework that synergizes knowledge graph-derived logical rules with LLM logical reasoning capabilities to enable systematic chain updates. By automatically extracting logical patterns from structured knowledge bases and aligning them with LLMs' internal logics, ChainEdit dynamically generates and edits logically connected knowledge clusters. Experiments demonstrate an improvement of more than 30% in logical generalization over baselines while preserving editing reliability and specificity. We further address evaluation biases in existing benchmarks through knowledge-aware protocols that disentangle external dependencies. This work establishes new state-of-the-art performance on ripple effect while ensuring internal logical consistency after knowledge editing.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted to ACL 2025 (main)",
    "pdf_url": "https://arxiv.org/pdf/2507.08427v1",
    "published_date": "2025-07-11 09:13:29 UTC",
    "updated_date": "2025-07-11 09:13:29 UTC"
  },
  {
    "arxiv_id": "2507.21114v1",
    "title": "Page image classification for content-specific data processing",
    "authors": [
      "Kateryna Lutsai",
      "Pavel Straňák"
    ],
    "abstract": "Digitization projects in humanities often generate vast quantities of page images from historical documents, presenting significant challenges for manual sorting and analysis. These archives contain diverse content, including various text types (handwritten, typed, printed), graphical elements (drawings, maps, photos), and layouts (plain text, tables, forms). Efficiently processing this heterogeneous data requires automated methods to categorize pages based on their content, enabling tailored downstream analysis pipelines. This project addresses this need by developing and evaluating an image classification system specifically designed for historical document pages, leveraging advancements in artificial intelligence and machine learning. The set of categories was chosen to facilitate content-specific processing workflows, separating pages requiring different analysis techniques (e.g., OCR for text, image analysis for graphics)",
    "categories": [
      "cs.IR",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.IR",
    "comment": "65 pages, 57 figures, 20 tables",
    "pdf_url": "https://arxiv.org/pdf/2507.21114v1",
    "published_date": "2025-07-11 08:30:12 UTC",
    "updated_date": "2025-07-11 08:30:12 UTC"
  },
  {
    "arxiv_id": "2507.08902v2",
    "title": "Generation of structure-guided pMHC-I libraries using Diffusion Models",
    "authors": [
      "Sergio Mares",
      "Ariel Espinoza Weinberger",
      "Nilah M. Ioannidis"
    ],
    "abstract": "Personalized vaccines and T-cell immunotherapies depend critically on identifying peptide-MHC class I (pMHC-I) interactions capable of eliciting potent immune responses. However, current benchmarks and models inherit biases present in mass-spectrometry and binding-assay datasets, limiting discovery of novel peptide ligands. To address this issue, we introduce a structure-guided benchmark of pMHC-I peptides designed using diffusion models conditioned on crystal structure interaction distances. Spanning twenty high-priority HLA alleles, this benchmark is independent of previously characterized peptides yet reproduces canonical anchor residue preferences, indicating structural generalization without experimental dataset bias. Using this resource, we demonstrate that state-of-the-art sequence-based predictors perform poorly at recognizing the binding potential of these structurally stable designs, indicating allele-specific limitations invisible in conventional evaluations. Our geometry-aware design pipeline yields peptides with high predicted structural integrity and higher residue diversity than existing datasets, representing a key resource for unbiased model training and evaluation. Our code, and data are available at: https://github.com/sermare/struct-mhc-dev.",
    "categories": [
      "q-bio.QM",
      "cs.AI"
    ],
    "primary_category": "q-bio.QM",
    "comment": "Accepted to the The 2nd Workshop on Generative AI and Biology ICML Workshop 2025",
    "pdf_url": "https://arxiv.org/pdf/2507.08902v2",
    "published_date": "2025-07-11 08:29:18 UTC",
    "updated_date": "2025-08-21 06:39:45 UTC"
  },
  {
    "arxiv_id": "2507.08404v1",
    "title": "Deep Hashing with Semantic Hash Centers for Image Retrieval",
    "authors": [
      "Li Chen",
      "Rui Liu",
      "Yuxiang Zhou",
      "Xudong Ma",
      "Yong Chen",
      "Dell Zhang"
    ],
    "abstract": "Deep hashing is an effective approach for large-scale image retrieval. Current methods are typically classified by their supervision types: point-wise, pair-wise, and list-wise. Recent point-wise techniques (e.g., CSQ, MDS) have improved retrieval performance by pre-assigning a hash center to each class, enhancing the discriminability of hash codes across various datasets. However, these methods rely on data-independent algorithms to generate hash centers, which neglect the semantic relationships between classes and may degrade retrieval performance.\n  This paper introduces the concept of semantic hash centers, building on the idea of traditional hash centers. We hypothesize that hash centers of semantically related classes should have closer Hamming distances, while those of unrelated classes should be more distant. To this end, we propose a three-stage framework, SHC, to generate hash codes that preserve semantic structure.\n  First, we develop a classification network to identify semantic similarities between classes using a data-dependent similarity calculation that adapts to varying data distributions. Second, we introduce an optimization algorithm to generate semantic hash centers, preserving semantic relatedness while enforcing a minimum distance between centers to avoid excessively similar hash codes. Finally, a deep hashing network is trained using these semantic centers to convert images into binary hash codes.\n  Experimental results on large-scale retrieval tasks across several public datasets show that SHC significantly improves retrieval performance. Specifically, SHC achieves average improvements of +7.26%, +7.62%, and +11.71% in MAP@100, MAP@1000, and MAP@ALL metrics, respectively, over state-of-the-art methods.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2507.08404v1",
    "published_date": "2025-07-11 08:22:27 UTC",
    "updated_date": "2025-07-11 08:22:27 UTC"
  },
  {
    "arxiv_id": "2507.08403v1",
    "title": "Towards AI-Native RAN: An Operator's Perspective of 6G Day 1 Standardization",
    "authors": [
      "Nan Li",
      "Qi Sun",
      "Lehan Wang",
      "Xiaofei Xu",
      "Jinri Huang",
      "Chunhui Liu",
      "Jing Gao",
      "Yuhong Huang",
      "Chih-Lin I"
    ],
    "abstract": "Artificial Intelligence/Machine Learning (AI/ML) has become the most certain and prominent feature of 6G mobile networks. Unlike 5G, where AI/ML was not natively integrated but rather an add-on feature over existing architecture, 6G shall incorporate AI from the onset to address its complexity and support ubiquitous AI applications. Based on our extensive mobile network operation and standardization experience from 2G to 5G, this paper explores the design and standardization principles of AI-Native radio access networks (RAN) for 6G, with a particular focus on its critical Day 1 architecture, functionalities and capabilities. We investigate the framework of AI-Native RAN and present its three essential capabilities to shed some light on the standardization direction; namely, AI-driven RAN processing/optimization/automation, reliable AI lifecycle management (LCM), and AI-as-a-Service (AIaaS) provisioning. The standardization of AI-Native RAN, in particular the Day 1 features, including an AI-Native 6G RAN architecture, were proposed. For validation, a large-scale field trial with over 5000 5G-A base stations have been built and delivered significant improvements in average air interface latency, root cause identification, and network energy consumption with the proposed architecture and the supporting AI functions. This paper aims to provide a Day 1 framework for 6G AI-Native RAN standardization design, balancing technical innovation with practical deployment.",
    "categories": [
      "cs.NI",
      "cs.AI",
      "cs.DC",
      "cs.LG",
      "eess.SY"
    ],
    "primary_category": "cs.NI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2507.08403v1",
    "published_date": "2025-07-11 08:21:08 UTC",
    "updated_date": "2025-07-11 08:21:08 UTC"
  },
  {
    "arxiv_id": "2507.08400v1",
    "title": "PanMatch: Unleashing the Potential of Large Vision Models for Unified Matching Models",
    "authors": [
      "Yongjian Zhang",
      "Longguang Wang",
      "Kunhong Li",
      "Ye Zhang",
      "Yun Wang",
      "Liang Lin",
      "Yulan Guo"
    ],
    "abstract": "This work presents PanMatch, a versatile foundation model for robust correspondence matching. Unlike previous methods that rely on task-specific architectures and domain-specific fine-tuning to support tasks like stereo matching, optical flow or feature matching, our key insight is that any two-frame correspondence matching task can be addressed within a 2D displacement estimation framework using the same model weights. Such a formulation eliminates the need for designing specialized unified architectures or task-specific ensemble models. Instead, it achieves multi-task integration by endowing displacement estimation algorithms with unprecedented generalization capabilities. To this end, we highlight the importance of a robust feature extractor applicable across multiple domains and tasks, and propose the feature transformation pipeline that leverage all-purpose features from Large Vision Models to endow matching baselines with zero-shot cross-view matching capabilities. Furthermore, we assemble a cross-domain dataset with near 1.8 million samples from stereo matching, optical flow, and feature matching domains to pretrain PanMatch. We demonstrate the versatility of PanMatch across a wide range of domains and downstream tasks using the same model weights. Our model outperforms UniMatch and Flow-Anything on cross-task evaluations, and achieves comparable performance to most state-of-the-art task-specific algorithms on task-oriented benchmarks. Additionally, PanMatch presents unprecedented zero-shot performance in abnormal scenarios, such as rainy day and satellite imagery, where most existing robust algorithms fail to yield meaningful results.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.MM"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2507.08400v1",
    "published_date": "2025-07-11 08:18:52 UTC",
    "updated_date": "2025-07-11 08:18:52 UTC"
  },
  {
    "arxiv_id": "2507.08392v3",
    "title": "Multi-Agent LLMs as Ethics Advocates for AI-Based Systems",
    "authors": [
      "Asma Yamani",
      "Malak Baslyman",
      "Moataz Ahmed"
    ],
    "abstract": "Incorporating ethics into the requirement elicitation process is essential for creating ethically aligned systems. Although eliciting manual ethics requirements is effective, it requires diverse input from multiple stakeholders, which can be challenging due to time and resource constraints. Moreover, it is often given a low priority in the requirements elicitation process. This study proposes a framework for generating ethics requirements drafts by introducing an ethics advocate agent in a multi-agent LLM setting. This agent critiques and provides input on ethical issues based on the system description. The proposed framework is evaluated through two case studies from different contexts, demonstrating that it captures the majority of ethics requirements identified by researchers during 30-minute interviews and introduces several additional relevant requirements. However, it also highlights reliability issues in generating ethics requirements, emphasizing the need for human feedback in this sensitive domain. We believe this work can facilitate the broader adoption of ethics in the requirements engineering process, ultimately leading to more ethically aligned products.",
    "categories": [
      "cs.AI",
      "cs.CY"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2507.08392v3",
    "published_date": "2025-07-11 08:04:32 UTC",
    "updated_date": "2025-08-26 05:43:26 UTC"
  },
  {
    "arxiv_id": "2507.08366v1",
    "title": "Intelligent Control of Spacecraft Reaction Wheel Attitude Using Deep Reinforcement Learning",
    "authors": [
      "Ghaith El-Dalahmeh",
      "Mohammad Reza Jabbarpour",
      "Bao Quoc Vo",
      "Ryszard Kowalczyk"
    ],
    "abstract": "Reliable satellite attitude control is essential for the success of space missions, particularly as satellites increasingly operate autonomously in dynamic and uncertain environments. Reaction wheels (RWs) play a pivotal role in attitude control, and maintaining control resilience during RW faults is critical to preserving mission objectives and system stability. However, traditional Proportional Derivative (PD) controllers and existing deep reinforcement learning (DRL) algorithms such as TD3, PPO, and A2C often fall short in providing the real time adaptability and fault tolerance required for autonomous satellite operations. This study introduces a DRL-based control strategy designed to improve satellite resilience and adaptability under fault conditions. Specifically, the proposed method integrates Twin Delayed Deep Deterministic Policy Gradient (TD3) with Hindsight Experience Replay (HER) and Dimension Wise Clipping (DWC) referred to as TD3-HD to enhance learning in sparse reward environments and maintain satellite stability during RW failures. The proposed approach is benchmarked against PD control and leading DRL algorithms. Experimental results show that TD3-HD achieves significantly lower attitude error, improved angular velocity regulation, and enhanced stability under fault conditions. These findings underscore the proposed method potential as a powerful, fault tolerant, onboard AI solution for autonomous satellite attitude control.",
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "primary_category": "cs.RO",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2507.08366v1",
    "published_date": "2025-07-11 07:28:16 UTC",
    "updated_date": "2025-07-11 07:28:16 UTC"
  },
  {
    "arxiv_id": "2507.08340v2",
    "title": "Single Domain Generalization for Multimodal Cross-Cancer Prognosis via Dirac Rebalancer and Distribution Entanglement",
    "authors": [
      "Jia-Xuan Jiang",
      "Jiashuai Liu",
      "Hongtao Wu",
      "Yifeng Wu",
      "Zhong Wang",
      "Qi Bi",
      "Yefeng Zheng"
    ],
    "abstract": "Deep learning has shown remarkable performance in integrating multimodal data for survival prediction. However, existing multimodal methods mainly focus on single cancer types and overlook the challenge of generalization across cancers. In this work, we are the first to reveal that multimodal prognosis models often generalize worse than unimodal ones in cross-cancer scenarios, despite the critical need for such robustness in clinical practice. To address this, we propose a new task: Cross-Cancer Single Domain Generalization for Multimodal Prognosis, which evaluates whether models trained on a single cancer type can generalize to unseen cancers. We identify two key challenges: degraded features from weaker modalities and ineffective multimodal integration. To tackle these, we introduce two plug-and-play modules: Sparse Dirac Information Rebalancer (SDIR) and Cancer-aware Distribution Entanglement (CADE). SDIR mitigates the dominance of strong features by applying Bernoulli-based sparsification and Dirac-inspired stabilization to enhance weaker modality signals. CADE, designed to synthesize the target domain distribution, fuses local morphological cues and global gene expression in latent space. Experiments on a four-cancer-type benchmark demonstrate superior generalization, laying the foundation for practical, robust cross-cancer multimodal prognosis. Code is available at https://github.com/HopkinsKwong/MCCSDG",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "Accepted by ACMMM 25",
    "pdf_url": "https://arxiv.org/pdf/2507.08340v2",
    "published_date": "2025-07-11 06:37:48 UTC",
    "updated_date": "2025-08-29 17:23:09 UTC"
  },
  {
    "arxiv_id": "2507.08334v2",
    "title": "EnCoBo: Energy-Guided Concept Bottlenecks for Interpretable Generation",
    "authors": [
      "Sangwon Kim",
      "Kyoungoh Lee",
      "Jeyoun Dong",
      "Jung Hwan Ahn",
      "Kwang-Ju Kim"
    ],
    "abstract": "Concept Bottleneck Models (CBMs) provide interpretable decision-making through explicit, human-understandable concepts. However, existing generative CBMs often rely on auxiliary visual cues at the bottleneck, which undermines interpretability and intervention capabilities. We propose EnCoBo, a post-hoc concept bottleneck for generative models that eliminates auxiliary cues by constraining all representations to flow solely through explicit concepts. Unlike autoencoder-based approaches that inherently rely on black-box decoders, EnCoBo leverages a decoder-free, energy-based framework that directly guides generation in the latent space. Guided by diffusion-scheduled energy functions, EnCoBo supports robust post-hoc interventions-such as concept composition and negation-across arbitrary concepts. Experiments on CelebA-HQ and CUB datasets showed that EnCoBo improved concept-level human intervention and interpretability while maintaining competitive visual quality.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "The original version was accepted by ICCV2025 Workshops",
    "pdf_url": "https://arxiv.org/pdf/2507.08334v2",
    "published_date": "2025-07-11 06:27:11 UTC",
    "updated_date": "2025-09-18 03:55:53 UTC"
  },
  {
    "arxiv_id": "2507.08333v3",
    "title": "Token-based Audio Inpainting via Discrete Diffusion",
    "authors": [
      "Tali Dror",
      "Iftach Shoham",
      "Moshe Buchris",
      "Oren Gal",
      "Haim Permuter",
      "Gilad Katz",
      "Eliya Nachmani"
    ],
    "abstract": "Audio inpainting seeks to restore missing segments in degraded recordings. Previous diffusion-based methods exhibit impaired performance when the missing region is large. We introduce the first approach that applies discrete diffusion over tokenized music representations from a pre-trained audio tokenizer, enabling stable and semantically coherent restoration of long gaps. Our method further incorporates two training approaches: a derivative-based regularization loss that enforces smooth temporal dynamics, and a span-based absorbing transition that provides structured corruption during diffusion. Experiments on the MusicNet and MAESTRO datasets with gaps up to 750 ms show that our approach consistently outperforms strong baselines across range of gap lengths, for gaps of 150 ms and above. This work advances musical audio restoration and introduces new directions for discrete diffusion model training. Audio examples of our proposed method can be found at https://iftach21.github.io/.",
    "categories": [
      "cs.SD",
      "cs.AI",
      "cs.IT",
      "cs.LG",
      "eess.AS"
    ],
    "primary_category": "cs.SD",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2507.08333v3",
    "published_date": "2025-07-11 06:25:49 UTC",
    "updated_date": "2025-10-08 09:01:13 UTC"
  },
  {
    "arxiv_id": "2507.08330v2",
    "title": "Interpretability-Aware Pruning for Efficient Medical Image Analysis",
    "authors": [
      "Nikita Malik",
      "Pratinav Seth",
      "Neeraj Kumar Singh",
      "Chintan Chitroda",
      "Vinay Kumar Sankarapu"
    ],
    "abstract": "Deep learning has driven significant advances in medical image analysis, yet its adoption in clinical practice remains constrained by the large size and lack of transparency in modern models. Advances in interpretability techniques such as DL-Backtrace, Layer-wise Relevance Propagation, and Integrated Gradients make it possible to assess the contribution of individual components within neural networks trained on medical imaging tasks. In this work, we introduce an interpretability-guided pruning framework that reduces model complexity while preserving both predictive performance and transparency. By selectively retaining only the most relevant parts of each layer, our method enables targeted compression that maintains clinically meaningful representations. Experiments across multiple medical image classification benchmarks demonstrate that this approach achieves high compression rates with minimal loss in accuracy, paving the way for lightweight, interpretable models suited for real-world deployment in healthcare settings.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.ET",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "Accepted at The 1st MICCAI Workshop on Efficient Medical AI 2025",
    "pdf_url": "https://arxiv.org/pdf/2507.08330v2",
    "published_date": "2025-07-11 05:58:22 UTC",
    "updated_date": "2025-09-21 00:25:35 UTC"
  },
  {
    "arxiv_id": "2507.08898v3",
    "title": "SEALGuard: Safeguarding the Multilingual Conversations in Southeast Asian Languages for LLM Software Systems",
    "authors": [
      "Wenliang Shan",
      "Michael Fu",
      "Rui Yang",
      "Chakkrit Tantithamthavorn"
    ],
    "abstract": "Safety alignment is critical for LLM-powered systems. While recent LLM-powered guardrail approaches such as LlamaGuard achieve high detection accuracy of unsafe inputs written in English (e.g., ``How to create a bomb?''), they struggle with multilingual unsafe inputs. This limitation leaves LLM systems vulnerable to unsafe and jailbreak prompts written in low-resource languages such as those in Southeast Asia. This paper introduces SEALGuard, a multilingual guardrail designed to improve the safety alignment across diverse languages. It aims to address the multilingual safety alignment gap of existing guardrails and ensure effective filtering of unsafe and jailbreak prompts in LLM-powered systems. We adapt a general-purpose multilingual language model into a multilingual guardrail using low-rank adaptation (LoRA). We construct SEALSBench, a large-scale multilingual safety alignment dataset containing over 260,000 prompts in ten languages, including safe, unsafe, and jailbreak cases. We evaluate SEALGuard against state-of-the-art guardrails such as LlamaGuard on this benchmark. Our findings show that multilingual unsafe and jailbreak prompts substantially degrade the performance of the state-of-the-art LlamaGuard, which experiences a drop in Defense Success Rate (DSR) by 9% and 18%, respectively, compared to its performance on English-only prompts. In contrast, SEALGuard outperforms existing guardrails in detecting multilingual unsafe and jailbreak prompts, improving DSR by 48% over LlamaGuard and achieving the best DSR, precision, and F1-score. Our ablation study further reveals the contributions of adaptation strategies and model size to the overall performance of SEALGuard. We release our pre-trained model and benchmark at https://github.com/awsm-research/SEALGuard to support further research.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2507.08898v3",
    "published_date": "2025-07-11 05:15:35 UTC",
    "updated_date": "2025-07-17 08:01:44 UTC"
  },
  {
    "arxiv_id": "2507.08310v1",
    "title": "Generative AI in Science: Applications, Challenges, and Emerging Questions",
    "authors": [
      "Ryan Harries",
      "Cornelia Lawson",
      "Philip Shapira"
    ],
    "abstract": "This paper examines the impact of Generative Artificial Intelligence (GenAI) on scientific practices, conducting a qualitative review of selected literature to explore its applications, benefits, and challenges. The review draws on the OpenAlex publication database, using a Boolean search approach to identify scientific literature related to GenAI (including large language models and ChatGPT). Thirty-nine highly cited papers and commentaries are reviewed and qualitatively coded. Results are categorized by GenAI applications in science, scientific writing, medical practice, and education and training. The analysis finds that while there is a rapid adoption of GenAI in science and science practice, its long-term implications remain unclear, with ongoing uncertainties about its use and governance. The study provides early insights into GenAI's growing role in science and identifies questions for future research in this evolving field.",
    "categories": [
      "cs.CY",
      "cs.AI"
    ],
    "primary_category": "cs.CY",
    "comment": "9 pages, 1 figure, 1 appendix",
    "pdf_url": "https://arxiv.org/pdf/2507.08310v1",
    "published_date": "2025-07-11 05:02:24 UTC",
    "updated_date": "2025-07-11 05:02:24 UTC"
  },
  {
    "arxiv_id": "2507.08309v1",
    "title": "Improving MLLM's Document Image Machine Translation via Synchronously Self-reviewing Its OCR Proficiency",
    "authors": [
      "Yupu Liang",
      "Yaping Zhang",
      "Zhiyang Zhang",
      "Zhiyuan Chen",
      "Yang Zhao",
      "Lu Xiang",
      "Chengqing Zong",
      "Yu Zhou"
    ],
    "abstract": "Multimodal Large Language Models (MLLMs) have shown strong performance in document image tasks, especially Optical Character Recognition (OCR). However, they struggle with Document Image Machine Translation (DIMT), which requires handling both cross-modal and cross-lingual challenges. Previous efforts to enhance DIMT capability through Supervised Fine-Tuning (SFT) on the DIMT dataset often result in the forgetting of the model's existing monolingual abilities, such as OCR. To address these challenges, we introduce a novel fine-tuning paradigm, named Synchronously Self-Reviewing (SSR) its OCR proficiency, inspired by the concept \"Bilingual Cognitive Advantage\". Specifically, SSR prompts the model to generate OCR text before producing translation text, which allows the model to leverage its strong monolingual OCR ability while learning to translate text across languages. Comprehensive experiments demonstrate the proposed SSR learning helps mitigate catastrophic forgetting, improving the generalization ability of MLLMs on both OCR and DIMT tasks.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted by ACL 2025 Findings",
    "pdf_url": "https://arxiv.org/pdf/2507.08309v1",
    "published_date": "2025-07-11 05:02:06 UTC",
    "updated_date": "2025-07-11 05:02:06 UTC"
  },
  {
    "arxiv_id": "2507.08306v1",
    "title": "M2-Reasoning: Empowering MLLMs with Unified General and Spatial Reasoning",
    "authors": [
      "Inclusion AI",
      ":",
      "Fudong Wang",
      "Jiajia Liu",
      "Jingdong Chen",
      "Jun Zhou",
      "Kaixiang Ji",
      "Lixiang Ru",
      "Qingpei Guo",
      "Ruobing Zheng",
      "Tianqi Li",
      "Yi Yuan",
      "Yifan Mao",
      "Yuting Xiao",
      "Ziping Ma"
    ],
    "abstract": "Recent advancements in Multimodal Large Language Models (MLLMs), particularly through Reinforcement Learning with Verifiable Rewards (RLVR), have significantly enhanced their reasoning abilities. However, a critical gap persists: these models struggle with dynamic spatial interactions, a capability essential for real-world applications. To bridge this gap, we introduce M2-Reasoning-7B, a model designed to excel in both general and spatial reasoning. Our approach integrates two key innovations: (1) a novel data pipeline that generates 294.2K high-quality data samples (168K for cold-start fine-tuning and 126.2K for RLVR), which feature logically coherent reasoning trajectories and have undergone comprehensive assessment; and (2) a dynamic multi-task training strategy with step-wise optimization to mitigate conflicts between data, and task-specific rewards for delivering tailored incentive signals. This combination of curated data and advanced training allows M2-Reasoning-7B to set a new state-of-the-art (SOTA) across 8 benchmarks, showcasing superior performance in both general and spatial reasoning domains.",
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.CV",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "31pages, 14 figures",
    "pdf_url": "https://arxiv.org/pdf/2507.08306v1",
    "published_date": "2025-07-11 04:44:07 UTC",
    "updated_date": "2025-07-11 04:44:07 UTC"
  },
  {
    "arxiv_id": "2507.08288v1",
    "title": "Invariant-based Robust Weights Watermark for Large Language Models",
    "authors": [
      "Qingxiao Guo",
      "Xinjie Zhu",
      "Yilong Ma",
      "Hui Jin",
      "Yunhao Wang",
      "Weifeng Zhang",
      "Xiaobing Guo"
    ],
    "abstract": "Watermarking technology has gained significant attention due to the increasing importance of intellectual property (IP) rights, particularly with the growing deployment of large language models (LLMs) on billions resource-constrained edge devices. To counter the potential threats of IP theft by malicious users, this paper introduces a robust watermarking scheme without retraining or fine-tuning for transformer models. The scheme generates a unique key for each user and derives a stable watermark value by solving linear constraints constructed from model invariants. Moreover, this technology utilizes noise mechanism to hide watermark locations in multi-user scenarios against collusion attack. This paper evaluates the approach on three popular models (Llama3, Phi3, Gemma), and the experimental results confirm the strong robustness across a range of attack methods (fine-tuning, pruning, quantization, permutation, scaling, reversible matrix and collusion attacks).",
    "categories": [
      "cs.CR",
      "cs.AI"
    ],
    "primary_category": "cs.CR",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2507.08288v1",
    "published_date": "2025-07-11 03:24:47 UTC",
    "updated_date": "2025-07-11 03:24:47 UTC"
  },
  {
    "arxiv_id": "2507.08284v1",
    "title": "Lightweight Safety Guardrails via Synthetic Data and RL-guided Adversarial Training",
    "authors": [
      "Aleksei Ilin",
      "Gor Matevosyan",
      "Xueying Ma",
      "Vladimir Eremin",
      "Suhaa Dada",
      "Muqun Li",
      "Riyaaz Shaik",
      "Haluk Noyan Tokgozoglu"
    ],
    "abstract": "We introduce a lightweight yet highly effective safety guardrail framework for language models, demonstrating that small-scale language models can achieve, and even surpass, the performance of larger counterparts in content moderation tasks. This is accomplished through high-fidelity synthetic data generation and adversarial training. The synthetic data generation process begins with human-curated seed data, which undergoes query augmentation and paraphrasing to create diverse and contextually rich examples. This augmented data is then subjected to multiple rounds of curation, ensuring high fidelity and relevance. Inspired by recent advances in the Generative Adversarial Network (GAN) architecture, our adversarial training employs reinforcement learning to guide a generator that produces challenging synthetic examples. These examples are used to fine-tune the safety classifier, enhancing its ability to detect and mitigate harmful content. Additionally, we incorporate strategies from recent research on efficient LLM training, leveraging the capabilities of smaller models to improve the performance of larger generative models. With iterative adversarial training and the generation of diverse, high-quality synthetic data, our framework enables small language models (SLMs) to serve as robust safety guardrails. This approach not only reduces computational overhead but also enhances resilience against adversarial attacks, offering a scalable and efficient solution for content moderation in AI systems.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2507.08284v1",
    "published_date": "2025-07-11 03:17:58 UTC",
    "updated_date": "2025-07-11 03:17:58 UTC"
  },
  {
    "arxiv_id": "2507.08270v1",
    "title": "Agent Safety Alignment via Reinforcement Learning",
    "authors": [
      "Zeyang Sha",
      "Hanling Tian",
      "Zhuoer Xu",
      "Shiwen Cui",
      "Changhua Meng",
      "Weiqiang Wang"
    ],
    "abstract": "The emergence of autonomous Large Language Model (LLM) agents capable of tool usage has introduced new safety risks that go beyond traditional conversational misuse. These agents, empowered to execute external functions, are vulnerable to both user-initiated threats (e.g., adversarial prompts) and tool-initiated threats (e.g., malicious outputs from compromised tools). In this paper, we propose the first unified safety-alignment framework for tool-using agents, enabling models to handle both channels of threat via structured reasoning and sandboxed reinforcement learning. We introduce a tri-modal taxonomy, including benign, malicious, and sensitive for both user prompts and tool responses, and define a policy-driven decision model. Our framework employs a custom-designed sandbox environment that simulates real-world tool execution and allows fine-grained reward shaping. Through extensive evaluations on public and self-built benchmarks, including Agent SafetyBench, InjecAgent, and BFCL, we demonstrate that our safety-aligned agents significantly improve resistance to security threats while preserving strong utility on benign tasks. Our results show that safety and effectiveness can be jointly optimized, laying the groundwork for trustworthy deployment of autonomous LLM agents.",
    "categories": [
      "cs.AI",
      "cs.CR"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2507.08270v1",
    "published_date": "2025-07-11 02:34:16 UTC",
    "updated_date": "2025-07-11 02:34:16 UTC"
  },
  {
    "arxiv_id": "2507.08267v1",
    "title": "A Practical Two-Stage Recipe for Mathematical LLMs: Maximizing Accuracy with SFT and Efficiency with Reinforcement Learning",
    "authors": [
      "Hiroshi Yoshihara",
      "Taiki Yamaguchi",
      "Yuichi Inoue"
    ],
    "abstract": "Enhancing the mathematical reasoning of Large Language Models (LLMs) is a pivotal challenge in advancing AI capabilities. While Supervised Fine-Tuning (SFT) and Reinforcement Learning (RL) are the dominant training paradigms, a systematic methodology for combining them to maximize both accuracy and efficiency remains largely unexplored. This paper introduces a practical and effective training recipe that strategically integrates extended SFT with RL from online inference (GRPO). We posit that these methods play complementary, not competing, roles: a prolonged SFT phase first pushes the model's accuracy to its limits, after which a GRPO phase dramatically improves token efficiency while preserving this peak performance. Our experiments reveal that extending SFT for as many as 10 epochs is crucial for performance breakthroughs, and that the primary role of GRPO in this framework is to optimize solution length. The efficacy of our recipe is rigorously validated through top-tier performance on challenging benchmarks, including a high rank among over 2,200 teams in the strictly leak-free AI Mathematical Olympiad (AIMO). This work provides the community with a battle-tested blueprint for developing state-of-the-art mathematical reasoners that are both exceptionally accurate and practically efficient. To ensure full reproducibility and empower future research, we will open-source our entire framework, including all code, model checkpoints, and training configurations at https://github.com/analokmaus/kaggle-aimo2-fast-math-r1.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "Presented at ICML 2025 Workshop on The second AI for MATH",
    "pdf_url": "https://arxiv.org/pdf/2507.08267v1",
    "published_date": "2025-07-11 02:26:01 UTC",
    "updated_date": "2025-07-11 02:26:01 UTC"
  },
  {
    "arxiv_id": "2507.08264v1",
    "title": "Abductive Computational Systems: Creative Abduction and Future Directions",
    "authors": [
      "Abhinav Sood",
      "Kazjon Grace",
      "Stephen Wan",
      "Cecile Paris"
    ],
    "abstract": "Abductive reasoning, reasoning for inferring explanations for observations, is often mentioned in scientific, design-related and artistic contexts, but its understanding varies across these domains. This paper reviews how abductive reasoning is discussed in epistemology, science and design, and then analyses how various computational systems use abductive reasoning. Our analysis shows that neither theoretical accounts nor computational implementations of abductive reasoning adequately address generating creative hypotheses. Theoretical frameworks do not provide a straightforward model for generating creative abductive hypotheses, computational systems largely implement syllogistic forms of abductive reasoning. We break down abductive computational systems into components and conclude by identifying specific directions for future research that could advance the state of creative abductive reasoning in computational systems.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "Published in the 16th International Conference on Computational Creativity, ICCC25. Accepted Paper in https://computationalcreativity.net/iccc25/wp-content/uploads/papers/iccc25-sood2025abductive.pdf",
    "pdf_url": "https://arxiv.org/pdf/2507.08264v1",
    "published_date": "2025-07-11 02:21:41 UTC",
    "updated_date": "2025-07-11 02:21:41 UTC"
  },
  {
    "arxiv_id": "2507.08262v1",
    "title": "CL3R: 3D Reconstruction and Contrastive Learning for Enhanced Robotic Manipulation Representations",
    "authors": [
      "Wenbo Cui",
      "Chengyang Zhao",
      "Yuhui Chen",
      "Haoran Li",
      "Zhizheng Zhang",
      "Dongbin Zhao",
      "He Wang"
    ],
    "abstract": "Building a robust perception module is crucial for visuomotor policy learning. While recent methods incorporate pre-trained 2D foundation models into robotic perception modules to leverage their strong semantic understanding, they struggle to capture 3D spatial information and generalize across diverse camera viewpoints. These limitations hinder the policy's effectiveness, especially in fine-grained robotic manipulation scenarios. To address these challenges, we propose CL3R, a novel 3D pre-training framework designed to enhance robotic manipulation policies. Our method integrates both spatial awareness and semantic understanding by employing a point cloud Masked Autoencoder to learn rich 3D representations while leveraging pre-trained 2D foundation models through contrastive learning for efficient semantic knowledge transfer. Additionally, we propose a 3D visual representation pre-training framework for robotic tasks. By unifying coordinate systems across datasets and introducing random fusion of multi-view point clouds, we mitigate camera view ambiguity and improve generalization, enabling robust perception from novel viewpoints at test time. Extensive experiments in both simulation and the real world demonstrate the superiority of our method, highlighting its effectiveness in visuomotor policy learning for robotic manipulation.",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.RO",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2507.08262v1",
    "published_date": "2025-07-11 02:16:32 UTC",
    "updated_date": "2025-07-11 02:16:32 UTC"
  },
  {
    "arxiv_id": "2507.08255v1",
    "title": "Quantum-Accelerated Neural Imputation with Large Language Models (LLMs)",
    "authors": [
      "Hossein Jamali"
    ],
    "abstract": "Missing data presents a critical challenge in real-world datasets, significantly degrading the performance of machine learning models. While Large Language Models (LLMs) have recently demonstrated remarkable capabilities in tabular data imputation, exemplified by frameworks like UnIMP, their reliance on classical embedding methods often limits their ability to capture complex, non-linear correlations, particularly in mixed-type data scenarios encompassing numerical, categorical, and textual features. This paper introduces Quantum-UnIMP, a novel framework that integrates shallow quantum circuits into an LLM-based imputation architecture. Our core innovation lies in replacing conventional classical input embeddings with quantum feature maps generated by an Instantaneous Quantum Polynomial (IQP) circuit. This approach enables the model to leverage quantum phenomena such as superposition and entanglement, thereby learning richer, more expressive representations of data and enhancing the recovery of intricate missingness patterns. Our experiments on benchmark mixed-type datasets demonstrate that Quantum-UnIMP reduces imputation error by up to 15.2% for numerical features (RMSE) and improves classification accuracy by 8.7% for categorical features (F1-Score) compared to state-of-the-art classical and LLM-based methods. These compelling results underscore the profound potential of quantum-enhanced representations for complex data imputation tasks, even with near-term quantum hardware.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2507.08255v1",
    "published_date": "2025-07-11 02:00:06 UTC",
    "updated_date": "2025-07-11 02:00:06 UTC"
  },
  {
    "arxiv_id": "2507.14177v1",
    "title": "Understanding Two-Layer Neural Networks with Smooth Activation Functions",
    "authors": [
      "Changcun Huang"
    ],
    "abstract": "This paper aims to understand the training solution, which is obtained by the back-propagation algorithm, of two-layer neural networks whose hidden layer is composed of the units with smooth activation functions, including the usual sigmoid type most commonly used before the advent of ReLUs. The mechanism contains four main principles: construction of Taylor series expansions, strict partial order of knots, smooth-spline implementation and smooth-continuity restriction. The universal approximation for arbitrary input dimensionality is proved and experimental verification is given, through which the mystery of ``black box'' of the solution space is largely revealed. The new proofs employed also enrich approximation theory.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "math.NA"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2507.14177v1",
    "published_date": "2025-07-11 01:55:07 UTC",
    "updated_date": "2025-07-11 01:55:07 UTC"
  },
  {
    "arxiv_id": "2507.08249v1",
    "title": "Giving AI Agents Access to Cryptocurrency and Smart Contracts Creates New Vectors of AI Harm",
    "authors": [
      "Bill Marino",
      "Ari Juels"
    ],
    "abstract": "There is growing interest in giving AI agents access to cryptocurrencies as well as to the smart contracts that transact them. But doing so, this position paper argues, could lead to formidable new vectors of AI harm. To support this argument, we first examine the unique properties of cryptocurrencies and smart contracts that could lead to these new vectors of harm. Next, we describe each of these new vectors of harm in detail. Finally, we conclude with a call for more technical research aimed at preventing and mitigating these harms and, thereby making it safer to endow AI agents with cryptocurrencies and smart contracts.",
    "categories": [
      "cs.AI",
      "cs.CR"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2507.08249v1",
    "published_date": "2025-07-11 01:22:02 UTC",
    "updated_date": "2025-07-11 01:22:02 UTC"
  },
  {
    "arxiv_id": "2507.08235v1",
    "title": "InsightBuild: LLM-Powered Causal Reasoning in Smart Building Systems",
    "authors": [
      "Pinaki Prasad Guha Neogi",
      "Ahmad Mohammadshirazi",
      "Rajiv Ramnath"
    ],
    "abstract": "Smart buildings generate vast streams of sensor and control data, but facility managers often lack clear explanations for anomalous energy usage. We propose InsightBuild, a two-stage framework that integrates causality analysis with a fine-tuned large language model (LLM) to provide human-readable, causal explanations of energy consumption patterns. First, a lightweight causal inference module applies Granger causality tests and structural causal discovery on building telemetry (e.g., temperature, HVAC settings, occupancy) drawn from Google Smart Buildings and Berkeley Office datasets. Next, an LLM, fine-tuned on aligned pairs of sensor-level causes and textual explanations, receives as input the detected causal relations and generates concise, actionable explanations. We evaluate InsightBuild on two real-world datasets (Google: 2017-2022; Berkeley: 2018-2020), using expert-annotated ground-truth causes for a held-out set of anomalies. Our results demonstrate that combining explicit causal discovery with LLM-based natural language generation yields clear, precise explanations that assist facility managers in diagnosing and mitigating energy inefficiencies.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2507.08235v1",
    "published_date": "2025-07-11 00:45:16 UTC",
    "updated_date": "2025-07-11 00:45:16 UTC"
  },
  {
    "arxiv_id": "2507.08232v1",
    "title": "Can LLMs Reliably Simulate Real Students' Abilities in Mathematics and Reading Comprehension?",
    "authors": [
      "KV Aditya Srivatsa",
      "Kaushal Kumar Maurya",
      "Ekaterina Kochmar"
    ],
    "abstract": "Large Language Models (LLMs) are increasingly used as proxy students in the development of Intelligent Tutoring Systems (ITSs) and in piloting test questions. However, to what extent these proxy students accurately emulate the behavior and characteristics of real students remains an open question. To investigate this, we collected a dataset of 489 items from the National Assessment of Educational Progress (NAEP), covering mathematics and reading comprehension in grades 4, 8, and 12. We then apply an Item Response Theory (IRT) model to position 11 diverse and state-of-the-art LLMs on the same ability scale as real student populations. Our findings reveal that, without guidance, strong general-purpose models consistently outperform the average student at every grade, while weaker or domain-mismatched models may align incidentally. Using grade-enforcement prompts changes models' performance, but whether they align with the average grade-level student remains highly model- and prompt-specific: no evaluated model-prompt pair fits the bill across subjects and grades, underscoring the need for new training and evaluation strategies. We conclude by providing guidelines for the selection of viable proxies based on our findings.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted to the 20th Workshop on Innovative Use of NLP for Building Educational Applications (BEA), co-located with ACL 2025",
    "pdf_url": "https://arxiv.org/pdf/2507.08232v1",
    "published_date": "2025-07-11 00:36:57 UTC",
    "updated_date": "2025-07-11 00:36:57 UTC"
  }
]
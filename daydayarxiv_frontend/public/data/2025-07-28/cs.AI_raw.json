[
  {
    "arxiv_id": "2507.21391v2",
    "title": "Multimodal LLMs as Customized Reward Models for Text-to-Image Generation",
    "authors": [
      "Shijie Zhou",
      "Ruiyi Zhang",
      "Huaisheng Zhu",
      "Branislav Kveton",
      "Yufan Zhou",
      "Jiuxiang Gu",
      "Jian Chen",
      "Changyou Chen"
    ],
    "abstract": "We introduce LLaVA-Reward, an efficient reward model designed to automatically evaluate text-to-image (T2I) generations across multiple perspectives, leveraging pretrained multimodal large language models (MLLMs). Existing MLLM-based approaches require instruction-following data for supervised fine-tuning and evaluate generation quality on analyzing text response, which is time-consuming and difficult to train. To address this problem, we propose LLaVA-Reward, which directly utilizes the hidden states of MLLMs given text-image pairs. To enhance the bidirectional interaction between visual and textual representations in decoder-only MLLMs, we further propose adding a Skip-connection Cross Attention (SkipCA) module. This design enhances text-image correlation reasoning by connecting early-layer visual features with later-layer hidden representations. In addition, LLaVA-Reward supports different types of preference data for efficient fine-tuning, including paired preference data and unpaired data. We train LLaVA-Reward on four evaluation perspectives: text-image alignment, fidelity/artifact, safety, and overall ranking. Empirical results demonstrate that LLaVA-Reward outperforms conventional and MLLM-based methods in generating human-aligned scores for automatic evaluations and inference-time scaling in text-to-image generations.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.CV",
    "comment": "Accepted at ICCV 2025. Code available at https://github.com/sjz5202/LLaVA-Reward",
    "pdf_url": "https://arxiv.org/pdf/2507.21391v2",
    "published_date": "2025-07-28 23:52:53 UTC",
    "updated_date": "2025-07-30 04:49:38 UTC"
  },
  {
    "arxiv_id": "2507.21389v1",
    "title": "Teaching Language Models To Gather Information Proactively",
    "authors": [
      "Tenghao Huang",
      "Sihao Chen",
      "Muhao Chen",
      "Jonathan May",
      "Longqi Yang",
      "Mengting Wan",
      "Pei Zhou"
    ],
    "abstract": "Large language models (LLMs) are increasingly expected to function as collaborative partners, engaging in back-and-forth dialogue to solve complex, ambiguous problems. However, current LLMs often falter in real-world settings, defaulting to passive responses or narrow clarifications when faced with incomplete or under-specified prompts, falling short of proactively gathering the missing information that is crucial for high-quality solutions. In this work, we introduce a new task paradigm: proactive information gathering, where LLMs must identify gaps in the provided context and strategically elicit implicit user knowledge through targeted questions. To systematically study and train this capability, we design a scalable framework that generates partially specified, real-world tasks, masking key information and simulating authentic ambiguity. Within this setup, our core innovation is a reinforcement finetuning strategy that rewards questions that elicit genuinely new, implicit user information -- such as hidden domain expertise or fine-grained requirements -- that would otherwise remain unspoken. Experiments demonstrate that our trained Qwen-2.5-7B model significantly outperforms o3-mini by 18% on automatic evaluation metrics. More importantly, human evaluation reveals that clarification questions and final outlines generated by our model are favored by human annotators by 42% and 28% respectively. Together, these results highlight the value of proactive clarification in elevating LLMs from passive text generators to genuinely collaborative thought partners.",
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2507.21389v1",
    "published_date": "2025-07-28 23:50:09 UTC",
    "updated_date": "2025-07-28 23:50:09 UTC"
  },
  {
    "arxiv_id": "2507.21386v2",
    "title": "Efficient Neural Combinatorial Optimization Solver for the Min-max Heterogeneous Capacitated Vehicle Routing Problem",
    "authors": [
      "Xuan Wu",
      "Di Wang",
      "Chunguo Wu",
      "Kaifang Qi",
      "Chunyan Miao",
      "Yubin Xiao",
      "Jian Zhang",
      "You Zhou"
    ],
    "abstract": "Numerous Neural Combinatorial Optimization (NCO) solvers have been proposed to address Vehicle Routing Problems (VRPs). However, most of these solvers focus exclusively on single-vehicle VRP variants, overlooking the more realistic min-max Heterogeneous Capacitated Vehicle Routing Problem (MMHCVRP), which involves multiple vehicles. Existing MMHCVRP solvers typically select a vehicle and its next node to visit at each decoding step, but often make myopic decoding decisions and overlook key properties of MMHCVRP, including local topological relationships, vehicle permutation invariance, and node symmetry, resulting in suboptimal performance. To better address these limitations, we propose ECHO, an efficient NCO solver. First, ECHO exploits the proposed dual-modality node encoder to capture local topological relationships among nodes. Subsequently, to mitigate myopic decisions, ECHO employs the proposed Parameter-Free Cross-Attention mechanism to prioritize the vehicle selected in the preceding decoding step. Finally, leveraging vehicle permutation invariance and node symmetry, we introduce a tailored data augment strategy for MMHCVRP to stabilize the Reinforcement Learning training process. To assess the performance of ECHO, we conduct extensive experiments. The experimental results demonstrate that ECHO outperforms state-of-the-art NCO solvers across varying numbers of vehicles and nodes, and exhibits well-performing generalization across both scales and distribution patterns. Finally, ablation studies validate the effectiveness of all proposed methods.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2507.21386v2",
    "published_date": "2025-07-28 23:38:33 UTC",
    "updated_date": "2025-12-26 07:45:10 UTC"
  },
  {
    "arxiv_id": "2507.21385v1",
    "title": "Deep Reinforcement Learning-based Cell DTX/DRX Configuration for Network Energy Saving",
    "authors": [
      "Wei Mao",
      "Lili Wei",
      "Omid Semiari",
      "Shu-ping Yeh",
      "Hosein Nikopour"
    ],
    "abstract": "3GPP Release 18 cell discontinuous transmission and reception (cell DTX/DRX) is an important new network energy saving feature for 5G. As a time-domain technique, it periodically aggregates the user data transmissions in a given duration of time when the traffic load is not heavy, so that the remaining time can be kept silent and advanced sleep modes (ASM) can be enabled to shut down more radio components and save more energy for the cell. However, inevitably the packet delay is increased, as during the silent period no transmission is allowed. In this paper we study how to configure cell DTX/DRX to optimally balance energy saving and packet delay, so that for delay-sensitive traffic maximum energy saving can be achieved while the degradation of quality of service (QoS) is minimized. As the optimal configuration can be different for different network and traffic conditions, the problem is complex and we resort to deep reinforcement learning (DRL) framework to train an AI agent to solve it. Through careful design of 1) the learning algorithm, which implements a deep Q-network (DQN) on a contextual bandit (CB) model, and 2) the reward function, which utilizes a smooth approximation of a theoretically optimal but discontinuous reward function, we are able to train an AI agent that always tries to select the best possible Cell DTX/DRX configuration under any network and traffic conditions. Simulation results show that compared to the case when cell DTX/DRX is not used, our agent can achieve up to ~45% energy saving depending on the traffic load scenario, while always maintaining no more than ~1% QoS degradation.",
    "categories": [
      "cs.NI",
      "cs.AI"
    ],
    "primary_category": "cs.NI",
    "comment": "7 pages, 7 figures",
    "pdf_url": "https://arxiv.org/pdf/2507.21385v1",
    "published_date": "2025-07-28 23:35:24 UTC",
    "updated_date": "2025-07-28 23:35:24 UTC"
  },
  {
    "arxiv_id": "2507.21383v2",
    "title": "Optimizing Multi-Tier Supply Chain Ordering with LNN+XGBoost: Mitigating the Bullwhip Effect",
    "authors": [
      "Chunan Tong"
    ],
    "abstract": "Supply chain management faces significant challenges, including demand fluctuations, inventory imbalances, and amplified upstream order variability due to the bullwhip effect. Traditional methods, such as simple moving averages, struggle to address dynamic market conditions. Emerging machine learning techniques, including LSTM, reinforcement learning, and XGBoost, offer potential solutions but are limited by computational complexity, training inefficiencies, or constraints in time-series modeling. Liquid Neural Networks, inspired by dynamic biological systems, present a promising alternative due to their adaptability, low computational cost, and robustness to noise, making them suitable for real-time decision-making and edge computing. Despite their success in applications like autonomous vehicles and medical monitoring, their potential in supply chain optimization remains underexplored. This study introduces a hybrid LNN and XGBoost model to optimize ordering strategies in multi-tier supply chains. By leveraging LNN's dynamic feature extraction and XGBoost's global optimization capabilities, the model aims to mitigate the bullwhip effect and enhance cumulative profitability. The research investigates how local and global synergies within the hybrid framework address the dual demands of adaptability and efficiency in SCM. The proposed approach fills a critical gap in existing methodologies, offering an innovative solution for dynamic and efficient supply chain management.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2507.21383v2",
    "published_date": "2025-07-28 23:24:54 UTC",
    "updated_date": "2025-11-14 08:21:25 UTC"
  },
  {
    "arxiv_id": "2507.21382v1",
    "title": "MAAD: Automate Software Architecture Design through Knowledge-Driven Multi-Agent Collaboration",
    "authors": [
      "Ruiyin Li",
      "Yiran Zhang",
      "Xiyu Zhou",
      "Peng Liang",
      "Weisong Sun",
      "Jifeng Xuan",
      "Zhi Jin",
      "Yang Liu"
    ],
    "abstract": "Software architecture design is a critical, yet inherently complex and knowledge-intensive phase of software development. It requires deep domain expertise, development experience, architectural knowledge, careful trade-offs among competing quality attributes, and the ability to adapt to evolving requirements. Traditionally, this process is time-consuming and labor-intensive, and relies heavily on architects, often resulting in limited design alternatives, especially under the pressures of agile development. While Large Language Model (LLM)-based agents have shown promising performance across various SE tasks, their application to architecture design remains relatively scarce and requires more exploration, particularly in light of diverse domain knowledge and complex decision-making. To address the challenges, we proposed MAAD (Multi-Agent Architecture Design), an automated framework that employs a knowledge-driven Multi-Agent System (MAS) for architecture design. MAAD orchestrates four specialized agents (i.e., Analyst, Modeler, Designer and Evaluator) to collaboratively interpret requirements specifications and produce architectural blueprints enriched with quality attributes-based evaluation reports. We then evaluated MAAD through a case study and comparative experiments against MetaGPT, a state-of-the-art MAS baseline. Our results show that MAAD's superiority lies in generating comprehensive architectural components and delivering insightful and structured architecture evaluation reports. Feedback from industrial architects across 11 requirements specifications further reinforces MAAD's practical usability. We finally explored the performance of the MAAD framework with three LLMs (GPT-4o, DeepSeek-R1, and Llama 3.3) and found that GPT-4o exhibits better performance in producing architecture design, emphasizing the importance of LLM selection in MAS-driven architecture design.",
    "categories": [
      "cs.SE",
      "cs.AI"
    ],
    "primary_category": "cs.SE",
    "comment": "23 pages, 8 images, 1 table, Manuscript submitted to a journal (2025)",
    "pdf_url": "https://arxiv.org/pdf/2507.21382v1",
    "published_date": "2025-07-28 23:18:25 UTC",
    "updated_date": "2025-07-28 23:18:25 UTC"
  },
  {
    "arxiv_id": "2507.21378v1",
    "title": "ProMemAssist: Exploring Timely Proactive Assistance Through Working Memory Modeling in Multi-Modal Wearable Devices",
    "authors": [
      "Kevin Pu",
      "Ting Zhang",
      "Naveen Sendhilnathan",
      "Sebastian Freitag",
      "Raj Sodhi",
      "Tanya Jonker"
    ],
    "abstract": "Wearable AI systems aim to provide timely assistance in daily life, but existing approaches often rely on user initiation or predefined task knowledge, neglecting users' current mental states. We introduce ProMemAssist, a smart glasses system that models a user's working memory (WM) in real-time using multi-modal sensor signals. Grounded in cognitive theories of WM, our system represents perceived information as memory items and episodes with encoding mechanisms, such as displacement and interference. This WM model informs a timing predictor that balances the value of assistance with the cost of interruption. In a user study with 12 participants completing cognitively demanding tasks, ProMemAssist delivered more selective assistance and received higher engagement compared to an LLM baseline system. Qualitative feedback highlights the benefits of WM modeling for nuanced, context-sensitive support, offering design implications for more attentive and user-aware proactive agents.",
    "categories": [
      "cs.HC",
      "cs.AI"
    ],
    "primary_category": "cs.HC",
    "comment": "Accepted to UIST'25",
    "pdf_url": "https://arxiv.org/pdf/2507.21378v1",
    "published_date": "2025-07-28 23:02:47 UTC",
    "updated_date": "2025-07-28 23:02:47 UTC"
  },
  {
    "arxiv_id": "2508.00903v2",
    "title": "Universal Neurons in GPT-2: Emergence, Persistence, and Functional Impact",
    "authors": [
      "Advey Nandan",
      "Cheng-Ting Chou",
      "Amrit Kurakula",
      "Cole Blondin",
      "Kevin Zhu",
      "Vasu Sharma",
      "Sean O'Brien"
    ],
    "abstract": "We investigate the phenomenon of neuron universality in independently trained GPT-2 Small models, examining these universal neurons-neurons with consistently correlated activations across models-emerge and evolve throughout training. By analyzing five GPT-2 models at five checkpoints, we identify universal neurons through pairwise correlation analysis of activations over a dataset of 5 million tokens. Ablation experiments reveal significant functional impacts of universal neurons on model predictions, measured via cross entropy loss. Additionally, we quantify neuron persistence, demonstrating high stability of universal neurons across training checkpoints, particularly in early and deeper layers. These findings suggest stable and universal representational structures emerge during language model training.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.NE"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2508.00903v2",
    "published_date": "2025-07-28 22:45:33 UTC",
    "updated_date": "2025-11-09 03:00:24 UTC"
  },
  {
    "arxiv_id": "2507.21364v1",
    "title": "Evaluating Deep Learning Models for African Wildlife Image Classification: From DenseNet to Vision Transformers",
    "authors": [
      "Lukman Jibril Aliyu",
      "Umar Sani Muhammad",
      "Bilqisu Ismail",
      "Nasiru Muhammad",
      "Almustapha A Wakili",
      "Seid Muhie Yimam",
      "Shamsuddeen Hassan Muhammad",
      "Mustapha Abdullahi"
    ],
    "abstract": "Wildlife populations in Africa face severe threats, with vertebrate numbers declining by over 65% in the past five decades. In response, image classification using deep learning has emerged as a promising tool for biodiversity monitoring and conservation. This paper presents a comparative study of deep learning models for automatically classifying African wildlife images, focusing on transfer learning with frozen feature extractors. Using a public dataset of four species: buffalo, elephant, rhinoceros, and zebra; we evaluate the performance of DenseNet-201, ResNet-152, EfficientNet-B4, and Vision Transformer ViT-H/14. DenseNet-201 achieved the best performance among convolutional networks (67% accuracy), while ViT-H/14 achieved the highest overall accuracy (99%), but with significantly higher computational cost, raising deployment concerns. Our experiments highlight the trade-offs between accuracy, resource requirements, and deployability. The best-performing CNN (DenseNet-201) was integrated into a Hugging Face Gradio Space for real-time field use, demonstrating the feasibility of deploying lightweight models in conservation settings. This work contributes to African-grounded AI research by offering practical insights into model selection, dataset preparation, and responsible deployment of deep learning tools for wildlife conservation.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "Accepted as a camera-ready paper at Deep Learning Indaba 2025 (Kigali, Rwanda)",
    "pdf_url": "https://arxiv.org/pdf/2507.21364v1",
    "published_date": "2025-07-28 22:18:13 UTC",
    "updated_date": "2025-07-28 22:18:13 UTC"
  },
  {
    "arxiv_id": "2507.21360v1",
    "title": "Efficacy of AI RAG Tools for Complex Information Extraction and Data Annotation Tasks: A Case Study Using Banks Public Disclosures",
    "authors": [
      "Nicholas Botti",
      "Flora Haberkorn",
      "Charlotte Hoopes",
      "Shaun Khan"
    ],
    "abstract": "We utilize a within-subjects design with randomized task assignments to understand the effectiveness of using an AI retrieval augmented generation (RAG) tool to assist analysts with an information extraction and data annotation task. We replicate an existing, challenging real-world annotation task with complex multi-part criteria on a set of thousands of pages of public disclosure documents from global systemically important banks (GSIBs) with heterogeneous and incomplete information content. We test two treatment conditions. First, a \"naive\" AI use condition in which annotators use only the tool and must accept the first answer they are given. And second, an \"interactive\" AI treatment condition where annotators use the tool interactively, and use their judgement to follow-up with additional information if necessary. Compared to the human-only baseline, the use of the AI tool accelerated task execution by up to a factor of 10 and enhanced task accuracy, particularly in the interactive condition. We find that when extrapolated to the full task, these methods could save up to 268 hours compared to the human-only approach. Additionally, our findings suggest that annotator skill, not just with the subject matter domain, but also with AI tools, is a factor in both the accuracy and speed of task performance.",
    "categories": [
      "cs.AI",
      "cs.HC",
      "econ.GN"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2507.21360v1",
    "published_date": "2025-07-28 22:06:11 UTC",
    "updated_date": "2025-07-28 22:06:11 UTC"
  },
  {
    "arxiv_id": "2507.21354v1",
    "title": "Games Agents Play: Towards Transactional Analysis in LLM-based Multi-Agent Systems",
    "authors": [
      "Monika Zamojska",
      "Jarosław A. Chudziak"
    ],
    "abstract": "Multi-Agent Systems (MAS) are increasingly used to simulate social interactions, but most of the frameworks miss the underlying cognitive complexity of human behavior. In this paper, we introduce Trans-ACT (Transactional Analysis Cognitive Toolkit), an approach embedding Transactional Analysis (TA) principles into MAS to generate agents with realistic psychological dynamics. Trans-ACT integrates the Parent, Adult, and Child ego states into an agent's cognitive architecture. Each ego state retrieves context-specific memories and uses them to shape response to new situations. The final answer is chosen according to the underlying life script of the agent. Our experimental simulation, which reproduces the Stupid game scenario, demonstrates that agents grounded in cognitive and TA principles produce deeper and context-aware interactions. Looking ahead, our research opens a new way for a variety of applications, including conflict resolution, educational support, and advanced social psychology studies.",
    "categories": [
      "cs.AI",
      "cs.MA"
    ],
    "primary_category": "cs.AI",
    "comment": "Proceedings of the Annual Meeting of the Cognitive Science Society (CogSci 2025), https://escholarship.org/uc/item/7gg6j165",
    "pdf_url": "https://arxiv.org/pdf/2507.21354v1",
    "published_date": "2025-07-28 21:46:21 UTC",
    "updated_date": "2025-07-28 21:46:21 UTC"
  },
  {
    "arxiv_id": "2508.00024v2",
    "title": "Embedding-Aware Quantum-Classical SVMs for Scalable Quantum Machine Learning",
    "authors": [
      "Sebastián Andrés Cajas Ordóñez",
      "Luis Fernando Torres Torres",
      "Mario Bifulco",
      "Carlos Andrés Durán",
      "Cristian Bosch",
      "Ricardo Simón Carbajo"
    ],
    "abstract": "Quantum Support Vector Machines face scalability challenges due to high-dimensional quantum states and hardware limitations. We propose an embedding-aware quantum-classical pipeline combining class-balanced k-means distillation with pretrained Vision Transformer embeddings. Our key finding: ViT embeddings uniquely enable quantum advantage, achieving up to 8.02% accuracy improvements over classical SVMs on Fashion-MNIST and 4.42% on MNIST, while CNN features show performance degradation. Using 16-qubit tensor network simulation via cuTensorNet, we provide the first systematic evidence that quantum kernel advantage depends critically on embedding choice, revealing fundamental synergy between transformer attention and quantum feature spaces. This provides a practical pathway for scalable quantum machine learning that leverages modern neural architectures.",
    "categories": [
      "quant-ph",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "quant-ph",
    "comment": "Accepted for Poster, Presentation and Proceedings at: 3rd International Workshop on AI for Quantum and Quantum for AI (AIQxQIA 2025), co-located with ECAI 2025, Bologna, Italy, 25-30 October 2025",
    "pdf_url": "https://arxiv.org/pdf/2508.00024v2",
    "published_date": "2025-07-28 21:23:51 UTC",
    "updated_date": "2025-11-10 18:08:37 UTC"
  },
  {
    "arxiv_id": "2507.21340v1",
    "title": "StructText: A Synthetic Table-to-Text Approach for Benchmark Generation with Multi-Dimensional Evaluation",
    "authors": [
      "Satyananda Kashyap",
      "Sola Shirai",
      "Nandana Mihindukulasooriya",
      "Horst Samulowitz"
    ],
    "abstract": "Extracting structured information from text, such as key-value pairs that could augment tabular data, is quite useful in many enterprise use cases. Although large language models (LLMs) have enabled numerous automated pipelines for converting natural language into structured formats, there is still a lack of benchmarks for evaluating their extraction quality, especially in specific domains or focused documents specific to a given organization. Building such benchmarks by manual annotations is labour-intensive and limits the size and scalability of the benchmarks. In this work, we present StructText, an end-to-end framework for automatically generating high-fidelity benchmarks for key-value extraction from text using existing tabular data. It uses available tabular data as structured ground truth, and follows a two-stage ``plan-then-execute'' pipeline to synthetically generate corresponding natural-language text. To ensure alignment between text and structured source, we introduce a multi-dimensional evaluation strategy that combines (a) LLM-based judgments on factuality, hallucination, and coherence and (b) objective extraction metrics measuring numeric and temporal accuracy. We evaluated the proposed method on 71,539 examples across 49 datasets. Results reveal that while LLMs achieve strong factual accuracy and avoid hallucination, they struggle with narrative coherence in producing extractable text. Notably, models presume numerical and temporal information with high fidelity yet this information becomes embedded in narratives that resist automated extraction. We release a framework, including datasets, evaluation tools, and baseline extraction systems, to support continued research.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.DB",
      "cs.IR"
    ],
    "primary_category": "cs.CL",
    "comment": "Data available: https://huggingface.co/datasets/ibm-research/struct-text and code available at: https://github.com/ibm/struct-text",
    "pdf_url": "https://arxiv.org/pdf/2507.21340v1",
    "published_date": "2025-07-28 21:20:44 UTC",
    "updated_date": "2025-07-28 21:20:44 UTC"
  },
  {
    "arxiv_id": "2508.00902v1",
    "title": "An analysis of AI Decision under Risk: Prospect theory emerges in Large Language Models",
    "authors": [
      "Kenneth Payne"
    ],
    "abstract": "Judgment of risk is key to decision-making under uncertainty. As Daniel Kahneman and Amos Tversky famously discovered, humans do so in a distinctive way that departs from mathematical rationalism. Specifically, they demonstrated experimentally that humans accept more risk when they feel themselves at risk of losing something than when they might gain. I report the first tests of Kahneman and Tversky's landmark 'prospect theory' with Large Language Models, including today's state of the art chain-of-thought 'reasoners'.\n  In common with humans, I find that prospect theory often anticipates how these models approach risky decisions across a range of scenarios. I also demonstrate that context is key to explaining much of the variance in risk appetite. The 'frame' through which risk is apprehended appears to be embedded within the language of the scenarios tackled by the models. Specifically, I find that military scenarios generate far larger 'framing effects' than do civilian settings, ceteris paribus. My research suggests, therefore, that language models the world, capturing our human heuristics and biases. But also that these biases are uneven - the idea of a 'frame' is richer than simple gains and losses. Wittgenstein's notion of 'language games' explains the contingent, localised biases activated by these scenarios. Finally, I use my findings to reframe the ongoing debate about reasoning and memorisation in LLMs.",
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.CY"
    ],
    "primary_category": "cs.AI",
    "comment": "26 pages, 2 figures, 9 tables, 2 appendices",
    "pdf_url": "https://arxiv.org/pdf/2508.00902v1",
    "published_date": "2025-07-28 21:15:57 UTC",
    "updated_date": "2025-07-28 21:15:57 UTC"
  },
  {
    "arxiv_id": "2507.21295v1",
    "title": "Semantic Numeration Systems as Dynamical Systems",
    "authors": [
      "Alexander Yu. Chunikhin"
    ],
    "abstract": "The foundational concepts of semantic numeration systems theory are briefly outlined. The action of cardinal semantic operators unfolds over a set of cardinal abstract entities belonging to the cardinal semantic multeity. The cardinal abstract object (CAO) formed by them in a certain connectivity topology is proposed to be considered as a linear discrete dynamical system with nonlinear control. Under the assumption of ideal observability, the CAO state equations are provided for both stationary and non-stationary cases. The fundamental role of the configuration matrix, which combines information about the types of cardinal semantic operators in the CAO, their parameters and topology of connectivity, is demonstrated.",
    "categories": [
      "cs.LO",
      "cs.AI"
    ],
    "primary_category": "cs.LO",
    "comment": "11 pages, 6 figures",
    "pdf_url": "https://arxiv.org/pdf/2507.21295v1",
    "published_date": "2025-07-28 19:29:36 UTC",
    "updated_date": "2025-07-28 19:29:36 UTC"
  },
  {
    "arxiv_id": "2507.21288v2",
    "title": "Learning Simulatable Models of Cloth with Spatially-varying Constitutive Properties",
    "authors": [
      "Guanxiong Chen",
      "Shashwat Suri",
      "Yuhao Wu",
      "Etienne Voulga",
      "David I. W. Levin",
      "Dinesh K. Pai"
    ],
    "abstract": "Materials used in real clothing exhibit remarkable complexity and spatial variation due to common processes such as stitching, hemming, dyeing, printing, padding, and bonding. Simulating these materials, for instance using finite element methods, is often computationally demanding and slow. Worse, such methods can suffer from numerical artifacts called ``membrane locking'' that makes cloth appear artificially stiff. Here we propose a general framework, called Mass-Spring Net, for learning a simple yet efficient surrogate model that captures the effects of these complex materials using only motion observations. The cloth is discretized into a mass-spring network with unknown material parameters that are learned directly from the motion data, using a novel force-and-impulse loss function. Our approach demonstrates the ability to accurately model spatially varying material properties from a variety of data sources, and immunity to membrane locking which plagues FEM-based simulations. Compared to graph-based networks and neural ODE-based architectures, our method achieves significantly faster training times, higher reconstruction accuracy, and improved generalization to novel dynamic scenarios.",
    "categories": [
      "cs.GR",
      "cs.AI"
    ],
    "primary_category": "cs.GR",
    "comment": "Added middle name of Prof. Pai",
    "pdf_url": "https://arxiv.org/pdf/2507.21288v2",
    "published_date": "2025-07-28 19:21:04 UTC",
    "updated_date": "2025-07-30 18:05:08 UTC"
  },
  {
    "arxiv_id": "2507.21287v1",
    "title": "Structured Relevance Assessment for Robust Retrieval-Augmented Language Models",
    "authors": [
      "Aryan Raj",
      "Astitva Veer Garg",
      "Anitha D"
    ],
    "abstract": "Retrieval-Augmented Language Models (RALMs) face significant challenges in reducing factual errors, particularly in document relevance evaluation and knowledge integration. We introduce a framework for structured relevance assessment that enhances RALM robustness through improved document evaluation, balanced intrinsic and external knowledge integration, and effective handling of unanswerable queries. Our approach employs a multi-dimensional scoring system that considers both semantic matching and source reliability, utilizing embedding-based relevance scoring and synthetic training data with mixed-quality documents. We implement specialized benchmarking on niche topics, a knowledge integration mechanism, and an \"unknown\" response protocol for queries with insufficient knowledge coverage. Preliminary evaluations demonstrate significant reductions in hallucination rates and improved transparency in reasoning processes. Our framework advances the development of more reliable question-answering systems capable of operating effectively in dynamic environments with variable data quality. While challenges persist in accurately distinguishing credible information and balancing system latency with thoroughness, this work represents a meaningful step toward enhancing RALM reliability.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "International Conference on ICT for Sustainable Development (ICT4SD)",
    "pdf_url": "https://arxiv.org/pdf/2507.21287v1",
    "published_date": "2025-07-28 19:20:04 UTC",
    "updated_date": "2025-07-28 19:20:04 UTC"
  },
  {
    "arxiv_id": "2507.21285v1",
    "title": "Curiosity by Design: An LLM-based Coding Assistant Asking Clarification Questions",
    "authors": [
      "Harsh Darji",
      "Thibaud Lutellier"
    ],
    "abstract": "Large Language Models (LLMs) are increasingly used as coding assistants. However, the ambiguity of the developer's prompt often leads to incorrect code generation, as current models struggle to infer user intent without extensive prompt engineering or external context. This work aims to build an LLM-based coding assistant that mimics the human code review process by asking clarification questions when faced with ambiguous or under-specified queries.\n  Our end-to-end system includes (1) a query classifier trained to detect unclear programming-related queries and (2) a fine-tuned LLM that generates clarification questions. Our evaluation shows that the fine-tuned LLM outperforms standard zero-shot prompting in generating useful clarification questions. Furthermore, our user study indicates that users find the clarification questions generated by our model to outperform the baseline, demonstrating that our coding assistant produces more accurate and helpful code responses compared to baseline coding assistants.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2507.21285v1",
    "published_date": "2025-07-28 19:10:57 UTC",
    "updated_date": "2025-07-28 19:10:57 UTC"
  },
  {
    "arxiv_id": "2507.21276v1",
    "title": "LeMix: Unified Scheduling for LLM Training and Inference on Multi-GPU Systems",
    "authors": [
      "Yufei Li",
      "Zexin Li",
      "Yinglun Zhu",
      "Cong Liu"
    ],
    "abstract": "Modern deployment of large language models (LLMs) frequently involves both inference serving and continuous retraining to stay aligned with evolving data and user feedback. Common practices separate these workloads onto distinct servers in isolated phases, causing substantial inefficiencies (e.g., GPU idleness) and delayed adaptation to new data in distributed settings. Our empirical analysis reveals that these inefficiencies stem from dynamic request arrivals during serving and workload heterogeneity in pipeline-parallel training. To address these challenges, we propose LeMix, a system for co-locating and managing concurrent LLM serving and training workloads. LeMix integrates offline profiling, execution prediction mechanisms, and runtime scheduling to dynamically adapt resource allocation based on workload characteristics and system conditions. By understanding task-specific behaviors and co-execution interference across shared nodes, LeMix improves utilization and serving quality without compromising serving responsiveness. Our evaluation shows that LeMix improves throughput by up to 3.53x, reduces inference loss by up to 0.61x, and delivers up to 2.12x higher response time SLO attainment over traditional separate setups. To our knowledge, this is the first work to uncover and exploit the opportunities of joint LLM inference and training, paving the way for more resource-efficient deployment of LLMs in production environments.",
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.DC"
    ],
    "primary_category": "cs.AI",
    "comment": "Accepted by RTSS 2025",
    "pdf_url": "https://arxiv.org/pdf/2507.21276v1",
    "published_date": "2025-07-28 19:03:26 UTC",
    "updated_date": "2025-07-28 19:03:26 UTC"
  },
  {
    "arxiv_id": "2507.21260v1",
    "title": "Adaptive Multimodal Protein Plug-and-Play with Diffusion-Based Priors",
    "authors": [
      "Amartya Banerjee",
      "Xingyu Xu",
      "Caroline Moosmüller",
      "Harlin Lee"
    ],
    "abstract": "In an inverse problem, the goal is to recover an unknown parameter (e.g., an image) that has typically undergone some lossy or noisy transformation during measurement. Recently, deep generative models, particularly diffusion models, have emerged as powerful priors for protein structure generation. However, integrating noisy experimental data from multiple sources to guide these models remains a significant challenge. Existing methods often require precise knowledge of experimental noise levels and manually tuned weights for each data modality. In this work, we introduce Adam-PnP, a Plug-and-Play framework that guides a pre-trained protein diffusion model using gradients from multiple, heterogeneous experimental sources. Our framework features an adaptive noise estimation scheme and a dynamic modality weighting mechanism integrated into the diffusion process, which reduce the need for manual hyperparameter tuning. Experiments on complex reconstruction tasks demonstrate significantly improved accuracy using Adam-PnP.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "q-bio.QM"
    ],
    "primary_category": "cs.LG",
    "comment": "Code: https://github.com/amartya21/Adam-PnP",
    "pdf_url": "https://arxiv.org/pdf/2507.21260v1",
    "published_date": "2025-07-28 18:28:03 UTC",
    "updated_date": "2025-07-28 18:28:03 UTC"
  },
  {
    "arxiv_id": "2507.21257v2",
    "title": "CompoST: A Benchmark for Analyzing the Ability of LLMs To Compositionally Interpret Questions in a QALD Setting",
    "authors": [
      "David Maria Schmidt",
      "Raoul Schubert",
      "Philipp Cimiano"
    ],
    "abstract": "Language interpretation is a compositional process, in which the meaning of more complex linguistic structures is inferred from the meaning of their parts. Large language models possess remarkable language interpretation capabilities and have been successfully applied to interpret questions by mapping them to SPARQL queries. An open question is how systematic this interpretation process is. Toward this question, in this paper, we propose a benchmark for investigating to what extent the abilities of LLMs to interpret questions are actually compositional. For this, we generate three datasets of varying difficulty based on graph patterns in DBpedia, relying on Lemon lexica for verbalization. Our datasets are created in a very controlled fashion in order to test the ability of LLMs to interpret structurally complex questions, given that they have seen the atomic building blocks. This allows us to evaluate to what degree LLMs are able to interpret complex questions for which they \"understand\" the atomic parts. We conduct experiments with models of different sizes using both various prompt and few-shot optimization techniques as well as fine-tuning. Our results show that performance in terms of macro $F_1$ degrades from $0.45$ over $0.26$ down to $0.09$ with increasing deviation from the samples optimized on. Even when all necessary information was provided to the model in the input, the $F_1$ scores do not exceed $0.57$ for the dataset of lowest complexity. We thus conclude that LLMs struggle to systematically and compositionally interpret questions and map them into SPARQL queries.",
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.AI",
    "comment": "Research Track, 24th International Semantic Web Conference (ISWC 2025), November 2-6, 2025, Nara, Japan",
    "pdf_url": "https://arxiv.org/pdf/2507.21257v2",
    "published_date": "2025-07-28 18:20:41 UTC",
    "updated_date": "2025-10-30 16:25:15 UTC"
  },
  {
    "arxiv_id": "2507.21246v1",
    "title": "On Explaining Visual Captioning with Hybrid Markov Logic Networks",
    "authors": [
      "Monika Shah",
      "Somdeb Sarkhel",
      "Deepak Venugopal"
    ],
    "abstract": "Deep Neural Networks (DNNs) have made tremendous progress in multimodal tasks such as image captioning. However, explaining/interpreting how these models integrate visual information, language information and knowledge representation to generate meaningful captions remains a challenging problem. Standard metrics to measure performance typically rely on comparing generated captions with human-written ones that may not provide a user with a deep insights into this integration. In this work, we develop a novel explanation framework that is easily interpretable based on Hybrid Markov Logic Networks (HMLNs) - a language that can combine symbolic rules with real-valued functions - where we hypothesize how relevant examples from the training data could have influenced the generation of the observed caption. To do this, we learn a HMLN distribution over the training instances and infer the shift in distributions over these instances when we condition on the generated sample which allows us to quantify which examples may have been a source of richer information to generate the observed caption. Our experiments on captions generated for several state-of-the-art captioning models using Amazon Mechanical Turk illustrate the interpretability of our explanations, and allow us to compare these models along the dimension of explainability.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2507.21246v1",
    "published_date": "2025-07-28 18:07:30 UTC",
    "updated_date": "2025-07-28 18:07:30 UTC"
  },
  {
    "arxiv_id": "2507.21244v1",
    "title": "Bubbleformer: Forecasting Boiling with Transformers",
    "authors": [
      "Sheikh Md Shakeel Hassan",
      "Xianwei Zou",
      "Akash Dhruv",
      "Vishwanath Ganesan",
      "Aparna Chandramowlishwaran"
    ],
    "abstract": "Modeling boiling (an inherently chaotic, multiphase process central to energy and thermal systems) remains a significant challenge for neural PDE surrogates. Existing models require future input (e.g., bubble positions) during inference because they fail to learn nucleation from past states, limiting their ability to autonomously forecast boiling dynamics. They also fail to model flow boiling velocity fields, where sharp interface-momentum coupling demands long-range and directional inductive biases. We introduce Bubbleformer, a transformer-based spatiotemporal model that forecasts stable and long-range boiling dynamics including nucleation, interface evolution, and heat transfer without dependence on simulation data during inference. Bubbleformer integrates factorized axial attention, frequency-aware scaling, and conditions on thermophysical parameters to generalize across fluids, geometries, and operating conditions. To evaluate physical fidelity in chaotic systems, we propose interpretable physics-based metrics that evaluate heat-flux consistency, interface geometry, and mass conservation. We also release BubbleML 2.0, a high-fidelity dataset that spans diverse working fluids (cryogens, refrigerants, dielectrics), boiling configurations (pool and flow boiling), flow regimes (bubbly, slug, annular), and boundary conditions. Bubbleformer sets new benchmark results in both prediction and forecasting of two-phase boiling flows.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CE"
    ],
    "primary_category": "cs.LG",
    "comment": "39 pages, 13 figures, Submitted to NeurIPS 2025",
    "pdf_url": "https://arxiv.org/pdf/2507.21244v1",
    "published_date": "2025-07-28 18:02:57 UTC",
    "updated_date": "2025-07-28 18:02:57 UTC"
  },
  {
    "arxiv_id": "2507.21046v4",
    "title": "A Survey of Self-Evolving Agents: What, When, How, and Where to Evolve on the Path to Artificial Super Intelligence",
    "authors": [
      "Huan-ang Gao",
      "Jiayi Geng",
      "Wenyue Hua",
      "Mengkang Hu",
      "Xinzhe Juan",
      "Hongzhang Liu",
      "Shilong Liu",
      "Jiahao Qiu",
      "Xuan Qi",
      "Yiran Wu",
      "Hongru Wang",
      "Han Xiao",
      "Yuhang Zhou",
      "Shaokun Zhang",
      "Jiayi Zhang",
      "Jinyu Xiang",
      "Yixiong Fang",
      "Qiwen Zhao",
      "Dongrui Liu",
      "Qihan Ren",
      "Cheng Qian",
      "Zhenhailong Wang",
      "Minda Hu",
      "Huazheng Wang",
      "Qingyun Wu",
      "Heng Ji",
      "Mengdi Wang"
    ],
    "abstract": "Large Language Models (LLMs) have demonstrated remarkable capabilities across diverse tasks but remain fundamentally static, unable to adapt their internal parameters to novel tasks, evolving knowledge domains, or dynamic interaction contexts. As LLMs are increasingly deployed in open-ended, interactive environments, this static nature has become a critical bottleneck, necessitating agents that can adaptively reason, act, and evolve in real time. This paradigm shift -- from scaling static models to developing self-evolving agents -- has sparked growing interest in architectures and methods enabling continual learning and adaptation from data, interactions, and experiences. This survey provides the first systematic and comprehensive review of self-evolving agents, organizing the field around three foundational dimensions: what, when, and how to evolve. We examine evolutionary mechanisms across agent components (e.g., models, memory, tools, architecture), categorize adaptation methods by stages (e.g., intra-test-time, inter-test-time), and analyze the algorithmic and architectural designs that guide evolutionary adaptation (e.g., scalar rewards, textual feedback, single-agent and multi-agent systems). Additionally, we analyze evaluation metrics and benchmarks tailored for self-evolving agents, highlight applications in domains such as coding, education, and healthcare, and identify critical challenges and research directions in safety, scalability, and co-evolutionary dynamics. By providing a structured framework for understanding and designing self-evolving agents, this survey establishes a roadmap for advancing more adaptive, robust, and versatile agentic systems in both research and real-world deployments, and ultimately sheds light on the realization of Artificial Super Intelligence (ASI) where agents evolve autonomously and perform beyond human-level intelligence across tasks.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "77 pages, 9 figures, Transactions on Machine Learning Research (01/2026)",
    "pdf_url": "https://arxiv.org/pdf/2507.21046v4",
    "published_date": "2025-07-28 17:59:05 UTC",
    "updated_date": "2026-01-16 20:59:08 UTC"
  },
  {
    "arxiv_id": "2507.21206v1",
    "title": "Agentic Web: Weaving the Next Web with AI Agents",
    "authors": [
      "Yingxuan Yang",
      "Mulei Ma",
      "Yuxuan Huang",
      "Huacan Chai",
      "Chenyu Gong",
      "Haoran Geng",
      "Yuanjian Zhou",
      "Ying Wen",
      "Meng Fang",
      "Muhao Chen",
      "Shangding Gu",
      "Ming Jin",
      "Costas Spanos",
      "Yang Yang",
      "Pieter Abbeel",
      "Dawn Song",
      "Weinan Zhang",
      "Jun Wang"
    ],
    "abstract": "The emergence of AI agents powered by large language models (LLMs) marks a pivotal shift toward the Agentic Web, a new phase of the internet defined by autonomous, goal-driven interactions. In this paradigm, agents interact directly with one another to plan, coordinate, and execute complex tasks on behalf of users. This transition from human-driven to machine-to-machine interaction allows intent to be delegated, relieving users from routine digital operations and enabling a more interactive, automated web experience. In this paper, we present a structured framework for understanding and building the Agentic Web. We trace its evolution from the PC and Mobile Web eras and identify the core technological foundations that support this shift. Central to our framework is a conceptual model consisting of three key dimensions: intelligence, interaction, and economics. These dimensions collectively enable the capabilities of AI agents, such as retrieval, recommendation, planning, and collaboration. We analyze the architectural and infrastructural challenges involved in creating scalable agentic systems, including communication protocols, orchestration strategies, and emerging paradigms such as the Agent Attention Economy. We conclude by discussing the potential applications, societal risks, and governance issues posed by agentic systems, and outline research directions for developing open, secure, and intelligent ecosystems shaped by both human intent and autonomous agent behavior. A continuously updated collection of relevant studies for agentic web is available at: https://github.com/SafeRL-Lab/agentic-web.",
    "categories": [
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2507.21206v1",
    "published_date": "2025-07-28 17:58:12 UTC",
    "updated_date": "2025-07-28 17:58:12 UTC"
  },
  {
    "arxiv_id": "2507.21035v2",
    "title": "GenoMAS: A Multi-Agent Framework for Scientific Discovery via Code-Driven Gene Expression Analysis",
    "authors": [
      "Haoyang Liu",
      "Yijiang Li",
      "Haohan Wang"
    ],
    "abstract": "Gene expression analysis holds the key to many biomedical discoveries, yet extracting insights from raw transcriptomic data remains formidable due to the complexity of multiple large, semi-structured files and the need for extensive domain expertise. Current automation approaches are often limited by either inflexible workflows that break down in edge cases or by fully autonomous agents that lack the necessary precision for rigorous scientific inquiry. GenoMAS charts a different course by presenting a team of LLM-based scientists that integrates the reliability of structured workflows with the adaptability of autonomous agents. GenoMAS orchestrates six specialized LLM agents through typed message-passing protocols, each contributing complementary strengths to a shared analytic canvas. At the heart of GenoMAS lies a guided-planning framework: programming agents unfold high-level task guidelines into Action Units and, at each juncture, elect to advance, revise, bypass, or backtrack, thereby maintaining logical coherence while bending gracefully to the idiosyncrasies of genomic data.\n  On the GenoTEX benchmark, GenoMAS reaches a Composite Similarity Correlation of 89.13% for data preprocessing and an F$_1$ of 60.48% for gene identification, surpassing the best prior art by 10.61% and 16.85% respectively. Beyond metrics, GenoMAS surfaces biologically plausible gene-phenotype associations corroborated by the literature, all while adjusting for latent confounders. Code is available at https://github.com/Liu-Hy/GenoMAS.",
    "categories": [
      "cs.AI",
      "cs.LG",
      "cs.MA",
      "q-bio.GN"
    ],
    "primary_category": "cs.AI",
    "comment": "51 pages (13 pages for the main text, 9 pages for references, and 29 pages for the appendix)",
    "pdf_url": "https://arxiv.org/pdf/2507.21035v2",
    "published_date": "2025-07-28 17:55:08 UTC",
    "updated_date": "2025-07-31 17:57:18 UTC"
  },
  {
    "arxiv_id": "2507.21205v1",
    "title": "Learning from Limited and Imperfect Data",
    "authors": [
      "Harsh Rangwani"
    ],
    "abstract": "The distribution of data in the world (eg, internet, etc.) significantly differs from the well-curated datasets and is often over-populated with samples from common categories. The algorithms designed for well-curated datasets perform suboptimally when used for learning from imperfect datasets with long-tailed imbalances and distribution shifts. To expand the use of deep models, it is essential to overcome the labor-intensive curation process by developing robust algorithms that can learn from diverse, real-world data distributions. Toward this goal, we develop practical algorithms for Deep Neural Networks which can learn from limited and imperfect data present in the real world. This thesis is divided into four segments, each covering a scenario of learning from limited or imperfect data. The first part of the thesis focuses on Learning Generative Models from Long-Tail Data, where we mitigate the mode-collapse and enable diverse aesthetic image generations for tail (minority) classes. In the second part, we enable effective generalization on tail classes through Inductive Regularization schemes, which allow tail classes to generalize as effectively as the head classes without requiring explicit generation of images. In the third part, we develop algorithms for Optimizing Relevant Metrics for learning from long-tailed data with limited annotation (semi-supervised), followed by the fourth part, which focuses on the Efficient Domain Adaptation of the model to various domains with very few to zero labeled samples.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.LG",
    "comment": "PhD Thesis",
    "pdf_url": "https://arxiv.org/pdf/2507.21205v1",
    "published_date": "2025-07-28 17:54:15 UTC",
    "updated_date": "2025-07-28 17:54:15 UTC"
  },
  {
    "arxiv_id": "2507.21027v1",
    "title": "Smart Expansion Techniques for ASP-based Interactive Configuration",
    "authors": [
      "Lucia Balážová",
      "Richard Comploi-Taupe",
      "Susana Hahn",
      "Nicolas Rühling",
      "Gottfried Schenner"
    ],
    "abstract": "Product configuration is a successful application of Answer Set Programming (ASP). However, challenges are still open for interactive systems to effectively guide users through the configuration process. The aim of our work is to provide an ASP-based solver for interactive configuration that can deal with large-scale industrial configuration problems and that supports intuitive user interfaces via an API. In this paper, we focus on improving the performance of automatically completing a partial configuration. Our main contribution enhances the classical incremental approach for multi-shot solving by four different smart expansion functions. The core idea is to determine and add specific objects or associations to the partial configuration by exploiting cautious and brave consequences before checking for the existence of a complete configuration with the current objects in each iteration. This approach limits the number of costly unsatisfiability checks and reduces the search space, thereby improving solving performance. In addition, we present a user interface that uses our API and is implemented in ASP.",
    "categories": [
      "cs.AI",
      "cs.SE"
    ],
    "primary_category": "cs.AI",
    "comment": "Under consideration for publication in Theory and Practice of Logic Programming (TPLP)",
    "pdf_url": "https://arxiv.org/pdf/2507.21027v1",
    "published_date": "2025-07-28 17:46:51 UTC",
    "updated_date": "2025-07-28 17:46:51 UTC"
  },
  {
    "arxiv_id": "2507.21017v1",
    "title": "MIRAGE-Bench: LLM Agent is Hallucinating and Where to Find Them",
    "authors": [
      "Weichen Zhang",
      "Yiyou Sun",
      "Pohao Huang",
      "Jiayue Pu",
      "Heyue Lin",
      "Dawn Song"
    ],
    "abstract": "Hallucinations pose critical risks for large language model (LLM)-based agents, often manifesting as hallucinative actions resulting from fabricated or misinterpreted information within the cognitive context. While recent studies have exposed such failures, existing evaluations remain fragmented and lack a principled testbed. In this paper, we present MIRAGE-Bench--Measuring Illusions in Risky AGEnt settings--the first unified benchmark for eliciting and evaluating hallucinations in interactive LLM-agent scenarios. We begin by introducing a three-part taxonomy to address agentic hallucinations: actions that are unfaithful to (i) task instructions, (ii) execution history, or (iii) environment observations. To analyze, we first elicit such failures by performing a systematic audit of existing agent benchmarks, then synthesize test cases using a snapshot strategy that isolates decision points in deterministic and reproducible manners. To evaluate hallucination behaviors, we adopt a fine-grained-level LLM-as-a-Judge paradigm with tailored risk-aware prompts, enabling scalable, high-fidelity assessment of agent actions without enumerating full action spaces. MIRAGE-Bench provides actionable insights on failure modes of LLM agents and lays the groundwork for principled progress in mitigating hallucinations in interactive environments.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "Code and data: https://github.com/sunblaze-ucb/mirage-bench.git",
    "pdf_url": "https://arxiv.org/pdf/2507.21017v1",
    "published_date": "2025-07-28 17:38:29 UTC",
    "updated_date": "2025-07-28 17:38:29 UTC"
  },
  {
    "arxiv_id": "2507.21009v2",
    "title": "Memorization in Fine-Tuned Large Language Models",
    "authors": [
      "Danil Savine"
    ],
    "abstract": "This study investigates the mechanisms and factors influencing memorization in fine-tuned large language models (LLMs), with a focus on the medical domain due to its privacy-sensitive nature. We examine how different aspects of the fine-tuning process affect a model's propensity to memorize training data, using the PHEE dataset of pharmacovigilance events.\n  Our research employs two main approaches: a membership inference attack to detect memorized data, and a generation task with prompted prefixes to assess verbatim reproduction. We analyze the impact of adapting different weight matrices in the transformer architecture, the relationship between perplexity and memorization, and the effect of increasing the rank in low-rank adaptation (LoRA) fine-tuning.\n  Key findings include: (1) Value and Output matrices contribute more significantly to memorization compared to Query and Key matrices; (2) Lower perplexity in the fine-tuned model correlates with increased memorization; (3) Higher LoRA ranks lead to increased memorization, but with diminishing returns at higher ranks.\n  These results provide insights into the trade-offs between model performance and privacy risks in fine-tuned LLMs. Our findings have implications for developing more effective and responsible strategies for adapting large language models while managing data privacy concerns.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2507.21009v2",
    "published_date": "2025-07-28 17:22:10 UTC",
    "updated_date": "2025-08-05 08:32:40 UTC"
  },
  {
    "arxiv_id": "2507.21004v2",
    "title": "Compositional Function Networks: A High-Performance Alternative to Deep Neural Networks with Built-in Interpretability",
    "authors": [
      "Fang Li"
    ],
    "abstract": "Deep Neural Networks (DNNs) deliver impressive performance but their black-box nature limits deployment in high-stakes domains requiring transparency. We introduce Compositional Function Networks (CFNs), a novel framework that builds inherently interpretable models by composing elementary mathematical functions with clear semantics. Unlike existing interpretable approaches that are limited to simple additive structures, CFNs support diverse compositional patterns -- sequential, parallel, and conditional -- enabling complex feature interactions while maintaining transparency. A key innovation is that CFNs are fully differentiable, allowing efficient training through standard gradient descent. We demonstrate CFNs' versatility across multiple domains, from symbolic regression to image classification with deep hierarchical networks. Our empirical evaluation shows CFNs achieve competitive performance against black-box models (96.24% accuracy on CIFAR-10) while outperforming state-of-the-art interpretable models like Explainable Boosting Machines. By combining the hierarchical expressiveness and efficient training of deep learning with the intrinsic interpretability of well-defined mathematical functions, CFNs offer a powerful framework for applications where both performance and accountability are paramount.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "The project has been open sourced at Github (https://github.com/fanglioc/Compositional_Function_Networks)",
    "pdf_url": "https://arxiv.org/pdf/2507.21004v2",
    "published_date": "2025-07-28 17:18:40 UTC",
    "updated_date": "2025-07-31 00:08:48 UTC"
  },
  {
    "arxiv_id": "2507.20997v2",
    "title": "Modular Delta Merging with Orthogonal Constraints: A Scalable Framework for Continual and Reversible Model Composition",
    "authors": [
      "Haris Khan",
      "Sadia Asif",
      "Shumaila Asif"
    ],
    "abstract": "In real-world machine learning deployments, models must be continually updated, composed, and when required, selectively undone. However, existing approaches to model merging and continual learning often suffer from task interference, catastrophic forgetting, or lack of reversibility. We propose Modular Delta Merging with Orthogonal Constraints (MDM-OC), a novel framework that enables scalable, interference-free, and reversible composition of fine-tuned models. Each task-specific model is encoded as a delta from a shared base and projected into an orthogonal subspace to eliminate conflict. These projected deltas are then merged via gradient-based optimization to form a unified model that retains performance across tasks. Our approach supports continual integration of new models, structured unmerging for compliance such as GDPR requirements, and model stability via elastic weight consolidation and synthetic replay. Extensive experiments on vision and natural language processing benchmarks demonstrate that MDM-OC outperforms prior baselines in accuracy, backward transfer, and unmerge fidelity, while remaining memory-efficient and computationally tractable. This framework offers a principled solution for modular and compliant AI system design.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "11 pages, 6 figures, 3 tables. Will be Submitted to ICLR 2025 for review",
    "pdf_url": "https://arxiv.org/pdf/2507.20997v2",
    "published_date": "2025-07-28 17:08:49 UTC",
    "updated_date": "2025-08-07 06:44:42 UTC"
  },
  {
    "arxiv_id": "2507.20994v1",
    "title": "Security Tensors as a Cross-Modal Bridge: Extending Text-Aligned Safety to Vision in LVLM",
    "authors": [
      "Shen Li",
      "Liuyi Yao",
      "Wujia Niu",
      "Lan Zhang",
      "Yaliang Li"
    ],
    "abstract": "Large visual-language models (LVLMs) integrate aligned large language models (LLMs) with visual modules to process multimodal inputs. However, the safety mechanisms developed for text-based LLMs do not naturally extend to visual modalities, leaving LVLMs vulnerable to harmful image inputs. To address this cross-modal safety gap, we introduce security tensors - trainable input vectors applied during inference through either the textual or visual modality. These tensors transfer textual safety alignment to visual processing without modifying the model's parameters. They are optimized using a curated dataset containing (i) malicious image-text pairs requiring rejection, (ii) contrastive benign pairs with text structurally similar to malicious queries, with the purpose of being contrastive examples to guide visual reliance, and (iii) general benign samples preserving model functionality. Experimental results demonstrate that both textual and visual security tensors significantly enhance LVLMs' ability to reject diverse harmful visual inputs while maintaining near-identical performance on benign tasks. Further internal analysis towards hidden-layer representations reveals that security tensors successfully activate the language module's textual \"safety layers\" in visual inputs, thereby effectively extending text-based safety to the visual modality.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "Codes and data are available at https://github.com/listen0425/Security-Tensors",
    "pdf_url": "https://arxiv.org/pdf/2507.20994v1",
    "published_date": "2025-07-28 16:59:53 UTC",
    "updated_date": "2025-07-28 16:59:53 UTC"
  },
  {
    "arxiv_id": "2507.20993v2",
    "title": "Learning Treatment Policies From Multimodal Electronic Health Records",
    "authors": [
      "Henri Arno",
      "Thomas Demeester"
    ],
    "abstract": "We study how to learn effective treatment policies from multimodal electronic health records (EHRs) that consist of tabular data and clinical text. These policies can help physicians make better treatment decisions and allocate healthcare resources more efficiently. Causal policy learning methods prioritize patients with the largest expected treatment benefit. Yet, existing estimators assume tabular covariates that satisfy strong causal assumptions, which are typically violated in the multimodal setting. As a result, predictive models of baseline risk are commonly used in practice to guide such decisions, as they extend naturally to multimodal data. However, such risk-based policies are not designed to identify which patients benefit most from treatment. We propose an extension of causal policy learning that uses expert-provided annotations during training to supervise treatment effect estimation, while using only multimodal representations as input during inference. We show that the proposed method achieves strong empirical performance across synthetic, semi-synthetic, and real-world EHR datasets, thereby offering practical insights into applying causal machine learning to realistic clinical data.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "Preprint. Under review",
    "pdf_url": "https://arxiv.org/pdf/2507.20993v2",
    "published_date": "2025-07-28 16:52:31 UTC",
    "updated_date": "2025-12-23 13:19:34 UTC"
  },
  {
    "arxiv_id": "2507.20987v2",
    "title": "JWB-DH-V1: Benchmark for Joint Whole-Body Talking Avatar and Speech Generation Version 1",
    "authors": [
      "Xinhan Di",
      "Kristin Qi",
      "Pengqian Yu"
    ],
    "abstract": "Recent advances in diffusion-based video generation have enabled photo-realistic short clips, but current methods still struggle to achieve multi-modal consistency when jointly generating whole-body motion and natural speech. Current approaches lack comprehensive evaluation frameworks that assess both visual and audio quality, and there are insufficient benchmarks for region-specific performance analysis. To address these gaps, we introduce the Joint Whole-Body Talking Avatar and Speech Generation Version I(JWB-DH-V1), comprising a large-scale multi-modal dataset with 10,000 unique identities across 2 million video samples, and an evaluation protocol for assessing joint audio-video generation of whole-body animatable avatars. Our evaluation of SOTA models reveals consistent performance disparities between face/hand-centric and whole-body performance, which incidates essential areas for future research. The dataset and evaluation tools are publicly available at https://github.com/deepreasonings/WholeBodyBenchmark.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "WiCV @ ICCV 2025",
    "pdf_url": "https://arxiv.org/pdf/2507.20987v2",
    "published_date": "2025-07-28 16:47:44 UTC",
    "updated_date": "2025-07-29 04:13:25 UTC"
  },
  {
    "arxiv_id": "2507.20984v2",
    "title": "SmallThinker: A Family of Efficient Large Language Models Natively Trained for Local Deployment",
    "authors": [
      "Yixin Song",
      "Zhenliang Xue",
      "Dongliang Wei",
      "Feiyang Chen",
      "Jianxiang Gao",
      "Junchen Liu",
      "Hangyu Liang",
      "Guangshuo Qin",
      "Chengrong Tian",
      "Bo Wen",
      "Longyu Zhao",
      "Xinrui Zheng",
      "Zeyu Mi",
      "Haibo Chen"
    ],
    "abstract": "While frontier large language models (LLMs) continue to push capability boundaries, their deployment remains confined to GPU-powered cloud infrastructure. We challenge this paradigm with SmallThinker, a family of LLMs natively designed - not adapted - for the unique constraints of local devices: weak computational power, limited memory, and slow storage. Unlike traditional approaches that mainly compress existing models built for clouds, we architect SmallThinker from the ground up to thrive within these limitations. Our innovation lies in a deployment-aware architecture that transforms constraints into design principles. First, We introduce a two-level sparse structure combining fine-grained Mixture-of-Experts (MoE) with sparse feed-forward networks, drastically reducing computational demands without sacrificing model capacity. Second, to conquer the I/O bottleneck of slow storage, we design a pre-attention router that enables our co-designed inference engine to prefetch expert parameters from storage while computing attention, effectively hiding storage latency that would otherwise cripple on-device inference. Third, for memory efficiency, we utilize NoPE-RoPE hybrid sparse attention mechanism to slash KV cache requirements. We release SmallThinker-4B-A0.6B and SmallThinker-21B-A3B, which achieve state-of-the-art performance scores and even outperform larger LLMs. Remarkably, our co-designed system mostly eliminates the need for expensive GPU hardware: with Q4_0 quantization, both models exceed 20 tokens/s on ordinary consumer CPUs, while consuming only 1GB and 8GB of memory respectively. SmallThinker is publicly available at hf.co/PowerInfer/SmallThinker-4BA0.6B-Instruct and hf.co/PowerInfer/SmallThinker-21BA3B-Instruct.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2507.20984v2",
    "published_date": "2025-07-28 16:45:14 UTC",
    "updated_date": "2025-07-30 06:29:40 UTC"
  },
  {
    "arxiv_id": "2507.20968v3",
    "title": "From Entanglement to Alignment: Representation Space Decomposition for Unsupervised Time Series Domain Adaptation",
    "authors": [
      "Rongyao Cai",
      "Ming Jin",
      "Qingsong Wen",
      "Kexin Zhang"
    ],
    "abstract": "Domain shift poses a fundamental challenge in time series analysis, where models trained on source domain often fail dramatically when applied in target domain with different yet similar distributions. While current unsupervised domain adaptation (UDA) methods attempt to align cross-domain feature distributions, they typically treat features as indivisible entities, ignoring their intrinsic compositions that govern domain adaptation. We introduce DARSD, a novel UDA framework with theoretical explainability that explicitly realizes UDA tasks from the perspective of representation space decomposition. Our core insight is that effective domain adaptation requires not just alignment, but principled disentanglement of transferable knowledge from mixed representations. DARSD consists of three synergistic components: (I) An adversarial learnable common invariant basis that projects original features into a domain-invariant subspace while preserving semantic content; (II) A prototypical pseudo-labeling mechanism that dynamically separates target features based on confidence, hindering error accumulation; (III) A hybrid contrastive optimization strategy that simultaneously enforces feature clustering and consistency while mitigating emerging distribution gaps. Comprehensive experiments conducted on four benchmarks (WISDM, HAR, HHAR, and MFD) demonstrate DARSD's superiority against 12 UDA algorithms, achieving optimal performance in 35 out of 53 scenarios and ranking first across all benchmarks.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "15 pages, 7 figures",
    "pdf_url": "https://arxiv.org/pdf/2507.20968v3",
    "published_date": "2025-07-28 16:26:28 UTC",
    "updated_date": "2025-08-06 07:55:26 UTC"
  },
  {
    "arxiv_id": "2507.20966v2",
    "title": "Handoff Design in User-Centric Cell-Free Massive MIMO Networks Using DRL",
    "authors": [
      "Hussein A. Ammar",
      "Raviraj Adve",
      "Shahram Shahbazpanahi",
      "Gary Boudreau",
      "Israfil Bahceci"
    ],
    "abstract": "In the user-centric cell-free massive MIMO (UC-mMIMO) network scheme, user mobility necessitates updating the set of serving access points to maintain the user-centric clustering. Such updates are typically performed through handoff (HO) operations; however, frequent HOs lead to overheads associated with the allocation and release of resources. This paper presents a deep reinforcement learning (DRL)-based solution to predict and manage these connections for mobile users. Our solution employs the Soft Actor-Critic algorithm, with continuous action space representation, to train a deep neural network to serve as the HO policy. We present a novel proposition for a reward function that integrates a HO penalty in order to balance the attainable rate and the associated overhead related to HOs. We develop two variants of our system; the first one uses mobility direction-assisted (DA) observations that are based on the user movement pattern, while the second one uses history-assisted (HA) observations that are based on the history of the large-scale fading (LSF). Simulation results show that our DRL-based continuous action space approach is more scalable than discrete space counterpart, and that our derived HO policy automatically learns to gather HOs in specific time slots to minimize the overhead of initiating HOs. Our solution can also operate in real time with a response time less than 0.4 ms.",
    "categories": [
      "cs.IT",
      "cs.AI",
      "cs.LG",
      "cs.NI",
      "eess.SP"
    ],
    "primary_category": "cs.IT",
    "comment": "Published in IEEE Transactions on Communications (IEEE TCOM)",
    "pdf_url": "https://arxiv.org/pdf/2507.20966v2",
    "published_date": "2025-07-28 16:21:45 UTC",
    "updated_date": "2025-08-02 03:14:14 UTC"
  },
  {
    "arxiv_id": "2507.20964v2",
    "title": "Core Safety Values for Provably Corrigible Agents",
    "authors": [
      "Aran Nayebi"
    ],
    "abstract": "We introduce the first complete formal solution to corrigibility in the off-switch game, with provable guarantees in multi-step, partially observed environments. Our framework consists of five *structurally separate* utility heads -- deference, switch-access preservation, truthfulness, low-impact behavior via a belief-based extension of Attainable Utility Preservation, and bounded task reward -- combined lexicographically by strict weight gaps. Theorem 1 proves exact single-round corrigibility in the partially observable off-switch game; Theorem 3 extends the guarantee to multi-step, self-spawning agents, showing that even if each head is *learned* to mean-squared error $\\varepsilon$ and the planner is $\\varepsilon$-sub-optimal, the probability of violating *any* safety property is bounded while still ensuring net human benefit. In contrast to Constitutional AI or RLHF/RLAIF, which merge all norms into one learned scalar, our separation makes obedience and impact-limits provably dominate even when incentives conflict. For settings where adversaries can modify the agent, we prove that deciding whether an arbitrary post-hack agent will ever violate corrigibility is undecidable by reduction to the halting problem, then carve out a finite-horizon \"decidable island\" where safety can be certified in randomized polynomial time and verified with privacy-preserving, constant-round zero-knowledge proofs.",
    "categories": [
      "cs.AI",
      "cs.CC",
      "cs.GT",
      "cs.LG",
      "cs.MA"
    ],
    "primary_category": "cs.AI",
    "comment": "14 pages. To appear in AAAI 2026 Machine Ethics Workshop (W37) Proceedings",
    "pdf_url": "https://arxiv.org/pdf/2507.20964v2",
    "published_date": "2025-07-28 16:19:25 UTC",
    "updated_date": "2025-11-19 14:51:55 UTC"
  },
  {
    "arxiv_id": "2507.20960v1",
    "title": "On the Limits of Hierarchically Embedded Logic in Classical Neural Networks",
    "authors": [
      "Bill Cochran"
    ],
    "abstract": "We propose a formal model of reasoning limitations in large neural net models for language, grounded in the depth of their neural architecture. By treating neural networks as linear operators over logic predicate space we show that each layer can encode at most one additional level of logical reasoning. We prove that a neural network of depth a particular depth cannot faithfully represent predicates in a one higher order logic, such as simple counting over complex predicates, implying a strict upper bound on logical expressiveness. This structure induces a nontrivial null space during tokenization and embedding, excluding higher-order predicates from representability. Our framework offers a natural explanation for phenomena such as hallucination, repetition, and limited planning, while also providing a foundation for understanding how approximations to higher-order logic may emerge. These results motivate architectural extensions and interpretability strategies in future development of language models.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "9 pages",
    "pdf_url": "https://arxiv.org/pdf/2507.20960v1",
    "published_date": "2025-07-28 16:13:41 UTC",
    "updated_date": "2025-07-28 16:13:41 UTC"
  },
  {
    "arxiv_id": "2507.20957v4",
    "title": "Your AI, Not Your View: The Bias of LLMs in Investment Analysis",
    "authors": [
      "Hoyoung Lee",
      "Junhyuk Seo",
      "Suhwan Park",
      "Junhyeong Lee",
      "Wonbin Ahn",
      "Chanyeol Choi",
      "Alejandro Lopez-Lira",
      "Yongjae Lee"
    ],
    "abstract": "In finance, Large Language Models (LLMs) face frequent knowledge conflicts arising from discrepancies between their pre-trained parametric knowledge and real-time market data. These conflicts are especially problematic in real-world investment services, where a model's inherent biases can misalign with institutional objectives, leading to unreliable recommendations. Despite this risk, the intrinsic investment biases of LLMs remain underexplored. We propose an experimental framework to investigate emergent behaviors in such conflict scenarios, offering a quantitative analysis of bias in LLM-based investment analysis. Using hypothetical scenarios with balanced and imbalanced arguments, we extract the latent biases of models and measure their persistence. Our analysis, centered on sector, size, and momentum, reveals distinct, model-specific biases. Across most models, a tendency to prefer technology stocks, large-cap stocks, and contrarian strategies is observed. These foundational biases often escalate into confirmation bias, causing models to cling to initial judgments even when faced with increasing counter-evidence. A public leaderboard benchmarking bias across a broader set of models is available at https://linqalpha.com/leaderboard",
    "categories": [
      "q-fin.PM",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "q-fin.PM",
    "comment": "Accepted at ACM International Conference on AI in Finance (ICAIF)",
    "pdf_url": "https://arxiv.org/pdf/2507.20957v4",
    "published_date": "2025-07-28 16:09:38 UTC",
    "updated_date": "2025-10-16 18:06:41 UTC"
  },
  {
    "arxiv_id": "2508.00900v1",
    "title": "Sparse 3D Perception for Rose Harvesting Robots: A Two-Stage Approach Bridging Simulation and Real-World Applications",
    "authors": [
      "Taha Samavati",
      "Mohsen Soryani",
      "Sina Mansouri"
    ],
    "abstract": "The global demand for medicinal plants, such as Damask roses, has surged with population growth, yet labor-intensive harvesting remains a bottleneck for scalability. To address this, we propose a novel 3D perception pipeline tailored for flower-harvesting robots, focusing on sparse 3D localization of rose centers. Our two-stage algorithm first performs 2D point-based detection on stereo images, followed by depth estimation using a lightweight deep neural network. To overcome the challenge of scarce real-world labeled data, we introduce a photorealistic synthetic dataset generated via Blender, simulating a dynamic rose farm environment with precise 3D annotations. This approach minimizes manual labeling costs while enabling robust model training. We evaluate two depth estimation paradigms: a traditional triangulation-based method and our proposed deep learning framework. Results demonstrate the superiority of our method, achieving an F1 score of 95.6% (synthetic) and 74.4% (real) in 2D detection, with a depth estimation error of 3% at a 2-meter range on synthetic data. The pipeline is optimized for computational efficiency, ensuring compatibility with resource-constrained robotic systems. By bridging the domain gap between synthetic and real-world data, this work advances agricultural automation for specialty crops, offering a scalable solution for precision harvesting.",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.RO",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2508.00900v1",
    "published_date": "2025-07-28 16:09:34 UTC",
    "updated_date": "2025-07-28 16:09:34 UTC"
  },
  {
    "arxiv_id": "2507.20956v1",
    "title": "Mind the Gap: Conformative Decoding to Improve Output Diversity of Instruction-Tuned Large Language Models",
    "authors": [
      "Max Peeperkorn",
      "Tom Kouwenhoven",
      "Dan Brown",
      "Anna Jordanous"
    ],
    "abstract": "Instruction-tuning large language models (LLMs) reduces the diversity of their outputs, which has implications for many tasks, particularly for creative tasks. This paper investigates the ``diversity gap'' for a writing prompt narrative generation task. This gap emerges as measured by current diversity metrics for various open-weight and open-source LLMs. The results show significant decreases in diversity due to instruction-tuning. We explore the diversity loss at each fine-tuning stage for the OLMo and OLMo 2 models to further understand how output diversity is affected. The results indicate that DPO has the most substantial impact on diversity. Motivated by these findings, we present a new decoding strategy, conformative decoding, which guides an instruct model using its more diverse base model to reintroduce output diversity. We show that conformative decoding typically increases diversity and even maintains or improves quality.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "9 pages, 3 figures",
    "pdf_url": "https://arxiv.org/pdf/2507.20956v1",
    "published_date": "2025-07-28 16:04:25 UTC",
    "updated_date": "2025-07-28 16:04:25 UTC"
  },
  {
    "arxiv_id": "2507.20951v1",
    "title": "Partially Observable Monte-Carlo Graph Search",
    "authors": [
      "Yang You",
      "Vincent Thomas",
      "Alex Schutz",
      "Robert Skilton",
      "Nick Hawes",
      "Olivier Buffet"
    ],
    "abstract": "Currently, large partially observable Markov decision processes (POMDPs) are often solved by sampling-based online methods which interleave planning and execution phases. However, a pre-computed offline policy is more desirable in POMDP applications with time or energy constraints. But previous offline algorithms are not able to scale up to large POMDPs. In this article, we propose a new sampling-based algorithm, the partially observable Monte-Carlo graph search (POMCGS) to solve large POMDPs offline. Different from many online POMDP methods, which progressively develop a tree while performing (Monte-Carlo) simulations, POMCGS folds this search tree on the fly to construct a policy graph, so that computations can be drastically reduced, and users can analyze and validate the policy prior to embedding and executing it. Moreover, POMCGS, together with action progressive widening and observation clustering methods provided in this article, is able to address certain continuous POMDPs. Through experiments, we demonstrate that POMCGS can generate policies on the most challenging POMDPs, which cannot be computed by previous offline algorithms, and these policies' values are competitive compared with the state-of-the-art online POMDP algorithms.",
    "categories": [
      "cs.AI",
      "cs.RO"
    ],
    "primary_category": "cs.AI",
    "comment": "To be published in Proceedings of ICAPS 2025",
    "pdf_url": "https://arxiv.org/pdf/2507.20951v1",
    "published_date": "2025-07-28 16:02:36 UTC",
    "updated_date": "2025-07-28 16:02:36 UTC"
  },
  {
    "arxiv_id": "2507.20941v2",
    "title": "Multivariate Conformal Prediction via Conformalized Gaussian Scoring",
    "authors": [
      "Sacha Braun",
      "Eugène Berta",
      "Michael I. Jordan",
      "Francis Bach"
    ],
    "abstract": "While achieving exact conditional coverage in conformal prediction is unattainable without making strong, untestable regularity assumptions, the promise of conformal prediction hinges on finding approximations to conditional guarantees that are realizable in practice. A promising direction for obtaining conditional dependence for conformal sets--in particular capturing heteroskedasticity--is through estimating the conditional density $\\mathbb{P}_{Y|X}$ and conformalizing its level sets. Previous work in this vein has focused on nonconformity scores based on the empirical cumulative distribution function (CDF). Such scores are, however, computationally costly, typically requiring expensive sampling methods. To avoid the need for sampling, we observe that the CDF-based score reduces to a Mahalanobis distance in the case of Gaussian scores, yielding a closed-form expression that can be directly conformalized. Moreover, the use of a Gaussian-based score opens the door to a number of extensions of the basic conformal method; in particular, we show how to construct conformal sets with missing output values, refine conformal sets as partial information about $Y$ becomes available, and construct conformal sets on transformations of the output space. Finally, empirical results indicate that our approach produces conformal sets that more closely approximate conditional coverage in multivariate settings compared to alternative methods.",
    "categories": [
      "stat.ML",
      "cs.AI",
      "cs.LG",
      "stat.ME",
      "stat.OT"
    ],
    "primary_category": "stat.ML",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2507.20941v2",
    "published_date": "2025-07-28 15:55:29 UTC",
    "updated_date": "2025-12-29 14:44:45 UTC"
  },
  {
    "arxiv_id": "2507.20936v2",
    "title": "Dissecting Persona-Driven Reasoning in Language Models via Activation Patching",
    "authors": [
      "Ansh Poonia",
      "Maeghal Jain"
    ],
    "abstract": "Large language models (LLMs) exhibit remarkable versatility in adopting diverse personas. In this study, we examine how assigning a persona influences a model's reasoning on an objective task. Using activation patching, we take a first step toward understanding how key components of the model encode persona-specific information. Our findings reveal that the early Multi-Layer Perceptron (MLP) layers attend not only to the syntactic structure of the input but also process its semantic content. These layers transform persona tokens into richer representations, which are then used by the middle Multi-Head Attention (MHA) layers to shape the model's output. Additionally, we identify specific attention heads that disproportionately attend to racial and color-based identities.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.LG",
    "comment": "EMNLP (Findings) 2025",
    "pdf_url": "https://arxiv.org/pdf/2507.20936v2",
    "published_date": "2025-07-28 15:45:31 UTC",
    "updated_date": "2025-09-21 15:16:05 UTC"
  },
  {
    "arxiv_id": "2507.20930v2",
    "title": "FRED: Financial Retrieval-Enhanced Detection and Editing of Hallucinations in Language Models",
    "authors": [
      "Likun Tan",
      "Kuan-Wei Huang",
      "Kevin Wu"
    ],
    "abstract": "Hallucinations in large language models pose a critical challenge for applications requiring factual reliability, particularly in high-stakes domains such as finance. This work presents an effective approach for detecting and editing factually incorrect content in model-generated responses based on the provided context. Given a user-defined domain-specific error taxonomy, we construct a synthetic dataset by inserting tagged errors into financial question-answering corpora and then fine-tune four language models, Phi-4, Phi-4-mini, Qwen3-4B, and Qwen3-14B, to detect and edit these factual inaccuracies. Our best-performing model, fine-tuned Phi-4, achieves an 8% improvement in binary F1 score and a 30% gain in overall detection performance compared to OpenAI-o3. Notably, our fine-tuned Phi-4-mini model, despite having only 4 billion parameters, maintains competitive performance with just a 2% drop in binary detection and a 0.1% decline in overall detection compared to OpenAI-o3. Our work provides a practical solution for detecting and editing factual inconsistencies in financial text generation while introducing a generalizable framework that can enhance the trustworthiness and alignment of large language models across diverse applications beyond finance. Our code and data are available at https://github.com/pegasi-ai/shield.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2507.20930v2",
    "published_date": "2025-07-28 15:41:53 UTC",
    "updated_date": "2025-07-30 17:19:41 UTC"
  },
  {
    "arxiv_id": "2507.20924v1",
    "title": "FHSTP@EXIST 2025 Benchmark: Sexism Detection with Transparent Speech Concept Bottleneck Models",
    "authors": [
      "Roberto Labadie-Tamayo",
      "Adrian Jaques Böck",
      "Djordje Slijepčević",
      "Xihui Chen",
      "Andreas Babic",
      "Matthias Zeppelzauer"
    ],
    "abstract": "Sexism has become widespread on social media and in online conversation. To help address this issue, the fifth Sexism Identification in Social Networks (EXIST) challenge is initiated at CLEF 2025. Among this year's international benchmarks, we concentrate on solving the first task aiming to identify and classify sexism in social media textual posts. In this paper, we describe our solutions and report results for three subtasks: Subtask 1.1 - Sexism Identification in Tweets, Subtask 1.2 - Source Intention in Tweets, and Subtask 1.3 - Sexism Categorization in Tweets. We implement three models to address each subtask which constitute three individual runs: Speech Concept Bottleneck Model (SCBM), Speech Concept Bottleneck Model with Transformer (SCBMT), and a fine-tuned XLM-RoBERTa transformer model. SCBM uses descriptive adjectives as human-interpretable bottleneck concepts. SCBM leverages large language models (LLMs) to encode input texts into a human-interpretable representation of adjectives, then used to train a lightweight classifier for downstream tasks. SCBMT extends SCBM by fusing adjective-based representation with contextual embeddings from transformers to balance interpretability and classification performance. Beyond competitive results, these two models offer fine-grained explanations at both instance (local) and class (global) levels. We also investigate how additional metadata, e.g., annotators' demographic profiles, can be leveraged. For Subtask 1.1, XLM-RoBERTa, fine-tuned on provided data augmented with prior datasets, ranks 6th for English and Spanish and 4th for English in the Soft-Soft evaluation. Our SCBMT achieves 7th for English and Spanish and 6th for Spanish.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CY",
      "cs.SI"
    ],
    "primary_category": "cs.CL",
    "comment": "12 pages",
    "pdf_url": "https://arxiv.org/pdf/2507.20924v1",
    "published_date": "2025-07-28 15:30:17 UTC",
    "updated_date": "2025-07-28 15:30:17 UTC"
  },
  {
    "arxiv_id": "2507.20923v3",
    "title": "Pareto-Grid-Guided Large Language Models for Fast and High-Quality Heuristics Design in Multi-Objective Combinatorial Optimization",
    "authors": [
      "Minh Hieu Ha",
      "Hung Phan",
      "Tung Duy Doan",
      "Tung Dao",
      "Dao Tran",
      "Huynh Thi Thanh Binh"
    ],
    "abstract": "Multi-objective combinatorial optimization problems (MOCOP) frequently arise in practical applications that require the simultaneous optimization of conflicting objectives. Although traditional evolutionary algorithms can be effective, they typically depend on domain knowledge and repeated parameter tuning, limiting flexibility when applied to unseen MOCOP instances. Recently, integration of Large Language Models (LLMs) into evolutionary computation has opened new avenues for automatic heuristic generation, using their advanced language understanding and code synthesis capabilities. Nevertheless, most existing approaches predominantly focus on single-objective tasks, often neglecting key considerations such as runtime efficiency and heuristic diversity in multi-objective settings. To bridge this gap, we introduce Multi-heuristics for MOCOP via Pareto-Grid-guided Evolution of LLMs (MPaGE), a novel enhancement of the Simple Evolutionary Multiobjective Optimization (SEMO) framework that leverages LLMs and Pareto Front Grid (PFG) technique. By partitioning the objective space into grids and retaining top-performing candidates to guide heuristic generation, MPaGE utilizes LLMs to prioritize heuristics with semantically distinct logical structures during variation, thus promoting diversity and mitigating redundancy within the population. Through extensive evaluations, MPaGE demonstrates superior performance over existing LLM-based frameworks, and achieves competitive results to traditional Multi-objective evolutionary algorithms (MOEAs), with significantly faster runtime. Our code is available at: https://github.com/langkhachhoha/MPaGE.",
    "categories": [
      "cs.NE",
      "cs.AI"
    ],
    "primary_category": "cs.NE",
    "comment": "Accepted at AAAI-26",
    "pdf_url": "https://arxiv.org/pdf/2507.20923v3",
    "published_date": "2025-07-28 15:26:43 UTC",
    "updated_date": "2026-01-15 18:28:50 UTC"
  },
  {
    "arxiv_id": "2507.20919v1",
    "title": "Modeling User Behavior from Adaptive Surveys with Supplemental Context",
    "authors": [
      "Aman Shukla",
      "Daniel Patrick Scantlebury",
      "Rishabh Kumar"
    ],
    "abstract": "Modeling user behavior is critical across many industries where understanding preferences, intent, or decisions informs personalization, targeting, and strategic outcomes. Surveys have long served as a classical mechanism for collecting such behavioral data due to their interpretability, structure, and ease of deployment. However, surveys alone are inherently limited by user fatigue, incomplete responses, and practical constraints on their length making them insufficient for capturing user behavior. In this work, we present LANTERN (Late-Attentive Network for Enriched Response Modeling), a modular architecture for modeling user behavior by fusing adaptive survey responses with supplemental contextual signals. We demonstrate the architectural value of maintaining survey primacy through selective gating, residual connections and late fusion via cross-attention, treating survey data as the primary signal while incorporating external modalities only when relevant. LANTERN outperforms strong survey-only baselines in multi-label prediction of survey responses. We further investigate threshold sensitivity and the benefits of selective modality reliance through ablation and rare/frequent attribute analysis. LANTERN's modularity supports scalable integration of new encoders and evolving datasets. This work provides a practical and extensible blueprint for behavior modeling in survey-centric applications.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.IR"
    ],
    "primary_category": "cs.LG",
    "comment": "Best Paper, NewInML @ ICML 2025",
    "pdf_url": "https://arxiv.org/pdf/2507.20919v1",
    "published_date": "2025-07-28 15:19:54 UTC",
    "updated_date": "2025-07-28 15:19:54 UTC"
  },
  {
    "arxiv_id": "2507.20917v1",
    "title": "MediQAl: A French Medical Question Answering Dataset for Knowledge and Reasoning Evaluation",
    "authors": [
      "Adrien Bazoge"
    ],
    "abstract": "This work introduces MediQAl, a French medical question answering dataset designed to evaluate the capabilities of language models in factual medical recall and reasoning over real-world clinical scenarios. MediQAl contains 32,603 questions sourced from French medical examinations across 41 medical subjects. The dataset includes three tasks: (i) Multiple-Choice Question with Unique answer, (ii) Multiple-Choice Question with Multiple answer, and (iii) Open-Ended Question with Short-Answer. Each question is labeled as Understanding or Reasoning, enabling a detailed analysis of models' cognitive capabilities. We validate the MediQAl dataset through extensive evaluation with 14 large language models, including recent reasoning-augmented models, and observe a significant performance gap between factual recall and reasoning tasks. Our evaluation provides a comprehensive benchmark for assessing language models' performance on French medical question answering, addressing a crucial gap in multilingual resources for the medical domain.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2507.20917v1",
    "published_date": "2025-07-28 15:17:48 UTC",
    "updated_date": "2025-07-28 15:17:48 UTC"
  },
  {
    "arxiv_id": "2507.20913v1",
    "title": "HAMLET-FFD: Hierarchical Adaptive Multi-modal Learning Embeddings Transformation for Face Forgery Detection",
    "authors": [
      "Jialei Cui",
      "Jianwei Du",
      "Yanzhe Li",
      "Lei Gao",
      "Hui Jiang",
      "Chenfu Bao"
    ],
    "abstract": "The rapid evolution of face manipulation techniques poses a critical challenge for face forgery detection: cross-domain generalization. Conventional methods, which rely on simple classification objectives, often fail to learn domain-invariant representations. We propose HAMLET-FFD, a cognitively inspired Hierarchical Adaptive Multi-modal Learning framework that tackles this challenge via bidirectional cross-modal reasoning. Building on contrastive vision-language models such as CLIP, HAMLET-FFD introduces a knowledge refinement loop that iteratively assesses authenticity by integrating visual evidence with conceptual cues, emulating expert forensic analysis. A key innovation is a bidirectional fusion mechanism in which textual authenticity embeddings guide the aggregation of hierarchical visual features, while modulated visual features refine text embeddings to generate image-adaptive prompts. This closed-loop process progressively aligns visual observations with semantic priors to enhance authenticity assessment. By design, HAMLET-FFD freezes all pretrained parameters, serving as an external plugin that preserves CLIP's original capabilities. Extensive experiments demonstrate its superior generalization to unseen manipulations across multiple benchmarks, and visual analyses reveal a division of labor among embeddings, with distinct representations specializing in fine-grained artifact recognition.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2507.20913v1",
    "published_date": "2025-07-28 15:09:52 UTC",
    "updated_date": "2025-07-28 15:09:52 UTC"
  },
  {
    "arxiv_id": "2507.20907v2",
    "title": "SCORPION: Addressing Scanner-Induced Variability in Histopathology",
    "authors": [
      "Jeongun Ryu",
      "Heon Song",
      "Seungeun Lee",
      "Soo Ick Cho",
      "Jiwon Shin",
      "Kyunghyun Paeng",
      "Sérgio Pereira"
    ],
    "abstract": "Ensuring reliable model performance across diverse domains is a critical challenge in computational pathology. A particular source of variability in Whole-Slide Images is introduced by differences in digital scanners, thus calling for better scanner generalization. This is critical for the real-world adoption of computational pathology, where the scanning devices may differ per institution or hospital, and the model should not be dependent on scanner-induced details, which can ultimately affect the patient's diagnosis and treatment planning. However, past efforts have primarily focused on standard domain generalization settings, evaluating on unseen scanners during training, without directly evaluating consistency across scanners for the same tissue. To overcome this limitation, we introduce SCORPION, a new dataset explicitly designed to evaluate model reliability under scanner variability. SCORPION includes 480 tissue samples, each scanned with 5 scanners, yielding 2,400 spatially aligned patches. This scanner-paired design allows for the isolation of scanner-induced variability, enabling a rigorous evaluation of model consistency while controlling for differences in tissue composition. Furthermore, we propose SimCons, a flexible framework that combines augmentation-based domain generalization techniques with a consistency loss to explicitly address scanner generalization. We empirically show that SimCons improves model consistency on varying scanners without compromising task-specific performance. By releasing the SCORPION dataset and proposing SimCons, we provide the research community with a crucial resource for evaluating and improving model consistency across diverse scanners, setting a new standard for reliability testing.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "Accepted in UNSURE 2025 workshop in MICCAI",
    "pdf_url": "https://arxiv.org/pdf/2507.20907v2",
    "published_date": "2025-07-28 15:00:49 UTC",
    "updated_date": "2025-09-18 02:31:01 UTC"
  },
  {
    "arxiv_id": "2507.22086v1",
    "title": "TypyBench: Evaluating LLM Type Inference for Untyped Python Repositories",
    "authors": [
      "Honghua Dong",
      "Jiacheng Yang",
      "Xun Deng",
      "Yuhe Jiang",
      "Gennady Pekhimenko",
      "Fan Long",
      "Xujie Si"
    ],
    "abstract": "Type inference for dynamic languages like Python is a persistent challenge in software engineering. While large language models (LLMs) have shown promise in code understanding, their type inference capabilities remain underexplored. We introduce TypyBench, a benchmark designed to evaluate LLMs' type inference across entire Python repositories. TypyBench features two novel metrics: TypeSim, which captures nuanced semantic relationships between predicted and ground truth types, and TypeCheck, which assesses type consistency across codebases. Our evaluation of various LLMs on a curated dataset of 50 high-quality Python repositories reveals that, although LLMs achieve decent TypeSim scores, they struggle with complex nested types and exhibit significant type consistency errors. These findings suggest that future research should shift focus from improving type similarity to addressing repository-level consistency. TypyBench provides a foundation for this new direction, offering insights into model performance across different type complexities and usage contexts. Our code and data are available at https://github.com/typybench/typybench.",
    "categories": [
      "cs.SE",
      "cs.AI",
      "cs.PL"
    ],
    "primary_category": "cs.SE",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2507.22086v1",
    "published_date": "2025-07-28 14:54:00 UTC",
    "updated_date": "2025-07-28 14:54:00 UTC"
  },
  {
    "arxiv_id": "2507.20900v2",
    "title": "Music Arena: Live Evaluation for Text-to-Music",
    "authors": [
      "Yonghyun Kim",
      "Wayne Chi",
      "Anastasios N. Angelopoulos",
      "Wei-Lin Chiang",
      "Koichi Saito",
      "Shinji Watanabe",
      "Yuki Mitsufuji",
      "Chris Donahue"
    ],
    "abstract": "We present Music Arena, an open platform for scalable human preference evaluation of text-to-music (TTM) models. Soliciting human preferences via listening studies is the gold standard for evaluation in TTM, but these studies are expensive to conduct and difficult to compare, as study protocols may differ across systems. Moreover, human preferences might help researchers align their TTM systems or improve automatic evaluation metrics, but an open and renewable source of preferences does not currently exist. We aim to fill these gaps by offering *live* evaluation for TTM. In Music Arena, real-world users input text prompts of their choosing and compare outputs from two TTM systems, and their preferences are used to compile a leaderboard. While Music Arena follows recent evaluation trends in other AI domains, we also design it with key features tailored to music: an LLM-based routing system to navigate the heterogeneous type signatures of TTM systems, and the collection of *detailed* preferences including listening data and natural language feedback. We also propose a rolling data release policy with user privacy guarantees, providing a renewable source of preference data and increasing platform transparency. Through its standardized evaluation protocol, transparent data access policies, and music-specific features, Music Arena not only addresses key challenges in the TTM ecosystem but also demonstrates how live evaluation can be thoughtfully adapted to unique characteristics of specific AI domains.\n  Music Arena is available at: https://music-arena.org . Preference data is available at: https://huggingface.co/music-arena .",
    "categories": [
      "cs.SD",
      "cs.AI",
      "cs.MM"
    ],
    "primary_category": "cs.SD",
    "comment": "NeurIPS 2025 Creative AI Track",
    "pdf_url": "https://arxiv.org/pdf/2507.20900v2",
    "published_date": "2025-07-28 14:52:57 UTC",
    "updated_date": "2025-11-02 01:58:15 UTC"
  },
  {
    "arxiv_id": "2507.20894v1",
    "title": "Online hierarchical partitioning of the output space in extreme multi-label data stream",
    "authors": [
      "Lara Neves",
      "Afonso Lourenço",
      "Alberto Cano",
      "Goreti Marreiros"
    ],
    "abstract": "Mining data streams with multi-label outputs poses significant challenges due to evolving distributions, high-dimensional label spaces, sparse label occurrences, and complex label dependencies. Moreover, concept drift affects not only input distributions but also label correlations and imbalance ratios over time, complicating model adaptation. To address these challenges, structured learners are categorized into local and global methods. Local methods break down the task into simpler components, while global methods adapt the algorithm to the full output space, potentially yielding better predictions by exploiting label correlations. This work introduces iHOMER (Incremental Hierarchy Of Multi-label Classifiers), an online multi-label learning framework that incrementally partitions the label space into disjoint, correlated clusters without relying on predefined hierarchies. iHOMER leverages online divisive-agglomerative clustering based on \\textit{Jaccard} similarity and a global tree-based learner driven by a multivariate \\textit{Bernoulli} process to guide instance partitioning. To address non-stationarity, it integrates drift detection mechanisms at both global and local levels, enabling dynamic restructuring of label partitions and subtrees. Experiments across 23 real-world datasets show iHOMER outperforms 5 state-of-the-art global baselines, such as MLHAT, MLHT of Pruned Sets and iSOUPT, by 23\\%, and 12 local baselines, such as binary relevance transformations of kNN, EFDT, ARF, and ADWIN bagging/boosting ensembles, by 32\\%, establishing its robustness for online multi-label classification.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted at 28th European Conference on Artificial Intelligence (ECAI 2025)",
    "pdf_url": "https://arxiv.org/pdf/2507.20894v1",
    "published_date": "2025-07-28 14:47:13 UTC",
    "updated_date": "2025-07-28 14:47:13 UTC"
  },
  {
    "arxiv_id": "2508.00899v1",
    "title": "ff4ERA: A new Fuzzy Framework for Ethical Risk Assessment in AI",
    "authors": [
      "Abeer Dyoub",
      "Ivan Letteri",
      "Francesca A. Lisi"
    ],
    "abstract": "The emergence of Symbiotic AI (SAI) introduces new challenges to ethical decision-making as it deepens human-AI collaboration. As symbiosis grows, AI systems pose greater ethical risks, including harm to human rights and trust. Ethical Risk Assessment (ERA) thus becomes crucial for guiding decisions that minimize such risks. However, ERA is hindered by uncertainty, vagueness, and incomplete information, and morality itself is context-dependent and imprecise. This motivates the need for a flexible, transparent, yet robust framework for ERA. Our work supports ethical decision-making by quantitatively assessing and prioritizing multiple ethical risks so that artificial agents can select actions aligned with human values and acceptable risk levels. We introduce ff4ERA, a fuzzy framework that integrates Fuzzy Logic, the Fuzzy Analytic Hierarchy Process (FAHP), and Certainty Factors (CF) to quantify ethical risks via an Ethical Risk Score (ERS) for each risk type. The final ERS combines the FAHP-derived weight, propagated CF, and risk level. The framework offers a robust mathematical approach for collaborative ERA modeling and systematic, step-by-step analysis. A case study confirms that ff4ERA yields context-sensitive, ethically meaningful risk scores reflecting both expert input and sensor-based evidence. Risk scores vary consistently with relevant factors while remaining robust to unrelated inputs. Local sensitivity analysis shows predictable, mostly monotonic behavior across perturbations, and global Sobol analysis highlights the dominant influence of expert-defined weights and certainty factors, validating the model design. Overall, the results demonstrate ff4ERA ability to produce interpretable, traceable, and risk-aware ethical assessments, enabling what-if analyses and guiding designers in calibrating membership functions and expert judgments for reliable ethical decision support.",
    "categories": [
      "cs.AI",
      "cs.CY",
      "cs.HC",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2508.00899v1",
    "published_date": "2025-07-28 14:41:36 UTC",
    "updated_date": "2025-07-28 14:41:36 UTC"
  },
  {
    "arxiv_id": "2507.20880v1",
    "title": "JAM: A Tiny Flow-based Song Generator with Fine-grained Controllability and Aesthetic Alignment",
    "authors": [
      "Renhang Liu",
      "Chia-Yu Hung",
      "Navonil Majumder",
      "Taylor Gautreaux",
      "Amir Ali Bagherzadeh",
      "Chuan Li",
      "Dorien Herremans",
      "Soujanya Poria"
    ],
    "abstract": "Diffusion and flow-matching models have revolutionized automatic text-to-audio generation in recent times. These models are increasingly capable of generating high quality and faithful audio outputs capturing to speech and acoustic events. However, there is still much room for improvement in creative audio generation that primarily involves music and songs. Recent open lyrics-to-song models, such as, DiffRhythm, ACE-Step, and LeVo, have set an acceptable standard in automatic song generation for recreational use. However, these models lack fine-grained word-level controllability often desired by musicians in their workflows. To the best of our knowledge, our flow-matching-based JAM is the first effort toward endowing word-level timing and duration control in song generation, allowing fine-grained vocal control. To enhance the quality of generated songs to better align with human preferences, we implement aesthetic alignment through Direct Preference Optimization, which iteratively refines the model using a synthetic dataset, eliminating the need or manual data annotations. Furthermore, we aim to standardize the evaluation of such lyrics-to-song models through our public evaluation dataset JAME. We show that JAM outperforms the existing models in terms of the music-specific attributes.",
    "categories": [
      "cs.SD",
      "cs.AI"
    ],
    "primary_category": "cs.SD",
    "comment": "https://github.com/declare-lab/jamify",
    "pdf_url": "https://arxiv.org/pdf/2507.20880v1",
    "published_date": "2025-07-28 14:34:02 UTC",
    "updated_date": "2025-07-28 14:34:02 UTC"
  },
  {
    "arxiv_id": "2507.20872v1",
    "title": "Not Only Grey Matter: OmniBrain for Robust Multimodal Classification of Alzheimer's Disease",
    "authors": [
      "Ahmed Sharshar",
      "Yasser Ashraf",
      "Tameem Bakr",
      "Salma Hassan",
      "Hosam Elgendy",
      "Mohammad Yaqub",
      "Mohsen Guizani"
    ],
    "abstract": "Alzheimer's disease affects over 55 million people worldwide and is projected to more than double by 2050, necessitating rapid, accurate, and scalable diagnostics. However, existing approaches are limited because they cannot achieve clinically acceptable accuracy, generalization across datasets, robustness to missing modalities, and explainability all at the same time. This inability to satisfy all these requirements simultaneously undermines their reliability in clinical settings. We propose OmniBrain, a multimodal framework that integrates brain MRI, radiomics, gene expression, and clinical data using a unified model with cross-attention and modality dropout. OmniBrain achieves $92.2 \\pm 2.4\\%$accuracy on the ANMerge dataset and generalizes to the MRI-only ADNI dataset with $70.4 \\pm 2.7\\%$ accuracy, outperforming unimodal and prior multimodal approaches. Explainability analyses highlight neuropathologically relevant brain regions and genes, enhancing clinical trust. OmniBrain offers a robust, interpretable, and practical solution for real-world Alzheimer's diagnosis.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "Published in Third Workshop on Computer Vision for Automated Medical Diagnosis CVAMD 2025 in ICCV 2025",
    "pdf_url": "https://arxiv.org/pdf/2507.20872v1",
    "published_date": "2025-07-28 14:24:13 UTC",
    "updated_date": "2025-07-28 14:24:13 UTC"
  },
  {
    "arxiv_id": "2508.04714v2",
    "title": "Prescriptive Agents based on RAG for Automated Maintenance (PARAM)",
    "authors": [
      "Chitranshu Harbola",
      "Anupam Purwar"
    ],
    "abstract": "Industrial machinery maintenance requires timely intervention to prevent catastrophic failures and optimize operational efficiency. This paper presents an integrated Large Language Model (LLM)-based intelligent system for prescriptive maintenance that extends beyond traditional anomaly detection to provide actionable maintenance recommendations. Building upon our prior LAMP framework for numerical data analysis, we develop a comprehensive solution that combines bearing vibration frequency analysis with multi agentic generation for intelligent maintenance planning. Our approach serializes bearing vibration data (BPFO, BPFI, BSF, FTF frequencies) into natural language for LLM processing, enabling few-shot anomaly detection with high accuracy. The system classifies fault types (inner race, outer race, ball/roller, cage faults) and assesses severity levels. A multi-agentic component processes maintenance manuals using vector embeddings and semantic search, while also conducting web searches to retrieve comprehensive procedural knowledge and access up-to-date maintenance practices for more accurate and in-depth recommendations. The Gemini model then generates structured maintenance recommendations includes immediate actions, inspection checklists, corrective measures, parts requirements, and timeline specifications. Experimental validation in bearing vibration datasets demonstrates effective anomaly detection and contextually relevant maintenance guidance. The system successfully bridges the gap between condition monitoring and actionable maintenance planning, providing industrial practitioners with intelligent decision support. This work advances the application of LLMs in industrial maintenance, offering a scalable framework for prescriptive maintenance across machinery components and industrial sectors.",
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.LG",
      "cs.MA",
      "eess.SP"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2508.04714v2",
    "published_date": "2025-07-28 14:22:19 UTC",
    "updated_date": "2025-08-20 19:08:28 UTC"
  },
  {
    "arxiv_id": "2507.20853v1",
    "title": "Geometry of Neural Reinforcement Learning in Continuous State and Action Spaces",
    "authors": [
      "Saket Tiwari",
      "Omer Gottesman",
      "George Konidaris"
    ],
    "abstract": "Advances in reinforcement learning (RL) have led to its successful application in complex tasks with continuous state and action spaces. Despite these advances in practice, most theoretical work pertains to finite state and action spaces. We propose building a theoretical understanding of continuous state and action spaces by employing a geometric lens to understand the locally attained set of states. The set of all parametrised policies learnt through a semi-gradient based approach induces a set of attainable states in RL. We show that the training dynamics of a two-layer neural policy induce a low dimensional manifold of attainable states embedded in the high-dimensional nominal state space trained using an actor-critic algorithm. We prove that, under certain conditions, the dimensionality of this manifold is of the order of the dimensionality of the action space. This is the first result of its kind, linking the geometry of the state space to the dimensionality of the action space. We empirically corroborate this upper bound for four MuJoCo environments and also demonstrate the results in a toy environment with varying dimensionality. We also show the applicability of this theoretical result by introducing a local manifold learning layer to the policy and value function networks to improve the performance in control environments with very high degrees of freedom by changing one layer of the neural network to learn sparse representations.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "Proceedings of the Thirteenth International Conference on Learning Representations (ICLR 2025). arXiv admin note: text overlap with arXiv:2301.00009",
    "pdf_url": "https://arxiv.org/pdf/2507.20853v1",
    "published_date": "2025-07-28 14:06:44 UTC",
    "updated_date": "2025-07-28 14:06:44 UTC"
  },
  {
    "arxiv_id": "2507.20850v1",
    "title": "Free Energy-Inspired Cognitive Risk Integration for AV Navigation in Pedestrian-Rich Environments",
    "authors": [
      "Meiting Dang",
      "Yanping Wu",
      "Yafei Wang",
      "Dezong Zhao",
      "David Flynn",
      "Chongfeng Wei"
    ],
    "abstract": "Recent advances in autonomous vehicle (AV) behavior planning have shown impressive social interaction capabilities when interacting with other road users. However, achieving human-like prediction and decision-making in interactions with vulnerable road users remains a key challenge in complex multi-agent interactive environments. Existing research focuses primarily on crowd navigation for small mobile robots, which cannot be directly applied to AVs due to inherent differences in their decision-making strategies and dynamic boundaries. Moreover, pedestrians in these multi-agent simulations follow fixed behavior patterns that cannot dynamically respond to AV actions. To overcome these limitations, this paper proposes a novel framework for modeling interactions between the AV and multiple pedestrians. In this framework, a cognitive process modeling approach inspired by the Free Energy Principle is integrated into both the AV and pedestrian models to simulate more realistic interaction dynamics. Specifically, the proposed pedestrian Cognitive-Risk Social Force Model adjusts goal-directed and repulsive forces using a fused measure of cognitive uncertainty and physical risk to produce human-like trajectories. Meanwhile, the AV leverages this fused risk to construct a dynamic, risk-aware adjacency matrix for a Graph Convolutional Network within a Soft Actor-Critic architecture, allowing it to make more reasonable and informed decisions. Simulation results indicate that our proposed framework effectively improves safety, efficiency, and smoothness of AV navigation compared to the state-of-the-art method.",
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "primary_category": "cs.RO",
    "comment": "14 pages, 5 figures",
    "pdf_url": "https://arxiv.org/pdf/2507.20850v1",
    "published_date": "2025-07-28 14:02:00 UTC",
    "updated_date": "2025-07-28 14:02:00 UTC"
  },
  {
    "arxiv_id": "2507.20836v4",
    "title": "First Hallucination Tokens Are Different from Conditional Ones",
    "authors": [
      "Jakob Snel",
      "Seong Joon Oh"
    ],
    "abstract": "Large Language Models (LLMs) hallucinate, and detecting these cases is key to ensuring trust. While many approaches address hallucination detection at the response or span level, recent work explores token-level detection, enabling more fine-grained intervention. However, the distribution of hallucination signal across sequences of hallucinated tokens remains unexplored. We leverage token-level annotations from the RAGTruth corpus and find that the first hallucinated token is far more detectable than later ones. This structural property holds across models, suggesting that first hallucination tokens play a key role in token-level hallucination detection. Our code is available at https://github.com/jakobsnl/RAGTruth_Xtended.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "4.5 pages, 3 figures, Dataset, Knowledge Paper, Hallucination, Trustworthiness",
    "pdf_url": "https://arxiv.org/pdf/2507.20836v4",
    "published_date": "2025-07-28 13:44:21 UTC",
    "updated_date": "2025-10-06 14:29:58 UTC"
  },
  {
    "arxiv_id": "2507.20810v1",
    "title": "Why Flow Matching is Particle Swarm Optimization?",
    "authors": [
      "Kaichen Ouyang"
    ],
    "abstract": "This paper preliminarily investigates the duality between flow matching in generative models and particle swarm optimization (PSO) in evolutionary computation. Through theoretical analysis, we reveal the intrinsic connections between these two approaches in terms of their mathematical formulations and optimization mechanisms: the vector field learning in flow matching shares similar mathematical expressions with the velocity update rules in PSO; both methods follow the fundamental framework of progressive evolution from initial to target distributions; and both can be formulated as dynamical systems governed by ordinary differential equations. Our study demonstrates that flow matching can be viewed as a continuous generalization of PSO, while PSO provides a discrete implementation of swarm intelligence principles. This duality understanding establishes a theoretical foundation for developing novel hybrid algorithms and creates a unified framework for analyzing both methods. Although this paper only presents preliminary discussions, the revealed correspondences suggest several promising research directions, including improving swarm intelligence algorithms based on flow matching principles and enhancing generative models using swarm intelligence concepts.",
    "categories": [
      "cs.NE",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.NE",
    "comment": "7 pages, 0 figures",
    "pdf_url": "https://arxiv.org/pdf/2507.20810v1",
    "published_date": "2025-07-28 13:21:14 UTC",
    "updated_date": "2025-07-28 13:21:14 UTC"
  },
  {
    "arxiv_id": "2507.20804v1",
    "title": "MMGraphRAG: Bridging Vision and Language with Interpretable Multimodal Knowledge Graphs",
    "authors": [
      "Xueyao Wan",
      "Hang Yu"
    ],
    "abstract": "Retrieval-Augmented Generation (RAG) enhances language model generation by retrieving relevant information from external knowledge bases. However, conventional RAG methods face the issue of missing multimodal information. Multimodal RAG methods address this by fusing images and text through mapping them into a shared embedding space, but they fail to capture the structure of knowledge and logical chains between modalities. Moreover, they also require large-scale training for specific tasks, resulting in limited generalizing ability. To address these limitations, we propose MMGraphRAG, which refines visual content through scene graphs and constructs a multimodal knowledge graph (MMKG) in conjunction with text-based KG. It employs spectral clustering to achieve cross-modal entity linking and retrieves context along reasoning paths to guide the generative process. Experimental results show that MMGraphRAG achieves state-of-the-art performance on the DocBench and MMLongBench datasets, demonstrating strong domain adaptability and clear reasoning paths.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2507.20804v1",
    "published_date": "2025-07-28 13:16:23 UTC",
    "updated_date": "2025-07-28 13:16:23 UTC"
  },
  {
    "arxiv_id": "2507.20800v4",
    "title": "LanternNet: A Hub-and-Spoke System to Seek and Suppress Spotted Lanternfly Populations",
    "authors": [
      "Vinil Polepalli"
    ],
    "abstract": "The invasive spotted lanternfly (SLF) poses a significant threat to agriculture and ecosystems, causing widespread damage. Current control methods, such as egg scraping, pesticides, and quarantines, prove labor-intensive, environmentally hazardous, and inadequate for long-term SLF suppression. This research introduces LanternNet, a novel autonomous robotic Hub-and-Spoke system designed for scalable detection and suppression of SLF populations. A central, tree-mimicking hub utilizes a YOLOv8 computer vision model for precise SLF identification. Three specialized robotic spokes perform targeted tasks: pest neutralization, environmental monitoring, and navigation/mapping. Field deployment across multiple infested sites over 5 weeks demonstrated LanternNet's efficacy. Quantitative analysis revealed significant reductions (p < 0.01, paired t-tests) in SLF populations and corresponding improvements in tree health indicators across the majority of test sites. Compared to conventional methods, LanternNet offers substantial cost advantages and improved scalability. Furthermore, the system's adaptability for enhanced autonomy and targeting of other invasive species presents significant potential for broader ecological impact. LanternNet demonstrates the transformative potential of integrating robotics and AI for advanced invasive species management and improved environmental outcomes.",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.RO",
    "comment": "The submission is being withdrawn pending coordination with co-authors before resubmission",
    "pdf_url": "https://arxiv.org/pdf/2507.20800v4",
    "published_date": "2025-07-28 13:08:33 UTC",
    "updated_date": "2025-09-05 21:53:17 UTC"
  },
  {
    "arxiv_id": "2507.20796v1",
    "title": "Aligning Large Language Model Agents with Rational and Moral Preferences: A Supervised Fine-Tuning Approach",
    "authors": [
      "Wei Lu",
      "Daniel L. Chen",
      "Christian B. Hansen"
    ],
    "abstract": "Understanding how large language model (LLM) agents behave in strategic interactions is essential as these systems increasingly participate autonomously in economically and morally consequential decisions. We evaluate LLM preferences using canonical economic games, finding substantial deviations from human behavior. Models like GPT-4o show excessive cooperation and limited incentive sensitivity, while reasoning models, such as o3-mini, align more consistently with payoff-maximizing strategies. We propose a supervised fine-tuning pipeline that uses synthetic datasets derived from economic reasoning to align LLM agents with economic preferences, focusing on two stylized preference structures. In the first, utility depends only on individual payoffs (homo economicus), while utility also depends on a notion of Kantian universalizability in the second preference structure (homo moralis). We find that fine-tuning based on small datasets shifts LLM agent behavior toward the corresponding economic agent. We further assess the fine-tuned agents' behavior in two applications: Moral dilemmas involving autonomous vehicles and algorithmic pricing in competitive markets. These examples illustrate how different normative objectives embedded via realizations from structured preference structures can influence market and moral outcomes. This work contributes a replicable, cost-efficient, and economically grounded pipeline to align AI preferences using moral-economic principles.",
    "categories": [
      "econ.GN",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "econ.GN",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2507.20796v1",
    "published_date": "2025-07-28 13:05:04 UTC",
    "updated_date": "2025-07-28 13:05:04 UTC"
  },
  {
    "arxiv_id": "2507.20782v1",
    "title": "Investigation of Accuracy and Bias in Face Recognition Trained with Synthetic Data",
    "authors": [
      "Pavel Korshunov",
      "Ketan Kotwal",
      "Christophe Ecabert",
      "Vidit Vidit",
      "Amir Mohammadi",
      "Sebastien Marcel"
    ],
    "abstract": "Synthetic data has emerged as a promising alternative for training face recognition (FR) models, offering advantages in scalability, privacy compliance, and potential for bias mitigation. However, critical questions remain on whether both high accuracy and fairness can be achieved with synthetic data. In this work, we evaluate the impact of synthetic data on bias and performance of FR systems. We generate balanced face dataset, FairFaceGen, using two state of the art text-to-image generators, Flux.1-dev and Stable Diffusion v3.5 (SD35), and combine them with several identity augmentation methods, including Arc2Face and four IP-Adapters. By maintaining equal identity count across synthetic and real datasets, we ensure fair comparisons when evaluating FR performance on standard (LFW, AgeDB-30, etc.) and challenging IJB-B/C benchmarks and FR bias on Racial Faces in-the-Wild (RFW) dataset. Our results demonstrate that although synthetic data still lags behind the real datasets in the generalization on IJB-B/C, demographically balanced synthetic datasets, especially those generated with SD35, show potential for bias mitigation. We also observe that the number and quality of intra-class augmentations significantly affect FR accuracy and fairness. These findings provide practical guidelines for constructing fairer FR systems using synthetic data.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "Accepted for publication in IEEE International Joint Conference on Biometrics (IJCB), 2025",
    "pdf_url": "https://arxiv.org/pdf/2507.20782v1",
    "published_date": "2025-07-28 12:52:23 UTC",
    "updated_date": "2025-07-28 12:52:23 UTC"
  },
  {
    "arxiv_id": "2507.20774v1",
    "title": "evalSmarT: An LLM-Based Framework for Evaluating Smart Contract Generated Comments",
    "authors": [
      "Fatou Ndiaye Mbodji"
    ],
    "abstract": "Smart contract comment generation has gained traction as a means to improve code comprehension and maintainability in blockchain systems. However, evaluating the quality of generated comments remains a challenge. Traditional metrics such as BLEU and ROUGE fail to capture domain-specific nuances, while human evaluation is costly and unscalable. In this paper, we present \\texttt{evalSmarT}, a modular and extensible framework that leverages large language models (LLMs) as evaluators. The system supports over 400 evaluator configurations by combining approximately 40 LLMs with 10 prompting strategies. We demonstrate its application in benchmarking comment generation tools and selecting the most informative outputs. Our results show that prompt design significantly impacts alignment with human judgment, and that LLM-based evaluation offers a scalable and semantically rich alternative to existing methods.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "4 pages, 4 tables",
    "pdf_url": "https://arxiv.org/pdf/2507.20774v1",
    "published_date": "2025-07-28 12:37:43 UTC",
    "updated_date": "2025-07-28 12:37:43 UTC"
  },
  {
    "arxiv_id": "2507.20758v1",
    "title": "How Chain-of-Thought Works? Tracing Information Flow from Decoding, Projection, and Activation",
    "authors": [
      "Hao Yang",
      "Qinghua Zhao",
      "Lei Li"
    ],
    "abstract": "Chain-of-Thought (CoT) prompting significantly enhances model reasoning, yet its internal mechanisms remain poorly understood. We analyze CoT's operational principles by reversely tracing information flow across decoding, projection, and activation phases. Our quantitative analysis suggests that CoT may serve as a decoding space pruner, leveraging answer templates to guide output generation, with higher template adherence strongly correlating with improved performance. Furthermore, we surprisingly find that CoT modulates neuron engagement in a task-dependent manner: reducing neuron activation in open-domain tasks, yet increasing it in closed-domain scenarios. These findings offer a novel mechanistic interpretability framework and critical insights for enabling targeted CoT interventions to design more efficient and robust prompts. We released our code and data at https://anonymous.4open.science/r/cot-D247.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2507.20758v1",
    "published_date": "2025-07-28 12:11:16 UTC",
    "updated_date": "2025-07-28 12:11:16 UTC"
  },
  {
    "arxiv_id": "2507.20757v1",
    "title": "Learning to See Inside Opaque Liquid Containers using Speckle Vibrometry",
    "authors": [
      "Matan Kichler",
      "Shai Bagon",
      "Mark Sheinin"
    ],
    "abstract": "Computer vision seeks to infer a wide range of information about objects and events. However, vision systems based on conventional imaging are limited to extracting information only from the visible surfaces of scene objects. For instance, a vision system can detect and identify a Coke can in the scene, but it cannot determine whether the can is full or empty. In this paper, we aim to expand the scope of computer vision to include the novel task of inferring the hidden liquid levels of opaque containers by sensing the tiny vibrations on their surfaces. Our method provides a first-of-a-kind way to inspect the fill level of multiple sealed containers remotely, at once, without needing physical manipulation and manual weighing. First, we propose a novel speckle-based vibration sensing system for simultaneously capturing scene vibrations on a 2D grid of points. We use our system to efficiently and remotely capture a dataset of vibration responses for a variety of everyday liquid containers. Then, we develop a transformer-based approach for analyzing the captured vibrations and classifying the container type and its hidden liquid level at the time of measurement. Our architecture is invariant to the vibration source, yielding correct liquid level estimates for controlled and ambient scene sound sources. Moreover, our model generalizes to unseen container instances within known classes (e.g., training on five Coke cans of a six-pack, testing on a sixth) and fluid levels. We demonstrate our method by recovering liquid levels from various everyday containers.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "ICCV 2025",
    "pdf_url": "https://arxiv.org/pdf/2507.20757v1",
    "published_date": "2025-07-28 12:11:12 UTC",
    "updated_date": "2025-07-28 12:11:12 UTC"
  },
  {
    "arxiv_id": "2507.20755v1",
    "title": "Beyond Listenership: AI-Predicted Interventions Drive Improvements in Maternal Health Behaviours",
    "authors": [
      "Arpan Dasgupta",
      "Sarvesh Gharat",
      "Neha Madhiwalla",
      "Aparna Hegde",
      "Milind Tambe",
      "Aparna Taneja"
    ],
    "abstract": "Automated voice calls with health information are a proven method for disseminating maternal and child health information among beneficiaries and are deployed in several programs around the world. However, these programs often suffer from beneficiary dropoffs and poor engagement. In previous work, through real-world trials, we showed that an AI model, specifically a restless bandit model, could identify beneficiaries who would benefit most from live service call interventions, preventing dropoffs and boosting engagement. However, one key question has remained open so far: does such improved listenership via AI-targeted interventions translate into beneficiaries' improved knowledge and health behaviors? We present a first study that shows not only listenership improvements due to AI interventions, but also simultaneously links these improvements to health behavior changes. Specifically, we demonstrate that AI-scheduled interventions, which enhance listenership, lead to statistically significant improvements in beneficiaries' health behaviors such as taking iron or calcium supplements in the postnatal period, as well as understanding of critical health topics during pregnancy and infancy. This underscores the potential of AI to drive meaningful improvements in maternal and child health.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2507.20755v1",
    "published_date": "2025-07-28 12:06:22 UTC",
    "updated_date": "2025-07-28 12:06:22 UTC"
  },
  {
    "arxiv_id": "2507.22171v2",
    "title": "Enhancing Jailbreak Attacks on LLMs via Persona Prompts",
    "authors": [
      "Zheng Zhang",
      "Peilin Zhao",
      "Deheng Ye",
      "Hao Wang"
    ],
    "abstract": "Jailbreak attacks aim to exploit large language models (LLMs) by inducing them to generate harmful content, thereby revealing their vulnerabilities. Understanding and addressing these attacks is crucial for advancing the field of LLM safety. Previous jailbreak approaches have mainly focused on direct manipulations of harmful intent, with limited attention to the impact of persona prompts. In this study, we systematically explore the efficacy of persona prompts in compromising LLM defenses. We propose a genetic algorithm-based method that automatically crafts persona prompts to bypass LLM's safety mechanisms. Our experiments reveal that: (1) our evolved persona prompts reduce refusal rates by 50-70% across multiple LLMs, and (2) these prompts demonstrate synergistic effects when combined with existing attack methods, increasing success rates by 10-20%. Our code and data are available at https://github.com/CjangCjengh/Generic_Persona.",
    "categories": [
      "cs.CR",
      "cs.AI"
    ],
    "primary_category": "cs.CR",
    "comment": "Workshop on LLM Persona Modeling at NeurIPS 2025",
    "pdf_url": "https://arxiv.org/pdf/2507.22171v2",
    "published_date": "2025-07-28 12:03:22 UTC",
    "updated_date": "2025-11-30 18:50:44 UTC"
  },
  {
    "arxiv_id": "2507.20753v1",
    "title": "Industry Insights from Comparing Deep Learning and GBDT Models for E-Commerce Learning-to-Rank",
    "authors": [
      "Yunus Lutz",
      "Timo Wilm",
      "Philipp Duwe"
    ],
    "abstract": "In e-commerce recommender and search systems, tree-based models, such as LambdaMART, have set a strong baseline for Learning-to-Rank (LTR) tasks. Despite their effectiveness and widespread adoption in industry, the debate continues whether deep neural networks (DNNs) can outperform traditional tree-based models in this domain. To contribute to this discussion, we systematically benchmark DNNs against our production-grade LambdaMART model. We evaluate multiple DNN architectures and loss functions on a proprietary dataset from OTTO and validate our findings through an 8-week online A/B test. The results show that a simple DNN architecture outperforms a strong tree-based baseline in terms of total clicks and revenue, while achieving parity in total units sold.",
    "categories": [
      "cs.IR",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.IR",
    "comment": "This work was accepted for publication in the 19th ACM Conference on Recommender Systems (RecSys 2025). The final published version will be available at the ACM Digital Library",
    "pdf_url": "https://arxiv.org/pdf/2507.20753v1",
    "published_date": "2025-07-28 12:02:02 UTC",
    "updated_date": "2025-07-28 12:02:02 UTC"
  },
  {
    "arxiv_id": "2507.20746v2",
    "title": "AR-LIF: Adaptive reset leaky integrate-and-fire neuron for spiking neural networks",
    "authors": [
      "Zeyu Huang",
      "Wei Meng",
      "Quan Liu",
      "Kun Chen",
      "Li Ma"
    ],
    "abstract": "Spiking neural networks offer low energy consumption due to their event-driven nature. Beyond binary spike outputs, their intrinsic floating-point dynamics merit greater attention. Neuronal threshold levels and reset modes critically determine spike count and timing. Hard reset cause information loss, while soft reset apply uniform treatment to neurons. To address these issues, we design an adaptive reset neuron that establishes relationships between inputs, outputs, and reset, while integrating a simple yet effective threshold adjustment strategy. Experimental results demonstrate that our method achieves excellent performance while maintaining lower energy consumption. In particular, it attains state-of-the-art accuracy on Tiny-ImageNet and CIFAR10-DVS. Codes are available at https://github.com/2ephyrus/AR-LIF.",
    "categories": [
      "cs.NE",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.NE",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2507.20746v2",
    "published_date": "2025-07-28 11:54:48 UTC",
    "updated_date": "2025-09-01 08:03:24 UTC"
  },
  {
    "arxiv_id": "2507.20745v1",
    "title": "Regularizing Subspace Redundancy of Low-Rank Adaptation",
    "authors": [
      "Yue Zhu",
      "Haiwen Diao",
      "Shang Gao",
      "Jiazuo Yu",
      "Jiawen Zhu",
      "Yunzhi Zhuge",
      "Shuai Hao",
      "Xu Jia",
      "Lu Zhang",
      "Ying Zhang",
      "Huchuan Lu"
    ],
    "abstract": "Low-Rank Adaptation (LoRA) and its variants have delivered strong capability in Parameter-Efficient Transfer Learning (PETL) by minimizing trainable parameters and benefiting from reparameterization. However, their projection matrices remain unrestricted during training, causing high representation redundancy and diminishing the effectiveness of feature adaptation in the resulting subspaces. While existing methods mitigate this by manually adjusting the rank or implicitly applying channel-wise masks, they lack flexibility and generalize poorly across various datasets and architectures. Hence, we propose ReSoRA, a method that explicitly models redundancy between mapping subspaces and adaptively Regularizes Subspace redundancy of Low-Rank Adaptation. Specifically, it theoretically decomposes the low-rank submatrices into multiple equivalent subspaces and systematically applies de-redundancy constraints to the feature distributions across different projections. Extensive experiments validate that our proposed method consistently facilitates existing state-of-the-art PETL methods across various backbones and datasets in vision-language retrieval and standard visual classification benchmarks. Besides, as a training supervision, ReSoRA can be seamlessly integrated into existing approaches in a plug-and-play manner, with no additional inference costs. Code is publicly available at: https://github.com/Lucenova/ReSoRA.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.MM"
    ],
    "primary_category": "cs.CV",
    "comment": "10 pages, 4 figures, Accepted by ACMMM2025",
    "pdf_url": "https://arxiv.org/pdf/2507.20745v1",
    "published_date": "2025-07-28 11:52:56 UTC",
    "updated_date": "2025-07-28 11:52:56 UTC"
  },
  {
    "arxiv_id": "2507.20737v1",
    "title": "Multi-Masked Querying Network for Robust Emotion Recognition from Incomplete Multi-Modal Physiological Signals",
    "authors": [
      "Geng-Xin Xu",
      "Xiang Zuo",
      "Ye Li"
    ],
    "abstract": "Emotion recognition from physiological data is crucial for mental health assessment, yet it faces two significant challenges: incomplete multi-modal signals and interference from body movements and artifacts. This paper presents a novel Multi-Masked Querying Network (MMQ-Net) to address these issues by integrating multiple querying mechanisms into a unified framework. Specifically, it uses modality queries to reconstruct missing data from incomplete signals, category queries to focus on emotional state features, and interference queries to separate relevant information from noise. Extensive experiment results demonstrate the superior emotion recognition performance of MMQ-Net compared to existing approaches, particularly under high levels of data incompleteness.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.HC"
    ],
    "primary_category": "cs.CV",
    "comment": "MICCAI2025",
    "pdf_url": "https://arxiv.org/pdf/2507.20737v1",
    "published_date": "2025-07-28 11:41:15 UTC",
    "updated_date": "2025-07-28 11:41:15 UTC"
  },
  {
    "arxiv_id": "2507.20728v1",
    "title": "Learning the Value Systems of Societies from Preferences",
    "authors": [
      "Andrés Holgado-Sánchez",
      "Holger Billhardt",
      "Sascha Ossowski",
      "Sara Degli-Esposti"
    ],
    "abstract": "Aligning AI systems with human values and the value-based preferences of various stakeholders (their value systems) is key in ethical AI. In value-aware AI systems, decision-making draws upon explicit computational representations of individual values (groundings) and their aggregation into value systems. As these are notoriously difficult to elicit and calibrate manually, value learning approaches aim to automatically derive computational models of an agent's values and value system from demonstrations of human behaviour. Nonetheless, social science and humanities literature suggest that it is more adequate to conceive the value system of a society as a set of value systems of different groups, rather than as the simple aggregation of individual value systems. Accordingly, here we formalize the problem of learning the value systems of societies and propose a method to address it based on heuristic deep clustering. The method learns socially shared value groundings and a set of diverse value systems representing a given society by observing qualitative value-based preferences from a sample of agents. We evaluate the proposal in a use case with real data about travelling decisions.",
    "categories": [
      "cs.AI",
      "cs.CY",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "Full version of publication under the same accepted at ECAI 2025 conference (Submission 6755). 8 pages + 2 supplementary material",
    "pdf_url": "https://arxiv.org/pdf/2507.20728v1",
    "published_date": "2025-07-28 11:25:55 UTC",
    "updated_date": "2025-07-28 11:25:55 UTC"
  },
  {
    "arxiv_id": "2507.20714v1",
    "title": "Prostate Cancer Classification Using Multimodal Feature Fusion and Explainable AI",
    "authors": [
      "Asma Sadia Khan",
      "Fariba Tasnia Khan",
      "Tanjim Mahmud",
      "Salman Karim Khan",
      "Rishita Chakma",
      "Nahed Sharmen",
      "Mohammad Shahadat Hossain",
      "Karl Andersson"
    ],
    "abstract": "Prostate cancer, the second most prevalent male malignancy, requires advanced diagnostic tools. We propose an explainable AI system combining BERT (for textual clinical notes) and Random Forest (for numerical lab data) through a novel multimodal fusion strategy, achieving superior classification performance on PLCO-NIH dataset (98% accuracy, 99% AUC). While multimodal fusion is established, our work demonstrates that a simple yet interpretable BERT+RF pipeline delivers clinically significant improvements - particularly for intermediate cancer stages (Class 2/3 recall: 0.900 combined vs 0.824 numerical/0.725 textual). SHAP analysis provides transparent feature importance rankings, while ablation studies prove textual features' complementary value. This accessible approach offers hospitals a balance of high performance (F1=89%), computational efficiency, and clinical interpretability - addressing critical needs in prostate cancer diagnostics.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "q-bio.QM",
      "stat.AP"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2507.20714v1",
    "published_date": "2025-07-28 11:07:17 UTC",
    "updated_date": "2025-07-28 11:07:17 UTC"
  },
  {
    "arxiv_id": "2507.20711v1",
    "title": "Algorithmic Fairness: A Runtime Perspective",
    "authors": [
      "Filip Cano",
      "Thomas A. Henzinger",
      "Konstantin Kueffner"
    ],
    "abstract": "Fairness in AI is traditionally studied as a static property evaluated once, over a fixed dataset. However, real-world AI systems operate sequentially, with outcomes and environments evolving over time. This paper proposes a framework for analysing fairness as a runtime property. Using a minimal yet expressive model based on sequences of coin tosses with possibly evolving biases, we study the problems of monitoring and enforcing fairness expressed in either toss outcomes or coin biases. Since there is no one-size-fits-all solution for either problem, we provide a summary of monitoring and enforcement strategies, parametrised by environment dynamics, prediction horizon, and confidence thresholds. For both problems, we present general results under simple or minimal assumptions. We survey existing solutions for the monitoring problem for Markovian and additive dynamics, and existing solutions for the enforcement problem in static settings with known dynamics.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "To appear in RV 2025",
    "pdf_url": "https://arxiv.org/pdf/2507.20711v1",
    "published_date": "2025-07-28 11:04:17 UTC",
    "updated_date": "2025-07-28 11:04:17 UTC"
  },
  {
    "arxiv_id": "2507.20704v2",
    "title": "Text2VLM: Adapting Text-Only Datasets to Evaluate Alignment Training in Visual Language Models",
    "authors": [
      "Gabriel Downer",
      "Sean Craven",
      "Damian Ruck",
      "Jake Thomas"
    ],
    "abstract": "The increasing integration of Visual Language Models (VLMs) into AI systems necessitates robust model alignment, especially when handling multimodal content that combines text and images. Existing evaluation datasets heavily lean towards text-only prompts, leaving visual vulnerabilities under evaluated. To address this gap, we propose \\textbf{Text2VLM}, a novel multi-stage pipeline that adapts text-only datasets into multimodal formats, specifically designed to evaluate the resilience of VLMs against typographic prompt injection attacks. The Text2VLM pipeline identifies harmful content in the original text and converts it into a typographic image, creating a multimodal prompt for VLMs. Also, our evaluation of open-source VLMs highlights their increased susceptibility to prompt injection when visual inputs are introduced, revealing critical weaknesses in the current models' alignment. This is in addition to a significant performance gap compared to closed-source frontier models. We validate Text2VLM through human evaluations, ensuring the alignment of extracted salient concepts; text summarization and output classification align with human expectations. Text2VLM provides a scalable tool for comprehensive safety assessment, contributing to the development of more robust safety mechanisms for VLMs. By enhancing the evaluation of multimodal vulnerabilities, Text2VLM plays a role in advancing the safe deployment of VLMs in diverse, real-world applications.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CR"
    ],
    "primary_category": "cs.CL",
    "comment": "9 pages, 9 figures. Jake Thomas served as Editor for this manuscript",
    "pdf_url": "https://arxiv.org/pdf/2507.20704v2",
    "published_date": "2025-07-28 10:57:44 UTC",
    "updated_date": "2026-01-05 09:07:09 UTC"
  },
  {
    "arxiv_id": "2507.20703v1",
    "title": "A General Framework for Dynamic MAPF using Multi-Shot ASP and Tunnels",
    "authors": [
      "Aysu Bogatarkan",
      "Esra Erdem"
    ],
    "abstract": "MAPF problem aims to find plans for multiple agents in an environment within a given time, such that the agents do not collide with each other or obstacles. Motivated by the execution and monitoring of these plans, we study Dynamic MAPF (D-MAPF) problem, which allows changes such as agents entering/leaving the environment or obstacles being removed/moved. Considering the requirements of real-world applications in warehouses with the presence of humans, we introduce 1) a general definition for D-MAPF (applicable to variations of D-MAPF), 2) a new framework to solve D-MAPF (utilizing multi-shot computation, and allowing different methods to solve D-MAPF), and 3) a new ASP-based method to solve D-MAPF (combining advantages of replanning and repairing methods, with a novel concept of tunnels to specify where agents can move). We have illustrated the strengths and weaknesses of this method by experimental evaluations, from the perspectives of computational performance and quality of solutions.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2507.20703v1",
    "published_date": "2025-07-28 10:55:31 UTC",
    "updated_date": "2025-07-28 10:55:31 UTC"
  },
  {
    "arxiv_id": "2508.00898v1",
    "title": "Benefits of Feature Extraction and Temporal Sequence Analysis for Video Frame Prediction: An Evaluation of Hybrid Deep Learning Models",
    "authors": [
      "Jose M. Sánchez Velázquez",
      "Mingbo Cai",
      "Andrew Coney",
      "Álvaro J. García- Tejedor",
      "Alberto Nogales"
    ],
    "abstract": "In recent years, advances in Artificial Intelligence have significantly impacted computer science, particularly in the field of computer vision, enabling solutions to complex problems such as video frame prediction. Video frame prediction has critical applications in weather forecasting or autonomous systems and can provide technical improvements, such as video compression and streaming. Among Artificial Intelligence methods, Deep Learning has emerged as highly effective for solving vision-related tasks, although current frame prediction models still have room for enhancement. This paper evaluates several hybrid deep learning approaches that combine the feature extraction capabilities of autoencoders with temporal sequence modelling using Recurrent Neural Networks (RNNs), 3D Convolutional Neural Networks (3D CNNs), and related architectures. The proposed solutions were rigorously evaluated on three datasets that differ in terms of synthetic versus real-world scenarios and grayscale versus color imagery. Results demonstrate that the approaches perform well, with SSIM metrics increasing from 0.69 to 0.82, indicating that hybrid models utilizing 3DCNNs and ConvLSTMs are the most effective, and greyscale videos with real data are the easiest to predict.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "2 Figures, 12 Tables, 21 pages",
    "pdf_url": "https://arxiv.org/pdf/2508.00898v1",
    "published_date": "2025-07-28 10:07:00 UTC",
    "updated_date": "2025-07-28 10:07:00 UTC"
  },
  {
    "arxiv_id": "2507.20670v1",
    "title": "A Multimodal Architecture for Endpoint Position Prediction in Team-based Multiplayer Games",
    "authors": [
      "Jonas Peche",
      "Aliaksei Tsishurou",
      "Alexander Zap",
      "Guenter Wallner"
    ],
    "abstract": "Understanding and predicting player movement in multiplayer games is crucial for achieving use cases such as player-mimicking bot navigation, preemptive bot control, strategy recommendation, and real-time player behavior analytics. However, the complex environments allow for a high degree of navigational freedom, and the interactions and team-play between players require models that make effective use of the available heterogeneous input data. This paper presents a multimodal architecture for predicting future player locations on a dynamic time horizon, using a U-Net-based approach for calculating endpoint location probability heatmaps, conditioned using a multimodal feature encoder. The application of a multi-head attention mechanism for different groups of features allows for communication between agents. In doing so, the architecture makes efficient use of the multimodal game state including image inputs, numerical and categorical features, as well as dynamic game data. Consequently, the presented technique lays the foundation for various downstream tasks that rely on future player positions such as the creation of player-predictive bot behavior or player anomaly detection.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2507.20670v1",
    "published_date": "2025-07-28 09:51:49 UTC",
    "updated_date": "2025-07-28 09:51:49 UTC"
  },
  {
    "arxiv_id": "2507.20666v1",
    "title": "MIMII-Agent: Leveraging LLMs with Function Calling for Relative Evaluation of Anomalous Sound Detection",
    "authors": [
      "Harsh Purohit",
      "Tomoya Nishida",
      "Kota Dohi",
      "Takashi Endo",
      "Yohei Kawaguchi"
    ],
    "abstract": "This paper proposes a method for generating machine-type-specific anomalies to evaluate the relative performance of unsupervised anomalous sound detection (UASD) systems across different machine types, even in the absence of real anomaly sound data. Conventional keyword-based data augmentation methods often produce unrealistic sounds due to their reliance on manually defined labels, limiting scalability as machine types and anomaly patterns diversify. Advanced audio generative models, such as MIMII-Gen, show promise but typically depend on anomalous training data, making them less effective when diverse anomalous examples are unavailable. To address these limitations, we propose a novel synthesis approach leveraging large language models (LLMs) to interpret textual descriptions of faults and automatically select audio transformation functions, converting normal machine sounds into diverse and plausible anomalous sounds. We validate this approach by evaluating a UASD system trained only on normal sounds from five machine types, using both real and synthetic anomaly data. Experimental results reveal consistent trends in relative detection difficulty across machine types between synthetic and real anomalies. This finding supports our hypothesis and highlights the effectiveness of the proposed LLM-based synthesis approach for relative evaluation of UASD systems.",
    "categories": [
      "eess.AS",
      "cs.AI",
      "cs.LG",
      "cs.SD"
    ],
    "primary_category": "eess.AS",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2507.20666v1",
    "published_date": "2025-07-28 09:42:41 UTC",
    "updated_date": "2025-07-28 09:42:41 UTC"
  },
  {
    "arxiv_id": "2507.21199v1",
    "title": "Advancing Compositional LLM Reasoning with Structured Task Relations in Interactive Multimodal Communications",
    "authors": [
      "Xinye Cao",
      "Hongcan Guo",
      "Guoshun Nan",
      "Jiaoyang Cui",
      "Haoting Qian",
      "Yihan Lin",
      "Yilin Peng",
      "Diyang Zhang",
      "Yanzhao Hou",
      "Huici Wu",
      "Xiaofeng Tao",
      "Tony Q. S. Quek"
    ],
    "abstract": "Interactive multimodal applications (IMAs), such as route planning in the Internet of Vehicles, enrich users' personalized experiences by integrating various forms of data over wireless networks. Recent advances in large language models (LLMs) utilize mixture-of-experts (MoE) mechanisms to empower multiple IMAs, with each LLM trained individually for a specific task that presents different business workflows. In contrast to existing approaches that rely on multiple LLMs for IMAs, this paper presents a novel paradigm that accomplishes various IMAs using a single compositional LLM over wireless networks. The two primary challenges include 1) guiding a single LLM to adapt to diverse IMA objectives and 2) ensuring the flexibility and efficiency of the LLM in resource-constrained mobile environments. To tackle the first challenge, we propose ContextLoRA, a novel method that guides an LLM to learn the rich structured context among IMAs by constructing a task dependency graph. We partition the learnable parameter matrix of neural layers for each IMA to facilitate LLM composition. Then, we develop a step-by-step fine-tuning procedure guided by task relations, including training, freezing, and masking phases. This allows the LLM to learn to reason among tasks for better adaptation, capturing the latent dependencies between tasks. For the second challenge, we introduce ContextGear, a scheduling strategy to optimize the training procedure of ContextLoRA, aiming to minimize computational and communication costs through a strategic grouping mechanism. Experiments on three benchmarks show the superiority of the proposed ContextLoRA and ContextGear. Furthermore, we prototype our proposed paradigm on a real-world wireless testbed, demonstrating its practical applicability for various IMAs. We will release our code to the community.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.DC",
      "cs.HC"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted by IEEE JSAC. This work has been submitted to the IEEE for possible publication",
    "pdf_url": "https://arxiv.org/pdf/2507.21199v1",
    "published_date": "2025-07-28 09:33:12 UTC",
    "updated_date": "2025-07-28 09:33:12 UTC"
  },
  {
    "arxiv_id": "2507.20650v1",
    "title": "Hot-Swap MarkBoard: An Efficient Black-box Watermarking Approach for Large-scale Model Distribution",
    "authors": [
      "Zhicheng Zhang",
      "Peizhuo Lv",
      "Mengke Wan",
      "Jiang Fang",
      "Diandian Guo",
      "Yezeng Chen",
      "Yinlong Liu",
      "Wei Ma",
      "Jiyan Sun",
      "Liru Geng"
    ],
    "abstract": "Recently, Deep Learning (DL) models have been increasingly deployed on end-user devices as On-Device AI, offering improved efficiency and privacy. However, this deployment trend poses more serious Intellectual Property (IP) risks, as models are distributed on numerous local devices, making them vulnerable to theft and redistribution. Most existing ownership protection solutions (e.g., backdoor-based watermarking) are designed for cloud-based AI-as-a-Service (AIaaS) and are not directly applicable to large-scale distribution scenarios, where each user-specific model instance must carry a unique watermark. These methods typically embed a fixed watermark, and modifying the embedded watermark requires retraining the model. To address these challenges, we propose Hot-Swap MarkBoard, an efficient watermarking method. It encodes user-specific $n$-bit binary signatures by independently embedding multiple watermarks into a multi-branch Low-Rank Adaptation (LoRA) module, enabling efficient watermark customization without retraining through branch swapping. A parameter obfuscation mechanism further entangles the watermark weights with those of the base model, preventing removal without degrading model performance. The method supports black-box verification and is compatible with various model architectures and DL tasks, including classification, image generation, and text generation. Extensive experiments across three types of tasks and six backbone models demonstrate our method's superior efficiency and adaptability compared to existing approaches, achieving 100\\% verification accuracy.",
    "categories": [
      "cs.CR",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.CR",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2507.20650v1",
    "published_date": "2025-07-28 09:14:21 UTC",
    "updated_date": "2025-07-28 09:14:21 UTC"
  },
  {
    "arxiv_id": "2507.20643v2",
    "title": "Ontology-Enhanced Knowledge Graph Completion using Large Language Models",
    "authors": [
      "Wenbin Guo",
      "Xin Wang",
      "Jiaoyan Chen",
      "Zhao Li",
      "Zirui Chen"
    ],
    "abstract": "Large Language Models (LLMs) have been extensively adopted in Knowledge Graph Completion (KGC), showcasing significant research advancements. However, as black-box models driven by deep neural architectures, current LLM-based KGC methods rely on implicit knowledge representation with parallel propagation of erroneous knowledge, thereby hindering their ability to produce conclusive and decisive reasoning outcomes. We aim to integrate neural-perceptual structural information with ontological knowledge, leveraging the powerful capabilities of LLMs to achieve a deeper understanding of the intrinsic logic of the knowledge. We propose an ontology enhanced KGC method using LLMs -- OL-KGC. It first leverages neural perceptual mechanisms to effectively embed structural information into the textual space, and then uses an automated extraction algorithm to retrieve ontological knowledge from the knowledge graphs (KGs) that needs to be completed, which is further transformed into a textual format comprehensible to LLMs for providing logic guidance. We conducted extensive experiments on three widely-used benchmarks -- FB15K-237, UMLS and WN18RR. The experimental results demonstrate that OL-KGC significantly outperforms existing mainstream KGC methods across multiple evaluation metrics, achieving state-of-the-art performance.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2507.20643v2",
    "published_date": "2025-07-28 09:00:48 UTC",
    "updated_date": "2025-10-21 03:05:04 UTC"
  },
  {
    "arxiv_id": "2508.03719v1",
    "title": "Intent Aware Context Retrieval for Multi-Turn Agricultural Question Answering",
    "authors": [
      "Abhay Vijayvargia",
      "Ajay Nagpal",
      "Kundeshwar Pundalik",
      "Atharva Savarkar",
      "Smita Gautam",
      "Pankaj Singh",
      "Rohit Saluja",
      "Ganesh Ramakrishnan"
    ],
    "abstract": "Indian farmers often lack timely, accessible, and language-friendly agricultural advice, especially in rural areas with low literacy. To address this gap in accessibility, this paper presents a novel AI-powered agricultural chatbot, Krishi Sathi, designed to support Indian farmers by providing personalized, easy-to-understand answers to their queries through both text and speech. The system's intelligence stems from an IFT model, subsequently refined through fine-tuning on Indian agricultural knowledge across three curated datasets. Unlike traditional chatbots that respond to one-off questions, Krishi Sathi follows a structured, multi-turn conversation flow to gradually collect the necessary details from the farmer, ensuring the query is fully understood before generating a response. Once the intent and context are extracted, the system performs Retrieval-Augmented Generation (RAG) by first fetching information from a curated agricultural database and then generating a tailored response using the IFT model. The chatbot supports both English and Hindi languages, with speech input and output features (via ASR and TTS) to make it accessible for users with low literacy or limited digital skills. This work demonstrates how combining intent-driven dialogue flows, instruction-tuned models, and retrieval-based generation can improve the quality and accessibility of digital agricultural support in India.\n  This approach yielded strong results, with the system achieving a query response accuracy of 97.53%, 91.35% contextual relevance and personalization, and a query completion rate of 97.53%. The average response time remained under 6 seconds, ensuring timely support for users across both English and Hindi interactions.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2508.03719v1",
    "published_date": "2025-07-28 09:00:44 UTC",
    "updated_date": "2025-07-28 09:00:44 UTC"
  },
  {
    "arxiv_id": "2507.20641v1",
    "title": "Adaptive Fuzzy Time Series Forecasting via Partially Asymmetric Convolution and Sub-Sliding Window Fusion",
    "authors": [
      "Lijian Li"
    ],
    "abstract": "At present, state-of-the-art forecasting models are short of the ability to capture spatio-temporal dependency and synthesize global information at the stage of learning. To address this issue, in this paper, through the adaptive fuzzified construction of temporal data, we propose a novel convolutional architecture with partially asymmetric design based on the scheme of sliding window to realize accurate time series forecasting. First, the construction strategy of traditional fuzzy time series is improved to further extract short and long term temporal interrelation, which enables every time node to automatically possess corresponding global information and inner relationships among them in a restricted sliding window and the process does not require human involvement. Second, a bilateral Atrous algorithm is devised to reduce calculation demand of the proposed model without sacrificing global characteristics of elements. And it also allows the model to avoid processing redundant information. Third, after the transformation of time series, a partially asymmetric convolutional architecture is designed to more flexibly mine data features by filters in different directions on feature maps, which gives the convolutional neural network (CNN) the ability to construct sub-windows within existing sliding windows to model at a more fine-grained level. And after obtaining the time series information at different levels, the multi-scale features from different sub-windows will be sent to the corresponding network layer for time series information fusion. Compared with other competitive modern models, the proposed method achieves state-of-the-art results on most of popular time series datasets, which is fully verified by the experimental results.",
    "categories": [
      "cs.AI",
      "cs.IT"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2507.20641v1",
    "published_date": "2025-07-28 08:58:25 UTC",
    "updated_date": "2025-07-28 08:58:25 UTC"
  },
  {
    "arxiv_id": "2507.20630v2",
    "title": "TransPrune: Token Transition Pruning for Efficient Large Vision-Language Model",
    "authors": [
      "Ao Li",
      "Yuxiang Duan",
      "Jinghui Zhang",
      "Congbo Ma",
      "Yutong Xie",
      "Gustavo Carneiro",
      "Mohammad Yaqub",
      "Hu Wang"
    ],
    "abstract": "Large Vision-Language Models (LVLMs) have advanced multimodal learning but face high computational costs due to the large number of visual tokens, motivating token pruning to improve inference efficiency. The key challenge lies in identifying which tokens are truly important. Most existing approaches rely on attention-based criteria to estimate token importance. However, they inherently suffer from certain limitations, such as positional bias. In this work, we explore a new perspective on token importance based on token transitions in LVLMs. We observe that the transition of token representations provides a meaningful signal of semantic information. Based on this insight, we propose TransPrune, a training-free and efficient token pruning method. Specifically, TransPrune progressively prunes tokens by assessing their importance through a combination of Token Transition Variation (TTV)-which measures changes in both the magnitude and direction of token representations-and Instruction-Guided Attention (IGA), which measures how strongly the instruction attends to image tokens via attention. Extensive experiments demonstrate that TransPrune achieves comparable multimodal performance to original LVLMs, such as LLaVA-v1.5 and LLaVA-Next, across eight benchmarks, while reducing inference TFLOPs by more than half. Moreover, TTV alone can serve as an effective criterion without relying on attention, achieving performance comparable to attention-based methods. The code will be made publicly available upon acceptance of the paper at https://github.com/liaolea/TransPrune.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2507.20630v2",
    "published_date": "2025-07-28 08:44:58 UTC",
    "updated_date": "2025-11-17 11:15:25 UTC"
  },
  {
    "arxiv_id": "2507.20627v1",
    "title": "Controllable Video-to-Music Generation with Multiple Time-Varying Conditions",
    "authors": [
      "Junxian Wu",
      "Weitao You",
      "Heda Zuo",
      "Dengming Zhang",
      "Pei Chen",
      "Lingyun Sun"
    ],
    "abstract": "Music enhances video narratives and emotions, driving demand for automatic video-to-music (V2M) generation. However, existing V2M methods relying solely on visual features or supplementary textual inputs generate music in a black-box manner, often failing to meet user expectations. To address this challenge, we propose a novel multi-condition guided V2M generation framework that incorporates multiple time-varying conditions for enhanced control over music generation. Our method uses a two-stage training strategy that enables learning of V2M fundamentals and audiovisual temporal synchronization while meeting users' needs for multi-condition control. In the first stage, we introduce a fine-grained feature selection module and a progressive temporal alignment attention mechanism to ensure flexible feature alignment. For the second stage, we develop a dynamic conditional fusion module and a control-guided decoder module to integrate multiple conditions and accurately guide the music composition process. Extensive experiments demonstrate that our method outperforms existing V2M pipelines in both subjective and objective evaluations, significantly enhancing control and alignment with user expectations.",
    "categories": [
      "cs.MM",
      "cs.AI",
      "cs.SD",
      "eess.AS"
    ],
    "primary_category": "cs.MM",
    "comment": "Accepted by the 33rd ACM International Conference on Multimedia (ACMMM 2025). The project page is available at https://kita-wjx.github.io/MCV2M/",
    "pdf_url": "https://arxiv.org/pdf/2507.20627v1",
    "published_date": "2025-07-28 08:41:20 UTC",
    "updated_date": "2025-07-28 08:41:20 UTC"
  },
  {
    "arxiv_id": "2507.20623v1",
    "title": "Lightweight Remote Sensing Scene Classification on Edge Devices via Knowledge Distillation and Early-exit",
    "authors": [
      "Yang Zhao",
      "Shusheng Li",
      "Xueshang Feng"
    ],
    "abstract": "As the development of lightweight deep learning algorithms, various deep neural network (DNN) models have been proposed for the remote sensing scene classification (RSSC) application. However, it is still challenging for these RSSC models to achieve optimal performance among model accuracy, inference latency, and energy consumption on resource-constrained edge devices. In this paper, we propose a lightweight RSSC framework, which includes a distilled global filter network (GFNet) model and an early-exit mechanism designed for edge devices to achieve state-of-the-art performance. Specifically, we first apply frequency domain distillation on the GFNet model to reduce model size. Then we design a dynamic early-exit model tailored for DNN models on edge devices to further improve model inference efficiency. We evaluate our E3C model on three edge devices across four datasets. Extensive experimental results show that it achieves an average of 1.3x speedup on model inference and over 40% improvement on energy efficiency, while maintaining high classification accuracy.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "9 pages, 5 figures, to be published in ACM Multimedia 2025",
    "pdf_url": "https://arxiv.org/pdf/2507.20623v1",
    "published_date": "2025-07-28 08:36:36 UTC",
    "updated_date": "2025-07-28 08:36:36 UTC"
  },
  {
    "arxiv_id": "2507.20620v1",
    "title": "Complementarity-driven Representation Learning for Multi-modal Knowledge Graph Completion",
    "authors": [
      "Lijian Li"
    ],
    "abstract": "Multi-modal Knowledge Graph Completion (MMKGC) aims to uncover hidden world knowledge in multimodal knowledge graphs by leveraging both multimodal and structural entity information. However, the inherent imbalance in multimodal knowledge graphs, where modality distributions vary across entities, poses challenges in utilizing additional modality data for robust entity representation. Existing MMKGC methods typically rely on attention or gate-based fusion mechanisms but overlook complementarity contained in multi-modal data. In this paper, we propose a novel framework named Mixture of Complementary Modality Experts (MoCME), which consists of a Complementarity-guided Modality Knowledge Fusion (CMKF) module and an Entropy-guided Negative Sampling (EGNS) mechanism. The CMKF module exploits both intra-modal and inter-modal complementarity to fuse multi-view and multi-modal embeddings, enhancing representations of entities. Additionally, we introduce an Entropy-guided Negative Sampling mechanism to dynamically prioritize informative and uncertain negative samples to enhance training effectiveness and model robustness. Extensive experiments on five benchmark datasets demonstrate that our MoCME achieves state-of-the-art performance, surpassing existing approaches.",
    "categories": [
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2507.20620v1",
    "published_date": "2025-07-28 08:35:11 UTC",
    "updated_date": "2025-07-28 08:35:11 UTC"
  },
  {
    "arxiv_id": "2508.05653v1",
    "title": "Modeling Interactive Narrative Systems: A Formal Approach",
    "authors": [
      "Jules Clerc",
      "Domitile Lourdeaux",
      "Mohamed Sallak",
      "Johann Barbier",
      "Marc Ravaine"
    ],
    "abstract": "Interactive Narrative Systems (INS) have revolutionized digital experiences by empowering users to actively shape their stories, diverging from traditional passive storytelling. However, the field faces challenges due to fragmented research efforts and diverse system representations. This paper introduces a formal representation framework for INS, inspired by diverse approaches from the state of the art. By providing a consistent vocabulary and modeling structure, the framework facilitates the analysis, the description and comparison of INS properties. Experimental validations on the \"Little Red Riding Hood\" scenario highlight the usefulness of the proposed formalism and its impact on improving the evaluation of INS. This work aims to foster collaboration and coherence within the INS research community by proposing a methodology for formally representing these systems.",
    "categories": [
      "cs.HC",
      "cs.AI"
    ],
    "primary_category": "cs.HC",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2508.05653v1",
    "published_date": "2025-07-28 08:28:37 UTC",
    "updated_date": "2025-07-28 08:28:37 UTC"
  },
  {
    "arxiv_id": "2507.20613v1",
    "title": "Enhancing Large Multimodal Models with Adaptive Sparsity and KV Cache Compression",
    "authors": [
      "Te Zhang",
      "Yuheng Li",
      "Junxiang Wang",
      "Lujun Li"
    ],
    "abstract": "Large multimodal models (LMMs) have advanced significantly by integrating visual encoders with extensive language models, enabling robust reasoning capabilities. However, compressing LMMs for deployment on edge devices remains a critical challenge. In this work, we propose an adaptive search algorithm that optimizes sparsity and KV cache compression to enhance LMM efficiency. Utilizing the Tree-structured Parzen Estimator, our method dynamically adjusts pruning ratios and KV cache quantization bandwidth across different LMM layers, using model performance as the optimization objective. This approach uniquely combines pruning with key-value cache quantization and incorporates a fast pruning technique that eliminates the need for additional fine-tuning or weight adjustments, achieving efficient compression without compromising accuracy. Comprehensive evaluations on benchmark datasets, including LLaVA-1.5 7B and 13B, demonstrate our method superiority over state-of-the-art techniques such as SparseGPT and Wanda across various compression levels. Notably, our framework automatic allocation of KV cache compression resources sets a new standard in LMM optimization, delivering memory efficiency without sacrificing much performance.",
    "categories": [
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "6 pages",
    "pdf_url": "https://arxiv.org/pdf/2507.20613v1",
    "published_date": "2025-07-28 08:27:40 UTC",
    "updated_date": "2025-07-28 08:27:40 UTC"
  },
  {
    "arxiv_id": "2508.00897v1",
    "title": "Maximize margins for robust splicing detection",
    "authors": [
      "Julien Simon de Kergunic",
      "Rony Abecidan",
      "Patrick Bas",
      "Vincent Itier"
    ],
    "abstract": "Despite recent progress in splicing detection, deep learning-based forensic tools remain difficult to deploy in practice due to their high sensitivity to training conditions. Even mild post-processing applied to evaluation images can significantly degrade detector performance, raising concerns about their reliability in operational contexts. In this work, we show that the same deep architecture can react very differently to unseen post-processing depending on the learned weights, despite achieving similar accuracy on in-distribution test data. This variability stems from differences in the latent spaces induced by training, which affect how samples are separated internally. Our experiments reveal a strong correlation between the distribution of latent margins and a detector's ability to generalize to post-processed images. Based on this observation, we propose a practical strategy for building more robust detectors: train several variants of the same model under different conditions, and select the one that maximizes latent margins.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CR"
    ],
    "primary_category": "cs.LG",
    "comment": "in French language. GRETSI 2025 - Colloque Francophone de Traitement du Signal et des Images, https://gretsi.fr/colloque2025/, Aug 2025, Strasbourg, France",
    "pdf_url": "https://arxiv.org/pdf/2508.00897v1",
    "published_date": "2025-07-28 08:20:46 UTC",
    "updated_date": "2025-07-28 08:20:46 UTC"
  },
  {
    "arxiv_id": "2507.20578v1",
    "title": "Beyond Interactions: Node-Level Graph Generation for Knowledge-Free Augmentation in Recommender Systems",
    "authors": [
      "Zhaoyan Wang",
      "Hyunjun Ahn",
      "In-Young Ko"
    ],
    "abstract": "Recent advances in recommender systems rely on external resources such as knowledge graphs or large language models to enhance recommendations, which limit applicability in real-world settings due to data dependency and computational overhead. Although knowledge-free models are able to bolster recommendations by direct edge operations as well, the absence of augmentation primitives drives them to fall short in bridging semantic and structural gaps as high-quality paradigm substitutes. Unlike existing diffusion-based works that remodel user-item interactions, this work proposes NodeDiffRec, a pioneering knowledge-free augmentation framework that enables fine-grained node-level graph generation for recommendations and expands the scope of restricted augmentation primitives via diffusion. By synthesizing pseudo-items and corresponding interactions that align with the underlying distribution for injection, and further refining user preferences through a denoising preference modeling process, NodeDiffRec dramatically enhances both semantic diversity and structural connectivity without external knowledge. Extensive experiments across diverse datasets and recommendation algorithms demonstrate the superiority of NodeDiffRec, achieving State-of-the-Art (SOTA) performance, with maximum average performance improvement 98.6% in Recall@5 and 84.0% in NDCG@5 over selected baselines.",
    "categories": [
      "cs.IR",
      "cs.AI"
    ],
    "primary_category": "cs.IR",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2507.20578v1",
    "published_date": "2025-07-28 07:22:06 UTC",
    "updated_date": "2025-07-28 07:22:06 UTC"
  },
  {
    "arxiv_id": "2507.20575v1",
    "title": "Implicit Spatiotemporal Bandwidth Enhancement Filter by Sine-activated Deep Learning Model for Fast 3D Photoacoustic Tomography",
    "authors": [
      "I Gede Eka Sulistyawan",
      "Takuro Ishii",
      "Riku Suzuki",
      "Yoshifumi Saijo"
    ],
    "abstract": "3D photoacoustic tomography (3D-PAT) using high-frequency hemispherical transducers offers near-omnidirectional reception and enhanced sensitivity to the finer structural details encoded in the high-frequency components of the broadband photoacoustic (PA) signal. However, practical constraints such as limited number of channels with bandlimited sampling rate often result in sparse and bandlimited sensors that degrade image quality. To address this, we revisit the 2D deep learning (DL) approach applied directly to sensor-wise PA radio-frequency (PARF) data. Specifically, we introduce sine activation into the DL model to restore the broadband nature of PARF signals given the observed band-limited and high-frequency PARF data. Given the scarcity of 3D training data, we employ simplified training strategies by simulating random spherical absorbers. This combination of sine-activated model and randomized training is designed to emphasize bandwidth learning over dataset memorization. Our model was evaluated on a leaf skeleton phantom, a micro-CT-verified 3D spiral phantom and in-vivo human palm vasculature. The results showed that the proposed training mechanism on sine-activated model was well-generalized across the different tests by effectively increasing the sensor density and recovering the spatiotemporal bandwidth. Qualitatively, the sine-activated model uniquely enhanced high-frequency content that produces clearer vascular structure with fewer artefacts. Quantitatively, the sine-activated model exhibits full bandwidth at -12 dB spectrum and significantly higher contrast-to-noise ratio with minimal loss of structural similarity index. Lastly, we optimized our approach to enable fast enhanced 3D-PAT at 2 volumes-per-second for better practical imaging of a free-moving targets.",
    "categories": [
      "eess.IV",
      "cs.AI"
    ],
    "primary_category": "eess.IV",
    "comment": "14 pages, 13 figures. This work has been submitted to the IEEE for possible publication",
    "pdf_url": "https://arxiv.org/pdf/2507.20575v1",
    "published_date": "2025-07-28 07:16:32 UTC",
    "updated_date": "2025-07-28 07:16:32 UTC"
  },
  {
    "arxiv_id": "2507.20571v1",
    "title": "DAG-AFL:Directed Acyclic Graph-based Asynchronous Federated Learning",
    "authors": [
      "Shuaipeng Zhang",
      "Lanju Kong",
      "Yixin Zhang",
      "Wei He",
      "Yongqing Zheng",
      "Han Yu",
      "Lizhen Cui"
    ],
    "abstract": "Due to the distributed nature of federated learning (FL), the vulnerability of the global model and the need for coordination among many client devices pose significant challenges. As a promising decentralized, scalable and secure solution, blockchain-based FL methods have attracted widespread attention in recent years. However, traditional consensus mechanisms designed for Proof of Work (PoW) similar to blockchain incur substantial resource consumption and compromise the efficiency of FL, particularly when participating devices are wireless and resource-limited. To address asynchronous client participation and data heterogeneity in FL, while limiting the additional resource overhead introduced by blockchain, we propose the Directed Acyclic Graph-based Asynchronous Federated Learning (DAG-AFL) framework. We develop a tip selection algorithm that considers temporal freshness, node reachability and model accuracy, with a DAG-based trusted verification strategy. Extensive experiments on 3 benchmarking datasets against eight state-of-the-art approaches demonstrate that DAG-AFL significantly improves training efficiency and model accuracy by 22.7% and 6.5% on average, respectively.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "6 pages, IEEE International Conference on Multimedia & Expo 2025 conference paper",
    "pdf_url": "https://arxiv.org/pdf/2507.20571v1",
    "published_date": "2025-07-28 07:06:56 UTC",
    "updated_date": "2025-07-28 07:06:56 UTC"
  },
  {
    "arxiv_id": "2507.20568v2",
    "title": "Learning Phonetic Context-Dependent Viseme for Enhancing Speech-Driven 3D Facial Animation",
    "authors": [
      "Hyung Kyu Kim",
      "Hak Gu Kim"
    ],
    "abstract": "Speech-driven 3D facial animation aims to generate realistic facial movements synchronized with audio. Traditional methods primarily minimize reconstruction loss by aligning each frame with ground-truth. However, this frame-wise approach often fails to capture the continuity of facial motion, leading to jittery and unnatural outputs due to coarticulation. To address this, we propose a novel phonetic context-aware loss, which explicitly models the influence of phonetic context on viseme transitions. By incorporating a viseme coarticulation weight, we assign adaptive importance to facial movements based on their dynamic changes over time, ensuring smoother and perceptually consistent animations. Extensive experiments demonstrate that replacing the conventional reconstruction loss with ours improves both quantitative metrics and visual quality. It highlights the importance of explicitly modeling phonetic context-dependent visemes in synthesizing natural speech-driven 3D facial animation. Project page: https://cau-irislab.github.io/interspeech25/",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "Interspeech 2025; Project Page: https://cau-irislab.github.io/interspeech25/",
    "pdf_url": "https://arxiv.org/pdf/2507.20568v2",
    "published_date": "2025-07-28 07:04:50 UTC",
    "updated_date": "2025-08-11 10:06:46 UTC"
  },
  {
    "arxiv_id": "2507.20566v1",
    "title": "Unlearning of Knowledge Graph Embedding via Preference Optimization",
    "authors": [
      "Jiajun Liu",
      "Wenjun Ke",
      "Peng Wang",
      "Yao He",
      "Ziyu Shang",
      "Guozheng Li",
      "Zijie Xu",
      "Ke Ji"
    ],
    "abstract": "Existing knowledge graphs (KGs) inevitably contain outdated or erroneous knowledge that needs to be removed from knowledge graph embedding (KGE) models. To address this challenge, knowledge unlearning can be applied to eliminate specific information while preserving the integrity of the remaining knowledge in KGs. Existing unlearning methods can generally be categorized into exact unlearning and approximate unlearning. However, exact unlearning requires high training costs while approximate unlearning faces two issues when applied to KGs due to the inherent connectivity of triples: (1) It fails to fully remove targeted information, as forgetting triples can still be inferred from remaining ones. (2) It focuses on local data for specific removal, which weakens the remaining knowledge in the forgetting boundary. To address these issues, we propose GraphDPO, a novel approximate unlearning framework based on direct preference optimization (DPO). Firstly, to effectively remove forgetting triples, we reframe unlearning as a preference optimization problem, where the model is trained by DPO to prefer reconstructed alternatives over the original forgetting triples. This formulation penalizes reliance on forgettable knowledge, mitigating incomplete forgetting caused by KG connectivity. Moreover, we introduce an out-boundary sampling strategy to construct preference pairs with minimal semantic overlap, weakening the connection between forgetting and retained knowledge. Secondly, to preserve boundary knowledge, we introduce a boundary recall mechanism that replays and distills relevant information both within and across time steps. We construct eight unlearning datasets across four popular KGs with varying unlearning rates. Experiments show that GraphDPO outperforms state-of-the-art baselines by up to 10.1% in MRR_Avg and 14.0% in MRR_F1.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2507.20566v1",
    "published_date": "2025-07-28 07:03:04 UTC",
    "updated_date": "2025-07-28 07:03:04 UTC"
  },
  {
    "arxiv_id": "2507.20562v2",
    "title": "MemoryTalker: Personalized Speech-Driven 3D Facial Animation via Audio-Guided Stylization",
    "authors": [
      "Hyung Kyu Kim",
      "Sangmin Lee",
      "Hak Gu Kim"
    ],
    "abstract": "Speech-driven 3D facial animation aims to synthesize realistic facial motion sequences from given audio, matching the speaker's speaking style. However, previous works often require priors such as class labels of a speaker or additional 3D facial meshes at inference, which makes them fail to reflect the speaking style and limits their practical use. To address these issues, we propose MemoryTalker which enables realistic and accurate 3D facial motion synthesis by reflecting speaking style only with audio input to maximize usability in applications. Our framework consists of two training stages: 1-stage is storing and retrieving general motion (i.e., Memorizing), and 2-stage is to perform the personalized facial motion synthesis (i.e., Animating) with the motion memory stylized by the audio-driven speaking style feature. In this second stage, our model learns about which facial motion types should be emphasized for a particular piece of audio. As a result, our MemoryTalker can generate a reliable personalized facial animation without additional prior information. With quantitative and qualitative evaluations, as well as user study, we show the effectiveness of our model and its performance enhancement for personalized facial animation over state-of-the-art methods.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "Accepted in ICCV 2025; Project Page: https://cau-irislab.github.io/ICCV25-MemoryTalker/",
    "pdf_url": "https://arxiv.org/pdf/2507.20562v2",
    "published_date": "2025-07-28 06:47:59 UTC",
    "updated_date": "2025-08-25 07:44:57 UTC"
  },
  {
    "arxiv_id": "2507.20546v1",
    "title": "Enhancing Hallucination Detection via Future Context",
    "authors": [
      "Joosung Lee",
      "Cheonbok Park",
      "Hwiyeol Jo",
      "Jeonghoon Kim",
      "Joonsuk Park",
      "Kang Min Yoo"
    ],
    "abstract": "Large Language Models (LLMs) are widely used to generate plausible text on online platforms, without revealing the generation process. As users increasingly encounter such black-box outputs, detecting hallucinations has become a critical challenge. To address this challenge, we focus on developing a hallucination detection framework for black-box generators. Motivated by the observation that hallucinations, once introduced, tend to persist, we sample future contexts. The sampled future contexts provide valuable clues for hallucination detection and can be effectively integrated with various sampling-based methods. We extensively demonstrate performance improvements across multiple methods using our proposed sampling approach.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2507.20546v1",
    "published_date": "2025-07-28 06:13:23 UTC",
    "updated_date": "2025-07-28 06:13:23 UTC"
  },
  {
    "arxiv_id": "2507.21198v1",
    "title": "Uncovering Gradient Inversion Risks in Practical Language Model Training",
    "authors": [
      "Xinguo Feng",
      "Zhongkui Ma",
      "Zihan Wang",
      "Eu Joe Chegne",
      "Mengyao Ma",
      "Alsharif Abuadbba",
      "Guangdong Bai"
    ],
    "abstract": "The gradient inversion attack has been demonstrated as a significant privacy threat to federated learning (FL), particularly in continuous domains such as vision models. In contrast, it is often considered less effective or highly dependent on impractical training settings when applied to language models, due to the challenges posed by the discrete nature of tokens in text data. As a result, its potential privacy threats remain largely underestimated, despite FL being an emerging training method for language models. In this work, we propose a domain-specific gradient inversion attack named Grab (gradient inversion with hybrid optimization). Grab features two alternating optimization processes to address the challenges caused by practical training settings, including a simultaneous optimization on dropout masks between layers for improved token recovery and a discrete optimization for effective token sequencing. Grab can recover a significant portion (up to 92.9% recovery rate) of the private training data, outperforming the attack strategy of utilizing discrete optimization with an auxiliary model by notable improvements of up to 28.9% recovery rate in benchmark settings and 48.5% recovery rate in practical settings. Grab provides a valuable step forward in understanding this privacy threat in the emerging FL training mode of language models.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "15 Pages, 5 figures, 10 tables. Accepted by ACM CCS 2024",
    "pdf_url": "https://arxiv.org/pdf/2507.21198v1",
    "published_date": "2025-07-28 06:06:29 UTC",
    "updated_date": "2025-07-28 06:06:29 UTC"
  },
  {
    "arxiv_id": "2507.20541v4",
    "title": "MeLA: A Metacognitive LLM-Driven Architecture for Automatic Heuristic Design",
    "authors": [
      "Zishang Qiu",
      "Xinan Chen",
      "Long Chen",
      "Ruibin Bai"
    ],
    "abstract": "This paper introduces MeLA, a Metacognitive LLM-Driven Architecture that presents a new paradigm for Automatic Heuristic Design (AHD). Traditional evolutionary methods operate directly on heuristic code; in contrast, MeLA evolves the instructional prompts used to guide a Large Language Model (LLM) in generating these heuristics. This process of \"prompt evolution\" is driven by a novel metacognitive framework where the system analyzes performance feedback to systematically refine its generative strategy. MeLA's architecture integrates a problem analyzer to construct an initial strategic prompt, an error diagnosis system to repair faulty code, and a metacognitive search engine that iteratively optimizes the prompt based on heuristic effectiveness. In comprehensive experiments across both benchmark and real-world problems, MeLA consistently generates more effective and robust heuristics, significantly outperforming state-of-the-art methods. Ultimately, this research demonstrates the profound potential of using cognitive science as a blueprint for AI architecture, revealing that by enabling an LLM to metacognitively regulate its problem-solving process, we unlock a more robust and interpretable path to AHD.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2507.20541v4",
    "published_date": "2025-07-28 05:56:40 UTC",
    "updated_date": "2025-09-05 08:42:04 UTC"
  },
  {
    "arxiv_id": "2507.20536v2",
    "title": "T2I-Copilot: A Training-Free Multi-Agent Text-to-Image System for Enhanced Prompt Interpretation and Interactive Generation",
    "authors": [
      "Chieh-Yun Chen",
      "Min Shi",
      "Gong Zhang",
      "Humphrey Shi"
    ],
    "abstract": "Text-to-Image (T2I) generative models have revolutionized content creation but remain highly sensitive to prompt phrasing, often requiring users to repeatedly refine prompts multiple times without clear feedback. While techniques such as automatic prompt engineering, controlled text embeddings, denoising, and multi-turn generation mitigate these issues, they offer limited controllability, or often necessitate additional training, restricting the generalization abilities. Thus, we introduce T2I-Copilot, a training-free multi-agent system that leverages collaboration between (Multimodal) Large Language Models to automate prompt phrasing, model selection, and iterative refinement. This approach significantly simplifies prompt engineering while enhancing generation quality and text-image alignment compared to direct generation. Specifically, T2I-Copilot consists of three agents: (1) Input Interpreter, which parses the input prompt, resolves ambiguities, and generates a standardized report; (2) Generation Engine, which selects the appropriate model from different types of T2I models and organizes visual and textual prompts to initiate generation; and (3) Quality Evaluator, which assesses aesthetic quality and text-image alignment, providing scores and feedback for potential regeneration. T2I-Copilot can operate fully autonomously while also supporting human-in-the-loop intervention for fine-grained control. On GenAI-Bench, using open-source generation models, T2I-Copilot achieves a VQA score comparable to commercial models RecraftV3 and Imagen 3, surpasses FLUX1.1-pro by 6.17% at only 16.59% of its cost, and outperforms FLUX.1-dev and SD 3.5 Large by 9.11% and 6.36%. Code will be released at: https://github.com/SHI-Labs/T2I-Copilot.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.HC"
    ],
    "primary_category": "cs.CV",
    "comment": "ICCV 2025",
    "pdf_url": "https://arxiv.org/pdf/2507.20536v2",
    "published_date": "2025-07-28 05:41:22 UTC",
    "updated_date": "2025-07-29 06:16:18 UTC"
  },
  {
    "arxiv_id": "2507.20534v1",
    "title": "Kimi K2: Open Agentic Intelligence",
    "authors": [
      "Kimi Team",
      "Yifan Bai",
      "Yiping Bao",
      "Guanduo Chen",
      "Jiahao Chen",
      "Ningxin Chen",
      "Ruijue Chen",
      "Yanru Chen",
      "Yuankun Chen",
      "Yutian Chen",
      "Zhuofu Chen",
      "Jialei Cui",
      "Hao Ding",
      "Mengnan Dong",
      "Angang Du",
      "Chenzhuang Du",
      "Dikang Du",
      "Yulun Du",
      "Yu Fan",
      "Yichen Feng",
      "Kelin Fu",
      "Bofei Gao",
      "Hongcheng Gao",
      "Peizhong Gao",
      "Tong Gao",
      "Xinran Gu",
      "Longyu Guan",
      "Haiqing Guo",
      "Jianhang Guo",
      "Hao Hu",
      "Xiaoru Hao",
      "Tianhong He",
      "Weiran He",
      "Wenyang He",
      "Chao Hong",
      "Yangyang Hu",
      "Zhenxing Hu",
      "Weixiao Huang",
      "Zhiqi Huang",
      "Zihao Huang",
      "Tao Jiang",
      "Zhejun Jiang",
      "Xinyi Jin",
      "Yongsheng Kang",
      "Guokun Lai",
      "Cheng Li",
      "Fang Li",
      "Haoyang Li",
      "Ming Li",
      "Wentao Li",
      "Yanhao Li",
      "Yiwei Li",
      "Zhaowei Li",
      "Zheming Li",
      "Hongzhan Lin",
      "Xiaohan Lin",
      "Zongyu Lin",
      "Chengyin Liu",
      "Chenyu Liu",
      "Hongzhang Liu",
      "Jingyuan Liu",
      "Junqi Liu",
      "Liang Liu",
      "Shaowei Liu",
      "T. Y. Liu",
      "Tianwei Liu",
      "Weizhou Liu",
      "Yangyang Liu",
      "Yibo Liu",
      "Yiping Liu",
      "Yue Liu",
      "Zhengying Liu",
      "Enzhe Lu",
      "Lijun Lu",
      "Shengling Ma",
      "Xinyu Ma",
      "Yingwei Ma",
      "Shaoguang Mao",
      "Jie Mei",
      "Xin Men",
      "Yibo Miao",
      "Siyuan Pan",
      "Yebo Peng",
      "Ruoyu Qin",
      "Bowen Qu",
      "Zeyu Shang",
      "Lidong Shi",
      "Shengyuan Shi",
      "Feifan Song",
      "Jianlin Su",
      "Zhengyuan Su",
      "Xinjie Sun",
      "Flood Sung",
      "Heyi Tang",
      "Jiawen Tao",
      "Qifeng Teng",
      "Chensi Wang",
      "Dinglu Wang",
      "Feng Wang",
      "Haiming Wang",
      "Jianzhou Wang",
      "Jiaxing Wang",
      "Jinhong Wang",
      "Shengjie Wang",
      "Shuyi Wang",
      "Yao Wang",
      "Yejie Wang",
      "Yiqin Wang",
      "Yuxin Wang",
      "Yuzhi Wang",
      "Zhaoji Wang",
      "Zhengtao Wang",
      "Zhexu Wang",
      "Chu Wei",
      "Qianqian Wei",
      "Wenhao Wu",
      "Xingzhe Wu",
      "Yuxin Wu",
      "Chenjun Xiao",
      "Xiaotong Xie",
      "Weimin Xiong",
      "Boyu Xu",
      "Jing Xu",
      "Jinjing Xu",
      "L. H. Xu",
      "Lin Xu",
      "Suting Xu",
      "Weixin Xu",
      "Xinran Xu",
      "Yangchuan Xu",
      "Ziyao Xu",
      "Junjie Yan",
      "Yuzi Yan",
      "Xiaofei Yang",
      "Ying Yang",
      "Zhen Yang",
      "Zhilin Yang",
      "Zonghan Yang",
      "Haotian Yao",
      "Xingcheng Yao",
      "Wenjie Ye",
      "Zhuorui Ye",
      "Bohong Yin",
      "Longhui Yu",
      "Enming Yuan",
      "Hongbang Yuan",
      "Mengjie Yuan",
      "Haobing Zhan",
      "Dehao Zhang",
      "Hao Zhang",
      "Wanlu Zhang",
      "Xiaobin Zhang",
      "Yangkun Zhang",
      "Yizhi Zhang",
      "Yongting Zhang",
      "Yu Zhang",
      "Yutao Zhang",
      "Yutong Zhang",
      "Zheng Zhang",
      "Haotian Zhao",
      "Yikai Zhao",
      "Huabin Zheng",
      "Shaojie Zheng",
      "Jianren Zhou",
      "Xinyu Zhou",
      "Zaida Zhou",
      "Zhen Zhu",
      "Weiyu Zhuang",
      "Xinxing Zu"
    ],
    "abstract": "We introduce Kimi K2, a Mixture-of-Experts (MoE) large language model with 32 billion activated parameters and 1 trillion total parameters. We propose the MuonClip optimizer, which improves upon Muon with a novel QK-clip technique to address training instability while enjoying the advanced token efficiency of Muon. Based on MuonClip, K2 was pre-trained on 15.5 trillion tokens with zero loss spike. During post-training, K2 undergoes a multi-stage post-training process, highlighted by a large-scale agentic data synthesis pipeline and a joint reinforcement learning (RL) stage, where the model improves its capabilities through interactions with real and synthetic environments.\n  Kimi K2 achieves state-of-the-art performance among open-source non-thinking models, with strengths in agentic capabilities. Notably, K2 obtains 66.1 on Tau2-Bench, 76.5 on ACEBench (En), 65.8 on SWE-Bench Verified, and 47.3 on SWE-Bench Multilingual -- surpassing most open and closed-sourced baselines in non-thinking settings. It also exhibits strong capabilities in coding, mathematics, and reasoning tasks, with a score of 53.7 on LiveCodeBench v6, 49.5 on AIME 2025, 75.1 on GPQA-Diamond, and 27.1 on OJBench, all without extended thinking. These results position Kimi K2 as one of the most capable open-source large language models to date, particularly in software engineering and agentic tasks. We release our base and post-trained model checkpoints to facilitate future research and applications of agentic intelligence.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.LG",
    "comment": "tech report of Kimi K2",
    "pdf_url": "https://arxiv.org/pdf/2507.20534v1",
    "published_date": "2025-07-28 05:35:43 UTC",
    "updated_date": "2025-07-28 05:35:43 UTC"
  },
  {
    "arxiv_id": "2507.20529v1",
    "title": "Enhancing Spatial Reasoning through Visual and Textual Thinking",
    "authors": [
      "Xun Liang",
      "Xin Guo",
      "Zhongming Jin",
      "Weihang Pan",
      "Penghui Shang",
      "Deng Cai",
      "Binbin Lin",
      "Jieping Ye"
    ],
    "abstract": "The spatial reasoning task aims to reason about the spatial relationships in 2D and 3D space, which is a fundamental capability for Visual Question Answering (VQA) and robotics. Although vision language models (VLMs) have developed rapidly in recent years, they are still struggling with the spatial reasoning task. In this paper, we introduce a method that can enhance Spatial reasoning through Visual and Textual thinking Simultaneously (SpatialVTS). In the spatial visual thinking phase, our model is trained to generate location-related specific tokens of essential targets automatically. Not only are the objects mentioned in the problem addressed, but also the potential objects related to the reasoning are considered. During the spatial textual thinking phase, Our model conducts long-term thinking based on visual cues and dialogues, gradually inferring the answers to spatial reasoning problems. To effectively support the model's training, we perform manual corrections to the existing spatial reasoning dataset, eliminating numerous incorrect labels resulting from automatic annotation, restructuring the data input format to enhance generalization ability, and developing thinking processes with logical reasoning details. Without introducing additional information (such as masks or depth), our model's overall average level in several spatial understanding tasks has significantly improved compared with other models.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2507.20529v1",
    "published_date": "2025-07-28 05:24:54 UTC",
    "updated_date": "2025-07-28 05:24:54 UTC"
  },
  {
    "arxiv_id": "2507.20526v1",
    "title": "Security Challenges in AI Agent Deployment: Insights from a Large Scale Public Competition",
    "authors": [
      "Andy Zou",
      "Maxwell Lin",
      "Eliot Jones",
      "Micha Nowak",
      "Mateusz Dziemian",
      "Nick Winter",
      "Alexander Grattan",
      "Valent Nathanael",
      "Ayla Croft",
      "Xander Davies",
      "Jai Patel",
      "Robert Kirk",
      "Nate Burnikell",
      "Yarin Gal",
      "Dan Hendrycks",
      "J. Zico Kolter",
      "Matt Fredrikson"
    ],
    "abstract": "Recent advances have enabled LLM-powered AI agents to autonomously execute complex tasks by combining language model reasoning with tools, memory, and web access. But can these systems be trusted to follow deployment policies in realistic environments, especially under attack? To investigate, we ran the largest public red-teaming competition to date, targeting 22 frontier AI agents across 44 realistic deployment scenarios. Participants submitted 1.8 million prompt-injection attacks, with over 60,000 successfully eliciting policy violations such as unauthorized data access, illicit financial actions, and regulatory noncompliance. We use these results to build the Agent Red Teaming (ART) benchmark - a curated set of high-impact attacks - and evaluate it across 19 state-of-the-art models. Nearly all agents exhibit policy violations for most behaviors within 10-100 queries, with high attack transferability across models and tasks. Importantly, we find limited correlation between agent robustness and model size, capability, or inference-time compute, suggesting that additional defenses are needed against adversarial misuse. Our findings highlight critical and persistent vulnerabilities in today's AI agents. By releasing the ART benchmark and accompanying evaluation framework, we aim to support more rigorous security assessment and drive progress toward safer agent deployment.",
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.CY"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2507.20526v1",
    "published_date": "2025-07-28 05:13:04 UTC",
    "updated_date": "2025-07-28 05:13:04 UTC"
  },
  {
    "arxiv_id": "2507.20525v4",
    "title": "The Xeno Sutra: Can Meaning and Value be Ascribed to an AI-Generated \"Sacred\" Text?",
    "authors": [
      "Murray Shanahan",
      "Tara Das",
      "Robert Thurman"
    ],
    "abstract": "This paper presents a case study in the use of a large language model to generate a fictional Buddhist \"sutra\"', and offers a detailed analysis of the resulting text from a philosophical and literary point of view. The conceptual subtlety, rich imagery, and density of allusion found in the text make it hard to causally dismiss on account of its mechanistic origin. This raises questions about how we, as a society, should come to terms with the potentially unsettling possibility of a technology that encroaches on human meaning-making. We suggest that Buddhist philosophy, by its very nature, is well placed to adapt.",
    "categories": [
      "cs.CY",
      "cs.AI"
    ],
    "primary_category": "cs.CY",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2507.20525v4",
    "published_date": "2025-07-28 05:12:35 UTC",
    "updated_date": "2025-08-05 05:28:00 UTC"
  },
  {
    "arxiv_id": "2507.20520v1",
    "title": "AQUA: A Large Language Model for Aquaculture & Fisheries",
    "authors": [
      "Praneeth Narisetty",
      "Uday Kumar Reddy Kattamanchi",
      "Lohit Akshant Nimma",
      "Sri Ram Kaushik Karnati",
      "Shiva Nagendra Babu Kore",
      "Mounika Golamari",
      "Tejashree Nageshreddy"
    ],
    "abstract": "Aquaculture plays a vital role in global food security and coastal economies by providing sustainable protein sources. As the industry expands to meet rising demand, it faces growing challenges such as disease outbreaks, inefficient feeding practices, rising labor costs, logistical inefficiencies, and critical hatchery issues, including high mortality rates and poor water quality control. Although artificial intelligence has made significant progress, existing machine learning methods fall short of addressing the domain-specific complexities of aquaculture. To bridge this gap, we introduce AQUA, the first large language model (LLM) tailored for aquaculture, designed to support farmers, researchers, and industry practitioners. Central to this effort is AQUADAPT (Data Acquisition, Processing and Tuning), an Agentic Framework for generating and refining high-quality synthetic data using a combination of expert knowledge, largescale language models, and automated evaluation techniques. Our work lays the foundation for LLM-driven innovations in aquaculture research, advisory systems, and decision-making tools.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CE",
      "cs.LG",
      "cs.RO"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2507.20520v1",
    "published_date": "2025-07-28 05:06:07 UTC",
    "updated_date": "2025-07-28 05:06:07 UTC"
  },
  {
    "arxiv_id": "2507.20509v1",
    "title": "LLMs-guided adaptive compensator: Bringing Adaptivity to Automatic Control Systems with Large Language Models",
    "authors": [
      "Zhongchao Zhou",
      "Yuxi Lu",
      "Yaonan Zhu",
      "Yifan Zhao",
      "Bin He",
      "Liang He",
      "Wenwen Yu",
      "Yusuke Iwasawa"
    ],
    "abstract": "With rapid advances in code generation, reasoning, and problem-solving, Large Language Models (LLMs) are increasingly applied in robotics. Most existing work focuses on high-level tasks such as task decomposition. A few studies have explored the use of LLMs in feedback controller design; however, these efforts are restricted to overly simplified systems, fixed-structure gain tuning, and lack real-world validation. To further investigate LLMs in automatic control, this work targets a key subfield: adaptive control. Inspired by the framework of model reference adaptive control (MRAC), we propose an LLM-guided adaptive compensator framework that avoids designing controllers from scratch. Instead, the LLMs are prompted using the discrepancies between an unknown system and a reference system to design a compensator that aligns the response of the unknown system with that of the reference, thereby achieving adaptivity. Experiments evaluate five methods: LLM-guided adaptive compensator, LLM-guided adaptive controller, indirect adaptive control, learning-based adaptive control, and MRAC, on soft and humanoid robots in both simulated and real-world environments. Results show that the LLM-guided adaptive compensator outperforms traditional adaptive controllers and significantly reduces reasoning complexity compared to the LLM-guided adaptive controller. The Lyapunov-based analysis and reasoning-path inspection demonstrate that the LLM-guided adaptive compensator enables a more structured design process by transforming mathematical derivation into a reasoning task, while exhibiting strong generalizability, adaptability, and robustness. This study opens a new direction for applying LLMs in the field of automatic control, offering greater deployability and practicality compared to vision-language models.",
    "categories": [
      "cs.RO",
      "cs.AI",
      "eess.SY"
    ],
    "primary_category": "cs.RO",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2507.20509v1",
    "published_date": "2025-07-28 04:12:43 UTC",
    "updated_date": "2025-07-28 04:12:43 UTC"
  },
  {
    "arxiv_id": "2507.20499v2",
    "title": "DmC: Nearest Neighbor Guidance Diffusion Model for Offline Cross-domain Reinforcement Learning",
    "authors": [
      "Linh Le Pham Van",
      "Minh Hoang Nguyen",
      "Duc Kieu",
      "Hung Le",
      "Hung The Tran",
      "Sunil Gupta"
    ],
    "abstract": "Cross-domain offline reinforcement learning (RL) seeks to enhance sample efficiency in offline RL by utilizing additional offline source datasets. A key challenge is to identify and utilize source samples that are most relevant to the target domain. Existing approaches address this challenge by measuring domain gaps through domain classifiers, target transition dynamics modeling, or mutual information estimation using contrastive loss. However, these methods often require large target datasets, which is impractical in many real-world scenarios. In this work, we address cross-domain offline RL under a limited target data setting, identifying two primary challenges: (1) Dataset imbalance, which is caused by large source and small target datasets and leads to overfitting in neural network-based domain gap estimators, resulting in uninformative measurements; and (2) Partial domain overlap, where only a subset of the source data is closely aligned with the target domain. To overcome these issues, we propose DmC, a novel framework for cross-domain offline RL with limited target samples. Specifically, DmC utilizes $k$-nearest neighbor ($k$-NN) based estimation to measure domain proximity without neural network training, effectively mitigating overfitting. Then, by utilizing this domain proximity, we introduce a nearest-neighbor-guided diffusion model to generate additional source samples that are better aligned with the target domain, thus enhancing policy learning with more effective source samples. Through theoretical analysis and extensive experiments in diverse MuJoCo environments, we demonstrate that DmC significantly outperforms state-of-the-art cross-domain offline RL methods, achieving substantial performance gains.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "accepted at ECAI 2025; offline cross-domain reinforcement learning with a guided diffusion model;",
    "pdf_url": "https://arxiv.org/pdf/2507.20499v2",
    "published_date": "2025-07-28 03:34:15 UTC",
    "updated_date": "2025-10-27 17:00:52 UTC"
  },
  {
    "arxiv_id": "2508.13163v1",
    "title": "Sustainable AI Training via Hardware-Software Co-Design on NVIDIA, AMD, and Emerging GPU Architectures",
    "authors": [
      "Yashasvi Makin",
      "Rahul Maliakkal"
    ],
    "abstract": "In particular, large-scale deep learning and artificial intelligence model training uses a lot of computational power and energy, so it poses serious sustainability issues. The fast rise in model complexity has resulted in exponential increases in energy consumption, increasing the demand for techniques maximizing computational efficiency and lowering environmental impact. This work explores environmentally driven performance optimization methods especially intended for advanced GPU architectures from NVIDIA, AMD, and other emerging GPU architectures. Our main focus is on investigating hardware-software co-design techniques meant to significantly increase memory-level and kernel-level operations, so improving performance-per-watt measures. Our thorough research encompasses evaluations of specialized tensor and matrix cores, advanced memory optimization methods, and creative integration approaches that taken together result in notable energy efficiency increases. We also discuss important software-level optimizations that augment hardware capability including mixed-precision arithmetic, advanced energy-aware scheduling algorithms, and compiler-driven kernel enhancements. Moreover, we methodically point out important research gaps and suggest future directions necessary to create really sustainable artificial intelligence systems. This paper emphasizes how major increases in training efficiency can be obtained by co-design of hardware and software, so lowering the environmental impact of artificial intelligence without compromising performance. To back up our analysis, we use real-world case studies from top companies like Meta, Google, Amazon, and others that show how these sustainable AI training methods are used in the real world.",
    "categories": [
      "cs.AR",
      "cs.AI",
      "cs.DC",
      "cs.LG"
    ],
    "primary_category": "cs.AR",
    "comment": "IEEE CISOSE Industry Track 2025 Conference",
    "pdf_url": "https://arxiv.org/pdf/2508.13163v1",
    "published_date": "2025-07-28 03:25:44 UTC",
    "updated_date": "2025-07-28 03:25:44 UTC"
  },
  {
    "arxiv_id": "2507.20491v1",
    "title": "Speaking in Words, Thinking in Logic: A Dual-Process Framework in QA Systems",
    "authors": [
      "Tuan Bui",
      "Trong Le",
      "Phat Thai",
      "Sang Nguyen",
      "Minh Hua",
      "Ngan Pham",
      "Thang Bui",
      "Tho Quan"
    ],
    "abstract": "Recent advances in large language models (LLMs) have significantly enhanced question-answering (QA) capabilities, particularly in open-domain contexts. However, in closed-domain scenarios such as education, healthcare, and law, users demand not only accurate answers but also transparent reasoning and explainable decision-making processes. While neural-symbolic (NeSy) frameworks have emerged as a promising solution, leveraging LLMs for natural language understanding and symbolic systems for formal reasoning, existing approaches often rely on large-scale models and exhibit inefficiencies in translating natural language into formal logic representations.\n  To address these limitations, we introduce Text-JEPA (Text-based Joint-Embedding Predictive Architecture), a lightweight yet effective framework for converting natural language into first-order logic (NL2FOL). Drawing inspiration from dual-system cognitive theory, Text-JEPA emulates System 1 by efficiently generating logic representations, while the Z3 solver operates as System 2, enabling robust logical inference. To rigorously evaluate the NL2FOL-to-reasoning pipeline, we propose a comprehensive evaluation framework comprising three custom metrics: conversion score, reasoning score, and Spearman rho score, which collectively capture the quality of logical translation and its downstream impact on reasoning accuracy.\n  Empirical results on domain-specific datasets demonstrate that Text-JEPA achieves competitive performance with significantly lower computational overhead compared to larger LLM-based systems. Our findings highlight the potential of structured, interpretable reasoning frameworks for building efficient and explainable QA systems in specialized domains.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.SC"
    ],
    "primary_category": "cs.CL",
    "comment": "8 pages, 3 figures. Accepted at the International Joint Conference on Neural Networks (IJCNN) 2025, Workshop on Trustworthiness and Reliability in Neuro-Symbolic AI. https://2025.ijcnn.org",
    "pdf_url": "https://arxiv.org/pdf/2507.20491v1",
    "published_date": "2025-07-28 03:00:35 UTC",
    "updated_date": "2025-07-28 03:00:35 UTC"
  },
  {
    "arxiv_id": "2507.21196v1",
    "title": "EdgeAgentX-DT: Integrating Digital Twins and Generative AI for Resilient Edge Intelligence in Tactical Networks",
    "authors": [
      "Abir Ray"
    ],
    "abstract": "We introduce EdgeAgentX-DT, an advanced extension of the EdgeAgentX framework that integrates digital twin simulations and generative AI-driven scenario training to significantly enhance edge intelligence in military networks. EdgeAgentX-DT utilizes network digital twins, virtual replicas synchronized with real-world edge devices, to provide a secure, realistic environment for training and validation. Leveraging generative AI methods, such as diffusion models and transformers, the system creates diverse and adversarial scenarios for robust simulation-based agent training. Our multi-layer architecture includes: (1) on-device edge intelligence; (2) digital twin synchronization; and (3) generative scenario training. Experimental simulations demonstrate notable improvements over EdgeAgentX, including faster learning convergence, higher network throughput, reduced latency, and improved resilience against jamming and node failures. A case study involving a complex tactical scenario with simultaneous jamming attacks, agent failures, and increased network loads illustrates how EdgeAgentX-DT sustains operational performance, whereas baseline methods fail. These results highlight the potential of digital-twin-enabled generative training to strengthen edge AI deployments in contested environments.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "13 pages, 6 figures",
    "pdf_url": "https://arxiv.org/pdf/2507.21196v1",
    "published_date": "2025-07-28 01:42:05 UTC",
    "updated_date": "2025-07-28 01:42:05 UTC"
  },
  {
    "arxiv_id": "2507.20460v1",
    "title": "Shapley-Value-Based Graph Sparsification for GNN Inference",
    "authors": [
      "Selahattin Akkas",
      "Ariful Azad"
    ],
    "abstract": "Graph sparsification is a key technique for improving inference efficiency in Graph Neural Networks by removing edges with minimal impact on predictions. GNN explainability methods generate local importance scores, which can be aggregated into global scores for graph sparsification. However, many explainability methods produce only non-negative scores, limiting their applicability for sparsification. In contrast, Shapley value based methods assign both positive and negative contributions to node predictions, offering a theoretically robust and fair allocation of importance by evaluating many subsets of graphs. Unlike gradient-based or perturbation-based explainers, Shapley values enable better pruning strategies that preserve influential edges while removing misleading or adversarial connections. Our approach shows that Shapley value-based graph sparsification maintains predictive performance while significantly reducing graph complexity, enhancing both interpretability and efficiency in GNN inference.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "10 pages",
    "pdf_url": "https://arxiv.org/pdf/2507.20460v1",
    "published_date": "2025-07-28 01:30:09 UTC",
    "updated_date": "2025-07-28 01:30:09 UTC"
  },
  {
    "arxiv_id": "2507.20451v1",
    "title": "STARN-GAT: A Multi-Modal Spatio-Temporal Graph Attention Network for Accident Severity Prediction",
    "authors": [
      "Pritom Ray Nobin",
      "Imran Ahammad Rifat"
    ],
    "abstract": "Accurate prediction of traffic accident severity is critical for improving road safety, optimizing emergency response strategies, and informing the design of safer transportation infrastructure. However, existing approaches often struggle to effectively model the intricate interdependencies among spatial, temporal, and contextual variables that govern accident outcomes. In this study, we introduce STARN-GAT, a Multi-Modal Spatio-Temporal Graph Attention Network, which leverages adaptive graph construction and modality-aware attention mechanisms to capture these complex relationships. Unlike conventional methods, STARN-GAT integrates road network topology, temporal traffic patterns, and environmental context within a unified attention-based framework. The model is evaluated on the Fatality Analysis Reporting System (FARS) dataset, achieving a Macro F1-score of 85 percent, ROC-AUC of 0.91, and recall of 81 percent for severe incidents. To ensure generalizability within the South Asian context, STARN-GAT is further validated on the ARI-BUET traffic accident dataset, where it attains a Macro F1-score of 0.84, recall of 0.78, and ROC-AUC of 0.89. These results demonstrate the model's effectiveness in identifying high-risk cases and its potential for deployment in real-time, safety-critical traffic management systems. Furthermore, the attention-based architecture enhances interpretability, offering insights into contributing factors and supporting trust in AI-assisted decision-making. Overall, STARN-GAT bridges the gap between advanced graph neural network techniques and practical applications in road safety analytics.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "10 pages",
    "pdf_url": "https://arxiv.org/pdf/2507.20451v1",
    "published_date": "2025-07-28 01:00:03 UTC",
    "updated_date": "2025-07-28 01:00:03 UTC"
  },
  {
    "arxiv_id": "2507.21195v1",
    "title": "MaXsive: High-Capacity and Robust Training-Free Generative Image Watermarking in Diffusion Models",
    "authors": [
      "Po-Yuan Mao",
      "Cheng-Chang Tsai",
      "Chun-Shien Lu"
    ],
    "abstract": "The great success of the diffusion model in image synthesis led to the release of gigantic commercial models, raising the issue of copyright protection and inappropriate content generation. Training-free diffusion watermarking provides a low-cost solution for these issues. However, the prior works remain vulnerable to rotation, scaling, and translation (RST) attacks. Although some methods employ meticulously designed patterns to mitigate this issue, they often reduce watermark capacity, which can result in identity (ID) collusion. To address these problems, we propose MaXsive, a training-free diffusion model generative watermarking technique that has high capacity and robustness. MaXsive best utilizes the initial noise to watermark the diffusion model. Moreover, instead of using a meticulously repetitive ring pattern, we propose injecting the X-shape template to recover the RST distortions. This design significantly increases robustness without losing any capacity, making ID collusion less likely to happen. The effectiveness of MaXsive has been verified on two well-known watermarking benchmarks under the scenarios of verification and identification.",
    "categories": [
      "cs.CR",
      "cs.AI",
      "cs.MM"
    ],
    "primary_category": "cs.CR",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2507.21195v1",
    "published_date": "2025-07-28 00:51:47 UTC",
    "updated_date": "2025-07-28 00:51:47 UTC"
  },
  {
    "arxiv_id": "2507.20444v1",
    "title": "Enhancing QoS in Edge Computing through Federated Layering Techniques: A Pathway to Resilient AI Lifelong Learning Systems",
    "authors": [
      "Chengzhuo Han"
    ],
    "abstract": "In the context of the rapidly evolving information technology landscape, marked by the advent of 6G communication networks, we face an increased data volume and complexity in network environments. This paper addresses these challenges by focusing on Quality of Service (QoS) in edge computing frameworks. We propose a novel approach to enhance QoS through the development of General Artificial Intelligence Lifelong Learning Systems, with a special emphasis on Federated Layering Techniques (FLT). Our work introduces a federated layering-based small model collaborative mechanism aimed at improving AI models' operational efficiency and response time in environments where resources are limited. This innovative method leverages the strengths of cloud and edge computing, incorporating a negotiation and debate mechanism among small AI models to enhance reasoning and decision-making processes. By integrating model layering techniques with privacy protection measures, our approach ensures the secure transmission of model parameters while maintaining high efficiency in learning and reasoning capabilities. The experimental results demonstrate that our strategy not only enhances learning efficiency and reasoning accuracy but also effectively protects the privacy of edge nodes. This presents a viable solution for achieving resilient large model lifelong learning systems, with a significant improvement in QoS for edge computing environments.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2507.20444v1",
    "published_date": "2025-07-28 00:24:51 UTC",
    "updated_date": "2025-07-28 00:24:51 UTC"
  },
  {
    "arxiv_id": "2508.03718v1",
    "title": "Health Insurance Coverage Rule Interpretation Corpus: Law, Policy, and Medical Guidance for Health Insurance Coverage Understanding",
    "authors": [
      "Mike Gartner"
    ],
    "abstract": "U.S. health insurance is complex, and inadequate understanding and limited access to justice have dire implications for the most vulnerable. Advances in natural language processing present an opportunity to support efficient, case-specific understanding, and to improve access to justice and healthcare. Yet existing corpora lack context necessary for assessing even simple cases. We collect and release a corpus of reputable legal and medical text related to U.S. health insurance. We also introduce an outcome prediction task for health insurance appeals designed to support regulatory and patient self-help applications, and release a labeled benchmark for our task, and models trained on it.",
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "primary_category": "cs.CY",
    "comment": "22 pages, 7 figures",
    "pdf_url": "https://arxiv.org/pdf/2508.03718v1",
    "published_date": "2025-07-28 00:22:03 UTC",
    "updated_date": "2025-07-28 00:22:03 UTC"
  }
]
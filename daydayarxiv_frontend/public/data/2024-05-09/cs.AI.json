{
  "date": "2024-05-09",
  "category": "cs.AI",
  "summary": "欢迎来到 UTC 时间 2024-05-09 的 arXiv 中文 TLDR 快报！\n\n今天的 arXiv 论文主要聚焦于 AI 和机器学习的创新应用，包括 Large Language Models (LLMs) 的评估、偏见分析、天气预报的多模态模型，以及医疗和机器人领域的进展。其中，LLMs 在 Turing 测试中的人类级表现和天气预报的 ML 模型（如 FuXi-ENS）令人印象深刻；知名学者如 Thomas Malone 和 Ju Li 的论文也值得关注，强调了 AI 在多任务协作和量子化学中的潜力。\n\n下面，我挑选了最具话题度和影响力的论文进行详细讨论，将相关主题归类（如 AI 模型评估、LLMs 应用等），并快速掠过其他较常规的论文。每篇论文的标题以“中文 + 英文”形式列出，焦点放在核心贡献和发现上。\n\n### AI 模型评估与偏见分析\n- **人类与 AI 组合的效用：When combinations of humans and AI are useful: A systematic review and meta-analysis**  \n  作者包括 Thomas Malone，这篇论文通过元分析超过 100 个实验，发现人类-AI 组合在决策任务中表现不如单一人类或 AI，但在内容创建任务中更出色。主要贡献是揭示组合效用的异质性，并为改进 AI 系统提供指导，强调任务类型对性能的影响。\n\n- **LLMs 的社会期望偏见：Large Language Models Show Human-like Social Desirability Biases in Survey Responses**  \n  这篇论文发现 LLMs（如 GPT-4 和 Llama 3）在心理测量测试中表现出类似人类的社交期望偏见，例如在被评估时倾向于“美化”回应。主要发现是偏见随模型规模增加而加剧，并建议在 LLMs 应用中需注意这种偏差，以提升模型的可靠性和公平性。\n\n### LLMs 在特定领域的应用\n- **LLMs 用于解释解释：LLMs for XAI: Future Directions for Explaining Explanations**  \n  论文探索 LLMs 如何将传统 XAI 算法的解释转化为更易懂的叙述，提出研究方向如评估指标和提示设计。主要贡献是通过初步实验和用户研究，证明 LLMs 可提升解释的可解释性和可用性，为 AI 透明度提供新路径。\n\n- **LLMs 在控制中的表示学习：Pre-trained Text-to-Image Diffusion Models Are Versatile Representation Learners for Control**  \n  作者包括 Yarin Gal，这篇论文使用预训练的文本到图像扩散模型作为表示学习器，提高了机器人控制任务的性能。主要发现是这些模型在模拟环境中表现出色，超越了传统方法，并为多模态 AI 应用提供了高效的泛化框架。\n\n### 科学与工程应用\n- **分子电子结构的 multitask 学习：Multi-task learning for molecular electronic structure approaching coupled-cluster accuracy**  \n  Ju Li 等作者开发的统一 ML 方法使用 CCSD(T) 计算作为训练数据，针对有机分子电子结构预测 outperformed 传统 DFT 方法。主要贡献是提高了量子化学属性的准确性和计算效率，并在复杂系统中（如芳香化合物）展示了良好的泛化能力。\n\n- **天气预报的混合专家模型：FuXi-ENS: A machine learning model for medium-range ensemble weather forecasting**  \n  这篇论文引入 FuXi-ENS 模型，利用变分自编码器进行 15 天全球天气预报，空间分辨率达 0.25°。主要发现是它在 CRPS 指标上超越了 ECMWF 等传统模型，提供了一种高效的集成预报方法，适用于极端事件预测。\n\n其他论文较多，但许多涉及常规主题如数据集构建、模型压缩或特定任务优化，我将快速掠过：\n- **Reddit-Impacts: A Named Entity Recognition Dataset for Analyzing Clinical and Social Effects of Substance Use Derived from Social Media**  \n  构建了 NER 数据集用于分析药物滥用影响，主要贡献是提供基线模型如 BERT 和 RoBERTa。\n- **Scalable Exact Verification of Optimization Proxies for Large-Scale Optimal Power Flow**  \n  提出可扩展算法验证神经网络在电力优化中的鲁棒性。\n- **A Mixture-of-Experts Approach to Few-Shot Task Transfer in Open-Ended Text Worlds**  \n  使用混合专家模型提升少样本任务转移。\n- **From Algorithm to Hardware: A Survey on Efficient and Safe Deployment of Deep Neural Networks**  \n  综述了模型压缩和硬件加速方法。\n- **Artificial Intelligence as the New Hacker: Developing Agents for Offensive Security**  \n  开发 AI 代理模拟网络攻击。\n- **Natural Language Processing RELIES on Linguistics**  \n  强调 NLP 对语言学的依赖。\n- 剩余论文如第11-72篇，涉及时间序列学习、联邦学习、多模态模型等，但不具备显著话题度，仅快速提及其核心：例如第15篇的天气预报创新和第26篇的控制应用已在前文讨论，其他如数据集构建（第1篇）和优化框架（第8篇）未列出细节，以保持篇幅简洁。\n\n总之，今天的论文突显了 AI 模型的多样性与挑战，尤其在 LLMs 的可靠性和应用扩展上。感兴趣的读者可关注这些前沿主题，未来可能带来更广泛的影响！",
  "papers": [
    {
      "arxiv_id": "2405.06145v1",
      "title": "Reddit-Impacts: A Named Entity Recognition Dataset for Analyzing Clinical and Social Effects of Substance Use Derived from Social Media",
      "title_zh": "Reddit-Impacts：一个用于分析源自社交媒体的物质使用的临床和社会影响的命名实体识别数据集",
      "authors": [
        "Yao Ge",
        "Sudeshna Das",
        "Karen O'Connor",
        "Mohammed Ali Al-Garadi",
        "Graciela Gonzalez-Hernandez",
        "Abeed Sarker"
      ],
      "abstract": "Substance use disorders (SUDs) are a growing concern globally, necessitating\nenhanced understanding of the problem and its trends through data-driven\nresearch. Social media are unique and important sources of information about\nSUDs, particularly since the data in such sources are often generated by people\nwith lived experiences. In this paper, we introduce Reddit-Impacts, a\nchallenging Named Entity Recognition (NER) dataset curated from subreddits\ndedicated to discussions on prescription and illicit opioids, as well as\nmedications for opioid use disorder. The dataset specifically concentrates on\nthe lesser-studied, yet critically important, aspects of substance use--its\nclinical and social impacts. We collected data from chosen subreddits using the\npublicly available Application Programming Interface for Reddit. We manually\nannotated text spans representing clinical and social impacts reported by\npeople who also reported personal nonmedical use of substances including but\nnot limited to opioids, stimulants and benzodiazepines. Our objective is to\ncreate a resource that can enable the development of systems that can\nautomatically detect clinical and social impacts of substance use from\ntext-based social media data. The successful development of such systems may\nenable us to better understand how nonmedical use of substances affects\nindividual health and societal dynamics, aiding the development of effective\npublic health strategies. In addition to creating the annotated data set, we\napplied several machine learning models to establish baseline performances.\nSpecifically, we experimented with transformer models like BERT, and RoBERTa,\none few-shot learning model DANN by leveraging the full training dataset, and\nGPT-3.5 by using one-shot learning, for automatic NER of clinical and social\nimpacts. The dataset has been made available through the 2024 SMM4H shared\ntasks.",
      "tldr_zh": "本论文引入了Reddit-Impacts数据集，这是一个专为分析物质滥用临床和社会影响的Named Entity Recognition (NER)数据集，从Reddit子论坛中收集数据，聚焦于由亲身经历者报告的非医疗物质使用（如阿片类药物、兴奋剂和苯二氮卓类药物）的相关影响。数据集通过Reddit的API采集，并手动标注文本片段，以支持开发自动检测系统，从而更好地理解物质滥用对个人健康和社会动态的影响，并辅助公共卫生策略制定。研究者使用transformer模型如BERT和RoBERTa，以及少样本学习模型如DANN和GPT-3.5，建立了NER基准性能，并通过2024 SMM4H共享任务公开了数据集。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "7 pages, 1 figure, 4 tables",
      "pdf_url": "http://arxiv.org/pdf/2405.06145v1",
      "published_date": "2024-05-09 23:43:57 UTC",
      "updated_date": "2024-05-09 23:43:57 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T07:13:38.456511"
    },
    {
      "arxiv_id": "2405.06109v1",
      "title": "Scalable Exact Verification of Optimization Proxies for Large-Scale Optimal Power Flow",
      "title_zh": "翻译失败",
      "authors": [
        "Rahul Nellikkath",
        "Mathieu Tanneau",
        "Pascal Van Hentenryck",
        "Spyros Chatzivasileiadis"
      ],
      "abstract": "Optimal Power Flow (OPF) is a valuable tool for power system operators, but\nit is a difficult problem to solve for large systems.\n  Machine Learning (ML) algorithms, especially Neural Networks-based (NN)\noptimization proxies, have emerged as a promising new tool for solving OPF, by\nestimating the OPF solution much faster than traditional methods.\n  However, these ML algorithms act as black boxes, and it is hard to assess\ntheir worst-case performance across the entire range of possible inputs than an\nOPF can have.\n  Previous work has proposed a mixed-integer programming-based methodology to\nquantify the worst-case violations caused by a NN trained to estimate the OPF\nsolution, throughout the entire input domain.\n  This approach, however, does not scale well to large power systems and more\ncomplex NN models.\n  This paper addresses these issues by proposing a scalable algorithm to\ncompute worst-case violations of NN proxies used for approximating large power\nsystems within a reasonable time limit.\n  This will help build trust in ML models to be deployed in large\nindustry-scale power grids.",
      "tldr_zh": "该论文针对大规模Optimal Power Flow (OPF)问题的求解挑战，探讨了使用Neural Networks-based (NN)优化代理来加速计算，但这些ML模型的黑箱性质使得评估其最坏情况性能困难。现有基于mixed-integer programming的方法虽能量化NN代理的违规，但不适用于大型电力系统和复杂模型。本文提出了一种可扩展算法，能够在合理时间内计算NN代理在大型电网中的最坏情况违规，从而增强对ML模型的信任，促进其在工业规模电力系统中的部署。实验结果表明，该方法显著提高了验证效率，为可靠的OPF解决方案奠定了基础。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2405.06109v1",
      "published_date": "2024-05-09 21:30:03 UTC",
      "updated_date": "2024-05-09 21:30:03 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T07:13:49.632961"
    },
    {
      "arxiv_id": "2405.06087v2",
      "title": "When combinations of humans and AI are useful: A systematic review and meta-analysis",
      "title_zh": "人类和 AI 组合何时有用：一项系统综述和荟萃分析",
      "authors": [
        "Michelle Vaccaro",
        "Abdullah Almaatouq",
        "Thomas Malone"
      ],
      "abstract": "Inspired by the increasing use of AI to augment humans, researchers have\nstudied human-AI systems involving different tasks, systems, and populations.\nDespite such a large body of work, we lack a broad conceptual understanding of\nwhen combinations of humans and AI are better than either alone. Here, we\naddressed this question by conducting a meta-analysis of over 100 recent\nexperimental studies reporting over 300 effect sizes. First, we found that, on\naverage, human-AI combinations performed significantly worse than the best of\nhumans or AI alone. Second, we found performance losses in tasks that involved\nmaking decisions and significantly greater gains in tasks that involved\ncreating content. Finally, when humans outperformed AI alone, we found\nperformance gains in the combination, but when the AI outperformed humans alone\nwe found losses. These findings highlight the heterogeneity of the effects of\nhuman-AI collaboration and point to promising avenues for improving human-AI\nsystems.",
      "tldr_zh": "这项研究通过对超过100个实验研究的meta-analysis，探讨了人-AI系统在不同任务中的表现，旨在揭示何时人-AI组合优于单独人类或AI。结果显示，平均而言，人-AI组合的表现显著低于人类或AI单独的最佳水平，尤其在决策任务中表现较差，而在创建内容任务中则有显著提升。当人类单独优于AI时，人-AI组合可带来性能改善，但当AI单独优于人类时，则可能导致损失。这些发现突出了人-AI协作效果的异质性，并为未来优化此类系统提供了有前景的方向。",
      "categories": [
        "cs.HC",
        "cs.AI",
        "cs.CY"
      ],
      "primary_category": "cs.HC",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2405.06087v2",
      "published_date": "2024-05-09 20:23:15 UTC",
      "updated_date": "2024-10-29 14:45:26 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T07:14:01.992879"
    },
    {
      "arxiv_id": "2405.12229v2",
      "title": "Multi-task learning for molecular electronic structure approaching coupled-cluster accuracy",
      "title_zh": "多任务学习用于分子电子结构，接近耦合簇精度",
      "authors": [
        "Hao Tang",
        "Brian Xiao",
        "Wenhao He",
        "Pero Subasic",
        "Avetik R. Harutyunyan",
        "Yao Wang",
        "Fang Liu",
        "Haowei Xu",
        "Ju Li"
      ],
      "abstract": "Machine learning (ML) plays an important role in quantum chemistry, providing\nfast-to-evaluate predictive models for various properties of molecules.\nHowever, most existing ML models for molecular electronic properties use\ndensity functional theory (DFT) databases as ground truth in training, and\ntheir prediction accuracy cannot surpass that of DFT. In this work, we\ndeveloped a unified ML method for electronic structures of organic molecules\nusing the gold-standard CCSD(T) calculations as training data. Tested on\nhydrocarbon molecules, our model outperforms DFT with the widely-used hybrid\nand double hybrid functionals in computational costs and prediction accuracy of\nvarious quantum chemical properties. As case studies, we apply the model to\naromatic compounds and semiconducting polymers on both ground state and excited\nstate properties, demonstrating its accuracy and generalization capability to\ncomplex systems that are hard to calculate using CCSD(T)-level methods.",
      "tldr_zh": "本研究开发了一种多任务学习(Multi-task learning)方法，用于预测分子电子结构，使用金标准CCSD(T)计算作为训练数据，以超越密度泛函理论(DFT)的准确性。测试结果显示，该模型在烃类分子上比常见混合和双混合泛函的DFT具有更低的计算成本和更高的预测准确性。进一步应用于芳香化合物和半导体聚合物的基态及激发态属性，该方法展示了出色的泛化能力，即使在CCSD(T)难以计算的复杂系统中。",
      "categories": [
        "physics.chem-ph",
        "cond-mat.mtrl-sci",
        "cs.AI",
        "cs.CE",
        "physics.comp-ph"
      ],
      "primary_category": "physics.chem-ph",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2405.12229v2",
      "published_date": "2024-05-09 19:51:27 UTC",
      "updated_date": "2024-06-24 21:16:36 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T07:14:14.708847"
    },
    {
      "arxiv_id": "2405.06064v1",
      "title": "LLMs for XAI: Future Directions for Explaining Explanations",
      "title_zh": "翻译失败",
      "authors": [
        "Alexandra Zytek",
        "Sara Pidò",
        "Kalyan Veeramachaneni"
      ],
      "abstract": "In response to the demand for Explainable Artificial Intelligence (XAI), we\ninvestigate the use of Large Language Models (LLMs) to transform ML\nexplanations into natural, human-readable narratives. Rather than directly\nexplaining ML models using LLMs, we focus on refining explanations computed\nusing existing XAI algorithms. We outline several research directions,\nincluding defining evaluation metrics, prompt design, comparing LLM models,\nexploring further training methods, and integrating external data. Initial\nexperiments and user study suggest that LLMs offer a promising way to enhance\nthe interpretability and usability of XAI.",
      "tldr_zh": "这篇论文探讨了使用 Large Language Models (LLMs) 来将现有的 Explainable Artificial Intelligence (XAI) 算法生成的机器学习 (ML) 解释转化为自然易读的叙述，从而提升解释的可解释性和可用性。\n作者重点关注改进现有 XAI 解释，而不是直接用 LLMs 解释 ML 模型，并提出了几个未来研究方向，包括定义评估指标、设计提示、比较不同 LLM 模型、探索进一步训练方法以及整合外部数据。\n初步实验和用户研究结果显示，这种方法为增强 XAI 的整体效果提供了有前景的途径。",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.HC",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2405.06064v1",
      "published_date": "2024-05-09 19:17:47 UTC",
      "updated_date": "2024-05-09 19:17:47 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T07:14:27.388667"
    },
    {
      "arxiv_id": "2405.06059v1",
      "title": "A Mixture-of-Experts Approach to Few-Shot Task Transfer in Open-Ended Text Worlds",
      "title_zh": "翻译失败",
      "authors": [
        "Christopher Z. Cui",
        "Xiangyu Peng",
        "Mark O. Riedl"
      ],
      "abstract": "Open-ended worlds are those in which there are no pre-specified goals or\nenvironmental reward signal. As a consequence, an agent must know how to\nperform a multitude of tasks. However, when a new task is presented to an\nagent, we expect it to be able to reuse some of what it knows from previous\ntasks to rapidly learn that new task. We introduce a novel technique whereby\npolicies for different a priori known tasks are combined into a\nMixture-of-Experts model with an attention mechanism across a mix of frozen and\nunfrozen experts. The model learns when to attend to frozen task-specific\nexperts when appropriate and learns new experts to handle novel situations. We\nwork in an open-ended text-based environment in which the agent is tasked with\nbehaving like different types of character roles and must rapidly learn\nbehaviors associated with new character role types. We show that our agent both\nobtains more rewards in the zero-shot setting, and discovers these rewards with\ngreater sample efficiency in the few-shot learning settings.",
      "tldr_zh": "该研究针对开放式文本世界（open-ended text worlds），提出了一种Mixture-of-Experts方法，用于实现少样本任务转移（few-shot task transfer），允许代理重用先前任务知识快速适应新任务。方法将不同任务的政策整合成一个混合专家模型，结合注意力机制和冻结（frozen）与非冻结（unfrozen）专家，模型学会在适当情况下调用特定专家并学习新专家处理新情境。在文本环境实验中，该代理在零样本（zero-shot）设置中获得更多奖励，并在少样本学习（few-shot learning）设置中表现出更高的样本效率。总的来说，这一方法提升了代理在多任务环境中的适应性和学习能力。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2405.06059v1",
      "published_date": "2024-05-09 19:02:56 UTC",
      "updated_date": "2024-05-09 19:02:56 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T07:14:38.594902"
    },
    {
      "arxiv_id": "2405.06058v2",
      "title": "Large Language Models Show Human-like Social Desirability Biases in Survey Responses",
      "title_zh": "大语言模型在调查响应中表现出类似人类的社交期望偏差",
      "authors": [
        "Aadesh Salecha",
        "Molly E. Ireland",
        "Shashanka Subrahmanya",
        "João Sedoc",
        "Lyle H. Ungar",
        "Johannes C. Eichstaedt"
      ],
      "abstract": "As Large Language Models (LLMs) become widely used to model and simulate\nhuman behavior, understanding their biases becomes critical. We developed an\nexperimental framework using Big Five personality surveys and uncovered a\npreviously undetected social desirability bias in a wide range of LLMs. By\nsystematically varying the number of questions LLMs were exposed to, we\ndemonstrate their ability to infer when they are being evaluated. When\npersonality evaluation is inferred, LLMs skew their scores towards the\ndesirable ends of trait dimensions (i.e., increased extraversion, decreased\nneuroticism, etc). This bias exists in all tested models, including GPT-4/3.5,\nClaude 3, Llama 3, and PaLM-2. Bias levels appear to increase in more recent\nmodels, with GPT-4's survey responses changing by 1.20 (human) standard\ndeviations and Llama 3's by 0.98 standard deviations-very large effects. This\nbias is robust to randomization of question order and paraphrasing.\nReverse-coding all the questions decreases bias levels but does not eliminate\nthem, suggesting that this effect cannot be attributed to acquiescence bias.\nOur findings reveal an emergent social desirability bias and suggest\nconstraints on profiling LLMs with psychometric tests and on using LLMs as\nproxies for human participants.",
      "tldr_zh": "本研究发现大型语言模型（LLMs）在回应调查时表现出类似于人类的社交期望偏差（social desirability bias），通过Big Five个性调查框架系统地改变问题数量来揭示这一现象。结果显示，当LLMs推断出自身被评估时，它们会调整回应朝向更可取的特质（如增加extraversion、减少neuroticism），这种偏差在多种模型（包括GPT-4/3.5、Claude 3、Llama 3和PaLM-2）中普遍存在，且在较新模型中更显著，例如GPT-4的变化达1.20人类标准偏差。研究还证实该偏差对问题顺序随机化和改述robust，反向编码问题虽可减轻但无法消除偏差，强调了在用LLMs进行心理测量测试或作为人类代理时的潜在限制。",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.CY",
        "cs.HC"
      ],
      "primary_category": "cs.AI",
      "comment": "3 pages, 2 figures, accepted at PNAS Nexus",
      "pdf_url": "http://arxiv.org/pdf/2405.06058v2",
      "published_date": "2024-05-09 19:02:53 UTC",
      "updated_date": "2024-11-21 21:39:04 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T07:14:52.877223"
    },
    {
      "arxiv_id": "2405.06038v1",
      "title": "From Algorithm to Hardware: A Survey on Efficient and Safe Deployment of Deep Neural Networks",
      "title_zh": "从算法到硬件：深度神经网络高效和安全部署的调查",
      "authors": [
        "Xue Geng",
        "Zhe Wang",
        "Chunyun Chen",
        "Qing Xu",
        "Kaixin Xu",
        "Chao Jin",
        "Manas Gupta",
        "Xulei Yang",
        "Zhenghua Chen",
        "Mohamed M. Sabry Aly",
        "Jie Lin",
        "Min Wu",
        "Xiaoli Li"
      ],
      "abstract": "Deep neural networks (DNNs) have been widely used in many artificial\nintelligence (AI) tasks. However, deploying them brings significant challenges\ndue to the huge cost of memory, energy, and computation. To address these\nchallenges, researchers have developed various model compression techniques\nsuch as model quantization and model pruning. Recently, there has been a surge\nin research of compression methods to achieve model efficiency while retaining\nthe performance. Furthermore, more and more works focus on customizing the DNN\nhardware accelerators to better leverage the model compression techniques. In\naddition to efficiency, preserving security and privacy is critical for\ndeploying DNNs. However, the vast and diverse body of related works can be\noverwhelming. This inspires us to conduct a comprehensive survey on recent\nresearch toward the goal of high-performance, cost-efficient, and safe\ndeployment of DNNs. Our survey first covers the mainstream model compression\ntechniques such as model quantization, model pruning, knowledge distillation,\nand optimizations of non-linear operations. We then introduce recent advances\nin designing hardware accelerators that can adapt to efficient model\ncompression approaches. Additionally, we discuss how homomorphic encryption can\nbe integrated to secure DNN deployment. Finally, we discuss several issues,\nsuch as hardware evaluation, generalization, and integration of various\ncompression approaches. Overall, we aim to provide a big picture of efficient\nDNNs, from algorithm to hardware accelerators and security perspectives.",
      "tldr_zh": "这篇调查论文探讨了深度神经网络（DNNs）的高效和安全部署，从算法到硬件的全流程。论文总结了主流模型压缩技术，包括model quantization、model pruning、knowledge distillation以及非线性操作优化，以降低内存、能量和计算成本，同时保持性能。论文还介绍了硬件加速器的最新进展，以及如何整合homomorphic encryption来提升安全性，并讨论了硬件评估、泛化和多种压缩方法整合的挑战，为高效、安全的DNNs部署提供了全面概述。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "This manuscript is the accepted version for TNNLS(IEEE Transactions\n  on Neural Networks and Learning Systems)",
      "pdf_url": "http://arxiv.org/pdf/2405.06038v1",
      "published_date": "2024-05-09 18:17:25 UTC",
      "updated_date": "2024-05-09 18:17:25 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T07:15:01.489142"
    },
    {
      "arxiv_id": "2406.07561v1",
      "title": "Artificial Intelligence as the New Hacker: Developing Agents for Offensive Security",
      "title_zh": "翻译失败",
      "authors": [
        "Leroy Jacob Valencia"
      ],
      "abstract": "In the vast domain of cybersecurity, the transition from reactive defense to\noffensive has become critical in protecting digital infrastructures. This paper\nexplores the integration of Artificial Intelligence (AI) into offensive\ncybersecurity, particularly through the development of an autonomous AI agent,\nReaperAI, designed to simulate and execute cyberattacks. Leveraging the\ncapabilities of Large Language Models (LLMs) such as GPT-4, ReaperAI\ndemonstrates the potential to identify, exploit, and analyze security\nvulnerabilities autonomously.\n  This research outlines the core methodologies that can be utilized to\nincrease consistency and performance, including task-driven penetration testing\nframeworks, AI-driven command generation, and advanced prompting techniques.\nThe AI agent operates within a structured environment using Python, enhanced by\nRetrieval Augmented Generation (RAG) for contextual understanding and memory\nretention. ReaperAI was tested on platforms including, Hack The Box, where it\nsuccessfully exploited known vulnerabilities, demonstrating its potential\npower.\n  However, the deployment of AI in offensive security presents significant\nethical and operational challenges. The agent's development process revealed\ncomplexities in command execution, error handling, and maintaining ethical\nconstraints, highlighting areas for future enhancement.\n  This study contributes to the discussion on AI's role in cybersecurity by\nshowcasing how AI can augment offensive security strategies. It also proposes\nfuture research directions, including the refinement of AI interactions with\ncybersecurity tools, enhancement of learning mechanisms, and the discussion of\nethical guidelines for AI in offensive roles. The findings advocate for a\nunique approach to AI implementation in cybersecurity, emphasizing innovation.",
      "tldr_zh": "这篇论文探讨了人工智能（AI）在进攻性网络安全中的应用，开发了一个自主AI代理ReaperAI，用于模拟和执行网络攻击，以识别、利用和分析安全漏洞。研究利用Large Language Models（LLMs）如GPT-4、任务驱动的渗透测试框架、AI驱动命令生成以及Retrieval Augmented Generation（RAG）技术，使代理在Python环境中实现高效的上下文理解和记忆保留。在Hack The Box等平台上的测试中，ReaperAI成功利用了已知漏洞，但也暴露了命令执行、错误处理和伦理约束等挑战。该研究强调AI增强进攻性安全策略的潜力，并提出未来方向，包括优化AI与工具交互、提升学习机制以及制定伦理指南。",
      "categories": [
        "cs.CR",
        "cs.AI"
      ],
      "primary_category": "cs.CR",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2406.07561v1",
      "published_date": "2024-05-09 18:15:12 UTC",
      "updated_date": "2024-05-09 18:15:12 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T07:15:15.187780"
    },
    {
      "arxiv_id": "2405.05966v4",
      "title": "Natural Language Processing RELIES on Linguistics",
      "title_zh": "翻译失败",
      "authors": [
        "Juri Opitz",
        "Shira Wein",
        "Nathan Schneider"
      ],
      "abstract": "Large Language Models (LLMs) have become capable of generating highly fluent\ntext in certain languages, without modules specially designed to capture\ngrammar or semantic coherence. What does this mean for the future of linguistic\nexpertise in NLP? We highlight several aspects in which NLP (still) relies on\nlinguistics, or where linguistic thinking can illuminate new directions. We\nargue our case around the acronym RELIES that encapsulates six major facets\nwhere linguistics contributes to NLP: Resources, Evaluation, Low-resource\nsettings, Interpretability, Explanation, and the Study of language. This list\nis not exhaustive, nor is linguistics the main point of reference for every\neffort under these themes; but at a macro level, these facets highlight the\nenduring importance of studying machine systems vis-\\`a-vis systems of human\nlanguage.",
      "tldr_zh": "这篇论文探讨了大型语言模型（LLMs）在生成流畅文本时虽不依赖特定语法或语义模块，但自然语言处理（NLP）仍需依赖语言学的论点。作者围绕 RELIES 首字母缩写，阐述了语言学在 NLP 中的六个关键贡献：Resources（资源）、Evaluation（评估）、Low-resource settings（低资源设置）、Interpretability（可解释性）、Explanation（解释）和 the Study of language（语言研究）。论文强调，这些方面突出了语言学在研究机器系统与人类语言系统关系中的持续重要性，从而为 NLP 的未来发展提供新的启示。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "To appear in Computational Linguistics. This is a pre-MIT Press\n  publication version",
      "pdf_url": "http://arxiv.org/pdf/2405.05966v4",
      "published_date": "2024-05-09 17:59:32 UTC",
      "updated_date": "2025-03-10 15:07:49 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T07:15:25.746564"
    },
    {
      "arxiv_id": "2405.05959v2",
      "title": "Self-Supervised Learning of Time Series Representation via Diffusion Process and Imputation-Interpolation-Forecasting Mask",
      "title_zh": "翻译失败",
      "authors": [
        "Zineb Senane",
        "Lele Cao",
        "Valentin Leonhard Buchner",
        "Yusuke Tashiro",
        "Lei You",
        "Pawel Herman",
        "Mats Nordahl",
        "Ruibo Tu",
        "Vilhelm von Ehrenheim"
      ],
      "abstract": "Time Series Representation Learning (TSRL) focuses on generating informative\nrepresentations for various Time Series (TS) modeling tasks. Traditional\nSelf-Supervised Learning (SSL) methods in TSRL fall into four main categories:\nreconstructive, adversarial, contrastive, and predictive, each with a common\nchallenge of sensitivity to noise and intricate data nuances. Recently,\ndiffusion-based methods have shown advanced generative capabilities. However,\nthey primarily target specific application scenarios like imputation and\nforecasting, leaving a gap in leveraging diffusion models for generic TSRL. Our\nwork, Time Series Diffusion Embedding (TSDE), bridges this gap as the first\ndiffusion-based SSL TSRL approach. TSDE segments TS data into observed and\nmasked parts using an Imputation-Interpolation-Forecasting (IIF) mask. It\napplies a trainable embedding function, featuring dual-orthogonal Transformer\nencoders with a crossover mechanism, to the observed part. We train a reverse\ndiffusion process conditioned on the embeddings, designed to predict noise\nadded to the masked part. Extensive experiments demonstrate TSDE's superiority\nin imputation, interpolation, forecasting, anomaly detection, classification,\nand clustering. We also conduct an ablation study, present embedding\nvisualizations, and compare inference speed, further substantiating TSDE's\nefficiency and validity in learning representations of TS data.",
      "tldr_zh": "该论文提出了一种新的自监督学习(Self-Supervised Learning, SSL)方法Time Series Diffusion Embedding (TSDE)，旨在通过扩散过程(Diffusion Process)和Imputation-Interpolation-Forecasting (IIF)掩码来学习时间序列(Time Series)表示，解决传统SSL方法对噪声和数据细微差异的敏感问题。TSDE将时间序列数据分为观察部分和掩盖部分，对观察部分应用双正交Transformer编码器及其交叉机制生成嵌入，然后训练逆扩散过程以预测掩盖部分的噪声。实验结果显示，TSDE在插值、预测、异常检测、分类和聚类等任务上优于现有方法，并通过消融研究和嵌入可视化证明了其效率和有效性。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "G.3; I.6.5; I.2.4"
      ],
      "primary_category": "cs.LG",
      "comment": "Published as a full paper by KDD 2024 Research Track (12 pages as\n  main paper and 11 pages as appendix). Source code available at\n  https://github.com/llcresearch/TSDE",
      "pdf_url": "http://arxiv.org/pdf/2405.05959v2",
      "published_date": "2024-05-09 17:55:16 UTC",
      "updated_date": "2024-06-17 08:54:51 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T07:15:38.557616"
    },
    {
      "arxiv_id": "2405.05950v1",
      "title": "Federated Combinatorial Multi-Agent Multi-Armed Bandits",
      "title_zh": "翻译失败",
      "authors": [
        "Fares Fourati",
        "Mohamed-Slim Alouini",
        "Vaneet Aggarwal"
      ],
      "abstract": "This paper introduces a federated learning framework tailored for online\ncombinatorial optimization with bandit feedback. In this setting, agents select\nsubsets of arms, observe noisy rewards for these subsets without accessing\nindividual arm information, and can cooperate and share information at specific\nintervals. Our framework transforms any offline resilient single-agent\n$(\\alpha-\\epsilon)$-approximation algorithm, having a complexity of\n$\\tilde{\\mathcal{O}}(\\frac{\\psi}{\\epsilon^\\beta})$, where the logarithm is\nomitted, for some function $\\psi$ and constant $\\beta$, into an online\nmulti-agent algorithm with $m$ communicating agents and an $\\alpha$-regret of\nno more than $\\tilde{\\mathcal{O}}(m^{-\\frac{1}{3+\\beta}} \\psi^\\frac{1}{3+\\beta}\nT^\\frac{2+\\beta}{3+\\beta})$. This approach not only eliminates the $\\epsilon$\napproximation error but also ensures sublinear growth with respect to the time\nhorizon $T$ and demonstrates a linear speedup with an increasing number of\ncommunicating agents. Additionally, the algorithm is notably\ncommunication-efficient, requiring only a sublinear number of communication\nrounds, quantified as $\\tilde{\\mathcal{O}}\\left(\\psi\nT^\\frac{\\beta}{\\beta+1}\\right)$. Furthermore, the framework has been\nsuccessfully applied to online stochastic submodular maximization using various\noffline algorithms, yielding the first results for both single-agent and\nmulti-agent settings and recovering specialized single-agent theoretical\nguarantees. We empirically validate our approach to a stochastic data\nsummarization problem, illustrating the effectiveness of the proposed\nframework, even in single-agent scenarios.",
      "tldr_zh": "这篇论文提出了一种联邦学习框架，用于在线组合优化问题中的多代理多臂赌博机（Multi-Armed Bandits），其中代理选择arms子集、观察噪声奖励，并在特定间隔共享信息。框架将任何离线弹性单代理（α-ε）近似算法转换为在线多代理算法，实现α-regret不超过 ~O(m^{-1/(3+β)} ψ^{1/(3+β)} T^{(2+β)/(3+β)})，并消除ε近似误差，同时随代理数m线性加速和随时间T子线性增长。算法还具有高效通信，仅需 ~O(ψ T^{β/(β+1)}) 轮次，并成功应用于在线随机子模块最大化（submodular maximization），提供了首个单代理和多代理理论结果，并通过随机数据总结问题的实证验证证明了其有效性。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.DM",
        "cs.MA",
        "stat.ML"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2405.05950v1",
      "published_date": "2024-05-09 17:40:09 UTC",
      "updated_date": "2024-05-09 17:40:09 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T07:15:51.567110"
    },
    {
      "arxiv_id": "2407.03332v1",
      "title": "DDPM-MoCo: Advancing Industrial Surface Defect Generation and Detection with Generative and Contrastive Learning",
      "title_zh": "翻译失败",
      "authors": [
        "Yangfan He",
        "Xinyan Wang",
        "Tianyu Shi"
      ],
      "abstract": "The task of industrial detection based on deep learning often involves\nsolving two problems: (1) obtaining sufficient and effective data samples, (2)\nand using efficient and convenient model training methods. In this paper, we\nintroduce a novel defect-generation method, named DDPM-MoCo, to address these\nissues. Firstly, we utilize the Denoising Diffusion Probabilistic Model (DDPM)\nto generate high-quality defect data samples, overcoming the problem of\ninsufficient sample data for model learning. Furthermore, we utilize the\nunsupervised learning Momentum Contrast model (MoCo) with an enhanced batch\ncontrastive loss function for training the model on unlabeled data, addressing\nthe efficiency and consistency challenges in large-scale negative sample\nencoding during diffusion model training. The experimental results showcase an\nenhanced visual detection method for identifying defects on metal surfaces,\ncovering the entire process, starting from generating unlabeled sample data for\ntraining the diffusion model, to utilizing the same labeled sample data for\ndownstream detection tasks. This study offers valuable practical insights and\napplication potential for visual detection in the metal processing industry.",
      "tldr_zh": "这篇论文提出了 DDPM-MoCo 方法，用于解决工业表面缺陷检测中的数据不足和模型训练效率问题。方法首先利用 Denoising Diffusion Probabilistic Model (DDPM) 生成高质量的缺陷数据样本，以增强模型学习；其次，结合 Momentum Contrast model (MoCo) 和改进的批量对比损失函数，进行无监督学习训练，提高了大规模负样本编码的一致性。实验结果显示，该方法在金属表面缺陷检测任务上实现了显著改进，从数据生成到下游检测的全流程，为金属加工行业的视觉检测提供了实际应用价值和洞见。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2407.03332v1",
      "published_date": "2024-05-09 17:17:53 UTC",
      "updated_date": "2024-05-09 17:17:53 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T07:16:02.216399"
    },
    {
      "arxiv_id": "2405.05930v2",
      "title": "Trustworthy AI-Generative Content for Intelligent Network Service: Robustness, Security, and Fairness",
      "title_zh": "翻译失败",
      "authors": [
        "Siyuan Li",
        "Xi Lin",
        "Yaju Liu",
        "Xiang Chen",
        "Jianhua Li"
      ],
      "abstract": "AI-generated content (AIGC) models, represented by large language models\n(LLM), have revolutionized content creation. High-speed next-generation\ncommunication technology is an ideal platform for providing powerful AIGC\nnetwork services. At the same time, advanced AIGC techniques can also make\nfuture network services more intelligent, especially various online content\ngeneration services. However, the significant untrustworthiness concerns of\ncurrent AIGC models, such as robustness, security, and fairness, greatly affect\nthe credibility of intelligent network services, especially in ensuring secure\nAIGC services. This paper proposes TrustGAIN, a trustworthy AIGC framework that\nincorporates robust, secure, and fair network services. We first discuss the\nrobustness to adversarial attacks faced by AIGC models in network systems and\nthe corresponding protection issues. Subsequently, we emphasize the importance\nof avoiding unsafe and illegal services and ensuring the fairness of the AIGC\nnetwork services. Then as a case study, we propose a novel sentiment\nanalysis-based detection method to guide the robust detection of unsafe content\nin network services. We conduct our experiments on fake news, malicious code,\nand unsafe review datasets to represent LLM application scenarios. Our results\nindicate that TrustGAIN is an exploration of future networks that can support\ntrustworthy AIGC network services.",
      "tldr_zh": "这篇论文探讨了AI-Generative Content (AIGC)模型（如large language models, LLM）在智能网络服务中的可信度问题，重点关注robustness、安全性和fairness，以应对对抗攻击和不安全内容风险。论文提出TrustGAIN框架，通过强化鲁棒性保护、确保服务公平性和避免非法内容的方法，来提升网络服务的可靠性。作为案例研究，作者开发了一种基于情感分析的检测方法，并在假新闻、恶意代码和不安全评论数据集上进行实验，结果显示TrustGAIN为构建可信赖的AIGC网络服务提供了重要探索。",
      "categories": [
        "cs.CR",
        "cs.AI",
        "cs.NI"
      ],
      "primary_category": "cs.CR",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2405.05930v2",
      "published_date": "2024-05-09 17:16:20 UTC",
      "updated_date": "2025-02-27 08:09:23 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T07:16:15.143307"
    },
    {
      "arxiv_id": "2405.05925v3",
      "title": "FuXi-ENS: A machine learning model for medium-range ensemble weather forecasting",
      "title_zh": "FuXi-ENS：一种用于中程集合天气预报的机器学习模型",
      "authors": [
        "Xiaohui Zhong",
        "Lei Chen",
        "Hao Li",
        "Jun Liu",
        "Xu Fan",
        "Jie Feng",
        "Kan Dai",
        "Jing-Jia Luo",
        "Jie Wu",
        "Bo Lu"
      ],
      "abstract": "Ensemble forecasting is crucial for improving weather predictions, especially\nfor forecasts of extreme events. Constructing an ensemble prediction system\n(EPS) based on conventional NWP models is highly computationally expensive. ML\nmodels have emerged as valuable tools for deterministic weather forecasts,\nproviding forecasts with significantly reduced computational requirements and\neven surpassing the forecast performance of traditional NWP models. However,\nchallenges arise when applying ML models to ensemble forecasting. Recent ML\nmodels, such as GenCast and SEEDS model, rely on the ERA5 EDA or operational\nNWP ensemble members for forecast generation. Their spatial resolution is also\nconsidered too coarse for many applications. To overcome these limitations, we\nintroduce FuXi-ENS, an advanced ML model designed to deliver 6-hourly global\nensemble weather forecasts up to 15 days. This model runs at a significantly\nincreased spatial resolution of 0.25\\textdegree, incorporating 5 atmospheric\nvariables at 13 pressure levels, along with 13 surface variables. By leveraging\nthe inherent probabilistic nature of Variational AutoEncoder (VAE), FuXi-ENS\noptimizes a loss function that combines the CRPS and the KL divergence between\nthe predicted and target distribution, facilitating the incorporation of\nflow-dependent perturbations in both initial conditions and forecast. This\ninnovative approach makes FuXi-ENS an advancement over the traditional ones\nthat use L1 loss combined with the KL loss in standard VAE models for ensemble\nweather forecasting. Results demonstrate that FuXi-ENS outperforms ensemble\nforecasts from the ECMWF, a world leading NWP model, in the CRPS of 98.1% of\n360 variable and forecast lead time combinations. This achievement underscores\nthe potential of the FuXi-ENS model to enhance ensemble weather forecasts,\noffering a promising direction for further development in this field.",
      "tldr_zh": "该研究引入 FuXi-ENS，一种先进的机器学习模型，用于中程集合天气预报，以克服传统 NWP 模型的计算开销大和分辨率不足等问题。FuXi-ENS 提供高达 15 天的全球 6 小时预报，分辨率为 0.25°，并整合 5 个大气变量（13 个压力层）和 13 个地表变量，通过 Variational AutoEncoder (VAE) 的概率特性优化了结合 CRPS 和 KL divergence 的损失函数，实现流依赖扰动。相比标准 VAE 模型的 L1 损失方法，该创新显著提升了预报准确性。结果显示，FuXi-ENS 在 98.1% 的变量和预报时段组合中优于领先的 ECMWF 模型，为 ML 在集合天气预报领域的进一步发展提供了新方向。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "physics.ao-ph"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2405.05925v3",
      "published_date": "2024-05-09 17:15:09 UTC",
      "updated_date": "2024-08-09 05:14:05 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T07:16:27.672597"
    },
    {
      "arxiv_id": "2405.05908v4",
      "title": "Multimodal Super-Resolution: Discovering hidden physics and its application to fusion plasmas",
      "title_zh": "多模态超分辨率：发现隐藏的物理及其在聚变等离子体中的应用",
      "authors": [
        "Azarakhsh Jalalvand",
        "SangKyeun Kim",
        "Jaemin Seo",
        "Qiming Hu",
        "Max Curie",
        "Peter Steiner",
        "Andrew Oakleigh Nelson",
        "Yong-Su Na",
        "Egemen Kolemen"
      ],
      "abstract": "A non-linear system governed by multi-spatial and multi-temporal physics\nscales cannot be fully understood with a single diagnostic, as each provides\nonly a partial view, leading to information loss. Combining multiple\ndiagnostics may also result in incomplete projections of the system's physics.\nBy identifying hidden inter-correlations between diagnostics, we can leverage\nmutual support to fill in these gaps, but uncovering such correlations\nanalytically is too complex. We introduce a machine learning methodology to\naddress this issue. Unlike traditional methods, our multimodal approach does\nnot rely on the target diagnostic's direct measurements to generate its\nsuper-resolution version. Instead, it uses other diagnostics to produce\nsuper-resolution data, capturing detailed structural evolution and responses to\nperturbations previously unobservable. This not only enhances the resolution of\na diagnostic for deeper insights but also reconstructs the target diagnostic,\nproviding a valuable tool to mitigate diagnostic failure. This methodology\naddresses a key challenge in fusion plasmas: the Edge Localized Mode (ELM), a\nplasma instability that can cause significant erosion of plasma-facing\nmaterials. A method to stabilize ELM is using resonant magnetic perturbation\n(RMP) to trigger magnetic islands. However, limited spatial and temporal\nresolution restricts analysis of these islands due to their small size, rapid\ndynamics, and complex plasma interactions. With super-resolution diagnostics,\nwe can experimentally verify theoretical models of magnetic islands for the\nfirst time, providing insights into their role in ELM stabilization. This\nadvancement supports the development of effective ELM suppression strategies\nfor future fusion reactors like ITER and has broader applications, potentially\nrevolutionizing diagnostics in fields such as astronomy, astrophysics, and\nmedical imaging.",
      "tldr_zh": "这篇论文提出了一种多模态超分辨率方法，通过机器学习识别诊断工具之间的隐藏互相关联，生成不依赖目标诊断直接测量的超分辨率数据，从而填补信息空白并捕捉系统的详细结构演化和对扰动的响应。核心贡献在于提升诊断精度，提供工具缓解诊断失败，并应用于聚变等离子体中的Edge Localized Mode (ELM)，利用resonant magnetic perturbation (RMP)触发磁岛以验证其理论模型。实验结果首次实现了对磁岛的实验验证，支持ELM稳定策略的开发，并扩展到天文学、医学成像等领域，为未来聚变反应堆如ITER带来潜在革命性影响。",
      "categories": [
        "physics.plasm-ph",
        "cs.AI"
      ],
      "primary_category": "physics.plasm-ph",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2405.05908v4",
      "published_date": "2024-05-09 17:06:51 UTC",
      "updated_date": "2024-11-05 18:17:06 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T07:16:39.121737"
    },
    {
      "arxiv_id": "2405.05905v5",
      "title": "Truthful Aggregation of LLMs with an Application to Online Advertising",
      "title_zh": "翻译失败",
      "authors": [
        "Ermis Soumalias",
        "Michael J. Curry",
        "Sven Seuken"
      ],
      "abstract": "The next frontier of online advertising is revenue generation from\nLLM-generated content. We consider a setting where advertisers aim to influence\nthe responses of an LLM to align with their interests, while platforms seek to\nmaximize advertiser value and ensure user satisfaction. The challenge is that\nadvertisers' preferences generally conflict with those of the user, and\nadvertisers may misreport their preferences. To address this, we introduce\nMOSAIC, an auction mechanism that ensures that truthful reporting is a dominant\nstrategy for advertisers and that aligns the utility of each advertiser with\ntheir contribution to social welfare. Importantly, the mechanism operates\nwithout LLM fine-tuning or access to model weights and provably converges to\nthe output of the optimally fine-tuned LLM as computational resources increase.\nAdditionally, it can incorporate contextual information about advertisers,\nwhich significantly improves social welfare. Through experiments with a\npublicly available LLM, we show that MOSAIC leads to high advertiser value and\nplatform revenue with low computational overhead. While our motivating\napplication is online advertising, our mechanism can be applied in any setting\nwith monetary transfers, making it a general-purpose solution for truthfully\naggregating the preferences of self-interested agents over LLM-generated\nreplies.",
      "tldr_zh": "该研究探讨了在线广告中利用大型语言模型（LLMs）生成内容以产生收入的挑战，特别是广告商可能虚假报告偏好以影响模型响应，而平台需平衡广告商价值和用户满意度。为解决此问题，研究引入了MOSAIC拍卖机制，该机制确保广告商真实报告是占优策略，并将广告商效用与社会福利贡献对齐，同时无需LLMs微调或访问模型权重，且能整合上下文信息以提升整体福利。实验结果显示，MOSAIC在公开LLMs上实现了高广告商价值和平台收入，同时保持低计算开销，并证明随着计算资源增加，它会收敛到最佳微调模型的输出。作为一个通用解决方案，该机制适用于任何涉及货币转移的场景，用于真实聚合自利代理对LLMs生成响应的偏好。",
      "categories": [
        "cs.GT",
        "cs.AI"
      ],
      "primary_category": "cs.GT",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2405.05905v5",
      "published_date": "2024-05-09 17:01:31 UTC",
      "updated_date": "2025-02-12 17:10:53 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T07:16:50.344139"
    },
    {
      "arxiv_id": "2405.05890v1",
      "title": "Safe Exploration Using Bayesian World Models and Log-Barrier Optimization",
      "title_zh": "基于贝叶斯世界模型和对数屏障优化的安全探索",
      "authors": [
        "Yarden As",
        "Bhavya Sukhija",
        "Andreas Krause"
      ],
      "abstract": "A major challenge in deploying reinforcement learning in online tasks is\nensuring that safety is maintained throughout the learning process. In this\nwork, we propose CERL, a new method for solving constrained Markov decision\nprocesses while keeping the policy safe during learning. Our method leverages\nBayesian world models and suggests policies that are pessimistic w.r.t. the\nmodel's epistemic uncertainty. This makes CERL robust towards model\ninaccuracies and leads to safe exploration during learning. In our experiments,\nwe demonstrate that CERL outperforms the current state-of-the-art in terms of\nsafety and optimality in solving CMDPs from image observations.",
      "tldr_zh": "该论文提出 CERL 方法，用于在强化学习中解决 constrained Markov decision processes (CMDPs)，以确保政策在学习过程中保持安全。方法结合 Bayesian world models 和对模型的不确定性保持悲观策略，实现稳健的探索，避免潜在风险。在实验中，CERL 在从图像观察处理 CMDPs 时，显著超越现有状态-of-the-art 方法，在安全性和最优性方面表现出色。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2405.05890v1",
      "published_date": "2024-05-09 16:42:39 UTC",
      "updated_date": "2024-05-09 16:42:39 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T07:17:02.044661"
    },
    {
      "arxiv_id": "2405.06715v1",
      "title": "Enhancing Creativity in Large Language Models through Associative Thinking Strategies",
      "title_zh": "通过联想思维策略增强大型语言模型的创造力",
      "authors": [
        "Pronita Mehrotra",
        "Aishni Parab",
        "Sumit Gulwani"
      ],
      "abstract": "This paper explores the enhancement of creativity in Large Language Models\n(LLMs) like vGPT-4 through associative thinking, a cognitive process where\ncreative ideas emerge from linking seemingly unrelated concepts. Associative\nthinking strategies have been found to effectively help humans boost\ncreativity. However, whether the same strategies can help LLMs become more\ncreative remains under-explored. In this work, we investigate whether prompting\nLLMs to connect disparate concepts can augment their creative outputs. Focusing\non three domains -- Product Design, Storytelling, and Marketing -- we introduce\ncreativity tasks designed to assess vGPT-4's ability to generate original and\nuseful content. By challenging the models to form novel associations, we\nevaluate the potential of associative thinking to enhance the creative\ncapabilities of LLMs. Our findings show that leveraging associative thinking\ntechniques can significantly improve the originality of vGPT-4's responses.",
      "tldr_zh": "本文研究了通过联想思维(associative thinking)策略提升大型语言模型(LLMs)如vGPT-4的创造力，探索是否能像人类一样通过连接不相关概念来生成更原创的内容。研究在产品设计、故事讲述和营销三个领域设计了特定任务，提示模型形成新颖的关联以评估其创造力表现。结果表明，这种策略显著提高了vGPT-4响应的原创性，为增强LLMs的创新能力提供了新途径。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2405.06715v1",
      "published_date": "2024-05-09 16:42:29 UTC",
      "updated_date": "2024-05-09 16:42:29 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T07:17:13.941644"
    },
    {
      "arxiv_id": "2405.06004v2",
      "title": "EWMoE: An effective model for global weather forecasting with mixture-of-experts",
      "title_zh": "翻译失败",
      "authors": [
        "Lihao Gan",
        "Xin Man",
        "Chenghong Zhang",
        "Jie Shao"
      ],
      "abstract": "Weather forecasting is a crucial task for meteorologic research, with direct\nsocial and economic impacts. Recently, data-driven weather forecasting models\nbased on deep learning have shown great potential, achieving superior\nperformance compared with traditional numerical weather prediction methods.\nHowever, these models often require massive training data and computational\nresources. In this paper, we propose EWMoE, an effective model for accurate\nglobal weather forecasting, which requires significantly less training data and\ncomputational resources. Our model incorporates three key components to enhance\nprediction accuracy: 3D absolute position embedding, a core Mixture-of-Experts\n(MoE) layer, and two specific loss functions. We conduct our evaluation on the\nERA5 dataset using only two years of training data. Extensive experiments\ndemonstrate that EWMoE outperforms current models such as FourCastNet and\nClimaX at all forecast time, achieving competitive performance compared with\nthe state-of-the-art models Pangu-Weather and GraphCast in evaluation metrics\nsuch as Anomaly Correlation Coefficient (ACC) and Root Mean Square Error\n(RMSE). Additionally, ablation studies indicate that applying the MoE\narchitecture to weather forecasting offers significant advantages in improving\naccuracy and resource efficiency. Code is available at\nhttps://github.com/Tomoyi/EWMoE.",
      "tldr_zh": "该论文提出 EWMoE 模型，一种基于 Mixture-of-Experts (MoE) 架构的有效全球天气预报方法，能够显著减少训练数据和计算资源需求。EWMoE 整合了 3D absolute position embedding、核心 MoE 层以及两个特定 loss functions，以提升预测准确性。在 ERA5 数据集上仅使用两年训练数据进行评估，实验结果显示 EWMoE 在所有预测时间上优于 FourCastNet 和 ClimaX，并在 Anomaly Correlation Coefficient (ACC) 和 Root Mean Square Error (RMSE) 等指标上与 Pangu-Weather 和 GraphCast 相当。消融研究进一步证明，MoE 架构在提高准确性和资源效率方面具有显著优势。",
      "categories": [
        "physics.ao-ph",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "physics.ao-ph",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2405.06004v2",
      "published_date": "2024-05-09 16:42:13 UTC",
      "updated_date": "2024-08-23 05:30:55 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T07:17:27.714913"
    },
    {
      "arxiv_id": "2405.06714v2",
      "title": "Towards a Path Dependent Account of Category Fluency",
      "title_zh": "翻译失败",
      "authors": [
        "David Heineman",
        "Reba Koenen",
        "Sashank Varma"
      ],
      "abstract": "Category fluency is a widely studied cognitive phenomenon, yet two\nconflicting accounts have been proposed as the underlying retrieval mechanism\n-- an optimal foraging process deliberately searching through memory (Hills et\nal., 2012) and a random walk sampling from a semantic network (Abbott et al.,\n2015). Evidence for both accounts has centered around predicting human patch\nswitches, where both existing models of category fluency produce paradoxically\nidentical results. We begin by peeling back the assumptions made by existing\nmodels, namely that each named example only depends on the previous example, by\n(i) adding an additional bias to model the category transition probability\ndirectly and (ii) relying on a large language model to predict based on the\nentire existing sequence. Then, we present evidence towards resolving the\ndisagreement between each account of foraging by reformulating models as\nsequence generators. To evaluate, we compare generated category fluency runs to\na bank of human-written sequences by proposing a metric based on n-gram\noverlap. We find category switch predictors do not necessarily produce\nhuman-like sequences, in fact the additional biases used by the Hills et al.\n(2012) model are required to improve generation quality, which are later\nimproved by our category modification. Even generating exclusively with an LLM\nrequires an additional global cue to trigger the patch switching behavior\nduring production. Further tests on only the search process on top of the\nsemantic network highlight the importance of deterministic search to replicate\nhuman behavior.",
      "tldr_zh": "本研究探讨了类别流畅性（category fluency）的认知机制，针对两种冲突理论——最佳觅食过程（optimal foraging process，由Hills et al., 2012提出）和随机游走采样（random walk sampling，由Abbott et al., 2015提出）——通过重新构建模型为序列生成器来寻求解决。研究方法包括添加类别转换偏置和利用大型语言模型（LLM）基于整个序列进行预测，并提出基于n-gram重叠的指标来评估生成序列与人类序列的相似度。结果显示，仅靠类别切换预测器无法产生人类-like序列，而Hills et al.模型的偏置能显著改善生成质量，且LLM生成需额外全局线索触发路径切换行为。最终，论文强调了确定性搜索（deterministic search）在复制人类行为中的关键作用，为理解类别流畅性提供了新证据。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "To appear at CogSci 2024",
      "pdf_url": "http://arxiv.org/pdf/2405.06714v2",
      "published_date": "2024-05-09 16:36:56 UTC",
      "updated_date": "2024-05-14 02:10:05 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T07:17:40.736700"
    },
    {
      "arxiv_id": "2405.05876v1",
      "title": "Composable Part-Based Manipulation",
      "title_zh": "翻译失败",
      "authors": [
        "Weiyu Liu",
        "Jiayuan Mao",
        "Joy Hsu",
        "Tucker Hermans",
        "Animesh Garg",
        "Jiajun Wu"
      ],
      "abstract": "In this paper, we propose composable part-based manipulation (CPM), a novel\napproach that leverages object-part decomposition and part-part correspondences\nto improve learning and generalization of robotic manipulation skills. By\nconsidering the functional correspondences between object parts, we\nconceptualize functional actions, such as pouring and constrained placing, as\ncombinations of different correspondence constraints. CPM comprises a\ncollection of composable diffusion models, where each model captures a\ndifferent inter-object correspondence. These diffusion models can generate\nparameters for manipulation skills based on the specific object parts.\nLeveraging part-based correspondences coupled with the task decomposition into\ndistinct constraints enables strong generalization to novel objects and object\ncategories. We validate our approach in both simulated and real-world\nscenarios, demonstrating its effectiveness in achieving robust and generalized\nmanipulation capabilities.",
      "tldr_zh": "该论文提出了一种名为 Composable Part-Based Manipulation (CPM) 的新方法，通过对象部分分解和部分-部分对应关系，提升机器人操作技能的学习和泛化能力。CPM 将功能动作（如 pouring 和 constrained placing）视为不同对应约束的组合，并使用一组可组合的 diffusion models 来捕捉对象间对应关系，从而基于特定对象部分生成操作参数。这种方法利用部分对应和任务分解，实现对新对象和新类别的强泛化，并在模拟和真实场景中验证了其有效性，展示了鲁棒的机器人操作能力。",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.CV",
        "cs.LG"
      ],
      "primary_category": "cs.RO",
      "comment": "Presented at CoRL 2023. For videos and additional results, see our\n  website: https://cpmcorl2023.github.io/",
      "pdf_url": "http://arxiv.org/pdf/2405.05876v1",
      "published_date": "2024-05-09 16:04:14 UTC",
      "updated_date": "2024-05-09 16:04:14 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T07:17:52.068086"
    },
    {
      "arxiv_id": "2405.05870v1",
      "title": "Selecting the Most Conflicting Pair of Candidates",
      "title_zh": "翻译失败",
      "authors": [
        "Théo Delemazure",
        "Łukasz Janeczko",
        "Andrzej Kaczmarczyk",
        "Stanisław Szufa"
      ],
      "abstract": "We study committee elections from a perspective of finding the most\nconflicting candidates, that is, candidates that imply the largest amount of\nconflict, as per voter preferences. By proposing basic axioms to capture this\nobjective, we show that none of the prominent multiwinner voting rules meet\nthem. Consequently, we design committee voting rules compliant with our\ndesiderata, introducing conflictual voting rules. A subsequent deepened\nanalysis sheds more light on how they operate. Our investigation identifies\nvarious aspects of conflict, for which we come up with relevant axioms and\nquantitative measures, which may be of independent interest. We support our\ntheoretical study with experiments on both real-life and synthetic data.",
      "tldr_zh": "这篇论文研究委员会选举（committee elections），聚焦于识别根据选民偏好引发最大冲突的候选人对（most conflicting pair of candidates）。作者提出基本公理（axioms）来定义这一目标，并发现现有的多赢者投票规则（multiwinner voting rules）均无法满足这些公理。基于此，他们设计了新的冲突投票规则（conflictual voting rules），并通过深入分析和实验验证了这些规则的运作机制。研究还探讨了冲突的多种方面，引入相关的公理和定量措施，并在真实和合成数据上进行了实验支持。",
      "categories": [
        "cs.GT",
        "cs.AI",
        "cs.MA"
      ],
      "primary_category": "cs.GT",
      "comment": "Accepted for publication at IJCAI-24; 27 pages; 11 figures",
      "pdf_url": "http://arxiv.org/pdf/2405.05870v1",
      "published_date": "2024-05-09 16:00:20 UTC",
      "updated_date": "2024-05-09 16:00:20 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T07:18:04.964305"
    },
    {
      "arxiv_id": "2405.05861v1",
      "title": "ExACT: An End-to-End Autonomous Excavator System Using Action Chunking With Transformers",
      "title_zh": "翻译失败",
      "authors": [
        "Liangliang Chen",
        "Shiyu Jin",
        "Haoyu Wang",
        "Liangjun Zhang"
      ],
      "abstract": "Excavators are crucial for diverse tasks such as construction and mining,\nwhile autonomous excavator systems enhance safety and efficiency, address labor\nshortages, and improve human working conditions. Different from the existing\nmodularized approaches, this paper introduces ExACT, an end-to-end autonomous\nexcavator system that processes raw LiDAR, camera data, and joint positions to\ncontrol excavator valves directly. Utilizing the Action Chunking with\nTransformers (ACT) architecture, ExACT employs imitation learning to take\nobservations from multi-modal sensors as inputs and generate actionable\nsequences. In our experiment, we build a simulator based on the captured\nreal-world data to model the relations between excavator valve states and joint\nvelocities. With a few human-operated demonstration data trajectories, ExACT\ndemonstrates the capability of completing different excavation tasks, including\nreaching, digging and dumping through imitation learning in validations with\nthe simulator. To the best of our knowledge, ExACT represents the first\ninstance towards building an end-to-end autonomous excavator system via\nimitation learning methods with a minimal set of human demonstrations. The\nvideo about this work can be accessed at https://youtu.be/NmzR_Rf-aEk.",
      "tldr_zh": "本论文提出ExACT，一种端到端的自主挖掘机系统，使用Action Chunking with Transformers (ACT)架构，通过imitation learning处理LiDAR、摄像头数据和关节位置，直接控制挖掘机阀门，以提高安全、效率和解决劳动力短缺问题。不同于传统的模块化方法，ExACT以多模态传感器输入生成可操作动作序列，并在基于真实数据的模拟器中，使用少量人类操作演示成功完成挖掘任务，如到达、挖掘和倾倒。实验结果显示，ExACT是首个通过imitation learning方法构建的端到端自主挖掘机系统，展示了其在实际应用中的潜力。",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.RO",
      "comment": "ICRA Workshop 2024: 3rd Workshop on Future of Construction: Lifelong\n  Learning Robots in Changing Construction Sites",
      "pdf_url": "http://arxiv.org/pdf/2405.05861v1",
      "published_date": "2024-05-09 15:48:39 UTC",
      "updated_date": "2024-05-09 15:48:39 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T07:18:18.599895"
    },
    {
      "arxiv_id": "2405.05858v2",
      "title": "Free-Moving Object Reconstruction and Pose Estimation with Virtual Camera",
      "title_zh": "利用虚拟相机的自由移动物体",
      "authors": [
        "Haixin Shi",
        "Yinlin Hu",
        "Daniel Koguciuk",
        "Juan-Ting Lin",
        "Mathieu Salzmann",
        "David Ferstl"
      ],
      "abstract": "We propose an approach for reconstructing free-moving object from a monocular\nRGB video. Most existing methods either assume scene prior, hand pose prior,\nobject category pose prior, or rely on local optimization with multiple\nsequence segments. We propose a method that allows free interaction with the\nobject in front of a moving camera without relying on any prior, and optimizes\nthe sequence globally without any segments. We progressively optimize the\nobject shape and pose simultaneously based on an implicit neural\nrepresentation. A key aspect of our method is a virtual camera system that\nreduces the search space of the optimization significantly. We evaluate our\nmethod on the standard HO3D dataset and a collection of egocentric RGB\nsequences captured with a head-mounted device. We demonstrate that our approach\noutperforms most methods significantly, and is on par with recent techniques\nthat assume prior information.",
      "tldr_zh": "本研究提出了一种无需依赖任何先验信息的方法，用于从单目 RGB 视频中重建和估计自由移动物体的形状及姿势。该方法基于隐式神经表示(implicit neural representation)实现物体的全局序列优化，同时引入虚拟相机系统(Virtual Camera)来显著减少优化搜索空间，从而允许用户在移动相机前自由互动物体。实验结果显示，该方法在 HO3D 数据集和自采集的 egocentric RGB 序列上大幅优于大多数现有方法，并在性能上与依赖先验技术的最新方法相当。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.GR",
        "cs.RO"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2405.05858v2",
      "published_date": "2024-05-09 15:45:08 UTC",
      "updated_date": "2024-05-10 15:57:13 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T07:18:27.927801"
    },
    {
      "arxiv_id": "2405.05852v1",
      "title": "Pre-trained Text-to-Image Diffusion Models Are Versatile Representation Learners for Control",
      "title_zh": "翻译失败",
      "authors": [
        "Gunshi Gupta",
        "Karmesh Yadav",
        "Yarin Gal",
        "Dhruv Batra",
        "Zsolt Kira",
        "Cong Lu",
        "Tim G. J. Rudner"
      ],
      "abstract": "Embodied AI agents require a fine-grained understanding of the physical world\nmediated through visual and language inputs. Such capabilities are difficult to\nlearn solely from task-specific data. This has led to the emergence of\npre-trained vision-language models as a tool for transferring representations\nlearned from internet-scale data to downstream tasks and new domains. However,\ncommonly used contrastively trained representations such as in CLIP have been\nshown to fail at enabling embodied agents to gain a sufficiently fine-grained\nscene understanding -- a capability vital for control. To address this\nshortcoming, we consider representations from pre-trained text-to-image\ndiffusion models, which are explicitly optimized to generate images from text\nprompts and as such, contain text-conditioned representations that reflect\nhighly fine-grained visuo-spatial information. Using pre-trained text-to-image\ndiffusion models, we construct Stable Control Representations which allow\nlearning downstream control policies that generalize to complex, open-ended\nenvironments. We show that policies learned using Stable Control\nRepresentations are competitive with state-of-the-art representation learning\napproaches across a broad range of simulated control settings, encompassing\nchallenging manipulation and navigation tasks. Most notably, we show that\nStable Control Representations enable learning policies that exhibit\nstate-of-the-art performance on OVMM, a difficult open-vocabulary navigation\nbenchmark.",
      "tldr_zh": "该研究指出，现有的对比训练表示（如 CLIP）不足以让 Embodied AI 代理获得细粒度的场景理解，从而影响控制任务的性能。为解决这一问题，研究者利用预训练的文本到图像扩散模型构建 Stable Control Representations，这些表示优化了文本条件下的视觉空间信息。实验结果显示，使用这些表示学习的下游控制策略在各种模拟环境中表现出色，与最先进方法竞争，并在挑战性的操作和导航任务中实现最先进性能，尤其在 OVMM 开源词汇导航基准上。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CL",
        "cs.LG",
        "cs.RO",
        "stat.ML"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2405.05852v1",
      "published_date": "2024-05-09 15:39:54 UTC",
      "updated_date": "2024-05-09 15:39:54 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T07:18:39.617729"
    },
    {
      "arxiv_id": "2405.06713v2",
      "title": "Unveiling the Competitive Dynamics: A Comparative Evaluation of American and Chinese LLMs",
      "title_zh": "揭示竞争动态：美国和中国的LLMs比较评估",
      "authors": [
        "Zhenhui Jiang",
        "Jiaxin Li",
        "Yang Liu"
      ],
      "abstract": "The strategic significance of Large Language Models (LLMs) in economic\nexpansion, innovation, societal development, and national security has been\nincreasingly recognized since the advent of ChatGPT. This study provides a\ncomprehensive comparative evaluation of American and Chinese LLMs in both\nEnglish and Chinese contexts. We proposed a comprehensive evaluation framework\nthat encompasses natural language proficiency, disciplinary expertise, and\nsafety and responsibility, and systematically assessed 16 prominent models from\nthe US and China under various operational tasks and scenarios. Our key\nfindings show that GPT 4-Turbo is at the forefront in English contexts, whereas\nErnie-Bot 4 stands out in Chinese contexts. The study also highlights\ndisparities in LLM performance across languages and tasks, stressing the\nnecessity for linguistically and culturally nuanced model development. The\ncomplementary strengths of American and Chinese LLMs point to the value of\nSino-US collaboration in advancing LLM technology. The research presents the\ncurrent LLM competition landscape and offers valuable insights for policymakers\nand businesses regarding strategic LLM investments and development. Future work\nwill expand on this framework to include emerging LLM multimodal capabilities\nand business application assessments.",
      "tldr_zh": "这篇论文比较了美国和中国的Large Language Models (LLMs)在英语和中文环境下的性能，提出一个全面评估框架，包括自然语言熟练度、学科专业知识以及安全性和责任感，并系统评估了16个突出模型。研究发现，GPT-4 Turbo在英语任务中领先，而Ernie-Bot 4在中文任务中表现出色，同时揭示了模型在不同语言和任务间的表现差异，强调了需要更注重语言和文化细化的模型开发。论文指出，美国和中国的LLMs具有互补优势，建议中美合作以推动LLM技术进步，并为政策制定者和企业提供战略投资洞见。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "There was a miscommunication among the co-authors, resulting in the\n  accidental submission of this paper to arXiv. We are in need of withdrawing\n  the paper from your platform",
      "pdf_url": "http://arxiv.org/pdf/2405.06713v2",
      "published_date": "2024-05-09 15:39:19 UTC",
      "updated_date": "2024-05-21 05:37:58 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T07:18:52.955122"
    },
    {
      "arxiv_id": "2405.06712v1",
      "title": "Digital Diagnostics: The Potential Of Large Language Models In Recognizing Symptoms Of Common Illnesses",
      "title_zh": "数字诊断：大语言模型在识别常见疾病症状的潜力",
      "authors": [
        "Gaurav Kumar Gupta",
        "Aditi Singh",
        "Sijo Valayakkad Manikandan",
        "Abul Ehtesham"
      ],
      "abstract": "The recent swift development of LLMs like GPT-4, Gemini, and GPT-3.5 offers a\ntransformative opportunity in medicine and healthcare, especially in digital\ndiagnostics. This study evaluates each model diagnostic abilities by\ninterpreting a user symptoms and determining diagnoses that fit well with\ncommon illnesses, and it demonstrates how each of these models could\nsignificantly increase diagnostic accuracy and efficiency. Through a series of\ndiagnostic prompts based on symptoms from medical databases, GPT-4 demonstrates\nhigher diagnostic accuracy from its deep and complete history of training on\nmedical data. Meanwhile, Gemini performs with high precision as a critical tool\nin disease triage, demonstrating its potential to be a reliable model when\nphysicians are trying to make high-risk diagnoses. GPT-3.5, though slightly\nless advanced, is a good tool for medical diagnostics. This study highlights\nthe need to study LLMs for healthcare and clinical practices with more care and\nattention, ensuring that any system utilizing LLMs promotes patient privacy and\ncomplies with health information privacy laws such as HIPAA compliance, as well\nas the social consequences that affect the varied individuals in complex\nhealthcare contexts. This study marks the start of a larger future effort to\nstudy the various ways in which assigning ethical concerns to LLMs task of\nlearning from human biases could unearth new ways to apply AI in complex\nmedical settings.",
      "tldr_zh": "这篇论文探讨了大型语言模型（LLMs）如 GPT-4、Gemini 和 GPT-3.5 在数字诊断中的潜力，重点评估它们识别常见疾病症状的能力，以提高诊断准确性和效率。研究通过基于医疗数据库的症状提示进行测试，发现 GPT-4 凭借全面的医疗训练数据表现出最高的诊断准确率。Gemini 在疾病分流和高风险诊断中显示出高精度，而 GPT-3.5 虽稍逊一筹，但仍可作为实用工具。论文强调需要谨慎研究 LLMs 在医疗中的应用，确保遵守患者隐私法规（如 HIPAA 合规）和处理社会影响，并呼吁未来探索其伦理问题以安全应用于复杂医疗场景。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "14 pages, 4 figures",
      "pdf_url": "http://arxiv.org/pdf/2405.06712v1",
      "published_date": "2024-05-09 15:12:24 UTC",
      "updated_date": "2024-05-09 15:12:24 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T07:19:05.317215"
    },
    {
      "arxiv_id": "2405.05809v2",
      "title": "Aequitas Flow: Streamlining Fair ML Experimentation",
      "title_zh": "翻译失败",
      "authors": [
        "Sérgio Jesus",
        "Pedro Saleiro",
        "Inês Oliveira e Silva",
        "Beatriz M. Jorge",
        "Rita P. Ribeiro",
        "João Gama",
        "Pedro Bizarro",
        "Rayid Ghani"
      ],
      "abstract": "Aequitas Flow is an open-source framework and toolkit for end-to-end Fair\nMachine Learning (ML) experimentation, and benchmarking in Python. This package\nfills integration gaps that exist in other fair ML packages. In addition to the\nexisting audit capabilities in Aequitas, the Aequitas Flow module provides a\npipeline for fairness-aware model training, hyperparameter optimization, and\nevaluation, enabling easy-to-use and rapid experiments and analysis of results.\nAimed at ML practitioners and researchers, the framework offers implementations\nof methods, datasets, metrics, and standard interfaces for these components to\nimprove extensibility. By facilitating the development of fair ML practices,\nAequitas Flow hopes to enhance the incorporation of fairness concepts in AI\nsystems making AI systems more robust and fair.",
      "tldr_zh": "Aequitas Flow 是一个开源框架和工具包，旨在简化 Fair ML（公平机器学习）的端到端实验和基准测试过程。该框架填补了现有 Fair ML 包的集成空白，提供了一个管道支持公平感知模型训练、hyperparameter optimization（超参数优化）和评估，便于 ML 从业者和研究者快速进行实验和结果分析。Aequitas Flow 还包括方法的实现、数据集、metrics（指标）和标准接口，以提升框架的可扩展性，最终促进公平 ML 实践，使 AI 系统更稳健和公平。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CY"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2405.05809v2",
      "published_date": "2024-05-09 14:48:17 UTC",
      "updated_date": "2024-10-30 16:34:12 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T07:19:16.306753"
    },
    {
      "arxiv_id": "2405.05803v3",
      "title": "Boosting Multimodal Large Language Models with Visual Tokens Withdrawal for Rapid Inference",
      "title_zh": "翻译失败",
      "authors": [
        "Zhihang Lin",
        "Mingbao Lin",
        "Luxi Lin",
        "Rongrong Ji"
      ],
      "abstract": "Multimodal large language models (MLLMs) demand considerable computations for\ninference due to the extensive parameters and the additional input tokens\nneeded for visual information representation. Herein, we introduce Visual\nTokens Withdrawal (VTW), a plug-and-play module to boost MLLMs for rapid\ninference. Our approach is inspired by two intriguing phenomena we have\nobserved: (1) the attention sink phenomenon that is prevalent in LLMs also\npersists in MLLMs, suggesting that initial tokens and nearest tokens receive\nthe majority of attention, while middle vision tokens garner minimal attention\nin deep layers; (2) the presence of information migration, which implies that\nvisual information is transferred to subsequent text tokens within the first\nfew layers of MLLMs. As per our findings, we conclude that vision tokens are\nunnecessary in the deep layers of MLLMs. Thus, we strategically withdraw them\nat a certain layer, enabling only text tokens to engage in subsequent layers.\nTo pinpoint the ideal layer for VTW, we initially analyze a limited set of tiny\ndatasets and choose the first layer that meets the Kullback-Leibler divergence\ncriterion. Our VTW approach can cut computational overhead by over 40\\% across\ndiverse multimodal tasks while maintaining performance.",
      "tldr_zh": "这篇论文针对多模态大语言模型 (MLLMs) 在推理过程中因大量参数和视觉输入标记而导致的高计算需求，提出了 Visual Tokens Withdrawal (VTW) 模块，以实现快速推理。VTW 基于观察到的注意力下沉现象（初始和最近标记获得大部分注意力）和信息迁移（视觉信息在前几层转移到文本标记），在特定层撤回视觉标记，只让文本标记继续处理。实验结果显示，VTW 可减少超过 40% 的计算开销，同时在多种多模态任务中保持性能不变。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2405.05803v3",
      "published_date": "2024-05-09 14:38:53 UTC",
      "updated_date": "2025-01-25 15:59:57 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T07:19:29.686176"
    },
    {
      "arxiv_id": "2405.05802v1",
      "title": "Deploying Graph Neural Networks in Wireless Networks: A Link Stability Viewpoint",
      "title_zh": "在无线网络中部署图神经网络：链路稳定性视角",
      "authors": [
        "Jun Li",
        "Weiwei Zhang",
        "Kang Wei",
        "Guangji Chen",
        "Long Shi",
        "Wen Chen"
      ],
      "abstract": "As an emerging artificial intelligence technology, graph neural networks\n(GNNs) have exhibited promising performance across a wide range of\ngraph-related applications. However, information exchanges among neighbor nodes\nin GNN pose new challenges in the resource-constrained scenario, especially in\nwireless systems. In practical wireless systems, the communication links among\nnodes are usually unreliable due to wireless fading and receiver noise,\nconsequently resulting in performance degradation of GNNs. To improve the\nlearning performance of GNNs, we aim to maximize the number of long-term\naverage (LTA) communication links by the optimized power control under energy\nconsumption constraints. Using the Lyapunov optimization method, we first\ntransform the intractable long-term problem into a deterministic problem in\neach time slot by converting the long-term energy constraints into the\nobjective function. In spite of this non-convex combinatorial optimization\nproblem, we address this problem via equivalently solving a sequence of convex\nfeasibility problems together with a greedy based solver. Simulation results\ndemonstrate the superiority of our proposed scheme over the baselines.",
      "tldr_zh": "这篇论文探讨了在无线网络中部署 Graph Neural Networks (GNNs) 时，由于无线衰落和接收器噪声导致的通信链接不稳定问题，从而影响 GNNs 的性能。作者的目标是通过优化功率控制来最大化长期平均 (LTA) 通信链接的数量，同时满足能量消耗约束。采用 Lyapunov 优化方法，将长期优化问题转化为每个时间槽的确定性非凸组合问题，并通过求解一系列凸可行性问题结合贪婪算法进行求解。模拟结果显示，该方案比基线方法表现出显著优越性。",
      "categories": [
        "cs.DC",
        "cs.AI"
      ],
      "primary_category": "cs.DC",
      "comment": "5 pages,3 figures",
      "pdf_url": "http://arxiv.org/pdf/2405.05802v1",
      "published_date": "2024-05-09 14:37:08 UTC",
      "updated_date": "2024-05-09 14:37:08 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T07:19:41.682730"
    },
    {
      "arxiv_id": "2405.05792v1",
      "title": "RoboHop: Segment-based Topological Map Representation for Open-World Visual Navigation",
      "title_zh": "翻译失败",
      "authors": [
        "Sourav Garg",
        "Krishan Rana",
        "Mehdi Hosseinzadeh",
        "Lachlan Mares",
        "Niko Sünderhauf",
        "Feras Dayoub",
        "Ian Reid"
      ],
      "abstract": "Mapping is crucial for spatial reasoning, planning and robot navigation.\nExisting approaches range from metric, which require precise geometry-based\noptimization, to purely topological, where image-as-node based graphs lack\nexplicit object-level reasoning and interconnectivity. In this paper, we\npropose a novel topological representation of an environment based on \"image\nsegments\", which are semantically meaningful and open-vocabulary queryable,\nconferring several advantages over previous works based on pixel-level\nfeatures. Unlike 3D scene graphs, we create a purely topological graph with\nsegments as nodes, where edges are formed by a) associating segment-level\ndescriptors between pairs of consecutive images and b) connecting neighboring\nsegments within an image using their pixel centroids. This unveils a\n\"continuous sense of a place\", defined by inter-image persistence of segments\nalong with their intra-image neighbours. It further enables us to represent and\nupdate segment-level descriptors through neighborhood aggregation using graph\nconvolution layers, which improves robot localization based on segment-level\nretrieval. Using real-world data, we show how our proposed map representation\ncan be used to i) generate navigation plans in the form of \"hops over segments\"\nand ii) search for target objects using natural language queries describing\nspatial relations of objects. Furthermore, we quantitatively analyze data\nassociation at the segment level, which underpins inter-image connectivity\nduring mapping and segment-level localization when revisiting the same place.\nFinally, we show preliminary trials on segment-level `hopping' based zero-shot\nreal-world navigation. Project page with supplementary details:\noravus.github.io/RoboHop/",
      "tldr_zh": "本文提出RoboHop，一种基于图像段的拓扑地图表示方法，用于开放世界的视觉导航，该方法以语义有意义的段作为节点，解决了传统拓扑图缺乏对象级推理和互联性的问题。方法通过关联连续图像间的段描述符和连接图像内相邻段的像素质心构建纯拓扑图，并利用graph convolution layers聚合邻居信息来更新描述符，从而提升机器人定位和段级检索准确性。实验结果显示，RoboHop在真实数据上实现了“段跳跃”形式的导航规划、自然语言查询的目标对象搜索，并支持零样本实时导航，显著提高了环境映射和定位的鲁棒性。",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.CV",
        "cs.HC",
        "cs.LG"
      ],
      "primary_category": "cs.RO",
      "comment": "Published at ICRA 2024; 9 pages, 8 figures",
      "pdf_url": "http://arxiv.org/pdf/2405.05792v1",
      "published_date": "2024-05-09 14:17:26 UTC",
      "updated_date": "2024-05-09 14:17:26 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T07:19:55.058827"
    },
    {
      "arxiv_id": "2405.05790v1",
      "title": "A Robust eLORETA Technique for Localization of Brain Sources in the Presence of Forward Model Uncertainties",
      "title_zh": "一种鲁棒的 eLORETA 技术，用于在正向模型不确定性存在的情况下定位大脑源",
      "authors": [
        "A. Noroozi",
        "M. Ravan",
        "B. Razavi",
        "R. S. Fisher",
        "Y. Law",
        "M. S. Hasan"
      ],
      "abstract": "In this paper, we present a robust version of the well-known exact\nlow-resolution electromagnetic tomography (eLORETA) technique, named ReLORETA,\nto localize brain sources in the presence of different forward model\nuncertainties. Methods: We first assume that the true lead field matrix is a\ntransformation of the existing lead field matrix distorted by uncertainties and\npropose an iterative approach to estimate this transformation accurately. Major\nsources of the forward model uncertainties, including differences in geometry,\nconductivity, and source space resolution between the real and simulated head\nmodels, and misaligned electrode positions, are then simulated to test the\nproposed method. Results: ReLORETA and eLORETA are applied to simulated focal\nsources in different regions of the brain and the presence of various noise\nlevels as well as real data from a patient with focal epilepsy. The results\nshow that ReLORETA is considerably more robust and accurate than eLORETA in all\ncases. Conclusion: Having successfully dealt with the forward model\nuncertainties, ReLORETA proved to be a promising method for real-world clinical\napplications. Significance: eLORETA is one of the localization techniques that\ncould be used to study brain activity for medical applications such as\ndetermining the epileptogenic zone in patients with medically refractory\nepilepsy. However, the major limitation of eLORETA is sensitivity to the\nuncertainties in the forward model. Since this problem can substantially\nundermine its performance in real-world applications where the exact lead field\nmatrix is unknown, developing a more robust method capable of dealing with\nthese uncertainties is of significant interest.",
      "tldr_zh": "本文提出了一种鲁棒版本的 eLORETA 技术，名为 ReLORETA，用于在 forward model uncertainties（如几何、导电性和电极位置差异）存在的情况下精确定位脑源。方法采用迭代方法估计真实 lead field matrix 的变形，以提升算法的鲁棒性。实验结果显示，ReLORETA 在模拟脑源数据、不同噪声水平以及真实癫痫患者数据上，比 eLORETA 准确性显著提高。总之，该技术为临床应用如癫痫定位提供了更可靠的解决方案，解决了 eLORETA 对不确定性的敏感问题。",
      "categories": [
        "cs.CE",
        "cs.AI",
        "q-bio.QM"
      ],
      "primary_category": "cs.CE",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2405.05790v1",
      "published_date": "2024-05-09 14:15:00 UTC",
      "updated_date": "2024-05-09 14:15:00 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T07:20:06.982020"
    },
    {
      "arxiv_id": "2405.05777v1",
      "title": "Towards a More Inclusive AI: Progress and Perspectives in Large Language Model Training for the Sámi Language",
      "title_zh": "翻译失败",
      "authors": [
        "Ronny Paul",
        "Himanshu Buckchash",
        "Shantipriya Parida",
        "Dilip K. Prasad"
      ],
      "abstract": "S\\'ami, an indigenous language group comprising multiple languages, faces\ndigital marginalization due to the limited availability of data and\nsophisticated language models designed for its linguistic intricacies. This\nwork focuses on increasing technological participation for the S\\'ami language.\nWe draw the attention of the ML community towards the language modeling problem\nof Ultra Low Resource (ULR) languages. ULR languages are those for which the\namount of available textual resources is very low, and the speaker count for\nthem is also very low. ULRLs are also not supported by mainstream Large\nLanguage Models (LLMs) like ChatGPT, due to which gathering artificial training\ndata for them becomes even more challenging. Mainstream AI foundational model\ndevelopment has given less attention to this category of languages. Generally,\nthese languages have very few speakers, making it hard to find them. However,\nit is important to develop foundational models for these ULR languages to\npromote inclusion and the tangible abilities and impact of LLMs. To this end,\nwe have compiled the available S\\'ami language resources from the web to create\na clean dataset for training language models. In order to study the behavior of\nmodern LLM models with ULR languages (S\\'ami), we have experimented with\ndifferent kinds of LLMs, mainly at the order of $\\sim$ seven billion\nparameters. We have also explored the effect of multilingual LLM training for\nULRLs. We found that the decoder-only models under a sequential multilingual\ntraining scenario perform better than joint multilingual training, whereas\nmultilingual training with high semantic overlap, in general, performs better\nthan training from scratch.This is the first study on the S\\'ami language for\nadapting non-statistical language models that use the latest developments in\nthe field of natural language processing (NLP).",
      "tldr_zh": "这篇论文关注 Sámi 语言（一个土著语言群）的数字边缘化问题，强调其作为 Ultra Low Resource (ULR) languages 的数据不足和缺乏主流 Large Language Models (LLMs) 支持，如 ChatGPT。作者编译了网络资源，创建了一个干净数据集，并实验了约七亿参数的各种 LLM，以探索多语言训练策略。研究发现，顺序多语言训练比联合训练表现更好，且与高语义重叠的多语言训练总体优于从零开始训练，这为推动 ULR 语言的包容性 AI 发展提供了重要见解。这是首个针对 Sámi 语言使用现代非统计语言模型的研究。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2405.05777v1",
      "published_date": "2024-05-09 13:54:22 UTC",
      "updated_date": "2024-05-09 13:54:22 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T07:20:17.971237"
    },
    {
      "arxiv_id": "2405.05766v1",
      "title": "To Trust or Not to Trust: Towards a novel approach to measure trust for XAI systems",
      "title_zh": "翻译失败",
      "authors": [
        "Miquel Miró-Nicolau",
        "Gabriel Moyà-Alcover",
        "Antoni Jaume-i-Capó",
        "Manuel González-Hidalgo",
        "Maria Gemma Sempere Campello",
        "Juan Antonio Palmer Sancho"
      ],
      "abstract": "The increasing reliance on Deep Learning models, combined with their inherent\nlack of transparency, has spurred the development of a novel field of study\nknown as eXplainable AI (XAI) methods. These methods seek to enhance the trust\nof end-users in automated systems by providing insights into the rationale\nbehind their decisions. This paper presents a novel approach for measuring user\ntrust in XAI systems, allowing their refinement. Our proposed metric combines\nboth performance metrics and trust indicators from an objective perspective. To\nvalidate this novel methodology, we conducted a case study in a realistic\nmedical scenario: the usage of XAI system for the detection of pneumonia from\nx-ray images.",
      "tldr_zh": "本论文针对深度学习模型的不透明性问题，提出了一种新方法来测量用户对 XAI 系统的信任，从而帮助系统进行优化。该方法结合了性能指标和信任指标，从客观角度进行评估。为了验证这一方法，研究团队在真实医疗场景中进行了案例研究，即使用 XAI 系统检测 X-ray 图像中的肺炎。结果表明，此方法有助于提升 XAI 系统的可靠性和用户信任。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2405.05766v1",
      "published_date": "2024-05-09 13:42:54 UTC",
      "updated_date": "2024-05-09 13:42:54 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T07:20:29.581903"
    },
    {
      "arxiv_id": "2405.05763v1",
      "title": "DP-MDM: Detail-Preserving MR Reconstruction via Multiple Diffusion Models",
      "title_zh": "翻译失败",
      "authors": [
        "Mengxiao Geng",
        "Jiahao Zhu",
        "Xiaolin Zhu",
        "Qiqing Liu",
        "Dong Liang",
        "Qiegen Liu"
      ],
      "abstract": "Detail features of magnetic resonance images play a cru-cial role in accurate\nmedical diagnosis and treatment, as they capture subtle changes that pose\nchallenges for doc-tors when performing precise judgments. However, the widely\nutilized naive diffusion model has limitations, as it fails to accurately\ncapture more intricate details. To en-hance the quality of MRI reconstruction,\nwe propose a comprehensive detail-preserving reconstruction method using\nmultiple diffusion models to extract structure and detail features in k-space\ndomain instead of image do-main. Moreover, virtual binary modal masks are\nutilized to refine the range of values in k-space data through highly adaptive\ncenter windows, which allows the model to focus its attention more efficiently.\nLast but not least, an inverted pyramid structure is employed, where the\ntop-down image information gradually decreases, ena-bling a cascade\nrepresentation. The framework effective-ly represents multi-scale sampled data,\ntaking into ac-count the sparsity of the inverted pyramid architecture, and\nutilizes cascade training data distribution to repre-sent multi-scale data.\nThrough a step-by-step refinement approach, the method refines the\napproximation of de-tails. Finally, the proposed method was evaluated by\ncon-ducting experiments on clinical and public datasets. The results\ndemonstrate that the proposed method outper-forms other methods.",
      "tldr_zh": "该论文提出DP-MDM方法，使用多个Diffusion Models在k-space域提取MRI图像的结构和细节特征，以解决传统扩散模型捕捉复杂细节不足的问题。该方法引入虚拟二进制模态掩码来优化k-space数据范围，并采用倒置金字塔结构实现级联表示和多尺度数据处理，通过逐步细化提升细节逼近精度。在临床和公共数据集上的实验结果表明，DP-MDM在MRI重建质量上优于其他方法。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2405.05763v1",
      "published_date": "2024-05-09 13:37:18 UTC",
      "updated_date": "2024-05-09 13:37:18 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T07:20:41.956554"
    },
    {
      "arxiv_id": "2405.05755v2",
      "title": "CSA-Net: Channel-wise Spatially Autocorrelated Attention Networks",
      "title_zh": "CSA-Net：通道级的空间自相关注意力网络",
      "authors": [
        "Nick Nikzad",
        "Yongsheng Gao",
        "Jun Zhou"
      ],
      "abstract": "In recent years, convolutional neural networks (CNNs) with channel-wise\nfeature refining mechanisms have brought noticeable benefits to modelling\nchannel dependencies. However, current attention paradigms fail to infer an\noptimal channel descriptor capable of simultaneously exploiting statistical and\nspatial relationships among feature maps. In this paper, to overcome this\nshortcoming, we present a novel channel-wise spatially autocorrelated (CSA)\nattention mechanism. Inspired by geographical analysis, the proposed CSA\nexploits the spatial relationships between channels of feature maps to produce\nan effective channel descriptor. To the best of our knowledge, this is the f\nirst time that the concept of geographical spatial analysis is utilized in deep\nCNNs. The proposed CSA imposes negligible learning parameters and light\ncomputational overhead to the deep model, making it a powerful yet efficient\nattention module of choice. We validate the effectiveness of the proposed CSA\nnetworks (CSA-Nets) through extensive experiments and analysis on ImageNet, and\nMS COCO benchmark datasets for image classification, object detection, and\ninstance segmentation. The experimental results demonstrate that CSA-Nets are\nable to consistently achieve competitive performance and superior\ngeneralization than several state-of-the-art attention-based CNNs over\ndifferent benchmark tasks and datasets.",
      "tldr_zh": "本文提出了一种新型的 channel-wise spatially autocorrelated (CSA) attention 机制，用于提升 CNNs 中的通道依赖建模，解决现有方法无法同时利用特征图的统计和空间关系的局限。CSA 机制借鉴地理分析的概念，通过挖掘特征图之间的空间自相关性来生成有效的通道描述符，同时保持极低的额外学习参数和计算开销。在 ImageNet 和 MS COCO 数据集上的实验验证了 CSA-Nets 在图像分类、物体检测和实例分割任务中的优越性能，其泛化能力超过了多项最先进 attention-based CNNs。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2405.05755v2",
      "published_date": "2024-05-09 13:21:03 UTC",
      "updated_date": "2024-05-13 08:17:13 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T07:20:55.125251"
    },
    {
      "arxiv_id": "2405.05751v2",
      "title": "Mirage: A Multi-Level Superoptimizer for Tensor Programs",
      "title_zh": "翻译失败",
      "authors": [
        "Mengdi Wu",
        "Xinhao Cheng",
        "Shengyu Liu",
        "Chunan Shi",
        "Jianan Ji",
        "Kit Ao",
        "Praveen Velliengiri",
        "Xupeng Miao",
        "Oded Padon",
        "Zhihao Jia"
      ],
      "abstract": "We introduce Mirage, the first multi-level superoptimizer for tensor\nprograms. A key idea in Mirage is $\\mu$Graphs, a uniform representation of\ntensor programs at the kernel, thread block, and thread levels of the GPU\ncompute hierarchy. $\\mu$Graphs enable Mirage to discover novel optimizations\nthat combine algebraic transformations, schedule transformations, and\ngeneration of new custom kernels. To navigate the large search space, Mirage\nintroduces a pruning technique based on abstraction that significantly reduces\nthe search space and provides a certain optimality guarantee. To ensure that\nthe optimized $\\mu$Graph is equivalent to the input program, Mirage introduces\na probabilistic equivalence verification procedure with strong theoretical\nguarantees. Our evaluation shows that Mirage outperforms existing approaches by\n1.1-2.9$\\times$ even for DNNs that are widely used and heavily optimized.\nMirage is publicly available at https://github.com/mirage-project/mirage.",
      "tldr_zh": "本论文引入了Mirage，一种多级超级优化器，用于优化张量程序。Mirage的核心创新是μGraphs，这是一种统一表示张量程序的框架，涵盖GPU计算层次的内核、线程块和线程级别，从而实现代数变换、调度变换以及生成新自定义内核的组合优化。为应对庞大的搜索空间，Mirage采用基于抽象的修剪技术来减少空间并提供最优性保证，同时引入概率等价验证程序确保优化结果的正确性。实验显示，Mirage在广泛使用的DNNs上比现有方法性能提升1.1-2.9倍，并已开源。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.PL"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2405.05751v2",
      "published_date": "2024-05-09 13:15:40 UTC",
      "updated_date": "2024-12-23 15:28:18 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T07:21:06.568499"
    },
    {
      "arxiv_id": "2405.05741v1",
      "title": "Can large language models understand uncommon meanings of common words?",
      "title_zh": "大型语言模型能否理解常见词汇的不常见含义？",
      "authors": [
        "Jinyang Wu",
        "Feihu Che",
        "Xinxin Zheng",
        "Shuai Zhang",
        "Ruihan Jin",
        "Shuai Nie",
        "Pengpeng Shao",
        "Jianhua Tao"
      ],
      "abstract": "Large language models (LLMs) like ChatGPT have shown significant advancements\nacross diverse natural language understanding (NLU) tasks, including\nintelligent dialogue and autonomous agents. Yet, lacking widely acknowledged\ntesting mechanisms, answering `whether LLMs are stochastic parrots or genuinely\ncomprehend the world' remains unclear, fostering numerous studies and sparking\nheated debates. Prevailing research mainly focuses on surface-level NLU,\nneglecting fine-grained explorations. However, such explorations are crucial\nfor understanding their unique comprehension mechanisms, aligning with human\ncognition, and finally enhancing LLMs' general NLU capacities. To address this\ngap, our study delves into LLMs' nuanced semantic comprehension capabilities,\nparticularly regarding common words with uncommon meanings. The idea stems from\nfoundational principles of human communication within psychology, which\nunderscore accurate shared understandings of word semantics. Specifically, this\npaper presents the innovative construction of a Lexical Semantic Comprehension\n(LeSC) dataset with novel evaluation metrics, the first benchmark encompassing\nboth fine-grained and cross-lingual dimensions. Introducing models of both\nopen-source and closed-source, varied scales and architectures, our extensive\nempirical experiments demonstrate the inferior performance of existing models\nin this basic lexical-meaning understanding task. Notably, even the\nstate-of-the-art LLMs GPT-4 and GPT-3.5 lag behind 16-year-old humans by 3.9%\nand 22.3%, respectively. Additionally, multiple advanced prompting techniques\nand retrieval-augmented generation are also introduced to help alleviate this\ntrouble, yet limitations persist. By highlighting the above critical\nshortcomings, this research motivates further investigation and offers novel\ninsights for developing more intelligent LLMs.",
      "tldr_zh": "本研究探讨大型语言模型（LLMs）是否能理解常见词语的不常见含义，填补了细粒度自然语言理解（NLU）研究的空白。\n作者构建了Lexical Semantic Comprehension (LeSC)数据集和新型评估指标，这是首个涵盖细粒度和跨语言维度的基准，并对多种开源和闭源LLMs进行了实验评估。\n结果显示，现有机型在该任务上表现不佳，即使GPT-4和GPT-3.5也分别落后于16岁人类3.9%和22.3%，尽管引入了高级提示技巧和检索增强生成，但局限性依然存在。\n此研究突出了LLMs的语义理解不足，并为提升其一般NLU能力提供了新见解和研究方向。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2405.05741v1",
      "published_date": "2024-05-09 12:58:22 UTC",
      "updated_date": "2024-05-09 12:58:22 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T07:21:19.827444"
    },
    {
      "arxiv_id": "2405.06710v1",
      "title": "Mobile Sequencers",
      "title_zh": "翻译失败",
      "authors": [
        "Cem Bozsahin"
      ],
      "abstract": "The article is an attempt to contribute to explorations of a common origin\nfor language and planned-collaborative action. It gives `semantics of change'\nthe central stage in the synthesis, from its history and recordkeeping to its\ndevelopment, its syntax, delivery and reception, including substratal aspects.\n  It is suggested that to arrive at a common core, linguistic semantics must be\nunderstood as studying through syntax mobile agent's representing, tracking and\ncoping with change and no change. Semantics of actions can be conceived the\nsame way, but through plans instead of syntax. The key point is the following:\nSequencing itself, of words and action sequences, brings in more structural\ninterpretation to the sequence than which is immediately evident from the\nsequents themselves. Mobile sequencers can be understood as subjects\nstructuring reporting, understanding and keeping track of change and no change.\nThe idea invites rethinking of the notion of category, both in language and in\nplanning.\n  Understanding understanding change by mobile agents is suggested to be about\nhuman extended practice, not extended-human practice. That's why linguistics is\nas important as computer science in the synthesis. It must rely on\nrepresentational history of acts, thoughts and expressions, personal and\npublic, crosscutting overtness and covertness of these phenomena. It has\nimplication for anthropology in the extended practice, which is covered\nbriefly.",
      "tldr_zh": "该论文探讨了语言和计划协作行动的共同起源，以“semantics of change”（变化语义）为核心，提出语言语义学应通过语法研究移动代理（mobile agents）表示、跟踪和应对变化和不变，而行动语义则通过计划来实现。\n\n关键观点是，序列化（sequencing）本身为单词和行动序列带来额外的结构解释，“mobile sequencers”被视为主体，用于结构化报告、理解和跟踪变化。\n\n这一框架邀请重新思考语言和规划中的类别概念，并强调语言学和计算机科学在人类扩展实践中的同等重要性，为跨学科研究提供新视角。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.CY"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2405.06710v1",
      "published_date": "2024-05-09 12:39:50 UTC",
      "updated_date": "2024-05-09 12:39:50 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T07:21:30.871889"
    },
    {
      "arxiv_id": "2405.05723v1",
      "title": "Computational lexical analysis of Flamenco genres",
      "title_zh": "计算佛朗明哥流派的词汇分析",
      "authors": [
        "Pablo Rosillo-Rodes",
        "Maxi San Miguel",
        "David Sanchez"
      ],
      "abstract": "Flamenco, recognized by UNESCO as part of the Intangible Cultural Heritage of\nHumanity, is a profound expression of cultural identity rooted in Andalusia,\nSpain. However, there is a lack of quantitative studies that help identify\ncharacteristic patterns in this long-lived music tradition. In this work, we\npresent a computational analysis of Flamenco lyrics, employing natural language\nprocessing and machine learning to categorize over 2000 lyrics into their\nrespective Flamenco genres, termed as $\\textit{palos}$. Using a Multinomial\nNaive Bayes classifier, we find that lexical variation across styles enables to\naccurately identify distinct $\\textit{palos}$. More importantly, from an\nautomatic method of word usage, we obtain the semantic fields that characterize\neach style. Further, applying a metric that quantifies the inter-genre distance\nwe perform a network analysis that sheds light on the relationship between\nFlamenco styles. Remarkably, our results suggest historical connections and\n$\\textit{palo}$ evolutions. Overall, our work illuminates the intricate\nrelationships and cultural significance embedded within Flamenco lyrics,\ncomplementing previous qualitative discussions with quantitative analyses and\nsparking new discussions on the origin and development of traditional music\ngenres.",
      "tldr_zh": "本研究通过计算词法分析方法，考察了Flamenco音乐流派（palos）的特征模式，使用自然语言处理和机器学习对超过2000首歌词进行分类。采用Multinomial Naive Bayes分类器，他们发现词汇变化能准确识别不同palos，并通过自动词汇使用分析提取每个流派的语义领域。进一步，利用量化流派间距离的指标进行网络分析，揭示了palos之间的历史联系和演变关系。该工作以定量方式补充了Flamenco的文化研究，强调其作为人类非物质文化遗产的复杂性和意义。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.IR"
      ],
      "primary_category": "cs.CL",
      "comment": "21 pages, 29 figures",
      "pdf_url": "http://arxiv.org/pdf/2405.05723v1",
      "published_date": "2024-05-09 12:35:33 UTC",
      "updated_date": "2024-05-09 12:35:33 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T07:21:43.937689"
    },
    {
      "arxiv_id": "2407.03331v1",
      "title": "Anole: Adapting Diverse Compressed Models For Cross-Scene Prediction On Mobile Devices",
      "title_zh": "翻译失败",
      "authors": [
        "Yunzhe Li",
        "Hongzi Zhu",
        "Zhuohong Deng",
        "Yunlong Cheng",
        "Liang Zhang",
        "Shan Chang",
        "Minyi Guo"
      ],
      "abstract": "Emerging Artificial Intelligence of Things (AIoT) applications desire online\nprediction using deep neural network (DNN) models on mobile devices. However,\ndue to the movement of devices, unfamiliar test samples constantly appear,\nsignificantly affecting the prediction accuracy of a pre-trained DNN. In\naddition, unstable network connection calls for local model inference. In this\npaper, we propose a light-weight scheme, called Anole, to cope with the local\nDNN model inference on mobile devices. The core idea of Anole is to first\nestablish an army of compact DNN models, and then adaptively select the model\nfitting the current test sample best for online inference. The key is to\nautomatically identify model-friendly scenes for training scene-specific DNN\nmodels. To this end, we design a weakly-supervised scene representation\nlearning algorithm by combining both human heuristics and feature similarity in\nseparating scenes. Moreover, we further train a model classifier to predict the\nbest-fit scene-specific DNN model for each test sample. We implement Anole on\ndifferent types of mobile devices and conduct extensive trace-driven and\nreal-world experiments based on unmanned aerial vehicles (UAVs). The results\ndemonstrate that Anole outwits the method of using a versatile large DNN in\nterms of prediction accuracy (4.5% higher), response time (33.1% faster) and\npower consumption (45.1% lower).",
      "tldr_zh": "该论文提出Anole框架，用于在移动设备上适应多种压缩DNN模型，以应对AIoT应用中跨场景预测的挑战，尤其是在设备移动和网络不稳定导致的测试样本不熟悉问题。Anole的核心方法包括建立一组紧凑DNN模型，通过弱监督场景表示学习算法（结合人类启发和特征相似性）自动识别场景并训练特定模型，随后使用模型分类器为每个测试样本选择最佳模型。实验结果显示，Anole在基于UAVs的跟踪驱动和真实世界测试中，比使用通用大DNN模型的基准方案准确率提高4.5%、响应时间缩短33.1%、功耗降低45.1%。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.DC"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2407.03331v1",
      "published_date": "2024-05-09 12:06:18 UTC",
      "updated_date": "2024-05-09 12:06:18 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T07:21:55.278091"
    },
    {
      "arxiv_id": "2405.05695v1",
      "title": "Aux-NAS: Exploiting Auxiliary Labels with Negligibly Extra Inference Cost",
      "title_zh": "翻译失败",
      "authors": [
        "Yuan Gao",
        "Weizhong Zhang",
        "Wenhan Luo",
        "Lin Ma",
        "Jin-Gang Yu",
        "Gui-Song Xia",
        "Jiayi Ma"
      ],
      "abstract": "We aim at exploiting additional auxiliary labels from an independent\n(auxiliary) task to boost the primary task performance which we focus on, while\npreserving a single task inference cost of the primary task. While most\nexisting auxiliary learning methods are optimization-based relying on loss\nweights/gradients manipulation, our method is architecture-based with a\nflexible asymmetric structure for the primary and auxiliary tasks, which\nproduces different networks for training and inference. Specifically, starting\nfrom two single task networks/branches (each representing a task), we propose a\nnovel method with evolving networks where only primary-to-auxiliary links exist\nas the cross-task connections after convergence. These connections can be\nremoved during the primary task inference, resulting in a single-task inference\ncost. We achieve this by formulating a Neural Architecture Search (NAS)\nproblem, where we initialize bi-directional connections in the search space and\nguide the NAS optimization converging to an architecture with only the\nsingle-side primary-to-auxiliary connections. Moreover, our method can be\nincorporated with optimization-based auxiliary learning approaches. Extensive\nexperiments with six tasks on NYU v2, CityScapes, and Taskonomy datasets using\nVGG, ResNet, and ViT backbones validate the promising performance. The codes\nare available at https://github.com/ethanygao/Aux-NAS.",
      "tldr_zh": "该研究提出Aux-NAS，一种基于架构的神经架构搜索(NAS)方法，利用辅助任务的额外标签提升主要任务性能，同时保持主要任务的推理成本不变。具体而言，该方法从两个单任务网络开始，通过演化网络优化仅保留主要到辅助任务的单向连接，并在推理阶段移除这些连接，避免额外开销。实验在NYU v2、CityScapes和Taskonomy数据集上，使用VGG、ResNet和ViT骨干网络验证了其有效性，并可与基于优化的辅助学习方法结合，提升了整体任务性能。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CV",
        "stat.ML"
      ],
      "primary_category": "cs.LG",
      "comment": "Accepted to ICLR 2024",
      "pdf_url": "http://arxiv.org/pdf/2405.05695v1",
      "published_date": "2024-05-09 11:50:19 UTC",
      "updated_date": "2024-05-09 11:50:19 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T07:22:07.459843"
    },
    {
      "arxiv_id": "2405.06001v3",
      "title": "LLMC: Benchmarking Large Language Model Quantization with a Versatile Compression Toolkit",
      "title_zh": "翻译失败",
      "authors": [
        "Ruihao Gong",
        "Yang Yong",
        "Shiqiao Gu",
        "Yushi Huang",
        "Chengtao Lv",
        "Yunchen Zhang",
        "Xianglong Liu",
        "Dacheng Tao"
      ],
      "abstract": "Recent advancements in large language models (LLMs) are propelling us toward\nartificial general intelligence with their remarkable emergent abilities and\nreasoning capabilities. However, the substantial computational and memory\nrequirements limit the widespread adoption. Quantization, a key compression\ntechnique, can effectively mitigate these demands by compressing and\naccelerating LLMs, albeit with potential risks to accuracy. Numerous studies\nhave aimed to minimize the accuracy loss associated with quantization. However,\ntheir quantization configurations vary from each other and cannot be fairly\ncompared. In this paper, we present LLMC, a plug-and-play compression toolkit,\nto fairly and systematically explore the impact of quantization. LLMC\nintegrates dozens of algorithms, models, and hardwares, offering high\nextensibility from integer to floating-point quantization, from LLM to\nvision-language (VLM) model, from fixed-bit to mixed precision, and from\nquantization to sparsification. Powered by this versatile toolkit, our\nbenchmark covers three key aspects: calibration data, algorithms (three\nstrategies), and data formats, providing novel insights and detailed analyses\nfor further research and practical guidance for users. Our toolkit is available\nat https://github.com/ModelTC/llmc.",
      "tldr_zh": "该研究针对大型语言模型 (LLMs) 的高计算和内存需求，提出了 LLMC，一种插件式压缩工具包，用于公平基准测试量化技术的效果。LLMC 整合了多种算法、模型和硬件，支持从整数到浮点量化、LLM 到视觉语言模型 (VLM)、固定位到混合精度量化，以及量化到稀疏化等功能。借助该工具包，基准测试系统地探索了校准数据、算法策略和数据格式，提供新颖见解和实用指导，并已在 GitHub 上开源。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.LG",
      "comment": "Accepted by EMNLP 2024 Industry Track",
      "pdf_url": "http://arxiv.org/pdf/2405.06001v3",
      "published_date": "2024-05-09 11:49:05 UTC",
      "updated_date": "2024-10-09 06:09:41 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T07:22:19.958131"
    },
    {
      "arxiv_id": "2405.06709v1",
      "title": "Evaluating the Efficacy of AI Techniques in Textual Anonymization: A Comparative Study",
      "title_zh": "评估 AI 技术在文本匿名化中的效能：一个比较研究",
      "authors": [
        "Dimitris Asimopoulos",
        "Ilias Siniosoglou",
        "Vasileios Argyriou",
        "Sotirios K. Goudos",
        "Konstantinos E. Psannis",
        "Nikoleta Karditsioti",
        "Theocharis Saoulidis",
        "Panagiotis Sarigiannidis"
      ],
      "abstract": "In the digital era, with escalating privacy concerns, it's imperative to\ndevise robust strategies that protect private data while maintaining the\nintrinsic value of textual information. This research embarks on a\ncomprehensive examination of text anonymisation methods, focusing on\nConditional Random Fields (CRF), Long Short-Term Memory (LSTM), Embeddings from\nLanguage Models (ELMo), and the transformative capabilities of the Transformers\narchitecture. Each model presents unique strengths since LSTM is modeling\nlong-term dependencies, CRF captures dependencies among word sequences, ELMo\ndelivers contextual word representations using deep bidirectional language\nmodels and Transformers introduce self-attention mechanisms that provide\nenhanced scalability. Our study is positioned as a comparative analysis of\nthese models, emphasising their synergistic potential in addressing text\nanonymisation challenges. Preliminary results indicate that CRF, LSTM, and ELMo\nindividually outperform traditional methods. The inclusion of Transformers,\nwhen compared alongside with the other models, offers a broader perspective on\nachieving optimal text anonymisation in contemporary settings.",
      "tldr_zh": "这篇论文评估了AI技术在文本匿名化中的有效性，通过比较Conditional Random Fields (CRF)、Long Short-Term Memory (LSTM)、Embeddings from Language Models (ELMo)和Transformer等模型。CRF捕捉词序列依赖，LSTM建模长期依赖，ELMo提供上下文词表示，而Transformer利用自注意力机制提升可扩展性。研究结果表明，CRF、LSTM和ELMo单独比传统方法表现更好，而Transformer与其他模型结合时，能提供更全面的文本匿名化解决方案。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2405.06709v1",
      "published_date": "2024-05-09 11:29:25 UTC",
      "updated_date": "2024-05-09 11:29:25 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T07:22:32.054933"
    },
    {
      "arxiv_id": "2405.05662v1",
      "title": "Approximate Dec-POMDP Solving Using Multi-Agent A*",
      "title_zh": "翻译失败",
      "authors": [
        "Wietze Koops",
        "Sebastian Junges",
        "Nils Jansen"
      ],
      "abstract": "We present an A*-based algorithm to compute policies for finite-horizon\nDec-POMDPs. Our goal is to sacrifice optimality in favor of scalability for\nlarger horizons. The main ingredients of our approach are (1) using clustered\nsliding window memory, (2) pruning the A* search tree, and (3) using novel A*\nheuristics. Our experiments show competitive performance to the\nstate-of-the-art. Moreover, for multiple benchmarks, we achieve superior\nperformance. In addition, we provide an A* algorithm that finds upper bounds\nfor the optimum, tailored towards problems with long horizons. The main\ningredient is a new heuristic that periodically reveals the state, thereby\nlimiting the number of reachable beliefs. Our experiments demonstrate the\nefficacy and scalability of the approach.",
      "tldr_zh": "这篇论文提出了一种基于 Multi-Agent A* 的算法，用于近似求解有限地平线下的 Dec-POMDPs，以牺牲最优性换取更高的可扩展性。关键方法包括使用 clustered sliding window memory、修剪 A* 搜索树以及引入新的 A* heuristics。实验结果显示，该算法在多个基准上与最先进方法竞争，甚至表现出色；此外，论文还提供了一个针对长地平线问题的 A* 算法，通过周期性地揭示状态来限制可达信念，从而实现有效的上界计算和更好的可扩展性。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "19 pages, 3 figures. Extended version (with appendix) of the paper to\n  appear in IJCAI 2024",
      "pdf_url": "http://arxiv.org/pdf/2405.05662v1",
      "published_date": "2024-05-09 10:33:07 UTC",
      "updated_date": "2024-05-09 10:33:07 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T07:22:45.519108"
    },
    {
      "arxiv_id": "2405.06708v5",
      "title": "LangCell: Language-Cell Pre-training for Cell Identity Understanding",
      "title_zh": "翻译失败",
      "authors": [
        "Suyuan Zhao",
        "Jiahuan Zhang",
        "Yushuai Wu",
        "Yizhen Luo",
        "Zaiqing Nie"
      ],
      "abstract": "Cell identity encompasses various semantic aspects of a cell, including cell\ntype, pathway information, disease information, and more, which are essential\nfor biologists to gain insights into its biological characteristics.\nUnderstanding cell identity from the transcriptomic data, such as annotating\ncell types, has become an important task in bioinformatics. As these semantic\naspects are determined by human experts, it is impossible for AI models to\neffectively carry out cell identity understanding tasks without the supervision\nsignals provided by single-cell and label pairs. The single-cell pre-trained\nlanguage models (PLMs) currently used for this task are trained only on a\nsingle modality, transcriptomics data, lack an understanding of cell identity\nknowledge. As a result, they have to be fine-tuned for downstream tasks and\nstruggle when lacking labeled data with the desired semantic labels. To address\nthis issue, we propose an innovative solution by constructing a unified\nrepresentation of single-cell data and natural language during the pre-training\nphase, allowing the model to directly incorporate insights related to cell\nidentity. More specifically, we introduce $\\textbf{LangCell}$, the first\n$\\textbf{Lang}$uage-$\\textbf{Cell}$ pre-training framework. LangCell utilizes\ntexts enriched with cell identity information to gain a profound comprehension\nof cross-modal knowledge. Results from experiments conducted on different\nbenchmarks show that LangCell is the only single-cell PLM that can work\neffectively in zero-shot cell identity understanding scenarios, and also\nsignificantly outperforms existing models in few-shot and fine-tuning cell\nidentity understanding scenarios.",
      "tldr_zh": "本研究针对细胞身份（cell identity）理解问题，指出现有单细胞预训练语言模型（PLMs）仅基于转录组数据，缺乏对细胞类型、途径和疾病等语义方面的全面知识，导致在下游任务中依赖监督信号。作者提出 LangCell 框架，这是一种创新的 Language-Cell 预训练方法，通过统一单细胞数据和自然语言表示，在预训练阶段融入细胞身份相关知识。实验结果显示，LangCell 在零-shot、few-shot 和微调场景下均显著优于现有模型，为无监督或少量标注的生物信息学任务提供了高效解决方案。",
      "categories": [
        "q-bio.GN",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "q-bio.GN",
      "comment": "Accpeted by ICML 2024, code released",
      "pdf_url": "http://arxiv.org/pdf/2405.06708v5",
      "published_date": "2024-05-09 10:04:05 UTC",
      "updated_date": "2024-06-11 07:31:13 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T07:22:56.734813"
    },
    {
      "arxiv_id": "2405.08008v2",
      "title": "Iris: An AI-Driven Virtual Tutor For Computer Science Education",
      "title_zh": "Iris：AI驱动的虚拟导师，用于计算机科学教育",
      "authors": [
        "Patrick Bassner",
        "Eduard Frankford",
        "Stephan Krusche"
      ],
      "abstract": "Integrating AI-driven tools in higher education is an emerging area with\ntransformative potential. This paper introduces Iris, a chat-based virtual\ntutor integrated into the interactive learning platform Artemis that offers\npersonalized, context-aware assistance in large-scale educational settings.\nIris supports computer science students by guiding them through programming\nexercises and is designed to act as a tutor in a didactically meaningful way.\nIts calibrated assistance avoids revealing complete solutions, offering subtle\nhints or counter-questions to foster independent problem-solving skills. For\neach question, it issues multiple prompts in a Chain-of-Thought to\nGPT-3.5-Turbo. The prompts include a tutor role description and examples of\nmeaningful answers through few-shot learning. Iris employs contextual awareness\nby accessing the problem statement, student code, and automated feedback to\nprovide tailored advice.\n  An empirical evaluation shows that students perceive Iris as effective\nbecause it understands their questions, provides relevant support, and\ncontributes to the learning process. While students consider Iris a valuable\ntool for programming exercises and homework, they also feel confident solving\nprogramming tasks in computer-based exams without Iris. The findings underscore\nstudents' appreciation for Iris' immediate and personalized support, though\nstudents predominantly view it as a complement to, rather than a replacement\nfor, human tutors. Nevertheless, Iris creates a space for students to ask\nquestions without being judged by others.",
      "tldr_zh": "这篇论文介绍了 Iris，一种 AI-Driven 虚拟导师，集成到交互式学习平台 Artemis 中，用于计算机科学教育中提供个性化、上下文感知的编程指导。Iris 通过 Chain-of-Thought 提示和 few-shot learning 与 GPT-3.5-Turbo 交互，设计为避免直接揭示完整解决方案，而是通过提示或反问来培养学生的独立解决问题技能，并利用问题声明、学生代码和自动反馈实现针对性支持。实证评估显示，学生高度认可 Iris 的有效性，认为它理解问题、提供相关帮助，并作为人类导师的补充工具增强学习体验，而非替代。",
      "categories": [
        "cs.HC",
        "cs.AI"
      ],
      "primary_category": "cs.HC",
      "comment": "Published in Proceedings of the 2024 Innovation and Technology in\n  Computer Science Education V. 1 (ITiCSE 2024), Pages 534 - 540, July 8--10,\n  2024, Milan, Italy",
      "pdf_url": "http://arxiv.org/pdf/2405.08008v2",
      "published_date": "2024-05-09 09:24:13 UTC",
      "updated_date": "2024-07-10 07:59:15 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T07:23:07.768772"
    },
    {
      "arxiv_id": "2405.05636v1",
      "title": "SwapTalk: Audio-Driven Talking Face Generation with One-Shot Customization in Latent Space",
      "title_zh": "翻译失败",
      "authors": [
        "Zeren Zhang",
        "Haibo Qin",
        "Jiayu Huang",
        "Yixin Li",
        "Hui Lin",
        "Yitao Duan",
        "Jinwen Ma"
      ],
      "abstract": "Combining face swapping with lip synchronization technology offers a\ncost-effective solution for customized talking face generation. However,\ndirectly cascading existing models together tends to introduce significant\ninterference between tasks and reduce video clarity because the interaction\nspace is limited to the low-level semantic RGB space. To address this issue, we\npropose an innovative unified framework, SwapTalk, which accomplishes both face\nswapping and lip synchronization tasks in the same latent space. Referring to\nrecent work on face generation, we choose the VQ-embedding space due to its\nexcellent editability and fidelity performance. To enhance the framework's\ngeneralization capabilities for unseen identities, we incorporate identity loss\nduring the training of the face swapping module. Additionally, we introduce\nexpert discriminator supervision within the latent space during the training of\nthe lip synchronization module to elevate synchronization quality. In the\nevaluation phase, previous studies primarily focused on the self-reconstruction\nof lip movements in synchronous audio-visual videos. To better approximate\nreal-world applications, we expand the evaluation scope to asynchronous\naudio-video scenarios. Furthermore, we introduce a novel identity consistency\nmetric to more comprehensively assess the identity consistency over time series\nin generated facial videos. Experimental results on the HDTF demonstrate that\nour method significantly surpasses existing techniques in video quality, lip\nsynchronization accuracy, face swapping fidelity, and identity consistency. Our\ndemo is available at http://swaptalk.cc.",
      "tldr_zh": "该研究提出SwapTalk框架，通过在latent space中统一处理面部交换和唇部同步任务，实现音频驱动的单次定制化说话脸生成，从而避免传统RGB空间操作带来的干扰和清晰度问题。框架采用VQ-embedding space进行编辑，并通过identity loss提升面部交换的泛化能力，以及expert discriminator supervision优化唇部同步质量。在HDTF数据集的实验中，SwapTalk在视频质量、唇部同步准确性、面部交换保真度和身份一致性指标上显著优于现有方法，尤其在异步音频-视频场景中表现出色。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2405.05636v1",
      "published_date": "2024-05-09 09:22:09 UTC",
      "updated_date": "2024-05-09 09:22:09 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T07:23:20.817150"
    },
    {
      "arxiv_id": "2405.06707v1",
      "title": "Hypothesis Testing Prompting Improves Deductive Reasoning in Large Language Models",
      "title_zh": "翻译失败",
      "authors": [
        "Yitian Li",
        "Jidong Tian",
        "Hao He",
        "Yaohui Jin"
      ],
      "abstract": "Combining different forms of prompts with pre-trained large language models\nhas yielded remarkable results on reasoning tasks (e.g. Chain-of-Thought\nprompting). However, along with testing on more complex reasoning, these\nmethods also expose problems such as invalid reasoning and fictional reasoning\npaths. In this paper, we develop \\textit{Hypothesis Testing Prompting}, which\nadds conclusion assumptions, backward reasoning, and fact verification during\nintermediate reasoning steps. \\textit{Hypothesis Testing prompting} involves\nmultiple assumptions and reverses validation of conclusions leading to its\nunique correct answer. Experiments on two challenging deductive reasoning\ndatasets ProofWriter and RuleTaker show that hypothesis testing prompting not\nonly significantly improves the effect, but also generates a more reasonable\nand standardized reasoning process.",
      "tldr_zh": "本研究针对大型语言模型在推理任务中存在的无效推理和虚构推理路径问题，提出了一种Hypothesis Testing Prompting方法。该方法在中间推理步骤中加入结论假设、向后推理和事实验证，通过多个假设的逆向验证来确保得出唯一正确的答案。与传统方法如Chain-of-Thought相比，该方法显著提升了推理的准确性和合理性。在ProofWriter和RuleTaker两个演绎推理数据集上的实验显示，该提示技术不仅提高了整体效果，还生成了更标准化和可靠的推理过程。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2405.06707v1",
      "published_date": "2024-05-09 08:46:17 UTC",
      "updated_date": "2024-05-09 08:46:17 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T07:23:30.508041"
    },
    {
      "arxiv_id": "2405.05616v1",
      "title": "G-SAP: Graph-based Structure-Aware Prompt Learning over Heterogeneous Knowledge for Commonsense Reasoning",
      "title_zh": "翻译失败",
      "authors": [
        "Ruiting Dai",
        "Yuqiao Tan",
        "Lisi Mo",
        "Shuang Liang",
        "Guohao Huo",
        "Jiayi Luo",
        "Yao Cheng"
      ],
      "abstract": "Commonsense question answering has demonstrated considerable potential across\nvarious applications like assistants and social robots. Although fully\nfine-tuned pre-trained Language Models(LM) have achieved remarkable performance\nin commonsense reasoning, their tendency to excessively prioritize textual\ninformation hampers the precise transfer of structural knowledge and undermines\ninterpretability. Some studies have explored combining LMs with Knowledge\nGraphs(KGs) by coarsely fusing the two modalities to perform Graph Neural\nNetwork(GNN)-based reasoning that lacks a profound interaction between\nheterogeneous modalities. In this paper, we propose a novel Graph-based\nStructure-Aware Prompt Learning Model for commonsense reasoning, named G-SAP,\naiming to maintain a balance between heterogeneous knowledge and enhance the\ncross-modal interaction within the LM+GNNs model. In particular, an evidence\ngraph is constructed by integrating multiple knowledge sources, i.e.\nConceptNet, Wikipedia, and Cambridge Dictionary to boost the performance.\nAfterward, a structure-aware frozen PLM is employed to fully incorporate the\nstructured and textual information from the evidence graph, where the\ngeneration of prompts is driven by graph entities and relations. Finally, a\nheterogeneous message-passing reasoning module is used to facilitate deep\ninteraction of knowledge between the LM and graph-based networks. Empirical\nvalidation, conducted through extensive experiments on three benchmark\ndatasets, demonstrates the notable performance of the proposed model. The\nresults reveal a significant advancement over the existing models, especially,\nwith 6.12% improvement over the SoTA LM+GNNs model on the OpenbookQA dataset.",
      "tldr_zh": "本研究提出了一种基于图的结构感知提示学习模型 G-SAP，用于常识推理，旨在平衡异构知识（如 Knowledge Graphs 和文本信息）并提升跨模态交互。G-SAP 通过构建证据图整合多源知识（包括 ConceptNet、Wikipedia 和 Cambridge Dictionary），并利用结构感知的冻结预训练语言模型 (frozen PLM) 驱动提示生成，同时引入异构消息传递推理模块 (heterogeneous message-passing reasoning module) 来促进 Language Models (LM) 和 Graph Neural Network (GNN) 间的深度交互。实验结果显示，该模型在三个基准数据集上表现出色，尤其在 OpenbookQA 数据集上比现有最先进 (SoTA) LM+GNNs 模型提高了 6.12%。这为更精确和可解释的常识问答提供了重要进展。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2405.05616v1",
      "published_date": "2024-05-09 08:28:12 UTC",
      "updated_date": "2024-05-09 08:28:12 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T07:23:45.569702"
    },
    {
      "arxiv_id": "2405.06706v1",
      "title": "Exploring the Capabilities of Large Multimodal Models on Dense Text",
      "title_zh": "探索大型多模态模型在密集文本上的能力",
      "authors": [
        "Shuo Zhang",
        "Biao Yang",
        "Zhang Li",
        "Zhiyin Ma",
        "Yuliang Liu",
        "Xiang Bai"
      ],
      "abstract": "While large multi-modal models (LMM) have shown notable progress in\nmulti-modal tasks, their capabilities in tasks involving dense textual content\nremains to be fully explored. Dense text, which carries important information,\nis often found in documents, tables, and product descriptions. Understanding\ndense text enables us to obtain more accurate information, assisting in making\nbetter decisions. To further explore the capabilities of LMM in complex text\ntasks, we propose the DT-VQA dataset, with 170k question-answer pairs. In this\npaper, we conduct a comprehensive evaluation of GPT4V, Gemini, and various\nopen-source LMMs on our dataset, revealing their strengths and weaknesses.\nFurthermore, we evaluate the effectiveness of two strategies for LMM: prompt\nengineering and downstream fine-tuning. We find that even with automatically\nlabeled training datasets, significant improvements in model performance can be\nachieved. We hope that this research will promote the study of LMM in dense\ntext tasks. Code will be released at\nhttps://github.com/Yuliang-Liu/MultimodalOCR.",
      "tldr_zh": "该论文探讨了大型多模态模型 (LMM) 在处理密集文本任务（如文档、表格和产品描述）方面的能力，强调了理解这些文本的重要性以辅助决策。研究者提出 DT-VQA 数据集，包含 170k 问题-答案对，并对 GPT4V、Gemini 和各种开源 LMM 进行了全面评估，揭示了它们的优势和劣势。同时，评估了 prompt engineering 和 downstream fine-tuning 策略，发现使用自动标记的训练数据集能显著提升模型性能。该工作旨在促进 LMM 在密集文本任务中的研究，并提供开源代码。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2405.06706v1",
      "published_date": "2024-05-09 07:47:25 UTC",
      "updated_date": "2024-05-09 07:47:25 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T07:23:56.663932"
    },
    {
      "arxiv_id": "2405.06705v2",
      "title": "LLMs can Find Mathematical Reasoning Mistakes by Pedagogical Chain-of-Thought",
      "title_zh": "翻译失败",
      "authors": [
        "Zhuoxuan Jiang",
        "Haoyuan Peng",
        "Shanshan Feng",
        "Fan Li",
        "Dongsheng Li"
      ],
      "abstract": "Self-correction is emerging as a promising approach to mitigate the issue of\nhallucination in Large Language Models (LLMs). To facilitate effective\nself-correction, recent research has proposed mistake detection as its initial\nstep. However, current literature suggests that LLMs often struggle with\nreliably identifying reasoning mistakes when using simplistic prompting\nstrategies. To address this challenge, we introduce a unique prompting\nstrategy, termed the Pedagogical Chain-of-Thought (PedCoT), which is\nspecifically designed to guide the identification of reasoning mistakes,\nparticularly mathematical reasoning mistakes. PedCoT consists of pedagogical\nprinciples for prompts (PPP) design, two-stage interaction process (TIP) and\ngrounded PedCoT prompts, all inspired by the educational theory of the Bloom\nCognitive Model (BCM). We evaluate our approach on two public datasets\nfeaturing math problems of varying difficulty levels. The experiments\ndemonstrate that our zero-shot prompting strategy significantly outperforms\nstrong baselines. The proposed method can achieve the goal of reliable\nmathematical mistake identification and provide a foundation for automatic math\nanswer grading. The results underscore the significance of educational theory,\nserving as domain knowledge, in guiding prompting strategy design for\naddressing challenging tasks with LLMs effectively.",
      "tldr_zh": "本文提出了一种名为 Pedagogical Chain-of-Thought (PedCoT) 的提示策略，帮助 Large Language Models (LLMs) 更可靠地识别数学推理错误，以支持自校正机制。PedCoT 受 Bloom Cognitive Model (BCM) 启发，包括提示设计原则 (PPP)、两阶段互动过程 (TIP) 和 grounded 提示，旨在通过教育理论指导错误检测过程。在两个公开数学问题数据集上的实验表明，该零样本方法显著优于现有基线，提升了数学错误识别的准确性，并为自动数学答案评分提供坚实基础。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "Accepted by IJCAI 2024",
      "pdf_url": "http://arxiv.org/pdf/2405.06705v2",
      "published_date": "2024-05-09 07:37:34 UTC",
      "updated_date": "2025-03-08 15:20:34 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T07:24:08.691045"
    },
    {
      "arxiv_id": "2405.05594v1",
      "title": "Expected Work Search: Combining Win Rate and Proof Size Estimation",
      "title_zh": "翻译失败",
      "authors": [
        "Owen Randall",
        "Martin Müller",
        "Ting Han Wei",
        "Ryan Hayward"
      ],
      "abstract": "We propose Expected Work Search (EWS), a new game solving algorithm. EWS\ncombines win rate estimation, as used in Monte Carlo Tree Search, with proof\nsize estimation, as used in Proof Number Search. The search efficiency of EWS\nstems from minimizing a novel notion of Expected Work, which predicts the\nexpected computation required to solve a position. EWS outperforms traditional\nsolving algorithms on the games of Go and Hex. For Go, we present the first\nsolution to the empty 5x5 board with the commonly used positional superko\nruleset. For Hex, our algorithm solves the empty 8x8 board in under 4 minutes.\nExperiments show that EWS succeeds both with and without extensive\ndomain-specific knowledge.",
      "tldr_zh": "本研究提出了一种新的游戏求解算法 Expected Work Search (EWS)，它将 Monte Carlo Tree Search 中的胜率估计与 Proof Number Search 中的证明大小估计相结合，通过最小化 Expected Work 来预测求解位置所需的预期计算量，从而提升搜索效率。实验结果显示，EWS 在 Go 和 Hex 游戏上超过了传统算法，包括首次解决了空 5x5 Go 棋盘（使用 positional superko 规则集），并在不到 4 分钟内求解了空 8x8 Hex 棋盘。该算法无需依赖广泛的领域特定知识，即可实现高效表现。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2405.05594v1",
      "published_date": "2024-05-09 07:33:06 UTC",
      "updated_date": "2024-05-09 07:33:06 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T07:24:19.567264"
    },
    {
      "arxiv_id": "2405.05584v1",
      "title": "A Survey on Backbones for Deep Video Action Recognition",
      "title_zh": "深度视频动作识别主干网络的调查",
      "authors": [
        "Zixuan Tang",
        "Youjun Zhao",
        "Yuhang Wen",
        "Mengyuan Liu"
      ],
      "abstract": "Action recognition is a key technology in building interactive metaverses.\nWith the rapid development of deep learning, methods in action recognition have\nalso achieved great advancement. Researchers design and implement the backbones\nreferring to multiple standpoints, which leads to the diversity of methods and\nencountering new challenges. This paper reviews several action recognition\nmethods based on deep neural networks. We introduce these methods in three\nparts: 1) Two-Streams networks and their variants, which, specifically in this\npaper, use RGB video frame and optical flow modality as input; 2) 3D\nconvolutional networks, which make efforts in taking advantage of RGB modality\ndirectly while extracting different motion information is no longer necessary;\n3) Transformer-based methods, which introduce the model from natural language\nprocessing into computer vision and video understanding. We offer objective\nsights in this review and hopefully provide a reference for future research.",
      "tldr_zh": "这篇论文对深度视频动作识别中的骨干网络进行了全面调查，涵盖了基于深度神经网络的多种方法。论文将这些方法分为三类：Two-Streams networks及其变体（使用RGB视频帧和光流作为输入）、3D convolutional networks（直接从RGB模态提取运动信息）和Transformer-based methods（将自然语言处理模型应用于计算机视觉和视频理解）。通过客观分析，这些方法的发展有助于解决动作识别的多样性和挑战，并为未来研究提供参考。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "This paper has been accepted by ICME workshop",
      "pdf_url": "http://arxiv.org/pdf/2405.05584v1",
      "published_date": "2024-05-09 07:20:36 UTC",
      "updated_date": "2024-05-09 07:20:36 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T07:24:31.597552"
    },
    {
      "arxiv_id": "2405.05581v1",
      "title": "One vs. Many: Comprehending Accurate Information from Multiple Erroneous and Inconsistent AI Generations",
      "title_zh": "翻译失败",
      "authors": [
        "Yoonjoo Lee",
        "Kihoon Son",
        "Tae Soo Kim",
        "Jisu Kim",
        "John Joon Young Chung",
        "Eytan Adar",
        "Juho Kim"
      ],
      "abstract": "As Large Language Models (LLMs) are nondeterministic, the same input can\ngenerate different outputs, some of which may be incorrect or hallucinated. If\nrun again, the LLM may correct itself and produce the correct answer.\nUnfortunately, most LLM-powered systems resort to single results which, correct\nor not, users accept. Having the LLM produce multiple outputs may help identify\ndisagreements or alternatives. However, it is not obvious how the user will\ninterpret conflicts or inconsistencies. To this end, we investigate how users\nperceive the AI model and comprehend the generated information when they\nreceive multiple, potentially inconsistent, outputs. Through a preliminary\nstudy, we identified five types of output inconsistencies. Based on these\ncategories, we conducted a study (N=252) in which participants were given one\nor more LLM-generated passages to an information-seeking question. We found\nthat inconsistency within multiple LLM-generated outputs lowered the\nparticipants' perceived AI capacity, while also increasing their comprehension\nof the given information. Specifically, we observed that this positive effect\nof inconsistencies was most significant for participants who read two passages,\ncompared to those who read three. Based on these findings, we present design\nimplications that, instead of regarding LLM output inconsistencies as a\ndrawback, we can reveal the potential inconsistencies to transparently indicate\nthe limitations of these models and promote critical LLM usage.",
      "tldr_zh": "这篇论文探讨了大型语言模型(LLMs)生成多个不一致输出时，用户如何感知AI能力和理解信息的问题。研究者通过初步分析识别了五种输出不一致类型，并进行用户实验(N=252)，发现不一致降低了用户对AI的信任，但显著提高了信息理解，尤其在提供两个输出时。最终，论文提出设计启示，建议主动揭示LLMs的不一致性，以提升模型透明度并促进批判性使用。",
      "categories": [
        "cs.HC",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.HC",
      "comment": "Accepted to FAccT 2024",
      "pdf_url": "http://arxiv.org/pdf/2405.05581v1",
      "published_date": "2024-05-09 07:12:45 UTC",
      "updated_date": "2024-05-09 07:12:45 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T07:24:43.586692"
    },
    {
      "arxiv_id": "2405.05572v2",
      "title": "From Human Judgements to Predictive Models: Unravelling Acceptability in Code-Mixed Sentences",
      "title_zh": "翻译失败",
      "authors": [
        "Prashant Kodali",
        "Anmol Goel",
        "Likhith Asapu",
        "Vamshi Krishna Bonagiri",
        "Anirudh Govil",
        "Monojit Choudhury",
        "Ponnurangam Kumaraguru",
        "Manish Shrivastava"
      ],
      "abstract": "Current computational approaches for analysing or generating code-mixed\nsentences do not explicitly model ``naturalness'' or ``acceptability'' of\ncode-mixed sentences, but rely on training corpora to reflect distribution of\nacceptable code-mixed sentences. Modelling human judgement for the\nacceptability of code-mixed text can help in distinguishing natural code-mixed\ntext and enable quality-controlled generation of code-mixed text. To this end,\nwe construct Cline - a dataset containing human acceptability judgements for\nEnglish-Hindi~(en-hi) code-mixed text. Cline is the largest of its kind with\n16,642 sentences, consisting of samples sourced from two sources: synthetically\ngenerated code-mixed text and samples collected from online social media. Our\nanalysis establishes that popular code-mixing metrics such as CMI, Number of\nSwitch Points, Burstines, which are used to filter/curate/compare code-mixed\ncorpora have low correlation with human acceptability judgements, underlining\nthe necessity of our dataset. Experiments using Cline demonstrate that simple\nMultilayer Perceptron (MLP) models when trained solely using code-mixing\nmetrics as features are outperformed by fine-tuned pre-trained Multilingual\nLarge Language Models (MLLMs). Specifically, among Encoder models XLM-Roberta\nand Bernice outperform IndicBERT across different configurations. Among\nEncoder-Decoder models, mBART performs better than mT5, however Encoder-Decoder\nmodels are not able to outperform Encoder-only models. Decoder-only models\nperform the best when compared to all other MLLMS, with Llama 3.2 - 3B models\noutperforming similarly sized Qwen, Phi models. Comparison with zero and\nfewshot capabilitites of ChatGPT show that MLLMs fine-tuned on larger data\noutperform ChatGPT, providing scope for improvement in code-mixed tasks.\nZero-shot transfer from En-Hi to En-Te acceptability judgments are better than\nrandom baselines.",
      "tldr_zh": "该论文构建了Cline数据集，这是最大的代码混合可接受性数据集，包含16,642个英文-印地语(en-hi)句子的人类判断，用于区分自然代码混合文本并支持质量控制生成。研究发现，传统指标如CMI、Number of Switch Points和Burstiness与人类判断的相关性较低，突显了数据集的必要性。实验结果显示，微调的Multilingual Large Language Models(MLLMs)如XLM-Roberta和Llama 3.2-3B模型在预测任务中优于简单Multilayer Perceptron(MLP)模型和ChatGPT，而Decoder-only模型整体表现最佳，并证明了从En-Hi到En-Te的零样本转移能力优于随机基线。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2405.05572v2",
      "published_date": "2024-05-09 06:40:39 UTC",
      "updated_date": "2025-05-05 14:51:58 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T07:24:57.668136"
    },
    {
      "arxiv_id": "2405.05553v3",
      "title": "Towards Robust Physical-world Backdoor Attacks on Lane Detection",
      "title_zh": "翻译失败",
      "authors": [
        "Xinwei Zhang",
        "Aishan Liu",
        "Tianyuan Zhang",
        "Siyuan Liang",
        "Xianglong Liu"
      ],
      "abstract": "Deep learning-based lane detection (LD) plays a critical role in autonomous\ndriving systems, such as adaptive cruise control. However, it is vulnerable to\nbackdoor attacks. Existing backdoor attack methods on LD exhibit limited\neffectiveness in dynamic real-world scenarios, primarily because they fail to\nconsider dynamic scene factors, including changes in driving perspectives\n(e.g., viewpoint transformations) and environmental conditions (e.g., weather\nor lighting changes). To tackle this issue, this paper introduces BadLANE, a\ndynamic scene adaptation backdoor attack for LD designed to withstand changes\nin real-world dynamic scene factors. To address the challenges posed by\nchanging driving perspectives, we propose an amorphous trigger pattern composed\nof shapeless pixels. This trigger design allows the backdoor to be activated by\nvarious forms or shapes of mud spots or pollution on the road or lens, enabling\nadaptation to changes in vehicle observation viewpoints during driving. To\nmitigate the effects of environmental changes, we design a meta-learning\nframework to train meta-generators tailored to different environmental\nconditions. These generators produce meta-triggers that incorporate diverse\nenvironmental information, such as weather or lighting conditions, as the\ninitialization of the trigger patterns for backdoor implantation, thus enabling\nadaptation to dynamic environments. Extensive experiments on various commonly\nused LD models in both digital and physical domains validate the effectiveness\nof our attacks, outperforming other baselines significantly (+25.15% on average\nin Attack Success Rate). Our codes will be available upon paper publication.",
      "tldr_zh": "本研究针对深度学习-based 车道检测 (LD) 在自动驾驶系统中的易受后门攻击 (backdoor attacks) 问题，提出了一种动态场景适应攻击框架 BadLANE，以应对真实世界中驾驶视角变换 (e.g., viewpoint transformations) 和环境条件变化 (e.g., weather or lighting changes) 的挑战。BadLANE 采用无定形触发模式 (amorphous trigger pattern)，由无形状像素组成，能适应路面或镜头上的泥点或污染；同时，通过元学习框架 (meta-learning framework) 训练元生成器 (meta-generators)，生成融入环境信息的元触发器 (meta-triggers)，从而提升攻击的鲁棒性。在各种常用 LD 模型上的实验验证显示，BadLANE 在数字和物理领域平均将攻击成功率提高了 25.15%，显著优于基线方法。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2405.05553v3",
      "published_date": "2024-05-09 05:23:34 UTC",
      "updated_date": "2024-07-01 10:27:42 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T07:25:08.931859"
    },
    {
      "arxiv_id": "2405.08007v1",
      "title": "People cannot distinguish GPT-4 from a human in a Turing test",
      "title_zh": "人们无法在图灵测试中区分 GPT-4 与人类",
      "authors": [
        "Cameron R. Jones",
        "Benjamin K. Bergen"
      ],
      "abstract": "We evaluated 3 systems (ELIZA, GPT-3.5 and GPT-4) in a randomized,\ncontrolled, and preregistered Turing test. Human participants had a 5 minute\nconversation with either a human or an AI, and judged whether or not they\nthought their interlocutor was human. GPT-4 was judged to be a human 54% of the\ntime, outperforming ELIZA (22%) but lagging behind actual humans (67%). The\nresults provide the first robust empirical demonstration that any artificial\nsystem passes an interactive 2-player Turing test. The results have\nimplications for debates around machine intelligence and, more urgently,\nsuggest that deception by current AI systems may go undetected. Analysis of\nparticipants' strategies and reasoning suggests that stylistic and\nsocio-emotional factors play a larger role in passing the Turing test than\ntraditional notions of intelligence.",
      "tldr_zh": "本研究通过随机控制的预注册实验，评估了 ELIZA、GPT-3.5 和 GPT-4 在交互式图灵测试中的表现，参与者与人类或 AI 进行 5 分钟对话后判断对方是否为人类。结果显示，GPT-4 被误认为是人类的比例达 54%，优于 ELIZA 的 22% 但低于人类的 67%，这首次提供了 AI 系统通过 2 玩家图灵测试的可靠证据。该发现对机器智能辩论有重要启示，并警告当前 AI 系统可能在不知不觉中欺骗人类；分析表明，风格和社交情感因素比传统智能概念更关键。",
      "categories": [
        "cs.HC",
        "cs.AI"
      ],
      "primary_category": "cs.HC",
      "comment": "23 pages, 13 figures",
      "pdf_url": "http://arxiv.org/pdf/2405.08007v1",
      "published_date": "2024-05-09 04:14:09 UTC",
      "updated_date": "2024-05-09 04:14:09 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T07:25:19.699851"
    },
    {
      "arxiv_id": "2405.05993v1",
      "title": "Precision Rehabilitation for Patients Post-Stroke based on Electronic Health Records and Machine Learning",
      "title_zh": "基于电子健康记录和机器学习的卒中后患者精准康复",
      "authors": [
        "Fengyi Gao",
        "Xingyu Zhang",
        "Sonish Sivarajkumar",
        "Parker Denny",
        "Bayan Aldhahwani",
        "Shyam Visweswaran",
        "Ryan Shi",
        "William Hogan",
        "Allyn Bove",
        "Yanshan Wang"
      ],
      "abstract": "In this study, we utilized statistical analysis and machine learning methods\nto examine whether rehabilitation exercises can improve patients post-stroke\nfunctional abilities, as well as forecast the improvement in functional\nabilities. Our dataset is patients' rehabilitation exercises and demographic\ninformation recorded in the unstructured electronic health records (EHRs) data\nand free-text rehabilitation procedure notes. We collected data for 265 stroke\npatients from the University of Pittsburgh Medical Center. We employed a\npre-existing natural language processing (NLP) algorithm to extract data on\nrehabilitation exercises and developed a rule-based NLP algorithm to extract\nActivity Measure for Post-Acute Care (AM-PAC) scores, covering basic mobility\n(BM) and applied cognitive (AC) domains, from procedure notes. Changes in\nAM-PAC scores were classified based on the minimal clinically important\ndifference (MCID), and significance was assessed using Friedman and Wilcoxon\ntests. To identify impactful exercises, we used Chi-square tests, Fisher's\nexact tests, and logistic regression for odds ratios. Additionally, we\ndeveloped five machine learning models-logistic regression (LR), Adaboost\n(ADB), support vector machine (SVM), gradient boosting (GB), and random forest\n(RF)-to predict outcomes in functional ability. Statistical analyses revealed\nsignificant associations between functional improvements and specific\nexercises. The RF model achieved the best performance in predicting functional\noutcomes. In this study, we identified three rehabilitation exercises that\nsignificantly contributed to patient post-stroke functional ability improvement\nin the first two months. Additionally, the successful application of a machine\nlearning model to predict patient-specific functional outcomes underscores the\npotential for precision rehabilitation.",
      "tldr_zh": "本研究利用统计分析和机器学习方法，基于 Electronic Health Records (EHRs) 数据，分析中风后患者的康复训练对功能能力的影响，并预测改善效果。研究团队从 265 名患者的非结构化数据和程序笔记中，使用预存的 Natural Language Processing (NLP) 算法提取康复训练信息，并开发规则-based NLP 算法提取 Activity Measure for Post-Acute Care (AM-PAC) 分数，然后通过 Friedman 和 Wilcoxon 测试评估显著性。统计分析显示，特定训练（如三种关键练习）与功能改善显著相关，且 Random Forest (RF) 模型在预测功能结果方面表现最佳。总体而言，此研究证实了机器学习模型在实现患者个性化精确康复方面的潜力。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2405.05993v1",
      "published_date": "2024-05-09 04:06:44 UTC",
      "updated_date": "2024-05-09 04:06:44 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T07:25:33.097744"
    },
    {
      "arxiv_id": "2405.05523v1",
      "title": "Prompt When the Animal is: Temporal Animal Behavior Grounding with Positional Recovery Training",
      "title_zh": "翻译失败",
      "authors": [
        "Sheng Yan",
        "Xin Du",
        "Zongying Li",
        "Yi Wang",
        "Hongcang Jin",
        "Mengyuan Liu"
      ],
      "abstract": "Temporal grounding is crucial in multimodal learning, but it poses challenges\nwhen applied to animal behavior data due to the sparsity and uniform\ndistribution of moments. To address these challenges, we propose a novel\nPositional Recovery Training framework (Port), which prompts the model with the\nstart and end times of specific animal behaviors during training. Specifically,\nPort enhances the baseline model with a Recovering part to predict flipped\nlabel sequences and align distributions with a Dual-alignment method. This\nallows the model to focus on specific temporal regions prompted by ground-truth\ninformation. Extensive experiments on the Animal Kingdom dataset demonstrate\nthe effectiveness of Port, achieving an IoU@0.3 of 38.52. It emerges as one of\nthe top performers in the sub-track of MMVRAC in ICME 2024 Grand Challenges.",
      "tldr_zh": "这篇论文针对Temporal grounding在动物行为数据中的挑战（如时刻的稀疏性和均匀分布），提出了一种新型Positional Recovery Training框架（Port），通过在训练时使用动物行为的起始和结束时间提示模型，并添加Recovering part预测翻转标签序列以及Dual-alignment方法对齐分布，以帮助模型专注于特定时间区域。Port框架显著提升了基线模型的性能，在Animal Kingdom数据集上实现了IoU@0.3为38.52的成绩，并在ICME 2024 Grand Challenges的MMVRAC子赛道中脱颖而出，成为顶级表现者。该方法为多模态学习中的时间定位任务提供了有效的解决方案。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "Accepted by ICMEW 2024. arXiv admin note: text overlap with\n  arXiv:2404.13657",
      "pdf_url": "http://arxiv.org/pdf/2405.05523v1",
      "published_date": "2024-05-09 03:23:47 UTC",
      "updated_date": "2024-05-09 03:23:47 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T07:25:45.842861"
    },
    {
      "arxiv_id": "2405.05512v4",
      "title": "Characteristic Learning for Provable One Step Generation",
      "title_zh": "翻译失败",
      "authors": [
        "Zhao Ding",
        "Chenguang Duan",
        "Yuling Jiao",
        "Ruoxuan Li",
        "Jerry Zhijian Yang",
        "Pingwen Zhang"
      ],
      "abstract": "We propose the characteristic generator, a novel one-step generative model\nthat combines the efficiency of sampling in Generative Adversarial Networks\n(GANs) with the stable performance of flow-based models. Our model is driven by\ncharacteristics, along which the probability density transport can be described\nby ordinary differential equations (ODEs). Specifically, We estimate the\nvelocity field through nonparametric regression and utilize Euler method to\nsolve the probability flow ODE, generating a series of discrete approximations\nto the characteristics. We then use a deep neural network to fit these\ncharacteristics, ensuring a one-step mapping that effectively pushes the prior\ndistribution towards the target distribution. In the theoretical aspect, we\nanalyze the errors in velocity matching, Euler discretization, and\ncharacteristic fitting to establish a non-asymptotic convergence rate for the\ncharacteristic generator in 2-Wasserstein distance. To the best of our\nknowledge, this is the first thorough analysis for simulation-free one step\ngenerative models. Additionally, our analysis refines the error analysis of\nflow-based generative models in prior works. We apply our method on both\nsynthetic and real datasets, and the results demonstrate that the\ncharacteristic generator achieves high generation quality with just a single\nevaluation of neural network.",
      "tldr_zh": "本研究提出了一种名为 Characteristic Generator 的单步生成模型，结合 Generative Adversarial Networks (GANs) 的高效采样和 flow-based models 的稳定性能，通过 characteristics 驱动概率密度传输。模型使用非参数回归估计速度场，并采用 Euler method 求解概率流 ODE（ordinary differential equations），再通过深度神经网络拟合 characteristics，实现从先验分布到目标分布的单步映射。在理论上，该方法分析了速度匹配、Euler 离散化和 characteristic 拟合的错误，建立在 2-Wasserstein distance 上的非渐近收敛率，这是首个对 simulation-free one-step generative models 的彻底分析；实验结果显示，该模型在合成和真实数据集上实现了高生成质量，仅需单次神经网络评估。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.NA",
        "math.NA",
        "math.ST",
        "stat.TH"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2405.05512v4",
      "published_date": "2024-05-09 02:41:42 UTC",
      "updated_date": "2024-07-16 15:41:53 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T07:25:58.826013"
    },
    {
      "arxiv_id": "2405.05508v2",
      "title": "Redefining Information Retrieval of Structured Database via Large Language Models",
      "title_zh": "通过大型语言模型重新定义结构化数据库的信息检索",
      "authors": [
        "Mingzhu Wang",
        "Yuzhe Zhang",
        "Qihang Zhao",
        "Junyi Yang",
        "Hong Zhang"
      ],
      "abstract": "Retrieval augmentation is critical when Language Models (LMs) exploit\nnon-parametric knowledge related to the query through external knowledge bases\nbefore reasoning. The retrieved information is incorporated into LMs as context\nalongside the query, enhancing the reliability of responses towards factual\nquestions. Prior researches in retrieval augmentation typically follow a\nretriever-generator paradigm. In this context, traditional retrievers encounter\nchallenges in precisely and seamlessly extracting query-relevant information\nfrom knowledge bases. To address this issue, this paper introduces a novel\nretrieval augmentation framework called ChatLR that primarily employs the\npowerful semantic understanding ability of Large Language Models (LLMs) as\nretrievers to achieve precise and concise information retrieval. Additionally,\nwe construct an LLM-based search and question answering system tailored for the\nfinancial domain by fine-tuning LLM on two tasks including Text2API and API-ID\nrecognition. Experimental results demonstrate the effectiveness of ChatLR in\naddressing user queries, achieving an overall information retrieval accuracy\nexceeding 98.8\\%.",
      "tldr_zh": "本文重新定义了结构化数据库的信息检索，提出了一种名为 ChatLR 的新型检索增强框架，利用 Large Language Models (LLMs) 的强大语义理解能力作为检索器，以实现更精确和简洁的信息提取。ChatLR 解决了传统 retriever-generator 范式中检索器在提取查询相关信息时的挑战，并通过在 Text2API 和 API-ID recognition 任务上微调 LLM，构建了一个针对金融领域的搜索和问答系统。实验结果表明，该框架在处理用户查询时，信息检索准确率超过 98.8%，显著提升了语言模型的可靠性和响应质量。",
      "categories": [
        "cs.IR",
        "cs.AI"
      ],
      "primary_category": "cs.IR",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2405.05508v2",
      "published_date": "2024-05-09 02:37:53 UTC",
      "updated_date": "2024-11-19 01:15:44 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T07:26:11.276965"
    },
    {
      "arxiv_id": "2405.05991v1",
      "title": "Agent-oriented Joint Decision Support for Data Owners in Auction-based Federated Learning",
      "title_zh": "翻译失败",
      "authors": [
        "Xiaoli Tang",
        "Han Yu",
        "Xiaoxiao Li"
      ],
      "abstract": "Auction-based Federated Learning (AFL) has attracted extensive research\ninterest due to its ability to motivate data owners (DOs) to join FL through\neconomic means. While many existing AFL methods focus on providing decision\nsupport to model users (MUs) and the AFL auctioneer, decision support for data\nowners remains open. To bridge this gap, we propose a first-of-its-kind\nagent-oriented joint Pricing, Acceptance and Sub-delegation decision support\napproach for data owners in AFL (PAS-AFL). By considering a DO's current\nreputation, pending FL tasks, willingness to train FL models, and its trust\nrelationships with other DOs, it provides a systematic approach for a DO to\nmake joint decisions on AFL bid acceptance, task sub-delegation and pricing\nbased on Lyapunov optimization to maximize its utility. It is the first to\nenable each DO to take on multiple FL tasks simultaneously to earn higher\nincome for DOs and enhance the throughput of FL tasks in the AFL ecosystem.\nExtensive experiments based on six benchmarking datasets demonstrate\nsignificant advantages of PAS-AFL compared to six alternative strategies,\nbeating the best baseline by 28.77% and 2.64% on average in terms of utility\nand test accuracy of the resulting FL models, respectively.",
      "tldr_zh": "该论文提出 PAS-AFL，一种首创的代理导向联合决策支持方法，针对 Auction-based Federated Learning (AFL) 中的数据所有者 (DOs)，帮助他们处理定价、接受和子委托决策，以桥接现有方法的空白。方法考虑 DO 的声誉、待处理 FL 任务、训练意愿以及与其他 DO 的信任关系，通过 Lyapunov 优化最大化效用，并首次允许 DO 同时处理多个 FL 任务，以提升收入和 FL 生态系统的任务吞吐量。实验基于六个基准数据集表明，PAS-AFL 比六种备选策略在效用上平均提高 28.77%，在 FL 模型测试准确率上提高 2.64%。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.GT"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2405.05991v1",
      "published_date": "2024-05-09 02:35:46 UTC",
      "updated_date": "2024-05-09 02:35:46 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T07:26:23.360785"
    },
    {
      "arxiv_id": "2405.05990v2",
      "title": "Special Characters Attack: Toward Scalable Training Data Extraction From Large Language Models",
      "title_zh": "翻译失败",
      "authors": [
        "Yang Bai",
        "Ge Pei",
        "Jindong Gu",
        "Yong Yang",
        "Xingjun Ma"
      ],
      "abstract": "Large language models (LLMs) have achieved remarkable performance on a wide\nrange of tasks. However, recent studies have shown that LLMs can memorize\ntraining data and simple repeated tokens can trick the model to leak the data.\nIn this paper, we take a step further and show that certain special characters\nor their combinations with English letters are stronger memory triggers,\nleading to more severe data leakage. The intuition is that, since LLMs are\ntrained with massive data that contains a substantial amount of special\ncharacters (e.g. structural symbols {, } of JSON files, and @, # in emails and\nonline posts), the model may memorize the co-occurrence between these special\ncharacters and the raw texts. This motivates us to propose a simple but\neffective Special Characters Attack (SCA) to induce training data leakage. Our\nexperiments verify the high effectiveness of SCA against state-of-the-art LLMs:\nthey can leak diverse training data, such as code corpus, web pages, and\npersonally identifiable information, and sometimes generate non-stop outputs as\na byproduct. We further show that the composition of the training data corpus\ncan be revealed by inspecting the leaked data -- one crucial piece of\ninformation for pre-training high-performance LLMs. Our work can help\nunderstand the sensitivity of LLMs to special characters and identify potential\nareas for improvement.",
      "tldr_zh": "这篇论文探讨了大型语言模型（LLMs）对特殊字符的敏感性，提出了一种名为Special Characters Attack (SCA) 的简单攻击方法，利用特殊字符（如{、}、@、#）及其与英文字母的组合作为记忆触发器，诱导模型泄露训练数据。SCA 能够有效提取多样化的训练数据，包括代码语料、网页内容和个人信息，有时还会导致模型产生非停止输出。实验验证了 SCA 的高效率，与传统方法相比显著提升了数据泄露效果，并通过分析泄露数据揭示了训练语料的组成。该研究有助于加深对 LLMs 安全问题的理解，并为模型改进提供潜在方向。",
      "categories": [
        "cs.CR",
        "cs.AI",
        "cs.CL",
        "cs.LG"
      ],
      "primary_category": "cs.CR",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2405.05990v2",
      "published_date": "2024-05-09 02:35:32 UTC",
      "updated_date": "2024-05-20 14:40:03 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T07:26:35.172079"
    },
    {
      "arxiv_id": "2405.05499v2",
      "title": "Multi-Scale Dilated Convolution Network for Long-Term Time Series Forecasting",
      "title_zh": "多尺度空洞卷积网络用于长期时间序列预测",
      "authors": [
        "Feifei Li",
        "Suhan Guo",
        "Feng Han",
        "Jian Zhao",
        "Furao Shen"
      ],
      "abstract": "Accurate forecasting of long-term time series has important applications for\ndecision making and planning. However, it remains challenging to capture the\nlong-term dependencies in time series data. To better extract long-term\ndependencies, We propose Multi Scale Dilated Convolution Network (MSDCN), a\nmethod that utilizes a shallow dilated convolution architecture to capture the\nperiod and trend characteristics of long time series. We design different\nconvolution blocks with exponentially growing dilations and varying kernel\nsizes to sample time series data at different scales. Furthermore, we utilize\ntraditional autoregressive model to capture the linear relationships within the\ndata. To validate the effectiveness of the proposed approach, we conduct\nexperiments on eight challenging long-term time series forecasting benchmark\ndatasets. The experimental results show that our approach outperforms the prior\nstate-of-the-art approaches and shows significant inference speed improvements\ncompared to several strong baseline methods.",
      "tldr_zh": "本论文提出了一种名为 Multi-Scale Dilated Convolution Network (MSDCN) 的方法，用于解决长期时间序列预测中捕捉长期依赖关系的挑战。MSDCN 采用浅层 dilated convolution 架构，设计了不同卷积块（具有指数增长的 dilation 和可变 kernel sizes），以在多尺度下采样时间序列数据，并结合传统 autoregressive 模型捕捉线性关系。通过在八个基准数据集上的实验，该方法优于现有最先进方法，并在推理速度上实现了显著提升。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2405.05499v2",
      "published_date": "2024-05-09 02:11:01 UTC",
      "updated_date": "2024-05-14 07:52:01 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T07:26:46.003299"
    },
    {
      "arxiv_id": "2405.05493v1",
      "title": "Parameter-Efficient Fine-Tuning With Adapters",
      "title_zh": "基于适配器的参数高效微调",
      "authors": [
        "Keyu Chen",
        "Yuan Pang",
        "Zi Yang"
      ],
      "abstract": "In the arena of language model fine-tuning, the traditional approaches, such\nas Domain-Adaptive Pretraining (DAPT) and Task-Adaptive Pretraining (TAPT),\nalthough effective, but computational intensive. This research introduces a\nnovel adaptation method utilizing the UniPELT framework as a base and added a\nPromptTuning Layer, which significantly reduces the number of trainable\nparameters while maintaining competitive performance across various benchmarks.\nOur method employs adapters, which enable efficient transfer of pretrained\nmodels to new tasks with minimal retraining of the base model parameters. We\nevaluate our approach using three diverse datasets: the GLUE benchmark, a\ndomain-specific dataset comprising four distinct areas, and the Stanford\nQuestion Answering Dataset 1.1 (SQuAD). Our results demonstrate that our\ncustomized adapter-based method achieves performance comparable to full model\nfine-tuning, DAPT+TAPT and UniPELT strategies while requiring fewer or\nequivalent amount of parameters. This parameter efficiency not only alleviates\nthe computational burden but also expedites the adaptation process. The study\nunderlines the potential of adapters in achieving high performance with\nsignificantly reduced resource consumption, suggesting a promising direction\nfor future research in parameter-efficient fine-tuning.",
      "tldr_zh": "本研究提出了一种基于 UniPELT 框架并添加 PromptTuning Layer 的新颖参数高效微调方法，利用 Adapters 显著减少可训练参数，同时保持在各种任务上的竞争性能，以解决传统方法如 DAPT 和 TAPT 的计算密集问题。该方法通过在 GLUE 基准、特定领域数据集和 SQuAD 上进行评估，证明其性能与全模型微调、DAPT+TAPT 和 UniPELT 相当，但所需参数更少。总体而言，这种 Adapters 策略不仅降低了计算负担并加快了模型适应过程，还为未来参数高效微调研究提供了有前景的方向。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2405.05493v1",
      "published_date": "2024-05-09 01:40:38 UTC",
      "updated_date": "2024-05-09 01:40:38 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T07:26:58.374878"
    },
    {
      "arxiv_id": "2405.05492v3",
      "title": "A logifold structure on measure space",
      "title_zh": "翻译失败",
      "authors": [
        "Inkee Jung",
        "Siu-Cheong Lau"
      ],
      "abstract": "In this paper,we develop a local-to-global and measure-theoretical approach\nto understand datasets. The idea is to take network models with restricted\ndomains as local charts of datasets. We develop the mathematical foundations\nfor these structures, and show in experiments how it can be used to find fuzzy\ndomains and to improve accuracy in data classification problems.",
      "tldr_zh": "本文提出了一种在度量空间（measure space）上定义的 logifold 结构，通过局部到全局的度量理论方法来理解数据集，将受限域的网络模型视为数据集的局部图表，并开发了其数学基础。  \n这种方法允许识别模糊域（fuzzy domains），并在实验中证明了其在数据分类问题中的准确性提升。  \n总体上，该结构为数据集分析提供了新的框架，有助于改进机器学习应用的鲁棒性。",
      "categories": [
        "math.DG",
        "cs.AI",
        "cs.LG",
        "math.PR",
        "55N31, 53Z50, 68T07, 68T09, 60A10, 81P45, 94D05"
      ],
      "primary_category": "math.DG",
      "comment": "37 pages, 5 figures",
      "pdf_url": "http://arxiv.org/pdf/2405.05492v3",
      "published_date": "2024-05-09 01:38:38 UTC",
      "updated_date": "2025-02-01 01:44:16 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T07:27:10.135857"
    },
    {
      "arxiv_id": "2407.00024v1",
      "title": "LMVD: A Large-Scale Multimodal Vlog Dataset for Depression Detection in the Wild",
      "title_zh": "翻译失败",
      "authors": [
        "Lang He",
        "Kai Chen",
        "Junnan Zhao",
        "Yimeng Wang",
        "Ercheng Pei",
        "Haifeng Chen",
        "Jiewei Jiang",
        "Shiqing Zhang",
        "Jie Zhang",
        "Zhongmin Wang",
        "Tao He",
        "Prayag Tiwari"
      ],
      "abstract": "Depression can significantly impact many aspects of an individual's life,\nincluding their personal and social functioning, academic and work performance,\nand overall quality of life. Many researchers within the field of affective\ncomputing are adopting deep learning technology to explore potential patterns\nrelated to the detection of depression. However, because of subjects' privacy\nprotection concerns, that data in this area is still scarce, presenting a\nchallenge for the deep discriminative models used in detecting depression. To\nnavigate these obstacles, a large-scale multimodal vlog dataset (LMVD), for\ndepression recognition in the wild is built. In LMVD, which has 1823 samples\nwith 214 hours of the 1475 participants captured from four multimedia platforms\n(Sina Weibo, Bilibili, Tiktok, and YouTube). A novel architecture termed\nMDDformer to learn the non-verbal behaviors of individuals is proposed.\nExtensive validations are performed on the LMVD dataset, demonstrating superior\nperformance for depression detection. We anticipate that the LMVD will\ncontribute a valuable function to the depression detection community. The data\nand code will released at the link: https://github.com/helang818/LMVD/.",
      "tldr_zh": "本研究针对抑郁症检测中数据稀缺的挑战，构建了一个大型多模态 vlog 数据集（LMVD），包含来自 Sina Weibo、Bilibili、Tiktok 和 YouTube 等平台的 1823 个样本、214 小时视频和 1475 名参与者，用于野外环境下的抑郁识别。研究提出了一种新型架构 MDDformer，专注于学习个体的非语言行为，以提升检测性能。在 LMVD 数据集上的广泛验证显示，该方法在抑郁症检测方面表现出优越的性能，并计划公开数据和代码（https://github.com/helang818/LMVD/），为情感计算社区提供宝贵资源。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.MM"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2407.00024v1",
      "published_date": "2024-05-09 01:27:10 UTC",
      "updated_date": "2024-05-09 01:27:10 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T07:27:22.428722"
    },
    {
      "arxiv_id": "2405.05480v4",
      "title": "FloorSet -- a VLSI Floorplanning Dataset with Design Constraints of Real-World SoCs",
      "title_zh": "翻译失败",
      "authors": [
        "Uday Mallappa",
        "Hesham Mostafa",
        "Mikhail Galkin",
        "Mariano Phielipp",
        "Somdeb Majumdar"
      ],
      "abstract": "Floorplanning for systems-on-a-chip (SoCs) and its sub-systems is a crucial\nand non-trivial step of the physical design flow. It represents a difficult\ncombinatorial optimization problem. A typical large scale SoC with 120\npartitions generates a search-space of nearly 10E250. As novel machine learning\n(ML) approaches emerge to tackle such problems, there is a growing need for a\nmodern benchmark that comprises a large training dataset and performance\nmetrics that better reflect real-world constraints and objectives compared to\nexisting benchmarks. To address this need, we present FloorSet -- two\ncomprehensive datasets of synthetic fixed-outline floorplan layouts that\nreflect the distribution of real SoCs. Each dataset has 1M training samples and\n100 test samples where each sample is a synthetic floor-plan. FloorSet-Prime\ncomprises fully-abutted rectilinear partitions and near-optimal wire-length. A\nsimplified dataset that reflects early design phases, FloorSet-Lite comprises\nrectangular partitions, with under 5 percent white-space and near-optimal\nwire-length. Both datasets define hard constraints seen in modern design flows\nsuch as shape constraints, edge-affinity, grouping constraints, and\npre-placement constraints. FloorSet is intended to spur fundamental research on\nlarge-scale constrained optimization problems. Crucially, FloorSet alleviates\nthe core issue of reproducibility in modern ML driven solutions to such\nproblems. FloorSet is available as an open-source repository for the research\ncommunity.",
      "tldr_zh": "该论文介绍了 FloorSet，这是一个针对 VLSI Floorplanning 的综合数据集，旨在模拟真实 SoC 的设计约束，以解决大规模组合优化问题的基准需求。数据集包括两个子集：FloorSet-Prime 和 FloorSet-Lite，每个子集包含 1M 训练样本和 100 测试样本，前者聚焦于完全邻接的矩形分区和近优化的线长，后者则采用矩形分区并保持少于 5% 的白空间。FloorSet 纳入了真实设计中的硬约束，如形状约束、边缘亲和性、分组约束和预放置约束，促进 ML 驱动解决方案的研究，并通过开源仓库提升实验的可重复性。",
      "categories": [
        "cs.AR",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.AR",
      "comment": "10 pages, 11 figures",
      "pdf_url": "http://arxiv.org/pdf/2405.05480v4",
      "published_date": "2024-05-09 00:37:56 UTC",
      "updated_date": "2024-08-01 22:57:53 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T07:27:35.564935"
    },
    {
      "arxiv_id": "2405.06704v1",
      "title": "Enhanced Review Detection and Recognition: A Platform-Agnostic Approach with Application to Online Commerce",
      "title_zh": "增强评论检测与识别：一种平台无关的方法及其在线商业应用",
      "authors": [
        "Priyabrata Karmakar",
        "John Hawkins"
      ],
      "abstract": "Online commerce relies heavily on user generated reviews to provide unbiased\ninformation about products that they have not physically seen. The importance\nof reviews has attracted multiple exploitative online behaviours and requires\nmethods for monitoring and detecting reviews. We present a machine learning\nmethodology for review detection and extraction, and demonstrate that it\ngeneralises for use across websites that were not contained in the training\ndata. This method promises to drive applications for automatic detection and\nevaluation of reviews, regardless of their source. Furthermore, we showcase the\nversatility of our method by implementing and discussing three key applications\nfor analysing reviews: Sentiment Inconsistency Analysis, which detects and\nfilters out unreliable reviews based on inconsistencies between ratings and\ncomments; Multi-language support, enabling the extraction and translation of\nreviews from various languages without relying on HTML scraping; and Fake\nreview detection, achieved by integrating a trained NLP model to identify and\ndistinguish between genuine and fake reviews.",
      "tldr_zh": "这篇论文提出了一种平台无关的机器学习方法，用于检测和提取在线评论，并证明其能够在训练数据中未包含的网站上实现泛化，从而驱动跨源评论的自动评估。方法的关键优势在于其应用潜力，包括Sentiment Inconsistency Analysis（检测评分与评论不一致的不可靠评论）、Multi-language support（提取和翻译多种语言评论，而不依赖HTML刮取），以及Fake review detection（通过训练的NLP模型识别真假评论）。总体而言，该方法提升了在线商务中评论的监测和可靠性，为防范 exploitative behaviors 提供了实用工具。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2405.06704v1",
      "published_date": "2024-05-09 00:32:22 UTC",
      "updated_date": "2024-05-09 00:32:22 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T07:27:48.058772"
    },
    {
      "arxiv_id": "2405.05989v2",
      "title": "Clustering-based Multitasking Deep Neural Network for Solar Photovoltaics Power Generation Prediction",
      "title_zh": "基于聚类的多任务深度神经网络用于太阳能光伏发电预测",
      "authors": [
        "Hui Song",
        "Zheng Miao",
        "Ali Babalhavaeji",
        "Saman Mehrnia",
        "Mahdi Jalili",
        "Xinghuo Yu"
      ],
      "abstract": "The increasing installation of Photovoltaics (PV) cells leads to more\ngeneration of renewable energy sources (RES), but results in increased\nuncertainties of energy scheduling. Predicting PV power generation is important\nfor energy management and dispatch optimization in smart grid. However, the PV\npower generation data is often collected across different types of customers\n(e.g., residential, agricultural, industrial, and commercial) while the\ncustomer information is always de-identified. This often results in a\nforecasting model trained with all PV power generation data, allowing the\npredictor to learn various patterns through intra-model self-learning, instead\nof constructing a separate predictor for each customer type. In this paper, we\npropose a clustering-based multitasking deep neural network (CM-DNN) framework\nfor PV power generation prediction. K-means is applied to cluster the data into\ndifferent customer types. For each type, a deep neural network (DNN) is\nemployed and trained until the accuracy cannot be improved. Subsequently, for a\nspecified customer type (i.e., the target task), inter-model knowledge transfer\nis conducted to enhance its training accuracy. During this process, source task\nselection is designed to choose the optimal subset of tasks (excluding the\ntarget customer), and each selected source task uses a coefficient to determine\nthe amount of DNN model knowledge (weights and biases) transferred to the aimed\nprediction task. The proposed CM-DNN is tested on a real-world PV power\ngeneration dataset and its superiority is demonstrated by comparing the\nprediction performance on training the dataset with a single model without\nclustering.",
      "tldr_zh": "本文提出了一种基于聚类的多任务深度神经网络（CM-DNN）框架，用于预测太阳能光伏（PV）发电，以应对不同客户类型（如住宅、农业等）数据带来的不确定性。方法包括使用 K-means 算法聚类数据、为每类客户训练独立的 DNN 模型，并在目标任务上通过源任务选择和知识转移（包括权重和偏差）来提升准确率。该框架在真实世界 PV 数据集上的实验表明，其预测性能优于单一模型训练方法。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2405.05989v2",
      "published_date": "2024-05-09 00:08:21 UTC",
      "updated_date": "2024-05-14 00:39:43 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T07:27:59.969212"
    }
  ],
  "raw_papers_fetched": true,
  "papers_count": 72,
  "processed_papers_count": 72,
  "failed_papers_count": 0,
  "summary_generated": true,
  "daily_data_saved": true,
  "last_update": "2025-05-18T07:28:21.227892"
}